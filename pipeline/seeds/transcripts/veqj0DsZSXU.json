{"text": " Okay, hi everybody and welcome to lesson 14. The numbers are getting up pretty high now, huh? We had a lesson last time talking about calculus and how we implement the chain rule in neural network training in an efficient way called backpropagation. I just wanted to point out that one excellent student, Koshik Sinha, has produced a very nice explanation of the code that we looked at last time. And I've linked to it. So it's got the math and then the code. The code's slightly different to what I had, but it's basically the same thing, some minor changes. And it's helpful, it might be helpful to kind of link between the math and the code to see what's going on. So you'll find that in the lesson 13 resources. But I thought I'd just quickly, you know, try to explain it as well. So maybe I could try to copy this. And just explain what's going on here with this code. So the basic idea is that we have a neural network that is calculating, well a neural network and a loss function that together that calculate a loss. So let's imagine that, well let's just call the loss function, we'll call it L. And the loss function is being applied to the output of the neural network. So the neural network function we'll call N. And that takes two things, a bunch of weights and a bunch of inputs. The loss function also requires the targets, but I'm just going to ignore that for now because it's not really part of what we actually care about. And what we're interested in knowing is if we want to be able to update the weights, let's say this is just a single layer thing, keep it simple. If we want to be able to update the weights, we need to know how does the loss change if we change the weights. If we change one weight at a time, if you like. So how would we calculate that? Well what we could do is we could rewrite our loss function by saying, well let's call capital N the result of the neural network applied to the weights and the inputs. And that way we can now rewrite the loss function to say L equals, big L equals, little l, the loss function, applied to the output of the neural network. And so maybe you can see where this is going. We can now say, okay the derivative of the loss with respect to the weights is going to be equal to the derivative of the loss with respect to the outputs of that neural network layer, times, this is the chain rule, the derivative of the outputs of that neural network layer. I'm going to get my notation consistent since these are not scalar. With respect to the weights. Right, so you can see we can get rid of those and we end up with the change in loss with respect to the weights. And so we can just say this is a chain rule. That's what the chain rule is. So the change in the loss with respect to the output of the neural network. Well we did the forward pass here. And then we took here, this here is where we calculated the derivative of the loss with respect to the output of the neural network. Which came out from here and ended up in diff. So there it is. So out.g contains this derivative. So then to calculate, let's actually do one more. We could also say the change in the loss with respect to the inputs. We can do the same thing with the chain rule. Times. And so this time we have the inputs. So here you can see that is this line of code. So that is the change in the loss with respect to the inputs. That's what input.g means. And it's equal to the change in the loss with respect to the output. So that's what out.g means. Times it's actually matrix times, because we're doing matrix calculus. Times this derivative, and since this is a linear layer we were looking at, this derivative is simply the weights themselves. And then we have exactly the same thing for w.g, which is the change in the loss, the derivative of the loss, with respect to the weights. And so again you've got the same thing. You've got your out.g, and remember we actually showed how we can simplify this into also a matrix product with a transpose as well. So that's how what's happening in our code is mapping to the math. So hopefully that's useful. But as I say, do check out this really nice resource which has a lot more detail if you're interested in digging deeper. The other thing I'd say is, if you... Some people have mentioned that they actually didn't study this at high school, which is fine. We've provided resources on the forum for recommending how to learn the basics of derivatives and the chain rule. And so in particular I would recommend 3Blue1Brown's Essence of Calculus series, and also Khan Academy. It's not particularly difficult to learn. It'll only take you a few hours, and then you can... This will make a lot more sense. Or if you did it at high school but you've forgotten it, same deal. So don't worry if you found this difficult because you had forgotten, or had never learnt, the basic derivative and chain rule stuff. That's something that you can pick up now, and I would recommend doing so. Okay, so what we then did last time, which is actually pretty exciting, is we got to a point where we had successfully created a training loop, which did these four steps. So and the nice thing is that every single thing here is something that we have implemented from scratch. Now we didn't always use our implemented from scratch versions. There's no particular reason to. When we've re-implemented something that already exists, let's use the version that exists. But every single thing here, I guess not argmax, but that's trivially easy to implement. Every single thing here we have implemented ourselves. And we successfully trained an MNIST model to 96% accurately recognize handwritten digits. So I think that's super neat. This is not a great metric. It's only looking at the training set, and in particular it's only looking at one batch of the training set. Since last time I've just refactored a little bit. I've pulled out this report function, which is now just running at the end of each epoch. And it's just printing out the loss and the accuracy. Just something I wanted to mention here is, hopefully you've seen fstrings before. They're a really helpful part of Python that lets you pop a variable or an expression inside curly braces in a string, and it'll evaluate it. You might not have seen this colon thing. This is called a format specifier. And with a format specifier you can change how things are printed in an fstring. So this is how I'm printing it to do decimal places. This is a two decimal places floating point number called loss, printed out here, followed by a comma. So I'm not going to show you how to use those, other than to say, yeah, Python fstrings and format specifiers are really helpful. So if you haven't used them before, do go look them up. A tutorial of the documentation, because they're definitely something that you'll probably find useful to know about. Okay, so let's just rerun all those lines of code. If you're wondering how I just reran all the cells above where I was, there's a cell here that's run all above. And it's so helpful that I always make sure there's a keyboard shortcut for that. So you can see here I've added a keyboard shortcut, QA. So if I type QA, it runs all cells above. If I type QB, it runs all cells below. And so yeah, stuff that you do a lot, make sure you've got keyboard shortcuts for them. You don't want to be fiddling around, moving around your mouse everywhere. You want it to be as easy as thinking. So this is really exciting. We've successfully trained a neural, built and trained a neural network model from scratch, and it works okay. It's a bit clunky. There's a lot of code. There's features we're missing. So let's start refactoring it. And so refactoring is all about making it so we have to write less code to do the same work. And so we're now going to, I'm going to show you something that's part of PyTorch, and then I'm going to show you how to build it. And then you'll see why this is really useful. So PyTorch, so PyTorch has a sub-module called nn, torch.nn. And in there, there's something called the module class. Now we can, we don't normally use it this way, but I just want to show you how it works. We can create an instance of it in the usual way where we create instances of classes, and then we can assign things to attributes of that module. So for example, let's assign a linear layer to it. And if we now print out that, you'll see it says, oh, this is a module containing something called foo, which is a linear layer. But here's something quite tricky. This module, we can say, show me all of the named children of that module. And it says, oh, there's one called foo, and it's a linear layer. And we can say, oh, show me all of the parameters of this module. And it says, oh, okay, sure, there's two of them. There's this four by three tensor, that's the weights. And there's this four long vector, that's the biases. And so somehow, just by creating this module and assigning this to it, it's automatically tracked what's in this module, and what are its parameters. That's pretty neat. So we're going to see both how and why it does that. I'm just going to point out, by the way, why did I add list here? If I just said m1.namedchildren, it just prints out generator object, which is not very helpful. And that's because this is a kind of iterator, called a generator, and it's something which is going to only produce the contents of this when I actually do something with it, such as list them out. So just popping a list around a generator is one way to run the generator and get its output. So that's a little trick when you want to look inside a generator. Okay, so now, as I said, we don't normally use it this way. What we normally do is we create our own class. So for example, we'll create our own multi-layer perceptron, and we inherit it. We inherit from nn.module. And so then in dunder inner, this is the thing that constructs an object of the class. This is the special magic method that does that. We'll say, okay, well how many inputs are there to this multi-layer perceptron? How many hidden activations, and how many output activations are there? So it'll just be one hidden layer. And then here we can do just like we did up here, where we assigned things as attributes. We can do that in this constructor. So we'll create an l1 attribute, which is a linear layer from number in to number hidden. L2 is a linear layer from number hidden to number out. And we'll also create a value. And so when we call that module, we can take the input that we get, and run the linear layer, and then run the value, and then run the l2. And so I can create one of these, as you see. And I can have a look and see like, oh here's the attribute l1. And there it is, like I had. And I can say print out the model, and the model knows all the stuff that's in it. And I can go through each of the named children, and print out the name and the layer. Now of course, if you remember, although you can use dunder call, we actually showed how we can refactor things using forward, such that it would automatically kind of do the things necessary to make all the, you know, automatic gradient stuff work correctly. And so in practice, we're actually not going to do dunder call, we would do forward. So this is an example of creating a custom PyTorch module. And the key thing to recognize is that it knows what are all the attributes you added to it. And it also knows what are all the parameters. So if I go through the parameters and print out their shapes, you can see I've got my linear layers weights, first linear layer, sorry second linear layer, my, I don't know, yeah, first linear layers weights, my first linear layers biases, second linear layers weights, second linear layers biases, and this 50 is because we set nh, the number of hidden, to 50. So why is that interesting? Well, because now I don't have to write all this anymore, going through layers and having to make sure that they've all been put into a list. We've just been able to add them as attributes, and they're automatically going to appear as parameters. So we can just say, go through each parameter and update it based on the gradient and the learning rate. And furthermore, you can actually just go model.zero grad and it will zero out all of the gradients. So that's really made our code quite a lot nicer and quite a lot more flexible, which is cool. So let's check that this still works. There we go. So just to clarify, if I called report on this before I ran it, as you would expect, the accuracy is about 8%, well about 10%, a bit less, and the loss is pretty high. And so after I run this fit, this model, the accuracy goes up and the loss goes down. So basically it's all of this exactly the same as before. The only thing I've changed are these two lines of code. So that's a really useful refactoring. So how on earth did this happen? How did it know what the parameters and layers are automatically? It used a trick called dunder setattr, and we're going to create our own nn.module now. So if there was no such thing as nn.module, here's how we'd build it. And so let's actually build it and also add some things to it. So in dunder init, we would have to create a dictionary for our named children. This is going to contain a list, a dictionary of all of the layers. Okay, and then just like before, we'll create a couple of linear layers. And then what we're going to do is we're going to define this special magic thing that Python has called dunder setattr. And this is called automatically by Python, if you have it, every time you set an attribute, such as here or here. And it's going to be passed the name of the attribute, the key, and the value is the actual thing on the right hand side of the equals sign. Now generally speaking, things that start with an underscore, we use for private stuff. So we check that it doesn't start with an underscore. And if it doesn't start with an underscore, setattr will put this value into the modules dictionary with this key. And then call the normal Python setattr to make sure it just actually does the attribute setting. So super is how you call whatever is in the super class, the base class. So another useful thing to know about is how does it do this nifty thing where you can just type the name and it kind of lists out all this information about it. That's a special thing called dunder repr. So here dunder repr will just have it return a stringified version of the modules dictionary. And then here we've got parameters. How did parameters work? So how did this thing work? Well, we can go through each of those modules, go through each value. So the values of the modules is all the actual layers. And then go through each of the parameters in each module and yield p. So that's going to create an iterator, if you remember when we looked at iterators, for all the parameters. So let's try it. So we can create one of these modules. And if we just like before, loop through its parameters, there they are. Now I'll just mention something that's optional, kind of like advanced Python that a lot of people don't know about. Which is there's no need to loop through a list or a generator, or I guess say loop through an iterator and yield. There's actually a shortcut, which is you can just say yield from, and then give it the iterator. And so with that, we can get this all down to one line of code, and it will do exactly the same thing. So that's basically saying yield one at a time, everything in here. That's what yield from does. So there's a cool little advanced Python thing, totally optional, but if you're interested, I think it can be kind of neat. So we have now learned how to create our own implementation of nn.module, and therefore we are now allowed to use PyTorch's nn.module. So that's good news. So how would we do, using the PyTorch nn.module, how would we create the model that we started with? Which is where we had this self.layers. Because we want to somehow register all of these all at once. That's not going to happen based on the code we just wrote. So to do that, let's have a look. We can, so let's make a list of the layers we want. And so we'll create again a subclass of nn.module. Make sure you call the superclasses init first. And we'll just store the list of layers. And then to tell PyTorch about all those layers, we basically have to loop through them and call add module. And say what the name of the module is, and what the module is. And again, probably should have used forward here in the first place. And you can see this is now done exactly the same thing. So if you've used a sequential model before, you can see that we're on the path to creating a sequential model. Okay, so Ganesh has asked an interesting question, which is, what on earth is super calling? Because we actually, in fact we don't even need the parentheses here. We actually don't have a base class. That's because if you don't put any parentheses, or if you put empty parentheses, it's actually a shortcut for writing that. And so Python has stuff in object which does, you know, all the normal objecty things, like storing your attributes so that you can get them back later. So that's what's happening there. Okay, so this is a little bit awkward. Is to have to store the list, and then enumerate, and call addModule. So now that we've implemented that from scratch, we can use PyTorch's version, which is, they've just got something called moduleList that just does that for you. Okay, so if you use moduleList, and pass it a list of layers, it will just go ahead and register them, all those modules for you. So here's something called sequential model. So this is just like an nnot sequential now. So if I create it, passing in the layers, there you go. You can see there's my model containing my moduleList with my layers. And so, I don't know why I never used forward for these things. It's silly. I guess it doesn't edit terribly in this stage, but anyhow. Okay, so call fit, and there we go. So in forward here, I just go through each layer, and I set the result of that equal to calling that layer on the previous result, and then pass and return it at the end. Now there's a little, another way of doing this, which I think is kind of fun. It's not like shorter or anything at this stage. I just wanted to show an example of something that you see quite a lot in machine learning code, which is the use of reduce. This implementation here is exactly the same as this thing here. So let me explain how it works. What reduce does, so reduce is a very common kind of like fundamental computer science concept. Reductions. This is something that does a reduction. And what a reduction is, is it's something that says, start with the third parameter, some initial value. So we're going to start with x, the thing we've been passed, and then loop through a sequence. So loop through each of our layers, and then for each layer, call some function. Here is our function, and the function is going to get passed first time around, it'll be passed the initial value, and the first thing in your list. So your first layer and x. So it's just going to call the layer function on x. The second time around, it takes the output of that, and passes it in as the first parameter, and passes in the second layer. So then the second time this goes through, it's going to be calling the second layer on the result of the first layer, and so forth. And that's what a reduction is. And so when you might see reduce, you'll certainly see it talked about quite a lot in papers and books, and you might sometimes also see it in code. It's a very general concept. And so here's how you can implement a sequential model using reduce. So there's no explicit loop there, although the loop is still happening internally. All right, so now that we've re-implemented sequential, we can just go ahead and use PyTorch's version. So there's an n.sequential. We can pass in our layers, and we can fit, not surprisingly. We can see the model. So yeah, it looks very similar to the one we built ourselves. All right, so this thing of looping through parameters, and updating our parameters based on gradients and a learning rate, and then zeroing them, is very common. So common that there is something that does that all for us, and that's called an optimizer. It's the stuff in Optim. So let's create our own optimizer, and as you can see, it's just going to do the two things we just saw. It's going to go through each of the parameters, and update them using the gradient and the learning rate, and there's also zero grad, which will go through each parameter, and set their gradients to zero. If you use .data, it's just a way of avoiding having to say torch.no grad, basically. Okay so in optimizer, we're going to pass it the parameters that we want to optimize, and we're going to pass it the learning rate, and we're just going to store them away. And since the parameters might be a generator, we'll call list to turn them into a list. So we are going to create our optimizer, pass it in the model.parameters, which have been automatically constructed for us by nn.module, and so here's our new loop. Now we don't have to do any of the stuff manually, we can just say Opt.step, so that's going to call this, and Opt.zeroGrad, and that's going to call this. There it is. So we've now built our own SGD optimizer from scratch. So I think this is really interesting, right? Like these things which seem like they must be big and complicated, once we have this nice structure in place, you know, an SGD optimizer doesn't take much code at all. And so it's all very transparent, simple, clear. If you're having trouble using complex library code that you've found elsewhere, you know, this can be a really good approach, is to actually just go all the way back, remove as many of these abstractions as you can, and like run everything by hand to see exactly what's going on. It can be really freeing to see that you can do all this. Anyways, since PyTorch has this for us, in torch.optim, it's got a optim.sgd, and just like our version, you pass in the parameters, and you pass in the learning rate, so you really see it is just the same. So let's define something called getModel. That's going to return the model, the sequential model, and the optimizer for it. So if we go model, opt equals getModel, and then we can call the loss function to see where it's starting. And so then we can write our training loop again. Go through each epoch, go through each starting point for our batches, grab the slice, slice into our X and Y in the training set, calculate our predictions, calculate our loss, do the backward pass, do the optimizer step, do the zero gradient, and print out how you're going at the end of each one. And there we go. All right, so let's keep making this simpler. There's still too much code. So one thing we could do is we could replace these lines of code with one line of code, by using something we'll call the dataset class. So the dataset class is just something that we're going to pass in our independent and dependent variable. We'll store them away as self.x and self.y. We'll have something, so if you define dunder len, then that's the thing that allows the len function to work. So the length of the data set will just be the length of the independent variables. And then dunder get item is the thing that will be called automatically any time you use square brackets in Python. So that's just going to call this function passing in the indices that you want. So when we grab some items from our data set, we're going to return a tuple of the X values and the Y values. So then we'll be able to do this. So let's create a data set using this tiny little tree line class. It's going to be a data set containing the X and Y training, and they'll create another data set containing the X and Y valid. And those two data sets will call train ds and valid ds. So let's check the length of those data sets should be the same as the length of the Xs, and they are. And so now we can do exactly what we hoped we could do. We can say xb, yb equals train ds and pass in some slice. So that's going to give us back our, check the shapes are correct, it should be 5 by 28 by 28, 5 by 28 times 28, and the Ys should just be 5. And so here they are, the Xs and the Ys. So that's nice. We've created a data set from scratch. It's not complicated at all. And if you look at the actual PyTorch source code, this is basically all data sets do. So let's try it. We call getModel. And so now we've replaced our data set line with this one. And as per usual, it still runs. And so this is what I do when I'm writing code, is I try to like always make sure that my starting code works as I refactor. And so you can see all the steps. And so somebody reading my code can then see exactly like, why am I building everything I'm building? How does it all fit in? See that it still works. And I can also keep it clear in my own head. So I think this is a really nice way of implementing libraries as well. All right. So now we're going to replace these two lines of code with this one line of code. So we're going to create something called a data loader. And a data loader is something that's just going to do this. So we need to create an iterator. So an iterator is a class that has a dunder iter method. When you say for in, in Python, behind the scenes, it's actually calling dunder iter to get a special object, which it can then loop through using yield. So it's basically getting this thing that you can iterate through using yield. So a data loader is something that's going to have a data set and a batch size, because we're going to go through the batches and grab one batch at a time. So we have to store away the data set and the batch size. And so when you, when we call the for loop, it's going to call dunder iter, we're going to want to do exactly what we saw before. Go through the range, just like we did before. And then yield that bit of the data set. And that's all. So that's a data loader. So we can now create a train data loader and a valid data loader from our train data set and valid data set. And so now we can, if you remember the way you can get one thing out of an iterator, so you don't need to use a for loop, you can just say iter. And that will also call dunder iter. Next we'll just grab one value from it. So here we will run this, and you can see we've now just confirmed. XB is a 50 by 784, and YB, there it is. And then we can check what it looks like. So let's grab the first element of our X batch, make it 28 by 28. And there it is. So now that we've got a data loader, again, we can grab our model and we can simplify our fit function to just go for XB, YB, and train deal. So this is getting nice and small, don't you think? And it still works the same way. OK, so this is really cool. And now that it's nice and concise, we can start adding features to it. So one feature I think we should add is that our training set, each time we go through it, it should be in a different order. It should be randomized, the order. So instead of always just going through these indexes in order, we want some way to say go use random indexes. So the way we can do that is create a class called Sampler. And what Sampler is going to do, I'll show you, is if we create a Sampler without shuffle, without randomizing it, it's going to simply return all the numbers from 0 up to N, in order, and it'll be an iterator. See this is done to iter. But if I do want it shuffled, then it will randomly shuffle them. So here you can see I've created a Sampler without shuffle. So if I then make an iterator from that, and print a few things from the iterator, you can see it's just printing out the indexes it's going to want. Or I can do exactly the same thing as we learned earlier in the course using islice. We can grab the first five. So here's the first five things from a Sampler when it's not shuffled. So as you can see, these are just indexes. So we could add shuffle equals true, and now that's going to call random.shuffle, which just randomly permits them. And now if I do the same thing, I've got random indexes of my source data. So why is that useful? Well what we could now do is create something called a batch Sampler. And what the batch Sampler is going to do, is it's going to basically do this islice thing for us. So we're going to say, okay, pass in a Sampler, that's something that generates indices, and pass in a batch size. And remember we've looked at chunking before. It's going to chunk that iterator by that batch size. And so if I now say, all right, please take our Sampler and create batches of four. As you can see here, it's creating batches of four indices at a time. So rather than just looping through them in order, I can now loop through this batch Sampler. So we're going to change our data loader, so that now it's going to take some batch Sampler, and it's going to loop through the batch Sampler. That's going to give us indices. And then we're going to get that data set item from that batch, for everything in that batch. So that's going to give us a list, and then we have to stack all of the X's and all of the Y's together into tensors. So I've created something here called collate function, and we're going to default that to this little function here, which is going to grab our batch, pull out the X's and Y's separately, and then stack them up into tensors. So this is called our collate function. So if we put all that together, we can create a training sampler, which is a batch sampler over the training set, with shuffle true. A validation sampler will be a batch sampler over the validation set, with shuffle false. And so then we can pass that into this data loader class. The training data set, and the training sampler, and the collate function, which we don't really need because we're just using the default one. So I guess we can just get rid of that. And so now, there we go, we can do exactly the same thing as before. XB, YB is next iter, and this time we use the valid data loader. Check the shapes. And this is how PyTorch's actual data loaders work. This is all the pieces they have. They have samplers, they have batch samplers, they have a collation function, and they have data loaders. So remember that what I want you to be doing for your homework is experimenting with these carefully to see exactly what each thing is taking in. Okay, so Pieter is asking on the chat, what is this collate thing doing? So collate function, it defaults to collate. What does it do? Well, let's see, let's go through each of these steps. So we've got a batch sampler, so let's do just the valid sampler. So the batch sampler, here it is. So we're going to go through each thing in the batch sampler. So let's just grab one thing from the batch sampler. So the output of the batch sampler will be next iter. So here's what the batch sampler contains. All right, just the first 50 digits, not surprisingly, because this is our validation sampler. If we did a training sampler, that would be randomized. There they are. Okay, so then what we then do is we go self.dataset i for i and b. So let's copy that. Copy, paste. And so rather than self.dataset i, we'll just say valid TS i. Oh, and it's not i and b, it's i and o, that's what we called it. Oh, and we did it for training, sorry. Training. Okay, so what it's created here is a list of tuples of tensors, I think. So let's have a look. So we'll call this p, whatever. So p0, okay, is a tuple. It's got the x and the y, independent and dependent variable. So that's not what we want. What we want is something that we can loop through. We want to get batches. So what the collation model is going to do, sorry not collation model, the collate function is going to do, is it's going to take all of our x's and all of our y's and collate them into two tensors. One tensor of x's and one tensor of y's. So the way it does that is it first of all calls zip. So zip is a very, very commonly used Python function. It's got nothing to do with a compression program zip, but instead what it does is it effectively allows us to like transpose things. So that now, as you can see, we've got all of the second elements, or index one elements, all together, and all of the index zero elements together. So then we can stack those all up together. And that gives us our y's for our batch. So that's what collate does. So the collate function is used an awful lot in PyTorch. Increasingly nowadays where hugging face stuff uses it a lot. And so we'll be using it a lot as well. And basically it's a thing that allows us to customize how the data that we get back from our data set, once it's been kind of generating a list of things from the data set, how do we put it together into a bunch of things that our model can take as inputs. Because that's really what we want here. So that's what the collation function does. Oh, this is the wrong way around. Like so. This is something that I do so often that Fastcore has a quick little shortcut for it, just called store attr, store attributes. And so if you just put that in your dunder init, then you just need one line of code and it does exactly the same thing. So there's a little shortcut as you see. And so you'll see that quite a bit. All right, let's have a seven minute break and see you back here very soon. And we're going to look at a multi-processing data loader and then we'll have nearly finished this notebook. All right, see you soon. All right, let's keep going. So we've seen how to create a data loader and sampling from it. The PyTorch data loader works exactly like this, but it uses a lot more code because it implements multi-processing. And so multi-processing means that the actual, this thing here, that code can be run in multiple processes. They can be run in parallel for multiple items. So this code, for example, might be opening up a JPEG, rotating it, flipping it, etc. So because remember, this is just calling the dunder get item for a data set. So that could be doing a lot of work for each item and we're doing it for every item in the batch. So we'd love to do those all in parallel. So I'll show you a very quick and dirty way that basically does the job. So Python has a multi-processing library. It doesn't work particularly well with PyTorch tensors. So PyTorch has created an exact reimplementation of it. So it's identical API wise, but it does work well with tensors. So this is basically, we'll just grab the multi-processing. So this is not quite cheating because multi-processing is in the standard library and this is API equivalent. So I'm going to say we're allowed to do that. So as we've discussed, you know, when we call square brackets on a class, it's actually identical to calling the dunder get item function on the object. So you can see here, if we say give me items 3, 6, 8, and 1, it's the same as calling dunder get item passing in 3, 6, 8, and 1. Now why does this matter? Well, I'll show you why. It matters because we're going to be able to use map and I'll explain why we want to use map in a moment. Map is a really important concept. You might have heard of map reduce. So we've already talked about reductions and what those are. Maps are kind of the other key piece. So a map is something which takes a sequence and calls a function on every element of that sequence. So imagine we had a couple of batches of indices 3 and 6 and 8 and 1. Then we're going to call dunder get item on each of those batches. So that's what map does. Map calls this function on every element of the sequence. So that's going to give us the same stuff, but now this same as this, but now batched into two batches. Now why do we want to do that? Because multi-processing has something called pool, where you can tell it how many workers you want to run, how many processes you want to run. And it then has a map which works just like the Python, normal Python map, but it runs this function in parallel over the items from this iterator. So this is how we can create a multi-processing data loader. So here we're creating our data loader. Again we don't actually need to pass in the collect function because we're using the default one. So if we say n workers equals 2, and then create that. If we say next, see how it's taking a moment? It took a moment because it was firing off those two workers in the background. So the first batch actually comes out more slowly. But the reason that we would use a multi-processing data loader is if this is doing a lot of work, we want it to run in parallel, and even though the first item might come out a bit slower, once those processes are fired up it's going to be faster to run. So this is a really simplified multi-processing data loader. Because this needs to be super super efficient, PyTorch has lots more code than this to make it much more efficient. But the idea is this, and this is actually a perfectly good way of experimenting or building your own data loader to make things work exactly how you want. So now that we've re-implemented all this from PyTorch, let's just grab the PyTorch, as you can see they're exactly the same data loader. They don't have one thing called sampler that you pass shuffle to, they have two separate classes called sequential sampler and random sampler. I don't know why they do it that way, it's a little bit more work to me, but same idea. And I've got batch sampler, and so it's exactly the same idea. The training sampler is a batch sampler with a random sampler. The validation sampler is a batch sampler with a sequential sampler. Passing batch sizes, and so we can now pass those samplers to the data loader. This is now the PyTorch data loader, and just like ours, it also takes a collate function. And it works. Cool. So as you can see, it's doing exactly the same stuff that ours is doing, with exactly the same API. And it's got some shortcuts, as I'm sure you've noticed when you've used data loaders. So for example, calling batch sampler is going to be very very common. So you can actually just pass the batch size directly to a data loader, and it will then auto-create the batch samplers for you. So you don't have to pass in batch sampler at all. Instead you can just say sampler, and it will automatically wrap that in a batch sampler for you. So it does exactly the same thing. And in fact, because it's so common to create a random sampler, or a sequential sampler for a data set, you don't have to do that manually. You can just pass in shuffle equals true, or shuffle equals false to the data loader. And that does again exactly the same thing. There it is. Now something that is very interesting, is that when you think about it, the batch sampler and the collation function are things which are taking the result of the sampler, looping through them, and then collating them together. But what we could do, is actually because our data sets know how to grab multiple indices at once, we can actually just use the batch sampler as a sampler. We don't actually have to loop through them and collate them, because they're basically instantly, they come pre-collated. So this is a trick which actually HuggingFace stuff can use as well, and we'll be seeing it again. So this is an important thing to understand, is how come we can pass a batch sampler to sampler, and what's it doing? And so rather than trying to look through the PyTorch code, I suggest going back to our non-multiprocessing pure Python code, to see exactly how that would work. Because it's a really nifty trick for things that you can grab multiple things from at once, and it can save a whole lot of time, it can make your code a lot faster. OK, so now that we've got all that nicely implemented, we should now add a validation set. And there's not really too much to talk about here, we'll just take our fit function, and this is exactly the same code that we had before. And then we're just going to add something which goes through the validation set, and gets the predictions, and sums up the losses and accuracies, and from time to time prints out the loss and accuracy. And so getDls we will implement by using the PyTorch data loader now. And so now our whole process will be getDls, passing in the training and validation data set. Notice that for our validation data loader, I'm doubling the batch size, because it doesn't have to do back propagation, so it should use about half as much memory, so I can use a bigger batch size. Get our model, and then call this fit. And now it's printing out the loss and accuracy on the validation set. So finally we actually know how we're doing, which is that we're getting 97% accuracy on the validation set. And that's on the whole thing, not just on the last batch. So that's cool. We've now implemented a proper working, sensible training loop. It's still, you know, a bit more code than I would like, but it's not bad. And every line of code in there, and every line of code it's calling, is all stuff that we have built ourselves, re-implemented ourselves. So we know exactly what's going on, and that means it's going to be much easier for us to create anything we can think of. We don't have to rely on other people's code. So hopefully you're as excited about that as I am, because it really opens up a whole world for us. So one thing that we're going to want to be able to do now that we've got a training loop, is to grab data. And there's a really fantastic library of datasets available on HuggingFace nowadays. And so let's look at how we use those datasets, now that we know how to bring things into data loaders and stuff. So that now we can use the entire world of HuggingFace datasets with our code. So we're going to, so you need to pip install datasets. And once you've pipped installed datasets, you'll be able to say from datasets import, and you can import a few things, just these two things now, load dataset, load dataset builder. So we're going to look at a dataset called Fashion MNIST. And so the way things tend to work with HuggingFace is there's something called the HuggingFace hub, which has models and it has datasets, amongst other things. And generally you'll give them a name, and you can then say in this case, load a dataset builder for Fashion MNIST. Now a dataset builder is just basically something which has some metadata about this dataset. So the dataset builder has a .info and the .info has a .description. And here's a description of this. And as you can see, again, we've got 28 by 28 grayscale. So it's going to be very familiar to us, because it's just like MNIST. And again, we've got 10 categories. And again, we've got 60,000 training examples. And again, we've got 10,000 test examples. So this is cool. So as it says, it's a direct drop-in replacement for MNIST. And so the dataset builder also will tell us what's in this dataset. And so HuggingFace stuff generally uses dictionaries rather than tuples. So there's going to be an image of type image. There's going to be a label of type class label. There's 10 classes, and these are the names of the classes. So it's quite nice that in HuggingFace datasets, you know, we can kind of get this information directly. It also tells us if there are some recommended training test splits, we can find out those as well. So this is the size of the training split and the number of examples. So now that we're ready to start playing with it, we can load the dataset. Okay, so this is a different string, load dataset builder versus load dataset. So this will actually download it, cache it. And here it is. And it creates a dataset dictionary. So a dataset dictionary, if you've used Fast.ai, is basically just like what we call the datasets class. They call the dataset dict class. So it's a dictionary that contains, in this case, a train and a test item, and those are datasets. And these datasets are very much like the datasets that we created in the previous notebook. So we can now grab the training and test items from that dictionary, and just pop them into variables. And so we can now have a look at the zero index thing in training. And just like we were promised, it contains an image and a label. So as you can see, we're not getting tuples anymore. We're getting dictionaries containing the x and the y, in this case image and label. So I'm going to get pretty bored writing image and label in strings all the time, so I'm just going to store them as x and y. So x is going to be the string image, and y will be the string label. I guess the other way I could have done that would have been to say x comma y equals that. That would probably be a bit neater, because it's coming straight from the features. And if you iterate into a dictionary, you get back its keys. That's why that works. So anyway, I've done it manually here, which is a bit sad, but there you go. Okay, so we can now grab the from train zero, which we've already seen. We can grab the x, i.e. the image, and there it is. There's the image. We could grab the first five images, and the first five labels, for example. And there they are. We already know what the names of the classes are, so we could now see what these map to, by grabbing those features. So there they are. So this is a special Huckingface class, which most libraries have something, including Fast.ai, that works like this. There's something called int to string, which is going to take these and convert them to these. So if I call it on our y batch, you'll see we've got, first is ankle boot, and there that is indeed an ankle boot. Then we're going to have a couple t-shirts and a dress. Okay, so how do we use this to train a model? Well we're going to need a data loader, and we want a data loader that for now we're going to do just like we've done it before. It's going to return, well actually we're going to do something a bit different. We're going to have our collate function, is actually going to return a dictionary. Actually this is pretty common for Huckingface stuff, and PyTorch doesn't mind if you, it's happy for you to return a dictionary from a collation function. So rather than returning a tuple of the stacked up, hopefully this looks very familiar. This looks a lot like the thing that goes through the data set for each one and stacks them up, just like we did in the previous notebook. So that's what we're doing. We're doing all in one step here in our collate function. And then again exactly the same thing, go through our batch, grab the y, and this is just stacking them up with their integers, so we don't have to call stack. And so we're now going to have the image and label bits in our dictionary. So if we create our data loader using that collation function, grab one batch, so we can go batch x dot shape is a 16 by 1 by 28 by 28, and our y of the batch, here it is. So the thing to notice here is that we haven't done any transforms or anything, or written our own data set class or anything. We're actually putting all the work directly in the collation function. So this is like a really nice way to skip all of the kind of abstractions of your framework, if you want to, is you can just do all of your work in collate function. So it's going to pass you each item, so you're going to get the batch directly, you just go through each item. And so here we're saying, okay grab the x key from that dictionary, convert it to a tensor, and then do that for everything in the batch, and then stack them all together. So this is, yeah, this is like, can be quite a nice way to do things, if you want to do things just very manually, without having to think too much about, you know, a framework. Particularly if you're doing really custom stuff, this can be quite helpful. Having said that, hugging face data sets absolutely lets you avoid doing everything in collate function, which if we want to create really simple applications, that's where we're going to eventually want to head. So we can do this using a transform instead. And so the way we do that, is we create a function, we're going to take our batch, it's going to replace the x in our batch with the tensor version of each of those pal images. And I'm not even stacking them or anything, and then we're going to return that batch. And so hugging face data sets has something called with transform, and that's going to take your data set, your hugging face data set, and it's going to apply this function to every element. And it doesn't run it all now, it's going to basically, when it, behind the scenes, when it calls done to get item, it will call this function on the fly. So in other words, this could have data augmentation, which can be random or whatever, because it's going to be rerun every time you grab an item. It's not cached or anything like that. So other than that, this data set has exactly the API, same API as any other data set. It has a length, it has a done to get item, so you can pass it to a data loader. And so PyTorch already knows how to collate dictionaries of tensors. So we've got a dictionary of tensors now, so that means we don't need a collate function anymore. I can create a data loader from this without a collate function, as you can see. And so this is giving me exactly the same thing as before, but without having to create a custom collate function. Now even this is a bit more code than I want, having to return this seems a bit silly, but the reason I had to do this is because hugging face data sets expects the with transform function to return the new version of the data. So I wanted to be able to write it like this, transform in place, and just say the change I want to make, and have it automatically return that. So if I create this function that's exactly the same as the previous one, but doesn't have return, how would I turn this into something which does return the result? So here's an interesting trick. We could take that function, pass it to another function, to create a new function which is the version of this in place function that returns the result. And the way I do that is by creating a function called in place. It takes a function, it returns a function, the function it returns is one that calls my original function, and then returns the result. So this is the function, this is a function generating function, and it's modifying an in place function to become a function that returns the new version of that data. And so this is a function. This function is passed to this function, which returns a function, and here it is. So here's the version that HuggingFace will be able to use. So I can now pass that to with transform, and it does exactly the same thing. So this is very very common in Python. It's so common that this line of code can be entirely removed and replaced with this little token. If you have a function, and put at at the start, you can then put that before a function, and what it says is take this whole function, pass it to this function, and replace it with the result. So this is exactly the same as the combination of this and this. And when we do it this way, this kind of little syntax sugar is called a decorator. Okay, so there's nothing magic about decorators. It's literally, literally identical to this. Oh, I guess the only difference is we don't end up with this unnecessary intermediate underscore version. But the result is exactly the same, and therefore I can create a transformed data set by using this. And there we go, it's all working fine. Yeah, so I mean none of this is particularly necessary, but what we're doing is we're just kind of like seeing, you know, the pieces that we can we can put in place to make this stuff as easy as possible, and we don't have to think about things too much. All right, now with all this we can basically make things pretty automatic. And the way we can make things pretty automatic is we're going to use a cool thing in Python called itemGetter. And itemGetter is a function that returns a function. So hopefully you're getting used to this idea now. This creates a function that gets the a and c items from a dictionary, or something that looks like a dictionary. So here's a dictionary. It contains keys a, b, and c. So this function will take a dictionary and return the a and c values. And as you can see it has done exactly that. I'll explain why this is useful in a moment. I just wanted to briefly mention what did I mean when I said something that looks like a dictionary. I mean this is a dictionary. Okay, that looks like a dictionary. But Python doesn't care about what type things actually are. It only cares about what they look like. And remember that when we call something with square brackets, when we index into something, behind the scenes it's just calling done to get item. So we could create our own class. And it's done to get item, gets the key. And it's just going to manually return one if k equals a, or two if k equals b, or three otherwise. And look, that class also works just fine with an itemGetter. The reason this is interesting is because a lot of people write Python as if it's like C++ or Java or something. They write as if it's this kind of statically typed thing. But I really wanted to point out that it's an extremely dynamic language, and there's a lot more flexibility than you might have realized. Anyway that's a little aside. So what we can do is, think about a batch for example, where we've got these two dictionaries. So PyTorch comes with a default collation function called, not surprisingly, default collate. So that's part of PyTorch. And what default collate does with dictionaries, is it simply takes the matching keys, and then grabs their values, and stacks them together. And so that's why if I call default collate, a is now one three, b is now two four. That's actually what happened before, when we created this data loader, is it used the default collation function, which does that. It also works on things that are tuples, not dictionaries, which is what most of you would have used before. And what we can do therefore, is we could create something called collate dict, which is something which is going to take a dataset, and it's going to create a item getter function, for the features in that dataset, which in this case is image and label. So this is a function which will get the image and label items. And so we're now going to return a function, and that function is simply going to call our item getter on default collate. And what this is going to do, is it's going to take a dictionary, and collate it into a tuple. Just like we did up here. So if we run that, so we're now going to call data loader on our transform dataset, passing in, and remember this is a function that returns a function. So it's a collation function for this dataset, and there it is. So now this looks a lot like what we had in our previous notebook. This is not returning a dictionary, but it's returning a tuple. So this is a really important idea, particularly for working with hugging face datasets, is that they tend to do things with dictionaries, and most other things in the PyTorch world tend to work with tuples. So you can just use this now to convert anything that returns dictionaries into something that provides tuples, by passing it as a collation function to your data loader. So remember, you know, the thing you want to be doing this week is doing things like import PDB, PDB.setTrace, right. Put breakpoints, step through, see exactly what's happening, you know, not just here, but also, even more importantly, doing it inside, inside the inner function, so then you can see, what did I do wrong there? Oh, did I, oops, set underscore trace. So then we can see exactly what's going on, print out B, list the code, and I could step into it, and look, I'm now inside the default collate function, which is inside PyTorch, and so I can now see exactly how that works, and there it all is. So it's going to go through, and this code is going to look very familiar, because we've implemented all this ourselves, except it's being careful, it works for lots of different types of things, dictionaries, NumPy arrays, so on and so forth. So the first thing I wanted to do, oh, actually, something I did want to mention here, this is so useful, we want to be able to use it in all of our notebooks. So rather than copying and pasting this every time, it would be really nice to create a Python module that contains this definition. So we've created a library called nbdev, it's really a whole system, called nbdev, which does exactly that, it creates modules you can use from your notebooks, and the way you do it is you use this special thing we call comment directives, which is hashpipe, and then hashpipe export, so you put this at the top of a cell, and it says do something special for this cell. What this does is it says put this into a Python module for me please, export it to a Python module. What Python module is it going to put it in? Well if you go all the way to the top, you tell it what default export module to create, so it's going to create a module called datasets. So what I do at the very end of this module is I've got this line that says import nbdev, nbdev.nbdev export, and what that's going to do for me is create a library, a Python library, it's going to have a datasets.py in it, and we'll see everything that we exported, here it is, collate dict, will appear in this for me, and so what that means is now in the future in my notebooks I will be able to import collate dict from my datasets. Now you might wonder, well how does it know to call it mini AI, what's mini AI? Well in nbdev you create a settings.ini file, where you say what the name of your library is. So we're going to be using this quite a lot now because we're getting to the point where we're starting to implement stuff that didn't exist before. So previously most of the stuff, or pretty much all the stuff we've created, I've said like oh that already exists in PyTorch, so we don't need it, we just use PyTorches. But we're now getting to a point where we're starting to create stuff that doesn't exist anywhere, we've created it ourselves, and so therefore we want to be able to use it again. So during the rest of this course we're going to be building together a library called mini AI that's going to be our framework, our version of something like fast.ai, maybe it's something like what fast.ai3 will end up being, we'll see. So that's what's going on here. So we're going to be using, once I start using mini AI, I'll show you exactly how to install this, but that's what this export is. And so you might have noticed I also had an export on this in place thing, and I also had it on my necessary import statements. Okay, we want to be able to see what this data set looks like, so I thought now's a good time to talk a bit about plotting, because knowing how to visualize things well is really important, and again the idea is we're not allowed to use fast.ai's plotting library, so we've got to learn how to do everything ourselves. So here's the basic way to plot an image using matplotlib. So we can create a batch, grab the x part of it, grab the very first thing in that, and I am show, means show an image, and here it is, there is our ankle boot. So let's start to think about what stuff we might create which we can export to make this a bit easier. So let's create something called show image, which basically does, I am show, but we're going to do a few extra things. We will make sure that it's in the correct axis order. We will make sure it's not on CUDA, that it's on the CPU. If it's not a numpy array, we'll convert it to a numpy array. We'll be able to pass in an existing axis, which we'll talk about soon if we want to. We'll be able to set a title if we want to, and also this thing here removes all this ugly 0 5 blah blah blah axis, because we're showing an image, we don't want any of that. So if we try that, you can see, there we go, we've also been able to say what size we want the image, there it all is. Now here's something interesting, when I say help, the help shows the things that I implemented, but it also shows a whole lot more things. How did that magic thing happen? You can see they work, because here's figsize, which I didn't add, oh sorry I did add, well okay, that's a bad example. Anyway, these other ones all work as well. So how did that happen? Well the trick is that I added star star quags here, and star star quags says, you can pass as many or any other arguments as you like that aren't listed, and they'll all be put into a dictionary with this name. And then when I call I am show, I pass that entire dictionary, star star here means as separate arguments. And that's how come it works. And then how come it knows what help to provide? The reason why is that Fastcore has a special thing called delegates, which is a decorator, so now you know what a decorator is, and you tell it what is it that you're going to be passing quags to. I'm going to be passing it to I am show. And then it automatically creates the documentation correctly to show you what quags can do. So this is a really helpful way of being able to kind of extend existing functions, like I am show, and still get all of their functionality and all of their documentation and add your own. So delegates is one of the most useful things we have in Fastcore, in my opinion. So we're going to export that, so now we can use show image anytime we want, which is nice. Something that's really helpful to know about matplotlib is how to create subplots. So for example, what happens if you want to plot two images next to each other? So in matplotlib, subplots creates multiple plots, and you pass it number of rows and the number of columns. So this here has, as you see, one row and two columns. And it returns axes. Now what it calls axes is what it refers to as the individual plots. So if we now call show image on the first image, passing in axes 0, it's going to get that here. Then we call ax.amshow, that means put the image on this subplot. They don't call it a subplot, unfortunately, they call it an axis. Put it on this axis. So that's how come we're able to show an image, one image on the first axis, and then show a second image on the second axis, by which we mean subplot. And there's our two images. So that's pretty handy. So I've decided to add some additional functionality to subplots. So therefore I use delegates on subplots, because I'm adding functionality to it. And I'm going to be taking quags and passing it through to subplots. And the main thing I wanted to do is to automatically create an appropriate figure size by just finding out, you tell us what image size you want. And I also want to be able to add a title for the whole set of subplots. And so there it is. And then I also want to show you that it'll automatically, if we want to, create documentation for us as well, for our library. And here is the documentation. So as you can see here, for the stuff I've added, it's telling me exactly what each of these parameters are, their type, their defaults, and information about each one. And that information is automatically coming from these little comments. We call these documents. This is all automatic stuff done by FastCore and NBDev. And so you might have noticed when you look at FastAI library documentation, it always has all this info. That's why. You don't actually have to call show doc, it automatically added to your documentation for you. I'm just showing you here what it's going to end up looking like. And you can see that it's worked with delegates. It's put all the extra stuff from delegates in here as well. And they all listed out here as well. So anyway, subplots. So let's create a three by three set of plots. And we'll grab the first two images. And so now we can go through each of the subplots. Now it returns it as a three by three, basically a list of three lists of three items. So I flatten them all out into a single list. So we'll go through each of those subplots, and go through each image, and show each image on each axis. And so here's a quick way to quickly show them all. As you can see, it's a little bit ugly here. So we'll keep on adding more useful plotting functionality. So here's something that again, it calls our subplots, delegates to it. But we're going to be able to say, for example, how many subplots do we want? And it'll automatically calculate the rows and the columns. And it's going to remove the axes for any ones that we're not actually using. And so here we got that. So that's what getGrid's going to let us do. So we're getting quite close. And so finally, why don't we just create a single thing called showImages that's going to get our grid. And it's going to go through our images, optionally with a list of titles, and show each one. And we can use that here. You can see we have successfully got all of our labeled images. And so we, yeah, I think all this stuff for the plotting is pretty useful. So as you might have noticed, they were all exported. So in our datasets.py, we've got our getGrid, we've got our subplots, we've got our showImages. So that's going to make life easier for us now, since we have to create everything from scratch. We have created all of those things. So as I mentioned at the very end, we have this one line of code to run. And so just to show you, if I remove many.ai.datasets.py, so it's all empty. And then I run this line of code, and now it's back, as you can see. And it tells you it's auto-generated. All right. So we are nearly at the point where we can build our learner. And once we've built our learner, we're going to be able to really dive deep into training and studying models. So we've nearly got all of our infrastructure in place. Before we do, there's some pieces of Python, which not everybody knows, and I want to talk about, and computer science concepts I want to talk about. So that's what 06 Foundations is about. So this whole section is just going to talk about some stuff in Python that you may not have come across before. Or maybe it's a review for some of you as well. And it's all stuff we're going to be using basically in the next notebook. So that's why I wanted to cover it. So we're going to be creating a learner class. So a learner class is going to be a very general purpose training loop, which we can get to do anything that we wanted to do. We're going to be creating things called callbacks to make that happen. And so therefore we're going to just spend a few moments talking about what are callbacks, how are they used in computer science, how are they implemented, look at some examples. They come up a lot. Perhaps the most common place that you see callbacks in software is for GUI events. So for events from some graphical user interface. So the main graphical user interface library in Jupyter notebooks is called ipywidgets. And we can create a widget, like a button, like so. And when we display it, it shows me a button. And at the moment it doesn't do anything, if I click on it. What we can do though is we can add an onClick callback to it, which is something which is a function, we're going to pass it a function, which is called when you click it. So let's define that function. So I'm going to say w.onClickf is going to assign the f function to the onClick callback. Now if I click this, there you go, it's doing it. Now what does that mean? Well a callback is simply a callable that you've provided. So remember a callable is a more general version of a function, so in this case it is a function, that you've provided, that will be called back to when something happens. So in this case, if something that's happening is that they're clicking a button. So this is how we are defining and using a callback as a GUI event. So basically everything in IPyWidgets, if you want to create your own graphical user interfaces for Jupyter, you can do it with IPyWidgets and by using these callbacks. So these particular kinds of callbacks are called events, but it's just a callback. All right, so that's somebody else's callback. Let's create our own callback. So let's say we've got some very slow calculation, and so it takes a very long time to add up the numbers 0 to 5 squared, because we sleep for a second after each one. So let's run our slow calculation, still running. Oh, how's it going? Come on, finish our calculation. There we go, the answer is 30. Now for a slow calculation like that, such as training a model, it's a slow calculation, it would be nice to do things like, I don't know, print out the loss from time to time, or show a progress bar, or whatever. So generally for those kinds of things, we would like to define a callback that is called at the end of each epoch, or batch, or every few seconds, or something like that. So here's how we can modify our slow calculation routine, such that you can optionally pass it a callback. And so all of this code is the same, except we've added this one line of code that says, if there's a callback, then call it, and pass in where we're up to. So then we could create our callback function. So this is just like we created a callback function f, let's create a show progress callback function that's going to tell us how far we've got. So now if we call slow calculation passing in our callback, you can see it's going to call this function at the end of each step. So here we've created our own callback. So there's nothing special about a callback, like it doesn't require its own syntax, it's not a new concept, it's just an idea, really. Which is the idea of passing in a function which some other function will call at particular times. Such as at the end of a step, or such as when you click a button. So that's what we mean by callbacks. We don't have to define the function ahead of time. We could define the function at the same time that we call the slow calculation, by using lambda. So as we've discussed before, lambda just defines a function but it doesn't give it a name. So here's a function that takes one parameter and prints out exactly the same thing as before. So here's the same way as doing it, but using a lambda. We could make it more sophisticated now, and rather than always saying, awesome we finished epoch, whatever, we could let you pass in an exclamation, and we print that out. And so in this case we could now have our lambda call that function. And so one of the things that we can do now is to, again, we can create a function that returns a function. And so we could create a make show progress function, where you pass in the exclamation. We could then create, and there's no need to give it a name actually, let's just return it directly. We can return a function that calls that exclamation. So here we are passing in nice. And that's exactly the same as doing something like what we've done before. We could say instead of using a lambda, we can create an inner function like this. So here is now a function that returns a function. This does exactly the same thing. Okay, so one way with a lambda, one way without a lambda. One of the reasons I wanted to show you that is so I can, I've got so many here, is that we can do exactly the same thing using partial. So with partial, it's going to do exactly the same thing as this kind of make show progress. It's going to call show progress and pass, okay I guess. So this is again an example of a function returning a function. And so this is a function that calls show progress passing in this as the first parameter. And again, it does exactly the same thing. Okay so we tend to use partial a lot. So that's certainly something worth spending time practicing. Now as we've discussed, Python doesn't care about types in particular. And there's nothing about any of this that requires CB to be a function. It just has to be a callable. A callable is something that you can call. And so as we've discussed, another way of creating a callable is defining dunder call. So here's a class. And this is going to work exactly the same as our make show progress thing, but now as a class. So there's a dunder init, which stores the exclamation, and a dunder call that prints. And so now we're creating a object which is callable, and does exactly the same thing. Okay so these are all like fundamental ideas that I want you to get really comfortable with. The idea of dunder call, dunder things in general, partials, classes, because they come up all the time in PyTorch code, and in the code we'll be writing, and in fact pretty much all frameworks. So it's really important to feel comfortable with them. And remember you don't have to rely on the resources we're providing. You know, if there are certain things here that are very new to you, you know, Google around for some tutorials, or ask for help in the forums, finding things, and so forth. And then I'm just going to briefly recover something I've mentioned before, which is star args and star star quags, because again they come up a lot. I just wanted to show you how they work. So if we create a function that has star args and star star quags, nothing else, and I'm just going to have this function just print them. Now I'm going to call the function, I'm going to pass 3, I'm going to pass a, and I'm going to pass thing1 equals hello. Now these are passed what we would say by position. We haven't got a blah equals, they're just stuck there. Things that are passed by position are placed in star args, if you have one. It doesn't have to be called args, you can call this anything you like, but in the star bit. And so you can see here that args is a tuple containing the positionally passed arguments. And then quags is a dictionary containing the named arguments. So that is all that star args and star star quags do. And as I say, there's nothing special about these names. I'll call this a, I'll call this b, and it'll do exactly the same thing. So this comes up a lot, and so it's important to remember that this is literally all that they're doing. And then on the other hand, let's say we had a function which takes a couple of, okay let's try that, print a, actually we'll just print them directly, a, b, c. We can also, rather than just using them as parameters, we can also use them when calling something. So let's say I create something called args, again it doesn't have to be called args, called, which contains 1, 2. And I create something called quags, that contains a dictionary containing c, 3. I can then call g, and I can pass in star args, comma, star star quags. And that's going to take this 1, 2, and pass them as individual arguments, positionally. And it's going to take the c, 3, and pass that as a named argument, c equals 3. And there it is. Okay, so they're kind of two linked but different ways that use star and star star. Okay, now here's a slightly different way of doing callbacks, which I really like. In this case I've now passing in a callback that's not callable, but instead it's going to have a method called beforeCalc, and another method called afterCalc. And so now my callback is going to be a class containing a beforeCalc and an afterCalc method. And so if I run that, you can see it's, there it goes. Okay, and so this is printing before and after every step, by calling beforeCalc and afterCalc. So callback actually doesn't have to be a callable, doesn't have to be a function. A callback could be something that contains methods. So we could have a version of this which actually, as you can see here, it's going to pass in to afterCalc both the epoch number and the value it's up to. But by using star args and star star quarks, I can just safely ignore them if I don't want them. Right, so it's just going to chew them up and not complain. If I didn't have those here, it won't work. See because it got passed in val equals, and there's nothing here looking for val equals. And it doesn't like that. So this is one good use of star args and star star quarks, is to eat up arguments you don't want. Or we could use the argument. So let's actually use epoch and val and print them out. And there it is. So this is a more sophisticated callback that's giving us status as we go. I'm going to skip this bit because we don't really care about that. So finally, let's just review this idea of dunder, which we've mentioned before. But just to really nail this home, anything that looks like this, underscore underscore something underscore underscore something, is special. And basically it could be that Python has defined that special thing, or PyTorch has defined that special thing, or NumPy has defined that special thing, but they're special. These are called dunder methods. And some of them are defined as part of the Python data model. And so if you go to the Python documentation, it'll tell you about these various different ... here's Repra, which we used earlier. Here's init, that we used earlier. So they're all here. PyTorch has some of its own. NumPy has some of its own. So for example, if Python sees plus, what it actually does is it calls dunder add. So if we want to create something that's not very good at adding things, it actually already ... also always adds 0.01 to it. Then I can say sloppy adder 1 plus sloppy adder 2 equals 3.01. So plus here is actually calling dunder add. So if you're not familiar with these, click on this data model link and read about these specific 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11 methods. Because we'll be using all of these in the course. So I'll try to revise them when we can, but I'm generally going to assume that you know these. A particularly interesting one is getAttr. We've seen setAttr already. GetAttr is just the opposite. Take a look at this. Here's a class. It just contains two attributes, a and b, that are set to 1 and 2. So I'll create an object of that class, a.b equals 2, because I set b to 2. Now when you say a.b, that's just syntax sugar basically in Python. What it's actually calling behind the scenes is getAttr. It calls getAttr on the object, and so this one here is the same as getAttr a, b. Which hopefully, actually that'll do. So it calls getAttr a, b. This can kind of be fun, because you could call getAttr a, and then either b or a randomly. How's that for crazy? So if I run this, 2, 1, 1, 1, 2, as you can see it's random. So yeah, Python's such a dynamic language, you can even set it up so you literally don't know what attribute is going to be called. Now getAttr behind the scenes is actually calling something called doneToGetAttr. And by default it'll use the version in the object base class. So here's something just like a, I've got a and b defined. But I've also got doneToGetAttr defined. And so doneToGetAttr, it's only called for stuff that hasn't been defined yet. And it'll pass in the key, or the name of the attribute. So generally speaking, if the first character is an underscore, it's going to be private or special. So I'm just going to raise an attribute error. Otherwise I'm going to steal it and return hello from K. So if I go b.a, that's defined. So it gives me 1. If I go b.foo, that's not defined, so it calls getAttr. And I get back hello from foo. And so this gets used a lot in both fast AI code and also hugging face code to, you know, often make it more convenient to access things. So that's, yeah, that's how the getAttr function and the doneToGetAttr method work. Okay, so I went over that pretty quickly. Since I know for quite a few folks this will be all review. But I know for folks who haven't seen any of this, this is a lot to cover. So I'm hoping that you'll kind of go back over this, revise it slowly, experiment with it, and look up some additional resources and ask on the forum and stuff for anything that's not clear. Remember, everybody has parts of the course that's really easy for them, and parts of the course that are completely unfamiliar for them. And so if this particular part of the course is completely unfamiliar to you, it's not because this is harder, or going to be more difficult, or whatever. It just so happens that this is a bit that you're less familiar with. Or maybe the stuff about calculus in the last lesson was a bit that you're less familiar with. There isn't really anything particularly in the course that's more difficult than other parts. It's just that, you know, based on whether you happen to have that background. And so, yeah, if you spend a few hours studying and practicing, you know, you'll be able to pick up these things. And yeah, so don't stress if there are things that you don't get right away. Just take the time, and if you, yeah, if you do get lost, please ask. Because people are very keen to help. If you've tried asking on the forum, hopefully you've noticed that people are really keen to help. All right, so I think this has been a pretty successful lesson. We've got to a point where we've got a pretty nicely optimized training loop. We understand exactly what data loaders and data sets do. We've got an optimizer. We've been playing with hugging face data sets, and we've got those working really smoothly. So we really feel like we're in a pretty good position to write our generic learner training loop, and then we can start building and experimenting with lots of models. So look forward to seeing you next time to doing that together. Okay, bye.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 4.28, "text": " Okay, hi everybody and welcome to lesson 14.", "tokens": [50364, 1033, 11, 4879, 2201, 293, 2928, 281, 6898, 3499, 13, 50578], "temperature": 0.0, "avg_logprob": -0.32475240768924835, "compression_ratio": 1.398876404494382, "no_speech_prob": 9.990456601371989e-05}, {"id": 1, "seek": 0, "start": 4.28, "end": 9.24, "text": " The numbers are getting up pretty high now, huh?", "tokens": [50578, 440, 3547, 366, 1242, 493, 1238, 1090, 586, 11, 7020, 30, 50826], "temperature": 0.0, "avg_logprob": -0.32475240768924835, "compression_ratio": 1.398876404494382, "no_speech_prob": 9.990456601371989e-05}, {"id": 2, "seek": 0, "start": 9.24, "end": 19.32, "text": " We had a lesson last time talking about calculus and how we implement the chain rule in neural", "tokens": [50826, 492, 632, 257, 6898, 1036, 565, 1417, 466, 33400, 293, 577, 321, 4445, 264, 5021, 4978, 294, 18161, 51330], "temperature": 0.0, "avg_logprob": -0.32475240768924835, "compression_ratio": 1.398876404494382, "no_speech_prob": 9.990456601371989e-05}, {"id": 3, "seek": 0, "start": 19.32, "end": 24.8, "text": " network training in an efficient way called backpropagation.", "tokens": [51330, 3209, 3097, 294, 364, 7148, 636, 1219, 646, 79, 1513, 559, 399, 13, 51604], "temperature": 0.0, "avg_logprob": -0.32475240768924835, "compression_ratio": 1.398876404494382, "no_speech_prob": 9.990456601371989e-05}, {"id": 4, "seek": 2480, "start": 24.8, "end": 34.18, "text": " I just wanted to point out that one excellent student, Koshik Sinha, has produced a very", "tokens": [50364, 286, 445, 1415, 281, 935, 484, 300, 472, 7103, 3107, 11, 591, 3019, 1035, 318, 6727, 11, 575, 7126, 257, 588, 50833], "temperature": 0.0, "avg_logprob": -0.3312171848340966, "compression_ratio": 1.5, "no_speech_prob": 0.00030530389631167054}, {"id": 5, "seek": 2480, "start": 34.18, "end": 38.96, "text": " nice explanation of the code that we looked at last time.", "tokens": [50833, 1481, 10835, 295, 264, 3089, 300, 321, 2956, 412, 1036, 565, 13, 51072], "temperature": 0.0, "avg_logprob": -0.3312171848340966, "compression_ratio": 1.5, "no_speech_prob": 0.00030530389631167054}, {"id": 6, "seek": 2480, "start": 38.96, "end": 41.96, "text": " And I've linked to it.", "tokens": [51072, 400, 286, 600, 9408, 281, 309, 13, 51222], "temperature": 0.0, "avg_logprob": -0.3312171848340966, "compression_ratio": 1.5, "no_speech_prob": 0.00030530389631167054}, {"id": 7, "seek": 2480, "start": 41.96, "end": 47.8, "text": " So it's got the math and then the code.", "tokens": [51222, 407, 309, 311, 658, 264, 5221, 293, 550, 264, 3089, 13, 51514], "temperature": 0.0, "avg_logprob": -0.3312171848340966, "compression_ratio": 1.5, "no_speech_prob": 0.00030530389631167054}, {"id": 8, "seek": 2480, "start": 47.8, "end": 50.72, "text": " The code's slightly different to what I had, but it's basically the same thing, some minor", "tokens": [51514, 440, 3089, 311, 4748, 819, 281, 437, 286, 632, 11, 457, 309, 311, 1936, 264, 912, 551, 11, 512, 6696, 51660], "temperature": 0.0, "avg_logprob": -0.3312171848340966, "compression_ratio": 1.5, "no_speech_prob": 0.00030530389631167054}, {"id": 9, "seek": 2480, "start": 50.72, "end": 52.56, "text": " changes.", "tokens": [51660, 2962, 13, 51752], "temperature": 0.0, "avg_logprob": -0.3312171848340966, "compression_ratio": 1.5, "no_speech_prob": 0.00030530389631167054}, {"id": 10, "seek": 5256, "start": 52.56, "end": 57.480000000000004, "text": " And it's helpful, it might be helpful to kind of link between the math and the code to see", "tokens": [50364, 400, 309, 311, 4961, 11, 309, 1062, 312, 4961, 281, 733, 295, 2113, 1296, 264, 5221, 293, 264, 3089, 281, 536, 50610], "temperature": 0.0, "avg_logprob": -0.24892623800980418, "compression_ratio": 1.430939226519337, "no_speech_prob": 2.3551892809337005e-05}, {"id": 11, "seek": 5256, "start": 57.480000000000004, "end": 60.24, "text": " what's going on.", "tokens": [50610, 437, 311, 516, 322, 13, 50748], "temperature": 0.0, "avg_logprob": -0.24892623800980418, "compression_ratio": 1.430939226519337, "no_speech_prob": 2.3551892809337005e-05}, {"id": 12, "seek": 5256, "start": 60.24, "end": 63.96, "text": " So you'll find that in the lesson 13 resources.", "tokens": [50748, 407, 291, 603, 915, 300, 294, 264, 6898, 3705, 3593, 13, 50934], "temperature": 0.0, "avg_logprob": -0.24892623800980418, "compression_ratio": 1.430939226519337, "no_speech_prob": 2.3551892809337005e-05}, {"id": 13, "seek": 5256, "start": 63.96, "end": 71.24000000000001, "text": " But I thought I'd just quickly, you know, try to explain it as well.", "tokens": [50934, 583, 286, 1194, 286, 1116, 445, 2661, 11, 291, 458, 11, 853, 281, 2903, 309, 382, 731, 13, 51298], "temperature": 0.0, "avg_logprob": -0.24892623800980418, "compression_ratio": 1.430939226519337, "no_speech_prob": 2.3551892809337005e-05}, {"id": 14, "seek": 5256, "start": 71.24000000000001, "end": 78.88, "text": " So maybe I could try to copy this.", "tokens": [51298, 407, 1310, 286, 727, 853, 281, 5055, 341, 13, 51680], "temperature": 0.0, "avg_logprob": -0.24892623800980418, "compression_ratio": 1.430939226519337, "no_speech_prob": 2.3551892809337005e-05}, {"id": 15, "seek": 7888, "start": 78.88, "end": 85.64, "text": " And just explain what's going on here with this code.", "tokens": [50364, 400, 445, 2903, 437, 311, 516, 322, 510, 365, 341, 3089, 13, 50702], "temperature": 0.0, "avg_logprob": -0.2631646474202474, "compression_ratio": 1.8274111675126903, "no_speech_prob": 0.00011591800284804776}, {"id": 16, "seek": 7888, "start": 85.64, "end": 92.92, "text": " So the basic idea is that we have a neural network that is calculating, well a neural", "tokens": [50702, 407, 264, 3875, 1558, 307, 300, 321, 362, 257, 18161, 3209, 300, 307, 28258, 11, 731, 257, 18161, 51066], "temperature": 0.0, "avg_logprob": -0.2631646474202474, "compression_ratio": 1.8274111675126903, "no_speech_prob": 0.00011591800284804776}, {"id": 17, "seek": 7888, "start": 92.92, "end": 96.39999999999999, "text": " network and a loss function that together that calculate a loss.", "tokens": [51066, 3209, 293, 257, 4470, 2445, 300, 1214, 300, 8873, 257, 4470, 13, 51240], "temperature": 0.0, "avg_logprob": -0.2631646474202474, "compression_ratio": 1.8274111675126903, "no_speech_prob": 0.00011591800284804776}, {"id": 18, "seek": 7888, "start": 96.39999999999999, "end": 103.96, "text": " So let's imagine that, well let's just call the loss function, we'll call it L. And the", "tokens": [51240, 407, 718, 311, 3811, 300, 11, 731, 718, 311, 445, 818, 264, 4470, 2445, 11, 321, 603, 818, 309, 441, 13, 400, 264, 51618], "temperature": 0.0, "avg_logprob": -0.2631646474202474, "compression_ratio": 1.8274111675126903, "no_speech_prob": 0.00011591800284804776}, {"id": 19, "seek": 7888, "start": 103.96, "end": 107.96, "text": " loss function is being applied to the output of the neural network.", "tokens": [51618, 4470, 2445, 307, 885, 6456, 281, 264, 5598, 295, 264, 18161, 3209, 13, 51818], "temperature": 0.0, "avg_logprob": -0.2631646474202474, "compression_ratio": 1.8274111675126903, "no_speech_prob": 0.00011591800284804776}, {"id": 20, "seek": 10796, "start": 108.03999999999999, "end": 113.91999999999999, "text": " So the neural network function we'll call N. And that takes two things, a bunch of weights", "tokens": [50368, 407, 264, 18161, 3209, 2445, 321, 603, 818, 426, 13, 400, 300, 2516, 732, 721, 11, 257, 3840, 295, 17443, 50662], "temperature": 0.0, "avg_logprob": -0.24713607061476933, "compression_ratio": 1.6477732793522266, "no_speech_prob": 9.461231820750982e-05}, {"id": 21, "seek": 10796, "start": 113.91999999999999, "end": 116.63999999999999, "text": " and a bunch of inputs.", "tokens": [50662, 293, 257, 3840, 295, 15743, 13, 50798], "temperature": 0.0, "avg_logprob": -0.24713607061476933, "compression_ratio": 1.6477732793522266, "no_speech_prob": 9.461231820750982e-05}, {"id": 22, "seek": 10796, "start": 116.63999999999999, "end": 122.24, "text": " The loss function also requires the targets, but I'm just going to ignore that for now", "tokens": [50798, 440, 4470, 2445, 611, 7029, 264, 12911, 11, 457, 286, 478, 445, 516, 281, 11200, 300, 337, 586, 51078], "temperature": 0.0, "avg_logprob": -0.24713607061476933, "compression_ratio": 1.6477732793522266, "no_speech_prob": 9.461231820750982e-05}, {"id": 23, "seek": 10796, "start": 122.24, "end": 127.19999999999999, "text": " because it's not really part of what we actually care about.", "tokens": [51078, 570, 309, 311, 406, 534, 644, 295, 437, 321, 767, 1127, 466, 13, 51326], "temperature": 0.0, "avg_logprob": -0.24713607061476933, "compression_ratio": 1.6477732793522266, "no_speech_prob": 9.461231820750982e-05}, {"id": 24, "seek": 10796, "start": 127.19999999999999, "end": 131.35999999999999, "text": " And what we're interested in knowing is if we want to be able to update the weights,", "tokens": [51326, 400, 437, 321, 434, 3102, 294, 5276, 307, 498, 321, 528, 281, 312, 1075, 281, 5623, 264, 17443, 11, 51534], "temperature": 0.0, "avg_logprob": -0.24713607061476933, "compression_ratio": 1.6477732793522266, "no_speech_prob": 9.461231820750982e-05}, {"id": 25, "seek": 10796, "start": 131.35999999999999, "end": 134.51999999999998, "text": " let's say this is just a single layer thing, keep it simple.", "tokens": [51534, 718, 311, 584, 341, 307, 445, 257, 2167, 4583, 551, 11, 1066, 309, 2199, 13, 51692], "temperature": 0.0, "avg_logprob": -0.24713607061476933, "compression_ratio": 1.6477732793522266, "no_speech_prob": 9.461231820750982e-05}, {"id": 26, "seek": 13452, "start": 134.52, "end": 145.48000000000002, "text": " If we want to be able to update the weights, we need to know how does the loss change if", "tokens": [50364, 759, 321, 528, 281, 312, 1075, 281, 5623, 264, 17443, 11, 321, 643, 281, 458, 577, 775, 264, 4470, 1319, 498, 50912], "temperature": 0.0, "avg_logprob": -0.2293625868760146, "compression_ratio": 1.6352941176470588, "no_speech_prob": 0.0001795254647731781}, {"id": 27, "seek": 13452, "start": 145.48000000000002, "end": 147.88, "text": " we change the weights.", "tokens": [50912, 321, 1319, 264, 17443, 13, 51032], "temperature": 0.0, "avg_logprob": -0.2293625868760146, "compression_ratio": 1.6352941176470588, "no_speech_prob": 0.0001795254647731781}, {"id": 28, "seek": 13452, "start": 147.88, "end": 150.96, "text": " If we change one weight at a time, if you like.", "tokens": [51032, 759, 321, 1319, 472, 3364, 412, 257, 565, 11, 498, 291, 411, 13, 51186], "temperature": 0.0, "avg_logprob": -0.2293625868760146, "compression_ratio": 1.6352941176470588, "no_speech_prob": 0.0001795254647731781}, {"id": 29, "seek": 13452, "start": 150.96, "end": 153.44, "text": " So how would we calculate that?", "tokens": [51186, 407, 577, 576, 321, 8873, 300, 30, 51310], "temperature": 0.0, "avg_logprob": -0.2293625868760146, "compression_ratio": 1.6352941176470588, "no_speech_prob": 0.0001795254647731781}, {"id": 30, "seek": 13452, "start": 153.44, "end": 161.12, "text": " Well what we could do is we could rewrite our loss function by saying, well let's call", "tokens": [51310, 1042, 437, 321, 727, 360, 307, 321, 727, 28132, 527, 4470, 2445, 538, 1566, 11, 731, 718, 311, 818, 51694], "temperature": 0.0, "avg_logprob": -0.2293625868760146, "compression_ratio": 1.6352941176470588, "no_speech_prob": 0.0001795254647731781}, {"id": 31, "seek": 16112, "start": 161.20000000000002, "end": 169.68, "text": " capital N the result of the neural network applied to the weights and the inputs.", "tokens": [50368, 4238, 426, 264, 1874, 295, 264, 18161, 3209, 6456, 281, 264, 17443, 293, 264, 15743, 13, 50792], "temperature": 0.0, "avg_logprob": -0.2670946915944417, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.00011591924703679979}, {"id": 32, "seek": 16112, "start": 169.68, "end": 177.36, "text": " And that way we can now rewrite the loss function to say L equals, big L equals, little l, the", "tokens": [50792, 400, 300, 636, 321, 393, 586, 28132, 264, 4470, 2445, 281, 584, 441, 6915, 11, 955, 441, 6915, 11, 707, 287, 11, 264, 51176], "temperature": 0.0, "avg_logprob": -0.2670946915944417, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.00011591924703679979}, {"id": 33, "seek": 16112, "start": 177.36, "end": 184.44, "text": " loss function, applied to the output of the neural network.", "tokens": [51176, 4470, 2445, 11, 6456, 281, 264, 5598, 295, 264, 18161, 3209, 13, 51530], "temperature": 0.0, "avg_logprob": -0.2670946915944417, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.00011591924703679979}, {"id": 34, "seek": 16112, "start": 184.44, "end": 186.48000000000002, "text": " And so maybe you can see where this is going.", "tokens": [51530, 400, 370, 1310, 291, 393, 536, 689, 341, 307, 516, 13, 51632], "temperature": 0.0, "avg_logprob": -0.2670946915944417, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.00011591924703679979}, {"id": 35, "seek": 18648, "start": 186.48, "end": 191.92, "text": " We can now say, okay the derivative of the loss with respect to the weights is going", "tokens": [50364, 492, 393, 586, 584, 11, 1392, 264, 13760, 295, 264, 4470, 365, 3104, 281, 264, 17443, 307, 516, 50636], "temperature": 0.0, "avg_logprob": -0.317535400390625, "compression_ratio": 2.037037037037037, "no_speech_prob": 0.000969728222116828}, {"id": 36, "seek": 18648, "start": 191.92, "end": 200.28, "text": " to be equal to the derivative of the loss with respect to the outputs of that neural", "tokens": [50636, 281, 312, 2681, 281, 264, 13760, 295, 264, 4470, 365, 3104, 281, 264, 23930, 295, 300, 18161, 51054], "temperature": 0.0, "avg_logprob": -0.317535400390625, "compression_ratio": 2.037037037037037, "no_speech_prob": 0.000969728222116828}, {"id": 37, "seek": 18648, "start": 200.28, "end": 209.6, "text": " network layer, times, this is the chain rule, the derivative of the outputs of that neural", "tokens": [51054, 3209, 4583, 11, 1413, 11, 341, 307, 264, 5021, 4978, 11, 264, 13760, 295, 264, 23930, 295, 300, 18161, 51520], "temperature": 0.0, "avg_logprob": -0.317535400390625, "compression_ratio": 2.037037037037037, "no_speech_prob": 0.000969728222116828}, {"id": 38, "seek": 18648, "start": 209.6, "end": 211.16, "text": " network layer.", "tokens": [51520, 3209, 4583, 13, 51598], "temperature": 0.0, "avg_logprob": -0.317535400390625, "compression_ratio": 2.037037037037037, "no_speech_prob": 0.000969728222116828}, {"id": 39, "seek": 21116, "start": 212.16, "end": 223.44, "text": " I'm going to get my notation consistent since these are not scalar.", "tokens": [50414, 286, 478, 516, 281, 483, 452, 24657, 8398, 1670, 613, 366, 406, 39684, 13, 50978], "temperature": 0.0, "avg_logprob": -0.35291198945381275, "compression_ratio": 1.5875, "no_speech_prob": 0.0009399367263540626}, {"id": 40, "seek": 21116, "start": 223.44, "end": 226.68, "text": " With respect to the weights.", "tokens": [50978, 2022, 3104, 281, 264, 17443, 13, 51140], "temperature": 0.0, "avg_logprob": -0.35291198945381275, "compression_ratio": 1.5875, "no_speech_prob": 0.0009399367263540626}, {"id": 41, "seek": 21116, "start": 226.68, "end": 233.56, "text": " Right, so you can see we can get rid of those and we end up with the change in loss with", "tokens": [51140, 1779, 11, 370, 291, 393, 536, 321, 393, 483, 3973, 295, 729, 293, 321, 917, 493, 365, 264, 1319, 294, 4470, 365, 51484], "temperature": 0.0, "avg_logprob": -0.35291198945381275, "compression_ratio": 1.5875, "no_speech_prob": 0.0009399367263540626}, {"id": 42, "seek": 21116, "start": 233.56, "end": 235.76, "text": " respect to the weights.", "tokens": [51484, 3104, 281, 264, 17443, 13, 51594], "temperature": 0.0, "avg_logprob": -0.35291198945381275, "compression_ratio": 1.5875, "no_speech_prob": 0.0009399367263540626}, {"id": 43, "seek": 21116, "start": 235.76, "end": 239.12, "text": " And so we can just say this is a chain rule.", "tokens": [51594, 400, 370, 321, 393, 445, 584, 341, 307, 257, 5021, 4978, 13, 51762], "temperature": 0.0, "avg_logprob": -0.35291198945381275, "compression_ratio": 1.5875, "no_speech_prob": 0.0009399367263540626}, {"id": 44, "seek": 23912, "start": 239.12, "end": 242.24, "text": " That's what the chain rule is.", "tokens": [50364, 663, 311, 437, 264, 5021, 4978, 307, 13, 50520], "temperature": 0.0, "avg_logprob": -0.2557211875915527, "compression_ratio": 1.5578231292517006, "no_speech_prob": 0.00038596146623604}, {"id": 45, "seek": 23912, "start": 242.24, "end": 250.92000000000002, "text": " So the change in the loss with respect to the output of the neural network.", "tokens": [50520, 407, 264, 1319, 294, 264, 4470, 365, 3104, 281, 264, 5598, 295, 264, 18161, 3209, 13, 50954], "temperature": 0.0, "avg_logprob": -0.2557211875915527, "compression_ratio": 1.5578231292517006, "no_speech_prob": 0.00038596146623604}, {"id": 46, "seek": 23912, "start": 250.92000000000002, "end": 254.08, "text": " Well we did the forward pass here.", "tokens": [50954, 1042, 321, 630, 264, 2128, 1320, 510, 13, 51112], "temperature": 0.0, "avg_logprob": -0.2557211875915527, "compression_ratio": 1.5578231292517006, "no_speech_prob": 0.00038596146623604}, {"id": 47, "seek": 23912, "start": 254.08, "end": 261.84000000000003, "text": " And then we took here, this here is where we calculated the derivative of the loss with", "tokens": [51112, 400, 550, 321, 1890, 510, 11, 341, 510, 307, 689, 321, 15598, 264, 13760, 295, 264, 4470, 365, 51500], "temperature": 0.0, "avg_logprob": -0.2557211875915527, "compression_ratio": 1.5578231292517006, "no_speech_prob": 0.00038596146623604}, {"id": 48, "seek": 26184, "start": 261.84, "end": 266.91999999999996, "text": " respect to the output of the neural network.", "tokens": [50364, 3104, 281, 264, 5598, 295, 264, 18161, 3209, 13, 50618], "temperature": 0.0, "avg_logprob": -0.27121953796922116, "compression_ratio": 1.4014598540145986, "no_speech_prob": 0.0084451325237751}, {"id": 49, "seek": 26184, "start": 266.91999999999996, "end": 271.52, "text": " Which came out from here and ended up in diff.", "tokens": [50618, 3013, 1361, 484, 490, 510, 293, 4590, 493, 294, 7593, 13, 50848], "temperature": 0.0, "avg_logprob": -0.27121953796922116, "compression_ratio": 1.4014598540145986, "no_speech_prob": 0.0084451325237751}, {"id": 50, "seek": 26184, "start": 271.52, "end": 273.64, "text": " So there it is.", "tokens": [50848, 407, 456, 309, 307, 13, 50954], "temperature": 0.0, "avg_logprob": -0.27121953796922116, "compression_ratio": 1.4014598540145986, "no_speech_prob": 0.0084451325237751}, {"id": 51, "seek": 26184, "start": 273.64, "end": 281.2, "text": " So out.g contains this derivative.", "tokens": [50954, 407, 484, 13, 70, 8306, 341, 13760, 13, 51332], "temperature": 0.0, "avg_logprob": -0.27121953796922116, "compression_ratio": 1.4014598540145986, "no_speech_prob": 0.0084451325237751}, {"id": 52, "seek": 26184, "start": 281.2, "end": 286.08, "text": " So then to calculate, let's actually do one more.", "tokens": [51332, 407, 550, 281, 8873, 11, 718, 311, 767, 360, 472, 544, 13, 51576], "temperature": 0.0, "avg_logprob": -0.27121953796922116, "compression_ratio": 1.4014598540145986, "no_speech_prob": 0.0084451325237751}, {"id": 53, "seek": 28608, "start": 286.08, "end": 294.2, "text": " We could also say the change in the loss with respect to the inputs.", "tokens": [50364, 492, 727, 611, 584, 264, 1319, 294, 264, 4470, 365, 3104, 281, 264, 15743, 13, 50770], "temperature": 0.0, "avg_logprob": -0.3286768527741128, "compression_ratio": 1.4107142857142858, "no_speech_prob": 0.0009694603504613042}, {"id": 54, "seek": 28608, "start": 294.2, "end": 300.91999999999996, "text": " We can do the same thing with the chain rule.", "tokens": [50770, 492, 393, 360, 264, 912, 551, 365, 264, 5021, 4978, 13, 51106], "temperature": 0.0, "avg_logprob": -0.3286768527741128, "compression_ratio": 1.4107142857142858, "no_speech_prob": 0.0009694603504613042}, {"id": 55, "seek": 28608, "start": 300.91999999999996, "end": 305.12, "text": " Times.", "tokens": [51106, 11366, 13, 51316], "temperature": 0.0, "avg_logprob": -0.3286768527741128, "compression_ratio": 1.4107142857142858, "no_speech_prob": 0.0009694603504613042}, {"id": 56, "seek": 28608, "start": 305.12, "end": 308.86, "text": " And so this time we have the inputs.", "tokens": [51316, 400, 370, 341, 565, 321, 362, 264, 15743, 13, 51503], "temperature": 0.0, "avg_logprob": -0.3286768527741128, "compression_ratio": 1.4107142857142858, "no_speech_prob": 0.0009694603504613042}, {"id": 57, "seek": 30886, "start": 308.86, "end": 319.3, "text": " So here you can see that is this line of code.", "tokens": [50364, 407, 510, 291, 393, 536, 300, 307, 341, 1622, 295, 3089, 13, 50886], "temperature": 0.0, "avg_logprob": -0.21859591347830637, "compression_ratio": 1.3636363636363635, "no_speech_prob": 0.002934782300144434}, {"id": 58, "seek": 30886, "start": 319.3, "end": 329.42, "text": " So that is the change in the loss with respect to the inputs.", "tokens": [50886, 407, 300, 307, 264, 1319, 294, 264, 4470, 365, 3104, 281, 264, 15743, 13, 51392], "temperature": 0.0, "avg_logprob": -0.21859591347830637, "compression_ratio": 1.3636363636363635, "no_speech_prob": 0.002934782300144434}, {"id": 59, "seek": 30886, "start": 329.42, "end": 331.14, "text": " That's what input.g means.", "tokens": [51392, 663, 311, 437, 4846, 13, 70, 1355, 13, 51478], "temperature": 0.0, "avg_logprob": -0.21859591347830637, "compression_ratio": 1.3636363636363635, "no_speech_prob": 0.002934782300144434}, {"id": 60, "seek": 33114, "start": 331.14, "end": 340.41999999999996, "text": " And it's equal to the change in the loss with respect to the output.", "tokens": [50364, 400, 309, 311, 2681, 281, 264, 1319, 294, 264, 4470, 365, 3104, 281, 264, 5598, 13, 50828], "temperature": 0.0, "avg_logprob": -0.2650780995686849, "compression_ratio": 1.5837837837837838, "no_speech_prob": 0.008315439336001873}, {"id": 61, "seek": 33114, "start": 340.41999999999996, "end": 344.26, "text": " So that's what out.g means.", "tokens": [50828, 407, 300, 311, 437, 484, 13, 70, 1355, 13, 51020], "temperature": 0.0, "avg_logprob": -0.2650780995686849, "compression_ratio": 1.5837837837837838, "no_speech_prob": 0.008315439336001873}, {"id": 62, "seek": 33114, "start": 344.26, "end": 349.09999999999997, "text": " Times it's actually matrix times, because we're doing matrix calculus.", "tokens": [51020, 11366, 309, 311, 767, 8141, 1413, 11, 570, 321, 434, 884, 8141, 33400, 13, 51262], "temperature": 0.0, "avg_logprob": -0.2650780995686849, "compression_ratio": 1.5837837837837838, "no_speech_prob": 0.008315439336001873}, {"id": 63, "seek": 33114, "start": 349.09999999999997, "end": 353.26, "text": " Times this derivative, and since this is a linear layer we were looking at, this derivative", "tokens": [51262, 11366, 341, 13760, 11, 293, 1670, 341, 307, 257, 8213, 4583, 321, 645, 1237, 412, 11, 341, 13760, 51470], "temperature": 0.0, "avg_logprob": -0.2650780995686849, "compression_ratio": 1.5837837837837838, "no_speech_prob": 0.008315439336001873}, {"id": 64, "seek": 33114, "start": 353.26, "end": 356.74, "text": " is simply the weights themselves.", "tokens": [51470, 307, 2935, 264, 17443, 2969, 13, 51644], "temperature": 0.0, "avg_logprob": -0.2650780995686849, "compression_ratio": 1.5837837837837838, "no_speech_prob": 0.008315439336001873}, {"id": 65, "seek": 35674, "start": 356.74, "end": 365.94, "text": " And then we have exactly the same thing for w.g, which is the change in the loss, the", "tokens": [50364, 400, 550, 321, 362, 2293, 264, 912, 551, 337, 261, 13, 70, 11, 597, 307, 264, 1319, 294, 264, 4470, 11, 264, 50824], "temperature": 0.0, "avg_logprob": -0.24287723359607516, "compression_ratio": 1.586734693877551, "no_speech_prob": 0.001388471806421876}, {"id": 66, "seek": 35674, "start": 365.94, "end": 370.90000000000003, "text": " derivative of the loss, with respect to the weights.", "tokens": [50824, 13760, 295, 264, 4470, 11, 365, 3104, 281, 264, 17443, 13, 51072], "temperature": 0.0, "avg_logprob": -0.24287723359607516, "compression_ratio": 1.586734693877551, "no_speech_prob": 0.001388471806421876}, {"id": 67, "seek": 35674, "start": 370.90000000000003, "end": 372.14, "text": " And so again you've got the same thing.", "tokens": [51072, 400, 370, 797, 291, 600, 658, 264, 912, 551, 13, 51134], "temperature": 0.0, "avg_logprob": -0.24287723359607516, "compression_ratio": 1.586734693877551, "no_speech_prob": 0.001388471806421876}, {"id": 68, "seek": 35674, "start": 372.14, "end": 376.26, "text": " You've got your out.g, and remember we actually showed how we can simplify this into also", "tokens": [51134, 509, 600, 658, 428, 484, 13, 70, 11, 293, 1604, 321, 767, 4712, 577, 321, 393, 20460, 341, 666, 611, 51340], "temperature": 0.0, "avg_logprob": -0.24287723359607516, "compression_ratio": 1.586734693877551, "no_speech_prob": 0.001388471806421876}, {"id": 69, "seek": 35674, "start": 376.26, "end": 379.46000000000004, "text": " a matrix product with a transpose as well.", "tokens": [51340, 257, 8141, 1674, 365, 257, 25167, 382, 731, 13, 51500], "temperature": 0.0, "avg_logprob": -0.24287723359607516, "compression_ratio": 1.586734693877551, "no_speech_prob": 0.001388471806421876}, {"id": 70, "seek": 37946, "start": 379.46, "end": 388.53999999999996, "text": " So that's how what's happening in our code is mapping to the math.", "tokens": [50364, 407, 300, 311, 577, 437, 311, 2737, 294, 527, 3089, 307, 18350, 281, 264, 5221, 13, 50818], "temperature": 0.0, "avg_logprob": -0.2717946012255172, "compression_ratio": 1.4795321637426901, "no_speech_prob": 0.005383068695664406}, {"id": 71, "seek": 37946, "start": 388.53999999999996, "end": 390.09999999999997, "text": " So hopefully that's useful.", "tokens": [50818, 407, 4696, 300, 311, 4420, 13, 50896], "temperature": 0.0, "avg_logprob": -0.2717946012255172, "compression_ratio": 1.4795321637426901, "no_speech_prob": 0.005383068695664406}, {"id": 72, "seek": 37946, "start": 390.09999999999997, "end": 396.09999999999997, "text": " But as I say, do check out this really nice resource which has a lot more detail if you're", "tokens": [50896, 583, 382, 286, 584, 11, 360, 1520, 484, 341, 534, 1481, 7684, 597, 575, 257, 688, 544, 2607, 498, 291, 434, 51196], "temperature": 0.0, "avg_logprob": -0.2717946012255172, "compression_ratio": 1.4795321637426901, "no_speech_prob": 0.005383068695664406}, {"id": 73, "seek": 37946, "start": 396.09999999999997, "end": 398.62, "text": " interested in digging deeper.", "tokens": [51196, 3102, 294, 17343, 7731, 13, 51322], "temperature": 0.0, "avg_logprob": -0.2717946012255172, "compression_ratio": 1.4795321637426901, "no_speech_prob": 0.005383068695664406}, {"id": 74, "seek": 37946, "start": 398.62, "end": 403.02, "text": " The other thing I'd say is, if you...", "tokens": [51322, 440, 661, 551, 286, 1116, 584, 307, 11, 498, 291, 485, 51542], "temperature": 0.0, "avg_logprob": -0.2717946012255172, "compression_ratio": 1.4795321637426901, "no_speech_prob": 0.005383068695664406}, {"id": 75, "seek": 40302, "start": 403.02, "end": 408.58, "text": " Some people have mentioned that they actually didn't study this at high school, which is", "tokens": [50364, 2188, 561, 362, 2835, 300, 436, 767, 994, 380, 2979, 341, 412, 1090, 1395, 11, 597, 307, 50642], "temperature": 0.0, "avg_logprob": -0.2828508441367846, "compression_ratio": 1.5063829787234042, "no_speech_prob": 0.04467201605439186}, {"id": 76, "seek": 40302, "start": 408.58, "end": 410.26, "text": " fine.", "tokens": [50642, 2489, 13, 50726], "temperature": 0.0, "avg_logprob": -0.2828508441367846, "compression_ratio": 1.5063829787234042, "no_speech_prob": 0.04467201605439186}, {"id": 77, "seek": 40302, "start": 410.26, "end": 418.46, "text": " We've provided resources on the forum for recommending how to learn the basics of derivatives", "tokens": [50726, 492, 600, 5649, 3593, 322, 264, 17542, 337, 30559, 577, 281, 1466, 264, 14688, 295, 33733, 51136], "temperature": 0.0, "avg_logprob": -0.2828508441367846, "compression_ratio": 1.5063829787234042, "no_speech_prob": 0.04467201605439186}, {"id": 78, "seek": 40302, "start": 418.46, "end": 420.82, "text": " and the chain rule.", "tokens": [51136, 293, 264, 5021, 4978, 13, 51254], "temperature": 0.0, "avg_logprob": -0.2828508441367846, "compression_ratio": 1.5063829787234042, "no_speech_prob": 0.04467201605439186}, {"id": 79, "seek": 40302, "start": 420.82, "end": 426.78, "text": " And so in particular I would recommend 3Blue1Brown's Essence of Calculus series, and also Khan", "tokens": [51254, 400, 370, 294, 1729, 286, 576, 2748, 805, 45231, 16, 22170, 648, 311, 14357, 655, 295, 3511, 36002, 2638, 11, 293, 611, 18136, 51552], "temperature": 0.0, "avg_logprob": -0.2828508441367846, "compression_ratio": 1.5063829787234042, "no_speech_prob": 0.04467201605439186}, {"id": 80, "seek": 40302, "start": 426.78, "end": 428.7, "text": " Academy.", "tokens": [51552, 11735, 13, 51648], "temperature": 0.0, "avg_logprob": -0.2828508441367846, "compression_ratio": 1.5063829787234042, "no_speech_prob": 0.04467201605439186}, {"id": 81, "seek": 40302, "start": 428.7, "end": 430.62, "text": " It's not particularly difficult to learn.", "tokens": [51648, 467, 311, 406, 4098, 2252, 281, 1466, 13, 51744], "temperature": 0.0, "avg_logprob": -0.2828508441367846, "compression_ratio": 1.5063829787234042, "no_speech_prob": 0.04467201605439186}, {"id": 82, "seek": 43062, "start": 430.62, "end": 433.9, "text": " It'll only take you a few hours, and then you can...", "tokens": [50364, 467, 603, 787, 747, 291, 257, 1326, 2496, 11, 293, 550, 291, 393, 485, 50528], "temperature": 0.0, "avg_logprob": -0.31830532034647835, "compression_ratio": 1.5603448275862069, "no_speech_prob": 0.0373242162168026}, {"id": 83, "seek": 43062, "start": 433.9, "end": 436.26, "text": " This will make a lot more sense.", "tokens": [50528, 639, 486, 652, 257, 688, 544, 2020, 13, 50646], "temperature": 0.0, "avg_logprob": -0.31830532034647835, "compression_ratio": 1.5603448275862069, "no_speech_prob": 0.0373242162168026}, {"id": 84, "seek": 43062, "start": 436.26, "end": 440.94, "text": " Or if you did it at high school but you've forgotten it, same deal.", "tokens": [50646, 1610, 498, 291, 630, 309, 412, 1090, 1395, 457, 291, 600, 11832, 309, 11, 912, 2028, 13, 50880], "temperature": 0.0, "avg_logprob": -0.31830532034647835, "compression_ratio": 1.5603448275862069, "no_speech_prob": 0.0373242162168026}, {"id": 85, "seek": 43062, "start": 440.94, "end": 447.7, "text": " So don't worry if you found this difficult because you had forgotten, or had never learnt,", "tokens": [50880, 407, 500, 380, 3292, 498, 291, 1352, 341, 2252, 570, 291, 632, 11832, 11, 420, 632, 1128, 18991, 11, 51218], "temperature": 0.0, "avg_logprob": -0.31830532034647835, "compression_ratio": 1.5603448275862069, "no_speech_prob": 0.0373242162168026}, {"id": 86, "seek": 43062, "start": 447.7, "end": 451.22, "text": " the basic derivative and chain rule stuff.", "tokens": [51218, 264, 3875, 13760, 293, 5021, 4978, 1507, 13, 51394], "temperature": 0.0, "avg_logprob": -0.31830532034647835, "compression_ratio": 1.5603448275862069, "no_speech_prob": 0.0373242162168026}, {"id": 87, "seek": 43062, "start": 451.22, "end": 457.06, "text": " That's something that you can pick up now, and I would recommend doing so.", "tokens": [51394, 663, 311, 746, 300, 291, 393, 1888, 493, 586, 11, 293, 286, 576, 2748, 884, 370, 13, 51686], "temperature": 0.0, "avg_logprob": -0.31830532034647835, "compression_ratio": 1.5603448275862069, "no_speech_prob": 0.0373242162168026}, {"id": 88, "seek": 45706, "start": 457.06, "end": 465.34, "text": " Okay, so what we then did last time, which is actually pretty exciting, is we got to", "tokens": [50364, 1033, 11, 370, 437, 321, 550, 630, 1036, 565, 11, 597, 307, 767, 1238, 4670, 11, 307, 321, 658, 281, 50778], "temperature": 0.0, "avg_logprob": -0.3168002234564887, "compression_ratio": 1.6634146341463414, "no_speech_prob": 0.0006986654479987919}, {"id": 89, "seek": 45706, "start": 465.34, "end": 474.74, "text": " a point where we had successfully created a training loop, which did these four steps.", "tokens": [50778, 257, 935, 689, 321, 632, 10727, 2942, 257, 3097, 6367, 11, 597, 630, 613, 1451, 4439, 13, 51248], "temperature": 0.0, "avg_logprob": -0.3168002234564887, "compression_ratio": 1.6634146341463414, "no_speech_prob": 0.0006986654479987919}, {"id": 90, "seek": 45706, "start": 474.74, "end": 481.06, "text": " So and the nice thing is that every single thing here is something that we have implemented", "tokens": [51248, 407, 293, 264, 1481, 551, 307, 300, 633, 2167, 551, 510, 307, 746, 300, 321, 362, 12270, 51564], "temperature": 0.0, "avg_logprob": -0.3168002234564887, "compression_ratio": 1.6634146341463414, "no_speech_prob": 0.0006986654479987919}, {"id": 91, "seek": 45706, "start": 481.06, "end": 482.06, "text": " from scratch.", "tokens": [51564, 490, 8459, 13, 51614], "temperature": 0.0, "avg_logprob": -0.3168002234564887, "compression_ratio": 1.6634146341463414, "no_speech_prob": 0.0006986654479987919}, {"id": 92, "seek": 45706, "start": 482.06, "end": 485.3, "text": " Now we didn't always use our implemented from scratch versions.", "tokens": [51614, 823, 321, 994, 380, 1009, 764, 527, 12270, 490, 8459, 9606, 13, 51776], "temperature": 0.0, "avg_logprob": -0.3168002234564887, "compression_ratio": 1.6634146341463414, "no_speech_prob": 0.0006986654479987919}, {"id": 93, "seek": 48530, "start": 485.3, "end": 486.54, "text": " There's no particular reason to.", "tokens": [50364, 821, 311, 572, 1729, 1778, 281, 13, 50426], "temperature": 0.0, "avg_logprob": -0.2662700481629104, "compression_ratio": 1.5341880341880343, "no_speech_prob": 0.04401735961437225}, {"id": 94, "seek": 48530, "start": 486.54, "end": 490.3, "text": " When we've re-implemented something that already exists, let's use the version that exists.", "tokens": [50426, 1133, 321, 600, 319, 12, 332, 781, 14684, 746, 300, 1217, 8198, 11, 718, 311, 764, 264, 3037, 300, 8198, 13, 50614], "temperature": 0.0, "avg_logprob": -0.2662700481629104, "compression_ratio": 1.5341880341880343, "no_speech_prob": 0.04401735961437225}, {"id": 95, "seek": 48530, "start": 490.3, "end": 496.74, "text": " But every single thing here, I guess not argmax, but that's trivially easy to implement.", "tokens": [50614, 583, 633, 2167, 551, 510, 11, 286, 2041, 406, 3882, 41167, 11, 457, 300, 311, 1376, 85, 2270, 1858, 281, 4445, 13, 50936], "temperature": 0.0, "avg_logprob": -0.2662700481629104, "compression_ratio": 1.5341880341880343, "no_speech_prob": 0.04401735961437225}, {"id": 96, "seek": 48530, "start": 496.74, "end": 502.94, "text": " Every single thing here we have implemented ourselves.", "tokens": [50936, 2048, 2167, 551, 510, 321, 362, 12270, 4175, 13, 51246], "temperature": 0.0, "avg_logprob": -0.2662700481629104, "compression_ratio": 1.5341880341880343, "no_speech_prob": 0.04401735961437225}, {"id": 97, "seek": 48530, "start": 502.94, "end": 513.94, "text": " And we successfully trained an MNIST model to 96% accurately recognize handwritten digits.", "tokens": [51246, 400, 321, 10727, 8895, 364, 376, 45, 19756, 2316, 281, 24124, 4, 20095, 5521, 1011, 26859, 27011, 13, 51796], "temperature": 0.0, "avg_logprob": -0.2662700481629104, "compression_ratio": 1.5341880341880343, "no_speech_prob": 0.04401735961437225}, {"id": 98, "seek": 51394, "start": 513.94, "end": 519.0200000000001, "text": " So I think that's super neat.", "tokens": [50364, 407, 286, 519, 300, 311, 1687, 10654, 13, 50618], "temperature": 0.0, "avg_logprob": -0.23014391530858408, "compression_ratio": 1.6790697674418604, "no_speech_prob": 4.832550621358678e-05}, {"id": 99, "seek": 51394, "start": 519.0200000000001, "end": 522.58, "text": " This is not a great metric.", "tokens": [50618, 639, 307, 406, 257, 869, 20678, 13, 50796], "temperature": 0.0, "avg_logprob": -0.23014391530858408, "compression_ratio": 1.6790697674418604, "no_speech_prob": 4.832550621358678e-05}, {"id": 100, "seek": 51394, "start": 522.58, "end": 525.58, "text": " It's only looking at the training set, and in particular it's only looking at one batch", "tokens": [50796, 467, 311, 787, 1237, 412, 264, 3097, 992, 11, 293, 294, 1729, 309, 311, 787, 1237, 412, 472, 15245, 50946], "temperature": 0.0, "avg_logprob": -0.23014391530858408, "compression_ratio": 1.6790697674418604, "no_speech_prob": 4.832550621358678e-05}, {"id": 101, "seek": 51394, "start": 525.58, "end": 527.46, "text": " of the training set.", "tokens": [50946, 295, 264, 3097, 992, 13, 51040], "temperature": 0.0, "avg_logprob": -0.23014391530858408, "compression_ratio": 1.6790697674418604, "no_speech_prob": 4.832550621358678e-05}, {"id": 102, "seek": 51394, "start": 527.46, "end": 529.5400000000001, "text": " Since last time I've just refactored a little bit.", "tokens": [51040, 4162, 1036, 565, 286, 600, 445, 1895, 578, 2769, 257, 707, 857, 13, 51144], "temperature": 0.0, "avg_logprob": -0.23014391530858408, "compression_ratio": 1.6790697674418604, "no_speech_prob": 4.832550621358678e-05}, {"id": 103, "seek": 51394, "start": 529.5400000000001, "end": 538.0200000000001, "text": " I've pulled out this report function, which is now just running at the end of each epoch.", "tokens": [51144, 286, 600, 7373, 484, 341, 2275, 2445, 11, 597, 307, 586, 445, 2614, 412, 264, 917, 295, 1184, 30992, 339, 13, 51568], "temperature": 0.0, "avg_logprob": -0.23014391530858408, "compression_ratio": 1.6790697674418604, "no_speech_prob": 4.832550621358678e-05}, {"id": 104, "seek": 51394, "start": 538.0200000000001, "end": 542.2800000000001, "text": " And it's just printing out the loss and the accuracy.", "tokens": [51568, 400, 309, 311, 445, 14699, 484, 264, 4470, 293, 264, 14170, 13, 51781], "temperature": 0.0, "avg_logprob": -0.23014391530858408, "compression_ratio": 1.6790697674418604, "no_speech_prob": 4.832550621358678e-05}, {"id": 105, "seek": 54228, "start": 542.28, "end": 546.56, "text": " Just something I wanted to mention here is, hopefully you've seen fstrings before.", "tokens": [50364, 1449, 746, 286, 1415, 281, 2152, 510, 307, 11, 4696, 291, 600, 1612, 283, 50035, 949, 13, 50578], "temperature": 0.0, "avg_logprob": -0.27168899398666246, "compression_ratio": 1.6292134831460674, "no_speech_prob": 0.003824434010311961}, {"id": 106, "seek": 54228, "start": 546.56, "end": 553.92, "text": " They're a really helpful part of Python that lets you pop a variable or an expression inside", "tokens": [50578, 814, 434, 257, 534, 4961, 644, 295, 15329, 300, 6653, 291, 1665, 257, 7006, 420, 364, 6114, 1854, 50946], "temperature": 0.0, "avg_logprob": -0.27168899398666246, "compression_ratio": 1.6292134831460674, "no_speech_prob": 0.003824434010311961}, {"id": 107, "seek": 54228, "start": 553.92, "end": 557.88, "text": " curly braces in a string, and it'll evaluate it.", "tokens": [50946, 32066, 41537, 294, 257, 6798, 11, 293, 309, 603, 13059, 309, 13, 51144], "temperature": 0.0, "avg_logprob": -0.27168899398666246, "compression_ratio": 1.6292134831460674, "no_speech_prob": 0.003824434010311961}, {"id": 108, "seek": 54228, "start": 557.88, "end": 560.62, "text": " You might not have seen this colon thing.", "tokens": [51144, 509, 1062, 406, 362, 1612, 341, 8255, 551, 13, 51281], "temperature": 0.0, "avg_logprob": -0.27168899398666246, "compression_ratio": 1.6292134831460674, "no_speech_prob": 0.003824434010311961}, {"id": 109, "seek": 54228, "start": 560.62, "end": 563.88, "text": " This is called a format specifier.", "tokens": [51281, 639, 307, 1219, 257, 7877, 1608, 9902, 13, 51444], "temperature": 0.0, "avg_logprob": -0.27168899398666246, "compression_ratio": 1.6292134831460674, "no_speech_prob": 0.003824434010311961}, {"id": 110, "seek": 54228, "start": 563.88, "end": 568.52, "text": " And with a format specifier you can change how things are printed in an fstring.", "tokens": [51444, 400, 365, 257, 7877, 1608, 9902, 291, 393, 1319, 577, 721, 366, 13567, 294, 364, 283, 37045, 13, 51676], "temperature": 0.0, "avg_logprob": -0.27168899398666246, "compression_ratio": 1.6292134831460674, "no_speech_prob": 0.003824434010311961}, {"id": 111, "seek": 54228, "start": 568.52, "end": 571.56, "text": " So this is how I'm printing it to do decimal places.", "tokens": [51676, 407, 341, 307, 577, 286, 478, 14699, 309, 281, 360, 26601, 3190, 13, 51828], "temperature": 0.0, "avg_logprob": -0.27168899398666246, "compression_ratio": 1.6292134831460674, "no_speech_prob": 0.003824434010311961}, {"id": 112, "seek": 57156, "start": 571.8399999999999, "end": 578.1199999999999, "text": " This is a two decimal places floating point number called loss, printed out here, followed", "tokens": [50378, 639, 307, 257, 732, 26601, 3190, 12607, 935, 1230, 1219, 4470, 11, 13567, 484, 510, 11, 6263, 50692], "temperature": 0.0, "avg_logprob": -0.32583038623516375, "compression_ratio": 1.549618320610687, "no_speech_prob": 0.00012533740664366633}, {"id": 113, "seek": 57156, "start": 578.1199999999999, "end": 581.8399999999999, "text": " by a comma.", "tokens": [50692, 538, 257, 22117, 13, 50878], "temperature": 0.0, "avg_logprob": -0.32583038623516375, "compression_ratio": 1.549618320610687, "no_speech_prob": 0.00012533740664366633}, {"id": 114, "seek": 57156, "start": 581.8399999999999, "end": 586.8399999999999, "text": " So I'm not going to show you how to use those, other than to say, yeah, Python fstrings and", "tokens": [50878, 407, 286, 478, 406, 516, 281, 855, 291, 577, 281, 764, 729, 11, 661, 813, 281, 584, 11, 1338, 11, 15329, 283, 50035, 293, 51128], "temperature": 0.0, "avg_logprob": -0.32583038623516375, "compression_ratio": 1.549618320610687, "no_speech_prob": 0.00012533740664366633}, {"id": 115, "seek": 57156, "start": 586.8399999999999, "end": 589.1999999999999, "text": " format specifiers are really helpful.", "tokens": [51128, 7877, 1608, 23463, 366, 534, 4961, 13, 51246], "temperature": 0.0, "avg_logprob": -0.32583038623516375, "compression_ratio": 1.549618320610687, "no_speech_prob": 0.00012533740664366633}, {"id": 116, "seek": 57156, "start": 589.1999999999999, "end": 592.3199999999999, "text": " So if you haven't used them before, do go look them up.", "tokens": [51246, 407, 498, 291, 2378, 380, 1143, 552, 949, 11, 360, 352, 574, 552, 493, 13, 51402], "temperature": 0.0, "avg_logprob": -0.32583038623516375, "compression_ratio": 1.549618320610687, "no_speech_prob": 0.00012533740664366633}, {"id": 117, "seek": 57156, "start": 592.3199999999999, "end": 596.9599999999999, "text": " A tutorial of the documentation, because they're definitely something that you'll probably", "tokens": [51402, 316, 7073, 295, 264, 14333, 11, 570, 436, 434, 2138, 746, 300, 291, 603, 1391, 51634], "temperature": 0.0, "avg_logprob": -0.32583038623516375, "compression_ratio": 1.549618320610687, "no_speech_prob": 0.00012533740664366633}, {"id": 118, "seek": 57156, "start": 596.9599999999999, "end": 599.28, "text": " find useful to know about.", "tokens": [51634, 915, 4420, 281, 458, 466, 13, 51750], "temperature": 0.0, "avg_logprob": -0.32583038623516375, "compression_ratio": 1.549618320610687, "no_speech_prob": 0.00012533740664366633}, {"id": 119, "seek": 59928, "start": 599.36, "end": 604.3199999999999, "text": " Okay, so let's just rerun all those lines of code.", "tokens": [50368, 1033, 11, 370, 718, 311, 445, 43819, 409, 439, 729, 3876, 295, 3089, 13, 50616], "temperature": 0.0, "avg_logprob": -0.2456378717532103, "compression_ratio": 1.6216216216216217, "no_speech_prob": 6.643392225669231e-06}, {"id": 120, "seek": 59928, "start": 604.3199999999999, "end": 612.3199999999999, "text": " If you're wondering how I just reran all the cells above where I was, there's a cell here", "tokens": [50616, 759, 291, 434, 6359, 577, 286, 445, 43819, 282, 439, 264, 5438, 3673, 689, 286, 390, 11, 456, 311, 257, 2815, 510, 51016], "temperature": 0.0, "avg_logprob": -0.2456378717532103, "compression_ratio": 1.6216216216216217, "no_speech_prob": 6.643392225669231e-06}, {"id": 121, "seek": 59928, "start": 612.3199999999999, "end": 615.48, "text": " that's run all above.", "tokens": [51016, 300, 311, 1190, 439, 3673, 13, 51174], "temperature": 0.0, "avg_logprob": -0.2456378717532103, "compression_ratio": 1.6216216216216217, "no_speech_prob": 6.643392225669231e-06}, {"id": 122, "seek": 59928, "start": 615.48, "end": 622.9599999999999, "text": " And it's so helpful that I always make sure there's a keyboard shortcut for that.", "tokens": [51174, 400, 309, 311, 370, 4961, 300, 286, 1009, 652, 988, 456, 311, 257, 10186, 24822, 337, 300, 13, 51548], "temperature": 0.0, "avg_logprob": -0.2456378717532103, "compression_ratio": 1.6216216216216217, "no_speech_prob": 6.643392225669231e-06}, {"id": 123, "seek": 59928, "start": 622.9599999999999, "end": 627.18, "text": " So you can see here I've added a keyboard shortcut, QA.", "tokens": [51548, 407, 291, 393, 536, 510, 286, 600, 3869, 257, 10186, 24822, 11, 1249, 32, 13, 51759], "temperature": 0.0, "avg_logprob": -0.2456378717532103, "compression_ratio": 1.6216216216216217, "no_speech_prob": 6.643392225669231e-06}, {"id": 124, "seek": 62718, "start": 627.18, "end": 630.3399999999999, "text": " So if I type QA, it runs all cells above.", "tokens": [50364, 407, 498, 286, 2010, 1249, 32, 11, 309, 6676, 439, 5438, 3673, 13, 50522], "temperature": 0.0, "avg_logprob": -0.2106375367674109, "compression_ratio": 1.706896551724138, "no_speech_prob": 0.0006563711795024574}, {"id": 125, "seek": 62718, "start": 630.3399999999999, "end": 634.9799999999999, "text": " If I type QB, it runs all cells below.", "tokens": [50522, 759, 286, 2010, 1249, 33, 11, 309, 6676, 439, 5438, 2507, 13, 50754], "temperature": 0.0, "avg_logprob": -0.2106375367674109, "compression_ratio": 1.706896551724138, "no_speech_prob": 0.0006563711795024574}, {"id": 126, "seek": 62718, "start": 634.9799999999999, "end": 638.26, "text": " And so yeah, stuff that you do a lot, make sure you've got keyboard shortcuts for them.", "tokens": [50754, 400, 370, 1338, 11, 1507, 300, 291, 360, 257, 688, 11, 652, 988, 291, 600, 658, 10186, 34620, 337, 552, 13, 50918], "temperature": 0.0, "avg_logprob": -0.2106375367674109, "compression_ratio": 1.706896551724138, "no_speech_prob": 0.0006563711795024574}, {"id": 127, "seek": 62718, "start": 638.26, "end": 640.78, "text": " You don't want to be fiddling around, moving around your mouse everywhere.", "tokens": [50918, 509, 500, 380, 528, 281, 312, 283, 14273, 1688, 926, 11, 2684, 926, 428, 9719, 5315, 13, 51044], "temperature": 0.0, "avg_logprob": -0.2106375367674109, "compression_ratio": 1.706896551724138, "no_speech_prob": 0.0006563711795024574}, {"id": 128, "seek": 62718, "start": 640.78, "end": 645.38, "text": " You want it to be as easy as thinking.", "tokens": [51044, 509, 528, 309, 281, 312, 382, 1858, 382, 1953, 13, 51274], "temperature": 0.0, "avg_logprob": -0.2106375367674109, "compression_ratio": 1.706896551724138, "no_speech_prob": 0.0006563711795024574}, {"id": 129, "seek": 62718, "start": 645.38, "end": 646.38, "text": " So this is really exciting.", "tokens": [51274, 407, 341, 307, 534, 4670, 13, 51324], "temperature": 0.0, "avg_logprob": -0.2106375367674109, "compression_ratio": 1.706896551724138, "no_speech_prob": 0.0006563711795024574}, {"id": 130, "seek": 62718, "start": 646.38, "end": 650.7399999999999, "text": " We've successfully trained a neural, built and trained a neural network model from scratch,", "tokens": [51324, 492, 600, 10727, 8895, 257, 18161, 11, 3094, 293, 8895, 257, 18161, 3209, 2316, 490, 8459, 11, 51542], "temperature": 0.0, "avg_logprob": -0.2106375367674109, "compression_ratio": 1.706896551724138, "no_speech_prob": 0.0006563711795024574}, {"id": 131, "seek": 62718, "start": 650.7399999999999, "end": 652.26, "text": " and it works okay.", "tokens": [51542, 293, 309, 1985, 1392, 13, 51618], "temperature": 0.0, "avg_logprob": -0.2106375367674109, "compression_ratio": 1.706896551724138, "no_speech_prob": 0.0006563711795024574}, {"id": 132, "seek": 62718, "start": 652.26, "end": 653.62, "text": " It's a bit clunky.", "tokens": [51618, 467, 311, 257, 857, 596, 25837, 13, 51686], "temperature": 0.0, "avg_logprob": -0.2106375367674109, "compression_ratio": 1.706896551724138, "no_speech_prob": 0.0006563711795024574}, {"id": 133, "seek": 62718, "start": 653.62, "end": 654.62, "text": " There's a lot of code.", "tokens": [51686, 821, 311, 257, 688, 295, 3089, 13, 51736], "temperature": 0.0, "avg_logprob": -0.2106375367674109, "compression_ratio": 1.706896551724138, "no_speech_prob": 0.0006563711795024574}, {"id": 134, "seek": 62718, "start": 654.62, "end": 655.62, "text": " There's features we're missing.", "tokens": [51736, 821, 311, 4122, 321, 434, 5361, 13, 51786], "temperature": 0.0, "avg_logprob": -0.2106375367674109, "compression_ratio": 1.706896551724138, "no_speech_prob": 0.0006563711795024574}, {"id": 135, "seek": 65562, "start": 655.62, "end": 658.86, "text": " So let's start refactoring it.", "tokens": [50364, 407, 718, 311, 722, 1895, 578, 3662, 309, 13, 50526], "temperature": 0.0, "avg_logprob": -0.24945541138344623, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.15814976394176483}, {"id": 136, "seek": 65562, "start": 658.86, "end": 664.74, "text": " And so refactoring is all about making it so we have to write less code to do the same", "tokens": [50526, 400, 370, 1895, 578, 3662, 307, 439, 466, 1455, 309, 370, 321, 362, 281, 2464, 1570, 3089, 281, 360, 264, 912, 50820], "temperature": 0.0, "avg_logprob": -0.24945541138344623, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.15814976394176483}, {"id": 137, "seek": 65562, "start": 664.74, "end": 671.66, "text": " work.", "tokens": [50820, 589, 13, 51166], "temperature": 0.0, "avg_logprob": -0.24945541138344623, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.15814976394176483}, {"id": 138, "seek": 65562, "start": 671.66, "end": 677.42, "text": " And so we're now going to, I'm going to show you something that's part of PyTorch, and", "tokens": [51166, 400, 370, 321, 434, 586, 516, 281, 11, 286, 478, 516, 281, 855, 291, 746, 300, 311, 644, 295, 9953, 51, 284, 339, 11, 293, 51454], "temperature": 0.0, "avg_logprob": -0.24945541138344623, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.15814976394176483}, {"id": 139, "seek": 65562, "start": 677.42, "end": 679.66, "text": " then I'm going to show you how to build it.", "tokens": [51454, 550, 286, 478, 516, 281, 855, 291, 577, 281, 1322, 309, 13, 51566], "temperature": 0.0, "avg_logprob": -0.24945541138344623, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.15814976394176483}, {"id": 140, "seek": 65562, "start": 679.66, "end": 681.66, "text": " And then you'll see why this is really useful.", "tokens": [51566, 400, 550, 291, 603, 536, 983, 341, 307, 534, 4420, 13, 51666], "temperature": 0.0, "avg_logprob": -0.24945541138344623, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.15814976394176483}, {"id": 141, "seek": 68166, "start": 681.66, "end": 686.78, "text": " So PyTorch, so PyTorch has a sub-module called nn, torch.nn.", "tokens": [50364, 407, 9953, 51, 284, 339, 11, 370, 9953, 51, 284, 339, 575, 257, 1422, 12, 8014, 2271, 1219, 297, 77, 11, 27822, 13, 26384, 13, 50620], "temperature": 0.0, "avg_logprob": -0.2369696406994836, "compression_ratio": 1.6763485477178424, "no_speech_prob": 0.005139550659805536}, {"id": 142, "seek": 68166, "start": 686.78, "end": 689.3, "text": " And in there, there's something called the module class.", "tokens": [50620, 400, 294, 456, 11, 456, 311, 746, 1219, 264, 10088, 1508, 13, 50746], "temperature": 0.0, "avg_logprob": -0.2369696406994836, "compression_ratio": 1.6763485477178424, "no_speech_prob": 0.005139550659805536}, {"id": 143, "seek": 68166, "start": 689.3, "end": 692.64, "text": " Now we can, we don't normally use it this way, but I just want to show you how it works.", "tokens": [50746, 823, 321, 393, 11, 321, 500, 380, 5646, 764, 309, 341, 636, 11, 457, 286, 445, 528, 281, 855, 291, 577, 309, 1985, 13, 50913], "temperature": 0.0, "avg_logprob": -0.2369696406994836, "compression_ratio": 1.6763485477178424, "no_speech_prob": 0.005139550659805536}, {"id": 144, "seek": 68166, "start": 692.64, "end": 697.18, "text": " We can create an instance of it in the usual way where we create instances of classes,", "tokens": [50913, 492, 393, 1884, 364, 5197, 295, 309, 294, 264, 7713, 636, 689, 321, 1884, 14519, 295, 5359, 11, 51140], "temperature": 0.0, "avg_logprob": -0.2369696406994836, "compression_ratio": 1.6763485477178424, "no_speech_prob": 0.005139550659805536}, {"id": 145, "seek": 68166, "start": 697.18, "end": 703.06, "text": " and then we can assign things to attributes of that module.", "tokens": [51140, 293, 550, 321, 393, 6269, 721, 281, 17212, 295, 300, 10088, 13, 51434], "temperature": 0.0, "avg_logprob": -0.2369696406994836, "compression_ratio": 1.6763485477178424, "no_speech_prob": 0.005139550659805536}, {"id": 146, "seek": 68166, "start": 703.06, "end": 706.48, "text": " So for example, let's assign a linear layer to it.", "tokens": [51434, 407, 337, 1365, 11, 718, 311, 6269, 257, 8213, 4583, 281, 309, 13, 51605], "temperature": 0.0, "avg_logprob": -0.2369696406994836, "compression_ratio": 1.6763485477178424, "no_speech_prob": 0.005139550659805536}, {"id": 147, "seek": 70648, "start": 706.48, "end": 713.9200000000001, "text": " And if we now print out that, you'll see it says, oh, this is a module containing something", "tokens": [50364, 400, 498, 321, 586, 4482, 484, 300, 11, 291, 603, 536, 309, 1619, 11, 1954, 11, 341, 307, 257, 10088, 19273, 746, 50736], "temperature": 0.0, "avg_logprob": -0.21430247017506804, "compression_ratio": 1.6703296703296704, "no_speech_prob": 0.006388199981302023}, {"id": 148, "seek": 70648, "start": 713.9200000000001, "end": 717.88, "text": " called foo, which is a linear layer.", "tokens": [50736, 1219, 726, 78, 11, 597, 307, 257, 8213, 4583, 13, 50934], "temperature": 0.0, "avg_logprob": -0.21430247017506804, "compression_ratio": 1.6703296703296704, "no_speech_prob": 0.006388199981302023}, {"id": 149, "seek": 70648, "start": 717.88, "end": 720.08, "text": " But here's something quite tricky.", "tokens": [50934, 583, 510, 311, 746, 1596, 12414, 13, 51044], "temperature": 0.0, "avg_logprob": -0.21430247017506804, "compression_ratio": 1.6703296703296704, "no_speech_prob": 0.006388199981302023}, {"id": 150, "seek": 70648, "start": 720.08, "end": 726.08, "text": " This module, we can say, show me all of the named children of that module.", "tokens": [51044, 639, 10088, 11, 321, 393, 584, 11, 855, 385, 439, 295, 264, 4926, 2227, 295, 300, 10088, 13, 51344], "temperature": 0.0, "avg_logprob": -0.21430247017506804, "compression_ratio": 1.6703296703296704, "no_speech_prob": 0.006388199981302023}, {"id": 151, "seek": 70648, "start": 726.08, "end": 732.6, "text": " And it says, oh, there's one called foo, and it's a linear layer.", "tokens": [51344, 400, 309, 1619, 11, 1954, 11, 456, 311, 472, 1219, 726, 78, 11, 293, 309, 311, 257, 8213, 4583, 13, 51670], "temperature": 0.0, "avg_logprob": -0.21430247017506804, "compression_ratio": 1.6703296703296704, "no_speech_prob": 0.006388199981302023}, {"id": 152, "seek": 73260, "start": 732.6, "end": 738.84, "text": " And we can say, oh, show me all of the parameters of this module.", "tokens": [50364, 400, 321, 393, 584, 11, 1954, 11, 855, 385, 439, 295, 264, 9834, 295, 341, 10088, 13, 50676], "temperature": 0.0, "avg_logprob": -0.20173064242588, "compression_ratio": 1.7252747252747254, "no_speech_prob": 0.0015730381710454822}, {"id": 153, "seek": 73260, "start": 738.84, "end": 741.72, "text": " And it says, oh, okay, sure, there's two of them.", "tokens": [50676, 400, 309, 1619, 11, 1954, 11, 1392, 11, 988, 11, 456, 311, 732, 295, 552, 13, 50820], "temperature": 0.0, "avg_logprob": -0.20173064242588, "compression_ratio": 1.7252747252747254, "no_speech_prob": 0.0015730381710454822}, {"id": 154, "seek": 73260, "start": 741.72, "end": 747.2, "text": " There's this four by three tensor, that's the weights.", "tokens": [50820, 821, 311, 341, 1451, 538, 1045, 40863, 11, 300, 311, 264, 17443, 13, 51094], "temperature": 0.0, "avg_logprob": -0.20173064242588, "compression_ratio": 1.7252747252747254, "no_speech_prob": 0.0015730381710454822}, {"id": 155, "seek": 73260, "start": 747.2, "end": 754.4200000000001, "text": " And there's this four long vector, that's the biases.", "tokens": [51094, 400, 456, 311, 341, 1451, 938, 8062, 11, 300, 311, 264, 32152, 13, 51455], "temperature": 0.0, "avg_logprob": -0.20173064242588, "compression_ratio": 1.7252747252747254, "no_speech_prob": 0.0015730381710454822}, {"id": 156, "seek": 73260, "start": 754.4200000000001, "end": 759.52, "text": " And so somehow, just by creating this module and assigning this to it, it's automatically", "tokens": [51455, 400, 370, 6063, 11, 445, 538, 4084, 341, 10088, 293, 49602, 341, 281, 309, 11, 309, 311, 6772, 51710], "temperature": 0.0, "avg_logprob": -0.20173064242588, "compression_ratio": 1.7252747252747254, "no_speech_prob": 0.0015730381710454822}, {"id": 157, "seek": 75952, "start": 759.52, "end": 764.52, "text": " tracked what's in this module, and what are its parameters.", "tokens": [50364, 31703, 437, 311, 294, 341, 10088, 11, 293, 437, 366, 1080, 9834, 13, 50614], "temperature": 0.0, "avg_logprob": -0.22376349212926464, "compression_ratio": 1.5991735537190082, "no_speech_prob": 0.0067974296398460865}, {"id": 158, "seek": 75952, "start": 764.52, "end": 765.52, "text": " That's pretty neat.", "tokens": [50614, 663, 311, 1238, 10654, 13, 50664], "temperature": 0.0, "avg_logprob": -0.22376349212926464, "compression_ratio": 1.5991735537190082, "no_speech_prob": 0.0067974296398460865}, {"id": 159, "seek": 75952, "start": 765.52, "end": 768.1999999999999, "text": " So we're going to see both how and why it does that.", "tokens": [50664, 407, 321, 434, 516, 281, 536, 1293, 577, 293, 983, 309, 775, 300, 13, 50798], "temperature": 0.0, "avg_logprob": -0.22376349212926464, "compression_ratio": 1.5991735537190082, "no_speech_prob": 0.0067974296398460865}, {"id": 160, "seek": 75952, "start": 768.1999999999999, "end": 773.1999999999999, "text": " I'm just going to point out, by the way, why did I add list here?", "tokens": [50798, 286, 478, 445, 516, 281, 935, 484, 11, 538, 264, 636, 11, 983, 630, 286, 909, 1329, 510, 30, 51048], "temperature": 0.0, "avg_logprob": -0.22376349212926464, "compression_ratio": 1.5991735537190082, "no_speech_prob": 0.0067974296398460865}, {"id": 161, "seek": 75952, "start": 773.1999999999999, "end": 780.12, "text": " If I just said m1.namedchildren, it just prints out generator object, which is not very helpful.", "tokens": [51048, 759, 286, 445, 848, 275, 16, 13, 33465, 21172, 11, 309, 445, 22305, 484, 19265, 2657, 11, 597, 307, 406, 588, 4961, 13, 51394], "temperature": 0.0, "avg_logprob": -0.22376349212926464, "compression_ratio": 1.5991735537190082, "no_speech_prob": 0.0067974296398460865}, {"id": 162, "seek": 75952, "start": 780.12, "end": 788.16, "text": " And that's because this is a kind of iterator, called a generator, and it's something which", "tokens": [51394, 400, 300, 311, 570, 341, 307, 257, 733, 295, 17138, 1639, 11, 1219, 257, 19265, 11, 293, 309, 311, 746, 597, 51796], "temperature": 0.0, "avg_logprob": -0.22376349212926464, "compression_ratio": 1.5991735537190082, "no_speech_prob": 0.0067974296398460865}, {"id": 163, "seek": 78816, "start": 788.24, "end": 794.36, "text": " is going to only produce the contents of this when I actually do something with it, such", "tokens": [50368, 307, 516, 281, 787, 5258, 264, 15768, 295, 341, 562, 286, 767, 360, 746, 365, 309, 11, 1270, 50674], "temperature": 0.0, "avg_logprob": -0.28671322054075965, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.0008167362539097667}, {"id": 164, "seek": 78816, "start": 794.36, "end": 795.56, "text": " as list them out.", "tokens": [50674, 382, 1329, 552, 484, 13, 50734], "temperature": 0.0, "avg_logprob": -0.28671322054075965, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.0008167362539097667}, {"id": 165, "seek": 78816, "start": 795.56, "end": 801.16, "text": " So just popping a list around a generator is one way to run the generator and get its", "tokens": [50734, 407, 445, 18374, 257, 1329, 926, 257, 19265, 307, 472, 636, 281, 1190, 264, 19265, 293, 483, 1080, 51014], "temperature": 0.0, "avg_logprob": -0.28671322054075965, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.0008167362539097667}, {"id": 166, "seek": 78816, "start": 801.16, "end": 802.16, "text": " output.", "tokens": [51014, 5598, 13, 51064], "temperature": 0.0, "avg_logprob": -0.28671322054075965, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.0008167362539097667}, {"id": 167, "seek": 78816, "start": 802.16, "end": 806.52, "text": " So that's a little trick when you want to look inside a generator.", "tokens": [51064, 407, 300, 311, 257, 707, 4282, 562, 291, 528, 281, 574, 1854, 257, 19265, 13, 51282], "temperature": 0.0, "avg_logprob": -0.28671322054075965, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.0008167362539097667}, {"id": 168, "seek": 78816, "start": 806.52, "end": 812.72, "text": " Okay, so now, as I said, we don't normally use it this way.", "tokens": [51282, 1033, 11, 370, 586, 11, 382, 286, 848, 11, 321, 500, 380, 5646, 764, 309, 341, 636, 13, 51592], "temperature": 0.0, "avg_logprob": -0.28671322054075965, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.0008167362539097667}, {"id": 169, "seek": 78816, "start": 812.72, "end": 815.4399999999999, "text": " What we normally do is we create our own class.", "tokens": [51592, 708, 321, 5646, 360, 307, 321, 1884, 527, 1065, 1508, 13, 51728], "temperature": 0.0, "avg_logprob": -0.28671322054075965, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.0008167362539097667}, {"id": 170, "seek": 81544, "start": 815.44, "end": 819.6, "text": " So for example, we'll create our own multi-layer perceptron, and we inherit it.", "tokens": [50364, 407, 337, 1365, 11, 321, 603, 1884, 527, 1065, 4825, 12, 8376, 260, 43276, 2044, 11, 293, 321, 21389, 309, 13, 50572], "temperature": 0.0, "avg_logprob": -0.26609086208656185, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.00014202343299984932}, {"id": 171, "seek": 81544, "start": 819.6, "end": 821.8800000000001, "text": " We inherit from nn.module.", "tokens": [50572, 492, 21389, 490, 297, 77, 13, 8014, 2271, 13, 50686], "temperature": 0.0, "avg_logprob": -0.26609086208656185, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.00014202343299984932}, {"id": 172, "seek": 81544, "start": 821.8800000000001, "end": 827.7, "text": " And so then in dunder inner, this is the thing that constructs an object of the class.", "tokens": [50686, 400, 370, 550, 294, 274, 6617, 7284, 11, 341, 307, 264, 551, 300, 7690, 82, 364, 2657, 295, 264, 1508, 13, 50977], "temperature": 0.0, "avg_logprob": -0.26609086208656185, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.00014202343299984932}, {"id": 173, "seek": 81544, "start": 827.7, "end": 830.0, "text": " This is the special magic method that does that.", "tokens": [50977, 639, 307, 264, 2121, 5585, 3170, 300, 775, 300, 13, 51092], "temperature": 0.0, "avg_logprob": -0.26609086208656185, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.00014202343299984932}, {"id": 174, "seek": 81544, "start": 830.0, "end": 834.32, "text": " We'll say, okay, well how many inputs are there to this multi-layer perceptron?", "tokens": [51092, 492, 603, 584, 11, 1392, 11, 731, 577, 867, 15743, 366, 456, 281, 341, 4825, 12, 8376, 260, 43276, 2044, 30, 51308], "temperature": 0.0, "avg_logprob": -0.26609086208656185, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.00014202343299984932}, {"id": 175, "seek": 81544, "start": 834.32, "end": 838.8000000000001, "text": " How many hidden activations, and how many output activations are there?", "tokens": [51308, 1012, 867, 7633, 2430, 763, 11, 293, 577, 867, 5598, 2430, 763, 366, 456, 30, 51532], "temperature": 0.0, "avg_logprob": -0.26609086208656185, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.00014202343299984932}, {"id": 176, "seek": 81544, "start": 838.8000000000001, "end": 840.6, "text": " So it'll just be one hidden layer.", "tokens": [51532, 407, 309, 603, 445, 312, 472, 7633, 4583, 13, 51622], "temperature": 0.0, "avg_logprob": -0.26609086208656185, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.00014202343299984932}, {"id": 177, "seek": 84060, "start": 840.6, "end": 847.44, "text": " And then here we can do just like we did up here, where we assigned things as attributes.", "tokens": [50364, 400, 550, 510, 321, 393, 360, 445, 411, 321, 630, 493, 510, 11, 689, 321, 13279, 721, 382, 17212, 13, 50706], "temperature": 0.0, "avg_logprob": -0.2757684893724395, "compression_ratio": 1.72, "no_speech_prob": 0.0023231396917253733}, {"id": 178, "seek": 84060, "start": 847.44, "end": 849.1, "text": " We can do that in this constructor.", "tokens": [50706, 492, 393, 360, 300, 294, 341, 47479, 13, 50789], "temperature": 0.0, "avg_logprob": -0.2757684893724395, "compression_ratio": 1.72, "no_speech_prob": 0.0023231396917253733}, {"id": 179, "seek": 84060, "start": 849.1, "end": 854.6, "text": " So we'll create an l1 attribute, which is a linear layer from number in to number hidden.", "tokens": [50789, 407, 321, 603, 1884, 364, 287, 16, 19667, 11, 597, 307, 257, 8213, 4583, 490, 1230, 294, 281, 1230, 7633, 13, 51064], "temperature": 0.0, "avg_logprob": -0.2757684893724395, "compression_ratio": 1.72, "no_speech_prob": 0.0023231396917253733}, {"id": 180, "seek": 84060, "start": 854.6, "end": 859.22, "text": " L2 is a linear layer from number hidden to number out.", "tokens": [51064, 441, 17, 307, 257, 8213, 4583, 490, 1230, 7633, 281, 1230, 484, 13, 51295], "temperature": 0.0, "avg_logprob": -0.2757684893724395, "compression_ratio": 1.72, "no_speech_prob": 0.0023231396917253733}, {"id": 181, "seek": 84060, "start": 859.22, "end": 861.6, "text": " And we'll also create a value.", "tokens": [51295, 400, 321, 603, 611, 1884, 257, 2158, 13, 51414], "temperature": 0.0, "avg_logprob": -0.2757684893724395, "compression_ratio": 1.72, "no_speech_prob": 0.0023231396917253733}, {"id": 182, "seek": 86160, "start": 861.6, "end": 877.28, "text": " And so when we call that module, we can take the input that we get, and run the linear", "tokens": [50364, 400, 370, 562, 321, 818, 300, 10088, 11, 321, 393, 747, 264, 4846, 300, 321, 483, 11, 293, 1190, 264, 8213, 51148], "temperature": 0.0, "avg_logprob": -0.22259799221105742, "compression_ratio": 1.5081967213114753, "no_speech_prob": 0.0037069693207740784}, {"id": 183, "seek": 86160, "start": 877.28, "end": 882.5600000000001, "text": " layer, and then run the value, and then run the l2.", "tokens": [51148, 4583, 11, 293, 550, 1190, 264, 2158, 11, 293, 550, 1190, 264, 287, 17, 13, 51412], "temperature": 0.0, "avg_logprob": -0.22259799221105742, "compression_ratio": 1.5081967213114753, "no_speech_prob": 0.0037069693207740784}, {"id": 184, "seek": 86160, "start": 882.5600000000001, "end": 889.32, "text": " And so I can create one of these, as you see.", "tokens": [51412, 400, 370, 286, 393, 1884, 472, 295, 613, 11, 382, 291, 536, 13, 51750], "temperature": 0.0, "avg_logprob": -0.22259799221105742, "compression_ratio": 1.5081967213114753, "no_speech_prob": 0.0037069693207740784}, {"id": 185, "seek": 88932, "start": 889.32, "end": 895.0400000000001, "text": " And I can have a look and see like, oh here's the attribute l1.", "tokens": [50364, 400, 286, 393, 362, 257, 574, 293, 536, 411, 11, 1954, 510, 311, 264, 19667, 287, 16, 13, 50650], "temperature": 0.0, "avg_logprob": -0.22351566314697266, "compression_ratio": 1.6556603773584906, "no_speech_prob": 0.0007436947198584676}, {"id": 186, "seek": 88932, "start": 895.0400000000001, "end": 897.24, "text": " And there it is, like I had.", "tokens": [50650, 400, 456, 309, 307, 11, 411, 286, 632, 13, 50760], "temperature": 0.0, "avg_logprob": -0.22351566314697266, "compression_ratio": 1.6556603773584906, "no_speech_prob": 0.0007436947198584676}, {"id": 187, "seek": 88932, "start": 897.24, "end": 902.96, "text": " And I can say print out the model, and the model knows all the stuff that's in it.", "tokens": [50760, 400, 286, 393, 584, 4482, 484, 264, 2316, 11, 293, 264, 2316, 3255, 439, 264, 1507, 300, 311, 294, 309, 13, 51046], "temperature": 0.0, "avg_logprob": -0.22351566314697266, "compression_ratio": 1.6556603773584906, "no_speech_prob": 0.0007436947198584676}, {"id": 188, "seek": 88932, "start": 902.96, "end": 909.36, "text": " And I can go through each of the named children, and print out the name and the layer.", "tokens": [51046, 400, 286, 393, 352, 807, 1184, 295, 264, 4926, 2227, 11, 293, 4482, 484, 264, 1315, 293, 264, 4583, 13, 51366], "temperature": 0.0, "avg_logprob": -0.22351566314697266, "compression_ratio": 1.6556603773584906, "no_speech_prob": 0.0007436947198584676}, {"id": 189, "seek": 88932, "start": 909.36, "end": 917.8800000000001, "text": " Now of course, if you remember, although you can use dunder call, we actually showed how", "tokens": [51366, 823, 295, 1164, 11, 498, 291, 1604, 11, 4878, 291, 393, 764, 274, 6617, 818, 11, 321, 767, 4712, 577, 51792], "temperature": 0.0, "avg_logprob": -0.22351566314697266, "compression_ratio": 1.6556603773584906, "no_speech_prob": 0.0007436947198584676}, {"id": 190, "seek": 91788, "start": 918.04, "end": 928.08, "text": " we can refactor things using forward, such that it would automatically kind of do the", "tokens": [50372, 321, 393, 1895, 15104, 721, 1228, 2128, 11, 1270, 300, 309, 576, 6772, 733, 295, 360, 264, 50874], "temperature": 0.0, "avg_logprob": -0.2509330318820092, "compression_ratio": 1.526946107784431, "no_speech_prob": 0.00014202318561729044}, {"id": 191, "seek": 91788, "start": 928.08, "end": 936.64, "text": " things necessary to make all the, you know, automatic gradient stuff work correctly.", "tokens": [50874, 721, 4818, 281, 652, 439, 264, 11, 291, 458, 11, 12509, 16235, 1507, 589, 8944, 13, 51302], "temperature": 0.0, "avg_logprob": -0.2509330318820092, "compression_ratio": 1.526946107784431, "no_speech_prob": 0.00014202318561729044}, {"id": 192, "seek": 91788, "start": 936.64, "end": 945.88, "text": " And so in practice, we're actually not going to do dunder call, we would do forward.", "tokens": [51302, 400, 370, 294, 3124, 11, 321, 434, 767, 406, 516, 281, 360, 274, 6617, 818, 11, 321, 576, 360, 2128, 13, 51764], "temperature": 0.0, "avg_logprob": -0.2509330318820092, "compression_ratio": 1.526946107784431, "no_speech_prob": 0.00014202318561729044}, {"id": 193, "seek": 94588, "start": 945.88, "end": 951.0, "text": " So this is an example of creating a custom PyTorch module.", "tokens": [50364, 407, 341, 307, 364, 1365, 295, 4084, 257, 2375, 9953, 51, 284, 339, 10088, 13, 50620], "temperature": 0.0, "avg_logprob": -0.2706834189912193, "compression_ratio": 1.9125, "no_speech_prob": 9.915243572322652e-05}, {"id": 194, "seek": 94588, "start": 951.0, "end": 955.7, "text": " And the key thing to recognize is that it knows what are all the attributes you added", "tokens": [50620, 400, 264, 2141, 551, 281, 5521, 307, 300, 309, 3255, 437, 366, 439, 264, 17212, 291, 3869, 50855], "temperature": 0.0, "avg_logprob": -0.2706834189912193, "compression_ratio": 1.9125, "no_speech_prob": 9.915243572322652e-05}, {"id": 195, "seek": 94588, "start": 955.7, "end": 957.32, "text": " to it.", "tokens": [50855, 281, 309, 13, 50936], "temperature": 0.0, "avg_logprob": -0.2706834189912193, "compression_ratio": 1.9125, "no_speech_prob": 9.915243572322652e-05}, {"id": 196, "seek": 94588, "start": 957.32, "end": 961.38, "text": " And it also knows what are all the parameters.", "tokens": [50936, 400, 309, 611, 3255, 437, 366, 439, 264, 9834, 13, 51139], "temperature": 0.0, "avg_logprob": -0.2706834189912193, "compression_ratio": 1.9125, "no_speech_prob": 9.915243572322652e-05}, {"id": 197, "seek": 94588, "start": 961.38, "end": 964.16, "text": " So if I go through the parameters and print out their shapes, you can see I've got my", "tokens": [51139, 407, 498, 286, 352, 807, 264, 9834, 293, 4482, 484, 641, 10854, 11, 291, 393, 536, 286, 600, 658, 452, 51278], "temperature": 0.0, "avg_logprob": -0.2706834189912193, "compression_ratio": 1.9125, "no_speech_prob": 9.915243572322652e-05}, {"id": 198, "seek": 94588, "start": 964.16, "end": 969.92, "text": " linear layers weights, first linear layer, sorry second linear layer, my, I don't know,", "tokens": [51278, 8213, 7914, 17443, 11, 700, 8213, 4583, 11, 2597, 1150, 8213, 4583, 11, 452, 11, 286, 500, 380, 458, 11, 51566], "temperature": 0.0, "avg_logprob": -0.2706834189912193, "compression_ratio": 1.9125, "no_speech_prob": 9.915243572322652e-05}, {"id": 199, "seek": 94588, "start": 969.92, "end": 974.8, "text": " yeah, first linear layers weights, my first linear layers biases, second linear layers", "tokens": [51566, 1338, 11, 700, 8213, 7914, 17443, 11, 452, 700, 8213, 7914, 32152, 11, 1150, 8213, 7914, 51810], "temperature": 0.0, "avg_logprob": -0.2706834189912193, "compression_ratio": 1.9125, "no_speech_prob": 9.915243572322652e-05}, {"id": 200, "seek": 97480, "start": 974.8, "end": 979.04, "text": " weights, second linear layers biases, and this 50 is because we set nh, the number of", "tokens": [50364, 17443, 11, 1150, 8213, 7914, 32152, 11, 293, 341, 2625, 307, 570, 321, 992, 6245, 11, 264, 1230, 295, 50576], "temperature": 0.0, "avg_logprob": -0.31978112298089106, "compression_ratio": 1.4754098360655739, "no_speech_prob": 0.0015731080202385783}, {"id": 201, "seek": 97480, "start": 979.04, "end": 984.6999999999999, "text": " hidden, to 50.", "tokens": [50576, 7633, 11, 281, 2625, 13, 50859], "temperature": 0.0, "avg_logprob": -0.31978112298089106, "compression_ratio": 1.4754098360655739, "no_speech_prob": 0.0015731080202385783}, {"id": 202, "seek": 97480, "start": 984.6999999999999, "end": 988.52, "text": " So why is that interesting?", "tokens": [50859, 407, 983, 307, 300, 1880, 30, 51050], "temperature": 0.0, "avg_logprob": -0.31978112298089106, "compression_ratio": 1.4754098360655739, "no_speech_prob": 0.0015731080202385783}, {"id": 203, "seek": 97480, "start": 988.52, "end": 998.28, "text": " Well, because now I don't have to write all this anymore, going through layers and having", "tokens": [51050, 1042, 11, 570, 586, 286, 500, 380, 362, 281, 2464, 439, 341, 3602, 11, 516, 807, 7914, 293, 1419, 51538], "temperature": 0.0, "avg_logprob": -0.31978112298089106, "compression_ratio": 1.4754098360655739, "no_speech_prob": 0.0015731080202385783}, {"id": 204, "seek": 97480, "start": 998.28, "end": 1001.4, "text": " to make sure that they've all been put into a list.", "tokens": [51538, 281, 652, 988, 300, 436, 600, 439, 668, 829, 666, 257, 1329, 13, 51694], "temperature": 0.0, "avg_logprob": -0.31978112298089106, "compression_ratio": 1.4754098360655739, "no_speech_prob": 0.0015731080202385783}, {"id": 205, "seek": 100140, "start": 1001.4, "end": 1005.88, "text": " We've just been able to add them as attributes, and they're automatically going to appear", "tokens": [50364, 492, 600, 445, 668, 1075, 281, 909, 552, 382, 17212, 11, 293, 436, 434, 6772, 516, 281, 4204, 50588], "temperature": 0.0, "avg_logprob": -0.26482709248860675, "compression_ratio": 1.6995708154506437, "no_speech_prob": 0.01032770611345768}, {"id": 206, "seek": 100140, "start": 1005.88, "end": 1006.88, "text": " as parameters.", "tokens": [50588, 382, 9834, 13, 50638], "temperature": 0.0, "avg_logprob": -0.26482709248860675, "compression_ratio": 1.6995708154506437, "no_speech_prob": 0.01032770611345768}, {"id": 207, "seek": 100140, "start": 1006.88, "end": 1014.16, "text": " So we can just say, go through each parameter and update it based on the gradient and the", "tokens": [50638, 407, 321, 393, 445, 584, 11, 352, 807, 1184, 13075, 293, 5623, 309, 2361, 322, 264, 16235, 293, 264, 51002], "temperature": 0.0, "avg_logprob": -0.26482709248860675, "compression_ratio": 1.6995708154506437, "no_speech_prob": 0.01032770611345768}, {"id": 208, "seek": 100140, "start": 1014.16, "end": 1015.9399999999999, "text": " learning rate.", "tokens": [51002, 2539, 3314, 13, 51091], "temperature": 0.0, "avg_logprob": -0.26482709248860675, "compression_ratio": 1.6995708154506437, "no_speech_prob": 0.01032770611345768}, {"id": 209, "seek": 100140, "start": 1015.9399999999999, "end": 1020.64, "text": " And furthermore, you can actually just go model.zero grad and it will zero out all of", "tokens": [51091, 400, 3052, 3138, 11, 291, 393, 767, 445, 352, 2316, 13, 32226, 2771, 293, 309, 486, 4018, 484, 439, 295, 51326], "temperature": 0.0, "avg_logprob": -0.26482709248860675, "compression_ratio": 1.6995708154506437, "no_speech_prob": 0.01032770611345768}, {"id": 210, "seek": 100140, "start": 1020.64, "end": 1023.92, "text": " the gradients.", "tokens": [51326, 264, 2771, 2448, 13, 51490], "temperature": 0.0, "avg_logprob": -0.26482709248860675, "compression_ratio": 1.6995708154506437, "no_speech_prob": 0.01032770611345768}, {"id": 211, "seek": 100140, "start": 1023.92, "end": 1029.52, "text": " So that's really made our code quite a lot nicer and quite a lot more flexible, which", "tokens": [51490, 407, 300, 311, 534, 1027, 527, 3089, 1596, 257, 688, 22842, 293, 1596, 257, 688, 544, 11358, 11, 597, 51770], "temperature": 0.0, "avg_logprob": -0.26482709248860675, "compression_ratio": 1.6995708154506437, "no_speech_prob": 0.01032770611345768}, {"id": 212, "seek": 102952, "start": 1029.52, "end": 1032.44, "text": " is cool.", "tokens": [50364, 307, 1627, 13, 50510], "temperature": 0.0, "avg_logprob": -0.27100140707833426, "compression_ratio": 1.3757575757575757, "no_speech_prob": 0.006003508809953928}, {"id": 213, "seek": 102952, "start": 1032.44, "end": 1039.72, "text": " So let's check that this still works.", "tokens": [50510, 407, 718, 311, 1520, 300, 341, 920, 1985, 13, 50874], "temperature": 0.0, "avg_logprob": -0.27100140707833426, "compression_ratio": 1.3757575757575757, "no_speech_prob": 0.006003508809953928}, {"id": 214, "seek": 102952, "start": 1039.72, "end": 1040.72, "text": " There we go.", "tokens": [50874, 821, 321, 352, 13, 50924], "temperature": 0.0, "avg_logprob": -0.27100140707833426, "compression_ratio": 1.3757575757575757, "no_speech_prob": 0.006003508809953928}, {"id": 215, "seek": 102952, "start": 1040.72, "end": 1047.08, "text": " So just to clarify, if I called report on this before I ran it, as you would expect,", "tokens": [50924, 407, 445, 281, 17594, 11, 498, 286, 1219, 2275, 322, 341, 949, 286, 5872, 309, 11, 382, 291, 576, 2066, 11, 51242], "temperature": 0.0, "avg_logprob": -0.27100140707833426, "compression_ratio": 1.3757575757575757, "no_speech_prob": 0.006003508809953928}, {"id": 216, "seek": 102952, "start": 1047.08, "end": 1053.48, "text": " the accuracy is about 8%, well about 10%, a bit less, and the loss is pretty high.", "tokens": [51242, 264, 14170, 307, 466, 1649, 8923, 731, 466, 1266, 8923, 257, 857, 1570, 11, 293, 264, 4470, 307, 1238, 1090, 13, 51562], "temperature": 0.0, "avg_logprob": -0.27100140707833426, "compression_ratio": 1.3757575757575757, "no_speech_prob": 0.006003508809953928}, {"id": 217, "seek": 105348, "start": 1053.48, "end": 1062.3600000000001, "text": " And so after I run this fit, this model, the accuracy goes up and the loss goes down.", "tokens": [50364, 400, 370, 934, 286, 1190, 341, 3318, 11, 341, 2316, 11, 264, 14170, 1709, 493, 293, 264, 4470, 1709, 760, 13, 50808], "temperature": 0.0, "avg_logprob": -0.24561309814453125, "compression_ratio": 1.5868544600938967, "no_speech_prob": 0.0033765132538974285}, {"id": 218, "seek": 105348, "start": 1062.3600000000001, "end": 1066.56, "text": " So basically it's all of this exactly the same as before.", "tokens": [50808, 407, 1936, 309, 311, 439, 295, 341, 2293, 264, 912, 382, 949, 13, 51018], "temperature": 0.0, "avg_logprob": -0.24561309814453125, "compression_ratio": 1.5868544600938967, "no_speech_prob": 0.0033765132538974285}, {"id": 219, "seek": 105348, "start": 1066.56, "end": 1069.06, "text": " The only thing I've changed are these two lines of code.", "tokens": [51018, 440, 787, 551, 286, 600, 3105, 366, 613, 732, 3876, 295, 3089, 13, 51143], "temperature": 0.0, "avg_logprob": -0.24561309814453125, "compression_ratio": 1.5868544600938967, "no_speech_prob": 0.0033765132538974285}, {"id": 220, "seek": 105348, "start": 1069.06, "end": 1070.92, "text": " So that's a really useful refactoring.", "tokens": [51143, 407, 300, 311, 257, 534, 4420, 1895, 578, 3662, 13, 51236], "temperature": 0.0, "avg_logprob": -0.24561309814453125, "compression_ratio": 1.5868544600938967, "no_speech_prob": 0.0033765132538974285}, {"id": 221, "seek": 105348, "start": 1070.92, "end": 1073.2, "text": " So how on earth did this happen?", "tokens": [51236, 407, 577, 322, 4120, 630, 341, 1051, 30, 51350], "temperature": 0.0, "avg_logprob": -0.24561309814453125, "compression_ratio": 1.5868544600938967, "no_speech_prob": 0.0033765132538974285}, {"id": 222, "seek": 105348, "start": 1073.2, "end": 1079.8, "text": " How did it know what the parameters and layers are automatically?", "tokens": [51350, 1012, 630, 309, 458, 437, 264, 9834, 293, 7914, 366, 6772, 30, 51680], "temperature": 0.0, "avg_logprob": -0.24561309814453125, "compression_ratio": 1.5868544600938967, "no_speech_prob": 0.0033765132538974285}, {"id": 223, "seek": 107980, "start": 1079.8, "end": 1092.84, "text": " It used a trick called dunder setattr, and we're going to create our own nn.module now.", "tokens": [50364, 467, 1143, 257, 4282, 1219, 274, 6617, 992, 1591, 81, 11, 293, 321, 434, 516, 281, 1884, 527, 1065, 297, 77, 13, 8014, 2271, 586, 13, 51016], "temperature": 0.0, "avg_logprob": -0.23573959420580382, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.04271914064884186}, {"id": 224, "seek": 107980, "start": 1092.84, "end": 1097.34, "text": " So if there was no such thing as nn.module, here's how we'd build it.", "tokens": [51016, 407, 498, 456, 390, 572, 1270, 551, 382, 297, 77, 13, 8014, 2271, 11, 510, 311, 577, 321, 1116, 1322, 309, 13, 51241], "temperature": 0.0, "avg_logprob": -0.23573959420580382, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.04271914064884186}, {"id": 225, "seek": 107980, "start": 1097.34, "end": 1099.7, "text": " And so let's actually build it and also add some things to it.", "tokens": [51241, 400, 370, 718, 311, 767, 1322, 309, 293, 611, 909, 512, 721, 281, 309, 13, 51359], "temperature": 0.0, "avg_logprob": -0.23573959420580382, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.04271914064884186}, {"id": 226, "seek": 107980, "start": 1099.7, "end": 1104.72, "text": " So in dunder init, we would have to create a dictionary for our named children.", "tokens": [51359, 407, 294, 274, 6617, 3157, 11, 321, 576, 362, 281, 1884, 257, 25890, 337, 527, 4926, 2227, 13, 51610], "temperature": 0.0, "avg_logprob": -0.23573959420580382, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.04271914064884186}, {"id": 227, "seek": 107980, "start": 1104.72, "end": 1108.84, "text": " This is going to contain a list, a dictionary of all of the layers.", "tokens": [51610, 639, 307, 516, 281, 5304, 257, 1329, 11, 257, 25890, 295, 439, 295, 264, 7914, 13, 51816], "temperature": 0.0, "avg_logprob": -0.23573959420580382, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.04271914064884186}, {"id": 228, "seek": 110884, "start": 1108.8799999999999, "end": 1114.4399999999998, "text": " Okay, and then just like before, we'll create a couple of linear layers.", "tokens": [50366, 1033, 11, 293, 550, 445, 411, 949, 11, 321, 603, 1884, 257, 1916, 295, 8213, 7914, 13, 50644], "temperature": 0.0, "avg_logprob": -0.2597171465555827, "compression_ratio": 1.7677165354330708, "no_speech_prob": 1.5446303223143332e-05}, {"id": 229, "seek": 110884, "start": 1114.4399999999998, "end": 1118.08, "text": " And then what we're going to do is we're going to define this special magic thing that Python", "tokens": [50644, 400, 550, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 6964, 341, 2121, 5585, 551, 300, 15329, 50826], "temperature": 0.0, "avg_logprob": -0.2597171465555827, "compression_ratio": 1.7677165354330708, "no_speech_prob": 1.5446303223143332e-05}, {"id": 230, "seek": 110884, "start": 1118.08, "end": 1119.84, "text": " has called dunder setattr.", "tokens": [50826, 575, 1219, 274, 6617, 992, 1591, 81, 13, 50914], "temperature": 0.0, "avg_logprob": -0.2597171465555827, "compression_ratio": 1.7677165354330708, "no_speech_prob": 1.5446303223143332e-05}, {"id": 231, "seek": 110884, "start": 1119.84, "end": 1125.72, "text": " And this is called automatically by Python, if you have it, every time you set an attribute,", "tokens": [50914, 400, 341, 307, 1219, 6772, 538, 15329, 11, 498, 291, 362, 309, 11, 633, 565, 291, 992, 364, 19667, 11, 51208], "temperature": 0.0, "avg_logprob": -0.2597171465555827, "compression_ratio": 1.7677165354330708, "no_speech_prob": 1.5446303223143332e-05}, {"id": 232, "seek": 110884, "start": 1125.72, "end": 1128.28, "text": " such as here or here.", "tokens": [51208, 1270, 382, 510, 420, 510, 13, 51336], "temperature": 0.0, "avg_logprob": -0.2597171465555827, "compression_ratio": 1.7677165354330708, "no_speech_prob": 1.5446303223143332e-05}, {"id": 233, "seek": 110884, "start": 1128.28, "end": 1133.1599999999999, "text": " And it's going to be passed the name of the attribute, the key, and the value is the actual", "tokens": [51336, 400, 309, 311, 516, 281, 312, 4678, 264, 1315, 295, 264, 19667, 11, 264, 2141, 11, 293, 264, 2158, 307, 264, 3539, 51580], "temperature": 0.0, "avg_logprob": -0.2597171465555827, "compression_ratio": 1.7677165354330708, "no_speech_prob": 1.5446303223143332e-05}, {"id": 234, "seek": 110884, "start": 1133.1599999999999, "end": 1136.48, "text": " thing on the right hand side of the equals sign.", "tokens": [51580, 551, 322, 264, 558, 1011, 1252, 295, 264, 6915, 1465, 13, 51746], "temperature": 0.0, "avg_logprob": -0.2597171465555827, "compression_ratio": 1.7677165354330708, "no_speech_prob": 1.5446303223143332e-05}, {"id": 235, "seek": 113648, "start": 1136.48, "end": 1143.3600000000001, "text": " Now generally speaking, things that start with an underscore, we use for private stuff.", "tokens": [50364, 823, 5101, 4124, 11, 721, 300, 722, 365, 364, 37556, 11, 321, 764, 337, 4551, 1507, 13, 50708], "temperature": 0.0, "avg_logprob": -0.2154298573732376, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.00255087623372674}, {"id": 236, "seek": 113648, "start": 1143.3600000000001, "end": 1147.8, "text": " So we check that it doesn't start with an underscore.", "tokens": [50708, 407, 321, 1520, 300, 309, 1177, 380, 722, 365, 364, 37556, 13, 50930], "temperature": 0.0, "avg_logprob": -0.2154298573732376, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.00255087623372674}, {"id": 237, "seek": 113648, "start": 1147.8, "end": 1158.1200000000001, "text": " And if it doesn't start with an underscore, setattr will put this value into the modules", "tokens": [50930, 400, 498, 309, 1177, 380, 722, 365, 364, 37556, 11, 992, 1591, 81, 486, 829, 341, 2158, 666, 264, 16679, 51446], "temperature": 0.0, "avg_logprob": -0.2154298573732376, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.00255087623372674}, {"id": 238, "seek": 113648, "start": 1158.1200000000001, "end": 1161.34, "text": " dictionary with this key.", "tokens": [51446, 25890, 365, 341, 2141, 13, 51607], "temperature": 0.0, "avg_logprob": -0.2154298573732376, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.00255087623372674}, {"id": 239, "seek": 116134, "start": 1161.34, "end": 1168.98, "text": " And then call the normal Python setattr to make sure it just actually does the attribute", "tokens": [50364, 400, 550, 818, 264, 2710, 15329, 992, 1591, 81, 281, 652, 988, 309, 445, 767, 775, 264, 19667, 50746], "temperature": 0.0, "avg_logprob": -0.25146488307677595, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.004133897367864847}, {"id": 240, "seek": 116134, "start": 1168.98, "end": 1170.9399999999998, "text": " setting.", "tokens": [50746, 3287, 13, 50844], "temperature": 0.0, "avg_logprob": -0.25146488307677595, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.004133897367864847}, {"id": 241, "seek": 116134, "start": 1170.9399999999998, "end": 1177.74, "text": " So super is how you call whatever is in the super class, the base class.", "tokens": [50844, 407, 1687, 307, 577, 291, 818, 2035, 307, 294, 264, 1687, 1508, 11, 264, 3096, 1508, 13, 51184], "temperature": 0.0, "avg_logprob": -0.25146488307677595, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.004133897367864847}, {"id": 242, "seek": 116134, "start": 1177.74, "end": 1183.5, "text": " So another useful thing to know about is how does it do this nifty thing where you can", "tokens": [51184, 407, 1071, 4420, 551, 281, 458, 466, 307, 577, 775, 309, 360, 341, 297, 37177, 551, 689, 291, 393, 51472], "temperature": 0.0, "avg_logprob": -0.25146488307677595, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.004133897367864847}, {"id": 243, "seek": 116134, "start": 1183.5, "end": 1187.5, "text": " just type the name and it kind of lists out all this information about it.", "tokens": [51472, 445, 2010, 264, 1315, 293, 309, 733, 295, 14511, 484, 439, 341, 1589, 466, 309, 13, 51672], "temperature": 0.0, "avg_logprob": -0.25146488307677595, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.004133897367864847}, {"id": 244, "seek": 116134, "start": 1187.5, "end": 1190.1999999999998, "text": " That's a special thing called dunder repr.", "tokens": [51672, 663, 311, 257, 2121, 551, 1219, 274, 6617, 1085, 81, 13, 51807], "temperature": 0.0, "avg_logprob": -0.25146488307677595, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.004133897367864847}, {"id": 245, "seek": 119020, "start": 1190.2, "end": 1197.96, "text": " So here dunder repr will just have it return a stringified version of the modules dictionary.", "tokens": [50364, 407, 510, 274, 6617, 1085, 81, 486, 445, 362, 309, 2736, 257, 6798, 2587, 3037, 295, 264, 16679, 25890, 13, 50752], "temperature": 0.0, "avg_logprob": -0.253946111659811, "compression_ratio": 1.826086956521739, "no_speech_prob": 7.602472032885998e-05}, {"id": 246, "seek": 119020, "start": 1197.96, "end": 1199.92, "text": " And then here we've got parameters.", "tokens": [50752, 400, 550, 510, 321, 600, 658, 9834, 13, 50850], "temperature": 0.0, "avg_logprob": -0.253946111659811, "compression_ratio": 1.826086956521739, "no_speech_prob": 7.602472032885998e-05}, {"id": 247, "seek": 119020, "start": 1199.92, "end": 1202.2, "text": " How did parameters work?", "tokens": [50850, 1012, 630, 9834, 589, 30, 50964], "temperature": 0.0, "avg_logprob": -0.253946111659811, "compression_ratio": 1.826086956521739, "no_speech_prob": 7.602472032885998e-05}, {"id": 248, "seek": 119020, "start": 1202.2, "end": 1204.3600000000001, "text": " So how did this thing work?", "tokens": [50964, 407, 577, 630, 341, 551, 589, 30, 51072], "temperature": 0.0, "avg_logprob": -0.253946111659811, "compression_ratio": 1.826086956521739, "no_speech_prob": 7.602472032885998e-05}, {"id": 249, "seek": 119020, "start": 1204.3600000000001, "end": 1210.4, "text": " Well, we can go through each of those modules, go through each value.", "tokens": [51072, 1042, 11, 321, 393, 352, 807, 1184, 295, 729, 16679, 11, 352, 807, 1184, 2158, 13, 51374], "temperature": 0.0, "avg_logprob": -0.253946111659811, "compression_ratio": 1.826086956521739, "no_speech_prob": 7.602472032885998e-05}, {"id": 250, "seek": 119020, "start": 1210.4, "end": 1213.3600000000001, "text": " So the values of the modules is all the actual layers.", "tokens": [51374, 407, 264, 4190, 295, 264, 16679, 307, 439, 264, 3539, 7914, 13, 51522], "temperature": 0.0, "avg_logprob": -0.253946111659811, "compression_ratio": 1.826086956521739, "no_speech_prob": 7.602472032885998e-05}, {"id": 251, "seek": 119020, "start": 1213.3600000000001, "end": 1219.44, "text": " And then go through each of the parameters in each module and yield p.", "tokens": [51522, 400, 550, 352, 807, 1184, 295, 264, 9834, 294, 1184, 10088, 293, 11257, 280, 13, 51826], "temperature": 0.0, "avg_logprob": -0.253946111659811, "compression_ratio": 1.826086956521739, "no_speech_prob": 7.602472032885998e-05}, {"id": 252, "seek": 121944, "start": 1219.72, "end": 1224.3200000000002, "text": " So that's going to create an iterator, if you remember when we looked at iterators,", "tokens": [50378, 407, 300, 311, 516, 281, 1884, 364, 17138, 1639, 11, 498, 291, 1604, 562, 321, 2956, 412, 17138, 3391, 11, 50608], "temperature": 0.0, "avg_logprob": -0.26467695236206057, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.00013135149492882192}, {"id": 253, "seek": 121944, "start": 1224.3200000000002, "end": 1225.76, "text": " for all the parameters.", "tokens": [50608, 337, 439, 264, 9834, 13, 50680], "temperature": 0.0, "avg_logprob": -0.26467695236206057, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.00013135149492882192}, {"id": 254, "seek": 121944, "start": 1225.76, "end": 1226.76, "text": " So let's try it.", "tokens": [50680, 407, 718, 311, 853, 309, 13, 50730], "temperature": 0.0, "avg_logprob": -0.26467695236206057, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.00013135149492882192}, {"id": 255, "seek": 121944, "start": 1226.76, "end": 1229.0, "text": " So we can create one of these modules.", "tokens": [50730, 407, 321, 393, 1884, 472, 295, 613, 16679, 13, 50842], "temperature": 0.0, "avg_logprob": -0.26467695236206057, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.00013135149492882192}, {"id": 256, "seek": 121944, "start": 1229.0, "end": 1235.0, "text": " And if we just like before, loop through its parameters, there they are.", "tokens": [50842, 400, 498, 321, 445, 411, 949, 11, 6367, 807, 1080, 9834, 11, 456, 436, 366, 13, 51142], "temperature": 0.0, "avg_logprob": -0.26467695236206057, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.00013135149492882192}, {"id": 257, "seek": 121944, "start": 1235.0, "end": 1240.24, "text": " Now I'll just mention something that's optional, kind of like advanced Python that a lot of", "tokens": [51142, 823, 286, 603, 445, 2152, 746, 300, 311, 17312, 11, 733, 295, 411, 7339, 15329, 300, 257, 688, 295, 51404], "temperature": 0.0, "avg_logprob": -0.26467695236206057, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.00013135149492882192}, {"id": 258, "seek": 121944, "start": 1240.24, "end": 1241.96, "text": " people don't know about.", "tokens": [51404, 561, 500, 380, 458, 466, 13, 51490], "temperature": 0.0, "avg_logprob": -0.26467695236206057, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.00013135149492882192}, {"id": 259, "seek": 121944, "start": 1241.96, "end": 1247.88, "text": " Which is there's no need to loop through a list or a generator, or I guess say loop through", "tokens": [51490, 3013, 307, 456, 311, 572, 643, 281, 6367, 807, 257, 1329, 420, 257, 19265, 11, 420, 286, 2041, 584, 6367, 807, 51786], "temperature": 0.0, "avg_logprob": -0.26467695236206057, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.00013135149492882192}, {"id": 260, "seek": 124788, "start": 1247.88, "end": 1250.6000000000001, "text": " an iterator and yield.", "tokens": [50364, 364, 17138, 1639, 293, 11257, 13, 50500], "temperature": 0.0, "avg_logprob": -0.23601806804698, "compression_ratio": 1.6080402010050252, "no_speech_prob": 0.0002780306385830045}, {"id": 261, "seek": 124788, "start": 1250.6000000000001, "end": 1259.72, "text": " There's actually a shortcut, which is you can just say yield from, and then give it", "tokens": [50500, 821, 311, 767, 257, 24822, 11, 597, 307, 291, 393, 445, 584, 11257, 490, 11, 293, 550, 976, 309, 50956], "temperature": 0.0, "avg_logprob": -0.23601806804698, "compression_ratio": 1.6080402010050252, "no_speech_prob": 0.0002780306385830045}, {"id": 262, "seek": 124788, "start": 1259.72, "end": 1261.64, "text": " the iterator.", "tokens": [50956, 264, 17138, 1639, 13, 51052], "temperature": 0.0, "avg_logprob": -0.23601806804698, "compression_ratio": 1.6080402010050252, "no_speech_prob": 0.0002780306385830045}, {"id": 263, "seek": 124788, "start": 1261.64, "end": 1268.16, "text": " And so with that, we can get this all down to one line of code, and it will do exactly", "tokens": [51052, 400, 370, 365, 300, 11, 321, 393, 483, 341, 439, 760, 281, 472, 1622, 295, 3089, 11, 293, 309, 486, 360, 2293, 51378], "temperature": 0.0, "avg_logprob": -0.23601806804698, "compression_ratio": 1.6080402010050252, "no_speech_prob": 0.0002780306385830045}, {"id": 264, "seek": 124788, "start": 1268.16, "end": 1269.8000000000002, "text": " the same thing.", "tokens": [51378, 264, 912, 551, 13, 51460], "temperature": 0.0, "avg_logprob": -0.23601806804698, "compression_ratio": 1.6080402010050252, "no_speech_prob": 0.0002780306385830045}, {"id": 265, "seek": 124788, "start": 1269.8000000000002, "end": 1274.8000000000002, "text": " So that's basically saying yield one at a time, everything in here.", "tokens": [51460, 407, 300, 311, 1936, 1566, 11257, 472, 412, 257, 565, 11, 1203, 294, 510, 13, 51710], "temperature": 0.0, "avg_logprob": -0.23601806804698, "compression_ratio": 1.6080402010050252, "no_speech_prob": 0.0002780306385830045}, {"id": 266, "seek": 124788, "start": 1274.8000000000002, "end": 1276.88, "text": " That's what yield from does.", "tokens": [51710, 663, 311, 437, 11257, 490, 775, 13, 51814], "temperature": 0.0, "avg_logprob": -0.23601806804698, "compression_ratio": 1.6080402010050252, "no_speech_prob": 0.0002780306385830045}, {"id": 267, "seek": 127688, "start": 1276.88, "end": 1281.88, "text": " So there's a cool little advanced Python thing, totally optional, but if you're interested,", "tokens": [50364, 407, 456, 311, 257, 1627, 707, 7339, 15329, 551, 11, 3879, 17312, 11, 457, 498, 291, 434, 3102, 11, 50614], "temperature": 0.0, "avg_logprob": -0.21912409311317535, "compression_ratio": 1.4684210526315788, "no_speech_prob": 7.141875539673492e-05}, {"id": 268, "seek": 127688, "start": 1281.88, "end": 1284.72, "text": " I think it can be kind of neat.", "tokens": [50614, 286, 519, 309, 393, 312, 733, 295, 10654, 13, 50756], "temperature": 0.0, "avg_logprob": -0.21912409311317535, "compression_ratio": 1.4684210526315788, "no_speech_prob": 7.141875539673492e-05}, {"id": 269, "seek": 127688, "start": 1284.72, "end": 1289.92, "text": " So we have now learned how to create our own implementation of nn.module, and therefore", "tokens": [50756, 407, 321, 362, 586, 3264, 577, 281, 1884, 527, 1065, 11420, 295, 297, 77, 13, 8014, 2271, 11, 293, 4412, 51016], "temperature": 0.0, "avg_logprob": -0.21912409311317535, "compression_ratio": 1.4684210526315788, "no_speech_prob": 7.141875539673492e-05}, {"id": 270, "seek": 127688, "start": 1289.92, "end": 1292.8400000000001, "text": " we are now allowed to use PyTorch's nn.module.", "tokens": [51016, 321, 366, 586, 4350, 281, 764, 9953, 51, 284, 339, 311, 297, 77, 13, 8014, 2271, 13, 51162], "temperature": 0.0, "avg_logprob": -0.21912409311317535, "compression_ratio": 1.4684210526315788, "no_speech_prob": 7.141875539673492e-05}, {"id": 271, "seek": 127688, "start": 1292.8400000000001, "end": 1298.0600000000002, "text": " So that's good news.", "tokens": [51162, 407, 300, 311, 665, 2583, 13, 51423], "temperature": 0.0, "avg_logprob": -0.21912409311317535, "compression_ratio": 1.4684210526315788, "no_speech_prob": 7.141875539673492e-05}, {"id": 272, "seek": 129806, "start": 1298.06, "end": 1310.22, "text": " So how would we do, using the PyTorch nn.module, how would we create the model that we started", "tokens": [50364, 407, 577, 576, 321, 360, 11, 1228, 264, 9953, 51, 284, 339, 297, 77, 13, 8014, 2271, 11, 577, 576, 321, 1884, 264, 2316, 300, 321, 1409, 50972], "temperature": 0.0, "avg_logprob": -0.21031936766609313, "compression_ratio": 1.4225352112676057, "no_speech_prob": 0.0015486490447074175}, {"id": 273, "seek": 129806, "start": 1310.22, "end": 1311.26, "text": " with?", "tokens": [50972, 365, 30, 51024], "temperature": 0.0, "avg_logprob": -0.21031936766609313, "compression_ratio": 1.4225352112676057, "no_speech_prob": 0.0015486490447074175}, {"id": 274, "seek": 129806, "start": 1311.26, "end": 1316.02, "text": " Which is where we had this self.layers.", "tokens": [51024, 3013, 307, 689, 321, 632, 341, 2698, 13, 8376, 433, 13, 51262], "temperature": 0.0, "avg_logprob": -0.21031936766609313, "compression_ratio": 1.4225352112676057, "no_speech_prob": 0.0015486490447074175}, {"id": 275, "seek": 129806, "start": 1316.02, "end": 1320.78, "text": " Because we want to somehow register all of these all at once.", "tokens": [51262, 1436, 321, 528, 281, 6063, 7280, 439, 295, 613, 439, 412, 1564, 13, 51500], "temperature": 0.0, "avg_logprob": -0.21031936766609313, "compression_ratio": 1.4225352112676057, "no_speech_prob": 0.0015486490447074175}, {"id": 276, "seek": 132078, "start": 1320.78, "end": 1329.78, "text": " That's not going to happen based on the code we just wrote.", "tokens": [50364, 663, 311, 406, 516, 281, 1051, 2361, 322, 264, 3089, 321, 445, 4114, 13, 50814], "temperature": 0.0, "avg_logprob": -0.2351926248284835, "compression_ratio": 1.4352941176470588, "no_speech_prob": 0.021947581321001053}, {"id": 277, "seek": 132078, "start": 1329.78, "end": 1333.16, "text": " So to do that, let's have a look.", "tokens": [50814, 407, 281, 360, 300, 11, 718, 311, 362, 257, 574, 13, 50983], "temperature": 0.0, "avg_logprob": -0.2351926248284835, "compression_ratio": 1.4352941176470588, "no_speech_prob": 0.021947581321001053}, {"id": 278, "seek": 132078, "start": 1333.16, "end": 1336.8, "text": " We can, so let's make a list of the layers we want.", "tokens": [50983, 492, 393, 11, 370, 718, 311, 652, 257, 1329, 295, 264, 7914, 321, 528, 13, 51165], "temperature": 0.0, "avg_logprob": -0.2351926248284835, "compression_ratio": 1.4352941176470588, "no_speech_prob": 0.021947581321001053}, {"id": 279, "seek": 132078, "start": 1336.8, "end": 1340.8999999999999, "text": " And so we'll create again a subclass of nn.module.", "tokens": [51165, 400, 370, 321, 603, 1884, 797, 257, 1422, 11665, 295, 297, 77, 13, 8014, 2271, 13, 51370], "temperature": 0.0, "avg_logprob": -0.2351926248284835, "compression_ratio": 1.4352941176470588, "no_speech_prob": 0.021947581321001053}, {"id": 280, "seek": 132078, "start": 1340.8999999999999, "end": 1345.1399999999999, "text": " Make sure you call the superclasses init first.", "tokens": [51370, 4387, 988, 291, 818, 264, 1687, 11665, 279, 3157, 700, 13, 51582], "temperature": 0.0, "avg_logprob": -0.2351926248284835, "compression_ratio": 1.4352941176470588, "no_speech_prob": 0.021947581321001053}, {"id": 281, "seek": 134514, "start": 1345.14, "end": 1347.74, "text": " And we'll just store the list of layers.", "tokens": [50364, 400, 321, 603, 445, 3531, 264, 1329, 295, 7914, 13, 50494], "temperature": 0.0, "avg_logprob": -0.2518774503237241, "compression_ratio": 1.5635359116022098, "no_speech_prob": 0.0024725894909352064}, {"id": 282, "seek": 134514, "start": 1347.74, "end": 1356.46, "text": " And then to tell PyTorch about all those layers, we basically have to loop through them and", "tokens": [50494, 400, 550, 281, 980, 9953, 51, 284, 339, 466, 439, 729, 7914, 11, 321, 1936, 362, 281, 6367, 807, 552, 293, 50930], "temperature": 0.0, "avg_logprob": -0.2518774503237241, "compression_ratio": 1.5635359116022098, "no_speech_prob": 0.0024725894909352064}, {"id": 283, "seek": 134514, "start": 1356.46, "end": 1358.7800000000002, "text": " call add module.", "tokens": [50930, 818, 909, 10088, 13, 51046], "temperature": 0.0, "avg_logprob": -0.2518774503237241, "compression_ratio": 1.5635359116022098, "no_speech_prob": 0.0024725894909352064}, {"id": 284, "seek": 134514, "start": 1358.7800000000002, "end": 1366.3000000000002, "text": " And say what the name of the module is, and what the module is.", "tokens": [51046, 400, 584, 437, 264, 1315, 295, 264, 10088, 307, 11, 293, 437, 264, 10088, 307, 13, 51422], "temperature": 0.0, "avg_logprob": -0.2518774503237241, "compression_ratio": 1.5635359116022098, "no_speech_prob": 0.0024725894909352064}, {"id": 285, "seek": 134514, "start": 1366.3000000000002, "end": 1372.8000000000002, "text": " And again, probably should have used forward here in the first place.", "tokens": [51422, 400, 797, 11, 1391, 820, 362, 1143, 2128, 510, 294, 264, 700, 1081, 13, 51747], "temperature": 0.0, "avg_logprob": -0.2518774503237241, "compression_ratio": 1.5635359116022098, "no_speech_prob": 0.0024725894909352064}, {"id": 286, "seek": 137280, "start": 1372.84, "end": 1376.3999999999999, "text": " And you can see this is now done exactly the same thing.", "tokens": [50366, 400, 291, 393, 536, 341, 307, 586, 1096, 2293, 264, 912, 551, 13, 50544], "temperature": 0.0, "avg_logprob": -0.292180567371602, "compression_ratio": 1.6211453744493391, "no_speech_prob": 0.025561755523085594}, {"id": 287, "seek": 137280, "start": 1376.3999999999999, "end": 1381.32, "text": " So if you've used a sequential model before, you can see that we're on the path to creating", "tokens": [50544, 407, 498, 291, 600, 1143, 257, 42881, 2316, 949, 11, 291, 393, 536, 300, 321, 434, 322, 264, 3100, 281, 4084, 50790], "temperature": 0.0, "avg_logprob": -0.292180567371602, "compression_ratio": 1.6211453744493391, "no_speech_prob": 0.025561755523085594}, {"id": 288, "seek": 137280, "start": 1381.32, "end": 1382.72, "text": " a sequential model.", "tokens": [50790, 257, 42881, 2316, 13, 50860], "temperature": 0.0, "avg_logprob": -0.292180567371602, "compression_ratio": 1.6211453744493391, "no_speech_prob": 0.025561755523085594}, {"id": 289, "seek": 137280, "start": 1382.72, "end": 1391.36, "text": " Okay, so Ganesh has asked an interesting question, which is, what on earth is super calling?", "tokens": [50860, 1033, 11, 370, 460, 12779, 71, 575, 2351, 364, 1880, 1168, 11, 597, 307, 11, 437, 322, 4120, 307, 1687, 5141, 30, 51292], "temperature": 0.0, "avg_logprob": -0.292180567371602, "compression_ratio": 1.6211453744493391, "no_speech_prob": 0.025561755523085594}, {"id": 290, "seek": 137280, "start": 1391.36, "end": 1394.3999999999999, "text": " Because we actually, in fact we don't even need the parentheses here.", "tokens": [51292, 1436, 321, 767, 11, 294, 1186, 321, 500, 380, 754, 643, 264, 34153, 510, 13, 51444], "temperature": 0.0, "avg_logprob": -0.292180567371602, "compression_ratio": 1.6211453744493391, "no_speech_prob": 0.025561755523085594}, {"id": 291, "seek": 137280, "start": 1394.3999999999999, "end": 1397.96, "text": " We actually don't have a base class.", "tokens": [51444, 492, 767, 500, 380, 362, 257, 3096, 1508, 13, 51622], "temperature": 0.0, "avg_logprob": -0.292180567371602, "compression_ratio": 1.6211453744493391, "no_speech_prob": 0.025561755523085594}, {"id": 292, "seek": 139796, "start": 1397.96, "end": 1403.0, "text": " That's because if you don't put any parentheses, or if you put empty parentheses, it's actually", "tokens": [50364, 663, 311, 570, 498, 291, 500, 380, 829, 604, 34153, 11, 420, 498, 291, 829, 6707, 34153, 11, 309, 311, 767, 50616], "temperature": 0.0, "avg_logprob": -0.24286823687346085, "compression_ratio": 1.583710407239819, "no_speech_prob": 0.005910936743021011}, {"id": 293, "seek": 139796, "start": 1403.0, "end": 1406.28, "text": " a shortcut for writing that.", "tokens": [50616, 257, 24822, 337, 3579, 300, 13, 50780], "temperature": 0.0, "avg_logprob": -0.24286823687346085, "compression_ratio": 1.583710407239819, "no_speech_prob": 0.005910936743021011}, {"id": 294, "seek": 139796, "start": 1406.28, "end": 1414.68, "text": " And so Python has stuff in object which does, you know, all the normal objecty things, like", "tokens": [50780, 400, 370, 15329, 575, 1507, 294, 2657, 597, 775, 11, 291, 458, 11, 439, 264, 2710, 2657, 88, 721, 11, 411, 51200], "temperature": 0.0, "avg_logprob": -0.24286823687346085, "compression_ratio": 1.583710407239819, "no_speech_prob": 0.005910936743021011}, {"id": 295, "seek": 139796, "start": 1414.68, "end": 1418.8, "text": " storing your attributes so that you can get them back later.", "tokens": [51200, 26085, 428, 17212, 370, 300, 291, 393, 483, 552, 646, 1780, 13, 51406], "temperature": 0.0, "avg_logprob": -0.24286823687346085, "compression_ratio": 1.583710407239819, "no_speech_prob": 0.005910936743021011}, {"id": 296, "seek": 139796, "start": 1418.8, "end": 1421.8, "text": " So that's what's happening there.", "tokens": [51406, 407, 300, 311, 437, 311, 2737, 456, 13, 51556], "temperature": 0.0, "avg_logprob": -0.24286823687346085, "compression_ratio": 1.583710407239819, "no_speech_prob": 0.005910936743021011}, {"id": 297, "seek": 139796, "start": 1421.8, "end": 1426.48, "text": " Okay, so this is a little bit awkward.", "tokens": [51556, 1033, 11, 370, 341, 307, 257, 707, 857, 11411, 13, 51790], "temperature": 0.0, "avg_logprob": -0.24286823687346085, "compression_ratio": 1.583710407239819, "no_speech_prob": 0.005910936743021011}, {"id": 298, "seek": 142648, "start": 1426.48, "end": 1431.52, "text": " Is to have to store the list, and then enumerate, and call addModule.", "tokens": [50364, 1119, 281, 362, 281, 3531, 264, 1329, 11, 293, 550, 465, 15583, 473, 11, 293, 818, 909, 44, 378, 2271, 13, 50616], "temperature": 0.0, "avg_logprob": -0.2823197200380523, "compression_ratio": 1.7887323943661972, "no_speech_prob": 0.0007793649565428495}, {"id": 299, "seek": 142648, "start": 1431.52, "end": 1436.1200000000001, "text": " So now that we've implemented that from scratch, we can use PyTorch's version, which is, they've", "tokens": [50616, 407, 586, 300, 321, 600, 12270, 300, 490, 8459, 11, 321, 393, 764, 9953, 51, 284, 339, 311, 3037, 11, 597, 307, 11, 436, 600, 50846], "temperature": 0.0, "avg_logprob": -0.2823197200380523, "compression_ratio": 1.7887323943661972, "no_speech_prob": 0.0007793649565428495}, {"id": 300, "seek": 142648, "start": 1436.1200000000001, "end": 1439.24, "text": " just got something called moduleList that just does that for you.", "tokens": [50846, 445, 658, 746, 1219, 10088, 43, 468, 300, 445, 775, 300, 337, 291, 13, 51002], "temperature": 0.0, "avg_logprob": -0.2823197200380523, "compression_ratio": 1.7887323943661972, "no_speech_prob": 0.0007793649565428495}, {"id": 301, "seek": 142648, "start": 1439.24, "end": 1444.96, "text": " Okay, so if you use moduleList, and pass it a list of layers, it will just go ahead and", "tokens": [51002, 1033, 11, 370, 498, 291, 764, 10088, 43, 468, 11, 293, 1320, 309, 257, 1329, 295, 7914, 11, 309, 486, 445, 352, 2286, 293, 51288], "temperature": 0.0, "avg_logprob": -0.2823197200380523, "compression_ratio": 1.7887323943661972, "no_speech_prob": 0.0007793649565428495}, {"id": 302, "seek": 142648, "start": 1444.96, "end": 1447.4, "text": " register them, all those modules for you.", "tokens": [51288, 7280, 552, 11, 439, 729, 16679, 337, 291, 13, 51410], "temperature": 0.0, "avg_logprob": -0.2823197200380523, "compression_ratio": 1.7887323943661972, "no_speech_prob": 0.0007793649565428495}, {"id": 303, "seek": 142648, "start": 1447.4, "end": 1448.92, "text": " So here's something called sequential model.", "tokens": [51410, 407, 510, 311, 746, 1219, 42881, 2316, 13, 51486], "temperature": 0.0, "avg_logprob": -0.2823197200380523, "compression_ratio": 1.7887323943661972, "no_speech_prob": 0.0007793649565428495}, {"id": 304, "seek": 142648, "start": 1448.92, "end": 1451.8, "text": " So this is just like an nnot sequential now.", "tokens": [51486, 407, 341, 307, 445, 411, 364, 297, 2247, 42881, 586, 13, 51630], "temperature": 0.0, "avg_logprob": -0.2823197200380523, "compression_ratio": 1.7887323943661972, "no_speech_prob": 0.0007793649565428495}, {"id": 305, "seek": 142648, "start": 1451.8, "end": 1454.98, "text": " So if I create it, passing in the layers, there you go.", "tokens": [51630, 407, 498, 286, 1884, 309, 11, 8437, 294, 264, 7914, 11, 456, 291, 352, 13, 51789], "temperature": 0.0, "avg_logprob": -0.2823197200380523, "compression_ratio": 1.7887323943661972, "no_speech_prob": 0.0007793649565428495}, {"id": 306, "seek": 145498, "start": 1454.98, "end": 1460.18, "text": " You can see there's my model containing my moduleList with my layers.", "tokens": [50364, 509, 393, 536, 456, 311, 452, 2316, 19273, 452, 10088, 43, 468, 365, 452, 7914, 13, 50624], "temperature": 0.0, "avg_logprob": -0.41604083858124197, "compression_ratio": 1.4176470588235295, "no_speech_prob": 0.023330023512244225}, {"id": 307, "seek": 145498, "start": 1460.18, "end": 1466.06, "text": " And so, I don't know why I never used forward for these things.", "tokens": [50624, 400, 370, 11, 286, 500, 380, 458, 983, 286, 1128, 1143, 2128, 337, 613, 721, 13, 50918], "temperature": 0.0, "avg_logprob": -0.41604083858124197, "compression_ratio": 1.4176470588235295, "no_speech_prob": 0.023330023512244225}, {"id": 308, "seek": 145498, "start": 1466.06, "end": 1467.06, "text": " It's silly.", "tokens": [50918, 467, 311, 11774, 13, 50968], "temperature": 0.0, "avg_logprob": -0.41604083858124197, "compression_ratio": 1.4176470588235295, "no_speech_prob": 0.023330023512244225}, {"id": 309, "seek": 145498, "start": 1467.06, "end": 1471.78, "text": " I guess it doesn't edit terribly in this stage, but anyhow.", "tokens": [50968, 286, 2041, 309, 1177, 380, 8129, 22903, 294, 341, 3233, 11, 457, 44995, 13, 51204], "temperature": 0.0, "avg_logprob": -0.41604083858124197, "compression_ratio": 1.4176470588235295, "no_speech_prob": 0.023330023512244225}, {"id": 310, "seek": 145498, "start": 1471.78, "end": 1480.18, "text": " Okay, so call fit, and there we go.", "tokens": [51204, 1033, 11, 370, 818, 3318, 11, 293, 456, 321, 352, 13, 51624], "temperature": 0.0, "avg_logprob": -0.41604083858124197, "compression_ratio": 1.4176470588235295, "no_speech_prob": 0.023330023512244225}, {"id": 311, "seek": 148018, "start": 1480.18, "end": 1487.8, "text": " So in forward here, I just go through each layer, and I set the result of that equal", "tokens": [50364, 407, 294, 2128, 510, 11, 286, 445, 352, 807, 1184, 4583, 11, 293, 286, 992, 264, 1874, 295, 300, 2681, 50745], "temperature": 0.0, "avg_logprob": -0.25171061679049656, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.0018969055963680148}, {"id": 312, "seek": 148018, "start": 1487.8, "end": 1493.14, "text": " to calling that layer on the previous result, and then pass and return it at the end.", "tokens": [50745, 281, 5141, 300, 4583, 322, 264, 3894, 1874, 11, 293, 550, 1320, 293, 2736, 309, 412, 264, 917, 13, 51012], "temperature": 0.0, "avg_logprob": -0.25171061679049656, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.0018969055963680148}, {"id": 313, "seek": 148018, "start": 1493.14, "end": 1496.64, "text": " Now there's a little, another way of doing this, which I think is kind of fun.", "tokens": [51012, 823, 456, 311, 257, 707, 11, 1071, 636, 295, 884, 341, 11, 597, 286, 519, 307, 733, 295, 1019, 13, 51187], "temperature": 0.0, "avg_logprob": -0.25171061679049656, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.0018969055963680148}, {"id": 314, "seek": 148018, "start": 1496.64, "end": 1499.8600000000001, "text": " It's not like shorter or anything at this stage.", "tokens": [51187, 467, 311, 406, 411, 11639, 420, 1340, 412, 341, 3233, 13, 51348], "temperature": 0.0, "avg_logprob": -0.25171061679049656, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.0018969055963680148}, {"id": 315, "seek": 148018, "start": 1499.8600000000001, "end": 1503.1000000000001, "text": " I just wanted to show an example of something that you see quite a lot in machine learning", "tokens": [51348, 286, 445, 1415, 281, 855, 364, 1365, 295, 746, 300, 291, 536, 1596, 257, 688, 294, 3479, 2539, 51510], "temperature": 0.0, "avg_logprob": -0.25171061679049656, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.0018969055963680148}, {"id": 316, "seek": 148018, "start": 1503.1000000000001, "end": 1506.54, "text": " code, which is the use of reduce.", "tokens": [51510, 3089, 11, 597, 307, 264, 764, 295, 5407, 13, 51682], "temperature": 0.0, "avg_logprob": -0.25171061679049656, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.0018969055963680148}, {"id": 317, "seek": 150654, "start": 1506.54, "end": 1514.98, "text": " This implementation here is exactly the same as this thing here.", "tokens": [50364, 639, 11420, 510, 307, 2293, 264, 912, 382, 341, 551, 510, 13, 50786], "temperature": 0.0, "avg_logprob": -0.26104170411497685, "compression_ratio": 1.6325581395348838, "no_speech_prob": 0.005060257855802774}, {"id": 318, "seek": 150654, "start": 1514.98, "end": 1518.34, "text": " So let me explain how it works.", "tokens": [50786, 407, 718, 385, 2903, 577, 309, 1985, 13, 50954], "temperature": 0.0, "avg_logprob": -0.26104170411497685, "compression_ratio": 1.6325581395348838, "no_speech_prob": 0.005060257855802774}, {"id": 319, "seek": 150654, "start": 1518.34, "end": 1523.94, "text": " What reduce does, so reduce is a very common kind of like fundamental computer science", "tokens": [50954, 708, 5407, 775, 11, 370, 5407, 307, 257, 588, 2689, 733, 295, 411, 8088, 3820, 3497, 51234], "temperature": 0.0, "avg_logprob": -0.26104170411497685, "compression_ratio": 1.6325581395348838, "no_speech_prob": 0.005060257855802774}, {"id": 320, "seek": 150654, "start": 1523.94, "end": 1524.94, "text": " concept.", "tokens": [51234, 3410, 13, 51284], "temperature": 0.0, "avg_logprob": -0.26104170411497685, "compression_ratio": 1.6325581395348838, "no_speech_prob": 0.005060257855802774}, {"id": 321, "seek": 150654, "start": 1524.94, "end": 1525.94, "text": " Reductions.", "tokens": [51284, 4477, 84, 3916, 13, 51334], "temperature": 0.0, "avg_logprob": -0.26104170411497685, "compression_ratio": 1.6325581395348838, "no_speech_prob": 0.005060257855802774}, {"id": 322, "seek": 150654, "start": 1525.94, "end": 1527.22, "text": " This is something that does a reduction.", "tokens": [51334, 639, 307, 746, 300, 775, 257, 11004, 13, 51398], "temperature": 0.0, "avg_logprob": -0.26104170411497685, "compression_ratio": 1.6325581395348838, "no_speech_prob": 0.005060257855802774}, {"id": 323, "seek": 150654, "start": 1527.22, "end": 1534.3, "text": " And what a reduction is, is it's something that says, start with the third parameter,", "tokens": [51398, 400, 437, 257, 11004, 307, 11, 307, 309, 311, 746, 300, 1619, 11, 722, 365, 264, 2636, 13075, 11, 51752], "temperature": 0.0, "avg_logprob": -0.26104170411497685, "compression_ratio": 1.6325581395348838, "no_speech_prob": 0.005060257855802774}, {"id": 324, "seek": 150654, "start": 1534.3, "end": 1535.3, "text": " some initial value.", "tokens": [51752, 512, 5883, 2158, 13, 51802], "temperature": 0.0, "avg_logprob": -0.26104170411497685, "compression_ratio": 1.6325581395348838, "no_speech_prob": 0.005060257855802774}, {"id": 325, "seek": 153530, "start": 1535.56, "end": 1541.4199999999998, "text": " So we're going to start with x, the thing we've been passed, and then loop through a", "tokens": [50377, 407, 321, 434, 516, 281, 722, 365, 2031, 11, 264, 551, 321, 600, 668, 4678, 11, 293, 550, 6367, 807, 257, 50670], "temperature": 0.0, "avg_logprob": -0.30843130412854647, "compression_ratio": 1.8051282051282052, "no_speech_prob": 9.461260196985677e-05}, {"id": 326, "seek": 153530, "start": 1541.4199999999998, "end": 1542.4199999999998, "text": " sequence.", "tokens": [50670, 8310, 13, 50720], "temperature": 0.0, "avg_logprob": -0.30843130412854647, "compression_ratio": 1.8051282051282052, "no_speech_prob": 9.461260196985677e-05}, {"id": 327, "seek": 153530, "start": 1542.4199999999998, "end": 1548.36, "text": " So loop through each of our layers, and then for each layer, call some function.", "tokens": [50720, 407, 6367, 807, 1184, 295, 527, 7914, 11, 293, 550, 337, 1184, 4583, 11, 818, 512, 2445, 13, 51017], "temperature": 0.0, "avg_logprob": -0.30843130412854647, "compression_ratio": 1.8051282051282052, "no_speech_prob": 9.461260196985677e-05}, {"id": 328, "seek": 153530, "start": 1548.36, "end": 1555.4199999999998, "text": " Here is our function, and the function is going to get passed first time around, it'll", "tokens": [51017, 1692, 307, 527, 2445, 11, 293, 264, 2445, 307, 516, 281, 483, 4678, 700, 565, 926, 11, 309, 603, 51370], "temperature": 0.0, "avg_logprob": -0.30843130412854647, "compression_ratio": 1.8051282051282052, "no_speech_prob": 9.461260196985677e-05}, {"id": 329, "seek": 153530, "start": 1555.4199999999998, "end": 1559.94, "text": " be passed the initial value, and the first thing in your list.", "tokens": [51370, 312, 4678, 264, 5883, 2158, 11, 293, 264, 700, 551, 294, 428, 1329, 13, 51596], "temperature": 0.0, "avg_logprob": -0.30843130412854647, "compression_ratio": 1.8051282051282052, "no_speech_prob": 9.461260196985677e-05}, {"id": 330, "seek": 153530, "start": 1559.94, "end": 1562.52, "text": " So your first layer and x.", "tokens": [51596, 407, 428, 700, 4583, 293, 2031, 13, 51725], "temperature": 0.0, "avg_logprob": -0.30843130412854647, "compression_ratio": 1.8051282051282052, "no_speech_prob": 9.461260196985677e-05}, {"id": 331, "seek": 156252, "start": 1562.56, "end": 1567.28, "text": " So it's just going to call the layer function on x.", "tokens": [50366, 407, 309, 311, 445, 516, 281, 818, 264, 4583, 2445, 322, 2031, 13, 50602], "temperature": 0.0, "avg_logprob": -0.2559252531632133, "compression_ratio": 1.8125, "no_speech_prob": 8.220187010010704e-05}, {"id": 332, "seek": 156252, "start": 1567.28, "end": 1573.0, "text": " The second time around, it takes the output of that, and passes it in as the first parameter,", "tokens": [50602, 440, 1150, 565, 926, 11, 309, 2516, 264, 5598, 295, 300, 11, 293, 11335, 309, 294, 382, 264, 700, 13075, 11, 50888], "temperature": 0.0, "avg_logprob": -0.2559252531632133, "compression_ratio": 1.8125, "no_speech_prob": 8.220187010010704e-05}, {"id": 333, "seek": 156252, "start": 1573.0, "end": 1574.56, "text": " and passes in the second layer.", "tokens": [50888, 293, 11335, 294, 264, 1150, 4583, 13, 50966], "temperature": 0.0, "avg_logprob": -0.2559252531632133, "compression_ratio": 1.8125, "no_speech_prob": 8.220187010010704e-05}, {"id": 334, "seek": 156252, "start": 1574.56, "end": 1578.84, "text": " So then the second time this goes through, it's going to be calling the second layer", "tokens": [50966, 407, 550, 264, 1150, 565, 341, 1709, 807, 11, 309, 311, 516, 281, 312, 5141, 264, 1150, 4583, 51180], "temperature": 0.0, "avg_logprob": -0.2559252531632133, "compression_ratio": 1.8125, "no_speech_prob": 8.220187010010704e-05}, {"id": 335, "seek": 156252, "start": 1578.84, "end": 1582.58, "text": " on the result of the first layer, and so forth.", "tokens": [51180, 322, 264, 1874, 295, 264, 700, 4583, 11, 293, 370, 5220, 13, 51367], "temperature": 0.0, "avg_logprob": -0.2559252531632133, "compression_ratio": 1.8125, "no_speech_prob": 8.220187010010704e-05}, {"id": 336, "seek": 156252, "start": 1582.58, "end": 1584.44, "text": " And that's what a reduction is.", "tokens": [51367, 400, 300, 311, 437, 257, 11004, 307, 13, 51460], "temperature": 0.0, "avg_logprob": -0.2559252531632133, "compression_ratio": 1.8125, "no_speech_prob": 8.220187010010704e-05}, {"id": 337, "seek": 156252, "start": 1584.44, "end": 1590.74, "text": " And so when you might see reduce, you'll certainly see it talked about quite a lot in papers", "tokens": [51460, 400, 370, 562, 291, 1062, 536, 5407, 11, 291, 603, 3297, 536, 309, 2825, 466, 1596, 257, 688, 294, 10577, 51775], "temperature": 0.0, "avg_logprob": -0.2559252531632133, "compression_ratio": 1.8125, "no_speech_prob": 8.220187010010704e-05}, {"id": 338, "seek": 159074, "start": 1590.78, "end": 1593.38, "text": " and books, and you might sometimes also see it in code.", "tokens": [50366, 293, 3642, 11, 293, 291, 1062, 2171, 611, 536, 309, 294, 3089, 13, 50496], "temperature": 0.0, "avg_logprob": -0.2606162614719842, "compression_ratio": 1.5360360360360361, "no_speech_prob": 0.00048029678873717785}, {"id": 339, "seek": 159074, "start": 1593.38, "end": 1595.9, "text": " It's a very general concept.", "tokens": [50496, 467, 311, 257, 588, 2674, 3410, 13, 50622], "temperature": 0.0, "avg_logprob": -0.2606162614719842, "compression_ratio": 1.5360360360360361, "no_speech_prob": 0.00048029678873717785}, {"id": 340, "seek": 159074, "start": 1595.9, "end": 1605.34, "text": " And so here's how you can implement a sequential model using reduce.", "tokens": [50622, 400, 370, 510, 311, 577, 291, 393, 4445, 257, 42881, 2316, 1228, 5407, 13, 51094], "temperature": 0.0, "avg_logprob": -0.2606162614719842, "compression_ratio": 1.5360360360360361, "no_speech_prob": 0.00048029678873717785}, {"id": 341, "seek": 159074, "start": 1605.34, "end": 1608.26, "text": " So there's no explicit loop there, although the loop is still happening internally.", "tokens": [51094, 407, 456, 311, 572, 13691, 6367, 456, 11, 4878, 264, 6367, 307, 920, 2737, 19501, 13, 51240], "temperature": 0.0, "avg_logprob": -0.2606162614719842, "compression_ratio": 1.5360360360360361, "no_speech_prob": 0.00048029678873717785}, {"id": 342, "seek": 159074, "start": 1608.26, "end": 1613.74, "text": " All right, so now that we've re-implemented sequential, we can just go ahead and use PyTorch's", "tokens": [51240, 1057, 558, 11, 370, 586, 300, 321, 600, 319, 12, 332, 781, 14684, 42881, 11, 321, 393, 445, 352, 2286, 293, 764, 9953, 51, 284, 339, 311, 51514], "temperature": 0.0, "avg_logprob": -0.2606162614719842, "compression_ratio": 1.5360360360360361, "no_speech_prob": 0.00048029678873717785}, {"id": 343, "seek": 159074, "start": 1613.74, "end": 1614.74, "text": " version.", "tokens": [51514, 3037, 13, 51564], "temperature": 0.0, "avg_logprob": -0.2606162614719842, "compression_ratio": 1.5360360360360361, "no_speech_prob": 0.00048029678873717785}, {"id": 344, "seek": 161474, "start": 1614.74, "end": 1615.74, "text": " So there's an n.sequential.", "tokens": [50364, 407, 456, 311, 364, 297, 13, 11834, 2549, 13, 50414], "temperature": 0.0, "avg_logprob": -0.3270472173821436, "compression_ratio": 1.5057471264367817, "no_speech_prob": 0.028435569256544113}, {"id": 345, "seek": 161474, "start": 1615.74, "end": 1622.3, "text": " We can pass in our layers, and we can fit, not surprisingly.", "tokens": [50414, 492, 393, 1320, 294, 527, 7914, 11, 293, 321, 393, 3318, 11, 406, 17600, 13, 50742], "temperature": 0.0, "avg_logprob": -0.3270472173821436, "compression_ratio": 1.5057471264367817, "no_speech_prob": 0.028435569256544113}, {"id": 346, "seek": 161474, "start": 1622.3, "end": 1623.7, "text": " We can see the model.", "tokens": [50742, 492, 393, 536, 264, 2316, 13, 50812], "temperature": 0.0, "avg_logprob": -0.3270472173821436, "compression_ratio": 1.5057471264367817, "no_speech_prob": 0.028435569256544113}, {"id": 347, "seek": 161474, "start": 1623.7, "end": 1628.22, "text": " So yeah, it looks very similar to the one we built ourselves.", "tokens": [50812, 407, 1338, 11, 309, 1542, 588, 2531, 281, 264, 472, 321, 3094, 4175, 13, 51038], "temperature": 0.0, "avg_logprob": -0.3270472173821436, "compression_ratio": 1.5057471264367817, "no_speech_prob": 0.028435569256544113}, {"id": 348, "seek": 161474, "start": 1628.22, "end": 1640.54, "text": " All right, so this thing of looping through parameters, and updating our parameters based", "tokens": [51038, 1057, 558, 11, 370, 341, 551, 295, 6367, 278, 807, 9834, 11, 293, 25113, 527, 9834, 2361, 51654], "temperature": 0.0, "avg_logprob": -0.3270472173821436, "compression_ratio": 1.5057471264367817, "no_speech_prob": 0.028435569256544113}, {"id": 349, "seek": 164054, "start": 1640.54, "end": 1650.1399999999999, "text": " on gradients and a learning rate, and then zeroing them, is very common.", "tokens": [50364, 322, 2771, 2448, 293, 257, 2539, 3314, 11, 293, 550, 4018, 278, 552, 11, 307, 588, 2689, 13, 50844], "temperature": 0.0, "avg_logprob": -0.2554454584231322, "compression_ratio": 1.6298342541436464, "no_speech_prob": 0.0541965514421463}, {"id": 350, "seek": 164054, "start": 1650.1399999999999, "end": 1656.86, "text": " So common that there is something that does that all for us, and that's called an optimizer.", "tokens": [50844, 407, 2689, 300, 456, 307, 746, 300, 775, 300, 439, 337, 505, 11, 293, 300, 311, 1219, 364, 5028, 6545, 13, 51180], "temperature": 0.0, "avg_logprob": -0.2554454584231322, "compression_ratio": 1.6298342541436464, "no_speech_prob": 0.0541965514421463}, {"id": 351, "seek": 164054, "start": 1656.86, "end": 1658.42, "text": " It's the stuff in Optim.", "tokens": [51180, 467, 311, 264, 1507, 294, 21455, 332, 13, 51258], "temperature": 0.0, "avg_logprob": -0.2554454584231322, "compression_ratio": 1.6298342541436464, "no_speech_prob": 0.0541965514421463}, {"id": 352, "seek": 164054, "start": 1658.42, "end": 1662.78, "text": " So let's create our own optimizer, and as you can see, it's just going to do the two", "tokens": [51258, 407, 718, 311, 1884, 527, 1065, 5028, 6545, 11, 293, 382, 291, 393, 536, 11, 309, 311, 445, 516, 281, 360, 264, 732, 51476], "temperature": 0.0, "avg_logprob": -0.2554454584231322, "compression_ratio": 1.6298342541436464, "no_speech_prob": 0.0541965514421463}, {"id": 353, "seek": 164054, "start": 1662.78, "end": 1665.0, "text": " things we just saw.", "tokens": [51476, 721, 321, 445, 1866, 13, 51587], "temperature": 0.0, "avg_logprob": -0.2554454584231322, "compression_ratio": 1.6298342541436464, "no_speech_prob": 0.0541965514421463}, {"id": 354, "seek": 166500, "start": 1665.0, "end": 1670.88, "text": " It's going to go through each of the parameters, and update them using the gradient and the", "tokens": [50364, 467, 311, 516, 281, 352, 807, 1184, 295, 264, 9834, 11, 293, 5623, 552, 1228, 264, 16235, 293, 264, 50658], "temperature": 0.0, "avg_logprob": -0.32320232391357423, "compression_ratio": 1.7534883720930232, "no_speech_prob": 0.028435006737709045}, {"id": 355, "seek": 166500, "start": 1670.88, "end": 1676.24, "text": " learning rate, and there's also zero grad, which will go through each parameter, and", "tokens": [50658, 2539, 3314, 11, 293, 456, 311, 611, 4018, 2771, 11, 597, 486, 352, 807, 1184, 13075, 11, 293, 50926], "temperature": 0.0, "avg_logprob": -0.32320232391357423, "compression_ratio": 1.7534883720930232, "no_speech_prob": 0.028435006737709045}, {"id": 356, "seek": 166500, "start": 1676.24, "end": 1681.76, "text": " set their gradients to zero.", "tokens": [50926, 992, 641, 2771, 2448, 281, 4018, 13, 51202], "temperature": 0.0, "avg_logprob": -0.32320232391357423, "compression_ratio": 1.7534883720930232, "no_speech_prob": 0.028435006737709045}, {"id": 357, "seek": 166500, "start": 1681.76, "end": 1688.2, "text": " If you use .data, it's just a way of avoiding having to say torch.no grad, basically.", "tokens": [51202, 759, 291, 764, 2411, 67, 3274, 11, 309, 311, 445, 257, 636, 295, 20220, 1419, 281, 584, 27822, 13, 1771, 2771, 11, 1936, 13, 51524], "temperature": 0.0, "avg_logprob": -0.32320232391357423, "compression_ratio": 1.7534883720930232, "no_speech_prob": 0.028435006737709045}, {"id": 358, "seek": 166500, "start": 1688.2, "end": 1691.08, "text": " Okay so in optimizer, we're going to pass it the parameters that we want to optimize,", "tokens": [51524, 1033, 370, 294, 5028, 6545, 11, 321, 434, 516, 281, 1320, 309, 264, 9834, 300, 321, 528, 281, 19719, 11, 51668], "temperature": 0.0, "avg_logprob": -0.32320232391357423, "compression_ratio": 1.7534883720930232, "no_speech_prob": 0.028435006737709045}, {"id": 359, "seek": 169108, "start": 1691.08, "end": 1697.52, "text": " and we're going to pass it the learning rate, and we're just going to store them away.", "tokens": [50364, 293, 321, 434, 516, 281, 1320, 309, 264, 2539, 3314, 11, 293, 321, 434, 445, 516, 281, 3531, 552, 1314, 13, 50686], "temperature": 0.0, "avg_logprob": -0.2046841303507487, "compression_ratio": 1.6848249027237354, "no_speech_prob": 0.03161754831671715}, {"id": 360, "seek": 169108, "start": 1697.52, "end": 1704.6599999999999, "text": " And since the parameters might be a generator, we'll call list to turn them into a list.", "tokens": [50686, 400, 1670, 264, 9834, 1062, 312, 257, 19265, 11, 321, 603, 818, 1329, 281, 1261, 552, 666, 257, 1329, 13, 51043], "temperature": 0.0, "avg_logprob": -0.2046841303507487, "compression_ratio": 1.6848249027237354, "no_speech_prob": 0.03161754831671715}, {"id": 361, "seek": 169108, "start": 1704.6599999999999, "end": 1709.9199999999998, "text": " So we are going to create our optimizer, pass it in the model.parameters, which have been", "tokens": [51043, 407, 321, 366, 516, 281, 1884, 527, 5028, 6545, 11, 1320, 309, 294, 264, 2316, 13, 2181, 335, 6202, 11, 597, 362, 668, 51306], "temperature": 0.0, "avg_logprob": -0.2046841303507487, "compression_ratio": 1.6848249027237354, "no_speech_prob": 0.03161754831671715}, {"id": 362, "seek": 169108, "start": 1709.9199999999998, "end": 1714.48, "text": " automatically constructed for us by nn.module, and so here's our new loop.", "tokens": [51306, 6772, 17083, 337, 505, 538, 297, 77, 13, 8014, 2271, 11, 293, 370, 510, 311, 527, 777, 6367, 13, 51534], "temperature": 0.0, "avg_logprob": -0.2046841303507487, "compression_ratio": 1.6848249027237354, "no_speech_prob": 0.03161754831671715}, {"id": 363, "seek": 169108, "start": 1714.48, "end": 1720.52, "text": " Now we don't have to do any of the stuff manually, we can just say Opt.step, so that's going", "tokens": [51534, 823, 321, 500, 380, 362, 281, 360, 604, 295, 264, 1507, 16945, 11, 321, 393, 445, 584, 21455, 13, 16792, 11, 370, 300, 311, 516, 51836], "temperature": 0.0, "avg_logprob": -0.2046841303507487, "compression_ratio": 1.6848249027237354, "no_speech_prob": 0.03161754831671715}, {"id": 364, "seek": 172052, "start": 1720.52, "end": 1729.76, "text": " to call this, and Opt.zeroGrad, and that's going to call this.", "tokens": [50364, 281, 818, 341, 11, 293, 21455, 13, 32226, 38, 6206, 11, 293, 300, 311, 516, 281, 818, 341, 13, 50826], "temperature": 0.0, "avg_logprob": -0.23026897931339765, "compression_ratio": 1.5265486725663717, "no_speech_prob": 2.4682904040673748e-05}, {"id": 365, "seek": 172052, "start": 1729.76, "end": 1731.52, "text": " There it is.", "tokens": [50826, 821, 309, 307, 13, 50914], "temperature": 0.0, "avg_logprob": -0.23026897931339765, "compression_ratio": 1.5265486725663717, "no_speech_prob": 2.4682904040673748e-05}, {"id": 366, "seek": 172052, "start": 1731.52, "end": 1735.12, "text": " So we've now built our own SGD optimizer from scratch.", "tokens": [50914, 407, 321, 600, 586, 3094, 527, 1065, 34520, 35, 5028, 6545, 490, 8459, 13, 51094], "temperature": 0.0, "avg_logprob": -0.23026897931339765, "compression_ratio": 1.5265486725663717, "no_speech_prob": 2.4682904040673748e-05}, {"id": 367, "seek": 172052, "start": 1735.12, "end": 1737.44, "text": " So I think this is really interesting, right?", "tokens": [51094, 407, 286, 519, 341, 307, 534, 1880, 11, 558, 30, 51210], "temperature": 0.0, "avg_logprob": -0.23026897931339765, "compression_ratio": 1.5265486725663717, "no_speech_prob": 2.4682904040673748e-05}, {"id": 368, "seek": 172052, "start": 1737.44, "end": 1743.6, "text": " Like these things which seem like they must be big and complicated, once we have this", "tokens": [51210, 1743, 613, 721, 597, 1643, 411, 436, 1633, 312, 955, 293, 6179, 11, 1564, 321, 362, 341, 51518], "temperature": 0.0, "avg_logprob": -0.23026897931339765, "compression_ratio": 1.5265486725663717, "no_speech_prob": 2.4682904040673748e-05}, {"id": 369, "seek": 172052, "start": 1743.6, "end": 1749.84, "text": " nice structure in place, you know, an SGD optimizer doesn't take much code at all.", "tokens": [51518, 1481, 3877, 294, 1081, 11, 291, 458, 11, 364, 34520, 35, 5028, 6545, 1177, 380, 747, 709, 3089, 412, 439, 13, 51830], "temperature": 0.0, "avg_logprob": -0.23026897931339765, "compression_ratio": 1.5265486725663717, "no_speech_prob": 2.4682904040673748e-05}, {"id": 370, "seek": 174984, "start": 1749.8799999999999, "end": 1753.56, "text": " And so it's all very transparent, simple, clear.", "tokens": [50366, 400, 370, 309, 311, 439, 588, 12737, 11, 2199, 11, 1850, 13, 50550], "temperature": 0.0, "avg_logprob": -0.25243087843352674, "compression_ratio": 1.6260504201680672, "no_speech_prob": 0.00010889684199355543}, {"id": 371, "seek": 174984, "start": 1753.56, "end": 1760.52, "text": " If you're having trouble using complex library code that you've found elsewhere, you know,", "tokens": [50550, 759, 291, 434, 1419, 5253, 1228, 3997, 6405, 3089, 300, 291, 600, 1352, 14517, 11, 291, 458, 11, 50898], "temperature": 0.0, "avg_logprob": -0.25243087843352674, "compression_ratio": 1.6260504201680672, "no_speech_prob": 0.00010889684199355543}, {"id": 372, "seek": 174984, "start": 1760.52, "end": 1764.9199999999998, "text": " this can be a really good approach, is to actually just go all the way back, remove", "tokens": [50898, 341, 393, 312, 257, 534, 665, 3109, 11, 307, 281, 767, 445, 352, 439, 264, 636, 646, 11, 4159, 51118], "temperature": 0.0, "avg_logprob": -0.25243087843352674, "compression_ratio": 1.6260504201680672, "no_speech_prob": 0.00010889684199355543}, {"id": 373, "seek": 174984, "start": 1764.9199999999998, "end": 1772.1, "text": " as many of these abstractions as you can, and like run everything by hand to see exactly", "tokens": [51118, 382, 867, 295, 613, 12649, 626, 382, 291, 393, 11, 293, 411, 1190, 1203, 538, 1011, 281, 536, 2293, 51477], "temperature": 0.0, "avg_logprob": -0.25243087843352674, "compression_ratio": 1.6260504201680672, "no_speech_prob": 0.00010889684199355543}, {"id": 374, "seek": 174984, "start": 1772.1, "end": 1773.1, "text": " what's going on.", "tokens": [51477, 437, 311, 516, 322, 13, 51527], "temperature": 0.0, "avg_logprob": -0.25243087843352674, "compression_ratio": 1.6260504201680672, "no_speech_prob": 0.00010889684199355543}, {"id": 375, "seek": 174984, "start": 1773.1, "end": 1777.76, "text": " It can be really freeing to see that you can do all this.", "tokens": [51527, 467, 393, 312, 534, 1737, 278, 281, 536, 300, 291, 393, 360, 439, 341, 13, 51760], "temperature": 0.0, "avg_logprob": -0.25243087843352674, "compression_ratio": 1.6260504201680672, "no_speech_prob": 0.00010889684199355543}, {"id": 376, "seek": 177776, "start": 1777.76, "end": 1785.44, "text": " Anyways, since PyTorch has this for us, in torch.optim, it's got a optim.sgd, and just", "tokens": [50364, 15585, 11, 1670, 9953, 51, 284, 339, 575, 341, 337, 505, 11, 294, 27822, 13, 5747, 332, 11, 309, 311, 658, 257, 2427, 332, 13, 82, 70, 67, 11, 293, 445, 50748], "temperature": 0.0, "avg_logprob": -0.24496402740478515, "compression_ratio": 1.6146341463414635, "no_speech_prob": 2.19078083318891e-06}, {"id": 377, "seek": 177776, "start": 1785.44, "end": 1789.12, "text": " like our version, you pass in the parameters, and you pass in the learning rate, so you", "tokens": [50748, 411, 527, 3037, 11, 291, 1320, 294, 264, 9834, 11, 293, 291, 1320, 294, 264, 2539, 3314, 11, 370, 291, 50932], "temperature": 0.0, "avg_logprob": -0.24496402740478515, "compression_ratio": 1.6146341463414635, "no_speech_prob": 2.19078083318891e-06}, {"id": 378, "seek": 177776, "start": 1789.12, "end": 1791.6, "text": " really see it is just the same.", "tokens": [50932, 534, 536, 309, 307, 445, 264, 912, 13, 51056], "temperature": 0.0, "avg_logprob": -0.24496402740478515, "compression_ratio": 1.6146341463414635, "no_speech_prob": 2.19078083318891e-06}, {"id": 379, "seek": 177776, "start": 1791.6, "end": 1795.8, "text": " So let's define something called getModel.", "tokens": [51056, 407, 718, 311, 6964, 746, 1219, 483, 44, 41147, 13, 51266], "temperature": 0.0, "avg_logprob": -0.24496402740478515, "compression_ratio": 1.6146341463414635, "no_speech_prob": 2.19078083318891e-06}, {"id": 380, "seek": 177776, "start": 1795.8, "end": 1802.28, "text": " That's going to return the model, the sequential model, and the optimizer for it.", "tokens": [51266, 663, 311, 516, 281, 2736, 264, 2316, 11, 264, 42881, 2316, 11, 293, 264, 5028, 6545, 337, 309, 13, 51590], "temperature": 0.0, "avg_logprob": -0.24496402740478515, "compression_ratio": 1.6146341463414635, "no_speech_prob": 2.19078083318891e-06}, {"id": 381, "seek": 180228, "start": 1802.28, "end": 1808.3999999999999, "text": " So if we go model, opt equals getModel, and then we can call the loss function to see", "tokens": [50364, 407, 498, 321, 352, 2316, 11, 2427, 6915, 483, 44, 41147, 11, 293, 550, 321, 393, 818, 264, 4470, 2445, 281, 536, 50670], "temperature": 0.0, "avg_logprob": -0.24898584338201993, "compression_ratio": 1.546583850931677, "no_speech_prob": 4.757637361763045e-05}, {"id": 382, "seek": 180228, "start": 1808.3999999999999, "end": 1811.32, "text": " where it's starting.", "tokens": [50670, 689, 309, 311, 2891, 13, 50816], "temperature": 0.0, "avg_logprob": -0.24898584338201993, "compression_ratio": 1.546583850931677, "no_speech_prob": 4.757637361763045e-05}, {"id": 383, "seek": 180228, "start": 1811.32, "end": 1817.86, "text": " And so then we can write our training loop again.", "tokens": [50816, 400, 370, 550, 321, 393, 2464, 527, 3097, 6367, 797, 13, 51143], "temperature": 0.0, "avg_logprob": -0.24898584338201993, "compression_ratio": 1.546583850931677, "no_speech_prob": 4.757637361763045e-05}, {"id": 384, "seek": 180228, "start": 1817.86, "end": 1827.6399999999999, "text": " Go through each epoch, go through each starting point for our batches, grab the slice, slice", "tokens": [51143, 1037, 807, 1184, 30992, 339, 11, 352, 807, 1184, 2891, 935, 337, 527, 15245, 279, 11, 4444, 264, 13153, 11, 13153, 51632], "temperature": 0.0, "avg_logprob": -0.24898584338201993, "compression_ratio": 1.546583850931677, "no_speech_prob": 4.757637361763045e-05}, {"id": 385, "seek": 182764, "start": 1827.64, "end": 1833.2, "text": " into our X and Y in the training set, calculate our predictions, calculate our loss, do the", "tokens": [50364, 666, 527, 1783, 293, 398, 294, 264, 3097, 992, 11, 8873, 527, 21264, 11, 8873, 527, 4470, 11, 360, 264, 50642], "temperature": 0.0, "avg_logprob": -0.2733560900821864, "compression_ratio": 1.7079646017699115, "no_speech_prob": 0.0038844565860927105}, {"id": 386, "seek": 182764, "start": 1833.2, "end": 1837.8000000000002, "text": " backward pass, do the optimizer step, do the zero gradient, and print out how you're going", "tokens": [50642, 23897, 1320, 11, 360, 264, 5028, 6545, 1823, 11, 360, 264, 4018, 16235, 11, 293, 4482, 484, 577, 291, 434, 516, 50872], "temperature": 0.0, "avg_logprob": -0.2733560900821864, "compression_ratio": 1.7079646017699115, "no_speech_prob": 0.0038844565860927105}, {"id": 387, "seek": 182764, "start": 1837.8000000000002, "end": 1840.0800000000002, "text": " at the end of each one.", "tokens": [50872, 412, 264, 917, 295, 1184, 472, 13, 50986], "temperature": 0.0, "avg_logprob": -0.2733560900821864, "compression_ratio": 1.7079646017699115, "no_speech_prob": 0.0038844565860927105}, {"id": 388, "seek": 182764, "start": 1840.0800000000002, "end": 1842.24, "text": " And there we go.", "tokens": [50986, 400, 456, 321, 352, 13, 51094], "temperature": 0.0, "avg_logprob": -0.2733560900821864, "compression_ratio": 1.7079646017699115, "no_speech_prob": 0.0038844565860927105}, {"id": 389, "seek": 182764, "start": 1842.24, "end": 1846.4, "text": " All right, so let's keep making this simpler.", "tokens": [51094, 1057, 558, 11, 370, 718, 311, 1066, 1455, 341, 18587, 13, 51302], "temperature": 0.0, "avg_logprob": -0.2733560900821864, "compression_ratio": 1.7079646017699115, "no_speech_prob": 0.0038844565860927105}, {"id": 390, "seek": 182764, "start": 1846.4, "end": 1848.88, "text": " There's still too much code.", "tokens": [51302, 821, 311, 920, 886, 709, 3089, 13, 51426], "temperature": 0.0, "avg_logprob": -0.2733560900821864, "compression_ratio": 1.7079646017699115, "no_speech_prob": 0.0038844565860927105}, {"id": 391, "seek": 182764, "start": 1848.88, "end": 1855.88, "text": " So one thing we could do is we could replace these lines of code with one line of code,", "tokens": [51426, 407, 472, 551, 321, 727, 360, 307, 321, 727, 7406, 613, 3876, 295, 3089, 365, 472, 1622, 295, 3089, 11, 51776], "temperature": 0.0, "avg_logprob": -0.2733560900821864, "compression_ratio": 1.7079646017699115, "no_speech_prob": 0.0038844565860927105}, {"id": 392, "seek": 185588, "start": 1855.88, "end": 1859.3200000000002, "text": " by using something we'll call the dataset class.", "tokens": [50364, 538, 1228, 746, 321, 603, 818, 264, 28872, 1508, 13, 50536], "temperature": 0.0, "avg_logprob": -0.25907496109749506, "compression_ratio": 1.912621359223301, "no_speech_prob": 2.46828803938115e-05}, {"id": 393, "seek": 185588, "start": 1859.3200000000002, "end": 1863.8400000000001, "text": " So the dataset class is just something that we're going to pass in our independent and", "tokens": [50536, 407, 264, 28872, 1508, 307, 445, 746, 300, 321, 434, 516, 281, 1320, 294, 527, 6695, 293, 50762], "temperature": 0.0, "avg_logprob": -0.25907496109749506, "compression_ratio": 1.912621359223301, "no_speech_prob": 2.46828803938115e-05}, {"id": 394, "seek": 185588, "start": 1863.8400000000001, "end": 1865.88, "text": " dependent variable.", "tokens": [50762, 12334, 7006, 13, 50864], "temperature": 0.0, "avg_logprob": -0.25907496109749506, "compression_ratio": 1.912621359223301, "no_speech_prob": 2.46828803938115e-05}, {"id": 395, "seek": 185588, "start": 1865.88, "end": 1871.64, "text": " We'll store them away as self.x and self.y.", "tokens": [50864, 492, 603, 3531, 552, 1314, 382, 2698, 13, 87, 293, 2698, 13, 88, 13, 51152], "temperature": 0.0, "avg_logprob": -0.25907496109749506, "compression_ratio": 1.912621359223301, "no_speech_prob": 2.46828803938115e-05}, {"id": 396, "seek": 185588, "start": 1871.64, "end": 1876.68, "text": " We'll have something, so if you define dunder len, then that's the thing that allows the", "tokens": [51152, 492, 603, 362, 746, 11, 370, 498, 291, 6964, 274, 6617, 40116, 11, 550, 300, 311, 264, 551, 300, 4045, 264, 51404], "temperature": 0.0, "avg_logprob": -0.25907496109749506, "compression_ratio": 1.912621359223301, "no_speech_prob": 2.46828803938115e-05}, {"id": 397, "seek": 185588, "start": 1876.68, "end": 1878.42, "text": " len function to work.", "tokens": [51404, 40116, 2445, 281, 589, 13, 51491], "temperature": 0.0, "avg_logprob": -0.25907496109749506, "compression_ratio": 1.912621359223301, "no_speech_prob": 2.46828803938115e-05}, {"id": 398, "seek": 185588, "start": 1878.42, "end": 1882.7600000000002, "text": " So the length of the data set will just be the length of the independent variables.", "tokens": [51491, 407, 264, 4641, 295, 264, 1412, 992, 486, 445, 312, 264, 4641, 295, 264, 6695, 9102, 13, 51708], "temperature": 0.0, "avg_logprob": -0.25907496109749506, "compression_ratio": 1.912621359223301, "no_speech_prob": 2.46828803938115e-05}, {"id": 399, "seek": 188276, "start": 1882.76, "end": 1887.44, "text": " And then dunder get item is the thing that will be called automatically any time you", "tokens": [50364, 400, 550, 274, 6617, 483, 3174, 307, 264, 551, 300, 486, 312, 1219, 6772, 604, 565, 291, 50598], "temperature": 0.0, "avg_logprob": -0.24606669714691443, "compression_ratio": 1.6544715447154472, "no_speech_prob": 0.0015011380892246962}, {"id": 400, "seek": 188276, "start": 1887.44, "end": 1890.52, "text": " use square brackets in Python.", "tokens": [50598, 764, 3732, 26179, 294, 15329, 13, 50752], "temperature": 0.0, "avg_logprob": -0.24606669714691443, "compression_ratio": 1.6544715447154472, "no_speech_prob": 0.0015011380892246962}, {"id": 401, "seek": 188276, "start": 1890.52, "end": 1895.22, "text": " So that's just going to call this function passing in the indices that you want.", "tokens": [50752, 407, 300, 311, 445, 516, 281, 818, 341, 2445, 8437, 294, 264, 43840, 300, 291, 528, 13, 50987], "temperature": 0.0, "avg_logprob": -0.24606669714691443, "compression_ratio": 1.6544715447154472, "no_speech_prob": 0.0015011380892246962}, {"id": 402, "seek": 188276, "start": 1895.22, "end": 1901.7, "text": " So when we grab some items from our data set, we're going to return a tuple of the X values", "tokens": [50987, 407, 562, 321, 4444, 512, 4754, 490, 527, 1412, 992, 11, 321, 434, 516, 281, 2736, 257, 2604, 781, 295, 264, 1783, 4190, 51311], "temperature": 0.0, "avg_logprob": -0.24606669714691443, "compression_ratio": 1.6544715447154472, "no_speech_prob": 0.0015011380892246962}, {"id": 403, "seek": 188276, "start": 1901.7, "end": 1903.48, "text": " and the Y values.", "tokens": [51311, 293, 264, 398, 4190, 13, 51400], "temperature": 0.0, "avg_logprob": -0.24606669714691443, "compression_ratio": 1.6544715447154472, "no_speech_prob": 0.0015011380892246962}, {"id": 404, "seek": 188276, "start": 1903.48, "end": 1906.96, "text": " So then we'll be able to do this.", "tokens": [51400, 407, 550, 321, 603, 312, 1075, 281, 360, 341, 13, 51574], "temperature": 0.0, "avg_logprob": -0.24606669714691443, "compression_ratio": 1.6544715447154472, "no_speech_prob": 0.0015011380892246962}, {"id": 405, "seek": 188276, "start": 1906.96, "end": 1912.2, "text": " So let's create a data set using this tiny little tree line class.", "tokens": [51574, 407, 718, 311, 1884, 257, 1412, 992, 1228, 341, 5870, 707, 4230, 1622, 1508, 13, 51836], "temperature": 0.0, "avg_logprob": -0.24606669714691443, "compression_ratio": 1.6544715447154472, "no_speech_prob": 0.0015011380892246962}, {"id": 406, "seek": 191220, "start": 1912.2, "end": 1916.52, "text": " It's going to be a data set containing the X and Y training, and they'll create another", "tokens": [50364, 467, 311, 516, 281, 312, 257, 1412, 992, 19273, 264, 1783, 293, 398, 3097, 11, 293, 436, 603, 1884, 1071, 50580], "temperature": 0.0, "avg_logprob": -0.28091117313929964, "compression_ratio": 1.8387096774193548, "no_speech_prob": 0.00017400515207555145}, {"id": 407, "seek": 191220, "start": 1916.52, "end": 1919.22, "text": " data set containing the X and Y valid.", "tokens": [50580, 1412, 992, 19273, 264, 1783, 293, 398, 7363, 13, 50715], "temperature": 0.0, "avg_logprob": -0.28091117313929964, "compression_ratio": 1.8387096774193548, "no_speech_prob": 0.00017400515207555145}, {"id": 408, "seek": 191220, "start": 1919.22, "end": 1925.0800000000002, "text": " And those two data sets will call train ds and valid ds.", "tokens": [50715, 400, 729, 732, 1412, 6352, 486, 818, 3847, 274, 82, 293, 7363, 274, 82, 13, 51008], "temperature": 0.0, "avg_logprob": -0.28091117313929964, "compression_ratio": 1.8387096774193548, "no_speech_prob": 0.00017400515207555145}, {"id": 409, "seek": 191220, "start": 1925.0800000000002, "end": 1929.16, "text": " So let's check the length of those data sets should be the same as the length of the Xs,", "tokens": [51008, 407, 718, 311, 1520, 264, 4641, 295, 729, 1412, 6352, 820, 312, 264, 912, 382, 264, 4641, 295, 264, 1783, 82, 11, 51212], "temperature": 0.0, "avg_logprob": -0.28091117313929964, "compression_ratio": 1.8387096774193548, "no_speech_prob": 0.00017400515207555145}, {"id": 410, "seek": 191220, "start": 1929.16, "end": 1933.2, "text": " and they are.", "tokens": [51212, 293, 436, 366, 13, 51414], "temperature": 0.0, "avg_logprob": -0.28091117313929964, "compression_ratio": 1.8387096774193548, "no_speech_prob": 0.00017400515207555145}, {"id": 411, "seek": 191220, "start": 1933.2, "end": 1936.28, "text": " And so now we can do exactly what we hoped we could do.", "tokens": [51414, 400, 370, 586, 321, 393, 360, 2293, 437, 321, 19737, 321, 727, 360, 13, 51568], "temperature": 0.0, "avg_logprob": -0.28091117313929964, "compression_ratio": 1.8387096774193548, "no_speech_prob": 0.00017400515207555145}, {"id": 412, "seek": 193628, "start": 1936.28, "end": 1944.12, "text": " We can say xb, yb equals train ds and pass in some slice.", "tokens": [50364, 492, 393, 584, 2031, 65, 11, 288, 65, 6915, 3847, 274, 82, 293, 1320, 294, 512, 13153, 13, 50756], "temperature": 0.0, "avg_logprob": -0.27005720138549805, "compression_ratio": 1.5206185567010309, "no_speech_prob": 0.0012448136694729328}, {"id": 413, "seek": 193628, "start": 1944.12, "end": 1951.24, "text": " So that's going to give us back our, check the shapes are correct, it should be 5 by", "tokens": [50756, 407, 300, 311, 516, 281, 976, 505, 646, 527, 11, 1520, 264, 10854, 366, 3006, 11, 309, 820, 312, 1025, 538, 51112], "temperature": 0.0, "avg_logprob": -0.27005720138549805, "compression_ratio": 1.5206185567010309, "no_speech_prob": 0.0012448136694729328}, {"id": 414, "seek": 193628, "start": 1951.24, "end": 1958.3999999999999, "text": " 28 by 28, 5 by 28 times 28, and the Ys should just be 5.", "tokens": [51112, 7562, 538, 7562, 11, 1025, 538, 7562, 1413, 7562, 11, 293, 264, 398, 82, 820, 445, 312, 1025, 13, 51470], "temperature": 0.0, "avg_logprob": -0.27005720138549805, "compression_ratio": 1.5206185567010309, "no_speech_prob": 0.0012448136694729328}, {"id": 415, "seek": 193628, "start": 1958.3999999999999, "end": 1962.04, "text": " And so here they are, the Xs and the Ys.", "tokens": [51470, 400, 370, 510, 436, 366, 11, 264, 1783, 82, 293, 264, 398, 82, 13, 51652], "temperature": 0.0, "avg_logprob": -0.27005720138549805, "compression_ratio": 1.5206185567010309, "no_speech_prob": 0.0012448136694729328}, {"id": 416, "seek": 193628, "start": 1962.04, "end": 1963.04, "text": " So that's nice.", "tokens": [51652, 407, 300, 311, 1481, 13, 51702], "temperature": 0.0, "avg_logprob": -0.27005720138549805, "compression_ratio": 1.5206185567010309, "no_speech_prob": 0.0012448136694729328}, {"id": 417, "seek": 193628, "start": 1963.04, "end": 1964.72, "text": " We've created a data set from scratch.", "tokens": [51702, 492, 600, 2942, 257, 1412, 992, 490, 8459, 13, 51786], "temperature": 0.0, "avg_logprob": -0.27005720138549805, "compression_ratio": 1.5206185567010309, "no_speech_prob": 0.0012448136694729328}, {"id": 418, "seek": 196472, "start": 1965.16, "end": 1968.48, "text": " It's not complicated at all.", "tokens": [50386, 467, 311, 406, 6179, 412, 439, 13, 50552], "temperature": 0.0, "avg_logprob": -0.2275025152391003, "compression_ratio": 1.6169354838709677, "no_speech_prob": 0.011868834495544434}, {"id": 419, "seek": 196472, "start": 1968.48, "end": 1973.6200000000001, "text": " And if you look at the actual PyTorch source code, this is basically all data sets do.", "tokens": [50552, 400, 498, 291, 574, 412, 264, 3539, 9953, 51, 284, 339, 4009, 3089, 11, 341, 307, 1936, 439, 1412, 6352, 360, 13, 50809], "temperature": 0.0, "avg_logprob": -0.2275025152391003, "compression_ratio": 1.6169354838709677, "no_speech_prob": 0.011868834495544434}, {"id": 420, "seek": 196472, "start": 1973.6200000000001, "end": 1974.6200000000001, "text": " So let's try it.", "tokens": [50809, 407, 718, 311, 853, 309, 13, 50859], "temperature": 0.0, "avg_logprob": -0.2275025152391003, "compression_ratio": 1.6169354838709677, "no_speech_prob": 0.011868834495544434}, {"id": 421, "seek": 196472, "start": 1974.6200000000001, "end": 1976.04, "text": " We call getModel.", "tokens": [50859, 492, 818, 483, 44, 41147, 13, 50930], "temperature": 0.0, "avg_logprob": -0.2275025152391003, "compression_ratio": 1.6169354838709677, "no_speech_prob": 0.011868834495544434}, {"id": 422, "seek": 196472, "start": 1976.04, "end": 1981.1200000000001, "text": " And so now we've replaced our data set line with this one.", "tokens": [50930, 400, 370, 586, 321, 600, 10772, 527, 1412, 992, 1622, 365, 341, 472, 13, 51184], "temperature": 0.0, "avg_logprob": -0.2275025152391003, "compression_ratio": 1.6169354838709677, "no_speech_prob": 0.011868834495544434}, {"id": 423, "seek": 196472, "start": 1981.1200000000001, "end": 1983.32, "text": " And as per usual, it still runs.", "tokens": [51184, 400, 382, 680, 7713, 11, 309, 920, 6676, 13, 51294], "temperature": 0.0, "avg_logprob": -0.2275025152391003, "compression_ratio": 1.6169354838709677, "no_speech_prob": 0.011868834495544434}, {"id": 424, "seek": 196472, "start": 1983.32, "end": 1990.16, "text": " And so this is what I do when I'm writing code, is I try to like always make sure that", "tokens": [51294, 400, 370, 341, 307, 437, 286, 360, 562, 286, 478, 3579, 3089, 11, 307, 286, 853, 281, 411, 1009, 652, 988, 300, 51636], "temperature": 0.0, "avg_logprob": -0.2275025152391003, "compression_ratio": 1.6169354838709677, "no_speech_prob": 0.011868834495544434}, {"id": 425, "seek": 196472, "start": 1990.16, "end": 1992.64, "text": " my starting code works as I refactor.", "tokens": [51636, 452, 2891, 3089, 1985, 382, 286, 1895, 15104, 13, 51760], "temperature": 0.0, "avg_logprob": -0.2275025152391003, "compression_ratio": 1.6169354838709677, "no_speech_prob": 0.011868834495544434}, {"id": 426, "seek": 196472, "start": 1992.64, "end": 1994.56, "text": " And so you can see all the steps.", "tokens": [51760, 400, 370, 291, 393, 536, 439, 264, 4439, 13, 51856], "temperature": 0.0, "avg_logprob": -0.2275025152391003, "compression_ratio": 1.6169354838709677, "no_speech_prob": 0.011868834495544434}, {"id": 427, "seek": 199456, "start": 1995.3999999999999, "end": 1998.1599999999999, "text": " And so somebody reading my code can then see exactly like, why am I building everything", "tokens": [50406, 400, 370, 2618, 3760, 452, 3089, 393, 550, 536, 2293, 411, 11, 983, 669, 286, 2390, 1203, 50544], "temperature": 0.0, "avg_logprob": -0.23495255377059593, "compression_ratio": 1.7318840579710144, "no_speech_prob": 1.4285501492850017e-05}, {"id": 428, "seek": 199456, "start": 1998.1599999999999, "end": 1999.1599999999999, "text": " I'm building?", "tokens": [50544, 286, 478, 2390, 30, 50594], "temperature": 0.0, "avg_logprob": -0.23495255377059593, "compression_ratio": 1.7318840579710144, "no_speech_prob": 1.4285501492850017e-05}, {"id": 429, "seek": 199456, "start": 1999.1599999999999, "end": 2000.1599999999999, "text": " How does it all fit in?", "tokens": [50594, 1012, 775, 309, 439, 3318, 294, 30, 50644], "temperature": 0.0, "avg_logprob": -0.23495255377059593, "compression_ratio": 1.7318840579710144, "no_speech_prob": 1.4285501492850017e-05}, {"id": 430, "seek": 199456, "start": 2000.1599999999999, "end": 2002.0, "text": " See that it still works.", "tokens": [50644, 3008, 300, 309, 920, 1985, 13, 50736], "temperature": 0.0, "avg_logprob": -0.23495255377059593, "compression_ratio": 1.7318840579710144, "no_speech_prob": 1.4285501492850017e-05}, {"id": 431, "seek": 199456, "start": 2002.0, "end": 2003.56, "text": " And I can also keep it clear in my own head.", "tokens": [50736, 400, 286, 393, 611, 1066, 309, 1850, 294, 452, 1065, 1378, 13, 50814], "temperature": 0.0, "avg_logprob": -0.23495255377059593, "compression_ratio": 1.7318840579710144, "no_speech_prob": 1.4285501492850017e-05}, {"id": 432, "seek": 199456, "start": 2003.56, "end": 2009.32, "text": " So I think this is a really nice way of implementing libraries as well.", "tokens": [50814, 407, 286, 519, 341, 307, 257, 534, 1481, 636, 295, 18114, 15148, 382, 731, 13, 51102], "temperature": 0.0, "avg_logprob": -0.23495255377059593, "compression_ratio": 1.7318840579710144, "no_speech_prob": 1.4285501492850017e-05}, {"id": 433, "seek": 199456, "start": 2009.32, "end": 2010.72, "text": " All right.", "tokens": [51102, 1057, 558, 13, 51172], "temperature": 0.0, "avg_logprob": -0.23495255377059593, "compression_ratio": 1.7318840579710144, "no_speech_prob": 1.4285501492850017e-05}, {"id": 434, "seek": 199456, "start": 2010.72, "end": 2017.72, "text": " So now we're going to replace these two lines of code with this one line of code.", "tokens": [51172, 407, 586, 321, 434, 516, 281, 7406, 613, 732, 3876, 295, 3089, 365, 341, 472, 1622, 295, 3089, 13, 51522], "temperature": 0.0, "avg_logprob": -0.23495255377059593, "compression_ratio": 1.7318840579710144, "no_speech_prob": 1.4285501492850017e-05}, {"id": 435, "seek": 199456, "start": 2017.72, "end": 2019.8, "text": " So we're going to create something called a data loader.", "tokens": [51522, 407, 321, 434, 516, 281, 1884, 746, 1219, 257, 1412, 3677, 260, 13, 51626], "temperature": 0.0, "avg_logprob": -0.23495255377059593, "compression_ratio": 1.7318840579710144, "no_speech_prob": 1.4285501492850017e-05}, {"id": 436, "seek": 199456, "start": 2019.8, "end": 2023.84, "text": " And a data loader is something that's just going to do this.", "tokens": [51626, 400, 257, 1412, 3677, 260, 307, 746, 300, 311, 445, 516, 281, 360, 341, 13, 51828], "temperature": 0.0, "avg_logprob": -0.23495255377059593, "compression_ratio": 1.7318840579710144, "no_speech_prob": 1.4285501492850017e-05}, {"id": 437, "seek": 202384, "start": 2024.12, "end": 2026.52, "text": " So we need to create an iterator.", "tokens": [50378, 407, 321, 643, 281, 1884, 364, 17138, 1639, 13, 50498], "temperature": 0.0, "avg_logprob": -0.22101896862651027, "compression_ratio": 1.6822916666666667, "no_speech_prob": 2.392347414570395e-05}, {"id": 438, "seek": 202384, "start": 2026.52, "end": 2031.24, "text": " So an iterator is a class that has a dunder iter method.", "tokens": [50498, 407, 364, 17138, 1639, 307, 257, 1508, 300, 575, 257, 274, 6617, 17138, 3170, 13, 50734], "temperature": 0.0, "avg_logprob": -0.22101896862651027, "compression_ratio": 1.6822916666666667, "no_speech_prob": 2.392347414570395e-05}, {"id": 439, "seek": 202384, "start": 2031.24, "end": 2040.0, "text": " When you say for in, in Python, behind the scenes, it's actually calling dunder iter", "tokens": [50734, 1133, 291, 584, 337, 294, 11, 294, 15329, 11, 2261, 264, 8026, 11, 309, 311, 767, 5141, 274, 6617, 17138, 51172], "temperature": 0.0, "avg_logprob": -0.22101896862651027, "compression_ratio": 1.6822916666666667, "no_speech_prob": 2.392347414570395e-05}, {"id": 440, "seek": 202384, "start": 2040.0, "end": 2046.12, "text": " to get a special object, which it can then loop through using yield.", "tokens": [51172, 281, 483, 257, 2121, 2657, 11, 597, 309, 393, 550, 6367, 807, 1228, 11257, 13, 51478], "temperature": 0.0, "avg_logprob": -0.22101896862651027, "compression_ratio": 1.6822916666666667, "no_speech_prob": 2.392347414570395e-05}, {"id": 441, "seek": 202384, "start": 2046.12, "end": 2051.36, "text": " So it's basically getting this thing that you can iterate through using yield.", "tokens": [51478, 407, 309, 311, 1936, 1242, 341, 551, 300, 291, 393, 44497, 807, 1228, 11257, 13, 51740], "temperature": 0.0, "avg_logprob": -0.22101896862651027, "compression_ratio": 1.6822916666666667, "no_speech_prob": 2.392347414570395e-05}, {"id": 442, "seek": 205136, "start": 2051.36, "end": 2056.48, "text": " So a data loader is something that's going to have a data set and a batch size, because", "tokens": [50364, 407, 257, 1412, 3677, 260, 307, 746, 300, 311, 516, 281, 362, 257, 1412, 992, 293, 257, 15245, 2744, 11, 570, 50620], "temperature": 0.0, "avg_logprob": -0.22448568945532446, "compression_ratio": 1.8055555555555556, "no_speech_prob": 2.1782640033052303e-05}, {"id": 443, "seek": 205136, "start": 2056.48, "end": 2062.6, "text": " we're going to go through the batches and grab one batch at a time.", "tokens": [50620, 321, 434, 516, 281, 352, 807, 264, 15245, 279, 293, 4444, 472, 15245, 412, 257, 565, 13, 50926], "temperature": 0.0, "avg_logprob": -0.22448568945532446, "compression_ratio": 1.8055555555555556, "no_speech_prob": 2.1782640033052303e-05}, {"id": 444, "seek": 205136, "start": 2062.6, "end": 2065.28, "text": " So we have to store away the data set and the batch size.", "tokens": [50926, 407, 321, 362, 281, 3531, 1314, 264, 1412, 992, 293, 264, 15245, 2744, 13, 51060], "temperature": 0.0, "avg_logprob": -0.22448568945532446, "compression_ratio": 1.8055555555555556, "no_speech_prob": 2.1782640033052303e-05}, {"id": 445, "seek": 205136, "start": 2065.28, "end": 2069.32, "text": " And so when you, when we call the for loop, it's going to call dunder iter, we're going", "tokens": [51060, 400, 370, 562, 291, 11, 562, 321, 818, 264, 337, 6367, 11, 309, 311, 516, 281, 818, 274, 6617, 17138, 11, 321, 434, 516, 51262], "temperature": 0.0, "avg_logprob": -0.22448568945532446, "compression_ratio": 1.8055555555555556, "no_speech_prob": 2.1782640033052303e-05}, {"id": 446, "seek": 205136, "start": 2069.32, "end": 2071.6400000000003, "text": " to want to do exactly what we saw before.", "tokens": [51262, 281, 528, 281, 360, 2293, 437, 321, 1866, 949, 13, 51378], "temperature": 0.0, "avg_logprob": -0.22448568945532446, "compression_ratio": 1.8055555555555556, "no_speech_prob": 2.1782640033052303e-05}, {"id": 447, "seek": 205136, "start": 2071.6400000000003, "end": 2076.5, "text": " Go through the range, just like we did before.", "tokens": [51378, 1037, 807, 264, 3613, 11, 445, 411, 321, 630, 949, 13, 51621], "temperature": 0.0, "avg_logprob": -0.22448568945532446, "compression_ratio": 1.8055555555555556, "no_speech_prob": 2.1782640033052303e-05}, {"id": 448, "seek": 207650, "start": 2076.5, "end": 2081.46, "text": " And then yield that bit of the data set.", "tokens": [50364, 400, 550, 11257, 300, 857, 295, 264, 1412, 992, 13, 50612], "temperature": 0.0, "avg_logprob": -0.20054180415596548, "compression_ratio": 1.8558558558558558, "no_speech_prob": 0.02716815285384655}, {"id": 449, "seek": 207650, "start": 2081.46, "end": 2082.82, "text": " And that's all.", "tokens": [50612, 400, 300, 311, 439, 13, 50680], "temperature": 0.0, "avg_logprob": -0.20054180415596548, "compression_ratio": 1.8558558558558558, "no_speech_prob": 0.02716815285384655}, {"id": 450, "seek": 207650, "start": 2082.82, "end": 2084.44, "text": " So that's a data loader.", "tokens": [50680, 407, 300, 311, 257, 1412, 3677, 260, 13, 50761], "temperature": 0.0, "avg_logprob": -0.20054180415596548, "compression_ratio": 1.8558558558558558, "no_speech_prob": 0.02716815285384655}, {"id": 451, "seek": 207650, "start": 2084.44, "end": 2088.46, "text": " So we can now create a train data loader and a valid data loader from our train data set", "tokens": [50761, 407, 321, 393, 586, 1884, 257, 3847, 1412, 3677, 260, 293, 257, 7363, 1412, 3677, 260, 490, 527, 3847, 1412, 992, 50962], "temperature": 0.0, "avg_logprob": -0.20054180415596548, "compression_ratio": 1.8558558558558558, "no_speech_prob": 0.02716815285384655}, {"id": 452, "seek": 207650, "start": 2088.46, "end": 2091.12, "text": " and valid data set.", "tokens": [50962, 293, 7363, 1412, 992, 13, 51095], "temperature": 0.0, "avg_logprob": -0.20054180415596548, "compression_ratio": 1.8558558558558558, "no_speech_prob": 0.02716815285384655}, {"id": 453, "seek": 207650, "start": 2091.12, "end": 2097.7, "text": " And so now we can, if you remember the way you can get one thing out of an iterator,", "tokens": [51095, 400, 370, 586, 321, 393, 11, 498, 291, 1604, 264, 636, 291, 393, 483, 472, 551, 484, 295, 364, 17138, 1639, 11, 51424], "temperature": 0.0, "avg_logprob": -0.20054180415596548, "compression_ratio": 1.8558558558558558, "no_speech_prob": 0.02716815285384655}, {"id": 454, "seek": 207650, "start": 2097.7, "end": 2101.26, "text": " so you don't need to use a for loop, you can just say iter.", "tokens": [51424, 370, 291, 500, 380, 643, 281, 764, 257, 337, 6367, 11, 291, 393, 445, 584, 17138, 13, 51602], "temperature": 0.0, "avg_logprob": -0.20054180415596548, "compression_ratio": 1.8558558558558558, "no_speech_prob": 0.02716815285384655}, {"id": 455, "seek": 207650, "start": 2101.26, "end": 2103.54, "text": " And that will also call dunder iter.", "tokens": [51602, 400, 300, 486, 611, 818, 274, 6617, 17138, 13, 51716], "temperature": 0.0, "avg_logprob": -0.20054180415596548, "compression_ratio": 1.8558558558558558, "no_speech_prob": 0.02716815285384655}, {"id": 456, "seek": 207650, "start": 2103.54, "end": 2105.9, "text": " Next we'll just grab one value from it.", "tokens": [51716, 3087, 321, 603, 445, 4444, 472, 2158, 490, 309, 13, 51834], "temperature": 0.0, "avg_logprob": -0.20054180415596548, "compression_ratio": 1.8558558558558558, "no_speech_prob": 0.02716815285384655}, {"id": 457, "seek": 210590, "start": 2106.3, "end": 2111.14, "text": " So here we will run this, and you can see we've now just confirmed.", "tokens": [50384, 407, 510, 321, 486, 1190, 341, 11, 293, 291, 393, 536, 321, 600, 586, 445, 11341, 13, 50626], "temperature": 0.0, "avg_logprob": -0.2689404904263691, "compression_ratio": 1.5071090047393365, "no_speech_prob": 8.139689271047246e-06}, {"id": 458, "seek": 210590, "start": 2111.14, "end": 2118.78, "text": " XB is a 50 by 784, and YB, there it is.", "tokens": [50626, 1783, 33, 307, 257, 2625, 538, 1614, 25494, 11, 293, 398, 33, 11, 456, 309, 307, 13, 51008], "temperature": 0.0, "avg_logprob": -0.2689404904263691, "compression_ratio": 1.5071090047393365, "no_speech_prob": 8.139689271047246e-06}, {"id": 459, "seek": 210590, "start": 2118.78, "end": 2120.42, "text": " And then we can check what it looks like.", "tokens": [51008, 400, 550, 321, 393, 1520, 437, 309, 1542, 411, 13, 51090], "temperature": 0.0, "avg_logprob": -0.2689404904263691, "compression_ratio": 1.5071090047393365, "no_speech_prob": 8.139689271047246e-06}, {"id": 460, "seek": 210590, "start": 2120.42, "end": 2127.44, "text": " So let's grab the first element of our X batch, make it 28 by 28.", "tokens": [51090, 407, 718, 311, 4444, 264, 700, 4478, 295, 527, 1783, 15245, 11, 652, 309, 7562, 538, 7562, 13, 51441], "temperature": 0.0, "avg_logprob": -0.2689404904263691, "compression_ratio": 1.5071090047393365, "no_speech_prob": 8.139689271047246e-06}, {"id": 461, "seek": 210590, "start": 2127.44, "end": 2130.26, "text": " And there it is.", "tokens": [51441, 400, 456, 309, 307, 13, 51582], "temperature": 0.0, "avg_logprob": -0.2689404904263691, "compression_ratio": 1.5071090047393365, "no_speech_prob": 8.139689271047246e-06}, {"id": 462, "seek": 210590, "start": 2130.26, "end": 2134.2000000000003, "text": " So now that we've got a data loader, again, we can grab our model and we can simplify", "tokens": [51582, 407, 586, 300, 321, 600, 658, 257, 1412, 3677, 260, 11, 797, 11, 321, 393, 4444, 527, 2316, 293, 321, 393, 20460, 51779], "temperature": 0.0, "avg_logprob": -0.2689404904263691, "compression_ratio": 1.5071090047393365, "no_speech_prob": 8.139689271047246e-06}, {"id": 463, "seek": 213420, "start": 2134.2, "end": 2139.56, "text": " our fit function to just go for XB, YB, and train deal.", "tokens": [50364, 527, 3318, 2445, 281, 445, 352, 337, 1783, 33, 11, 398, 33, 11, 293, 3847, 2028, 13, 50632], "temperature": 0.0, "avg_logprob": -0.29674969689320707, "compression_ratio": 1.6209677419354838, "no_speech_prob": 0.0015487548662349582}, {"id": 464, "seek": 213420, "start": 2139.56, "end": 2142.3999999999996, "text": " So this is getting nice and small, don't you think?", "tokens": [50632, 407, 341, 307, 1242, 1481, 293, 1359, 11, 500, 380, 291, 519, 30, 50774], "temperature": 0.0, "avg_logprob": -0.29674969689320707, "compression_ratio": 1.6209677419354838, "no_speech_prob": 0.0015487548662349582}, {"id": 465, "seek": 213420, "start": 2142.3999999999996, "end": 2144.72, "text": " And it still works the same way.", "tokens": [50774, 400, 309, 920, 1985, 264, 912, 636, 13, 50890], "temperature": 0.0, "avg_logprob": -0.29674969689320707, "compression_ratio": 1.6209677419354838, "no_speech_prob": 0.0015487548662349582}, {"id": 466, "seek": 213420, "start": 2144.72, "end": 2149.12, "text": " OK, so this is really cool.", "tokens": [50890, 2264, 11, 370, 341, 307, 534, 1627, 13, 51110], "temperature": 0.0, "avg_logprob": -0.29674969689320707, "compression_ratio": 1.6209677419354838, "no_speech_prob": 0.0015487548662349582}, {"id": 467, "seek": 213420, "start": 2149.12, "end": 2153.8999999999996, "text": " And now that it's nice and concise, we can start adding features to it.", "tokens": [51110, 400, 586, 300, 309, 311, 1481, 293, 44882, 11, 321, 393, 722, 5127, 4122, 281, 309, 13, 51349], "temperature": 0.0, "avg_logprob": -0.29674969689320707, "compression_ratio": 1.6209677419354838, "no_speech_prob": 0.0015487548662349582}, {"id": 468, "seek": 213420, "start": 2153.8999999999996, "end": 2159.48, "text": " So one feature I think we should add is that our training set, each time we go through", "tokens": [51349, 407, 472, 4111, 286, 519, 321, 820, 909, 307, 300, 527, 3097, 992, 11, 1184, 565, 321, 352, 807, 51628], "temperature": 0.0, "avg_logprob": -0.29674969689320707, "compression_ratio": 1.6209677419354838, "no_speech_prob": 0.0015487548662349582}, {"id": 469, "seek": 213420, "start": 2159.48, "end": 2161.08, "text": " it, it should be in a different order.", "tokens": [51628, 309, 11, 309, 820, 312, 294, 257, 819, 1668, 13, 51708], "temperature": 0.0, "avg_logprob": -0.29674969689320707, "compression_ratio": 1.6209677419354838, "no_speech_prob": 0.0015487548662349582}, {"id": 470, "seek": 213420, "start": 2161.08, "end": 2164.08, "text": " It should be randomized, the order.", "tokens": [51708, 467, 820, 312, 38513, 11, 264, 1668, 13, 51858], "temperature": 0.0, "avg_logprob": -0.29674969689320707, "compression_ratio": 1.6209677419354838, "no_speech_prob": 0.0015487548662349582}, {"id": 471, "seek": 216408, "start": 2164.96, "end": 2174.3199999999997, "text": " So instead of always just going through these indexes in order, we want some way to say", "tokens": [50408, 407, 2602, 295, 1009, 445, 516, 807, 613, 8186, 279, 294, 1668, 11, 321, 528, 512, 636, 281, 584, 50876], "temperature": 0.0, "avg_logprob": -0.24352906201336835, "compression_ratio": 1.5535714285714286, "no_speech_prob": 3.2377602110500447e-06}, {"id": 472, "seek": 216408, "start": 2174.3199999999997, "end": 2177.4, "text": " go use random indexes.", "tokens": [50876, 352, 764, 4974, 8186, 279, 13, 51030], "temperature": 0.0, "avg_logprob": -0.24352906201336835, "compression_ratio": 1.5535714285714286, "no_speech_prob": 3.2377602110500447e-06}, {"id": 473, "seek": 216408, "start": 2177.4, "end": 2181.3199999999997, "text": " So the way we can do that is create a class called Sampler.", "tokens": [51030, 407, 264, 636, 321, 393, 360, 300, 307, 1884, 257, 1508, 1219, 4832, 22732, 13, 51226], "temperature": 0.0, "avg_logprob": -0.24352906201336835, "compression_ratio": 1.5535714285714286, "no_speech_prob": 3.2377602110500447e-06}, {"id": 474, "seek": 216408, "start": 2181.3199999999997, "end": 2191.36, "text": " And what Sampler is going to do, I'll show you, is if we create a Sampler without shuffle,", "tokens": [51226, 400, 437, 4832, 22732, 307, 516, 281, 360, 11, 286, 603, 855, 291, 11, 307, 498, 321, 1884, 257, 4832, 22732, 1553, 39426, 11, 51728], "temperature": 0.0, "avg_logprob": -0.24352906201336835, "compression_ratio": 1.5535714285714286, "no_speech_prob": 3.2377602110500447e-06}, {"id": 475, "seek": 219136, "start": 2191.52, "end": 2198.52, "text": " without randomizing it, it's going to simply return all the numbers from 0 up to N, in", "tokens": [50372, 1553, 4974, 3319, 309, 11, 309, 311, 516, 281, 2935, 2736, 439, 264, 3547, 490, 1958, 493, 281, 426, 11, 294, 50722], "temperature": 0.0, "avg_logprob": -0.26755474163935733, "compression_ratio": 1.6272727272727272, "no_speech_prob": 0.00012931560922879726}, {"id": 476, "seek": 219136, "start": 2200.2000000000003, "end": 2201.8, "text": " order, and it'll be an iterator.", "tokens": [50806, 1668, 11, 293, 309, 603, 312, 364, 17138, 1639, 13, 50886], "temperature": 0.0, "avg_logprob": -0.26755474163935733, "compression_ratio": 1.6272727272727272, "no_speech_prob": 0.00012931560922879726}, {"id": 477, "seek": 219136, "start": 2201.8, "end": 2203.7000000000003, "text": " See this is done to iter.", "tokens": [50886, 3008, 341, 307, 1096, 281, 17138, 13, 50981], "temperature": 0.0, "avg_logprob": -0.26755474163935733, "compression_ratio": 1.6272727272727272, "no_speech_prob": 0.00012931560922879726}, {"id": 478, "seek": 219136, "start": 2203.7000000000003, "end": 2208.58, "text": " But if I do want it shuffled, then it will randomly shuffle them.", "tokens": [50981, 583, 498, 286, 360, 528, 309, 402, 33974, 11, 550, 309, 486, 16979, 39426, 552, 13, 51225], "temperature": 0.0, "avg_logprob": -0.26755474163935733, "compression_ratio": 1.6272727272727272, "no_speech_prob": 0.00012931560922879726}, {"id": 479, "seek": 219136, "start": 2208.58, "end": 2212.82, "text": " So here you can see I've created a Sampler without shuffle.", "tokens": [51225, 407, 510, 291, 393, 536, 286, 600, 2942, 257, 4832, 22732, 1553, 39426, 13, 51437], "temperature": 0.0, "avg_logprob": -0.26755474163935733, "compression_ratio": 1.6272727272727272, "no_speech_prob": 0.00012931560922879726}, {"id": 480, "seek": 219136, "start": 2212.82, "end": 2218.48, "text": " So if I then make an iterator from that, and print a few things from the iterator, you", "tokens": [51437, 407, 498, 286, 550, 652, 364, 17138, 1639, 490, 300, 11, 293, 4482, 257, 1326, 721, 490, 264, 17138, 1639, 11, 291, 51720], "temperature": 0.0, "avg_logprob": -0.26755474163935733, "compression_ratio": 1.6272727272727272, "no_speech_prob": 0.00012931560922879726}, {"id": 481, "seek": 221848, "start": 2218.48, "end": 2223.52, "text": " can see it's just printing out the indexes it's going to want.", "tokens": [50364, 393, 536, 309, 311, 445, 14699, 484, 264, 8186, 279, 309, 311, 516, 281, 528, 13, 50616], "temperature": 0.0, "avg_logprob": -0.26441418191661004, "compression_ratio": 1.7161016949152543, "no_speech_prob": 0.000561478198505938}, {"id": 482, "seek": 221848, "start": 2223.52, "end": 2227.56, "text": " Or I can do exactly the same thing as we learned earlier in the course using islice.", "tokens": [50616, 1610, 286, 393, 360, 2293, 264, 912, 551, 382, 321, 3264, 3071, 294, 264, 1164, 1228, 307, 75, 573, 13, 50818], "temperature": 0.0, "avg_logprob": -0.26441418191661004, "compression_ratio": 1.7161016949152543, "no_speech_prob": 0.000561478198505938}, {"id": 483, "seek": 221848, "start": 2227.56, "end": 2228.72, "text": " We can grab the first five.", "tokens": [50818, 492, 393, 4444, 264, 700, 1732, 13, 50876], "temperature": 0.0, "avg_logprob": -0.26441418191661004, "compression_ratio": 1.7161016949152543, "no_speech_prob": 0.000561478198505938}, {"id": 484, "seek": 221848, "start": 2228.72, "end": 2232.28, "text": " So here's the first five things from a Sampler when it's not shuffled.", "tokens": [50876, 407, 510, 311, 264, 700, 1732, 721, 490, 257, 4832, 22732, 562, 309, 311, 406, 402, 33974, 13, 51054], "temperature": 0.0, "avg_logprob": -0.26441418191661004, "compression_ratio": 1.7161016949152543, "no_speech_prob": 0.000561478198505938}, {"id": 485, "seek": 221848, "start": 2232.28, "end": 2235.64, "text": " So as you can see, these are just indexes.", "tokens": [51054, 407, 382, 291, 393, 536, 11, 613, 366, 445, 8186, 279, 13, 51222], "temperature": 0.0, "avg_logprob": -0.26441418191661004, "compression_ratio": 1.7161016949152543, "no_speech_prob": 0.000561478198505938}, {"id": 486, "seek": 221848, "start": 2235.64, "end": 2242.48, "text": " So we could add shuffle equals true, and now that's going to call random.shuffle, which", "tokens": [51222, 407, 321, 727, 909, 39426, 6915, 2074, 11, 293, 586, 300, 311, 516, 281, 818, 4974, 13, 2716, 21665, 11, 597, 51564], "temperature": 0.0, "avg_logprob": -0.26441418191661004, "compression_ratio": 1.7161016949152543, "no_speech_prob": 0.000561478198505938}, {"id": 487, "seek": 221848, "start": 2242.48, "end": 2244.56, "text": " just randomly permits them.", "tokens": [51564, 445, 16979, 30990, 552, 13, 51668], "temperature": 0.0, "avg_logprob": -0.26441418191661004, "compression_ratio": 1.7161016949152543, "no_speech_prob": 0.000561478198505938}, {"id": 488, "seek": 224456, "start": 2244.56, "end": 2252.2799999999997, "text": " And now if I do the same thing, I've got random indexes of my source data.", "tokens": [50364, 400, 586, 498, 286, 360, 264, 912, 551, 11, 286, 600, 658, 4974, 8186, 279, 295, 452, 4009, 1412, 13, 50750], "temperature": 0.0, "avg_logprob": -0.2597327743257795, "compression_ratio": 1.7008928571428572, "no_speech_prob": 0.0001823544007493183}, {"id": 489, "seek": 224456, "start": 2252.2799999999997, "end": 2253.92, "text": " So why is that useful?", "tokens": [50750, 407, 983, 307, 300, 4420, 30, 50832], "temperature": 0.0, "avg_logprob": -0.2597327743257795, "compression_ratio": 1.7008928571428572, "no_speech_prob": 0.0001823544007493183}, {"id": 490, "seek": 224456, "start": 2253.92, "end": 2259.04, "text": " Well what we could now do is create something called a batch Sampler.", "tokens": [50832, 1042, 437, 321, 727, 586, 360, 307, 1884, 746, 1219, 257, 15245, 4832, 22732, 13, 51088], "temperature": 0.0, "avg_logprob": -0.2597327743257795, "compression_ratio": 1.7008928571428572, "no_speech_prob": 0.0001823544007493183}, {"id": 491, "seek": 224456, "start": 2259.04, "end": 2263.36, "text": " And what the batch Sampler is going to do, is it's going to basically do this islice", "tokens": [51088, 400, 437, 264, 15245, 4832, 22732, 307, 516, 281, 360, 11, 307, 309, 311, 516, 281, 1936, 360, 341, 307, 75, 573, 51304], "temperature": 0.0, "avg_logprob": -0.2597327743257795, "compression_ratio": 1.7008928571428572, "no_speech_prob": 0.0001823544007493183}, {"id": 492, "seek": 224456, "start": 2263.36, "end": 2264.58, "text": " thing for us.", "tokens": [51304, 551, 337, 505, 13, 51365], "temperature": 0.0, "avg_logprob": -0.2597327743257795, "compression_ratio": 1.7008928571428572, "no_speech_prob": 0.0001823544007493183}, {"id": 493, "seek": 224456, "start": 2264.58, "end": 2268.7999999999997, "text": " So we're going to say, okay, pass in a Sampler, that's something that generates indices, and", "tokens": [51365, 407, 321, 434, 516, 281, 584, 11, 1392, 11, 1320, 294, 257, 4832, 22732, 11, 300, 311, 746, 300, 23815, 43840, 11, 293, 51576], "temperature": 0.0, "avg_logprob": -0.2597327743257795, "compression_ratio": 1.7008928571428572, "no_speech_prob": 0.0001823544007493183}, {"id": 494, "seek": 224456, "start": 2268.7999999999997, "end": 2271.24, "text": " pass in a batch size.", "tokens": [51576, 1320, 294, 257, 15245, 2744, 13, 51698], "temperature": 0.0, "avg_logprob": -0.2597327743257795, "compression_ratio": 1.7008928571428572, "no_speech_prob": 0.0001823544007493183}, {"id": 495, "seek": 227124, "start": 2271.24, "end": 2272.9199999999996, "text": " And remember we've looked at chunking before.", "tokens": [50364, 400, 1604, 321, 600, 2956, 412, 16635, 278, 949, 13, 50448], "temperature": 0.0, "avg_logprob": -0.2106197958123194, "compression_ratio": 1.4970414201183433, "no_speech_prob": 0.00014425981498789042}, {"id": 496, "seek": 227124, "start": 2272.9199999999996, "end": 2280.3999999999996, "text": " It's going to chunk that iterator by that batch size.", "tokens": [50448, 467, 311, 516, 281, 16635, 300, 17138, 1639, 538, 300, 15245, 2744, 13, 50822], "temperature": 0.0, "avg_logprob": -0.2106197958123194, "compression_ratio": 1.4970414201183433, "no_speech_prob": 0.00014425981498789042}, {"id": 497, "seek": 227124, "start": 2280.3999999999996, "end": 2290.4199999999996, "text": " And so if I now say, all right, please take our Sampler and create batches of four.", "tokens": [50822, 400, 370, 498, 286, 586, 584, 11, 439, 558, 11, 1767, 747, 527, 4832, 22732, 293, 1884, 15245, 279, 295, 1451, 13, 51323], "temperature": 0.0, "avg_logprob": -0.2106197958123194, "compression_ratio": 1.4970414201183433, "no_speech_prob": 0.00014425981498789042}, {"id": 498, "seek": 227124, "start": 2290.4199999999996, "end": 2295.9199999999996, "text": " As you can see here, it's creating batches of four indices at a time.", "tokens": [51323, 1018, 291, 393, 536, 510, 11, 309, 311, 4084, 15245, 279, 295, 1451, 43840, 412, 257, 565, 13, 51598], "temperature": 0.0, "avg_logprob": -0.2106197958123194, "compression_ratio": 1.4970414201183433, "no_speech_prob": 0.00014425981498789042}, {"id": 499, "seek": 229592, "start": 2295.92, "end": 2305.12, "text": " So rather than just looping through them in order, I can now loop through this batch", "tokens": [50364, 407, 2831, 813, 445, 6367, 278, 807, 552, 294, 1668, 11, 286, 393, 586, 6367, 807, 341, 15245, 50824], "temperature": 0.0, "avg_logprob": -0.2514404106140137, "compression_ratio": 1.4274193548387097, "no_speech_prob": 0.00022692990023642778}, {"id": 500, "seek": 229592, "start": 2305.12, "end": 2309.38, "text": " Sampler.", "tokens": [50824, 4832, 22732, 13, 51037], "temperature": 0.0, "avg_logprob": -0.2514404106140137, "compression_ratio": 1.4274193548387097, "no_speech_prob": 0.00022692990023642778}, {"id": 501, "seek": 229592, "start": 2309.38, "end": 2322.16, "text": " So we're going to change our data loader, so that now it's going to take some batch", "tokens": [51037, 407, 321, 434, 516, 281, 1319, 527, 1412, 3677, 260, 11, 370, 300, 586, 309, 311, 516, 281, 747, 512, 15245, 51676], "temperature": 0.0, "avg_logprob": -0.2514404106140137, "compression_ratio": 1.4274193548387097, "no_speech_prob": 0.00022692990023642778}, {"id": 502, "seek": 232216, "start": 2322.16, "end": 2326.06, "text": " Sampler, and it's going to loop through the batch Sampler.", "tokens": [50364, 4832, 22732, 11, 293, 309, 311, 516, 281, 6367, 807, 264, 15245, 4832, 22732, 13, 50559], "temperature": 0.0, "avg_logprob": -0.25744412740071615, "compression_ratio": 1.7215909090909092, "no_speech_prob": 0.21467171609401703}, {"id": 503, "seek": 232216, "start": 2326.06, "end": 2328.12, "text": " That's going to give us indices.", "tokens": [50559, 663, 311, 516, 281, 976, 505, 43840, 13, 50662], "temperature": 0.0, "avg_logprob": -0.25744412740071615, "compression_ratio": 1.7215909090909092, "no_speech_prob": 0.21467171609401703}, {"id": 504, "seek": 232216, "start": 2328.12, "end": 2333.16, "text": " And then we're going to get that data set item from that batch, for everything in that", "tokens": [50662, 400, 550, 321, 434, 516, 281, 483, 300, 1412, 992, 3174, 490, 300, 15245, 11, 337, 1203, 294, 300, 50914], "temperature": 0.0, "avg_logprob": -0.25744412740071615, "compression_ratio": 1.7215909090909092, "no_speech_prob": 0.21467171609401703}, {"id": 505, "seek": 232216, "start": 2333.16, "end": 2336.22, "text": " batch.", "tokens": [50914, 15245, 13, 51067], "temperature": 0.0, "avg_logprob": -0.25744412740071615, "compression_ratio": 1.7215909090909092, "no_speech_prob": 0.21467171609401703}, {"id": 506, "seek": 232216, "start": 2336.22, "end": 2342.72, "text": " So that's going to give us a list, and then we have to stack all of the X's and all of", "tokens": [51067, 407, 300, 311, 516, 281, 976, 505, 257, 1329, 11, 293, 550, 321, 362, 281, 8630, 439, 295, 264, 1783, 311, 293, 439, 295, 51392], "temperature": 0.0, "avg_logprob": -0.25744412740071615, "compression_ratio": 1.7215909090909092, "no_speech_prob": 0.21467171609401703}, {"id": 507, "seek": 232216, "start": 2342.72, "end": 2345.22, "text": " the Y's together into tensors.", "tokens": [51392, 264, 398, 311, 1214, 666, 10688, 830, 13, 51517], "temperature": 0.0, "avg_logprob": -0.25744412740071615, "compression_ratio": 1.7215909090909092, "no_speech_prob": 0.21467171609401703}, {"id": 508, "seek": 234522, "start": 2345.22, "end": 2354.02, "text": " So I've created something here called collate function, and we're going to default that", "tokens": [50364, 407, 286, 600, 2942, 746, 510, 1219, 1263, 473, 2445, 11, 293, 321, 434, 516, 281, 7576, 300, 50804], "temperature": 0.0, "avg_logprob": -0.28860130310058596, "compression_ratio": 1.5680473372781065, "no_speech_prob": 0.0006263297982513905}, {"id": 509, "seek": 234522, "start": 2354.02, "end": 2362.7, "text": " to this little function here, which is going to grab our batch, pull out the X's and Y's", "tokens": [50804, 281, 341, 707, 2445, 510, 11, 597, 307, 516, 281, 4444, 527, 15245, 11, 2235, 484, 264, 1783, 311, 293, 398, 311, 51238], "temperature": 0.0, "avg_logprob": -0.28860130310058596, "compression_ratio": 1.5680473372781065, "no_speech_prob": 0.0006263297982513905}, {"id": 510, "seek": 234522, "start": 2362.7, "end": 2367.9599999999996, "text": " separately, and then stack them up into tensors.", "tokens": [51238, 14759, 11, 293, 550, 8630, 552, 493, 666, 10688, 830, 13, 51501], "temperature": 0.0, "avg_logprob": -0.28860130310058596, "compression_ratio": 1.5680473372781065, "no_speech_prob": 0.0006263297982513905}, {"id": 511, "seek": 234522, "start": 2367.9599999999996, "end": 2371.58, "text": " So this is called our collate function.", "tokens": [51501, 407, 341, 307, 1219, 527, 1263, 473, 2445, 13, 51682], "temperature": 0.0, "avg_logprob": -0.28860130310058596, "compression_ratio": 1.5680473372781065, "no_speech_prob": 0.0006263297982513905}, {"id": 512, "seek": 237158, "start": 2371.7799999999997, "end": 2377.18, "text": " So if we put all that together, we can create a training sampler, which is a batch sampler", "tokens": [50374, 407, 498, 321, 829, 439, 300, 1214, 11, 321, 393, 1884, 257, 3097, 3247, 22732, 11, 597, 307, 257, 15245, 3247, 22732, 50644], "temperature": 0.0, "avg_logprob": -0.24743196915607063, "compression_ratio": 1.9635416666666667, "no_speech_prob": 0.002550893696025014}, {"id": 513, "seek": 237158, "start": 2377.18, "end": 2381.2599999999998, "text": " over the training set, with shuffle true.", "tokens": [50644, 670, 264, 3097, 992, 11, 365, 39426, 2074, 13, 50848], "temperature": 0.0, "avg_logprob": -0.24743196915607063, "compression_ratio": 1.9635416666666667, "no_speech_prob": 0.002550893696025014}, {"id": 514, "seek": 237158, "start": 2381.2599999999998, "end": 2389.56, "text": " A validation sampler will be a batch sampler over the validation set, with shuffle false.", "tokens": [50848, 316, 24071, 3247, 22732, 486, 312, 257, 15245, 3247, 22732, 670, 264, 24071, 992, 11, 365, 39426, 7908, 13, 51263], "temperature": 0.0, "avg_logprob": -0.24743196915607063, "compression_ratio": 1.9635416666666667, "no_speech_prob": 0.002550893696025014}, {"id": 515, "seek": 237158, "start": 2389.56, "end": 2395.14, "text": " And so then we can pass that into this data loader class.", "tokens": [51263, 400, 370, 550, 321, 393, 1320, 300, 666, 341, 1412, 3677, 260, 1508, 13, 51542], "temperature": 0.0, "avg_logprob": -0.24743196915607063, "compression_ratio": 1.9635416666666667, "no_speech_prob": 0.002550893696025014}, {"id": 516, "seek": 237158, "start": 2395.14, "end": 2401.2999999999997, "text": " The training data set, and the training sampler, and the collate function, which we don't really", "tokens": [51542, 440, 3097, 1412, 992, 11, 293, 264, 3097, 3247, 22732, 11, 293, 264, 1263, 473, 2445, 11, 597, 321, 500, 380, 534, 51850], "temperature": 0.0, "avg_logprob": -0.24743196915607063, "compression_ratio": 1.9635416666666667, "no_speech_prob": 0.002550893696025014}, {"id": 517, "seek": 240130, "start": 2401.3, "end": 2403.7000000000003, "text": " need because we're just using the default one.", "tokens": [50364, 643, 570, 321, 434, 445, 1228, 264, 7576, 472, 13, 50484], "temperature": 0.0, "avg_logprob": -0.28981660759967304, "compression_ratio": 1.4720812182741116, "no_speech_prob": 0.001304504694417119}, {"id": 518, "seek": 240130, "start": 2403.7000000000003, "end": 2410.1000000000004, "text": " So I guess we can just get rid of that.", "tokens": [50484, 407, 286, 2041, 321, 393, 445, 483, 3973, 295, 300, 13, 50804], "temperature": 0.0, "avg_logprob": -0.28981660759967304, "compression_ratio": 1.4720812182741116, "no_speech_prob": 0.001304504694417119}, {"id": 519, "seek": 240130, "start": 2410.1000000000004, "end": 2413.86, "text": " And so now, there we go, we can do exactly the same thing as before.", "tokens": [50804, 400, 370, 586, 11, 456, 321, 352, 11, 321, 393, 360, 2293, 264, 912, 551, 382, 949, 13, 50992], "temperature": 0.0, "avg_logprob": -0.28981660759967304, "compression_ratio": 1.4720812182741116, "no_speech_prob": 0.001304504694417119}, {"id": 520, "seek": 240130, "start": 2413.86, "end": 2419.7000000000003, "text": " XB, YB is next iter, and this time we use the valid data loader.", "tokens": [50992, 1783, 33, 11, 398, 33, 307, 958, 17138, 11, 293, 341, 565, 321, 764, 264, 7363, 1412, 3677, 260, 13, 51284], "temperature": 0.0, "avg_logprob": -0.28981660759967304, "compression_ratio": 1.4720812182741116, "no_speech_prob": 0.001304504694417119}, {"id": 521, "seek": 240130, "start": 2419.7000000000003, "end": 2422.0600000000004, "text": " Check the shapes.", "tokens": [51284, 6881, 264, 10854, 13, 51402], "temperature": 0.0, "avg_logprob": -0.28981660759967304, "compression_ratio": 1.4720812182741116, "no_speech_prob": 0.001304504694417119}, {"id": 522, "seek": 240130, "start": 2422.0600000000004, "end": 2429.78, "text": " And this is how PyTorch's actual data loaders work.", "tokens": [51402, 400, 341, 307, 577, 9953, 51, 284, 339, 311, 3539, 1412, 3677, 433, 589, 13, 51788], "temperature": 0.0, "avg_logprob": -0.28981660759967304, "compression_ratio": 1.4720812182741116, "no_speech_prob": 0.001304504694417119}, {"id": 523, "seek": 242978, "start": 2429.78, "end": 2432.02, "text": " This is all the pieces they have.", "tokens": [50364, 639, 307, 439, 264, 3755, 436, 362, 13, 50476], "temperature": 0.0, "avg_logprob": -0.23142723083496095, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.00037998301559127867}, {"id": 524, "seek": 242978, "start": 2432.02, "end": 2439.34, "text": " They have samplers, they have batch samplers, they have a collation function, and they have", "tokens": [50476, 814, 362, 3247, 564, 433, 11, 436, 362, 15245, 3247, 564, 433, 11, 436, 362, 257, 1263, 399, 2445, 11, 293, 436, 362, 50842], "temperature": 0.0, "avg_logprob": -0.23142723083496095, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.00037998301559127867}, {"id": 525, "seek": 242978, "start": 2439.34, "end": 2442.1000000000004, "text": " data loaders.", "tokens": [50842, 1412, 3677, 433, 13, 50980], "temperature": 0.0, "avg_logprob": -0.23142723083496095, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.00037998301559127867}, {"id": 526, "seek": 242978, "start": 2442.1000000000004, "end": 2455.5800000000004, "text": " So remember that what I want you to be doing for your homework is experimenting with these", "tokens": [50980, 407, 1604, 300, 437, 286, 528, 291, 281, 312, 884, 337, 428, 14578, 307, 29070, 365, 613, 51654], "temperature": 0.0, "avg_logprob": -0.23142723083496095, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.00037998301559127867}, {"id": 527, "seek": 242978, "start": 2455.5800000000004, "end": 2458.86, "text": " carefully to see exactly what each thing is taking in.", "tokens": [51654, 7500, 281, 536, 2293, 437, 1184, 551, 307, 1940, 294, 13, 51818], "temperature": 0.0, "avg_logprob": -0.23142723083496095, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.00037998301559127867}, {"id": 528, "seek": 245886, "start": 2458.94, "end": 2465.2200000000003, "text": " Okay, so Pieter is asking on the chat, what is this collate thing doing?", "tokens": [50368, 1033, 11, 370, 41970, 260, 307, 3365, 322, 264, 5081, 11, 437, 307, 341, 1263, 473, 551, 884, 30, 50682], "temperature": 0.0, "avg_logprob": -0.30497783567847275, "compression_ratio": 1.5178571428571428, "no_speech_prob": 9.516244972473942e-06}, {"id": 529, "seek": 245886, "start": 2465.2200000000003, "end": 2471.9, "text": " So collate function, it defaults to collate.", "tokens": [50682, 407, 1263, 473, 2445, 11, 309, 7576, 82, 281, 1263, 473, 13, 51016], "temperature": 0.0, "avg_logprob": -0.30497783567847275, "compression_ratio": 1.5178571428571428, "no_speech_prob": 9.516244972473942e-06}, {"id": 530, "seek": 245886, "start": 2471.9, "end": 2472.9, "text": " What does it do?", "tokens": [51016, 708, 775, 309, 360, 30, 51066], "temperature": 0.0, "avg_logprob": -0.30497783567847275, "compression_ratio": 1.5178571428571428, "no_speech_prob": 9.516244972473942e-06}, {"id": 531, "seek": 245886, "start": 2472.9, "end": 2476.1400000000003, "text": " Well, let's see, let's go through each of these steps.", "tokens": [51066, 1042, 11, 718, 311, 536, 11, 718, 311, 352, 807, 1184, 295, 613, 4439, 13, 51228], "temperature": 0.0, "avg_logprob": -0.30497783567847275, "compression_ratio": 1.5178571428571428, "no_speech_prob": 9.516244972473942e-06}, {"id": 532, "seek": 245886, "start": 2476.1400000000003, "end": 2487.06, "text": " So we've got a batch sampler, so let's do just the valid sampler.", "tokens": [51228, 407, 321, 600, 658, 257, 15245, 3247, 22732, 11, 370, 718, 311, 360, 445, 264, 7363, 3247, 22732, 13, 51774], "temperature": 0.0, "avg_logprob": -0.30497783567847275, "compression_ratio": 1.5178571428571428, "no_speech_prob": 9.516244972473942e-06}, {"id": 533, "seek": 248706, "start": 2487.06, "end": 2490.02, "text": " So the batch sampler, here it is.", "tokens": [50364, 407, 264, 15245, 3247, 22732, 11, 510, 309, 307, 13, 50512], "temperature": 0.0, "avg_logprob": -0.2862995081934436, "compression_ratio": 1.791304347826087, "no_speech_prob": 0.0011160115245729685}, {"id": 534, "seek": 248706, "start": 2490.02, "end": 2493.1, "text": " So we're going to go through each thing in the batch sampler.", "tokens": [50512, 407, 321, 434, 516, 281, 352, 807, 1184, 551, 294, 264, 15245, 3247, 22732, 13, 50666], "temperature": 0.0, "avg_logprob": -0.2862995081934436, "compression_ratio": 1.791304347826087, "no_speech_prob": 0.0011160115245729685}, {"id": 535, "seek": 248706, "start": 2493.1, "end": 2496.54, "text": " So let's just grab one thing from the batch sampler.", "tokens": [50666, 407, 718, 311, 445, 4444, 472, 551, 490, 264, 15245, 3247, 22732, 13, 50838], "temperature": 0.0, "avg_logprob": -0.2862995081934436, "compression_ratio": 1.791304347826087, "no_speech_prob": 0.0011160115245729685}, {"id": 536, "seek": 248706, "start": 2496.54, "end": 2503.62, "text": " So the output of the batch sampler will be next iter.", "tokens": [50838, 407, 264, 5598, 295, 264, 15245, 3247, 22732, 486, 312, 958, 17138, 13, 51192], "temperature": 0.0, "avg_logprob": -0.2862995081934436, "compression_ratio": 1.791304347826087, "no_speech_prob": 0.0011160115245729685}, {"id": 537, "seek": 248706, "start": 2503.62, "end": 2506.9, "text": " So here's what the batch sampler contains.", "tokens": [51192, 407, 510, 311, 437, 264, 15245, 3247, 22732, 8306, 13, 51356], "temperature": 0.0, "avg_logprob": -0.2862995081934436, "compression_ratio": 1.791304347826087, "no_speech_prob": 0.0011160115245729685}, {"id": 538, "seek": 248706, "start": 2506.9, "end": 2511.94, "text": " All right, just the first 50 digits, not surprisingly, because this is our validation sampler.", "tokens": [51356, 1057, 558, 11, 445, 264, 700, 2625, 27011, 11, 406, 17600, 11, 570, 341, 307, 527, 24071, 3247, 22732, 13, 51608], "temperature": 0.0, "avg_logprob": -0.2862995081934436, "compression_ratio": 1.791304347826087, "no_speech_prob": 0.0011160115245729685}, {"id": 539, "seek": 248706, "start": 2511.94, "end": 2514.46, "text": " If we did a training sampler, that would be randomized.", "tokens": [51608, 759, 321, 630, 257, 3097, 3247, 22732, 11, 300, 576, 312, 38513, 13, 51734], "temperature": 0.0, "avg_logprob": -0.2862995081934436, "compression_ratio": 1.791304347826087, "no_speech_prob": 0.0011160115245729685}, {"id": 540, "seek": 248706, "start": 2514.46, "end": 2517.02, "text": " There they are.", "tokens": [51734, 821, 436, 366, 13, 51862], "temperature": 0.0, "avg_logprob": -0.2862995081934436, "compression_ratio": 1.791304347826087, "no_speech_prob": 0.0011160115245729685}, {"id": 541, "seek": 251702, "start": 2517.98, "end": 2523.38, "text": " Okay, so then what we then do is we go self.dataset i for i and b.", "tokens": [50412, 1033, 11, 370, 550, 437, 321, 550, 360, 307, 321, 352, 2698, 13, 20367, 296, 302, 741, 337, 741, 293, 272, 13, 50682], "temperature": 0.0, "avg_logprob": -0.39282436058169506, "compression_ratio": 1.2983870967741935, "no_speech_prob": 2.430019958410412e-05}, {"id": 542, "seek": 251702, "start": 2523.38, "end": 2527.2599999999998, "text": " So let's copy that.", "tokens": [50682, 407, 718, 311, 5055, 300, 13, 50876], "temperature": 0.0, "avg_logprob": -0.39282436058169506, "compression_ratio": 1.2983870967741935, "no_speech_prob": 2.430019958410412e-05}, {"id": 543, "seek": 251702, "start": 2527.2599999999998, "end": 2532.08, "text": " Copy, paste.", "tokens": [50876, 25653, 11, 9163, 13, 51117], "temperature": 0.0, "avg_logprob": -0.39282436058169506, "compression_ratio": 1.2983870967741935, "no_speech_prob": 2.430019958410412e-05}, {"id": 544, "seek": 251702, "start": 2532.08, "end": 2538.94, "text": " And so rather than self.dataset i, we'll just say valid TS i.", "tokens": [51117, 400, 370, 2831, 813, 2698, 13, 20367, 296, 302, 741, 11, 321, 603, 445, 584, 7363, 37645, 741, 13, 51460], "temperature": 0.0, "avg_logprob": -0.39282436058169506, "compression_ratio": 1.2983870967741935, "no_speech_prob": 2.430019958410412e-05}, {"id": 545, "seek": 253894, "start": 2538.94, "end": 2550.06, "text": " Oh, and it's not i and b, it's i and o, that's what we called it.", "tokens": [50364, 876, 11, 293, 309, 311, 406, 741, 293, 272, 11, 309, 311, 741, 293, 277, 11, 300, 311, 437, 321, 1219, 309, 13, 50920], "temperature": 0.0, "avg_logprob": -0.35859879451011545, "compression_ratio": 1.449612403100775, "no_speech_prob": 0.0076955873519182205}, {"id": 546, "seek": 253894, "start": 2550.06, "end": 2556.78, "text": " Oh, and we did it for training, sorry.", "tokens": [50920, 876, 11, 293, 321, 630, 309, 337, 3097, 11, 2597, 13, 51256], "temperature": 0.0, "avg_logprob": -0.35859879451011545, "compression_ratio": 1.449612403100775, "no_speech_prob": 0.0076955873519182205}, {"id": 547, "seek": 253894, "start": 2556.78, "end": 2557.78, "text": " Training.", "tokens": [51256, 20620, 13, 51306], "temperature": 0.0, "avg_logprob": -0.35859879451011545, "compression_ratio": 1.449612403100775, "no_speech_prob": 0.0076955873519182205}, {"id": 548, "seek": 253894, "start": 2557.78, "end": 2568.2200000000003, "text": " Okay, so what it's created here is a list of tuples of tensors, I think.", "tokens": [51306, 1033, 11, 370, 437, 309, 311, 2942, 510, 307, 257, 1329, 295, 2604, 2622, 295, 10688, 830, 11, 286, 519, 13, 51828], "temperature": 0.0, "avg_logprob": -0.35859879451011545, "compression_ratio": 1.449612403100775, "no_speech_prob": 0.0076955873519182205}, {"id": 549, "seek": 256822, "start": 2568.5, "end": 2569.5, "text": " So let's have a look.", "tokens": [50378, 407, 718, 311, 362, 257, 574, 13, 50428], "temperature": 0.0, "avg_logprob": -0.2988418211419898, "compression_ratio": 1.5345911949685536, "no_speech_prob": 3.219231803086586e-05}, {"id": 550, "seek": 256822, "start": 2569.5, "end": 2576.8999999999996, "text": " So we'll call this p, whatever.", "tokens": [50428, 407, 321, 603, 818, 341, 280, 11, 2035, 13, 50798], "temperature": 0.0, "avg_logprob": -0.2988418211419898, "compression_ratio": 1.5345911949685536, "no_speech_prob": 3.219231803086586e-05}, {"id": 551, "seek": 256822, "start": 2576.8999999999996, "end": 2585.06, "text": " So p0, okay, is a tuple.", "tokens": [50798, 407, 280, 15, 11, 1392, 11, 307, 257, 2604, 781, 13, 51206], "temperature": 0.0, "avg_logprob": -0.2988418211419898, "compression_ratio": 1.5345911949685536, "no_speech_prob": 3.219231803086586e-05}, {"id": 552, "seek": 256822, "start": 2585.06, "end": 2589.8599999999997, "text": " It's got the x and the y, independent and dependent variable.", "tokens": [51206, 467, 311, 658, 264, 2031, 293, 264, 288, 11, 6695, 293, 12334, 7006, 13, 51446], "temperature": 0.0, "avg_logprob": -0.2988418211419898, "compression_ratio": 1.5345911949685536, "no_speech_prob": 3.219231803086586e-05}, {"id": 553, "seek": 256822, "start": 2589.8599999999997, "end": 2592.18, "text": " So that's not what we want.", "tokens": [51446, 407, 300, 311, 406, 437, 321, 528, 13, 51562], "temperature": 0.0, "avg_logprob": -0.2988418211419898, "compression_ratio": 1.5345911949685536, "no_speech_prob": 3.219231803086586e-05}, {"id": 554, "seek": 256822, "start": 2592.18, "end": 2595.7999999999997, "text": " What we want is something that we can loop through.", "tokens": [51562, 708, 321, 528, 307, 746, 300, 321, 393, 6367, 807, 13, 51743], "temperature": 0.0, "avg_logprob": -0.2988418211419898, "compression_ratio": 1.5345911949685536, "no_speech_prob": 3.219231803086586e-05}, {"id": 555, "seek": 256822, "start": 2595.7999999999997, "end": 2598.08, "text": " We want to get batches.", "tokens": [51743, 492, 528, 281, 483, 15245, 279, 13, 51857], "temperature": 0.0, "avg_logprob": -0.2988418211419898, "compression_ratio": 1.5345911949685536, "no_speech_prob": 3.219231803086586e-05}, {"id": 556, "seek": 259808, "start": 2598.94, "end": 2604.6, "text": " So what the collation model is going to do, sorry not collation model, the collate function", "tokens": [50407, 407, 437, 264, 1263, 399, 2316, 307, 516, 281, 360, 11, 2597, 406, 1263, 399, 2316, 11, 264, 1263, 473, 2445, 50690], "temperature": 0.0, "avg_logprob": -0.23855902353922526, "compression_ratio": 1.8544303797468353, "no_speech_prob": 9.666105142969172e-06}, {"id": 557, "seek": 259808, "start": 2604.6, "end": 2612.64, "text": " is going to do, is it's going to take all of our x's and all of our y's and collate", "tokens": [50690, 307, 516, 281, 360, 11, 307, 309, 311, 516, 281, 747, 439, 295, 527, 2031, 311, 293, 439, 295, 527, 288, 311, 293, 1263, 473, 51092], "temperature": 0.0, "avg_logprob": -0.23855902353922526, "compression_ratio": 1.8544303797468353, "no_speech_prob": 9.666105142969172e-06}, {"id": 558, "seek": 259808, "start": 2612.64, "end": 2613.7999999999997, "text": " them into two tensors.", "tokens": [51092, 552, 666, 732, 10688, 830, 13, 51150], "temperature": 0.0, "avg_logprob": -0.23855902353922526, "compression_ratio": 1.8544303797468353, "no_speech_prob": 9.666105142969172e-06}, {"id": 559, "seek": 259808, "start": 2613.7999999999997, "end": 2616.56, "text": " One tensor of x's and one tensor of y's.", "tokens": [51150, 1485, 40863, 295, 2031, 311, 293, 472, 40863, 295, 288, 311, 13, 51288], "temperature": 0.0, "avg_logprob": -0.23855902353922526, "compression_ratio": 1.8544303797468353, "no_speech_prob": 9.666105142969172e-06}, {"id": 560, "seek": 259808, "start": 2616.56, "end": 2627.88, "text": " So the way it does that is it first of all calls zip.", "tokens": [51288, 407, 264, 636, 309, 775, 300, 307, 309, 700, 295, 439, 5498, 20730, 13, 51854], "temperature": 0.0, "avg_logprob": -0.23855902353922526, "compression_ratio": 1.8544303797468353, "no_speech_prob": 9.666105142969172e-06}, {"id": 561, "seek": 262788, "start": 2628.6800000000003, "end": 2632.0, "text": " So zip is a very, very commonly used Python function.", "tokens": [50404, 407, 20730, 307, 257, 588, 11, 588, 12719, 1143, 15329, 2445, 13, 50570], "temperature": 0.0, "avg_logprob": -0.2984940950260606, "compression_ratio": 1.6, "no_speech_prob": 5.771917130914517e-06}, {"id": 562, "seek": 262788, "start": 2632.0, "end": 2635.7200000000003, "text": " It's got nothing to do with a compression program zip, but instead what it does is it", "tokens": [50570, 467, 311, 658, 1825, 281, 360, 365, 257, 19355, 1461, 20730, 11, 457, 2602, 437, 309, 775, 307, 309, 50756], "temperature": 0.0, "avg_logprob": -0.2984940950260606, "compression_ratio": 1.6, "no_speech_prob": 5.771917130914517e-06}, {"id": 563, "seek": 262788, "start": 2635.7200000000003, "end": 2638.88, "text": " effectively allows us to like transpose things.", "tokens": [50756, 8659, 4045, 505, 281, 411, 25167, 721, 13, 50914], "temperature": 0.0, "avg_logprob": -0.2984940950260606, "compression_ratio": 1.6, "no_speech_prob": 5.771917130914517e-06}, {"id": 564, "seek": 262788, "start": 2638.88, "end": 2647.52, "text": " So that now, as you can see, we've got all of the second elements, or index one elements,", "tokens": [50914, 407, 300, 586, 11, 382, 291, 393, 536, 11, 321, 600, 658, 439, 295, 264, 1150, 4959, 11, 420, 8186, 472, 4959, 11, 51346], "temperature": 0.0, "avg_logprob": -0.2984940950260606, "compression_ratio": 1.6, "no_speech_prob": 5.771917130914517e-06}, {"id": 565, "seek": 262788, "start": 2647.52, "end": 2651.62, "text": " all together, and all of the index zero elements together.", "tokens": [51346, 439, 1214, 11, 293, 439, 295, 264, 8186, 4018, 4959, 1214, 13, 51551], "temperature": 0.0, "avg_logprob": -0.2984940950260606, "compression_ratio": 1.6, "no_speech_prob": 5.771917130914517e-06}, {"id": 566, "seek": 265162, "start": 2651.7, "end": 2659.14, "text": " So then we can stack those all up together.", "tokens": [50368, 407, 550, 321, 393, 8630, 729, 439, 493, 1214, 13, 50740], "temperature": 0.0, "avg_logprob": -0.25827430543445407, "compression_ratio": 1.5195530726256983, "no_speech_prob": 0.0013885049847885966}, {"id": 567, "seek": 265162, "start": 2659.14, "end": 2661.58, "text": " And that gives us our y's for our batch.", "tokens": [50740, 400, 300, 2709, 505, 527, 288, 311, 337, 527, 15245, 13, 50862], "temperature": 0.0, "avg_logprob": -0.25827430543445407, "compression_ratio": 1.5195530726256983, "no_speech_prob": 0.0013885049847885966}, {"id": 568, "seek": 265162, "start": 2661.58, "end": 2662.8599999999997, "text": " So that's what collate does.", "tokens": [50862, 407, 300, 311, 437, 1263, 473, 775, 13, 50926], "temperature": 0.0, "avg_logprob": -0.25827430543445407, "compression_ratio": 1.5195530726256983, "no_speech_prob": 0.0013885049847885966}, {"id": 569, "seek": 265162, "start": 2662.8599999999997, "end": 2673.54, "text": " So the collate function is used an awful lot in PyTorch.", "tokens": [50926, 407, 264, 1263, 473, 2445, 307, 1143, 364, 11232, 688, 294, 9953, 51, 284, 339, 13, 51460], "temperature": 0.0, "avg_logprob": -0.25827430543445407, "compression_ratio": 1.5195530726256983, "no_speech_prob": 0.0013885049847885966}, {"id": 570, "seek": 265162, "start": 2673.54, "end": 2678.62, "text": " Increasingly nowadays where hugging face stuff uses it a lot.", "tokens": [51460, 30367, 3349, 356, 13434, 689, 41706, 1851, 1507, 4960, 309, 257, 688, 13, 51714], "temperature": 0.0, "avg_logprob": -0.25827430543445407, "compression_ratio": 1.5195530726256983, "no_speech_prob": 0.0013885049847885966}, {"id": 571, "seek": 265162, "start": 2678.62, "end": 2681.42, "text": " And so we'll be using it a lot as well.", "tokens": [51714, 400, 370, 321, 603, 312, 1228, 309, 257, 688, 382, 731, 13, 51854], "temperature": 0.0, "avg_logprob": -0.25827430543445407, "compression_ratio": 1.5195530726256983, "no_speech_prob": 0.0013885049847885966}, {"id": 572, "seek": 268142, "start": 2682.2200000000003, "end": 2686.86, "text": " And basically it's a thing that allows us to customize how the data that we get back", "tokens": [50404, 400, 1936, 309, 311, 257, 551, 300, 4045, 505, 281, 19734, 577, 264, 1412, 300, 321, 483, 646, 50636], "temperature": 0.0, "avg_logprob": -0.29699293415198164, "compression_ratio": 1.6682926829268292, "no_speech_prob": 6.302750261966139e-05}, {"id": 573, "seek": 268142, "start": 2686.86, "end": 2693.02, "text": " from our data set, once it's been kind of generating a list of things from the data", "tokens": [50636, 490, 527, 1412, 992, 11, 1564, 309, 311, 668, 733, 295, 17746, 257, 1329, 295, 721, 490, 264, 1412, 50944], "temperature": 0.0, "avg_logprob": -0.29699293415198164, "compression_ratio": 1.6682926829268292, "no_speech_prob": 6.302750261966139e-05}, {"id": 574, "seek": 268142, "start": 2693.02, "end": 2700.26, "text": " set, how do we put it together into a bunch of things that our model can take as inputs.", "tokens": [50944, 992, 11, 577, 360, 321, 829, 309, 1214, 666, 257, 3840, 295, 721, 300, 527, 2316, 393, 747, 382, 15743, 13, 51306], "temperature": 0.0, "avg_logprob": -0.29699293415198164, "compression_ratio": 1.6682926829268292, "no_speech_prob": 6.302750261966139e-05}, {"id": 575, "seek": 268142, "start": 2700.26, "end": 2701.86, "text": " Because that's really what we want here.", "tokens": [51306, 1436, 300, 311, 534, 437, 321, 528, 510, 13, 51386], "temperature": 0.0, "avg_logprob": -0.29699293415198164, "compression_ratio": 1.6682926829268292, "no_speech_prob": 6.302750261966139e-05}, {"id": 576, "seek": 268142, "start": 2701.86, "end": 2704.38, "text": " So that's what the collation function does.", "tokens": [51386, 407, 300, 311, 437, 264, 1263, 399, 2445, 775, 13, 51512], "temperature": 0.0, "avg_logprob": -0.29699293415198164, "compression_ratio": 1.6682926829268292, "no_speech_prob": 6.302750261966139e-05}, {"id": 577, "seek": 270438, "start": 2705.1800000000003, "end": 2708.38, "text": " Oh, this is the wrong way around.", "tokens": [50404, 876, 11, 341, 307, 264, 2085, 636, 926, 13, 50564], "temperature": 0.0, "avg_logprob": -0.4150738034929548, "compression_ratio": 1.5208333333333333, "no_speech_prob": 0.008847180753946304}, {"id": 578, "seek": 270438, "start": 2715.06, "end": 2717.82, "text": " Like so.", "tokens": [50898, 1743, 370, 13, 51036], "temperature": 0.0, "avg_logprob": -0.4150738034929548, "compression_ratio": 1.5208333333333333, "no_speech_prob": 0.008847180753946304}, {"id": 579, "seek": 270438, "start": 2717.82, "end": 2723.02, "text": " This is something that I do so often that Fastcore has a quick little shortcut for it,", "tokens": [51036, 639, 307, 746, 300, 286, 360, 370, 2049, 300, 15968, 12352, 575, 257, 1702, 707, 24822, 337, 309, 11, 51296], "temperature": 0.0, "avg_logprob": -0.4150738034929548, "compression_ratio": 1.5208333333333333, "no_speech_prob": 0.008847180753946304}, {"id": 580, "seek": 270438, "start": 2723.02, "end": 2725.54, "text": " just called store attr, store attributes.", "tokens": [51296, 445, 1219, 3531, 951, 81, 11, 3531, 17212, 13, 51422], "temperature": 0.0, "avg_logprob": -0.4150738034929548, "compression_ratio": 1.5208333333333333, "no_speech_prob": 0.008847180753946304}, {"id": 581, "seek": 270438, "start": 2725.54, "end": 2731.62, "text": " And so if you just put that in your dunder init, then you just need one line of code", "tokens": [51422, 400, 370, 498, 291, 445, 829, 300, 294, 428, 274, 997, 260, 3157, 11, 550, 291, 445, 643, 472, 1622, 295, 3089, 51726], "temperature": 0.0, "avg_logprob": -0.4150738034929548, "compression_ratio": 1.5208333333333333, "no_speech_prob": 0.008847180753946304}, {"id": 582, "seek": 270438, "start": 2731.62, "end": 2733.3, "text": " and it does exactly the same thing.", "tokens": [51726, 293, 309, 775, 2293, 264, 912, 551, 13, 51810], "temperature": 0.0, "avg_logprob": -0.4150738034929548, "compression_ratio": 1.5208333333333333, "no_speech_prob": 0.008847180753946304}, {"id": 583, "seek": 273330, "start": 2733.3, "end": 2736.42, "text": " So there's a little shortcut as you see.", "tokens": [50364, 407, 456, 311, 257, 707, 24822, 382, 291, 536, 13, 50520], "temperature": 0.0, "avg_logprob": -0.30911607945219, "compression_ratio": 1.640625, "no_speech_prob": 3.591293716453947e-05}, {"id": 584, "seek": 273330, "start": 2736.42, "end": 2738.98, "text": " And so you'll see that quite a bit.", "tokens": [50520, 400, 370, 291, 603, 536, 300, 1596, 257, 857, 13, 50648], "temperature": 0.0, "avg_logprob": -0.30911607945219, "compression_ratio": 1.640625, "no_speech_prob": 3.591293716453947e-05}, {"id": 585, "seek": 273330, "start": 2738.98, "end": 2746.9, "text": " All right, let's have a seven minute break and see you back here very soon.", "tokens": [50648, 1057, 558, 11, 718, 311, 362, 257, 3407, 3456, 1821, 293, 536, 291, 646, 510, 588, 2321, 13, 51044], "temperature": 0.0, "avg_logprob": -0.30911607945219, "compression_ratio": 1.640625, "no_speech_prob": 3.591293716453947e-05}, {"id": 586, "seek": 273330, "start": 2746.9, "end": 2751.7000000000003, "text": " And we're going to look at a multi-processing data loader and then we'll have nearly finished", "tokens": [51044, 400, 321, 434, 516, 281, 574, 412, 257, 4825, 12, 41075, 278, 1412, 3677, 260, 293, 550, 321, 603, 362, 6217, 4335, 51284], "temperature": 0.0, "avg_logprob": -0.30911607945219, "compression_ratio": 1.640625, "no_speech_prob": 3.591293716453947e-05}, {"id": 587, "seek": 273330, "start": 2751.7000000000003, "end": 2752.7000000000003, "text": " this notebook.", "tokens": [51284, 341, 21060, 13, 51334], "temperature": 0.0, "avg_logprob": -0.30911607945219, "compression_ratio": 1.640625, "no_speech_prob": 3.591293716453947e-05}, {"id": 588, "seek": 273330, "start": 2752.7000000000003, "end": 2758.02, "text": " All right, see you soon.", "tokens": [51334, 1057, 558, 11, 536, 291, 2321, 13, 51600], "temperature": 0.0, "avg_logprob": -0.30911607945219, "compression_ratio": 1.640625, "no_speech_prob": 3.591293716453947e-05}, {"id": 589, "seek": 273330, "start": 2758.02, "end": 2761.02, "text": " All right, let's keep going.", "tokens": [51600, 1057, 558, 11, 718, 311, 1066, 516, 13, 51750], "temperature": 0.0, "avg_logprob": -0.30911607945219, "compression_ratio": 1.640625, "no_speech_prob": 3.591293716453947e-05}, {"id": 590, "seek": 276102, "start": 2761.02, "end": 2778.06, "text": " So we've seen how to create a data loader and sampling from it.", "tokens": [50364, 407, 321, 600, 1612, 577, 281, 1884, 257, 1412, 3677, 260, 293, 21179, 490, 309, 13, 51216], "temperature": 0.0, "avg_logprob": -0.25293166682405294, "compression_ratio": 1.3636363636363635, "no_speech_prob": 3.9442500565201044e-05}, {"id": 591, "seek": 276102, "start": 2778.06, "end": 2786.54, "text": " The PyTorch data loader works exactly like this, but it uses a lot more code because", "tokens": [51216, 440, 9953, 51, 284, 339, 1412, 3677, 260, 1985, 2293, 411, 341, 11, 457, 309, 4960, 257, 688, 544, 3089, 570, 51640], "temperature": 0.0, "avg_logprob": -0.25293166682405294, "compression_ratio": 1.3636363636363635, "no_speech_prob": 3.9442500565201044e-05}, {"id": 592, "seek": 276102, "start": 2786.54, "end": 2789.34, "text": " it implements multi-processing.", "tokens": [51640, 309, 704, 17988, 4825, 12, 41075, 278, 13, 51780], "temperature": 0.0, "avg_logprob": -0.25293166682405294, "compression_ratio": 1.3636363636363635, "no_speech_prob": 3.9442500565201044e-05}, {"id": 593, "seek": 278934, "start": 2789.38, "end": 2800.6200000000003, "text": " And so multi-processing means that the actual, this thing here, that code can be run in multiple", "tokens": [50366, 400, 370, 4825, 12, 41075, 278, 1355, 300, 264, 3539, 11, 341, 551, 510, 11, 300, 3089, 393, 312, 1190, 294, 3866, 50928], "temperature": 0.0, "avg_logprob": -0.2919648127122359, "compression_ratio": 1.619289340101523, "no_speech_prob": 0.00017130807100329548}, {"id": 594, "seek": 278934, "start": 2800.6200000000003, "end": 2801.6200000000003, "text": " processes.", "tokens": [50928, 7555, 13, 50978], "temperature": 0.0, "avg_logprob": -0.2919648127122359, "compression_ratio": 1.619289340101523, "no_speech_prob": 0.00017130807100329548}, {"id": 595, "seek": 278934, "start": 2801.6200000000003, "end": 2803.06, "text": " They can be run in parallel for multiple items.", "tokens": [50978, 814, 393, 312, 1190, 294, 8952, 337, 3866, 4754, 13, 51050], "temperature": 0.0, "avg_logprob": -0.2919648127122359, "compression_ratio": 1.619289340101523, "no_speech_prob": 0.00017130807100329548}, {"id": 596, "seek": 278934, "start": 2803.06, "end": 2812.1000000000004, "text": " So this code, for example, might be opening up a JPEG, rotating it, flipping it, etc.", "tokens": [51050, 407, 341, 3089, 11, 337, 1365, 11, 1062, 312, 5193, 493, 257, 508, 5208, 38, 11, 19627, 309, 11, 26886, 309, 11, 5183, 13, 51502], "temperature": 0.0, "avg_logprob": -0.2919648127122359, "compression_ratio": 1.619289340101523, "no_speech_prob": 0.00017130807100329548}, {"id": 597, "seek": 278934, "start": 2812.1000000000004, "end": 2817.1400000000003, "text": " So because remember, this is just calling the dunder get item for a data set.", "tokens": [51502, 407, 570, 1604, 11, 341, 307, 445, 5141, 264, 274, 6617, 483, 3174, 337, 257, 1412, 992, 13, 51754], "temperature": 0.0, "avg_logprob": -0.2919648127122359, "compression_ratio": 1.619289340101523, "no_speech_prob": 0.00017130807100329548}, {"id": 598, "seek": 281714, "start": 2817.14, "end": 2820.3399999999997, "text": " So that could be doing a lot of work for each item and we're doing it for every item", "tokens": [50364, 407, 300, 727, 312, 884, 257, 688, 295, 589, 337, 1184, 3174, 293, 321, 434, 884, 309, 337, 633, 3174, 50524], "temperature": 0.0, "avg_logprob": -0.1745166778564453, "compression_ratio": 1.6513409961685823, "no_speech_prob": 0.0020507057197391987}, {"id": 599, "seek": 281714, "start": 2820.3399999999997, "end": 2821.3399999999997, "text": " in the batch.", "tokens": [50524, 294, 264, 15245, 13, 50574], "temperature": 0.0, "avg_logprob": -0.1745166778564453, "compression_ratio": 1.6513409961685823, "no_speech_prob": 0.0020507057197391987}, {"id": 600, "seek": 281714, "start": 2821.3399999999997, "end": 2823.8199999999997, "text": " So we'd love to do those all in parallel.", "tokens": [50574, 407, 321, 1116, 959, 281, 360, 729, 439, 294, 8952, 13, 50698], "temperature": 0.0, "avg_logprob": -0.1745166778564453, "compression_ratio": 1.6513409961685823, "no_speech_prob": 0.0020507057197391987}, {"id": 601, "seek": 281714, "start": 2823.8199999999997, "end": 2830.58, "text": " So I'll show you a very quick and dirty way that basically does the job.", "tokens": [50698, 407, 286, 603, 855, 291, 257, 588, 1702, 293, 9360, 636, 300, 1936, 775, 264, 1691, 13, 51036], "temperature": 0.0, "avg_logprob": -0.1745166778564453, "compression_ratio": 1.6513409961685823, "no_speech_prob": 0.0020507057197391987}, {"id": 602, "seek": 281714, "start": 2830.58, "end": 2835.5, "text": " So Python has a multi-processing library.", "tokens": [51036, 407, 15329, 575, 257, 4825, 12, 41075, 278, 6405, 13, 51282], "temperature": 0.0, "avg_logprob": -0.1745166778564453, "compression_ratio": 1.6513409961685823, "no_speech_prob": 0.0020507057197391987}, {"id": 603, "seek": 281714, "start": 2835.5, "end": 2839.58, "text": " It doesn't work particularly well with PyTorch tensors.", "tokens": [51282, 467, 1177, 380, 589, 4098, 731, 365, 9953, 51, 284, 339, 10688, 830, 13, 51486], "temperature": 0.0, "avg_logprob": -0.1745166778564453, "compression_ratio": 1.6513409961685823, "no_speech_prob": 0.0020507057197391987}, {"id": 604, "seek": 281714, "start": 2839.58, "end": 2843.14, "text": " So PyTorch has created an exact reimplementation of it.", "tokens": [51486, 407, 9953, 51, 284, 339, 575, 2942, 364, 1900, 33433, 781, 19631, 295, 309, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1745166778564453, "compression_ratio": 1.6513409961685823, "no_speech_prob": 0.0020507057197391987}, {"id": 605, "seek": 281714, "start": 2843.14, "end": 2846.58, "text": " So it's identical API wise, but it does work well with tensors.", "tokens": [51664, 407, 309, 311, 14800, 9362, 10829, 11, 457, 309, 775, 589, 731, 365, 10688, 830, 13, 51836], "temperature": 0.0, "avg_logprob": -0.1745166778564453, "compression_ratio": 1.6513409961685823, "no_speech_prob": 0.0020507057197391987}, {"id": 606, "seek": 284658, "start": 2847.02, "end": 2849.5, "text": " So this is basically, we'll just grab the multi-processing.", "tokens": [50386, 407, 341, 307, 1936, 11, 321, 603, 445, 4444, 264, 4825, 12, 41075, 278, 13, 50510], "temperature": 0.0, "avg_logprob": -0.24289728670704122, "compression_ratio": 1.6, "no_speech_prob": 2.2827991415397264e-05}, {"id": 607, "seek": 284658, "start": 2849.5, "end": 2854.2999999999997, "text": " So this is not quite cheating because multi-processing is in the standard library and this is API", "tokens": [50510, 407, 341, 307, 406, 1596, 18309, 570, 4825, 12, 41075, 278, 307, 294, 264, 3832, 6405, 293, 341, 307, 9362, 50750], "temperature": 0.0, "avg_logprob": -0.24289728670704122, "compression_ratio": 1.6, "no_speech_prob": 2.2827991415397264e-05}, {"id": 608, "seek": 284658, "start": 2854.2999999999997, "end": 2855.2999999999997, "text": " equivalent.", "tokens": [50750, 10344, 13, 50800], "temperature": 0.0, "avg_logprob": -0.24289728670704122, "compression_ratio": 1.6, "no_speech_prob": 2.2827991415397264e-05}, {"id": 609, "seek": 284658, "start": 2855.2999999999997, "end": 2858.94, "text": " So I'm going to say we're allowed to do that.", "tokens": [50800, 407, 286, 478, 516, 281, 584, 321, 434, 4350, 281, 360, 300, 13, 50982], "temperature": 0.0, "avg_logprob": -0.24289728670704122, "compression_ratio": 1.6, "no_speech_prob": 2.2827991415397264e-05}, {"id": 610, "seek": 284658, "start": 2858.94, "end": 2868.7799999999997, "text": " So as we've discussed, you know, when we call square brackets on a class, it's actually", "tokens": [50982, 407, 382, 321, 600, 7152, 11, 291, 458, 11, 562, 321, 818, 3732, 26179, 322, 257, 1508, 11, 309, 311, 767, 51474], "temperature": 0.0, "avg_logprob": -0.24289728670704122, "compression_ratio": 1.6, "no_speech_prob": 2.2827991415397264e-05}, {"id": 611, "seek": 284658, "start": 2868.7799999999997, "end": 2875.86, "text": " identical to calling the dunder get item function on the object.", "tokens": [51474, 14800, 281, 5141, 264, 274, 6617, 483, 3174, 2445, 322, 264, 2657, 13, 51828], "temperature": 0.0, "avg_logprob": -0.24289728670704122, "compression_ratio": 1.6, "no_speech_prob": 2.2827991415397264e-05}, {"id": 612, "seek": 287586, "start": 2875.9, "end": 2882.58, "text": " So you can see here, if we say give me items 3, 6, 8, and 1, it's the same as calling dunder", "tokens": [50366, 407, 291, 393, 536, 510, 11, 498, 321, 584, 976, 385, 4754, 805, 11, 1386, 11, 1649, 11, 293, 502, 11, 309, 311, 264, 912, 382, 5141, 274, 6617, 50700], "temperature": 0.0, "avg_logprob": -0.23136991773332868, "compression_ratio": 1.6192170818505338, "no_speech_prob": 3.0415926630666945e-06}, {"id": 613, "seek": 287586, "start": 2882.58, "end": 2888.1800000000003, "text": " get item passing in 3, 6, 8, and 1.", "tokens": [50700, 483, 3174, 8437, 294, 805, 11, 1386, 11, 1649, 11, 293, 502, 13, 50980], "temperature": 0.0, "avg_logprob": -0.23136991773332868, "compression_ratio": 1.6192170818505338, "no_speech_prob": 3.0415926630666945e-06}, {"id": 614, "seek": 287586, "start": 2888.1800000000003, "end": 2889.54, "text": " Now why does this matter?", "tokens": [50980, 823, 983, 775, 341, 1871, 30, 51048], "temperature": 0.0, "avg_logprob": -0.23136991773332868, "compression_ratio": 1.6192170818505338, "no_speech_prob": 3.0415926630666945e-06}, {"id": 615, "seek": 287586, "start": 2889.54, "end": 2892.1800000000003, "text": " Well, I'll show you why.", "tokens": [51048, 1042, 11, 286, 603, 855, 291, 983, 13, 51180], "temperature": 0.0, "avg_logprob": -0.23136991773332868, "compression_ratio": 1.6192170818505338, "no_speech_prob": 3.0415926630666945e-06}, {"id": 616, "seek": 287586, "start": 2892.1800000000003, "end": 2895.46, "text": " It matters because we're going to be able to use map and I'll explain why we want to", "tokens": [51180, 467, 7001, 570, 321, 434, 516, 281, 312, 1075, 281, 764, 4471, 293, 286, 603, 2903, 983, 321, 528, 281, 51344], "temperature": 0.0, "avg_logprob": -0.23136991773332868, "compression_ratio": 1.6192170818505338, "no_speech_prob": 3.0415926630666945e-06}, {"id": 617, "seek": 287586, "start": 2895.46, "end": 2896.6200000000003, "text": " use map in a moment.", "tokens": [51344, 764, 4471, 294, 257, 1623, 13, 51402], "temperature": 0.0, "avg_logprob": -0.23136991773332868, "compression_ratio": 1.6192170818505338, "no_speech_prob": 3.0415926630666945e-06}, {"id": 618, "seek": 287586, "start": 2896.6200000000003, "end": 2898.78, "text": " Map is a really important concept.", "tokens": [51402, 22053, 307, 257, 534, 1021, 3410, 13, 51510], "temperature": 0.0, "avg_logprob": -0.23136991773332868, "compression_ratio": 1.6192170818505338, "no_speech_prob": 3.0415926630666945e-06}, {"id": 619, "seek": 287586, "start": 2898.78, "end": 2900.2200000000003, "text": " You might have heard of map reduce.", "tokens": [51510, 509, 1062, 362, 2198, 295, 4471, 5407, 13, 51582], "temperature": 0.0, "avg_logprob": -0.23136991773332868, "compression_ratio": 1.6192170818505338, "no_speech_prob": 3.0415926630666945e-06}, {"id": 620, "seek": 287586, "start": 2900.2200000000003, "end": 2903.6200000000003, "text": " So we've already talked about reductions and what those are.", "tokens": [51582, 407, 321, 600, 1217, 2825, 466, 40296, 293, 437, 729, 366, 13, 51752], "temperature": 0.0, "avg_logprob": -0.23136991773332868, "compression_ratio": 1.6192170818505338, "no_speech_prob": 3.0415926630666945e-06}, {"id": 621, "seek": 287586, "start": 2903.6200000000003, "end": 2905.38, "text": " Maps are kind of the other key piece.", "tokens": [51752, 28978, 366, 733, 295, 264, 661, 2141, 2522, 13, 51840], "temperature": 0.0, "avg_logprob": -0.23136991773332868, "compression_ratio": 1.6192170818505338, "no_speech_prob": 3.0415926630666945e-06}, {"id": 622, "seek": 290538, "start": 2905.9, "end": 2912.06, "text": " So a map is something which takes a sequence and calls a function on every element of that", "tokens": [50390, 407, 257, 4471, 307, 746, 597, 2516, 257, 8310, 293, 5498, 257, 2445, 322, 633, 4478, 295, 300, 50698], "temperature": 0.0, "avg_logprob": -0.2521861033006148, "compression_ratio": 1.675392670157068, "no_speech_prob": 0.0010987253626808524}, {"id": 623, "seek": 290538, "start": 2912.06, "end": 2913.06, "text": " sequence.", "tokens": [50698, 8310, 13, 50748], "temperature": 0.0, "avg_logprob": -0.2521861033006148, "compression_ratio": 1.675392670157068, "no_speech_prob": 0.0010987253626808524}, {"id": 624, "seek": 290538, "start": 2913.06, "end": 2920.34, "text": " So imagine we had a couple of batches of indices 3 and 6 and 8 and 1.", "tokens": [50748, 407, 3811, 321, 632, 257, 1916, 295, 15245, 279, 295, 43840, 805, 293, 1386, 293, 1649, 293, 502, 13, 51112], "temperature": 0.0, "avg_logprob": -0.2521861033006148, "compression_ratio": 1.675392670157068, "no_speech_prob": 0.0010987253626808524}, {"id": 625, "seek": 290538, "start": 2920.34, "end": 2925.3, "text": " Then we're going to call dunder get item on each of those batches.", "tokens": [51112, 1396, 321, 434, 516, 281, 818, 274, 6617, 483, 3174, 322, 1184, 295, 729, 15245, 279, 13, 51360], "temperature": 0.0, "avg_logprob": -0.2521861033006148, "compression_ratio": 1.675392670157068, "no_speech_prob": 0.0010987253626808524}, {"id": 626, "seek": 290538, "start": 2925.3, "end": 2927.34, "text": " So that's what map does.", "tokens": [51360, 407, 300, 311, 437, 4471, 775, 13, 51462], "temperature": 0.0, "avg_logprob": -0.2521861033006148, "compression_ratio": 1.675392670157068, "no_speech_prob": 0.0010987253626808524}, {"id": 627, "seek": 290538, "start": 2927.34, "end": 2932.42, "text": " Map calls this function on every element of the sequence.", "tokens": [51462, 22053, 5498, 341, 2445, 322, 633, 4478, 295, 264, 8310, 13, 51716], "temperature": 0.0, "avg_logprob": -0.2521861033006148, "compression_ratio": 1.675392670157068, "no_speech_prob": 0.0010987253626808524}, {"id": 628, "seek": 293242, "start": 2932.46, "end": 2939.34, "text": " So that's going to give us the same stuff, but now this same as this, but now batched", "tokens": [50366, 407, 300, 311, 516, 281, 976, 505, 264, 912, 1507, 11, 457, 586, 341, 912, 382, 341, 11, 457, 586, 15245, 292, 50710], "temperature": 0.0, "avg_logprob": -0.2743767464515006, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00011235270358156413}, {"id": 629, "seek": 293242, "start": 2939.34, "end": 2942.02, "text": " into two batches.", "tokens": [50710, 666, 732, 15245, 279, 13, 50844], "temperature": 0.0, "avg_logprob": -0.2743767464515006, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00011235270358156413}, {"id": 630, "seek": 293242, "start": 2942.02, "end": 2944.3, "text": " Now why do we want to do that?", "tokens": [50844, 823, 983, 360, 321, 528, 281, 360, 300, 30, 50958], "temperature": 0.0, "avg_logprob": -0.2743767464515006, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00011235270358156413}, {"id": 631, "seek": 293242, "start": 2944.3, "end": 2950.3, "text": " Because multi-processing has something called pool, where you can tell it how many workers", "tokens": [50958, 1436, 4825, 12, 41075, 278, 575, 746, 1219, 7005, 11, 689, 291, 393, 980, 309, 577, 867, 5600, 51258], "temperature": 0.0, "avg_logprob": -0.2743767464515006, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00011235270358156413}, {"id": 632, "seek": 293242, "start": 2950.3, "end": 2953.66, "text": " you want to run, how many processes you want to run.", "tokens": [51258, 291, 528, 281, 1190, 11, 577, 867, 7555, 291, 528, 281, 1190, 13, 51426], "temperature": 0.0, "avg_logprob": -0.2743767464515006, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00011235270358156413}, {"id": 633, "seek": 293242, "start": 2953.66, "end": 2960.06, "text": " And it then has a map which works just like the Python, normal Python map, but it runs", "tokens": [51426, 400, 309, 550, 575, 257, 4471, 597, 1985, 445, 411, 264, 15329, 11, 2710, 15329, 4471, 11, 457, 309, 6676, 51746], "temperature": 0.0, "avg_logprob": -0.2743767464515006, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00011235270358156413}, {"id": 634, "seek": 296006, "start": 2960.06, "end": 2966.72, "text": " this function in parallel over the items from this iterator.", "tokens": [50364, 341, 2445, 294, 8952, 670, 264, 4754, 490, 341, 17138, 1639, 13, 50697], "temperature": 0.0, "avg_logprob": -0.2995447593160195, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.009708362631499767}, {"id": 635, "seek": 296006, "start": 2966.72, "end": 2972.2599999999998, "text": " So this is how we can create a multi-processing data loader.", "tokens": [50697, 407, 341, 307, 577, 321, 393, 1884, 257, 4825, 12, 41075, 278, 1412, 3677, 260, 13, 50974], "temperature": 0.0, "avg_logprob": -0.2995447593160195, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.009708362631499767}, {"id": 636, "seek": 296006, "start": 2972.2599999999998, "end": 2975.94, "text": " So here we're creating our data loader.", "tokens": [50974, 407, 510, 321, 434, 4084, 527, 1412, 3677, 260, 13, 51158], "temperature": 0.0, "avg_logprob": -0.2995447593160195, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.009708362631499767}, {"id": 637, "seek": 296006, "start": 2975.94, "end": 2979.06, "text": " Again we don't actually need to pass in the collect function because we're using the default", "tokens": [51158, 3764, 321, 500, 380, 767, 643, 281, 1320, 294, 264, 2500, 2445, 570, 321, 434, 1228, 264, 7576, 51314], "temperature": 0.0, "avg_logprob": -0.2995447593160195, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.009708362631499767}, {"id": 638, "seek": 296006, "start": 2979.06, "end": 2980.06, "text": " one.", "tokens": [51314, 472, 13, 51364], "temperature": 0.0, "avg_logprob": -0.2995447593160195, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.009708362631499767}, {"id": 639, "seek": 296006, "start": 2980.06, "end": 2982.94, "text": " So if we say n workers equals 2, and then create that.", "tokens": [51364, 407, 498, 321, 584, 297, 5600, 6915, 568, 11, 293, 550, 1884, 300, 13, 51508], "temperature": 0.0, "avg_logprob": -0.2995447593160195, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.009708362631499767}, {"id": 640, "seek": 296006, "start": 2982.94, "end": 2987.18, "text": " If we say next, see how it's taking a moment?", "tokens": [51508, 759, 321, 584, 958, 11, 536, 577, 309, 311, 1940, 257, 1623, 30, 51720], "temperature": 0.0, "avg_logprob": -0.2995447593160195, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.009708362631499767}, {"id": 641, "seek": 298718, "start": 2987.18, "end": 2991.14, "text": " It took a moment because it was firing off those two workers in the background.", "tokens": [50364, 467, 1890, 257, 1623, 570, 309, 390, 16045, 766, 729, 732, 5600, 294, 264, 3678, 13, 50562], "temperature": 0.0, "avg_logprob": -0.23356203825577446, "compression_ratio": 1.7529411764705882, "no_speech_prob": 0.00013341967132873833}, {"id": 642, "seek": 298718, "start": 2991.14, "end": 2993.8599999999997, "text": " So the first batch actually comes out more slowly.", "tokens": [50562, 407, 264, 700, 15245, 767, 1487, 484, 544, 5692, 13, 50698], "temperature": 0.0, "avg_logprob": -0.23356203825577446, "compression_ratio": 1.7529411764705882, "no_speech_prob": 0.00013341967132873833}, {"id": 643, "seek": 298718, "start": 2993.8599999999997, "end": 3000.1, "text": " But the reason that we would use a multi-processing data loader is if this is doing a lot of work,", "tokens": [50698, 583, 264, 1778, 300, 321, 576, 764, 257, 4825, 12, 41075, 278, 1412, 3677, 260, 307, 498, 341, 307, 884, 257, 688, 295, 589, 11, 51010], "temperature": 0.0, "avg_logprob": -0.23356203825577446, "compression_ratio": 1.7529411764705882, "no_speech_prob": 0.00013341967132873833}, {"id": 644, "seek": 298718, "start": 3000.1, "end": 3007.46, "text": " we want it to run in parallel, and even though the first item might come out a bit slower,", "tokens": [51010, 321, 528, 309, 281, 1190, 294, 8952, 11, 293, 754, 1673, 264, 700, 3174, 1062, 808, 484, 257, 857, 14009, 11, 51378], "temperature": 0.0, "avg_logprob": -0.23356203825577446, "compression_ratio": 1.7529411764705882, "no_speech_prob": 0.00013341967132873833}, {"id": 645, "seek": 298718, "start": 3007.46, "end": 3011.3799999999997, "text": " once those processes are fired up it's going to be faster to run.", "tokens": [51378, 1564, 729, 7555, 366, 11777, 493, 309, 311, 516, 281, 312, 4663, 281, 1190, 13, 51574], "temperature": 0.0, "avg_logprob": -0.23356203825577446, "compression_ratio": 1.7529411764705882, "no_speech_prob": 0.00013341967132873833}, {"id": 646, "seek": 298718, "start": 3011.3799999999997, "end": 3016.18, "text": " So this is a really simplified multi-processing data loader.", "tokens": [51574, 407, 341, 307, 257, 534, 26335, 4825, 12, 41075, 278, 1412, 3677, 260, 13, 51814], "temperature": 0.0, "avg_logprob": -0.23356203825577446, "compression_ratio": 1.7529411764705882, "no_speech_prob": 0.00013341967132873833}, {"id": 647, "seek": 301618, "start": 3016.18, "end": 3022.7, "text": " Because this needs to be super super efficient, PyTorch has lots more code than this to make", "tokens": [50364, 1436, 341, 2203, 281, 312, 1687, 1687, 7148, 11, 9953, 51, 284, 339, 575, 3195, 544, 3089, 813, 341, 281, 652, 50690], "temperature": 0.0, "avg_logprob": -0.22929164582649164, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00033534984686411917}, {"id": 648, "seek": 301618, "start": 3022.7, "end": 3025.14, "text": " it much more efficient.", "tokens": [50690, 309, 709, 544, 7148, 13, 50812], "temperature": 0.0, "avg_logprob": -0.22929164582649164, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00033534984686411917}, {"id": 649, "seek": 301618, "start": 3025.14, "end": 3031.52, "text": " But the idea is this, and this is actually a perfectly good way of experimenting or building", "tokens": [50812, 583, 264, 1558, 307, 341, 11, 293, 341, 307, 767, 257, 6239, 665, 636, 295, 29070, 420, 2390, 51131], "temperature": 0.0, "avg_logprob": -0.22929164582649164, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00033534984686411917}, {"id": 650, "seek": 301618, "start": 3031.52, "end": 3036.8399999999997, "text": " your own data loader to make things work exactly how you want.", "tokens": [51131, 428, 1065, 1412, 3677, 260, 281, 652, 721, 589, 2293, 577, 291, 528, 13, 51397], "temperature": 0.0, "avg_logprob": -0.22929164582649164, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00033534984686411917}, {"id": 651, "seek": 301618, "start": 3036.8399999999997, "end": 3041.8599999999997, "text": " So now that we've re-implemented all this from PyTorch, let's just grab the PyTorch,", "tokens": [51397, 407, 586, 300, 321, 600, 319, 12, 332, 781, 14684, 439, 341, 490, 9953, 51, 284, 339, 11, 718, 311, 445, 4444, 264, 9953, 51, 284, 339, 11, 51648], "temperature": 0.0, "avg_logprob": -0.22929164582649164, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00033534984686411917}, {"id": 652, "seek": 301618, "start": 3041.8599999999997, "end": 3045.02, "text": " as you can see they're exactly the same data loader.", "tokens": [51648, 382, 291, 393, 536, 436, 434, 2293, 264, 912, 1412, 3677, 260, 13, 51806], "temperature": 0.0, "avg_logprob": -0.22929164582649164, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00033534984686411917}, {"id": 653, "seek": 304502, "start": 3045.02, "end": 3048.46, "text": " They don't have one thing called sampler that you pass shuffle to, they have two separate", "tokens": [50364, 814, 500, 380, 362, 472, 551, 1219, 3247, 22732, 300, 291, 1320, 39426, 281, 11, 436, 362, 732, 4994, 50536], "temperature": 0.0, "avg_logprob": -0.2606352930483611, "compression_ratio": 1.9423076923076923, "no_speech_prob": 0.000310155184706673}, {"id": 654, "seek": 304502, "start": 3048.46, "end": 3050.9, "text": " classes called sequential sampler and random sampler.", "tokens": [50536, 5359, 1219, 42881, 3247, 22732, 293, 4974, 3247, 22732, 13, 50658], "temperature": 0.0, "avg_logprob": -0.2606352930483611, "compression_ratio": 1.9423076923076923, "no_speech_prob": 0.000310155184706673}, {"id": 655, "seek": 304502, "start": 3050.9, "end": 3055.82, "text": " I don't know why they do it that way, it's a little bit more work to me, but same idea.", "tokens": [50658, 286, 500, 380, 458, 983, 436, 360, 309, 300, 636, 11, 309, 311, 257, 707, 857, 544, 589, 281, 385, 11, 457, 912, 1558, 13, 50904], "temperature": 0.0, "avg_logprob": -0.2606352930483611, "compression_ratio": 1.9423076923076923, "no_speech_prob": 0.000310155184706673}, {"id": 656, "seek": 304502, "start": 3055.82, "end": 3059.3, "text": " And I've got batch sampler, and so it's exactly the same idea.", "tokens": [50904, 400, 286, 600, 658, 15245, 3247, 22732, 11, 293, 370, 309, 311, 2293, 264, 912, 1558, 13, 51078], "temperature": 0.0, "avg_logprob": -0.2606352930483611, "compression_ratio": 1.9423076923076923, "no_speech_prob": 0.000310155184706673}, {"id": 657, "seek": 304502, "start": 3059.3, "end": 3063.74, "text": " The training sampler is a batch sampler with a random sampler.", "tokens": [51078, 440, 3097, 3247, 22732, 307, 257, 15245, 3247, 22732, 365, 257, 4974, 3247, 22732, 13, 51300], "temperature": 0.0, "avg_logprob": -0.2606352930483611, "compression_ratio": 1.9423076923076923, "no_speech_prob": 0.000310155184706673}, {"id": 658, "seek": 304502, "start": 3063.74, "end": 3067.7, "text": " The validation sampler is a batch sampler with a sequential sampler.", "tokens": [51300, 440, 24071, 3247, 22732, 307, 257, 15245, 3247, 22732, 365, 257, 42881, 3247, 22732, 13, 51498], "temperature": 0.0, "avg_logprob": -0.2606352930483611, "compression_ratio": 1.9423076923076923, "no_speech_prob": 0.000310155184706673}, {"id": 659, "seek": 304502, "start": 3067.7, "end": 3074.7, "text": " Passing batch sizes, and so we can now pass those samplers to the data loader.", "tokens": [51498, 10319, 278, 15245, 11602, 11, 293, 370, 321, 393, 586, 1320, 729, 3247, 564, 433, 281, 264, 1412, 3677, 260, 13, 51848], "temperature": 0.0, "avg_logprob": -0.2606352930483611, "compression_ratio": 1.9423076923076923, "no_speech_prob": 0.000310155184706673}, {"id": 660, "seek": 307470, "start": 3074.7, "end": 3082.62, "text": " This is now the PyTorch data loader, and just like ours, it also takes a collate function.", "tokens": [50364, 639, 307, 586, 264, 9953, 51, 284, 339, 1412, 3677, 260, 11, 293, 445, 411, 11896, 11, 309, 611, 2516, 257, 1263, 473, 2445, 13, 50760], "temperature": 0.0, "avg_logprob": -0.2900337806114784, "compression_ratio": 1.538860103626943, "no_speech_prob": 1.8631732018548064e-05}, {"id": 661, "seek": 307470, "start": 3082.62, "end": 3088.58, "text": " And it works.", "tokens": [50760, 400, 309, 1985, 13, 51058], "temperature": 0.0, "avg_logprob": -0.2900337806114784, "compression_ratio": 1.538860103626943, "no_speech_prob": 1.8631732018548064e-05}, {"id": 662, "seek": 307470, "start": 3088.58, "end": 3092.18, "text": " Cool.", "tokens": [51058, 8561, 13, 51238], "temperature": 0.0, "avg_logprob": -0.2900337806114784, "compression_ratio": 1.538860103626943, "no_speech_prob": 1.8631732018548064e-05}, {"id": 663, "seek": 307470, "start": 3092.18, "end": 3096.3399999999997, "text": " So as you can see, it's doing exactly the same stuff that ours is doing, with exactly", "tokens": [51238, 407, 382, 291, 393, 536, 11, 309, 311, 884, 2293, 264, 912, 1507, 300, 11896, 307, 884, 11, 365, 2293, 51446], "temperature": 0.0, "avg_logprob": -0.2900337806114784, "compression_ratio": 1.538860103626943, "no_speech_prob": 1.8631732018548064e-05}, {"id": 664, "seek": 307470, "start": 3096.3399999999997, "end": 3098.58, "text": " the same API.", "tokens": [51446, 264, 912, 9362, 13, 51558], "temperature": 0.0, "avg_logprob": -0.2900337806114784, "compression_ratio": 1.538860103626943, "no_speech_prob": 1.8631732018548064e-05}, {"id": 665, "seek": 307470, "start": 3098.58, "end": 3103.62, "text": " And it's got some shortcuts, as I'm sure you've noticed when you've used data loaders.", "tokens": [51558, 400, 309, 311, 658, 512, 34620, 11, 382, 286, 478, 988, 291, 600, 5694, 562, 291, 600, 1143, 1412, 3677, 433, 13, 51810], "temperature": 0.0, "avg_logprob": -0.2900337806114784, "compression_ratio": 1.538860103626943, "no_speech_prob": 1.8631732018548064e-05}, {"id": 666, "seek": 310362, "start": 3103.62, "end": 3110.42, "text": " So for example, calling batch sampler is going to be very very common.", "tokens": [50364, 407, 337, 1365, 11, 5141, 15245, 3247, 22732, 307, 516, 281, 312, 588, 588, 2689, 13, 50704], "temperature": 0.0, "avg_logprob": -0.25027746624416775, "compression_ratio": 1.8413461538461537, "no_speech_prob": 5.144209717400372e-05}, {"id": 667, "seek": 310362, "start": 3110.42, "end": 3116.14, "text": " So you can actually just pass the batch size directly to a data loader, and it will then", "tokens": [50704, 407, 291, 393, 767, 445, 1320, 264, 15245, 2744, 3838, 281, 257, 1412, 3677, 260, 11, 293, 309, 486, 550, 50990], "temperature": 0.0, "avg_logprob": -0.25027746624416775, "compression_ratio": 1.8413461538461537, "no_speech_prob": 5.144209717400372e-05}, {"id": 668, "seek": 310362, "start": 3116.14, "end": 3118.14, "text": " auto-create the batch samplers for you.", "tokens": [50990, 8399, 12, 14066, 473, 264, 15245, 3247, 564, 433, 337, 291, 13, 51090], "temperature": 0.0, "avg_logprob": -0.25027746624416775, "compression_ratio": 1.8413461538461537, "no_speech_prob": 5.144209717400372e-05}, {"id": 669, "seek": 310362, "start": 3118.14, "end": 3122.22, "text": " So you don't have to pass in batch sampler at all.", "tokens": [51090, 407, 291, 500, 380, 362, 281, 1320, 294, 15245, 3247, 22732, 412, 439, 13, 51294], "temperature": 0.0, "avg_logprob": -0.25027746624416775, "compression_ratio": 1.8413461538461537, "no_speech_prob": 5.144209717400372e-05}, {"id": 670, "seek": 310362, "start": 3122.22, "end": 3125.9, "text": " Instead you can just say sampler, and it will automatically wrap that in a batch sampler", "tokens": [51294, 7156, 291, 393, 445, 584, 3247, 22732, 11, 293, 309, 486, 6772, 7019, 300, 294, 257, 15245, 3247, 22732, 51478], "temperature": 0.0, "avg_logprob": -0.25027746624416775, "compression_ratio": 1.8413461538461537, "no_speech_prob": 5.144209717400372e-05}, {"id": 671, "seek": 310362, "start": 3125.9, "end": 3126.9, "text": " for you.", "tokens": [51478, 337, 291, 13, 51528], "temperature": 0.0, "avg_logprob": -0.25027746624416775, "compression_ratio": 1.8413461538461537, "no_speech_prob": 5.144209717400372e-05}, {"id": 672, "seek": 310362, "start": 3126.9, "end": 3129.18, "text": " So it does exactly the same thing.", "tokens": [51528, 407, 309, 775, 2293, 264, 912, 551, 13, 51642], "temperature": 0.0, "avg_logprob": -0.25027746624416775, "compression_ratio": 1.8413461538461537, "no_speech_prob": 5.144209717400372e-05}, {"id": 673, "seek": 312918, "start": 3129.18, "end": 3133.8199999999997, "text": " And in fact, because it's so common to create a random sampler, or a sequential sampler", "tokens": [50364, 400, 294, 1186, 11, 570, 309, 311, 370, 2689, 281, 1884, 257, 4974, 3247, 22732, 11, 420, 257, 42881, 3247, 22732, 50596], "temperature": 0.0, "avg_logprob": -0.24486423492431642, "compression_ratio": 1.6288209606986899, "no_speech_prob": 0.0001971669844351709}, {"id": 674, "seek": 312918, "start": 3133.8199999999997, "end": 3136.54, "text": " for a data set, you don't have to do that manually.", "tokens": [50596, 337, 257, 1412, 992, 11, 291, 500, 380, 362, 281, 360, 300, 16945, 13, 50732], "temperature": 0.0, "avg_logprob": -0.24486423492431642, "compression_ratio": 1.6288209606986899, "no_speech_prob": 0.0001971669844351709}, {"id": 675, "seek": 312918, "start": 3136.54, "end": 3140.58, "text": " You can just pass in shuffle equals true, or shuffle equals false to the data loader.", "tokens": [50732, 509, 393, 445, 1320, 294, 39426, 6915, 2074, 11, 420, 39426, 6915, 7908, 281, 264, 1412, 3677, 260, 13, 50934], "temperature": 0.0, "avg_logprob": -0.24486423492431642, "compression_ratio": 1.6288209606986899, "no_speech_prob": 0.0001971669844351709}, {"id": 676, "seek": 312918, "start": 3140.58, "end": 3144.74, "text": " And that does again exactly the same thing.", "tokens": [50934, 400, 300, 775, 797, 2293, 264, 912, 551, 13, 51142], "temperature": 0.0, "avg_logprob": -0.24486423492431642, "compression_ratio": 1.6288209606986899, "no_speech_prob": 0.0001971669844351709}, {"id": 677, "seek": 312918, "start": 3144.74, "end": 3147.4199999999996, "text": " There it is.", "tokens": [51142, 821, 309, 307, 13, 51276], "temperature": 0.0, "avg_logprob": -0.24486423492431642, "compression_ratio": 1.6288209606986899, "no_speech_prob": 0.0001971669844351709}, {"id": 678, "seek": 312918, "start": 3147.4199999999996, "end": 3156.7799999999997, "text": " Now something that is very interesting, is that when you think about it, the batch sampler", "tokens": [51276, 823, 746, 300, 307, 588, 1880, 11, 307, 300, 562, 291, 519, 466, 309, 11, 264, 15245, 3247, 22732, 51744], "temperature": 0.0, "avg_logprob": -0.24486423492431642, "compression_ratio": 1.6288209606986899, "no_speech_prob": 0.0001971669844351709}, {"id": 679, "seek": 315678, "start": 3156.94, "end": 3163.7400000000002, "text": " and the collation function are things which are taking the result of the sampler, looping", "tokens": [50372, 293, 264, 1263, 399, 2445, 366, 721, 597, 366, 1940, 264, 1874, 295, 264, 3247, 22732, 11, 6367, 278, 50712], "temperature": 0.0, "avg_logprob": -0.2329178896817294, "compression_ratio": 1.5133333333333334, "no_speech_prob": 8.801096555544063e-06}, {"id": 680, "seek": 315678, "start": 3163.7400000000002, "end": 3167.42, "text": " through them, and then collating them together.", "tokens": [50712, 807, 552, 11, 293, 550, 1263, 990, 552, 1214, 13, 50896], "temperature": 0.0, "avg_logprob": -0.2329178896817294, "compression_ratio": 1.5133333333333334, "no_speech_prob": 8.801096555544063e-06}, {"id": 681, "seek": 315678, "start": 3167.42, "end": 3177.38, "text": " But what we could do, is actually because our data sets know how to grab multiple indices", "tokens": [50896, 583, 437, 321, 727, 360, 11, 307, 767, 570, 527, 1412, 6352, 458, 577, 281, 4444, 3866, 43840, 51394], "temperature": 0.0, "avg_logprob": -0.2329178896817294, "compression_ratio": 1.5133333333333334, "no_speech_prob": 8.801096555544063e-06}, {"id": 682, "seek": 317738, "start": 3177.38, "end": 3190.5, "text": " at once, we can actually just use the batch sampler as a sampler.", "tokens": [50364, 412, 1564, 11, 321, 393, 767, 445, 764, 264, 15245, 3247, 22732, 382, 257, 3247, 22732, 13, 51020], "temperature": 0.0, "avg_logprob": -0.24720639358332128, "compression_ratio": 1.5265957446808511, "no_speech_prob": 0.01322230789810419}, {"id": 683, "seek": 317738, "start": 3190.5, "end": 3195.02, "text": " We don't actually have to loop through them and collate them, because they're basically", "tokens": [51020, 492, 500, 380, 767, 362, 281, 6367, 807, 552, 293, 1263, 473, 552, 11, 570, 436, 434, 1936, 51246], "temperature": 0.0, "avg_logprob": -0.24720639358332128, "compression_ratio": 1.5265957446808511, "no_speech_prob": 0.01322230789810419}, {"id": 684, "seek": 317738, "start": 3195.02, "end": 3199.9, "text": " instantly, they come pre-collated.", "tokens": [51246, 13518, 11, 436, 808, 659, 12, 33891, 770, 13, 51490], "temperature": 0.0, "avg_logprob": -0.24720639358332128, "compression_ratio": 1.5265957446808511, "no_speech_prob": 0.01322230789810419}, {"id": 685, "seek": 317738, "start": 3199.9, "end": 3203.7000000000003, "text": " So this is a trick which actually HuggingFace stuff can use as well, and we'll be seeing", "tokens": [51490, 407, 341, 307, 257, 4282, 597, 767, 46892, 3249, 37, 617, 1507, 393, 764, 382, 731, 11, 293, 321, 603, 312, 2577, 51680], "temperature": 0.0, "avg_logprob": -0.24720639358332128, "compression_ratio": 1.5265957446808511, "no_speech_prob": 0.01322230789810419}, {"id": 686, "seek": 317738, "start": 3203.7000000000003, "end": 3204.82, "text": " it again.", "tokens": [51680, 309, 797, 13, 51736], "temperature": 0.0, "avg_logprob": -0.24720639358332128, "compression_ratio": 1.5265957446808511, "no_speech_prob": 0.01322230789810419}, {"id": 687, "seek": 320482, "start": 3204.82, "end": 3209.6600000000003, "text": " So this is an important thing to understand, is how come we can pass a batch sampler to", "tokens": [50364, 407, 341, 307, 364, 1021, 551, 281, 1223, 11, 307, 577, 808, 321, 393, 1320, 257, 15245, 3247, 22732, 281, 50606], "temperature": 0.0, "avg_logprob": -0.23915979385375977, "compression_ratio": 1.5708154506437768, "no_speech_prob": 0.0007321739685721695}, {"id": 688, "seek": 320482, "start": 3209.6600000000003, "end": 3211.46, "text": " sampler, and what's it doing?", "tokens": [50606, 3247, 22732, 11, 293, 437, 311, 309, 884, 30, 50696], "temperature": 0.0, "avg_logprob": -0.23915979385375977, "compression_ratio": 1.5708154506437768, "no_speech_prob": 0.0007321739685721695}, {"id": 689, "seek": 320482, "start": 3211.46, "end": 3215.1000000000004, "text": " And so rather than trying to look through the PyTorch code, I suggest going back to", "tokens": [50696, 400, 370, 2831, 813, 1382, 281, 574, 807, 264, 9953, 51, 284, 339, 3089, 11, 286, 3402, 516, 646, 281, 50878], "temperature": 0.0, "avg_logprob": -0.23915979385375977, "compression_ratio": 1.5708154506437768, "no_speech_prob": 0.0007321739685721695}, {"id": 690, "seek": 320482, "start": 3215.1000000000004, "end": 3223.3, "text": " our non-multiprocessing pure Python code, to see exactly how that would work.", "tokens": [50878, 527, 2107, 12, 76, 723, 647, 340, 780, 278, 6075, 15329, 3089, 11, 281, 536, 2293, 577, 300, 576, 589, 13, 51288], "temperature": 0.0, "avg_logprob": -0.23915979385375977, "compression_ratio": 1.5708154506437768, "no_speech_prob": 0.0007321739685721695}, {"id": 691, "seek": 320482, "start": 3223.3, "end": 3229.38, "text": " Because it's a really nifty trick for things that you can grab multiple things from at", "tokens": [51288, 1436, 309, 311, 257, 534, 297, 37177, 4282, 337, 721, 300, 291, 393, 4444, 3866, 721, 490, 412, 51592], "temperature": 0.0, "avg_logprob": -0.23915979385375977, "compression_ratio": 1.5708154506437768, "no_speech_prob": 0.0007321739685721695}, {"id": 692, "seek": 322938, "start": 3229.38, "end": 3234.86, "text": " once, and it can save a whole lot of time, it can make your code a lot faster.", "tokens": [50364, 1564, 11, 293, 309, 393, 3155, 257, 1379, 688, 295, 565, 11, 309, 393, 652, 428, 3089, 257, 688, 4663, 13, 50638], "temperature": 0.0, "avg_logprob": -0.2561946868896484, "compression_ratio": 1.6527196652719665, "no_speech_prob": 0.0518437959253788}, {"id": 693, "seek": 322938, "start": 3234.86, "end": 3242.42, "text": " OK, so now that we've got all that nicely implemented, we should now add a validation", "tokens": [50638, 2264, 11, 370, 586, 300, 321, 600, 658, 439, 300, 9594, 12270, 11, 321, 820, 586, 909, 257, 24071, 51016], "temperature": 0.0, "avg_logprob": -0.2561946868896484, "compression_ratio": 1.6527196652719665, "no_speech_prob": 0.0518437959253788}, {"id": 694, "seek": 322938, "start": 3242.42, "end": 3243.42, "text": " set.", "tokens": [51016, 992, 13, 51066], "temperature": 0.0, "avg_logprob": -0.2561946868896484, "compression_ratio": 1.6527196652719665, "no_speech_prob": 0.0518437959253788}, {"id": 695, "seek": 322938, "start": 3243.42, "end": 3247.42, "text": " And there's not really too much to talk about here, we'll just take our fit function, and", "tokens": [51066, 400, 456, 311, 406, 534, 886, 709, 281, 751, 466, 510, 11, 321, 603, 445, 747, 527, 3318, 2445, 11, 293, 51266], "temperature": 0.0, "avg_logprob": -0.2561946868896484, "compression_ratio": 1.6527196652719665, "no_speech_prob": 0.0518437959253788}, {"id": 696, "seek": 322938, "start": 3247.42, "end": 3252.1400000000003, "text": " this is exactly the same code that we had before.", "tokens": [51266, 341, 307, 2293, 264, 912, 3089, 300, 321, 632, 949, 13, 51502], "temperature": 0.0, "avg_logprob": -0.2561946868896484, "compression_ratio": 1.6527196652719665, "no_speech_prob": 0.0518437959253788}, {"id": 697, "seek": 322938, "start": 3252.1400000000003, "end": 3258.46, "text": " And then we're just going to add something which goes through the validation set, and", "tokens": [51502, 400, 550, 321, 434, 445, 516, 281, 909, 746, 597, 1709, 807, 264, 24071, 992, 11, 293, 51818], "temperature": 0.0, "avg_logprob": -0.2561946868896484, "compression_ratio": 1.6527196652719665, "no_speech_prob": 0.0518437959253788}, {"id": 698, "seek": 325846, "start": 3258.54, "end": 3267.02, "text": " gets the predictions, and sums up the losses and accuracies, and from time to time prints", "tokens": [50368, 2170, 264, 21264, 11, 293, 34499, 493, 264, 15352, 293, 5771, 20330, 11, 293, 490, 565, 281, 565, 22305, 50792], "temperature": 0.0, "avg_logprob": -0.24681472778320312, "compression_ratio": 1.6091954022988506, "no_speech_prob": 6.302744441200048e-05}, {"id": 699, "seek": 325846, "start": 3267.02, "end": 3274.88, "text": " out the loss and accuracy.", "tokens": [50792, 484, 264, 4470, 293, 14170, 13, 51185], "temperature": 0.0, "avg_logprob": -0.24681472778320312, "compression_ratio": 1.6091954022988506, "no_speech_prob": 6.302744441200048e-05}, {"id": 700, "seek": 325846, "start": 3274.88, "end": 3280.7400000000002, "text": " And so getDls we will implement by using the PyTorch data loader now.", "tokens": [51185, 400, 370, 483, 35, 11784, 321, 486, 4445, 538, 1228, 264, 9953, 51, 284, 339, 1412, 3677, 260, 586, 13, 51478], "temperature": 0.0, "avg_logprob": -0.24681472778320312, "compression_ratio": 1.6091954022988506, "no_speech_prob": 6.302744441200048e-05}, {"id": 701, "seek": 325846, "start": 3280.7400000000002, "end": 3286.2200000000003, "text": " And so now our whole process will be getDls, passing in the training and validation data", "tokens": [51478, 400, 370, 586, 527, 1379, 1399, 486, 312, 483, 35, 11784, 11, 8437, 294, 264, 3097, 293, 24071, 1412, 51752], "temperature": 0.0, "avg_logprob": -0.24681472778320312, "compression_ratio": 1.6091954022988506, "no_speech_prob": 6.302744441200048e-05}, {"id": 702, "seek": 325846, "start": 3286.2200000000003, "end": 3287.2200000000003, "text": " set.", "tokens": [51752, 992, 13, 51802], "temperature": 0.0, "avg_logprob": -0.24681472778320312, "compression_ratio": 1.6091954022988506, "no_speech_prob": 6.302744441200048e-05}, {"id": 703, "seek": 328722, "start": 3287.8599999999997, "end": 3291.66, "text": " Notice that for our validation data loader, I'm doubling the batch size, because it doesn't", "tokens": [50396, 13428, 300, 337, 527, 24071, 1412, 3677, 260, 11, 286, 478, 33651, 264, 15245, 2744, 11, 570, 309, 1177, 380, 50586], "temperature": 0.0, "avg_logprob": -0.2398533454308143, "compression_ratio": 1.6072874493927125, "no_speech_prob": 0.00035143704735673964}, {"id": 704, "seek": 328722, "start": 3291.66, "end": 3296.2599999999998, "text": " have to do back propagation, so it should use about half as much memory, so I can use", "tokens": [50586, 362, 281, 360, 646, 38377, 11, 370, 309, 820, 764, 466, 1922, 382, 709, 4675, 11, 370, 286, 393, 764, 50816], "temperature": 0.0, "avg_logprob": -0.2398533454308143, "compression_ratio": 1.6072874493927125, "no_speech_prob": 0.00035143704735673964}, {"id": 705, "seek": 328722, "start": 3296.2599999999998, "end": 3298.98, "text": " a bigger batch size.", "tokens": [50816, 257, 3801, 15245, 2744, 13, 50952], "temperature": 0.0, "avg_logprob": -0.2398533454308143, "compression_ratio": 1.6072874493927125, "no_speech_prob": 0.00035143704735673964}, {"id": 706, "seek": 328722, "start": 3298.98, "end": 3303.4599999999996, "text": " Get our model, and then call this fit.", "tokens": [50952, 3240, 527, 2316, 11, 293, 550, 818, 341, 3318, 13, 51176], "temperature": 0.0, "avg_logprob": -0.2398533454308143, "compression_ratio": 1.6072874493927125, "no_speech_prob": 0.00035143704735673964}, {"id": 707, "seek": 328722, "start": 3303.4599999999996, "end": 3310.22, "text": " And now it's printing out the loss and accuracy on the validation set.", "tokens": [51176, 400, 586, 309, 311, 14699, 484, 264, 4470, 293, 14170, 322, 264, 24071, 992, 13, 51514], "temperature": 0.0, "avg_logprob": -0.2398533454308143, "compression_ratio": 1.6072874493927125, "no_speech_prob": 0.00035143704735673964}, {"id": 708, "seek": 328722, "start": 3310.22, "end": 3315.58, "text": " So finally we actually know how we're doing, which is that we're getting 97% accuracy on", "tokens": [51514, 407, 2721, 321, 767, 458, 577, 321, 434, 884, 11, 597, 307, 300, 321, 434, 1242, 23399, 4, 14170, 322, 51782], "temperature": 0.0, "avg_logprob": -0.2398533454308143, "compression_ratio": 1.6072874493927125, "no_speech_prob": 0.00035143704735673964}, {"id": 709, "seek": 331558, "start": 3315.58, "end": 3318.2599999999998, "text": " the validation set.", "tokens": [50364, 264, 24071, 992, 13, 50498], "temperature": 0.0, "avg_logprob": -0.21342553825021904, "compression_ratio": 1.6756756756756757, "no_speech_prob": 0.001754611381329596}, {"id": 710, "seek": 331558, "start": 3318.2599999999998, "end": 3321.18, "text": " And that's on the whole thing, not just on the last batch.", "tokens": [50498, 400, 300, 311, 322, 264, 1379, 551, 11, 406, 445, 322, 264, 1036, 15245, 13, 50644], "temperature": 0.0, "avg_logprob": -0.21342553825021904, "compression_ratio": 1.6756756756756757, "no_speech_prob": 0.001754611381329596}, {"id": 711, "seek": 331558, "start": 3321.18, "end": 3322.18, "text": " So that's cool.", "tokens": [50644, 407, 300, 311, 1627, 13, 50694], "temperature": 0.0, "avg_logprob": -0.21342553825021904, "compression_ratio": 1.6756756756756757, "no_speech_prob": 0.001754611381329596}, {"id": 712, "seek": 331558, "start": 3322.18, "end": 3330.74, "text": " We've now implemented a proper working, sensible training loop.", "tokens": [50694, 492, 600, 586, 12270, 257, 2296, 1364, 11, 25380, 3097, 6367, 13, 51122], "temperature": 0.0, "avg_logprob": -0.21342553825021904, "compression_ratio": 1.6756756756756757, "no_speech_prob": 0.001754611381329596}, {"id": 713, "seek": 331558, "start": 3330.74, "end": 3334.74, "text": " It's still, you know, a bit more code than I would like, but it's not bad.", "tokens": [51122, 467, 311, 920, 11, 291, 458, 11, 257, 857, 544, 3089, 813, 286, 576, 411, 11, 457, 309, 311, 406, 1578, 13, 51322], "temperature": 0.0, "avg_logprob": -0.21342553825021904, "compression_ratio": 1.6756756756756757, "no_speech_prob": 0.001754611381329596}, {"id": 714, "seek": 331558, "start": 3334.74, "end": 3340.06, "text": " And every line of code in there, and every line of code it's calling, is all stuff that", "tokens": [51322, 400, 633, 1622, 295, 3089, 294, 456, 11, 293, 633, 1622, 295, 3089, 309, 311, 5141, 11, 307, 439, 1507, 300, 51588], "temperature": 0.0, "avg_logprob": -0.21342553825021904, "compression_ratio": 1.6756756756756757, "no_speech_prob": 0.001754611381329596}, {"id": 715, "seek": 331558, "start": 3340.06, "end": 3345.18, "text": " we have built ourselves, re-implemented ourselves.", "tokens": [51588, 321, 362, 3094, 4175, 11, 319, 12, 332, 781, 14684, 4175, 13, 51844], "temperature": 0.0, "avg_logprob": -0.21342553825021904, "compression_ratio": 1.6756756756756757, "no_speech_prob": 0.001754611381329596}, {"id": 716, "seek": 334518, "start": 3345.18, "end": 3348.58, "text": " So we know exactly what's going on, and that means it's going to be much easier for us", "tokens": [50364, 407, 321, 458, 2293, 437, 311, 516, 322, 11, 293, 300, 1355, 309, 311, 516, 281, 312, 709, 3571, 337, 505, 50534], "temperature": 0.0, "avg_logprob": -0.17338711337039345, "compression_ratio": 1.4725274725274726, "no_speech_prob": 0.00041084617259912193}, {"id": 717, "seek": 334518, "start": 3348.58, "end": 3351.98, "text": " to create anything we can think of.", "tokens": [50534, 281, 1884, 1340, 321, 393, 519, 295, 13, 50704], "temperature": 0.0, "avg_logprob": -0.17338711337039345, "compression_ratio": 1.4725274725274726, "no_speech_prob": 0.00041084617259912193}, {"id": 718, "seek": 334518, "start": 3351.98, "end": 3360.3799999999997, "text": " We don't have to rely on other people's code.", "tokens": [50704, 492, 500, 380, 362, 281, 10687, 322, 661, 561, 311, 3089, 13, 51124], "temperature": 0.0, "avg_logprob": -0.17338711337039345, "compression_ratio": 1.4725274725274726, "no_speech_prob": 0.00041084617259912193}, {"id": 719, "seek": 334518, "start": 3360.3799999999997, "end": 3365.8999999999996, "text": " So hopefully you're as excited about that as I am, because it really opens up a whole", "tokens": [51124, 407, 4696, 291, 434, 382, 2919, 466, 300, 382, 286, 669, 11, 570, 309, 534, 9870, 493, 257, 1379, 51400], "temperature": 0.0, "avg_logprob": -0.17338711337039345, "compression_ratio": 1.4725274725274726, "no_speech_prob": 0.00041084617259912193}, {"id": 720, "seek": 334518, "start": 3365.8999999999996, "end": 3371.58, "text": " world for us.", "tokens": [51400, 1002, 337, 505, 13, 51684], "temperature": 0.0, "avg_logprob": -0.17338711337039345, "compression_ratio": 1.4725274725274726, "no_speech_prob": 0.00041084617259912193}, {"id": 721, "seek": 337158, "start": 3371.58, "end": 3375.54, "text": " So one thing that we're going to want to be able to do now that we've got a training loop,", "tokens": [50364, 407, 472, 551, 300, 321, 434, 516, 281, 528, 281, 312, 1075, 281, 360, 586, 300, 321, 600, 658, 257, 3097, 6367, 11, 50562], "temperature": 0.0, "avg_logprob": -0.23065760261134097, "compression_ratio": 1.5810055865921788, "no_speech_prob": 0.006692306604236364}, {"id": 722, "seek": 337158, "start": 3375.54, "end": 3380.1, "text": " is to grab data.", "tokens": [50562, 307, 281, 4444, 1412, 13, 50790], "temperature": 0.0, "avg_logprob": -0.23065760261134097, "compression_ratio": 1.5810055865921788, "no_speech_prob": 0.006692306604236364}, {"id": 723, "seek": 337158, "start": 3380.1, "end": 3389.58, "text": " And there's a really fantastic library of datasets available on HuggingFace nowadays.", "tokens": [50790, 400, 456, 311, 257, 534, 5456, 6405, 295, 42856, 2435, 322, 46892, 3249, 37, 617, 13434, 13, 51264], "temperature": 0.0, "avg_logprob": -0.23065760261134097, "compression_ratio": 1.5810055865921788, "no_speech_prob": 0.006692306604236364}, {"id": 724, "seek": 337158, "start": 3389.58, "end": 3394.38, "text": " And so let's look at how we use those datasets, now that we know how to bring things into", "tokens": [51264, 400, 370, 718, 311, 574, 412, 577, 321, 764, 729, 42856, 11, 586, 300, 321, 458, 577, 281, 1565, 721, 666, 51504], "temperature": 0.0, "avg_logprob": -0.23065760261134097, "compression_ratio": 1.5810055865921788, "no_speech_prob": 0.006692306604236364}, {"id": 725, "seek": 339438, "start": 3394.38, "end": 3396.7000000000003, "text": " data loaders and stuff.", "tokens": [50364, 1412, 3677, 433, 293, 1507, 13, 50480], "temperature": 0.0, "avg_logprob": -0.3232423515730007, "compression_ratio": 1.6553398058252426, "no_speech_prob": 0.19434493780136108}, {"id": 726, "seek": 339438, "start": 3396.7000000000003, "end": 3404.38, "text": " So that now we can use the entire world of HuggingFace datasets with our code.", "tokens": [50480, 407, 300, 586, 321, 393, 764, 264, 2302, 1002, 295, 46892, 3249, 37, 617, 42856, 365, 527, 3089, 13, 50864], "temperature": 0.0, "avg_logprob": -0.3232423515730007, "compression_ratio": 1.6553398058252426, "no_speech_prob": 0.19434493780136108}, {"id": 727, "seek": 339438, "start": 3404.38, "end": 3412.7000000000003, "text": " So we're going to, so you need to pip install datasets.", "tokens": [50864, 407, 321, 434, 516, 281, 11, 370, 291, 643, 281, 8489, 3625, 42856, 13, 51280], "temperature": 0.0, "avg_logprob": -0.3232423515730007, "compression_ratio": 1.6553398058252426, "no_speech_prob": 0.19434493780136108}, {"id": 728, "seek": 339438, "start": 3412.7000000000003, "end": 3416.82, "text": " And once you've pipped installed datasets, you'll be able to say from datasets import,", "tokens": [51280, 400, 1564, 291, 600, 8489, 3452, 8899, 42856, 11, 291, 603, 312, 1075, 281, 584, 490, 42856, 974, 11, 51486], "temperature": 0.0, "avg_logprob": -0.3232423515730007, "compression_ratio": 1.6553398058252426, "no_speech_prob": 0.19434493780136108}, {"id": 729, "seek": 339438, "start": 3416.82, "end": 3421.06, "text": " and you can import a few things, just these two things now, load dataset, load dataset", "tokens": [51486, 293, 291, 393, 974, 257, 1326, 721, 11, 445, 613, 732, 721, 586, 11, 3677, 28872, 11, 3677, 28872, 51698], "temperature": 0.0, "avg_logprob": -0.3232423515730007, "compression_ratio": 1.6553398058252426, "no_speech_prob": 0.19434493780136108}, {"id": 730, "seek": 339438, "start": 3421.06, "end": 3422.06, "text": " builder.", "tokens": [51698, 27377, 13, 51748], "temperature": 0.0, "avg_logprob": -0.3232423515730007, "compression_ratio": 1.6553398058252426, "no_speech_prob": 0.19434493780136108}, {"id": 731, "seek": 342206, "start": 3422.06, "end": 3427.86, "text": " So we're going to look at a dataset called Fashion MNIST.", "tokens": [50364, 407, 321, 434, 516, 281, 574, 412, 257, 28872, 1219, 32782, 376, 45, 19756, 13, 50654], "temperature": 0.0, "avg_logprob": -0.2811643144358759, "compression_ratio": 1.5904761904761904, "no_speech_prob": 0.004070092923939228}, {"id": 732, "seek": 342206, "start": 3427.86, "end": 3432.2999999999997, "text": " And so the way things tend to work with HuggingFace is there's something called the HuggingFace", "tokens": [50654, 400, 370, 264, 636, 721, 3928, 281, 589, 365, 46892, 3249, 37, 617, 307, 456, 311, 746, 1219, 264, 46892, 3249, 37, 617, 50876], "temperature": 0.0, "avg_logprob": -0.2811643144358759, "compression_ratio": 1.5904761904761904, "no_speech_prob": 0.004070092923939228}, {"id": 733, "seek": 342206, "start": 3432.2999999999997, "end": 3438.4, "text": " hub, which has models and it has datasets, amongst other things.", "tokens": [50876, 11838, 11, 597, 575, 5245, 293, 309, 575, 42856, 11, 12918, 661, 721, 13, 51181], "temperature": 0.0, "avg_logprob": -0.2811643144358759, "compression_ratio": 1.5904761904761904, "no_speech_prob": 0.004070092923939228}, {"id": 734, "seek": 342206, "start": 3438.4, "end": 3445.14, "text": " And generally you'll give them a name, and you can then say in this case, load a dataset", "tokens": [51181, 400, 5101, 291, 603, 976, 552, 257, 1315, 11, 293, 291, 393, 550, 584, 294, 341, 1389, 11, 3677, 257, 28872, 51518], "temperature": 0.0, "avg_logprob": -0.2811643144358759, "compression_ratio": 1.5904761904761904, "no_speech_prob": 0.004070092923939228}, {"id": 735, "seek": 342206, "start": 3445.14, "end": 3447.44, "text": " builder for Fashion MNIST.", "tokens": [51518, 27377, 337, 32782, 376, 45, 19756, 13, 51633], "temperature": 0.0, "avg_logprob": -0.2811643144358759, "compression_ratio": 1.5904761904761904, "no_speech_prob": 0.004070092923939228}, {"id": 736, "seek": 344744, "start": 3447.44, "end": 3458.0, "text": " Now a dataset builder is just basically something which has some metadata about this dataset.", "tokens": [50364, 823, 257, 28872, 27377, 307, 445, 1936, 746, 597, 575, 512, 26603, 466, 341, 28872, 13, 50892], "temperature": 0.0, "avg_logprob": -0.23392752639385833, "compression_ratio": 1.7100840336134453, "no_speech_prob": 0.010652213357388973}, {"id": 737, "seek": 344744, "start": 3458.0, "end": 3462.98, "text": " So the dataset builder has a .info and the .info has a .description.", "tokens": [50892, 407, 264, 28872, 27377, 575, 257, 2411, 259, 16931, 293, 264, 2411, 259, 16931, 575, 257, 2411, 14792, 12432, 13, 51141], "temperature": 0.0, "avg_logprob": -0.23392752639385833, "compression_ratio": 1.7100840336134453, "no_speech_prob": 0.010652213357388973}, {"id": 738, "seek": 344744, "start": 3462.98, "end": 3465.26, "text": " And here's a description of this.", "tokens": [51141, 400, 510, 311, 257, 3855, 295, 341, 13, 51255], "temperature": 0.0, "avg_logprob": -0.23392752639385833, "compression_ratio": 1.7100840336134453, "no_speech_prob": 0.010652213357388973}, {"id": 739, "seek": 344744, "start": 3465.26, "end": 3468.28, "text": " And as you can see, again, we've got 28 by 28 grayscale.", "tokens": [51255, 400, 382, 291, 393, 536, 11, 797, 11, 321, 600, 658, 7562, 538, 7562, 677, 3772, 37088, 13, 51406], "temperature": 0.0, "avg_logprob": -0.23392752639385833, "compression_ratio": 1.7100840336134453, "no_speech_prob": 0.010652213357388973}, {"id": 740, "seek": 344744, "start": 3468.28, "end": 3471.86, "text": " So it's going to be very familiar to us, because it's just like MNIST.", "tokens": [51406, 407, 309, 311, 516, 281, 312, 588, 4963, 281, 505, 11, 570, 309, 311, 445, 411, 376, 45, 19756, 13, 51585], "temperature": 0.0, "avg_logprob": -0.23392752639385833, "compression_ratio": 1.7100840336134453, "no_speech_prob": 0.010652213357388973}, {"id": 741, "seek": 344744, "start": 3471.86, "end": 3474.2200000000003, "text": " And again, we've got 10 categories.", "tokens": [51585, 400, 797, 11, 321, 600, 658, 1266, 10479, 13, 51703], "temperature": 0.0, "avg_logprob": -0.23392752639385833, "compression_ratio": 1.7100840336134453, "no_speech_prob": 0.010652213357388973}, {"id": 742, "seek": 344744, "start": 3474.2200000000003, "end": 3476.52, "text": " And again, we've got 60,000 training examples.", "tokens": [51703, 400, 797, 11, 321, 600, 658, 4060, 11, 1360, 3097, 5110, 13, 51818], "temperature": 0.0, "avg_logprob": -0.23392752639385833, "compression_ratio": 1.7100840336134453, "no_speech_prob": 0.010652213357388973}, {"id": 743, "seek": 347652, "start": 3476.6, "end": 3479.32, "text": " And again, we've got 10,000 test examples.", "tokens": [50368, 400, 797, 11, 321, 600, 658, 1266, 11, 1360, 1500, 5110, 13, 50504], "temperature": 0.0, "avg_logprob": -0.24218432799629544, "compression_ratio": 1.4528301886792452, "no_speech_prob": 8.22018482722342e-05}, {"id": 744, "seek": 347652, "start": 3479.32, "end": 3480.32, "text": " So this is cool.", "tokens": [50504, 407, 341, 307, 1627, 13, 50554], "temperature": 0.0, "avg_logprob": -0.24218432799629544, "compression_ratio": 1.4528301886792452, "no_speech_prob": 8.22018482722342e-05}, {"id": 745, "seek": 347652, "start": 3480.32, "end": 3487.38, "text": " So as it says, it's a direct drop-in replacement for MNIST.", "tokens": [50554, 407, 382, 309, 1619, 11, 309, 311, 257, 2047, 3270, 12, 259, 14419, 337, 376, 45, 19756, 13, 50907], "temperature": 0.0, "avg_logprob": -0.24218432799629544, "compression_ratio": 1.4528301886792452, "no_speech_prob": 8.22018482722342e-05}, {"id": 746, "seek": 347652, "start": 3487.38, "end": 3495.92, "text": " And so the dataset builder also will tell us what's in this dataset.", "tokens": [50907, 400, 370, 264, 28872, 27377, 611, 486, 980, 505, 437, 311, 294, 341, 28872, 13, 51334], "temperature": 0.0, "avg_logprob": -0.24218432799629544, "compression_ratio": 1.4528301886792452, "no_speech_prob": 8.22018482722342e-05}, {"id": 747, "seek": 347652, "start": 3495.92, "end": 3500.02, "text": " And so HuggingFace stuff generally uses dictionaries rather than tuples.", "tokens": [51334, 400, 370, 46892, 3249, 37, 617, 1507, 5101, 4960, 22352, 4889, 2831, 813, 2604, 2622, 13, 51539], "temperature": 0.0, "avg_logprob": -0.24218432799629544, "compression_ratio": 1.4528301886792452, "no_speech_prob": 8.22018482722342e-05}, {"id": 748, "seek": 347652, "start": 3500.02, "end": 3503.62, "text": " So there's going to be an image of type image.", "tokens": [51539, 407, 456, 311, 516, 281, 312, 364, 3256, 295, 2010, 3256, 13, 51719], "temperature": 0.0, "avg_logprob": -0.24218432799629544, "compression_ratio": 1.4528301886792452, "no_speech_prob": 8.22018482722342e-05}, {"id": 749, "seek": 350362, "start": 3503.72, "end": 3506.8399999999997, "text": " There's going to be a label of type class label.", "tokens": [50369, 821, 311, 516, 281, 312, 257, 7645, 295, 2010, 1508, 7645, 13, 50525], "temperature": 0.0, "avg_logprob": -0.27041825970399724, "compression_ratio": 1.6724890829694323, "no_speech_prob": 0.000646193278953433}, {"id": 750, "seek": 350362, "start": 3506.8399999999997, "end": 3509.44, "text": " There's 10 classes, and these are the names of the classes.", "tokens": [50525, 821, 311, 1266, 5359, 11, 293, 613, 366, 264, 5288, 295, 264, 5359, 13, 50655], "temperature": 0.0, "avg_logprob": -0.27041825970399724, "compression_ratio": 1.6724890829694323, "no_speech_prob": 0.000646193278953433}, {"id": 751, "seek": 350362, "start": 3509.44, "end": 3515.48, "text": " So it's quite nice that in HuggingFace datasets, you know, we can kind of get this information", "tokens": [50655, 407, 309, 311, 1596, 1481, 300, 294, 46892, 3249, 37, 617, 42856, 11, 291, 458, 11, 321, 393, 733, 295, 483, 341, 1589, 50957], "temperature": 0.0, "avg_logprob": -0.27041825970399724, "compression_ratio": 1.6724890829694323, "no_speech_prob": 0.000646193278953433}, {"id": 752, "seek": 350362, "start": 3515.48, "end": 3517.66, "text": " directly.", "tokens": [50957, 3838, 13, 51066], "temperature": 0.0, "avg_logprob": -0.27041825970399724, "compression_ratio": 1.6724890829694323, "no_speech_prob": 0.000646193278953433}, {"id": 753, "seek": 350362, "start": 3517.66, "end": 3521.8199999999997, "text": " It also tells us if there are some recommended training test splits, we can find out those", "tokens": [51066, 467, 611, 5112, 505, 498, 456, 366, 512, 9628, 3097, 1500, 37741, 11, 321, 393, 915, 484, 729, 51274], "temperature": 0.0, "avg_logprob": -0.27041825970399724, "compression_ratio": 1.6724890829694323, "no_speech_prob": 0.000646193278953433}, {"id": 754, "seek": 350362, "start": 3521.8199999999997, "end": 3523.24, "text": " as well.", "tokens": [51274, 382, 731, 13, 51345], "temperature": 0.0, "avg_logprob": -0.27041825970399724, "compression_ratio": 1.6724890829694323, "no_speech_prob": 0.000646193278953433}, {"id": 755, "seek": 350362, "start": 3523.24, "end": 3531.16, "text": " So this is the size of the training split and the number of examples.", "tokens": [51345, 407, 341, 307, 264, 2744, 295, 264, 3097, 7472, 293, 264, 1230, 295, 5110, 13, 51741], "temperature": 0.0, "avg_logprob": -0.27041825970399724, "compression_ratio": 1.6724890829694323, "no_speech_prob": 0.000646193278953433}, {"id": 756, "seek": 353116, "start": 3531.16, "end": 3535.66, "text": " So now that we're ready to start playing with it, we can load the dataset.", "tokens": [50364, 407, 586, 300, 321, 434, 1919, 281, 722, 2433, 365, 309, 11, 321, 393, 3677, 264, 28872, 13, 50589], "temperature": 0.0, "avg_logprob": -0.30167586916968936, "compression_ratio": 1.7647058823529411, "no_speech_prob": 9.028009662870318e-05}, {"id": 757, "seek": 353116, "start": 3535.66, "end": 3539.54, "text": " Okay, so this is a different string, load dataset builder versus load dataset.", "tokens": [50589, 1033, 11, 370, 341, 307, 257, 819, 6798, 11, 3677, 28872, 27377, 5717, 3677, 28872, 13, 50783], "temperature": 0.0, "avg_logprob": -0.30167586916968936, "compression_ratio": 1.7647058823529411, "no_speech_prob": 9.028009662870318e-05}, {"id": 758, "seek": 353116, "start": 3539.54, "end": 3543.7, "text": " So this will actually download it, cache it.", "tokens": [50783, 407, 341, 486, 767, 5484, 309, 11, 19459, 309, 13, 50991], "temperature": 0.0, "avg_logprob": -0.30167586916968936, "compression_ratio": 1.7647058823529411, "no_speech_prob": 9.028009662870318e-05}, {"id": 759, "seek": 353116, "start": 3543.7, "end": 3545.58, "text": " And here it is.", "tokens": [50991, 400, 510, 309, 307, 13, 51085], "temperature": 0.0, "avg_logprob": -0.30167586916968936, "compression_ratio": 1.7647058823529411, "no_speech_prob": 9.028009662870318e-05}, {"id": 760, "seek": 353116, "start": 3545.58, "end": 3548.46, "text": " And it creates a dataset dictionary.", "tokens": [51085, 400, 309, 7829, 257, 28872, 25890, 13, 51229], "temperature": 0.0, "avg_logprob": -0.30167586916968936, "compression_ratio": 1.7647058823529411, "no_speech_prob": 9.028009662870318e-05}, {"id": 761, "seek": 353116, "start": 3548.46, "end": 3553.66, "text": " So a dataset dictionary, if you've used Fast.ai, is basically just like what we call the datasets", "tokens": [51229, 407, 257, 28872, 25890, 11, 498, 291, 600, 1143, 15968, 13, 1301, 11, 307, 1936, 445, 411, 437, 321, 818, 264, 42856, 51489], "temperature": 0.0, "avg_logprob": -0.30167586916968936, "compression_ratio": 1.7647058823529411, "no_speech_prob": 9.028009662870318e-05}, {"id": 762, "seek": 353116, "start": 3553.66, "end": 3554.66, "text": " class.", "tokens": [51489, 1508, 13, 51539], "temperature": 0.0, "avg_logprob": -0.30167586916968936, "compression_ratio": 1.7647058823529411, "no_speech_prob": 9.028009662870318e-05}, {"id": 763, "seek": 353116, "start": 3554.66, "end": 3556.6, "text": " They call the dataset dict class.", "tokens": [51539, 814, 818, 264, 28872, 12569, 1508, 13, 51636], "temperature": 0.0, "avg_logprob": -0.30167586916968936, "compression_ratio": 1.7647058823529411, "no_speech_prob": 9.028009662870318e-05}, {"id": 764, "seek": 355660, "start": 3556.62, "end": 3562.8199999999997, "text": " So it's a dictionary that contains, in this case, a train and a test item, and those are", "tokens": [50365, 407, 309, 311, 257, 25890, 300, 8306, 11, 294, 341, 1389, 11, 257, 3847, 293, 257, 1500, 3174, 11, 293, 729, 366, 50675], "temperature": 0.0, "avg_logprob": -0.26537584221881366, "compression_ratio": 1.7355769230769231, "no_speech_prob": 0.000129315274534747}, {"id": 765, "seek": 355660, "start": 3562.8199999999997, "end": 3563.8199999999997, "text": " datasets.", "tokens": [50675, 42856, 13, 50725], "temperature": 0.0, "avg_logprob": -0.26537584221881366, "compression_ratio": 1.7355769230769231, "no_speech_prob": 0.000129315274534747}, {"id": 766, "seek": 355660, "start": 3563.8199999999997, "end": 3571.5, "text": " And these datasets are very much like the datasets that we created in the previous notebook.", "tokens": [50725, 400, 613, 42856, 366, 588, 709, 411, 264, 42856, 300, 321, 2942, 294, 264, 3894, 21060, 13, 51109], "temperature": 0.0, "avg_logprob": -0.26537584221881366, "compression_ratio": 1.7355769230769231, "no_speech_prob": 0.000129315274534747}, {"id": 767, "seek": 355660, "start": 3571.5, "end": 3578.3399999999997, "text": " So we can now grab the training and test items from that dictionary, and just pop them into", "tokens": [51109, 407, 321, 393, 586, 4444, 264, 3097, 293, 1500, 4754, 490, 300, 25890, 11, 293, 445, 1665, 552, 666, 51451], "temperature": 0.0, "avg_logprob": -0.26537584221881366, "compression_ratio": 1.7355769230769231, "no_speech_prob": 0.000129315274534747}, {"id": 768, "seek": 355660, "start": 3578.3399999999997, "end": 3580.06, "text": " variables.", "tokens": [51451, 9102, 13, 51537], "temperature": 0.0, "avg_logprob": -0.26537584221881366, "compression_ratio": 1.7355769230769231, "no_speech_prob": 0.000129315274534747}, {"id": 769, "seek": 355660, "start": 3580.06, "end": 3584.92, "text": " And so we can now have a look at the zero index thing in training.", "tokens": [51537, 400, 370, 321, 393, 586, 362, 257, 574, 412, 264, 4018, 8186, 551, 294, 3097, 13, 51780], "temperature": 0.0, "avg_logprob": -0.26537584221881366, "compression_ratio": 1.7355769230769231, "no_speech_prob": 0.000129315274534747}, {"id": 770, "seek": 358492, "start": 3585.06, "end": 3589.54, "text": " And just like we were promised, it contains an image and a label.", "tokens": [50371, 400, 445, 411, 321, 645, 10768, 11, 309, 8306, 364, 3256, 293, 257, 7645, 13, 50595], "temperature": 0.0, "avg_logprob": -0.23768096786361556, "compression_ratio": 1.8472222222222223, "no_speech_prob": 0.006097541656345129}, {"id": 771, "seek": 358492, "start": 3589.54, "end": 3591.98, "text": " So as you can see, we're not getting tuples anymore.", "tokens": [50595, 407, 382, 291, 393, 536, 11, 321, 434, 406, 1242, 2604, 2622, 3602, 13, 50717], "temperature": 0.0, "avg_logprob": -0.23768096786361556, "compression_ratio": 1.8472222222222223, "no_speech_prob": 0.006097541656345129}, {"id": 772, "seek": 358492, "start": 3591.98, "end": 3597.02, "text": " We're getting dictionaries containing the x and the y, in this case image and label.", "tokens": [50717, 492, 434, 1242, 22352, 4889, 19273, 264, 2031, 293, 264, 288, 11, 294, 341, 1389, 3256, 293, 7645, 13, 50969], "temperature": 0.0, "avg_logprob": -0.23768096786361556, "compression_ratio": 1.8472222222222223, "no_speech_prob": 0.006097541656345129}, {"id": 773, "seek": 358492, "start": 3597.02, "end": 3601.98, "text": " So I'm going to get pretty bored writing image and label in strings all the time, so I'm", "tokens": [50969, 407, 286, 478, 516, 281, 483, 1238, 13521, 3579, 3256, 293, 7645, 294, 13985, 439, 264, 565, 11, 370, 286, 478, 51217], "temperature": 0.0, "avg_logprob": -0.23768096786361556, "compression_ratio": 1.8472222222222223, "no_speech_prob": 0.006097541656345129}, {"id": 774, "seek": 358492, "start": 3601.98, "end": 3603.94, "text": " just going to store them as x and y.", "tokens": [51217, 445, 516, 281, 3531, 552, 382, 2031, 293, 288, 13, 51315], "temperature": 0.0, "avg_logprob": -0.23768096786361556, "compression_ratio": 1.8472222222222223, "no_speech_prob": 0.006097541656345129}, {"id": 775, "seek": 358492, "start": 3603.94, "end": 3612.7000000000003, "text": " So x is going to be the string image, and y will be the string label.", "tokens": [51315, 407, 2031, 307, 516, 281, 312, 264, 6798, 3256, 11, 293, 288, 486, 312, 264, 6798, 7645, 13, 51753], "temperature": 0.0, "avg_logprob": -0.23768096786361556, "compression_ratio": 1.8472222222222223, "no_speech_prob": 0.006097541656345129}, {"id": 776, "seek": 361270, "start": 3612.72, "end": 3622.56, "text": " I guess the other way I could have done that would have been to say x comma y equals that.", "tokens": [50365, 286, 2041, 264, 661, 636, 286, 727, 362, 1096, 300, 576, 362, 668, 281, 584, 2031, 22117, 288, 6915, 300, 13, 50857], "temperature": 0.0, "avg_logprob": -0.2734794616699219, "compression_ratio": 1.5774647887323943, "no_speech_prob": 1.165952744486276e-05}, {"id": 777, "seek": 361270, "start": 3622.56, "end": 3629.52, "text": " That would probably be a bit neater, because it's coming straight from the features.", "tokens": [50857, 663, 576, 1391, 312, 257, 857, 408, 771, 11, 570, 309, 311, 1348, 2997, 490, 264, 4122, 13, 51205], "temperature": 0.0, "avg_logprob": -0.2734794616699219, "compression_ratio": 1.5774647887323943, "no_speech_prob": 1.165952744486276e-05}, {"id": 778, "seek": 361270, "start": 3629.52, "end": 3634.6, "text": " And if you iterate into a dictionary, you get back its keys.", "tokens": [51205, 400, 498, 291, 44497, 666, 257, 25890, 11, 291, 483, 646, 1080, 9317, 13, 51459], "temperature": 0.0, "avg_logprob": -0.2734794616699219, "compression_ratio": 1.5774647887323943, "no_speech_prob": 1.165952744486276e-05}, {"id": 779, "seek": 361270, "start": 3634.6, "end": 3635.6, "text": " That's why that works.", "tokens": [51459, 663, 311, 983, 300, 1985, 13, 51509], "temperature": 0.0, "avg_logprob": -0.2734794616699219, "compression_ratio": 1.5774647887323943, "no_speech_prob": 1.165952744486276e-05}, {"id": 780, "seek": 361270, "start": 3635.6, "end": 3640.8799999999997, "text": " So anyway, I've done it manually here, which is a bit sad, but there you go.", "tokens": [51509, 407, 4033, 11, 286, 600, 1096, 309, 16945, 510, 11, 597, 307, 257, 857, 4227, 11, 457, 456, 291, 352, 13, 51773], "temperature": 0.0, "avg_logprob": -0.2734794616699219, "compression_ratio": 1.5774647887323943, "no_speech_prob": 1.165952744486276e-05}, {"id": 781, "seek": 364088, "start": 3641.06, "end": 3646.34, "text": " Okay, so we can now grab the from train zero, which we've already seen.", "tokens": [50373, 1033, 11, 370, 321, 393, 586, 4444, 264, 490, 3847, 4018, 11, 597, 321, 600, 1217, 1612, 13, 50637], "temperature": 0.0, "avg_logprob": -0.3065556335449219, "compression_ratio": 1.604026845637584, "no_speech_prob": 1.568953121022787e-05}, {"id": 782, "seek": 364088, "start": 3646.34, "end": 3650.42, "text": " We can grab the x, i.e. the image, and there it is.", "tokens": [50637, 492, 393, 4444, 264, 2031, 11, 741, 13, 68, 13, 264, 3256, 11, 293, 456, 309, 307, 13, 50841], "temperature": 0.0, "avg_logprob": -0.3065556335449219, "compression_ratio": 1.604026845637584, "no_speech_prob": 1.568953121022787e-05}, {"id": 783, "seek": 364088, "start": 3650.42, "end": 3659.78, "text": " There's the image.", "tokens": [50841, 821, 311, 264, 3256, 13, 51309], "temperature": 0.0, "avg_logprob": -0.3065556335449219, "compression_ratio": 1.604026845637584, "no_speech_prob": 1.568953121022787e-05}, {"id": 784, "seek": 364088, "start": 3659.78, "end": 3666.78, "text": " We could grab the first five images, and the first five labels, for example.", "tokens": [51309, 492, 727, 4444, 264, 700, 1732, 5267, 11, 293, 264, 700, 1732, 16949, 11, 337, 1365, 13, 51659], "temperature": 0.0, "avg_logprob": -0.3065556335449219, "compression_ratio": 1.604026845637584, "no_speech_prob": 1.568953121022787e-05}, {"id": 785, "seek": 364088, "start": 3666.78, "end": 3667.98, "text": " And there they are.", "tokens": [51659, 400, 456, 436, 366, 13, 51719], "temperature": 0.0, "avg_logprob": -0.3065556335449219, "compression_ratio": 1.604026845637584, "no_speech_prob": 1.568953121022787e-05}, {"id": 786, "seek": 366798, "start": 3668.08, "end": 3677.48, "text": " We already know what the names of the classes are, so we could now see what these map to,", "tokens": [50369, 492, 1217, 458, 437, 264, 5288, 295, 264, 5359, 366, 11, 370, 321, 727, 586, 536, 437, 613, 4471, 281, 11, 50839], "temperature": 0.0, "avg_logprob": -0.37486374896505603, "compression_ratio": 1.4767441860465116, "no_speech_prob": 0.000626332126557827}, {"id": 787, "seek": 366798, "start": 3677.48, "end": 3681.4, "text": " by grabbing those features.", "tokens": [50839, 538, 23771, 729, 4122, 13, 51035], "temperature": 0.0, "avg_logprob": -0.37486374896505603, "compression_ratio": 1.4767441860465116, "no_speech_prob": 0.000626332126557827}, {"id": 788, "seek": 366798, "start": 3681.4, "end": 3682.8, "text": " So there they are.", "tokens": [51035, 407, 456, 436, 366, 13, 51105], "temperature": 0.0, "avg_logprob": -0.37486374896505603, "compression_ratio": 1.4767441860465116, "no_speech_prob": 0.000626332126557827}, {"id": 789, "seek": 366798, "start": 3682.8, "end": 3691.16, "text": " So this is a special Huckingface class, which most libraries have something, including Fast.ai,", "tokens": [51105, 407, 341, 307, 257, 2121, 389, 33260, 2868, 1508, 11, 597, 881, 15148, 362, 746, 11, 3009, 15968, 13, 1301, 11, 51523], "temperature": 0.0, "avg_logprob": -0.37486374896505603, "compression_ratio": 1.4767441860465116, "no_speech_prob": 0.000626332126557827}, {"id": 790, "seek": 366798, "start": 3691.16, "end": 3692.2, "text": " that works like this.", "tokens": [51523, 300, 1985, 411, 341, 13, 51575], "temperature": 0.0, "avg_logprob": -0.37486374896505603, "compression_ratio": 1.4767441860465116, "no_speech_prob": 0.000626332126557827}, {"id": 791, "seek": 369220, "start": 3692.2, "end": 3697.8399999999997, "text": " There's something called int to string, which is going to take these and convert them to", "tokens": [50364, 821, 311, 746, 1219, 560, 281, 6798, 11, 597, 307, 516, 281, 747, 613, 293, 7620, 552, 281, 50646], "temperature": 0.0, "avg_logprob": -0.3802961944251932, "compression_ratio": 1.5369458128078817, "no_speech_prob": 0.07585467398166656}, {"id": 792, "seek": 369220, "start": 3697.8399999999997, "end": 3699.04, "text": " these.", "tokens": [50646, 613, 13, 50706], "temperature": 0.0, "avg_logprob": -0.3802961944251932, "compression_ratio": 1.5369458128078817, "no_speech_prob": 0.07585467398166656}, {"id": 793, "seek": 369220, "start": 3699.04, "end": 3705.3199999999997, "text": " So if I call it on our y batch, you'll see we've got, first is ankle boot, and there", "tokens": [50706, 407, 498, 286, 818, 309, 322, 527, 288, 15245, 11, 291, 603, 536, 321, 600, 658, 11, 700, 307, 21999, 11450, 11, 293, 456, 51020], "temperature": 0.0, "avg_logprob": -0.3802961944251932, "compression_ratio": 1.5369458128078817, "no_speech_prob": 0.07585467398166656}, {"id": 794, "seek": 369220, "start": 3705.3199999999997, "end": 3706.56, "text": " that is indeed an ankle boot.", "tokens": [51020, 300, 307, 6451, 364, 21999, 11450, 13, 51082], "temperature": 0.0, "avg_logprob": -0.3802961944251932, "compression_ratio": 1.5369458128078817, "no_speech_prob": 0.07585467398166656}, {"id": 795, "seek": 369220, "start": 3706.56, "end": 3714.3599999999997, "text": " Then we're going to have a couple t-shirts and a dress.", "tokens": [51082, 1396, 321, 434, 516, 281, 362, 257, 1916, 256, 12, 25892, 293, 257, 5231, 13, 51472], "temperature": 0.0, "avg_logprob": -0.3802961944251932, "compression_ratio": 1.5369458128078817, "no_speech_prob": 0.07585467398166656}, {"id": 796, "seek": 369220, "start": 3714.3599999999997, "end": 3720.2, "text": " Okay, so how do we use this to train a model?", "tokens": [51472, 1033, 11, 370, 577, 360, 321, 764, 341, 281, 3847, 257, 2316, 30, 51764], "temperature": 0.0, "avg_logprob": -0.3802961944251932, "compression_ratio": 1.5369458128078817, "no_speech_prob": 0.07585467398166656}, {"id": 797, "seek": 372020, "start": 3720.2, "end": 3725.3599999999997, "text": " Well we're going to need a data loader, and we want a data loader that for now we're going", "tokens": [50364, 1042, 321, 434, 516, 281, 643, 257, 1412, 3677, 260, 11, 293, 321, 528, 257, 1412, 3677, 260, 300, 337, 586, 321, 434, 516, 50622], "temperature": 0.0, "avg_logprob": -0.276900299324477, "compression_ratio": 1.8945147679324894, "no_speech_prob": 0.004331476986408234}, {"id": 798, "seek": 372020, "start": 3725.3599999999997, "end": 3727.16, "text": " to do just like we've done it before.", "tokens": [50622, 281, 360, 445, 411, 321, 600, 1096, 309, 949, 13, 50712], "temperature": 0.0, "avg_logprob": -0.276900299324477, "compression_ratio": 1.8945147679324894, "no_speech_prob": 0.004331476986408234}, {"id": 799, "seek": 372020, "start": 3727.16, "end": 3734.8799999999997, "text": " It's going to return, well actually we're going to do something a bit different.", "tokens": [50712, 467, 311, 516, 281, 2736, 11, 731, 767, 321, 434, 516, 281, 360, 746, 257, 857, 819, 13, 51098], "temperature": 0.0, "avg_logprob": -0.276900299324477, "compression_ratio": 1.8945147679324894, "no_speech_prob": 0.004331476986408234}, {"id": 800, "seek": 372020, "start": 3734.8799999999997, "end": 3738.48, "text": " We're going to have our collate function, is actually going to return a dictionary.", "tokens": [51098, 492, 434, 516, 281, 362, 527, 1263, 473, 2445, 11, 307, 767, 516, 281, 2736, 257, 25890, 13, 51278], "temperature": 0.0, "avg_logprob": -0.276900299324477, "compression_ratio": 1.8945147679324894, "no_speech_prob": 0.004331476986408234}, {"id": 801, "seek": 372020, "start": 3738.48, "end": 3747.0, "text": " Actually this is pretty common for Huckingface stuff, and PyTorch doesn't mind if you, it's", "tokens": [51278, 5135, 341, 307, 1238, 2689, 337, 389, 33260, 2868, 1507, 11, 293, 9953, 51, 284, 339, 1177, 380, 1575, 498, 291, 11, 309, 311, 51704], "temperature": 0.0, "avg_logprob": -0.276900299324477, "compression_ratio": 1.8945147679324894, "no_speech_prob": 0.004331476986408234}, {"id": 802, "seek": 372020, "start": 3747.0, "end": 3749.3599999999997, "text": " happy for you to return a dictionary from a collation function.", "tokens": [51704, 2055, 337, 291, 281, 2736, 257, 25890, 490, 257, 1263, 399, 2445, 13, 51822], "temperature": 0.0, "avg_logprob": -0.276900299324477, "compression_ratio": 1.8945147679324894, "no_speech_prob": 0.004331476986408234}, {"id": 803, "seek": 374936, "start": 3749.52, "end": 3755.6, "text": " So rather than returning a tuple of the stacked up, hopefully this looks very familiar.", "tokens": [50372, 407, 2831, 813, 12678, 257, 2604, 781, 295, 264, 28867, 493, 11, 4696, 341, 1542, 588, 4963, 13, 50676], "temperature": 0.0, "avg_logprob": -0.2361922447498028, "compression_ratio": 1.6541666666666666, "no_speech_prob": 0.0001177444210043177}, {"id": 804, "seek": 374936, "start": 3755.6, "end": 3760.56, "text": " This looks a lot like the thing that goes through the data set for each one and stacks", "tokens": [50676, 639, 1542, 257, 688, 411, 264, 551, 300, 1709, 807, 264, 1412, 992, 337, 1184, 472, 293, 30792, 50924], "temperature": 0.0, "avg_logprob": -0.2361922447498028, "compression_ratio": 1.6541666666666666, "no_speech_prob": 0.0001177444210043177}, {"id": 805, "seek": 374936, "start": 3760.56, "end": 3764.0, "text": " them up, just like we did in the previous notebook.", "tokens": [50924, 552, 493, 11, 445, 411, 321, 630, 294, 264, 3894, 21060, 13, 51096], "temperature": 0.0, "avg_logprob": -0.2361922447498028, "compression_ratio": 1.6541666666666666, "no_speech_prob": 0.0001177444210043177}, {"id": 806, "seek": 374936, "start": 3764.0, "end": 3765.0, "text": " So that's what we're doing.", "tokens": [51096, 407, 300, 311, 437, 321, 434, 884, 13, 51146], "temperature": 0.0, "avg_logprob": -0.2361922447498028, "compression_ratio": 1.6541666666666666, "no_speech_prob": 0.0001177444210043177}, {"id": 807, "seek": 374936, "start": 3765.0, "end": 3769.52, "text": " We're doing all in one step here in our collate function.", "tokens": [51146, 492, 434, 884, 439, 294, 472, 1823, 510, 294, 527, 1263, 473, 2445, 13, 51372], "temperature": 0.0, "avg_logprob": -0.2361922447498028, "compression_ratio": 1.6541666666666666, "no_speech_prob": 0.0001177444210043177}, {"id": 808, "seek": 374936, "start": 3769.52, "end": 3775.04, "text": " And then again exactly the same thing, go through our batch, grab the y, and this is", "tokens": [51372, 400, 550, 797, 2293, 264, 912, 551, 11, 352, 807, 527, 15245, 11, 4444, 264, 288, 11, 293, 341, 307, 51648], "temperature": 0.0, "avg_logprob": -0.2361922447498028, "compression_ratio": 1.6541666666666666, "no_speech_prob": 0.0001177444210043177}, {"id": 809, "seek": 377504, "start": 3775.04, "end": 3779.8, "text": " just stacking them up with their integers, so we don't have to call stack.", "tokens": [50364, 445, 41376, 552, 493, 365, 641, 41674, 11, 370, 321, 500, 380, 362, 281, 818, 8630, 13, 50602], "temperature": 0.0, "avg_logprob": -0.2396397223839393, "compression_ratio": 1.5812807881773399, "no_speech_prob": 0.008985290303826332}, {"id": 810, "seek": 377504, "start": 3779.8, "end": 3784.8, "text": " And so we're now going to have the image and label bits in our dictionary.", "tokens": [50602, 400, 370, 321, 434, 586, 516, 281, 362, 264, 3256, 293, 7645, 9239, 294, 527, 25890, 13, 50852], "temperature": 0.0, "avg_logprob": -0.2396397223839393, "compression_ratio": 1.5812807881773399, "no_speech_prob": 0.008985290303826332}, {"id": 811, "seek": 377504, "start": 3784.8, "end": 3793.04, "text": " So if we create our data loader using that collation function, grab one batch, so we", "tokens": [50852, 407, 498, 321, 1884, 527, 1412, 3677, 260, 1228, 300, 1263, 399, 2445, 11, 4444, 472, 15245, 11, 370, 321, 51264], "temperature": 0.0, "avg_logprob": -0.2396397223839393, "compression_ratio": 1.5812807881773399, "no_speech_prob": 0.008985290303826332}, {"id": 812, "seek": 377504, "start": 3793.04, "end": 3801.48, "text": " can go batch x dot shape is a 16 by 1 by 28 by 28, and our y of the batch, here it is.", "tokens": [51264, 393, 352, 15245, 2031, 5893, 3909, 307, 257, 3165, 538, 502, 538, 7562, 538, 7562, 11, 293, 527, 288, 295, 264, 15245, 11, 510, 309, 307, 13, 51686], "temperature": 0.0, "avg_logprob": -0.2396397223839393, "compression_ratio": 1.5812807881773399, "no_speech_prob": 0.008985290303826332}, {"id": 813, "seek": 380148, "start": 3801.48, "end": 3811.28, "text": " So the thing to notice here is that we haven't done any transforms or anything, or written", "tokens": [50364, 407, 264, 551, 281, 3449, 510, 307, 300, 321, 2378, 380, 1096, 604, 35592, 420, 1340, 11, 420, 3720, 50854], "temperature": 0.0, "avg_logprob": -0.2421282696467574, "compression_ratio": 1.7009345794392523, "no_speech_prob": 1.9833340047625825e-05}, {"id": 814, "seek": 380148, "start": 3811.28, "end": 3814.04, "text": " our own data set class or anything.", "tokens": [50854, 527, 1065, 1412, 992, 1508, 420, 1340, 13, 50992], "temperature": 0.0, "avg_logprob": -0.2421282696467574, "compression_ratio": 1.7009345794392523, "no_speech_prob": 1.9833340047625825e-05}, {"id": 815, "seek": 380148, "start": 3814.04, "end": 3816.96, "text": " We're actually putting all the work directly in the collation function.", "tokens": [50992, 492, 434, 767, 3372, 439, 264, 589, 3838, 294, 264, 1263, 399, 2445, 13, 51138], "temperature": 0.0, "avg_logprob": -0.2421282696467574, "compression_ratio": 1.7009345794392523, "no_speech_prob": 1.9833340047625825e-05}, {"id": 816, "seek": 380148, "start": 3816.96, "end": 3825.2, "text": " So this is like a really nice way to skip all of the kind of abstractions of your framework,", "tokens": [51138, 407, 341, 307, 411, 257, 534, 1481, 636, 281, 10023, 439, 295, 264, 733, 295, 12649, 626, 295, 428, 8388, 11, 51550], "temperature": 0.0, "avg_logprob": -0.2421282696467574, "compression_ratio": 1.7009345794392523, "no_speech_prob": 1.9833340047625825e-05}, {"id": 817, "seek": 380148, "start": 3825.2, "end": 3828.78, "text": " if you want to, is you can just do all of your work in collate function.", "tokens": [51550, 498, 291, 528, 281, 11, 307, 291, 393, 445, 360, 439, 295, 428, 589, 294, 1263, 473, 2445, 13, 51729], "temperature": 0.0, "avg_logprob": -0.2421282696467574, "compression_ratio": 1.7009345794392523, "no_speech_prob": 1.9833340047625825e-05}, {"id": 818, "seek": 382878, "start": 3828.78, "end": 3834.5, "text": " So it's going to pass you each item, so you're going to get the batch directly, you just", "tokens": [50364, 407, 309, 311, 516, 281, 1320, 291, 1184, 3174, 11, 370, 291, 434, 516, 281, 483, 264, 15245, 3838, 11, 291, 445, 50650], "temperature": 0.0, "avg_logprob": -0.29190309765269457, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0010005012154579163}, {"id": 819, "seek": 382878, "start": 3834.5, "end": 3840.86, "text": " go through each item.", "tokens": [50650, 352, 807, 1184, 3174, 13, 50968], "temperature": 0.0, "avg_logprob": -0.29190309765269457, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0010005012154579163}, {"id": 820, "seek": 382878, "start": 3840.86, "end": 3847.94, "text": " And so here we're saying, okay grab the x key from that dictionary, convert it to a", "tokens": [50968, 400, 370, 510, 321, 434, 1566, 11, 1392, 4444, 264, 2031, 2141, 490, 300, 25890, 11, 7620, 309, 281, 257, 51322], "temperature": 0.0, "avg_logprob": -0.29190309765269457, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0010005012154579163}, {"id": 821, "seek": 382878, "start": 3847.94, "end": 3853.48, "text": " tensor, and then do that for everything in the batch, and then stack them all together.", "tokens": [51322, 40863, 11, 293, 550, 360, 300, 337, 1203, 294, 264, 15245, 11, 293, 550, 8630, 552, 439, 1214, 13, 51599], "temperature": 0.0, "avg_logprob": -0.29190309765269457, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0010005012154579163}, {"id": 822, "seek": 382878, "start": 3853.48, "end": 3857.52, "text": " So this is, yeah, this is like, can be quite a nice way to do things, if you want to do", "tokens": [51599, 407, 341, 307, 11, 1338, 11, 341, 307, 411, 11, 393, 312, 1596, 257, 1481, 636, 281, 360, 721, 11, 498, 291, 528, 281, 360, 51801], "temperature": 0.0, "avg_logprob": -0.29190309765269457, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0010005012154579163}, {"id": 823, "seek": 385752, "start": 3857.56, "end": 3864.4, "text": " things just very manually, without having to think too much about, you know, a framework.", "tokens": [50366, 721, 445, 588, 16945, 11, 1553, 1419, 281, 519, 886, 709, 466, 11, 291, 458, 11, 257, 8388, 13, 50708], "temperature": 0.0, "avg_logprob": -0.289816209639626, "compression_ratio": 1.5791666666666666, "no_speech_prob": 0.02096386067569256}, {"id": 824, "seek": 385752, "start": 3864.4, "end": 3868.56, "text": " Particularly if you're doing really custom stuff, this can be quite helpful.", "tokens": [50708, 32281, 498, 291, 434, 884, 534, 2375, 1507, 11, 341, 393, 312, 1596, 4961, 13, 50916], "temperature": 0.0, "avg_logprob": -0.289816209639626, "compression_ratio": 1.5791666666666666, "no_speech_prob": 0.02096386067569256}, {"id": 825, "seek": 385752, "start": 3868.56, "end": 3874.36, "text": " Having said that, hugging face data sets absolutely lets you avoid doing everything in collate", "tokens": [50916, 10222, 848, 300, 11, 41706, 1851, 1412, 6352, 3122, 6653, 291, 5042, 884, 1203, 294, 1263, 473, 51206], "temperature": 0.0, "avg_logprob": -0.289816209639626, "compression_ratio": 1.5791666666666666, "no_speech_prob": 0.02096386067569256}, {"id": 826, "seek": 385752, "start": 3874.36, "end": 3878.94, "text": " function, which if we want to create really simple applications, that's where we're going", "tokens": [51206, 2445, 11, 597, 498, 321, 528, 281, 1884, 534, 2199, 5821, 11, 300, 311, 689, 321, 434, 516, 51435], "temperature": 0.0, "avg_logprob": -0.289816209639626, "compression_ratio": 1.5791666666666666, "no_speech_prob": 0.02096386067569256}, {"id": 827, "seek": 385752, "start": 3878.94, "end": 3881.04, "text": " to eventually want to head.", "tokens": [51435, 281, 4728, 528, 281, 1378, 13, 51540], "temperature": 0.0, "avg_logprob": -0.289816209639626, "compression_ratio": 1.5791666666666666, "no_speech_prob": 0.02096386067569256}, {"id": 828, "seek": 388104, "start": 3881.04, "end": 3889.8, "text": " So we can do this using a transform instead.", "tokens": [50364, 407, 321, 393, 360, 341, 1228, 257, 4088, 2602, 13, 50802], "temperature": 0.0, "avg_logprob": -0.2418884599065206, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.008711212314665318}, {"id": 829, "seek": 388104, "start": 3889.8, "end": 3895.34, "text": " And so the way we do that, is we create a function, we're going to take our batch, it's", "tokens": [50802, 400, 370, 264, 636, 321, 360, 300, 11, 307, 321, 1884, 257, 2445, 11, 321, 434, 516, 281, 747, 527, 15245, 11, 309, 311, 51079], "temperature": 0.0, "avg_logprob": -0.2418884599065206, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.008711212314665318}, {"id": 830, "seek": 388104, "start": 3895.34, "end": 3903.68, "text": " going to replace the x in our batch with the tensor version of each of those pal images.", "tokens": [51079, 516, 281, 7406, 264, 2031, 294, 527, 15245, 365, 264, 40863, 3037, 295, 1184, 295, 729, 3984, 5267, 13, 51496], "temperature": 0.0, "avg_logprob": -0.2418884599065206, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.008711212314665318}, {"id": 831, "seek": 388104, "start": 3903.68, "end": 3907.5, "text": " And I'm not even stacking them or anything, and then we're going to return that batch.", "tokens": [51496, 400, 286, 478, 406, 754, 41376, 552, 420, 1340, 11, 293, 550, 321, 434, 516, 281, 2736, 300, 15245, 13, 51687], "temperature": 0.0, "avg_logprob": -0.2418884599065206, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.008711212314665318}, {"id": 832, "seek": 390750, "start": 3907.5, "end": 3912.06, "text": " And so hugging face data sets has something called with transform, and that's going to", "tokens": [50364, 400, 370, 41706, 1851, 1412, 6352, 575, 746, 1219, 365, 4088, 11, 293, 300, 311, 516, 281, 50592], "temperature": 0.0, "avg_logprob": -0.2391272669253142, "compression_ratio": 1.831275720164609, "no_speech_prob": 0.0008558984845876694}, {"id": 833, "seek": 390750, "start": 3912.06, "end": 3919.08, "text": " take your data set, your hugging face data set, and it's going to apply this function", "tokens": [50592, 747, 428, 1412, 992, 11, 428, 41706, 1851, 1412, 992, 11, 293, 309, 311, 516, 281, 3079, 341, 2445, 50943], "temperature": 0.0, "avg_logprob": -0.2391272669253142, "compression_ratio": 1.831275720164609, "no_speech_prob": 0.0008558984845876694}, {"id": 834, "seek": 390750, "start": 3919.08, "end": 3920.08, "text": " to every element.", "tokens": [50943, 281, 633, 4478, 13, 50993], "temperature": 0.0, "avg_logprob": -0.2391272669253142, "compression_ratio": 1.831275720164609, "no_speech_prob": 0.0008558984845876694}, {"id": 835, "seek": 390750, "start": 3920.08, "end": 3926.02, "text": " And it doesn't run it all now, it's going to basically, when it, behind the scenes,", "tokens": [50993, 400, 309, 1177, 380, 1190, 309, 439, 586, 11, 309, 311, 516, 281, 1936, 11, 562, 309, 11, 2261, 264, 8026, 11, 51290], "temperature": 0.0, "avg_logprob": -0.2391272669253142, "compression_ratio": 1.831275720164609, "no_speech_prob": 0.0008558984845876694}, {"id": 836, "seek": 390750, "start": 3926.02, "end": 3931.62, "text": " when it calls done to get item, it will call this function on the fly.", "tokens": [51290, 562, 309, 5498, 1096, 281, 483, 3174, 11, 309, 486, 818, 341, 2445, 322, 264, 3603, 13, 51570], "temperature": 0.0, "avg_logprob": -0.2391272669253142, "compression_ratio": 1.831275720164609, "no_speech_prob": 0.0008558984845876694}, {"id": 837, "seek": 390750, "start": 3931.62, "end": 3935.82, "text": " So in other words, this could have data augmentation, which can be random or whatever, because it's", "tokens": [51570, 407, 294, 661, 2283, 11, 341, 727, 362, 1412, 14501, 19631, 11, 597, 393, 312, 4974, 420, 2035, 11, 570, 309, 311, 51780], "temperature": 0.0, "avg_logprob": -0.2391272669253142, "compression_ratio": 1.831275720164609, "no_speech_prob": 0.0008558984845876694}, {"id": 838, "seek": 393582, "start": 3935.82, "end": 3939.46, "text": " going to be rerun every time you grab an item.", "tokens": [50364, 516, 281, 312, 43819, 409, 633, 565, 291, 4444, 364, 3174, 13, 50546], "temperature": 0.0, "avg_logprob": -0.2230054717702964, "compression_ratio": 1.5406698564593302, "no_speech_prob": 7.96729582361877e-05}, {"id": 839, "seek": 393582, "start": 3939.46, "end": 3942.04, "text": " It's not cached or anything like that.", "tokens": [50546, 467, 311, 406, 269, 15095, 420, 1340, 411, 300, 13, 50675], "temperature": 0.0, "avg_logprob": -0.2230054717702964, "compression_ratio": 1.5406698564593302, "no_speech_prob": 7.96729582361877e-05}, {"id": 840, "seek": 393582, "start": 3942.04, "end": 3946.38, "text": " So other than that, this data set has exactly the API, same API as any other data set.", "tokens": [50675, 407, 661, 813, 300, 11, 341, 1412, 992, 575, 2293, 264, 9362, 11, 912, 9362, 382, 604, 661, 1412, 992, 13, 50892], "temperature": 0.0, "avg_logprob": -0.2230054717702964, "compression_ratio": 1.5406698564593302, "no_speech_prob": 7.96729582361877e-05}, {"id": 841, "seek": 393582, "start": 3946.38, "end": 3952.6000000000004, "text": " It has a length, it has a done to get item, so you can pass it to a data loader.", "tokens": [50892, 467, 575, 257, 4641, 11, 309, 575, 257, 1096, 281, 483, 3174, 11, 370, 291, 393, 1320, 309, 281, 257, 1412, 3677, 260, 13, 51203], "temperature": 0.0, "avg_logprob": -0.2230054717702964, "compression_ratio": 1.5406698564593302, "no_speech_prob": 7.96729582361877e-05}, {"id": 842, "seek": 393582, "start": 3952.6000000000004, "end": 3962.1200000000003, "text": " And so PyTorch already knows how to collate dictionaries of tensors.", "tokens": [51203, 400, 370, 9953, 51, 284, 339, 1217, 3255, 577, 281, 1263, 473, 22352, 4889, 295, 10688, 830, 13, 51679], "temperature": 0.0, "avg_logprob": -0.2230054717702964, "compression_ratio": 1.5406698564593302, "no_speech_prob": 7.96729582361877e-05}, {"id": 843, "seek": 396212, "start": 3962.12, "end": 3966.52, "text": " So we've got a dictionary of tensors now, so that means we don't need a collate function", "tokens": [50364, 407, 321, 600, 658, 257, 25890, 295, 10688, 830, 586, 11, 370, 300, 1355, 321, 500, 380, 643, 257, 1263, 473, 2445, 50584], "temperature": 0.0, "avg_logprob": -0.22261931256550113, "compression_ratio": 1.7781954887218046, "no_speech_prob": 0.0029348970856517553}, {"id": 844, "seek": 396212, "start": 3966.52, "end": 3967.52, "text": " anymore.", "tokens": [50584, 3602, 13, 50634], "temperature": 0.0, "avg_logprob": -0.22261931256550113, "compression_ratio": 1.7781954887218046, "no_speech_prob": 0.0029348970856517553}, {"id": 845, "seek": 396212, "start": 3967.52, "end": 3973.9, "text": " I can create a data loader from this without a collate function, as you can see.", "tokens": [50634, 286, 393, 1884, 257, 1412, 3677, 260, 490, 341, 1553, 257, 1263, 473, 2445, 11, 382, 291, 393, 536, 13, 50953], "temperature": 0.0, "avg_logprob": -0.22261931256550113, "compression_ratio": 1.7781954887218046, "no_speech_prob": 0.0029348970856517553}, {"id": 846, "seek": 396212, "start": 3973.9, "end": 3978.12, "text": " And so this is giving me exactly the same thing as before, but without having to create", "tokens": [50953, 400, 370, 341, 307, 2902, 385, 2293, 264, 912, 551, 382, 949, 11, 457, 1553, 1419, 281, 1884, 51164], "temperature": 0.0, "avg_logprob": -0.22261931256550113, "compression_ratio": 1.7781954887218046, "no_speech_prob": 0.0029348970856517553}, {"id": 847, "seek": 396212, "start": 3978.12, "end": 3980.48, "text": " a custom collate function.", "tokens": [51164, 257, 2375, 1263, 473, 2445, 13, 51282], "temperature": 0.0, "avg_logprob": -0.22261931256550113, "compression_ratio": 1.7781954887218046, "no_speech_prob": 0.0029348970856517553}, {"id": 848, "seek": 396212, "start": 3980.48, "end": 3985.2, "text": " Now even this is a bit more code than I want, having to return this seems a bit silly, but", "tokens": [51282, 823, 754, 341, 307, 257, 857, 544, 3089, 813, 286, 528, 11, 1419, 281, 2736, 341, 2544, 257, 857, 11774, 11, 457, 51518], "temperature": 0.0, "avg_logprob": -0.22261931256550113, "compression_ratio": 1.7781954887218046, "no_speech_prob": 0.0029348970856517553}, {"id": 849, "seek": 396212, "start": 3985.2, "end": 3991.06, "text": " the reason I had to do this is because hugging face data sets expects the with transform", "tokens": [51518, 264, 1778, 286, 632, 281, 360, 341, 307, 570, 41706, 1851, 1412, 6352, 33280, 264, 365, 4088, 51811], "temperature": 0.0, "avg_logprob": -0.22261931256550113, "compression_ratio": 1.7781954887218046, "no_speech_prob": 0.0029348970856517553}, {"id": 850, "seek": 399106, "start": 3991.06, "end": 3998.7799999999997, "text": " function to return the new version of the data.", "tokens": [50364, 2445, 281, 2736, 264, 777, 3037, 295, 264, 1412, 13, 50750], "temperature": 0.0, "avg_logprob": -0.23135331471761067, "compression_ratio": 1.6651162790697673, "no_speech_prob": 0.00015598011668771505}, {"id": 851, "seek": 399106, "start": 3998.7799999999997, "end": 4004.48, "text": " So I wanted to be able to write it like this, transform in place, and just say the change", "tokens": [50750, 407, 286, 1415, 281, 312, 1075, 281, 2464, 309, 411, 341, 11, 4088, 294, 1081, 11, 293, 445, 584, 264, 1319, 51035], "temperature": 0.0, "avg_logprob": -0.23135331471761067, "compression_ratio": 1.6651162790697673, "no_speech_prob": 0.00015598011668771505}, {"id": 852, "seek": 399106, "start": 4004.48, "end": 4007.32, "text": " I want to make, and have it automatically return that.", "tokens": [51035, 286, 528, 281, 652, 11, 293, 362, 309, 6772, 2736, 300, 13, 51177], "temperature": 0.0, "avg_logprob": -0.23135331471761067, "compression_ratio": 1.6651162790697673, "no_speech_prob": 0.00015598011668771505}, {"id": 853, "seek": 399106, "start": 4007.32, "end": 4012.7799999999997, "text": " So if I create this function that's exactly the same as the previous one, but doesn't", "tokens": [51177, 407, 498, 286, 1884, 341, 2445, 300, 311, 2293, 264, 912, 382, 264, 3894, 472, 11, 457, 1177, 380, 51450], "temperature": 0.0, "avg_logprob": -0.23135331471761067, "compression_ratio": 1.6651162790697673, "no_speech_prob": 0.00015598011668771505}, {"id": 854, "seek": 399106, "start": 4012.7799999999997, "end": 4020.46, "text": " have return, how would I turn this into something which does return the result?", "tokens": [51450, 362, 2736, 11, 577, 576, 286, 1261, 341, 666, 746, 597, 775, 2736, 264, 1874, 30, 51834], "temperature": 0.0, "avg_logprob": -0.23135331471761067, "compression_ratio": 1.6651162790697673, "no_speech_prob": 0.00015598011668771505}, {"id": 855, "seek": 402046, "start": 4020.46, "end": 4022.6, "text": " So here's an interesting trick.", "tokens": [50364, 407, 510, 311, 364, 1880, 4282, 13, 50471], "temperature": 0.0, "avg_logprob": -0.2484679854059794, "compression_ratio": 1.8516483516483517, "no_speech_prob": 2.9773082133033313e-05}, {"id": 856, "seek": 402046, "start": 4022.6, "end": 4031.14, "text": " We could take that function, pass it to another function, to create a new function which is", "tokens": [50471, 492, 727, 747, 300, 2445, 11, 1320, 309, 281, 1071, 2445, 11, 281, 1884, 257, 777, 2445, 597, 307, 50898], "temperature": 0.0, "avg_logprob": -0.2484679854059794, "compression_ratio": 1.8516483516483517, "no_speech_prob": 2.9773082133033313e-05}, {"id": 857, "seek": 402046, "start": 4031.14, "end": 4035.16, "text": " the version of this in place function that returns the result.", "tokens": [50898, 264, 3037, 295, 341, 294, 1081, 2445, 300, 11247, 264, 1874, 13, 51099], "temperature": 0.0, "avg_logprob": -0.2484679854059794, "compression_ratio": 1.8516483516483517, "no_speech_prob": 2.9773082133033313e-05}, {"id": 858, "seek": 402046, "start": 4035.16, "end": 4038.78, "text": " And the way I do that is by creating a function called in place.", "tokens": [51099, 400, 264, 636, 286, 360, 300, 307, 538, 4084, 257, 2445, 1219, 294, 1081, 13, 51280], "temperature": 0.0, "avg_logprob": -0.2484679854059794, "compression_ratio": 1.8516483516483517, "no_speech_prob": 2.9773082133033313e-05}, {"id": 859, "seek": 402046, "start": 4038.78, "end": 4047.1, "text": " It takes a function, it returns a function, the function it returns is one that calls", "tokens": [51280, 467, 2516, 257, 2445, 11, 309, 11247, 257, 2445, 11, 264, 2445, 309, 11247, 307, 472, 300, 5498, 51696], "temperature": 0.0, "avg_logprob": -0.2484679854059794, "compression_ratio": 1.8516483516483517, "no_speech_prob": 2.9773082133033313e-05}, {"id": 860, "seek": 404710, "start": 4047.18, "end": 4051.7, "text": " my original function, and then returns the result.", "tokens": [50368, 452, 3380, 2445, 11, 293, 550, 11247, 264, 1874, 13, 50594], "temperature": 0.0, "avg_logprob": -0.24815234431514033, "compression_ratio": 1.9760479041916168, "no_speech_prob": 1.9833360056509264e-05}, {"id": 861, "seek": 404710, "start": 4051.7, "end": 4058.7799999999997, "text": " So this is the function, this is a function generating function, and it's modifying an", "tokens": [50594, 407, 341, 307, 264, 2445, 11, 341, 307, 257, 2445, 17746, 2445, 11, 293, 309, 311, 42626, 364, 50948], "temperature": 0.0, "avg_logprob": -0.24815234431514033, "compression_ratio": 1.9760479041916168, "no_speech_prob": 1.9833360056509264e-05}, {"id": 862, "seek": 404710, "start": 4058.7799999999997, "end": 4068.2999999999997, "text": " in place function to become a function that returns the new version of that data.", "tokens": [50948, 294, 1081, 2445, 281, 1813, 257, 2445, 300, 11247, 264, 777, 3037, 295, 300, 1412, 13, 51424], "temperature": 0.0, "avg_logprob": -0.24815234431514033, "compression_ratio": 1.9760479041916168, "no_speech_prob": 1.9833360056509264e-05}, {"id": 863, "seek": 404710, "start": 4068.2999999999997, "end": 4070.86, "text": " And so this is a function.", "tokens": [51424, 400, 370, 341, 307, 257, 2445, 13, 51552], "temperature": 0.0, "avg_logprob": -0.24815234431514033, "compression_ratio": 1.9760479041916168, "no_speech_prob": 1.9833360056509264e-05}, {"id": 864, "seek": 404710, "start": 4070.86, "end": 4074.86, "text": " This function is passed to this function, which returns a function, and here it is.", "tokens": [51552, 639, 2445, 307, 4678, 281, 341, 2445, 11, 597, 11247, 257, 2445, 11, 293, 510, 309, 307, 13, 51752], "temperature": 0.0, "avg_logprob": -0.24815234431514033, "compression_ratio": 1.9760479041916168, "no_speech_prob": 1.9833360056509264e-05}, {"id": 865, "seek": 407486, "start": 4075.06, "end": 4078.02, "text": " So here's the version that HuggingFace will be able to use.", "tokens": [50374, 407, 510, 311, 264, 3037, 300, 46892, 3249, 37, 617, 486, 312, 1075, 281, 764, 13, 50522], "temperature": 0.0, "avg_logprob": -0.2514633433024089, "compression_ratio": 1.5027322404371584, "no_speech_prob": 0.0002415659255348146}, {"id": 866, "seek": 407486, "start": 4078.02, "end": 4084.94, "text": " So I can now pass that to with transform, and it does exactly the same thing.", "tokens": [50522, 407, 286, 393, 586, 1320, 300, 281, 365, 4088, 11, 293, 309, 775, 2293, 264, 912, 551, 13, 50868], "temperature": 0.0, "avg_logprob": -0.2514633433024089, "compression_ratio": 1.5027322404371584, "no_speech_prob": 0.0002415659255348146}, {"id": 867, "seek": 407486, "start": 4084.94, "end": 4088.6400000000003, "text": " So this is very very common in Python.", "tokens": [50868, 407, 341, 307, 588, 588, 2689, 294, 15329, 13, 51053], "temperature": 0.0, "avg_logprob": -0.2514633433024089, "compression_ratio": 1.5027322404371584, "no_speech_prob": 0.0002415659255348146}, {"id": 868, "seek": 407486, "start": 4088.6400000000003, "end": 4096.400000000001, "text": " It's so common that this line of code can be entirely removed and replaced with this", "tokens": [51053, 467, 311, 370, 2689, 300, 341, 1622, 295, 3089, 393, 312, 7696, 7261, 293, 10772, 365, 341, 51441], "temperature": 0.0, "avg_logprob": -0.2514633433024089, "compression_ratio": 1.5027322404371584, "no_speech_prob": 0.0002415659255348146}, {"id": 869, "seek": 407486, "start": 4096.400000000001, "end": 4098.08, "text": " little token.", "tokens": [51441, 707, 14862, 13, 51525], "temperature": 0.0, "avg_logprob": -0.2514633433024089, "compression_ratio": 1.5027322404371584, "no_speech_prob": 0.0002415659255348146}, {"id": 870, "seek": 409808, "start": 4098.08, "end": 4105.04, "text": " If you have a function, and put at at the start, you can then put that before a function,", "tokens": [50364, 759, 291, 362, 257, 2445, 11, 293, 829, 412, 412, 264, 722, 11, 291, 393, 550, 829, 300, 949, 257, 2445, 11, 50712], "temperature": 0.0, "avg_logprob": -0.2360211885892428, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.01665642112493515}, {"id": 871, "seek": 409808, "start": 4105.04, "end": 4112.48, "text": " and what it says is take this whole function, pass it to this function, and replace it with", "tokens": [50712, 293, 437, 309, 1619, 307, 747, 341, 1379, 2445, 11, 1320, 309, 281, 341, 2445, 11, 293, 7406, 309, 365, 51084], "temperature": 0.0, "avg_logprob": -0.2360211885892428, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.01665642112493515}, {"id": 872, "seek": 409808, "start": 4112.48, "end": 4113.72, "text": " the result.", "tokens": [51084, 264, 1874, 13, 51146], "temperature": 0.0, "avg_logprob": -0.2360211885892428, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.01665642112493515}, {"id": 873, "seek": 409808, "start": 4113.72, "end": 4121.24, "text": " So this is exactly the same as the combination of this and this.", "tokens": [51146, 407, 341, 307, 2293, 264, 912, 382, 264, 6562, 295, 341, 293, 341, 13, 51522], "temperature": 0.0, "avg_logprob": -0.2360211885892428, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.01665642112493515}, {"id": 874, "seek": 409808, "start": 4121.24, "end": 4124.92, "text": " And when we do it this way, this kind of little syntax sugar is called a decorator.", "tokens": [51522, 400, 562, 321, 360, 309, 341, 636, 11, 341, 733, 295, 707, 28431, 5076, 307, 1219, 257, 7919, 1639, 13, 51706], "temperature": 0.0, "avg_logprob": -0.2360211885892428, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.01665642112493515}, {"id": 875, "seek": 412492, "start": 4125.24, "end": 4127.96, "text": " Okay, so there's nothing magic about decorators.", "tokens": [50380, 1033, 11, 370, 456, 311, 1825, 5585, 466, 7919, 3391, 13, 50516], "temperature": 0.0, "avg_logprob": -0.36771072387695314, "compression_ratio": 1.5151515151515151, "no_speech_prob": 0.014281781390309334}, {"id": 876, "seek": 412492, "start": 4127.96, "end": 4132.56, "text": " It's literally, literally identical to this.", "tokens": [50516, 467, 311, 3736, 11, 3736, 14800, 281, 341, 13, 50746], "temperature": 0.0, "avg_logprob": -0.36771072387695314, "compression_ratio": 1.5151515151515151, "no_speech_prob": 0.014281781390309334}, {"id": 877, "seek": 412492, "start": 4132.56, "end": 4136.88, "text": " Oh, I guess the only difference is we don't end up with this unnecessary intermediate", "tokens": [50746, 876, 11, 286, 2041, 264, 787, 2649, 307, 321, 500, 380, 917, 493, 365, 341, 19350, 19376, 50962], "temperature": 0.0, "avg_logprob": -0.36771072387695314, "compression_ratio": 1.5151515151515151, "no_speech_prob": 0.014281781390309334}, {"id": 878, "seek": 412492, "start": 4136.88, "end": 4138.88, "text": " underscore version.", "tokens": [50962, 37556, 3037, 13, 51062], "temperature": 0.0, "avg_logprob": -0.36771072387695314, "compression_ratio": 1.5151515151515151, "no_speech_prob": 0.014281781390309334}, {"id": 879, "seek": 412492, "start": 4138.88, "end": 4146.36, "text": " But the result is exactly the same, and therefore I can create a transformed data set by using", "tokens": [51062, 583, 264, 1874, 307, 2293, 264, 912, 11, 293, 4412, 286, 393, 1884, 257, 16894, 1412, 992, 538, 1228, 51436], "temperature": 0.0, "avg_logprob": -0.36771072387695314, "compression_ratio": 1.5151515151515151, "no_speech_prob": 0.014281781390309334}, {"id": 880, "seek": 412492, "start": 4146.36, "end": 4153.72, "text": " this.", "tokens": [51436, 341, 13, 51804], "temperature": 0.0, "avg_logprob": -0.36771072387695314, "compression_ratio": 1.5151515151515151, "no_speech_prob": 0.014281781390309334}, {"id": 881, "seek": 415372, "start": 4154.52, "end": 4156.52, "text": " And there we go, it's all working fine.", "tokens": [50404, 400, 456, 321, 352, 11, 309, 311, 439, 1364, 2489, 13, 50504], "temperature": 0.0, "avg_logprob": -0.34991042996630256, "compression_ratio": 1.5076923076923077, "no_speech_prob": 0.0017274330602958798}, {"id": 882, "seek": 415372, "start": 4156.52, "end": 4166.360000000001, "text": " Yeah, so I mean none of this is particularly necessary, but what we're doing is we're just", "tokens": [50504, 865, 11, 370, 286, 914, 6022, 295, 341, 307, 4098, 4818, 11, 457, 437, 321, 434, 884, 307, 321, 434, 445, 50996], "temperature": 0.0, "avg_logprob": -0.34991042996630256, "compression_ratio": 1.5076923076923077, "no_speech_prob": 0.0017274330602958798}, {"id": 883, "seek": 415372, "start": 4166.360000000001, "end": 4175.56, "text": " kind of like seeing, you know, the pieces that we can we can put in place to make this", "tokens": [50996, 733, 295, 411, 2577, 11, 291, 458, 11, 264, 3755, 300, 321, 393, 321, 393, 829, 294, 1081, 281, 652, 341, 51456], "temperature": 0.0, "avg_logprob": -0.34991042996630256, "compression_ratio": 1.5076923076923077, "no_speech_prob": 0.0017274330602958798}, {"id": 884, "seek": 415372, "start": 4175.56, "end": 4180.68, "text": " stuff as easy as possible, and we don't have to think about things too much.", "tokens": [51456, 1507, 382, 1858, 382, 1944, 11, 293, 321, 500, 380, 362, 281, 519, 466, 721, 886, 709, 13, 51712], "temperature": 0.0, "avg_logprob": -0.34991042996630256, "compression_ratio": 1.5076923076923077, "no_speech_prob": 0.0017274330602958798}, {"id": 885, "seek": 418068, "start": 4180.68, "end": 4189.4800000000005, "text": " All right, now with all this we can basically make things pretty automatic.", "tokens": [50364, 1057, 558, 11, 586, 365, 439, 341, 321, 393, 1936, 652, 721, 1238, 12509, 13, 50804], "temperature": 0.0, "avg_logprob": -0.26792203267415365, "compression_ratio": 1.6685714285714286, "no_speech_prob": 0.000261190056335181}, {"id": 886, "seek": 418068, "start": 4189.4800000000005, "end": 4193.56, "text": " And the way we can make things pretty automatic is we're going to use a cool thing in Python", "tokens": [50804, 400, 264, 636, 321, 393, 652, 721, 1238, 12509, 307, 321, 434, 516, 281, 764, 257, 1627, 551, 294, 15329, 51008], "temperature": 0.0, "avg_logprob": -0.26792203267415365, "compression_ratio": 1.6685714285714286, "no_speech_prob": 0.000261190056335181}, {"id": 887, "seek": 418068, "start": 4193.56, "end": 4195.280000000001, "text": " called itemGetter.", "tokens": [51008, 1219, 3174, 18133, 391, 13, 51094], "temperature": 0.0, "avg_logprob": -0.26792203267415365, "compression_ratio": 1.6685714285714286, "no_speech_prob": 0.000261190056335181}, {"id": 888, "seek": 418068, "start": 4195.280000000001, "end": 4199.72, "text": " And itemGetter is a function that returns a function.", "tokens": [51094, 400, 3174, 18133, 391, 307, 257, 2445, 300, 11247, 257, 2445, 13, 51316], "temperature": 0.0, "avg_logprob": -0.26792203267415365, "compression_ratio": 1.6685714285714286, "no_speech_prob": 0.000261190056335181}, {"id": 889, "seek": 418068, "start": 4199.72, "end": 4203.4800000000005, "text": " So hopefully you're getting used to this idea now.", "tokens": [51316, 407, 4696, 291, 434, 1242, 1143, 281, 341, 1558, 586, 13, 51504], "temperature": 0.0, "avg_logprob": -0.26792203267415365, "compression_ratio": 1.6685714285714286, "no_speech_prob": 0.000261190056335181}, {"id": 890, "seek": 420348, "start": 4203.48, "end": 4213.959999999999, "text": " This creates a function that gets the a and c items from a dictionary, or something that", "tokens": [50364, 639, 7829, 257, 2445, 300, 2170, 264, 257, 293, 269, 4754, 490, 257, 25890, 11, 420, 746, 300, 50888], "temperature": 0.0, "avg_logprob": -0.2419114690838438, "compression_ratio": 1.676056338028169, "no_speech_prob": 0.003222431056201458}, {"id": 891, "seek": 420348, "start": 4213.959999999999, "end": 4215.5199999999995, "text": " looks like a dictionary.", "tokens": [50888, 1542, 411, 257, 25890, 13, 50966], "temperature": 0.0, "avg_logprob": -0.2419114690838438, "compression_ratio": 1.676056338028169, "no_speech_prob": 0.003222431056201458}, {"id": 892, "seek": 420348, "start": 4215.5199999999995, "end": 4217.099999999999, "text": " So here's a dictionary.", "tokens": [50966, 407, 510, 311, 257, 25890, 13, 51045], "temperature": 0.0, "avg_logprob": -0.2419114690838438, "compression_ratio": 1.676056338028169, "no_speech_prob": 0.003222431056201458}, {"id": 893, "seek": 420348, "start": 4217.099999999999, "end": 4220.04, "text": " It contains keys a, b, and c.", "tokens": [51045, 467, 8306, 9317, 257, 11, 272, 11, 293, 269, 13, 51192], "temperature": 0.0, "avg_logprob": -0.2419114690838438, "compression_ratio": 1.676056338028169, "no_speech_prob": 0.003222431056201458}, {"id": 894, "seek": 420348, "start": 4220.04, "end": 4229.299999999999, "text": " So this function will take a dictionary and return the a and c values.", "tokens": [51192, 407, 341, 2445, 486, 747, 257, 25890, 293, 2736, 264, 257, 293, 269, 4190, 13, 51655], "temperature": 0.0, "avg_logprob": -0.2419114690838438, "compression_ratio": 1.676056338028169, "no_speech_prob": 0.003222431056201458}, {"id": 895, "seek": 422930, "start": 4229.3, "end": 4232.02, "text": " And as you can see it has done exactly that.", "tokens": [50364, 400, 382, 291, 393, 536, 309, 575, 1096, 2293, 300, 13, 50500], "temperature": 0.0, "avg_logprob": -0.2562954446543818, "compression_ratio": 1.8072289156626506, "no_speech_prob": 0.027584606781601906}, {"id": 896, "seek": 422930, "start": 4232.02, "end": 4235.900000000001, "text": " I'll explain why this is useful in a moment.", "tokens": [50500, 286, 603, 2903, 983, 341, 307, 4420, 294, 257, 1623, 13, 50694], "temperature": 0.0, "avg_logprob": -0.2562954446543818, "compression_ratio": 1.8072289156626506, "no_speech_prob": 0.027584606781601906}, {"id": 897, "seek": 422930, "start": 4235.900000000001, "end": 4240.04, "text": " I just wanted to briefly mention what did I mean when I said something that looks like", "tokens": [50694, 286, 445, 1415, 281, 10515, 2152, 437, 630, 286, 914, 562, 286, 848, 746, 300, 1542, 411, 50901], "temperature": 0.0, "avg_logprob": -0.2562954446543818, "compression_ratio": 1.8072289156626506, "no_speech_prob": 0.027584606781601906}, {"id": 898, "seek": 422930, "start": 4240.04, "end": 4241.04, "text": " a dictionary.", "tokens": [50901, 257, 25890, 13, 50951], "temperature": 0.0, "avg_logprob": -0.2562954446543818, "compression_ratio": 1.8072289156626506, "no_speech_prob": 0.027584606781601906}, {"id": 899, "seek": 422930, "start": 4241.04, "end": 4242.04, "text": " I mean this is a dictionary.", "tokens": [50951, 286, 914, 341, 307, 257, 25890, 13, 51001], "temperature": 0.0, "avg_logprob": -0.2562954446543818, "compression_ratio": 1.8072289156626506, "no_speech_prob": 0.027584606781601906}, {"id": 900, "seek": 422930, "start": 4242.04, "end": 4244.62, "text": " Okay, that looks like a dictionary.", "tokens": [51001, 1033, 11, 300, 1542, 411, 257, 25890, 13, 51130], "temperature": 0.0, "avg_logprob": -0.2562954446543818, "compression_ratio": 1.8072289156626506, "no_speech_prob": 0.027584606781601906}, {"id": 901, "seek": 422930, "start": 4244.62, "end": 4249.42, "text": " But Python doesn't care about what type things actually are.", "tokens": [51130, 583, 15329, 1177, 380, 1127, 466, 437, 2010, 721, 767, 366, 13, 51370], "temperature": 0.0, "avg_logprob": -0.2562954446543818, "compression_ratio": 1.8072289156626506, "no_speech_prob": 0.027584606781601906}, {"id": 902, "seek": 422930, "start": 4249.42, "end": 4251.860000000001, "text": " It only cares about what they look like.", "tokens": [51370, 467, 787, 12310, 466, 437, 436, 574, 411, 13, 51492], "temperature": 0.0, "avg_logprob": -0.2562954446543818, "compression_ratio": 1.8072289156626506, "no_speech_prob": 0.027584606781601906}, {"id": 903, "seek": 422930, "start": 4251.860000000001, "end": 4256.9400000000005, "text": " And remember that when we call something with square brackets, when we index into something,", "tokens": [51492, 400, 1604, 300, 562, 321, 818, 746, 365, 3732, 26179, 11, 562, 321, 8186, 666, 746, 11, 51746], "temperature": 0.0, "avg_logprob": -0.2562954446543818, "compression_ratio": 1.8072289156626506, "no_speech_prob": 0.027584606781601906}, {"id": 904, "seek": 425694, "start": 4256.94, "end": 4260.78, "text": " behind the scenes it's just calling done to get item.", "tokens": [50364, 2261, 264, 8026, 309, 311, 445, 5141, 1096, 281, 483, 3174, 13, 50556], "temperature": 0.0, "avg_logprob": -0.3007618526242814, "compression_ratio": 1.6535087719298245, "no_speech_prob": 0.0034834013786166906}, {"id": 905, "seek": 425694, "start": 4260.78, "end": 4263.379999999999, "text": " So we could create our own class.", "tokens": [50556, 407, 321, 727, 1884, 527, 1065, 1508, 13, 50686], "temperature": 0.0, "avg_logprob": -0.3007618526242814, "compression_ratio": 1.6535087719298245, "no_speech_prob": 0.0034834013786166906}, {"id": 906, "seek": 425694, "start": 4263.379999999999, "end": 4266.259999999999, "text": " And it's done to get item, gets the key.", "tokens": [50686, 400, 309, 311, 1096, 281, 483, 3174, 11, 2170, 264, 2141, 13, 50830], "temperature": 0.0, "avg_logprob": -0.3007618526242814, "compression_ratio": 1.6535087719298245, "no_speech_prob": 0.0034834013786166906}, {"id": 907, "seek": 425694, "start": 4266.259999999999, "end": 4270.94, "text": " And it's just going to manually return one if k equals a, or two if k equals b, or three", "tokens": [50830, 400, 309, 311, 445, 516, 281, 16945, 2736, 472, 498, 350, 6915, 257, 11, 420, 732, 498, 350, 6915, 272, 11, 420, 1045, 51064], "temperature": 0.0, "avg_logprob": -0.3007618526242814, "compression_ratio": 1.6535087719298245, "no_speech_prob": 0.0034834013786166906}, {"id": 908, "seek": 425694, "start": 4270.94, "end": 4272.62, "text": " otherwise.", "tokens": [51064, 5911, 13, 51148], "temperature": 0.0, "avg_logprob": -0.3007618526242814, "compression_ratio": 1.6535087719298245, "no_speech_prob": 0.0034834013786166906}, {"id": 909, "seek": 425694, "start": 4272.62, "end": 4278.219999999999, "text": " And look, that class also works just fine with an itemGetter.", "tokens": [51148, 400, 574, 11, 300, 1508, 611, 1985, 445, 2489, 365, 364, 3174, 18133, 391, 13, 51428], "temperature": 0.0, "avg_logprob": -0.3007618526242814, "compression_ratio": 1.6535087719298245, "no_speech_prob": 0.0034834013786166906}, {"id": 910, "seek": 425694, "start": 4278.219999999999, "end": 4285.099999999999, "text": " The reason this is interesting is because a lot of people write Python as if it's like", "tokens": [51428, 440, 1778, 341, 307, 1880, 307, 570, 257, 688, 295, 561, 2464, 15329, 382, 498, 309, 311, 411, 51772], "temperature": 0.0, "avg_logprob": -0.3007618526242814, "compression_ratio": 1.6535087719298245, "no_speech_prob": 0.0034834013786166906}, {"id": 911, "seek": 428510, "start": 4285.26, "end": 4287.3, "text": " C++ or Java or something.", "tokens": [50372, 383, 25472, 420, 10745, 420, 746, 13, 50474], "temperature": 0.0, "avg_logprob": -0.2812218385584214, "compression_ratio": 1.4375, "no_speech_prob": 0.0008558987756259739}, {"id": 912, "seek": 428510, "start": 4287.3, "end": 4291.46, "text": " They write as if it's this kind of statically typed thing.", "tokens": [50474, 814, 2464, 382, 498, 309, 311, 341, 733, 295, 2219, 984, 33941, 551, 13, 50682], "temperature": 0.0, "avg_logprob": -0.2812218385584214, "compression_ratio": 1.4375, "no_speech_prob": 0.0008558987756259739}, {"id": 913, "seek": 428510, "start": 4291.46, "end": 4295.22, "text": " But I really wanted to point out that it's an extremely dynamic language, and there's", "tokens": [50682, 583, 286, 534, 1415, 281, 935, 484, 300, 309, 311, 364, 4664, 8546, 2856, 11, 293, 456, 311, 50870], "temperature": 0.0, "avg_logprob": -0.2812218385584214, "compression_ratio": 1.4375, "no_speech_prob": 0.0008558987756259739}, {"id": 914, "seek": 428510, "start": 4295.22, "end": 4299.18, "text": " a lot more flexibility than you might have realized.", "tokens": [50870, 257, 688, 544, 12635, 813, 291, 1062, 362, 5334, 13, 51068], "temperature": 0.0, "avg_logprob": -0.2812218385584214, "compression_ratio": 1.4375, "no_speech_prob": 0.0008558987756259739}, {"id": 915, "seek": 428510, "start": 4299.18, "end": 4303.26, "text": " Anyway that's a little aside.", "tokens": [51068, 5684, 300, 311, 257, 707, 7359, 13, 51272], "temperature": 0.0, "avg_logprob": -0.2812218385584214, "compression_ratio": 1.4375, "no_speech_prob": 0.0008558987756259739}, {"id": 916, "seek": 430326, "start": 4303.26, "end": 4315.74, "text": " So what we can do is, think about a batch for example, where we've got these two dictionaries.", "tokens": [50364, 407, 437, 321, 393, 360, 307, 11, 519, 466, 257, 15245, 337, 1365, 11, 689, 321, 600, 658, 613, 732, 22352, 4889, 13, 50988], "temperature": 0.0, "avg_logprob": -0.2267758986529182, "compression_ratio": 1.6020942408376964, "no_speech_prob": 0.006487946957349777}, {"id": 917, "seek": 430326, "start": 4315.74, "end": 4322.26, "text": " So PyTorch comes with a default collation function called, not surprisingly, default", "tokens": [50988, 407, 9953, 51, 284, 339, 1487, 365, 257, 7576, 1263, 399, 2445, 1219, 11, 406, 17600, 11, 7576, 51314], "temperature": 0.0, "avg_logprob": -0.2267758986529182, "compression_ratio": 1.6020942408376964, "no_speech_prob": 0.006487946957349777}, {"id": 918, "seek": 430326, "start": 4322.26, "end": 4323.26, "text": " collate.", "tokens": [51314, 1263, 473, 13, 51364], "temperature": 0.0, "avg_logprob": -0.2267758986529182, "compression_ratio": 1.6020942408376964, "no_speech_prob": 0.006487946957349777}, {"id": 919, "seek": 430326, "start": 4323.26, "end": 4326.18, "text": " So that's part of PyTorch.", "tokens": [51364, 407, 300, 311, 644, 295, 9953, 51, 284, 339, 13, 51510], "temperature": 0.0, "avg_logprob": -0.2267758986529182, "compression_ratio": 1.6020942408376964, "no_speech_prob": 0.006487946957349777}, {"id": 920, "seek": 430326, "start": 4326.18, "end": 4331.780000000001, "text": " And what default collate does with dictionaries, is it simply takes the matching keys, and", "tokens": [51510, 400, 437, 7576, 1263, 473, 775, 365, 22352, 4889, 11, 307, 309, 2935, 2516, 264, 14324, 9317, 11, 293, 51790], "temperature": 0.0, "avg_logprob": -0.2267758986529182, "compression_ratio": 1.6020942408376964, "no_speech_prob": 0.006487946957349777}, {"id": 921, "seek": 433178, "start": 4331.78, "end": 4335.94, "text": " then grabs their values, and stacks them together.", "tokens": [50364, 550, 30028, 641, 4190, 11, 293, 30792, 552, 1214, 13, 50572], "temperature": 0.0, "avg_logprob": -0.21671961793805114, "compression_ratio": 1.6592920353982301, "no_speech_prob": 0.00033015257213264704}, {"id": 922, "seek": 433178, "start": 4335.94, "end": 4341.54, "text": " And so that's why if I call default collate, a is now one three, b is now two four.", "tokens": [50572, 400, 370, 300, 311, 983, 498, 286, 818, 7576, 1263, 473, 11, 257, 307, 586, 472, 1045, 11, 272, 307, 586, 732, 1451, 13, 50852], "temperature": 0.0, "avg_logprob": -0.21671961793805114, "compression_ratio": 1.6592920353982301, "no_speech_prob": 0.00033015257213264704}, {"id": 923, "seek": 433178, "start": 4341.54, "end": 4347.86, "text": " That's actually what happened before, when we created this data loader, is it used the", "tokens": [50852, 663, 311, 767, 437, 2011, 949, 11, 562, 321, 2942, 341, 1412, 3677, 260, 11, 307, 309, 1143, 264, 51168], "temperature": 0.0, "avg_logprob": -0.21671961793805114, "compression_ratio": 1.6592920353982301, "no_speech_prob": 0.00033015257213264704}, {"id": 924, "seek": 433178, "start": 4347.86, "end": 4351.219999999999, "text": " default collation function, which does that.", "tokens": [51168, 7576, 1263, 399, 2445, 11, 597, 775, 300, 13, 51336], "temperature": 0.0, "avg_logprob": -0.21671961793805114, "compression_ratio": 1.6592920353982301, "no_speech_prob": 0.00033015257213264704}, {"id": 925, "seek": 433178, "start": 4351.219999999999, "end": 4354.78, "text": " It also works on things that are tuples, not dictionaries, which is what most of you would", "tokens": [51336, 467, 611, 1985, 322, 721, 300, 366, 2604, 2622, 11, 406, 22352, 4889, 11, 597, 307, 437, 881, 295, 291, 576, 51514], "temperature": 0.0, "avg_logprob": -0.21671961793805114, "compression_ratio": 1.6592920353982301, "no_speech_prob": 0.00033015257213264704}, {"id": 926, "seek": 433178, "start": 4354.78, "end": 4356.54, "text": " have used before.", "tokens": [51514, 362, 1143, 949, 13, 51602], "temperature": 0.0, "avg_logprob": -0.21671961793805114, "compression_ratio": 1.6592920353982301, "no_speech_prob": 0.00033015257213264704}, {"id": 927, "seek": 435654, "start": 4356.54, "end": 4361.82, "text": " And what we can do therefore, is we could create something called collate dict, which", "tokens": [50364, 400, 437, 321, 393, 360, 4412, 11, 307, 321, 727, 1884, 746, 1219, 1263, 473, 12569, 11, 597, 50628], "temperature": 0.0, "avg_logprob": -0.28309943675994875, "compression_ratio": 1.7966101694915255, "no_speech_prob": 0.01615225151181221}, {"id": 928, "seek": 435654, "start": 4361.82, "end": 4374.18, "text": " is something which is going to take a dataset, and it's going to create a item getter function,", "tokens": [50628, 307, 746, 597, 307, 516, 281, 747, 257, 28872, 11, 293, 309, 311, 516, 281, 1884, 257, 3174, 483, 391, 2445, 11, 51246], "temperature": 0.0, "avg_logprob": -0.28309943675994875, "compression_ratio": 1.7966101694915255, "no_speech_prob": 0.01615225151181221}, {"id": 929, "seek": 435654, "start": 4374.18, "end": 4378.42, "text": " for the features in that dataset, which in this case is image and label.", "tokens": [51246, 337, 264, 4122, 294, 300, 28872, 11, 597, 294, 341, 1389, 307, 3256, 293, 7645, 13, 51458], "temperature": 0.0, "avg_logprob": -0.28309943675994875, "compression_ratio": 1.7966101694915255, "no_speech_prob": 0.01615225151181221}, {"id": 930, "seek": 435654, "start": 4378.42, "end": 4383.86, "text": " So this is a function which will get the image and label items.", "tokens": [51458, 407, 341, 307, 257, 2445, 597, 486, 483, 264, 3256, 293, 7645, 4754, 13, 51730], "temperature": 0.0, "avg_logprob": -0.28309943675994875, "compression_ratio": 1.7966101694915255, "no_speech_prob": 0.01615225151181221}, {"id": 931, "seek": 438386, "start": 4383.86, "end": 4388.82, "text": " And so we're now going to return a function, and that function is simply going to call", "tokens": [50364, 400, 370, 321, 434, 586, 516, 281, 2736, 257, 2445, 11, 293, 300, 2445, 307, 2935, 516, 281, 818, 50612], "temperature": 0.0, "avg_logprob": -0.2004960264478411, "compression_ratio": 1.841860465116279, "no_speech_prob": 2.4300234144902788e-05}, {"id": 932, "seek": 438386, "start": 4388.82, "end": 4391.94, "text": " our item getter on default collate.", "tokens": [50612, 527, 3174, 483, 391, 322, 7576, 1263, 473, 13, 50768], "temperature": 0.0, "avg_logprob": -0.2004960264478411, "compression_ratio": 1.841860465116279, "no_speech_prob": 2.4300234144902788e-05}, {"id": 933, "seek": 438386, "start": 4391.94, "end": 4397.099999999999, "text": " And what this is going to do, is it's going to take a dictionary, and collate it into", "tokens": [50768, 400, 437, 341, 307, 516, 281, 360, 11, 307, 309, 311, 516, 281, 747, 257, 25890, 11, 293, 1263, 473, 309, 666, 51026], "temperature": 0.0, "avg_logprob": -0.2004960264478411, "compression_ratio": 1.841860465116279, "no_speech_prob": 2.4300234144902788e-05}, {"id": 934, "seek": 438386, "start": 4397.099999999999, "end": 4399.5, "text": " a tuple.", "tokens": [51026, 257, 2604, 781, 13, 51146], "temperature": 0.0, "avg_logprob": -0.2004960264478411, "compression_ratio": 1.841860465116279, "no_speech_prob": 2.4300234144902788e-05}, {"id": 935, "seek": 438386, "start": 4399.5, "end": 4402.58, "text": " Just like we did up here.", "tokens": [51146, 1449, 411, 321, 630, 493, 510, 13, 51300], "temperature": 0.0, "avg_logprob": -0.2004960264478411, "compression_ratio": 1.841860465116279, "no_speech_prob": 2.4300234144902788e-05}, {"id": 936, "seek": 438386, "start": 4402.58, "end": 4408.339999999999, "text": " So if we run that, so we're now going to call data loader on our transform dataset, passing", "tokens": [51300, 407, 498, 321, 1190, 300, 11, 370, 321, 434, 586, 516, 281, 818, 1412, 3677, 260, 322, 527, 4088, 28872, 11, 8437, 51588], "temperature": 0.0, "avg_logprob": -0.2004960264478411, "compression_ratio": 1.841860465116279, "no_speech_prob": 2.4300234144902788e-05}, {"id": 937, "seek": 438386, "start": 4408.339999999999, "end": 4412.7, "text": " in, and remember this is a function that returns a function.", "tokens": [51588, 294, 11, 293, 1604, 341, 307, 257, 2445, 300, 11247, 257, 2445, 13, 51806], "temperature": 0.0, "avg_logprob": -0.2004960264478411, "compression_ratio": 1.841860465116279, "no_speech_prob": 2.4300234144902788e-05}, {"id": 938, "seek": 441270, "start": 4412.7, "end": 4417.9, "text": " So it's a collation function for this dataset, and there it is.", "tokens": [50364, 407, 309, 311, 257, 1263, 399, 2445, 337, 341, 28872, 11, 293, 456, 309, 307, 13, 50624], "temperature": 0.0, "avg_logprob": -0.23441768827892484, "compression_ratio": 1.7253218884120172, "no_speech_prob": 2.3187463739304803e-05}, {"id": 939, "seek": 441270, "start": 4417.9, "end": 4421.12, "text": " So now this looks a lot like what we had in our previous notebook.", "tokens": [50624, 407, 586, 341, 1542, 257, 688, 411, 437, 321, 632, 294, 527, 3894, 21060, 13, 50785], "temperature": 0.0, "avg_logprob": -0.23441768827892484, "compression_ratio": 1.7253218884120172, "no_speech_prob": 2.3187463739304803e-05}, {"id": 940, "seek": 441270, "start": 4421.12, "end": 4425.639999999999, "text": " This is not returning a dictionary, but it's returning a tuple.", "tokens": [50785, 639, 307, 406, 12678, 257, 25890, 11, 457, 309, 311, 12678, 257, 2604, 781, 13, 51011], "temperature": 0.0, "avg_logprob": -0.23441768827892484, "compression_ratio": 1.7253218884120172, "no_speech_prob": 2.3187463739304803e-05}, {"id": 941, "seek": 441270, "start": 4425.639999999999, "end": 4433.62, "text": " So this is a really important idea, particularly for working with hugging face datasets, is", "tokens": [51011, 407, 341, 307, 257, 534, 1021, 1558, 11, 4098, 337, 1364, 365, 41706, 1851, 42856, 11, 307, 51410], "temperature": 0.0, "avg_logprob": -0.23441768827892484, "compression_ratio": 1.7253218884120172, "no_speech_prob": 2.3187463739304803e-05}, {"id": 942, "seek": 441270, "start": 4433.62, "end": 4440.5, "text": " that they tend to do things with dictionaries, and most other things in the PyTorch world", "tokens": [51410, 300, 436, 3928, 281, 360, 721, 365, 22352, 4889, 11, 293, 881, 661, 721, 294, 264, 9953, 51, 284, 339, 1002, 51754], "temperature": 0.0, "avg_logprob": -0.23441768827892484, "compression_ratio": 1.7253218884120172, "no_speech_prob": 2.3187463739304803e-05}, {"id": 943, "seek": 441270, "start": 4440.5, "end": 4442.46, "text": " tend to work with tuples.", "tokens": [51754, 3928, 281, 589, 365, 2604, 2622, 13, 51852], "temperature": 0.0, "avg_logprob": -0.23441768827892484, "compression_ratio": 1.7253218884120172, "no_speech_prob": 2.3187463739304803e-05}, {"id": 944, "seek": 444246, "start": 4443.22, "end": 4449.7, "text": " So you can just use this now to convert anything that returns dictionaries into something that", "tokens": [50402, 407, 291, 393, 445, 764, 341, 586, 281, 7620, 1340, 300, 11247, 22352, 4889, 666, 746, 300, 50726], "temperature": 0.0, "avg_logprob": -0.27233586166844226, "compression_ratio": 1.5780590717299579, "no_speech_prob": 9.028028580360115e-05}, {"id": 945, "seek": 444246, "start": 4449.7, "end": 4454.74, "text": " provides tuples, by passing it as a collation function to your data loader.", "tokens": [50726, 6417, 2604, 2622, 11, 538, 8437, 309, 382, 257, 1263, 399, 2445, 281, 428, 1412, 3677, 260, 13, 50978], "temperature": 0.0, "avg_logprob": -0.27233586166844226, "compression_ratio": 1.5780590717299579, "no_speech_prob": 9.028028580360115e-05}, {"id": 946, "seek": 444246, "start": 4454.74, "end": 4460.3, "text": " So remember, you know, the thing you want to be doing this week is doing things like", "tokens": [50978, 407, 1604, 11, 291, 458, 11, 264, 551, 291, 528, 281, 312, 884, 341, 1243, 307, 884, 721, 411, 51256], "temperature": 0.0, "avg_logprob": -0.27233586166844226, "compression_ratio": 1.5780590717299579, "no_speech_prob": 9.028028580360115e-05}, {"id": 947, "seek": 444246, "start": 4460.3, "end": 4465.02, "text": " import PDB, PDB.setTrace, right.", "tokens": [51256, 974, 10464, 33, 11, 10464, 33, 13, 3854, 14252, 617, 11, 558, 13, 51492], "temperature": 0.0, "avg_logprob": -0.27233586166844226, "compression_ratio": 1.5780590717299579, "no_speech_prob": 9.028028580360115e-05}, {"id": 948, "seek": 444246, "start": 4465.02, "end": 4470.72, "text": " Put breakpoints, step through, see exactly what's happening, you know, not just here,", "tokens": [51492, 4935, 1821, 20552, 11, 1823, 807, 11, 536, 2293, 437, 311, 2737, 11, 291, 458, 11, 406, 445, 510, 11, 51777], "temperature": 0.0, "avg_logprob": -0.27233586166844226, "compression_ratio": 1.5780590717299579, "no_speech_prob": 9.028028580360115e-05}, {"id": 949, "seek": 447072, "start": 4470.72, "end": 4481.16, "text": " but also, even more importantly, doing it inside, inside the inner function, so then", "tokens": [50364, 457, 611, 11, 754, 544, 8906, 11, 884, 309, 1854, 11, 1854, 264, 7284, 2445, 11, 370, 550, 50886], "temperature": 0.0, "avg_logprob": -0.4815363883972168, "compression_ratio": 1.304, "no_speech_prob": 0.0010987225687131286}, {"id": 950, "seek": 447072, "start": 4481.16, "end": 4486.56, "text": " you can see, what did I do wrong there?", "tokens": [50886, 291, 393, 536, 11, 437, 630, 286, 360, 2085, 456, 30, 51156], "temperature": 0.0, "avg_logprob": -0.4815363883972168, "compression_ratio": 1.304, "no_speech_prob": 0.0010987225687131286}, {"id": 951, "seek": 447072, "start": 4486.56, "end": 4495.64, "text": " Oh, did I, oops, set underscore trace.", "tokens": [51156, 876, 11, 630, 286, 11, 34166, 11, 992, 37556, 13508, 13, 51610], "temperature": 0.0, "avg_logprob": -0.4815363883972168, "compression_ratio": 1.304, "no_speech_prob": 0.0010987225687131286}, {"id": 952, "seek": 449564, "start": 4495.64, "end": 4507.08, "text": " So then we can see exactly what's going on, print out B, list the code, and I could step", "tokens": [50364, 407, 550, 321, 393, 536, 2293, 437, 311, 516, 322, 11, 4482, 484, 363, 11, 1329, 264, 3089, 11, 293, 286, 727, 1823, 50936], "temperature": 0.0, "avg_logprob": -0.22749167994449013, "compression_ratio": 1.5885167464114833, "no_speech_prob": 1.4738979189132806e-05}, {"id": 953, "seek": 449564, "start": 4507.08, "end": 4514.160000000001, "text": " into it, and look, I'm now inside the default collate function, which is inside PyTorch,", "tokens": [50936, 666, 309, 11, 293, 574, 11, 286, 478, 586, 1854, 264, 7576, 1263, 473, 2445, 11, 597, 307, 1854, 9953, 51, 284, 339, 11, 51290], "temperature": 0.0, "avg_logprob": -0.22749167994449013, "compression_ratio": 1.5885167464114833, "no_speech_prob": 1.4738979189132806e-05}, {"id": 954, "seek": 449564, "start": 4514.160000000001, "end": 4518.400000000001, "text": " and so I can now see exactly how that works, and there it all is.", "tokens": [51290, 293, 370, 286, 393, 586, 536, 2293, 577, 300, 1985, 11, 293, 456, 309, 439, 307, 13, 51502], "temperature": 0.0, "avg_logprob": -0.22749167994449013, "compression_ratio": 1.5885167464114833, "no_speech_prob": 1.4738979189132806e-05}, {"id": 955, "seek": 449564, "start": 4518.400000000001, "end": 4523.8, "text": " So it's going to go through, and this code is going to look very familiar, because we've", "tokens": [51502, 407, 309, 311, 516, 281, 352, 807, 11, 293, 341, 3089, 307, 516, 281, 574, 588, 4963, 11, 570, 321, 600, 51772], "temperature": 0.0, "avg_logprob": -0.22749167994449013, "compression_ratio": 1.5885167464114833, "no_speech_prob": 1.4738979189132806e-05}, {"id": 956, "seek": 452380, "start": 4523.8, "end": 4528.84, "text": " implemented all this ourselves, except it's being careful, it works for lots of different", "tokens": [50364, 12270, 439, 341, 4175, 11, 3993, 309, 311, 885, 5026, 11, 309, 1985, 337, 3195, 295, 819, 50616], "temperature": 0.0, "avg_logprob": -0.2573921815404352, "compression_ratio": 1.6408163265306122, "no_speech_prob": 0.003376586362719536}, {"id": 957, "seek": 452380, "start": 4528.84, "end": 4538.4400000000005, "text": " types of things, dictionaries, NumPy arrays, so on and so forth.", "tokens": [50616, 3467, 295, 721, 11, 22352, 4889, 11, 22592, 47, 88, 41011, 11, 370, 322, 293, 370, 5220, 13, 51096], "temperature": 0.0, "avg_logprob": -0.2573921815404352, "compression_ratio": 1.6408163265306122, "no_speech_prob": 0.003376586362719536}, {"id": 958, "seek": 452380, "start": 4538.4400000000005, "end": 4542.4400000000005, "text": " So the first thing I wanted to do, oh, actually, something I did want to mention here, this", "tokens": [51096, 407, 264, 700, 551, 286, 1415, 281, 360, 11, 1954, 11, 767, 11, 746, 286, 630, 528, 281, 2152, 510, 11, 341, 51296], "temperature": 0.0, "avg_logprob": -0.2573921815404352, "compression_ratio": 1.6408163265306122, "no_speech_prob": 0.003376586362719536}, {"id": 959, "seek": 452380, "start": 4542.4400000000005, "end": 4549.08, "text": " is so useful, we want to be able to use it in all of our notebooks.", "tokens": [51296, 307, 370, 4420, 11, 321, 528, 281, 312, 1075, 281, 764, 309, 294, 439, 295, 527, 43782, 13, 51628], "temperature": 0.0, "avg_logprob": -0.2573921815404352, "compression_ratio": 1.6408163265306122, "no_speech_prob": 0.003376586362719536}, {"id": 960, "seek": 452380, "start": 4549.08, "end": 4552.64, "text": " So rather than copying and pasting this every time, it would be really nice to create a", "tokens": [51628, 407, 2831, 813, 27976, 293, 1791, 278, 341, 633, 565, 11, 309, 576, 312, 534, 1481, 281, 1884, 257, 51806], "temperature": 0.0, "avg_logprob": -0.2573921815404352, "compression_ratio": 1.6408163265306122, "no_speech_prob": 0.003376586362719536}, {"id": 961, "seek": 455264, "start": 4552.72, "end": 4556.56, "text": " Python module that contains this definition.", "tokens": [50368, 15329, 10088, 300, 8306, 341, 7123, 13, 50560], "temperature": 0.0, "avg_logprob": -0.2694639616374728, "compression_ratio": 1.5906735751295338, "no_speech_prob": 0.002672999631613493}, {"id": 962, "seek": 455264, "start": 4556.56, "end": 4564.360000000001, "text": " So we've created a library called nbdev, it's really a whole system, called nbdev, which", "tokens": [50560, 407, 321, 600, 2942, 257, 6405, 1219, 297, 65, 40343, 11, 309, 311, 534, 257, 1379, 1185, 11, 1219, 297, 65, 40343, 11, 597, 50950], "temperature": 0.0, "avg_logprob": -0.2694639616374728, "compression_ratio": 1.5906735751295338, "no_speech_prob": 0.002672999631613493}, {"id": 963, "seek": 455264, "start": 4564.360000000001, "end": 4569.88, "text": " does exactly that, it creates modules you can use from your notebooks, and the way you", "tokens": [50950, 775, 2293, 300, 11, 309, 7829, 16679, 291, 393, 764, 490, 428, 43782, 11, 293, 264, 636, 291, 51226], "temperature": 0.0, "avg_logprob": -0.2694639616374728, "compression_ratio": 1.5906735751295338, "no_speech_prob": 0.002672999631613493}, {"id": 964, "seek": 455264, "start": 4569.88, "end": 4579.280000000001, "text": " do it is you use this special thing we call comment directives, which is hashpipe, and", "tokens": [51226, 360, 309, 307, 291, 764, 341, 2121, 551, 321, 818, 2871, 2047, 1539, 11, 597, 307, 22019, 50042, 11, 293, 51696], "temperature": 0.0, "avg_logprob": -0.2694639616374728, "compression_ratio": 1.5906735751295338, "no_speech_prob": 0.002672999631613493}, {"id": 965, "seek": 457928, "start": 4579.28, "end": 4582.44, "text": " then hashpipe export, so you put this at the top of a cell, and it says do something", "tokens": [50364, 550, 22019, 50042, 10725, 11, 370, 291, 829, 341, 412, 264, 1192, 295, 257, 2815, 11, 293, 309, 1619, 360, 746, 50522], "temperature": 0.0, "avg_logprob": -0.2690137933801722, "compression_ratio": 1.875598086124402, "no_speech_prob": 0.012431611306965351}, {"id": 966, "seek": 457928, "start": 4582.44, "end": 4583.44, "text": " special for this cell.", "tokens": [50522, 2121, 337, 341, 2815, 13, 50572], "temperature": 0.0, "avg_logprob": -0.2690137933801722, "compression_ratio": 1.875598086124402, "no_speech_prob": 0.012431611306965351}, {"id": 967, "seek": 457928, "start": 4583.44, "end": 4588.719999999999, "text": " What this does is it says put this into a Python module for me please, export it to", "tokens": [50572, 708, 341, 775, 307, 309, 1619, 829, 341, 666, 257, 15329, 10088, 337, 385, 1767, 11, 10725, 309, 281, 50836], "temperature": 0.0, "avg_logprob": -0.2690137933801722, "compression_ratio": 1.875598086124402, "no_speech_prob": 0.012431611306965351}, {"id": 968, "seek": 457928, "start": 4588.719999999999, "end": 4590.599999999999, "text": " a Python module.", "tokens": [50836, 257, 15329, 10088, 13, 50930], "temperature": 0.0, "avg_logprob": -0.2690137933801722, "compression_ratio": 1.875598086124402, "no_speech_prob": 0.012431611306965351}, {"id": 969, "seek": 457928, "start": 4590.599999999999, "end": 4592.12, "text": " What Python module is it going to put it in?", "tokens": [50930, 708, 15329, 10088, 307, 309, 516, 281, 829, 309, 294, 30, 51006], "temperature": 0.0, "avg_logprob": -0.2690137933801722, "compression_ratio": 1.875598086124402, "no_speech_prob": 0.012431611306965351}, {"id": 970, "seek": 457928, "start": 4592.12, "end": 4598.719999999999, "text": " Well if you go all the way to the top, you tell it what default export module to create,", "tokens": [51006, 1042, 498, 291, 352, 439, 264, 636, 281, 264, 1192, 11, 291, 980, 309, 437, 7576, 10725, 10088, 281, 1884, 11, 51336], "temperature": 0.0, "avg_logprob": -0.2690137933801722, "compression_ratio": 1.875598086124402, "no_speech_prob": 0.012431611306965351}, {"id": 971, "seek": 457928, "start": 4598.719999999999, "end": 4602.2, "text": " so it's going to create a module called datasets.", "tokens": [51336, 370, 309, 311, 516, 281, 1884, 257, 10088, 1219, 42856, 13, 51510], "temperature": 0.0, "avg_logprob": -0.2690137933801722, "compression_ratio": 1.875598086124402, "no_speech_prob": 0.012431611306965351}, {"id": 972, "seek": 460220, "start": 4602.2, "end": 4611.5599999999995, "text": " So what I do at the very end of this module is I've got this line that says import nbdev,", "tokens": [50364, 407, 437, 286, 360, 412, 264, 588, 917, 295, 341, 10088, 307, 286, 600, 658, 341, 1622, 300, 1619, 974, 297, 65, 40343, 11, 50832], "temperature": 0.0, "avg_logprob": -0.2945864590731534, "compression_ratio": 1.359375, "no_speech_prob": 0.02975890040397644}, {"id": 973, "seek": 460220, "start": 4611.5599999999995, "end": 4627.04, "text": " nbdev.nbdev export, and what that's going to do for me is create a library, a Python", "tokens": [50832, 297, 65, 40343, 13, 77, 65, 40343, 10725, 11, 293, 437, 300, 311, 516, 281, 360, 337, 385, 307, 1884, 257, 6405, 11, 257, 15329, 51606], "temperature": 0.0, "avg_logprob": -0.2945864590731534, "compression_ratio": 1.359375, "no_speech_prob": 0.02975890040397644}, {"id": 974, "seek": 462704, "start": 4627.04, "end": 4632.56, "text": " library, it's going to have a datasets.py in it, and we'll see everything that we exported,", "tokens": [50364, 6405, 11, 309, 311, 516, 281, 362, 257, 42856, 13, 8200, 294, 309, 11, 293, 321, 603, 536, 1203, 300, 321, 42055, 11, 50640], "temperature": 0.0, "avg_logprob": -0.32071061385305305, "compression_ratio": 1.5787037037037037, "no_speech_prob": 0.11123709380626678}, {"id": 975, "seek": 462704, "start": 4632.56, "end": 4638.76, "text": " here it is, collate dict, will appear in this for me, and so what that means is now in the", "tokens": [50640, 510, 309, 307, 11, 1263, 473, 12569, 11, 486, 4204, 294, 341, 337, 385, 11, 293, 370, 437, 300, 1355, 307, 586, 294, 264, 50950], "temperature": 0.0, "avg_logprob": -0.32071061385305305, "compression_ratio": 1.5787037037037037, "no_speech_prob": 0.11123709380626678}, {"id": 976, "seek": 462704, "start": 4638.76, "end": 4646.36, "text": " future in my notebooks I will be able to import collate dict from my datasets.", "tokens": [50950, 2027, 294, 452, 43782, 286, 486, 312, 1075, 281, 974, 1263, 473, 12569, 490, 452, 42856, 13, 51330], "temperature": 0.0, "avg_logprob": -0.32071061385305305, "compression_ratio": 1.5787037037037037, "no_speech_prob": 0.11123709380626678}, {"id": 977, "seek": 462704, "start": 4646.36, "end": 4650.36, "text": " Now you might wonder, well how does it know to call it mini AI, what's mini AI?", "tokens": [51330, 823, 291, 1062, 2441, 11, 731, 577, 775, 309, 458, 281, 818, 309, 8382, 7318, 11, 437, 311, 8382, 7318, 30, 51530], "temperature": 0.0, "avg_logprob": -0.32071061385305305, "compression_ratio": 1.5787037037037037, "no_speech_prob": 0.11123709380626678}, {"id": 978, "seek": 465036, "start": 4650.799999999999, "end": 4657.4, "text": " Well in nbdev you create a settings.ini file, where you say what the name of your library", "tokens": [50386, 1042, 294, 297, 65, 40343, 291, 1884, 257, 6257, 13, 3812, 3991, 11, 689, 291, 584, 437, 264, 1315, 295, 428, 6405, 50716], "temperature": 0.0, "avg_logprob": -0.24770307540893555, "compression_ratio": 1.6788617886178863, "no_speech_prob": 0.008577379398047924}, {"id": 979, "seek": 465036, "start": 4657.4, "end": 4659.0, "text": " is.", "tokens": [50716, 307, 13, 50796], "temperature": 0.0, "avg_logprob": -0.24770307540893555, "compression_ratio": 1.6788617886178863, "no_speech_prob": 0.008577379398047924}, {"id": 980, "seek": 465036, "start": 4659.0, "end": 4665.5199999999995, "text": " So we're going to be using this quite a lot now because we're getting to the point where", "tokens": [50796, 407, 321, 434, 516, 281, 312, 1228, 341, 1596, 257, 688, 586, 570, 321, 434, 1242, 281, 264, 935, 689, 51122], "temperature": 0.0, "avg_logprob": -0.24770307540893555, "compression_ratio": 1.6788617886178863, "no_speech_prob": 0.008577379398047924}, {"id": 981, "seek": 465036, "start": 4665.5199999999995, "end": 4668.759999999999, "text": " we're starting to implement stuff that didn't exist before.", "tokens": [51122, 321, 434, 2891, 281, 4445, 1507, 300, 994, 380, 2514, 949, 13, 51284], "temperature": 0.0, "avg_logprob": -0.24770307540893555, "compression_ratio": 1.6788617886178863, "no_speech_prob": 0.008577379398047924}, {"id": 982, "seek": 465036, "start": 4668.759999999999, "end": 4673.4, "text": " So previously most of the stuff, or pretty much all the stuff we've created, I've said", "tokens": [51284, 407, 8046, 881, 295, 264, 1507, 11, 420, 1238, 709, 439, 264, 1507, 321, 600, 2942, 11, 286, 600, 848, 51516], "temperature": 0.0, "avg_logprob": -0.24770307540893555, "compression_ratio": 1.6788617886178863, "no_speech_prob": 0.008577379398047924}, {"id": 983, "seek": 465036, "start": 4673.4, "end": 4680.04, "text": " like oh that already exists in PyTorch, so we don't need it, we just use PyTorches.", "tokens": [51516, 411, 1954, 300, 1217, 8198, 294, 9953, 51, 284, 339, 11, 370, 321, 500, 380, 643, 309, 11, 321, 445, 764, 9953, 51, 284, 3781, 13, 51848], "temperature": 0.0, "avg_logprob": -0.24770307540893555, "compression_ratio": 1.6788617886178863, "no_speech_prob": 0.008577379398047924}, {"id": 984, "seek": 468004, "start": 4680.72, "end": 4683.92, "text": " But we're now getting to a point where we're starting to create stuff that doesn't exist", "tokens": [50398, 583, 321, 434, 586, 1242, 281, 257, 935, 689, 321, 434, 2891, 281, 1884, 1507, 300, 1177, 380, 2514, 50558], "temperature": 0.0, "avg_logprob": -0.21869039972987744, "compression_ratio": 1.7231404958677685, "no_speech_prob": 1.644248368393164e-05}, {"id": 985, "seek": 468004, "start": 4683.92, "end": 4689.6, "text": " anywhere, we've created it ourselves, and so therefore we want to be able to use it", "tokens": [50558, 4992, 11, 321, 600, 2942, 309, 4175, 11, 293, 370, 4412, 321, 528, 281, 312, 1075, 281, 764, 309, 50842], "temperature": 0.0, "avg_logprob": -0.21869039972987744, "compression_ratio": 1.7231404958677685, "no_speech_prob": 1.644248368393164e-05}, {"id": 986, "seek": 468004, "start": 4689.6, "end": 4690.6, "text": " again.", "tokens": [50842, 797, 13, 50892], "temperature": 0.0, "avg_logprob": -0.21869039972987744, "compression_ratio": 1.7231404958677685, "no_speech_prob": 1.644248368393164e-05}, {"id": 987, "seek": 468004, "start": 4690.6, "end": 4696.68, "text": " So during the rest of this course we're going to be building together a library called mini", "tokens": [50892, 407, 1830, 264, 1472, 295, 341, 1164, 321, 434, 516, 281, 312, 2390, 1214, 257, 6405, 1219, 8382, 51196], "temperature": 0.0, "avg_logprob": -0.21869039972987744, "compression_ratio": 1.7231404958677685, "no_speech_prob": 1.644248368393164e-05}, {"id": 988, "seek": 468004, "start": 4696.68, "end": 4703.8, "text": " AI that's going to be our framework, our version of something like fast.ai, maybe it's something", "tokens": [51196, 7318, 300, 311, 516, 281, 312, 527, 8388, 11, 527, 3037, 295, 746, 411, 2370, 13, 1301, 11, 1310, 309, 311, 746, 51552], "temperature": 0.0, "avg_logprob": -0.21869039972987744, "compression_ratio": 1.7231404958677685, "no_speech_prob": 1.644248368393164e-05}, {"id": 989, "seek": 468004, "start": 4703.8, "end": 4709.28, "text": " like what fast.ai3 will end up being, we'll see.", "tokens": [51552, 411, 437, 2370, 13, 1301, 18, 486, 917, 493, 885, 11, 321, 603, 536, 13, 51826], "temperature": 0.0, "avg_logprob": -0.21869039972987744, "compression_ratio": 1.7231404958677685, "no_speech_prob": 1.644248368393164e-05}, {"id": 990, "seek": 470928, "start": 4709.28, "end": 4713.92, "text": " So that's what's going on here.", "tokens": [50364, 407, 300, 311, 437, 311, 516, 322, 510, 13, 50596], "temperature": 0.0, "avg_logprob": -0.2429441824191954, "compression_ratio": 1.6022099447513811, "no_speech_prob": 0.00013341916201170534}, {"id": 991, "seek": 470928, "start": 4713.92, "end": 4718.599999999999, "text": " So we're going to be using, once I start using mini AI, I'll show you exactly how to install", "tokens": [50596, 407, 321, 434, 516, 281, 312, 1228, 11, 1564, 286, 722, 1228, 8382, 7318, 11, 286, 603, 855, 291, 2293, 577, 281, 3625, 50830], "temperature": 0.0, "avg_logprob": -0.2429441824191954, "compression_ratio": 1.6022099447513811, "no_speech_prob": 0.00013341916201170534}, {"id": 992, "seek": 470928, "start": 4718.599999999999, "end": 4720.759999999999, "text": " this, but that's what this export is.", "tokens": [50830, 341, 11, 457, 300, 311, 437, 341, 10725, 307, 13, 50938], "temperature": 0.0, "avg_logprob": -0.2429441824191954, "compression_ratio": 1.6022099447513811, "no_speech_prob": 0.00013341916201170534}, {"id": 993, "seek": 470928, "start": 4720.759999999999, "end": 4727.36, "text": " And so you might have noticed I also had an export on this in place thing, and I also", "tokens": [50938, 400, 370, 291, 1062, 362, 5694, 286, 611, 632, 364, 10725, 322, 341, 294, 1081, 551, 11, 293, 286, 611, 51268], "temperature": 0.0, "avg_logprob": -0.2429441824191954, "compression_ratio": 1.6022099447513811, "no_speech_prob": 0.00013341916201170534}, {"id": 994, "seek": 470928, "start": 4727.36, "end": 4735.04, "text": " had it on my necessary import statements.", "tokens": [51268, 632, 309, 322, 452, 4818, 974, 12363, 13, 51652], "temperature": 0.0, "avg_logprob": -0.2429441824191954, "compression_ratio": 1.6022099447513811, "no_speech_prob": 0.00013341916201170534}, {"id": 995, "seek": 473504, "start": 4735.04, "end": 4740.08, "text": " Okay, we want to be able to see what this data set looks like, so I thought now's a", "tokens": [50364, 1033, 11, 321, 528, 281, 312, 1075, 281, 536, 437, 341, 1412, 992, 1542, 411, 11, 370, 286, 1194, 586, 311, 257, 50616], "temperature": 0.0, "avg_logprob": -0.23891998291015626, "compression_ratio": 1.584033613445378, "no_speech_prob": 0.0053018308244645596}, {"id": 996, "seek": 473504, "start": 4740.08, "end": 4745.72, "text": " good time to talk a bit about plotting, because knowing how to visualize things well is really", "tokens": [50616, 665, 565, 281, 751, 257, 857, 466, 41178, 11, 570, 5276, 577, 281, 23273, 721, 731, 307, 534, 50898], "temperature": 0.0, "avg_logprob": -0.23891998291015626, "compression_ratio": 1.584033613445378, "no_speech_prob": 0.0053018308244645596}, {"id": 997, "seek": 473504, "start": 4745.72, "end": 4751.84, "text": " important, and again the idea is we're not allowed to use fast.ai's plotting library,", "tokens": [50898, 1021, 11, 293, 797, 264, 1558, 307, 321, 434, 406, 4350, 281, 764, 2370, 13, 1301, 311, 41178, 6405, 11, 51204], "temperature": 0.0, "avg_logprob": -0.23891998291015626, "compression_ratio": 1.584033613445378, "no_speech_prob": 0.0053018308244645596}, {"id": 998, "seek": 473504, "start": 4751.84, "end": 4755.6, "text": " so we've got to learn how to do everything ourselves.", "tokens": [51204, 370, 321, 600, 658, 281, 1466, 577, 281, 360, 1203, 4175, 13, 51392], "temperature": 0.0, "avg_logprob": -0.23891998291015626, "compression_ratio": 1.584033613445378, "no_speech_prob": 0.0053018308244645596}, {"id": 999, "seek": 473504, "start": 4755.6, "end": 4761.24, "text": " So here's the basic way to plot an image using matplotlib.", "tokens": [51392, 407, 510, 311, 264, 3875, 636, 281, 7542, 364, 3256, 1228, 3803, 564, 310, 38270, 13, 51674], "temperature": 0.0, "avg_logprob": -0.23891998291015626, "compression_ratio": 1.584033613445378, "no_speech_prob": 0.0053018308244645596}, {"id": 1000, "seek": 476124, "start": 4761.24, "end": 4770.679999999999, "text": " So we can create a batch, grab the x part of it, grab the very first thing in that,", "tokens": [50364, 407, 321, 393, 1884, 257, 15245, 11, 4444, 264, 2031, 644, 295, 309, 11, 4444, 264, 588, 700, 551, 294, 300, 11, 50836], "temperature": 0.0, "avg_logprob": -0.23812406941464073, "compression_ratio": 1.5529411764705883, "no_speech_prob": 0.0012065800838172436}, {"id": 1001, "seek": 476124, "start": 4770.679999999999, "end": 4780.32, "text": " and I am show, means show an image, and here it is, there is our ankle boot.", "tokens": [50836, 293, 286, 669, 855, 11, 1355, 855, 364, 3256, 11, 293, 510, 309, 307, 11, 456, 307, 527, 21999, 11450, 13, 51318], "temperature": 0.0, "avg_logprob": -0.23812406941464073, "compression_ratio": 1.5529411764705883, "no_speech_prob": 0.0012065800838172436}, {"id": 1002, "seek": 476124, "start": 4780.32, "end": 4785.16, "text": " So let's start to think about what stuff we might create which we can export to make this", "tokens": [51318, 407, 718, 311, 722, 281, 519, 466, 437, 1507, 321, 1062, 1884, 597, 321, 393, 10725, 281, 652, 341, 51560], "temperature": 0.0, "avg_logprob": -0.23812406941464073, "compression_ratio": 1.5529411764705883, "no_speech_prob": 0.0012065800838172436}, {"id": 1003, "seek": 476124, "start": 4785.16, "end": 4786.8, "text": " a bit easier.", "tokens": [51560, 257, 857, 3571, 13, 51642], "temperature": 0.0, "avg_logprob": -0.23812406941464073, "compression_ratio": 1.5529411764705883, "no_speech_prob": 0.0012065800838172436}, {"id": 1004, "seek": 478680, "start": 4786.8, "end": 4796.88, "text": " So let's create something called show image, which basically does, I am show, but we're", "tokens": [50364, 407, 718, 311, 1884, 746, 1219, 855, 3256, 11, 597, 1936, 775, 11, 286, 669, 855, 11, 457, 321, 434, 50868], "temperature": 0.0, "avg_logprob": -0.24189462523529495, "compression_ratio": 1.4683544303797469, "no_speech_prob": 0.004468322265893221}, {"id": 1005, "seek": 478680, "start": 4796.88, "end": 4799.900000000001, "text": " going to do a few extra things.", "tokens": [50868, 516, 281, 360, 257, 1326, 2857, 721, 13, 51019], "temperature": 0.0, "avg_logprob": -0.24189462523529495, "compression_ratio": 1.4683544303797469, "no_speech_prob": 0.004468322265893221}, {"id": 1006, "seek": 478680, "start": 4799.900000000001, "end": 4808.2, "text": " We will make sure that it's in the correct axis order.", "tokens": [51019, 492, 486, 652, 988, 300, 309, 311, 294, 264, 3006, 10298, 1668, 13, 51434], "temperature": 0.0, "avg_logprob": -0.24189462523529495, "compression_ratio": 1.4683544303797469, "no_speech_prob": 0.004468322265893221}, {"id": 1007, "seek": 478680, "start": 4808.2, "end": 4812.24, "text": " We will make sure it's not on CUDA, that it's on the CPU.", "tokens": [51434, 492, 486, 652, 988, 309, 311, 406, 322, 29777, 7509, 11, 300, 309, 311, 322, 264, 13199, 13, 51636], "temperature": 0.0, "avg_logprob": -0.24189462523529495, "compression_ratio": 1.4683544303797469, "no_speech_prob": 0.004468322265893221}, {"id": 1008, "seek": 481224, "start": 4812.24, "end": 4820.32, "text": " If it's not a numpy array, we'll convert it to a numpy array.", "tokens": [50364, 759, 309, 311, 406, 257, 1031, 8200, 10225, 11, 321, 603, 7620, 309, 281, 257, 1031, 8200, 10225, 13, 50768], "temperature": 0.0, "avg_logprob": -0.2534499921296772, "compression_ratio": 1.6683937823834196, "no_speech_prob": 0.040236275643110275}, {"id": 1009, "seek": 481224, "start": 4820.32, "end": 4824.599999999999, "text": " We'll be able to pass in an existing axis, which we'll talk about soon if we want to.", "tokens": [50768, 492, 603, 312, 1075, 281, 1320, 294, 364, 6741, 10298, 11, 597, 321, 603, 751, 466, 2321, 498, 321, 528, 281, 13, 50982], "temperature": 0.0, "avg_logprob": -0.2534499921296772, "compression_ratio": 1.6683937823834196, "no_speech_prob": 0.040236275643110275}, {"id": 1010, "seek": 481224, "start": 4824.599999999999, "end": 4829.96, "text": " We'll be able to set a title if we want to, and also this thing here removes all this", "tokens": [50982, 492, 603, 312, 1075, 281, 992, 257, 4876, 498, 321, 528, 281, 11, 293, 611, 341, 551, 510, 30445, 439, 341, 51250], "temperature": 0.0, "avg_logprob": -0.2534499921296772, "compression_ratio": 1.6683937823834196, "no_speech_prob": 0.040236275643110275}, {"id": 1011, "seek": 481224, "start": 4829.96, "end": 4836.66, "text": " ugly 0 5 blah blah blah axis, because we're showing an image, we don't want any of that.", "tokens": [51250, 12246, 1958, 1025, 12288, 12288, 12288, 10298, 11, 570, 321, 434, 4099, 364, 3256, 11, 321, 500, 380, 528, 604, 295, 300, 13, 51585], "temperature": 0.0, "avg_logprob": -0.2534499921296772, "compression_ratio": 1.6683937823834196, "no_speech_prob": 0.040236275643110275}, {"id": 1012, "seek": 483666, "start": 4836.66, "end": 4842.5, "text": " So if we try that, you can see, there we go, we've also been able to say what size", "tokens": [50364, 407, 498, 321, 853, 300, 11, 291, 393, 536, 11, 456, 321, 352, 11, 321, 600, 611, 668, 1075, 281, 584, 437, 2744, 50656], "temperature": 0.0, "avg_logprob": -0.2607154380984423, "compression_ratio": 1.572972972972973, "no_speech_prob": 0.012053514830768108}, {"id": 1013, "seek": 483666, "start": 4842.5, "end": 4846.54, "text": " we want the image, there it all is.", "tokens": [50656, 321, 528, 264, 3256, 11, 456, 309, 439, 307, 13, 50858], "temperature": 0.0, "avg_logprob": -0.2607154380984423, "compression_ratio": 1.572972972972973, "no_speech_prob": 0.012053514830768108}, {"id": 1014, "seek": 483666, "start": 4846.54, "end": 4857.5, "text": " Now here's something interesting, when I say help, the help shows the things that I implemented,", "tokens": [50858, 823, 510, 311, 746, 1880, 11, 562, 286, 584, 854, 11, 264, 854, 3110, 264, 721, 300, 286, 12270, 11, 51406], "temperature": 0.0, "avg_logprob": -0.2607154380984423, "compression_ratio": 1.572972972972973, "no_speech_prob": 0.012053514830768108}, {"id": 1015, "seek": 483666, "start": 4857.5, "end": 4860.74, "text": " but it also shows a whole lot more things.", "tokens": [51406, 457, 309, 611, 3110, 257, 1379, 688, 544, 721, 13, 51568], "temperature": 0.0, "avg_logprob": -0.2607154380984423, "compression_ratio": 1.572972972972973, "no_speech_prob": 0.012053514830768108}, {"id": 1016, "seek": 483666, "start": 4860.74, "end": 4862.099999999999, "text": " How did that magic thing happen?", "tokens": [51568, 1012, 630, 300, 5585, 551, 1051, 30, 51636], "temperature": 0.0, "avg_logprob": -0.2607154380984423, "compression_ratio": 1.572972972972973, "no_speech_prob": 0.012053514830768108}, {"id": 1017, "seek": 486210, "start": 4862.1, "end": 4867.780000000001, "text": " You can see they work, because here's figsize, which I didn't add, oh sorry I did add, well", "tokens": [50364, 509, 393, 536, 436, 589, 11, 570, 510, 311, 2147, 27553, 11, 597, 286, 994, 380, 909, 11, 1954, 2597, 286, 630, 909, 11, 731, 50648], "temperature": 0.0, "avg_logprob": -0.36867415230229217, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.07695295661687851}, {"id": 1018, "seek": 486210, "start": 4867.780000000001, "end": 4869.54, "text": " okay, that's a bad example.", "tokens": [50648, 1392, 11, 300, 311, 257, 1578, 1365, 13, 50736], "temperature": 0.0, "avg_logprob": -0.36867415230229217, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.07695295661687851}, {"id": 1019, "seek": 486210, "start": 4869.54, "end": 4873.5, "text": " Anyway, these other ones all work as well.", "tokens": [50736, 5684, 11, 613, 661, 2306, 439, 589, 382, 731, 13, 50934], "temperature": 0.0, "avg_logprob": -0.36867415230229217, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.07695295661687851}, {"id": 1020, "seek": 486210, "start": 4873.5, "end": 4874.820000000001, "text": " So how did that happen?", "tokens": [50934, 407, 577, 630, 300, 1051, 30, 51000], "temperature": 0.0, "avg_logprob": -0.36867415230229217, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.07695295661687851}, {"id": 1021, "seek": 486210, "start": 4874.820000000001, "end": 4883.700000000001, "text": " Well the trick is that I added star star quags here, and star star quags says, you can pass", "tokens": [51000, 1042, 264, 4282, 307, 300, 286, 3869, 3543, 3543, 421, 12109, 510, 11, 293, 3543, 3543, 421, 12109, 1619, 11, 291, 393, 1320, 51444], "temperature": 0.0, "avg_logprob": -0.36867415230229217, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.07695295661687851}, {"id": 1022, "seek": 486210, "start": 4883.700000000001, "end": 4888.9400000000005, "text": " as many or any other arguments as you like that aren't listed, and they'll all be put", "tokens": [51444, 382, 867, 420, 604, 661, 12869, 382, 291, 411, 300, 3212, 380, 10052, 11, 293, 436, 603, 439, 312, 829, 51706], "temperature": 0.0, "avg_logprob": -0.36867415230229217, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.07695295661687851}, {"id": 1023, "seek": 488894, "start": 4888.94, "end": 4892.339999999999, "text": " into a dictionary with this name.", "tokens": [50364, 666, 257, 25890, 365, 341, 1315, 13, 50534], "temperature": 0.0, "avg_logprob": -0.31934023477944984, "compression_ratio": 1.586734693877551, "no_speech_prob": 0.07920625805854797}, {"id": 1024, "seek": 488894, "start": 4892.339999999999, "end": 4899.86, "text": " And then when I call I am show, I pass that entire dictionary, star star here means as", "tokens": [50534, 400, 550, 562, 286, 818, 286, 669, 855, 11, 286, 1320, 300, 2302, 25890, 11, 3543, 3543, 510, 1355, 382, 50910], "temperature": 0.0, "avg_logprob": -0.31934023477944984, "compression_ratio": 1.586734693877551, "no_speech_prob": 0.07920625805854797}, {"id": 1025, "seek": 488894, "start": 4899.86, "end": 4901.259999999999, "text": " separate arguments.", "tokens": [50910, 4994, 12869, 13, 50980], "temperature": 0.0, "avg_logprob": -0.31934023477944984, "compression_ratio": 1.586734693877551, "no_speech_prob": 0.07920625805854797}, {"id": 1026, "seek": 488894, "start": 4901.259999999999, "end": 4903.74, "text": " And that's how come it works.", "tokens": [50980, 400, 300, 311, 577, 808, 309, 1985, 13, 51104], "temperature": 0.0, "avg_logprob": -0.31934023477944984, "compression_ratio": 1.586734693877551, "no_speech_prob": 0.07920625805854797}, {"id": 1027, "seek": 488894, "start": 4903.74, "end": 4907.66, "text": " And then how come it knows what help to provide?", "tokens": [51104, 400, 550, 577, 808, 309, 3255, 437, 854, 281, 2893, 30, 51300], "temperature": 0.0, "avg_logprob": -0.31934023477944984, "compression_ratio": 1.586734693877551, "no_speech_prob": 0.07920625805854797}, {"id": 1028, "seek": 488894, "start": 4907.66, "end": 4914.419999999999, "text": " The reason why is that Fastcore has a special thing called delegates, which is a decorator,", "tokens": [51300, 440, 1778, 983, 307, 300, 15968, 12352, 575, 257, 2121, 551, 1219, 45756, 11, 597, 307, 257, 7919, 1639, 11, 51638], "temperature": 0.0, "avg_logprob": -0.31934023477944984, "compression_ratio": 1.586734693877551, "no_speech_prob": 0.07920625805854797}, {"id": 1029, "seek": 491442, "start": 4914.42, "end": 4919.78, "text": " so now you know what a decorator is, and you tell it what is it that you're going to", "tokens": [50364, 370, 586, 291, 458, 437, 257, 7919, 1639, 307, 11, 293, 291, 980, 309, 437, 307, 309, 300, 291, 434, 516, 281, 50632], "temperature": 0.0, "avg_logprob": -0.21435456988455234, "compression_ratio": 1.6318407960199004, "no_speech_prob": 0.003593544941395521}, {"id": 1030, "seek": 491442, "start": 4919.78, "end": 4921.38, "text": " be passing quags to.", "tokens": [50632, 312, 8437, 421, 12109, 281, 13, 50712], "temperature": 0.0, "avg_logprob": -0.21435456988455234, "compression_ratio": 1.6318407960199004, "no_speech_prob": 0.003593544941395521}, {"id": 1031, "seek": 491442, "start": 4921.38, "end": 4924.66, "text": " I'm going to be passing it to I am show.", "tokens": [50712, 286, 478, 516, 281, 312, 8437, 309, 281, 286, 669, 855, 13, 50876], "temperature": 0.0, "avg_logprob": -0.21435456988455234, "compression_ratio": 1.6318407960199004, "no_speech_prob": 0.003593544941395521}, {"id": 1032, "seek": 491442, "start": 4924.66, "end": 4933.38, "text": " And then it automatically creates the documentation correctly to show you what quags can do.", "tokens": [50876, 400, 550, 309, 6772, 7829, 264, 14333, 8944, 281, 855, 291, 437, 421, 12109, 393, 360, 13, 51312], "temperature": 0.0, "avg_logprob": -0.21435456988455234, "compression_ratio": 1.6318407960199004, "no_speech_prob": 0.003593544941395521}, {"id": 1033, "seek": 491442, "start": 4933.38, "end": 4939.86, "text": " So this is a really helpful way of being able to kind of extend existing functions, like", "tokens": [51312, 407, 341, 307, 257, 534, 4961, 636, 295, 885, 1075, 281, 733, 295, 10101, 6741, 6828, 11, 411, 51636], "temperature": 0.0, "avg_logprob": -0.21435456988455234, "compression_ratio": 1.6318407960199004, "no_speech_prob": 0.003593544941395521}, {"id": 1034, "seek": 493986, "start": 4939.86, "end": 4944.98, "text": " I am show, and still get all of their functionality and all of their documentation and add your", "tokens": [50364, 286, 669, 855, 11, 293, 920, 483, 439, 295, 641, 14980, 293, 439, 295, 641, 14333, 293, 909, 428, 50620], "temperature": 0.0, "avg_logprob": -0.22164918650751528, "compression_ratio": 1.6590909090909092, "no_speech_prob": 0.15608389675617218}, {"id": 1035, "seek": 493986, "start": 4944.98, "end": 4945.98, "text": " own.", "tokens": [50620, 1065, 13, 50670], "temperature": 0.0, "avg_logprob": -0.22164918650751528, "compression_ratio": 1.6590909090909092, "no_speech_prob": 0.15608389675617218}, {"id": 1036, "seek": 493986, "start": 4945.98, "end": 4950.78, "text": " So delegates is one of the most useful things we have in Fastcore, in my opinion.", "tokens": [50670, 407, 45756, 307, 472, 295, 264, 881, 4420, 721, 321, 362, 294, 15968, 12352, 11, 294, 452, 4800, 13, 50910], "temperature": 0.0, "avg_logprob": -0.22164918650751528, "compression_ratio": 1.6590909090909092, "no_speech_prob": 0.15608389675617218}, {"id": 1037, "seek": 493986, "start": 4950.78, "end": 4957.36, "text": " So we're going to export that, so now we can use show image anytime we want, which is nice.", "tokens": [50910, 407, 321, 434, 516, 281, 10725, 300, 11, 370, 586, 321, 393, 764, 855, 3256, 13038, 321, 528, 11, 597, 307, 1481, 13, 51239], "temperature": 0.0, "avg_logprob": -0.22164918650751528, "compression_ratio": 1.6590909090909092, "no_speech_prob": 0.15608389675617218}, {"id": 1038, "seek": 493986, "start": 4957.36, "end": 4964.339999999999, "text": " Something that's really helpful to know about matplotlib is how to create subplots.", "tokens": [51239, 6595, 300, 311, 534, 4961, 281, 458, 466, 3803, 564, 310, 38270, 307, 577, 281, 1884, 1422, 564, 1971, 13, 51588], "temperature": 0.0, "avg_logprob": -0.22164918650751528, "compression_ratio": 1.6590909090909092, "no_speech_prob": 0.15608389675617218}, {"id": 1039, "seek": 493986, "start": 4964.339999999999, "end": 4968.759999999999, "text": " So for example, what happens if you want to plot two images next to each other?", "tokens": [51588, 407, 337, 1365, 11, 437, 2314, 498, 291, 528, 281, 7542, 732, 5267, 958, 281, 1184, 661, 30, 51809], "temperature": 0.0, "avg_logprob": -0.22164918650751528, "compression_ratio": 1.6590909090909092, "no_speech_prob": 0.15608389675617218}, {"id": 1040, "seek": 496876, "start": 4968.76, "end": 4977.08, "text": " So in matplotlib, subplots creates multiple plots, and you pass it number of rows and", "tokens": [50364, 407, 294, 3803, 564, 310, 38270, 11, 1422, 564, 1971, 7829, 3866, 28609, 11, 293, 291, 1320, 309, 1230, 295, 13241, 293, 50780], "temperature": 0.0, "avg_logprob": -0.21755504608154297, "compression_ratio": 1.5714285714285714, "no_speech_prob": 6.605211819987744e-05}, {"id": 1041, "seek": 496876, "start": 4977.08, "end": 4978.74, "text": " the number of columns.", "tokens": [50780, 264, 1230, 295, 13766, 13, 50863], "temperature": 0.0, "avg_logprob": -0.21755504608154297, "compression_ratio": 1.5714285714285714, "no_speech_prob": 6.605211819987744e-05}, {"id": 1042, "seek": 496876, "start": 4978.74, "end": 4984.24, "text": " So this here has, as you see, one row and two columns.", "tokens": [50863, 407, 341, 510, 575, 11, 382, 291, 536, 11, 472, 5386, 293, 732, 13766, 13, 51138], "temperature": 0.0, "avg_logprob": -0.21755504608154297, "compression_ratio": 1.5714285714285714, "no_speech_prob": 6.605211819987744e-05}, {"id": 1043, "seek": 496876, "start": 4984.24, "end": 4987.280000000001, "text": " And it returns axes.", "tokens": [51138, 400, 309, 11247, 35387, 13, 51290], "temperature": 0.0, "avg_logprob": -0.21755504608154297, "compression_ratio": 1.5714285714285714, "no_speech_prob": 6.605211819987744e-05}, {"id": 1044, "seek": 496876, "start": 4987.280000000001, "end": 4992.56, "text": " Now what it calls axes is what it refers to as the individual plots.", "tokens": [51290, 823, 437, 309, 5498, 35387, 307, 437, 309, 14942, 281, 382, 264, 2609, 28609, 13, 51554], "temperature": 0.0, "avg_logprob": -0.21755504608154297, "compression_ratio": 1.5714285714285714, "no_speech_prob": 6.605211819987744e-05}, {"id": 1045, "seek": 499256, "start": 4992.56, "end": 5000.240000000001, "text": " So if we now call show image on the first image, passing in axes 0, it's going to get", "tokens": [50364, 407, 498, 321, 586, 818, 855, 3256, 322, 264, 700, 3256, 11, 8437, 294, 35387, 1958, 11, 309, 311, 516, 281, 483, 50748], "temperature": 0.0, "avg_logprob": -0.2649756840297154, "compression_ratio": 1.7309644670050761, "no_speech_prob": 0.0111576858907938}, {"id": 1046, "seek": 499256, "start": 5000.240000000001, "end": 5002.92, "text": " that here.", "tokens": [50748, 300, 510, 13, 50882], "temperature": 0.0, "avg_logprob": -0.2649756840297154, "compression_ratio": 1.7309644670050761, "no_speech_prob": 0.0111576858907938}, {"id": 1047, "seek": 499256, "start": 5002.92, "end": 5008.400000000001, "text": " Then we call ax.amshow, that means put the image on this subplot.", "tokens": [50882, 1396, 321, 818, 6360, 13, 335, 34436, 11, 300, 1355, 829, 264, 3256, 322, 341, 1422, 564, 310, 13, 51156], "temperature": 0.0, "avg_logprob": -0.2649756840297154, "compression_ratio": 1.7309644670050761, "no_speech_prob": 0.0111576858907938}, {"id": 1048, "seek": 499256, "start": 5008.400000000001, "end": 5011.280000000001, "text": " They don't call it a subplot, unfortunately, they call it an axis.", "tokens": [51156, 814, 500, 380, 818, 309, 257, 1422, 564, 310, 11, 7015, 11, 436, 818, 309, 364, 10298, 13, 51300], "temperature": 0.0, "avg_logprob": -0.2649756840297154, "compression_ratio": 1.7309644670050761, "no_speech_prob": 0.0111576858907938}, {"id": 1049, "seek": 499256, "start": 5011.280000000001, "end": 5012.92, "text": " Put it on this axis.", "tokens": [51300, 4935, 309, 322, 341, 10298, 13, 51382], "temperature": 0.0, "avg_logprob": -0.2649756840297154, "compression_ratio": 1.7309644670050761, "no_speech_prob": 0.0111576858907938}, {"id": 1050, "seek": 499256, "start": 5012.92, "end": 5018.72, "text": " So that's how come we're able to show an image, one image on the first axis, and then show", "tokens": [51382, 407, 300, 311, 577, 808, 321, 434, 1075, 281, 855, 364, 3256, 11, 472, 3256, 322, 264, 700, 10298, 11, 293, 550, 855, 51672], "temperature": 0.0, "avg_logprob": -0.2649756840297154, "compression_ratio": 1.7309644670050761, "no_speech_prob": 0.0111576858907938}, {"id": 1051, "seek": 501872, "start": 5018.72, "end": 5023.52, "text": " a second image on the second axis, by which we mean subplot.", "tokens": [50364, 257, 1150, 3256, 322, 264, 1150, 10298, 11, 538, 597, 321, 914, 1422, 564, 310, 13, 50604], "temperature": 0.0, "avg_logprob": -0.20168907030493813, "compression_ratio": 1.7489711934156378, "no_speech_prob": 0.010652215220034122}, {"id": 1052, "seek": 501872, "start": 5023.52, "end": 5026.4800000000005, "text": " And there's our two images.", "tokens": [50604, 400, 456, 311, 527, 732, 5267, 13, 50752], "temperature": 0.0, "avg_logprob": -0.20168907030493813, "compression_ratio": 1.7489711934156378, "no_speech_prob": 0.010652215220034122}, {"id": 1053, "seek": 501872, "start": 5026.4800000000005, "end": 5029.04, "text": " So that's pretty handy.", "tokens": [50752, 407, 300, 311, 1238, 13239, 13, 50880], "temperature": 0.0, "avg_logprob": -0.20168907030493813, "compression_ratio": 1.7489711934156378, "no_speech_prob": 0.010652215220034122}, {"id": 1054, "seek": 501872, "start": 5029.04, "end": 5032.6, "text": " So I've decided to add some additional functionality to subplots.", "tokens": [50880, 407, 286, 600, 3047, 281, 909, 512, 4497, 14980, 281, 1422, 564, 1971, 13, 51058], "temperature": 0.0, "avg_logprob": -0.20168907030493813, "compression_ratio": 1.7489711934156378, "no_speech_prob": 0.010652215220034122}, {"id": 1055, "seek": 501872, "start": 5032.6, "end": 5037.92, "text": " So therefore I use delegates on subplots, because I'm adding functionality to it.", "tokens": [51058, 407, 4412, 286, 764, 45756, 322, 1422, 564, 1971, 11, 570, 286, 478, 5127, 14980, 281, 309, 13, 51324], "temperature": 0.0, "avg_logprob": -0.20168907030493813, "compression_ratio": 1.7489711934156378, "no_speech_prob": 0.010652215220034122}, {"id": 1056, "seek": 501872, "start": 5037.92, "end": 5042.68, "text": " And I'm going to be taking quags and passing it through to subplots.", "tokens": [51324, 400, 286, 478, 516, 281, 312, 1940, 421, 12109, 293, 8437, 309, 807, 281, 1422, 564, 1971, 13, 51562], "temperature": 0.0, "avg_logprob": -0.20168907030493813, "compression_ratio": 1.7489711934156378, "no_speech_prob": 0.010652215220034122}, {"id": 1057, "seek": 501872, "start": 5042.68, "end": 5048.52, "text": " And the main thing I wanted to do is to automatically create an appropriate figure size by just", "tokens": [51562, 400, 264, 2135, 551, 286, 1415, 281, 360, 307, 281, 6772, 1884, 364, 6854, 2573, 2744, 538, 445, 51854], "temperature": 0.0, "avg_logprob": -0.20168907030493813, "compression_ratio": 1.7489711934156378, "no_speech_prob": 0.010652215220034122}, {"id": 1058, "seek": 504852, "start": 5049.320000000001, "end": 5052.56, "text": " finding out, you tell us what image size you want.", "tokens": [50404, 5006, 484, 11, 291, 980, 505, 437, 3256, 2744, 291, 528, 13, 50566], "temperature": 0.0, "avg_logprob": -0.24112964498585668, "compression_ratio": 1.6777777777777778, "no_speech_prob": 0.0004238835535943508}, {"id": 1059, "seek": 504852, "start": 5052.56, "end": 5059.6, "text": " And I also want to be able to add a title for the whole set of subplots.", "tokens": [50566, 400, 286, 611, 528, 281, 312, 1075, 281, 909, 257, 4876, 337, 264, 1379, 992, 295, 1422, 564, 1971, 13, 50918], "temperature": 0.0, "avg_logprob": -0.24112964498585668, "compression_ratio": 1.6777777777777778, "no_speech_prob": 0.0004238835535943508}, {"id": 1060, "seek": 504852, "start": 5059.6, "end": 5062.400000000001, "text": " And so there it is.", "tokens": [50918, 400, 370, 456, 309, 307, 13, 51058], "temperature": 0.0, "avg_logprob": -0.24112964498585668, "compression_ratio": 1.6777777777777778, "no_speech_prob": 0.0004238835535943508}, {"id": 1061, "seek": 504852, "start": 5062.400000000001, "end": 5068.160000000001, "text": " And then I also want to show you that it'll automatically, if we want to, create documentation", "tokens": [51058, 400, 550, 286, 611, 528, 281, 855, 291, 300, 309, 603, 6772, 11, 498, 321, 528, 281, 11, 1884, 14333, 51346], "temperature": 0.0, "avg_logprob": -0.24112964498585668, "compression_ratio": 1.6777777777777778, "no_speech_prob": 0.0004238835535943508}, {"id": 1062, "seek": 504852, "start": 5068.160000000001, "end": 5070.76, "text": " for us as well, for our library.", "tokens": [51346, 337, 505, 382, 731, 11, 337, 527, 6405, 13, 51476], "temperature": 0.0, "avg_logprob": -0.24112964498585668, "compression_ratio": 1.6777777777777778, "no_speech_prob": 0.0004238835535943508}, {"id": 1063, "seek": 504852, "start": 5070.76, "end": 5073.360000000001, "text": " And here is the documentation.", "tokens": [51476, 400, 510, 307, 264, 14333, 13, 51606], "temperature": 0.0, "avg_logprob": -0.24112964498585668, "compression_ratio": 1.6777777777777778, "no_speech_prob": 0.0004238835535943508}, {"id": 1064, "seek": 507336, "start": 5073.4, "end": 5080.36, "text": " So as you can see here, for the stuff I've added, it's telling me exactly what each of", "tokens": [50366, 407, 382, 291, 393, 536, 510, 11, 337, 264, 1507, 286, 600, 3869, 11, 309, 311, 3585, 385, 2293, 437, 1184, 295, 50714], "temperature": 0.0, "avg_logprob": -0.2543966255935968, "compression_ratio": 1.6558704453441295, "no_speech_prob": 0.0006771819898858666}, {"id": 1065, "seek": 507336, "start": 5080.36, "end": 5087.28, "text": " these parameters are, their type, their defaults, and information about each one.", "tokens": [50714, 613, 9834, 366, 11, 641, 2010, 11, 641, 7576, 82, 11, 293, 1589, 466, 1184, 472, 13, 51060], "temperature": 0.0, "avg_logprob": -0.2543966255935968, "compression_ratio": 1.6558704453441295, "no_speech_prob": 0.0006771819898858666}, {"id": 1066, "seek": 507336, "start": 5087.28, "end": 5091.2, "text": " And that information is automatically coming from these little comments.", "tokens": [51060, 400, 300, 1589, 307, 6772, 1348, 490, 613, 707, 3053, 13, 51256], "temperature": 0.0, "avg_logprob": -0.2543966255935968, "compression_ratio": 1.6558704453441295, "no_speech_prob": 0.0006771819898858666}, {"id": 1067, "seek": 507336, "start": 5091.2, "end": 5092.54, "text": " We call these documents.", "tokens": [51256, 492, 818, 613, 8512, 13, 51323], "temperature": 0.0, "avg_logprob": -0.2543966255935968, "compression_ratio": 1.6558704453441295, "no_speech_prob": 0.0006771819898858666}, {"id": 1068, "seek": 507336, "start": 5092.54, "end": 5098.259999999999, "text": " This is all automatic stuff done by FastCore and NBDev.", "tokens": [51323, 639, 307, 439, 12509, 1507, 1096, 538, 15968, 34, 418, 293, 426, 33, 11089, 85, 13, 51609], "temperature": 0.0, "avg_logprob": -0.2543966255935968, "compression_ratio": 1.6558704453441295, "no_speech_prob": 0.0006771819898858666}, {"id": 1069, "seek": 507336, "start": 5098.259999999999, "end": 5102.759999999999, "text": " And so you might have noticed when you look at FastAI library documentation, it always", "tokens": [51609, 400, 370, 291, 1062, 362, 5694, 562, 291, 574, 412, 15968, 48698, 6405, 14333, 11, 309, 1009, 51834], "temperature": 0.0, "avg_logprob": -0.2543966255935968, "compression_ratio": 1.6558704453441295, "no_speech_prob": 0.0006771819898858666}, {"id": 1070, "seek": 510276, "start": 5102.76, "end": 5105.04, "text": " has all this info.", "tokens": [50364, 575, 439, 341, 13614, 13, 50478], "temperature": 0.0, "avg_logprob": -0.2623157501220703, "compression_ratio": 1.679245283018868, "no_speech_prob": 8.092748612398282e-05}, {"id": 1071, "seek": 510276, "start": 5105.04, "end": 5106.04, "text": " That's why.", "tokens": [50478, 663, 311, 983, 13, 50528], "temperature": 0.0, "avg_logprob": -0.2623157501220703, "compression_ratio": 1.679245283018868, "no_speech_prob": 8.092748612398282e-05}, {"id": 1072, "seek": 510276, "start": 5106.04, "end": 5108.84, "text": " You don't actually have to call show doc, it automatically added to your documentation", "tokens": [50528, 509, 500, 380, 767, 362, 281, 818, 855, 3211, 11, 309, 6772, 3869, 281, 428, 14333, 50668], "temperature": 0.0, "avg_logprob": -0.2623157501220703, "compression_ratio": 1.679245283018868, "no_speech_prob": 8.092748612398282e-05}, {"id": 1073, "seek": 510276, "start": 5108.84, "end": 5109.84, "text": " for you.", "tokens": [50668, 337, 291, 13, 50718], "temperature": 0.0, "avg_logprob": -0.2623157501220703, "compression_ratio": 1.679245283018868, "no_speech_prob": 8.092748612398282e-05}, {"id": 1074, "seek": 510276, "start": 5109.84, "end": 5113.280000000001, "text": " I'm just showing you here what it's going to end up looking like.", "tokens": [50718, 286, 478, 445, 4099, 291, 510, 437, 309, 311, 516, 281, 917, 493, 1237, 411, 13, 50890], "temperature": 0.0, "avg_logprob": -0.2623157501220703, "compression_ratio": 1.679245283018868, "no_speech_prob": 8.092748612398282e-05}, {"id": 1075, "seek": 510276, "start": 5113.280000000001, "end": 5115.64, "text": " And you can see that it's worked with delegates.", "tokens": [50890, 400, 291, 393, 536, 300, 309, 311, 2732, 365, 45756, 13, 51008], "temperature": 0.0, "avg_logprob": -0.2623157501220703, "compression_ratio": 1.679245283018868, "no_speech_prob": 8.092748612398282e-05}, {"id": 1076, "seek": 510276, "start": 5115.64, "end": 5119.820000000001, "text": " It's put all the extra stuff from delegates in here as well.", "tokens": [51008, 467, 311, 829, 439, 264, 2857, 1507, 490, 45756, 294, 510, 382, 731, 13, 51217], "temperature": 0.0, "avg_logprob": -0.2623157501220703, "compression_ratio": 1.679245283018868, "no_speech_prob": 8.092748612398282e-05}, {"id": 1077, "seek": 510276, "start": 5119.820000000001, "end": 5122.68, "text": " And they all listed out here as well.", "tokens": [51217, 400, 436, 439, 10052, 484, 510, 382, 731, 13, 51360], "temperature": 0.0, "avg_logprob": -0.2623157501220703, "compression_ratio": 1.679245283018868, "no_speech_prob": 8.092748612398282e-05}, {"id": 1078, "seek": 510276, "start": 5122.68, "end": 5124.9800000000005, "text": " So anyway, subplots.", "tokens": [51360, 407, 4033, 11, 1422, 564, 1971, 13, 51475], "temperature": 0.0, "avg_logprob": -0.2623157501220703, "compression_ratio": 1.679245283018868, "no_speech_prob": 8.092748612398282e-05}, {"id": 1079, "seek": 510276, "start": 5124.9800000000005, "end": 5130.400000000001, "text": " So let's create a three by three set of plots.", "tokens": [51475, 407, 718, 311, 1884, 257, 1045, 538, 1045, 992, 295, 28609, 13, 51746], "temperature": 0.0, "avg_logprob": -0.2623157501220703, "compression_ratio": 1.679245283018868, "no_speech_prob": 8.092748612398282e-05}, {"id": 1080, "seek": 510276, "start": 5130.400000000001, "end": 5132.54, "text": " And we'll grab the first two images.", "tokens": [51746, 400, 321, 603, 4444, 264, 700, 732, 5267, 13, 51853], "temperature": 0.0, "avg_logprob": -0.2623157501220703, "compression_ratio": 1.679245283018868, "no_speech_prob": 8.092748612398282e-05}, {"id": 1081, "seek": 513254, "start": 5133.32, "end": 5135.78, "text": " And so now we can go through each of the subplots.", "tokens": [50403, 400, 370, 586, 321, 393, 352, 807, 1184, 295, 264, 1422, 564, 1971, 13, 50526], "temperature": 0.0, "avg_logprob": -0.18928129483113246, "compression_ratio": 1.8160377358490567, "no_speech_prob": 1.3631341062136926e-05}, {"id": 1082, "seek": 513254, "start": 5135.78, "end": 5142.2, "text": " Now it returns it as a three by three, basically a list of three lists of three items.", "tokens": [50526, 823, 309, 11247, 309, 382, 257, 1045, 538, 1045, 11, 1936, 257, 1329, 295, 1045, 14511, 295, 1045, 4754, 13, 50847], "temperature": 0.0, "avg_logprob": -0.18928129483113246, "compression_ratio": 1.8160377358490567, "no_speech_prob": 1.3631341062136926e-05}, {"id": 1083, "seek": 513254, "start": 5142.2, "end": 5146.06, "text": " So I flatten them all out into a single list.", "tokens": [50847, 407, 286, 24183, 552, 439, 484, 666, 257, 2167, 1329, 13, 51040], "temperature": 0.0, "avg_logprob": -0.18928129483113246, "compression_ratio": 1.8160377358490567, "no_speech_prob": 1.3631341062136926e-05}, {"id": 1084, "seek": 513254, "start": 5146.06, "end": 5151.74, "text": " So we'll go through each of those subplots, and go through each image, and show each image", "tokens": [51040, 407, 321, 603, 352, 807, 1184, 295, 729, 1422, 564, 1971, 11, 293, 352, 807, 1184, 3256, 11, 293, 855, 1184, 3256, 51324], "temperature": 0.0, "avg_logprob": -0.18928129483113246, "compression_ratio": 1.8160377358490567, "no_speech_prob": 1.3631341062136926e-05}, {"id": 1085, "seek": 513254, "start": 5151.74, "end": 5152.74, "text": " on each axis.", "tokens": [51324, 322, 1184, 10298, 13, 51374], "temperature": 0.0, "avg_logprob": -0.18928129483113246, "compression_ratio": 1.8160377358490567, "no_speech_prob": 1.3631341062136926e-05}, {"id": 1086, "seek": 513254, "start": 5152.74, "end": 5157.1, "text": " And so here's a quick way to quickly show them all.", "tokens": [51374, 400, 370, 510, 311, 257, 1702, 636, 281, 2661, 855, 552, 439, 13, 51592], "temperature": 0.0, "avg_logprob": -0.18928129483113246, "compression_ratio": 1.8160377358490567, "no_speech_prob": 1.3631341062136926e-05}, {"id": 1087, "seek": 513254, "start": 5157.1, "end": 5159.5199999999995, "text": " As you can see, it's a little bit ugly here.", "tokens": [51592, 1018, 291, 393, 536, 11, 309, 311, 257, 707, 857, 12246, 510, 13, 51713], "temperature": 0.0, "avg_logprob": -0.18928129483113246, "compression_ratio": 1.8160377358490567, "no_speech_prob": 1.3631341062136926e-05}, {"id": 1088, "seek": 515952, "start": 5159.52, "end": 5164.76, "text": " So we'll keep on adding more useful plotting functionality.", "tokens": [50364, 407, 321, 603, 1066, 322, 5127, 544, 4420, 41178, 14980, 13, 50626], "temperature": 0.0, "avg_logprob": -0.22589456917035697, "compression_ratio": 1.6244541484716157, "no_speech_prob": 0.0001355205604340881}, {"id": 1089, "seek": 515952, "start": 5164.76, "end": 5171.160000000001, "text": " So here's something that again, it calls our subplots, delegates to it.", "tokens": [50626, 407, 510, 311, 746, 300, 797, 11, 309, 5498, 527, 1422, 564, 1971, 11, 45756, 281, 309, 13, 50946], "temperature": 0.0, "avg_logprob": -0.22589456917035697, "compression_ratio": 1.6244541484716157, "no_speech_prob": 0.0001355205604340881}, {"id": 1090, "seek": 515952, "start": 5171.160000000001, "end": 5175.8, "text": " But we're going to be able to say, for example, how many subplots do we want?", "tokens": [50946, 583, 321, 434, 516, 281, 312, 1075, 281, 584, 11, 337, 1365, 11, 577, 867, 1422, 564, 1971, 360, 321, 528, 30, 51178], "temperature": 0.0, "avg_logprob": -0.22589456917035697, "compression_ratio": 1.6244541484716157, "no_speech_prob": 0.0001355205604340881}, {"id": 1091, "seek": 515952, "start": 5175.8, "end": 5182.56, "text": " And it'll automatically calculate the rows and the columns.", "tokens": [51178, 400, 309, 603, 6772, 8873, 264, 13241, 293, 264, 13766, 13, 51516], "temperature": 0.0, "avg_logprob": -0.22589456917035697, "compression_ratio": 1.6244541484716157, "no_speech_prob": 0.0001355205604340881}, {"id": 1092, "seek": 515952, "start": 5182.56, "end": 5187.68, "text": " And it's going to remove the axes for any ones that we're not actually using.", "tokens": [51516, 400, 309, 311, 516, 281, 4159, 264, 35387, 337, 604, 2306, 300, 321, 434, 406, 767, 1228, 13, 51772], "temperature": 0.0, "avg_logprob": -0.22589456917035697, "compression_ratio": 1.6244541484716157, "no_speech_prob": 0.0001355205604340881}, {"id": 1093, "seek": 515952, "start": 5187.68, "end": 5188.8, "text": " And so here we got that.", "tokens": [51772, 400, 370, 510, 321, 658, 300, 13, 51828], "temperature": 0.0, "avg_logprob": -0.22589456917035697, "compression_ratio": 1.6244541484716157, "no_speech_prob": 0.0001355205604340881}, {"id": 1094, "seek": 518880, "start": 5188.84, "end": 5190.8, "text": " So that's what getGrid's going to let us do.", "tokens": [50366, 407, 300, 311, 437, 483, 38, 8558, 311, 516, 281, 718, 505, 360, 13, 50464], "temperature": 0.0, "avg_logprob": -0.23059444427490233, "compression_ratio": 1.5851063829787233, "no_speech_prob": 2.6841992166737327e-06}, {"id": 1095, "seek": 518880, "start": 5190.8, "end": 5193.76, "text": " So we're getting quite close.", "tokens": [50464, 407, 321, 434, 1242, 1596, 1998, 13, 50612], "temperature": 0.0, "avg_logprob": -0.23059444427490233, "compression_ratio": 1.5851063829787233, "no_speech_prob": 2.6841992166737327e-06}, {"id": 1096, "seek": 518880, "start": 5193.76, "end": 5200.360000000001, "text": " And so finally, why don't we just create a single thing called showImages that's going", "tokens": [50612, 400, 370, 2721, 11, 983, 500, 380, 321, 445, 1884, 257, 2167, 551, 1219, 855, 31128, 1660, 300, 311, 516, 50942], "temperature": 0.0, "avg_logprob": -0.23059444427490233, "compression_ratio": 1.5851063829787233, "no_speech_prob": 2.6841992166737327e-06}, {"id": 1097, "seek": 518880, "start": 5200.360000000001, "end": 5202.24, "text": " to get our grid.", "tokens": [50942, 281, 483, 527, 10748, 13, 51036], "temperature": 0.0, "avg_logprob": -0.23059444427490233, "compression_ratio": 1.5851063829787233, "no_speech_prob": 2.6841992166737327e-06}, {"id": 1098, "seek": 518880, "start": 5202.24, "end": 5213.12, "text": " And it's going to go through our images, optionally with a list of titles, and show each one.", "tokens": [51036, 400, 309, 311, 516, 281, 352, 807, 527, 5267, 11, 3614, 379, 365, 257, 1329, 295, 12992, 11, 293, 855, 1184, 472, 13, 51580], "temperature": 0.0, "avg_logprob": -0.23059444427490233, "compression_ratio": 1.5851063829787233, "no_speech_prob": 2.6841992166737327e-06}, {"id": 1099, "seek": 518880, "start": 5213.12, "end": 5214.4400000000005, "text": " And we can use that here.", "tokens": [51580, 400, 321, 393, 764, 300, 510, 13, 51646], "temperature": 0.0, "avg_logprob": -0.23059444427490233, "compression_ratio": 1.5851063829787233, "no_speech_prob": 2.6841992166737327e-06}, {"id": 1100, "seek": 521444, "start": 5214.44, "end": 5229.12, "text": " You can see we have successfully got all of our labeled images.", "tokens": [50364, 509, 393, 536, 321, 362, 10727, 658, 439, 295, 527, 21335, 5267, 13, 51098], "temperature": 0.0, "avg_logprob": -0.20489953813098727, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.020331094041466713}, {"id": 1101, "seek": 521444, "start": 5229.12, "end": 5233.799999999999, "text": " And so we, yeah, I think all this stuff for the plotting is pretty useful.", "tokens": [51098, 400, 370, 321, 11, 1338, 11, 286, 519, 439, 341, 1507, 337, 264, 41178, 307, 1238, 4420, 13, 51332], "temperature": 0.0, "avg_logprob": -0.20489953813098727, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.020331094041466713}, {"id": 1102, "seek": 521444, "start": 5233.799999999999, "end": 5236.879999999999, "text": " So as you might have noticed, they were all exported.", "tokens": [51332, 407, 382, 291, 1062, 362, 5694, 11, 436, 645, 439, 42055, 13, 51486], "temperature": 0.0, "avg_logprob": -0.20489953813098727, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.020331094041466713}, {"id": 1103, "seek": 521444, "start": 5236.879999999999, "end": 5242.66, "text": " So in our datasets.py, we've got our getGrid, we've got our subplots, we've got our showImages.", "tokens": [51486, 407, 294, 527, 42856, 13, 8200, 11, 321, 600, 658, 527, 483, 38, 8558, 11, 321, 600, 658, 527, 1422, 564, 1971, 11, 321, 600, 658, 527, 855, 31128, 1660, 13, 51775], "temperature": 0.0, "avg_logprob": -0.20489953813098727, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.020331094041466713}, {"id": 1104, "seek": 524266, "start": 5242.66, "end": 5246.42, "text": " So that's going to make life easier for us now, since we have to create everything from", "tokens": [50364, 407, 300, 311, 516, 281, 652, 993, 3571, 337, 505, 586, 11, 1670, 321, 362, 281, 1884, 1203, 490, 50552], "temperature": 0.0, "avg_logprob": -0.314743485561637, "compression_ratio": 1.4894736842105263, "no_speech_prob": 3.0718179914401844e-05}, {"id": 1105, "seek": 524266, "start": 5246.42, "end": 5248.3, "text": " scratch.", "tokens": [50552, 8459, 13, 50646], "temperature": 0.0, "avg_logprob": -0.314743485561637, "compression_ratio": 1.4894736842105263, "no_speech_prob": 3.0718179914401844e-05}, {"id": 1106, "seek": 524266, "start": 5248.3, "end": 5251.26, "text": " We have created all of those things.", "tokens": [50646, 492, 362, 2942, 439, 295, 729, 721, 13, 50794], "temperature": 0.0, "avg_logprob": -0.314743485561637, "compression_ratio": 1.4894736842105263, "no_speech_prob": 3.0718179914401844e-05}, {"id": 1107, "seek": 524266, "start": 5251.26, "end": 5260.74, "text": " So as I mentioned at the very end, we have this one line of code to run.", "tokens": [50794, 407, 382, 286, 2835, 412, 264, 588, 917, 11, 321, 362, 341, 472, 1622, 295, 3089, 281, 1190, 13, 51268], "temperature": 0.0, "avg_logprob": -0.314743485561637, "compression_ratio": 1.4894736842105263, "no_speech_prob": 3.0718179914401844e-05}, {"id": 1108, "seek": 524266, "start": 5260.74, "end": 5271.9, "text": " And so just to show you, if I remove many.ai.datasets.py, so it's all empty.", "tokens": [51268, 400, 370, 445, 281, 855, 291, 11, 498, 286, 4159, 867, 13, 1301, 13, 20367, 296, 1385, 13, 8200, 11, 370, 309, 311, 439, 6707, 13, 51826], "temperature": 0.0, "avg_logprob": -0.314743485561637, "compression_ratio": 1.4894736842105263, "no_speech_prob": 3.0718179914401844e-05}, {"id": 1109, "seek": 527190, "start": 5272.9, "end": 5279.82, "text": " And then I run this line of code, and now it's back, as you can see.", "tokens": [50414, 400, 550, 286, 1190, 341, 1622, 295, 3089, 11, 293, 586, 309, 311, 646, 11, 382, 291, 393, 536, 13, 50760], "temperature": 0.0, "avg_logprob": -0.3107486990995185, "compression_ratio": 1.547872340425532, "no_speech_prob": 8.349619020009413e-05}, {"id": 1110, "seek": 527190, "start": 5279.82, "end": 5283.339999999999, "text": " And it tells you it's auto-generated.", "tokens": [50760, 400, 309, 5112, 291, 309, 311, 8399, 12, 21848, 770, 13, 50936], "temperature": 0.0, "avg_logprob": -0.3107486990995185, "compression_ratio": 1.547872340425532, "no_speech_prob": 8.349619020009413e-05}, {"id": 1111, "seek": 527190, "start": 5283.339999999999, "end": 5286.139999999999, "text": " All right.", "tokens": [50936, 1057, 558, 13, 51076], "temperature": 0.0, "avg_logprob": -0.3107486990995185, "compression_ratio": 1.547872340425532, "no_speech_prob": 8.349619020009413e-05}, {"id": 1112, "seek": 527190, "start": 5286.139999999999, "end": 5293.5, "text": " So we are nearly at the point where we can build our learner.", "tokens": [51076, 407, 321, 366, 6217, 412, 264, 935, 689, 321, 393, 1322, 527, 33347, 13, 51444], "temperature": 0.0, "avg_logprob": -0.3107486990995185, "compression_ratio": 1.547872340425532, "no_speech_prob": 8.349619020009413e-05}, {"id": 1113, "seek": 527190, "start": 5293.5, "end": 5298.94, "text": " And once we've built our learner, we're going to be able to really dive deep into training", "tokens": [51444, 400, 1564, 321, 600, 3094, 527, 33347, 11, 321, 434, 516, 281, 312, 1075, 281, 534, 9192, 2452, 666, 3097, 51716], "temperature": 0.0, "avg_logprob": -0.3107486990995185, "compression_ratio": 1.547872340425532, "no_speech_prob": 8.349619020009413e-05}, {"id": 1114, "seek": 527190, "start": 5298.94, "end": 5300.5199999999995, "text": " and studying models.", "tokens": [51716, 293, 7601, 5245, 13, 51795], "temperature": 0.0, "avg_logprob": -0.3107486990995185, "compression_ratio": 1.547872340425532, "no_speech_prob": 8.349619020009413e-05}, {"id": 1115, "seek": 530052, "start": 5300.52, "end": 5304.68, "text": " So we've nearly got all of our infrastructure in place.", "tokens": [50364, 407, 321, 600, 6217, 658, 439, 295, 527, 6896, 294, 1081, 13, 50572], "temperature": 0.0, "avg_logprob": -0.2666279548822447, "compression_ratio": 1.5857142857142856, "no_speech_prob": 8.939682629716117e-06}, {"id": 1116, "seek": 530052, "start": 5304.68, "end": 5315.6, "text": " Before we do, there's some pieces of Python, which not everybody knows, and I want to talk", "tokens": [50572, 4546, 321, 360, 11, 456, 311, 512, 3755, 295, 15329, 11, 597, 406, 2201, 3255, 11, 293, 286, 528, 281, 751, 51118], "temperature": 0.0, "avg_logprob": -0.2666279548822447, "compression_ratio": 1.5857142857142856, "no_speech_prob": 8.939682629716117e-06}, {"id": 1117, "seek": 530052, "start": 5315.6, "end": 5318.780000000001, "text": " about, and computer science concepts I want to talk about.", "tokens": [51118, 466, 11, 293, 3820, 3497, 10392, 286, 528, 281, 751, 466, 13, 51277], "temperature": 0.0, "avg_logprob": -0.2666279548822447, "compression_ratio": 1.5857142857142856, "no_speech_prob": 8.939682629716117e-06}, {"id": 1118, "seek": 530052, "start": 5318.780000000001, "end": 5323.280000000001, "text": " So that's what 06 Foundations is about.", "tokens": [51277, 407, 300, 311, 437, 1958, 21, 8207, 763, 307, 466, 13, 51502], "temperature": 0.0, "avg_logprob": -0.2666279548822447, "compression_ratio": 1.5857142857142856, "no_speech_prob": 8.939682629716117e-06}, {"id": 1119, "seek": 530052, "start": 5323.280000000001, "end": 5329.88, "text": " So this whole section is just going to talk about some stuff in Python that you may not", "tokens": [51502, 407, 341, 1379, 3541, 307, 445, 516, 281, 751, 466, 512, 1507, 294, 15329, 300, 291, 815, 406, 51832], "temperature": 0.0, "avg_logprob": -0.2666279548822447, "compression_ratio": 1.5857142857142856, "no_speech_prob": 8.939682629716117e-06}, {"id": 1120, "seek": 532988, "start": 5329.88, "end": 5334.400000000001, "text": " have come across before.", "tokens": [50364, 362, 808, 2108, 949, 13, 50590], "temperature": 0.0, "avg_logprob": -0.2489868927001953, "compression_ratio": 1.6872037914691944, "no_speech_prob": 0.0004655249649658799}, {"id": 1121, "seek": 532988, "start": 5334.400000000001, "end": 5337.4800000000005, "text": " Or maybe it's a review for some of you as well.", "tokens": [50590, 1610, 1310, 309, 311, 257, 3131, 337, 512, 295, 291, 382, 731, 13, 50744], "temperature": 0.0, "avg_logprob": -0.2489868927001953, "compression_ratio": 1.6872037914691944, "no_speech_prob": 0.0004655249649658799}, {"id": 1122, "seek": 532988, "start": 5337.4800000000005, "end": 5341.0, "text": " And it's all stuff we're going to be using basically in the next notebook.", "tokens": [50744, 400, 309, 311, 439, 1507, 321, 434, 516, 281, 312, 1228, 1936, 294, 264, 958, 21060, 13, 50920], "temperature": 0.0, "avg_logprob": -0.2489868927001953, "compression_ratio": 1.6872037914691944, "no_speech_prob": 0.0004655249649658799}, {"id": 1123, "seek": 532988, "start": 5341.0, "end": 5344.12, "text": " So that's why I wanted to cover it.", "tokens": [50920, 407, 300, 311, 983, 286, 1415, 281, 2060, 309, 13, 51076], "temperature": 0.0, "avg_logprob": -0.2489868927001953, "compression_ratio": 1.6872037914691944, "no_speech_prob": 0.0004655249649658799}, {"id": 1124, "seek": 532988, "start": 5344.12, "end": 5347.28, "text": " So we're going to be creating a learner class.", "tokens": [51076, 407, 321, 434, 516, 281, 312, 4084, 257, 33347, 1508, 13, 51234], "temperature": 0.0, "avg_logprob": -0.2489868927001953, "compression_ratio": 1.6872037914691944, "no_speech_prob": 0.0004655249649658799}, {"id": 1125, "seek": 532988, "start": 5347.28, "end": 5353.52, "text": " So a learner class is going to be a very general purpose training loop, which we can get to", "tokens": [51234, 407, 257, 33347, 1508, 307, 516, 281, 312, 257, 588, 2674, 4334, 3097, 6367, 11, 597, 321, 393, 483, 281, 51546], "temperature": 0.0, "avg_logprob": -0.2489868927001953, "compression_ratio": 1.6872037914691944, "no_speech_prob": 0.0004655249649658799}, {"id": 1126, "seek": 532988, "start": 5353.52, "end": 5355.96, "text": " do anything that we wanted to do.", "tokens": [51546, 360, 1340, 300, 321, 1415, 281, 360, 13, 51668], "temperature": 0.0, "avg_logprob": -0.2489868927001953, "compression_ratio": 1.6872037914691944, "no_speech_prob": 0.0004655249649658799}, {"id": 1127, "seek": 535596, "start": 5356.04, "end": 5360.56, "text": " We're going to be creating things called callbacks to make that happen.", "tokens": [50368, 492, 434, 516, 281, 312, 4084, 721, 1219, 818, 17758, 281, 652, 300, 1051, 13, 50594], "temperature": 0.0, "avg_logprob": -0.28689725471265387, "compression_ratio": 1.6694214876033058, "no_speech_prob": 0.025564158335328102}, {"id": 1128, "seek": 535596, "start": 5360.56, "end": 5364.96, "text": " And so therefore we're going to just spend a few moments talking about what are callbacks,", "tokens": [50594, 400, 370, 4412, 321, 434, 516, 281, 445, 3496, 257, 1326, 6065, 1417, 466, 437, 366, 818, 17758, 11, 50814], "temperature": 0.0, "avg_logprob": -0.28689725471265387, "compression_ratio": 1.6694214876033058, "no_speech_prob": 0.025564158335328102}, {"id": 1129, "seek": 535596, "start": 5364.96, "end": 5372.16, "text": " how are they used in computer science, how are they implemented, look at some examples.", "tokens": [50814, 577, 366, 436, 1143, 294, 3820, 3497, 11, 577, 366, 436, 12270, 11, 574, 412, 512, 5110, 13, 51174], "temperature": 0.0, "avg_logprob": -0.28689725471265387, "compression_ratio": 1.6694214876033058, "no_speech_prob": 0.025564158335328102}, {"id": 1130, "seek": 535596, "start": 5372.16, "end": 5374.04, "text": " They come up a lot.", "tokens": [51174, 814, 808, 493, 257, 688, 13, 51268], "temperature": 0.0, "avg_logprob": -0.28689725471265387, "compression_ratio": 1.6694214876033058, "no_speech_prob": 0.025564158335328102}, {"id": 1131, "seek": 535596, "start": 5374.04, "end": 5380.8, "text": " Perhaps the most common place that you see callbacks in software is for GUI events.", "tokens": [51268, 10517, 264, 881, 2689, 1081, 300, 291, 536, 818, 17758, 294, 4722, 307, 337, 17917, 40, 3931, 13, 51606], "temperature": 0.0, "avg_logprob": -0.28689725471265387, "compression_ratio": 1.6694214876033058, "no_speech_prob": 0.025564158335328102}, {"id": 1132, "seek": 535596, "start": 5380.8, "end": 5383.8, "text": " So for events from some graphical user interface.", "tokens": [51606, 407, 337, 3931, 490, 512, 35942, 4195, 9226, 13, 51756], "temperature": 0.0, "avg_logprob": -0.28689725471265387, "compression_ratio": 1.6694214876033058, "no_speech_prob": 0.025564158335328102}, {"id": 1133, "seek": 538380, "start": 5383.8, "end": 5390.76, "text": " So the main graphical user interface library in Jupyter notebooks is called ipywidgets.", "tokens": [50364, 407, 264, 2135, 35942, 4195, 9226, 6405, 294, 22125, 88, 391, 43782, 307, 1219, 28501, 88, 17697, 16284, 13, 50712], "temperature": 0.0, "avg_logprob": -0.2506087665826502, "compression_ratio": 1.4848484848484849, "no_speech_prob": 6.605196540476754e-05}, {"id": 1134, "seek": 538380, "start": 5390.76, "end": 5396.64, "text": " And we can create a widget, like a button, like so.", "tokens": [50712, 400, 321, 393, 1884, 257, 34047, 11, 411, 257, 2960, 11, 411, 370, 13, 51006], "temperature": 0.0, "avg_logprob": -0.2506087665826502, "compression_ratio": 1.4848484848484849, "no_speech_prob": 6.605196540476754e-05}, {"id": 1135, "seek": 538380, "start": 5396.64, "end": 5400.16, "text": " And when we display it, it shows me a button.", "tokens": [51006, 400, 562, 321, 4674, 309, 11, 309, 3110, 385, 257, 2960, 13, 51182], "temperature": 0.0, "avg_logprob": -0.2506087665826502, "compression_ratio": 1.4848484848484849, "no_speech_prob": 6.605196540476754e-05}, {"id": 1136, "seek": 538380, "start": 5400.16, "end": 5405.4400000000005, "text": " And at the moment it doesn't do anything, if I click on it.", "tokens": [51182, 400, 412, 264, 1623, 309, 1177, 380, 360, 1340, 11, 498, 286, 2052, 322, 309, 13, 51446], "temperature": 0.0, "avg_logprob": -0.2506087665826502, "compression_ratio": 1.4848484848484849, "no_speech_prob": 6.605196540476754e-05}, {"id": 1137, "seek": 540544, "start": 5405.44, "end": 5416.08, "text": " What we can do though is we can add an onClick callback to it, which is something which is", "tokens": [50364, 708, 321, 393, 360, 1673, 307, 321, 393, 909, 364, 322, 9966, 618, 818, 3207, 281, 309, 11, 597, 307, 746, 597, 307, 50896], "temperature": 0.0, "avg_logprob": -0.24380773785470547, "compression_ratio": 1.7975460122699387, "no_speech_prob": 0.005469202529639006}, {"id": 1138, "seek": 540544, "start": 5416.08, "end": 5423.04, "text": " a function, we're going to pass it a function, which is called when you click it.", "tokens": [50896, 257, 2445, 11, 321, 434, 516, 281, 1320, 309, 257, 2445, 11, 597, 307, 1219, 562, 291, 2052, 309, 13, 51244], "temperature": 0.0, "avg_logprob": -0.24380773785470547, "compression_ratio": 1.7975460122699387, "no_speech_prob": 0.005469202529639006}, {"id": 1139, "seek": 540544, "start": 5423.04, "end": 5424.839999999999, "text": " So let's define that function.", "tokens": [51244, 407, 718, 311, 6964, 300, 2445, 13, 51334], "temperature": 0.0, "avg_logprob": -0.24380773785470547, "compression_ratio": 1.7975460122699387, "no_speech_prob": 0.005469202529639006}, {"id": 1140, "seek": 540544, "start": 5424.839999999999, "end": 5433.0, "text": " So I'm going to say w.onClickf is going to assign the f function to the onClick callback.", "tokens": [51334, 407, 286, 478, 516, 281, 584, 261, 13, 266, 9966, 618, 69, 307, 516, 281, 6269, 264, 283, 2445, 281, 264, 322, 9966, 618, 818, 3207, 13, 51742], "temperature": 0.0, "avg_logprob": -0.24380773785470547, "compression_ratio": 1.7975460122699387, "no_speech_prob": 0.005469202529639006}, {"id": 1141, "seek": 543300, "start": 5433.0, "end": 5438.44, "text": " Now if I click this, there you go, it's doing it.", "tokens": [50364, 823, 498, 286, 2052, 341, 11, 456, 291, 352, 11, 309, 311, 884, 309, 13, 50636], "temperature": 0.0, "avg_logprob": -0.23891432625906808, "compression_ratio": 1.7432432432432432, "no_speech_prob": 6.7480500547389966e-06}, {"id": 1142, "seek": 543300, "start": 5438.44, "end": 5439.96, "text": " Now what does that mean?", "tokens": [50636, 823, 437, 775, 300, 914, 30, 50712], "temperature": 0.0, "avg_logprob": -0.23891432625906808, "compression_ratio": 1.7432432432432432, "no_speech_prob": 6.7480500547389966e-06}, {"id": 1143, "seek": 543300, "start": 5439.96, "end": 5445.64, "text": " Well a callback is simply a callable that you've provided.", "tokens": [50712, 1042, 257, 818, 3207, 307, 2935, 257, 818, 712, 300, 291, 600, 5649, 13, 50996], "temperature": 0.0, "avg_logprob": -0.23891432625906808, "compression_ratio": 1.7432432432432432, "no_speech_prob": 6.7480500547389966e-06}, {"id": 1144, "seek": 543300, "start": 5445.64, "end": 5449.84, "text": " So remember a callable is a more general version of a function, so in this case it is a function,", "tokens": [50996, 407, 1604, 257, 818, 712, 307, 257, 544, 2674, 3037, 295, 257, 2445, 11, 370, 294, 341, 1389, 309, 307, 257, 2445, 11, 51206], "temperature": 0.0, "avg_logprob": -0.23891432625906808, "compression_ratio": 1.7432432432432432, "no_speech_prob": 6.7480500547389966e-06}, {"id": 1145, "seek": 543300, "start": 5449.84, "end": 5455.56, "text": " that you've provided, that will be called back to when something happens.", "tokens": [51206, 300, 291, 600, 5649, 11, 300, 486, 312, 1219, 646, 281, 562, 746, 2314, 13, 51492], "temperature": 0.0, "avg_logprob": -0.23891432625906808, "compression_ratio": 1.7432432432432432, "no_speech_prob": 6.7480500547389966e-06}, {"id": 1146, "seek": 543300, "start": 5455.56, "end": 5459.64, "text": " So in this case, if something that's happening is that they're clicking a button.", "tokens": [51492, 407, 294, 341, 1389, 11, 498, 746, 300, 311, 2737, 307, 300, 436, 434, 9697, 257, 2960, 13, 51696], "temperature": 0.0, "avg_logprob": -0.23891432625906808, "compression_ratio": 1.7432432432432432, "no_speech_prob": 6.7480500547389966e-06}, {"id": 1147, "seek": 545964, "start": 5459.64, "end": 5466.320000000001, "text": " So this is how we are defining and using a callback as a GUI event.", "tokens": [50364, 407, 341, 307, 577, 321, 366, 17827, 293, 1228, 257, 818, 3207, 382, 257, 17917, 40, 2280, 13, 50698], "temperature": 0.0, "avg_logprob": -0.2173582355627853, "compression_ratio": 1.5763546798029557, "no_speech_prob": 0.001896906178444624}, {"id": 1148, "seek": 545964, "start": 5466.320000000001, "end": 5470.52, "text": " So basically everything in IPyWidgets, if you want to create your own graphical user", "tokens": [50698, 407, 1936, 1203, 294, 8671, 88, 54, 327, 16284, 11, 498, 291, 528, 281, 1884, 428, 1065, 35942, 4195, 50908], "temperature": 0.0, "avg_logprob": -0.2173582355627853, "compression_ratio": 1.5763546798029557, "no_speech_prob": 0.001896906178444624}, {"id": 1149, "seek": 545964, "start": 5470.52, "end": 5477.96, "text": " interfaces for Jupyter, you can do it with IPyWidgets and by using these callbacks.", "tokens": [50908, 28416, 337, 22125, 88, 391, 11, 291, 393, 360, 309, 365, 8671, 88, 54, 327, 16284, 293, 538, 1228, 613, 818, 17758, 13, 51280], "temperature": 0.0, "avg_logprob": -0.2173582355627853, "compression_ratio": 1.5763546798029557, "no_speech_prob": 0.001896906178444624}, {"id": 1150, "seek": 545964, "start": 5477.96, "end": 5487.360000000001, "text": " So these particular kinds of callbacks are called events, but it's just a callback.", "tokens": [51280, 407, 613, 1729, 3685, 295, 818, 17758, 366, 1219, 3931, 11, 457, 309, 311, 445, 257, 818, 3207, 13, 51750], "temperature": 0.0, "avg_logprob": -0.2173582355627853, "compression_ratio": 1.5763546798029557, "no_speech_prob": 0.001896906178444624}, {"id": 1151, "seek": 548736, "start": 5487.36, "end": 5489.96, "text": " All right, so that's somebody else's callback.", "tokens": [50364, 1057, 558, 11, 370, 300, 311, 2618, 1646, 311, 818, 3207, 13, 50494], "temperature": 0.0, "avg_logprob": -0.27256153311048237, "compression_ratio": 1.59915611814346, "no_speech_prob": 1.1843128959299065e-05}, {"id": 1152, "seek": 548736, "start": 5489.96, "end": 5493.44, "text": " Let's create our own callback.", "tokens": [50494, 961, 311, 1884, 527, 1065, 818, 3207, 13, 50668], "temperature": 0.0, "avg_logprob": -0.27256153311048237, "compression_ratio": 1.59915611814346, "no_speech_prob": 1.1843128959299065e-05}, {"id": 1153, "seek": 548736, "start": 5493.44, "end": 5502.04, "text": " So let's say we've got some very slow calculation, and so it takes a very long time to add up", "tokens": [50668, 407, 718, 311, 584, 321, 600, 658, 512, 588, 2964, 17108, 11, 293, 370, 309, 2516, 257, 588, 938, 565, 281, 909, 493, 51098], "temperature": 0.0, "avg_logprob": -0.27256153311048237, "compression_ratio": 1.59915611814346, "no_speech_prob": 1.1843128959299065e-05}, {"id": 1154, "seek": 548736, "start": 5502.04, "end": 5506.639999999999, "text": " the numbers 0 to 5 squared, because we sleep for a second after each one.", "tokens": [51098, 264, 3547, 1958, 281, 1025, 8889, 11, 570, 321, 2817, 337, 257, 1150, 934, 1184, 472, 13, 51328], "temperature": 0.0, "avg_logprob": -0.27256153311048237, "compression_ratio": 1.59915611814346, "no_speech_prob": 1.1843128959299065e-05}, {"id": 1155, "seek": 548736, "start": 5506.639999999999, "end": 5509.5199999999995, "text": " So let's run our slow calculation, still running.", "tokens": [51328, 407, 718, 311, 1190, 527, 2964, 17108, 11, 920, 2614, 13, 51472], "temperature": 0.0, "avg_logprob": -0.27256153311048237, "compression_ratio": 1.59915611814346, "no_speech_prob": 1.1843128959299065e-05}, {"id": 1156, "seek": 548736, "start": 5509.5199999999995, "end": 5511.24, "text": " Oh, how's it going?", "tokens": [51472, 876, 11, 577, 311, 309, 516, 30, 51558], "temperature": 0.0, "avg_logprob": -0.27256153311048237, "compression_ratio": 1.59915611814346, "no_speech_prob": 1.1843128959299065e-05}, {"id": 1157, "seek": 548736, "start": 5511.24, "end": 5512.88, "text": " Come on, finish our calculation.", "tokens": [51558, 2492, 322, 11, 2413, 527, 17108, 13, 51640], "temperature": 0.0, "avg_logprob": -0.27256153311048237, "compression_ratio": 1.59915611814346, "no_speech_prob": 1.1843128959299065e-05}, {"id": 1158, "seek": 548736, "start": 5512.88, "end": 5514.78, "text": " There we go, the answer is 30.", "tokens": [51640, 821, 321, 352, 11, 264, 1867, 307, 2217, 13, 51735], "temperature": 0.0, "avg_logprob": -0.27256153311048237, "compression_ratio": 1.59915611814346, "no_speech_prob": 1.1843128959299065e-05}, {"id": 1159, "seek": 551478, "start": 5514.78, "end": 5519.82, "text": " Now for a slow calculation like that, such as training a model, it's a slow calculation,", "tokens": [50364, 823, 337, 257, 2964, 17108, 411, 300, 11, 1270, 382, 3097, 257, 2316, 11, 309, 311, 257, 2964, 17108, 11, 50616], "temperature": 0.0, "avg_logprob": -0.2285912587092473, "compression_ratio": 1.6812227074235808, "no_speech_prob": 0.0008693644194863737}, {"id": 1160, "seek": 551478, "start": 5519.82, "end": 5525.66, "text": " it would be nice to do things like, I don't know, print out the loss from time to time,", "tokens": [50616, 309, 576, 312, 1481, 281, 360, 721, 411, 11, 286, 500, 380, 458, 11, 4482, 484, 264, 4470, 490, 565, 281, 565, 11, 50908], "temperature": 0.0, "avg_logprob": -0.2285912587092473, "compression_ratio": 1.6812227074235808, "no_speech_prob": 0.0008693644194863737}, {"id": 1161, "seek": 551478, "start": 5525.66, "end": 5528.46, "text": " or show a progress bar, or whatever.", "tokens": [50908, 420, 855, 257, 4205, 2159, 11, 420, 2035, 13, 51048], "temperature": 0.0, "avg_logprob": -0.2285912587092473, "compression_ratio": 1.6812227074235808, "no_speech_prob": 0.0008693644194863737}, {"id": 1162, "seek": 551478, "start": 5528.46, "end": 5535.219999999999, "text": " So generally for those kinds of things, we would like to define a callback that is called", "tokens": [51048, 407, 5101, 337, 729, 3685, 295, 721, 11, 321, 576, 411, 281, 6964, 257, 818, 3207, 300, 307, 1219, 51386], "temperature": 0.0, "avg_logprob": -0.2285912587092473, "compression_ratio": 1.6812227074235808, "no_speech_prob": 0.0008693644194863737}, {"id": 1163, "seek": 551478, "start": 5535.219999999999, "end": 5541.86, "text": " at the end of each epoch, or batch, or every few seconds, or something like that.", "tokens": [51386, 412, 264, 917, 295, 1184, 30992, 339, 11, 420, 15245, 11, 420, 633, 1326, 3949, 11, 420, 746, 411, 300, 13, 51718], "temperature": 0.0, "avg_logprob": -0.2285912587092473, "compression_ratio": 1.6812227074235808, "no_speech_prob": 0.0008693644194863737}, {"id": 1164, "seek": 554186, "start": 5541.94, "end": 5546.78, "text": " So here's how we can modify our slow calculation routine, such that you can optionally pass", "tokens": [50368, 407, 510, 311, 577, 321, 393, 16927, 527, 2964, 17108, 9927, 11, 1270, 300, 291, 393, 3614, 379, 1320, 50610], "temperature": 0.0, "avg_logprob": -0.2309518934370161, "compression_ratio": 1.7445887445887447, "no_speech_prob": 0.001597839524038136}, {"id": 1165, "seek": 554186, "start": 5546.78, "end": 5548.9, "text": " it a callback.", "tokens": [50610, 309, 257, 818, 3207, 13, 50716], "temperature": 0.0, "avg_logprob": -0.2309518934370161, "compression_ratio": 1.7445887445887447, "no_speech_prob": 0.001597839524038136}, {"id": 1166, "seek": 554186, "start": 5548.9, "end": 5553.179999999999, "text": " And so all of this code is the same, except we've added this one line of code that says,", "tokens": [50716, 400, 370, 439, 295, 341, 3089, 307, 264, 912, 11, 3993, 321, 600, 3869, 341, 472, 1622, 295, 3089, 300, 1619, 11, 50930], "temperature": 0.0, "avg_logprob": -0.2309518934370161, "compression_ratio": 1.7445887445887447, "no_speech_prob": 0.001597839524038136}, {"id": 1167, "seek": 554186, "start": 5553.179999999999, "end": 5561.9, "text": " if there's a callback, then call it, and pass in where we're up to.", "tokens": [50930, 498, 456, 311, 257, 818, 3207, 11, 550, 818, 309, 11, 293, 1320, 294, 689, 321, 434, 493, 281, 13, 51366], "temperature": 0.0, "avg_logprob": -0.2309518934370161, "compression_ratio": 1.7445887445887447, "no_speech_prob": 0.001597839524038136}, {"id": 1168, "seek": 554186, "start": 5561.9, "end": 5564.7, "text": " So then we could create our callback function.", "tokens": [51366, 407, 550, 321, 727, 1884, 527, 818, 3207, 2445, 13, 51506], "temperature": 0.0, "avg_logprob": -0.2309518934370161, "compression_ratio": 1.7445887445887447, "no_speech_prob": 0.001597839524038136}, {"id": 1169, "seek": 554186, "start": 5564.7, "end": 5568.98, "text": " So this is just like we created a callback function f, let's create a show progress callback", "tokens": [51506, 407, 341, 307, 445, 411, 321, 2942, 257, 818, 3207, 2445, 283, 11, 718, 311, 1884, 257, 855, 4205, 818, 3207, 51720], "temperature": 0.0, "avg_logprob": -0.2309518934370161, "compression_ratio": 1.7445887445887447, "no_speech_prob": 0.001597839524038136}, {"id": 1170, "seek": 556898, "start": 5569.379999999999, "end": 5572.78, "text": " function that's going to tell us how far we've got.", "tokens": [50384, 2445, 300, 311, 516, 281, 980, 505, 577, 1400, 321, 600, 658, 13, 50554], "temperature": 0.0, "avg_logprob": -0.2279040536215139, "compression_ratio": 1.681081081081081, "no_speech_prob": 0.0014550602063536644}, {"id": 1171, "seek": 556898, "start": 5572.78, "end": 5579.82, "text": " So now if we call slow calculation passing in our callback, you can see it's going to", "tokens": [50554, 407, 586, 498, 321, 818, 2964, 17108, 8437, 294, 527, 818, 3207, 11, 291, 393, 536, 309, 311, 516, 281, 50906], "temperature": 0.0, "avg_logprob": -0.2279040536215139, "compression_ratio": 1.681081081081081, "no_speech_prob": 0.0014550602063536644}, {"id": 1172, "seek": 556898, "start": 5579.82, "end": 5586.5, "text": " call this function at the end of each step.", "tokens": [50906, 818, 341, 2445, 412, 264, 917, 295, 1184, 1823, 13, 51240], "temperature": 0.0, "avg_logprob": -0.2279040536215139, "compression_ratio": 1.681081081081081, "no_speech_prob": 0.0014550602063536644}, {"id": 1173, "seek": 556898, "start": 5586.5, "end": 5589.339999999999, "text": " So here we've created our own callback.", "tokens": [51240, 407, 510, 321, 600, 2942, 527, 1065, 818, 3207, 13, 51382], "temperature": 0.0, "avg_logprob": -0.2279040536215139, "compression_ratio": 1.681081081081081, "no_speech_prob": 0.0014550602063536644}, {"id": 1174, "seek": 556898, "start": 5589.339999999999, "end": 5595.54, "text": " So there's nothing special about a callback, like it doesn't require its own syntax, it's", "tokens": [51382, 407, 456, 311, 1825, 2121, 466, 257, 818, 3207, 11, 411, 309, 1177, 380, 3651, 1080, 1065, 28431, 11, 309, 311, 51692], "temperature": 0.0, "avg_logprob": -0.2279040536215139, "compression_ratio": 1.681081081081081, "no_speech_prob": 0.0014550602063536644}, {"id": 1175, "seek": 559554, "start": 5595.54, "end": 5599.58, "text": " not a new concept, it's just an idea, really.", "tokens": [50364, 406, 257, 777, 3410, 11, 309, 311, 445, 364, 1558, 11, 534, 13, 50566], "temperature": 0.0, "avg_logprob": -0.2190645794535792, "compression_ratio": 1.5625, "no_speech_prob": 0.000194113003090024}, {"id": 1176, "seek": 559554, "start": 5599.58, "end": 5606.38, "text": " Which is the idea of passing in a function which some other function will call at particular", "tokens": [50566, 3013, 307, 264, 1558, 295, 8437, 294, 257, 2445, 597, 512, 661, 2445, 486, 818, 412, 1729, 50906], "temperature": 0.0, "avg_logprob": -0.2190645794535792, "compression_ratio": 1.5625, "no_speech_prob": 0.000194113003090024}, {"id": 1177, "seek": 559554, "start": 5606.38, "end": 5607.38, "text": " times.", "tokens": [50906, 1413, 13, 50956], "temperature": 0.0, "avg_logprob": -0.2190645794535792, "compression_ratio": 1.5625, "no_speech_prob": 0.000194113003090024}, {"id": 1178, "seek": 559554, "start": 5607.38, "end": 5612.6, "text": " Such as at the end of a step, or such as when you click a button.", "tokens": [50956, 9653, 382, 412, 264, 917, 295, 257, 1823, 11, 420, 1270, 382, 562, 291, 2052, 257, 2960, 13, 51217], "temperature": 0.0, "avg_logprob": -0.2190645794535792, "compression_ratio": 1.5625, "no_speech_prob": 0.000194113003090024}, {"id": 1179, "seek": 559554, "start": 5612.6, "end": 5617.38, "text": " So that's what we mean by callbacks.", "tokens": [51217, 407, 300, 311, 437, 321, 914, 538, 818, 17758, 13, 51456], "temperature": 0.0, "avg_logprob": -0.2190645794535792, "compression_ratio": 1.5625, "no_speech_prob": 0.000194113003090024}, {"id": 1180, "seek": 559554, "start": 5617.38, "end": 5620.84, "text": " We don't have to define the function ahead of time.", "tokens": [51456, 492, 500, 380, 362, 281, 6964, 264, 2445, 2286, 295, 565, 13, 51629], "temperature": 0.0, "avg_logprob": -0.2190645794535792, "compression_ratio": 1.5625, "no_speech_prob": 0.000194113003090024}, {"id": 1181, "seek": 562084, "start": 5620.84, "end": 5628.400000000001, "text": " We could define the function at the same time that we call the slow calculation, by", "tokens": [50364, 492, 727, 6964, 264, 2445, 412, 264, 912, 565, 300, 321, 818, 264, 2964, 17108, 11, 538, 50742], "temperature": 0.0, "avg_logprob": -0.24781865752145146, "compression_ratio": 1.7435897435897436, "no_speech_prob": 0.03161691874265671}, {"id": 1182, "seek": 562084, "start": 5628.400000000001, "end": 5629.52, "text": " using lambda.", "tokens": [50742, 1228, 13607, 13, 50798], "temperature": 0.0, "avg_logprob": -0.24781865752145146, "compression_ratio": 1.7435897435897436, "no_speech_prob": 0.03161691874265671}, {"id": 1183, "seek": 562084, "start": 5629.52, "end": 5633.2, "text": " So as we've discussed before, lambda just defines a function but it doesn't give it", "tokens": [50798, 407, 382, 321, 600, 7152, 949, 11, 13607, 445, 23122, 257, 2445, 457, 309, 1177, 380, 976, 309, 50982], "temperature": 0.0, "avg_logprob": -0.24781865752145146, "compression_ratio": 1.7435897435897436, "no_speech_prob": 0.03161691874265671}, {"id": 1184, "seek": 562084, "start": 5633.2, "end": 5634.2, "text": " a name.", "tokens": [50982, 257, 1315, 13, 51032], "temperature": 0.0, "avg_logprob": -0.24781865752145146, "compression_ratio": 1.7435897435897436, "no_speech_prob": 0.03161691874265671}, {"id": 1185, "seek": 562084, "start": 5634.2, "end": 5638.56, "text": " So here's a function that takes one parameter and prints out exactly the same thing as before.", "tokens": [51032, 407, 510, 311, 257, 2445, 300, 2516, 472, 13075, 293, 22305, 484, 2293, 264, 912, 551, 382, 949, 13, 51250], "temperature": 0.0, "avg_logprob": -0.24781865752145146, "compression_ratio": 1.7435897435897436, "no_speech_prob": 0.03161691874265671}, {"id": 1186, "seek": 562084, "start": 5638.56, "end": 5645.96, "text": " So here's the same way as doing it, but using a lambda.", "tokens": [51250, 407, 510, 311, 264, 912, 636, 382, 884, 309, 11, 457, 1228, 257, 13607, 13, 51620], "temperature": 0.0, "avg_logprob": -0.24781865752145146, "compression_ratio": 1.7435897435897436, "no_speech_prob": 0.03161691874265671}, {"id": 1187, "seek": 564596, "start": 5645.96, "end": 5650.52, "text": " We could make it more sophisticated now, and rather than always saying, awesome we", "tokens": [50364, 492, 727, 652, 309, 544, 16950, 586, 11, 293, 2831, 813, 1009, 1566, 11, 3476, 321, 50592], "temperature": 0.0, "avg_logprob": -0.27673350098312544, "compression_ratio": 1.7024390243902439, "no_speech_prob": 0.012240694835782051}, {"id": 1188, "seek": 564596, "start": 5650.52, "end": 5657.8, "text": " finished epoch, whatever, we could let you pass in an exclamation, and we print that", "tokens": [50592, 4335, 30992, 339, 11, 2035, 11, 321, 727, 718, 291, 1320, 294, 364, 1624, 43233, 11, 293, 321, 4482, 300, 50956], "temperature": 0.0, "avg_logprob": -0.27673350098312544, "compression_ratio": 1.7024390243902439, "no_speech_prob": 0.012240694835782051}, {"id": 1189, "seek": 564596, "start": 5657.8, "end": 5658.8, "text": " out.", "tokens": [50956, 484, 13, 51006], "temperature": 0.0, "avg_logprob": -0.27673350098312544, "compression_ratio": 1.7024390243902439, "no_speech_prob": 0.012240694835782051}, {"id": 1190, "seek": 564596, "start": 5658.8, "end": 5667.14, "text": " And so in this case we could now have our lambda call that function.", "tokens": [51006, 400, 370, 294, 341, 1389, 321, 727, 586, 362, 527, 13607, 818, 300, 2445, 13, 51423], "temperature": 0.0, "avg_logprob": -0.27673350098312544, "compression_ratio": 1.7024390243902439, "no_speech_prob": 0.012240694835782051}, {"id": 1191, "seek": 564596, "start": 5667.14, "end": 5672.68, "text": " And so one of the things that we can do now is to, again, we can create a function that", "tokens": [51423, 400, 370, 472, 295, 264, 721, 300, 321, 393, 360, 586, 307, 281, 11, 797, 11, 321, 393, 1884, 257, 2445, 300, 51700], "temperature": 0.0, "avg_logprob": -0.27673350098312544, "compression_ratio": 1.7024390243902439, "no_speech_prob": 0.012240694835782051}, {"id": 1192, "seek": 564596, "start": 5672.68, "end": 5675.12, "text": " returns a function.", "tokens": [51700, 11247, 257, 2445, 13, 51822], "temperature": 0.0, "avg_logprob": -0.27673350098312544, "compression_ratio": 1.7024390243902439, "no_speech_prob": 0.012240694835782051}, {"id": 1193, "seek": 567512, "start": 5675.28, "end": 5681.92, "text": " And so we could create a make show progress function, where you pass in the exclamation.", "tokens": [50372, 400, 370, 321, 727, 1884, 257, 652, 855, 4205, 2445, 11, 689, 291, 1320, 294, 264, 1624, 43233, 13, 50704], "temperature": 0.0, "avg_logprob": -0.24906947161700274, "compression_ratio": 1.5804597701149425, "no_speech_prob": 0.00011412232561269775}, {"id": 1194, "seek": 567512, "start": 5681.92, "end": 5687.16, "text": " We could then create, and there's no need to give it a name actually, let's just return", "tokens": [50704, 492, 727, 550, 1884, 11, 293, 456, 311, 572, 643, 281, 976, 309, 257, 1315, 767, 11, 718, 311, 445, 2736, 50966], "temperature": 0.0, "avg_logprob": -0.24906947161700274, "compression_ratio": 1.5804597701149425, "no_speech_prob": 0.00011412232561269775}, {"id": 1195, "seek": 567512, "start": 5687.16, "end": 5691.58, "text": " it directly.", "tokens": [50966, 309, 3838, 13, 51187], "temperature": 0.0, "avg_logprob": -0.24906947161700274, "compression_ratio": 1.5804597701149425, "no_speech_prob": 0.00011412232561269775}, {"id": 1196, "seek": 567512, "start": 5691.58, "end": 5696.9, "text": " We can return a function that calls that exclamation.", "tokens": [51187, 492, 393, 2736, 257, 2445, 300, 5498, 300, 1624, 43233, 13, 51453], "temperature": 0.0, "avg_logprob": -0.24906947161700274, "compression_ratio": 1.5804597701149425, "no_speech_prob": 0.00011412232561269775}, {"id": 1197, "seek": 567512, "start": 5696.9, "end": 5701.74, "text": " So here we are passing in nice.", "tokens": [51453, 407, 510, 321, 366, 8437, 294, 1481, 13, 51695], "temperature": 0.0, "avg_logprob": -0.24906947161700274, "compression_ratio": 1.5804597701149425, "no_speech_prob": 0.00011412232561269775}, {"id": 1198, "seek": 570174, "start": 5701.74, "end": 5707.82, "text": " And that's exactly the same as doing something like what we've done before.", "tokens": [50364, 400, 300, 311, 2293, 264, 912, 382, 884, 746, 411, 437, 321, 600, 1096, 949, 13, 50668], "temperature": 0.0, "avg_logprob": -0.3271647827534736, "compression_ratio": 1.6358695652173914, "no_speech_prob": 0.06465139240026474}, {"id": 1199, "seek": 570174, "start": 5707.82, "end": 5717.58, "text": " We could say instead of using a lambda, we can create an inner function like this.", "tokens": [50668, 492, 727, 584, 2602, 295, 1228, 257, 13607, 11, 321, 393, 1884, 364, 7284, 2445, 411, 341, 13, 51156], "temperature": 0.0, "avg_logprob": -0.3271647827534736, "compression_ratio": 1.6358695652173914, "no_speech_prob": 0.06465139240026474}, {"id": 1200, "seek": 570174, "start": 5717.58, "end": 5719.98, "text": " So here is now a function that returns a function.", "tokens": [51156, 407, 510, 307, 586, 257, 2445, 300, 11247, 257, 2445, 13, 51276], "temperature": 0.0, "avg_logprob": -0.3271647827534736, "compression_ratio": 1.6358695652173914, "no_speech_prob": 0.06465139240026474}, {"id": 1201, "seek": 570174, "start": 5719.98, "end": 5722.62, "text": " This does exactly the same thing.", "tokens": [51276, 639, 775, 2293, 264, 912, 551, 13, 51408], "temperature": 0.0, "avg_logprob": -0.3271647827534736, "compression_ratio": 1.6358695652173914, "no_speech_prob": 0.06465139240026474}, {"id": 1202, "seek": 570174, "start": 5722.62, "end": 5728.62, "text": " Okay, so one way with a lambda, one way without a lambda.", "tokens": [51408, 1033, 11, 370, 472, 636, 365, 257, 13607, 11, 472, 636, 1553, 257, 13607, 13, 51708], "temperature": 0.0, "avg_logprob": -0.3271647827534736, "compression_ratio": 1.6358695652173914, "no_speech_prob": 0.06465139240026474}, {"id": 1203, "seek": 572862, "start": 5728.62, "end": 5738.26, "text": " One of the reasons I wanted to show you that is so I can, I've got so many here, is that", "tokens": [50364, 1485, 295, 264, 4112, 286, 1415, 281, 855, 291, 300, 307, 370, 286, 393, 11, 286, 600, 658, 370, 867, 510, 11, 307, 300, 50846], "temperature": 0.0, "avg_logprob": -0.25528115864041484, "compression_ratio": 1.672514619883041, "no_speech_prob": 0.042720723897218704}, {"id": 1204, "seek": 572862, "start": 5738.26, "end": 5742.74, "text": " we can do exactly the same thing using partial.", "tokens": [50846, 321, 393, 360, 2293, 264, 912, 551, 1228, 14641, 13, 51070], "temperature": 0.0, "avg_logprob": -0.25528115864041484, "compression_ratio": 1.672514619883041, "no_speech_prob": 0.042720723897218704}, {"id": 1205, "seek": 572862, "start": 5742.74, "end": 5750.7, "text": " So with partial, it's going to do exactly the same thing as this kind of make show progress.", "tokens": [51070, 407, 365, 14641, 11, 309, 311, 516, 281, 360, 2293, 264, 912, 551, 382, 341, 733, 295, 652, 855, 4205, 13, 51468], "temperature": 0.0, "avg_logprob": -0.25528115864041484, "compression_ratio": 1.672514619883041, "no_speech_prob": 0.042720723897218704}, {"id": 1206, "seek": 572862, "start": 5750.7, "end": 5755.0, "text": " It's going to call show progress and pass, okay I guess.", "tokens": [51468, 467, 311, 516, 281, 818, 855, 4205, 293, 1320, 11, 1392, 286, 2041, 13, 51683], "temperature": 0.0, "avg_logprob": -0.25528115864041484, "compression_ratio": 1.672514619883041, "no_speech_prob": 0.042720723897218704}, {"id": 1207, "seek": 575500, "start": 5755.0, "end": 5759.08, "text": " So this is again an example of a function returning a function.", "tokens": [50364, 407, 341, 307, 797, 364, 1365, 295, 257, 2445, 12678, 257, 2445, 13, 50568], "temperature": 0.0, "avg_logprob": -0.28938497255926265, "compression_ratio": 1.596774193548387, "no_speech_prob": 0.001345852972008288}, {"id": 1208, "seek": 575500, "start": 5759.08, "end": 5765.08, "text": " And so this is a function that calls show progress passing in this as the first parameter.", "tokens": [50568, 400, 370, 341, 307, 257, 2445, 300, 5498, 855, 4205, 8437, 294, 341, 382, 264, 700, 13075, 13, 50868], "temperature": 0.0, "avg_logprob": -0.28938497255926265, "compression_ratio": 1.596774193548387, "no_speech_prob": 0.001345852972008288}, {"id": 1209, "seek": 575500, "start": 5765.08, "end": 5772.12, "text": " And again, it does exactly the same thing.", "tokens": [50868, 400, 797, 11, 309, 775, 2293, 264, 912, 551, 13, 51220], "temperature": 0.0, "avg_logprob": -0.28938497255926265, "compression_ratio": 1.596774193548387, "no_speech_prob": 0.001345852972008288}, {"id": 1210, "seek": 575500, "start": 5772.12, "end": 5776.4, "text": " Okay so we tend to use partial a lot.", "tokens": [51220, 1033, 370, 321, 3928, 281, 764, 14641, 257, 688, 13, 51434], "temperature": 0.0, "avg_logprob": -0.28938497255926265, "compression_ratio": 1.596774193548387, "no_speech_prob": 0.001345852972008288}, {"id": 1211, "seek": 575500, "start": 5776.4, "end": 5783.2, "text": " So that's certainly something worth spending time practicing.", "tokens": [51434, 407, 300, 311, 3297, 746, 3163, 6434, 565, 11350, 13, 51774], "temperature": 0.0, "avg_logprob": -0.28938497255926265, "compression_ratio": 1.596774193548387, "no_speech_prob": 0.001345852972008288}, {"id": 1212, "seek": 578320, "start": 5783.2, "end": 5789.72, "text": " Now as we've discussed, Python doesn't care about types in particular.", "tokens": [50364, 823, 382, 321, 600, 7152, 11, 15329, 1177, 380, 1127, 466, 3467, 294, 1729, 13, 50690], "temperature": 0.0, "avg_logprob": -0.26594371330447314, "compression_ratio": 1.6117021276595744, "no_speech_prob": 0.000636189361102879}, {"id": 1213, "seek": 578320, "start": 5789.72, "end": 5796.8, "text": " And there's nothing about any of this that requires CB to be a function.", "tokens": [50690, 400, 456, 311, 1825, 466, 604, 295, 341, 300, 7029, 18745, 281, 312, 257, 2445, 13, 51044], "temperature": 0.0, "avg_logprob": -0.26594371330447314, "compression_ratio": 1.6117021276595744, "no_speech_prob": 0.000636189361102879}, {"id": 1214, "seek": 578320, "start": 5796.8, "end": 5798.88, "text": " It just has to be a callable.", "tokens": [51044, 467, 445, 575, 281, 312, 257, 818, 712, 13, 51148], "temperature": 0.0, "avg_logprob": -0.26594371330447314, "compression_ratio": 1.6117021276595744, "no_speech_prob": 0.000636189361102879}, {"id": 1215, "seek": 578320, "start": 5798.88, "end": 5804.16, "text": " A callable is something that you can call.", "tokens": [51148, 316, 818, 712, 307, 746, 300, 291, 393, 818, 13, 51412], "temperature": 0.0, "avg_logprob": -0.26594371330447314, "compression_ratio": 1.6117021276595744, "no_speech_prob": 0.000636189361102879}, {"id": 1216, "seek": 578320, "start": 5804.16, "end": 5809.24, "text": " And so as we've discussed, another way of creating a callable is defining dunder call.", "tokens": [51412, 400, 370, 382, 321, 600, 7152, 11, 1071, 636, 295, 4084, 257, 818, 712, 307, 17827, 274, 6617, 818, 13, 51666], "temperature": 0.0, "avg_logprob": -0.26594371330447314, "compression_ratio": 1.6117021276595744, "no_speech_prob": 0.000636189361102879}, {"id": 1217, "seek": 580924, "start": 5809.28, "end": 5811.84, "text": " So here's a class.", "tokens": [50366, 407, 510, 311, 257, 1508, 13, 50494], "temperature": 0.0, "avg_logprob": -0.2618381918930426, "compression_ratio": 1.6271186440677967, "no_speech_prob": 0.0006563700735569}, {"id": 1218, "seek": 580924, "start": 5811.84, "end": 5815.639999999999, "text": " And this is going to work exactly the same as our make show progress thing, but now as", "tokens": [50494, 400, 341, 307, 516, 281, 589, 2293, 264, 912, 382, 527, 652, 855, 4205, 551, 11, 457, 586, 382, 50684], "temperature": 0.0, "avg_logprob": -0.2618381918930426, "compression_ratio": 1.6271186440677967, "no_speech_prob": 0.0006563700735569}, {"id": 1219, "seek": 580924, "start": 5815.639999999999, "end": 5816.639999999999, "text": " a class.", "tokens": [50684, 257, 1508, 13, 50734], "temperature": 0.0, "avg_logprob": -0.2618381918930426, "compression_ratio": 1.6271186440677967, "no_speech_prob": 0.0006563700735569}, {"id": 1220, "seek": 580924, "start": 5816.639999999999, "end": 5823.3, "text": " So there's a dunder init, which stores the exclamation, and a dunder call that prints.", "tokens": [50734, 407, 456, 311, 257, 274, 6617, 3157, 11, 597, 9512, 264, 1624, 43233, 11, 293, 257, 274, 6617, 818, 300, 22305, 13, 51067], "temperature": 0.0, "avg_logprob": -0.2618381918930426, "compression_ratio": 1.6271186440677967, "no_speech_prob": 0.0006563700735569}, {"id": 1221, "seek": 580924, "start": 5823.3, "end": 5835.28, "text": " And so now we're creating a object which is callable, and does exactly the same thing.", "tokens": [51067, 400, 370, 586, 321, 434, 4084, 257, 2657, 597, 307, 818, 712, 11, 293, 775, 2293, 264, 912, 551, 13, 51666], "temperature": 0.0, "avg_logprob": -0.2618381918930426, "compression_ratio": 1.6271186440677967, "no_speech_prob": 0.0006563700735569}, {"id": 1222, "seek": 583528, "start": 5835.32, "end": 5841.16, "text": " Okay so these are all like fundamental ideas that I want you to get really comfortable", "tokens": [50366, 1033, 370, 613, 366, 439, 411, 8088, 3487, 300, 286, 528, 291, 281, 483, 534, 4619, 50658], "temperature": 0.0, "avg_logprob": -0.24005288663117783, "compression_ratio": 1.5953488372093023, "no_speech_prob": 0.005060224328190088}, {"id": 1223, "seek": 583528, "start": 5841.16, "end": 5842.16, "text": " with.", "tokens": [50658, 365, 13, 50708], "temperature": 0.0, "avg_logprob": -0.24005288663117783, "compression_ratio": 1.5953488372093023, "no_speech_prob": 0.005060224328190088}, {"id": 1224, "seek": 583528, "start": 5842.16, "end": 5849.12, "text": " The idea of dunder call, dunder things in general, partials, classes, because they come", "tokens": [50708, 440, 1558, 295, 274, 6617, 818, 11, 274, 6617, 721, 294, 2674, 11, 644, 12356, 11, 5359, 11, 570, 436, 808, 51056], "temperature": 0.0, "avg_logprob": -0.24005288663117783, "compression_ratio": 1.5953488372093023, "no_speech_prob": 0.005060224328190088}, {"id": 1225, "seek": 583528, "start": 5849.12, "end": 5858.84, "text": " up all the time in PyTorch code, and in the code we'll be writing, and in fact pretty", "tokens": [51056, 493, 439, 264, 565, 294, 9953, 51, 284, 339, 3089, 11, 293, 294, 264, 3089, 321, 603, 312, 3579, 11, 293, 294, 1186, 1238, 51542], "temperature": 0.0, "avg_logprob": -0.24005288663117783, "compression_ratio": 1.5953488372093023, "no_speech_prob": 0.005060224328190088}, {"id": 1226, "seek": 583528, "start": 5858.84, "end": 5861.5199999999995, "text": " much all frameworks.", "tokens": [51542, 709, 439, 29834, 13, 51676], "temperature": 0.0, "avg_logprob": -0.24005288663117783, "compression_ratio": 1.5953488372093023, "no_speech_prob": 0.005060224328190088}, {"id": 1227, "seek": 583528, "start": 5861.5199999999995, "end": 5864.32, "text": " So it's really important to feel comfortable with them.", "tokens": [51676, 407, 309, 311, 534, 1021, 281, 841, 4619, 365, 552, 13, 51816], "temperature": 0.0, "avg_logprob": -0.24005288663117783, "compression_ratio": 1.5953488372093023, "no_speech_prob": 0.005060224328190088}, {"id": 1228, "seek": 586432, "start": 5864.36, "end": 5868.2, "text": " And remember you don't have to rely on the resources we're providing.", "tokens": [50366, 400, 1604, 291, 500, 380, 362, 281, 10687, 322, 264, 3593, 321, 434, 6530, 13, 50558], "temperature": 0.0, "avg_logprob": -0.2702470796298137, "compression_ratio": 1.650190114068441, "no_speech_prob": 0.0015978406881913543}, {"id": 1229, "seek": 586432, "start": 5868.2, "end": 5872.24, "text": " You know, if there are certain things here that are very new to you, you know, Google", "tokens": [50558, 509, 458, 11, 498, 456, 366, 1629, 721, 510, 300, 366, 588, 777, 281, 291, 11, 291, 458, 11, 3329, 50760], "temperature": 0.0, "avg_logprob": -0.2702470796298137, "compression_ratio": 1.650190114068441, "no_speech_prob": 0.0015978406881913543}, {"id": 1230, "seek": 586432, "start": 5872.24, "end": 5878.5599999999995, "text": " around for some tutorials, or ask for help in the forums, finding things, and so forth.", "tokens": [50760, 926, 337, 512, 17616, 11, 420, 1029, 337, 854, 294, 264, 26998, 11, 5006, 721, 11, 293, 370, 5220, 13, 51076], "temperature": 0.0, "avg_logprob": -0.2702470796298137, "compression_ratio": 1.650190114068441, "no_speech_prob": 0.0015978406881913543}, {"id": 1231, "seek": 586432, "start": 5878.5599999999995, "end": 5882.759999999999, "text": " And then I'm just going to briefly recover something I've mentioned before, which is", "tokens": [51076, 400, 550, 286, 478, 445, 516, 281, 10515, 8114, 746, 286, 600, 2835, 949, 11, 597, 307, 51286], "temperature": 0.0, "avg_logprob": -0.2702470796298137, "compression_ratio": 1.650190114068441, "no_speech_prob": 0.0015978406881913543}, {"id": 1232, "seek": 586432, "start": 5882.759999999999, "end": 5885.92, "text": " star args and star star quags, because again they come up a lot.", "tokens": [51286, 3543, 3882, 82, 293, 3543, 3543, 421, 12109, 11, 570, 797, 436, 808, 493, 257, 688, 13, 51444], "temperature": 0.0, "avg_logprob": -0.2702470796298137, "compression_ratio": 1.650190114068441, "no_speech_prob": 0.0015978406881913543}, {"id": 1233, "seek": 586432, "start": 5885.92, "end": 5889.5199999999995, "text": " I just wanted to show you how they work.", "tokens": [51444, 286, 445, 1415, 281, 855, 291, 577, 436, 589, 13, 51624], "temperature": 0.0, "avg_logprob": -0.2702470796298137, "compression_ratio": 1.650190114068441, "no_speech_prob": 0.0015978406881913543}, {"id": 1234, "seek": 588952, "start": 5889.52, "end": 5896.120000000001, "text": " So if we create a function that has star args and star star quags, nothing else, and", "tokens": [50364, 407, 498, 321, 1884, 257, 2445, 300, 575, 3543, 3882, 82, 293, 3543, 3543, 421, 12109, 11, 1825, 1646, 11, 293, 50694], "temperature": 0.0, "avg_logprob": -0.22418275869117593, "compression_ratio": 1.7729468599033817, "no_speech_prob": 0.0010322215966880322}, {"id": 1235, "seek": 588952, "start": 5896.120000000001, "end": 5900.240000000001, "text": " I'm just going to have this function just print them.", "tokens": [50694, 286, 478, 445, 516, 281, 362, 341, 2445, 445, 4482, 552, 13, 50900], "temperature": 0.0, "avg_logprob": -0.22418275869117593, "compression_ratio": 1.7729468599033817, "no_speech_prob": 0.0010322215966880322}, {"id": 1236, "seek": 588952, "start": 5900.240000000001, "end": 5905.6, "text": " Now I'm going to call the function, I'm going to pass 3, I'm going to pass a, and I'm going", "tokens": [50900, 823, 286, 478, 516, 281, 818, 264, 2445, 11, 286, 478, 516, 281, 1320, 805, 11, 286, 478, 516, 281, 1320, 257, 11, 293, 286, 478, 516, 51168], "temperature": 0.0, "avg_logprob": -0.22418275869117593, "compression_ratio": 1.7729468599033817, "no_speech_prob": 0.0010322215966880322}, {"id": 1237, "seek": 588952, "start": 5905.6, "end": 5908.5, "text": " to pass thing1 equals hello.", "tokens": [51168, 281, 1320, 551, 16, 6915, 7751, 13, 51313], "temperature": 0.0, "avg_logprob": -0.22418275869117593, "compression_ratio": 1.7729468599033817, "no_speech_prob": 0.0010322215966880322}, {"id": 1238, "seek": 588952, "start": 5908.5, "end": 5911.42, "text": " Now these are passed what we would say by position.", "tokens": [51313, 823, 613, 366, 4678, 437, 321, 576, 584, 538, 2535, 13, 51459], "temperature": 0.0, "avg_logprob": -0.22418275869117593, "compression_ratio": 1.7729468599033817, "no_speech_prob": 0.0010322215966880322}, {"id": 1239, "seek": 588952, "start": 5911.42, "end": 5915.360000000001, "text": " We haven't got a blah equals, they're just stuck there.", "tokens": [51459, 492, 2378, 380, 658, 257, 12288, 6915, 11, 436, 434, 445, 5541, 456, 13, 51656], "temperature": 0.0, "avg_logprob": -0.22418275869117593, "compression_ratio": 1.7729468599033817, "no_speech_prob": 0.0010322215966880322}, {"id": 1240, "seek": 591536, "start": 5915.36, "end": 5920.36, "text": " Things that are passed by position are placed in star args, if you have one.", "tokens": [50364, 9514, 300, 366, 4678, 538, 2535, 366, 7074, 294, 3543, 3882, 82, 11, 498, 291, 362, 472, 13, 50614], "temperature": 0.0, "avg_logprob": -0.23074785251061894, "compression_ratio": 1.8300970873786409, "no_speech_prob": 0.017711831256747246}, {"id": 1241, "seek": 591536, "start": 5920.36, "end": 5922.96, "text": " It doesn't have to be called args, you can call this anything you like, but in the star", "tokens": [50614, 467, 1177, 380, 362, 281, 312, 1219, 3882, 82, 11, 291, 393, 818, 341, 1340, 291, 411, 11, 457, 294, 264, 3543, 50744], "temperature": 0.0, "avg_logprob": -0.23074785251061894, "compression_ratio": 1.8300970873786409, "no_speech_prob": 0.017711831256747246}, {"id": 1242, "seek": 591536, "start": 5922.96, "end": 5924.2, "text": " bit.", "tokens": [50744, 857, 13, 50806], "temperature": 0.0, "avg_logprob": -0.23074785251061894, "compression_ratio": 1.8300970873786409, "no_speech_prob": 0.017711831256747246}, {"id": 1243, "seek": 591536, "start": 5924.2, "end": 5933.32, "text": " And so you can see here that args is a tuple containing the positionally passed arguments.", "tokens": [50806, 400, 370, 291, 393, 536, 510, 300, 3882, 82, 307, 257, 2604, 781, 19273, 264, 2535, 379, 4678, 12869, 13, 51262], "temperature": 0.0, "avg_logprob": -0.23074785251061894, "compression_ratio": 1.8300970873786409, "no_speech_prob": 0.017711831256747246}, {"id": 1244, "seek": 591536, "start": 5933.32, "end": 5940.0, "text": " And then quags is a dictionary containing the named arguments.", "tokens": [51262, 400, 550, 421, 12109, 307, 257, 25890, 19273, 264, 4926, 12869, 13, 51596], "temperature": 0.0, "avg_logprob": -0.23074785251061894, "compression_ratio": 1.8300970873786409, "no_speech_prob": 0.017711831256747246}, {"id": 1245, "seek": 591536, "start": 5940.0, "end": 5943.599999999999, "text": " So that is all that star args and star star quags do.", "tokens": [51596, 407, 300, 307, 439, 300, 3543, 3882, 82, 293, 3543, 3543, 421, 12109, 360, 13, 51776], "temperature": 0.0, "avg_logprob": -0.23074785251061894, "compression_ratio": 1.8300970873786409, "no_speech_prob": 0.017711831256747246}, {"id": 1246, "seek": 594360, "start": 5943.84, "end": 5945.76, "text": " And as I say, there's nothing special about these names.", "tokens": [50376, 400, 382, 286, 584, 11, 456, 311, 1825, 2121, 466, 613, 5288, 13, 50472], "temperature": 0.0, "avg_logprob": -0.3085704971762264, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.010169520042836666}, {"id": 1247, "seek": 594360, "start": 5945.76, "end": 5957.92, "text": " I'll call this a, I'll call this b, and it'll do exactly the same thing.", "tokens": [50472, 286, 603, 818, 341, 257, 11, 286, 603, 818, 341, 272, 11, 293, 309, 603, 360, 2293, 264, 912, 551, 13, 51080], "temperature": 0.0, "avg_logprob": -0.3085704971762264, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.010169520042836666}, {"id": 1248, "seek": 594360, "start": 5957.92, "end": 5966.72, "text": " So this comes up a lot, and so it's important to remember that this is literally all that", "tokens": [51080, 407, 341, 1487, 493, 257, 688, 11, 293, 370, 309, 311, 1021, 281, 1604, 300, 341, 307, 3736, 439, 300, 51520], "temperature": 0.0, "avg_logprob": -0.3085704971762264, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.010169520042836666}, {"id": 1249, "seek": 594360, "start": 5966.72, "end": 5968.280000000001, "text": " they're doing.", "tokens": [51520, 436, 434, 884, 13, 51598], "temperature": 0.0, "avg_logprob": -0.3085704971762264, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.010169520042836666}, {"id": 1250, "seek": 596828, "start": 5968.28, "end": 5983.4, "text": " And then on the other hand, let's say we had a function which takes a couple of, okay let's", "tokens": [50364, 400, 550, 322, 264, 661, 1011, 11, 718, 311, 584, 321, 632, 257, 2445, 597, 2516, 257, 1916, 295, 11, 1392, 718, 311, 51120], "temperature": 0.0, "avg_logprob": -0.37352628124003506, "compression_ratio": 1.322314049586777, "no_speech_prob": 0.01717645302414894}, {"id": 1251, "seek": 596828, "start": 5983.4, "end": 5994.639999999999, "text": " try that, print a, actually we'll just print them directly, a, b, c.", "tokens": [51120, 853, 300, 11, 4482, 257, 11, 767, 321, 603, 445, 4482, 552, 3838, 11, 257, 11, 272, 11, 269, 13, 51682], "temperature": 0.0, "avg_logprob": -0.37352628124003506, "compression_ratio": 1.322314049586777, "no_speech_prob": 0.01717645302414894}, {"id": 1252, "seek": 599464, "start": 5994.64, "end": 6000.72, "text": " We can also, rather than just using them as parameters, we can also use them when calling", "tokens": [50364, 492, 393, 611, 11, 2831, 813, 445, 1228, 552, 382, 9834, 11, 321, 393, 611, 764, 552, 562, 5141, 50668], "temperature": 0.0, "avg_logprob": -0.25531821250915526, "compression_ratio": 1.7209302325581395, "no_speech_prob": 0.001957010244950652}, {"id": 1253, "seek": 599464, "start": 6000.72, "end": 6001.72, "text": " something.", "tokens": [50668, 746, 13, 50718], "temperature": 0.0, "avg_logprob": -0.25531821250915526, "compression_ratio": 1.7209302325581395, "no_speech_prob": 0.001957010244950652}, {"id": 1254, "seek": 599464, "start": 6001.72, "end": 6006.64, "text": " So let's say I create something called args, again it doesn't have to be called args, called,", "tokens": [50718, 407, 718, 311, 584, 286, 1884, 746, 1219, 3882, 82, 11, 797, 309, 1177, 380, 362, 281, 312, 1219, 3882, 82, 11, 1219, 11, 50964], "temperature": 0.0, "avg_logprob": -0.25531821250915526, "compression_ratio": 1.7209302325581395, "no_speech_prob": 0.001957010244950652}, {"id": 1255, "seek": 599464, "start": 6006.64, "end": 6009.92, "text": " which contains 1, 2.", "tokens": [50964, 597, 8306, 502, 11, 568, 13, 51128], "temperature": 0.0, "avg_logprob": -0.25531821250915526, "compression_ratio": 1.7209302325581395, "no_speech_prob": 0.001957010244950652}, {"id": 1256, "seek": 599464, "start": 6009.92, "end": 6019.280000000001, "text": " And I create something called quags, that contains a dictionary containing c, 3.", "tokens": [51128, 400, 286, 1884, 746, 1219, 421, 12109, 11, 300, 8306, 257, 25890, 19273, 269, 11, 805, 13, 51596], "temperature": 0.0, "avg_logprob": -0.25531821250915526, "compression_ratio": 1.7209302325581395, "no_speech_prob": 0.001957010244950652}, {"id": 1257, "seek": 601928, "start": 6019.28, "end": 6028.24, "text": " I can then call g, and I can pass in star args, comma, star star quags.", "tokens": [50364, 286, 393, 550, 818, 290, 11, 293, 286, 393, 1320, 294, 3543, 3882, 82, 11, 22117, 11, 3543, 3543, 421, 12109, 13, 50812], "temperature": 0.0, "avg_logprob": -0.24259023313169126, "compression_ratio": 1.6369426751592357, "no_speech_prob": 0.0004878557228948921}, {"id": 1258, "seek": 601928, "start": 6028.24, "end": 6035.24, "text": " And that's going to take this 1, 2, and pass them as individual arguments, positionally.", "tokens": [50812, 400, 300, 311, 516, 281, 747, 341, 502, 11, 568, 11, 293, 1320, 552, 382, 2609, 12869, 11, 2535, 379, 13, 51162], "temperature": 0.0, "avg_logprob": -0.24259023313169126, "compression_ratio": 1.6369426751592357, "no_speech_prob": 0.0004878557228948921}, {"id": 1259, "seek": 601928, "start": 6035.24, "end": 6042.139999999999, "text": " And it's going to take the c, 3, and pass that as a named argument, c equals 3.", "tokens": [51162, 400, 309, 311, 516, 281, 747, 264, 269, 11, 805, 11, 293, 1320, 300, 382, 257, 4926, 6770, 11, 269, 6915, 805, 13, 51507], "temperature": 0.0, "avg_logprob": -0.24259023313169126, "compression_ratio": 1.6369426751592357, "no_speech_prob": 0.0004878557228948921}, {"id": 1260, "seek": 601928, "start": 6042.139999999999, "end": 6043.139999999999, "text": " And there it is.", "tokens": [51507, 400, 456, 309, 307, 13, 51557], "temperature": 0.0, "avg_logprob": -0.24259023313169126, "compression_ratio": 1.6369426751592357, "no_speech_prob": 0.0004878557228948921}, {"id": 1261, "seek": 604314, "start": 6043.14, "end": 6052.14, "text": " Okay, so they're kind of two linked but different ways that use star and star star.", "tokens": [50364, 1033, 11, 370, 436, 434, 733, 295, 732, 9408, 457, 819, 2098, 300, 764, 3543, 293, 3543, 3543, 13, 50814], "temperature": 0.0, "avg_logprob": -0.2865341410917394, "compression_ratio": 1.5119047619047619, "no_speech_prob": 6.240919447009219e-06}, {"id": 1262, "seek": 604314, "start": 6052.14, "end": 6060.660000000001, "text": " Okay, now here's a slightly different way of doing callbacks, which I really like.", "tokens": [50814, 1033, 11, 586, 510, 311, 257, 4748, 819, 636, 295, 884, 818, 17758, 11, 597, 286, 534, 411, 13, 51240], "temperature": 0.0, "avg_logprob": -0.2865341410917394, "compression_ratio": 1.5119047619047619, "no_speech_prob": 6.240919447009219e-06}, {"id": 1263, "seek": 604314, "start": 6060.660000000001, "end": 6067.46, "text": " In this case I've now passing in a callback that's not callable, but instead it's going", "tokens": [51240, 682, 341, 1389, 286, 600, 586, 8437, 294, 257, 818, 3207, 300, 311, 406, 818, 712, 11, 457, 2602, 309, 311, 516, 51580], "temperature": 0.0, "avg_logprob": -0.2865341410917394, "compression_ratio": 1.5119047619047619, "no_speech_prob": 6.240919447009219e-06}, {"id": 1264, "seek": 606746, "start": 6067.46, "end": 6075.38, "text": " to have a method called beforeCalc, and another method called afterCalc.", "tokens": [50364, 281, 362, 257, 3170, 1219, 949, 31279, 66, 11, 293, 1071, 3170, 1219, 934, 31279, 66, 13, 50760], "temperature": 0.0, "avg_logprob": -0.2811557307387843, "compression_ratio": 1.5633802816901408, "no_speech_prob": 0.011330886743962765}, {"id": 1265, "seek": 606746, "start": 6075.38, "end": 6087.58, "text": " And so now my callback is going to be a class containing a beforeCalc and an afterCalc method.", "tokens": [50760, 400, 370, 586, 452, 818, 3207, 307, 516, 281, 312, 257, 1508, 19273, 257, 949, 31279, 66, 293, 364, 934, 31279, 66, 3170, 13, 51370], "temperature": 0.0, "avg_logprob": -0.2811557307387843, "compression_ratio": 1.5633802816901408, "no_speech_prob": 0.011330886743962765}, {"id": 1266, "seek": 606746, "start": 6087.58, "end": 6093.06, "text": " And so if I run that, you can see it's, there it goes.", "tokens": [51370, 400, 370, 498, 286, 1190, 300, 11, 291, 393, 536, 309, 311, 11, 456, 309, 1709, 13, 51644], "temperature": 0.0, "avg_logprob": -0.2811557307387843, "compression_ratio": 1.5633802816901408, "no_speech_prob": 0.011330886743962765}, {"id": 1267, "seek": 609306, "start": 6093.06, "end": 6101.26, "text": " Okay, and so this is printing before and after every step, by calling beforeCalc and", "tokens": [50364, 1033, 11, 293, 370, 341, 307, 14699, 949, 293, 934, 633, 1823, 11, 538, 5141, 949, 31279, 66, 293, 50774], "temperature": 0.0, "avg_logprob": -0.2556187490398964, "compression_ratio": 1.6581632653061225, "no_speech_prob": 5.829119618283585e-05}, {"id": 1268, "seek": 609306, "start": 6101.26, "end": 6102.26, "text": " afterCalc.", "tokens": [50774, 934, 31279, 66, 13, 50824], "temperature": 0.0, "avg_logprob": -0.2556187490398964, "compression_ratio": 1.6581632653061225, "no_speech_prob": 5.829119618283585e-05}, {"id": 1269, "seek": 609306, "start": 6102.26, "end": 6105.860000000001, "text": " So callback actually doesn't have to be a callable, doesn't have to be a function.", "tokens": [50824, 407, 818, 3207, 767, 1177, 380, 362, 281, 312, 257, 818, 712, 11, 1177, 380, 362, 281, 312, 257, 2445, 13, 51004], "temperature": 0.0, "avg_logprob": -0.2556187490398964, "compression_ratio": 1.6581632653061225, "no_speech_prob": 5.829119618283585e-05}, {"id": 1270, "seek": 609306, "start": 6105.860000000001, "end": 6112.42, "text": " A callback could be something that contains methods.", "tokens": [51004, 316, 818, 3207, 727, 312, 746, 300, 8306, 7150, 13, 51332], "temperature": 0.0, "avg_logprob": -0.2556187490398964, "compression_ratio": 1.6581632653061225, "no_speech_prob": 5.829119618283585e-05}, {"id": 1271, "seek": 609306, "start": 6112.42, "end": 6119.860000000001, "text": " So we could have a version of this which actually, as you can see here, it's going to pass in", "tokens": [51332, 407, 321, 727, 362, 257, 3037, 295, 341, 597, 767, 11, 382, 291, 393, 536, 510, 11, 309, 311, 516, 281, 1320, 294, 51704], "temperature": 0.0, "avg_logprob": -0.2556187490398964, "compression_ratio": 1.6581632653061225, "no_speech_prob": 5.829119618283585e-05}, {"id": 1272, "seek": 611986, "start": 6119.86, "end": 6125.0599999999995, "text": " to afterCalc both the epoch number and the value it's up to.", "tokens": [50364, 281, 934, 31279, 66, 1293, 264, 30992, 339, 1230, 293, 264, 2158, 309, 311, 493, 281, 13, 50624], "temperature": 0.0, "avg_logprob": -0.2660855413640587, "compression_ratio": 1.5810810810810811, "no_speech_prob": 0.039047304540872574}, {"id": 1273, "seek": 611986, "start": 6125.0599999999995, "end": 6129.54, "text": " But by using star args and star star quarks, I can just safely ignore them if I don't want", "tokens": [50624, 583, 538, 1228, 3543, 3882, 82, 293, 3543, 3543, 421, 20851, 11, 286, 393, 445, 11750, 11200, 552, 498, 286, 500, 380, 528, 50848], "temperature": 0.0, "avg_logprob": -0.2660855413640587, "compression_ratio": 1.5810810810810811, "no_speech_prob": 0.039047304540872574}, {"id": 1274, "seek": 611986, "start": 6129.54, "end": 6130.54, "text": " them.", "tokens": [50848, 552, 13, 50898], "temperature": 0.0, "avg_logprob": -0.2660855413640587, "compression_ratio": 1.5810810810810811, "no_speech_prob": 0.039047304540872574}, {"id": 1275, "seek": 611986, "start": 6130.54, "end": 6134.219999999999, "text": " Right, so it's just going to chew them up and not complain.", "tokens": [50898, 1779, 11, 370, 309, 311, 445, 516, 281, 21200, 552, 493, 293, 406, 11024, 13, 51082], "temperature": 0.0, "avg_logprob": -0.2660855413640587, "compression_ratio": 1.5810810810810811, "no_speech_prob": 0.039047304540872574}, {"id": 1276, "seek": 611986, "start": 6134.219999999999, "end": 6141.46, "text": " If I didn't have those here, it won't work.", "tokens": [51082, 759, 286, 994, 380, 362, 729, 510, 11, 309, 1582, 380, 589, 13, 51444], "temperature": 0.0, "avg_logprob": -0.2660855413640587, "compression_ratio": 1.5810810810810811, "no_speech_prob": 0.039047304540872574}, {"id": 1277, "seek": 611986, "start": 6141.46, "end": 6148.9, "text": " See because it got passed in val equals, and there's nothing here looking for val equals.", "tokens": [51444, 3008, 570, 309, 658, 4678, 294, 1323, 6915, 11, 293, 456, 311, 1825, 510, 1237, 337, 1323, 6915, 13, 51816], "temperature": 0.0, "avg_logprob": -0.2660855413640587, "compression_ratio": 1.5810810810810811, "no_speech_prob": 0.039047304540872574}, {"id": 1278, "seek": 614890, "start": 6148.94, "end": 6150.9, "text": " And it doesn't like that.", "tokens": [50366, 400, 309, 1177, 380, 411, 300, 13, 50464], "temperature": 0.0, "avg_logprob": -0.23975194643621575, "compression_ratio": 1.4675324675324675, "no_speech_prob": 0.0010005033109337091}, {"id": 1279, "seek": 614890, "start": 6150.9, "end": 6156.259999999999, "text": " So this is one good use of star args and star star quarks, is to eat up arguments you don't", "tokens": [50464, 407, 341, 307, 472, 665, 764, 295, 3543, 3882, 82, 293, 3543, 3543, 421, 20851, 11, 307, 281, 1862, 493, 12869, 291, 500, 380, 50732], "temperature": 0.0, "avg_logprob": -0.23975194643621575, "compression_ratio": 1.4675324675324675, "no_speech_prob": 0.0010005033109337091}, {"id": 1280, "seek": 614890, "start": 6156.259999999999, "end": 6158.86, "text": " want.", "tokens": [50732, 528, 13, 50862], "temperature": 0.0, "avg_logprob": -0.23975194643621575, "compression_ratio": 1.4675324675324675, "no_speech_prob": 0.0010005033109337091}, {"id": 1281, "seek": 614890, "start": 6158.86, "end": 6160.0199999999995, "text": " Or we could use the argument.", "tokens": [50862, 1610, 321, 727, 764, 264, 6770, 13, 50920], "temperature": 0.0, "avg_logprob": -0.23975194643621575, "compression_ratio": 1.4675324675324675, "no_speech_prob": 0.0010005033109337091}, {"id": 1282, "seek": 614890, "start": 6160.0199999999995, "end": 6168.139999999999, "text": " So let's actually use epoch and val and print them out.", "tokens": [50920, 407, 718, 311, 767, 764, 30992, 339, 293, 1323, 293, 4482, 552, 484, 13, 51326], "temperature": 0.0, "avg_logprob": -0.23975194643621575, "compression_ratio": 1.4675324675324675, "no_speech_prob": 0.0010005033109337091}, {"id": 1283, "seek": 614890, "start": 6168.139999999999, "end": 6171.74, "text": " And there it is.", "tokens": [51326, 400, 456, 309, 307, 13, 51506], "temperature": 0.0, "avg_logprob": -0.23975194643621575, "compression_ratio": 1.4675324675324675, "no_speech_prob": 0.0010005033109337091}, {"id": 1284, "seek": 617174, "start": 6171.74, "end": 6179.86, "text": " So this is a more sophisticated callback that's giving us status as we go.", "tokens": [50364, 407, 341, 307, 257, 544, 16950, 818, 3207, 300, 311, 2902, 505, 6558, 382, 321, 352, 13, 50770], "temperature": 0.0, "avg_logprob": -0.4060138311141577, "compression_ratio": 1.2678571428571428, "no_speech_prob": 0.017985550686717033}, {"id": 1285, "seek": 617174, "start": 6179.86, "end": 6196.0599999999995, "text": " I'm going to skip this bit because we don't really care about that.", "tokens": [50770, 286, 478, 516, 281, 10023, 341, 857, 570, 321, 500, 380, 534, 1127, 466, 300, 13, 51580], "temperature": 0.0, "avg_logprob": -0.4060138311141577, "compression_ratio": 1.2678571428571428, "no_speech_prob": 0.017985550686717033}, {"id": 1286, "seek": 619606, "start": 6196.06, "end": 6201.700000000001, "text": " So finally, let's just review this idea of dunder, which we've mentioned before.", "tokens": [50364, 407, 2721, 11, 718, 311, 445, 3131, 341, 1558, 295, 274, 6617, 11, 597, 321, 600, 2835, 949, 13, 50646], "temperature": 0.0, "avg_logprob": -0.26962585819577706, "compression_ratio": 1.959090909090909, "no_speech_prob": 0.008711215108633041}, {"id": 1287, "seek": 619606, "start": 6201.700000000001, "end": 6206.54, "text": " But just to really nail this home, anything that looks like this, underscore underscore", "tokens": [50646, 583, 445, 281, 534, 10173, 341, 1280, 11, 1340, 300, 1542, 411, 341, 11, 37556, 37556, 50888], "temperature": 0.0, "avg_logprob": -0.26962585819577706, "compression_ratio": 1.959090909090909, "no_speech_prob": 0.008711215108633041}, {"id": 1288, "seek": 619606, "start": 6206.54, "end": 6210.54, "text": " something underscore underscore something, is special.", "tokens": [50888, 746, 37556, 37556, 746, 11, 307, 2121, 13, 51088], "temperature": 0.0, "avg_logprob": -0.26962585819577706, "compression_ratio": 1.959090909090909, "no_speech_prob": 0.008711215108633041}, {"id": 1289, "seek": 619606, "start": 6210.54, "end": 6214.9400000000005, "text": " And basically it could be that Python has defined that special thing, or PyTorch has", "tokens": [51088, 400, 1936, 309, 727, 312, 300, 15329, 575, 7642, 300, 2121, 551, 11, 420, 9953, 51, 284, 339, 575, 51308], "temperature": 0.0, "avg_logprob": -0.26962585819577706, "compression_ratio": 1.959090909090909, "no_speech_prob": 0.008711215108633041}, {"id": 1290, "seek": 619606, "start": 6214.9400000000005, "end": 6219.5, "text": " defined that special thing, or NumPy has defined that special thing, but they're special.", "tokens": [51308, 7642, 300, 2121, 551, 11, 420, 22592, 47, 88, 575, 7642, 300, 2121, 551, 11, 457, 436, 434, 2121, 13, 51536], "temperature": 0.0, "avg_logprob": -0.26962585819577706, "compression_ratio": 1.959090909090909, "no_speech_prob": 0.008711215108633041}, {"id": 1291, "seek": 619606, "start": 6219.5, "end": 6224.06, "text": " These are called dunder methods.", "tokens": [51536, 1981, 366, 1219, 274, 6617, 7150, 13, 51764], "temperature": 0.0, "avg_logprob": -0.26962585819577706, "compression_ratio": 1.959090909090909, "no_speech_prob": 0.008711215108633041}, {"id": 1292, "seek": 622406, "start": 6224.06, "end": 6229.740000000001, "text": " And some of them are defined as part of the Python data model.", "tokens": [50364, 400, 512, 295, 552, 366, 7642, 382, 644, 295, 264, 15329, 1412, 2316, 13, 50648], "temperature": 0.0, "avg_logprob": -0.35675795360278056, "compression_ratio": 1.6105263157894736, "no_speech_prob": 0.00028685375582426786}, {"id": 1293, "seek": 622406, "start": 6229.740000000001, "end": 6234.700000000001, "text": " And so if you go to the Python documentation, it'll tell you about these various different", "tokens": [50648, 400, 370, 498, 291, 352, 281, 264, 15329, 14333, 11, 309, 603, 980, 291, 466, 613, 3683, 819, 50896], "temperature": 0.0, "avg_logprob": -0.35675795360278056, "compression_ratio": 1.6105263157894736, "no_speech_prob": 0.00028685375582426786}, {"id": 1294, "seek": 622406, "start": 6234.700000000001, "end": 6239.42, "text": " ... here's Repra, which we used earlier.", "tokens": [50896, 1097, 510, 311, 3696, 424, 11, 597, 321, 1143, 3071, 13, 51132], "temperature": 0.0, "avg_logprob": -0.35675795360278056, "compression_ratio": 1.6105263157894736, "no_speech_prob": 0.00028685375582426786}, {"id": 1295, "seek": 622406, "start": 6239.42, "end": 6241.700000000001, "text": " Here's init, that we used earlier.", "tokens": [51132, 1692, 311, 3157, 11, 300, 321, 1143, 3071, 13, 51246], "temperature": 0.0, "avg_logprob": -0.35675795360278056, "compression_ratio": 1.6105263157894736, "no_speech_prob": 0.00028685375582426786}, {"id": 1296, "seek": 622406, "start": 6241.700000000001, "end": 6243.660000000001, "text": " So they're all here.", "tokens": [51246, 407, 436, 434, 439, 510, 13, 51344], "temperature": 0.0, "avg_logprob": -0.35675795360278056, "compression_ratio": 1.6105263157894736, "no_speech_prob": 0.00028685375582426786}, {"id": 1297, "seek": 622406, "start": 6243.660000000001, "end": 6245.34, "text": " PyTorch has some of its own.", "tokens": [51344, 9953, 51, 284, 339, 575, 512, 295, 1080, 1065, 13, 51428], "temperature": 0.0, "avg_logprob": -0.35675795360278056, "compression_ratio": 1.6105263157894736, "no_speech_prob": 0.00028685375582426786}, {"id": 1298, "seek": 622406, "start": 6245.34, "end": 6247.1, "text": " NumPy has some of its own.", "tokens": [51428, 22592, 47, 88, 575, 512, 295, 1080, 1065, 13, 51516], "temperature": 0.0, "avg_logprob": -0.35675795360278056, "compression_ratio": 1.6105263157894736, "no_speech_prob": 0.00028685375582426786}, {"id": 1299, "seek": 624710, "start": 6247.1, "end": 6255.04, "text": " So for example, if Python sees plus, what it actually does is it calls dunder add.", "tokens": [50364, 407, 337, 1365, 11, 498, 15329, 8194, 1804, 11, 437, 309, 767, 775, 307, 309, 5498, 274, 6617, 909, 13, 50761], "temperature": 0.0, "avg_logprob": -0.31117516059380074, "compression_ratio": 1.489010989010989, "no_speech_prob": 0.006003372371196747}, {"id": 1300, "seek": 624710, "start": 6255.04, "end": 6259.860000000001, "text": " So if we want to create something that's not very good at adding things, it actually already", "tokens": [50761, 407, 498, 321, 528, 281, 1884, 746, 300, 311, 406, 588, 665, 412, 5127, 721, 11, 309, 767, 1217, 51002], "temperature": 0.0, "avg_logprob": -0.31117516059380074, "compression_ratio": 1.489010989010989, "no_speech_prob": 0.006003372371196747}, {"id": 1301, "seek": 624710, "start": 6259.860000000001, "end": 6266.740000000001, "text": " ... also always adds 0.01 to it.", "tokens": [51002, 1097, 611, 1009, 10860, 1958, 13, 10607, 281, 309, 13, 51346], "temperature": 0.0, "avg_logprob": -0.31117516059380074, "compression_ratio": 1.489010989010989, "no_speech_prob": 0.006003372371196747}, {"id": 1302, "seek": 624710, "start": 6266.740000000001, "end": 6272.14, "text": " Then I can say sloppy adder 1 plus sloppy adder 2 equals 3.01.", "tokens": [51346, 1396, 286, 393, 584, 43684, 909, 260, 502, 1804, 43684, 909, 260, 568, 6915, 805, 13, 10607, 13, 51616], "temperature": 0.0, "avg_logprob": -0.31117516059380074, "compression_ratio": 1.489010989010989, "no_speech_prob": 0.006003372371196747}, {"id": 1303, "seek": 627214, "start": 6272.14, "end": 6278.700000000001, "text": " So plus here is actually calling dunder add.", "tokens": [50364, 407, 1804, 510, 307, 767, 5141, 274, 6617, 909, 13, 50692], "temperature": 0.0, "avg_logprob": -0.21883498921113856, "compression_ratio": 1.445414847161572, "no_speech_prob": 0.001325014280155301}, {"id": 1304, "seek": 627214, "start": 6278.700000000001, "end": 6282.740000000001, "text": " So if you're not familiar with these, click on this data model link and read about these", "tokens": [50692, 407, 498, 291, 434, 406, 4963, 365, 613, 11, 2052, 322, 341, 1412, 2316, 2113, 293, 1401, 466, 613, 50894], "temperature": 0.0, "avg_logprob": -0.21883498921113856, "compression_ratio": 1.445414847161572, "no_speech_prob": 0.001325014280155301}, {"id": 1305, "seek": 627214, "start": 6282.740000000001, "end": 6288.820000000001, "text": " specific 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11 methods.", "tokens": [50894, 2685, 502, 11, 568, 11, 805, 11, 1017, 11, 1025, 11, 1386, 11, 1614, 11, 1649, 11, 1722, 11, 1266, 11, 2975, 7150, 13, 51198], "temperature": 0.0, "avg_logprob": -0.21883498921113856, "compression_ratio": 1.445414847161572, "no_speech_prob": 0.001325014280155301}, {"id": 1306, "seek": 627214, "start": 6288.820000000001, "end": 6291.860000000001, "text": " Because we'll be using all of these in the course.", "tokens": [51198, 1436, 321, 603, 312, 1228, 439, 295, 613, 294, 264, 1164, 13, 51350], "temperature": 0.0, "avg_logprob": -0.21883498921113856, "compression_ratio": 1.445414847161572, "no_speech_prob": 0.001325014280155301}, {"id": 1307, "seek": 627214, "start": 6291.860000000001, "end": 6299.06, "text": " So I'll try to revise them when we can, but I'm generally going to assume that you know", "tokens": [51350, 407, 286, 603, 853, 281, 44252, 552, 562, 321, 393, 11, 457, 286, 478, 5101, 516, 281, 6552, 300, 291, 458, 51710], "temperature": 0.0, "avg_logprob": -0.21883498921113856, "compression_ratio": 1.445414847161572, "no_speech_prob": 0.001325014280155301}, {"id": 1308, "seek": 627214, "start": 6299.06, "end": 6300.06, "text": " these.", "tokens": [51710, 613, 13, 51760], "temperature": 0.0, "avg_logprob": -0.21883498921113856, "compression_ratio": 1.445414847161572, "no_speech_prob": 0.001325014280155301}, {"id": 1309, "seek": 630006, "start": 6300.06, "end": 6305.820000000001, "text": " A particularly interesting one is getAttr.", "tokens": [50364, 316, 4098, 1880, 472, 307, 483, 38151, 81, 13, 50652], "temperature": 0.0, "avg_logprob": -0.2811392034803118, "compression_ratio": 1.5064935064935066, "no_speech_prob": 0.002511455211788416}, {"id": 1310, "seek": 630006, "start": 6305.820000000001, "end": 6307.5, "text": " We've seen setAttr already.", "tokens": [50652, 492, 600, 1612, 992, 38151, 81, 1217, 13, 50736], "temperature": 0.0, "avg_logprob": -0.2811392034803118, "compression_ratio": 1.5064935064935066, "no_speech_prob": 0.002511455211788416}, {"id": 1311, "seek": 630006, "start": 6307.5, "end": 6309.22, "text": " GetAttr is just the opposite.", "tokens": [50736, 3240, 38151, 81, 307, 445, 264, 6182, 13, 50822], "temperature": 0.0, "avg_logprob": -0.2811392034803118, "compression_ratio": 1.5064935064935066, "no_speech_prob": 0.002511455211788416}, {"id": 1312, "seek": 630006, "start": 6309.22, "end": 6310.22, "text": " Take a look at this.", "tokens": [50822, 3664, 257, 574, 412, 341, 13, 50872], "temperature": 0.0, "avg_logprob": -0.2811392034803118, "compression_ratio": 1.5064935064935066, "no_speech_prob": 0.002511455211788416}, {"id": 1313, "seek": 630006, "start": 6310.22, "end": 6312.700000000001, "text": " Here's a class.", "tokens": [50872, 1692, 311, 257, 1508, 13, 50996], "temperature": 0.0, "avg_logprob": -0.2811392034803118, "compression_ratio": 1.5064935064935066, "no_speech_prob": 0.002511455211788416}, {"id": 1314, "seek": 630006, "start": 6312.700000000001, "end": 6316.46, "text": " It just contains two attributes, a and b, that are set to 1 and 2.", "tokens": [50996, 467, 445, 8306, 732, 17212, 11, 257, 293, 272, 11, 300, 366, 992, 281, 502, 293, 568, 13, 51184], "temperature": 0.0, "avg_logprob": -0.2811392034803118, "compression_ratio": 1.5064935064935066, "no_speech_prob": 0.002511455211788416}, {"id": 1315, "seek": 630006, "start": 6316.46, "end": 6323.9800000000005, "text": " So I'll create an object of that class, a.b equals 2, because I set b to 2.", "tokens": [51184, 407, 286, 603, 1884, 364, 2657, 295, 300, 1508, 11, 257, 13, 65, 6915, 568, 11, 570, 286, 992, 272, 281, 568, 13, 51560], "temperature": 0.0, "avg_logprob": -0.2811392034803118, "compression_ratio": 1.5064935064935066, "no_speech_prob": 0.002511455211788416}, {"id": 1316, "seek": 630006, "start": 6323.9800000000005, "end": 6328.72, "text": " Now when you say a.b, that's just syntax sugar basically in Python.", "tokens": [51560, 823, 562, 291, 584, 257, 13, 65, 11, 300, 311, 445, 28431, 5076, 1936, 294, 15329, 13, 51797], "temperature": 0.0, "avg_logprob": -0.2811392034803118, "compression_ratio": 1.5064935064935066, "no_speech_prob": 0.002511455211788416}, {"id": 1317, "seek": 632872, "start": 6328.72, "end": 6332.96, "text": " What it's actually calling behind the scenes is getAttr.", "tokens": [50364, 708, 309, 311, 767, 5141, 2261, 264, 8026, 307, 483, 38151, 81, 13, 50576], "temperature": 0.0, "avg_logprob": -0.34887451171875, "compression_ratio": 1.6205128205128205, "no_speech_prob": 0.07695610821247101}, {"id": 1318, "seek": 632872, "start": 6332.96, "end": 6345.02, "text": " It calls getAttr on the object, and so this one here is the same as getAttr a, b.", "tokens": [50576, 467, 5498, 483, 38151, 81, 322, 264, 2657, 11, 293, 370, 341, 472, 510, 307, 264, 912, 382, 483, 38151, 81, 257, 11, 272, 13, 51179], "temperature": 0.0, "avg_logprob": -0.34887451171875, "compression_ratio": 1.6205128205128205, "no_speech_prob": 0.07695610821247101}, {"id": 1319, "seek": 632872, "start": 6345.02, "end": 6348.4400000000005, "text": " Which hopefully, actually that'll do.", "tokens": [51179, 3013, 4696, 11, 767, 300, 603, 360, 13, 51350], "temperature": 0.0, "avg_logprob": -0.34887451171875, "compression_ratio": 1.6205128205128205, "no_speech_prob": 0.07695610821247101}, {"id": 1320, "seek": 632872, "start": 6348.4400000000005, "end": 6350.08, "text": " So it calls getAttr a, b.", "tokens": [51350, 407, 309, 5498, 483, 38151, 81, 257, 11, 272, 13, 51432], "temperature": 0.0, "avg_logprob": -0.34887451171875, "compression_ratio": 1.6205128205128205, "no_speech_prob": 0.07695610821247101}, {"id": 1321, "seek": 632872, "start": 6350.08, "end": 6356.400000000001, "text": " This can kind of be fun, because you could call getAttr a, and then either b or a randomly.", "tokens": [51432, 639, 393, 733, 295, 312, 1019, 11, 570, 291, 727, 818, 483, 38151, 81, 257, 11, 293, 550, 2139, 272, 420, 257, 16979, 13, 51748], "temperature": 0.0, "avg_logprob": -0.34887451171875, "compression_ratio": 1.6205128205128205, "no_speech_prob": 0.07695610821247101}, {"id": 1322, "seek": 632872, "start": 6356.400000000001, "end": 6358.280000000001, "text": " How's that for crazy?", "tokens": [51748, 1012, 311, 300, 337, 3219, 30, 51842], "temperature": 0.0, "avg_logprob": -0.34887451171875, "compression_ratio": 1.6205128205128205, "no_speech_prob": 0.07695610821247101}, {"id": 1323, "seek": 635828, "start": 6358.84, "end": 6366.5199999999995, "text": " So if I run this, 2, 1, 1, 1, 2, as you can see it's random.", "tokens": [50392, 407, 498, 286, 1190, 341, 11, 568, 11, 502, 11, 502, 11, 502, 11, 568, 11, 382, 291, 393, 536, 309, 311, 4974, 13, 50776], "temperature": 0.0, "avg_logprob": -0.2813201631818499, "compression_ratio": 1.5044642857142858, "no_speech_prob": 7.183259640441975e-06}, {"id": 1324, "seek": 635828, "start": 6366.5199999999995, "end": 6371.92, "text": " So yeah, Python's such a dynamic language, you can even set it up so you literally don't", "tokens": [50776, 407, 1338, 11, 15329, 311, 1270, 257, 8546, 2856, 11, 291, 393, 754, 992, 309, 493, 370, 291, 3736, 500, 380, 51046], "temperature": 0.0, "avg_logprob": -0.2813201631818499, "compression_ratio": 1.5044642857142858, "no_speech_prob": 7.183259640441975e-06}, {"id": 1325, "seek": 635828, "start": 6371.92, "end": 6375.44, "text": " know what attribute is going to be called.", "tokens": [51046, 458, 437, 19667, 307, 516, 281, 312, 1219, 13, 51222], "temperature": 0.0, "avg_logprob": -0.2813201631818499, "compression_ratio": 1.5044642857142858, "no_speech_prob": 7.183259640441975e-06}, {"id": 1326, "seek": 635828, "start": 6375.44, "end": 6383.759999999999, "text": " Now getAttr behind the scenes is actually calling something called doneToGetAttr.", "tokens": [51222, 823, 483, 38151, 81, 2261, 264, 8026, 307, 767, 5141, 746, 1219, 1096, 13342, 18133, 38151, 81, 13, 51638], "temperature": 0.0, "avg_logprob": -0.2813201631818499, "compression_ratio": 1.5044642857142858, "no_speech_prob": 7.183259640441975e-06}, {"id": 1327, "seek": 635828, "start": 6383.759999999999, "end": 6388.0, "text": " And by default it'll use the version in the object base class.", "tokens": [51638, 400, 538, 7576, 309, 603, 764, 264, 3037, 294, 264, 2657, 3096, 1508, 13, 51850], "temperature": 0.0, "avg_logprob": -0.2813201631818499, "compression_ratio": 1.5044642857142858, "no_speech_prob": 7.183259640441975e-06}, {"id": 1328, "seek": 638800, "start": 6388.72, "end": 6391.92, "text": " So here's something just like a, I've got a and b defined.", "tokens": [50400, 407, 510, 311, 746, 445, 411, 257, 11, 286, 600, 658, 257, 293, 272, 7642, 13, 50560], "temperature": 0.0, "avg_logprob": -0.25313775879996164, "compression_ratio": 1.6437768240343347, "no_speech_prob": 8.801098374533467e-06}, {"id": 1329, "seek": 638800, "start": 6391.92, "end": 6394.6, "text": " But I've also got doneToGetAttr defined.", "tokens": [50560, 583, 286, 600, 611, 658, 1096, 13342, 18133, 38151, 81, 7642, 13, 50694], "temperature": 0.0, "avg_logprob": -0.25313775879996164, "compression_ratio": 1.6437768240343347, "no_speech_prob": 8.801098374533467e-06}, {"id": 1330, "seek": 638800, "start": 6394.6, "end": 6399.92, "text": " And so doneToGetAttr, it's only called for stuff that hasn't been defined yet.", "tokens": [50694, 400, 370, 1096, 13342, 18133, 38151, 81, 11, 309, 311, 787, 1219, 337, 1507, 300, 6132, 380, 668, 7642, 1939, 13, 50960], "temperature": 0.0, "avg_logprob": -0.25313775879996164, "compression_ratio": 1.6437768240343347, "no_speech_prob": 8.801098374533467e-06}, {"id": 1331, "seek": 638800, "start": 6399.92, "end": 6405.68, "text": " And it'll pass in the key, or the name of the attribute.", "tokens": [50960, 400, 309, 603, 1320, 294, 264, 2141, 11, 420, 264, 1315, 295, 264, 19667, 13, 51248], "temperature": 0.0, "avg_logprob": -0.25313775879996164, "compression_ratio": 1.6437768240343347, "no_speech_prob": 8.801098374533467e-06}, {"id": 1332, "seek": 638800, "start": 6405.68, "end": 6410.84, "text": " So generally speaking, if the first character is an underscore, it's going to be private", "tokens": [51248, 407, 5101, 4124, 11, 498, 264, 700, 2517, 307, 364, 37556, 11, 309, 311, 516, 281, 312, 4551, 51506], "temperature": 0.0, "avg_logprob": -0.25313775879996164, "compression_ratio": 1.6437768240343347, "no_speech_prob": 8.801098374533467e-06}, {"id": 1333, "seek": 638800, "start": 6410.84, "end": 6412.08, "text": " or special.", "tokens": [51506, 420, 2121, 13, 51568], "temperature": 0.0, "avg_logprob": -0.25313775879996164, "compression_ratio": 1.6437768240343347, "no_speech_prob": 8.801098374533467e-06}, {"id": 1334, "seek": 638800, "start": 6412.08, "end": 6414.64, "text": " So I'm just going to raise an attribute error.", "tokens": [51568, 407, 286, 478, 445, 516, 281, 5300, 364, 19667, 6713, 13, 51696], "temperature": 0.0, "avg_logprob": -0.25313775879996164, "compression_ratio": 1.6437768240343347, "no_speech_prob": 8.801098374533467e-06}, {"id": 1335, "seek": 641464, "start": 6414.88, "end": 6419.96, "text": " Otherwise I'm going to steal it and return hello from K.", "tokens": [50376, 10328, 286, 478, 516, 281, 11009, 309, 293, 2736, 7751, 490, 591, 13, 50630], "temperature": 0.0, "avg_logprob": -0.3106679717699687, "compression_ratio": 1.546448087431694, "no_speech_prob": 0.0675436481833458}, {"id": 1336, "seek": 641464, "start": 6419.96, "end": 6425.360000000001, "text": " So if I go b.a, that's defined.", "tokens": [50630, 407, 498, 286, 352, 272, 13, 64, 11, 300, 311, 7642, 13, 50900], "temperature": 0.0, "avg_logprob": -0.3106679717699687, "compression_ratio": 1.546448087431694, "no_speech_prob": 0.0675436481833458}, {"id": 1337, "seek": 641464, "start": 6425.360000000001, "end": 6426.76, "text": " So it gives me 1.", "tokens": [50900, 407, 309, 2709, 385, 502, 13, 50970], "temperature": 0.0, "avg_logprob": -0.3106679717699687, "compression_ratio": 1.546448087431694, "no_speech_prob": 0.0675436481833458}, {"id": 1338, "seek": 641464, "start": 6426.76, "end": 6431.200000000001, "text": " If I go b.foo, that's not defined, so it calls getAttr.", "tokens": [50970, 759, 286, 352, 272, 13, 69, 1986, 11, 300, 311, 406, 7642, 11, 370, 309, 5498, 483, 38151, 81, 13, 51192], "temperature": 0.0, "avg_logprob": -0.3106679717699687, "compression_ratio": 1.546448087431694, "no_speech_prob": 0.0675436481833458}, {"id": 1339, "seek": 641464, "start": 6431.200000000001, "end": 6432.92, "text": " And I get back hello from foo.", "tokens": [51192, 400, 286, 483, 646, 7751, 490, 726, 78, 13, 51278], "temperature": 0.0, "avg_logprob": -0.3106679717699687, "compression_ratio": 1.546448087431694, "no_speech_prob": 0.0675436481833458}, {"id": 1340, "seek": 641464, "start": 6432.92, "end": 6440.240000000001, "text": " And so this gets used a lot in both fast AI code and also hugging face code to, you know,", "tokens": [51278, 400, 370, 341, 2170, 1143, 257, 688, 294, 1293, 2370, 7318, 3089, 293, 611, 41706, 1851, 3089, 281, 11, 291, 458, 11, 51644], "temperature": 0.0, "avg_logprob": -0.3106679717699687, "compression_ratio": 1.546448087431694, "no_speech_prob": 0.0675436481833458}, {"id": 1341, "seek": 644024, "start": 6440.24, "end": 6446.04, "text": " often make it more convenient to access things.", "tokens": [50364, 2049, 652, 309, 544, 10851, 281, 2105, 721, 13, 50654], "temperature": 0.0, "avg_logprob": -0.2509015229371217, "compression_ratio": 1.592741935483871, "no_speech_prob": 0.008711062371730804}, {"id": 1342, "seek": 644024, "start": 6446.04, "end": 6453.36, "text": " So that's, yeah, that's how the getAttr function and the doneToGetAttr method work.", "tokens": [50654, 407, 300, 311, 11, 1338, 11, 300, 311, 577, 264, 483, 38151, 81, 2445, 293, 264, 1096, 13342, 18133, 38151, 81, 3170, 589, 13, 51020], "temperature": 0.0, "avg_logprob": -0.2509015229371217, "compression_ratio": 1.592741935483871, "no_speech_prob": 0.008711062371730804}, {"id": 1343, "seek": 644024, "start": 6453.36, "end": 6458.32, "text": " Okay, so I went over that pretty quickly.", "tokens": [51020, 1033, 11, 370, 286, 1437, 670, 300, 1238, 2661, 13, 51268], "temperature": 0.0, "avg_logprob": -0.2509015229371217, "compression_ratio": 1.592741935483871, "no_speech_prob": 0.008711062371730804}, {"id": 1344, "seek": 644024, "start": 6458.32, "end": 6460.92, "text": " Since I know for quite a few folks this will be all review.", "tokens": [51268, 4162, 286, 458, 337, 1596, 257, 1326, 4024, 341, 486, 312, 439, 3131, 13, 51398], "temperature": 0.0, "avg_logprob": -0.2509015229371217, "compression_ratio": 1.592741935483871, "no_speech_prob": 0.008711062371730804}, {"id": 1345, "seek": 644024, "start": 6460.92, "end": 6465.44, "text": " But I know for folks who haven't seen any of this, this is a lot to cover.", "tokens": [51398, 583, 286, 458, 337, 4024, 567, 2378, 380, 1612, 604, 295, 341, 11, 341, 307, 257, 688, 281, 2060, 13, 51624], "temperature": 0.0, "avg_logprob": -0.2509015229371217, "compression_ratio": 1.592741935483871, "no_speech_prob": 0.008711062371730804}, {"id": 1346, "seek": 644024, "start": 6465.44, "end": 6469.08, "text": " So I'm hoping that you'll kind of go back over this, revise it slowly, experiment with", "tokens": [51624, 407, 286, 478, 7159, 300, 291, 603, 733, 295, 352, 646, 670, 341, 11, 44252, 309, 5692, 11, 5120, 365, 51806], "temperature": 0.0, "avg_logprob": -0.2509015229371217, "compression_ratio": 1.592741935483871, "no_speech_prob": 0.008711062371730804}, {"id": 1347, "seek": 646908, "start": 6469.12, "end": 6474.36, "text": " it, and look up some additional resources and ask on the forum and stuff for anything", "tokens": [50366, 309, 11, 293, 574, 493, 512, 4497, 3593, 293, 1029, 322, 264, 17542, 293, 1507, 337, 1340, 50628], "temperature": 0.0, "avg_logprob": -0.2671845555305481, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.0009399399859830737}, {"id": 1348, "seek": 646908, "start": 6474.36, "end": 6476.48, "text": " that's not clear.", "tokens": [50628, 300, 311, 406, 1850, 13, 50734], "temperature": 0.0, "avg_logprob": -0.2671845555305481, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.0009399399859830737}, {"id": 1349, "seek": 646908, "start": 6476.48, "end": 6483.6, "text": " Remember, everybody has parts of the course that's really easy for them, and parts of", "tokens": [50734, 5459, 11, 2201, 575, 3166, 295, 264, 1164, 300, 311, 534, 1858, 337, 552, 11, 293, 3166, 295, 51090], "temperature": 0.0, "avg_logprob": -0.2671845555305481, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.0009399399859830737}, {"id": 1350, "seek": 646908, "start": 6483.6, "end": 6486.32, "text": " the course that are completely unfamiliar for them.", "tokens": [51090, 264, 1164, 300, 366, 2584, 29415, 337, 552, 13, 51226], "temperature": 0.0, "avg_logprob": -0.2671845555305481, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.0009399399859830737}, {"id": 1351, "seek": 646908, "start": 6486.32, "end": 6490.16, "text": " And so if this particular part of the course is completely unfamiliar to you, it's not", "tokens": [51226, 400, 370, 498, 341, 1729, 644, 295, 264, 1164, 307, 2584, 29415, 281, 291, 11, 309, 311, 406, 51418], "temperature": 0.0, "avg_logprob": -0.2671845555305481, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.0009399399859830737}, {"id": 1352, "seek": 646908, "start": 6490.16, "end": 6496.04, "text": " because this is harder, or going to be more difficult, or whatever.", "tokens": [51418, 570, 341, 307, 6081, 11, 420, 516, 281, 312, 544, 2252, 11, 420, 2035, 13, 51712], "temperature": 0.0, "avg_logprob": -0.2671845555305481, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.0009399399859830737}, {"id": 1353, "seek": 649604, "start": 6496.04, "end": 6501.2, "text": " It just so happens that this is a bit that you're less familiar with.", "tokens": [50364, 467, 445, 370, 2314, 300, 341, 307, 257, 857, 300, 291, 434, 1570, 4963, 365, 13, 50622], "temperature": 0.0, "avg_logprob": -0.23089115436260516, "compression_ratio": 1.765625, "no_speech_prob": 0.008577156811952591}, {"id": 1354, "seek": 649604, "start": 6501.2, "end": 6505.16, "text": " Or maybe the stuff about calculus in the last lesson was a bit that you're less familiar", "tokens": [50622, 1610, 1310, 264, 1507, 466, 33400, 294, 264, 1036, 6898, 390, 257, 857, 300, 291, 434, 1570, 4963, 50820], "temperature": 0.0, "avg_logprob": -0.23089115436260516, "compression_ratio": 1.765625, "no_speech_prob": 0.008577156811952591}, {"id": 1355, "seek": 649604, "start": 6505.16, "end": 6506.16, "text": " with.", "tokens": [50820, 365, 13, 50870], "temperature": 0.0, "avg_logprob": -0.23089115436260516, "compression_ratio": 1.765625, "no_speech_prob": 0.008577156811952591}, {"id": 1356, "seek": 649604, "start": 6506.16, "end": 6511.08, "text": " There isn't really anything particularly in the course that's more difficult than other", "tokens": [50870, 821, 1943, 380, 534, 1340, 4098, 294, 264, 1164, 300, 311, 544, 2252, 813, 661, 51116], "temperature": 0.0, "avg_logprob": -0.23089115436260516, "compression_ratio": 1.765625, "no_speech_prob": 0.008577156811952591}, {"id": 1357, "seek": 649604, "start": 6511.08, "end": 6512.08, "text": " parts.", "tokens": [51116, 3166, 13, 51166], "temperature": 0.0, "avg_logprob": -0.23089115436260516, "compression_ratio": 1.765625, "no_speech_prob": 0.008577156811952591}, {"id": 1358, "seek": 649604, "start": 6512.08, "end": 6515.9, "text": " It's just that, you know, based on whether you happen to have that background.", "tokens": [51166, 467, 311, 445, 300, 11, 291, 458, 11, 2361, 322, 1968, 291, 1051, 281, 362, 300, 3678, 13, 51357], "temperature": 0.0, "avg_logprob": -0.23089115436260516, "compression_ratio": 1.765625, "no_speech_prob": 0.008577156811952591}, {"id": 1359, "seek": 649604, "start": 6515.9, "end": 6521.24, "text": " And so, yeah, if you spend a few hours studying and practicing, you know, you'll be able to", "tokens": [51357, 400, 370, 11, 1338, 11, 498, 291, 3496, 257, 1326, 2496, 7601, 293, 11350, 11, 291, 458, 11, 291, 603, 312, 1075, 281, 51624], "temperature": 0.0, "avg_logprob": -0.23089115436260516, "compression_ratio": 1.765625, "no_speech_prob": 0.008577156811952591}, {"id": 1360, "seek": 649604, "start": 6521.24, "end": 6524.12, "text": " pick up these things.", "tokens": [51624, 1888, 493, 613, 721, 13, 51768], "temperature": 0.0, "avg_logprob": -0.23089115436260516, "compression_ratio": 1.765625, "no_speech_prob": 0.008577156811952591}, {"id": 1361, "seek": 652412, "start": 6524.2, "end": 6528.16, "text": " And yeah, so don't stress if there are things that you don't get right away.", "tokens": [50368, 400, 1338, 11, 370, 500, 380, 4244, 498, 456, 366, 721, 300, 291, 500, 380, 483, 558, 1314, 13, 50566], "temperature": 0.0, "avg_logprob": -0.24931048607641412, "compression_ratio": 1.743682310469314, "no_speech_prob": 0.0013044915394857526}, {"id": 1362, "seek": 652412, "start": 6528.16, "end": 6532.8, "text": " Just take the time, and if you, yeah, if you do get lost, please ask.", "tokens": [50566, 1449, 747, 264, 565, 11, 293, 498, 291, 11, 1338, 11, 498, 291, 360, 483, 2731, 11, 1767, 1029, 13, 50798], "temperature": 0.0, "avg_logprob": -0.24931048607641412, "compression_ratio": 1.743682310469314, "no_speech_prob": 0.0013044915394857526}, {"id": 1363, "seek": 652412, "start": 6532.8, "end": 6534.92, "text": " Because people are very keen to help.", "tokens": [50798, 1436, 561, 366, 588, 20297, 281, 854, 13, 50904], "temperature": 0.0, "avg_logprob": -0.24931048607641412, "compression_ratio": 1.743682310469314, "no_speech_prob": 0.0013044915394857526}, {"id": 1364, "seek": 652412, "start": 6534.92, "end": 6538.72, "text": " If you've tried asking on the forum, hopefully you've noticed that people are really keen", "tokens": [50904, 759, 291, 600, 3031, 3365, 322, 264, 17542, 11, 4696, 291, 600, 5694, 300, 561, 366, 534, 20297, 51094], "temperature": 0.0, "avg_logprob": -0.24931048607641412, "compression_ratio": 1.743682310469314, "no_speech_prob": 0.0013044915394857526}, {"id": 1365, "seek": 652412, "start": 6538.72, "end": 6539.72, "text": " to help.", "tokens": [51094, 281, 854, 13, 51144], "temperature": 0.0, "avg_logprob": -0.24931048607641412, "compression_ratio": 1.743682310469314, "no_speech_prob": 0.0013044915394857526}, {"id": 1366, "seek": 652412, "start": 6539.72, "end": 6545.66, "text": " All right, so I think this has been a pretty successful lesson.", "tokens": [51144, 1057, 558, 11, 370, 286, 519, 341, 575, 668, 257, 1238, 4406, 6898, 13, 51441], "temperature": 0.0, "avg_logprob": -0.24931048607641412, "compression_ratio": 1.743682310469314, "no_speech_prob": 0.0013044915394857526}, {"id": 1367, "seek": 652412, "start": 6545.66, "end": 6549.0, "text": " We've got to a point where we've got a pretty nicely optimized training loop.", "tokens": [51441, 492, 600, 658, 281, 257, 935, 689, 321, 600, 658, 257, 1238, 9594, 26941, 3097, 6367, 13, 51608], "temperature": 0.0, "avg_logprob": -0.24931048607641412, "compression_ratio": 1.743682310469314, "no_speech_prob": 0.0013044915394857526}, {"id": 1368, "seek": 652412, "start": 6549.0, "end": 6552.92, "text": " We understand exactly what data loaders and data sets do.", "tokens": [51608, 492, 1223, 2293, 437, 1412, 3677, 433, 293, 1412, 6352, 360, 13, 51804], "temperature": 0.0, "avg_logprob": -0.24931048607641412, "compression_ratio": 1.743682310469314, "no_speech_prob": 0.0013044915394857526}, {"id": 1369, "seek": 655292, "start": 6552.92, "end": 6555.0, "text": " We've got an optimizer.", "tokens": [50364, 492, 600, 658, 364, 5028, 6545, 13, 50468], "temperature": 0.0, "avg_logprob": -0.2844112910581439, "compression_ratio": 1.592920353982301, "no_speech_prob": 0.00016346359916497022}, {"id": 1370, "seek": 655292, "start": 6555.0, "end": 6559.72, "text": " We've been playing with hugging face data sets, and we've got those working really smoothly.", "tokens": [50468, 492, 600, 668, 2433, 365, 41706, 1851, 1412, 6352, 11, 293, 321, 600, 658, 729, 1364, 534, 19565, 13, 50704], "temperature": 0.0, "avg_logprob": -0.2844112910581439, "compression_ratio": 1.592920353982301, "no_speech_prob": 0.00016346359916497022}, {"id": 1371, "seek": 655292, "start": 6559.72, "end": 6565.24, "text": " So we really feel like we're in a pretty good position to write our generic learner training", "tokens": [50704, 407, 321, 534, 841, 411, 321, 434, 294, 257, 1238, 665, 2535, 281, 2464, 527, 19577, 33347, 3097, 50980], "temperature": 0.0, "avg_logprob": -0.2844112910581439, "compression_ratio": 1.592920353982301, "no_speech_prob": 0.00016346359916497022}, {"id": 1372, "seek": 655292, "start": 6565.24, "end": 6569.52, "text": " loop, and then we can start building and experimenting with lots of models.", "tokens": [50980, 6367, 11, 293, 550, 321, 393, 722, 2390, 293, 29070, 365, 3195, 295, 5245, 13, 51194], "temperature": 0.0, "avg_logprob": -0.2844112910581439, "compression_ratio": 1.592920353982301, "no_speech_prob": 0.00016346359916497022}, {"id": 1373, "seek": 655292, "start": 6569.52, "end": 6572.92, "text": " So look forward to seeing you next time to doing that together.", "tokens": [51194, 407, 574, 2128, 281, 2577, 291, 958, 565, 281, 884, 300, 1214, 13, 51364], "temperature": 0.0, "avg_logprob": -0.2844112910581439, "compression_ratio": 1.592920353982301, "no_speech_prob": 0.00016346359916497022}, {"id": 1374, "seek": 655292, "start": 6572.92, "end": 6574.04, "text": " Okay, bye.", "tokens": [51364, 1033, 11, 6543, 13, 51420], "temperature": 0.0, "avg_logprob": -0.2844112910581439, "compression_ratio": 1.592920353982301, "no_speech_prob": 0.00016346359916497022}], "language": "en"}