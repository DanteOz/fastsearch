{"text": " Well welcome back to machine learning one of the most exciting things this week Almost certainly the most exciting thing this week is that fast AI is now on pip so you can pip install fast AI and So thank you to Prince and to Karem for making that happen To usf students who had never published a pip package before and this is one of the harder ones to publish because it's got a lot of dependencies So it's you know probably still easiest just to do the conda end of update thing But a couple of places that it would be handy instead to pip install fast AI would be well obviously if you're working Outside of the the repo and the notebooks then this gives you access to fast AI everywhere Also, I believe they submitted a pull request to Kaggle to try and get it added to the Kaggle kernels So hopefully you'll be able to use it on capital kernels soon And yeah, you can use it at your work or whatever else So that's that's exciting. I mean I'm not going to say it's like officially released yet You know it's still very early obviously and we're still you're helping add documentation and all that kind of stuff, but It's great that that's now there a Couple of cool kernels from usf students this week thought of highlight two that were both from the text normalization competition which was about Trying to take text which was Written out you know written a standard English text they also had one for Russian and you're trying to kind of identify Things that could be like a first second third and say like that's a cardinal number or this is a phone number or whatever and I did a quick little bit of searching and I saw that There had been some attempts in academia to use deep learning for this, but they hadn't managed to make much progress and actually noticed us our Vera's Kernel here which gets point 992 on the leaderboard, which I think is like top 20 Is yeah, it's kind of entirely heuristic, and it's a great example of Kind of feature engineering this in this case the whole thing is basically entirely feature engineering So it's basically looking through and using lots of regular expressions to figure out for each token What is it you know? and I think she's done a great job here kind of laying it all out clearly as to what all the different pieces are and how they all fit together and She mentioned that she's maybe hoping to turn this into a library, which I think would be great right you know you could use this to Grab a piece of text and pull out. What are all the pieces in it? It's the kind of thing that The neural the natural language can like natural language processing community hopes to be able to do Without like lots of handwritten code like this, but for now This is well be interesting to see like what the winners turn out to have done But I haven't seen machine learning being used really to do to do this particularly well Perhaps the best approach is the ones which combine this kind of feature engineering along with some machine learning But I think this is a great example of effective feature engineering and This is a another USS student who did has done much the same thing got a similar kind of score But I used used her own different sort of rules Again this is gets you would get you a good leaderboard position with these as well So I thought that was interesting to see examples of some of our students entering a Competition and getting kind of top 20 ish results by you know basically just handwritten heuristics, and this is where for example computer vision was Six years ago still basically all the best approaches was a whole lot of like carefully handwritten heuristics often combined with some simple machine learning and So I think over time You know the field is kind of? Definitely trying to move towards automating much more of this and actually interestingly very interestingly in the safe driver Prediction competition was just finished One of the Netflix prize winners won this competition and he invented a new Algorithm for dealing with structured data which basically doesn't require any feature engineering at all so he came first place using nothing but five deep learning models and one gradient boosting machine And his his basic approach was very similar to What we've been learning in this class so far and what we'll be learning also tomorrow Which is using fully connected neural networks and we're and one hot encoding And specifically embedding which we'll learn about but he had a very clever technique Which was there was a lot of data in this competition which was unlabeled so in other words Where they didn't know whether that? Driver would go under claim or not or whatever so unlabeled data So when you've got some labeled and some unlabeled data we call that semi supervised learning and in real life Most learning is semi supervised learning like in real life normally you have some things that are labeled and some things that are unlabeled So this is kind of the most practically Useful kind of learning and then structured data is it's the most common kind of data that companies deal with day-to-day so the fact that this competition was a semi-supervised structured data competition made it incredibly practically useful and So what his technique for winning this was was to? Do data augmentation which those of you doing the deep learning course have learned about which is basically the idea like if you had Pictures you would like flip them horizontally or rotate them a bit data augmentation means creating new data examples Which are kind of slightly different Versions of ones you already have and the way he did it was for each row in the data he would like at random replace 15% of the variables with a different row So each row now would represent like a mix of like 80% 85% of the original row But 15% randomly selected from a different row and so this was a way of like randomly changing the data a little bit and then He used something called an autoencoder, which we will Probably won't study into a part two of the deep learning course But the basic idea of an autoencoder is your dependent variable is the same as your independent variable So in other words you try to predict your input which obviously is Trivial if you're allowed to like like you know the identity transform For example trivially predicts the input but the trick with an autoencoder is to have less activations in At least one of your layers than your input right so if your input was like a hundred dimensional vector And you put it through a 100 pi 10 Matrix to create 10 activations and then had to recreate the original hundred long vector from that Then you've basically come you had to have compressed it effectively and so it turns out that That kind of neural network You know it's forced to find Correlations and features and interesting relationships in the data even when it's not labeled so he used that Rather than doing any he didn't do any hand engineering. He just used an autoencoder So you know these are some interesting kind of directions that if you keep going with your machine learning studies You know particularly if you Do part two of the deep learning course next year? you'll learn about and You can kind of see how The feature engineering is going away, and this was just yeah an hour ago, so this is very recent news indeed But it's one of this is one of the most important breakthroughs I've seen in a long time Okay, so we were working through a a simple logistic regression trained with SGD for MNIST and Here's the summary of where we got to we have nearly built a module a Model module and a training loop from scratch and we were going to kind of try and finish that and after we finish that I'm then going to go through this entire notebook backwards right so having gone like top to bottom We're then going to go back through bottom to top, okay, so You know this was that little Hand written and end up module class we created We defined our loss We defined our learning rate, and we defined our optimizer. This is the thing that we're going to try and write by hand in a moment So that stuff That and that we're stealing with from pytorch, but that we've written ourselves and this we've written ourselves So the basic idea was we're going to go through some number of epochs, so let's go through one epoch Right and we're going to keep track of how much for each mini batch. What was the loss so that we can report it at the end? We're going to turn our training data loader into an iterator So that we can loop through it live through every mini batch and so now we can go and go ahead and say for tensor in The length of the data loader, and then we can call next to grab the next independent variables and the dependent variables From our data loader from that iterator, okay? So then remember we can then pass The X tensor into our model by calling the model as if it was a function But first of all we have to turn it into a variable Last week we were typing variable blah dot CUDA to turn it into a variable a shorthand for that is just the capital B That's a capital T for a tensor capital B for a fee for a variable. That's just a shortcut in fast AI Okay, so that returns our predictions And so the next thing we needed was to calculate our loss Because we can't calculate the derivatives of the loss if you haven't calculated loss So the loss takes the predictions and the actuals Okay, so the actuals again are the the Y tensor and again we have to turn that into a variable Now can anybody remind me what a variable is and why we would want to use a variable here? I think once you turn into variable then it tracks it so then you can do it backward on it So you can everyone sorry when you turn to variable it? It can track like its process of like you know as you add the function as the function start getting layered in each other You can track it and then we do backward on it back propagates and does the yeah, right? Right so a variable keeps Track of all of the steps to get computed And so there's actually a fantastic tutorial on the pie torch website so On the pipe torch website. There's a tutorial section And there's a tutorial there about autograd autograd is the name of the automatic differentiation package that comes with pie torch And it's a it's an implementation of automatic differentiation, and so the variable class is really the key the Key class here because that's the thing that makes turns a tensor into something where we can keep track of its gradients So basically here they show how to create a variable do an operation to a variable and Then you can go back and actually look at the grad function Which is the the function that it's keeping track of? Basically to calculate the gradient right so as we do more and more operations to this Very variable and the variables calculated from that variable it keeps keeping track of it so later on we can go Dot backward and then print dot grad and find out the gradient right and so you notice we never defined the gradient We just defined it as being x plus 2 2 Squared times 3 whatever and it can calculate the gradient Okay, so that's why we need to turn that into a variable so L is now a Variable containing the loss so it contains a single number for this mini-batch, which is the loss for this mini-batch But it's not just a number. It's a it's a number as a variable, so it's a number that knows how it was calculated right So we're going to append that loss to our array just so we can Get the average of it later basically And now we're going to calculate the gradient so L dot backward is the thing that says Calculate the gradient so remember when we call the the network It's actually calling our forward function So that's like cat go through it forward and then backward is like using the chain rule to calculate the gradients backwards Okay, and then this is the thing we're about to write which is update the weights based on the gradients and the learning rate, okay? Zero grad will explain when we write this out by hand Okay And so then at the end we can turn our validation data loader into an iterator and We can then go through its length grabbing each X and y out of that and asking for the score which we defined up here to be equal to Which thing did you predict which thing was actual and so check whether they're equal right and then the Main of that is going to be our accuracy, okay? Could you pass that over to Chen Shi? What's the advantage that you converted into iterator residents like use normal? Python loop or We're using a normal Python loop Yeah, so it's Dylan. This is a normal Python loop so the question really is like Compared to what right so like The alternative perhaps you're thinking it would be like we could use like a something like a list with an indexer, okay? So you know the problem there is that we want Was a few things I mean one key one is we want each time we grab a new mini-batch We want to be random. We want a different different shuffled thing so this you can actually kind of iterate from Forever you know you can loop through it as many times as you like so There's this kind of idea. It's called different things in different languages But a lot of languages are called like stream processing And it's this basic idea that rather than saying I want the third thing or the ninth thing It's just like I want the next thing right it's great for like network programming It's like grab the next thing from the network. It's great for UI programming it's like grab the next event whether somebody clicked a button it also turns out to be great for This kind of numeric programming. It's like I just want the next batch of data It means that the data like can be kind of arbitrarily long as we're just grabbing one piece at a time Yeah, so you know I mean and also I guess the short answer is because it's how pytorch works pytorch that's pytorch as data loaders are designed to be Called in this way and then so python has this concept of a generator which is like an Different type of generator. I wonder if this is gonna be a snake generator or a computer generator, okay a generator is a Way that you can create a function that as it says behaves like an iterator so black python has recognized that this stream processing approach to programming is like super handy and helpful and Supports it everywhere so basically anywhere that you use a for in loop anyway use a list comprehension Those things can always be generators or iterators so by programming this way we just get a lot of them flexibility I guess Is that sound about right Terrence you're the programming language expert did you? Do you want to grab the box so we can hear? So Terrence actually does programming languages for a living so we should ask him. Yeah, I mean the short answer is what you said You might say something about space But in this case that all that data has to be in memory anyway because we've got no doesn't have to be in memory So in fact most of the time we could pull a mini batch from something in fact most of the time with pytorch the mini batch Will be read from like separate images spread over your disk on demand so most of the time it's not in memory But in general you want to keep as little in memory as possible at a time and so the idea of stream processing Also is great because you can do compositions you can pipe the data to a different machine you can yeah Yeah, the composition is great You can grab the next thing from here and then send it off to the next dream which can then grab it and do something Else which you guys all recognize of course in the command line pipes and I'll redirection yes Okay, thanks Terrence The benefit of working with people that actually know what they're talking about All right, so let's now take that and get rid of the optimizer Okay, so the only thing that we're going to be left with is the negative log likelihood loss function which we could also replace actually we have a implementation of that from scratch that you're net wrote in the In the notebooks, so it's only one line of code as we learned earlier. You can do it with a single if statement, okay? So I don't know why I was so lazy as to include this So what we're going to do is we're going to again grab this module that we've written ourselves the logistic regression module We're going to have one epoch again. We're going to loop through each thing in an iterator again. We're going to grab our independent independent variable for the mini batch again Pass it into our network again Calculate the loss so this is all the same as before but now we're going to get rid of this optimizer dot step And we're going to do it by hand so the basic trick is As I mentioned we're not going to do the calculus by hand so we'll call L dot backward to calculate the gradients automatically And that's going to fill in our weight matrix, so do you remember when we created our? let's go back and look at the code for Here's that module we built so the weight matrix for the for the Linear layer weights we called L1w and for the bias we called L1b. All right, so they were the attributes we created so I've just put them into things called W and B just to save some typing basically so W was our weights B is our biases and So the weights remember the weights are a variable and to get the tensor out of the variable we have to use dot data Right so we want to update the actual tensor that's in this variable so we say weights dot data Minus equals so we want to go in the opposite direction to the gradient the gradient tells us which way is up We want to go down Whatever is currently in the gradients Times the learning rate so that is the formula for gradient descent right so as you can see it's it's like as As as easier thing as you can possibly imagine it's like literally update the weights to be equal to be equal to whatever They are now minus the great the gradients times the learning rate and do the same thing for the bias So anybody have any questions about that step in terms of like why we do it or how did you have a question? Do you want to grab that? That step, but when we do the next of deal The next year yes, yes So when we use the end of the loop how do you grab the next element? So this is going through each Each index in range of length so this is going 0 1 2 3 at the end of this loop It's going to print out the mean of the validation set go back to the start of the epoch at which point It's going to recreate a new a new iterator Okay, so basically behind the scenes in Python when you call it a On on this it basically tells it to like reset its state to create a new iterator And if you're interested in how that works the The code is all you know available for you to look at so we could look at like MD train DL is Is a fast AI dot data set dot model data loader, so we could like take a look at the code of that So we could take a look at the code of that And see exactly how it's being built right and so you can see here. Here's the next function Right which basically is Keeping track of how many times it's been through in this self dot I And here's the inner function which is the thing that gets pretty cold when you when you create a new iterator And you can see it's basically passing it off to something else Which is a type data loader, and then you can check out data loader if you're interested to see how that's implemented as well So the data loader that we wrote Basically uses multi-threading to allow it to have multiple of these going on at the same time It's actually a great. It's really simple. It's like it's only about a screen full of code So if you're interested in simple multi-threaded programming, it's a good thing to look at Okay now um oh yes Why have you wrapped this in for epoch in range one since that'll only run once? Because in real life we would normally be running multiple So like in this case because it's a linear model it actually basically trains to As good as it's going to get in one epoch, so if I type three here it actually It actually won't really improve after the first epoch much at all as you can see right But when we go back up to the top we're going to look at some slightly deeper and more interesting Versions which will take more epochs, so you know if I was turning this into a into a function You know I'd be going like you know death train model And one of the things you would pass in is like number of epochs Okay great So one thing to remember is that When you're you know creating these neural network layers then remember like This is just as part which is concerned. This is just a it's an end up module It could be a we could be using it as a layer. We could be using the function We could be using it as a neural net pytorch doesn't think of those as different things right so this could be a layer Inside some other network right? So how do gradients work so if you've got a layer which remember is just a bunch of we can think of it basically as its Activations right or some activations that get computed through some other nonlinear activation function or through some linear function and From that layer We it's very likely that we're then like let's say putting it through a matrix product right? to create some new layer and So each one of these so if we were to grab like One of these activations right is actually going to be Used to calculate every one of these outputs right and so if you want to calculate the The derivative you have to know how this weight matrix Impacts that output and that output and that output and that output right and then you have to add all of those together To find like the total impact of this you know across all of its outputs and So that's why in pytorch You have to tell it when to set the gradients to zero right because the idea is that you know you could be like having Lots of different loss functions or lots of different outputs in your next actor very set of activations or whatever all adding up Increasing or decreasing your gradients right so you basically have to say okay. This is a new calculation Reset okay, so here is where we do that right so before we do L dot backward we say Reset okay, so let's take our weights Let's take the gradients. Let's take the tensor that they point to and And then zero underscore does anybody remember from last week what underscore does as a suffix in pytorch? Yeah I forgot the language, but basically it changes it within the place right there the language is in place Yeah Exactly so it sounds like a minor technicality But it's super useful to remember every function pretty much has an underscore version suffix which does it in place Yeah, so normally zero returns a Tensor of zeros of a particular size so zero underscore means replace the contents of this with a bunch of zeros, okay? All right, so that's That's it right so that's like SGD from scratch And if I get rid of my menu bar we can officially say it fits within a screen Okay, so Of course we haven't got our definition of logistic regression here. That's another half the screen, but basically this there's not much to it. Yes, Vash So later on if we have to do this more the gradient is it because you might find like a wrong minima local minima Is that way so you have to kick it out, and that's what you have to do multiple times when the surfaces get more Why do you need multiple epochs is that your question well? I mean a simple way to answer that would be let's say our learning rate was tiny right Then It's just not going to get very far Right there's nothing that says going through one epoch is enough to get you all the way there So then it'd be like okay. Well. Let's increase our learning rate, and it's like yeah sure we'll increase our learning rate but who's to say that the highest learning rate that learn stably is is enough to Learn this as well as it can be learned and for most data sets for most architectures one epoch is very rarely enough to get you To the best result you can get to You know linear models are just They're very nicely behaved. You know so you can often use higher learning rates and learn them more quickly also they They don't you can't like generally get as good accuracy, so there's not as far to take them either So yeah doing one epoch is going to be the rarity all right, so let's go backwards So going backwards we're basically going to say all right. Let's not write Those two lines again and again again. Let's not write those three lines again and again and again Let's have somebody do that for us right so Right so that's like that's the only difference between that version and this version is rather than saying dot zero ourselves Rather than saying minus gradient times LR ourselves These are wrapped up for us, okay? There is another wrinkle here, which is this approach to updating The the weights is actually pretty inefficient It doesn't take advantage of momentum and curvature and so In the DL course we learned about how to do momentum from scratch as well, okay, so If we actually just use plain old SGD Then you'll see that this learns much slower, so now that I've typed just plain old SGD here This is now literally doing exactly the same thing As our slow version, so we have to increase the learning rate Okay there we go, so this this is now the same as the the one we wrote by hand So then all right Let's do a little bit more stuff automatically Let's not you know given that every time we train something We have to loop through epoch loop through batch do forward get the loss Zero the gradient do backward do a step of the optimizer. Let's put all that in a function Okay, and that function is called fit All right there it is Okay, so let's take a look at fit Fit go through each epoch go through each batch Do one step Keep track of the loss and at the end calculate the validation right and so then step So if you're interested in looking at this this stuff's all inside fast AI dot model and So here is step right Zero the gradients Calculate the loss remember pipe torch tends to call it criterion rather than loss right do backward And then there's something else we haven't learned here, but we do learn the deep learning course which is gradient So you can ignore that right so you can see now like all the stuff that we've learned when you look inside the actual frameworks That's the code you see okay? So that's what fit does and So then the next step would be like okay. Well this idea of like having some weights and a bias and doing a matrix product in addition Let's put that in a function This thing of doing the log soft max Let's put that in a function and then the very idea of like first doing this and then doing that This idea of like chaining functions together. Let's put that into a function and that finally gets us to that Okay, so sequential simply means do this function take the result send it to this function etc right And linear means create the weight matrix create the biases Okay So that's that's it right? So we can then you know as we started to talk about like turn this into a deep neural network By saying you know rather than sending this straight off into Ten activations, let's let's put it into say a hundred activations. We could pick whatever one number we like Put it through a value to make it nonlinear Put it through another linear layer another relu and then our final output with our final Activation function right and so this is now a deep network So we could fit that and This time now because it's like deeper I'm actually going to run a few more epochs that and you can see the accuracy Increasing right so if you try and increase the learning rate here. It's like 0.1 further it actually Starts to become unstable Now I'll show you a trick This is called learning rate annealing and the trick is this when you're Trying to fit to a function right you've been taking a few steps Step step step as you get close to the middle like get close to the bottom Your steps probably want to become smaller right otherwise what tends to happen is you start finding you're doing this Right and so you can actually see it here right it got 93 94 in a bit 94 6 94 8 like it's kind of starting to flatten out Right now that could be because it's kind of done as well as it can Or it could be that it's going to going backwards and forwards So what is a good idea is is later on in training is to decrease your learning rate and to take smaller steps Okay, that's called learning rate annealing, so there's a function in fast AI called set learning rates you can pass in your optimizer and your new learning rate and You know see if that helps right and very often it does About about an order of magnitude in the deep learning course We learn a much much better technique than this to do this all automatically and at a more granular level But if you're doing it by hand, you know like an order of magnitude at a time is what? people generally do So you'll see people in papers talk about learning rate schedules This is like an ending rate schedule so this schedule just a moment Erica I just come to earnest first has got us to 97 right and I tried Kind of going further, and we don't seem to be able to get much better than that so yeah So here we've got something where we can get 97 percent accuracy yes, Erica So it seems like you change a learning rate to something very small Ten times smaller than we started with so we had point one now. It's point. I won yep But that makes the whole model train really slow So I was wondering if you can make it so that it changes dynamically as it approaches Closer to the minima yeah pretty much yeah, so so that's some of the stuff We learn in the deep learning course is these more advanced approaches. Yeah, so So how it is different from using atom optimizer or something that's the kind of stuff we can do I mean you still need annealing as I say we do this kind of stuff in the deep learning course so for now We're just going to stick to standard SGD I Had a question about the data loading yeah, I know it's a fast AI function But could you go into a little bit detail of how it's creating batches how it's learning data and how it's making those decisions Sure It'd be good to ask that on Monday night, so we can talk about in detail in the deep learning class But let's let's do the quick version here so basically There's a really nice design in pytorch Where they basically say let's let's create a thing called a data set Right and a data set is basically something that looks like a list it has a length right And so that's like how many images are in the data set and it has the ability to Index into it like a list right so if you had like D equals data set You can do length D, and you can do D of some index right that's basically all the data set Is as far as pytorch is concerned and so you start with a data set so it's like okay? D three gives you the third image you know or whatever and So then the idea is that you can take a data set and you can pass that into a constructor for a data loader And That gives you something which is now iterable right so you can now say it a Dl, and that's something that you can call next on and What that now is going to do is? If when you do this you can choose to have shuffle on or shuffle off shuffle on means give me random mini-batch Shuffle off means go through it sequentially and so What the data loader does now when you say next is it basically assuming you said shuffle equals true is it's going to grab? You know if you've got a batch size of 64 64 random integers between zero and length and call this 64 times to get 64 different items and jam them together So fast AI uses the exact same Terminology and the exact same API we Just do some of the details differently so specifically particularly with computer vision you often want to do a lot of So much pre-processing data augmentation like flipping changing the colors a little bit rotating those turn out to be really computationally expensive Even just reading the JPEGs turns out to be computationally expensive So pie torch uses an approach where it fires off multiple processes to do that in parallel Where else the fast AI library instead does something called multi-threading? Which is a much can be a much faster way of doing it. Yes, you're net So An epoch is a real epoch in the sense that all of the elements So it's a shuffle at the beginning of the epoch something like that. Yeah. Yeah I mean not all libraries work the same way some do sampling with replacement some don't We actually the fast AI library Hands-off the shuffling off to the set to the actual pie torch version and I believe the pie torch version Yeah, actually shuffles and an epoch covers everything once I believe Okay now the thing is when you start to get these bigger networks Potentially you're getting quite a few parameters right, so I want to ask you to calculate how many parameters there are but let's let's remember here. We've got 28 by 28 input into a hundred output and then a hundred into a hundred and then a hundred into ten Alright, and then for each of those you've got weights and biases So we can actually Do this net dot parameters Returns a list where each element of the list is a matrix So actually a tensor of the parameters for that not just for that layer But if it's a layer with both weights and biases that would be two parameters, right? So basically returns us a list of all of the tensors containing the the parameters Num elements in pie torch tells you how how big that is right so if I run this Here is the number of Parameters in each layer so I've got 784 inputs and the first layer has a hundred outputs so therefore the first weight matrix is of size 78,400 Okay, and the first bias vector is of size 100 and then the next one is a hundred by a hundred Okay, and there's a hundred and then the next one is a hundred by ten and then there's my bias Okay, so there's the number of elements in each layer, and if I add them all up. It's nearly a hundred thousand Okay And so I'm possibly at risk of overfitting here right so We might want to think about using regularization So a really simple common approach to regularization in all of machine learning Is something called L2? Regularization and It's super important super handy you can use it with just about anything right and the basic idea Anyway so L2 regularization the basic idea is this normally would say our loss is Is equal to let's just do RMSE to keep things kind of simple. It's equal to our predictions minus our actuals You know squared and then we sum them up take the average Take the square root, okay so What if we then want to say you know what like if I've got lots and lots of parameters Don't use them unless they're really helping enough right like if you've got a million Parameters, and you only really needed 10 Parameters to be useful just use 10 right so how could we like tell the loss function to do that? And so basically what we want to say is hey if a parameter is zero That's no problem. It's like it doesn't exist at all so let's penalize a parameter For not being zero All right, so what would be a way we could measure that? How can we like calculate how unzero our parameters are? Can you pass that to Chen Chi please Ernest? You calculate the average of all the parameters. That's my first can't quite be the average Close yes, Taylor. Yes, you figured it out, okay? I think so I think if we like Assuming all of our data has been normalized standardized however you want to call it We want to check that they're like significantly different from zero right would that be not the data that the parameters? The parameter is rather would be significantly and the parameters don't have to be normalized or anything that is calculated right yes So it's a different from zero right I just Assuming that the data has been normalized so that we can compare them on the oh yeah, I've got it Yeah, right, and then those that are not significantly different from zero We can probably just drop and I think Chen Chi is going to tell us how to do that You just figured it out right the meaning of the absolute value could do that that would be called L1 Which is great so L1? would be the absolute Value of the weights average L2 is actually the sum square root sum of squares Yeah, yeah exactly so we just take this we can just we don't even have to square root So we just take the squares of the weights themselves, and then like we want to be able to say like okay How much do we want to penalize not being zero right because if we actually don't have that many parameters We don't want to regularize much at all if we've got heaps. We do want to regularize a lot right so then we put a a parameter Yeah, right except. I have a rule in my classes which is never to use Greek letters, so normally people use alpha I'm going to use a okay, so So this is some number which you often see something around kind of 1e neg 6 to 1e neg 4 Ish right Now We actually don't care about the loss When you think about it We don't actually care about the loss other than like maybe to print it out or we actually care about is the gradient of the loss Okay, so the gradient of that right is that right so There are two ways to do this we can actually modify our loss function to add in this square Penalty or we could modify that thing where we said weights equals weights minus gradient times learning rate to subtract that as well right right so to add that as well and These are roughly these are kind of basically equivalent, but they have different names This is called L2 regularization right this is called weight decay So in the neural network literature You know that version kind of With how it was first posed in the neural network literature. Whereas this other version is kind of How it was posed in the statistics literature, and yeah, you know they're they're equivalent As we talked about in the deep learning class it turns out. They're not exactly a prevalent Because when you have things like momentum and Adam it can behave differently and two weeks ago a researcher figured out a way to actually Do proper weight decay in modern optimizers and one of our fast AI students just implemented that in the fast AI library So fast AI is now the first Library to actually support this properly so anyway, so for now, let's do the the version which Pie torch calls weight decay, but actually it turns out based on this paper two weeks ago is actually L2 regularization It's not quite correct, but it's close enough so here. We can say weight decay is 1e neg 3 So this is going to set our constant our penalty multiplier a to 1e neg 3 and it's going to add that to the loss function Okay, and so let's make a copy of these cells Just so we can compare hope this actually works Okay, and we'll set this running okay, this is now optimizing Well except If you actually so I've made a mistake here, which is I didn't rerun This cell this is an important thing to kind of remember since I didn't run this rerun this cell Here when it created the optimizer and said net dot parameters it started with the parameters that I had already trained right? So I actually hadn't recreated my network Okay, so I actually need to go back and rerun this cell first to recreate the network Then go through and run this Okay, there we go, so let's see what happens So you might know some notice something kind of kind of counterintuitive here Which is that? That's our training error right now. You would expect our training error with regularization to be worse that Makes sense right because we're like we're penalizing parameters that specifically can make it better and yet Actually it started out better not worse So why could that be? So the reason that can happen is that if you have a function That looks like that Right it takes potentially a really long time to train or else if you have a function that kind of looks more like That it's going to train a lot more quickly And there are certain things that you can do which sometimes just like can take a function That's kind of horrible and make it less horrible and it's sometimes weight decay can actually Make your functions a little more nicely behaved, and that's actually happened here So like I just mentioned that to say like don't let that confuse you right like weight decay really does Penalize the training set and look so strictly speaking The final number we get to for the training set shouldn't end up being better, but it can train sometimes more quickly right Yes, can you pass it attention I Don't get it okay why making it faster like the time matters like the training time matters No, it's this is after one epoch right so after one epoch I Congratulations for saying I don't get it. That's like the best thing anybody can say you know so helpful This here was our training without weight decay Okay, and this here is our training with weight decay okay, so this is not related to time This is related to just an epoch Right after one epoch My claim was that you would expect the training set all other things being equal to have a worse loss with weight decay because we're penalizing it You know this has no penalty this has a penalty so the thing with a penalty should be worse, and I'm saying oh It's not that's weird right and so the reason it's not is Because in a single epoch it matters a lot as to whether you're trying to optimize something That's very bumpy or whether you're trying to optimize something. That's kind of nice and smooth If you're trying to optimize something that's really bumpy like imagine in some high dimensional space right You end up kind of rolling around through all these different tubes and tunnels and stuff You know or else if it's just smooth you just go boom Right, I meant it's like imagine a marble rolling down a hill where one of them you've got like It's a good Lombard Street in San Francisco. It's like backwards forwards backwards forwards It takes a long time to drive down the road right Where else you know if you kind of took a motorbike and just went straight over the top you just make boom right so So whether it's a kind of the shape of the loss function surface You know impacts or kind of defines how easy it is to optimize and therefore how? Far can it get in a single epoch and based on these results it would appear that weight decay here has made it this function easier to optimize so just to make sure it's The penalizing is making the optimizer more than likely to reach the global minimum Rather than no I wouldn't say that my claim actually is that at the end It's probably going to be less good on the training set and indeed this does look to be the case at the end after five epochs our Training set is now worse with weight decay now. That's what I would expect right I would expect like if you actually find like I never use the term global optimum because It's just not something we have any guarantees about we don't really care about we just care like where do we get to after a certain? number of epochs We hope that we found somewhere That's like a good solution and so by the time we get to like a good solution the training set with weight decay the loss is worse Because it's very right but on the validation set the loss is better Right because we penalized the training set in order to kind of try and create something that generalizes better So we've got more parameter You know the parameters that are kind of pointless are now zero and it generalizes better Right so so always saying is that it just got to a good point After one epoch is really always saying So is it always true? No, no If you're bit by it you mean does weight decay always make the function surface smoother No, it's not always true, but it's like it's worth remembering that If you're having trouble training a function adding a little bit of weight decay may may help What so by recognizing the parameters what it does is it's I mean, it's not it's not why we do it You know the reason why we do it is because we want to penalize things that aren't zero to say like Don't make this parameter a high number unless it's really helping the loss a lot right set it to zero if you can Because setting as many parameters to zero as possible means it's going to generalize better Right it's like the same as having a smaller Network, right so that's that's we do that's why we do it But it can change how it learns as well So let's okay this one moment, Erica So I just wanted to check how we actually went here so after the second epoch yeah, so you can see here It's really has helped right after the second epoch Before we got to 97 percent accuracy now. We're nearly up to about 98 percent accuracy Right and you can see that the loss was point oh eight versus point one three right so adding regularization Has allowed us to find a you know? 3% versus 2% so like a 50% better Solution yes, Erica, so there are two pieces to this right what is L2 regularization? And the weight decay no there's so my claim was that the same thing right so weight decay is the version if you just take the Derivative of L2 regularization you get weight decay So you can implement it either by changing the loss function with an with a squared loss Penalty or you can implement it by adding The weights themselves as part of the gradient, okay? Yeah, I was just going to finish the questions. Yes, okay pass that to the fish Can we use regularization convolutional here as well absolutely so a convolution layer just is weights so yeah Jeremy can you explain why you thought you needed weight decay in this particular problem? Not easily I mean other than to say it's something that I would always try you're overfitting thunder well yeah, I mean okay, so Even if I yeah, okay, that's a good point unit, so if if my training loss Was higher than my validation loss then I'm under fitting Right so there's definitely no point regularizing right if like that would always be a bad thing that would always mean you need like more parameters in your model In this case I'm I'm over fitting that doesn't necessarily mean regularization will help But it's certainly worth trying. Thank you, and that's a great point. There's one more question. Yes Tyler do you want to pass over there? So how do you choose the optimal number of epoch? You do my deep learning course It's a it's that's a long story and a lot of lots of you do it do it by here on Herosely or there any it's a bit of both. We just don't as I say we don't have time to cover Best practices in this class we're going to learn the kind of fundamentals. Yeah, okay, so let's take a Six minute break and come back at 1110 All right So something that we cover in great detail in the deep learning course But it's like really important to mention here is that is that the secret in my opinion to kind of modern machine learning techniques is to Massively over parameter eyes the solution to your problem right like as we've done here You know we've got like a hundred thousand weights when we only had a small number of 28 by 28 images And then use regularization, okay, it's like the direct opposite of how nearly all Statistics and learning was done for decades before and still most kind of like Senior lecturers at most universities in most areas of have this background where they've learned the correct way to build a model is to Like have as few parameters as possible Right and so hopefully we've learned two things so far. You know one is we can build Very accurate models even when they have lots and lots of parameters like a random forest has a lot of parameters and You know this here deep network has a lot of parameters, and they can be accurate right? And we can do that by either using bagging or by using regularization okay and regularization in neural nets means either weight decay also known as kind of fail to regularization or Drop out which we won't worry too much about here so Like it's a It's a very different way of thinking about building useful models and like I just wanted to kind of warn you that once you leave this classroom Like even possibly when you go to the next faculty members talk like there'll be people at USF as well who? Entirely trained in the world of like Models with small numbers of parameters you know your next boss is very likely to have been trained in the world of like models with small numbers of parameters The idea that they are somehow More pure or easier or better or more interpretable or whatever I? I am convinced that that is not true probably not ever true certainly very rarely true and that actually Models with lots of parameters can be extremely interpretable as we learned from our whole lesson of random forest interpretation You can use most of the same techniques with neural nets, but with neural nets are even easier right remember how we did feature importance by Randomizing a column to see how it changes in that column would impact the output Well, that's just like a kind of dumb way of calculating its gradient How much does varying this import change the output with a neural net we can actually calculate its gradient? Right so with pi torch you could actually say what's the gradient of the output with respect to this column? right You can do the same kind of thing to do partial dependence plot with a neural net And you know I'll mention for those of you interested in making a real impact Nobody's written Basically any of these things for neural nets right so that that that whole area Needs like libraries to be written blog posts to be written You know some papers have been written But only in very narrow domains like computer vision as far as I know nobody's written the paper saying here's how to do structured data Neural networks you know interpretation methods So it's a really exciting big area So What we're going to do though is we're going to start with applying this With a simple linear model This is mildly terrifying for me because we're going to do NLP and our NLP Faculty expert is in the room so David just yell at me if I screw this up too badly And so NLP refers to you know any any kind of modeling where we're working with with natural language text All right, and it interestingly enough We're going to look at a situation where a Linear model is pretty close to the state of the art for solving a particular problem it's actually something where I actually surpassed the state of the art in this using a recurrent neural network a few weeks ago But this is actually going to show you pretty close to the state of art with with a linear model We're going to be working with the IMDb IMDb data set so this is a data set of movie reviews you can download it by following these steps and Once you download it you'll see that you've got a train and a test directory and In your train directory you'll see there's a negative and a positive directory and in your positive Directory you'll see there's a bunch of text files And here's an example of a text file So somehow we've managed to pick out a story of a man who has unnatural feelings for a pig as our first choice That wasn't intentional, but it'll be fine So We're going to look at these movie reviews And for each one we're going to look to see whether they were positive or negative So they've been put into one of these folders They were downloaded from from IMDb the movie database and review site The ones that were strongly positive went in positive strongly negative went negative and the rest they didn't label at all So these are only highly polarized reviews so in this case, you know We have an insane violent mob which unfortunately is too absurd Too off-putting those in the area be turned off so the label for this was a zero which is Negative okay, so this is a negative review so In the first AI library, there's lots of little functions and classes to help with Most kinds of domains that you do machine learning on the NLP one of the simple things we have is texts from folders That's just going to go ahead and go through and find all of the folders in here with these names and create a Labeled data set and you know don't let these things Ever stop you from understanding what's going on behind the scenes Right we can grab its source code and as you can see it's tiny, you know, it's like five lines Okay, so I don't like to write these things out in full, you know But hide them behind at all functions so you can reuse them But basically it's just going to go through each directory and then within that so I go through Yeah, go through each directory And then go through each file in that directory and then stick that into This array of texts and figure out what folder it's in and stick that into the array of labels. Okay, so that's how we Basically end up with something where we have an array of the reviews and an array of the labels Okay, so that's our data. So our job will be to take that and to predict that Okay, and the way we're going to do it is we're going to throw away Like all of the interesting stuff about language, which is the order in which the words are in right now This is very often not a good idea But in this particular case, it's going to turn out to work like not too badly So let me show what I mean by like throwing away the order of the words like normally the order of the words Matters a lot if you've got a not Before something then that not refers to that thing right so but the thing is when in this case We're trying to predict whether something's positive or negative if you see the word absurd appear a lot Right then maybe that's a sign that this isn't very good So you know cryptic maybe that's a sign that it's not very good so the idea is that we're going to turn it into something called a Term document matrix where for each document I each review We're just going to create a list of what words are in it rather than what order they're in so let me give an example Can you see this okay? Okay So here are four Movie reviews that I made up This movie is good the movie is good. They're both positive this movie is bad the movie is bad They're both negative right so I'm going to turn this into a term document matrix So the first thing I need to do is create something called a vocabulary a vocabulary is a list of all the unique words that appear Okay, so here's my vocabulary this movie is good the bad. That's all the words Okay, and so now I'm going to take each one of my movie reviews and turn it into a Vector of which words appear and how often do they appear right and in this case none of my words appear twice so This movie is good has those four words in it Where else this movie is bad has? those four words in it Okay, so this Is called a term document matrix Right and this representation we call a bag of words representation, right? So this here is a bag of words representation of the view of the review it doesn't contain the order of the text anymore It's just a bag of the words what words are in it it contains bad is Movie this okay, so that's the first thing we're going to do is we're going to turn it into a bag of words representation And the reason that this is convenient for linear models is that this is a nice? Rectangular matrix that we can like do math on Okay, and specifically we can do a logistic regression, and that's what we're going to do is we're going to get to a point We do a logistic regression Before we get there though. We're going to do something else which is called naive base Okay so Sk learn Has something which will create a term document matrix for us. It's called count vectorizer. Okay, so we'll just use it now in NLP You have to turn your text into a list of words And that's called tokenization Okay, and that's actually non-trivial because like if this was actually this movie is good Dot right or if it was this movie Is good like how do you deal with like that? Punctuation or perhaps more interestingly what if it was this movie isn't good right, so How you turn a piece of text into a list of tokens is called tokenization, right? And so a good tokenizer would turn this movie isn't good Into this this space Quote movie space is space and good space right so you can see in this version here If I now split this on spaces every token is either a single piece of punctuation Or like this suffix and is considered like a word right that's kind of like how we would probably want to tokenize That piece of text because you wouldn't want good full stop To be like an object right because it does there's no concept of good full stop right or Double quote movie is not like an object so Tokenization is something we hand off to a tokenizer Fast AI has a tokenizer in it that we can use So this is how we create our term document matrix with a tokenizer Sklearn has a pretty standard API which is nice. I'm sure you've seen it a few times now before So once we've built some kind of model we can kind of think of this as a model Just ish this is just defining what it's going to do we can call fit transform to To do that right so in this case fit transform is going to create the vocabulary Okay, and create the term document matrix based on the training set Transform is a little bit different that says use the Previously fitted model which in this case means use the previously created vocabulary We wouldn't want the validation set and the training set to have you know the words in different orders in the matrices Right because then they'd like to have different meanings, so this is here saying use the same vocabulary To create a bag of words for the validation set could you pass that back, please? What if the violation set has different set of words other than training yeah, that's a great question so generally most Of these kind of vocab creating approaches will have a special token for unknown Sometimes you can you'll also say like hey if a word appears less than three times call it unknown But otherwise it's like yeah if you see something you haven't seen before call it unknown So that would just become a column in the bag of words is is unknown Good question all right, so when we create this term Document matrix of the training set we have 25,000 rows because there are 25,000 movie reviews And there are 75,000 132 columns What does that represent what does that mean there are seven hundred and thirty five thousand one hundred thirty two what can you pass that to? Devesh Just a moment to compare such a mesh All vocabulary yeah go on what do you mean? So like the the number of words a union of a number of words that the number of unique words yeah exactly good, okay? now most documents Don't have most of these 75,000 Words right so we don't want to actually store that As a normal array in memory because it's going to be very wasteful So instead we store it as a sparse matrix Okay, and what a sparse matrix does is it just stores it as? something that says Whereabouts of the non-zeros right so it says like okay term number so document number one word number four Appears and it has four of them you know document one term number hundred and twenty four three Has that that appears and it's a one right and so forth that's basically How it's stored there's actually a number of different ways of storing and if you do Rachel's? Computational linear algebra course you'll learn about the different types and why you choose them and how to convert and so forth But they're all kind of something like this right and you don't really on the whole have to worry about the details The important thing to know is it's it's efficient Okay, and so we could grab the first review Right and that gives us 75,000 long sparse One long one row long matrix okay with 93 stored elements so in other words 93 of those words are actually used in the first document, okay? We can have a look at the vocabulary by saying vectorizer dot get feature feature names that gives us the vocab And so here's an example of a few of the elements of get feature names I Didn't intentionally pick the one that had Ozzy, but you know that's the important words obviously I Haven't used the tokenizer here. I'm just bidding on space so this isn't quite the same as what the Vectorizer did but to simplify things Let's grab a set of all the lower-cased words By making it a set we make them unique, so this is Roughly the list of words that would appear right and that length is 91 Which is pretty similar to 93 and just the difference will be that I didn't use a real tokenizer. Yeah, right So that's basically all that's been done there. It's kind of created this unique list of words and mapped them We could check by calling vectorizer dot vocabulary underscore to find the idea of a particular word So this is like the reverse map of this one right so this is like integer to word Here is word to integer and so we saw absurd appear twice in the first document So let's check train term doc zero comma one two nine seven there it is is two right or else Unfortunately Ozzy didn't appear in the unnatural relationship with a pig movie So zero comma five thousand is zero okay, so That's that's our term document matrix Yes, so does it care about the Relative relationship between the words As in the ordering of the words no we've thrown away the orderings. That's why it's a bag of words and I'm not claiming that this is like Necessarily a good idea what I will say is that like the vast majority of NLP work That's been done over the last few decades generally uses this representation because we didn't really know much better Nowadays increasingly we're using recurrent neural networks instead which we'll learn about in our last deep learning lesson of part one But sometimes this representation works pretty well, and it's actually going to work pretty well in this case Okay, so and in fact it almost like back when I was at fast mail my email company a Lot of the spam filtering we did used this next technique naive Bayes Which is as a bag of words approach just kind of like you know if you're getting a lot of Email containing the word Viagra, and it's always been a spam And you never get email from your friends talking about Viagra Then it's very likely something that says Viagra regardless of the detail of the language is probably from a spammer All right, so that's the basic theory about like classification using a term document matrix, okay, so let's talk about naive Bayes And here's the basic idea we're going to start with our term document matrix right and These first two is our corpus of positive reviews These next two is our corpus of negative reviews, and so here's our whole corpus of all reviews right so what I could do is now to create a probability I'm going to call these as we tend to call these more generically features rather than words right this is a feature movie is a Feature is as a feature right? So it's kind of more now like machine learning language a column is a feature We'll call those we often call those F in naive base, so we can basically say the probability That you would see the word this Given that the class is one given that it's a positive review is just the average of How often do you see this in the positive reviews? right Now we've got to be a bit careful though because If you never ever see a particular word in a particular class Right so if I've never received an email from a friend that said Viagra, right? That doesn't actually mean the probability of us of a friend sending me sending me an email about Viagra is zero It's not really zero right I Hope I don't get an email you know from Terrence tomorrow saying like Jeremy you probably could use this you know advertisement for Viagra, but you know it could happen and you know You know I'm sure it'd be in my best interest Yeah, so so what we do is we say actually what we've seen so far is not the full sample of everything that could happen It's like a sample of what's happened so far So let's assume that the next email you get actually does mention Viagra and every other possible word right so basically We're going to add a row of ones Okay, so that's like the email that contains every possible word so that way nothing's ever infinitely unlikely Okay, so I take the average of All of the Times that this appears in my positive corpus plus the ones okay, so that's like the the probability that Feature equals this appears in a document given that class equals one And so not surprisingly here's the same thing For probability that this feature this appears given classical zero right same calculation except for the zero Rows and obviously these are the same because this appears Twice in the positives sorry once in the positives and once in the negatives, okay? Let's just put this back to what it was all Right So we can do that for every feature for every class Right so our trick now is to basically use a base rule to kind of fill this in so What we want is the probability that Given that I've got this particular document, so somebody sent me this particular email, or I have this particular IMDb review What's the probability that its class is? equal to I Don't know positive right so for this particular movie review. What's the probability that its class is positive? Right and so we can say well. That's equal to the probability That we got this particular movie review Given that its class is positive Multiplied by the probability that any movie reviews class is positive divided by the probability of getting this particular movie review All right, that's just basis rule okay, and so we can calculate All of those things, but actually what we really want to know is Is it more likely that this is class zero or class one right so what if we actually took? Probability that's class one and divided by probability that it's class zero What if we did that right and so then we could say like okay if this number is bigger than one Then it's more likely to be class one if it's smaller than one it's more likely to be class zero Right so in that case we could just divide This whole thing Right by the same version for class zero right which is the same as multiplying it by the reciprocal and So the nice thing is now that's going to put a probability D on top here Which we can get rid of right and a probability of getting the data given class zero down here And the probability of getting class Zero here right and so if we basically what that means is we want to calculate The probability that we would get this particular document given that the class is one Times the probability that the class is one Divided by the probability of getting this particular document given the class is two zero Times the probability that the class is zero So the probability that the class is one is Just equal to the average of the labels Right probability that the class is zero is just one minus that right so So there are those two numbers right I've got an equal amount of both so it's both point five What is the probability of getting this document given that the class is one can anybody tell me how I would calculate that Can somebody pass that that please Look at all the documents which have class equal to one aha and one divided by that will give you So remember, it's though. It's going to be for a particular document so for example. We'd be saying like what's the probability that This review is positive right so what so you're on the right track But what we have to have to do is we're going to have to say let's just look at the words it has and Then multiply the probabilities together For class equals one right so the probability that a class one review has this is Two-thirds the probability it has movie is one is is one and good is one So the probability it has all of them is all of those multiplied together Kinda and the kinder Tyler why is it not really can you pass that to Tyler? So glad you look horrified and skeptical Word choice is not independent. Thank you So nobody can call Tyler naive Because the reason this is naive Bayes is because this is what happens if you take Bayes's theorems in a naive way And Tyler is not naive anything bad right so naive Bayes says let's assume that if you have This movie is bloody stupid I hate it but the probability of hate is independent of the probability of bloody is it independent of the probability of stupid right? Which is definitely not true right and so naive Bayes ain't actually very good But I'm kind of teaching it to you because it's going to turn out to be a convenient Piece for something we're about to learn later It's okay right. I mean it's it's it's I would never I would never choose it like I don't think it's better than any other Technique that's equally fast and equally easy But you know it's the thing you can do and it's certainly going to be a useful foundation so So here is our calculation right of the probability that this document is That we get this particular document assuming. It's a positive review Here's the probability given. It's a negative and here's the ratio and this ratio is above one So we're going to say I think that this is probably a positive review, okay, so that's the excel version and So you can tell that I let your net touch this because it's got latech in it. We've got actual math so So here is the here is the same thing the log count ratio for each feature F each word F and so here it is Written out as Python okay, so our independent variable is our term document matrix Our dependent variable is just the labels of the Y So using numpy This is going to grab the rows Where the dependent variable is one? Okay, and so then we can sum them over the rows to get the total word count For that feature across all the documents right plus one right because that's the email Terrence It's totally going to send me something about Viagra today. I can tell that's that's that yeah, okay So I'll do the same thing for the negative reviews right and then of course it's nicer to take the log Right because if we take the log then we can add things together rather than multiply them together And once you like multiply enough of these things together It's going to get kind of so close to zero that you'll probably run out of plotting point right so we take the log of the ratios and Then we can as I say we then multiply that or in log we subtract that from the sorry add that to the ratio of the class the whole class probabilities alright So in order to say for each document multiply the Bayes probabilities by the accounts we can just use matrix multiply okay, and then to add on the the Log of the class ratios we can just use plus B And so we end up with something that looks a lot like our Logistic regression right, but we're not learning anything right not in kind of a SGD point of view We're just we're calculating it using this theoretical model okay And so as I said we can then compare that as to whether it's bigger or smaller than zero Not one anymore because we're now in log space right and And then we can compare that to the mean and we say okay. That's 80% accurate 81% accurate Right so naive Bayes You know is not is not nothing it gave us something okay? It turns out that This version where we're actually looking at how often a word appears Like absurd appeared twice it turns out at least for this problem and quite often It doesn't matter whether absurd appeared twice or once all that matters is that it appeared So what what people tend to try doing is to say take the term? Document matrix and go dot sign dot sign Replaces anything positive with one and anything negative with negative one. We don't have any negative counts obviously so this Binarizes it so it says it's I don't care that you saw absurd twice. I just care that you saw it Right so if we do exactly the same thing with the binarized version Then you get a better result, okay? Okay now this is the difference between theory and practice right in theory I'm naive Bayes sounds okay, but it's it's naive unlike Tyler. It's naive right so what Tyler would probably do would instead say rather than assuming that I Should use these coefficients are why don't we learn them so it's unreasonable Tyler. Yeah, okay? So let's learn them so we can you know we can totally learn them so let's create a logistic regression Right and let's fit Some coefficients and that's going to literally give us something with exactly the same functional form that we had before But now rather than using a theoretical R and a theoretical B. We're going to calculate the two things based on logistic regression and That's better Okay, so So it's kind of like yeah, why? Why do something based on some theoretical model because theoretical models are never? Going to be as accurate pretty much as a data driven model right because theoretical models Unless you're dealing with some I don't know like physics thing or something where you're like okay? This is actually how the world works there really is no I don't know we're working in a vacuum And this is the exact gravity and blah blah blah right but most of the real world This is how things are like it's better to learn your coefficients and calculate them. Yes, you know Jeremy was this dual equal through Hoping you'd ignore not notice, but you saw it basically in this case our Term document matrix is much wider than it is tall There is a reformulation Of mathematically basically almost a mathematically equivalent reformulation of logistic regression that happens to be a lot faster When it's wider than it is tall so the short answer is if you don't put that here Anytime it's wider than it is tall put dual equals true, and it will run this runs in like two seconds If you don't have it here, it'll take a few minutes So like in math there's this kind of concept of dual versions of problems which are kind of like Equivalent versions that sometimes work better for certain situations Okay Here is so here is the binarized version right And it's it's about the same right so you can see I've fitted it with the the sign of the doc of the doc term doc matrix and predicted it with this right Now the thing is that this is going to be a Coefficient for every term There was about 75,000 terms in our vocabulary And that seems like a lot of coefficients given that we've only got 25,000 reviews, so maybe we should try regularizing this So we can use regularization built into SK learns logistic regression plus which is c is the parameter that they use A smaller this is slightly weird a smaller parameter is more regularization, right? So that's why I used one eight to basically turn off regularization here, so if I turn on regularization set it to point one Then now it's 88 percent okay, which makes sense You know you wouldn't you would think like 75,000 parameters for 25,000 documents you know it's likely to overfit indeed it did overfit So this is adding L2 regularization to avoid overfitting I Mentioned earlier that as well as L2 which is looking at the weight squared. There's also L1 Which is looking at just the absolute value of the weights right I Was Kind of pretty sloppy in my wording before I said that L2 tries to make things zero That's kind of true, but if you've got two things that are highly correlated Then L2 regularization will like move them both down together It won't make one of them zero and one of the non zero right so L1 regularization Actually has the property that it'll try to make as many things zero as possible Whereas L2 regularization has a property that tends to try to make kind of everything smaller We actually don't care about that difference in in Really any modern machine learning because we very rarely try to directly interpret the coefficients We try to understand our models through Interrogation using the kind of techniques that we've learned The reason that we would care about L1 versus L2 is simply like which one ends up with a better error on the validation set Okay, and you can try both With SK learns logistic regression L2 actually turns out to be a lot faster because you can't use dual equals true unless you have L2 So you know and L2 is the default so I didn't really worry too much about that difference here So you can see here if we use Regularization and binarized we actually do pretty well, okay So Um yes, can you pass that back to W please? Before we learned about elastic net right like combining L1 and L2 yeah, yeah, yeah, you can do that, but I mean It's like you know with with deeper models Yeah, I've never seen anybody find that useful Okay So the last thing I mentioned is That you can when you do your count vectorizer Where that was when you do your count vectorizer you can also ask for n grams right by default we get Unigrams that is single words, but if we if we say n gram range equals 1 comma 3 that's also going to give us By grams and trigrams by which I mean if I now say okay, let's go ahead and do the count vectorizer I get feature names now my vocabulary includes a bigram Right by vast by vengeance and a trigram by vengeance full stop by Vera miles right So this is now doing the same thing but after tokenizing It's not just grabbing each word and saying that's part of our vocabulary But each two words next to each other and each three words next to each other and this ten this turns out to be like Super helpful in like taking advantage of bag of word approaches because we now can see like the difference between like You know not good versus not bad Versus not terrible right Or even like double quote good double quote which is probably going to be sarcastic, right? So using trigram features Actually is going to turn out to make both naive bays and logistic regression Quite a lot better. It really takes us quite a lot further and makes them quite useful I have a question about the Tokenizers so you are saying some max features. So how are these? By grams and trigrams selected right so since I'm using a linear model I Didn't want to create too many features. I mean it actually worked fine even without max features. I think I had something like I Can't remember 70 million coefficients. It still worked right, but just there's no need to have 70 million coefficients So if you say max features equals 800,000 The count vectorizer will sort the vocabulary by how often everything appears whether it be unigram by gram Trigram and it will cut it off After the first 800,000 most common n grams n gram is just the generic word for unigram by gram and trigram so that's why the train term doc dot shape is now 25,000 by 800,000 and like if you're not sure what number this should be I Just picked something that was really big and you know didn't didn't worry about it too much And it seemed to be fine like it's not terribly sensitive All right, okay, well that's we're out of time so what we're going to see Next week and by the way, you know we could have Replaced this logistic regression with our pytorch version and next week We'll actually see something in the fast AI library that does exactly that But also what we'll see next week. Sorry next week tomorrow is How to combine logistic regression and naive Bayes together to get something that's better than either and then we'll learn how to move from there to create a A deeper neural network to get pretty much state-of-the-art result for structured learning alright, so we'll see you then", "segments": [{"id": 0, "seek": 0, "start": 0.52, "end": 5.5200000000000005, "text": " Well welcome back to machine learning one of the most exciting things this week", "tokens": [1042, 2928, 646, 281, 3479, 2539, 472, 295, 264, 881, 4670, 721, 341, 1243], "temperature": 0.0, "avg_logprob": -0.26474604399307916, "compression_ratio": 1.7048458149779735, "no_speech_prob": 0.012045512907207012}, {"id": 1, "seek": 0, "start": 7.16, "end": 14.8, "text": " Almost certainly the most exciting thing this week is that fast AI is now on pip so you can pip install fast AI", "tokens": [12627, 3297, 264, 881, 4670, 551, 341, 1243, 307, 300, 2370, 7318, 307, 586, 322, 8489, 370, 291, 393, 8489, 3625, 2370, 7318], "temperature": 0.0, "avg_logprob": -0.26474604399307916, "compression_ratio": 1.7048458149779735, "no_speech_prob": 0.012045512907207012}, {"id": 2, "seek": 0, "start": 15.92, "end": 17.12, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.26474604399307916, "compression_ratio": 1.7048458149779735, "no_speech_prob": 0.012045512907207012}, {"id": 3, "seek": 0, "start": 17.12, "end": 21.2, "text": " So thank you to Prince and to Karem for making that happen", "tokens": [407, 1309, 291, 281, 9821, 293, 281, 591, 19183, 337, 1455, 300, 1051], "temperature": 0.0, "avg_logprob": -0.26474604399307916, "compression_ratio": 1.7048458149779735, "no_speech_prob": 0.012045512907207012}, {"id": 4, "seek": 0, "start": 22.04, "end": 28.32, "text": " To usf students who had never published a pip package before and this is one of the harder ones to publish because it's got a lot of", "tokens": [1407, 505, 69, 1731, 567, 632, 1128, 6572, 257, 8489, 7372, 949, 293, 341, 307, 472, 295, 264, 6081, 2306, 281, 11374, 570, 309, 311, 658, 257, 688, 295], "temperature": 0.0, "avg_logprob": -0.26474604399307916, "compression_ratio": 1.7048458149779735, "no_speech_prob": 0.012045512907207012}, {"id": 5, "seek": 2832, "start": 28.32, "end": 30.32, "text": " dependencies", "tokens": [36606], "temperature": 0.0, "avg_logprob": -0.1944639285405477, "compression_ratio": 1.6007905138339922, "no_speech_prob": 1.2218397387186997e-05}, {"id": 6, "seek": 2832, "start": 31.0, "end": 36.78, "text": " So it's you know probably still easiest just to do the conda end of update thing", "tokens": [407, 309, 311, 291, 458, 1391, 920, 12889, 445, 281, 360, 264, 2224, 64, 917, 295, 5623, 551], "temperature": 0.0, "avg_logprob": -0.1944639285405477, "compression_ratio": 1.6007905138339922, "no_speech_prob": 1.2218397387186997e-05}, {"id": 7, "seek": 2832, "start": 36.78, "end": 42.92, "text": " But a couple of places that it would be handy instead to pip install fast AI would be well obviously if you're working", "tokens": [583, 257, 1916, 295, 3190, 300, 309, 576, 312, 13239, 2602, 281, 8489, 3625, 2370, 7318, 576, 312, 731, 2745, 498, 291, 434, 1364], "temperature": 0.0, "avg_logprob": -0.1944639285405477, "compression_ratio": 1.6007905138339922, "no_speech_prob": 1.2218397387186997e-05}, {"id": 8, "seek": 2832, "start": 44.64, "end": 49.7, "text": " Outside of the the repo and the notebooks then this gives you access to fast AI everywhere", "tokens": [28218, 295, 264, 264, 49040, 293, 264, 43782, 550, 341, 2709, 291, 2105, 281, 2370, 7318, 5315], "temperature": 0.0, "avg_logprob": -0.1944639285405477, "compression_ratio": 1.6007905138339922, "no_speech_prob": 1.2218397387186997e-05}, {"id": 9, "seek": 2832, "start": 51.32, "end": 56.0, "text": " Also, I believe they submitted a pull request to Kaggle to try and get it added to the Kaggle kernels", "tokens": [2743, 11, 286, 1697, 436, 14405, 257, 2235, 5308, 281, 48751, 22631, 281, 853, 293, 483, 309, 3869, 281, 264, 48751, 22631, 23434, 1625], "temperature": 0.0, "avg_logprob": -0.1944639285405477, "compression_ratio": 1.6007905138339922, "no_speech_prob": 1.2218397387186997e-05}, {"id": 10, "seek": 5600, "start": 56.0, "end": 58.36, "text": " So hopefully you'll be able to use it on capital kernels soon", "tokens": [407, 4696, 291, 603, 312, 1075, 281, 764, 309, 322, 4238, 23434, 1625, 2321], "temperature": 0.0, "avg_logprob": -0.19609224796295166, "compression_ratio": 1.5947136563876652, "no_speech_prob": 6.438964192057028e-06}, {"id": 11, "seek": 5600, "start": 59.6, "end": 64.18, "text": " And yeah, you can use it at your work or whatever else", "tokens": [400, 1338, 11, 291, 393, 764, 309, 412, 428, 589, 420, 2035, 1646], "temperature": 0.0, "avg_logprob": -0.19609224796295166, "compression_ratio": 1.5947136563876652, "no_speech_prob": 6.438964192057028e-06}, {"id": 12, "seek": 5600, "start": 65.32, "end": 70.44, "text": " So that's that's exciting. I mean I'm not going to say it's like officially released yet", "tokens": [407, 300, 311, 300, 311, 4670, 13, 286, 914, 286, 478, 406, 516, 281, 584, 309, 311, 411, 12053, 4736, 1939], "temperature": 0.0, "avg_logprob": -0.19609224796295166, "compression_ratio": 1.5947136563876652, "no_speech_prob": 6.438964192057028e-06}, {"id": 13, "seek": 5600, "start": 70.44, "end": 77.68, "text": " You know it's still very early obviously and we're still you're helping add documentation and all that kind of stuff, but", "tokens": [509, 458, 309, 311, 920, 588, 2440, 2745, 293, 321, 434, 920, 291, 434, 4315, 909, 14333, 293, 439, 300, 733, 295, 1507, 11, 457], "temperature": 0.0, "avg_logprob": -0.19609224796295166, "compression_ratio": 1.5947136563876652, "no_speech_prob": 6.438964192057028e-06}, {"id": 14, "seek": 5600, "start": 78.24000000000001, "end": 80.24000000000001, "text": " It's great that that's now there", "tokens": [467, 311, 869, 300, 300, 311, 586, 456], "temperature": 0.0, "avg_logprob": -0.19609224796295166, "compression_ratio": 1.5947136563876652, "no_speech_prob": 6.438964192057028e-06}, {"id": 15, "seek": 5600, "start": 81.4, "end": 82.64, "text": " a", "tokens": [257], "temperature": 0.0, "avg_logprob": -0.19609224796295166, "compression_ratio": 1.5947136563876652, "no_speech_prob": 6.438964192057028e-06}, {"id": 16, "seek": 8264, "start": 82.64, "end": 89.26, "text": " Couple of cool kernels from usf students this week thought of highlight two that were both from the", "tokens": [38266, 295, 1627, 23434, 1625, 490, 505, 69, 1731, 341, 1243, 1194, 295, 5078, 732, 300, 645, 1293, 490, 264], "temperature": 0.0, "avg_logprob": -0.29685866314431897, "compression_ratio": 1.5549738219895288, "no_speech_prob": 2.4060857413132908e-06}, {"id": 17, "seek": 8264, "start": 90.0, "end": 92.0, "text": " text normalization", "tokens": [2487, 2710, 2144], "temperature": 0.0, "avg_logprob": -0.29685866314431897, "compression_ratio": 1.5549738219895288, "no_speech_prob": 2.4060857413132908e-06}, {"id": 18, "seek": 8264, "start": 92.4, "end": 94.48, "text": " competition which was about", "tokens": [6211, 597, 390, 466], "temperature": 0.0, "avg_logprob": -0.29685866314431897, "compression_ratio": 1.5549738219895288, "no_speech_prob": 2.4060857413132908e-06}, {"id": 19, "seek": 8264, "start": 95.92, "end": 98.18, "text": " Trying to take text which was", "tokens": [20180, 281, 747, 2487, 597, 390], "temperature": 0.0, "avg_logprob": -0.29685866314431897, "compression_ratio": 1.5549738219895288, "no_speech_prob": 2.4060857413132908e-06}, {"id": 20, "seek": 8264, "start": 101.88, "end": 107.52, "text": " Written out you know written a standard English text they also had one for Russian and you're trying to kind of identify", "tokens": [10159, 2987, 484, 291, 458, 3720, 257, 3832, 3669, 2487, 436, 611, 632, 472, 337, 7220, 293, 291, 434, 1382, 281, 733, 295, 5876], "temperature": 0.0, "avg_logprob": -0.29685866314431897, "compression_ratio": 1.5549738219895288, "no_speech_prob": 2.4060857413132908e-06}, {"id": 21, "seek": 10752, "start": 107.52, "end": 112.8, "text": " Things that could be like a first second third and say like that's a cardinal number", "tokens": [9514, 300, 727, 312, 411, 257, 700, 1150, 2636, 293, 584, 411, 300, 311, 257, 2920, 2071, 1230], "temperature": 0.0, "avg_logprob": -0.21369904453314623, "compression_ratio": 1.6046511627906976, "no_speech_prob": 7.527852631028509e-06}, {"id": 22, "seek": 10752, "start": 112.8, "end": 117.84, "text": " or this is a phone number or whatever and I did a quick little bit of searching and I saw that", "tokens": [420, 341, 307, 257, 2593, 1230, 420, 2035, 293, 286, 630, 257, 1702, 707, 857, 295, 10808, 293, 286, 1866, 300], "temperature": 0.0, "avg_logprob": -0.21369904453314623, "compression_ratio": 1.6046511627906976, "no_speech_prob": 7.527852631028509e-06}, {"id": 23, "seek": 10752, "start": 118.44, "end": 121.88, "text": " There had been some attempts in academia to use", "tokens": [821, 632, 668, 512, 15257, 294, 28937, 281, 764], "temperature": 0.0, "avg_logprob": -0.21369904453314623, "compression_ratio": 1.6046511627906976, "no_speech_prob": 7.527852631028509e-06}, {"id": 24, "seek": 10752, "start": 122.72, "end": 126.24, "text": " deep learning for this, but they hadn't managed to make much progress and", "tokens": [2452, 2539, 337, 341, 11, 457, 436, 8782, 380, 6453, 281, 652, 709, 4205, 293], "temperature": 0.0, "avg_logprob": -0.21369904453314623, "compression_ratio": 1.6046511627906976, "no_speech_prob": 7.527852631028509e-06}, {"id": 25, "seek": 10752, "start": 127.24, "end": 129.24, "text": " actually noticed us our Vera's", "tokens": [767, 5694, 505, 527, 46982, 311], "temperature": 0.0, "avg_logprob": -0.21369904453314623, "compression_ratio": 1.6046511627906976, "no_speech_prob": 7.527852631028509e-06}, {"id": 26, "seek": 10752, "start": 129.76, "end": 133.84, "text": " Kernel here which gets point 992 on the leaderboard, which I think is like top 20", "tokens": [40224, 338, 510, 597, 2170, 935, 11803, 17, 322, 264, 5263, 3787, 11, 597, 286, 519, 307, 411, 1192, 945], "temperature": 0.0, "avg_logprob": -0.21369904453314623, "compression_ratio": 1.6046511627906976, "no_speech_prob": 7.527852631028509e-06}, {"id": 27, "seek": 13384, "start": 133.84, "end": 138.68, "text": " Is yeah, it's kind of entirely heuristic, and it's a great example of", "tokens": [1119, 1338, 11, 309, 311, 733, 295, 7696, 415, 374, 3142, 11, 293, 309, 311, 257, 869, 1365, 295], "temperature": 0.0, "avg_logprob": -0.13943053654261997, "compression_ratio": 1.757936507936508, "no_speech_prob": 1.7778069377527572e-05}, {"id": 28, "seek": 13384, "start": 139.52, "end": 143.84, "text": " Kind of feature engineering this in this case the whole thing is basically entirely feature engineering", "tokens": [9242, 295, 4111, 7043, 341, 294, 341, 1389, 264, 1379, 551, 307, 1936, 7696, 4111, 7043], "temperature": 0.0, "avg_logprob": -0.13943053654261997, "compression_ratio": 1.757936507936508, "no_speech_prob": 1.7778069377527572e-05}, {"id": 29, "seek": 13384, "start": 143.84, "end": 149.64000000000001, "text": " So it's basically looking through and using lots of regular expressions to figure out for each token", "tokens": [407, 309, 311, 1936, 1237, 807, 293, 1228, 3195, 295, 3890, 15277, 281, 2573, 484, 337, 1184, 14862], "temperature": 0.0, "avg_logprob": -0.13943053654261997, "compression_ratio": 1.757936507936508, "no_speech_prob": 1.7778069377527572e-05}, {"id": 30, "seek": 13384, "start": 150.2, "end": 151.8, "text": " What is it you know?", "tokens": [708, 307, 309, 291, 458, 30], "temperature": 0.0, "avg_logprob": -0.13943053654261997, "compression_ratio": 1.757936507936508, "no_speech_prob": 1.7778069377527572e-05}, {"id": 31, "seek": 13384, "start": 151.8, "end": 157.0, "text": " and I think she's done a great job here kind of laying it all out clearly as to what all the different pieces are and", "tokens": [293, 286, 519, 750, 311, 1096, 257, 869, 1691, 510, 733, 295, 14903, 309, 439, 484, 4448, 382, 281, 437, 439, 264, 819, 3755, 366, 293], "temperature": 0.0, "avg_logprob": -0.13943053654261997, "compression_ratio": 1.757936507936508, "no_speech_prob": 1.7778069377527572e-05}, {"id": 32, "seek": 13384, "start": 157.04, "end": 159.04, "text": " how they all fit together and", "tokens": [577, 436, 439, 3318, 1214, 293], "temperature": 0.0, "avg_logprob": -0.13943053654261997, "compression_ratio": 1.757936507936508, "no_speech_prob": 1.7778069377527572e-05}, {"id": 33, "seek": 15904, "start": 159.04, "end": 165.04, "text": " She mentioned that she's maybe hoping to turn this into a library, which I think would be great right you know you could use this to", "tokens": [1240, 2835, 300, 750, 311, 1310, 7159, 281, 1261, 341, 666, 257, 6405, 11, 597, 286, 519, 576, 312, 869, 558, 291, 458, 291, 727, 764, 341, 281], "temperature": 0.0, "avg_logprob": -0.17682768617357528, "compression_ratio": 1.701818181818182, "no_speech_prob": 4.565927611110965e-06}, {"id": 34, "seek": 15904, "start": 166.23999999999998, "end": 170.12, "text": " Grab a piece of text and pull out. What are all the pieces in it?", "tokens": [20357, 257, 2522, 295, 2487, 293, 2235, 484, 13, 708, 366, 439, 264, 3755, 294, 309, 30], "temperature": 0.0, "avg_logprob": -0.17682768617357528, "compression_ratio": 1.701818181818182, "no_speech_prob": 4.565927611110965e-06}, {"id": 35, "seek": 15904, "start": 171.44, "end": 173.44, "text": " It's the kind of thing that", "tokens": [467, 311, 264, 733, 295, 551, 300], "temperature": 0.0, "avg_logprob": -0.17682768617357528, "compression_ratio": 1.701818181818182, "no_speech_prob": 4.565927611110965e-06}, {"id": 36, "seek": 15904, "start": 173.44, "end": 178.7, "text": " The neural the natural language can like natural language processing community hopes to be able to do", "tokens": [440, 18161, 264, 3303, 2856, 393, 411, 3303, 2856, 9007, 1768, 13681, 281, 312, 1075, 281, 360], "temperature": 0.0, "avg_logprob": -0.17682768617357528, "compression_ratio": 1.701818181818182, "no_speech_prob": 4.565927611110965e-06}, {"id": 37, "seek": 15904, "start": 179.12, "end": 183.29999999999998, "text": " Without like lots of handwritten code like this, but for now", "tokens": [9129, 411, 3195, 295, 1011, 26859, 3089, 411, 341, 11, 457, 337, 586], "temperature": 0.0, "avg_logprob": -0.17682768617357528, "compression_ratio": 1.701818181818182, "no_speech_prob": 4.565927611110965e-06}, {"id": 38, "seek": 15904, "start": 183.29999999999998, "end": 187.12, "text": " This is well be interesting to see like what the winners turn out to have done", "tokens": [639, 307, 731, 312, 1880, 281, 536, 411, 437, 264, 17193, 1261, 484, 281, 362, 1096], "temperature": 0.0, "avg_logprob": -0.17682768617357528, "compression_ratio": 1.701818181818182, "no_speech_prob": 4.565927611110965e-06}, {"id": 39, "seek": 18712, "start": 187.12, "end": 193.56, "text": " But I haven't seen machine learning being used really to do to do this particularly well", "tokens": [583, 286, 2378, 380, 1612, 3479, 2539, 885, 1143, 534, 281, 360, 281, 360, 341, 4098, 731], "temperature": 0.0, "avg_logprob": -0.21033982916192695, "compression_ratio": 1.7366255144032923, "no_speech_prob": 9.81807170319371e-06}, {"id": 40, "seek": 18712, "start": 194.28, "end": 199.68, "text": " Perhaps the best approach is the ones which combine this kind of feature engineering along with some machine learning", "tokens": [10517, 264, 1151, 3109, 307, 264, 2306, 597, 10432, 341, 733, 295, 4111, 7043, 2051, 365, 512, 3479, 2539], "temperature": 0.0, "avg_logprob": -0.21033982916192695, "compression_ratio": 1.7366255144032923, "no_speech_prob": 9.81807170319371e-06}, {"id": 41, "seek": 18712, "start": 201.28, "end": 204.48000000000002, "text": " But I think this is a great example of effective feature engineering and", "tokens": [583, 286, 519, 341, 307, 257, 869, 1365, 295, 4942, 4111, 7043, 293], "temperature": 0.0, "avg_logprob": -0.21033982916192695, "compression_ratio": 1.7366255144032923, "no_speech_prob": 9.81807170319371e-06}, {"id": 42, "seek": 18712, "start": 205.08, "end": 211.04000000000002, "text": " This is a another USS student who did has done much the same thing got a similar kind of score", "tokens": [639, 307, 257, 1071, 30385, 3107, 567, 630, 575, 1096, 709, 264, 912, 551, 658, 257, 2531, 733, 295, 6175], "temperature": 0.0, "avg_logprob": -0.21033982916192695, "compression_ratio": 1.7366255144032923, "no_speech_prob": 9.81807170319371e-06}, {"id": 43, "seek": 18712, "start": 211.68, "end": 214.68, "text": " But I used used her own different sort of rules", "tokens": [583, 286, 1143, 1143, 720, 1065, 819, 1333, 295, 4474], "temperature": 0.0, "avg_logprob": -0.21033982916192695, "compression_ratio": 1.7366255144032923, "no_speech_prob": 9.81807170319371e-06}, {"id": 44, "seek": 21468, "start": 214.68, "end": 220.52, "text": " Again this is gets you would get you a good leaderboard position with these as well", "tokens": [3764, 341, 307, 2170, 291, 576, 483, 291, 257, 665, 5263, 3787, 2535, 365, 613, 382, 731], "temperature": 0.0, "avg_logprob": -0.2035043462117513, "compression_ratio": 1.5384615384615385, "no_speech_prob": 2.2959034140512813e-06}, {"id": 45, "seek": 21468, "start": 221.12, "end": 223.12, "text": " So I thought that was interesting to see", "tokens": [407, 286, 1194, 300, 390, 1880, 281, 536], "temperature": 0.0, "avg_logprob": -0.2035043462117513, "compression_ratio": 1.5384615384615385, "no_speech_prob": 2.2959034140512813e-06}, {"id": 46, "seek": 21468, "start": 223.16, "end": 225.8, "text": " examples of some of our students entering a", "tokens": [5110, 295, 512, 295, 527, 1731, 11104, 257], "temperature": 0.0, "avg_logprob": -0.2035043462117513, "compression_ratio": 1.5384615384615385, "no_speech_prob": 2.2959034140512813e-06}, {"id": 47, "seek": 21468, "start": 226.76000000000002, "end": 234.78, "text": " Competition and getting kind of top 20 ish results by you know basically just handwritten heuristics, and this is where", "tokens": [43634, 293, 1242, 733, 295, 1192, 945, 307, 71, 3542, 538, 291, 458, 1936, 445, 1011, 26859, 415, 374, 6006, 11, 293, 341, 307, 689], "temperature": 0.0, "avg_logprob": -0.2035043462117513, "compression_ratio": 1.5384615384615385, "no_speech_prob": 2.2959034140512813e-06}, {"id": 48, "seek": 21468, "start": 236.0, "end": 238.08, "text": " for example computer vision was", "tokens": [337, 1365, 3820, 5201, 390], "temperature": 0.0, "avg_logprob": -0.2035043462117513, "compression_ratio": 1.5384615384615385, "no_speech_prob": 2.2959034140512813e-06}, {"id": 49, "seek": 23808, "start": 238.08, "end": 245.60000000000002, "text": " Six years ago still basically all the best approaches was a whole lot of like carefully handwritten", "tokens": [11678, 924, 2057, 920, 1936, 439, 264, 1151, 11587, 390, 257, 1379, 688, 295, 411, 7500, 1011, 26859], "temperature": 0.0, "avg_logprob": -0.24225866794586182, "compression_ratio": 1.573394495412844, "no_speech_prob": 2.684183527890127e-06}, {"id": 50, "seek": 23808, "start": 246.20000000000002, "end": 250.52, "text": " heuristics often combined with some simple machine learning and", "tokens": [415, 374, 6006, 2049, 9354, 365, 512, 2199, 3479, 2539, 293], "temperature": 0.0, "avg_logprob": -0.24225866794586182, "compression_ratio": 1.573394495412844, "no_speech_prob": 2.684183527890127e-06}, {"id": 51, "seek": 23808, "start": 251.16000000000003, "end": 253.16000000000003, "text": " So I think over time", "tokens": [407, 286, 519, 670, 565], "temperature": 0.0, "avg_logprob": -0.24225866794586182, "compression_ratio": 1.573394495412844, "no_speech_prob": 2.684183527890127e-06}, {"id": 52, "seek": 23808, "start": 253.52, "end": 255.52, "text": " You know the field is kind of?", "tokens": [509, 458, 264, 2519, 307, 733, 295, 30], "temperature": 0.0, "avg_logprob": -0.24225866794586182, "compression_ratio": 1.573394495412844, "no_speech_prob": 2.684183527890127e-06}, {"id": 53, "seek": 23808, "start": 256.32, "end": 258.32, "text": " Definitely trying to move towards", "tokens": [12151, 1382, 281, 1286, 3030], "temperature": 0.0, "avg_logprob": -0.24225866794586182, "compression_ratio": 1.573394495412844, "no_speech_prob": 2.684183527890127e-06}, {"id": 54, "seek": 23808, "start": 259.12, "end": 262.24, "text": " automating much more of this and actually interestingly", "tokens": [3553, 990, 709, 544, 295, 341, 293, 767, 25873], "temperature": 0.0, "avg_logprob": -0.24225866794586182, "compression_ratio": 1.573394495412844, "no_speech_prob": 2.684183527890127e-06}, {"id": 55, "seek": 23808, "start": 263.36, "end": 265.36, "text": " very interestingly in the", "tokens": [588, 25873, 294, 264], "temperature": 0.0, "avg_logprob": -0.24225866794586182, "compression_ratio": 1.573394495412844, "no_speech_prob": 2.684183527890127e-06}, {"id": 56, "seek": 23808, "start": 265.6, "end": 267.2, "text": " safe driver", "tokens": [3273, 6787], "temperature": 0.0, "avg_logprob": -0.24225866794586182, "compression_ratio": 1.573394495412844, "no_speech_prob": 2.684183527890127e-06}, {"id": 57, "seek": 26720, "start": 267.2, "end": 269.2, "text": " Prediction competition was just finished", "tokens": [32969, 4105, 6211, 390, 445, 4335], "temperature": 0.0, "avg_logprob": -0.2174193691199934, "compression_ratio": 1.5380952380952382, "no_speech_prob": 8.139492820191663e-06}, {"id": 58, "seek": 26720, "start": 271.28, "end": 277.0, "text": " One of the Netflix prize winners won this competition and he invented a new", "tokens": [1485, 295, 264, 12778, 12818, 17193, 1582, 341, 6211, 293, 415, 14479, 257, 777], "temperature": 0.0, "avg_logprob": -0.2174193691199934, "compression_ratio": 1.5380952380952382, "no_speech_prob": 8.139492820191663e-06}, {"id": 59, "seek": 26720, "start": 277.84, "end": 284.08, "text": " Algorithm for dealing with structured data which basically doesn't require any feature engineering at all", "tokens": [35014, 6819, 76, 337, 6260, 365, 18519, 1412, 597, 1936, 1177, 380, 3651, 604, 4111, 7043, 412, 439], "temperature": 0.0, "avg_logprob": -0.2174193691199934, "compression_ratio": 1.5380952380952382, "no_speech_prob": 8.139492820191663e-06}, {"id": 60, "seek": 26720, "start": 284.59999999999997, "end": 287.2, "text": " so he came first place using nothing but", "tokens": [370, 415, 1361, 700, 1081, 1228, 1825, 457], "temperature": 0.0, "avg_logprob": -0.2174193691199934, "compression_ratio": 1.5380952380952382, "no_speech_prob": 8.139492820191663e-06}, {"id": 61, "seek": 26720, "start": 288.44, "end": 289.76, "text": " five", "tokens": [1732], "temperature": 0.0, "avg_logprob": -0.2174193691199934, "compression_ratio": 1.5380952380952382, "no_speech_prob": 8.139492820191663e-06}, {"id": 62, "seek": 26720, "start": 289.76, "end": 291.84, "text": " deep learning models and one", "tokens": [2452, 2539, 5245, 293, 472], "temperature": 0.0, "avg_logprob": -0.2174193691199934, "compression_ratio": 1.5380952380952382, "no_speech_prob": 8.139492820191663e-06}, {"id": 63, "seek": 26720, "start": 292.44, "end": 294.2, "text": " gradient boosting machine", "tokens": [16235, 43117, 3479], "temperature": 0.0, "avg_logprob": -0.2174193691199934, "compression_ratio": 1.5380952380952382, "no_speech_prob": 8.139492820191663e-06}, {"id": 64, "seek": 29420, "start": 294.2, "end": 297.64, "text": " And his his basic approach was very similar to", "tokens": [400, 702, 702, 3875, 3109, 390, 588, 2531, 281], "temperature": 0.0, "avg_logprob": -0.1593082408712368, "compression_ratio": 1.697211155378486, "no_speech_prob": 4.2892856981779914e-06}, {"id": 65, "seek": 29420, "start": 298.12, "end": 302.2, "text": " What we've been learning in this class so far and what we'll be learning also tomorrow", "tokens": [708, 321, 600, 668, 2539, 294, 341, 1508, 370, 1400, 293, 437, 321, 603, 312, 2539, 611, 4153], "temperature": 0.0, "avg_logprob": -0.1593082408712368, "compression_ratio": 1.697211155378486, "no_speech_prob": 4.2892856981779914e-06}, {"id": 66, "seek": 29420, "start": 303.76, "end": 307.88, "text": " Which is using fully connected neural networks and we're and one hot encoding", "tokens": [3013, 307, 1228, 4498, 4582, 18161, 9590, 293, 321, 434, 293, 472, 2368, 43430], "temperature": 0.0, "avg_logprob": -0.1593082408712368, "compression_ratio": 1.697211155378486, "no_speech_prob": 4.2892856981779914e-06}, {"id": 67, "seek": 29420, "start": 308.52, "end": 313.32, "text": " And specifically embedding which we'll learn about but he had a very clever technique", "tokens": [400, 4682, 12240, 3584, 597, 321, 603, 1466, 466, 457, 415, 632, 257, 588, 13494, 6532], "temperature": 0.0, "avg_logprob": -0.1593082408712368, "compression_ratio": 1.697211155378486, "no_speech_prob": 4.2892856981779914e-06}, {"id": 68, "seek": 29420, "start": 313.32, "end": 318.02, "text": " Which was there was a lot of data in this competition which was unlabeled so in other words", "tokens": [3013, 390, 456, 390, 257, 688, 295, 1412, 294, 341, 6211, 597, 390, 32118, 18657, 292, 370, 294, 661, 2283], "temperature": 0.0, "avg_logprob": -0.1593082408712368, "compression_ratio": 1.697211155378486, "no_speech_prob": 4.2892856981779914e-06}, {"id": 69, "seek": 29420, "start": 319.0, "end": 321.56, "text": " Where they didn't know whether that?", "tokens": [2305, 436, 994, 380, 458, 1968, 300, 30], "temperature": 0.0, "avg_logprob": -0.1593082408712368, "compression_ratio": 1.697211155378486, "no_speech_prob": 4.2892856981779914e-06}, {"id": 70, "seek": 32156, "start": 321.56, "end": 326.56, "text": " Driver would go under claim or not or whatever so unlabeled data", "tokens": [36048, 576, 352, 833, 3932, 420, 406, 420, 2035, 370, 32118, 18657, 292, 1412], "temperature": 0.0, "avg_logprob": -0.1495204752141779, "compression_ratio": 2.0382978723404257, "no_speech_prob": 3.187538140991819e-06}, {"id": 71, "seek": 32156, "start": 326.72, "end": 332.92, "text": " So when you've got some labeled and some unlabeled data we call that semi supervised learning and in real life", "tokens": [407, 562, 291, 600, 658, 512, 21335, 293, 512, 32118, 18657, 292, 1412, 321, 818, 300, 12909, 46533, 2539, 293, 294, 957, 993], "temperature": 0.0, "avg_logprob": -0.1495204752141779, "compression_ratio": 2.0382978723404257, "no_speech_prob": 3.187538140991819e-06}, {"id": 72, "seek": 32156, "start": 333.36, "end": 340.1, "text": " Most learning is semi supervised learning like in real life normally you have some things that are labeled and some things that are unlabeled", "tokens": [4534, 2539, 307, 12909, 46533, 2539, 411, 294, 957, 993, 5646, 291, 362, 512, 721, 300, 366, 21335, 293, 512, 721, 300, 366, 32118, 18657, 292], "temperature": 0.0, "avg_logprob": -0.1495204752141779, "compression_ratio": 2.0382978723404257, "no_speech_prob": 3.187538140991819e-06}, {"id": 73, "seek": 32156, "start": 340.16, "end": 342.16, "text": " So this is kind of the most practically", "tokens": [407, 341, 307, 733, 295, 264, 881, 15667], "temperature": 0.0, "avg_logprob": -0.1495204752141779, "compression_ratio": 2.0382978723404257, "no_speech_prob": 3.187538140991819e-06}, {"id": 74, "seek": 32156, "start": 342.72, "end": 349.32, "text": " Useful kind of learning and then structured data is it's the most common kind of data that companies deal with day-to-day", "tokens": [8278, 906, 733, 295, 2539, 293, 550, 18519, 1412, 307, 309, 311, 264, 881, 2689, 733, 295, 1412, 300, 3431, 2028, 365, 786, 12, 1353, 12, 810], "temperature": 0.0, "avg_logprob": -0.1495204752141779, "compression_ratio": 2.0382978723404257, "no_speech_prob": 3.187538140991819e-06}, {"id": 75, "seek": 34932, "start": 349.32, "end": 351.32, "text": " so the fact that this competition was a", "tokens": [370, 264, 1186, 300, 341, 6211, 390, 257], "temperature": 0.0, "avg_logprob": -0.23195966084798178, "compression_ratio": 1.7272727272727273, "no_speech_prob": 2.9944098969281185e-06}, {"id": 76, "seek": 34932, "start": 352.56, "end": 357.68, "text": " semi-supervised structured data competition made it incredibly practically useful and", "tokens": [12909, 12, 48172, 24420, 18519, 1412, 6211, 1027, 309, 6252, 15667, 4420, 293], "temperature": 0.0, "avg_logprob": -0.23195966084798178, "compression_ratio": 1.7272727272727273, "no_speech_prob": 2.9944098969281185e-06}, {"id": 77, "seek": 34932, "start": 358.24, "end": 361.8, "text": " So what his technique for winning this was was to?", "tokens": [407, 437, 702, 6532, 337, 8224, 341, 390, 390, 281, 30], "temperature": 0.0, "avg_logprob": -0.23195966084798178, "compression_ratio": 1.7272727272727273, "no_speech_prob": 2.9944098969281185e-06}, {"id": 78, "seek": 34932, "start": 364.2, "end": 369.8, "text": " Do data augmentation which those of you doing the deep learning course have learned about which is basically the idea like if you had", "tokens": [1144, 1412, 14501, 19631, 597, 729, 295, 291, 884, 264, 2452, 2539, 1164, 362, 3264, 466, 597, 307, 1936, 264, 1558, 411, 498, 291, 632], "temperature": 0.0, "avg_logprob": -0.23195966084798178, "compression_ratio": 1.7272727272727273, "no_speech_prob": 2.9944098969281185e-06}, {"id": 79, "seek": 34932, "start": 370.44, "end": 376.4, "text": " Pictures you would like flip them horizontally or rotate them a bit data augmentation means creating new data examples", "tokens": [45877, 291, 576, 411, 7929, 552, 33796, 420, 13121, 552, 257, 857, 1412, 14501, 19631, 1355, 4084, 777, 1412, 5110], "temperature": 0.0, "avg_logprob": -0.23195966084798178, "compression_ratio": 1.7272727272727273, "no_speech_prob": 2.9944098969281185e-06}, {"id": 80, "seek": 34932, "start": 376.56, "end": 378.56, "text": " Which are kind of slightly", "tokens": [3013, 366, 733, 295, 4748], "temperature": 0.0, "avg_logprob": -0.23195966084798178, "compression_ratio": 1.7272727272727273, "no_speech_prob": 2.9944098969281185e-06}, {"id": 81, "seek": 37856, "start": 378.56, "end": 379.64, "text": " different", "tokens": [819], "temperature": 0.0, "avg_logprob": -0.20078799189353475, "compression_ratio": 1.7429906542056075, "no_speech_prob": 5.955091637588339e-06}, {"id": 82, "seek": 37856, "start": 379.64, "end": 385.84, "text": " Versions of ones you already have and the way he did it was for each row in the data he would like", "tokens": [12226, 626, 295, 2306, 291, 1217, 362, 293, 264, 636, 415, 630, 309, 390, 337, 1184, 5386, 294, 264, 1412, 415, 576, 411], "temperature": 0.0, "avg_logprob": -0.20078799189353475, "compression_ratio": 1.7429906542056075, "no_speech_prob": 5.955091637588339e-06}, {"id": 83, "seek": 37856, "start": 386.72, "end": 389.68, "text": " at random replace 15% of the", "tokens": [412, 4974, 7406, 2119, 4, 295, 264], "temperature": 0.0, "avg_logprob": -0.20078799189353475, "compression_ratio": 1.7429906542056075, "no_speech_prob": 5.955091637588339e-06}, {"id": 84, "seek": 37856, "start": 390.96, "end": 392.96, "text": " variables with a different row", "tokens": [9102, 365, 257, 819, 5386], "temperature": 0.0, "avg_logprob": -0.20078799189353475, "compression_ratio": 1.7429906542056075, "no_speech_prob": 5.955091637588339e-06}, {"id": 85, "seek": 37856, "start": 393.4, "end": 398.36, "text": " So each row now would represent like a mix of like 80% 85% of the original row", "tokens": [407, 1184, 5386, 586, 576, 2906, 411, 257, 2890, 295, 411, 4688, 4, 14695, 4, 295, 264, 3380, 5386], "temperature": 0.0, "avg_logprob": -0.20078799189353475, "compression_ratio": 1.7429906542056075, "no_speech_prob": 5.955091637588339e-06}, {"id": 86, "seek": 37856, "start": 398.4, "end": 401.2, "text": " But 15% randomly selected from a different row", "tokens": [583, 2119, 4, 16979, 8209, 490, 257, 819, 5386], "temperature": 0.0, "avg_logprob": -0.20078799189353475, "compression_ratio": 1.7429906542056075, "no_speech_prob": 5.955091637588339e-06}, {"id": 87, "seek": 37856, "start": 401.96, "end": 403.96, "text": " and so this was a way of like", "tokens": [293, 370, 341, 390, 257, 636, 295, 411], "temperature": 0.0, "avg_logprob": -0.20078799189353475, "compression_ratio": 1.7429906542056075, "no_speech_prob": 5.955091637588339e-06}, {"id": 88, "seek": 40396, "start": 403.96, "end": 407.88, "text": " randomly changing the data a little bit and then", "tokens": [16979, 4473, 264, 1412, 257, 707, 857, 293, 550], "temperature": 0.0, "avg_logprob": -0.20600274168414834, "compression_ratio": 1.6585365853658536, "no_speech_prob": 4.356846602604492e-06}, {"id": 89, "seek": 40396, "start": 408.59999999999997, "end": 411.15999999999997, "text": " He used something called an autoencoder, which we will", "tokens": [634, 1143, 746, 1219, 364, 8399, 22660, 19866, 11, 597, 321, 486], "temperature": 0.0, "avg_logprob": -0.20600274168414834, "compression_ratio": 1.6585365853658536, "no_speech_prob": 4.356846602604492e-06}, {"id": 90, "seek": 40396, "start": 412.35999999999996, "end": 414.96, "text": " Probably won't study into a part two of the deep learning course", "tokens": [9210, 1582, 380, 2979, 666, 257, 644, 732, 295, 264, 2452, 2539, 1164], "temperature": 0.0, "avg_logprob": -0.20600274168414834, "compression_ratio": 1.6585365853658536, "no_speech_prob": 4.356846602604492e-06}, {"id": 91, "seek": 40396, "start": 414.96, "end": 421.03999999999996, "text": " But the basic idea of an autoencoder is your dependent variable is the same as your independent variable", "tokens": [583, 264, 3875, 1558, 295, 364, 8399, 22660, 19866, 307, 428, 12334, 7006, 307, 264, 912, 382, 428, 6695, 7006], "temperature": 0.0, "avg_logprob": -0.20600274168414834, "compression_ratio": 1.6585365853658536, "no_speech_prob": 4.356846602604492e-06}, {"id": 92, "seek": 40396, "start": 421.08, "end": 423.08, "text": " So in other words you try to predict", "tokens": [407, 294, 661, 2283, 291, 853, 281, 6069], "temperature": 0.0, "avg_logprob": -0.20600274168414834, "compression_ratio": 1.6585365853658536, "no_speech_prob": 4.356846602604492e-06}, {"id": 93, "seek": 40396, "start": 423.79999999999995, "end": 426.2, "text": " your input which obviously is", "tokens": [428, 4846, 597, 2745, 307], "temperature": 0.0, "avg_logprob": -0.20600274168414834, "compression_ratio": 1.6585365853658536, "no_speech_prob": 4.356846602604492e-06}, {"id": 94, "seek": 42620, "start": 426.2, "end": 432.92, "text": " Trivial if you're allowed to like like you know the identity transform", "tokens": [10931, 22640, 498, 291, 434, 4350, 281, 411, 411, 291, 458, 264, 6575, 4088], "temperature": 0.0, "avg_logprob": -0.2158746100091315, "compression_ratio": 1.5566502463054188, "no_speech_prob": 4.737899814699631e-07}, {"id": 95, "seek": 42620, "start": 432.92, "end": 438.64, "text": " For example trivially predicts the input but the trick with an autoencoder is to have less activations in", "tokens": [1171, 1365, 1376, 85, 2270, 6069, 82, 264, 4846, 457, 264, 4282, 365, 364, 8399, 22660, 19866, 307, 281, 362, 1570, 2430, 763, 294], "temperature": 0.0, "avg_logprob": -0.2158746100091315, "compression_ratio": 1.5566502463054188, "no_speech_prob": 4.737899814699631e-07}, {"id": 96, "seek": 42620, "start": 439.56, "end": 446.08, "text": " At least one of your layers than your input right so if your input was like a hundred dimensional vector", "tokens": [1711, 1935, 472, 295, 428, 7914, 813, 428, 4846, 558, 370, 498, 428, 4846, 390, 411, 257, 3262, 18795, 8062], "temperature": 0.0, "avg_logprob": -0.2158746100091315, "compression_ratio": 1.5566502463054188, "no_speech_prob": 4.737899814699631e-07}, {"id": 97, "seek": 42620, "start": 446.08, "end": 449.28, "text": " And you put it through a 100 pi 10", "tokens": [400, 291, 829, 309, 807, 257, 2319, 3895, 1266], "temperature": 0.0, "avg_logprob": -0.2158746100091315, "compression_ratio": 1.5566502463054188, "no_speech_prob": 4.737899814699631e-07}, {"id": 98, "seek": 44928, "start": 449.28, "end": 456.29999999999995, "text": " Matrix to create 10 activations and then had to recreate the original hundred long vector from that", "tokens": [36274, 281, 1884, 1266, 2430, 763, 293, 550, 632, 281, 25833, 264, 3380, 3262, 938, 8062, 490, 300], "temperature": 0.0, "avg_logprob": -0.18565992661464362, "compression_ratio": 1.6261261261261262, "no_speech_prob": 1.7880597624753136e-06}, {"id": 99, "seek": 44928, "start": 456.59999999999997, "end": 462.0, "text": " Then you've basically come you had to have compressed it effectively and so it turns out that", "tokens": [1396, 291, 600, 1936, 808, 291, 632, 281, 362, 30353, 309, 8659, 293, 370, 309, 4523, 484, 300], "temperature": 0.0, "avg_logprob": -0.18565992661464362, "compression_ratio": 1.6261261261261262, "no_speech_prob": 1.7880597624753136e-06}, {"id": 100, "seek": 44928, "start": 462.59999999999997, "end": 464.59999999999997, "text": " That kind of neural network", "tokens": [663, 733, 295, 18161, 3209], "temperature": 0.0, "avg_logprob": -0.18565992661464362, "compression_ratio": 1.6261261261261262, "no_speech_prob": 1.7880597624753136e-06}, {"id": 101, "seek": 44928, "start": 465.44, "end": 467.44, "text": " You know it's forced to find", "tokens": [509, 458, 309, 311, 7579, 281, 915], "temperature": 0.0, "avg_logprob": -0.18565992661464362, "compression_ratio": 1.6261261261261262, "no_speech_prob": 1.7880597624753136e-06}, {"id": 102, "seek": 44928, "start": 469.0, "end": 475.28, "text": " Correlations and features and interesting relationships in the data even when it's not labeled so he used that", "tokens": [3925, 4419, 763, 293, 4122, 293, 1880, 6159, 294, 264, 1412, 754, 562, 309, 311, 406, 21335, 370, 415, 1143, 300], "temperature": 0.0, "avg_logprob": -0.18565992661464362, "compression_ratio": 1.6261261261261262, "no_speech_prob": 1.7880597624753136e-06}, {"id": 103, "seek": 47528, "start": 475.28, "end": 479.38, "text": " Rather than doing any he didn't do any hand engineering. He just used an autoencoder", "tokens": [16571, 813, 884, 604, 415, 994, 380, 360, 604, 1011, 7043, 13, 634, 445, 1143, 364, 8399, 22660, 19866], "temperature": 0.0, "avg_logprob": -0.2214510202407837, "compression_ratio": 1.6029411764705883, "no_speech_prob": 3.966958956880262e-06}, {"id": 104, "seek": 47528, "start": 480.15999999999997, "end": 486.96, "text": " So you know these are some interesting kind of directions that if you keep going with your machine learning studies", "tokens": [407, 291, 458, 613, 366, 512, 1880, 733, 295, 11095, 300, 498, 291, 1066, 516, 365, 428, 3479, 2539, 5313], "temperature": 0.0, "avg_logprob": -0.2214510202407837, "compression_ratio": 1.6029411764705883, "no_speech_prob": 3.966958956880262e-06}, {"id": 105, "seek": 47528, "start": 486.96, "end": 488.96, "text": " You know particularly if you", "tokens": [509, 458, 4098, 498, 291], "temperature": 0.0, "avg_logprob": -0.2214510202407837, "compression_ratio": 1.6029411764705883, "no_speech_prob": 3.966958956880262e-06}, {"id": 106, "seek": 47528, "start": 489.55999999999995, "end": 492.32, "text": " Do part two of the deep learning course next year?", "tokens": [1144, 644, 732, 295, 264, 2452, 2539, 1164, 958, 1064, 30], "temperature": 0.0, "avg_logprob": -0.2214510202407837, "compression_ratio": 1.6029411764705883, "no_speech_prob": 3.966958956880262e-06}, {"id": 107, "seek": 47528, "start": 493.4, "end": 495.4, "text": " you'll learn about and", "tokens": [291, 603, 1466, 466, 293], "temperature": 0.0, "avg_logprob": -0.2214510202407837, "compression_ratio": 1.6029411764705883, "no_speech_prob": 3.966958956880262e-06}, {"id": 108, "seek": 47528, "start": 496.88, "end": 498.88, "text": " You can kind of see how", "tokens": [509, 393, 733, 295, 536, 577], "temperature": 0.0, "avg_logprob": -0.2214510202407837, "compression_ratio": 1.6029411764705883, "no_speech_prob": 3.966958956880262e-06}, {"id": 109, "seek": 49888, "start": 498.88, "end": 505.8, "text": " The feature engineering is going away, and this was just yeah an hour ago, so this is very recent news indeed", "tokens": [440, 4111, 7043, 307, 516, 1314, 11, 293, 341, 390, 445, 1338, 364, 1773, 2057, 11, 370, 341, 307, 588, 5162, 2583, 6451], "temperature": 0.0, "avg_logprob": -0.17197755177815754, "compression_ratio": 1.4568527918781726, "no_speech_prob": 3.393112820049282e-06}, {"id": 110, "seek": 49888, "start": 505.8, "end": 507.8, "text": " But it's one of this is one of the most important", "tokens": [583, 309, 311, 472, 295, 341, 307, 472, 295, 264, 881, 1021], "temperature": 0.0, "avg_logprob": -0.17197755177815754, "compression_ratio": 1.4568527918781726, "no_speech_prob": 3.393112820049282e-06}, {"id": 111, "seek": 49888, "start": 508.44, "end": 510.44, "text": " breakthroughs I've seen in a long time", "tokens": [22397, 82, 286, 600, 1612, 294, 257, 938, 565], "temperature": 0.0, "avg_logprob": -0.17197755177815754, "compression_ratio": 1.4568527918781726, "no_speech_prob": 3.393112820049282e-06}, {"id": 112, "seek": 49888, "start": 512.8, "end": 516.92, "text": " Okay, so we were working through a", "tokens": [1033, 11, 370, 321, 645, 1364, 807, 257], "temperature": 0.0, "avg_logprob": -0.17197755177815754, "compression_ratio": 1.4568527918781726, "no_speech_prob": 3.393112820049282e-06}, {"id": 113, "seek": 51692, "start": 516.92, "end": 525.16, "text": " a simple logistic regression trained with SGD for MNIST and", "tokens": [257, 2199, 3565, 3142, 24590, 8895, 365, 34520, 35, 337, 376, 45, 19756, 293], "temperature": 0.0, "avg_logprob": -0.2101142257452011, "compression_ratio": 1.4733727810650887, "no_speech_prob": 7.766829185129609e-06}, {"id": 114, "seek": 51692, "start": 530.24, "end": 535.52, "text": " Here's the summary of where we got to we have nearly built a", "tokens": [1692, 311, 264, 12691, 295, 689, 321, 658, 281, 321, 362, 6217, 3094, 257], "temperature": 0.0, "avg_logprob": -0.2101142257452011, "compression_ratio": 1.4733727810650887, "no_speech_prob": 7.766829185129609e-06}, {"id": 115, "seek": 51692, "start": 536.52, "end": 538.36, "text": " module a", "tokens": [10088, 257], "temperature": 0.0, "avg_logprob": -0.2101142257452011, "compression_ratio": 1.4733727810650887, "no_speech_prob": 7.766829185129609e-06}, {"id": 116, "seek": 51692, "start": 538.36, "end": 544.52, "text": " Model module and a training loop from scratch and we were going to kind of try and finish that and after we finish that", "tokens": [17105, 10088, 293, 257, 3097, 6367, 490, 8459, 293, 321, 645, 516, 281, 733, 295, 853, 293, 2413, 300, 293, 934, 321, 2413, 300], "temperature": 0.0, "avg_logprob": -0.2101142257452011, "compression_ratio": 1.4733727810650887, "no_speech_prob": 7.766829185129609e-06}, {"id": 117, "seek": 54452, "start": 544.52, "end": 550.0, "text": " I'm then going to go through this entire notebook backwards right so having gone like top to bottom", "tokens": [286, 478, 550, 516, 281, 352, 807, 341, 2302, 21060, 12204, 558, 370, 1419, 2780, 411, 1192, 281, 2767], "temperature": 0.0, "avg_logprob": -0.20680780971751495, "compression_ratio": 1.7296137339055795, "no_speech_prob": 2.156805521735805e-06}, {"id": 118, "seek": 54452, "start": 550.12, "end": 554.02, "text": " We're then going to go back through bottom to top, okay, so", "tokens": [492, 434, 550, 516, 281, 352, 646, 807, 2767, 281, 1192, 11, 1392, 11, 370], "temperature": 0.0, "avg_logprob": -0.20680780971751495, "compression_ratio": 1.7296137339055795, "no_speech_prob": 2.156805521735805e-06}, {"id": 119, "seek": 54452, "start": 555.64, "end": 557.64, "text": " You know this was that little", "tokens": [509, 458, 341, 390, 300, 707], "temperature": 0.0, "avg_logprob": -0.20680780971751495, "compression_ratio": 1.7296137339055795, "no_speech_prob": 2.156805521735805e-06}, {"id": 120, "seek": 54452, "start": 558.12, "end": 561.5799999999999, "text": " Hand written and end up module class we created", "tokens": [8854, 3720, 293, 917, 493, 10088, 1508, 321, 2942], "temperature": 0.0, "avg_logprob": -0.20680780971751495, "compression_ratio": 1.7296137339055795, "no_speech_prob": 2.156805521735805e-06}, {"id": 121, "seek": 54452, "start": 562.28, "end": 564.12, "text": " We defined our loss", "tokens": [492, 7642, 527, 4470], "temperature": 0.0, "avg_logprob": -0.20680780971751495, "compression_ratio": 1.7296137339055795, "no_speech_prob": 2.156805521735805e-06}, {"id": 122, "seek": 54452, "start": 564.12, "end": 569.6999999999999, "text": " We defined our learning rate, and we defined our optimizer. This is the thing that we're going to try and write by hand in a moment", "tokens": [492, 7642, 527, 2539, 3314, 11, 293, 321, 7642, 527, 5028, 6545, 13, 639, 307, 264, 551, 300, 321, 434, 516, 281, 853, 293, 2464, 538, 1011, 294, 257, 1623], "temperature": 0.0, "avg_logprob": -0.20680780971751495, "compression_ratio": 1.7296137339055795, "no_speech_prob": 2.156805521735805e-06}, {"id": 123, "seek": 54452, "start": 570.4399999999999, "end": 572.4399999999999, "text": " So that stuff", "tokens": [407, 300, 1507], "temperature": 0.0, "avg_logprob": -0.20680780971751495, "compression_ratio": 1.7296137339055795, "no_speech_prob": 2.156805521735805e-06}, {"id": 124, "seek": 57244, "start": 572.44, "end": 578.7600000000001, "text": " That and that we're stealing with from pytorch, but that we've written ourselves and this we've written ourselves", "tokens": [663, 293, 300, 321, 434, 19757, 365, 490, 25878, 284, 339, 11, 457, 300, 321, 600, 3720, 4175, 293, 341, 321, 600, 3720, 4175], "temperature": 0.0, "avg_logprob": -0.15160829103910006, "compression_ratio": 1.9107806691449813, "no_speech_prob": 4.2228052734571975e-06}, {"id": 125, "seek": 57244, "start": 578.7600000000001, "end": 583.48, "text": " So the basic idea was we're going to go through some number of epochs, so let's go through one epoch", "tokens": [407, 264, 3875, 1558, 390, 321, 434, 516, 281, 352, 807, 512, 1230, 295, 30992, 28346, 11, 370, 718, 311, 352, 807, 472, 30992, 339], "temperature": 0.0, "avg_logprob": -0.15160829103910006, "compression_ratio": 1.9107806691449813, "no_speech_prob": 4.2228052734571975e-06}, {"id": 126, "seek": 57244, "start": 584.5600000000001, "end": 590.8800000000001, "text": " Right and we're going to keep track of how much for each mini batch. What was the loss so that we can report it at the end?", "tokens": [1779, 293, 321, 434, 516, 281, 1066, 2837, 295, 577, 709, 337, 1184, 8382, 15245, 13, 708, 390, 264, 4470, 370, 300, 321, 393, 2275, 309, 412, 264, 917, 30], "temperature": 0.0, "avg_logprob": -0.15160829103910006, "compression_ratio": 1.9107806691449813, "no_speech_prob": 4.2228052734571975e-06}, {"id": 127, "seek": 57244, "start": 591.8800000000001, "end": 595.1600000000001, "text": " We're going to turn our training data loader into an iterator", "tokens": [492, 434, 516, 281, 1261, 527, 3097, 1412, 3677, 260, 666, 364, 17138, 1639], "temperature": 0.0, "avg_logprob": -0.15160829103910006, "compression_ratio": 1.9107806691449813, "no_speech_prob": 4.2228052734571975e-06}, {"id": 128, "seek": 57244, "start": 595.72, "end": 601.4000000000001, "text": " So that we can loop through it live through every mini batch and so now we can go and go ahead and say for tensor", "tokens": [407, 300, 321, 393, 6367, 807, 309, 1621, 807, 633, 8382, 15245, 293, 370, 586, 321, 393, 352, 293, 352, 2286, 293, 584, 337, 40863], "temperature": 0.0, "avg_logprob": -0.15160829103910006, "compression_ratio": 1.9107806691449813, "no_speech_prob": 4.2228052734571975e-06}, {"id": 129, "seek": 60140, "start": 601.4, "end": 603.4, "text": " in", "tokens": [294], "temperature": 0.0, "avg_logprob": -0.15490458732427553, "compression_ratio": 1.6683417085427135, "no_speech_prob": 2.9944262678327505e-06}, {"id": 130, "seek": 60140, "start": 604.16, "end": 611.3199999999999, "text": " The length of the data loader, and then we can call next to grab the next independent variables and the dependent variables", "tokens": [440, 4641, 295, 264, 1412, 3677, 260, 11, 293, 550, 321, 393, 818, 958, 281, 4444, 264, 958, 6695, 9102, 293, 264, 12334, 9102], "temperature": 0.0, "avg_logprob": -0.15490458732427553, "compression_ratio": 1.6683417085427135, "no_speech_prob": 2.9944262678327505e-06}, {"id": 131, "seek": 60140, "start": 612.04, "end": 614.76, "text": " From our data loader from that iterator, okay?", "tokens": [3358, 527, 1412, 3677, 260, 490, 300, 17138, 1639, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.15490458732427553, "compression_ratio": 1.6683417085427135, "no_speech_prob": 2.9944262678327505e-06}, {"id": 132, "seek": 60140, "start": 615.9599999999999, "end": 617.9599999999999, "text": " So then remember we can then pass", "tokens": [407, 550, 1604, 321, 393, 550, 1320], "temperature": 0.0, "avg_logprob": -0.15490458732427553, "compression_ratio": 1.6683417085427135, "no_speech_prob": 2.9944262678327505e-06}, {"id": 133, "seek": 60140, "start": 618.48, "end": 623.56, "text": " The X tensor into our model by calling the model as if it was a function", "tokens": [440, 1783, 40863, 666, 527, 2316, 538, 5141, 264, 2316, 382, 498, 309, 390, 257, 2445], "temperature": 0.0, "avg_logprob": -0.15490458732427553, "compression_ratio": 1.6683417085427135, "no_speech_prob": 2.9944262678327505e-06}, {"id": 134, "seek": 60140, "start": 624.12, "end": 626.12, "text": " But first of all we have to turn it into a variable", "tokens": [583, 700, 295, 439, 321, 362, 281, 1261, 309, 666, 257, 7006], "temperature": 0.0, "avg_logprob": -0.15490458732427553, "compression_ratio": 1.6683417085427135, "no_speech_prob": 2.9944262678327505e-06}, {"id": 135, "seek": 62612, "start": 626.12, "end": 633.94, "text": " Last week we were typing variable blah dot CUDA to turn it into a variable a shorthand for that is just the capital B", "tokens": [5264, 1243, 321, 645, 18444, 7006, 12288, 5893, 29777, 7509, 281, 1261, 309, 666, 257, 7006, 257, 402, 2652, 474, 337, 300, 307, 445, 264, 4238, 363], "temperature": 0.0, "avg_logprob": -0.20397811752181869, "compression_ratio": 1.796812749003984, "no_speech_prob": 3.844919774564914e-06}, {"id": 136, "seek": 62612, "start": 633.94, "end": 639.6, "text": " That's a capital T for a tensor capital B for a fee for a variable. That's just a shortcut in fast AI", "tokens": [663, 311, 257, 4238, 314, 337, 257, 40863, 4238, 363, 337, 257, 12054, 337, 257, 7006, 13, 663, 311, 445, 257, 24822, 294, 2370, 7318], "temperature": 0.0, "avg_logprob": -0.20397811752181869, "compression_ratio": 1.796812749003984, "no_speech_prob": 3.844919774564914e-06}, {"id": 137, "seek": 62612, "start": 639.88, "end": 642.24, "text": " Okay, so that returns our predictions", "tokens": [1033, 11, 370, 300, 11247, 527, 21264], "temperature": 0.0, "avg_logprob": -0.20397811752181869, "compression_ratio": 1.796812749003984, "no_speech_prob": 3.844919774564914e-06}, {"id": 138, "seek": 62612, "start": 643.24, "end": 645.5600000000001, "text": " And so the next thing we needed was to calculate our loss", "tokens": [400, 370, 264, 958, 551, 321, 2978, 390, 281, 8873, 527, 4470], "temperature": 0.0, "avg_logprob": -0.20397811752181869, "compression_ratio": 1.796812749003984, "no_speech_prob": 3.844919774564914e-06}, {"id": 139, "seek": 62612, "start": 647.48, "end": 650.96, "text": " Because we can't calculate the derivatives of the loss if you haven't calculated loss", "tokens": [1436, 321, 393, 380, 8873, 264, 33733, 295, 264, 4470, 498, 291, 2378, 380, 15598, 4470], "temperature": 0.0, "avg_logprob": -0.20397811752181869, "compression_ratio": 1.796812749003984, "no_speech_prob": 3.844919774564914e-06}, {"id": 140, "seek": 62612, "start": 651.04, "end": 654.24, "text": " So the loss takes the predictions and the actuals", "tokens": [407, 264, 4470, 2516, 264, 21264, 293, 264, 3539, 82], "temperature": 0.0, "avg_logprob": -0.20397811752181869, "compression_ratio": 1.796812749003984, "no_speech_prob": 3.844919774564914e-06}, {"id": 141, "seek": 65424, "start": 654.24, "end": 659.64, "text": " Okay, so the actuals again are the the Y tensor and again we have to turn that into a variable", "tokens": [1033, 11, 370, 264, 3539, 82, 797, 366, 264, 264, 398, 40863, 293, 797, 321, 362, 281, 1261, 300, 666, 257, 7006], "temperature": 0.0, "avg_logprob": -0.26205576116388496, "compression_ratio": 1.8273092369477912, "no_speech_prob": 2.5215613277396187e-06}, {"id": 142, "seek": 65424, "start": 660.28, "end": 666.92, "text": " Now can anybody remind me what a variable is and why we would want to use a variable here?", "tokens": [823, 393, 4472, 4160, 385, 437, 257, 7006, 307, 293, 983, 321, 576, 528, 281, 764, 257, 7006, 510, 30], "temperature": 0.0, "avg_logprob": -0.26205576116388496, "compression_ratio": 1.8273092369477912, "no_speech_prob": 2.5215613277396187e-06}, {"id": 143, "seek": 65424, "start": 671.44, "end": 676.2, "text": " I think once you turn into variable then it tracks it so then you can do it backward on it", "tokens": [286, 519, 1564, 291, 1261, 666, 7006, 550, 309, 10218, 309, 370, 550, 291, 393, 360, 309, 23897, 322, 309], "temperature": 0.0, "avg_logprob": -0.26205576116388496, "compression_ratio": 1.8273092369477912, "no_speech_prob": 2.5215613277396187e-06}, {"id": 144, "seek": 65424, "start": 676.2, "end": 678.2, "text": " So you can everyone sorry when you turn to variable it?", "tokens": [407, 291, 393, 1518, 2597, 562, 291, 1261, 281, 7006, 309, 30], "temperature": 0.0, "avg_logprob": -0.26205576116388496, "compression_ratio": 1.8273092369477912, "no_speech_prob": 2.5215613277396187e-06}, {"id": 145, "seek": 65424, "start": 678.44, "end": 683.4, "text": " It can track like its process of like you know as you add the function as the function start getting layered in each other", "tokens": [467, 393, 2837, 411, 1080, 1399, 295, 411, 291, 458, 382, 291, 909, 264, 2445, 382, 264, 2445, 722, 1242, 34666, 294, 1184, 661], "temperature": 0.0, "avg_logprob": -0.26205576116388496, "compression_ratio": 1.8273092369477912, "no_speech_prob": 2.5215613277396187e-06}, {"id": 146, "seek": 68340, "start": 683.4, "end": 687.72, "text": " You can track it and then we do backward on it back propagates and does the yeah, right?", "tokens": [509, 393, 2837, 309, 293, 550, 321, 360, 23897, 322, 309, 646, 12425, 1024, 293, 775, 264, 1338, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.3112935534978317, "compression_ratio": 1.4303797468354431, "no_speech_prob": 1.1365600585122593e-06}, {"id": 147, "seek": 68340, "start": 691.16, "end": 694.04, "text": " Right so a variable keeps", "tokens": [1779, 370, 257, 7006, 5965], "temperature": 0.0, "avg_logprob": -0.3112935534978317, "compression_ratio": 1.4303797468354431, "no_speech_prob": 1.1365600585122593e-06}, {"id": 148, "seek": 68340, "start": 695.3199999999999, "end": 699.48, "text": " Track of all of the steps to get computed", "tokens": [31903, 295, 439, 295, 264, 4439, 281, 483, 40610], "temperature": 0.0, "avg_logprob": -0.3112935534978317, "compression_ratio": 1.4303797468354431, "no_speech_prob": 1.1365600585122593e-06}, {"id": 149, "seek": 68340, "start": 700.0, "end": 703.1999999999999, "text": " And so there's actually a fantastic tutorial", "tokens": [400, 370, 456, 311, 767, 257, 5456, 7073], "temperature": 0.0, "avg_logprob": -0.3112935534978317, "compression_ratio": 1.4303797468354431, "no_speech_prob": 1.1365600585122593e-06}, {"id": 150, "seek": 70320, "start": 703.2, "end": 713.0, "text": " on the pie torch website so", "tokens": [322, 264, 1730, 27822, 3144, 370], "temperature": 0.0, "avg_logprob": -0.24232579583991065, "compression_ratio": 1.8135593220338984, "no_speech_prob": 1.5056959909998113e-06}, {"id": 151, "seek": 70320, "start": 713.36, "end": 716.6800000000001, "text": " On the pipe torch website. There's a tutorial section", "tokens": [1282, 264, 11240, 27822, 3144, 13, 821, 311, 257, 7073, 3541], "temperature": 0.0, "avg_logprob": -0.24232579583991065, "compression_ratio": 1.8135593220338984, "no_speech_prob": 1.5056959909998113e-06}, {"id": 152, "seek": 70320, "start": 716.6800000000001, "end": 723.9200000000001, "text": " And there's a tutorial there about autograd autograd is the name of the automatic differentiation package that comes with pie torch", "tokens": [400, 456, 311, 257, 7073, 456, 466, 1476, 664, 6206, 1476, 664, 6206, 307, 264, 1315, 295, 264, 12509, 38902, 7372, 300, 1487, 365, 1730, 27822], "temperature": 0.0, "avg_logprob": -0.24232579583991065, "compression_ratio": 1.8135593220338984, "no_speech_prob": 1.5056959909998113e-06}, {"id": 153, "seek": 72392, "start": 723.92, "end": 733.24, "text": " And it's a it's an implementation of automatic differentiation, and so the variable class is really the key the", "tokens": [400, 309, 311, 257, 309, 311, 364, 11420, 295, 12509, 38902, 11, 293, 370, 264, 7006, 1508, 307, 534, 264, 2141, 264], "temperature": 0.0, "avg_logprob": -0.17965227014878216, "compression_ratio": 1.6964285714285714, "no_speech_prob": 1.9333524505782407e-06}, {"id": 154, "seek": 72392, "start": 733.4, "end": 739.28, "text": " Key class here because that's the thing that makes turns a tensor into something where we can keep track of its gradients", "tokens": [12759, 1508, 510, 570, 300, 311, 264, 551, 300, 1669, 4523, 257, 40863, 666, 746, 689, 321, 393, 1066, 2837, 295, 1080, 2771, 2448], "temperature": 0.0, "avg_logprob": -0.17965227014878216, "compression_ratio": 1.6964285714285714, "no_speech_prob": 1.9333524505782407e-06}, {"id": 155, "seek": 72392, "start": 740.36, "end": 746.0799999999999, "text": " So basically here they show how to create a variable do an operation to a variable and", "tokens": [407, 1936, 510, 436, 855, 577, 281, 1884, 257, 7006, 360, 364, 6916, 281, 257, 7006, 293], "temperature": 0.0, "avg_logprob": -0.17965227014878216, "compression_ratio": 1.6964285714285714, "no_speech_prob": 1.9333524505782407e-06}, {"id": 156, "seek": 72392, "start": 746.4799999999999, "end": 750.4, "text": " Then you can go back and actually look at the grad function", "tokens": [1396, 291, 393, 352, 646, 293, 767, 574, 412, 264, 2771, 2445], "temperature": 0.0, "avg_logprob": -0.17965227014878216, "compression_ratio": 1.6964285714285714, "no_speech_prob": 1.9333524505782407e-06}, {"id": 157, "seek": 75040, "start": 750.4, "end": 753.8199999999999, "text": " Which is the the function that it's keeping track of?", "tokens": [3013, 307, 264, 264, 2445, 300, 309, 311, 5145, 2837, 295, 30], "temperature": 0.0, "avg_logprob": -0.2340150501417077, "compression_ratio": 1.808888888888889, "no_speech_prob": 2.3320683339989046e-06}, {"id": 158, "seek": 75040, "start": 754.6, "end": 760.6, "text": " Basically to calculate the gradient right so as we do more and more operations to this", "tokens": [8537, 281, 8873, 264, 16235, 558, 370, 382, 321, 360, 544, 293, 544, 7705, 281, 341], "temperature": 0.0, "avg_logprob": -0.2340150501417077, "compression_ratio": 1.808888888888889, "no_speech_prob": 2.3320683339989046e-06}, {"id": 159, "seek": 75040, "start": 760.9599999999999, "end": 766.74, "text": " Very variable and the variables calculated from that variable it keeps keeping track of it so later on we can go", "tokens": [4372, 7006, 293, 264, 9102, 15598, 490, 300, 7006, 309, 5965, 5145, 2837, 295, 309, 370, 1780, 322, 321, 393, 352], "temperature": 0.0, "avg_logprob": -0.2340150501417077, "compression_ratio": 1.808888888888889, "no_speech_prob": 2.3320683339989046e-06}, {"id": 160, "seek": 75040, "start": 766.88, "end": 775.68, "text": " Dot backward and then print dot grad and find out the gradient right and so you notice we never defined the gradient", "tokens": [38753, 23897, 293, 550, 4482, 5893, 2771, 293, 915, 484, 264, 16235, 558, 293, 370, 291, 3449, 321, 1128, 7642, 264, 16235], "temperature": 0.0, "avg_logprob": -0.2340150501417077, "compression_ratio": 1.808888888888889, "no_speech_prob": 2.3320683339989046e-06}, {"id": 161, "seek": 75040, "start": 775.68, "end": 778.52, "text": " We just defined it as being x plus 2", "tokens": [492, 445, 7642, 309, 382, 885, 2031, 1804, 568], "temperature": 0.0, "avg_logprob": -0.2340150501417077, "compression_ratio": 1.808888888888889, "no_speech_prob": 2.3320683339989046e-06}, {"id": 162, "seek": 77852, "start": 778.52, "end": 779.56, "text": " 2", "tokens": [568], "temperature": 0.0, "avg_logprob": -0.2055962880452474, "compression_ratio": 1.5337423312883436, "no_speech_prob": 6.179386105031881e-07}, {"id": 163, "seek": 77852, "start": 779.56, "end": 784.64, "text": " Squared times 3 whatever and it can calculate the gradient", "tokens": [8683, 1642, 1413, 805, 2035, 293, 309, 393, 8873, 264, 16235], "temperature": 0.0, "avg_logprob": -0.2055962880452474, "compression_ratio": 1.5337423312883436, "no_speech_prob": 6.179386105031881e-07}, {"id": 164, "seek": 77852, "start": 792.12, "end": 796.84, "text": " Okay, so that's why we need to turn that into a variable so L is now a", "tokens": [1033, 11, 370, 300, 311, 983, 321, 643, 281, 1261, 300, 666, 257, 7006, 370, 441, 307, 586, 257], "temperature": 0.0, "avg_logprob": -0.2055962880452474, "compression_ratio": 1.5337423312883436, "no_speech_prob": 6.179386105031881e-07}, {"id": 165, "seek": 77852, "start": 798.0, "end": 804.8199999999999, "text": " Variable containing the loss so it contains a single number for this mini-batch, which is the loss for this mini-batch", "tokens": [32511, 712, 19273, 264, 4470, 370, 309, 8306, 257, 2167, 1230, 337, 341, 8382, 12, 65, 852, 11, 597, 307, 264, 4470, 337, 341, 8382, 12, 65, 852], "temperature": 0.0, "avg_logprob": -0.2055962880452474, "compression_ratio": 1.5337423312883436, "no_speech_prob": 6.179386105031881e-07}, {"id": 166, "seek": 80482, "start": 804.82, "end": 810.7800000000001, "text": " But it's not just a number. It's a it's a number as a variable, so it's a number that knows how it was calculated", "tokens": [583, 309, 311, 406, 445, 257, 1230, 13, 467, 311, 257, 309, 311, 257, 1230, 382, 257, 7006, 11, 370, 309, 311, 257, 1230, 300, 3255, 577, 309, 390, 15598], "temperature": 0.0, "avg_logprob": -0.1775634834565312, "compression_ratio": 1.6243386243386244, "no_speech_prob": 4.247029323778406e-07}, {"id": 167, "seek": 80482, "start": 811.38, "end": 812.96, "text": " right", "tokens": [558], "temperature": 0.0, "avg_logprob": -0.1775634834565312, "compression_ratio": 1.6243386243386244, "no_speech_prob": 4.247029323778406e-07}, {"id": 168, "seek": 80482, "start": 812.96, "end": 817.0600000000001, "text": " So we're going to append that loss to our array just so we can", "tokens": [407, 321, 434, 516, 281, 34116, 300, 4470, 281, 527, 10225, 445, 370, 321, 393], "temperature": 0.0, "avg_logprob": -0.1775634834565312, "compression_ratio": 1.6243386243386244, "no_speech_prob": 4.247029323778406e-07}, {"id": 169, "seek": 80482, "start": 817.94, "end": 819.94, "text": " Get the average of it later basically", "tokens": [3240, 264, 4274, 295, 309, 1780, 1936], "temperature": 0.0, "avg_logprob": -0.1775634834565312, "compression_ratio": 1.6243386243386244, "no_speech_prob": 4.247029323778406e-07}, {"id": 170, "seek": 80482, "start": 822.0600000000001, "end": 826.88, "text": " And now we're going to calculate the gradient so L dot backward is the thing that says", "tokens": [400, 586, 321, 434, 516, 281, 8873, 264, 16235, 370, 441, 5893, 23897, 307, 264, 551, 300, 1619], "temperature": 0.0, "avg_logprob": -0.1775634834565312, "compression_ratio": 1.6243386243386244, "no_speech_prob": 4.247029323778406e-07}, {"id": 171, "seek": 82688, "start": 826.88, "end": 833.08, "text": " Calculate the gradient so remember when we call the the network", "tokens": [3511, 2444, 473, 264, 16235, 370, 1604, 562, 321, 818, 264, 264, 3209], "temperature": 0.0, "avg_logprob": -0.1350468016162361, "compression_ratio": 1.7829787234042553, "no_speech_prob": 2.8573085728567094e-06}, {"id": 172, "seek": 82688, "start": 833.08, "end": 835.6, "text": " It's actually calling our forward function", "tokens": [467, 311, 767, 5141, 527, 2128, 2445], "temperature": 0.0, "avg_logprob": -0.1350468016162361, "compression_ratio": 1.7829787234042553, "no_speech_prob": 2.8573085728567094e-06}, {"id": 173, "seek": 82688, "start": 835.6, "end": 842.9399999999999, "text": " So that's like cat go through it forward and then backward is like using the chain rule to calculate the gradients backwards", "tokens": [407, 300, 311, 411, 3857, 352, 807, 309, 2128, 293, 550, 23897, 307, 411, 1228, 264, 5021, 4978, 281, 8873, 264, 2771, 2448, 12204], "temperature": 0.0, "avg_logprob": -0.1350468016162361, "compression_ratio": 1.7829787234042553, "no_speech_prob": 2.8573085728567094e-06}, {"id": 174, "seek": 82688, "start": 843.48, "end": 850.98, "text": " Okay, and then this is the thing we're about to write which is update the weights based on the gradients and the learning rate, okay?", "tokens": [1033, 11, 293, 550, 341, 307, 264, 551, 321, 434, 466, 281, 2464, 597, 307, 5623, 264, 17443, 2361, 322, 264, 2771, 2448, 293, 264, 2539, 3314, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.1350468016162361, "compression_ratio": 1.7829787234042553, "no_speech_prob": 2.8573085728567094e-06}, {"id": 175, "seek": 82688, "start": 851.88, "end": 854.56, "text": " Zero grad will explain when we write this out by hand", "tokens": [17182, 2771, 486, 2903, 562, 321, 2464, 341, 484, 538, 1011], "temperature": 0.0, "avg_logprob": -0.1350468016162361, "compression_ratio": 1.7829787234042553, "no_speech_prob": 2.8573085728567094e-06}, {"id": 176, "seek": 85456, "start": 854.56, "end": 856.56, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.21296084204385446, "compression_ratio": 1.6169154228855722, "no_speech_prob": 2.0904519715259084e-06}, {"id": 177, "seek": 85456, "start": 856.7199999999999, "end": 862.1999999999999, "text": " And so then at the end we can turn our validation data loader into an iterator and", "tokens": [400, 370, 550, 412, 264, 917, 321, 393, 1261, 527, 24071, 1412, 3677, 260, 666, 364, 17138, 1639, 293], "temperature": 0.0, "avg_logprob": -0.21296084204385446, "compression_ratio": 1.6169154228855722, "no_speech_prob": 2.0904519715259084e-06}, {"id": 178, "seek": 85456, "start": 862.92, "end": 865.4799999999999, "text": " We can then go through its length", "tokens": [492, 393, 550, 352, 807, 1080, 4641], "temperature": 0.0, "avg_logprob": -0.21296084204385446, "compression_ratio": 1.6169154228855722, "no_speech_prob": 2.0904519715259084e-06}, {"id": 179, "seek": 85456, "start": 866.4799999999999, "end": 868.4, "text": " grabbing each", "tokens": [23771, 1184], "temperature": 0.0, "avg_logprob": -0.21296084204385446, "compression_ratio": 1.6169154228855722, "no_speech_prob": 2.0904519715259084e-06}, {"id": 180, "seek": 85456, "start": 868.4, "end": 870.4, "text": " X and y out of that and", "tokens": [1783, 293, 288, 484, 295, 300, 293], "temperature": 0.0, "avg_logprob": -0.21296084204385446, "compression_ratio": 1.6169154228855722, "no_speech_prob": 2.0904519715259084e-06}, {"id": 181, "seek": 85456, "start": 871.0, "end": 873.0, "text": " asking for the score", "tokens": [3365, 337, 264, 6175], "temperature": 0.0, "avg_logprob": -0.21296084204385446, "compression_ratio": 1.6169154228855722, "no_speech_prob": 2.0904519715259084e-06}, {"id": 182, "seek": 85456, "start": 873.2399999999999, "end": 875.64, "text": " which we defined up here to be equal to", "tokens": [597, 321, 7642, 493, 510, 281, 312, 2681, 281], "temperature": 0.0, "avg_logprob": -0.21296084204385446, "compression_ratio": 1.6169154228855722, "no_speech_prob": 2.0904519715259084e-06}, {"id": 183, "seek": 87564, "start": 875.64, "end": 883.36, "text": " Which thing did you predict which thing was actual and so check whether they're equal right and then the", "tokens": [3013, 551, 630, 291, 6069, 597, 551, 390, 3539, 293, 370, 1520, 1968, 436, 434, 2681, 558, 293, 550, 264], "temperature": 0.0, "avg_logprob": -0.36826822757720945, "compression_ratio": 1.5714285714285714, "no_speech_prob": 2.1907744667259976e-06}, {"id": 184, "seek": 87564, "start": 884.88, "end": 889.0, "text": " Main of that is going to be our accuracy, okay?", "tokens": [12383, 295, 300, 307, 516, 281, 312, 527, 14170, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.36826822757720945, "compression_ratio": 1.5714285714285714, "no_speech_prob": 2.1907744667259976e-06}, {"id": 185, "seek": 87564, "start": 889.6, "end": 891.76, "text": " Could you pass that over to Chen Shi?", "tokens": [7497, 291, 1320, 300, 670, 281, 13682, 25580, 30], "temperature": 0.0, "avg_logprob": -0.36826822757720945, "compression_ratio": 1.5714285714285714, "no_speech_prob": 2.1907744667259976e-06}, {"id": 186, "seek": 87564, "start": 893.6, "end": 899.0, "text": " What's the advantage that you converted into iterator residents like use normal?", "tokens": [708, 311, 264, 5002, 300, 291, 16424, 666, 17138, 1639, 9630, 411, 764, 2710, 30], "temperature": 0.0, "avg_logprob": -0.36826822757720945, "compression_ratio": 1.5714285714285714, "no_speech_prob": 2.1907744667259976e-06}, {"id": 187, "seek": 87564, "start": 900.08, "end": 902.08, "text": " Python loop or", "tokens": [15329, 6367, 420], "temperature": 0.0, "avg_logprob": -0.36826822757720945, "compression_ratio": 1.5714285714285714, "no_speech_prob": 2.1907744667259976e-06}, {"id": 188, "seek": 87564, "start": 902.08, "end": 904.12, "text": " We're using a normal Python loop", "tokens": [492, 434, 1228, 257, 2710, 15329, 6367], "temperature": 0.0, "avg_logprob": -0.36826822757720945, "compression_ratio": 1.5714285714285714, "no_speech_prob": 2.1907744667259976e-06}, {"id": 189, "seek": 90412, "start": 904.12, "end": 908.68, "text": " Yeah, so it's Dylan. This is a normal Python loop so the question really is like", "tokens": [865, 11, 370, 309, 311, 28160, 13, 639, 307, 257, 2710, 15329, 6367, 370, 264, 1168, 534, 307, 411], "temperature": 0.0, "avg_logprob": -0.19641131848360585, "compression_ratio": 1.6199261992619927, "no_speech_prob": 2.5612719127821038e-06}, {"id": 190, "seek": 90412, "start": 909.96, "end": 912.58, "text": " Compared to what right so like", "tokens": [30539, 281, 437, 558, 370, 411], "temperature": 0.0, "avg_logprob": -0.19641131848360585, "compression_ratio": 1.6199261992619927, "no_speech_prob": 2.5612719127821038e-06}, {"id": 191, "seek": 90412, "start": 913.36, "end": 919.92, "text": " The alternative perhaps you're thinking it would be like we could use like a something like a list with an indexer, okay?", "tokens": [440, 8535, 4317, 291, 434, 1953, 309, 576, 312, 411, 321, 727, 764, 411, 257, 746, 411, 257, 1329, 365, 364, 8186, 260, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.19641131848360585, "compression_ratio": 1.6199261992619927, "no_speech_prob": 2.5612719127821038e-06}, {"id": 192, "seek": 90412, "start": 919.92, "end": 923.6, "text": " So you know the problem there is that we want", "tokens": [407, 291, 458, 264, 1154, 456, 307, 300, 321, 528], "temperature": 0.0, "avg_logprob": -0.19641131848360585, "compression_ratio": 1.6199261992619927, "no_speech_prob": 2.5612719127821038e-06}, {"id": 193, "seek": 90412, "start": 924.6, "end": 928.5600000000001, "text": " Was a few things I mean one key one is we want each time we grab a new mini-batch", "tokens": [3027, 257, 1326, 721, 286, 914, 472, 2141, 472, 307, 321, 528, 1184, 565, 321, 4444, 257, 777, 8382, 12, 65, 852], "temperature": 0.0, "avg_logprob": -0.19641131848360585, "compression_ratio": 1.6199261992619927, "no_speech_prob": 2.5612719127821038e-06}, {"id": 194, "seek": 92856, "start": 928.56, "end": 936.0, "text": " We want to be random. We want a different different shuffled thing so this you can actually kind of iterate from", "tokens": [492, 528, 281, 312, 4974, 13, 492, 528, 257, 819, 819, 402, 33974, 551, 370, 341, 291, 393, 767, 733, 295, 44497, 490], "temperature": 0.0, "avg_logprob": -0.1644160515439194, "compression_ratio": 1.8566037735849057, "no_speech_prob": 7.81147036832408e-07}, {"id": 195, "seek": 92856, "start": 936.76, "end": 940.16, "text": " Forever you know you can loop through it as many times as you like so", "tokens": [30703, 291, 458, 291, 393, 6367, 807, 309, 382, 867, 1413, 382, 291, 411, 370], "temperature": 0.0, "avg_logprob": -0.1644160515439194, "compression_ratio": 1.8566037735849057, "no_speech_prob": 7.81147036832408e-07}, {"id": 196, "seek": 92856, "start": 941.04, "end": 944.16, "text": " There's this kind of idea. It's called different things in different languages", "tokens": [821, 311, 341, 733, 295, 1558, 13, 467, 311, 1219, 819, 721, 294, 819, 8650], "temperature": 0.0, "avg_logprob": -0.1644160515439194, "compression_ratio": 1.8566037735849057, "no_speech_prob": 7.81147036832408e-07}, {"id": 197, "seek": 92856, "start": 944.4399999999999, "end": 947.52, "text": " But a lot of languages are called like stream processing", "tokens": [583, 257, 688, 295, 8650, 366, 1219, 411, 4309, 9007], "temperature": 0.0, "avg_logprob": -0.1644160515439194, "compression_ratio": 1.8566037735849057, "no_speech_prob": 7.81147036832408e-07}, {"id": 198, "seek": 92856, "start": 947.52, "end": 951.78, "text": " And it's this basic idea that rather than saying I want the third thing or the ninth thing", "tokens": [400, 309, 311, 341, 3875, 1558, 300, 2831, 813, 1566, 286, 528, 264, 2636, 551, 420, 264, 28207, 551], "temperature": 0.0, "avg_logprob": -0.1644160515439194, "compression_ratio": 1.8566037735849057, "no_speech_prob": 7.81147036832408e-07}, {"id": 199, "seek": 92856, "start": 951.78, "end": 955.8, "text": " It's just like I want the next thing right it's great for like network programming", "tokens": [467, 311, 445, 411, 286, 528, 264, 958, 551, 558, 309, 311, 869, 337, 411, 3209, 9410], "temperature": 0.0, "avg_logprob": -0.1644160515439194, "compression_ratio": 1.8566037735849057, "no_speech_prob": 7.81147036832408e-07}, {"id": 200, "seek": 95580, "start": 955.8, "end": 959.4799999999999, "text": " It's like grab the next thing from the network. It's great for", "tokens": [467, 311, 411, 4444, 264, 958, 551, 490, 264, 3209, 13, 467, 311, 869, 337], "temperature": 0.0, "avg_logprob": -0.19316655939275568, "compression_ratio": 1.7246376811594204, "no_speech_prob": 2.5215592813765397e-06}, {"id": 201, "seek": 95580, "start": 960.68, "end": 966.4, "text": " UI programming it's like grab the next event whether somebody clicked a button it also turns out to be great for", "tokens": [15682, 9410, 309, 311, 411, 4444, 264, 958, 2280, 1968, 2618, 23370, 257, 2960, 309, 611, 4523, 484, 281, 312, 869, 337], "temperature": 0.0, "avg_logprob": -0.19316655939275568, "compression_ratio": 1.7246376811594204, "no_speech_prob": 2.5215592813765397e-06}, {"id": 202, "seek": 95580, "start": 967.04, "end": 970.3599999999999, "text": " This kind of numeric programming. It's like I just want the next batch of data", "tokens": [639, 733, 295, 7866, 299, 9410, 13, 467, 311, 411, 286, 445, 528, 264, 958, 15245, 295, 1412], "temperature": 0.0, "avg_logprob": -0.19316655939275568, "compression_ratio": 1.7246376811594204, "no_speech_prob": 2.5215592813765397e-06}, {"id": 203, "seek": 95580, "start": 972.0799999999999, "end": 978.52, "text": " It means that the data like can be kind of arbitrarily long as we're just grabbing one piece at a time", "tokens": [467, 1355, 300, 264, 1412, 411, 393, 312, 733, 295, 19071, 3289, 938, 382, 321, 434, 445, 23771, 472, 2522, 412, 257, 565], "temperature": 0.0, "avg_logprob": -0.19316655939275568, "compression_ratio": 1.7246376811594204, "no_speech_prob": 2.5215592813765397e-06}, {"id": 204, "seek": 97852, "start": 978.52, "end": 986.88, "text": " Yeah, so you know I mean and also I guess the short answer is because it's how pytorch works", "tokens": [865, 11, 370, 291, 458, 286, 914, 293, 611, 286, 2041, 264, 2099, 1867, 307, 570, 309, 311, 577, 25878, 284, 339, 1985], "temperature": 0.0, "avg_logprob": -0.3196584979693095, "compression_ratio": 1.6666666666666667, "no_speech_prob": 4.710857865575235e-06}, {"id": 205, "seek": 97852, "start": 987.4, "end": 990.36, "text": " pytorch that's pytorch as data loaders are designed to be", "tokens": [25878, 284, 339, 300, 311, 25878, 284, 339, 382, 1412, 3677, 433, 366, 4761, 281, 312], "temperature": 0.0, "avg_logprob": -0.3196584979693095, "compression_ratio": 1.6666666666666667, "no_speech_prob": 4.710857865575235e-06}, {"id": 206, "seek": 97852, "start": 990.88, "end": 995.68, "text": " Called in this way and then so python has this concept of a generator", "tokens": [45001, 294, 341, 636, 293, 550, 370, 38797, 575, 341, 3410, 295, 257, 19265], "temperature": 0.0, "avg_logprob": -0.3196584979693095, "compression_ratio": 1.6666666666666667, "no_speech_prob": 4.710857865575235e-06}, {"id": 207, "seek": 97852, "start": 996.48, "end": 998.48, "text": " which is like an", "tokens": [597, 307, 411, 364], "temperature": 0.0, "avg_logprob": -0.3196584979693095, "compression_ratio": 1.6666666666666667, "no_speech_prob": 4.710857865575235e-06}, {"id": 208, "seek": 97852, "start": 998.92, "end": 1004.68, "text": " Different type of generator. I wonder if this is gonna be a snake generator or a computer generator, okay a", "tokens": [20825, 2010, 295, 19265, 13, 286, 2441, 498, 341, 307, 799, 312, 257, 12650, 19265, 420, 257, 3820, 19265, 11, 1392, 257], "temperature": 0.0, "avg_logprob": -0.3196584979693095, "compression_ratio": 1.6666666666666667, "no_speech_prob": 4.710857865575235e-06}, {"id": 209, "seek": 97852, "start": 1005.56, "end": 1007.3199999999999, "text": " generator is a", "tokens": [19265, 307, 257], "temperature": 0.0, "avg_logprob": -0.3196584979693095, "compression_ratio": 1.6666666666666667, "no_speech_prob": 4.710857865575235e-06}, {"id": 210, "seek": 100732, "start": 1007.32, "end": 1013.94, "text": " Way that you can create a function that as it says behaves like an iterator so black python has recognized that this stream", "tokens": [9558, 300, 291, 393, 1884, 257, 2445, 300, 382, 309, 1619, 36896, 411, 364, 17138, 1639, 370, 2211, 38797, 575, 9823, 300, 341, 4309], "temperature": 0.0, "avg_logprob": -0.20294691721598307, "compression_ratio": 1.7224489795918367, "no_speech_prob": 1.7061740891222144e-06}, {"id": 211, "seek": 100732, "start": 1014.5600000000001, "end": 1017.86, "text": " processing approach to programming is like super handy and helpful and", "tokens": [9007, 3109, 281, 9410, 307, 411, 1687, 13239, 293, 4961, 293], "temperature": 0.0, "avg_logprob": -0.20294691721598307, "compression_ratio": 1.7224489795918367, "no_speech_prob": 1.7061740891222144e-06}, {"id": 212, "seek": 100732, "start": 1018.44, "end": 1025.8, "text": " Supports it everywhere so basically anywhere that you use a for in loop anyway use a list comprehension", "tokens": [9391, 3299, 309, 5315, 370, 1936, 4992, 300, 291, 764, 257, 337, 294, 6367, 4033, 764, 257, 1329, 44991], "temperature": 0.0, "avg_logprob": -0.20294691721598307, "compression_ratio": 1.7224489795918367, "no_speech_prob": 1.7061740891222144e-06}, {"id": 213, "seek": 100732, "start": 1026.44, "end": 1032.28, "text": " Those things can always be generators or iterators so by programming this way we just get a lot of them", "tokens": [3950, 721, 393, 1009, 312, 38662, 420, 17138, 3391, 370, 538, 9410, 341, 636, 321, 445, 483, 257, 688, 295, 552], "temperature": 0.0, "avg_logprob": -0.20294691721598307, "compression_ratio": 1.7224489795918367, "no_speech_prob": 1.7061740891222144e-06}, {"id": 214, "seek": 100732, "start": 1033.28, "end": 1035.28, "text": " flexibility I guess", "tokens": [12635, 286, 2041], "temperature": 0.0, "avg_logprob": -0.20294691721598307, "compression_ratio": 1.7224489795918367, "no_speech_prob": 1.7061740891222144e-06}, {"id": 215, "seek": 103528, "start": 1035.28, "end": 1038.98, "text": " Is that sound about right Terrence you're the programming language expert did you?", "tokens": [1119, 300, 1626, 466, 558, 6564, 10760, 291, 434, 264, 9410, 2856, 5844, 630, 291, 30], "temperature": 0.0, "avg_logprob": -0.1858010402945585, "compression_ratio": 1.8269896193771626, "no_speech_prob": 1.01298619483714e-05}, {"id": 216, "seek": 103528, "start": 1039.72, "end": 1041.72, "text": " Do you want to grab the box so we can hear?", "tokens": [1144, 291, 528, 281, 4444, 264, 2424, 370, 321, 393, 1568, 30], "temperature": 0.0, "avg_logprob": -0.1858010402945585, "compression_ratio": 1.8269896193771626, "no_speech_prob": 1.01298619483714e-05}, {"id": 217, "seek": 103528, "start": 1042.68, "end": 1049.36, "text": " So Terrence actually does programming languages for a living so we should ask him. Yeah, I mean the short answer is what you said", "tokens": [407, 6564, 10760, 767, 775, 9410, 8650, 337, 257, 2647, 370, 321, 820, 1029, 796, 13, 865, 11, 286, 914, 264, 2099, 1867, 307, 437, 291, 848], "temperature": 0.0, "avg_logprob": -0.1858010402945585, "compression_ratio": 1.8269896193771626, "no_speech_prob": 1.01298619483714e-05}, {"id": 218, "seek": 103528, "start": 1050.76, "end": 1052.3999999999999, "text": " You might say something about space", "tokens": [509, 1062, 584, 746, 466, 1901], "temperature": 0.0, "avg_logprob": -0.1858010402945585, "compression_ratio": 1.8269896193771626, "no_speech_prob": 1.01298619483714e-05}, {"id": 219, "seek": 103528, "start": 1052.3999999999999, "end": 1059.16, "text": " But in this case that all that data has to be in memory anyway because we've got no doesn't have to be in memory", "tokens": [583, 294, 341, 1389, 300, 439, 300, 1412, 575, 281, 312, 294, 4675, 4033, 570, 321, 600, 658, 572, 1177, 380, 362, 281, 312, 294, 4675], "temperature": 0.0, "avg_logprob": -0.1858010402945585, "compression_ratio": 1.8269896193771626, "no_speech_prob": 1.01298619483714e-05}, {"id": 220, "seek": 103528, "start": 1059.16, "end": 1064.94, "text": " So in fact most of the time we could pull a mini batch from something in fact most of the time with pytorch the mini batch", "tokens": [407, 294, 1186, 881, 295, 264, 565, 321, 727, 2235, 257, 8382, 15245, 490, 746, 294, 1186, 881, 295, 264, 565, 365, 25878, 284, 339, 264, 8382, 15245], "temperature": 0.0, "avg_logprob": -0.1858010402945585, "compression_ratio": 1.8269896193771626, "no_speech_prob": 1.01298619483714e-05}, {"id": 221, "seek": 106494, "start": 1064.94, "end": 1071.68, "text": " Will be read from like separate images spread over your disk on demand so most of the time it's not in memory", "tokens": [3099, 312, 1401, 490, 411, 4994, 5267, 3974, 670, 428, 12355, 322, 4733, 370, 881, 295, 264, 565, 309, 311, 406, 294, 4675], "temperature": 0.0, "avg_logprob": -0.19358796563767294, "compression_ratio": 1.835483870967742, "no_speech_prob": 2.2252700091485167e-06}, {"id": 222, "seek": 106494, "start": 1071.8, "end": 1078.28, "text": " But in general you want to keep as little in memory as possible at a time and so the idea of stream processing", "tokens": [583, 294, 2674, 291, 528, 281, 1066, 382, 707, 294, 4675, 382, 1944, 412, 257, 565, 293, 370, 264, 1558, 295, 4309, 9007], "temperature": 0.0, "avg_logprob": -0.19358796563767294, "compression_ratio": 1.835483870967742, "no_speech_prob": 2.2252700091485167e-06}, {"id": 223, "seek": 106494, "start": 1078.28, "end": 1083.4, "text": " Also is great because you can do compositions you can pipe the data to a different machine you can yeah", "tokens": [2743, 307, 869, 570, 291, 393, 360, 43401, 291, 393, 11240, 264, 1412, 281, 257, 819, 3479, 291, 393, 1338], "temperature": 0.0, "avg_logprob": -0.19358796563767294, "compression_ratio": 1.835483870967742, "no_speech_prob": 2.2252700091485167e-06}, {"id": 224, "seek": 106494, "start": 1083.64, "end": 1085.0, "text": " Yeah, the composition is great", "tokens": [865, 11, 264, 12686, 307, 869], "temperature": 0.0, "avg_logprob": -0.19358796563767294, "compression_ratio": 1.835483870967742, "no_speech_prob": 2.2252700091485167e-06}, {"id": 225, "seek": 106494, "start": 1085.0, "end": 1089.4, "text": " You can grab the next thing from here and then send it off to the next dream which can then grab it and do something", "tokens": [509, 393, 4444, 264, 958, 551, 490, 510, 293, 550, 2845, 309, 766, 281, 264, 958, 3055, 597, 393, 550, 4444, 309, 293, 360, 746], "temperature": 0.0, "avg_logprob": -0.19358796563767294, "compression_ratio": 1.835483870967742, "no_speech_prob": 2.2252700091485167e-06}, {"id": 226, "seek": 108940, "start": 1089.4, "end": 1095.3200000000002, "text": " Else which you guys all recognize of course in the command line pipes and I'll redirection yes", "tokens": [45472, 597, 291, 1074, 439, 5521, 295, 1164, 294, 264, 5622, 1622, 21882, 293, 286, 603, 2182, 621, 882, 2086], "temperature": 0.0, "avg_logprob": -0.18228374827991833, "compression_ratio": 1.616600790513834, "no_speech_prob": 3.4465629141777754e-06}, {"id": 227, "seek": 108940, "start": 1095.6000000000001, "end": 1097.6000000000001, "text": " Okay, thanks Terrence", "tokens": [1033, 11, 3231, 6564, 10760], "temperature": 0.0, "avg_logprob": -0.18228374827991833, "compression_ratio": 1.616600790513834, "no_speech_prob": 3.4465629141777754e-06}, {"id": 228, "seek": 108940, "start": 1099.1200000000001, "end": 1101.9, "text": " The benefit of working with people that actually know what they're talking about", "tokens": [440, 5121, 295, 1364, 365, 561, 300, 767, 458, 437, 436, 434, 1417, 466], "temperature": 0.0, "avg_logprob": -0.18228374827991833, "compression_ratio": 1.616600790513834, "no_speech_prob": 3.4465629141777754e-06}, {"id": 229, "seek": 108940, "start": 1103.44, "end": 1108.3600000000001, "text": " All right, so let's now take that and get rid of the optimizer", "tokens": [1057, 558, 11, 370, 718, 311, 586, 747, 300, 293, 483, 3973, 295, 264, 5028, 6545], "temperature": 0.0, "avg_logprob": -0.18228374827991833, "compression_ratio": 1.616600790513834, "no_speech_prob": 3.4465629141777754e-06}, {"id": 230, "seek": 108940, "start": 1108.88, "end": 1113.96, "text": " Okay, so the only thing that we're going to be left with is the negative log likelihood loss function", "tokens": [1033, 11, 370, 264, 787, 551, 300, 321, 434, 516, 281, 312, 1411, 365, 307, 264, 3671, 3565, 22119, 4470, 2445], "temperature": 0.0, "avg_logprob": -0.18228374827991833, "compression_ratio": 1.616600790513834, "no_speech_prob": 3.4465629141777754e-06}, {"id": 231, "seek": 108940, "start": 1114.8400000000001, "end": 1117.76, "text": " which we could also replace actually we have a", "tokens": [597, 321, 727, 611, 7406, 767, 321, 362, 257], "temperature": 0.0, "avg_logprob": -0.18228374827991833, "compression_ratio": 1.616600790513834, "no_speech_prob": 3.4465629141777754e-06}, {"id": 232, "seek": 111776, "start": 1117.76, "end": 1121.16, "text": " implementation of that from scratch that you're net wrote in the", "tokens": [11420, 295, 300, 490, 8459, 300, 291, 434, 2533, 4114, 294, 264], "temperature": 0.0, "avg_logprob": -0.15539679488515465, "compression_ratio": 1.77007299270073, "no_speech_prob": 4.93696597914095e-06}, {"id": 233, "seek": 111776, "start": 1122.0, "end": 1128.36, "text": " In the notebooks, so it's only one line of code as we learned earlier. You can do it with a single if statement, okay?", "tokens": [682, 264, 43782, 11, 370, 309, 311, 787, 472, 1622, 295, 3089, 382, 321, 3264, 3071, 13, 509, 393, 360, 309, 365, 257, 2167, 498, 5629, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.15539679488515465, "compression_ratio": 1.77007299270073, "no_speech_prob": 4.93696597914095e-06}, {"id": 234, "seek": 111776, "start": 1129.08, "end": 1131.84, "text": " So I don't know why I was so lazy as to include this", "tokens": [407, 286, 500, 380, 458, 983, 286, 390, 370, 14847, 382, 281, 4090, 341], "temperature": 0.0, "avg_logprob": -0.15539679488515465, "compression_ratio": 1.77007299270073, "no_speech_prob": 4.93696597914095e-06}, {"id": 235, "seek": 111776, "start": 1133.52, "end": 1138.8799999999999, "text": " So what we're going to do is we're going to again grab this module that we've written ourselves the logistic regression module", "tokens": [407, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 797, 4444, 341, 10088, 300, 321, 600, 3720, 4175, 264, 3565, 3142, 24590, 10088], "temperature": 0.0, "avg_logprob": -0.15539679488515465, "compression_ratio": 1.77007299270073, "no_speech_prob": 4.93696597914095e-06}, {"id": 236, "seek": 111776, "start": 1138.8799999999999, "end": 1146.24, "text": " We're going to have one epoch again. We're going to loop through each thing in an iterator again. We're going to grab our", "tokens": [492, 434, 516, 281, 362, 472, 30992, 339, 797, 13, 492, 434, 516, 281, 6367, 807, 1184, 551, 294, 364, 17138, 1639, 797, 13, 492, 434, 516, 281, 4444, 527], "temperature": 0.0, "avg_logprob": -0.15539679488515465, "compression_ratio": 1.77007299270073, "no_speech_prob": 4.93696597914095e-06}, {"id": 237, "seek": 114624, "start": 1146.24, "end": 1149.32, "text": " independent independent variable for the mini batch again", "tokens": [6695, 6695, 7006, 337, 264, 8382, 15245, 797], "temperature": 0.0, "avg_logprob": -0.1950421785053454, "compression_ratio": 1.7442922374429224, "no_speech_prob": 4.157351668254705e-06}, {"id": 238, "seek": 114624, "start": 1149.84, "end": 1151.84, "text": " Pass it into our network again", "tokens": [10319, 309, 666, 527, 3209, 797], "temperature": 0.0, "avg_logprob": -0.1950421785053454, "compression_ratio": 1.7442922374429224, "no_speech_prob": 4.157351668254705e-06}, {"id": 239, "seek": 114624, "start": 1152.32, "end": 1158.04, "text": " Calculate the loss so this is all the same as before but now we're going to get rid of this optimizer dot step", "tokens": [3511, 2444, 473, 264, 4470, 370, 341, 307, 439, 264, 912, 382, 949, 457, 586, 321, 434, 516, 281, 483, 3973, 295, 341, 5028, 6545, 5893, 1823], "temperature": 0.0, "avg_logprob": -0.1950421785053454, "compression_ratio": 1.7442922374429224, "no_speech_prob": 4.157351668254705e-06}, {"id": 240, "seek": 114624, "start": 1159.16, "end": 1161.08, "text": " And we're going to do it by hand", "tokens": [400, 321, 434, 516, 281, 360, 309, 538, 1011], "temperature": 0.0, "avg_logprob": -0.1950421785053454, "compression_ratio": 1.7442922374429224, "no_speech_prob": 4.157351668254705e-06}, {"id": 241, "seek": 114624, "start": 1161.08, "end": 1163.52, "text": " so the basic trick is", "tokens": [370, 264, 3875, 4282, 307], "temperature": 0.0, "avg_logprob": -0.1950421785053454, "compression_ratio": 1.7442922374429224, "no_speech_prob": 4.157351668254705e-06}, {"id": 242, "seek": 114624, "start": 1164.68, "end": 1170.32, "text": " As I mentioned we're not going to do the calculus by hand so we'll call L dot backward to calculate the gradients automatically", "tokens": [1018, 286, 2835, 321, 434, 406, 516, 281, 360, 264, 33400, 538, 1011, 370, 321, 603, 818, 441, 5893, 23897, 281, 8873, 264, 2771, 2448, 6772], "temperature": 0.0, "avg_logprob": -0.1950421785053454, "compression_ratio": 1.7442922374429224, "no_speech_prob": 4.157351668254705e-06}, {"id": 243, "seek": 117032, "start": 1170.32, "end": 1176.32, "text": " And that's going to fill in our weight matrix, so do you remember when we created our?", "tokens": [400, 300, 311, 516, 281, 2836, 294, 527, 3364, 8141, 11, 370, 360, 291, 1604, 562, 321, 2942, 527, 30], "temperature": 0.0, "avg_logprob": -0.21237992679371553, "compression_ratio": 1.6868131868131868, "no_speech_prob": 2.443981429678388e-06}, {"id": 244, "seek": 117032, "start": 1176.96, "end": 1178.76, "text": " let's go back and", "tokens": [718, 311, 352, 646, 293], "temperature": 0.0, "avg_logprob": -0.21237992679371553, "compression_ratio": 1.6868131868131868, "no_speech_prob": 2.443981429678388e-06}, {"id": 245, "seek": 117032, "start": 1178.76, "end": 1180.76, "text": " look at the code for", "tokens": [574, 412, 264, 3089, 337], "temperature": 0.0, "avg_logprob": -0.21237992679371553, "compression_ratio": 1.6868131868131868, "no_speech_prob": 2.443981429678388e-06}, {"id": 246, "seek": 117032, "start": 1184.36, "end": 1188.48, "text": " Here's that module we built so the weight matrix for the for the", "tokens": [1692, 311, 300, 10088, 321, 3094, 370, 264, 3364, 8141, 337, 264, 337, 264], "temperature": 0.0, "avg_logprob": -0.21237992679371553, "compression_ratio": 1.6868131868131868, "no_speech_prob": 2.443981429678388e-06}, {"id": 247, "seek": 117032, "start": 1189.36, "end": 1196.4199999999998, "text": " Linear layer weights we called L1w and for the bias we called L1b. All right, so they were the attributes we created", "tokens": [14670, 289, 4583, 17443, 321, 1219, 441, 16, 86, 293, 337, 264, 12577, 321, 1219, 441, 16, 65, 13, 1057, 558, 11, 370, 436, 645, 264, 17212, 321, 2942], "temperature": 0.0, "avg_logprob": -0.21237992679371553, "compression_ratio": 1.6868131868131868, "no_speech_prob": 2.443981429678388e-06}, {"id": 248, "seek": 119642, "start": 1196.42, "end": 1198.42, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.20096236903492998, "compression_ratio": 1.7643979057591623, "no_speech_prob": 1.2289136748222518e-06}, {"id": 249, "seek": 119642, "start": 1202.1000000000001, "end": 1210.0600000000002, "text": " I've just put them into things called W and B just to save some typing basically so W was our weights B is our biases and", "tokens": [286, 600, 445, 829, 552, 666, 721, 1219, 343, 293, 363, 445, 281, 3155, 512, 18444, 1936, 370, 343, 390, 527, 17443, 363, 307, 527, 32152, 293], "temperature": 0.0, "avg_logprob": -0.20096236903492998, "compression_ratio": 1.7643979057591623, "no_speech_prob": 1.2289136748222518e-06}, {"id": 250, "seek": 119642, "start": 1210.8600000000001, "end": 1217.78, "text": " So the weights remember the weights are a variable and to get the tensor out of the variable we have to use dot data", "tokens": [407, 264, 17443, 1604, 264, 17443, 366, 257, 7006, 293, 281, 483, 264, 40863, 484, 295, 264, 7006, 321, 362, 281, 764, 5893, 1412], "temperature": 0.0, "avg_logprob": -0.20096236903492998, "compression_ratio": 1.7643979057591623, "no_speech_prob": 1.2289136748222518e-06}, {"id": 251, "seek": 119642, "start": 1218.0600000000002, "end": 1222.94, "text": " Right so we want to update the actual tensor that's in this variable so we say weights dot data", "tokens": [1779, 370, 321, 528, 281, 5623, 264, 3539, 40863, 300, 311, 294, 341, 7006, 370, 321, 584, 17443, 5893, 1412], "temperature": 0.0, "avg_logprob": -0.20096236903492998, "compression_ratio": 1.7643979057591623, "no_speech_prob": 1.2289136748222518e-06}, {"id": 252, "seek": 122294, "start": 1222.94, "end": 1228.8400000000001, "text": " Minus equals so we want to go in the opposite direction to the gradient the gradient tells us which way is up", "tokens": [2829, 301, 6915, 370, 321, 528, 281, 352, 294, 264, 6182, 3513, 281, 264, 16235, 264, 16235, 5112, 505, 597, 636, 307, 493], "temperature": 0.0, "avg_logprob": -0.21245491261384924, "compression_ratio": 1.8254716981132075, "no_speech_prob": 1.1544605058588786e-06}, {"id": 253, "seek": 122294, "start": 1229.02, "end": 1231.02, "text": " We want to go down", "tokens": [492, 528, 281, 352, 760], "temperature": 0.0, "avg_logprob": -0.21245491261384924, "compression_ratio": 1.8254716981132075, "no_speech_prob": 1.1544605058588786e-06}, {"id": 254, "seek": 122294, "start": 1232.06, "end": 1234.06, "text": " Whatever is currently in", "tokens": [8541, 307, 4362, 294], "temperature": 0.0, "avg_logprob": -0.21245491261384924, "compression_ratio": 1.8254716981132075, "no_speech_prob": 1.1544605058588786e-06}, {"id": 255, "seek": 122294, "start": 1234.46, "end": 1236.46, "text": " the gradients", "tokens": [264, 2771, 2448], "temperature": 0.0, "avg_logprob": -0.21245491261384924, "compression_ratio": 1.8254716981132075, "no_speech_prob": 1.1544605058588786e-06}, {"id": 256, "seek": 122294, "start": 1236.6200000000001, "end": 1240.3400000000001, "text": " Times the learning rate so that is the formula for", "tokens": [11366, 264, 2539, 3314, 370, 300, 307, 264, 8513, 337], "temperature": 0.0, "avg_logprob": -0.21245491261384924, "compression_ratio": 1.8254716981132075, "no_speech_prob": 1.1544605058588786e-06}, {"id": 257, "seek": 122294, "start": 1241.46, "end": 1243.26, "text": " gradient descent", "tokens": [16235, 23475], "temperature": 0.0, "avg_logprob": -0.21245491261384924, "compression_ratio": 1.8254716981132075, "no_speech_prob": 1.1544605058588786e-06}, {"id": 258, "seek": 122294, "start": 1243.26, "end": 1245.7, "text": " right so as you can see it's it's like as", "tokens": [558, 370, 382, 291, 393, 536, 309, 311, 309, 311, 411, 382], "temperature": 0.0, "avg_logprob": -0.21245491261384924, "compression_ratio": 1.8254716981132075, "no_speech_prob": 1.1544605058588786e-06}, {"id": 259, "seek": 124570, "start": 1245.7, "end": 1253.18, "text": " As as easier thing as you can possibly imagine it's like literally update the weights to be equal to be equal to whatever", "tokens": [1018, 382, 3571, 551, 382, 291, 393, 6264, 3811, 309, 311, 411, 3736, 5623, 264, 17443, 281, 312, 2681, 281, 312, 2681, 281, 2035], "temperature": 0.0, "avg_logprob": -0.25602928651582213, "compression_ratio": 1.7056451612903225, "no_speech_prob": 3.5008351915166713e-06}, {"id": 260, "seek": 124570, "start": 1253.18, "end": 1258.82, "text": " They are now minus the great the gradients times the learning rate and do the same thing", "tokens": [814, 366, 586, 3175, 264, 869, 264, 2771, 2448, 1413, 264, 2539, 3314, 293, 360, 264, 912, 551], "temperature": 0.0, "avg_logprob": -0.25602928651582213, "compression_ratio": 1.7056451612903225, "no_speech_prob": 3.5008351915166713e-06}, {"id": 261, "seek": 124570, "start": 1259.78, "end": 1261.78, "text": " for the bias", "tokens": [337, 264, 12577], "temperature": 0.0, "avg_logprob": -0.25602928651582213, "compression_ratio": 1.7056451612903225, "no_speech_prob": 3.5008351915166713e-06}, {"id": 262, "seek": 124570, "start": 1261.82, "end": 1266.02, "text": " So anybody have any questions about that step in terms of like why we do it or how did you have a question?", "tokens": [407, 4472, 362, 604, 1651, 466, 300, 1823, 294, 2115, 295, 411, 983, 321, 360, 309, 420, 577, 630, 291, 362, 257, 1168, 30], "temperature": 0.0, "avg_logprob": -0.25602928651582213, "compression_ratio": 1.7056451612903225, "no_speech_prob": 3.5008351915166713e-06}, {"id": 263, "seek": 124570, "start": 1266.02, "end": 1268.02, "text": " Do you want to grab that?", "tokens": [1144, 291, 528, 281, 4444, 300, 30], "temperature": 0.0, "avg_logprob": -0.25602928651582213, "compression_ratio": 1.7056451612903225, "no_speech_prob": 3.5008351915166713e-06}, {"id": 264, "seek": 124570, "start": 1268.3, "end": 1271.5800000000002, "text": " That step, but when we do the next of deal", "tokens": [663, 1823, 11, 457, 562, 321, 360, 264, 958, 295, 2028], "temperature": 0.0, "avg_logprob": -0.25602928651582213, "compression_ratio": 1.7056451612903225, "no_speech_prob": 3.5008351915166713e-06}, {"id": 265, "seek": 124570, "start": 1272.78, "end": 1274.78, "text": " The next year yes, yes", "tokens": [440, 958, 1064, 2086, 11, 2086], "temperature": 0.0, "avg_logprob": -0.25602928651582213, "compression_ratio": 1.7056451612903225, "no_speech_prob": 3.5008351915166713e-06}, {"id": 266, "seek": 127478, "start": 1274.78, "end": 1278.96, "text": " So when we use the end of the loop how do you grab the next element?", "tokens": [407, 562, 321, 764, 264, 917, 295, 264, 6367, 577, 360, 291, 4444, 264, 958, 4478, 30], "temperature": 0.0, "avg_logprob": -0.19875751071506076, "compression_ratio": 1.6927083333333333, "no_speech_prob": 7.934470431791851e-07}, {"id": 267, "seek": 127478, "start": 1279.66, "end": 1281.66, "text": " So this is going through each", "tokens": [407, 341, 307, 516, 807, 1184], "temperature": 0.0, "avg_logprob": -0.19875751071506076, "compression_ratio": 1.6927083333333333, "no_speech_prob": 7.934470431791851e-07}, {"id": 268, "seek": 127478, "start": 1283.8999999999999, "end": 1289.98, "text": " Each index in range of length so this is going 0 1 2 3 at the end of this loop", "tokens": [6947, 8186, 294, 3613, 295, 4641, 370, 341, 307, 516, 1958, 502, 568, 805, 412, 264, 917, 295, 341, 6367], "temperature": 0.0, "avg_logprob": -0.19875751071506076, "compression_ratio": 1.6927083333333333, "no_speech_prob": 7.934470431791851e-07}, {"id": 269, "seek": 127478, "start": 1290.62, "end": 1296.8999999999999, "text": " It's going to print out the mean of the validation set go back to the start of the epoch at which point", "tokens": [467, 311, 516, 281, 4482, 484, 264, 914, 295, 264, 24071, 992, 352, 646, 281, 264, 722, 295, 264, 30992, 339, 412, 597, 935], "temperature": 0.0, "avg_logprob": -0.19875751071506076, "compression_ratio": 1.6927083333333333, "no_speech_prob": 7.934470431791851e-07}, {"id": 270, "seek": 127478, "start": 1296.8999999999999, "end": 1299.7, "text": " It's going to recreate a new a new iterator", "tokens": [467, 311, 516, 281, 25833, 257, 777, 257, 777, 17138, 1639], "temperature": 0.0, "avg_logprob": -0.19875751071506076, "compression_ratio": 1.6927083333333333, "no_speech_prob": 7.934470431791851e-07}, {"id": 271, "seek": 129970, "start": 1299.7, "end": 1304.42, "text": " Okay, so basically behind the scenes in Python when you call it a", "tokens": [1033, 11, 370, 1936, 2261, 264, 8026, 294, 15329, 562, 291, 818, 309, 257], "temperature": 0.0, "avg_logprob": -0.22581448872884113, "compression_ratio": 1.5319148936170213, "no_speech_prob": 5.89641103942995e-07}, {"id": 272, "seek": 129970, "start": 1305.22, "end": 1310.46, "text": " On on this it basically tells it to like reset its state to create a new iterator", "tokens": [1282, 322, 341, 309, 1936, 5112, 309, 281, 411, 14322, 1080, 1785, 281, 1884, 257, 777, 17138, 1639], "temperature": 0.0, "avg_logprob": -0.22581448872884113, "compression_ratio": 1.5319148936170213, "no_speech_prob": 5.89641103942995e-07}, {"id": 273, "seek": 129970, "start": 1312.3, "end": 1317.18, "text": " And if you're interested in how that works the", "tokens": [400, 498, 291, 434, 3102, 294, 577, 300, 1985, 264], "temperature": 0.0, "avg_logprob": -0.22581448872884113, "compression_ratio": 1.5319148936170213, "no_speech_prob": 5.89641103942995e-07}, {"id": 274, "seek": 129970, "start": 1321.5800000000002, "end": 1328.5800000000002, "text": " The code is all you know available for you to look at so we could look at like MD train DL is", "tokens": [440, 3089, 307, 439, 291, 458, 2435, 337, 291, 281, 574, 412, 370, 321, 727, 574, 412, 411, 22521, 3847, 413, 43, 307], "temperature": 0.0, "avg_logprob": -0.22581448872884113, "compression_ratio": 1.5319148936170213, "no_speech_prob": 5.89641103942995e-07}, {"id": 275, "seek": 132858, "start": 1328.58, "end": 1334.98, "text": " Is a fast AI dot data set dot model data loader, so we could like take a look at the code of that", "tokens": [1119, 257, 2370, 7318, 5893, 1412, 992, 5893, 2316, 1412, 3677, 260, 11, 370, 321, 727, 411, 747, 257, 574, 412, 264, 3089, 295, 300], "temperature": 0.0, "avg_logprob": -0.2411366352992775, "compression_ratio": 1.7845528455284554, "no_speech_prob": 5.507566129381303e-06}, {"id": 276, "seek": 132858, "start": 1335.8999999999999, "end": 1337.8999999999999, "text": " So we could take a look at the code of that", "tokens": [407, 321, 727, 747, 257, 574, 412, 264, 3089, 295, 300], "temperature": 0.0, "avg_logprob": -0.2411366352992775, "compression_ratio": 1.7845528455284554, "no_speech_prob": 5.507566129381303e-06}, {"id": 277, "seek": 132858, "start": 1338.82, "end": 1344.74, "text": " And see exactly how it's being built right and so you can see here. Here's the next function", "tokens": [400, 536, 2293, 577, 309, 311, 885, 3094, 558, 293, 370, 291, 393, 536, 510, 13, 1692, 311, 264, 958, 2445], "temperature": 0.0, "avg_logprob": -0.2411366352992775, "compression_ratio": 1.7845528455284554, "no_speech_prob": 5.507566129381303e-06}, {"id": 278, "seek": 132858, "start": 1345.46, "end": 1347.46, "text": " Right which basically is", "tokens": [1779, 597, 1936, 307], "temperature": 0.0, "avg_logprob": -0.2411366352992775, "compression_ratio": 1.7845528455284554, "no_speech_prob": 5.507566129381303e-06}, {"id": 279, "seek": 132858, "start": 1348.3799999999999, "end": 1351.3, "text": " Keeping track of how many times it's been through in this self dot I", "tokens": [30187, 2837, 295, 577, 867, 1413, 309, 311, 668, 807, 294, 341, 2698, 5893, 286], "temperature": 0.0, "avg_logprob": -0.2411366352992775, "compression_ratio": 1.7845528455284554, "no_speech_prob": 5.507566129381303e-06}, {"id": 280, "seek": 132858, "start": 1352.26, "end": 1357.24, "text": " And here's the inner function which is the thing that gets pretty cold when you when you create a new iterator", "tokens": [400, 510, 311, 264, 7284, 2445, 597, 307, 264, 551, 300, 2170, 1238, 3554, 562, 291, 562, 291, 1884, 257, 777, 17138, 1639], "temperature": 0.0, "avg_logprob": -0.2411366352992775, "compression_ratio": 1.7845528455284554, "no_speech_prob": 5.507566129381303e-06}, {"id": 281, "seek": 135724, "start": 1357.24, "end": 1359.82, "text": " And you can see it's basically passing it off to something else", "tokens": [400, 291, 393, 536, 309, 311, 1936, 8437, 309, 766, 281, 746, 1646], "temperature": 0.0, "avg_logprob": -0.12724739540624255, "compression_ratio": 1.8248175182481752, "no_speech_prob": 6.438976470235502e-06}, {"id": 282, "seek": 135724, "start": 1359.82, "end": 1364.78, "text": " Which is a type data loader, and then you can check out data loader if you're interested to see how that's implemented", "tokens": [3013, 307, 257, 2010, 1412, 3677, 260, 11, 293, 550, 291, 393, 1520, 484, 1412, 3677, 260, 498, 291, 434, 3102, 281, 536, 577, 300, 311, 12270], "temperature": 0.0, "avg_logprob": -0.12724739540624255, "compression_ratio": 1.8248175182481752, "no_speech_prob": 6.438976470235502e-06}, {"id": 283, "seek": 135724, "start": 1365.38, "end": 1367.26, "text": " as well", "tokens": [382, 731], "temperature": 0.0, "avg_logprob": -0.12724739540624255, "compression_ratio": 1.8248175182481752, "no_speech_prob": 6.438976470235502e-06}, {"id": 284, "seek": 135724, "start": 1367.26, "end": 1369.26, "text": " So the data loader that we wrote", "tokens": [407, 264, 1412, 3677, 260, 300, 321, 4114], "temperature": 0.0, "avg_logprob": -0.12724739540624255, "compression_ratio": 1.8248175182481752, "no_speech_prob": 6.438976470235502e-06}, {"id": 285, "seek": 135724, "start": 1370.06, "end": 1375.1200000000001, "text": " Basically uses multi-threading to allow it to have multiple of these going on at the same time", "tokens": [8537, 4960, 4825, 12, 392, 35908, 281, 2089, 309, 281, 362, 3866, 295, 613, 516, 322, 412, 264, 912, 565], "temperature": 0.0, "avg_logprob": -0.12724739540624255, "compression_ratio": 1.8248175182481752, "no_speech_prob": 6.438976470235502e-06}, {"id": 286, "seek": 135724, "start": 1376.38, "end": 1380.34, "text": " It's actually a great. It's really simple. It's like it's only about a screen full of code", "tokens": [467, 311, 767, 257, 869, 13, 467, 311, 534, 2199, 13, 467, 311, 411, 309, 311, 787, 466, 257, 2568, 1577, 295, 3089], "temperature": 0.0, "avg_logprob": -0.12724739540624255, "compression_ratio": 1.8248175182481752, "no_speech_prob": 6.438976470235502e-06}, {"id": 287, "seek": 135724, "start": 1380.7, "end": 1383.9, "text": " So if you're interested in simple multi-threaded programming, it's a good thing to look at", "tokens": [407, 498, 291, 434, 3102, 294, 2199, 4825, 12, 392, 2538, 292, 9410, 11, 309, 311, 257, 665, 551, 281, 574, 412], "temperature": 0.0, "avg_logprob": -0.12724739540624255, "compression_ratio": 1.8248175182481752, "no_speech_prob": 6.438976470235502e-06}, {"id": 288, "seek": 138390, "start": 1383.9, "end": 1388.74, "text": " Okay now um oh yes", "tokens": [1033, 586, 1105, 1954, 2086], "temperature": 0.0, "avg_logprob": -0.27504150955765333, "compression_ratio": 1.5073891625615763, "no_speech_prob": 2.8736789658978523e-07}, {"id": 289, "seek": 138390, "start": 1390.18, "end": 1395.6200000000001, "text": " Why have you wrapped this in for epoch in range one since that'll only run once?", "tokens": [1545, 362, 291, 14226, 341, 294, 337, 30992, 339, 294, 3613, 472, 1670, 300, 603, 787, 1190, 1564, 30], "temperature": 0.0, "avg_logprob": -0.27504150955765333, "compression_ratio": 1.5073891625615763, "no_speech_prob": 2.8736789658978523e-07}, {"id": 290, "seek": 138390, "start": 1396.26, "end": 1399.46, "text": " Because in real life we would normally be running multiple", "tokens": [1436, 294, 957, 993, 321, 576, 5646, 312, 2614, 3866], "temperature": 0.0, "avg_logprob": -0.27504150955765333, "compression_ratio": 1.5073891625615763, "no_speech_prob": 2.8736789658978523e-07}, {"id": 291, "seek": 138390, "start": 1401.14, "end": 1406.88, "text": " So like in this case because it's a linear model it actually basically trains to", "tokens": [407, 411, 294, 341, 1389, 570, 309, 311, 257, 8213, 2316, 309, 767, 1936, 16329, 281], "temperature": 0.0, "avg_logprob": -0.27504150955765333, "compression_ratio": 1.5073891625615763, "no_speech_prob": 2.8736789658978523e-07}, {"id": 292, "seek": 138390, "start": 1407.38, "end": 1411.26, "text": " As good as it's going to get in one epoch, so if I type three here", "tokens": [1018, 665, 382, 309, 311, 516, 281, 483, 294, 472, 30992, 339, 11, 370, 498, 286, 2010, 1045, 510], "temperature": 0.0, "avg_logprob": -0.27504150955765333, "compression_ratio": 1.5073891625615763, "no_speech_prob": 2.8736789658978523e-07}, {"id": 293, "seek": 141126, "start": 1411.26, "end": 1413.26, "text": " it actually", "tokens": [309, 767], "temperature": 0.0, "avg_logprob": -0.14039251437553993, "compression_ratio": 1.6747967479674797, "no_speech_prob": 2.123368176398799e-06}, {"id": 294, "seek": 141126, "start": 1416.1, "end": 1421.26, "text": " It actually won't really improve after the first epoch much at all as you can see right", "tokens": [467, 767, 1582, 380, 534, 3470, 934, 264, 700, 30992, 339, 709, 412, 439, 382, 291, 393, 536, 558], "temperature": 0.0, "avg_logprob": -0.14039251437553993, "compression_ratio": 1.6747967479674797, "no_speech_prob": 2.123368176398799e-06}, {"id": 295, "seek": 141126, "start": 1421.62, "end": 1425.98, "text": " But when we go back up to the top we're going to look at some slightly deeper and more interesting", "tokens": [583, 562, 321, 352, 646, 493, 281, 264, 1192, 321, 434, 516, 281, 574, 412, 512, 4748, 7731, 293, 544, 1880], "temperature": 0.0, "avg_logprob": -0.14039251437553993, "compression_ratio": 1.6747967479674797, "no_speech_prob": 2.123368176398799e-06}, {"id": 296, "seek": 141126, "start": 1426.5, "end": 1432.4, "text": " Versions which will take more epochs, so you know if I was turning this into a into a function", "tokens": [12226, 626, 597, 486, 747, 544, 30992, 28346, 11, 370, 291, 458, 498, 286, 390, 6246, 341, 666, 257, 666, 257, 2445], "temperature": 0.0, "avg_logprob": -0.14039251437553993, "compression_ratio": 1.6747967479674797, "no_speech_prob": 2.123368176398799e-06}, {"id": 297, "seek": 141126, "start": 1432.4, "end": 1436.86, "text": " You know I'd be going like you know death train model", "tokens": [509, 458, 286, 1116, 312, 516, 411, 291, 458, 2966, 3847, 2316], "temperature": 0.0, "avg_logprob": -0.14039251437553993, "compression_ratio": 1.6747967479674797, "no_speech_prob": 2.123368176398799e-06}, {"id": 298, "seek": 143686, "start": 1436.86, "end": 1440.4599999999998, "text": " And one of the things you would pass in is like number of epochs", "tokens": [400, 472, 295, 264, 721, 291, 576, 1320, 294, 307, 411, 1230, 295, 30992, 28346], "temperature": 0.0, "avg_logprob": -0.27539841788155695, "compression_ratio": 1.5202312138728324, "no_speech_prob": 1.0348494470235892e-06}, {"id": 299, "seek": 143686, "start": 1445.1, "end": 1447.1, "text": " Okay great", "tokens": [1033, 869], "temperature": 0.0, "avg_logprob": -0.27539841788155695, "compression_ratio": 1.5202312138728324, "no_speech_prob": 1.0348494470235892e-06}, {"id": 300, "seek": 143686, "start": 1450.1399999999999, "end": 1452.1399999999999, "text": " So one thing to remember is that", "tokens": [407, 472, 551, 281, 1604, 307, 300], "temperature": 0.0, "avg_logprob": -0.27539841788155695, "compression_ratio": 1.5202312138728324, "no_speech_prob": 1.0348494470235892e-06}, {"id": 301, "seek": 143686, "start": 1453.74, "end": 1457.82, "text": " When you're you know creating these neural network layers", "tokens": [1133, 291, 434, 291, 458, 4084, 613, 18161, 3209, 7914], "temperature": 0.0, "avg_logprob": -0.27539841788155695, "compression_ratio": 1.5202312138728324, "no_speech_prob": 1.0348494470235892e-06}, {"id": 302, "seek": 143686, "start": 1458.8999999999999, "end": 1460.8999999999999, "text": " then remember like", "tokens": [550, 1604, 411], "temperature": 0.0, "avg_logprob": -0.27539841788155695, "compression_ratio": 1.5202312138728324, "no_speech_prob": 1.0348494470235892e-06}, {"id": 303, "seek": 143686, "start": 1460.9799999999998, "end": 1465.02, "text": " This is just as part which is concerned. This is just a it's an end up module", "tokens": [639, 307, 445, 382, 644, 597, 307, 5922, 13, 639, 307, 445, 257, 309, 311, 364, 917, 493, 10088], "temperature": 0.0, "avg_logprob": -0.27539841788155695, "compression_ratio": 1.5202312138728324, "no_speech_prob": 1.0348494470235892e-06}, {"id": 304, "seek": 146502, "start": 1465.02, "end": 1468.6399999999999, "text": " It could be a we could be using it as a layer. We could be using the function", "tokens": [467, 727, 312, 257, 321, 727, 312, 1228, 309, 382, 257, 4583, 13, 492, 727, 312, 1228, 264, 2445], "temperature": 0.0, "avg_logprob": -0.18998616317222858, "compression_ratio": 1.9596774193548387, "no_speech_prob": 4.289318439987255e-06}, {"id": 305, "seek": 146502, "start": 1468.6399999999999, "end": 1475.22, "text": " We could be using it as a neural net pytorch doesn't think of those as different things right so this could be a layer", "tokens": [492, 727, 312, 1228, 309, 382, 257, 18161, 2533, 25878, 284, 339, 1177, 380, 519, 295, 729, 382, 819, 721, 558, 370, 341, 727, 312, 257, 4583], "temperature": 0.0, "avg_logprob": -0.18998616317222858, "compression_ratio": 1.9596774193548387, "no_speech_prob": 4.289318439987255e-06}, {"id": 306, "seek": 146502, "start": 1475.3, "end": 1477.3, "text": " Inside some other network right?", "tokens": [15123, 512, 661, 3209, 558, 30], "temperature": 0.0, "avg_logprob": -0.18998616317222858, "compression_ratio": 1.9596774193548387, "no_speech_prob": 4.289318439987255e-06}, {"id": 307, "seek": 146502, "start": 1478.1, "end": 1484.62, "text": " So how do gradients work so if you've got a layer which remember is just a bunch of we can think of it basically as its", "tokens": [407, 577, 360, 2771, 2448, 589, 370, 498, 291, 600, 658, 257, 4583, 597, 1604, 307, 445, 257, 3840, 295, 321, 393, 519, 295, 309, 1936, 382, 1080], "temperature": 0.0, "avg_logprob": -0.18998616317222858, "compression_ratio": 1.9596774193548387, "no_speech_prob": 4.289318439987255e-06}, {"id": 308, "seek": 146502, "start": 1484.78, "end": 1488.86, "text": " Activations right or some activations that get computed through some", "tokens": [28550, 763, 558, 420, 512, 2430, 763, 300, 483, 40610, 807, 512], "temperature": 0.0, "avg_logprob": -0.18998616317222858, "compression_ratio": 1.9596774193548387, "no_speech_prob": 4.289318439987255e-06}, {"id": 309, "seek": 146502, "start": 1489.66, "end": 1493.16, "text": " other nonlinear activation function or through some linear function", "tokens": [661, 2107, 28263, 24433, 2445, 420, 807, 512, 8213, 2445], "temperature": 0.0, "avg_logprob": -0.18998616317222858, "compression_ratio": 1.9596774193548387, "no_speech_prob": 4.289318439987255e-06}, {"id": 310, "seek": 149316, "start": 1493.16, "end": 1495.16, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.18638177351518112, "compression_ratio": 1.5675675675675675, "no_speech_prob": 2.1233736333670095e-06}, {"id": 311, "seek": 149316, "start": 1496.0, "end": 1497.8000000000002, "text": " From that layer", "tokens": [3358, 300, 4583], "temperature": 0.0, "avg_logprob": -0.18638177351518112, "compression_ratio": 1.5675675675675675, "no_speech_prob": 2.1233736333670095e-06}, {"id": 312, "seek": 149316, "start": 1497.8000000000002, "end": 1503.0800000000002, "text": " We it's very likely that we're then like let's say putting it through a matrix product right?", "tokens": [492, 309, 311, 588, 3700, 300, 321, 434, 550, 411, 718, 311, 584, 3372, 309, 807, 257, 8141, 1674, 558, 30], "temperature": 0.0, "avg_logprob": -0.18638177351518112, "compression_ratio": 1.5675675675675675, "no_speech_prob": 2.1233736333670095e-06}, {"id": 313, "seek": 149316, "start": 1504.24, "end": 1506.24, "text": " to create some new layer and", "tokens": [281, 1884, 512, 777, 4583, 293], "temperature": 0.0, "avg_logprob": -0.18638177351518112, "compression_ratio": 1.5675675675675675, "no_speech_prob": 2.1233736333670095e-06}, {"id": 314, "seek": 149316, "start": 1508.64, "end": 1511.2, "text": " So each one of these so if we were to grab like", "tokens": [407, 1184, 472, 295, 613, 370, 498, 321, 645, 281, 4444, 411], "temperature": 0.0, "avg_logprob": -0.18638177351518112, "compression_ratio": 1.5675675675675675, "no_speech_prob": 2.1233736333670095e-06}, {"id": 315, "seek": 149316, "start": 1512.68, "end": 1517.28, "text": " One of these activations right is actually going to be", "tokens": [1485, 295, 613, 2430, 763, 558, 307, 767, 516, 281, 312], "temperature": 0.0, "avg_logprob": -0.18638177351518112, "compression_ratio": 1.5675675675675675, "no_speech_prob": 2.1233736333670095e-06}, {"id": 316, "seek": 151728, "start": 1517.28, "end": 1522.32, "text": " Used to calculate every one of these outputs", "tokens": [43237, 281, 8873, 633, 472, 295, 613, 23930], "temperature": 0.0, "avg_logprob": -0.19996114210648971, "compression_ratio": 1.9364161849710984, "no_speech_prob": 1.5294094737328123e-06}, {"id": 317, "seek": 151728, "start": 1523.46, "end": 1527.02, "text": " right and so if you want to calculate the", "tokens": [558, 293, 370, 498, 291, 528, 281, 8873, 264], "temperature": 0.0, "avg_logprob": -0.19996114210648971, "compression_ratio": 1.9364161849710984, "no_speech_prob": 1.5294094737328123e-06}, {"id": 318, "seek": 151728, "start": 1528.96, "end": 1533.8, "text": " The derivative you have to know how this weight matrix", "tokens": [440, 13760, 291, 362, 281, 458, 577, 341, 3364, 8141], "temperature": 0.0, "avg_logprob": -0.19996114210648971, "compression_ratio": 1.9364161849710984, "no_speech_prob": 1.5294094737328123e-06}, {"id": 319, "seek": 151728, "start": 1535.0, "end": 1541.76, "text": " Impacts that output and that output and that output and that output right and then you have to add all of those together", "tokens": [8270, 15295, 300, 5598, 293, 300, 5598, 293, 300, 5598, 293, 300, 5598, 558, 293, 550, 291, 362, 281, 909, 439, 295, 729, 1214], "temperature": 0.0, "avg_logprob": -0.19996114210648971, "compression_ratio": 1.9364161849710984, "no_speech_prob": 1.5294094737328123e-06}, {"id": 320, "seek": 154176, "start": 1541.76, "end": 1547.52, "text": " To find like the total impact of this you know across all of its outputs and", "tokens": [1407, 915, 411, 264, 3217, 2712, 295, 341, 291, 458, 2108, 439, 295, 1080, 23930, 293], "temperature": 0.0, "avg_logprob": -0.1986074901762463, "compression_ratio": 1.6556603773584906, "no_speech_prob": 6.57791758840176e-07}, {"id": 321, "seek": 154176, "start": 1548.44, "end": 1551.84, "text": " So that's why in pytorch", "tokens": [407, 300, 311, 983, 294, 25878, 284, 339], "temperature": 0.0, "avg_logprob": -0.1986074901762463, "compression_ratio": 1.6556603773584906, "no_speech_prob": 6.57791758840176e-07}, {"id": 322, "seek": 154176, "start": 1552.92, "end": 1559.8799999999999, "text": " You have to tell it when to set the gradients to zero right because the idea is that you know you could be like having", "tokens": [509, 362, 281, 980, 309, 562, 281, 992, 264, 2771, 2448, 281, 4018, 558, 570, 264, 1558, 307, 300, 291, 458, 291, 727, 312, 411, 1419], "temperature": 0.0, "avg_logprob": -0.1986074901762463, "compression_ratio": 1.6556603773584906, "no_speech_prob": 6.57791758840176e-07}, {"id": 323, "seek": 154176, "start": 1559.8799999999999, "end": 1566.6, "text": " Lots of different loss functions or lots of different outputs in your next actor very set of activations or whatever all adding up", "tokens": [15908, 295, 819, 4470, 6828, 420, 3195, 295, 819, 23930, 294, 428, 958, 8747, 588, 992, 295, 2430, 763, 420, 2035, 439, 5127, 493], "temperature": 0.0, "avg_logprob": -0.1986074901762463, "compression_ratio": 1.6556603773584906, "no_speech_prob": 6.57791758840176e-07}, {"id": 324, "seek": 156660, "start": 1566.6, "end": 1572.8, "text": " Increasing or decreasing your gradients right so you basically have to say okay. This is a new", "tokens": [30367, 3349, 420, 23223, 428, 2771, 2448, 558, 370, 291, 1936, 362, 281, 584, 1392, 13, 639, 307, 257, 777], "temperature": 0.0, "avg_logprob": -0.14083748099244672, "compression_ratio": 1.7383720930232558, "no_speech_prob": 7.571135256512207e-07}, {"id": 325, "seek": 156660, "start": 1573.9199999999998, "end": 1575.7199999999998, "text": " calculation", "tokens": [17108], "temperature": 0.0, "avg_logprob": -0.14083748099244672, "compression_ratio": 1.7383720930232558, "no_speech_prob": 7.571135256512207e-07}, {"id": 326, "seek": 156660, "start": 1575.7199999999998, "end": 1582.36, "text": " Reset okay, so here is where we do that right so before we do L dot backward we say", "tokens": [5015, 302, 1392, 11, 370, 510, 307, 689, 321, 360, 300, 558, 370, 949, 321, 360, 441, 5893, 23897, 321, 584], "temperature": 0.0, "avg_logprob": -0.14083748099244672, "compression_ratio": 1.7383720930232558, "no_speech_prob": 7.571135256512207e-07}, {"id": 327, "seek": 156660, "start": 1582.8799999999999, "end": 1585.32, "text": " Reset okay, so let's take our weights", "tokens": [5015, 302, 1392, 11, 370, 718, 311, 747, 527, 17443], "temperature": 0.0, "avg_logprob": -0.14083748099244672, "compression_ratio": 1.7383720930232558, "no_speech_prob": 7.571135256512207e-07}, {"id": 328, "seek": 156660, "start": 1586.28, "end": 1590.32, "text": " Let's take the gradients. Let's take the tensor that they point to and", "tokens": [961, 311, 747, 264, 2771, 2448, 13, 961, 311, 747, 264, 40863, 300, 436, 935, 281, 293], "temperature": 0.0, "avg_logprob": -0.14083748099244672, "compression_ratio": 1.7383720930232558, "no_speech_prob": 7.571135256512207e-07}, {"id": 329, "seek": 159032, "start": 1590.32, "end": 1598.04, "text": " And then zero underscore does anybody remember from last week what underscore does as a suffix in pytorch?", "tokens": [400, 550, 4018, 37556, 775, 4472, 1604, 490, 1036, 1243, 437, 37556, 775, 382, 257, 3889, 970, 294, 25878, 284, 339, 30], "temperature": 0.0, "avg_logprob": -0.23056281163142278, "compression_ratio": 1.5141242937853108, "no_speech_prob": 1.2289149253774667e-06}, {"id": 330, "seek": 159032, "start": 1600.6799999999998, "end": 1602.6799999999998, "text": " Yeah", "tokens": [865], "temperature": 0.0, "avg_logprob": -0.23056281163142278, "compression_ratio": 1.5141242937853108, "no_speech_prob": 1.2289149253774667e-06}, {"id": 331, "seek": 159032, "start": 1604.8799999999999, "end": 1609.56, "text": " I forgot the language, but basically it changes it within the place right there the language is in place", "tokens": [286, 5298, 264, 2856, 11, 457, 1936, 309, 2962, 309, 1951, 264, 1081, 558, 456, 264, 2856, 307, 294, 1081], "temperature": 0.0, "avg_logprob": -0.23056281163142278, "compression_ratio": 1.5141242937853108, "no_speech_prob": 1.2289149253774667e-06}, {"id": 332, "seek": 159032, "start": 1610.28, "end": 1612.1599999999999, "text": " Yeah", "tokens": [865], "temperature": 0.0, "avg_logprob": -0.23056281163142278, "compression_ratio": 1.5141242937853108, "no_speech_prob": 1.2289149253774667e-06}, {"id": 333, "seek": 159032, "start": 1612.1599999999999, "end": 1615.24, "text": " Exactly so it sounds like a minor technicality", "tokens": [7587, 370, 309, 3263, 411, 257, 6696, 6191, 507], "temperature": 0.0, "avg_logprob": -0.23056281163142278, "compression_ratio": 1.5141242937853108, "no_speech_prob": 1.2289149253774667e-06}, {"id": 334, "seek": 161524, "start": 1615.24, "end": 1621.76, "text": " But it's super useful to remember every function pretty much has an underscore version suffix which does it in place", "tokens": [583, 309, 311, 1687, 4420, 281, 1604, 633, 2445, 1238, 709, 575, 364, 37556, 3037, 3889, 970, 597, 775, 309, 294, 1081], "temperature": 0.0, "avg_logprob": -0.18301955858866373, "compression_ratio": 1.5662100456621004, "no_speech_prob": 3.089482106588548e-06}, {"id": 335, "seek": 161524, "start": 1623.24, "end": 1625.88, "text": " Yeah, so normally zero returns a", "tokens": [865, 11, 370, 5646, 4018, 11247, 257], "temperature": 0.0, "avg_logprob": -0.18301955858866373, "compression_ratio": 1.5662100456621004, "no_speech_prob": 3.089482106588548e-06}, {"id": 336, "seek": 161524, "start": 1627.2, "end": 1634.76, "text": " Tensor of zeros of a particular size so zero underscore means replace the contents of this with a bunch of zeros, okay?", "tokens": [34306, 295, 35193, 295, 257, 1729, 2744, 370, 4018, 37556, 1355, 7406, 264, 15768, 295, 341, 365, 257, 3840, 295, 35193, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.18301955858866373, "compression_ratio": 1.5662100456621004, "no_speech_prob": 3.089482106588548e-06}, {"id": 337, "seek": 161524, "start": 1636.1200000000001, "end": 1638.36, "text": " All right, so that's", "tokens": [1057, 558, 11, 370, 300, 311], "temperature": 0.0, "avg_logprob": -0.18301955858866373, "compression_ratio": 1.5662100456621004, "no_speech_prob": 3.089482106588548e-06}, {"id": 338, "seek": 163836, "start": 1638.36, "end": 1644.28, "text": " That's it right so that's like SGD from scratch", "tokens": [663, 311, 309, 558, 370, 300, 311, 411, 34520, 35, 490, 8459], "temperature": 0.0, "avg_logprob": -0.17902856502892836, "compression_ratio": 1.576470588235294, "no_speech_prob": 6.854257662780583e-06}, {"id": 339, "seek": 163836, "start": 1644.28, "end": 1648.3999999999999, "text": " And if I get rid of my menu bar we can officially say it fits within a screen", "tokens": [400, 498, 286, 483, 3973, 295, 452, 6510, 2159, 321, 393, 12053, 584, 309, 9001, 1951, 257, 2568], "temperature": 0.0, "avg_logprob": -0.17902856502892836, "compression_ratio": 1.576470588235294, "no_speech_prob": 6.854257662780583e-06}, {"id": 340, "seek": 163836, "start": 1648.84, "end": 1650.84, "text": " Okay, so", "tokens": [1033, 11, 370], "temperature": 0.0, "avg_logprob": -0.17902856502892836, "compression_ratio": 1.576470588235294, "no_speech_prob": 6.854257662780583e-06}, {"id": 341, "seek": 163836, "start": 1651.4399999999998, "end": 1658.04, "text": " Of course we haven't got our definition of logistic regression here. That's another half the screen, but basically this there's not much to it. Yes, Vash", "tokens": [2720, 1164, 321, 2378, 380, 658, 527, 7123, 295, 3565, 3142, 24590, 510, 13, 663, 311, 1071, 1922, 264, 2568, 11, 457, 1936, 341, 456, 311, 406, 709, 281, 309, 13, 1079, 11, 691, 1299], "temperature": 0.0, "avg_logprob": -0.17902856502892836, "compression_ratio": 1.576470588235294, "no_speech_prob": 6.854257662780583e-06}, {"id": 342, "seek": 163836, "start": 1659.1999999999998, "end": 1666.28, "text": " So later on if we have to do this more the gradient is it because you might find like a wrong minima local minima", "tokens": [407, 1780, 322, 498, 321, 362, 281, 360, 341, 544, 264, 16235, 307, 309, 570, 291, 1062, 915, 411, 257, 2085, 4464, 64, 2654, 4464, 64], "temperature": 0.0, "avg_logprob": -0.17902856502892836, "compression_ratio": 1.576470588235294, "no_speech_prob": 6.854257662780583e-06}, {"id": 343, "seek": 166628, "start": 1666.28, "end": 1670.3, "text": " Is that way so you have to kick it out, and that's what you have to do multiple times when the surfaces get more", "tokens": [1119, 300, 636, 370, 291, 362, 281, 4437, 309, 484, 11, 293, 300, 311, 437, 291, 362, 281, 360, 3866, 1413, 562, 264, 16130, 483, 544], "temperature": 0.0, "avg_logprob": -0.1665301146330657, "compression_ratio": 1.8469750889679715, "no_speech_prob": 2.295905915161711e-06}, {"id": 344, "seek": 166628, "start": 1670.3, "end": 1673.24, "text": " Why do you need multiple epochs is that your question well?", "tokens": [1545, 360, 291, 643, 3866, 30992, 28346, 307, 300, 428, 1168, 731, 30], "temperature": 0.0, "avg_logprob": -0.1665301146330657, "compression_ratio": 1.8469750889679715, "no_speech_prob": 2.295905915161711e-06}, {"id": 345, "seek": 166628, "start": 1673.24, "end": 1676.6, "text": " I mean a simple way to answer that would be let's say our learning rate was tiny", "tokens": [286, 914, 257, 2199, 636, 281, 1867, 300, 576, 312, 718, 311, 584, 527, 2539, 3314, 390, 5870], "temperature": 0.0, "avg_logprob": -0.1665301146330657, "compression_ratio": 1.8469750889679715, "no_speech_prob": 2.295905915161711e-06}, {"id": 346, "seek": 166628, "start": 1677.36, "end": 1679.36, "text": " right", "tokens": [558], "temperature": 0.0, "avg_logprob": -0.1665301146330657, "compression_ratio": 1.8469750889679715, "no_speech_prob": 2.295905915161711e-06}, {"id": 347, "seek": 166628, "start": 1681.32, "end": 1682.96, "text": " Then", "tokens": [1396], "temperature": 0.0, "avg_logprob": -0.1665301146330657, "compression_ratio": 1.8469750889679715, "no_speech_prob": 2.295905915161711e-06}, {"id": 348, "seek": 166628, "start": 1682.96, "end": 1684.96, "text": " It's just not going to get very far", "tokens": [467, 311, 445, 406, 516, 281, 483, 588, 1400], "temperature": 0.0, "avg_logprob": -0.1665301146330657, "compression_ratio": 1.8469750889679715, "no_speech_prob": 2.295905915161711e-06}, {"id": 349, "seek": 166628, "start": 1684.96, "end": 1689.84, "text": " Right there's nothing that says going through one epoch is enough to get you all the way there", "tokens": [1779, 456, 311, 1825, 300, 1619, 516, 807, 472, 30992, 339, 307, 1547, 281, 483, 291, 439, 264, 636, 456], "temperature": 0.0, "avg_logprob": -0.1665301146330657, "compression_ratio": 1.8469750889679715, "no_speech_prob": 2.295905915161711e-06}, {"id": 350, "seek": 166628, "start": 1690.84, "end": 1695.04, "text": " So then it'd be like okay. Well. Let's increase our learning rate, and it's like yeah sure we'll increase our learning rate", "tokens": [407, 550, 309, 1116, 312, 411, 1392, 13, 1042, 13, 961, 311, 3488, 527, 2539, 3314, 11, 293, 309, 311, 411, 1338, 988, 321, 603, 3488, 527, 2539, 3314], "temperature": 0.0, "avg_logprob": -0.1665301146330657, "compression_ratio": 1.8469750889679715, "no_speech_prob": 2.295905915161711e-06}, {"id": 351, "seek": 169504, "start": 1695.04, "end": 1701.12, "text": " but who's to say that the highest learning rate that learn stably is is enough to", "tokens": [457, 567, 311, 281, 584, 300, 264, 6343, 2539, 3314, 300, 1466, 342, 1188, 307, 307, 1547, 281], "temperature": 0.0, "avg_logprob": -0.1752585762425473, "compression_ratio": 1.7400881057268722, "no_speech_prob": 2.5215611003659433e-06}, {"id": 352, "seek": 169504, "start": 1701.56, "end": 1709.76, "text": " Learn this as well as it can be learned and for most data sets for most architectures one epoch is very rarely enough to get you", "tokens": [17216, 341, 382, 731, 382, 309, 393, 312, 3264, 293, 337, 881, 1412, 6352, 337, 881, 6331, 1303, 472, 30992, 339, 307, 588, 13752, 1547, 281, 483, 291], "temperature": 0.0, "avg_logprob": -0.1752585762425473, "compression_ratio": 1.7400881057268722, "no_speech_prob": 2.5215611003659433e-06}, {"id": 353, "seek": 169504, "start": 1710.6, "end": 1712.6, "text": " To the best result you can get to", "tokens": [1407, 264, 1151, 1874, 291, 393, 483, 281], "temperature": 0.0, "avg_logprob": -0.1752585762425473, "compression_ratio": 1.7400881057268722, "no_speech_prob": 2.5215611003659433e-06}, {"id": 354, "seek": 169504, "start": 1714.68, "end": 1716.68, "text": " You know linear models are just", "tokens": [509, 458, 8213, 5245, 366, 445], "temperature": 0.0, "avg_logprob": -0.1752585762425473, "compression_ratio": 1.7400881057268722, "no_speech_prob": 2.5215611003659433e-06}, {"id": 355, "seek": 169504, "start": 1717.36, "end": 1723.54, "text": " They're very nicely behaved. You know so you can often use higher learning rates and learn them more quickly also they", "tokens": [814, 434, 588, 9594, 48249, 13, 509, 458, 370, 291, 393, 2049, 764, 2946, 2539, 6846, 293, 1466, 552, 544, 2661, 611, 436], "temperature": 0.0, "avg_logprob": -0.1752585762425473, "compression_ratio": 1.7400881057268722, "no_speech_prob": 2.5215611003659433e-06}, {"id": 356, "seek": 172354, "start": 1723.54, "end": 1729.74, "text": " They don't you can't like generally get as good accuracy, so there's not as far to take them either", "tokens": [814, 500, 380, 291, 393, 380, 411, 5101, 483, 382, 665, 14170, 11, 370, 456, 311, 406, 382, 1400, 281, 747, 552, 2139], "temperature": 0.0, "avg_logprob": -0.3600044624478209, "compression_ratio": 1.8518518518518519, "no_speech_prob": 1.3287738056533271e-06}, {"id": 357, "seek": 172354, "start": 1730.06, "end": 1735.02, "text": " So yeah doing one epoch is going to be the rarity all right, so let's go backwards", "tokens": [407, 1338, 884, 472, 30992, 339, 307, 516, 281, 312, 264, 367, 17409, 439, 558, 11, 370, 718, 311, 352, 12204], "temperature": 0.0, "avg_logprob": -0.3600044624478209, "compression_ratio": 1.8518518518518519, "no_speech_prob": 1.3287738056533271e-06}, {"id": 358, "seek": 172354, "start": 1735.74, "end": 1740.06, "text": " So going backwards we're basically going to say all right. Let's not write", "tokens": [407, 516, 12204, 321, 434, 1936, 516, 281, 584, 439, 558, 13, 961, 311, 406, 2464], "temperature": 0.0, "avg_logprob": -0.3600044624478209, "compression_ratio": 1.8518518518518519, "no_speech_prob": 1.3287738056533271e-06}, {"id": 359, "seek": 172354, "start": 1741.18, "end": 1745.7, "text": " Those two lines again and again again. Let's not write those three lines again and again and again", "tokens": [3950, 732, 3876, 797, 293, 797, 797, 13, 961, 311, 406, 2464, 729, 1045, 3876, 797, 293, 797, 293, 797], "temperature": 0.0, "avg_logprob": -0.3600044624478209, "compression_ratio": 1.8518518518518519, "no_speech_prob": 1.3287738056533271e-06}, {"id": 360, "seek": 172354, "start": 1746.02, "end": 1749.42, "text": " Let's have somebody do that for us right so", "tokens": [961, 311, 362, 2618, 360, 300, 337, 505, 558, 370], "temperature": 0.0, "avg_logprob": -0.3600044624478209, "compression_ratio": 1.8518518518518519, "no_speech_prob": 1.3287738056533271e-06}, {"id": 361, "seek": 174942, "start": 1749.42, "end": 1756.8200000000002, "text": " Right so that's like that's the only difference between that version and this version is rather than saying dot zero ourselves", "tokens": [1779, 370, 300, 311, 411, 300, 311, 264, 787, 2649, 1296, 300, 3037, 293, 341, 3037, 307, 2831, 813, 1566, 5893, 4018, 4175], "temperature": 0.0, "avg_logprob": -0.2672843933105469, "compression_ratio": 1.6029411764705883, "no_speech_prob": 3.966965778090525e-06}, {"id": 362, "seek": 174942, "start": 1757.46, "end": 1761.26, "text": " Rather than saying minus gradient times LR ourselves", "tokens": [16571, 813, 1566, 3175, 16235, 1413, 441, 49, 4175], "temperature": 0.0, "avg_logprob": -0.2672843933105469, "compression_ratio": 1.6029411764705883, "no_speech_prob": 3.966965778090525e-06}, {"id": 363, "seek": 174942, "start": 1762.5, "end": 1764.5800000000002, "text": " These are wrapped up for us, okay?", "tokens": [1981, 366, 14226, 493, 337, 505, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.2672843933105469, "compression_ratio": 1.6029411764705883, "no_speech_prob": 3.966965778090525e-06}, {"id": 364, "seek": 174942, "start": 1766.1000000000001, "end": 1768.9, "text": " There is another wrinkle here, which is", "tokens": [821, 307, 1071, 928, 14095, 510, 11, 597, 307], "temperature": 0.0, "avg_logprob": -0.2672843933105469, "compression_ratio": 1.6029411764705883, "no_speech_prob": 3.966965778090525e-06}, {"id": 365, "seek": 174942, "start": 1770.26, "end": 1772.26, "text": " this approach to updating", "tokens": [341, 3109, 281, 25113], "temperature": 0.0, "avg_logprob": -0.2672843933105469, "compression_ratio": 1.6029411764705883, "no_speech_prob": 3.966965778090525e-06}, {"id": 366, "seek": 174942, "start": 1773.14, "end": 1776.6200000000001, "text": " The the weights is actually pretty inefficient", "tokens": [440, 264, 17443, 307, 767, 1238, 43495], "temperature": 0.0, "avg_logprob": -0.2672843933105469, "compression_ratio": 1.6029411764705883, "no_speech_prob": 3.966965778090525e-06}, {"id": 367, "seek": 177662, "start": 1776.62, "end": 1780.06, "text": " It doesn't take advantage of momentum", "tokens": [467, 1177, 380, 747, 5002, 295, 11244], "temperature": 0.0, "avg_logprob": -0.24687649653508112, "compression_ratio": 1.4918032786885247, "no_speech_prob": 4.029440788144711e-06}, {"id": 368, "seek": 177662, "start": 1780.7399999999998, "end": 1782.62, "text": " and curvature", "tokens": [293, 37638], "temperature": 0.0, "avg_logprob": -0.24687649653508112, "compression_ratio": 1.4918032786885247, "no_speech_prob": 4.029440788144711e-06}, {"id": 369, "seek": 177662, "start": 1782.62, "end": 1784.62, "text": " and so", "tokens": [293, 370], "temperature": 0.0, "avg_logprob": -0.24687649653508112, "compression_ratio": 1.4918032786885247, "no_speech_prob": 4.029440788144711e-06}, {"id": 370, "seek": 177662, "start": 1784.82, "end": 1791.02, "text": " In the DL course we learned about how to do momentum from scratch as well, okay, so", "tokens": [682, 264, 413, 43, 1164, 321, 3264, 466, 577, 281, 360, 11244, 490, 8459, 382, 731, 11, 1392, 11, 370], "temperature": 0.0, "avg_logprob": -0.24687649653508112, "compression_ratio": 1.4918032786885247, "no_speech_prob": 4.029440788144711e-06}, {"id": 371, "seek": 177662, "start": 1791.6999999999998, "end": 1795.1599999999999, "text": " If we actually just use plain old", "tokens": [759, 321, 767, 445, 764, 11121, 1331], "temperature": 0.0, "avg_logprob": -0.24687649653508112, "compression_ratio": 1.4918032786885247, "no_speech_prob": 4.029440788144711e-06}, {"id": 372, "seek": 177662, "start": 1795.8999999999999, "end": 1797.8999999999999, "text": " SGD", "tokens": [34520, 35], "temperature": 0.0, "avg_logprob": -0.24687649653508112, "compression_ratio": 1.4918032786885247, "no_speech_prob": 4.029440788144711e-06}, {"id": 373, "seek": 177662, "start": 1799.7399999999998, "end": 1805.78, "text": " Then you'll see that this learns much slower, so now that I've typed just plain old SGD here", "tokens": [1396, 291, 603, 536, 300, 341, 27152, 709, 14009, 11, 370, 586, 300, 286, 600, 33941, 445, 11121, 1331, 34520, 35, 510], "temperature": 0.0, "avg_logprob": -0.24687649653508112, "compression_ratio": 1.4918032786885247, "no_speech_prob": 4.029440788144711e-06}, {"id": 374, "seek": 180578, "start": 1805.78, "end": 1807.78, "text": " This is now literally doing exactly the same thing", "tokens": [639, 307, 586, 3736, 884, 2293, 264, 912, 551], "temperature": 0.0, "avg_logprob": -0.23936926428951436, "compression_ratio": 1.5238095238095237, "no_speech_prob": 5.42218276677886e-06}, {"id": 375, "seek": 180578, "start": 1808.46, "end": 1811.8999999999999, "text": " As our slow version, so we have to increase the learning rate", "tokens": [1018, 527, 2964, 3037, 11, 370, 321, 362, 281, 3488, 264, 2539, 3314], "temperature": 0.0, "avg_logprob": -0.23936926428951436, "compression_ratio": 1.5238095238095237, "no_speech_prob": 5.42218276677886e-06}, {"id": 376, "seek": 180578, "start": 1815.3, "end": 1820.12, "text": " Okay there we go, so this this is now the same as the the one we wrote by hand", "tokens": [1033, 456, 321, 352, 11, 370, 341, 341, 307, 586, 264, 912, 382, 264, 264, 472, 321, 4114, 538, 1011], "temperature": 0.0, "avg_logprob": -0.23936926428951436, "compression_ratio": 1.5238095238095237, "no_speech_prob": 5.42218276677886e-06}, {"id": 377, "seek": 180578, "start": 1821.82, "end": 1823.82, "text": " So then all right", "tokens": [407, 550, 439, 558], "temperature": 0.0, "avg_logprob": -0.23936926428951436, "compression_ratio": 1.5238095238095237, "no_speech_prob": 5.42218276677886e-06}, {"id": 378, "seek": 180578, "start": 1825.74, "end": 1828.94, "text": " Let's do a little bit more stuff automatically", "tokens": [961, 311, 360, 257, 707, 857, 544, 1507, 6772], "temperature": 0.0, "avg_logprob": -0.23936926428951436, "compression_ratio": 1.5238095238095237, "no_speech_prob": 5.42218276677886e-06}, {"id": 379, "seek": 182894, "start": 1828.94, "end": 1834.66, "text": " Let's not you know given that every time we train something", "tokens": [961, 311, 406, 291, 458, 2212, 300, 633, 565, 321, 3847, 746], "temperature": 0.0, "avg_logprob": -0.4302299969816861, "compression_ratio": 1.5297297297297296, "no_speech_prob": 4.96527263749158e-07}, {"id": 380, "seek": 182894, "start": 1834.66, "end": 1840.9, "text": " We have to loop through epoch loop through batch do forward get the loss", "tokens": [492, 362, 281, 6367, 807, 30992, 339, 6367, 807, 15245, 360, 2128, 483, 264, 4470], "temperature": 0.0, "avg_logprob": -0.4302299969816861, "compression_ratio": 1.5297297297297296, "no_speech_prob": 4.96527263749158e-07}, {"id": 381, "seek": 182894, "start": 1840.9, "end": 1846.3, "text": " Zero the gradient do backward do a step of the optimizer. Let's put all that in a function", "tokens": [17182, 264, 16235, 360, 23897, 360, 257, 1823, 295, 264, 5028, 6545, 13, 961, 311, 829, 439, 300, 294, 257, 2445], "temperature": 0.0, "avg_logprob": -0.4302299969816861, "compression_ratio": 1.5297297297297296, "no_speech_prob": 4.96527263749158e-07}, {"id": 382, "seek": 182894, "start": 1847.3, "end": 1849.78, "text": " Okay, and that function is called fit", "tokens": [1033, 11, 293, 300, 2445, 307, 1219, 3318], "temperature": 0.0, "avg_logprob": -0.4302299969816861, "compression_ratio": 1.5297297297297296, "no_speech_prob": 4.96527263749158e-07}, {"id": 383, "seek": 182894, "start": 1853.8600000000001, "end": 1855.8600000000001, "text": " All right there it is", "tokens": [1057, 558, 456, 309, 307], "temperature": 0.0, "avg_logprob": -0.4302299969816861, "compression_ratio": 1.5297297297297296, "no_speech_prob": 4.96527263749158e-07}, {"id": 384, "seek": 185586, "start": 1855.86, "end": 1862.02, "text": " Okay, so let's take a look at fit", "tokens": [1033, 11, 370, 718, 311, 747, 257, 574, 412, 3318], "temperature": 0.0, "avg_logprob": -0.2030056037154852, "compression_ratio": 1.432, "no_speech_prob": 4.356852969067404e-06}, {"id": 385, "seek": 185586, "start": 1865.82, "end": 1871.1399999999999, "text": " Fit go through each epoch go through each batch", "tokens": [29263, 352, 807, 1184, 30992, 339, 352, 807, 1184, 15245], "temperature": 0.0, "avg_logprob": -0.2030056037154852, "compression_ratio": 1.432, "no_speech_prob": 4.356852969067404e-06}, {"id": 386, "seek": 185586, "start": 1872.58, "end": 1874.5, "text": " Do one step", "tokens": [1144, 472, 1823], "temperature": 0.0, "avg_logprob": -0.2030056037154852, "compression_ratio": 1.432, "no_speech_prob": 4.356852969067404e-06}, {"id": 387, "seek": 185586, "start": 1874.5, "end": 1880.3, "text": " Keep track of the loss and at the end calculate the validation right and so then", "tokens": [5527, 2837, 295, 264, 4470, 293, 412, 264, 917, 8873, 264, 24071, 558, 293, 370, 550], "temperature": 0.0, "avg_logprob": -0.2030056037154852, "compression_ratio": 1.432, "no_speech_prob": 4.356852969067404e-06}, {"id": 388, "seek": 188030, "start": 1880.3, "end": 1895.3, "text": " step", "tokens": [1823], "temperature": 0.0, "avg_logprob": -0.34364514473157054, "compression_ratio": 1.2407407407407407, "no_speech_prob": 1.6280456520689768e-06}, {"id": 389, "seek": 188030, "start": 1895.46, "end": 1899.82, "text": " So if you're interested in looking at this this stuff's all inside fast AI dot model and", "tokens": [407, 498, 291, 434, 3102, 294, 1237, 412, 341, 341, 1507, 311, 439, 1854, 2370, 7318, 5893, 2316, 293], "temperature": 0.0, "avg_logprob": -0.34364514473157054, "compression_ratio": 1.2407407407407407, "no_speech_prob": 1.6280456520689768e-06}, {"id": 390, "seek": 188030, "start": 1902.74, "end": 1904.74, "text": " So here is step right", "tokens": [407, 510, 307, 1823, 558], "temperature": 0.0, "avg_logprob": -0.34364514473157054, "compression_ratio": 1.2407407407407407, "no_speech_prob": 1.6280456520689768e-06}, {"id": 391, "seek": 188030, "start": 1906.06, "end": 1907.94, "text": " Zero the gradients", "tokens": [17182, 264, 2771, 2448], "temperature": 0.0, "avg_logprob": -0.34364514473157054, "compression_ratio": 1.2407407407407407, "no_speech_prob": 1.6280456520689768e-06}, {"id": 392, "seek": 190794, "start": 1907.94, "end": 1915.06, "text": " Calculate the loss remember pipe torch tends to call it criterion rather than loss right do backward", "tokens": [3511, 2444, 473, 264, 4470, 1604, 11240, 27822, 12258, 281, 818, 309, 46691, 2831, 813, 4470, 558, 360, 23897], "temperature": 0.0, "avg_logprob": -0.21694895164253786, "compression_ratio": 1.6584362139917694, "no_speech_prob": 2.3687900920776883e-06}, {"id": 393, "seek": 190794, "start": 1916.38, "end": 1920.14, "text": " And then there's something else we haven't learned here, but we do learn the deep learning course which is gradient", "tokens": [400, 550, 456, 311, 746, 1646, 321, 2378, 380, 3264, 510, 11, 457, 321, 360, 1466, 264, 2452, 2539, 1164, 597, 307, 16235], "temperature": 0.0, "avg_logprob": -0.21694895164253786, "compression_ratio": 1.6584362139917694, "no_speech_prob": 2.3687900920776883e-06}, {"id": 394, "seek": 190794, "start": 1920.14, "end": 1927.14, "text": " So you can ignore that right so you can see now like all the stuff that we've learned when you look inside the actual frameworks", "tokens": [407, 291, 393, 11200, 300, 558, 370, 291, 393, 536, 586, 411, 439, 264, 1507, 300, 321, 600, 3264, 562, 291, 574, 1854, 264, 3539, 29834], "temperature": 0.0, "avg_logprob": -0.21694895164253786, "compression_ratio": 1.6584362139917694, "no_speech_prob": 2.3687900920776883e-06}, {"id": 395, "seek": 190794, "start": 1927.66, "end": 1929.66, "text": " That's the code you see okay?", "tokens": [663, 311, 264, 3089, 291, 536, 1392, 30], "temperature": 0.0, "avg_logprob": -0.21694895164253786, "compression_ratio": 1.6584362139917694, "no_speech_prob": 2.3687900920776883e-06}, {"id": 396, "seek": 190794, "start": 1931.3400000000001, "end": 1933.3400000000001, "text": " So that's what fit does", "tokens": [407, 300, 311, 437, 3318, 775], "temperature": 0.0, "avg_logprob": -0.21694895164253786, "compression_ratio": 1.6584362139917694, "no_speech_prob": 2.3687900920776883e-06}, {"id": 397, "seek": 190794, "start": 1933.8200000000002, "end": 1934.98, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.21694895164253786, "compression_ratio": 1.6584362139917694, "no_speech_prob": 2.3687900920776883e-06}, {"id": 398, "seek": 193498, "start": 1934.98, "end": 1939.66, "text": " So then the next step would be like okay. Well this idea of like having some", "tokens": [407, 550, 264, 958, 1823, 576, 312, 411, 1392, 13, 1042, 341, 1558, 295, 411, 1419, 512], "temperature": 0.0, "avg_logprob": -0.17592966438520072, "compression_ratio": 1.995049504950495, "no_speech_prob": 6.048868726793444e-06}, {"id": 399, "seek": 193498, "start": 1941.06, "end": 1945.32, "text": " weights and a bias and doing a matrix product in addition", "tokens": [17443, 293, 257, 12577, 293, 884, 257, 8141, 1674, 294, 4500], "temperature": 0.0, "avg_logprob": -0.17592966438520072, "compression_ratio": 1.995049504950495, "no_speech_prob": 6.048868726793444e-06}, {"id": 400, "seek": 193498, "start": 1946.3, "end": 1948.3, "text": " Let's put that in a function", "tokens": [961, 311, 829, 300, 294, 257, 2445], "temperature": 0.0, "avg_logprob": -0.17592966438520072, "compression_ratio": 1.995049504950495, "no_speech_prob": 6.048868726793444e-06}, {"id": 401, "seek": 193498, "start": 1948.34, "end": 1950.5, "text": " This thing of doing the log soft max", "tokens": [639, 551, 295, 884, 264, 3565, 2787, 11469], "temperature": 0.0, "avg_logprob": -0.17592966438520072, "compression_ratio": 1.995049504950495, "no_speech_prob": 6.048868726793444e-06}, {"id": 402, "seek": 193498, "start": 1950.9, "end": 1956.6200000000001, "text": " Let's put that in a function and then the very idea of like first doing this and then doing that", "tokens": [961, 311, 829, 300, 294, 257, 2445, 293, 550, 264, 588, 1558, 295, 411, 700, 884, 341, 293, 550, 884, 300], "temperature": 0.0, "avg_logprob": -0.17592966438520072, "compression_ratio": 1.995049504950495, "no_speech_prob": 6.048868726793444e-06}, {"id": 403, "seek": 193498, "start": 1957.06, "end": 1961.7, "text": " This idea of like chaining functions together. Let's put that into a function and", "tokens": [639, 1558, 295, 411, 417, 3686, 6828, 1214, 13, 961, 311, 829, 300, 666, 257, 2445, 293], "temperature": 0.0, "avg_logprob": -0.17592966438520072, "compression_ratio": 1.995049504950495, "no_speech_prob": 6.048868726793444e-06}, {"id": 404, "seek": 196170, "start": 1961.7, "end": 1964.42, "text": " that finally gets us to that", "tokens": [300, 2721, 2170, 505, 281, 300], "temperature": 0.0, "avg_logprob": -0.25632028920309885, "compression_ratio": 1.556338028169014, "no_speech_prob": 2.2959127363719745e-06}, {"id": 405, "seek": 196170, "start": 1967.66, "end": 1975.1000000000001, "text": " Okay, so sequential simply means do this function take the result send it to this function etc right", "tokens": [1033, 11, 370, 42881, 2935, 1355, 360, 341, 2445, 747, 264, 1874, 2845, 309, 281, 341, 2445, 5183, 558], "temperature": 0.0, "avg_logprob": -0.25632028920309885, "compression_ratio": 1.556338028169014, "no_speech_prob": 2.2959127363719745e-06}, {"id": 406, "seek": 196170, "start": 1975.94, "end": 1979.66, "text": " And linear means create the weight matrix create the biases", "tokens": [400, 8213, 1355, 1884, 264, 3364, 8141, 1884, 264, 32152], "temperature": 0.0, "avg_logprob": -0.25632028920309885, "compression_ratio": 1.556338028169014, "no_speech_prob": 2.2959127363719745e-06}, {"id": 407, "seek": 196170, "start": 1980.78, "end": 1982.7, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.25632028920309885, "compression_ratio": 1.556338028169014, "no_speech_prob": 2.2959127363719745e-06}, {"id": 408, "seek": 196170, "start": 1982.7, "end": 1985.46, "text": " So that's that's it right?", "tokens": [407, 300, 311, 300, 311, 309, 558, 30], "temperature": 0.0, "avg_logprob": -0.25632028920309885, "compression_ratio": 1.556338028169014, "no_speech_prob": 2.2959127363719745e-06}, {"id": 409, "seek": 198546, "start": 1985.46, "end": 1993.54, "text": " So we can then you know as we started to talk about like turn this into a deep neural network", "tokens": [407, 321, 393, 550, 291, 458, 382, 321, 1409, 281, 751, 466, 411, 1261, 341, 666, 257, 2452, 18161, 3209], "temperature": 0.0, "avg_logprob": -0.17752169307909513, "compression_ratio": 1.5527638190954773, "no_speech_prob": 2.190769009757787e-06}, {"id": 410, "seek": 198546, "start": 1995.1000000000001, "end": 1999.14, "text": " By saying you know rather than sending this straight off into", "tokens": [3146, 1566, 291, 458, 2831, 813, 7750, 341, 2997, 766, 666], "temperature": 0.0, "avg_logprob": -0.17752169307909513, "compression_ratio": 1.5527638190954773, "no_speech_prob": 2.190769009757787e-06}, {"id": 411, "seek": 198546, "start": 2000.1000000000001, "end": 2006.46, "text": " Ten activations, let's let's put it into say a hundred activations. We could pick whatever one number we like", "tokens": [9380, 2430, 763, 11, 718, 311, 718, 311, 829, 309, 666, 584, 257, 3262, 2430, 763, 13, 492, 727, 1888, 2035, 472, 1230, 321, 411], "temperature": 0.0, "avg_logprob": -0.17752169307909513, "compression_ratio": 1.5527638190954773, "no_speech_prob": 2.190769009757787e-06}, {"id": 412, "seek": 198546, "start": 2008.1000000000001, "end": 2010.1000000000001, "text": " Put it through a value to make it nonlinear", "tokens": [4935, 309, 807, 257, 2158, 281, 652, 309, 2107, 28263], "temperature": 0.0, "avg_logprob": -0.17752169307909513, "compression_ratio": 1.5527638190954773, "no_speech_prob": 2.190769009757787e-06}, {"id": 413, "seek": 201010, "start": 2010.1, "end": 2016.6399999999999, "text": " Put it through another linear layer another relu and then our final output with our final", "tokens": [4935, 309, 807, 1071, 8213, 4583, 1071, 1039, 84, 293, 550, 527, 2572, 5598, 365, 527, 2572], "temperature": 0.0, "avg_logprob": -0.2572751854950527, "compression_ratio": 1.4791666666666667, "no_speech_prob": 1.8738710423349403e-06}, {"id": 414, "seek": 201010, "start": 2016.9399999999998, "end": 2021.9199999999998, "text": " Activation function right and so this is now a deep network", "tokens": [28550, 399, 2445, 558, 293, 370, 341, 307, 586, 257, 2452, 3209], "temperature": 0.0, "avg_logprob": -0.2572751854950527, "compression_ratio": 1.4791666666666667, "no_speech_prob": 1.8738710423349403e-06}, {"id": 415, "seek": 201010, "start": 2028.82, "end": 2030.82, "text": " So we could fit that and", "tokens": [407, 321, 727, 3318, 300, 293], "temperature": 0.0, "avg_logprob": -0.2572751854950527, "compression_ratio": 1.4791666666666667, "no_speech_prob": 1.8738710423349403e-06}, {"id": 416, "seek": 201010, "start": 2032.54, "end": 2034.98, "text": " This time now because it's like deeper", "tokens": [639, 565, 586, 570, 309, 311, 411, 7731], "temperature": 0.0, "avg_logprob": -0.2572751854950527, "compression_ratio": 1.4791666666666667, "no_speech_prob": 1.8738710423349403e-06}, {"id": 417, "seek": 203498, "start": 2034.98, "end": 2040.78, "text": " I'm actually going to run a few more epochs that and you can see the accuracy", "tokens": [286, 478, 767, 516, 281, 1190, 257, 1326, 544, 30992, 28346, 300, 293, 291, 393, 536, 264, 14170], "temperature": 0.0, "avg_logprob": -0.21792547092881315, "compression_ratio": 1.5751295336787565, "no_speech_prob": 1.1079039552441827e-07}, {"id": 418, "seek": 203498, "start": 2041.46, "end": 2046.46, "text": " Increasing right so if you try and increase the learning rate here. It's like 0.1", "tokens": [30367, 3349, 558, 370, 498, 291, 853, 293, 3488, 264, 2539, 3314, 510, 13, 467, 311, 411, 1958, 13, 16], "temperature": 0.0, "avg_logprob": -0.21792547092881315, "compression_ratio": 1.5751295336787565, "no_speech_prob": 1.1079039552441827e-07}, {"id": 419, "seek": 203498, "start": 2047.6200000000001, "end": 2048.9, "text": " further", "tokens": [3052], "temperature": 0.0, "avg_logprob": -0.21792547092881315, "compression_ratio": 1.5751295336787565, "no_speech_prob": 1.1079039552441827e-07}, {"id": 420, "seek": 203498, "start": 2048.9, "end": 2050.9, "text": " it actually", "tokens": [309, 767], "temperature": 0.0, "avg_logprob": -0.21792547092881315, "compression_ratio": 1.5751295336787565, "no_speech_prob": 1.1079039552441827e-07}, {"id": 421, "seek": 203498, "start": 2050.94, "end": 2052.78, "text": " Starts to become unstable", "tokens": [6481, 82, 281, 1813, 23742], "temperature": 0.0, "avg_logprob": -0.21792547092881315, "compression_ratio": 1.5751295336787565, "no_speech_prob": 1.1079039552441827e-07}, {"id": 422, "seek": 203498, "start": 2052.78, "end": 2054.78, "text": " Now I'll show you a trick", "tokens": [823, 286, 603, 855, 291, 257, 4282], "temperature": 0.0, "avg_logprob": -0.21792547092881315, "compression_ratio": 1.5751295336787565, "no_speech_prob": 1.1079039552441827e-07}, {"id": 423, "seek": 203498, "start": 2054.78, "end": 2058.08, "text": " This is called learning rate annealing and the trick is this", "tokens": [639, 307, 1219, 2539, 3314, 22256, 4270, 293, 264, 4282, 307, 341], "temperature": 0.0, "avg_logprob": -0.21792547092881315, "compression_ratio": 1.5751295336787565, "no_speech_prob": 1.1079039552441827e-07}, {"id": 424, "seek": 203498, "start": 2058.86, "end": 2060.86, "text": " when you're", "tokens": [562, 291, 434], "temperature": 0.0, "avg_logprob": -0.21792547092881315, "compression_ratio": 1.5751295336787565, "no_speech_prob": 1.1079039552441827e-07}, {"id": 425, "seek": 206086, "start": 2060.86, "end": 2065.78, "text": " Trying to fit to a function right you've been taking a few steps", "tokens": [20180, 281, 3318, 281, 257, 2445, 558, 291, 600, 668, 1940, 257, 1326, 4439], "temperature": 0.0, "avg_logprob": -0.19226192235946654, "compression_ratio": 1.616504854368932, "no_speech_prob": 6.786721655771544e-07}, {"id": 426, "seek": 206086, "start": 2068.42, "end": 2072.78, "text": " Step step step as you get close to the middle like get close to the bottom", "tokens": [5470, 1823, 1823, 382, 291, 483, 1998, 281, 264, 2808, 411, 483, 1998, 281, 264, 2767], "temperature": 0.0, "avg_logprob": -0.19226192235946654, "compression_ratio": 1.616504854368932, "no_speech_prob": 6.786721655771544e-07}, {"id": 427, "seek": 206086, "start": 2073.5, "end": 2080.1400000000003, "text": " Your steps probably want to become smaller right otherwise what tends to happen is you start finding you're doing this", "tokens": [2260, 4439, 1391, 528, 281, 1813, 4356, 558, 5911, 437, 12258, 281, 1051, 307, 291, 722, 5006, 291, 434, 884, 341], "temperature": 0.0, "avg_logprob": -0.19226192235946654, "compression_ratio": 1.616504854368932, "no_speech_prob": 6.786721655771544e-07}, {"id": 428, "seek": 206086, "start": 2081.54, "end": 2088.1800000000003, "text": " Right and so you can actually see it here right it got 93 94 in a bit 94 6", "tokens": [1779, 293, 370, 291, 393, 767, 536, 309, 510, 558, 309, 658, 28876, 30849, 294, 257, 857, 30849, 1386], "temperature": 0.0, "avg_logprob": -0.19226192235946654, "compression_ratio": 1.616504854368932, "no_speech_prob": 6.786721655771544e-07}, {"id": 429, "seek": 208818, "start": 2088.18, "end": 2090.8199999999997, "text": " 94 8 like it's kind of starting to flatten out", "tokens": [30849, 1649, 411, 309, 311, 733, 295, 2891, 281, 24183, 484], "temperature": 0.0, "avg_logprob": -0.1738191604614258, "compression_ratio": 1.796, "no_speech_prob": 1.6797263242551708e-06}, {"id": 430, "seek": 208818, "start": 2091.62, "end": 2095.46, "text": " Right now that could be because it's kind of done as well as it can", "tokens": [1779, 586, 300, 727, 312, 570, 309, 311, 733, 295, 1096, 382, 731, 382, 309, 393], "temperature": 0.0, "avg_logprob": -0.1738191604614258, "compression_ratio": 1.796, "no_speech_prob": 1.6797263242551708e-06}, {"id": 431, "seek": 208818, "start": 2095.98, "end": 2098.62, "text": " Or it could be that it's going to going backwards and forwards", "tokens": [1610, 309, 727, 312, 300, 309, 311, 516, 281, 516, 12204, 293, 30126], "temperature": 0.0, "avg_logprob": -0.1738191604614258, "compression_ratio": 1.796, "no_speech_prob": 1.6797263242551708e-06}, {"id": 432, "seek": 208818, "start": 2098.8599999999997, "end": 2105.62, "text": " So what is a good idea is is later on in training is to decrease your learning rate and to take smaller steps", "tokens": [407, 437, 307, 257, 665, 1558, 307, 307, 1780, 322, 294, 3097, 307, 281, 11514, 428, 2539, 3314, 293, 281, 747, 4356, 4439], "temperature": 0.0, "avg_logprob": -0.1738191604614258, "compression_ratio": 1.796, "no_speech_prob": 1.6797263242551708e-06}, {"id": 433, "seek": 208818, "start": 2107.14, "end": 2112.7999999999997, "text": " Okay, that's called learning rate annealing, so there's a function in fast AI called set learning rates", "tokens": [1033, 11, 300, 311, 1219, 2539, 3314, 22256, 4270, 11, 370, 456, 311, 257, 2445, 294, 2370, 7318, 1219, 992, 2539, 6846], "temperature": 0.0, "avg_logprob": -0.1738191604614258, "compression_ratio": 1.796, "no_speech_prob": 1.6797263242551708e-06}, {"id": 434, "seek": 211280, "start": 2112.8, "end": 2117.96, "text": " you can pass in your optimizer and your new learning rate and", "tokens": [291, 393, 1320, 294, 428, 5028, 6545, 293, 428, 777, 2539, 3314, 293], "temperature": 0.0, "avg_logprob": -0.16686372554048579, "compression_ratio": 1.6752136752136753, "no_speech_prob": 4.222807547193952e-06}, {"id": 435, "seek": 211280, "start": 2118.76, "end": 2122.8, "text": " You know see if that helps right and very often it does", "tokens": [509, 458, 536, 498, 300, 3665, 558, 293, 588, 2049, 309, 775], "temperature": 0.0, "avg_logprob": -0.16686372554048579, "compression_ratio": 1.6752136752136753, "no_speech_prob": 4.222807547193952e-06}, {"id": 436, "seek": 211280, "start": 2125.84, "end": 2129.36, "text": " About about an order of magnitude in the deep learning course", "tokens": [7769, 466, 364, 1668, 295, 15668, 294, 264, 2452, 2539, 1164], "temperature": 0.0, "avg_logprob": -0.16686372554048579, "compression_ratio": 1.6752136752136753, "no_speech_prob": 4.222807547193952e-06}, {"id": 437, "seek": 211280, "start": 2129.36, "end": 2134.7200000000003, "text": " We learn a much much better technique than this to do this all automatically and at a more granular level", "tokens": [492, 1466, 257, 709, 709, 1101, 6532, 813, 341, 281, 360, 341, 439, 6772, 293, 412, 257, 544, 39962, 1496], "temperature": 0.0, "avg_logprob": -0.16686372554048579, "compression_ratio": 1.6752136752136753, "no_speech_prob": 4.222807547193952e-06}, {"id": 438, "seek": 211280, "start": 2134.7200000000003, "end": 2139.36, "text": " But if you're doing it by hand, you know like an order of magnitude at a time is what?", "tokens": [583, 498, 291, 434, 884, 309, 538, 1011, 11, 291, 458, 411, 364, 1668, 295, 15668, 412, 257, 565, 307, 437, 30], "temperature": 0.0, "avg_logprob": -0.16686372554048579, "compression_ratio": 1.6752136752136753, "no_speech_prob": 4.222807547193952e-06}, {"id": 439, "seek": 213936, "start": 2139.36, "end": 2142.36, "text": " people generally do", "tokens": [561, 5101, 360], "temperature": 0.0, "avg_logprob": -0.24779410111276726, "compression_ratio": 1.6166666666666667, "no_speech_prob": 8.059418519223982e-07}, {"id": 440, "seek": 213936, "start": 2142.92, "end": 2146.8, "text": " So you'll see people in papers talk about learning rate schedules", "tokens": [407, 291, 603, 536, 561, 294, 10577, 751, 466, 2539, 3314, 28078], "temperature": 0.0, "avg_logprob": -0.24779410111276726, "compression_ratio": 1.6166666666666667, "no_speech_prob": 8.059418519223982e-07}, {"id": 441, "seek": 213936, "start": 2147.32, "end": 2151.1800000000003, "text": " This is like an ending rate schedule so this schedule just a moment Erica", "tokens": [639, 307, 411, 364, 8121, 3314, 7567, 370, 341, 7567, 445, 257, 1623, 37429], "temperature": 0.0, "avg_logprob": -0.24779410111276726, "compression_ratio": 1.6166666666666667, "no_speech_prob": 8.059418519223982e-07}, {"id": 442, "seek": 213936, "start": 2151.1800000000003, "end": 2155.7400000000002, "text": " I just come to earnest first has got us to 97 right and I tried", "tokens": [286, 445, 808, 281, 48171, 700, 575, 658, 505, 281, 23399, 558, 293, 286, 3031], "temperature": 0.0, "avg_logprob": -0.24779410111276726, "compression_ratio": 1.6166666666666667, "no_speech_prob": 8.059418519223982e-07}, {"id": 443, "seek": 213936, "start": 2157.1200000000003, "end": 2160.8, "text": " Kind of going further, and we don't seem to be able to get much better than that so yeah", "tokens": [9242, 295, 516, 3052, 11, 293, 321, 500, 380, 1643, 281, 312, 1075, 281, 483, 709, 1101, 813, 300, 370, 1338], "temperature": 0.0, "avg_logprob": -0.24779410111276726, "compression_ratio": 1.6166666666666667, "no_speech_prob": 8.059418519223982e-07}, {"id": 444, "seek": 213936, "start": 2160.8, "end": 2165.96, "text": " So here we've got something where we can get 97 percent accuracy yes, Erica", "tokens": [407, 510, 321, 600, 658, 746, 689, 321, 393, 483, 23399, 3043, 14170, 2086, 11, 37429], "temperature": 0.0, "avg_logprob": -0.24779410111276726, "compression_ratio": 1.6166666666666667, "no_speech_prob": 8.059418519223982e-07}, {"id": 445, "seek": 216596, "start": 2165.96, "end": 2171.28, "text": " So it seems like you change a learning rate to something very small", "tokens": [407, 309, 2544, 411, 291, 1319, 257, 2539, 3314, 281, 746, 588, 1359], "temperature": 0.0, "avg_logprob": -0.2586800286529261, "compression_ratio": 1.6818181818181819, "no_speech_prob": 5.77182663619169e-06}, {"id": 446, "seek": 216596, "start": 2171.92, "end": 2175.84, "text": " Ten times smaller than we started with so we had point one now. It's point. I won yep", "tokens": [9380, 1413, 4356, 813, 321, 1409, 365, 370, 321, 632, 935, 472, 586, 13, 467, 311, 935, 13, 286, 1582, 18633], "temperature": 0.0, "avg_logprob": -0.2586800286529261, "compression_ratio": 1.6818181818181819, "no_speech_prob": 5.77182663619169e-06}, {"id": 447, "seek": 216596, "start": 2176.32, "end": 2179.56, "text": " But that makes the whole model train really slow", "tokens": [583, 300, 1669, 264, 1379, 2316, 3847, 534, 2964], "temperature": 0.0, "avg_logprob": -0.2586800286529261, "compression_ratio": 1.6818181818181819, "no_speech_prob": 5.77182663619169e-06}, {"id": 448, "seek": 216596, "start": 2179.56, "end": 2184.2, "text": " So I was wondering if you can make it so that it changes dynamically as it approaches", "tokens": [407, 286, 390, 6359, 498, 291, 393, 652, 309, 370, 300, 309, 2962, 43492, 382, 309, 11587], "temperature": 0.0, "avg_logprob": -0.2586800286529261, "compression_ratio": 1.6818181818181819, "no_speech_prob": 5.77182663619169e-06}, {"id": 449, "seek": 216596, "start": 2184.92, "end": 2188.76, "text": " Closer to the minima yeah pretty much yeah, so so that's some of the stuff", "tokens": [2033, 22150, 281, 264, 4464, 64, 1338, 1238, 709, 1338, 11, 370, 370, 300, 311, 512, 295, 264, 1507], "temperature": 0.0, "avg_logprob": -0.2586800286529261, "compression_ratio": 1.6818181818181819, "no_speech_prob": 5.77182663619169e-06}, {"id": 450, "seek": 216596, "start": 2188.76, "end": 2193.16, "text": " We learn in the deep learning course is these more advanced approaches. Yeah, so", "tokens": [492, 1466, 294, 264, 2452, 2539, 1164, 307, 613, 544, 7339, 11587, 13, 865, 11, 370], "temperature": 0.0, "avg_logprob": -0.2586800286529261, "compression_ratio": 1.6818181818181819, "no_speech_prob": 5.77182663619169e-06}, {"id": 451, "seek": 219316, "start": 2193.16, "end": 2199.8399999999997, "text": " So how it is different from using atom optimizer or something that's the kind of stuff we can do", "tokens": [407, 577, 309, 307, 819, 490, 1228, 12018, 5028, 6545, 420, 746, 300, 311, 264, 733, 295, 1507, 321, 393, 360], "temperature": 0.0, "avg_logprob": -0.20923446986986244, "compression_ratio": 1.6851851851851851, "no_speech_prob": 1.7777552784536965e-05}, {"id": 452, "seek": 219316, "start": 2199.8399999999997, "end": 2204.3999999999996, "text": " I mean you still need annealing as I say we do this kind of stuff in the deep learning course so for now", "tokens": [286, 914, 291, 920, 643, 22256, 4270, 382, 286, 584, 321, 360, 341, 733, 295, 1507, 294, 264, 2452, 2539, 1164, 370, 337, 586], "temperature": 0.0, "avg_logprob": -0.20923446986986244, "compression_ratio": 1.6851851851851851, "no_speech_prob": 1.7777552784536965e-05}, {"id": 453, "seek": 219316, "start": 2204.3999999999996, "end": 2206.56, "text": " We're just going to stick to standard SGD I", "tokens": [492, 434, 445, 516, 281, 2897, 281, 3832, 34520, 35, 286], "temperature": 0.0, "avg_logprob": -0.20923446986986244, "compression_ratio": 1.6851851851851851, "no_speech_prob": 1.7777552784536965e-05}, {"id": 454, "seek": 219316, "start": 2209.24, "end": 2213.6, "text": " Had a question about the data loading yeah, I know it's a fast AI function", "tokens": [12298, 257, 1168, 466, 264, 1412, 15114, 1338, 11, 286, 458, 309, 311, 257, 2370, 7318, 2445], "temperature": 0.0, "avg_logprob": -0.20923446986986244, "compression_ratio": 1.6851851851851851, "no_speech_prob": 1.7777552784536965e-05}, {"id": 455, "seek": 219316, "start": 2213.6, "end": 2218.64, "text": " But could you go into a little bit detail of how it's creating batches how it's learning data and how it's making those decisions", "tokens": [583, 727, 291, 352, 666, 257, 707, 857, 2607, 295, 577, 309, 311, 4084, 15245, 279, 577, 309, 311, 2539, 1412, 293, 577, 309, 311, 1455, 729, 5327], "temperature": 0.0, "avg_logprob": -0.20923446986986244, "compression_ratio": 1.6851851851851851, "no_speech_prob": 1.7777552784536965e-05}, {"id": 456, "seek": 221864, "start": 2218.64, "end": 2223.3599999999997, "text": " Sure", "tokens": [4894], "temperature": 0.0, "avg_logprob": -0.20119183037870675, "compression_ratio": 1.6682464454976302, "no_speech_prob": 5.255335054243915e-06}, {"id": 457, "seek": 221864, "start": 2223.3599999999997, "end": 2228.16, "text": " It'd be good to ask that on Monday night, so we can talk about in detail in the deep learning class", "tokens": [467, 1116, 312, 665, 281, 1029, 300, 322, 8138, 1818, 11, 370, 321, 393, 751, 466, 294, 2607, 294, 264, 2452, 2539, 1508], "temperature": 0.0, "avg_logprob": -0.20119183037870675, "compression_ratio": 1.6682464454976302, "no_speech_prob": 5.255335054243915e-06}, {"id": 458, "seek": 221864, "start": 2228.64, "end": 2230.96, "text": " But let's let's do the quick version here", "tokens": [583, 718, 311, 718, 311, 360, 264, 1702, 3037, 510], "temperature": 0.0, "avg_logprob": -0.20119183037870675, "compression_ratio": 1.6682464454976302, "no_speech_prob": 5.255335054243915e-06}, {"id": 459, "seek": 221864, "start": 2231.8799999999997, "end": 2233.7599999999998, "text": " so basically", "tokens": [370, 1936], "temperature": 0.0, "avg_logprob": -0.20119183037870675, "compression_ratio": 1.6682464454976302, "no_speech_prob": 5.255335054243915e-06}, {"id": 460, "seek": 221864, "start": 2233.7599999999998, "end": 2236.3199999999997, "text": " There's a really nice design in pytorch", "tokens": [821, 311, 257, 534, 1481, 1715, 294, 25878, 284, 339], "temperature": 0.0, "avg_logprob": -0.20119183037870675, "compression_ratio": 1.6682464454976302, "no_speech_prob": 5.255335054243915e-06}, {"id": 461, "seek": 221864, "start": 2237.12, "end": 2241.12, "text": " Where they basically say let's let's create a thing called a data set", "tokens": [2305, 436, 1936, 584, 718, 311, 718, 311, 1884, 257, 551, 1219, 257, 1412, 992], "temperature": 0.0, "avg_logprob": -0.20119183037870675, "compression_ratio": 1.6682464454976302, "no_speech_prob": 5.255335054243915e-06}, {"id": 462, "seek": 224112, "start": 2241.12, "end": 2248.12, "text": " Right and a data set is basically something that looks like a list it has a length right", "tokens": [1779, 293, 257, 1412, 992, 307, 1936, 746, 300, 1542, 411, 257, 1329, 309, 575, 257, 4641, 558], "temperature": 0.0, "avg_logprob": -0.21517872529871324, "compression_ratio": 1.8131868131868132, "no_speech_prob": 7.183206435001921e-06}, {"id": 463, "seek": 224112, "start": 2250.0, "end": 2255.7799999999997, "text": " And so that's like how many images are in the data set and it has the ability to", "tokens": [400, 370, 300, 311, 411, 577, 867, 5267, 366, 294, 264, 1412, 992, 293, 309, 575, 264, 3485, 281], "temperature": 0.0, "avg_logprob": -0.21517872529871324, "compression_ratio": 1.8131868131868132, "no_speech_prob": 7.183206435001921e-06}, {"id": 464, "seek": 224112, "start": 2256.92, "end": 2261.7999999999997, "text": " Index into it like a list right so if you had like D equals data set", "tokens": [33552, 666, 309, 411, 257, 1329, 558, 370, 498, 291, 632, 411, 413, 6915, 1412, 992], "temperature": 0.0, "avg_logprob": -0.21517872529871324, "compression_ratio": 1.8131868131868132, "no_speech_prob": 7.183206435001921e-06}, {"id": 465, "seek": 224112, "start": 2262.2799999999997, "end": 2267.8399999999997, "text": " You can do length D, and you can do D of some index right that's basically all the data set", "tokens": [509, 393, 360, 4641, 413, 11, 293, 291, 393, 360, 413, 295, 512, 8186, 558, 300, 311, 1936, 439, 264, 1412, 992], "temperature": 0.0, "avg_logprob": -0.21517872529871324, "compression_ratio": 1.8131868131868132, "no_speech_prob": 7.183206435001921e-06}, {"id": 466, "seek": 226784, "start": 2267.84, "end": 2273.2200000000003, "text": " Is as far as pytorch is concerned and so you start with a data set so it's like okay?", "tokens": [1119, 382, 1400, 382, 25878, 284, 339, 307, 5922, 293, 370, 291, 722, 365, 257, 1412, 992, 370, 309, 311, 411, 1392, 30], "temperature": 0.0, "avg_logprob": -0.2338486789317613, "compression_ratio": 1.6567164179104477, "no_speech_prob": 2.7693954507412855e-06}, {"id": 467, "seek": 226784, "start": 2274.08, "end": 2278.44, "text": " D three gives you the third image you know or whatever and", "tokens": [413, 1045, 2709, 291, 264, 2636, 3256, 291, 458, 420, 2035, 293], "temperature": 0.0, "avg_logprob": -0.2338486789317613, "compression_ratio": 1.6567164179104477, "no_speech_prob": 2.7693954507412855e-06}, {"id": 468, "seek": 226784, "start": 2278.96, "end": 2284.98, "text": " So then the idea is that you can take a data set and you can pass that into a constructor for a data loader", "tokens": [407, 550, 264, 1558, 307, 300, 291, 393, 747, 257, 1412, 992, 293, 291, 393, 1320, 300, 666, 257, 47479, 337, 257, 1412, 3677, 260], "temperature": 0.0, "avg_logprob": -0.2338486789317613, "compression_ratio": 1.6567164179104477, "no_speech_prob": 2.7693954507412855e-06}, {"id": 469, "seek": 226784, "start": 2290.8, "end": 2292.04, "text": " And", "tokens": [400], "temperature": 0.0, "avg_logprob": -0.2338486789317613, "compression_ratio": 1.6567164179104477, "no_speech_prob": 2.7693954507412855e-06}, {"id": 470, "seek": 229204, "start": 2292.04, "end": 2297.96, "text": " That gives you something which is now iterable right so you can now say it a", "tokens": [663, 2709, 291, 746, 597, 307, 586, 17138, 712, 558, 370, 291, 393, 586, 584, 309, 257], "temperature": 0.0, "avg_logprob": -0.23033307946246603, "compression_ratio": 1.7172774869109948, "no_speech_prob": 1.9033815306102042e-06}, {"id": 471, "seek": 229204, "start": 2298.4, "end": 2301.54, "text": " Dl, and that's something that you can call next on", "tokens": [413, 75, 11, 293, 300, 311, 746, 300, 291, 393, 818, 958, 322], "temperature": 0.0, "avg_logprob": -0.23033307946246603, "compression_ratio": 1.7172774869109948, "no_speech_prob": 1.9033815306102042e-06}, {"id": 472, "seek": 229204, "start": 2302.56, "end": 2303.68, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.23033307946246603, "compression_ratio": 1.7172774869109948, "no_speech_prob": 1.9033815306102042e-06}, {"id": 473, "seek": 229204, "start": 2303.68, "end": 2305.68, "text": " What that now is going to do is?", "tokens": [708, 300, 586, 307, 516, 281, 360, 307, 30], "temperature": 0.0, "avg_logprob": -0.23033307946246603, "compression_ratio": 1.7172774869109948, "no_speech_prob": 1.9033815306102042e-06}, {"id": 474, "seek": 229204, "start": 2306.04, "end": 2312.96, "text": " If when you do this you can choose to have shuffle on or shuffle off shuffle on means give me random mini-batch", "tokens": [759, 562, 291, 360, 341, 291, 393, 2826, 281, 362, 39426, 322, 420, 39426, 766, 39426, 322, 1355, 976, 385, 4974, 8382, 12, 65, 852], "temperature": 0.0, "avg_logprob": -0.23033307946246603, "compression_ratio": 1.7172774869109948, "no_speech_prob": 1.9033815306102042e-06}, {"id": 475, "seek": 229204, "start": 2313.2, "end": 2315.2, "text": " Shuffle off means go through it sequentially", "tokens": [1160, 21665, 766, 1355, 352, 807, 309, 5123, 3137], "temperature": 0.0, "avg_logprob": -0.23033307946246603, "compression_ratio": 1.7172774869109948, "no_speech_prob": 1.9033815306102042e-06}, {"id": 476, "seek": 229204, "start": 2317.16, "end": 2319.0, "text": " and so", "tokens": [293, 370], "temperature": 0.0, "avg_logprob": -0.23033307946246603, "compression_ratio": 1.7172774869109948, "no_speech_prob": 1.9033815306102042e-06}, {"id": 477, "seek": 231900, "start": 2319.0, "end": 2325.24, "text": " What the data loader does now when you say next is it basically assuming you said shuffle equals true is it's going to grab?", "tokens": [708, 264, 1412, 3677, 260, 775, 586, 562, 291, 584, 958, 307, 309, 1936, 11926, 291, 848, 39426, 6915, 2074, 307, 309, 311, 516, 281, 4444, 30], "temperature": 0.0, "avg_logprob": -0.17782267858815748, "compression_ratio": 1.5109170305676856, "no_speech_prob": 1.3925435951023246e-06}, {"id": 478, "seek": 231900, "start": 2325.4, "end": 2332.36, "text": " You know if you've got a batch size of 64 64 random integers between zero and length and call this", "tokens": [509, 458, 498, 291, 600, 658, 257, 15245, 2744, 295, 12145, 12145, 4974, 41674, 1296, 4018, 293, 4641, 293, 818, 341], "temperature": 0.0, "avg_logprob": -0.17782267858815748, "compression_ratio": 1.5109170305676856, "no_speech_prob": 1.3925435951023246e-06}, {"id": 479, "seek": 231900, "start": 2333.24, "end": 2338.36, "text": " 64 times to get 64 different items and jam them together", "tokens": [12145, 1413, 281, 483, 12145, 819, 4754, 293, 7872, 552, 1214], "temperature": 0.0, "avg_logprob": -0.17782267858815748, "compression_ratio": 1.5109170305676856, "no_speech_prob": 1.3925435951023246e-06}, {"id": 480, "seek": 231900, "start": 2339.96, "end": 2342.92, "text": " So fast AI uses the exact same", "tokens": [407, 2370, 7318, 4960, 264, 1900, 912], "temperature": 0.0, "avg_logprob": -0.17782267858815748, "compression_ratio": 1.5109170305676856, "no_speech_prob": 1.3925435951023246e-06}, {"id": 481, "seek": 231900, "start": 2344.08, "end": 2346.52, "text": " Terminology and the exact same API", "tokens": [19835, 259, 1793, 293, 264, 1900, 912, 9362], "temperature": 0.0, "avg_logprob": -0.17782267858815748, "compression_ratio": 1.5109170305676856, "no_speech_prob": 1.3925435951023246e-06}, {"id": 482, "seek": 234652, "start": 2346.52, "end": 2348.44, "text": " we", "tokens": [321], "temperature": 0.0, "avg_logprob": -0.26072317620982294, "compression_ratio": 1.7086614173228347, "no_speech_prob": 7.766874659864698e-06}, {"id": 483, "seek": 234652, "start": 2348.44, "end": 2355.32, "text": " Just do some of the details differently so specifically particularly with computer vision you often want to do a lot of", "tokens": [1449, 360, 512, 295, 264, 4365, 7614, 370, 4682, 4098, 365, 3820, 5201, 291, 2049, 528, 281, 360, 257, 688, 295], "temperature": 0.0, "avg_logprob": -0.26072317620982294, "compression_ratio": 1.7086614173228347, "no_speech_prob": 7.766874659864698e-06}, {"id": 484, "seek": 234652, "start": 2356.32, "end": 2363.96, "text": " So much pre-processing data augmentation like flipping changing the colors a little bit rotating those turn out to be really computationally expensive", "tokens": [407, 709, 659, 12, 41075, 278, 1412, 14501, 19631, 411, 26886, 4473, 264, 4577, 257, 707, 857, 19627, 729, 1261, 484, 281, 312, 534, 24903, 379, 5124], "temperature": 0.0, "avg_logprob": -0.26072317620982294, "compression_ratio": 1.7086614173228347, "no_speech_prob": 7.766874659864698e-06}, {"id": 485, "seek": 234652, "start": 2364.64, "end": 2367.8, "text": " Even just reading the JPEGs turns out to be computationally expensive", "tokens": [2754, 445, 3760, 264, 508, 5208, 33715, 4523, 484, 281, 312, 24903, 379, 5124], "temperature": 0.0, "avg_logprob": -0.26072317620982294, "compression_ratio": 1.7086614173228347, "no_speech_prob": 7.766874659864698e-06}, {"id": 486, "seek": 234652, "start": 2368.64, "end": 2373.7599999999998, "text": " So pie torch uses an approach where it fires off multiple processes to do that in parallel", "tokens": [407, 1730, 27822, 4960, 364, 3109, 689, 309, 15044, 766, 3866, 7555, 281, 360, 300, 294, 8952], "temperature": 0.0, "avg_logprob": -0.26072317620982294, "compression_ratio": 1.7086614173228347, "no_speech_prob": 7.766874659864698e-06}, {"id": 487, "seek": 237376, "start": 2373.76, "end": 2377.1200000000003, "text": " Where else the fast AI library instead does something called multi-threading?", "tokens": [2305, 1646, 264, 2370, 7318, 6405, 2602, 775, 746, 1219, 4825, 12, 392, 35908, 30], "temperature": 0.0, "avg_logprob": -0.2515479893360323, "compression_ratio": 1.6858407079646018, "no_speech_prob": 3.668833642223035e-06}, {"id": 488, "seek": 237376, "start": 2377.76, "end": 2382.7200000000003, "text": " Which is a much can be a much faster way of doing it. Yes, you're net", "tokens": [3013, 307, 257, 709, 393, 312, 257, 709, 4663, 636, 295, 884, 309, 13, 1079, 11, 291, 434, 2533], "temperature": 0.0, "avg_logprob": -0.2515479893360323, "compression_ratio": 1.6858407079646018, "no_speech_prob": 3.668833642223035e-06}, {"id": 489, "seek": 237376, "start": 2386.1200000000003, "end": 2387.2000000000003, "text": " So", "tokens": [407], "temperature": 0.0, "avg_logprob": -0.2515479893360323, "compression_ratio": 1.6858407079646018, "no_speech_prob": 3.668833642223035e-06}, {"id": 490, "seek": 237376, "start": 2387.2000000000003, "end": 2392.0800000000004, "text": " An epoch is a real epoch in the sense that all of the elements", "tokens": [1107, 30992, 339, 307, 257, 957, 30992, 339, 294, 264, 2020, 300, 439, 295, 264, 4959], "temperature": 0.0, "avg_logprob": -0.2515479893360323, "compression_ratio": 1.6858407079646018, "no_speech_prob": 3.668833642223035e-06}, {"id": 491, "seek": 237376, "start": 2392.0800000000004, "end": 2395.84, "text": " So it's a shuffle at the beginning of the epoch something like that. Yeah. Yeah", "tokens": [407, 309, 311, 257, 39426, 412, 264, 2863, 295, 264, 30992, 339, 746, 411, 300, 13, 865, 13, 865], "temperature": 0.0, "avg_logprob": -0.2515479893360323, "compression_ratio": 1.6858407079646018, "no_speech_prob": 3.668833642223035e-06}, {"id": 492, "seek": 237376, "start": 2395.84, "end": 2399.5600000000004, "text": " I mean not all libraries work the same way some do sampling with replacement", "tokens": [286, 914, 406, 439, 15148, 589, 264, 912, 636, 512, 360, 21179, 365, 14419], "temperature": 0.0, "avg_logprob": -0.2515479893360323, "compression_ratio": 1.6858407079646018, "no_speech_prob": 3.668833642223035e-06}, {"id": 493, "seek": 237376, "start": 2400.1200000000003, "end": 2401.6400000000003, "text": " some don't", "tokens": [512, 500, 380], "temperature": 0.0, "avg_logprob": -0.2515479893360323, "compression_ratio": 1.6858407079646018, "no_speech_prob": 3.668833642223035e-06}, {"id": 494, "seek": 240164, "start": 2401.64, "end": 2404.44, "text": " We actually the fast AI library", "tokens": [492, 767, 264, 2370, 7318, 6405], "temperature": 0.0, "avg_logprob": -0.20570495982228973, "compression_ratio": 1.6435643564356435, "no_speech_prob": 3.2377399747929303e-06}, {"id": 495, "seek": 240164, "start": 2404.96, "end": 2410.8399999999997, "text": " Hands-off the shuffling off to the set to the actual pie torch version and I believe the pie torch version", "tokens": [21369, 12, 4506, 264, 402, 1245, 1688, 766, 281, 264, 992, 281, 264, 3539, 1730, 27822, 3037, 293, 286, 1697, 264, 1730, 27822, 3037], "temperature": 0.0, "avg_logprob": -0.20570495982228973, "compression_ratio": 1.6435643564356435, "no_speech_prob": 3.2377399747929303e-06}, {"id": 496, "seek": 240164, "start": 2410.8399999999997, "end": 2415.2, "text": " Yeah, actually shuffles and an epoch covers everything once I believe", "tokens": [865, 11, 767, 402, 1245, 904, 293, 364, 30992, 339, 10538, 1203, 1564, 286, 1697], "temperature": 0.0, "avg_logprob": -0.20570495982228973, "compression_ratio": 1.6435643564356435, "no_speech_prob": 3.2377399747929303e-06}, {"id": 497, "seek": 240164, "start": 2417.16, "end": 2425.12, "text": " Okay now the thing is when you start to get these bigger networks", "tokens": [1033, 586, 264, 551, 307, 562, 291, 722, 281, 483, 613, 3801, 9590], "temperature": 0.0, "avg_logprob": -0.20570495982228973, "compression_ratio": 1.6435643564356435, "no_speech_prob": 3.2377399747929303e-06}, {"id": 498, "seek": 242512, "start": 2425.12, "end": 2429.7599999999998, "text": " Potentially you're getting quite a few parameters", "tokens": [9145, 3137, 291, 434, 1242, 1596, 257, 1326, 9834], "temperature": 0.0, "avg_logprob": -0.23353567990389737, "compression_ratio": 1.6995073891625616, "no_speech_prob": 2.0261347799532814e-06}, {"id": 499, "seek": 242512, "start": 2430.88, "end": 2432.88, "text": " right, so", "tokens": [558, 11, 370], "temperature": 0.0, "avg_logprob": -0.23353567990389737, "compression_ratio": 1.6995073891625616, "no_speech_prob": 2.0261347799532814e-06}, {"id": 500, "seek": 242512, "start": 2432.88, "end": 2437.92, "text": " I want to ask you to calculate how many parameters there are but let's let's remember here. We've got", "tokens": [286, 528, 281, 1029, 291, 281, 8873, 577, 867, 9834, 456, 366, 457, 718, 311, 718, 311, 1604, 510, 13, 492, 600, 658], "temperature": 0.0, "avg_logprob": -0.23353567990389737, "compression_ratio": 1.6995073891625616, "no_speech_prob": 2.0261347799532814e-06}, {"id": 501, "seek": 242512, "start": 2438.8399999999997, "end": 2444.06, "text": " 28 by 28 input into a hundred output and then a hundred into a hundred and then a hundred into ten", "tokens": [7562, 538, 7562, 4846, 666, 257, 3262, 5598, 293, 550, 257, 3262, 666, 257, 3262, 293, 550, 257, 3262, 666, 2064], "temperature": 0.0, "avg_logprob": -0.23353567990389737, "compression_ratio": 1.6995073891625616, "no_speech_prob": 2.0261347799532814e-06}, {"id": 502, "seek": 242512, "start": 2444.7999999999997, "end": 2447.92, "text": " Alright, and then for each of those you've got weights and biases", "tokens": [2798, 11, 293, 550, 337, 1184, 295, 729, 291, 600, 658, 17443, 293, 32152], "temperature": 0.0, "avg_logprob": -0.23353567990389737, "compression_ratio": 1.6995073891625616, "no_speech_prob": 2.0261347799532814e-06}, {"id": 503, "seek": 242512, "start": 2449.08, "end": 2451.08, "text": " So we can actually", "tokens": [407, 321, 393, 767], "temperature": 0.0, "avg_logprob": -0.23353567990389737, "compression_ratio": 1.6995073891625616, "no_speech_prob": 2.0261347799532814e-06}, {"id": 504, "seek": 245108, "start": 2451.08, "end": 2455.54, "text": " Do this net dot parameters", "tokens": [1144, 341, 2533, 5893, 9834], "temperature": 0.0, "avg_logprob": -0.22014866599553748, "compression_ratio": 1.7195767195767195, "no_speech_prob": 7.646469384781085e-06}, {"id": 505, "seek": 245108, "start": 2456.3199999999997, "end": 2460.7999999999997, "text": " Returns a list where each element of the list is a matrix", "tokens": [24350, 82, 257, 1329, 689, 1184, 4478, 295, 264, 1329, 307, 257, 8141], "temperature": 0.0, "avg_logprob": -0.22014866599553748, "compression_ratio": 1.7195767195767195, "no_speech_prob": 7.646469384781085e-06}, {"id": 506, "seek": 245108, "start": 2460.7999999999997, "end": 2465.88, "text": " So actually a tensor of the parameters for that not just for that layer", "tokens": [407, 767, 257, 40863, 295, 264, 9834, 337, 300, 406, 445, 337, 300, 4583], "temperature": 0.0, "avg_logprob": -0.22014866599553748, "compression_ratio": 1.7195767195767195, "no_speech_prob": 7.646469384781085e-06}, {"id": 507, "seek": 245108, "start": 2465.88, "end": 2469.84, "text": " But if it's a layer with both weights and biases that would be two parameters, right?", "tokens": [583, 498, 309, 311, 257, 4583, 365, 1293, 17443, 293, 32152, 300, 576, 312, 732, 9834, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.22014866599553748, "compression_ratio": 1.7195767195767195, "no_speech_prob": 7.646469384781085e-06}, {"id": 508, "seek": 245108, "start": 2469.84, "end": 2475.0, "text": " So basically returns us a list of all of the tensors containing the the parameters", "tokens": [407, 1936, 11247, 505, 257, 1329, 295, 439, 295, 264, 10688, 830, 19273, 264, 264, 9834], "temperature": 0.0, "avg_logprob": -0.22014866599553748, "compression_ratio": 1.7195767195767195, "no_speech_prob": 7.646469384781085e-06}, {"id": 509, "seek": 247500, "start": 2475.0, "end": 2482.0, "text": " Num elements in pie torch tells you how how big that is right so if I run this", "tokens": [22592, 4959, 294, 1730, 27822, 5112, 291, 577, 577, 955, 300, 307, 558, 370, 498, 286, 1190, 341], "temperature": 0.0, "avg_logprob": -0.2327054055889001, "compression_ratio": 1.6504854368932038, "no_speech_prob": 9.276342325392761e-07}, {"id": 510, "seek": 247500, "start": 2483.84, "end": 2485.84, "text": " Here is the number of", "tokens": [1692, 307, 264, 1230, 295], "temperature": 0.0, "avg_logprob": -0.2327054055889001, "compression_ratio": 1.6504854368932038, "no_speech_prob": 9.276342325392761e-07}, {"id": 511, "seek": 247500, "start": 2486.76, "end": 2488.76, "text": " Parameters in each layer so I've got", "tokens": [34882, 6202, 294, 1184, 4583, 370, 286, 600, 658], "temperature": 0.0, "avg_logprob": -0.2327054055889001, "compression_ratio": 1.6504854368932038, "no_speech_prob": 9.276342325392761e-07}, {"id": 512, "seek": 247500, "start": 2489.32, "end": 2496.48, "text": " 784 inputs and the first layer has a hundred outputs so therefore the first weight matrix is of size 78,400", "tokens": [26369, 19, 15743, 293, 264, 700, 4583, 575, 257, 3262, 23930, 370, 4412, 264, 700, 3364, 8141, 307, 295, 2744, 26369, 11, 13741], "temperature": 0.0, "avg_logprob": -0.2327054055889001, "compression_ratio": 1.6504854368932038, "no_speech_prob": 9.276342325392761e-07}, {"id": 513, "seek": 247500, "start": 2497.32, "end": 2502.92, "text": " Okay, and the first bias vector is of size 100 and then the next one is a hundred by a hundred", "tokens": [1033, 11, 293, 264, 700, 12577, 8062, 307, 295, 2744, 2319, 293, 550, 264, 958, 472, 307, 257, 3262, 538, 257, 3262], "temperature": 0.0, "avg_logprob": -0.2327054055889001, "compression_ratio": 1.6504854368932038, "no_speech_prob": 9.276342325392761e-07}, {"id": 514, "seek": 250292, "start": 2502.92, "end": 2508.28, "text": " Okay, and there's a hundred and then the next one is a hundred by ten and then there's my bias", "tokens": [1033, 11, 293, 456, 311, 257, 3262, 293, 550, 264, 958, 472, 307, 257, 3262, 538, 2064, 293, 550, 456, 311, 452, 12577], "temperature": 0.0, "avg_logprob": -0.19763464086196003, "compression_ratio": 1.6936170212765957, "no_speech_prob": 7.93448748481751e-07}, {"id": 515, "seek": 250292, "start": 2508.36, "end": 2514.6800000000003, "text": " Okay, so there's the number of elements in each layer, and if I add them all up. It's nearly a hundred thousand", "tokens": [1033, 11, 370, 456, 311, 264, 1230, 295, 4959, 294, 1184, 4583, 11, 293, 498, 286, 909, 552, 439, 493, 13, 467, 311, 6217, 257, 3262, 4714], "temperature": 0.0, "avg_logprob": -0.19763464086196003, "compression_ratio": 1.6936170212765957, "no_speech_prob": 7.93448748481751e-07}, {"id": 516, "seek": 250292, "start": 2515.2000000000003, "end": 2516.36, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.19763464086196003, "compression_ratio": 1.6936170212765957, "no_speech_prob": 7.93448748481751e-07}, {"id": 517, "seek": 250292, "start": 2516.36, "end": 2521.64, "text": " And so I'm possibly at risk of overfitting here right so", "tokens": [400, 370, 286, 478, 6264, 412, 3148, 295, 670, 69, 2414, 510, 558, 370], "temperature": 0.0, "avg_logprob": -0.19763464086196003, "compression_ratio": 1.6936170212765957, "no_speech_prob": 7.93448748481751e-07}, {"id": 518, "seek": 250292, "start": 2522.48, "end": 2524.48, "text": " We might want to think about using regularization", "tokens": [492, 1062, 528, 281, 519, 466, 1228, 3890, 2144], "temperature": 0.0, "avg_logprob": -0.19763464086196003, "compression_ratio": 1.6936170212765957, "no_speech_prob": 7.93448748481751e-07}, {"id": 519, "seek": 250292, "start": 2525.04, "end": 2530.92, "text": " So a really simple common approach to regularization in all of machine learning", "tokens": [407, 257, 534, 2199, 2689, 3109, 281, 3890, 2144, 294, 439, 295, 3479, 2539], "temperature": 0.0, "avg_logprob": -0.19763464086196003, "compression_ratio": 1.6936170212765957, "no_speech_prob": 7.93448748481751e-07}, {"id": 520, "seek": 253092, "start": 2530.92, "end": 2534.52, "text": " Is something called L2?", "tokens": [1119, 746, 1219, 441, 17, 30], "temperature": 0.0, "avg_logprob": -0.30796198259320173, "compression_ratio": 1.4701986754966887, "no_speech_prob": 3.0415885703405365e-06}, {"id": 521, "seek": 253092, "start": 2538.04, "end": 2540.04, "text": " Regularization and", "tokens": [45659, 2144, 293], "temperature": 0.0, "avg_logprob": -0.30796198259320173, "compression_ratio": 1.4701986754966887, "no_speech_prob": 3.0415885703405365e-06}, {"id": 522, "seek": 253092, "start": 2540.2000000000003, "end": 2545.64, "text": " It's super important super handy you can use it with just about anything right and the basic idea", "tokens": [467, 311, 1687, 1021, 1687, 13239, 291, 393, 764, 309, 365, 445, 466, 1340, 558, 293, 264, 3875, 1558], "temperature": 0.0, "avg_logprob": -0.30796198259320173, "compression_ratio": 1.4701986754966887, "no_speech_prob": 3.0415885703405365e-06}, {"id": 523, "seek": 253092, "start": 2547.52, "end": 2549.52, "text": " Anyway so", "tokens": [5684, 370], "temperature": 0.0, "avg_logprob": -0.30796198259320173, "compression_ratio": 1.4701986754966887, "no_speech_prob": 3.0415885703405365e-06}, {"id": 524, "seek": 253092, "start": 2551.36, "end": 2555.7200000000003, "text": " L2 regularization the basic idea is this normally would say our loss is", "tokens": [441, 17, 3890, 2144, 264, 3875, 1558, 307, 341, 5646, 576, 584, 527, 4470, 307], "temperature": 0.0, "avg_logprob": -0.30796198259320173, "compression_ratio": 1.4701986754966887, "no_speech_prob": 3.0415885703405365e-06}, {"id": 525, "seek": 255572, "start": 2555.72, "end": 2563.24, "text": " Is equal to let's just do RMSE to keep things kind of simple. It's equal to our predictions minus our actuals", "tokens": [1119, 2681, 281, 718, 311, 445, 360, 23790, 5879, 281, 1066, 721, 733, 295, 2199, 13, 467, 311, 2681, 281, 527, 21264, 3175, 527, 3539, 82], "temperature": 0.0, "avg_logprob": -0.15616475921316245, "compression_ratio": 1.6200873362445414, "no_speech_prob": 1.8448196215103962e-06}, {"id": 526, "seek": 255572, "start": 2563.68, "end": 2567.7999999999997, "text": " You know squared and then we sum them up take the average", "tokens": [509, 458, 8889, 293, 550, 321, 2408, 552, 493, 747, 264, 4274], "temperature": 0.0, "avg_logprob": -0.15616475921316245, "compression_ratio": 1.6200873362445414, "no_speech_prob": 1.8448196215103962e-06}, {"id": 527, "seek": 255572, "start": 2568.3999999999996, "end": 2570.3999999999996, "text": " Take the square root, okay", "tokens": [3664, 264, 3732, 5593, 11, 1392], "temperature": 0.0, "avg_logprob": -0.15616475921316245, "compression_ratio": 1.6200873362445414, "no_speech_prob": 1.8448196215103962e-06}, {"id": 528, "seek": 255572, "start": 2570.9199999999996, "end": 2572.68, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.15616475921316245, "compression_ratio": 1.6200873362445414, "no_speech_prob": 1.8448196215103962e-06}, {"id": 529, "seek": 255572, "start": 2572.68, "end": 2577.3999999999996, "text": " What if we then want to say you know what like if I've got lots and lots of parameters", "tokens": [708, 498, 321, 550, 528, 281, 584, 291, 458, 437, 411, 498, 286, 600, 658, 3195, 293, 3195, 295, 9834], "temperature": 0.0, "avg_logprob": -0.15616475921316245, "compression_ratio": 1.6200873362445414, "no_speech_prob": 1.8448196215103962e-06}, {"id": 530, "seek": 255572, "start": 2578.72, "end": 2584.0, "text": " Don't use them unless they're really helping enough right like if you've got a million", "tokens": [1468, 380, 764, 552, 5969, 436, 434, 534, 4315, 1547, 558, 411, 498, 291, 600, 658, 257, 2459], "temperature": 0.0, "avg_logprob": -0.15616475921316245, "compression_ratio": 1.6200873362445414, "no_speech_prob": 1.8448196215103962e-06}, {"id": 531, "seek": 258400, "start": 2584.0, "end": 2586.0, "text": " Parameters, and you only really needed 10", "tokens": [34882, 6202, 11, 293, 291, 787, 534, 2978, 1266], "temperature": 0.0, "avg_logprob": -0.15871423663515033, "compression_ratio": 1.6079295154185023, "no_speech_prob": 4.1811830442384235e-07}, {"id": 532, "seek": 258400, "start": 2586.6, "end": 2592.84, "text": " Parameters to be useful just use 10 right so how could we like tell the loss function to do that?", "tokens": [34882, 6202, 281, 312, 4420, 445, 764, 1266, 558, 370, 577, 727, 321, 411, 980, 264, 4470, 2445, 281, 360, 300, 30], "temperature": 0.0, "avg_logprob": -0.15871423663515033, "compression_ratio": 1.6079295154185023, "no_speech_prob": 4.1811830442384235e-07}, {"id": 533, "seek": 258400, "start": 2592.84, "end": 2597.24, "text": " And so basically what we want to say is hey if a parameter is zero", "tokens": [400, 370, 1936, 437, 321, 528, 281, 584, 307, 4177, 498, 257, 13075, 307, 4018], "temperature": 0.0, "avg_logprob": -0.15871423663515033, "compression_ratio": 1.6079295154185023, "no_speech_prob": 4.1811830442384235e-07}, {"id": 534, "seek": 258400, "start": 2598.12, "end": 2603.0, "text": " That's no problem. It's like it doesn't exist at all so let's penalize a parameter", "tokens": [663, 311, 572, 1154, 13, 467, 311, 411, 309, 1177, 380, 2514, 412, 439, 370, 718, 311, 13661, 1125, 257, 13075], "temperature": 0.0, "avg_logprob": -0.15871423663515033, "compression_ratio": 1.6079295154185023, "no_speech_prob": 4.1811830442384235e-07}, {"id": 535, "seek": 258400, "start": 2604.0, "end": 2606.0, "text": " For not being zero", "tokens": [1171, 406, 885, 4018], "temperature": 0.0, "avg_logprob": -0.15871423663515033, "compression_ratio": 1.6079295154185023, "no_speech_prob": 4.1811830442384235e-07}, {"id": 536, "seek": 258400, "start": 2606.76, "end": 2609.94, "text": " All right, so what would be a way we could measure that?", "tokens": [1057, 558, 11, 370, 437, 576, 312, 257, 636, 321, 727, 3481, 300, 30], "temperature": 0.0, "avg_logprob": -0.15871423663515033, "compression_ratio": 1.6079295154185023, "no_speech_prob": 4.1811830442384235e-07}, {"id": 537, "seek": 260994, "start": 2609.94, "end": 2617.14, "text": " How can we like calculate how unzero our parameters are?", "tokens": [1012, 393, 321, 411, 8873, 577, 517, 32226, 527, 9834, 366, 30], "temperature": 0.0, "avg_logprob": -0.35226466438987036, "compression_ratio": 1.4550898203592815, "no_speech_prob": 3.340513785587973e-06}, {"id": 538, "seek": 260994, "start": 2620.9, "end": 2622.9, "text": " Can you pass that to Chen Chi please Ernest?", "tokens": [1664, 291, 1320, 300, 281, 13682, 17730, 1767, 24147, 377, 30], "temperature": 0.0, "avg_logprob": -0.35226466438987036, "compression_ratio": 1.4550898203592815, "no_speech_prob": 3.340513785587973e-06}, {"id": 539, "seek": 260994, "start": 2627.38, "end": 2633.06, "text": " You calculate the average of all the parameters. That's my first can't quite be the average", "tokens": [509, 8873, 264, 4274, 295, 439, 264, 9834, 13, 663, 311, 452, 700, 393, 380, 1596, 312, 264, 4274], "temperature": 0.0, "avg_logprob": -0.35226466438987036, "compression_ratio": 1.4550898203592815, "no_speech_prob": 3.340513785587973e-06}, {"id": 540, "seek": 260994, "start": 2633.82, "end": 2636.78, "text": " Close yes, Taylor. Yes, you figured it out, okay?", "tokens": [16346, 2086, 11, 12060, 13, 1079, 11, 291, 8932, 309, 484, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.35226466438987036, "compression_ratio": 1.4550898203592815, "no_speech_prob": 3.340513785587973e-06}, {"id": 541, "seek": 263678, "start": 2636.78, "end": 2639.34, "text": " I think so I think if we like", "tokens": [286, 519, 370, 286, 519, 498, 321, 411], "temperature": 0.0, "avg_logprob": -0.33058120857956064, "compression_ratio": 1.965648854961832, "no_speech_prob": 9.818168109632097e-06}, {"id": 542, "seek": 263678, "start": 2639.94, "end": 2643.82, "text": " Assuming all of our data has been normalized standardized however you want to call it", "tokens": [6281, 24919, 439, 295, 527, 1412, 575, 668, 48704, 31677, 4461, 291, 528, 281, 818, 309], "temperature": 0.0, "avg_logprob": -0.33058120857956064, "compression_ratio": 1.965648854961832, "no_speech_prob": 9.818168109632097e-06}, {"id": 543, "seek": 263678, "start": 2644.1000000000004, "end": 2649.6600000000003, "text": " We want to check that they're like significantly different from zero right would that be not the data that the parameters?", "tokens": [492, 528, 281, 1520, 300, 436, 434, 411, 10591, 819, 490, 4018, 558, 576, 300, 312, 406, 264, 1412, 300, 264, 9834, 30], "temperature": 0.0, "avg_logprob": -0.33058120857956064, "compression_ratio": 1.965648854961832, "no_speech_prob": 9.818168109632097e-06}, {"id": 544, "seek": 263678, "start": 2649.6600000000003, "end": 2655.0600000000004, "text": " The parameter is rather would be significantly and the parameters don't have to be normalized or anything that is calculated right yes", "tokens": [440, 13075, 307, 2831, 576, 312, 10591, 293, 264, 9834, 500, 380, 362, 281, 312, 48704, 420, 1340, 300, 307, 15598, 558, 2086], "temperature": 0.0, "avg_logprob": -0.33058120857956064, "compression_ratio": 1.965648854961832, "no_speech_prob": 9.818168109632097e-06}, {"id": 545, "seek": 263678, "start": 2655.0600000000004, "end": 2657.9, "text": " So it's a different from zero right I just", "tokens": [407, 309, 311, 257, 819, 490, 4018, 558, 286, 445], "temperature": 0.0, "avg_logprob": -0.33058120857956064, "compression_ratio": 1.965648854961832, "no_speech_prob": 9.818168109632097e-06}, {"id": 546, "seek": 263678, "start": 2658.7000000000003, "end": 2662.76, "text": " Assuming that the data has been normalized so that we can compare them on the oh yeah, I've got it", "tokens": [6281, 24919, 300, 264, 1412, 575, 668, 48704, 370, 300, 321, 393, 6794, 552, 322, 264, 1954, 1338, 11, 286, 600, 658, 309], "temperature": 0.0, "avg_logprob": -0.33058120857956064, "compression_ratio": 1.965648854961832, "no_speech_prob": 9.818168109632097e-06}, {"id": 547, "seek": 266276, "start": 2662.76, "end": 2667.38, "text": " Yeah, right, and then those that are not significantly different from zero", "tokens": [865, 11, 558, 11, 293, 550, 729, 300, 366, 406, 10591, 819, 490, 4018], "temperature": 0.0, "avg_logprob": -0.22601085238986546, "compression_ratio": 1.6282051282051282, "no_speech_prob": 1.7061751123037538e-06}, {"id": 548, "seek": 266276, "start": 2667.38, "end": 2670.2200000000003, "text": " We can probably just drop and I think Chen Chi is going to tell us how to do that", "tokens": [492, 393, 1391, 445, 3270, 293, 286, 519, 13682, 17730, 307, 516, 281, 980, 505, 577, 281, 360, 300], "temperature": 0.0, "avg_logprob": -0.22601085238986546, "compression_ratio": 1.6282051282051282, "no_speech_prob": 1.7061751123037538e-06}, {"id": 549, "seek": 266276, "start": 2670.2200000000003, "end": 2676.36, "text": " You just figured it out right the meaning of the absolute value could do that that would be called L1", "tokens": [509, 445, 8932, 309, 484, 558, 264, 3620, 295, 264, 8236, 2158, 727, 360, 300, 300, 576, 312, 1219, 441, 16], "temperature": 0.0, "avg_logprob": -0.22601085238986546, "compression_ratio": 1.6282051282051282, "no_speech_prob": 1.7061751123037538e-06}, {"id": 550, "seek": 266276, "start": 2676.36, "end": 2678.36, "text": " Which is great so L1?", "tokens": [3013, 307, 869, 370, 441, 16, 30], "temperature": 0.0, "avg_logprob": -0.22601085238986546, "compression_ratio": 1.6282051282051282, "no_speech_prob": 1.7061751123037538e-06}, {"id": 551, "seek": 266276, "start": 2679.0200000000004, "end": 2681.0200000000004, "text": " would be the", "tokens": [576, 312, 264], "temperature": 0.0, "avg_logprob": -0.22601085238986546, "compression_ratio": 1.6282051282051282, "no_speech_prob": 1.7061751123037538e-06}, {"id": 552, "seek": 266276, "start": 2681.2200000000003, "end": 2683.2200000000003, "text": " absolute", "tokens": [8236], "temperature": 0.0, "avg_logprob": -0.22601085238986546, "compression_ratio": 1.6282051282051282, "no_speech_prob": 1.7061751123037538e-06}, {"id": 553, "seek": 266276, "start": 2683.2200000000003, "end": 2688.0800000000004, "text": " Value of the weights average L2 is actually the sum", "tokens": [39352, 295, 264, 17443, 4274, 441, 17, 307, 767, 264, 2408], "temperature": 0.0, "avg_logprob": -0.22601085238986546, "compression_ratio": 1.6282051282051282, "no_speech_prob": 1.7061751123037538e-06}, {"id": 554, "seek": 266276, "start": 2689.0600000000004, "end": 2691.0600000000004, "text": " square root sum of squares", "tokens": [3732, 5593, 2408, 295, 19368], "temperature": 0.0, "avg_logprob": -0.22601085238986546, "compression_ratio": 1.6282051282051282, "no_speech_prob": 1.7061751123037538e-06}, {"id": 555, "seek": 269106, "start": 2691.06, "end": 2695.38, "text": " Yeah, yeah exactly so we just take this we can just we don't even have to square root", "tokens": [865, 11, 1338, 2293, 370, 321, 445, 747, 341, 321, 393, 445, 321, 500, 380, 754, 362, 281, 3732, 5593], "temperature": 0.0, "avg_logprob": -0.13193800764263802, "compression_ratio": 1.7835497835497836, "no_speech_prob": 2.368786908846232e-06}, {"id": 556, "seek": 269106, "start": 2695.38, "end": 2702.18, "text": " So we just take the squares of the weights themselves, and then like we want to be able to say like okay", "tokens": [407, 321, 445, 747, 264, 19368, 295, 264, 17443, 2969, 11, 293, 550, 411, 321, 528, 281, 312, 1075, 281, 584, 411, 1392], "temperature": 0.0, "avg_logprob": -0.13193800764263802, "compression_ratio": 1.7835497835497836, "no_speech_prob": 2.368786908846232e-06}, {"id": 557, "seek": 269106, "start": 2704.62, "end": 2710.7999999999997, "text": " How much do we want to penalize not being zero right because if we actually don't have that many parameters", "tokens": [1012, 709, 360, 321, 528, 281, 13661, 1125, 406, 885, 4018, 558, 570, 498, 321, 767, 500, 380, 362, 300, 867, 9834], "temperature": 0.0, "avg_logprob": -0.13193800764263802, "compression_ratio": 1.7835497835497836, "no_speech_prob": 2.368786908846232e-06}, {"id": 558, "seek": 269106, "start": 2710.7999999999997, "end": 2716.2999999999997, "text": " We don't want to regularize much at all if we've got heaps. We do want to regularize a lot right so then we put a", "tokens": [492, 500, 380, 528, 281, 3890, 1125, 709, 412, 439, 498, 321, 600, 658, 415, 2382, 13, 492, 360, 528, 281, 3890, 1125, 257, 688, 558, 370, 550, 321, 829, 257], "temperature": 0.0, "avg_logprob": -0.13193800764263802, "compression_ratio": 1.7835497835497836, "no_speech_prob": 2.368786908846232e-06}, {"id": 559, "seek": 271630, "start": 2716.3, "end": 2719.34, "text": " a parameter", "tokens": [257, 13075], "temperature": 0.0, "avg_logprob": -0.2731036644477349, "compression_ratio": 1.4143646408839778, "no_speech_prob": 5.9550975493039005e-06}, {"id": 560, "seek": 271630, "start": 2719.34, "end": 2724.86, "text": " Yeah, right except. I have a rule in my classes which is never to use Greek letters, so normally people use alpha", "tokens": [865, 11, 558, 3993, 13, 286, 362, 257, 4978, 294, 452, 5359, 597, 307, 1128, 281, 764, 10281, 7825, 11, 370, 5646, 561, 764, 8961], "temperature": 0.0, "avg_logprob": -0.2731036644477349, "compression_ratio": 1.4143646408839778, "no_speech_prob": 5.9550975493039005e-06}, {"id": 561, "seek": 271630, "start": 2724.86, "end": 2727.6000000000004, "text": " I'm going to use a okay, so", "tokens": [286, 478, 516, 281, 764, 257, 1392, 11, 370], "temperature": 0.0, "avg_logprob": -0.2731036644477349, "compression_ratio": 1.4143646408839778, "no_speech_prob": 5.9550975493039005e-06}, {"id": 562, "seek": 271630, "start": 2729.02, "end": 2735.5600000000004, "text": " So this is some number which you often see something around kind of 1e neg 6", "tokens": [407, 341, 307, 512, 1230, 597, 291, 2049, 536, 746, 926, 733, 295, 502, 68, 2485, 1386], "temperature": 0.0, "avg_logprob": -0.2731036644477349, "compression_ratio": 1.4143646408839778, "no_speech_prob": 5.9550975493039005e-06}, {"id": 563, "seek": 271630, "start": 2735.98, "end": 2737.98, "text": " to 1e neg 4", "tokens": [281, 502, 68, 2485, 1017], "temperature": 0.0, "avg_logprob": -0.2731036644477349, "compression_ratio": 1.4143646408839778, "no_speech_prob": 5.9550975493039005e-06}, {"id": 564, "seek": 271630, "start": 2738.38, "end": 2740.38, "text": " Ish right", "tokens": [42854, 558], "temperature": 0.0, "avg_logprob": -0.2731036644477349, "compression_ratio": 1.4143646408839778, "no_speech_prob": 5.9550975493039005e-06}, {"id": 565, "seek": 271630, "start": 2741.7000000000003, "end": 2743.7000000000003, "text": " Now", "tokens": [823], "temperature": 0.0, "avg_logprob": -0.2731036644477349, "compression_ratio": 1.4143646408839778, "no_speech_prob": 5.9550975493039005e-06}, {"id": 566, "seek": 274370, "start": 2743.7, "end": 2746.18, "text": " We actually don't care about the loss", "tokens": [492, 767, 500, 380, 1127, 466, 264, 4470], "temperature": 0.0, "avg_logprob": -0.24141234020854152, "compression_ratio": 1.8975903614457832, "no_speech_prob": 2.813011178659508e-06}, {"id": 567, "seek": 274370, "start": 2747.98, "end": 2749.06, "text": " When you think about it", "tokens": [1133, 291, 519, 466, 309], "temperature": 0.0, "avg_logprob": -0.24141234020854152, "compression_ratio": 1.8975903614457832, "no_speech_prob": 2.813011178659508e-06}, {"id": 568, "seek": 274370, "start": 2749.06, "end": 2754.18, "text": " We don't actually care about the loss other than like maybe to print it out or we actually care about is the gradient of the loss", "tokens": [492, 500, 380, 767, 1127, 466, 264, 4470, 661, 813, 411, 1310, 281, 4482, 309, 484, 420, 321, 767, 1127, 466, 307, 264, 16235, 295, 264, 4470], "temperature": 0.0, "avg_logprob": -0.24141234020854152, "compression_ratio": 1.8975903614457832, "no_speech_prob": 2.813011178659508e-06}, {"id": 569, "seek": 274370, "start": 2755.18, "end": 2757.18, "text": " Okay, so the gradient of", "tokens": [1033, 11, 370, 264, 16235, 295], "temperature": 0.0, "avg_logprob": -0.24141234020854152, "compression_ratio": 1.8975903614457832, "no_speech_prob": 2.813011178659508e-06}, {"id": 570, "seek": 274370, "start": 2758.8599999999997, "end": 2760.8599999999997, "text": " that", "tokens": [300], "temperature": 0.0, "avg_logprob": -0.24141234020854152, "compression_ratio": 1.8975903614457832, "no_speech_prob": 2.813011178659508e-06}, {"id": 571, "seek": 274370, "start": 2761.54, "end": 2763.54, "text": " right is", "tokens": [558, 307], "temperature": 0.0, "avg_logprob": -0.24141234020854152, "compression_ratio": 1.8975903614457832, "no_speech_prob": 2.813011178659508e-06}, {"id": 572, "seek": 274370, "start": 2765.3399999999997, "end": 2767.22, "text": " that", "tokens": [300], "temperature": 0.0, "avg_logprob": -0.24141234020854152, "compression_ratio": 1.8975903614457832, "no_speech_prob": 2.813011178659508e-06}, {"id": 573, "seek": 274370, "start": 2767.22, "end": 2768.7799999999997, "text": " right so", "tokens": [558, 370], "temperature": 0.0, "avg_logprob": -0.24141234020854152, "compression_ratio": 1.8975903614457832, "no_speech_prob": 2.813011178659508e-06}, {"id": 574, "seek": 274370, "start": 2768.7799999999997, "end": 2772.16, "text": " There are two ways to do this we can actually modify our loss function", "tokens": [821, 366, 732, 2098, 281, 360, 341, 321, 393, 767, 16927, 527, 4470, 2445], "temperature": 0.0, "avg_logprob": -0.24141234020854152, "compression_ratio": 1.8975903614457832, "no_speech_prob": 2.813011178659508e-06}, {"id": 575, "seek": 277216, "start": 2772.16, "end": 2775.0, "text": " to add in this square", "tokens": [281, 909, 294, 341, 3732], "temperature": 0.0, "avg_logprob": -0.30045251285328584, "compression_ratio": 1.5898876404494382, "no_speech_prob": 2.2252759208640782e-06}, {"id": 576, "seek": 277216, "start": 2776.44, "end": 2783.7599999999998, "text": " Penalty or we could modify that thing where we said weights equals weights minus", "tokens": [10571, 304, 874, 420, 321, 727, 16927, 300, 551, 689, 321, 848, 17443, 6915, 17443, 3175], "temperature": 0.0, "avg_logprob": -0.30045251285328584, "compression_ratio": 1.5898876404494382, "no_speech_prob": 2.2252759208640782e-06}, {"id": 577, "seek": 277216, "start": 2784.6, "end": 2787.72, "text": " gradient times learning rate to subtract that as", "tokens": [16235, 1413, 2539, 3314, 281, 16390, 300, 382], "temperature": 0.0, "avg_logprob": -0.30045251285328584, "compression_ratio": 1.5898876404494382, "no_speech_prob": 2.2252759208640782e-06}, {"id": 578, "seek": 277216, "start": 2788.3999999999996, "end": 2789.72, "text": " well", "tokens": [731], "temperature": 0.0, "avg_logprob": -0.30045251285328584, "compression_ratio": 1.5898876404494382, "no_speech_prob": 2.2252759208640782e-06}, {"id": 579, "seek": 277216, "start": 2789.72, "end": 2793.08, "text": " right right so to add that as well and", "tokens": [558, 558, 370, 281, 909, 300, 382, 731, 293], "temperature": 0.0, "avg_logprob": -0.30045251285328584, "compression_ratio": 1.5898876404494382, "no_speech_prob": 2.2252759208640782e-06}, {"id": 580, "seek": 277216, "start": 2794.7999999999997, "end": 2798.3399999999997, "text": " These are roughly these are kind of basically equivalent, but they have different names", "tokens": [1981, 366, 9810, 613, 366, 733, 295, 1936, 10344, 11, 457, 436, 362, 819, 5288], "temperature": 0.0, "avg_logprob": -0.30045251285328584, "compression_ratio": 1.5898876404494382, "no_speech_prob": 2.2252759208640782e-06}, {"id": 581, "seek": 279834, "start": 2798.34, "end": 2803.36, "text": " This is called L2 regularization right this is called weight decay", "tokens": [639, 307, 1219, 441, 17, 3890, 2144, 558, 341, 307, 1219, 3364, 21039], "temperature": 0.0, "avg_logprob": -0.19530601501464845, "compression_ratio": 1.7857142857142858, "no_speech_prob": 6.37553625892906e-07}, {"id": 582, "seek": 279834, "start": 2804.7200000000003, "end": 2806.7200000000003, "text": " So in the neural network literature", "tokens": [407, 294, 264, 18161, 3209, 10394], "temperature": 0.0, "avg_logprob": -0.19530601501464845, "compression_ratio": 1.7857142857142858, "no_speech_prob": 6.37553625892906e-07}, {"id": 583, "seek": 279834, "start": 2807.52, "end": 2809.7200000000003, "text": " You know that version kind of", "tokens": [509, 458, 300, 3037, 733, 295], "temperature": 0.0, "avg_logprob": -0.19530601501464845, "compression_ratio": 1.7857142857142858, "no_speech_prob": 6.37553625892906e-07}, {"id": 584, "seek": 279834, "start": 2810.28, "end": 2816.1600000000003, "text": " With how it was first posed in the neural network literature. Whereas this other version is kind of", "tokens": [2022, 577, 309, 390, 700, 31399, 294, 264, 18161, 3209, 10394, 13, 13813, 341, 661, 3037, 307, 733, 295], "temperature": 0.0, "avg_logprob": -0.19530601501464845, "compression_ratio": 1.7857142857142858, "no_speech_prob": 6.37553625892906e-07}, {"id": 585, "seek": 279834, "start": 2817.0, "end": 2823.08, "text": " How it was posed in the statistics literature, and yeah, you know they're they're equivalent", "tokens": [1012, 309, 390, 31399, 294, 264, 12523, 10394, 11, 293, 1338, 11, 291, 458, 436, 434, 436, 434, 10344], "temperature": 0.0, "avg_logprob": -0.19530601501464845, "compression_ratio": 1.7857142857142858, "no_speech_prob": 6.37553625892906e-07}, {"id": 586, "seek": 282308, "start": 2823.08, "end": 2827.64, "text": " As we talked about in the deep learning class it turns out. They're not exactly a prevalent", "tokens": [1018, 321, 2825, 466, 294, 264, 2452, 2539, 1508, 309, 4523, 484, 13, 814, 434, 406, 2293, 257, 30652], "temperature": 0.0, "avg_logprob": -0.18476883870250774, "compression_ratio": 1.6205673758865249, "no_speech_prob": 3.9897105352793005e-07}, {"id": 587, "seek": 282308, "start": 2828.04, "end": 2833.92, "text": " Because when you have things like momentum and Adam it can behave differently and two weeks ago a researcher", "tokens": [1436, 562, 291, 362, 721, 411, 11244, 293, 7938, 309, 393, 15158, 7614, 293, 732, 3259, 2057, 257, 21751], "temperature": 0.0, "avg_logprob": -0.18476883870250774, "compression_ratio": 1.6205673758865249, "no_speech_prob": 3.9897105352793005e-07}, {"id": 588, "seek": 282308, "start": 2834.68, "end": 2836.68, "text": " figured out a way to actually", "tokens": [8932, 484, 257, 636, 281, 767], "temperature": 0.0, "avg_logprob": -0.18476883870250774, "compression_ratio": 1.6205673758865249, "no_speech_prob": 3.9897105352793005e-07}, {"id": 589, "seek": 282308, "start": 2837.2799999999997, "end": 2844.44, "text": " Do proper weight decay in modern optimizers and one of our fast AI students just implemented that in the fast AI library", "tokens": [1144, 2296, 3364, 21039, 294, 4363, 5028, 22525, 293, 472, 295, 527, 2370, 7318, 1731, 445, 12270, 300, 294, 264, 2370, 7318, 6405], "temperature": 0.0, "avg_logprob": -0.18476883870250774, "compression_ratio": 1.6205673758865249, "no_speech_prob": 3.9897105352793005e-07}, {"id": 590, "seek": 282308, "start": 2844.44, "end": 2846.44, "text": " So fast AI is now the first", "tokens": [407, 2370, 7318, 307, 586, 264, 700], "temperature": 0.0, "avg_logprob": -0.18476883870250774, "compression_ratio": 1.6205673758865249, "no_speech_prob": 3.9897105352793005e-07}, {"id": 591, "seek": 282308, "start": 2846.96, "end": 2848.96, "text": " Library to actually support this properly", "tokens": [12806, 281, 767, 1406, 341, 6108], "temperature": 0.0, "avg_logprob": -0.18476883870250774, "compression_ratio": 1.6205673758865249, "no_speech_prob": 3.9897105352793005e-07}, {"id": 592, "seek": 284896, "start": 2848.96, "end": 2855.08, "text": " so anyway, so for now, let's do the the version which", "tokens": [370, 4033, 11, 370, 337, 586, 11, 718, 311, 360, 264, 264, 3037, 597], "temperature": 0.0, "avg_logprob": -0.2358631134033203, "compression_ratio": 1.6218487394957983, "no_speech_prob": 2.6016077754320577e-06}, {"id": 593, "seek": 284896, "start": 2857.32, "end": 2863.98, "text": " Pie torch calls weight decay, but actually it turns out based on this paper two weeks ago is actually L2 regularization", "tokens": [22914, 27822, 5498, 3364, 21039, 11, 457, 767, 309, 4523, 484, 2361, 322, 341, 3035, 732, 3259, 2057, 307, 767, 441, 17, 3890, 2144], "temperature": 0.0, "avg_logprob": -0.2358631134033203, "compression_ratio": 1.6218487394957983, "no_speech_prob": 2.6016077754320577e-06}, {"id": 594, "seek": 284896, "start": 2863.98, "end": 2868.8, "text": " It's not quite correct, but it's close enough so here. We can say weight decay is 1e neg 3", "tokens": [467, 311, 406, 1596, 3006, 11, 457, 309, 311, 1998, 1547, 370, 510, 13, 492, 393, 584, 3364, 21039, 307, 502, 68, 2485, 805], "temperature": 0.0, "avg_logprob": -0.2358631134033203, "compression_ratio": 1.6218487394957983, "no_speech_prob": 2.6016077754320577e-06}, {"id": 595, "seek": 284896, "start": 2868.8, "end": 2876.76, "text": " So this is going to set our constant our penalty multiplier a to 1e neg 3 and it's going to add that to the loss function", "tokens": [407, 341, 307, 516, 281, 992, 527, 5754, 527, 16263, 44106, 257, 281, 502, 68, 2485, 805, 293, 309, 311, 516, 281, 909, 300, 281, 264, 4470, 2445], "temperature": 0.0, "avg_logprob": -0.2358631134033203, "compression_ratio": 1.6218487394957983, "no_speech_prob": 2.6016077754320577e-06}, {"id": 596, "seek": 287676, "start": 2876.76, "end": 2881.0400000000004, "text": " Okay, and so let's make a copy of these cells", "tokens": [1033, 11, 293, 370, 718, 311, 652, 257, 5055, 295, 613, 5438], "temperature": 0.0, "avg_logprob": -0.20186097886827256, "compression_ratio": 1.608695652173913, "no_speech_prob": 8.664590495754965e-06}, {"id": 597, "seek": 287676, "start": 2881.96, "end": 2884.6800000000003, "text": " Just so we can compare hope this actually works", "tokens": [1449, 370, 321, 393, 6794, 1454, 341, 767, 1985], "temperature": 0.0, "avg_logprob": -0.20186097886827256, "compression_ratio": 1.608695652173913, "no_speech_prob": 8.664590495754965e-06}, {"id": 598, "seek": 287676, "start": 2886.2000000000003, "end": 2889.88, "text": " Okay, and we'll set this running okay, this is now optimizing", "tokens": [1033, 11, 293, 321, 603, 992, 341, 2614, 1392, 11, 341, 307, 586, 40425], "temperature": 0.0, "avg_logprob": -0.20186097886827256, "compression_ratio": 1.608695652173913, "no_speech_prob": 8.664590495754965e-06}, {"id": 599, "seek": 287676, "start": 2891.48, "end": 2893.48, "text": " Well except", "tokens": [1042, 3993], "temperature": 0.0, "avg_logprob": -0.20186097886827256, "compression_ratio": 1.608695652173913, "no_speech_prob": 8.664590495754965e-06}, {"id": 600, "seek": 287676, "start": 2893.5200000000004, "end": 2897.8, "text": " If you actually so I've made a mistake here, which is I didn't rerun", "tokens": [759, 291, 767, 370, 286, 600, 1027, 257, 6146, 510, 11, 597, 307, 286, 994, 380, 43819, 409], "temperature": 0.0, "avg_logprob": -0.20186097886827256, "compression_ratio": 1.608695652173913, "no_speech_prob": 8.664590495754965e-06}, {"id": 601, "seek": 287676, "start": 2898.4, "end": 2903.32, "text": " This cell this is an important thing to kind of remember since I didn't run this rerun this cell", "tokens": [639, 2815, 341, 307, 364, 1021, 551, 281, 733, 295, 1604, 1670, 286, 994, 380, 1190, 341, 43819, 409, 341, 2815], "temperature": 0.0, "avg_logprob": -0.20186097886827256, "compression_ratio": 1.608695652173913, "no_speech_prob": 8.664590495754965e-06}, {"id": 602, "seek": 290332, "start": 2903.32, "end": 2911.4, "text": " Here when it created the optimizer and said net dot parameters it started with the parameters that I had already trained right?", "tokens": [1692, 562, 309, 2942, 264, 5028, 6545, 293, 848, 2533, 5893, 9834, 309, 1409, 365, 264, 9834, 300, 286, 632, 1217, 8895, 558, 30], "temperature": 0.0, "avg_logprob": -0.16146562099456788, "compression_ratio": 1.6734693877551021, "no_speech_prob": 2.090452198899584e-06}, {"id": 603, "seek": 290332, "start": 2911.4, "end": 2913.4, "text": " So I actually hadn't recreated my network", "tokens": [407, 286, 767, 8782, 380, 850, 26559, 452, 3209], "temperature": 0.0, "avg_logprob": -0.16146562099456788, "compression_ratio": 1.6734693877551021, "no_speech_prob": 2.090452198899584e-06}, {"id": 604, "seek": 290332, "start": 2913.84, "end": 2918.88, "text": " Okay, so I actually need to go back and rerun this cell first to recreate the network", "tokens": [1033, 11, 370, 286, 767, 643, 281, 352, 646, 293, 43819, 409, 341, 2815, 700, 281, 25833, 264, 3209], "temperature": 0.0, "avg_logprob": -0.16146562099456788, "compression_ratio": 1.6734693877551021, "no_speech_prob": 2.090452198899584e-06}, {"id": 605, "seek": 290332, "start": 2919.6400000000003, "end": 2921.6400000000003, "text": " Then go through and run this", "tokens": [1396, 352, 807, 293, 1190, 341], "temperature": 0.0, "avg_logprob": -0.16146562099456788, "compression_ratio": 1.6734693877551021, "no_speech_prob": 2.090452198899584e-06}, {"id": 606, "seek": 292164, "start": 2921.64, "end": 2929.52, "text": " Okay, there we go, so let's see what happens", "tokens": [1033, 11, 456, 321, 352, 11, 370, 718, 311, 536, 437, 2314], "temperature": 0.0, "avg_logprob": -0.16223146147647147, "compression_ratio": 1.4876543209876543, "no_speech_prob": 1.3287721003507613e-06}, {"id": 607, "seek": 292164, "start": 2934.7599999999998, "end": 2938.3399999999997, "text": " So you might know some notice something kind of kind of counterintuitive here", "tokens": [407, 291, 1062, 458, 512, 3449, 746, 733, 295, 733, 295, 5682, 686, 48314, 510], "temperature": 0.0, "avg_logprob": -0.16223146147647147, "compression_ratio": 1.4876543209876543, "no_speech_prob": 1.3287721003507613e-06}, {"id": 608, "seek": 292164, "start": 2939.8399999999997, "end": 2941.8399999999997, "text": " Which is that?", "tokens": [3013, 307, 300, 30], "temperature": 0.0, "avg_logprob": -0.16223146147647147, "compression_ratio": 1.4876543209876543, "no_speech_prob": 1.3287721003507613e-06}, {"id": 609, "seek": 294184, "start": 2941.84, "end": 2951.2400000000002, "text": " That's our training error right now. You would expect our training error with regularization to be worse that", "tokens": [663, 311, 527, 3097, 6713, 558, 586, 13, 509, 576, 2066, 527, 3097, 6713, 365, 3890, 2144, 281, 312, 5324, 300], "temperature": 0.0, "avg_logprob": -0.2530491211835076, "compression_ratio": 1.5494505494505495, "no_speech_prob": 6.083583912186441e-07}, {"id": 610, "seek": 294184, "start": 2951.52, "end": 2954.1600000000003, "text": " Makes sense right because we're like we're penalizing", "tokens": [25245, 2020, 558, 570, 321, 434, 411, 321, 434, 13661, 3319], "temperature": 0.0, "avg_logprob": -0.2530491211835076, "compression_ratio": 1.5494505494505495, "no_speech_prob": 6.083583912186441e-07}, {"id": 611, "seek": 294184, "start": 2955.32, "end": 2957.32, "text": " parameters that", "tokens": [9834, 300], "temperature": 0.0, "avg_logprob": -0.2530491211835076, "compression_ratio": 1.5494505494505495, "no_speech_prob": 6.083583912186441e-07}, {"id": 612, "seek": 294184, "start": 2957.36, "end": 2959.7200000000003, "text": " specifically can make it better and yet", "tokens": [4682, 393, 652, 309, 1101, 293, 1939], "temperature": 0.0, "avg_logprob": -0.2530491211835076, "compression_ratio": 1.5494505494505495, "no_speech_prob": 6.083583912186441e-07}, {"id": 613, "seek": 294184, "start": 2961.36, "end": 2963.48, "text": " Actually it started out better not worse", "tokens": [5135, 309, 1409, 484, 1101, 406, 5324], "temperature": 0.0, "avg_logprob": -0.2530491211835076, "compression_ratio": 1.5494505494505495, "no_speech_prob": 6.083583912186441e-07}, {"id": 614, "seek": 294184, "start": 2964.92, "end": 2966.92, "text": " So why could that be?", "tokens": [407, 983, 727, 300, 312, 30], "temperature": 0.0, "avg_logprob": -0.2530491211835076, "compression_ratio": 1.5494505494505495, "no_speech_prob": 6.083583912186441e-07}, {"id": 615, "seek": 296692, "start": 2966.92, "end": 2972.76, "text": " So the reason that can happen is that if you have a function", "tokens": [407, 264, 1778, 300, 393, 1051, 307, 300, 498, 291, 362, 257, 2445], "temperature": 0.0, "avg_logprob": -0.2014032496084081, "compression_ratio": 1.868421052631579, "no_speech_prob": 2.873683229154267e-07}, {"id": 616, "seek": 296692, "start": 2973.56, "end": 2975.48, "text": " That looks like that", "tokens": [663, 1542, 411, 300], "temperature": 0.0, "avg_logprob": -0.2014032496084081, "compression_ratio": 1.868421052631579, "no_speech_prob": 2.873683229154267e-07}, {"id": 617, "seek": 296692, "start": 2975.48, "end": 2978.88, "text": " Right it takes potentially a really long time to train", "tokens": [1779, 309, 2516, 7263, 257, 534, 938, 565, 281, 3847], "temperature": 0.0, "avg_logprob": -0.2014032496084081, "compression_ratio": 1.868421052631579, "no_speech_prob": 2.873683229154267e-07}, {"id": 618, "seek": 296692, "start": 2979.6, "end": 2982.28, "text": " or else if you have a function that kind of looks more like", "tokens": [420, 1646, 498, 291, 362, 257, 2445, 300, 733, 295, 1542, 544, 411], "temperature": 0.0, "avg_logprob": -0.2014032496084081, "compression_ratio": 1.868421052631579, "no_speech_prob": 2.873683229154267e-07}, {"id": 619, "seek": 296692, "start": 2983.64, "end": 2985.7200000000003, "text": " That it's going to train a lot more quickly", "tokens": [663, 309, 311, 516, 281, 3847, 257, 688, 544, 2661], "temperature": 0.0, "avg_logprob": -0.2014032496084081, "compression_ratio": 1.868421052631579, "no_speech_prob": 2.873683229154267e-07}, {"id": 620, "seek": 296692, "start": 2985.7200000000003, "end": 2990.8, "text": " And there are certain things that you can do which sometimes just like can take a function", "tokens": [400, 456, 366, 1629, 721, 300, 291, 393, 360, 597, 2171, 445, 411, 393, 747, 257, 2445], "temperature": 0.0, "avg_logprob": -0.2014032496084081, "compression_ratio": 1.868421052631579, "no_speech_prob": 2.873683229154267e-07}, {"id": 621, "seek": 296692, "start": 2990.8, "end": 2996.04, "text": " That's kind of horrible and make it less horrible and it's sometimes weight decay can actually", "tokens": [663, 311, 733, 295, 9263, 293, 652, 309, 1570, 9263, 293, 309, 311, 2171, 3364, 21039, 393, 767], "temperature": 0.0, "avg_logprob": -0.2014032496084081, "compression_ratio": 1.868421052631579, "no_speech_prob": 2.873683229154267e-07}, {"id": 622, "seek": 299604, "start": 2996.04, "end": 3001.04, "text": " Make your functions a little more nicely behaved, and that's actually happened here", "tokens": [4387, 428, 6828, 257, 707, 544, 9594, 48249, 11, 293, 300, 311, 767, 2011, 510], "temperature": 0.0, "avg_logprob": -0.1788049103111349, "compression_ratio": 1.6330645161290323, "no_speech_prob": 1.1015913514711428e-06}, {"id": 623, "seek": 299604, "start": 3001.04, "end": 3007.08, "text": " So like I just mentioned that to say like don't let that confuse you right like weight decay really does", "tokens": [407, 411, 286, 445, 2835, 300, 281, 584, 411, 500, 380, 718, 300, 28584, 291, 558, 411, 3364, 21039, 534, 775], "temperature": 0.0, "avg_logprob": -0.1788049103111349, "compression_ratio": 1.6330645161290323, "no_speech_prob": 1.1015913514711428e-06}, {"id": 624, "seek": 299604, "start": 3007.2, "end": 3010.16, "text": " Penalize the training set and look so strictly speaking", "tokens": [10571, 304, 1125, 264, 3097, 992, 293, 574, 370, 20792, 4124], "temperature": 0.0, "avg_logprob": -0.1788049103111349, "compression_ratio": 1.6330645161290323, "no_speech_prob": 1.1015913514711428e-06}, {"id": 625, "seek": 299604, "start": 3011.68, "end": 3018.8, "text": " The final number we get to for the training set shouldn't end up being better, but it can train sometimes more quickly right", "tokens": [440, 2572, 1230, 321, 483, 281, 337, 264, 3097, 992, 4659, 380, 917, 493, 885, 1101, 11, 457, 309, 393, 3847, 2171, 544, 2661, 558], "temperature": 0.0, "avg_logprob": -0.1788049103111349, "compression_ratio": 1.6330645161290323, "no_speech_prob": 1.1015913514711428e-06}, {"id": 626, "seek": 301880, "start": 3018.8, "end": 3023.5600000000004, "text": " Yes, can you pass it attention I", "tokens": [1079, 11, 393, 291, 1320, 309, 3202, 286], "temperature": 0.0, "avg_logprob": -0.36682140350341796, "compression_ratio": 1.5041322314049588, "no_speech_prob": 2.6015943603852065e-06}, {"id": 627, "seek": 301880, "start": 3026.32, "end": 3032.86, "text": " Don't get it okay why making it faster like the time matters like the training time matters", "tokens": [1468, 380, 483, 309, 1392, 983, 1455, 309, 4663, 411, 264, 565, 7001, 411, 264, 3097, 565, 7001], "temperature": 0.0, "avg_logprob": -0.36682140350341796, "compression_ratio": 1.5041322314049588, "no_speech_prob": 2.6015943603852065e-06}, {"id": 628, "seek": 301880, "start": 3032.92, "end": 3038.0800000000004, "text": " No, it's this is after one epoch right so after one epoch", "tokens": [883, 11, 309, 311, 341, 307, 934, 472, 30992, 339, 558, 370, 934, 472, 30992, 339], "temperature": 0.0, "avg_logprob": -0.36682140350341796, "compression_ratio": 1.5041322314049588, "no_speech_prob": 2.6015943603852065e-06}, {"id": 629, "seek": 303808, "start": 3038.08, "end": 3042.08, "text": " I", "tokens": [286], "temperature": 0.0, "avg_logprob": -0.24994829618013822, "compression_ratio": 1.5279503105590062, "no_speech_prob": 1.2098599881937844e-06}, {"id": 630, "seek": 303808, "start": 3045.72, "end": 3051.08, "text": " Congratulations for saying I don't get it. That's like the best thing anybody can say you know so helpful", "tokens": [9694, 337, 1566, 286, 500, 380, 483, 309, 13, 663, 311, 411, 264, 1151, 551, 4472, 393, 584, 291, 458, 370, 4961], "temperature": 0.0, "avg_logprob": -0.24994829618013822, "compression_ratio": 1.5279503105590062, "no_speech_prob": 1.2098599881937844e-06}, {"id": 631, "seek": 303808, "start": 3053.16, "end": 3055.16, "text": " This here was our training", "tokens": [639, 510, 390, 527, 3097], "temperature": 0.0, "avg_logprob": -0.24994829618013822, "compression_ratio": 1.5279503105590062, "no_speech_prob": 1.2098599881937844e-06}, {"id": 632, "seek": 303808, "start": 3055.84, "end": 3057.04, "text": " without", "tokens": [1553], "temperature": 0.0, "avg_logprob": -0.24994829618013822, "compression_ratio": 1.5279503105590062, "no_speech_prob": 1.2098599881937844e-06}, {"id": 633, "seek": 303808, "start": 3057.04, "end": 3058.48, "text": " weight decay", "tokens": [3364, 21039], "temperature": 0.0, "avg_logprob": -0.24994829618013822, "compression_ratio": 1.5279503105590062, "no_speech_prob": 1.2098599881937844e-06}, {"id": 634, "seek": 303808, "start": 3058.48, "end": 3065.88, "text": " Okay, and this here is our training with weight decay okay, so this is not related to time", "tokens": [1033, 11, 293, 341, 510, 307, 527, 3097, 365, 3364, 21039, 1392, 11, 370, 341, 307, 406, 4077, 281, 565], "temperature": 0.0, "avg_logprob": -0.24994829618013822, "compression_ratio": 1.5279503105590062, "no_speech_prob": 1.2098599881937844e-06}, {"id": 635, "seek": 306588, "start": 3065.88, "end": 3068.48, "text": " This is related to just an epoch", "tokens": [639, 307, 4077, 281, 445, 364, 30992, 339], "temperature": 0.0, "avg_logprob": -0.17503250604388357, "compression_ratio": 1.6105769230769231, "no_speech_prob": 1.6797278021840611e-06}, {"id": 636, "seek": 306588, "start": 3069.52, "end": 3071.52, "text": " Right after one epoch", "tokens": [1779, 934, 472, 30992, 339], "temperature": 0.0, "avg_logprob": -0.17503250604388357, "compression_ratio": 1.6105769230769231, "no_speech_prob": 1.6797278021840611e-06}, {"id": 637, "seek": 306588, "start": 3071.84, "end": 3078.76, "text": " My claim was that you would expect the training set all other things being equal to have a", "tokens": [1222, 3932, 390, 300, 291, 576, 2066, 264, 3097, 992, 439, 661, 721, 885, 2681, 281, 362, 257], "temperature": 0.0, "avg_logprob": -0.17503250604388357, "compression_ratio": 1.6105769230769231, "no_speech_prob": 1.6797278021840611e-06}, {"id": 638, "seek": 306588, "start": 3079.96, "end": 3084.6, "text": " worse loss with weight decay because we're penalizing it", "tokens": [5324, 4470, 365, 3364, 21039, 570, 321, 434, 13661, 3319, 309], "temperature": 0.0, "avg_logprob": -0.17503250604388357, "compression_ratio": 1.6105769230769231, "no_speech_prob": 1.6797278021840611e-06}, {"id": 639, "seek": 306588, "start": 3084.84, "end": 3091.7200000000003, "text": " You know this has no penalty this has a penalty so the thing with a penalty should be worse, and I'm saying oh", "tokens": [509, 458, 341, 575, 572, 16263, 341, 575, 257, 16263, 370, 264, 551, 365, 257, 16263, 820, 312, 5324, 11, 293, 286, 478, 1566, 1954], "temperature": 0.0, "avg_logprob": -0.17503250604388357, "compression_ratio": 1.6105769230769231, "no_speech_prob": 1.6797278021840611e-06}, {"id": 640, "seek": 306588, "start": 3092.04, "end": 3094.04, "text": " It's not that's weird", "tokens": [467, 311, 406, 300, 311, 3657], "temperature": 0.0, "avg_logprob": -0.17503250604388357, "compression_ratio": 1.6105769230769231, "no_speech_prob": 1.6797278021840611e-06}, {"id": 641, "seek": 309404, "start": 3094.04, "end": 3098.0, "text": " right and so the reason it's not is", "tokens": [558, 293, 370, 264, 1778, 309, 311, 406, 307], "temperature": 0.0, "avg_logprob": -0.15525620109156557, "compression_ratio": 1.9013452914798206, "no_speech_prob": 5.62638888368383e-07}, {"id": 642, "seek": 309404, "start": 3098.96, "end": 3104.2799999999997, "text": " Because in a single epoch it matters a lot as to whether you're trying to optimize something", "tokens": [1436, 294, 257, 2167, 30992, 339, 309, 7001, 257, 688, 382, 281, 1968, 291, 434, 1382, 281, 19719, 746], "temperature": 0.0, "avg_logprob": -0.15525620109156557, "compression_ratio": 1.9013452914798206, "no_speech_prob": 5.62638888368383e-07}, {"id": 643, "seek": 309404, "start": 3104.2799999999997, "end": 3108.56, "text": " That's very bumpy or whether you're trying to optimize something. That's kind of nice and smooth", "tokens": [663, 311, 588, 49400, 420, 1968, 291, 434, 1382, 281, 19719, 746, 13, 663, 311, 733, 295, 1481, 293, 5508], "temperature": 0.0, "avg_logprob": -0.15525620109156557, "compression_ratio": 1.9013452914798206, "no_speech_prob": 5.62638888368383e-07}, {"id": 644, "seek": 309404, "start": 3110.36, "end": 3116.2, "text": " If you're trying to optimize something that's really bumpy like imagine in some high dimensional space right", "tokens": [759, 291, 434, 1382, 281, 19719, 746, 300, 311, 534, 49400, 411, 3811, 294, 512, 1090, 18795, 1901, 558], "temperature": 0.0, "avg_logprob": -0.15525620109156557, "compression_ratio": 1.9013452914798206, "no_speech_prob": 5.62638888368383e-07}, {"id": 645, "seek": 309404, "start": 3117.04, "end": 3121.92, "text": " You end up kind of rolling around through all these different tubes and tunnels and stuff", "tokens": [509, 917, 493, 733, 295, 9439, 926, 807, 439, 613, 819, 21458, 293, 30804, 293, 1507], "temperature": 0.0, "avg_logprob": -0.15525620109156557, "compression_ratio": 1.9013452914798206, "no_speech_prob": 5.62638888368383e-07}, {"id": 646, "seek": 312192, "start": 3121.92, "end": 3124.8, "text": " You know or else if it's just smooth you just go boom", "tokens": [509, 458, 420, 1646, 498, 309, 311, 445, 5508, 291, 445, 352, 9351], "temperature": 0.0, "avg_logprob": -0.20970142493813726, "compression_ratio": 1.7326007326007327, "no_speech_prob": 5.338116807251936e-06}, {"id": 647, "seek": 312192, "start": 3125.28, "end": 3129.96, "text": " Right, I meant it's like imagine a marble rolling down a hill where one of them you've got like", "tokens": [1779, 11, 286, 4140, 309, 311, 411, 3811, 257, 26844, 9439, 760, 257, 10997, 689, 472, 295, 552, 291, 600, 658, 411], "temperature": 0.0, "avg_logprob": -0.20970142493813726, "compression_ratio": 1.7326007326007327, "no_speech_prob": 5.338116807251936e-06}, {"id": 648, "seek": 312192, "start": 3131.08, "end": 3135.2400000000002, "text": " It's a good Lombard Street in San Francisco. It's like backwards forwards backwards forwards", "tokens": [467, 311, 257, 665, 441, 3548, 515, 7638, 294, 5271, 12279, 13, 467, 311, 411, 12204, 30126, 12204, 30126], "temperature": 0.0, "avg_logprob": -0.20970142493813726, "compression_ratio": 1.7326007326007327, "no_speech_prob": 5.338116807251936e-06}, {"id": 649, "seek": 312192, "start": 3135.2400000000002, "end": 3137.44, "text": " It takes a long time to drive down the road right", "tokens": [467, 2516, 257, 938, 565, 281, 3332, 760, 264, 3060, 558], "temperature": 0.0, "avg_logprob": -0.20970142493813726, "compression_ratio": 1.7326007326007327, "no_speech_prob": 5.338116807251936e-06}, {"id": 650, "seek": 312192, "start": 3138.0, "end": 3143.52, "text": " Where else you know if you kind of took a motorbike and just went straight over the top you just make boom right so", "tokens": [2305, 1646, 291, 458, 498, 291, 733, 295, 1890, 257, 5932, 30283, 293, 445, 1437, 2997, 670, 264, 1192, 291, 445, 652, 9351, 558, 370], "temperature": 0.0, "avg_logprob": -0.20970142493813726, "compression_ratio": 1.7326007326007327, "no_speech_prob": 5.338116807251936e-06}, {"id": 651, "seek": 312192, "start": 3144.7200000000003, "end": 3148.58, "text": " So whether it's a kind of the shape of the loss function surface", "tokens": [407, 1968, 309, 311, 257, 733, 295, 264, 3909, 295, 264, 4470, 2445, 3753], "temperature": 0.0, "avg_logprob": -0.20970142493813726, "compression_ratio": 1.7326007326007327, "no_speech_prob": 5.338116807251936e-06}, {"id": 652, "seek": 314858, "start": 3148.58, "end": 3154.48, "text": " You know impacts or kind of defines how easy it is to optimize and therefore how?", "tokens": [509, 458, 11606, 420, 733, 295, 23122, 577, 1858, 309, 307, 281, 19719, 293, 4412, 577, 30], "temperature": 0.0, "avg_logprob": -0.2058721326061131, "compression_ratio": 1.5903614457831325, "no_speech_prob": 5.17388525622664e-06}, {"id": 653, "seek": 314858, "start": 3155.1, "end": 3163.12, "text": " Far can it get in a single epoch and based on these results it would appear that weight decay here has made it this function", "tokens": [9067, 393, 309, 483, 294, 257, 2167, 30992, 339, 293, 2361, 322, 613, 3542, 309, 576, 4204, 300, 3364, 21039, 510, 575, 1027, 309, 341, 2445], "temperature": 0.0, "avg_logprob": -0.2058721326061131, "compression_ratio": 1.5903614457831325, "no_speech_prob": 5.17388525622664e-06}, {"id": 654, "seek": 314858, "start": 3163.12, "end": 3165.12, "text": " easier to optimize", "tokens": [3571, 281, 19719], "temperature": 0.0, "avg_logprob": -0.2058721326061131, "compression_ratio": 1.5903614457831325, "no_speech_prob": 5.17388525622664e-06}, {"id": 655, "seek": 314858, "start": 3165.14, "end": 3167.2999999999997, "text": " so just to make sure it's", "tokens": [370, 445, 281, 652, 988, 309, 311], "temperature": 0.0, "avg_logprob": -0.2058721326061131, "compression_ratio": 1.5903614457831325, "no_speech_prob": 5.17388525622664e-06}, {"id": 656, "seek": 314858, "start": 3168.14, "end": 3174.1, "text": " The penalizing is making the optimizer more than likely to reach the global minimum", "tokens": [440, 13661, 3319, 307, 1455, 264, 5028, 6545, 544, 813, 3700, 281, 2524, 264, 4338, 7285], "temperature": 0.0, "avg_logprob": -0.2058721326061131, "compression_ratio": 1.5903614457831325, "no_speech_prob": 5.17388525622664e-06}, {"id": 657, "seek": 317410, "start": 3174.1, "end": 3178.14, "text": " Rather than no I wouldn't say that my claim actually is that at the end", "tokens": [16571, 813, 572, 286, 2759, 380, 584, 300, 452, 3932, 767, 307, 300, 412, 264, 917], "temperature": 0.0, "avg_logprob": -0.16043056980256112, "compression_ratio": 1.7727272727272727, "no_speech_prob": 3.5559673960960936e-06}, {"id": 658, "seek": 317410, "start": 3178.98, "end": 3183.24, "text": " It's probably going to be less good on the training set and indeed this does look to be the case at the end", "tokens": [467, 311, 1391, 516, 281, 312, 1570, 665, 322, 264, 3097, 992, 293, 6451, 341, 775, 574, 281, 312, 264, 1389, 412, 264, 917], "temperature": 0.0, "avg_logprob": -0.16043056980256112, "compression_ratio": 1.7727272727272727, "no_speech_prob": 3.5559673960960936e-06}, {"id": 659, "seek": 317410, "start": 3184.2599999999998, "end": 3186.2599999999998, "text": " after five epochs", "tokens": [934, 1732, 30992, 28346], "temperature": 0.0, "avg_logprob": -0.16043056980256112, "compression_ratio": 1.7727272727272727, "no_speech_prob": 3.5559673960960936e-06}, {"id": 660, "seek": 317410, "start": 3186.5, "end": 3187.9, "text": " our", "tokens": [527], "temperature": 0.0, "avg_logprob": -0.16043056980256112, "compression_ratio": 1.7727272727272727, "no_speech_prob": 3.5559673960960936e-06}, {"id": 661, "seek": 317410, "start": 3187.9, "end": 3192.8199999999997, "text": " Training set is now worse with weight decay now. That's what I would expect right", "tokens": [20620, 992, 307, 586, 5324, 365, 3364, 21039, 586, 13, 663, 311, 437, 286, 576, 2066, 558], "temperature": 0.0, "avg_logprob": -0.16043056980256112, "compression_ratio": 1.7727272727272727, "no_speech_prob": 3.5559673960960936e-06}, {"id": 662, "seek": 317410, "start": 3192.8199999999997, "end": 3197.3399999999997, "text": " I would expect like if you actually find like I never use the term global optimum because", "tokens": [286, 576, 2066, 411, 498, 291, 767, 915, 411, 286, 1128, 764, 264, 1433, 4338, 39326, 570], "temperature": 0.0, "avg_logprob": -0.16043056980256112, "compression_ratio": 1.7727272727272727, "no_speech_prob": 3.5559673960960936e-06}, {"id": 663, "seek": 317410, "start": 3198.46, "end": 3203.9, "text": " It's just not something we have any guarantees about we don't really care about we just care like where do we get to after a certain?", "tokens": [467, 311, 445, 406, 746, 321, 362, 604, 32567, 466, 321, 500, 380, 534, 1127, 466, 321, 445, 1127, 411, 689, 360, 321, 483, 281, 934, 257, 1629, 30], "temperature": 0.0, "avg_logprob": -0.16043056980256112, "compression_ratio": 1.7727272727272727, "no_speech_prob": 3.5559673960960936e-06}, {"id": 664, "seek": 320390, "start": 3203.9, "end": 3205.7400000000002, "text": " number of epochs", "tokens": [1230, 295, 30992, 28346], "temperature": 0.0, "avg_logprob": -0.2071048777590516, "compression_ratio": 1.768181818181818, "no_speech_prob": 7.224429623420292e-07}, {"id": 665, "seek": 320390, "start": 3205.7400000000002, "end": 3207.44, "text": " We hope that we found somewhere", "tokens": [492, 1454, 300, 321, 1352, 4079], "temperature": 0.0, "avg_logprob": -0.2071048777590516, "compression_ratio": 1.768181818181818, "no_speech_prob": 7.224429623420292e-07}, {"id": 666, "seek": 320390, "start": 3207.44, "end": 3214.1800000000003, "text": " That's like a good solution and so by the time we get to like a good solution the training set with weight decay the loss is worse", "tokens": [663, 311, 411, 257, 665, 3827, 293, 370, 538, 264, 565, 321, 483, 281, 411, 257, 665, 3827, 264, 3097, 992, 365, 3364, 21039, 264, 4470, 307, 5324], "temperature": 0.0, "avg_logprob": -0.2071048777590516, "compression_ratio": 1.768181818181818, "no_speech_prob": 7.224429623420292e-07}, {"id": 667, "seek": 320390, "start": 3215.26, "end": 3221.88, "text": " Because it's very right but on the validation set the loss is better", "tokens": [1436, 309, 311, 588, 558, 457, 322, 264, 24071, 992, 264, 4470, 307, 1101], "temperature": 0.0, "avg_logprob": -0.2071048777590516, "compression_ratio": 1.768181818181818, "no_speech_prob": 7.224429623420292e-07}, {"id": 668, "seek": 320390, "start": 3222.46, "end": 3228.82, "text": " Right because we penalized the training set in order to kind of try and create something that generalizes better", "tokens": [1779, 570, 321, 13661, 1602, 264, 3097, 992, 294, 1668, 281, 733, 295, 853, 293, 1884, 746, 300, 2674, 5660, 1101], "temperature": 0.0, "avg_logprob": -0.2071048777590516, "compression_ratio": 1.768181818181818, "no_speech_prob": 7.224429623420292e-07}, {"id": 669, "seek": 320390, "start": 3228.82, "end": 3229.98, "text": " So we've got more parameter", "tokens": [407, 321, 600, 658, 544, 13075], "temperature": 0.0, "avg_logprob": -0.2071048777590516, "compression_ratio": 1.768181818181818, "no_speech_prob": 7.224429623420292e-07}, {"id": 670, "seek": 322998, "start": 3229.98, "end": 3234.08, "text": " You know the parameters that are kind of pointless are now zero and it generalizes better", "tokens": [509, 458, 264, 9834, 300, 366, 733, 295, 32824, 366, 586, 4018, 293, 309, 2674, 5660, 1101], "temperature": 0.0, "avg_logprob": -0.28945135768455793, "compression_ratio": 1.537313432835821, "no_speech_prob": 1.308168975810986e-06}, {"id": 671, "seek": 322998, "start": 3234.34, "end": 3240.14, "text": " Right so so always saying is that it just got to a good point", "tokens": [1779, 370, 370, 1009, 1566, 307, 300, 309, 445, 658, 281, 257, 665, 935], "temperature": 0.0, "avg_logprob": -0.28945135768455793, "compression_ratio": 1.537313432835821, "no_speech_prob": 1.308168975810986e-06}, {"id": 672, "seek": 322998, "start": 3240.82, "end": 3243.7, "text": " After one epoch is really always saying", "tokens": [2381, 472, 30992, 339, 307, 534, 1009, 1566], "temperature": 0.0, "avg_logprob": -0.28945135768455793, "compression_ratio": 1.537313432835821, "no_speech_prob": 1.308168975810986e-06}, {"id": 673, "seek": 322998, "start": 3245.06, "end": 3247.06, "text": " So is it always true?", "tokens": [407, 307, 309, 1009, 2074, 30], "temperature": 0.0, "avg_logprob": -0.28945135768455793, "compression_ratio": 1.537313432835821, "no_speech_prob": 1.308168975810986e-06}, {"id": 674, "seek": 322998, "start": 3247.7400000000002, "end": 3249.7400000000002, "text": " No, no", "tokens": [883, 11, 572], "temperature": 0.0, "avg_logprob": -0.28945135768455793, "compression_ratio": 1.537313432835821, "no_speech_prob": 1.308168975810986e-06}, {"id": 675, "seek": 322998, "start": 3250.02, "end": 3254.06, "text": " If you're bit by it you mean does weight decay always make the function surface smoother", "tokens": [759, 291, 434, 857, 538, 309, 291, 914, 775, 3364, 21039, 1009, 652, 264, 2445, 3753, 28640], "temperature": 0.0, "avg_logprob": -0.28945135768455793, "compression_ratio": 1.537313432835821, "no_speech_prob": 1.308168975810986e-06}, {"id": 676, "seek": 325406, "start": 3254.06, "end": 3261.2599999999998, "text": " No, it's not always true, but it's like it's worth remembering that", "tokens": [883, 11, 309, 311, 406, 1009, 2074, 11, 457, 309, 311, 411, 309, 311, 3163, 20719, 300], "temperature": 0.0, "avg_logprob": -0.22418594360351562, "compression_ratio": 1.6291079812206573, "no_speech_prob": 1.6373745381770277e-07}, {"id": 677, "seek": 325406, "start": 3262.46, "end": 3268.5, "text": " If you're having trouble training a function adding a little bit of weight decay may may help", "tokens": [759, 291, 434, 1419, 5253, 3097, 257, 2445, 5127, 257, 707, 857, 295, 3364, 21039, 815, 815, 854], "temperature": 0.0, "avg_logprob": -0.22418594360351562, "compression_ratio": 1.6291079812206573, "no_speech_prob": 1.6373745381770277e-07}, {"id": 678, "seek": 325406, "start": 3270.66, "end": 3275.06, "text": " What so by recognizing the parameters what it does is it's", "tokens": [708, 370, 538, 18538, 264, 9834, 437, 309, 775, 307, 309, 311], "temperature": 0.0, "avg_logprob": -0.22418594360351562, "compression_ratio": 1.6291079812206573, "no_speech_prob": 1.6373745381770277e-07}, {"id": 679, "seek": 325406, "start": 3277.82, "end": 3279.7999999999997, "text": " I mean, it's not it's not why we do it", "tokens": [286, 914, 11, 309, 311, 406, 309, 311, 406, 983, 321, 360, 309], "temperature": 0.0, "avg_logprob": -0.22418594360351562, "compression_ratio": 1.6291079812206573, "no_speech_prob": 1.6373745381770277e-07}, {"id": 680, "seek": 327980, "start": 3279.8, "end": 3284.84, "text": " You know the reason why we do it is because we want to penalize things that aren't zero to say like", "tokens": [509, 458, 264, 1778, 983, 321, 360, 309, 307, 570, 321, 528, 281, 13661, 1125, 721, 300, 3212, 380, 4018, 281, 584, 411], "temperature": 0.0, "avg_logprob": -0.1650988930150082, "compression_ratio": 1.7607843137254902, "no_speech_prob": 1.816216695260664e-06}, {"id": 681, "seek": 327980, "start": 3285.5800000000004, "end": 3291.7000000000003, "text": " Don't make this parameter a high number unless it's really helping the loss a lot right set it to zero if you can", "tokens": [1468, 380, 652, 341, 13075, 257, 1090, 1230, 5969, 309, 311, 534, 4315, 264, 4470, 257, 688, 558, 992, 309, 281, 4018, 498, 291, 393], "temperature": 0.0, "avg_logprob": -0.1650988930150082, "compression_ratio": 1.7607843137254902, "no_speech_prob": 1.816216695260664e-06}, {"id": 682, "seek": 327980, "start": 3291.94, "end": 3296.44, "text": " Because setting as many parameters to zero as possible means it's going to generalize better", "tokens": [1436, 3287, 382, 867, 9834, 281, 4018, 382, 1944, 1355, 309, 311, 516, 281, 2674, 1125, 1101], "temperature": 0.0, "avg_logprob": -0.1650988930150082, "compression_ratio": 1.7607843137254902, "no_speech_prob": 1.816216695260664e-06}, {"id": 683, "seek": 327980, "start": 3296.6200000000003, "end": 3299.0800000000004, "text": " Right it's like the same as having a smaller", "tokens": [1779, 309, 311, 411, 264, 912, 382, 1419, 257, 4356], "temperature": 0.0, "avg_logprob": -0.1650988930150082, "compression_ratio": 1.7607843137254902, "no_speech_prob": 1.816216695260664e-06}, {"id": 684, "seek": 327980, "start": 3299.82, "end": 3303.38, "text": " Network, right so that's that's we do that's why we do it", "tokens": [12640, 11, 558, 370, 300, 311, 300, 311, 321, 360, 300, 311, 983, 321, 360, 309], "temperature": 0.0, "avg_logprob": -0.1650988930150082, "compression_ratio": 1.7607843137254902, "no_speech_prob": 1.816216695260664e-06}, {"id": 685, "seek": 327980, "start": 3304.3, "end": 3307.82, "text": " But it can change how it learns as well", "tokens": [583, 309, 393, 1319, 577, 309, 27152, 382, 731], "temperature": 0.0, "avg_logprob": -0.1650988930150082, "compression_ratio": 1.7607843137254902, "no_speech_prob": 1.816216695260664e-06}, {"id": 686, "seek": 330782, "start": 3307.82, "end": 3311.36, "text": " So let's okay this one moment, Erica", "tokens": [407, 718, 311, 1392, 341, 472, 1623, 11, 37429], "temperature": 0.0, "avg_logprob": -0.23803489613083173, "compression_ratio": 1.6588235294117648, "no_speech_prob": 1.963797103599063e-06}, {"id": 687, "seek": 330782, "start": 3311.36, "end": 3315.6400000000003, "text": " So I just wanted to check how we actually went here so after the second epoch yeah, so you can see here", "tokens": [407, 286, 445, 1415, 281, 1520, 577, 321, 767, 1437, 510, 370, 934, 264, 1150, 30992, 339, 1338, 11, 370, 291, 393, 536, 510], "temperature": 0.0, "avg_logprob": -0.23803489613083173, "compression_ratio": 1.6588235294117648, "no_speech_prob": 1.963797103599063e-06}, {"id": 688, "seek": 330782, "start": 3315.6400000000003, "end": 3317.8, "text": " It's really has helped right after the second epoch", "tokens": [467, 311, 534, 575, 4254, 558, 934, 264, 1150, 30992, 339], "temperature": 0.0, "avg_logprob": -0.23803489613083173, "compression_ratio": 1.6588235294117648, "no_speech_prob": 1.963797103599063e-06}, {"id": 689, "seek": 330782, "start": 3319.56, "end": 3323.7200000000003, "text": " Before we got to 97 percent accuracy now. We're nearly up to about 98 percent accuracy", "tokens": [4546, 321, 658, 281, 23399, 3043, 14170, 586, 13, 492, 434, 6217, 493, 281, 466, 20860, 3043, 14170], "temperature": 0.0, "avg_logprob": -0.23803489613083173, "compression_ratio": 1.6588235294117648, "no_speech_prob": 1.963797103599063e-06}, {"id": 690, "seek": 330782, "start": 3324.36, "end": 3330.56, "text": " Right and you can see that the loss was point oh eight versus point one three right so adding regularization", "tokens": [1779, 293, 291, 393, 536, 300, 264, 4470, 390, 935, 1954, 3180, 5717, 935, 472, 1045, 558, 370, 5127, 3890, 2144], "temperature": 0.0, "avg_logprob": -0.23803489613083173, "compression_ratio": 1.6588235294117648, "no_speech_prob": 1.963797103599063e-06}, {"id": 691, "seek": 330782, "start": 3331.32, "end": 3333.48, "text": " Has allowed us to find a you know?", "tokens": [8646, 4350, 505, 281, 915, 257, 291, 458, 30], "temperature": 0.0, "avg_logprob": -0.23803489613083173, "compression_ratio": 1.6588235294117648, "no_speech_prob": 1.963797103599063e-06}, {"id": 692, "seek": 333348, "start": 3333.48, "end": 3338.36, "text": " 3% versus 2% so like a 50% better", "tokens": [805, 4, 5717, 568, 4, 370, 411, 257, 2625, 4, 1101], "temperature": 0.0, "avg_logprob": -0.33723235424653986, "compression_ratio": 1.5935828877005347, "no_speech_prob": 9.276328682972235e-07}, {"id": 693, "seek": 333348, "start": 3339.68, "end": 3345.64, "text": " Solution yes, Erica, so there are two pieces to this right what is L2 regularization?", "tokens": [318, 3386, 2086, 11, 37429, 11, 370, 456, 366, 732, 3755, 281, 341, 558, 437, 307, 441, 17, 3890, 2144, 30], "temperature": 0.0, "avg_logprob": -0.33723235424653986, "compression_ratio": 1.5935828877005347, "no_speech_prob": 9.276328682972235e-07}, {"id": 694, "seek": 333348, "start": 3346.04, "end": 3354.64, "text": " And the weight decay no there's so my claim was that the same thing right so weight decay is the version if you just take the", "tokens": [400, 264, 3364, 21039, 572, 456, 311, 370, 452, 3932, 390, 300, 264, 912, 551, 558, 370, 3364, 21039, 307, 264, 3037, 498, 291, 445, 747, 264], "temperature": 0.0, "avg_logprob": -0.33723235424653986, "compression_ratio": 1.5935828877005347, "no_speech_prob": 9.276328682972235e-07}, {"id": 695, "seek": 333348, "start": 3355.04, "end": 3357.88, "text": " Derivative of L2 regularization you get weight decay", "tokens": [5618, 592, 1166, 295, 441, 17, 3890, 2144, 291, 483, 3364, 21039], "temperature": 0.0, "avg_logprob": -0.33723235424653986, "compression_ratio": 1.5935828877005347, "no_speech_prob": 9.276328682972235e-07}, {"id": 696, "seek": 335788, "start": 3357.88, "end": 3363.36, "text": " So you can implement it either by changing the loss function with an with a squared loss", "tokens": [407, 291, 393, 4445, 309, 2139, 538, 4473, 264, 4470, 2445, 365, 364, 365, 257, 8889, 4470], "temperature": 0.0, "avg_logprob": -0.3135820805341348, "compression_ratio": 1.6383928571428572, "no_speech_prob": 8.714278578736412e-07}, {"id": 697, "seek": 335788, "start": 3364.1600000000003, "end": 3366.7200000000003, "text": " Penalty or you can implement it by adding", "tokens": [10571, 304, 874, 420, 291, 393, 4445, 309, 538, 5127], "temperature": 0.0, "avg_logprob": -0.3135820805341348, "compression_ratio": 1.6383928571428572, "no_speech_prob": 8.714278578736412e-07}, {"id": 698, "seek": 335788, "start": 3367.7200000000003, "end": 3371.8, "text": " The weights themselves as part of the gradient, okay?", "tokens": [440, 17443, 2969, 382, 644, 295, 264, 16235, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.3135820805341348, "compression_ratio": 1.6383928571428572, "no_speech_prob": 8.714278578736412e-07}, {"id": 699, "seek": 335788, "start": 3373.32, "end": 3376.7200000000003, "text": " Yeah, I was just going to finish the questions. Yes, okay pass that to the fish", "tokens": [865, 11, 286, 390, 445, 516, 281, 2413, 264, 1651, 13, 1079, 11, 1392, 1320, 300, 281, 264, 3506], "temperature": 0.0, "avg_logprob": -0.3135820805341348, "compression_ratio": 1.6383928571428572, "no_speech_prob": 8.714278578736412e-07}, {"id": 700, "seek": 335788, "start": 3379.6800000000003, "end": 3386.0, "text": " Can we use regularization convolutional here as well absolutely so a convolution layer just is weights", "tokens": [1664, 321, 764, 3890, 2144, 45216, 304, 510, 382, 731, 3122, 370, 257, 45216, 4583, 445, 307, 17443], "temperature": 0.0, "avg_logprob": -0.3135820805341348, "compression_ratio": 1.6383928571428572, "no_speech_prob": 8.714278578736412e-07}, {"id": 701, "seek": 338600, "start": 3386.0, "end": 3388.0, "text": " so yeah", "tokens": [370, 1338], "temperature": 0.0, "avg_logprob": -0.30076405876561213, "compression_ratio": 1.5228426395939085, "no_speech_prob": 3.237719283788465e-06}, {"id": 702, "seek": 338600, "start": 3389.12, "end": 3394.62, "text": " Jeremy can you explain why you thought you needed weight decay in this particular problem?", "tokens": [17809, 393, 291, 2903, 983, 291, 1194, 291, 2978, 3364, 21039, 294, 341, 1729, 1154, 30], "temperature": 0.0, "avg_logprob": -0.30076405876561213, "compression_ratio": 1.5228426395939085, "no_speech_prob": 3.237719283788465e-06}, {"id": 703, "seek": 338600, "start": 3397.12, "end": 3404.98, "text": " Not easily I mean other than to say it's something that I would always try you're overfitting thunder well yeah, I mean okay, so", "tokens": [1726, 3612, 286, 914, 661, 813, 281, 584, 309, 311, 746, 300, 286, 576, 1009, 853, 291, 434, 670, 69, 2414, 19898, 731, 1338, 11, 286, 914, 1392, 11, 370], "temperature": 0.0, "avg_logprob": -0.30076405876561213, "compression_ratio": 1.5228426395939085, "no_speech_prob": 3.237719283788465e-06}, {"id": 704, "seek": 340498, "start": 3404.98, "end": 3415.86, "text": " Even if I yeah, okay, that's a good point unit, so if if my training loss", "tokens": [2754, 498, 286, 1338, 11, 1392, 11, 300, 311, 257, 665, 935, 4985, 11, 370, 498, 498, 452, 3097, 4470], "temperature": 0.0, "avg_logprob": -0.21587008900112575, "compression_ratio": 1.5691489361702127, "no_speech_prob": 1.2878911093139322e-06}, {"id": 705, "seek": 340498, "start": 3416.7, "end": 3420.42, "text": " Was higher than my validation loss then I'm under fitting", "tokens": [3027, 2946, 813, 452, 24071, 4470, 550, 286, 478, 833, 15669], "temperature": 0.0, "avg_logprob": -0.21587008900112575, "compression_ratio": 1.5691489361702127, "no_speech_prob": 1.2878911093139322e-06}, {"id": 706, "seek": 340498, "start": 3421.06, "end": 3428.34, "text": " Right so there's definitely no point regularizing right if like that would always be a bad thing that would always mean you need like", "tokens": [1779, 370, 456, 311, 2138, 572, 935, 3890, 3319, 558, 498, 411, 300, 576, 1009, 312, 257, 1578, 551, 300, 576, 1009, 914, 291, 643, 411], "temperature": 0.0, "avg_logprob": -0.21587008900112575, "compression_ratio": 1.5691489361702127, "no_speech_prob": 1.2878911093139322e-06}, {"id": 707, "seek": 340498, "start": 3428.38, "end": 3430.38, "text": " more parameters in your model", "tokens": [544, 9834, 294, 428, 2316], "temperature": 0.0, "avg_logprob": -0.21587008900112575, "compression_ratio": 1.5691489361702127, "no_speech_prob": 1.2878911093139322e-06}, {"id": 708, "seek": 343038, "start": 3430.38, "end": 3436.58, "text": " In this case I'm I'm over fitting that doesn't necessarily mean regularization will help", "tokens": [682, 341, 1389, 286, 478, 286, 478, 670, 15669, 300, 1177, 380, 4725, 914, 3890, 2144, 486, 854], "temperature": 0.0, "avg_logprob": -0.24878078036838108, "compression_ratio": 1.4528301886792452, "no_speech_prob": 5.507529294845881e-06}, {"id": 709, "seek": 343038, "start": 3436.78, "end": 3441.78, "text": " But it's certainly worth trying. Thank you, and that's a great point. There's one more question. Yes", "tokens": [583, 309, 311, 3297, 3163, 1382, 13, 1044, 291, 11, 293, 300, 311, 257, 869, 935, 13, 821, 311, 472, 544, 1168, 13, 1079], "temperature": 0.0, "avg_logprob": -0.24878078036838108, "compression_ratio": 1.4528301886792452, "no_speech_prob": 5.507529294845881e-06}, {"id": 710, "seek": 343038, "start": 3442.7000000000003, "end": 3444.7000000000003, "text": " Tyler do you want to pass over there?", "tokens": [16869, 360, 291, 528, 281, 1320, 670, 456, 30], "temperature": 0.0, "avg_logprob": -0.24878078036838108, "compression_ratio": 1.4528301886792452, "no_speech_prob": 5.507529294845881e-06}, {"id": 711, "seek": 343038, "start": 3447.86, "end": 3451.02, "text": " So how do you choose the optimal number of epoch?", "tokens": [407, 577, 360, 291, 2826, 264, 16252, 1230, 295, 30992, 339, 30], "temperature": 0.0, "avg_logprob": -0.24878078036838108, "compression_ratio": 1.4528301886792452, "no_speech_prob": 5.507529294845881e-06}, {"id": 712, "seek": 343038, "start": 3455.1400000000003, "end": 3457.1400000000003, "text": " You do my deep learning course", "tokens": [509, 360, 452, 2452, 2539, 1164], "temperature": 0.0, "avg_logprob": -0.24878078036838108, "compression_ratio": 1.4528301886792452, "no_speech_prob": 5.507529294845881e-06}, {"id": 713, "seek": 345714, "start": 3457.14, "end": 3463.66, "text": " It's a it's that's a long story and a lot of lots of you do it do it by here on", "tokens": [467, 311, 257, 309, 311, 300, 311, 257, 938, 1657, 293, 257, 688, 295, 3195, 295, 291, 360, 309, 360, 309, 538, 510, 322], "temperature": 0.0, "avg_logprob": -0.3299141340358283, "compression_ratio": 1.5119617224880382, "no_speech_prob": 8.939508006733377e-06}, {"id": 714, "seek": 345714, "start": 3464.5, "end": 3470.1, "text": " Herosely or there any it's a bit of both. We just don't as I say we don't have time to cover", "tokens": [3204, 329, 736, 420, 456, 604, 309, 311, 257, 857, 295, 1293, 13, 492, 445, 500, 380, 382, 286, 584, 321, 500, 380, 362, 565, 281, 2060], "temperature": 0.0, "avg_logprob": -0.3299141340358283, "compression_ratio": 1.5119617224880382, "no_speech_prob": 8.939508006733377e-06}, {"id": 715, "seek": 345714, "start": 3470.8599999999997, "end": 3478.16, "text": " Best practices in this class we're going to learn the kind of fundamentals. Yeah, okay, so let's take a", "tokens": [9752, 7525, 294, 341, 1508, 321, 434, 516, 281, 1466, 264, 733, 295, 29505, 13, 865, 11, 1392, 11, 370, 718, 311, 747, 257], "temperature": 0.0, "avg_logprob": -0.3299141340358283, "compression_ratio": 1.5119617224880382, "no_speech_prob": 8.939508006733377e-06}, {"id": 716, "seek": 347816, "start": 3478.16, "end": 3486.52, "text": " Six minute break and come back at 1110", "tokens": [11678, 3456, 1821, 293, 808, 646, 412, 2975, 3279], "temperature": 0.0, "avg_logprob": -0.22505128951299758, "compression_ratio": 1.4971098265895955, "no_speech_prob": 5.955059805273777e-06}, {"id": 717, "seek": 347816, "start": 3490.08, "end": 3492.08, "text": " All right", "tokens": [1057, 558], "temperature": 0.0, "avg_logprob": -0.22505128951299758, "compression_ratio": 1.4971098265895955, "no_speech_prob": 5.955059805273777e-06}, {"id": 718, "seek": 347816, "start": 3494.3599999999997, "end": 3498.08, "text": " So something that we cover in great detail in the deep learning course", "tokens": [407, 746, 300, 321, 2060, 294, 869, 2607, 294, 264, 2452, 2539, 1164], "temperature": 0.0, "avg_logprob": -0.22505128951299758, "compression_ratio": 1.4971098265895955, "no_speech_prob": 5.955059805273777e-06}, {"id": 719, "seek": 347816, "start": 3498.08, "end": 3505.0, "text": " But it's like really important to mention here is that is that the secret in my opinion to kind of modern machine learning techniques is", "tokens": [583, 309, 311, 411, 534, 1021, 281, 2152, 510, 307, 300, 307, 300, 264, 4054, 294, 452, 4800, 281, 733, 295, 4363, 3479, 2539, 7512, 307], "temperature": 0.0, "avg_logprob": -0.22505128951299758, "compression_ratio": 1.4971098265895955, "no_speech_prob": 5.955059805273777e-06}, {"id": 720, "seek": 347816, "start": 3505.52, "end": 3506.8399999999997, "text": " to", "tokens": [281], "temperature": 0.0, "avg_logprob": -0.22505128951299758, "compression_ratio": 1.4971098265895955, "no_speech_prob": 5.955059805273777e-06}, {"id": 721, "seek": 350684, "start": 3506.84, "end": 3512.44, "text": " Massively over parameter eyes the solution to your problem right like as we've done here", "tokens": [10482, 3413, 670, 13075, 2575, 264, 3827, 281, 428, 1154, 558, 411, 382, 321, 600, 1096, 510], "temperature": 0.0, "avg_logprob": -0.21229776103844802, "compression_ratio": 1.5414847161572052, "no_speech_prob": 3.5008147278858814e-06}, {"id": 722, "seek": 350684, "start": 3512.44, "end": 3518.44, "text": " You know we've got like a hundred thousand weights when we only had a small number of 28 by 28 images", "tokens": [509, 458, 321, 600, 658, 411, 257, 3262, 4714, 17443, 562, 321, 787, 632, 257, 1359, 1230, 295, 7562, 538, 7562, 5267], "temperature": 0.0, "avg_logprob": -0.21229776103844802, "compression_ratio": 1.5414847161572052, "no_speech_prob": 3.5008147278858814e-06}, {"id": 723, "seek": 350684, "start": 3519.0, "end": 3522.84, "text": " And then use regularization, okay, it's like the", "tokens": [400, 550, 764, 3890, 2144, 11, 1392, 11, 309, 311, 411, 264], "temperature": 0.0, "avg_logprob": -0.21229776103844802, "compression_ratio": 1.5414847161572052, "no_speech_prob": 3.5008147278858814e-06}, {"id": 724, "seek": 350684, "start": 3524.1200000000003, "end": 3526.1200000000003, "text": " direct opposite of", "tokens": [2047, 6182, 295], "temperature": 0.0, "avg_logprob": -0.21229776103844802, "compression_ratio": 1.5414847161572052, "no_speech_prob": 3.5008147278858814e-06}, {"id": 725, "seek": 350684, "start": 3526.28, "end": 3527.88, "text": " how", "tokens": [577], "temperature": 0.0, "avg_logprob": -0.21229776103844802, "compression_ratio": 1.5414847161572052, "no_speech_prob": 3.5008147278858814e-06}, {"id": 726, "seek": 350684, "start": 3527.88, "end": 3529.48, "text": " nearly all", "tokens": [6217, 439], "temperature": 0.0, "avg_logprob": -0.21229776103844802, "compression_ratio": 1.5414847161572052, "no_speech_prob": 3.5008147278858814e-06}, {"id": 727, "seek": 350684, "start": 3529.48, "end": 3532.6400000000003, "text": " Statistics and learning was done for decades before", "tokens": [49226, 293, 2539, 390, 1096, 337, 7878, 949], "temperature": 0.0, "avg_logprob": -0.21229776103844802, "compression_ratio": 1.5414847161572052, "no_speech_prob": 3.5008147278858814e-06}, {"id": 728, "seek": 350684, "start": 3533.6400000000003, "end": 3535.6400000000003, "text": " and still most kind of like", "tokens": [293, 920, 881, 733, 295, 411], "temperature": 0.0, "avg_logprob": -0.21229776103844802, "compression_ratio": 1.5414847161572052, "no_speech_prob": 3.5008147278858814e-06}, {"id": 729, "seek": 353564, "start": 3535.64, "end": 3543.64, "text": " Senior lecturers at most universities in most areas of have this background where they've learned the correct way to build a model is to", "tokens": [18370, 5899, 14198, 412, 881, 11779, 294, 881, 3179, 295, 362, 341, 3678, 689, 436, 600, 3264, 264, 3006, 636, 281, 1322, 257, 2316, 307, 281], "temperature": 0.0, "avg_logprob": -0.15535477095959233, "compression_ratio": 1.8293650793650793, "no_speech_prob": 2.1568073407252086e-06}, {"id": 730, "seek": 353564, "start": 3543.64, "end": 3545.64, "text": " Like have as few parameters as possible", "tokens": [1743, 362, 382, 1326, 9834, 382, 1944], "temperature": 0.0, "avg_logprob": -0.15535477095959233, "compression_ratio": 1.8293650793650793, "no_speech_prob": 2.1568073407252086e-06}, {"id": 731, "seek": 353564, "start": 3546.2799999999997, "end": 3551.7599999999998, "text": " Right and so hopefully we've learned two things so far. You know one is we can build", "tokens": [1779, 293, 370, 4696, 321, 600, 3264, 732, 721, 370, 1400, 13, 509, 458, 472, 307, 321, 393, 1322], "temperature": 0.0, "avg_logprob": -0.15535477095959233, "compression_ratio": 1.8293650793650793, "no_speech_prob": 2.1568073407252086e-06}, {"id": 732, "seek": 353564, "start": 3554.3199999999997, "end": 3559.96, "text": " Very accurate models even when they have lots and lots of parameters like a random forest has a lot of parameters and", "tokens": [4372, 8559, 5245, 754, 562, 436, 362, 3195, 293, 3195, 295, 9834, 411, 257, 4974, 6719, 575, 257, 688, 295, 9834, 293], "temperature": 0.0, "avg_logprob": -0.15535477095959233, "compression_ratio": 1.8293650793650793, "no_speech_prob": 2.1568073407252086e-06}, {"id": 733, "seek": 355996, "start": 3559.96, "end": 3565.88, "text": " You know this here deep network has a lot of parameters, and they can be accurate right?", "tokens": [509, 458, 341, 510, 2452, 3209, 575, 257, 688, 295, 9834, 11, 293, 436, 393, 312, 8559, 558, 30], "temperature": 0.0, "avg_logprob": -0.23434897166926685, "compression_ratio": 1.621212121212121, "no_speech_prob": 1.136560172199097e-06}, {"id": 734, "seek": 355996, "start": 3566.8, "end": 3570.68, "text": " And we can do that by either using bagging or", "tokens": [400, 321, 393, 360, 300, 538, 2139, 1228, 3411, 3249, 420], "temperature": 0.0, "avg_logprob": -0.23434897166926685, "compression_ratio": 1.621212121212121, "no_speech_prob": 1.136560172199097e-06}, {"id": 735, "seek": 355996, "start": 3571.56, "end": 3573.4, "text": " by using", "tokens": [538, 1228], "temperature": 0.0, "avg_logprob": -0.23434897166926685, "compression_ratio": 1.621212121212121, "no_speech_prob": 1.136560172199097e-06}, {"id": 736, "seek": 355996, "start": 3573.4, "end": 3577.64, "text": " regularization okay and regularization in neural nets means either", "tokens": [3890, 2144, 1392, 293, 3890, 2144, 294, 18161, 36170, 1355, 2139], "temperature": 0.0, "avg_logprob": -0.23434897166926685, "compression_ratio": 1.621212121212121, "no_speech_prob": 1.136560172199097e-06}, {"id": 737, "seek": 355996, "start": 3578.28, "end": 3579.92, "text": " weight decay", "tokens": [3364, 21039], "temperature": 0.0, "avg_logprob": -0.23434897166926685, "compression_ratio": 1.621212121212121, "no_speech_prob": 1.136560172199097e-06}, {"id": 738, "seek": 355996, "start": 3579.92, "end": 3582.68, "text": " also known as kind of fail to regularization or", "tokens": [611, 2570, 382, 733, 295, 3061, 281, 3890, 2144, 420], "temperature": 0.0, "avg_logprob": -0.23434897166926685, "compression_ratio": 1.621212121212121, "no_speech_prob": 1.136560172199097e-06}, {"id": 739, "seek": 355996, "start": 3583.4, "end": 3586.84, "text": " Drop out which we won't worry too much about here", "tokens": [17675, 484, 597, 321, 1582, 380, 3292, 886, 709, 466, 510], "temperature": 0.0, "avg_logprob": -0.23434897166926685, "compression_ratio": 1.621212121212121, "no_speech_prob": 1.136560172199097e-06}, {"id": 740, "seek": 358684, "start": 3586.84, "end": 3588.84, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.24231917683671161, "compression_ratio": 1.5024630541871922, "no_speech_prob": 7.453745070051809e-07}, {"id": 741, "seek": 358684, "start": 3589.52, "end": 3591.28, "text": " Like it's a", "tokens": [1743, 309, 311, 257], "temperature": 0.0, "avg_logprob": -0.24231917683671161, "compression_ratio": 1.5024630541871922, "no_speech_prob": 7.453745070051809e-07}, {"id": 742, "seek": 358684, "start": 3591.28, "end": 3593.28, "text": " It's a very different way of thinking", "tokens": [467, 311, 257, 588, 819, 636, 295, 1953], "temperature": 0.0, "avg_logprob": -0.24231917683671161, "compression_ratio": 1.5024630541871922, "no_speech_prob": 7.453745070051809e-07}, {"id": 743, "seek": 358684, "start": 3594.28, "end": 3595.28, "text": " about", "tokens": [466], "temperature": 0.0, "avg_logprob": -0.24231917683671161, "compression_ratio": 1.5024630541871922, "no_speech_prob": 7.453745070051809e-07}, {"id": 744, "seek": 358684, "start": 3595.28, "end": 3601.6800000000003, "text": " building useful models and like I just wanted to kind of warn you that once you leave this classroom", "tokens": [2390, 4420, 5245, 293, 411, 286, 445, 1415, 281, 733, 295, 12286, 291, 300, 1564, 291, 1856, 341, 7419], "temperature": 0.0, "avg_logprob": -0.24231917683671161, "compression_ratio": 1.5024630541871922, "no_speech_prob": 7.453745070051809e-07}, {"id": 745, "seek": 358684, "start": 3602.4, "end": 3608.32, "text": " Like even possibly when you go to the next faculty members talk like there'll be people at USF as well who?", "tokens": [1743, 754, 6264, 562, 291, 352, 281, 264, 958, 6389, 2679, 751, 411, 456, 603, 312, 561, 412, 2546, 37, 382, 731, 567, 30], "temperature": 0.0, "avg_logprob": -0.24231917683671161, "compression_ratio": 1.5024630541871922, "no_speech_prob": 7.453745070051809e-07}, {"id": 746, "seek": 358684, "start": 3609.76, "end": 3612.48, "text": " Entirely trained in the world of like", "tokens": [3951, 621, 356, 8895, 294, 264, 1002, 295, 411], "temperature": 0.0, "avg_logprob": -0.24231917683671161, "compression_ratio": 1.5024630541871922, "no_speech_prob": 7.453745070051809e-07}, {"id": 747, "seek": 361248, "start": 3612.48, "end": 3620.0, "text": " Models with small numbers of parameters you know your next boss is very likely to have been trained in the world of like models with small", "tokens": [6583, 1625, 365, 1359, 3547, 295, 9834, 291, 458, 428, 958, 5741, 307, 588, 3700, 281, 362, 668, 8895, 294, 264, 1002, 295, 411, 5245, 365, 1359], "temperature": 0.0, "avg_logprob": -0.2606267340389299, "compression_ratio": 1.736318407960199, "no_speech_prob": 5.28549662703881e-07}, {"id": 748, "seek": 361248, "start": 3620.0, "end": 3621.36, "text": " numbers of parameters", "tokens": [3547, 295, 9834], "temperature": 0.0, "avg_logprob": -0.2606267340389299, "compression_ratio": 1.736318407960199, "no_speech_prob": 5.28549662703881e-07}, {"id": 749, "seek": 361248, "start": 3621.36, "end": 3623.84, "text": " The idea that they are somehow", "tokens": [440, 1558, 300, 436, 366, 6063], "temperature": 0.0, "avg_logprob": -0.2606267340389299, "compression_ratio": 1.736318407960199, "no_speech_prob": 5.28549662703881e-07}, {"id": 750, "seek": 361248, "start": 3624.72, "end": 3628.64, "text": " More pure or easier or better or more interpretable or whatever I?", "tokens": [5048, 6075, 420, 3571, 420, 1101, 420, 544, 7302, 712, 420, 2035, 286, 30], "temperature": 0.0, "avg_logprob": -0.2606267340389299, "compression_ratio": 1.736318407960199, "no_speech_prob": 5.28549662703881e-07}, {"id": 751, "seek": 361248, "start": 3631.28, "end": 3638.56, "text": " I am convinced that that is not true probably not ever true certainly very rarely true", "tokens": [286, 669, 12561, 300, 300, 307, 406, 2074, 1391, 406, 1562, 2074, 3297, 588, 13752, 2074], "temperature": 0.0, "avg_logprob": -0.2606267340389299, "compression_ratio": 1.736318407960199, "no_speech_prob": 5.28549662703881e-07}, {"id": 752, "seek": 361248, "start": 3639.6, "end": 3641.32, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.2606267340389299, "compression_ratio": 1.736318407960199, "no_speech_prob": 5.28549662703881e-07}, {"id": 753, "seek": 364132, "start": 3641.32, "end": 3643.32, "text": " that actually", "tokens": [300, 767], "temperature": 0.0, "avg_logprob": -0.12723880118512093, "compression_ratio": 1.6404494382022472, "no_speech_prob": 1.8162147625844227e-06}, {"id": 754, "seek": 364132, "start": 3644.6000000000004, "end": 3651.0, "text": " Models with lots of parameters can be extremely interpretable as we learned from our whole lesson of random forest interpretation", "tokens": [6583, 1625, 365, 3195, 295, 9834, 393, 312, 4664, 7302, 712, 382, 321, 3264, 490, 527, 1379, 6898, 295, 4974, 6719, 14174], "temperature": 0.0, "avg_logprob": -0.12723880118512093, "compression_ratio": 1.6404494382022472, "no_speech_prob": 1.8162147625844227e-06}, {"id": 755, "seek": 364132, "start": 3652.0, "end": 3659.76, "text": " You can use most of the same techniques with neural nets, but with neural nets are even easier right remember how we did feature importance by", "tokens": [509, 393, 764, 881, 295, 264, 912, 7512, 365, 18161, 36170, 11, 457, 365, 18161, 36170, 366, 754, 3571, 558, 1604, 577, 321, 630, 4111, 7379, 538], "temperature": 0.0, "avg_logprob": -0.12723880118512093, "compression_ratio": 1.6404494382022472, "no_speech_prob": 1.8162147625844227e-06}, {"id": 756, "seek": 364132, "start": 3661.0800000000004, "end": 3664.92, "text": " Randomizing a column to see how it changes in that column would impact the output", "tokens": [37603, 3319, 257, 7738, 281, 536, 577, 309, 2962, 294, 300, 7738, 576, 2712, 264, 5598], "temperature": 0.0, "avg_logprob": -0.12723880118512093, "compression_ratio": 1.6404494382022472, "no_speech_prob": 1.8162147625844227e-06}, {"id": 757, "seek": 364132, "start": 3665.44, "end": 3669.52, "text": " Well, that's just like a kind of dumb way of calculating its gradient", "tokens": [1042, 11, 300, 311, 445, 411, 257, 733, 295, 10316, 636, 295, 28258, 1080, 16235], "temperature": 0.0, "avg_logprob": -0.12723880118512093, "compression_ratio": 1.6404494382022472, "no_speech_prob": 1.8162147625844227e-06}, {"id": 758, "seek": 366952, "start": 3669.52, "end": 3675.36, "text": " How much does varying this import change the output with a neural net we can actually calculate its gradient?", "tokens": [1012, 709, 775, 22984, 341, 974, 1319, 264, 5598, 365, 257, 18161, 2533, 321, 393, 767, 8873, 1080, 16235, 30], "temperature": 0.0, "avg_logprob": -0.21646341022692228, "compression_ratio": 1.6778242677824269, "no_speech_prob": 1.0511473647056846e-06}, {"id": 759, "seek": 366952, "start": 3675.84, "end": 3680.82, "text": " Right so with pi torch you could actually say what's the gradient of the output with respect to this column?", "tokens": [1779, 370, 365, 3895, 27822, 291, 727, 767, 584, 437, 311, 264, 16235, 295, 264, 5598, 365, 3104, 281, 341, 7738, 30], "temperature": 0.0, "avg_logprob": -0.21646341022692228, "compression_ratio": 1.6778242677824269, "no_speech_prob": 1.0511473647056846e-06}, {"id": 760, "seek": 366952, "start": 3681.52, "end": 3682.6, "text": " right", "tokens": [558], "temperature": 0.0, "avg_logprob": -0.21646341022692228, "compression_ratio": 1.6778242677824269, "no_speech_prob": 1.0511473647056846e-06}, {"id": 761, "seek": 366952, "start": 3682.6, "end": 3684.6, "text": " You can do the same kind of thing to do", "tokens": [509, 393, 360, 264, 912, 733, 295, 551, 281, 360], "temperature": 0.0, "avg_logprob": -0.21646341022692228, "compression_ratio": 1.6778242677824269, "no_speech_prob": 1.0511473647056846e-06}, {"id": 762, "seek": 366952, "start": 3685.92, "end": 3687.92, "text": " partial dependence plot with a neural net", "tokens": [14641, 31704, 7542, 365, 257, 18161, 2533], "temperature": 0.0, "avg_logprob": -0.21646341022692228, "compression_ratio": 1.6778242677824269, "no_speech_prob": 1.0511473647056846e-06}, {"id": 763, "seek": 366952, "start": 3688.88, "end": 3692.52, "text": " And you know I'll mention for those of you interested in making a real impact", "tokens": [400, 291, 458, 286, 603, 2152, 337, 729, 295, 291, 3102, 294, 1455, 257, 957, 2712], "temperature": 0.0, "avg_logprob": -0.21646341022692228, "compression_ratio": 1.6778242677824269, "no_speech_prob": 1.0511473647056846e-06}, {"id": 764, "seek": 366952, "start": 3693.36, "end": 3695.08, "text": " Nobody's written", "tokens": [9297, 311, 3720], "temperature": 0.0, "avg_logprob": -0.21646341022692228, "compression_ratio": 1.6778242677824269, "no_speech_prob": 1.0511473647056846e-06}, {"id": 765, "seek": 369508, "start": 3695.08, "end": 3700.24, "text": " Basically any of these things for neural nets right so that that that whole area", "tokens": [8537, 604, 295, 613, 721, 337, 18161, 36170, 558, 370, 300, 300, 300, 1379, 1859], "temperature": 0.0, "avg_logprob": -0.2045727190764054, "compression_ratio": 1.6835443037974684, "no_speech_prob": 2.058041445707204e-06}, {"id": 766, "seek": 369508, "start": 3700.7599999999998, "end": 3704.16, "text": " Needs like libraries to be written blog posts to be written", "tokens": [1734, 5147, 411, 15148, 281, 312, 3720, 6968, 12300, 281, 312, 3720], "temperature": 0.0, "avg_logprob": -0.2045727190764054, "compression_ratio": 1.6835443037974684, "no_speech_prob": 2.058041445707204e-06}, {"id": 767, "seek": 369508, "start": 3704.4, "end": 3706.68, "text": " You know some papers have been written", "tokens": [509, 458, 512, 10577, 362, 668, 3720], "temperature": 0.0, "avg_logprob": -0.2045727190764054, "compression_ratio": 1.6835443037974684, "no_speech_prob": 2.058041445707204e-06}, {"id": 768, "seek": 369508, "start": 3706.68, "end": 3714.48, "text": " But only in very narrow domains like computer vision as far as I know nobody's written the paper saying here's how to do structured data", "tokens": [583, 787, 294, 588, 9432, 25514, 411, 3820, 5201, 382, 1400, 382, 286, 458, 5079, 311, 3720, 264, 3035, 1566, 510, 311, 577, 281, 360, 18519, 1412], "temperature": 0.0, "avg_logprob": -0.2045727190764054, "compression_ratio": 1.6835443037974684, "no_speech_prob": 2.058041445707204e-06}, {"id": 769, "seek": 369508, "start": 3715.12, "end": 3718.42, "text": " Neural networks you know interpretation methods", "tokens": [1734, 1807, 9590, 291, 458, 14174, 7150], "temperature": 0.0, "avg_logprob": -0.2045727190764054, "compression_ratio": 1.6835443037974684, "no_speech_prob": 2.058041445707204e-06}, {"id": 770, "seek": 369508, "start": 3719.08, "end": 3721.08, "text": " So it's a really exciting big", "tokens": [407, 309, 311, 257, 534, 4670, 955], "temperature": 0.0, "avg_logprob": -0.2045727190764054, "compression_ratio": 1.6835443037974684, "no_speech_prob": 2.058041445707204e-06}, {"id": 771, "seek": 369508, "start": 3721.64, "end": 3723.64, "text": " area", "tokens": [1859], "temperature": 0.0, "avg_logprob": -0.2045727190764054, "compression_ratio": 1.6835443037974684, "no_speech_prob": 2.058041445707204e-06}, {"id": 772, "seek": 372364, "start": 3723.64, "end": 3725.3599999999997, "text": " So", "tokens": [407], "temperature": 0.0, "avg_logprob": -0.20337978998819986, "compression_ratio": 1.6283185840707965, "no_speech_prob": 1.733042040541477e-06}, {"id": 773, "seek": 372364, "start": 3725.3599999999997, "end": 3729.96, "text": " What we're going to do though is we're going to start with applying this", "tokens": [708, 321, 434, 516, 281, 360, 1673, 307, 321, 434, 516, 281, 722, 365, 9275, 341], "temperature": 0.0, "avg_logprob": -0.20337978998819986, "compression_ratio": 1.6283185840707965, "no_speech_prob": 1.733042040541477e-06}, {"id": 774, "seek": 372364, "start": 3731.48, "end": 3733.48, "text": " With a simple linear model", "tokens": [2022, 257, 2199, 8213, 2316], "temperature": 0.0, "avg_logprob": -0.20337978998819986, "compression_ratio": 1.6283185840707965, "no_speech_prob": 1.733042040541477e-06}, {"id": 775, "seek": 372364, "start": 3734.2, "end": 3738.04, "text": " This is mildly terrifying for me because we're going to do NLP and our NLP", "tokens": [639, 307, 15154, 356, 18106, 337, 385, 570, 321, 434, 516, 281, 360, 426, 45196, 293, 527, 426, 45196], "temperature": 0.0, "avg_logprob": -0.20337978998819986, "compression_ratio": 1.6283185840707965, "no_speech_prob": 1.733042040541477e-06}, {"id": 776, "seek": 372364, "start": 3738.4, "end": 3742.5, "text": " Faculty expert is in the room so David just yell at me if I screw this up too badly", "tokens": [32689, 5844, 307, 294, 264, 1808, 370, 4389, 445, 20525, 412, 385, 498, 286, 5630, 341, 493, 886, 13425], "temperature": 0.0, "avg_logprob": -0.20337978998819986, "compression_ratio": 1.6283185840707965, "no_speech_prob": 1.733042040541477e-06}, {"id": 777, "seek": 372364, "start": 3745.08, "end": 3753.04, "text": " And so NLP refers to you know any any kind of modeling where we're working with with natural language text", "tokens": [400, 370, 426, 45196, 14942, 281, 291, 458, 604, 604, 733, 295, 15983, 689, 321, 434, 1364, 365, 365, 3303, 2856, 2487], "temperature": 0.0, "avg_logprob": -0.20337978998819986, "compression_ratio": 1.6283185840707965, "no_speech_prob": 1.733042040541477e-06}, {"id": 778, "seek": 375304, "start": 3753.04, "end": 3755.84, "text": " All right, and it interestingly enough", "tokens": [1057, 558, 11, 293, 309, 25873, 1547], "temperature": 0.0, "avg_logprob": -0.20328664277729236, "compression_ratio": 1.8110599078341014, "no_speech_prob": 2.1233674942777725e-06}, {"id": 779, "seek": 375304, "start": 3758.44, "end": 3760.92, "text": " We're going to look at a situation where a", "tokens": [492, 434, 516, 281, 574, 412, 257, 2590, 689, 257], "temperature": 0.0, "avg_logprob": -0.20328664277729236, "compression_ratio": 1.8110599078341014, "no_speech_prob": 2.1233674942777725e-06}, {"id": 780, "seek": 375304, "start": 3762.0, "end": 3767.2799999999997, "text": " Linear model is pretty close to the state of the art for solving a particular problem", "tokens": [14670, 289, 2316, 307, 1238, 1998, 281, 264, 1785, 295, 264, 1523, 337, 12606, 257, 1729, 1154], "temperature": 0.0, "avg_logprob": -0.20328664277729236, "compression_ratio": 1.8110599078341014, "no_speech_prob": 2.1233674942777725e-06}, {"id": 781, "seek": 375304, "start": 3767.88, "end": 3769.88, "text": " it's actually something where I", "tokens": [309, 311, 767, 746, 689, 286], "temperature": 0.0, "avg_logprob": -0.20328664277729236, "compression_ratio": 1.8110599078341014, "no_speech_prob": 2.1233674942777725e-06}, {"id": 782, "seek": 375304, "start": 3770.56, "end": 3774.2799999999997, "text": " actually surpassed the state of the art in this using a", "tokens": [767, 27650, 292, 264, 1785, 295, 264, 1523, 294, 341, 1228, 257], "temperature": 0.0, "avg_logprob": -0.20328664277729236, "compression_ratio": 1.8110599078341014, "no_speech_prob": 2.1233674942777725e-06}, {"id": 783, "seek": 375304, "start": 3775.04, "end": 3777.04, "text": " recurrent neural network a few weeks ago", "tokens": [18680, 1753, 18161, 3209, 257, 1326, 3259, 2057], "temperature": 0.0, "avg_logprob": -0.20328664277729236, "compression_ratio": 1.8110599078341014, "no_speech_prob": 2.1233674942777725e-06}, {"id": 784, "seek": 377704, "start": 3777.04, "end": 3782.48, "text": " But this is actually going to show you pretty close to the state of art with with a linear model", "tokens": [583, 341, 307, 767, 516, 281, 855, 291, 1238, 1998, 281, 264, 1785, 295, 1523, 365, 365, 257, 8213, 2316], "temperature": 0.0, "avg_logprob": -0.253738834744408, "compression_ratio": 1.6701570680628273, "no_speech_prob": 2.726455932133831e-06}, {"id": 785, "seek": 377704, "start": 3783.24, "end": 3785.96, "text": " We're going to be working with the IMDb", "tokens": [492, 434, 516, 281, 312, 1364, 365, 264, 21463, 35, 65], "temperature": 0.0, "avg_logprob": -0.253738834744408, "compression_ratio": 1.6701570680628273, "no_speech_prob": 2.726455932133831e-06}, {"id": 786, "seek": 377704, "start": 3786.4, "end": 3793.08, "text": " IMDb data set so this is a data set of movie reviews you can download it by following these steps", "tokens": [21463, 35, 65, 1412, 992, 370, 341, 307, 257, 1412, 992, 295, 3169, 10229, 291, 393, 5484, 309, 538, 3480, 613, 4439], "temperature": 0.0, "avg_logprob": -0.253738834744408, "compression_ratio": 1.6701570680628273, "no_speech_prob": 2.726455932133831e-06}, {"id": 787, "seek": 377704, "start": 3794.12, "end": 3796.12, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.253738834744408, "compression_ratio": 1.6701570680628273, "no_speech_prob": 2.726455932133831e-06}, {"id": 788, "seek": 377704, "start": 3797.8, "end": 3804.7599999999998, "text": " Once you download it you'll see that you've got a train and a test directory and", "tokens": [3443, 291, 5484, 309, 291, 603, 536, 300, 291, 600, 658, 257, 3847, 293, 257, 1500, 21120, 293], "temperature": 0.0, "avg_logprob": -0.253738834744408, "compression_ratio": 1.6701570680628273, "no_speech_prob": 2.726455932133831e-06}, {"id": 789, "seek": 380476, "start": 3804.76, "end": 3812.38, "text": " In your train directory you'll see there's a negative and a positive directory and in your positive", "tokens": [682, 428, 3847, 21120, 291, 603, 536, 456, 311, 257, 3671, 293, 257, 3353, 21120, 293, 294, 428, 3353], "temperature": 0.0, "avg_logprob": -0.18026607686823065, "compression_ratio": 1.7114427860696517, "no_speech_prob": 1.6028031950554578e-06}, {"id": 790, "seek": 380476, "start": 3812.6800000000003, "end": 3814.8, "text": " Directory you'll see there's a bunch of text files", "tokens": [49598, 291, 603, 536, 456, 311, 257, 3840, 295, 2487, 7098], "temperature": 0.0, "avg_logprob": -0.18026607686823065, "compression_ratio": 1.7114427860696517, "no_speech_prob": 1.6028031950554578e-06}, {"id": 791, "seek": 380476, "start": 3817.92, "end": 3819.92, "text": " And here's an example of a text file", "tokens": [400, 510, 311, 364, 1365, 295, 257, 2487, 3991], "temperature": 0.0, "avg_logprob": -0.18026607686823065, "compression_ratio": 1.7114427860696517, "no_speech_prob": 1.6028031950554578e-06}, {"id": 792, "seek": 380476, "start": 3820.0800000000004, "end": 3825.96, "text": " So somehow we've managed to pick out a story of a man who has unnatural feelings for a pig as our first choice", "tokens": [407, 6063, 321, 600, 6453, 281, 1888, 484, 257, 1657, 295, 257, 587, 567, 575, 43470, 6640, 337, 257, 8120, 382, 527, 700, 3922], "temperature": 0.0, "avg_logprob": -0.18026607686823065, "compression_ratio": 1.7114427860696517, "no_speech_prob": 1.6028031950554578e-06}, {"id": 793, "seek": 380476, "start": 3826.28, "end": 3828.6800000000003, "text": " That wasn't intentional, but it'll be fine", "tokens": [663, 2067, 380, 21935, 11, 457, 309, 603, 312, 2489], "temperature": 0.0, "avg_logprob": -0.18026607686823065, "compression_ratio": 1.7114427860696517, "no_speech_prob": 1.6028031950554578e-06}, {"id": 794, "seek": 380476, "start": 3831.48, "end": 3832.96, "text": " So", "tokens": [407], "temperature": 0.0, "avg_logprob": -0.18026607686823065, "compression_ratio": 1.7114427860696517, "no_speech_prob": 1.6028031950554578e-06}, {"id": 795, "seek": 383296, "start": 3832.96, "end": 3835.36, "text": " We're going to look at these movie reviews", "tokens": [492, 434, 516, 281, 574, 412, 613, 3169, 10229], "temperature": 0.0, "avg_logprob": -0.14526692390441895, "compression_ratio": 1.8049792531120332, "no_speech_prob": 6.540340109495446e-06}, {"id": 796, "seek": 383296, "start": 3836.32, "end": 3839.76, "text": " And for each one we're going to look to see whether they were positive or negative", "tokens": [400, 337, 1184, 472, 321, 434, 516, 281, 574, 281, 536, 1968, 436, 645, 3353, 420, 3671], "temperature": 0.0, "avg_logprob": -0.14526692390441895, "compression_ratio": 1.8049792531120332, "no_speech_prob": 6.540340109495446e-06}, {"id": 797, "seek": 383296, "start": 3839.76, "end": 3841.96, "text": " So they've been put into one of these folders", "tokens": [407, 436, 600, 668, 829, 666, 472, 295, 613, 31082], "temperature": 0.0, "avg_logprob": -0.14526692390441895, "compression_ratio": 1.8049792531120332, "no_speech_prob": 6.540340109495446e-06}, {"id": 798, "seek": 383296, "start": 3841.96, "end": 3846.88, "text": " They were downloaded from from IMDb the movie database and review site", "tokens": [814, 645, 21748, 490, 490, 21463, 35, 65, 264, 3169, 8149, 293, 3131, 3621], "temperature": 0.0, "avg_logprob": -0.14526692390441895, "compression_ratio": 1.8049792531120332, "no_speech_prob": 6.540340109495446e-06}, {"id": 799, "seek": 383296, "start": 3847.44, "end": 3854.64, "text": " The ones that were strongly positive went in positive strongly negative went negative and the rest they didn't label at all", "tokens": [440, 2306, 300, 645, 10613, 3353, 1437, 294, 3353, 10613, 3671, 1437, 3671, 293, 264, 1472, 436, 994, 380, 7645, 412, 439], "temperature": 0.0, "avg_logprob": -0.14526692390441895, "compression_ratio": 1.8049792531120332, "no_speech_prob": 6.540340109495446e-06}, {"id": 800, "seek": 383296, "start": 3854.64, "end": 3858.12, "text": " So these are only highly polarized reviews so in this case, you know", "tokens": [407, 613, 366, 787, 5405, 48623, 10229, 370, 294, 341, 1389, 11, 291, 458], "temperature": 0.0, "avg_logprob": -0.14526692390441895, "compression_ratio": 1.8049792531120332, "no_speech_prob": 6.540340109495446e-06}, {"id": 801, "seek": 385812, "start": 3858.12, "end": 3863.56, "text": " We have an insane violent mob which unfortunately is too absurd", "tokens": [492, 362, 364, 10838, 11867, 4298, 597, 7015, 307, 886, 19774], "temperature": 0.0, "avg_logprob": -0.2736583658166834, "compression_ratio": 1.4973544973544974, "no_speech_prob": 1.0348497880841023e-06}, {"id": 802, "seek": 385812, "start": 3863.96, "end": 3871.48, "text": " Too off-putting those in the area be turned off so the label for this was a zero which is", "tokens": [11395, 766, 12, 2582, 783, 729, 294, 264, 1859, 312, 3574, 766, 370, 264, 7645, 337, 341, 390, 257, 4018, 597, 307], "temperature": 0.0, "avg_logprob": -0.2736583658166834, "compression_ratio": 1.4973544973544974, "no_speech_prob": 1.0348497880841023e-06}, {"id": 803, "seek": 385812, "start": 3873.3199999999997, "end": 3877.0, "text": " Negative okay, so this is a negative review", "tokens": [43230, 1392, 11, 370, 341, 307, 257, 3671, 3131], "temperature": 0.0, "avg_logprob": -0.2736583658166834, "compression_ratio": 1.4973544973544974, "no_speech_prob": 1.0348497880841023e-06}, {"id": 804, "seek": 385812, "start": 3877.96, "end": 3879.3199999999997, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.2736583658166834, "compression_ratio": 1.4973544973544974, "no_speech_prob": 1.0348497880841023e-06}, {"id": 805, "seek": 385812, "start": 3879.3199999999997, "end": 3881.6, "text": " In the first AI library, there's lots of little", "tokens": [682, 264, 700, 7318, 6405, 11, 456, 311, 3195, 295, 707], "temperature": 0.0, "avg_logprob": -0.2736583658166834, "compression_ratio": 1.4973544973544974, "no_speech_prob": 1.0348497880841023e-06}, {"id": 806, "seek": 385812, "start": 3882.3599999999997, "end": 3884.64, "text": " functions and classes to help with", "tokens": [6828, 293, 5359, 281, 854, 365], "temperature": 0.0, "avg_logprob": -0.2736583658166834, "compression_ratio": 1.4973544973544974, "no_speech_prob": 1.0348497880841023e-06}, {"id": 807, "seek": 388464, "start": 3884.64, "end": 3891.58, "text": " Most kinds of domains that you do machine learning on the NLP one of the simple things we have is texts from folders", "tokens": [4534, 3685, 295, 25514, 300, 291, 360, 3479, 2539, 322, 264, 426, 45196, 472, 295, 264, 2199, 721, 321, 362, 307, 15765, 490, 31082], "temperature": 0.0, "avg_logprob": -0.20792118419300426, "compression_ratio": 1.6806083650190113, "no_speech_prob": 3.785299895753269e-06}, {"id": 808, "seek": 388464, "start": 3891.66, "end": 3896.2, "text": " That's just going to go ahead and go through and find all of the folders in here", "tokens": [663, 311, 445, 516, 281, 352, 2286, 293, 352, 807, 293, 915, 439, 295, 264, 31082, 294, 510], "temperature": 0.0, "avg_logprob": -0.20792118419300426, "compression_ratio": 1.6806083650190113, "no_speech_prob": 3.785299895753269e-06}, {"id": 809, "seek": 388464, "start": 3896.72, "end": 3899.04, "text": " with these names and create a", "tokens": [365, 613, 5288, 293, 1884, 257], "temperature": 0.0, "avg_logprob": -0.20792118419300426, "compression_ratio": 1.6806083650190113, "no_speech_prob": 3.785299895753269e-06}, {"id": 810, "seek": 388464, "start": 3899.6, "end": 3903.8399999999997, "text": " Labeled data set and you know don't let these things", "tokens": [10137, 31689, 1412, 992, 293, 291, 458, 500, 380, 718, 613, 721], "temperature": 0.0, "avg_logprob": -0.20792118419300426, "compression_ratio": 1.6806083650190113, "no_speech_prob": 3.785299895753269e-06}, {"id": 811, "seek": 388464, "start": 3904.7999999999997, "end": 3907.68, "text": " Ever stop you from understanding what's going on behind the scenes", "tokens": [12123, 1590, 291, 490, 3701, 437, 311, 516, 322, 2261, 264, 8026], "temperature": 0.0, "avg_logprob": -0.20792118419300426, "compression_ratio": 1.6806083650190113, "no_speech_prob": 3.785299895753269e-06}, {"id": 812, "seek": 388464, "start": 3908.16, "end": 3912.8799999999997, "text": " Right we can grab its source code and as you can see it's tiny, you know, it's like five lines", "tokens": [1779, 321, 393, 4444, 1080, 4009, 3089, 293, 382, 291, 393, 536, 309, 311, 5870, 11, 291, 458, 11, 309, 311, 411, 1732, 3876], "temperature": 0.0, "avg_logprob": -0.20792118419300426, "compression_ratio": 1.6806083650190113, "no_speech_prob": 3.785299895753269e-06}, {"id": 813, "seek": 391288, "start": 3912.88, "end": 3916.92, "text": " Okay, so I don't like to write these things out in full, you know", "tokens": [1033, 11, 370, 286, 500, 380, 411, 281, 2464, 613, 721, 484, 294, 1577, 11, 291, 458], "temperature": 0.0, "avg_logprob": -0.13463323925613263, "compression_ratio": 1.873913043478261, "no_speech_prob": 1.9033778926313971e-06}, {"id": 814, "seek": 391288, "start": 3916.92, "end": 3919.92, "text": " But hide them behind at all functions so you can reuse them", "tokens": [583, 6479, 552, 2261, 412, 439, 6828, 370, 291, 393, 26225, 552], "temperature": 0.0, "avg_logprob": -0.13463323925613263, "compression_ratio": 1.873913043478261, "no_speech_prob": 1.9033778926313971e-06}, {"id": 815, "seek": 391288, "start": 3919.92, "end": 3924.32, "text": " But basically it's just going to go through each directory and then within that so I go through", "tokens": [583, 1936, 309, 311, 445, 516, 281, 352, 807, 1184, 21120, 293, 550, 1951, 300, 370, 286, 352, 807], "temperature": 0.0, "avg_logprob": -0.13463323925613263, "compression_ratio": 1.873913043478261, "no_speech_prob": 1.9033778926313971e-06}, {"id": 816, "seek": 391288, "start": 3925.44, "end": 3927.44, "text": " Yeah, go through each directory", "tokens": [865, 11, 352, 807, 1184, 21120], "temperature": 0.0, "avg_logprob": -0.13463323925613263, "compression_ratio": 1.873913043478261, "no_speech_prob": 1.9033778926313971e-06}, {"id": 817, "seek": 391288, "start": 3927.6, "end": 3929.6, "text": " And then go through each", "tokens": [400, 550, 352, 807, 1184], "temperature": 0.0, "avg_logprob": -0.13463323925613263, "compression_ratio": 1.873913043478261, "no_speech_prob": 1.9033778926313971e-06}, {"id": 818, "seek": 391288, "start": 3930.04, "end": 3932.04, "text": " file in that directory", "tokens": [3991, 294, 300, 21120], "temperature": 0.0, "avg_logprob": -0.13463323925613263, "compression_ratio": 1.873913043478261, "no_speech_prob": 1.9033778926313971e-06}, {"id": 819, "seek": 391288, "start": 3932.32, "end": 3934.32, "text": " and then stick that into", "tokens": [293, 550, 2897, 300, 666], "temperature": 0.0, "avg_logprob": -0.13463323925613263, "compression_ratio": 1.873913043478261, "no_speech_prob": 1.9033778926313971e-06}, {"id": 820, "seek": 393432, "start": 3934.32, "end": 3942.32, "text": " This array of texts and figure out what folder it's in and stick that into the array of labels. Okay, so that's", "tokens": [639, 10225, 295, 15765, 293, 2573, 484, 437, 10820, 309, 311, 294, 293, 2897, 300, 666, 264, 10225, 295, 16949, 13, 1033, 11, 370, 300, 311], "temperature": 0.0, "avg_logprob": -0.18952051798502603, "compression_ratio": 1.655367231638418, "no_speech_prob": 4.42546843260061e-06}, {"id": 821, "seek": 393432, "start": 3943.0800000000004, "end": 3944.7200000000003, "text": " how we", "tokens": [577, 321], "temperature": 0.0, "avg_logprob": -0.18952051798502603, "compression_ratio": 1.655367231638418, "no_speech_prob": 4.42546843260061e-06}, {"id": 822, "seek": 393432, "start": 3944.7200000000003, "end": 3950.82, "text": " Basically end up with something where we have an array of the reviews and an array of the labels", "tokens": [8537, 917, 493, 365, 746, 689, 321, 362, 364, 10225, 295, 264, 10229, 293, 364, 10225, 295, 264, 16949], "temperature": 0.0, "avg_logprob": -0.18952051798502603, "compression_ratio": 1.655367231638418, "no_speech_prob": 4.42546843260061e-06}, {"id": 823, "seek": 393432, "start": 3950.92, "end": 3954.6400000000003, "text": " Okay, so that's our data. So our job will be to take", "tokens": [1033, 11, 370, 300, 311, 527, 1412, 13, 407, 527, 1691, 486, 312, 281, 747], "temperature": 0.0, "avg_logprob": -0.18952051798502603, "compression_ratio": 1.655367231638418, "no_speech_prob": 4.42546843260061e-06}, {"id": 824, "seek": 393432, "start": 3955.56, "end": 3957.1600000000003, "text": " that and", "tokens": [300, 293], "temperature": 0.0, "avg_logprob": -0.18952051798502603, "compression_ratio": 1.655367231638418, "no_speech_prob": 4.42546843260061e-06}, {"id": 825, "seek": 393432, "start": 3957.1600000000003, "end": 3959.1600000000003, "text": " to predict that", "tokens": [281, 6069, 300], "temperature": 0.0, "avg_logprob": -0.18952051798502603, "compression_ratio": 1.655367231638418, "no_speech_prob": 4.42546843260061e-06}, {"id": 826, "seek": 395916, "start": 3959.16, "end": 3964.68, "text": " Okay, and the way we're going to do it is we're going to throw away", "tokens": [1033, 11, 293, 264, 636, 321, 434, 516, 281, 360, 309, 307, 321, 434, 516, 281, 3507, 1314], "temperature": 0.0, "avg_logprob": -0.11745931421007429, "compression_ratio": 1.7654320987654322, "no_speech_prob": 1.05114759207936e-06}, {"id": 827, "seek": 395916, "start": 3965.44, "end": 3972.68, "text": " Like all of the interesting stuff about language, which is the order in which the words are in right now", "tokens": [1743, 439, 295, 264, 1880, 1507, 466, 2856, 11, 597, 307, 264, 1668, 294, 597, 264, 2283, 366, 294, 558, 586], "temperature": 0.0, "avg_logprob": -0.11745931421007429, "compression_ratio": 1.7654320987654322, "no_speech_prob": 1.05114759207936e-06}, {"id": 828, "seek": 395916, "start": 3972.68, "end": 3974.68, "text": " This is very often not a good idea", "tokens": [639, 307, 588, 2049, 406, 257, 665, 1558], "temperature": 0.0, "avg_logprob": -0.11745931421007429, "compression_ratio": 1.7654320987654322, "no_speech_prob": 1.05114759207936e-06}, {"id": 829, "seek": 395916, "start": 3975.48, "end": 3979.04, "text": " But in this particular case, it's going to turn out to work like not too badly", "tokens": [583, 294, 341, 1729, 1389, 11, 309, 311, 516, 281, 1261, 484, 281, 589, 411, 406, 886, 13425], "temperature": 0.0, "avg_logprob": -0.11745931421007429, "compression_ratio": 1.7654320987654322, "no_speech_prob": 1.05114759207936e-06}, {"id": 830, "seek": 395916, "start": 3979.2, "end": 3983.98, "text": " So let me show what I mean by like throwing away the order of the words like normally the order of the words", "tokens": [407, 718, 385, 855, 437, 286, 914, 538, 411, 10238, 1314, 264, 1668, 295, 264, 2283, 411, 5646, 264, 1668, 295, 264, 2283], "temperature": 0.0, "avg_logprob": -0.11745931421007429, "compression_ratio": 1.7654320987654322, "no_speech_prob": 1.05114759207936e-06}, {"id": 831, "seek": 395916, "start": 3984.48, "end": 3986.48, "text": " Matters a lot if you've got a not", "tokens": [20285, 82, 257, 688, 498, 291, 600, 658, 257, 406], "temperature": 0.0, "avg_logprob": -0.11745931421007429, "compression_ratio": 1.7654320987654322, "no_speech_prob": 1.05114759207936e-06}, {"id": 832, "seek": 398648, "start": 3986.48, "end": 3992.88, "text": " Before something then that not refers to that thing right so but the thing is when in this case", "tokens": [4546, 746, 550, 300, 406, 14942, 281, 300, 551, 558, 370, 457, 264, 551, 307, 562, 294, 341, 1389], "temperature": 0.0, "avg_logprob": -0.15233774082635038, "compression_ratio": 1.830188679245283, "no_speech_prob": 2.0904526536469348e-06}, {"id": 833, "seek": 398648, "start": 3992.88, "end": 3998.08, "text": " We're trying to predict whether something's positive or negative if you see the word absurd appear a lot", "tokens": [492, 434, 1382, 281, 6069, 1968, 746, 311, 3353, 420, 3671, 498, 291, 536, 264, 1349, 19774, 4204, 257, 688], "temperature": 0.0, "avg_logprob": -0.15233774082635038, "compression_ratio": 1.830188679245283, "no_speech_prob": 2.0904526536469348e-06}, {"id": 834, "seek": 398648, "start": 3998.64, "end": 4002.08, "text": " Right then maybe that's a sign that this isn't very good", "tokens": [1779, 550, 1310, 300, 311, 257, 1465, 300, 341, 1943, 380, 588, 665], "temperature": 0.0, "avg_logprob": -0.15233774082635038, "compression_ratio": 1.830188679245283, "no_speech_prob": 2.0904526536469348e-06}, {"id": 835, "seek": 398648, "start": 4004.64, "end": 4011.2, "text": " So you know cryptic maybe that's a sign that it's not very good so the idea is that we're going to turn it into something called a", "tokens": [407, 291, 458, 9844, 299, 1310, 300, 311, 257, 1465, 300, 309, 311, 406, 588, 665, 370, 264, 1558, 307, 300, 321, 434, 516, 281, 1261, 309, 666, 746, 1219, 257], "temperature": 0.0, "avg_logprob": -0.15233774082635038, "compression_ratio": 1.830188679245283, "no_speech_prob": 2.0904526536469348e-06}, {"id": 836, "seek": 401120, "start": 4011.2, "end": 4016.7999999999997, "text": " Term document matrix where for each document I each review", "tokens": [19835, 4166, 8141, 689, 337, 1184, 4166, 286, 1184, 3131], "temperature": 0.0, "avg_logprob": -0.18248317005870107, "compression_ratio": 1.674757281553398, "no_speech_prob": 2.123369313267176e-06}, {"id": 837, "seek": 401120, "start": 4016.7999999999997, "end": 4022.04, "text": " We're just going to create a list of what words are in it rather than what order they're in so let me give an example", "tokens": [492, 434, 445, 516, 281, 1884, 257, 1329, 295, 437, 2283, 366, 294, 309, 2831, 813, 437, 1668, 436, 434, 294, 370, 718, 385, 976, 364, 1365], "temperature": 0.0, "avg_logprob": -0.18248317005870107, "compression_ratio": 1.674757281553398, "no_speech_prob": 2.123369313267176e-06}, {"id": 838, "seek": 401120, "start": 4023.56, "end": 4025.3599999999997, "text": " Can you see this okay?", "tokens": [1664, 291, 536, 341, 1392, 30], "temperature": 0.0, "avg_logprob": -0.18248317005870107, "compression_ratio": 1.674757281553398, "no_speech_prob": 2.123369313267176e-06}, {"id": 839, "seek": 401120, "start": 4025.3599999999997, "end": 4026.52, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.18248317005870107, "compression_ratio": 1.674757281553398, "no_speech_prob": 2.123369313267176e-06}, {"id": 840, "seek": 401120, "start": 4026.52, "end": 4028.52, "text": " So here are four", "tokens": [407, 510, 366, 1451], "temperature": 0.0, "avg_logprob": -0.18248317005870107, "compression_ratio": 1.674757281553398, "no_speech_prob": 2.123369313267176e-06}, {"id": 841, "seek": 401120, "start": 4029.16, "end": 4031.2799999999997, "text": " Movie reviews that I made up", "tokens": [28766, 10229, 300, 286, 1027, 493], "temperature": 0.0, "avg_logprob": -0.18248317005870107, "compression_ratio": 1.674757281553398, "no_speech_prob": 2.123369313267176e-06}, {"id": 842, "seek": 401120, "start": 4032.68, "end": 4038.3999999999996, "text": " This movie is good the movie is good. They're both positive this movie is bad the movie is bad", "tokens": [639, 3169, 307, 665, 264, 3169, 307, 665, 13, 814, 434, 1293, 3353, 341, 3169, 307, 1578, 264, 3169, 307, 1578], "temperature": 0.0, "avg_logprob": -0.18248317005870107, "compression_ratio": 1.674757281553398, "no_speech_prob": 2.123369313267176e-06}, {"id": 843, "seek": 403840, "start": 4038.4, "end": 4043.4, "text": " They're both negative right so I'm going to turn this into a term document matrix", "tokens": [814, 434, 1293, 3671, 558, 370, 286, 478, 516, 281, 1261, 341, 666, 257, 1433, 4166, 8141], "temperature": 0.0, "avg_logprob": -0.12834089085207148, "compression_ratio": 1.8233082706766917, "no_speech_prob": 4.1163602304550295e-07}, {"id": 844, "seek": 403840, "start": 4043.6, "end": 4049.44, "text": " So the first thing I need to do is create something called a vocabulary a vocabulary is a list of all the unique words that appear", "tokens": [407, 264, 700, 551, 286, 643, 281, 360, 307, 1884, 746, 1219, 257, 19864, 257, 19864, 307, 257, 1329, 295, 439, 264, 3845, 2283, 300, 4204], "temperature": 0.0, "avg_logprob": -0.12834089085207148, "compression_ratio": 1.8233082706766917, "no_speech_prob": 4.1163602304550295e-07}, {"id": 845, "seek": 403840, "start": 4049.76, "end": 4054.92, "text": " Okay, so here's my vocabulary this movie is good the bad. That's all the words", "tokens": [1033, 11, 370, 510, 311, 452, 19864, 341, 3169, 307, 665, 264, 1578, 13, 663, 311, 439, 264, 2283], "temperature": 0.0, "avg_logprob": -0.12834089085207148, "compression_ratio": 1.8233082706766917, "no_speech_prob": 4.1163602304550295e-07}, {"id": 846, "seek": 403840, "start": 4055.6, "end": 4060.48, "text": " Okay, and so now I'm going to take each one of my movie reviews and turn it into a", "tokens": [1033, 11, 293, 370, 586, 286, 478, 516, 281, 747, 1184, 472, 295, 452, 3169, 10229, 293, 1261, 309, 666, 257], "temperature": 0.0, "avg_logprob": -0.12834089085207148, "compression_ratio": 1.8233082706766917, "no_speech_prob": 4.1163602304550295e-07}, {"id": 847, "seek": 403840, "start": 4061.28, "end": 4067.48, "text": " Vector of which words appear and how often do they appear right and in this case none of my words appear twice", "tokens": [691, 20814, 295, 597, 2283, 4204, 293, 577, 2049, 360, 436, 4204, 558, 293, 294, 341, 1389, 6022, 295, 452, 2283, 4204, 6091], "temperature": 0.0, "avg_logprob": -0.12834089085207148, "compression_ratio": 1.8233082706766917, "no_speech_prob": 4.1163602304550295e-07}, {"id": 848, "seek": 406748, "start": 4067.48, "end": 4069.04, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.18542576884175396, "compression_ratio": 1.8864864864864865, "no_speech_prob": 8.579229984206904e-07}, {"id": 849, "seek": 406748, "start": 4069.04, "end": 4072.48, "text": " This movie is good has those four words in it", "tokens": [639, 3169, 307, 665, 575, 729, 1451, 2283, 294, 309], "temperature": 0.0, "avg_logprob": -0.18542576884175396, "compression_ratio": 1.8864864864864865, "no_speech_prob": 8.579229984206904e-07}, {"id": 850, "seek": 406748, "start": 4073.2, "end": 4075.36, "text": " Where else this movie is bad has?", "tokens": [2305, 1646, 341, 3169, 307, 1578, 575, 30], "temperature": 0.0, "avg_logprob": -0.18542576884175396, "compression_ratio": 1.8864864864864865, "no_speech_prob": 8.579229984206904e-07}, {"id": 851, "seek": 406748, "start": 4076.12, "end": 4078.12, "text": " those four words in it", "tokens": [729, 1451, 2283, 294, 309], "temperature": 0.0, "avg_logprob": -0.18542576884175396, "compression_ratio": 1.8864864864864865, "no_speech_prob": 8.579229984206904e-07}, {"id": 852, "seek": 406748, "start": 4078.4, "end": 4080.4, "text": " Okay, so this", "tokens": [1033, 11, 370, 341], "temperature": 0.0, "avg_logprob": -0.18542576884175396, "compression_ratio": 1.8864864864864865, "no_speech_prob": 8.579229984206904e-07}, {"id": 853, "seek": 406748, "start": 4081.4, "end": 4083.48, "text": " Is called a term document matrix", "tokens": [1119, 1219, 257, 1433, 4166, 8141], "temperature": 0.0, "avg_logprob": -0.18542576884175396, "compression_ratio": 1.8864864864864865, "no_speech_prob": 8.579229984206904e-07}, {"id": 854, "seek": 406748, "start": 4084.04, "end": 4089.7, "text": " Right and this representation we call a bag of words representation, right?", "tokens": [1779, 293, 341, 10290, 321, 818, 257, 3411, 295, 2283, 10290, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.18542576884175396, "compression_ratio": 1.8864864864864865, "no_speech_prob": 8.579229984206904e-07}, {"id": 855, "seek": 406748, "start": 4089.7, "end": 4096.44, "text": " So this here is a bag of words representation of the view of the review it doesn't contain the order of the text anymore", "tokens": [407, 341, 510, 307, 257, 3411, 295, 2283, 10290, 295, 264, 1910, 295, 264, 3131, 309, 1177, 380, 5304, 264, 1668, 295, 264, 2487, 3602], "temperature": 0.0, "avg_logprob": -0.18542576884175396, "compression_ratio": 1.8864864864864865, "no_speech_prob": 8.579229984206904e-07}, {"id": 856, "seek": 409644, "start": 4096.44, "end": 4101.839999999999, "text": " It's just a bag of the words what words are in it it contains bad is", "tokens": [467, 311, 445, 257, 3411, 295, 264, 2283, 437, 2283, 366, 294, 309, 309, 8306, 1578, 307], "temperature": 0.0, "avg_logprob": -0.14684445282508587, "compression_ratio": 1.8451882845188285, "no_speech_prob": 2.123373405993334e-06}, {"id": 857, "seek": 409644, "start": 4102.839999999999, "end": 4108.5199999999995, "text": " Movie this okay, so that's the first thing we're going to do is we're going to turn it into a bag of words representation", "tokens": [28766, 341, 1392, 11, 370, 300, 311, 264, 700, 551, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 1261, 309, 666, 257, 3411, 295, 2283, 10290], "temperature": 0.0, "avg_logprob": -0.14684445282508587, "compression_ratio": 1.8451882845188285, "no_speech_prob": 2.123373405993334e-06}, {"id": 858, "seek": 409644, "start": 4108.5199999999995, "end": 4115.12, "text": " And the reason that this is convenient for linear models is that this is a nice?", "tokens": [400, 264, 1778, 300, 341, 307, 10851, 337, 8213, 5245, 307, 300, 341, 307, 257, 1481, 30], "temperature": 0.0, "avg_logprob": -0.14684445282508587, "compression_ratio": 1.8451882845188285, "no_speech_prob": 2.123373405993334e-06}, {"id": 859, "seek": 409644, "start": 4116.32, "end": 4119.219999999999, "text": " Rectangular matrix that we can like do math on", "tokens": [497, 557, 656, 1040, 8141, 300, 321, 393, 411, 360, 5221, 322], "temperature": 0.0, "avg_logprob": -0.14684445282508587, "compression_ratio": 1.8451882845188285, "no_speech_prob": 2.123373405993334e-06}, {"id": 860, "seek": 409644, "start": 4119.839999999999, "end": 4124.9, "text": " Okay, and specifically we can do a logistic regression, and that's what we're going to do is we're going to get to a point", "tokens": [1033, 11, 293, 4682, 321, 393, 360, 257, 3565, 3142, 24590, 11, 293, 300, 311, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 483, 281, 257, 935], "temperature": 0.0, "avg_logprob": -0.14684445282508587, "compression_ratio": 1.8451882845188285, "no_speech_prob": 2.123373405993334e-06}, {"id": 861, "seek": 412490, "start": 4124.9, "end": 4126.9, "text": " We do a logistic regression", "tokens": [492, 360, 257, 3565, 3142, 24590], "temperature": 0.0, "avg_logprob": -0.23815202713012695, "compression_ratio": 1.4934497816593886, "no_speech_prob": 1.9637982404674403e-06}, {"id": 862, "seek": 412490, "start": 4126.92, "end": 4130.08, "text": " Before we get there though. We're going to do something else which is called naive base", "tokens": [4546, 321, 483, 456, 1673, 13, 492, 434, 516, 281, 360, 746, 1646, 597, 307, 1219, 29052, 3096], "temperature": 0.0, "avg_logprob": -0.23815202713012695, "compression_ratio": 1.4934497816593886, "no_speech_prob": 1.9637982404674403e-06}, {"id": 863, "seek": 412490, "start": 4130.719999999999, "end": 4131.799999999999, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.23815202713012695, "compression_ratio": 1.4934497816593886, "no_speech_prob": 1.9637982404674403e-06}, {"id": 864, "seek": 412490, "start": 4131.799999999999, "end": 4133.0, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.23815202713012695, "compression_ratio": 1.4934497816593886, "no_speech_prob": 1.9637982404674403e-06}, {"id": 865, "seek": 412490, "start": 4133.0, "end": 4134.679999999999, "text": " Sk learn", "tokens": [7324, 1466], "temperature": 0.0, "avg_logprob": -0.23815202713012695, "compression_ratio": 1.4934497816593886, "no_speech_prob": 1.9637982404674403e-06}, {"id": 866, "seek": 412490, "start": 4134.679999999999, "end": 4141.78, "text": " Has something which will create a term document matrix for us. It's called count vectorizer. Okay, so we'll just use it now", "tokens": [8646, 746, 597, 486, 1884, 257, 1433, 4166, 8141, 337, 505, 13, 467, 311, 1219, 1207, 8062, 6545, 13, 1033, 11, 370, 321, 603, 445, 764, 309, 586], "temperature": 0.0, "avg_logprob": -0.23815202713012695, "compression_ratio": 1.4934497816593886, "no_speech_prob": 1.9637982404674403e-06}, {"id": 867, "seek": 412490, "start": 4142.48, "end": 4144.48, "text": " in NLP", "tokens": [294, 426, 45196], "temperature": 0.0, "avg_logprob": -0.23815202713012695, "compression_ratio": 1.4934497816593886, "no_speech_prob": 1.9637982404674403e-06}, {"id": 868, "seek": 412490, "start": 4144.679999999999, "end": 4148.9, "text": " You have to turn your text into a list of words", "tokens": [509, 362, 281, 1261, 428, 2487, 666, 257, 1329, 295, 2283], "temperature": 0.0, "avg_logprob": -0.23815202713012695, "compression_ratio": 1.4934497816593886, "no_speech_prob": 1.9637982404674403e-06}, {"id": 869, "seek": 412490, "start": 4149.719999999999, "end": 4151.719999999999, "text": " And that's called tokenization", "tokens": [400, 300, 311, 1219, 14862, 2144], "temperature": 0.0, "avg_logprob": -0.23815202713012695, "compression_ratio": 1.4934497816593886, "no_speech_prob": 1.9637982404674403e-06}, {"id": 870, "seek": 415172, "start": 4151.72, "end": 4157.68, "text": " Okay, and that's actually non-trivial because like if this was actually this movie is good", "tokens": [1033, 11, 293, 300, 311, 767, 2107, 12, 83, 470, 22640, 570, 411, 498, 341, 390, 767, 341, 3169, 307, 665], "temperature": 0.0, "avg_logprob": -0.22033137463508767, "compression_ratio": 1.691542288557214, "no_speech_prob": 9.132537002187746e-07}, {"id": 871, "seek": 415172, "start": 4158.360000000001, "end": 4160.5, "text": " Dot right or if it was this", "tokens": [38753, 558, 420, 498, 309, 390, 341], "temperature": 0.0, "avg_logprob": -0.22033137463508767, "compression_ratio": 1.691542288557214, "no_speech_prob": 9.132537002187746e-07}, {"id": 872, "seek": 415172, "start": 4161.360000000001, "end": 4163.12, "text": " movie", "tokens": [3169], "temperature": 0.0, "avg_logprob": -0.22033137463508767, "compression_ratio": 1.691542288557214, "no_speech_prob": 9.132537002187746e-07}, {"id": 873, "seek": 415172, "start": 4163.12, "end": 4167.16, "text": " Is good like how do you deal with like that?", "tokens": [1119, 665, 411, 577, 360, 291, 2028, 365, 411, 300, 30], "temperature": 0.0, "avg_logprob": -0.22033137463508767, "compression_ratio": 1.691542288557214, "no_speech_prob": 9.132537002187746e-07}, {"id": 874, "seek": 415172, "start": 4168.52, "end": 4172.68, "text": " Punctuation or perhaps more interestingly what if it was this movie isn't good", "tokens": [22574, 349, 16073, 420, 4317, 544, 25873, 437, 498, 309, 390, 341, 3169, 1943, 380, 665], "temperature": 0.0, "avg_logprob": -0.22033137463508767, "compression_ratio": 1.691542288557214, "no_speech_prob": 9.132537002187746e-07}, {"id": 875, "seek": 415172, "start": 4173.92, "end": 4175.8, "text": " right, so", "tokens": [558, 11, 370], "temperature": 0.0, "avg_logprob": -0.22033137463508767, "compression_ratio": 1.691542288557214, "no_speech_prob": 9.132537002187746e-07}, {"id": 876, "seek": 415172, "start": 4175.8, "end": 4181.400000000001, "text": " How you turn a piece of text into a list of tokens is called tokenization, right?", "tokens": [1012, 291, 1261, 257, 2522, 295, 2487, 666, 257, 1329, 295, 22667, 307, 1219, 14862, 2144, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.22033137463508767, "compression_ratio": 1.691542288557214, "no_speech_prob": 9.132537002187746e-07}, {"id": 877, "seek": 418140, "start": 4181.4, "end": 4186.2, "text": " And so a good tokenizer would turn this movie isn't good", "tokens": [400, 370, 257, 665, 14862, 6545, 576, 1261, 341, 3169, 1943, 380, 665], "temperature": 0.0, "avg_logprob": -0.2123913068449899, "compression_ratio": 1.7047619047619047, "no_speech_prob": 1.4593726973544108e-06}, {"id": 878, "seek": 418140, "start": 4187.599999999999, "end": 4189.599999999999, "text": " Into this this space", "tokens": [23373, 341, 341, 1901], "temperature": 0.0, "avg_logprob": -0.2123913068449899, "compression_ratio": 1.7047619047619047, "no_speech_prob": 1.4593726973544108e-06}, {"id": 879, "seek": 418140, "start": 4189.879999999999, "end": 4196.839999999999, "text": " Quote movie space is space and good space right so you can see in this version here", "tokens": [2326, 1370, 3169, 1901, 307, 1901, 293, 665, 1901, 558, 370, 291, 393, 536, 294, 341, 3037, 510], "temperature": 0.0, "avg_logprob": -0.2123913068449899, "compression_ratio": 1.7047619047619047, "no_speech_prob": 1.4593726973544108e-06}, {"id": 880, "seek": 418140, "start": 4197.32, "end": 4201.839999999999, "text": " If I now split this on spaces every token is either a single piece of punctuation", "tokens": [759, 286, 586, 7472, 341, 322, 7673, 633, 14862, 307, 2139, 257, 2167, 2522, 295, 27006, 16073], "temperature": 0.0, "avg_logprob": -0.2123913068449899, "compression_ratio": 1.7047619047619047, "no_speech_prob": 1.4593726973544108e-06}, {"id": 881, "seek": 418140, "start": 4202.44, "end": 4209.719999999999, "text": " Or like this suffix and is considered like a word right that's kind of like how we would probably want to tokenize", "tokens": [1610, 411, 341, 3889, 970, 293, 307, 4888, 411, 257, 1349, 558, 300, 311, 733, 295, 411, 577, 321, 576, 1391, 528, 281, 14862, 1125], "temperature": 0.0, "avg_logprob": -0.2123913068449899, "compression_ratio": 1.7047619047619047, "no_speech_prob": 1.4593726973544108e-06}, {"id": 882, "seek": 420972, "start": 4209.72, "end": 4213.68, "text": " That piece of text because you wouldn't want good full stop", "tokens": [663, 2522, 295, 2487, 570, 291, 2759, 380, 528, 665, 1577, 1590], "temperature": 0.0, "avg_logprob": -0.17414158646778394, "compression_ratio": 1.6651162790697673, "no_speech_prob": 1.5534960766672157e-06}, {"id": 883, "seek": 420972, "start": 4214.2, "end": 4220.08, "text": " To be like an object right because it does there's no concept of good full stop right or", "tokens": [1407, 312, 411, 364, 2657, 558, 570, 309, 775, 456, 311, 572, 3410, 295, 665, 1577, 1590, 558, 420], "temperature": 0.0, "avg_logprob": -0.17414158646778394, "compression_ratio": 1.6651162790697673, "no_speech_prob": 1.5534960766672157e-06}, {"id": 884, "seek": 420972, "start": 4220.6, "end": 4222.6, "text": " Double quote movie is not like an object", "tokens": [16633, 6513, 3169, 307, 406, 411, 364, 2657], "temperature": 0.0, "avg_logprob": -0.17414158646778394, "compression_ratio": 1.6651162790697673, "no_speech_prob": 1.5534960766672157e-06}, {"id": 885, "seek": 420972, "start": 4223.76, "end": 4225.68, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.17414158646778394, "compression_ratio": 1.6651162790697673, "no_speech_prob": 1.5534960766672157e-06}, {"id": 886, "seek": 420972, "start": 4225.68, "end": 4227.76, "text": " Tokenization is something we hand off to a tokenizer", "tokens": [314, 8406, 2144, 307, 746, 321, 1011, 766, 281, 257, 14862, 6545], "temperature": 0.0, "avg_logprob": -0.17414158646778394, "compression_ratio": 1.6651162790697673, "no_speech_prob": 1.5534960766672157e-06}, {"id": 887, "seek": 420972, "start": 4229.12, "end": 4231.68, "text": " Fast AI has a tokenizer in it that we can use", "tokens": [15968, 7318, 575, 257, 14862, 6545, 294, 309, 300, 321, 393, 764], "temperature": 0.0, "avg_logprob": -0.17414158646778394, "compression_ratio": 1.6651162790697673, "no_speech_prob": 1.5534960766672157e-06}, {"id": 888, "seek": 423168, "start": 4231.68, "end": 4237.56, "text": " So this is how we create our term document matrix with a tokenizer", "tokens": [407, 341, 307, 577, 321, 1884, 527, 1433, 4166, 8141, 365, 257, 14862, 6545], "temperature": 0.0, "avg_logprob": -0.17471046447753907, "compression_ratio": 1.5235849056603774, "no_speech_prob": 3.0415837954933522e-06}, {"id": 889, "seek": 423168, "start": 4242.280000000001, "end": 4248.200000000001, "text": " Sklearn has a pretty standard API which is nice. I'm sure you've seen it a few times now before", "tokens": [7324, 306, 1083, 575, 257, 1238, 3832, 9362, 597, 307, 1481, 13, 286, 478, 988, 291, 600, 1612, 309, 257, 1326, 1413, 586, 949], "temperature": 0.0, "avg_logprob": -0.17471046447753907, "compression_ratio": 1.5235849056603774, "no_speech_prob": 3.0415837954933522e-06}, {"id": 890, "seek": 423168, "start": 4248.76, "end": 4253.84, "text": " So once we've built some kind of model we can kind of think of this as a model", "tokens": [407, 1564, 321, 600, 3094, 512, 733, 295, 2316, 321, 393, 733, 295, 519, 295, 341, 382, 257, 2316], "temperature": 0.0, "avg_logprob": -0.17471046447753907, "compression_ratio": 1.5235849056603774, "no_speech_prob": 3.0415837954933522e-06}, {"id": 891, "seek": 425384, "start": 4253.84, "end": 4260.82, "text": " Just ish this is just defining what it's going to do we can call fit transform to", "tokens": [1449, 307, 71, 341, 307, 445, 17827, 437, 309, 311, 516, 281, 360, 321, 393, 818, 3318, 4088, 281], "temperature": 0.0, "avg_logprob": -0.18816243339987362, "compression_ratio": 1.8097560975609757, "no_speech_prob": 2.026137053690036e-06}, {"id": 892, "seek": 425384, "start": 4261.64, "end": 4266.0, "text": " To do that right so in this case fit transform is going to create the vocabulary", "tokens": [1407, 360, 300, 558, 370, 294, 341, 1389, 3318, 4088, 307, 516, 281, 1884, 264, 19864], "temperature": 0.0, "avg_logprob": -0.18816243339987362, "compression_ratio": 1.8097560975609757, "no_speech_prob": 2.026137053690036e-06}, {"id": 893, "seek": 425384, "start": 4266.6, "end": 4269.24, "text": " Okay, and create the term document matrix", "tokens": [1033, 11, 293, 1884, 264, 1433, 4166, 8141], "temperature": 0.0, "avg_logprob": -0.18816243339987362, "compression_ratio": 1.8097560975609757, "no_speech_prob": 2.026137053690036e-06}, {"id": 894, "seek": 425384, "start": 4269.96, "end": 4271.96, "text": " based on the training set", "tokens": [2361, 322, 264, 3097, 992], "temperature": 0.0, "avg_logprob": -0.18816243339987362, "compression_ratio": 1.8097560975609757, "no_speech_prob": 2.026137053690036e-06}, {"id": 895, "seek": 425384, "start": 4273.72, "end": 4276.8, "text": " Transform is a little bit different that says use the", "tokens": [27938, 307, 257, 707, 857, 819, 300, 1619, 764, 264], "temperature": 0.0, "avg_logprob": -0.18816243339987362, "compression_ratio": 1.8097560975609757, "no_speech_prob": 2.026137053690036e-06}, {"id": 896, "seek": 425384, "start": 4277.2, "end": 4281.56, "text": " Previously fitted model which in this case means use the previously created vocabulary", "tokens": [33606, 26321, 2316, 597, 294, 341, 1389, 1355, 764, 264, 8046, 2942, 19864], "temperature": 0.0, "avg_logprob": -0.18816243339987362, "compression_ratio": 1.8097560975609757, "no_speech_prob": 2.026137053690036e-06}, {"id": 897, "seek": 428156, "start": 4281.56, "end": 4287.76, "text": " We wouldn't want the validation set and the training set to have you know the words in different orders in the matrices", "tokens": [492, 2759, 380, 528, 264, 24071, 992, 293, 264, 3097, 992, 281, 362, 291, 458, 264, 2283, 294, 819, 9470, 294, 264, 32284], "temperature": 0.0, "avg_logprob": -0.17848936040350732, "compression_ratio": 1.8059071729957805, "no_speech_prob": 2.4824673801049357e-06}, {"id": 898, "seek": 428156, "start": 4287.76, "end": 4292.160000000001, "text": " Right because then they'd like to have different meanings, so this is here saying use the same vocabulary", "tokens": [1779, 570, 550, 436, 1116, 411, 281, 362, 819, 28138, 11, 370, 341, 307, 510, 1566, 764, 264, 912, 19864], "temperature": 0.0, "avg_logprob": -0.17848936040350732, "compression_ratio": 1.8059071729957805, "no_speech_prob": 2.4824673801049357e-06}, {"id": 899, "seek": 428156, "start": 4292.68, "end": 4298.280000000001, "text": " To create a bag of words for the validation set could you pass that back, please?", "tokens": [1407, 1884, 257, 3411, 295, 2283, 337, 264, 24071, 992, 727, 291, 1320, 300, 646, 11, 1767, 30], "temperature": 0.0, "avg_logprob": -0.17848936040350732, "compression_ratio": 1.8059071729957805, "no_speech_prob": 2.4824673801049357e-06}, {"id": 900, "seek": 428156, "start": 4301.080000000001, "end": 4307.400000000001, "text": " What if the violation set has different set of words other than training yeah, that's a great question so generally most", "tokens": [708, 498, 264, 22840, 992, 575, 819, 992, 295, 2283, 661, 813, 3097, 1338, 11, 300, 311, 257, 869, 1168, 370, 5101, 881], "temperature": 0.0, "avg_logprob": -0.17848936040350732, "compression_ratio": 1.8059071729957805, "no_speech_prob": 2.4824673801049357e-06}, {"id": 901, "seek": 430740, "start": 4307.4, "end": 4312.96, "text": " Of these kind of vocab creating approaches will have a special token for unknown", "tokens": [2720, 613, 733, 295, 2329, 455, 4084, 11587, 486, 362, 257, 2121, 14862, 337, 9841], "temperature": 0.0, "avg_logprob": -0.1754681905110677, "compression_ratio": 1.628691983122363, "no_speech_prob": 1.4593712194255204e-06}, {"id": 902, "seek": 430740, "start": 4314.799999999999, "end": 4320.639999999999, "text": " Sometimes you can you'll also say like hey if a word appears less than three times call it unknown", "tokens": [4803, 291, 393, 291, 603, 611, 584, 411, 4177, 498, 257, 1349, 7038, 1570, 813, 1045, 1413, 818, 309, 9841], "temperature": 0.0, "avg_logprob": -0.1754681905110677, "compression_ratio": 1.628691983122363, "no_speech_prob": 1.4593712194255204e-06}, {"id": 903, "seek": 430740, "start": 4320.92, "end": 4325.0, "text": " But otherwise it's like yeah if you see something you haven't seen before call it unknown", "tokens": [583, 5911, 309, 311, 411, 1338, 498, 291, 536, 746, 291, 2378, 380, 1612, 949, 818, 309, 9841], "temperature": 0.0, "avg_logprob": -0.1754681905110677, "compression_ratio": 1.628691983122363, "no_speech_prob": 1.4593712194255204e-06}, {"id": 904, "seek": 430740, "start": 4325.0, "end": 4329.08, "text": " So that would just become a column in the bag of words is is unknown", "tokens": [407, 300, 576, 445, 1813, 257, 7738, 294, 264, 3411, 295, 2283, 307, 307, 9841], "temperature": 0.0, "avg_logprob": -0.1754681905110677, "compression_ratio": 1.628691983122363, "no_speech_prob": 1.4593712194255204e-06}, {"id": 905, "seek": 430740, "start": 4331.5199999999995, "end": 4335.08, "text": " Good question all right, so when we create this", "tokens": [2205, 1168, 439, 558, 11, 370, 562, 321, 1884, 341], "temperature": 0.0, "avg_logprob": -0.1754681905110677, "compression_ratio": 1.628691983122363, "no_speech_prob": 1.4593712194255204e-06}, {"id": 906, "seek": 433508, "start": 4335.08, "end": 4336.64, "text": " term", "tokens": [1433], "temperature": 0.0, "avg_logprob": -0.38639332385773356, "compression_ratio": 1.6470588235294117, "no_speech_prob": 1.9222650735173374e-05}, {"id": 907, "seek": 433508, "start": 4336.64, "end": 4341.68, "text": " Document matrix of the training set we have 25,000 rows because there are 25,000 movie reviews", "tokens": [37684, 8141, 295, 264, 3097, 992, 321, 362, 3552, 11, 1360, 13241, 570, 456, 366, 3552, 11, 1360, 3169, 10229], "temperature": 0.0, "avg_logprob": -0.38639332385773356, "compression_ratio": 1.6470588235294117, "no_speech_prob": 1.9222650735173374e-05}, {"id": 908, "seek": 433508, "start": 4342.32, "end": 4345.62, "text": " And there are 75,000 132 columns", "tokens": [400, 456, 366, 9562, 11, 1360, 3705, 17, 13766], "temperature": 0.0, "avg_logprob": -0.38639332385773356, "compression_ratio": 1.6470588235294117, "no_speech_prob": 1.9222650735173374e-05}, {"id": 909, "seek": 433508, "start": 4346.2, "end": 4350.68, "text": " What does that represent what does that mean there are seven hundred and thirty five thousand one hundred thirty two what can you pass that to?", "tokens": [708, 775, 300, 2906, 437, 775, 300, 914, 456, 366, 3407, 3262, 293, 11790, 1732, 4714, 472, 3262, 11790, 732, 437, 393, 291, 1320, 300, 281, 30], "temperature": 0.0, "avg_logprob": -0.38639332385773356, "compression_ratio": 1.6470588235294117, "no_speech_prob": 1.9222650735173374e-05}, {"id": 910, "seek": 433508, "start": 4350.68, "end": 4351.88, "text": " Devesh", "tokens": [1346, 977, 71], "temperature": 0.0, "avg_logprob": -0.38639332385773356, "compression_ratio": 1.6470588235294117, "no_speech_prob": 1.9222650735173374e-05}, {"id": 911, "seek": 433508, "start": 4351.88, "end": 4353.88, "text": " Just a moment to compare such a mesh", "tokens": [1449, 257, 1623, 281, 6794, 1270, 257, 17407], "temperature": 0.0, "avg_logprob": -0.38639332385773356, "compression_ratio": 1.6470588235294117, "no_speech_prob": 1.9222650735173374e-05}, {"id": 912, "seek": 433508, "start": 4356.2, "end": 4358.88, "text": " All vocabulary yeah go on what do you mean?", "tokens": [1057, 19864, 1338, 352, 322, 437, 360, 291, 914, 30], "temperature": 0.0, "avg_logprob": -0.38639332385773356, "compression_ratio": 1.6470588235294117, "no_speech_prob": 1.9222650735173374e-05}, {"id": 913, "seek": 435888, "start": 4358.88, "end": 4366.76, "text": " So like the the number of words a union of a number of words that the number of unique words yeah exactly good, okay?", "tokens": [407, 411, 264, 264, 1230, 295, 2283, 257, 11671, 295, 257, 1230, 295, 2283, 300, 264, 1230, 295, 3845, 2283, 1338, 2293, 665, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.24298187855924114, "compression_ratio": 1.5707317073170732, "no_speech_prob": 3.1875363220024155e-06}, {"id": 914, "seek": 435888, "start": 4367.24, "end": 4369.24, "text": " now", "tokens": [586], "temperature": 0.0, "avg_logprob": -0.24298187855924114, "compression_ratio": 1.5707317073170732, "no_speech_prob": 3.1875363220024155e-06}, {"id": 915, "seek": 435888, "start": 4369.24, "end": 4371.24, "text": " most documents", "tokens": [881, 8512], "temperature": 0.0, "avg_logprob": -0.24298187855924114, "compression_ratio": 1.5707317073170732, "no_speech_prob": 3.1875363220024155e-06}, {"id": 916, "seek": 435888, "start": 4371.24, "end": 4374.04, "text": " Don't have most of these 75,000", "tokens": [1468, 380, 362, 881, 295, 613, 9562, 11, 1360], "temperature": 0.0, "avg_logprob": -0.24298187855924114, "compression_ratio": 1.5707317073170732, "no_speech_prob": 3.1875363220024155e-06}, {"id": 917, "seek": 435888, "start": 4374.68, "end": 4378.64, "text": " Words right so we don't want to actually store that", "tokens": [32857, 558, 370, 321, 500, 380, 528, 281, 767, 3531, 300], "temperature": 0.0, "avg_logprob": -0.24298187855924114, "compression_ratio": 1.5707317073170732, "no_speech_prob": 3.1875363220024155e-06}, {"id": 918, "seek": 435888, "start": 4379.36, "end": 4383.68, "text": " As a normal array in memory because it's going to be very wasteful", "tokens": [1018, 257, 2710, 10225, 294, 4675, 570, 309, 311, 516, 281, 312, 588, 5964, 906], "temperature": 0.0, "avg_logprob": -0.24298187855924114, "compression_ratio": 1.5707317073170732, "no_speech_prob": 3.1875363220024155e-06}, {"id": 919, "seek": 435888, "start": 4384.32, "end": 4386.5, "text": " So instead we store it as a sparse", "tokens": [407, 2602, 321, 3531, 309, 382, 257, 637, 11668], "temperature": 0.0, "avg_logprob": -0.24298187855924114, "compression_ratio": 1.5707317073170732, "no_speech_prob": 3.1875363220024155e-06}, {"id": 920, "seek": 438650, "start": 4386.5, "end": 4388.18, "text": " matrix", "tokens": [8141], "temperature": 0.0, "avg_logprob": -0.34347693920135497, "compression_ratio": 1.6348314606741574, "no_speech_prob": 3.966956683143508e-06}, {"id": 921, "seek": 438650, "start": 4388.18, "end": 4392.82, "text": " Okay, and what a sparse matrix does is it just stores it as?", "tokens": [1033, 11, 293, 437, 257, 637, 11668, 8141, 775, 307, 309, 445, 9512, 309, 382, 30], "temperature": 0.0, "avg_logprob": -0.34347693920135497, "compression_ratio": 1.6348314606741574, "no_speech_prob": 3.966956683143508e-06}, {"id": 922, "seek": 438650, "start": 4394.38, "end": 4396.38, "text": " something that says", "tokens": [746, 300, 1619], "temperature": 0.0, "avg_logprob": -0.34347693920135497, "compression_ratio": 1.6348314606741574, "no_speech_prob": 3.966956683143508e-06}, {"id": 923, "seek": 438650, "start": 4396.9, "end": 4402.86, "text": " Whereabouts of the non-zeros right so it says like okay term number so document number one", "tokens": [2305, 41620, 295, 264, 2107, 12, 4527, 329, 558, 370, 309, 1619, 411, 1392, 1433, 1230, 370, 4166, 1230, 472], "temperature": 0.0, "avg_logprob": -0.34347693920135497, "compression_ratio": 1.6348314606741574, "no_speech_prob": 3.966956683143508e-06}, {"id": 924, "seek": 438650, "start": 4403.54, "end": 4405.54, "text": " word number four", "tokens": [1349, 1230, 1451], "temperature": 0.0, "avg_logprob": -0.34347693920135497, "compression_ratio": 1.6348314606741574, "no_speech_prob": 3.966956683143508e-06}, {"id": 925, "seek": 438650, "start": 4406.02, "end": 4409.1, "text": " Appears and it has four of them you know", "tokens": [41322, 685, 293, 309, 575, 1451, 295, 552, 291, 458], "temperature": 0.0, "avg_logprob": -0.34347693920135497, "compression_ratio": 1.6348314606741574, "no_speech_prob": 3.966956683143508e-06}, {"id": 926, "seek": 438650, "start": 4410.06, "end": 4412.14, "text": " document one term number", "tokens": [4166, 472, 1433, 1230], "temperature": 0.0, "avg_logprob": -0.34347693920135497, "compression_ratio": 1.6348314606741574, "no_speech_prob": 3.966956683143508e-06}, {"id": 927, "seek": 438650, "start": 4412.98, "end": 4414.98, "text": " hundred and twenty four three", "tokens": [3262, 293, 7699, 1451, 1045], "temperature": 0.0, "avg_logprob": -0.34347693920135497, "compression_ratio": 1.6348314606741574, "no_speech_prob": 3.966956683143508e-06}, {"id": 928, "seek": 441498, "start": 4414.98, "end": 4419.58, "text": " Has that that appears and it's a one right and so forth that's basically", "tokens": [8646, 300, 300, 7038, 293, 309, 311, 257, 472, 558, 293, 370, 5220, 300, 311, 1936], "temperature": 0.0, "avg_logprob": -0.17332163563481084, "compression_ratio": 1.7748091603053435, "no_speech_prob": 3.3405221984139644e-06}, {"id": 929, "seek": 441498, "start": 4420.139999999999, "end": 4425.66, "text": " How it's stored there's actually a number of different ways of storing and if you do Rachel's?", "tokens": [1012, 309, 311, 12187, 456, 311, 767, 257, 1230, 295, 819, 2098, 295, 26085, 293, 498, 291, 360, 14246, 311, 30], "temperature": 0.0, "avg_logprob": -0.17332163563481084, "compression_ratio": 1.7748091603053435, "no_speech_prob": 3.3405221984139644e-06}, {"id": 930, "seek": 441498, "start": 4426.219999999999, "end": 4431.219999999999, "text": " Computational linear algebra course you'll learn about the different types and why you choose them and how to convert and so forth", "tokens": [37804, 1478, 8213, 21989, 1164, 291, 603, 1466, 466, 264, 819, 3467, 293, 983, 291, 2826, 552, 293, 577, 281, 7620, 293, 370, 5220], "temperature": 0.0, "avg_logprob": -0.17332163563481084, "compression_ratio": 1.7748091603053435, "no_speech_prob": 3.3405221984139644e-06}, {"id": 931, "seek": 441498, "start": 4431.219999999999, "end": 4437.7, "text": " But they're all kind of something like this right and you don't really on the whole have to worry about the details", "tokens": [583, 436, 434, 439, 733, 295, 746, 411, 341, 558, 293, 291, 500, 380, 534, 322, 264, 1379, 362, 281, 3292, 466, 264, 4365], "temperature": 0.0, "avg_logprob": -0.17332163563481084, "compression_ratio": 1.7748091603053435, "no_speech_prob": 3.3405221984139644e-06}, {"id": 932, "seek": 441498, "start": 4438.54, "end": 4441.959999999999, "text": " The important thing to know is it's it's efficient", "tokens": [440, 1021, 551, 281, 458, 307, 309, 311, 309, 311, 7148], "temperature": 0.0, "avg_logprob": -0.17332163563481084, "compression_ratio": 1.7748091603053435, "no_speech_prob": 3.3405221984139644e-06}, {"id": 933, "seek": 444196, "start": 4441.96, "end": 4445.56, "text": " Okay, and so we could grab the first review", "tokens": [1033, 11, 293, 370, 321, 727, 4444, 264, 700, 3131], "temperature": 0.0, "avg_logprob": -0.15332326121713924, "compression_ratio": 1.540909090909091, "no_speech_prob": 8.851546340338245e-07}, {"id": 934, "seek": 444196, "start": 4446.08, "end": 4448.08, "text": " Right and that gives us", "tokens": [1779, 293, 300, 2709, 505], "temperature": 0.0, "avg_logprob": -0.15332326121713924, "compression_ratio": 1.540909090909091, "no_speech_prob": 8.851546340338245e-07}, {"id": 935, "seek": 444196, "start": 4448.92, "end": 4450.92, "text": " 75,000 long sparse", "tokens": [9562, 11, 1360, 938, 637, 11668], "temperature": 0.0, "avg_logprob": -0.15332326121713924, "compression_ratio": 1.540909090909091, "no_speech_prob": 8.851546340338245e-07}, {"id": 936, "seek": 444196, "start": 4451.44, "end": 4457.0, "text": " One long one row long matrix okay with 93 stored elements so in other words", "tokens": [1485, 938, 472, 5386, 938, 8141, 1392, 365, 28876, 12187, 4959, 370, 294, 661, 2283], "temperature": 0.0, "avg_logprob": -0.15332326121713924, "compression_ratio": 1.540909090909091, "no_speech_prob": 8.851546340338245e-07}, {"id": 937, "seek": 444196, "start": 4457.68, "end": 4462.84, "text": " 93 of those words are actually used in the first document, okay?", "tokens": [28876, 295, 729, 2283, 366, 767, 1143, 294, 264, 700, 4166, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.15332326121713924, "compression_ratio": 1.540909090909091, "no_speech_prob": 8.851546340338245e-07}, {"id": 938, "seek": 444196, "start": 4463.6, "end": 4469.4800000000005, "text": " We can have a look at the vocabulary by saying vectorizer dot get feature feature names that gives us the vocab", "tokens": [492, 393, 362, 257, 574, 412, 264, 19864, 538, 1566, 8062, 6545, 5893, 483, 4111, 4111, 5288, 300, 2709, 505, 264, 2329, 455], "temperature": 0.0, "avg_logprob": -0.15332326121713924, "compression_ratio": 1.540909090909091, "no_speech_prob": 8.851546340338245e-07}, {"id": 939, "seek": 446948, "start": 4469.48, "end": 4474.08, "text": " And so here's an example of a few of the elements of get feature names", "tokens": [400, 370, 510, 311, 364, 1365, 295, 257, 1326, 295, 264, 4959, 295, 483, 4111, 5288], "temperature": 0.0, "avg_logprob": -0.23482111702978084, "compression_ratio": 1.5632183908045978, "no_speech_prob": 1.5056970141813508e-06}, {"id": 940, "seek": 446948, "start": 4474.879999999999, "end": 4476.879999999999, "text": " I", "tokens": [286], "temperature": 0.0, "avg_logprob": -0.23482111702978084, "compression_ratio": 1.5632183908045978, "no_speech_prob": 1.5056970141813508e-06}, {"id": 941, "seek": 446948, "start": 4476.919999999999, "end": 4481.5599999999995, "text": " Didn't intentionally pick the one that had Ozzy, but you know that's the important words obviously", "tokens": [11151, 380, 22062, 1888, 264, 472, 300, 632, 29843, 1229, 11, 457, 291, 458, 300, 311, 264, 1021, 2283, 2745], "temperature": 0.0, "avg_logprob": -0.23482111702978084, "compression_ratio": 1.5632183908045978, "no_speech_prob": 1.5056970141813508e-06}, {"id": 942, "seek": 446948, "start": 4482.2, "end": 4484.2, "text": " I", "tokens": [286], "temperature": 0.0, "avg_logprob": -0.23482111702978084, "compression_ratio": 1.5632183908045978, "no_speech_prob": 1.5056970141813508e-06}, {"id": 943, "seek": 446948, "start": 4484.28, "end": 4488.5599999999995, "text": " Haven't used the tokenizer here. I'm just bidding on space so this isn't quite the same as what the", "tokens": [23770, 380, 1143, 264, 14862, 6545, 510, 13, 286, 478, 445, 39702, 322, 1901, 370, 341, 1943, 380, 1596, 264, 912, 382, 437, 264], "temperature": 0.0, "avg_logprob": -0.23482111702978084, "compression_ratio": 1.5632183908045978, "no_speech_prob": 1.5056970141813508e-06}, {"id": 944, "seek": 446948, "start": 4489.16, "end": 4491.4, "text": " Vectorizer did but to simplify things", "tokens": [691, 20814, 6545, 630, 457, 281, 20460, 721], "temperature": 0.0, "avg_logprob": -0.23482111702978084, "compression_ratio": 1.5632183908045978, "no_speech_prob": 1.5056970141813508e-06}, {"id": 945, "seek": 446948, "start": 4493.0, "end": 4495.4, "text": " Let's grab a set of all the lower-cased words", "tokens": [961, 311, 4444, 257, 992, 295, 439, 264, 3126, 12, 66, 1937, 2283], "temperature": 0.0, "avg_logprob": -0.23482111702978084, "compression_ratio": 1.5632183908045978, "no_speech_prob": 1.5056970141813508e-06}, {"id": 946, "seek": 449540, "start": 4495.4, "end": 4498.92, "text": " By making it a set we make them unique, so this is", "tokens": [3146, 1455, 309, 257, 992, 321, 652, 552, 3845, 11, 370, 341, 307], "temperature": 0.0, "avg_logprob": -0.13825710340477954, "compression_ratio": 1.5520361990950227, "no_speech_prob": 1.5779564819240477e-06}, {"id": 947, "seek": 449540, "start": 4499.679999999999, "end": 4504.719999999999, "text": " Roughly the list of words that would appear right and that length is 91", "tokens": [42791, 356, 264, 1329, 295, 2283, 300, 576, 4204, 558, 293, 300, 4641, 307, 31064], "temperature": 0.0, "avg_logprob": -0.13825710340477954, "compression_ratio": 1.5520361990950227, "no_speech_prob": 1.5779564819240477e-06}, {"id": 948, "seek": 449540, "start": 4505.28, "end": 4511.5599999999995, "text": " Which is pretty similar to 93 and just the difference will be that I didn't use a real tokenizer. Yeah, right", "tokens": [3013, 307, 1238, 2531, 281, 28876, 293, 445, 264, 2649, 486, 312, 300, 286, 994, 380, 764, 257, 957, 14862, 6545, 13, 865, 11, 558], "temperature": 0.0, "avg_logprob": -0.13825710340477954, "compression_ratio": 1.5520361990950227, "no_speech_prob": 1.5779564819240477e-06}, {"id": 949, "seek": 449540, "start": 4514.04, "end": 4519.599999999999, "text": " So that's basically all that's been done there. It's kind of created this unique list of words and mapped them", "tokens": [407, 300, 311, 1936, 439, 300, 311, 668, 1096, 456, 13, 467, 311, 733, 295, 2942, 341, 3845, 1329, 295, 2283, 293, 33318, 552], "temperature": 0.0, "avg_logprob": -0.13825710340477954, "compression_ratio": 1.5520361990950227, "no_speech_prob": 1.5779564819240477e-06}, {"id": 950, "seek": 451960, "start": 4519.6, "end": 4527.240000000001, "text": " We could check by calling vectorizer dot vocabulary underscore to find the idea of a particular word", "tokens": [492, 727, 1520, 538, 5141, 8062, 6545, 5893, 19864, 37556, 281, 915, 264, 1558, 295, 257, 1729, 1349], "temperature": 0.0, "avg_logprob": -0.22045140094067678, "compression_ratio": 1.6511627906976745, "no_speech_prob": 2.8573056169989286e-06}, {"id": 951, "seek": 451960, "start": 4527.240000000001, "end": 4531.76, "text": " So this is like the reverse map of this one right so this is like integer to word", "tokens": [407, 341, 307, 411, 264, 9943, 4471, 295, 341, 472, 558, 370, 341, 307, 411, 24922, 281, 1349], "temperature": 0.0, "avg_logprob": -0.22045140094067678, "compression_ratio": 1.6511627906976745, "no_speech_prob": 2.8573056169989286e-06}, {"id": 952, "seek": 451960, "start": 4532.320000000001, "end": 4538.02, "text": " Here is word to integer and so we saw absurd appear twice in the first document", "tokens": [1692, 307, 1349, 281, 24922, 293, 370, 321, 1866, 19774, 4204, 6091, 294, 264, 700, 4166], "temperature": 0.0, "avg_logprob": -0.22045140094067678, "compression_ratio": 1.6511627906976745, "no_speech_prob": 2.8573056169989286e-06}, {"id": 953, "seek": 451960, "start": 4538.04, "end": 4544.52, "text": " So let's check train term doc zero comma one two nine seven there it is is two right or else", "tokens": [407, 718, 311, 1520, 3847, 1433, 3211, 4018, 22117, 472, 732, 4949, 3407, 456, 309, 307, 307, 732, 558, 420, 1646], "temperature": 0.0, "avg_logprob": -0.22045140094067678, "compression_ratio": 1.6511627906976745, "no_speech_prob": 2.8573056169989286e-06}, {"id": 954, "seek": 454452, "start": 4544.52, "end": 4549.72, "text": " Unfortunately Ozzy didn't appear in the unnatural relationship with a pig movie", "tokens": [8590, 29843, 1229, 994, 380, 4204, 294, 264, 43470, 2480, 365, 257, 8120, 3169], "temperature": 0.0, "avg_logprob": -0.25863467945772056, "compression_ratio": 1.616504854368932, "no_speech_prob": 5.422179583547404e-06}, {"id": 955, "seek": 454452, "start": 4550.64, "end": 4555.200000000001, "text": " So zero comma five thousand is zero okay, so", "tokens": [407, 4018, 22117, 1732, 4714, 307, 4018, 1392, 11, 370], "temperature": 0.0, "avg_logprob": -0.25863467945772056, "compression_ratio": 1.616504854368932, "no_speech_prob": 5.422179583547404e-06}, {"id": 956, "seek": 454452, "start": 4555.92, "end": 4559.34, "text": " That's that's our term document matrix", "tokens": [663, 311, 300, 311, 527, 1433, 4166, 8141], "temperature": 0.0, "avg_logprob": -0.25863467945772056, "compression_ratio": 1.616504854368932, "no_speech_prob": 5.422179583547404e-06}, {"id": 957, "seek": 454452, "start": 4561.120000000001, "end": 4564.38, "text": " Yes, so does it care about the", "tokens": [1079, 11, 370, 775, 309, 1127, 466, 264], "temperature": 0.0, "avg_logprob": -0.25863467945772056, "compression_ratio": 1.616504854368932, "no_speech_prob": 5.422179583547404e-06}, {"id": 958, "seek": 454452, "start": 4564.96, "end": 4567.52, "text": " Relative relationship between the words", "tokens": [8738, 1166, 2480, 1296, 264, 2283], "temperature": 0.0, "avg_logprob": -0.25863467945772056, "compression_ratio": 1.616504854368932, "no_speech_prob": 5.422179583547404e-06}, {"id": 959, "seek": 454452, "start": 4568.56, "end": 4572.6, "text": " As in the ordering of the words no we've thrown away the orderings. That's why it's a bag of words", "tokens": [1018, 294, 264, 21739, 295, 264, 2283, 572, 321, 600, 11732, 1314, 264, 1668, 1109, 13, 663, 311, 983, 309, 311, 257, 3411, 295, 2283], "temperature": 0.0, "avg_logprob": -0.25863467945772056, "compression_ratio": 1.616504854368932, "no_speech_prob": 5.422179583547404e-06}, {"id": 960, "seek": 457260, "start": 4572.6, "end": 4574.6, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.1477532704671224, "compression_ratio": 1.575, "no_speech_prob": 2.4824698812153656e-06}, {"id": 961, "seek": 457260, "start": 4574.64, "end": 4576.64, "text": " I'm not claiming that this is like", "tokens": [286, 478, 406, 19232, 300, 341, 307, 411], "temperature": 0.0, "avg_logprob": -0.1477532704671224, "compression_ratio": 1.575, "no_speech_prob": 2.4824698812153656e-06}, {"id": 962, "seek": 457260, "start": 4577.8, "end": 4583.92, "text": " Necessarily a good idea what I will say is that like the vast majority of NLP work", "tokens": [1734, 780, 3289, 257, 665, 1558, 437, 286, 486, 584, 307, 300, 411, 264, 8369, 6286, 295, 426, 45196, 589], "temperature": 0.0, "avg_logprob": -0.1477532704671224, "compression_ratio": 1.575, "no_speech_prob": 2.4824698812153656e-06}, {"id": 963, "seek": 457260, "start": 4583.92, "end": 4589.84, "text": " That's been done over the last few decades generally uses this representation because we didn't really know much better", "tokens": [663, 311, 668, 1096, 670, 264, 1036, 1326, 7878, 5101, 4960, 341, 10290, 570, 321, 994, 380, 534, 458, 709, 1101], "temperature": 0.0, "avg_logprob": -0.1477532704671224, "compression_ratio": 1.575, "no_speech_prob": 2.4824698812153656e-06}, {"id": 964, "seek": 457260, "start": 4590.72, "end": 4595.72, "text": " Nowadays increasingly we're using recurrent neural networks instead which we'll learn about in our", "tokens": [28908, 12980, 321, 434, 1228, 18680, 1753, 18161, 9590, 2602, 597, 321, 603, 1466, 466, 294, 527], "temperature": 0.0, "avg_logprob": -0.1477532704671224, "compression_ratio": 1.575, "no_speech_prob": 2.4824698812153656e-06}, {"id": 965, "seek": 457260, "start": 4596.52, "end": 4597.88, "text": " last", "tokens": [1036], "temperature": 0.0, "avg_logprob": -0.1477532704671224, "compression_ratio": 1.575, "no_speech_prob": 2.4824698812153656e-06}, {"id": 966, "seek": 457260, "start": 4597.88, "end": 4600.08, "text": " deep learning lesson of part one", "tokens": [2452, 2539, 6898, 295, 644, 472], "temperature": 0.0, "avg_logprob": -0.1477532704671224, "compression_ratio": 1.575, "no_speech_prob": 2.4824698812153656e-06}, {"id": 967, "seek": 460008, "start": 4600.08, "end": 4606.96, "text": " But sometimes this representation works pretty well, and it's actually going to work pretty well in this case", "tokens": [583, 2171, 341, 10290, 1985, 1238, 731, 11, 293, 309, 311, 767, 516, 281, 589, 1238, 731, 294, 341, 1389], "temperature": 0.0, "avg_logprob": -0.19256867801441865, "compression_ratio": 1.5550660792951543, "no_speech_prob": 1.7330447690255824e-06}, {"id": 968, "seek": 460008, "start": 4609.16, "end": 4615.44, "text": " Okay, so and in fact it almost like back when I was at fast mail my email company a", "tokens": [1033, 11, 370, 293, 294, 1186, 309, 1920, 411, 646, 562, 286, 390, 412, 2370, 10071, 452, 3796, 2237, 257], "temperature": 0.0, "avg_logprob": -0.19256867801441865, "compression_ratio": 1.5550660792951543, "no_speech_prob": 1.7330447690255824e-06}, {"id": 969, "seek": 460008, "start": 4615.92, "end": 4620.44, "text": " Lot of the spam filtering we did used this next technique naive Bayes", "tokens": [20131, 295, 264, 24028, 30822, 321, 630, 1143, 341, 958, 6532, 29052, 7840, 279], "temperature": 0.0, "avg_logprob": -0.19256867801441865, "compression_ratio": 1.5550660792951543, "no_speech_prob": 1.7330447690255824e-06}, {"id": 970, "seek": 460008, "start": 4620.44, "end": 4625.12, "text": " Which is as a bag of words approach just kind of like you know if you're getting a lot of", "tokens": [3013, 307, 382, 257, 3411, 295, 2283, 3109, 445, 733, 295, 411, 291, 458, 498, 291, 434, 1242, 257, 688, 295], "temperature": 0.0, "avg_logprob": -0.19256867801441865, "compression_ratio": 1.5550660792951543, "no_speech_prob": 1.7330447690255824e-06}, {"id": 971, "seek": 462512, "start": 4625.12, "end": 4629.76, "text": " Email containing the word Viagra, and it's always been a spam", "tokens": [49482, 19273, 264, 1349, 6626, 559, 424, 11, 293, 309, 311, 1009, 668, 257, 24028], "temperature": 0.0, "avg_logprob": -0.15296529840540002, "compression_ratio": 1.7346153846153847, "no_speech_prob": 2.156805521735805e-06}, {"id": 972, "seek": 462512, "start": 4629.76, "end": 4633.48, "text": " And you never get email from your friends talking about Viagra", "tokens": [400, 291, 1128, 483, 3796, 490, 428, 1855, 1417, 466, 6626, 559, 424], "temperature": 0.0, "avg_logprob": -0.15296529840540002, "compression_ratio": 1.7346153846153847, "no_speech_prob": 2.156805521735805e-06}, {"id": 973, "seek": 462512, "start": 4633.5199999999995, "end": 4639.88, "text": " Then it's very likely something that says Viagra regardless of the detail of the language is probably from a spammer", "tokens": [1396, 309, 311, 588, 3700, 746, 300, 1619, 6626, 559, 424, 10060, 295, 264, 2607, 295, 264, 2856, 307, 1391, 490, 257, 24028, 936], "temperature": 0.0, "avg_logprob": -0.15296529840540002, "compression_ratio": 1.7346153846153847, "no_speech_prob": 2.156805521735805e-06}, {"id": 974, "seek": 462512, "start": 4640.2, "end": 4648.3, "text": " All right, so that's the basic theory about like classification using a term document matrix, okay, so let's talk about naive Bayes", "tokens": [1057, 558, 11, 370, 300, 311, 264, 3875, 5261, 466, 411, 21538, 1228, 257, 1433, 4166, 8141, 11, 1392, 11, 370, 718, 311, 751, 466, 29052, 7840, 279], "temperature": 0.0, "avg_logprob": -0.15296529840540002, "compression_ratio": 1.7346153846153847, "no_speech_prob": 2.156805521735805e-06}, {"id": 975, "seek": 464830, "start": 4648.3, "end": 4654.22, "text": " And here's the basic idea we're going to start with our term document matrix", "tokens": [400, 510, 311, 264, 3875, 1558, 321, 434, 516, 281, 722, 365, 527, 1433, 4166, 8141], "temperature": 0.0, "avg_logprob": -0.21612877594797233, "compression_ratio": 1.6647398843930636, "no_speech_prob": 1.873870360213914e-06}, {"id": 976, "seek": 464830, "start": 4654.900000000001, "end": 4656.900000000001, "text": " right and", "tokens": [558, 293], "temperature": 0.0, "avg_logprob": -0.21612877594797233, "compression_ratio": 1.6647398843930636, "no_speech_prob": 1.873870360213914e-06}, {"id": 977, "seek": 464830, "start": 4657.18, "end": 4661.66, "text": " These first two is our corpus of positive reviews", "tokens": [1981, 700, 732, 307, 527, 1181, 31624, 295, 3353, 10229], "temperature": 0.0, "avg_logprob": -0.21612877594797233, "compression_ratio": 1.6647398843930636, "no_speech_prob": 1.873870360213914e-06}, {"id": 978, "seek": 464830, "start": 4662.46, "end": 4668.26, "text": " These next two is our corpus of negative reviews, and so here's our whole corpus of all reviews", "tokens": [1981, 958, 732, 307, 527, 1181, 31624, 295, 3671, 10229, 11, 293, 370, 510, 311, 527, 1379, 1181, 31624, 295, 439, 10229], "temperature": 0.0, "avg_logprob": -0.21612877594797233, "compression_ratio": 1.6647398843930636, "no_speech_prob": 1.873870360213914e-06}, {"id": 979, "seek": 464830, "start": 4669.06, "end": 4672.14, "text": " right so what I could do is now to create a", "tokens": [558, 370, 437, 286, 727, 360, 307, 586, 281, 1884, 257], "temperature": 0.0, "avg_logprob": -0.21612877594797233, "compression_ratio": 1.6647398843930636, "no_speech_prob": 1.873870360213914e-06}, {"id": 980, "seek": 464830, "start": 4673.74, "end": 4675.74, "text": " probability", "tokens": [8482], "temperature": 0.0, "avg_logprob": -0.21612877594797233, "compression_ratio": 1.6647398843930636, "no_speech_prob": 1.873870360213914e-06}, {"id": 981, "seek": 467574, "start": 4675.74, "end": 4682.139999999999, "text": " I'm going to call these as we tend to call these more generically features rather than words right this is a feature movie is a", "tokens": [286, 478, 516, 281, 818, 613, 382, 321, 3928, 281, 818, 613, 544, 1337, 984, 4122, 2831, 813, 2283, 558, 341, 307, 257, 4111, 3169, 307, 257], "temperature": 0.0, "avg_logprob": -0.19739274111661045, "compression_ratio": 1.7777777777777777, "no_speech_prob": 2.1907753762206994e-06}, {"id": 982, "seek": 467574, "start": 4682.139999999999, "end": 4684.139999999999, "text": " Feature is as a feature right?", "tokens": [3697, 1503, 307, 382, 257, 4111, 558, 30], "temperature": 0.0, "avg_logprob": -0.19739274111661045, "compression_ratio": 1.7777777777777777, "no_speech_prob": 2.1907753762206994e-06}, {"id": 983, "seek": 467574, "start": 4684.62, "end": 4688.66, "text": " So it's kind of more now like machine learning language a column is a feature", "tokens": [407, 309, 311, 733, 295, 544, 586, 411, 3479, 2539, 2856, 257, 7738, 307, 257, 4111], "temperature": 0.0, "avg_logprob": -0.19739274111661045, "compression_ratio": 1.7777777777777777, "no_speech_prob": 2.1907753762206994e-06}, {"id": 984, "seek": 467574, "start": 4688.66, "end": 4695.139999999999, "text": " We'll call those we often call those F in naive base, so we can basically say the probability", "tokens": [492, 603, 818, 729, 321, 2049, 818, 729, 479, 294, 29052, 3096, 11, 370, 321, 393, 1936, 584, 264, 8482], "temperature": 0.0, "avg_logprob": -0.19739274111661045, "compression_ratio": 1.7777777777777777, "no_speech_prob": 2.1907753762206994e-06}, {"id": 985, "seek": 467574, "start": 4695.7, "end": 4697.86, "text": " That you would see the word this", "tokens": [663, 291, 576, 536, 264, 1349, 341], "temperature": 0.0, "avg_logprob": -0.19739274111661045, "compression_ratio": 1.7777777777777777, "no_speech_prob": 2.1907753762206994e-06}, {"id": 986, "seek": 469786, "start": 4697.86, "end": 4705.42, "text": " Given that the class is one given that it's a positive review is just the average of", "tokens": [18600, 300, 264, 1508, 307, 472, 2212, 300, 309, 311, 257, 3353, 3131, 307, 445, 264, 4274, 295], "temperature": 0.0, "avg_logprob": -0.21208328075623245, "compression_ratio": 1.6517412935323383, "no_speech_prob": 7.453759849340713e-07}, {"id": 987, "seek": 469786, "start": 4705.82, "end": 4709.259999999999, "text": " How often do you see this in the positive reviews?", "tokens": [1012, 2049, 360, 291, 536, 341, 294, 264, 3353, 10229, 30], "temperature": 0.0, "avg_logprob": -0.21208328075623245, "compression_ratio": 1.6517412935323383, "no_speech_prob": 7.453759849340713e-07}, {"id": 988, "seek": 469786, "start": 4710.42, "end": 4711.66, "text": " right", "tokens": [558], "temperature": 0.0, "avg_logprob": -0.21208328075623245, "compression_ratio": 1.6517412935323383, "no_speech_prob": 7.453759849340713e-07}, {"id": 989, "seek": 469786, "start": 4711.66, "end": 4713.66, "text": " Now we've got to be a bit careful though", "tokens": [823, 321, 600, 658, 281, 312, 257, 857, 5026, 1673], "temperature": 0.0, "avg_logprob": -0.21208328075623245, "compression_ratio": 1.6517412935323383, "no_speech_prob": 7.453759849340713e-07}, {"id": 990, "seek": 469786, "start": 4714.139999999999, "end": 4716.139999999999, "text": " because", "tokens": [570], "temperature": 0.0, "avg_logprob": -0.21208328075623245, "compression_ratio": 1.6517412935323383, "no_speech_prob": 7.453759849340713e-07}, {"id": 991, "seek": 469786, "start": 4716.139999999999, "end": 4722.099999999999, "text": " If you never ever see a particular word in a particular class", "tokens": [759, 291, 1128, 1562, 536, 257, 1729, 1349, 294, 257, 1729, 1508], "temperature": 0.0, "avg_logprob": -0.21208328075623245, "compression_ratio": 1.6517412935323383, "no_speech_prob": 7.453759849340713e-07}, {"id": 992, "seek": 469786, "start": 4722.339999999999, "end": 4727.74, "text": " Right so if I've never received an email from a friend that said Viagra, right?", "tokens": [1779, 370, 498, 286, 600, 1128, 4613, 364, 3796, 490, 257, 1277, 300, 848, 6626, 559, 424, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.21208328075623245, "compression_ratio": 1.6517412935323383, "no_speech_prob": 7.453759849340713e-07}, {"id": 993, "seek": 472774, "start": 4727.74, "end": 4733.639999999999, "text": " That doesn't actually mean the probability of us of a friend sending me sending me an email about Viagra is zero", "tokens": [663, 1177, 380, 767, 914, 264, 8482, 295, 505, 295, 257, 1277, 7750, 385, 7750, 385, 364, 3796, 466, 6626, 559, 424, 307, 4018], "temperature": 0.0, "avg_logprob": -0.19250094506048387, "compression_ratio": 1.669683257918552, "no_speech_prob": 4.222788447805215e-06}, {"id": 994, "seek": 472774, "start": 4733.7, "end": 4736.3, "text": " It's not really zero right I", "tokens": [467, 311, 406, 534, 4018, 558, 286], "temperature": 0.0, "avg_logprob": -0.19250094506048387, "compression_ratio": 1.669683257918552, "no_speech_prob": 4.222788447805215e-06}, {"id": 995, "seek": 472774, "start": 4737.58, "end": 4741.46, "text": " Hope I don't get an email you know from Terrence tomorrow saying like", "tokens": [6483, 286, 500, 380, 483, 364, 3796, 291, 458, 490, 6564, 10760, 4153, 1566, 411], "temperature": 0.0, "avg_logprob": -0.19250094506048387, "compression_ratio": 1.669683257918552, "no_speech_prob": 4.222788447805215e-06}, {"id": 996, "seek": 472774, "start": 4742.139999999999, "end": 4748.66, "text": " Jeremy you probably could use this you know advertisement for Viagra, but you know it could happen and you know", "tokens": [17809, 291, 1391, 727, 764, 341, 291, 458, 31370, 337, 6626, 559, 424, 11, 457, 291, 458, 309, 727, 1051, 293, 291, 458], "temperature": 0.0, "avg_logprob": -0.19250094506048387, "compression_ratio": 1.669683257918552, "no_speech_prob": 4.222788447805215e-06}, {"id": 997, "seek": 472774, "start": 4750.38, "end": 4752.38, "text": " You know I'm sure it'd be in my best interest", "tokens": [509, 458, 286, 478, 988, 309, 1116, 312, 294, 452, 1151, 1179], "temperature": 0.0, "avg_logprob": -0.19250094506048387, "compression_ratio": 1.669683257918552, "no_speech_prob": 4.222788447805215e-06}, {"id": 998, "seek": 475238, "start": 4752.38, "end": 4760.1, "text": " Yeah, so so what we do is we say actually what we've seen so far is not the full sample of everything that could happen", "tokens": [865, 11, 370, 370, 437, 321, 360, 307, 321, 584, 767, 437, 321, 600, 1612, 370, 1400, 307, 406, 264, 1577, 6889, 295, 1203, 300, 727, 1051], "temperature": 0.0, "avg_logprob": -0.17475221707270697, "compression_ratio": 1.7341772151898733, "no_speech_prob": 4.1811810547187633e-07}, {"id": 999, "seek": 475238, "start": 4760.38, "end": 4762.62, "text": " It's like a sample of what's happened so far", "tokens": [467, 311, 411, 257, 6889, 295, 437, 311, 2011, 370, 1400], "temperature": 0.0, "avg_logprob": -0.17475221707270697, "compression_ratio": 1.7341772151898733, "no_speech_prob": 4.1811810547187633e-07}, {"id": 1000, "seek": 475238, "start": 4762.74, "end": 4771.1, "text": " So let's assume that the next email you get actually does mention Viagra and every other possible word right so basically", "tokens": [407, 718, 311, 6552, 300, 264, 958, 3796, 291, 483, 767, 775, 2152, 6626, 559, 424, 293, 633, 661, 1944, 1349, 558, 370, 1936], "temperature": 0.0, "avg_logprob": -0.17475221707270697, "compression_ratio": 1.7341772151898733, "no_speech_prob": 4.1811810547187633e-07}, {"id": 1001, "seek": 475238, "start": 4771.26, "end": 4773.66, "text": " We're going to add a row of", "tokens": [492, 434, 516, 281, 909, 257, 5386, 295], "temperature": 0.0, "avg_logprob": -0.17475221707270697, "compression_ratio": 1.7341772151898733, "no_speech_prob": 4.1811810547187633e-07}, {"id": 1002, "seek": 475238, "start": 4774.26, "end": 4775.46, "text": " ones", "tokens": [2306], "temperature": 0.0, "avg_logprob": -0.17475221707270697, "compression_ratio": 1.7341772151898733, "no_speech_prob": 4.1811810547187633e-07}, {"id": 1003, "seek": 475238, "start": 4775.46, "end": 4780.18, "text": " Okay, so that's like the email that contains every possible word so that way nothing's ever", "tokens": [1033, 11, 370, 300, 311, 411, 264, 3796, 300, 8306, 633, 1944, 1349, 370, 300, 636, 1825, 311, 1562], "temperature": 0.0, "avg_logprob": -0.17475221707270697, "compression_ratio": 1.7341772151898733, "no_speech_prob": 4.1811810547187633e-07}, {"id": 1004, "seek": 478018, "start": 4780.18, "end": 4782.18, "text": " infinitely unlikely", "tokens": [36227, 17518], "temperature": 0.0, "avg_logprob": -0.2771529843730311, "compression_ratio": 1.5389610389610389, "no_speech_prob": 3.8669597302032344e-07}, {"id": 1005, "seek": 478018, "start": 4782.5, "end": 4784.5, "text": " Okay, so I take the average of", "tokens": [1033, 11, 370, 286, 747, 264, 4274, 295], "temperature": 0.0, "avg_logprob": -0.2771529843730311, "compression_ratio": 1.5389610389610389, "no_speech_prob": 3.8669597302032344e-07}, {"id": 1006, "seek": 478018, "start": 4786.5, "end": 4788.34, "text": " All of the", "tokens": [1057, 295, 264], "temperature": 0.0, "avg_logprob": -0.2771529843730311, "compression_ratio": 1.5389610389610389, "no_speech_prob": 3.8669597302032344e-07}, {"id": 1007, "seek": 478018, "start": 4788.34, "end": 4796.02, "text": " Times that this appears in my positive corpus plus the ones okay, so that's like the", "tokens": [11366, 300, 341, 7038, 294, 452, 3353, 1181, 31624, 1804, 264, 2306, 1392, 11, 370, 300, 311, 411, 264], "temperature": 0.0, "avg_logprob": -0.2771529843730311, "compression_ratio": 1.5389610389610389, "no_speech_prob": 3.8669597302032344e-07}, {"id": 1008, "seek": 478018, "start": 4797.3, "end": 4799.3, "text": " the probability that", "tokens": [264, 8482, 300], "temperature": 0.0, "avg_logprob": -0.2771529843730311, "compression_ratio": 1.5389610389610389, "no_speech_prob": 3.8669597302032344e-07}, {"id": 1009, "seek": 478018, "start": 4801.3, "end": 4806.860000000001, "text": " Feature equals this appears in a document given that class equals one", "tokens": [3697, 1503, 6915, 341, 7038, 294, 257, 4166, 2212, 300, 1508, 6915, 472], "temperature": 0.0, "avg_logprob": -0.2771529843730311, "compression_ratio": 1.5389610389610389, "no_speech_prob": 3.8669597302032344e-07}, {"id": 1010, "seek": 480686, "start": 4806.86, "end": 4810.299999999999, "text": " And so not surprisingly here's the same thing", "tokens": [400, 370, 406, 17600, 510, 311, 264, 912, 551], "temperature": 0.0, "avg_logprob": -0.18708794911702473, "compression_ratio": 1.645631067961165, "no_speech_prob": 1.8448174614604795e-06}, {"id": 1011, "seek": 480686, "start": 4810.94, "end": 4818.0599999999995, "text": " For probability that this feature this appears given classical zero right same calculation except for the zero", "tokens": [1171, 8482, 300, 341, 4111, 341, 7038, 2212, 13735, 4018, 558, 912, 17108, 3993, 337, 264, 4018], "temperature": 0.0, "avg_logprob": -0.18708794911702473, "compression_ratio": 1.645631067961165, "no_speech_prob": 1.8448174614604795e-06}, {"id": 1012, "seek": 480686, "start": 4818.42, "end": 4823.0199999999995, "text": " Rows and obviously these are the same because this appears", "tokens": [497, 1509, 293, 2745, 613, 366, 264, 912, 570, 341, 7038], "temperature": 0.0, "avg_logprob": -0.18708794911702473, "compression_ratio": 1.645631067961165, "no_speech_prob": 1.8448174614604795e-06}, {"id": 1013, "seek": 480686, "start": 4824.66, "end": 4829.74, "text": " Twice in the positives sorry once in the positives and once in the negatives, okay?", "tokens": [46964, 294, 264, 35127, 2597, 1564, 294, 264, 35127, 293, 1564, 294, 264, 40019, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.18708794911702473, "compression_ratio": 1.645631067961165, "no_speech_prob": 1.8448174614604795e-06}, {"id": 1014, "seek": 482974, "start": 4829.74, "end": 4835.54, "text": " Let's just put this back to what it was all", "tokens": [961, 311, 445, 829, 341, 646, 281, 437, 309, 390, 439], "temperature": 0.0, "avg_logprob": -0.2906426323784722, "compression_ratio": 1.3875968992248062, "no_speech_prob": 1.5779562545503723e-06}, {"id": 1015, "seek": 482974, "start": 4840.38, "end": 4842.38, "text": " Right", "tokens": [1779], "temperature": 0.0, "avg_logprob": -0.2906426323784722, "compression_ratio": 1.3875968992248062, "no_speech_prob": 1.5779562545503723e-06}, {"id": 1016, "seek": 482974, "start": 4843.0599999999995, "end": 4845.0599999999995, "text": " So we can do that for every feature", "tokens": [407, 321, 393, 360, 300, 337, 633, 4111], "temperature": 0.0, "avg_logprob": -0.2906426323784722, "compression_ratio": 1.3875968992248062, "no_speech_prob": 1.5779562545503723e-06}, {"id": 1017, "seek": 482974, "start": 4846.26, "end": 4848.26, "text": " for every class", "tokens": [337, 633, 1508], "temperature": 0.0, "avg_logprob": -0.2906426323784722, "compression_ratio": 1.3875968992248062, "no_speech_prob": 1.5779562545503723e-06}, {"id": 1018, "seek": 482974, "start": 4848.62, "end": 4853.86, "text": " Right so our trick now is to basically use", "tokens": [1779, 370, 527, 4282, 586, 307, 281, 1936, 764], "temperature": 0.0, "avg_logprob": -0.2906426323784722, "compression_ratio": 1.3875968992248062, "no_speech_prob": 1.5779562545503723e-06}, {"id": 1019, "seek": 485386, "start": 4853.86, "end": 4858.94, "text": " a base rule to kind of fill this in so", "tokens": [257, 3096, 4978, 281, 733, 295, 2836, 341, 294, 370], "temperature": 0.0, "avg_logprob": -0.25100873121574746, "compression_ratio": 1.624203821656051, "no_speech_prob": 3.726625664057792e-06}, {"id": 1020, "seek": 485386, "start": 4861.58, "end": 4864.9, "text": " What we want is the probability that", "tokens": [708, 321, 528, 307, 264, 8482, 300], "temperature": 0.0, "avg_logprob": -0.25100873121574746, "compression_ratio": 1.624203821656051, "no_speech_prob": 3.726625664057792e-06}, {"id": 1021, "seek": 485386, "start": 4867.66, "end": 4874.42, "text": " Given that I've got this particular document, so somebody sent me this particular email, or I have this particular IMDb review", "tokens": [18600, 300, 286, 600, 658, 341, 1729, 4166, 11, 370, 2618, 2279, 385, 341, 1729, 3796, 11, 420, 286, 362, 341, 1729, 21463, 35, 65, 3131], "temperature": 0.0, "avg_logprob": -0.25100873121574746, "compression_ratio": 1.624203821656051, "no_speech_prob": 3.726625664057792e-06}, {"id": 1022, "seek": 485386, "start": 4875.219999999999, "end": 4878.66, "text": " What's the probability that its class is?", "tokens": [708, 311, 264, 8482, 300, 1080, 1508, 307, 30], "temperature": 0.0, "avg_logprob": -0.25100873121574746, "compression_ratio": 1.624203821656051, "no_speech_prob": 3.726625664057792e-06}, {"id": 1023, "seek": 485386, "start": 4879.46, "end": 4881.219999999999, "text": " equal to I", "tokens": [2681, 281, 286], "temperature": 0.0, "avg_logprob": -0.25100873121574746, "compression_ratio": 1.624203821656051, "no_speech_prob": 3.726625664057792e-06}, {"id": 1024, "seek": 488122, "start": 4881.22, "end": 4887.18, "text": " Don't know positive right so for this particular movie review. What's the probability that its class is positive?", "tokens": [1468, 380, 458, 3353, 558, 370, 337, 341, 1729, 3169, 3131, 13, 708, 311, 264, 8482, 300, 1080, 1508, 307, 3353, 30], "temperature": 0.0, "avg_logprob": -0.19563012374074837, "compression_ratio": 1.9631901840490797, "no_speech_prob": 3.7853135381737957e-06}, {"id": 1025, "seek": 488122, "start": 4887.66, "end": 4891.4800000000005, "text": " Right and so we can say well. That's equal to the probability", "tokens": [1779, 293, 370, 321, 393, 584, 731, 13, 663, 311, 2681, 281, 264, 8482], "temperature": 0.0, "avg_logprob": -0.19563012374074837, "compression_ratio": 1.9631901840490797, "no_speech_prob": 3.7853135381737957e-06}, {"id": 1026, "seek": 488122, "start": 4893.26, "end": 4895.26, "text": " That we got this particular", "tokens": [663, 321, 658, 341, 1729], "temperature": 0.0, "avg_logprob": -0.19563012374074837, "compression_ratio": 1.9631901840490797, "no_speech_prob": 3.7853135381737957e-06}, {"id": 1027, "seek": 488122, "start": 4896.34, "end": 4898.34, "text": " movie review", "tokens": [3169, 3131], "temperature": 0.0, "avg_logprob": -0.19563012374074837, "compression_ratio": 1.9631901840490797, "no_speech_prob": 3.7853135381737957e-06}, {"id": 1028, "seek": 488122, "start": 4898.62, "end": 4900.62, "text": " Given that its class is positive", "tokens": [18600, 300, 1080, 1508, 307, 3353], "temperature": 0.0, "avg_logprob": -0.19563012374074837, "compression_ratio": 1.9631901840490797, "no_speech_prob": 3.7853135381737957e-06}, {"id": 1029, "seek": 488122, "start": 4902.46, "end": 4906.9800000000005, "text": " Multiplied by the probability that any movie reviews class is positive", "tokens": [29238, 564, 1091, 538, 264, 8482, 300, 604, 3169, 10229, 1508, 307, 3353], "temperature": 0.0, "avg_logprob": -0.19563012374074837, "compression_ratio": 1.9631901840490797, "no_speech_prob": 3.7853135381737957e-06}, {"id": 1030, "seek": 490698, "start": 4906.98, "end": 4911.959999999999, "text": " divided by the probability of getting this particular movie review", "tokens": [6666, 538, 264, 8482, 295, 1242, 341, 1729, 3169, 3131], "temperature": 0.0, "avg_logprob": -0.27228598367600215, "compression_ratio": 1.6919431279620853, "no_speech_prob": 8.851559982758772e-07}, {"id": 1031, "seek": 490698, "start": 4913.299999999999, "end": 4917.0199999999995, "text": " All right, that's just basis rule okay, and so we can calculate", "tokens": [1057, 558, 11, 300, 311, 445, 5143, 4978, 1392, 11, 293, 370, 321, 393, 8873], "temperature": 0.0, "avg_logprob": -0.27228598367600215, "compression_ratio": 1.6919431279620853, "no_speech_prob": 8.851559982758772e-07}, {"id": 1032, "seek": 490698, "start": 4918.419999999999, "end": 4922.719999999999, "text": " All of those things, but actually what we really want to know is", "tokens": [1057, 295, 729, 721, 11, 457, 767, 437, 321, 534, 528, 281, 458, 307], "temperature": 0.0, "avg_logprob": -0.27228598367600215, "compression_ratio": 1.6919431279620853, "no_speech_prob": 8.851559982758772e-07}, {"id": 1033, "seek": 490698, "start": 4923.459999999999, "end": 4930.54, "text": " Is it more likely that this is class zero or class one right so what if we actually took?", "tokens": [1119, 309, 544, 3700, 300, 341, 307, 1508, 4018, 420, 1508, 472, 558, 370, 437, 498, 321, 767, 1890, 30], "temperature": 0.0, "avg_logprob": -0.27228598367600215, "compression_ratio": 1.6919431279620853, "no_speech_prob": 8.851559982758772e-07}, {"id": 1034, "seek": 493054, "start": 4930.54, "end": 4936.14, "text": " Probability that's class one and divided by probability that it's class zero", "tokens": [8736, 2310, 300, 311, 1508, 472, 293, 6666, 538, 8482, 300, 309, 311, 1508, 4018], "temperature": 0.0, "avg_logprob": -0.20037198353962726, "compression_ratio": 1.8742857142857143, "no_speech_prob": 4.5921322566755407e-07}, {"id": 1035, "seek": 493054, "start": 4937.46, "end": 4943.34, "text": " What if we did that right and so then we could say like okay if this number is bigger than one", "tokens": [708, 498, 321, 630, 300, 558, 293, 370, 550, 321, 727, 584, 411, 1392, 498, 341, 1230, 307, 3801, 813, 472], "temperature": 0.0, "avg_logprob": -0.20037198353962726, "compression_ratio": 1.8742857142857143, "no_speech_prob": 4.5921322566755407e-07}, {"id": 1036, "seek": 493054, "start": 4943.54, "end": 4948.3, "text": " Then it's more likely to be class one if it's smaller than one it's more likely to be class zero", "tokens": [1396, 309, 311, 544, 3700, 281, 312, 1508, 472, 498, 309, 311, 4356, 813, 472, 309, 311, 544, 3700, 281, 312, 1508, 4018], "temperature": 0.0, "avg_logprob": -0.20037198353962726, "compression_ratio": 1.8742857142857143, "no_speech_prob": 4.5921322566755407e-07}, {"id": 1037, "seek": 493054, "start": 4948.58, "end": 4951.5, "text": " Right so in that case we could just divide", "tokens": [1779, 370, 294, 300, 1389, 321, 727, 445, 9845], "temperature": 0.0, "avg_logprob": -0.20037198353962726, "compression_ratio": 1.8742857142857143, "no_speech_prob": 4.5921322566755407e-07}, {"id": 1038, "seek": 493054, "start": 4953.3, "end": 4955.3, "text": " This whole thing", "tokens": [639, 1379, 551], "temperature": 0.0, "avg_logprob": -0.20037198353962726, "compression_ratio": 1.8742857142857143, "no_speech_prob": 4.5921322566755407e-07}, {"id": 1039, "seek": 495530, "start": 4955.3, "end": 4961.74, "text": " Right by the same version for class zero right which is the same as multiplying it by the reciprocal and", "tokens": [1779, 538, 264, 912, 3037, 337, 1508, 4018, 558, 597, 307, 264, 912, 382, 30955, 309, 538, 264, 46948, 293], "temperature": 0.0, "avg_logprob": -0.18310288323296442, "compression_ratio": 1.7880184331797235, "no_speech_prob": 2.3687941848038463e-06}, {"id": 1040, "seek": 495530, "start": 4961.9800000000005, "end": 4965.7, "text": " So the nice thing is now that's going to put a probability D on top here", "tokens": [407, 264, 1481, 551, 307, 586, 300, 311, 516, 281, 829, 257, 8482, 413, 322, 1192, 510], "temperature": 0.0, "avg_logprob": -0.18310288323296442, "compression_ratio": 1.7880184331797235, "no_speech_prob": 2.3687941848038463e-06}, {"id": 1041, "seek": 495530, "start": 4965.900000000001, "end": 4973.22, "text": " Which we can get rid of right and a probability of getting the data given class zero down here", "tokens": [3013, 321, 393, 483, 3973, 295, 558, 293, 257, 8482, 295, 1242, 264, 1412, 2212, 1508, 4018, 760, 510], "temperature": 0.0, "avg_logprob": -0.18310288323296442, "compression_ratio": 1.7880184331797235, "no_speech_prob": 2.3687941848038463e-06}, {"id": 1042, "seek": 495530, "start": 4973.34, "end": 4975.62, "text": " And the probability of getting class", "tokens": [400, 264, 8482, 295, 1242, 1508], "temperature": 0.0, "avg_logprob": -0.18310288323296442, "compression_ratio": 1.7880184331797235, "no_speech_prob": 2.3687941848038463e-06}, {"id": 1043, "seek": 495530, "start": 4976.3, "end": 4982.58, "text": " Zero here right and so if we basically what that means is we want to calculate", "tokens": [17182, 510, 558, 293, 370, 498, 321, 1936, 437, 300, 1355, 307, 321, 528, 281, 8873], "temperature": 0.0, "avg_logprob": -0.18310288323296442, "compression_ratio": 1.7880184331797235, "no_speech_prob": 2.3687941848038463e-06}, {"id": 1044, "seek": 498258, "start": 4982.58, "end": 4987.98, "text": " The probability that we would get this particular document given that the class is one", "tokens": [440, 8482, 300, 321, 576, 483, 341, 1729, 4166, 2212, 300, 264, 1508, 307, 472], "temperature": 0.0, "avg_logprob": -0.20843155776398092, "compression_ratio": 2.364864864864865, "no_speech_prob": 2.2732719173745863e-07}, {"id": 1045, "seek": 498258, "start": 4988.78, "end": 4991.0599999999995, "text": " Times the probability that the class is one", "tokens": [11366, 264, 8482, 300, 264, 1508, 307, 472], "temperature": 0.0, "avg_logprob": -0.20843155776398092, "compression_ratio": 2.364864864864865, "no_speech_prob": 2.2732719173745863e-07}, {"id": 1046, "seek": 498258, "start": 4991.94, "end": 4996.3, "text": " Divided by the probability of getting this particular document given the class is two zero", "tokens": [413, 1843, 292, 538, 264, 8482, 295, 1242, 341, 1729, 4166, 2212, 264, 1508, 307, 732, 4018], "temperature": 0.0, "avg_logprob": -0.20843155776398092, "compression_ratio": 2.364864864864865, "no_speech_prob": 2.2732719173745863e-07}, {"id": 1047, "seek": 498258, "start": 4997.0199999999995, "end": 4999.1, "text": " Times the probability that the class is zero", "tokens": [11366, 264, 8482, 300, 264, 1508, 307, 4018], "temperature": 0.0, "avg_logprob": -0.20843155776398092, "compression_ratio": 2.364864864864865, "no_speech_prob": 2.2732719173745863e-07}, {"id": 1048, "seek": 498258, "start": 5000.0199999999995, "end": 5002.62, "text": " So the probability that the class is one is", "tokens": [407, 264, 8482, 300, 264, 1508, 307, 472, 307], "temperature": 0.0, "avg_logprob": -0.20843155776398092, "compression_ratio": 2.364864864864865, "no_speech_prob": 2.2732719173745863e-07}, {"id": 1049, "seek": 498258, "start": 5003.94, "end": 5006.42, "text": " Just equal to the average of the labels", "tokens": [1449, 2681, 281, 264, 4274, 295, 264, 16949], "temperature": 0.0, "avg_logprob": -0.20843155776398092, "compression_ratio": 2.364864864864865, "no_speech_prob": 2.2732719173745863e-07}, {"id": 1050, "seek": 500642, "start": 5006.42, "end": 5012.66, "text": " Right probability that the class is zero is just one minus that right so", "tokens": [1779, 8482, 300, 264, 1508, 307, 4018, 307, 445, 472, 3175, 300, 558, 370], "temperature": 0.0, "avg_logprob": -0.18772378112330582, "compression_ratio": 1.6022099447513811, "no_speech_prob": 5.043470423515828e-07}, {"id": 1051, "seek": 500642, "start": 5016.7, "end": 5021.02, "text": " So there are those two numbers right I've got an equal amount of both so it's both point five", "tokens": [407, 456, 366, 729, 732, 3547, 558, 286, 600, 658, 364, 2681, 2372, 295, 1293, 370, 309, 311, 1293, 935, 1732], "temperature": 0.0, "avg_logprob": -0.18772378112330582, "compression_ratio": 1.6022099447513811, "no_speech_prob": 5.043470423515828e-07}, {"id": 1052, "seek": 500642, "start": 5023.82, "end": 5031.86, "text": " What is the probability of getting this document given that the class is one can anybody tell me how I would calculate that", "tokens": [708, 307, 264, 8482, 295, 1242, 341, 4166, 2212, 300, 264, 1508, 307, 472, 393, 4472, 980, 385, 577, 286, 576, 8873, 300], "temperature": 0.0, "avg_logprob": -0.18772378112330582, "compression_ratio": 1.6022099447513811, "no_speech_prob": 5.043470423515828e-07}, {"id": 1053, "seek": 503186, "start": 5031.86, "end": 5036.179999999999, "text": " Can somebody pass that that please", "tokens": [1664, 2618, 1320, 300, 300, 1767], "temperature": 0.0, "avg_logprob": -0.21095623970031738, "compression_ratio": 1.5514018691588785, "no_speech_prob": 2.4824616957630496e-06}, {"id": 1054, "seek": 503186, "start": 5042.66, "end": 5048.54, "text": " Look at all the documents which have class equal to one aha and one divided by that will give you", "tokens": [2053, 412, 439, 264, 8512, 597, 362, 1508, 2681, 281, 472, 47340, 293, 472, 6666, 538, 300, 486, 976, 291], "temperature": 0.0, "avg_logprob": -0.21095623970031738, "compression_ratio": 1.5514018691588785, "no_speech_prob": 2.4824616957630496e-06}, {"id": 1055, "seek": 503186, "start": 5049.219999999999, "end": 5054.7, "text": " So remember, it's though. It's going to be for a particular document so for example. We'd be saying like what's the probability that", "tokens": [407, 1604, 11, 309, 311, 1673, 13, 467, 311, 516, 281, 312, 337, 257, 1729, 4166, 370, 337, 1365, 13, 492, 1116, 312, 1566, 411, 437, 311, 264, 8482, 300], "temperature": 0.0, "avg_logprob": -0.21095623970031738, "compression_ratio": 1.5514018691588785, "no_speech_prob": 2.4824616957630496e-06}, {"id": 1056, "seek": 503186, "start": 5055.82, "end": 5060.2, "text": " This review is positive right so what so you're on the right track", "tokens": [639, 3131, 307, 3353, 558, 370, 437, 370, 291, 434, 322, 264, 558, 2837], "temperature": 0.0, "avg_logprob": -0.21095623970031738, "compression_ratio": 1.5514018691588785, "no_speech_prob": 2.4824616957630496e-06}, {"id": 1057, "seek": 506020, "start": 5060.2, "end": 5063.3, "text": " But what we have to have to do is we're going to have to say let's just look at", "tokens": [583, 437, 321, 362, 281, 362, 281, 360, 307, 321, 434, 516, 281, 362, 281, 584, 718, 311, 445, 574, 412], "temperature": 0.0, "avg_logprob": -0.23793191176194411, "compression_ratio": 1.7109826589595376, "no_speech_prob": 1.1911059800695512e-06}, {"id": 1058, "seek": 506020, "start": 5064.34, "end": 5066.34, "text": " the words it has and", "tokens": [264, 2283, 309, 575, 293], "temperature": 0.0, "avg_logprob": -0.23793191176194411, "compression_ratio": 1.7109826589595376, "no_speech_prob": 1.1911059800695512e-06}, {"id": 1059, "seek": 506020, "start": 5066.94, "end": 5069.22, "text": " Then multiply the probabilities together", "tokens": [1396, 12972, 264, 33783, 1214], "temperature": 0.0, "avg_logprob": -0.23793191176194411, "compression_ratio": 1.7109826589595376, "no_speech_prob": 1.1911059800695512e-06}, {"id": 1060, "seek": 506020, "start": 5071.34, "end": 5078.62, "text": " For class equals one right so the probability that a class one review has", "tokens": [1171, 1508, 6915, 472, 558, 370, 264, 8482, 300, 257, 1508, 472, 3131, 575], "temperature": 0.0, "avg_logprob": -0.23793191176194411, "compression_ratio": 1.7109826589595376, "no_speech_prob": 1.1911059800695512e-06}, {"id": 1061, "seek": 506020, "start": 5079.26, "end": 5081.26, "text": " this is", "tokens": [341, 307], "temperature": 0.0, "avg_logprob": -0.23793191176194411, "compression_ratio": 1.7109826589595376, "no_speech_prob": 1.1911059800695512e-06}, {"id": 1062, "seek": 506020, "start": 5081.5, "end": 5087.96, "text": " Two-thirds the probability it has movie is one is is one and good is one", "tokens": [4453, 12, 38507, 264, 8482, 309, 575, 3169, 307, 472, 307, 307, 472, 293, 665, 307, 472], "temperature": 0.0, "avg_logprob": -0.23793191176194411, "compression_ratio": 1.7109826589595376, "no_speech_prob": 1.1911059800695512e-06}, {"id": 1063, "seek": 508796, "start": 5087.96, "end": 5091.88, "text": " So the probability it has all of them is all of those multiplied together", "tokens": [407, 264, 8482, 309, 575, 439, 295, 552, 307, 439, 295, 729, 17207, 1214], "temperature": 0.0, "avg_logprob": -0.2628515705917821, "compression_ratio": 1.5202312138728324, "no_speech_prob": 1.5294094737328123e-06}, {"id": 1064, "seek": 508796, "start": 5093.24, "end": 5098.12, "text": " Kinda and the kinder Tyler why is it not really can you pass that to Tyler?", "tokens": [35553, 293, 264, 733, 260, 16869, 983, 307, 309, 406, 534, 393, 291, 1320, 300, 281, 16869, 30], "temperature": 0.0, "avg_logprob": -0.2628515705917821, "compression_ratio": 1.5202312138728324, "no_speech_prob": 1.5294094737328123e-06}, {"id": 1065, "seek": 508796, "start": 5102.72, "end": 5104.72, "text": " So glad you look horrified and skeptical", "tokens": [407, 5404, 291, 574, 17582, 2587, 293, 28601], "temperature": 0.0, "avg_logprob": -0.2628515705917821, "compression_ratio": 1.5202312138728324, "no_speech_prob": 1.5294094737328123e-06}, {"id": 1066, "seek": 508796, "start": 5105.84, "end": 5108.6, "text": " Word choice is not independent. Thank you", "tokens": [8725, 3922, 307, 406, 6695, 13, 1044, 291], "temperature": 0.0, "avg_logprob": -0.2628515705917821, "compression_ratio": 1.5202312138728324, "no_speech_prob": 1.5294094737328123e-06}, {"id": 1067, "seek": 508796, "start": 5110.0, "end": 5112.76, "text": " So nobody can call Tyler naive", "tokens": [407, 5079, 393, 818, 16869, 29052], "temperature": 0.0, "avg_logprob": -0.2628515705917821, "compression_ratio": 1.5202312138728324, "no_speech_prob": 1.5294094737328123e-06}, {"id": 1068, "seek": 511276, "start": 5112.76, "end": 5120.4400000000005, "text": " Because the reason this is naive Bayes is because this is what happens if you take Bayes's theorems in a naive way", "tokens": [1436, 264, 1778, 341, 307, 29052, 7840, 279, 307, 570, 341, 307, 437, 2314, 498, 291, 747, 7840, 279, 311, 10299, 2592, 294, 257, 29052, 636], "temperature": 0.0, "avg_logprob": -0.18488463988670936, "compression_ratio": 1.9353448275862069, "no_speech_prob": 1.7330463606413105e-06}, {"id": 1069, "seek": 511276, "start": 5120.4400000000005, "end": 5127.12, "text": " And Tyler is not naive anything bad right so naive Bayes says let's assume that if you have", "tokens": [400, 16869, 307, 406, 29052, 1340, 1578, 558, 370, 29052, 7840, 279, 1619, 718, 311, 6552, 300, 498, 291, 362], "temperature": 0.0, "avg_logprob": -0.18488463988670936, "compression_ratio": 1.9353448275862069, "no_speech_prob": 1.7330463606413105e-06}, {"id": 1070, "seek": 511276, "start": 5127.360000000001, "end": 5129.52, "text": " This movie is bloody stupid", "tokens": [639, 3169, 307, 18938, 6631], "temperature": 0.0, "avg_logprob": -0.18488463988670936, "compression_ratio": 1.9353448275862069, "no_speech_prob": 1.7330463606413105e-06}, {"id": 1071, "seek": 511276, "start": 5129.52, "end": 5136.92, "text": " I hate it but the probability of hate is independent of the probability of bloody is it independent of the probability of stupid right?", "tokens": [286, 4700, 309, 457, 264, 8482, 295, 4700, 307, 6695, 295, 264, 8482, 295, 18938, 307, 309, 6695, 295, 264, 8482, 295, 6631, 558, 30], "temperature": 0.0, "avg_logprob": -0.18488463988670936, "compression_ratio": 1.9353448275862069, "no_speech_prob": 1.7330463606413105e-06}, {"id": 1072, "seek": 511276, "start": 5136.92, "end": 5141.96, "text": " Which is definitely not true right and so naive Bayes ain't actually very good", "tokens": [3013, 307, 2138, 406, 2074, 558, 293, 370, 29052, 7840, 279, 7862, 380, 767, 588, 665], "temperature": 0.0, "avg_logprob": -0.18488463988670936, "compression_ratio": 1.9353448275862069, "no_speech_prob": 1.7330463606413105e-06}, {"id": 1073, "seek": 514196, "start": 5141.96, "end": 5146.52, "text": " But I'm kind of teaching it to you because it's going to turn out to be a convenient", "tokens": [583, 286, 478, 733, 295, 4571, 309, 281, 291, 570, 309, 311, 516, 281, 1261, 484, 281, 312, 257, 10851], "temperature": 0.0, "avg_logprob": -0.18771068028041293, "compression_ratio": 1.6982758620689655, "no_speech_prob": 2.7693929496308556e-06}, {"id": 1074, "seek": 514196, "start": 5147.24, "end": 5149.4800000000005, "text": " Piece for something we're about to learn later", "tokens": [42868, 337, 746, 321, 434, 466, 281, 1466, 1780], "temperature": 0.0, "avg_logprob": -0.18771068028041293, "compression_ratio": 1.6982758620689655, "no_speech_prob": 2.7693929496308556e-06}, {"id": 1075, "seek": 514196, "start": 5151.12, "end": 5157.36, "text": " It's okay right. I mean it's it's it's I would never I would never choose it like I don't think it's better than any other", "tokens": [467, 311, 1392, 558, 13, 286, 914, 309, 311, 309, 311, 309, 311, 286, 576, 1128, 286, 576, 1128, 2826, 309, 411, 286, 500, 380, 519, 309, 311, 1101, 813, 604, 661], "temperature": 0.0, "avg_logprob": -0.18771068028041293, "compression_ratio": 1.6982758620689655, "no_speech_prob": 2.7693929496308556e-06}, {"id": 1076, "seek": 514196, "start": 5157.4800000000005, "end": 5159.4800000000005, "text": " Technique that's equally fast and equally easy", "tokens": [8337, 1925, 300, 311, 12309, 2370, 293, 12309, 1858], "temperature": 0.0, "avg_logprob": -0.18771068028041293, "compression_ratio": 1.6982758620689655, "no_speech_prob": 2.7693929496308556e-06}, {"id": 1077, "seek": 514196, "start": 5160.76, "end": 5165.86, "text": " But you know it's the thing you can do and it's certainly going to be a useful foundation", "tokens": [583, 291, 458, 309, 311, 264, 551, 291, 393, 360, 293, 309, 311, 3297, 516, 281, 312, 257, 4420, 7030], "temperature": 0.0, "avg_logprob": -0.18771068028041293, "compression_ratio": 1.6982758620689655, "no_speech_prob": 2.7693929496308556e-06}, {"id": 1078, "seek": 514196, "start": 5166.72, "end": 5168.04, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.18771068028041293, "compression_ratio": 1.6982758620689655, "no_speech_prob": 2.7693929496308556e-06}, {"id": 1079, "seek": 516804, "start": 5168.04, "end": 5174.12, "text": " So here is our calculation right of the probability that this document is", "tokens": [407, 510, 307, 527, 17108, 558, 295, 264, 8482, 300, 341, 4166, 307], "temperature": 0.0, "avg_logprob": -0.19509631111508324, "compression_ratio": 1.743718592964824, "no_speech_prob": 3.237718829041114e-06}, {"id": 1080, "seek": 516804, "start": 5175.36, "end": 5179.0, "text": " That we get this particular document assuming. It's a positive review", "tokens": [663, 321, 483, 341, 1729, 4166, 11926, 13, 467, 311, 257, 3353, 3131], "temperature": 0.0, "avg_logprob": -0.19509631111508324, "compression_ratio": 1.743718592964824, "no_speech_prob": 3.237718829041114e-06}, {"id": 1081, "seek": 516804, "start": 5179.32, "end": 5185.3, "text": " Here's the probability given. It's a negative and here's the ratio and this ratio is above one", "tokens": [1692, 311, 264, 8482, 2212, 13, 467, 311, 257, 3671, 293, 510, 311, 264, 8509, 293, 341, 8509, 307, 3673, 472], "temperature": 0.0, "avg_logprob": -0.19509631111508324, "compression_ratio": 1.743718592964824, "no_speech_prob": 3.237718829041114e-06}, {"id": 1082, "seek": 516804, "start": 5185.4, "end": 5191.66, "text": " So we're going to say I think that this is probably a positive review, okay, so that's the excel version", "tokens": [407, 321, 434, 516, 281, 584, 286, 519, 300, 341, 307, 1391, 257, 3353, 3131, 11, 1392, 11, 370, 300, 311, 264, 24015, 3037], "temperature": 0.0, "avg_logprob": -0.19509631111508324, "compression_ratio": 1.743718592964824, "no_speech_prob": 3.237718829041114e-06}, {"id": 1083, "seek": 516804, "start": 5192.48, "end": 5194.08, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.19509631111508324, "compression_ratio": 1.743718592964824, "no_speech_prob": 3.237718829041114e-06}, {"id": 1084, "seek": 519408, "start": 5194.08, "end": 5200.28, "text": " So you can tell that I let your net touch this because it's got latech in it. We've got actual math so", "tokens": [407, 291, 393, 980, 300, 286, 718, 428, 2533, 2557, 341, 570, 309, 311, 658, 3469, 339, 294, 309, 13, 492, 600, 658, 3539, 5221, 370], "temperature": 0.0, "avg_logprob": -0.2543419253441595, "compression_ratio": 1.638095238095238, "no_speech_prob": 3.1381218832393643e-06}, {"id": 1085, "seek": 519408, "start": 5201.24, "end": 5203.64, "text": " So here is the here is the same thing", "tokens": [407, 510, 307, 264, 510, 307, 264, 912, 551], "temperature": 0.0, "avg_logprob": -0.2543419253441595, "compression_ratio": 1.638095238095238, "no_speech_prob": 3.1381218832393643e-06}, {"id": 1086, "seek": 519408, "start": 5204.44, "end": 5206.44, "text": " the log count ratio for each", "tokens": [264, 3565, 1207, 8509, 337, 1184], "temperature": 0.0, "avg_logprob": -0.2543419253441595, "compression_ratio": 1.638095238095238, "no_speech_prob": 3.1381218832393643e-06}, {"id": 1087, "seek": 519408, "start": 5207.04, "end": 5209.04, "text": " feature F each word F", "tokens": [4111, 479, 1184, 1349, 479], "temperature": 0.0, "avg_logprob": -0.2543419253441595, "compression_ratio": 1.638095238095238, "no_speech_prob": 3.1381218832393643e-06}, {"id": 1088, "seek": 519408, "start": 5210.0, "end": 5212.0, "text": " and so here it is", "tokens": [293, 370, 510, 309, 307], "temperature": 0.0, "avg_logprob": -0.2543419253441595, "compression_ratio": 1.638095238095238, "no_speech_prob": 3.1381218832393643e-06}, {"id": 1089, "seek": 519408, "start": 5213.16, "end": 5218.5599999999995, "text": " Written out as Python okay, so our independent variable is our term document matrix", "tokens": [10159, 2987, 484, 382, 15329, 1392, 11, 370, 527, 6695, 7006, 307, 527, 1433, 4166, 8141], "temperature": 0.0, "avg_logprob": -0.2543419253441595, "compression_ratio": 1.638095238095238, "no_speech_prob": 3.1381218832393643e-06}, {"id": 1090, "seek": 519408, "start": 5219.12, "end": 5221.5, "text": " Our dependent variable is just the labels of the Y", "tokens": [2621, 12334, 7006, 307, 445, 264, 16949, 295, 264, 398], "temperature": 0.0, "avg_logprob": -0.2543419253441595, "compression_ratio": 1.638095238095238, "no_speech_prob": 3.1381218832393643e-06}, {"id": 1091, "seek": 522150, "start": 5221.5, "end": 5223.5, "text": " So using numpy", "tokens": [407, 1228, 1031, 8200], "temperature": 0.0, "avg_logprob": -0.18241236286778603, "compression_ratio": 1.6108597285067874, "no_speech_prob": 3.611966576499981e-06}, {"id": 1092, "seek": 522150, "start": 5224.18, "end": 5226.18, "text": " This is going to grab the rows", "tokens": [639, 307, 516, 281, 4444, 264, 13241], "temperature": 0.0, "avg_logprob": -0.18241236286778603, "compression_ratio": 1.6108597285067874, "no_speech_prob": 3.611966576499981e-06}, {"id": 1093, "seek": 522150, "start": 5226.66, "end": 5228.66, "text": " Where the dependent variable is one?", "tokens": [2305, 264, 12334, 7006, 307, 472, 30], "temperature": 0.0, "avg_logprob": -0.18241236286778603, "compression_ratio": 1.6108597285067874, "no_speech_prob": 3.611966576499981e-06}, {"id": 1094, "seek": 522150, "start": 5229.7, "end": 5236.78, "text": " Okay, and so then we can sum them over the rows to get the total word count", "tokens": [1033, 11, 293, 370, 550, 321, 393, 2408, 552, 670, 264, 13241, 281, 483, 264, 3217, 1349, 1207], "temperature": 0.0, "avg_logprob": -0.18241236286778603, "compression_ratio": 1.6108597285067874, "no_speech_prob": 3.611966576499981e-06}, {"id": 1095, "seek": 522150, "start": 5237.46, "end": 5244.58, "text": " For that feature across all the documents right plus one right because that's the email Terrence", "tokens": [1171, 300, 4111, 2108, 439, 264, 8512, 558, 1804, 472, 558, 570, 300, 311, 264, 3796, 6564, 10760], "temperature": 0.0, "avg_logprob": -0.18241236286778603, "compression_ratio": 1.6108597285067874, "no_speech_prob": 3.611966576499981e-06}, {"id": 1096, "seek": 522150, "start": 5244.58, "end": 5249.3, "text": " It's totally going to send me something about Viagra today. I can tell that's that's that yeah, okay", "tokens": [467, 311, 3879, 516, 281, 2845, 385, 746, 466, 6626, 559, 424, 965, 13, 286, 393, 980, 300, 311, 300, 311, 300, 1338, 11, 1392], "temperature": 0.0, "avg_logprob": -0.18241236286778603, "compression_ratio": 1.6108597285067874, "no_speech_prob": 3.611966576499981e-06}, {"id": 1097, "seek": 524930, "start": 5249.3, "end": 5256.02, "text": " So I'll do the same thing for the negative reviews right and then of course it's nicer to take the log", "tokens": [407, 286, 603, 360, 264, 912, 551, 337, 264, 3671, 10229, 558, 293, 550, 295, 1164, 309, 311, 22842, 281, 747, 264, 3565], "temperature": 0.0, "avg_logprob": -0.18086497658177425, "compression_ratio": 1.7828054298642535, "no_speech_prob": 4.029437604913255e-06}, {"id": 1098, "seek": 524930, "start": 5256.58, "end": 5261.7, "text": " Right because if we take the log then we can add things together rather than multiply them together", "tokens": [1779, 570, 498, 321, 747, 264, 3565, 550, 321, 393, 909, 721, 1214, 2831, 813, 12972, 552, 1214], "temperature": 0.0, "avg_logprob": -0.18086497658177425, "compression_ratio": 1.7828054298642535, "no_speech_prob": 4.029437604913255e-06}, {"id": 1099, "seek": 524930, "start": 5261.860000000001, "end": 5264.3, "text": " And once you like multiply enough of these things together", "tokens": [400, 1564, 291, 411, 12972, 1547, 295, 613, 721, 1214], "temperature": 0.0, "avg_logprob": -0.18086497658177425, "compression_ratio": 1.7828054298642535, "no_speech_prob": 4.029437604913255e-06}, {"id": 1100, "seek": 524930, "start": 5264.3, "end": 5269.68, "text": " It's going to get kind of so close to zero that you'll probably run out of plotting point right so we take the log", "tokens": [467, 311, 516, 281, 483, 733, 295, 370, 1998, 281, 4018, 300, 291, 603, 1391, 1190, 484, 295, 41178, 935, 558, 370, 321, 747, 264, 3565], "temperature": 0.0, "avg_logprob": -0.18086497658177425, "compression_ratio": 1.7828054298642535, "no_speech_prob": 4.029437604913255e-06}, {"id": 1101, "seek": 524930, "start": 5270.820000000001, "end": 5272.820000000001, "text": " of the ratios", "tokens": [295, 264, 32435], "temperature": 0.0, "avg_logprob": -0.18086497658177425, "compression_ratio": 1.7828054298642535, "no_speech_prob": 4.029437604913255e-06}, {"id": 1102, "seek": 524930, "start": 5273.42, "end": 5274.54, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.18086497658177425, "compression_ratio": 1.7828054298642535, "no_speech_prob": 4.029437604913255e-06}, {"id": 1103, "seek": 527454, "start": 5274.54, "end": 5281.78, "text": " Then we can as I say we then multiply that or in log we subtract that from the sorry add that to the", "tokens": [1396, 321, 393, 382, 286, 584, 321, 550, 12972, 300, 420, 294, 3565, 321, 16390, 300, 490, 264, 2597, 909, 300, 281, 264], "temperature": 0.0, "avg_logprob": -0.24716836978227663, "compression_ratio": 1.6983240223463687, "no_speech_prob": 2.4824691990943393e-06}, {"id": 1104, "seek": 527454, "start": 5282.54, "end": 5284.54, "text": " ratio of the class", "tokens": [8509, 295, 264, 1508], "temperature": 0.0, "avg_logprob": -0.24716836978227663, "compression_ratio": 1.6983240223463687, "no_speech_prob": 2.4824691990943393e-06}, {"id": 1105, "seek": 527454, "start": 5284.82, "end": 5286.5, "text": " the whole class", "tokens": [264, 1379, 1508], "temperature": 0.0, "avg_logprob": -0.24716836978227663, "compression_ratio": 1.6983240223463687, "no_speech_prob": 2.4824691990943393e-06}, {"id": 1106, "seek": 527454, "start": 5286.5, "end": 5288.5, "text": " probabilities alright", "tokens": [33783, 5845], "temperature": 0.0, "avg_logprob": -0.24716836978227663, "compression_ratio": 1.6983240223463687, "no_speech_prob": 2.4824691990943393e-06}, {"id": 1107, "seek": 527454, "start": 5288.86, "end": 5291.82, "text": " So in order to say for each document", "tokens": [407, 294, 1668, 281, 584, 337, 1184, 4166], "temperature": 0.0, "avg_logprob": -0.24716836978227663, "compression_ratio": 1.6983240223463687, "no_speech_prob": 2.4824691990943393e-06}, {"id": 1108, "seek": 527454, "start": 5293.22, "end": 5294.58, "text": " multiply the", "tokens": [12972, 264], "temperature": 0.0, "avg_logprob": -0.24716836978227663, "compression_ratio": 1.6983240223463687, "no_speech_prob": 2.4824691990943393e-06}, {"id": 1109, "seek": 527454, "start": 5294.58, "end": 5302.42, "text": " Bayes probabilities by the accounts we can just use matrix multiply okay, and then to add on the", "tokens": [7840, 279, 33783, 538, 264, 9402, 321, 393, 445, 764, 8141, 12972, 1392, 11, 293, 550, 281, 909, 322, 264], "temperature": 0.0, "avg_logprob": -0.24716836978227663, "compression_ratio": 1.6983240223463687, "no_speech_prob": 2.4824691990943393e-06}, {"id": 1110, "seek": 530242, "start": 5302.42, "end": 5303.74, "text": " the", "tokens": [264], "temperature": 0.0, "avg_logprob": -0.19259336952851197, "compression_ratio": 1.6216216216216217, "no_speech_prob": 3.5559730804379797e-06}, {"id": 1111, "seek": 530242, "start": 5303.74, "end": 5307.7, "text": " Log of the class ratios we can just use plus B", "tokens": [10824, 295, 264, 1508, 32435, 321, 393, 445, 764, 1804, 363], "temperature": 0.0, "avg_logprob": -0.19259336952851197, "compression_ratio": 1.6216216216216217, "no_speech_prob": 3.5559730804379797e-06}, {"id": 1112, "seek": 530242, "start": 5307.7, "end": 5310.7, "text": " And so we end up with something that looks a lot like our", "tokens": [400, 370, 321, 917, 493, 365, 746, 300, 1542, 257, 688, 411, 527], "temperature": 0.0, "avg_logprob": -0.19259336952851197, "compression_ratio": 1.6216216216216217, "no_speech_prob": 3.5559730804379797e-06}, {"id": 1113, "seek": 530242, "start": 5311.5, "end": 5318.3, "text": " Logistic regression right, but we're not learning anything right not in kind of a SGD point of view", "tokens": [10824, 3142, 24590, 558, 11, 457, 321, 434, 406, 2539, 1340, 558, 406, 294, 733, 295, 257, 34520, 35, 935, 295, 1910], "temperature": 0.0, "avg_logprob": -0.19259336952851197, "compression_ratio": 1.6216216216216217, "no_speech_prob": 3.5559730804379797e-06}, {"id": 1114, "seek": 530242, "start": 5318.3, "end": 5322.24, "text": " We're just we're calculating it using this theoretical model okay", "tokens": [492, 434, 445, 321, 434, 28258, 309, 1228, 341, 20864, 2316, 1392], "temperature": 0.0, "avg_logprob": -0.19259336952851197, "compression_ratio": 1.6216216216216217, "no_speech_prob": 3.5559730804379797e-06}, {"id": 1115, "seek": 530242, "start": 5322.24, "end": 5326.42, "text": " And so as I said we can then compare that as to whether it's bigger or smaller than zero", "tokens": [400, 370, 382, 286, 848, 321, 393, 550, 6794, 300, 382, 281, 1968, 309, 311, 3801, 420, 4356, 813, 4018], "temperature": 0.0, "avg_logprob": -0.19259336952851197, "compression_ratio": 1.6216216216216217, "no_speech_prob": 3.5559730804379797e-06}, {"id": 1116, "seek": 530242, "start": 5326.7, "end": 5330.06, "text": " Not one anymore because we're now in log space right and", "tokens": [1726, 472, 3602, 570, 321, 434, 586, 294, 3565, 1901, 558, 293], "temperature": 0.0, "avg_logprob": -0.19259336952851197, "compression_ratio": 1.6216216216216217, "no_speech_prob": 3.5559730804379797e-06}, {"id": 1117, "seek": 533006, "start": 5330.06, "end": 5335.38, "text": " And then we can compare that to the mean and we say okay. That's 80% accurate 81% accurate", "tokens": [400, 550, 321, 393, 6794, 300, 281, 264, 914, 293, 321, 584, 1392, 13, 663, 311, 4688, 4, 8559, 30827, 4, 8559], "temperature": 0.0, "avg_logprob": -0.18478573200314544, "compression_ratio": 1.5201793721973094, "no_speech_prob": 1.4367490166478092e-06}, {"id": 1118, "seek": 533006, "start": 5336.02, "end": 5338.02, "text": " Right so naive Bayes", "tokens": [1779, 370, 29052, 7840, 279], "temperature": 0.0, "avg_logprob": -0.18478573200314544, "compression_ratio": 1.5201793721973094, "no_speech_prob": 1.4367490166478092e-06}, {"id": 1119, "seek": 533006, "start": 5338.22, "end": 5342.620000000001, "text": " You know is not is not nothing it gave us something okay?", "tokens": [509, 458, 307, 406, 307, 406, 1825, 309, 2729, 505, 746, 1392, 30], "temperature": 0.0, "avg_logprob": -0.18478573200314544, "compression_ratio": 1.5201793721973094, "no_speech_prob": 1.4367490166478092e-06}, {"id": 1120, "seek": 533006, "start": 5343.34, "end": 5345.34, "text": " It turns out that", "tokens": [467, 4523, 484, 300], "temperature": 0.0, "avg_logprob": -0.18478573200314544, "compression_ratio": 1.5201793721973094, "no_speech_prob": 1.4367490166478092e-06}, {"id": 1121, "seek": 533006, "start": 5346.18, "end": 5350.3, "text": " This version where we're actually looking at how often a word appears", "tokens": [639, 3037, 689, 321, 434, 767, 1237, 412, 577, 2049, 257, 1349, 7038], "temperature": 0.0, "avg_logprob": -0.18478573200314544, "compression_ratio": 1.5201793721973094, "no_speech_prob": 1.4367490166478092e-06}, {"id": 1122, "seek": 533006, "start": 5351.02, "end": 5356.1, "text": " Like absurd appeared twice it turns out at least for this problem and quite often", "tokens": [1743, 19774, 8516, 6091, 309, 4523, 484, 412, 1935, 337, 341, 1154, 293, 1596, 2049], "temperature": 0.0, "avg_logprob": -0.18478573200314544, "compression_ratio": 1.5201793721973094, "no_speech_prob": 1.4367490166478092e-06}, {"id": 1123, "seek": 535610, "start": 5356.1, "end": 5360.660000000001, "text": " It doesn't matter whether absurd appeared twice or once all that matters is that it appeared", "tokens": [467, 1177, 380, 1871, 1968, 19774, 8516, 6091, 420, 1564, 439, 300, 7001, 307, 300, 309, 8516], "temperature": 0.0, "avg_logprob": -0.20791131198996365, "compression_ratio": 1.7881355932203389, "no_speech_prob": 1.018807438413205e-06}, {"id": 1124, "seek": 535610, "start": 5361.18, "end": 5366.6, "text": " So what what people tend to try doing is to say take the term?", "tokens": [407, 437, 437, 561, 3928, 281, 853, 884, 307, 281, 584, 747, 264, 1433, 30], "temperature": 0.0, "avg_logprob": -0.20791131198996365, "compression_ratio": 1.7881355932203389, "no_speech_prob": 1.018807438413205e-06}, {"id": 1125, "seek": 535610, "start": 5368.54, "end": 5371.06, "text": " Document matrix and go dot sign dot sign", "tokens": [37684, 8141, 293, 352, 5893, 1465, 5893, 1465], "temperature": 0.0, "avg_logprob": -0.20791131198996365, "compression_ratio": 1.7881355932203389, "no_speech_prob": 1.018807438413205e-06}, {"id": 1126, "seek": 535610, "start": 5371.860000000001, "end": 5378.18, "text": " Replaces anything positive with one and anything negative with negative one. We don't have any negative counts obviously so this", "tokens": [47762, 2116, 1340, 3353, 365, 472, 293, 1340, 3671, 365, 3671, 472, 13, 492, 500, 380, 362, 604, 3671, 14893, 2745, 370, 341], "temperature": 0.0, "avg_logprob": -0.20791131198996365, "compression_ratio": 1.7881355932203389, "no_speech_prob": 1.018807438413205e-06}, {"id": 1127, "seek": 535610, "start": 5378.58, "end": 5385.0, "text": " Binarizes it so it says it's I don't care that you saw absurd twice. I just care that you saw it", "tokens": [363, 6470, 5660, 309, 370, 309, 1619, 309, 311, 286, 500, 380, 1127, 300, 291, 1866, 19774, 6091, 13, 286, 445, 1127, 300, 291, 1866, 309], "temperature": 0.0, "avg_logprob": -0.20791131198996365, "compression_ratio": 1.7881355932203389, "no_speech_prob": 1.018807438413205e-06}, {"id": 1128, "seek": 538500, "start": 5385.0, "end": 5388.32, "text": " Right so if we do exactly the same thing", "tokens": [1779, 370, 498, 321, 360, 2293, 264, 912, 551], "temperature": 0.0, "avg_logprob": -0.2057312061260273, "compression_ratio": 1.5174129353233832, "no_speech_prob": 1.0677015325200045e-06}, {"id": 1129, "seek": 538500, "start": 5389.04, "end": 5391.04, "text": " with the binarized version", "tokens": [365, 264, 5171, 289, 1602, 3037], "temperature": 0.0, "avg_logprob": -0.2057312061260273, "compression_ratio": 1.5174129353233832, "no_speech_prob": 1.0677015325200045e-06}, {"id": 1130, "seek": 538500, "start": 5392.04, "end": 5395.08, "text": " Then you get a better result, okay?", "tokens": [1396, 291, 483, 257, 1101, 1874, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.2057312061260273, "compression_ratio": 1.5174129353233832, "no_speech_prob": 1.0677015325200045e-06}, {"id": 1131, "seek": 538500, "start": 5401.04, "end": 5405.78, "text": " Okay now this is the difference between theory and practice right in theory", "tokens": [1033, 586, 341, 307, 264, 2649, 1296, 5261, 293, 3124, 558, 294, 5261], "temperature": 0.0, "avg_logprob": -0.2057312061260273, "compression_ratio": 1.5174129353233832, "no_speech_prob": 1.0677015325200045e-06}, {"id": 1132, "seek": 540578, "start": 5405.78, "end": 5414.86, "text": " I'm naive Bayes sounds okay, but it's it's naive unlike Tyler. It's naive right so what Tyler would probably do would instead say", "tokens": [286, 478, 29052, 7840, 279, 3263, 1392, 11, 457, 309, 311, 309, 311, 29052, 8343, 16869, 13, 467, 311, 29052, 558, 370, 437, 16869, 576, 1391, 360, 576, 2602, 584], "temperature": 0.0, "avg_logprob": -0.27120880285898846, "compression_ratio": 1.68, "no_speech_prob": 1.4823539231656468e-06}, {"id": 1133, "seek": 540578, "start": 5415.38, "end": 5418.0599999999995, "text": " rather than assuming that I", "tokens": [2831, 813, 11926, 300, 286], "temperature": 0.0, "avg_logprob": -0.27120880285898846, "compression_ratio": 1.68, "no_speech_prob": 1.4823539231656468e-06}, {"id": 1134, "seek": 540578, "start": 5418.62, "end": 5425.78, "text": " Should use these coefficients are why don't we learn them so it's unreasonable Tyler. Yeah, okay?", "tokens": [6454, 764, 613, 31994, 366, 983, 500, 380, 321, 1466, 552, 370, 309, 311, 41730, 16869, 13, 865, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.27120880285898846, "compression_ratio": 1.68, "no_speech_prob": 1.4823539231656468e-06}, {"id": 1135, "seek": 540578, "start": 5425.82, "end": 5431.24, "text": " So let's learn them so we can you know we can totally learn them so let's create a logistic regression", "tokens": [407, 718, 311, 1466, 552, 370, 321, 393, 291, 458, 321, 393, 3879, 1466, 552, 370, 718, 311, 1884, 257, 3565, 3142, 24590], "temperature": 0.0, "avg_logprob": -0.27120880285898846, "compression_ratio": 1.68, "no_speech_prob": 1.4823539231656468e-06}, {"id": 1136, "seek": 540578, "start": 5431.78, "end": 5433.78, "text": " Right and let's fit", "tokens": [1779, 293, 718, 311, 3318], "temperature": 0.0, "avg_logprob": -0.27120880285898846, "compression_ratio": 1.68, "no_speech_prob": 1.4823539231656468e-06}, {"id": 1137, "seek": 543378, "start": 5433.78, "end": 5439.82, "text": " Some coefficients and that's going to literally give us something with exactly the same functional form that we had before", "tokens": [2188, 31994, 293, 300, 311, 516, 281, 3736, 976, 505, 746, 365, 2293, 264, 912, 11745, 1254, 300, 321, 632, 949], "temperature": 0.0, "avg_logprob": -0.17637426956840183, "compression_ratio": 1.6978723404255318, "no_speech_prob": 8.851550887811754e-07}, {"id": 1138, "seek": 543378, "start": 5440.0599999999995, "end": 5442.5, "text": " But now rather than using a theoretical", "tokens": [583, 586, 2831, 813, 1228, 257, 20864], "temperature": 0.0, "avg_logprob": -0.17637426956840183, "compression_ratio": 1.6978723404255318, "no_speech_prob": 8.851550887811754e-07}, {"id": 1139, "seek": 543378, "start": 5443.139999999999, "end": 5448.099999999999, "text": " R and a theoretical B. We're going to calculate the two things based on logistic regression and", "tokens": [497, 293, 257, 20864, 363, 13, 492, 434, 516, 281, 8873, 264, 732, 721, 2361, 322, 3565, 3142, 24590, 293], "temperature": 0.0, "avg_logprob": -0.17637426956840183, "compression_ratio": 1.6978723404255318, "no_speech_prob": 8.851550887811754e-07}, {"id": 1140, "seek": 543378, "start": 5448.78, "end": 5450.7, "text": " That's better", "tokens": [663, 311, 1101], "temperature": 0.0, "avg_logprob": -0.17637426956840183, "compression_ratio": 1.6978723404255318, "no_speech_prob": 8.851550887811754e-07}, {"id": 1141, "seek": 543378, "start": 5450.7, "end": 5452.7, "text": " Okay, so", "tokens": [1033, 11, 370], "temperature": 0.0, "avg_logprob": -0.17637426956840183, "compression_ratio": 1.6978723404255318, "no_speech_prob": 8.851550887811754e-07}, {"id": 1142, "seek": 543378, "start": 5455.66, "end": 5457.66, "text": " So it's kind of like yeah, why?", "tokens": [407, 309, 311, 733, 295, 411, 1338, 11, 983, 30], "temperature": 0.0, "avg_logprob": -0.17637426956840183, "compression_ratio": 1.6978723404255318, "no_speech_prob": 8.851550887811754e-07}, {"id": 1143, "seek": 545766, "start": 5457.66, "end": 5463.18, "text": " Why do something based on some theoretical model because theoretical models are never?", "tokens": [1545, 360, 746, 2361, 322, 512, 20864, 2316, 570, 20864, 5245, 366, 1128, 30], "temperature": 0.0, "avg_logprob": -0.17199807119841623, "compression_ratio": 1.853658536585366, "no_speech_prob": 4.49515300715575e-06}, {"id": 1144, "seek": 545766, "start": 5464.0199999999995, "end": 5467.98, "text": " Going to be as accurate pretty much as a data driven model right because theoretical models", "tokens": [10963, 281, 312, 382, 8559, 1238, 709, 382, 257, 1412, 9555, 2316, 558, 570, 20864, 5245], "temperature": 0.0, "avg_logprob": -0.17199807119841623, "compression_ratio": 1.853658536585366, "no_speech_prob": 4.49515300715575e-06}, {"id": 1145, "seek": 545766, "start": 5468.62, "end": 5473.86, "text": " Unless you're dealing with some I don't know like physics thing or something where you're like okay?", "tokens": [16581, 291, 434, 6260, 365, 512, 286, 500, 380, 458, 411, 10649, 551, 420, 746, 689, 291, 434, 411, 1392, 30], "temperature": 0.0, "avg_logprob": -0.17199807119841623, "compression_ratio": 1.853658536585366, "no_speech_prob": 4.49515300715575e-06}, {"id": 1146, "seek": 545766, "start": 5473.86, "end": 5478.5, "text": " This is actually how the world works there really is no I don't know we're working in a vacuum", "tokens": [639, 307, 767, 577, 264, 1002, 1985, 456, 534, 307, 572, 286, 500, 380, 458, 321, 434, 1364, 294, 257, 14224], "temperature": 0.0, "avg_logprob": -0.17199807119841623, "compression_ratio": 1.853658536585366, "no_speech_prob": 4.49515300715575e-06}, {"id": 1147, "seek": 545766, "start": 5478.66, "end": 5483.099999999999, "text": " And this is the exact gravity and blah blah blah right but most of the real world", "tokens": [400, 341, 307, 264, 1900, 12110, 293, 12288, 12288, 12288, 558, 457, 881, 295, 264, 957, 1002], "temperature": 0.0, "avg_logprob": -0.17199807119841623, "compression_ratio": 1.853658536585366, "no_speech_prob": 4.49515300715575e-06}, {"id": 1148, "seek": 548310, "start": 5483.1, "end": 5488.46, "text": " This is how things are like it's better to learn your coefficients and calculate them. Yes, you know", "tokens": [639, 307, 577, 721, 366, 411, 309, 311, 1101, 281, 1466, 428, 31994, 293, 8873, 552, 13, 1079, 11, 291, 458], "temperature": 0.0, "avg_logprob": -0.26711805101851344, "compression_ratio": 1.4615384615384615, "no_speech_prob": 1.9947192413383164e-06}, {"id": 1149, "seek": 548310, "start": 5490.860000000001, "end": 5493.160000000001, "text": " Jeremy was this dual equal through", "tokens": [17809, 390, 341, 11848, 2681, 807], "temperature": 0.0, "avg_logprob": -0.26711805101851344, "compression_ratio": 1.4615384615384615, "no_speech_prob": 1.9947192413383164e-06}, {"id": 1150, "seek": 548310, "start": 5495.14, "end": 5500.860000000001, "text": " Hoping you'd ignore not notice, but you saw it basically in this case our", "tokens": [13438, 278, 291, 1116, 11200, 406, 3449, 11, 457, 291, 1866, 309, 1936, 294, 341, 1389, 527], "temperature": 0.0, "avg_logprob": -0.26711805101851344, "compression_ratio": 1.4615384615384615, "no_speech_prob": 1.9947192413383164e-06}, {"id": 1151, "seek": 548310, "start": 5501.860000000001, "end": 5504.740000000001, "text": " Term document matrix is much wider than it is tall", "tokens": [19835, 4166, 8141, 307, 709, 11842, 813, 309, 307, 6764], "temperature": 0.0, "avg_logprob": -0.26711805101851344, "compression_ratio": 1.4615384615384615, "no_speech_prob": 1.9947192413383164e-06}, {"id": 1152, "seek": 548310, "start": 5505.820000000001, "end": 5507.820000000001, "text": " There is a reformulation", "tokens": [821, 307, 257, 8290, 2776], "temperature": 0.0, "avg_logprob": -0.26711805101851344, "compression_ratio": 1.4615384615384615, "no_speech_prob": 1.9947192413383164e-06}, {"id": 1153, "seek": 550782, "start": 5507.82, "end": 5514.0199999999995, "text": " Of mathematically basically almost a mathematically equivalent reformulation of logistic regression that happens to be a lot faster", "tokens": [2720, 44003, 1936, 1920, 257, 44003, 10344, 8290, 2776, 295, 3565, 3142, 24590, 300, 2314, 281, 312, 257, 688, 4663], "temperature": 0.0, "avg_logprob": -0.18766910657970184, "compression_ratio": 1.768060836501901, "no_speech_prob": 1.8738668359219446e-06}, {"id": 1154, "seek": 550782, "start": 5514.219999999999, "end": 5518.099999999999, "text": " When it's wider than it is tall so the short answer is if you don't put that here", "tokens": [1133, 309, 311, 11842, 813, 309, 307, 6764, 370, 264, 2099, 1867, 307, 498, 291, 500, 380, 829, 300, 510], "temperature": 0.0, "avg_logprob": -0.18766910657970184, "compression_ratio": 1.768060836501901, "no_speech_prob": 1.8738668359219446e-06}, {"id": 1155, "seek": 550782, "start": 5518.299999999999, "end": 5523.299999999999, "text": " Anytime it's wider than it is tall put dual equals true, and it will run this runs in like two seconds", "tokens": [39401, 309, 311, 11842, 813, 309, 307, 6764, 829, 11848, 6915, 2074, 11, 293, 309, 486, 1190, 341, 6676, 294, 411, 732, 3949], "temperature": 0.0, "avg_logprob": -0.18766910657970184, "compression_ratio": 1.768060836501901, "no_speech_prob": 1.8738668359219446e-06}, {"id": 1156, "seek": 550782, "start": 5523.299999999999, "end": 5525.54, "text": " If you don't have it here, it'll take a few minutes", "tokens": [759, 291, 500, 380, 362, 309, 510, 11, 309, 603, 747, 257, 1326, 2077], "temperature": 0.0, "avg_logprob": -0.18766910657970184, "compression_ratio": 1.768060836501901, "no_speech_prob": 1.8738668359219446e-06}, {"id": 1157, "seek": 550782, "start": 5526.9, "end": 5532.5, "text": " So like in math there's this kind of concept of dual versions of problems which are kind of like", "tokens": [407, 411, 294, 5221, 456, 311, 341, 733, 295, 3410, 295, 11848, 9606, 295, 2740, 597, 366, 733, 295, 411], "temperature": 0.0, "avg_logprob": -0.18766910657970184, "compression_ratio": 1.768060836501901, "no_speech_prob": 1.8738668359219446e-06}, {"id": 1158, "seek": 553250, "start": 5532.5, "end": 5537.38, "text": " Equivalent versions that sometimes work better for certain situations", "tokens": [15624, 3576, 317, 9606, 300, 2171, 589, 1101, 337, 1629, 6851], "temperature": 0.0, "avg_logprob": -0.2652203790072737, "compression_ratio": 1.668421052631579, "no_speech_prob": 3.3405192425561836e-06}, {"id": 1159, "seek": 553250, "start": 5540.22, "end": 5541.38, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.2652203790072737, "compression_ratio": 1.668421052631579, "no_speech_prob": 3.3405192425561836e-06}, {"id": 1160, "seek": 553250, "start": 5541.38, "end": 5544.52, "text": " Here is so here is the binarized version", "tokens": [1692, 307, 370, 510, 307, 264, 5171, 289, 1602, 3037], "temperature": 0.0, "avg_logprob": -0.2652203790072737, "compression_ratio": 1.668421052631579, "no_speech_prob": 3.3405192425561836e-06}, {"id": 1161, "seek": 553250, "start": 5545.22, "end": 5546.66, "text": " right", "tokens": [558], "temperature": 0.0, "avg_logprob": -0.2652203790072737, "compression_ratio": 1.668421052631579, "no_speech_prob": 3.3405192425561836e-06}, {"id": 1162, "seek": 553250, "start": 5546.66, "end": 5553.62, "text": " And it's it's about the same right so you can see I've fitted it with the the sign of the doc of the doc", "tokens": [400, 309, 311, 309, 311, 466, 264, 912, 558, 370, 291, 393, 536, 286, 600, 26321, 309, 365, 264, 264, 1465, 295, 264, 3211, 295, 264, 3211], "temperature": 0.0, "avg_logprob": -0.2652203790072737, "compression_ratio": 1.668421052631579, "no_speech_prob": 3.3405192425561836e-06}, {"id": 1163, "seek": 553250, "start": 5553.62, "end": 5555.62, "text": " term doc matrix and", "tokens": [1433, 3211, 8141, 293], "temperature": 0.0, "avg_logprob": -0.2652203790072737, "compression_ratio": 1.668421052631579, "no_speech_prob": 3.3405192425561836e-06}, {"id": 1164, "seek": 553250, "start": 5555.86, "end": 5557.86, "text": " predicted it with this right", "tokens": [19147, 309, 365, 341, 558], "temperature": 0.0, "avg_logprob": -0.2652203790072737, "compression_ratio": 1.668421052631579, "no_speech_prob": 3.3405192425561836e-06}, {"id": 1165, "seek": 553250, "start": 5558.58, "end": 5560.9, "text": " Now the thing is that this is going to be", "tokens": [823, 264, 551, 307, 300, 341, 307, 516, 281, 312], "temperature": 0.0, "avg_logprob": -0.2652203790072737, "compression_ratio": 1.668421052631579, "no_speech_prob": 3.3405192425561836e-06}, {"id": 1166, "seek": 556090, "start": 5560.9, "end": 5562.9, "text": " a", "tokens": [257], "temperature": 0.0, "avg_logprob": -0.26636314392089844, "compression_ratio": 1.5023696682464456, "no_speech_prob": 4.157347575528547e-06}, {"id": 1167, "seek": 556090, "start": 5563.379999999999, "end": 5565.379999999999, "text": " Coefficient for every term", "tokens": [3066, 68, 7816, 337, 633, 1433], "temperature": 0.0, "avg_logprob": -0.26636314392089844, "compression_ratio": 1.5023696682464456, "no_speech_prob": 4.157347575528547e-06}, {"id": 1168, "seek": 556090, "start": 5566.179999999999, "end": 5569.339999999999, "text": " There was about 75,000 terms in our vocabulary", "tokens": [821, 390, 466, 9562, 11, 1360, 2115, 294, 527, 19864], "temperature": 0.0, "avg_logprob": -0.26636314392089844, "compression_ratio": 1.5023696682464456, "no_speech_prob": 4.157347575528547e-06}, {"id": 1169, "seek": 556090, "start": 5570.259999999999, "end": 5573.42, "text": " And that seems like a lot of coefficients given that we've only got", "tokens": [400, 300, 2544, 411, 257, 688, 295, 31994, 2212, 300, 321, 600, 787, 658], "temperature": 0.0, "avg_logprob": -0.26636314392089844, "compression_ratio": 1.5023696682464456, "no_speech_prob": 4.157347575528547e-06}, {"id": 1170, "seek": 556090, "start": 5574.46, "end": 5577.5, "text": " 25,000 reviews, so maybe we should try regularizing this", "tokens": [3552, 11, 1360, 10229, 11, 370, 1310, 321, 820, 853, 3890, 3319, 341], "temperature": 0.0, "avg_logprob": -0.26636314392089844, "compression_ratio": 1.5023696682464456, "no_speech_prob": 4.157347575528547e-06}, {"id": 1171, "seek": 556090, "start": 5578.179999999999, "end": 5586.36, "text": " So we can use regularization built into SK learns logistic regression plus which is c is the parameter that they use", "tokens": [407, 321, 393, 764, 3890, 2144, 3094, 666, 21483, 27152, 3565, 3142, 24590, 1804, 597, 307, 269, 307, 264, 13075, 300, 436, 764], "temperature": 0.0, "avg_logprob": -0.26636314392089844, "compression_ratio": 1.5023696682464456, "no_speech_prob": 4.157347575528547e-06}, {"id": 1172, "seek": 558636, "start": 5586.36, "end": 5592.12, "text": " A smaller this is slightly weird a smaller parameter is more regularization, right?", "tokens": [316, 4356, 341, 307, 4748, 3657, 257, 4356, 13075, 307, 544, 3890, 2144, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.22680706260478603, "compression_ratio": 1.744186046511628, "no_speech_prob": 1.1365602858859347e-06}, {"id": 1173, "seek": 558636, "start": 5592.12, "end": 5598.46, "text": " So that's why I used one eight to basically turn off regularization here, so if I turn on regularization set it to point one", "tokens": [407, 300, 311, 983, 286, 1143, 472, 3180, 281, 1936, 1261, 766, 3890, 2144, 510, 11, 370, 498, 286, 1261, 322, 3890, 2144, 992, 309, 281, 935, 472], "temperature": 0.0, "avg_logprob": -0.22680706260478603, "compression_ratio": 1.744186046511628, "no_speech_prob": 1.1365602858859347e-06}, {"id": 1174, "seek": 558636, "start": 5599.28, "end": 5602.46, "text": " Then now it's 88 percent okay, which makes sense", "tokens": [1396, 586, 309, 311, 24587, 3043, 1392, 11, 597, 1669, 2020], "temperature": 0.0, "avg_logprob": -0.22680706260478603, "compression_ratio": 1.744186046511628, "no_speech_prob": 1.1365602858859347e-06}, {"id": 1175, "seek": 558636, "start": 5602.46, "end": 5604.48, "text": " You know you wouldn't you would think like", "tokens": [509, 458, 291, 2759, 380, 291, 576, 519, 411], "temperature": 0.0, "avg_logprob": -0.22680706260478603, "compression_ratio": 1.744186046511628, "no_speech_prob": 1.1365602858859347e-06}, {"id": 1176, "seek": 558636, "start": 5605.4, "end": 5610.92, "text": " 75,000 parameters for 25,000 documents you know it's likely to overfit indeed it did overfit", "tokens": [9562, 11, 1360, 9834, 337, 3552, 11, 1360, 8512, 291, 458, 309, 311, 3700, 281, 670, 6845, 6451, 309, 630, 670, 6845], "temperature": 0.0, "avg_logprob": -0.22680706260478603, "compression_ratio": 1.744186046511628, "no_speech_prob": 1.1365602858859347e-06}, {"id": 1177, "seek": 558636, "start": 5611.4, "end": 5615.44, "text": " So this is adding L2 regularization to avoid overfitting", "tokens": [407, 341, 307, 5127, 441, 17, 3890, 2144, 281, 5042, 670, 69, 2414], "temperature": 0.0, "avg_logprob": -0.22680706260478603, "compression_ratio": 1.744186046511628, "no_speech_prob": 1.1365602858859347e-06}, {"id": 1178, "seek": 561544, "start": 5615.44, "end": 5617.44, "text": " I", "tokens": [286], "temperature": 0.0, "avg_logprob": -0.21254796140334187, "compression_ratio": 1.456140350877193, "no_speech_prob": 1.1911065485037398e-06}, {"id": 1179, "seek": 561544, "start": 5617.879999999999, "end": 5624.48, "text": " Mentioned earlier that as well as L2 which is looking at the weight squared. There's also L1", "tokens": [376, 1251, 292, 3071, 300, 382, 731, 382, 441, 17, 597, 307, 1237, 412, 264, 3364, 8889, 13, 821, 311, 611, 441, 16], "temperature": 0.0, "avg_logprob": -0.21254796140334187, "compression_ratio": 1.456140350877193, "no_speech_prob": 1.1911065485037398e-06}, {"id": 1180, "seek": 561544, "start": 5626.08, "end": 5633.0, "text": " Which is looking at just the absolute value of the weights right I", "tokens": [3013, 307, 1237, 412, 445, 264, 8236, 2158, 295, 264, 17443, 558, 286], "temperature": 0.0, "avg_logprob": -0.21254796140334187, "compression_ratio": 1.456140350877193, "no_speech_prob": 1.1911065485037398e-06}, {"id": 1181, "seek": 561544, "start": 5634.48, "end": 5635.799999999999, "text": " Was", "tokens": [3027], "temperature": 0.0, "avg_logprob": -0.21254796140334187, "compression_ratio": 1.456140350877193, "no_speech_prob": 1.1911065485037398e-06}, {"id": 1182, "seek": 561544, "start": 5635.799999999999, "end": 5640.919999999999, "text": " Kind of pretty sloppy in my wording before I said that L2 tries to make things zero", "tokens": [9242, 295, 1238, 43684, 294, 452, 47602, 949, 286, 848, 300, 441, 17, 9898, 281, 652, 721, 4018], "temperature": 0.0, "avg_logprob": -0.21254796140334187, "compression_ratio": 1.456140350877193, "no_speech_prob": 1.1911065485037398e-06}, {"id": 1183, "seek": 564092, "start": 5640.92, "end": 5645.32, "text": " That's kind of true, but if you've got two things that are highly correlated", "tokens": [663, 311, 733, 295, 2074, 11, 457, 498, 291, 600, 658, 732, 721, 300, 366, 5405, 38574], "temperature": 0.0, "avg_logprob": -0.19469815380168412, "compression_ratio": 1.7509881422924902, "no_speech_prob": 2.0261375084373867e-06}, {"id": 1184, "seek": 564092, "start": 5645.88, "end": 5650.04, "text": " Then L2 regularization will like move them both down together", "tokens": [1396, 441, 17, 3890, 2144, 486, 411, 1286, 552, 1293, 760, 1214], "temperature": 0.0, "avg_logprob": -0.19469815380168412, "compression_ratio": 1.7509881422924902, "no_speech_prob": 2.0261375084373867e-06}, {"id": 1185, "seek": 564092, "start": 5650.24, "end": 5654.8, "text": " It won't make one of them zero and one of the non zero right so L1 regularization", "tokens": [467, 1582, 380, 652, 472, 295, 552, 4018, 293, 472, 295, 264, 2107, 4018, 558, 370, 441, 16, 3890, 2144], "temperature": 0.0, "avg_logprob": -0.19469815380168412, "compression_ratio": 1.7509881422924902, "no_speech_prob": 2.0261375084373867e-06}, {"id": 1186, "seek": 564092, "start": 5655.6, "end": 5660.04, "text": " Actually has the property that it'll try to make as many things zero as possible", "tokens": [5135, 575, 264, 4707, 300, 309, 603, 853, 281, 652, 382, 867, 721, 4018, 382, 1944], "temperature": 0.0, "avg_logprob": -0.19469815380168412, "compression_ratio": 1.7509881422924902, "no_speech_prob": 2.0261375084373867e-06}, {"id": 1187, "seek": 564092, "start": 5660.68, "end": 5665.54, "text": " Whereas L2 regularization has a property that tends to try to make kind of everything smaller", "tokens": [13813, 441, 17, 3890, 2144, 575, 257, 4707, 300, 12258, 281, 853, 281, 652, 733, 295, 1203, 4356], "temperature": 0.0, "avg_logprob": -0.19469815380168412, "compression_ratio": 1.7509881422924902, "no_speech_prob": 2.0261375084373867e-06}, {"id": 1188, "seek": 564092, "start": 5666.28, "end": 5668.64, "text": " We actually don't care about that difference in", "tokens": [492, 767, 500, 380, 1127, 466, 300, 2649, 294], "temperature": 0.0, "avg_logprob": -0.19469815380168412, "compression_ratio": 1.7509881422924902, "no_speech_prob": 2.0261375084373867e-06}, {"id": 1189, "seek": 566864, "start": 5668.64, "end": 5669.96, "text": " in", "tokens": [294], "temperature": 0.0, "avg_logprob": -0.16939823444073016, "compression_ratio": 1.5577689243027888, "no_speech_prob": 3.3931273719645105e-06}, {"id": 1190, "seek": 566864, "start": 5669.96, "end": 5674.88, "text": " Really any modern machine learning because we very rarely try to directly interpret the coefficients", "tokens": [4083, 604, 4363, 3479, 2539, 570, 321, 588, 13752, 853, 281, 3838, 7302, 264, 31994], "temperature": 0.0, "avg_logprob": -0.16939823444073016, "compression_ratio": 1.5577689243027888, "no_speech_prob": 3.3931273719645105e-06}, {"id": 1191, "seek": 566864, "start": 5674.96, "end": 5677.68, "text": " We try to understand our models through", "tokens": [492, 853, 281, 1223, 527, 5245, 807], "temperature": 0.0, "avg_logprob": -0.16939823444073016, "compression_ratio": 1.5577689243027888, "no_speech_prob": 3.3931273719645105e-06}, {"id": 1192, "seek": 566864, "start": 5678.8, "end": 5680.8, "text": " Interrogation using the kind of techniques that we've learned", "tokens": [5751, 6675, 399, 1228, 264, 733, 295, 7512, 300, 321, 600, 3264], "temperature": 0.0, "avg_logprob": -0.16939823444073016, "compression_ratio": 1.5577689243027888, "no_speech_prob": 3.3931273719645105e-06}, {"id": 1193, "seek": 566864, "start": 5681.280000000001, "end": 5687.56, "text": " The reason that we would care about L1 versus L2 is simply like which one ends up with a better error on the validation set", "tokens": [440, 1778, 300, 321, 576, 1127, 466, 441, 16, 5717, 441, 17, 307, 2935, 411, 597, 472, 5314, 493, 365, 257, 1101, 6713, 322, 264, 24071, 992], "temperature": 0.0, "avg_logprob": -0.16939823444073016, "compression_ratio": 1.5577689243027888, "no_speech_prob": 3.3931273719645105e-06}, {"id": 1194, "seek": 566864, "start": 5687.68, "end": 5689.68, "text": " Okay, and you can try both", "tokens": [1033, 11, 293, 291, 393, 853, 1293], "temperature": 0.0, "avg_logprob": -0.16939823444073016, "compression_ratio": 1.5577689243027888, "no_speech_prob": 3.3931273719645105e-06}, {"id": 1195, "seek": 566864, "start": 5690.320000000001, "end": 5692.320000000001, "text": " With SK learns logistic regression", "tokens": [2022, 21483, 27152, 3565, 3142, 24590], "temperature": 0.0, "avg_logprob": -0.16939823444073016, "compression_ratio": 1.5577689243027888, "no_speech_prob": 3.3931273719645105e-06}, {"id": 1196, "seek": 569232, "start": 5692.32, "end": 5698.679999999999, "text": " L2 actually turns out to be a lot faster because you can't use dual equals true unless you have L2", "tokens": [441, 17, 767, 4523, 484, 281, 312, 257, 688, 4663, 570, 291, 393, 380, 764, 11848, 6915, 2074, 5969, 291, 362, 441, 17], "temperature": 0.0, "avg_logprob": -0.15326989634653157, "compression_ratio": 1.5545023696682465, "no_speech_prob": 1.5534905060121673e-06}, {"id": 1197, "seek": 569232, "start": 5698.679999999999, "end": 5703.5599999999995, "text": " So you know and L2 is the default so I didn't really worry too much about that difference here", "tokens": [407, 291, 458, 293, 441, 17, 307, 264, 7576, 370, 286, 994, 380, 534, 3292, 886, 709, 466, 300, 2649, 510], "temperature": 0.0, "avg_logprob": -0.15326989634653157, "compression_ratio": 1.5545023696682465, "no_speech_prob": 1.5534905060121673e-06}, {"id": 1198, "seek": 569232, "start": 5704.719999999999, "end": 5706.719999999999, "text": " So you can see here if we use", "tokens": [407, 291, 393, 536, 510, 498, 321, 764], "temperature": 0.0, "avg_logprob": -0.15326989634653157, "compression_ratio": 1.5545023696682465, "no_speech_prob": 1.5534905060121673e-06}, {"id": 1199, "seek": 569232, "start": 5708.28, "end": 5713.16, "text": " Regularization and binarized we actually do pretty well, okay", "tokens": [45659, 2144, 293, 5171, 289, 1602, 321, 767, 360, 1238, 731, 11, 1392], "temperature": 0.0, "avg_logprob": -0.15326989634653157, "compression_ratio": 1.5545023696682465, "no_speech_prob": 1.5534905060121673e-06}, {"id": 1200, "seek": 569232, "start": 5715.04, "end": 5717.04, "text": " So", "tokens": [407], "temperature": 0.0, "avg_logprob": -0.15326989634653157, "compression_ratio": 1.5545023696682465, "no_speech_prob": 1.5534905060121673e-06}, {"id": 1201, "seek": 571704, "start": 5717.04, "end": 5721.24, "text": " Um yes, can you pass that back to W please?", "tokens": [3301, 2086, 11, 393, 291, 1320, 300, 646, 281, 343, 1767, 30], "temperature": 0.0, "avg_logprob": -0.22877834461353444, "compression_ratio": 1.4343434343434343, "no_speech_prob": 5.338083610695321e-06}, {"id": 1202, "seek": 571704, "start": 5723.84, "end": 5730.28, "text": " Before we learned about elastic net right like combining L1 and L2 yeah, yeah, yeah, you can do that, but I mean", "tokens": [4546, 321, 3264, 466, 17115, 2533, 558, 411, 21928, 441, 16, 293, 441, 17, 1338, 11, 1338, 11, 1338, 11, 291, 393, 360, 300, 11, 457, 286, 914], "temperature": 0.0, "avg_logprob": -0.22877834461353444, "compression_ratio": 1.4343434343434343, "no_speech_prob": 5.338083610695321e-06}, {"id": 1203, "seek": 571704, "start": 5731.12, "end": 5734.08, "text": " It's like you know with with deeper models", "tokens": [467, 311, 411, 291, 458, 365, 365, 7731, 5245], "temperature": 0.0, "avg_logprob": -0.22877834461353444, "compression_ratio": 1.4343434343434343, "no_speech_prob": 5.338083610695321e-06}, {"id": 1204, "seek": 571704, "start": 5734.84, "end": 5737.5199999999995, "text": " Yeah, I've never seen anybody find that useful", "tokens": [865, 11, 286, 600, 1128, 1612, 4472, 915, 300, 4420], "temperature": 0.0, "avg_logprob": -0.22877834461353444, "compression_ratio": 1.4343434343434343, "no_speech_prob": 5.338083610695321e-06}, {"id": 1205, "seek": 571704, "start": 5739.8, "end": 5740.96, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.22877834461353444, "compression_ratio": 1.4343434343434343, "no_speech_prob": 5.338083610695321e-06}, {"id": 1206, "seek": 571704, "start": 5740.96, "end": 5742.96, "text": " So the last thing I mentioned is", "tokens": [407, 264, 1036, 551, 286, 2835, 307], "temperature": 0.0, "avg_logprob": -0.22877834461353444, "compression_ratio": 1.4343434343434343, "no_speech_prob": 5.338083610695321e-06}, {"id": 1207, "seek": 574296, "start": 5742.96, "end": 5747.42, "text": " That you can when you do your count vectorizer", "tokens": [663, 291, 393, 562, 291, 360, 428, 1207, 8062, 6545], "temperature": 0.0, "avg_logprob": -0.23197451139751232, "compression_ratio": 1.7, "no_speech_prob": 4.029426690976834e-06}, {"id": 1208, "seek": 574296, "start": 5748.24, "end": 5755.0, "text": " Where that was when you do your count vectorizer you can also ask for n grams right by default we get", "tokens": [2305, 300, 390, 562, 291, 360, 428, 1207, 8062, 6545, 291, 393, 611, 1029, 337, 297, 11899, 558, 538, 7576, 321, 483], "temperature": 0.0, "avg_logprob": -0.23197451139751232, "compression_ratio": 1.7, "no_speech_prob": 4.029426690976834e-06}, {"id": 1209, "seek": 574296, "start": 5755.56, "end": 5759.6, "text": " Unigrams that is single words, but if we if we say", "tokens": [1156, 33737, 82, 300, 307, 2167, 2283, 11, 457, 498, 321, 498, 321, 584], "temperature": 0.0, "avg_logprob": -0.23197451139751232, "compression_ratio": 1.7, "no_speech_prob": 4.029426690976834e-06}, {"id": 1210, "seek": 574296, "start": 5760.8, "end": 5764.9800000000005, "text": " n gram range equals 1 comma 3 that's also going to give us", "tokens": [297, 21353, 3613, 6915, 502, 22117, 805, 300, 311, 611, 516, 281, 976, 505], "temperature": 0.0, "avg_logprob": -0.23197451139751232, "compression_ratio": 1.7, "no_speech_prob": 4.029426690976834e-06}, {"id": 1211, "seek": 576498, "start": 5764.98, "end": 5773.58, "text": " By grams and trigrams by which I mean if I now say okay, let's go ahead and do the count vectorizer", "tokens": [3146, 11899, 293, 504, 33737, 82, 538, 597, 286, 914, 498, 286, 586, 584, 1392, 11, 718, 311, 352, 2286, 293, 360, 264, 1207, 8062, 6545], "temperature": 0.0, "avg_logprob": -0.2574125515517368, "compression_ratio": 1.6371681415929205, "no_speech_prob": 2.225272282885271e-06}, {"id": 1212, "seek": 576498, "start": 5773.58, "end": 5777.66, "text": " I get feature names now my vocabulary includes a bigram", "tokens": [286, 483, 4111, 5288, 586, 452, 19864, 5974, 257, 955, 2356], "temperature": 0.0, "avg_logprob": -0.2574125515517368, "compression_ratio": 1.6371681415929205, "no_speech_prob": 2.225272282885271e-06}, {"id": 1213, "seek": 576498, "start": 5778.259999999999, "end": 5785.58, "text": " Right by vast by vengeance and a trigram by vengeance full stop by Vera miles right", "tokens": [1779, 538, 8369, 538, 43818, 293, 257, 504, 33737, 538, 43818, 1577, 1590, 538, 46982, 6193, 558], "temperature": 0.0, "avg_logprob": -0.2574125515517368, "compression_ratio": 1.6371681415929205, "no_speech_prob": 2.225272282885271e-06}, {"id": 1214, "seek": 576498, "start": 5785.66, "end": 5788.98, "text": " So this is now doing the same thing but after tokenizing", "tokens": [407, 341, 307, 586, 884, 264, 912, 551, 457, 934, 14862, 3319], "temperature": 0.0, "avg_logprob": -0.2574125515517368, "compression_ratio": 1.6371681415929205, "no_speech_prob": 2.225272282885271e-06}, {"id": 1215, "seek": 576498, "start": 5789.379999999999, "end": 5792.86, "text": " It's not just grabbing each word and saying that's part of our vocabulary", "tokens": [467, 311, 406, 445, 23771, 1184, 1349, 293, 1566, 300, 311, 644, 295, 527, 19864], "temperature": 0.0, "avg_logprob": -0.2574125515517368, "compression_ratio": 1.6371681415929205, "no_speech_prob": 2.225272282885271e-06}, {"id": 1216, "seek": 579286, "start": 5792.86, "end": 5798.5599999999995, "text": " But each two words next to each other and each three words next to each other and this ten this turns out to be like", "tokens": [583, 1184, 732, 2283, 958, 281, 1184, 661, 293, 1184, 1045, 2283, 958, 281, 1184, 661, 293, 341, 2064, 341, 4523, 484, 281, 312, 411], "temperature": 0.0, "avg_logprob": -0.19918382295998194, "compression_ratio": 1.7636363636363637, "no_speech_prob": 7.002161623859138e-07}, {"id": 1217, "seek": 579286, "start": 5799.62, "end": 5803.0199999999995, "text": " Super helpful in like taking advantage of bag of word", "tokens": [4548, 4961, 294, 411, 1940, 5002, 295, 3411, 295, 1349], "temperature": 0.0, "avg_logprob": -0.19918382295998194, "compression_ratio": 1.7636363636363637, "no_speech_prob": 7.002161623859138e-07}, {"id": 1218, "seek": 579286, "start": 5804.299999999999, "end": 5808.0599999999995, "text": " approaches because we now can see like the difference between like", "tokens": [11587, 570, 321, 586, 393, 536, 411, 264, 2649, 1296, 411], "temperature": 0.0, "avg_logprob": -0.19918382295998194, "compression_ratio": 1.7636363636363637, "no_speech_prob": 7.002161623859138e-07}, {"id": 1219, "seek": 579286, "start": 5808.66, "end": 5811.94, "text": " You know not good versus not bad", "tokens": [509, 458, 406, 665, 5717, 406, 1578], "temperature": 0.0, "avg_logprob": -0.19918382295998194, "compression_ratio": 1.7636363636363637, "no_speech_prob": 7.002161623859138e-07}, {"id": 1220, "seek": 579286, "start": 5812.9, "end": 5814.9, "text": " Versus not terrible", "tokens": [12226, 301, 406, 6237], "temperature": 0.0, "avg_logprob": -0.19918382295998194, "compression_ratio": 1.7636363636363637, "no_speech_prob": 7.002161623859138e-07}, {"id": 1221, "seek": 579286, "start": 5814.94, "end": 5816.099999999999, "text": " right", "tokens": [558], "temperature": 0.0, "avg_logprob": -0.19918382295998194, "compression_ratio": 1.7636363636363637, "no_speech_prob": 7.002161623859138e-07}, {"id": 1222, "seek": 579286, "start": 5816.099999999999, "end": 5822.219999999999, "text": " Or even like double quote good double quote which is probably going to be sarcastic, right?", "tokens": [1610, 754, 411, 3834, 6513, 665, 3834, 6513, 597, 307, 1391, 516, 281, 312, 36836, 2750, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.19918382295998194, "compression_ratio": 1.7636363636363637, "no_speech_prob": 7.002161623859138e-07}, {"id": 1223, "seek": 582222, "start": 5822.22, "end": 5824.22, "text": " So using trigram features", "tokens": [407, 1228, 504, 33737, 4122], "temperature": 0.0, "avg_logprob": -0.3276358537895735, "compression_ratio": 1.565217391304348, "no_speech_prob": 2.5215551886503818e-06}, {"id": 1224, "seek": 582222, "start": 5825.66, "end": 5828.26, "text": " Actually is going to turn out to make both naive bays", "tokens": [5135, 307, 516, 281, 1261, 484, 281, 652, 1293, 29052, 272, 3772], "temperature": 0.0, "avg_logprob": -0.3276358537895735, "compression_ratio": 1.565217391304348, "no_speech_prob": 2.5215551886503818e-06}, {"id": 1225, "seek": 582222, "start": 5828.860000000001, "end": 5830.860000000001, "text": " and logistic regression", "tokens": [293, 3565, 3142, 24590], "temperature": 0.0, "avg_logprob": -0.3276358537895735, "compression_ratio": 1.565217391304348, "no_speech_prob": 2.5215551886503818e-06}, {"id": 1226, "seek": 582222, "start": 5830.9400000000005, "end": 5835.5, "text": " Quite a lot better. It really takes us quite a lot further and makes them quite useful", "tokens": [20464, 257, 688, 1101, 13, 467, 534, 2516, 505, 1596, 257, 688, 3052, 293, 1669, 552, 1596, 4420], "temperature": 0.0, "avg_logprob": -0.3276358537895735, "compression_ratio": 1.565217391304348, "no_speech_prob": 2.5215551886503818e-06}, {"id": 1227, "seek": 582222, "start": 5837.26, "end": 5839.900000000001, "text": " I have a question about the", "tokens": [286, 362, 257, 1168, 466, 264], "temperature": 0.0, "avg_logprob": -0.3276358537895735, "compression_ratio": 1.565217391304348, "no_speech_prob": 2.5215551886503818e-06}, {"id": 1228, "seek": 582222, "start": 5841.58, "end": 5846.22, "text": " Tokenizers so you are saying some max features. So how are these?", "tokens": [314, 8406, 22525, 370, 291, 366, 1566, 512, 11469, 4122, 13, 407, 577, 366, 613, 30], "temperature": 0.0, "avg_logprob": -0.3276358537895735, "compression_ratio": 1.565217391304348, "no_speech_prob": 2.5215551886503818e-06}, {"id": 1229, "seek": 582222, "start": 5847.780000000001, "end": 5850.54, "text": " By grams and trigrams selected right so", "tokens": [3146, 11899, 293, 504, 33737, 82, 8209, 558, 370], "temperature": 0.0, "avg_logprob": -0.3276358537895735, "compression_ratio": 1.565217391304348, "no_speech_prob": 2.5215551886503818e-06}, {"id": 1230, "seek": 585054, "start": 5850.54, "end": 5852.54, "text": " since", "tokens": [1670], "temperature": 0.0, "avg_logprob": -0.17284694372438916, "compression_ratio": 1.6179775280898876, "no_speech_prob": 4.860420631302986e-06}, {"id": 1231, "seek": 585054, "start": 5853.46, "end": 5855.46, "text": " I'm using a linear model I", "tokens": [286, 478, 1228, 257, 8213, 2316, 286], "temperature": 0.0, "avg_logprob": -0.17284694372438916, "compression_ratio": 1.6179775280898876, "no_speech_prob": 4.860420631302986e-06}, {"id": 1232, "seek": 585054, "start": 5856.46, "end": 5862.1, "text": " Didn't want to create too many features. I mean it actually worked fine even without max features. I think I had something like I", "tokens": [11151, 380, 528, 281, 1884, 886, 867, 4122, 13, 286, 914, 309, 767, 2732, 2489, 754, 1553, 11469, 4122, 13, 286, 519, 286, 632, 746, 411, 286], "temperature": 0.0, "avg_logprob": -0.17284694372438916, "compression_ratio": 1.6179775280898876, "no_speech_prob": 4.860420631302986e-06}, {"id": 1233, "seek": 585054, "start": 5862.82, "end": 5868.98, "text": " Can't remember 70 million coefficients. It still worked right, but just there's no need to have 70 million coefficients", "tokens": [1664, 380, 1604, 5285, 2459, 31994, 13, 467, 920, 2732, 558, 11, 457, 445, 456, 311, 572, 643, 281, 362, 5285, 2459, 31994], "temperature": 0.0, "avg_logprob": -0.17284694372438916, "compression_ratio": 1.6179775280898876, "no_speech_prob": 4.860420631302986e-06}, {"id": 1234, "seek": 585054, "start": 5868.98, "end": 5871.78, "text": " So if you say max features equals 800,000", "tokens": [407, 498, 291, 584, 11469, 4122, 6915, 13083, 11, 1360], "temperature": 0.0, "avg_logprob": -0.17284694372438916, "compression_ratio": 1.6179775280898876, "no_speech_prob": 4.860420631302986e-06}, {"id": 1235, "seek": 585054, "start": 5872.42, "end": 5880.1, "text": " The count vectorizer will sort the vocabulary by how often everything appears whether it be unigram by gram", "tokens": [440, 1207, 8062, 6545, 486, 1333, 264, 19864, 538, 577, 2049, 1203, 7038, 1968, 309, 312, 517, 33737, 538, 21353], "temperature": 0.0, "avg_logprob": -0.17284694372438916, "compression_ratio": 1.6179775280898876, "no_speech_prob": 4.860420631302986e-06}, {"id": 1236, "seek": 588010, "start": 5880.1, "end": 5882.1, "text": " Trigram and it will cut it off", "tokens": [1765, 33737, 293, 309, 486, 1723, 309, 766], "temperature": 0.0, "avg_logprob": -0.22241308251205755, "compression_ratio": 1.5682819383259912, "no_speech_prob": 2.0904478787997505e-06}, {"id": 1237, "seek": 588010, "start": 5883.46, "end": 5889.84, "text": " After the first 800,000 most common n grams n gram is just the generic word for", "tokens": [2381, 264, 700, 13083, 11, 1360, 881, 2689, 297, 11899, 297, 21353, 307, 445, 264, 19577, 1349, 337], "temperature": 0.0, "avg_logprob": -0.22241308251205755, "compression_ratio": 1.5682819383259912, "no_speech_prob": 2.0904478787997505e-06}, {"id": 1238, "seek": 588010, "start": 5890.740000000001, "end": 5892.740000000001, "text": " unigram by gram and trigram", "tokens": [517, 33737, 538, 21353, 293, 504, 33737], "temperature": 0.0, "avg_logprob": -0.22241308251205755, "compression_ratio": 1.5682819383259912, "no_speech_prob": 2.0904478787997505e-06}, {"id": 1239, "seek": 588010, "start": 5893.06, "end": 5898.34, "text": " so that's why the train term doc dot shape is now 25,000 by", "tokens": [370, 300, 311, 983, 264, 3847, 1433, 3211, 5893, 3909, 307, 586, 3552, 11, 1360, 538], "temperature": 0.0, "avg_logprob": -0.22241308251205755, "compression_ratio": 1.5682819383259912, "no_speech_prob": 2.0904478787997505e-06}, {"id": 1240, "seek": 588010, "start": 5899.34, "end": 5902.26, "text": " 800,000 and like if you're not sure what number this should be I", "tokens": [13083, 11, 1360, 293, 411, 498, 291, 434, 406, 988, 437, 1230, 341, 820, 312, 286], "temperature": 0.0, "avg_logprob": -0.22241308251205755, "compression_ratio": 1.5682819383259912, "no_speech_prob": 2.0904478787997505e-06}, {"id": 1241, "seek": 588010, "start": 5903.02, "end": 5908.06, "text": " Just picked something that was really big and you know didn't didn't worry about it too much", "tokens": [1449, 6183, 746, 300, 390, 534, 955, 293, 291, 458, 994, 380, 994, 380, 3292, 466, 309, 886, 709], "temperature": 0.0, "avg_logprob": -0.22241308251205755, "compression_ratio": 1.5682819383259912, "no_speech_prob": 2.0904478787997505e-06}, {"id": 1242, "seek": 590806, "start": 5908.06, "end": 5910.06, "text": " And it seemed to be fine like it's not terribly", "tokens": [400, 309, 6576, 281, 312, 2489, 411, 309, 311, 406, 22903], "temperature": 0.0, "avg_logprob": -0.22031033833821614, "compression_ratio": 1.6224066390041494, "no_speech_prob": 6.438981017709011e-06}, {"id": 1243, "seek": 590806, "start": 5910.860000000001, "end": 5912.860000000001, "text": " sensitive", "tokens": [9477], "temperature": 0.0, "avg_logprob": -0.22031033833821614, "compression_ratio": 1.6224066390041494, "no_speech_prob": 6.438981017709011e-06}, {"id": 1244, "seek": 590806, "start": 5914.1, "end": 5918.22, "text": " All right, okay, well that's we're out of time so what we're going to see", "tokens": [1057, 558, 11, 1392, 11, 731, 300, 311, 321, 434, 484, 295, 565, 370, 437, 321, 434, 516, 281, 536], "temperature": 0.0, "avg_logprob": -0.22031033833821614, "compression_ratio": 1.6224066390041494, "no_speech_prob": 6.438981017709011e-06}, {"id": 1245, "seek": 590806, "start": 5919.5, "end": 5921.740000000001, "text": " Next week and by the way, you know we could have", "tokens": [3087, 1243, 293, 538, 264, 636, 11, 291, 458, 321, 727, 362], "temperature": 0.0, "avg_logprob": -0.22031033833821614, "compression_ratio": 1.6224066390041494, "no_speech_prob": 6.438981017709011e-06}, {"id": 1246, "seek": 590806, "start": 5922.9400000000005, "end": 5927.9400000000005, "text": " Replaced this logistic regression with our pytorch version and next week", "tokens": [47762, 3839, 341, 3565, 3142, 24590, 365, 527, 25878, 284, 339, 3037, 293, 958, 1243], "temperature": 0.0, "avg_logprob": -0.22031033833821614, "compression_ratio": 1.6224066390041494, "no_speech_prob": 6.438981017709011e-06}, {"id": 1247, "seek": 590806, "start": 5927.9400000000005, "end": 5931.54, "text": " We'll actually see something in the fast AI library that does exactly that", "tokens": [492, 603, 767, 536, 746, 294, 264, 2370, 7318, 6405, 300, 775, 2293, 300], "temperature": 0.0, "avg_logprob": -0.22031033833821614, "compression_ratio": 1.6224066390041494, "no_speech_prob": 6.438981017709011e-06}, {"id": 1248, "seek": 590806, "start": 5932.14, "end": 5935.26, "text": " But also what we'll see next week. Sorry next week tomorrow", "tokens": [583, 611, 437, 321, 603, 536, 958, 1243, 13, 4919, 958, 1243, 4153], "temperature": 0.0, "avg_logprob": -0.22031033833821614, "compression_ratio": 1.6224066390041494, "no_speech_prob": 6.438981017709011e-06}, {"id": 1249, "seek": 590806, "start": 5935.9800000000005, "end": 5937.54, "text": " is", "tokens": [307], "temperature": 0.0, "avg_logprob": -0.22031033833821614, "compression_ratio": 1.6224066390041494, "no_speech_prob": 6.438981017709011e-06}, {"id": 1250, "seek": 593754, "start": 5937.54, "end": 5943.1, "text": " How to combine logistic regression and naive Bayes together to get something that's better than either", "tokens": [1012, 281, 10432, 3565, 3142, 24590, 293, 29052, 7840, 279, 1214, 281, 483, 746, 300, 311, 1101, 813, 2139], "temperature": 0.0, "avg_logprob": -0.18135845127390393, "compression_ratio": 1.4919786096256684, "no_speech_prob": 2.2252509097597795e-06}, {"id": 1251, "seek": 593754, "start": 5943.7, "end": 5946.9, "text": " and then we'll learn how to move from there to create a", "tokens": [293, 550, 321, 603, 1466, 577, 281, 1286, 490, 456, 281, 1884, 257], "temperature": 0.0, "avg_logprob": -0.18135845127390393, "compression_ratio": 1.4919786096256684, "no_speech_prob": 2.2252509097597795e-06}, {"id": 1252, "seek": 594690, "start": 5946.9, "end": 5967.219999999999, "text": " A deeper neural network to get pretty much state-of-the-art result for structured learning alright, so we'll see you then", "tokens": [50364, 316, 7731, 18161, 3209, 281, 483, 1238, 709, 1785, 12, 2670, 12, 3322, 12, 446, 1874, 337, 18519, 2539, 5845, 11, 370, 321, 603, 536, 291, 550, 51380], "temperature": 0.0, "avg_logprob": -0.22408326466878256, "compression_ratio": 1.1862745098039216, "no_speech_prob": 1.5205871022772044e-05}], "language": "en"}