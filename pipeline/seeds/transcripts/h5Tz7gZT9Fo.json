{"text": " Welcome to Lesson 10, or as somebody on the forum described it, Lesson 10 Mod 7, which is probably a clearer way to think about this. We're going to be talking about NLP. Before we do, let's do a quick review of last week. Last week, there's quite a few people who have flown here to San Francisco for this in-person course. I'm seeing them pretty much every day. They're working full-time on this, and quite a few of them are still struggling to understand the material from last week. So if you're finding it difficult, that's fine. One of the reasons I kind of put it up there up front is so that we've got something to cogitate about and think about and gradually work towards so that by Lesson 14 Mod 7, you'll get a second crack at it. But it's well worth it. There's so many pieces, so hopefully you can keep developing better understanding. To understand the pieces, you'll need to understand the shapes of convolutional layer outputs, and receptive fields, and loss functions, and everything. So it's all stuff that you need to understand for all of your deep learning studies anyway. So everything you do to develop an understanding of last week's lesson is going to help you with everything else. So one key thing I wanted to mention is we started out with something which is really pretty simple, which is a single object classifier. Single object bounding box without a classifier, and then single object classifier and bounding box. And anybody who's hopefully spent some time studying since Lesson 8 Mod 7 has got to the point where they understand this bit. Now the reason I mention this is because the bit where we go to multiple objects is actually almost identical to this, except we first have to solve the matching problem. We end up creating far more activations than we need for our number of bounding boxes, ground truth bounding boxes. So we match each ground truth object to a subset of those activations. And once we've done that, the loss function that we then do to each matched pair is almost identical to this loss function. So if you're feeling stuck, go back to Lesson 8 and make sure you understand the data set, the data loader, and most importantly the loss function from the end of Lesson 8 or the start of Lesson 9. So once we've got this thing which can predict the class and bounding box for one object, we went to multiple objects by just creating more activations. We had to then deal with the matching problem. Having dealt with the matching problem, we then basically moved each of those anchor boxes in and out a little bit and around a little bit so they tried to line up with particular ground truth objects. And we talked about how we took advantage of the convolutional nature of the network to try to have activations that had a receptive field that was similar to the ground truth object we were predicting. And Chloe Sultan provided this fantastic picture for her own notes, but she shared it with everybody which is lovely, to talk about what does SSD multihead.forward do line by line. And I partly wanted to show this to help you with your revision, but I also partly wanted to show this to say, doing this kind of stuff is very useful for you to do, like walk through and in whatever way helps you make sure you understand something. And you can see what Chloe's done here is she's focused particularly on the dimensions of the tensor at each point in the path as we gradually downsample using these drive-tube convolutions, making sure she understands why those grid sizes happen, and then understanding how the outputs come out of those. And so one thing you might be wondering is, how did Chloe calculate these numbers? So I don't know the answer, I haven't spoken to her, but obviously one approach would be like from first principles, just thinking through it. But then you want to know, am I right? And so this is where you've got to remember this pdb.setTrace idea. So I just went in just before class and went into SSD multihead.forward and entered pdb.setTrace and then I ran a single batch. And so I put the trace at the end and then I could just print out the size of all of these guys. So which by the way reminds me, last week there may have been a point where I said 21 plus 4 equals 26, which is not true in most universes. And by the way, when I code, I do that stuff, that's the kind of thing I do all the time. So that's why we have debuggers and know how to check things and do things in small little bits along the way. So this idea of putting a debugger inside your forward function and printing out the sizes is something which is damn super helpful. Or you could just put a print statement here as well. So I actually don't know if that's how Chloe figured it out, but that's how I would if I was her. And then we talked about increasing k, which is the number of anchor boxes for each convolutional grid cell, which we can do with different zooms and different aspect ratios. And so that gives us a plethora of activations and therefore predicted bounding boxes, which we then went down to a small number using non-maximum suppression. And I'll try to remember to put a link. There's a really interesting paper that one of our students told me about that I hadn't heard about, which is attempting to... You know, I've mentioned non-maximum suppression. It's like kind of hacky, kind of ugly, totally heuristic. I didn't even talk about the code because it seems kind of hideous. So somebody actually came up with a paper recently which attempts to do an end-to-end conv net to replace that NMS piece. So I'll put that paper up. Nobody's created a PyTorch implementation yet, so it would be an interesting project if anybody wanted to try that. One thing I've noticed in our study groups during the week is not enough people reading papers. What we are doing in class now is implementing papers. The papers are the real ground truth. And I think from talking to people, a lot of the reason people aren't reading papers is because a lot of people don't think they're capable of reading papers. They don't think they're the kind of people that read papers. But you are. You're here. We started looking at a paper last week and we read the words that were in English and we largely understood them. So if you actually look through this picture from SSD, carefully, you'll realize SSD multihead.forward is not doing the same as this. And then you might think, oh, I wonder if this is better. My answer is probably. Because SSD multihead.forward was the first thing I tried just to get something out there. But between this and the YOLO version 3 paper and stuff, there are probably much better ways. One thing you'll notice in particular is they use a smaller K, but they have a lot more sets of grids. 1x1, 3x3, 5x5, 10x10, 19x19, 38x38, 8700 per class, so a lot more than we had. So that would be an interesting thing to experiment with. Another thing I noticed is that whereas we had 4x4, 2x2, 1x1, which means there's a lot of overlap, like every set fits within every other set, in this case where you've got 1, 5, you don't have that overlap. So it might actually make it easier to learn. So there's lots of interesting things you can play with based on stuff that's either trying to make it closer to the paper or think about other things you could try that aren't in the paper or whatever. Perhaps the most important thing I would recommend is to put the code and the equations next to each other. Yes, Rachel. Question from the audience. Question of whether you could speak about the use cyclic learning rate argument in the fit function. We will get there, yes. So put the code and the equations from the paper next to each other. And draw in one of two groups. You're either a code person like me who's not that happy about math, in which case I start with the code and then I look at the math and I learn about how the math maps to the code and end up eventually understanding the math. All your PhD in stochastic differential equations like Rachel, whatever that means, in which case you can look at the math and then learn about how the code implements the math. But either way, unless you're one of those rare people who's equally comfortable in either world, you'll learn about one or the other. Now learning about code is pretty easy because there's documentation and we know how it's indexed and we know how to look it up and so forth. Sometimes learning the math is hard because the notation might seem hard to look up, but there's actually a lot of resources. For example, the list of mathematical symbols on Wikipedia is amazingly great. It has examples of them, explanations of what they mean, and tells you what to search for to find out more about it. Really terrific. If you Google for math notation cheat sheet, you'll find more of these kinds of terrific resources. So, over time, you do need to learn the notation, but as you'll see from the Wikipedia page, there's not actually that much of it. Obviously there's a lot of concepts behind it, but once you know the notation, you can then quickly look up the concept as it pertains to a particular thing you're studying. Nobody learns all of math and then starts machine learning. Everybody, even top researchers I know, when they're reading a new paper, will very often come to bits of math they haven't seen before and they'll have to go away and learn that bit of math. Another thing you should try doing is to recreate things that you see in the papers. So here was the key, most important, figure 1 from the focal loss paper, the retina paper. So recreate it. And very often, I put these challenges up on the forums. So keep an eye on the forums. So I put this challenge up there and within about 3 minutes, Serata had said done it in Microsoft Excel naturally, along with a lot more information than the original paper. The nice thing here is that she was actually able to draw a line showing at a 0.5 ground truth probability what's the loss for different amounts of gamma, which is kind of cool. And if you want to cheat, she's also provided Python code on the forum too. I did discover a minor bug in my code last week. The way that I was flattening out the convolutional activations did not line up with how I was using them in the loss function. Fixing that actually made it quite a bit better, so my motorbikes and cows and stuff are actually in the right place. So when you go back to the notebook, you'll see it's a little less bad than it was last time. So there's some quick coverage of what's gone before. Yes? Question, are you going to put the PowerPoint on GitHub? I'll put a subset of it on GitHub. And then secondly, usually when we down sample, we increase the number of filters or depth. When we're doing sampling from 77 to 44, why are we decreasing the number from 512 to 256? Why not decrease dimension in SSD head? Is it performance related? 7x7 to 4x4? I guess they've got the star of 70. It's because, well largely it's because that's kind of what the papers tend to do. We have a number of out-paths and we kind of want each one to be the same, so we don't want each one to have a different number of filters. And also this is what the papers did, so I was trying to match up with that with having these 256. It's a different concept because we're taking advantage of not just the last layer, but the layers before that as well. Life's easier if we make them more consistent. So we're now going to move to NLP. And so let me kind of lay out where we're going here. We've seen a couple of times now this idea of taking a pre-trained model, in fact we've seen it in every lesson, take a pre-trained model, whip off some stuff on the top, replace it with some new stuff, get it to do something similar. And so we've kind of dived in a little bit deeper to that, to say with conv-learner.pre-trained, it had a standard way of sticking stuff on the top which does a particular thing, which was classification. And then we learned actually we can stick any PyTorch module we like on the end and have it do anything we like with a custom head. And so suddenly you discover, wow, there's some really interesting things we can do. In fact, that reminds me, it reminds me, Yang Liu said, well, what if we did a different kind of custom head? And so the different custom head was, well, let's take the original pictures and rotate them and then make our dependent variable the opposite of that rotation basically and see if it can learn to un-rotate it. And this is like a super useful thing, obviously. In fact, I think Google Photos nowadays has this option that it will actually automatically rotate your photos for you. But the cool thing is, as Yang Liu shows here, you can build that network right now by doing exactly the same as our previous lesson, but your custom head is one that spits out a single number, which is how much to rotate by, and your dataset has a dependent variable, which is how much did you rotate by. So you suddenly realize with this idea of a backbone plus a custom head, you can do almost anything you can think about. So today we're going to look at the same idea and say, how does that apply to NLP? And then in the next lesson, we're going to go further and say, well, if NLP and computer vision kind of let you do the same basic ideas, how do we combine the two? And we're going to learn about a model that can actually learn to find word structures from images or images from word structures or images from images. And that will form the basis, if you wanted to go further, of doing things like going from an image to a sentence, that's called image captioning, or going from a sentence to an image, which will kind of start to do a phrased image. And so from there, we're going to go deeper then into computer vision to think about what other kinds of things we can do with this idea of a pre-trained network plus a custom head. And so we'll look at various kinds of image enhancement, like increasing the resolution of a low-res photo to guess what was missing, or adding artistic filters on top of photos, or changing photos of horses into photos of zebras and stuff like that. And then finally, that's going to bring us all the way back to bounding boxes again. And so to get there, we're going to first of all learn about segmentation, which is not just figuring out where a bounding box is, but figuring out what every single pixel in an image is part of. So this pixel is part of a person, this pixel is part of a car. And then we're going to use that idea, particularly an idea called UNET. And it turns out that this idea from UNET, we can apply to bounding boxes where it's called feature pyramids. Everything has to have a different name in every slightly different area. And we'll use that to hopefully get some even better results, some really good results actually with bounding boxes. So that's kind of our path from here. So it's all going to build on each other, but take us into lots of different areas. Now for NLP last part, we relied on a pretty great library called Torch Text. But as pretty great as it was, I've since then found the limitations of it too problematic to keep using it. As a lot of you complained on the forums, it's pretty damn slow. Partly that's because it's not doing parallel processing. And partly it's because it doesn't remember what you did last time and it does it all over again from scratch. And then it's kind of hard to do fairly simple things. Like a lot of you were trying to get into the toxic comment competition on Kaggle, which was a multi-label problem, and trying to do that with Torch Text. I eventually got it working, but it took me like a week of hacking away, which is kind of ridiculous. So to fix all these problems, we've created a new library called Fastai.text. Fastai.text is a replacement for the combination of Torch Text and Fastai.nlp. So don't use Fastai.nlp anymore. That's obsolete. It's slower, it's more confusing, it's less good in every way. But there's a lot of overlaps. Intentionally, a lot of the classes have the same names, a lot of the functions have the same names, but this is the non-Torch Text version. So we're going to work with IMDB again. So for those of you who have forgotten, go back and check out Lesson 4. Basically this is a dataset of Moji reviews. You remember we used it to find out whether we might enjoy Zombie Gettin or not, and we thought probably my kind of thing. So we're going to use the same dataset, and by default it calls itself aclimdb. So this is just the raw dataset that you can download. And as you can see, I'm doing from Fastai.text import star. There's no Torch Text and I'm not using Fastai.nlp. I'm going to use pathlib as per usual. We're going to learn about what these tags are later. So you might remember the basic path for NLP is that we have to take sentences and turn them into numbers. And there's a couple of steps to get there. So at the moment, somewhat intentionally, Fastai.text doesn't provide that many helper functions. It's really designed more to let you handle things in a fairly flexible way. So as you can see here, I wrote something called getTexts, which goes through each thing in classes. And these are the three classes that they have in IMDB. Negative, positive, and then there's another folder unsupervised. That's stuff they haven't gotten around to labeling yet. So I'm just going to call that a class for now. And so I just go through each one of those classes, and then I just find every file in that folder with that name, and I open it up and read it and chuck it into the end of this array. And as you can see, with pathlib, it's super easy to grab stuff and pull it in, and then the label is just whatever class I'm up to so far. So I'll go ahead and do that for the train bit, and I'll go ahead and do that for the test bit. So there's 70,000 in train, 25,000 in test, 50,000 of the train ones are unsupervised. We won't actually be able to use them when we get to the classification piece. So I actually find this much easier than the kind of torch text approach of having lots of layers and wrappers and stuff, because in the end, reading text files is not that hard. One thing that's always a good idea is to sort things randomly. It's useful to know this simple trick for sorting things randomly, particularly when you've got multiple things you have to sort the same way. In this case, we've got labels and texts. np.random.permutation, if you give it an integer, it gives you a random list from 0 up to and not including the number you give it in some random order. And so you can then just pass that in as an indexer to give you a list that's sorted in that random order. So in this case, it's going to sort train texts and train labels in the same random way. So that's a useful little idiom to use. So now I've got my texts and my labels sorted. I can go ahead and create a data frame from them. Why am I doing this? The reason I'm doing this is because there is a somewhat standard approach starting to appear for text classification datasets, which is to have your training set as a CSV file with the labels first and the text of the NLP document second, train.csv and test.csv. So basically it looks like this. You've got your labels and your texts, and then a file called classes.txt which just lists the classes. I think somewhat standard is in a reasonably recent academic paper, Yan LeCun and a team of researchers looked at quite a few datasets and they used this format for all of them. And so that's what I've started using as well for my recent paper. So what I've done is you'll find that this notebook, if you put your data into this format, the whole notebook will work every time. So rather than having 1000 different classes or formats and readers and writers and whatever, I've just said let's just pick a standard format and your job, your coders, you can do it perfectly well, is to put it in that format which is the CSV file. The CSV files have no header by default. Now you'll notice at the start here that I had two different paths. One was the classification path, one was the language model path. In NLP you'll see LM all the time. LM means language model in NLP. So the classification path is going to contain the information that we're going to use to create a sentiment analysis model. The language model path is going to contain the information we need to create a language model. So they're a little bit different. One thing that's different is that when we create the train.csv and the classification path, we remove everything that has a label of 2 because label of 2 is unsupervised. So we remove the unsupervised data from the classifier. We can't use it. So that means this is going to have actually 25,000 positive, 25,000 negative. The second difference is the labels. For the classification path, the labels are the actual labels. But for the language model, there are no labels, so we just use a bunch of zeros. That just makes it a little bit easier because we can use a consistent data frame format or CSV format. Now the language model, we can create our own validation set. So you've probably come across by now sklearn.modelSelection.trainTestSplit, which is a really simple little function that grabs a data set and randomly splits it into a training set and a validation set according to whatever proportion you specify. And so in this case, I concatenate my classification, training and validation together, so it's going to be 100,000 altogether, split it by 10%, so now I've got 90,000 training, 10,000 validation for my language model. So I'll go ahead and save that. So that's my basic get the data in a standard format for my language model and my classifier. So the next thing we need to do is tokenization. So tokenization means at this stage, we've got for a document, we've got a big long string and we want to turn it into a list of tokens, which are kind of a list of words, but not quite. For example, don't, we want to be doh nt, we probably want full stop to be a token, and so forth, right, so tokenization is something that we passed off to a terrific library called spaCy, partly terrific because an Australian wrote it, and partly terrific because it's good at what it does. We put a bit of stuff on top of spaCy, but the vast majority of the work is being done by spaCy. Before we pass it to spaCy, I've written this simple fixup function, which is basically each time I look at a different dataset, and I've looked at about a dozen in building this, everyone had different weird things that needed to be replaced. So here are all the ones I've come up with so far. Hopefully this will help you out as well. So I html and escape all the entities, and then there's a bunch more things I replace. Have a look at the result of running this on text that you put in and make sure there's not more weird tokens in there. It's amazing how many weird things people do to text. So basically I've got this function called getAll, which is going to go ahead and call getTexts, and getTexts is going to go ahead and do a few things, one of which is to apply that fixup that we just mentioned. So let's kind of look through this because there's some interesting things to point out. So I'm going to use pandas to open our train.csv from the language model path, but I'm passing in an extra parameter you may not have seen before called chunkSize. Python and pandas can both be pretty inefficient when it comes to storing and using text data. And so you'll see that very few people in NLP are working with large corpuses. And I think part of the reason is that traditional tools have just made it really difficult. You run out of memory all the time. So this process I'm showing you today I have used on corpuses of over a billion words successfully using this exact code. And so one of the simple tricks is to use this thing called chunkSize with pandas. What that means is that pandas does not return a data frame, but it returns an iterator that we can iterate through chunks of a data frame. And so that's why I don't say topTrain equals getTexts, but instead I call getAll which loops through the data frame. But actually what it's really doing is it's looping through chunks of the data frame. So each of those chunks is basically a data frame representing a subset of the data. And I'm working with NLP data, many times I come across data with foreign text or characters. Is it better to discard them or keep them? No, no, definitely keep them. And this whole process is Unicode. And I've actually used this on Chinese text. This is designed to work on pretty much anything. In general, most of the time it's not a good idea to remove anything. Like old-fashioned NLP approaches tend to do all this like lemmatization and all these kind of normalization steps to get rid of lowercase everything. But that's throwing away information which you don't know ahead of time whether it's useful or not. So don't throw away information. So we go through each chunk, each of which is a data frame, and we call getTexts. getTexts is going to grab the labels and make them into ints. It's going to grab then the texts. And I'll point out a couple of things. The first is that before we include the text, we have this beginning of stream token, which you might remember we used way back up here. There's nothing special about these particular strings of letters, they're just ones I figured don't appear in normal texts very often. So every text is going to start with xbos. Why is that? Because it's often really useful for your model to know when a new text is starting. For example, if it's a language model, you're going to concatenate all the text together and so it would be really helpful for it to know this article is finished and a new one started so I should probably forget some of that context now. Ditto is quite often texts have multiple fields, like a title, an abstract, and then the main document. So by the same token, I've got this thing here which lets us actually have multiple fields in our CSP. So this process is designed to be very flexible. And again, at the start of each one, we put a special field starts here token, followed by the number of the field that's starting here for as many fields as we have. Then we apply our fixup to it. And then most importantly, we tokenize it. And we tokenize it by doing a process-all multiprocessor. And so tokenizing tends to be pretty slow, but we've all got multiple cores in our machines now and some of the better machines on AWS and stuff can have dozens of cores. Here on our university computer, we've got 56 cores. So spaCy is not very amenable to multiprocessing, but I finally figured out how to get it to work. And the good news is it's all wrapped up in this one function now. And so all you need to pass to that function is a list of things to tokenize, which each part of that list will be tokenized on a different core. And so I've also created this function called partitionByCores, which takes a list and splits it into sublists. The number of sublists is the number of cores that you have in your computer. So on my machine, without multiprocessing, this takes about an hour and a half. And with multiprocessing, it takes about 2 minutes. So it's a really handy thing to have. And now that this code is here, feel free to look inside it and take advantage of it for your own stuff. Remember, we all have multiple cores even in our laptops, and very few things in Python take advantage of it unless you make an effort to make it work. So there's a couple of tricks to get things working quickly and reliably. As it runs, it prints out how it's going. And so here's the result of the end. Beginning of stream token, beginning of field number 1 token, here's the tokenized text. You'll see that the punctuation is on the whole now, a separate token. You'll see there's a few interesting little things. One is this. What's this tup? TupMGM, well MGM obviously was originally capitalized. But the interesting thing is that normally people either lowercase everything or they leave the case as is. Now if you leave the case as is, then screw you, all caps, and screw you, lowercase, are two totally different sets of tokens that have to be learned from scratch. Or if you lowercase them all, then there's no difference at all between screw you and screw you. So how do you fix this so that you both get the semantic impact of like, I'm shouting now, but not have every single word have to learn the shouted version versus the normal version. And so the idea I came up with, and I'm sure other people have done this too, is to come up with a unique token to mean the next thing is all uppercase. So then I lowercase it, so now whatever used to be uppercase is now lowercase, it's just one token, and then we can learn the semantic meaning of all uppercase. And so I've done a similar thing if you've got like 29 exclamation marks in a row, we don't learn a separate token for 29 exclamation marks. Instead I put in a special token for the next thing repeats lots of times, and then I put the number 29, and then I put the exclamation mark. And so there's a few little tricks like that. And if you're interested in LP, have a look at the code for tokenizer for these little tricks that I've added in, because some of them are kind of fun. So the nice thing with doing things this way is we can now just np.save that and load it back up later. So we don't have to recalculate all this stuff each time like we tend to have to do with TorchText or a lot of other libraries. So we've now got it tokenized. The next thing we need to do is to turn it into numbers, which we call numericalizing it. And the way we numericalize it is very simple. We make a list of all the words that appear in some order, and then we replace every word with its index into that list. The list of all the words that appear, or all the tokens that appear, we call the vocabulary. So here's an example of some of the vocabulary. The counter class in Python is very handy for this. It basically gives us a list of unique items and their counts. So here are the 25 most common things in the vocabulary. You can see there are things like apostrophe s and double quote and end of paragraph and stuff like that. Now generally speaking, we don't want every unique token in our vocabulary. If it doesn't appear at least 2 times, then it might just be a spelling mistake or a word. We can't learn anything about it if it doesn't appear that often. Also the stuff that we're going to be learning about at least so far in this part gets a bit clunky once you've got a vocabulary bigger than 60,000. Time permitting, we may look at some work I've been doing recently on handling larger vocabularies, otherwise that might have to come in a future course. But actually for classification, I've discovered that doing more than about 60,000 words doesn't seem to help anyway. So we're going to limit our vocabulary to 60,000 words, things that appear at least twice. And so here's a simple way to do that. Use that dot most common, pass in the max vocab size. That'll sort it by the frequency, by the way. And if it appears less often than a minimum frequency, then don't bother with it at all. So that gives us I to S, that's the same name that Torch Text used. Remember it means int to string. So this is just the list of the unique tokens in the vocab. I'm going to insert 2 more tokens. A token for unknown, a vocab item for unknown, and a vocab item for padding. Then we can create the dictionary which goes in the opposite direction, so string to int. And that won't cover everything because we intentionally truncated it down to 60,000 words. And so if we come across something that's not in the dictionary, we want to replace it with 0 for unknown. So we can use a default dict for that with a lambda function that always returns 0. So you can see all these things we're using that keep coming back up. So now that we've got our S2I dictionary defined, we can then just call that for every word, for every sentence. And so there's our numericalized version, and there it is. And so of course the nice thing is again, we can save that step as well. So each time we get to another step, we can save it. And these are not very big files compared to what you're used to with images. Text is generally pretty small. Very important to also save that vocabulary, because this list of numbers means nothing unless you know what each number refers to, and that's what I2S tells you. So you save those 3 things, and then later on you can load them back up. So now our vocab size is 60,002, and our training language model has 90,000 documents in it. So that's the preprocessing you do. We can probably wrap a little bit more of that in utility functions if we want to, but it's all pretty straightforward and basically that exact code will work for any dataset you have once you've got it in that CSV format. So here is a kind of a new insight that's not new at all, which is that we'd like to pre-train something. We know from Lesson 4 that if we pre-train our classifier by first creating a language model and then fine-tuning that as a classifier, that was helpful. Remember it actually got us a new state-of-the-art result. We got the best IMDB classifier result that had ever been published, but quite a bit. But we're not going far enough though, because IMDB movie reviews are not that different to any other English document compared to how different they are to a random string or even to a Chinese document. So just like ImageNet allowed us to train things that recognize stuff that kind of looks like pictures and we could use it on stuff that has nothing to do with ImageNet like satellite images, why don't we train a language model that's just like good at English and then fine-tune it to be good at movie reviews. So this basic insight led me to try building a language model on Wikipedia. So my friend Stephen Merrity has already processed Wikipedia, found a subset of nearly the most of it, but throwing away the stupid little articles, so most of the bigger articles, and he calls that Wikitext 103. So I grabbed Wikitext 103 and I trained a language model on it. And I used exactly the same approach I'm about to show you for training an IMDB language model, but instead I trained a Wikitext 103 language model. And then I saved it and I've made it available for anybody who wants to use it at this URL. So this is not a URL for Wikitext 103, the documents, this is the Wikitext 103, the language model. So the idea now is let's train an IMDB language model which starts with these words. Now hopefully to you folks, this is an extremely obvious, extremely non-controversial idea because it's basically what we've done in nearly every class so far. But when I first mentioned this to people in the NLP community, I guess June, July of last year, there couldn't have been less interest. I asked on Twitter, where a lot of the top Twitter researchers are people that I follow and they follow me back, I was like, hey, what if we pre-trained a general language model? And they're like, no, our language is different, you can't do that, or I don't know why you would bother anyway. I've talked to people at conferences and they're like, I'm pretty sure people have tried that and it's stupid. There was just this like, I don't know, this weird straight past. And I guess because I am arrogant and it's de-stress, I ignored them even though they know much more about NLP than I do and just tried it anyway. And let me show you what happened. So here's how we do it. Grab the wiki text models. And if you use wget-r, it will actually recursively grab the whole directory. We need to make sure that our language model has exactly the same embedding size, number of hidden, and number of layers as my wiki text one did. So here's our pre-trained path, here's our pre-trained language model path. Let's go ahead and torch.load in those weights from the forward wiki text 103 model. We don't normally use torch.load, but that's the PyTorch way of grabbing a file. It basically gives you a dictionary containing the name of the layer and a tensor of those weights, or an array of those weights. Now here's the problem. That wiki text language model was built with a certain vocabulary which was not the same as this one was built on. So my number 40 was not the same as wiki text 103 model's number 40. So we need to map one to the other. That's very, very simple because luckily I saved the I2S for the wiki text vocab. So here's the list of what each word is when I trained the wiki text 103 model. And so we can do the same default dict trick to map it in reverse. I'm going to use minus 1 to mean that it's not in the wiki text dictionary. And so now I can just say, okay, my new set of weights is just a whole bunch of zeros with vocab size by embedding size. So we're going to create an embedding matrix. I'm then going to go through every one of the words in my IMDB vocabulary. I'm going to look it up in S to I2, so string to int for the wiki text 103 vocabulary, and see if that word's there. And if that is word there, then I'm not going to get this minus 1, so R will be greater than or equal to 0. So in that case, I will just set that row of the embedding matrix to the weight that I just looked at, which was stored inside this named element. And so these names, you can just look at this dictionary and it's pretty obvious what each name corresponds to because it looks very similar to the names that you gave it when you set up your module. So here are the encoder weights. So I'll grab it from the encoder weights. If I don't find it, then I will use the row mean. In other words, here is the average embedding weight across all of the wiki text 103 things. So that's pretty simple. So I'm going to end up with an embedding matrix for every word that's in both my vocabulary for IMDB and the wiki text 103 vocab. I will use the wiki text 103's embedding matrix weights. For anything else, I will just use whatever was the average weight from the wiki text 103 embedding matrix. And then I'll go ahead and replace the encoder weights with that turned into a tensor. We haven't talked much about weight tying, we might do so later. But basically the decoder, so the thing that turns the final prediction back into a word, uses exactly the same weights. So I pop it there as well. And then there's a bit of a weird thing with how we do embedding dropout that ends up with a whole separate copy of them for a reason that doesn't matter much. So we just pop the weights back where they need to go. So this is now something that a dictionary we can now, or a set of torch state which we can load in. So let's go ahead and create our language model. And so the basic approach we're going to use, and I'm going to look at this in more detail in a moment, is I'm going to concatenate all of the documents together into a single list of tokens of length 24.998 million. So that's going to be what I pass in as my training set. So the language model, we basically just take all our documents and just concatenate them back to back. And we're going to be continuously trying to predict what's the next word after these words. What's the next word after these words. And we'll look at these details in a moment. I'm going to set up a whole bunch of dropout. Once we've got a model data object, we can then grab the model from it. So that's going to give us a learner. And then as per usual, we can call learner.fit. So we first of all, as per usual, just do a single epoch on the last layer just to get that okay. And the way I've set it up is the last layer is actually the embedding weights. Because that's obviously the thing that's going to be the most wrong, because a lot of those embedding weights didn't even exist in the vocab. So we're just going to train a single epoch of just the embedding weights. And then we'll start doing a few epochs of the full model. And so how is that looking? Well here's lesson 4, which was our academic world's best ever result. And after 14 epochs, we had a 4.23 loss. Here after 1 epoch, we have a 4.12 loss. So by pre-training on Wikitext 103, in fact let's go and have a look, we kept training and training at a different rate, eventually we got to 4.16. So by pre-training on Wikitext 103, we have a better loss after 1 epoch than the best loss we got for the language model otherwise. Yes, Rachel. What is the Wikitext 103 model? Is it AWD LSTM again? And we're about to dig into that. The way I trained it was literally the same lines of code that you see here, but without pre-training it on Wikitext 103. So let's take a 10 minute break, come back at 7.40 and we'll dig in and have a look at these models. Okay, welcome back. Before we go back into language models and NLP classifiers, a quick discussion about something pretty new at the moment, which is the FastAI.doc project. So the goal of the FastAI.doc project is to create documentation that makes readers say, wow, that's the most fantastic documentation I've ever read. And so we have some specific ideas about how to do that, but it's the same kind of idea of like top-down, thoughtful, take full advantage of the medium approach, interactive experimental code first that we're all familiar with. If you're interested in getting involved, the basic approach you can see in the docs directory. So this is the README in the docs directory. In there, there is amongst other things a transforms-template.adoc. What the hell is adoc? adoc is ASCII doc. How many people here have come across ASCII doc? That's awesome. ASCII doc is, people are laughing because there's one hand up and it's somebody who was in our study group today who talked to me about ASCII doc. ASCII doc is the most amazing project. It's like Markdown, but it's like what Markdown needs to be to create actual books. Like a lot of actual books are written in ASCII doc. And so it's as easy to use as Markdown, but there's way more cool stuff you can do with it. In fact, here is an ASCII doc file here. And as you'll see, it looks very normal. There's headings. This is pre-formatted text. And there's lists and whatever else. It looks pretty standard. And actually I'll show you a more complete ASCII doc thing. A more standard ASCII doc thing. But you can do stuff like say put a table of contents here please. You can say colon colon means put a definition list here please. Plus means this is a continuation of the previous list item. So there's just like little things that you can do which are super handy. Or like put this thing, make it slightly smaller than everything else. So it's like turbocharged Markdown. And so this ASCII doc creates this HTML. And I didn't add any CSS or do anything myself. We literally started this project like 4 hours ago. So this is just an example basically. And so you can see we've got a table of contents. We can jump straight to here. We've got a cross-reference we can click on to jump straight to the cross-reference. Each method kind of comes along with its details and so on and so forth. And to make things even easier, rather than having to know that this is meant to be like the argument list is meant to be smaller than the main part or how do you create a cross-reference or how are you meant to format the arguments to the method name and list out each one of its arguments. We've created a special template where you can just write various stuff in curly brackets like please put the arguments here and here is an example of one argument and here is a cross-reference and here is a method and so forth. So we're in the process of documenting the documentation template but there's basically like 5 or 6 of these little curly bracket things you'll need to learn. But for you to create the documentation of a class or a method, you can just copy one that's already there basically. And so the idea is we're going to have like, it'll almost be like a book. There'll be tables and pictures and little video segments and hyperlink throughout and all that stuff. You might be wondering what about docstrings. But actually I don't know if you've noticed, but if you look at the Python standard library and look at the docstring for example for regex compile, it's a single line. Nearly every docstring in Python is a single line. And Python then does exactly this. They have then a website containing the documentation that says like, hey, this is what regular expressions are and this is what you need to know about them and if you want them to go faster, you need to use compile and here's lots of information about compile and here's the examples. It's not in the docstring. And that's how we're doing it as well. Our docstrings will be one line unless you need like two sometimes. It's going to be very similar to Python, but even better. So everybody is welcome to help contribute to the documentation and hopefully by the time you're watching this on the MOOC, it'll be reasonably fleshed out and we'll try to keep a list of things to do. Alright, so I'm going to do one first. So one question that came up in the break was how does this compare to Word2Vec? And this is actually a great thing for you to spend time thinking about during the week, is how does this compare to Word2Vec. I'll give you the summary now, but it's a very important conceptual difference. The main conceptual difference is what is Word2Vec? Word2Vec is a single embedding matrix. Each word has a vector and that's it. So in other words, it's a single layer from a pre-trained model. Specifically, that layer is the input layer. And also specifically, that pre-trained model is a linear model that is pre-trained on something called a co-occurrence matrix. So we have no particular reason to believe that this model has learned anything much about the English language, or that it has any particular capabilities because it's just a single linear layer and that's it. So what's this Wikitext 103 model? It's a language model. It has a 400-dimensional embedding matrix, 3 hidden layers with 1150 activations per layer and regularization and all of that stuff, tied input, output, matrices, it's basically a state-of-the-art AWD ISTM. So what's the difference between a single layer of a single linear model versus a 3-layer recurrent neural network? Everything. They're very different levels of capability. And so you'll see when you try using a pre-trained language model versus a Word2Vec layer, you'll get very, very different results for the vast majority of tasks. What if the NumPy array does not fit in memory? Is it possible to write a PyTorch data loader directly from a large CSV file? It almost certainly won't come up, so I'm not going to spend time on it. These things are tiny. They're just ints. Think about how many ints you would need to write out a memory. It's not going to happen. They don't have to fit in GPU memory, just in your memory. So I've actually done another Wikipedia model, which I called Gigawiki, which was on all of Wikipedia, and even that easily fits in memory. The reason I'm not using it is because it turned out not to really help very much versus Wikitext 103, but I've built a bigger model than anybody else I've found in the academic literature pretty much, and it fits in memory on a single machine. What is the idea behind averaging the weights of embeddings? They've got to be set to something. There are words that weren't there. So other options is we could leave them at 0. That seems like a very extreme thing to do. Like 0 is a very extreme number. Why would it be 0? We could set it equal to some random numbers, but if so, what would be the mean and standard deviation of those random numbers, or should they be uniform? If we just average the rest of the embeddings, then we have something that's a reasonable scale. Just to clarify, this is how you're initializing words that didn't appear in the training corpus. I think you've pretty much just answered this one, but someone had asked if there's a specific advantage to creating our own pre-trained embedding over using GloVe or Word2Vec. I think I have. We're not creating a pre-trained embedding, we're creating a pre-trained model. Let's talk a little bit more. This is the kind of stuff we've seen before, but it's changed a little bit. It's actually a lot easier than it was in Part 1, but I want to go a little bit deeper into the language model loader. So this is the language model loader, and I really hope that by now you've learned in your editor or IDE how to jump to symbols. I don't want it to be a burden for you to find out what the source code of language model loader is. If it's still a burden, please go back and try and learn those keyboard shortcuts in VS Code. If your editor doesn't make it easy, don't use that editor anymore. There's lots of good free editors that make this easy. So here's the source code for language model loader. It's interesting to notice that it's not doing anything particularly tricky. It's not deriving from anything at all. What makes it something that's capable of being a data loader is that it's something you can iterate over. And so specifically, here's the fit function inside fastai.model, where everything ends up eventually, which goes through each epoch, and then it creates an iterator from the data loader, and then it just does a for loop through it. So anything you can do a for loop through can be a data loader. Basically it needs to return tuples of many batches, independent variable for many batches. So anything with a dunder-etter method is something that can act as an iterator. And yield is a neat little Python keyword. You probably should learn about it if you don't already know it, but it basically spits out a thing and waits for you to ask for another thing. So in this case, we start by initializing the language model, passing it in the numbers. So this is the numericalized big long list of all of our documents concatenated together. And the first thing we do is to batchify it. And this is the thing which quite a few of you got confused about last time. If our batch size is 64, and we have 25 million numbers in our list, we are not creating items of length 64. We're not doing that. We're creating 64 items in total. So each of them is of size t divided by 64, which is 390,000. So that's what we do here when we reshape it so that this axis here is of length 64, and then this minus 1 is everything else. So that's 390,000 long. And then we transpose it. So that means that we now have 64 columns, 390,000 rows, and then what we do each time we do an iterate is we grab one batch of some sequence length, we'll look at that in a moment, but basically it's approximately equal to BPTT, which we set to 70, stands for back prop through time. And we just grab that many rows. So from i to i plus 70 rows, and then we try to predict that plus 1. So we're trying to predict 1 past where we're up to. So we've got 64 columns, and each of those is 1 64th of our 25 million tokens, hundreds of thousands long, and we just grab 70 at a time. So each of those columns, each time we grab it, is going to hook up to the previous column. And so that's why we get this consistency, this language model. It's stateful, it's really important. Pretty much all the cool stuff in the language model is stolen from Stephen Meridy's AWD LSTM, including this little trick here. If we always grab 70 at a time, and then we go back and we do a new epoch, we're going to grab exactly the same batches every time. There's no randomness. Now normally we shuffle our data every time we do an epoch, or every time we grab some data, we grab it at random. But you can't do that with a language model because this set has to join up to the previous set because it's trying to learn the sentence. And if you suddenly jump somewhere else, then that doesn't make any sense as a sentence. So Stephen's idea is to say, since we can't shuffle the order, let's instead randomly change the size, the sequence length. And so basically he says, 95% of the time we'll use BPTT, 70, but 5% of the time we'll use half that. And then he says, you know what, I'm not even going to make that the sequence length, I'm going to create a normally distributed random number with that average and a standard deviation of 5, and I'll make that the sequence length. So the sequence length is 70-ish. And that means every time we go through, we're getting slightly different batches. So we've got that little bit of extra randomness. I asked Stephen Meridy where he came up with this idea, did he think of it, and he was like, I think I thought of it, but it seems so obvious that I bet I didn't think of it. Which is true of every time I come up with an idea in deep learning, it always seems so obvious that you assume somebody else has thought of it, but I think he thought of it. So this is a nice thing to look at if you're trying to do something a bit unusual with a data loader, it's like, okay, here's a simple kind of role model you can use as to creating a data loader from scratch, something that spits out batches of data. So our language model loader just took in all of the documents concatenated together along with the batch size and the BPTT. Now generally speaking, we want to create a learner, and the way we normally do that is by getting a model data object and by calling some kind of method which have various names, but often we call that method getModel. And so the idea is that the model data object has enough information to know what kind of model to give you. So we have to create that model data object, which means we need that class. And so that's very easy to do. So here are all of the pieces. We're going to create a custom learner, a custom model data class, and a custom model class. So a model data class, again, this one doesn't inherit from anything, so you really see there's almost nothing to do. You need to tell it, most importantly, what's your training set? Give it a data loader. What's the validation set? Give it a data loader. And optionally, give it a test set. Data loader. Plus anything else it needs to know. So it might need to know the BPTT. It needs to know the number of tokens, that's the vocab size. It needs to know what is the padding index. And so that it can save temporary files and models, model data always needs to know the path. So we just grab all that stuff and we dump it. And that's it. That's the entire initializer. There's no logic there at all. So then all of the work happens inside GetModel. And so GetModel calls something we'll look at later, which just grabs a normal PyTorch nn.module architecture and chucks it on the GPU. Note with PyTorch, normally we would say.cuda. With FastAI, it's better to say to GPU. And the reason is that if you don't have a GPU, it'll leave it on the CPU. And it also provides a global variable you can set to choose whether it goes on the GPU or not. So it's a better approach. So we wrap the model in a language model. And the language model is this. Basically a language model is a subclass of basic model. It basically almost does nothing except it defines layer groups. And so remember how when we do like discriminative learning rates where different layers have different learning rates, or like we freeze different amounts, we don't provide a different learning rate for every layer because there can be like a thousand layers. We provide a different learning rate for every layer group. So when you create a custom model, you just have to override this one thing which returns a list of all of your layer groups. So in this case, my last layer group contains the last part of the model and one bit of dropout. And the rest of it, this star here means pull this apart. So this is basically going to be one layer per RNN layer. So that's all that is. And then finally, turn that into a learner. And so a learner you just pass in the model and it turns it into a learner. In this case we have overridden learner. And the only thing we've done is to say I want the default loss function to be cross-entropy. So this entire set of custom model, custom model data, custom learner all fits on a single screen and they always basically look like this. So that's a kind of little dig inside this pretty boring part of the code base. So the interesting part of this code base is getLanguageModel. getLanguageModel is actually the thing that gives us our AWD LSTM. And it actually contains the big idea. The big incredibly simple idea that everybody else here thinks is really obvious, that everybody in the NLP community I spoke to thought was insane, which is basically every model can be thought of as a backbone plus a head. And if you pre-train the backbone and stick on a random head, you can do fine-tuning and that's a good idea. And so these two bits of the code are literally right next to each other. This is kind of all there is inside this bit of fastai.lmrn. Here's getLanguageModel, here's getClassifier. getLanguageModel creates an RNN encoder and then creates a sequential model that sticks on top of that, a linear decoder. getClassifier creates an RNN encoder and then a sequential model that sticks on top of that, a pooling linear classifier. We'll see what these differences are in a moment, but you get the basic idea. They're basically doing pretty much the same thing. They've got this head and they're sticking on a simple linear layer on top. So it's worth digging in a little bit deeper and seeing what's going on here. Question about whether any of this translates to other languages. This whole thing works in any language you like. Would you have to retrain your language model on a corpus from that language? So the WikiText 103 pretrained language model knows English. You could use it maybe as a pre-trained start for a French or German model, start by retraining the embedding layer from scratch. Might be helpful. Chinese, maybe not so much. But given that a language model can be trained from any unlabeled documents at all, you never have to do that. Because almost every language in the world has plenty of documents you can grab. Newspapers, webpages, parliamentary records, whatever. As long as you've got a few thousand documents showing somewhat normal usage of that language, you can create a language model. And so I know some of our students, one of our students, I have to look up during the week, very embarrassing, tried this approach for Thai. He said the first model he built easily beat the previous data-the-art Thai classifier. For those of you that are international fellows, this is an easy way for you to whip out a paper in which you either create the first ever classifier in your language or beat everybody else's classifier in your language. And then you can tell them that you've been a student of deep learning for 6 months and piss off all the academics in your country. So here's our identity coder. It's just a standard NN.module. Most of the text in it is actually just documentation, as you can see. It looks like there's more going on in it than there actually is. But really all there is is we create an embedding layer, we create an LSTM for each layer that's been asked for, and that's it. Everything else in it is dropout. Basically all of the interesting stuff in the AWED LSTM paper is all of the places you can put dropout. And then the forward is basically the same thing. Call the embedding layer, add some dropout, go through each layer, call that RNN layer, append it to our list of outputs, add dropout, and that's about it. So it's really pretty straightforward. And the paper you want to be reading, as I've mentioned, is the AWD LSTM paper, which is this one here, Regularizing and Optimizing LSTM Language Models. And it's well written and pretty accessible and entirely implemented inside FastAI as well, so you can see all of the code for that paper. And like a lot of the code is shamelessly plagiarized with Stephen's permission from his excellent GitHub repo, AWD LSTM, and the process of which I picked some of his bugs as well. I even told him about them. I'm talking increasingly about Please Read the Papers, so here's the paper, Please Read this paper. And it refers to other papers. So for things like, why is it that the encoder weight and the decoder weight are the same? Well it's because there's this thing called tie weights, this is inside that Get Language Model, there's a thing called tie weights that defaults to true. And if it's true, then we actually tie, we literally use the same weight matrix for the encoder and the decoder. So they're literally pointing at the same block of memory. And so why is that, what's the result of it, that's one of the citations in Stephen's paper, which is also a well written paper, you can go and look up and learn about work time. So there's a lot of cool stuff in there. Okay, so we have basically a standard RNN, the only reason why it's not standard is it's just got lots more types of dropout in it. And then a sequential model, on top of that we stick a linear decoder, which is literally half the screen of code. It's got a single linear layer, we initialize the weights to some range, we add some dropout, and that's it. So it's a linear layer with dropout. So we've got an RNN, on top of that we stick a linear layer with dropout, and we're finished. So that's the language model. So what dropout you choose matters a lot. And through a lot of experimentation, I found a bunch of dropouts. You can see here we've got each of these corresponds to a particular argument. A bunch of dropouts tend to work pretty well for language models. But if you have less data for your language model, you'll need more dropout. If you have more data, you can benefit from less dropout. You don't want to regularize more than you have to. Makes sense, right? Other than having to tune every one of these 5 things, my claim is they're already pretty good ratios to each other, so just tune this number. I just multiply it all by something. So there's really just one number you have to tune. So if you're overfitting, then you'll need to increase this number. If you're underfitting, you'll need to decrease this number. Other than that, these ratios actually seem pretty good. So one important idea, which may seem pretty minor, but again it's incredibly controversial, is that we should measure accuracy when we look at a language model. So normally language models, we look at this loss value, which is just cross-entropy loss. Specifically, we nearly always take e to the power of that, which the NLP community calls perplexity. So perplexity is just e to the power of cross-entropy. There's a lot of problems with comparing things based on cross-entropy loss. I'm not sure I've got time to go into it in detail now, but the basic problem is that it's kind of like that thing we learned about focal loss. Cross-entropy loss, if you're right, it wants you to be really confident that you're right. So it really penalizes a model that doesn't say, I'm so sure this is wrong. Whereas accuracy doesn't care at all about how confident you are, it just kind of cares about whether you're right. And this is much more often the thing which you care about in real life. So this accuracy is how often do we guess the next word correctly. And I just find that a much more stable number to keep track of. So that's a simple little thing that I do. So we train for a while, and we get down to a 3.9 cross-entropy loss. And if you go e to the power of that, and to give you a sense of what's happened with language models, if you look at academic papers from about 18 months ago, you'll see them talking about perplexities, state-of-the-art perplexities of over 100. The rate at which our ability to understand language, and I think measuring language model accuracy or perplexity is not a terrible proxy for understanding language. If I can guess what you're going to say next, I pretty much need to understand language pretty well, and also the kind of things you might talk about pretty well. So this number has just come down so much. It's been amazing, NLP in the last 12-18 months, and it's going to come down a lot more. It really feels like 2011-2012 computer vision. We're just starting to understand transfer learning and fine-tuning, and these basic models are getting so much better. So everything you thought about, like what NLP can and can't do, is very rapidly going out of date. There's still lots of stuff NLP is not good at, to be clear. Just like in 2012, there was lots of stuff computer vision wasn't good at. But it's changing incredibly rapidly, and now is a very, very good time to be getting very, very good at NLP, or starting startups based on NLP, because there's a whole bunch of stuff which computers would absolutely shit at two years ago, and now are not quite as good at people, and then next year they'll be much better than people. Two questions. One, what is your ratio of paper reading versus coding in a week? What do you think, Rachel? You see me. I mean it's a lot more coding, right? It's a lot more coding. I feel like it also really varies from week to week. Like with that bounding box stuff, there was all these papers and no map through them, and so I didn't even know which one to read first, and then I'd read the citations and didn't understand any of them. So there was a few weeks of just kind of reading papers before I even knew what to start coding. That's unusual though. Like most of the time, I don't know. Anytime I start reading a paper, I'm always convinced that I'm not smart enough to understand it, always, regardless of the paper, and somehow eventually I do. But yeah, I try to spend as much time as I can coding. The second question, is your dropout rate the same through the training or do you adjust it and the weights accordingly? I'll just say one more thing about the last bit, which is very often, like the vast majority, nearly always, after I've read a paper, even after I've read the bit that says this is the problem I'm trying to solve, I'll kind of stop there and try to implement something that I think might solve that problem, and then I'll go back and read the paper and I'll read little bits about how I solved these problem bits, and I'll be like, oh that's a good idea, and then I'll try to implement those. And so that's why, for example, I didn't actually implement SSD, my custom head is not the same as their head, it's because I kind of read the gist of it and then I tried to create something best as I could and then go back to the papers and try to see why. So by the time I got to the focal loss paper, Rachel will tell you, I was driving myself crazy with like, how come I can't find small objects, how come it's always predicting background, you know, and I read the focal loss paper and I was like, that's why! You know, so it's so much better when you deeply understand the problem they're trying to solve. And I do find the vast majority of the time, by the time I read that bit of the paper, which is like solving the problem, I'm then like, yeah, but these 3 ideas I came up with, they didn't try. You know, and you suddenly realize that you've got new ideas. Or else if you just implement the paper mindlessly, you tend not to have these insights about better ways to do it. Varying dropout is really interesting and there are some recent papers actually that suggest gradually changing dropout. And it was either a good idea to gradually make it smaller or to gradually make it bigger, I'm not sure which. Maybe one of us can try and find it during the week. I haven't seen it widely used. I tried it a little bit with the most recent paper I wrote and it had some good results. I think I was gradually making it smaller. The next question is, am I correct in thinking that this language model is built on word embeddings? Would it be valuable to try this with phrase or sentence embeddings? I asked this because I saw from Google the other day, universal sentence encoder. Yes, this is much better than that. This is not just an embedding of a sentence, this is an entire model. So an embedding by definition is like a fixed thing. I think they're asking, they're saying that this language, the first question is, is this language model built on word embeddings? But as I was saying, a sentence or a phrase embedding is always a model that creates that. We've got a model that's trying to understand language. It's not just a phrase, it's not just a sentence, it's a document in the end and it's not just an embedding, we're training through the whole thing. So this has been a huge problem with NLP for years now, is this attachment they have to embeddings. So even the paper that the community has been most excited about recently from AI2, the Allen Institute, called ELMO, E-L-M-O, they found much better results across lots of models. But again, it was an embedding. They took a fixed model and created a fixed set of numbers which they then fed into a model. But in computer vision, we've known for years that that approach of having a fixed set of features, they're called hypercolons in computer vision. People stopped using them like 3 or 4 years ago because fine-tuning the entire model works much better. So for those of you that have spent quite a lot of time with NLP and not much time with computer vision, you're going to have to start relearning. All that stuff you have been told about this idea that there are these things called embeddings and that you learn them ahead of time and then you apply these fixed things, whether it be word level or phrase level or whatever level, don't do that. You want to actually create a pre-trained model and fine-tune it. You'll see some specific results. For using accuracy instead of perplexity as a metric for the model, could we work that into the loss function rather than just use it as a metric? No, you never want to do that whether it be computer vision or NLP or whatever. It's too bumpy. So cross-entropy is fine as a loss function. And I'm not saying instead of, I use it in addition to. I think it's good to look at the accuracy and to look at the cross-entropy. But for your loss function, you need something nice and smooth. Accuracy doesn't work very well. You'll see there's two different versions of save. There's save and saveencoder. Save saves the whole model as per usual. Saveencoder saves just that bit. In other words, in the sequential model, it saves just that bit and not that bit. In other words, this bit, which is the bit that actually makes it into a language model, we don't care about in the classifier. We just care about that bit. So that's why we save two different models here. So let's now create the classifier. And I'm going to go through this bit pretty quickly because it's the same. But when you go back during the week and look at the code, convince yourself it's the same. We do getAll, pd, readCSV again, chuckSize again, getAll again, save those tokens again. We don't create a new I2S vocabulary. We obviously want to use the same vocabulary we had in the language model because we're about to reload the same encoder. Same default dict, same way of creating our numericalized list, which as per before, we can save. So that's all the same. Later on we can reload those rather than having to rebuild them. So all of our hyperparameters are the same. The construction of the model hyperparameters are the same. We can change the dropout. We can change the size of function. Pick a batch size that is as big as you can that doesn't run out of memory. And so this bit's a bit interesting. There's some fun stuff going on here. The basic idea here is that for the classifier, we do really want to look at our document. We need to say is this document positive or negative. And so we do want to shuffle the documents. Because we like to shuffle things. But those documents are different lengths, so if we stick them all into one batch, this is a handy thing that FastAI does for you, you can stick things of different lengths into a batch and it will automatically pad them. So you don't have to worry about that. But if they're wildly different lengths, then you're going to be wasting a lot of computation time, there might be one thing there that's 2000 words long and everything else is 50 words long, and that means you end up with a 2000-wide tensor. That's pretty annoying. So James Bradbury, who's actually one of Stephen Murady's colleagues and the guy who came up with TorchText, came up with a neat idea, which was let's sort the dataset by length ish. So kind of make it so the first things in the list are on the whole shorter than the things at the end, but a little bit random as well. And so I'll show you how I implemented that. So the first thing we need is a dataset. And so we have a dataset passing in the documents and their labels. And so here's a text dataset and it inherits from dataset. Here is dataset from torch, from PyTorch. And actually, dataset doesn't do anything at all. It says you need to get item, if you don't have one, you're going to get an error, you need a length, if you don't have one, you're going to get an error. So this is an abstract class. So we're going to pass in our x, we're going to pass in our y, and get item is going to grab the x and grab the y and return them. It couldn't be much simpler. Optionally it could reverse it, optionally it could stick an end of stream at the end, optionally it could stick a start of stream at the beginning. We're not doing any of those things. So literally all we're doing is we're putting in an x, putting in a y, and then grab an item, we're returning the x and the y as a double. And the length is however long the x for a is. So that's all a dataset is. Something with a length that you can index. So to turn it into a data loader, you simply pass the dataset to the data loader constructor and it's now going to go ahead and give you a batch of that at a time. Normally you can say shuffle equals true or shuffle equals false, it will decide whether to randomize it for you. In this case though, we're actually going to pass in a sampler parameter. The sampler is a class we're going to define that tells the data loader how to shuffle. So for the validation set, we're going to define something that actually just sorts it. It just deterministically sorts it so all the shortest documents will be at the start, all the longest documents will be at the end, and that's going to minimize the amount of padding. For the training sampler, we're going to create this thing I call a sort-ish sampler, which also sorts-ish. So this is where I really like PyTorch, is that they came up with this idea for an API for their data loader where we can hook in new classes to make it behave in different ways. So here's a sort sampler, it's simply something which again, it has a length, which is the length of the data source, and it has an iterator, which is simply an iterator which goes through the data source sorted by length, or the key. I pass in as the key a lambda function which returns the length. And so for the sort-ish sampler, I won't go through the details, but it basically does the same thing with a little bit of randomness. So it's a really beautiful, just another of these beautiful little design things in PyTorch that I discovered I could take James Bradbury's ideas, which he had written a whole new set of classes around, and I could actually just use the inbuilt hooks inside PyTorch. You will notice data loader is not actually PyTorch's data loader, it's actually FastAI's data loader, but it's basically almost entirely plagiarized from PyTorch, but customized in some ways to make it faster, mainly by using multi-threading instead of multi-processing. Does the pre-trained LSTM depth and BBTT need to match with the new one we are training? The BBTT doesn't need to match at all. That's just like how many things do we look at at a time, it's got nothing to do with the architecture. So now we can call that function we just saw before, getRNNClassifier, it's going to create exactly the same encoder, more or less, and we're going to pass in the same architectural details as before. But this time, the head that we add on, you've got a few more things you can do. One is you can add more than one hidden layer. So this layer here says this is what the input to my classifier section, my head, is going to be. This is the output of the first layer, this is the output of the second layer, and you can add as many as you like. So you can basically create a multi-layered neural net classifier at the end. And so ditto, these are the dropouts to go after each of these layers. And then here are all of the AWD LSTM dropouts, which we're going to basically plagiarize that idea for our classifier. We're going to use the RNN learner just like before. We're going to use discriminative learning rates for different layers. You can try using weight decay or not, I've been fiddling around a bit with that to see what happens. And so we start out just training the last layer, and we get 92.9% accuracy. Then we unfreeze one more layer, get 93.3% accuracy, and then we fine-tune the whole thing. And after 3 epochs, so here is the famous James Bradbury we're talking about. This was kind of the main attempt before our paper came along at using a pre-trained model. And what they did is they used a pre-trained translation model. But they didn't fine-tune the whole thing. They just took the activations of the translation model. And when they tried IMDB, they got 91.8%, which we beat easily after only fine-tuning one layer. They weren't state of the art there. The state of the art is 94.1%, which we beat after fine-tuning the whole thing for 3 epochs. And so by the end, we're at 94.8%, which is obviously a huge difference because in terms of error rate, that's gone down from 5.9%. And then I'll tell you a simple little trick. Go back to the start of this notebook and reverse the order of all of the documents and then rerun the whole thing. And when you get to the bit that says wt103, replace this fwd for forward with bwd for backward. That's a backward English language model that learns to read English backwards. So if you redo this whole thing, put all the documents in reverse and change this to backward, you now have a second classifier which classifies things by positive or negative sentiment based on the reverse document. If you then take the two predictions and take the average of them, you basically have a bi-directional model that you've trained each bit separately. That gets you to 95.4% accuracy. So we've basically lowered it from 5.9% to 4.6%. So this kind of 20% change in the state of the art, it's almost unheard of. You have to go back to Jeffrey Hinton's ImageNet computer vision thing where they chopped 30% off the state of the art. It doesn't happen very often. So you can see this idea of just use transfer learning is ridiculously powerful, but every new field thinks their new field is too special and you can't do it. So it's a big opportunity for all of us. So we turn this into a paper. When I say we, I did it with this guy Sebastian Ruda. Now you might remember his name, because in Lesson 5, I told you that I actually had shared Lesson 4 with Sebastian because I think he's an awesome researcher who I thought might like it. I didn't know him personally at all. And much to my surprise, he actually watched the damn video. I was like, what NLP researcher is going to watch some beginner's video? But he watched the whole video. He was like, that's actually quite fantastic. I was like, thank you very much, that's awesome coming from you. And he said, hey, we should turn this into a paper. And I said, I don't write papers. I don't care about papers, I'm not interested in papers. That sounds really boring. And he said, okay, how about I write the paper for you? And I said, you can't really write a paper about this yet because you'd have to do studies to compare it to other things. They're called ablation studies to see which bits actually work. There's no rigor here. I just put in everything that came in my head and chucked it all together and it happened to work. And he was like, okay, what if I write all the paper and do all the ablation studies, then can we write the paper? And I said, well, it's like a whole library that I haven't documented, and I'm not going to yet, and you don't know how it all works. He said, okay, if I write the paper and do the ablation studies and figure out from scratch how the code works without bothering you, then can we write the paper? I was like, yeah, if you did all those things, then you could write the paper. He was like, okay. And so then two days later he comes back and he says, okay, I've done a draft of the paper. So I share this story to say, if you're some student in Ireland, he's a student in Ireland, and you want to do good work, don't let anybody stop you. I did not encourage him to say the least. But in the end, he was like, look, I want to do this work. I think it's going to be good and I'll figure it out. And you know, he wrote a fantastic paper and he did the ablation studies and he figured out how fast AI works, and now we're planning to write another paper together. And so you've got to be a bit careful, because sometimes I get messages from random people saying like, I've got lots of good ideas, can we have coffee? I can have coffee in my office anytime, thank you. But it's very different to say, hey, I took your ideas and I wrote a paper and I did a bunch of experiments and I figured out how your code works, I added documentation to it. Should we submit this to a conference? Do you see what I mean? There's nothing to stop you doing amazing work, and if you do amazing work that helps somebody else, like in this case, I'm happy that we have a paper. I don't care about papers, but I think it's cool that these ideas now have this rigorous study. Let me show you what he did. So he took all my code, so I'd already done all the fastai.txt and stuff like that, and as you've seen, it lets us work with large corpuses. So Sebastian is fantastically well-read, and he said here's a paper that Jan Lakudin and some guys just came out with where they tried lots of different classification datasets, so I'm going to try running your code on all these datasets. So these are the datasets. And so some of them had many, many hundreds of thousands of documents, and they were far bigger than anything I had tried, but I thought it should work. And so he had a few good little ideas as we went along, so you should totally make sure you read the paper. And he said, well this thing that you called in the lessons, differential learning rates, differential kind of means something else, maybe we should rename it. So we renamed it. It's now called discriminative learning rates. So this idea that we had from part 1 where we used different learning rates for different layers, after doing some literature research, it does seem like that hasn't been done before, so it's now officially a thing, discriminative learning rates. And so all these ideas, this is something we learned in lesson 1. It now has an equation with Greek and everything. So when you see an equation with Greek and everything, that doesn't necessarily mean it's more complex than anything we did in lesson 1, because this one isn't. Again, that idea of unfreezing a layer at a time also seems to have never been done before, so it's now a thing, and it's got the very clever name gradual unfreezing. So then, long promised, we're going to look at this. Slanted triangular learning rates. So this actually was not my idea. Leslie Smith, one of my favorite researchers who you all know about, emailed me a while ago and said, I'm so over certain learning rates, I don't do that anymore, I now do a slightly different version where I have one cycle which goes up quickly at the start and then slowly down afterwards. And he said I often find it works better, I've tried going back over all of my old data sets and it works better for all of them, every one I tried. So this is what the learning rate looks like. You can use it in Fast AI just by adding UCLR equals to your fit. This first number is the ratio between the highest learning rate and the lowest learning rate. So here this is 1.32 of that. The second number is the ratio between the first peak and the last peak. And so the basic idea is if you're doing a cycle length 10, that you want the first epoch to be the upward bit and the other 9 epochs to be the downward bit, then you would use 10. And I find that works pretty well, that was also Leslie's suggestion, is make about a tenth of it the upward bit and about 9 tenths the downward bit. Since he told me about it, actually it was maybe 2 days ago, he wrote this amazing paper, A Disciplined Approach to Neural Network Hyperparameters, in which he described something very slightly different to this again, but the same basic idea. This is a must-read paper. It's got all the kinds of ideas that FastAI talks about a lot in great depth, and nobody else is talking about this stuff. It's kind of a slog. Unfortunately Leslie had to go away on a trip before he really had time to edit it properly, so it's a little bit slow reading, but don't let that stop you, it's amazing. So this triangle, this is the equation from my paper with Sebastian. Sebastian was like, Jeremy, can you send me the math equation behind that code you wrote? I was like, no, I just wrote the code, I could not turn it into math. So he figured out the math for it. So you might have noticed the first layer of our classifier was equal to embedding size times 3. Why times 3? Times 3 because, and again this seems to be something people haven't done before, so new idea, CONCAT pooling, which is that we take the average pooling over the sequence of the activations, the max pooling of the sequence over the activations, and the final set of activations and just concatenate them all together. Again, this is something which we talked about in part 1, but doesn't seem to be in the literature before, so it's now called CONCAT pooling and again it's now got an equation and everything, but this is the entirety of the implementation. Pool with average, pool with max, concatenate those two along with the final sequence. So you can go through this paper and see how the Fast AI code implements each piece. So then, to me one of the interesting pieces is the difference between RNN encoder, which you've already seen, and multi-batch RNN encoder. So what's the difference there? So the key difference is that the normal RNN encoder for the language model, we could just do BPTT chunk at a time, no problem. But for the classifier, we need to do the whole document. We need to do the whole movie review before we decide if it's positive or negative. The whole movie review can easily be 2000 words long, and I can't fit 2000 words worth of gradients in my GPU memory for every single one of my activations, sorry, for every one of my weights, so what do I do? And so the idea was very simple, which is I go through my whole sequence length, one batch of BPTT at a time, and I call super.forward, so in other words the RNN encoder, so I just call the usual RNN encoder to grab its outputs. And then I've got this maximum sequence length parameter where it says, as long as you're doing no more than that sequence length, then start appending it to my list of outputs. So in other words, the thing that it sends back to this pooling is only as many activations as we've asked it to keep. And so that way you can basically figure out what's maxsec can your particular GPU handle. So it's still using the whole document, but let's say maxsec is 1000 words and your longest document length is 2000 words. And it's still going through the RNN, creating state for those first 1000 words, but it's not actually going to store the activations for the backprop for the first 1000, it's only going to keep the last 1000. So that means that it can't backpropagate the loss back to any state that was created in the first 1000 words. Basically that's now gone. So it's a really simple piece of code, and honestly when I wrote it, it was like, I didn't spend much time thinking about it, it seems so obviously the only way that this could possibly work. But again, it seems to be a new thing. So we now have backprop through time for text classification. So you can see there's lots of little pieces in this paper. So what was the result? So the result was, on every single dataset we tried, we got a better result than any previous academic for text classification. So IMDB, Trek6, AGnews, Dbpedia, Yelp, all different types. And honestly, IMDB was the only one I spent any time trying to optimize the model, so like most of them, we just did it like whatever came out first. So if we actually spent time on it, I think these would be a lot better. And the things that these are comparing to, most of them are, you'll see they're different on each table, because they're optimized. These are customized algorithms on the whole. So this is saying one simple fine-tuning algorithm can beat these really customized algorithms. And so here's one of the really cool things that Sebastian did, these ablation studies. I was really keen that if we were going to publish a paper, we had to say, why does it work? So Sebastian went through and tried removing all of those different contributions I mentioned. So what if we don't use gradual freezing? What if we don't use discriminative learning rates? What if instead of discriminative learning rates, we use cosine annealing? What if we don't do any pre-training with Wikipedia? What if we don't do any fine-tuning? And then the really interesting one to me was, what's the validation error rate on IMDB if we only use 100 training examples, versus 200 versus 500? And you can see, very interestingly, the full version of this approach is nearly as accurate on just 100 training examples, like it's still very accurate versus all 20,000 training examples. Where else if you're training from scratch on 100, it's almost random. So it's what I expected. I said to Sebastian, I really think that this is most beneficial when you don't have much data. And this is where fast AI is most interested in contributing. There's small data regimes, small compute regimes, and so forth. So he did these studies to check. So I want to show you a couple of tricks as to how you can run these kinds of studies. The first trick is something which I know you're all going to find really handy. I know you've all been annoyed when you're running something in a Jupyter Notebook and you lose your Internet connection for long enough that it decides you've gone away and then your session disappears and you have to start it again from scratch. So what do you do? There's a very simple cool thing called VNC where basically you can install on your AWS instance or paper space or whatever, XWindows, a lightweight window manager, a VNC server, a Firefox, a terminal, and some fonts. Chuck these lines at the end of your VNC XStartup configuration file and then run this command. It's now running a server where you can then run the type VNC viewer, or any VNC viewer, on your computer and you point it at your server. But specifically what you do is you use SSH port forwarding to port forward port 5913 to localhost 5913. And so then you connect to port 5913 on localhost. It will send it off to port 5913 on your server, which is the VNC port, and it will display an XWindows desktop. And then you can click on the Linux Start Like button and click on Firefox, and you now have Firefox. And you'll see here in Firefox it says localhost because this Firefox is running on my AWS server. So you now run Firefox, you start your thing running, and then you close your VNC viewer, remembering that Firefox is displaying on this virtual VNC display, not on the real display. And so then later on that day, you log back into VNC viewer and it pops up again. So it's like a persistent desktop. And it's shockingly fast. It works really well. So there's trick number one. And there's lots of different VNC servers and clients and whatever, but this one worked fine for me. So you can see here I connect to localhost 5913. Trick number two is to create Python scripts. This is what we ended up doing. So I ended up creating a little Python script for Sebastian to say this is the basic steps you need to do, and now you need to create different versions for everything else. And I suggested to him that he try using this thing called Google Fire. What Google Fire does is you create a function with shitloads of parameters. And so these are all the things that Sebastian wanted to try doing. Different dropout amounts, different learning rates, do I use pre-training or not, do I use CLR or not, do I use discriminative learning rate or not. Do I go backwards or not, blah blah blah. So you create a function and then you add something saying if name equals main fire.fire and the function name. You do nothing else at all. You don't have to add any metadata, any doc strings, anything at all. And you then call that script and automatically you now have a command line interface. And that's it. So that's a super fantastic easy way to run lots of different variations in a terminal. And this ends up being easier if you want to do lots of variations than using a notebook because you can just have a bash script that tries all of them and spits them all out. You'll find inside the DL2 course directory there's now something called imdb-scripts and I've put there all of the scripts that Sebastian and I used. So you'll see, because we needed to like tokenize every single dataset, we had to turn every dataset, numericalize every dataset, we had to train a language model on every dataset, we had to train a classified every dataset, we had to do all of those things in a variety of different ways to compare them, we had a script for all those things. So you can check out and see all of the scripts that we used. When you're doing a lot of scripts and stuff, they've got different code all over the place, eventually it might get frustrating that you don't want to symlink your fast.io library again and again, but you probably don't want to pip install it because that version tends to be a little bit old, we move so fast, you want to use the current version in Git. If you say pip install minus e dot from the fastai-repo base, it does something quite neat which basically creates a symlink to the fastai library in your Git installation right here inside your site packages directory. Your site packages directory is like your main Python library. And so if you do this, you can then access fastai from anywhere, but every time you do git pull, you've got the most recent version. One downside of this is that it installs any updated versions of packages from pip which can kind of confuse conda a little bit. So another alternative here is just to symlink the fastai library to your site packages library. That works just as well. Then you can use fastai again from anywhere, and it's quite handy when you want to run scripts that use fastai from different directories on your system. So one more thing before we go, which is something you can try if you like. You don't have to tokenize words. Instead of tokenizing words, you can tokenize what are called subword units. And so for example, unsupervised could be tokenized as unsupervised. Tokenizer could be tokenized as tokenizer. And then you can do the same thing, the language model that works on subword units, the classifier that works on subword units, etc. So how well does that work? I started playing with it, and with not too much playing, I was getting classification results that were nearly as good as using word-level tokenization. Not quite as good, but nearly as good. I suspect with more careful thinking and playing around, maybe I could have got as good or better. But even if I couldn't, if you create a subword unit wiki text model, then IMDB model, language model, and then classifier, forwards and backwards for subword units, and then ensemble it with the forwards and backwards word-level ones, you should be able to beat us. So here's an approach you may be able to beat our state-of-the-art result. Google has a project called Sentence Piece, which actually uses a neural net to figure out the optimal splitting up of words. And so you end up with a vocabulary of subword units. In my playing around, I found that creating a vocabulary of about 30,000 subword units seems to be about optimal. So if you're interested, there's something you can try. It's a bit of a pain to install. It's C++, it doesn't have great error messages, but it will work. There is a Python library for it, and if anybody tries this, I'm happy to help them get it working. There's been little, if any, experiments with ensembling subword and word-level stuff classification, and I do think it should be the best approach. All right. Thanks, everybody. Have a great week, and see you next Monday. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.640000000000001, "text": " Welcome to Lesson 10, or as somebody on the forum described it, Lesson 10 Mod 7, which", "tokens": [4027, 281, 18649, 266, 1266, 11, 420, 382, 2618, 322, 264, 17542, 7619, 309, 11, 18649, 266, 1266, 6583, 1614, 11, 597], "temperature": 0.0, "avg_logprob": -0.20014311717106745, "compression_ratio": 1.4696356275303644, "no_speech_prob": 0.008442949503660202}, {"id": 1, "seek": 0, "start": 7.640000000000001, "end": 11.34, "text": " is probably a clearer way to think about this.", "tokens": [307, 1391, 257, 26131, 636, 281, 519, 466, 341, 13], "temperature": 0.0, "avg_logprob": -0.20014311717106745, "compression_ratio": 1.4696356275303644, "no_speech_prob": 0.008442949503660202}, {"id": 2, "seek": 0, "start": 11.34, "end": 15.88, "text": " We're going to be talking about NLP.", "tokens": [492, 434, 516, 281, 312, 1417, 466, 426, 45196, 13], "temperature": 0.0, "avg_logprob": -0.20014311717106745, "compression_ratio": 1.4696356275303644, "no_speech_prob": 0.008442949503660202}, {"id": 3, "seek": 0, "start": 15.88, "end": 21.94, "text": " Before we do, let's do a quick review of last week.", "tokens": [4546, 321, 360, 11, 718, 311, 360, 257, 1702, 3131, 295, 1036, 1243, 13], "temperature": 0.0, "avg_logprob": -0.20014311717106745, "compression_ratio": 1.4696356275303644, "no_speech_prob": 0.008442949503660202}, {"id": 4, "seek": 0, "start": 21.94, "end": 26.34, "text": " Last week, there's quite a few people who have flown here to San Francisco for this", "tokens": [5264, 1243, 11, 456, 311, 1596, 257, 1326, 561, 567, 362, 34536, 510, 281, 5271, 12279, 337, 341], "temperature": 0.0, "avg_logprob": -0.20014311717106745, "compression_ratio": 1.4696356275303644, "no_speech_prob": 0.008442949503660202}, {"id": 5, "seek": 0, "start": 26.34, "end": 27.34, "text": " in-person course.", "tokens": [294, 12, 10813, 1164, 13], "temperature": 0.0, "avg_logprob": -0.20014311717106745, "compression_ratio": 1.4696356275303644, "no_speech_prob": 0.008442949503660202}, {"id": 6, "seek": 0, "start": 27.34, "end": 29.580000000000002, "text": " I'm seeing them pretty much every day.", "tokens": [286, 478, 2577, 552, 1238, 709, 633, 786, 13], "temperature": 0.0, "avg_logprob": -0.20014311717106745, "compression_ratio": 1.4696356275303644, "no_speech_prob": 0.008442949503660202}, {"id": 7, "seek": 2958, "start": 29.58, "end": 33.6, "text": " They're working full-time on this, and quite a few of them are still struggling to understand", "tokens": [814, 434, 1364, 1577, 12, 3766, 322, 341, 11, 293, 1596, 257, 1326, 295, 552, 366, 920, 9314, 281, 1223], "temperature": 0.0, "avg_logprob": -0.20085080965297428, "compression_ratio": 1.6187290969899666, "no_speech_prob": 4.7570702008670196e-05}, {"id": 8, "seek": 2958, "start": 33.6, "end": 35.32, "text": " the material from last week.", "tokens": [264, 2527, 490, 1036, 1243, 13], "temperature": 0.0, "avg_logprob": -0.20085080965297428, "compression_ratio": 1.6187290969899666, "no_speech_prob": 4.7570702008670196e-05}, {"id": 9, "seek": 2958, "start": 35.32, "end": 38.519999999999996, "text": " So if you're finding it difficult, that's fine.", "tokens": [407, 498, 291, 434, 5006, 309, 2252, 11, 300, 311, 2489, 13], "temperature": 0.0, "avg_logprob": -0.20085080965297428, "compression_ratio": 1.6187290969899666, "no_speech_prob": 4.7570702008670196e-05}, {"id": 10, "seek": 2958, "start": 38.519999999999996, "end": 42.92, "text": " One of the reasons I kind of put it up there up front is so that we've got something to", "tokens": [1485, 295, 264, 4112, 286, 733, 295, 829, 309, 493, 456, 493, 1868, 307, 370, 300, 321, 600, 658, 746, 281], "temperature": 0.0, "avg_logprob": -0.20085080965297428, "compression_ratio": 1.6187290969899666, "no_speech_prob": 4.7570702008670196e-05}, {"id": 11, "seek": 2958, "start": 42.92, "end": 50.28, "text": " cogitate about and think about and gradually work towards so that by Lesson 14 Mod 7, you'll", "tokens": [46521, 8086, 466, 293, 519, 466, 293, 13145, 589, 3030, 370, 300, 538, 18649, 266, 3499, 6583, 1614, 11, 291, 603], "temperature": 0.0, "avg_logprob": -0.20085080965297428, "compression_ratio": 1.6187290969899666, "no_speech_prob": 4.7570702008670196e-05}, {"id": 12, "seek": 2958, "start": 50.28, "end": 53.0, "text": " get a second crack at it.", "tokens": [483, 257, 1150, 6226, 412, 309, 13], "temperature": 0.0, "avg_logprob": -0.20085080965297428, "compression_ratio": 1.6187290969899666, "no_speech_prob": 4.7570702008670196e-05}, {"id": 13, "seek": 2958, "start": 53.0, "end": 54.480000000000004, "text": " But it's well worth it.", "tokens": [583, 309, 311, 731, 3163, 309, 13], "temperature": 0.0, "avg_logprob": -0.20085080965297428, "compression_ratio": 1.6187290969899666, "no_speech_prob": 4.7570702008670196e-05}, {"id": 14, "seek": 2958, "start": 54.480000000000004, "end": 57.92, "text": " There's so many pieces, so hopefully you can keep developing better understanding.", "tokens": [821, 311, 370, 867, 3755, 11, 370, 4696, 291, 393, 1066, 6416, 1101, 3701, 13], "temperature": 0.0, "avg_logprob": -0.20085080965297428, "compression_ratio": 1.6187290969899666, "no_speech_prob": 4.7570702008670196e-05}, {"id": 15, "seek": 5792, "start": 57.92, "end": 64.0, "text": " To understand the pieces, you'll need to understand the shapes of convolutional layer outputs,", "tokens": [1407, 1223, 264, 3755, 11, 291, 603, 643, 281, 1223, 264, 10854, 295, 45216, 304, 4583, 23930, 11], "temperature": 0.0, "avg_logprob": -0.16609805645328937, "compression_ratio": 1.834710743801653, "no_speech_prob": 1.0615896826493554e-05}, {"id": 16, "seek": 5792, "start": 64.0, "end": 68.56, "text": " and receptive fields, and loss functions, and everything.", "tokens": [293, 45838, 7909, 11, 293, 4470, 6828, 11, 293, 1203, 13], "temperature": 0.0, "avg_logprob": -0.16609805645328937, "compression_ratio": 1.834710743801653, "no_speech_prob": 1.0615896826493554e-05}, {"id": 17, "seek": 5792, "start": 68.56, "end": 75.68, "text": " So it's all stuff that you need to understand for all of your deep learning studies anyway.", "tokens": [407, 309, 311, 439, 1507, 300, 291, 643, 281, 1223, 337, 439, 295, 428, 2452, 2539, 5313, 4033, 13], "temperature": 0.0, "avg_logprob": -0.16609805645328937, "compression_ratio": 1.834710743801653, "no_speech_prob": 1.0615896826493554e-05}, {"id": 18, "seek": 5792, "start": 75.68, "end": 79.68, "text": " So everything you do to develop an understanding of last week's lesson is going to help you", "tokens": [407, 1203, 291, 360, 281, 1499, 364, 3701, 295, 1036, 1243, 311, 6898, 307, 516, 281, 854, 291], "temperature": 0.0, "avg_logprob": -0.16609805645328937, "compression_ratio": 1.834710743801653, "no_speech_prob": 1.0615896826493554e-05}, {"id": 19, "seek": 5792, "start": 79.68, "end": 81.5, "text": " with everything else.", "tokens": [365, 1203, 1646, 13], "temperature": 0.0, "avg_logprob": -0.16609805645328937, "compression_ratio": 1.834710743801653, "no_speech_prob": 1.0615896826493554e-05}, {"id": 20, "seek": 5792, "start": 81.5, "end": 86.08, "text": " So one key thing I wanted to mention is we started out with something which is really", "tokens": [407, 472, 2141, 551, 286, 1415, 281, 2152, 307, 321, 1409, 484, 365, 746, 597, 307, 534], "temperature": 0.0, "avg_logprob": -0.16609805645328937, "compression_ratio": 1.834710743801653, "no_speech_prob": 1.0615896826493554e-05}, {"id": 21, "seek": 8608, "start": 86.08, "end": 90.8, "text": " pretty simple, which is a single object classifier.", "tokens": [1238, 2199, 11, 597, 307, 257, 2167, 2657, 1508, 9902, 13], "temperature": 0.0, "avg_logprob": -0.1608160654703776, "compression_ratio": 1.7142857142857142, "no_speech_prob": 7.889174412412103e-06}, {"id": 22, "seek": 8608, "start": 90.8, "end": 96.48, "text": " Single object bounding box without a classifier, and then single object classifier and bounding", "tokens": [31248, 2657, 5472, 278, 2424, 1553, 257, 1508, 9902, 11, 293, 550, 2167, 2657, 1508, 9902, 293, 5472, 278], "temperature": 0.0, "avg_logprob": -0.1608160654703776, "compression_ratio": 1.7142857142857142, "no_speech_prob": 7.889174412412103e-06}, {"id": 23, "seek": 8608, "start": 96.48, "end": 98.12, "text": " box.", "tokens": [2424, 13], "temperature": 0.0, "avg_logprob": -0.1608160654703776, "compression_ratio": 1.7142857142857142, "no_speech_prob": 7.889174412412103e-06}, {"id": 24, "seek": 8608, "start": 98.12, "end": 106.64, "text": " And anybody who's hopefully spent some time studying since Lesson 8 Mod 7 has got to the", "tokens": [400, 4472, 567, 311, 4696, 4418, 512, 565, 7601, 1670, 18649, 266, 1649, 6583, 1614, 575, 658, 281, 264], "temperature": 0.0, "avg_logprob": -0.1608160654703776, "compression_ratio": 1.7142857142857142, "no_speech_prob": 7.889174412412103e-06}, {"id": 25, "seek": 8608, "start": 106.64, "end": 109.75999999999999, "text": " point where they understand this bit.", "tokens": [935, 689, 436, 1223, 341, 857, 13], "temperature": 0.0, "avg_logprob": -0.1608160654703776, "compression_ratio": 1.7142857142857142, "no_speech_prob": 7.889174412412103e-06}, {"id": 26, "seek": 8608, "start": 109.75999999999999, "end": 116.0, "text": " Now the reason I mention this is because the bit where we go to multiple objects is actually", "tokens": [823, 264, 1778, 286, 2152, 341, 307, 570, 264, 857, 689, 321, 352, 281, 3866, 6565, 307, 767], "temperature": 0.0, "avg_logprob": -0.1608160654703776, "compression_ratio": 1.7142857142857142, "no_speech_prob": 7.889174412412103e-06}, {"id": 27, "seek": 11600, "start": 116.0, "end": 120.88, "text": " almost identical to this, except we first have to solve the matching problem.", "tokens": [1920, 14800, 281, 341, 11, 3993, 321, 700, 362, 281, 5039, 264, 14324, 1154, 13], "temperature": 0.0, "avg_logprob": -0.1355886459350586, "compression_ratio": 1.8046511627906976, "no_speech_prob": 5.9550975493039005e-06}, {"id": 28, "seek": 11600, "start": 120.88, "end": 127.48, "text": " We end up creating far more activations than we need for our number of bounding boxes,", "tokens": [492, 917, 493, 4084, 1400, 544, 2430, 763, 813, 321, 643, 337, 527, 1230, 295, 5472, 278, 9002, 11], "temperature": 0.0, "avg_logprob": -0.1355886459350586, "compression_ratio": 1.8046511627906976, "no_speech_prob": 5.9550975493039005e-06}, {"id": 29, "seek": 11600, "start": 127.48, "end": 129.04, "text": " ground truth bounding boxes.", "tokens": [2727, 3494, 5472, 278, 9002, 13], "temperature": 0.0, "avg_logprob": -0.1355886459350586, "compression_ratio": 1.8046511627906976, "no_speech_prob": 5.9550975493039005e-06}, {"id": 30, "seek": 11600, "start": 129.04, "end": 133.0, "text": " So we match each ground truth object to a subset of those activations.", "tokens": [407, 321, 2995, 1184, 2727, 3494, 2657, 281, 257, 25993, 295, 729, 2430, 763, 13], "temperature": 0.0, "avg_logprob": -0.1355886459350586, "compression_ratio": 1.8046511627906976, "no_speech_prob": 5.9550975493039005e-06}, {"id": 31, "seek": 11600, "start": 133.0, "end": 138.16, "text": " And once we've done that, the loss function that we then do to each matched pair is almost", "tokens": [400, 1564, 321, 600, 1096, 300, 11, 264, 4470, 2445, 300, 321, 550, 360, 281, 1184, 21447, 6119, 307, 1920], "temperature": 0.0, "avg_logprob": -0.1355886459350586, "compression_ratio": 1.8046511627906976, "no_speech_prob": 5.9550975493039005e-06}, {"id": 32, "seek": 11600, "start": 138.16, "end": 140.88, "text": " identical to this loss function.", "tokens": [14800, 281, 341, 4470, 2445, 13], "temperature": 0.0, "avg_logprob": -0.1355886459350586, "compression_ratio": 1.8046511627906976, "no_speech_prob": 5.9550975493039005e-06}, {"id": 33, "seek": 14088, "start": 140.88, "end": 150.12, "text": " So if you're feeling stuck, go back to Lesson 8 and make sure you understand the data set,", "tokens": [407, 498, 291, 434, 2633, 5541, 11, 352, 646, 281, 18649, 266, 1649, 293, 652, 988, 291, 1223, 264, 1412, 992, 11], "temperature": 0.0, "avg_logprob": -0.12154780901395358, "compression_ratio": 1.53475935828877, "no_speech_prob": 2.684188984858338e-06}, {"id": 34, "seek": 14088, "start": 150.12, "end": 155.72, "text": " the data loader, and most importantly the loss function from the end of Lesson 8 or", "tokens": [264, 1412, 3677, 260, 11, 293, 881, 8906, 264, 4470, 2445, 490, 264, 917, 295, 18649, 266, 1649, 420], "temperature": 0.0, "avg_logprob": -0.12154780901395358, "compression_ratio": 1.53475935828877, "no_speech_prob": 2.684188984858338e-06}, {"id": 35, "seek": 14088, "start": 155.72, "end": 160.92, "text": " the start of Lesson 9.", "tokens": [264, 722, 295, 18649, 266, 1722, 13], "temperature": 0.0, "avg_logprob": -0.12154780901395358, "compression_ratio": 1.53475935828877, "no_speech_prob": 2.684188984858338e-06}, {"id": 36, "seek": 14088, "start": 160.92, "end": 167.32, "text": " So once we've got this thing which can predict the class and bounding box for one object,", "tokens": [407, 1564, 321, 600, 658, 341, 551, 597, 393, 6069, 264, 1508, 293, 5472, 278, 2424, 337, 472, 2657, 11], "temperature": 0.0, "avg_logprob": -0.12154780901395358, "compression_ratio": 1.53475935828877, "no_speech_prob": 2.684188984858338e-06}, {"id": 37, "seek": 16732, "start": 167.32, "end": 171.88, "text": " we went to multiple objects by just creating more activations.", "tokens": [321, 1437, 281, 3866, 6565, 538, 445, 4084, 544, 2430, 763, 13], "temperature": 0.0, "avg_logprob": -0.15317437936971476, "compression_ratio": 1.7142857142857142, "no_speech_prob": 5.507543392013758e-06}, {"id": 38, "seek": 16732, "start": 171.88, "end": 174.35999999999999, "text": " We had to then deal with the matching problem.", "tokens": [492, 632, 281, 550, 2028, 365, 264, 14324, 1154, 13], "temperature": 0.0, "avg_logprob": -0.15317437936971476, "compression_ratio": 1.7142857142857142, "no_speech_prob": 5.507543392013758e-06}, {"id": 39, "seek": 16732, "start": 174.35999999999999, "end": 178.72, "text": " Having dealt with the matching problem, we then basically moved each of those anchor", "tokens": [10222, 15991, 365, 264, 14324, 1154, 11, 321, 550, 1936, 4259, 1184, 295, 729, 18487], "temperature": 0.0, "avg_logprob": -0.15317437936971476, "compression_ratio": 1.7142857142857142, "no_speech_prob": 5.507543392013758e-06}, {"id": 40, "seek": 16732, "start": 178.72, "end": 185.68, "text": " boxes in and out a little bit and around a little bit so they tried to line up with particular", "tokens": [9002, 294, 293, 484, 257, 707, 857, 293, 926, 257, 707, 857, 370, 436, 3031, 281, 1622, 493, 365, 1729], "temperature": 0.0, "avg_logprob": -0.15317437936971476, "compression_ratio": 1.7142857142857142, "no_speech_prob": 5.507543392013758e-06}, {"id": 41, "seek": 16732, "start": 185.68, "end": 187.79999999999998, "text": " ground truth objects.", "tokens": [2727, 3494, 6565, 13], "temperature": 0.0, "avg_logprob": -0.15317437936971476, "compression_ratio": 1.7142857142857142, "no_speech_prob": 5.507543392013758e-06}, {"id": 42, "seek": 16732, "start": 187.79999999999998, "end": 193.95999999999998, "text": " And we talked about how we took advantage of the convolutional nature of the network", "tokens": [400, 321, 2825, 466, 577, 321, 1890, 5002, 295, 264, 45216, 304, 3687, 295, 264, 3209], "temperature": 0.0, "avg_logprob": -0.15317437936971476, "compression_ratio": 1.7142857142857142, "no_speech_prob": 5.507543392013758e-06}, {"id": 43, "seek": 19396, "start": 193.96, "end": 201.52, "text": " to try to have activations that had a receptive field that was similar to the ground truth", "tokens": [281, 853, 281, 362, 2430, 763, 300, 632, 257, 45838, 2519, 300, 390, 2531, 281, 264, 2727, 3494], "temperature": 0.0, "avg_logprob": -0.2054638033327849, "compression_ratio": 1.583673469387755, "no_speech_prob": 3.8448911254818086e-06}, {"id": 44, "seek": 19396, "start": 201.52, "end": 203.4, "text": " object we were predicting.", "tokens": [2657, 321, 645, 32884, 13], "temperature": 0.0, "avg_logprob": -0.2054638033327849, "compression_ratio": 1.583673469387755, "no_speech_prob": 3.8448911254818086e-06}, {"id": 45, "seek": 19396, "start": 203.4, "end": 210.0, "text": " And Chloe Sultan provided this fantastic picture for her own notes, but she shared it with", "tokens": [400, 29694, 23528, 5649, 341, 5456, 3036, 337, 720, 1065, 5570, 11, 457, 750, 5507, 309, 365], "temperature": 0.0, "avg_logprob": -0.2054638033327849, "compression_ratio": 1.583673469387755, "no_speech_prob": 3.8448911254818086e-06}, {"id": 46, "seek": 19396, "start": 210.0, "end": 217.92000000000002, "text": " everybody which is lovely, to talk about what does SSD multihead.forward do line by line.", "tokens": [2201, 597, 307, 7496, 11, 281, 751, 466, 437, 775, 30262, 2120, 4247, 2056, 13, 13305, 360, 1622, 538, 1622, 13], "temperature": 0.0, "avg_logprob": -0.2054638033327849, "compression_ratio": 1.583673469387755, "no_speech_prob": 3.8448911254818086e-06}, {"id": 47, "seek": 19396, "start": 217.92000000000002, "end": 221.72, "text": " And I partly wanted to show this to help you with your revision, but I also partly wanted", "tokens": [400, 286, 17031, 1415, 281, 855, 341, 281, 854, 291, 365, 428, 34218, 11, 457, 286, 611, 17031, 1415], "temperature": 0.0, "avg_logprob": -0.2054638033327849, "compression_ratio": 1.583673469387755, "no_speech_prob": 3.8448911254818086e-06}, {"id": 48, "seek": 22172, "start": 221.72, "end": 229.68, "text": " to show this to say, doing this kind of stuff is very useful for you to do, like walk through", "tokens": [281, 855, 341, 281, 584, 11, 884, 341, 733, 295, 1507, 307, 588, 4420, 337, 291, 281, 360, 11, 411, 1792, 807], "temperature": 0.0, "avg_logprob": -0.23561475335097895, "compression_ratio": 1.5627906976744186, "no_speech_prob": 1.1842979802167974e-05}, {"id": 49, "seek": 22172, "start": 229.68, "end": 232.76, "text": " and in whatever way helps you make sure you understand something.", "tokens": [293, 294, 2035, 636, 3665, 291, 652, 988, 291, 1223, 746, 13], "temperature": 0.0, "avg_logprob": -0.23561475335097895, "compression_ratio": 1.5627906976744186, "no_speech_prob": 1.1842979802167974e-05}, {"id": 50, "seek": 22172, "start": 232.76, "end": 239.44, "text": " And you can see what Chloe's done here is she's focused particularly on the dimensions", "tokens": [400, 291, 393, 536, 437, 29694, 311, 1096, 510, 307, 750, 311, 5178, 4098, 322, 264, 12819], "temperature": 0.0, "avg_logprob": -0.23561475335097895, "compression_ratio": 1.5627906976744186, "no_speech_prob": 1.1842979802167974e-05}, {"id": 51, "seek": 22172, "start": 239.44, "end": 246.96, "text": " of the tensor at each point in the path as we gradually downsample using these drive-tube", "tokens": [295, 264, 40863, 412, 1184, 935, 294, 264, 3100, 382, 321, 13145, 760, 19988, 781, 1228, 613, 3332, 12, 83, 1977], "temperature": 0.0, "avg_logprob": -0.23561475335097895, "compression_ratio": 1.5627906976744186, "no_speech_prob": 1.1842979802167974e-05}, {"id": 52, "seek": 24696, "start": 246.96, "end": 253.08, "text": " convolutions, making sure she understands why those grid sizes happen, and then understanding", "tokens": [3754, 15892, 11, 1455, 988, 750, 15146, 983, 729, 10748, 11602, 1051, 11, 293, 550, 3701], "temperature": 0.0, "avg_logprob": -0.16622629853867993, "compression_ratio": 1.572, "no_speech_prob": 1.963779595826054e-06}, {"id": 53, "seek": 24696, "start": 253.08, "end": 256.72, "text": " how the outputs come out of those.", "tokens": [577, 264, 23930, 808, 484, 295, 729, 13], "temperature": 0.0, "avg_logprob": -0.16622629853867993, "compression_ratio": 1.572, "no_speech_prob": 1.963779595826054e-06}, {"id": 54, "seek": 24696, "start": 256.72, "end": 263.52, "text": " And so one thing you might be wondering is, how did Chloe calculate these numbers?", "tokens": [400, 370, 472, 551, 291, 1062, 312, 6359, 307, 11, 577, 630, 29694, 8873, 613, 3547, 30], "temperature": 0.0, "avg_logprob": -0.16622629853867993, "compression_ratio": 1.572, "no_speech_prob": 1.963779595826054e-06}, {"id": 55, "seek": 24696, "start": 263.52, "end": 267.88, "text": " So I don't know the answer, I haven't spoken to her, but obviously one approach would be", "tokens": [407, 286, 500, 380, 458, 264, 1867, 11, 286, 2378, 380, 10759, 281, 720, 11, 457, 2745, 472, 3109, 576, 312], "temperature": 0.0, "avg_logprob": -0.16622629853867993, "compression_ratio": 1.572, "no_speech_prob": 1.963779595826054e-06}, {"id": 56, "seek": 24696, "start": 267.88, "end": 270.40000000000003, "text": " like from first principles, just thinking through it.", "tokens": [411, 490, 700, 9156, 11, 445, 1953, 807, 309, 13], "temperature": 0.0, "avg_logprob": -0.16622629853867993, "compression_ratio": 1.572, "no_speech_prob": 1.963779595826054e-06}, {"id": 57, "seek": 24696, "start": 270.40000000000003, "end": 273.08, "text": " But then you want to know, am I right?", "tokens": [583, 550, 291, 528, 281, 458, 11, 669, 286, 558, 30], "temperature": 0.0, "avg_logprob": -0.16622629853867993, "compression_ratio": 1.572, "no_speech_prob": 1.963779595826054e-06}, {"id": 58, "seek": 27308, "start": 273.08, "end": 278.64, "text": " And so this is where you've got to remember this pdb.setTrace idea.", "tokens": [400, 370, 341, 307, 689, 291, 600, 658, 281, 1604, 341, 280, 67, 65, 13, 3854, 14252, 617, 1558, 13], "temperature": 0.0, "avg_logprob": -0.16519576178656684, "compression_ratio": 1.5698924731182795, "no_speech_prob": 3.6688456930278335e-06}, {"id": 59, "seek": 27308, "start": 278.64, "end": 287.03999999999996, "text": " So I just went in just before class and went into SSD multihead.forward and entered pdb.setTrace", "tokens": [407, 286, 445, 1437, 294, 445, 949, 1508, 293, 1437, 666, 30262, 2120, 4247, 2056, 13, 13305, 293, 9065, 280, 67, 65, 13, 3854, 14252, 617], "temperature": 0.0, "avg_logprob": -0.16519576178656684, "compression_ratio": 1.5698924731182795, "no_speech_prob": 3.6688456930278335e-06}, {"id": 60, "seek": 27308, "start": 287.03999999999996, "end": 290.08, "text": " and then I ran a single batch.", "tokens": [293, 550, 286, 5872, 257, 2167, 15245, 13], "temperature": 0.0, "avg_logprob": -0.16519576178656684, "compression_ratio": 1.5698924731182795, "no_speech_prob": 3.6688456930278335e-06}, {"id": 61, "seek": 27308, "start": 290.08, "end": 296.56, "text": " And so I put the trace at the end and then I could just print out the size of all of", "tokens": [400, 370, 286, 829, 264, 13508, 412, 264, 917, 293, 550, 286, 727, 445, 4482, 484, 264, 2744, 295, 439, 295], "temperature": 0.0, "avg_logprob": -0.16519576178656684, "compression_ratio": 1.5698924731182795, "no_speech_prob": 3.6688456930278335e-06}, {"id": 62, "seek": 27308, "start": 296.56, "end": 298.56, "text": " these guys.", "tokens": [613, 1074, 13], "temperature": 0.0, "avg_logprob": -0.16519576178656684, "compression_ratio": 1.5698924731182795, "no_speech_prob": 3.6688456930278335e-06}, {"id": 63, "seek": 29856, "start": 298.56, "end": 307.32, "text": " So which by the way reminds me, last week there may have been a point where I said 21", "tokens": [407, 597, 538, 264, 636, 12025, 385, 11, 1036, 1243, 456, 815, 362, 668, 257, 935, 689, 286, 848, 5080], "temperature": 0.0, "avg_logprob": -0.18094570957013031, "compression_ratio": 1.4197530864197532, "no_speech_prob": 2.3687900920776883e-06}, {"id": 64, "seek": 29856, "start": 307.32, "end": 321.88, "text": " plus 4 equals 26, which is not true in most universes.", "tokens": [1804, 1017, 6915, 7551, 11, 597, 307, 406, 2074, 294, 881, 50168, 13], "temperature": 0.0, "avg_logprob": -0.18094570957013031, "compression_ratio": 1.4197530864197532, "no_speech_prob": 2.3687900920776883e-06}, {"id": 65, "seek": 29856, "start": 321.88, "end": 325.96, "text": " And by the way, when I code, I do that stuff, that's the kind of thing I do all the time.", "tokens": [400, 538, 264, 636, 11, 562, 286, 3089, 11, 286, 360, 300, 1507, 11, 300, 311, 264, 733, 295, 551, 286, 360, 439, 264, 565, 13], "temperature": 0.0, "avg_logprob": -0.18094570957013031, "compression_ratio": 1.4197530864197532, "no_speech_prob": 2.3687900920776883e-06}, {"id": 66, "seek": 32596, "start": 325.96, "end": 330.84, "text": " So that's why we have debuggers and know how to check things and do things in small little", "tokens": [407, 300, 311, 983, 321, 362, 3001, 3562, 433, 293, 458, 577, 281, 1520, 721, 293, 360, 721, 294, 1359, 707], "temperature": 0.0, "avg_logprob": -0.13324340941414, "compression_ratio": 1.6655405405405406, "no_speech_prob": 6.962151019251905e-06}, {"id": 67, "seek": 32596, "start": 330.84, "end": 332.56, "text": " bits along the way.", "tokens": [9239, 2051, 264, 636, 13], "temperature": 0.0, "avg_logprob": -0.13324340941414, "compression_ratio": 1.6655405405405406, "no_speech_prob": 6.962151019251905e-06}, {"id": 68, "seek": 32596, "start": 332.56, "end": 336.28, "text": " So this idea of putting a debugger inside your forward function and printing out the", "tokens": [407, 341, 1558, 295, 3372, 257, 24083, 1321, 1854, 428, 2128, 2445, 293, 14699, 484, 264], "temperature": 0.0, "avg_logprob": -0.13324340941414, "compression_ratio": 1.6655405405405406, "no_speech_prob": 6.962151019251905e-06}, {"id": 69, "seek": 32596, "start": 336.28, "end": 340.47999999999996, "text": " sizes is something which is damn super helpful.", "tokens": [11602, 307, 746, 597, 307, 8151, 1687, 4961, 13], "temperature": 0.0, "avg_logprob": -0.13324340941414, "compression_ratio": 1.6655405405405406, "no_speech_prob": 6.962151019251905e-06}, {"id": 70, "seek": 32596, "start": 340.47999999999996, "end": 344.79999999999995, "text": " Or you could just put a print statement here as well.", "tokens": [1610, 291, 727, 445, 829, 257, 4482, 5629, 510, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.13324340941414, "compression_ratio": 1.6655405405405406, "no_speech_prob": 6.962151019251905e-06}, {"id": 71, "seek": 32596, "start": 344.79999999999995, "end": 348.35999999999996, "text": " So I actually don't know if that's how Chloe figured it out, but that's how I would if", "tokens": [407, 286, 767, 500, 380, 458, 498, 300, 311, 577, 29694, 8932, 309, 484, 11, 457, 300, 311, 577, 286, 576, 498], "temperature": 0.0, "avg_logprob": -0.13324340941414, "compression_ratio": 1.6655405405405406, "no_speech_prob": 6.962151019251905e-06}, {"id": 72, "seek": 32596, "start": 348.35999999999996, "end": 350.59999999999997, "text": " I was her.", "tokens": [286, 390, 720, 13], "temperature": 0.0, "avg_logprob": -0.13324340941414, "compression_ratio": 1.6655405405405406, "no_speech_prob": 6.962151019251905e-06}, {"id": 73, "seek": 32596, "start": 350.59999999999997, "end": 355.44, "text": " And then we talked about increasing k, which is the number of anchor boxes for each convolutional", "tokens": [400, 550, 321, 2825, 466, 5662, 350, 11, 597, 307, 264, 1230, 295, 18487, 9002, 337, 1184, 45216, 304], "temperature": 0.0, "avg_logprob": -0.13324340941414, "compression_ratio": 1.6655405405405406, "no_speech_prob": 6.962151019251905e-06}, {"id": 74, "seek": 35544, "start": 355.44, "end": 359.8, "text": " grid cell, which we can do with different zooms and different aspect ratios.", "tokens": [10748, 2815, 11, 597, 321, 393, 360, 365, 819, 5721, 4785, 293, 819, 4171, 32435, 13], "temperature": 0.0, "avg_logprob": -0.153436929276846, "compression_ratio": 1.6184738955823292, "no_speech_prob": 2.2473701392300427e-05}, {"id": 75, "seek": 35544, "start": 359.8, "end": 367.44, "text": " And so that gives us a plethora of activations and therefore predicted bounding boxes, which", "tokens": [400, 370, 300, 2709, 505, 257, 499, 302, 7013, 295, 2430, 763, 293, 4412, 19147, 5472, 278, 9002, 11, 597], "temperature": 0.0, "avg_logprob": -0.153436929276846, "compression_ratio": 1.6184738955823292, "no_speech_prob": 2.2473701392300427e-05}, {"id": 76, "seek": 35544, "start": 367.44, "end": 375.96, "text": " we then went down to a small number using non-maximum suppression.", "tokens": [321, 550, 1437, 760, 281, 257, 1359, 1230, 1228, 2107, 12, 1696, 3081, 449, 36807, 13], "temperature": 0.0, "avg_logprob": -0.153436929276846, "compression_ratio": 1.6184738955823292, "no_speech_prob": 2.2473701392300427e-05}, {"id": 77, "seek": 35544, "start": 375.96, "end": 377.44, "text": " And I'll try to remember to put a link.", "tokens": [400, 286, 603, 853, 281, 1604, 281, 829, 257, 2113, 13], "temperature": 0.0, "avg_logprob": -0.153436929276846, "compression_ratio": 1.6184738955823292, "no_speech_prob": 2.2473701392300427e-05}, {"id": 78, "seek": 35544, "start": 377.44, "end": 380.88, "text": " There's a really interesting paper that one of our students told me about that I hadn't", "tokens": [821, 311, 257, 534, 1880, 3035, 300, 472, 295, 527, 1731, 1907, 385, 466, 300, 286, 8782, 380], "temperature": 0.0, "avg_logprob": -0.153436929276846, "compression_ratio": 1.6184738955823292, "no_speech_prob": 2.2473701392300427e-05}, {"id": 79, "seek": 35544, "start": 380.88, "end": 383.88, "text": " heard about, which is attempting to...", "tokens": [2198, 466, 11, 597, 307, 22001, 281, 1097], "temperature": 0.0, "avg_logprob": -0.153436929276846, "compression_ratio": 1.6184738955823292, "no_speech_prob": 2.2473701392300427e-05}, {"id": 80, "seek": 38388, "start": 383.88, "end": 386.36, "text": " You know, I've mentioned non-maximum suppression.", "tokens": [509, 458, 11, 286, 600, 2835, 2107, 12, 1696, 3081, 449, 36807, 13], "temperature": 0.0, "avg_logprob": -0.18730755510001346, "compression_ratio": 1.5261194029850746, "no_speech_prob": 1.280535252590198e-05}, {"id": 81, "seek": 38388, "start": 386.36, "end": 390.08, "text": " It's like kind of hacky, kind of ugly, totally heuristic.", "tokens": [467, 311, 411, 733, 295, 10339, 88, 11, 733, 295, 12246, 11, 3879, 415, 374, 3142, 13], "temperature": 0.0, "avg_logprob": -0.18730755510001346, "compression_ratio": 1.5261194029850746, "no_speech_prob": 1.280535252590198e-05}, {"id": 82, "seek": 38388, "start": 390.08, "end": 395.0, "text": " I didn't even talk about the code because it seems kind of hideous.", "tokens": [286, 994, 380, 754, 751, 466, 264, 3089, 570, 309, 2544, 733, 295, 6479, 563, 13], "temperature": 0.0, "avg_logprob": -0.18730755510001346, "compression_ratio": 1.5261194029850746, "no_speech_prob": 1.280535252590198e-05}, {"id": 83, "seek": 38388, "start": 395.0, "end": 398.92, "text": " So somebody actually came up with a paper recently which attempts to do an end-to-end", "tokens": [407, 2618, 767, 1361, 493, 365, 257, 3035, 3938, 597, 15257, 281, 360, 364, 917, 12, 1353, 12, 521], "temperature": 0.0, "avg_logprob": -0.18730755510001346, "compression_ratio": 1.5261194029850746, "no_speech_prob": 1.280535252590198e-05}, {"id": 84, "seek": 38388, "start": 398.92, "end": 402.88, "text": " conv net to replace that NMS piece.", "tokens": [3754, 2533, 281, 7406, 300, 426, 10288, 2522, 13], "temperature": 0.0, "avg_logprob": -0.18730755510001346, "compression_ratio": 1.5261194029850746, "no_speech_prob": 1.280535252590198e-05}, {"id": 85, "seek": 38388, "start": 402.88, "end": 405.36, "text": " So I'll put that paper up.", "tokens": [407, 286, 603, 829, 300, 3035, 493, 13], "temperature": 0.0, "avg_logprob": -0.18730755510001346, "compression_ratio": 1.5261194029850746, "no_speech_prob": 1.280535252590198e-05}, {"id": 86, "seek": 38388, "start": 405.36, "end": 410.92, "text": " Nobody's created a PyTorch implementation yet, so it would be an interesting project", "tokens": [9297, 311, 2942, 257, 9953, 51, 284, 339, 11420, 1939, 11, 370, 309, 576, 312, 364, 1880, 1716], "temperature": 0.0, "avg_logprob": -0.18730755510001346, "compression_ratio": 1.5261194029850746, "no_speech_prob": 1.280535252590198e-05}, {"id": 87, "seek": 41092, "start": 410.92, "end": 415.84000000000003, "text": " if anybody wanted to try that.", "tokens": [498, 4472, 1415, 281, 853, 300, 13], "temperature": 0.0, "avg_logprob": -0.129572215833162, "compression_ratio": 1.75, "no_speech_prob": 1.6280465615636786e-06}, {"id": 88, "seek": 41092, "start": 415.84000000000003, "end": 420.84000000000003, "text": " One thing I've noticed in our study groups during the week is not enough people reading", "tokens": [1485, 551, 286, 600, 5694, 294, 527, 2979, 3935, 1830, 264, 1243, 307, 406, 1547, 561, 3760], "temperature": 0.0, "avg_logprob": -0.129572215833162, "compression_ratio": 1.75, "no_speech_prob": 1.6280465615636786e-06}, {"id": 89, "seek": 41092, "start": 420.84000000000003, "end": 423.26, "text": " papers.", "tokens": [10577, 13], "temperature": 0.0, "avg_logprob": -0.129572215833162, "compression_ratio": 1.75, "no_speech_prob": 1.6280465615636786e-06}, {"id": 90, "seek": 41092, "start": 423.26, "end": 426.56, "text": " What we are doing in class now is implementing papers.", "tokens": [708, 321, 366, 884, 294, 1508, 586, 307, 18114, 10577, 13], "temperature": 0.0, "avg_logprob": -0.129572215833162, "compression_ratio": 1.75, "no_speech_prob": 1.6280465615636786e-06}, {"id": 91, "seek": 41092, "start": 426.56, "end": 430.96000000000004, "text": " The papers are the real ground truth.", "tokens": [440, 10577, 366, 264, 957, 2727, 3494, 13], "temperature": 0.0, "avg_logprob": -0.129572215833162, "compression_ratio": 1.75, "no_speech_prob": 1.6280465615636786e-06}, {"id": 92, "seek": 41092, "start": 430.96000000000004, "end": 434.40000000000003, "text": " And I think from talking to people, a lot of the reason people aren't reading papers", "tokens": [400, 286, 519, 490, 1417, 281, 561, 11, 257, 688, 295, 264, 1778, 561, 3212, 380, 3760, 10577], "temperature": 0.0, "avg_logprob": -0.129572215833162, "compression_ratio": 1.75, "no_speech_prob": 1.6280465615636786e-06}, {"id": 93, "seek": 41092, "start": 434.40000000000003, "end": 438.12, "text": " is because a lot of people don't think they're capable of reading papers.", "tokens": [307, 570, 257, 688, 295, 561, 500, 380, 519, 436, 434, 8189, 295, 3760, 10577, 13], "temperature": 0.0, "avg_logprob": -0.129572215833162, "compression_ratio": 1.75, "no_speech_prob": 1.6280465615636786e-06}, {"id": 94, "seek": 43812, "start": 438.12, "end": 441.44, "text": " They don't think they're the kind of people that read papers.", "tokens": [814, 500, 380, 519, 436, 434, 264, 733, 295, 561, 300, 1401, 10577, 13], "temperature": 0.0, "avg_logprob": -0.22659723493787978, "compression_ratio": 1.5412844036697249, "no_speech_prob": 2.9022860417171614e-06}, {"id": 95, "seek": 43812, "start": 441.44, "end": 442.44, "text": " But you are.", "tokens": [583, 291, 366, 13], "temperature": 0.0, "avg_logprob": -0.22659723493787978, "compression_ratio": 1.5412844036697249, "no_speech_prob": 2.9022860417171614e-06}, {"id": 96, "seek": 43812, "start": 442.44, "end": 443.44, "text": " You're here.", "tokens": [509, 434, 510, 13], "temperature": 0.0, "avg_logprob": -0.22659723493787978, "compression_ratio": 1.5412844036697249, "no_speech_prob": 2.9022860417171614e-06}, {"id": 97, "seek": 43812, "start": 443.44, "end": 449.4, "text": " We started looking at a paper last week and we read the words that were in English and", "tokens": [492, 1409, 1237, 412, 257, 3035, 1036, 1243, 293, 321, 1401, 264, 2283, 300, 645, 294, 3669, 293], "temperature": 0.0, "avg_logprob": -0.22659723493787978, "compression_ratio": 1.5412844036697249, "no_speech_prob": 2.9022860417171614e-06}, {"id": 98, "seek": 43812, "start": 449.4, "end": 452.16, "text": " we largely understood them.", "tokens": [321, 11611, 7320, 552, 13], "temperature": 0.0, "avg_logprob": -0.22659723493787978, "compression_ratio": 1.5412844036697249, "no_speech_prob": 2.9022860417171614e-06}, {"id": 99, "seek": 43812, "start": 452.16, "end": 462.32, "text": " So if you actually look through this picture from SSD, carefully, you'll realize SSD multihead.forward", "tokens": [407, 498, 291, 767, 574, 807, 341, 3036, 490, 30262, 11, 7500, 11, 291, 603, 4325, 30262, 4825, 1934, 13, 13305], "temperature": 0.0, "avg_logprob": -0.22659723493787978, "compression_ratio": 1.5412844036697249, "no_speech_prob": 2.9022860417171614e-06}, {"id": 100, "seek": 43812, "start": 462.32, "end": 464.92, "text": " is not doing the same as this.", "tokens": [307, 406, 884, 264, 912, 382, 341, 13], "temperature": 0.0, "avg_logprob": -0.22659723493787978, "compression_ratio": 1.5412844036697249, "no_speech_prob": 2.9022860417171614e-06}, {"id": 101, "seek": 46492, "start": 464.92, "end": 469.28000000000003, "text": " And then you might think, oh, I wonder if this is better.", "tokens": [400, 550, 291, 1062, 519, 11, 1954, 11, 286, 2441, 498, 341, 307, 1101, 13], "temperature": 0.0, "avg_logprob": -0.23871159317469834, "compression_ratio": 1.5206611570247934, "no_speech_prob": 7.766888302285224e-06}, {"id": 102, "seek": 46492, "start": 469.28000000000003, "end": 471.64000000000004, "text": " My answer is probably.", "tokens": [1222, 1867, 307, 1391, 13], "temperature": 0.0, "avg_logprob": -0.23871159317469834, "compression_ratio": 1.5206611570247934, "no_speech_prob": 7.766888302285224e-06}, {"id": 103, "seek": 46492, "start": 471.64000000000004, "end": 478.36, "text": " Because SSD multihead.forward was the first thing I tried just to get something out there.", "tokens": [1436, 30262, 4825, 1934, 13, 13305, 390, 264, 700, 551, 286, 3031, 445, 281, 483, 746, 484, 456, 13], "temperature": 0.0, "avg_logprob": -0.23871159317469834, "compression_ratio": 1.5206611570247934, "no_speech_prob": 7.766888302285224e-06}, {"id": 104, "seek": 46492, "start": 478.36, "end": 484.76, "text": " But between this and the YOLO version 3 paper and stuff, there are probably much better", "tokens": [583, 1296, 341, 293, 264, 398, 5046, 46, 3037, 805, 3035, 293, 1507, 11, 456, 366, 1391, 709, 1101], "temperature": 0.0, "avg_logprob": -0.23871159317469834, "compression_ratio": 1.5206611570247934, "no_speech_prob": 7.766888302285224e-06}, {"id": 105, "seek": 46492, "start": 484.76, "end": 485.76, "text": " ways.", "tokens": [2098, 13], "temperature": 0.0, "avg_logprob": -0.23871159317469834, "compression_ratio": 1.5206611570247934, "no_speech_prob": 7.766888302285224e-06}, {"id": 106, "seek": 46492, "start": 485.76, "end": 490.64, "text": " One thing you'll notice in particular is they use a smaller K, but they have a lot more", "tokens": [1485, 551, 291, 603, 3449, 294, 1729, 307, 436, 764, 257, 4356, 591, 11, 457, 436, 362, 257, 688, 544], "temperature": 0.0, "avg_logprob": -0.23871159317469834, "compression_ratio": 1.5206611570247934, "no_speech_prob": 7.766888302285224e-06}, {"id": 107, "seek": 46492, "start": 490.64, "end": 491.64, "text": " sets of grids.", "tokens": [6352, 295, 677, 3742, 13], "temperature": 0.0, "avg_logprob": -0.23871159317469834, "compression_ratio": 1.5206611570247934, "no_speech_prob": 7.766888302285224e-06}, {"id": 108, "seek": 49164, "start": 491.64, "end": 502.28, "text": " 1x1, 3x3, 5x5, 10x10, 19x19, 38x38, 8700 per class, so a lot more than we had.", "tokens": [502, 87, 16, 11, 805, 87, 18, 11, 1025, 87, 20, 11, 1266, 87, 3279, 11, 1294, 87, 3405, 11, 12843, 87, 12625, 11, 1649, 18197, 680, 1508, 11, 370, 257, 688, 544, 813, 321, 632, 13], "temperature": 0.0, "avg_logprob": -0.1881161095961085, "compression_ratio": 1.4976076555023923, "no_speech_prob": 1.5534895965174655e-06}, {"id": 109, "seek": 49164, "start": 502.28, "end": 505.64, "text": " So that would be an interesting thing to experiment with.", "tokens": [407, 300, 576, 312, 364, 1880, 551, 281, 5120, 365, 13], "temperature": 0.0, "avg_logprob": -0.1881161095961085, "compression_ratio": 1.4976076555023923, "no_speech_prob": 1.5534895965174655e-06}, {"id": 110, "seek": 49164, "start": 505.64, "end": 512.64, "text": " Another thing I noticed is that whereas we had 4x4, 2x2, 1x1, which means there's a lot", "tokens": [3996, 551, 286, 5694, 307, 300, 9735, 321, 632, 1017, 87, 19, 11, 568, 87, 17, 11, 502, 87, 16, 11, 597, 1355, 456, 311, 257, 688], "temperature": 0.0, "avg_logprob": -0.1881161095961085, "compression_ratio": 1.4976076555023923, "no_speech_prob": 1.5534895965174655e-06}, {"id": 111, "seek": 49164, "start": 512.64, "end": 519.2, "text": " of overlap, like every set fits within every other set, in this case where you've got 1,", "tokens": [295, 19959, 11, 411, 633, 992, 9001, 1951, 633, 661, 992, 11, 294, 341, 1389, 689, 291, 600, 658, 502, 11], "temperature": 0.0, "avg_logprob": -0.1881161095961085, "compression_ratio": 1.4976076555023923, "no_speech_prob": 1.5534895965174655e-06}, {"id": 112, "seek": 51920, "start": 519.2, "end": 522.32, "text": " 5, you don't have that overlap.", "tokens": [1025, 11, 291, 500, 380, 362, 300, 19959, 13], "temperature": 0.0, "avg_logprob": -0.2202302056389886, "compression_ratio": 1.6823529411764706, "no_speech_prob": 1.7231330275535583e-05}, {"id": 113, "seek": 51920, "start": 522.32, "end": 524.1600000000001, "text": " So it might actually make it easier to learn.", "tokens": [407, 309, 1062, 767, 652, 309, 3571, 281, 1466, 13], "temperature": 0.0, "avg_logprob": -0.2202302056389886, "compression_ratio": 1.6823529411764706, "no_speech_prob": 1.7231330275535583e-05}, {"id": 114, "seek": 51920, "start": 524.1600000000001, "end": 529.24, "text": " So there's lots of interesting things you can play with based on stuff that's either", "tokens": [407, 456, 311, 3195, 295, 1880, 721, 291, 393, 862, 365, 2361, 322, 1507, 300, 311, 2139], "temperature": 0.0, "avg_logprob": -0.2202302056389886, "compression_ratio": 1.6823529411764706, "no_speech_prob": 1.7231330275535583e-05}, {"id": 115, "seek": 51920, "start": 529.24, "end": 533.0400000000001, "text": " trying to make it closer to the paper or think about other things you could try that aren't", "tokens": [1382, 281, 652, 309, 4966, 281, 264, 3035, 420, 519, 466, 661, 721, 291, 727, 853, 300, 3212, 380], "temperature": 0.0, "avg_logprob": -0.2202302056389886, "compression_ratio": 1.6823529411764706, "no_speech_prob": 1.7231330275535583e-05}, {"id": 116, "seek": 51920, "start": 533.0400000000001, "end": 535.76, "text": " in the paper or whatever.", "tokens": [294, 264, 3035, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.2202302056389886, "compression_ratio": 1.6823529411764706, "no_speech_prob": 1.7231330275535583e-05}, {"id": 117, "seek": 51920, "start": 535.76, "end": 542.5600000000001, "text": " Perhaps the most important thing I would recommend is to put the code and the equations next", "tokens": [10517, 264, 881, 1021, 551, 286, 576, 2748, 307, 281, 829, 264, 3089, 293, 264, 11787, 958], "temperature": 0.0, "avg_logprob": -0.2202302056389886, "compression_ratio": 1.6823529411764706, "no_speech_prob": 1.7231330275535583e-05}, {"id": 118, "seek": 51920, "start": 542.5600000000001, "end": 543.5600000000001, "text": " to each other.", "tokens": [281, 1184, 661, 13], "temperature": 0.0, "avg_logprob": -0.2202302056389886, "compression_ratio": 1.6823529411764706, "no_speech_prob": 1.7231330275535583e-05}, {"id": 119, "seek": 51920, "start": 543.5600000000001, "end": 544.5600000000001, "text": " Yes, Rachel.", "tokens": [1079, 11, 14246, 13], "temperature": 0.0, "avg_logprob": -0.2202302056389886, "compression_ratio": 1.6823529411764706, "no_speech_prob": 1.7231330275535583e-05}, {"id": 120, "seek": 51920, "start": 544.5600000000001, "end": 545.5600000000001, "text": " Question from the audience.", "tokens": [14464, 490, 264, 4034, 13], "temperature": 0.0, "avg_logprob": -0.2202302056389886, "compression_ratio": 1.6823529411764706, "no_speech_prob": 1.7231330275535583e-05}, {"id": 121, "seek": 54556, "start": 545.56, "end": 549.3199999999999, "text": " Question of whether you could speak about the use cyclic learning rate argument in the", "tokens": [14464, 295, 1968, 291, 727, 1710, 466, 264, 764, 38154, 1050, 2539, 3314, 6770, 294, 264], "temperature": 0.0, "avg_logprob": -0.20167623247419084, "compression_ratio": 1.6431535269709543, "no_speech_prob": 1.5936473573674448e-05}, {"id": 122, "seek": 54556, "start": 549.3199999999999, "end": 550.3199999999999, "text": " fit function.", "tokens": [3318, 2445, 13], "temperature": 0.0, "avg_logprob": -0.20167623247419084, "compression_ratio": 1.6431535269709543, "no_speech_prob": 1.5936473573674448e-05}, {"id": 123, "seek": 54556, "start": 550.3199999999999, "end": 553.9599999999999, "text": " We will get there, yes.", "tokens": [492, 486, 483, 456, 11, 2086, 13], "temperature": 0.0, "avg_logprob": -0.20167623247419084, "compression_ratio": 1.6431535269709543, "no_speech_prob": 1.5936473573674448e-05}, {"id": 124, "seek": 54556, "start": 553.9599999999999, "end": 560.88, "text": " So put the code and the equations from the paper next to each other.", "tokens": [407, 829, 264, 3089, 293, 264, 11787, 490, 264, 3035, 958, 281, 1184, 661, 13], "temperature": 0.0, "avg_logprob": -0.20167623247419084, "compression_ratio": 1.6431535269709543, "no_speech_prob": 1.5936473573674448e-05}, {"id": 125, "seek": 54556, "start": 560.88, "end": 563.3199999999999, "text": " And draw in one of two groups.", "tokens": [400, 2642, 294, 472, 295, 732, 3935, 13], "temperature": 0.0, "avg_logprob": -0.20167623247419084, "compression_ratio": 1.6431535269709543, "no_speech_prob": 1.5936473573674448e-05}, {"id": 126, "seek": 54556, "start": 563.3199999999999, "end": 569.52, "text": " You're either a code person like me who's not that happy about math, in which case I", "tokens": [509, 434, 2139, 257, 3089, 954, 411, 385, 567, 311, 406, 300, 2055, 466, 5221, 11, 294, 597, 1389, 286], "temperature": 0.0, "avg_logprob": -0.20167623247419084, "compression_ratio": 1.6431535269709543, "no_speech_prob": 1.5936473573674448e-05}, {"id": 127, "seek": 54556, "start": 569.52, "end": 574.56, "text": " start with the code and then I look at the math and I learn about how the math maps to", "tokens": [722, 365, 264, 3089, 293, 550, 286, 574, 412, 264, 5221, 293, 286, 1466, 466, 577, 264, 5221, 11317, 281], "temperature": 0.0, "avg_logprob": -0.20167623247419084, "compression_ratio": 1.6431535269709543, "no_speech_prob": 1.5936473573674448e-05}, {"id": 128, "seek": 57456, "start": 574.56, "end": 578.5999999999999, "text": " the code and end up eventually understanding the math.", "tokens": [264, 3089, 293, 917, 493, 4728, 3701, 264, 5221, 13], "temperature": 0.0, "avg_logprob": -0.17517922141335227, "compression_ratio": 1.6096491228070176, "no_speech_prob": 5.714992994398926e-07}, {"id": 129, "seek": 57456, "start": 578.5999999999999, "end": 587.4, "text": " All your PhD in stochastic differential equations like Rachel, whatever that means, in which", "tokens": [1057, 428, 14476, 294, 342, 8997, 2750, 15756, 11787, 411, 14246, 11, 2035, 300, 1355, 11, 294, 597], "temperature": 0.0, "avg_logprob": -0.17517922141335227, "compression_ratio": 1.6096491228070176, "no_speech_prob": 5.714992994398926e-07}, {"id": 130, "seek": 57456, "start": 587.4, "end": 593.04, "text": " case you can look at the math and then learn about how the code implements the math.", "tokens": [1389, 291, 393, 574, 412, 264, 5221, 293, 550, 1466, 466, 577, 264, 3089, 704, 17988, 264, 5221, 13], "temperature": 0.0, "avg_logprob": -0.17517922141335227, "compression_ratio": 1.6096491228070176, "no_speech_prob": 5.714992994398926e-07}, {"id": 131, "seek": 57456, "start": 593.04, "end": 597.1999999999999, "text": " But either way, unless you're one of those rare people who's equally comfortable in either", "tokens": [583, 2139, 636, 11, 5969, 291, 434, 472, 295, 729, 5892, 561, 567, 311, 12309, 4619, 294, 2139], "temperature": 0.0, "avg_logprob": -0.17517922141335227, "compression_ratio": 1.6096491228070176, "no_speech_prob": 5.714992994398926e-07}, {"id": 132, "seek": 57456, "start": 597.1999999999999, "end": 602.8, "text": " world, you'll learn about one or the other.", "tokens": [1002, 11, 291, 603, 1466, 466, 472, 420, 264, 661, 13], "temperature": 0.0, "avg_logprob": -0.17517922141335227, "compression_ratio": 1.6096491228070176, "no_speech_prob": 5.714992994398926e-07}, {"id": 133, "seek": 60280, "start": 602.8, "end": 606.88, "text": " Now learning about code is pretty easy because there's documentation and we know how it's", "tokens": [823, 2539, 466, 3089, 307, 1238, 1858, 570, 456, 311, 14333, 293, 321, 458, 577, 309, 311], "temperature": 0.0, "avg_logprob": -0.16090298514080864, "compression_ratio": 1.708185053380783, "no_speech_prob": 4.565947165247053e-06}, {"id": 134, "seek": 60280, "start": 606.88, "end": 609.3599999999999, "text": " indexed and we know how to look it up and so forth.", "tokens": [8186, 292, 293, 321, 458, 577, 281, 574, 309, 493, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.16090298514080864, "compression_ratio": 1.708185053380783, "no_speech_prob": 4.565947165247053e-06}, {"id": 135, "seek": 60280, "start": 609.3599999999999, "end": 613.8399999999999, "text": " Sometimes learning the math is hard because the notation might seem hard to look up, but", "tokens": [4803, 2539, 264, 5221, 307, 1152, 570, 264, 24657, 1062, 1643, 1152, 281, 574, 493, 11, 457], "temperature": 0.0, "avg_logprob": -0.16090298514080864, "compression_ratio": 1.708185053380783, "no_speech_prob": 4.565947165247053e-06}, {"id": 136, "seek": 60280, "start": 613.8399999999999, "end": 615.76, "text": " there's actually a lot of resources.", "tokens": [456, 311, 767, 257, 688, 295, 3593, 13], "temperature": 0.0, "avg_logprob": -0.16090298514080864, "compression_ratio": 1.708185053380783, "no_speech_prob": 4.565947165247053e-06}, {"id": 137, "seek": 60280, "start": 615.76, "end": 621.5999999999999, "text": " For example, the list of mathematical symbols on Wikipedia is amazingly great.", "tokens": [1171, 1365, 11, 264, 1329, 295, 18894, 16944, 322, 28999, 307, 31762, 869, 13], "temperature": 0.0, "avg_logprob": -0.16090298514080864, "compression_ratio": 1.708185053380783, "no_speech_prob": 4.565947165247053e-06}, {"id": 138, "seek": 60280, "start": 621.5999999999999, "end": 627.28, "text": " It has examples of them, explanations of what they mean, and tells you what to search for", "tokens": [467, 575, 5110, 295, 552, 11, 28708, 295, 437, 436, 914, 11, 293, 5112, 291, 437, 281, 3164, 337], "temperature": 0.0, "avg_logprob": -0.16090298514080864, "compression_ratio": 1.708185053380783, "no_speech_prob": 4.565947165247053e-06}, {"id": 139, "seek": 60280, "start": 627.28, "end": 630.92, "text": " to find out more about it.", "tokens": [281, 915, 484, 544, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.16090298514080864, "compression_ratio": 1.708185053380783, "no_speech_prob": 4.565947165247053e-06}, {"id": 140, "seek": 60280, "start": 630.92, "end": 631.92, "text": " Really terrific.", "tokens": [4083, 20899, 13], "temperature": 0.0, "avg_logprob": -0.16090298514080864, "compression_ratio": 1.708185053380783, "no_speech_prob": 4.565947165247053e-06}, {"id": 141, "seek": 63192, "start": 631.92, "end": 638.12, "text": " If you Google for math notation cheat sheet, you'll find more of these kinds of terrific", "tokens": [759, 291, 3329, 337, 5221, 24657, 17470, 8193, 11, 291, 603, 915, 544, 295, 613, 3685, 295, 20899], "temperature": 0.0, "avg_logprob": -0.16684856229615444, "compression_ratio": 1.662551440329218, "no_speech_prob": 1.101592943086871e-06}, {"id": 142, "seek": 63192, "start": 638.12, "end": 639.12, "text": " resources.", "tokens": [3593, 13], "temperature": 0.0, "avg_logprob": -0.16684856229615444, "compression_ratio": 1.662551440329218, "no_speech_prob": 1.101592943086871e-06}, {"id": 143, "seek": 63192, "start": 639.12, "end": 646.64, "text": " So, over time, you do need to learn the notation, but as you'll see from the Wikipedia page,", "tokens": [407, 11, 670, 565, 11, 291, 360, 643, 281, 1466, 264, 24657, 11, 457, 382, 291, 603, 536, 490, 264, 28999, 3028, 11], "temperature": 0.0, "avg_logprob": -0.16684856229615444, "compression_ratio": 1.662551440329218, "no_speech_prob": 1.101592943086871e-06}, {"id": 144, "seek": 63192, "start": 646.64, "end": 649.64, "text": " there's not actually that much of it.", "tokens": [456, 311, 406, 767, 300, 709, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.16684856229615444, "compression_ratio": 1.662551440329218, "no_speech_prob": 1.101592943086871e-06}, {"id": 145, "seek": 63192, "start": 649.64, "end": 653.88, "text": " Obviously there's a lot of concepts behind it, but once you know the notation, you can", "tokens": [7580, 456, 311, 257, 688, 295, 10392, 2261, 309, 11, 457, 1564, 291, 458, 264, 24657, 11, 291, 393], "temperature": 0.0, "avg_logprob": -0.16684856229615444, "compression_ratio": 1.662551440329218, "no_speech_prob": 1.101592943086871e-06}, {"id": 146, "seek": 63192, "start": 653.88, "end": 659.1999999999999, "text": " then quickly look up the concept as it pertains to a particular thing you're studying.", "tokens": [550, 2661, 574, 493, 264, 3410, 382, 309, 13269, 2315, 281, 257, 1729, 551, 291, 434, 7601, 13], "temperature": 0.0, "avg_logprob": -0.16684856229615444, "compression_ratio": 1.662551440329218, "no_speech_prob": 1.101592943086871e-06}, {"id": 147, "seek": 65920, "start": 659.2, "end": 665.48, "text": " Nobody learns all of math and then starts machine learning.", "tokens": [9297, 27152, 439, 295, 5221, 293, 550, 3719, 3479, 2539, 13], "temperature": 0.0, "avg_logprob": -0.16446304321289062, "compression_ratio": 1.6390243902439023, "no_speech_prob": 2.0580419004545547e-06}, {"id": 148, "seek": 65920, "start": 665.48, "end": 670.96, "text": " Everybody, even top researchers I know, when they're reading a new paper, will very often", "tokens": [7646, 11, 754, 1192, 10309, 286, 458, 11, 562, 436, 434, 3760, 257, 777, 3035, 11, 486, 588, 2049], "temperature": 0.0, "avg_logprob": -0.16446304321289062, "compression_ratio": 1.6390243902439023, "no_speech_prob": 2.0580419004545547e-06}, {"id": 149, "seek": 65920, "start": 670.96, "end": 675.2, "text": " come to bits of math they haven't seen before and they'll have to go away and learn that", "tokens": [808, 281, 9239, 295, 5221, 436, 2378, 380, 1612, 949, 293, 436, 603, 362, 281, 352, 1314, 293, 1466, 300], "temperature": 0.0, "avg_logprob": -0.16446304321289062, "compression_ratio": 1.6390243902439023, "no_speech_prob": 2.0580419004545547e-06}, {"id": 150, "seek": 65920, "start": 675.2, "end": 678.84, "text": " bit of math.", "tokens": [857, 295, 5221, 13], "temperature": 0.0, "avg_logprob": -0.16446304321289062, "compression_ratio": 1.6390243902439023, "no_speech_prob": 2.0580419004545547e-06}, {"id": 151, "seek": 65920, "start": 678.84, "end": 684.72, "text": " Another thing you should try doing is to recreate things that you see in the papers.", "tokens": [3996, 551, 291, 820, 853, 884, 307, 281, 25833, 721, 300, 291, 536, 294, 264, 10577, 13], "temperature": 0.0, "avg_logprob": -0.16446304321289062, "compression_ratio": 1.6390243902439023, "no_speech_prob": 2.0580419004545547e-06}, {"id": 152, "seek": 68472, "start": 684.72, "end": 692.44, "text": " So here was the key, most important, figure 1 from the focal loss paper, the retina paper.", "tokens": [407, 510, 390, 264, 2141, 11, 881, 1021, 11, 2573, 502, 490, 264, 26592, 4470, 3035, 11, 264, 1533, 1426, 3035, 13], "temperature": 0.0, "avg_logprob": -0.27625054950955547, "compression_ratio": 1.5698324022346368, "no_speech_prob": 3.237745431761141e-06}, {"id": 153, "seek": 68472, "start": 692.44, "end": 695.08, "text": " So recreate it.", "tokens": [407, 25833, 309, 13], "temperature": 0.0, "avg_logprob": -0.27625054950955547, "compression_ratio": 1.5698324022346368, "no_speech_prob": 3.237745431761141e-06}, {"id": 154, "seek": 68472, "start": 695.08, "end": 700.0, "text": " And very often, I put these challenges up on the forums.", "tokens": [400, 588, 2049, 11, 286, 829, 613, 4759, 493, 322, 264, 26998, 13], "temperature": 0.0, "avg_logprob": -0.27625054950955547, "compression_ratio": 1.5698324022346368, "no_speech_prob": 3.237745431761141e-06}, {"id": 155, "seek": 68472, "start": 700.0, "end": 703.0400000000001, "text": " So keep an eye on the forums.", "tokens": [407, 1066, 364, 3313, 322, 264, 26998, 13], "temperature": 0.0, "avg_logprob": -0.27625054950955547, "compression_ratio": 1.5698324022346368, "no_speech_prob": 3.237745431761141e-06}, {"id": 156, "seek": 68472, "start": 703.0400000000001, "end": 708.72, "text": " So I put this challenge up there and within about 3 minutes, Serata had said done it in", "tokens": [407, 286, 829, 341, 3430, 493, 456, 293, 1951, 466, 805, 2077, 11, 4210, 3274, 632, 848, 1096, 309, 294], "temperature": 0.0, "avg_logprob": -0.27625054950955547, "compression_ratio": 1.5698324022346368, "no_speech_prob": 3.237745431761141e-06}, {"id": 157, "seek": 70872, "start": 708.72, "end": 715.6, "text": " Microsoft Excel naturally, along with a lot more information than the original paper.", "tokens": [8116, 19060, 8195, 11, 2051, 365, 257, 688, 544, 1589, 813, 264, 3380, 3035, 13], "temperature": 0.0, "avg_logprob": -0.17268357625821742, "compression_ratio": 1.4739130434782608, "no_speech_prob": 1.6187277651624754e-05}, {"id": 158, "seek": 70872, "start": 715.6, "end": 720.64, "text": " The nice thing here is that she was actually able to draw a line showing at a 0.5 ground", "tokens": [440, 1481, 551, 510, 307, 300, 750, 390, 767, 1075, 281, 2642, 257, 1622, 4099, 412, 257, 1958, 13, 20, 2727], "temperature": 0.0, "avg_logprob": -0.17268357625821742, "compression_ratio": 1.4739130434782608, "no_speech_prob": 1.6187277651624754e-05}, {"id": 159, "seek": 70872, "start": 720.64, "end": 727.4, "text": " truth probability what's the loss for different amounts of gamma, which is kind of cool.", "tokens": [3494, 8482, 437, 311, 264, 4470, 337, 819, 11663, 295, 15546, 11, 597, 307, 733, 295, 1627, 13], "temperature": 0.0, "avg_logprob": -0.17268357625821742, "compression_ratio": 1.4739130434782608, "no_speech_prob": 1.6187277651624754e-05}, {"id": 160, "seek": 70872, "start": 727.4, "end": 735.8000000000001, "text": " And if you want to cheat, she's also provided Python code on the forum too.", "tokens": [400, 498, 291, 528, 281, 17470, 11, 750, 311, 611, 5649, 15329, 3089, 322, 264, 17542, 886, 13], "temperature": 0.0, "avg_logprob": -0.17268357625821742, "compression_ratio": 1.4739130434782608, "no_speech_prob": 1.6187277651624754e-05}, {"id": 161, "seek": 73580, "start": 735.8, "end": 739.24, "text": " I did discover a minor bug in my code last week.", "tokens": [286, 630, 4411, 257, 6696, 7426, 294, 452, 3089, 1036, 1243, 13], "temperature": 0.0, "avg_logprob": -0.1765083063428647, "compression_ratio": 1.5772357723577235, "no_speech_prob": 1.9222836272092536e-05}, {"id": 162, "seek": 73580, "start": 739.24, "end": 745.16, "text": " The way that I was flattening out the convolutional activations did not line up with how I was", "tokens": [440, 636, 300, 286, 390, 24183, 278, 484, 264, 45216, 304, 2430, 763, 630, 406, 1622, 493, 365, 577, 286, 390], "temperature": 0.0, "avg_logprob": -0.1765083063428647, "compression_ratio": 1.5772357723577235, "no_speech_prob": 1.9222836272092536e-05}, {"id": 163, "seek": 73580, "start": 745.16, "end": 746.92, "text": " using them in the loss function.", "tokens": [1228, 552, 294, 264, 4470, 2445, 13], "temperature": 0.0, "avg_logprob": -0.1765083063428647, "compression_ratio": 1.5772357723577235, "no_speech_prob": 1.9222836272092536e-05}, {"id": 164, "seek": 73580, "start": 746.92, "end": 752.1999999999999, "text": " Fixing that actually made it quite a bit better, so my motorbikes and cows and stuff are actually", "tokens": [25538, 278, 300, 767, 1027, 309, 1596, 257, 857, 1101, 11, 370, 452, 5932, 65, 8916, 293, 19148, 293, 1507, 366, 767], "temperature": 0.0, "avg_logprob": -0.1765083063428647, "compression_ratio": 1.5772357723577235, "no_speech_prob": 1.9222836272092536e-05}, {"id": 165, "seek": 73580, "start": 752.1999999999999, "end": 753.1999999999999, "text": " in the right place.", "tokens": [294, 264, 558, 1081, 13], "temperature": 0.0, "avg_logprob": -0.1765083063428647, "compression_ratio": 1.5772357723577235, "no_speech_prob": 1.9222836272092536e-05}, {"id": 166, "seek": 73580, "start": 753.1999999999999, "end": 759.4799999999999, "text": " So when you go back to the notebook, you'll see it's a little less bad than it was last", "tokens": [407, 562, 291, 352, 646, 281, 264, 21060, 11, 291, 603, 536, 309, 311, 257, 707, 1570, 1578, 813, 309, 390, 1036], "temperature": 0.0, "avg_logprob": -0.1765083063428647, "compression_ratio": 1.5772357723577235, "no_speech_prob": 1.9222836272092536e-05}, {"id": 167, "seek": 73580, "start": 759.4799999999999, "end": 761.18, "text": " time.", "tokens": [565, 13], "temperature": 0.0, "avg_logprob": -0.1765083063428647, "compression_ratio": 1.5772357723577235, "no_speech_prob": 1.9222836272092536e-05}, {"id": 168, "seek": 76118, "start": 761.18, "end": 768.12, "text": " So there's some quick coverage of what's gone before.", "tokens": [407, 456, 311, 512, 1702, 9645, 295, 437, 311, 2780, 949, 13], "temperature": 0.0, "avg_logprob": -0.28720161941025285, "compression_ratio": 1.4866071428571428, "no_speech_prob": 1.0451356502017006e-05}, {"id": 169, "seek": 76118, "start": 768.12, "end": 769.12, "text": " Yes?", "tokens": [1079, 30], "temperature": 0.0, "avg_logprob": -0.28720161941025285, "compression_ratio": 1.4866071428571428, "no_speech_prob": 1.0451356502017006e-05}, {"id": 170, "seek": 76118, "start": 769.12, "end": 774.4399999999999, "text": " Question, are you going to put the PowerPoint on GitHub?", "tokens": [14464, 11, 366, 291, 516, 281, 829, 264, 25584, 322, 23331, 30], "temperature": 0.0, "avg_logprob": -0.28720161941025285, "compression_ratio": 1.4866071428571428, "no_speech_prob": 1.0451356502017006e-05}, {"id": 171, "seek": 76118, "start": 774.4399999999999, "end": 778.2399999999999, "text": " I'll put a subset of it on GitHub.", "tokens": [286, 603, 829, 257, 25993, 295, 309, 322, 23331, 13], "temperature": 0.0, "avg_logprob": -0.28720161941025285, "compression_ratio": 1.4866071428571428, "no_speech_prob": 1.0451356502017006e-05}, {"id": 172, "seek": 76118, "start": 778.2399999999999, "end": 784.12, "text": " And then secondly, usually when we down sample, we increase the number of filters or depth.", "tokens": [400, 550, 26246, 11, 2673, 562, 321, 760, 6889, 11, 321, 3488, 264, 1230, 295, 15995, 420, 7161, 13], "temperature": 0.0, "avg_logprob": -0.28720161941025285, "compression_ratio": 1.4866071428571428, "no_speech_prob": 1.0451356502017006e-05}, {"id": 173, "seek": 76118, "start": 784.12, "end": 791.16, "text": " When we're doing sampling from 77 to 44, why are we decreasing the number from 512 to 256?", "tokens": [1133, 321, 434, 884, 21179, 490, 25546, 281, 16408, 11, 983, 366, 321, 23223, 264, 1230, 490, 1025, 4762, 281, 38882, 30], "temperature": 0.0, "avg_logprob": -0.28720161941025285, "compression_ratio": 1.4866071428571428, "no_speech_prob": 1.0451356502017006e-05}, {"id": 174, "seek": 79116, "start": 791.16, "end": 794.24, "text": " Why not decrease dimension in SSD head?", "tokens": [1545, 406, 11514, 10139, 294, 30262, 1378, 30], "temperature": 0.0, "avg_logprob": -0.49381256103515625, "compression_ratio": 1.304635761589404, "no_speech_prob": 1.0952864613500424e-05}, {"id": 175, "seek": 79116, "start": 794.24, "end": 795.7199999999999, "text": " Is it performance related?", "tokens": [1119, 309, 3389, 4077, 30], "temperature": 0.0, "avg_logprob": -0.49381256103515625, "compression_ratio": 1.304635761589404, "no_speech_prob": 1.0952864613500424e-05}, {"id": 176, "seek": 79116, "start": 795.7199999999999, "end": 798.7199999999999, "text": " 7x7 to 4x4?", "tokens": [1614, 87, 22, 281, 1017, 87, 19, 30], "temperature": 0.0, "avg_logprob": -0.49381256103515625, "compression_ratio": 1.304635761589404, "no_speech_prob": 1.0952864613500424e-05}, {"id": 177, "seek": 79116, "start": 798.7199999999999, "end": 812.92, "text": " I guess they've got the star of 70.", "tokens": [286, 2041, 436, 600, 658, 264, 3543, 295, 5285, 13], "temperature": 0.0, "avg_logprob": -0.49381256103515625, "compression_ratio": 1.304635761589404, "no_speech_prob": 1.0952864613500424e-05}, {"id": 178, "seek": 79116, "start": 812.92, "end": 820.72, "text": " It's because, well largely it's because that's kind of what the papers tend to do.", "tokens": [467, 311, 570, 11, 731, 11611, 309, 311, 570, 300, 311, 733, 295, 437, 264, 10577, 3928, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.49381256103515625, "compression_ratio": 1.304635761589404, "no_speech_prob": 1.0952864613500424e-05}, {"id": 179, "seek": 82072, "start": 820.72, "end": 826.2, "text": " We have a number of out-paths and we kind of want each one to be the same, so we don't", "tokens": [492, 362, 257, 1230, 295, 484, 12, 31852, 82, 293, 321, 733, 295, 528, 1184, 472, 281, 312, 264, 912, 11, 370, 321, 500, 380], "temperature": 0.0, "avg_logprob": -0.18526427599848533, "compression_ratio": 1.6126126126126126, "no_speech_prob": 5.255313226371072e-06}, {"id": 180, "seek": 82072, "start": 826.2, "end": 831.12, "text": " want each one to have a different number of filters.", "tokens": [528, 1184, 472, 281, 362, 257, 819, 1230, 295, 15995, 13], "temperature": 0.0, "avg_logprob": -0.18526427599848533, "compression_ratio": 1.6126126126126126, "no_speech_prob": 5.255313226371072e-06}, {"id": 181, "seek": 82072, "start": 831.12, "end": 835.72, "text": " And also this is what the papers did, so I was trying to match up with that with having", "tokens": [400, 611, 341, 307, 437, 264, 10577, 630, 11, 370, 286, 390, 1382, 281, 2995, 493, 365, 300, 365, 1419], "temperature": 0.0, "avg_logprob": -0.18526427599848533, "compression_ratio": 1.6126126126126126, "no_speech_prob": 5.255313226371072e-06}, {"id": 182, "seek": 82072, "start": 835.72, "end": 837.8000000000001, "text": " these 256.", "tokens": [613, 38882, 13], "temperature": 0.0, "avg_logprob": -0.18526427599848533, "compression_ratio": 1.6126126126126126, "no_speech_prob": 5.255313226371072e-06}, {"id": 183, "seek": 82072, "start": 837.8000000000001, "end": 842.88, "text": " It's a different concept because we're taking advantage of not just the last layer, but", "tokens": [467, 311, 257, 819, 3410, 570, 321, 434, 1940, 5002, 295, 406, 445, 264, 1036, 4583, 11, 457], "temperature": 0.0, "avg_logprob": -0.18526427599848533, "compression_ratio": 1.6126126126126126, "no_speech_prob": 5.255313226371072e-06}, {"id": 184, "seek": 82072, "start": 842.88, "end": 846.24, "text": " the layers before that as well.", "tokens": [264, 7914, 949, 300, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.18526427599848533, "compression_ratio": 1.6126126126126126, "no_speech_prob": 5.255313226371072e-06}, {"id": 185, "seek": 84624, "start": 846.24, "end": 852.08, "text": " Life's easier if we make them more consistent.", "tokens": [7720, 311, 3571, 498, 321, 652, 552, 544, 8398, 13], "temperature": 0.0, "avg_logprob": -0.14694759409914734, "compression_ratio": 1.5544554455445545, "no_speech_prob": 5.338117716746638e-06}, {"id": 186, "seek": 84624, "start": 852.08, "end": 856.12, "text": " So we're now going to move to NLP.", "tokens": [407, 321, 434, 586, 516, 281, 1286, 281, 426, 45196, 13], "temperature": 0.0, "avg_logprob": -0.14694759409914734, "compression_ratio": 1.5544554455445545, "no_speech_prob": 5.338117716746638e-06}, {"id": 187, "seek": 84624, "start": 856.12, "end": 863.52, "text": " And so let me kind of lay out where we're going here.", "tokens": [400, 370, 718, 385, 733, 295, 2360, 484, 689, 321, 434, 516, 510, 13], "temperature": 0.0, "avg_logprob": -0.14694759409914734, "compression_ratio": 1.5544554455445545, "no_speech_prob": 5.338117716746638e-06}, {"id": 188, "seek": 84624, "start": 863.52, "end": 868.4, "text": " We've seen a couple of times now this idea of taking a pre-trained model, in fact we've", "tokens": [492, 600, 1612, 257, 1916, 295, 1413, 586, 341, 1558, 295, 1940, 257, 659, 12, 17227, 2001, 2316, 11, 294, 1186, 321, 600], "temperature": 0.0, "avg_logprob": -0.14694759409914734, "compression_ratio": 1.5544554455445545, "no_speech_prob": 5.338117716746638e-06}, {"id": 189, "seek": 84624, "start": 868.4, "end": 873.28, "text": " seen it in every lesson, take a pre-trained model, whip off some stuff on the top, replace", "tokens": [1612, 309, 294, 633, 6898, 11, 747, 257, 659, 12, 17227, 2001, 2316, 11, 22377, 766, 512, 1507, 322, 264, 1192, 11, 7406], "temperature": 0.0, "avg_logprob": -0.14694759409914734, "compression_ratio": 1.5544554455445545, "no_speech_prob": 5.338117716746638e-06}, {"id": 190, "seek": 87328, "start": 873.28, "end": 882.0, "text": " it with some new stuff, get it to do something similar.", "tokens": [309, 365, 512, 777, 1507, 11, 483, 309, 281, 360, 746, 2531, 13], "temperature": 0.0, "avg_logprob": -0.19703793199095007, "compression_ratio": 1.4857142857142858, "no_speech_prob": 2.994420356117189e-06}, {"id": 191, "seek": 87328, "start": 882.0, "end": 893.8, "text": " And so we've kind of dived in a little bit deeper to that, to say with conv-learner.pre-trained,", "tokens": [400, 370, 321, 600, 733, 295, 274, 3194, 294, 257, 707, 857, 7731, 281, 300, 11, 281, 584, 365, 3754, 12, 306, 22916, 13, 3712, 12, 17227, 2001, 11], "temperature": 0.0, "avg_logprob": -0.19703793199095007, "compression_ratio": 1.4857142857142858, "no_speech_prob": 2.994420356117189e-06}, {"id": 192, "seek": 87328, "start": 893.8, "end": 899.12, "text": " it had a standard way of sticking stuff on the top which does a particular thing, which", "tokens": [309, 632, 257, 3832, 636, 295, 13465, 1507, 322, 264, 1192, 597, 775, 257, 1729, 551, 11, 597], "temperature": 0.0, "avg_logprob": -0.19703793199095007, "compression_ratio": 1.4857142857142858, "no_speech_prob": 2.994420356117189e-06}, {"id": 193, "seek": 87328, "start": 899.12, "end": 901.64, "text": " was classification.", "tokens": [390, 21538, 13], "temperature": 0.0, "avg_logprob": -0.19703793199095007, "compression_ratio": 1.4857142857142858, "no_speech_prob": 2.994420356117189e-06}, {"id": 194, "seek": 90164, "start": 901.64, "end": 907.68, "text": " And then we learned actually we can stick any PyTorch module we like on the end and", "tokens": [400, 550, 321, 3264, 767, 321, 393, 2897, 604, 9953, 51, 284, 339, 10088, 321, 411, 322, 264, 917, 293], "temperature": 0.0, "avg_logprob": -0.14106973012288412, "compression_ratio": 1.4210526315789473, "no_speech_prob": 2.090454245262663e-06}, {"id": 195, "seek": 90164, "start": 907.68, "end": 912.4, "text": " have it do anything we like with a custom head.", "tokens": [362, 309, 360, 1340, 321, 411, 365, 257, 2375, 1378, 13], "temperature": 0.0, "avg_logprob": -0.14106973012288412, "compression_ratio": 1.4210526315789473, "no_speech_prob": 2.090454245262663e-06}, {"id": 196, "seek": 90164, "start": 912.4, "end": 919.68, "text": " And so suddenly you discover, wow, there's some really interesting things we can do.", "tokens": [400, 370, 5800, 291, 4411, 11, 6076, 11, 456, 311, 512, 534, 1880, 721, 321, 393, 360, 13], "temperature": 0.0, "avg_logprob": -0.14106973012288412, "compression_ratio": 1.4210526315789473, "no_speech_prob": 2.090454245262663e-06}, {"id": 197, "seek": 91968, "start": 919.68, "end": 938.7399999999999, "text": " In fact, that reminds me, it reminds me, Yang Liu said, well, what if we did a different", "tokens": [682, 1186, 11, 300, 12025, 385, 11, 309, 12025, 385, 11, 11978, 18056, 848, 11, 731, 11, 437, 498, 321, 630, 257, 819], "temperature": 0.0, "avg_logprob": -0.2061094531306514, "compression_ratio": 1.4924242424242424, "no_speech_prob": 9.721503602122539e-07}, {"id": 198, "seek": 91968, "start": 938.7399999999999, "end": 940.76, "text": " kind of custom head?", "tokens": [733, 295, 2375, 1378, 30], "temperature": 0.0, "avg_logprob": -0.2061094531306514, "compression_ratio": 1.4924242424242424, "no_speech_prob": 9.721503602122539e-07}, {"id": 199, "seek": 91968, "start": 940.76, "end": 946.12, "text": " And so the different custom head was, well, let's take the original pictures and rotate", "tokens": [400, 370, 264, 819, 2375, 1378, 390, 11, 731, 11, 718, 311, 747, 264, 3380, 5242, 293, 13121], "temperature": 0.0, "avg_logprob": -0.2061094531306514, "compression_ratio": 1.4924242424242424, "no_speech_prob": 9.721503602122539e-07}, {"id": 200, "seek": 94612, "start": 946.12, "end": 953.5600000000001, "text": " them and then make our dependent variable the opposite of that rotation basically and", "tokens": [552, 293, 550, 652, 527, 12334, 7006, 264, 6182, 295, 300, 12447, 1936, 293], "temperature": 0.0, "avg_logprob": -0.15629019486276727, "compression_ratio": 1.5975103734439835, "no_speech_prob": 1.130064447352197e-05}, {"id": 201, "seek": 94612, "start": 953.5600000000001, "end": 957.6, "text": " see if it can learn to un-rotate it.", "tokens": [536, 498, 309, 393, 1466, 281, 517, 12, 10536, 473, 309, 13], "temperature": 0.0, "avg_logprob": -0.15629019486276727, "compression_ratio": 1.5975103734439835, "no_speech_prob": 1.130064447352197e-05}, {"id": 202, "seek": 94612, "start": 957.6, "end": 960.2, "text": " And this is like a super useful thing, obviously.", "tokens": [400, 341, 307, 411, 257, 1687, 4420, 551, 11, 2745, 13], "temperature": 0.0, "avg_logprob": -0.15629019486276727, "compression_ratio": 1.5975103734439835, "no_speech_prob": 1.130064447352197e-05}, {"id": 203, "seek": 94612, "start": 960.2, "end": 966.2, "text": " In fact, I think Google Photos nowadays has this option that it will actually automatically", "tokens": [682, 1186, 11, 286, 519, 3329, 13919, 329, 13434, 575, 341, 3614, 300, 309, 486, 767, 6772], "temperature": 0.0, "avg_logprob": -0.15629019486276727, "compression_ratio": 1.5975103734439835, "no_speech_prob": 1.130064447352197e-05}, {"id": 204, "seek": 94612, "start": 966.2, "end": 969.2, "text": " rotate your photos for you.", "tokens": [13121, 428, 5787, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.15629019486276727, "compression_ratio": 1.5975103734439835, "no_speech_prob": 1.130064447352197e-05}, {"id": 205, "seek": 94612, "start": 969.2, "end": 975.04, "text": " But the cool thing is, as Yang Liu shows here, you can build that network right now by doing", "tokens": [583, 264, 1627, 551, 307, 11, 382, 11978, 18056, 3110, 510, 11, 291, 393, 1322, 300, 3209, 558, 586, 538, 884], "temperature": 0.0, "avg_logprob": -0.15629019486276727, "compression_ratio": 1.5975103734439835, "no_speech_prob": 1.130064447352197e-05}, {"id": 206, "seek": 97504, "start": 975.04, "end": 981.36, "text": " exactly the same as our previous lesson, but your custom head is one that spits out a single", "tokens": [2293, 264, 912, 382, 527, 3894, 6898, 11, 457, 428, 2375, 1378, 307, 472, 300, 637, 1208, 484, 257, 2167], "temperature": 0.0, "avg_logprob": -0.12315042813618977, "compression_ratio": 1.6567164179104477, "no_speech_prob": 1.679721776781662e-06}, {"id": 207, "seek": 97504, "start": 981.36, "end": 987.16, "text": " number, which is how much to rotate by, and your dataset has a dependent variable, which", "tokens": [1230, 11, 597, 307, 577, 709, 281, 13121, 538, 11, 293, 428, 28872, 575, 257, 12334, 7006, 11, 597], "temperature": 0.0, "avg_logprob": -0.12315042813618977, "compression_ratio": 1.6567164179104477, "no_speech_prob": 1.679721776781662e-06}, {"id": 208, "seek": 97504, "start": 987.16, "end": 989.98, "text": " is how much did you rotate by.", "tokens": [307, 577, 709, 630, 291, 13121, 538, 13], "temperature": 0.0, "avg_logprob": -0.12315042813618977, "compression_ratio": 1.6567164179104477, "no_speech_prob": 1.679721776781662e-06}, {"id": 209, "seek": 97504, "start": 989.98, "end": 996.76, "text": " So you suddenly realize with this idea of a backbone plus a custom head, you can do", "tokens": [407, 291, 5800, 4325, 365, 341, 1558, 295, 257, 34889, 1804, 257, 2375, 1378, 11, 291, 393, 360], "temperature": 0.0, "avg_logprob": -0.12315042813618977, "compression_ratio": 1.6567164179104477, "no_speech_prob": 1.679721776781662e-06}, {"id": 210, "seek": 97504, "start": 996.76, "end": 1001.9599999999999, "text": " almost anything you can think about.", "tokens": [1920, 1340, 291, 393, 519, 466, 13], "temperature": 0.0, "avg_logprob": -0.12315042813618977, "compression_ratio": 1.6567164179104477, "no_speech_prob": 1.679721776781662e-06}, {"id": 211, "seek": 100196, "start": 1001.96, "end": 1009.6800000000001, "text": " So today we're going to look at the same idea and say, how does that apply to NLP?", "tokens": [407, 965, 321, 434, 516, 281, 574, 412, 264, 912, 1558, 293, 584, 11, 577, 775, 300, 3079, 281, 426, 45196, 30], "temperature": 0.0, "avg_logprob": -0.13139251073201497, "compression_ratio": 1.67, "no_speech_prob": 1.5056922393341665e-06}, {"id": 212, "seek": 100196, "start": 1009.6800000000001, "end": 1016.64, "text": " And then in the next lesson, we're going to go further and say, well, if NLP and computer", "tokens": [400, 550, 294, 264, 958, 6898, 11, 321, 434, 516, 281, 352, 3052, 293, 584, 11, 731, 11, 498, 426, 45196, 293, 3820], "temperature": 0.0, "avg_logprob": -0.13139251073201497, "compression_ratio": 1.67, "no_speech_prob": 1.5056922393341665e-06}, {"id": 213, "seek": 100196, "start": 1016.64, "end": 1021.96, "text": " vision kind of let you do the same basic ideas, how do we combine the two?", "tokens": [5201, 733, 295, 718, 291, 360, 264, 912, 3875, 3487, 11, 577, 360, 321, 10432, 264, 732, 30], "temperature": 0.0, "avg_logprob": -0.13139251073201497, "compression_ratio": 1.67, "no_speech_prob": 1.5056922393341665e-06}, {"id": 214, "seek": 100196, "start": 1021.96, "end": 1029.08, "text": " And we're going to learn about a model that can actually learn to find word structures", "tokens": [400, 321, 434, 516, 281, 1466, 466, 257, 2316, 300, 393, 767, 1466, 281, 915, 1349, 9227], "temperature": 0.0, "avg_logprob": -0.13139251073201497, "compression_ratio": 1.67, "no_speech_prob": 1.5056922393341665e-06}, {"id": 215, "seek": 102908, "start": 1029.08, "end": 1036.1599999999999, "text": " from images or images from word structures or images from images.", "tokens": [490, 5267, 420, 5267, 490, 1349, 9227, 420, 5267, 490, 5267, 13], "temperature": 0.0, "avg_logprob": -0.1425179682279888, "compression_ratio": 1.722543352601156, "no_speech_prob": 5.507520654646214e-06}, {"id": 216, "seek": 102908, "start": 1036.1599999999999, "end": 1040.04, "text": " And that will form the basis, if you wanted to go further, of doing things like going", "tokens": [400, 300, 486, 1254, 264, 5143, 11, 498, 291, 1415, 281, 352, 3052, 11, 295, 884, 721, 411, 516], "temperature": 0.0, "avg_logprob": -0.1425179682279888, "compression_ratio": 1.722543352601156, "no_speech_prob": 5.507520654646214e-06}, {"id": 217, "seek": 102908, "start": 1040.04, "end": 1046.6, "text": " from an image to a sentence, that's called image captioning, or going from a sentence", "tokens": [490, 364, 3256, 281, 257, 8174, 11, 300, 311, 1219, 3256, 31974, 278, 11, 420, 516, 490, 257, 8174], "temperature": 0.0, "avg_logprob": -0.1425179682279888, "compression_ratio": 1.722543352601156, "no_speech_prob": 5.507520654646214e-06}, {"id": 218, "seek": 102908, "start": 1046.6, "end": 1051.1599999999999, "text": " to an image, which will kind of start to do a phrased image.", "tokens": [281, 364, 3256, 11, 597, 486, 733, 295, 722, 281, 360, 257, 7636, 1937, 3256, 13], "temperature": 0.0, "avg_logprob": -0.1425179682279888, "compression_ratio": 1.722543352601156, "no_speech_prob": 5.507520654646214e-06}, {"id": 219, "seek": 105116, "start": 1051.16, "end": 1059.8400000000001, "text": " And so from there, we're going to go deeper then into computer vision to think about what", "tokens": [400, 370, 490, 456, 11, 321, 434, 516, 281, 352, 7731, 550, 666, 3820, 5201, 281, 519, 466, 437], "temperature": 0.0, "avg_logprob": -0.126873779296875, "compression_ratio": 1.6071428571428572, "no_speech_prob": 1.1910980219909106e-06}, {"id": 220, "seek": 105116, "start": 1059.8400000000001, "end": 1064.4, "text": " other kinds of things we can do with this idea of a pre-trained network plus a custom", "tokens": [661, 3685, 295, 721, 321, 393, 360, 365, 341, 1558, 295, 257, 659, 12, 17227, 2001, 3209, 1804, 257, 2375], "temperature": 0.0, "avg_logprob": -0.126873779296875, "compression_ratio": 1.6071428571428572, "no_speech_prob": 1.1910980219909106e-06}, {"id": 221, "seek": 105116, "start": 1064.4, "end": 1065.4, "text": " head.", "tokens": [1378, 13], "temperature": 0.0, "avg_logprob": -0.126873779296875, "compression_ratio": 1.6071428571428572, "no_speech_prob": 1.1910980219909106e-06}, {"id": 222, "seek": 105116, "start": 1065.4, "end": 1070.0, "text": " And so we'll look at various kinds of image enhancement, like increasing the resolution", "tokens": [400, 370, 321, 603, 574, 412, 3683, 3685, 295, 3256, 40776, 11, 411, 5662, 264, 8669], "temperature": 0.0, "avg_logprob": -0.126873779296875, "compression_ratio": 1.6071428571428572, "no_speech_prob": 1.1910980219909106e-06}, {"id": 223, "seek": 105116, "start": 1070.0, "end": 1078.24, "text": " of a low-res photo to guess what was missing, or adding artistic filters on top of photos,", "tokens": [295, 257, 2295, 12, 495, 5052, 281, 2041, 437, 390, 5361, 11, 420, 5127, 17090, 15995, 322, 1192, 295, 5787, 11], "temperature": 0.0, "avg_logprob": -0.126873779296875, "compression_ratio": 1.6071428571428572, "no_speech_prob": 1.1910980219909106e-06}, {"id": 224, "seek": 107824, "start": 1078.24, "end": 1084.32, "text": " or changing photos of horses into photos of zebras and stuff like that.", "tokens": [420, 4473, 5787, 295, 13112, 666, 5787, 295, 5277, 38182, 293, 1507, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.12404147998706715, "compression_ratio": 1.793103448275862, "no_speech_prob": 6.083570269765914e-07}, {"id": 225, "seek": 107824, "start": 1084.32, "end": 1091.28, "text": " And then finally, that's going to bring us all the way back to bounding boxes again.", "tokens": [400, 550, 2721, 11, 300, 311, 516, 281, 1565, 505, 439, 264, 636, 646, 281, 5472, 278, 9002, 797, 13], "temperature": 0.0, "avg_logprob": -0.12404147998706715, "compression_ratio": 1.793103448275862, "no_speech_prob": 6.083570269765914e-07}, {"id": 226, "seek": 107824, "start": 1091.28, "end": 1094.52, "text": " And so to get there, we're going to first of all learn about segmentation, which is", "tokens": [400, 370, 281, 483, 456, 11, 321, 434, 516, 281, 700, 295, 439, 1466, 466, 9469, 399, 11, 597, 307], "temperature": 0.0, "avg_logprob": -0.12404147998706715, "compression_ratio": 1.793103448275862, "no_speech_prob": 6.083570269765914e-07}, {"id": 227, "seek": 107824, "start": 1094.52, "end": 1099.72, "text": " not just figuring out where a bounding box is, but figuring out what every single pixel", "tokens": [406, 445, 15213, 484, 689, 257, 5472, 278, 2424, 307, 11, 457, 15213, 484, 437, 633, 2167, 19261], "temperature": 0.0, "avg_logprob": -0.12404147998706715, "compression_ratio": 1.793103448275862, "no_speech_prob": 6.083570269765914e-07}, {"id": 228, "seek": 107824, "start": 1099.72, "end": 1101.2, "text": " in an image is part of.", "tokens": [294, 364, 3256, 307, 644, 295, 13], "temperature": 0.0, "avg_logprob": -0.12404147998706715, "compression_ratio": 1.793103448275862, "no_speech_prob": 6.083570269765914e-07}, {"id": 229, "seek": 107824, "start": 1101.2, "end": 1105.68, "text": " So this pixel is part of a person, this pixel is part of a car.", "tokens": [407, 341, 19261, 307, 644, 295, 257, 954, 11, 341, 19261, 307, 644, 295, 257, 1032, 13], "temperature": 0.0, "avg_logprob": -0.12404147998706715, "compression_ratio": 1.793103448275862, "no_speech_prob": 6.083570269765914e-07}, {"id": 230, "seek": 110568, "start": 1105.68, "end": 1109.04, "text": " And then we're going to use that idea, particularly an idea called UNET.", "tokens": [400, 550, 321, 434, 516, 281, 764, 300, 1558, 11, 4098, 364, 1558, 1219, 8229, 4850, 13], "temperature": 0.0, "avg_logprob": -0.1758115495954241, "compression_ratio": 1.670731707317073, "no_speech_prob": 6.540376944030868e-06}, {"id": 231, "seek": 110568, "start": 1109.04, "end": 1114.44, "text": " And it turns out that this idea from UNET, we can apply to bounding boxes where it's", "tokens": [400, 309, 4523, 484, 300, 341, 1558, 490, 8229, 4850, 11, 321, 393, 3079, 281, 5472, 278, 9002, 689, 309, 311], "temperature": 0.0, "avg_logprob": -0.1758115495954241, "compression_ratio": 1.670731707317073, "no_speech_prob": 6.540376944030868e-06}, {"id": 232, "seek": 110568, "start": 1114.44, "end": 1116.3200000000002, "text": " called feature pyramids.", "tokens": [1219, 4111, 20543, 3742, 13], "temperature": 0.0, "avg_logprob": -0.1758115495954241, "compression_ratio": 1.670731707317073, "no_speech_prob": 6.540376944030868e-06}, {"id": 233, "seek": 110568, "start": 1116.3200000000002, "end": 1121.2, "text": " Everything has to have a different name in every slightly different area.", "tokens": [5471, 575, 281, 362, 257, 819, 1315, 294, 633, 4748, 819, 1859, 13], "temperature": 0.0, "avg_logprob": -0.1758115495954241, "compression_ratio": 1.670731707317073, "no_speech_prob": 6.540376944030868e-06}, {"id": 234, "seek": 110568, "start": 1121.2, "end": 1127.02, "text": " And we'll use that to hopefully get some even better results, some really good results actually", "tokens": [400, 321, 603, 764, 300, 281, 4696, 483, 512, 754, 1101, 3542, 11, 512, 534, 665, 3542, 767], "temperature": 0.0, "avg_logprob": -0.1758115495954241, "compression_ratio": 1.670731707317073, "no_speech_prob": 6.540376944030868e-06}, {"id": 235, "seek": 110568, "start": 1127.02, "end": 1128.28, "text": " with bounding boxes.", "tokens": [365, 5472, 278, 9002, 13], "temperature": 0.0, "avg_logprob": -0.1758115495954241, "compression_ratio": 1.670731707317073, "no_speech_prob": 6.540376944030868e-06}, {"id": 236, "seek": 110568, "start": 1128.28, "end": 1131.8400000000001, "text": " So that's kind of our path from here.", "tokens": [407, 300, 311, 733, 295, 527, 3100, 490, 510, 13], "temperature": 0.0, "avg_logprob": -0.1758115495954241, "compression_ratio": 1.670731707317073, "no_speech_prob": 6.540376944030868e-06}, {"id": 237, "seek": 113184, "start": 1131.84, "end": 1137.52, "text": " So it's all going to build on each other, but take us into lots of different areas.", "tokens": [407, 309, 311, 439, 516, 281, 1322, 322, 1184, 661, 11, 457, 747, 505, 666, 3195, 295, 819, 3179, 13], "temperature": 0.0, "avg_logprob": -0.1655230679354825, "compression_ratio": 1.5158371040723981, "no_speech_prob": 4.495151188166346e-06}, {"id": 238, "seek": 113184, "start": 1137.52, "end": 1145.8799999999999, "text": " Now for NLP last part, we relied on a pretty great library called Torch Text.", "tokens": [823, 337, 426, 45196, 1036, 644, 11, 321, 35463, 322, 257, 1238, 869, 6405, 1219, 7160, 339, 18643, 13], "temperature": 0.0, "avg_logprob": -0.1655230679354825, "compression_ratio": 1.5158371040723981, "no_speech_prob": 4.495151188166346e-06}, {"id": 239, "seek": 113184, "start": 1145.8799999999999, "end": 1152.76, "text": " But as pretty great as it was, I've since then found the limitations of it too problematic", "tokens": [583, 382, 1238, 869, 382, 309, 390, 11, 286, 600, 1670, 550, 1352, 264, 15705, 295, 309, 886, 19011], "temperature": 0.0, "avg_logprob": -0.1655230679354825, "compression_ratio": 1.5158371040723981, "no_speech_prob": 4.495151188166346e-06}, {"id": 240, "seek": 113184, "start": 1152.76, "end": 1154.56, "text": " to keep using it.", "tokens": [281, 1066, 1228, 309, 13], "temperature": 0.0, "avg_logprob": -0.1655230679354825, "compression_ratio": 1.5158371040723981, "no_speech_prob": 4.495151188166346e-06}, {"id": 241, "seek": 113184, "start": 1154.56, "end": 1160.3999999999999, "text": " As a lot of you complained on the forums, it's pretty damn slow.", "tokens": [1018, 257, 688, 295, 291, 33951, 322, 264, 26998, 11, 309, 311, 1238, 8151, 2964, 13], "temperature": 0.0, "avg_logprob": -0.1655230679354825, "compression_ratio": 1.5158371040723981, "no_speech_prob": 4.495151188166346e-06}, {"id": 242, "seek": 116040, "start": 1160.4, "end": 1164.96, "text": " Partly that's because it's not doing parallel processing.", "tokens": [4100, 356, 300, 311, 570, 309, 311, 406, 884, 8952, 9007, 13], "temperature": 0.0, "avg_logprob": -0.13825401306152343, "compression_ratio": 1.5815899581589958, "no_speech_prob": 3.13813006869168e-06}, {"id": 243, "seek": 116040, "start": 1164.96, "end": 1170.88, "text": " And partly it's because it doesn't remember what you did last time and it does it all", "tokens": [400, 17031, 309, 311, 570, 309, 1177, 380, 1604, 437, 291, 630, 1036, 565, 293, 309, 775, 309, 439], "temperature": 0.0, "avg_logprob": -0.13825401306152343, "compression_ratio": 1.5815899581589958, "no_speech_prob": 3.13813006869168e-06}, {"id": 244, "seek": 116040, "start": 1170.88, "end": 1174.96, "text": " over again from scratch.", "tokens": [670, 797, 490, 8459, 13], "temperature": 0.0, "avg_logprob": -0.13825401306152343, "compression_ratio": 1.5815899581589958, "no_speech_prob": 3.13813006869168e-06}, {"id": 245, "seek": 116040, "start": 1174.96, "end": 1177.8000000000002, "text": " And then it's kind of hard to do fairly simple things.", "tokens": [400, 550, 309, 311, 733, 295, 1152, 281, 360, 6457, 2199, 721, 13], "temperature": 0.0, "avg_logprob": -0.13825401306152343, "compression_ratio": 1.5815899581589958, "no_speech_prob": 3.13813006869168e-06}, {"id": 246, "seek": 116040, "start": 1177.8000000000002, "end": 1181.5600000000002, "text": " Like a lot of you were trying to get into the toxic comment competition on Kaggle, which", "tokens": [1743, 257, 688, 295, 291, 645, 1382, 281, 483, 666, 264, 12786, 2871, 6211, 322, 48751, 22631, 11, 597], "temperature": 0.0, "avg_logprob": -0.13825401306152343, "compression_ratio": 1.5815899581589958, "no_speech_prob": 3.13813006869168e-06}, {"id": 247, "seek": 116040, "start": 1181.5600000000002, "end": 1186.24, "text": " was a multi-label problem, and trying to do that with Torch Text.", "tokens": [390, 257, 4825, 12, 75, 18657, 1154, 11, 293, 1382, 281, 360, 300, 365, 7160, 339, 18643, 13], "temperature": 0.0, "avg_logprob": -0.13825401306152343, "compression_ratio": 1.5815899581589958, "no_speech_prob": 3.13813006869168e-06}, {"id": 248, "seek": 118624, "start": 1186.24, "end": 1191.72, "text": " I eventually got it working, but it took me like a week of hacking away, which is kind", "tokens": [286, 4728, 658, 309, 1364, 11, 457, 309, 1890, 385, 411, 257, 1243, 295, 31422, 1314, 11, 597, 307, 733], "temperature": 0.0, "avg_logprob": -0.1691086748813061, "compression_ratio": 1.5172413793103448, "no_speech_prob": 1.52066177179222e-05}, {"id": 249, "seek": 118624, "start": 1191.72, "end": 1193.04, "text": " of ridiculous.", "tokens": [295, 11083, 13], "temperature": 0.0, "avg_logprob": -0.1691086748813061, "compression_ratio": 1.5172413793103448, "no_speech_prob": 1.52066177179222e-05}, {"id": 250, "seek": 118624, "start": 1193.04, "end": 1198.84, "text": " So to fix all these problems, we've created a new library called Fastai.text.", "tokens": [407, 281, 3191, 439, 613, 2740, 11, 321, 600, 2942, 257, 777, 6405, 1219, 15968, 1301, 13, 25111, 13], "temperature": 0.0, "avg_logprob": -0.1691086748813061, "compression_ratio": 1.5172413793103448, "no_speech_prob": 1.52066177179222e-05}, {"id": 251, "seek": 118624, "start": 1198.84, "end": 1206.96, "text": " Fastai.text is a replacement for the combination of Torch Text and Fastai.nlp.", "tokens": [15968, 1301, 13, 25111, 307, 257, 14419, 337, 264, 6562, 295, 7160, 339, 18643, 293, 15968, 1301, 13, 77, 75, 79, 13], "temperature": 0.0, "avg_logprob": -0.1691086748813061, "compression_ratio": 1.5172413793103448, "no_speech_prob": 1.52066177179222e-05}, {"id": 252, "seek": 118624, "start": 1206.96, "end": 1210.96, "text": " So don't use Fastai.nlp anymore.", "tokens": [407, 500, 380, 764, 15968, 1301, 13, 77, 75, 79, 3602, 13], "temperature": 0.0, "avg_logprob": -0.1691086748813061, "compression_ratio": 1.5172413793103448, "no_speech_prob": 1.52066177179222e-05}, {"id": 253, "seek": 118624, "start": 1210.96, "end": 1213.2, "text": " That's obsolete.", "tokens": [663, 311, 46333, 13], "temperature": 0.0, "avg_logprob": -0.1691086748813061, "compression_ratio": 1.5172413793103448, "no_speech_prob": 1.52066177179222e-05}, {"id": 254, "seek": 121320, "start": 1213.2, "end": 1218.72, "text": " It's slower, it's more confusing, it's less good in every way.", "tokens": [467, 311, 14009, 11, 309, 311, 544, 13181, 11, 309, 311, 1570, 665, 294, 633, 636, 13], "temperature": 0.0, "avg_logprob": -0.1619824000767299, "compression_ratio": 1.5871559633027523, "no_speech_prob": 2.931113340309821e-05}, {"id": 255, "seek": 121320, "start": 1218.72, "end": 1220.72, "text": " But there's a lot of overlaps.", "tokens": [583, 456, 311, 257, 688, 295, 15986, 2382, 13], "temperature": 0.0, "avg_logprob": -0.1619824000767299, "compression_ratio": 1.5871559633027523, "no_speech_prob": 2.931113340309821e-05}, {"id": 256, "seek": 121320, "start": 1220.72, "end": 1224.52, "text": " Intentionally, a lot of the classes have the same names, a lot of the functions have the", "tokens": [5681, 1251, 379, 11, 257, 688, 295, 264, 5359, 362, 264, 912, 5288, 11, 257, 688, 295, 264, 6828, 362, 264], "temperature": 0.0, "avg_logprob": -0.1619824000767299, "compression_ratio": 1.5871559633027523, "no_speech_prob": 2.931113340309821e-05}, {"id": 257, "seek": 121320, "start": 1224.52, "end": 1233.52, "text": " same names, but this is the non-Torch Text version.", "tokens": [912, 5288, 11, 457, 341, 307, 264, 2107, 12, 51, 284, 339, 18643, 3037, 13], "temperature": 0.0, "avg_logprob": -0.1619824000767299, "compression_ratio": 1.5871559633027523, "no_speech_prob": 2.931113340309821e-05}, {"id": 258, "seek": 121320, "start": 1233.52, "end": 1236.16, "text": " So we're going to work with IMDB again.", "tokens": [407, 321, 434, 516, 281, 589, 365, 21463, 27735, 797, 13], "temperature": 0.0, "avg_logprob": -0.1619824000767299, "compression_ratio": 1.5871559633027523, "no_speech_prob": 2.931113340309821e-05}, {"id": 259, "seek": 121320, "start": 1236.16, "end": 1242.68, "text": " So for those of you who have forgotten, go back and check out Lesson 4.", "tokens": [407, 337, 729, 295, 291, 567, 362, 11832, 11, 352, 646, 293, 1520, 484, 18649, 266, 1017, 13], "temperature": 0.0, "avg_logprob": -0.1619824000767299, "compression_ratio": 1.5871559633027523, "no_speech_prob": 2.931113340309821e-05}, {"id": 260, "seek": 124268, "start": 1242.68, "end": 1246.4, "text": " Basically this is a dataset of Moji reviews.", "tokens": [8537, 341, 307, 257, 28872, 295, 3335, 4013, 10229, 13], "temperature": 0.0, "avg_logprob": -0.2814178466796875, "compression_ratio": 1.5024875621890548, "no_speech_prob": 8.939503459259868e-06}, {"id": 261, "seek": 124268, "start": 1246.4, "end": 1251.4, "text": " You remember we used it to find out whether we might enjoy Zombie Gettin or not, and we", "tokens": [509, 1604, 321, 1143, 309, 281, 915, 484, 1968, 321, 1062, 2103, 48952, 460, 3093, 259, 420, 406, 11, 293, 321], "temperature": 0.0, "avg_logprob": -0.2814178466796875, "compression_ratio": 1.5024875621890548, "no_speech_prob": 8.939503459259868e-06}, {"id": 262, "seek": 124268, "start": 1251.4, "end": 1256.48, "text": " thought probably my kind of thing.", "tokens": [1194, 1391, 452, 733, 295, 551, 13], "temperature": 0.0, "avg_logprob": -0.2814178466796875, "compression_ratio": 1.5024875621890548, "no_speech_prob": 8.939503459259868e-06}, {"id": 263, "seek": 124268, "start": 1256.48, "end": 1262.96, "text": " So we're going to use the same dataset, and by default it calls itself aclimdb.", "tokens": [407, 321, 434, 516, 281, 764, 264, 912, 28872, 11, 293, 538, 7576, 309, 5498, 2564, 696, 4197, 67, 65, 13], "temperature": 0.0, "avg_logprob": -0.2814178466796875, "compression_ratio": 1.5024875621890548, "no_speech_prob": 8.939503459259868e-06}, {"id": 264, "seek": 124268, "start": 1262.96, "end": 1270.0800000000002, "text": " So this is just the raw dataset that you can download.", "tokens": [407, 341, 307, 445, 264, 8936, 28872, 300, 291, 393, 5484, 13], "temperature": 0.0, "avg_logprob": -0.2814178466796875, "compression_ratio": 1.5024875621890548, "no_speech_prob": 8.939503459259868e-06}, {"id": 265, "seek": 127008, "start": 1270.08, "end": 1274.3999999999999, "text": " And as you can see, I'm doing from Fastai.text import star.", "tokens": [400, 382, 291, 393, 536, 11, 286, 478, 884, 490, 15968, 1301, 13, 25111, 974, 3543, 13], "temperature": 0.0, "avg_logprob": -0.1213508884558517, "compression_ratio": 1.46, "no_speech_prob": 5.682369646820007e-06}, {"id": 266, "seek": 127008, "start": 1274.3999999999999, "end": 1281.1999999999998, "text": " There's no Torch Text and I'm not using Fastai.nlp.", "tokens": [821, 311, 572, 7160, 339, 18643, 293, 286, 478, 406, 1228, 15968, 1301, 13, 77, 75, 79, 13], "temperature": 0.0, "avg_logprob": -0.1213508884558517, "compression_ratio": 1.46, "no_speech_prob": 5.682369646820007e-06}, {"id": 267, "seek": 127008, "start": 1281.1999999999998, "end": 1283.56, "text": " I'm going to use pathlib as per usual.", "tokens": [286, 478, 516, 281, 764, 3100, 38270, 382, 680, 7713, 13], "temperature": 0.0, "avg_logprob": -0.1213508884558517, "compression_ratio": 1.46, "no_speech_prob": 5.682369646820007e-06}, {"id": 268, "seek": 127008, "start": 1283.56, "end": 1287.74, "text": " We're going to learn about what these tags are later.", "tokens": [492, 434, 516, 281, 1466, 466, 437, 613, 18632, 366, 1780, 13], "temperature": 0.0, "avg_logprob": -0.1213508884558517, "compression_ratio": 1.46, "no_speech_prob": 5.682369646820007e-06}, {"id": 269, "seek": 127008, "start": 1287.74, "end": 1297.3999999999999, "text": " So you might remember the basic path for NLP is that we have to take sentences and turn", "tokens": [407, 291, 1062, 1604, 264, 3875, 3100, 337, 426, 45196, 307, 300, 321, 362, 281, 747, 16579, 293, 1261], "temperature": 0.0, "avg_logprob": -0.1213508884558517, "compression_ratio": 1.46, "no_speech_prob": 5.682369646820007e-06}, {"id": 270, "seek": 129740, "start": 1297.4, "end": 1300.64, "text": " them into numbers.", "tokens": [552, 666, 3547, 13], "temperature": 0.0, "avg_logprob": -0.11304462657255285, "compression_ratio": 1.5092592592592593, "no_speech_prob": 5.771881660621148e-06}, {"id": 271, "seek": 129740, "start": 1300.64, "end": 1304.3200000000002, "text": " And there's a couple of steps to get there.", "tokens": [400, 456, 311, 257, 1916, 295, 4439, 281, 483, 456, 13], "temperature": 0.0, "avg_logprob": -0.11304462657255285, "compression_ratio": 1.5092592592592593, "no_speech_prob": 5.771881660621148e-06}, {"id": 272, "seek": 129740, "start": 1304.3200000000002, "end": 1312.3600000000001, "text": " So at the moment, somewhat intentionally, Fastai.text doesn't provide that many helper", "tokens": [407, 412, 264, 1623, 11, 8344, 22062, 11, 15968, 1301, 13, 25111, 1177, 380, 2893, 300, 867, 36133], "temperature": 0.0, "avg_logprob": -0.11304462657255285, "compression_ratio": 1.5092592592592593, "no_speech_prob": 5.771881660621148e-06}, {"id": 273, "seek": 129740, "start": 1312.3600000000001, "end": 1314.2800000000002, "text": " functions.", "tokens": [6828, 13], "temperature": 0.0, "avg_logprob": -0.11304462657255285, "compression_ratio": 1.5092592592592593, "no_speech_prob": 5.771881660621148e-06}, {"id": 274, "seek": 129740, "start": 1314.2800000000002, "end": 1319.88, "text": " It's really designed more to let you handle things in a fairly flexible way.", "tokens": [467, 311, 534, 4761, 544, 281, 718, 291, 4813, 721, 294, 257, 6457, 11358, 636, 13], "temperature": 0.0, "avg_logprob": -0.11304462657255285, "compression_ratio": 1.5092592592592593, "no_speech_prob": 5.771881660621148e-06}, {"id": 275, "seek": 129740, "start": 1319.88, "end": 1326.52, "text": " So as you can see here, I wrote something called getTexts, which goes through each thing", "tokens": [407, 382, 291, 393, 536, 510, 11, 286, 4114, 746, 1219, 483, 50198, 82, 11, 597, 1709, 807, 1184, 551], "temperature": 0.0, "avg_logprob": -0.11304462657255285, "compression_ratio": 1.5092592592592593, "no_speech_prob": 5.771881660621148e-06}, {"id": 276, "seek": 132652, "start": 1326.52, "end": 1328.4, "text": " in classes.", "tokens": [294, 5359, 13], "temperature": 0.0, "avg_logprob": -0.2034967536600227, "compression_ratio": 1.726530612244898, "no_speech_prob": 1.568944935570471e-05}, {"id": 277, "seek": 132652, "start": 1328.4, "end": 1331.52, "text": " And these are the three classes that they have in IMDB.", "tokens": [400, 613, 366, 264, 1045, 5359, 300, 436, 362, 294, 21463, 27735, 13], "temperature": 0.0, "avg_logprob": -0.2034967536600227, "compression_ratio": 1.726530612244898, "no_speech_prob": 1.568944935570471e-05}, {"id": 278, "seek": 132652, "start": 1331.52, "end": 1335.84, "text": " Negative, positive, and then there's another folder unsupervised.", "tokens": [43230, 11, 3353, 11, 293, 550, 456, 311, 1071, 10820, 2693, 12879, 24420, 13], "temperature": 0.0, "avg_logprob": -0.2034967536600227, "compression_ratio": 1.726530612244898, "no_speech_prob": 1.568944935570471e-05}, {"id": 279, "seek": 132652, "start": 1335.84, "end": 1337.8799999999999, "text": " That's stuff they haven't gotten around to labeling yet.", "tokens": [663, 311, 1507, 436, 2378, 380, 5768, 926, 281, 40244, 1939, 13], "temperature": 0.0, "avg_logprob": -0.2034967536600227, "compression_ratio": 1.726530612244898, "no_speech_prob": 1.568944935570471e-05}, {"id": 280, "seek": 132652, "start": 1337.8799999999999, "end": 1340.44, "text": " So I'm just going to call that a class for now.", "tokens": [407, 286, 478, 445, 516, 281, 818, 300, 257, 1508, 337, 586, 13], "temperature": 0.0, "avg_logprob": -0.2034967536600227, "compression_ratio": 1.726530612244898, "no_speech_prob": 1.568944935570471e-05}, {"id": 281, "seek": 132652, "start": 1340.44, "end": 1347.8799999999999, "text": " And so I just go through each one of those classes, and then I just find every file in", "tokens": [400, 370, 286, 445, 352, 807, 1184, 472, 295, 729, 5359, 11, 293, 550, 286, 445, 915, 633, 3991, 294], "temperature": 0.0, "avg_logprob": -0.2034967536600227, "compression_ratio": 1.726530612244898, "no_speech_prob": 1.568944935570471e-05}, {"id": 282, "seek": 132652, "start": 1347.8799999999999, "end": 1353.12, "text": " that folder with that name, and I open it up and read it and chuck it into the end of", "tokens": [300, 10820, 365, 300, 1315, 11, 293, 286, 1269, 309, 493, 293, 1401, 309, 293, 20870, 309, 666, 264, 917, 295], "temperature": 0.0, "avg_logprob": -0.2034967536600227, "compression_ratio": 1.726530612244898, "no_speech_prob": 1.568944935570471e-05}, {"id": 283, "seek": 132652, "start": 1353.12, "end": 1355.84, "text": " this array.", "tokens": [341, 10225, 13], "temperature": 0.0, "avg_logprob": -0.2034967536600227, "compression_ratio": 1.726530612244898, "no_speech_prob": 1.568944935570471e-05}, {"id": 284, "seek": 135584, "start": 1355.84, "end": 1361.76, "text": " And as you can see, with pathlib, it's super easy to grab stuff and pull it in, and then", "tokens": [400, 382, 291, 393, 536, 11, 365, 3100, 38270, 11, 309, 311, 1687, 1858, 281, 4444, 1507, 293, 2235, 309, 294, 11, 293, 550], "temperature": 0.0, "avg_logprob": -0.12743534529504696, "compression_ratio": 1.6778242677824269, "no_speech_prob": 5.955088909104234e-06}, {"id": 285, "seek": 135584, "start": 1361.76, "end": 1367.84, "text": " the label is just whatever class I'm up to so far.", "tokens": [264, 7645, 307, 445, 2035, 1508, 286, 478, 493, 281, 370, 1400, 13], "temperature": 0.0, "avg_logprob": -0.12743534529504696, "compression_ratio": 1.6778242677824269, "no_speech_prob": 5.955088909104234e-06}, {"id": 286, "seek": 135584, "start": 1367.84, "end": 1372.0, "text": " So I'll go ahead and do that for the train bit, and I'll go ahead and do that for the", "tokens": [407, 286, 603, 352, 2286, 293, 360, 300, 337, 264, 3847, 857, 11, 293, 286, 603, 352, 2286, 293, 360, 300, 337, 264], "temperature": 0.0, "avg_logprob": -0.12743534529504696, "compression_ratio": 1.6778242677824269, "no_speech_prob": 5.955088909104234e-06}, {"id": 287, "seek": 135584, "start": 1372.0, "end": 1374.24, "text": " test bit.", "tokens": [1500, 857, 13], "temperature": 0.0, "avg_logprob": -0.12743534529504696, "compression_ratio": 1.6778242677824269, "no_speech_prob": 5.955088909104234e-06}, {"id": 288, "seek": 135584, "start": 1374.24, "end": 1380.32, "text": " So there's 70,000 in train, 25,000 in test, 50,000 of the train ones are unsupervised.", "tokens": [407, 456, 311, 5285, 11, 1360, 294, 3847, 11, 3552, 11, 1360, 294, 1500, 11, 2625, 11, 1360, 295, 264, 3847, 2306, 366, 2693, 12879, 24420, 13], "temperature": 0.0, "avg_logprob": -0.12743534529504696, "compression_ratio": 1.6778242677824269, "no_speech_prob": 5.955088909104234e-06}, {"id": 289, "seek": 135584, "start": 1380.32, "end": 1384.32, "text": " We won't actually be able to use them when we get to the classification piece.", "tokens": [492, 1582, 380, 767, 312, 1075, 281, 764, 552, 562, 321, 483, 281, 264, 21538, 2522, 13], "temperature": 0.0, "avg_logprob": -0.12743534529504696, "compression_ratio": 1.6778242677824269, "no_speech_prob": 5.955088909104234e-06}, {"id": 290, "seek": 138432, "start": 1384.32, "end": 1391.58, "text": " So I actually find this much easier than the kind of torch text approach of having lots", "tokens": [407, 286, 767, 915, 341, 709, 3571, 813, 264, 733, 295, 27822, 2487, 3109, 295, 1419, 3195], "temperature": 0.0, "avg_logprob": -0.15179892803760284, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.1233731786196586e-06}, {"id": 291, "seek": 138432, "start": 1391.58, "end": 1396.1599999999999, "text": " of layers and wrappers and stuff, because in the end, reading text files is not that", "tokens": [295, 7914, 293, 7843, 15226, 293, 1507, 11, 570, 294, 264, 917, 11, 3760, 2487, 7098, 307, 406, 300], "temperature": 0.0, "avg_logprob": -0.15179892803760284, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.1233731786196586e-06}, {"id": 292, "seek": 138432, "start": 1396.1599999999999, "end": 1400.52, "text": " hard.", "tokens": [1152, 13], "temperature": 0.0, "avg_logprob": -0.15179892803760284, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.1233731786196586e-06}, {"id": 293, "seek": 138432, "start": 1400.52, "end": 1407.8, "text": " One thing that's always a good idea is to sort things randomly.", "tokens": [1485, 551, 300, 311, 1009, 257, 665, 1558, 307, 281, 1333, 721, 16979, 13], "temperature": 0.0, "avg_logprob": -0.15179892803760284, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.1233731786196586e-06}, {"id": 294, "seek": 138432, "start": 1407.8, "end": 1411.08, "text": " It's useful to know this simple trick for sorting things randomly, particularly when", "tokens": [467, 311, 4420, 281, 458, 341, 2199, 4282, 337, 32411, 721, 16979, 11, 4098, 562], "temperature": 0.0, "avg_logprob": -0.15179892803760284, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.1233731786196586e-06}, {"id": 295, "seek": 138432, "start": 1411.08, "end": 1413.12, "text": " you've got multiple things you have to sort the same way.", "tokens": [291, 600, 658, 3866, 721, 291, 362, 281, 1333, 264, 912, 636, 13], "temperature": 0.0, "avg_logprob": -0.15179892803760284, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.1233731786196586e-06}, {"id": 296, "seek": 141312, "start": 1413.12, "end": 1415.6399999999999, "text": " In this case, we've got labels and texts.", "tokens": [682, 341, 1389, 11, 321, 600, 658, 16949, 293, 15765, 13], "temperature": 0.0, "avg_logprob": -0.1491812684319236, "compression_ratio": 1.6538461538461537, "no_speech_prob": 3.288739208073821e-06}, {"id": 297, "seek": 141312, "start": 1415.6399999999999, "end": 1424.08, "text": " np.random.permutation, if you give it an integer, it gives you a random list from 0 up to and", "tokens": [33808, 13, 3699, 298, 13, 610, 76, 11380, 11, 498, 291, 976, 309, 364, 24922, 11, 309, 2709, 291, 257, 4974, 1329, 490, 1958, 493, 281, 293], "temperature": 0.0, "avg_logprob": -0.1491812684319236, "compression_ratio": 1.6538461538461537, "no_speech_prob": 3.288739208073821e-06}, {"id": 298, "seek": 141312, "start": 1424.08, "end": 1428.7199999999998, "text": " not including the number you give it in some random order.", "tokens": [406, 3009, 264, 1230, 291, 976, 309, 294, 512, 4974, 1668, 13], "temperature": 0.0, "avg_logprob": -0.1491812684319236, "compression_ratio": 1.6538461538461537, "no_speech_prob": 3.288739208073821e-06}, {"id": 299, "seek": 141312, "start": 1428.7199999999998, "end": 1435.9199999999998, "text": " And so you can then just pass that in as an indexer to give you a list that's sorted in", "tokens": [400, 370, 291, 393, 550, 445, 1320, 300, 294, 382, 364, 8186, 260, 281, 976, 291, 257, 1329, 300, 311, 25462, 294], "temperature": 0.0, "avg_logprob": -0.1491812684319236, "compression_ratio": 1.6538461538461537, "no_speech_prob": 3.288739208073821e-06}, {"id": 300, "seek": 141312, "start": 1435.9199999999998, "end": 1437.1399999999999, "text": " that random order.", "tokens": [300, 4974, 1668, 13], "temperature": 0.0, "avg_logprob": -0.1491812684319236, "compression_ratio": 1.6538461538461537, "no_speech_prob": 3.288739208073821e-06}, {"id": 301, "seek": 143714, "start": 1437.14, "end": 1443.2800000000002, "text": " So in this case, it's going to sort train texts and train labels in the same random", "tokens": [407, 294, 341, 1389, 11, 309, 311, 516, 281, 1333, 3847, 15765, 293, 3847, 16949, 294, 264, 912, 4974], "temperature": 0.0, "avg_logprob": -0.1134741357032289, "compression_ratio": 1.608695652173913, "no_speech_prob": 1.6536868088223855e-06}, {"id": 302, "seek": 143714, "start": 1443.2800000000002, "end": 1444.2800000000002, "text": " way.", "tokens": [636, 13], "temperature": 0.0, "avg_logprob": -0.1134741357032289, "compression_ratio": 1.608695652173913, "no_speech_prob": 1.6536868088223855e-06}, {"id": 303, "seek": 143714, "start": 1444.2800000000002, "end": 1448.16, "text": " So that's a useful little idiom to use.", "tokens": [407, 300, 311, 257, 4420, 707, 18014, 298, 281, 764, 13], "temperature": 0.0, "avg_logprob": -0.1134741357032289, "compression_ratio": 1.608695652173913, "no_speech_prob": 1.6536868088223855e-06}, {"id": 304, "seek": 143714, "start": 1448.16, "end": 1451.24, "text": " So now I've got my texts and my labels sorted.", "tokens": [407, 586, 286, 600, 658, 452, 15765, 293, 452, 16949, 25462, 13], "temperature": 0.0, "avg_logprob": -0.1134741357032289, "compression_ratio": 1.608695652173913, "no_speech_prob": 1.6536868088223855e-06}, {"id": 305, "seek": 143714, "start": 1451.24, "end": 1453.96, "text": " I can go ahead and create a data frame from them.", "tokens": [286, 393, 352, 2286, 293, 1884, 257, 1412, 3920, 490, 552, 13], "temperature": 0.0, "avg_logprob": -0.1134741357032289, "compression_ratio": 1.608695652173913, "no_speech_prob": 1.6536868088223855e-06}, {"id": 306, "seek": 143714, "start": 1453.96, "end": 1455.64, "text": " Why am I doing this?", "tokens": [1545, 669, 286, 884, 341, 30], "temperature": 0.0, "avg_logprob": -0.1134741357032289, "compression_ratio": 1.608695652173913, "no_speech_prob": 1.6536868088223855e-06}, {"id": 307, "seek": 143714, "start": 1455.64, "end": 1463.64, "text": " The reason I'm doing this is because there is a somewhat standard approach starting to", "tokens": [440, 1778, 286, 478, 884, 341, 307, 570, 456, 307, 257, 8344, 3832, 3109, 2891, 281], "temperature": 0.0, "avg_logprob": -0.1134741357032289, "compression_ratio": 1.608695652173913, "no_speech_prob": 1.6536868088223855e-06}, {"id": 308, "seek": 146364, "start": 1463.64, "end": 1477.24, "text": " appear for text classification datasets, which is to have your training set as a CSV file", "tokens": [4204, 337, 2487, 21538, 42856, 11, 597, 307, 281, 362, 428, 3097, 992, 382, 257, 48814, 3991], "temperature": 0.0, "avg_logprob": -0.2638183321271624, "compression_ratio": 1.3660130718954249, "no_speech_prob": 3.187543143212679e-06}, {"id": 309, "seek": 146364, "start": 1477.24, "end": 1487.48, "text": " with the labels first and the text of the NLP document second, train.csv and test.csv.", "tokens": [365, 264, 16949, 700, 293, 264, 2487, 295, 264, 426, 45196, 4166, 1150, 11, 3847, 13, 14368, 85, 293, 1500, 13, 14368, 85, 13], "temperature": 0.0, "avg_logprob": -0.2638183321271624, "compression_ratio": 1.3660130718954249, "no_speech_prob": 3.187543143212679e-06}, {"id": 310, "seek": 146364, "start": 1487.48, "end": 1488.48, "text": " So basically it looks like this.", "tokens": [407, 1936, 309, 1542, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.2638183321271624, "compression_ratio": 1.3660130718954249, "no_speech_prob": 3.187543143212679e-06}, {"id": 311, "seek": 148848, "start": 1488.48, "end": 1494.04, "text": " You've got your labels and your texts, and then a file called classes.txt which just", "tokens": [509, 600, 658, 428, 16949, 293, 428, 15765, 11, 293, 550, 257, 3991, 1219, 5359, 13, 83, 734, 597, 445], "temperature": 0.0, "avg_logprob": -0.19746990418166258, "compression_ratio": 1.5605381165919283, "no_speech_prob": 6.083571406634292e-07}, {"id": 312, "seek": 148848, "start": 1494.04, "end": 1497.0, "text": " lists the classes.", "tokens": [14511, 264, 5359, 13], "temperature": 0.0, "avg_logprob": -0.19746990418166258, "compression_ratio": 1.5605381165919283, "no_speech_prob": 6.083571406634292e-07}, {"id": 313, "seek": 148848, "start": 1497.0, "end": 1504.72, "text": " I think somewhat standard is in a reasonably recent academic paper, Yan LeCun and a team", "tokens": [286, 519, 8344, 3832, 307, 294, 257, 23551, 5162, 7778, 3035, 11, 13633, 1456, 34, 409, 293, 257, 1469], "temperature": 0.0, "avg_logprob": -0.19746990418166258, "compression_ratio": 1.5605381165919283, "no_speech_prob": 6.083571406634292e-07}, {"id": 314, "seek": 148848, "start": 1504.72, "end": 1511.24, "text": " of researchers looked at quite a few datasets and they used this format for all of them.", "tokens": [295, 10309, 2956, 412, 1596, 257, 1326, 42856, 293, 436, 1143, 341, 7877, 337, 439, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.19746990418166258, "compression_ratio": 1.5605381165919283, "no_speech_prob": 6.083571406634292e-07}, {"id": 315, "seek": 148848, "start": 1511.24, "end": 1518.0, "text": " And so that's what I've started using as well for my recent paper.", "tokens": [400, 370, 300, 311, 437, 286, 600, 1409, 1228, 382, 731, 337, 452, 5162, 3035, 13], "temperature": 0.0, "avg_logprob": -0.19746990418166258, "compression_ratio": 1.5605381165919283, "no_speech_prob": 6.083571406634292e-07}, {"id": 316, "seek": 151800, "start": 1518.0, "end": 1526.48, "text": " So what I've done is you'll find that this notebook, if you put your data into this format,", "tokens": [407, 437, 286, 600, 1096, 307, 291, 603, 915, 300, 341, 21060, 11, 498, 291, 829, 428, 1412, 666, 341, 7877, 11], "temperature": 0.0, "avg_logprob": -0.1881609257963515, "compression_ratio": 1.6437768240343347, "no_speech_prob": 1.6027983065214357e-06}, {"id": 317, "seek": 151800, "start": 1526.48, "end": 1529.28, "text": " the whole notebook will work every time.", "tokens": [264, 1379, 21060, 486, 589, 633, 565, 13], "temperature": 0.0, "avg_logprob": -0.1881609257963515, "compression_ratio": 1.6437768240343347, "no_speech_prob": 1.6027983065214357e-06}, {"id": 318, "seek": 151800, "start": 1529.28, "end": 1535.4, "text": " So rather than having 1000 different classes or formats and readers and writers and whatever,", "tokens": [407, 2831, 813, 1419, 9714, 819, 5359, 420, 25879, 293, 17147, 293, 13491, 293, 2035, 11], "temperature": 0.0, "avg_logprob": -0.1881609257963515, "compression_ratio": 1.6437768240343347, "no_speech_prob": 1.6027983065214357e-06}, {"id": 319, "seek": 151800, "start": 1535.4, "end": 1539.96, "text": " I've just said let's just pick a standard format and your job, your coders, you can", "tokens": [286, 600, 445, 848, 718, 311, 445, 1888, 257, 3832, 7877, 293, 428, 1691, 11, 428, 17656, 433, 11, 291, 393], "temperature": 0.0, "avg_logprob": -0.1881609257963515, "compression_ratio": 1.6437768240343347, "no_speech_prob": 1.6027983065214357e-06}, {"id": 320, "seek": 151800, "start": 1539.96, "end": 1546.08, "text": " do it perfectly well, is to put it in that format which is the CSV file.", "tokens": [360, 309, 6239, 731, 11, 307, 281, 829, 309, 294, 300, 7877, 597, 307, 264, 48814, 3991, 13], "temperature": 0.0, "avg_logprob": -0.1881609257963515, "compression_ratio": 1.6437768240343347, "no_speech_prob": 1.6027983065214357e-06}, {"id": 321, "seek": 154608, "start": 1546.08, "end": 1552.6, "text": " The CSV files have no header by default.", "tokens": [440, 48814, 7098, 362, 572, 23117, 538, 7576, 13], "temperature": 0.0, "avg_logprob": -0.17654593237515154, "compression_ratio": 1.655, "no_speech_prob": 2.6841755698114866e-06}, {"id": 322, "seek": 154608, "start": 1552.6, "end": 1556.6399999999999, "text": " Now you'll notice at the start here that I had two different paths.", "tokens": [823, 291, 603, 3449, 412, 264, 722, 510, 300, 286, 632, 732, 819, 14518, 13], "temperature": 0.0, "avg_logprob": -0.17654593237515154, "compression_ratio": 1.655, "no_speech_prob": 2.6841755698114866e-06}, {"id": 323, "seek": 154608, "start": 1556.6399999999999, "end": 1561.52, "text": " One was the classification path, one was the language model path.", "tokens": [1485, 390, 264, 21538, 3100, 11, 472, 390, 264, 2856, 2316, 3100, 13], "temperature": 0.0, "avg_logprob": -0.17654593237515154, "compression_ratio": 1.655, "no_speech_prob": 2.6841755698114866e-06}, {"id": 324, "seek": 154608, "start": 1561.52, "end": 1564.12, "text": " In NLP you'll see LM all the time.", "tokens": [682, 426, 45196, 291, 603, 536, 46529, 439, 264, 565, 13], "temperature": 0.0, "avg_logprob": -0.17654593237515154, "compression_ratio": 1.655, "no_speech_prob": 2.6841755698114866e-06}, {"id": 325, "seek": 154608, "start": 1564.12, "end": 1568.84, "text": " LM means language model in NLP.", "tokens": [46529, 1355, 2856, 2316, 294, 426, 45196, 13], "temperature": 0.0, "avg_logprob": -0.17654593237515154, "compression_ratio": 1.655, "no_speech_prob": 2.6841755698114866e-06}, {"id": 326, "seek": 154608, "start": 1568.84, "end": 1574.6, "text": " So the classification path is going to contain the information that we're going to use to", "tokens": [407, 264, 21538, 3100, 307, 516, 281, 5304, 264, 1589, 300, 321, 434, 516, 281, 764, 281], "temperature": 0.0, "avg_logprob": -0.17654593237515154, "compression_ratio": 1.655, "no_speech_prob": 2.6841755698114866e-06}, {"id": 327, "seek": 157460, "start": 1574.6, "end": 1577.24, "text": " create a sentiment analysis model.", "tokens": [1884, 257, 16149, 5215, 2316, 13], "temperature": 0.0, "avg_logprob": -0.14795612566398852, "compression_ratio": 1.8110599078341014, "no_speech_prob": 1.5779564819240477e-06}, {"id": 328, "seek": 157460, "start": 1577.24, "end": 1581.52, "text": " The language model path is going to contain the information we need to create a language", "tokens": [440, 2856, 2316, 3100, 307, 516, 281, 5304, 264, 1589, 321, 643, 281, 1884, 257, 2856], "temperature": 0.0, "avg_logprob": -0.14795612566398852, "compression_ratio": 1.8110599078341014, "no_speech_prob": 1.5779564819240477e-06}, {"id": 329, "seek": 157460, "start": 1581.52, "end": 1582.52, "text": " model.", "tokens": [2316, 13], "temperature": 0.0, "avg_logprob": -0.14795612566398852, "compression_ratio": 1.8110599078341014, "no_speech_prob": 1.5779564819240477e-06}, {"id": 330, "seek": 157460, "start": 1582.52, "end": 1583.9599999999998, "text": " So they're a little bit different.", "tokens": [407, 436, 434, 257, 707, 857, 819, 13], "temperature": 0.0, "avg_logprob": -0.14795612566398852, "compression_ratio": 1.8110599078341014, "no_speech_prob": 1.5779564819240477e-06}, {"id": 331, "seek": 157460, "start": 1583.9599999999998, "end": 1590.4399999999998, "text": " One thing that's different is that when we create the train.csv and the classification", "tokens": [1485, 551, 300, 311, 819, 307, 300, 562, 321, 1884, 264, 3847, 13, 14368, 85, 293, 264, 21538], "temperature": 0.0, "avg_logprob": -0.14795612566398852, "compression_ratio": 1.8110599078341014, "no_speech_prob": 1.5779564819240477e-06}, {"id": 332, "seek": 157460, "start": 1590.4399999999998, "end": 1600.6399999999999, "text": " path, we remove everything that has a label of 2 because label of 2 is unsupervised.", "tokens": [3100, 11, 321, 4159, 1203, 300, 575, 257, 7645, 295, 568, 570, 7645, 295, 568, 307, 2693, 12879, 24420, 13], "temperature": 0.0, "avg_logprob": -0.14795612566398852, "compression_ratio": 1.8110599078341014, "no_speech_prob": 1.5779564819240477e-06}, {"id": 333, "seek": 157460, "start": 1600.6399999999999, "end": 1602.7199999999998, "text": " So we remove the unsupervised data from the classifier.", "tokens": [407, 321, 4159, 264, 2693, 12879, 24420, 1412, 490, 264, 1508, 9902, 13], "temperature": 0.0, "avg_logprob": -0.14795612566398852, "compression_ratio": 1.8110599078341014, "no_speech_prob": 1.5779564819240477e-06}, {"id": 334, "seek": 160272, "start": 1602.72, "end": 1606.88, "text": " We can't use it.", "tokens": [492, 393, 380, 764, 309, 13], "temperature": 0.0, "avg_logprob": -0.1615703683150442, "compression_ratio": 1.5844155844155845, "no_speech_prob": 3.9669416764809284e-06}, {"id": 335, "seek": 160272, "start": 1606.88, "end": 1612.04, "text": " So that means this is going to have actually 25,000 positive, 25,000 negative.", "tokens": [407, 300, 1355, 341, 307, 516, 281, 362, 767, 3552, 11, 1360, 3353, 11, 3552, 11, 1360, 3671, 13], "temperature": 0.0, "avg_logprob": -0.1615703683150442, "compression_ratio": 1.5844155844155845, "no_speech_prob": 3.9669416764809284e-06}, {"id": 336, "seek": 160272, "start": 1612.04, "end": 1614.6000000000001, "text": " The second difference is the labels.", "tokens": [440, 1150, 2649, 307, 264, 16949, 13], "temperature": 0.0, "avg_logprob": -0.1615703683150442, "compression_ratio": 1.5844155844155845, "no_speech_prob": 3.9669416764809284e-06}, {"id": 337, "seek": 160272, "start": 1614.6000000000001, "end": 1619.56, "text": " For the classification path, the labels are the actual labels.", "tokens": [1171, 264, 21538, 3100, 11, 264, 16949, 366, 264, 3539, 16949, 13], "temperature": 0.0, "avg_logprob": -0.1615703683150442, "compression_ratio": 1.5844155844155845, "no_speech_prob": 3.9669416764809284e-06}, {"id": 338, "seek": 160272, "start": 1619.56, "end": 1624.64, "text": " But for the language model, there are no labels, so we just use a bunch of zeros.", "tokens": [583, 337, 264, 2856, 2316, 11, 456, 366, 572, 16949, 11, 370, 321, 445, 764, 257, 3840, 295, 35193, 13], "temperature": 0.0, "avg_logprob": -0.1615703683150442, "compression_ratio": 1.5844155844155845, "no_speech_prob": 3.9669416764809284e-06}, {"id": 339, "seek": 160272, "start": 1624.64, "end": 1629.3600000000001, "text": " That just makes it a little bit easier because we can use a consistent data frame format", "tokens": [663, 445, 1669, 309, 257, 707, 857, 3571, 570, 321, 393, 764, 257, 8398, 1412, 3920, 7877], "temperature": 0.0, "avg_logprob": -0.1615703683150442, "compression_ratio": 1.5844155844155845, "no_speech_prob": 3.9669416764809284e-06}, {"id": 340, "seek": 162936, "start": 1629.36, "end": 1632.9199999999998, "text": " or CSV format.", "tokens": [420, 48814, 7877, 13], "temperature": 0.0, "avg_logprob": -0.14788915970746208, "compression_ratio": 1.5598086124401913, "no_speech_prob": 2.9480052035069093e-06}, {"id": 341, "seek": 162936, "start": 1632.9199999999998, "end": 1638.7199999999998, "text": " Now the language model, we can create our own validation set.", "tokens": [823, 264, 2856, 2316, 11, 321, 393, 1884, 527, 1065, 24071, 992, 13], "temperature": 0.0, "avg_logprob": -0.14788915970746208, "compression_ratio": 1.5598086124401913, "no_speech_prob": 2.9480052035069093e-06}, {"id": 342, "seek": 162936, "start": 1638.7199999999998, "end": 1645.36, "text": " So you've probably come across by now sklearn.modelSelection.trainTestSplit, which is a really simple little", "tokens": [407, 291, 600, 1391, 808, 2108, 538, 586, 1110, 306, 1083, 13, 8014, 338, 10637, 5450, 13, 83, 7146, 51, 377, 50, 564, 270, 11, 597, 307, 257, 534, 2199, 707], "temperature": 0.0, "avg_logprob": -0.14788915970746208, "compression_ratio": 1.5598086124401913, "no_speech_prob": 2.9480052035069093e-06}, {"id": 343, "seek": 162936, "start": 1645.36, "end": 1651.36, "text": " function that grabs a data set and randomly splits it into a training set and a validation", "tokens": [2445, 300, 30028, 257, 1412, 992, 293, 16979, 37741, 309, 666, 257, 3097, 992, 293, 257, 24071], "temperature": 0.0, "avg_logprob": -0.14788915970746208, "compression_ratio": 1.5598086124401913, "no_speech_prob": 2.9480052035069093e-06}, {"id": 344, "seek": 162936, "start": 1651.36, "end": 1655.36, "text": " set according to whatever proportion you specify.", "tokens": [992, 4650, 281, 2035, 16068, 291, 16500, 13], "temperature": 0.0, "avg_logprob": -0.14788915970746208, "compression_ratio": 1.5598086124401913, "no_speech_prob": 2.9480052035069093e-06}, {"id": 345, "seek": 165536, "start": 1655.36, "end": 1661.12, "text": " And so in this case, I concatenate my classification, training and validation together, so it's", "tokens": [400, 370, 294, 341, 1389, 11, 286, 1588, 7186, 473, 452, 21538, 11, 3097, 293, 24071, 1214, 11, 370, 309, 311], "temperature": 0.0, "avg_logprob": -0.18896938123201068, "compression_ratio": 1.702970297029703, "no_speech_prob": 1.593649540154729e-05}, {"id": 346, "seek": 165536, "start": 1661.12, "end": 1667.36, "text": " going to be 100,000 altogether, split it by 10%, so now I've got 90,000 training, 10,000", "tokens": [516, 281, 312, 2319, 11, 1360, 19051, 11, 7472, 309, 538, 1266, 8923, 370, 586, 286, 600, 658, 4289, 11, 1360, 3097, 11, 1266, 11, 1360], "temperature": 0.0, "avg_logprob": -0.18896938123201068, "compression_ratio": 1.702970297029703, "no_speech_prob": 1.593649540154729e-05}, {"id": 347, "seek": 165536, "start": 1667.36, "end": 1669.28, "text": " validation for my language model.", "tokens": [24071, 337, 452, 2856, 2316, 13], "temperature": 0.0, "avg_logprob": -0.18896938123201068, "compression_ratio": 1.702970297029703, "no_speech_prob": 1.593649540154729e-05}, {"id": 348, "seek": 165536, "start": 1669.28, "end": 1672.7199999999998, "text": " So I'll go ahead and save that.", "tokens": [407, 286, 603, 352, 2286, 293, 3155, 300, 13], "temperature": 0.0, "avg_logprob": -0.18896938123201068, "compression_ratio": 1.702970297029703, "no_speech_prob": 1.593649540154729e-05}, {"id": 349, "seek": 165536, "start": 1672.7199999999998, "end": 1684.28, "text": " So that's my basic get the data in a standard format for my language model and my classifier.", "tokens": [407, 300, 311, 452, 3875, 483, 264, 1412, 294, 257, 3832, 7877, 337, 452, 2856, 2316, 293, 452, 1508, 9902, 13], "temperature": 0.0, "avg_logprob": -0.18896938123201068, "compression_ratio": 1.702970297029703, "no_speech_prob": 1.593649540154729e-05}, {"id": 350, "seek": 168428, "start": 1684.28, "end": 1689.3999999999999, "text": " So the next thing we need to do is tokenization.", "tokens": [407, 264, 958, 551, 321, 643, 281, 360, 307, 14862, 2144, 13], "temperature": 0.0, "avg_logprob": -0.24103901783625284, "compression_ratio": 1.6528497409326426, "no_speech_prob": 9.81821540335659e-06}, {"id": 351, "seek": 168428, "start": 1689.3999999999999, "end": 1696.24, "text": " So tokenization means at this stage, we've got for a document, we've got a big long string", "tokens": [407, 14862, 2144, 1355, 412, 341, 3233, 11, 321, 600, 658, 337, 257, 4166, 11, 321, 600, 658, 257, 955, 938, 6798], "temperature": 0.0, "avg_logprob": -0.24103901783625284, "compression_ratio": 1.6528497409326426, "no_speech_prob": 9.81821540335659e-06}, {"id": 352, "seek": 168428, "start": 1696.24, "end": 1703.32, "text": " and we want to turn it into a list of tokens, which are kind of a list of words, but not", "tokens": [293, 321, 528, 281, 1261, 309, 666, 257, 1329, 295, 22667, 11, 597, 366, 733, 295, 257, 1329, 295, 2283, 11, 457, 406], "temperature": 0.0, "avg_logprob": -0.24103901783625284, "compression_ratio": 1.6528497409326426, "no_speech_prob": 9.81821540335659e-06}, {"id": 353, "seek": 168428, "start": 1703.32, "end": 1704.92, "text": " quite.", "tokens": [1596, 13], "temperature": 0.0, "avg_logprob": -0.24103901783625284, "compression_ratio": 1.6528497409326426, "no_speech_prob": 9.81821540335659e-06}, {"id": 354, "seek": 168428, "start": 1704.92, "end": 1712.96, "text": " For example, don't, we want to be doh nt, we probably want full stop to be a token,", "tokens": [1171, 1365, 11, 500, 380, 11, 321, 528, 281, 312, 360, 71, 297, 83, 11, 321, 1391, 528, 1577, 1590, 281, 312, 257, 14862, 11], "temperature": 0.0, "avg_logprob": -0.24103901783625284, "compression_ratio": 1.6528497409326426, "no_speech_prob": 9.81821540335659e-06}, {"id": 355, "seek": 171296, "start": 1712.96, "end": 1719.28, "text": " and so forth, right, so tokenization is something that we passed off to a terrific library called", "tokens": [293, 370, 5220, 11, 558, 11, 370, 14862, 2144, 307, 746, 300, 321, 4678, 766, 281, 257, 20899, 6405, 1219], "temperature": 0.0, "avg_logprob": -0.19597035362606957, "compression_ratio": 1.5916230366492146, "no_speech_prob": 5.093640993436566e-06}, {"id": 356, "seek": 171296, "start": 1719.28, "end": 1725.64, "text": " spaCy, partly terrific because an Australian wrote it, and partly terrific because it's", "tokens": [32543, 34, 88, 11, 17031, 20899, 570, 364, 13337, 4114, 309, 11, 293, 17031, 20899, 570, 309, 311], "temperature": 0.0, "avg_logprob": -0.19597035362606957, "compression_ratio": 1.5916230366492146, "no_speech_prob": 5.093640993436566e-06}, {"id": 357, "seek": 171296, "start": 1725.64, "end": 1729.72, "text": " good at what it does.", "tokens": [665, 412, 437, 309, 775, 13], "temperature": 0.0, "avg_logprob": -0.19597035362606957, "compression_ratio": 1.5916230366492146, "no_speech_prob": 5.093640993436566e-06}, {"id": 358, "seek": 171296, "start": 1729.72, "end": 1733.8, "text": " We put a bit of stuff on top of spaCy, but the vast majority of the work is being done", "tokens": [492, 829, 257, 857, 295, 1507, 322, 1192, 295, 32543, 34, 88, 11, 457, 264, 8369, 6286, 295, 264, 589, 307, 885, 1096], "temperature": 0.0, "avg_logprob": -0.19597035362606957, "compression_ratio": 1.5916230366492146, "no_speech_prob": 5.093640993436566e-06}, {"id": 359, "seek": 171296, "start": 1733.8, "end": 1735.6000000000001, "text": " by spaCy.", "tokens": [538, 32543, 34, 88, 13], "temperature": 0.0, "avg_logprob": -0.19597035362606957, "compression_ratio": 1.5916230366492146, "no_speech_prob": 5.093640993436566e-06}, {"id": 360, "seek": 173560, "start": 1735.6, "end": 1743.36, "text": " Before we pass it to spaCy, I've written this simple fixup function, which is basically", "tokens": [4546, 321, 1320, 309, 281, 32543, 34, 88, 11, 286, 600, 3720, 341, 2199, 3191, 1010, 2445, 11, 597, 307, 1936], "temperature": 0.0, "avg_logprob": -0.12691416523673318, "compression_ratio": 1.5135135135135136, "no_speech_prob": 1.844815642471076e-06}, {"id": 361, "seek": 173560, "start": 1743.36, "end": 1747.8, "text": " each time I look at a different dataset, and I've looked at about a dozen in building this,", "tokens": [1184, 565, 286, 574, 412, 257, 819, 28872, 11, 293, 286, 600, 2956, 412, 466, 257, 16654, 294, 2390, 341, 11], "temperature": 0.0, "avg_logprob": -0.12691416523673318, "compression_ratio": 1.5135135135135136, "no_speech_prob": 1.844815642471076e-06}, {"id": 362, "seek": 173560, "start": 1747.8, "end": 1753.48, "text": " everyone had different weird things that needed to be replaced.", "tokens": [1518, 632, 819, 3657, 721, 300, 2978, 281, 312, 10772, 13], "temperature": 0.0, "avg_logprob": -0.12691416523673318, "compression_ratio": 1.5135135135135136, "no_speech_prob": 1.844815642471076e-06}, {"id": 363, "seek": 173560, "start": 1753.48, "end": 1758.32, "text": " So here are all the ones I've come up with so far.", "tokens": [407, 510, 366, 439, 264, 2306, 286, 600, 808, 493, 365, 370, 1400, 13], "temperature": 0.0, "avg_logprob": -0.12691416523673318, "compression_ratio": 1.5135135135135136, "no_speech_prob": 1.844815642471076e-06}, {"id": 364, "seek": 173560, "start": 1758.32, "end": 1762.6599999999999, "text": " Hopefully this will help you out as well.", "tokens": [10429, 341, 486, 854, 291, 484, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.12691416523673318, "compression_ratio": 1.5135135135135136, "no_speech_prob": 1.844815642471076e-06}, {"id": 365, "seek": 176266, "start": 1762.66, "end": 1768.76, "text": " So I html and escape all the entities, and then there's a bunch more things I replace.", "tokens": [407, 286, 276, 83, 15480, 293, 7615, 439, 264, 16667, 11, 293, 550, 456, 311, 257, 3840, 544, 721, 286, 7406, 13], "temperature": 0.0, "avg_logprob": -0.13919646843619968, "compression_ratio": 1.591743119266055, "no_speech_prob": 2.3552214770461433e-05}, {"id": 366, "seek": 176266, "start": 1768.76, "end": 1772.48, "text": " Have a look at the result of running this on text that you put in and make sure there's", "tokens": [3560, 257, 574, 412, 264, 1874, 295, 2614, 341, 322, 2487, 300, 291, 829, 294, 293, 652, 988, 456, 311], "temperature": 0.0, "avg_logprob": -0.13919646843619968, "compression_ratio": 1.591743119266055, "no_speech_prob": 2.3552214770461433e-05}, {"id": 367, "seek": 176266, "start": 1772.48, "end": 1776.0, "text": " not more weird tokens in there.", "tokens": [406, 544, 3657, 22667, 294, 456, 13], "temperature": 0.0, "avg_logprob": -0.13919646843619968, "compression_ratio": 1.591743119266055, "no_speech_prob": 2.3552214770461433e-05}, {"id": 368, "seek": 176266, "start": 1776.0, "end": 1781.18, "text": " It's amazing how many weird things people do to text.", "tokens": [467, 311, 2243, 577, 867, 3657, 721, 561, 360, 281, 2487, 13], "temperature": 0.0, "avg_logprob": -0.13919646843619968, "compression_ratio": 1.591743119266055, "no_speech_prob": 2.3552214770461433e-05}, {"id": 369, "seek": 176266, "start": 1781.18, "end": 1787.8400000000001, "text": " So basically I've got this function called getAll, which is going to go ahead and call", "tokens": [407, 1936, 286, 600, 658, 341, 2445, 1219, 483, 7868, 11, 597, 307, 516, 281, 352, 2286, 293, 818], "temperature": 0.0, "avg_logprob": -0.13919646843619968, "compression_ratio": 1.591743119266055, "no_speech_prob": 2.3552214770461433e-05}, {"id": 370, "seek": 178784, "start": 1787.84, "end": 1792.56, "text": " getTexts, and getTexts is going to go ahead and do a few things, one of which is to apply", "tokens": [483, 50198, 82, 11, 293, 483, 50198, 82, 307, 516, 281, 352, 2286, 293, 360, 257, 1326, 721, 11, 472, 295, 597, 307, 281, 3079], "temperature": 0.0, "avg_logprob": -0.16092939284241314, "compression_ratio": 1.590717299578059, "no_speech_prob": 7.296327567019034e-06}, {"id": 371, "seek": 178784, "start": 1792.56, "end": 1797.52, "text": " that fixup that we just mentioned.", "tokens": [300, 3191, 1010, 300, 321, 445, 2835, 13], "temperature": 0.0, "avg_logprob": -0.16092939284241314, "compression_ratio": 1.590717299578059, "no_speech_prob": 7.296327567019034e-06}, {"id": 372, "seek": 178784, "start": 1797.52, "end": 1801.0, "text": " So let's kind of look through this because there's some interesting things to point out.", "tokens": [407, 718, 311, 733, 295, 574, 807, 341, 570, 456, 311, 512, 1880, 721, 281, 935, 484, 13], "temperature": 0.0, "avg_logprob": -0.16092939284241314, "compression_ratio": 1.590717299578059, "no_speech_prob": 7.296327567019034e-06}, {"id": 373, "seek": 178784, "start": 1801.0, "end": 1807.84, "text": " So I'm going to use pandas to open our train.csv from the language model path, but I'm passing", "tokens": [407, 286, 478, 516, 281, 764, 4565, 296, 281, 1269, 527, 3847, 13, 14368, 85, 490, 264, 2856, 2316, 3100, 11, 457, 286, 478, 8437], "temperature": 0.0, "avg_logprob": -0.16092939284241314, "compression_ratio": 1.590717299578059, "no_speech_prob": 7.296327567019034e-06}, {"id": 374, "seek": 178784, "start": 1807.84, "end": 1814.72, "text": " in an extra parameter you may not have seen before called chunkSize.", "tokens": [294, 364, 2857, 13075, 291, 815, 406, 362, 1612, 949, 1219, 16635, 50, 1125, 13], "temperature": 0.0, "avg_logprob": -0.16092939284241314, "compression_ratio": 1.590717299578059, "no_speech_prob": 7.296327567019034e-06}, {"id": 375, "seek": 181472, "start": 1814.72, "end": 1823.48, "text": " Python and pandas can both be pretty inefficient when it comes to storing and using text data.", "tokens": [15329, 293, 4565, 296, 393, 1293, 312, 1238, 43495, 562, 309, 1487, 281, 26085, 293, 1228, 2487, 1412, 13], "temperature": 0.0, "avg_logprob": -0.1312696808262875, "compression_ratio": 1.4660194174757282, "no_speech_prob": 5.173874342290219e-06}, {"id": 376, "seek": 181472, "start": 1823.48, "end": 1833.1200000000001, "text": " And so you'll see that very few people in NLP are working with large corpuses.", "tokens": [400, 370, 291, 603, 536, 300, 588, 1326, 561, 294, 426, 45196, 366, 1364, 365, 2416, 1181, 79, 8355, 13], "temperature": 0.0, "avg_logprob": -0.1312696808262875, "compression_ratio": 1.4660194174757282, "no_speech_prob": 5.173874342290219e-06}, {"id": 377, "seek": 181472, "start": 1833.1200000000001, "end": 1837.56, "text": " And I think part of the reason is that traditional tools have just made it really difficult.", "tokens": [400, 286, 519, 644, 295, 264, 1778, 307, 300, 5164, 3873, 362, 445, 1027, 309, 534, 2252, 13], "temperature": 0.0, "avg_logprob": -0.1312696808262875, "compression_ratio": 1.4660194174757282, "no_speech_prob": 5.173874342290219e-06}, {"id": 378, "seek": 181472, "start": 1837.56, "end": 1840.34, "text": " You run out of memory all the time.", "tokens": [509, 1190, 484, 295, 4675, 439, 264, 565, 13], "temperature": 0.0, "avg_logprob": -0.1312696808262875, "compression_ratio": 1.4660194174757282, "no_speech_prob": 5.173874342290219e-06}, {"id": 379, "seek": 184034, "start": 1840.34, "end": 1848.48, "text": " So this process I'm showing you today I have used on corpuses of over a billion words successfully", "tokens": [407, 341, 1399, 286, 478, 4099, 291, 965, 286, 362, 1143, 322, 1181, 79, 8355, 295, 670, 257, 5218, 2283, 10727], "temperature": 0.0, "avg_logprob": -0.08585710204049443, "compression_ratio": 1.5852534562211982, "no_speech_prob": 1.2289158348721685e-06}, {"id": 380, "seek": 184034, "start": 1848.48, "end": 1850.6399999999999, "text": " using this exact code.", "tokens": [1228, 341, 1900, 3089, 13], "temperature": 0.0, "avg_logprob": -0.08585710204049443, "compression_ratio": 1.5852534562211982, "no_speech_prob": 1.2289158348721685e-06}, {"id": 381, "seek": 184034, "start": 1850.6399999999999, "end": 1855.26, "text": " And so one of the simple tricks is to use this thing called chunkSize with pandas.", "tokens": [400, 370, 472, 295, 264, 2199, 11733, 307, 281, 764, 341, 551, 1219, 16635, 50, 1125, 365, 4565, 296, 13], "temperature": 0.0, "avg_logprob": -0.08585710204049443, "compression_ratio": 1.5852534562211982, "no_speech_prob": 1.2289158348721685e-06}, {"id": 382, "seek": 184034, "start": 1855.26, "end": 1861.08, "text": " What that means is that pandas does not return a data frame, but it returns an iterator that", "tokens": [708, 300, 1355, 307, 300, 4565, 296, 775, 406, 2736, 257, 1412, 3920, 11, 457, 309, 11247, 364, 17138, 1639, 300], "temperature": 0.0, "avg_logprob": -0.08585710204049443, "compression_ratio": 1.5852534562211982, "no_speech_prob": 1.2289158348721685e-06}, {"id": 383, "seek": 184034, "start": 1861.08, "end": 1866.08, "text": " we can iterate through chunks of a data frame.", "tokens": [321, 393, 44497, 807, 24004, 295, 257, 1412, 3920, 13], "temperature": 0.0, "avg_logprob": -0.08585710204049443, "compression_ratio": 1.5852534562211982, "no_speech_prob": 1.2289158348721685e-06}, {"id": 384, "seek": 186608, "start": 1866.08, "end": 1877.8799999999999, "text": " And so that's why I don't say topTrain equals getTexts, but instead I call getAll which", "tokens": [400, 370, 300, 311, 983, 286, 500, 380, 584, 1192, 51, 7146, 6915, 483, 50198, 82, 11, 457, 2602, 286, 818, 483, 7868, 597], "temperature": 0.0, "avg_logprob": -0.16002862077010305, "compression_ratio": 1.6179775280898876, "no_speech_prob": 5.5075752243283205e-06}, {"id": 385, "seek": 186608, "start": 1877.8799999999999, "end": 1880.48, "text": " loops through the data frame.", "tokens": [16121, 807, 264, 1412, 3920, 13], "temperature": 0.0, "avg_logprob": -0.16002862077010305, "compression_ratio": 1.6179775280898876, "no_speech_prob": 5.5075752243283205e-06}, {"id": 386, "seek": 186608, "start": 1880.48, "end": 1884.9199999999998, "text": " But actually what it's really doing is it's looping through chunks of the data frame.", "tokens": [583, 767, 437, 309, 311, 534, 884, 307, 309, 311, 6367, 278, 807, 24004, 295, 264, 1412, 3920, 13], "temperature": 0.0, "avg_logprob": -0.16002862077010305, "compression_ratio": 1.6179775280898876, "no_speech_prob": 5.5075752243283205e-06}, {"id": 387, "seek": 186608, "start": 1884.9199999999998, "end": 1891.8799999999999, "text": " So each of those chunks is basically a data frame representing a subset of the data.", "tokens": [407, 1184, 295, 729, 24004, 307, 1936, 257, 1412, 3920, 13460, 257, 25993, 295, 264, 1412, 13], "temperature": 0.0, "avg_logprob": -0.16002862077010305, "compression_ratio": 1.6179775280898876, "no_speech_prob": 5.5075752243283205e-06}, {"id": 388, "seek": 189188, "start": 1891.88, "end": 1897.96, "text": " And I'm working with NLP data, many times I come across data with foreign text or characters.", "tokens": [400, 286, 478, 1364, 365, 426, 45196, 1412, 11, 867, 1413, 286, 808, 2108, 1412, 365, 5329, 2487, 420, 4342, 13], "temperature": 0.0, "avg_logprob": -0.16882985181147508, "compression_ratio": 1.5378151260504203, "no_speech_prob": 1.9833167243632488e-05}, {"id": 389, "seek": 189188, "start": 1897.96, "end": 1900.0, "text": " Is it better to discard them or keep them?", "tokens": [1119, 309, 1101, 281, 31597, 552, 420, 1066, 552, 30], "temperature": 0.0, "avg_logprob": -0.16882985181147508, "compression_ratio": 1.5378151260504203, "no_speech_prob": 1.9833167243632488e-05}, {"id": 390, "seek": 189188, "start": 1900.0, "end": 1902.0400000000002, "text": " No, no, definitely keep them.", "tokens": [883, 11, 572, 11, 2138, 1066, 552, 13], "temperature": 0.0, "avg_logprob": -0.16882985181147508, "compression_ratio": 1.5378151260504203, "no_speech_prob": 1.9833167243632488e-05}, {"id": 391, "seek": 189188, "start": 1902.0400000000002, "end": 1905.16, "text": " And this whole process is Unicode.", "tokens": [400, 341, 1379, 1399, 307, 1156, 299, 1429, 13], "temperature": 0.0, "avg_logprob": -0.16882985181147508, "compression_ratio": 1.5378151260504203, "no_speech_prob": 1.9833167243632488e-05}, {"id": 392, "seek": 189188, "start": 1905.16, "end": 1908.68, "text": " And I've actually used this on Chinese text.", "tokens": [400, 286, 600, 767, 1143, 341, 322, 4649, 2487, 13], "temperature": 0.0, "avg_logprob": -0.16882985181147508, "compression_ratio": 1.5378151260504203, "no_speech_prob": 1.9833167243632488e-05}, {"id": 393, "seek": 189188, "start": 1908.68, "end": 1914.24, "text": " This is designed to work on pretty much anything.", "tokens": [639, 307, 4761, 281, 589, 322, 1238, 709, 1340, 13], "temperature": 0.0, "avg_logprob": -0.16882985181147508, "compression_ratio": 1.5378151260504203, "no_speech_prob": 1.9833167243632488e-05}, {"id": 394, "seek": 189188, "start": 1914.24, "end": 1920.24, "text": " In general, most of the time it's not a good idea to remove anything.", "tokens": [682, 2674, 11, 881, 295, 264, 565, 309, 311, 406, 257, 665, 1558, 281, 4159, 1340, 13], "temperature": 0.0, "avg_logprob": -0.16882985181147508, "compression_ratio": 1.5378151260504203, "no_speech_prob": 1.9833167243632488e-05}, {"id": 395, "seek": 192024, "start": 1920.24, "end": 1925.2, "text": " Like old-fashioned NLP approaches tend to do all this like lemmatization and all these", "tokens": [1743, 1331, 12, 37998, 426, 45196, 11587, 3928, 281, 360, 439, 341, 411, 7495, 15677, 2144, 293, 439, 613], "temperature": 0.0, "avg_logprob": -0.2054689327875773, "compression_ratio": 1.5938864628820961, "no_speech_prob": 1.6442132618976757e-05}, {"id": 396, "seek": 192024, "start": 1925.2, "end": 1930.1200000000001, "text": " kind of normalization steps to get rid of lowercase everything.", "tokens": [733, 295, 2710, 2144, 4439, 281, 483, 3973, 295, 3126, 9765, 1203, 13], "temperature": 0.0, "avg_logprob": -0.2054689327875773, "compression_ratio": 1.5938864628820961, "no_speech_prob": 1.6442132618976757e-05}, {"id": 397, "seek": 192024, "start": 1930.1200000000001, "end": 1935.2, "text": " But that's throwing away information which you don't know ahead of time whether it's", "tokens": [583, 300, 311, 10238, 1314, 1589, 597, 291, 500, 380, 458, 2286, 295, 565, 1968, 309, 311], "temperature": 0.0, "avg_logprob": -0.2054689327875773, "compression_ratio": 1.5938864628820961, "no_speech_prob": 1.6442132618976757e-05}, {"id": 398, "seek": 192024, "start": 1935.2, "end": 1936.2, "text": " useful or not.", "tokens": [4420, 420, 406, 13], "temperature": 0.0, "avg_logprob": -0.2054689327875773, "compression_ratio": 1.5938864628820961, "no_speech_prob": 1.6442132618976757e-05}, {"id": 399, "seek": 192024, "start": 1936.2, "end": 1940.72, "text": " So don't throw away information.", "tokens": [407, 500, 380, 3507, 1314, 1589, 13], "temperature": 0.0, "avg_logprob": -0.2054689327875773, "compression_ratio": 1.5938864628820961, "no_speech_prob": 1.6442132618976757e-05}, {"id": 400, "seek": 192024, "start": 1940.72, "end": 1946.04, "text": " So we go through each chunk, each of which is a data frame, and we call getTexts.", "tokens": [407, 321, 352, 807, 1184, 16635, 11, 1184, 295, 597, 307, 257, 1412, 3920, 11, 293, 321, 818, 483, 50198, 82, 13], "temperature": 0.0, "avg_logprob": -0.2054689327875773, "compression_ratio": 1.5938864628820961, "no_speech_prob": 1.6442132618976757e-05}, {"id": 401, "seek": 194604, "start": 1946.04, "end": 1950.92, "text": " getTexts is going to grab the labels and make them into ints.", "tokens": [483, 50198, 82, 307, 516, 281, 4444, 264, 16949, 293, 652, 552, 666, 560, 82, 13], "temperature": 0.0, "avg_logprob": -0.14505449596204256, "compression_ratio": 1.6026200873362446, "no_speech_prob": 1.2029498066112865e-05}, {"id": 402, "seek": 194604, "start": 1950.92, "end": 1958.0, "text": " It's going to grab then the texts.", "tokens": [467, 311, 516, 281, 4444, 550, 264, 15765, 13], "temperature": 0.0, "avg_logprob": -0.14505449596204256, "compression_ratio": 1.6026200873362446, "no_speech_prob": 1.2029498066112865e-05}, {"id": 403, "seek": 194604, "start": 1958.0, "end": 1959.8, "text": " And I'll point out a couple of things.", "tokens": [400, 286, 603, 935, 484, 257, 1916, 295, 721, 13], "temperature": 0.0, "avg_logprob": -0.14505449596204256, "compression_ratio": 1.6026200873362446, "no_speech_prob": 1.2029498066112865e-05}, {"id": 404, "seek": 194604, "start": 1959.8, "end": 1965.72, "text": " The first is that before we include the text, we have this beginning of stream token, which", "tokens": [440, 700, 307, 300, 949, 321, 4090, 264, 2487, 11, 321, 362, 341, 2863, 295, 4309, 14862, 11, 597], "temperature": 0.0, "avg_logprob": -0.14505449596204256, "compression_ratio": 1.6026200873362446, "no_speech_prob": 1.2029498066112865e-05}, {"id": 405, "seek": 194604, "start": 1965.72, "end": 1969.84, "text": " you might remember we used way back up here.", "tokens": [291, 1062, 1604, 321, 1143, 636, 646, 493, 510, 13], "temperature": 0.0, "avg_logprob": -0.14505449596204256, "compression_ratio": 1.6026200873362446, "no_speech_prob": 1.2029498066112865e-05}, {"id": 406, "seek": 194604, "start": 1969.84, "end": 1973.56, "text": " There's nothing special about these particular strings of letters, they're just ones I figured", "tokens": [821, 311, 1825, 2121, 466, 613, 1729, 13985, 295, 7825, 11, 436, 434, 445, 2306, 286, 8932], "temperature": 0.0, "avg_logprob": -0.14505449596204256, "compression_ratio": 1.6026200873362446, "no_speech_prob": 1.2029498066112865e-05}, {"id": 407, "seek": 197356, "start": 1973.56, "end": 1978.24, "text": " don't appear in normal texts very often.", "tokens": [500, 380, 4204, 294, 2710, 15765, 588, 2049, 13], "temperature": 0.0, "avg_logprob": -0.18691996938174532, "compression_ratio": 1.634703196347032, "no_speech_prob": 3.089482106588548e-06}, {"id": 408, "seek": 197356, "start": 1978.24, "end": 1982.0, "text": " So every text is going to start with xbos.", "tokens": [407, 633, 2487, 307, 516, 281, 722, 365, 2031, 65, 329, 13], "temperature": 0.0, "avg_logprob": -0.18691996938174532, "compression_ratio": 1.634703196347032, "no_speech_prob": 3.089482106588548e-06}, {"id": 409, "seek": 197356, "start": 1982.0, "end": 1983.1599999999999, "text": " Why is that?", "tokens": [1545, 307, 300, 30], "temperature": 0.0, "avg_logprob": -0.18691996938174532, "compression_ratio": 1.634703196347032, "no_speech_prob": 3.089482106588548e-06}, {"id": 410, "seek": 197356, "start": 1983.1599999999999, "end": 1988.36, "text": " Because it's often really useful for your model to know when a new text is starting.", "tokens": [1436, 309, 311, 2049, 534, 4420, 337, 428, 2316, 281, 458, 562, 257, 777, 2487, 307, 2891, 13], "temperature": 0.0, "avg_logprob": -0.18691996938174532, "compression_ratio": 1.634703196347032, "no_speech_prob": 3.089482106588548e-06}, {"id": 411, "seek": 197356, "start": 1988.36, "end": 1994.1599999999999, "text": " For example, if it's a language model, you're going to concatenate all the text together", "tokens": [1171, 1365, 11, 498, 309, 311, 257, 2856, 2316, 11, 291, 434, 516, 281, 1588, 7186, 473, 439, 264, 2487, 1214], "temperature": 0.0, "avg_logprob": -0.18691996938174532, "compression_ratio": 1.634703196347032, "no_speech_prob": 3.089482106588548e-06}, {"id": 412, "seek": 197356, "start": 1994.1599999999999, "end": 1997.24, "text": " and so it would be really helpful for it to know this article is finished and a new one", "tokens": [293, 370, 309, 576, 312, 534, 4961, 337, 309, 281, 458, 341, 7222, 307, 4335, 293, 257, 777, 472], "temperature": 0.0, "avg_logprob": -0.18691996938174532, "compression_ratio": 1.634703196347032, "no_speech_prob": 3.089482106588548e-06}, {"id": 413, "seek": 199724, "start": 1997.24, "end": 2004.04, "text": " started so I should probably forget some of that context now.", "tokens": [1409, 370, 286, 820, 1391, 2870, 512, 295, 300, 4319, 586, 13], "temperature": 0.0, "avg_logprob": -0.2065781002952939, "compression_ratio": 1.594488188976378, "no_speech_prob": 2.6841858016268816e-06}, {"id": 414, "seek": 199724, "start": 2004.04, "end": 2011.2, "text": " Ditto is quite often texts have multiple fields, like a title, an abstract, and then the main", "tokens": [413, 34924, 307, 1596, 2049, 15765, 362, 3866, 7909, 11, 411, 257, 4876, 11, 364, 12649, 11, 293, 550, 264, 2135], "temperature": 0.0, "avg_logprob": -0.2065781002952939, "compression_ratio": 1.594488188976378, "no_speech_prob": 2.6841858016268816e-06}, {"id": 415, "seek": 199724, "start": 2011.2, "end": 2012.2, "text": " document.", "tokens": [4166, 13], "temperature": 0.0, "avg_logprob": -0.2065781002952939, "compression_ratio": 1.594488188976378, "no_speech_prob": 2.6841858016268816e-06}, {"id": 416, "seek": 199724, "start": 2012.2, "end": 2016.2, "text": " So by the same token, I've got this thing here which lets us actually have multiple", "tokens": [407, 538, 264, 912, 14862, 11, 286, 600, 658, 341, 551, 510, 597, 6653, 505, 767, 362, 3866], "temperature": 0.0, "avg_logprob": -0.2065781002952939, "compression_ratio": 1.594488188976378, "no_speech_prob": 2.6841858016268816e-06}, {"id": 417, "seek": 199724, "start": 2016.2, "end": 2018.96, "text": " fields in our CSP.", "tokens": [7909, 294, 527, 9460, 47, 13], "temperature": 0.0, "avg_logprob": -0.2065781002952939, "compression_ratio": 1.594488188976378, "no_speech_prob": 2.6841858016268816e-06}, {"id": 418, "seek": 199724, "start": 2018.96, "end": 2021.6, "text": " So this process is designed to be very flexible.", "tokens": [407, 341, 1399, 307, 4761, 281, 312, 588, 11358, 13], "temperature": 0.0, "avg_logprob": -0.2065781002952939, "compression_ratio": 1.594488188976378, "no_speech_prob": 2.6841858016268816e-06}, {"id": 419, "seek": 199724, "start": 2021.6, "end": 2026.52, "text": " And again, at the start of each one, we put a special field starts here token, followed", "tokens": [400, 797, 11, 412, 264, 722, 295, 1184, 472, 11, 321, 829, 257, 2121, 2519, 3719, 510, 14862, 11, 6263], "temperature": 0.0, "avg_logprob": -0.2065781002952939, "compression_ratio": 1.594488188976378, "no_speech_prob": 2.6841858016268816e-06}, {"id": 420, "seek": 202652, "start": 2026.52, "end": 2032.28, "text": " by the number of the field that's starting here for as many fields as we have.", "tokens": [538, 264, 1230, 295, 264, 2519, 300, 311, 2891, 510, 337, 382, 867, 7909, 382, 321, 362, 13], "temperature": 0.0, "avg_logprob": -0.1805286407470703, "compression_ratio": 1.5894736842105264, "no_speech_prob": 3.1381271128338994e-06}, {"id": 421, "seek": 202652, "start": 2032.28, "end": 2034.48, "text": " Then we apply our fixup to it.", "tokens": [1396, 321, 3079, 527, 3191, 1010, 281, 309, 13], "temperature": 0.0, "avg_logprob": -0.1805286407470703, "compression_ratio": 1.5894736842105264, "no_speech_prob": 3.1381271128338994e-06}, {"id": 422, "seek": 202652, "start": 2034.48, "end": 2037.94, "text": " And then most importantly, we tokenize it.", "tokens": [400, 550, 881, 8906, 11, 321, 14862, 1125, 309, 13], "temperature": 0.0, "avg_logprob": -0.1805286407470703, "compression_ratio": 1.5894736842105264, "no_speech_prob": 3.1381271128338994e-06}, {"id": 423, "seek": 202652, "start": 2037.94, "end": 2048.36, "text": " And we tokenize it by doing a process-all multiprocessor.", "tokens": [400, 321, 14862, 1125, 309, 538, 884, 257, 1399, 12, 336, 3311, 340, 25432, 13], "temperature": 0.0, "avg_logprob": -0.1805286407470703, "compression_ratio": 1.5894736842105264, "no_speech_prob": 3.1381271128338994e-06}, {"id": 424, "seek": 202652, "start": 2048.36, "end": 2054.2, "text": " And so tokenizing tends to be pretty slow, but we've all got multiple cores in our machines", "tokens": [400, 370, 14862, 3319, 12258, 281, 312, 1238, 2964, 11, 457, 321, 600, 439, 658, 3866, 24826, 294, 527, 8379], "temperature": 0.0, "avg_logprob": -0.1805286407470703, "compression_ratio": 1.5894736842105264, "no_speech_prob": 3.1381271128338994e-06}, {"id": 425, "seek": 205420, "start": 2054.2, "end": 2059.7999999999997, "text": " now and some of the better machines on AWS and stuff can have dozens of cores.", "tokens": [586, 293, 512, 295, 264, 1101, 8379, 322, 17650, 293, 1507, 393, 362, 18431, 295, 24826, 13], "temperature": 0.0, "avg_logprob": -0.12512867472996222, "compression_ratio": 1.5502008032128514, "no_speech_prob": 1.8058290152112022e-05}, {"id": 426, "seek": 205420, "start": 2059.7999999999997, "end": 2065.24, "text": " Here on our university computer, we've got 56 cores.", "tokens": [1692, 322, 527, 5454, 3820, 11, 321, 600, 658, 19687, 24826, 13], "temperature": 0.0, "avg_logprob": -0.12512867472996222, "compression_ratio": 1.5502008032128514, "no_speech_prob": 1.8058290152112022e-05}, {"id": 427, "seek": 205420, "start": 2065.24, "end": 2072.68, "text": " So spaCy is not very amenable to multiprocessing, but I finally figured out how to get it to", "tokens": [407, 32543, 34, 88, 307, 406, 588, 18497, 712, 281, 3311, 340, 780, 278, 11, 457, 286, 2721, 8932, 484, 577, 281, 483, 309, 281], "temperature": 0.0, "avg_logprob": -0.12512867472996222, "compression_ratio": 1.5502008032128514, "no_speech_prob": 1.8058290152112022e-05}, {"id": 428, "seek": 205420, "start": 2072.68, "end": 2073.68, "text": " work.", "tokens": [589, 13], "temperature": 0.0, "avg_logprob": -0.12512867472996222, "compression_ratio": 1.5502008032128514, "no_speech_prob": 1.8058290152112022e-05}, {"id": 429, "seek": 205420, "start": 2073.68, "end": 2077.3199999999997, "text": " And the good news is it's all wrapped up in this one function now.", "tokens": [400, 264, 665, 2583, 307, 309, 311, 439, 14226, 493, 294, 341, 472, 2445, 586, 13], "temperature": 0.0, "avg_logprob": -0.12512867472996222, "compression_ratio": 1.5502008032128514, "no_speech_prob": 1.8058290152112022e-05}, {"id": 430, "seek": 205420, "start": 2077.3199999999997, "end": 2083.3399999999997, "text": " And so all you need to pass to that function is a list of things to tokenize, which each", "tokens": [400, 370, 439, 291, 643, 281, 1320, 281, 300, 2445, 307, 257, 1329, 295, 721, 281, 14862, 1125, 11, 597, 1184], "temperature": 0.0, "avg_logprob": -0.12512867472996222, "compression_ratio": 1.5502008032128514, "no_speech_prob": 1.8058290152112022e-05}, {"id": 431, "seek": 208334, "start": 2083.34, "end": 2087.26, "text": " part of that list will be tokenized on a different core.", "tokens": [644, 295, 300, 1329, 486, 312, 14862, 1602, 322, 257, 819, 4965, 13], "temperature": 0.0, "avg_logprob": -0.12996353822595932, "compression_ratio": 1.6578947368421053, "no_speech_prob": 2.561268502176972e-06}, {"id": 432, "seek": 208334, "start": 2087.26, "end": 2092.56, "text": " And so I've also created this function called partitionByCores, which takes a list and splits", "tokens": [400, 370, 286, 600, 611, 2942, 341, 2445, 1219, 24808, 27690, 34, 2706, 11, 597, 2516, 257, 1329, 293, 37741], "temperature": 0.0, "avg_logprob": -0.12996353822595932, "compression_ratio": 1.6578947368421053, "no_speech_prob": 2.561268502176972e-06}, {"id": 433, "seek": 208334, "start": 2092.56, "end": 2093.92, "text": " it into sublists.", "tokens": [309, 666, 1422, 36693, 13], "temperature": 0.0, "avg_logprob": -0.12996353822595932, "compression_ratio": 1.6578947368421053, "no_speech_prob": 2.561268502176972e-06}, {"id": 434, "seek": 208334, "start": 2093.92, "end": 2098.6400000000003, "text": " The number of sublists is the number of cores that you have in your computer.", "tokens": [440, 1230, 295, 1422, 36693, 307, 264, 1230, 295, 24826, 300, 291, 362, 294, 428, 3820, 13], "temperature": 0.0, "avg_logprob": -0.12996353822595932, "compression_ratio": 1.6578947368421053, "no_speech_prob": 2.561268502176972e-06}, {"id": 435, "seek": 208334, "start": 2098.6400000000003, "end": 2107.84, "text": " So on my machine, without multiprocessing, this takes about an hour and a half.", "tokens": [407, 322, 452, 3479, 11, 1553, 3311, 340, 780, 278, 11, 341, 2516, 466, 364, 1773, 293, 257, 1922, 13], "temperature": 0.0, "avg_logprob": -0.12996353822595932, "compression_ratio": 1.6578947368421053, "no_speech_prob": 2.561268502176972e-06}, {"id": 436, "seek": 208334, "start": 2107.84, "end": 2111.0, "text": " And with multiprocessing, it takes about 2 minutes.", "tokens": [400, 365, 3311, 340, 780, 278, 11, 309, 2516, 466, 568, 2077, 13], "temperature": 0.0, "avg_logprob": -0.12996353822595932, "compression_ratio": 1.6578947368421053, "no_speech_prob": 2.561268502176972e-06}, {"id": 437, "seek": 211100, "start": 2111.0, "end": 2114.12, "text": " So it's a really handy thing to have.", "tokens": [407, 309, 311, 257, 534, 13239, 551, 281, 362, 13], "temperature": 0.0, "avg_logprob": -0.17474686472039475, "compression_ratio": 1.6283185840707965, "no_speech_prob": 1.0129863767360803e-05}, {"id": 438, "seek": 211100, "start": 2114.12, "end": 2119.04, "text": " And now that this code is here, feel free to look inside it and take advantage of it", "tokens": [400, 586, 300, 341, 3089, 307, 510, 11, 841, 1737, 281, 574, 1854, 309, 293, 747, 5002, 295, 309], "temperature": 0.0, "avg_logprob": -0.17474686472039475, "compression_ratio": 1.6283185840707965, "no_speech_prob": 1.0129863767360803e-05}, {"id": 439, "seek": 211100, "start": 2119.04, "end": 2120.84, "text": " for your own stuff.", "tokens": [337, 428, 1065, 1507, 13], "temperature": 0.0, "avg_logprob": -0.17474686472039475, "compression_ratio": 1.6283185840707965, "no_speech_prob": 1.0129863767360803e-05}, {"id": 440, "seek": 211100, "start": 2120.84, "end": 2128.36, "text": " Remember, we all have multiple cores even in our laptops, and very few things in Python", "tokens": [5459, 11, 321, 439, 362, 3866, 24826, 754, 294, 527, 27642, 11, 293, 588, 1326, 721, 294, 15329], "temperature": 0.0, "avg_logprob": -0.17474686472039475, "compression_ratio": 1.6283185840707965, "no_speech_prob": 1.0129863767360803e-05}, {"id": 441, "seek": 211100, "start": 2128.36, "end": 2135.12, "text": " take advantage of it unless you make an effort to make it work.", "tokens": [747, 5002, 295, 309, 5969, 291, 652, 364, 4630, 281, 652, 309, 589, 13], "temperature": 0.0, "avg_logprob": -0.17474686472039475, "compression_ratio": 1.6283185840707965, "no_speech_prob": 1.0129863767360803e-05}, {"id": 442, "seek": 211100, "start": 2135.12, "end": 2138.84, "text": " So there's a couple of tricks to get things working quickly and reliably.", "tokens": [407, 456, 311, 257, 1916, 295, 11733, 281, 483, 721, 1364, 2661, 293, 49927, 13], "temperature": 0.0, "avg_logprob": -0.17474686472039475, "compression_ratio": 1.6283185840707965, "no_speech_prob": 1.0129863767360803e-05}, {"id": 443, "seek": 213884, "start": 2138.84, "end": 2142.54, "text": " As it runs, it prints out how it's going.", "tokens": [1018, 309, 6676, 11, 309, 22305, 484, 577, 309, 311, 516, 13], "temperature": 0.0, "avg_logprob": -0.20351839567485608, "compression_ratio": 1.6243654822335025, "no_speech_prob": 8.267804332717787e-06}, {"id": 444, "seek": 213884, "start": 2142.54, "end": 2146.7200000000003, "text": " And so here's the result of the end.", "tokens": [400, 370, 510, 311, 264, 1874, 295, 264, 917, 13], "temperature": 0.0, "avg_logprob": -0.20351839567485608, "compression_ratio": 1.6243654822335025, "no_speech_prob": 8.267804332717787e-06}, {"id": 445, "seek": 213884, "start": 2146.7200000000003, "end": 2152.6000000000004, "text": " Beginning of stream token, beginning of field number 1 token, here's the tokenized text.", "tokens": [45705, 295, 4309, 14862, 11, 2863, 295, 2519, 1230, 502, 14862, 11, 510, 311, 264, 14862, 1602, 2487, 13], "temperature": 0.0, "avg_logprob": -0.20351839567485608, "compression_ratio": 1.6243654822335025, "no_speech_prob": 8.267804332717787e-06}, {"id": 446, "seek": 213884, "start": 2152.6000000000004, "end": 2161.2000000000003, "text": " You'll see that the punctuation is on the whole now, a separate token.", "tokens": [509, 603, 536, 300, 264, 27006, 16073, 307, 322, 264, 1379, 586, 11, 257, 4994, 14862, 13], "temperature": 0.0, "avg_logprob": -0.20351839567485608, "compression_ratio": 1.6243654822335025, "no_speech_prob": 8.267804332717787e-06}, {"id": 447, "seek": 213884, "start": 2161.2000000000003, "end": 2163.1200000000003, "text": " You'll see there's a few interesting little things.", "tokens": [509, 603, 536, 456, 311, 257, 1326, 1880, 707, 721, 13], "temperature": 0.0, "avg_logprob": -0.20351839567485608, "compression_ratio": 1.6243654822335025, "no_speech_prob": 8.267804332717787e-06}, {"id": 448, "seek": 213884, "start": 2163.1200000000003, "end": 2164.1200000000003, "text": " One is this.", "tokens": [1485, 307, 341, 13], "temperature": 0.0, "avg_logprob": -0.20351839567485608, "compression_ratio": 1.6243654822335025, "no_speech_prob": 8.267804332717787e-06}, {"id": 449, "seek": 213884, "start": 2164.1200000000003, "end": 2165.1200000000003, "text": " What's this tup?", "tokens": [708, 311, 341, 256, 1010, 30], "temperature": 0.0, "avg_logprob": -0.20351839567485608, "compression_ratio": 1.6243654822335025, "no_speech_prob": 8.267804332717787e-06}, {"id": 450, "seek": 216512, "start": 2165.12, "end": 2172.0, "text": " TupMGM, well MGM obviously was originally capitalized.", "tokens": [314, 1010, 44, 25152, 11, 731, 376, 25152, 2745, 390, 7993, 4238, 1602, 13], "temperature": 0.0, "avg_logprob": -0.21942956107003347, "compression_ratio": 1.63, "no_speech_prob": 8.267837984021753e-06}, {"id": 451, "seek": 216512, "start": 2172.0, "end": 2178.16, "text": " But the interesting thing is that normally people either lowercase everything or they", "tokens": [583, 264, 1880, 551, 307, 300, 5646, 561, 2139, 3126, 9765, 1203, 420, 436], "temperature": 0.0, "avg_logprob": -0.21942956107003347, "compression_ratio": 1.63, "no_speech_prob": 8.267837984021753e-06}, {"id": 452, "seek": 216512, "start": 2178.16, "end": 2179.7599999999998, "text": " leave the case as is.", "tokens": [1856, 264, 1389, 382, 307, 13], "temperature": 0.0, "avg_logprob": -0.21942956107003347, "compression_ratio": 1.63, "no_speech_prob": 8.267837984021753e-06}, {"id": 453, "seek": 216512, "start": 2179.7599999999998, "end": 2189.04, "text": " Now if you leave the case as is, then screw you, all caps, and screw you, lowercase, are", "tokens": [823, 498, 291, 1856, 264, 1389, 382, 307, 11, 550, 5630, 291, 11, 439, 13855, 11, 293, 5630, 291, 11, 3126, 9765, 11, 366], "temperature": 0.0, "avg_logprob": -0.21942956107003347, "compression_ratio": 1.63, "no_speech_prob": 8.267837984021753e-06}, {"id": 454, "seek": 216512, "start": 2189.04, "end": 2193.8599999999997, "text": " two totally different sets of tokens that have to be learned from scratch.", "tokens": [732, 3879, 819, 6352, 295, 22667, 300, 362, 281, 312, 3264, 490, 8459, 13], "temperature": 0.0, "avg_logprob": -0.21942956107003347, "compression_ratio": 1.63, "no_speech_prob": 8.267837984021753e-06}, {"id": 455, "seek": 219386, "start": 2193.86, "end": 2198.6400000000003, "text": " Or if you lowercase them all, then there's no difference at all between screw you and", "tokens": [1610, 498, 291, 3126, 9765, 552, 439, 11, 550, 456, 311, 572, 2649, 412, 439, 1296, 5630, 291, 293], "temperature": 0.0, "avg_logprob": -0.1417429885085748, "compression_ratio": 1.6355555555555557, "no_speech_prob": 1.816219992178958e-06}, {"id": 456, "seek": 219386, "start": 2198.6400000000003, "end": 2201.2400000000002, "text": " screw you.", "tokens": [5630, 291, 13], "temperature": 0.0, "avg_logprob": -0.1417429885085748, "compression_ratio": 1.6355555555555557, "no_speech_prob": 1.816219992178958e-06}, {"id": 457, "seek": 219386, "start": 2201.2400000000002, "end": 2208.7200000000003, "text": " So how do you fix this so that you both get the semantic impact of like, I'm shouting", "tokens": [407, 577, 360, 291, 3191, 341, 370, 300, 291, 1293, 483, 264, 47982, 2712, 295, 411, 11, 286, 478, 20382], "temperature": 0.0, "avg_logprob": -0.1417429885085748, "compression_ratio": 1.6355555555555557, "no_speech_prob": 1.816219992178958e-06}, {"id": 458, "seek": 219386, "start": 2208.7200000000003, "end": 2214.04, "text": " now, but not have every single word have to learn the shouted version versus the normal", "tokens": [586, 11, 457, 406, 362, 633, 2167, 1349, 362, 281, 1466, 264, 37310, 3037, 5717, 264, 2710], "temperature": 0.0, "avg_logprob": -0.1417429885085748, "compression_ratio": 1.6355555555555557, "no_speech_prob": 1.816219992178958e-06}, {"id": 459, "seek": 219386, "start": 2214.04, "end": 2215.08, "text": " version.", "tokens": [3037, 13], "temperature": 0.0, "avg_logprob": -0.1417429885085748, "compression_ratio": 1.6355555555555557, "no_speech_prob": 1.816219992178958e-06}, {"id": 460, "seek": 219386, "start": 2215.08, "end": 2219.44, "text": " And so the idea I came up with, and I'm sure other people have done this too, is to come", "tokens": [400, 370, 264, 1558, 286, 1361, 493, 365, 11, 293, 286, 478, 988, 661, 561, 362, 1096, 341, 886, 11, 307, 281, 808], "temperature": 0.0, "avg_logprob": -0.1417429885085748, "compression_ratio": 1.6355555555555557, "no_speech_prob": 1.816219992178958e-06}, {"id": 461, "seek": 221944, "start": 2219.44, "end": 2224.7200000000003, "text": " up with a unique token to mean the next thing is all uppercase.", "tokens": [493, 365, 257, 3845, 14862, 281, 914, 264, 958, 551, 307, 439, 11775, 2869, 651, 13], "temperature": 0.0, "avg_logprob": -0.10956690979003907, "compression_ratio": 1.82, "no_speech_prob": 4.1573580347176176e-06}, {"id": 462, "seek": 221944, "start": 2224.7200000000003, "end": 2229.0, "text": " So then I lowercase it, so now whatever used to be uppercase is now lowercase, it's just", "tokens": [407, 550, 286, 3126, 9765, 309, 11, 370, 586, 2035, 1143, 281, 312, 11775, 2869, 651, 307, 586, 3126, 9765, 11, 309, 311, 445], "temperature": 0.0, "avg_logprob": -0.10956690979003907, "compression_ratio": 1.82, "no_speech_prob": 4.1573580347176176e-06}, {"id": 463, "seek": 221944, "start": 2229.0, "end": 2234.52, "text": " one token, and then we can learn the semantic meaning of all uppercase.", "tokens": [472, 14862, 11, 293, 550, 321, 393, 1466, 264, 47982, 3620, 295, 439, 11775, 2869, 651, 13], "temperature": 0.0, "avg_logprob": -0.10956690979003907, "compression_ratio": 1.82, "no_speech_prob": 4.1573580347176176e-06}, {"id": 464, "seek": 221944, "start": 2234.52, "end": 2239.08, "text": " And so I've done a similar thing if you've got like 29 exclamation marks in a row, we", "tokens": [400, 370, 286, 600, 1096, 257, 2531, 551, 498, 291, 600, 658, 411, 9413, 1624, 43233, 10640, 294, 257, 5386, 11, 321], "temperature": 0.0, "avg_logprob": -0.10956690979003907, "compression_ratio": 1.82, "no_speech_prob": 4.1573580347176176e-06}, {"id": 465, "seek": 221944, "start": 2239.08, "end": 2242.84, "text": " don't learn a separate token for 29 exclamation marks.", "tokens": [500, 380, 1466, 257, 4994, 14862, 337, 9413, 1624, 43233, 10640, 13], "temperature": 0.0, "avg_logprob": -0.10956690979003907, "compression_ratio": 1.82, "no_speech_prob": 4.1573580347176176e-06}, {"id": 466, "seek": 221944, "start": 2242.84, "end": 2248.08, "text": " Instead I put in a special token for the next thing repeats lots of times, and then I put", "tokens": [7156, 286, 829, 294, 257, 2121, 14862, 337, 264, 958, 551, 35038, 3195, 295, 1413, 11, 293, 550, 286, 829], "temperature": 0.0, "avg_logprob": -0.10956690979003907, "compression_ratio": 1.82, "no_speech_prob": 4.1573580347176176e-06}, {"id": 467, "seek": 224808, "start": 2248.08, "end": 2252.04, "text": " the number 29, and then I put the exclamation mark.", "tokens": [264, 1230, 9413, 11, 293, 550, 286, 829, 264, 1624, 43233, 1491, 13], "temperature": 0.0, "avg_logprob": -0.14110742568969725, "compression_ratio": 1.5482456140350878, "no_speech_prob": 3.5008433769689873e-06}, {"id": 468, "seek": 224808, "start": 2252.04, "end": 2254.08, "text": " And so there's a few little tricks like that.", "tokens": [400, 370, 456, 311, 257, 1326, 707, 11733, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.14110742568969725, "compression_ratio": 1.5482456140350878, "no_speech_prob": 3.5008433769689873e-06}, {"id": 469, "seek": 224808, "start": 2254.08, "end": 2258.7599999999998, "text": " And if you're interested in LP, have a look at the code for tokenizer for these little", "tokens": [400, 498, 291, 434, 3102, 294, 38095, 11, 362, 257, 574, 412, 264, 3089, 337, 14862, 6545, 337, 613, 707], "temperature": 0.0, "avg_logprob": -0.14110742568969725, "compression_ratio": 1.5482456140350878, "no_speech_prob": 3.5008433769689873e-06}, {"id": 470, "seek": 224808, "start": 2258.7599999999998, "end": 2265.48, "text": " tricks that I've added in, because some of them are kind of fun.", "tokens": [11733, 300, 286, 600, 3869, 294, 11, 570, 512, 295, 552, 366, 733, 295, 1019, 13], "temperature": 0.0, "avg_logprob": -0.14110742568969725, "compression_ratio": 1.5482456140350878, "no_speech_prob": 3.5008433769689873e-06}, {"id": 471, "seek": 224808, "start": 2265.48, "end": 2272.2, "text": " So the nice thing with doing things this way is we can now just np.save that and load it", "tokens": [407, 264, 1481, 551, 365, 884, 721, 341, 636, 307, 321, 393, 586, 445, 33808, 13, 82, 946, 300, 293, 3677, 309], "temperature": 0.0, "avg_logprob": -0.14110742568969725, "compression_ratio": 1.5482456140350878, "no_speech_prob": 3.5008433769689873e-06}, {"id": 472, "seek": 224808, "start": 2272.2, "end": 2273.2, "text": " back up later.", "tokens": [646, 493, 1780, 13], "temperature": 0.0, "avg_logprob": -0.14110742568969725, "compression_ratio": 1.5482456140350878, "no_speech_prob": 3.5008433769689873e-06}, {"id": 473, "seek": 227320, "start": 2273.2, "end": 2278.48, "text": " So we don't have to recalculate all this stuff each time like we tend to have to do with", "tokens": [407, 321, 500, 380, 362, 281, 850, 304, 2444, 473, 439, 341, 1507, 1184, 565, 411, 321, 3928, 281, 362, 281, 360, 365], "temperature": 0.0, "avg_logprob": -0.14075335334329045, "compression_ratio": 1.6442687747035574, "no_speech_prob": 6.643405868089758e-06}, {"id": 474, "seek": 227320, "start": 2278.48, "end": 2282.24, "text": " TorchText or a lot of other libraries.", "tokens": [7160, 339, 50198, 420, 257, 688, 295, 661, 15148, 13], "temperature": 0.0, "avg_logprob": -0.14075335334329045, "compression_ratio": 1.6442687747035574, "no_speech_prob": 6.643405868089758e-06}, {"id": 475, "seek": 227320, "start": 2282.24, "end": 2285.24, "text": " So we've now got it tokenized.", "tokens": [407, 321, 600, 586, 658, 309, 14862, 1602, 13], "temperature": 0.0, "avg_logprob": -0.14075335334329045, "compression_ratio": 1.6442687747035574, "no_speech_prob": 6.643405868089758e-06}, {"id": 476, "seek": 227320, "start": 2285.24, "end": 2289.96, "text": " The next thing we need to do is to turn it into numbers, which we call numericalizing", "tokens": [440, 958, 551, 321, 643, 281, 360, 307, 281, 1261, 309, 666, 3547, 11, 597, 321, 818, 29054, 3319], "temperature": 0.0, "avg_logprob": -0.14075335334329045, "compression_ratio": 1.6442687747035574, "no_speech_prob": 6.643405868089758e-06}, {"id": 477, "seek": 227320, "start": 2289.96, "end": 2291.6, "text": " it.", "tokens": [309, 13], "temperature": 0.0, "avg_logprob": -0.14075335334329045, "compression_ratio": 1.6442687747035574, "no_speech_prob": 6.643405868089758e-06}, {"id": 478, "seek": 227320, "start": 2291.6, "end": 2293.7999999999997, "text": " And the way we numericalize it is very simple.", "tokens": [400, 264, 636, 321, 29054, 1125, 309, 307, 588, 2199, 13], "temperature": 0.0, "avg_logprob": -0.14075335334329045, "compression_ratio": 1.6442687747035574, "no_speech_prob": 6.643405868089758e-06}, {"id": 479, "seek": 227320, "start": 2293.7999999999997, "end": 2298.96, "text": " We make a list of all the words that appear in some order, and then we replace every word", "tokens": [492, 652, 257, 1329, 295, 439, 264, 2283, 300, 4204, 294, 512, 1668, 11, 293, 550, 321, 7406, 633, 1349], "temperature": 0.0, "avg_logprob": -0.14075335334329045, "compression_ratio": 1.6442687747035574, "no_speech_prob": 6.643405868089758e-06}, {"id": 480, "seek": 227320, "start": 2298.96, "end": 2301.7999999999997, "text": " with its index into that list.", "tokens": [365, 1080, 8186, 666, 300, 1329, 13], "temperature": 0.0, "avg_logprob": -0.14075335334329045, "compression_ratio": 1.6442687747035574, "no_speech_prob": 6.643405868089758e-06}, {"id": 481, "seek": 230180, "start": 2301.8, "end": 2309.2400000000002, "text": " The list of all the words that appear, or all the tokens that appear, we call the vocabulary.", "tokens": [440, 1329, 295, 439, 264, 2283, 300, 4204, 11, 420, 439, 264, 22667, 300, 4204, 11, 321, 818, 264, 19864, 13], "temperature": 0.0, "avg_logprob": -0.1977760910987854, "compression_ratio": 1.6737967914438503, "no_speech_prob": 5.507587502506794e-06}, {"id": 482, "seek": 230180, "start": 2309.2400000000002, "end": 2311.84, "text": " So here's an example of some of the vocabulary.", "tokens": [407, 510, 311, 364, 1365, 295, 512, 295, 264, 19864, 13], "temperature": 0.0, "avg_logprob": -0.1977760910987854, "compression_ratio": 1.6737967914438503, "no_speech_prob": 5.507587502506794e-06}, {"id": 483, "seek": 230180, "start": 2311.84, "end": 2314.88, "text": " The counter class in Python is very handy for this.", "tokens": [440, 5682, 1508, 294, 15329, 307, 588, 13239, 337, 341, 13], "temperature": 0.0, "avg_logprob": -0.1977760910987854, "compression_ratio": 1.6737967914438503, "no_speech_prob": 5.507587502506794e-06}, {"id": 484, "seek": 230180, "start": 2314.88, "end": 2320.6800000000003, "text": " It basically gives us a list of unique items and their counts.", "tokens": [467, 1936, 2709, 505, 257, 1329, 295, 3845, 4754, 293, 641, 14893, 13], "temperature": 0.0, "avg_logprob": -0.1977760910987854, "compression_ratio": 1.6737967914438503, "no_speech_prob": 5.507587502506794e-06}, {"id": 485, "seek": 230180, "start": 2320.6800000000003, "end": 2326.6800000000003, "text": " So here are the 25 most common things in the vocabulary.", "tokens": [407, 510, 366, 264, 3552, 881, 2689, 721, 294, 264, 19864, 13], "temperature": 0.0, "avg_logprob": -0.1977760910987854, "compression_ratio": 1.6737967914438503, "no_speech_prob": 5.507587502506794e-06}, {"id": 486, "seek": 232668, "start": 2326.68, "end": 2331.8799999999997, "text": " You can see there are things like apostrophe s and double quote and end of paragraph and", "tokens": [509, 393, 536, 456, 366, 721, 411, 19484, 27194, 262, 293, 3834, 6513, 293, 917, 295, 18865, 293], "temperature": 0.0, "avg_logprob": -0.19161662348994501, "compression_ratio": 1.6423076923076922, "no_speech_prob": 3.844921593554318e-06}, {"id": 487, "seek": 232668, "start": 2331.8799999999997, "end": 2334.3999999999996, "text": " stuff like that.", "tokens": [1507, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.19161662348994501, "compression_ratio": 1.6423076923076922, "no_speech_prob": 3.844921593554318e-06}, {"id": 488, "seek": 232668, "start": 2334.3999999999996, "end": 2340.56, "text": " Now generally speaking, we don't want every unique token in our vocabulary.", "tokens": [823, 5101, 4124, 11, 321, 500, 380, 528, 633, 3845, 14862, 294, 527, 19864, 13], "temperature": 0.0, "avg_logprob": -0.19161662348994501, "compression_ratio": 1.6423076923076922, "no_speech_prob": 3.844921593554318e-06}, {"id": 489, "seek": 232668, "start": 2340.56, "end": 2346.04, "text": " If it doesn't appear at least 2 times, then it might just be a spelling mistake or a word.", "tokens": [759, 309, 1177, 380, 4204, 412, 1935, 568, 1413, 11, 550, 309, 1062, 445, 312, 257, 22254, 6146, 420, 257, 1349, 13], "temperature": 0.0, "avg_logprob": -0.19161662348994501, "compression_ratio": 1.6423076923076922, "no_speech_prob": 3.844921593554318e-06}, {"id": 490, "seek": 232668, "start": 2346.04, "end": 2350.2799999999997, "text": " We can't learn anything about it if it doesn't appear that often.", "tokens": [492, 393, 380, 1466, 1340, 466, 309, 498, 309, 1177, 380, 4204, 300, 2049, 13], "temperature": 0.0, "avg_logprob": -0.19161662348994501, "compression_ratio": 1.6423076923076922, "no_speech_prob": 3.844921593554318e-06}, {"id": 491, "seek": 232668, "start": 2350.2799999999997, "end": 2354.7599999999998, "text": " Also the stuff that we're going to be learning about at least so far in this part gets a", "tokens": [2743, 264, 1507, 300, 321, 434, 516, 281, 312, 2539, 466, 412, 1935, 370, 1400, 294, 341, 644, 2170, 257], "temperature": 0.0, "avg_logprob": -0.19161662348994501, "compression_ratio": 1.6423076923076922, "no_speech_prob": 3.844921593554318e-06}, {"id": 492, "seek": 235476, "start": 2354.76, "end": 2359.6400000000003, "text": " bit clunky once you've got a vocabulary bigger than 60,000.", "tokens": [857, 596, 25837, 1564, 291, 600, 658, 257, 19864, 3801, 813, 4060, 11, 1360, 13], "temperature": 0.0, "avg_logprob": -0.14387690536374967, "compression_ratio": 1.6347517730496455, "no_speech_prob": 4.222797542752232e-06}, {"id": 493, "seek": 235476, "start": 2359.6400000000003, "end": 2363.6000000000004, "text": " Time permitting, we may look at some work I've been doing recently on handling larger", "tokens": [6161, 4784, 2414, 11, 321, 815, 574, 412, 512, 589, 286, 600, 668, 884, 3938, 322, 13175, 4833], "temperature": 0.0, "avg_logprob": -0.14387690536374967, "compression_ratio": 1.6347517730496455, "no_speech_prob": 4.222797542752232e-06}, {"id": 494, "seek": 235476, "start": 2363.6000000000004, "end": 2368.6400000000003, "text": " vocabularies, otherwise that might have to come in a future course.", "tokens": [2329, 455, 1040, 530, 11, 5911, 300, 1062, 362, 281, 808, 294, 257, 2027, 1164, 13], "temperature": 0.0, "avg_logprob": -0.14387690536374967, "compression_ratio": 1.6347517730496455, "no_speech_prob": 4.222797542752232e-06}, {"id": 495, "seek": 235476, "start": 2368.6400000000003, "end": 2372.96, "text": " But actually for classification, I've discovered that doing more than about 60,000 words doesn't", "tokens": [583, 767, 337, 21538, 11, 286, 600, 6941, 300, 884, 544, 813, 466, 4060, 11, 1360, 2283, 1177, 380], "temperature": 0.0, "avg_logprob": -0.14387690536374967, "compression_ratio": 1.6347517730496455, "no_speech_prob": 4.222797542752232e-06}, {"id": 496, "seek": 235476, "start": 2372.96, "end": 2374.3, "text": " seem to help anyway.", "tokens": [1643, 281, 854, 4033, 13], "temperature": 0.0, "avg_logprob": -0.14387690536374967, "compression_ratio": 1.6347517730496455, "no_speech_prob": 4.222797542752232e-06}, {"id": 497, "seek": 235476, "start": 2374.3, "end": 2379.1200000000003, "text": " So we're going to limit our vocabulary to 60,000 words, things that appear at least", "tokens": [407, 321, 434, 516, 281, 4948, 527, 19864, 281, 4060, 11, 1360, 2283, 11, 721, 300, 4204, 412, 1935], "temperature": 0.0, "avg_logprob": -0.14387690536374967, "compression_ratio": 1.6347517730496455, "no_speech_prob": 4.222797542752232e-06}, {"id": 498, "seek": 235476, "start": 2379.1200000000003, "end": 2380.38, "text": " twice.", "tokens": [6091, 13], "temperature": 0.0, "avg_logprob": -0.14387690536374967, "compression_ratio": 1.6347517730496455, "no_speech_prob": 4.222797542752232e-06}, {"id": 499, "seek": 235476, "start": 2380.38, "end": 2382.84, "text": " And so here's a simple way to do that.", "tokens": [400, 370, 510, 311, 257, 2199, 636, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.14387690536374967, "compression_ratio": 1.6347517730496455, "no_speech_prob": 4.222797542752232e-06}, {"id": 500, "seek": 238284, "start": 2382.84, "end": 2387.1200000000003, "text": " Use that dot most common, pass in the max vocab size.", "tokens": [8278, 300, 5893, 881, 2689, 11, 1320, 294, 264, 11469, 2329, 455, 2744, 13], "temperature": 0.0, "avg_logprob": -0.21986830234527588, "compression_ratio": 1.5867768595041323, "no_speech_prob": 4.63782089354936e-06}, {"id": 501, "seek": 238284, "start": 2387.1200000000003, "end": 2390.6000000000004, "text": " That'll sort it by the frequency, by the way.", "tokens": [663, 603, 1333, 309, 538, 264, 7893, 11, 538, 264, 636, 13], "temperature": 0.0, "avg_logprob": -0.21986830234527588, "compression_ratio": 1.5867768595041323, "no_speech_prob": 4.63782089354936e-06}, {"id": 502, "seek": 238284, "start": 2390.6000000000004, "end": 2397.06, "text": " And if it appears less often than a minimum frequency, then don't bother with it at all.", "tokens": [400, 498, 309, 7038, 1570, 2049, 813, 257, 7285, 7893, 11, 550, 500, 380, 8677, 365, 309, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.21986830234527588, "compression_ratio": 1.5867768595041323, "no_speech_prob": 4.63782089354936e-06}, {"id": 503, "seek": 238284, "start": 2397.06, "end": 2401.8, "text": " So that gives us I to S, that's the same name that Torch Text used.", "tokens": [407, 300, 2709, 505, 286, 281, 318, 11, 300, 311, 264, 912, 1315, 300, 7160, 339, 18643, 1143, 13], "temperature": 0.0, "avg_logprob": -0.21986830234527588, "compression_ratio": 1.5867768595041323, "no_speech_prob": 4.63782089354936e-06}, {"id": 504, "seek": 238284, "start": 2401.8, "end": 2403.94, "text": " Remember it means int to string.", "tokens": [5459, 309, 1355, 560, 281, 6798, 13], "temperature": 0.0, "avg_logprob": -0.21986830234527588, "compression_ratio": 1.5867768595041323, "no_speech_prob": 4.63782089354936e-06}, {"id": 505, "seek": 238284, "start": 2403.94, "end": 2409.36, "text": " So this is just the list of the unique tokens in the vocab.", "tokens": [407, 341, 307, 445, 264, 1329, 295, 264, 3845, 22667, 294, 264, 2329, 455, 13], "temperature": 0.0, "avg_logprob": -0.21986830234527588, "compression_ratio": 1.5867768595041323, "no_speech_prob": 4.63782089354936e-06}, {"id": 506, "seek": 238284, "start": 2409.36, "end": 2411.8, "text": " I'm going to insert 2 more tokens.", "tokens": [286, 478, 516, 281, 8969, 568, 544, 22667, 13], "temperature": 0.0, "avg_logprob": -0.21986830234527588, "compression_ratio": 1.5867768595041323, "no_speech_prob": 4.63782089354936e-06}, {"id": 507, "seek": 241180, "start": 2411.8, "end": 2420.0, "text": " A token for unknown, a vocab item for unknown, and a vocab item for padding.", "tokens": [316, 14862, 337, 9841, 11, 257, 2329, 455, 3174, 337, 9841, 11, 293, 257, 2329, 455, 3174, 337, 39562, 13], "temperature": 0.0, "avg_logprob": -0.145349246939433, "compression_ratio": 1.6820276497695852, "no_speech_prob": 4.029442607134115e-06}, {"id": 508, "seek": 241180, "start": 2420.0, "end": 2426.84, "text": " Then we can create the dictionary which goes in the opposite direction, so string to int.", "tokens": [1396, 321, 393, 1884, 264, 25890, 597, 1709, 294, 264, 6182, 3513, 11, 370, 6798, 281, 560, 13], "temperature": 0.0, "avg_logprob": -0.145349246939433, "compression_ratio": 1.6820276497695852, "no_speech_prob": 4.029442607134115e-06}, {"id": 509, "seek": 241180, "start": 2426.84, "end": 2432.52, "text": " And that won't cover everything because we intentionally truncated it down to 60,000", "tokens": [400, 300, 1582, 380, 2060, 1203, 570, 321, 22062, 504, 409, 66, 770, 309, 760, 281, 4060, 11, 1360], "temperature": 0.0, "avg_logprob": -0.145349246939433, "compression_ratio": 1.6820276497695852, "no_speech_prob": 4.029442607134115e-06}, {"id": 510, "seek": 241180, "start": 2432.52, "end": 2433.52, "text": " words.", "tokens": [2283, 13], "temperature": 0.0, "avg_logprob": -0.145349246939433, "compression_ratio": 1.6820276497695852, "no_speech_prob": 4.029442607134115e-06}, {"id": 511, "seek": 241180, "start": 2433.52, "end": 2437.0, "text": " And so if we come across something that's not in the dictionary, we want to replace", "tokens": [400, 370, 498, 321, 808, 2108, 746, 300, 311, 406, 294, 264, 25890, 11, 321, 528, 281, 7406], "temperature": 0.0, "avg_logprob": -0.145349246939433, "compression_ratio": 1.6820276497695852, "no_speech_prob": 4.029442607134115e-06}, {"id": 512, "seek": 241180, "start": 2437.0, "end": 2439.0, "text": " it with 0 for unknown.", "tokens": [309, 365, 1958, 337, 9841, 13], "temperature": 0.0, "avg_logprob": -0.145349246939433, "compression_ratio": 1.6820276497695852, "no_speech_prob": 4.029442607134115e-06}, {"id": 513, "seek": 243900, "start": 2439.0, "end": 2446.92, "text": " So we can use a default dict for that with a lambda function that always returns 0.", "tokens": [407, 321, 393, 764, 257, 7576, 12569, 337, 300, 365, 257, 13607, 2445, 300, 1009, 11247, 1958, 13], "temperature": 0.0, "avg_logprob": -0.16284824239796605, "compression_ratio": 1.5625, "no_speech_prob": 3.611974307204946e-06}, {"id": 514, "seek": 243900, "start": 2446.92, "end": 2451.52, "text": " So you can see all these things we're using that keep coming back up.", "tokens": [407, 291, 393, 536, 439, 613, 721, 321, 434, 1228, 300, 1066, 1348, 646, 493, 13], "temperature": 0.0, "avg_logprob": -0.16284824239796605, "compression_ratio": 1.5625, "no_speech_prob": 3.611974307204946e-06}, {"id": 515, "seek": 243900, "start": 2451.52, "end": 2458.76, "text": " So now that we've got our S2I dictionary defined, we can then just call that for every word,", "tokens": [407, 586, 300, 321, 600, 658, 527, 318, 17, 40, 25890, 7642, 11, 321, 393, 550, 445, 818, 300, 337, 633, 1349, 11], "temperature": 0.0, "avg_logprob": -0.16284824239796605, "compression_ratio": 1.5625, "no_speech_prob": 3.611974307204946e-06}, {"id": 516, "seek": 243900, "start": 2458.76, "end": 2460.96, "text": " for every sentence.", "tokens": [337, 633, 8174, 13], "temperature": 0.0, "avg_logprob": -0.16284824239796605, "compression_ratio": 1.5625, "no_speech_prob": 3.611974307204946e-06}, {"id": 517, "seek": 243900, "start": 2460.96, "end": 2466.96, "text": " And so there's our numericalized version, and there it is.", "tokens": [400, 370, 456, 311, 527, 29054, 1602, 3037, 11, 293, 456, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.16284824239796605, "compression_ratio": 1.5625, "no_speech_prob": 3.611974307204946e-06}, {"id": 518, "seek": 246696, "start": 2466.96, "end": 2471.92, "text": " And so of course the nice thing is again, we can save that step as well.", "tokens": [400, 370, 295, 1164, 264, 1481, 551, 307, 797, 11, 321, 393, 3155, 300, 1823, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.1538704546486459, "compression_ratio": 1.5603864734299517, "no_speech_prob": 2.3687923658144427e-06}, {"id": 519, "seek": 246696, "start": 2471.92, "end": 2475.7200000000003, "text": " So each time we get to another step, we can save it.", "tokens": [407, 1184, 565, 321, 483, 281, 1071, 1823, 11, 321, 393, 3155, 309, 13], "temperature": 0.0, "avg_logprob": -0.1538704546486459, "compression_ratio": 1.5603864734299517, "no_speech_prob": 2.3687923658144427e-06}, {"id": 520, "seek": 246696, "start": 2475.7200000000003, "end": 2480.3, "text": " And these are not very big files compared to what you're used to with images.", "tokens": [400, 613, 366, 406, 588, 955, 7098, 5347, 281, 437, 291, 434, 1143, 281, 365, 5267, 13], "temperature": 0.0, "avg_logprob": -0.1538704546486459, "compression_ratio": 1.5603864734299517, "no_speech_prob": 2.3687923658144427e-06}, {"id": 521, "seek": 246696, "start": 2480.3, "end": 2485.18, "text": " Text is generally pretty small.", "tokens": [18643, 307, 5101, 1238, 1359, 13], "temperature": 0.0, "avg_logprob": -0.1538704546486459, "compression_ratio": 1.5603864734299517, "no_speech_prob": 2.3687923658144427e-06}, {"id": 522, "seek": 246696, "start": 2485.18, "end": 2494.56, "text": " Very important to also save that vocabulary, because this list of numbers means nothing", "tokens": [4372, 1021, 281, 611, 3155, 300, 19864, 11, 570, 341, 1329, 295, 3547, 1355, 1825], "temperature": 0.0, "avg_logprob": -0.1538704546486459, "compression_ratio": 1.5603864734299517, "no_speech_prob": 2.3687923658144427e-06}, {"id": 523, "seek": 249456, "start": 2494.56, "end": 2499.96, "text": " unless you know what each number refers to, and that's what I2S tells you.", "tokens": [5969, 291, 458, 437, 1184, 1230, 14942, 281, 11, 293, 300, 311, 437, 286, 17, 50, 5112, 291, 13], "temperature": 0.0, "avg_logprob": -0.11076024066971009, "compression_ratio": 1.4680851063829787, "no_speech_prob": 6.438984655687818e-06}, {"id": 524, "seek": 249456, "start": 2499.96, "end": 2506.88, "text": " So you save those 3 things, and then later on you can load them back up.", "tokens": [407, 291, 3155, 729, 805, 721, 11, 293, 550, 1780, 322, 291, 393, 3677, 552, 646, 493, 13], "temperature": 0.0, "avg_logprob": -0.11076024066971009, "compression_ratio": 1.4680851063829787, "no_speech_prob": 6.438984655687818e-06}, {"id": 525, "seek": 249456, "start": 2506.88, "end": 2520.14, "text": " So now our vocab size is 60,002, and our training language model has 90,000 documents in it.", "tokens": [407, 586, 527, 2329, 455, 2744, 307, 4060, 11, 628, 17, 11, 293, 527, 3097, 2856, 2316, 575, 4289, 11, 1360, 8512, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.11076024066971009, "compression_ratio": 1.4680851063829787, "no_speech_prob": 6.438984655687818e-06}, {"id": 526, "seek": 249456, "start": 2520.14, "end": 2523.6, "text": " So that's the preprocessing you do.", "tokens": [407, 300, 311, 264, 2666, 340, 780, 278, 291, 360, 13], "temperature": 0.0, "avg_logprob": -0.11076024066971009, "compression_ratio": 1.4680851063829787, "no_speech_prob": 6.438984655687818e-06}, {"id": 527, "seek": 252360, "start": 2523.6, "end": 2527.6, "text": " We can probably wrap a little bit more of that in utility functions if we want to, but", "tokens": [492, 393, 1391, 7019, 257, 707, 857, 544, 295, 300, 294, 14877, 6828, 498, 321, 528, 281, 11, 457], "temperature": 0.0, "avg_logprob": -0.15453749689562568, "compression_ratio": 1.5115207373271888, "no_speech_prob": 9.665977813710924e-06}, {"id": 528, "seek": 252360, "start": 2527.6, "end": 2532.88, "text": " it's all pretty straightforward and basically that exact code will work for any dataset", "tokens": [309, 311, 439, 1238, 15325, 293, 1936, 300, 1900, 3089, 486, 589, 337, 604, 28872], "temperature": 0.0, "avg_logprob": -0.15453749689562568, "compression_ratio": 1.5115207373271888, "no_speech_prob": 9.665977813710924e-06}, {"id": 529, "seek": 252360, "start": 2532.88, "end": 2538.2599999999998, "text": " you have once you've got it in that CSV format.", "tokens": [291, 362, 1564, 291, 600, 658, 309, 294, 300, 48814, 7877, 13], "temperature": 0.0, "avg_logprob": -0.15453749689562568, "compression_ratio": 1.5115207373271888, "no_speech_prob": 9.665977813710924e-06}, {"id": 530, "seek": 252360, "start": 2538.2599999999998, "end": 2551.3199999999997, "text": " So here is a kind of a new insight that's not new at all, which is that we'd like to", "tokens": [407, 510, 307, 257, 733, 295, 257, 777, 11269, 300, 311, 406, 777, 412, 439, 11, 597, 307, 300, 321, 1116, 411, 281], "temperature": 0.0, "avg_logprob": -0.15453749689562568, "compression_ratio": 1.5115207373271888, "no_speech_prob": 9.665977813710924e-06}, {"id": 531, "seek": 252360, "start": 2551.3199999999997, "end": 2553.12, "text": " pre-train something.", "tokens": [659, 12, 83, 7146, 746, 13], "temperature": 0.0, "avg_logprob": -0.15453749689562568, "compression_ratio": 1.5115207373271888, "no_speech_prob": 9.665977813710924e-06}, {"id": 532, "seek": 255312, "start": 2553.12, "end": 2560.64, "text": " We know from Lesson 4 that if we pre-train our classifier by first creating a language", "tokens": [492, 458, 490, 18649, 266, 1017, 300, 498, 321, 659, 12, 83, 7146, 527, 1508, 9902, 538, 700, 4084, 257, 2856], "temperature": 0.0, "avg_logprob": -0.18908775420415969, "compression_ratio": 1.4825870646766168, "no_speech_prob": 3.5008404211112065e-06}, {"id": 533, "seek": 255312, "start": 2560.64, "end": 2565.24, "text": " model and then fine-tuning that as a classifier, that was helpful.", "tokens": [2316, 293, 550, 2489, 12, 83, 37726, 300, 382, 257, 1508, 9902, 11, 300, 390, 4961, 13], "temperature": 0.0, "avg_logprob": -0.18908775420415969, "compression_ratio": 1.4825870646766168, "no_speech_prob": 3.5008404211112065e-06}, {"id": 534, "seek": 255312, "start": 2565.24, "end": 2567.7999999999997, "text": " Remember it actually got us a new state-of-the-art result.", "tokens": [5459, 309, 767, 658, 505, 257, 777, 1785, 12, 2670, 12, 3322, 12, 446, 1874, 13], "temperature": 0.0, "avg_logprob": -0.18908775420415969, "compression_ratio": 1.4825870646766168, "no_speech_prob": 3.5008404211112065e-06}, {"id": 535, "seek": 255312, "start": 2567.7999999999997, "end": 2575.8399999999997, "text": " We got the best IMDB classifier result that had ever been published, but quite a bit.", "tokens": [492, 658, 264, 1151, 21463, 27735, 1508, 9902, 1874, 300, 632, 1562, 668, 6572, 11, 457, 1596, 257, 857, 13], "temperature": 0.0, "avg_logprob": -0.18908775420415969, "compression_ratio": 1.4825870646766168, "no_speech_prob": 3.5008404211112065e-06}, {"id": 536, "seek": 257584, "start": 2575.84, "end": 2587.06, "text": " But we're not going far enough though, because IMDB movie reviews are not that different", "tokens": [583, 321, 434, 406, 516, 1400, 1547, 1673, 11, 570, 21463, 27735, 3169, 10229, 366, 406, 300, 819], "temperature": 0.0, "avg_logprob": -0.13806256251548654, "compression_ratio": 1.482233502538071, "no_speech_prob": 1.1544575500010978e-06}, {"id": 537, "seek": 257584, "start": 2587.06, "end": 2594.52, "text": " to any other English document compared to how different they are to a random string", "tokens": [281, 604, 661, 3669, 4166, 5347, 281, 577, 819, 436, 366, 281, 257, 4974, 6798], "temperature": 0.0, "avg_logprob": -0.13806256251548654, "compression_ratio": 1.482233502538071, "no_speech_prob": 1.1544575500010978e-06}, {"id": 538, "seek": 257584, "start": 2594.52, "end": 2597.32, "text": " or even to a Chinese document.", "tokens": [420, 754, 281, 257, 4649, 4166, 13], "temperature": 0.0, "avg_logprob": -0.13806256251548654, "compression_ratio": 1.482233502538071, "no_speech_prob": 1.1544575500010978e-06}, {"id": 539, "seek": 257584, "start": 2597.32, "end": 2605.56, "text": " So just like ImageNet allowed us to train things that recognize stuff that kind of looks", "tokens": [407, 445, 411, 29903, 31890, 4350, 505, 281, 3847, 721, 300, 5521, 1507, 300, 733, 295, 1542], "temperature": 0.0, "avg_logprob": -0.13806256251548654, "compression_ratio": 1.482233502538071, "no_speech_prob": 1.1544575500010978e-06}, {"id": 540, "seek": 260556, "start": 2605.56, "end": 2609.52, "text": " like pictures and we could use it on stuff that has nothing to do with ImageNet like", "tokens": [411, 5242, 293, 321, 727, 764, 309, 322, 1507, 300, 575, 1825, 281, 360, 365, 29903, 31890, 411], "temperature": 0.0, "avg_logprob": -0.11724468231201172, "compression_ratio": 1.5126903553299493, "no_speech_prob": 7.690370580348826e-07}, {"id": 541, "seek": 260556, "start": 2609.52, "end": 2616.7, "text": " satellite images, why don't we train a language model that's just like good at English and", "tokens": [16016, 5267, 11, 983, 500, 380, 321, 3847, 257, 2856, 2316, 300, 311, 445, 411, 665, 412, 3669, 293], "temperature": 0.0, "avg_logprob": -0.11724468231201172, "compression_ratio": 1.5126903553299493, "no_speech_prob": 7.690370580348826e-07}, {"id": 542, "seek": 260556, "start": 2616.7, "end": 2621.16, "text": " then fine-tune it to be good at movie reviews.", "tokens": [550, 2489, 12, 83, 2613, 309, 281, 312, 665, 412, 3169, 10229, 13], "temperature": 0.0, "avg_logprob": -0.11724468231201172, "compression_ratio": 1.5126903553299493, "no_speech_prob": 7.690370580348826e-07}, {"id": 543, "seek": 260556, "start": 2621.16, "end": 2631.68, "text": " So this basic insight led me to try building a language model on Wikipedia.", "tokens": [407, 341, 3875, 11269, 4684, 385, 281, 853, 2390, 257, 2856, 2316, 322, 28999, 13], "temperature": 0.0, "avg_logprob": -0.11724468231201172, "compression_ratio": 1.5126903553299493, "no_speech_prob": 7.690370580348826e-07}, {"id": 544, "seek": 263168, "start": 2631.68, "end": 2641.2, "text": " So my friend Stephen Merrity has already processed Wikipedia, found a subset of nearly the most", "tokens": [407, 452, 1277, 13391, 6124, 81, 507, 575, 1217, 18846, 28999, 11, 1352, 257, 25993, 295, 6217, 264, 881], "temperature": 0.0, "avg_logprob": -0.2002355514034148, "compression_ratio": 1.5598290598290598, "no_speech_prob": 7.690350685152225e-07}, {"id": 545, "seek": 263168, "start": 2641.2, "end": 2647.7999999999997, "text": " of it, but throwing away the stupid little articles, so most of the bigger articles,", "tokens": [295, 309, 11, 457, 10238, 1314, 264, 6631, 707, 11290, 11, 370, 881, 295, 264, 3801, 11290, 11], "temperature": 0.0, "avg_logprob": -0.2002355514034148, "compression_ratio": 1.5598290598290598, "no_speech_prob": 7.690350685152225e-07}, {"id": 546, "seek": 263168, "start": 2647.7999999999997, "end": 2650.52, "text": " and he calls that Wikitext 103.", "tokens": [293, 415, 5498, 300, 23377, 642, 734, 48784, 13], "temperature": 0.0, "avg_logprob": -0.2002355514034148, "compression_ratio": 1.5598290598290598, "no_speech_prob": 7.690350685152225e-07}, {"id": 547, "seek": 263168, "start": 2650.52, "end": 2655.8799999999997, "text": " So I grabbed Wikitext 103 and I trained a language model on it.", "tokens": [407, 286, 18607, 23377, 642, 734, 48784, 293, 286, 8895, 257, 2856, 2316, 322, 309, 13], "temperature": 0.0, "avg_logprob": -0.2002355514034148, "compression_ratio": 1.5598290598290598, "no_speech_prob": 7.690350685152225e-07}, {"id": 548, "seek": 263168, "start": 2655.8799999999997, "end": 2660.72, "text": " And I used exactly the same approach I'm about to show you for training an IMDB language", "tokens": [400, 286, 1143, 2293, 264, 912, 3109, 286, 478, 466, 281, 855, 291, 337, 3097, 364, 21463, 27735, 2856], "temperature": 0.0, "avg_logprob": -0.2002355514034148, "compression_ratio": 1.5598290598290598, "no_speech_prob": 7.690350685152225e-07}, {"id": 549, "seek": 266072, "start": 2660.72, "end": 2667.2799999999997, "text": " model, but instead I trained a Wikitext 103 language model.", "tokens": [2316, 11, 457, 2602, 286, 8895, 257, 23377, 642, 734, 48784, 2856, 2316, 13], "temperature": 0.0, "avg_logprob": -0.119681423657561, "compression_ratio": 1.56875, "no_speech_prob": 2.7264586606179364e-06}, {"id": 550, "seek": 266072, "start": 2667.2799999999997, "end": 2673.7599999999998, "text": " And then I saved it and I've made it available for anybody who wants to use it at this URL.", "tokens": [400, 550, 286, 6624, 309, 293, 286, 600, 1027, 309, 2435, 337, 4472, 567, 2738, 281, 764, 309, 412, 341, 12905, 13], "temperature": 0.0, "avg_logprob": -0.119681423657561, "compression_ratio": 1.56875, "no_speech_prob": 2.7264586606179364e-06}, {"id": 551, "seek": 266072, "start": 2673.7599999999998, "end": 2680.12, "text": " So this is not a URL for Wikitext 103, the documents, this is the Wikitext 103, the language", "tokens": [407, 341, 307, 406, 257, 12905, 337, 23377, 642, 734, 48784, 11, 264, 8512, 11, 341, 307, 264, 23377, 642, 734, 48784, 11, 264, 2856], "temperature": 0.0, "avg_logprob": -0.119681423657561, "compression_ratio": 1.56875, "no_speech_prob": 2.7264586606179364e-06}, {"id": 552, "seek": 266072, "start": 2680.12, "end": 2681.12, "text": " model.", "tokens": [2316, 13], "temperature": 0.0, "avg_logprob": -0.119681423657561, "compression_ratio": 1.56875, "no_speech_prob": 2.7264586606179364e-06}, {"id": 553, "seek": 268112, "start": 2681.12, "end": 2691.6, "text": " So the idea now is let's train an IMDB language model which starts with these words.", "tokens": [407, 264, 1558, 586, 307, 718, 311, 3847, 364, 21463, 27735, 2856, 2316, 597, 3719, 365, 613, 2283, 13], "temperature": 0.0, "avg_logprob": -0.16384716956846176, "compression_ratio": 1.3785310734463276, "no_speech_prob": 7.766887392790522e-06}, {"id": 554, "seek": 268112, "start": 2691.6, "end": 2698.56, "text": " Now hopefully to you folks, this is an extremely obvious, extremely non-controversial idea", "tokens": [823, 4696, 281, 291, 4024, 11, 341, 307, 364, 4664, 6322, 11, 4664, 2107, 12, 9000, 340, 840, 831, 1558], "temperature": 0.0, "avg_logprob": -0.16384716956846176, "compression_ratio": 1.3785310734463276, "no_speech_prob": 7.766887392790522e-06}, {"id": 555, "seek": 268112, "start": 2698.56, "end": 2704.48, "text": " because it's basically what we've done in nearly every class so far.", "tokens": [570, 309, 311, 1936, 437, 321, 600, 1096, 294, 6217, 633, 1508, 370, 1400, 13], "temperature": 0.0, "avg_logprob": -0.16384716956846176, "compression_ratio": 1.3785310734463276, "no_speech_prob": 7.766887392790522e-06}, {"id": 556, "seek": 270448, "start": 2704.48, "end": 2715.04, "text": " But when I first mentioned this to people in the NLP community, I guess June, July of", "tokens": [583, 562, 286, 700, 2835, 341, 281, 561, 294, 264, 426, 45196, 1768, 11, 286, 2041, 6928, 11, 7370, 295], "temperature": 0.0, "avg_logprob": -0.21146733901079964, "compression_ratio": 1.4858490566037736, "no_speech_prob": 5.0147045840276405e-06}, {"id": 557, "seek": 270448, "start": 2715.04, "end": 2720.16, "text": " last year, there couldn't have been less interest.", "tokens": [1036, 1064, 11, 456, 2809, 380, 362, 668, 1570, 1179, 13], "temperature": 0.0, "avg_logprob": -0.21146733901079964, "compression_ratio": 1.4858490566037736, "no_speech_prob": 5.0147045840276405e-06}, {"id": 558, "seek": 270448, "start": 2720.16, "end": 2725.44, "text": " I asked on Twitter, where a lot of the top Twitter researchers are people that I follow", "tokens": [286, 2351, 322, 5794, 11, 689, 257, 688, 295, 264, 1192, 5794, 10309, 366, 561, 300, 286, 1524], "temperature": 0.0, "avg_logprob": -0.21146733901079964, "compression_ratio": 1.4858490566037736, "no_speech_prob": 5.0147045840276405e-06}, {"id": 559, "seek": 270448, "start": 2725.44, "end": 2729.84, "text": " and they follow me back, I was like, hey, what if we pre-trained a general language", "tokens": [293, 436, 1524, 385, 646, 11, 286, 390, 411, 11, 4177, 11, 437, 498, 321, 659, 12, 17227, 2001, 257, 2674, 2856], "temperature": 0.0, "avg_logprob": -0.21146733901079964, "compression_ratio": 1.4858490566037736, "no_speech_prob": 5.0147045840276405e-06}, {"id": 560, "seek": 270448, "start": 2729.84, "end": 2730.84, "text": " model?", "tokens": [2316, 30], "temperature": 0.0, "avg_logprob": -0.21146733901079964, "compression_ratio": 1.4858490566037736, "no_speech_prob": 5.0147045840276405e-06}, {"id": 561, "seek": 273084, "start": 2730.84, "end": 2736.56, "text": " And they're like, no, our language is different, you can't do that, or I don't know why you", "tokens": [400, 436, 434, 411, 11, 572, 11, 527, 2856, 307, 819, 11, 291, 393, 380, 360, 300, 11, 420, 286, 500, 380, 458, 983, 291], "temperature": 0.0, "avg_logprob": -0.27765585736530585, "compression_ratio": 1.5454545454545454, "no_speech_prob": 1.568907464388758e-05}, {"id": 562, "seek": 273084, "start": 2736.56, "end": 2739.56, "text": " would bother anyway.", "tokens": [576, 8677, 4033, 13], "temperature": 0.0, "avg_logprob": -0.27765585736530585, "compression_ratio": 1.5454545454545454, "no_speech_prob": 1.568907464388758e-05}, {"id": 563, "seek": 273084, "start": 2739.56, "end": 2744.2400000000002, "text": " I've talked to people at conferences and they're like, I'm pretty sure people have tried that", "tokens": [286, 600, 2825, 281, 561, 412, 22032, 293, 436, 434, 411, 11, 286, 478, 1238, 988, 561, 362, 3031, 300], "temperature": 0.0, "avg_logprob": -0.27765585736530585, "compression_ratio": 1.5454545454545454, "no_speech_prob": 1.568907464388758e-05}, {"id": 564, "seek": 273084, "start": 2744.2400000000002, "end": 2746.96, "text": " and it's stupid.", "tokens": [293, 309, 311, 6631, 13], "temperature": 0.0, "avg_logprob": -0.27765585736530585, "compression_ratio": 1.5454545454545454, "no_speech_prob": 1.568907464388758e-05}, {"id": 565, "seek": 273084, "start": 2746.96, "end": 2752.88, "text": " There was just this like, I don't know, this weird straight past.", "tokens": [821, 390, 445, 341, 411, 11, 286, 500, 380, 458, 11, 341, 3657, 2997, 1791, 13], "temperature": 0.0, "avg_logprob": -0.27765585736530585, "compression_ratio": 1.5454545454545454, "no_speech_prob": 1.568907464388758e-05}, {"id": 566, "seek": 275288, "start": 2752.88, "end": 2762.36, "text": " And I guess because I am arrogant and it's de-stress, I ignored them even though they", "tokens": [400, 286, 2041, 570, 286, 669, 30467, 293, 309, 311, 368, 12, 372, 735, 11, 286, 19735, 552, 754, 1673, 436], "temperature": 0.0, "avg_logprob": -0.2500545340524593, "compression_ratio": 1.3488372093023255, "no_speech_prob": 9.51614674704615e-06}, {"id": 567, "seek": 275288, "start": 2762.36, "end": 2766.96, "text": " know much more about NLP than I do and just tried it anyway.", "tokens": [458, 709, 544, 466, 426, 45196, 813, 286, 360, 293, 445, 3031, 309, 4033, 13], "temperature": 0.0, "avg_logprob": -0.2500545340524593, "compression_ratio": 1.3488372093023255, "no_speech_prob": 9.51614674704615e-06}, {"id": 568, "seek": 275288, "start": 2766.96, "end": 2770.44, "text": " And let me show you what happened.", "tokens": [400, 718, 385, 855, 291, 437, 2011, 13], "temperature": 0.0, "avg_logprob": -0.2500545340524593, "compression_ratio": 1.3488372093023255, "no_speech_prob": 9.51614674704615e-06}, {"id": 569, "seek": 275288, "start": 2770.44, "end": 2775.1600000000003, "text": " So here's how we do it.", "tokens": [407, 510, 311, 577, 321, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.2500545340524593, "compression_ratio": 1.3488372093023255, "no_speech_prob": 9.51614674704615e-06}, {"id": 570, "seek": 275288, "start": 2775.1600000000003, "end": 2777.6400000000003, "text": " Grab the wiki text models.", "tokens": [20357, 264, 261, 9850, 2487, 5245, 13], "temperature": 0.0, "avg_logprob": -0.2500545340524593, "compression_ratio": 1.3488372093023255, "no_speech_prob": 9.51614674704615e-06}, {"id": 571, "seek": 277764, "start": 2777.64, "end": 2786.8799999999997, "text": " And if you use wget-r, it will actually recursively grab the whole directory.", "tokens": [400, 498, 291, 764, 261, 847, 12, 81, 11, 309, 486, 767, 20560, 3413, 4444, 264, 1379, 21120, 13], "temperature": 0.0, "avg_logprob": -0.13315800586378718, "compression_ratio": 1.582010582010582, "no_speech_prob": 2.4682640287210234e-05}, {"id": 572, "seek": 277764, "start": 2786.8799999999997, "end": 2791.64, "text": " We need to make sure that our language model has exactly the same embedding size, number", "tokens": [492, 643, 281, 652, 988, 300, 527, 2856, 2316, 575, 2293, 264, 912, 12240, 3584, 2744, 11, 1230], "temperature": 0.0, "avg_logprob": -0.13315800586378718, "compression_ratio": 1.582010582010582, "no_speech_prob": 2.4682640287210234e-05}, {"id": 573, "seek": 277764, "start": 2791.64, "end": 2801.44, "text": " of hidden, and number of layers as my wiki text one did.", "tokens": [295, 7633, 11, 293, 1230, 295, 7914, 382, 452, 261, 9850, 2487, 472, 630, 13], "temperature": 0.0, "avg_logprob": -0.13315800586378718, "compression_ratio": 1.582010582010582, "no_speech_prob": 2.4682640287210234e-05}, {"id": 574, "seek": 277764, "start": 2801.44, "end": 2805.04, "text": " So here's our pre-trained path, here's our pre-trained language model path.", "tokens": [407, 510, 311, 527, 659, 12, 17227, 2001, 3100, 11, 510, 311, 527, 659, 12, 17227, 2001, 2856, 2316, 3100, 13], "temperature": 0.0, "avg_logprob": -0.13315800586378718, "compression_ratio": 1.582010582010582, "no_speech_prob": 2.4682640287210234e-05}, {"id": 575, "seek": 280504, "start": 2805.04, "end": 2815.8, "text": " Let's go ahead and torch.load in those weights from the forward wiki text 103 model.", "tokens": [961, 311, 352, 2286, 293, 27822, 13, 2907, 294, 729, 17443, 490, 264, 2128, 261, 9850, 2487, 48784, 2316, 13], "temperature": 0.0, "avg_logprob": -0.16346017620231532, "compression_ratio": 1.528497409326425, "no_speech_prob": 3.844910224870546e-06}, {"id": 576, "seek": 280504, "start": 2815.8, "end": 2824.52, "text": " We don't normally use torch.load, but that's the PyTorch way of grabbing a file.", "tokens": [492, 500, 380, 5646, 764, 27822, 13, 2907, 11, 457, 300, 311, 264, 9953, 51, 284, 339, 636, 295, 23771, 257, 3991, 13], "temperature": 0.0, "avg_logprob": -0.16346017620231532, "compression_ratio": 1.528497409326425, "no_speech_prob": 3.844910224870546e-06}, {"id": 577, "seek": 280504, "start": 2824.52, "end": 2830.56, "text": " It basically gives you a dictionary containing the name of the layer and a tensor of those", "tokens": [467, 1936, 2709, 291, 257, 25890, 19273, 264, 1315, 295, 264, 4583, 293, 257, 40863, 295, 729], "temperature": 0.0, "avg_logprob": -0.16346017620231532, "compression_ratio": 1.528497409326425, "no_speech_prob": 3.844910224870546e-06}, {"id": 578, "seek": 280504, "start": 2830.56, "end": 2834.7599999999998, "text": " weights, or an array of those weights.", "tokens": [17443, 11, 420, 364, 10225, 295, 729, 17443, 13], "temperature": 0.0, "avg_logprob": -0.16346017620231532, "compression_ratio": 1.528497409326425, "no_speech_prob": 3.844910224870546e-06}, {"id": 579, "seek": 283476, "start": 2834.76, "end": 2836.6800000000003, "text": " Now here's the problem.", "tokens": [823, 510, 311, 264, 1154, 13], "temperature": 0.0, "avg_logprob": -0.1242689071817601, "compression_ratio": 1.6331658291457287, "no_speech_prob": 1.0616094186843839e-05}, {"id": 580, "seek": 283476, "start": 2836.6800000000003, "end": 2842.5200000000004, "text": " That wiki text language model was built with a certain vocabulary which was not the same", "tokens": [663, 261, 9850, 2487, 2856, 2316, 390, 3094, 365, 257, 1629, 19864, 597, 390, 406, 264, 912], "temperature": 0.0, "avg_logprob": -0.1242689071817601, "compression_ratio": 1.6331658291457287, "no_speech_prob": 1.0616094186843839e-05}, {"id": 581, "seek": 283476, "start": 2842.5200000000004, "end": 2844.44, "text": " as this one was built on.", "tokens": [382, 341, 472, 390, 3094, 322, 13], "temperature": 0.0, "avg_logprob": -0.1242689071817601, "compression_ratio": 1.6331658291457287, "no_speech_prob": 1.0616094186843839e-05}, {"id": 582, "seek": 283476, "start": 2844.44, "end": 2850.2000000000003, "text": " So my number 40 was not the same as wiki text 103 model's number 40.", "tokens": [407, 452, 1230, 3356, 390, 406, 264, 912, 382, 261, 9850, 2487, 48784, 2316, 311, 1230, 3356, 13], "temperature": 0.0, "avg_logprob": -0.1242689071817601, "compression_ratio": 1.6331658291457287, "no_speech_prob": 1.0616094186843839e-05}, {"id": 583, "seek": 283476, "start": 2850.2000000000003, "end": 2853.0, "text": " So we need to map one to the other.", "tokens": [407, 321, 643, 281, 4471, 472, 281, 264, 661, 13], "temperature": 0.0, "avg_logprob": -0.1242689071817601, "compression_ratio": 1.6331658291457287, "no_speech_prob": 1.0616094186843839e-05}, {"id": 584, "seek": 283476, "start": 2853.0, "end": 2860.84, "text": " That's very, very simple because luckily I saved the I2S for the wiki text vocab.", "tokens": [663, 311, 588, 11, 588, 2199, 570, 22880, 286, 6624, 264, 286, 17, 50, 337, 264, 261, 9850, 2487, 2329, 455, 13], "temperature": 0.0, "avg_logprob": -0.1242689071817601, "compression_ratio": 1.6331658291457287, "no_speech_prob": 1.0616094186843839e-05}, {"id": 585, "seek": 286084, "start": 2860.84, "end": 2868.2000000000003, "text": " So here's the list of what each word is when I trained the wiki text 103 model.", "tokens": [407, 510, 311, 264, 1329, 295, 437, 1184, 1349, 307, 562, 286, 8895, 264, 261, 9850, 2487, 48784, 2316, 13], "temperature": 0.0, "avg_logprob": -0.15545414175306047, "compression_ratio": 1.6008230452674896, "no_speech_prob": 4.289317075745203e-06}, {"id": 586, "seek": 286084, "start": 2868.2000000000003, "end": 2871.96, "text": " And so we can do the same default dict trick to map it in reverse.", "tokens": [400, 370, 321, 393, 360, 264, 912, 7576, 12569, 4282, 281, 4471, 309, 294, 9943, 13], "temperature": 0.0, "avg_logprob": -0.15545414175306047, "compression_ratio": 1.6008230452674896, "no_speech_prob": 4.289317075745203e-06}, {"id": 587, "seek": 286084, "start": 2871.96, "end": 2877.76, "text": " I'm going to use minus 1 to mean that it's not in the wiki text dictionary.", "tokens": [286, 478, 516, 281, 764, 3175, 502, 281, 914, 300, 309, 311, 406, 294, 264, 261, 9850, 2487, 25890, 13], "temperature": 0.0, "avg_logprob": -0.15545414175306047, "compression_ratio": 1.6008230452674896, "no_speech_prob": 4.289317075745203e-06}, {"id": 588, "seek": 286084, "start": 2877.76, "end": 2885.04, "text": " And so now I can just say, okay, my new set of weights is just a whole bunch of zeros", "tokens": [400, 370, 586, 286, 393, 445, 584, 11, 1392, 11, 452, 777, 992, 295, 17443, 307, 445, 257, 1379, 3840, 295, 35193], "temperature": 0.0, "avg_logprob": -0.15545414175306047, "compression_ratio": 1.6008230452674896, "no_speech_prob": 4.289317075745203e-06}, {"id": 589, "seek": 286084, "start": 2885.04, "end": 2887.8, "text": " with vocab size by embedding size.", "tokens": [365, 2329, 455, 2744, 538, 12240, 3584, 2744, 13], "temperature": 0.0, "avg_logprob": -0.15545414175306047, "compression_ratio": 1.6008230452674896, "no_speech_prob": 4.289317075745203e-06}, {"id": 590, "seek": 286084, "start": 2887.8, "end": 2890.2400000000002, "text": " So we're going to create an embedding matrix.", "tokens": [407, 321, 434, 516, 281, 1884, 364, 12240, 3584, 8141, 13], "temperature": 0.0, "avg_logprob": -0.15545414175306047, "compression_ratio": 1.6008230452674896, "no_speech_prob": 4.289317075745203e-06}, {"id": 591, "seek": 289024, "start": 2890.24, "end": 2896.52, "text": " I'm then going to go through every one of the words in my IMDB vocabulary.", "tokens": [286, 478, 550, 516, 281, 352, 807, 633, 472, 295, 264, 2283, 294, 452, 21463, 27735, 19864, 13], "temperature": 0.0, "avg_logprob": -0.1569011922468219, "compression_ratio": 1.6551724137931034, "no_speech_prob": 4.092895323992707e-06}, {"id": 592, "seek": 289024, "start": 2896.52, "end": 2903.6, "text": " I'm going to look it up in S to I2, so string to int for the wiki text 103 vocabulary, and", "tokens": [286, 478, 516, 281, 574, 309, 493, 294, 318, 281, 286, 17, 11, 370, 6798, 281, 560, 337, 264, 261, 9850, 2487, 48784, 19864, 11, 293], "temperature": 0.0, "avg_logprob": -0.1569011922468219, "compression_ratio": 1.6551724137931034, "no_speech_prob": 4.092895323992707e-06}, {"id": 593, "seek": 289024, "start": 2903.6, "end": 2906.4399999999996, "text": " see if that word's there.", "tokens": [536, 498, 300, 1349, 311, 456, 13], "temperature": 0.0, "avg_logprob": -0.1569011922468219, "compression_ratio": 1.6551724137931034, "no_speech_prob": 4.092895323992707e-06}, {"id": 594, "seek": 289024, "start": 2906.4399999999996, "end": 2911.54, "text": " And if that is word there, then I'm not going to get this minus 1, so R will be greater", "tokens": [400, 498, 300, 307, 1349, 456, 11, 550, 286, 478, 406, 516, 281, 483, 341, 3175, 502, 11, 370, 497, 486, 312, 5044], "temperature": 0.0, "avg_logprob": -0.1569011922468219, "compression_ratio": 1.6551724137931034, "no_speech_prob": 4.092895323992707e-06}, {"id": 595, "seek": 289024, "start": 2911.54, "end": 2912.7999999999997, "text": " than or equal to 0.", "tokens": [813, 420, 2681, 281, 1958, 13], "temperature": 0.0, "avg_logprob": -0.1569011922468219, "compression_ratio": 1.6551724137931034, "no_speech_prob": 4.092895323992707e-06}, {"id": 596, "seek": 289024, "start": 2912.7999999999997, "end": 2917.3199999999997, "text": " So in that case, I will just set that row of the embedding matrix to the weight that", "tokens": [407, 294, 300, 1389, 11, 286, 486, 445, 992, 300, 5386, 295, 264, 12240, 3584, 8141, 281, 264, 3364, 300], "temperature": 0.0, "avg_logprob": -0.1569011922468219, "compression_ratio": 1.6551724137931034, "no_speech_prob": 4.092895323992707e-06}, {"id": 597, "seek": 291732, "start": 2917.32, "end": 2924.84, "text": " I just looked at, which was stored inside this named element.", "tokens": [286, 445, 2956, 412, 11, 597, 390, 12187, 1854, 341, 4926, 4478, 13], "temperature": 0.0, "avg_logprob": -0.10835031531322961, "compression_ratio": 1.6310679611650485, "no_speech_prob": 3.4465665521565825e-06}, {"id": 598, "seek": 291732, "start": 2924.84, "end": 2930.36, "text": " And so these names, you can just look at this dictionary and it's pretty obvious what each", "tokens": [400, 370, 613, 5288, 11, 291, 393, 445, 574, 412, 341, 25890, 293, 309, 311, 1238, 6322, 437, 1184], "temperature": 0.0, "avg_logprob": -0.10835031531322961, "compression_ratio": 1.6310679611650485, "no_speech_prob": 3.4465665521565825e-06}, {"id": 599, "seek": 291732, "start": 2930.36, "end": 2934.36, "text": " name corresponds to because it looks very similar to the names that you gave it when", "tokens": [1315, 23249, 281, 570, 309, 1542, 588, 2531, 281, 264, 5288, 300, 291, 2729, 309, 562], "temperature": 0.0, "avg_logprob": -0.10835031531322961, "compression_ratio": 1.6310679611650485, "no_speech_prob": 3.4465665521565825e-06}, {"id": 600, "seek": 291732, "start": 2934.36, "end": 2935.48, "text": " you set up your module.", "tokens": [291, 992, 493, 428, 10088, 13], "temperature": 0.0, "avg_logprob": -0.10835031531322961, "compression_ratio": 1.6310679611650485, "no_speech_prob": 3.4465665521565825e-06}, {"id": 601, "seek": 291732, "start": 2935.48, "end": 2941.28, "text": " So here are the encoder weights.", "tokens": [407, 510, 366, 264, 2058, 19866, 17443, 13], "temperature": 0.0, "avg_logprob": -0.10835031531322961, "compression_ratio": 1.6310679611650485, "no_speech_prob": 3.4465665521565825e-06}, {"id": 602, "seek": 291732, "start": 2941.28, "end": 2943.2400000000002, "text": " So I'll grab it from the encoder weights.", "tokens": [407, 286, 603, 4444, 309, 490, 264, 2058, 19866, 17443, 13], "temperature": 0.0, "avg_logprob": -0.10835031531322961, "compression_ratio": 1.6310679611650485, "no_speech_prob": 3.4465665521565825e-06}, {"id": 603, "seek": 294324, "start": 2943.24, "end": 2947.9199999999996, "text": " If I don't find it, then I will use the row mean.", "tokens": [759, 286, 500, 380, 915, 309, 11, 550, 286, 486, 764, 264, 5386, 914, 13], "temperature": 0.0, "avg_logprob": -0.15035691307586374, "compression_ratio": 1.69377990430622, "no_speech_prob": 2.2603148863709066e-06}, {"id": 604, "seek": 294324, "start": 2947.9199999999996, "end": 2955.58, "text": " In other words, here is the average embedding weight across all of the wiki text 103 things.", "tokens": [682, 661, 2283, 11, 510, 307, 264, 4274, 12240, 3584, 3364, 2108, 439, 295, 264, 261, 9850, 2487, 48784, 721, 13], "temperature": 0.0, "avg_logprob": -0.15035691307586374, "compression_ratio": 1.69377990430622, "no_speech_prob": 2.2603148863709066e-06}, {"id": 605, "seek": 294324, "start": 2955.58, "end": 2957.72, "text": " So that's pretty simple.", "tokens": [407, 300, 311, 1238, 2199, 13], "temperature": 0.0, "avg_logprob": -0.15035691307586374, "compression_ratio": 1.69377990430622, "no_speech_prob": 2.2603148863709066e-06}, {"id": 606, "seek": 294324, "start": 2957.72, "end": 2962.9199999999996, "text": " So I'm going to end up with an embedding matrix for every word that's in both my vocabulary", "tokens": [407, 286, 478, 516, 281, 917, 493, 365, 364, 12240, 3584, 8141, 337, 633, 1349, 300, 311, 294, 1293, 452, 19864], "temperature": 0.0, "avg_logprob": -0.15035691307586374, "compression_ratio": 1.69377990430622, "no_speech_prob": 2.2603148863709066e-06}, {"id": 607, "seek": 294324, "start": 2962.9199999999996, "end": 2965.4799999999996, "text": " for IMDB and the wiki text 103 vocab.", "tokens": [337, 21463, 27735, 293, 264, 261, 9850, 2487, 48784, 2329, 455, 13], "temperature": 0.0, "avg_logprob": -0.15035691307586374, "compression_ratio": 1.69377990430622, "no_speech_prob": 2.2603148863709066e-06}, {"id": 608, "seek": 294324, "start": 2965.4799999999996, "end": 2969.3199999999997, "text": " I will use the wiki text 103's embedding matrix weights.", "tokens": [286, 486, 764, 264, 261, 9850, 2487, 48784, 311, 12240, 3584, 8141, 17443, 13], "temperature": 0.0, "avg_logprob": -0.15035691307586374, "compression_ratio": 1.69377990430622, "no_speech_prob": 2.2603148863709066e-06}, {"id": 609, "seek": 296932, "start": 2969.32, "end": 2973.92, "text": " For anything else, I will just use whatever was the average weight from the wiki text", "tokens": [1171, 1340, 1646, 11, 286, 486, 445, 764, 2035, 390, 264, 4274, 3364, 490, 264, 261, 9850, 2487], "temperature": 0.0, "avg_logprob": -0.1600353312942217, "compression_ratio": 1.583657587548638, "no_speech_prob": 1.8162152173317736e-06}, {"id": 610, "seek": 296932, "start": 2973.92, "end": 2976.1600000000003, "text": " 103 embedding matrix.", "tokens": [48784, 12240, 3584, 8141, 13], "temperature": 0.0, "avg_logprob": -0.1600353312942217, "compression_ratio": 1.583657587548638, "no_speech_prob": 1.8162152173317736e-06}, {"id": 611, "seek": 296932, "start": 2976.1600000000003, "end": 2983.6400000000003, "text": " And then I'll go ahead and replace the encoder weights with that turned into a tensor.", "tokens": [400, 550, 286, 603, 352, 2286, 293, 7406, 264, 2058, 19866, 17443, 365, 300, 3574, 666, 257, 40863, 13], "temperature": 0.0, "avg_logprob": -0.1600353312942217, "compression_ratio": 1.583657587548638, "no_speech_prob": 1.8162152173317736e-06}, {"id": 612, "seek": 296932, "start": 2983.6400000000003, "end": 2986.6400000000003, "text": " We haven't talked much about weight tying, we might do so later.", "tokens": [492, 2378, 380, 2825, 709, 466, 3364, 32405, 11, 321, 1062, 360, 370, 1780, 13], "temperature": 0.0, "avg_logprob": -0.1600353312942217, "compression_ratio": 1.583657587548638, "no_speech_prob": 1.8162152173317736e-06}, {"id": 613, "seek": 296932, "start": 2986.6400000000003, "end": 2994.56, "text": " But basically the decoder, so the thing that turns the final prediction back into a word,", "tokens": [583, 1936, 264, 979, 19866, 11, 370, 264, 551, 300, 4523, 264, 2572, 17630, 646, 666, 257, 1349, 11], "temperature": 0.0, "avg_logprob": -0.1600353312942217, "compression_ratio": 1.583657587548638, "no_speech_prob": 1.8162152173317736e-06}, {"id": 614, "seek": 296932, "start": 2994.56, "end": 2996.42, "text": " uses exactly the same weights.", "tokens": [4960, 2293, 264, 912, 17443, 13], "temperature": 0.0, "avg_logprob": -0.1600353312942217, "compression_ratio": 1.583657587548638, "no_speech_prob": 1.8162152173317736e-06}, {"id": 615, "seek": 296932, "start": 2996.42, "end": 2999.04, "text": " So I pop it there as well.", "tokens": [407, 286, 1665, 309, 456, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.1600353312942217, "compression_ratio": 1.583657587548638, "no_speech_prob": 1.8162152173317736e-06}, {"id": 616, "seek": 299904, "start": 2999.04, "end": 3003.24, "text": " And then there's a bit of a weird thing with how we do embedding dropout that ends up with", "tokens": [400, 550, 456, 311, 257, 857, 295, 257, 3657, 551, 365, 577, 321, 360, 12240, 3584, 3270, 346, 300, 5314, 493, 365], "temperature": 0.0, "avg_logprob": -0.12396930291400693, "compression_ratio": 1.691449814126394, "no_speech_prob": 7.646474841749296e-06}, {"id": 617, "seek": 299904, "start": 3003.24, "end": 3007.08, "text": " a whole separate copy of them for a reason that doesn't matter much.", "tokens": [257, 1379, 4994, 5055, 295, 552, 337, 257, 1778, 300, 1177, 380, 1871, 709, 13], "temperature": 0.0, "avg_logprob": -0.12396930291400693, "compression_ratio": 1.691449814126394, "no_speech_prob": 7.646474841749296e-06}, {"id": 618, "seek": 299904, "start": 3007.08, "end": 3010.02, "text": " So we just pop the weights back where they need to go.", "tokens": [407, 321, 445, 1665, 264, 17443, 646, 689, 436, 643, 281, 352, 13], "temperature": 0.0, "avg_logprob": -0.12396930291400693, "compression_ratio": 1.691449814126394, "no_speech_prob": 7.646474841749296e-06}, {"id": 619, "seek": 299904, "start": 3010.02, "end": 3016.96, "text": " So this is now something that a dictionary we can now, or a set of torch state which", "tokens": [407, 341, 307, 586, 746, 300, 257, 25890, 321, 393, 586, 11, 420, 257, 992, 295, 27822, 1785, 597], "temperature": 0.0, "avg_logprob": -0.12396930291400693, "compression_ratio": 1.691449814126394, "no_speech_prob": 7.646474841749296e-06}, {"id": 620, "seek": 299904, "start": 3016.96, "end": 3019.48, "text": " we can load in.", "tokens": [321, 393, 3677, 294, 13], "temperature": 0.0, "avg_logprob": -0.12396930291400693, "compression_ratio": 1.691449814126394, "no_speech_prob": 7.646474841749296e-06}, {"id": 621, "seek": 299904, "start": 3019.48, "end": 3024.08, "text": " So let's go ahead and create our language model.", "tokens": [407, 718, 311, 352, 2286, 293, 1884, 527, 2856, 2316, 13], "temperature": 0.0, "avg_logprob": -0.12396930291400693, "compression_ratio": 1.691449814126394, "no_speech_prob": 7.646474841749296e-06}, {"id": 622, "seek": 299904, "start": 3024.08, "end": 3027.4, "text": " And so the basic approach we're going to use, and I'm going to look at this in more detail", "tokens": [400, 370, 264, 3875, 3109, 321, 434, 516, 281, 764, 11, 293, 286, 478, 516, 281, 574, 412, 341, 294, 544, 2607], "temperature": 0.0, "avg_logprob": -0.12396930291400693, "compression_ratio": 1.691449814126394, "no_speech_prob": 7.646474841749296e-06}, {"id": 623, "seek": 302740, "start": 3027.4, "end": 3041.32, "text": " in a moment, is I'm going to concatenate all of the documents together into a single list", "tokens": [294, 257, 1623, 11, 307, 286, 478, 516, 281, 1588, 7186, 473, 439, 295, 264, 8512, 1214, 666, 257, 2167, 1329], "temperature": 0.0, "avg_logprob": -0.11704895947430585, "compression_ratio": 1.5632183908045978, "no_speech_prob": 3.8449156818387564e-06}, {"id": 624, "seek": 302740, "start": 3041.32, "end": 3047.28, "text": " of tokens of length 24.998 million.", "tokens": [295, 22667, 295, 4641, 4022, 13, 8494, 23, 2459, 13], "temperature": 0.0, "avg_logprob": -0.11704895947430585, "compression_ratio": 1.5632183908045978, "no_speech_prob": 3.8449156818387564e-06}, {"id": 625, "seek": 302740, "start": 3047.28, "end": 3052.38, "text": " So that's going to be what I pass in as my training set.", "tokens": [407, 300, 311, 516, 281, 312, 437, 286, 1320, 294, 382, 452, 3097, 992, 13], "temperature": 0.0, "avg_logprob": -0.11704895947430585, "compression_ratio": 1.5632183908045978, "no_speech_prob": 3.8449156818387564e-06}, {"id": 626, "seek": 302740, "start": 3052.38, "end": 3057.04, "text": " So the language model, we basically just take all our documents and just concatenate them", "tokens": [407, 264, 2856, 2316, 11, 321, 1936, 445, 747, 439, 527, 8512, 293, 445, 1588, 7186, 473, 552], "temperature": 0.0, "avg_logprob": -0.11704895947430585, "compression_ratio": 1.5632183908045978, "no_speech_prob": 3.8449156818387564e-06}, {"id": 627, "seek": 305704, "start": 3057.04, "end": 3058.04, "text": " back to back.", "tokens": [646, 281, 646, 13], "temperature": 0.0, "avg_logprob": -0.17944572318313468, "compression_ratio": 1.7873303167420815, "no_speech_prob": 8.530241757398471e-06}, {"id": 628, "seek": 305704, "start": 3058.04, "end": 3062.24, "text": " And we're going to be continuously trying to predict what's the next word after these", "tokens": [400, 321, 434, 516, 281, 312, 15684, 1382, 281, 6069, 437, 311, 264, 958, 1349, 934, 613], "temperature": 0.0, "avg_logprob": -0.17944572318313468, "compression_ratio": 1.7873303167420815, "no_speech_prob": 8.530241757398471e-06}, {"id": 629, "seek": 305704, "start": 3062.24, "end": 3063.24, "text": " words.", "tokens": [2283, 13], "temperature": 0.0, "avg_logprob": -0.17944572318313468, "compression_ratio": 1.7873303167420815, "no_speech_prob": 8.530241757398471e-06}, {"id": 630, "seek": 305704, "start": 3063.24, "end": 3064.92, "text": " What's the next word after these words.", "tokens": [708, 311, 264, 958, 1349, 934, 613, 2283, 13], "temperature": 0.0, "avg_logprob": -0.17944572318313468, "compression_ratio": 1.7873303167420815, "no_speech_prob": 8.530241757398471e-06}, {"id": 631, "seek": 305704, "start": 3064.92, "end": 3067.64, "text": " And we'll look at these details in a moment.", "tokens": [400, 321, 603, 574, 412, 613, 4365, 294, 257, 1623, 13], "temperature": 0.0, "avg_logprob": -0.17944572318313468, "compression_ratio": 1.7873303167420815, "no_speech_prob": 8.530241757398471e-06}, {"id": 632, "seek": 305704, "start": 3067.64, "end": 3073.56, "text": " I'm going to set up a whole bunch of dropout.", "tokens": [286, 478, 516, 281, 992, 493, 257, 1379, 3840, 295, 3270, 346, 13], "temperature": 0.0, "avg_logprob": -0.17944572318313468, "compression_ratio": 1.7873303167420815, "no_speech_prob": 8.530241757398471e-06}, {"id": 633, "seek": 305704, "start": 3073.56, "end": 3077.56, "text": " Once we've got a model data object, we can then grab the model from it.", "tokens": [3443, 321, 600, 658, 257, 2316, 1412, 2657, 11, 321, 393, 550, 4444, 264, 2316, 490, 309, 13], "temperature": 0.0, "avg_logprob": -0.17944572318313468, "compression_ratio": 1.7873303167420815, "no_speech_prob": 8.530241757398471e-06}, {"id": 634, "seek": 305704, "start": 3077.56, "end": 3080.8, "text": " So that's going to give us a learner.", "tokens": [407, 300, 311, 516, 281, 976, 505, 257, 33347, 13], "temperature": 0.0, "avg_logprob": -0.17944572318313468, "compression_ratio": 1.7873303167420815, "no_speech_prob": 8.530241757398471e-06}, {"id": 635, "seek": 305704, "start": 3080.8, "end": 3086.74, "text": " And then as per usual, we can call learner.fit.", "tokens": [400, 550, 382, 680, 7713, 11, 321, 393, 818, 33347, 13, 6845, 13], "temperature": 0.0, "avg_logprob": -0.17944572318313468, "compression_ratio": 1.7873303167420815, "no_speech_prob": 8.530241757398471e-06}, {"id": 636, "seek": 308674, "start": 3086.74, "end": 3093.3599999999997, "text": " So we first of all, as per usual, just do a single epoch on the last layer just to get", "tokens": [407, 321, 700, 295, 439, 11, 382, 680, 7713, 11, 445, 360, 257, 2167, 30992, 339, 322, 264, 1036, 4583, 445, 281, 483], "temperature": 0.0, "avg_logprob": -0.09694321193392315, "compression_ratio": 1.883817427385892, "no_speech_prob": 2.5215647383447504e-06}, {"id": 637, "seek": 308674, "start": 3093.3599999999997, "end": 3094.3599999999997, "text": " that okay.", "tokens": [300, 1392, 13], "temperature": 0.0, "avg_logprob": -0.09694321193392315, "compression_ratio": 1.883817427385892, "no_speech_prob": 2.5215647383447504e-06}, {"id": 638, "seek": 308674, "start": 3094.3599999999997, "end": 3098.5, "text": " And the way I've set it up is the last layer is actually the embedding weights.", "tokens": [400, 264, 636, 286, 600, 992, 309, 493, 307, 264, 1036, 4583, 307, 767, 264, 12240, 3584, 17443, 13], "temperature": 0.0, "avg_logprob": -0.09694321193392315, "compression_ratio": 1.883817427385892, "no_speech_prob": 2.5215647383447504e-06}, {"id": 639, "seek": 308674, "start": 3098.5, "end": 3101.3799999999997, "text": " Because that's obviously the thing that's going to be the most wrong, because a lot", "tokens": [1436, 300, 311, 2745, 264, 551, 300, 311, 516, 281, 312, 264, 881, 2085, 11, 570, 257, 688], "temperature": 0.0, "avg_logprob": -0.09694321193392315, "compression_ratio": 1.883817427385892, "no_speech_prob": 2.5215647383447504e-06}, {"id": 640, "seek": 308674, "start": 3101.3799999999997, "end": 3105.14, "text": " of those embedding weights didn't even exist in the vocab.", "tokens": [295, 729, 12240, 3584, 17443, 994, 380, 754, 2514, 294, 264, 2329, 455, 13], "temperature": 0.0, "avg_logprob": -0.09694321193392315, "compression_ratio": 1.883817427385892, "no_speech_prob": 2.5215647383447504e-06}, {"id": 641, "seek": 308674, "start": 3105.14, "end": 3109.8999999999996, "text": " So we're just going to train a single epoch of just the embedding weights.", "tokens": [407, 321, 434, 445, 516, 281, 3847, 257, 2167, 30992, 339, 295, 445, 264, 12240, 3584, 17443, 13], "temperature": 0.0, "avg_logprob": -0.09694321193392315, "compression_ratio": 1.883817427385892, "no_speech_prob": 2.5215647383447504e-06}, {"id": 642, "seek": 308674, "start": 3109.8999999999996, "end": 3114.22, "text": " And then we'll start doing a few epochs of the full model.", "tokens": [400, 550, 321, 603, 722, 884, 257, 1326, 30992, 28346, 295, 264, 1577, 2316, 13], "temperature": 0.0, "avg_logprob": -0.09694321193392315, "compression_ratio": 1.883817427385892, "no_speech_prob": 2.5215647383447504e-06}, {"id": 643, "seek": 311422, "start": 3114.22, "end": 3117.9199999999996, "text": " And so how is that looking?", "tokens": [400, 370, 577, 307, 300, 1237, 30], "temperature": 0.0, "avg_logprob": -0.12531182805045707, "compression_ratio": 1.3432835820895523, "no_speech_prob": 1.1911052979485248e-06}, {"id": 644, "seek": 311422, "start": 3117.9199999999996, "end": 3125.2999999999997, "text": " Well here's lesson 4, which was our academic world's best ever result.", "tokens": [1042, 510, 311, 6898, 1017, 11, 597, 390, 527, 7778, 1002, 311, 1151, 1562, 1874, 13], "temperature": 0.0, "avg_logprob": -0.12531182805045707, "compression_ratio": 1.3432835820895523, "no_speech_prob": 1.1911052979485248e-06}, {"id": 645, "seek": 311422, "start": 3125.2999999999997, "end": 3135.3199999999997, "text": " And after 14 epochs, we had a 4.23 loss.", "tokens": [400, 934, 3499, 30992, 28346, 11, 321, 632, 257, 1017, 13, 9356, 4470, 13], "temperature": 0.0, "avg_logprob": -0.12531182805045707, "compression_ratio": 1.3432835820895523, "no_speech_prob": 1.1911052979485248e-06}, {"id": 646, "seek": 311422, "start": 3135.3199999999997, "end": 3139.8399999999997, "text": " Here after 1 epoch, we have a 4.12 loss.", "tokens": [1692, 934, 502, 30992, 339, 11, 321, 362, 257, 1017, 13, 4762, 4470, 13], "temperature": 0.0, "avg_logprob": -0.12531182805045707, "compression_ratio": 1.3432835820895523, "no_speech_prob": 1.1911052979485248e-06}, {"id": 647, "seek": 313984, "start": 3139.84, "end": 3146.76, "text": " So by pre-training on Wikitext 103, in fact let's go and have a look, we kept training", "tokens": [407, 538, 659, 12, 17227, 1760, 322, 23377, 642, 734, 48784, 11, 294, 1186, 718, 311, 352, 293, 362, 257, 574, 11, 321, 4305, 3097], "temperature": 0.0, "avg_logprob": -0.18663335920454147, "compression_ratio": 1.6018518518518519, "no_speech_prob": 3.2887344332266366e-06}, {"id": 648, "seek": 313984, "start": 3146.76, "end": 3150.76, "text": " and training at a different rate, eventually we got to 4.16.", "tokens": [293, 3097, 412, 257, 819, 3314, 11, 4728, 321, 658, 281, 1017, 13, 6866, 13], "temperature": 0.0, "avg_logprob": -0.18663335920454147, "compression_ratio": 1.6018518518518519, "no_speech_prob": 3.2887344332266366e-06}, {"id": 649, "seek": 313984, "start": 3150.76, "end": 3156.4, "text": " So by pre-training on Wikitext 103, we have a better loss after 1 epoch than the best", "tokens": [407, 538, 659, 12, 17227, 1760, 322, 23377, 642, 734, 48784, 11, 321, 362, 257, 1101, 4470, 934, 502, 30992, 339, 813, 264, 1151], "temperature": 0.0, "avg_logprob": -0.18663335920454147, "compression_ratio": 1.6018518518518519, "no_speech_prob": 3.2887344332266366e-06}, {"id": 650, "seek": 313984, "start": 3156.4, "end": 3159.76, "text": " loss we got for the language model otherwise.", "tokens": [4470, 321, 658, 337, 264, 2856, 2316, 5911, 13], "temperature": 0.0, "avg_logprob": -0.18663335920454147, "compression_ratio": 1.6018518518518519, "no_speech_prob": 3.2887344332266366e-06}, {"id": 651, "seek": 313984, "start": 3159.76, "end": 3162.2400000000002, "text": " Yes, Rachel.", "tokens": [1079, 11, 14246, 13], "temperature": 0.0, "avg_logprob": -0.18663335920454147, "compression_ratio": 1.6018518518518519, "no_speech_prob": 3.2887344332266366e-06}, {"id": 652, "seek": 313984, "start": 3162.2400000000002, "end": 3164.88, "text": " What is the Wikitext 103 model?", "tokens": [708, 307, 264, 23377, 642, 734, 48784, 2316, 30], "temperature": 0.0, "avg_logprob": -0.18663335920454147, "compression_ratio": 1.6018518518518519, "no_speech_prob": 3.2887344332266366e-06}, {"id": 653, "seek": 313984, "start": 3164.88, "end": 3167.76, "text": " Is it AWD LSTM again?", "tokens": [1119, 309, 25815, 35, 441, 6840, 44, 797, 30], "temperature": 0.0, "avg_logprob": -0.18663335920454147, "compression_ratio": 1.6018518518518519, "no_speech_prob": 3.2887344332266366e-06}, {"id": 654, "seek": 316776, "start": 3167.76, "end": 3170.84, "text": " And we're about to dig into that.", "tokens": [400, 321, 434, 466, 281, 2528, 666, 300, 13], "temperature": 0.0, "avg_logprob": -0.1824764096459677, "compression_ratio": 1.422680412371134, "no_speech_prob": 6.24087897449499e-06}, {"id": 655, "seek": 316776, "start": 3170.84, "end": 3177.36, "text": " The way I trained it was literally the same lines of code that you see here, but without", "tokens": [440, 636, 286, 8895, 309, 390, 3736, 264, 912, 3876, 295, 3089, 300, 291, 536, 510, 11, 457, 1553], "temperature": 0.0, "avg_logprob": -0.1824764096459677, "compression_ratio": 1.422680412371134, "no_speech_prob": 6.24087897449499e-06}, {"id": 656, "seek": 316776, "start": 3177.36, "end": 3180.8, "text": " pre-training it on Wikitext 103.", "tokens": [659, 12, 17227, 1760, 309, 322, 23377, 642, 734, 48784, 13], "temperature": 0.0, "avg_logprob": -0.1824764096459677, "compression_ratio": 1.422680412371134, "no_speech_prob": 6.24087897449499e-06}, {"id": 657, "seek": 316776, "start": 3180.8, "end": 3186.8, "text": " So let's take a 10 minute break, come back at 7.40 and we'll dig in and have a look at", "tokens": [407, 718, 311, 747, 257, 1266, 3456, 1821, 11, 808, 646, 412, 1614, 13, 5254, 293, 321, 603, 2528, 294, 293, 362, 257, 574, 412], "temperature": 0.0, "avg_logprob": -0.1824764096459677, "compression_ratio": 1.422680412371134, "no_speech_prob": 6.24087897449499e-06}, {"id": 658, "seek": 316776, "start": 3186.8, "end": 3187.8, "text": " these models.", "tokens": [613, 5245, 13], "temperature": 0.0, "avg_logprob": -0.1824764096459677, "compression_ratio": 1.422680412371134, "no_speech_prob": 6.24087897449499e-06}, {"id": 659, "seek": 316776, "start": 3187.8, "end": 3190.5200000000004, "text": " Okay, welcome back.", "tokens": [1033, 11, 2928, 646, 13], "temperature": 0.0, "avg_logprob": -0.1824764096459677, "compression_ratio": 1.422680412371134, "no_speech_prob": 6.24087897449499e-06}, {"id": 660, "seek": 319052, "start": 3190.52, "end": 3197.56, "text": " Before we go back into language models and NLP classifiers, a quick discussion about", "tokens": [4546, 321, 352, 646, 666, 2856, 5245, 293, 426, 45196, 1508, 23463, 11, 257, 1702, 5017, 466], "temperature": 0.0, "avg_logprob": -0.1341119591070681, "compression_ratio": 1.649789029535865, "no_speech_prob": 2.2473152057500556e-05}, {"id": 661, "seek": 319052, "start": 3197.56, "end": 3202.54, "text": " something pretty new at the moment, which is the FastAI.doc project.", "tokens": [746, 1238, 777, 412, 264, 1623, 11, 597, 307, 264, 15968, 48698, 13, 39966, 1716, 13], "temperature": 0.0, "avg_logprob": -0.1341119591070681, "compression_ratio": 1.649789029535865, "no_speech_prob": 2.2473152057500556e-05}, {"id": 662, "seek": 319052, "start": 3202.54, "end": 3209.0, "text": " So the goal of the FastAI.doc project is to create documentation that makes readers say,", "tokens": [407, 264, 3387, 295, 264, 15968, 48698, 13, 39966, 1716, 307, 281, 1884, 14333, 300, 1669, 17147, 584, 11], "temperature": 0.0, "avg_logprob": -0.1341119591070681, "compression_ratio": 1.649789029535865, "no_speech_prob": 2.2473152057500556e-05}, {"id": 663, "seek": 319052, "start": 3209.0, "end": 3215.36, "text": " wow, that's the most fantastic documentation I've ever read.", "tokens": [6076, 11, 300, 311, 264, 881, 5456, 14333, 286, 600, 1562, 1401, 13], "temperature": 0.0, "avg_logprob": -0.1341119591070681, "compression_ratio": 1.649789029535865, "no_speech_prob": 2.2473152057500556e-05}, {"id": 664, "seek": 319052, "start": 3215.36, "end": 3220.2, "text": " And so we have some specific ideas about how to do that, but it's the same kind of idea", "tokens": [400, 370, 321, 362, 512, 2685, 3487, 466, 577, 281, 360, 300, 11, 457, 309, 311, 264, 912, 733, 295, 1558], "temperature": 0.0, "avg_logprob": -0.1341119591070681, "compression_ratio": 1.649789029535865, "no_speech_prob": 2.2473152057500556e-05}, {"id": 665, "seek": 322020, "start": 3220.2, "end": 3229.72, "text": " of like top-down, thoughtful, take full advantage of the medium approach, interactive experimental", "tokens": [295, 411, 1192, 12, 5093, 11, 21566, 11, 747, 1577, 5002, 295, 264, 6399, 3109, 11, 15141, 17069], "temperature": 0.0, "avg_logprob": -0.17485331126621792, "compression_ratio": 1.4973262032085561, "no_speech_prob": 4.029418960271869e-06}, {"id": 666, "seek": 322020, "start": 3229.72, "end": 3234.08, "text": " code first that we're all familiar with.", "tokens": [3089, 700, 300, 321, 434, 439, 4963, 365, 13], "temperature": 0.0, "avg_logprob": -0.17485331126621792, "compression_ratio": 1.4973262032085561, "no_speech_prob": 4.029418960271869e-06}, {"id": 667, "seek": 322020, "start": 3234.08, "end": 3241.2, "text": " If you're interested in getting involved, the basic approach you can see in the docs", "tokens": [759, 291, 434, 3102, 294, 1242, 3288, 11, 264, 3875, 3109, 291, 393, 536, 294, 264, 45623], "temperature": 0.0, "avg_logprob": -0.17485331126621792, "compression_ratio": 1.4973262032085561, "no_speech_prob": 4.029418960271869e-06}, {"id": 668, "seek": 322020, "start": 3241.2, "end": 3243.12, "text": " directory.", "tokens": [21120, 13], "temperature": 0.0, "avg_logprob": -0.17485331126621792, "compression_ratio": 1.4973262032085561, "no_speech_prob": 4.029418960271869e-06}, {"id": 669, "seek": 322020, "start": 3243.12, "end": 3247.8999999999996, "text": " So this is the README in the docs directory.", "tokens": [407, 341, 307, 264, 10869, 6112, 15454, 294, 264, 45623, 21120, 13], "temperature": 0.0, "avg_logprob": -0.17485331126621792, "compression_ratio": 1.4973262032085561, "no_speech_prob": 4.029418960271869e-06}, {"id": 670, "seek": 324790, "start": 3247.9, "end": 3254.08, "text": " In there, there is amongst other things a transforms-template.adoc.", "tokens": [682, 456, 11, 456, 307, 12918, 661, 721, 257, 35592, 12, 83, 5895, 473, 13, 345, 905, 13], "temperature": 0.0, "avg_logprob": -0.24026117915600803, "compression_ratio": 1.651376146788991, "no_speech_prob": 9.97276947600767e-06}, {"id": 671, "seek": 324790, "start": 3254.08, "end": 3255.08, "text": " What the hell is adoc?", "tokens": [708, 264, 4921, 307, 614, 905, 30], "temperature": 0.0, "avg_logprob": -0.24026117915600803, "compression_ratio": 1.651376146788991, "no_speech_prob": 9.97276947600767e-06}, {"id": 672, "seek": 324790, "start": 3255.08, "end": 3257.4, "text": " adoc is ASCII doc.", "tokens": [614, 905, 307, 7469, 34, 9503, 3211, 13], "temperature": 0.0, "avg_logprob": -0.24026117915600803, "compression_ratio": 1.651376146788991, "no_speech_prob": 9.97276947600767e-06}, {"id": 673, "seek": 324790, "start": 3257.4, "end": 3260.64, "text": " How many people here have come across ASCII doc?", "tokens": [1012, 867, 561, 510, 362, 808, 2108, 7469, 34, 9503, 3211, 30], "temperature": 0.0, "avg_logprob": -0.24026117915600803, "compression_ratio": 1.651376146788991, "no_speech_prob": 9.97276947600767e-06}, {"id": 674, "seek": 324790, "start": 3260.64, "end": 3261.64, "text": " That's awesome.", "tokens": [663, 311, 3476, 13], "temperature": 0.0, "avg_logprob": -0.24026117915600803, "compression_ratio": 1.651376146788991, "no_speech_prob": 9.97276947600767e-06}, {"id": 675, "seek": 324790, "start": 3261.64, "end": 3267.2000000000003, "text": " ASCII doc is, people are laughing because there's one hand up and it's somebody who", "tokens": [7469, 34, 9503, 3211, 307, 11, 561, 366, 5059, 570, 456, 311, 472, 1011, 493, 293, 309, 311, 2618, 567], "temperature": 0.0, "avg_logprob": -0.24026117915600803, "compression_ratio": 1.651376146788991, "no_speech_prob": 9.97276947600767e-06}, {"id": 676, "seek": 324790, "start": 3267.2000000000003, "end": 3271.12, "text": " was in our study group today who talked to me about ASCII doc.", "tokens": [390, 294, 527, 2979, 1594, 965, 567, 2825, 281, 385, 466, 7469, 34, 9503, 3211, 13], "temperature": 0.0, "avg_logprob": -0.24026117915600803, "compression_ratio": 1.651376146788991, "no_speech_prob": 9.97276947600767e-06}, {"id": 677, "seek": 324790, "start": 3271.12, "end": 3273.1600000000003, "text": " ASCII doc is the most amazing project.", "tokens": [7469, 34, 9503, 3211, 307, 264, 881, 2243, 1716, 13], "temperature": 0.0, "avg_logprob": -0.24026117915600803, "compression_ratio": 1.651376146788991, "no_speech_prob": 9.97276947600767e-06}, {"id": 678, "seek": 327316, "start": 3273.16, "end": 3280.3599999999997, "text": " It's like Markdown, but it's like what Markdown needs to be to create actual books.", "tokens": [467, 311, 411, 3934, 5093, 11, 457, 309, 311, 411, 437, 3934, 5093, 2203, 281, 312, 281, 1884, 3539, 3642, 13], "temperature": 0.0, "avg_logprob": -0.17750987177309782, "compression_ratio": 1.6255707762557077, "no_speech_prob": 8.530233571946155e-06}, {"id": 679, "seek": 327316, "start": 3280.3599999999997, "end": 3283.3199999999997, "text": " Like a lot of actual books are written in ASCII doc.", "tokens": [1743, 257, 688, 295, 3539, 3642, 366, 3720, 294, 7469, 34, 9503, 3211, 13], "temperature": 0.0, "avg_logprob": -0.17750987177309782, "compression_ratio": 1.6255707762557077, "no_speech_prob": 8.530233571946155e-06}, {"id": 680, "seek": 327316, "start": 3283.3199999999997, "end": 3288.24, "text": " And so it's as easy to use as Markdown, but there's way more cool stuff you can do with", "tokens": [400, 370, 309, 311, 382, 1858, 281, 764, 382, 3934, 5093, 11, 457, 456, 311, 636, 544, 1627, 1507, 291, 393, 360, 365], "temperature": 0.0, "avg_logprob": -0.17750987177309782, "compression_ratio": 1.6255707762557077, "no_speech_prob": 8.530233571946155e-06}, {"id": 681, "seek": 327316, "start": 3288.24, "end": 3289.24, "text": " it.", "tokens": [309, 13], "temperature": 0.0, "avg_logprob": -0.17750987177309782, "compression_ratio": 1.6255707762557077, "no_speech_prob": 8.530233571946155e-06}, {"id": 682, "seek": 327316, "start": 3289.24, "end": 3292.08, "text": " In fact, here is an ASCII doc file here.", "tokens": [682, 1186, 11, 510, 307, 364, 7469, 34, 9503, 3211, 3991, 510, 13], "temperature": 0.0, "avg_logprob": -0.17750987177309782, "compression_ratio": 1.6255707762557077, "no_speech_prob": 8.530233571946155e-06}, {"id": 683, "seek": 327316, "start": 3292.08, "end": 3293.7599999999998, "text": " And as you'll see, it looks very normal.", "tokens": [400, 382, 291, 603, 536, 11, 309, 1542, 588, 2710, 13], "temperature": 0.0, "avg_logprob": -0.17750987177309782, "compression_ratio": 1.6255707762557077, "no_speech_prob": 8.530233571946155e-06}, {"id": 684, "seek": 327316, "start": 3293.7599999999998, "end": 3295.7599999999998, "text": " There's headings.", "tokens": [821, 311, 1378, 1109, 13], "temperature": 0.0, "avg_logprob": -0.17750987177309782, "compression_ratio": 1.6255707762557077, "no_speech_prob": 8.530233571946155e-06}, {"id": 685, "seek": 327316, "start": 3295.7599999999998, "end": 3299.7999999999997, "text": " This is pre-formatted text.", "tokens": [639, 307, 659, 12, 837, 32509, 2487, 13], "temperature": 0.0, "avg_logprob": -0.17750987177309782, "compression_ratio": 1.6255707762557077, "no_speech_prob": 8.530233571946155e-06}, {"id": 686, "seek": 329980, "start": 3299.8, "end": 3305.0, "text": " And there's lists and whatever else.", "tokens": [400, 456, 311, 14511, 293, 2035, 1646, 13], "temperature": 0.0, "avg_logprob": -0.28838444963286197, "compression_ratio": 1.6033519553072626, "no_speech_prob": 2.642569143063156e-06}, {"id": 687, "seek": 329980, "start": 3305.0, "end": 3309.1200000000003, "text": " It looks pretty standard.", "tokens": [467, 1542, 1238, 3832, 13], "temperature": 0.0, "avg_logprob": -0.28838444963286197, "compression_ratio": 1.6033519553072626, "no_speech_prob": 2.642569143063156e-06}, {"id": 688, "seek": 329980, "start": 3309.1200000000003, "end": 3312.6800000000003, "text": " And actually I'll show you a more complete ASCII doc thing.", "tokens": [400, 767, 286, 603, 855, 291, 257, 544, 3566, 7469, 34, 9503, 3211, 551, 13], "temperature": 0.0, "avg_logprob": -0.28838444963286197, "compression_ratio": 1.6033519553072626, "no_speech_prob": 2.642569143063156e-06}, {"id": 689, "seek": 329980, "start": 3312.6800000000003, "end": 3315.0800000000004, "text": " A more standard ASCII doc thing.", "tokens": [316, 544, 3832, 7469, 34, 9503, 3211, 551, 13], "temperature": 0.0, "avg_logprob": -0.28838444963286197, "compression_ratio": 1.6033519553072626, "no_speech_prob": 2.642569143063156e-06}, {"id": 690, "seek": 329980, "start": 3315.0800000000004, "end": 3320.84, "text": " But you can do stuff like say put a table of contents here please.", "tokens": [583, 291, 393, 360, 1507, 411, 584, 829, 257, 3199, 295, 15768, 510, 1767, 13], "temperature": 0.0, "avg_logprob": -0.28838444963286197, "compression_ratio": 1.6033519553072626, "no_speech_prob": 2.642569143063156e-06}, {"id": 691, "seek": 329980, "start": 3320.84, "end": 3327.0, "text": " You can say colon colon means put a definition list here please.", "tokens": [509, 393, 584, 8255, 8255, 1355, 829, 257, 7123, 1329, 510, 1767, 13], "temperature": 0.0, "avg_logprob": -0.28838444963286197, "compression_ratio": 1.6033519553072626, "no_speech_prob": 2.642569143063156e-06}, {"id": 692, "seek": 332700, "start": 3327.0, "end": 3330.88, "text": " Plus means this is a continuation of the previous list item.", "tokens": [7721, 1355, 341, 307, 257, 29357, 295, 264, 3894, 1329, 3174, 13], "temperature": 0.0, "avg_logprob": -0.14175023078918458, "compression_ratio": 1.5219123505976095, "no_speech_prob": 1.165933736047009e-05}, {"id": 693, "seek": 332700, "start": 3330.88, "end": 3335.24, "text": " So there's just like little things that you can do which are super handy.", "tokens": [407, 456, 311, 445, 411, 707, 721, 300, 291, 393, 360, 597, 366, 1687, 13239, 13], "temperature": 0.0, "avg_logprob": -0.14175023078918458, "compression_ratio": 1.5219123505976095, "no_speech_prob": 1.165933736047009e-05}, {"id": 694, "seek": 332700, "start": 3335.24, "end": 3339.76, "text": " Or like put this thing, make it slightly smaller than everything else.", "tokens": [1610, 411, 829, 341, 551, 11, 652, 309, 4748, 4356, 813, 1203, 1646, 13], "temperature": 0.0, "avg_logprob": -0.14175023078918458, "compression_ratio": 1.5219123505976095, "no_speech_prob": 1.165933736047009e-05}, {"id": 695, "seek": 332700, "start": 3339.76, "end": 3343.84, "text": " So it's like turbocharged Markdown.", "tokens": [407, 309, 311, 411, 20902, 25064, 3934, 5093, 13], "temperature": 0.0, "avg_logprob": -0.14175023078918458, "compression_ratio": 1.5219123505976095, "no_speech_prob": 1.165933736047009e-05}, {"id": 696, "seek": 332700, "start": 3343.84, "end": 3349.66, "text": " And so this ASCII doc creates this HTML.", "tokens": [400, 370, 341, 7469, 34, 9503, 3211, 7829, 341, 17995, 13], "temperature": 0.0, "avg_logprob": -0.14175023078918458, "compression_ratio": 1.5219123505976095, "no_speech_prob": 1.165933736047009e-05}, {"id": 697, "seek": 332700, "start": 3349.66, "end": 3352.32, "text": " And I didn't add any CSS or do anything myself.", "tokens": [400, 286, 994, 380, 909, 604, 24387, 420, 360, 1340, 2059, 13], "temperature": 0.0, "avg_logprob": -0.14175023078918458, "compression_ratio": 1.5219123505976095, "no_speech_prob": 1.165933736047009e-05}, {"id": 698, "seek": 332700, "start": 3352.32, "end": 3355.48, "text": " We literally started this project like 4 hours ago.", "tokens": [492, 3736, 1409, 341, 1716, 411, 1017, 2496, 2057, 13], "temperature": 0.0, "avg_logprob": -0.14175023078918458, "compression_ratio": 1.5219123505976095, "no_speech_prob": 1.165933736047009e-05}, {"id": 699, "seek": 335548, "start": 3355.48, "end": 3358.52, "text": " So this is just an example basically.", "tokens": [407, 341, 307, 445, 364, 1365, 1936, 13], "temperature": 0.0, "avg_logprob": -0.11795490919941604, "compression_ratio": 1.6545454545454545, "no_speech_prob": 7.411197202600306e-06}, {"id": 700, "seek": 335548, "start": 3358.52, "end": 3362.96, "text": " And so you can see we've got a table of contents.", "tokens": [400, 370, 291, 393, 536, 321, 600, 658, 257, 3199, 295, 15768, 13], "temperature": 0.0, "avg_logprob": -0.11795490919941604, "compression_ratio": 1.6545454545454545, "no_speech_prob": 7.411197202600306e-06}, {"id": 701, "seek": 335548, "start": 3362.96, "end": 3365.68, "text": " We can jump straight to here.", "tokens": [492, 393, 3012, 2997, 281, 510, 13], "temperature": 0.0, "avg_logprob": -0.11795490919941604, "compression_ratio": 1.6545454545454545, "no_speech_prob": 7.411197202600306e-06}, {"id": 702, "seek": 335548, "start": 3365.68, "end": 3370.88, "text": " We've got a cross-reference we can click on to jump straight to the cross-reference.", "tokens": [492, 600, 658, 257, 3278, 12, 265, 5158, 321, 393, 2052, 322, 281, 3012, 2997, 281, 264, 3278, 12, 265, 5158, 13], "temperature": 0.0, "avg_logprob": -0.11795490919941604, "compression_ratio": 1.6545454545454545, "no_speech_prob": 7.411197202600306e-06}, {"id": 703, "seek": 335548, "start": 3370.88, "end": 3375.8, "text": " Each method kind of comes along with its details and so on and so forth.", "tokens": [6947, 3170, 733, 295, 1487, 2051, 365, 1080, 4365, 293, 370, 322, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.11795490919941604, "compression_ratio": 1.6545454545454545, "no_speech_prob": 7.411197202600306e-06}, {"id": 704, "seek": 335548, "start": 3375.8, "end": 3382.88, "text": " And to make things even easier, rather than having to know that this is meant to be like", "tokens": [400, 281, 652, 721, 754, 3571, 11, 2831, 813, 1419, 281, 458, 300, 341, 307, 4140, 281, 312, 411], "temperature": 0.0, "avg_logprob": -0.11795490919941604, "compression_ratio": 1.6545454545454545, "no_speech_prob": 7.411197202600306e-06}, {"id": 705, "seek": 338288, "start": 3382.88, "end": 3387.6800000000003, "text": " the argument list is meant to be smaller than the main part or how do you create a cross-reference", "tokens": [264, 6770, 1329, 307, 4140, 281, 312, 4356, 813, 264, 2135, 644, 420, 577, 360, 291, 1884, 257, 3278, 12, 265, 5158], "temperature": 0.0, "avg_logprob": -0.09649329412551154, "compression_ratio": 1.8986784140969164, "no_speech_prob": 3.611950205595349e-06}, {"id": 706, "seek": 338288, "start": 3387.6800000000003, "end": 3393.76, "text": " or how are you meant to format the arguments to the method name and list out each one of", "tokens": [420, 577, 366, 291, 4140, 281, 7877, 264, 12869, 281, 264, 3170, 1315, 293, 1329, 484, 1184, 472, 295], "temperature": 0.0, "avg_logprob": -0.09649329412551154, "compression_ratio": 1.8986784140969164, "no_speech_prob": 3.611950205595349e-06}, {"id": 707, "seek": 338288, "start": 3393.76, "end": 3395.6400000000003, "text": " its arguments.", "tokens": [1080, 12869, 13], "temperature": 0.0, "avg_logprob": -0.09649329412551154, "compression_ratio": 1.8986784140969164, "no_speech_prob": 3.611950205595349e-06}, {"id": 708, "seek": 338288, "start": 3395.6400000000003, "end": 3401.08, "text": " We've created a special template where you can just write various stuff in curly brackets", "tokens": [492, 600, 2942, 257, 2121, 12379, 689, 291, 393, 445, 2464, 3683, 1507, 294, 32066, 26179], "temperature": 0.0, "avg_logprob": -0.09649329412551154, "compression_ratio": 1.8986784140969164, "no_speech_prob": 3.611950205595349e-06}, {"id": 709, "seek": 338288, "start": 3401.08, "end": 3406.28, "text": " like please put the arguments here and here is an example of one argument and here is", "tokens": [411, 1767, 829, 264, 12869, 510, 293, 510, 307, 364, 1365, 295, 472, 6770, 293, 510, 307], "temperature": 0.0, "avg_logprob": -0.09649329412551154, "compression_ratio": 1.8986784140969164, "no_speech_prob": 3.611950205595349e-06}, {"id": 710, "seek": 338288, "start": 3406.28, "end": 3410.4, "text": " a cross-reference and here is a method and so forth.", "tokens": [257, 3278, 12, 265, 5158, 293, 510, 307, 257, 3170, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.09649329412551154, "compression_ratio": 1.8986784140969164, "no_speech_prob": 3.611950205595349e-06}, {"id": 711, "seek": 341040, "start": 3410.4, "end": 3415.5, "text": " So we're in the process of documenting the documentation template but there's basically", "tokens": [407, 321, 434, 294, 264, 1399, 295, 42360, 264, 14333, 12379, 457, 456, 311, 1936], "temperature": 0.0, "avg_logprob": -0.19490955897739956, "compression_ratio": 1.7003891050583657, "no_speech_prob": 9.223302186001092e-06}, {"id": 712, "seek": 341040, "start": 3415.5, "end": 3419.32, "text": " like 5 or 6 of these little curly bracket things you'll need to learn.", "tokens": [411, 1025, 420, 1386, 295, 613, 707, 32066, 16904, 721, 291, 603, 643, 281, 1466, 13], "temperature": 0.0, "avg_logprob": -0.19490955897739956, "compression_ratio": 1.7003891050583657, "no_speech_prob": 9.223302186001092e-06}, {"id": 713, "seek": 341040, "start": 3419.32, "end": 3423.8, "text": " But for you to create the documentation of a class or a method, you can just copy one", "tokens": [583, 337, 291, 281, 1884, 264, 14333, 295, 257, 1508, 420, 257, 3170, 11, 291, 393, 445, 5055, 472], "temperature": 0.0, "avg_logprob": -0.19490955897739956, "compression_ratio": 1.7003891050583657, "no_speech_prob": 9.223302186001092e-06}, {"id": 714, "seek": 341040, "start": 3423.8, "end": 3426.36, "text": " that's already there basically.", "tokens": [300, 311, 1217, 456, 1936, 13], "temperature": 0.0, "avg_logprob": -0.19490955897739956, "compression_ratio": 1.7003891050583657, "no_speech_prob": 9.223302186001092e-06}, {"id": 715, "seek": 341040, "start": 3426.36, "end": 3431.44, "text": " And so the idea is we're going to have like, it'll almost be like a book.", "tokens": [400, 370, 264, 1558, 307, 321, 434, 516, 281, 362, 411, 11, 309, 603, 1920, 312, 411, 257, 1446, 13], "temperature": 0.0, "avg_logprob": -0.19490955897739956, "compression_ratio": 1.7003891050583657, "no_speech_prob": 9.223302186001092e-06}, {"id": 716, "seek": 341040, "start": 3431.44, "end": 3439.48, "text": " There'll be tables and pictures and little video segments and hyperlink throughout and", "tokens": [821, 603, 312, 8020, 293, 5242, 293, 707, 960, 19904, 293, 9848, 22473, 3710, 293], "temperature": 0.0, "avg_logprob": -0.19490955897739956, "compression_ratio": 1.7003891050583657, "no_speech_prob": 9.223302186001092e-06}, {"id": 717, "seek": 343948, "start": 3439.48, "end": 3441.32, "text": " all that stuff.", "tokens": [439, 300, 1507, 13], "temperature": 0.0, "avg_logprob": -0.17087203979492188, "compression_ratio": 1.7686832740213523, "no_speech_prob": 3.4465558655938366e-06}, {"id": 718, "seek": 343948, "start": 3441.32, "end": 3444.72, "text": " You might be wondering what about docstrings.", "tokens": [509, 1062, 312, 6359, 437, 466, 3211, 50035, 13], "temperature": 0.0, "avg_logprob": -0.17087203979492188, "compression_ratio": 1.7686832740213523, "no_speech_prob": 3.4465558655938366e-06}, {"id": 719, "seek": 343948, "start": 3444.72, "end": 3449.4, "text": " But actually I don't know if you've noticed, but if you look at the Python standard library", "tokens": [583, 767, 286, 500, 380, 458, 498, 291, 600, 5694, 11, 457, 498, 291, 574, 412, 264, 15329, 3832, 6405], "temperature": 0.0, "avg_logprob": -0.17087203979492188, "compression_ratio": 1.7686832740213523, "no_speech_prob": 3.4465558655938366e-06}, {"id": 720, "seek": 343948, "start": 3449.4, "end": 3454.6, "text": " and look at the docstring for example for regex compile, it's a single line.", "tokens": [293, 574, 412, 264, 3211, 37045, 337, 1365, 337, 319, 432, 87, 31413, 11, 309, 311, 257, 2167, 1622, 13], "temperature": 0.0, "avg_logprob": -0.17087203979492188, "compression_ratio": 1.7686832740213523, "no_speech_prob": 3.4465558655938366e-06}, {"id": 721, "seek": 343948, "start": 3454.6, "end": 3458.06, "text": " Nearly every docstring in Python is a single line.", "tokens": [38000, 633, 3211, 37045, 294, 15329, 307, 257, 2167, 1622, 13], "temperature": 0.0, "avg_logprob": -0.17087203979492188, "compression_ratio": 1.7686832740213523, "no_speech_prob": 3.4465558655938366e-06}, {"id": 722, "seek": 343948, "start": 3458.06, "end": 3459.76, "text": " And Python then does exactly this.", "tokens": [400, 15329, 550, 775, 2293, 341, 13], "temperature": 0.0, "avg_logprob": -0.17087203979492188, "compression_ratio": 1.7686832740213523, "no_speech_prob": 3.4465558655938366e-06}, {"id": 723, "seek": 343948, "start": 3459.76, "end": 3464.88, "text": " They have then a website containing the documentation that says like, hey, this is what regular", "tokens": [814, 362, 550, 257, 3144, 19273, 264, 14333, 300, 1619, 411, 11, 4177, 11, 341, 307, 437, 3890], "temperature": 0.0, "avg_logprob": -0.17087203979492188, "compression_ratio": 1.7686832740213523, "no_speech_prob": 3.4465558655938366e-06}, {"id": 724, "seek": 343948, "start": 3464.88, "end": 3468.12, "text": " expressions are and this is what you need to know about them and if you want them to", "tokens": [15277, 366, 293, 341, 307, 437, 291, 643, 281, 458, 466, 552, 293, 498, 291, 528, 552, 281], "temperature": 0.0, "avg_logprob": -0.17087203979492188, "compression_ratio": 1.7686832740213523, "no_speech_prob": 3.4465558655938366e-06}, {"id": 725, "seek": 346812, "start": 3468.12, "end": 3471.88, "text": " go faster, you need to use compile and here's lots of information about compile and here's", "tokens": [352, 4663, 11, 291, 643, 281, 764, 31413, 293, 510, 311, 3195, 295, 1589, 466, 31413, 293, 510, 311], "temperature": 0.0, "avg_logprob": -0.22448780560734297, "compression_ratio": 1.6565217391304348, "no_speech_prob": 3.041586296603782e-06}, {"id": 726, "seek": 346812, "start": 3471.88, "end": 3472.88, "text": " the examples.", "tokens": [264, 5110, 13], "temperature": 0.0, "avg_logprob": -0.22448780560734297, "compression_ratio": 1.6565217391304348, "no_speech_prob": 3.041586296603782e-06}, {"id": 727, "seek": 346812, "start": 3472.88, "end": 3475.24, "text": " It's not in the docstring.", "tokens": [467, 311, 406, 294, 264, 3211, 37045, 13], "temperature": 0.0, "avg_logprob": -0.22448780560734297, "compression_ratio": 1.6565217391304348, "no_speech_prob": 3.041586296603782e-06}, {"id": 728, "seek": 346812, "start": 3475.24, "end": 3476.64, "text": " And that's how we're doing it as well.", "tokens": [400, 300, 311, 577, 321, 434, 884, 309, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.22448780560734297, "compression_ratio": 1.6565217391304348, "no_speech_prob": 3.041586296603782e-06}, {"id": 729, "seek": 346812, "start": 3476.64, "end": 3482.72, "text": " Our docstrings will be one line unless you need like two sometimes.", "tokens": [2621, 3211, 50035, 486, 312, 472, 1622, 5969, 291, 643, 411, 732, 2171, 13], "temperature": 0.0, "avg_logprob": -0.22448780560734297, "compression_ratio": 1.6565217391304348, "no_speech_prob": 3.041586296603782e-06}, {"id": 730, "seek": 346812, "start": 3482.72, "end": 3488.52, "text": " It's going to be very similar to Python, but even better.", "tokens": [467, 311, 516, 281, 312, 588, 2531, 281, 15329, 11, 457, 754, 1101, 13], "temperature": 0.0, "avg_logprob": -0.22448780560734297, "compression_ratio": 1.6565217391304348, "no_speech_prob": 3.041586296603782e-06}, {"id": 731, "seek": 346812, "start": 3488.52, "end": 3493.96, "text": " So everybody is welcome to help contribute to the documentation and hopefully by the", "tokens": [407, 2201, 307, 2928, 281, 854, 10586, 281, 264, 14333, 293, 4696, 538, 264], "temperature": 0.0, "avg_logprob": -0.22448780560734297, "compression_ratio": 1.6565217391304348, "no_speech_prob": 3.041586296603782e-06}, {"id": 732, "seek": 349396, "start": 3493.96, "end": 3499.88, "text": " time you're watching this on the MOOC, it'll be reasonably fleshed out and we'll try to", "tokens": [565, 291, 434, 1976, 341, 322, 264, 49197, 34, 11, 309, 603, 312, 23551, 12497, 292, 484, 293, 321, 603, 853, 281], "temperature": 0.0, "avg_logprob": -0.1828761677165608, "compression_ratio": 1.4840182648401827, "no_speech_prob": 7.527916295657633e-06}, {"id": 733, "seek": 349396, "start": 3499.88, "end": 3503.04, "text": " keep a list of things to do.", "tokens": [1066, 257, 1329, 295, 721, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.1828761677165608, "compression_ratio": 1.4840182648401827, "no_speech_prob": 7.527916295657633e-06}, {"id": 734, "seek": 349396, "start": 3503.04, "end": 3511.56, "text": " Alright, so I'm going to do one first.", "tokens": [2798, 11, 370, 286, 478, 516, 281, 360, 472, 700, 13], "temperature": 0.0, "avg_logprob": -0.1828761677165608, "compression_ratio": 1.4840182648401827, "no_speech_prob": 7.527916295657633e-06}, {"id": 735, "seek": 349396, "start": 3511.56, "end": 3518.44, "text": " So one question that came up in the break was how does this compare to Word2Vec?", "tokens": [407, 472, 1168, 300, 1361, 493, 294, 264, 1821, 390, 577, 775, 341, 6794, 281, 8725, 17, 53, 3045, 30], "temperature": 0.0, "avg_logprob": -0.1828761677165608, "compression_ratio": 1.4840182648401827, "no_speech_prob": 7.527916295657633e-06}, {"id": 736, "seek": 349396, "start": 3518.44, "end": 3523.04, "text": " And this is actually a great thing for you to spend time thinking about during the week,", "tokens": [400, 341, 307, 767, 257, 869, 551, 337, 291, 281, 3496, 565, 1953, 466, 1830, 264, 1243, 11], "temperature": 0.0, "avg_logprob": -0.1828761677165608, "compression_ratio": 1.4840182648401827, "no_speech_prob": 7.527916295657633e-06}, {"id": 737, "seek": 352304, "start": 3523.04, "end": 3524.52, "text": " is how does this compare to Word2Vec.", "tokens": [307, 577, 775, 341, 6794, 281, 8725, 17, 53, 3045, 13], "temperature": 0.0, "avg_logprob": -0.15222950165088361, "compression_ratio": 1.6108597285067874, "no_speech_prob": 1.9525370589690283e-05}, {"id": 738, "seek": 352304, "start": 3524.52, "end": 3529.44, "text": " I'll give you the summary now, but it's a very important conceptual difference.", "tokens": [286, 603, 976, 291, 264, 12691, 586, 11, 457, 309, 311, 257, 588, 1021, 24106, 2649, 13], "temperature": 0.0, "avg_logprob": -0.15222950165088361, "compression_ratio": 1.6108597285067874, "no_speech_prob": 1.9525370589690283e-05}, {"id": 739, "seek": 352304, "start": 3529.44, "end": 3533.24, "text": " The main conceptual difference is what is Word2Vec?", "tokens": [440, 2135, 24106, 2649, 307, 437, 307, 8725, 17, 53, 3045, 30], "temperature": 0.0, "avg_logprob": -0.15222950165088361, "compression_ratio": 1.6108597285067874, "no_speech_prob": 1.9525370589690283e-05}, {"id": 740, "seek": 352304, "start": 3533.24, "end": 3536.68, "text": " Word2Vec is a single embedding matrix.", "tokens": [8725, 17, 53, 3045, 307, 257, 2167, 12240, 3584, 8141, 13], "temperature": 0.0, "avg_logprob": -0.15222950165088361, "compression_ratio": 1.6108597285067874, "no_speech_prob": 1.9525370589690283e-05}, {"id": 741, "seek": 352304, "start": 3536.68, "end": 3540.4, "text": " Each word has a vector and that's it.", "tokens": [6947, 1349, 575, 257, 8062, 293, 300, 311, 309, 13], "temperature": 0.0, "avg_logprob": -0.15222950165088361, "compression_ratio": 1.6108597285067874, "no_speech_prob": 1.9525370589690283e-05}, {"id": 742, "seek": 352304, "start": 3540.4, "end": 3546.2, "text": " So in other words, it's a single layer from a pre-trained model.", "tokens": [407, 294, 661, 2283, 11, 309, 311, 257, 2167, 4583, 490, 257, 659, 12, 17227, 2001, 2316, 13], "temperature": 0.0, "avg_logprob": -0.15222950165088361, "compression_ratio": 1.6108597285067874, "no_speech_prob": 1.9525370589690283e-05}, {"id": 743, "seek": 352304, "start": 3546.2, "end": 3549.84, "text": " Specifically, that layer is the input layer.", "tokens": [26058, 11, 300, 4583, 307, 264, 4846, 4583, 13], "temperature": 0.0, "avg_logprob": -0.15222950165088361, "compression_ratio": 1.6108597285067874, "no_speech_prob": 1.9525370589690283e-05}, {"id": 744, "seek": 354984, "start": 3549.84, "end": 3557.6400000000003, "text": " And also specifically, that pre-trained model is a linear model that is pre-trained on something", "tokens": [400, 611, 4682, 11, 300, 659, 12, 17227, 2001, 2316, 307, 257, 8213, 2316, 300, 307, 659, 12, 17227, 2001, 322, 746], "temperature": 0.0, "avg_logprob": -0.12889379200182463, "compression_ratio": 1.6288209606986899, "no_speech_prob": 2.8130043574492447e-06}, {"id": 745, "seek": 354984, "start": 3557.6400000000003, "end": 3559.96, "text": " called a co-occurrence matrix.", "tokens": [1219, 257, 598, 12, 905, 14112, 10760, 8141, 13], "temperature": 0.0, "avg_logprob": -0.12889379200182463, "compression_ratio": 1.6288209606986899, "no_speech_prob": 2.8130043574492447e-06}, {"id": 746, "seek": 354984, "start": 3559.96, "end": 3564.56, "text": " So we have no particular reason to believe that this model has learned anything much", "tokens": [407, 321, 362, 572, 1729, 1778, 281, 1697, 300, 341, 2316, 575, 3264, 1340, 709], "temperature": 0.0, "avg_logprob": -0.12889379200182463, "compression_ratio": 1.6288209606986899, "no_speech_prob": 2.8130043574492447e-06}, {"id": 747, "seek": 354984, "start": 3564.56, "end": 3568.8, "text": " about the English language, or that it has any particular capabilities because it's just", "tokens": [466, 264, 3669, 2856, 11, 420, 300, 309, 575, 604, 1729, 10862, 570, 309, 311, 445], "temperature": 0.0, "avg_logprob": -0.12889379200182463, "compression_ratio": 1.6288209606986899, "no_speech_prob": 2.8130043574492447e-06}, {"id": 748, "seek": 354984, "start": 3568.8, "end": 3574.36, "text": " a single linear layer and that's it.", "tokens": [257, 2167, 8213, 4583, 293, 300, 311, 309, 13], "temperature": 0.0, "avg_logprob": -0.12889379200182463, "compression_ratio": 1.6288209606986899, "no_speech_prob": 2.8130043574492447e-06}, {"id": 749, "seek": 354984, "start": 3574.36, "end": 3578.1200000000003, "text": " So what's this Wikitext 103 model?", "tokens": [407, 437, 311, 341, 23377, 642, 734, 48784, 2316, 30], "temperature": 0.0, "avg_logprob": -0.12889379200182463, "compression_ratio": 1.6288209606986899, "no_speech_prob": 2.8130043574492447e-06}, {"id": 750, "seek": 357812, "start": 3578.12, "end": 3580.24, "text": " It's a language model.", "tokens": [467, 311, 257, 2856, 2316, 13], "temperature": 0.0, "avg_logprob": -0.2194584003393201, "compression_ratio": 1.323699421965318, "no_speech_prob": 3.0415756100410363e-06}, {"id": 751, "seek": 357812, "start": 3580.24, "end": 3593.24, "text": " It has a 400-dimensional embedding matrix, 3 hidden layers with 1150 activations per", "tokens": [467, 575, 257, 8423, 12, 18759, 12240, 3584, 8141, 11, 805, 7633, 7914, 365, 2975, 2803, 2430, 763, 680], "temperature": 0.0, "avg_logprob": -0.2194584003393201, "compression_ratio": 1.323699421965318, "no_speech_prob": 3.0415756100410363e-06}, {"id": 752, "seek": 357812, "start": 3593.24, "end": 3600.7999999999997, "text": " layer and regularization and all of that stuff, tied input, output, matrices, it's basically", "tokens": [4583, 293, 3890, 2144, 293, 439, 295, 300, 1507, 11, 9601, 4846, 11, 5598, 11, 32284, 11, 309, 311, 1936], "temperature": 0.0, "avg_logprob": -0.2194584003393201, "compression_ratio": 1.323699421965318, "no_speech_prob": 3.0415756100410363e-06}, {"id": 753, "seek": 357812, "start": 3600.7999999999997, "end": 3605.3599999999997, "text": " a state-of-the-art AWD ISTM.", "tokens": [257, 1785, 12, 2670, 12, 3322, 12, 446, 25815, 35, 286, 6840, 44, 13], "temperature": 0.0, "avg_logprob": -0.2194584003393201, "compression_ratio": 1.323699421965318, "no_speech_prob": 3.0415756100410363e-06}, {"id": 754, "seek": 360536, "start": 3605.36, "end": 3614.44, "text": " So what's the difference between a single layer of a single linear model versus a 3-layer", "tokens": [407, 437, 311, 264, 2649, 1296, 257, 2167, 4583, 295, 257, 2167, 8213, 2316, 5717, 257, 805, 12, 8376, 260], "temperature": 0.0, "avg_logprob": -0.2097604058005593, "compression_ratio": 1.5727699530516432, "no_speech_prob": 2.3687764496571617e-06}, {"id": 755, "seek": 360536, "start": 3614.44, "end": 3616.48, "text": " recurrent neural network?", "tokens": [18680, 1753, 18161, 3209, 30], "temperature": 0.0, "avg_logprob": -0.2097604058005593, "compression_ratio": 1.5727699530516432, "no_speech_prob": 2.3687764496571617e-06}, {"id": 756, "seek": 360536, "start": 3616.48, "end": 3617.48, "text": " Everything.", "tokens": [5471, 13], "temperature": 0.0, "avg_logprob": -0.2097604058005593, "compression_ratio": 1.5727699530516432, "no_speech_prob": 2.3687764496571617e-06}, {"id": 757, "seek": 360536, "start": 3617.48, "end": 3621.34, "text": " They're very different levels of capability.", "tokens": [814, 434, 588, 819, 4358, 295, 13759, 13], "temperature": 0.0, "avg_logprob": -0.2097604058005593, "compression_ratio": 1.5727699530516432, "no_speech_prob": 2.3687764496571617e-06}, {"id": 758, "seek": 360536, "start": 3621.34, "end": 3628.84, "text": " And so you'll see when you try using a pre-trained language model versus a Word2Vec layer, you'll", "tokens": [400, 370, 291, 603, 536, 562, 291, 853, 1228, 257, 659, 12, 17227, 2001, 2856, 2316, 5717, 257, 8725, 17, 53, 3045, 4583, 11, 291, 603], "temperature": 0.0, "avg_logprob": -0.2097604058005593, "compression_ratio": 1.5727699530516432, "no_speech_prob": 2.3687764496571617e-06}, {"id": 759, "seek": 360536, "start": 3628.84, "end": 3633.42, "text": " get very, very different results for the vast majority of tasks.", "tokens": [483, 588, 11, 588, 819, 3542, 337, 264, 8369, 6286, 295, 9608, 13], "temperature": 0.0, "avg_logprob": -0.2097604058005593, "compression_ratio": 1.5727699530516432, "no_speech_prob": 2.3687764496571617e-06}, {"id": 760, "seek": 363342, "start": 3633.42, "end": 3636.02, "text": " What if the NumPy array does not fit in memory?", "tokens": [708, 498, 264, 22592, 47, 88, 10225, 775, 406, 3318, 294, 4675, 30], "temperature": 0.0, "avg_logprob": -0.15286262964798233, "compression_ratio": 1.58130081300813, "no_speech_prob": 1.130062901211204e-05}, {"id": 761, "seek": 363342, "start": 3636.02, "end": 3642.48, "text": " Is it possible to write a PyTorch data loader directly from a large CSV file?", "tokens": [1119, 309, 1944, 281, 2464, 257, 9953, 51, 284, 339, 1412, 3677, 260, 3838, 490, 257, 2416, 48814, 3991, 30], "temperature": 0.0, "avg_logprob": -0.15286262964798233, "compression_ratio": 1.58130081300813, "no_speech_prob": 1.130062901211204e-05}, {"id": 762, "seek": 363342, "start": 3642.48, "end": 3645.92, "text": " It almost certainly won't come up, so I'm not going to spend time on it.", "tokens": [467, 1920, 3297, 1582, 380, 808, 493, 11, 370, 286, 478, 406, 516, 281, 3496, 565, 322, 309, 13], "temperature": 0.0, "avg_logprob": -0.15286262964798233, "compression_ratio": 1.58130081300813, "no_speech_prob": 1.130062901211204e-05}, {"id": 763, "seek": 363342, "start": 3645.92, "end": 3649.12, "text": " These things are tiny.", "tokens": [1981, 721, 366, 5870, 13], "temperature": 0.0, "avg_logprob": -0.15286262964798233, "compression_ratio": 1.58130081300813, "no_speech_prob": 1.130062901211204e-05}, {"id": 764, "seek": 363342, "start": 3649.12, "end": 3650.12, "text": " They're just ints.", "tokens": [814, 434, 445, 560, 82, 13], "temperature": 0.0, "avg_logprob": -0.15286262964798233, "compression_ratio": 1.58130081300813, "no_speech_prob": 1.130062901211204e-05}, {"id": 765, "seek": 363342, "start": 3650.12, "end": 3652.64, "text": " Think about how many ints you would need to write out a memory.", "tokens": [6557, 466, 577, 867, 560, 82, 291, 576, 643, 281, 2464, 484, 257, 4675, 13], "temperature": 0.0, "avg_logprob": -0.15286262964798233, "compression_ratio": 1.58130081300813, "no_speech_prob": 1.130062901211204e-05}, {"id": 766, "seek": 363342, "start": 3652.64, "end": 3653.64, "text": " It's not going to happen.", "tokens": [467, 311, 406, 516, 281, 1051, 13], "temperature": 0.0, "avg_logprob": -0.15286262964798233, "compression_ratio": 1.58130081300813, "no_speech_prob": 1.130062901211204e-05}, {"id": 767, "seek": 363342, "start": 3653.64, "end": 3657.7000000000003, "text": " They don't have to fit in GPU memory, just in your memory.", "tokens": [814, 500, 380, 362, 281, 3318, 294, 18407, 4675, 11, 445, 294, 428, 4675, 13], "temperature": 0.0, "avg_logprob": -0.15286262964798233, "compression_ratio": 1.58130081300813, "no_speech_prob": 1.130062901211204e-05}, {"id": 768, "seek": 365770, "start": 3657.7, "end": 3665.08, "text": " So I've actually done another Wikipedia model, which I called Gigawiki, which was on all", "tokens": [407, 286, 600, 767, 1096, 1071, 28999, 2316, 11, 597, 286, 1219, 40489, 1607, 9850, 11, 597, 390, 322, 439], "temperature": 0.0, "avg_logprob": -0.18056822314704815, "compression_ratio": 1.5867768595041323, "no_speech_prob": 2.6425652777106734e-06}, {"id": 769, "seek": 365770, "start": 3665.08, "end": 3668.8799999999997, "text": " of Wikipedia, and even that easily fits in memory.", "tokens": [295, 28999, 11, 293, 754, 300, 3612, 9001, 294, 4675, 13], "temperature": 0.0, "avg_logprob": -0.18056822314704815, "compression_ratio": 1.5867768595041323, "no_speech_prob": 2.6425652777106734e-06}, {"id": 770, "seek": 365770, "start": 3668.8799999999997, "end": 3672.4399999999996, "text": " The reason I'm not using it is because it turned out not to really help very much versus", "tokens": [440, 1778, 286, 478, 406, 1228, 309, 307, 570, 309, 3574, 484, 406, 281, 534, 854, 588, 709, 5717], "temperature": 0.0, "avg_logprob": -0.18056822314704815, "compression_ratio": 1.5867768595041323, "no_speech_prob": 2.6425652777106734e-06}, {"id": 771, "seek": 365770, "start": 3672.4399999999996, "end": 3679.04, "text": " Wikitext 103, but I've built a bigger model than anybody else I've found in the academic", "tokens": [23377, 642, 734, 48784, 11, 457, 286, 600, 3094, 257, 3801, 2316, 813, 4472, 1646, 286, 600, 1352, 294, 264, 7778], "temperature": 0.0, "avg_logprob": -0.18056822314704815, "compression_ratio": 1.5867768595041323, "no_speech_prob": 2.6425652777106734e-06}, {"id": 772, "seek": 365770, "start": 3679.04, "end": 3684.64, "text": " literature pretty much, and it fits in memory on a single machine.", "tokens": [10394, 1238, 709, 11, 293, 309, 9001, 294, 4675, 322, 257, 2167, 3479, 13], "temperature": 0.0, "avg_logprob": -0.18056822314704815, "compression_ratio": 1.5867768595041323, "no_speech_prob": 2.6425652777106734e-06}, {"id": 773, "seek": 368464, "start": 3684.64, "end": 3687.8799999999997, "text": " What is the idea behind averaging the weights of embeddings?", "tokens": [708, 307, 264, 1558, 2261, 47308, 264, 17443, 295, 12240, 29432, 30], "temperature": 0.0, "avg_logprob": -0.20283064882979435, "compression_ratio": 1.7023809523809523, "no_speech_prob": 1.56892383529339e-05}, {"id": 774, "seek": 368464, "start": 3687.8799999999997, "end": 3691.44, "text": " They've got to be set to something.", "tokens": [814, 600, 658, 281, 312, 992, 281, 746, 13], "temperature": 0.0, "avg_logprob": -0.20283064882979435, "compression_ratio": 1.7023809523809523, "no_speech_prob": 1.56892383529339e-05}, {"id": 775, "seek": 368464, "start": 3691.44, "end": 3693.92, "text": " There are words that weren't there.", "tokens": [821, 366, 2283, 300, 4999, 380, 456, 13], "temperature": 0.0, "avg_logprob": -0.20283064882979435, "compression_ratio": 1.7023809523809523, "no_speech_prob": 1.56892383529339e-05}, {"id": 776, "seek": 368464, "start": 3693.92, "end": 3696.56, "text": " So other options is we could leave them at 0.", "tokens": [407, 661, 3956, 307, 321, 727, 1856, 552, 412, 1958, 13], "temperature": 0.0, "avg_logprob": -0.20283064882979435, "compression_ratio": 1.7023809523809523, "no_speech_prob": 1.56892383529339e-05}, {"id": 777, "seek": 368464, "start": 3696.56, "end": 3698.16, "text": " That seems like a very extreme thing to do.", "tokens": [663, 2544, 411, 257, 588, 8084, 551, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.20283064882979435, "compression_ratio": 1.7023809523809523, "no_speech_prob": 1.56892383529339e-05}, {"id": 778, "seek": 368464, "start": 3698.16, "end": 3700.44, "text": " Like 0 is a very extreme number.", "tokens": [1743, 1958, 307, 257, 588, 8084, 1230, 13], "temperature": 0.0, "avg_logprob": -0.20283064882979435, "compression_ratio": 1.7023809523809523, "no_speech_prob": 1.56892383529339e-05}, {"id": 779, "seek": 368464, "start": 3700.44, "end": 3703.3599999999997, "text": " Why would it be 0?", "tokens": [1545, 576, 309, 312, 1958, 30], "temperature": 0.0, "avg_logprob": -0.20283064882979435, "compression_ratio": 1.7023809523809523, "no_speech_prob": 1.56892383529339e-05}, {"id": 780, "seek": 368464, "start": 3703.3599999999997, "end": 3708.12, "text": " We could set it equal to some random numbers, but if so, what would be the mean and standard", "tokens": [492, 727, 992, 309, 2681, 281, 512, 4974, 3547, 11, 457, 498, 370, 11, 437, 576, 312, 264, 914, 293, 3832], "temperature": 0.0, "avg_logprob": -0.20283064882979435, "compression_ratio": 1.7023809523809523, "no_speech_prob": 1.56892383529339e-05}, {"id": 781, "seek": 368464, "start": 3708.12, "end": 3710.96, "text": " deviation of those random numbers, or should they be uniform?", "tokens": [25163, 295, 729, 4974, 3547, 11, 420, 820, 436, 312, 9452, 30], "temperature": 0.0, "avg_logprob": -0.20283064882979435, "compression_ratio": 1.7023809523809523, "no_speech_prob": 1.56892383529339e-05}, {"id": 782, "seek": 371096, "start": 3710.96, "end": 3716.8, "text": " If we just average the rest of the embeddings, then we have something that's a reasonable", "tokens": [759, 321, 445, 4274, 264, 1472, 295, 264, 12240, 29432, 11, 550, 321, 362, 746, 300, 311, 257, 10585], "temperature": 0.0, "avg_logprob": -0.18744920194149017, "compression_ratio": 1.755639097744361, "no_speech_prob": 4.710875145974569e-06}, {"id": 783, "seek": 371096, "start": 3716.8, "end": 3717.8, "text": " scale.", "tokens": [4373, 13], "temperature": 0.0, "avg_logprob": -0.18744920194149017, "compression_ratio": 1.755639097744361, "no_speech_prob": 4.710875145974569e-06}, {"id": 784, "seek": 371096, "start": 3717.8, "end": 3724.68, "text": " Just to clarify, this is how you're initializing words that didn't appear in the training corpus.", "tokens": [1449, 281, 17594, 11, 341, 307, 577, 291, 434, 5883, 3319, 2283, 300, 994, 380, 4204, 294, 264, 3097, 1181, 31624, 13], "temperature": 0.0, "avg_logprob": -0.18744920194149017, "compression_ratio": 1.755639097744361, "no_speech_prob": 4.710875145974569e-06}, {"id": 785, "seek": 371096, "start": 3724.68, "end": 3729.34, "text": " I think you've pretty much just answered this one, but someone had asked if there's a specific", "tokens": [286, 519, 291, 600, 1238, 709, 445, 10103, 341, 472, 11, 457, 1580, 632, 2351, 498, 456, 311, 257, 2685], "temperature": 0.0, "avg_logprob": -0.18744920194149017, "compression_ratio": 1.755639097744361, "no_speech_prob": 4.710875145974569e-06}, {"id": 786, "seek": 371096, "start": 3729.34, "end": 3734.56, "text": " advantage to creating our own pre-trained embedding over using GloVe or Word2Vec.", "tokens": [5002, 281, 4084, 527, 1065, 659, 12, 17227, 2001, 12240, 3584, 670, 1228, 10786, 53, 68, 420, 8725, 17, 53, 3045, 13], "temperature": 0.0, "avg_logprob": -0.18744920194149017, "compression_ratio": 1.755639097744361, "no_speech_prob": 4.710875145974569e-06}, {"id": 787, "seek": 371096, "start": 3734.56, "end": 3735.56, "text": " I think I have.", "tokens": [286, 519, 286, 362, 13], "temperature": 0.0, "avg_logprob": -0.18744920194149017, "compression_ratio": 1.755639097744361, "no_speech_prob": 4.710875145974569e-06}, {"id": 788, "seek": 371096, "start": 3735.56, "end": 3739.52, "text": " We're not creating a pre-trained embedding, we're creating a pre-trained model.", "tokens": [492, 434, 406, 4084, 257, 659, 12, 17227, 2001, 12240, 3584, 11, 321, 434, 4084, 257, 659, 12, 17227, 2001, 2316, 13], "temperature": 0.0, "avg_logprob": -0.18744920194149017, "compression_ratio": 1.755639097744361, "no_speech_prob": 4.710875145974569e-06}, {"id": 789, "seek": 373952, "start": 3739.52, "end": 3744.68, "text": " Let's talk a little bit more.", "tokens": [961, 311, 751, 257, 707, 857, 544, 13], "temperature": 0.0, "avg_logprob": -0.162097225189209, "compression_ratio": 1.6192660550458715, "no_speech_prob": 1.3211818441050127e-05}, {"id": 790, "seek": 373952, "start": 3744.68, "end": 3747.6, "text": " This is the kind of stuff we've seen before, but it's changed a little bit.", "tokens": [639, 307, 264, 733, 295, 1507, 321, 600, 1612, 949, 11, 457, 309, 311, 3105, 257, 707, 857, 13], "temperature": 0.0, "avg_logprob": -0.162097225189209, "compression_ratio": 1.6192660550458715, "no_speech_prob": 1.3211818441050127e-05}, {"id": 791, "seek": 373952, "start": 3747.6, "end": 3751.72, "text": " It's actually a lot easier than it was in Part 1, but I want to go a little bit deeper", "tokens": [467, 311, 767, 257, 688, 3571, 813, 309, 390, 294, 4100, 502, 11, 457, 286, 528, 281, 352, 257, 707, 857, 7731], "temperature": 0.0, "avg_logprob": -0.162097225189209, "compression_ratio": 1.6192660550458715, "no_speech_prob": 1.3211818441050127e-05}, {"id": 792, "seek": 373952, "start": 3751.72, "end": 3758.04, "text": " into the language model loader.", "tokens": [666, 264, 2856, 2316, 3677, 260, 13], "temperature": 0.0, "avg_logprob": -0.162097225189209, "compression_ratio": 1.6192660550458715, "no_speech_prob": 1.3211818441050127e-05}, {"id": 793, "seek": 373952, "start": 3758.04, "end": 3761.32, "text": " So this is the language model loader, and I really hope that by now you've learned in", "tokens": [407, 341, 307, 264, 2856, 2316, 3677, 260, 11, 293, 286, 534, 1454, 300, 538, 586, 291, 600, 3264, 294], "temperature": 0.0, "avg_logprob": -0.162097225189209, "compression_ratio": 1.6192660550458715, "no_speech_prob": 1.3211818441050127e-05}, {"id": 794, "seek": 373952, "start": 3761.32, "end": 3765.08, "text": " your editor or IDE how to jump to symbols.", "tokens": [428, 9839, 420, 40930, 577, 281, 3012, 281, 16944, 13], "temperature": 0.0, "avg_logprob": -0.162097225189209, "compression_ratio": 1.6192660550458715, "no_speech_prob": 1.3211818441050127e-05}, {"id": 795, "seek": 376508, "start": 3765.08, "end": 3771.36, "text": " I don't want it to be a burden for you to find out what the source code of language", "tokens": [286, 500, 380, 528, 309, 281, 312, 257, 12578, 337, 291, 281, 915, 484, 437, 264, 4009, 3089, 295, 2856], "temperature": 0.0, "avg_logprob": -0.13530213182622736, "compression_ratio": 1.553921568627451, "no_speech_prob": 2.6016055016953032e-06}, {"id": 796, "seek": 376508, "start": 3771.36, "end": 3772.36, "text": " model loader is.", "tokens": [2316, 3677, 260, 307, 13], "temperature": 0.0, "avg_logprob": -0.13530213182622736, "compression_ratio": 1.553921568627451, "no_speech_prob": 2.6016055016953032e-06}, {"id": 797, "seek": 376508, "start": 3772.36, "end": 3777.48, "text": " If it's still a burden, please go back and try and learn those keyboard shortcuts in", "tokens": [759, 309, 311, 920, 257, 12578, 11, 1767, 352, 646, 293, 853, 293, 1466, 729, 10186, 34620, 294], "temperature": 0.0, "avg_logprob": -0.13530213182622736, "compression_ratio": 1.553921568627451, "no_speech_prob": 2.6016055016953032e-06}, {"id": 798, "seek": 376508, "start": 3777.48, "end": 3780.16, "text": " VS Code.", "tokens": [25091, 15549, 13], "temperature": 0.0, "avg_logprob": -0.13530213182622736, "compression_ratio": 1.553921568627451, "no_speech_prob": 2.6016055016953032e-06}, {"id": 799, "seek": 376508, "start": 3780.16, "end": 3783.88, "text": " If your editor doesn't make it easy, don't use that editor anymore.", "tokens": [759, 428, 9839, 1177, 380, 652, 309, 1858, 11, 500, 380, 764, 300, 9839, 3602, 13], "temperature": 0.0, "avg_logprob": -0.13530213182622736, "compression_ratio": 1.553921568627451, "no_speech_prob": 2.6016055016953032e-06}, {"id": 800, "seek": 376508, "start": 3783.88, "end": 3790.4, "text": " There's lots of good free editors that make this easy.", "tokens": [821, 311, 3195, 295, 665, 1737, 31446, 300, 652, 341, 1858, 13], "temperature": 0.0, "avg_logprob": -0.13530213182622736, "compression_ratio": 1.553921568627451, "no_speech_prob": 2.6016055016953032e-06}, {"id": 801, "seek": 379040, "start": 3790.4, "end": 3796.2400000000002, "text": " So here's the source code for language model loader.", "tokens": [407, 510, 311, 264, 4009, 3089, 337, 2856, 2316, 3677, 260, 13], "temperature": 0.0, "avg_logprob": -0.10604348652799364, "compression_ratio": 1.6140350877192982, "no_speech_prob": 2.5215638288500486e-06}, {"id": 802, "seek": 379040, "start": 3796.2400000000002, "end": 3805.3, "text": " It's interesting to notice that it's not doing anything particularly tricky.", "tokens": [467, 311, 1880, 281, 3449, 300, 309, 311, 406, 884, 1340, 4098, 12414, 13], "temperature": 0.0, "avg_logprob": -0.10604348652799364, "compression_ratio": 1.6140350877192982, "no_speech_prob": 2.5215638288500486e-06}, {"id": 803, "seek": 379040, "start": 3805.3, "end": 3810.1, "text": " It's not deriving from anything at all.", "tokens": [467, 311, 406, 1163, 2123, 490, 1340, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.10604348652799364, "compression_ratio": 1.6140350877192982, "no_speech_prob": 2.5215638288500486e-06}, {"id": 804, "seek": 379040, "start": 3810.1, "end": 3814.36, "text": " What makes it something that's capable of being a data loader is that it's something", "tokens": [708, 1669, 309, 746, 300, 311, 8189, 295, 885, 257, 1412, 3677, 260, 307, 300, 309, 311, 746], "temperature": 0.0, "avg_logprob": -0.10604348652799364, "compression_ratio": 1.6140350877192982, "no_speech_prob": 2.5215638288500486e-06}, {"id": 805, "seek": 379040, "start": 3814.36, "end": 3816.6800000000003, "text": " you can iterate over.", "tokens": [291, 393, 44497, 670, 13], "temperature": 0.0, "avg_logprob": -0.10604348652799364, "compression_ratio": 1.6140350877192982, "no_speech_prob": 2.5215638288500486e-06}, {"id": 806, "seek": 381668, "start": 3816.68, "end": 3828.0, "text": " And so specifically, here's the fit function inside fastai.model, where everything ends", "tokens": [400, 370, 4682, 11, 510, 311, 264, 3318, 2445, 1854, 2370, 1301, 13, 8014, 338, 11, 689, 1203, 5314], "temperature": 0.0, "avg_logprob": -0.1814408362666263, "compression_ratio": 1.6444444444444444, "no_speech_prob": 1.4593711057386827e-06}, {"id": 807, "seek": 381668, "start": 3828.0, "end": 3833.2799999999997, "text": " up eventually, which goes through each epoch, and then it creates an iterator from the data", "tokens": [493, 4728, 11, 597, 1709, 807, 1184, 30992, 339, 11, 293, 550, 309, 7829, 364, 17138, 1639, 490, 264, 1412], "temperature": 0.0, "avg_logprob": -0.1814408362666263, "compression_ratio": 1.6444444444444444, "no_speech_prob": 1.4593711057386827e-06}, {"id": 808, "seek": 381668, "start": 3833.2799999999997, "end": 3836.2799999999997, "text": " loader, and then it just does a for loop through it.", "tokens": [3677, 260, 11, 293, 550, 309, 445, 775, 257, 337, 6367, 807, 309, 13], "temperature": 0.0, "avg_logprob": -0.1814408362666263, "compression_ratio": 1.6444444444444444, "no_speech_prob": 1.4593711057386827e-06}, {"id": 809, "seek": 381668, "start": 3836.2799999999997, "end": 3840.12, "text": " So anything you can do a for loop through can be a data loader.", "tokens": [407, 1340, 291, 393, 360, 257, 337, 6367, 807, 393, 312, 257, 1412, 3677, 260, 13], "temperature": 0.0, "avg_logprob": -0.1814408362666263, "compression_ratio": 1.6444444444444444, "no_speech_prob": 1.4593711057386827e-06}, {"id": 810, "seek": 384012, "start": 3840.12, "end": 3849.3599999999997, "text": " Basically it needs to return tuples of many batches, independent variable for many batches.", "tokens": [8537, 309, 2203, 281, 2736, 2604, 2622, 295, 867, 15245, 279, 11, 6695, 7006, 337, 867, 15245, 279, 13], "temperature": 0.0, "avg_logprob": -0.22726985386439733, "compression_ratio": 1.5608695652173914, "no_speech_prob": 1.0677003956516273e-06}, {"id": 811, "seek": 384012, "start": 3849.3599999999997, "end": 3857.64, "text": " So anything with a dunder-etter method is something that can act as an iterator.", "tokens": [407, 1340, 365, 257, 274, 6617, 12, 27296, 3170, 307, 746, 300, 393, 605, 382, 364, 17138, 1639, 13], "temperature": 0.0, "avg_logprob": -0.22726985386439733, "compression_ratio": 1.5608695652173914, "no_speech_prob": 1.0677003956516273e-06}, {"id": 812, "seek": 384012, "start": 3857.64, "end": 3861.02, "text": " And yield is a neat little Python keyword.", "tokens": [400, 11257, 307, 257, 10654, 707, 15329, 20428, 13], "temperature": 0.0, "avg_logprob": -0.22726985386439733, "compression_ratio": 1.5608695652173914, "no_speech_prob": 1.0677003956516273e-06}, {"id": 813, "seek": 384012, "start": 3861.02, "end": 3864.24, "text": " You probably should learn about it if you don't already know it, but it basically spits", "tokens": [509, 1391, 820, 1466, 466, 309, 498, 291, 500, 380, 1217, 458, 309, 11, 457, 309, 1936, 637, 1208], "temperature": 0.0, "avg_logprob": -0.22726985386439733, "compression_ratio": 1.5608695652173914, "no_speech_prob": 1.0677003956516273e-06}, {"id": 814, "seek": 384012, "start": 3864.24, "end": 3868.04, "text": " out a thing and waits for you to ask for another thing.", "tokens": [484, 257, 551, 293, 40597, 337, 291, 281, 1029, 337, 1071, 551, 13], "temperature": 0.0, "avg_logprob": -0.22726985386439733, "compression_ratio": 1.5608695652173914, "no_speech_prob": 1.0677003956516273e-06}, {"id": 815, "seek": 386804, "start": 3868.04, "end": 3878.64, "text": " So in this case, we start by initializing the language model, passing it in the numbers.", "tokens": [407, 294, 341, 1389, 11, 321, 722, 538, 5883, 3319, 264, 2856, 2316, 11, 8437, 309, 294, 264, 3547, 13], "temperature": 0.0, "avg_logprob": -0.15537346326387846, "compression_ratio": 1.5957446808510638, "no_speech_prob": 4.157352123002056e-06}, {"id": 816, "seek": 386804, "start": 3878.64, "end": 3886.08, "text": " So this is the numericalized big long list of all of our documents concatenated together.", "tokens": [407, 341, 307, 264, 29054, 1602, 955, 938, 1329, 295, 439, 295, 527, 8512, 1588, 7186, 770, 1214, 13], "temperature": 0.0, "avg_logprob": -0.15537346326387846, "compression_ratio": 1.5957446808510638, "no_speech_prob": 4.157352123002056e-06}, {"id": 817, "seek": 386804, "start": 3886.08, "end": 3889.88, "text": " And the first thing we do is to batchify it.", "tokens": [400, 264, 700, 551, 321, 360, 307, 281, 15245, 2505, 309, 13], "temperature": 0.0, "avg_logprob": -0.15537346326387846, "compression_ratio": 1.5957446808510638, "no_speech_prob": 4.157352123002056e-06}, {"id": 818, "seek": 386804, "start": 3889.88, "end": 3894.98, "text": " And this is the thing which quite a few of you got confused about last time.", "tokens": [400, 341, 307, 264, 551, 597, 1596, 257, 1326, 295, 291, 658, 9019, 466, 1036, 565, 13], "temperature": 0.0, "avg_logprob": -0.15537346326387846, "compression_ratio": 1.5957446808510638, "no_speech_prob": 4.157352123002056e-06}, {"id": 819, "seek": 389498, "start": 3894.98, "end": 3910.52, "text": " If our batch size is 64, and we have 25 million numbers in our list, we are not creating items", "tokens": [759, 527, 15245, 2744, 307, 12145, 11, 293, 321, 362, 3552, 2459, 3547, 294, 527, 1329, 11, 321, 366, 406, 4084, 4754], "temperature": 0.0, "avg_logprob": -0.121810859357807, "compression_ratio": 1.4037267080745341, "no_speech_prob": 3.905472112819552e-06}, {"id": 820, "seek": 389498, "start": 3910.52, "end": 3911.52, "text": " of length 64.", "tokens": [295, 4641, 12145, 13], "temperature": 0.0, "avg_logprob": -0.121810859357807, "compression_ratio": 1.4037267080745341, "no_speech_prob": 3.905472112819552e-06}, {"id": 821, "seek": 389498, "start": 3911.52, "end": 3913.44, "text": " We're not doing that.", "tokens": [492, 434, 406, 884, 300, 13], "temperature": 0.0, "avg_logprob": -0.121810859357807, "compression_ratio": 1.4037267080745341, "no_speech_prob": 3.905472112819552e-06}, {"id": 822, "seek": 389498, "start": 3913.44, "end": 3916.46, "text": " We're creating 64 items in total.", "tokens": [492, 434, 4084, 12145, 4754, 294, 3217, 13], "temperature": 0.0, "avg_logprob": -0.121810859357807, "compression_ratio": 1.4037267080745341, "no_speech_prob": 3.905472112819552e-06}, {"id": 823, "seek": 389498, "start": 3916.46, "end": 3923.14, "text": " So each of them is of size t divided by 64, which is 390,000.", "tokens": [407, 1184, 295, 552, 307, 295, 2744, 256, 6666, 538, 12145, 11, 597, 307, 805, 7771, 11, 1360, 13], "temperature": 0.0, "avg_logprob": -0.121810859357807, "compression_ratio": 1.4037267080745341, "no_speech_prob": 3.905472112819552e-06}, {"id": 824, "seek": 392314, "start": 3923.14, "end": 3934.92, "text": " So that's what we do here when we reshape it so that this axis here is of length 64,", "tokens": [407, 300, 311, 437, 321, 360, 510, 562, 321, 725, 42406, 309, 370, 300, 341, 10298, 510, 307, 295, 4641, 12145, 11], "temperature": 0.0, "avg_logprob": -0.12439830214888961, "compression_ratio": 1.5843373493975903, "no_speech_prob": 6.144141934782965e-06}, {"id": 825, "seek": 392314, "start": 3934.92, "end": 3937.3599999999997, "text": " and then this minus 1 is everything else.", "tokens": [293, 550, 341, 3175, 502, 307, 1203, 1646, 13], "temperature": 0.0, "avg_logprob": -0.12439830214888961, "compression_ratio": 1.5843373493975903, "no_speech_prob": 6.144141934782965e-06}, {"id": 826, "seek": 392314, "start": 3937.3599999999997, "end": 3942.64, "text": " So that's 390,000 long.", "tokens": [407, 300, 311, 805, 7771, 11, 1360, 938, 13], "temperature": 0.0, "avg_logprob": -0.12439830214888961, "compression_ratio": 1.5843373493975903, "no_speech_prob": 6.144141934782965e-06}, {"id": 827, "seek": 392314, "start": 3942.64, "end": 3944.58, "text": " And then we transpose it.", "tokens": [400, 550, 321, 25167, 309, 13], "temperature": 0.0, "avg_logprob": -0.12439830214888961, "compression_ratio": 1.5843373493975903, "no_speech_prob": 6.144141934782965e-06}, {"id": 828, "seek": 392314, "start": 3944.58, "end": 3952.6, "text": " So that means that we now have 64 columns, 390,000 rows, and then what we do each time", "tokens": [407, 300, 1355, 300, 321, 586, 362, 12145, 13766, 11, 805, 7771, 11, 1360, 13241, 11, 293, 550, 437, 321, 360, 1184, 565], "temperature": 0.0, "avg_logprob": -0.12439830214888961, "compression_ratio": 1.5843373493975903, "no_speech_prob": 6.144141934782965e-06}, {"id": 829, "seek": 395260, "start": 3952.6, "end": 3960.12, "text": " we do an iterate is we grab one batch of some sequence length, we'll look at that in a moment,", "tokens": [321, 360, 364, 44497, 307, 321, 4444, 472, 15245, 295, 512, 8310, 4641, 11, 321, 603, 574, 412, 300, 294, 257, 1623, 11], "temperature": 0.0, "avg_logprob": -0.21744172275066376, "compression_ratio": 1.3609467455621302, "no_speech_prob": 3.500848833937198e-06}, {"id": 830, "seek": 395260, "start": 3960.12, "end": 3969.88, "text": " but basically it's approximately equal to BPTT, which we set to 70, stands for back", "tokens": [457, 1936, 309, 311, 10447, 2681, 281, 40533, 28178, 11, 597, 321, 992, 281, 5285, 11, 7382, 337, 646], "temperature": 0.0, "avg_logprob": -0.21744172275066376, "compression_ratio": 1.3609467455621302, "no_speech_prob": 3.500848833937198e-06}, {"id": 831, "seek": 395260, "start": 3969.88, "end": 3973.5, "text": " prop through time.", "tokens": [2365, 807, 565, 13], "temperature": 0.0, "avg_logprob": -0.21744172275066376, "compression_ratio": 1.3609467455621302, "no_speech_prob": 3.500848833937198e-06}, {"id": 832, "seek": 395260, "start": 3973.5, "end": 3978.3199999999997, "text": " And we just grab that many rows.", "tokens": [400, 321, 445, 4444, 300, 867, 13241, 13], "temperature": 0.0, "avg_logprob": -0.21744172275066376, "compression_ratio": 1.3609467455621302, "no_speech_prob": 3.500848833937198e-06}, {"id": 833, "seek": 397832, "start": 3978.32, "end": 3986.2400000000002, "text": " So from i to i plus 70 rows, and then we try to predict that plus 1.", "tokens": [407, 490, 741, 281, 741, 1804, 5285, 13241, 11, 293, 550, 321, 853, 281, 6069, 300, 1804, 502, 13], "temperature": 0.0, "avg_logprob": -0.19821653610620743, "compression_ratio": 1.5146198830409356, "no_speech_prob": 1.6536846487724688e-06}, {"id": 834, "seek": 397832, "start": 3986.2400000000002, "end": 3990.0800000000004, "text": " So we're trying to predict 1 past where we're up to.", "tokens": [407, 321, 434, 1382, 281, 6069, 502, 1791, 689, 321, 434, 493, 281, 13], "temperature": 0.0, "avg_logprob": -0.19821653610620743, "compression_ratio": 1.5146198830409356, "no_speech_prob": 1.6536846487724688e-06}, {"id": 835, "seek": 397832, "start": 3990.0800000000004, "end": 4000.2000000000003, "text": " So we've got 64 columns, and each of those is 1 64th of our 25 million tokens, hundreds", "tokens": [407, 321, 600, 658, 12145, 13766, 11, 293, 1184, 295, 729, 307, 502, 12145, 392, 295, 527, 3552, 2459, 22667, 11, 6779], "temperature": 0.0, "avg_logprob": -0.19821653610620743, "compression_ratio": 1.5146198830409356, "no_speech_prob": 1.6536846487724688e-06}, {"id": 836, "seek": 397832, "start": 4000.2000000000003, "end": 4004.92, "text": " of thousands long, and we just grab 70 at a time.", "tokens": [295, 5383, 938, 11, 293, 321, 445, 4444, 5285, 412, 257, 565, 13], "temperature": 0.0, "avg_logprob": -0.19821653610620743, "compression_ratio": 1.5146198830409356, "no_speech_prob": 1.6536846487724688e-06}, {"id": 837, "seek": 400492, "start": 4004.92, "end": 4011.36, "text": " So each of those columns, each time we grab it, is going to hook up to the previous column.", "tokens": [407, 1184, 295, 729, 13766, 11, 1184, 565, 321, 4444, 309, 11, 307, 516, 281, 6328, 493, 281, 264, 3894, 7738, 13], "temperature": 0.0, "avg_logprob": -0.20588105300377155, "compression_ratio": 1.5046728971962617, "no_speech_prob": 1.1189420092705404e-06}, {"id": 838, "seek": 400492, "start": 4011.36, "end": 4014.32, "text": " And so that's why we get this consistency, this language model.", "tokens": [400, 370, 300, 311, 983, 321, 483, 341, 14416, 11, 341, 2856, 2316, 13], "temperature": 0.0, "avg_logprob": -0.20588105300377155, "compression_ratio": 1.5046728971962617, "no_speech_prob": 1.1189420092705404e-06}, {"id": 839, "seek": 400492, "start": 4014.32, "end": 4019.64, "text": " It's stateful, it's really important.", "tokens": [467, 311, 1785, 906, 11, 309, 311, 534, 1021, 13], "temperature": 0.0, "avg_logprob": -0.20588105300377155, "compression_ratio": 1.5046728971962617, "no_speech_prob": 1.1189420092705404e-06}, {"id": 840, "seek": 400492, "start": 4019.64, "end": 4026.6, "text": " Pretty much all the cool stuff in the language model is stolen from Stephen Meridy's AWD", "tokens": [10693, 709, 439, 264, 1627, 1507, 294, 264, 2856, 2316, 307, 15900, 490, 13391, 6124, 38836, 311, 25815, 35], "temperature": 0.0, "avg_logprob": -0.20588105300377155, "compression_ratio": 1.5046728971962617, "no_speech_prob": 1.1189420092705404e-06}, {"id": 841, "seek": 400492, "start": 4026.6, "end": 4032.08, "text": " LSTM, including this little trick here.", "tokens": [441, 6840, 44, 11, 3009, 341, 707, 4282, 510, 13], "temperature": 0.0, "avg_logprob": -0.20588105300377155, "compression_ratio": 1.5046728971962617, "no_speech_prob": 1.1189420092705404e-06}, {"id": 842, "seek": 403208, "start": 4032.08, "end": 4037.3199999999997, "text": " If we always grab 70 at a time, and then we go back and we do a new epoch, we're going", "tokens": [759, 321, 1009, 4444, 5285, 412, 257, 565, 11, 293, 550, 321, 352, 646, 293, 321, 360, 257, 777, 30992, 339, 11, 321, 434, 516], "temperature": 0.0, "avg_logprob": -0.16571539173955502, "compression_ratio": 1.6790123456790123, "no_speech_prob": 5.3381422731035855e-06}, {"id": 843, "seek": 403208, "start": 4037.3199999999997, "end": 4040.52, "text": " to grab exactly the same batches every time.", "tokens": [281, 4444, 2293, 264, 912, 15245, 279, 633, 565, 13], "temperature": 0.0, "avg_logprob": -0.16571539173955502, "compression_ratio": 1.6790123456790123, "no_speech_prob": 5.3381422731035855e-06}, {"id": 844, "seek": 403208, "start": 4040.52, "end": 4041.52, "text": " There's no randomness.", "tokens": [821, 311, 572, 4974, 1287, 13], "temperature": 0.0, "avg_logprob": -0.16571539173955502, "compression_ratio": 1.6790123456790123, "no_speech_prob": 5.3381422731035855e-06}, {"id": 845, "seek": 403208, "start": 4041.52, "end": 4046.7999999999997, "text": " Now normally we shuffle our data every time we do an epoch, or every time we grab some", "tokens": [823, 5646, 321, 39426, 527, 1412, 633, 565, 321, 360, 364, 30992, 339, 11, 420, 633, 565, 321, 4444, 512], "temperature": 0.0, "avg_logprob": -0.16571539173955502, "compression_ratio": 1.6790123456790123, "no_speech_prob": 5.3381422731035855e-06}, {"id": 846, "seek": 403208, "start": 4046.7999999999997, "end": 4047.7999999999997, "text": " data, we grab it at random.", "tokens": [1412, 11, 321, 4444, 309, 412, 4974, 13], "temperature": 0.0, "avg_logprob": -0.16571539173955502, "compression_ratio": 1.6790123456790123, "no_speech_prob": 5.3381422731035855e-06}, {"id": 847, "seek": 403208, "start": 4047.7999999999997, "end": 4053.7999999999997, "text": " But you can't do that with a language model because this set has to join up to the previous", "tokens": [583, 291, 393, 380, 360, 300, 365, 257, 2856, 2316, 570, 341, 992, 575, 281, 3917, 493, 281, 264, 3894], "temperature": 0.0, "avg_logprob": -0.16571539173955502, "compression_ratio": 1.6790123456790123, "no_speech_prob": 5.3381422731035855e-06}, {"id": 848, "seek": 403208, "start": 4053.7999999999997, "end": 4058.08, "text": " set because it's trying to learn the sentence.", "tokens": [992, 570, 309, 311, 1382, 281, 1466, 264, 8174, 13], "temperature": 0.0, "avg_logprob": -0.16571539173955502, "compression_ratio": 1.6790123456790123, "no_speech_prob": 5.3381422731035855e-06}, {"id": 849, "seek": 405808, "start": 4058.08, "end": 4063.44, "text": " And if you suddenly jump somewhere else, then that doesn't make any sense as a sentence.", "tokens": [400, 498, 291, 5800, 3012, 4079, 1646, 11, 550, 300, 1177, 380, 652, 604, 2020, 382, 257, 8174, 13], "temperature": 0.0, "avg_logprob": -0.13532767825656467, "compression_ratio": 1.4952380952380953, "no_speech_prob": 4.029446245112922e-06}, {"id": 850, "seek": 405808, "start": 4063.44, "end": 4071.42, "text": " So Stephen's idea is to say, since we can't shuffle the order, let's instead randomly", "tokens": [407, 13391, 311, 1558, 307, 281, 584, 11, 1670, 321, 393, 380, 39426, 264, 1668, 11, 718, 311, 2602, 16979], "temperature": 0.0, "avg_logprob": -0.13532767825656467, "compression_ratio": 1.4952380952380953, "no_speech_prob": 4.029446245112922e-06}, {"id": 851, "seek": 405808, "start": 4071.42, "end": 4075.08, "text": " change the size, the sequence length.", "tokens": [1319, 264, 2744, 11, 264, 8310, 4641, 13], "temperature": 0.0, "avg_logprob": -0.13532767825656467, "compression_ratio": 1.4952380952380953, "no_speech_prob": 4.029446245112922e-06}, {"id": 852, "seek": 405808, "start": 4075.08, "end": 4083.7999999999997, "text": " And so basically he says, 95% of the time we'll use BPTT, 70, but 5% of the time we'll", "tokens": [400, 370, 1936, 415, 1619, 11, 13420, 4, 295, 264, 565, 321, 603, 764, 40533, 28178, 11, 5285, 11, 457, 1025, 4, 295, 264, 565, 321, 603], "temperature": 0.0, "avg_logprob": -0.13532767825656467, "compression_ratio": 1.4952380952380953, "no_speech_prob": 4.029446245112922e-06}, {"id": 853, "seek": 405808, "start": 4083.7999999999997, "end": 4086.6, "text": " use half that.", "tokens": [764, 1922, 300, 13], "temperature": 0.0, "avg_logprob": -0.13532767825656467, "compression_ratio": 1.4952380952380953, "no_speech_prob": 4.029446245112922e-06}, {"id": 854, "seek": 408660, "start": 4086.6, "end": 4091.48, "text": " And then he says, you know what, I'm not even going to make that the sequence length, I'm", "tokens": [400, 550, 415, 1619, 11, 291, 458, 437, 11, 286, 478, 406, 754, 516, 281, 652, 300, 264, 8310, 4641, 11, 286, 478], "temperature": 0.0, "avg_logprob": -0.15610899644739487, "compression_ratio": 1.6864406779661016, "no_speech_prob": 3.3405217436666135e-06}, {"id": 855, "seek": 408660, "start": 4091.48, "end": 4097.08, "text": " going to create a normally distributed random number with that average and a standard deviation", "tokens": [516, 281, 1884, 257, 5646, 12631, 4974, 1230, 365, 300, 4274, 293, 257, 3832, 25163], "temperature": 0.0, "avg_logprob": -0.15610899644739487, "compression_ratio": 1.6864406779661016, "no_speech_prob": 3.3405217436666135e-06}, {"id": 856, "seek": 408660, "start": 4097.08, "end": 4100.12, "text": " of 5, and I'll make that the sequence length.", "tokens": [295, 1025, 11, 293, 286, 603, 652, 300, 264, 8310, 4641, 13], "temperature": 0.0, "avg_logprob": -0.15610899644739487, "compression_ratio": 1.6864406779661016, "no_speech_prob": 3.3405217436666135e-06}, {"id": 857, "seek": 408660, "start": 4100.12, "end": 4104.48, "text": " So the sequence length is 70-ish.", "tokens": [407, 264, 8310, 4641, 307, 5285, 12, 742, 13], "temperature": 0.0, "avg_logprob": -0.15610899644739487, "compression_ratio": 1.6864406779661016, "no_speech_prob": 3.3405217436666135e-06}, {"id": 858, "seek": 408660, "start": 4104.48, "end": 4109.68, "text": " And that means every time we go through, we're getting slightly different batches.", "tokens": [400, 300, 1355, 633, 565, 321, 352, 807, 11, 321, 434, 1242, 4748, 819, 15245, 279, 13], "temperature": 0.0, "avg_logprob": -0.15610899644739487, "compression_ratio": 1.6864406779661016, "no_speech_prob": 3.3405217436666135e-06}, {"id": 859, "seek": 408660, "start": 4109.68, "end": 4113.44, "text": " So we've got that little bit of extra randomness.", "tokens": [407, 321, 600, 658, 300, 707, 857, 295, 2857, 4974, 1287, 13], "temperature": 0.0, "avg_logprob": -0.15610899644739487, "compression_ratio": 1.6864406779661016, "no_speech_prob": 3.3405217436666135e-06}, {"id": 860, "seek": 411344, "start": 4113.44, "end": 4118.799999999999, "text": " I asked Stephen Meridy where he came up with this idea, did he think of it, and he was", "tokens": [286, 2351, 13391, 6124, 38836, 689, 415, 1361, 493, 365, 341, 1558, 11, 630, 415, 519, 295, 309, 11, 293, 415, 390], "temperature": 0.0, "avg_logprob": -0.15338530907264122, "compression_ratio": 1.8242677824267783, "no_speech_prob": 2.9772545531159267e-05}, {"id": 861, "seek": 411344, "start": 4118.799999999999, "end": 4125.16, "text": " like, I think I thought of it, but it seems so obvious that I bet I didn't think of it.", "tokens": [411, 11, 286, 519, 286, 1194, 295, 309, 11, 457, 309, 2544, 370, 6322, 300, 286, 778, 286, 994, 380, 519, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.15338530907264122, "compression_ratio": 1.8242677824267783, "no_speech_prob": 2.9772545531159267e-05}, {"id": 862, "seek": 411344, "start": 4125.16, "end": 4128.599999999999, "text": " Which is true of every time I come up with an idea in deep learning, it always seems", "tokens": [3013, 307, 2074, 295, 633, 565, 286, 808, 493, 365, 364, 1558, 294, 2452, 2539, 11, 309, 1009, 2544], "temperature": 0.0, "avg_logprob": -0.15338530907264122, "compression_ratio": 1.8242677824267783, "no_speech_prob": 2.9772545531159267e-05}, {"id": 863, "seek": 411344, "start": 4128.599999999999, "end": 4135.44, "text": " so obvious that you assume somebody else has thought of it, but I think he thought of it.", "tokens": [370, 6322, 300, 291, 6552, 2618, 1646, 575, 1194, 295, 309, 11, 457, 286, 519, 415, 1194, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.15338530907264122, "compression_ratio": 1.8242677824267783, "no_speech_prob": 2.9772545531159267e-05}, {"id": 864, "seek": 411344, "start": 4135.44, "end": 4141.599999999999, "text": " So this is a nice thing to look at if you're trying to do something a bit unusual with", "tokens": [407, 341, 307, 257, 1481, 551, 281, 574, 412, 498, 291, 434, 1382, 281, 360, 746, 257, 857, 10901, 365], "temperature": 0.0, "avg_logprob": -0.15338530907264122, "compression_ratio": 1.8242677824267783, "no_speech_prob": 2.9772545531159267e-05}, {"id": 865, "seek": 414160, "start": 4141.6, "end": 4147.860000000001, "text": " a data loader, it's like, okay, here's a simple kind of role model you can use as to creating", "tokens": [257, 1412, 3677, 260, 11, 309, 311, 411, 11, 1392, 11, 510, 311, 257, 2199, 733, 295, 3090, 2316, 291, 393, 764, 382, 281, 4084], "temperature": 0.0, "avg_logprob": -0.1359530073223692, "compression_ratio": 1.5940170940170941, "no_speech_prob": 5.771895303041674e-06}, {"id": 866, "seek": 414160, "start": 4147.860000000001, "end": 4155.68, "text": " a data loader from scratch, something that spits out batches of data.", "tokens": [257, 1412, 3677, 260, 490, 8459, 11, 746, 300, 637, 1208, 484, 15245, 279, 295, 1412, 13], "temperature": 0.0, "avg_logprob": -0.1359530073223692, "compression_ratio": 1.5940170940170941, "no_speech_prob": 5.771895303041674e-06}, {"id": 867, "seek": 414160, "start": 4155.68, "end": 4160.360000000001, "text": " So our language model loader just took in all of the documents concatenated together", "tokens": [407, 527, 2856, 2316, 3677, 260, 445, 1890, 294, 439, 295, 264, 8512, 1588, 7186, 770, 1214], "temperature": 0.0, "avg_logprob": -0.1359530073223692, "compression_ratio": 1.5940170940170941, "no_speech_prob": 5.771895303041674e-06}, {"id": 868, "seek": 414160, "start": 4160.360000000001, "end": 4164.0, "text": " along with the batch size and the BPTT.", "tokens": [2051, 365, 264, 15245, 2744, 293, 264, 40533, 28178, 13], "temperature": 0.0, "avg_logprob": -0.1359530073223692, "compression_ratio": 1.5940170940170941, "no_speech_prob": 5.771895303041674e-06}, {"id": 869, "seek": 414160, "start": 4164.0, "end": 4168.72, "text": " Now generally speaking, we want to create a learner, and the way we normally do that", "tokens": [823, 5101, 4124, 11, 321, 528, 281, 1884, 257, 33347, 11, 293, 264, 636, 321, 5646, 360, 300], "temperature": 0.0, "avg_logprob": -0.1359530073223692, "compression_ratio": 1.5940170940170941, "no_speech_prob": 5.771895303041674e-06}, {"id": 870, "seek": 416872, "start": 4168.72, "end": 4174.400000000001, "text": " is by getting a model data object and by calling some kind of method which have various names,", "tokens": [307, 538, 1242, 257, 2316, 1412, 2657, 293, 538, 5141, 512, 733, 295, 3170, 597, 362, 3683, 5288, 11], "temperature": 0.0, "avg_logprob": -0.12926018471811332, "compression_ratio": 1.7363636363636363, "no_speech_prob": 1.1015912377843051e-06}, {"id": 871, "seek": 416872, "start": 4174.400000000001, "end": 4177.56, "text": " but often we call that method getModel.", "tokens": [457, 2049, 321, 818, 300, 3170, 483, 44, 41147, 13], "temperature": 0.0, "avg_logprob": -0.12926018471811332, "compression_ratio": 1.7363636363636363, "no_speech_prob": 1.1015912377843051e-06}, {"id": 872, "seek": 416872, "start": 4177.56, "end": 4182.280000000001, "text": " And so the idea is that the model data object has enough information to know what kind of", "tokens": [400, 370, 264, 1558, 307, 300, 264, 2316, 1412, 2657, 575, 1547, 1589, 281, 458, 437, 733, 295], "temperature": 0.0, "avg_logprob": -0.12926018471811332, "compression_ratio": 1.7363636363636363, "no_speech_prob": 1.1015912377843051e-06}, {"id": 873, "seek": 416872, "start": 4182.280000000001, "end": 4184.16, "text": " model to give you.", "tokens": [2316, 281, 976, 291, 13], "temperature": 0.0, "avg_logprob": -0.12926018471811332, "compression_ratio": 1.7363636363636363, "no_speech_prob": 1.1015912377843051e-06}, {"id": 874, "seek": 416872, "start": 4184.16, "end": 4192.52, "text": " So we have to create that model data object, which means we need that class.", "tokens": [407, 321, 362, 281, 1884, 300, 2316, 1412, 2657, 11, 597, 1355, 321, 643, 300, 1508, 13], "temperature": 0.0, "avg_logprob": -0.12926018471811332, "compression_ratio": 1.7363636363636363, "no_speech_prob": 1.1015912377843051e-06}, {"id": 875, "seek": 416872, "start": 4192.52, "end": 4195.88, "text": " And so that's very easy to do.", "tokens": [400, 370, 300, 311, 588, 1858, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.12926018471811332, "compression_ratio": 1.7363636363636363, "no_speech_prob": 1.1015912377843051e-06}, {"id": 876, "seek": 416872, "start": 4195.88, "end": 4197.6, "text": " So here are all of the pieces.", "tokens": [407, 510, 366, 439, 295, 264, 3755, 13], "temperature": 0.0, "avg_logprob": -0.12926018471811332, "compression_ratio": 1.7363636363636363, "no_speech_prob": 1.1015912377843051e-06}, {"id": 877, "seek": 419760, "start": 4197.6, "end": 4202.240000000001, "text": " We're going to create a custom learner, a custom model data class, and a custom model", "tokens": [492, 434, 516, 281, 1884, 257, 2375, 33347, 11, 257, 2375, 2316, 1412, 1508, 11, 293, 257, 2375, 2316], "temperature": 0.0, "avg_logprob": -0.17639776717784794, "compression_ratio": 1.8312236286919832, "no_speech_prob": 4.637857728084782e-06}, {"id": 878, "seek": 419760, "start": 4202.240000000001, "end": 4204.620000000001, "text": " class.", "tokens": [1508, 13], "temperature": 0.0, "avg_logprob": -0.17639776717784794, "compression_ratio": 1.8312236286919832, "no_speech_prob": 4.637857728084782e-06}, {"id": 879, "seek": 419760, "start": 4204.620000000001, "end": 4209.8, "text": " So a model data class, again, this one doesn't inherit from anything, so you really see there's", "tokens": [407, 257, 2316, 1412, 1508, 11, 797, 11, 341, 472, 1177, 380, 21389, 490, 1340, 11, 370, 291, 534, 536, 456, 311], "temperature": 0.0, "avg_logprob": -0.17639776717784794, "compression_ratio": 1.8312236286919832, "no_speech_prob": 4.637857728084782e-06}, {"id": 880, "seek": 419760, "start": 4209.8, "end": 4212.72, "text": " almost nothing to do.", "tokens": [1920, 1825, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.17639776717784794, "compression_ratio": 1.8312236286919832, "no_speech_prob": 4.637857728084782e-06}, {"id": 881, "seek": 419760, "start": 4212.72, "end": 4215.76, "text": " You need to tell it, most importantly, what's your training set?", "tokens": [509, 643, 281, 980, 309, 11, 881, 8906, 11, 437, 311, 428, 3097, 992, 30], "temperature": 0.0, "avg_logprob": -0.17639776717784794, "compression_ratio": 1.8312236286919832, "no_speech_prob": 4.637857728084782e-06}, {"id": 882, "seek": 419760, "start": 4215.76, "end": 4217.92, "text": " Give it a data loader.", "tokens": [5303, 309, 257, 1412, 3677, 260, 13], "temperature": 0.0, "avg_logprob": -0.17639776717784794, "compression_ratio": 1.8312236286919832, "no_speech_prob": 4.637857728084782e-06}, {"id": 883, "seek": 419760, "start": 4217.92, "end": 4219.360000000001, "text": " What's the validation set?", "tokens": [708, 311, 264, 24071, 992, 30], "temperature": 0.0, "avg_logprob": -0.17639776717784794, "compression_ratio": 1.8312236286919832, "no_speech_prob": 4.637857728084782e-06}, {"id": 884, "seek": 419760, "start": 4219.360000000001, "end": 4220.64, "text": " Give it a data loader.", "tokens": [5303, 309, 257, 1412, 3677, 260, 13], "temperature": 0.0, "avg_logprob": -0.17639776717784794, "compression_ratio": 1.8312236286919832, "no_speech_prob": 4.637857728084782e-06}, {"id": 885, "seek": 419760, "start": 4220.64, "end": 4222.8, "text": " And optionally, give it a test set.", "tokens": [400, 3614, 379, 11, 976, 309, 257, 1500, 992, 13], "temperature": 0.0, "avg_logprob": -0.17639776717784794, "compression_ratio": 1.8312236286919832, "no_speech_prob": 4.637857728084782e-06}, {"id": 886, "seek": 419760, "start": 4222.8, "end": 4224.04, "text": " Data loader.", "tokens": [11888, 3677, 260, 13], "temperature": 0.0, "avg_logprob": -0.17639776717784794, "compression_ratio": 1.8312236286919832, "no_speech_prob": 4.637857728084782e-06}, {"id": 887, "seek": 419760, "start": 4224.04, "end": 4226.92, "text": " Plus anything else it needs to know.", "tokens": [7721, 1340, 1646, 309, 2203, 281, 458, 13], "temperature": 0.0, "avg_logprob": -0.17639776717784794, "compression_ratio": 1.8312236286919832, "no_speech_prob": 4.637857728084782e-06}, {"id": 888, "seek": 422692, "start": 4226.92, "end": 4233.6, "text": " So it might need to know the BPTT.", "tokens": [407, 309, 1062, 643, 281, 458, 264, 40533, 28178, 13], "temperature": 0.0, "avg_logprob": -0.13691363511262117, "compression_ratio": 1.6886792452830188, "no_speech_prob": 1.482355514781375e-06}, {"id": 889, "seek": 422692, "start": 4233.6, "end": 4238.28, "text": " It needs to know the number of tokens, that's the vocab size.", "tokens": [467, 2203, 281, 458, 264, 1230, 295, 22667, 11, 300, 311, 264, 2329, 455, 2744, 13], "temperature": 0.0, "avg_logprob": -0.13691363511262117, "compression_ratio": 1.6886792452830188, "no_speech_prob": 1.482355514781375e-06}, {"id": 890, "seek": 422692, "start": 4238.28, "end": 4243.0, "text": " It needs to know what is the padding index.", "tokens": [467, 2203, 281, 458, 437, 307, 264, 39562, 8186, 13], "temperature": 0.0, "avg_logprob": -0.13691363511262117, "compression_ratio": 1.6886792452830188, "no_speech_prob": 1.482355514781375e-06}, {"id": 891, "seek": 422692, "start": 4243.0, "end": 4248.08, "text": " And so that it can save temporary files and models, model data always needs to know the", "tokens": [400, 370, 300, 309, 393, 3155, 13413, 7098, 293, 5245, 11, 2316, 1412, 1009, 2203, 281, 458, 264], "temperature": 0.0, "avg_logprob": -0.13691363511262117, "compression_ratio": 1.6886792452830188, "no_speech_prob": 1.482355514781375e-06}, {"id": 892, "seek": 422692, "start": 4248.08, "end": 4249.08, "text": " path.", "tokens": [3100, 13], "temperature": 0.0, "avg_logprob": -0.13691363511262117, "compression_ratio": 1.6886792452830188, "no_speech_prob": 1.482355514781375e-06}, {"id": 893, "seek": 422692, "start": 4249.08, "end": 4252.04, "text": " So we just grab all that stuff and we dump it.", "tokens": [407, 321, 445, 4444, 439, 300, 1507, 293, 321, 11430, 309, 13], "temperature": 0.0, "avg_logprob": -0.13691363511262117, "compression_ratio": 1.6886792452830188, "no_speech_prob": 1.482355514781375e-06}, {"id": 894, "seek": 422692, "start": 4252.04, "end": 4253.04, "text": " And that's it.", "tokens": [400, 300, 311, 309, 13], "temperature": 0.0, "avg_logprob": -0.13691363511262117, "compression_ratio": 1.6886792452830188, "no_speech_prob": 1.482355514781375e-06}, {"id": 895, "seek": 422692, "start": 4253.04, "end": 4254.04, "text": " That's the entire initializer.", "tokens": [663, 311, 264, 2302, 5883, 6545, 13], "temperature": 0.0, "avg_logprob": -0.13691363511262117, "compression_ratio": 1.6886792452830188, "no_speech_prob": 1.482355514781375e-06}, {"id": 896, "seek": 422692, "start": 4254.04, "end": 4255.96, "text": " There's no logic there at all.", "tokens": [821, 311, 572, 9952, 456, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.13691363511262117, "compression_ratio": 1.6886792452830188, "no_speech_prob": 1.482355514781375e-06}, {"id": 897, "seek": 425596, "start": 4255.96, "end": 4259.96, "text": " So then all of the work happens inside GetModel.", "tokens": [407, 550, 439, 295, 264, 589, 2314, 1854, 3240, 44, 41147, 13], "temperature": 0.0, "avg_logprob": -0.19597270570952316, "compression_ratio": 1.4114583333333333, "no_speech_prob": 5.594313279289054e-06}, {"id": 898, "seek": 425596, "start": 4259.96, "end": 4267.08, "text": " And so GetModel calls something we'll look at later, which just grabs a normal PyTorch", "tokens": [400, 370, 3240, 44, 41147, 5498, 746, 321, 603, 574, 412, 1780, 11, 597, 445, 30028, 257, 2710, 9953, 51, 284, 339], "temperature": 0.0, "avg_logprob": -0.19597270570952316, "compression_ratio": 1.4114583333333333, "no_speech_prob": 5.594313279289054e-06}, {"id": 899, "seek": 425596, "start": 4267.08, "end": 4274.52, "text": " nn.module architecture and chucks it on the GPU.", "tokens": [297, 77, 13, 8014, 2271, 9482, 293, 417, 15493, 309, 322, 264, 18407, 13], "temperature": 0.0, "avg_logprob": -0.19597270570952316, "compression_ratio": 1.4114583333333333, "no_speech_prob": 5.594313279289054e-06}, {"id": 900, "seek": 425596, "start": 4274.52, "end": 4277.52, "text": " Note with PyTorch, normally we would say.cuda.", "tokens": [11633, 365, 9953, 51, 284, 339, 11, 5646, 321, 576, 584, 2411, 66, 11152, 13], "temperature": 0.0, "avg_logprob": -0.19597270570952316, "compression_ratio": 1.4114583333333333, "no_speech_prob": 5.594313279289054e-06}, {"id": 901, "seek": 425596, "start": 4277.52, "end": 4281.04, "text": " With FastAI, it's better to say to GPU.", "tokens": [2022, 15968, 48698, 11, 309, 311, 1101, 281, 584, 281, 18407, 13], "temperature": 0.0, "avg_logprob": -0.19597270570952316, "compression_ratio": 1.4114583333333333, "no_speech_prob": 5.594313279289054e-06}, {"id": 902, "seek": 428104, "start": 4281.04, "end": 4286.4, "text": " And the reason is that if you don't have a GPU, it'll leave it on the CPU.", "tokens": [400, 264, 1778, 307, 300, 498, 291, 500, 380, 362, 257, 18407, 11, 309, 603, 1856, 309, 322, 264, 13199, 13], "temperature": 0.0, "avg_logprob": -0.12287869351975461, "compression_ratio": 1.64, "no_speech_prob": 2.2252725102589466e-06}, {"id": 903, "seek": 428104, "start": 4286.4, "end": 4291.76, "text": " And it also provides a global variable you can set to choose whether it goes on the GPU", "tokens": [400, 309, 611, 6417, 257, 4338, 7006, 291, 393, 992, 281, 2826, 1968, 309, 1709, 322, 264, 18407], "temperature": 0.0, "avg_logprob": -0.12287869351975461, "compression_ratio": 1.64, "no_speech_prob": 2.2252725102589466e-06}, {"id": 904, "seek": 428104, "start": 4291.76, "end": 4292.76, "text": " or not.", "tokens": [420, 406, 13], "temperature": 0.0, "avg_logprob": -0.12287869351975461, "compression_ratio": 1.64, "no_speech_prob": 2.2252725102589466e-06}, {"id": 905, "seek": 428104, "start": 4292.76, "end": 4295.56, "text": " So it's a better approach.", "tokens": [407, 309, 311, 257, 1101, 3109, 13], "temperature": 0.0, "avg_logprob": -0.12287869351975461, "compression_ratio": 1.64, "no_speech_prob": 2.2252725102589466e-06}, {"id": 906, "seek": 428104, "start": 4295.56, "end": 4298.28, "text": " So we wrap the model in a language model.", "tokens": [407, 321, 7019, 264, 2316, 294, 257, 2856, 2316, 13], "temperature": 0.0, "avg_logprob": -0.12287869351975461, "compression_ratio": 1.64, "no_speech_prob": 2.2252725102589466e-06}, {"id": 907, "seek": 428104, "start": 4298.28, "end": 4300.88, "text": " And the language model is this.", "tokens": [400, 264, 2856, 2316, 307, 341, 13], "temperature": 0.0, "avg_logprob": -0.12287869351975461, "compression_ratio": 1.64, "no_speech_prob": 2.2252725102589466e-06}, {"id": 908, "seek": 428104, "start": 4300.88, "end": 4306.28, "text": " Basically a language model is a subclass of basic model.", "tokens": [8537, 257, 2856, 2316, 307, 257, 1422, 11665, 295, 3875, 2316, 13], "temperature": 0.0, "avg_logprob": -0.12287869351975461, "compression_ratio": 1.64, "no_speech_prob": 2.2252725102589466e-06}, {"id": 909, "seek": 430628, "start": 4306.28, "end": 4311.42, "text": " It basically almost does nothing except it defines layer groups.", "tokens": [467, 1936, 1920, 775, 1825, 3993, 309, 23122, 4583, 3935, 13], "temperature": 0.0, "avg_logprob": -0.13380520886713915, "compression_ratio": 1.9308943089430894, "no_speech_prob": 1.0289435522281565e-05}, {"id": 910, "seek": 430628, "start": 4311.42, "end": 4316.48, "text": " And so remember how when we do like discriminative learning rates where different layers have", "tokens": [400, 370, 1604, 577, 562, 321, 360, 411, 20828, 1166, 2539, 6846, 689, 819, 7914, 362], "temperature": 0.0, "avg_logprob": -0.13380520886713915, "compression_ratio": 1.9308943089430894, "no_speech_prob": 1.0289435522281565e-05}, {"id": 911, "seek": 430628, "start": 4316.48, "end": 4323.84, "text": " different learning rates, or like we freeze different amounts, we don't provide a different", "tokens": [819, 2539, 6846, 11, 420, 411, 321, 15959, 819, 11663, 11, 321, 500, 380, 2893, 257, 819], "temperature": 0.0, "avg_logprob": -0.13380520886713915, "compression_ratio": 1.9308943089430894, "no_speech_prob": 1.0289435522281565e-05}, {"id": 912, "seek": 430628, "start": 4323.84, "end": 4327.719999999999, "text": " learning rate for every layer because there can be like a thousand layers.", "tokens": [2539, 3314, 337, 633, 4583, 570, 456, 393, 312, 411, 257, 4714, 7914, 13], "temperature": 0.0, "avg_logprob": -0.13380520886713915, "compression_ratio": 1.9308943089430894, "no_speech_prob": 1.0289435522281565e-05}, {"id": 913, "seek": 430628, "start": 4327.719999999999, "end": 4331.16, "text": " We provide a different learning rate for every layer group.", "tokens": [492, 2893, 257, 819, 2539, 3314, 337, 633, 4583, 1594, 13], "temperature": 0.0, "avg_logprob": -0.13380520886713915, "compression_ratio": 1.9308943089430894, "no_speech_prob": 1.0289435522281565e-05}, {"id": 914, "seek": 430628, "start": 4331.16, "end": 4336.24, "text": " So when you create a custom model, you just have to override this one thing which returns", "tokens": [407, 562, 291, 1884, 257, 2375, 2316, 11, 291, 445, 362, 281, 42321, 341, 472, 551, 597, 11247], "temperature": 0.0, "avg_logprob": -0.13380520886713915, "compression_ratio": 1.9308943089430894, "no_speech_prob": 1.0289435522281565e-05}, {"id": 915, "seek": 433624, "start": 4336.24, "end": 4341.88, "text": " a list of all of your layer groups.", "tokens": [257, 1329, 295, 439, 295, 428, 4583, 3935, 13], "temperature": 0.0, "avg_logprob": -0.1655782417014793, "compression_ratio": 1.6035502958579881, "no_speech_prob": 8.013462320377585e-06}, {"id": 916, "seek": 433624, "start": 4341.88, "end": 4348.76, "text": " So in this case, my last layer group contains the last part of the model and one bit of", "tokens": [407, 294, 341, 1389, 11, 452, 1036, 4583, 1594, 8306, 264, 1036, 644, 295, 264, 2316, 293, 472, 857, 295], "temperature": 0.0, "avg_logprob": -0.1655782417014793, "compression_ratio": 1.6035502958579881, "no_speech_prob": 8.013462320377585e-06}, {"id": 917, "seek": 433624, "start": 4348.76, "end": 4350.3, "text": " dropout.", "tokens": [3270, 346, 13], "temperature": 0.0, "avg_logprob": -0.1655782417014793, "compression_ratio": 1.6035502958579881, "no_speech_prob": 8.013462320377585e-06}, {"id": 918, "seek": 433624, "start": 4350.3, "end": 4353.44, "text": " And the rest of it, this star here means pull this apart.", "tokens": [400, 264, 1472, 295, 309, 11, 341, 3543, 510, 1355, 2235, 341, 4936, 13], "temperature": 0.0, "avg_logprob": -0.1655782417014793, "compression_ratio": 1.6035502958579881, "no_speech_prob": 8.013462320377585e-06}, {"id": 919, "seek": 433624, "start": 4353.44, "end": 4360.24, "text": " So this is basically going to be one layer per RNN layer.", "tokens": [407, 341, 307, 1936, 516, 281, 312, 472, 4583, 680, 45702, 45, 4583, 13], "temperature": 0.0, "avg_logprob": -0.1655782417014793, "compression_ratio": 1.6035502958579881, "no_speech_prob": 8.013462320377585e-06}, {"id": 920, "seek": 433624, "start": 4360.24, "end": 4362.04, "text": " So that's all that is.", "tokens": [407, 300, 311, 439, 300, 307, 13], "temperature": 0.0, "avg_logprob": -0.1655782417014793, "compression_ratio": 1.6035502958579881, "no_speech_prob": 8.013462320377585e-06}, {"id": 921, "seek": 436204, "start": 4362.04, "end": 4366.84, "text": " And then finally, turn that into a learner.", "tokens": [400, 550, 2721, 11, 1261, 300, 666, 257, 33347, 13], "temperature": 0.0, "avg_logprob": -0.16994197472282077, "compression_ratio": 1.7164179104477613, "no_speech_prob": 2.4060952910076594e-06}, {"id": 922, "seek": 436204, "start": 4366.84, "end": 4371.04, "text": " And so a learner you just pass in the model and it turns it into a learner.", "tokens": [400, 370, 257, 33347, 291, 445, 1320, 294, 264, 2316, 293, 309, 4523, 309, 666, 257, 33347, 13], "temperature": 0.0, "avg_logprob": -0.16994197472282077, "compression_ratio": 1.7164179104477613, "no_speech_prob": 2.4060952910076594e-06}, {"id": 923, "seek": 436204, "start": 4371.04, "end": 4374.04, "text": " In this case we have overridden learner.", "tokens": [682, 341, 1389, 321, 362, 670, 81, 6171, 33347, 13], "temperature": 0.0, "avg_logprob": -0.16994197472282077, "compression_ratio": 1.7164179104477613, "no_speech_prob": 2.4060952910076594e-06}, {"id": 924, "seek": 436204, "start": 4374.04, "end": 4381.16, "text": " And the only thing we've done is to say I want the default loss function to be cross-entropy.", "tokens": [400, 264, 787, 551, 321, 600, 1096, 307, 281, 584, 286, 528, 264, 7576, 4470, 2445, 281, 312, 3278, 12, 317, 27514, 13], "temperature": 0.0, "avg_logprob": -0.16994197472282077, "compression_ratio": 1.7164179104477613, "no_speech_prob": 2.4060952910076594e-06}, {"id": 925, "seek": 436204, "start": 4381.16, "end": 4388.92, "text": " So this entire set of custom model, custom model data, custom learner all fits on a single", "tokens": [407, 341, 2302, 992, 295, 2375, 2316, 11, 2375, 2316, 1412, 11, 2375, 33347, 439, 9001, 322, 257, 2167], "temperature": 0.0, "avg_logprob": -0.16994197472282077, "compression_ratio": 1.7164179104477613, "no_speech_prob": 2.4060952910076594e-06}, {"id": 926, "seek": 438892, "start": 4388.92, "end": 4393.92, "text": " screen and they always basically look like this.", "tokens": [2568, 293, 436, 1009, 1936, 574, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.17227334861295768, "compression_ratio": 1.6229508196721312, "no_speech_prob": 3.966974873037543e-06}, {"id": 927, "seek": 438892, "start": 4393.92, "end": 4399.24, "text": " So that's a kind of little dig inside this pretty boring part of the code base.", "tokens": [407, 300, 311, 257, 733, 295, 707, 2528, 1854, 341, 1238, 9989, 644, 295, 264, 3089, 3096, 13], "temperature": 0.0, "avg_logprob": -0.17227334861295768, "compression_ratio": 1.6229508196721312, "no_speech_prob": 3.966974873037543e-06}, {"id": 928, "seek": 438892, "start": 4399.24, "end": 4403.04, "text": " So the interesting part of this code base is getLanguageModel.", "tokens": [407, 264, 1880, 644, 295, 341, 3089, 3096, 307, 483, 43, 656, 20473, 44, 41147, 13], "temperature": 0.0, "avg_logprob": -0.17227334861295768, "compression_ratio": 1.6229508196721312, "no_speech_prob": 3.966974873037543e-06}, {"id": 929, "seek": 438892, "start": 4403.04, "end": 4410.14, "text": " getLanguageModel is actually the thing that gives us our AWD LSTM.", "tokens": [483, 43, 656, 20473, 44, 41147, 307, 767, 264, 551, 300, 2709, 505, 527, 25815, 35, 441, 6840, 44, 13], "temperature": 0.0, "avg_logprob": -0.17227334861295768, "compression_ratio": 1.6229508196721312, "no_speech_prob": 3.966974873037543e-06}, {"id": 930, "seek": 438892, "start": 4410.14, "end": 4414.32, "text": " And it actually contains the big idea.", "tokens": [400, 309, 767, 8306, 264, 955, 1558, 13], "temperature": 0.0, "avg_logprob": -0.17227334861295768, "compression_ratio": 1.6229508196721312, "no_speech_prob": 3.966974873037543e-06}, {"id": 931, "seek": 441432, "start": 4414.32, "end": 4420.08, "text": " The big incredibly simple idea that everybody else here thinks is really obvious, that everybody", "tokens": [440, 955, 6252, 2199, 1558, 300, 2201, 1646, 510, 7309, 307, 534, 6322, 11, 300, 2201], "temperature": 0.0, "avg_logprob": -0.1467776852984761, "compression_ratio": 1.570754716981132, "no_speech_prob": 6.962211955396924e-06}, {"id": 932, "seek": 441432, "start": 4420.08, "end": 4427.32, "text": " in the NLP community I spoke to thought was insane, which is basically every model can", "tokens": [294, 264, 426, 45196, 1768, 286, 7179, 281, 1194, 390, 10838, 11, 597, 307, 1936, 633, 2316, 393], "temperature": 0.0, "avg_logprob": -0.1467776852984761, "compression_ratio": 1.570754716981132, "no_speech_prob": 6.962211955396924e-06}, {"id": 933, "seek": 441432, "start": 4427.32, "end": 4432.08, "text": " be thought of as a backbone plus a head.", "tokens": [312, 1194, 295, 382, 257, 34889, 1804, 257, 1378, 13], "temperature": 0.0, "avg_logprob": -0.1467776852984761, "compression_ratio": 1.570754716981132, "no_speech_prob": 6.962211955396924e-06}, {"id": 934, "seek": 441432, "start": 4432.08, "end": 4438.16, "text": " And if you pre-train the backbone and stick on a random head, you can do fine-tuning and", "tokens": [400, 498, 291, 659, 12, 83, 7146, 264, 34889, 293, 2897, 322, 257, 4974, 1378, 11, 291, 393, 360, 2489, 12, 83, 37726, 293], "temperature": 0.0, "avg_logprob": -0.1467776852984761, "compression_ratio": 1.570754716981132, "no_speech_prob": 6.962211955396924e-06}, {"id": 935, "seek": 441432, "start": 4438.16, "end": 4440.16, "text": " that's a good idea.", "tokens": [300, 311, 257, 665, 1558, 13], "temperature": 0.0, "avg_logprob": -0.1467776852984761, "compression_ratio": 1.570754716981132, "no_speech_prob": 6.962211955396924e-06}, {"id": 936, "seek": 444016, "start": 4440.16, "end": 4445.36, "text": " And so these two bits of the code are literally right next to each other.", "tokens": [400, 370, 613, 732, 9239, 295, 264, 3089, 366, 3736, 558, 958, 281, 1184, 661, 13], "temperature": 0.0, "avg_logprob": -0.17841961072838824, "compression_ratio": 1.5618556701030928, "no_speech_prob": 4.710887878900394e-06}, {"id": 937, "seek": 444016, "start": 4445.36, "end": 4453.32, "text": " This is kind of all there is inside this bit of fastai.lmrn.", "tokens": [639, 307, 733, 295, 439, 456, 307, 1854, 341, 857, 295, 2370, 1301, 13, 75, 76, 81, 77, 13], "temperature": 0.0, "avg_logprob": -0.17841961072838824, "compression_ratio": 1.5618556701030928, "no_speech_prob": 4.710887878900394e-06}, {"id": 938, "seek": 444016, "start": 4453.32, "end": 4456.88, "text": " Here's getLanguageModel, here's getClassifier.", "tokens": [1692, 311, 483, 43, 656, 20473, 44, 41147, 11, 510, 311, 483, 44621, 9902, 13], "temperature": 0.0, "avg_logprob": -0.17841961072838824, "compression_ratio": 1.5618556701030928, "no_speech_prob": 4.710887878900394e-06}, {"id": 939, "seek": 444016, "start": 4456.88, "end": 4463.24, "text": " getLanguageModel creates an RNN encoder and then creates a sequential model that sticks", "tokens": [483, 43, 656, 20473, 44, 41147, 7829, 364, 45702, 45, 2058, 19866, 293, 550, 7829, 257, 42881, 2316, 300, 12518], "temperature": 0.0, "avg_logprob": -0.17841961072838824, "compression_ratio": 1.5618556701030928, "no_speech_prob": 4.710887878900394e-06}, {"id": 940, "seek": 444016, "start": 4463.24, "end": 4466.4, "text": " on top of that, a linear decoder.", "tokens": [322, 1192, 295, 300, 11, 257, 8213, 979, 19866, 13], "temperature": 0.0, "avg_logprob": -0.17841961072838824, "compression_ratio": 1.5618556701030928, "no_speech_prob": 4.710887878900394e-06}, {"id": 941, "seek": 446640, "start": 4466.4, "end": 4470.719999999999, "text": " getClassifier creates an RNN encoder and then a sequential model that sticks on top of that,", "tokens": [483, 44621, 9902, 7829, 364, 45702, 45, 2058, 19866, 293, 550, 257, 42881, 2316, 300, 12518, 322, 1192, 295, 300, 11], "temperature": 0.0, "avg_logprob": -0.20457220531645276, "compression_ratio": 1.653061224489796, "no_speech_prob": 1.834272188716568e-05}, {"id": 942, "seek": 446640, "start": 4470.719999999999, "end": 4472.719999999999, "text": " a pooling linear classifier.", "tokens": [257, 7005, 278, 8213, 1508, 9902, 13], "temperature": 0.0, "avg_logprob": -0.20457220531645276, "compression_ratio": 1.653061224489796, "no_speech_prob": 1.834272188716568e-05}, {"id": 943, "seek": 446640, "start": 4472.719999999999, "end": 4477.2, "text": " We'll see what these differences are in a moment, but you get the basic idea.", "tokens": [492, 603, 536, 437, 613, 7300, 366, 294, 257, 1623, 11, 457, 291, 483, 264, 3875, 1558, 13], "temperature": 0.0, "avg_logprob": -0.20457220531645276, "compression_ratio": 1.653061224489796, "no_speech_prob": 1.834272188716568e-05}, {"id": 944, "seek": 446640, "start": 4477.2, "end": 4479.759999999999, "text": " They're basically doing pretty much the same thing.", "tokens": [814, 434, 1936, 884, 1238, 709, 264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.20457220531645276, "compression_ratio": 1.653061224489796, "no_speech_prob": 1.834272188716568e-05}, {"id": 945, "seek": 446640, "start": 4479.759999999999, "end": 4486.32, "text": " They've got this head and they're sticking on a simple linear layer on top.", "tokens": [814, 600, 658, 341, 1378, 293, 436, 434, 13465, 322, 257, 2199, 8213, 4583, 322, 1192, 13], "temperature": 0.0, "avg_logprob": -0.20457220531645276, "compression_ratio": 1.653061224489796, "no_speech_prob": 1.834272188716568e-05}, {"id": 946, "seek": 446640, "start": 4486.32, "end": 4491.48, "text": " So it's worth digging in a little bit deeper and seeing what's going on here.", "tokens": [407, 309, 311, 3163, 17343, 294, 257, 707, 857, 7731, 293, 2577, 437, 311, 516, 322, 510, 13], "temperature": 0.0, "avg_logprob": -0.20457220531645276, "compression_ratio": 1.653061224489796, "no_speech_prob": 1.834272188716568e-05}, {"id": 947, "seek": 449148, "start": 4491.48, "end": 4500.679999999999, "text": " Question about whether any of this translates to other languages.", "tokens": [14464, 466, 1968, 604, 295, 341, 28468, 281, 661, 8650, 13], "temperature": 0.0, "avg_logprob": -0.22990676561991374, "compression_ratio": 1.5240963855421688, "no_speech_prob": 2.586692244221922e-05}, {"id": 948, "seek": 449148, "start": 4500.679999999999, "end": 4505.5599999999995, "text": " This whole thing works in any language you like.", "tokens": [639, 1379, 551, 1985, 294, 604, 2856, 291, 411, 13], "temperature": 0.0, "avg_logprob": -0.22990676561991374, "compression_ratio": 1.5240963855421688, "no_speech_prob": 2.586692244221922e-05}, {"id": 949, "seek": 449148, "start": 4505.5599999999995, "end": 4513.04, "text": " Would you have to retrain your language model on a corpus from that language?", "tokens": [6068, 291, 362, 281, 1533, 7146, 428, 2856, 2316, 322, 257, 1181, 31624, 490, 300, 2856, 30], "temperature": 0.0, "avg_logprob": -0.22990676561991374, "compression_ratio": 1.5240963855421688, "no_speech_prob": 2.586692244221922e-05}, {"id": 950, "seek": 449148, "start": 4513.04, "end": 4519.4, "text": " So the WikiText 103 pretrained language model knows English.", "tokens": [407, 264, 35892, 50198, 48784, 1162, 31774, 2856, 2316, 3255, 3669, 13], "temperature": 0.0, "avg_logprob": -0.22990676561991374, "compression_ratio": 1.5240963855421688, "no_speech_prob": 2.586692244221922e-05}, {"id": 951, "seek": 451940, "start": 4519.4, "end": 4527.04, "text": " You could use it maybe as a pre-trained start for a French or German model, start by retraining", "tokens": [509, 727, 764, 309, 1310, 382, 257, 659, 12, 17227, 2001, 722, 337, 257, 5522, 420, 6521, 2316, 11, 722, 538, 49356, 1760], "temperature": 0.0, "avg_logprob": -0.19974506024232844, "compression_ratio": 1.581896551724138, "no_speech_prob": 2.156805066988454e-06}, {"id": 952, "seek": 451940, "start": 4527.04, "end": 4529.799999999999, "text": " the embedding layer from scratch.", "tokens": [264, 12240, 3584, 4583, 490, 8459, 13], "temperature": 0.0, "avg_logprob": -0.19974506024232844, "compression_ratio": 1.581896551724138, "no_speech_prob": 2.156805066988454e-06}, {"id": 953, "seek": 451940, "start": 4529.799999999999, "end": 4530.799999999999, "text": " Might be helpful.", "tokens": [23964, 312, 4961, 13], "temperature": 0.0, "avg_logprob": -0.19974506024232844, "compression_ratio": 1.581896551724138, "no_speech_prob": 2.156805066988454e-06}, {"id": 954, "seek": 451940, "start": 4530.799999999999, "end": 4533.24, "text": " Chinese, maybe not so much.", "tokens": [4649, 11, 1310, 406, 370, 709, 13], "temperature": 0.0, "avg_logprob": -0.19974506024232844, "compression_ratio": 1.581896551724138, "no_speech_prob": 2.156805066988454e-06}, {"id": 955, "seek": 451940, "start": 4533.24, "end": 4540.759999999999, "text": " But given that a language model can be trained from any unlabeled documents at all, you never", "tokens": [583, 2212, 300, 257, 2856, 2316, 393, 312, 8895, 490, 604, 32118, 18657, 292, 8512, 412, 439, 11, 291, 1128], "temperature": 0.0, "avg_logprob": -0.19974506024232844, "compression_ratio": 1.581896551724138, "no_speech_prob": 2.156805066988454e-06}, {"id": 956, "seek": 451940, "start": 4540.759999999999, "end": 4542.32, "text": " have to do that.", "tokens": [362, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.19974506024232844, "compression_ratio": 1.581896551724138, "no_speech_prob": 2.156805066988454e-06}, {"id": 957, "seek": 451940, "start": 4542.32, "end": 4549.32, "text": " Because almost every language in the world has plenty of documents you can grab.", "tokens": [1436, 1920, 633, 2856, 294, 264, 1002, 575, 7140, 295, 8512, 291, 393, 4444, 13], "temperature": 0.0, "avg_logprob": -0.19974506024232844, "compression_ratio": 1.581896551724138, "no_speech_prob": 2.156805066988454e-06}, {"id": 958, "seek": 454932, "start": 4549.32, "end": 4554.0, "text": " Newspapers, webpages, parliamentary records, whatever.", "tokens": [1873, 4952, 14441, 11, 3670, 79, 1660, 11, 43067, 7724, 11, 2035, 13], "temperature": 0.0, "avg_logprob": -0.273607347069717, "compression_ratio": 1.5380952380952382, "no_speech_prob": 1.7502436094218865e-05}, {"id": 959, "seek": 454932, "start": 4554.0, "end": 4562.799999999999, "text": " As long as you've got a few thousand documents showing somewhat normal usage of that language,", "tokens": [1018, 938, 382, 291, 600, 658, 257, 1326, 4714, 8512, 4099, 8344, 2710, 14924, 295, 300, 2856, 11], "temperature": 0.0, "avg_logprob": -0.273607347069717, "compression_ratio": 1.5380952380952382, "no_speech_prob": 1.7502436094218865e-05}, {"id": 960, "seek": 454932, "start": 4562.799999999999, "end": 4564.679999999999, "text": " you can create a language model.", "tokens": [291, 393, 1884, 257, 2856, 2316, 13], "temperature": 0.0, "avg_logprob": -0.273607347069717, "compression_ratio": 1.5380952380952382, "no_speech_prob": 1.7502436094218865e-05}, {"id": 961, "seek": 454932, "start": 4564.679999999999, "end": 4569.759999999999, "text": " And so I know some of our students, one of our students, I have to look up during the", "tokens": [400, 370, 286, 458, 512, 295, 527, 1731, 11, 472, 295, 527, 1731, 11, 286, 362, 281, 574, 493, 1830, 264], "temperature": 0.0, "avg_logprob": -0.273607347069717, "compression_ratio": 1.5380952380952382, "no_speech_prob": 1.7502436094218865e-05}, {"id": 962, "seek": 454932, "start": 4569.759999999999, "end": 4574.639999999999, "text": " week, very embarrassing, tried this approach for Thai.", "tokens": [1243, 11, 588, 17299, 11, 3031, 341, 3109, 337, 19254, 13], "temperature": 0.0, "avg_logprob": -0.273607347069717, "compression_ratio": 1.5380952380952382, "no_speech_prob": 1.7502436094218865e-05}, {"id": 963, "seek": 457464, "start": 4574.64, "end": 4583.76, "text": " He said the first model he built easily beat the previous data-the-art Thai classifier.", "tokens": [634, 848, 264, 700, 2316, 415, 3094, 3612, 4224, 264, 3894, 1412, 12, 3322, 12, 446, 19254, 1508, 9902, 13], "temperature": 0.0, "avg_logprob": -0.16733761628468832, "compression_ratio": 1.6851063829787234, "no_speech_prob": 3.822423968813382e-05}, {"id": 964, "seek": 457464, "start": 4583.76, "end": 4590.280000000001, "text": " For those of you that are international fellows, this is an easy way for you to whip out a", "tokens": [1171, 729, 295, 291, 300, 366, 5058, 35595, 11, 341, 307, 364, 1858, 636, 337, 291, 281, 22377, 484, 257], "temperature": 0.0, "avg_logprob": -0.16733761628468832, "compression_ratio": 1.6851063829787234, "no_speech_prob": 3.822423968813382e-05}, {"id": 965, "seek": 457464, "start": 4590.280000000001, "end": 4595.96, "text": " paper in which you either create the first ever classifier in your language or beat everybody", "tokens": [3035, 294, 597, 291, 2139, 1884, 264, 700, 1562, 1508, 9902, 294, 428, 2856, 420, 4224, 2201], "temperature": 0.0, "avg_logprob": -0.16733761628468832, "compression_ratio": 1.6851063829787234, "no_speech_prob": 3.822423968813382e-05}, {"id": 966, "seek": 457464, "start": 4595.96, "end": 4598.400000000001, "text": " else's classifier in your language.", "tokens": [1646, 311, 1508, 9902, 294, 428, 2856, 13], "temperature": 0.0, "avg_logprob": -0.16733761628468832, "compression_ratio": 1.6851063829787234, "no_speech_prob": 3.822423968813382e-05}, {"id": 967, "seek": 457464, "start": 4598.400000000001, "end": 4602.400000000001, "text": " And then you can tell them that you've been a student of deep learning for 6 months and", "tokens": [400, 550, 291, 393, 980, 552, 300, 291, 600, 668, 257, 3107, 295, 2452, 2539, 337, 1386, 2493, 293], "temperature": 0.0, "avg_logprob": -0.16733761628468832, "compression_ratio": 1.6851063829787234, "no_speech_prob": 3.822423968813382e-05}, {"id": 968, "seek": 460240, "start": 4602.4, "end": 4607.12, "text": " piss off all the academics in your country.", "tokens": [15171, 766, 439, 264, 25695, 294, 428, 1941, 13], "temperature": 0.0, "avg_logprob": -0.25048027825109737, "compression_ratio": 1.5694444444444444, "no_speech_prob": 1.406345563736977e-05}, {"id": 969, "seek": 460240, "start": 4607.12, "end": 4612.44, "text": " So here's our identity coder.", "tokens": [407, 510, 311, 527, 6575, 17656, 260, 13], "temperature": 0.0, "avg_logprob": -0.25048027825109737, "compression_ratio": 1.5694444444444444, "no_speech_prob": 1.406345563736977e-05}, {"id": 970, "seek": 460240, "start": 4612.44, "end": 4614.799999999999, "text": " It's just a standard NN.module.", "tokens": [467, 311, 445, 257, 3832, 426, 45, 13, 8014, 2271, 13], "temperature": 0.0, "avg_logprob": -0.25048027825109737, "compression_ratio": 1.5694444444444444, "no_speech_prob": 1.406345563736977e-05}, {"id": 971, "seek": 460240, "start": 4614.799999999999, "end": 4621.5199999999995, "text": " Most of the text in it is actually just documentation, as you can see.", "tokens": [4534, 295, 264, 2487, 294, 309, 307, 767, 445, 14333, 11, 382, 291, 393, 536, 13], "temperature": 0.0, "avg_logprob": -0.25048027825109737, "compression_ratio": 1.5694444444444444, "no_speech_prob": 1.406345563736977e-05}, {"id": 972, "seek": 460240, "start": 4621.5199999999995, "end": 4624.32, "text": " It looks like there's more going on in it than there actually is.", "tokens": [467, 1542, 411, 456, 311, 544, 516, 322, 294, 309, 813, 456, 767, 307, 13], "temperature": 0.0, "avg_logprob": -0.25048027825109737, "compression_ratio": 1.5694444444444444, "no_speech_prob": 1.406345563736977e-05}, {"id": 973, "seek": 460240, "start": 4624.32, "end": 4630.759999999999, "text": " But really all there is is we create an embedding layer, we create an LSTM for each layer that's", "tokens": [583, 534, 439, 456, 307, 307, 321, 1884, 364, 12240, 3584, 4583, 11, 321, 1884, 364, 441, 6840, 44, 337, 1184, 4583, 300, 311], "temperature": 0.0, "avg_logprob": -0.25048027825109737, "compression_ratio": 1.5694444444444444, "no_speech_prob": 1.406345563736977e-05}, {"id": 974, "seek": 463076, "start": 4630.76, "end": 4637.04, "text": " been asked for, and that's it.", "tokens": [668, 2351, 337, 11, 293, 300, 311, 309, 13], "temperature": 0.0, "avg_logprob": -0.1707044243812561, "compression_ratio": 1.4342105263157894, "no_speech_prob": 7.183170509961201e-06}, {"id": 975, "seek": 463076, "start": 4637.04, "end": 4639.52, "text": " Everything else in it is dropout.", "tokens": [5471, 1646, 294, 309, 307, 3270, 346, 13], "temperature": 0.0, "avg_logprob": -0.1707044243812561, "compression_ratio": 1.4342105263157894, "no_speech_prob": 7.183170509961201e-06}, {"id": 976, "seek": 463076, "start": 4639.52, "end": 4645.360000000001, "text": " Basically all of the interesting stuff in the AWED LSTM paper is all of the places you", "tokens": [8537, 439, 295, 264, 1880, 1507, 294, 264, 25815, 4731, 441, 6840, 44, 3035, 307, 439, 295, 264, 3190, 291], "temperature": 0.0, "avg_logprob": -0.1707044243812561, "compression_ratio": 1.4342105263157894, "no_speech_prob": 7.183170509961201e-06}, {"id": 977, "seek": 463076, "start": 4645.360000000001, "end": 4649.24, "text": " can put dropout.", "tokens": [393, 829, 3270, 346, 13], "temperature": 0.0, "avg_logprob": -0.1707044243812561, "compression_ratio": 1.4342105263157894, "no_speech_prob": 7.183170509961201e-06}, {"id": 978, "seek": 463076, "start": 4649.24, "end": 4654.76, "text": " And then the forward is basically the same thing.", "tokens": [400, 550, 264, 2128, 307, 1936, 264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.1707044243812561, "compression_ratio": 1.4342105263157894, "no_speech_prob": 7.183170509961201e-06}, {"id": 979, "seek": 465476, "start": 4654.76, "end": 4663.84, "text": " Call the embedding layer, add some dropout, go through each layer, call that RNN layer,", "tokens": [7807, 264, 12240, 3584, 4583, 11, 909, 512, 3270, 346, 11, 352, 807, 1184, 4583, 11, 818, 300, 45702, 45, 4583, 11], "temperature": 0.0, "avg_logprob": -0.16029515699906782, "compression_ratio": 1.416058394160584, "no_speech_prob": 6.083573111936857e-07}, {"id": 980, "seek": 465476, "start": 4663.84, "end": 4674.360000000001, "text": " append it to our list of outputs, add dropout, and that's about it.", "tokens": [34116, 309, 281, 527, 1329, 295, 23930, 11, 909, 3270, 346, 11, 293, 300, 311, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.16029515699906782, "compression_ratio": 1.416058394160584, "no_speech_prob": 6.083573111936857e-07}, {"id": 981, "seek": 465476, "start": 4674.360000000001, "end": 4677.84, "text": " So it's really pretty straightforward.", "tokens": [407, 309, 311, 534, 1238, 15325, 13], "temperature": 0.0, "avg_logprob": -0.16029515699906782, "compression_ratio": 1.416058394160584, "no_speech_prob": 6.083573111936857e-07}, {"id": 982, "seek": 467784, "start": 4677.84, "end": 4687.2, "text": " And the paper you want to be reading, as I've mentioned, is the AWD LSTM paper, which is", "tokens": [400, 264, 3035, 291, 528, 281, 312, 3760, 11, 382, 286, 600, 2835, 11, 307, 264, 25815, 35, 441, 6840, 44, 3035, 11, 597, 307], "temperature": 0.0, "avg_logprob": -0.17391074895858766, "compression_ratio": 1.4527363184079602, "no_speech_prob": 1.436745037608489e-06}, {"id": 983, "seek": 467784, "start": 4687.2, "end": 4691.76, "text": " this one here, Regularizing and Optimizing LSTM Language Models.", "tokens": [341, 472, 510, 11, 45659, 3319, 293, 35013, 3319, 441, 6840, 44, 24445, 6583, 1625, 13], "temperature": 0.0, "avg_logprob": -0.17391074895858766, "compression_ratio": 1.4527363184079602, "no_speech_prob": 1.436745037608489e-06}, {"id": 984, "seek": 467784, "start": 4691.76, "end": 4700.96, "text": " And it's well written and pretty accessible and entirely implemented inside FastAI as", "tokens": [400, 309, 311, 731, 3720, 293, 1238, 9515, 293, 7696, 12270, 1854, 15968, 48698, 382], "temperature": 0.0, "avg_logprob": -0.17391074895858766, "compression_ratio": 1.4527363184079602, "no_speech_prob": 1.436745037608489e-06}, {"id": 985, "seek": 467784, "start": 4700.96, "end": 4706.400000000001, "text": " well, so you can see all of the code for that paper.", "tokens": [731, 11, 370, 291, 393, 536, 439, 295, 264, 3089, 337, 300, 3035, 13], "temperature": 0.0, "avg_logprob": -0.17391074895858766, "compression_ratio": 1.4527363184079602, "no_speech_prob": 1.436745037608489e-06}, {"id": 986, "seek": 470640, "start": 4706.4, "end": 4711.719999999999, "text": " And like a lot of the code is shamelessly plagiarized with Stephen's permission from", "tokens": [400, 411, 257, 688, 295, 264, 3089, 307, 40164, 356, 33756, 9448, 1602, 365, 13391, 311, 11226, 490], "temperature": 0.0, "avg_logprob": -0.26175898501747535, "compression_ratio": 1.48471615720524, "no_speech_prob": 1.1300524420221336e-05}, {"id": 987, "seek": 470640, "start": 4711.719999999999, "end": 4718.44, "text": " his excellent GitHub repo, AWD LSTM, and the process of which I picked some of his bugs", "tokens": [702, 7103, 23331, 49040, 11, 25815, 35, 441, 6840, 44, 11, 293, 264, 1399, 295, 597, 286, 6183, 512, 295, 702, 15120], "temperature": 0.0, "avg_logprob": -0.26175898501747535, "compression_ratio": 1.48471615720524, "no_speech_prob": 1.1300524420221336e-05}, {"id": 988, "seek": 470640, "start": 4718.44, "end": 4719.44, "text": " as well.", "tokens": [382, 731, 13], "temperature": 0.0, "avg_logprob": -0.26175898501747535, "compression_ratio": 1.48471615720524, "no_speech_prob": 1.1300524420221336e-05}, {"id": 989, "seek": 470640, "start": 4719.44, "end": 4727.0, "text": " I even told him about them.", "tokens": [286, 754, 1907, 796, 466, 552, 13], "temperature": 0.0, "avg_logprob": -0.26175898501747535, "compression_ratio": 1.48471615720524, "no_speech_prob": 1.1300524420221336e-05}, {"id": 990, "seek": 470640, "start": 4727.0, "end": 4731.639999999999, "text": " I'm talking increasingly about Please Read the Papers, so here's the paper, Please Read", "tokens": [286, 478, 1417, 12980, 466, 2555, 17604, 264, 430, 14441, 11, 370, 510, 311, 264, 3035, 11, 2555, 17604], "temperature": 0.0, "avg_logprob": -0.26175898501747535, "compression_ratio": 1.48471615720524, "no_speech_prob": 1.1300524420221336e-05}, {"id": 991, "seek": 470640, "start": 4731.639999999999, "end": 4733.799999999999, "text": " this paper.", "tokens": [341, 3035, 13], "temperature": 0.0, "avg_logprob": -0.26175898501747535, "compression_ratio": 1.48471615720524, "no_speech_prob": 1.1300524420221336e-05}, {"id": 992, "seek": 470640, "start": 4733.799999999999, "end": 4736.08, "text": " And it refers to other papers.", "tokens": [400, 309, 14942, 281, 661, 10577, 13], "temperature": 0.0, "avg_logprob": -0.26175898501747535, "compression_ratio": 1.48471615720524, "no_speech_prob": 1.1300524420221336e-05}, {"id": 993, "seek": 473608, "start": 4736.08, "end": 4744.6, "text": " So for things like, why is it that the encoder weight and the decoder weight are the same?", "tokens": [407, 337, 721, 411, 11, 983, 307, 309, 300, 264, 2058, 19866, 3364, 293, 264, 979, 19866, 3364, 366, 264, 912, 30], "temperature": 0.0, "avg_logprob": -0.2225334984915597, "compression_ratio": 1.6442953020134228, "no_speech_prob": 5.3381299949251115e-06}, {"id": 994, "seek": 473608, "start": 4744.6, "end": 4759.64, "text": " Well it's because there's this thing called tie weights, this is inside that Get Language", "tokens": [1042, 309, 311, 570, 456, 311, 341, 551, 1219, 7582, 17443, 11, 341, 307, 1854, 300, 3240, 24445], "temperature": 0.0, "avg_logprob": -0.2225334984915597, "compression_ratio": 1.6442953020134228, "no_speech_prob": 5.3381299949251115e-06}, {"id": 995, "seek": 473608, "start": 4759.64, "end": 4762.88, "text": " Model, there's a thing called tie weights that defaults to true.", "tokens": [17105, 11, 456, 311, 257, 551, 1219, 7582, 17443, 300, 7576, 82, 281, 2074, 13], "temperature": 0.0, "avg_logprob": -0.2225334984915597, "compression_ratio": 1.6442953020134228, "no_speech_prob": 5.3381299949251115e-06}, {"id": 996, "seek": 476288, "start": 4762.88, "end": 4772.400000000001, "text": " And if it's true, then we actually tie, we literally use the same weight matrix for the", "tokens": [400, 498, 309, 311, 2074, 11, 550, 321, 767, 7582, 11, 321, 3736, 764, 264, 912, 3364, 8141, 337, 264], "temperature": 0.0, "avg_logprob": -0.1622768892060726, "compression_ratio": 1.670940170940171, "no_speech_prob": 2.5215647383447504e-06}, {"id": 997, "seek": 476288, "start": 4772.400000000001, "end": 4775.24, "text": " encoder and the decoder.", "tokens": [2058, 19866, 293, 264, 979, 19866, 13], "temperature": 0.0, "avg_logprob": -0.1622768892060726, "compression_ratio": 1.670940170940171, "no_speech_prob": 2.5215647383447504e-06}, {"id": 998, "seek": 476288, "start": 4775.24, "end": 4779.16, "text": " So they're literally pointing at the same block of memory.", "tokens": [407, 436, 434, 3736, 12166, 412, 264, 912, 3461, 295, 4675, 13], "temperature": 0.0, "avg_logprob": -0.1622768892060726, "compression_ratio": 1.670940170940171, "no_speech_prob": 2.5215647383447504e-06}, {"id": 999, "seek": 476288, "start": 4779.16, "end": 4784.92, "text": " And so why is that, what's the result of it, that's one of the citations in Stephen's paper,", "tokens": [400, 370, 983, 307, 300, 11, 437, 311, 264, 1874, 295, 309, 11, 300, 311, 472, 295, 264, 4814, 763, 294, 13391, 311, 3035, 11], "temperature": 0.0, "avg_logprob": -0.1622768892060726, "compression_ratio": 1.670940170940171, "no_speech_prob": 2.5215647383447504e-06}, {"id": 1000, "seek": 476288, "start": 4784.92, "end": 4789.32, "text": " which is also a well written paper, you can go and look up and learn about work time.", "tokens": [597, 307, 611, 257, 731, 3720, 3035, 11, 291, 393, 352, 293, 574, 493, 293, 1466, 466, 589, 565, 13], "temperature": 0.0, "avg_logprob": -0.1622768892060726, "compression_ratio": 1.670940170940171, "no_speech_prob": 2.5215647383447504e-06}, {"id": 1001, "seek": 476288, "start": 4789.32, "end": 4792.6, "text": " So there's a lot of cool stuff in there.", "tokens": [407, 456, 311, 257, 688, 295, 1627, 1507, 294, 456, 13], "temperature": 0.0, "avg_logprob": -0.1622768892060726, "compression_ratio": 1.670940170940171, "no_speech_prob": 2.5215647383447504e-06}, {"id": 1002, "seek": 479260, "start": 4792.6, "end": 4797.400000000001, "text": " Okay, so we have basically a standard RNN, the only reason why it's not standard is it's", "tokens": [1033, 11, 370, 321, 362, 1936, 257, 3832, 45702, 45, 11, 264, 787, 1778, 983, 309, 311, 406, 3832, 307, 309, 311], "temperature": 0.0, "avg_logprob": -0.15908277152788522, "compression_ratio": 1.5777777777777777, "no_speech_prob": 3.844912043859949e-06}, {"id": 1003, "seek": 479260, "start": 4797.400000000001, "end": 4800.240000000001, "text": " just got lots more types of dropout in it.", "tokens": [445, 658, 3195, 544, 3467, 295, 3270, 346, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.15908277152788522, "compression_ratio": 1.5777777777777777, "no_speech_prob": 3.844912043859949e-06}, {"id": 1004, "seek": 479260, "start": 4800.240000000001, "end": 4809.04, "text": " And then a sequential model, on top of that we stick a linear decoder, which is literally", "tokens": [400, 550, 257, 42881, 2316, 11, 322, 1192, 295, 300, 321, 2897, 257, 8213, 979, 19866, 11, 597, 307, 3736], "temperature": 0.0, "avg_logprob": -0.15908277152788522, "compression_ratio": 1.5777777777777777, "no_speech_prob": 3.844912043859949e-06}, {"id": 1005, "seek": 479260, "start": 4809.04, "end": 4811.280000000001, "text": " half the screen of code.", "tokens": [1922, 264, 2568, 295, 3089, 13], "temperature": 0.0, "avg_logprob": -0.15908277152788522, "compression_ratio": 1.5777777777777777, "no_speech_prob": 3.844912043859949e-06}, {"id": 1006, "seek": 479260, "start": 4811.280000000001, "end": 4819.360000000001, "text": " It's got a single linear layer, we initialize the weights to some range, we add some dropout,", "tokens": [467, 311, 658, 257, 2167, 8213, 4583, 11, 321, 5883, 1125, 264, 17443, 281, 512, 3613, 11, 321, 909, 512, 3270, 346, 11], "temperature": 0.0, "avg_logprob": -0.15908277152788522, "compression_ratio": 1.5777777777777777, "no_speech_prob": 3.844912043859949e-06}, {"id": 1007, "seek": 479260, "start": 4819.360000000001, "end": 4820.360000000001, "text": " and that's it.", "tokens": [293, 300, 311, 309, 13], "temperature": 0.0, "avg_logprob": -0.15908277152788522, "compression_ratio": 1.5777777777777777, "no_speech_prob": 3.844912043859949e-06}, {"id": 1008, "seek": 482036, "start": 4820.36, "end": 4822.92, "text": " So it's a linear layer with dropout.", "tokens": [407, 309, 311, 257, 8213, 4583, 365, 3270, 346, 13], "temperature": 0.0, "avg_logprob": -0.11220680475234986, "compression_ratio": 1.5917159763313609, "no_speech_prob": 4.785074906976661e-06}, {"id": 1009, "seek": 482036, "start": 4822.92, "end": 4829.36, "text": " So we've got an RNN, on top of that we stick a linear layer with dropout, and we're finished.", "tokens": [407, 321, 600, 658, 364, 45702, 45, 11, 322, 1192, 295, 300, 321, 2897, 257, 8213, 4583, 365, 3270, 346, 11, 293, 321, 434, 4335, 13], "temperature": 0.0, "avg_logprob": -0.11220680475234986, "compression_ratio": 1.5917159763313609, "no_speech_prob": 4.785074906976661e-06}, {"id": 1010, "seek": 482036, "start": 4829.36, "end": 4836.96, "text": " So that's the language model.", "tokens": [407, 300, 311, 264, 2856, 2316, 13], "temperature": 0.0, "avg_logprob": -0.11220680475234986, "compression_ratio": 1.5917159763313609, "no_speech_prob": 4.785074906976661e-06}, {"id": 1011, "seek": 482036, "start": 4836.96, "end": 4843.08, "text": " So what dropout you choose matters a lot.", "tokens": [407, 437, 3270, 346, 291, 2826, 7001, 257, 688, 13], "temperature": 0.0, "avg_logprob": -0.11220680475234986, "compression_ratio": 1.5917159763313609, "no_speech_prob": 4.785074906976661e-06}, {"id": 1012, "seek": 482036, "start": 4843.08, "end": 4849.0, "text": " And through a lot of experimentation, I found a bunch of dropouts.", "tokens": [400, 807, 257, 688, 295, 37142, 11, 286, 1352, 257, 3840, 295, 3270, 7711, 13], "temperature": 0.0, "avg_logprob": -0.11220680475234986, "compression_ratio": 1.5917159763313609, "no_speech_prob": 4.785074906976661e-06}, {"id": 1013, "seek": 484900, "start": 4849.0, "end": 4853.48, "text": " You can see here we've got each of these corresponds to a particular argument.", "tokens": [509, 393, 536, 510, 321, 600, 658, 1184, 295, 613, 23249, 281, 257, 1729, 6770, 13], "temperature": 0.0, "avg_logprob": -0.1685058411131514, "compression_ratio": 1.6175115207373272, "no_speech_prob": 1.6536827160962275e-06}, {"id": 1014, "seek": 484900, "start": 4853.48, "end": 4858.84, "text": " A bunch of dropouts tend to work pretty well for language models.", "tokens": [316, 3840, 295, 3270, 7711, 3928, 281, 589, 1238, 731, 337, 2856, 5245, 13], "temperature": 0.0, "avg_logprob": -0.1685058411131514, "compression_ratio": 1.6175115207373272, "no_speech_prob": 1.6536827160962275e-06}, {"id": 1015, "seek": 484900, "start": 4858.84, "end": 4866.72, "text": " But if you have less data for your language model, you'll need more dropout.", "tokens": [583, 498, 291, 362, 1570, 1412, 337, 428, 2856, 2316, 11, 291, 603, 643, 544, 3270, 346, 13], "temperature": 0.0, "avg_logprob": -0.1685058411131514, "compression_ratio": 1.6175115207373272, "no_speech_prob": 1.6536827160962275e-06}, {"id": 1016, "seek": 484900, "start": 4866.72, "end": 4870.16, "text": " If you have more data, you can benefit from less dropout.", "tokens": [759, 291, 362, 544, 1412, 11, 291, 393, 5121, 490, 1570, 3270, 346, 13], "temperature": 0.0, "avg_logprob": -0.1685058411131514, "compression_ratio": 1.6175115207373272, "no_speech_prob": 1.6536827160962275e-06}, {"id": 1017, "seek": 484900, "start": 4870.16, "end": 4872.08, "text": " You don't want to regularize more than you have to.", "tokens": [509, 500, 380, 528, 281, 3890, 1125, 544, 813, 291, 362, 281, 13], "temperature": 0.0, "avg_logprob": -0.1685058411131514, "compression_ratio": 1.6175115207373272, "no_speech_prob": 1.6536827160962275e-06}, {"id": 1018, "seek": 484900, "start": 4872.08, "end": 4873.7, "text": " Makes sense, right?", "tokens": [25245, 2020, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1685058411131514, "compression_ratio": 1.6175115207373272, "no_speech_prob": 1.6536827160962275e-06}, {"id": 1019, "seek": 487370, "start": 4873.7, "end": 4879.7, "text": " Other than having to tune every one of these 5 things, my claim is they're already pretty", "tokens": [5358, 813, 1419, 281, 10864, 633, 472, 295, 613, 1025, 721, 11, 452, 3932, 307, 436, 434, 1217, 1238], "temperature": 0.0, "avg_logprob": -0.17146739763082916, "compression_ratio": 1.7733990147783252, "no_speech_prob": 6.048883733456023e-06}, {"id": 1020, "seek": 487370, "start": 4879.7, "end": 4883.32, "text": " good ratios to each other, so just tune this number.", "tokens": [665, 32435, 281, 1184, 661, 11, 370, 445, 10864, 341, 1230, 13], "temperature": 0.0, "avg_logprob": -0.17146739763082916, "compression_ratio": 1.7733990147783252, "no_speech_prob": 6.048883733456023e-06}, {"id": 1021, "seek": 487370, "start": 4883.32, "end": 4886.44, "text": " I just multiply it all by something.", "tokens": [286, 445, 12972, 309, 439, 538, 746, 13], "temperature": 0.0, "avg_logprob": -0.17146739763082916, "compression_ratio": 1.7733990147783252, "no_speech_prob": 6.048883733456023e-06}, {"id": 1022, "seek": 487370, "start": 4886.44, "end": 4890.84, "text": " So there's really just one number you have to tune.", "tokens": [407, 456, 311, 534, 445, 472, 1230, 291, 362, 281, 10864, 13], "temperature": 0.0, "avg_logprob": -0.17146739763082916, "compression_ratio": 1.7733990147783252, "no_speech_prob": 6.048883733456023e-06}, {"id": 1023, "seek": 487370, "start": 4890.84, "end": 4895.12, "text": " So if you're overfitting, then you'll need to increase this number.", "tokens": [407, 498, 291, 434, 670, 69, 2414, 11, 550, 291, 603, 643, 281, 3488, 341, 1230, 13], "temperature": 0.0, "avg_logprob": -0.17146739763082916, "compression_ratio": 1.7733990147783252, "no_speech_prob": 6.048883733456023e-06}, {"id": 1024, "seek": 487370, "start": 4895.12, "end": 4897.599999999999, "text": " If you're underfitting, you'll need to decrease this number.", "tokens": [759, 291, 434, 833, 69, 2414, 11, 291, 603, 643, 281, 11514, 341, 1230, 13], "temperature": 0.0, "avg_logprob": -0.17146739763082916, "compression_ratio": 1.7733990147783252, "no_speech_prob": 6.048883733456023e-06}, {"id": 1025, "seek": 489760, "start": 4897.6, "end": 4905.68, "text": " Other than that, these ratios actually seem pretty good.", "tokens": [5358, 813, 300, 11, 613, 32435, 767, 1643, 1238, 665, 13], "temperature": 0.0, "avg_logprob": -0.13434058266717033, "compression_ratio": 1.609375, "no_speech_prob": 8.714323485037312e-07}, {"id": 1026, "seek": 489760, "start": 4905.68, "end": 4915.04, "text": " So one important idea, which may seem pretty minor, but again it's incredibly controversial,", "tokens": [407, 472, 1021, 1558, 11, 597, 815, 1643, 1238, 6696, 11, 457, 797, 309, 311, 6252, 17323, 11], "temperature": 0.0, "avg_logprob": -0.13434058266717033, "compression_ratio": 1.609375, "no_speech_prob": 8.714323485037312e-07}, {"id": 1027, "seek": 489760, "start": 4915.04, "end": 4920.4800000000005, "text": " is that we should measure accuracy when we look at a language model.", "tokens": [307, 300, 321, 820, 3481, 14170, 562, 321, 574, 412, 257, 2856, 2316, 13], "temperature": 0.0, "avg_logprob": -0.13434058266717033, "compression_ratio": 1.609375, "no_speech_prob": 8.714323485037312e-07}, {"id": 1028, "seek": 489760, "start": 4920.4800000000005, "end": 4926.96, "text": " So normally language models, we look at this loss value, which is just cross-entropy loss.", "tokens": [407, 5646, 2856, 5245, 11, 321, 574, 412, 341, 4470, 2158, 11, 597, 307, 445, 3278, 12, 317, 27514, 4470, 13], "temperature": 0.0, "avg_logprob": -0.13434058266717033, "compression_ratio": 1.609375, "no_speech_prob": 8.714323485037312e-07}, {"id": 1029, "seek": 492696, "start": 4926.96, "end": 4933.68, "text": " Specifically, we nearly always take e to the power of that, which the NLP community calls", "tokens": [26058, 11, 321, 6217, 1009, 747, 308, 281, 264, 1347, 295, 300, 11, 597, 264, 426, 45196, 1768, 5498], "temperature": 0.0, "avg_logprob": -0.13018101586235895, "compression_ratio": 1.5792079207920793, "no_speech_prob": 3.0415853871090803e-06}, {"id": 1030, "seek": 492696, "start": 4933.68, "end": 4934.68, "text": " perplexity.", "tokens": [680, 18945, 507, 13], "temperature": 0.0, "avg_logprob": -0.13018101586235895, "compression_ratio": 1.5792079207920793, "no_speech_prob": 3.0415853871090803e-06}, {"id": 1031, "seek": 492696, "start": 4934.68, "end": 4942.24, "text": " So perplexity is just e to the power of cross-entropy.", "tokens": [407, 680, 18945, 507, 307, 445, 308, 281, 264, 1347, 295, 3278, 12, 317, 27514, 13], "temperature": 0.0, "avg_logprob": -0.13018101586235895, "compression_ratio": 1.5792079207920793, "no_speech_prob": 3.0415853871090803e-06}, {"id": 1032, "seek": 492696, "start": 4942.24, "end": 4948.44, "text": " There's a lot of problems with comparing things based on cross-entropy loss.", "tokens": [821, 311, 257, 688, 295, 2740, 365, 15763, 721, 2361, 322, 3278, 12, 317, 27514, 4470, 13], "temperature": 0.0, "avg_logprob": -0.13018101586235895, "compression_ratio": 1.5792079207920793, "no_speech_prob": 3.0415853871090803e-06}, {"id": 1033, "seek": 492696, "start": 4948.44, "end": 4955.14, "text": " I'm not sure I've got time to go into it in detail now, but the basic problem is that", "tokens": [286, 478, 406, 988, 286, 600, 658, 565, 281, 352, 666, 309, 294, 2607, 586, 11, 457, 264, 3875, 1154, 307, 300], "temperature": 0.0, "avg_logprob": -0.13018101586235895, "compression_ratio": 1.5792079207920793, "no_speech_prob": 3.0415853871090803e-06}, {"id": 1034, "seek": 495514, "start": 4955.14, "end": 4957.88, "text": " it's kind of like that thing we learned about focal loss.", "tokens": [309, 311, 733, 295, 411, 300, 551, 321, 3264, 466, 26592, 4470, 13], "temperature": 0.0, "avg_logprob": -0.16449130527556888, "compression_ratio": 1.7814814814814814, "no_speech_prob": 3.6688461477751844e-06}, {"id": 1035, "seek": 495514, "start": 4957.88, "end": 4963.400000000001, "text": " Cross-entropy loss, if you're right, it wants you to be really confident that you're right.", "tokens": [11623, 12, 317, 27514, 4470, 11, 498, 291, 434, 558, 11, 309, 2738, 291, 281, 312, 534, 6679, 300, 291, 434, 558, 13], "temperature": 0.0, "avg_logprob": -0.16449130527556888, "compression_ratio": 1.7814814814814814, "no_speech_prob": 3.6688461477751844e-06}, {"id": 1036, "seek": 495514, "start": 4963.400000000001, "end": 4970.280000000001, "text": " So it really penalizes a model that doesn't say, I'm so sure this is wrong.", "tokens": [407, 309, 534, 13661, 5660, 257, 2316, 300, 1177, 380, 584, 11, 286, 478, 370, 988, 341, 307, 2085, 13], "temperature": 0.0, "avg_logprob": -0.16449130527556888, "compression_ratio": 1.7814814814814814, "no_speech_prob": 3.6688461477751844e-06}, {"id": 1037, "seek": 495514, "start": 4970.280000000001, "end": 4973.6, "text": " Whereas accuracy doesn't care at all about how confident you are, it just kind of cares", "tokens": [13813, 14170, 1177, 380, 1127, 412, 439, 466, 577, 6679, 291, 366, 11, 309, 445, 733, 295, 12310], "temperature": 0.0, "avg_logprob": -0.16449130527556888, "compression_ratio": 1.7814814814814814, "no_speech_prob": 3.6688461477751844e-06}, {"id": 1038, "seek": 495514, "start": 4973.6, "end": 4974.88, "text": " about whether you're right.", "tokens": [466, 1968, 291, 434, 558, 13], "temperature": 0.0, "avg_logprob": -0.16449130527556888, "compression_ratio": 1.7814814814814814, "no_speech_prob": 3.6688461477751844e-06}, {"id": 1039, "seek": 495514, "start": 4974.88, "end": 4979.160000000001, "text": " And this is much more often the thing which you care about in real life.", "tokens": [400, 341, 307, 709, 544, 2049, 264, 551, 597, 291, 1127, 466, 294, 957, 993, 13], "temperature": 0.0, "avg_logprob": -0.16449130527556888, "compression_ratio": 1.7814814814814814, "no_speech_prob": 3.6688461477751844e-06}, {"id": 1040, "seek": 495514, "start": 4979.160000000001, "end": 4985.12, "text": " So this accuracy is how often do we guess the next word correctly.", "tokens": [407, 341, 14170, 307, 577, 2049, 360, 321, 2041, 264, 958, 1349, 8944, 13], "temperature": 0.0, "avg_logprob": -0.16449130527556888, "compression_ratio": 1.7814814814814814, "no_speech_prob": 3.6688461477751844e-06}, {"id": 1041, "seek": 498512, "start": 4985.12, "end": 4989.32, "text": " And I just find that a much more stable number to keep track of.", "tokens": [400, 286, 445, 915, 300, 257, 709, 544, 8351, 1230, 281, 1066, 2837, 295, 13], "temperature": 0.0, "avg_logprob": -0.1330744709287371, "compression_ratio": 1.2919708029197081, "no_speech_prob": 5.255362339084968e-06}, {"id": 1042, "seek": 498512, "start": 4989.32, "end": 4994.76, "text": " So that's a simple little thing that I do.", "tokens": [407, 300, 311, 257, 2199, 707, 551, 300, 286, 360, 13], "temperature": 0.0, "avg_logprob": -0.1330744709287371, "compression_ratio": 1.2919708029197081, "no_speech_prob": 5.255362339084968e-06}, {"id": 1043, "seek": 498512, "start": 4994.76, "end": 5011.9, "text": " So we train for a while, and we get down to a 3.9 cross-entropy loss.", "tokens": [407, 321, 3847, 337, 257, 1339, 11, 293, 321, 483, 760, 281, 257, 805, 13, 24, 3278, 12, 317, 27514, 4470, 13], "temperature": 0.0, "avg_logprob": -0.1330744709287371, "compression_ratio": 1.2919708029197081, "no_speech_prob": 5.255362339084968e-06}, {"id": 1044, "seek": 501190, "start": 5011.9, "end": 5024.0, "text": " And if you go e to the power of that, and to give you a sense of what's happened with", "tokens": [400, 498, 291, 352, 308, 281, 264, 1347, 295, 300, 11, 293, 281, 976, 291, 257, 2020, 295, 437, 311, 2011, 365], "temperature": 0.0, "avg_logprob": -0.15045156340668167, "compression_ratio": 1.4642857142857142, "no_speech_prob": 2.2252750113693764e-06}, {"id": 1045, "seek": 501190, "start": 5024.0, "end": 5032.2, "text": " language models, if you look at academic papers from about 18 months ago, you'll see them", "tokens": [2856, 5245, 11, 498, 291, 574, 412, 7778, 10577, 490, 466, 2443, 2493, 2057, 11, 291, 603, 536, 552], "temperature": 0.0, "avg_logprob": -0.15045156340668167, "compression_ratio": 1.4642857142857142, "no_speech_prob": 2.2252750113693764e-06}, {"id": 1046, "seek": 501190, "start": 5032.2, "end": 5040.48, "text": " talking about perplexities, state-of-the-art perplexities of over 100.", "tokens": [1417, 466, 680, 18945, 1088, 11, 1785, 12, 2670, 12, 3322, 12, 446, 680, 18945, 1088, 295, 670, 2319, 13], "temperature": 0.0, "avg_logprob": -0.15045156340668167, "compression_ratio": 1.4642857142857142, "no_speech_prob": 2.2252750113693764e-06}, {"id": 1047, "seek": 504048, "start": 5040.48, "end": 5046.36, "text": " The rate at which our ability to understand language, and I think measuring language model", "tokens": [440, 3314, 412, 597, 527, 3485, 281, 1223, 2856, 11, 293, 286, 519, 13389, 2856, 2316], "temperature": 0.0, "avg_logprob": -0.13319582226632656, "compression_ratio": 1.6863636363636363, "no_speech_prob": 4.092883045814233e-06}, {"id": 1048, "seek": 504048, "start": 5046.36, "end": 5052.5599999999995, "text": " accuracy or perplexity is not a terrible proxy for understanding language.", "tokens": [14170, 420, 680, 18945, 507, 307, 406, 257, 6237, 29690, 337, 3701, 2856, 13], "temperature": 0.0, "avg_logprob": -0.13319582226632656, "compression_ratio": 1.6863636363636363, "no_speech_prob": 4.092883045814233e-06}, {"id": 1049, "seek": 504048, "start": 5052.5599999999995, "end": 5057.639999999999, "text": " If I can guess what you're going to say next, I pretty much need to understand language", "tokens": [759, 286, 393, 2041, 437, 291, 434, 516, 281, 584, 958, 11, 286, 1238, 709, 643, 281, 1223, 2856], "temperature": 0.0, "avg_logprob": -0.13319582226632656, "compression_ratio": 1.6863636363636363, "no_speech_prob": 4.092883045814233e-06}, {"id": 1050, "seek": 504048, "start": 5057.639999999999, "end": 5061.839999999999, "text": " pretty well, and also the kind of things you might talk about pretty well.", "tokens": [1238, 731, 11, 293, 611, 264, 733, 295, 721, 291, 1062, 751, 466, 1238, 731, 13], "temperature": 0.0, "avg_logprob": -0.13319582226632656, "compression_ratio": 1.6863636363636363, "no_speech_prob": 4.092883045814233e-06}, {"id": 1051, "seek": 504048, "start": 5061.839999999999, "end": 5066.799999999999, "text": " So this number has just come down so much.", "tokens": [407, 341, 1230, 575, 445, 808, 760, 370, 709, 13], "temperature": 0.0, "avg_logprob": -0.13319582226632656, "compression_ratio": 1.6863636363636363, "no_speech_prob": 4.092883045814233e-06}, {"id": 1052, "seek": 506680, "start": 5066.8, "end": 5072.320000000001, "text": " It's been amazing, NLP in the last 12-18 months, and it's going to come down a lot more.", "tokens": [467, 311, 668, 2243, 11, 426, 45196, 294, 264, 1036, 2272, 12, 6494, 2493, 11, 293, 309, 311, 516, 281, 808, 760, 257, 688, 544, 13], "temperature": 0.0, "avg_logprob": -0.16423726322674992, "compression_ratio": 1.4895397489539748, "no_speech_prob": 3.237746568629518e-06}, {"id": 1053, "seek": 506680, "start": 5072.320000000001, "end": 5077.400000000001, "text": " It really feels like 2011-2012 computer vision.", "tokens": [467, 534, 3417, 411, 10154, 12, 2009, 4762, 3820, 5201, 13], "temperature": 0.0, "avg_logprob": -0.16423726322674992, "compression_ratio": 1.4895397489539748, "no_speech_prob": 3.237746568629518e-06}, {"id": 1054, "seek": 506680, "start": 5077.400000000001, "end": 5081.76, "text": " We're just starting to understand transfer learning and fine-tuning, and these basic", "tokens": [492, 434, 445, 2891, 281, 1223, 5003, 2539, 293, 2489, 12, 83, 37726, 11, 293, 613, 3875], "temperature": 0.0, "avg_logprob": -0.16423726322674992, "compression_ratio": 1.4895397489539748, "no_speech_prob": 3.237746568629518e-06}, {"id": 1055, "seek": 506680, "start": 5081.76, "end": 5084.92, "text": " models are getting so much better.", "tokens": [5245, 366, 1242, 370, 709, 1101, 13], "temperature": 0.0, "avg_logprob": -0.16423726322674992, "compression_ratio": 1.4895397489539748, "no_speech_prob": 3.237746568629518e-06}, {"id": 1056, "seek": 506680, "start": 5084.92, "end": 5092.8, "text": " So everything you thought about, like what NLP can and can't do, is very rapidly going", "tokens": [407, 1203, 291, 1194, 466, 11, 411, 437, 426, 45196, 393, 293, 393, 380, 360, 11, 307, 588, 12910, 516], "temperature": 0.0, "avg_logprob": -0.16423726322674992, "compression_ratio": 1.4895397489539748, "no_speech_prob": 3.237746568629518e-06}, {"id": 1057, "seek": 506680, "start": 5092.8, "end": 5093.8, "text": " out of date.", "tokens": [484, 295, 4002, 13], "temperature": 0.0, "avg_logprob": -0.16423726322674992, "compression_ratio": 1.4895397489539748, "no_speech_prob": 3.237746568629518e-06}, {"id": 1058, "seek": 509380, "start": 5093.8, "end": 5097.56, "text": " There's still lots of stuff NLP is not good at, to be clear.", "tokens": [821, 311, 920, 3195, 295, 1507, 426, 45196, 307, 406, 665, 412, 11, 281, 312, 1850, 13], "temperature": 0.0, "avg_logprob": -0.15335430548741266, "compression_ratio": 1.6541666666666666, "no_speech_prob": 5.42217730981065e-06}, {"id": 1059, "seek": 509380, "start": 5097.56, "end": 5101.16, "text": " Just like in 2012, there was lots of stuff computer vision wasn't good at.", "tokens": [1449, 411, 294, 9125, 11, 456, 390, 3195, 295, 1507, 3820, 5201, 2067, 380, 665, 412, 13], "temperature": 0.0, "avg_logprob": -0.15335430548741266, "compression_ratio": 1.6541666666666666, "no_speech_prob": 5.42217730981065e-06}, {"id": 1060, "seek": 509380, "start": 5101.16, "end": 5106.64, "text": " But it's changing incredibly rapidly, and now is a very, very good time to be getting", "tokens": [583, 309, 311, 4473, 6252, 12910, 11, 293, 586, 307, 257, 588, 11, 588, 665, 565, 281, 312, 1242], "temperature": 0.0, "avg_logprob": -0.15335430548741266, "compression_ratio": 1.6541666666666666, "no_speech_prob": 5.42217730981065e-06}, {"id": 1061, "seek": 509380, "start": 5106.64, "end": 5112.2, "text": " very, very good at NLP, or starting startups based on NLP, because there's a whole bunch", "tokens": [588, 11, 588, 665, 412, 426, 45196, 11, 420, 2891, 28041, 2361, 322, 426, 45196, 11, 570, 456, 311, 257, 1379, 3840], "temperature": 0.0, "avg_logprob": -0.15335430548741266, "compression_ratio": 1.6541666666666666, "no_speech_prob": 5.42217730981065e-06}, {"id": 1062, "seek": 509380, "start": 5112.2, "end": 5119.6, "text": " of stuff which computers would absolutely shit at two years ago, and now are not quite", "tokens": [295, 1507, 597, 10807, 576, 3122, 4611, 412, 732, 924, 2057, 11, 293, 586, 366, 406, 1596], "temperature": 0.0, "avg_logprob": -0.15335430548741266, "compression_ratio": 1.6541666666666666, "no_speech_prob": 5.42217730981065e-06}, {"id": 1063, "seek": 511960, "start": 5119.6, "end": 5124.280000000001, "text": " as good at people, and then next year they'll be much better than people.", "tokens": [382, 665, 412, 561, 11, 293, 550, 958, 1064, 436, 603, 312, 709, 1101, 813, 561, 13], "temperature": 0.0, "avg_logprob": -0.22078767735907373, "compression_ratio": 1.5323383084577114, "no_speech_prob": 6.048879640729865e-06}, {"id": 1064, "seek": 511960, "start": 5124.280000000001, "end": 5126.280000000001, "text": " Two questions.", "tokens": [4453, 1651, 13], "temperature": 0.0, "avg_logprob": -0.22078767735907373, "compression_ratio": 1.5323383084577114, "no_speech_prob": 6.048879640729865e-06}, {"id": 1065, "seek": 511960, "start": 5126.280000000001, "end": 5135.0, "text": " One, what is your ratio of paper reading versus coding in a week?", "tokens": [1485, 11, 437, 307, 428, 8509, 295, 3035, 3760, 5717, 17720, 294, 257, 1243, 30], "temperature": 0.0, "avg_logprob": -0.22078767735907373, "compression_ratio": 1.5323383084577114, "no_speech_prob": 6.048879640729865e-06}, {"id": 1066, "seek": 511960, "start": 5135.0, "end": 5136.0, "text": " What do you think, Rachel?", "tokens": [708, 360, 291, 519, 11, 14246, 30], "temperature": 0.0, "avg_logprob": -0.22078767735907373, "compression_ratio": 1.5323383084577114, "no_speech_prob": 6.048879640729865e-06}, {"id": 1067, "seek": 511960, "start": 5136.0, "end": 5137.0, "text": " You see me.", "tokens": [509, 536, 385, 13], "temperature": 0.0, "avg_logprob": -0.22078767735907373, "compression_ratio": 1.5323383084577114, "no_speech_prob": 6.048879640729865e-06}, {"id": 1068, "seek": 511960, "start": 5137.0, "end": 5139.04, "text": " I mean it's a lot more coding, right?", "tokens": [286, 914, 309, 311, 257, 688, 544, 17720, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.22078767735907373, "compression_ratio": 1.5323383084577114, "no_speech_prob": 6.048879640729865e-06}, {"id": 1069, "seek": 511960, "start": 5139.04, "end": 5140.04, "text": " It's a lot more coding.", "tokens": [467, 311, 257, 688, 544, 17720, 13], "temperature": 0.0, "avg_logprob": -0.22078767735907373, "compression_ratio": 1.5323383084577114, "no_speech_prob": 6.048879640729865e-06}, {"id": 1070, "seek": 511960, "start": 5140.04, "end": 5144.320000000001, "text": " I feel like it also really varies from week to week.", "tokens": [286, 841, 411, 309, 611, 534, 21716, 490, 1243, 281, 1243, 13], "temperature": 0.0, "avg_logprob": -0.22078767735907373, "compression_ratio": 1.5323383084577114, "no_speech_prob": 6.048879640729865e-06}, {"id": 1071, "seek": 514432, "start": 5144.32, "end": 5154.04, "text": " Like with that bounding box stuff, there was all these papers and no map through them,", "tokens": [1743, 365, 300, 5472, 278, 2424, 1507, 11, 456, 390, 439, 613, 10577, 293, 572, 4471, 807, 552, 11], "temperature": 0.0, "avg_logprob": -0.1685632455228555, "compression_ratio": 1.6484018264840183, "no_speech_prob": 1.0129905604117084e-05}, {"id": 1072, "seek": 514432, "start": 5154.04, "end": 5158.2, "text": " and so I didn't even know which one to read first, and then I'd read the citations and", "tokens": [293, 370, 286, 994, 380, 754, 458, 597, 472, 281, 1401, 700, 11, 293, 550, 286, 1116, 1401, 264, 4814, 763, 293], "temperature": 0.0, "avg_logprob": -0.1685632455228555, "compression_ratio": 1.6484018264840183, "no_speech_prob": 1.0129905604117084e-05}, {"id": 1073, "seek": 514432, "start": 5158.2, "end": 5160.36, "text": " didn't understand any of them.", "tokens": [994, 380, 1223, 604, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.1685632455228555, "compression_ratio": 1.6484018264840183, "no_speech_prob": 1.0129905604117084e-05}, {"id": 1074, "seek": 514432, "start": 5160.36, "end": 5167.36, "text": " So there was a few weeks of just kind of reading papers before I even knew what to start coding.", "tokens": [407, 456, 390, 257, 1326, 3259, 295, 445, 733, 295, 3760, 10577, 949, 286, 754, 2586, 437, 281, 722, 17720, 13], "temperature": 0.0, "avg_logprob": -0.1685632455228555, "compression_ratio": 1.6484018264840183, "no_speech_prob": 1.0129905604117084e-05}, {"id": 1075, "seek": 514432, "start": 5167.36, "end": 5168.36, "text": " That's unusual though.", "tokens": [663, 311, 10901, 1673, 13], "temperature": 0.0, "avg_logprob": -0.1685632455228555, "compression_ratio": 1.6484018264840183, "no_speech_prob": 1.0129905604117084e-05}, {"id": 1076, "seek": 514432, "start": 5168.36, "end": 5171.04, "text": " Like most of the time, I don't know.", "tokens": [1743, 881, 295, 264, 565, 11, 286, 500, 380, 458, 13], "temperature": 0.0, "avg_logprob": -0.1685632455228555, "compression_ratio": 1.6484018264840183, "no_speech_prob": 1.0129905604117084e-05}, {"id": 1077, "seek": 517104, "start": 5171.04, "end": 5175.8, "text": " Anytime I start reading a paper, I'm always convinced that I'm not smart enough to understand", "tokens": [39401, 286, 722, 3760, 257, 3035, 11, 286, 478, 1009, 12561, 300, 286, 478, 406, 4069, 1547, 281, 1223], "temperature": 0.0, "avg_logprob": -0.2427273021024816, "compression_ratio": 1.536697247706422, "no_speech_prob": 3.340529701745254e-06}, {"id": 1078, "seek": 517104, "start": 5175.8, "end": 5181.76, "text": " it, always, regardless of the paper, and somehow eventually I do.", "tokens": [309, 11, 1009, 11, 10060, 295, 264, 3035, 11, 293, 6063, 4728, 286, 360, 13], "temperature": 0.0, "avg_logprob": -0.2427273021024816, "compression_ratio": 1.536697247706422, "no_speech_prob": 3.340529701745254e-06}, {"id": 1079, "seek": 517104, "start": 5181.76, "end": 5188.24, "text": " But yeah, I try to spend as much time as I can coding.", "tokens": [583, 1338, 11, 286, 853, 281, 3496, 382, 709, 565, 382, 286, 393, 17720, 13], "temperature": 0.0, "avg_logprob": -0.2427273021024816, "compression_ratio": 1.536697247706422, "no_speech_prob": 3.340529701745254e-06}, {"id": 1080, "seek": 517104, "start": 5188.24, "end": 5192.88, "text": " The second question, is your dropout rate the same through the training or do you adjust", "tokens": [440, 1150, 1168, 11, 307, 428, 3270, 346, 3314, 264, 912, 807, 264, 3097, 420, 360, 291, 4369], "temperature": 0.0, "avg_logprob": -0.2427273021024816, "compression_ratio": 1.536697247706422, "no_speech_prob": 3.340529701745254e-06}, {"id": 1081, "seek": 517104, "start": 5192.88, "end": 5194.56, "text": " it and the weights accordingly?", "tokens": [309, 293, 264, 17443, 19717, 30], "temperature": 0.0, "avg_logprob": -0.2427273021024816, "compression_ratio": 1.536697247706422, "no_speech_prob": 3.340529701745254e-06}, {"id": 1082, "seek": 519456, "start": 5194.56, "end": 5201.92, "text": " I'll just say one more thing about the last bit, which is very often, like the vast majority,", "tokens": [286, 603, 445, 584, 472, 544, 551, 466, 264, 1036, 857, 11, 597, 307, 588, 2049, 11, 411, 264, 8369, 6286, 11], "temperature": 0.0, "avg_logprob": -0.1769832024207482, "compression_ratio": 1.9294117647058824, "no_speech_prob": 1.118938939725922e-06}, {"id": 1083, "seek": 519456, "start": 5201.92, "end": 5209.92, "text": " nearly always, after I've read a paper, even after I've read the bit that says this is", "tokens": [6217, 1009, 11, 934, 286, 600, 1401, 257, 3035, 11, 754, 934, 286, 600, 1401, 264, 857, 300, 1619, 341, 307], "temperature": 0.0, "avg_logprob": -0.1769832024207482, "compression_ratio": 1.9294117647058824, "no_speech_prob": 1.118938939725922e-06}, {"id": 1084, "seek": 519456, "start": 5209.92, "end": 5214.120000000001, "text": " the problem I'm trying to solve, I'll kind of stop there and try to implement something", "tokens": [264, 1154, 286, 478, 1382, 281, 5039, 11, 286, 603, 733, 295, 1590, 456, 293, 853, 281, 4445, 746], "temperature": 0.0, "avg_logprob": -0.1769832024207482, "compression_ratio": 1.9294117647058824, "no_speech_prob": 1.118938939725922e-06}, {"id": 1085, "seek": 519456, "start": 5214.120000000001, "end": 5217.64, "text": " that I think might solve that problem, and then I'll go back and read the paper and I'll", "tokens": [300, 286, 519, 1062, 5039, 300, 1154, 11, 293, 550, 286, 603, 352, 646, 293, 1401, 264, 3035, 293, 286, 603], "temperature": 0.0, "avg_logprob": -0.1769832024207482, "compression_ratio": 1.9294117647058824, "no_speech_prob": 1.118938939725922e-06}, {"id": 1086, "seek": 519456, "start": 5217.64, "end": 5221.92, "text": " read little bits about how I solved these problem bits, and I'll be like, oh that's", "tokens": [1401, 707, 9239, 466, 577, 286, 13041, 613, 1154, 9239, 11, 293, 286, 603, 312, 411, 11, 1954, 300, 311], "temperature": 0.0, "avg_logprob": -0.1769832024207482, "compression_ratio": 1.9294117647058824, "no_speech_prob": 1.118938939725922e-06}, {"id": 1087, "seek": 519456, "start": 5221.92, "end": 5224.160000000001, "text": " a good idea, and then I'll try to implement those.", "tokens": [257, 665, 1558, 11, 293, 550, 286, 603, 853, 281, 4445, 729, 13], "temperature": 0.0, "avg_logprob": -0.1769832024207482, "compression_ratio": 1.9294117647058824, "no_speech_prob": 1.118938939725922e-06}, {"id": 1088, "seek": 522416, "start": 5224.16, "end": 5231.4, "text": " And so that's why, for example, I didn't actually implement SSD, my custom head is not the same", "tokens": [400, 370, 300, 311, 983, 11, 337, 1365, 11, 286, 994, 380, 767, 4445, 30262, 11, 452, 2375, 1378, 307, 406, 264, 912], "temperature": 0.0, "avg_logprob": -0.18180711860330695, "compression_ratio": 1.6263736263736264, "no_speech_prob": 1.0616021427267697e-05}, {"id": 1089, "seek": 522416, "start": 5231.4, "end": 5235.639999999999, "text": " as their head, it's because I kind of read the gist of it and then I tried to create", "tokens": [382, 641, 1378, 11, 309, 311, 570, 286, 733, 295, 1401, 264, 290, 468, 295, 309, 293, 550, 286, 3031, 281, 1884], "temperature": 0.0, "avg_logprob": -0.18180711860330695, "compression_ratio": 1.6263736263736264, "no_speech_prob": 1.0616021427267697e-05}, {"id": 1090, "seek": 522416, "start": 5235.639999999999, "end": 5240.72, "text": " something best as I could and then go back to the papers and try to see why.", "tokens": [746, 1151, 382, 286, 727, 293, 550, 352, 646, 281, 264, 10577, 293, 853, 281, 536, 983, 13], "temperature": 0.0, "avg_logprob": -0.18180711860330695, "compression_ratio": 1.6263736263736264, "no_speech_prob": 1.0616021427267697e-05}, {"id": 1091, "seek": 522416, "start": 5240.72, "end": 5246.16, "text": " So by the time I got to the focal loss paper, Rachel will tell you, I was driving myself", "tokens": [407, 538, 264, 565, 286, 658, 281, 264, 26592, 4470, 3035, 11, 14246, 486, 980, 291, 11, 286, 390, 4840, 2059], "temperature": 0.0, "avg_logprob": -0.18180711860330695, "compression_ratio": 1.6263736263736264, "no_speech_prob": 1.0616021427267697e-05}, {"id": 1092, "seek": 522416, "start": 5246.16, "end": 5251.84, "text": " crazy with like, how come I can't find small objects, how come it's always predicting background,", "tokens": [3219, 365, 411, 11, 577, 808, 286, 393, 380, 915, 1359, 6565, 11, 577, 808, 309, 311, 1009, 32884, 3678, 11], "temperature": 0.0, "avg_logprob": -0.18180711860330695, "compression_ratio": 1.6263736263736264, "no_speech_prob": 1.0616021427267697e-05}, {"id": 1093, "seek": 525184, "start": 5251.84, "end": 5256.96, "text": " you know, and I read the focal loss paper and I was like, that's why!", "tokens": [291, 458, 11, 293, 286, 1401, 264, 26592, 4470, 3035, 293, 286, 390, 411, 11, 300, 311, 983, 0], "temperature": 0.0, "avg_logprob": -0.17659247716267903, "compression_ratio": 1.7154471544715446, "no_speech_prob": 1.7502639821032062e-05}, {"id": 1094, "seek": 525184, "start": 5256.96, "end": 5262.76, "text": " You know, so it's so much better when you deeply understand the problem they're trying", "tokens": [509, 458, 11, 370, 309, 311, 370, 709, 1101, 562, 291, 8760, 1223, 264, 1154, 436, 434, 1382], "temperature": 0.0, "avg_logprob": -0.17659247716267903, "compression_ratio": 1.7154471544715446, "no_speech_prob": 1.7502639821032062e-05}, {"id": 1095, "seek": 525184, "start": 5262.76, "end": 5263.76, "text": " to solve.", "tokens": [281, 5039, 13], "temperature": 0.0, "avg_logprob": -0.17659247716267903, "compression_ratio": 1.7154471544715446, "no_speech_prob": 1.7502639821032062e-05}, {"id": 1096, "seek": 525184, "start": 5263.76, "end": 5267.52, "text": " And I do find the vast majority of the time, by the time I read that bit of the paper,", "tokens": [400, 286, 360, 915, 264, 8369, 6286, 295, 264, 565, 11, 538, 264, 565, 286, 1401, 300, 857, 295, 264, 3035, 11], "temperature": 0.0, "avg_logprob": -0.17659247716267903, "compression_ratio": 1.7154471544715446, "no_speech_prob": 1.7502639821032062e-05}, {"id": 1097, "seek": 525184, "start": 5267.52, "end": 5273.08, "text": " which is like solving the problem, I'm then like, yeah, but these 3 ideas I came up with,", "tokens": [597, 307, 411, 12606, 264, 1154, 11, 286, 478, 550, 411, 11, 1338, 11, 457, 613, 805, 3487, 286, 1361, 493, 365, 11], "temperature": 0.0, "avg_logprob": -0.17659247716267903, "compression_ratio": 1.7154471544715446, "no_speech_prob": 1.7502639821032062e-05}, {"id": 1098, "seek": 525184, "start": 5273.08, "end": 5274.08, "text": " they didn't try.", "tokens": [436, 994, 380, 853, 13], "temperature": 0.0, "avg_logprob": -0.17659247716267903, "compression_ratio": 1.7154471544715446, "no_speech_prob": 1.7502639821032062e-05}, {"id": 1099, "seek": 525184, "start": 5274.08, "end": 5276.84, "text": " You know, and you suddenly realize that you've got new ideas.", "tokens": [509, 458, 11, 293, 291, 5800, 4325, 300, 291, 600, 658, 777, 3487, 13], "temperature": 0.0, "avg_logprob": -0.17659247716267903, "compression_ratio": 1.7154471544715446, "no_speech_prob": 1.7502639821032062e-05}, {"id": 1100, "seek": 527684, "start": 5276.84, "end": 5284.88, "text": " Or else if you just implement the paper mindlessly, you tend not to have these insights about", "tokens": [1610, 1646, 498, 291, 445, 4445, 264, 3035, 1575, 12048, 11, 291, 3928, 406, 281, 362, 613, 14310, 466], "temperature": 0.0, "avg_logprob": -0.1832090157728929, "compression_ratio": 1.6683673469387754, "no_speech_prob": 7.889150765549857e-06}, {"id": 1101, "seek": 527684, "start": 5284.88, "end": 5290.16, "text": " better ways to do it.", "tokens": [1101, 2098, 281, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.1832090157728929, "compression_ratio": 1.6683673469387754, "no_speech_prob": 7.889150765549857e-06}, {"id": 1102, "seek": 527684, "start": 5290.16, "end": 5295.12, "text": " Varying dropout is really interesting and there are some recent papers actually that", "tokens": [691, 822, 278, 3270, 346, 307, 534, 1880, 293, 456, 366, 512, 5162, 10577, 767, 300], "temperature": 0.0, "avg_logprob": -0.1832090157728929, "compression_ratio": 1.6683673469387754, "no_speech_prob": 7.889150765549857e-06}, {"id": 1103, "seek": 527684, "start": 5295.12, "end": 5299.56, "text": " suggest gradually changing dropout.", "tokens": [3402, 13145, 4473, 3270, 346, 13], "temperature": 0.0, "avg_logprob": -0.1832090157728929, "compression_ratio": 1.6683673469387754, "no_speech_prob": 7.889150765549857e-06}, {"id": 1104, "seek": 527684, "start": 5299.56, "end": 5303.8, "text": " And it was either a good idea to gradually make it smaller or to gradually make it bigger,", "tokens": [400, 309, 390, 2139, 257, 665, 1558, 281, 13145, 652, 309, 4356, 420, 281, 13145, 652, 309, 3801, 11], "temperature": 0.0, "avg_logprob": -0.1832090157728929, "compression_ratio": 1.6683673469387754, "no_speech_prob": 7.889150765549857e-06}, {"id": 1105, "seek": 530380, "start": 5303.8, "end": 5308.52, "text": " I'm not sure which.", "tokens": [286, 478, 406, 988, 597, 13], "temperature": 0.0, "avg_logprob": -0.19909932536463584, "compression_ratio": 1.5135135135135136, "no_speech_prob": 3.024148099939339e-05}, {"id": 1106, "seek": 530380, "start": 5308.52, "end": 5310.400000000001, "text": " Maybe one of us can try and find it during the week.", "tokens": [2704, 472, 295, 505, 393, 853, 293, 915, 309, 1830, 264, 1243, 13], "temperature": 0.0, "avg_logprob": -0.19909932536463584, "compression_ratio": 1.5135135135135136, "no_speech_prob": 3.024148099939339e-05}, {"id": 1107, "seek": 530380, "start": 5310.400000000001, "end": 5312.92, "text": " I haven't seen it widely used.", "tokens": [286, 2378, 380, 1612, 309, 13371, 1143, 13], "temperature": 0.0, "avg_logprob": -0.19909932536463584, "compression_ratio": 1.5135135135135136, "no_speech_prob": 3.024148099939339e-05}, {"id": 1108, "seek": 530380, "start": 5312.92, "end": 5320.360000000001, "text": " I tried it a little bit with the most recent paper I wrote and it had some good results.", "tokens": [286, 3031, 309, 257, 707, 857, 365, 264, 881, 5162, 3035, 286, 4114, 293, 309, 632, 512, 665, 3542, 13], "temperature": 0.0, "avg_logprob": -0.19909932536463584, "compression_ratio": 1.5135135135135136, "no_speech_prob": 3.024148099939339e-05}, {"id": 1109, "seek": 530380, "start": 5320.360000000001, "end": 5325.8, "text": " I think I was gradually making it smaller.", "tokens": [286, 519, 286, 390, 13145, 1455, 309, 4356, 13], "temperature": 0.0, "avg_logprob": -0.19909932536463584, "compression_ratio": 1.5135135135135136, "no_speech_prob": 3.024148099939339e-05}, {"id": 1110, "seek": 530380, "start": 5325.8, "end": 5330.4800000000005, "text": " The next question is, am I correct in thinking that this language model is built on word", "tokens": [440, 958, 1168, 307, 11, 669, 286, 3006, 294, 1953, 300, 341, 2856, 2316, 307, 3094, 322, 1349], "temperature": 0.0, "avg_logprob": -0.19909932536463584, "compression_ratio": 1.5135135135135136, "no_speech_prob": 3.024148099939339e-05}, {"id": 1111, "seek": 530380, "start": 5330.4800000000005, "end": 5331.4800000000005, "text": " embeddings?", "tokens": [12240, 29432, 30], "temperature": 0.0, "avg_logprob": -0.19909932536463584, "compression_ratio": 1.5135135135135136, "no_speech_prob": 3.024148099939339e-05}, {"id": 1112, "seek": 533148, "start": 5331.48, "end": 5336.12, "text": " Would it be valuable to try this with phrase or sentence embeddings?", "tokens": [6068, 309, 312, 8263, 281, 853, 341, 365, 9535, 420, 8174, 12240, 29432, 30], "temperature": 0.0, "avg_logprob": -0.243029135244864, "compression_ratio": 1.592783505154639, "no_speech_prob": 7.296281182789244e-06}, {"id": 1113, "seek": 533148, "start": 5336.12, "end": 5342.36, "text": " I asked this because I saw from Google the other day, universal sentence encoder.", "tokens": [286, 2351, 341, 570, 286, 1866, 490, 3329, 264, 661, 786, 11, 11455, 8174, 2058, 19866, 13], "temperature": 0.0, "avg_logprob": -0.243029135244864, "compression_ratio": 1.592783505154639, "no_speech_prob": 7.296281182789244e-06}, {"id": 1114, "seek": 533148, "start": 5342.36, "end": 5346.36, "text": " Yes, this is much better than that.", "tokens": [1079, 11, 341, 307, 709, 1101, 813, 300, 13], "temperature": 0.0, "avg_logprob": -0.243029135244864, "compression_ratio": 1.592783505154639, "no_speech_prob": 7.296281182789244e-06}, {"id": 1115, "seek": 533148, "start": 5346.36, "end": 5350.78, "text": " This is not just an embedding of a sentence, this is an entire model.", "tokens": [639, 307, 406, 445, 364, 12240, 3584, 295, 257, 8174, 11, 341, 307, 364, 2302, 2316, 13], "temperature": 0.0, "avg_logprob": -0.243029135244864, "compression_ratio": 1.592783505154639, "no_speech_prob": 7.296281182789244e-06}, {"id": 1116, "seek": 533148, "start": 5350.78, "end": 5357.2, "text": " So an embedding by definition is like a fixed thing.", "tokens": [407, 364, 12240, 3584, 538, 7123, 307, 411, 257, 6806, 551, 13], "temperature": 0.0, "avg_logprob": -0.243029135244864, "compression_ratio": 1.592783505154639, "no_speech_prob": 7.296281182789244e-06}, {"id": 1117, "seek": 535720, "start": 5357.2, "end": 5362.16, "text": " I think they're asking, they're saying that this language, the first question is, is this", "tokens": [286, 519, 436, 434, 3365, 11, 436, 434, 1566, 300, 341, 2856, 11, 264, 700, 1168, 307, 11, 307, 341], "temperature": 0.0, "avg_logprob": -0.22623880774573943, "compression_ratio": 1.868421052631579, "no_speech_prob": 2.144438803952653e-05}, {"id": 1118, "seek": 535720, "start": 5362.16, "end": 5364.2, "text": " language model built on word embeddings?", "tokens": [2856, 2316, 3094, 322, 1349, 12240, 29432, 30], "temperature": 0.0, "avg_logprob": -0.22623880774573943, "compression_ratio": 1.868421052631579, "no_speech_prob": 2.144438803952653e-05}, {"id": 1119, "seek": 535720, "start": 5364.2, "end": 5373.8, "text": " But as I was saying, a sentence or a phrase embedding is always a model that creates that.", "tokens": [583, 382, 286, 390, 1566, 11, 257, 8174, 420, 257, 9535, 12240, 3584, 307, 1009, 257, 2316, 300, 7829, 300, 13], "temperature": 0.0, "avg_logprob": -0.22623880774573943, "compression_ratio": 1.868421052631579, "no_speech_prob": 2.144438803952653e-05}, {"id": 1120, "seek": 535720, "start": 5373.8, "end": 5378.5199999999995, "text": " We've got a model that's trying to understand language.", "tokens": [492, 600, 658, 257, 2316, 300, 311, 1382, 281, 1223, 2856, 13], "temperature": 0.0, "avg_logprob": -0.22623880774573943, "compression_ratio": 1.868421052631579, "no_speech_prob": 2.144438803952653e-05}, {"id": 1121, "seek": 535720, "start": 5378.5199999999995, "end": 5384.32, "text": " It's not just a phrase, it's not just a sentence, it's a document in the end and it's not just", "tokens": [467, 311, 406, 445, 257, 9535, 11, 309, 311, 406, 445, 257, 8174, 11, 309, 311, 257, 4166, 294, 264, 917, 293, 309, 311, 406, 445], "temperature": 0.0, "avg_logprob": -0.22623880774573943, "compression_ratio": 1.868421052631579, "no_speech_prob": 2.144438803952653e-05}, {"id": 1122, "seek": 535720, "start": 5384.32, "end": 5387.0, "text": " an embedding, we're training through the whole thing.", "tokens": [364, 12240, 3584, 11, 321, 434, 3097, 807, 264, 1379, 551, 13], "temperature": 0.0, "avg_logprob": -0.22623880774573943, "compression_ratio": 1.868421052631579, "no_speech_prob": 2.144438803952653e-05}, {"id": 1123, "seek": 538700, "start": 5387.0, "end": 5394.16, "text": " So this has been a huge problem with NLP for years now, is this attachment they have to", "tokens": [407, 341, 575, 668, 257, 2603, 1154, 365, 426, 45196, 337, 924, 586, 11, 307, 341, 19431, 436, 362, 281], "temperature": 0.0, "avg_logprob": -0.21812394400623358, "compression_ratio": 1.5133079847908746, "no_speech_prob": 3.6688404634332983e-06}, {"id": 1124, "seek": 538700, "start": 5394.16, "end": 5395.16, "text": " embeddings.", "tokens": [12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.21812394400623358, "compression_ratio": 1.5133079847908746, "no_speech_prob": 3.6688404634332983e-06}, {"id": 1125, "seek": 538700, "start": 5395.16, "end": 5401.12, "text": " So even the paper that the community has been most excited about recently from AI2, the", "tokens": [407, 754, 264, 3035, 300, 264, 1768, 575, 668, 881, 2919, 466, 3938, 490, 7318, 17, 11, 264], "temperature": 0.0, "avg_logprob": -0.21812394400623358, "compression_ratio": 1.5133079847908746, "no_speech_prob": 3.6688404634332983e-06}, {"id": 1126, "seek": 538700, "start": 5401.12, "end": 5408.4, "text": " Allen Institute, called ELMO, E-L-M-O, they found much better results across lots of models.", "tokens": [17160, 9446, 11, 1219, 14426, 18976, 11, 462, 12, 43, 12, 44, 12, 46, 11, 436, 1352, 709, 1101, 3542, 2108, 3195, 295, 5245, 13], "temperature": 0.0, "avg_logprob": -0.21812394400623358, "compression_ratio": 1.5133079847908746, "no_speech_prob": 3.6688404634332983e-06}, {"id": 1127, "seek": 538700, "start": 5408.4, "end": 5409.88, "text": " But again, it was an embedding.", "tokens": [583, 797, 11, 309, 390, 364, 12240, 3584, 13], "temperature": 0.0, "avg_logprob": -0.21812394400623358, "compression_ratio": 1.5133079847908746, "no_speech_prob": 3.6688404634332983e-06}, {"id": 1128, "seek": 538700, "start": 5409.88, "end": 5416.08, "text": " They took a fixed model and created a fixed set of numbers which they then fed into a", "tokens": [814, 1890, 257, 6806, 2316, 293, 2942, 257, 6806, 992, 295, 3547, 597, 436, 550, 4636, 666, 257], "temperature": 0.0, "avg_logprob": -0.21812394400623358, "compression_ratio": 1.5133079847908746, "no_speech_prob": 3.6688404634332983e-06}, {"id": 1129, "seek": 541608, "start": 5416.08, "end": 5417.08, "text": " model.", "tokens": [2316, 13], "temperature": 0.0, "avg_logprob": -0.15996490054660373, "compression_ratio": 1.547085201793722, "no_speech_prob": 4.092883955308935e-06}, {"id": 1130, "seek": 541608, "start": 5417.08, "end": 5424.36, "text": " But in computer vision, we've known for years that that approach of having a fixed set of", "tokens": [583, 294, 3820, 5201, 11, 321, 600, 2570, 337, 924, 300, 300, 3109, 295, 1419, 257, 6806, 992, 295], "temperature": 0.0, "avg_logprob": -0.15996490054660373, "compression_ratio": 1.547085201793722, "no_speech_prob": 4.092883955308935e-06}, {"id": 1131, "seek": 541608, "start": 5424.36, "end": 5428.68, "text": " features, they're called hypercolons in computer vision.", "tokens": [4122, 11, 436, 434, 1219, 9848, 8768, 892, 294, 3820, 5201, 13], "temperature": 0.0, "avg_logprob": -0.15996490054660373, "compression_ratio": 1.547085201793722, "no_speech_prob": 4.092883955308935e-06}, {"id": 1132, "seek": 541608, "start": 5428.68, "end": 5435.6, "text": " People stopped using them like 3 or 4 years ago because fine-tuning the entire model works", "tokens": [3432, 5936, 1228, 552, 411, 805, 420, 1017, 924, 2057, 570, 2489, 12, 83, 37726, 264, 2302, 2316, 1985], "temperature": 0.0, "avg_logprob": -0.15996490054660373, "compression_ratio": 1.547085201793722, "no_speech_prob": 4.092883955308935e-06}, {"id": 1133, "seek": 541608, "start": 5435.6, "end": 5437.68, "text": " much better.", "tokens": [709, 1101, 13], "temperature": 0.0, "avg_logprob": -0.15996490054660373, "compression_ratio": 1.547085201793722, "no_speech_prob": 4.092883955308935e-06}, {"id": 1134, "seek": 541608, "start": 5437.68, "end": 5442.12, "text": " So for those of you that have spent quite a lot of time with NLP and not much time with", "tokens": [407, 337, 729, 295, 291, 300, 362, 4418, 1596, 257, 688, 295, 565, 365, 426, 45196, 293, 406, 709, 565, 365], "temperature": 0.0, "avg_logprob": -0.15996490054660373, "compression_ratio": 1.547085201793722, "no_speech_prob": 4.092883955308935e-06}, {"id": 1135, "seek": 544212, "start": 5442.12, "end": 5446.04, "text": " computer vision, you're going to have to start relearning.", "tokens": [3820, 5201, 11, 291, 434, 516, 281, 362, 281, 722, 2951, 2341, 13], "temperature": 0.0, "avg_logprob": -0.1488944926160447, "compression_ratio": 1.6607142857142858, "no_speech_prob": 4.222794359520776e-06}, {"id": 1136, "seek": 544212, "start": 5446.04, "end": 5451.92, "text": " All that stuff you have been told about this idea that there are these things called embeddings", "tokens": [1057, 300, 1507, 291, 362, 668, 1907, 466, 341, 1558, 300, 456, 366, 613, 721, 1219, 12240, 29432], "temperature": 0.0, "avg_logprob": -0.1488944926160447, "compression_ratio": 1.6607142857142858, "no_speech_prob": 4.222794359520776e-06}, {"id": 1137, "seek": 544212, "start": 5451.92, "end": 5458.0, "text": " and that you learn them ahead of time and then you apply these fixed things, whether", "tokens": [293, 300, 291, 1466, 552, 2286, 295, 565, 293, 550, 291, 3079, 613, 6806, 721, 11, 1968], "temperature": 0.0, "avg_logprob": -0.1488944926160447, "compression_ratio": 1.6607142857142858, "no_speech_prob": 4.222794359520776e-06}, {"id": 1138, "seek": 544212, "start": 5458.0, "end": 5464.42, "text": " it be word level or phrase level or whatever level, don't do that.", "tokens": [309, 312, 1349, 1496, 420, 9535, 1496, 420, 2035, 1496, 11, 500, 380, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.1488944926160447, "compression_ratio": 1.6607142857142858, "no_speech_prob": 4.222794359520776e-06}, {"id": 1139, "seek": 544212, "start": 5464.42, "end": 5471.44, "text": " You want to actually create a pre-trained model and fine-tune it.", "tokens": [509, 528, 281, 767, 1884, 257, 659, 12, 17227, 2001, 2316, 293, 2489, 12, 83, 2613, 309, 13], "temperature": 0.0, "avg_logprob": -0.1488944926160447, "compression_ratio": 1.6607142857142858, "no_speech_prob": 4.222794359520776e-06}, {"id": 1140, "seek": 547144, "start": 5471.44, "end": 5482.08, "text": " You'll see some specific results.", "tokens": [509, 603, 536, 512, 2685, 3542, 13], "temperature": 0.0, "avg_logprob": -0.17585443646720286, "compression_ratio": 1.4930232558139536, "no_speech_prob": 2.429938285786193e-05}, {"id": 1141, "seek": 547144, "start": 5482.08, "end": 5486.96, "text": " For using accuracy instead of perplexity as a metric for the model, could we work that", "tokens": [1171, 1228, 14170, 2602, 295, 680, 18945, 507, 382, 257, 20678, 337, 264, 2316, 11, 727, 321, 589, 300], "temperature": 0.0, "avg_logprob": -0.17585443646720286, "compression_ratio": 1.4930232558139536, "no_speech_prob": 2.429938285786193e-05}, {"id": 1142, "seek": 547144, "start": 5486.96, "end": 5490.08, "text": " into the loss function rather than just use it as a metric?", "tokens": [666, 264, 4470, 2445, 2831, 813, 445, 764, 309, 382, 257, 20678, 30], "temperature": 0.0, "avg_logprob": -0.17585443646720286, "compression_ratio": 1.4930232558139536, "no_speech_prob": 2.429938285786193e-05}, {"id": 1143, "seek": 547144, "start": 5490.08, "end": 5494.0, "text": " No, you never want to do that whether it be computer vision or NLP or whatever.", "tokens": [883, 11, 291, 1128, 528, 281, 360, 300, 1968, 309, 312, 3820, 5201, 420, 426, 45196, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.17585443646720286, "compression_ratio": 1.4930232558139536, "no_speech_prob": 2.429938285786193e-05}, {"id": 1144, "seek": 547144, "start": 5494.0, "end": 5496.839999999999, "text": " It's too bumpy.", "tokens": [467, 311, 886, 49400, 13], "temperature": 0.0, "avg_logprob": -0.17585443646720286, "compression_ratio": 1.4930232558139536, "no_speech_prob": 2.429938285786193e-05}, {"id": 1145, "seek": 547144, "start": 5496.839999999999, "end": 5500.28, "text": " So cross-entropy is fine as a loss function.", "tokens": [407, 3278, 12, 317, 27514, 307, 2489, 382, 257, 4470, 2445, 13], "temperature": 0.0, "avg_logprob": -0.17585443646720286, "compression_ratio": 1.4930232558139536, "no_speech_prob": 2.429938285786193e-05}, {"id": 1146, "seek": 550028, "start": 5500.28, "end": 5502.8, "text": " And I'm not saying instead of, I use it in addition to.", "tokens": [400, 286, 478, 406, 1566, 2602, 295, 11, 286, 764, 309, 294, 4500, 281, 13], "temperature": 0.0, "avg_logprob": -0.21784612447908608, "compression_ratio": 1.5765765765765767, "no_speech_prob": 3.844926141027827e-06}, {"id": 1147, "seek": 550028, "start": 5502.8, "end": 5507.5199999999995, "text": " I think it's good to look at the accuracy and to look at the cross-entropy.", "tokens": [286, 519, 309, 311, 665, 281, 574, 412, 264, 14170, 293, 281, 574, 412, 264, 3278, 12, 317, 27514, 13], "temperature": 0.0, "avg_logprob": -0.21784612447908608, "compression_ratio": 1.5765765765765767, "no_speech_prob": 3.844926141027827e-06}, {"id": 1148, "seek": 550028, "start": 5507.5199999999995, "end": 5511.639999999999, "text": " But for your loss function, you need something nice and smooth.", "tokens": [583, 337, 428, 4470, 2445, 11, 291, 643, 746, 1481, 293, 5508, 13], "temperature": 0.0, "avg_logprob": -0.21784612447908608, "compression_ratio": 1.5765765765765767, "no_speech_prob": 3.844926141027827e-06}, {"id": 1149, "seek": 550028, "start": 5511.639999999999, "end": 5515.4, "text": " Accuracy doesn't work very well.", "tokens": [5725, 374, 2551, 1177, 380, 589, 588, 731, 13], "temperature": 0.0, "avg_logprob": -0.21784612447908608, "compression_ratio": 1.5765765765765767, "no_speech_prob": 3.844926141027827e-06}, {"id": 1150, "seek": 550028, "start": 5515.4, "end": 5517.28, "text": " You'll see there's two different versions of save.", "tokens": [509, 603, 536, 456, 311, 732, 819, 9606, 295, 3155, 13], "temperature": 0.0, "avg_logprob": -0.21784612447908608, "compression_ratio": 1.5765765765765767, "no_speech_prob": 3.844926141027827e-06}, {"id": 1151, "seek": 550028, "start": 5517.28, "end": 5520.12, "text": " There's save and saveencoder.", "tokens": [821, 311, 3155, 293, 3155, 22660, 19866, 13], "temperature": 0.0, "avg_logprob": -0.21784612447908608, "compression_ratio": 1.5765765765765767, "no_speech_prob": 3.844926141027827e-06}, {"id": 1152, "seek": 550028, "start": 5520.12, "end": 5523.08, "text": " Save saves the whole model as per usual.", "tokens": [15541, 19155, 264, 1379, 2316, 382, 680, 7713, 13], "temperature": 0.0, "avg_logprob": -0.21784612447908608, "compression_ratio": 1.5765765765765767, "no_speech_prob": 3.844926141027827e-06}, {"id": 1153, "seek": 552308, "start": 5523.08, "end": 5531.04, "text": " Saveencoder saves just that bit.", "tokens": [15541, 22660, 19866, 19155, 445, 300, 857, 13], "temperature": 0.0, "avg_logprob": -0.14519342143883865, "compression_ratio": 1.7582417582417582, "no_speech_prob": 3.1875556487648282e-06}, {"id": 1154, "seek": 552308, "start": 5531.04, "end": 5535.84, "text": " In other words, in the sequential model, it saves just that bit and not that bit.", "tokens": [682, 661, 2283, 11, 294, 264, 42881, 2316, 11, 309, 19155, 445, 300, 857, 293, 406, 300, 857, 13], "temperature": 0.0, "avg_logprob": -0.14519342143883865, "compression_ratio": 1.7582417582417582, "no_speech_prob": 3.1875556487648282e-06}, {"id": 1155, "seek": 552308, "start": 5535.84, "end": 5541.08, "text": " In other words, this bit, which is the bit that actually makes it into a language model,", "tokens": [682, 661, 2283, 11, 341, 857, 11, 597, 307, 264, 857, 300, 767, 1669, 309, 666, 257, 2856, 2316, 11], "temperature": 0.0, "avg_logprob": -0.14519342143883865, "compression_ratio": 1.7582417582417582, "no_speech_prob": 3.1875556487648282e-06}, {"id": 1156, "seek": 552308, "start": 5541.08, "end": 5543.6, "text": " we don't care about in the classifier.", "tokens": [321, 500, 380, 1127, 466, 294, 264, 1508, 9902, 13], "temperature": 0.0, "avg_logprob": -0.14519342143883865, "compression_ratio": 1.7582417582417582, "no_speech_prob": 3.1875556487648282e-06}, {"id": 1157, "seek": 552308, "start": 5543.6, "end": 5546.36, "text": " We just care about that bit.", "tokens": [492, 445, 1127, 466, 300, 857, 13], "temperature": 0.0, "avg_logprob": -0.14519342143883865, "compression_ratio": 1.7582417582417582, "no_speech_prob": 3.1875556487648282e-06}, {"id": 1158, "seek": 552308, "start": 5546.36, "end": 5551.48, "text": " So that's why we save two different models here.", "tokens": [407, 300, 311, 983, 321, 3155, 732, 819, 5245, 510, 13], "temperature": 0.0, "avg_logprob": -0.14519342143883865, "compression_ratio": 1.7582417582417582, "no_speech_prob": 3.1875556487648282e-06}, {"id": 1159, "seek": 555148, "start": 5551.48, "end": 5553.639999999999, "text": " So let's now create the classifier.", "tokens": [407, 718, 311, 586, 1884, 264, 1508, 9902, 13], "temperature": 0.0, "avg_logprob": -0.16511920141795325, "compression_ratio": 1.6703703703703703, "no_speech_prob": 1.4367439007401117e-06}, {"id": 1160, "seek": 555148, "start": 5553.639999999999, "end": 5556.5599999999995, "text": " And I'm going to go through this bit pretty quickly because it's the same.", "tokens": [400, 286, 478, 516, 281, 352, 807, 341, 857, 1238, 2661, 570, 309, 311, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.16511920141795325, "compression_ratio": 1.6703703703703703, "no_speech_prob": 1.4367439007401117e-06}, {"id": 1161, "seek": 555148, "start": 5556.5599999999995, "end": 5560.759999999999, "text": " But when you go back during the week and look at the code, convince yourself it's the same.", "tokens": [583, 562, 291, 352, 646, 1830, 264, 1243, 293, 574, 412, 264, 3089, 11, 13447, 1803, 309, 311, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.16511920141795325, "compression_ratio": 1.6703703703703703, "no_speech_prob": 1.4367439007401117e-06}, {"id": 1162, "seek": 555148, "start": 5560.759999999999, "end": 5568.679999999999, "text": " We do getAll, pd, readCSV again, chuckSize again, getAll again, save those tokens again.", "tokens": [492, 360, 483, 7868, 11, 280, 67, 11, 1401, 26283, 53, 797, 11, 20870, 50, 1125, 797, 11, 483, 7868, 797, 11, 3155, 729, 22667, 797, 13], "temperature": 0.0, "avg_logprob": -0.16511920141795325, "compression_ratio": 1.6703703703703703, "no_speech_prob": 1.4367439007401117e-06}, {"id": 1163, "seek": 555148, "start": 5568.679999999999, "end": 5572.5599999999995, "text": " We don't create a new I2S vocabulary.", "tokens": [492, 500, 380, 1884, 257, 777, 286, 17, 50, 19864, 13], "temperature": 0.0, "avg_logprob": -0.16511920141795325, "compression_ratio": 1.6703703703703703, "no_speech_prob": 1.4367439007401117e-06}, {"id": 1164, "seek": 555148, "start": 5572.5599999999995, "end": 5577.879999999999, "text": " We obviously want to use the same vocabulary we had in the language model because we're", "tokens": [492, 2745, 528, 281, 764, 264, 912, 19864, 321, 632, 294, 264, 2856, 2316, 570, 321, 434], "temperature": 0.0, "avg_logprob": -0.16511920141795325, "compression_ratio": 1.6703703703703703, "no_speech_prob": 1.4367439007401117e-06}, {"id": 1165, "seek": 555148, "start": 5577.879999999999, "end": 5581.2, "text": " about to reload the same encoder.", "tokens": [466, 281, 25628, 264, 912, 2058, 19866, 13], "temperature": 0.0, "avg_logprob": -0.16511920141795325, "compression_ratio": 1.6703703703703703, "no_speech_prob": 1.4367439007401117e-06}, {"id": 1166, "seek": 558120, "start": 5581.2, "end": 5589.16, "text": " Same default dict, same way of creating our numericalized list, which as per before, we", "tokens": [10635, 7576, 12569, 11, 912, 636, 295, 4084, 527, 29054, 1602, 1329, 11, 597, 382, 680, 949, 11, 321], "temperature": 0.0, "avg_logprob": -0.16868171691894532, "compression_ratio": 1.6772486772486772, "no_speech_prob": 7.411236765619833e-06}, {"id": 1167, "seek": 558120, "start": 5589.16, "end": 5590.16, "text": " can save.", "tokens": [393, 3155, 13], "temperature": 0.0, "avg_logprob": -0.16868171691894532, "compression_ratio": 1.6772486772486772, "no_speech_prob": 7.411236765619833e-06}, {"id": 1168, "seek": 558120, "start": 5590.16, "end": 5592.599999999999, "text": " So that's all the same.", "tokens": [407, 300, 311, 439, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.16868171691894532, "compression_ratio": 1.6772486772486772, "no_speech_prob": 7.411236765619833e-06}, {"id": 1169, "seek": 558120, "start": 5592.599999999999, "end": 5597.04, "text": " Later on we can reload those rather than having to rebuild them.", "tokens": [11965, 322, 321, 393, 25628, 729, 2831, 813, 1419, 281, 16877, 552, 13], "temperature": 0.0, "avg_logprob": -0.16868171691894532, "compression_ratio": 1.6772486772486772, "no_speech_prob": 7.411236765619833e-06}, {"id": 1170, "seek": 558120, "start": 5597.04, "end": 5603.16, "text": " So all of our hyperparameters are the same.", "tokens": [407, 439, 295, 527, 9848, 2181, 335, 6202, 366, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.16868171691894532, "compression_ratio": 1.6772486772486772, "no_speech_prob": 7.411236765619833e-06}, {"id": 1171, "seek": 558120, "start": 5603.16, "end": 5605.5599999999995, "text": " The construction of the model hyperparameters are the same.", "tokens": [440, 6435, 295, 264, 2316, 9848, 2181, 335, 6202, 366, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.16868171691894532, "compression_ratio": 1.6772486772486772, "no_speech_prob": 7.411236765619833e-06}, {"id": 1172, "seek": 558120, "start": 5605.5599999999995, "end": 5609.44, "text": " We can change the dropout.", "tokens": [492, 393, 1319, 264, 3270, 346, 13], "temperature": 0.0, "avg_logprob": -0.16868171691894532, "compression_ratio": 1.6772486772486772, "no_speech_prob": 7.411236765619833e-06}, {"id": 1173, "seek": 560944, "start": 5609.44, "end": 5611.16, "text": " We can change the size of function.", "tokens": [492, 393, 1319, 264, 2744, 295, 2445, 13], "temperature": 0.0, "avg_logprob": -0.24387319795377962, "compression_ratio": 1.5613207547169812, "no_speech_prob": 4.356847966846544e-06}, {"id": 1174, "seek": 560944, "start": 5611.16, "end": 5618.24, "text": " Pick a batch size that is as big as you can that doesn't run out of memory.", "tokens": [14129, 257, 15245, 2744, 300, 307, 382, 955, 382, 291, 393, 300, 1177, 380, 1190, 484, 295, 4675, 13], "temperature": 0.0, "avg_logprob": -0.24387319795377962, "compression_ratio": 1.5613207547169812, "no_speech_prob": 4.356847966846544e-06}, {"id": 1175, "seek": 560944, "start": 5618.24, "end": 5621.48, "text": " And so this bit's a bit interesting.", "tokens": [400, 370, 341, 857, 311, 257, 857, 1880, 13], "temperature": 0.0, "avg_logprob": -0.24387319795377962, "compression_ratio": 1.5613207547169812, "no_speech_prob": 4.356847966846544e-06}, {"id": 1176, "seek": 560944, "start": 5621.48, "end": 5627.919999999999, "text": " There's some fun stuff going on here.", "tokens": [821, 311, 512, 1019, 1507, 516, 322, 510, 13], "temperature": 0.0, "avg_logprob": -0.24387319795377962, "compression_ratio": 1.5613207547169812, "no_speech_prob": 4.356847966846544e-06}, {"id": 1177, "seek": 560944, "start": 5627.919999999999, "end": 5635.719999999999, "text": " The basic idea here is that for the classifier, we do really want to look at our document.", "tokens": [440, 3875, 1558, 510, 307, 300, 337, 264, 1508, 9902, 11, 321, 360, 534, 528, 281, 574, 412, 527, 4166, 13], "temperature": 0.0, "avg_logprob": -0.24387319795377962, "compression_ratio": 1.5613207547169812, "no_speech_prob": 4.356847966846544e-06}, {"id": 1178, "seek": 560944, "start": 5635.719999999999, "end": 5638.32, "text": " We need to say is this document positive or negative.", "tokens": [492, 643, 281, 584, 307, 341, 4166, 3353, 420, 3671, 13], "temperature": 0.0, "avg_logprob": -0.24387319795377962, "compression_ratio": 1.5613207547169812, "no_speech_prob": 4.356847966846544e-06}, {"id": 1179, "seek": 563832, "start": 5638.32, "end": 5641.84, "text": " And so we do want to shuffle the documents.", "tokens": [400, 370, 321, 360, 528, 281, 39426, 264, 8512, 13], "temperature": 0.0, "avg_logprob": -0.2269553961577239, "compression_ratio": 1.7745901639344261, "no_speech_prob": 1.1478663509478793e-05}, {"id": 1180, "seek": 563832, "start": 5641.84, "end": 5645.48, "text": " Because we like to shuffle things.", "tokens": [1436, 321, 411, 281, 39426, 721, 13], "temperature": 0.0, "avg_logprob": -0.2269553961577239, "compression_ratio": 1.7745901639344261, "no_speech_prob": 1.1478663509478793e-05}, {"id": 1181, "seek": 563832, "start": 5645.48, "end": 5652.0, "text": " But those documents are different lengths, so if we stick them all into one batch, this", "tokens": [583, 729, 8512, 366, 819, 26329, 11, 370, 498, 321, 2897, 552, 439, 666, 472, 15245, 11, 341], "temperature": 0.0, "avg_logprob": -0.2269553961577239, "compression_ratio": 1.7745901639344261, "no_speech_prob": 1.1478663509478793e-05}, {"id": 1182, "seek": 563832, "start": 5652.0, "end": 5655.4, "text": " is a handy thing that FastAI does for you, you can stick things of different lengths", "tokens": [307, 257, 13239, 551, 300, 15968, 48698, 775, 337, 291, 11, 291, 393, 2897, 721, 295, 819, 26329], "temperature": 0.0, "avg_logprob": -0.2269553961577239, "compression_ratio": 1.7745901639344261, "no_speech_prob": 1.1478663509478793e-05}, {"id": 1183, "seek": 563832, "start": 5655.4, "end": 5657.719999999999, "text": " into a batch and it will automatically pad them.", "tokens": [666, 257, 15245, 293, 309, 486, 6772, 6887, 552, 13], "temperature": 0.0, "avg_logprob": -0.2269553961577239, "compression_ratio": 1.7745901639344261, "no_speech_prob": 1.1478663509478793e-05}, {"id": 1184, "seek": 563832, "start": 5657.719999999999, "end": 5660.96, "text": " So you don't have to worry about that.", "tokens": [407, 291, 500, 380, 362, 281, 3292, 466, 300, 13], "temperature": 0.0, "avg_logprob": -0.2269553961577239, "compression_ratio": 1.7745901639344261, "no_speech_prob": 1.1478663509478793e-05}, {"id": 1185, "seek": 563832, "start": 5660.96, "end": 5665.12, "text": " But if they're wildly different lengths, then you're going to be wasting a lot of computation", "tokens": [583, 498, 436, 434, 34731, 819, 26329, 11, 550, 291, 434, 516, 281, 312, 20457, 257, 688, 295, 24903], "temperature": 0.0, "avg_logprob": -0.2269553961577239, "compression_ratio": 1.7745901639344261, "no_speech_prob": 1.1478663509478793e-05}, {"id": 1186, "seek": 566512, "start": 5665.12, "end": 5669.32, "text": " time, there might be one thing there that's 2000 words long and everything else is 50", "tokens": [565, 11, 456, 1062, 312, 472, 551, 456, 300, 311, 8132, 2283, 938, 293, 1203, 1646, 307, 2625], "temperature": 0.0, "avg_logprob": -0.2439443116546959, "compression_ratio": 1.5695067264573992, "no_speech_prob": 2.521557462387136e-06}, {"id": 1187, "seek": 566512, "start": 5669.32, "end": 5673.84, "text": " words long, and that means you end up with a 2000-wide tensor.", "tokens": [2283, 938, 11, 293, 300, 1355, 291, 917, 493, 365, 257, 8132, 12, 7990, 40863, 13], "temperature": 0.0, "avg_logprob": -0.2439443116546959, "compression_ratio": 1.5695067264573992, "no_speech_prob": 2.521557462387136e-06}, {"id": 1188, "seek": 566512, "start": 5673.84, "end": 5676.04, "text": " That's pretty annoying.", "tokens": [663, 311, 1238, 11304, 13], "temperature": 0.0, "avg_logprob": -0.2439443116546959, "compression_ratio": 1.5695067264573992, "no_speech_prob": 2.521557462387136e-06}, {"id": 1189, "seek": 566512, "start": 5676.04, "end": 5681.5199999999995, "text": " So James Bradbury, who's actually one of Stephen Murady's colleagues and the guy who came up", "tokens": [407, 5678, 11895, 22536, 11, 567, 311, 767, 472, 295, 13391, 9373, 880, 311, 7734, 293, 264, 2146, 567, 1361, 493], "temperature": 0.0, "avg_logprob": -0.2439443116546959, "compression_ratio": 1.5695067264573992, "no_speech_prob": 2.521557462387136e-06}, {"id": 1190, "seek": 566512, "start": 5681.5199999999995, "end": 5691.76, "text": " with TorchText, came up with a neat idea, which was let's sort the dataset by length", "tokens": [365, 7160, 339, 50198, 11, 1361, 493, 365, 257, 10654, 1558, 11, 597, 390, 718, 311, 1333, 264, 28872, 538, 4641], "temperature": 0.0, "avg_logprob": -0.2439443116546959, "compression_ratio": 1.5695067264573992, "no_speech_prob": 2.521557462387136e-06}, {"id": 1191, "seek": 569176, "start": 5691.76, "end": 5695.16, "text": " ish.", "tokens": [307, 71, 13], "temperature": 0.0, "avg_logprob": -0.13344826417810776, "compression_ratio": 1.5133333333333334, "no_speech_prob": 3.9669371290074196e-06}, {"id": 1192, "seek": 569176, "start": 5695.16, "end": 5703.2, "text": " So kind of make it so the first things in the list are on the whole shorter than the", "tokens": [407, 733, 295, 652, 309, 370, 264, 700, 721, 294, 264, 1329, 366, 322, 264, 1379, 11639, 813, 264], "temperature": 0.0, "avg_logprob": -0.13344826417810776, "compression_ratio": 1.5133333333333334, "no_speech_prob": 3.9669371290074196e-06}, {"id": 1193, "seek": 569176, "start": 5703.2, "end": 5709.400000000001, "text": " things at the end, but a little bit random as well.", "tokens": [721, 412, 264, 917, 11, 457, 257, 707, 857, 4974, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.13344826417810776, "compression_ratio": 1.5133333333333334, "no_speech_prob": 3.9669371290074196e-06}, {"id": 1194, "seek": 569176, "start": 5709.400000000001, "end": 5714.860000000001, "text": " And so I'll show you how I implemented that.", "tokens": [400, 370, 286, 603, 855, 291, 577, 286, 12270, 300, 13], "temperature": 0.0, "avg_logprob": -0.13344826417810776, "compression_ratio": 1.5133333333333334, "no_speech_prob": 3.9669371290074196e-06}, {"id": 1195, "seek": 569176, "start": 5714.860000000001, "end": 5720.18, "text": " So the first thing we need is a dataset.", "tokens": [407, 264, 700, 551, 321, 643, 307, 257, 28872, 13], "temperature": 0.0, "avg_logprob": -0.13344826417810776, "compression_ratio": 1.5133333333333334, "no_speech_prob": 3.9669371290074196e-06}, {"id": 1196, "seek": 572018, "start": 5720.18, "end": 5726.280000000001, "text": " And so we have a dataset passing in the documents and their labels.", "tokens": [400, 370, 321, 362, 257, 28872, 8437, 294, 264, 8512, 293, 641, 16949, 13], "temperature": 0.0, "avg_logprob": -0.1445074162931524, "compression_ratio": 1.8657407407407407, "no_speech_prob": 5.255356882116757e-06}, {"id": 1197, "seek": 572018, "start": 5726.280000000001, "end": 5730.700000000001, "text": " And so here's a text dataset and it inherits from dataset.", "tokens": [400, 370, 510, 311, 257, 2487, 28872, 293, 309, 9484, 1208, 490, 28872, 13], "temperature": 0.0, "avg_logprob": -0.1445074162931524, "compression_ratio": 1.8657407407407407, "no_speech_prob": 5.255356882116757e-06}, {"id": 1198, "seek": 572018, "start": 5730.700000000001, "end": 5735.06, "text": " Here is dataset from torch, from PyTorch.", "tokens": [1692, 307, 28872, 490, 27822, 11, 490, 9953, 51, 284, 339, 13], "temperature": 0.0, "avg_logprob": -0.1445074162931524, "compression_ratio": 1.8657407407407407, "no_speech_prob": 5.255356882116757e-06}, {"id": 1199, "seek": 572018, "start": 5735.06, "end": 5737.96, "text": " And actually, dataset doesn't do anything at all.", "tokens": [400, 767, 11, 28872, 1177, 380, 360, 1340, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.1445074162931524, "compression_ratio": 1.8657407407407407, "no_speech_prob": 5.255356882116757e-06}, {"id": 1200, "seek": 572018, "start": 5737.96, "end": 5741.72, "text": " It says you need to get item, if you don't have one, you're going to get an error, you", "tokens": [467, 1619, 291, 643, 281, 483, 3174, 11, 498, 291, 500, 380, 362, 472, 11, 291, 434, 516, 281, 483, 364, 6713, 11, 291], "temperature": 0.0, "avg_logprob": -0.1445074162931524, "compression_ratio": 1.8657407407407407, "no_speech_prob": 5.255356882116757e-06}, {"id": 1201, "seek": 572018, "start": 5741.72, "end": 5744.64, "text": " need a length, if you don't have one, you're going to get an error.", "tokens": [643, 257, 4641, 11, 498, 291, 500, 380, 362, 472, 11, 291, 434, 516, 281, 483, 364, 6713, 13], "temperature": 0.0, "avg_logprob": -0.1445074162931524, "compression_ratio": 1.8657407407407407, "no_speech_prob": 5.255356882116757e-06}, {"id": 1202, "seek": 572018, "start": 5744.64, "end": 5748.66, "text": " So this is an abstract class.", "tokens": [407, 341, 307, 364, 12649, 1508, 13], "temperature": 0.0, "avg_logprob": -0.1445074162931524, "compression_ratio": 1.8657407407407407, "no_speech_prob": 5.255356882116757e-06}, {"id": 1203, "seek": 574866, "start": 5748.66, "end": 5754.68, "text": " So we're going to pass in our x, we're going to pass in our y, and get item is going to", "tokens": [407, 321, 434, 516, 281, 1320, 294, 527, 2031, 11, 321, 434, 516, 281, 1320, 294, 527, 288, 11, 293, 483, 3174, 307, 516, 281], "temperature": 0.0, "avg_logprob": -0.18282883507864817, "compression_ratio": 2.0508474576271185, "no_speech_prob": 1.9333483578520827e-06}, {"id": 1204, "seek": 574866, "start": 5754.68, "end": 5760.04, "text": " grab the x and grab the y and return them.", "tokens": [4444, 264, 2031, 293, 4444, 264, 288, 293, 2736, 552, 13], "temperature": 0.0, "avg_logprob": -0.18282883507864817, "compression_ratio": 2.0508474576271185, "no_speech_prob": 1.9333483578520827e-06}, {"id": 1205, "seek": 574866, "start": 5760.04, "end": 5762.28, "text": " It couldn't be much simpler.", "tokens": [467, 2809, 380, 312, 709, 18587, 13], "temperature": 0.0, "avg_logprob": -0.18282883507864817, "compression_ratio": 2.0508474576271185, "no_speech_prob": 1.9333483578520827e-06}, {"id": 1206, "seek": 574866, "start": 5762.28, "end": 5766.28, "text": " Optionally it could reverse it, optionally it could stick an end of stream at the end,", "tokens": [29284, 379, 309, 727, 9943, 309, 11, 3614, 379, 309, 727, 2897, 364, 917, 295, 4309, 412, 264, 917, 11], "temperature": 0.0, "avg_logprob": -0.18282883507864817, "compression_ratio": 2.0508474576271185, "no_speech_prob": 1.9333483578520827e-06}, {"id": 1207, "seek": 574866, "start": 5766.28, "end": 5767.639999999999, "text": " optionally it could stick a start of stream at the beginning.", "tokens": [3614, 379, 309, 727, 2897, 257, 722, 295, 4309, 412, 264, 2863, 13], "temperature": 0.0, "avg_logprob": -0.18282883507864817, "compression_ratio": 2.0508474576271185, "no_speech_prob": 1.9333483578520827e-06}, {"id": 1208, "seek": 574866, "start": 5767.639999999999, "end": 5769.28, "text": " We're not doing any of those things.", "tokens": [492, 434, 406, 884, 604, 295, 729, 721, 13], "temperature": 0.0, "avg_logprob": -0.18282883507864817, "compression_ratio": 2.0508474576271185, "no_speech_prob": 1.9333483578520827e-06}, {"id": 1209, "seek": 574866, "start": 5769.28, "end": 5773.2, "text": " So literally all we're doing is we're putting in an x, putting in a y, and then grab an", "tokens": [407, 3736, 439, 321, 434, 884, 307, 321, 434, 3372, 294, 364, 2031, 11, 3372, 294, 257, 288, 11, 293, 550, 4444, 364], "temperature": 0.0, "avg_logprob": -0.18282883507864817, "compression_ratio": 2.0508474576271185, "no_speech_prob": 1.9333483578520827e-06}, {"id": 1210, "seek": 574866, "start": 5773.2, "end": 5777.68, "text": " item, we're returning the x and the y as a double.", "tokens": [3174, 11, 321, 434, 12678, 264, 2031, 293, 264, 288, 382, 257, 3834, 13], "temperature": 0.0, "avg_logprob": -0.18282883507864817, "compression_ratio": 2.0508474576271185, "no_speech_prob": 1.9333483578520827e-06}, {"id": 1211, "seek": 577768, "start": 5777.68, "end": 5781.08, "text": " And the length is however long the x for a is.", "tokens": [400, 264, 4641, 307, 4461, 938, 264, 2031, 337, 257, 307, 13], "temperature": 0.0, "avg_logprob": -0.15579695171780056, "compression_ratio": 1.6851063829787234, "no_speech_prob": 3.288738298579119e-06}, {"id": 1212, "seek": 577768, "start": 5781.08, "end": 5784.08, "text": " So that's all a dataset is.", "tokens": [407, 300, 311, 439, 257, 28872, 307, 13], "temperature": 0.0, "avg_logprob": -0.15579695171780056, "compression_ratio": 1.6851063829787234, "no_speech_prob": 3.288738298579119e-06}, {"id": 1213, "seek": 577768, "start": 5784.08, "end": 5787.96, "text": " Something with a length that you can index.", "tokens": [6595, 365, 257, 4641, 300, 291, 393, 8186, 13], "temperature": 0.0, "avg_logprob": -0.15579695171780056, "compression_ratio": 1.6851063829787234, "no_speech_prob": 3.288738298579119e-06}, {"id": 1214, "seek": 577768, "start": 5787.96, "end": 5794.14, "text": " So to turn it into a data loader, you simply pass the dataset to the data loader constructor", "tokens": [407, 281, 1261, 309, 666, 257, 1412, 3677, 260, 11, 291, 2935, 1320, 264, 28872, 281, 264, 1412, 3677, 260, 47479], "temperature": 0.0, "avg_logprob": -0.15579695171780056, "compression_ratio": 1.6851063829787234, "no_speech_prob": 3.288738298579119e-06}, {"id": 1215, "seek": 577768, "start": 5794.14, "end": 5799.320000000001, "text": " and it's now going to go ahead and give you a batch of that at a time.", "tokens": [293, 309, 311, 586, 516, 281, 352, 2286, 293, 976, 291, 257, 15245, 295, 300, 412, 257, 565, 13], "temperature": 0.0, "avg_logprob": -0.15579695171780056, "compression_ratio": 1.6851063829787234, "no_speech_prob": 3.288738298579119e-06}, {"id": 1216, "seek": 577768, "start": 5799.320000000001, "end": 5802.6, "text": " Normally you can say shuffle equals true or shuffle equals false, it will decide whether", "tokens": [17424, 291, 393, 584, 39426, 6915, 2074, 420, 39426, 6915, 7908, 11, 309, 486, 4536, 1968], "temperature": 0.0, "avg_logprob": -0.15579695171780056, "compression_ratio": 1.6851063829787234, "no_speech_prob": 3.288738298579119e-06}, {"id": 1217, "seek": 577768, "start": 5802.6, "end": 5804.92, "text": " to randomize it for you.", "tokens": [281, 4974, 1125, 309, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.15579695171780056, "compression_ratio": 1.6851063829787234, "no_speech_prob": 3.288738298579119e-06}, {"id": 1218, "seek": 580492, "start": 5804.92, "end": 5810.32, "text": " In this case though, we're actually going to pass in a sampler parameter.", "tokens": [682, 341, 1389, 1673, 11, 321, 434, 767, 516, 281, 1320, 294, 257, 3247, 22732, 13075, 13], "temperature": 0.0, "avg_logprob": -0.13972390372798127, "compression_ratio": 1.8973214285714286, "no_speech_prob": 9.42243332247017e-07}, {"id": 1219, "seek": 580492, "start": 5810.32, "end": 5818.16, "text": " The sampler is a class we're going to define that tells the data loader how to shuffle.", "tokens": [440, 3247, 22732, 307, 257, 1508, 321, 434, 516, 281, 6964, 300, 5112, 264, 1412, 3677, 260, 577, 281, 39426, 13], "temperature": 0.0, "avg_logprob": -0.13972390372798127, "compression_ratio": 1.8973214285714286, "no_speech_prob": 9.42243332247017e-07}, {"id": 1220, "seek": 580492, "start": 5818.16, "end": 5822.24, "text": " So for the validation set, we're going to define something that actually just sorts", "tokens": [407, 337, 264, 24071, 992, 11, 321, 434, 516, 281, 6964, 746, 300, 767, 445, 7527], "temperature": 0.0, "avg_logprob": -0.13972390372798127, "compression_ratio": 1.8973214285714286, "no_speech_prob": 9.42243332247017e-07}, {"id": 1221, "seek": 580492, "start": 5822.24, "end": 5823.24, "text": " it.", "tokens": [309, 13], "temperature": 0.0, "avg_logprob": -0.13972390372798127, "compression_ratio": 1.8973214285714286, "no_speech_prob": 9.42243332247017e-07}, {"id": 1222, "seek": 580492, "start": 5823.24, "end": 5828.96, "text": " It just deterministically sorts it so all the shortest documents will be at the start,", "tokens": [467, 445, 15957, 20458, 7527, 309, 370, 439, 264, 31875, 8512, 486, 312, 412, 264, 722, 11], "temperature": 0.0, "avg_logprob": -0.13972390372798127, "compression_ratio": 1.8973214285714286, "no_speech_prob": 9.42243332247017e-07}, {"id": 1223, "seek": 580492, "start": 5828.96, "end": 5832.32, "text": " all the longest documents will be at the end, and that's going to minimize the amount of", "tokens": [439, 264, 15438, 8512, 486, 312, 412, 264, 917, 11, 293, 300, 311, 516, 281, 17522, 264, 2372, 295], "temperature": 0.0, "avg_logprob": -0.13972390372798127, "compression_ratio": 1.8973214285714286, "no_speech_prob": 9.42243332247017e-07}, {"id": 1224, "seek": 583232, "start": 5832.32, "end": 5835.36, "text": " padding.", "tokens": [39562, 13], "temperature": 0.0, "avg_logprob": -0.16562796187126774, "compression_ratio": 1.49746192893401, "no_speech_prob": 1.9033826674785814e-06}, {"id": 1225, "seek": 583232, "start": 5835.36, "end": 5842.08, "text": " For the training sampler, we're going to create this thing I call a sort-ish sampler, which", "tokens": [1171, 264, 3097, 3247, 22732, 11, 321, 434, 516, 281, 1884, 341, 551, 286, 818, 257, 1333, 12, 742, 3247, 22732, 11, 597], "temperature": 0.0, "avg_logprob": -0.16562796187126774, "compression_ratio": 1.49746192893401, "no_speech_prob": 1.9033826674785814e-06}, {"id": 1226, "seek": 583232, "start": 5842.08, "end": 5847.5199999999995, "text": " also sorts-ish.", "tokens": [611, 7527, 12, 742, 13], "temperature": 0.0, "avg_logprob": -0.16562796187126774, "compression_ratio": 1.49746192893401, "no_speech_prob": 1.9033826674785814e-06}, {"id": 1227, "seek": 583232, "start": 5847.5199999999995, "end": 5853.16, "text": " So this is where I really like PyTorch, is that they came up with this idea for an API", "tokens": [407, 341, 307, 689, 286, 534, 411, 9953, 51, 284, 339, 11, 307, 300, 436, 1361, 493, 365, 341, 1558, 337, 364, 9362], "temperature": 0.0, "avg_logprob": -0.16562796187126774, "compression_ratio": 1.49746192893401, "no_speech_prob": 1.9033826674785814e-06}, {"id": 1228, "seek": 583232, "start": 5853.16, "end": 5858.799999999999, "text": " for their data loader where we can hook in new classes to make it behave in different", "tokens": [337, 641, 1412, 3677, 260, 689, 321, 393, 6328, 294, 777, 5359, 281, 652, 309, 15158, 294, 819], "temperature": 0.0, "avg_logprob": -0.16562796187126774, "compression_ratio": 1.49746192893401, "no_speech_prob": 1.9033826674785814e-06}, {"id": 1229, "seek": 583232, "start": 5858.799999999999, "end": 5860.179999999999, "text": " ways.", "tokens": [2098, 13], "temperature": 0.0, "avg_logprob": -0.16562796187126774, "compression_ratio": 1.49746192893401, "no_speech_prob": 1.9033826674785814e-06}, {"id": 1230, "seek": 586018, "start": 5860.18, "end": 5866.68, "text": " So here's a sort sampler, it's simply something which again, it has a length, which is the", "tokens": [407, 510, 311, 257, 1333, 3247, 22732, 11, 309, 311, 2935, 746, 597, 797, 11, 309, 575, 257, 4641, 11, 597, 307, 264], "temperature": 0.0, "avg_logprob": -0.18338428437709808, "compression_ratio": 1.7462686567164178, "no_speech_prob": 2.8130118607805343e-06}, {"id": 1231, "seek": 586018, "start": 5866.68, "end": 5873.56, "text": " length of the data source, and it has an iterator, which is simply an iterator which goes through", "tokens": [4641, 295, 264, 1412, 4009, 11, 293, 309, 575, 364, 17138, 1639, 11, 597, 307, 2935, 364, 17138, 1639, 597, 1709, 807], "temperature": 0.0, "avg_logprob": -0.18338428437709808, "compression_ratio": 1.7462686567164178, "no_speech_prob": 2.8130118607805343e-06}, {"id": 1232, "seek": 586018, "start": 5873.56, "end": 5880.88, "text": " the data source sorted by length, or the key.", "tokens": [264, 1412, 4009, 25462, 538, 4641, 11, 420, 264, 2141, 13], "temperature": 0.0, "avg_logprob": -0.18338428437709808, "compression_ratio": 1.7462686567164178, "no_speech_prob": 2.8130118607805343e-06}, {"id": 1233, "seek": 588088, "start": 5880.88, "end": 5890.36, "text": " I pass in as the key a lambda function which returns the length.", "tokens": [286, 1320, 294, 382, 264, 2141, 257, 13607, 2445, 597, 11247, 264, 4641, 13], "temperature": 0.0, "avg_logprob": -0.1172248666936701, "compression_ratio": 1.5, "no_speech_prob": 1.0845151336980052e-06}, {"id": 1234, "seek": 588088, "start": 5890.36, "end": 5896.12, "text": " And so for the sort-ish sampler, I won't go through the details, but it basically does", "tokens": [400, 370, 337, 264, 1333, 12, 742, 3247, 22732, 11, 286, 1582, 380, 352, 807, 264, 4365, 11, 457, 309, 1936, 775], "temperature": 0.0, "avg_logprob": -0.1172248666936701, "compression_ratio": 1.5, "no_speech_prob": 1.0845151336980052e-06}, {"id": 1235, "seek": 588088, "start": 5896.12, "end": 5901.24, "text": " the same thing with a little bit of randomness.", "tokens": [264, 912, 551, 365, 257, 707, 857, 295, 4974, 1287, 13], "temperature": 0.0, "avg_logprob": -0.1172248666936701, "compression_ratio": 1.5, "no_speech_prob": 1.0845151336980052e-06}, {"id": 1236, "seek": 588088, "start": 5901.24, "end": 5907.88, "text": " So it's a really beautiful, just another of these beautiful little design things in PyTorch", "tokens": [407, 309, 311, 257, 534, 2238, 11, 445, 1071, 295, 613, 2238, 707, 1715, 721, 294, 9953, 51, 284, 339], "temperature": 0.0, "avg_logprob": -0.1172248666936701, "compression_ratio": 1.5, "no_speech_prob": 1.0845151336980052e-06}, {"id": 1237, "seek": 590788, "start": 5907.88, "end": 5914.4800000000005, "text": " that I discovered I could take James Bradbury's ideas, which he had written a whole new set", "tokens": [300, 286, 6941, 286, 727, 747, 5678, 11895, 22536, 311, 3487, 11, 597, 415, 632, 3720, 257, 1379, 777, 992], "temperature": 0.0, "avg_logprob": -0.1218545933564504, "compression_ratio": 1.6255707762557077, "no_speech_prob": 7.296301191672683e-06}, {"id": 1238, "seek": 590788, "start": 5914.4800000000005, "end": 5922.32, "text": " of classes around, and I could actually just use the inbuilt hooks inside PyTorch.", "tokens": [295, 5359, 926, 11, 293, 286, 727, 767, 445, 764, 264, 294, 23018, 26485, 1854, 9953, 51, 284, 339, 13], "temperature": 0.0, "avg_logprob": -0.1218545933564504, "compression_ratio": 1.6255707762557077, "no_speech_prob": 7.296301191672683e-06}, {"id": 1239, "seek": 590788, "start": 5922.32, "end": 5927.56, "text": " You will notice data loader is not actually PyTorch's data loader, it's actually FastAI's", "tokens": [509, 486, 3449, 1412, 3677, 260, 307, 406, 767, 9953, 51, 284, 339, 311, 1412, 3677, 260, 11, 309, 311, 767, 15968, 48698, 311], "temperature": 0.0, "avg_logprob": -0.1218545933564504, "compression_ratio": 1.6255707762557077, "no_speech_prob": 7.296301191672683e-06}, {"id": 1240, "seek": 590788, "start": 5927.56, "end": 5933.8, "text": " data loader, but it's basically almost entirely plagiarized from PyTorch, but customized in", "tokens": [1412, 3677, 260, 11, 457, 309, 311, 1936, 1920, 7696, 33756, 9448, 1602, 490, 9953, 51, 284, 339, 11, 457, 30581, 294], "temperature": 0.0, "avg_logprob": -0.1218545933564504, "compression_ratio": 1.6255707762557077, "no_speech_prob": 7.296301191672683e-06}, {"id": 1241, "seek": 593380, "start": 5933.8, "end": 5940.360000000001, "text": " some ways to make it faster, mainly by using multi-threading instead of multi-processing.", "tokens": [512, 2098, 281, 652, 309, 4663, 11, 8704, 538, 1228, 4825, 12, 392, 35908, 2602, 295, 4825, 12, 41075, 278, 13], "temperature": 0.0, "avg_logprob": -0.2147752297024767, "compression_ratio": 1.6007751937984496, "no_speech_prob": 1.2029488061671145e-05}, {"id": 1242, "seek": 593380, "start": 5940.360000000001, "end": 5946.96, "text": " Does the pre-trained LSTM depth and BBTT need to match with the new one we are training?", "tokens": [4402, 264, 659, 12, 17227, 2001, 441, 6840, 44, 7161, 293, 19168, 28178, 643, 281, 2995, 365, 264, 777, 472, 321, 366, 3097, 30], "temperature": 0.0, "avg_logprob": -0.2147752297024767, "compression_ratio": 1.6007751937984496, "no_speech_prob": 1.2029488061671145e-05}, {"id": 1243, "seek": 593380, "start": 5946.96, "end": 5949.4800000000005, "text": " The BBTT doesn't need to match at all.", "tokens": [440, 19168, 28178, 1177, 380, 643, 281, 2995, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.2147752297024767, "compression_ratio": 1.6007751937984496, "no_speech_prob": 1.2029488061671145e-05}, {"id": 1244, "seek": 593380, "start": 5949.4800000000005, "end": 5954.08, "text": " That's just like how many things do we look at at a time, it's got nothing to do with", "tokens": [663, 311, 445, 411, 577, 867, 721, 360, 321, 574, 412, 412, 257, 565, 11, 309, 311, 658, 1825, 281, 360, 365], "temperature": 0.0, "avg_logprob": -0.2147752297024767, "compression_ratio": 1.6007751937984496, "no_speech_prob": 1.2029488061671145e-05}, {"id": 1245, "seek": 593380, "start": 5954.08, "end": 5956.72, "text": " the architecture.", "tokens": [264, 9482, 13], "temperature": 0.0, "avg_logprob": -0.2147752297024767, "compression_ratio": 1.6007751937984496, "no_speech_prob": 1.2029488061671145e-05}, {"id": 1246, "seek": 593380, "start": 5956.72, "end": 5962.24, "text": " So now we can call that function we just saw before, getRNNClassifier, it's going to create", "tokens": [407, 586, 321, 393, 818, 300, 2445, 321, 445, 1866, 949, 11, 483, 49, 45, 45, 44621, 9902, 11, 309, 311, 516, 281, 1884], "temperature": 0.0, "avg_logprob": -0.2147752297024767, "compression_ratio": 1.6007751937984496, "no_speech_prob": 1.2029488061671145e-05}, {"id": 1247, "seek": 596224, "start": 5962.24, "end": 5968.76, "text": " exactly the same encoder, more or less, and we're going to pass in the same architectural", "tokens": [2293, 264, 912, 2058, 19866, 11, 544, 420, 1570, 11, 293, 321, 434, 516, 281, 1320, 294, 264, 912, 26621], "temperature": 0.0, "avg_logprob": -0.1171040183619449, "compression_ratio": 1.6135265700483092, "no_speech_prob": 1.8448203036314226e-06}, {"id": 1248, "seek": 596224, "start": 5968.76, "end": 5971.24, "text": " details as before.", "tokens": [4365, 382, 949, 13], "temperature": 0.0, "avg_logprob": -0.1171040183619449, "compression_ratio": 1.6135265700483092, "no_speech_prob": 1.8448203036314226e-06}, {"id": 1249, "seek": 596224, "start": 5971.24, "end": 5978.2, "text": " But this time, the head that we add on, you've got a few more things you can do.", "tokens": [583, 341, 565, 11, 264, 1378, 300, 321, 909, 322, 11, 291, 600, 658, 257, 1326, 544, 721, 291, 393, 360, 13], "temperature": 0.0, "avg_logprob": -0.1171040183619449, "compression_ratio": 1.6135265700483092, "no_speech_prob": 1.8448203036314226e-06}, {"id": 1250, "seek": 596224, "start": 5978.2, "end": 5981.4, "text": " One is you can add more than one hidden layer.", "tokens": [1485, 307, 291, 393, 909, 544, 813, 472, 7633, 4583, 13], "temperature": 0.0, "avg_logprob": -0.1171040183619449, "compression_ratio": 1.6135265700483092, "no_speech_prob": 1.8448203036314226e-06}, {"id": 1251, "seek": 596224, "start": 5981.4, "end": 5989.0199999999995, "text": " So this layer here says this is what the input to my classifier section, my head, is going", "tokens": [407, 341, 4583, 510, 1619, 341, 307, 437, 264, 4846, 281, 452, 1508, 9902, 3541, 11, 452, 1378, 11, 307, 516], "temperature": 0.0, "avg_logprob": -0.1171040183619449, "compression_ratio": 1.6135265700483092, "no_speech_prob": 1.8448203036314226e-06}, {"id": 1252, "seek": 596224, "start": 5989.0199999999995, "end": 5990.4, "text": " to be.", "tokens": [281, 312, 13], "temperature": 0.0, "avg_logprob": -0.1171040183619449, "compression_ratio": 1.6135265700483092, "no_speech_prob": 1.8448203036314226e-06}, {"id": 1253, "seek": 599040, "start": 5990.4, "end": 5995.2, "text": " This is the output of the first layer, this is the output of the second layer, and you", "tokens": [639, 307, 264, 5598, 295, 264, 700, 4583, 11, 341, 307, 264, 5598, 295, 264, 1150, 4583, 11, 293, 291], "temperature": 0.0, "avg_logprob": -0.12448234733091582, "compression_ratio": 1.6991150442477876, "no_speech_prob": 7.071838354022475e-06}, {"id": 1254, "seek": 599040, "start": 5995.2, "end": 5996.5199999999995, "text": " can add as many as you like.", "tokens": [393, 909, 382, 867, 382, 291, 411, 13], "temperature": 0.0, "avg_logprob": -0.12448234733091582, "compression_ratio": 1.6991150442477876, "no_speech_prob": 7.071838354022475e-06}, {"id": 1255, "seek": 599040, "start": 5996.5199999999995, "end": 6002.16, "text": " So you can basically create a multi-layered neural net classifier at the end.", "tokens": [407, 291, 393, 1936, 1884, 257, 4825, 12, 8376, 4073, 18161, 2533, 1508, 9902, 412, 264, 917, 13], "temperature": 0.0, "avg_logprob": -0.12448234733091582, "compression_ratio": 1.6991150442477876, "no_speech_prob": 7.071838354022475e-06}, {"id": 1256, "seek": 599040, "start": 6002.16, "end": 6008.12, "text": " And so ditto, these are the dropouts to go after each of these layers.", "tokens": [400, 370, 274, 34924, 11, 613, 366, 264, 3270, 7711, 281, 352, 934, 1184, 295, 613, 7914, 13], "temperature": 0.0, "avg_logprob": -0.12448234733091582, "compression_ratio": 1.6991150442477876, "no_speech_prob": 7.071838354022475e-06}, {"id": 1257, "seek": 599040, "start": 6008.12, "end": 6013.28, "text": " And then here are all of the AWD LSTM dropouts, which we're going to basically plagiarize", "tokens": [400, 550, 510, 366, 439, 295, 264, 25815, 35, 441, 6840, 44, 3270, 7711, 11, 597, 321, 434, 516, 281, 1936, 33756, 9448, 1125], "temperature": 0.0, "avg_logprob": -0.12448234733091582, "compression_ratio": 1.6991150442477876, "no_speech_prob": 7.071838354022475e-06}, {"id": 1258, "seek": 599040, "start": 6013.28, "end": 6017.24, "text": " that idea for our classifier.", "tokens": [300, 1558, 337, 527, 1508, 9902, 13], "temperature": 0.0, "avg_logprob": -0.12448234733091582, "compression_ratio": 1.6991150442477876, "no_speech_prob": 7.071838354022475e-06}, {"id": 1259, "seek": 601724, "start": 6017.24, "end": 6021.719999999999, "text": " We're going to use the RNN learner just like before.", "tokens": [492, 434, 516, 281, 764, 264, 45702, 45, 33347, 445, 411, 949, 13], "temperature": 0.0, "avg_logprob": -0.17092063628047346, "compression_ratio": 1.4876847290640394, "no_speech_prob": 5.594311915047001e-06}, {"id": 1260, "seek": 601724, "start": 6021.719999999999, "end": 6029.88, "text": " We're going to use discriminative learning rates for different layers.", "tokens": [492, 434, 516, 281, 764, 20828, 1166, 2539, 6846, 337, 819, 7914, 13], "temperature": 0.0, "avg_logprob": -0.17092063628047346, "compression_ratio": 1.4876847290640394, "no_speech_prob": 5.594311915047001e-06}, {"id": 1261, "seek": 601724, "start": 6029.88, "end": 6033.36, "text": " You can try using weight decay or not, I've been fiddling around a bit with that to see", "tokens": [509, 393, 853, 1228, 3364, 21039, 420, 406, 11, 286, 600, 668, 283, 14273, 1688, 926, 257, 857, 365, 300, 281, 536], "temperature": 0.0, "avg_logprob": -0.17092063628047346, "compression_ratio": 1.4876847290640394, "no_speech_prob": 5.594311915047001e-06}, {"id": 1262, "seek": 601724, "start": 6033.36, "end": 6035.92, "text": " what happens.", "tokens": [437, 2314, 13], "temperature": 0.0, "avg_logprob": -0.17092063628047346, "compression_ratio": 1.4876847290640394, "no_speech_prob": 5.594311915047001e-06}, {"id": 1263, "seek": 601724, "start": 6035.92, "end": 6042.98, "text": " And so we start out just training the last layer, and we get 92.9% accuracy.", "tokens": [400, 370, 321, 722, 484, 445, 3097, 264, 1036, 4583, 11, 293, 321, 483, 28225, 13, 24, 4, 14170, 13], "temperature": 0.0, "avg_logprob": -0.17092063628047346, "compression_ratio": 1.4876847290640394, "no_speech_prob": 5.594311915047001e-06}, {"id": 1264, "seek": 604298, "start": 6042.98, "end": 6049.0, "text": " Then we unfreeze one more layer, get 93.3% accuracy, and then we fine-tune the whole", "tokens": [1396, 321, 3971, 701, 1381, 472, 544, 4583, 11, 483, 28876, 13, 18, 4, 14170, 11, 293, 550, 321, 2489, 12, 83, 2613, 264, 1379], "temperature": 0.0, "avg_logprob": -0.17535188462999132, "compression_ratio": 1.2426470588235294, "no_speech_prob": 2.902305368479574e-06}, {"id": 1265, "seek": 604298, "start": 6049.0, "end": 6050.639999999999, "text": " thing.", "tokens": [551, 13], "temperature": 0.0, "avg_logprob": -0.17535188462999132, "compression_ratio": 1.2426470588235294, "no_speech_prob": 2.902305368479574e-06}, {"id": 1266, "seek": 604298, "start": 6050.639999999999, "end": 6066.54, "text": " And after 3 epochs, so here is the famous James Bradbury we're talking about.", "tokens": [400, 934, 805, 30992, 28346, 11, 370, 510, 307, 264, 4618, 5678, 11895, 22536, 321, 434, 1417, 466, 13], "temperature": 0.0, "avg_logprob": -0.17535188462999132, "compression_ratio": 1.2426470588235294, "no_speech_prob": 2.902305368479574e-06}, {"id": 1267, "seek": 606654, "start": 6066.54, "end": 6074.44, "text": " This was kind of the main attempt before our paper came along at using a pre-trained model.", "tokens": [639, 390, 733, 295, 264, 2135, 5217, 949, 527, 3035, 1361, 2051, 412, 1228, 257, 659, 12, 17227, 2001, 2316, 13], "temperature": 0.0, "avg_logprob": -0.11210670332977737, "compression_ratio": 1.624203821656051, "no_speech_prob": 1.2482656757129007e-06}, {"id": 1268, "seek": 606654, "start": 6074.44, "end": 6081.72, "text": " And what they did is they used a pre-trained translation model.", "tokens": [400, 437, 436, 630, 307, 436, 1143, 257, 659, 12, 17227, 2001, 12853, 2316, 13], "temperature": 0.0, "avg_logprob": -0.11210670332977737, "compression_ratio": 1.624203821656051, "no_speech_prob": 1.2482656757129007e-06}, {"id": 1269, "seek": 606654, "start": 6081.72, "end": 6085.5, "text": " But they didn't fine-tune the whole thing.", "tokens": [583, 436, 994, 380, 2489, 12, 83, 2613, 264, 1379, 551, 13], "temperature": 0.0, "avg_logprob": -0.11210670332977737, "compression_ratio": 1.624203821656051, "no_speech_prob": 1.2482656757129007e-06}, {"id": 1270, "seek": 606654, "start": 6085.5, "end": 6091.8, "text": " They just took the activations of the translation model.", "tokens": [814, 445, 1890, 264, 2430, 763, 295, 264, 12853, 2316, 13], "temperature": 0.0, "avg_logprob": -0.11210670332977737, "compression_ratio": 1.624203821656051, "no_speech_prob": 1.2482656757129007e-06}, {"id": 1271, "seek": 609180, "start": 6091.8, "end": 6112.0, "text": " And when they tried IMDB, they got 91.8%, which we beat easily after only fine-tuning", "tokens": [400, 562, 436, 3031, 21463, 27735, 11, 436, 658, 31064, 13, 23, 8923, 597, 321, 4224, 3612, 934, 787, 2489, 12, 83, 37726], "temperature": 0.0, "avg_logprob": -0.18429513310277185, "compression_ratio": 1.146551724137931, "no_speech_prob": 2.7693888569046976e-06}, {"id": 1272, "seek": 609180, "start": 6112.0, "end": 6116.64, "text": " one layer.", "tokens": [472, 4583, 13], "temperature": 0.0, "avg_logprob": -0.18429513310277185, "compression_ratio": 1.146551724137931, "no_speech_prob": 2.7693888569046976e-06}, {"id": 1273, "seek": 609180, "start": 6116.64, "end": 6117.64, "text": " They weren't state of the art there.", "tokens": [814, 4999, 380, 1785, 295, 264, 1523, 456, 13], "temperature": 0.0, "avg_logprob": -0.18429513310277185, "compression_ratio": 1.146551724137931, "no_speech_prob": 2.7693888569046976e-06}, {"id": 1274, "seek": 611764, "start": 6117.64, "end": 6127.360000000001, "text": " The state of the art is 94.1%, which we beat after fine-tuning the whole thing for 3 epochs.", "tokens": [440, 1785, 295, 264, 1523, 307, 30849, 13, 16, 8923, 597, 321, 4224, 934, 2489, 12, 83, 37726, 264, 1379, 551, 337, 805, 30992, 28346, 13], "temperature": 0.0, "avg_logprob": -0.14631265547217392, "compression_ratio": 1.3917525773195876, "no_speech_prob": 6.339154879242415e-06}, {"id": 1275, "seek": 611764, "start": 6127.360000000001, "end": 6135.52, "text": " And so by the end, we're at 94.8%, which is obviously a huge difference because in terms", "tokens": [400, 370, 538, 264, 917, 11, 321, 434, 412, 30849, 13, 23, 8923, 597, 307, 2745, 257, 2603, 2649, 570, 294, 2115], "temperature": 0.0, "avg_logprob": -0.14631265547217392, "compression_ratio": 1.3917525773195876, "no_speech_prob": 6.339154879242415e-06}, {"id": 1276, "seek": 611764, "start": 6135.52, "end": 6139.4800000000005, "text": " of error rate, that's gone down from 5.9%.", "tokens": [295, 6713, 3314, 11, 300, 311, 2780, 760, 490, 1025, 13, 24, 6856], "temperature": 0.0, "avg_logprob": -0.14631265547217392, "compression_ratio": 1.3917525773195876, "no_speech_prob": 6.339154879242415e-06}, {"id": 1277, "seek": 611764, "start": 6139.4800000000005, "end": 6142.4800000000005, "text": " And then I'll tell you a simple little trick.", "tokens": [400, 550, 286, 603, 980, 291, 257, 2199, 707, 4282, 13], "temperature": 0.0, "avg_logprob": -0.14631265547217392, "compression_ratio": 1.3917525773195876, "no_speech_prob": 6.339154879242415e-06}, {"id": 1278, "seek": 614248, "start": 6142.48, "end": 6150.44, "text": " Go back to the start of this notebook and reverse the order of all of the documents", "tokens": [1037, 646, 281, 264, 722, 295, 341, 21060, 293, 9943, 264, 1668, 295, 439, 295, 264, 8512], "temperature": 0.0, "avg_logprob": -0.16035742508737663, "compression_ratio": 1.5614973262032086, "no_speech_prob": 5.014661383029306e-06}, {"id": 1279, "seek": 614248, "start": 6150.44, "end": 6154.2, "text": " and then rerun the whole thing.", "tokens": [293, 550, 43819, 409, 264, 1379, 551, 13], "temperature": 0.0, "avg_logprob": -0.16035742508737663, "compression_ratio": 1.5614973262032086, "no_speech_prob": 5.014661383029306e-06}, {"id": 1280, "seek": 614248, "start": 6154.2, "end": 6163.36, "text": " And when you get to the bit that says wt103, replace this fwd for forward with bwd for", "tokens": [400, 562, 291, 483, 281, 264, 857, 300, 1619, 23105, 3279, 18, 11, 7406, 341, 283, 43778, 337, 2128, 365, 272, 43778, 337], "temperature": 0.0, "avg_logprob": -0.16035742508737663, "compression_ratio": 1.5614973262032086, "no_speech_prob": 5.014661383029306e-06}, {"id": 1281, "seek": 614248, "start": 6163.36, "end": 6164.44, "text": " backward.", "tokens": [23897, 13], "temperature": 0.0, "avg_logprob": -0.16035742508737663, "compression_ratio": 1.5614973262032086, "no_speech_prob": 5.014661383029306e-06}, {"id": 1282, "seek": 614248, "start": 6164.44, "end": 6169.799999999999, "text": " That's a backward English language model that learns to read English backwards.", "tokens": [663, 311, 257, 23897, 3669, 2856, 2316, 300, 27152, 281, 1401, 3669, 12204, 13], "temperature": 0.0, "avg_logprob": -0.16035742508737663, "compression_ratio": 1.5614973262032086, "no_speech_prob": 5.014661383029306e-06}, {"id": 1283, "seek": 616980, "start": 6169.8, "end": 6176.02, "text": " So if you redo this whole thing, put all the documents in reverse and change this to backward,", "tokens": [407, 498, 291, 29956, 341, 1379, 551, 11, 829, 439, 264, 8512, 294, 9943, 293, 1319, 341, 281, 23897, 11], "temperature": 0.0, "avg_logprob": -0.12470751693568279, "compression_ratio": 1.6487603305785123, "no_speech_prob": 5.173831596039236e-06}, {"id": 1284, "seek": 616980, "start": 6176.02, "end": 6182.26, "text": " you now have a second classifier which classifies things by positive or negative sentiment based", "tokens": [291, 586, 362, 257, 1150, 1508, 9902, 597, 1508, 11221, 721, 538, 3353, 420, 3671, 16149, 2361], "temperature": 0.0, "avg_logprob": -0.12470751693568279, "compression_ratio": 1.6487603305785123, "no_speech_prob": 5.173831596039236e-06}, {"id": 1285, "seek": 616980, "start": 6182.26, "end": 6185.16, "text": " on the reverse document.", "tokens": [322, 264, 9943, 4166, 13], "temperature": 0.0, "avg_logprob": -0.12470751693568279, "compression_ratio": 1.6487603305785123, "no_speech_prob": 5.173831596039236e-06}, {"id": 1286, "seek": 616980, "start": 6185.16, "end": 6191.320000000001, "text": " If you then take the two predictions and take the average of them, you basically have a", "tokens": [759, 291, 550, 747, 264, 732, 21264, 293, 747, 264, 4274, 295, 552, 11, 291, 1936, 362, 257], "temperature": 0.0, "avg_logprob": -0.12470751693568279, "compression_ratio": 1.6487603305785123, "no_speech_prob": 5.173831596039236e-06}, {"id": 1287, "seek": 616980, "start": 6191.320000000001, "end": 6194.76, "text": " bi-directional model that you've trained each bit separately.", "tokens": [3228, 12, 18267, 41048, 2316, 300, 291, 600, 8895, 1184, 857, 14759, 13], "temperature": 0.0, "avg_logprob": -0.12470751693568279, "compression_ratio": 1.6487603305785123, "no_speech_prob": 5.173831596039236e-06}, {"id": 1288, "seek": 616980, "start": 6194.76, "end": 6198.76, "text": " That gets you to 95.4% accuracy.", "tokens": [663, 2170, 291, 281, 13420, 13, 19, 4, 14170, 13], "temperature": 0.0, "avg_logprob": -0.12470751693568279, "compression_ratio": 1.6487603305785123, "no_speech_prob": 5.173831596039236e-06}, {"id": 1289, "seek": 619876, "start": 6198.76, "end": 6202.92, "text": " So we've basically lowered it from 5.9% to 4.6%.", "tokens": [407, 321, 600, 1936, 28466, 309, 490, 1025, 13, 24, 4, 281, 1017, 13, 21, 6856], "temperature": 0.0, "avg_logprob": -0.2046116978289133, "compression_ratio": 1.405128205128205, "no_speech_prob": 3.844918410322862e-06}, {"id": 1290, "seek": 619876, "start": 6202.92, "end": 6210.72, "text": " So this kind of 20% change in the state of the art, it's almost unheard of.", "tokens": [407, 341, 733, 295, 945, 4, 1319, 294, 264, 1785, 295, 264, 1523, 11, 309, 311, 1920, 517, 42915, 295, 13], "temperature": 0.0, "avg_logprob": -0.2046116978289133, "compression_ratio": 1.405128205128205, "no_speech_prob": 3.844918410322862e-06}, {"id": 1291, "seek": 619876, "start": 6210.72, "end": 6218.0, "text": " You have to go back to Jeffrey Hinton's ImageNet computer vision thing where they chopped 30%", "tokens": [509, 362, 281, 352, 646, 281, 28721, 389, 12442, 311, 29903, 31890, 3820, 5201, 551, 689, 436, 16497, 2217, 4], "temperature": 0.0, "avg_logprob": -0.2046116978289133, "compression_ratio": 1.405128205128205, "no_speech_prob": 3.844918410322862e-06}, {"id": 1292, "seek": 619876, "start": 6218.0, "end": 6219.400000000001, "text": " off the state of the art.", "tokens": [766, 264, 1785, 295, 264, 1523, 13], "temperature": 0.0, "avg_logprob": -0.2046116978289133, "compression_ratio": 1.405128205128205, "no_speech_prob": 3.844918410322862e-06}, {"id": 1293, "seek": 619876, "start": 6219.400000000001, "end": 6221.0, "text": " It doesn't happen very often.", "tokens": [467, 1177, 380, 1051, 588, 2049, 13], "temperature": 0.0, "avg_logprob": -0.2046116978289133, "compression_ratio": 1.405128205128205, "no_speech_prob": 3.844918410322862e-06}, {"id": 1294, "seek": 622100, "start": 6221.0, "end": 6230.76, "text": " So you can see this idea of just use transfer learning is ridiculously powerful, but every", "tokens": [407, 291, 393, 536, 341, 1558, 295, 445, 764, 5003, 2539, 307, 41358, 4005, 11, 457, 633], "temperature": 0.0, "avg_logprob": -0.20242385622821277, "compression_ratio": 1.486910994764398, "no_speech_prob": 1.6028072877816157e-06}, {"id": 1295, "seek": 622100, "start": 6230.76, "end": 6237.12, "text": " new field thinks their new field is too special and you can't do it.", "tokens": [777, 2519, 7309, 641, 777, 2519, 307, 886, 2121, 293, 291, 393, 380, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.20242385622821277, "compression_ratio": 1.486910994764398, "no_speech_prob": 1.6028072877816157e-06}, {"id": 1296, "seek": 622100, "start": 6237.12, "end": 6243.02, "text": " So it's a big opportunity for all of us.", "tokens": [407, 309, 311, 257, 955, 2650, 337, 439, 295, 505, 13], "temperature": 0.0, "avg_logprob": -0.20242385622821277, "compression_ratio": 1.486910994764398, "no_speech_prob": 1.6028072877816157e-06}, {"id": 1297, "seek": 622100, "start": 6243.02, "end": 6244.52, "text": " So we turn this into a paper.", "tokens": [407, 321, 1261, 341, 666, 257, 3035, 13], "temperature": 0.0, "avg_logprob": -0.20242385622821277, "compression_ratio": 1.486910994764398, "no_speech_prob": 1.6028072877816157e-06}, {"id": 1298, "seek": 622100, "start": 6244.52, "end": 6248.24, "text": " When I say we, I did it with this guy Sebastian Ruda.", "tokens": [1133, 286, 584, 321, 11, 286, 630, 309, 365, 341, 2146, 31102, 497, 11152, 13], "temperature": 0.0, "avg_logprob": -0.20242385622821277, "compression_ratio": 1.486910994764398, "no_speech_prob": 1.6028072877816157e-06}, {"id": 1299, "seek": 624824, "start": 6248.24, "end": 6254.639999999999, "text": " Now you might remember his name, because in Lesson 5, I told you that I actually had shared", "tokens": [823, 291, 1062, 1604, 702, 1315, 11, 570, 294, 18649, 266, 1025, 11, 286, 1907, 291, 300, 286, 767, 632, 5507], "temperature": 0.0, "avg_logprob": -0.23554773745329483, "compression_ratio": 1.6704980842911878, "no_speech_prob": 8.397979399887845e-06}, {"id": 1300, "seek": 624824, "start": 6254.639999999999, "end": 6260.16, "text": " Lesson 4 with Sebastian because I think he's an awesome researcher who I thought might", "tokens": [18649, 266, 1017, 365, 31102, 570, 286, 519, 415, 311, 364, 3476, 21751, 567, 286, 1194, 1062], "temperature": 0.0, "avg_logprob": -0.23554773745329483, "compression_ratio": 1.6704980842911878, "no_speech_prob": 8.397979399887845e-06}, {"id": 1301, "seek": 624824, "start": 6260.16, "end": 6261.16, "text": " like it.", "tokens": [411, 309, 13], "temperature": 0.0, "avg_logprob": -0.23554773745329483, "compression_ratio": 1.6704980842911878, "no_speech_prob": 8.397979399887845e-06}, {"id": 1302, "seek": 624824, "start": 6261.16, "end": 6264.719999999999, "text": " I didn't know him personally at all.", "tokens": [286, 994, 380, 458, 796, 5665, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.23554773745329483, "compression_ratio": 1.6704980842911878, "no_speech_prob": 8.397979399887845e-06}, {"id": 1303, "seek": 624824, "start": 6264.719999999999, "end": 6268.08, "text": " And much to my surprise, he actually watched the damn video.", "tokens": [400, 709, 281, 452, 6365, 11, 415, 767, 6337, 264, 8151, 960, 13], "temperature": 0.0, "avg_logprob": -0.23554773745329483, "compression_ratio": 1.6704980842911878, "no_speech_prob": 8.397979399887845e-06}, {"id": 1304, "seek": 624824, "start": 6268.08, "end": 6273.639999999999, "text": " I was like, what NLP researcher is going to watch some beginner's video?", "tokens": [286, 390, 411, 11, 437, 426, 45196, 21751, 307, 516, 281, 1159, 512, 22080, 311, 960, 30], "temperature": 0.0, "avg_logprob": -0.23554773745329483, "compression_ratio": 1.6704980842911878, "no_speech_prob": 8.397979399887845e-06}, {"id": 1305, "seek": 624824, "start": 6273.639999999999, "end": 6274.88, "text": " But he watched the whole video.", "tokens": [583, 415, 6337, 264, 1379, 960, 13], "temperature": 0.0, "avg_logprob": -0.23554773745329483, "compression_ratio": 1.6704980842911878, "no_speech_prob": 8.397979399887845e-06}, {"id": 1306, "seek": 624824, "start": 6274.88, "end": 6277.44, "text": " He was like, that's actually quite fantastic.", "tokens": [634, 390, 411, 11, 300, 311, 767, 1596, 5456, 13], "temperature": 0.0, "avg_logprob": -0.23554773745329483, "compression_ratio": 1.6704980842911878, "no_speech_prob": 8.397979399887845e-06}, {"id": 1307, "seek": 627744, "start": 6277.44, "end": 6281.44, "text": " I was like, thank you very much, that's awesome coming from you.", "tokens": [286, 390, 411, 11, 1309, 291, 588, 709, 11, 300, 311, 3476, 1348, 490, 291, 13], "temperature": 0.0, "avg_logprob": -0.15088882446289062, "compression_ratio": 1.7682403433476395, "no_speech_prob": 4.469204577617347e-05}, {"id": 1308, "seek": 627744, "start": 6281.44, "end": 6286.759999999999, "text": " And he said, hey, we should turn this into a paper.", "tokens": [400, 415, 848, 11, 4177, 11, 321, 820, 1261, 341, 666, 257, 3035, 13], "temperature": 0.0, "avg_logprob": -0.15088882446289062, "compression_ratio": 1.7682403433476395, "no_speech_prob": 4.469204577617347e-05}, {"id": 1309, "seek": 627744, "start": 6286.759999999999, "end": 6289.0, "text": " And I said, I don't write papers.", "tokens": [400, 286, 848, 11, 286, 500, 380, 2464, 10577, 13], "temperature": 0.0, "avg_logprob": -0.15088882446289062, "compression_ratio": 1.7682403433476395, "no_speech_prob": 4.469204577617347e-05}, {"id": 1310, "seek": 627744, "start": 6289.0, "end": 6292.32, "text": " I don't care about papers, I'm not interested in papers.", "tokens": [286, 500, 380, 1127, 466, 10577, 11, 286, 478, 406, 3102, 294, 10577, 13], "temperature": 0.0, "avg_logprob": -0.15088882446289062, "compression_ratio": 1.7682403433476395, "no_speech_prob": 4.469204577617347e-05}, {"id": 1311, "seek": 627744, "start": 6292.32, "end": 6295.0199999999995, "text": " That sounds really boring.", "tokens": [663, 3263, 534, 9989, 13], "temperature": 0.0, "avg_logprob": -0.15088882446289062, "compression_ratio": 1.7682403433476395, "no_speech_prob": 4.469204577617347e-05}, {"id": 1312, "seek": 627744, "start": 6295.0199999999995, "end": 6300.4, "text": " And he said, okay, how about I write the paper for you?", "tokens": [400, 415, 848, 11, 1392, 11, 577, 466, 286, 2464, 264, 3035, 337, 291, 30], "temperature": 0.0, "avg_logprob": -0.15088882446289062, "compression_ratio": 1.7682403433476395, "no_speech_prob": 4.469204577617347e-05}, {"id": 1313, "seek": 627744, "start": 6300.4, "end": 6305.7, "text": " And I said, you can't really write a paper about this yet because you'd have to do studies", "tokens": [400, 286, 848, 11, 291, 393, 380, 534, 2464, 257, 3035, 466, 341, 1939, 570, 291, 1116, 362, 281, 360, 5313], "temperature": 0.0, "avg_logprob": -0.15088882446289062, "compression_ratio": 1.7682403433476395, "no_speech_prob": 4.469204577617347e-05}, {"id": 1314, "seek": 627744, "start": 6305.7, "end": 6307.0, "text": " to compare it to other things.", "tokens": [281, 6794, 309, 281, 661, 721, 13], "temperature": 0.0, "avg_logprob": -0.15088882446289062, "compression_ratio": 1.7682403433476395, "no_speech_prob": 4.469204577617347e-05}, {"id": 1315, "seek": 630700, "start": 6307.0, "end": 6310.24, "text": " They're called ablation studies to see which bits actually work.", "tokens": [814, 434, 1219, 410, 24278, 5313, 281, 536, 597, 9239, 767, 589, 13], "temperature": 0.0, "avg_logprob": -0.15050679161435082, "compression_ratio": 1.8172413793103448, "no_speech_prob": 0.00010720197315094993}, {"id": 1316, "seek": 630700, "start": 6310.24, "end": 6311.24, "text": " There's no rigor here.", "tokens": [821, 311, 572, 42191, 510, 13], "temperature": 0.0, "avg_logprob": -0.15050679161435082, "compression_ratio": 1.8172413793103448, "no_speech_prob": 0.00010720197315094993}, {"id": 1317, "seek": 630700, "start": 6311.24, "end": 6314.68, "text": " I just put in everything that came in my head and chucked it all together and it happened", "tokens": [286, 445, 829, 294, 1203, 300, 1361, 294, 452, 1378, 293, 20870, 292, 309, 439, 1214, 293, 309, 2011], "temperature": 0.0, "avg_logprob": -0.15050679161435082, "compression_ratio": 1.8172413793103448, "no_speech_prob": 0.00010720197315094993}, {"id": 1318, "seek": 630700, "start": 6314.68, "end": 6315.68, "text": " to work.", "tokens": [281, 589, 13], "temperature": 0.0, "avg_logprob": -0.15050679161435082, "compression_ratio": 1.8172413793103448, "no_speech_prob": 0.00010720197315094993}, {"id": 1319, "seek": 630700, "start": 6315.68, "end": 6320.0, "text": " And he was like, okay, what if I write all the paper and do all the ablation studies,", "tokens": [400, 415, 390, 411, 11, 1392, 11, 437, 498, 286, 2464, 439, 264, 3035, 293, 360, 439, 264, 410, 24278, 5313, 11], "temperature": 0.0, "avg_logprob": -0.15050679161435082, "compression_ratio": 1.8172413793103448, "no_speech_prob": 0.00010720197315094993}, {"id": 1320, "seek": 630700, "start": 6320.0, "end": 6321.66, "text": " then can we write the paper?", "tokens": [550, 393, 321, 2464, 264, 3035, 30], "temperature": 0.0, "avg_logprob": -0.15050679161435082, "compression_ratio": 1.8172413793103448, "no_speech_prob": 0.00010720197315094993}, {"id": 1321, "seek": 630700, "start": 6321.66, "end": 6329.6, "text": " And I said, well, it's like a whole library that I haven't documented, and I'm not going", "tokens": [400, 286, 848, 11, 731, 11, 309, 311, 411, 257, 1379, 6405, 300, 286, 2378, 380, 23007, 11, 293, 286, 478, 406, 516], "temperature": 0.0, "avg_logprob": -0.15050679161435082, "compression_ratio": 1.8172413793103448, "no_speech_prob": 0.00010720197315094993}, {"id": 1322, "seek": 630700, "start": 6329.6, "end": 6332.04, "text": " to yet, and you don't know how it all works.", "tokens": [281, 1939, 11, 293, 291, 500, 380, 458, 577, 309, 439, 1985, 13], "temperature": 0.0, "avg_logprob": -0.15050679161435082, "compression_ratio": 1.8172413793103448, "no_speech_prob": 0.00010720197315094993}, {"id": 1323, "seek": 630700, "start": 6332.04, "end": 6336.16, "text": " He said, okay, if I write the paper and do the ablation studies and figure out from scratch", "tokens": [634, 848, 11, 1392, 11, 498, 286, 2464, 264, 3035, 293, 360, 264, 410, 24278, 5313, 293, 2573, 484, 490, 8459], "temperature": 0.0, "avg_logprob": -0.15050679161435082, "compression_ratio": 1.8172413793103448, "no_speech_prob": 0.00010720197315094993}, {"id": 1324, "seek": 633616, "start": 6336.16, "end": 6340.28, "text": " how the code works without bothering you, then can we write the paper?", "tokens": [577, 264, 3089, 1985, 1553, 31432, 291, 11, 550, 393, 321, 2464, 264, 3035, 30], "temperature": 0.0, "avg_logprob": -0.2464204978942871, "compression_ratio": 1.7170731707317073, "no_speech_prob": 1.3211853001848795e-05}, {"id": 1325, "seek": 633616, "start": 6340.28, "end": 6347.36, "text": " I was like, yeah, if you did all those things, then you could write the paper.", "tokens": [286, 390, 411, 11, 1338, 11, 498, 291, 630, 439, 729, 721, 11, 550, 291, 727, 2464, 264, 3035, 13], "temperature": 0.0, "avg_logprob": -0.2464204978942871, "compression_ratio": 1.7170731707317073, "no_speech_prob": 1.3211853001848795e-05}, {"id": 1326, "seek": 633616, "start": 6347.36, "end": 6349.4, "text": " He was like, okay.", "tokens": [634, 390, 411, 11, 1392, 13], "temperature": 0.0, "avg_logprob": -0.2464204978942871, "compression_ratio": 1.7170731707317073, "no_speech_prob": 1.3211853001848795e-05}, {"id": 1327, "seek": 633616, "start": 6349.4, "end": 6354.5199999999995, "text": " And so then two days later he comes back and he says, okay, I've done a draft of the paper.", "tokens": [400, 370, 550, 732, 1708, 1780, 415, 1487, 646, 293, 415, 1619, 11, 1392, 11, 286, 600, 1096, 257, 11206, 295, 264, 3035, 13], "temperature": 0.0, "avg_logprob": -0.2464204978942871, "compression_ratio": 1.7170731707317073, "no_speech_prob": 1.3211853001848795e-05}, {"id": 1328, "seek": 633616, "start": 6354.5199999999995, "end": 6362.96, "text": " So I share this story to say, if you're some student in Ireland, he's a student in Ireland,", "tokens": [407, 286, 2073, 341, 1657, 281, 584, 11, 498, 291, 434, 512, 3107, 294, 15880, 11, 415, 311, 257, 3107, 294, 15880, 11], "temperature": 0.0, "avg_logprob": -0.2464204978942871, "compression_ratio": 1.7170731707317073, "no_speech_prob": 1.3211853001848795e-05}, {"id": 1329, "seek": 636296, "start": 6362.96, "end": 6367.12, "text": " and you want to do good work, don't let anybody stop you.", "tokens": [293, 291, 528, 281, 360, 665, 589, 11, 500, 380, 718, 4472, 1590, 291, 13], "temperature": 0.0, "avg_logprob": -0.18715699513753256, "compression_ratio": 1.6550218340611353, "no_speech_prob": 1.3006956578465179e-05}, {"id": 1330, "seek": 636296, "start": 6367.12, "end": 6372.88, "text": " I did not encourage him to say the least.", "tokens": [286, 630, 406, 5373, 796, 281, 584, 264, 1935, 13], "temperature": 0.0, "avg_logprob": -0.18715699513753256, "compression_ratio": 1.6550218340611353, "no_speech_prob": 1.3006956578465179e-05}, {"id": 1331, "seek": 636296, "start": 6372.88, "end": 6375.84, "text": " But in the end, he was like, look, I want to do this work.", "tokens": [583, 294, 264, 917, 11, 415, 390, 411, 11, 574, 11, 286, 528, 281, 360, 341, 589, 13], "temperature": 0.0, "avg_logprob": -0.18715699513753256, "compression_ratio": 1.6550218340611353, "no_speech_prob": 1.3006956578465179e-05}, {"id": 1332, "seek": 636296, "start": 6375.84, "end": 6378.52, "text": " I think it's going to be good and I'll figure it out.", "tokens": [286, 519, 309, 311, 516, 281, 312, 665, 293, 286, 603, 2573, 309, 484, 13], "temperature": 0.0, "avg_logprob": -0.18715699513753256, "compression_ratio": 1.6550218340611353, "no_speech_prob": 1.3006956578465179e-05}, {"id": 1333, "seek": 636296, "start": 6378.52, "end": 6383.4, "text": " And you know, he wrote a fantastic paper and he did the ablation studies and he figured", "tokens": [400, 291, 458, 11, 415, 4114, 257, 5456, 3035, 293, 415, 630, 264, 410, 24278, 5313, 293, 415, 8932], "temperature": 0.0, "avg_logprob": -0.18715699513753256, "compression_ratio": 1.6550218340611353, "no_speech_prob": 1.3006956578465179e-05}, {"id": 1334, "seek": 636296, "start": 6383.4, "end": 6388.76, "text": " out how fast AI works, and now we're planning to write another paper together.", "tokens": [484, 577, 2370, 7318, 1985, 11, 293, 586, 321, 434, 5038, 281, 2464, 1071, 3035, 1214, 13], "temperature": 0.0, "avg_logprob": -0.18715699513753256, "compression_ratio": 1.6550218340611353, "no_speech_prob": 1.3006956578465179e-05}, {"id": 1335, "seek": 638876, "start": 6388.76, "end": 6396.360000000001, "text": " And so you've got to be a bit careful, because sometimes I get messages from random people", "tokens": [400, 370, 291, 600, 658, 281, 312, 257, 857, 5026, 11, 570, 2171, 286, 483, 7897, 490, 4974, 561], "temperature": 0.0, "avg_logprob": -0.21066691828709022, "compression_ratio": 1.59915611814346, "no_speech_prob": 1.6187022993108258e-05}, {"id": 1336, "seek": 638876, "start": 6396.360000000001, "end": 6402.6, "text": " saying like, I've got lots of good ideas, can we have coffee?", "tokens": [1566, 411, 11, 286, 600, 658, 3195, 295, 665, 3487, 11, 393, 321, 362, 4982, 30], "temperature": 0.0, "avg_logprob": -0.21066691828709022, "compression_ratio": 1.59915611814346, "no_speech_prob": 1.6187022993108258e-05}, {"id": 1337, "seek": 638876, "start": 6402.6, "end": 6406.96, "text": " I can have coffee in my office anytime, thank you.", "tokens": [286, 393, 362, 4982, 294, 452, 3398, 13038, 11, 1309, 291, 13], "temperature": 0.0, "avg_logprob": -0.21066691828709022, "compression_ratio": 1.59915611814346, "no_speech_prob": 1.6187022993108258e-05}, {"id": 1338, "seek": 638876, "start": 6406.96, "end": 6410.96, "text": " But it's very different to say, hey, I took your ideas and I wrote a paper and I did a", "tokens": [583, 309, 311, 588, 819, 281, 584, 11, 4177, 11, 286, 1890, 428, 3487, 293, 286, 4114, 257, 3035, 293, 286, 630, 257], "temperature": 0.0, "avg_logprob": -0.21066691828709022, "compression_ratio": 1.59915611814346, "no_speech_prob": 1.6187022993108258e-05}, {"id": 1339, "seek": 638876, "start": 6410.96, "end": 6414.64, "text": " bunch of experiments and I figured out how your code works, I added documentation to", "tokens": [3840, 295, 12050, 293, 286, 8932, 484, 577, 428, 3089, 1985, 11, 286, 3869, 14333, 281], "temperature": 0.0, "avg_logprob": -0.21066691828709022, "compression_ratio": 1.59915611814346, "no_speech_prob": 1.6187022993108258e-05}, {"id": 1340, "seek": 638876, "start": 6414.64, "end": 6417.92, "text": " it.", "tokens": [309, 13], "temperature": 0.0, "avg_logprob": -0.21066691828709022, "compression_ratio": 1.59915611814346, "no_speech_prob": 1.6187022993108258e-05}, {"id": 1341, "seek": 641792, "start": 6417.92, "end": 6420.36, "text": " Should we submit this to a conference?", "tokens": [6454, 321, 10315, 341, 281, 257, 7586, 30], "temperature": 0.0, "avg_logprob": -0.19770161310831705, "compression_ratio": 1.547945205479452, "no_speech_prob": 1.4285079487308394e-05}, {"id": 1342, "seek": 641792, "start": 6420.36, "end": 6422.32, "text": " Do you see what I mean?", "tokens": [1144, 291, 536, 437, 286, 914, 30], "temperature": 0.0, "avg_logprob": -0.19770161310831705, "compression_ratio": 1.547945205479452, "no_speech_prob": 1.4285079487308394e-05}, {"id": 1343, "seek": 641792, "start": 6422.32, "end": 6428.32, "text": " There's nothing to stop you doing amazing work, and if you do amazing work that helps", "tokens": [821, 311, 1825, 281, 1590, 291, 884, 2243, 589, 11, 293, 498, 291, 360, 2243, 589, 300, 3665], "temperature": 0.0, "avg_logprob": -0.19770161310831705, "compression_ratio": 1.547945205479452, "no_speech_prob": 1.4285079487308394e-05}, {"id": 1344, "seek": 641792, "start": 6428.32, "end": 6433.84, "text": " somebody else, like in this case, I'm happy that we have a paper.", "tokens": [2618, 1646, 11, 411, 294, 341, 1389, 11, 286, 478, 2055, 300, 321, 362, 257, 3035, 13], "temperature": 0.0, "avg_logprob": -0.19770161310831705, "compression_ratio": 1.547945205479452, "no_speech_prob": 1.4285079487308394e-05}, {"id": 1345, "seek": 641792, "start": 6433.84, "end": 6439.56, "text": " I don't care about papers, but I think it's cool that these ideas now have this rigorous", "tokens": [286, 500, 380, 1127, 466, 10577, 11, 457, 286, 519, 309, 311, 1627, 300, 613, 3487, 586, 362, 341, 29882], "temperature": 0.0, "avg_logprob": -0.19770161310831705, "compression_ratio": 1.547945205479452, "no_speech_prob": 1.4285079487308394e-05}, {"id": 1346, "seek": 641792, "start": 6439.56, "end": 6440.56, "text": " study.", "tokens": [2979, 13], "temperature": 0.0, "avg_logprob": -0.19770161310831705, "compression_ratio": 1.547945205479452, "no_speech_prob": 1.4285079487308394e-05}, {"id": 1347, "seek": 641792, "start": 6440.56, "end": 6442.68, "text": " Let me show you what he did.", "tokens": [961, 385, 855, 291, 437, 415, 630, 13], "temperature": 0.0, "avg_logprob": -0.19770161310831705, "compression_ratio": 1.547945205479452, "no_speech_prob": 1.4285079487308394e-05}, {"id": 1348, "seek": 644268, "start": 6442.68, "end": 6449.4800000000005, "text": " So he took all my code, so I'd already done all the fastai.txt and stuff like that, and", "tokens": [407, 415, 1890, 439, 452, 3089, 11, 370, 286, 1116, 1217, 1096, 439, 264, 2370, 1301, 13, 83, 734, 293, 1507, 411, 300, 11, 293], "temperature": 0.0, "avg_logprob": -0.2120879061587222, "compression_ratio": 1.6571428571428573, "no_speech_prob": 8.66449045133777e-06}, {"id": 1349, "seek": 644268, "start": 6449.4800000000005, "end": 6454.4400000000005, "text": " as you've seen, it lets us work with large corpuses.", "tokens": [382, 291, 600, 1612, 11, 309, 6653, 505, 589, 365, 2416, 1181, 79, 8355, 13], "temperature": 0.0, "avg_logprob": -0.2120879061587222, "compression_ratio": 1.6571428571428573, "no_speech_prob": 8.66449045133777e-06}, {"id": 1350, "seek": 644268, "start": 6454.4400000000005, "end": 6459.52, "text": " So Sebastian is fantastically well-read, and he said here's a paper that Jan Lakudin and", "tokens": [407, 31102, 307, 4115, 22808, 731, 12, 2538, 11, 293, 415, 848, 510, 311, 257, 3035, 300, 4956, 37327, 532, 259, 293], "temperature": 0.0, "avg_logprob": -0.2120879061587222, "compression_ratio": 1.6571428571428573, "no_speech_prob": 8.66449045133777e-06}, {"id": 1351, "seek": 644268, "start": 6459.52, "end": 6464.04, "text": " some guys just came out with where they tried lots of different classification datasets,", "tokens": [512, 1074, 445, 1361, 484, 365, 689, 436, 3031, 3195, 295, 819, 21538, 42856, 11], "temperature": 0.0, "avg_logprob": -0.2120879061587222, "compression_ratio": 1.6571428571428573, "no_speech_prob": 8.66449045133777e-06}, {"id": 1352, "seek": 644268, "start": 6464.04, "end": 6467.88, "text": " so I'm going to try running your code on all these datasets.", "tokens": [370, 286, 478, 516, 281, 853, 2614, 428, 3089, 322, 439, 613, 42856, 13], "temperature": 0.0, "avg_logprob": -0.2120879061587222, "compression_ratio": 1.6571428571428573, "no_speech_prob": 8.66449045133777e-06}, {"id": 1353, "seek": 644268, "start": 6467.88, "end": 6469.52, "text": " So these are the datasets.", "tokens": [407, 613, 366, 264, 42856, 13], "temperature": 0.0, "avg_logprob": -0.2120879061587222, "compression_ratio": 1.6571428571428573, "no_speech_prob": 8.66449045133777e-06}, {"id": 1354, "seek": 646952, "start": 6469.52, "end": 6474.280000000001, "text": " And so some of them had many, many hundreds of thousands of documents, and they were far", "tokens": [400, 370, 512, 295, 552, 632, 867, 11, 867, 6779, 295, 5383, 295, 8512, 11, 293, 436, 645, 1400], "temperature": 0.0, "avg_logprob": -0.2012038230895996, "compression_ratio": 1.5384615384615385, "no_speech_prob": 9.368614882987458e-06}, {"id": 1355, "seek": 646952, "start": 6474.280000000001, "end": 6481.88, "text": " bigger than anything I had tried, but I thought it should work.", "tokens": [3801, 813, 1340, 286, 632, 3031, 11, 457, 286, 1194, 309, 820, 589, 13], "temperature": 0.0, "avg_logprob": -0.2012038230895996, "compression_ratio": 1.5384615384615385, "no_speech_prob": 9.368614882987458e-06}, {"id": 1356, "seek": 646952, "start": 6481.88, "end": 6488.200000000001, "text": " And so he had a few good little ideas as we went along, so you should totally make sure", "tokens": [400, 370, 415, 632, 257, 1326, 665, 707, 3487, 382, 321, 1437, 2051, 11, 370, 291, 820, 3879, 652, 988], "temperature": 0.0, "avg_logprob": -0.2012038230895996, "compression_ratio": 1.5384615384615385, "no_speech_prob": 9.368614882987458e-06}, {"id": 1357, "seek": 646952, "start": 6488.200000000001, "end": 6494.88, "text": " you read the paper.", "tokens": [291, 1401, 264, 3035, 13], "temperature": 0.0, "avg_logprob": -0.2012038230895996, "compression_ratio": 1.5384615384615385, "no_speech_prob": 9.368614882987458e-06}, {"id": 1358, "seek": 649488, "start": 6494.88, "end": 6500.400000000001, "text": " And he said, well this thing that you called in the lessons, differential learning rates,", "tokens": [400, 415, 848, 11, 731, 341, 551, 300, 291, 1219, 294, 264, 8820, 11, 15756, 2539, 6846, 11], "temperature": 0.0, "avg_logprob": -0.19422326521439987, "compression_ratio": 1.891566265060241, "no_speech_prob": 6.50261135888286e-05}, {"id": 1359, "seek": 649488, "start": 6500.400000000001, "end": 6503.56, "text": " differential kind of means something else, maybe we should rename it.", "tokens": [15756, 733, 295, 1355, 746, 1646, 11, 1310, 321, 820, 36741, 309, 13], "temperature": 0.0, "avg_logprob": -0.19422326521439987, "compression_ratio": 1.891566265060241, "no_speech_prob": 6.50261135888286e-05}, {"id": 1360, "seek": 649488, "start": 6503.56, "end": 6504.56, "text": " So we renamed it.", "tokens": [407, 321, 40949, 309, 13], "temperature": 0.0, "avg_logprob": -0.19422326521439987, "compression_ratio": 1.891566265060241, "no_speech_prob": 6.50261135888286e-05}, {"id": 1361, "seek": 649488, "start": 6504.56, "end": 6506.24, "text": " It's now called discriminative learning rates.", "tokens": [467, 311, 586, 1219, 20828, 1166, 2539, 6846, 13], "temperature": 0.0, "avg_logprob": -0.19422326521439987, "compression_ratio": 1.891566265060241, "no_speech_prob": 6.50261135888286e-05}, {"id": 1362, "seek": 649488, "start": 6506.24, "end": 6510.4400000000005, "text": " So this idea that we had from part 1 where we used different learning rates for different", "tokens": [407, 341, 1558, 300, 321, 632, 490, 644, 502, 689, 321, 1143, 819, 2539, 6846, 337, 819], "temperature": 0.0, "avg_logprob": -0.19422326521439987, "compression_ratio": 1.891566265060241, "no_speech_prob": 6.50261135888286e-05}, {"id": 1363, "seek": 649488, "start": 6510.4400000000005, "end": 6516.74, "text": " layers, after doing some literature research, it does seem like that hasn't been done before,", "tokens": [7914, 11, 934, 884, 512, 10394, 2132, 11, 309, 775, 1643, 411, 300, 6132, 380, 668, 1096, 949, 11], "temperature": 0.0, "avg_logprob": -0.19422326521439987, "compression_ratio": 1.891566265060241, "no_speech_prob": 6.50261135888286e-05}, {"id": 1364, "seek": 649488, "start": 6516.74, "end": 6521.400000000001, "text": " so it's now officially a thing, discriminative learning rates.", "tokens": [370, 309, 311, 586, 12053, 257, 551, 11, 20828, 1166, 2539, 6846, 13], "temperature": 0.0, "avg_logprob": -0.19422326521439987, "compression_ratio": 1.891566265060241, "no_speech_prob": 6.50261135888286e-05}, {"id": 1365, "seek": 652140, "start": 6521.4, "end": 6524.92, "text": " And so all these ideas, this is something we learned in lesson 1.", "tokens": [400, 370, 439, 613, 3487, 11, 341, 307, 746, 321, 3264, 294, 6898, 502, 13], "temperature": 0.0, "avg_logprob": -0.15720029084578804, "compression_ratio": 1.7440944881889764, "no_speech_prob": 6.2408084886556026e-06}, {"id": 1366, "seek": 652140, "start": 6524.92, "end": 6528.639999999999, "text": " It now has an equation with Greek and everything.", "tokens": [467, 586, 575, 364, 5367, 365, 10281, 293, 1203, 13], "temperature": 0.0, "avg_logprob": -0.15720029084578804, "compression_ratio": 1.7440944881889764, "no_speech_prob": 6.2408084886556026e-06}, {"id": 1367, "seek": 652140, "start": 6528.639999999999, "end": 6532.62, "text": " So when you see an equation with Greek and everything, that doesn't necessarily mean", "tokens": [407, 562, 291, 536, 364, 5367, 365, 10281, 293, 1203, 11, 300, 1177, 380, 4725, 914], "temperature": 0.0, "avg_logprob": -0.15720029084578804, "compression_ratio": 1.7440944881889764, "no_speech_prob": 6.2408084886556026e-06}, {"id": 1368, "seek": 652140, "start": 6532.62, "end": 6536.36, "text": " it's more complex than anything we did in lesson 1, because this one isn't.", "tokens": [309, 311, 544, 3997, 813, 1340, 321, 630, 294, 6898, 502, 11, 570, 341, 472, 1943, 380, 13], "temperature": 0.0, "avg_logprob": -0.15720029084578804, "compression_ratio": 1.7440944881889764, "no_speech_prob": 6.2408084886556026e-06}, {"id": 1369, "seek": 652140, "start": 6536.36, "end": 6542.759999999999, "text": " Again, that idea of unfreezing a layer at a time also seems to have never been done", "tokens": [3764, 11, 300, 1558, 295, 3971, 701, 8781, 257, 4583, 412, 257, 565, 611, 2544, 281, 362, 1128, 668, 1096], "temperature": 0.0, "avg_logprob": -0.15720029084578804, "compression_ratio": 1.7440944881889764, "no_speech_prob": 6.2408084886556026e-06}, {"id": 1370, "seek": 652140, "start": 6542.759999999999, "end": 6551.2, "text": " before, so it's now a thing, and it's got the very clever name gradual unfreezing.", "tokens": [949, 11, 370, 309, 311, 586, 257, 551, 11, 293, 309, 311, 658, 264, 588, 13494, 1315, 32890, 3971, 701, 8781, 13], "temperature": 0.0, "avg_logprob": -0.15720029084578804, "compression_ratio": 1.7440944881889764, "no_speech_prob": 6.2408084886556026e-06}, {"id": 1371, "seek": 655120, "start": 6551.2, "end": 6557.4, "text": " So then, long promised, we're going to look at this.", "tokens": [407, 550, 11, 938, 10768, 11, 321, 434, 516, 281, 574, 412, 341, 13], "temperature": 0.0, "avg_logprob": -0.22407609111857865, "compression_ratio": 1.5884615384615384, "no_speech_prob": 4.936942332278704e-06}, {"id": 1372, "seek": 655120, "start": 6557.4, "end": 6559.92, "text": " Slanted triangular learning rates.", "tokens": [6187, 15587, 38190, 2539, 6846, 13], "temperature": 0.0, "avg_logprob": -0.22407609111857865, "compression_ratio": 1.5884615384615384, "no_speech_prob": 4.936942332278704e-06}, {"id": 1373, "seek": 655120, "start": 6559.92, "end": 6562.679999999999, "text": " So this actually was not my idea.", "tokens": [407, 341, 767, 390, 406, 452, 1558, 13], "temperature": 0.0, "avg_logprob": -0.22407609111857865, "compression_ratio": 1.5884615384615384, "no_speech_prob": 4.936942332278704e-06}, {"id": 1374, "seek": 655120, "start": 6562.679999999999, "end": 6568.0, "text": " Leslie Smith, one of my favorite researchers who you all know about, emailed me a while", "tokens": [28140, 8538, 11, 472, 295, 452, 2954, 10309, 567, 291, 439, 458, 466, 11, 45460, 385, 257, 1339], "temperature": 0.0, "avg_logprob": -0.22407609111857865, "compression_ratio": 1.5884615384615384, "no_speech_prob": 4.936942332278704e-06}, {"id": 1375, "seek": 655120, "start": 6568.0, "end": 6573.8, "text": " ago and said, I'm so over certain learning rates, I don't do that anymore, I now do a", "tokens": [2057, 293, 848, 11, 286, 478, 370, 670, 1629, 2539, 6846, 11, 286, 500, 380, 360, 300, 3602, 11, 286, 586, 360, 257], "temperature": 0.0, "avg_logprob": -0.22407609111857865, "compression_ratio": 1.5884615384615384, "no_speech_prob": 4.936942332278704e-06}, {"id": 1376, "seek": 655120, "start": 6573.8, "end": 6577.96, "text": " slightly different version where I have one cycle which goes up quickly at the start and", "tokens": [4748, 819, 3037, 689, 286, 362, 472, 6586, 597, 1709, 493, 2661, 412, 264, 722, 293], "temperature": 0.0, "avg_logprob": -0.22407609111857865, "compression_ratio": 1.5884615384615384, "no_speech_prob": 4.936942332278704e-06}, {"id": 1377, "seek": 655120, "start": 6577.96, "end": 6580.48, "text": " then slowly down afterwards.", "tokens": [550, 5692, 760, 10543, 13], "temperature": 0.0, "avg_logprob": -0.22407609111857865, "compression_ratio": 1.5884615384615384, "no_speech_prob": 4.936942332278704e-06}, {"id": 1378, "seek": 658048, "start": 6580.48, "end": 6584.28, "text": " And he said I often find it works better, I've tried going back over all of my old data", "tokens": [400, 415, 848, 286, 2049, 915, 309, 1985, 1101, 11, 286, 600, 3031, 516, 646, 670, 439, 295, 452, 1331, 1412], "temperature": 0.0, "avg_logprob": -0.17371454778707252, "compression_ratio": 1.6382978723404256, "no_speech_prob": 8.530202649126295e-06}, {"id": 1379, "seek": 658048, "start": 6584.28, "end": 6588.08, "text": " sets and it works better for all of them, every one I tried.", "tokens": [6352, 293, 309, 1985, 1101, 337, 439, 295, 552, 11, 633, 472, 286, 3031, 13], "temperature": 0.0, "avg_logprob": -0.17371454778707252, "compression_ratio": 1.6382978723404256, "no_speech_prob": 8.530202649126295e-06}, {"id": 1380, "seek": 658048, "start": 6588.08, "end": 6590.839999999999, "text": " So this is what the learning rate looks like.", "tokens": [407, 341, 307, 437, 264, 2539, 3314, 1542, 411, 13], "temperature": 0.0, "avg_logprob": -0.17371454778707252, "compression_ratio": 1.6382978723404256, "no_speech_prob": 8.530202649126295e-06}, {"id": 1381, "seek": 658048, "start": 6590.839999999999, "end": 6597.0, "text": " You can use it in Fast AI just by adding UCLR equals to your fit.", "tokens": [509, 393, 764, 309, 294, 15968, 7318, 445, 538, 5127, 14079, 31722, 6915, 281, 428, 3318, 13], "temperature": 0.0, "avg_logprob": -0.17371454778707252, "compression_ratio": 1.6382978723404256, "no_speech_prob": 8.530202649126295e-06}, {"id": 1382, "seek": 658048, "start": 6597.0, "end": 6601.759999999999, "text": " This first number is the ratio between the highest learning rate and the lowest learning", "tokens": [639, 700, 1230, 307, 264, 8509, 1296, 264, 6343, 2539, 3314, 293, 264, 12437, 2539], "temperature": 0.0, "avg_logprob": -0.17371454778707252, "compression_ratio": 1.6382978723404256, "no_speech_prob": 8.530202649126295e-06}, {"id": 1383, "seek": 658048, "start": 6601.759999999999, "end": 6602.759999999999, "text": " rate.", "tokens": [3314, 13], "temperature": 0.0, "avg_logprob": -0.17371454778707252, "compression_ratio": 1.6382978723404256, "no_speech_prob": 8.530202649126295e-06}, {"id": 1384, "seek": 658048, "start": 6602.759999999999, "end": 6606.32, "text": " So here this is 1.32 of that.", "tokens": [407, 510, 341, 307, 502, 13, 11440, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.17371454778707252, "compression_ratio": 1.6382978723404256, "no_speech_prob": 8.530202649126295e-06}, {"id": 1385, "seek": 660632, "start": 6606.32, "end": 6612.5199999999995, "text": " The second number is the ratio between the first peak and the last peak.", "tokens": [440, 1150, 1230, 307, 264, 8509, 1296, 264, 700, 10651, 293, 264, 1036, 10651, 13], "temperature": 0.0, "avg_logprob": -0.16507214766282302, "compression_ratio": 1.6142857142857143, "no_speech_prob": 5.093676463729935e-06}, {"id": 1386, "seek": 660632, "start": 6612.5199999999995, "end": 6621.28, "text": " And so the basic idea is if you're doing a cycle length 10, that you want the first epoch", "tokens": [400, 370, 264, 3875, 1558, 307, 498, 291, 434, 884, 257, 6586, 4641, 1266, 11, 300, 291, 528, 264, 700, 30992, 339], "temperature": 0.0, "avg_logprob": -0.16507214766282302, "compression_ratio": 1.6142857142857143, "no_speech_prob": 5.093676463729935e-06}, {"id": 1387, "seek": 660632, "start": 6621.28, "end": 6625.719999999999, "text": " to be the upward bit and the other 9 epochs to be the downward bit, then you would use", "tokens": [281, 312, 264, 23452, 857, 293, 264, 661, 1722, 30992, 28346, 281, 312, 264, 24805, 857, 11, 550, 291, 576, 764], "temperature": 0.0, "avg_logprob": -0.16507214766282302, "compression_ratio": 1.6142857142857143, "no_speech_prob": 5.093676463729935e-06}, {"id": 1388, "seek": 660632, "start": 6625.719999999999, "end": 6626.719999999999, "text": " 10.", "tokens": [1266, 13], "temperature": 0.0, "avg_logprob": -0.16507214766282302, "compression_ratio": 1.6142857142857143, "no_speech_prob": 5.093676463729935e-06}, {"id": 1389, "seek": 660632, "start": 6626.719999999999, "end": 6631.04, "text": " And I find that works pretty well, that was also Leslie's suggestion, is make about a", "tokens": [400, 286, 915, 300, 1985, 1238, 731, 11, 300, 390, 611, 28140, 311, 16541, 11, 307, 652, 466, 257], "temperature": 0.0, "avg_logprob": -0.16507214766282302, "compression_ratio": 1.6142857142857143, "no_speech_prob": 5.093676463729935e-06}, {"id": 1390, "seek": 663104, "start": 6631.04, "end": 6637.0, "text": " tenth of it the upward bit and about 9 tenths the downward bit.", "tokens": [27269, 295, 309, 264, 23452, 857, 293, 466, 1722, 27269, 82, 264, 24805, 857, 13], "temperature": 0.0, "avg_logprob": -0.17637157440185547, "compression_ratio": 1.4844444444444445, "no_speech_prob": 2.5612655463191913e-06}, {"id": 1391, "seek": 663104, "start": 6637.0, "end": 6643.12, "text": " Since he told me about it, actually it was maybe 2 days ago, he wrote this amazing paper,", "tokens": [4162, 415, 1907, 385, 466, 309, 11, 767, 309, 390, 1310, 568, 1708, 2057, 11, 415, 4114, 341, 2243, 3035, 11], "temperature": 0.0, "avg_logprob": -0.17637157440185547, "compression_ratio": 1.4844444444444445, "no_speech_prob": 2.5612655463191913e-06}, {"id": 1392, "seek": 663104, "start": 6643.12, "end": 6649.48, "text": " A Disciplined Approach to Neural Network Hyperparameters, in which he described something very slightly", "tokens": [316, 4208, 19246, 2001, 29551, 608, 281, 1734, 1807, 12640, 29592, 2181, 335, 6202, 11, 294, 597, 415, 7619, 746, 588, 4748], "temperature": 0.0, "avg_logprob": -0.17637157440185547, "compression_ratio": 1.4844444444444445, "no_speech_prob": 2.5612655463191913e-06}, {"id": 1393, "seek": 663104, "start": 6649.48, "end": 6653.04, "text": " different to this again, but the same basic idea.", "tokens": [819, 281, 341, 797, 11, 457, 264, 912, 3875, 1558, 13], "temperature": 0.0, "avg_logprob": -0.17637157440185547, "compression_ratio": 1.4844444444444445, "no_speech_prob": 2.5612655463191913e-06}, {"id": 1394, "seek": 663104, "start": 6653.04, "end": 6655.96, "text": " This is a must-read paper.", "tokens": [639, 307, 257, 1633, 12, 2538, 3035, 13], "temperature": 0.0, "avg_logprob": -0.17637157440185547, "compression_ratio": 1.4844444444444445, "no_speech_prob": 2.5612655463191913e-06}, {"id": 1395, "seek": 665596, "start": 6655.96, "end": 6664.52, "text": " It's got all the kinds of ideas that FastAI talks about a lot in great depth, and nobody", "tokens": [467, 311, 658, 439, 264, 3685, 295, 3487, 300, 15968, 48698, 6686, 466, 257, 688, 294, 869, 7161, 11, 293, 5079], "temperature": 0.0, "avg_logprob": -0.15095220954672803, "compression_ratio": 1.563265306122449, "no_speech_prob": 6.7479941208148375e-06}, {"id": 1396, "seek": 665596, "start": 6664.52, "end": 6668.32, "text": " else is talking about this stuff.", "tokens": [1646, 307, 1417, 466, 341, 1507, 13], "temperature": 0.0, "avg_logprob": -0.15095220954672803, "compression_ratio": 1.563265306122449, "no_speech_prob": 6.7479941208148375e-06}, {"id": 1397, "seek": 665596, "start": 6668.32, "end": 6669.8, "text": " It's kind of a slog.", "tokens": [467, 311, 733, 295, 257, 49760, 13], "temperature": 0.0, "avg_logprob": -0.15095220954672803, "compression_ratio": 1.563265306122449, "no_speech_prob": 6.7479941208148375e-06}, {"id": 1398, "seek": 665596, "start": 6669.8, "end": 6673.6, "text": " Unfortunately Leslie had to go away on a trip before he really had time to edit it properly,", "tokens": [8590, 28140, 632, 281, 352, 1314, 322, 257, 4931, 949, 415, 534, 632, 565, 281, 8129, 309, 6108, 11], "temperature": 0.0, "avg_logprob": -0.15095220954672803, "compression_ratio": 1.563265306122449, "no_speech_prob": 6.7479941208148375e-06}, {"id": 1399, "seek": 665596, "start": 6673.6, "end": 6679.78, "text": " so it's a little bit slow reading, but don't let that stop you, it's amazing.", "tokens": [370, 309, 311, 257, 707, 857, 2964, 3760, 11, 457, 500, 380, 718, 300, 1590, 291, 11, 309, 311, 2243, 13], "temperature": 0.0, "avg_logprob": -0.15095220954672803, "compression_ratio": 1.563265306122449, "no_speech_prob": 6.7479941208148375e-06}, {"id": 1400, "seek": 665596, "start": 6679.78, "end": 6683.28, "text": " So this triangle, this is the equation from my paper with Sebastian.", "tokens": [407, 341, 13369, 11, 341, 307, 264, 5367, 490, 452, 3035, 365, 31102, 13], "temperature": 0.0, "avg_logprob": -0.15095220954672803, "compression_ratio": 1.563265306122449, "no_speech_prob": 6.7479941208148375e-06}, {"id": 1401, "seek": 668328, "start": 6683.28, "end": 6688.5199999999995, "text": " Sebastian was like, Jeremy, can you send me the math equation behind that code you wrote?", "tokens": [31102, 390, 411, 11, 17809, 11, 393, 291, 2845, 385, 264, 5221, 5367, 2261, 300, 3089, 291, 4114, 30], "temperature": 0.0, "avg_logprob": -0.16865625052616515, "compression_ratio": 1.5431472081218274, "no_speech_prob": 6.144112830952508e-06}, {"id": 1402, "seek": 668328, "start": 6688.5199999999995, "end": 6692.32, "text": " I was like, no, I just wrote the code, I could not turn it into math.", "tokens": [286, 390, 411, 11, 572, 11, 286, 445, 4114, 264, 3089, 11, 286, 727, 406, 1261, 309, 666, 5221, 13], "temperature": 0.0, "avg_logprob": -0.16865625052616515, "compression_ratio": 1.5431472081218274, "no_speech_prob": 6.144112830952508e-06}, {"id": 1403, "seek": 668328, "start": 6692.32, "end": 6697.16, "text": " So he figured out the math for it.", "tokens": [407, 415, 8932, 484, 264, 5221, 337, 309, 13], "temperature": 0.0, "avg_logprob": -0.16865625052616515, "compression_ratio": 1.5431472081218274, "no_speech_prob": 6.144112830952508e-06}, {"id": 1404, "seek": 668328, "start": 6697.16, "end": 6707.84, "text": " So you might have noticed the first layer of our classifier was equal to embedding size", "tokens": [407, 291, 1062, 362, 5694, 264, 700, 4583, 295, 527, 1508, 9902, 390, 2681, 281, 12240, 3584, 2744], "temperature": 0.0, "avg_logprob": -0.16865625052616515, "compression_ratio": 1.5431472081218274, "no_speech_prob": 6.144112830952508e-06}, {"id": 1405, "seek": 668328, "start": 6707.84, "end": 6710.04, "text": " times 3.", "tokens": [1413, 805, 13], "temperature": 0.0, "avg_logprob": -0.16865625052616515, "compression_ratio": 1.5431472081218274, "no_speech_prob": 6.144112830952508e-06}, {"id": 1406, "seek": 668328, "start": 6710.04, "end": 6712.24, "text": " Why times 3?", "tokens": [1545, 1413, 805, 30], "temperature": 0.0, "avg_logprob": -0.16865625052616515, "compression_ratio": 1.5431472081218274, "no_speech_prob": 6.144112830952508e-06}, {"id": 1407, "seek": 671224, "start": 6712.24, "end": 6717.639999999999, "text": " Times 3 because, and again this seems to be something people haven't done before, so new", "tokens": [11366, 805, 570, 11, 293, 797, 341, 2544, 281, 312, 746, 561, 2378, 380, 1096, 949, 11, 370, 777], "temperature": 0.0, "avg_logprob": -0.21078847489267025, "compression_ratio": 1.7261410788381744, "no_speech_prob": 1.3211849363869987e-05}, {"id": 1408, "seek": 671224, "start": 6717.639999999999, "end": 6726.44, "text": " idea, CONCAT pooling, which is that we take the average pooling over the sequence of the", "tokens": [1558, 11, 16596, 34, 2218, 7005, 278, 11, 597, 307, 300, 321, 747, 264, 4274, 7005, 278, 670, 264, 8310, 295, 264], "temperature": 0.0, "avg_logprob": -0.21078847489267025, "compression_ratio": 1.7261410788381744, "no_speech_prob": 1.3211849363869987e-05}, {"id": 1409, "seek": 671224, "start": 6726.44, "end": 6732.16, "text": " activations, the max pooling of the sequence over the activations, and the final set of", "tokens": [2430, 763, 11, 264, 11469, 7005, 278, 295, 264, 8310, 670, 264, 2430, 763, 11, 293, 264, 2572, 992, 295], "temperature": 0.0, "avg_logprob": -0.21078847489267025, "compression_ratio": 1.7261410788381744, "no_speech_prob": 1.3211849363869987e-05}, {"id": 1410, "seek": 671224, "start": 6732.16, "end": 6734.8, "text": " activations and just concatenate them all together.", "tokens": [2430, 763, 293, 445, 1588, 7186, 473, 552, 439, 1214, 13], "temperature": 0.0, "avg_logprob": -0.21078847489267025, "compression_ratio": 1.7261410788381744, "no_speech_prob": 1.3211849363869987e-05}, {"id": 1411, "seek": 671224, "start": 6734.8, "end": 6741.5199999999995, "text": " Again, this is something which we talked about in part 1, but doesn't seem to be in the literature", "tokens": [3764, 11, 341, 307, 746, 597, 321, 2825, 466, 294, 644, 502, 11, 457, 1177, 380, 1643, 281, 312, 294, 264, 10394], "temperature": 0.0, "avg_logprob": -0.21078847489267025, "compression_ratio": 1.7261410788381744, "no_speech_prob": 1.3211849363869987e-05}, {"id": 1412, "seek": 674152, "start": 6741.52, "end": 6747.040000000001, "text": " before, so it's now called CONCAT pooling and again it's now got an equation and everything,", "tokens": [949, 11, 370, 309, 311, 586, 1219, 16596, 34, 2218, 7005, 278, 293, 797, 309, 311, 586, 658, 364, 5367, 293, 1203, 11], "temperature": 0.0, "avg_logprob": -0.17308109998703003, "compression_ratio": 1.5072463768115942, "no_speech_prob": 7.646457561349962e-06}, {"id": 1413, "seek": 674152, "start": 6747.040000000001, "end": 6751.6, "text": " but this is the entirety of the implementation.", "tokens": [457, 341, 307, 264, 31557, 295, 264, 11420, 13], "temperature": 0.0, "avg_logprob": -0.17308109998703003, "compression_ratio": 1.5072463768115942, "no_speech_prob": 7.646457561349962e-06}, {"id": 1414, "seek": 674152, "start": 6751.6, "end": 6758.4800000000005, "text": " Pool with average, pool with max, concatenate those two along with the final sequence.", "tokens": [46188, 365, 4274, 11, 7005, 365, 11469, 11, 1588, 7186, 473, 729, 732, 2051, 365, 264, 2572, 8310, 13], "temperature": 0.0, "avg_logprob": -0.17308109998703003, "compression_ratio": 1.5072463768115942, "no_speech_prob": 7.646457561349962e-06}, {"id": 1415, "seek": 674152, "start": 6758.4800000000005, "end": 6767.120000000001, "text": " So you can go through this paper and see how the Fast AI code implements each piece.", "tokens": [407, 291, 393, 352, 807, 341, 3035, 293, 536, 577, 264, 15968, 7318, 3089, 704, 17988, 1184, 2522, 13], "temperature": 0.0, "avg_logprob": -0.17308109998703003, "compression_ratio": 1.5072463768115942, "no_speech_prob": 7.646457561349962e-06}, {"id": 1416, "seek": 676712, "start": 6767.12, "end": 6775.24, "text": " So then, to me one of the interesting pieces is the difference between RNN encoder, which", "tokens": [407, 550, 11, 281, 385, 472, 295, 264, 1880, 3755, 307, 264, 2649, 1296, 45702, 45, 2058, 19866, 11, 597], "temperature": 0.0, "avg_logprob": -0.19885808159323298, "compression_ratio": 1.5736842105263158, "no_speech_prob": 4.425463885127101e-06}, {"id": 1417, "seek": 676712, "start": 6775.24, "end": 6779.0, "text": " you've already seen, and multi-batch RNN encoder.", "tokens": [291, 600, 1217, 1612, 11, 293, 4825, 12, 65, 852, 45702, 45, 2058, 19866, 13], "temperature": 0.0, "avg_logprob": -0.19885808159323298, "compression_ratio": 1.5736842105263158, "no_speech_prob": 4.425463885127101e-06}, {"id": 1418, "seek": 676712, "start": 6779.0, "end": 6780.8, "text": " So what's the difference there?", "tokens": [407, 437, 311, 264, 2649, 456, 30], "temperature": 0.0, "avg_logprob": -0.19885808159323298, "compression_ratio": 1.5736842105263158, "no_speech_prob": 4.425463885127101e-06}, {"id": 1419, "seek": 676712, "start": 6780.8, "end": 6785.92, "text": " So the key difference is that the normal RNN encoder for the language model, we could just", "tokens": [407, 264, 2141, 2649, 307, 300, 264, 2710, 45702, 45, 2058, 19866, 337, 264, 2856, 2316, 11, 321, 727, 445], "temperature": 0.0, "avg_logprob": -0.19885808159323298, "compression_ratio": 1.5736842105263158, "no_speech_prob": 4.425463885127101e-06}, {"id": 1420, "seek": 676712, "start": 6785.92, "end": 6792.72, "text": " do BPTT chunk at a time, no problem.", "tokens": [360, 40533, 28178, 16635, 412, 257, 565, 11, 572, 1154, 13], "temperature": 0.0, "avg_logprob": -0.19885808159323298, "compression_ratio": 1.5736842105263158, "no_speech_prob": 4.425463885127101e-06}, {"id": 1421, "seek": 679272, "start": 6792.72, "end": 6801.4800000000005, "text": " But for the classifier, we need to do the whole document.", "tokens": [583, 337, 264, 1508, 9902, 11, 321, 643, 281, 360, 264, 1379, 4166, 13], "temperature": 0.0, "avg_logprob": -0.18155273577062095, "compression_ratio": 1.6358974358974359, "no_speech_prob": 4.356848421593895e-06}, {"id": 1422, "seek": 679272, "start": 6801.4800000000005, "end": 6805.6, "text": " We need to do the whole movie review before we decide if it's positive or negative.", "tokens": [492, 643, 281, 360, 264, 1379, 3169, 3131, 949, 321, 4536, 498, 309, 311, 3353, 420, 3671, 13], "temperature": 0.0, "avg_logprob": -0.18155273577062095, "compression_ratio": 1.6358974358974359, "no_speech_prob": 4.356848421593895e-06}, {"id": 1423, "seek": 679272, "start": 6805.6, "end": 6811.84, "text": " The whole movie review can easily be 2000 words long, and I can't fit 2000 words worth", "tokens": [440, 1379, 3169, 3131, 393, 3612, 312, 8132, 2283, 938, 11, 293, 286, 393, 380, 3318, 8132, 2283, 3163], "temperature": 0.0, "avg_logprob": -0.18155273577062095, "compression_ratio": 1.6358974358974359, "no_speech_prob": 4.356848421593895e-06}, {"id": 1424, "seek": 679272, "start": 6811.84, "end": 6820.96, "text": " of gradients in my GPU memory for every single one of my activations, sorry, for every one", "tokens": [295, 2771, 2448, 294, 452, 18407, 4675, 337, 633, 2167, 472, 295, 452, 2430, 763, 11, 2597, 11, 337, 633, 472], "temperature": 0.0, "avg_logprob": -0.18155273577062095, "compression_ratio": 1.6358974358974359, "no_speech_prob": 4.356848421593895e-06}, {"id": 1425, "seek": 682096, "start": 6820.96, "end": 6824.72, "text": " of my weights, so what do I do?", "tokens": [295, 452, 17443, 11, 370, 437, 360, 286, 360, 30], "temperature": 0.0, "avg_logprob": -0.17896969710724264, "compression_ratio": 1.4207650273224044, "no_speech_prob": 8.013455953914672e-06}, {"id": 1426, "seek": 682096, "start": 6824.72, "end": 6832.16, "text": " And so the idea was very simple, which is I go through my whole sequence length, one", "tokens": [400, 370, 264, 1558, 390, 588, 2199, 11, 597, 307, 286, 352, 807, 452, 1379, 8310, 4641, 11, 472], "temperature": 0.0, "avg_logprob": -0.17896969710724264, "compression_ratio": 1.4207650273224044, "no_speech_prob": 8.013455953914672e-06}, {"id": 1427, "seek": 682096, "start": 6832.16, "end": 6841.92, "text": " batch of BPTT at a time, and I call super.forward, so in other words the RNN encoder, so I just", "tokens": [15245, 295, 40533, 28178, 412, 257, 565, 11, 293, 286, 818, 1687, 13, 13305, 11, 370, 294, 661, 2283, 264, 45702, 45, 2058, 19866, 11, 370, 286, 445], "temperature": 0.0, "avg_logprob": -0.17896969710724264, "compression_ratio": 1.4207650273224044, "no_speech_prob": 8.013455953914672e-06}, {"id": 1428, "seek": 682096, "start": 6841.92, "end": 6847.08, "text": " call the usual RNN encoder to grab its outputs.", "tokens": [818, 264, 7713, 45702, 45, 2058, 19866, 281, 4444, 1080, 23930, 13], "temperature": 0.0, "avg_logprob": -0.17896969710724264, "compression_ratio": 1.4207650273224044, "no_speech_prob": 8.013455953914672e-06}, {"id": 1429, "seek": 684708, "start": 6847.08, "end": 6857.28, "text": " And then I've got this maximum sequence length parameter where it says, as long as you're", "tokens": [400, 550, 286, 600, 658, 341, 6674, 8310, 4641, 13075, 689, 309, 1619, 11, 382, 938, 382, 291, 434], "temperature": 0.0, "avg_logprob": -0.15817313407784078, "compression_ratio": 1.5371428571428571, "no_speech_prob": 3.3931278267118614e-06}, {"id": 1430, "seek": 684708, "start": 6857.28, "end": 6865.42, "text": " doing no more than that sequence length, then start appending it to my list of outputs.", "tokens": [884, 572, 544, 813, 300, 8310, 4641, 11, 550, 722, 724, 2029, 309, 281, 452, 1329, 295, 23930, 13], "temperature": 0.0, "avg_logprob": -0.15817313407784078, "compression_ratio": 1.5371428571428571, "no_speech_prob": 3.3931278267118614e-06}, {"id": 1431, "seek": 684708, "start": 6865.42, "end": 6877.0, "text": " So in other words, the thing that it sends back to this pooling is only as many activations", "tokens": [407, 294, 661, 2283, 11, 264, 551, 300, 309, 14790, 646, 281, 341, 7005, 278, 307, 787, 382, 867, 2430, 763], "temperature": 0.0, "avg_logprob": -0.15817313407784078, "compression_ratio": 1.5371428571428571, "no_speech_prob": 3.3931278267118614e-06}, {"id": 1432, "seek": 687700, "start": 6877.0, "end": 6880.88, "text": " as we've asked it to keep.", "tokens": [382, 321, 600, 2351, 309, 281, 1066, 13], "temperature": 0.0, "avg_logprob": -0.1820523738861084, "compression_ratio": 1.4201183431952662, "no_speech_prob": 4.936964160151547e-06}, {"id": 1433, "seek": 687700, "start": 6880.88, "end": 6891.98, "text": " And so that way you can basically figure out what's maxsec can your particular GPU handle.", "tokens": [400, 370, 300, 636, 291, 393, 1936, 2573, 484, 437, 311, 11469, 8159, 393, 428, 1729, 18407, 4813, 13], "temperature": 0.0, "avg_logprob": -0.1820523738861084, "compression_ratio": 1.4201183431952662, "no_speech_prob": 4.936964160151547e-06}, {"id": 1434, "seek": 687700, "start": 6891.98, "end": 6899.58, "text": " So it's still using the whole document, but let's say maxsec is 1000 words and your longest", "tokens": [407, 309, 311, 920, 1228, 264, 1379, 4166, 11, 457, 718, 311, 584, 11469, 8159, 307, 9714, 2283, 293, 428, 15438], "temperature": 0.0, "avg_logprob": -0.1820523738861084, "compression_ratio": 1.4201183431952662, "no_speech_prob": 4.936964160151547e-06}, {"id": 1435, "seek": 687700, "start": 6899.58, "end": 6902.48, "text": " document length is 2000 words.", "tokens": [4166, 4641, 307, 8132, 2283, 13], "temperature": 0.0, "avg_logprob": -0.1820523738861084, "compression_ratio": 1.4201183431952662, "no_speech_prob": 4.936964160151547e-06}, {"id": 1436, "seek": 690248, "start": 6902.48, "end": 6908.459999999999, "text": " And it's still going through the RNN, creating state for those first 1000 words, but it's", "tokens": [400, 309, 311, 920, 516, 807, 264, 45702, 45, 11, 4084, 1785, 337, 729, 700, 9714, 2283, 11, 457, 309, 311], "temperature": 0.0, "avg_logprob": -0.1405718425909678, "compression_ratio": 1.7704081632653061, "no_speech_prob": 1.4593696278097923e-06}, {"id": 1437, "seek": 690248, "start": 6908.459999999999, "end": 6915.08, "text": " not actually going to store the activations for the backprop for the first 1000, it's", "tokens": [406, 767, 516, 281, 3531, 264, 2430, 763, 337, 264, 646, 79, 1513, 337, 264, 700, 9714, 11, 309, 311], "temperature": 0.0, "avg_logprob": -0.1405718425909678, "compression_ratio": 1.7704081632653061, "no_speech_prob": 1.4593696278097923e-06}, {"id": 1438, "seek": 690248, "start": 6915.08, "end": 6917.54, "text": " only going to keep the last 1000.", "tokens": [787, 516, 281, 1066, 264, 1036, 9714, 13], "temperature": 0.0, "avg_logprob": -0.1405718425909678, "compression_ratio": 1.7704081632653061, "no_speech_prob": 1.4593696278097923e-06}, {"id": 1439, "seek": 690248, "start": 6917.54, "end": 6926.32, "text": " So that means that it can't backpropagate the loss back to any state that was created", "tokens": [407, 300, 1355, 300, 309, 393, 380, 646, 79, 1513, 559, 473, 264, 4470, 646, 281, 604, 1785, 300, 390, 2942], "temperature": 0.0, "avg_logprob": -0.1405718425909678, "compression_ratio": 1.7704081632653061, "no_speech_prob": 1.4593696278097923e-06}, {"id": 1440, "seek": 690248, "start": 6926.32, "end": 6928.799999999999, "text": " in the first 1000 words.", "tokens": [294, 264, 700, 9714, 2283, 13], "temperature": 0.0, "avg_logprob": -0.1405718425909678, "compression_ratio": 1.7704081632653061, "no_speech_prob": 1.4593696278097923e-06}, {"id": 1441, "seek": 690248, "start": 6928.799999999999, "end": 6931.719999999999, "text": " Basically that's now gone.", "tokens": [8537, 300, 311, 586, 2780, 13], "temperature": 0.0, "avg_logprob": -0.1405718425909678, "compression_ratio": 1.7704081632653061, "no_speech_prob": 1.4593696278097923e-06}, {"id": 1442, "seek": 693172, "start": 6931.72, "end": 6939.280000000001, "text": " So it's a really simple piece of code, and honestly when I wrote it, it was like, I didn't", "tokens": [407, 309, 311, 257, 534, 2199, 2522, 295, 3089, 11, 293, 6095, 562, 286, 4114, 309, 11, 309, 390, 411, 11, 286, 994, 380], "temperature": 0.0, "avg_logprob": -0.20465616079477164, "compression_ratio": 1.5732217573221758, "no_speech_prob": 5.862792022526264e-06}, {"id": 1443, "seek": 693172, "start": 6939.280000000001, "end": 6942.4800000000005, "text": " spend much time thinking about it, it seems so obviously the only way that this could", "tokens": [3496, 709, 565, 1953, 466, 309, 11, 309, 2544, 370, 2745, 264, 787, 636, 300, 341, 727], "temperature": 0.0, "avg_logprob": -0.20465616079477164, "compression_ratio": 1.5732217573221758, "no_speech_prob": 5.862792022526264e-06}, {"id": 1444, "seek": 693172, "start": 6942.4800000000005, "end": 6943.8, "text": " possibly work.", "tokens": [6264, 589, 13], "temperature": 0.0, "avg_logprob": -0.20465616079477164, "compression_ratio": 1.5732217573221758, "no_speech_prob": 5.862792022526264e-06}, {"id": 1445, "seek": 693172, "start": 6943.8, "end": 6946.68, "text": " But again, it seems to be a new thing.", "tokens": [583, 797, 11, 309, 2544, 281, 312, 257, 777, 551, 13], "temperature": 0.0, "avg_logprob": -0.20465616079477164, "compression_ratio": 1.5732217573221758, "no_speech_prob": 5.862792022526264e-06}, {"id": 1446, "seek": 693172, "start": 6946.68, "end": 6951.6, "text": " So we now have backprop through time for text classification.", "tokens": [407, 321, 586, 362, 646, 79, 1513, 807, 565, 337, 2487, 21538, 13], "temperature": 0.0, "avg_logprob": -0.20465616079477164, "compression_ratio": 1.5732217573221758, "no_speech_prob": 5.862792022526264e-06}, {"id": 1447, "seek": 693172, "start": 6951.6, "end": 6956.4800000000005, "text": " So you can see there's lots of little pieces in this paper.", "tokens": [407, 291, 393, 536, 456, 311, 3195, 295, 707, 3755, 294, 341, 3035, 13], "temperature": 0.0, "avg_logprob": -0.20465616079477164, "compression_ratio": 1.5732217573221758, "no_speech_prob": 5.862792022526264e-06}, {"id": 1448, "seek": 693172, "start": 6956.4800000000005, "end": 6959.0, "text": " So what was the result?", "tokens": [407, 437, 390, 264, 1874, 30], "temperature": 0.0, "avg_logprob": -0.20465616079477164, "compression_ratio": 1.5732217573221758, "no_speech_prob": 5.862792022526264e-06}, {"id": 1449, "seek": 695900, "start": 6959.0, "end": 6966.2, "text": " So the result was, on every single dataset we tried, we got a better result than any", "tokens": [407, 264, 1874, 390, 11, 322, 633, 2167, 28872, 321, 3031, 11, 321, 658, 257, 1101, 1874, 813, 604], "temperature": 0.0, "avg_logprob": -0.19318289634508964, "compression_ratio": 1.385786802030457, "no_speech_prob": 2.295903186677606e-06}, {"id": 1450, "seek": 695900, "start": 6966.2, "end": 6971.52, "text": " previous academic for text classification.", "tokens": [3894, 7778, 337, 2487, 21538, 13], "temperature": 0.0, "avg_logprob": -0.19318289634508964, "compression_ratio": 1.385786802030457, "no_speech_prob": 2.295903186677606e-06}, {"id": 1451, "seek": 695900, "start": 6971.52, "end": 6980.88, "text": " So IMDB, Trek6, AGnews, Dbpedia, Yelp, all different types.", "tokens": [407, 21463, 27735, 11, 25845, 21, 11, 28406, 7686, 82, 11, 413, 65, 3452, 654, 11, 398, 28591, 11, 439, 819, 3467, 13], "temperature": 0.0, "avg_logprob": -0.19318289634508964, "compression_ratio": 1.385786802030457, "no_speech_prob": 2.295903186677606e-06}, {"id": 1452, "seek": 695900, "start": 6980.88, "end": 6985.72, "text": " And honestly, IMDB was the only one I spent any time trying to optimize the model, so", "tokens": [400, 6095, 11, 21463, 27735, 390, 264, 787, 472, 286, 4418, 604, 565, 1382, 281, 19719, 264, 2316, 11, 370], "temperature": 0.0, "avg_logprob": -0.19318289634508964, "compression_ratio": 1.385786802030457, "no_speech_prob": 2.295903186677606e-06}, {"id": 1453, "seek": 698572, "start": 6985.72, "end": 6989.0, "text": " like most of them, we just did it like whatever came out first.", "tokens": [411, 881, 295, 552, 11, 321, 445, 630, 309, 411, 2035, 1361, 484, 700, 13], "temperature": 0.0, "avg_logprob": -0.20140866535465893, "compression_ratio": 1.5671641791044777, "no_speech_prob": 6.962196039239643e-06}, {"id": 1454, "seek": 698572, "start": 6989.0, "end": 6993.400000000001, "text": " So if we actually spent time on it, I think these would be a lot better.", "tokens": [407, 498, 321, 767, 4418, 565, 322, 309, 11, 286, 519, 613, 576, 312, 257, 688, 1101, 13], "temperature": 0.0, "avg_logprob": -0.20140866535465893, "compression_ratio": 1.5671641791044777, "no_speech_prob": 6.962196039239643e-06}, {"id": 1455, "seek": 698572, "start": 6993.400000000001, "end": 7000.2, "text": " And the things that these are comparing to, most of them are, you'll see they're different", "tokens": [400, 264, 721, 300, 613, 366, 15763, 281, 11, 881, 295, 552, 366, 11, 291, 603, 536, 436, 434, 819], "temperature": 0.0, "avg_logprob": -0.20140866535465893, "compression_ratio": 1.5671641791044777, "no_speech_prob": 6.962196039239643e-06}, {"id": 1456, "seek": 698572, "start": 7000.2, "end": 7003.08, "text": " on each table, because they're optimized.", "tokens": [322, 1184, 3199, 11, 570, 436, 434, 26941, 13], "temperature": 0.0, "avg_logprob": -0.20140866535465893, "compression_ratio": 1.5671641791044777, "no_speech_prob": 6.962196039239643e-06}, {"id": 1457, "seek": 698572, "start": 7003.08, "end": 7005.56, "text": " These are customized algorithms on the whole.", "tokens": [1981, 366, 30581, 14642, 322, 264, 1379, 13], "temperature": 0.0, "avg_logprob": -0.20140866535465893, "compression_ratio": 1.5671641791044777, "no_speech_prob": 6.962196039239643e-06}, {"id": 1458, "seek": 700556, "start": 7005.56, "end": 7016.4400000000005, "text": " So this is saying one simple fine-tuning algorithm can beat these really customized algorithms.", "tokens": [407, 341, 307, 1566, 472, 2199, 2489, 12, 83, 37726, 9284, 393, 4224, 613, 534, 30581, 14642, 13], "temperature": 0.0, "avg_logprob": -0.232323149560203, "compression_ratio": 1.491891891891892, "no_speech_prob": 4.092898507224163e-06}, {"id": 1459, "seek": 700556, "start": 7016.4400000000005, "end": 7022.68, "text": " And so here's one of the really cool things that Sebastian did, these ablation studies.", "tokens": [400, 370, 510, 311, 472, 295, 264, 534, 1627, 721, 300, 31102, 630, 11, 613, 410, 24278, 5313, 13], "temperature": 0.0, "avg_logprob": -0.232323149560203, "compression_ratio": 1.491891891891892, "no_speech_prob": 4.092898507224163e-06}, {"id": 1460, "seek": 700556, "start": 7022.68, "end": 7027.0, "text": " I was really keen that if we were going to publish a paper, we had to say, why does it", "tokens": [286, 390, 534, 20297, 300, 498, 321, 645, 516, 281, 11374, 257, 3035, 11, 321, 632, 281, 584, 11, 983, 775, 309], "temperature": 0.0, "avg_logprob": -0.232323149560203, "compression_ratio": 1.491891891891892, "no_speech_prob": 4.092898507224163e-06}, {"id": 1461, "seek": 700556, "start": 7027.0, "end": 7029.04, "text": " work?", "tokens": [589, 30], "temperature": 0.0, "avg_logprob": -0.232323149560203, "compression_ratio": 1.491891891891892, "no_speech_prob": 4.092898507224163e-06}, {"id": 1462, "seek": 702904, "start": 7029.04, "end": 7037.6, "text": " So Sebastian went through and tried removing all of those different contributions I mentioned.", "tokens": [407, 31102, 1437, 807, 293, 3031, 12720, 439, 295, 729, 819, 15725, 286, 2835, 13], "temperature": 0.0, "avg_logprob": -0.07680844022082044, "compression_ratio": 1.7362637362637363, "no_speech_prob": 3.966958502132911e-06}, {"id": 1463, "seek": 702904, "start": 7037.6, "end": 7042.38, "text": " So what if we don't use gradual freezing?", "tokens": [407, 437, 498, 321, 500, 380, 764, 32890, 20200, 30], "temperature": 0.0, "avg_logprob": -0.07680844022082044, "compression_ratio": 1.7362637362637363, "no_speech_prob": 3.966958502132911e-06}, {"id": 1464, "seek": 702904, "start": 7042.38, "end": 7044.88, "text": " What if we don't use discriminative learning rates?", "tokens": [708, 498, 321, 500, 380, 764, 20828, 1166, 2539, 6846, 30], "temperature": 0.0, "avg_logprob": -0.07680844022082044, "compression_ratio": 1.7362637362637363, "no_speech_prob": 3.966958502132911e-06}, {"id": 1465, "seek": 702904, "start": 7044.88, "end": 7048.92, "text": " What if instead of discriminative learning rates, we use cosine annealing?", "tokens": [708, 498, 2602, 295, 20828, 1166, 2539, 6846, 11, 321, 764, 23565, 22256, 4270, 30], "temperature": 0.0, "avg_logprob": -0.07680844022082044, "compression_ratio": 1.7362637362637363, "no_speech_prob": 3.966958502132911e-06}, {"id": 1466, "seek": 702904, "start": 7048.92, "end": 7057.36, "text": " What if we don't do any pre-training with Wikipedia?", "tokens": [708, 498, 321, 500, 380, 360, 604, 659, 12, 17227, 1760, 365, 28999, 30], "temperature": 0.0, "avg_logprob": -0.07680844022082044, "compression_ratio": 1.7362637362637363, "no_speech_prob": 3.966958502132911e-06}, {"id": 1467, "seek": 705736, "start": 7057.36, "end": 7060.599999999999, "text": " What if we don't do any fine-tuning?", "tokens": [708, 498, 321, 500, 380, 360, 604, 2489, 12, 83, 37726, 30], "temperature": 0.0, "avg_logprob": -0.15618803766038683, "compression_ratio": 1.470899470899471, "no_speech_prob": 1.3081704537398764e-06}, {"id": 1468, "seek": 705736, "start": 7060.599999999999, "end": 7066.599999999999, "text": " And then the really interesting one to me was, what's the validation error rate on IMDB", "tokens": [400, 550, 264, 534, 1880, 472, 281, 385, 390, 11, 437, 311, 264, 24071, 6713, 3314, 322, 21463, 27735], "temperature": 0.0, "avg_logprob": -0.15618803766038683, "compression_ratio": 1.470899470899471, "no_speech_prob": 1.3081704537398764e-06}, {"id": 1469, "seek": 705736, "start": 7066.599999999999, "end": 7070.96, "text": " if we only use 100 training examples, versus 200 versus 500?", "tokens": [498, 321, 787, 764, 2319, 3097, 5110, 11, 5717, 2331, 5717, 5923, 30], "temperature": 0.0, "avg_logprob": -0.15618803766038683, "compression_ratio": 1.470899470899471, "no_speech_prob": 1.3081704537398764e-06}, {"id": 1470, "seek": 705736, "start": 7070.96, "end": 7081.16, "text": " And you can see, very interestingly, the full version of this approach is nearly as accurate", "tokens": [400, 291, 393, 536, 11, 588, 25873, 11, 264, 1577, 3037, 295, 341, 3109, 307, 6217, 382, 8559], "temperature": 0.0, "avg_logprob": -0.15618803766038683, "compression_ratio": 1.470899470899471, "no_speech_prob": 1.3081704537398764e-06}, {"id": 1471, "seek": 708116, "start": 7081.16, "end": 7089.48, "text": " on just 100 training examples, like it's still very accurate versus all 20,000 training examples.", "tokens": [322, 445, 2319, 3097, 5110, 11, 411, 309, 311, 920, 588, 8559, 5717, 439, 945, 11, 1360, 3097, 5110, 13], "temperature": 0.0, "avg_logprob": -0.23159341992072338, "compression_ratio": 1.625, "no_speech_prob": 2.0261338704585796e-06}, {"id": 1472, "seek": 708116, "start": 7089.48, "end": 7094.5599999999995, "text": " Where else if you're training from scratch on 100, it's almost random.", "tokens": [2305, 1646, 498, 291, 434, 3097, 490, 8459, 322, 2319, 11, 309, 311, 1920, 4974, 13], "temperature": 0.0, "avg_logprob": -0.23159341992072338, "compression_ratio": 1.625, "no_speech_prob": 2.0261338704585796e-06}, {"id": 1473, "seek": 708116, "start": 7094.5599999999995, "end": 7096.599999999999, "text": " So it's what I expected.", "tokens": [407, 309, 311, 437, 286, 5176, 13], "temperature": 0.0, "avg_logprob": -0.23159341992072338, "compression_ratio": 1.625, "no_speech_prob": 2.0261338704585796e-06}, {"id": 1474, "seek": 708116, "start": 7096.599999999999, "end": 7102.44, "text": " I said to Sebastian, I really think that this is most beneficial when you don't have much", "tokens": [286, 848, 281, 31102, 11, 286, 534, 519, 300, 341, 307, 881, 14072, 562, 291, 500, 380, 362, 709], "temperature": 0.0, "avg_logprob": -0.23159341992072338, "compression_ratio": 1.625, "no_speech_prob": 2.0261338704585796e-06}, {"id": 1475, "seek": 708116, "start": 7102.44, "end": 7103.44, "text": " data.", "tokens": [1412, 13], "temperature": 0.0, "avg_logprob": -0.23159341992072338, "compression_ratio": 1.625, "no_speech_prob": 2.0261338704585796e-06}, {"id": 1476, "seek": 708116, "start": 7103.44, "end": 7106.48, "text": " And this is where fast AI is most interested in contributing.", "tokens": [400, 341, 307, 689, 2370, 7318, 307, 881, 3102, 294, 19270, 13], "temperature": 0.0, "avg_logprob": -0.23159341992072338, "compression_ratio": 1.625, "no_speech_prob": 2.0261338704585796e-06}, {"id": 1477, "seek": 708116, "start": 7106.48, "end": 7109.8, "text": " There's small data regimes, small compute regimes, and so forth.", "tokens": [821, 311, 1359, 1412, 45738, 11, 1359, 14722, 45738, 11, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.23159341992072338, "compression_ratio": 1.625, "no_speech_prob": 2.0261338704585796e-06}, {"id": 1478, "seek": 710980, "start": 7109.8, "end": 7113.12, "text": " So he did these studies to check.", "tokens": [407, 415, 630, 613, 5313, 281, 1520, 13], "temperature": 0.0, "avg_logprob": -0.09776285861400848, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.9054475564626046e-06}, {"id": 1479, "seek": 710980, "start": 7113.12, "end": 7122.96, "text": " So I want to show you a couple of tricks as to how you can run these kinds of studies.", "tokens": [407, 286, 528, 281, 855, 291, 257, 1916, 295, 11733, 382, 281, 577, 291, 393, 1190, 613, 3685, 295, 5313, 13], "temperature": 0.0, "avg_logprob": -0.09776285861400848, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.9054475564626046e-06}, {"id": 1480, "seek": 710980, "start": 7122.96, "end": 7129.12, "text": " The first trick is something which I know you're all going to find really handy.", "tokens": [440, 700, 4282, 307, 746, 597, 286, 458, 291, 434, 439, 516, 281, 915, 534, 13239, 13], "temperature": 0.0, "avg_logprob": -0.09776285861400848, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.9054475564626046e-06}, {"id": 1481, "seek": 710980, "start": 7129.12, "end": 7132.68, "text": " I know you've all been annoyed when you're running something in a Jupyter Notebook and", "tokens": [286, 458, 291, 600, 439, 668, 25921, 562, 291, 434, 2614, 746, 294, 257, 22125, 88, 391, 11633, 2939, 293], "temperature": 0.0, "avg_logprob": -0.09776285861400848, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.9054475564626046e-06}, {"id": 1482, "seek": 710980, "start": 7132.68, "end": 7137.76, "text": " you lose your Internet connection for long enough that it decides you've gone away and", "tokens": [291, 3624, 428, 7703, 4984, 337, 938, 1547, 300, 309, 14898, 291, 600, 2780, 1314, 293], "temperature": 0.0, "avg_logprob": -0.09776285861400848, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.9054475564626046e-06}, {"id": 1483, "seek": 713776, "start": 7137.76, "end": 7142.76, "text": " then your session disappears and you have to start it again from scratch.", "tokens": [550, 428, 5481, 25527, 293, 291, 362, 281, 722, 309, 797, 490, 8459, 13], "temperature": 0.0, "avg_logprob": -0.21526797388640928, "compression_ratio": 1.456989247311828, "no_speech_prob": 5.8627588259696495e-06}, {"id": 1484, "seek": 713776, "start": 7142.76, "end": 7145.84, "text": " So what do you do?", "tokens": [407, 437, 360, 291, 360, 30], "temperature": 0.0, "avg_logprob": -0.21526797388640928, "compression_ratio": 1.456989247311828, "no_speech_prob": 5.8627588259696495e-06}, {"id": 1485, "seek": 713776, "start": 7145.84, "end": 7153.04, "text": " There's a very simple cool thing called VNC where basically you can install on your AWS", "tokens": [821, 311, 257, 588, 2199, 1627, 551, 1219, 691, 45, 34, 689, 1936, 291, 393, 3625, 322, 428, 17650], "temperature": 0.0, "avg_logprob": -0.21526797388640928, "compression_ratio": 1.456989247311828, "no_speech_prob": 5.8627588259696495e-06}, {"id": 1486, "seek": 713776, "start": 7153.04, "end": 7162.52, "text": " instance or paper space or whatever, XWindows, a lightweight window manager, a VNC server,", "tokens": [5197, 420, 3035, 1901, 420, 2035, 11, 1783, 45813, 1509, 11, 257, 22052, 4910, 6598, 11, 257, 691, 45, 34, 7154, 11], "temperature": 0.0, "avg_logprob": -0.21526797388640928, "compression_ratio": 1.456989247311828, "no_speech_prob": 5.8627588259696495e-06}, {"id": 1487, "seek": 716252, "start": 7162.52, "end": 7168.56, "text": " a Firefox, a terminal, and some fonts.", "tokens": [257, 46613, 11, 257, 14709, 11, 293, 512, 35316, 13], "temperature": 0.0, "avg_logprob": -0.17898976252629206, "compression_ratio": 1.4387096774193548, "no_speech_prob": 2.9023028673691442e-06}, {"id": 1488, "seek": 716252, "start": 7168.56, "end": 7178.72, "text": " Chuck these lines at the end of your VNC XStartup configuration file and then run this command.", "tokens": [21607, 613, 3876, 412, 264, 917, 295, 428, 691, 45, 34, 1783, 33996, 1010, 11694, 3991, 293, 550, 1190, 341, 5622, 13], "temperature": 0.0, "avg_logprob": -0.17898976252629206, "compression_ratio": 1.4387096774193548, "no_speech_prob": 2.9023028673691442e-06}, {"id": 1489, "seek": 716252, "start": 7178.72, "end": 7192.160000000001, "text": " It's now running a server where you can then run the type VNC viewer, or any VNC viewer,", "tokens": [467, 311, 586, 2614, 257, 7154, 689, 291, 393, 550, 1190, 264, 2010, 691, 45, 34, 16767, 11, 420, 604, 691, 45, 34, 16767, 11], "temperature": 0.0, "avg_logprob": -0.17898976252629206, "compression_ratio": 1.4387096774193548, "no_speech_prob": 2.9023028673691442e-06}, {"id": 1490, "seek": 719216, "start": 7192.16, "end": 7197.44, "text": " on your computer and you point it at your server.", "tokens": [322, 428, 3820, 293, 291, 935, 309, 412, 428, 7154, 13], "temperature": 0.0, "avg_logprob": -0.18998267433860086, "compression_ratio": 1.3275862068965518, "no_speech_prob": 1.873871951829642e-06}, {"id": 1491, "seek": 719216, "start": 7197.44, "end": 7209.96, "text": " But specifically what you do is you use SSH port forwarding to port forward port 5913", "tokens": [583, 4682, 437, 291, 360, 307, 291, 764, 12238, 39, 2436, 2128, 278, 281, 2436, 2128, 2436, 24624, 7668], "temperature": 0.0, "avg_logprob": -0.18998267433860086, "compression_ratio": 1.3275862068965518, "no_speech_prob": 1.873871951829642e-06}, {"id": 1492, "seek": 719216, "start": 7209.96, "end": 7213.8, "text": " to localhost 5913.", "tokens": [281, 2654, 6037, 24624, 7668, 13], "temperature": 0.0, "avg_logprob": -0.18998267433860086, "compression_ratio": 1.3275862068965518, "no_speech_prob": 1.873871951829642e-06}, {"id": 1493, "seek": 721380, "start": 7213.8, "end": 7222.320000000001, "text": " And so then you connect to port 5913 on localhost.", "tokens": [400, 370, 550, 291, 1745, 281, 2436, 24624, 7668, 322, 2654, 6037, 13], "temperature": 0.0, "avg_logprob": -0.14441503622593024, "compression_ratio": 1.497175141242938, "no_speech_prob": 1.577959665155504e-06}, {"id": 1494, "seek": 721380, "start": 7222.320000000001, "end": 7230.4400000000005, "text": " It will send it off to port 5913 on your server, which is the VNC port, and it will display", "tokens": [467, 486, 2845, 309, 766, 281, 2436, 24624, 7668, 322, 428, 7154, 11, 597, 307, 264, 691, 45, 34, 2436, 11, 293, 309, 486, 4674], "temperature": 0.0, "avg_logprob": -0.14441503622593024, "compression_ratio": 1.497175141242938, "no_speech_prob": 1.577959665155504e-06}, {"id": 1495, "seek": 721380, "start": 7230.4400000000005, "end": 7232.4800000000005, "text": " an XWindows desktop.", "tokens": [364, 1783, 45813, 1509, 14502, 13], "temperature": 0.0, "avg_logprob": -0.14441503622593024, "compression_ratio": 1.497175141242938, "no_speech_prob": 1.577959665155504e-06}, {"id": 1496, "seek": 721380, "start": 7232.4800000000005, "end": 7237.4400000000005, "text": " And then you can click on the Linux Start Like button and click on Firefox, and you", "tokens": [400, 550, 291, 393, 2052, 322, 264, 18734, 6481, 1743, 2960, 293, 2052, 322, 46613, 11, 293, 291], "temperature": 0.0, "avg_logprob": -0.14441503622593024, "compression_ratio": 1.497175141242938, "no_speech_prob": 1.577959665155504e-06}, {"id": 1497, "seek": 721380, "start": 7237.4400000000005, "end": 7238.92, "text": " now have Firefox.", "tokens": [586, 362, 46613, 13], "temperature": 0.0, "avg_logprob": -0.14441503622593024, "compression_ratio": 1.497175141242938, "no_speech_prob": 1.577959665155504e-06}, {"id": 1498, "seek": 723892, "start": 7238.92, "end": 7246.68, "text": " And you'll see here in Firefox it says localhost because this Firefox is running on my AWS", "tokens": [400, 291, 603, 536, 510, 294, 46613, 309, 1619, 2654, 6037, 570, 341, 46613, 307, 2614, 322, 452, 17650], "temperature": 0.0, "avg_logprob": -0.18696717862729673, "compression_ratio": 1.6864406779661016, "no_speech_prob": 1.228916403306357e-06}, {"id": 1499, "seek": 723892, "start": 7246.68, "end": 7247.68, "text": " server.", "tokens": [7154, 13], "temperature": 0.0, "avg_logprob": -0.18696717862729673, "compression_ratio": 1.6864406779661016, "no_speech_prob": 1.228916403306357e-06}, {"id": 1500, "seek": 723892, "start": 7247.68, "end": 7253.72, "text": " So you now run Firefox, you start your thing running, and then you close your VNC viewer,", "tokens": [407, 291, 586, 1190, 46613, 11, 291, 722, 428, 551, 2614, 11, 293, 550, 291, 1998, 428, 691, 45, 34, 16767, 11], "temperature": 0.0, "avg_logprob": -0.18696717862729673, "compression_ratio": 1.6864406779661016, "no_speech_prob": 1.228916403306357e-06}, {"id": 1501, "seek": 723892, "start": 7253.72, "end": 7259.68, "text": " remembering that Firefox is displaying on this virtual VNC display, not on the real", "tokens": [20719, 300, 46613, 307, 36834, 322, 341, 6374, 691, 45, 34, 4674, 11, 406, 322, 264, 957], "temperature": 0.0, "avg_logprob": -0.18696717862729673, "compression_ratio": 1.6864406779661016, "no_speech_prob": 1.228916403306357e-06}, {"id": 1502, "seek": 723892, "start": 7259.68, "end": 7260.68, "text": " display.", "tokens": [4674, 13], "temperature": 0.0, "avg_logprob": -0.18696717862729673, "compression_ratio": 1.6864406779661016, "no_speech_prob": 1.228916403306357e-06}, {"id": 1503, "seek": 723892, "start": 7260.68, "end": 7265.32, "text": " And so then later on that day, you log back into VNC viewer and it pops up again.", "tokens": [400, 370, 550, 1780, 322, 300, 786, 11, 291, 3565, 646, 666, 691, 45, 34, 16767, 293, 309, 16795, 493, 797, 13], "temperature": 0.0, "avg_logprob": -0.18696717862729673, "compression_ratio": 1.6864406779661016, "no_speech_prob": 1.228916403306357e-06}, {"id": 1504, "seek": 723892, "start": 7265.32, "end": 7268.32, "text": " So it's like a persistent desktop.", "tokens": [407, 309, 311, 411, 257, 24315, 14502, 13], "temperature": 0.0, "avg_logprob": -0.18696717862729673, "compression_ratio": 1.6864406779661016, "no_speech_prob": 1.228916403306357e-06}, {"id": 1505, "seek": 726832, "start": 7268.32, "end": 7269.719999999999, "text": " And it's shockingly fast.", "tokens": [400, 309, 311, 5588, 12163, 2370, 13], "temperature": 0.0, "avg_logprob": -0.16107643975151908, "compression_ratio": 1.4339622641509433, "no_speech_prob": 1.6964273527264595e-05}, {"id": 1506, "seek": 726832, "start": 7269.719999999999, "end": 7271.4, "text": " It works really well.", "tokens": [467, 1985, 534, 731, 13], "temperature": 0.0, "avg_logprob": -0.16107643975151908, "compression_ratio": 1.4339622641509433, "no_speech_prob": 1.6964273527264595e-05}, {"id": 1507, "seek": 726832, "start": 7271.4, "end": 7274.08, "text": " So there's trick number one.", "tokens": [407, 456, 311, 4282, 1230, 472, 13], "temperature": 0.0, "avg_logprob": -0.16107643975151908, "compression_ratio": 1.4339622641509433, "no_speech_prob": 1.6964273527264595e-05}, {"id": 1508, "seek": 726832, "start": 7274.08, "end": 7278.92, "text": " And there's lots of different VNC servers and clients and whatever, but this one worked", "tokens": [400, 456, 311, 3195, 295, 819, 691, 45, 34, 15909, 293, 6982, 293, 2035, 11, 457, 341, 472, 2732], "temperature": 0.0, "avg_logprob": -0.16107643975151908, "compression_ratio": 1.4339622641509433, "no_speech_prob": 1.6964273527264595e-05}, {"id": 1509, "seek": 726832, "start": 7278.92, "end": 7282.219999999999, "text": " fine for me.", "tokens": [2489, 337, 385, 13], "temperature": 0.0, "avg_logprob": -0.16107643975151908, "compression_ratio": 1.4339622641509433, "no_speech_prob": 1.6964273527264595e-05}, {"id": 1510, "seek": 726832, "start": 7282.219999999999, "end": 7288.0, "text": " So you can see here I connect to localhost 5913.", "tokens": [407, 291, 393, 536, 510, 286, 1745, 281, 2654, 6037, 24624, 7668, 13], "temperature": 0.0, "avg_logprob": -0.16107643975151908, "compression_ratio": 1.4339622641509433, "no_speech_prob": 1.6964273527264595e-05}, {"id": 1511, "seek": 726832, "start": 7288.0, "end": 7293.04, "text": " Trick number two is to create Python scripts.", "tokens": [43367, 1230, 732, 307, 281, 1884, 15329, 23294, 13], "temperature": 0.0, "avg_logprob": -0.16107643975151908, "compression_ratio": 1.4339622641509433, "no_speech_prob": 1.6964273527264595e-05}, {"id": 1512, "seek": 726832, "start": 7293.04, "end": 7295.0, "text": " This is what we ended up doing.", "tokens": [639, 307, 437, 321, 4590, 493, 884, 13], "temperature": 0.0, "avg_logprob": -0.16107643975151908, "compression_ratio": 1.4339622641509433, "no_speech_prob": 1.6964273527264595e-05}, {"id": 1513, "seek": 729500, "start": 7295.0, "end": 7300.0, "text": " So I ended up creating a little Python script for Sebastian to say this is the basic steps", "tokens": [407, 286, 4590, 493, 4084, 257, 707, 15329, 5755, 337, 31102, 281, 584, 341, 307, 264, 3875, 4439], "temperature": 0.0, "avg_logprob": -0.15879309804815994, "compression_ratio": 1.8243243243243243, "no_speech_prob": 5.33811362402048e-06}, {"id": 1514, "seek": 729500, "start": 7300.0, "end": 7303.32, "text": " you need to do, and now you need to create different versions for everything else.", "tokens": [291, 643, 281, 360, 11, 293, 586, 291, 643, 281, 1884, 819, 9606, 337, 1203, 1646, 13], "temperature": 0.0, "avg_logprob": -0.15879309804815994, "compression_ratio": 1.8243243243243243, "no_speech_prob": 5.33811362402048e-06}, {"id": 1515, "seek": 729500, "start": 7303.32, "end": 7307.16, "text": " And I suggested to him that he try using this thing called Google Fire.", "tokens": [400, 286, 10945, 281, 796, 300, 415, 853, 1228, 341, 551, 1219, 3329, 7652, 13], "temperature": 0.0, "avg_logprob": -0.15879309804815994, "compression_ratio": 1.8243243243243243, "no_speech_prob": 5.33811362402048e-06}, {"id": 1516, "seek": 729500, "start": 7307.16, "end": 7313.12, "text": " What Google Fire does is you create a function with shitloads of parameters.", "tokens": [708, 3329, 7652, 775, 307, 291, 1884, 257, 2445, 365, 4611, 2907, 82, 295, 9834, 13], "temperature": 0.0, "avg_logprob": -0.15879309804815994, "compression_ratio": 1.8243243243243243, "no_speech_prob": 5.33811362402048e-06}, {"id": 1517, "seek": 729500, "start": 7313.12, "end": 7316.14, "text": " And so these are all the things that Sebastian wanted to try doing.", "tokens": [400, 370, 613, 366, 439, 264, 721, 300, 31102, 1415, 281, 853, 884, 13], "temperature": 0.0, "avg_logprob": -0.15879309804815994, "compression_ratio": 1.8243243243243243, "no_speech_prob": 5.33811362402048e-06}, {"id": 1518, "seek": 729500, "start": 7316.14, "end": 7320.4, "text": " Different dropout amounts, different learning rates, do I use pre-training or not, do I", "tokens": [20825, 3270, 346, 11663, 11, 819, 2539, 6846, 11, 360, 286, 764, 659, 12, 17227, 1760, 420, 406, 11, 360, 286], "temperature": 0.0, "avg_logprob": -0.15879309804815994, "compression_ratio": 1.8243243243243243, "no_speech_prob": 5.33811362402048e-06}, {"id": 1519, "seek": 729500, "start": 7320.4, "end": 7324.56, "text": " use CLR or not, do I use discriminative learning rate or not.", "tokens": [764, 12855, 49, 420, 406, 11, 360, 286, 764, 20828, 1166, 2539, 3314, 420, 406, 13], "temperature": 0.0, "avg_logprob": -0.15879309804815994, "compression_ratio": 1.8243243243243243, "no_speech_prob": 5.33811362402048e-06}, {"id": 1520, "seek": 732456, "start": 7324.56, "end": 7326.68, "text": " Do I go backwards or not, blah blah blah.", "tokens": [1144, 286, 352, 12204, 420, 406, 11, 12288, 12288, 12288, 13], "temperature": 0.0, "avg_logprob": -0.22930988512541117, "compression_ratio": 1.6889952153110048, "no_speech_prob": 1.7231399397132918e-05}, {"id": 1521, "seek": 732456, "start": 7326.68, "end": 7332.0, "text": " So you create a function and then you add something saying if name equals main fire.fire and the", "tokens": [407, 291, 1884, 257, 2445, 293, 550, 291, 909, 746, 1566, 498, 1315, 6915, 2135, 2610, 13, 12037, 293, 264], "temperature": 0.0, "avg_logprob": -0.22930988512541117, "compression_ratio": 1.6889952153110048, "no_speech_prob": 1.7231399397132918e-05}, {"id": 1522, "seek": 732456, "start": 7332.0, "end": 7333.0, "text": " function name.", "tokens": [2445, 1315, 13], "temperature": 0.0, "avg_logprob": -0.22930988512541117, "compression_ratio": 1.6889952153110048, "no_speech_prob": 1.7231399397132918e-05}, {"id": 1523, "seek": 732456, "start": 7333.0, "end": 7334.6, "text": " You do nothing else at all.", "tokens": [509, 360, 1825, 1646, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.22930988512541117, "compression_ratio": 1.6889952153110048, "no_speech_prob": 1.7231399397132918e-05}, {"id": 1524, "seek": 732456, "start": 7334.6, "end": 7338.360000000001, "text": " You don't have to add any metadata, any doc strings, anything at all.", "tokens": [509, 500, 380, 362, 281, 909, 604, 26603, 11, 604, 3211, 13985, 11, 1340, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.22930988512541117, "compression_ratio": 1.6889952153110048, "no_speech_prob": 1.7231399397132918e-05}, {"id": 1525, "seek": 732456, "start": 7338.360000000001, "end": 7344.96, "text": " And you then call that script and automatically you now have a command line interface.", "tokens": [400, 291, 550, 818, 300, 5755, 293, 6772, 291, 586, 362, 257, 5622, 1622, 9226, 13], "temperature": 0.0, "avg_logprob": -0.22930988512541117, "compression_ratio": 1.6889952153110048, "no_speech_prob": 1.7231399397132918e-05}, {"id": 1526, "seek": 732456, "start": 7344.96, "end": 7347.080000000001, "text": " And that's it.", "tokens": [400, 300, 311, 309, 13], "temperature": 0.0, "avg_logprob": -0.22930988512541117, "compression_ratio": 1.6889952153110048, "no_speech_prob": 1.7231399397132918e-05}, {"id": 1527, "seek": 734708, "start": 7347.08, "end": 7354.76, "text": " So that's a super fantastic easy way to run lots of different variations in a terminal.", "tokens": [407, 300, 311, 257, 1687, 5456, 1858, 636, 281, 1190, 3195, 295, 819, 17840, 294, 257, 14709, 13], "temperature": 0.0, "avg_logprob": -0.1291244626045227, "compression_ratio": 1.5380116959064327, "no_speech_prob": 9.422423659088963e-07}, {"id": 1528, "seek": 734708, "start": 7354.76, "end": 7360.96, "text": " And this ends up being easier if you want to do lots of variations than using a notebook", "tokens": [400, 341, 5314, 493, 885, 3571, 498, 291, 528, 281, 360, 3195, 295, 17840, 813, 1228, 257, 21060], "temperature": 0.0, "avg_logprob": -0.1291244626045227, "compression_ratio": 1.5380116959064327, "no_speech_prob": 9.422423659088963e-07}, {"id": 1529, "seek": 734708, "start": 7360.96, "end": 7368.24, "text": " because you can just have a bash script that tries all of them and spits them all out.", "tokens": [570, 291, 393, 445, 362, 257, 46183, 5755, 300, 9898, 439, 295, 552, 293, 637, 1208, 552, 439, 484, 13], "temperature": 0.0, "avg_logprob": -0.1291244626045227, "compression_ratio": 1.5380116959064327, "no_speech_prob": 9.422423659088963e-07}, {"id": 1530, "seek": 736824, "start": 7368.24, "end": 7377.92, "text": " You'll find inside the DL2 course directory there's now something called imdb-scripts", "tokens": [509, 603, 915, 1854, 264, 413, 43, 17, 1164, 21120, 456, 311, 586, 746, 1219, 566, 67, 65, 12, 82, 5944, 82], "temperature": 0.0, "avg_logprob": -0.17804401974345363, "compression_ratio": 1.6467661691542288, "no_speech_prob": 7.1831918830866925e-06}, {"id": 1531, "seek": 736824, "start": 7377.92, "end": 7382.82, "text": " and I've put there all of the scripts that Sebastian and I used.", "tokens": [293, 286, 600, 829, 456, 439, 295, 264, 23294, 300, 31102, 293, 286, 1143, 13], "temperature": 0.0, "avg_logprob": -0.17804401974345363, "compression_ratio": 1.6467661691542288, "no_speech_prob": 7.1831918830866925e-06}, {"id": 1532, "seek": 736824, "start": 7382.82, "end": 7390.48, "text": " So you'll see, because we needed to like tokenize every single dataset, we had to turn every", "tokens": [407, 291, 603, 536, 11, 570, 321, 2978, 281, 411, 14862, 1125, 633, 2167, 28872, 11, 321, 632, 281, 1261, 633], "temperature": 0.0, "avg_logprob": -0.17804401974345363, "compression_ratio": 1.6467661691542288, "no_speech_prob": 7.1831918830866925e-06}, {"id": 1533, "seek": 736824, "start": 7390.48, "end": 7395.24, "text": " dataset, numericalize every dataset, we had to train a language model on every dataset,", "tokens": [28872, 11, 29054, 1125, 633, 28872, 11, 321, 632, 281, 3847, 257, 2856, 2316, 322, 633, 28872, 11], "temperature": 0.0, "avg_logprob": -0.17804401974345363, "compression_ratio": 1.6467661691542288, "no_speech_prob": 7.1831918830866925e-06}, {"id": 1534, "seek": 739524, "start": 7395.24, "end": 7398.44, "text": " we had to train a classified every dataset, we had to do all of those things in a variety", "tokens": [321, 632, 281, 3847, 257, 20627, 633, 28872, 11, 321, 632, 281, 360, 439, 295, 729, 721, 294, 257, 5673], "temperature": 0.0, "avg_logprob": -0.17857596453498392, "compression_ratio": 1.7234042553191489, "no_speech_prob": 8.397993951803073e-06}, {"id": 1535, "seek": 739524, "start": 7398.44, "end": 7401.98, "text": " of different ways to compare them, we had a script for all those things.", "tokens": [295, 819, 2098, 281, 6794, 552, 11, 321, 632, 257, 5755, 337, 439, 729, 721, 13], "temperature": 0.0, "avg_logprob": -0.17857596453498392, "compression_ratio": 1.7234042553191489, "no_speech_prob": 8.397993951803073e-06}, {"id": 1536, "seek": 739524, "start": 7401.98, "end": 7412.5, "text": " So you can check out and see all of the scripts that we used.", "tokens": [407, 291, 393, 1520, 484, 293, 536, 439, 295, 264, 23294, 300, 321, 1143, 13], "temperature": 0.0, "avg_logprob": -0.17857596453498392, "compression_ratio": 1.7234042553191489, "no_speech_prob": 8.397993951803073e-06}, {"id": 1537, "seek": 739524, "start": 7412.5, "end": 7417.44, "text": " When you're doing a lot of scripts and stuff, they've got different code all over the place,", "tokens": [1133, 291, 434, 884, 257, 688, 295, 23294, 293, 1507, 11, 436, 600, 658, 819, 3089, 439, 670, 264, 1081, 11], "temperature": 0.0, "avg_logprob": -0.17857596453498392, "compression_ratio": 1.7234042553191489, "no_speech_prob": 8.397993951803073e-06}, {"id": 1538, "seek": 739524, "start": 7417.44, "end": 7423.48, "text": " eventually it might get frustrating that you don't want to symlink your fast.io library", "tokens": [4728, 309, 1062, 483, 16522, 300, 291, 500, 380, 528, 281, 6697, 22473, 428, 2370, 13, 1004, 6405], "temperature": 0.0, "avg_logprob": -0.17857596453498392, "compression_ratio": 1.7234042553191489, "no_speech_prob": 8.397993951803073e-06}, {"id": 1539, "seek": 742348, "start": 7423.48, "end": 7428.48, "text": " again and again, but you probably don't want to pip install it because that version tends", "tokens": [797, 293, 797, 11, 457, 291, 1391, 500, 380, 528, 281, 8489, 3625, 309, 570, 300, 3037, 12258], "temperature": 0.0, "avg_logprob": -0.17182402775205416, "compression_ratio": 1.6476190476190475, "no_speech_prob": 8.01339319878025e-06}, {"id": 1540, "seek": 742348, "start": 7428.48, "end": 7434.16, "text": " to be a little bit old, we move so fast, you want to use the current version in Git.", "tokens": [281, 312, 257, 707, 857, 1331, 11, 321, 1286, 370, 2370, 11, 291, 528, 281, 764, 264, 2190, 3037, 294, 16939, 13], "temperature": 0.0, "avg_logprob": -0.17182402775205416, "compression_ratio": 1.6476190476190475, "no_speech_prob": 8.01339319878025e-06}, {"id": 1541, "seek": 742348, "start": 7434.16, "end": 7442.32, "text": " If you say pip install minus e dot from the fastai-repo base, it does something quite", "tokens": [759, 291, 584, 8489, 3625, 3175, 308, 5893, 490, 264, 2370, 1301, 12, 265, 2259, 3096, 11, 309, 775, 746, 1596], "temperature": 0.0, "avg_logprob": -0.17182402775205416, "compression_ratio": 1.6476190476190475, "no_speech_prob": 8.01339319878025e-06}, {"id": 1542, "seek": 742348, "start": 7442.32, "end": 7449.799999999999, "text": " neat which basically creates a symlink to the fastai library in your Git installation", "tokens": [10654, 597, 1936, 7829, 257, 6697, 22473, 281, 264, 2370, 1301, 6405, 294, 428, 16939, 13260], "temperature": 0.0, "avg_logprob": -0.17182402775205416, "compression_ratio": 1.6476190476190475, "no_speech_prob": 8.01339319878025e-06}, {"id": 1543, "seek": 744980, "start": 7449.8, "end": 7455.56, "text": " right here inside your site packages directory.", "tokens": [558, 510, 1854, 428, 3621, 17401, 21120, 13], "temperature": 0.0, "avg_logprob": -0.14548840522766113, "compression_ratio": 1.66, "no_speech_prob": 3.2377463412558427e-06}, {"id": 1544, "seek": 744980, "start": 7455.56, "end": 7460.92, "text": " Your site packages directory is like your main Python library.", "tokens": [2260, 3621, 17401, 21120, 307, 411, 428, 2135, 15329, 6405, 13], "temperature": 0.0, "avg_logprob": -0.14548840522766113, "compression_ratio": 1.66, "no_speech_prob": 3.2377463412558427e-06}, {"id": 1545, "seek": 744980, "start": 7460.92, "end": 7468.08, "text": " And so if you do this, you can then access fastai from anywhere, but every time you do", "tokens": [400, 370, 498, 291, 360, 341, 11, 291, 393, 550, 2105, 2370, 1301, 490, 4992, 11, 457, 633, 565, 291, 360], "temperature": 0.0, "avg_logprob": -0.14548840522766113, "compression_ratio": 1.66, "no_speech_prob": 3.2377463412558427e-06}, {"id": 1546, "seek": 744980, "start": 7468.08, "end": 7471.92, "text": " git pull, you've got the most recent version.", "tokens": [18331, 2235, 11, 291, 600, 658, 264, 881, 5162, 3037, 13], "temperature": 0.0, "avg_logprob": -0.14548840522766113, "compression_ratio": 1.66, "no_speech_prob": 3.2377463412558427e-06}, {"id": 1547, "seek": 744980, "start": 7471.92, "end": 7478.14, "text": " One downside of this is that it installs any updated versions of packages from pip which", "tokens": [1485, 25060, 295, 341, 307, 300, 309, 3625, 82, 604, 10588, 9606, 295, 17401, 490, 8489, 597], "temperature": 0.0, "avg_logprob": -0.14548840522766113, "compression_ratio": 1.66, "no_speech_prob": 3.2377463412558427e-06}, {"id": 1548, "seek": 747814, "start": 7478.14, "end": 7482.14, "text": " can kind of confuse conda a little bit.", "tokens": [393, 733, 295, 28584, 2224, 64, 257, 707, 857, 13], "temperature": 0.0, "avg_logprob": -0.1613392114639282, "compression_ratio": 1.598984771573604, "no_speech_prob": 3.6119627111474983e-06}, {"id": 1549, "seek": 747814, "start": 7482.14, "end": 7491.04, "text": " So another alternative here is just to symlink the fastai library to your site packages library.", "tokens": [407, 1071, 8535, 510, 307, 445, 281, 6697, 22473, 264, 2370, 1301, 6405, 281, 428, 3621, 17401, 6405, 13], "temperature": 0.0, "avg_logprob": -0.1613392114639282, "compression_ratio": 1.598984771573604, "no_speech_prob": 3.6119627111474983e-06}, {"id": 1550, "seek": 747814, "start": 7491.04, "end": 7493.4800000000005, "text": " That works just as well.", "tokens": [663, 1985, 445, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.1613392114639282, "compression_ratio": 1.598984771573604, "no_speech_prob": 3.6119627111474983e-06}, {"id": 1551, "seek": 747814, "start": 7493.4800000000005, "end": 7500.04, "text": " Then you can use fastai again from anywhere, and it's quite handy when you want to run", "tokens": [1396, 291, 393, 764, 2370, 1301, 797, 490, 4992, 11, 293, 309, 311, 1596, 13239, 562, 291, 528, 281, 1190], "temperature": 0.0, "avg_logprob": -0.1613392114639282, "compression_ratio": 1.598984771573604, "no_speech_prob": 3.6119627111474983e-06}, {"id": 1552, "seek": 747814, "start": 7500.04, "end": 7507.4400000000005, "text": " scripts that use fastai from different directories on your system.", "tokens": [23294, 300, 764, 2370, 1301, 490, 819, 5391, 530, 322, 428, 1185, 13], "temperature": 0.0, "avg_logprob": -0.1613392114639282, "compression_ratio": 1.598984771573604, "no_speech_prob": 3.6119627111474983e-06}, {"id": 1553, "seek": 750744, "start": 7507.44, "end": 7517.679999999999, "text": " So one more thing before we go, which is something you can try if you like.", "tokens": [407, 472, 544, 551, 949, 321, 352, 11, 597, 307, 746, 291, 393, 853, 498, 291, 411, 13], "temperature": 0.0, "avg_logprob": -0.10773418499873234, "compression_ratio": 1.3984962406015038, "no_speech_prob": 4.785034434462432e-06}, {"id": 1554, "seek": 750744, "start": 7517.679999999999, "end": 7523.5199999999995, "text": " You don't have to tokenize words.", "tokens": [509, 500, 380, 362, 281, 14862, 1125, 2283, 13], "temperature": 0.0, "avg_logprob": -0.10773418499873234, "compression_ratio": 1.3984962406015038, "no_speech_prob": 4.785034434462432e-06}, {"id": 1555, "seek": 750744, "start": 7523.5199999999995, "end": 7529.2, "text": " Instead of tokenizing words, you can tokenize what are called subword units.", "tokens": [7156, 295, 14862, 3319, 2283, 11, 291, 393, 14862, 1125, 437, 366, 1219, 1422, 7462, 6815, 13], "temperature": 0.0, "avg_logprob": -0.10773418499873234, "compression_ratio": 1.3984962406015038, "no_speech_prob": 4.785034434462432e-06}, {"id": 1556, "seek": 752920, "start": 7529.2, "end": 7540.5599999999995, "text": " And so for example, unsupervised could be tokenized as unsupervised.", "tokens": [400, 370, 337, 1365, 11, 2693, 12879, 24420, 727, 312, 14862, 1602, 382, 2693, 12879, 24420, 13], "temperature": 0.0, "avg_logprob": -0.1429434067163712, "compression_ratio": 1.7435897435897436, "no_speech_prob": 3.3405212889192626e-06}, {"id": 1557, "seek": 752920, "start": 7540.5599999999995, "end": 7545.32, "text": " Tokenizer could be tokenized as tokenizer.", "tokens": [314, 8406, 6545, 727, 312, 14862, 1602, 382, 14862, 6545, 13], "temperature": 0.0, "avg_logprob": -0.1429434067163712, "compression_ratio": 1.7435897435897436, "no_speech_prob": 3.3405212889192626e-06}, {"id": 1558, "seek": 752920, "start": 7545.32, "end": 7549.84, "text": " And then you can do the same thing, the language model that works on subword units, the classifier", "tokens": [400, 550, 291, 393, 360, 264, 912, 551, 11, 264, 2856, 2316, 300, 1985, 322, 1422, 7462, 6815, 11, 264, 1508, 9902], "temperature": 0.0, "avg_logprob": -0.1429434067163712, "compression_ratio": 1.7435897435897436, "no_speech_prob": 3.3405212889192626e-06}, {"id": 1559, "seek": 752920, "start": 7549.84, "end": 7555.76, "text": " that works on subword units, etc.", "tokens": [300, 1985, 322, 1422, 7462, 6815, 11, 5183, 13], "temperature": 0.0, "avg_logprob": -0.1429434067163712, "compression_ratio": 1.7435897435897436, "no_speech_prob": 3.3405212889192626e-06}, {"id": 1560, "seek": 752920, "start": 7555.76, "end": 7558.5599999999995, "text": " So how well does that work?", "tokens": [407, 577, 731, 775, 300, 589, 30], "temperature": 0.0, "avg_logprob": -0.1429434067163712, "compression_ratio": 1.7435897435897436, "no_speech_prob": 3.3405212889192626e-06}, {"id": 1561, "seek": 755856, "start": 7558.56, "end": 7565.4400000000005, "text": " I started playing with it, and with not too much playing, I was getting classification", "tokens": [286, 1409, 2433, 365, 309, 11, 293, 365, 406, 886, 709, 2433, 11, 286, 390, 1242, 21538], "temperature": 0.0, "avg_logprob": -0.11359678555841315, "compression_ratio": 1.5815217391304348, "no_speech_prob": 1.0129941074410453e-05}, {"id": 1562, "seek": 755856, "start": 7565.4400000000005, "end": 7570.280000000001, "text": " results that were nearly as good as using word-level tokenization.", "tokens": [3542, 300, 645, 6217, 382, 665, 382, 1228, 1349, 12, 12418, 14862, 2144, 13], "temperature": 0.0, "avg_logprob": -0.11359678555841315, "compression_ratio": 1.5815217391304348, "no_speech_prob": 1.0129941074410453e-05}, {"id": 1563, "seek": 755856, "start": 7570.280000000001, "end": 7574.84, "text": " Not quite as good, but nearly as good.", "tokens": [1726, 1596, 382, 665, 11, 457, 6217, 382, 665, 13], "temperature": 0.0, "avg_logprob": -0.11359678555841315, "compression_ratio": 1.5815217391304348, "no_speech_prob": 1.0129941074410453e-05}, {"id": 1564, "seek": 755856, "start": 7574.84, "end": 7581.04, "text": " I suspect with more careful thinking and playing around, maybe I could have got as good or", "tokens": [286, 9091, 365, 544, 5026, 1953, 293, 2433, 926, 11, 1310, 286, 727, 362, 658, 382, 665, 420], "temperature": 0.0, "avg_logprob": -0.11359678555841315, "compression_ratio": 1.5815217391304348, "no_speech_prob": 1.0129941074410453e-05}, {"id": 1565, "seek": 755856, "start": 7581.04, "end": 7582.52, "text": " better.", "tokens": [1101, 13], "temperature": 0.0, "avg_logprob": -0.11359678555841315, "compression_ratio": 1.5815217391304348, "no_speech_prob": 1.0129941074410453e-05}, {"id": 1566, "seek": 758252, "start": 7582.52, "end": 7594.280000000001, "text": " But even if I couldn't, if you create a subword unit wiki text model, then IMDB model, language", "tokens": [583, 754, 498, 286, 2809, 380, 11, 498, 291, 1884, 257, 1422, 7462, 4985, 261, 9850, 2487, 2316, 11, 550, 21463, 27735, 2316, 11, 2856], "temperature": 0.0, "avg_logprob": -0.15874036153157553, "compression_ratio": 1.688118811881188, "no_speech_prob": 7.002134339018085e-07}, {"id": 1567, "seek": 758252, "start": 7594.280000000001, "end": 7600.320000000001, "text": " model, and then classifier, forwards and backwards for subword units, and then ensemble it with", "tokens": [2316, 11, 293, 550, 1508, 9902, 11, 30126, 293, 12204, 337, 1422, 7462, 6815, 11, 293, 550, 19492, 309, 365], "temperature": 0.0, "avg_logprob": -0.15874036153157553, "compression_ratio": 1.688118811881188, "no_speech_prob": 7.002134339018085e-07}, {"id": 1568, "seek": 758252, "start": 7600.320000000001, "end": 7606.240000000001, "text": " the forwards and backwards word-level ones, you should be able to beat us.", "tokens": [264, 30126, 293, 12204, 1349, 12, 12418, 2306, 11, 291, 820, 312, 1075, 281, 4224, 505, 13], "temperature": 0.0, "avg_logprob": -0.15874036153157553, "compression_ratio": 1.688118811881188, "no_speech_prob": 7.002134339018085e-07}, {"id": 1569, "seek": 758252, "start": 7606.240000000001, "end": 7612.160000000001, "text": " So here's an approach you may be able to beat our state-of-the-art result.", "tokens": [407, 510, 311, 364, 3109, 291, 815, 312, 1075, 281, 4224, 527, 1785, 12, 2670, 12, 3322, 12, 446, 1874, 13], "temperature": 0.0, "avg_logprob": -0.15874036153157553, "compression_ratio": 1.688118811881188, "no_speech_prob": 7.002134339018085e-07}, {"id": 1570, "seek": 761216, "start": 7612.16, "end": 7622.32, "text": " Google has a project called Sentence Piece, which actually uses a neural net to figure", "tokens": [3329, 575, 257, 1716, 1219, 23652, 655, 42868, 11, 597, 767, 4960, 257, 18161, 2533, 281, 2573], "temperature": 0.0, "avg_logprob": -0.13280340830485027, "compression_ratio": 1.5421052631578946, "no_speech_prob": 1.2805314327124506e-05}, {"id": 1571, "seek": 761216, "start": 7622.32, "end": 7627.12, "text": " out the optimal splitting up of words.", "tokens": [484, 264, 16252, 30348, 493, 295, 2283, 13], "temperature": 0.0, "avg_logprob": -0.13280340830485027, "compression_ratio": 1.5421052631578946, "no_speech_prob": 1.2805314327124506e-05}, {"id": 1572, "seek": 761216, "start": 7627.12, "end": 7631.0199999999995, "text": " And so you end up with a vocabulary of subword units.", "tokens": [400, 370, 291, 917, 493, 365, 257, 19864, 295, 1422, 7462, 6815, 13], "temperature": 0.0, "avg_logprob": -0.13280340830485027, "compression_ratio": 1.5421052631578946, "no_speech_prob": 1.2805314327124506e-05}, {"id": 1573, "seek": 761216, "start": 7631.0199999999995, "end": 7637.0599999999995, "text": " In my playing around, I found that creating a vocabulary of about 30,000 subword units", "tokens": [682, 452, 2433, 926, 11, 286, 1352, 300, 4084, 257, 19864, 295, 466, 2217, 11, 1360, 1422, 7462, 6815], "temperature": 0.0, "avg_logprob": -0.13280340830485027, "compression_ratio": 1.5421052631578946, "no_speech_prob": 1.2805314327124506e-05}, {"id": 1574, "seek": 761216, "start": 7637.0599999999995, "end": 7639.96, "text": " seems to be about optimal.", "tokens": [2544, 281, 312, 466, 16252, 13], "temperature": 0.0, "avg_logprob": -0.13280340830485027, "compression_ratio": 1.5421052631578946, "no_speech_prob": 1.2805314327124506e-05}, {"id": 1575, "seek": 763996, "start": 7639.96, "end": 7643.52, "text": " So if you're interested, there's something you can try.", "tokens": [407, 498, 291, 434, 3102, 11, 456, 311, 746, 291, 393, 853, 13], "temperature": 0.0, "avg_logprob": -0.156605134930527, "compression_ratio": 1.55, "no_speech_prob": 4.157324383413652e-06}, {"id": 1576, "seek": 763996, "start": 7643.52, "end": 7645.36, "text": " It's a bit of a pain to install.", "tokens": [467, 311, 257, 857, 295, 257, 1822, 281, 3625, 13], "temperature": 0.0, "avg_logprob": -0.156605134930527, "compression_ratio": 1.55, "no_speech_prob": 4.157324383413652e-06}, {"id": 1577, "seek": 763996, "start": 7645.36, "end": 7650.88, "text": " It's C++, it doesn't have great error messages, but it will work.", "tokens": [467, 311, 383, 25472, 11, 309, 1177, 380, 362, 869, 6713, 7897, 11, 457, 309, 486, 589, 13], "temperature": 0.0, "avg_logprob": -0.156605134930527, "compression_ratio": 1.55, "no_speech_prob": 4.157324383413652e-06}, {"id": 1578, "seek": 763996, "start": 7650.88, "end": 7656.68, "text": " There is a Python library for it, and if anybody tries this, I'm happy to help them get it", "tokens": [821, 307, 257, 15329, 6405, 337, 309, 11, 293, 498, 4472, 9898, 341, 11, 286, 478, 2055, 281, 854, 552, 483, 309], "temperature": 0.0, "avg_logprob": -0.156605134930527, "compression_ratio": 1.55, "no_speech_prob": 4.157324383413652e-06}, {"id": 1579, "seek": 763996, "start": 7656.68, "end": 7657.68, "text": " working.", "tokens": [1364, 13], "temperature": 0.0, "avg_logprob": -0.156605134930527, "compression_ratio": 1.55, "no_speech_prob": 4.157324383413652e-06}, {"id": 1580, "seek": 763996, "start": 7657.68, "end": 7666.08, "text": " There's been little, if any, experiments with ensembling subword and word-level stuff classification,", "tokens": [821, 311, 668, 707, 11, 498, 604, 11, 12050, 365, 12567, 2504, 1688, 1422, 7462, 293, 1349, 12, 12418, 1507, 21538, 11], "temperature": 0.0, "avg_logprob": -0.156605134930527, "compression_ratio": 1.55, "no_speech_prob": 4.157324383413652e-06}, {"id": 1581, "seek": 763996, "start": 7666.08, "end": 7669.4800000000005, "text": " and I do think it should be the best approach.", "tokens": [293, 286, 360, 519, 309, 820, 312, 264, 1151, 3109, 13], "temperature": 0.0, "avg_logprob": -0.156605134930527, "compression_ratio": 1.55, "no_speech_prob": 4.157324383413652e-06}, {"id": 1582, "seek": 766948, "start": 7669.48, "end": 7670.48, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.22802083832877024, "compression_ratio": 1.0, "no_speech_prob": 4.2361250962130725e-05}, {"id": 1583, "seek": 766948, "start": 7670.48, "end": 7671.48, "text": " Thanks, everybody.", "tokens": [2561, 11, 2201, 13], "temperature": 0.0, "avg_logprob": -0.22802083832877024, "compression_ratio": 1.0, "no_speech_prob": 4.2361250962130725e-05}, {"id": 1584, "seek": 766948, "start": 7671.48, "end": 7672.48, "text": " Have a great week, and see you next Monday.", "tokens": [3560, 257, 869, 1243, 11, 293, 536, 291, 958, 8138, 13], "temperature": 0.0, "avg_logprob": -0.22802083832877024, "compression_ratio": 1.0, "no_speech_prob": 4.2361250962130725e-05}, {"id": 1585, "seek": 767248, "start": 7672.48, "end": 7700.48, "text": " Thank you.", "tokens": [50364, 1044, 291, 13, 51764], "temperature": 0.0, "avg_logprob": -0.852593739827474, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.00012552460248116404}], "language": "en"}