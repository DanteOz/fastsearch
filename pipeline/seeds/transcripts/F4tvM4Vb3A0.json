{"text": " Hi everybody, welcome to lesson two. Thanks for coming back. Slight change of environment here. We had a bit of an administrative issue at our university. Somebody booked our room. So I'm doing this from the study at home. So sorry about the lack of decorations behind me. I'm actually really, really pumped about this lesson. It feels like going back to what things were like in the very early days because we're doing like some really new, really cool stuff, which, you know, stuff that hasn't really been in courses like this before. So I'm super, super excited. So thanks a lot for coming back after lesson one and I hope it's worth you coming back. I think you're going to love it. I am, yeah, I'm really excited about this. Now remember that, you know, the course goes with the book. So be sure that you're following, I mean, not following along in the book because we're covering similar things in different directions, but like read the book as well. And remember the book is entirely available for free as well. You can go to the Fast AI, Fast Book repo to see the notebooks or through course.fast.ai. You can read it there through example, through, through Colab. And also remember that the book, I mean, the book's got a lot of stuff that we didn't cover in the course, like, you know, stuff I find pretty interesting about the history of neural networks, some of which has some really interesting personal stories actually, as you'll read here. And at the end of each chapter, there is a quiz. And remember, it's not a bad idea before you watch the video to read the quiz. So if you want to read the chapter two quiz, you know, and then come back, that's not a bad idea. And then make sure that you can do the quiz after you've watched the video and you've read chapter two of the book. Something I didn't mention last week is there's also a very cool thing that Radek, who I mentioned last week, has written called aiquizzes.com, which is a site specifically for quizzes about the book. And it actually uses repetitive space learning techniques to make sure that you never forget. So do check out aiquizzes.com. It's all brand new questions. They're different to the ones in the book and they're really nicely curated and put together. So check out aiquizzes.com as well. Remember as well as course.fast.ai, there's also forums.fast.ai. So course.fast.ai is where you want to go to get, you know, links to all the notebooks and Kaggle stuff and all that stuff. You'll also find on forums.fast.ai every lesson has an official topic, you know, with all the information you'll need. Generally there'll be a bit more info on the forums. We try to keep the course lean and mean and the forums are a bit more detailed. So if you find, well in this case, you'd want to look at the lesson two official topic, but here's the lesson one official topic so far. So from the lesson one official topic, already after just a few days since I recorded it, we haven't even launched the course, so it's just the people doing it live. There's already a lot of replies and that can get pretty overwhelming. So be aware that there's a button at the bottom of my post that says summarize this topic and if you hit that, then you'll just see the most upvoted replies and that's a really good way to just make sure that you hit on the main stuff. So there's the button and here's what it looks like after you hit it. You'll just get the upvoted stuff from fast.ai legends like Sanyam and Tanishk. So hopefully you'll find that a useful way to use the forum. So one of the cool things about this week is I, as promised, put up the show us what you've made post and already a lot of people have posted. I took the screenshot a few days ago. It's way above 39 replies already if I remember correctly. I had a lot of trouble deciding which ones to share because they're all so good. So I've actually decided to kind of, you know, went the easy route and I just picked the first. So I'm just going to show you the first ones that were posted because they're also good. So the first, the very, very first one to be posted is a damaged car classifier. So that worked out pretty well it looks like. And I really liked what Matt, the creator said about this is that, you know, wow, it's a bit uncomfortable to run this code. I don't really understand yet, but I'm just doing it. And so I'm like, yeah, good on you, Matt, for just, for just doing it. That's the way to get started. It's all going to make sense. Don't worry. Very nice to see that the next one posted was actually a blog post in fast pages. Very nice to see. So just describing some stuff, some experiments that they ran over the week and what did they find? Next one was the amazing beard detector, which if I understand correctly was mainly because it's very easy to get from bird to beard by just changing one letter to two. And this is doing a very good job of finding gentlemen with beards. So very nice. And then this one is another level again. It's a whole in production web app to classify food, which is kind of like extra credit. Apparently we're up to 80 replies now in that thread. Thank you, Sanjan. Very cool. So you know, obviously, so this was actually created by Suvash who's been doing the courses for a few years now, I believe. And so, you know, one day you too might be able to create your very own web app and put it in production. And when I say one day, more specifically today, I'm actually going to show you how to do this right now. So it's actually quite lucky coincidence that Suvash put this up there because it's exactly the topic that we're going to pick today. So how do we go about putting a model in production? Step one is, well, you've kind of done step one, right? Step one is step one, two, three, four, is figure out what problem you want to solve, figure out how to find the data for it, gather some data and so forth. So what's the kind of first step after you've got your data? The next step is data cleaning. And if you go to chapter two of the book, which I'm going to go ahead and open up now. So here is the book. So you can open it in Colab directly from the course or if you've cloned it to your computer or whatever, you can do it there. So remember, course.fast.ai will run you through exactly how to run these notebooks. And so you can see chapter two is all about putting stuff in production. And so here is chapter two. All right. And so remember, we hit shift enter to run cells, okay, to execute them. And so we're going to go to the part of the book where we start cleaning the data. So I'll click on navigate and we'll go down here, gathering data. There we are. So we could do a quick bit of revision first. Now, by the way, I will mention a lot of people ask me what are the little tricks I use for getting around Jupyter Notebook so quickly and easily. One of the really nice ones, as you'll see, is this navigate menu, which actually doesn't appear by default. So if you install something called Jupyter Notebook extensions, Jupyter Notebook extensions, and so you just pip install them, follow the instructions, and then restart Jupyter. Obviously, this colab already has a table of contents, by the way. This is just if you're using something local, for example, then you'll see here that this NB extension thing will appear. And if you click on table of contents two, that gives you this handy navigation bar. The other thing I really like is this one here called collapsible headings. And that's the one which gives me these nice little things here to close and open up. And actually, that's not even the best part. The best part for me is if I hit right arrow, it goes to the end of a section. And if I hit left arrow, it goes to the start of a section. So it's like if I want to move around sections, I just press up, left, down, right, down, right. Very handy. And if you hit left again, when you're here, it'll close it up. Hit right again here, open it up. So that's collapsible headings. Anyway, a couple of really handy things. And we'll be talking a lot more about getting your notebook set up today at the moment. So one thing you'll notice is in the book, we use the Bing API for searching images. I've just gone ahead and replaced Bing with DDG because the Bing API requires getting an SDK key, which honestly, it's like the hardest thing in deep learning is figuring out the Bing Azure website and getting that sorted out. DDG doesn't. So it's basically exactly the same. And you can, I'll share this notebook as well on the course website and the forum. But all I've basically done is I've replaced Bing with DDG and got rid of the key. So then just like we did last week, we can search for things. And so in the book, we did a bear detector because at the time I wrote it, my then toddler was very interested in me helping identify teddy bears. And I certainly didn't want her accidentally cuddling a grizzly bear. So we show here how we can search for grizzly bears, just like last week, something that loops through grizzly bears, black bears and teddy bears, just like last week, get rid of the ones that failed, just like last week. And one thing a few people have asked on the forum is how do I find out more information about basically any Python or fast AI or PyTorch thing? There's a few tips here in the book. One is that if you put a double question mark next to any function name, you'll actually get the whole source code for it. And by the same token, if you put a single question mark, you'll get a brief, you know, little bit of information. If you've got NB dev installed, I think it's NB dev you need, then you can type doc and that'll give you, perhaps most importantly, a link straight to the documentation where you can find out more information. And generally there'll be examples as well. And also a link here to the source code. If you want to, let's do that with a control. Okay. A link to the source code and that way you can jump around. Notice that in GitHub in the source code, you can click on things and jump to their definition so it's kind of a nice way of skipping around to understand exactly what's going on. Okay. So lots of great ways of getting help. But what I promised you is that we're going to now clean the data. So I'm going to tell you something that you might find really surprising. Before you clean the data, you train a model. Now I know that's going to sound really backwards to what you've probably heard a thousand times, which is that first you train your data and then you train your model. But I'm going to show you something really amazing. First we're going to train a model and you'll see why in a moment. So to train a model, just like before, we use a data block to grab our data loaders. There's lots of information here in the book about what's going on here. There we go. And so then we can call show batch to see them as per usual. There's a little side by here in the book I'll quickly mention, which is about the different ways we could resize. I think we briefly mentioned it last week. We can squish. Last week I used a string. You can use a string or this kind of enum like thing that we have. You can see with a squish, you can end up with some very thin bears. Right? So this is the real site that's shaped with the bear. Here it's become thin, but you can see now we've got all of its cubs. Are they called cubs? Yeah, bear cubs. So it's squished it to make sure we can see the whole picture. Same one here. This one was out of the picture. We squished it. This guy now looks weirdly thin, but we can see the whole thing. So that's squishing. Whereas this one here is cropping. It's cropped out just the center of the image. So we get a better aspect ratio, but we lose some stuff. This is so we can get square images. And the other approach is we can use pad. And so you can pad with various different things. If you pad with zeros, which is black, you can see here now we've got the whole image and the correct aspect ratio. So that's another way we can do it. And you know, different situations, you know, result in different quality models. You can try them all. It doesn't normally make too big a difference, so I wouldn't worry about it too much. I tell you one though that is very interesting is random resized crop. So instead of saying resize, we can say random resized crop. And if we do that, you'll see we get a different bit of an image every time. So during the week this week, somebody asked on the forum, I'm trying to, this is a really interesting idea, which it turned out worked slightly, was they wanted to recognize pictures of French and German texts. So obviously this is not the normal way you would do that, but just for a bit of experiment and I love experiments. So they had very big scans of documents and they wanted to figure out whether it was French or German just by looking at images. And they said the pictures were too big. What should I do? I said use random resized crop and that way you would grab different bits of the image. And this is very nice because you could run lots and lots of epochs and get slightly different pictures each time. So this is a very good technique. And this idea of getting different pictures each time from the same image is called data augmentation. And again, I'm not going to go into too much detail about data augmentation because it's in the book, but I'll just quickly point out here that if you use this thing called aug transforms, so augmentation transforms, and here I have multiplied them by two. So I've made them super big so you can see them more easily. You can see that these teddies are getting turned and squished and warped and recolored and saturated, all this stuff to make every picture different. And generally speaking, if you're training for more than about five or 10 epochs, which you'll probably want to do most of the time, unless you've got a super easy problem to solve, you'll probably want to use random resized crop and these aug transforms. Don't put the multi-cols too, just leave that empty. I'm just putting it there so you can see them more clearly. So I've got an interesting question here from Alex in our audience, which is, is this copying the image multiple times during something like this or something like this? And the answer is no, we're not copying the image. What happens is that image, so each epoch, every image gets read. And what happens here is though, is kind of in memory, in RAM, the image is being warped, right? It's being recropping it and recoloring it and so forth. So it's a real time process that's happening during model training. So there's no copies being stored on your computer, but effectively it's almost like there's infinitely slightly different copies because that's what the model ends up seeing. So I hope that makes sense, Alex and everybody else. So that's a great question. Okay, so we've got, we're going to use random resized crop, we're going to use augmentation transforms so that we can get data loaders from that and then we can go ahead and train our model. It takes about a minute. In this case, we only did four epochs of fine tuning. We'll talk about why there's five here later in the course, but four main epochs of fine tuning. So we probably didn't really need random resized crop and aug transforms because there's so few epochs, but you know, if you want to run more epochs, this is a good, good approach. Under 3% error. That's good. Okay. So remember I said we're going to train a model before we clean. Okay. So let's go ahead and train it. So while that's training, that's running on my laptop, which only has a four gigabyte GPU. It's pretty basic, but it's enough to get started. While that's training, we'll take a look at the next one. So the first thing we're going to look at is the confusion matrix and the confusion matrix is something that it only is meaningful for when your labels are categories. And what it says is hell, what category errors are you making? And so this is showing that the model that we've got at this point, there was two times when there was actually a grizzly bear and it thought it was a black bear. And there was two times when there was actually a black bear and it thought it was a grizzly bear and there was no times that it got Teddy's wrong, which makes sense, right? Teddy's two look quite different to me. In a lot of situations, when you look at this, it'll kind of give you a real sense of like, okay, well, what are the hard ones? Right? So for example, if you use the pets data set that we quite often play with in the book and the course, this classification metric matrix for different breeds of pet, you know, really shows you which ones are difficult to identify. And I've actually gone in and like read Wikipedia pages and, and, and pet breeding reports about how to identify these particular types because it's so difficult and even experts find it difficult. And one of the things I've learned from doing the course actually is black bears and grizzly bears are much harder to pick apart than I had realized. So I'm not even going to try, but I'll show you the really interesting thing we can do with this model is that now we've created this classification interpretation object, which we use for confusion metrics. We can say plot top losses. We can say plot top losses. And this is very interesting. What it does is it tells us the places where the loss is the highest. Now if you remember from the last lesson, the loss is that measurement of how good our model is that we take after each time we run through an item of data, a loss will be bad if we predict wrongly and we're very confident about that prediction. So here's an example where we predicted his the order here prediction, actual loss probability where we predicted grizzly and it was actually a black and we were 96% sure our model was that it's a grizzly. Now I don't know enough about bears to know whether the model made a mistake or whether this actually is a picture of a grizzly bear. But so an expert would obviously go back and check those out. Right? Now you'll notice a couple here. It's got grizzly grizzly Teddy Teddy. They're actually correct. Right? So why is this loss bad when it's correct? And the reason is because it wasn't very confident. It was only 66% confident. Right? So here's a Teddy. It's only 72% confident. Right? So you can have a bad loss either by being wrong and confident or being right and unconfident. Now the reason that's really helpful is that now we can use something called the fast AI image classifier cleaner to clean up the ones that are wrongly labeled in our data set. So when we use the image classifier cleaner, it actually runs our models. That's why we pass it learn. Right? And I mentioned that I don't know much about black bears and grizzly bears, but I do know a lot about teddy bears. So I'll pick teddy bears. And if I click teddy bears, it's now showing me all the things in the training set. You can pick training or valid that were marked as teddy bears. And here's what's really important. They're ordered by loss. So they're ordered by confidence. Right? So I can scroll through just the first few and check they're correct. Right? And oh, here's a mistake. Right? So when I find one that was wrongly gathered, I can either put it if it's in the wrong category and I can choose the correct category or if it shouldn't be there at all, I click delete. So here I'll go ahead and click delete. Right? So I can see some reasons that some of these are hard. Like for example, here's two teddies, which is just, I guess, confusing. So it doesn't see that often. This one here is a bit weird looking. It looks almost like a wombat. This is an awful lot of teddies. This one maybe is just a bit hard to see from the background, but these otherwise they look fine. So we just look through the first few. And if you don't see any problem or problems in the first few, you're probably fine. So that's cleaned up our training set. Let's clean up our validation set as well. So here's that one it had trouble with. I don't know why it had trouble with that one, but so be it. And we'll have a quick scroll through. Okay. I'm not really sure that's a bear. So I'm just going to go ahead and delete it. So teddy something, but you know, it's a problem. Okay that's not a teddy either. So you see the idea, right? So after we've done that, what that does is the cleaner has now stored a list of the ones that we changed and the list of the ones we deleted. So we can now go ahead and run this cell. And so that's going to go through a list of all of the indexes that we said to delete and it will delete those files and it'll go through all the ones we said to change and it will move them to the new folder. There we go. Done. So this is like not just something for image models. It's just, it's actually a really powerful technique that almost nobody knows about and uses which is before you start data cleaning, always build a model to find out what things are difficult to recognize in your data and to find the things that the model can help you find data problems. And then as you see them, you'll kind of say, okay, I see the kinds of problems we're having and you might find better ways to gather the next data set or you might find ways to kind of automate some of the cleaning and so forth. Okay so that is data cleaning. And since I only have a four gigabyte GPU, it's very important for me to close and halt because that will free up the memory. So it's important to know on your computer, your normal RAM doesn't really get filled up because if you use up too much RAM, what will happen is that instead your computer will start, it's called swapping, which is basically to save that RAM onto the hard disk to use it later. GPUs can't swap. GPUs when they run out of RAM, that's it, you're done. So you need to make sure that you close any notebooks that are using the GPU that you're not using and really only use it one thing at a time on the GPU. Otherwise, you'll almost certainly run out of memory. So we've got the first few reds starting to appear. So remember to ask. And in terms of the yellows, it's important to know as you watch the video, I'm not asking you to run all this code. Okay, the idea is to kind of watch it and then go back and pause, you know, as you go along or you can just stop, try, stop, try. The approach I really like and a lot of students really like for watching these videos is to actually watch the entire thing without touching the keyboard to get a sense of what the video is about and then go back to the start and watch it again and follow along. That way at every point, you know what it is you're doing, you know what's going to happen next. That can actually save you some time. It's a bit of an unusual way because obviously like real life lectures, you can't do that. You can't rewind the professor and get them to say it again, but it's a good way to do it here. So now that we've cleaned our data, how are we going to put it into production? Well, in the book we use something called voila and it's, it's pretty good. But there's actually something that I think most of you are probably going to find a lot more useful nowadays, which is something called hugging face spaces. And there's a couple of things you can use with that. We're going to look at something called Gradio today. And there isn't a chapter about this in the book, but that doesn't matter because Tanishq Abraham, who's actually one of the TAs in the course, has written a fantastic blog post about really everything we're going to cover today. So there's a link to that from the forum and from the course page. So this is like the equivalent of the chapter of the book, if you like. And I would be remiss if I didn't stop for a moment and call out Tanishq in a big way for two reasons. The first is he is one of the most helpful people in the fast AI community. He's been around quite a few years, incredibly tenacious, thoughtful and patient. And also because I have this fantastic picture of him a few years ago with Conan when he was a famous child prodigy. So now you know what happens to famous child prodigies when they grow up. They became even more famous fast AI community members and declining experts. So you should definitely check out this video of him telling jokes to Conan. I think he's still only 18 actually. This is probably not that many years ago. So thank you very much Tanishq for all your help in the community. And sorry for embarrassing you with that picture of you as a nine year old. I'm not really. Okay now the thing is for doing Gradio and Hugging Face Spaces, well it's easy enough to start. Okay we start over here on the Hugging Face Spaces page which we've linked to from the forum and the course. And we're going to put a model in production where we're going to take the model we trained and we are going to basically copy it to this Hugging Face Spaces server and write a user interface for it. So let's go. Create new space. Okay so you can just go ahead and say alright so obviously you sign up. The whole thing is free. Basically everything I'm showing you in this entire course you can do for free. That's the good news. Okay so give it a name. Just create something minimal. I always use the Apache license because it means other people can use your work really easily, but you don't have to worry too much about patents. As I say there's a few different products you can use with it. We're going to use Gradio also free. If you make it public then you can share it which is always a good idea when you're a student particularly to really be building up that portfolio. Okay so we're done. We've created a space. Now what do we do next? Well Spaces works through Git. Now most software developers will be very familiar with Git. Some data scientists might not be and so Git's a very very useful tool. I'm not going to talk about it in detail but let's kind of quickly learn about how to use it. Right now Git you can use it through something called GitHub desktop which is actually pretty great and even people who use Git through the console should probably be considering using GitHub desktop as well because some things just much faster and easier in it. In fact I was talking to my friend Hamill today and I was like oh help I've accidentally committed this two things by mistake. What's the easiest way to revert it? And he used to work at GitHub and I thought he was going to have some fancy console command and he was like oh you should use GitHub desktop and you can just click on it. Oh that's a great idea. So that's useful. But most of the time we do use Git from the console from the terminal. If you're a Linux user or a Mac user you've already got a terminal very straightforward no worries. If you're a Windows user I've got good news nowadays Windows has a terrific terminal. It's called Windows terminal you get it from the Microsoft store. So in fact every time you see me using a terminal I'm actually using that Windows terminal. Works very well. God knows why I'd want it to have all these ridiculous colors but there you go. Now what do you want to be running inside your terminal? Obviously if you're in Linux or Mac you've already got a shell set up. In Windows you almost certainly want to use Ubuntu. So Windows believe it or not can actually run a full Linux environment and to do it is typing a single line which is this. So if you go to just Google for WSL install run PowerShell as administrator. Paste that command wait about five minutes reboot you're done. You now have a complete Linux environment. Now the one of the reasons I'm mentioning this is I'm going to show you how to do stuff on your own machine now. And so this is like going to a bit of an extra level of geekery which some data scientists may be less familiar with. So you know don't be worried about the terminal. You're going to think you're going to find it really helpful and much less scary than you expect. And I particularly say like for me I choose to use Windows and that's because I get you know all the nice Windows GUI apps and I can draw on my screen and do presentations and I have a complete Linux environment as well and that Linux environment uses my GPU and everything. So for me my first choice is to use Windows. My second choice by not very much really like it would be to use Linux. Mac is a little bit harder but it's still usable. So some things are a little bit trickier on Mac but you should be fine. Okay so whatever you've got at this point you've now got a terminal available and so in your terminal one of the really nice things about using a terminal is you don't have to follow lots of instructions about click here click here click here you just copy and paste things. So you just copy this and you go over to your terminal and you paste it in and you run it and after you do that you'll find that you've now got a directory. And so that new directory initially is empty and they tell you okay go ahead and create a file with this in it. Okay so how do you create a file with that in it when we're in here in our Linux environment on Windows or in the terminal on Mac or whatever. Well all you do in Windows if you just type explorer.exe. It'll open up Explorer here. Well better still on either Mac or Linux or Windows. So yeah so regardless of what computer type of computer on you can just type code. And it will pop up Visual Studio code and open up your folder. And so then you can just go ahead and if you haven't used VS code before it's really well worth taking a few minutes to read the some tutorials. It's a really great IDE and so you can go ahead and create an app.py file like they tell you to app.py file containing what they told you to put in it. Here it is here. All right we're nearly there. So you can now go ahead and save that and then you need to commit it to Gradio. To Hacking Space Spaces. So one really easy way is just in Visual Studio itself you can just click here and that'll give you a place where you type a message and you hit tick and it'll send it off to Hacking Space Spaces for you. So once you've done that you can then go to back to the exact same website you were on before Hacking Space Spaces JP HRO Minimal. And what you'll find now is that it'll take about a minute to build your website and it's the website it's building is going to have a Gradio interface with a text input a text output and it's going to run a function called greet on the input and my function called greet or return hello name. So that's what it's going to do. There it goes. Let's try it. We'll say hello to Tanishk. I'm not always very good at remembering how to spell his name. I think it's like that. And there you go. So you can see it's put the output for our input. So not a very exciting app but we now have to be fair an app running in production. Right. I told you we'd have a deep learning model running in production. So now we have to take the next step which is to turn this into a deep learning model. All right. So first we're going to need a deep learning model and there's a few different ways we can get ourselves a deep learning model but basically we're going to have to train one. So I've got a couple of examples I've got a Kaggle example and a Colab example. Maybe I'll quickly show you both. They're going to do the same thing and I'm just going to create a dog or a cat classifier. Okay. So here's our Kaggle model. I'll click on edit so you can actually see what it looks like in edit view. Now Kaggle already has fast AI installed but I always put this first just to make sure we've got the latest version and obviously import stuff. So we're going to grab the pets data set a function to check whether it's a cat. That's our labeling function for our image data loaders. Remember this is just another way of doing data blocks. It's like a little shorthand and we create our learner and we fine tune it. Okay so that's all stuff we've seen before. So in Kaggle every notebook has a edit view which is what you just saw and a reader view and so you can share your notebook if you want to and then anybody can read the reader view as you see and so you can see it shows you what happened when I ran it and so I trained it, it took you know so that the GPUs on Kaggle are a bit slower than most modern GPUs but they're still fast enough. I mean it takes five minutes and there's one bit at the end here which you haven't seen before which is I go learn.export and I give it a name. Now that's going to create a file containing our trained model and that's the only thing creating this file is the only thing you need a GPU for. Right so you do that on Kaggle or on Colab so here's exactly the same thing on Colab. You can see pip install, here's cat, untar data, image data loaders so I've got a show batch here as well just for fun. Create my learner and then export. So while we wait I might go ahead and just run that. One nice thing about Kaggle is once you've run it and saved it you can then go to the data tab and here is basically anything you've saved it's going to appear here and here it is model.pickle. So now I can go ahead and download that and that will then be downloaded to my downloads folder and then I need to copy it into the same directory that my Hackingface Bases app's in. Now my Hackingface Bases app is currently open in my terminal. On Mac you can type open dot or in Windows you can type explorer.xc dot and that will bring up your finder or explorer in that directory and so then you can just paste that thing you downloaded into this directory. Something by the way in Windows I do which I find really helpful is I actually grab my home directory in Linux and I pin it to my quick access and that way I can always jump in Windows straight to my Linux files. Not really something you have to worry about on Mac because it's all kind of integrated but on Windows they're like kind of like two separate machines. Okay so let's do, so I created a space called testing and I downloaded my model.pickle and I pasted it into testing. So now we need to know how do we do predictions on a saved model. So we've got a notebook for that. Okay so we've got a notebook for that and so I'm going to take you through how we use a model that we've trained to make predictions. There's a few funny things with hash pipe which I'll explain in a moment just ignore those for now. So we import fastai as usual, we import radio as we did before and we copy in the exact same is cat definition we had before. It's important any external functions that you used in your labeling need to be included here as well because that learner refers to those functions. Okay it saves that learner saved everything about your model but it doesn't have the source code to the function so you need to keep those with you. So let's try running this. So for example I just grabbed as you might have seen in my explorer I just popped a dog picture there and so we can create a python image library image from that dog, turn it into a slightly smaller one so it doesn't overwhelm our whole screen and there is a picture of a dog. So how do we make predictions of whether that's a dog or a cat. So it's very simple all we do is instead of training a learner we use load learner. We pass in the file name that we saved and that returns a learner. This learner is exactly the same as the learner you get when you finish training. So here we are his colab right we've just been training a learner so at the end of that there's a learner that's been trained and so we kind of froze it in time something called a pickle file which is a python concept. It's like a frozen object. We saved it to disk we transferred it to our computer and we've now loaded it and we've now un-thruth thought it. Here's our unpickled learner and we can now do whatever we like with that. So one of the things that the one of the methods that a learner has is a dot predict method. So if I run it you can see even on my laptop it's basically instance instant. In fact we can see how long it took. If you in Jupiter things that start with percent are called magic's this special Jupiter things. So for example there's a thing to see how long something takes. There you go. Okay so it took 54 milliseconds to figure out that this is not a cat. So it's returning two things. Is it a cat as a string. Is it a cat as a zero or one and then the probability that it's a dog and the probability that it's a cat. So the probability of zero false and one true. Is it a cat. So definitely a dog. So we now want to create a Gradio interface which basically has this information. So Gradio requires us to give it a function that it's going to call. So here's our function. So we're going to call predict and that returns us we said three things. The prediction is a string the index of that and the probabilities of whether it's a dog or a cat. And what Gradio wants is it wants to get back a dictionary containing each of the possible categories which in this case is dog or cat and the probability of each one. So if you haven't done much Python before a dict of a zip may be something you haven't seen very handy little idiom well worth checking out. Do you know if you haven't seen map before. Anyway here it is one slightly annoying thing about Gradio at the moment is that it doesn't handle pie torch tensors. You can see here pie torch is not returning normal numbers it's returning tensors it's not even returning numpy arrays. In fact Gradio can't handle numpy either. So we have to change everything just to a normal float. So that's all that this is doing is changing each one to a float. So for example if I now call classify image with our doggy image we get back a dictionary of a dog. Yes definitely cat definitely not. So now we've got all that we can go ahead and create a Gradio interface so Gradio interface is something where we say well what function do you call to get the output. What is the input in this case we say oh the input is an image and so check out the Gradio docs it can be all kinds of things like a webcam picture or text or you know all kinds of things. Give it a shape that it's going to put it into the outputs just going to be a label. So we're going to create very very simple interface and we can also provide some examples and so there's a dog a cat and a don't know which I'll do about in a moment which you'll see here there's a dog and a cat and a don't know. So once I launch it it says OK that's now running on this URL. So if I open that up you can see now we have just like Souvash we have our own not yet in production but running on our own box classifier. So let's check dog so you can click and upload one or just choose the examples. Yeah yeah so it's running on my own laptop basically instant and I really have to tell you the story about this guy here. This is the don't know submit. Wait why is it saying 100 normally this says like 50 50. That's a bummer. This model's got messed up my whole story. The last time I trained this model and I ran it on the don't know it said it said like it's almost exactly 50 50 and the way we found this picture is I showed my six year old daughter she's like what are you doing dad's like I'm coding what are you coding. Oh you know dog cat classifier. She checks it out and her first question is can I take your keyboard for a moment and she goes to Google and she's like what is a dog mixed with a cat called. There's no such thing as a dog mix with a cat. Anyway she goes to the images tab and finds this picture and she's like look there's a dog mixed with a cat. She said run it on that dad run it on that and I ran it and it was like 50 50 it had no idea if it was a dog or a cat. Now this model I just retrained today and I was sure it's a cat. So there you go. I think I used a slightly different training schedule or something or I gave it an extra epoch. Anyway so that's a dog cat but apparently it's a cat. I guess it is a cat. It's probably right. I shouldn't have trained it for as long. Okay so there's our interface. Now that's actually running so you actually have to click the stop button to stop it running so otherwise you won't be able to do anything else in your notebook. So now we have to turn that into a Python script. So one way to turn it into a Python script would be to copy and paste into a Python script all the things that you need. I read a copy and paste into a Python script all those parts of this that you need. So for example we wouldn't need this is just to check something out. We wouldn't need this. It was just experimenting. This was just experimenting. We'd need this right. So what I did is I went through and I wrote hash pipe export at the top of each cell that contains information that I'm going to need in my final script and then so there are the steps right and then at the very bottom here I've imported something called notebook to script from NB dev and if I run that and pass in the name of this notebook that creates a file for me called app.py containing that script. So this is a nice easy way to like when you're working with stuff that's expecting a script and not a notebook like hugging face spaces does it's fine to just copy and paste into a text file if you like but I really like this way of doing it because that way I can do all of my experimentation in a notebook and when I'm done I just have a cell at the bottom I just run and export it. How does it know to call it app.py. That's because there's a special thing at the top default export default x which says what python file name to create. So that's just a little trick that I use. So now we've got an app.py we need to upload this to Gradio. How do we do that. You just you just push it to get so in you can either do it with Visual Studio code or you can type commit and then get push and once you've done that. If we change minimal to testing. I think this hopefully might still be running my previous model because I didn't push it and that way we can see our crazy dog cat. All right. So here it is. You can see it running in production. So now this is something that anybody can if you set it to public anybody can go here and check out your model and so they can upload it and so here's my doggy. Yep definitely a dog cat. Yeah I think I'm going to train this for a epoch or two less so it's less confident. Yeah definitely a cat dog cat. Hey dog cat. Hmm still thinks it's definitely a cat. Oh well so be it. Okay so that is. Okay so that is an example of getting a simple model in production. There's a couple of questions from the forum from the community. Okay so one person's asking what's the difference between a pie torch model and a fast AI learner. Okay that's fine. We will get to that shortly. Don't know if it'll be that lesson. It might be this lesson or the next lesson. And then somebody else asked basically is asking how many epochs do we train for. So as you train a model your error rate as you can see it improves. And so the question is should I run more. Should I increase the number of epochs. This is doing three epochs right. Here's my three epochs plus one to get started. Look it's up to you right. I mean this is here saying there's a 1 percent error. I'm okay with the 1 percent error. You know if you want it to be better then you could use more data augmentation and you could train it for longer. If you train for long enough as we'll learn about soon in the next maybe the next lesson if you train for long enough your error rate actually starts getting worse and you'll see we learn about why. So basically yeah you can train until it's good enough or until you've run out of patience or time or run out of compute or until you or until the error rate starts getting worse. Okay oh and then in Colab how do you grab your model. All you need to do in Colab is after you've exported it is if you go into their file browser you'll actually see it here right and you can click download. It's a bit weird. It doesn't like pop up a box saying where do you want to download it to but instead this kind of progress circle thing pops up and so depending on how big it is and so forth it can take a few minutes and once that circle fills up then it'll the browser thing will finally pop up and say okay you can save it. Okay so that's how you that's how you actually grab your model. So as you can see that the step where you actually need a GPU you can use these totally free resources Colab Kaggle and there are other ones we'll talk about in future lessons and then you can do everything else on your own computer including the predictions. The predictions are fast right so you really don't need to use a GPU for that unless you're doing thousands of them. Okay here we go now it's asking me to save it. Okay so now one big issue is we needed to run it on our computer we needed Python and Jupyter notebooks running on our computer. So how do you do that because this is where often people get in all kinds of trouble trying to figure out how to get this all working. So the good news is we've actually got something that makes it very very straightforward. It's called Fast Setup there's really only just one part of it you need. So let me show you it's it's actually a Git repository on GitHub. Github's the place where most Git repositories live. So if you go to GitHub Fast AI Fast Setup you'll see it. And so what you can do is you can now grab this whole repository just by clicking here on code and if you've got GitHub desktop installed click on open with GitHub desktop. And as you'll see it brings this up saying okay I'm ready to save this for you so I'll click clone. So it's making a copy of it. There we go. So basically once you've cloned it you'll then find there's a file in there called setup conda dot sh which you know the details don't really matter it's pretty short but that's the thing that's going to install Python for you. So at that point you can just run dot slash setup conda and it'll run this installer. Now if you've got Linux or Mac you've already got Python on your machine. Don't use that Python. And the reason is because that Python is called the system Python. It's used by your computer to do computer stuff right. It's actually it's actually needed. You don't want to be messing with it. I promise you like it's it always leads to disaster. Always you want your own development version of Python. It's also going to make sure you've got the latest version and all the libraries you want and by the by far the best one for you is almost certainly going to be these conda based Python distribution. So if you run setup conda you'll get the one that we recommend and the one we recommend at the moment is something called Mamba Forge. So basically once you run it you'll find that you've now and you close your terminal and reopen it you'll find you've now got one extra command which is called Mamba and Mamba lets you install stuff. So once you've run it you'll be able to go Mamba install fast AI and that's going to actually we should probably I should mention this actually more bit more detail about how to install it correctly. We go to docs dot fast dot AI installing. Yeah. OK. We actually want to do conda install minus C fast chain. So this is copy and paste sorry not actually. And then the other thing I'll say is instead of using conda replace conda with Mamba because nowadays it's much faster. So Mamba install minus C fast chain fast AI. Now this is going to install everything you need. It's going to install PyTorch. It's going to install NumPy. It's going to install fast AI and so forth. And so obviously I've already got it. And then the other thing you want to do is install NB dev. So you can do exactly the same thing for NB dev. You don't have to write. It's just that but that'll install Jupiter for you amongst other things. And so at that point you can now you can now use Jupiter. And so the way Jupiter works is you can see it over here. This is by going close it so we can start again. So basically to use Jupiter you just type Jupiter notebook. OK. And when you run it it'll say OK we're now running a server for you. And so if you click on that hyperlink it'll pop up this is exactly what you see me use all the time. OK so you know that hopefully is enough to kind of get you started with with Python and with Jupiter notebook. The other way people tend to install software is using something called pip instead of Mamba. Pretty much anything you can do with Mamba you can also do with pip. But if you've got a GPU pip isn't going to install things generally so that it works on your GPU. You have to install lots of other stuff which is annoying. So that's why I kind of tell people to use Mamba but you can use pip otherwise. So a little bit of red. Please let us know how we can help you going. OK so let's see how we're going with our steps. I forgot I had these steps here to remind myself. We created a space tick. We created a basic interface tick. OK we got get set up. We got Condor set up or Mamba. So Mamba and Condor are the same thing. Mamba is just a much faster version. And we'll keep some notes on the course website because at the moment they're actually working on including the speed ups from Mamba into Condor. So at some point maybe it'll be fine to use Condor again. At the moment Condor is way too slow so don't use it. OK we've done dogs versus cats. No problem. Yeah so we could also look at pet breeds. Yeah we'll look at that. OK we've used exported learner. No problem. Used NB dev. No problem. Oh OK. Try the API. All right. This is interesting. So I think we can all agree hopefully that this is pretty cool that we can provide to anybody who wants to use it for free a real working model. And you know with Gradio there's actually you know a reasonable amount of flexibility around like how you can make your your website look you know using these various different widgets. It's not amazingly flexible but it's flexible enough to kind of it really just for prototyping. So Gradio has lots of widgets and things that you can use. The other main platform at the moment that Hugging First Spaces supports is called Streamlet. Streamlet is more flexible I would say than Gradio. Not quite as easy to get started with but you know it's kind of that nice in between I guess. It's also a very good thing again mainly for kind of building prototypes. But at some point you're going to want to build more than a prototype. You want to build an app. And one of the things I really like about Gradio in Hugging First Spaces is there's a button down here. View the API. So we can actually create any app we want and the key point is that the thing that does the actual model predictions for us is going to be handled by Hugging First Spaces Gradio. And then we can write a JavaScript application that then talks to that. Now there's going to be two reactions here. Anybody who's done some front-end engineering is going to be like oh great I can now literally create anything in the world because I just write any code and I can do it. They'll be excited and a lot of data scientists might be going uh-oh I have no idea how to use JavaScript. It's not in my inventory. So this is again where I'm going to say look don't be too afraid of JavaScript. I mean obviously one option here is just to kind of say hey I've got a model throw it over to your wall over the wall to your mate who does know JavaScript and say please create a JavaScript interface for me. But let me just give you a sense of like how really not hard this actually is. So there's a end point. There's now a URL that's running with our model on it. And if you pass it some data and image some image data to this to this URL it's going to return back the dictionary. So it's going to do exactly the same thing that this UI does but as an as an API as a function we can call. And so it's got like examples here of how to call it. So for example I can actually let me show you the API as an example using that minimal interface we had because it's just going to be a bit simpler. So if I click curl and I copy that copy that and paste. So you can see there oh that's not a great example passing in hello world if I pass in an issue again let's see how I'm going with his name to niche. Saying returns back hello to niche. So this is how these APIs work right. So we can use JavaScript to call the API and we've got some examples. So I've created a website and here is my website tiny pets and on this website as you can see it's not the most amazingly beautiful thing but it's a website. It's a start right and up here I've got some examples. Here you go single file click choose file click. And in this example I'm actually doing full pet classification so actually trained a model to classify breed which we'll talk about more next week rather than just dog versus cat. So let's pick a particular breed and we run it. And there it is now not very amazing right. But the fact is that this is now a JavaScript app means we have no restrictions about what we can do and let's take a look at that HTML. That's it. It easily fits in a screen right and the basic steps are not crazy right. It's basically we create an import for our photo. We add an event listener that says when you change the photo call the read function. The read function says create a file reader read the file and when you finished loading call loaded and then loaded says fetch that. Now that path there is that path there right. Except we're doing the full pets one. So this is basically just copied and pasted from their sample. And then grab the JSON and then grab from the data the first thing the confidences the label and then set the HTML. So as you can see it's like OK if you haven't used JavaScript before these are all new things right but they're not it's not harder than Python. It's just it's just another just another language to learn. And so from here you can start to build up right. So for example we've created a multi file version. So with the multi file version let me show you multi file choose. So we can now click a few. So we've got a new fee and ragdoll a basset hound and some kind of cat. I'm not much of a cat person so he chose four files and bang they've all been classified. Apparently it's a Bengal. I wouldn't know. There's a new found. So there's the multi file version and if you look at the code it's not much more right. It's now just doing it's getting all the files and mapping them to read and now appending each one. So not much more code at all. And as you might have seen on our site here there's a few more examples which is some of the community during the week has created their own versions. So this one here is I think this is yeah this is from one of the radio guys. They called it get to know your pet. So if I choose a pet I kind of I really like this because it actually combines two models. So first of all it says oh it's a basset hound and then it lets me type in and ask things about it so I can say oh what kind of tail does it have. Search and so that's now going to call an NLP model which asks about this. It's a curved saber tail. There we go. What maintenance does it need. So again like here you can kind of see how a basset hound's ears must be cleaned and set it out frequently. So this is like combining models so you can see this is something that you couldn't do with just a kind of a ready to go interface. And so the next thing I wanted to point out is how did we create the website that I showed you how to create an HTML file. But like how do you create those and how do you make a website out of them. Well watch this. Let's here's here's a here's the source code to our most basic version. So I could just save this. There we go. Okay so we could open that with Visual Studio code. And what we could actually do is we could just use an explorer or Mac and finder. I could just double click on it. And here it is. It's a working app. So you can see I don't need any software installed on my computer to use a JavaScript app. It's a single file. I just run it in a browser. A browser is our complete execution environment. It's got a debugger. It's got the whole thing. So here you can see it's just calling out to this external hugging faces end point so I can do it all sitting here on my computer. So once I've got my HTML file that's working fine on my computer in VS code how do I then put it on the web so that other people can use it. Again the whole thing's free. There's a really cool thing called GitHub pages which basically will host your website for you. And because it's just JavaScript it'll all work just fine. The easiest way to create a GitHub pages site in my opinion is to use something called fast pages which is a fast AI thing. And basically all you do is you follow the setup process. So first it does it. Let's just go through it. So it says generate a copy by clicking on this link. So I click the link. All right. Okay give it a name. I try to make everything public. I always think it's good practice. You don't have to create repo. Generating okay. And then there's basically two more steps. It takes about five minutes. We don't have five minutes so I'll show you the one that I've already built which is fast AI slash tiny pets. And so once it's done you'll basically end up with this empty site which again you just go code open with GitHub desktop or open with Visual Studio whatever. So open with GitHub desktop or you can copy and paste this to your terminal. And so any one of those is going to get you this whole thing on your computer. You can save your HTML files there push it back up to GitHub. And what you'll find is we've we've real fast pages will show you the link to the website that is created for you. Now the website that's created for you you can make it look however you want using something called a theme. So you'll see it's created a file called config dot yaml where you can pick a theme. So in this case I picked a theme called Alembic for no particular reason. So GitHub pages uses something called Jekyll. And so any Jekyll theme will basically work. So I picked out this theme. And so as a result when I now save things into this repo they will automatically appear in this website and the files automatically appear up here in this list. So if you look at my index that's the home page the entire file is just this. The only slightly weird thing is at the top of every GitHub pages file you have to have three dashes title and layout and three dashes. It's called front matter. And so once you do that and save it it will appear in your website. So something else I did then I was like okay well that's all very well that Fastai has created this website but I don't really like what it looks like. I want to create a different version. No worries. You can go to Fastai tiny pets and click fork. And when you click fork it's going to create your own copy. So I did that under my personal account which is JP H00. And look now I've got my own version of it and now I can make changes here. So I made a few changes. One change I made was I went to config.yml and I changed the theme to pages themes hacker. So once you fork one thing you have to do which normally FastPages does for you is you do have to go to settings and click pages and actually enable GitHub pages. So you basically have to by default it's turned off. So here you'll just have to turn it on. So use the master branch root save and then it'll say no worries it's ready to be published. And so I changed the config.yml file to point at a different theme. And so if you look at now the JP H's tiny pets it's different. OK. It's got the same info but it's much more hackerish because JP H00 is a serious hacker as you can tell from his website. So anyway look it's a very brief taste of this kind of world of JavaScript and websites and so forth. But I wanted to give you a sense of like you know you don't need any money. You don't need any ideas. You know you don't really need much code to get started with writing your own web apps. And thanks to hugging face spaces you know they'll even host your model for you. And all you need to do is just have the magic string as a thing to call. OK. So signing out hacker Jeremy Howard. Thanks very much for watching. And in the next lesson we're going to be digging into some natural language processing. We're going to be doing some of the same stuff but we're going to be doing it with language rather than pictures. And we're going to be diving under the hood to see how these models actually work. We're going to learn about things like stochastic gradient descent and we might even be having to brush off a little bit of calculus. I hope I haven't put you off by saying the C word. I will see you next time. Thanks all.aked", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 4.22, "text": " Hi everybody, welcome to lesson two.", "tokens": [2421, 2201, 11, 2928, 281, 6898, 732, 13], "temperature": 0.0, "avg_logprob": -0.22866200852668148, "compression_ratio": 1.5377358490566038, "no_speech_prob": 0.025925694033503532}, {"id": 1, "seek": 0, "start": 4.22, "end": 7.0, "text": " Thanks for coming back.", "tokens": [2561, 337, 1348, 646, 13], "temperature": 0.0, "avg_logprob": -0.22866200852668148, "compression_ratio": 1.5377358490566038, "no_speech_prob": 0.025925694033503532}, {"id": 2, "seek": 0, "start": 7.0, "end": 8.14, "text": " Slight change of environment here.", "tokens": [318, 2764, 1319, 295, 2823, 510, 13], "temperature": 0.0, "avg_logprob": -0.22866200852668148, "compression_ratio": 1.5377358490566038, "no_speech_prob": 0.025925694033503532}, {"id": 3, "seek": 0, "start": 8.14, "end": 11.92, "text": " We had a bit of an administrative issue at our university.", "tokens": [492, 632, 257, 857, 295, 364, 17900, 2734, 412, 527, 5454, 13], "temperature": 0.0, "avg_logprob": -0.22866200852668148, "compression_ratio": 1.5377358490566038, "no_speech_prob": 0.025925694033503532}, {"id": 4, "seek": 0, "start": 11.92, "end": 13.64, "text": " Somebody booked our room.", "tokens": [13463, 26735, 527, 1808, 13], "temperature": 0.0, "avg_logprob": -0.22866200852668148, "compression_ratio": 1.5377358490566038, "no_speech_prob": 0.025925694033503532}, {"id": 5, "seek": 0, "start": 13.64, "end": 20.04, "text": " So I'm doing this from the study at home.", "tokens": [407, 286, 478, 884, 341, 490, 264, 2979, 412, 1280, 13], "temperature": 0.0, "avg_logprob": -0.22866200852668148, "compression_ratio": 1.5377358490566038, "no_speech_prob": 0.025925694033503532}, {"id": 6, "seek": 0, "start": 20.04, "end": 25.76, "text": " So sorry about the lack of decorations behind me.", "tokens": [407, 2597, 466, 264, 5011, 295, 32367, 2261, 385, 13], "temperature": 0.0, "avg_logprob": -0.22866200852668148, "compression_ratio": 1.5377358490566038, "no_speech_prob": 0.025925694033503532}, {"id": 7, "seek": 0, "start": 25.76, "end": 29.32, "text": " I'm actually really, really pumped about this lesson.", "tokens": [286, 478, 767, 534, 11, 534, 27774, 466, 341, 6898, 13], "temperature": 0.0, "avg_logprob": -0.22866200852668148, "compression_ratio": 1.5377358490566038, "no_speech_prob": 0.025925694033503532}, {"id": 8, "seek": 2932, "start": 29.32, "end": 34.76, "text": " It feels like going back to what things were like in the very early days because we're", "tokens": [467, 3417, 411, 516, 646, 281, 437, 721, 645, 411, 294, 264, 588, 2440, 1708, 570, 321, 434], "temperature": 0.0, "avg_logprob": -0.13964847217906606, "compression_ratio": 1.6962025316455696, "no_speech_prob": 7.840035686967894e-05}, {"id": 9, "seek": 2932, "start": 34.76, "end": 42.36, "text": " doing like some really new, really cool stuff, which, you know, stuff that hasn't really", "tokens": [884, 411, 512, 534, 777, 11, 534, 1627, 1507, 11, 597, 11, 291, 458, 11, 1507, 300, 6132, 380, 534], "temperature": 0.0, "avg_logprob": -0.13964847217906606, "compression_ratio": 1.6962025316455696, "no_speech_prob": 7.840035686967894e-05}, {"id": 10, "seek": 2932, "start": 42.36, "end": 43.92, "text": " been in courses like this before.", "tokens": [668, 294, 7712, 411, 341, 949, 13], "temperature": 0.0, "avg_logprob": -0.13964847217906606, "compression_ratio": 1.6962025316455696, "no_speech_prob": 7.840035686967894e-05}, {"id": 11, "seek": 2932, "start": 43.92, "end": 48.16, "text": " So I'm super, super excited.", "tokens": [407, 286, 478, 1687, 11, 1687, 2919, 13], "temperature": 0.0, "avg_logprob": -0.13964847217906606, "compression_ratio": 1.6962025316455696, "no_speech_prob": 7.840035686967894e-05}, {"id": 12, "seek": 2932, "start": 48.16, "end": 52.6, "text": " So thanks a lot for coming back after lesson one and I hope it's worth you coming back.", "tokens": [407, 3231, 257, 688, 337, 1348, 646, 934, 6898, 472, 293, 286, 1454, 309, 311, 3163, 291, 1348, 646, 13], "temperature": 0.0, "avg_logprob": -0.13964847217906606, "compression_ratio": 1.6962025316455696, "no_speech_prob": 7.840035686967894e-05}, {"id": 13, "seek": 2932, "start": 52.6, "end": 54.28, "text": " I think you're going to love it.", "tokens": [286, 519, 291, 434, 516, 281, 959, 309, 13], "temperature": 0.0, "avg_logprob": -0.13964847217906606, "compression_ratio": 1.6962025316455696, "no_speech_prob": 7.840035686967894e-05}, {"id": 14, "seek": 2932, "start": 54.28, "end": 57.96, "text": " I am, yeah, I'm really excited about this.", "tokens": [286, 669, 11, 1338, 11, 286, 478, 534, 2919, 466, 341, 13], "temperature": 0.0, "avg_logprob": -0.13964847217906606, "compression_ratio": 1.6962025316455696, "no_speech_prob": 7.840035686967894e-05}, {"id": 15, "seek": 5796, "start": 57.96, "end": 62.64, "text": " Now remember that, you know, the course goes with the book.", "tokens": [823, 1604, 300, 11, 291, 458, 11, 264, 1164, 1709, 365, 264, 1446, 13], "temperature": 0.0, "avg_logprob": -0.19004568031855992, "compression_ratio": 1.7509881422924902, "no_speech_prob": 2.7105528715765104e-05}, {"id": 16, "seek": 5796, "start": 62.64, "end": 65.32000000000001, "text": " So be sure that you're following, I mean, not following along in the book because we're", "tokens": [407, 312, 988, 300, 291, 434, 3480, 11, 286, 914, 11, 406, 3480, 2051, 294, 264, 1446, 570, 321, 434], "temperature": 0.0, "avg_logprob": -0.19004568031855992, "compression_ratio": 1.7509881422924902, "no_speech_prob": 2.7105528715765104e-05}, {"id": 17, "seek": 5796, "start": 65.32000000000001, "end": 70.2, "text": " covering similar things in different directions, but like read the book as well.", "tokens": [10322, 2531, 721, 294, 819, 11095, 11, 457, 411, 1401, 264, 1446, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.19004568031855992, "compression_ratio": 1.7509881422924902, "no_speech_prob": 2.7105528715765104e-05}, {"id": 18, "seek": 5796, "start": 70.2, "end": 74.04, "text": " And remember the book is entirely available for free as well.", "tokens": [400, 1604, 264, 1446, 307, 7696, 2435, 337, 1737, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.19004568031855992, "compression_ratio": 1.7509881422924902, "no_speech_prob": 2.7105528715765104e-05}, {"id": 19, "seek": 5796, "start": 74.04, "end": 80.12, "text": " You can go to the Fast AI, Fast Book repo to see the notebooks or through course.fast.ai.", "tokens": [509, 393, 352, 281, 264, 15968, 7318, 11, 15968, 9476, 49040, 281, 536, 264, 43782, 420, 807, 1164, 13, 7011, 13, 1301, 13], "temperature": 0.0, "avg_logprob": -0.19004568031855992, "compression_ratio": 1.7509881422924902, "no_speech_prob": 2.7105528715765104e-05}, {"id": 20, "seek": 5796, "start": 80.12, "end": 86.52000000000001, "text": " You can read it there through example, through, through Colab.", "tokens": [509, 393, 1401, 309, 456, 807, 1365, 11, 807, 11, 807, 4004, 455, 13], "temperature": 0.0, "avg_logprob": -0.19004568031855992, "compression_ratio": 1.7509881422924902, "no_speech_prob": 2.7105528715765104e-05}, {"id": 21, "seek": 8652, "start": 86.52, "end": 94.19999999999999, "text": " And also remember that the book, I mean, the book's got a lot of stuff that we didn't cover", "tokens": [400, 611, 1604, 300, 264, 1446, 11, 286, 914, 11, 264, 1446, 311, 658, 257, 688, 295, 1507, 300, 321, 994, 380, 2060], "temperature": 0.0, "avg_logprob": -0.13527874634644696, "compression_ratio": 1.6942148760330578, "no_speech_prob": 1.2029422578052618e-05}, {"id": 22, "seek": 8652, "start": 94.19999999999999, "end": 98.11999999999999, "text": " in the course, like, you know, stuff I find pretty interesting about the history of neural", "tokens": [294, 264, 1164, 11, 411, 11, 291, 458, 11, 1507, 286, 915, 1238, 1880, 466, 264, 2503, 295, 18161], "temperature": 0.0, "avg_logprob": -0.13527874634644696, "compression_ratio": 1.6942148760330578, "no_speech_prob": 1.2029422578052618e-05}, {"id": 23, "seek": 8652, "start": 98.11999999999999, "end": 105.12, "text": " networks, some of which has some really interesting personal stories actually, as you'll read", "tokens": [9590, 11, 512, 295, 597, 575, 512, 534, 1880, 2973, 3676, 767, 11, 382, 291, 603, 1401], "temperature": 0.0, "avg_logprob": -0.13527874634644696, "compression_ratio": 1.6942148760330578, "no_speech_prob": 1.2029422578052618e-05}, {"id": 24, "seek": 8652, "start": 105.12, "end": 106.16, "text": " here.", "tokens": [510, 13], "temperature": 0.0, "avg_logprob": -0.13527874634644696, "compression_ratio": 1.6942148760330578, "no_speech_prob": 1.2029422578052618e-05}, {"id": 25, "seek": 8652, "start": 106.16, "end": 109.39999999999999, "text": " And at the end of each chapter, there is a quiz.", "tokens": [400, 412, 264, 917, 295, 1184, 7187, 11, 456, 307, 257, 15450, 13], "temperature": 0.0, "avg_logprob": -0.13527874634644696, "compression_ratio": 1.6942148760330578, "no_speech_prob": 1.2029422578052618e-05}, {"id": 26, "seek": 8652, "start": 109.39999999999999, "end": 115.22, "text": " And remember, it's not a bad idea before you watch the video to read the quiz.", "tokens": [400, 1604, 11, 309, 311, 406, 257, 1578, 1558, 949, 291, 1159, 264, 960, 281, 1401, 264, 15450, 13], "temperature": 0.0, "avg_logprob": -0.13527874634644696, "compression_ratio": 1.6942148760330578, "no_speech_prob": 1.2029422578052618e-05}, {"id": 27, "seek": 11522, "start": 115.22, "end": 119.0, "text": " So if you want to read the chapter two quiz, you know, and then come back, that's not a", "tokens": [407, 498, 291, 528, 281, 1401, 264, 7187, 732, 15450, 11, 291, 458, 11, 293, 550, 808, 646, 11, 300, 311, 406, 257], "temperature": 0.0, "avg_logprob": -0.11840014024214311, "compression_ratio": 1.7762237762237763, "no_speech_prob": 4.222802999720443e-06}, {"id": 28, "seek": 11522, "start": 119.0, "end": 120.6, "text": " bad idea.", "tokens": [1578, 1558, 13], "temperature": 0.0, "avg_logprob": -0.11840014024214311, "compression_ratio": 1.7762237762237763, "no_speech_prob": 4.222802999720443e-06}, {"id": 29, "seek": 11522, "start": 120.6, "end": 124.03999999999999, "text": " And then make sure that you can do the quiz after you've watched the video and you've", "tokens": [400, 550, 652, 988, 300, 291, 393, 360, 264, 15450, 934, 291, 600, 6337, 264, 960, 293, 291, 600], "temperature": 0.0, "avg_logprob": -0.11840014024214311, "compression_ratio": 1.7762237762237763, "no_speech_prob": 4.222802999720443e-06}, {"id": 30, "seek": 11522, "start": 124.03999999999999, "end": 127.88, "text": " read chapter two of the book.", "tokens": [1401, 7187, 732, 295, 264, 1446, 13], "temperature": 0.0, "avg_logprob": -0.11840014024214311, "compression_ratio": 1.7762237762237763, "no_speech_prob": 4.222802999720443e-06}, {"id": 31, "seek": 11522, "start": 127.88, "end": 131.76, "text": " Something I didn't mention last week is there's also a very cool thing that Radek, who I mentioned", "tokens": [6595, 286, 994, 380, 2152, 1036, 1243, 307, 456, 311, 611, 257, 588, 1627, 551, 300, 9654, 916, 11, 567, 286, 2835], "temperature": 0.0, "avg_logprob": -0.11840014024214311, "compression_ratio": 1.7762237762237763, "no_speech_prob": 4.222802999720443e-06}, {"id": 32, "seek": 11522, "start": 131.76, "end": 137.76, "text": " last week, has written called aiquizzes.com, which is a site specifically for quizzes about", "tokens": [1036, 1243, 11, 575, 3720, 1219, 9783, 358, 8072, 279, 13, 1112, 11, 597, 307, 257, 3621, 4682, 337, 48955, 466], "temperature": 0.0, "avg_logprob": -0.11840014024214311, "compression_ratio": 1.7762237762237763, "no_speech_prob": 4.222802999720443e-06}, {"id": 33, "seek": 11522, "start": 137.76, "end": 139.56, "text": " the book.", "tokens": [264, 1446, 13], "temperature": 0.0, "avg_logprob": -0.11840014024214311, "compression_ratio": 1.7762237762237763, "no_speech_prob": 4.222802999720443e-06}, {"id": 34, "seek": 11522, "start": 139.56, "end": 144.52, "text": " And it actually uses repetitive space learning techniques to make sure that you never forget.", "tokens": [400, 309, 767, 4960, 29404, 1901, 2539, 7512, 281, 652, 988, 300, 291, 1128, 2870, 13], "temperature": 0.0, "avg_logprob": -0.11840014024214311, "compression_ratio": 1.7762237762237763, "no_speech_prob": 4.222802999720443e-06}, {"id": 35, "seek": 14452, "start": 144.52, "end": 147.16, "text": " So do check out aiquizzes.com.", "tokens": [407, 360, 1520, 484, 9783, 358, 8072, 279, 13, 1112, 13], "temperature": 0.0, "avg_logprob": -0.1224544828183183, "compression_ratio": 1.693069306930693, "no_speech_prob": 1.1659323718049563e-05}, {"id": 36, "seek": 14452, "start": 147.16, "end": 149.28, "text": " It's all brand new questions.", "tokens": [467, 311, 439, 3360, 777, 1651, 13], "temperature": 0.0, "avg_logprob": -0.1224544828183183, "compression_ratio": 1.693069306930693, "no_speech_prob": 1.1659323718049563e-05}, {"id": 37, "seek": 14452, "start": 149.28, "end": 153.52, "text": " They're different to the ones in the book and they're really nicely curated and put", "tokens": [814, 434, 819, 281, 264, 2306, 294, 264, 1446, 293, 436, 434, 534, 9594, 47851, 293, 829], "temperature": 0.0, "avg_logprob": -0.1224544828183183, "compression_ratio": 1.693069306930693, "no_speech_prob": 1.1659323718049563e-05}, {"id": 38, "seek": 14452, "start": 153.52, "end": 154.52, "text": " together.", "tokens": [1214, 13], "temperature": 0.0, "avg_logprob": -0.1224544828183183, "compression_ratio": 1.693069306930693, "no_speech_prob": 1.1659323718049563e-05}, {"id": 39, "seek": 14452, "start": 154.52, "end": 159.48000000000002, "text": " So check out aiquizzes.com as well.", "tokens": [407, 1520, 484, 9783, 358, 8072, 279, 13, 1112, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.1224544828183183, "compression_ratio": 1.693069306930693, "no_speech_prob": 1.1659323718049563e-05}, {"id": 40, "seek": 14452, "start": 159.48000000000002, "end": 165.04000000000002, "text": " Remember as well as course.fast.ai, there's also forums.fast.ai.", "tokens": [5459, 382, 731, 382, 1164, 13, 7011, 13, 1301, 11, 456, 311, 611, 26998, 13, 7011, 13, 1301, 13], "temperature": 0.0, "avg_logprob": -0.1224544828183183, "compression_ratio": 1.693069306930693, "no_speech_prob": 1.1659323718049563e-05}, {"id": 41, "seek": 14452, "start": 165.04000000000002, "end": 170.92000000000002, "text": " So course.fast.ai is where you want to go to get, you know, links to all the notebooks", "tokens": [407, 1164, 13, 7011, 13, 1301, 307, 689, 291, 528, 281, 352, 281, 483, 11, 291, 458, 11, 6123, 281, 439, 264, 43782], "temperature": 0.0, "avg_logprob": -0.1224544828183183, "compression_ratio": 1.693069306930693, "no_speech_prob": 1.1659323718049563e-05}, {"id": 42, "seek": 17092, "start": 170.92, "end": 175.6, "text": " and Kaggle stuff and all that stuff.", "tokens": [293, 48751, 22631, 1507, 293, 439, 300, 1507, 13], "temperature": 0.0, "avg_logprob": -0.1688492784222353, "compression_ratio": 1.6936936936936937, "no_speech_prob": 7.527843536081491e-06}, {"id": 43, "seek": 17092, "start": 175.6, "end": 181.76, "text": " You'll also find on forums.fast.ai every lesson has an official topic, you know, with all", "tokens": [509, 603, 611, 915, 322, 26998, 13, 7011, 13, 1301, 633, 6898, 575, 364, 4783, 4829, 11, 291, 458, 11, 365, 439], "temperature": 0.0, "avg_logprob": -0.1688492784222353, "compression_ratio": 1.6936936936936937, "no_speech_prob": 7.527843536081491e-06}, {"id": 44, "seek": 17092, "start": 181.76, "end": 184.51999999999998, "text": " the information you'll need.", "tokens": [264, 1589, 291, 603, 643, 13], "temperature": 0.0, "avg_logprob": -0.1688492784222353, "compression_ratio": 1.6936936936936937, "no_speech_prob": 7.527843536081491e-06}, {"id": 45, "seek": 17092, "start": 184.51999999999998, "end": 187.04, "text": " Generally there'll be a bit more info on the forums.", "tokens": [21082, 456, 603, 312, 257, 857, 544, 13614, 322, 264, 26998, 13], "temperature": 0.0, "avg_logprob": -0.1688492784222353, "compression_ratio": 1.6936936936936937, "no_speech_prob": 7.527843536081491e-06}, {"id": 46, "seek": 17092, "start": 187.04, "end": 194.01999999999998, "text": " We try to keep the course lean and mean and the forums are a bit more detailed.", "tokens": [492, 853, 281, 1066, 264, 1164, 11659, 293, 914, 293, 264, 26998, 366, 257, 857, 544, 9942, 13], "temperature": 0.0, "avg_logprob": -0.1688492784222353, "compression_ratio": 1.6936936936936937, "no_speech_prob": 7.527843536081491e-06}, {"id": 47, "seek": 17092, "start": 194.01999999999998, "end": 198.39999999999998, "text": " So if you find, well in this case, you'd want to look at the lesson two official topic,", "tokens": [407, 498, 291, 915, 11, 731, 294, 341, 1389, 11, 291, 1116, 528, 281, 574, 412, 264, 6898, 732, 4783, 4829, 11], "temperature": 0.0, "avg_logprob": -0.1688492784222353, "compression_ratio": 1.6936936936936937, "no_speech_prob": 7.527843536081491e-06}, {"id": 48, "seek": 19840, "start": 198.4, "end": 205.04000000000002, "text": " but here's the lesson one official topic so far.", "tokens": [457, 510, 311, 264, 6898, 472, 4783, 4829, 370, 1400, 13], "temperature": 0.0, "avg_logprob": -0.12315764478457872, "compression_ratio": 1.6968325791855203, "no_speech_prob": 7.411128081002971e-06}, {"id": 49, "seek": 19840, "start": 205.04000000000002, "end": 212.68, "text": " So from the lesson one official topic, already after just a few days since I recorded it,", "tokens": [407, 490, 264, 6898, 472, 4783, 4829, 11, 1217, 934, 445, 257, 1326, 1708, 1670, 286, 8287, 309, 11], "temperature": 0.0, "avg_logprob": -0.12315764478457872, "compression_ratio": 1.6968325791855203, "no_speech_prob": 7.411128081002971e-06}, {"id": 50, "seek": 19840, "start": 212.68, "end": 215.92000000000002, "text": " we haven't even launched the course, so it's just the people doing it live.", "tokens": [321, 2378, 380, 754, 8730, 264, 1164, 11, 370, 309, 311, 445, 264, 561, 884, 309, 1621, 13], "temperature": 0.0, "avg_logprob": -0.12315764478457872, "compression_ratio": 1.6968325791855203, "no_speech_prob": 7.411128081002971e-06}, {"id": 51, "seek": 19840, "start": 215.92000000000002, "end": 220.32, "text": " There's already a lot of replies and that can get pretty overwhelming.", "tokens": [821, 311, 1217, 257, 688, 295, 42289, 293, 300, 393, 483, 1238, 13373, 13], "temperature": 0.0, "avg_logprob": -0.12315764478457872, "compression_ratio": 1.6968325791855203, "no_speech_prob": 7.411128081002971e-06}, {"id": 52, "seek": 19840, "start": 220.32, "end": 228.24, "text": " So be aware that there's a button at the bottom of my post that says summarize this topic", "tokens": [407, 312, 3650, 300, 456, 311, 257, 2960, 412, 264, 2767, 295, 452, 2183, 300, 1619, 20858, 341, 4829], "temperature": 0.0, "avg_logprob": -0.12315764478457872, "compression_ratio": 1.6968325791855203, "no_speech_prob": 7.411128081002971e-06}, {"id": 53, "seek": 22824, "start": 228.24, "end": 234.16, "text": " and if you hit that, then you'll just see the most upvoted replies and that's a really", "tokens": [293, 498, 291, 2045, 300, 11, 550, 291, 603, 445, 536, 264, 881, 493, 85, 23325, 42289, 293, 300, 311, 257, 534], "temperature": 0.0, "avg_logprob": -0.14810655631271064, "compression_ratio": 1.7281553398058251, "no_speech_prob": 1.6186964785447344e-05}, {"id": 54, "seek": 22824, "start": 234.16, "end": 238.8, "text": " good way to just make sure that you hit on the main stuff.", "tokens": [665, 636, 281, 445, 652, 988, 300, 291, 2045, 322, 264, 2135, 1507, 13], "temperature": 0.0, "avg_logprob": -0.14810655631271064, "compression_ratio": 1.7281553398058251, "no_speech_prob": 1.6186964785447344e-05}, {"id": 55, "seek": 22824, "start": 238.8, "end": 241.72, "text": " So there's the button and here's what it looks like after you hit it.", "tokens": [407, 456, 311, 264, 2960, 293, 510, 311, 437, 309, 1542, 411, 934, 291, 2045, 309, 13], "temperature": 0.0, "avg_logprob": -0.14810655631271064, "compression_ratio": 1.7281553398058251, "no_speech_prob": 1.6186964785447344e-05}, {"id": 56, "seek": 22824, "start": 241.72, "end": 249.8, "text": " You'll just get the upvoted stuff from fast.ai legends like Sanyam and Tanishk.", "tokens": [509, 603, 445, 483, 264, 493, 85, 23325, 1507, 490, 2370, 13, 1301, 27695, 411, 318, 1325, 335, 293, 314, 7524, 74, 13], "temperature": 0.0, "avg_logprob": -0.14810655631271064, "compression_ratio": 1.7281553398058251, "no_speech_prob": 1.6186964785447344e-05}, {"id": 57, "seek": 22824, "start": 249.8, "end": 254.68, "text": " So hopefully you'll find that a useful way to use the forum.", "tokens": [407, 4696, 291, 603, 915, 300, 257, 4420, 636, 281, 764, 264, 17542, 13], "temperature": 0.0, "avg_logprob": -0.14810655631271064, "compression_ratio": 1.7281553398058251, "no_speech_prob": 1.6186964785447344e-05}, {"id": 58, "seek": 25468, "start": 254.68, "end": 261.32, "text": " So one of the cool things about this week is I, as promised, put up the show us what", "tokens": [407, 472, 295, 264, 1627, 721, 466, 341, 1243, 307, 286, 11, 382, 10768, 11, 829, 493, 264, 855, 505, 437], "temperature": 0.0, "avg_logprob": -0.1367109971887925, "compression_ratio": 1.509433962264151, "no_speech_prob": 4.9368854888598435e-06}, {"id": 59, "seek": 25468, "start": 261.32, "end": 267.64, "text": " you've made post and already a lot of people have posted.", "tokens": [291, 600, 1027, 2183, 293, 1217, 257, 688, 295, 561, 362, 9437, 13], "temperature": 0.0, "avg_logprob": -0.1367109971887925, "compression_ratio": 1.509433962264151, "no_speech_prob": 4.9368854888598435e-06}, {"id": 60, "seek": 25468, "start": 267.64, "end": 270.24, "text": " I took the screenshot a few days ago.", "tokens": [286, 1890, 264, 27712, 257, 1326, 1708, 2057, 13], "temperature": 0.0, "avg_logprob": -0.1367109971887925, "compression_ratio": 1.509433962264151, "no_speech_prob": 4.9368854888598435e-06}, {"id": 61, "seek": 25468, "start": 270.24, "end": 274.16, "text": " It's way above 39 replies already if I remember correctly.", "tokens": [467, 311, 636, 3673, 15238, 42289, 1217, 498, 286, 1604, 8944, 13], "temperature": 0.0, "avg_logprob": -0.1367109971887925, "compression_ratio": 1.509433962264151, "no_speech_prob": 4.9368854888598435e-06}, {"id": 62, "seek": 25468, "start": 274.16, "end": 279.26, "text": " I had a lot of trouble deciding which ones to share because they're all so good.", "tokens": [286, 632, 257, 688, 295, 5253, 17990, 597, 2306, 281, 2073, 570, 436, 434, 439, 370, 665, 13], "temperature": 0.0, "avg_logprob": -0.1367109971887925, "compression_ratio": 1.509433962264151, "no_speech_prob": 4.9368854888598435e-06}, {"id": 63, "seek": 27926, "start": 279.26, "end": 286.32, "text": " So I've actually decided to kind of, you know, went the easy route and I just picked the", "tokens": [407, 286, 600, 767, 3047, 281, 733, 295, 11, 291, 458, 11, 1437, 264, 1858, 7955, 293, 286, 445, 6183, 264], "temperature": 0.0, "avg_logprob": -0.15407990207191274, "compression_ratio": 1.7098039215686274, "no_speech_prob": 4.425441602506908e-06}, {"id": 64, "seek": 27926, "start": 286.32, "end": 287.32, "text": " first.", "tokens": [700, 13], "temperature": 0.0, "avg_logprob": -0.15407990207191274, "compression_ratio": 1.7098039215686274, "no_speech_prob": 4.425441602506908e-06}, {"id": 65, "seek": 27926, "start": 287.32, "end": 290.96, "text": " So I'm just going to show you the first ones that were posted because they're also good.", "tokens": [407, 286, 478, 445, 516, 281, 855, 291, 264, 700, 2306, 300, 645, 9437, 570, 436, 434, 611, 665, 13], "temperature": 0.0, "avg_logprob": -0.15407990207191274, "compression_ratio": 1.7098039215686274, "no_speech_prob": 4.425441602506908e-06}, {"id": 66, "seek": 27926, "start": 290.96, "end": 298.58, "text": " So the first, the very, very first one to be posted is a damaged car classifier.", "tokens": [407, 264, 700, 11, 264, 588, 11, 588, 700, 472, 281, 312, 9437, 307, 257, 14080, 1032, 1508, 9902, 13], "temperature": 0.0, "avg_logprob": -0.15407990207191274, "compression_ratio": 1.7098039215686274, "no_speech_prob": 4.425441602506908e-06}, {"id": 67, "seek": 27926, "start": 298.58, "end": 300.88, "text": " So that worked out pretty well it looks like.", "tokens": [407, 300, 2732, 484, 1238, 731, 309, 1542, 411, 13], "temperature": 0.0, "avg_logprob": -0.15407990207191274, "compression_ratio": 1.7098039215686274, "no_speech_prob": 4.425441602506908e-06}, {"id": 68, "seek": 27926, "start": 300.88, "end": 307.52, "text": " And I really liked what Matt, the creator said about this is that, you know, wow, it's", "tokens": [400, 286, 534, 4501, 437, 7397, 11, 264, 14181, 848, 466, 341, 307, 300, 11, 291, 458, 11, 6076, 11, 309, 311], "temperature": 0.0, "avg_logprob": -0.15407990207191274, "compression_ratio": 1.7098039215686274, "no_speech_prob": 4.425441602506908e-06}, {"id": 69, "seek": 27926, "start": 307.52, "end": 308.8, "text": " a bit uncomfortable to run this code.", "tokens": [257, 857, 10532, 281, 1190, 341, 3089, 13], "temperature": 0.0, "avg_logprob": -0.15407990207191274, "compression_ratio": 1.7098039215686274, "no_speech_prob": 4.425441602506908e-06}, {"id": 70, "seek": 30880, "start": 308.8, "end": 312.08, "text": " I don't really understand yet, but I'm just doing it.", "tokens": [286, 500, 380, 534, 1223, 1939, 11, 457, 286, 478, 445, 884, 309, 13], "temperature": 0.0, "avg_logprob": -0.13294680797270614, "compression_ratio": 1.6955017301038062, "no_speech_prob": 1.4063602066016756e-05}, {"id": 71, "seek": 30880, "start": 312.08, "end": 316.04, "text": " And so I'm like, yeah, good on you, Matt, for just, for just doing it.", "tokens": [400, 370, 286, 478, 411, 11, 1338, 11, 665, 322, 291, 11, 7397, 11, 337, 445, 11, 337, 445, 884, 309, 13], "temperature": 0.0, "avg_logprob": -0.13294680797270614, "compression_ratio": 1.6955017301038062, "no_speech_prob": 1.4063602066016756e-05}, {"id": 72, "seek": 30880, "start": 316.04, "end": 317.04, "text": " That's the way to get started.", "tokens": [663, 311, 264, 636, 281, 483, 1409, 13], "temperature": 0.0, "avg_logprob": -0.13294680797270614, "compression_ratio": 1.6955017301038062, "no_speech_prob": 1.4063602066016756e-05}, {"id": 73, "seek": 30880, "start": 317.04, "end": 318.04, "text": " It's all going to make sense.", "tokens": [467, 311, 439, 516, 281, 652, 2020, 13], "temperature": 0.0, "avg_logprob": -0.13294680797270614, "compression_ratio": 1.6955017301038062, "no_speech_prob": 1.4063602066016756e-05}, {"id": 74, "seek": 30880, "start": 318.04, "end": 320.78000000000003, "text": " Don't worry.", "tokens": [1468, 380, 3292, 13], "temperature": 0.0, "avg_logprob": -0.13294680797270614, "compression_ratio": 1.6955017301038062, "no_speech_prob": 1.4063602066016756e-05}, {"id": 75, "seek": 30880, "start": 320.78000000000003, "end": 326.48, "text": " Very nice to see that the next one posted was actually a blog post in fast pages.", "tokens": [4372, 1481, 281, 536, 300, 264, 958, 472, 9437, 390, 767, 257, 6968, 2183, 294, 2370, 7183, 13], "temperature": 0.0, "avg_logprob": -0.13294680797270614, "compression_ratio": 1.6955017301038062, "no_speech_prob": 1.4063602066016756e-05}, {"id": 76, "seek": 30880, "start": 326.48, "end": 327.48, "text": " Very nice to see.", "tokens": [4372, 1481, 281, 536, 13], "temperature": 0.0, "avg_logprob": -0.13294680797270614, "compression_ratio": 1.6955017301038062, "no_speech_prob": 1.4063602066016756e-05}, {"id": 77, "seek": 30880, "start": 327.48, "end": 331.68, "text": " So just describing some stuff, some experiments that they ran over the week and what did they", "tokens": [407, 445, 16141, 512, 1507, 11, 512, 12050, 300, 436, 5872, 670, 264, 1243, 293, 437, 630, 436], "temperature": 0.0, "avg_logprob": -0.13294680797270614, "compression_ratio": 1.6955017301038062, "no_speech_prob": 1.4063602066016756e-05}, {"id": 78, "seek": 30880, "start": 331.68, "end": 333.72, "text": " find?", "tokens": [915, 30], "temperature": 0.0, "avg_logprob": -0.13294680797270614, "compression_ratio": 1.6955017301038062, "no_speech_prob": 1.4063602066016756e-05}, {"id": 79, "seek": 30880, "start": 333.72, "end": 338.14, "text": " Next one was the amazing beard detector, which if I understand correctly was mainly because", "tokens": [3087, 472, 390, 264, 2243, 17455, 25712, 11, 597, 498, 286, 1223, 8944, 390, 8704, 570], "temperature": 0.0, "avg_logprob": -0.13294680797270614, "compression_ratio": 1.6955017301038062, "no_speech_prob": 1.4063602066016756e-05}, {"id": 80, "seek": 33814, "start": 338.14, "end": 342.84, "text": " it's very easy to get from bird to beard by just changing one letter to two.", "tokens": [309, 311, 588, 1858, 281, 483, 490, 5255, 281, 17455, 538, 445, 4473, 472, 5063, 281, 732, 13], "temperature": 0.0, "avg_logprob": -0.19209709167480468, "compression_ratio": 1.5146443514644352, "no_speech_prob": 5.173855242901482e-06}, {"id": 81, "seek": 33814, "start": 342.84, "end": 346.84, "text": " And this is doing a very good job of finding gentlemen with beards.", "tokens": [400, 341, 307, 884, 257, 588, 665, 1691, 295, 5006, 11669, 365, 312, 2287, 13], "temperature": 0.0, "avg_logprob": -0.19209709167480468, "compression_ratio": 1.5146443514644352, "no_speech_prob": 5.173855242901482e-06}, {"id": 82, "seek": 33814, "start": 346.84, "end": 349.08, "text": " So very nice.", "tokens": [407, 588, 1481, 13], "temperature": 0.0, "avg_logprob": -0.19209709167480468, "compression_ratio": 1.5146443514644352, "no_speech_prob": 5.173855242901482e-06}, {"id": 83, "seek": 33814, "start": 349.08, "end": 352.68, "text": " And then this one is another level again.", "tokens": [400, 550, 341, 472, 307, 1071, 1496, 797, 13], "temperature": 0.0, "avg_logprob": -0.19209709167480468, "compression_ratio": 1.5146443514644352, "no_speech_prob": 5.173855242901482e-06}, {"id": 84, "seek": 33814, "start": 352.68, "end": 360.2, "text": " It's a whole in production web app to classify food, which is kind of like extra credit.", "tokens": [467, 311, 257, 1379, 294, 4265, 3670, 724, 281, 33872, 1755, 11, 597, 307, 733, 295, 411, 2857, 5397, 13], "temperature": 0.0, "avg_logprob": -0.19209709167480468, "compression_ratio": 1.5146443514644352, "no_speech_prob": 5.173855242901482e-06}, {"id": 85, "seek": 33814, "start": 360.2, "end": 362.91999999999996, "text": " Apparently we're up to 80 replies now in that thread.", "tokens": [16755, 321, 434, 493, 281, 4688, 42289, 586, 294, 300, 7207, 13], "temperature": 0.0, "avg_logprob": -0.19209709167480468, "compression_ratio": 1.5146443514644352, "no_speech_prob": 5.173855242901482e-06}, {"id": 86, "seek": 33814, "start": 362.91999999999996, "end": 366.71999999999997, "text": " Thank you, Sanjan.", "tokens": [1044, 291, 11, 5271, 14763, 13], "temperature": 0.0, "avg_logprob": -0.19209709167480468, "compression_ratio": 1.5146443514644352, "no_speech_prob": 5.173855242901482e-06}, {"id": 87, "seek": 36672, "start": 366.72, "end": 370.84000000000003, "text": " Very cool.", "tokens": [4372, 1627, 13], "temperature": 0.0, "avg_logprob": -0.09795255246369736, "compression_ratio": 1.5728155339805825, "no_speech_prob": 7.183090019680094e-06}, {"id": 88, "seek": 36672, "start": 370.84000000000003, "end": 380.0, "text": " So you know, obviously, so this was actually created by Suvash who's been doing the courses", "tokens": [407, 291, 458, 11, 2745, 11, 370, 341, 390, 767, 2942, 538, 2746, 85, 1299, 567, 311, 668, 884, 264, 7712], "temperature": 0.0, "avg_logprob": -0.09795255246369736, "compression_ratio": 1.5728155339805825, "no_speech_prob": 7.183090019680094e-06}, {"id": 89, "seek": 36672, "start": 380.0, "end": 383.88000000000005, "text": " for a few years now, I believe.", "tokens": [337, 257, 1326, 924, 586, 11, 286, 1697, 13], "temperature": 0.0, "avg_logprob": -0.09795255246369736, "compression_ratio": 1.5728155339805825, "no_speech_prob": 7.183090019680094e-06}, {"id": 90, "seek": 36672, "start": 383.88000000000005, "end": 389.40000000000003, "text": " And so, you know, one day you too might be able to create your very own web app and put", "tokens": [400, 370, 11, 291, 458, 11, 472, 786, 291, 886, 1062, 312, 1075, 281, 1884, 428, 588, 1065, 3670, 724, 293, 829], "temperature": 0.0, "avg_logprob": -0.09795255246369736, "compression_ratio": 1.5728155339805825, "no_speech_prob": 7.183090019680094e-06}, {"id": 91, "seek": 36672, "start": 389.40000000000003, "end": 390.40000000000003, "text": " it in production.", "tokens": [309, 294, 4265, 13], "temperature": 0.0, "avg_logprob": -0.09795255246369736, "compression_ratio": 1.5728155339805825, "no_speech_prob": 7.183090019680094e-06}, {"id": 92, "seek": 36672, "start": 390.40000000000003, "end": 395.72, "text": " And when I say one day, more specifically today, I'm actually going to show you how", "tokens": [400, 562, 286, 584, 472, 786, 11, 544, 4682, 965, 11, 286, 478, 767, 516, 281, 855, 291, 577], "temperature": 0.0, "avg_logprob": -0.09795255246369736, "compression_ratio": 1.5728155339805825, "no_speech_prob": 7.183090019680094e-06}, {"id": 93, "seek": 39572, "start": 395.72, "end": 397.68, "text": " to do this right now.", "tokens": [281, 360, 341, 558, 586, 13], "temperature": 0.0, "avg_logprob": -0.16725270435063525, "compression_ratio": 1.627906976744186, "no_speech_prob": 1.1478414307930507e-05}, {"id": 94, "seek": 39572, "start": 397.68, "end": 402.6, "text": " So it's actually quite lucky coincidence that Suvash put this up there because it's exactly", "tokens": [407, 309, 311, 767, 1596, 6356, 22137, 300, 2746, 85, 1299, 829, 341, 493, 456, 570, 309, 311, 2293], "temperature": 0.0, "avg_logprob": -0.16725270435063525, "compression_ratio": 1.627906976744186, "no_speech_prob": 1.1478414307930507e-05}, {"id": 95, "seek": 39572, "start": 402.6, "end": 406.22, "text": " the topic that we're going to pick today.", "tokens": [264, 4829, 300, 321, 434, 516, 281, 1888, 965, 13], "temperature": 0.0, "avg_logprob": -0.16725270435063525, "compression_ratio": 1.627906976744186, "no_speech_prob": 1.1478414307930507e-05}, {"id": 96, "seek": 39572, "start": 406.22, "end": 411.8, "text": " So how do we go about putting a model in production?", "tokens": [407, 577, 360, 321, 352, 466, 3372, 257, 2316, 294, 4265, 30], "temperature": 0.0, "avg_logprob": -0.16725270435063525, "compression_ratio": 1.627906976744186, "no_speech_prob": 1.1478414307930507e-05}, {"id": 97, "seek": 39572, "start": 411.8, "end": 417.18, "text": " Step one is, well, you've kind of done step one, right?", "tokens": [5470, 472, 307, 11, 731, 11, 291, 600, 733, 295, 1096, 1823, 472, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.16725270435063525, "compression_ratio": 1.627906976744186, "no_speech_prob": 1.1478414307930507e-05}, {"id": 98, "seek": 39572, "start": 417.18, "end": 423.24, "text": " Step one is step one, two, three, four, is figure out what problem you want to solve,", "tokens": [5470, 472, 307, 1823, 472, 11, 732, 11, 1045, 11, 1451, 11, 307, 2573, 484, 437, 1154, 291, 528, 281, 5039, 11], "temperature": 0.0, "avg_logprob": -0.16725270435063525, "compression_ratio": 1.627906976744186, "no_speech_prob": 1.1478414307930507e-05}, {"id": 99, "seek": 42324, "start": 423.24, "end": 428.76, "text": " figure out how to find the data for it, gather some data and so forth.", "tokens": [2573, 484, 577, 281, 915, 264, 1412, 337, 309, 11, 5448, 512, 1412, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.12678865784580268, "compression_ratio": 1.6136363636363635, "no_speech_prob": 7.1829676926427055e-06}, {"id": 100, "seek": 42324, "start": 428.76, "end": 433.64, "text": " So what's the kind of first step after you've got your data?", "tokens": [407, 437, 311, 264, 733, 295, 700, 1823, 934, 291, 600, 658, 428, 1412, 30], "temperature": 0.0, "avg_logprob": -0.12678865784580268, "compression_ratio": 1.6136363636363635, "no_speech_prob": 7.1829676926427055e-06}, {"id": 101, "seek": 42324, "start": 433.64, "end": 435.88, "text": " The next step is data cleaning.", "tokens": [440, 958, 1823, 307, 1412, 8924, 13], "temperature": 0.0, "avg_logprob": -0.12678865784580268, "compression_ratio": 1.6136363636363635, "no_speech_prob": 7.1829676926427055e-06}, {"id": 102, "seek": 42324, "start": 435.88, "end": 444.1, "text": " And if you go to chapter two of the book, which I'm going to go ahead and open up now.", "tokens": [400, 498, 291, 352, 281, 7187, 732, 295, 264, 1446, 11, 597, 286, 478, 516, 281, 352, 2286, 293, 1269, 493, 586, 13], "temperature": 0.0, "avg_logprob": -0.12678865784580268, "compression_ratio": 1.6136363636363635, "no_speech_prob": 7.1829676926427055e-06}, {"id": 103, "seek": 42324, "start": 444.1, "end": 446.88, "text": " So here is the book.", "tokens": [407, 510, 307, 264, 1446, 13], "temperature": 0.0, "avg_logprob": -0.12678865784580268, "compression_ratio": 1.6136363636363635, "no_speech_prob": 7.1829676926427055e-06}, {"id": 104, "seek": 42324, "start": 446.88, "end": 451.96000000000004, "text": " So you can open it in Colab directly from the course or if you've cloned it to your", "tokens": [407, 291, 393, 1269, 309, 294, 4004, 455, 3838, 490, 264, 1164, 420, 498, 291, 600, 596, 19009, 309, 281, 428], "temperature": 0.0, "avg_logprob": -0.12678865784580268, "compression_ratio": 1.6136363636363635, "no_speech_prob": 7.1829676926427055e-06}, {"id": 105, "seek": 45196, "start": 451.96, "end": 454.68, "text": " computer or whatever, you can do it there.", "tokens": [3820, 420, 2035, 11, 291, 393, 360, 309, 456, 13], "temperature": 0.0, "avg_logprob": -0.1344635839815493, "compression_ratio": 1.70995670995671, "no_speech_prob": 7.64643118600361e-06}, {"id": 106, "seek": 45196, "start": 454.68, "end": 460.41999999999996, "text": " So remember, course.fast.ai will run you through exactly how to run these notebooks.", "tokens": [407, 1604, 11, 1164, 13, 7011, 13, 1301, 486, 1190, 291, 807, 2293, 577, 281, 1190, 613, 43782, 13], "temperature": 0.0, "avg_logprob": -0.1344635839815493, "compression_ratio": 1.70995670995671, "no_speech_prob": 7.64643118600361e-06}, {"id": 107, "seek": 45196, "start": 460.41999999999996, "end": 464.59999999999997, "text": " And so you can see chapter two is all about putting stuff in production.", "tokens": [400, 370, 291, 393, 536, 7187, 732, 307, 439, 466, 3372, 1507, 294, 4265, 13], "temperature": 0.0, "avg_logprob": -0.1344635839815493, "compression_ratio": 1.70995670995671, "no_speech_prob": 7.64643118600361e-06}, {"id": 108, "seek": 45196, "start": 464.59999999999997, "end": 466.0, "text": " And so here is chapter two.", "tokens": [400, 370, 510, 307, 7187, 732, 13], "temperature": 0.0, "avg_logprob": -0.1344635839815493, "compression_ratio": 1.70995670995671, "no_speech_prob": 7.64643118600361e-06}, {"id": 109, "seek": 45196, "start": 466.0, "end": 467.0, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.1344635839815493, "compression_ratio": 1.70995670995671, "no_speech_prob": 7.64643118600361e-06}, {"id": 110, "seek": 45196, "start": 467.0, "end": 473.91999999999996, "text": " And so remember, we hit shift enter to run cells, okay, to execute them.", "tokens": [400, 370, 1604, 11, 321, 2045, 5513, 3242, 281, 1190, 5438, 11, 1392, 11, 281, 14483, 552, 13], "temperature": 0.0, "avg_logprob": -0.1344635839815493, "compression_ratio": 1.70995670995671, "no_speech_prob": 7.64643118600361e-06}, {"id": 111, "seek": 45196, "start": 473.91999999999996, "end": 479.91999999999996, "text": " And so we're going to go to the part of the book where we start cleaning the data.", "tokens": [400, 370, 321, 434, 516, 281, 352, 281, 264, 644, 295, 264, 1446, 689, 321, 722, 8924, 264, 1412, 13], "temperature": 0.0, "avg_logprob": -0.1344635839815493, "compression_ratio": 1.70995670995671, "no_speech_prob": 7.64643118600361e-06}, {"id": 112, "seek": 47992, "start": 479.92, "end": 484.2, "text": " So I'll click on navigate and we'll go down here, gathering data.", "tokens": [407, 286, 603, 2052, 322, 12350, 293, 321, 603, 352, 760, 510, 11, 13519, 1412, 13], "temperature": 0.0, "avg_logprob": -0.16465589933306257, "compression_ratio": 1.54251012145749, "no_speech_prob": 6.048880095477216e-06}, {"id": 113, "seek": 47992, "start": 484.2, "end": 488.44, "text": " There we are.", "tokens": [821, 321, 366, 13], "temperature": 0.0, "avg_logprob": -0.16465589933306257, "compression_ratio": 1.54251012145749, "no_speech_prob": 6.048880095477216e-06}, {"id": 114, "seek": 47992, "start": 488.44, "end": 490.8, "text": " So we could do a quick bit of revision first.", "tokens": [407, 321, 727, 360, 257, 1702, 857, 295, 34218, 700, 13], "temperature": 0.0, "avg_logprob": -0.16465589933306257, "compression_ratio": 1.54251012145749, "no_speech_prob": 6.048880095477216e-06}, {"id": 115, "seek": 47992, "start": 490.8, "end": 496.48, "text": " Now, by the way, I will mention a lot of people ask me what are the little tricks I use for", "tokens": [823, 11, 538, 264, 636, 11, 286, 486, 2152, 257, 688, 295, 561, 1029, 385, 437, 366, 264, 707, 11733, 286, 764, 337], "temperature": 0.0, "avg_logprob": -0.16465589933306257, "compression_ratio": 1.54251012145749, "no_speech_prob": 6.048880095477216e-06}, {"id": 116, "seek": 47992, "start": 496.48, "end": 500.76, "text": " getting around Jupyter Notebook so quickly and easily.", "tokens": [1242, 926, 22125, 88, 391, 11633, 2939, 370, 2661, 293, 3612, 13], "temperature": 0.0, "avg_logprob": -0.16465589933306257, "compression_ratio": 1.54251012145749, "no_speech_prob": 6.048880095477216e-06}, {"id": 117, "seek": 47992, "start": 500.76, "end": 503.84000000000003, "text": " One of the really nice ones, as you'll see, is this navigate menu, which actually doesn't", "tokens": [1485, 295, 264, 534, 1481, 2306, 11, 382, 291, 603, 536, 11, 307, 341, 12350, 6510, 11, 597, 767, 1177, 380], "temperature": 0.0, "avg_logprob": -0.16465589933306257, "compression_ratio": 1.54251012145749, "no_speech_prob": 6.048880095477216e-06}, {"id": 118, "seek": 47992, "start": 503.84000000000003, "end": 505.94, "text": " appear by default.", "tokens": [4204, 538, 7576, 13], "temperature": 0.0, "avg_logprob": -0.16465589933306257, "compression_ratio": 1.54251012145749, "no_speech_prob": 6.048880095477216e-06}, {"id": 119, "seek": 50594, "start": 505.94, "end": 517.24, "text": " So if you install something called Jupyter Notebook extensions, Jupyter Notebook extensions,", "tokens": [407, 498, 291, 3625, 746, 1219, 22125, 88, 391, 11633, 2939, 25129, 11, 22125, 88, 391, 11633, 2939, 25129, 11], "temperature": 0.0, "avg_logprob": -0.22854786827450707, "compression_ratio": 1.605263157894737, "no_speech_prob": 2.7693961328623118e-06}, {"id": 120, "seek": 50594, "start": 517.24, "end": 527.8, "text": " and so you just pip install them, follow the instructions, and then restart Jupyter.", "tokens": [293, 370, 291, 445, 8489, 3625, 552, 11, 1524, 264, 9415, 11, 293, 550, 21022, 22125, 88, 391, 13], "temperature": 0.0, "avg_logprob": -0.22854786827450707, "compression_ratio": 1.605263157894737, "no_speech_prob": 2.7693961328623118e-06}, {"id": 121, "seek": 50594, "start": 527.8, "end": 532.48, "text": " Obviously, this colab already has a table of contents, by the way.", "tokens": [7580, 11, 341, 1173, 455, 1217, 575, 257, 3199, 295, 15768, 11, 538, 264, 636, 13], "temperature": 0.0, "avg_logprob": -0.22854786827450707, "compression_ratio": 1.605263157894737, "no_speech_prob": 2.7693961328623118e-06}, {"id": 122, "seek": 53248, "start": 532.48, "end": 537.64, "text": " This is just if you're using something local, for example, then you'll see here that this", "tokens": [639, 307, 445, 498, 291, 434, 1228, 746, 2654, 11, 337, 1365, 11, 550, 291, 603, 536, 510, 300, 341], "temperature": 0.0, "avg_logprob": -0.1320418494088309, "compression_ratio": 1.6586345381526104, "no_speech_prob": 3.187548372807214e-06}, {"id": 123, "seek": 53248, "start": 537.64, "end": 540.04, "text": " NB extension thing will appear.", "tokens": [426, 33, 10320, 551, 486, 4204, 13], "temperature": 0.0, "avg_logprob": -0.1320418494088309, "compression_ratio": 1.6586345381526104, "no_speech_prob": 3.187548372807214e-06}, {"id": 124, "seek": 53248, "start": 540.04, "end": 545.52, "text": " And if you click on table of contents two, that gives you this handy navigation bar.", "tokens": [400, 498, 291, 2052, 322, 3199, 295, 15768, 732, 11, 300, 2709, 291, 341, 13239, 17346, 2159, 13], "temperature": 0.0, "avg_logprob": -0.1320418494088309, "compression_ratio": 1.6586345381526104, "no_speech_prob": 3.187548372807214e-06}, {"id": 125, "seek": 53248, "start": 545.52, "end": 551.8000000000001, "text": " The other thing I really like is this one here called collapsible headings.", "tokens": [440, 661, 551, 286, 534, 411, 307, 341, 472, 510, 1219, 16567, 964, 1378, 1109, 13], "temperature": 0.0, "avg_logprob": -0.1320418494088309, "compression_ratio": 1.6586345381526104, "no_speech_prob": 3.187548372807214e-06}, {"id": 126, "seek": 53248, "start": 551.8000000000001, "end": 559.3000000000001, "text": " And that's the one which gives me these nice little things here to close and open up.", "tokens": [400, 300, 311, 264, 472, 597, 2709, 385, 613, 1481, 707, 721, 510, 281, 1998, 293, 1269, 493, 13], "temperature": 0.0, "avg_logprob": -0.1320418494088309, "compression_ratio": 1.6586345381526104, "no_speech_prob": 3.187548372807214e-06}, {"id": 127, "seek": 53248, "start": 559.3000000000001, "end": 560.72, "text": " And actually, that's not even the best part.", "tokens": [400, 767, 11, 300, 311, 406, 754, 264, 1151, 644, 13], "temperature": 0.0, "avg_logprob": -0.1320418494088309, "compression_ratio": 1.6586345381526104, "no_speech_prob": 3.187548372807214e-06}, {"id": 128, "seek": 56072, "start": 560.72, "end": 566.08, "text": " The best part for me is if I hit right arrow, it goes to the end of a section.", "tokens": [440, 1151, 644, 337, 385, 307, 498, 286, 2045, 558, 11610, 11, 309, 1709, 281, 264, 917, 295, 257, 3541, 13], "temperature": 0.0, "avg_logprob": -0.1734163761138916, "compression_ratio": 1.8333333333333333, "no_speech_prob": 7.4112190304731485e-06}, {"id": 129, "seek": 56072, "start": 566.08, "end": 569.6800000000001, "text": " And if I hit left arrow, it goes to the start of a section.", "tokens": [400, 498, 286, 2045, 1411, 11610, 11, 309, 1709, 281, 264, 722, 295, 257, 3541, 13], "temperature": 0.0, "avg_logprob": -0.1734163761138916, "compression_ratio": 1.8333333333333333, "no_speech_prob": 7.4112190304731485e-06}, {"id": 130, "seek": 56072, "start": 569.6800000000001, "end": 574.28, "text": " So it's like if I want to move around sections, I just press up, left, down, right, down,", "tokens": [407, 309, 311, 411, 498, 286, 528, 281, 1286, 926, 10863, 11, 286, 445, 1886, 493, 11, 1411, 11, 760, 11, 558, 11, 760, 11], "temperature": 0.0, "avg_logprob": -0.1734163761138916, "compression_ratio": 1.8333333333333333, "no_speech_prob": 7.4112190304731485e-06}, {"id": 131, "seek": 56072, "start": 574.28, "end": 575.28, "text": " right.", "tokens": [558, 13], "temperature": 0.0, "avg_logprob": -0.1734163761138916, "compression_ratio": 1.8333333333333333, "no_speech_prob": 7.4112190304731485e-06}, {"id": 132, "seek": 56072, "start": 575.28, "end": 576.28, "text": " Very handy.", "tokens": [4372, 13239, 13], "temperature": 0.0, "avg_logprob": -0.1734163761138916, "compression_ratio": 1.8333333333333333, "no_speech_prob": 7.4112190304731485e-06}, {"id": 133, "seek": 56072, "start": 576.28, "end": 578.64, "text": " And if you hit left again, when you're here, it'll close it up.", "tokens": [400, 498, 291, 2045, 1411, 797, 11, 562, 291, 434, 510, 11, 309, 603, 1998, 309, 493, 13], "temperature": 0.0, "avg_logprob": -0.1734163761138916, "compression_ratio": 1.8333333333333333, "no_speech_prob": 7.4112190304731485e-06}, {"id": 134, "seek": 56072, "start": 578.64, "end": 580.36, "text": " Hit right again here, open it up.", "tokens": [9217, 558, 797, 510, 11, 1269, 309, 493, 13], "temperature": 0.0, "avg_logprob": -0.1734163761138916, "compression_ratio": 1.8333333333333333, "no_speech_prob": 7.4112190304731485e-06}, {"id": 135, "seek": 56072, "start": 580.36, "end": 581.36, "text": " So that's collapsible headings.", "tokens": [407, 300, 311, 16567, 964, 1378, 1109, 13], "temperature": 0.0, "avg_logprob": -0.1734163761138916, "compression_ratio": 1.8333333333333333, "no_speech_prob": 7.4112190304731485e-06}, {"id": 136, "seek": 56072, "start": 581.36, "end": 583.1600000000001, "text": " Anyway, a couple of really handy things.", "tokens": [5684, 11, 257, 1916, 295, 534, 13239, 721, 13], "temperature": 0.0, "avg_logprob": -0.1734163761138916, "compression_ratio": 1.8333333333333333, "no_speech_prob": 7.4112190304731485e-06}, {"id": 137, "seek": 56072, "start": 583.1600000000001, "end": 588.9200000000001, "text": " And we'll be talking a lot more about getting your notebook set up today at the moment.", "tokens": [400, 321, 603, 312, 1417, 257, 688, 544, 466, 1242, 428, 21060, 992, 493, 965, 412, 264, 1623, 13], "temperature": 0.0, "avg_logprob": -0.1734163761138916, "compression_ratio": 1.8333333333333333, "no_speech_prob": 7.4112190304731485e-06}, {"id": 138, "seek": 58892, "start": 588.92, "end": 598.0, "text": " So one thing you'll notice is in the book, we use the Bing API for searching images.", "tokens": [407, 472, 551, 291, 603, 3449, 307, 294, 264, 1446, 11, 321, 764, 264, 30755, 9362, 337, 10808, 5267, 13], "temperature": 0.0, "avg_logprob": -0.14149907123611635, "compression_ratio": 1.5355450236966826, "no_speech_prob": 6.540351932926569e-06}, {"id": 139, "seek": 58892, "start": 598.0, "end": 604.24, "text": " I've just gone ahead and replaced Bing with DDG because the Bing API requires getting", "tokens": [286, 600, 445, 2780, 2286, 293, 10772, 30755, 365, 30778, 38, 570, 264, 30755, 9362, 7029, 1242], "temperature": 0.0, "avg_logprob": -0.14149907123611635, "compression_ratio": 1.5355450236966826, "no_speech_prob": 6.540351932926569e-06}, {"id": 140, "seek": 58892, "start": 604.24, "end": 609.76, "text": " an SDK key, which honestly, it's like the hardest thing in deep learning is figuring", "tokens": [364, 37135, 2141, 11, 597, 6095, 11, 309, 311, 411, 264, 13158, 551, 294, 2452, 2539, 307, 15213], "temperature": 0.0, "avg_logprob": -0.14149907123611635, "compression_ratio": 1.5355450236966826, "no_speech_prob": 6.540351932926569e-06}, {"id": 141, "seek": 58892, "start": 609.76, "end": 613.92, "text": " out the Bing Azure website and getting that sorted out.", "tokens": [484, 264, 30755, 11969, 3144, 293, 1242, 300, 25462, 484, 13], "temperature": 0.0, "avg_logprob": -0.14149907123611635, "compression_ratio": 1.5355450236966826, "no_speech_prob": 6.540351932926569e-06}, {"id": 142, "seek": 58892, "start": 613.92, "end": 614.8, "text": " DDG doesn't.", "tokens": [30778, 38, 1177, 380, 13], "temperature": 0.0, "avg_logprob": -0.14149907123611635, "compression_ratio": 1.5355450236966826, "no_speech_prob": 6.540351932926569e-06}, {"id": 143, "seek": 61480, "start": 614.8, "end": 620.16, "text": " So it's basically exactly the same.", "tokens": [407, 309, 311, 1936, 2293, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.08853638049253483, "compression_ratio": 1.5526315789473684, "no_speech_prob": 3.2377276966144564e-06}, {"id": 144, "seek": 61480, "start": 620.16, "end": 625.92, "text": " And you can, I'll share this notebook as well on the course website and the forum.", "tokens": [400, 291, 393, 11, 286, 603, 2073, 341, 21060, 382, 731, 322, 264, 1164, 3144, 293, 264, 17542, 13], "temperature": 0.0, "avg_logprob": -0.08853638049253483, "compression_ratio": 1.5526315789473684, "no_speech_prob": 3.2377276966144564e-06}, {"id": 145, "seek": 61480, "start": 625.92, "end": 630.3599999999999, "text": " But all I've basically done is I've replaced Bing with DDG and got rid of the key.", "tokens": [583, 439, 286, 600, 1936, 1096, 307, 286, 600, 10772, 30755, 365, 30778, 38, 293, 658, 3973, 295, 264, 2141, 13], "temperature": 0.0, "avg_logprob": -0.08853638049253483, "compression_ratio": 1.5526315789473684, "no_speech_prob": 3.2377276966144564e-06}, {"id": 146, "seek": 61480, "start": 630.3599999999999, "end": 635.0799999999999, "text": " So then just like we did last week, we can search for things.", "tokens": [407, 550, 445, 411, 321, 630, 1036, 1243, 11, 321, 393, 3164, 337, 721, 13], "temperature": 0.0, "avg_logprob": -0.08853638049253483, "compression_ratio": 1.5526315789473684, "no_speech_prob": 3.2377276966144564e-06}, {"id": 147, "seek": 61480, "start": 635.0799999999999, "end": 641.12, "text": " And so in the book, we did a bear detector because at the time I wrote it, my then toddler", "tokens": [400, 370, 294, 264, 1446, 11, 321, 630, 257, 6155, 25712, 570, 412, 264, 565, 286, 4114, 309, 11, 452, 550, 44348], "temperature": 0.0, "avg_logprob": -0.08853638049253483, "compression_ratio": 1.5526315789473684, "no_speech_prob": 3.2377276966144564e-06}, {"id": 148, "seek": 64112, "start": 641.12, "end": 645.84, "text": " was very interested in me helping identify teddy bears.", "tokens": [390, 588, 3102, 294, 385, 4315, 5876, 45116, 17276, 13], "temperature": 0.0, "avg_logprob": -0.10752489278604696, "compression_ratio": 1.7614213197969544, "no_speech_prob": 8.397885721933562e-06}, {"id": 149, "seek": 64112, "start": 645.84, "end": 650.52, "text": " And I certainly didn't want her accidentally cuddling a grizzly bear.", "tokens": [400, 286, 3297, 994, 380, 528, 720, 15715, 269, 26656, 1688, 257, 17865, 4313, 356, 6155, 13], "temperature": 0.0, "avg_logprob": -0.10752489278604696, "compression_ratio": 1.7614213197969544, "no_speech_prob": 8.397885721933562e-06}, {"id": 150, "seek": 64112, "start": 650.52, "end": 656.92, "text": " So we show here how we can search for grizzly bears, just like last week, something that", "tokens": [407, 321, 855, 510, 577, 321, 393, 3164, 337, 17865, 4313, 356, 17276, 11, 445, 411, 1036, 1243, 11, 746, 300], "temperature": 0.0, "avg_logprob": -0.10752489278604696, "compression_ratio": 1.7614213197969544, "no_speech_prob": 8.397885721933562e-06}, {"id": 151, "seek": 64112, "start": 656.92, "end": 662.72, "text": " loops through grizzly bears, black bears and teddy bears, just like last week, get rid", "tokens": [16121, 807, 17865, 4313, 356, 17276, 11, 2211, 17276, 293, 45116, 17276, 11, 445, 411, 1036, 1243, 11, 483, 3973], "temperature": 0.0, "avg_logprob": -0.10752489278604696, "compression_ratio": 1.7614213197969544, "no_speech_prob": 8.397885721933562e-06}, {"id": 152, "seek": 64112, "start": 662.72, "end": 666.96, "text": " of the ones that failed, just like last week.", "tokens": [295, 264, 2306, 300, 7612, 11, 445, 411, 1036, 1243, 13], "temperature": 0.0, "avg_logprob": -0.10752489278604696, "compression_ratio": 1.7614213197969544, "no_speech_prob": 8.397885721933562e-06}, {"id": 153, "seek": 66696, "start": 666.96, "end": 674.1600000000001, "text": " And one thing a few people have asked on the forum is how do I find out more information", "tokens": [400, 472, 551, 257, 1326, 561, 362, 2351, 322, 264, 17542, 307, 577, 360, 286, 915, 484, 544, 1589], "temperature": 0.0, "avg_logprob": -0.10557225855385385, "compression_ratio": 1.4901960784313726, "no_speech_prob": 4.565901235764613e-06}, {"id": 154, "seek": 66696, "start": 674.1600000000001, "end": 680.6, "text": " about basically any Python or fast AI or PyTorch thing?", "tokens": [466, 1936, 604, 15329, 420, 2370, 7318, 420, 9953, 51, 284, 339, 551, 30], "temperature": 0.0, "avg_logprob": -0.10557225855385385, "compression_ratio": 1.4901960784313726, "no_speech_prob": 4.565901235764613e-06}, {"id": 155, "seek": 66696, "start": 680.6, "end": 684.32, "text": " There's a few tips here in the book.", "tokens": [821, 311, 257, 1326, 6082, 510, 294, 264, 1446, 13], "temperature": 0.0, "avg_logprob": -0.10557225855385385, "compression_ratio": 1.4901960784313726, "no_speech_prob": 4.565901235764613e-06}, {"id": 156, "seek": 66696, "start": 684.32, "end": 689.34, "text": " One is that if you put a double question mark next to any function name, you'll actually", "tokens": [1485, 307, 300, 498, 291, 829, 257, 3834, 1168, 1491, 958, 281, 604, 2445, 1315, 11, 291, 603, 767], "temperature": 0.0, "avg_logprob": -0.10557225855385385, "compression_ratio": 1.4901960784313726, "no_speech_prob": 4.565901235764613e-06}, {"id": 157, "seek": 66696, "start": 689.34, "end": 692.2, "text": " get the whole source code for it.", "tokens": [483, 264, 1379, 4009, 3089, 337, 309, 13], "temperature": 0.0, "avg_logprob": -0.10557225855385385, "compression_ratio": 1.4901960784313726, "no_speech_prob": 4.565901235764613e-06}, {"id": 158, "seek": 69220, "start": 692.2, "end": 702.44, "text": " And by the same token, if you put a single question mark, you'll get a brief, you know,", "tokens": [400, 538, 264, 912, 14862, 11, 498, 291, 829, 257, 2167, 1168, 1491, 11, 291, 603, 483, 257, 5353, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.1833049117541704, "compression_ratio": 1.3741496598639455, "no_speech_prob": 5.338112714525778e-06}, {"id": 159, "seek": 69220, "start": 702.44, "end": 704.6800000000001, "text": " little bit of information.", "tokens": [707, 857, 295, 1589, 13], "temperature": 0.0, "avg_logprob": -0.1833049117541704, "compression_ratio": 1.3741496598639455, "no_speech_prob": 5.338112714525778e-06}, {"id": 160, "seek": 69220, "start": 704.6800000000001, "end": 715.88, "text": " If you've got NB dev installed, I think it's NB dev you need, then you can type doc and", "tokens": [759, 291, 600, 658, 426, 33, 1905, 8899, 11, 286, 519, 309, 311, 426, 33, 1905, 291, 643, 11, 550, 291, 393, 2010, 3211, 293], "temperature": 0.0, "avg_logprob": -0.1833049117541704, "compression_ratio": 1.3741496598639455, "no_speech_prob": 5.338112714525778e-06}, {"id": 161, "seek": 71588, "start": 715.88, "end": 723.56, "text": " that'll give you, perhaps most importantly, a link straight to the documentation where", "tokens": [300, 603, 976, 291, 11, 4317, 881, 8906, 11, 257, 2113, 2997, 281, 264, 14333, 689], "temperature": 0.0, "avg_logprob": -0.21769168889411142, "compression_ratio": 1.7179487179487178, "no_speech_prob": 1.9333513137098635e-06}, {"id": 162, "seek": 71588, "start": 723.56, "end": 726.72, "text": " you can find out more information.", "tokens": [291, 393, 915, 484, 544, 1589, 13], "temperature": 0.0, "avg_logprob": -0.21769168889411142, "compression_ratio": 1.7179487179487178, "no_speech_prob": 1.9333513137098635e-06}, {"id": 163, "seek": 71588, "start": 726.72, "end": 729.88, "text": " And generally there'll be examples as well.", "tokens": [400, 5101, 456, 603, 312, 5110, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.21769168889411142, "compression_ratio": 1.7179487179487178, "no_speech_prob": 1.9333513137098635e-06}, {"id": 164, "seek": 71588, "start": 729.88, "end": 731.52, "text": " And also a link here to the source code.", "tokens": [400, 611, 257, 2113, 510, 281, 264, 4009, 3089, 13], "temperature": 0.0, "avg_logprob": -0.21769168889411142, "compression_ratio": 1.7179487179487178, "no_speech_prob": 1.9333513137098635e-06}, {"id": 165, "seek": 71588, "start": 731.52, "end": 735.0, "text": " If you want to, let's do that with a control.", "tokens": [759, 291, 528, 281, 11, 718, 311, 360, 300, 365, 257, 1969, 13], "temperature": 0.0, "avg_logprob": -0.21769168889411142, "compression_ratio": 1.7179487179487178, "no_speech_prob": 1.9333513137098635e-06}, {"id": 166, "seek": 71588, "start": 735.0, "end": 736.0, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.21769168889411142, "compression_ratio": 1.7179487179487178, "no_speech_prob": 1.9333513137098635e-06}, {"id": 167, "seek": 71588, "start": 736.0, "end": 739.88, "text": " A link to the source code and that way you can jump around.", "tokens": [316, 2113, 281, 264, 4009, 3089, 293, 300, 636, 291, 393, 3012, 926, 13], "temperature": 0.0, "avg_logprob": -0.21769168889411142, "compression_ratio": 1.7179487179487178, "no_speech_prob": 1.9333513137098635e-06}, {"id": 168, "seek": 71588, "start": 739.88, "end": 743.84, "text": " Notice that in GitHub in the source code, you can click on things and jump to their", "tokens": [13428, 300, 294, 23331, 294, 264, 4009, 3089, 11, 291, 393, 2052, 322, 721, 293, 3012, 281, 641], "temperature": 0.0, "avg_logprob": -0.21769168889411142, "compression_ratio": 1.7179487179487178, "no_speech_prob": 1.9333513137098635e-06}, {"id": 169, "seek": 74384, "start": 743.84, "end": 749.36, "text": " definition so it's kind of a nice way of skipping around to understand exactly what's going", "tokens": [7123, 370, 309, 311, 733, 295, 257, 1481, 636, 295, 31533, 926, 281, 1223, 2293, 437, 311, 516], "temperature": 0.0, "avg_logprob": -0.16919821187069542, "compression_ratio": 1.543956043956044, "no_speech_prob": 5.682401024387218e-06}, {"id": 170, "seek": 74384, "start": 749.36, "end": 753.2800000000001, "text": " on.", "tokens": [322, 13], "temperature": 0.0, "avg_logprob": -0.16919821187069542, "compression_ratio": 1.543956043956044, "no_speech_prob": 5.682401024387218e-06}, {"id": 171, "seek": 74384, "start": 753.2800000000001, "end": 756.58, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.16919821187069542, "compression_ratio": 1.543956043956044, "no_speech_prob": 5.682401024387218e-06}, {"id": 172, "seek": 74384, "start": 756.58, "end": 762.24, "text": " So lots of great ways of getting help.", "tokens": [407, 3195, 295, 869, 2098, 295, 1242, 854, 13], "temperature": 0.0, "avg_logprob": -0.16919821187069542, "compression_ratio": 1.543956043956044, "no_speech_prob": 5.682401024387218e-06}, {"id": 173, "seek": 74384, "start": 762.24, "end": 766.5400000000001, "text": " But what I promised you is that we're going to now clean the data.", "tokens": [583, 437, 286, 10768, 291, 307, 300, 321, 434, 516, 281, 586, 2541, 264, 1412, 13], "temperature": 0.0, "avg_logprob": -0.16919821187069542, "compression_ratio": 1.543956043956044, "no_speech_prob": 5.682401024387218e-06}, {"id": 174, "seek": 74384, "start": 766.5400000000001, "end": 771.9200000000001, "text": " So I'm going to tell you something that you might find really surprising.", "tokens": [407, 286, 478, 516, 281, 980, 291, 746, 300, 291, 1062, 915, 534, 8830, 13], "temperature": 0.0, "avg_logprob": -0.16919821187069542, "compression_ratio": 1.543956043956044, "no_speech_prob": 5.682401024387218e-06}, {"id": 175, "seek": 77192, "start": 771.92, "end": 775.4799999999999, "text": " Before you clean the data, you train a model.", "tokens": [4546, 291, 2541, 264, 1412, 11, 291, 3847, 257, 2316, 13], "temperature": 0.0, "avg_logprob": -0.12683095065030184, "compression_ratio": 1.778723404255319, "no_speech_prob": 3.500812454149127e-06}, {"id": 176, "seek": 77192, "start": 775.4799999999999, "end": 780.7199999999999, "text": " Now I know that's going to sound really backwards to what you've probably heard a thousand times,", "tokens": [823, 286, 458, 300, 311, 516, 281, 1626, 534, 12204, 281, 437, 291, 600, 1391, 2198, 257, 4714, 1413, 11], "temperature": 0.0, "avg_logprob": -0.12683095065030184, "compression_ratio": 1.778723404255319, "no_speech_prob": 3.500812454149127e-06}, {"id": 177, "seek": 77192, "start": 780.7199999999999, "end": 786.7199999999999, "text": " which is that first you train your data and then you train your model.", "tokens": [597, 307, 300, 700, 291, 3847, 428, 1412, 293, 550, 291, 3847, 428, 2316, 13], "temperature": 0.0, "avg_logprob": -0.12683095065030184, "compression_ratio": 1.778723404255319, "no_speech_prob": 3.500812454149127e-06}, {"id": 178, "seek": 77192, "start": 786.7199999999999, "end": 788.88, "text": " But I'm going to show you something really amazing.", "tokens": [583, 286, 478, 516, 281, 855, 291, 746, 534, 2243, 13], "temperature": 0.0, "avg_logprob": -0.12683095065030184, "compression_ratio": 1.778723404255319, "no_speech_prob": 3.500812454149127e-06}, {"id": 179, "seek": 77192, "start": 788.88, "end": 791.52, "text": " First we're going to train a model and you'll see why in a moment.", "tokens": [2386, 321, 434, 516, 281, 3847, 257, 2316, 293, 291, 603, 536, 983, 294, 257, 1623, 13], "temperature": 0.0, "avg_logprob": -0.12683095065030184, "compression_ratio": 1.778723404255319, "no_speech_prob": 3.500812454149127e-06}, {"id": 180, "seek": 77192, "start": 791.52, "end": 796.52, "text": " So to train a model, just like before, we use a data block to grab our data loaders.", "tokens": [407, 281, 3847, 257, 2316, 11, 445, 411, 949, 11, 321, 764, 257, 1412, 3461, 281, 4444, 527, 1412, 3677, 433, 13], "temperature": 0.0, "avg_logprob": -0.12683095065030184, "compression_ratio": 1.778723404255319, "no_speech_prob": 3.500812454149127e-06}, {"id": 181, "seek": 79652, "start": 796.52, "end": 802.3199999999999, "text": " There's lots of information here in the book about what's going on here.", "tokens": [821, 311, 3195, 295, 1589, 510, 294, 264, 1446, 466, 437, 311, 516, 322, 510, 13], "temperature": 0.0, "avg_logprob": -0.15206661029737822, "compression_ratio": 1.6093023255813954, "no_speech_prob": 1.1125462151539978e-05}, {"id": 182, "seek": 79652, "start": 802.3199999999999, "end": 807.88, "text": " There we go.", "tokens": [821, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.15206661029737822, "compression_ratio": 1.6093023255813954, "no_speech_prob": 1.1125462151539978e-05}, {"id": 183, "seek": 79652, "start": 807.88, "end": 814.12, "text": " And so then we can call show batch to see them as per usual.", "tokens": [400, 370, 550, 321, 393, 818, 855, 15245, 281, 536, 552, 382, 680, 7713, 13], "temperature": 0.0, "avg_logprob": -0.15206661029737822, "compression_ratio": 1.6093023255813954, "no_speech_prob": 1.1125462151539978e-05}, {"id": 184, "seek": 79652, "start": 814.12, "end": 817.8, "text": " There's a little side by here in the book I'll quickly mention, which is about the different", "tokens": [821, 311, 257, 707, 1252, 538, 510, 294, 264, 1446, 286, 603, 2661, 2152, 11, 597, 307, 466, 264, 819], "temperature": 0.0, "avg_logprob": -0.15206661029737822, "compression_ratio": 1.6093023255813954, "no_speech_prob": 1.1125462151539978e-05}, {"id": 185, "seek": 79652, "start": 817.8, "end": 819.4399999999999, "text": " ways we could resize.", "tokens": [2098, 321, 727, 50069, 13], "temperature": 0.0, "avg_logprob": -0.15206661029737822, "compression_ratio": 1.6093023255813954, "no_speech_prob": 1.1125462151539978e-05}, {"id": 186, "seek": 79652, "start": 819.4399999999999, "end": 821.4399999999999, "text": " I think we briefly mentioned it last week.", "tokens": [286, 519, 321, 10515, 2835, 309, 1036, 1243, 13], "temperature": 0.0, "avg_logprob": -0.15206661029737822, "compression_ratio": 1.6093023255813954, "no_speech_prob": 1.1125462151539978e-05}, {"id": 187, "seek": 79652, "start": 821.4399999999999, "end": 824.4399999999999, "text": " We can squish.", "tokens": [492, 393, 31379, 13], "temperature": 0.0, "avg_logprob": -0.15206661029737822, "compression_ratio": 1.6093023255813954, "no_speech_prob": 1.1125462151539978e-05}, {"id": 188, "seek": 79652, "start": 824.4399999999999, "end": 825.4399999999999, "text": " Last week I used a string.", "tokens": [5264, 1243, 286, 1143, 257, 6798, 13], "temperature": 0.0, "avg_logprob": -0.15206661029737822, "compression_ratio": 1.6093023255813954, "no_speech_prob": 1.1125462151539978e-05}, {"id": 189, "seek": 82544, "start": 825.44, "end": 828.8000000000001, "text": " You can use a string or this kind of enum like thing that we have.", "tokens": [509, 393, 764, 257, 6798, 420, 341, 733, 295, 465, 449, 411, 551, 300, 321, 362, 13], "temperature": 0.0, "avg_logprob": -0.18671346675453848, "compression_ratio": 1.8494983277591974, "no_speech_prob": 1.4285295947047416e-05}, {"id": 190, "seek": 82544, "start": 828.8000000000001, "end": 833.44, "text": " You can see with a squish, you can end up with some very thin bears.", "tokens": [509, 393, 536, 365, 257, 31379, 11, 291, 393, 917, 493, 365, 512, 588, 5862, 17276, 13], "temperature": 0.0, "avg_logprob": -0.18671346675453848, "compression_ratio": 1.8494983277591974, "no_speech_prob": 1.4285295947047416e-05}, {"id": 191, "seek": 82544, "start": 833.44, "end": 834.44, "text": " Right?", "tokens": [1779, 30], "temperature": 0.0, "avg_logprob": -0.18671346675453848, "compression_ratio": 1.8494983277591974, "no_speech_prob": 1.4285295947047416e-05}, {"id": 192, "seek": 82544, "start": 834.44, "end": 836.5200000000001, "text": " So this is the real site that's shaped with the bear.", "tokens": [407, 341, 307, 264, 957, 3621, 300, 311, 13475, 365, 264, 6155, 13], "temperature": 0.0, "avg_logprob": -0.18671346675453848, "compression_ratio": 1.8494983277591974, "no_speech_prob": 1.4285295947047416e-05}, {"id": 193, "seek": 82544, "start": 836.5200000000001, "end": 839.9200000000001, "text": " Here it's become thin, but you can see now we've got all of its cubs.", "tokens": [1692, 309, 311, 1813, 5862, 11, 457, 291, 393, 536, 586, 321, 600, 658, 439, 295, 1080, 269, 5432, 13], "temperature": 0.0, "avg_logprob": -0.18671346675453848, "compression_ratio": 1.8494983277591974, "no_speech_prob": 1.4285295947047416e-05}, {"id": 194, "seek": 82544, "start": 839.9200000000001, "end": 841.2800000000001, "text": " Are they called cubs?", "tokens": [2014, 436, 1219, 269, 5432, 30], "temperature": 0.0, "avg_logprob": -0.18671346675453848, "compression_ratio": 1.8494983277591974, "no_speech_prob": 1.4285295947047416e-05}, {"id": 195, "seek": 82544, "start": 841.2800000000001, "end": 842.9200000000001, "text": " Yeah, bear cubs.", "tokens": [865, 11, 6155, 269, 5432, 13], "temperature": 0.0, "avg_logprob": -0.18671346675453848, "compression_ratio": 1.8494983277591974, "no_speech_prob": 1.4285295947047416e-05}, {"id": 196, "seek": 82544, "start": 842.9200000000001, "end": 845.2800000000001, "text": " So it's squished it to make sure we can see the whole picture.", "tokens": [407, 309, 311, 2339, 4729, 309, 281, 652, 988, 321, 393, 536, 264, 1379, 3036, 13], "temperature": 0.0, "avg_logprob": -0.18671346675453848, "compression_ratio": 1.8494983277591974, "no_speech_prob": 1.4285295947047416e-05}, {"id": 197, "seek": 82544, "start": 845.2800000000001, "end": 846.2800000000001, "text": " Same one here.", "tokens": [10635, 472, 510, 13], "temperature": 0.0, "avg_logprob": -0.18671346675453848, "compression_ratio": 1.8494983277591974, "no_speech_prob": 1.4285295947047416e-05}, {"id": 198, "seek": 82544, "start": 846.2800000000001, "end": 847.2800000000001, "text": " This one was out of the picture.", "tokens": [639, 472, 390, 484, 295, 264, 3036, 13], "temperature": 0.0, "avg_logprob": -0.18671346675453848, "compression_ratio": 1.8494983277591974, "no_speech_prob": 1.4285295947047416e-05}, {"id": 199, "seek": 82544, "start": 847.2800000000001, "end": 848.2800000000001, "text": " We squished it.", "tokens": [492, 2339, 4729, 309, 13], "temperature": 0.0, "avg_logprob": -0.18671346675453848, "compression_ratio": 1.8494983277591974, "no_speech_prob": 1.4285295947047416e-05}, {"id": 200, "seek": 82544, "start": 848.2800000000001, "end": 851.8800000000001, "text": " This guy now looks weirdly thin, but we can see the whole thing.", "tokens": [639, 2146, 586, 1542, 48931, 5862, 11, 457, 321, 393, 536, 264, 1379, 551, 13], "temperature": 0.0, "avg_logprob": -0.18671346675453848, "compression_ratio": 1.8494983277591974, "no_speech_prob": 1.4285295947047416e-05}, {"id": 201, "seek": 82544, "start": 851.8800000000001, "end": 852.8800000000001, "text": " So that's squishing.", "tokens": [407, 300, 311, 2339, 3807, 13], "temperature": 0.0, "avg_logprob": -0.18671346675453848, "compression_ratio": 1.8494983277591974, "no_speech_prob": 1.4285295947047416e-05}, {"id": 202, "seek": 82544, "start": 852.8800000000001, "end": 854.44, "text": " Whereas this one here is cropping.", "tokens": [13813, 341, 472, 510, 307, 4848, 3759, 13], "temperature": 0.0, "avg_logprob": -0.18671346675453848, "compression_ratio": 1.8494983277591974, "no_speech_prob": 1.4285295947047416e-05}, {"id": 203, "seek": 85444, "start": 854.44, "end": 856.1600000000001, "text": " It's cropped out just the center of the image.", "tokens": [467, 311, 4848, 3320, 484, 445, 264, 3056, 295, 264, 3256, 13], "temperature": 0.0, "avg_logprob": -0.11820129787220675, "compression_ratio": 1.781021897810219, "no_speech_prob": 7.646498488611542e-06}, {"id": 204, "seek": 85444, "start": 856.1600000000001, "end": 858.9000000000001, "text": " So we get a better aspect ratio, but we lose some stuff.", "tokens": [407, 321, 483, 257, 1101, 4171, 8509, 11, 457, 321, 3624, 512, 1507, 13], "temperature": 0.0, "avg_logprob": -0.11820129787220675, "compression_ratio": 1.781021897810219, "no_speech_prob": 7.646498488611542e-06}, {"id": 205, "seek": 85444, "start": 858.9000000000001, "end": 861.6, "text": " This is so we can get square images.", "tokens": [639, 307, 370, 321, 393, 483, 3732, 5267, 13], "temperature": 0.0, "avg_logprob": -0.11820129787220675, "compression_ratio": 1.781021897810219, "no_speech_prob": 7.646498488611542e-06}, {"id": 206, "seek": 85444, "start": 861.6, "end": 864.8000000000001, "text": " And the other approach is we can use pad.", "tokens": [400, 264, 661, 3109, 307, 321, 393, 764, 6887, 13], "temperature": 0.0, "avg_logprob": -0.11820129787220675, "compression_ratio": 1.781021897810219, "no_speech_prob": 7.646498488611542e-06}, {"id": 207, "seek": 85444, "start": 864.8000000000001, "end": 866.4000000000001, "text": " And so you can pad with various different things.", "tokens": [400, 370, 291, 393, 6887, 365, 3683, 819, 721, 13], "temperature": 0.0, "avg_logprob": -0.11820129787220675, "compression_ratio": 1.781021897810219, "no_speech_prob": 7.646498488611542e-06}, {"id": 208, "seek": 85444, "start": 866.4000000000001, "end": 870.96, "text": " If you pad with zeros, which is black, you can see here now we've got the whole image", "tokens": [759, 291, 6887, 365, 35193, 11, 597, 307, 2211, 11, 291, 393, 536, 510, 586, 321, 600, 658, 264, 1379, 3256], "temperature": 0.0, "avg_logprob": -0.11820129787220675, "compression_ratio": 1.781021897810219, "no_speech_prob": 7.646498488611542e-06}, {"id": 209, "seek": 85444, "start": 870.96, "end": 873.5600000000001, "text": " and the correct aspect ratio.", "tokens": [293, 264, 3006, 4171, 8509, 13], "temperature": 0.0, "avg_logprob": -0.11820129787220675, "compression_ratio": 1.781021897810219, "no_speech_prob": 7.646498488611542e-06}, {"id": 210, "seek": 85444, "start": 873.5600000000001, "end": 875.0, "text": " So that's another way we can do it.", "tokens": [407, 300, 311, 1071, 636, 321, 393, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.11820129787220675, "compression_ratio": 1.781021897810219, "no_speech_prob": 7.646498488611542e-06}, {"id": 211, "seek": 85444, "start": 875.0, "end": 881.5200000000001, "text": " And you know, different situations, you know, result in different quality models.", "tokens": [400, 291, 458, 11, 819, 6851, 11, 291, 458, 11, 1874, 294, 819, 3125, 5245, 13], "temperature": 0.0, "avg_logprob": -0.11820129787220675, "compression_ratio": 1.781021897810219, "no_speech_prob": 7.646498488611542e-06}, {"id": 212, "seek": 85444, "start": 881.5200000000001, "end": 882.5600000000001, "text": " You can try them all.", "tokens": [509, 393, 853, 552, 439, 13], "temperature": 0.0, "avg_logprob": -0.11820129787220675, "compression_ratio": 1.781021897810219, "no_speech_prob": 7.646498488611542e-06}, {"id": 213, "seek": 88256, "start": 882.56, "end": 887.1999999999999, "text": " It doesn't normally make too big a difference, so I wouldn't worry about it too much.", "tokens": [467, 1177, 380, 5646, 652, 886, 955, 257, 2649, 11, 370, 286, 2759, 380, 3292, 466, 309, 886, 709, 13], "temperature": 0.0, "avg_logprob": -0.13357169882765094, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.594307367573492e-06}, {"id": 214, "seek": 88256, "start": 887.1999999999999, "end": 892.2199999999999, "text": " I tell you one though that is very interesting is random resized crop.", "tokens": [286, 980, 291, 472, 1673, 300, 307, 588, 1880, 307, 4974, 725, 1602, 9086, 13], "temperature": 0.0, "avg_logprob": -0.13357169882765094, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.594307367573492e-06}, {"id": 215, "seek": 88256, "start": 892.2199999999999, "end": 898.7199999999999, "text": " So instead of saying resize, we can say random resized crop.", "tokens": [407, 2602, 295, 1566, 50069, 11, 321, 393, 584, 4974, 725, 1602, 9086, 13], "temperature": 0.0, "avg_logprob": -0.13357169882765094, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.594307367573492e-06}, {"id": 216, "seek": 88256, "start": 898.7199999999999, "end": 904.4599999999999, "text": " And if we do that, you'll see we get a different bit of an image every time.", "tokens": [400, 498, 321, 360, 300, 11, 291, 603, 536, 321, 483, 257, 819, 857, 295, 364, 3256, 633, 565, 13], "temperature": 0.0, "avg_logprob": -0.13357169882765094, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.594307367573492e-06}, {"id": 217, "seek": 88256, "start": 904.4599999999999, "end": 910.4399999999999, "text": " So during the week this week, somebody asked on the forum, I'm trying to, this is a really", "tokens": [407, 1830, 264, 1243, 341, 1243, 11, 2618, 2351, 322, 264, 17542, 11, 286, 478, 1382, 281, 11, 341, 307, 257, 534], "temperature": 0.0, "avg_logprob": -0.13357169882765094, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.594307367573492e-06}, {"id": 218, "seek": 91044, "start": 910.44, "end": 918.0, "text": " interesting idea, which it turned out worked slightly, was they wanted to recognize pictures", "tokens": [1880, 1558, 11, 597, 309, 3574, 484, 2732, 4748, 11, 390, 436, 1415, 281, 5521, 5242], "temperature": 0.0, "avg_logprob": -0.1083002181280227, "compression_ratio": 1.65625, "no_speech_prob": 4.785033979715081e-06}, {"id": 219, "seek": 91044, "start": 918.0, "end": 922.84, "text": " of French and German texts.", "tokens": [295, 5522, 293, 6521, 15765, 13], "temperature": 0.0, "avg_logprob": -0.1083002181280227, "compression_ratio": 1.65625, "no_speech_prob": 4.785033979715081e-06}, {"id": 220, "seek": 91044, "start": 922.84, "end": 926.3800000000001, "text": " So obviously this is not the normal way you would do that, but just for a bit of experiment", "tokens": [407, 2745, 341, 307, 406, 264, 2710, 636, 291, 576, 360, 300, 11, 457, 445, 337, 257, 857, 295, 5120], "temperature": 0.0, "avg_logprob": -0.1083002181280227, "compression_ratio": 1.65625, "no_speech_prob": 4.785033979715081e-06}, {"id": 221, "seek": 91044, "start": 926.3800000000001, "end": 927.8800000000001, "text": " and I love experiments.", "tokens": [293, 286, 959, 12050, 13], "temperature": 0.0, "avg_logprob": -0.1083002181280227, "compression_ratio": 1.65625, "no_speech_prob": 4.785033979715081e-06}, {"id": 222, "seek": 91044, "start": 927.8800000000001, "end": 932.6800000000001, "text": " So they had very big scans of documents and they wanted to figure out whether it was French", "tokens": [407, 436, 632, 588, 955, 35116, 295, 8512, 293, 436, 1415, 281, 2573, 484, 1968, 309, 390, 5522], "temperature": 0.0, "avg_logprob": -0.1083002181280227, "compression_ratio": 1.65625, "no_speech_prob": 4.785033979715081e-06}, {"id": 223, "seek": 91044, "start": 932.6800000000001, "end": 934.72, "text": " or German just by looking at images.", "tokens": [420, 6521, 445, 538, 1237, 412, 5267, 13], "temperature": 0.0, "avg_logprob": -0.1083002181280227, "compression_ratio": 1.65625, "no_speech_prob": 4.785033979715081e-06}, {"id": 224, "seek": 91044, "start": 934.72, "end": 936.1600000000001, "text": " And they said the pictures were too big.", "tokens": [400, 436, 848, 264, 5242, 645, 886, 955, 13], "temperature": 0.0, "avg_logprob": -0.1083002181280227, "compression_ratio": 1.65625, "no_speech_prob": 4.785033979715081e-06}, {"id": 225, "seek": 91044, "start": 936.1600000000001, "end": 937.1600000000001, "text": " What should I do?", "tokens": [708, 820, 286, 360, 30], "temperature": 0.0, "avg_logprob": -0.1083002181280227, "compression_ratio": 1.65625, "no_speech_prob": 4.785033979715081e-06}, {"id": 226, "seek": 93716, "start": 937.16, "end": 941.7199999999999, "text": " I said use random resized crop and that way you would grab different bits of the image.", "tokens": [286, 848, 764, 4974, 725, 1602, 9086, 293, 300, 636, 291, 576, 4444, 819, 9239, 295, 264, 3256, 13], "temperature": 0.0, "avg_logprob": -0.09666200365339006, "compression_ratio": 1.8059071729957805, "no_speech_prob": 4.289298431103816e-06}, {"id": 227, "seek": 93716, "start": 941.7199999999999, "end": 945.8399999999999, "text": " And this is very nice because you could run lots and lots of epochs and get slightly", "tokens": [400, 341, 307, 588, 1481, 570, 291, 727, 1190, 3195, 293, 3195, 295, 30992, 28346, 293, 483, 4748], "temperature": 0.0, "avg_logprob": -0.09666200365339006, "compression_ratio": 1.8059071729957805, "no_speech_prob": 4.289298431103816e-06}, {"id": 228, "seek": 93716, "start": 945.8399999999999, "end": 949.7199999999999, "text": " different pictures each time.", "tokens": [819, 5242, 1184, 565, 13], "temperature": 0.0, "avg_logprob": -0.09666200365339006, "compression_ratio": 1.8059071729957805, "no_speech_prob": 4.289298431103816e-06}, {"id": 229, "seek": 93716, "start": 949.7199999999999, "end": 951.8, "text": " So this is a very good technique.", "tokens": [407, 341, 307, 257, 588, 665, 6532, 13], "temperature": 0.0, "avg_logprob": -0.09666200365339006, "compression_ratio": 1.8059071729957805, "no_speech_prob": 4.289298431103816e-06}, {"id": 230, "seek": 93716, "start": 951.8, "end": 956.1999999999999, "text": " And this idea of getting different pictures each time from the same image is called data", "tokens": [400, 341, 1558, 295, 1242, 819, 5242, 1184, 565, 490, 264, 912, 3256, 307, 1219, 1412], "temperature": 0.0, "avg_logprob": -0.09666200365339006, "compression_ratio": 1.8059071729957805, "no_speech_prob": 4.289298431103816e-06}, {"id": 231, "seek": 93716, "start": 956.1999999999999, "end": 957.1999999999999, "text": " augmentation.", "tokens": [14501, 19631, 13], "temperature": 0.0, "avg_logprob": -0.09666200365339006, "compression_ratio": 1.8059071729957805, "no_speech_prob": 4.289298431103816e-06}, {"id": 232, "seek": 93716, "start": 957.1999999999999, "end": 965.9599999999999, "text": " And again, I'm not going to go into too much detail about data augmentation because it's", "tokens": [400, 797, 11, 286, 478, 406, 516, 281, 352, 666, 886, 709, 2607, 466, 1412, 14501, 19631, 570, 309, 311], "temperature": 0.0, "avg_logprob": -0.09666200365339006, "compression_ratio": 1.8059071729957805, "no_speech_prob": 4.289298431103816e-06}, {"id": 233, "seek": 96596, "start": 965.96, "end": 971.24, "text": " in the book, but I'll just quickly point out here that if you use this thing called aug", "tokens": [294, 264, 1446, 11, 457, 286, 603, 445, 2661, 935, 484, 510, 300, 498, 291, 764, 341, 551, 1219, 14501], "temperature": 0.0, "avg_logprob": -0.12928808111893503, "compression_ratio": 1.6478260869565218, "no_speech_prob": 7.183158686530078e-06}, {"id": 234, "seek": 96596, "start": 971.24, "end": 977.08, "text": " transforms, so augmentation transforms, and here I have multiplied them by two.", "tokens": [35592, 11, 370, 14501, 19631, 35592, 11, 293, 510, 286, 362, 17207, 552, 538, 732, 13], "temperature": 0.0, "avg_logprob": -0.12928808111893503, "compression_ratio": 1.6478260869565218, "no_speech_prob": 7.183158686530078e-06}, {"id": 235, "seek": 96596, "start": 977.08, "end": 979.9000000000001, "text": " So I've made them super big so you can see them more easily.", "tokens": [407, 286, 600, 1027, 552, 1687, 955, 370, 291, 393, 536, 552, 544, 3612, 13], "temperature": 0.0, "avg_logprob": -0.12928808111893503, "compression_ratio": 1.6478260869565218, "no_speech_prob": 7.183158686530078e-06}, {"id": 236, "seek": 96596, "start": 979.9000000000001, "end": 987.2800000000001, "text": " You can see that these teddies are getting turned and squished and warped and recolored", "tokens": [509, 393, 536, 300, 613, 22337, 22018, 366, 1242, 3574, 293, 2339, 4729, 293, 1516, 3452, 293, 850, 401, 2769], "temperature": 0.0, "avg_logprob": -0.12928808111893503, "compression_ratio": 1.6478260869565218, "no_speech_prob": 7.183158686530078e-06}, {"id": 237, "seek": 96596, "start": 987.2800000000001, "end": 991.88, "text": " and saturated, all this stuff to make every picture different.", "tokens": [293, 25408, 11, 439, 341, 1507, 281, 652, 633, 3036, 819, 13], "temperature": 0.0, "avg_logprob": -0.12928808111893503, "compression_ratio": 1.6478260869565218, "no_speech_prob": 7.183158686530078e-06}, {"id": 238, "seek": 99188, "start": 991.88, "end": 997.64, "text": " And generally speaking, if you're training for more than about five or 10 epochs, which", "tokens": [400, 5101, 4124, 11, 498, 291, 434, 3097, 337, 544, 813, 466, 1732, 420, 1266, 30992, 28346, 11, 597], "temperature": 0.0, "avg_logprob": -0.15384949359697164, "compression_ratio": 1.6, "no_speech_prob": 3.0415787932724925e-06}, {"id": 239, "seek": 99188, "start": 997.64, "end": 1001.68, "text": " you'll probably want to do most of the time, unless you've got a super easy problem to", "tokens": [291, 603, 1391, 528, 281, 360, 881, 295, 264, 565, 11, 5969, 291, 600, 658, 257, 1687, 1858, 1154, 281], "temperature": 0.0, "avg_logprob": -0.15384949359697164, "compression_ratio": 1.6, "no_speech_prob": 3.0415787932724925e-06}, {"id": 240, "seek": 99188, "start": 1001.68, "end": 1008.56, "text": " solve, you'll probably want to use random resized crop and these aug transforms.", "tokens": [5039, 11, 291, 603, 1391, 528, 281, 764, 4974, 725, 1602, 9086, 293, 613, 14501, 35592, 13], "temperature": 0.0, "avg_logprob": -0.15384949359697164, "compression_ratio": 1.6, "no_speech_prob": 3.0415787932724925e-06}, {"id": 241, "seek": 99188, "start": 1008.56, "end": 1010.56, "text": " Don't put the multi-cols too, just leave that empty.", "tokens": [1468, 380, 829, 264, 4825, 12, 8768, 82, 886, 11, 445, 1856, 300, 6707, 13], "temperature": 0.0, "avg_logprob": -0.15384949359697164, "compression_ratio": 1.6, "no_speech_prob": 3.0415787932724925e-06}, {"id": 242, "seek": 99188, "start": 1010.56, "end": 1016.92, "text": " I'm just putting it there so you can see them more clearly.", "tokens": [286, 478, 445, 3372, 309, 456, 370, 291, 393, 536, 552, 544, 4448, 13], "temperature": 0.0, "avg_logprob": -0.15384949359697164, "compression_ratio": 1.6, "no_speech_prob": 3.0415787932724925e-06}, {"id": 243, "seek": 101692, "start": 1016.92, "end": 1025.22, "text": " So I've got an interesting question here from Alex in our audience, which is, is this copying", "tokens": [407, 286, 600, 658, 364, 1880, 1168, 510, 490, 5202, 294, 527, 4034, 11, 597, 307, 11, 307, 341, 27976], "temperature": 0.0, "avg_logprob": -0.11108610365125868, "compression_ratio": 1.580110497237569, "no_speech_prob": 4.860404260398354e-06}, {"id": 244, "seek": 101692, "start": 1025.22, "end": 1031.36, "text": " the image multiple times during something like this or something like this?", "tokens": [264, 3256, 3866, 1413, 1830, 746, 411, 341, 420, 746, 411, 341, 30], "temperature": 0.0, "avg_logprob": -0.11108610365125868, "compression_ratio": 1.580110497237569, "no_speech_prob": 4.860404260398354e-06}, {"id": 245, "seek": 101692, "start": 1031.36, "end": 1036.2, "text": " And the answer is no, we're not copying the image.", "tokens": [400, 264, 1867, 307, 572, 11, 321, 434, 406, 27976, 264, 3256, 13], "temperature": 0.0, "avg_logprob": -0.11108610365125868, "compression_ratio": 1.580110497237569, "no_speech_prob": 4.860404260398354e-06}, {"id": 246, "seek": 101692, "start": 1036.2, "end": 1041.6, "text": " What happens is that image, so each epoch, every image gets read.", "tokens": [708, 2314, 307, 300, 3256, 11, 370, 1184, 30992, 339, 11, 633, 3256, 2170, 1401, 13], "temperature": 0.0, "avg_logprob": -0.11108610365125868, "compression_ratio": 1.580110497237569, "no_speech_prob": 4.860404260398354e-06}, {"id": 247, "seek": 104160, "start": 1041.6, "end": 1048.36, "text": " And what happens here is though, is kind of in memory, in RAM, the image is being warped,", "tokens": [400, 437, 2314, 510, 307, 1673, 11, 307, 733, 295, 294, 4675, 11, 294, 14561, 11, 264, 3256, 307, 885, 1516, 3452, 11], "temperature": 0.0, "avg_logprob": -0.155978614208745, "compression_ratio": 1.6751054852320675, "no_speech_prob": 2.4060882424237207e-06}, {"id": 248, "seek": 104160, "start": 1048.36, "end": 1049.36, "text": " right?", "tokens": [558, 30], "temperature": 0.0, "avg_logprob": -0.155978614208745, "compression_ratio": 1.6751054852320675, "no_speech_prob": 2.4060882424237207e-06}, {"id": 249, "seek": 104160, "start": 1049.36, "end": 1052.28, "text": " It's being recropping it and recoloring it and so forth.", "tokens": [467, 311, 885, 850, 340, 3759, 309, 293, 850, 401, 3662, 309, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.155978614208745, "compression_ratio": 1.6751054852320675, "no_speech_prob": 2.4060882424237207e-06}, {"id": 250, "seek": 104160, "start": 1052.28, "end": 1057.12, "text": " So it's a real time process that's happening during model training.", "tokens": [407, 309, 311, 257, 957, 565, 1399, 300, 311, 2737, 1830, 2316, 3097, 13], "temperature": 0.0, "avg_logprob": -0.155978614208745, "compression_ratio": 1.6751054852320675, "no_speech_prob": 2.4060882424237207e-06}, {"id": 251, "seek": 104160, "start": 1057.12, "end": 1061.84, "text": " So there's no copies being stored on your computer, but effectively it's almost like", "tokens": [407, 456, 311, 572, 14341, 885, 12187, 322, 428, 3820, 11, 457, 8659, 309, 311, 1920, 411], "temperature": 0.0, "avg_logprob": -0.155978614208745, "compression_ratio": 1.6751054852320675, "no_speech_prob": 2.4060882424237207e-06}, {"id": 252, "seek": 104160, "start": 1061.84, "end": 1067.12, "text": " there's infinitely slightly different copies because that's what the model ends up seeing.", "tokens": [456, 311, 36227, 4748, 819, 14341, 570, 300, 311, 437, 264, 2316, 5314, 493, 2577, 13], "temperature": 0.0, "avg_logprob": -0.155978614208745, "compression_ratio": 1.6751054852320675, "no_speech_prob": 2.4060882424237207e-06}, {"id": 253, "seek": 106712, "start": 1067.12, "end": 1071.76, "text": " So I hope that makes sense, Alex and everybody else.", "tokens": [407, 286, 1454, 300, 1669, 2020, 11, 5202, 293, 2201, 1646, 13], "temperature": 0.0, "avg_logprob": -0.1866694724205697, "compression_ratio": 1.5695067264573992, "no_speech_prob": 2.406089379292098e-06}, {"id": 254, "seek": 106712, "start": 1071.76, "end": 1074.76, "text": " So that's a great question.", "tokens": [407, 300, 311, 257, 869, 1168, 13], "temperature": 0.0, "avg_logprob": -0.1866694724205697, "compression_ratio": 1.5695067264573992, "no_speech_prob": 2.406089379292098e-06}, {"id": 255, "seek": 106712, "start": 1074.76, "end": 1080.1599999999999, "text": " Okay, so we've got, we're going to use random resized crop, we're going to use augmentation", "tokens": [1033, 11, 370, 321, 600, 658, 11, 321, 434, 516, 281, 764, 4974, 725, 1602, 9086, 11, 321, 434, 516, 281, 764, 14501, 19631], "temperature": 0.0, "avg_logprob": -0.1866694724205697, "compression_ratio": 1.5695067264573992, "no_speech_prob": 2.406089379292098e-06}, {"id": 256, "seek": 106712, "start": 1080.1599999999999, "end": 1085.8799999999999, "text": " transforms so that we can get data loaders from that and then we can go ahead and train", "tokens": [35592, 370, 300, 321, 393, 483, 1412, 3677, 433, 490, 300, 293, 550, 321, 393, 352, 2286, 293, 3847], "temperature": 0.0, "avg_logprob": -0.1866694724205697, "compression_ratio": 1.5695067264573992, "no_speech_prob": 2.406089379292098e-06}, {"id": 257, "seek": 106712, "start": 1085.8799999999999, "end": 1086.8799999999999, "text": " our model.", "tokens": [527, 2316, 13], "temperature": 0.0, "avg_logprob": -0.1866694724205697, "compression_ratio": 1.5695067264573992, "no_speech_prob": 2.406089379292098e-06}, {"id": 258, "seek": 106712, "start": 1086.8799999999999, "end": 1088.6, "text": " It takes about a minute.", "tokens": [467, 2516, 466, 257, 3456, 13], "temperature": 0.0, "avg_logprob": -0.1866694724205697, "compression_ratio": 1.5695067264573992, "no_speech_prob": 2.406089379292098e-06}, {"id": 259, "seek": 106712, "start": 1088.6, "end": 1093.12, "text": " In this case, we only did four epochs of fine tuning.", "tokens": [682, 341, 1389, 11, 321, 787, 630, 1451, 30992, 28346, 295, 2489, 15164, 13], "temperature": 0.0, "avg_logprob": -0.1866694724205697, "compression_ratio": 1.5695067264573992, "no_speech_prob": 2.406089379292098e-06}, {"id": 260, "seek": 109312, "start": 1093.12, "end": 1097.8, "text": " We'll talk about why there's five here later in the course, but four main epochs of fine", "tokens": [492, 603, 751, 466, 983, 456, 311, 1732, 510, 1780, 294, 264, 1164, 11, 457, 1451, 2135, 30992, 28346, 295, 2489], "temperature": 0.0, "avg_logprob": -0.14905822486208195, "compression_ratio": 1.5443548387096775, "no_speech_prob": 3.6119536162004806e-06}, {"id": 261, "seek": 109312, "start": 1097.8, "end": 1098.8, "text": " tuning.", "tokens": [15164, 13], "temperature": 0.0, "avg_logprob": -0.14905822486208195, "compression_ratio": 1.5443548387096775, "no_speech_prob": 3.6119536162004806e-06}, {"id": 262, "seek": 109312, "start": 1098.8, "end": 1103.28, "text": " So we probably didn't really need random resized crop and aug transforms because there's so", "tokens": [407, 321, 1391, 994, 380, 534, 643, 4974, 725, 1602, 9086, 293, 14501, 35592, 570, 456, 311, 370], "temperature": 0.0, "avg_logprob": -0.14905822486208195, "compression_ratio": 1.5443548387096775, "no_speech_prob": 3.6119536162004806e-06}, {"id": 263, "seek": 109312, "start": 1103.28, "end": 1109.2399999999998, "text": " few epochs, but you know, if you want to run more epochs, this is a good, good approach.", "tokens": [1326, 30992, 28346, 11, 457, 291, 458, 11, 498, 291, 528, 281, 1190, 544, 30992, 28346, 11, 341, 307, 257, 665, 11, 665, 3109, 13], "temperature": 0.0, "avg_logprob": -0.14905822486208195, "compression_ratio": 1.5443548387096775, "no_speech_prob": 3.6119536162004806e-06}, {"id": 264, "seek": 109312, "start": 1109.2399999999998, "end": 1110.7199999999998, "text": " Under 3% error.", "tokens": [6974, 805, 4, 6713, 13], "temperature": 0.0, "avg_logprob": -0.14905822486208195, "compression_ratio": 1.5443548387096775, "no_speech_prob": 3.6119536162004806e-06}, {"id": 265, "seek": 109312, "start": 1110.7199999999998, "end": 1111.7199999999998, "text": " That's good.", "tokens": [663, 311, 665, 13], "temperature": 0.0, "avg_logprob": -0.14905822486208195, "compression_ratio": 1.5443548387096775, "no_speech_prob": 3.6119536162004806e-06}, {"id": 266, "seek": 109312, "start": 1111.7199999999998, "end": 1112.7199999999998, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.14905822486208195, "compression_ratio": 1.5443548387096775, "no_speech_prob": 3.6119536162004806e-06}, {"id": 267, "seek": 109312, "start": 1112.7199999999998, "end": 1115.56, "text": " So remember I said we're going to train a model before we clean.", "tokens": [407, 1604, 286, 848, 321, 434, 516, 281, 3847, 257, 2316, 949, 321, 2541, 13], "temperature": 0.0, "avg_logprob": -0.14905822486208195, "compression_ratio": 1.5443548387096775, "no_speech_prob": 3.6119536162004806e-06}, {"id": 268, "seek": 109312, "start": 1115.56, "end": 1117.2399999999998, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.14905822486208195, "compression_ratio": 1.5443548387096775, "no_speech_prob": 3.6119536162004806e-06}, {"id": 269, "seek": 111724, "start": 1117.24, "end": 1126.56, "text": " So let's go ahead and train it.", "tokens": [407, 718, 311, 352, 2286, 293, 3847, 309, 13], "temperature": 0.0, "avg_logprob": -0.12775680753919813, "compression_ratio": 1.6237113402061856, "no_speech_prob": 2.1907696918788133e-06}, {"id": 270, "seek": 111724, "start": 1126.56, "end": 1133.0, "text": " So while that's training, that's running on my laptop, which only has a four gigabyte", "tokens": [407, 1339, 300, 311, 3097, 11, 300, 311, 2614, 322, 452, 10732, 11, 597, 787, 575, 257, 1451, 8741, 34529], "temperature": 0.0, "avg_logprob": -0.12775680753919813, "compression_ratio": 1.6237113402061856, "no_speech_prob": 2.1907696918788133e-06}, {"id": 271, "seek": 111724, "start": 1133.0, "end": 1134.0, "text": " GPU.", "tokens": [18407, 13], "temperature": 0.0, "avg_logprob": -0.12775680753919813, "compression_ratio": 1.6237113402061856, "no_speech_prob": 2.1907696918788133e-06}, {"id": 272, "seek": 111724, "start": 1134.0, "end": 1136.1200000000001, "text": " It's pretty basic, but it's enough to get started.", "tokens": [467, 311, 1238, 3875, 11, 457, 309, 311, 1547, 281, 483, 1409, 13], "temperature": 0.0, "avg_logprob": -0.12775680753919813, "compression_ratio": 1.6237113402061856, "no_speech_prob": 2.1907696918788133e-06}, {"id": 273, "seek": 111724, "start": 1136.1200000000001, "end": 1139.58, "text": " While that's training, we'll take a look at the next one.", "tokens": [3987, 300, 311, 3097, 11, 321, 603, 747, 257, 574, 412, 264, 958, 472, 13], "temperature": 0.0, "avg_logprob": -0.12775680753919813, "compression_ratio": 1.6237113402061856, "no_speech_prob": 2.1907696918788133e-06}, {"id": 274, "seek": 111724, "start": 1139.58, "end": 1143.76, "text": " So the first thing we're going to look at is the confusion matrix and the confusion", "tokens": [407, 264, 700, 551, 321, 434, 516, 281, 574, 412, 307, 264, 15075, 8141, 293, 264, 15075], "temperature": 0.0, "avg_logprob": -0.12775680753919813, "compression_ratio": 1.6237113402061856, "no_speech_prob": 2.1907696918788133e-06}, {"id": 275, "seek": 114376, "start": 1143.76, "end": 1149.68, "text": " matrix is something that it only is meaningful for when your labels are categories.", "tokens": [8141, 307, 746, 300, 309, 787, 307, 10995, 337, 562, 428, 16949, 366, 10479, 13], "temperature": 0.0, "avg_logprob": -0.14611371883676072, "compression_ratio": 2.037974683544304, "no_speech_prob": 1.0451457455928903e-05}, {"id": 276, "seek": 114376, "start": 1149.68, "end": 1153.58, "text": " And what it says is hell, what category errors are you making?", "tokens": [400, 437, 309, 1619, 307, 4921, 11, 437, 7719, 13603, 366, 291, 1455, 30], "temperature": 0.0, "avg_logprob": -0.14611371883676072, "compression_ratio": 2.037974683544304, "no_speech_prob": 1.0451457455928903e-05}, {"id": 277, "seek": 114376, "start": 1153.58, "end": 1159.64, "text": " And so this is showing that the model that we've got at this point, there was two times", "tokens": [400, 370, 341, 307, 4099, 300, 264, 2316, 300, 321, 600, 658, 412, 341, 935, 11, 456, 390, 732, 1413], "temperature": 0.0, "avg_logprob": -0.14611371883676072, "compression_ratio": 2.037974683544304, "no_speech_prob": 1.0451457455928903e-05}, {"id": 278, "seek": 114376, "start": 1159.64, "end": 1164.92, "text": " when there was actually a grizzly bear and it thought it was a black bear.", "tokens": [562, 456, 390, 767, 257, 17865, 4313, 356, 6155, 293, 309, 1194, 309, 390, 257, 2211, 6155, 13], "temperature": 0.0, "avg_logprob": -0.14611371883676072, "compression_ratio": 2.037974683544304, "no_speech_prob": 1.0451457455928903e-05}, {"id": 279, "seek": 114376, "start": 1164.92, "end": 1168.32, "text": " And there was two times when there was actually a black bear and it thought it was a grizzly", "tokens": [400, 456, 390, 732, 1413, 562, 456, 390, 767, 257, 2211, 6155, 293, 309, 1194, 309, 390, 257, 17865, 4313, 356], "temperature": 0.0, "avg_logprob": -0.14611371883676072, "compression_ratio": 2.037974683544304, "no_speech_prob": 1.0451457455928903e-05}, {"id": 280, "seek": 114376, "start": 1168.32, "end": 1172.32, "text": " bear and there was no times that it got Teddy's wrong, which makes sense, right?", "tokens": [6155, 293, 456, 390, 572, 1413, 300, 309, 658, 34330, 311, 2085, 11, 597, 1669, 2020, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.14611371883676072, "compression_ratio": 2.037974683544304, "no_speech_prob": 1.0451457455928903e-05}, {"id": 281, "seek": 117232, "start": 1172.32, "end": 1179.28, "text": " Teddy's two look quite different to me.", "tokens": [34330, 311, 732, 574, 1596, 819, 281, 385, 13], "temperature": 0.0, "avg_logprob": -0.17052620428579826, "compression_ratio": 1.6135458167330676, "no_speech_prob": 2.156804384867428e-06}, {"id": 282, "seek": 117232, "start": 1179.28, "end": 1183.8, "text": " In a lot of situations, when you look at this, it'll kind of give you a real sense of like,", "tokens": [682, 257, 688, 295, 6851, 11, 562, 291, 574, 412, 341, 11, 309, 603, 733, 295, 976, 291, 257, 957, 2020, 295, 411, 11], "temperature": 0.0, "avg_logprob": -0.17052620428579826, "compression_ratio": 1.6135458167330676, "no_speech_prob": 2.156804384867428e-06}, {"id": 283, "seek": 117232, "start": 1183.8, "end": 1185.2, "text": " okay, well, what are the hard ones?", "tokens": [1392, 11, 731, 11, 437, 366, 264, 1152, 2306, 30], "temperature": 0.0, "avg_logprob": -0.17052620428579826, "compression_ratio": 1.6135458167330676, "no_speech_prob": 2.156804384867428e-06}, {"id": 284, "seek": 117232, "start": 1185.2, "end": 1186.2, "text": " Right?", "tokens": [1779, 30], "temperature": 0.0, "avg_logprob": -0.17052620428579826, "compression_ratio": 1.6135458167330676, "no_speech_prob": 2.156804384867428e-06}, {"id": 285, "seek": 117232, "start": 1186.2, "end": 1190.04, "text": " So for example, if you use the pets data set that we quite often play with in the book", "tokens": [407, 337, 1365, 11, 498, 291, 764, 264, 19897, 1412, 992, 300, 321, 1596, 2049, 862, 365, 294, 264, 1446], "temperature": 0.0, "avg_logprob": -0.17052620428579826, "compression_ratio": 1.6135458167330676, "no_speech_prob": 2.156804384867428e-06}, {"id": 286, "seek": 117232, "start": 1190.04, "end": 1196.9199999999998, "text": " and the course, this classification metric matrix for different breeds of pet, you know,", "tokens": [293, 264, 1164, 11, 341, 21538, 20678, 8141, 337, 819, 41609, 295, 3817, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.17052620428579826, "compression_ratio": 1.6135458167330676, "no_speech_prob": 2.156804384867428e-06}, {"id": 287, "seek": 117232, "start": 1196.9199999999998, "end": 1199.8, "text": " really shows you which ones are difficult to identify.", "tokens": [534, 3110, 291, 597, 2306, 366, 2252, 281, 5876, 13], "temperature": 0.0, "avg_logprob": -0.17052620428579826, "compression_ratio": 1.6135458167330676, "no_speech_prob": 2.156804384867428e-06}, {"id": 288, "seek": 119980, "start": 1199.8, "end": 1206.2, "text": " And I've actually gone in and like read Wikipedia pages and, and, and pet breeding reports about", "tokens": [400, 286, 600, 767, 2780, 294, 293, 411, 1401, 28999, 7183, 293, 11, 293, 11, 293, 3817, 26051, 7122, 466], "temperature": 0.0, "avg_logprob": -0.12863919394356862, "compression_ratio": 1.650190114068441, "no_speech_prob": 9.132519380727899e-07}, {"id": 289, "seek": 119980, "start": 1206.2, "end": 1210.36, "text": " how to identify these particular types because it's so difficult and even experts find it", "tokens": [577, 281, 5876, 613, 1729, 3467, 570, 309, 311, 370, 2252, 293, 754, 8572, 915, 309], "temperature": 0.0, "avg_logprob": -0.12863919394356862, "compression_ratio": 1.650190114068441, "no_speech_prob": 9.132519380727899e-07}, {"id": 290, "seek": 119980, "start": 1210.36, "end": 1211.36, "text": " difficult.", "tokens": [2252, 13], "temperature": 0.0, "avg_logprob": -0.12863919394356862, "compression_ratio": 1.650190114068441, "no_speech_prob": 9.132519380727899e-07}, {"id": 291, "seek": 119980, "start": 1211.36, "end": 1214.9199999999998, "text": " And one of the things I've learned from doing the course actually is black bears and grizzly", "tokens": [400, 472, 295, 264, 721, 286, 600, 3264, 490, 884, 264, 1164, 767, 307, 2211, 17276, 293, 17865, 4313, 356], "temperature": 0.0, "avg_logprob": -0.12863919394356862, "compression_ratio": 1.650190114068441, "no_speech_prob": 9.132519380727899e-07}, {"id": 292, "seek": 119980, "start": 1214.9199999999998, "end": 1219.3799999999999, "text": " bears are much harder to pick apart than I had realized.", "tokens": [17276, 366, 709, 6081, 281, 1888, 4936, 813, 286, 632, 5334, 13], "temperature": 0.0, "avg_logprob": -0.12863919394356862, "compression_ratio": 1.650190114068441, "no_speech_prob": 9.132519380727899e-07}, {"id": 293, "seek": 119980, "start": 1219.3799999999999, "end": 1223.9199999999998, "text": " So I'm not even going to try, but I'll show you the really interesting thing we can do", "tokens": [407, 286, 478, 406, 754, 516, 281, 853, 11, 457, 286, 603, 855, 291, 264, 534, 1880, 551, 321, 393, 360], "temperature": 0.0, "avg_logprob": -0.12863919394356862, "compression_ratio": 1.650190114068441, "no_speech_prob": 9.132519380727899e-07}, {"id": 294, "seek": 122392, "start": 1223.92, "end": 1231.96, "text": " with this model is that now we've created this classification interpretation object,", "tokens": [365, 341, 2316, 307, 300, 586, 321, 600, 2942, 341, 21538, 14174, 2657, 11], "temperature": 0.0, "avg_logprob": -0.14356632952420217, "compression_ratio": 1.5488721804511278, "no_speech_prob": 1.6797239368315786e-06}, {"id": 295, "seek": 122392, "start": 1231.96, "end": 1233.8000000000002, "text": " which we use for confusion metrics.", "tokens": [597, 321, 764, 337, 15075, 16367, 13], "temperature": 0.0, "avg_logprob": -0.14356632952420217, "compression_ratio": 1.5488721804511278, "no_speech_prob": 1.6797239368315786e-06}, {"id": 296, "seek": 122392, "start": 1233.8000000000002, "end": 1242.68, "text": " We can say plot top losses.", "tokens": [492, 393, 584, 7542, 1192, 15352, 13], "temperature": 0.0, "avg_logprob": -0.14356632952420217, "compression_ratio": 1.5488721804511278, "no_speech_prob": 1.6797239368315786e-06}, {"id": 297, "seek": 122392, "start": 1242.68, "end": 1245.68, "text": " We can say plot top losses.", "tokens": [492, 393, 584, 7542, 1192, 15352, 13], "temperature": 0.0, "avg_logprob": -0.14356632952420217, "compression_ratio": 1.5488721804511278, "no_speech_prob": 1.6797239368315786e-06}, {"id": 298, "seek": 122392, "start": 1245.68, "end": 1247.64, "text": " And this is very interesting.", "tokens": [400, 341, 307, 588, 1880, 13], "temperature": 0.0, "avg_logprob": -0.14356632952420217, "compression_ratio": 1.5488721804511278, "no_speech_prob": 1.6797239368315786e-06}, {"id": 299, "seek": 124764, "start": 1247.64, "end": 1254.1200000000001, "text": " What it does is it tells us the places where the loss is the highest.", "tokens": [708, 309, 775, 307, 309, 5112, 505, 264, 3190, 689, 264, 4470, 307, 264, 6343, 13], "temperature": 0.0, "avg_logprob": -0.07137547263616248, "compression_ratio": 1.5909090909090908, "no_speech_prob": 6.786712560824526e-07}, {"id": 300, "seek": 124764, "start": 1254.1200000000001, "end": 1260.64, "text": " Now if you remember from the last lesson, the loss is that measurement of how good our", "tokens": [823, 498, 291, 1604, 490, 264, 1036, 6898, 11, 264, 4470, 307, 300, 13160, 295, 577, 665, 527], "temperature": 0.0, "avg_logprob": -0.07137547263616248, "compression_ratio": 1.5909090909090908, "no_speech_prob": 6.786712560824526e-07}, {"id": 301, "seek": 124764, "start": 1260.64, "end": 1267.66, "text": " model is that we take after each time we run through an item of data, a loss will be bad", "tokens": [2316, 307, 300, 321, 747, 934, 1184, 565, 321, 1190, 807, 364, 3174, 295, 1412, 11, 257, 4470, 486, 312, 1578], "temperature": 0.0, "avg_logprob": -0.07137547263616248, "compression_ratio": 1.5909090909090908, "no_speech_prob": 6.786712560824526e-07}, {"id": 302, "seek": 124764, "start": 1267.66, "end": 1273.4, "text": " if we predict wrongly and we're very confident about that prediction.", "tokens": [498, 321, 6069, 2085, 356, 293, 321, 434, 588, 6679, 466, 300, 17630, 13], "temperature": 0.0, "avg_logprob": -0.07137547263616248, "compression_ratio": 1.5909090909090908, "no_speech_prob": 6.786712560824526e-07}, {"id": 303, "seek": 127340, "start": 1273.4, "end": 1278.8000000000002, "text": " So here's an example where we predicted his the order here prediction, actual loss probability", "tokens": [407, 510, 311, 364, 1365, 689, 321, 19147, 702, 264, 1668, 510, 17630, 11, 3539, 4470, 8482], "temperature": 0.0, "avg_logprob": -0.17827253252546363, "compression_ratio": 1.7, "no_speech_prob": 3.3931273719645105e-06}, {"id": 304, "seek": 127340, "start": 1278.8000000000002, "end": 1286.8000000000002, "text": " where we predicted grizzly and it was actually a black and we were 96% sure our model was", "tokens": [689, 321, 19147, 17865, 4313, 356, 293, 309, 390, 767, 257, 2211, 293, 321, 645, 24124, 4, 988, 527, 2316, 390], "temperature": 0.0, "avg_logprob": -0.17827253252546363, "compression_ratio": 1.7, "no_speech_prob": 3.3931273719645105e-06}, {"id": 305, "seek": 127340, "start": 1286.8000000000002, "end": 1289.18, "text": " that it's a grizzly.", "tokens": [300, 309, 311, 257, 17865, 4313, 356, 13], "temperature": 0.0, "avg_logprob": -0.17827253252546363, "compression_ratio": 1.7, "no_speech_prob": 3.3931273719645105e-06}, {"id": 306, "seek": 127340, "start": 1289.18, "end": 1294.24, "text": " Now I don't know enough about bears to know whether the model made a mistake or whether", "tokens": [823, 286, 500, 380, 458, 1547, 466, 17276, 281, 458, 1968, 264, 2316, 1027, 257, 6146, 420, 1968], "temperature": 0.0, "avg_logprob": -0.17827253252546363, "compression_ratio": 1.7, "no_speech_prob": 3.3931273719645105e-06}, {"id": 307, "seek": 127340, "start": 1294.24, "end": 1298.6000000000001, "text": " this actually is a picture of a grizzly bear.", "tokens": [341, 767, 307, 257, 3036, 295, 257, 17865, 4313, 356, 6155, 13], "temperature": 0.0, "avg_logprob": -0.17827253252546363, "compression_ratio": 1.7, "no_speech_prob": 3.3931273719645105e-06}, {"id": 308, "seek": 127340, "start": 1298.6000000000001, "end": 1301.92, "text": " But so an expert would obviously go back and check those out.", "tokens": [583, 370, 364, 5844, 576, 2745, 352, 646, 293, 1520, 729, 484, 13], "temperature": 0.0, "avg_logprob": -0.17827253252546363, "compression_ratio": 1.7, "no_speech_prob": 3.3931273719645105e-06}, {"id": 309, "seek": 127340, "start": 1301.92, "end": 1302.92, "text": " Right?", "tokens": [1779, 30], "temperature": 0.0, "avg_logprob": -0.17827253252546363, "compression_ratio": 1.7, "no_speech_prob": 3.3931273719645105e-06}, {"id": 310, "seek": 130292, "start": 1302.92, "end": 1305.24, "text": " Now you'll notice a couple here.", "tokens": [823, 291, 603, 3449, 257, 1916, 510, 13], "temperature": 0.0, "avg_logprob": -0.20307326316833496, "compression_ratio": 1.7104072398190044, "no_speech_prob": 6.6433995016268454e-06}, {"id": 311, "seek": 130292, "start": 1305.24, "end": 1308.28, "text": " It's got grizzly grizzly Teddy Teddy.", "tokens": [467, 311, 658, 17865, 4313, 356, 17865, 4313, 356, 34330, 34330, 13], "temperature": 0.0, "avg_logprob": -0.20307326316833496, "compression_ratio": 1.7104072398190044, "no_speech_prob": 6.6433995016268454e-06}, {"id": 312, "seek": 130292, "start": 1308.28, "end": 1309.28, "text": " They're actually correct.", "tokens": [814, 434, 767, 3006, 13], "temperature": 0.0, "avg_logprob": -0.20307326316833496, "compression_ratio": 1.7104072398190044, "no_speech_prob": 6.6433995016268454e-06}, {"id": 313, "seek": 130292, "start": 1309.28, "end": 1310.28, "text": " Right?", "tokens": [1779, 30], "temperature": 0.0, "avg_logprob": -0.20307326316833496, "compression_ratio": 1.7104072398190044, "no_speech_prob": 6.6433995016268454e-06}, {"id": 314, "seek": 130292, "start": 1310.28, "end": 1312.2, "text": " So why is this loss bad when it's correct?", "tokens": [407, 983, 307, 341, 4470, 1578, 562, 309, 311, 3006, 30], "temperature": 0.0, "avg_logprob": -0.20307326316833496, "compression_ratio": 1.7104072398190044, "no_speech_prob": 6.6433995016268454e-06}, {"id": 315, "seek": 130292, "start": 1312.2, "end": 1314.48, "text": " And the reason is because it wasn't very confident.", "tokens": [400, 264, 1778, 307, 570, 309, 2067, 380, 588, 6679, 13], "temperature": 0.0, "avg_logprob": -0.20307326316833496, "compression_ratio": 1.7104072398190044, "no_speech_prob": 6.6433995016268454e-06}, {"id": 316, "seek": 130292, "start": 1314.48, "end": 1316.52, "text": " It was only 66% confident.", "tokens": [467, 390, 787, 21126, 4, 6679, 13], "temperature": 0.0, "avg_logprob": -0.20307326316833496, "compression_ratio": 1.7104072398190044, "no_speech_prob": 6.6433995016268454e-06}, {"id": 317, "seek": 130292, "start": 1316.52, "end": 1317.52, "text": " Right?", "tokens": [1779, 30], "temperature": 0.0, "avg_logprob": -0.20307326316833496, "compression_ratio": 1.7104072398190044, "no_speech_prob": 6.6433995016268454e-06}, {"id": 318, "seek": 130292, "start": 1317.52, "end": 1318.72, "text": " So here's a Teddy.", "tokens": [407, 510, 311, 257, 34330, 13], "temperature": 0.0, "avg_logprob": -0.20307326316833496, "compression_ratio": 1.7104072398190044, "no_speech_prob": 6.6433995016268454e-06}, {"id": 319, "seek": 130292, "start": 1318.72, "end": 1320.6000000000001, "text": " It's only 72% confident.", "tokens": [467, 311, 787, 18731, 4, 6679, 13], "temperature": 0.0, "avg_logprob": -0.20307326316833496, "compression_ratio": 1.7104072398190044, "no_speech_prob": 6.6433995016268454e-06}, {"id": 320, "seek": 130292, "start": 1320.6000000000001, "end": 1321.6000000000001, "text": " Right?", "tokens": [1779, 30], "temperature": 0.0, "avg_logprob": -0.20307326316833496, "compression_ratio": 1.7104072398190044, "no_speech_prob": 6.6433995016268454e-06}, {"id": 321, "seek": 130292, "start": 1321.6000000000001, "end": 1328.04, "text": " So you can have a bad loss either by being wrong and confident or being right and unconfident.", "tokens": [407, 291, 393, 362, 257, 1578, 4470, 2139, 538, 885, 2085, 293, 6679, 420, 885, 558, 293, 517, 24697, 1078, 13], "temperature": 0.0, "avg_logprob": -0.20307326316833496, "compression_ratio": 1.7104072398190044, "no_speech_prob": 6.6433995016268454e-06}, {"id": 322, "seek": 132804, "start": 1328.04, "end": 1334.8799999999999, "text": " Now the reason that's really helpful is that now we can use something called the fast AI", "tokens": [823, 264, 1778, 300, 311, 534, 4961, 307, 300, 586, 321, 393, 764, 746, 1219, 264, 2370, 7318], "temperature": 0.0, "avg_logprob": -0.1276095708211263, "compression_ratio": 1.6277056277056277, "no_speech_prob": 2.0261334157112287e-06}, {"id": 323, "seek": 132804, "start": 1334.8799999999999, "end": 1345.1399999999999, "text": " image classifier cleaner to clean up the ones that are wrongly labeled in our data set.", "tokens": [3256, 1508, 9902, 16532, 281, 2541, 493, 264, 2306, 300, 366, 2085, 356, 21335, 294, 527, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.1276095708211263, "compression_ratio": 1.6277056277056277, "no_speech_prob": 2.0261334157112287e-06}, {"id": 324, "seek": 132804, "start": 1345.1399999999999, "end": 1349.6, "text": " So when we use the image classifier cleaner, it actually runs our models.", "tokens": [407, 562, 321, 764, 264, 3256, 1508, 9902, 16532, 11, 309, 767, 6676, 527, 5245, 13], "temperature": 0.0, "avg_logprob": -0.1276095708211263, "compression_ratio": 1.6277056277056277, "no_speech_prob": 2.0261334157112287e-06}, {"id": 325, "seek": 132804, "start": 1349.6, "end": 1351.32, "text": " That's why we pass it learn.", "tokens": [663, 311, 983, 321, 1320, 309, 1466, 13], "temperature": 0.0, "avg_logprob": -0.1276095708211263, "compression_ratio": 1.6277056277056277, "no_speech_prob": 2.0261334157112287e-06}, {"id": 326, "seek": 132804, "start": 1351.32, "end": 1352.32, "text": " Right?", "tokens": [1779, 30], "temperature": 0.0, "avg_logprob": -0.1276095708211263, "compression_ratio": 1.6277056277056277, "no_speech_prob": 2.0261334157112287e-06}, {"id": 327, "seek": 132804, "start": 1352.32, "end": 1356.96, "text": " And I mentioned that I don't know much about black bears and grizzly bears, but I do know", "tokens": [400, 286, 2835, 300, 286, 500, 380, 458, 709, 466, 2211, 17276, 293, 17865, 4313, 356, 17276, 11, 457, 286, 360, 458], "temperature": 0.0, "avg_logprob": -0.1276095708211263, "compression_ratio": 1.6277056277056277, "no_speech_prob": 2.0261334157112287e-06}, {"id": 328, "seek": 135696, "start": 1356.96, "end": 1358.24, "text": " a lot about teddy bears.", "tokens": [257, 688, 466, 45116, 17276, 13], "temperature": 0.0, "avg_logprob": -0.16402956904197225, "compression_ratio": 1.84, "no_speech_prob": 2.2959106900088955e-06}, {"id": 329, "seek": 135696, "start": 1358.24, "end": 1361.28, "text": " So I'll pick teddy bears.", "tokens": [407, 286, 603, 1888, 45116, 17276, 13], "temperature": 0.0, "avg_logprob": -0.16402956904197225, "compression_ratio": 1.84, "no_speech_prob": 2.2959106900088955e-06}, {"id": 330, "seek": 135696, "start": 1361.28, "end": 1365.32, "text": " And if I click teddy bears, it's now showing me all the things in the training set.", "tokens": [400, 498, 286, 2052, 45116, 17276, 11, 309, 311, 586, 4099, 385, 439, 264, 721, 294, 264, 3097, 992, 13], "temperature": 0.0, "avg_logprob": -0.16402956904197225, "compression_ratio": 1.84, "no_speech_prob": 2.2959106900088955e-06}, {"id": 331, "seek": 135696, "start": 1365.32, "end": 1368.64, "text": " You can pick training or valid that were marked as teddy bears.", "tokens": [509, 393, 1888, 3097, 420, 7363, 300, 645, 12658, 382, 45116, 17276, 13], "temperature": 0.0, "avg_logprob": -0.16402956904197225, "compression_ratio": 1.84, "no_speech_prob": 2.2959106900088955e-06}, {"id": 332, "seek": 135696, "start": 1368.64, "end": 1370.52, "text": " And here's what's really important.", "tokens": [400, 510, 311, 437, 311, 534, 1021, 13], "temperature": 0.0, "avg_logprob": -0.16402956904197225, "compression_ratio": 1.84, "no_speech_prob": 2.2959106900088955e-06}, {"id": 333, "seek": 135696, "start": 1370.52, "end": 1372.68, "text": " They're ordered by loss.", "tokens": [814, 434, 8866, 538, 4470, 13], "temperature": 0.0, "avg_logprob": -0.16402956904197225, "compression_ratio": 1.84, "no_speech_prob": 2.2959106900088955e-06}, {"id": 334, "seek": 135696, "start": 1372.68, "end": 1374.24, "text": " So they're ordered by confidence.", "tokens": [407, 436, 434, 8866, 538, 6687, 13], "temperature": 0.0, "avg_logprob": -0.16402956904197225, "compression_ratio": 1.84, "no_speech_prob": 2.2959106900088955e-06}, {"id": 335, "seek": 135696, "start": 1374.24, "end": 1375.24, "text": " Right?", "tokens": [1779, 30], "temperature": 0.0, "avg_logprob": -0.16402956904197225, "compression_ratio": 1.84, "no_speech_prob": 2.2959106900088955e-06}, {"id": 336, "seek": 135696, "start": 1375.24, "end": 1378.88, "text": " So I can scroll through just the first few and check they're correct.", "tokens": [407, 286, 393, 11369, 807, 445, 264, 700, 1326, 293, 1520, 436, 434, 3006, 13], "temperature": 0.0, "avg_logprob": -0.16402956904197225, "compression_ratio": 1.84, "no_speech_prob": 2.2959106900088955e-06}, {"id": 337, "seek": 135696, "start": 1378.88, "end": 1379.88, "text": " Right?", "tokens": [1779, 30], "temperature": 0.0, "avg_logprob": -0.16402956904197225, "compression_ratio": 1.84, "no_speech_prob": 2.2959106900088955e-06}, {"id": 338, "seek": 135696, "start": 1379.88, "end": 1381.6000000000001, "text": " And oh, here's a mistake.", "tokens": [400, 1954, 11, 510, 311, 257, 6146, 13], "temperature": 0.0, "avg_logprob": -0.16402956904197225, "compression_ratio": 1.84, "no_speech_prob": 2.2959106900088955e-06}, {"id": 339, "seek": 135696, "start": 1381.6000000000001, "end": 1382.6000000000001, "text": " Right?", "tokens": [1779, 30], "temperature": 0.0, "avg_logprob": -0.16402956904197225, "compression_ratio": 1.84, "no_speech_prob": 2.2959106900088955e-06}, {"id": 340, "seek": 135696, "start": 1382.6000000000001, "end": 1386.88, "text": " So when I find one that was wrongly gathered, I can either put it if it's in the wrong category", "tokens": [407, 562, 286, 915, 472, 300, 390, 2085, 356, 13032, 11, 286, 393, 2139, 829, 309, 498, 309, 311, 294, 264, 2085, 7719], "temperature": 0.0, "avg_logprob": -0.16402956904197225, "compression_ratio": 1.84, "no_speech_prob": 2.2959106900088955e-06}, {"id": 341, "seek": 138688, "start": 1386.88, "end": 1391.16, "text": " and I can choose the correct category or if it shouldn't be there at all, I click delete.", "tokens": [293, 286, 393, 2826, 264, 3006, 7719, 420, 498, 309, 4659, 380, 312, 456, 412, 439, 11, 286, 2052, 12097, 13], "temperature": 0.0, "avg_logprob": -0.19663264410836356, "compression_ratio": 1.7183098591549295, "no_speech_prob": 6.439008757297415e-06}, {"id": 342, "seek": 138688, "start": 1391.16, "end": 1393.4, "text": " So here I'll go ahead and click delete.", "tokens": [407, 510, 286, 603, 352, 2286, 293, 2052, 12097, 13], "temperature": 0.0, "avg_logprob": -0.19663264410836356, "compression_ratio": 1.7183098591549295, "no_speech_prob": 6.439008757297415e-06}, {"id": 343, "seek": 138688, "start": 1393.4, "end": 1394.4, "text": " Right?", "tokens": [1779, 30], "temperature": 0.0, "avg_logprob": -0.19663264410836356, "compression_ratio": 1.7183098591549295, "no_speech_prob": 6.439008757297415e-06}, {"id": 344, "seek": 138688, "start": 1394.4, "end": 1396.44, "text": " So I can see some reasons that some of these are hard.", "tokens": [407, 286, 393, 536, 512, 4112, 300, 512, 295, 613, 366, 1152, 13], "temperature": 0.0, "avg_logprob": -0.19663264410836356, "compression_ratio": 1.7183098591549295, "no_speech_prob": 6.439008757297415e-06}, {"id": 345, "seek": 138688, "start": 1396.44, "end": 1399.5600000000002, "text": " Like for example, here's two teddies, which is just, I guess, confusing.", "tokens": [1743, 337, 1365, 11, 510, 311, 732, 22337, 22018, 11, 597, 307, 445, 11, 286, 2041, 11, 13181, 13], "temperature": 0.0, "avg_logprob": -0.19663264410836356, "compression_ratio": 1.7183098591549295, "no_speech_prob": 6.439008757297415e-06}, {"id": 346, "seek": 138688, "start": 1399.5600000000002, "end": 1400.88, "text": " So it doesn't see that often.", "tokens": [407, 309, 1177, 380, 536, 300, 2049, 13], "temperature": 0.0, "avg_logprob": -0.19663264410836356, "compression_ratio": 1.7183098591549295, "no_speech_prob": 6.439008757297415e-06}, {"id": 347, "seek": 138688, "start": 1400.88, "end": 1403.48, "text": " This one here is a bit weird looking.", "tokens": [639, 472, 510, 307, 257, 857, 3657, 1237, 13], "temperature": 0.0, "avg_logprob": -0.19663264410836356, "compression_ratio": 1.7183098591549295, "no_speech_prob": 6.439008757297415e-06}, {"id": 348, "seek": 138688, "start": 1403.48, "end": 1406.8000000000002, "text": " It looks almost like a wombat.", "tokens": [467, 1542, 1920, 411, 257, 1579, 11980, 13], "temperature": 0.0, "avg_logprob": -0.19663264410836356, "compression_ratio": 1.7183098591549295, "no_speech_prob": 6.439008757297415e-06}, {"id": 349, "seek": 138688, "start": 1406.8000000000002, "end": 1412.16, "text": " This is an awful lot of teddies.", "tokens": [639, 307, 364, 11232, 688, 295, 22337, 22018, 13], "temperature": 0.0, "avg_logprob": -0.19663264410836356, "compression_ratio": 1.7183098591549295, "no_speech_prob": 6.439008757297415e-06}, {"id": 350, "seek": 138688, "start": 1412.16, "end": 1415.2800000000002, "text": " This one maybe is just a bit hard to see from the background, but these otherwise they look", "tokens": [639, 472, 1310, 307, 445, 257, 857, 1152, 281, 536, 490, 264, 3678, 11, 457, 613, 5911, 436, 574], "temperature": 0.0, "avg_logprob": -0.19663264410836356, "compression_ratio": 1.7183098591549295, "no_speech_prob": 6.439008757297415e-06}, {"id": 351, "seek": 141528, "start": 1415.28, "end": 1418.04, "text": " fine.", "tokens": [2489, 13], "temperature": 0.0, "avg_logprob": -0.20113432585303462, "compression_ratio": 1.7272727272727273, "no_speech_prob": 3.4465654152882053e-06}, {"id": 352, "seek": 141528, "start": 1418.04, "end": 1419.6399999999999, "text": " So we just look through the first few.", "tokens": [407, 321, 445, 574, 807, 264, 700, 1326, 13], "temperature": 0.0, "avg_logprob": -0.20113432585303462, "compression_ratio": 1.7272727272727273, "no_speech_prob": 3.4465654152882053e-06}, {"id": 353, "seek": 141528, "start": 1419.6399999999999, "end": 1422.8999999999999, "text": " And if you don't see any problem or problems in the first few, you're probably fine.", "tokens": [400, 498, 291, 500, 380, 536, 604, 1154, 420, 2740, 294, 264, 700, 1326, 11, 291, 434, 1391, 2489, 13], "temperature": 0.0, "avg_logprob": -0.20113432585303462, "compression_ratio": 1.7272727272727273, "no_speech_prob": 3.4465654152882053e-06}, {"id": 354, "seek": 141528, "start": 1422.8999999999999, "end": 1424.84, "text": " So that's cleaned up our training set.", "tokens": [407, 300, 311, 16146, 493, 527, 3097, 992, 13], "temperature": 0.0, "avg_logprob": -0.20113432585303462, "compression_ratio": 1.7272727272727273, "no_speech_prob": 3.4465654152882053e-06}, {"id": 355, "seek": 141528, "start": 1424.84, "end": 1430.36, "text": " Let's clean up our validation set as well.", "tokens": [961, 311, 2541, 493, 527, 24071, 992, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.20113432585303462, "compression_ratio": 1.7272727272727273, "no_speech_prob": 3.4465654152882053e-06}, {"id": 356, "seek": 141528, "start": 1430.36, "end": 1431.56, "text": " So here's that one it had trouble with.", "tokens": [407, 510, 311, 300, 472, 309, 632, 5253, 365, 13], "temperature": 0.0, "avg_logprob": -0.20113432585303462, "compression_ratio": 1.7272727272727273, "no_speech_prob": 3.4465654152882053e-06}, {"id": 357, "seek": 141528, "start": 1431.56, "end": 1434.72, "text": " I don't know why it had trouble with that one, but so be it.", "tokens": [286, 500, 380, 458, 983, 309, 632, 5253, 365, 300, 472, 11, 457, 370, 312, 309, 13], "temperature": 0.0, "avg_logprob": -0.20113432585303462, "compression_ratio": 1.7272727272727273, "no_speech_prob": 3.4465654152882053e-06}, {"id": 358, "seek": 141528, "start": 1434.72, "end": 1437.6399999999999, "text": " And we'll have a quick scroll through.", "tokens": [400, 321, 603, 362, 257, 1702, 11369, 807, 13], "temperature": 0.0, "avg_logprob": -0.20113432585303462, "compression_ratio": 1.7272727272727273, "no_speech_prob": 3.4465654152882053e-06}, {"id": 359, "seek": 141528, "start": 1437.6399999999999, "end": 1440.16, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.20113432585303462, "compression_ratio": 1.7272727272727273, "no_speech_prob": 3.4465654152882053e-06}, {"id": 360, "seek": 141528, "start": 1440.16, "end": 1442.6399999999999, "text": " I'm not really sure that's a bear.", "tokens": [286, 478, 406, 534, 988, 300, 311, 257, 6155, 13], "temperature": 0.0, "avg_logprob": -0.20113432585303462, "compression_ratio": 1.7272727272727273, "no_speech_prob": 3.4465654152882053e-06}, {"id": 361, "seek": 141528, "start": 1442.6399999999999, "end": 1444.24, "text": " So I'm just going to go ahead and delete it.", "tokens": [407, 286, 478, 445, 516, 281, 352, 2286, 293, 12097, 309, 13], "temperature": 0.0, "avg_logprob": -0.20113432585303462, "compression_ratio": 1.7272727272727273, "no_speech_prob": 3.4465654152882053e-06}, {"id": 362, "seek": 144424, "start": 1444.24, "end": 1450.6, "text": " So teddy something, but you know, it's a problem.", "tokens": [407, 45116, 746, 11, 457, 291, 458, 11, 309, 311, 257, 1154, 13], "temperature": 0.0, "avg_logprob": -0.1232208164258935, "compression_ratio": 1.6065573770491803, "no_speech_prob": 2.123372269124957e-06}, {"id": 363, "seek": 144424, "start": 1450.6, "end": 1455.56, "text": " Okay that's not a teddy either.", "tokens": [1033, 300, 311, 406, 257, 45116, 2139, 13], "temperature": 0.0, "avg_logprob": -0.1232208164258935, "compression_ratio": 1.6065573770491803, "no_speech_prob": 2.123372269124957e-06}, {"id": 364, "seek": 144424, "start": 1455.56, "end": 1456.96, "text": " So you see the idea, right?", "tokens": [407, 291, 536, 264, 1558, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1232208164258935, "compression_ratio": 1.6065573770491803, "no_speech_prob": 2.123372269124957e-06}, {"id": 365, "seek": 144424, "start": 1456.96, "end": 1463.52, "text": " So after we've done that, what that does is the cleaner has now stored a list of the ones", "tokens": [407, 934, 321, 600, 1096, 300, 11, 437, 300, 775, 307, 264, 16532, 575, 586, 12187, 257, 1329, 295, 264, 2306], "temperature": 0.0, "avg_logprob": -0.1232208164258935, "compression_ratio": 1.6065573770491803, "no_speech_prob": 2.123372269124957e-06}, {"id": 366, "seek": 144424, "start": 1463.52, "end": 1466.28, "text": " that we changed and the list of the ones we deleted.", "tokens": [300, 321, 3105, 293, 264, 1329, 295, 264, 2306, 321, 22981, 13], "temperature": 0.0, "avg_logprob": -0.1232208164258935, "compression_ratio": 1.6065573770491803, "no_speech_prob": 2.123372269124957e-06}, {"id": 367, "seek": 144424, "start": 1466.28, "end": 1470.4, "text": " So we can now go ahead and run this cell.", "tokens": [407, 321, 393, 586, 352, 2286, 293, 1190, 341, 2815, 13], "temperature": 0.0, "avg_logprob": -0.1232208164258935, "compression_ratio": 1.6065573770491803, "no_speech_prob": 2.123372269124957e-06}, {"id": 368, "seek": 147040, "start": 1470.4, "end": 1474.48, "text": " And so that's going to go through a list of all of the indexes that we said to delete", "tokens": [400, 370, 300, 311, 516, 281, 352, 807, 257, 1329, 295, 439, 295, 264, 8186, 279, 300, 321, 848, 281, 12097], "temperature": 0.0, "avg_logprob": -0.13774820536124607, "compression_ratio": 1.6911764705882353, "no_speech_prob": 1.3925416624260833e-06}, {"id": 369, "seek": 147040, "start": 1474.48, "end": 1479.44, "text": " and it will delete those files and it'll go through all the ones we said to change and", "tokens": [293, 309, 486, 12097, 729, 7098, 293, 309, 603, 352, 807, 439, 264, 2306, 321, 848, 281, 1319, 293], "temperature": 0.0, "avg_logprob": -0.13774820536124607, "compression_ratio": 1.6911764705882353, "no_speech_prob": 1.3925416624260833e-06}, {"id": 370, "seek": 147040, "start": 1479.44, "end": 1481.92, "text": " it will move them to the new folder.", "tokens": [309, 486, 1286, 552, 281, 264, 777, 10820, 13], "temperature": 0.0, "avg_logprob": -0.13774820536124607, "compression_ratio": 1.6911764705882353, "no_speech_prob": 1.3925416624260833e-06}, {"id": 371, "seek": 147040, "start": 1481.92, "end": 1484.72, "text": " There we go.", "tokens": [821, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.13774820536124607, "compression_ratio": 1.6911764705882353, "no_speech_prob": 1.3925416624260833e-06}, {"id": 372, "seek": 147040, "start": 1484.72, "end": 1485.72, "text": " Done.", "tokens": [18658, 13], "temperature": 0.0, "avg_logprob": -0.13774820536124607, "compression_ratio": 1.6911764705882353, "no_speech_prob": 1.3925416624260833e-06}, {"id": 373, "seek": 147040, "start": 1485.72, "end": 1488.72, "text": " So this is like not just something for image models.", "tokens": [407, 341, 307, 411, 406, 445, 746, 337, 3256, 5245, 13], "temperature": 0.0, "avg_logprob": -0.13774820536124607, "compression_ratio": 1.6911764705882353, "no_speech_prob": 1.3925416624260833e-06}, {"id": 374, "seek": 147040, "start": 1488.72, "end": 1492.8400000000001, "text": " It's just, it's actually a really powerful technique that almost nobody knows about and", "tokens": [467, 311, 445, 11, 309, 311, 767, 257, 534, 4005, 6532, 300, 1920, 5079, 3255, 466, 293], "temperature": 0.0, "avg_logprob": -0.13774820536124607, "compression_ratio": 1.6911764705882353, "no_speech_prob": 1.3925416624260833e-06}, {"id": 375, "seek": 147040, "start": 1492.8400000000001, "end": 1500.24, "text": " uses which is before you start data cleaning, always build a model to find out what things", "tokens": [4960, 597, 307, 949, 291, 722, 1412, 8924, 11, 1009, 1322, 257, 2316, 281, 915, 484, 437, 721], "temperature": 0.0, "avg_logprob": -0.13774820536124607, "compression_ratio": 1.6911764705882353, "no_speech_prob": 1.3925416624260833e-06}, {"id": 376, "seek": 150024, "start": 1500.24, "end": 1505.16, "text": " are difficult to recognize in your data and to find the things that the model can help", "tokens": [366, 2252, 281, 5521, 294, 428, 1412, 293, 281, 915, 264, 721, 300, 264, 2316, 393, 854], "temperature": 0.0, "avg_logprob": -0.14365396896998087, "compression_ratio": 1.7836538461538463, "no_speech_prob": 6.643374035775196e-06}, {"id": 377, "seek": 150024, "start": 1505.16, "end": 1506.84, "text": " you find data problems.", "tokens": [291, 915, 1412, 2740, 13], "temperature": 0.0, "avg_logprob": -0.14365396896998087, "compression_ratio": 1.7836538461538463, "no_speech_prob": 6.643374035775196e-06}, {"id": 378, "seek": 150024, "start": 1506.84, "end": 1510.6, "text": " And then as you see them, you'll kind of say, okay, I see the kinds of problems we're having", "tokens": [400, 550, 382, 291, 536, 552, 11, 291, 603, 733, 295, 584, 11, 1392, 11, 286, 536, 264, 3685, 295, 2740, 321, 434, 1419], "temperature": 0.0, "avg_logprob": -0.14365396896998087, "compression_ratio": 1.7836538461538463, "no_speech_prob": 6.643374035775196e-06}, {"id": 379, "seek": 150024, "start": 1510.6, "end": 1515.44, "text": " and you might find better ways to gather the next data set or you might find ways to kind", "tokens": [293, 291, 1062, 915, 1101, 2098, 281, 5448, 264, 958, 1412, 992, 420, 291, 1062, 915, 2098, 281, 733], "temperature": 0.0, "avg_logprob": -0.14365396896998087, "compression_ratio": 1.7836538461538463, "no_speech_prob": 6.643374035775196e-06}, {"id": 380, "seek": 150024, "start": 1515.44, "end": 1524.1200000000001, "text": " of automate some of the cleaning and so forth.", "tokens": [295, 31605, 512, 295, 264, 8924, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.14365396896998087, "compression_ratio": 1.7836538461538463, "no_speech_prob": 6.643374035775196e-06}, {"id": 381, "seek": 150024, "start": 1524.1200000000001, "end": 1528.4, "text": " Okay so that is data cleaning.", "tokens": [1033, 370, 300, 307, 1412, 8924, 13], "temperature": 0.0, "avg_logprob": -0.14365396896998087, "compression_ratio": 1.7836538461538463, "no_speech_prob": 6.643374035775196e-06}, {"id": 382, "seek": 152840, "start": 1528.4, "end": 1533.44, "text": " And since I only have a four gigabyte GPU, it's very important for me to close and halt", "tokens": [400, 1670, 286, 787, 362, 257, 1451, 8741, 34529, 18407, 11, 309, 311, 588, 1021, 337, 385, 281, 1998, 293, 12479], "temperature": 0.0, "avg_logprob": -0.10380513889273417, "compression_ratio": 1.6150627615062763, "no_speech_prob": 6.240891707420815e-06}, {"id": 383, "seek": 152840, "start": 1533.44, "end": 1534.88, "text": " because that will free up the memory.", "tokens": [570, 300, 486, 1737, 493, 264, 4675, 13], "temperature": 0.0, "avg_logprob": -0.10380513889273417, "compression_ratio": 1.6150627615062763, "no_speech_prob": 6.240891707420815e-06}, {"id": 384, "seek": 152840, "start": 1534.88, "end": 1543.68, "text": " So it's important to know on your computer, your normal RAM doesn't really get filled", "tokens": [407, 309, 311, 1021, 281, 458, 322, 428, 3820, 11, 428, 2710, 14561, 1177, 380, 534, 483, 6412], "temperature": 0.0, "avg_logprob": -0.10380513889273417, "compression_ratio": 1.6150627615062763, "no_speech_prob": 6.240891707420815e-06}, {"id": 385, "seek": 152840, "start": 1543.68, "end": 1551.96, "text": " up because if you use up too much RAM, what will happen is that instead your computer", "tokens": [493, 570, 498, 291, 764, 493, 886, 709, 14561, 11, 437, 486, 1051, 307, 300, 2602, 428, 3820], "temperature": 0.0, "avg_logprob": -0.10380513889273417, "compression_ratio": 1.6150627615062763, "no_speech_prob": 6.240891707420815e-06}, {"id": 386, "seek": 152840, "start": 1551.96, "end": 1556.48, "text": " will start, it's called swapping, which is basically to save that RAM onto the hard disk", "tokens": [486, 722, 11, 309, 311, 1219, 1693, 10534, 11, 597, 307, 1936, 281, 3155, 300, 14561, 3911, 264, 1152, 12355], "temperature": 0.0, "avg_logprob": -0.10380513889273417, "compression_ratio": 1.6150627615062763, "no_speech_prob": 6.240891707420815e-06}, {"id": 387, "seek": 155648, "start": 1556.48, "end": 1558.52, "text": " to use it later.", "tokens": [281, 764, 309, 1780, 13], "temperature": 0.0, "avg_logprob": -0.1553754979913885, "compression_ratio": 1.6283185840707965, "no_speech_prob": 4.356857516540913e-06}, {"id": 388, "seek": 155648, "start": 1558.52, "end": 1560.08, "text": " GPUs can't swap.", "tokens": [18407, 82, 393, 380, 18135, 13], "temperature": 0.0, "avg_logprob": -0.1553754979913885, "compression_ratio": 1.6283185840707965, "no_speech_prob": 4.356857516540913e-06}, {"id": 389, "seek": 155648, "start": 1560.08, "end": 1564.24, "text": " GPUs when they run out of RAM, that's it, you're done.", "tokens": [18407, 82, 562, 436, 1190, 484, 295, 14561, 11, 300, 311, 309, 11, 291, 434, 1096, 13], "temperature": 0.0, "avg_logprob": -0.1553754979913885, "compression_ratio": 1.6283185840707965, "no_speech_prob": 4.356857516540913e-06}, {"id": 390, "seek": 155648, "start": 1564.24, "end": 1568.88, "text": " So you need to make sure that you close any notebooks that are using the GPU that you're", "tokens": [407, 291, 643, 281, 652, 988, 300, 291, 1998, 604, 43782, 300, 366, 1228, 264, 18407, 300, 291, 434], "temperature": 0.0, "avg_logprob": -0.1553754979913885, "compression_ratio": 1.6283185840707965, "no_speech_prob": 4.356857516540913e-06}, {"id": 391, "seek": 155648, "start": 1568.88, "end": 1574.0, "text": " not using and really only use it one thing at a time on the GPU.", "tokens": [406, 1228, 293, 534, 787, 764, 309, 472, 551, 412, 257, 565, 322, 264, 18407, 13], "temperature": 0.0, "avg_logprob": -0.1553754979913885, "compression_ratio": 1.6283185840707965, "no_speech_prob": 4.356857516540913e-06}, {"id": 392, "seek": 155648, "start": 1574.0, "end": 1576.24, "text": " Otherwise, you'll almost certainly run out of memory.", "tokens": [10328, 11, 291, 603, 1920, 3297, 1190, 484, 295, 4675, 13], "temperature": 0.0, "avg_logprob": -0.1553754979913885, "compression_ratio": 1.6283185840707965, "no_speech_prob": 4.356857516540913e-06}, {"id": 393, "seek": 155648, "start": 1576.24, "end": 1579.72, "text": " So we've got the first few reds starting to appear.", "tokens": [407, 321, 600, 658, 264, 700, 1326, 2182, 82, 2891, 281, 4204, 13], "temperature": 0.0, "avg_logprob": -0.1553754979913885, "compression_ratio": 1.6283185840707965, "no_speech_prob": 4.356857516540913e-06}, {"id": 394, "seek": 155648, "start": 1579.72, "end": 1582.48, "text": " So remember to ask.", "tokens": [407, 1604, 281, 1029, 13], "temperature": 0.0, "avg_logprob": -0.1553754979913885, "compression_ratio": 1.6283185840707965, "no_speech_prob": 4.356857516540913e-06}, {"id": 395, "seek": 158248, "start": 1582.48, "end": 1588.32, "text": " And in terms of the yellows, it's important to know as you watch the video, I'm not asking", "tokens": [400, 294, 2115, 295, 264, 5566, 82, 11, 309, 311, 1021, 281, 458, 382, 291, 1159, 264, 960, 11, 286, 478, 406, 3365], "temperature": 0.0, "avg_logprob": -0.09918582750403362, "compression_ratio": 1.736, "no_speech_prob": 1.1659400115604512e-05}, {"id": 396, "seek": 158248, "start": 1588.32, "end": 1590.3600000000001, "text": " you to run all this code.", "tokens": [291, 281, 1190, 439, 341, 3089, 13], "temperature": 0.0, "avg_logprob": -0.09918582750403362, "compression_ratio": 1.736, "no_speech_prob": 1.1659400115604512e-05}, {"id": 397, "seek": 158248, "start": 1590.3600000000001, "end": 1595.88, "text": " Okay, the idea is to kind of watch it and then go back and pause, you know, as you go", "tokens": [1033, 11, 264, 1558, 307, 281, 733, 295, 1159, 309, 293, 550, 352, 646, 293, 10465, 11, 291, 458, 11, 382, 291, 352], "temperature": 0.0, "avg_logprob": -0.09918582750403362, "compression_ratio": 1.736, "no_speech_prob": 1.1659400115604512e-05}, {"id": 398, "seek": 158248, "start": 1595.88, "end": 1600.92, "text": " along or you can just stop, try, stop, try.", "tokens": [2051, 420, 291, 393, 445, 1590, 11, 853, 11, 1590, 11, 853, 13], "temperature": 0.0, "avg_logprob": -0.09918582750403362, "compression_ratio": 1.736, "no_speech_prob": 1.1659400115604512e-05}, {"id": 399, "seek": 158248, "start": 1600.92, "end": 1604.84, "text": " The approach I really like and a lot of students really like for watching these videos is to", "tokens": [440, 3109, 286, 534, 411, 293, 257, 688, 295, 1731, 534, 411, 337, 1976, 613, 2145, 307, 281], "temperature": 0.0, "avg_logprob": -0.09918582750403362, "compression_ratio": 1.736, "no_speech_prob": 1.1659400115604512e-05}, {"id": 400, "seek": 158248, "start": 1604.84, "end": 1612.1200000000001, "text": " actually watch the entire thing without touching the keyboard to get a sense of what the video", "tokens": [767, 1159, 264, 2302, 551, 1553, 11175, 264, 10186, 281, 483, 257, 2020, 295, 437, 264, 960], "temperature": 0.0, "avg_logprob": -0.09918582750403362, "compression_ratio": 1.736, "no_speech_prob": 1.1659400115604512e-05}, {"id": 401, "seek": 161212, "start": 1612.12, "end": 1618.4799999999998, "text": " is about and then go back to the start and watch it again and follow along.", "tokens": [307, 466, 293, 550, 352, 646, 281, 264, 722, 293, 1159, 309, 797, 293, 1524, 2051, 13], "temperature": 0.0, "avg_logprob": -0.141618384971275, "compression_ratio": 1.6680672268907564, "no_speech_prob": 1.922269439091906e-05}, {"id": 402, "seek": 161212, "start": 1618.4799999999998, "end": 1621.6799999999998, "text": " That way at every point, you know what it is you're doing, you know what's going to", "tokens": [663, 636, 412, 633, 935, 11, 291, 458, 437, 309, 307, 291, 434, 884, 11, 291, 458, 437, 311, 516, 281], "temperature": 0.0, "avg_logprob": -0.141618384971275, "compression_ratio": 1.6680672268907564, "no_speech_prob": 1.922269439091906e-05}, {"id": 403, "seek": 161212, "start": 1621.6799999999998, "end": 1622.6799999999998, "text": " happen next.", "tokens": [1051, 958, 13], "temperature": 0.0, "avg_logprob": -0.141618384971275, "compression_ratio": 1.6680672268907564, "no_speech_prob": 1.922269439091906e-05}, {"id": 404, "seek": 161212, "start": 1622.6799999999998, "end": 1624.12, "text": " That can actually save you some time.", "tokens": [663, 393, 767, 3155, 291, 512, 565, 13], "temperature": 0.0, "avg_logprob": -0.141618384971275, "compression_ratio": 1.6680672268907564, "no_speech_prob": 1.922269439091906e-05}, {"id": 405, "seek": 161212, "start": 1624.12, "end": 1630.9199999999998, "text": " It's a bit of an unusual way because obviously like real life lectures, you can't do that.", "tokens": [467, 311, 257, 857, 295, 364, 10901, 636, 570, 2745, 411, 957, 993, 16564, 11, 291, 393, 380, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.141618384971275, "compression_ratio": 1.6680672268907564, "no_speech_prob": 1.922269439091906e-05}, {"id": 406, "seek": 161212, "start": 1630.9199999999998, "end": 1636.04, "text": " You can't rewind the professor and get them to say it again, but it's a good way to do", "tokens": [509, 393, 380, 41458, 264, 8304, 293, 483, 552, 281, 584, 309, 797, 11, 457, 309, 311, 257, 665, 636, 281, 360], "temperature": 0.0, "avg_logprob": -0.141618384971275, "compression_ratio": 1.6680672268907564, "no_speech_prob": 1.922269439091906e-05}, {"id": 407, "seek": 161212, "start": 1636.04, "end": 1638.8799999999999, "text": " it here.", "tokens": [309, 510, 13], "temperature": 0.0, "avg_logprob": -0.141618384971275, "compression_ratio": 1.6680672268907564, "no_speech_prob": 1.922269439091906e-05}, {"id": 408, "seek": 163888, "start": 1638.88, "end": 1642.16, "text": " So now that we've cleaned our data, how are we going to put it into production?", "tokens": [407, 586, 300, 321, 600, 16146, 527, 1412, 11, 577, 366, 321, 516, 281, 829, 309, 666, 4265, 30], "temperature": 0.0, "avg_logprob": -0.1651485300509729, "compression_ratio": 1.7183673469387755, "no_speech_prob": 1.0783021025417838e-05}, {"id": 409, "seek": 163888, "start": 1642.16, "end": 1649.4, "text": " Well, in the book we use something called voila and it's, it's pretty good.", "tokens": [1042, 11, 294, 264, 1446, 321, 764, 746, 1219, 45565, 293, 309, 311, 11, 309, 311, 1238, 665, 13], "temperature": 0.0, "avg_logprob": -0.1651485300509729, "compression_ratio": 1.7183673469387755, "no_speech_prob": 1.0783021025417838e-05}, {"id": 410, "seek": 163888, "start": 1649.4, "end": 1653.0600000000002, "text": " But there's actually something that I think most of you are probably going to find a lot", "tokens": [583, 456, 311, 767, 746, 300, 286, 519, 881, 295, 291, 366, 1391, 516, 281, 915, 257, 688], "temperature": 0.0, "avg_logprob": -0.1651485300509729, "compression_ratio": 1.7183673469387755, "no_speech_prob": 1.0783021025417838e-05}, {"id": 411, "seek": 163888, "start": 1653.0600000000002, "end": 1658.2800000000002, "text": " more useful nowadays, which is something called hugging face spaces.", "tokens": [544, 4420, 13434, 11, 597, 307, 746, 1219, 41706, 1851, 7673, 13], "temperature": 0.0, "avg_logprob": -0.1651485300509729, "compression_ratio": 1.7183673469387755, "no_speech_prob": 1.0783021025417838e-05}, {"id": 412, "seek": 163888, "start": 1658.2800000000002, "end": 1660.2, "text": " And there's a couple of things you can use with that.", "tokens": [400, 456, 311, 257, 1916, 295, 721, 291, 393, 764, 365, 300, 13], "temperature": 0.0, "avg_logprob": -0.1651485300509729, "compression_ratio": 1.7183673469387755, "no_speech_prob": 1.0783021025417838e-05}, {"id": 413, "seek": 163888, "start": 1660.2, "end": 1663.1200000000001, "text": " We're going to look at something called Gradio today.", "tokens": [492, 434, 516, 281, 574, 412, 746, 1219, 16710, 1004, 965, 13], "temperature": 0.0, "avg_logprob": -0.1651485300509729, "compression_ratio": 1.7183673469387755, "no_speech_prob": 1.0783021025417838e-05}, {"id": 414, "seek": 166312, "start": 1663.12, "end": 1669.7199999999998, "text": " And there isn't a chapter about this in the book, but that doesn't matter because Tanishq", "tokens": [400, 456, 1943, 380, 257, 7187, 466, 341, 294, 264, 1446, 11, 457, 300, 1177, 380, 1871, 570, 314, 7524, 80], "temperature": 0.0, "avg_logprob": -0.10986421505610149, "compression_ratio": 1.6803652968036529, "no_speech_prob": 4.222748430038337e-06}, {"id": 415, "seek": 166312, "start": 1669.7199999999998, "end": 1675.04, "text": " Abraham, who's actually one of the TAs in the course, has written a fantastic blog post", "tokens": [17782, 11, 567, 311, 767, 472, 295, 264, 314, 10884, 294, 264, 1164, 11, 575, 3720, 257, 5456, 6968, 2183], "temperature": 0.0, "avg_logprob": -0.10986421505610149, "compression_ratio": 1.6803652968036529, "no_speech_prob": 4.222748430038337e-06}, {"id": 416, "seek": 166312, "start": 1675.04, "end": 1678.8799999999999, "text": " about really everything we're going to cover today.", "tokens": [466, 534, 1203, 321, 434, 516, 281, 2060, 965, 13], "temperature": 0.0, "avg_logprob": -0.10986421505610149, "compression_ratio": 1.6803652968036529, "no_speech_prob": 4.222748430038337e-06}, {"id": 417, "seek": 166312, "start": 1678.8799999999999, "end": 1683.9199999999998, "text": " So there's a link to that from the forum and from the course page.", "tokens": [407, 456, 311, 257, 2113, 281, 300, 490, 264, 17542, 293, 490, 264, 1164, 3028, 13], "temperature": 0.0, "avg_logprob": -0.10986421505610149, "compression_ratio": 1.6803652968036529, "no_speech_prob": 4.222748430038337e-06}, {"id": 418, "seek": 166312, "start": 1683.9199999999998, "end": 1687.9199999999998, "text": " So this is like the equivalent of the chapter of the book, if you like.", "tokens": [407, 341, 307, 411, 264, 10344, 295, 264, 7187, 295, 264, 1446, 11, 498, 291, 411, 13], "temperature": 0.0, "avg_logprob": -0.10986421505610149, "compression_ratio": 1.6803652968036529, "no_speech_prob": 4.222748430038337e-06}, {"id": 419, "seek": 168792, "start": 1687.92, "end": 1693.24, "text": " And I would be remiss if I didn't stop for a moment and call out Tanishq in a big way", "tokens": [400, 286, 576, 312, 890, 891, 498, 286, 994, 380, 1590, 337, 257, 1623, 293, 818, 484, 314, 7524, 80, 294, 257, 955, 636], "temperature": 0.0, "avg_logprob": -0.0956799924866227, "compression_ratio": 1.604982206405694, "no_speech_prob": 5.337948550732108e-06}, {"id": 420, "seek": 168792, "start": 1693.24, "end": 1694.24, "text": " for two reasons.", "tokens": [337, 732, 4112, 13], "temperature": 0.0, "avg_logprob": -0.0956799924866227, "compression_ratio": 1.604982206405694, "no_speech_prob": 5.337948550732108e-06}, {"id": 421, "seek": 168792, "start": 1694.24, "end": 1699.52, "text": " The first is he is one of the most helpful people in the fast AI community.", "tokens": [440, 700, 307, 415, 307, 472, 295, 264, 881, 4961, 561, 294, 264, 2370, 7318, 1768, 13], "temperature": 0.0, "avg_logprob": -0.0956799924866227, "compression_ratio": 1.604982206405694, "no_speech_prob": 5.337948550732108e-06}, {"id": 422, "seek": 168792, "start": 1699.52, "end": 1705.1000000000001, "text": " He's been around quite a few years, incredibly tenacious, thoughtful and patient.", "tokens": [634, 311, 668, 926, 1596, 257, 1326, 924, 11, 6252, 2064, 22641, 11, 21566, 293, 4537, 13], "temperature": 0.0, "avg_logprob": -0.0956799924866227, "compression_ratio": 1.604982206405694, "no_speech_prob": 5.337948550732108e-06}, {"id": 423, "seek": 168792, "start": 1705.1000000000001, "end": 1710.92, "text": " And also because I have this fantastic picture of him a few years ago with Conan when he", "tokens": [400, 611, 570, 286, 362, 341, 5456, 3036, 295, 796, 257, 1326, 924, 2057, 365, 47691, 562, 415], "temperature": 0.0, "avg_logprob": -0.0956799924866227, "compression_ratio": 1.604982206405694, "no_speech_prob": 5.337948550732108e-06}, {"id": 424, "seek": 168792, "start": 1710.92, "end": 1712.72, "text": " was a famous child prodigy.", "tokens": [390, 257, 4618, 1440, 15792, 328, 88, 13], "temperature": 0.0, "avg_logprob": -0.0956799924866227, "compression_ratio": 1.604982206405694, "no_speech_prob": 5.337948550732108e-06}, {"id": 425, "seek": 168792, "start": 1712.72, "end": 1715.8000000000002, "text": " So now you know what happens to famous child prodigies when they grow up.", "tokens": [407, 586, 291, 458, 437, 2314, 281, 4618, 1440, 15792, 328, 530, 562, 436, 1852, 493, 13], "temperature": 0.0, "avg_logprob": -0.0956799924866227, "compression_ratio": 1.604982206405694, "no_speech_prob": 5.337948550732108e-06}, {"id": 426, "seek": 171580, "start": 1715.8, "end": 1721.56, "text": " They became even more famous fast AI community members and declining experts.", "tokens": [814, 3062, 754, 544, 4618, 2370, 7318, 1768, 2679, 293, 34298, 8572, 13], "temperature": 0.0, "avg_logprob": -0.21057288815276792, "compression_ratio": 1.5372549019607844, "no_speech_prob": 1.184233133244561e-05}, {"id": 427, "seek": 171580, "start": 1721.56, "end": 1725.8, "text": " So you should definitely check out this video of him telling jokes to Conan.", "tokens": [407, 291, 820, 2138, 1520, 484, 341, 960, 295, 796, 3585, 14439, 281, 47691, 13], "temperature": 0.0, "avg_logprob": -0.21057288815276792, "compression_ratio": 1.5372549019607844, "no_speech_prob": 1.184233133244561e-05}, {"id": 428, "seek": 171580, "start": 1725.8, "end": 1727.6, "text": " I think he's still only 18 actually.", "tokens": [286, 519, 415, 311, 920, 787, 2443, 767, 13], "temperature": 0.0, "avg_logprob": -0.21057288815276792, "compression_ratio": 1.5372549019607844, "no_speech_prob": 1.184233133244561e-05}, {"id": 429, "seek": 171580, "start": 1727.6, "end": 1730.3, "text": " This is probably not that many years ago.", "tokens": [639, 307, 1391, 406, 300, 867, 924, 2057, 13], "temperature": 0.0, "avg_logprob": -0.21057288815276792, "compression_ratio": 1.5372549019607844, "no_speech_prob": 1.184233133244561e-05}, {"id": 430, "seek": 171580, "start": 1730.3, "end": 1734.24, "text": " So thank you very much Tanishq for all your help in the community.", "tokens": [407, 1309, 291, 588, 709, 314, 7524, 80, 337, 439, 428, 854, 294, 264, 1768, 13], "temperature": 0.0, "avg_logprob": -0.21057288815276792, "compression_ratio": 1.5372549019607844, "no_speech_prob": 1.184233133244561e-05}, {"id": 431, "seek": 171580, "start": 1734.24, "end": 1737.9199999999998, "text": " And sorry for embarrassing you with that picture of you as a nine year old.", "tokens": [400, 2597, 337, 17299, 291, 365, 300, 3036, 295, 291, 382, 257, 4949, 1064, 1331, 13], "temperature": 0.0, "avg_logprob": -0.21057288815276792, "compression_ratio": 1.5372549019607844, "no_speech_prob": 1.184233133244561e-05}, {"id": 432, "seek": 171580, "start": 1737.9199999999998, "end": 1739.72, "text": " I'm not really.", "tokens": [286, 478, 406, 534, 13], "temperature": 0.0, "avg_logprob": -0.21057288815276792, "compression_ratio": 1.5372549019607844, "no_speech_prob": 1.184233133244561e-05}, {"id": 433, "seek": 173972, "start": 1739.72, "end": 1746.52, "text": " Okay now the thing is for doing Gradio and Hugging Face Spaces, well it's easy enough", "tokens": [1033, 586, 264, 551, 307, 337, 884, 16710, 1004, 293, 46892, 3249, 4047, 1738, 2116, 11, 731, 309, 311, 1858, 1547], "temperature": 0.0, "avg_logprob": -0.14013960754987106, "compression_ratio": 1.746606334841629, "no_speech_prob": 9.817040336201899e-06}, {"id": 434, "seek": 173972, "start": 1746.52, "end": 1747.52, "text": " to start.", "tokens": [281, 722, 13], "temperature": 0.0, "avg_logprob": -0.14013960754987106, "compression_ratio": 1.746606334841629, "no_speech_prob": 9.817040336201899e-06}, {"id": 435, "seek": 173972, "start": 1747.52, "end": 1751.68, "text": " Okay we start over here on the Hugging Face Spaces page which we've linked to from the", "tokens": [1033, 321, 722, 670, 510, 322, 264, 46892, 3249, 4047, 1738, 2116, 3028, 597, 321, 600, 9408, 281, 490, 264], "temperature": 0.0, "avg_logprob": -0.14013960754987106, "compression_ratio": 1.746606334841629, "no_speech_prob": 9.817040336201899e-06}, {"id": 436, "seek": 173972, "start": 1751.68, "end": 1754.64, "text": " forum and the course.", "tokens": [17542, 293, 264, 1164, 13], "temperature": 0.0, "avg_logprob": -0.14013960754987106, "compression_ratio": 1.746606334841629, "no_speech_prob": 9.817040336201899e-06}, {"id": 437, "seek": 173972, "start": 1754.64, "end": 1759.68, "text": " And we're going to put a model in production where we're going to take the model we trained", "tokens": [400, 321, 434, 516, 281, 829, 257, 2316, 294, 4265, 689, 321, 434, 516, 281, 747, 264, 2316, 321, 8895], "temperature": 0.0, "avg_logprob": -0.14013960754987106, "compression_ratio": 1.746606334841629, "no_speech_prob": 9.817040336201899e-06}, {"id": 438, "seek": 173972, "start": 1759.68, "end": 1765.48, "text": " and we are going to basically copy it to this Hugging Face Spaces server and write a user", "tokens": [293, 321, 366, 516, 281, 1936, 5055, 309, 281, 341, 46892, 3249, 4047, 1738, 2116, 7154, 293, 2464, 257, 4195], "temperature": 0.0, "avg_logprob": -0.14013960754987106, "compression_ratio": 1.746606334841629, "no_speech_prob": 9.817040336201899e-06}, {"id": 439, "seek": 176548, "start": 1765.48, "end": 1770.16, "text": " interface for it.", "tokens": [9226, 337, 309, 13], "temperature": 0.0, "avg_logprob": -0.25822774479898175, "compression_ratio": 1.5252525252525253, "no_speech_prob": 2.355127980990801e-05}, {"id": 440, "seek": 176548, "start": 1770.16, "end": 1772.3600000000001, "text": " So let's go.", "tokens": [407, 718, 311, 352, 13], "temperature": 0.0, "avg_logprob": -0.25822774479898175, "compression_ratio": 1.5252525252525253, "no_speech_prob": 2.355127980990801e-05}, {"id": 441, "seek": 176548, "start": 1772.3600000000001, "end": 1774.24, "text": " Create new space.", "tokens": [20248, 777, 1901, 13], "temperature": 0.0, "avg_logprob": -0.25822774479898175, "compression_ratio": 1.5252525252525253, "no_speech_prob": 2.355127980990801e-05}, {"id": 442, "seek": 176548, "start": 1774.24, "end": 1778.28, "text": " Okay so you can just go ahead and say alright so obviously you sign up.", "tokens": [1033, 370, 291, 393, 445, 352, 2286, 293, 584, 5845, 370, 2745, 291, 1465, 493, 13], "temperature": 0.0, "avg_logprob": -0.25822774479898175, "compression_ratio": 1.5252525252525253, "no_speech_prob": 2.355127980990801e-05}, {"id": 443, "seek": 176548, "start": 1778.28, "end": 1779.28, "text": " The whole thing is free.", "tokens": [440, 1379, 551, 307, 1737, 13], "temperature": 0.0, "avg_logprob": -0.25822774479898175, "compression_ratio": 1.5252525252525253, "no_speech_prob": 2.355127980990801e-05}, {"id": 444, "seek": 176548, "start": 1779.28, "end": 1785.0, "text": " Basically everything I'm showing you in this entire course you can do for free.", "tokens": [8537, 1203, 286, 478, 4099, 291, 294, 341, 2302, 1164, 291, 393, 360, 337, 1737, 13], "temperature": 0.0, "avg_logprob": -0.25822774479898175, "compression_ratio": 1.5252525252525253, "no_speech_prob": 2.355127980990801e-05}, {"id": 445, "seek": 176548, "start": 1785.0, "end": 1786.0, "text": " That's the good news.", "tokens": [663, 311, 264, 665, 2583, 13], "temperature": 0.0, "avg_logprob": -0.25822774479898175, "compression_ratio": 1.5252525252525253, "no_speech_prob": 2.355127980990801e-05}, {"id": 446, "seek": 176548, "start": 1786.0, "end": 1788.88, "text": " Okay so give it a name.", "tokens": [1033, 370, 976, 309, 257, 1315, 13], "temperature": 0.0, "avg_logprob": -0.25822774479898175, "compression_ratio": 1.5252525252525253, "no_speech_prob": 2.355127980990801e-05}, {"id": 447, "seek": 176548, "start": 1788.88, "end": 1791.88, "text": " Just create something minimal.", "tokens": [1449, 1884, 746, 13206, 13], "temperature": 0.0, "avg_logprob": -0.25822774479898175, "compression_ratio": 1.5252525252525253, "no_speech_prob": 2.355127980990801e-05}, {"id": 448, "seek": 179188, "start": 1791.88, "end": 1795.5200000000002, "text": " I always use the Apache license because it means other people can use your work really", "tokens": [286, 1009, 764, 264, 46597, 10476, 570, 309, 1355, 661, 561, 393, 764, 428, 589, 534], "temperature": 0.0, "avg_logprob": -0.17068170732067478, "compression_ratio": 1.6289752650176679, "no_speech_prob": 4.222787993057864e-06}, {"id": 449, "seek": 179188, "start": 1795.5200000000002, "end": 1797.5200000000002, "text": " easily, but you don't have to worry too much about patents.", "tokens": [3612, 11, 457, 291, 500, 380, 362, 281, 3292, 886, 709, 466, 38142, 13], "temperature": 0.0, "avg_logprob": -0.17068170732067478, "compression_ratio": 1.6289752650176679, "no_speech_prob": 4.222787993057864e-06}, {"id": 450, "seek": 179188, "start": 1797.5200000000002, "end": 1800.92, "text": " As I say there's a few different products you can use with it.", "tokens": [1018, 286, 584, 456, 311, 257, 1326, 819, 3383, 291, 393, 764, 365, 309, 13], "temperature": 0.0, "avg_logprob": -0.17068170732067478, "compression_ratio": 1.6289752650176679, "no_speech_prob": 4.222787993057864e-06}, {"id": 451, "seek": 179188, "start": 1800.92, "end": 1804.8400000000001, "text": " We're going to use Gradio also free.", "tokens": [492, 434, 516, 281, 764, 16710, 1004, 611, 1737, 13], "temperature": 0.0, "avg_logprob": -0.17068170732067478, "compression_ratio": 1.6289752650176679, "no_speech_prob": 4.222787993057864e-06}, {"id": 452, "seek": 179188, "start": 1804.8400000000001, "end": 1808.1200000000001, "text": " If you make it public then you can share it which is always a good idea when you're a", "tokens": [759, 291, 652, 309, 1908, 550, 291, 393, 2073, 309, 597, 307, 1009, 257, 665, 1558, 562, 291, 434, 257], "temperature": 0.0, "avg_logprob": -0.17068170732067478, "compression_ratio": 1.6289752650176679, "no_speech_prob": 4.222787993057864e-06}, {"id": 453, "seek": 179188, "start": 1808.1200000000001, "end": 1812.5600000000002, "text": " student particularly to really be building up that portfolio.", "tokens": [3107, 4098, 281, 534, 312, 2390, 493, 300, 12583, 13], "temperature": 0.0, "avg_logprob": -0.17068170732067478, "compression_ratio": 1.6289752650176679, "no_speech_prob": 4.222787993057864e-06}, {"id": 454, "seek": 179188, "start": 1812.5600000000002, "end": 1813.96, "text": " Okay so we're done.", "tokens": [1033, 370, 321, 434, 1096, 13], "temperature": 0.0, "avg_logprob": -0.17068170732067478, "compression_ratio": 1.6289752650176679, "no_speech_prob": 4.222787993057864e-06}, {"id": 455, "seek": 179188, "start": 1813.96, "end": 1818.0, "text": " We've created a space.", "tokens": [492, 600, 2942, 257, 1901, 13], "temperature": 0.0, "avg_logprob": -0.17068170732067478, "compression_ratio": 1.6289752650176679, "no_speech_prob": 4.222787993057864e-06}, {"id": 456, "seek": 179188, "start": 1818.0, "end": 1819.72, "text": " Now what do we do next?", "tokens": [823, 437, 360, 321, 360, 958, 30], "temperature": 0.0, "avg_logprob": -0.17068170732067478, "compression_ratio": 1.6289752650176679, "no_speech_prob": 4.222787993057864e-06}, {"id": 457, "seek": 181972, "start": 1819.72, "end": 1823.04, "text": " Well Spaces works through Git.", "tokens": [1042, 1738, 2116, 1985, 807, 16939, 13], "temperature": 0.0, "avg_logprob": -0.16135168623650212, "compression_ratio": 1.5669642857142858, "no_speech_prob": 1.1478498890937772e-05}, {"id": 458, "seek": 181972, "start": 1823.04, "end": 1827.72, "text": " Now most software developers will be very familiar with Git.", "tokens": [823, 881, 4722, 8849, 486, 312, 588, 4963, 365, 16939, 13], "temperature": 0.0, "avg_logprob": -0.16135168623650212, "compression_ratio": 1.5669642857142858, "no_speech_prob": 1.1478498890937772e-05}, {"id": 459, "seek": 181972, "start": 1827.72, "end": 1832.28, "text": " Some data scientists might not be and so Git's a very very useful tool.", "tokens": [2188, 1412, 7708, 1062, 406, 312, 293, 370, 16939, 311, 257, 588, 588, 4420, 2290, 13], "temperature": 0.0, "avg_logprob": -0.16135168623650212, "compression_ratio": 1.5669642857142858, "no_speech_prob": 1.1478498890937772e-05}, {"id": 460, "seek": 181972, "start": 1832.28, "end": 1839.64, "text": " I'm not going to talk about it in detail but let's kind of quickly learn about how to use", "tokens": [286, 478, 406, 516, 281, 751, 466, 309, 294, 2607, 457, 718, 311, 733, 295, 2661, 1466, 466, 577, 281, 764], "temperature": 0.0, "avg_logprob": -0.16135168623650212, "compression_ratio": 1.5669642857142858, "no_speech_prob": 1.1478498890937772e-05}, {"id": 461, "seek": 181972, "start": 1839.64, "end": 1840.64, "text": " it.", "tokens": [309, 13], "temperature": 0.0, "avg_logprob": -0.16135168623650212, "compression_ratio": 1.5669642857142858, "no_speech_prob": 1.1478498890937772e-05}, {"id": 462, "seek": 181972, "start": 1840.64, "end": 1847.84, "text": " Right now Git you can use it through something called GitHub desktop which is actually pretty", "tokens": [1779, 586, 16939, 291, 393, 764, 309, 807, 746, 1219, 23331, 14502, 597, 307, 767, 1238], "temperature": 0.0, "avg_logprob": -0.16135168623650212, "compression_ratio": 1.5669642857142858, "no_speech_prob": 1.1478498890937772e-05}, {"id": 463, "seek": 184784, "start": 1847.84, "end": 1854.08, "text": " great and even people who use Git through the console should probably be considering", "tokens": [869, 293, 754, 561, 567, 764, 16939, 807, 264, 11076, 820, 1391, 312, 8079], "temperature": 0.0, "avg_logprob": -0.1727911631266276, "compression_ratio": 1.7285223367697595, "no_speech_prob": 1.6700892956578173e-05}, {"id": 464, "seek": 184784, "start": 1854.08, "end": 1858.84, "text": " using GitHub desktop as well because some things just much faster and easier in it.", "tokens": [1228, 23331, 14502, 382, 731, 570, 512, 721, 445, 709, 4663, 293, 3571, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.1727911631266276, "compression_ratio": 1.7285223367697595, "no_speech_prob": 1.6700892956578173e-05}, {"id": 465, "seek": 184784, "start": 1858.84, "end": 1863.8799999999999, "text": " In fact I was talking to my friend Hamill today and I was like oh help I've accidentally committed", "tokens": [682, 1186, 286, 390, 1417, 281, 452, 1277, 8234, 373, 965, 293, 286, 390, 411, 1954, 854, 286, 600, 15715, 7784], "temperature": 0.0, "avg_logprob": -0.1727911631266276, "compression_ratio": 1.7285223367697595, "no_speech_prob": 1.6700892956578173e-05}, {"id": 466, "seek": 184784, "start": 1863.8799999999999, "end": 1865.9599999999998, "text": " this two things by mistake.", "tokens": [341, 732, 721, 538, 6146, 13], "temperature": 0.0, "avg_logprob": -0.1727911631266276, "compression_ratio": 1.7285223367697595, "no_speech_prob": 1.6700892956578173e-05}, {"id": 467, "seek": 184784, "start": 1865.9599999999998, "end": 1867.76, "text": " What's the easiest way to revert it?", "tokens": [708, 311, 264, 12889, 636, 281, 319, 3281, 309, 30], "temperature": 0.0, "avg_logprob": -0.1727911631266276, "compression_ratio": 1.7285223367697595, "no_speech_prob": 1.6700892956578173e-05}, {"id": 468, "seek": 184784, "start": 1867.76, "end": 1872.08, "text": " And he used to work at GitHub and I thought he was going to have some fancy console command", "tokens": [400, 415, 1143, 281, 589, 412, 23331, 293, 286, 1194, 415, 390, 516, 281, 362, 512, 10247, 11076, 5622], "temperature": 0.0, "avg_logprob": -0.1727911631266276, "compression_ratio": 1.7285223367697595, "no_speech_prob": 1.6700892956578173e-05}, {"id": 469, "seek": 184784, "start": 1872.08, "end": 1875.76, "text": " and he was like oh you should use GitHub desktop and you can just click on it.", "tokens": [293, 415, 390, 411, 1954, 291, 820, 764, 23331, 14502, 293, 291, 393, 445, 2052, 322, 309, 13], "temperature": 0.0, "avg_logprob": -0.1727911631266276, "compression_ratio": 1.7285223367697595, "no_speech_prob": 1.6700892956578173e-05}, {"id": 470, "seek": 187576, "start": 1875.76, "end": 1878.16, "text": " Oh that's a great idea.", "tokens": [876, 300, 311, 257, 869, 1558, 13], "temperature": 0.0, "avg_logprob": -0.1573862632115682, "compression_ratio": 1.7251184834123223, "no_speech_prob": 9.516095815342851e-06}, {"id": 471, "seek": 187576, "start": 1878.16, "end": 1879.16, "text": " So that's useful.", "tokens": [407, 300, 311, 4420, 13], "temperature": 0.0, "avg_logprob": -0.1573862632115682, "compression_ratio": 1.7251184834123223, "no_speech_prob": 9.516095815342851e-06}, {"id": 472, "seek": 187576, "start": 1879.16, "end": 1885.16, "text": " But most of the time we do use Git from the console from the terminal.", "tokens": [583, 881, 295, 264, 565, 321, 360, 764, 16939, 490, 264, 11076, 490, 264, 14709, 13], "temperature": 0.0, "avg_logprob": -0.1573862632115682, "compression_ratio": 1.7251184834123223, "no_speech_prob": 9.516095815342851e-06}, {"id": 473, "seek": 187576, "start": 1885.16, "end": 1889.4, "text": " If you're a Linux user or a Mac user you've already got a terminal very straightforward", "tokens": [759, 291, 434, 257, 18734, 4195, 420, 257, 5707, 4195, 291, 600, 1217, 658, 257, 14709, 588, 15325], "temperature": 0.0, "avg_logprob": -0.1573862632115682, "compression_ratio": 1.7251184834123223, "no_speech_prob": 9.516095815342851e-06}, {"id": 474, "seek": 187576, "start": 1889.4, "end": 1890.64, "text": " no worries.", "tokens": [572, 16340, 13], "temperature": 0.0, "avg_logprob": -0.1573862632115682, "compression_ratio": 1.7251184834123223, "no_speech_prob": 9.516095815342851e-06}, {"id": 475, "seek": 187576, "start": 1890.64, "end": 1895.96, "text": " If you're a Windows user I've got good news nowadays Windows has a terrific terminal.", "tokens": [759, 291, 434, 257, 8591, 4195, 286, 600, 658, 665, 2583, 13434, 8591, 575, 257, 20899, 14709, 13], "temperature": 0.0, "avg_logprob": -0.1573862632115682, "compression_ratio": 1.7251184834123223, "no_speech_prob": 9.516095815342851e-06}, {"id": 476, "seek": 187576, "start": 1895.96, "end": 1900.04, "text": " It's called Windows terminal you get it from the Microsoft store.", "tokens": [467, 311, 1219, 8591, 14709, 291, 483, 309, 490, 264, 8116, 3531, 13], "temperature": 0.0, "avg_logprob": -0.1573862632115682, "compression_ratio": 1.7251184834123223, "no_speech_prob": 9.516095815342851e-06}, {"id": 477, "seek": 190004, "start": 1900.04, "end": 1907.36, "text": " So in fact every time you see me using a terminal I'm actually using that Windows terminal.", "tokens": [407, 294, 1186, 633, 565, 291, 536, 385, 1228, 257, 14709, 286, 478, 767, 1228, 300, 8591, 14709, 13], "temperature": 0.0, "avg_logprob": -0.1622295379638672, "compression_ratio": 1.5793991416309012, "no_speech_prob": 2.4439771095785545e-06}, {"id": 478, "seek": 190004, "start": 1907.36, "end": 1908.36, "text": " Works very well.", "tokens": [27914, 588, 731, 13], "temperature": 0.0, "avg_logprob": -0.1622295379638672, "compression_ratio": 1.5793991416309012, "no_speech_prob": 2.4439771095785545e-06}, {"id": 479, "seek": 190004, "start": 1908.36, "end": 1914.36, "text": " God knows why I'd want it to have all these ridiculous colors but there you go.", "tokens": [1265, 3255, 983, 286, 1116, 528, 309, 281, 362, 439, 613, 11083, 4577, 457, 456, 291, 352, 13], "temperature": 0.0, "avg_logprob": -0.1622295379638672, "compression_ratio": 1.5793991416309012, "no_speech_prob": 2.4439771095785545e-06}, {"id": 480, "seek": 190004, "start": 1914.36, "end": 1917.68, "text": " Now what do you want to be running inside your terminal?", "tokens": [823, 437, 360, 291, 528, 281, 312, 2614, 1854, 428, 14709, 30], "temperature": 0.0, "avg_logprob": -0.1622295379638672, "compression_ratio": 1.5793991416309012, "no_speech_prob": 2.4439771095785545e-06}, {"id": 481, "seek": 190004, "start": 1917.68, "end": 1921.96, "text": " Obviously if you're in Linux or Mac you've already got a shell set up.", "tokens": [7580, 498, 291, 434, 294, 18734, 420, 5707, 291, 600, 1217, 658, 257, 8720, 992, 493, 13], "temperature": 0.0, "avg_logprob": -0.1622295379638672, "compression_ratio": 1.5793991416309012, "no_speech_prob": 2.4439771095785545e-06}, {"id": 482, "seek": 190004, "start": 1921.96, "end": 1926.32, "text": " In Windows you almost certainly want to use Ubuntu.", "tokens": [682, 8591, 291, 1920, 3297, 528, 281, 764, 30230, 45605, 13], "temperature": 0.0, "avg_logprob": -0.1622295379638672, "compression_ratio": 1.5793991416309012, "no_speech_prob": 2.4439771095785545e-06}, {"id": 483, "seek": 192632, "start": 1926.32, "end": 1932.6399999999999, "text": " So Windows believe it or not can actually run a full Linux environment and to do it", "tokens": [407, 8591, 1697, 309, 420, 406, 393, 767, 1190, 257, 1577, 18734, 2823, 293, 281, 360, 309], "temperature": 0.0, "avg_logprob": -0.17074001462836014, "compression_ratio": 1.4950980392156863, "no_speech_prob": 3.905447101715254e-06}, {"id": 484, "seek": 192632, "start": 1932.6399999999999, "end": 1937.32, "text": " is typing a single line which is this.", "tokens": [307, 18444, 257, 2167, 1622, 597, 307, 341, 13], "temperature": 0.0, "avg_logprob": -0.17074001462836014, "compression_ratio": 1.4950980392156863, "no_speech_prob": 3.905447101715254e-06}, {"id": 485, "seek": 192632, "start": 1937.32, "end": 1947.24, "text": " So if you go to just Google for WSL install run PowerShell as administrator.", "tokens": [407, 498, 291, 352, 281, 445, 3329, 337, 343, 47012, 3625, 1190, 7086, 9526, 285, 382, 25529, 13], "temperature": 0.0, "avg_logprob": -0.17074001462836014, "compression_ratio": 1.4950980392156863, "no_speech_prob": 3.905447101715254e-06}, {"id": 486, "seek": 192632, "start": 1947.24, "end": 1950.76, "text": " Paste that command wait about five minutes reboot you're done.", "tokens": [43827, 300, 5622, 1699, 466, 1732, 2077, 33818, 291, 434, 1096, 13], "temperature": 0.0, "avg_logprob": -0.17074001462836014, "compression_ratio": 1.4950980392156863, "no_speech_prob": 3.905447101715254e-06}, {"id": 487, "seek": 192632, "start": 1950.76, "end": 1953.08, "text": " You now have a complete Linux environment.", "tokens": [509, 586, 362, 257, 3566, 18734, 2823, 13], "temperature": 0.0, "avg_logprob": -0.17074001462836014, "compression_ratio": 1.4950980392156863, "no_speech_prob": 3.905447101715254e-06}, {"id": 488, "seek": 195308, "start": 1953.08, "end": 1957.84, "text": " Now the one of the reasons I'm mentioning this is I'm going to show you how to do stuff", "tokens": [823, 264, 472, 295, 264, 4112, 286, 478, 18315, 341, 307, 286, 478, 516, 281, 855, 291, 577, 281, 360, 1507], "temperature": 0.0, "avg_logprob": -0.16390309475436068, "compression_ratio": 1.625, "no_speech_prob": 4.936916411679704e-06}, {"id": 489, "seek": 195308, "start": 1957.84, "end": 1961.1999999999998, "text": " on your own machine now.", "tokens": [322, 428, 1065, 3479, 586, 13], "temperature": 0.0, "avg_logprob": -0.16390309475436068, "compression_ratio": 1.625, "no_speech_prob": 4.936916411679704e-06}, {"id": 490, "seek": 195308, "start": 1961.1999999999998, "end": 1968.34, "text": " And so this is like going to a bit of an extra level of geekery which some data scientists", "tokens": [400, 370, 341, 307, 411, 516, 281, 257, 857, 295, 364, 2857, 1496, 295, 36162, 2109, 597, 512, 1412, 7708], "temperature": 0.0, "avg_logprob": -0.16390309475436068, "compression_ratio": 1.625, "no_speech_prob": 4.936916411679704e-06}, {"id": 491, "seek": 195308, "start": 1968.34, "end": 1970.3999999999999, "text": " may be less familiar with.", "tokens": [815, 312, 1570, 4963, 365, 13], "temperature": 0.0, "avg_logprob": -0.16390309475436068, "compression_ratio": 1.625, "no_speech_prob": 4.936916411679704e-06}, {"id": 492, "seek": 195308, "start": 1970.3999999999999, "end": 1973.1599999999999, "text": " So you know don't be worried about the terminal.", "tokens": [407, 291, 458, 500, 380, 312, 5804, 466, 264, 14709, 13], "temperature": 0.0, "avg_logprob": -0.16390309475436068, "compression_ratio": 1.625, "no_speech_prob": 4.936916411679704e-06}, {"id": 493, "seek": 195308, "start": 1973.1599999999999, "end": 1977.76, "text": " You're going to think you're going to find it really helpful and much less scary than", "tokens": [509, 434, 516, 281, 519, 291, 434, 516, 281, 915, 309, 534, 4961, 293, 709, 1570, 6958, 813], "temperature": 0.0, "avg_logprob": -0.16390309475436068, "compression_ratio": 1.625, "no_speech_prob": 4.936916411679704e-06}, {"id": 494, "seek": 195308, "start": 1977.76, "end": 1979.6399999999999, "text": " you expect.", "tokens": [291, 2066, 13], "temperature": 0.0, "avg_logprob": -0.16390309475436068, "compression_ratio": 1.625, "no_speech_prob": 4.936916411679704e-06}, {"id": 495, "seek": 197964, "start": 1979.64, "end": 1986.3600000000001, "text": " And I particularly say like for me I choose to use Windows and that's because I get you", "tokens": [400, 286, 4098, 584, 411, 337, 385, 286, 2826, 281, 764, 8591, 293, 300, 311, 570, 286, 483, 291], "temperature": 0.0, "avg_logprob": -0.14433045731377356, "compression_ratio": 1.6853448275862069, "no_speech_prob": 8.397907549806405e-06}, {"id": 496, "seek": 197964, "start": 1986.3600000000001, "end": 1991.4, "text": " know all the nice Windows GUI apps and I can draw on my screen and do presentations and", "tokens": [458, 439, 264, 1481, 8591, 17917, 40, 7733, 293, 286, 393, 2642, 322, 452, 2568, 293, 360, 18964, 293], "temperature": 0.0, "avg_logprob": -0.14433045731377356, "compression_ratio": 1.6853448275862069, "no_speech_prob": 8.397907549806405e-06}, {"id": 497, "seek": 197964, "start": 1991.4, "end": 1997.2, "text": " I have a complete Linux environment as well and that Linux environment uses my GPU and", "tokens": [286, 362, 257, 3566, 18734, 2823, 382, 731, 293, 300, 18734, 2823, 4960, 452, 18407, 293], "temperature": 0.0, "avg_logprob": -0.14433045731377356, "compression_ratio": 1.6853448275862069, "no_speech_prob": 8.397907549806405e-06}, {"id": 498, "seek": 197964, "start": 1997.2, "end": 1998.2, "text": " everything.", "tokens": [1203, 13], "temperature": 0.0, "avg_logprob": -0.14433045731377356, "compression_ratio": 1.6853448275862069, "no_speech_prob": 8.397907549806405e-06}, {"id": 499, "seek": 197964, "start": 1998.2, "end": 2002.0, "text": " So for me my first choice is to use Windows.", "tokens": [407, 337, 385, 452, 700, 3922, 307, 281, 764, 8591, 13], "temperature": 0.0, "avg_logprob": -0.14433045731377356, "compression_ratio": 1.6853448275862069, "no_speech_prob": 8.397907549806405e-06}, {"id": 500, "seek": 197964, "start": 2002.0, "end": 2006.3600000000001, "text": " My second choice by not very much really like it would be to use Linux.", "tokens": [1222, 1150, 3922, 538, 406, 588, 709, 534, 411, 309, 576, 312, 281, 764, 18734, 13], "temperature": 0.0, "avg_logprob": -0.14433045731377356, "compression_ratio": 1.6853448275862069, "no_speech_prob": 8.397907549806405e-06}, {"id": 501, "seek": 200636, "start": 2006.36, "end": 2014.1599999999999, "text": " Mac is a little bit harder but it's still usable.", "tokens": [5707, 307, 257, 707, 857, 6081, 457, 309, 311, 920, 29975, 13], "temperature": 0.0, "avg_logprob": -0.12788886440043545, "compression_ratio": 1.7837837837837838, "no_speech_prob": 3.726616341737099e-06}, {"id": 502, "seek": 200636, "start": 2014.1599999999999, "end": 2017.9599999999998, "text": " So some things are a little bit trickier on Mac but you should be fine.", "tokens": [407, 512, 721, 366, 257, 707, 857, 4282, 811, 322, 5707, 457, 291, 820, 312, 2489, 13], "temperature": 0.0, "avg_logprob": -0.12788886440043545, "compression_ratio": 1.7837837837837838, "no_speech_prob": 3.726616341737099e-06}, {"id": 503, "seek": 200636, "start": 2017.9599999999998, "end": 2025.1599999999999, "text": " Okay so whatever you've got at this point you've now got a terminal available and so", "tokens": [1033, 370, 2035, 291, 600, 658, 412, 341, 935, 291, 600, 586, 658, 257, 14709, 2435, 293, 370], "temperature": 0.0, "avg_logprob": -0.12788886440043545, "compression_ratio": 1.7837837837837838, "no_speech_prob": 3.726616341737099e-06}, {"id": 504, "seek": 200636, "start": 2025.1599999999999, "end": 2028.7199999999998, "text": " in your terminal one of the really nice things about using a terminal is you don't have to", "tokens": [294, 428, 14709, 472, 295, 264, 534, 1481, 721, 466, 1228, 257, 14709, 307, 291, 500, 380, 362, 281], "temperature": 0.0, "avg_logprob": -0.12788886440043545, "compression_ratio": 1.7837837837837838, "no_speech_prob": 3.726616341737099e-06}, {"id": 505, "seek": 200636, "start": 2028.7199999999998, "end": 2032.76, "text": " follow lots of instructions about click here click here click here you just copy and paste", "tokens": [1524, 3195, 295, 9415, 466, 2052, 510, 2052, 510, 2052, 510, 291, 445, 5055, 293, 9163], "temperature": 0.0, "avg_logprob": -0.12788886440043545, "compression_ratio": 1.7837837837837838, "no_speech_prob": 3.726616341737099e-06}, {"id": 506, "seek": 200636, "start": 2032.76, "end": 2033.76, "text": " things.", "tokens": [721, 13], "temperature": 0.0, "avg_logprob": -0.12788886440043545, "compression_ratio": 1.7837837837837838, "no_speech_prob": 3.726616341737099e-06}, {"id": 507, "seek": 203376, "start": 2033.76, "end": 2040.36, "text": " So you just copy this and you go over to your terminal and you paste it in and you run it", "tokens": [407, 291, 445, 5055, 341, 293, 291, 352, 670, 281, 428, 14709, 293, 291, 9163, 309, 294, 293, 291, 1190, 309], "temperature": 0.0, "avg_logprob": -0.1045340551456935, "compression_ratio": 1.654320987654321, "no_speech_prob": 8.800930118013639e-06}, {"id": 508, "seek": 203376, "start": 2040.36, "end": 2047.28, "text": " and after you do that you'll find that you've now got a directory.", "tokens": [293, 934, 291, 360, 300, 291, 603, 915, 300, 291, 600, 586, 658, 257, 21120, 13], "temperature": 0.0, "avg_logprob": -0.1045340551456935, "compression_ratio": 1.654320987654321, "no_speech_prob": 8.800930118013639e-06}, {"id": 509, "seek": 203376, "start": 2047.28, "end": 2054.0, "text": " And so that new directory initially is empty and they tell you okay go ahead and create", "tokens": [400, 370, 300, 777, 21120, 9105, 307, 6707, 293, 436, 980, 291, 1392, 352, 2286, 293, 1884], "temperature": 0.0, "avg_logprob": -0.1045340551456935, "compression_ratio": 1.654320987654321, "no_speech_prob": 8.800930118013639e-06}, {"id": 510, "seek": 203376, "start": 2054.0, "end": 2057.52, "text": " a file with this in it.", "tokens": [257, 3991, 365, 341, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.1045340551456935, "compression_ratio": 1.654320987654321, "no_speech_prob": 8.800930118013639e-06}, {"id": 511, "seek": 205752, "start": 2057.52, "end": 2063.92, "text": " Okay so how do you create a file with that in it when we're in here in our Linux environment", "tokens": [1033, 370, 577, 360, 291, 1884, 257, 3991, 365, 300, 294, 309, 562, 321, 434, 294, 510, 294, 527, 18734, 2823], "temperature": 0.0, "avg_logprob": -0.20482932747184457, "compression_ratio": 1.540983606557377, "no_speech_prob": 9.66596053331159e-06}, {"id": 512, "seek": 205752, "start": 2063.92, "end": 2067.6, "text": " on Windows or in the terminal on Mac or whatever.", "tokens": [322, 8591, 420, 294, 264, 14709, 322, 5707, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.20482932747184457, "compression_ratio": 1.540983606557377, "no_speech_prob": 9.66596053331159e-06}, {"id": 513, "seek": 205752, "start": 2067.6, "end": 2072.44, "text": " Well all you do in Windows if you just type explorer.exe.", "tokens": [1042, 439, 291, 360, 294, 8591, 498, 291, 445, 2010, 39680, 13, 3121, 68, 13], "temperature": 0.0, "avg_logprob": -0.20482932747184457, "compression_ratio": 1.540983606557377, "no_speech_prob": 9.66596053331159e-06}, {"id": 514, "seek": 205752, "start": 2072.44, "end": 2074.12, "text": " It'll open up Explorer here.", "tokens": [467, 603, 1269, 493, 31895, 510, 13], "temperature": 0.0, "avg_logprob": -0.20482932747184457, "compression_ratio": 1.540983606557377, "no_speech_prob": 9.66596053331159e-06}, {"id": 515, "seek": 205752, "start": 2074.12, "end": 2082.48, "text": " Well better still on either Mac or Linux or Windows.", "tokens": [1042, 1101, 920, 322, 2139, 5707, 420, 18734, 420, 8591, 13], "temperature": 0.0, "avg_logprob": -0.20482932747184457, "compression_ratio": 1.540983606557377, "no_speech_prob": 9.66596053331159e-06}, {"id": 516, "seek": 208248, "start": 2082.48, "end": 2090.12, "text": " So yeah so regardless of what computer type of computer on you can just type code.", "tokens": [407, 1338, 370, 10060, 295, 437, 3820, 2010, 295, 3820, 322, 291, 393, 445, 2010, 3089, 13], "temperature": 0.0, "avg_logprob": -0.15785472033775017, "compression_ratio": 1.5675675675675675, "no_speech_prob": 1.4593724699807353e-06}, {"id": 517, "seek": 208248, "start": 2090.12, "end": 2096.6, "text": " And it will pop up Visual Studio code and open up your folder.", "tokens": [400, 309, 486, 1665, 493, 23187, 13500, 3089, 293, 1269, 493, 428, 10820, 13], "temperature": 0.0, "avg_logprob": -0.15785472033775017, "compression_ratio": 1.5675675675675675, "no_speech_prob": 1.4593724699807353e-06}, {"id": 518, "seek": 208248, "start": 2096.6, "end": 2101.76, "text": " And so then you can just go ahead and if you haven't used VS code before it's really well", "tokens": [400, 370, 550, 291, 393, 445, 352, 2286, 293, 498, 291, 2378, 380, 1143, 25091, 3089, 949, 309, 311, 534, 731], "temperature": 0.0, "avg_logprob": -0.15785472033775017, "compression_ratio": 1.5675675675675675, "no_speech_prob": 1.4593724699807353e-06}, {"id": 519, "seek": 208248, "start": 2101.76, "end": 2105.56, "text": " worth taking a few minutes to read the some tutorials.", "tokens": [3163, 1940, 257, 1326, 2077, 281, 1401, 264, 512, 17616, 13], "temperature": 0.0, "avg_logprob": -0.15785472033775017, "compression_ratio": 1.5675675675675675, "no_speech_prob": 1.4593724699807353e-06}, {"id": 520, "seek": 210556, "start": 2105.56, "end": 2113.7599999999998, "text": " It's a really great IDE and so you can go ahead and create an app.py file like they", "tokens": [467, 311, 257, 534, 869, 40930, 293, 370, 291, 393, 352, 2286, 293, 1884, 364, 724, 13, 8200, 3991, 411, 436], "temperature": 0.0, "avg_logprob": -0.1954727638058546, "compression_ratio": 1.6162790697674418, "no_speech_prob": 2.9944287689431803e-06}, {"id": 521, "seek": 210556, "start": 2113.7599999999998, "end": 2119.56, "text": " tell you to app.py file containing what they told you to put in it.", "tokens": [980, 291, 281, 724, 13, 8200, 3991, 19273, 437, 436, 1907, 291, 281, 829, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.1954727638058546, "compression_ratio": 1.6162790697674418, "no_speech_prob": 2.9944287689431803e-06}, {"id": 522, "seek": 210556, "start": 2119.56, "end": 2121.56, "text": " Here it is here.", "tokens": [1692, 309, 307, 510, 13], "temperature": 0.0, "avg_logprob": -0.1954727638058546, "compression_ratio": 1.6162790697674418, "no_speech_prob": 2.9944287689431803e-06}, {"id": 523, "seek": 210556, "start": 2121.56, "end": 2126.24, "text": " All right we're nearly there.", "tokens": [1057, 558, 321, 434, 6217, 456, 13], "temperature": 0.0, "avg_logprob": -0.1954727638058546, "compression_ratio": 1.6162790697674418, "no_speech_prob": 2.9944287689431803e-06}, {"id": 524, "seek": 210556, "start": 2126.24, "end": 2135.2, "text": " So you can now go ahead and save that and then you need to commit it to Gradio.", "tokens": [407, 291, 393, 586, 352, 2286, 293, 3155, 300, 293, 550, 291, 643, 281, 5599, 309, 281, 16710, 1004, 13], "temperature": 0.0, "avg_logprob": -0.1954727638058546, "compression_ratio": 1.6162790697674418, "no_speech_prob": 2.9944287689431803e-06}, {"id": 525, "seek": 213520, "start": 2135.2, "end": 2137.56, "text": " To Hacking Space Spaces.", "tokens": [1407, 389, 14134, 8705, 1738, 2116, 13], "temperature": 0.0, "avg_logprob": -0.21969215393066407, "compression_ratio": 1.7307692307692308, "no_speech_prob": 8.267791599791963e-06}, {"id": 526, "seek": 213520, "start": 2137.56, "end": 2143.56, "text": " So one really easy way is just in Visual Studio itself you can just click here and that'll", "tokens": [407, 472, 534, 1858, 636, 307, 445, 294, 23187, 13500, 2564, 291, 393, 445, 2052, 510, 293, 300, 603], "temperature": 0.0, "avg_logprob": -0.21969215393066407, "compression_ratio": 1.7307692307692308, "no_speech_prob": 8.267791599791963e-06}, {"id": 527, "seek": 213520, "start": 2143.56, "end": 2148.48, "text": " give you a place where you type a message and you hit tick and it'll send it off to", "tokens": [976, 291, 257, 1081, 689, 291, 2010, 257, 3636, 293, 291, 2045, 5204, 293, 309, 603, 2845, 309, 766, 281], "temperature": 0.0, "avg_logprob": -0.21969215393066407, "compression_ratio": 1.7307692307692308, "no_speech_prob": 8.267791599791963e-06}, {"id": 528, "seek": 213520, "start": 2148.48, "end": 2150.64, "text": " Hacking Space Spaces for you.", "tokens": [389, 14134, 8705, 1738, 2116, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.21969215393066407, "compression_ratio": 1.7307692307692308, "no_speech_prob": 8.267791599791963e-06}, {"id": 529, "seek": 213520, "start": 2150.64, "end": 2158.64, "text": " So once you've done that you can then go to back to the exact same website you were on", "tokens": [407, 1564, 291, 600, 1096, 300, 291, 393, 550, 352, 281, 646, 281, 264, 1900, 912, 3144, 291, 645, 322], "temperature": 0.0, "avg_logprob": -0.21969215393066407, "compression_ratio": 1.7307692307692308, "no_speech_prob": 8.267791599791963e-06}, {"id": 530, "seek": 213520, "start": 2158.64, "end": 2163.3199999999997, "text": " before Hacking Space Spaces JP HRO Minimal.", "tokens": [949, 389, 14134, 8705, 1738, 2116, 34336, 389, 7142, 2829, 10650, 13], "temperature": 0.0, "avg_logprob": -0.21969215393066407, "compression_ratio": 1.7307692307692308, "no_speech_prob": 8.267791599791963e-06}, {"id": 531, "seek": 216332, "start": 2163.32, "end": 2175.84, "text": " And what you'll find now is that it'll take about a minute to build your website and it's", "tokens": [400, 437, 291, 603, 915, 586, 307, 300, 309, 603, 747, 466, 257, 3456, 281, 1322, 428, 3144, 293, 309, 311], "temperature": 0.0, "avg_logprob": -0.10942845633535674, "compression_ratio": 1.7096774193548387, "no_speech_prob": 8.579207815273548e-07}, {"id": 532, "seek": 216332, "start": 2175.84, "end": 2185.28, "text": " the website it's building is going to have a Gradio interface with a text input a text", "tokens": [264, 3144, 309, 311, 2390, 307, 516, 281, 362, 257, 16710, 1004, 9226, 365, 257, 2487, 4846, 257, 2487], "temperature": 0.0, "avg_logprob": -0.10942845633535674, "compression_ratio": 1.7096774193548387, "no_speech_prob": 8.579207815273548e-07}, {"id": 533, "seek": 216332, "start": 2185.28, "end": 2192.1600000000003, "text": " output and it's going to run a function called greet on the input and my function called", "tokens": [5598, 293, 309, 311, 516, 281, 1190, 257, 2445, 1219, 12044, 322, 264, 4846, 293, 452, 2445, 1219], "temperature": 0.0, "avg_logprob": -0.10942845633535674, "compression_ratio": 1.7096774193548387, "no_speech_prob": 8.579207815273548e-07}, {"id": 534, "seek": 219216, "start": 2192.16, "end": 2196.2, "text": " greet or return hello name.", "tokens": [12044, 420, 2736, 7751, 1315, 13], "temperature": 0.0, "avg_logprob": -0.24235526585983017, "compression_ratio": 1.5714285714285714, "no_speech_prob": 3.1875424610916525e-06}, {"id": 535, "seek": 219216, "start": 2196.2, "end": 2199.2, "text": " So that's what it's going to do.", "tokens": [407, 300, 311, 437, 309, 311, 516, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.24235526585983017, "compression_ratio": 1.5714285714285714, "no_speech_prob": 3.1875424610916525e-06}, {"id": 536, "seek": 219216, "start": 2199.2, "end": 2201.56, "text": " There it goes.", "tokens": [821, 309, 1709, 13], "temperature": 0.0, "avg_logprob": -0.24235526585983017, "compression_ratio": 1.5714285714285714, "no_speech_prob": 3.1875424610916525e-06}, {"id": 537, "seek": 219216, "start": 2201.56, "end": 2202.56, "text": " Let's try it.", "tokens": [961, 311, 853, 309, 13], "temperature": 0.0, "avg_logprob": -0.24235526585983017, "compression_ratio": 1.5714285714285714, "no_speech_prob": 3.1875424610916525e-06}, {"id": 538, "seek": 219216, "start": 2202.56, "end": 2204.56, "text": " We'll say hello to Tanishk.", "tokens": [492, 603, 584, 7751, 281, 314, 7524, 74, 13], "temperature": 0.0, "avg_logprob": -0.24235526585983017, "compression_ratio": 1.5714285714285714, "no_speech_prob": 3.1875424610916525e-06}, {"id": 539, "seek": 219216, "start": 2204.56, "end": 2207.7599999999998, "text": " I'm not always very good at remembering how to spell his name.", "tokens": [286, 478, 406, 1009, 588, 665, 412, 20719, 577, 281, 9827, 702, 1315, 13], "temperature": 0.0, "avg_logprob": -0.24235526585983017, "compression_ratio": 1.5714285714285714, "no_speech_prob": 3.1875424610916525e-06}, {"id": 540, "seek": 219216, "start": 2207.7599999999998, "end": 2210.62, "text": " I think it's like that.", "tokens": [286, 519, 309, 311, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.24235526585983017, "compression_ratio": 1.5714285714285714, "no_speech_prob": 3.1875424610916525e-06}, {"id": 541, "seek": 219216, "start": 2210.62, "end": 2211.62, "text": " And there you go.", "tokens": [400, 456, 291, 352, 13], "temperature": 0.0, "avg_logprob": -0.24235526585983017, "compression_ratio": 1.5714285714285714, "no_speech_prob": 3.1875424610916525e-06}, {"id": 542, "seek": 219216, "start": 2211.62, "end": 2213.72, "text": " So you can see it's put the output for our input.", "tokens": [407, 291, 393, 536, 309, 311, 829, 264, 5598, 337, 527, 4846, 13], "temperature": 0.0, "avg_logprob": -0.24235526585983017, "compression_ratio": 1.5714285714285714, "no_speech_prob": 3.1875424610916525e-06}, {"id": 543, "seek": 219216, "start": 2213.72, "end": 2219.8799999999997, "text": " So not a very exciting app but we now have to be fair an app running in production.", "tokens": [407, 406, 257, 588, 4670, 724, 457, 321, 586, 362, 281, 312, 3143, 364, 724, 2614, 294, 4265, 13], "temperature": 0.0, "avg_logprob": -0.24235526585983017, "compression_ratio": 1.5714285714285714, "no_speech_prob": 3.1875424610916525e-06}, {"id": 544, "seek": 219216, "start": 2219.8799999999997, "end": 2220.8799999999997, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.24235526585983017, "compression_ratio": 1.5714285714285714, "no_speech_prob": 3.1875424610916525e-06}, {"id": 545, "seek": 222088, "start": 2220.88, "end": 2225.2400000000002, "text": " I told you we'd have a deep learning model running in production.", "tokens": [286, 1907, 291, 321, 1116, 362, 257, 2452, 2539, 2316, 2614, 294, 4265, 13], "temperature": 0.0, "avg_logprob": -0.1293680279753929, "compression_ratio": 1.797872340425532, "no_speech_prob": 3.237702912883833e-06}, {"id": 546, "seek": 222088, "start": 2225.2400000000002, "end": 2231.2000000000003, "text": " So now we have to take the next step which is to turn this into a deep learning model.", "tokens": [407, 586, 321, 362, 281, 747, 264, 958, 1823, 597, 307, 281, 1261, 341, 666, 257, 2452, 2539, 2316, 13], "temperature": 0.0, "avg_logprob": -0.1293680279753929, "compression_ratio": 1.797872340425532, "no_speech_prob": 3.237702912883833e-06}, {"id": 547, "seek": 222088, "start": 2231.2000000000003, "end": 2232.2000000000003, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.1293680279753929, "compression_ratio": 1.797872340425532, "no_speech_prob": 3.237702912883833e-06}, {"id": 548, "seek": 222088, "start": 2232.2000000000003, "end": 2239.56, "text": " So first we're going to need a deep learning model and there's a few different ways we", "tokens": [407, 700, 321, 434, 516, 281, 643, 257, 2452, 2539, 2316, 293, 456, 311, 257, 1326, 819, 2098, 321], "temperature": 0.0, "avg_logprob": -0.1293680279753929, "compression_ratio": 1.797872340425532, "no_speech_prob": 3.237702912883833e-06}, {"id": 549, "seek": 222088, "start": 2239.56, "end": 2245.4, "text": " can get ourselves a deep learning model but basically we're going to have to train one.", "tokens": [393, 483, 4175, 257, 2452, 2539, 2316, 457, 1936, 321, 434, 516, 281, 362, 281, 3847, 472, 13], "temperature": 0.0, "avg_logprob": -0.1293680279753929, "compression_ratio": 1.797872340425532, "no_speech_prob": 3.237702912883833e-06}, {"id": 550, "seek": 224540, "start": 2245.4, "end": 2251.92, "text": " So I've got a couple of examples I've got a Kaggle example and a Colab example.", "tokens": [407, 286, 600, 658, 257, 1916, 295, 5110, 286, 600, 658, 257, 48751, 22631, 1365, 293, 257, 4004, 455, 1365, 13], "temperature": 0.0, "avg_logprob": -0.21608377010264296, "compression_ratio": 1.5544554455445545, "no_speech_prob": 3.905452103936113e-06}, {"id": 551, "seek": 224540, "start": 2251.92, "end": 2252.92, "text": " Maybe I'll quickly show you both.", "tokens": [2704, 286, 603, 2661, 855, 291, 1293, 13], "temperature": 0.0, "avg_logprob": -0.21608377010264296, "compression_ratio": 1.5544554455445545, "no_speech_prob": 3.905452103936113e-06}, {"id": 552, "seek": 224540, "start": 2252.92, "end": 2258.28, "text": " They're going to do the same thing and I'm just going to create a dog or a cat classifier.", "tokens": [814, 434, 516, 281, 360, 264, 912, 551, 293, 286, 478, 445, 516, 281, 1884, 257, 3000, 420, 257, 3857, 1508, 9902, 13], "temperature": 0.0, "avg_logprob": -0.21608377010264296, "compression_ratio": 1.5544554455445545, "no_speech_prob": 3.905452103936113e-06}, {"id": 553, "seek": 224540, "start": 2258.28, "end": 2259.84, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.21608377010264296, "compression_ratio": 1.5544554455445545, "no_speech_prob": 3.905452103936113e-06}, {"id": 554, "seek": 224540, "start": 2259.84, "end": 2262.0, "text": " So here's our Kaggle model.", "tokens": [407, 510, 311, 527, 48751, 22631, 2316, 13], "temperature": 0.0, "avg_logprob": -0.21608377010264296, "compression_ratio": 1.5544554455445545, "no_speech_prob": 3.905452103936113e-06}, {"id": 555, "seek": 224540, "start": 2262.0, "end": 2272.28, "text": " I'll click on edit so you can actually see what it looks like in edit view.", "tokens": [286, 603, 2052, 322, 8129, 370, 291, 393, 767, 536, 437, 309, 1542, 411, 294, 8129, 1910, 13], "temperature": 0.0, "avg_logprob": -0.21608377010264296, "compression_ratio": 1.5544554455445545, "no_speech_prob": 3.905452103936113e-06}, {"id": 556, "seek": 227228, "start": 2272.28, "end": 2276.44, "text": " Now Kaggle already has fast AI installed but I always put this first just to make sure", "tokens": [823, 48751, 22631, 1217, 575, 2370, 7318, 8899, 457, 286, 1009, 829, 341, 700, 445, 281, 652, 988], "temperature": 0.0, "avg_logprob": -0.16019305274600074, "compression_ratio": 1.5945945945945945, "no_speech_prob": 1.963789827641449e-06}, {"id": 557, "seek": 227228, "start": 2276.44, "end": 2280.8, "text": " we've got the latest version and obviously import stuff.", "tokens": [321, 600, 658, 264, 6792, 3037, 293, 2745, 974, 1507, 13], "temperature": 0.0, "avg_logprob": -0.16019305274600074, "compression_ratio": 1.5945945945945945, "no_speech_prob": 1.963789827641449e-06}, {"id": 558, "seek": 227228, "start": 2280.8, "end": 2286.84, "text": " So we're going to grab the pets data set a function to check whether it's a cat.", "tokens": [407, 321, 434, 516, 281, 4444, 264, 19897, 1412, 992, 257, 2445, 281, 1520, 1968, 309, 311, 257, 3857, 13], "temperature": 0.0, "avg_logprob": -0.16019305274600074, "compression_ratio": 1.5945945945945945, "no_speech_prob": 1.963789827641449e-06}, {"id": 559, "seek": 227228, "start": 2286.84, "end": 2289.4, "text": " That's our labeling function for our image data loaders.", "tokens": [663, 311, 527, 40244, 2445, 337, 527, 3256, 1412, 3677, 433, 13], "temperature": 0.0, "avg_logprob": -0.16019305274600074, "compression_ratio": 1.5945945945945945, "no_speech_prob": 1.963789827641449e-06}, {"id": 560, "seek": 227228, "start": 2289.4, "end": 2291.36, "text": " Remember this is just another way of doing data blocks.", "tokens": [5459, 341, 307, 445, 1071, 636, 295, 884, 1412, 8474, 13], "temperature": 0.0, "avg_logprob": -0.16019305274600074, "compression_ratio": 1.5945945945945945, "no_speech_prob": 1.963789827641449e-06}, {"id": 561, "seek": 227228, "start": 2291.36, "end": 2296.76, "text": " It's like a little shorthand and we create our learner and we fine tune it.", "tokens": [467, 311, 411, 257, 707, 402, 2652, 474, 293, 321, 1884, 527, 33347, 293, 321, 2489, 10864, 309, 13], "temperature": 0.0, "avg_logprob": -0.16019305274600074, "compression_ratio": 1.5945945945945945, "no_speech_prob": 1.963789827641449e-06}, {"id": 562, "seek": 229676, "start": 2296.76, "end": 2302.36, "text": " Okay so that's all stuff we've seen before.", "tokens": [1033, 370, 300, 311, 439, 1507, 321, 600, 1612, 949, 13], "temperature": 0.0, "avg_logprob": -0.10862850848539377, "compression_ratio": 1.6774193548387097, "no_speech_prob": 3.844916136586107e-06}, {"id": 563, "seek": 229676, "start": 2302.36, "end": 2307.5200000000004, "text": " So in Kaggle every notebook has a edit view which is what you just saw and a reader view", "tokens": [407, 294, 48751, 22631, 633, 21060, 575, 257, 8129, 1910, 597, 307, 437, 291, 445, 1866, 293, 257, 15149, 1910], "temperature": 0.0, "avg_logprob": -0.10862850848539377, "compression_ratio": 1.6774193548387097, "no_speech_prob": 3.844916136586107e-06}, {"id": 564, "seek": 229676, "start": 2307.5200000000004, "end": 2315.5200000000004, "text": " and so you can share your notebook if you want to and then anybody can read the reader", "tokens": [293, 370, 291, 393, 2073, 428, 21060, 498, 291, 528, 281, 293, 550, 4472, 393, 1401, 264, 15149], "temperature": 0.0, "avg_logprob": -0.10862850848539377, "compression_ratio": 1.6774193548387097, "no_speech_prob": 3.844916136586107e-06}, {"id": 565, "seek": 229676, "start": 2315.5200000000004, "end": 2323.6800000000003, "text": " view as you see and so you can see it shows you what happened when I ran it and so I trained", "tokens": [1910, 382, 291, 536, 293, 370, 291, 393, 536, 309, 3110, 291, 437, 2011, 562, 286, 5872, 309, 293, 370, 286, 8895], "temperature": 0.0, "avg_logprob": -0.10862850848539377, "compression_ratio": 1.6774193548387097, "no_speech_prob": 3.844916136586107e-06}, {"id": 566, "seek": 232368, "start": 2323.68, "end": 2329.3999999999996, "text": " it, it took you know so that the GPUs on Kaggle are a bit slower than most modern GPUs but", "tokens": [309, 11, 309, 1890, 291, 458, 370, 300, 264, 18407, 82, 322, 48751, 22631, 366, 257, 857, 14009, 813, 881, 4363, 18407, 82, 457], "temperature": 0.0, "avg_logprob": -0.1518132665135839, "compression_ratio": 1.6887966804979253, "no_speech_prob": 6.439003300329205e-06}, {"id": 567, "seek": 232368, "start": 2329.3999999999996, "end": 2330.3999999999996, "text": " they're still fast enough.", "tokens": [436, 434, 920, 2370, 1547, 13], "temperature": 0.0, "avg_logprob": -0.1518132665135839, "compression_ratio": 1.6887966804979253, "no_speech_prob": 6.439003300329205e-06}, {"id": 568, "seek": 232368, "start": 2330.3999999999996, "end": 2337.04, "text": " I mean it takes five minutes and there's one bit at the end here which you haven't seen", "tokens": [286, 914, 309, 2516, 1732, 2077, 293, 456, 311, 472, 857, 412, 264, 917, 510, 597, 291, 2378, 380, 1612], "temperature": 0.0, "avg_logprob": -0.1518132665135839, "compression_ratio": 1.6887966804979253, "no_speech_prob": 6.439003300329205e-06}, {"id": 569, "seek": 232368, "start": 2337.04, "end": 2342.8799999999997, "text": " before which is I go learn.export and I give it a name.", "tokens": [949, 597, 307, 286, 352, 1466, 13, 3121, 2707, 293, 286, 976, 309, 257, 1315, 13], "temperature": 0.0, "avg_logprob": -0.1518132665135839, "compression_ratio": 1.6887966804979253, "no_speech_prob": 6.439003300329205e-06}, {"id": 570, "seek": 232368, "start": 2342.8799999999997, "end": 2349.2799999999997, "text": " Now that's going to create a file containing our trained model and that's the only thing", "tokens": [823, 300, 311, 516, 281, 1884, 257, 3991, 19273, 527, 8895, 2316, 293, 300, 311, 264, 787, 551], "temperature": 0.0, "avg_logprob": -0.1518132665135839, "compression_ratio": 1.6887966804979253, "no_speech_prob": 6.439003300329205e-06}, {"id": 571, "seek": 232368, "start": 2349.2799999999997, "end": 2352.44, "text": " creating this file is the only thing you need a GPU for.", "tokens": [4084, 341, 3991, 307, 264, 787, 551, 291, 643, 257, 18407, 337, 13], "temperature": 0.0, "avg_logprob": -0.1518132665135839, "compression_ratio": 1.6887966804979253, "no_speech_prob": 6.439003300329205e-06}, {"id": 572, "seek": 235244, "start": 2352.44, "end": 2359.04, "text": " Right so you do that on Kaggle or on Colab so here's exactly the same thing on Colab.", "tokens": [1779, 370, 291, 360, 300, 322, 48751, 22631, 420, 322, 4004, 455, 370, 510, 311, 2293, 264, 912, 551, 322, 4004, 455, 13], "temperature": 0.0, "avg_logprob": -0.15714241374622692, "compression_ratio": 1.5857740585774058, "no_speech_prob": 5.955103915766813e-06}, {"id": 573, "seek": 235244, "start": 2359.04, "end": 2364.58, "text": " You can see pip install, here's cat, untar data, image data loaders so I've got a show", "tokens": [509, 393, 536, 8489, 3625, 11, 510, 311, 3857, 11, 1701, 289, 1412, 11, 3256, 1412, 3677, 433, 370, 286, 600, 658, 257, 855], "temperature": 0.0, "avg_logprob": -0.15714241374622692, "compression_ratio": 1.5857740585774058, "no_speech_prob": 5.955103915766813e-06}, {"id": 574, "seek": 235244, "start": 2364.58, "end": 2368.28, "text": " batch here as well just for fun.", "tokens": [15245, 510, 382, 731, 445, 337, 1019, 13], "temperature": 0.0, "avg_logprob": -0.15714241374622692, "compression_ratio": 1.5857740585774058, "no_speech_prob": 5.955103915766813e-06}, {"id": 575, "seek": 235244, "start": 2368.28, "end": 2371.86, "text": " Create my learner and then export.", "tokens": [20248, 452, 33347, 293, 550, 10725, 13], "temperature": 0.0, "avg_logprob": -0.15714241374622692, "compression_ratio": 1.5857740585774058, "no_speech_prob": 5.955103915766813e-06}, {"id": 576, "seek": 235244, "start": 2371.86, "end": 2376.92, "text": " So while we wait I might go ahead and just run that.", "tokens": [407, 1339, 321, 1699, 286, 1062, 352, 2286, 293, 445, 1190, 300, 13], "temperature": 0.0, "avg_logprob": -0.15714241374622692, "compression_ratio": 1.5857740585774058, "no_speech_prob": 5.955103915766813e-06}, {"id": 577, "seek": 235244, "start": 2376.92, "end": 2381.12, "text": " One nice thing about Kaggle is once you've run it and saved it you can then go to the", "tokens": [1485, 1481, 551, 466, 48751, 22631, 307, 1564, 291, 600, 1190, 309, 293, 6624, 309, 291, 393, 550, 352, 281, 264], "temperature": 0.0, "avg_logprob": -0.15714241374622692, "compression_ratio": 1.5857740585774058, "no_speech_prob": 5.955103915766813e-06}, {"id": 578, "seek": 238112, "start": 2381.12, "end": 2388.2, "text": " data tab and here is basically anything you've saved it's going to appear here and here it", "tokens": [1412, 4421, 293, 510, 307, 1936, 1340, 291, 600, 6624, 309, 311, 516, 281, 4204, 510, 293, 510, 309], "temperature": 0.0, "avg_logprob": -0.18903684012497526, "compression_ratio": 1.6022099447513811, "no_speech_prob": 1.7880585119200987e-06}, {"id": 579, "seek": 238112, "start": 2388.2, "end": 2391.7999999999997, "text": " is model.pickle.", "tokens": [307, 2316, 13, 79, 618, 306, 13], "temperature": 0.0, "avg_logprob": -0.18903684012497526, "compression_ratio": 1.6022099447513811, "no_speech_prob": 1.7880585119200987e-06}, {"id": 580, "seek": 238112, "start": 2391.7999999999997, "end": 2399.52, "text": " So now I can go ahead and download that and that will then be downloaded to my downloads", "tokens": [407, 586, 286, 393, 352, 2286, 293, 5484, 300, 293, 300, 486, 550, 312, 21748, 281, 452, 36553], "temperature": 0.0, "avg_logprob": -0.18903684012497526, "compression_ratio": 1.6022099447513811, "no_speech_prob": 1.7880585119200987e-06}, {"id": 581, "seek": 238112, "start": 2399.52, "end": 2405.8399999999997, "text": " folder and then I need to copy it into the same directory that my Hackingface Bases app's", "tokens": [10820, 293, 550, 286, 643, 281, 5055, 309, 666, 264, 912, 21120, 300, 452, 389, 14134, 2868, 363, 1957, 724, 311], "temperature": 0.0, "avg_logprob": -0.18903684012497526, "compression_ratio": 1.6022099447513811, "no_speech_prob": 1.7880585119200987e-06}, {"id": 582, "seek": 238112, "start": 2405.8399999999997, "end": 2406.8399999999997, "text": " in.", "tokens": [294, 13], "temperature": 0.0, "avg_logprob": -0.18903684012497526, "compression_ratio": 1.6022099447513811, "no_speech_prob": 1.7880585119200987e-06}, {"id": 583, "seek": 240684, "start": 2406.84, "end": 2413.6800000000003, "text": " Now my Hackingface Bases app is currently open in my terminal.", "tokens": [823, 452, 389, 14134, 2868, 363, 1957, 724, 307, 4362, 1269, 294, 452, 14709, 13], "temperature": 0.0, "avg_logprob": -0.1654095582559075, "compression_ratio": 1.6011560693641618, "no_speech_prob": 1.1911054116353625e-06}, {"id": 584, "seek": 240684, "start": 2413.6800000000003, "end": 2422.28, "text": " On Mac you can type open dot or in Windows you can type explorer.xc dot and that will", "tokens": [1282, 5707, 291, 393, 2010, 1269, 5893, 420, 294, 8591, 291, 393, 2010, 39680, 13, 87, 66, 5893, 293, 300, 486], "temperature": 0.0, "avg_logprob": -0.1654095582559075, "compression_ratio": 1.6011560693641618, "no_speech_prob": 1.1911054116353625e-06}, {"id": 585, "seek": 240684, "start": 2422.28, "end": 2432.8, "text": " bring up your finder or explorer in that directory and so then you can just paste that thing", "tokens": [1565, 493, 428, 915, 260, 420, 39680, 294, 300, 21120, 293, 370, 550, 291, 393, 445, 9163, 300, 551], "temperature": 0.0, "avg_logprob": -0.1654095582559075, "compression_ratio": 1.6011560693641618, "no_speech_prob": 1.1911054116353625e-06}, {"id": 586, "seek": 240684, "start": 2432.8, "end": 2436.2200000000003, "text": " you downloaded into this directory.", "tokens": [291, 21748, 666, 341, 21120, 13], "temperature": 0.0, "avg_logprob": -0.1654095582559075, "compression_ratio": 1.6011560693641618, "no_speech_prob": 1.1911054116353625e-06}, {"id": 587, "seek": 243622, "start": 2436.22, "end": 2441.7599999999998, "text": " Something by the way in Windows I do which I find really helpful is I actually grab my", "tokens": [6595, 538, 264, 636, 294, 8591, 286, 360, 597, 286, 915, 534, 4961, 307, 286, 767, 4444, 452], "temperature": 0.0, "avg_logprob": -0.14236284672528848, "compression_ratio": 1.6322869955156951, "no_speech_prob": 9.516157660982572e-06}, {"id": 588, "seek": 243622, "start": 2441.7599999999998, "end": 2448.3999999999996, "text": " home directory in Linux and I pin it to my quick access and that way I can always jump", "tokens": [1280, 21120, 294, 18734, 293, 286, 5447, 309, 281, 452, 1702, 2105, 293, 300, 636, 286, 393, 1009, 3012], "temperature": 0.0, "avg_logprob": -0.14236284672528848, "compression_ratio": 1.6322869955156951, "no_speech_prob": 9.516157660982572e-06}, {"id": 589, "seek": 243622, "start": 2448.3999999999996, "end": 2451.9599999999996, "text": " in Windows straight to my Linux files.", "tokens": [294, 8591, 2997, 281, 452, 18734, 7098, 13], "temperature": 0.0, "avg_logprob": -0.14236284672528848, "compression_ratio": 1.6322869955156951, "no_speech_prob": 9.516157660982572e-06}, {"id": 590, "seek": 243622, "start": 2451.9599999999996, "end": 2455.56, "text": " Not really something you have to worry about on Mac because it's all kind of integrated", "tokens": [1726, 534, 746, 291, 362, 281, 3292, 466, 322, 5707, 570, 309, 311, 439, 733, 295, 10919], "temperature": 0.0, "avg_logprob": -0.14236284672528848, "compression_ratio": 1.6322869955156951, "no_speech_prob": 9.516157660982572e-06}, {"id": 591, "seek": 243622, "start": 2455.56, "end": 2461.04, "text": " but on Windows they're like kind of like two separate machines.", "tokens": [457, 322, 8591, 436, 434, 411, 733, 295, 411, 732, 4994, 8379, 13], "temperature": 0.0, "avg_logprob": -0.14236284672528848, "compression_ratio": 1.6322869955156951, "no_speech_prob": 9.516157660982572e-06}, {"id": 592, "seek": 246104, "start": 2461.04, "end": 2472.6, "text": " Okay so let's do, so I created a space called testing and I downloaded my model.pickle and", "tokens": [1033, 370, 718, 311, 360, 11, 370, 286, 2942, 257, 1901, 1219, 4997, 293, 286, 21748, 452, 2316, 13, 79, 618, 306, 293], "temperature": 0.0, "avg_logprob": -0.11207645856417142, "compression_ratio": 1.44, "no_speech_prob": 4.860416993324179e-06}, {"id": 593, "seek": 246104, "start": 2472.6, "end": 2475.4, "text": " I pasted it into testing.", "tokens": [286, 1791, 292, 309, 666, 4997, 13], "temperature": 0.0, "avg_logprob": -0.11207645856417142, "compression_ratio": 1.44, "no_speech_prob": 4.860416993324179e-06}, {"id": 594, "seek": 246104, "start": 2475.4, "end": 2483.7599999999998, "text": " So now we need to know how do we do predictions on a saved model.", "tokens": [407, 586, 321, 643, 281, 458, 577, 360, 321, 360, 21264, 322, 257, 6624, 2316, 13], "temperature": 0.0, "avg_logprob": -0.11207645856417142, "compression_ratio": 1.44, "no_speech_prob": 4.860416993324179e-06}, {"id": 595, "seek": 246104, "start": 2483.7599999999998, "end": 2487.44, "text": " So we've got a notebook for that.", "tokens": [407, 321, 600, 658, 257, 21060, 337, 300, 13], "temperature": 0.0, "avg_logprob": -0.11207645856417142, "compression_ratio": 1.44, "no_speech_prob": 4.860416993324179e-06}, {"id": 596, "seek": 248744, "start": 2487.44, "end": 2492.56, "text": " Okay so we've got a notebook for that and so I'm going to take you through how we use", "tokens": [1033, 370, 321, 600, 658, 257, 21060, 337, 300, 293, 370, 286, 478, 516, 281, 747, 291, 807, 577, 321, 764], "temperature": 0.0, "avg_logprob": -0.17193889617919922, "compression_ratio": 1.5676855895196506, "no_speech_prob": 2.8130073133070255e-06}, {"id": 597, "seek": 248744, "start": 2492.56, "end": 2496.2000000000003, "text": " a model that we've trained to make predictions.", "tokens": [257, 2316, 300, 321, 600, 8895, 281, 652, 21264, 13], "temperature": 0.0, "avg_logprob": -0.17193889617919922, "compression_ratio": 1.5676855895196506, "no_speech_prob": 2.8130073133070255e-06}, {"id": 598, "seek": 248744, "start": 2496.2000000000003, "end": 2501.36, "text": " There's a few funny things with hash pipe which I'll explain in a moment just ignore", "tokens": [821, 311, 257, 1326, 4074, 721, 365, 22019, 11240, 597, 286, 603, 2903, 294, 257, 1623, 445, 11200], "temperature": 0.0, "avg_logprob": -0.17193889617919922, "compression_ratio": 1.5676855895196506, "no_speech_prob": 2.8130073133070255e-06}, {"id": 599, "seek": 248744, "start": 2501.36, "end": 2502.96, "text": " those for now.", "tokens": [729, 337, 586, 13], "temperature": 0.0, "avg_logprob": -0.17193889617919922, "compression_ratio": 1.5676855895196506, "no_speech_prob": 2.8130073133070255e-06}, {"id": 600, "seek": 248744, "start": 2502.96, "end": 2510.44, "text": " So we import fastai as usual, we import radio as we did before and we copy in the exact", "tokens": [407, 321, 974, 2370, 1301, 382, 7713, 11, 321, 974, 6477, 382, 321, 630, 949, 293, 321, 5055, 294, 264, 1900], "temperature": 0.0, "avg_logprob": -0.17193889617919922, "compression_ratio": 1.5676855895196506, "no_speech_prob": 2.8130073133070255e-06}, {"id": 601, "seek": 248744, "start": 2510.44, "end": 2513.48, "text": " same is cat definition we had before.", "tokens": [912, 307, 3857, 7123, 321, 632, 949, 13], "temperature": 0.0, "avg_logprob": -0.17193889617919922, "compression_ratio": 1.5676855895196506, "no_speech_prob": 2.8130073133070255e-06}, {"id": 602, "seek": 251348, "start": 2513.48, "end": 2519.76, "text": " It's important any external functions that you used in your labeling need to be included", "tokens": [467, 311, 1021, 604, 8320, 6828, 300, 291, 1143, 294, 428, 40244, 643, 281, 312, 5556], "temperature": 0.0, "avg_logprob": -0.1453037889380204, "compression_ratio": 1.6414141414141414, "no_speech_prob": 5.173881163500482e-06}, {"id": 603, "seek": 251348, "start": 2519.76, "end": 2523.76, "text": " here as well because that learner refers to those functions.", "tokens": [510, 382, 731, 570, 300, 33347, 14942, 281, 729, 6828, 13], "temperature": 0.0, "avg_logprob": -0.1453037889380204, "compression_ratio": 1.6414141414141414, "no_speech_prob": 5.173881163500482e-06}, {"id": 604, "seek": 251348, "start": 2523.76, "end": 2528.68, "text": " Okay it saves that learner saved everything about your model but it doesn't have the source", "tokens": [1033, 309, 19155, 300, 33347, 6624, 1203, 466, 428, 2316, 457, 309, 1177, 380, 362, 264, 4009], "temperature": 0.0, "avg_logprob": -0.1453037889380204, "compression_ratio": 1.6414141414141414, "no_speech_prob": 5.173881163500482e-06}, {"id": 605, "seek": 251348, "start": 2528.68, "end": 2532.5, "text": " code to the function so you need to keep those with you.", "tokens": [3089, 281, 264, 2445, 370, 291, 643, 281, 1066, 729, 365, 291, 13], "temperature": 0.0, "avg_logprob": -0.1453037889380204, "compression_ratio": 1.6414141414141414, "no_speech_prob": 5.173881163500482e-06}, {"id": 606, "seek": 251348, "start": 2532.5, "end": 2534.7, "text": " So let's try running this.", "tokens": [407, 718, 311, 853, 2614, 341, 13], "temperature": 0.0, "avg_logprob": -0.1453037889380204, "compression_ratio": 1.6414141414141414, "no_speech_prob": 5.173881163500482e-06}, {"id": 607, "seek": 253470, "start": 2534.7, "end": 2543.8199999999997, "text": " So for example I just grabbed as you might have seen in my explorer I just popped a dog", "tokens": [407, 337, 1365, 286, 445, 18607, 382, 291, 1062, 362, 1612, 294, 452, 39680, 286, 445, 21545, 257, 3000], "temperature": 0.0, "avg_logprob": -0.13705488768490878, "compression_ratio": 1.588785046728972, "no_speech_prob": 4.965278321833466e-07}, {"id": 608, "seek": 253470, "start": 2543.8199999999997, "end": 2553.96, "text": " picture there and so we can create a python image library image from that dog, turn it", "tokens": [3036, 456, 293, 370, 321, 393, 1884, 257, 38797, 3256, 6405, 3256, 490, 300, 3000, 11, 1261, 309], "temperature": 0.0, "avg_logprob": -0.13705488768490878, "compression_ratio": 1.588785046728972, "no_speech_prob": 4.965278321833466e-07}, {"id": 609, "seek": 253470, "start": 2553.96, "end": 2556.8799999999997, "text": " into a slightly smaller one so it doesn't overwhelm our whole screen and there is a", "tokens": [666, 257, 4748, 4356, 472, 370, 309, 1177, 380, 9103, 76, 527, 1379, 2568, 293, 456, 307, 257], "temperature": 0.0, "avg_logprob": -0.13705488768490878, "compression_ratio": 1.588785046728972, "no_speech_prob": 4.965278321833466e-07}, {"id": 610, "seek": 253470, "start": 2556.8799999999997, "end": 2558.7, "text": " picture of a dog.", "tokens": [3036, 295, 257, 3000, 13], "temperature": 0.0, "avg_logprob": -0.13705488768490878, "compression_ratio": 1.588785046728972, "no_speech_prob": 4.965278321833466e-07}, {"id": 611, "seek": 253470, "start": 2558.7, "end": 2561.7599999999998, "text": " So how do we make predictions of whether that's a dog or a cat.", "tokens": [407, 577, 360, 321, 652, 21264, 295, 1968, 300, 311, 257, 3000, 420, 257, 3857, 13], "temperature": 0.0, "avg_logprob": -0.13705488768490878, "compression_ratio": 1.588785046728972, "no_speech_prob": 4.965278321833466e-07}, {"id": 612, "seek": 256176, "start": 2561.76, "end": 2568.1200000000003, "text": " So it's very simple all we do is instead of training a learner we use load learner.", "tokens": [407, 309, 311, 588, 2199, 439, 321, 360, 307, 2602, 295, 3097, 257, 33347, 321, 764, 3677, 33347, 13], "temperature": 0.0, "avg_logprob": -0.1419824618919223, "compression_ratio": 1.8222222222222222, "no_speech_prob": 9.818245416681748e-06}, {"id": 613, "seek": 256176, "start": 2568.1200000000003, "end": 2572.36, "text": " We pass in the file name that we saved and that returns a learner.", "tokens": [492, 1320, 294, 264, 3991, 1315, 300, 321, 6624, 293, 300, 11247, 257, 33347, 13], "temperature": 0.0, "avg_logprob": -0.1419824618919223, "compression_ratio": 1.8222222222222222, "no_speech_prob": 9.818245416681748e-06}, {"id": 614, "seek": 256176, "start": 2572.36, "end": 2578.26, "text": " This learner is exactly the same as the learner you get when you finish training.", "tokens": [639, 33347, 307, 2293, 264, 912, 382, 264, 33347, 291, 483, 562, 291, 2413, 3097, 13], "temperature": 0.0, "avg_logprob": -0.1419824618919223, "compression_ratio": 1.8222222222222222, "no_speech_prob": 9.818245416681748e-06}, {"id": 615, "seek": 256176, "start": 2578.26, "end": 2583.2400000000002, "text": " So here we are his colab right we've just been training a learner so at the end of that", "tokens": [407, 510, 321, 366, 702, 1173, 455, 558, 321, 600, 445, 668, 3097, 257, 33347, 370, 412, 264, 917, 295, 300], "temperature": 0.0, "avg_logprob": -0.1419824618919223, "compression_ratio": 1.8222222222222222, "no_speech_prob": 9.818245416681748e-06}, {"id": 616, "seek": 256176, "start": 2583.2400000000002, "end": 2589.6000000000004, "text": " there's a learner that's been trained and so we kind of froze it in time something called", "tokens": [456, 311, 257, 33347, 300, 311, 668, 8895, 293, 370, 321, 733, 295, 46077, 309, 294, 565, 746, 1219], "temperature": 0.0, "avg_logprob": -0.1419824618919223, "compression_ratio": 1.8222222222222222, "no_speech_prob": 9.818245416681748e-06}, {"id": 617, "seek": 258960, "start": 2589.6, "end": 2593.52, "text": " a pickle file which is a python concept.", "tokens": [257, 31433, 3991, 597, 307, 257, 38797, 3410, 13], "temperature": 0.0, "avg_logprob": -0.22415622399777782, "compression_ratio": 1.6778846153846154, "no_speech_prob": 1.3006953849981073e-05}, {"id": 618, "seek": 258960, "start": 2593.52, "end": 2595.6, "text": " It's like a frozen object.", "tokens": [467, 311, 411, 257, 12496, 2657, 13], "temperature": 0.0, "avg_logprob": -0.22415622399777782, "compression_ratio": 1.6778846153846154, "no_speech_prob": 1.3006953849981073e-05}, {"id": 619, "seek": 258960, "start": 2595.6, "end": 2600.44, "text": " We saved it to disk we transferred it to our computer and we've now loaded it and we've", "tokens": [492, 6624, 309, 281, 12355, 321, 15809, 309, 281, 527, 3820, 293, 321, 600, 586, 13210, 309, 293, 321, 600], "temperature": 0.0, "avg_logprob": -0.22415622399777782, "compression_ratio": 1.6778846153846154, "no_speech_prob": 1.3006953849981073e-05}, {"id": 620, "seek": 258960, "start": 2600.44, "end": 2603.24, "text": " now un-thruth thought it.", "tokens": [586, 517, 12, 392, 81, 2910, 1194, 309, 13], "temperature": 0.0, "avg_logprob": -0.22415622399777782, "compression_ratio": 1.6778846153846154, "no_speech_prob": 1.3006953849981073e-05}, {"id": 621, "seek": 258960, "start": 2603.24, "end": 2606.44, "text": " Here's our unpickled learner and we can now do whatever we like with that.", "tokens": [1692, 311, 527, 20994, 618, 1493, 33347, 293, 321, 393, 586, 360, 2035, 321, 411, 365, 300, 13], "temperature": 0.0, "avg_logprob": -0.22415622399777782, "compression_ratio": 1.6778846153846154, "no_speech_prob": 1.3006953849981073e-05}, {"id": 622, "seek": 258960, "start": 2606.44, "end": 2614.74, "text": " So one of the things that the one of the methods that a learner has is a dot predict method.", "tokens": [407, 472, 295, 264, 721, 300, 264, 472, 295, 264, 7150, 300, 257, 33347, 575, 307, 257, 5893, 6069, 3170, 13], "temperature": 0.0, "avg_logprob": -0.22415622399777782, "compression_ratio": 1.6778846153846154, "no_speech_prob": 1.3006953849981073e-05}, {"id": 623, "seek": 261474, "start": 2614.74, "end": 2620.08, "text": " So if I run it you can see even on my laptop it's basically instance instant.", "tokens": [407, 498, 286, 1190, 309, 291, 393, 536, 754, 322, 452, 10732, 309, 311, 1936, 5197, 9836, 13], "temperature": 0.0, "avg_logprob": -0.22942781911312954, "compression_ratio": 1.6767241379310345, "no_speech_prob": 7.07183562553837e-06}, {"id": 624, "seek": 261474, "start": 2620.08, "end": 2622.9799999999996, "text": " In fact we can see how long it took.", "tokens": [682, 1186, 321, 393, 536, 577, 938, 309, 1890, 13], "temperature": 0.0, "avg_logprob": -0.22942781911312954, "compression_ratio": 1.6767241379310345, "no_speech_prob": 7.07183562553837e-06}, {"id": 625, "seek": 261474, "start": 2622.9799999999996, "end": 2628.8199999999997, "text": " If you in Jupiter things that start with percent are called magic's this special Jupiter things.", "tokens": [759, 291, 294, 24567, 721, 300, 722, 365, 3043, 366, 1219, 5585, 311, 341, 2121, 24567, 721, 13], "temperature": 0.0, "avg_logprob": -0.22942781911312954, "compression_ratio": 1.6767241379310345, "no_speech_prob": 7.07183562553837e-06}, {"id": 626, "seek": 261474, "start": 2628.8199999999997, "end": 2631.3199999999997, "text": " So for example there's a thing to see how long something takes.", "tokens": [407, 337, 1365, 456, 311, 257, 551, 281, 536, 577, 938, 746, 2516, 13], "temperature": 0.0, "avg_logprob": -0.22942781911312954, "compression_ratio": 1.6767241379310345, "no_speech_prob": 7.07183562553837e-06}, {"id": 627, "seek": 261474, "start": 2631.3199999999997, "end": 2632.7999999999997, "text": " There you go.", "tokens": [821, 291, 352, 13], "temperature": 0.0, "avg_logprob": -0.22942781911312954, "compression_ratio": 1.6767241379310345, "no_speech_prob": 7.07183562553837e-06}, {"id": 628, "seek": 261474, "start": 2632.7999999999997, "end": 2642.2799999999997, "text": " Okay so it took 54 milliseconds to figure out that this is not a cat.", "tokens": [1033, 370, 309, 1890, 20793, 34184, 281, 2573, 484, 300, 341, 307, 406, 257, 3857, 13], "temperature": 0.0, "avg_logprob": -0.22942781911312954, "compression_ratio": 1.6767241379310345, "no_speech_prob": 7.07183562553837e-06}, {"id": 629, "seek": 261474, "start": 2642.2799999999997, "end": 2643.2799999999997, "text": " So it's returning two things.", "tokens": [407, 309, 311, 12678, 732, 721, 13], "temperature": 0.0, "avg_logprob": -0.22942781911312954, "compression_ratio": 1.6767241379310345, "no_speech_prob": 7.07183562553837e-06}, {"id": 630, "seek": 264328, "start": 2643.28, "end": 2647.36, "text": " Is it a cat as a string.", "tokens": [1119, 309, 257, 3857, 382, 257, 6798, 13], "temperature": 0.0, "avg_logprob": -0.20108078266012258, "compression_ratio": 1.7926829268292683, "no_speech_prob": 2.2959086436458165e-06}, {"id": 631, "seek": 264328, "start": 2647.36, "end": 2654.5800000000004, "text": " Is it a cat as a zero or one and then the probability that it's a dog and the probability", "tokens": [1119, 309, 257, 3857, 382, 257, 4018, 420, 472, 293, 550, 264, 8482, 300, 309, 311, 257, 3000, 293, 264, 8482], "temperature": 0.0, "avg_logprob": -0.20108078266012258, "compression_ratio": 1.7926829268292683, "no_speech_prob": 2.2959086436458165e-06}, {"id": 632, "seek": 264328, "start": 2654.5800000000004, "end": 2655.5800000000004, "text": " that it's a cat.", "tokens": [300, 309, 311, 257, 3857, 13], "temperature": 0.0, "avg_logprob": -0.20108078266012258, "compression_ratio": 1.7926829268292683, "no_speech_prob": 2.2959086436458165e-06}, {"id": 633, "seek": 264328, "start": 2655.5800000000004, "end": 2659.28, "text": " So the probability of zero false and one true.", "tokens": [407, 264, 8482, 295, 4018, 7908, 293, 472, 2074, 13], "temperature": 0.0, "avg_logprob": -0.20108078266012258, "compression_ratio": 1.7926829268292683, "no_speech_prob": 2.2959086436458165e-06}, {"id": 634, "seek": 264328, "start": 2659.28, "end": 2660.48, "text": " Is it a cat.", "tokens": [1119, 309, 257, 3857, 13], "temperature": 0.0, "avg_logprob": -0.20108078266012258, "compression_ratio": 1.7926829268292683, "no_speech_prob": 2.2959086436458165e-06}, {"id": 635, "seek": 264328, "start": 2660.48, "end": 2662.36, "text": " So definitely a dog.", "tokens": [407, 2138, 257, 3000, 13], "temperature": 0.0, "avg_logprob": -0.20108078266012258, "compression_ratio": 1.7926829268292683, "no_speech_prob": 2.2959086436458165e-06}, {"id": 636, "seek": 264328, "start": 2662.36, "end": 2668.4, "text": " So we now want to create a Gradio interface which basically has this information.", "tokens": [407, 321, 586, 528, 281, 1884, 257, 16710, 1004, 9226, 597, 1936, 575, 341, 1589, 13], "temperature": 0.0, "avg_logprob": -0.20108078266012258, "compression_ratio": 1.7926829268292683, "no_speech_prob": 2.2959086436458165e-06}, {"id": 637, "seek": 266840, "start": 2668.4, "end": 2673.76, "text": " So Gradio requires us to give it a function that it's going to call.", "tokens": [407, 16710, 1004, 7029, 505, 281, 976, 309, 257, 2445, 300, 309, 311, 516, 281, 818, 13], "temperature": 0.0, "avg_logprob": -0.15925784777569515, "compression_ratio": 1.6985645933014355, "no_speech_prob": 2.0904478787997505e-06}, {"id": 638, "seek": 266840, "start": 2673.76, "end": 2675.64, "text": " So here's our function.", "tokens": [407, 510, 311, 527, 2445, 13], "temperature": 0.0, "avg_logprob": -0.15925784777569515, "compression_ratio": 1.6985645933014355, "no_speech_prob": 2.0904478787997505e-06}, {"id": 639, "seek": 266840, "start": 2675.64, "end": 2679.2400000000002, "text": " So we're going to call predict and that returns us we said three things.", "tokens": [407, 321, 434, 516, 281, 818, 6069, 293, 300, 11247, 505, 321, 848, 1045, 721, 13], "temperature": 0.0, "avg_logprob": -0.15925784777569515, "compression_ratio": 1.6985645933014355, "no_speech_prob": 2.0904478787997505e-06}, {"id": 640, "seek": 266840, "start": 2679.2400000000002, "end": 2685.6800000000003, "text": " The prediction is a string the index of that and the probabilities of whether it's a dog", "tokens": [440, 17630, 307, 257, 6798, 264, 8186, 295, 300, 293, 264, 33783, 295, 1968, 309, 311, 257, 3000], "temperature": 0.0, "avg_logprob": -0.15925784777569515, "compression_ratio": 1.6985645933014355, "no_speech_prob": 2.0904478787997505e-06}, {"id": 641, "seek": 266840, "start": 2685.6800000000003, "end": 2687.52, "text": " or a cat.", "tokens": [420, 257, 3857, 13], "temperature": 0.0, "avg_logprob": -0.15925784777569515, "compression_ratio": 1.6985645933014355, "no_speech_prob": 2.0904478787997505e-06}, {"id": 642, "seek": 266840, "start": 2687.52, "end": 2693.7200000000003, "text": " And what Gradio wants is it wants to get back a dictionary containing each of the possible", "tokens": [400, 437, 16710, 1004, 2738, 307, 309, 2738, 281, 483, 646, 257, 25890, 19273, 1184, 295, 264, 1944], "temperature": 0.0, "avg_logprob": -0.15925784777569515, "compression_ratio": 1.6985645933014355, "no_speech_prob": 2.0904478787997505e-06}, {"id": 643, "seek": 269372, "start": 2693.72, "end": 2699.4399999999996, "text": " categories which in this case is dog or cat and the probability of each one.", "tokens": [10479, 597, 294, 341, 1389, 307, 3000, 420, 3857, 293, 264, 8482, 295, 1184, 472, 13], "temperature": 0.0, "avg_logprob": -0.20166684180190883, "compression_ratio": 1.6059322033898304, "no_speech_prob": 5.68241239307099e-06}, {"id": 644, "seek": 269372, "start": 2699.4399999999996, "end": 2704.3199999999997, "text": " So if you haven't done much Python before a dict of a zip may be something you haven't", "tokens": [407, 498, 291, 2378, 380, 1096, 709, 15329, 949, 257, 12569, 295, 257, 20730, 815, 312, 746, 291, 2378, 380], "temperature": 0.0, "avg_logprob": -0.20166684180190883, "compression_ratio": 1.6059322033898304, "no_speech_prob": 5.68241239307099e-06}, {"id": 645, "seek": 269372, "start": 2704.3199999999997, "end": 2707.7999999999997, "text": " seen very handy little idiom well worth checking out.", "tokens": [1612, 588, 13239, 707, 18014, 298, 731, 3163, 8568, 484, 13], "temperature": 0.0, "avg_logprob": -0.20166684180190883, "compression_ratio": 1.6059322033898304, "no_speech_prob": 5.68241239307099e-06}, {"id": 646, "seek": 269372, "start": 2707.7999999999997, "end": 2710.68, "text": " Do you know if you haven't seen map before.", "tokens": [1144, 291, 458, 498, 291, 2378, 380, 1612, 4471, 949, 13], "temperature": 0.0, "avg_logprob": -0.20166684180190883, "compression_ratio": 1.6059322033898304, "no_speech_prob": 5.68241239307099e-06}, {"id": 647, "seek": 269372, "start": 2710.68, "end": 2717.4399999999996, "text": " Anyway here it is one slightly annoying thing about Gradio at the moment is that it doesn't", "tokens": [5684, 510, 309, 307, 472, 4748, 11304, 551, 466, 16710, 1004, 412, 264, 1623, 307, 300, 309, 1177, 380], "temperature": 0.0, "avg_logprob": -0.20166684180190883, "compression_ratio": 1.6059322033898304, "no_speech_prob": 5.68241239307099e-06}, {"id": 648, "seek": 269372, "start": 2717.4399999999996, "end": 2721.08, "text": " handle pie torch tensors.", "tokens": [4813, 1730, 27822, 10688, 830, 13], "temperature": 0.0, "avg_logprob": -0.20166684180190883, "compression_ratio": 1.6059322033898304, "no_speech_prob": 5.68241239307099e-06}, {"id": 649, "seek": 272108, "start": 2721.08, "end": 2724.96, "text": " You can see here pie torch is not returning normal numbers it's returning tensors it's", "tokens": [509, 393, 536, 510, 1730, 27822, 307, 406, 12678, 2710, 3547, 309, 311, 12678, 10688, 830, 309, 311], "temperature": 0.0, "avg_logprob": -0.16044668249181798, "compression_ratio": 1.7201646090534979, "no_speech_prob": 4.8604115363559686e-06}, {"id": 650, "seek": 272108, "start": 2724.96, "end": 2727.12, "text": " not even returning numpy arrays.", "tokens": [406, 754, 12678, 1031, 8200, 41011, 13], "temperature": 0.0, "avg_logprob": -0.16044668249181798, "compression_ratio": 1.7201646090534979, "no_speech_prob": 4.8604115363559686e-06}, {"id": 651, "seek": 272108, "start": 2727.12, "end": 2729.3199999999997, "text": " In fact Gradio can't handle numpy either.", "tokens": [682, 1186, 16710, 1004, 393, 380, 4813, 1031, 8200, 2139, 13], "temperature": 0.0, "avg_logprob": -0.16044668249181798, "compression_ratio": 1.7201646090534979, "no_speech_prob": 4.8604115363559686e-06}, {"id": 652, "seek": 272108, "start": 2729.3199999999997, "end": 2731.44, "text": " So we have to change everything just to a normal float.", "tokens": [407, 321, 362, 281, 1319, 1203, 445, 281, 257, 2710, 15706, 13], "temperature": 0.0, "avg_logprob": -0.16044668249181798, "compression_ratio": 1.7201646090534979, "no_speech_prob": 4.8604115363559686e-06}, {"id": 653, "seek": 272108, "start": 2731.44, "end": 2734.64, "text": " So that's all that this is doing is changing each one to a float.", "tokens": [407, 300, 311, 439, 300, 341, 307, 884, 307, 4473, 1184, 472, 281, 257, 15706, 13], "temperature": 0.0, "avg_logprob": -0.16044668249181798, "compression_ratio": 1.7201646090534979, "no_speech_prob": 4.8604115363559686e-06}, {"id": 654, "seek": 272108, "start": 2734.64, "end": 2741.16, "text": " So for example if I now call classify image with our doggy image we get back a dictionary", "tokens": [407, 337, 1365, 498, 286, 586, 818, 33872, 3256, 365, 527, 3000, 1480, 3256, 321, 483, 646, 257, 25890], "temperature": 0.0, "avg_logprob": -0.16044668249181798, "compression_ratio": 1.7201646090534979, "no_speech_prob": 4.8604115363559686e-06}, {"id": 655, "seek": 272108, "start": 2741.16, "end": 2742.16, "text": " of a dog.", "tokens": [295, 257, 3000, 13], "temperature": 0.0, "avg_logprob": -0.16044668249181798, "compression_ratio": 1.7201646090534979, "no_speech_prob": 4.8604115363559686e-06}, {"id": 656, "seek": 272108, "start": 2742.16, "end": 2745.48, "text": " Yes definitely cat definitely not.", "tokens": [1079, 2138, 3857, 2138, 406, 13], "temperature": 0.0, "avg_logprob": -0.16044668249181798, "compression_ratio": 1.7201646090534979, "no_speech_prob": 4.8604115363559686e-06}, {"id": 657, "seek": 274548, "start": 2745.48, "end": 2752.44, "text": " So now we've got all that we can go ahead and create a Gradio interface so Gradio interface", "tokens": [407, 586, 321, 600, 658, 439, 300, 321, 393, 352, 2286, 293, 1884, 257, 16710, 1004, 9226, 370, 16710, 1004, 9226], "temperature": 0.0, "avg_logprob": -0.15288846246127424, "compression_ratio": 1.8, "no_speech_prob": 3.3405206067982363e-06}, {"id": 658, "seek": 274548, "start": 2752.44, "end": 2757.08, "text": " is something where we say well what function do you call to get the output.", "tokens": [307, 746, 689, 321, 584, 731, 437, 2445, 360, 291, 818, 281, 483, 264, 5598, 13], "temperature": 0.0, "avg_logprob": -0.15288846246127424, "compression_ratio": 1.8, "no_speech_prob": 3.3405206067982363e-06}, {"id": 659, "seek": 274548, "start": 2757.08, "end": 2762.0, "text": " What is the input in this case we say oh the input is an image and so check out the Gradio", "tokens": [708, 307, 264, 4846, 294, 341, 1389, 321, 584, 1954, 264, 4846, 307, 364, 3256, 293, 370, 1520, 484, 264, 16710, 1004], "temperature": 0.0, "avg_logprob": -0.15288846246127424, "compression_ratio": 1.8, "no_speech_prob": 3.3405206067982363e-06}, {"id": 660, "seek": 274548, "start": 2762.0, "end": 2767.7, "text": " docs it can be all kinds of things like a webcam picture or text or you know all kinds", "tokens": [45623, 309, 393, 312, 439, 3685, 295, 721, 411, 257, 39490, 3036, 420, 2487, 420, 291, 458, 439, 3685], "temperature": 0.0, "avg_logprob": -0.15288846246127424, "compression_ratio": 1.8, "no_speech_prob": 3.3405206067982363e-06}, {"id": 661, "seek": 274548, "start": 2767.7, "end": 2768.7, "text": " of things.", "tokens": [295, 721, 13], "temperature": 0.0, "avg_logprob": -0.15288846246127424, "compression_ratio": 1.8, "no_speech_prob": 3.3405206067982363e-06}, {"id": 662, "seek": 274548, "start": 2768.7, "end": 2772.08, "text": " Give it a shape that it's going to put it into the outputs just going to be a label.", "tokens": [5303, 309, 257, 3909, 300, 309, 311, 516, 281, 829, 309, 666, 264, 23930, 445, 516, 281, 312, 257, 7645, 13], "temperature": 0.0, "avg_logprob": -0.15288846246127424, "compression_ratio": 1.8, "no_speech_prob": 3.3405206067982363e-06}, {"id": 663, "seek": 277208, "start": 2772.08, "end": 2779.12, "text": " So we're going to create very very simple interface and we can also provide some examples", "tokens": [407, 321, 434, 516, 281, 1884, 588, 588, 2199, 9226, 293, 321, 393, 611, 2893, 512, 5110], "temperature": 0.0, "avg_logprob": -0.1924039715918425, "compression_ratio": 1.676991150442478, "no_speech_prob": 1.165940921055153e-05}, {"id": 664, "seek": 277208, "start": 2779.12, "end": 2784.12, "text": " and so there's a dog a cat and a don't know which I'll do about in a moment which you'll", "tokens": [293, 370, 456, 311, 257, 3000, 257, 3857, 293, 257, 500, 380, 458, 597, 286, 603, 360, 466, 294, 257, 1623, 597, 291, 603], "temperature": 0.0, "avg_logprob": -0.1924039715918425, "compression_ratio": 1.676991150442478, "no_speech_prob": 1.165940921055153e-05}, {"id": 665, "seek": 277208, "start": 2784.12, "end": 2787.84, "text": " see here there's a dog and a cat and a don't know.", "tokens": [536, 510, 456, 311, 257, 3000, 293, 257, 3857, 293, 257, 500, 380, 458, 13], "temperature": 0.0, "avg_logprob": -0.1924039715918425, "compression_ratio": 1.676991150442478, "no_speech_prob": 1.165940921055153e-05}, {"id": 666, "seek": 277208, "start": 2787.84, "end": 2791.66, "text": " So once I launch it it says OK that's now running on this URL.", "tokens": [407, 1564, 286, 4025, 309, 309, 1619, 2264, 300, 311, 586, 2614, 322, 341, 12905, 13], "temperature": 0.0, "avg_logprob": -0.1924039715918425, "compression_ratio": 1.676991150442478, "no_speech_prob": 1.165940921055153e-05}, {"id": 667, "seek": 277208, "start": 2791.66, "end": 2799.84, "text": " So if I open that up you can see now we have just like Souvash we have our own not yet", "tokens": [407, 498, 286, 1269, 300, 493, 291, 393, 536, 586, 321, 362, 445, 411, 318, 22941, 1299, 321, 362, 527, 1065, 406, 1939], "temperature": 0.0, "avg_logprob": -0.1924039715918425, "compression_ratio": 1.676991150442478, "no_speech_prob": 1.165940921055153e-05}, {"id": 668, "seek": 279984, "start": 2799.84, "end": 2804.48, "text": " in production but running on our own box classifier.", "tokens": [294, 4265, 457, 2614, 322, 527, 1065, 2424, 1508, 9902, 13], "temperature": 0.0, "avg_logprob": -0.26526844718239523, "compression_ratio": 1.576470588235294, "no_speech_prob": 2.947998837043997e-06}, {"id": 669, "seek": 279984, "start": 2804.48, "end": 2809.4, "text": " So let's check dog so you can click and upload one or just choose the examples.", "tokens": [407, 718, 311, 1520, 3000, 370, 291, 393, 2052, 293, 6580, 472, 420, 445, 2826, 264, 5110, 13], "temperature": 0.0, "avg_logprob": -0.26526844718239523, "compression_ratio": 1.576470588235294, "no_speech_prob": 2.947998837043997e-06}, {"id": 670, "seek": 279984, "start": 2809.4, "end": 2816.7200000000003, "text": " Yeah yeah so it's running on my own laptop basically instant and I really have to tell", "tokens": [865, 1338, 370, 309, 311, 2614, 322, 452, 1065, 10732, 1936, 9836, 293, 286, 534, 362, 281, 980], "temperature": 0.0, "avg_logprob": -0.26526844718239523, "compression_ratio": 1.576470588235294, "no_speech_prob": 2.947998837043997e-06}, {"id": 671, "seek": 279984, "start": 2816.7200000000003, "end": 2820.28, "text": " you the story about this guy here.", "tokens": [291, 264, 1657, 466, 341, 2146, 510, 13], "temperature": 0.0, "avg_logprob": -0.26526844718239523, "compression_ratio": 1.576470588235294, "no_speech_prob": 2.947998837043997e-06}, {"id": 672, "seek": 279984, "start": 2820.28, "end": 2822.2400000000002, "text": " This is the don't know submit.", "tokens": [639, 307, 264, 500, 380, 458, 10315, 13], "temperature": 0.0, "avg_logprob": -0.26526844718239523, "compression_ratio": 1.576470588235294, "no_speech_prob": 2.947998837043997e-06}, {"id": 673, "seek": 279984, "start": 2822.2400000000002, "end": 2826.36, "text": " Wait why is it saying 100 normally this says like 50 50.", "tokens": [3802, 983, 307, 309, 1566, 2319, 5646, 341, 1619, 411, 2625, 2625, 13], "temperature": 0.0, "avg_logprob": -0.26526844718239523, "compression_ratio": 1.576470588235294, "no_speech_prob": 2.947998837043997e-06}, {"id": 674, "seek": 279984, "start": 2826.36, "end": 2827.36, "text": " That's a bummer.", "tokens": [663, 311, 257, 13309, 936, 13], "temperature": 0.0, "avg_logprob": -0.26526844718239523, "compression_ratio": 1.576470588235294, "no_speech_prob": 2.947998837043997e-06}, {"id": 675, "seek": 279984, "start": 2827.36, "end": 2829.78, "text": " This model's got messed up my whole story.", "tokens": [639, 2316, 311, 658, 16507, 493, 452, 1379, 1657, 13], "temperature": 0.0, "avg_logprob": -0.26526844718239523, "compression_ratio": 1.576470588235294, "no_speech_prob": 2.947998837043997e-06}, {"id": 676, "seek": 282978, "start": 2829.78, "end": 2834.8, "text": " The last time I trained this model and I ran it on the don't know it said it said like", "tokens": [440, 1036, 565, 286, 8895, 341, 2316, 293, 286, 5872, 309, 322, 264, 500, 380, 458, 309, 848, 309, 848, 411], "temperature": 0.0, "avg_logprob": -0.2092309725486626, "compression_ratio": 1.6741573033707866, "no_speech_prob": 1.4738709978701081e-05}, {"id": 677, "seek": 282978, "start": 2834.8, "end": 2843.2400000000002, "text": " it's almost exactly 50 50 and the way we found this picture is I showed my six year old daughter", "tokens": [309, 311, 1920, 2293, 2625, 2625, 293, 264, 636, 321, 1352, 341, 3036, 307, 286, 4712, 452, 2309, 1064, 1331, 4653], "temperature": 0.0, "avg_logprob": -0.2092309725486626, "compression_ratio": 1.6741573033707866, "no_speech_prob": 1.4738709978701081e-05}, {"id": 678, "seek": 282978, "start": 2843.2400000000002, "end": 2846.0800000000004, "text": " she's like what are you doing dad's like I'm coding what are you coding.", "tokens": [750, 311, 411, 437, 366, 291, 884, 3546, 311, 411, 286, 478, 17720, 437, 366, 291, 17720, 13], "temperature": 0.0, "avg_logprob": -0.2092309725486626, "compression_ratio": 1.6741573033707866, "no_speech_prob": 1.4738709978701081e-05}, {"id": 679, "seek": 282978, "start": 2846.0800000000004, "end": 2848.4, "text": " Oh you know dog cat classifier.", "tokens": [876, 291, 458, 3000, 3857, 1508, 9902, 13], "temperature": 0.0, "avg_logprob": -0.2092309725486626, "compression_ratio": 1.6741573033707866, "no_speech_prob": 1.4738709978701081e-05}, {"id": 680, "seek": 282978, "start": 2848.4, "end": 2852.7200000000003, "text": " She checks it out and her first question is can I take your keyboard for a moment and", "tokens": [1240, 13834, 309, 484, 293, 720, 700, 1168, 307, 393, 286, 747, 428, 10186, 337, 257, 1623, 293], "temperature": 0.0, "avg_logprob": -0.2092309725486626, "compression_ratio": 1.6741573033707866, "no_speech_prob": 1.4738709978701081e-05}, {"id": 681, "seek": 282978, "start": 2852.7200000000003, "end": 2857.48, "text": " she goes to Google and she's like what is a dog mixed with a cat called.", "tokens": [750, 1709, 281, 3329, 293, 750, 311, 411, 437, 307, 257, 3000, 7467, 365, 257, 3857, 1219, 13], "temperature": 0.0, "avg_logprob": -0.2092309725486626, "compression_ratio": 1.6741573033707866, "no_speech_prob": 1.4738709978701081e-05}, {"id": 682, "seek": 285748, "start": 2857.48, "end": 2860.4, "text": " There's no such thing as a dog mix with a cat.", "tokens": [821, 311, 572, 1270, 551, 382, 257, 3000, 2890, 365, 257, 3857, 13], "temperature": 0.0, "avg_logprob": -0.19534956519283467, "compression_ratio": 1.7451737451737452, "no_speech_prob": 8.139582860167138e-06}, {"id": 683, "seek": 285748, "start": 2860.4, "end": 2864.96, "text": " Anyway she goes to the images tab and finds this picture and she's like look there's a", "tokens": [5684, 750, 1709, 281, 264, 5267, 4421, 293, 10704, 341, 3036, 293, 750, 311, 411, 574, 456, 311, 257], "temperature": 0.0, "avg_logprob": -0.19534956519283467, "compression_ratio": 1.7451737451737452, "no_speech_prob": 8.139582860167138e-06}, {"id": 684, "seek": 285748, "start": 2864.96, "end": 2867.6, "text": " dog mixed with a cat.", "tokens": [3000, 7467, 365, 257, 3857, 13], "temperature": 0.0, "avg_logprob": -0.19534956519283467, "compression_ratio": 1.7451737451737452, "no_speech_prob": 8.139582860167138e-06}, {"id": 685, "seek": 285748, "start": 2867.6, "end": 2873.32, "text": " She said run it on that dad run it on that and I ran it and it was like 50 50 it had", "tokens": [1240, 848, 1190, 309, 322, 300, 3546, 1190, 309, 322, 300, 293, 286, 5872, 309, 293, 309, 390, 411, 2625, 2625, 309, 632], "temperature": 0.0, "avg_logprob": -0.19534956519283467, "compression_ratio": 1.7451737451737452, "no_speech_prob": 8.139582860167138e-06}, {"id": 686, "seek": 285748, "start": 2873.32, "end": 2875.52, "text": " no idea if it was a dog or a cat.", "tokens": [572, 1558, 498, 309, 390, 257, 3000, 420, 257, 3857, 13], "temperature": 0.0, "avg_logprob": -0.19534956519283467, "compression_ratio": 1.7451737451737452, "no_speech_prob": 8.139582860167138e-06}, {"id": 687, "seek": 285748, "start": 2875.52, "end": 2879.52, "text": " Now this model I just retrained today and I was sure it's a cat.", "tokens": [823, 341, 2316, 286, 445, 1533, 31774, 965, 293, 286, 390, 988, 309, 311, 257, 3857, 13], "temperature": 0.0, "avg_logprob": -0.19534956519283467, "compression_ratio": 1.7451737451737452, "no_speech_prob": 8.139582860167138e-06}, {"id": 688, "seek": 285748, "start": 2879.52, "end": 2880.52, "text": " So there you go.", "tokens": [407, 456, 291, 352, 13], "temperature": 0.0, "avg_logprob": -0.19534956519283467, "compression_ratio": 1.7451737451737452, "no_speech_prob": 8.139582860167138e-06}, {"id": 689, "seek": 285748, "start": 2880.52, "end": 2883.64, "text": " I think I used a slightly different training schedule or something or I gave it an extra", "tokens": [286, 519, 286, 1143, 257, 4748, 819, 3097, 7567, 420, 746, 420, 286, 2729, 309, 364, 2857], "temperature": 0.0, "avg_logprob": -0.19534956519283467, "compression_ratio": 1.7451737451737452, "no_speech_prob": 8.139582860167138e-06}, {"id": 690, "seek": 285748, "start": 2883.64, "end": 2885.44, "text": " epoch.", "tokens": [30992, 339, 13], "temperature": 0.0, "avg_logprob": -0.19534956519283467, "compression_ratio": 1.7451737451737452, "no_speech_prob": 8.139582860167138e-06}, {"id": 691, "seek": 288544, "start": 2885.44, "end": 2890.4, "text": " Anyway so that's a dog cat but apparently it's a cat.", "tokens": [5684, 370, 300, 311, 257, 3000, 3857, 457, 7970, 309, 311, 257, 3857, 13], "temperature": 0.0, "avg_logprob": -0.18341841787662147, "compression_ratio": 1.6919642857142858, "no_speech_prob": 1.2805212463717908e-05}, {"id": 692, "seek": 288544, "start": 2890.4, "end": 2892.7200000000003, "text": " I guess it is a cat.", "tokens": [286, 2041, 309, 307, 257, 3857, 13], "temperature": 0.0, "avg_logprob": -0.18341841787662147, "compression_ratio": 1.6919642857142858, "no_speech_prob": 1.2805212463717908e-05}, {"id": 693, "seek": 288544, "start": 2892.7200000000003, "end": 2893.7200000000003, "text": " It's probably right.", "tokens": [467, 311, 1391, 558, 13], "temperature": 0.0, "avg_logprob": -0.18341841787662147, "compression_ratio": 1.6919642857142858, "no_speech_prob": 1.2805212463717908e-05}, {"id": 694, "seek": 288544, "start": 2893.7200000000003, "end": 2896.36, "text": " I shouldn't have trained it for as long.", "tokens": [286, 4659, 380, 362, 8895, 309, 337, 382, 938, 13], "temperature": 0.0, "avg_logprob": -0.18341841787662147, "compression_ratio": 1.6919642857142858, "no_speech_prob": 1.2805212463717908e-05}, {"id": 695, "seek": 288544, "start": 2896.36, "end": 2897.96, "text": " Okay so there's our interface.", "tokens": [1033, 370, 456, 311, 527, 9226, 13], "temperature": 0.0, "avg_logprob": -0.18341841787662147, "compression_ratio": 1.6919642857142858, "no_speech_prob": 1.2805212463717908e-05}, {"id": 696, "seek": 288544, "start": 2897.96, "end": 2901.56, "text": " Now that's actually running so you actually have to click the stop button to stop it running", "tokens": [823, 300, 311, 767, 2614, 370, 291, 767, 362, 281, 2052, 264, 1590, 2960, 281, 1590, 309, 2614], "temperature": 0.0, "avg_logprob": -0.18341841787662147, "compression_ratio": 1.6919642857142858, "no_speech_prob": 1.2805212463717908e-05}, {"id": 697, "seek": 288544, "start": 2901.56, "end": 2909.04, "text": " so otherwise you won't be able to do anything else in your notebook.", "tokens": [370, 5911, 291, 1582, 380, 312, 1075, 281, 360, 1340, 1646, 294, 428, 21060, 13], "temperature": 0.0, "avg_logprob": -0.18341841787662147, "compression_ratio": 1.6919642857142858, "no_speech_prob": 1.2805212463717908e-05}, {"id": 698, "seek": 288544, "start": 2909.04, "end": 2913.9, "text": " So now we have to turn that into a Python script.", "tokens": [407, 586, 321, 362, 281, 1261, 300, 666, 257, 15329, 5755, 13], "temperature": 0.0, "avg_logprob": -0.18341841787662147, "compression_ratio": 1.6919642857142858, "no_speech_prob": 1.2805212463717908e-05}, {"id": 699, "seek": 291390, "start": 2913.9, "end": 2918.56, "text": " So one way to turn it into a Python script would be to copy and paste into a Python script", "tokens": [407, 472, 636, 281, 1261, 309, 666, 257, 15329, 5755, 576, 312, 281, 5055, 293, 9163, 666, 257, 15329, 5755], "temperature": 0.0, "avg_logprob": -0.14201919494136686, "compression_ratio": 2.0, "no_speech_prob": 2.9022921808063984e-06}, {"id": 700, "seek": 291390, "start": 2918.56, "end": 2921.04, "text": " all the things that you need.", "tokens": [439, 264, 721, 300, 291, 643, 13], "temperature": 0.0, "avg_logprob": -0.14201919494136686, "compression_ratio": 2.0, "no_speech_prob": 2.9022921808063984e-06}, {"id": 701, "seek": 291390, "start": 2921.04, "end": 2926.0, "text": " I read a copy and paste into a Python script all those parts of this that you need.", "tokens": [286, 1401, 257, 5055, 293, 9163, 666, 257, 15329, 5755, 439, 729, 3166, 295, 341, 300, 291, 643, 13], "temperature": 0.0, "avg_logprob": -0.14201919494136686, "compression_ratio": 2.0, "no_speech_prob": 2.9022921808063984e-06}, {"id": 702, "seek": 291390, "start": 2926.0, "end": 2929.12, "text": " So for example we wouldn't need this is just to check something out.", "tokens": [407, 337, 1365, 321, 2759, 380, 643, 341, 307, 445, 281, 1520, 746, 484, 13], "temperature": 0.0, "avg_logprob": -0.14201919494136686, "compression_ratio": 2.0, "no_speech_prob": 2.9022921808063984e-06}, {"id": 703, "seek": 291390, "start": 2929.12, "end": 2930.12, "text": " We wouldn't need this.", "tokens": [492, 2759, 380, 643, 341, 13], "temperature": 0.0, "avg_logprob": -0.14201919494136686, "compression_ratio": 2.0, "no_speech_prob": 2.9022921808063984e-06}, {"id": 704, "seek": 291390, "start": 2930.12, "end": 2931.96, "text": " It was just experimenting.", "tokens": [467, 390, 445, 29070, 13], "temperature": 0.0, "avg_logprob": -0.14201919494136686, "compression_ratio": 2.0, "no_speech_prob": 2.9022921808063984e-06}, {"id": 705, "seek": 291390, "start": 2931.96, "end": 2933.44, "text": " This was just experimenting.", "tokens": [639, 390, 445, 29070, 13], "temperature": 0.0, "avg_logprob": -0.14201919494136686, "compression_ratio": 2.0, "no_speech_prob": 2.9022921808063984e-06}, {"id": 706, "seek": 291390, "start": 2933.44, "end": 2935.2400000000002, "text": " We'd need this right.", "tokens": [492, 1116, 643, 341, 558, 13], "temperature": 0.0, "avg_logprob": -0.14201919494136686, "compression_ratio": 2.0, "no_speech_prob": 2.9022921808063984e-06}, {"id": 707, "seek": 291390, "start": 2935.2400000000002, "end": 2942.64, "text": " So what I did is I went through and I wrote hash pipe export at the top of each cell that", "tokens": [407, 437, 286, 630, 307, 286, 1437, 807, 293, 286, 4114, 22019, 11240, 10725, 412, 264, 1192, 295, 1184, 2815, 300], "temperature": 0.0, "avg_logprob": -0.14201919494136686, "compression_ratio": 2.0, "no_speech_prob": 2.9022921808063984e-06}, {"id": 708, "seek": 294264, "start": 2942.64, "end": 2949.2799999999997, "text": " contains information that I'm going to need in my final script and then so there are the", "tokens": [8306, 1589, 300, 286, 478, 516, 281, 643, 294, 452, 2572, 5755, 293, 550, 370, 456, 366, 264], "temperature": 0.0, "avg_logprob": -0.17126774787902832, "compression_ratio": 1.6458333333333333, "no_speech_prob": 2.156808477593586e-06}, {"id": 709, "seek": 294264, "start": 2949.2799999999997, "end": 2954.6, "text": " steps right and then at the very bottom here I've imported something called notebook to", "tokens": [4439, 558, 293, 550, 412, 264, 588, 2767, 510, 286, 600, 25524, 746, 1219, 21060, 281], "temperature": 0.0, "avg_logprob": -0.17126774787902832, "compression_ratio": 1.6458333333333333, "no_speech_prob": 2.156808477593586e-06}, {"id": 710, "seek": 294264, "start": 2954.6, "end": 2962.8799999999997, "text": " script from NB dev and if I run that and pass in the name of this notebook that creates", "tokens": [5755, 490, 426, 33, 1905, 293, 498, 286, 1190, 300, 293, 1320, 294, 264, 1315, 295, 341, 21060, 300, 7829], "temperature": 0.0, "avg_logprob": -0.17126774787902832, "compression_ratio": 1.6458333333333333, "no_speech_prob": 2.156808477593586e-06}, {"id": 711, "seek": 294264, "start": 2962.8799999999997, "end": 2971.52, "text": " a file for me called app.py containing that script.", "tokens": [257, 3991, 337, 385, 1219, 724, 13, 8200, 19273, 300, 5755, 13], "temperature": 0.0, "avg_logprob": -0.17126774787902832, "compression_ratio": 1.6458333333333333, "no_speech_prob": 2.156808477593586e-06}, {"id": 712, "seek": 297152, "start": 2971.52, "end": 2975.84, "text": " So this is a nice easy way to like when you're working with stuff that's expecting a script", "tokens": [407, 341, 307, 257, 1481, 1858, 636, 281, 411, 562, 291, 434, 1364, 365, 1507, 300, 311, 9650, 257, 5755], "temperature": 0.0, "avg_logprob": -0.12471807325208509, "compression_ratio": 1.6867469879518073, "no_speech_prob": 2.3320651507674484e-06}, {"id": 713, "seek": 297152, "start": 2975.84, "end": 2981.2, "text": " and not a notebook like hugging face spaces does it's fine to just copy and paste into", "tokens": [293, 406, 257, 21060, 411, 41706, 1851, 7673, 775, 309, 311, 2489, 281, 445, 5055, 293, 9163, 666], "temperature": 0.0, "avg_logprob": -0.12471807325208509, "compression_ratio": 1.6867469879518073, "no_speech_prob": 2.3320651507674484e-06}, {"id": 714, "seek": 297152, "start": 2981.2, "end": 2984.88, "text": " a text file if you like but I really like this way of doing it because that way I can", "tokens": [257, 2487, 3991, 498, 291, 411, 457, 286, 534, 411, 341, 636, 295, 884, 309, 570, 300, 636, 286, 393], "temperature": 0.0, "avg_logprob": -0.12471807325208509, "compression_ratio": 1.6867469879518073, "no_speech_prob": 2.3320651507674484e-06}, {"id": 715, "seek": 297152, "start": 2984.88, "end": 2990.92, "text": " do all of my experimentation in a notebook and when I'm done I just have a cell at the", "tokens": [360, 439, 295, 452, 37142, 294, 257, 21060, 293, 562, 286, 478, 1096, 286, 445, 362, 257, 2815, 412, 264], "temperature": 0.0, "avg_logprob": -0.12471807325208509, "compression_ratio": 1.6867469879518073, "no_speech_prob": 2.3320651507674484e-06}, {"id": 716, "seek": 297152, "start": 2990.92, "end": 2994.28, "text": " bottom I just run and export it.", "tokens": [2767, 286, 445, 1190, 293, 10725, 309, 13], "temperature": 0.0, "avg_logprob": -0.12471807325208509, "compression_ratio": 1.6867469879518073, "no_speech_prob": 2.3320651507674484e-06}, {"id": 717, "seek": 297152, "start": 2994.28, "end": 2996.64, "text": " How does it know to call it app.py.", "tokens": [1012, 775, 309, 458, 281, 818, 309, 724, 13, 8200, 13], "temperature": 0.0, "avg_logprob": -0.12471807325208509, "compression_ratio": 1.6867469879518073, "no_speech_prob": 2.3320651507674484e-06}, {"id": 718, "seek": 299664, "start": 2996.64, "end": 3001.52, "text": " That's because there's a special thing at the top default export default x which says", "tokens": [663, 311, 570, 456, 311, 257, 2121, 551, 412, 264, 1192, 7576, 10725, 7576, 2031, 597, 1619], "temperature": 0.0, "avg_logprob": -0.1832929426623929, "compression_ratio": 1.5645933014354068, "no_speech_prob": 1.3007026609557215e-05}, {"id": 719, "seek": 299664, "start": 3001.52, "end": 3004.16, "text": " what python file name to create.", "tokens": [437, 38797, 3991, 1315, 281, 1884, 13], "temperature": 0.0, "avg_logprob": -0.1832929426623929, "compression_ratio": 1.5645933014354068, "no_speech_prob": 1.3007026609557215e-05}, {"id": 720, "seek": 299664, "start": 3004.16, "end": 3009.04, "text": " So that's just a little trick that I use.", "tokens": [407, 300, 311, 445, 257, 707, 4282, 300, 286, 764, 13], "temperature": 0.0, "avg_logprob": -0.1832929426623929, "compression_ratio": 1.5645933014354068, "no_speech_prob": 1.3007026609557215e-05}, {"id": 721, "seek": 299664, "start": 3009.04, "end": 3016.2799999999997, "text": " So now we've got an app.py we need to upload this to Gradio.", "tokens": [407, 586, 321, 600, 658, 364, 724, 13, 8200, 321, 643, 281, 6580, 341, 281, 16710, 1004, 13], "temperature": 0.0, "avg_logprob": -0.1832929426623929, "compression_ratio": 1.5645933014354068, "no_speech_prob": 1.3007026609557215e-05}, {"id": 722, "seek": 299664, "start": 3016.2799999999997, "end": 3017.68, "text": " How do we do that.", "tokens": [1012, 360, 321, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.1832929426623929, "compression_ratio": 1.5645933014354068, "no_speech_prob": 1.3007026609557215e-05}, {"id": 723, "seek": 299664, "start": 3017.68, "end": 3022.52, "text": " You just you just push it to get so in you can either do it with Visual Studio code or", "tokens": [509, 445, 291, 445, 2944, 309, 281, 483, 370, 294, 291, 393, 2139, 360, 309, 365, 23187, 13500, 3089, 420], "temperature": 0.0, "avg_logprob": -0.1832929426623929, "compression_ratio": 1.5645933014354068, "no_speech_prob": 1.3007026609557215e-05}, {"id": 724, "seek": 302252, "start": 3022.52, "end": 3032.44, "text": " you can type commit and then get push and once you've done that.", "tokens": [291, 393, 2010, 5599, 293, 550, 483, 2944, 293, 1564, 291, 600, 1096, 300, 13], "temperature": 0.0, "avg_logprob": -0.21087420858987949, "compression_ratio": 1.5260416666666667, "no_speech_prob": 3.6688384170702193e-06}, {"id": 725, "seek": 302252, "start": 3032.44, "end": 3040.7599999999998, "text": " If we change minimal to testing.", "tokens": [759, 321, 1319, 13206, 281, 4997, 13], "temperature": 0.0, "avg_logprob": -0.21087420858987949, "compression_ratio": 1.5260416666666667, "no_speech_prob": 3.6688384170702193e-06}, {"id": 726, "seek": 302252, "start": 3040.7599999999998, "end": 3043.96, "text": " I think this hopefully might still be running my previous model because I didn't push it", "tokens": [286, 519, 341, 4696, 1062, 920, 312, 2614, 452, 3894, 2316, 570, 286, 994, 380, 2944, 309], "temperature": 0.0, "avg_logprob": -0.21087420858987949, "compression_ratio": 1.5260416666666667, "no_speech_prob": 3.6688384170702193e-06}, {"id": 727, "seek": 302252, "start": 3043.96, "end": 3047.12, "text": " and that way we can see our crazy dog cat.", "tokens": [293, 300, 636, 321, 393, 536, 527, 3219, 3000, 3857, 13], "temperature": 0.0, "avg_logprob": -0.21087420858987949, "compression_ratio": 1.5260416666666667, "no_speech_prob": 3.6688384170702193e-06}, {"id": 728, "seek": 302252, "start": 3047.12, "end": 3048.12, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.21087420858987949, "compression_ratio": 1.5260416666666667, "no_speech_prob": 3.6688384170702193e-06}, {"id": 729, "seek": 302252, "start": 3048.12, "end": 3049.12, "text": " So here it is.", "tokens": [407, 510, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.21087420858987949, "compression_ratio": 1.5260416666666667, "no_speech_prob": 3.6688384170702193e-06}, {"id": 730, "seek": 302252, "start": 3049.12, "end": 3051.56, "text": " You can see it running in production.", "tokens": [509, 393, 536, 309, 2614, 294, 4265, 13], "temperature": 0.0, "avg_logprob": -0.21087420858987949, "compression_ratio": 1.5260416666666667, "no_speech_prob": 3.6688384170702193e-06}, {"id": 731, "seek": 305156, "start": 3051.56, "end": 3055.0, "text": " So now this is something that anybody can if you set it to public anybody can go here", "tokens": [407, 586, 341, 307, 746, 300, 4472, 393, 498, 291, 992, 309, 281, 1908, 4472, 393, 352, 510], "temperature": 0.0, "avg_logprob": -0.328627438015408, "compression_ratio": 1.6844919786096257, "no_speech_prob": 6.854229923192179e-06}, {"id": 732, "seek": 305156, "start": 3055.0, "end": 3060.6, "text": " and check out your model and so they can upload it and so here's my doggy.", "tokens": [293, 1520, 484, 428, 2316, 293, 370, 436, 393, 6580, 309, 293, 370, 510, 311, 452, 3000, 1480, 13], "temperature": 0.0, "avg_logprob": -0.328627438015408, "compression_ratio": 1.6844919786096257, "no_speech_prob": 6.854229923192179e-06}, {"id": 733, "seek": 305156, "start": 3060.6, "end": 3065.08, "text": " Yep definitely a dog cat.", "tokens": [7010, 2138, 257, 3000, 3857, 13], "temperature": 0.0, "avg_logprob": -0.328627438015408, "compression_ratio": 1.6844919786096257, "no_speech_prob": 6.854229923192179e-06}, {"id": 734, "seek": 305156, "start": 3065.08, "end": 3069.0, "text": " Yeah I think I'm going to train this for a epoch or two less so it's less confident.", "tokens": [865, 286, 519, 286, 478, 516, 281, 3847, 341, 337, 257, 30992, 339, 420, 732, 1570, 370, 309, 311, 1570, 6679, 13], "temperature": 0.0, "avg_logprob": -0.328627438015408, "compression_ratio": 1.6844919786096257, "no_speech_prob": 6.854229923192179e-06}, {"id": 735, "seek": 305156, "start": 3069.0, "end": 3076.04, "text": " Yeah definitely a cat dog cat.", "tokens": [865, 2138, 257, 3857, 3000, 3857, 13], "temperature": 0.0, "avg_logprob": -0.328627438015408, "compression_ratio": 1.6844919786096257, "no_speech_prob": 6.854229923192179e-06}, {"id": 736, "seek": 305156, "start": 3076.04, "end": 3078.04, "text": " Hey dog cat.", "tokens": [1911, 3000, 3857, 13], "temperature": 0.0, "avg_logprob": -0.328627438015408, "compression_ratio": 1.6844919786096257, "no_speech_prob": 6.854229923192179e-06}, {"id": 737, "seek": 307804, "start": 3078.04, "end": 3083.04, "text": " Hmm still thinks it's definitely a cat.", "tokens": [8239, 920, 7309, 309, 311, 2138, 257, 3857, 13], "temperature": 0.0, "avg_logprob": -0.37207551435990766, "compression_ratio": 1.3148148148148149, "no_speech_prob": 1.112544759962475e-05}, {"id": 738, "seek": 307804, "start": 3083.04, "end": 3086.44, "text": " Oh well so be it.", "tokens": [876, 731, 370, 312, 309, 13], "temperature": 0.0, "avg_logprob": -0.37207551435990766, "compression_ratio": 1.3148148148148149, "no_speech_prob": 1.112544759962475e-05}, {"id": 739, "seek": 307804, "start": 3086.44, "end": 3099.08, "text": " Okay so that is.", "tokens": [1033, 370, 300, 307, 13], "temperature": 0.0, "avg_logprob": -0.37207551435990766, "compression_ratio": 1.3148148148148149, "no_speech_prob": 1.112544759962475e-05}, {"id": 740, "seek": 307804, "start": 3099.08, "end": 3103.24, "text": " Okay so that is an example of getting a simple model in production.", "tokens": [1033, 370, 300, 307, 364, 1365, 295, 1242, 257, 2199, 2316, 294, 4265, 13], "temperature": 0.0, "avg_logprob": -0.37207551435990766, "compression_ratio": 1.3148148148148149, "no_speech_prob": 1.112544759962475e-05}, {"id": 741, "seek": 310324, "start": 3103.24, "end": 3109.3999999999996, "text": " There's a couple of questions from the forum from the community.", "tokens": [821, 311, 257, 1916, 295, 1651, 490, 264, 17542, 490, 264, 1768, 13], "temperature": 0.0, "avg_logprob": -0.23090031147003173, "compression_ratio": 1.5263157894736843, "no_speech_prob": 1.9525465177139267e-05}, {"id": 742, "seek": 310324, "start": 3109.3999999999996, "end": 3117.68, "text": " Okay so one person's asking what's the difference between a pie torch model and a fast AI learner.", "tokens": [1033, 370, 472, 954, 311, 3365, 437, 311, 264, 2649, 1296, 257, 1730, 27822, 2316, 293, 257, 2370, 7318, 33347, 13], "temperature": 0.0, "avg_logprob": -0.23090031147003173, "compression_ratio": 1.5263157894736843, "no_speech_prob": 1.9525465177139267e-05}, {"id": 743, "seek": 310324, "start": 3117.68, "end": 3118.68, "text": " Okay that's fine.", "tokens": [1033, 300, 311, 2489, 13], "temperature": 0.0, "avg_logprob": -0.23090031147003173, "compression_ratio": 1.5263157894736843, "no_speech_prob": 1.9525465177139267e-05}, {"id": 744, "seek": 310324, "start": 3118.68, "end": 3119.68, "text": " We will get to that shortly.", "tokens": [492, 486, 483, 281, 300, 13392, 13], "temperature": 0.0, "avg_logprob": -0.23090031147003173, "compression_ratio": 1.5263157894736843, "no_speech_prob": 1.9525465177139267e-05}, {"id": 745, "seek": 310324, "start": 3119.68, "end": 3122.2799999999997, "text": " Don't know if it'll be that lesson.", "tokens": [1468, 380, 458, 498, 309, 603, 312, 300, 6898, 13], "temperature": 0.0, "avg_logprob": -0.23090031147003173, "compression_ratio": 1.5263157894736843, "no_speech_prob": 1.9525465177139267e-05}, {"id": 746, "seek": 310324, "start": 3122.2799999999997, "end": 3130.3599999999997, "text": " It might be this lesson or the next lesson.", "tokens": [467, 1062, 312, 341, 6898, 420, 264, 958, 6898, 13], "temperature": 0.0, "avg_logprob": -0.23090031147003173, "compression_ratio": 1.5263157894736843, "no_speech_prob": 1.9525465177139267e-05}, {"id": 747, "seek": 313036, "start": 3130.36, "end": 3134.92, "text": " And then somebody else asked basically is asking how many epochs do we train for.", "tokens": [400, 550, 2618, 1646, 2351, 1936, 307, 3365, 577, 867, 30992, 28346, 360, 321, 3847, 337, 13], "temperature": 0.0, "avg_logprob": -0.19363278248271004, "compression_ratio": 1.7228915662650603, "no_speech_prob": 6.540350113937166e-06}, {"id": 748, "seek": 313036, "start": 3134.92, "end": 3142.32, "text": " So as you train a model your error rate as you can see it improves.", "tokens": [407, 382, 291, 3847, 257, 2316, 428, 6713, 3314, 382, 291, 393, 536, 309, 24771, 13], "temperature": 0.0, "avg_logprob": -0.19363278248271004, "compression_ratio": 1.7228915662650603, "no_speech_prob": 6.540350113937166e-06}, {"id": 749, "seek": 313036, "start": 3142.32, "end": 3144.2000000000003, "text": " And so the question is should I run more.", "tokens": [400, 370, 264, 1168, 307, 820, 286, 1190, 544, 13], "temperature": 0.0, "avg_logprob": -0.19363278248271004, "compression_ratio": 1.7228915662650603, "no_speech_prob": 6.540350113937166e-06}, {"id": 750, "seek": 313036, "start": 3144.2000000000003, "end": 3146.6400000000003, "text": " Should I increase the number of epochs.", "tokens": [6454, 286, 3488, 264, 1230, 295, 30992, 28346, 13], "temperature": 0.0, "avg_logprob": -0.19363278248271004, "compression_ratio": 1.7228915662650603, "no_speech_prob": 6.540350113937166e-06}, {"id": 751, "seek": 313036, "start": 3146.6400000000003, "end": 3148.1600000000003, "text": " This is doing three epochs right.", "tokens": [639, 307, 884, 1045, 30992, 28346, 558, 13], "temperature": 0.0, "avg_logprob": -0.19363278248271004, "compression_ratio": 1.7228915662650603, "no_speech_prob": 6.540350113937166e-06}, {"id": 752, "seek": 313036, "start": 3148.1600000000003, "end": 3153.1200000000003, "text": " Here's my three epochs plus one to get started.", "tokens": [1692, 311, 452, 1045, 30992, 28346, 1804, 472, 281, 483, 1409, 13], "temperature": 0.0, "avg_logprob": -0.19363278248271004, "compression_ratio": 1.7228915662650603, "no_speech_prob": 6.540350113937166e-06}, {"id": 753, "seek": 313036, "start": 3153.1200000000003, "end": 3154.28, "text": " Look it's up to you right.", "tokens": [2053, 309, 311, 493, 281, 291, 558, 13], "temperature": 0.0, "avg_logprob": -0.19363278248271004, "compression_ratio": 1.7228915662650603, "no_speech_prob": 6.540350113937166e-06}, {"id": 754, "seek": 313036, "start": 3154.28, "end": 3158.04, "text": " I mean this is here saying there's a 1 percent error.", "tokens": [286, 914, 341, 307, 510, 1566, 456, 311, 257, 502, 3043, 6713, 13], "temperature": 0.0, "avg_logprob": -0.19363278248271004, "compression_ratio": 1.7228915662650603, "no_speech_prob": 6.540350113937166e-06}, {"id": 755, "seek": 313036, "start": 3158.04, "end": 3159.48, "text": " I'm okay with the 1 percent error.", "tokens": [286, 478, 1392, 365, 264, 502, 3043, 6713, 13], "temperature": 0.0, "avg_logprob": -0.19363278248271004, "compression_ratio": 1.7228915662650603, "no_speech_prob": 6.540350113937166e-06}, {"id": 756, "seek": 315948, "start": 3159.48, "end": 3163.28, "text": " You know if you want it to be better then you could use more data augmentation and you", "tokens": [509, 458, 498, 291, 528, 309, 281, 312, 1101, 550, 291, 727, 764, 544, 1412, 14501, 19631, 293, 291], "temperature": 0.0, "avg_logprob": -0.15061566323945016, "compression_ratio": 1.8071748878923768, "no_speech_prob": 7.183173238445306e-06}, {"id": 757, "seek": 315948, "start": 3163.28, "end": 3166.28, "text": " could train it for longer.", "tokens": [727, 3847, 309, 337, 2854, 13], "temperature": 0.0, "avg_logprob": -0.15061566323945016, "compression_ratio": 1.8071748878923768, "no_speech_prob": 7.183173238445306e-06}, {"id": 758, "seek": 315948, "start": 3166.28, "end": 3173.2400000000002, "text": " If you train for long enough as we'll learn about soon in the next maybe the next lesson", "tokens": [759, 291, 3847, 337, 938, 1547, 382, 321, 603, 1466, 466, 2321, 294, 264, 958, 1310, 264, 958, 6898], "temperature": 0.0, "avg_logprob": -0.15061566323945016, "compression_ratio": 1.8071748878923768, "no_speech_prob": 7.183173238445306e-06}, {"id": 759, "seek": 315948, "start": 3173.2400000000002, "end": 3177.84, "text": " if you train for long enough your error rate actually starts getting worse and you'll see", "tokens": [498, 291, 3847, 337, 938, 1547, 428, 6713, 3314, 767, 3719, 1242, 5324, 293, 291, 603, 536], "temperature": 0.0, "avg_logprob": -0.15061566323945016, "compression_ratio": 1.8071748878923768, "no_speech_prob": 7.183173238445306e-06}, {"id": 760, "seek": 315948, "start": 3177.84, "end": 3179.64, "text": " we learn about why.", "tokens": [321, 1466, 466, 983, 13], "temperature": 0.0, "avg_logprob": -0.15061566323945016, "compression_ratio": 1.8071748878923768, "no_speech_prob": 7.183173238445306e-06}, {"id": 761, "seek": 315948, "start": 3179.64, "end": 3184.84, "text": " So basically yeah you can train until it's good enough or until you've run out of patience", "tokens": [407, 1936, 1338, 291, 393, 3847, 1826, 309, 311, 665, 1547, 420, 1826, 291, 600, 1190, 484, 295, 14826], "temperature": 0.0, "avg_logprob": -0.15061566323945016, "compression_ratio": 1.8071748878923768, "no_speech_prob": 7.183173238445306e-06}, {"id": 762, "seek": 318484, "start": 3184.84, "end": 3194.48, "text": " or time or run out of compute or until you or until the error rate starts getting worse.", "tokens": [420, 565, 420, 1190, 484, 295, 14722, 420, 1826, 291, 420, 1826, 264, 6713, 3314, 3719, 1242, 5324, 13], "temperature": 0.0, "avg_logprob": -0.17410947406102742, "compression_ratio": 1.5161290322580645, "no_speech_prob": 7.766912858642172e-06}, {"id": 763, "seek": 318484, "start": 3194.48, "end": 3201.2000000000003, "text": " Okay oh and then in Colab how do you grab your model.", "tokens": [1033, 1954, 293, 550, 294, 4004, 455, 577, 360, 291, 4444, 428, 2316, 13], "temperature": 0.0, "avg_logprob": -0.17410947406102742, "compression_ratio": 1.5161290322580645, "no_speech_prob": 7.766912858642172e-06}, {"id": 764, "seek": 318484, "start": 3201.2000000000003, "end": 3210.88, "text": " All you need to do in Colab is after you've exported it is if you go into their file browser", "tokens": [1057, 291, 643, 281, 360, 294, 4004, 455, 307, 934, 291, 600, 42055, 309, 307, 498, 291, 352, 666, 641, 3991, 11185], "temperature": 0.0, "avg_logprob": -0.17410947406102742, "compression_ratio": 1.5161290322580645, "no_speech_prob": 7.766912858642172e-06}, {"id": 765, "seek": 321088, "start": 3210.88, "end": 3216.48, "text": " you'll actually see it here right and you can click download.", "tokens": [291, 603, 767, 536, 309, 510, 558, 293, 291, 393, 2052, 5484, 13], "temperature": 0.0, "avg_logprob": -0.12537025193036613, "compression_ratio": 1.765625, "no_speech_prob": 9.080380550585687e-06}, {"id": 766, "seek": 321088, "start": 3216.48, "end": 3219.6800000000003, "text": " It's a bit weird.", "tokens": [467, 311, 257, 857, 3657, 13], "temperature": 0.0, "avg_logprob": -0.12537025193036613, "compression_ratio": 1.765625, "no_speech_prob": 9.080380550585687e-06}, {"id": 767, "seek": 321088, "start": 3219.6800000000003, "end": 3224.04, "text": " It doesn't like pop up a box saying where do you want to download it to but instead", "tokens": [467, 1177, 380, 411, 1665, 493, 257, 2424, 1566, 689, 360, 291, 528, 281, 5484, 309, 281, 457, 2602], "temperature": 0.0, "avg_logprob": -0.12537025193036613, "compression_ratio": 1.765625, "no_speech_prob": 9.080380550585687e-06}, {"id": 768, "seek": 321088, "start": 3224.04, "end": 3229.28, "text": " this kind of progress circle thing pops up and so depending on how big it is and so forth", "tokens": [341, 733, 295, 4205, 6329, 551, 16795, 493, 293, 370, 5413, 322, 577, 955, 309, 307, 293, 370, 5220], "temperature": 0.0, "avg_logprob": -0.12537025193036613, "compression_ratio": 1.765625, "no_speech_prob": 9.080380550585687e-06}, {"id": 769, "seek": 321088, "start": 3229.28, "end": 3234.28, "text": " it can take a few minutes and once that circle fills up then it'll the browser thing will", "tokens": [309, 393, 747, 257, 1326, 2077, 293, 1564, 300, 6329, 22498, 493, 550, 309, 603, 264, 11185, 551, 486], "temperature": 0.0, "avg_logprob": -0.12537025193036613, "compression_ratio": 1.765625, "no_speech_prob": 9.080380550585687e-06}, {"id": 770, "seek": 321088, "start": 3234.28, "end": 3236.76, "text": " finally pop up and say okay you can save it.", "tokens": [2721, 1665, 493, 293, 584, 1392, 291, 393, 3155, 309, 13], "temperature": 0.0, "avg_logprob": -0.12537025193036613, "compression_ratio": 1.765625, "no_speech_prob": 9.080380550585687e-06}, {"id": 771, "seek": 321088, "start": 3236.76, "end": 3239.4, "text": " Okay so that's how you that's how you actually grab your model.", "tokens": [1033, 370, 300, 311, 577, 291, 300, 311, 577, 291, 767, 4444, 428, 2316, 13], "temperature": 0.0, "avg_logprob": -0.12537025193036613, "compression_ratio": 1.765625, "no_speech_prob": 9.080380550585687e-06}, {"id": 772, "seek": 323940, "start": 3239.4, "end": 3243.56, "text": " So as you can see that the step where you actually need a GPU you can use these totally", "tokens": [407, 382, 291, 393, 536, 300, 264, 1823, 689, 291, 767, 643, 257, 18407, 291, 393, 764, 613, 3879], "temperature": 0.0, "avg_logprob": -0.15989167873676008, "compression_ratio": 1.6785714285714286, "no_speech_prob": 5.173877525521675e-06}, {"id": 773, "seek": 323940, "start": 3243.56, "end": 3250.44, "text": " free resources Colab Kaggle and there are other ones we'll talk about in future lessons", "tokens": [1737, 3593, 4004, 455, 48751, 22631, 293, 456, 366, 661, 2306, 321, 603, 751, 466, 294, 2027, 8820], "temperature": 0.0, "avg_logprob": -0.15989167873676008, "compression_ratio": 1.6785714285714286, "no_speech_prob": 5.173877525521675e-06}, {"id": 774, "seek": 323940, "start": 3250.44, "end": 3253.64, "text": " and then you can do everything else on your own computer including the predictions.", "tokens": [293, 550, 291, 393, 360, 1203, 1646, 322, 428, 1065, 3820, 3009, 264, 21264, 13], "temperature": 0.0, "avg_logprob": -0.15989167873676008, "compression_ratio": 1.6785714285714286, "no_speech_prob": 5.173877525521675e-06}, {"id": 775, "seek": 323940, "start": 3253.64, "end": 3259.08, "text": " The predictions are fast right so you really don't need to use a GPU for that unless you're", "tokens": [440, 21264, 366, 2370, 558, 370, 291, 534, 500, 380, 643, 281, 764, 257, 18407, 337, 300, 5969, 291, 434], "temperature": 0.0, "avg_logprob": -0.15989167873676008, "compression_ratio": 1.6785714285714286, "no_speech_prob": 5.173877525521675e-06}, {"id": 776, "seek": 323940, "start": 3259.08, "end": 3260.08, "text": " doing thousands of them.", "tokens": [884, 5383, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.15989167873676008, "compression_ratio": 1.6785714285714286, "no_speech_prob": 5.173877525521675e-06}, {"id": 777, "seek": 323940, "start": 3260.08, "end": 3266.6, "text": " Okay here we go now it's asking me to save it.", "tokens": [1033, 510, 321, 352, 586, 309, 311, 3365, 385, 281, 3155, 309, 13], "temperature": 0.0, "avg_logprob": -0.15989167873676008, "compression_ratio": 1.6785714285714286, "no_speech_prob": 5.173877525521675e-06}, {"id": 778, "seek": 326660, "start": 3266.6, "end": 3274.56, "text": " Okay so now one big issue is we needed to run it on our computer we needed Python and", "tokens": [1033, 370, 586, 472, 955, 2734, 307, 321, 2978, 281, 1190, 309, 322, 527, 3820, 321, 2978, 15329, 293], "temperature": 0.0, "avg_logprob": -0.15051286165104355, "compression_ratio": 1.6192660550458715, "no_speech_prob": 6.438947366405046e-06}, {"id": 779, "seek": 326660, "start": 3274.56, "end": 3276.92, "text": " Jupyter notebooks running on our computer.", "tokens": [22125, 88, 391, 43782, 2614, 322, 527, 3820, 13], "temperature": 0.0, "avg_logprob": -0.15051286165104355, "compression_ratio": 1.6192660550458715, "no_speech_prob": 6.438947366405046e-06}, {"id": 780, "seek": 326660, "start": 3276.92, "end": 3287.2, "text": " So how do you do that because this is where often people get in all kinds of trouble trying", "tokens": [407, 577, 360, 291, 360, 300, 570, 341, 307, 689, 2049, 561, 483, 294, 439, 3685, 295, 5253, 1382], "temperature": 0.0, "avg_logprob": -0.15051286165104355, "compression_ratio": 1.6192660550458715, "no_speech_prob": 6.438947366405046e-06}, {"id": 781, "seek": 326660, "start": 3287.2, "end": 3289.56, "text": " to figure out how to get this all working.", "tokens": [281, 2573, 484, 577, 281, 483, 341, 439, 1364, 13], "temperature": 0.0, "avg_logprob": -0.15051286165104355, "compression_ratio": 1.6192660550458715, "no_speech_prob": 6.438947366405046e-06}, {"id": 782, "seek": 326660, "start": 3289.56, "end": 3293.72, "text": " So the good news is we've actually got something that makes it very very straightforward.", "tokens": [407, 264, 665, 2583, 307, 321, 600, 767, 658, 746, 300, 1669, 309, 588, 588, 15325, 13], "temperature": 0.0, "avg_logprob": -0.15051286165104355, "compression_ratio": 1.6192660550458715, "no_speech_prob": 6.438947366405046e-06}, {"id": 783, "seek": 329372, "start": 3293.72, "end": 3298.04, "text": " It's called Fast Setup there's really only just one part of it you need.", "tokens": [467, 311, 1219, 15968, 8928, 1010, 456, 311, 534, 787, 445, 472, 644, 295, 309, 291, 643, 13], "temperature": 0.0, "avg_logprob": -0.20416295528411865, "compression_ratio": 1.76890756302521, "no_speech_prob": 1.8924798496300355e-05}, {"id": 784, "seek": 329372, "start": 3298.04, "end": 3303.24, "text": " So let me show you it's it's actually a Git repository on GitHub.", "tokens": [407, 718, 385, 855, 291, 309, 311, 309, 311, 767, 257, 16939, 25841, 322, 23331, 13], "temperature": 0.0, "avg_logprob": -0.20416295528411865, "compression_ratio": 1.76890756302521, "no_speech_prob": 1.8924798496300355e-05}, {"id": 785, "seek": 329372, "start": 3303.24, "end": 3306.9599999999996, "text": " Github's the place where most Git repositories live.", "tokens": [460, 355, 836, 311, 264, 1081, 689, 881, 16939, 22283, 2083, 1621, 13], "temperature": 0.0, "avg_logprob": -0.20416295528411865, "compression_ratio": 1.76890756302521, "no_speech_prob": 1.8924798496300355e-05}, {"id": 786, "seek": 329372, "start": 3306.9599999999996, "end": 3310.56, "text": " So if you go to GitHub Fast AI Fast Setup you'll see it.", "tokens": [407, 498, 291, 352, 281, 23331, 15968, 7318, 15968, 8928, 1010, 291, 603, 536, 309, 13], "temperature": 0.0, "avg_logprob": -0.20416295528411865, "compression_ratio": 1.76890756302521, "no_speech_prob": 1.8924798496300355e-05}, {"id": 787, "seek": 329372, "start": 3310.56, "end": 3317.3799999999997, "text": " And so what you can do is you can now grab this whole repository just by clicking here", "tokens": [400, 370, 437, 291, 393, 360, 307, 291, 393, 586, 4444, 341, 1379, 25841, 445, 538, 9697, 510], "temperature": 0.0, "avg_logprob": -0.20416295528411865, "compression_ratio": 1.76890756302521, "no_speech_prob": 1.8924798496300355e-05}, {"id": 788, "seek": 329372, "start": 3317.3799999999997, "end": 3323.52, "text": " on code and if you've got GitHub desktop installed click on open with GitHub desktop.", "tokens": [322, 3089, 293, 498, 291, 600, 658, 23331, 14502, 8899, 2052, 322, 1269, 365, 23331, 14502, 13], "temperature": 0.0, "avg_logprob": -0.20416295528411865, "compression_ratio": 1.76890756302521, "no_speech_prob": 1.8924798496300355e-05}, {"id": 789, "seek": 332352, "start": 3323.52, "end": 3329.68, "text": " And as you'll see it brings this up saying okay I'm ready to save this for you so I'll", "tokens": [400, 382, 291, 603, 536, 309, 5607, 341, 493, 1566, 1392, 286, 478, 1919, 281, 3155, 341, 337, 291, 370, 286, 603], "temperature": 0.0, "avg_logprob": -0.19462409700666156, "compression_ratio": 1.464968152866242, "no_speech_prob": 3.28872079080611e-06}, {"id": 790, "seek": 332352, "start": 3329.68, "end": 3330.68, "text": " click clone.", "tokens": [2052, 26506, 13], "temperature": 0.0, "avg_logprob": -0.19462409700666156, "compression_ratio": 1.464968152866242, "no_speech_prob": 3.28872079080611e-06}, {"id": 791, "seek": 332352, "start": 3330.68, "end": 3334.92, "text": " So it's making a copy of it.", "tokens": [407, 309, 311, 1455, 257, 5055, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.19462409700666156, "compression_ratio": 1.464968152866242, "no_speech_prob": 3.28872079080611e-06}, {"id": 792, "seek": 332352, "start": 3334.92, "end": 3340.08, "text": " There we go.", "tokens": [821, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.19462409700666156, "compression_ratio": 1.464968152866242, "no_speech_prob": 3.28872079080611e-06}, {"id": 793, "seek": 332352, "start": 3340.08, "end": 3350.0, "text": " So basically once you've cloned it you'll then find there's a file in there called setup", "tokens": [407, 1936, 1564, 291, 600, 596, 19009, 309, 291, 603, 550, 915, 456, 311, 257, 3991, 294, 456, 1219, 8657], "temperature": 0.0, "avg_logprob": -0.19462409700666156, "compression_ratio": 1.464968152866242, "no_speech_prob": 3.28872079080611e-06}, {"id": 794, "seek": 335000, "start": 3350.0, "end": 3356.8, "text": " conda dot sh which you know the details don't really matter it's pretty short but that's", "tokens": [2224, 64, 5893, 402, 597, 291, 458, 264, 4365, 500, 380, 534, 1871, 309, 311, 1238, 2099, 457, 300, 311], "temperature": 0.0, "avg_logprob": -0.12435052890588741, "compression_ratio": 1.7092511013215859, "no_speech_prob": 3.7266011077008443e-06}, {"id": 795, "seek": 335000, "start": 3356.8, "end": 3359.92, "text": " the thing that's going to install Python for you.", "tokens": [264, 551, 300, 311, 516, 281, 3625, 15329, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.12435052890588741, "compression_ratio": 1.7092511013215859, "no_speech_prob": 3.7266011077008443e-06}, {"id": 796, "seek": 335000, "start": 3359.92, "end": 3365.8, "text": " So at that point you can just run dot slash setup conda and it'll run this installer.", "tokens": [407, 412, 300, 935, 291, 393, 445, 1190, 5893, 17330, 8657, 2224, 64, 293, 309, 603, 1190, 341, 46620, 13], "temperature": 0.0, "avg_logprob": -0.12435052890588741, "compression_ratio": 1.7092511013215859, "no_speech_prob": 3.7266011077008443e-06}, {"id": 797, "seek": 335000, "start": 3365.8, "end": 3373.0, "text": " Now if you've got Linux or Mac you've already got Python on your machine.", "tokens": [823, 498, 291, 600, 658, 18734, 420, 5707, 291, 600, 1217, 658, 15329, 322, 428, 3479, 13], "temperature": 0.0, "avg_logprob": -0.12435052890588741, "compression_ratio": 1.7092511013215859, "no_speech_prob": 3.7266011077008443e-06}, {"id": 798, "seek": 335000, "start": 3373.0, "end": 3374.48, "text": " Don't use that Python.", "tokens": [1468, 380, 764, 300, 15329, 13], "temperature": 0.0, "avg_logprob": -0.12435052890588741, "compression_ratio": 1.7092511013215859, "no_speech_prob": 3.7266011077008443e-06}, {"id": 799, "seek": 335000, "start": 3374.48, "end": 3378.24, "text": " And the reason is because that Python is called the system Python.", "tokens": [400, 264, 1778, 307, 570, 300, 15329, 307, 1219, 264, 1185, 15329, 13], "temperature": 0.0, "avg_logprob": -0.12435052890588741, "compression_ratio": 1.7092511013215859, "no_speech_prob": 3.7266011077008443e-06}, {"id": 800, "seek": 337824, "start": 3378.24, "end": 3381.16, "text": " It's used by your computer to do computer stuff right.", "tokens": [467, 311, 1143, 538, 428, 3820, 281, 360, 3820, 1507, 558, 13], "temperature": 0.0, "avg_logprob": -0.1682049376623971, "compression_ratio": 1.7341269841269842, "no_speech_prob": 5.422179583547404e-06}, {"id": 801, "seek": 337824, "start": 3381.16, "end": 3382.68, "text": " It's actually it's actually needed.", "tokens": [467, 311, 767, 309, 311, 767, 2978, 13], "temperature": 0.0, "avg_logprob": -0.1682049376623971, "compression_ratio": 1.7341269841269842, "no_speech_prob": 5.422179583547404e-06}, {"id": 802, "seek": 337824, "start": 3382.68, "end": 3384.7999999999997, "text": " You don't want to be messing with it.", "tokens": [509, 500, 380, 528, 281, 312, 23258, 365, 309, 13], "temperature": 0.0, "avg_logprob": -0.1682049376623971, "compression_ratio": 1.7341269841269842, "no_speech_prob": 5.422179583547404e-06}, {"id": 803, "seek": 337824, "start": 3384.7999999999997, "end": 3391.12, "text": " I promise you like it's it always leads to disaster.", "tokens": [286, 6228, 291, 411, 309, 311, 309, 1009, 6689, 281, 11293, 13], "temperature": 0.0, "avg_logprob": -0.1682049376623971, "compression_ratio": 1.7341269841269842, "no_speech_prob": 5.422179583547404e-06}, {"id": 804, "seek": 337824, "start": 3391.12, "end": 3394.3799999999997, "text": " Always you want your own development version of Python.", "tokens": [11270, 291, 528, 428, 1065, 3250, 3037, 295, 15329, 13], "temperature": 0.0, "avg_logprob": -0.1682049376623971, "compression_ratio": 1.7341269841269842, "no_speech_prob": 5.422179583547404e-06}, {"id": 805, "seek": 337824, "start": 3394.3799999999997, "end": 3397.68, "text": " It's also going to make sure you've got the latest version and all the libraries you want", "tokens": [467, 311, 611, 516, 281, 652, 988, 291, 600, 658, 264, 6792, 3037, 293, 439, 264, 15148, 291, 528], "temperature": 0.0, "avg_logprob": -0.1682049376623971, "compression_ratio": 1.7341269841269842, "no_speech_prob": 5.422179583547404e-06}, {"id": 806, "seek": 337824, "start": 3397.68, "end": 3406.24, "text": " and by the by far the best one for you is almost certainly going to be these conda based", "tokens": [293, 538, 264, 538, 1400, 264, 1151, 472, 337, 291, 307, 1920, 3297, 516, 281, 312, 613, 2224, 64, 2361], "temperature": 0.0, "avg_logprob": -0.1682049376623971, "compression_ratio": 1.7341269841269842, "no_speech_prob": 5.422179583547404e-06}, {"id": 807, "seek": 337824, "start": 3406.24, "end": 3407.24, "text": " Python distribution.", "tokens": [15329, 7316, 13], "temperature": 0.0, "avg_logprob": -0.1682049376623971, "compression_ratio": 1.7341269841269842, "no_speech_prob": 5.422179583547404e-06}, {"id": 808, "seek": 340724, "start": 3407.24, "end": 3413.08, "text": " So if you run setup conda you'll get the one that we recommend and the one we recommend", "tokens": [407, 498, 291, 1190, 8657, 2224, 64, 291, 603, 483, 264, 472, 300, 321, 2748, 293, 264, 472, 321, 2748], "temperature": 0.0, "avg_logprob": -0.11085431329135237, "compression_ratio": 1.815217391304348, "no_speech_prob": 5.862740636075614e-06}, {"id": 809, "seek": 340724, "start": 3413.08, "end": 3417.64, "text": " at the moment is something called Mamba Forge.", "tokens": [412, 264, 1623, 307, 746, 1219, 376, 23337, 1171, 432, 13], "temperature": 0.0, "avg_logprob": -0.11085431329135237, "compression_ratio": 1.815217391304348, "no_speech_prob": 5.862740636075614e-06}, {"id": 810, "seek": 340724, "start": 3417.64, "end": 3422.3599999999997, "text": " So basically once you run it you'll find that you've now and you close your terminal and", "tokens": [407, 1936, 1564, 291, 1190, 309, 291, 603, 915, 300, 291, 600, 586, 293, 291, 1998, 428, 14709, 293], "temperature": 0.0, "avg_logprob": -0.11085431329135237, "compression_ratio": 1.815217391304348, "no_speech_prob": 5.862740636075614e-06}, {"id": 811, "seek": 340724, "start": 3422.3599999999997, "end": 3428.16, "text": " reopen it you'll find you've now got one extra command which is called Mamba and Mamba lets", "tokens": [33861, 309, 291, 603, 915, 291, 600, 586, 658, 472, 2857, 5622, 597, 307, 1219, 376, 23337, 293, 376, 23337, 6653], "temperature": 0.0, "avg_logprob": -0.11085431329135237, "compression_ratio": 1.815217391304348, "no_speech_prob": 5.862740636075614e-06}, {"id": 812, "seek": 340724, "start": 3428.16, "end": 3431.24, "text": " you install stuff.", "tokens": [291, 3625, 1507, 13], "temperature": 0.0, "avg_logprob": -0.11085431329135237, "compression_ratio": 1.815217391304348, "no_speech_prob": 5.862740636075614e-06}, {"id": 813, "seek": 343124, "start": 3431.24, "end": 3440.2799999999997, "text": " So once you've run it you'll be able to go Mamba install fast AI and that's going to", "tokens": [407, 1564, 291, 600, 1190, 309, 291, 603, 312, 1075, 281, 352, 376, 23337, 3625, 2370, 7318, 293, 300, 311, 516, 281], "temperature": 0.0, "avg_logprob": -0.3556736441219554, "compression_ratio": 1.5958549222797926, "no_speech_prob": 4.860387434746372e-06}, {"id": 814, "seek": 343124, "start": 3440.2799999999997, "end": 3444.3999999999996, "text": " actually we should probably I should mention this actually more bit more detail about how", "tokens": [767, 321, 820, 1391, 286, 820, 2152, 341, 767, 544, 857, 544, 2607, 466, 577], "temperature": 0.0, "avg_logprob": -0.3556736441219554, "compression_ratio": 1.5958549222797926, "no_speech_prob": 4.860387434746372e-06}, {"id": 815, "seek": 343124, "start": 3444.3999999999996, "end": 3448.4799999999996, "text": " to install it correctly.", "tokens": [281, 3625, 309, 8944, 13], "temperature": 0.0, "avg_logprob": -0.3556736441219554, "compression_ratio": 1.5958549222797926, "no_speech_prob": 4.860387434746372e-06}, {"id": 816, "seek": 343124, "start": 3448.4799999999996, "end": 3452.2, "text": " We go to docs dot fast dot AI installing.", "tokens": [492, 352, 281, 45623, 5893, 2370, 5893, 7318, 20762, 13], "temperature": 0.0, "avg_logprob": -0.3556736441219554, "compression_ratio": 1.5958549222797926, "no_speech_prob": 4.860387434746372e-06}, {"id": 817, "seek": 343124, "start": 3452.2, "end": 3453.2, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.3556736441219554, "compression_ratio": 1.5958549222797926, "no_speech_prob": 4.860387434746372e-06}, {"id": 818, "seek": 343124, "start": 3453.2, "end": 3454.2, "text": " OK.", "tokens": [2264, 13], "temperature": 0.0, "avg_logprob": -0.3556736441219554, "compression_ratio": 1.5958549222797926, "no_speech_prob": 4.860387434746372e-06}, {"id": 819, "seek": 343124, "start": 3454.2, "end": 3457.2, "text": " We actually want to do conda install minus C fast chain.", "tokens": [492, 767, 528, 281, 360, 2224, 64, 3625, 3175, 383, 2370, 5021, 13], "temperature": 0.0, "avg_logprob": -0.3556736441219554, "compression_ratio": 1.5958549222797926, "no_speech_prob": 4.860387434746372e-06}, {"id": 820, "seek": 345720, "start": 3457.2, "end": 3461.9199999999996, "text": " So this is copy and paste sorry not actually.", "tokens": [407, 341, 307, 5055, 293, 9163, 2597, 406, 767, 13], "temperature": 0.0, "avg_logprob": -0.21537179766960865, "compression_ratio": 1.7843137254901962, "no_speech_prob": 7.071824711601948e-06}, {"id": 821, "seek": 345720, "start": 3461.9199999999996, "end": 3465.7999999999997, "text": " And then the other thing I'll say is instead of using conda replace conda with Mamba because", "tokens": [400, 550, 264, 661, 551, 286, 603, 584, 307, 2602, 295, 1228, 2224, 64, 7406, 2224, 64, 365, 376, 23337, 570], "temperature": 0.0, "avg_logprob": -0.21537179766960865, "compression_ratio": 1.7843137254901962, "no_speech_prob": 7.071824711601948e-06}, {"id": 822, "seek": 345720, "start": 3465.7999999999997, "end": 3467.52, "text": " nowadays it's much faster.", "tokens": [13434, 309, 311, 709, 4663, 13], "temperature": 0.0, "avg_logprob": -0.21537179766960865, "compression_ratio": 1.7843137254901962, "no_speech_prob": 7.071824711601948e-06}, {"id": 823, "seek": 345720, "start": 3467.52, "end": 3472.48, "text": " So Mamba install minus C fast chain fast AI.", "tokens": [407, 376, 23337, 3625, 3175, 383, 2370, 5021, 2370, 7318, 13], "temperature": 0.0, "avg_logprob": -0.21537179766960865, "compression_ratio": 1.7843137254901962, "no_speech_prob": 7.071824711601948e-06}, {"id": 824, "seek": 345720, "start": 3472.48, "end": 3477.2, "text": " Now this is going to install everything you need.", "tokens": [823, 341, 307, 516, 281, 3625, 1203, 291, 643, 13], "temperature": 0.0, "avg_logprob": -0.21537179766960865, "compression_ratio": 1.7843137254901962, "no_speech_prob": 7.071824711601948e-06}, {"id": 825, "seek": 345720, "start": 3477.2, "end": 3478.98, "text": " It's going to install PyTorch.", "tokens": [467, 311, 516, 281, 3625, 9953, 51, 284, 339, 13], "temperature": 0.0, "avg_logprob": -0.21537179766960865, "compression_ratio": 1.7843137254901962, "no_speech_prob": 7.071824711601948e-06}, {"id": 826, "seek": 345720, "start": 3478.98, "end": 3480.4399999999996, "text": " It's going to install NumPy.", "tokens": [467, 311, 516, 281, 3625, 22592, 47, 88, 13], "temperature": 0.0, "avg_logprob": -0.21537179766960865, "compression_ratio": 1.7843137254901962, "no_speech_prob": 7.071824711601948e-06}, {"id": 827, "seek": 345720, "start": 3480.4399999999996, "end": 3486.72, "text": " It's going to install fast AI and so forth.", "tokens": [467, 311, 516, 281, 3625, 2370, 7318, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.21537179766960865, "compression_ratio": 1.7843137254901962, "no_speech_prob": 7.071824711601948e-06}, {"id": 828, "seek": 348672, "start": 3486.72, "end": 3491.48, "text": " And so obviously I've already got it.", "tokens": [400, 370, 2745, 286, 600, 1217, 658, 309, 13], "temperature": 0.0, "avg_logprob": -0.21746019385326867, "compression_ratio": 1.644808743169399, "no_speech_prob": 1.0289388228557073e-05}, {"id": 829, "seek": 348672, "start": 3491.48, "end": 3499.3599999999997, "text": " And then the other thing you want to do is install NB dev.", "tokens": [400, 550, 264, 661, 551, 291, 528, 281, 360, 307, 3625, 426, 33, 1905, 13], "temperature": 0.0, "avg_logprob": -0.21746019385326867, "compression_ratio": 1.644808743169399, "no_speech_prob": 1.0289388228557073e-05}, {"id": 830, "seek": 348672, "start": 3499.3599999999997, "end": 3501.3199999999997, "text": " So you can do exactly the same thing for NB dev.", "tokens": [407, 291, 393, 360, 2293, 264, 912, 551, 337, 426, 33, 1905, 13], "temperature": 0.0, "avg_logprob": -0.21746019385326867, "compression_ratio": 1.644808743169399, "no_speech_prob": 1.0289388228557073e-05}, {"id": 831, "seek": 348672, "start": 3501.3199999999997, "end": 3502.48, "text": " You don't have to write.", "tokens": [509, 500, 380, 362, 281, 2464, 13], "temperature": 0.0, "avg_logprob": -0.21746019385326867, "compression_ratio": 1.644808743169399, "no_speech_prob": 1.0289388228557073e-05}, {"id": 832, "seek": 348672, "start": 3502.48, "end": 3508.8199999999997, "text": " It's just that but that'll install Jupiter for you amongst other things.", "tokens": [467, 311, 445, 300, 457, 300, 603, 3625, 24567, 337, 291, 12918, 661, 721, 13], "temperature": 0.0, "avg_logprob": -0.21746019385326867, "compression_ratio": 1.644808743169399, "no_speech_prob": 1.0289388228557073e-05}, {"id": 833, "seek": 348672, "start": 3508.8199999999997, "end": 3513.12, "text": " And so at that point you can now you can now use Jupiter.", "tokens": [400, 370, 412, 300, 935, 291, 393, 586, 291, 393, 586, 764, 24567, 13], "temperature": 0.0, "avg_logprob": -0.21746019385326867, "compression_ratio": 1.644808743169399, "no_speech_prob": 1.0289388228557073e-05}, {"id": 834, "seek": 351312, "start": 3513.12, "end": 3518.4, "text": " And so the way Jupiter works is you can see it over here.", "tokens": [400, 370, 264, 636, 24567, 1985, 307, 291, 393, 536, 309, 670, 510, 13], "temperature": 0.0, "avg_logprob": -0.19710232555002405, "compression_ratio": 1.4573170731707317, "no_speech_prob": 4.4254502427065745e-06}, {"id": 835, "seek": 351312, "start": 3518.4, "end": 3522.96, "text": " This is by going close it so we can start again.", "tokens": [639, 307, 538, 516, 1998, 309, 370, 321, 393, 722, 797, 13], "temperature": 0.0, "avg_logprob": -0.19710232555002405, "compression_ratio": 1.4573170731707317, "no_speech_prob": 4.4254502427065745e-06}, {"id": 836, "seek": 351312, "start": 3522.96, "end": 3528.0, "text": " So basically to use Jupiter you just type Jupiter notebook.", "tokens": [407, 1936, 281, 764, 24567, 291, 445, 2010, 24567, 21060, 13], "temperature": 0.0, "avg_logprob": -0.19710232555002405, "compression_ratio": 1.4573170731707317, "no_speech_prob": 4.4254502427065745e-06}, {"id": 837, "seek": 351312, "start": 3528.0, "end": 3531.64, "text": " OK.", "tokens": [2264, 13], "temperature": 0.0, "avg_logprob": -0.19710232555002405, "compression_ratio": 1.4573170731707317, "no_speech_prob": 4.4254502427065745e-06}, {"id": 838, "seek": 351312, "start": 3531.64, "end": 3535.44, "text": " And when you run it it'll say OK we're now running a server for you.", "tokens": [400, 562, 291, 1190, 309, 309, 603, 584, 2264, 321, 434, 586, 2614, 257, 7154, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.19710232555002405, "compression_ratio": 1.4573170731707317, "no_speech_prob": 4.4254502427065745e-06}, {"id": 839, "seek": 353544, "start": 3535.44, "end": 3544.04, "text": " And so if you click on that hyperlink it'll pop up this is exactly what you see me use", "tokens": [400, 370, 498, 291, 2052, 322, 300, 9848, 22473, 309, 603, 1665, 493, 341, 307, 2293, 437, 291, 536, 385, 764], "temperature": 0.0, "avg_logprob": -0.19865199975800096, "compression_ratio": 1.4161073825503356, "no_speech_prob": 3.6119381547905505e-06}, {"id": 840, "seek": 353544, "start": 3544.04, "end": 3547.64, "text": " all the time.", "tokens": [439, 264, 565, 13], "temperature": 0.0, "avg_logprob": -0.19865199975800096, "compression_ratio": 1.4161073825503356, "no_speech_prob": 3.6119381547905505e-06}, {"id": 841, "seek": 353544, "start": 3547.64, "end": 3553.2400000000002, "text": " OK so you know that hopefully is enough to kind of get you started with with Python and", "tokens": [2264, 370, 291, 458, 300, 4696, 307, 1547, 281, 733, 295, 483, 291, 1409, 365, 365, 15329, 293], "temperature": 0.0, "avg_logprob": -0.19865199975800096, "compression_ratio": 1.4161073825503356, "no_speech_prob": 3.6119381547905505e-06}, {"id": 842, "seek": 353544, "start": 3553.2400000000002, "end": 3558.0, "text": " with Jupiter notebook.", "tokens": [365, 24567, 21060, 13], "temperature": 0.0, "avg_logprob": -0.19865199975800096, "compression_ratio": 1.4161073825503356, "no_speech_prob": 3.6119381547905505e-06}, {"id": 843, "seek": 355800, "start": 3558.0, "end": 3567.0, "text": " The other way people tend to install software is using something called pip instead of Mamba.", "tokens": [440, 661, 636, 561, 3928, 281, 3625, 4722, 307, 1228, 746, 1219, 8489, 2602, 295, 376, 23337, 13], "temperature": 0.0, "avg_logprob": -0.1334537302406089, "compression_ratio": 1.6764705882352942, "no_speech_prob": 8.139522833516821e-06}, {"id": 844, "seek": 355800, "start": 3567.0, "end": 3569.56, "text": " Pretty much anything you can do with Mamba you can also do with pip.", "tokens": [10693, 709, 1340, 291, 393, 360, 365, 376, 23337, 291, 393, 611, 360, 365, 8489, 13], "temperature": 0.0, "avg_logprob": -0.1334537302406089, "compression_ratio": 1.6764705882352942, "no_speech_prob": 8.139522833516821e-06}, {"id": 845, "seek": 355800, "start": 3569.56, "end": 3574.44, "text": " But if you've got a GPU pip isn't going to install things generally so that it works", "tokens": [583, 498, 291, 600, 658, 257, 18407, 8489, 1943, 380, 516, 281, 3625, 721, 5101, 370, 300, 309, 1985], "temperature": 0.0, "avg_logprob": -0.1334537302406089, "compression_ratio": 1.6764705882352942, "no_speech_prob": 8.139522833516821e-06}, {"id": 846, "seek": 355800, "start": 3574.44, "end": 3575.44, "text": " on your GPU.", "tokens": [322, 428, 18407, 13], "temperature": 0.0, "avg_logprob": -0.1334537302406089, "compression_ratio": 1.6764705882352942, "no_speech_prob": 8.139522833516821e-06}, {"id": 847, "seek": 355800, "start": 3575.44, "end": 3578.48, "text": " You have to install lots of other stuff which is annoying.", "tokens": [509, 362, 281, 3625, 3195, 295, 661, 1507, 597, 307, 11304, 13], "temperature": 0.0, "avg_logprob": -0.1334537302406089, "compression_ratio": 1.6764705882352942, "no_speech_prob": 8.139522833516821e-06}, {"id": 848, "seek": 355800, "start": 3578.48, "end": 3587.28, "text": " So that's why I kind of tell people to use Mamba but you can use pip otherwise.", "tokens": [407, 300, 311, 983, 286, 733, 295, 980, 561, 281, 764, 376, 23337, 457, 291, 393, 764, 8489, 5911, 13], "temperature": 0.0, "avg_logprob": -0.1334537302406089, "compression_ratio": 1.6764705882352942, "no_speech_prob": 8.139522833516821e-06}, {"id": 849, "seek": 358728, "start": 3587.28, "end": 3588.28, "text": " So a little bit of red.", "tokens": [407, 257, 707, 857, 295, 2182, 13], "temperature": 0.0, "avg_logprob": -0.3131206244753118, "compression_ratio": 1.6081081081081081, "no_speech_prob": 2.3186265025287867e-05}, {"id": 850, "seek": 358728, "start": 3588.28, "end": 3594.96, "text": " Please let us know how we can help you going.", "tokens": [2555, 718, 505, 458, 577, 321, 393, 854, 291, 516, 13], "temperature": 0.0, "avg_logprob": -0.3131206244753118, "compression_ratio": 1.6081081081081081, "no_speech_prob": 2.3186265025287867e-05}, {"id": 851, "seek": 358728, "start": 3594.96, "end": 3596.88, "text": " OK so let's see how we're going with our steps.", "tokens": [2264, 370, 718, 311, 536, 577, 321, 434, 516, 365, 527, 4439, 13], "temperature": 0.0, "avg_logprob": -0.3131206244753118, "compression_ratio": 1.6081081081081081, "no_speech_prob": 2.3186265025287867e-05}, {"id": 852, "seek": 358728, "start": 3596.88, "end": 3599.1200000000003, "text": " I forgot I had these steps here to remind myself.", "tokens": [286, 5298, 286, 632, 613, 4439, 510, 281, 4160, 2059, 13], "temperature": 0.0, "avg_logprob": -0.3131206244753118, "compression_ratio": 1.6081081081081081, "no_speech_prob": 2.3186265025287867e-05}, {"id": 853, "seek": 358728, "start": 3599.1200000000003, "end": 3601.1600000000003, "text": " We created a space tick.", "tokens": [492, 2942, 257, 1901, 5204, 13], "temperature": 0.0, "avg_logprob": -0.3131206244753118, "compression_ratio": 1.6081081081081081, "no_speech_prob": 2.3186265025287867e-05}, {"id": 854, "seek": 358728, "start": 3601.1600000000003, "end": 3604.4, "text": " We created a basic interface tick.", "tokens": [492, 2942, 257, 3875, 9226, 5204, 13], "temperature": 0.0, "avg_logprob": -0.3131206244753118, "compression_ratio": 1.6081081081081081, "no_speech_prob": 2.3186265025287867e-05}, {"id": 855, "seek": 358728, "start": 3604.4, "end": 3605.44, "text": " OK we got get set up.", "tokens": [2264, 321, 658, 483, 992, 493, 13], "temperature": 0.0, "avg_logprob": -0.3131206244753118, "compression_ratio": 1.6081081081081081, "no_speech_prob": 2.3186265025287867e-05}, {"id": 856, "seek": 358728, "start": 3605.44, "end": 3607.84, "text": " We got Condor set up or Mamba.", "tokens": [492, 658, 21793, 284, 992, 493, 420, 376, 23337, 13], "temperature": 0.0, "avg_logprob": -0.3131206244753118, "compression_ratio": 1.6081081081081081, "no_speech_prob": 2.3186265025287867e-05}, {"id": 857, "seek": 358728, "start": 3607.84, "end": 3610.6800000000003, "text": " So Mamba and Condor are the same thing.", "tokens": [407, 376, 23337, 293, 21793, 284, 366, 264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.3131206244753118, "compression_ratio": 1.6081081081081081, "no_speech_prob": 2.3186265025287867e-05}, {"id": 858, "seek": 358728, "start": 3610.6800000000003, "end": 3612.96, "text": " Mamba is just a much faster version.", "tokens": [376, 23337, 307, 445, 257, 709, 4663, 3037, 13], "temperature": 0.0, "avg_logprob": -0.3131206244753118, "compression_ratio": 1.6081081081081081, "no_speech_prob": 2.3186265025287867e-05}, {"id": 859, "seek": 361296, "start": 3612.96, "end": 3617.64, "text": " And we'll keep some notes on the course website because at the moment they're actually working", "tokens": [400, 321, 603, 1066, 512, 5570, 322, 264, 1164, 3144, 570, 412, 264, 1623, 436, 434, 767, 1364], "temperature": 0.0, "avg_logprob": -0.2318658533784532, "compression_ratio": 1.559090909090909, "no_speech_prob": 1.1842785170301795e-05}, {"id": 860, "seek": 361296, "start": 3617.64, "end": 3620.56, "text": " on including the speed ups from Mamba into Condor.", "tokens": [322, 3009, 264, 3073, 15497, 490, 376, 23337, 666, 21793, 284, 13], "temperature": 0.0, "avg_logprob": -0.2318658533784532, "compression_ratio": 1.559090909090909, "no_speech_prob": 1.1842785170301795e-05}, {"id": 861, "seek": 361296, "start": 3620.56, "end": 3623.56, "text": " So at some point maybe it'll be fine to use Condor again.", "tokens": [407, 412, 512, 935, 1310, 309, 603, 312, 2489, 281, 764, 21793, 284, 797, 13], "temperature": 0.0, "avg_logprob": -0.2318658533784532, "compression_ratio": 1.559090909090909, "no_speech_prob": 1.1842785170301795e-05}, {"id": 862, "seek": 361296, "start": 3623.56, "end": 3626.64, "text": " At the moment Condor is way too slow so don't use it.", "tokens": [1711, 264, 1623, 21793, 284, 307, 636, 886, 2964, 370, 500, 380, 764, 309, 13], "temperature": 0.0, "avg_logprob": -0.2318658533784532, "compression_ratio": 1.559090909090909, "no_speech_prob": 1.1842785170301795e-05}, {"id": 863, "seek": 361296, "start": 3626.64, "end": 3629.2400000000002, "text": " OK we've done dogs versus cats.", "tokens": [2264, 321, 600, 1096, 7197, 5717, 11111, 13], "temperature": 0.0, "avg_logprob": -0.2318658533784532, "compression_ratio": 1.559090909090909, "no_speech_prob": 1.1842785170301795e-05}, {"id": 864, "seek": 361296, "start": 3629.2400000000002, "end": 3638.32, "text": " No problem.", "tokens": [883, 1154, 13], "temperature": 0.0, "avg_logprob": -0.2318658533784532, "compression_ratio": 1.559090909090909, "no_speech_prob": 1.1842785170301795e-05}, {"id": 865, "seek": 361296, "start": 3638.32, "end": 3640.88, "text": " Yeah so we could also look at pet breeds.", "tokens": [865, 370, 321, 727, 611, 574, 412, 3817, 41609, 13], "temperature": 0.0, "avg_logprob": -0.2318658533784532, "compression_ratio": 1.559090909090909, "no_speech_prob": 1.1842785170301795e-05}, {"id": 866, "seek": 364088, "start": 3640.88, "end": 3643.08, "text": " Yeah we'll look at that.", "tokens": [865, 321, 603, 574, 412, 300, 13], "temperature": 0.0, "avg_logprob": -0.24814603683796335, "compression_ratio": 1.4646464646464648, "no_speech_prob": 9.368510291096754e-06}, {"id": 867, "seek": 364088, "start": 3643.08, "end": 3645.0, "text": " OK we've used exported learner.", "tokens": [2264, 321, 600, 1143, 42055, 33347, 13], "temperature": 0.0, "avg_logprob": -0.24814603683796335, "compression_ratio": 1.4646464646464648, "no_speech_prob": 9.368510291096754e-06}, {"id": 868, "seek": 364088, "start": 3645.0, "end": 3646.0, "text": " No problem.", "tokens": [883, 1154, 13], "temperature": 0.0, "avg_logprob": -0.24814603683796335, "compression_ratio": 1.4646464646464648, "no_speech_prob": 9.368510291096754e-06}, {"id": 869, "seek": 364088, "start": 3646.0, "end": 3647.0, "text": " Used NB dev.", "tokens": [43237, 426, 33, 1905, 13], "temperature": 0.0, "avg_logprob": -0.24814603683796335, "compression_ratio": 1.4646464646464648, "no_speech_prob": 9.368510291096754e-06}, {"id": 870, "seek": 364088, "start": 3647.0, "end": 3648.0, "text": " No problem.", "tokens": [883, 1154, 13], "temperature": 0.0, "avg_logprob": -0.24814603683796335, "compression_ratio": 1.4646464646464648, "no_speech_prob": 9.368510291096754e-06}, {"id": 871, "seek": 364088, "start": 3648.0, "end": 3649.0, "text": " Oh OK.", "tokens": [876, 2264, 13], "temperature": 0.0, "avg_logprob": -0.24814603683796335, "compression_ratio": 1.4646464646464648, "no_speech_prob": 9.368510291096754e-06}, {"id": 872, "seek": 364088, "start": 3649.0, "end": 3650.0, "text": " Try the API.", "tokens": [6526, 264, 9362, 13], "temperature": 0.0, "avg_logprob": -0.24814603683796335, "compression_ratio": 1.4646464646464648, "no_speech_prob": 9.368510291096754e-06}, {"id": 873, "seek": 364088, "start": 3650.0, "end": 3651.0, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.24814603683796335, "compression_ratio": 1.4646464646464648, "no_speech_prob": 9.368510291096754e-06}, {"id": 874, "seek": 364088, "start": 3651.0, "end": 3652.0, "text": " This is interesting.", "tokens": [639, 307, 1880, 13], "temperature": 0.0, "avg_logprob": -0.24814603683796335, "compression_ratio": 1.4646464646464648, "no_speech_prob": 9.368510291096754e-06}, {"id": 875, "seek": 364088, "start": 3652.0, "end": 3658.36, "text": " So I think we can all agree hopefully that this is pretty cool that we can provide to", "tokens": [407, 286, 519, 321, 393, 439, 3986, 4696, 300, 341, 307, 1238, 1627, 300, 321, 393, 2893, 281], "temperature": 0.0, "avg_logprob": -0.24814603683796335, "compression_ratio": 1.4646464646464648, "no_speech_prob": 9.368510291096754e-06}, {"id": 876, "seek": 364088, "start": 3658.36, "end": 3664.76, "text": " anybody who wants to use it for free a real working model.", "tokens": [4472, 567, 2738, 281, 764, 309, 337, 1737, 257, 957, 1364, 2316, 13], "temperature": 0.0, "avg_logprob": -0.24814603683796335, "compression_ratio": 1.4646464646464648, "no_speech_prob": 9.368510291096754e-06}, {"id": 877, "seek": 366476, "start": 3664.76, "end": 3674.88, "text": " And you know with Gradio there's actually you know a reasonable amount of flexibility", "tokens": [400, 291, 458, 365, 16710, 1004, 456, 311, 767, 291, 458, 257, 10585, 2372, 295, 12635], "temperature": 0.0, "avg_logprob": -0.14614783227443695, "compression_ratio": 1.5271739130434783, "no_speech_prob": 6.438920536311343e-06}, {"id": 878, "seek": 366476, "start": 3674.88, "end": 3679.2000000000003, "text": " around like how you can make your your website look you know using these various different", "tokens": [926, 411, 577, 291, 393, 652, 428, 428, 3144, 574, 291, 458, 1228, 613, 3683, 819], "temperature": 0.0, "avg_logprob": -0.14614783227443695, "compression_ratio": 1.5271739130434783, "no_speech_prob": 6.438920536311343e-06}, {"id": 879, "seek": 366476, "start": 3679.2000000000003, "end": 3683.2400000000002, "text": " widgets.", "tokens": [43355, 13], "temperature": 0.0, "avg_logprob": -0.14614783227443695, "compression_ratio": 1.5271739130434783, "no_speech_prob": 6.438920536311343e-06}, {"id": 880, "seek": 366476, "start": 3683.2400000000002, "end": 3691.0, "text": " It's not amazingly flexible but it's flexible enough to kind of it really just for prototyping.", "tokens": [467, 311, 406, 31762, 11358, 457, 309, 311, 11358, 1547, 281, 733, 295, 309, 534, 445, 337, 46219, 3381, 13], "temperature": 0.0, "avg_logprob": -0.14614783227443695, "compression_ratio": 1.5271739130434783, "no_speech_prob": 6.438920536311343e-06}, {"id": 881, "seek": 369100, "start": 3691.0, "end": 3696.52, "text": " So Gradio has lots of widgets and things that you can use.", "tokens": [407, 16710, 1004, 575, 3195, 295, 43355, 293, 721, 300, 291, 393, 764, 13], "temperature": 0.0, "avg_logprob": -0.14668135099773166, "compression_ratio": 1.4825870646766168, "no_speech_prob": 5.014626822230639e-06}, {"id": 882, "seek": 369100, "start": 3696.52, "end": 3704.64, "text": " The other main platform at the moment that Hugging First Spaces supports is called Streamlet.", "tokens": [440, 661, 2135, 3663, 412, 264, 1623, 300, 46892, 3249, 2386, 1738, 2116, 9346, 307, 1219, 24904, 2631, 13], "temperature": 0.0, "avg_logprob": -0.14668135099773166, "compression_ratio": 1.4825870646766168, "no_speech_prob": 5.014626822230639e-06}, {"id": 883, "seek": 369100, "start": 3704.64, "end": 3710.28, "text": " Streamlet is more flexible I would say than Gradio.", "tokens": [24904, 2631, 307, 544, 11358, 286, 576, 584, 813, 16710, 1004, 13], "temperature": 0.0, "avg_logprob": -0.14668135099773166, "compression_ratio": 1.4825870646766168, "no_speech_prob": 5.014626822230639e-06}, {"id": 884, "seek": 369100, "start": 3710.28, "end": 3715.66, "text": " Not quite as easy to get started with but you know it's kind of that nice in between", "tokens": [1726, 1596, 382, 1858, 281, 483, 1409, 365, 457, 291, 458, 309, 311, 733, 295, 300, 1481, 294, 1296], "temperature": 0.0, "avg_logprob": -0.14668135099773166, "compression_ratio": 1.4825870646766168, "no_speech_prob": 5.014626822230639e-06}, {"id": 885, "seek": 369100, "start": 3715.66, "end": 3716.66, "text": " I guess.", "tokens": [286, 2041, 13], "temperature": 0.0, "avg_logprob": -0.14668135099773166, "compression_ratio": 1.4825870646766168, "no_speech_prob": 5.014626822230639e-06}, {"id": 886, "seek": 371666, "start": 3716.66, "end": 3721.68, "text": " It's also a very good thing again mainly for kind of building prototypes.", "tokens": [467, 311, 611, 257, 588, 665, 551, 797, 8704, 337, 733, 295, 2390, 42197, 13], "temperature": 0.0, "avg_logprob": -0.23333431761941792, "compression_ratio": 1.476923076923077, "no_speech_prob": 3.611945203374489e-06}, {"id": 887, "seek": 371666, "start": 3721.68, "end": 3724.3199999999997, "text": " But at some point you're going to want to build more than a prototype.", "tokens": [583, 412, 512, 935, 291, 434, 516, 281, 528, 281, 1322, 544, 813, 257, 19475, 13], "temperature": 0.0, "avg_logprob": -0.23333431761941792, "compression_ratio": 1.476923076923077, "no_speech_prob": 3.611945203374489e-06}, {"id": 888, "seek": 371666, "start": 3724.3199999999997, "end": 3727.2799999999997, "text": " You want to build an app.", "tokens": [509, 528, 281, 1322, 364, 724, 13], "temperature": 0.0, "avg_logprob": -0.23333431761941792, "compression_ratio": 1.476923076923077, "no_speech_prob": 3.611945203374489e-06}, {"id": 889, "seek": 371666, "start": 3727.2799999999997, "end": 3731.8799999999997, "text": " And one of the things I really like about Gradio in Hugging First Spaces is there's", "tokens": [400, 472, 295, 264, 721, 286, 534, 411, 466, 16710, 1004, 294, 46892, 3249, 2386, 1738, 2116, 307, 456, 311], "temperature": 0.0, "avg_logprob": -0.23333431761941792, "compression_ratio": 1.476923076923077, "no_speech_prob": 3.611945203374489e-06}, {"id": 890, "seek": 371666, "start": 3731.8799999999997, "end": 3733.3599999999997, "text": " a button down here.", "tokens": [257, 2960, 760, 510, 13], "temperature": 0.0, "avg_logprob": -0.23333431761941792, "compression_ratio": 1.476923076923077, "no_speech_prob": 3.611945203374489e-06}, {"id": 891, "seek": 371666, "start": 3733.3599999999997, "end": 3735.6, "text": " View the API.", "tokens": [13909, 264, 9362, 13], "temperature": 0.0, "avg_logprob": -0.23333431761941792, "compression_ratio": 1.476923076923077, "no_speech_prob": 3.611945203374489e-06}, {"id": 892, "seek": 373560, "start": 3735.6, "end": 3748.48, "text": " So we can actually create any app we want and the key point is that the thing that does", "tokens": [407, 321, 393, 767, 1884, 604, 724, 321, 528, 293, 264, 2141, 935, 307, 300, 264, 551, 300, 775], "temperature": 0.0, "avg_logprob": -0.11254751486856429, "compression_ratio": 1.524390243902439, "no_speech_prob": 1.2098568049623282e-06}, {"id": 893, "seek": 373560, "start": 3748.48, "end": 3755.7999999999997, "text": " the actual model predictions for us is going to be handled by Hugging First Spaces Gradio.", "tokens": [264, 3539, 2316, 21264, 337, 505, 307, 516, 281, 312, 18033, 538, 46892, 3249, 2386, 1738, 2116, 16710, 1004, 13], "temperature": 0.0, "avg_logprob": -0.11254751486856429, "compression_ratio": 1.524390243902439, "no_speech_prob": 1.2098568049623282e-06}, {"id": 894, "seek": 373560, "start": 3755.7999999999997, "end": 3761.72, "text": " And then we can write a JavaScript application that then talks to that.", "tokens": [400, 550, 321, 393, 2464, 257, 15778, 3861, 300, 550, 6686, 281, 300, 13], "temperature": 0.0, "avg_logprob": -0.11254751486856429, "compression_ratio": 1.524390243902439, "no_speech_prob": 1.2098568049623282e-06}, {"id": 895, "seek": 376172, "start": 3761.72, "end": 3765.8999999999996, "text": " Now there's going to be two reactions here.", "tokens": [823, 456, 311, 516, 281, 312, 732, 12215, 510, 13], "temperature": 0.0, "avg_logprob": -0.16611161148338988, "compression_ratio": 1.623574144486692, "no_speech_prob": 8.013341357582249e-06}, {"id": 896, "seek": 376172, "start": 3765.8999999999996, "end": 3770.3399999999997, "text": " Anybody who's done some front-end engineering is going to be like oh great I can now literally", "tokens": [19082, 567, 311, 1096, 512, 1868, 12, 521, 7043, 307, 516, 281, 312, 411, 1954, 869, 286, 393, 586, 3736], "temperature": 0.0, "avg_logprob": -0.16611161148338988, "compression_ratio": 1.623574144486692, "no_speech_prob": 8.013341357582249e-06}, {"id": 897, "seek": 376172, "start": 3770.3399999999997, "end": 3775.8399999999997, "text": " create anything in the world because I just write any code and I can do it.", "tokens": [1884, 1340, 294, 264, 1002, 570, 286, 445, 2464, 604, 3089, 293, 286, 393, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.16611161148338988, "compression_ratio": 1.623574144486692, "no_speech_prob": 8.013341357582249e-06}, {"id": 898, "seek": 376172, "start": 3775.8399999999997, "end": 3781.72, "text": " They'll be excited and a lot of data scientists might be going uh-oh I have no idea how to", "tokens": [814, 603, 312, 2919, 293, 257, 688, 295, 1412, 7708, 1062, 312, 516, 2232, 12, 1445, 286, 362, 572, 1558, 577, 281], "temperature": 0.0, "avg_logprob": -0.16611161148338988, "compression_ratio": 1.623574144486692, "no_speech_prob": 8.013341357582249e-06}, {"id": 899, "seek": 376172, "start": 3781.72, "end": 3783.52, "text": " use JavaScript.", "tokens": [764, 15778, 13], "temperature": 0.0, "avg_logprob": -0.16611161148338988, "compression_ratio": 1.623574144486692, "no_speech_prob": 8.013341357582249e-06}, {"id": 900, "seek": 376172, "start": 3783.52, "end": 3787.58, "text": " It's not in my inventory.", "tokens": [467, 311, 406, 294, 452, 14228, 13], "temperature": 0.0, "avg_logprob": -0.16611161148338988, "compression_ratio": 1.623574144486692, "no_speech_prob": 8.013341357582249e-06}, {"id": 901, "seek": 376172, "start": 3787.58, "end": 3790.3599999999997, "text": " So this is again where I'm going to say look don't be too afraid of JavaScript.", "tokens": [407, 341, 307, 797, 689, 286, 478, 516, 281, 584, 574, 500, 380, 312, 886, 4638, 295, 15778, 13], "temperature": 0.0, "avg_logprob": -0.16611161148338988, "compression_ratio": 1.623574144486692, "no_speech_prob": 8.013341357582249e-06}, {"id": 902, "seek": 379036, "start": 3790.36, "end": 3794.44, "text": " I mean obviously one option here is just to kind of say hey I've got a model throw it", "tokens": [286, 914, 2745, 472, 3614, 510, 307, 445, 281, 733, 295, 584, 4177, 286, 600, 658, 257, 2316, 3507, 309], "temperature": 0.0, "avg_logprob": -0.11193898781058714, "compression_ratio": 1.5844155844155845, "no_speech_prob": 1.2289123105801991e-06}, {"id": 903, "seek": 379036, "start": 3794.44, "end": 3798.4, "text": " over to your wall over the wall to your mate who does know JavaScript and say please create", "tokens": [670, 281, 428, 2929, 670, 264, 2929, 281, 428, 11709, 567, 775, 458, 15778, 293, 584, 1767, 1884], "temperature": 0.0, "avg_logprob": -0.11193898781058714, "compression_ratio": 1.5844155844155845, "no_speech_prob": 1.2289123105801991e-06}, {"id": 904, "seek": 379036, "start": 3798.4, "end": 3800.32, "text": " a JavaScript interface for me.", "tokens": [257, 15778, 9226, 337, 385, 13], "temperature": 0.0, "avg_logprob": -0.11193898781058714, "compression_ratio": 1.5844155844155845, "no_speech_prob": 1.2289123105801991e-06}, {"id": 905, "seek": 379036, "start": 3800.32, "end": 3807.44, "text": " But let me just give you a sense of like how really not hard this actually is.", "tokens": [583, 718, 385, 445, 976, 291, 257, 2020, 295, 411, 577, 534, 406, 1152, 341, 767, 307, 13], "temperature": 0.0, "avg_logprob": -0.11193898781058714, "compression_ratio": 1.5844155844155845, "no_speech_prob": 1.2289123105801991e-06}, {"id": 906, "seek": 379036, "start": 3807.44, "end": 3811.2000000000003, "text": " So there's a end point.", "tokens": [407, 456, 311, 257, 917, 935, 13], "temperature": 0.0, "avg_logprob": -0.11193898781058714, "compression_ratio": 1.5844155844155845, "no_speech_prob": 1.2289123105801991e-06}, {"id": 907, "seek": 379036, "start": 3811.2000000000003, "end": 3816.08, "text": " There's now a URL that's running with our model on it.", "tokens": [821, 311, 586, 257, 12905, 300, 311, 2614, 365, 527, 2316, 322, 309, 13], "temperature": 0.0, "avg_logprob": -0.11193898781058714, "compression_ratio": 1.5844155844155845, "no_speech_prob": 1.2289123105801991e-06}, {"id": 908, "seek": 381608, "start": 3816.08, "end": 3824.2, "text": " And if you pass it some data and image some image data to this to this URL it's going", "tokens": [400, 498, 291, 1320, 309, 512, 1412, 293, 3256, 512, 3256, 1412, 281, 341, 281, 341, 12905, 309, 311, 516], "temperature": 0.0, "avg_logprob": -0.14230328571947315, "compression_ratio": 1.5795454545454546, "no_speech_prob": 2.1568098418356385e-06}, {"id": 909, "seek": 381608, "start": 3824.2, "end": 3827.2799999999997, "text": " to return back the dictionary.", "tokens": [281, 2736, 646, 264, 25890, 13], "temperature": 0.0, "avg_logprob": -0.14230328571947315, "compression_ratio": 1.5795454545454546, "no_speech_prob": 2.1568098418356385e-06}, {"id": 910, "seek": 381608, "start": 3827.2799999999997, "end": 3836.16, "text": " So it's going to do exactly the same thing that this UI does but as an as an API as a", "tokens": [407, 309, 311, 516, 281, 360, 2293, 264, 912, 551, 300, 341, 15682, 775, 457, 382, 364, 382, 364, 9362, 382, 257], "temperature": 0.0, "avg_logprob": -0.14230328571947315, "compression_ratio": 1.5795454545454546, "no_speech_prob": 2.1568098418356385e-06}, {"id": 911, "seek": 381608, "start": 3836.16, "end": 3838.68, "text": " function we can call.", "tokens": [2445, 321, 393, 818, 13], "temperature": 0.0, "avg_logprob": -0.14230328571947315, "compression_ratio": 1.5795454545454546, "no_speech_prob": 2.1568098418356385e-06}, {"id": 912, "seek": 381608, "start": 3838.68, "end": 3841.7, "text": " And so it's got like examples here of how to call it.", "tokens": [400, 370, 309, 311, 658, 411, 5110, 510, 295, 577, 281, 818, 309, 13], "temperature": 0.0, "avg_logprob": -0.14230328571947315, "compression_ratio": 1.5795454545454546, "no_speech_prob": 2.1568098418356385e-06}, {"id": 913, "seek": 384170, "start": 3841.7, "end": 3848.96, "text": " So for example I can actually let me show you the API as an example using that minimal", "tokens": [407, 337, 1365, 286, 393, 767, 718, 385, 855, 291, 264, 9362, 382, 364, 1365, 1228, 300, 13206], "temperature": 0.0, "avg_logprob": -0.14112544059753418, "compression_ratio": 1.572972972972973, "no_speech_prob": 1.733039994178398e-06}, {"id": 914, "seek": 384170, "start": 3848.96, "end": 3853.2, "text": " interface we had because it's just going to be a bit simpler.", "tokens": [9226, 321, 632, 570, 309, 311, 445, 516, 281, 312, 257, 857, 18587, 13], "temperature": 0.0, "avg_logprob": -0.14112544059753418, "compression_ratio": 1.572972972972973, "no_speech_prob": 1.733039994178398e-06}, {"id": 915, "seek": 384170, "start": 3853.2, "end": 3865.6, "text": " So if I click curl and I copy that copy that and paste.", "tokens": [407, 498, 286, 2052, 22591, 293, 286, 5055, 300, 5055, 300, 293, 9163, 13], "temperature": 0.0, "avg_logprob": -0.14112544059753418, "compression_ratio": 1.572972972972973, "no_speech_prob": 1.733039994178398e-06}, {"id": 916, "seek": 384170, "start": 3865.6, "end": 3869.3199999999997, "text": " So you can see there oh that's not a great example passing in hello world if I pass in", "tokens": [407, 291, 393, 536, 456, 1954, 300, 311, 406, 257, 869, 1365, 8437, 294, 7751, 1002, 498, 286, 1320, 294], "temperature": 0.0, "avg_logprob": -0.14112544059753418, "compression_ratio": 1.572972972972973, "no_speech_prob": 1.733039994178398e-06}, {"id": 917, "seek": 386932, "start": 3869.32, "end": 3878.04, "text": " an issue again let's see how I'm going with his name to niche.", "tokens": [364, 2734, 797, 718, 311, 536, 577, 286, 478, 516, 365, 702, 1315, 281, 19956, 13], "temperature": 0.0, "avg_logprob": -0.312974502300394, "compression_ratio": 1.3825503355704698, "no_speech_prob": 1.602804331923835e-06}, {"id": 918, "seek": 386932, "start": 3878.04, "end": 3880.4, "text": " Saying returns back hello to niche.", "tokens": [34087, 11247, 646, 7751, 281, 19956, 13], "temperature": 0.0, "avg_logprob": -0.312974502300394, "compression_ratio": 1.3825503355704698, "no_speech_prob": 1.602804331923835e-06}, {"id": 919, "seek": 386932, "start": 3880.4, "end": 3883.92, "text": " So this is how these APIs work right.", "tokens": [407, 341, 307, 577, 613, 21445, 589, 558, 13], "temperature": 0.0, "avg_logprob": -0.312974502300394, "compression_ratio": 1.3825503355704698, "no_speech_prob": 1.602804331923835e-06}, {"id": 920, "seek": 386932, "start": 3883.92, "end": 3896.36, "text": " So we can use JavaScript to call the API and we've got some examples.", "tokens": [407, 321, 393, 764, 15778, 281, 818, 264, 9362, 293, 321, 600, 658, 512, 5110, 13], "temperature": 0.0, "avg_logprob": -0.312974502300394, "compression_ratio": 1.3825503355704698, "no_speech_prob": 1.602804331923835e-06}, {"id": 921, "seek": 389636, "start": 3896.36, "end": 3906.96, "text": " So I've created a website and here is my website tiny pets and on this website as you can see", "tokens": [407, 286, 600, 2942, 257, 3144, 293, 510, 307, 452, 3144, 5870, 19897, 293, 322, 341, 3144, 382, 291, 393, 536], "temperature": 0.0, "avg_logprob": -0.23045960025510925, "compression_ratio": 1.5914634146341464, "no_speech_prob": 1.209859760820109e-06}, {"id": 922, "seek": 389636, "start": 3906.96, "end": 3910.52, "text": " it's not the most amazingly beautiful thing but it's a website.", "tokens": [309, 311, 406, 264, 881, 31762, 2238, 551, 457, 309, 311, 257, 3144, 13], "temperature": 0.0, "avg_logprob": -0.23045960025510925, "compression_ratio": 1.5914634146341464, "no_speech_prob": 1.209859760820109e-06}, {"id": 923, "seek": 389636, "start": 3910.52, "end": 3913.56, "text": " It's a start right and up here I've got some examples.", "tokens": [467, 311, 257, 722, 558, 293, 493, 510, 286, 600, 658, 512, 5110, 13], "temperature": 0.0, "avg_logprob": -0.23045960025510925, "compression_ratio": 1.5914634146341464, "no_speech_prob": 1.209859760820109e-06}, {"id": 924, "seek": 389636, "start": 3913.56, "end": 3923.0, "text": " Here you go single file click choose file click.", "tokens": [1692, 291, 352, 2167, 3991, 2052, 2826, 3991, 2052, 13], "temperature": 0.0, "avg_logprob": -0.23045960025510925, "compression_ratio": 1.5914634146341464, "no_speech_prob": 1.209859760820109e-06}, {"id": 925, "seek": 392300, "start": 3923.0, "end": 3928.24, "text": " And in this example I'm actually doing full pet classification so actually trained a model", "tokens": [400, 294, 341, 1365, 286, 478, 767, 884, 1577, 3817, 21538, 370, 767, 8895, 257, 2316], "temperature": 0.0, "avg_logprob": -0.17090395725134647, "compression_ratio": 1.456989247311828, "no_speech_prob": 3.905457106156973e-06}, {"id": 926, "seek": 392300, "start": 3928.24, "end": 3933.92, "text": " to classify breed which we'll talk about more next week rather than just dog versus cat.", "tokens": [281, 33872, 18971, 597, 321, 603, 751, 466, 544, 958, 1243, 2831, 813, 445, 3000, 5717, 3857, 13], "temperature": 0.0, "avg_logprob": -0.17090395725134647, "compression_ratio": 1.456989247311828, "no_speech_prob": 3.905457106156973e-06}, {"id": 927, "seek": 392300, "start": 3933.92, "end": 3941.84, "text": " So let's pick a particular breed and we run it.", "tokens": [407, 718, 311, 1888, 257, 1729, 18971, 293, 321, 1190, 309, 13], "temperature": 0.0, "avg_logprob": -0.17090395725134647, "compression_ratio": 1.456989247311828, "no_speech_prob": 3.905457106156973e-06}, {"id": 928, "seek": 392300, "start": 3941.84, "end": 3947.8, "text": " And there it is now not very amazing right.", "tokens": [400, 456, 309, 307, 586, 406, 588, 2243, 558, 13], "temperature": 0.0, "avg_logprob": -0.17090395725134647, "compression_ratio": 1.456989247311828, "no_speech_prob": 3.905457106156973e-06}, {"id": 929, "seek": 394780, "start": 3947.8, "end": 3953.44, "text": " But the fact is that this is now a JavaScript app means we have no restrictions about what", "tokens": [583, 264, 1186, 307, 300, 341, 307, 586, 257, 15778, 724, 1355, 321, 362, 572, 14191, 466, 437], "temperature": 0.0, "avg_logprob": -0.1318145313778439, "compression_ratio": 1.456989247311828, "no_speech_prob": 1.9333481304784073e-06}, {"id": 930, "seek": 394780, "start": 3953.44, "end": 3960.0, "text": " we can do and let's take a look at that HTML.", "tokens": [321, 393, 360, 293, 718, 311, 747, 257, 574, 412, 300, 17995, 13], "temperature": 0.0, "avg_logprob": -0.1318145313778439, "compression_ratio": 1.456989247311828, "no_speech_prob": 1.9333481304784073e-06}, {"id": 931, "seek": 394780, "start": 3960.0, "end": 3961.2200000000003, "text": " That's it.", "tokens": [663, 311, 309, 13], "temperature": 0.0, "avg_logprob": -0.1318145313778439, "compression_ratio": 1.456989247311828, "no_speech_prob": 1.9333481304784073e-06}, {"id": 932, "seek": 394780, "start": 3961.2200000000003, "end": 3969.4, "text": " It easily fits in a screen right and the basic steps are not crazy right.", "tokens": [467, 3612, 9001, 294, 257, 2568, 558, 293, 264, 3875, 4439, 366, 406, 3219, 558, 13], "temperature": 0.0, "avg_logprob": -0.1318145313778439, "compression_ratio": 1.456989247311828, "no_speech_prob": 1.9333481304784073e-06}, {"id": 933, "seek": 394780, "start": 3969.4, "end": 3975.2400000000002, "text": " It's basically we create an import for our photo.", "tokens": [467, 311, 1936, 321, 1884, 364, 974, 337, 527, 5052, 13], "temperature": 0.0, "avg_logprob": -0.1318145313778439, "compression_ratio": 1.456989247311828, "no_speech_prob": 1.9333481304784073e-06}, {"id": 934, "seek": 397524, "start": 3975.24, "end": 3979.8399999999997, "text": " We add an event listener that says when you change the photo call the read function.", "tokens": [492, 909, 364, 2280, 31569, 300, 1619, 562, 291, 1319, 264, 5052, 818, 264, 1401, 2445, 13], "temperature": 0.0, "avg_logprob": -0.17170977919069055, "compression_ratio": 1.75, "no_speech_prob": 2.9944128527858993e-06}, {"id": 935, "seek": 397524, "start": 3979.8399999999997, "end": 3989.16, "text": " The read function says create a file reader read the file and when you finished loading", "tokens": [440, 1401, 2445, 1619, 1884, 257, 3991, 15149, 1401, 264, 3991, 293, 562, 291, 4335, 15114], "temperature": 0.0, "avg_logprob": -0.17170977919069055, "compression_ratio": 1.75, "no_speech_prob": 2.9944128527858993e-06}, {"id": 936, "seek": 397524, "start": 3989.16, "end": 3993.8799999999997, "text": " call loaded and then loaded says fetch that.", "tokens": [818, 13210, 293, 550, 13210, 1619, 23673, 300, 13], "temperature": 0.0, "avg_logprob": -0.17170977919069055, "compression_ratio": 1.75, "no_speech_prob": 2.9944128527858993e-06}, {"id": 937, "seek": 397524, "start": 3993.8799999999997, "end": 4001.8399999999997, "text": " Now that path there is that path there right.", "tokens": [823, 300, 3100, 456, 307, 300, 3100, 456, 558, 13], "temperature": 0.0, "avg_logprob": -0.17170977919069055, "compression_ratio": 1.75, "no_speech_prob": 2.9944128527858993e-06}, {"id": 938, "seek": 397524, "start": 4001.8399999999997, "end": 4004.7599999999998, "text": " Except we're doing the full pets one.", "tokens": [16192, 321, 434, 884, 264, 1577, 19897, 472, 13], "temperature": 0.0, "avg_logprob": -0.17170977919069055, "compression_ratio": 1.75, "no_speech_prob": 2.9944128527858993e-06}, {"id": 939, "seek": 400476, "start": 4004.76, "end": 4013.88, "text": " So this is basically just copied and pasted from their sample.", "tokens": [407, 341, 307, 1936, 445, 25365, 293, 1791, 292, 490, 641, 6889, 13], "temperature": 0.0, "avg_logprob": -0.171875034470156, "compression_ratio": 1.5528846153846154, "no_speech_prob": 3.668835688586114e-06}, {"id": 940, "seek": 400476, "start": 4013.88, "end": 4021.6400000000003, "text": " And then grab the JSON and then grab from the data the first thing the confidences the", "tokens": [400, 550, 4444, 264, 31828, 293, 550, 4444, 490, 264, 1412, 264, 700, 551, 264, 1497, 41298, 264], "temperature": 0.0, "avg_logprob": -0.171875034470156, "compression_ratio": 1.5528846153846154, "no_speech_prob": 3.668835688586114e-06}, {"id": 941, "seek": 400476, "start": 4021.6400000000003, "end": 4025.7000000000003, "text": " label and then set the HTML.", "tokens": [7645, 293, 550, 992, 264, 17995, 13], "temperature": 0.0, "avg_logprob": -0.171875034470156, "compression_ratio": 1.5528846153846154, "no_speech_prob": 3.668835688586114e-06}, {"id": 942, "seek": 400476, "start": 4025.7000000000003, "end": 4029.5400000000004, "text": " So as you can see it's like OK if you haven't used JavaScript before these are all new things", "tokens": [407, 382, 291, 393, 536, 309, 311, 411, 2264, 498, 291, 2378, 380, 1143, 15778, 949, 613, 366, 439, 777, 721], "temperature": 0.0, "avg_logprob": -0.171875034470156, "compression_ratio": 1.5528846153846154, "no_speech_prob": 3.668835688586114e-06}, {"id": 943, "seek": 400476, "start": 4029.5400000000004, "end": 4032.4, "text": " right but they're not it's not harder than Python.", "tokens": [558, 457, 436, 434, 406, 309, 311, 406, 6081, 813, 15329, 13], "temperature": 0.0, "avg_logprob": -0.171875034470156, "compression_ratio": 1.5528846153846154, "no_speech_prob": 3.668835688586114e-06}, {"id": 944, "seek": 403240, "start": 4032.4, "end": 4036.64, "text": " It's just it's just another just another language to learn.", "tokens": [467, 311, 445, 309, 311, 445, 1071, 445, 1071, 2856, 281, 1466, 13], "temperature": 0.0, "avg_logprob": -0.22770571201405626, "compression_ratio": 1.6581632653061225, "no_speech_prob": 4.936938239552546e-06}, {"id": 945, "seek": 403240, "start": 4036.64, "end": 4040.6800000000003, "text": " And so from here you can start to build up right.", "tokens": [400, 370, 490, 510, 291, 393, 722, 281, 1322, 493, 558, 13], "temperature": 0.0, "avg_logprob": -0.22770571201405626, "compression_ratio": 1.6581632653061225, "no_speech_prob": 4.936938239552546e-06}, {"id": 946, "seek": 403240, "start": 4040.6800000000003, "end": 4046.56, "text": " So for example we've created a multi file version.", "tokens": [407, 337, 1365, 321, 600, 2942, 257, 4825, 3991, 3037, 13], "temperature": 0.0, "avg_logprob": -0.22770571201405626, "compression_ratio": 1.6581632653061225, "no_speech_prob": 4.936938239552546e-06}, {"id": 947, "seek": 403240, "start": 4046.56, "end": 4052.4, "text": " So with the multi file version let me show you multi file choose.", "tokens": [407, 365, 264, 4825, 3991, 3037, 718, 385, 855, 291, 4825, 3991, 2826, 13], "temperature": 0.0, "avg_logprob": -0.22770571201405626, "compression_ratio": 1.6581632653061225, "no_speech_prob": 4.936938239552546e-06}, {"id": 948, "seek": 403240, "start": 4052.4, "end": 4056.0, "text": " So we can now click a few.", "tokens": [407, 321, 393, 586, 2052, 257, 1326, 13], "temperature": 0.0, "avg_logprob": -0.22770571201405626, "compression_ratio": 1.6581632653061225, "no_speech_prob": 4.936938239552546e-06}, {"id": 949, "seek": 403240, "start": 4056.0, "end": 4060.88, "text": " So we've got a new fee and ragdoll a basset hound and some kind of cat.", "tokens": [407, 321, 600, 658, 257, 777, 12054, 293, 17539, 67, 1833, 257, 10136, 302, 276, 554, 293, 512, 733, 295, 3857, 13], "temperature": 0.0, "avg_logprob": -0.22770571201405626, "compression_ratio": 1.6581632653061225, "no_speech_prob": 4.936938239552546e-06}, {"id": 950, "seek": 406088, "start": 4060.88, "end": 4069.0, "text": " I'm not much of a cat person so he chose four files and bang they've all been classified.", "tokens": [286, 478, 406, 709, 295, 257, 3857, 954, 370, 415, 5111, 1451, 7098, 293, 8550, 436, 600, 439, 668, 20627, 13], "temperature": 0.0, "avg_logprob": -0.22131275247644494, "compression_ratio": 1.6415929203539823, "no_speech_prob": 5.771857559011551e-06}, {"id": 951, "seek": 406088, "start": 4069.0, "end": 4070.12, "text": " Apparently it's a Bengal.", "tokens": [16755, 309, 311, 257, 50221, 13], "temperature": 0.0, "avg_logprob": -0.22131275247644494, "compression_ratio": 1.6415929203539823, "no_speech_prob": 5.771857559011551e-06}, {"id": 952, "seek": 406088, "start": 4070.12, "end": 4071.12, "text": " I wouldn't know.", "tokens": [286, 2759, 380, 458, 13], "temperature": 0.0, "avg_logprob": -0.22131275247644494, "compression_ratio": 1.6415929203539823, "no_speech_prob": 5.771857559011551e-06}, {"id": 953, "seek": 406088, "start": 4071.12, "end": 4073.1600000000003, "text": " There's a new found.", "tokens": [821, 311, 257, 777, 1352, 13], "temperature": 0.0, "avg_logprob": -0.22131275247644494, "compression_ratio": 1.6415929203539823, "no_speech_prob": 5.771857559011551e-06}, {"id": 954, "seek": 406088, "start": 4073.1600000000003, "end": 4078.1, "text": " So there's the multi file version and if you look at the code it's not much more right.", "tokens": [407, 456, 311, 264, 4825, 3991, 3037, 293, 498, 291, 574, 412, 264, 3089, 309, 311, 406, 709, 544, 558, 13], "temperature": 0.0, "avg_logprob": -0.22131275247644494, "compression_ratio": 1.6415929203539823, "no_speech_prob": 5.771857559011551e-06}, {"id": 955, "seek": 406088, "start": 4078.1, "end": 4084.36, "text": " It's now just doing it's getting all the files and mapping them to read and now appending", "tokens": [467, 311, 586, 445, 884, 309, 311, 1242, 439, 264, 7098, 293, 18350, 552, 281, 1401, 293, 586, 724, 2029], "temperature": 0.0, "avg_logprob": -0.22131275247644494, "compression_ratio": 1.6415929203539823, "no_speech_prob": 5.771857559011551e-06}, {"id": 956, "seek": 406088, "start": 4084.36, "end": 4085.36, "text": " each one.", "tokens": [1184, 472, 13], "temperature": 0.0, "avg_logprob": -0.22131275247644494, "compression_ratio": 1.6415929203539823, "no_speech_prob": 5.771857559011551e-06}, {"id": 957, "seek": 406088, "start": 4085.36, "end": 4090.12, "text": " So not much more code at all.", "tokens": [407, 406, 709, 544, 3089, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.22131275247644494, "compression_ratio": 1.6415929203539823, "no_speech_prob": 5.771857559011551e-06}, {"id": 958, "seek": 409012, "start": 4090.12, "end": 4096.16, "text": " And as you might have seen on our site here there's a few more examples which is some", "tokens": [400, 382, 291, 1062, 362, 1612, 322, 527, 3621, 510, 456, 311, 257, 1326, 544, 5110, 597, 307, 512], "temperature": 0.0, "avg_logprob": -0.14959586845649467, "compression_ratio": 1.5848214285714286, "no_speech_prob": 1.3496977544491529e-06}, {"id": 959, "seek": 409012, "start": 4096.16, "end": 4101.64, "text": " of the community during the week has created their own versions.", "tokens": [295, 264, 1768, 1830, 264, 1243, 575, 2942, 641, 1065, 9606, 13], "temperature": 0.0, "avg_logprob": -0.14959586845649467, "compression_ratio": 1.5848214285714286, "no_speech_prob": 1.3496977544491529e-06}, {"id": 960, "seek": 409012, "start": 4101.64, "end": 4110.64, "text": " So this one here is I think this is yeah this is from one of the radio guys.", "tokens": [407, 341, 472, 510, 307, 286, 519, 341, 307, 1338, 341, 307, 490, 472, 295, 264, 6477, 1074, 13], "temperature": 0.0, "avg_logprob": -0.14959586845649467, "compression_ratio": 1.5848214285714286, "no_speech_prob": 1.3496977544491529e-06}, {"id": 961, "seek": 409012, "start": 4110.64, "end": 4113.84, "text": " They called it get to know your pet.", "tokens": [814, 1219, 309, 483, 281, 458, 428, 3817, 13], "temperature": 0.0, "avg_logprob": -0.14959586845649467, "compression_ratio": 1.5848214285714286, "no_speech_prob": 1.3496977544491529e-06}, {"id": 962, "seek": 409012, "start": 4113.84, "end": 4119.68, "text": " So if I choose a pet I kind of I really like this because it actually combines two models.", "tokens": [407, 498, 286, 2826, 257, 3817, 286, 733, 295, 286, 534, 411, 341, 570, 309, 767, 29520, 732, 5245, 13], "temperature": 0.0, "avg_logprob": -0.14959586845649467, "compression_ratio": 1.5848214285714286, "no_speech_prob": 1.3496977544491529e-06}, {"id": 963, "seek": 411968, "start": 4119.68, "end": 4123.96, "text": " So first of all it says oh it's a basset hound and then it lets me type in and ask things", "tokens": [407, 700, 295, 439, 309, 1619, 1954, 309, 311, 257, 10136, 302, 276, 554, 293, 550, 309, 6653, 385, 2010, 294, 293, 1029, 721], "temperature": 0.0, "avg_logprob": -0.2611602783203125, "compression_ratio": 1.4857142857142858, "no_speech_prob": 1.922274532262236e-05}, {"id": 964, "seek": 411968, "start": 4123.96, "end": 4132.08, "text": " about it so I can say oh what kind of tail does it have.", "tokens": [466, 309, 370, 286, 393, 584, 1954, 437, 733, 295, 6838, 775, 309, 362, 13], "temperature": 0.0, "avg_logprob": -0.2611602783203125, "compression_ratio": 1.4857142857142858, "no_speech_prob": 1.922274532262236e-05}, {"id": 965, "seek": 411968, "start": 4132.08, "end": 4139.76, "text": " Search and so that's now going to call an NLP model which asks about this.", "tokens": [17180, 293, 370, 300, 311, 586, 516, 281, 818, 364, 426, 45196, 2316, 597, 8962, 466, 341, 13], "temperature": 0.0, "avg_logprob": -0.2611602783203125, "compression_ratio": 1.4857142857142858, "no_speech_prob": 1.922274532262236e-05}, {"id": 966, "seek": 411968, "start": 4139.76, "end": 4142.64, "text": " It's a curved saber tail.", "tokens": [467, 311, 257, 24991, 12489, 6838, 13], "temperature": 0.0, "avg_logprob": -0.2611602783203125, "compression_ratio": 1.4857142857142858, "no_speech_prob": 1.922274532262236e-05}, {"id": 967, "seek": 411968, "start": 4142.64, "end": 4144.84, "text": " There we go.", "tokens": [821, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.2611602783203125, "compression_ratio": 1.4857142857142858, "no_speech_prob": 1.922274532262236e-05}, {"id": 968, "seek": 414484, "start": 4144.84, "end": 4151.8, "text": " What maintenance does it need.", "tokens": [708, 11258, 775, 309, 643, 13], "temperature": 0.0, "avg_logprob": -0.15785057903969124, "compression_ratio": 1.5517241379310345, "no_speech_prob": 3.84490203941823e-06}, {"id": 969, "seek": 414484, "start": 4151.8, "end": 4156.88, "text": " So again like here you can kind of see how a basset hound's ears must be cleaned and", "tokens": [407, 797, 411, 510, 291, 393, 733, 295, 536, 577, 257, 10136, 302, 276, 554, 311, 8798, 1633, 312, 16146, 293], "temperature": 0.0, "avg_logprob": -0.15785057903969124, "compression_ratio": 1.5517241379310345, "no_speech_prob": 3.84490203941823e-06}, {"id": 970, "seek": 414484, "start": 4156.88, "end": 4159.12, "text": " set it out frequently.", "tokens": [992, 309, 484, 10374, 13], "temperature": 0.0, "avg_logprob": -0.15785057903969124, "compression_ratio": 1.5517241379310345, "no_speech_prob": 3.84490203941823e-06}, {"id": 971, "seek": 414484, "start": 4159.12, "end": 4163.24, "text": " So this is like combining models so you can see this is something that you couldn't do", "tokens": [407, 341, 307, 411, 21928, 5245, 370, 291, 393, 536, 341, 307, 746, 300, 291, 2809, 380, 360], "temperature": 0.0, "avg_logprob": -0.15785057903969124, "compression_ratio": 1.5517241379310345, "no_speech_prob": 3.84490203941823e-06}, {"id": 972, "seek": 414484, "start": 4163.24, "end": 4169.58, "text": " with just a kind of a ready to go interface.", "tokens": [365, 445, 257, 733, 295, 257, 1919, 281, 352, 9226, 13], "temperature": 0.0, "avg_logprob": -0.15785057903969124, "compression_ratio": 1.5517241379310345, "no_speech_prob": 3.84490203941823e-06}, {"id": 973, "seek": 416958, "start": 4169.58, "end": 4176.68, "text": " And so the next thing I wanted to point out is how did we create the website that I showed", "tokens": [400, 370, 264, 958, 551, 286, 1415, 281, 935, 484, 307, 577, 630, 321, 1884, 264, 3144, 300, 286, 4712], "temperature": 0.0, "avg_logprob": -0.19830514192581178, "compression_ratio": 1.6123595505617978, "no_speech_prob": 8.013404112716671e-06}, {"id": 974, "seek": 416958, "start": 4176.68, "end": 4179.24, "text": " you how to create an HTML file.", "tokens": [291, 577, 281, 1884, 364, 17995, 3991, 13], "temperature": 0.0, "avg_logprob": -0.19830514192581178, "compression_ratio": 1.6123595505617978, "no_speech_prob": 8.013404112716671e-06}, {"id": 975, "seek": 416958, "start": 4179.24, "end": 4183.68, "text": " But like how do you create those and how do you make a website out of them.", "tokens": [583, 411, 577, 360, 291, 1884, 729, 293, 577, 360, 291, 652, 257, 3144, 484, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.19830514192581178, "compression_ratio": 1.6123595505617978, "no_speech_prob": 8.013404112716671e-06}, {"id": 976, "seek": 416958, "start": 4183.68, "end": 4185.24, "text": " Well watch this.", "tokens": [1042, 1159, 341, 13], "temperature": 0.0, "avg_logprob": -0.19830514192581178, "compression_ratio": 1.6123595505617978, "no_speech_prob": 8.013404112716671e-06}, {"id": 977, "seek": 416958, "start": 4185.24, "end": 4195.92, "text": " Let's here's here's a here's the source code to our most basic version.", "tokens": [961, 311, 510, 311, 510, 311, 257, 510, 311, 264, 4009, 3089, 281, 527, 881, 3875, 3037, 13], "temperature": 0.0, "avg_logprob": -0.19830514192581178, "compression_ratio": 1.6123595505617978, "no_speech_prob": 8.013404112716671e-06}, {"id": 978, "seek": 419592, "start": 4195.92, "end": 4201.72, "text": " So I could just save this.", "tokens": [407, 286, 727, 445, 3155, 341, 13], "temperature": 0.0, "avg_logprob": -0.3630470201080921, "compression_ratio": 1.3410852713178294, "no_speech_prob": 2.521549731682171e-06}, {"id": 979, "seek": 419592, "start": 4201.72, "end": 4209.8, "text": " There we go.", "tokens": [821, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.3630470201080921, "compression_ratio": 1.3410852713178294, "no_speech_prob": 2.521549731682171e-06}, {"id": 980, "seek": 419592, "start": 4209.8, "end": 4220.4400000000005, "text": " Okay so we could open that with Visual Studio code.", "tokens": [1033, 370, 321, 727, 1269, 300, 365, 23187, 13500, 3089, 13], "temperature": 0.0, "avg_logprob": -0.3630470201080921, "compression_ratio": 1.3410852713178294, "no_speech_prob": 2.521549731682171e-06}, {"id": 981, "seek": 419592, "start": 4220.4400000000005, "end": 4225.16, "text": " And what we could actually do is we could just use an explorer or Mac and finder.", "tokens": [400, 437, 321, 727, 767, 360, 307, 321, 727, 445, 764, 364, 39680, 420, 5707, 293, 915, 260, 13], "temperature": 0.0, "avg_logprob": -0.3630470201080921, "compression_ratio": 1.3410852713178294, "no_speech_prob": 2.521549731682171e-06}, {"id": 982, "seek": 422516, "start": 4225.16, "end": 4228.5199999999995, "text": " I could just double click on it.", "tokens": [286, 727, 445, 3834, 2052, 322, 309, 13], "temperature": 0.0, "avg_logprob": -0.1662483513355255, "compression_ratio": 1.7019607843137254, "no_speech_prob": 2.282752757309936e-05}, {"id": 983, "seek": 422516, "start": 4228.5199999999995, "end": 4229.5199999999995, "text": " And here it is.", "tokens": [400, 510, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.1662483513355255, "compression_ratio": 1.7019607843137254, "no_speech_prob": 2.282752757309936e-05}, {"id": 984, "seek": 422516, "start": 4229.5199999999995, "end": 4232.639999999999, "text": " It's a working app.", "tokens": [467, 311, 257, 1364, 724, 13], "temperature": 0.0, "avg_logprob": -0.1662483513355255, "compression_ratio": 1.7019607843137254, "no_speech_prob": 2.282752757309936e-05}, {"id": 985, "seek": 422516, "start": 4232.639999999999, "end": 4238.5199999999995, "text": " So you can see I don't need any software installed on my computer to use a JavaScript app.", "tokens": [407, 291, 393, 536, 286, 500, 380, 643, 604, 4722, 8899, 322, 452, 3820, 281, 764, 257, 15778, 724, 13], "temperature": 0.0, "avg_logprob": -0.1662483513355255, "compression_ratio": 1.7019607843137254, "no_speech_prob": 2.282752757309936e-05}, {"id": 986, "seek": 422516, "start": 4238.5199999999995, "end": 4240.28, "text": " It's a single file.", "tokens": [467, 311, 257, 2167, 3991, 13], "temperature": 0.0, "avg_logprob": -0.1662483513355255, "compression_ratio": 1.7019607843137254, "no_speech_prob": 2.282752757309936e-05}, {"id": 987, "seek": 422516, "start": 4240.28, "end": 4241.28, "text": " I just run it in a browser.", "tokens": [286, 445, 1190, 309, 294, 257, 11185, 13], "temperature": 0.0, "avg_logprob": -0.1662483513355255, "compression_ratio": 1.7019607843137254, "no_speech_prob": 2.282752757309936e-05}, {"id": 988, "seek": 422516, "start": 4241.28, "end": 4244.72, "text": " A browser is our complete execution environment.", "tokens": [316, 11185, 307, 527, 3566, 15058, 2823, 13], "temperature": 0.0, "avg_logprob": -0.1662483513355255, "compression_ratio": 1.7019607843137254, "no_speech_prob": 2.282752757309936e-05}, {"id": 989, "seek": 422516, "start": 4244.72, "end": 4245.72, "text": " It's got a debugger.", "tokens": [467, 311, 658, 257, 24083, 1321, 13], "temperature": 0.0, "avg_logprob": -0.1662483513355255, "compression_ratio": 1.7019607843137254, "no_speech_prob": 2.282752757309936e-05}, {"id": 990, "seek": 422516, "start": 4245.72, "end": 4247.12, "text": " It's got the whole thing.", "tokens": [467, 311, 658, 264, 1379, 551, 13], "temperature": 0.0, "avg_logprob": -0.1662483513355255, "compression_ratio": 1.7019607843137254, "no_speech_prob": 2.282752757309936e-05}, {"id": 991, "seek": 422516, "start": 4247.12, "end": 4252.72, "text": " So here you can see it's just calling out to this external hugging faces end point so", "tokens": [407, 510, 291, 393, 536, 309, 311, 445, 5141, 484, 281, 341, 8320, 41706, 8475, 917, 935, 370], "temperature": 0.0, "avg_logprob": -0.1662483513355255, "compression_ratio": 1.7019607843137254, "no_speech_prob": 2.282752757309936e-05}, {"id": 992, "seek": 422516, "start": 4252.72, "end": 4255.0, "text": " I can do it all sitting here on my computer.", "tokens": [286, 393, 360, 309, 439, 3798, 510, 322, 452, 3820, 13], "temperature": 0.0, "avg_logprob": -0.1662483513355255, "compression_ratio": 1.7019607843137254, "no_speech_prob": 2.282752757309936e-05}, {"id": 993, "seek": 425500, "start": 4255.0, "end": 4262.96, "text": " So once I've got my HTML file that's working fine on my computer in VS code how do I then", "tokens": [407, 1564, 286, 600, 658, 452, 17995, 3991, 300, 311, 1364, 2489, 322, 452, 3820, 294, 25091, 3089, 577, 360, 286, 550], "temperature": 0.0, "avg_logprob": -0.15603835007240033, "compression_ratio": 1.5116279069767442, "no_speech_prob": 3.187544734828407e-06}, {"id": 994, "seek": 425500, "start": 4262.96, "end": 4266.92, "text": " put it on the web so that other people can use it.", "tokens": [829, 309, 322, 264, 3670, 370, 300, 661, 561, 393, 764, 309, 13], "temperature": 0.0, "avg_logprob": -0.15603835007240033, "compression_ratio": 1.5116279069767442, "no_speech_prob": 3.187544734828407e-06}, {"id": 995, "seek": 425500, "start": 4266.92, "end": 4269.04, "text": " Again the whole thing's free.", "tokens": [3764, 264, 1379, 551, 311, 1737, 13], "temperature": 0.0, "avg_logprob": -0.15603835007240033, "compression_ratio": 1.5116279069767442, "no_speech_prob": 3.187544734828407e-06}, {"id": 996, "seek": 425500, "start": 4269.04, "end": 4274.44, "text": " There's a really cool thing called GitHub pages which basically will host your website", "tokens": [821, 311, 257, 534, 1627, 551, 1219, 23331, 7183, 597, 1936, 486, 3975, 428, 3144], "temperature": 0.0, "avg_logprob": -0.15603835007240033, "compression_ratio": 1.5116279069767442, "no_speech_prob": 3.187544734828407e-06}, {"id": 997, "seek": 425500, "start": 4274.44, "end": 4275.48, "text": " for you.", "tokens": [337, 291, 13], "temperature": 0.0, "avg_logprob": -0.15603835007240033, "compression_ratio": 1.5116279069767442, "no_speech_prob": 3.187544734828407e-06}, {"id": 998, "seek": 425500, "start": 4275.48, "end": 4283.2, "text": " And because it's just JavaScript it'll all work just fine.", "tokens": [400, 570, 309, 311, 445, 15778, 309, 603, 439, 589, 445, 2489, 13], "temperature": 0.0, "avg_logprob": -0.15603835007240033, "compression_ratio": 1.5116279069767442, "no_speech_prob": 3.187544734828407e-06}, {"id": 999, "seek": 428320, "start": 4283.2, "end": 4288.4, "text": " The easiest way to create a GitHub pages site in my opinion is to use something called fast", "tokens": [440, 12889, 636, 281, 1884, 257, 23331, 7183, 3621, 294, 452, 4800, 307, 281, 764, 746, 1219, 2370], "temperature": 0.0, "avg_logprob": -0.23292848798963758, "compression_ratio": 1.5343137254901962, "no_speech_prob": 2.4060814212134574e-06}, {"id": 1000, "seek": 428320, "start": 4288.4, "end": 4293.4, "text": " pages which is a fast AI thing.", "tokens": [7183, 597, 307, 257, 2370, 7318, 551, 13], "temperature": 0.0, "avg_logprob": -0.23292848798963758, "compression_ratio": 1.5343137254901962, "no_speech_prob": 2.4060814212134574e-06}, {"id": 1001, "seek": 428320, "start": 4293.4, "end": 4303.639999999999, "text": " And basically all you do is you follow the setup process.", "tokens": [400, 1936, 439, 291, 360, 307, 291, 1524, 264, 8657, 1399, 13], "temperature": 0.0, "avg_logprob": -0.23292848798963758, "compression_ratio": 1.5343137254901962, "no_speech_prob": 2.4060814212134574e-06}, {"id": 1002, "seek": 428320, "start": 4303.639999999999, "end": 4304.639999999999, "text": " So first it does it.", "tokens": [407, 700, 309, 775, 309, 13], "temperature": 0.0, "avg_logprob": -0.23292848798963758, "compression_ratio": 1.5343137254901962, "no_speech_prob": 2.4060814212134574e-06}, {"id": 1003, "seek": 428320, "start": 4304.639999999999, "end": 4305.639999999999, "text": " Let's just go through it.", "tokens": [961, 311, 445, 352, 807, 309, 13], "temperature": 0.0, "avg_logprob": -0.23292848798963758, "compression_ratio": 1.5343137254901962, "no_speech_prob": 2.4060814212134574e-06}, {"id": 1004, "seek": 428320, "start": 4305.639999999999, "end": 4308.0, "text": " So it says generate a copy by clicking on this link.", "tokens": [407, 309, 1619, 8460, 257, 5055, 538, 9697, 322, 341, 2113, 13], "temperature": 0.0, "avg_logprob": -0.23292848798963758, "compression_ratio": 1.5343137254901962, "no_speech_prob": 2.4060814212134574e-06}, {"id": 1005, "seek": 428320, "start": 4308.0, "end": 4310.84, "text": " So I click the link.", "tokens": [407, 286, 2052, 264, 2113, 13], "temperature": 0.0, "avg_logprob": -0.23292848798963758, "compression_ratio": 1.5343137254901962, "no_speech_prob": 2.4060814212134574e-06}, {"id": 1006, "seek": 428320, "start": 4310.84, "end": 4312.96, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.23292848798963758, "compression_ratio": 1.5343137254901962, "no_speech_prob": 2.4060814212134574e-06}, {"id": 1007, "seek": 431296, "start": 4312.96, "end": 4320.56, "text": " Okay give it a name.", "tokens": [1033, 976, 309, 257, 1315, 13], "temperature": 0.0, "avg_logprob": -0.2951586493130388, "compression_ratio": 1.5126903553299493, "no_speech_prob": 2.332060830667615e-06}, {"id": 1008, "seek": 431296, "start": 4320.56, "end": 4321.56, "text": " I try to make everything public.", "tokens": [286, 853, 281, 652, 1203, 1908, 13], "temperature": 0.0, "avg_logprob": -0.2951586493130388, "compression_ratio": 1.5126903553299493, "no_speech_prob": 2.332060830667615e-06}, {"id": 1009, "seek": 431296, "start": 4321.56, "end": 4323.32, "text": " I always think it's good practice.", "tokens": [286, 1009, 519, 309, 311, 665, 3124, 13], "temperature": 0.0, "avg_logprob": -0.2951586493130388, "compression_ratio": 1.5126903553299493, "no_speech_prob": 2.332060830667615e-06}, {"id": 1010, "seek": 431296, "start": 4323.32, "end": 4329.6, "text": " You don't have to create repo.", "tokens": [509, 500, 380, 362, 281, 1884, 49040, 13], "temperature": 0.0, "avg_logprob": -0.2951586493130388, "compression_ratio": 1.5126903553299493, "no_speech_prob": 2.332060830667615e-06}, {"id": 1011, "seek": 431296, "start": 4329.6, "end": 4331.16, "text": " Generating okay.", "tokens": [15409, 990, 1392, 13], "temperature": 0.0, "avg_logprob": -0.2951586493130388, "compression_ratio": 1.5126903553299493, "no_speech_prob": 2.332060830667615e-06}, {"id": 1012, "seek": 431296, "start": 4331.16, "end": 4332.44, "text": " And then there's basically two more steps.", "tokens": [400, 550, 456, 311, 1936, 732, 544, 4439, 13], "temperature": 0.0, "avg_logprob": -0.2951586493130388, "compression_ratio": 1.5126903553299493, "no_speech_prob": 2.332060830667615e-06}, {"id": 1013, "seek": 431296, "start": 4332.44, "end": 4333.44, "text": " It takes about five minutes.", "tokens": [467, 2516, 466, 1732, 2077, 13], "temperature": 0.0, "avg_logprob": -0.2951586493130388, "compression_ratio": 1.5126903553299493, "no_speech_prob": 2.332060830667615e-06}, {"id": 1014, "seek": 431296, "start": 4333.44, "end": 4340.44, "text": " We don't have five minutes so I'll show you the one that I've already built which is fast", "tokens": [492, 500, 380, 362, 1732, 2077, 370, 286, 603, 855, 291, 264, 472, 300, 286, 600, 1217, 3094, 597, 307, 2370], "temperature": 0.0, "avg_logprob": -0.2951586493130388, "compression_ratio": 1.5126903553299493, "no_speech_prob": 2.332060830667615e-06}, {"id": 1015, "seek": 434044, "start": 4340.44, "end": 4344.5199999999995, "text": " AI slash tiny pets.", "tokens": [7318, 17330, 5870, 19897, 13], "temperature": 0.0, "avg_logprob": -0.1378207832875878, "compression_ratio": 1.6596638655462186, "no_speech_prob": 7.646354788448662e-06}, {"id": 1016, "seek": 434044, "start": 4344.5199999999995, "end": 4350.16, "text": " And so once it's done you'll basically end up with this empty site which again you just", "tokens": [400, 370, 1564, 309, 311, 1096, 291, 603, 1936, 917, 493, 365, 341, 6707, 3621, 597, 797, 291, 445], "temperature": 0.0, "avg_logprob": -0.1378207832875878, "compression_ratio": 1.6596638655462186, "no_speech_prob": 7.646354788448662e-06}, {"id": 1017, "seek": 434044, "start": 4350.16, "end": 4356.16, "text": " go code open with GitHub desktop or open with Visual Studio whatever.", "tokens": [352, 3089, 1269, 365, 23331, 14502, 420, 1269, 365, 23187, 13500, 2035, 13], "temperature": 0.0, "avg_logprob": -0.1378207832875878, "compression_ratio": 1.6596638655462186, "no_speech_prob": 7.646354788448662e-06}, {"id": 1018, "seek": 434044, "start": 4356.16, "end": 4360.7, "text": " So open with GitHub desktop or you can copy and paste this to your terminal.", "tokens": [407, 1269, 365, 23331, 14502, 420, 291, 393, 5055, 293, 9163, 341, 281, 428, 14709, 13], "temperature": 0.0, "avg_logprob": -0.1378207832875878, "compression_ratio": 1.6596638655462186, "no_speech_prob": 7.646354788448662e-06}, {"id": 1019, "seek": 434044, "start": 4360.7, "end": 4364.139999999999, "text": " And so any one of those is going to get you this whole thing on your computer.", "tokens": [400, 370, 604, 472, 295, 729, 307, 516, 281, 483, 291, 341, 1379, 551, 322, 428, 3820, 13], "temperature": 0.0, "avg_logprob": -0.1378207832875878, "compression_ratio": 1.6596638655462186, "no_speech_prob": 7.646354788448662e-06}, {"id": 1020, "seek": 434044, "start": 4364.139999999999, "end": 4369.4, "text": " You can save your HTML files there push it back up to GitHub.", "tokens": [509, 393, 3155, 428, 17995, 7098, 456, 2944, 309, 646, 493, 281, 23331, 13], "temperature": 0.0, "avg_logprob": -0.1378207832875878, "compression_ratio": 1.6596638655462186, "no_speech_prob": 7.646354788448662e-06}, {"id": 1021, "seek": 436940, "start": 4369.4, "end": 4375.32, "text": " And what you'll find is we've we've real fast pages will show you the link to the website", "tokens": [400, 437, 291, 603, 915, 307, 321, 600, 321, 600, 957, 2370, 7183, 486, 855, 291, 264, 2113, 281, 264, 3144], "temperature": 0.0, "avg_logprob": -0.12279982660330978, "compression_ratio": 1.8018867924528301, "no_speech_prob": 3.3405026442778762e-06}, {"id": 1022, "seek": 436940, "start": 4375.32, "end": 4377.08, "text": " that is created for you.", "tokens": [300, 307, 2942, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.12279982660330978, "compression_ratio": 1.8018867924528301, "no_speech_prob": 3.3405026442778762e-06}, {"id": 1023, "seek": 436940, "start": 4377.08, "end": 4384.0, "text": " Now the website that's created for you you can make it look however you want using something", "tokens": [823, 264, 3144, 300, 311, 2942, 337, 291, 291, 393, 652, 309, 574, 4461, 291, 528, 1228, 746], "temperature": 0.0, "avg_logprob": -0.12279982660330978, "compression_ratio": 1.8018867924528301, "no_speech_prob": 3.3405026442778762e-06}, {"id": 1024, "seek": 436940, "start": 4384.0, "end": 4385.0, "text": " called a theme.", "tokens": [1219, 257, 6314, 13], "temperature": 0.0, "avg_logprob": -0.12279982660330978, "compression_ratio": 1.8018867924528301, "no_speech_prob": 3.3405026442778762e-06}, {"id": 1025, "seek": 436940, "start": 4385.0, "end": 4390.679999999999, "text": " So you'll see it's created a file called config dot yaml where you can pick a theme.", "tokens": [407, 291, 603, 536, 309, 311, 2942, 257, 3991, 1219, 6662, 5893, 288, 335, 75, 689, 291, 393, 1888, 257, 6314, 13], "temperature": 0.0, "avg_logprob": -0.12279982660330978, "compression_ratio": 1.8018867924528301, "no_speech_prob": 3.3405026442778762e-06}, {"id": 1026, "seek": 436940, "start": 4390.679999999999, "end": 4398.4, "text": " So in this case I picked a theme called Alembic for no particular reason.", "tokens": [407, 294, 341, 1389, 286, 6183, 257, 6314, 1219, 9366, 2504, 299, 337, 572, 1729, 1778, 13], "temperature": 0.0, "avg_logprob": -0.12279982660330978, "compression_ratio": 1.8018867924528301, "no_speech_prob": 3.3405026442778762e-06}, {"id": 1027, "seek": 439840, "start": 4398.4, "end": 4401.759999999999, "text": " So GitHub pages uses something called Jekyll.", "tokens": [407, 23331, 7183, 4960, 746, 1219, 508, 916, 34353, 13], "temperature": 0.0, "avg_logprob": -0.12525244162116253, "compression_ratio": 1.631578947368421, "no_speech_prob": 1.994712420128053e-06}, {"id": 1028, "seek": 439840, "start": 4401.759999999999, "end": 4405.16, "text": " And so any Jekyll theme will basically work.", "tokens": [400, 370, 604, 508, 916, 34353, 6314, 486, 1936, 589, 13], "temperature": 0.0, "avg_logprob": -0.12525244162116253, "compression_ratio": 1.631578947368421, "no_speech_prob": 1.994712420128053e-06}, {"id": 1029, "seek": 439840, "start": 4405.16, "end": 4407.2, "text": " So I picked out this theme.", "tokens": [407, 286, 6183, 484, 341, 6314, 13], "temperature": 0.0, "avg_logprob": -0.12525244162116253, "compression_ratio": 1.631578947368421, "no_speech_prob": 1.994712420128053e-06}, {"id": 1030, "seek": 439840, "start": 4407.2, "end": 4413.839999999999, "text": " And so as a result when I now save things into this repo they will automatically appear", "tokens": [400, 370, 382, 257, 1874, 562, 286, 586, 3155, 721, 666, 341, 49040, 436, 486, 6772, 4204], "temperature": 0.0, "avg_logprob": -0.12525244162116253, "compression_ratio": 1.631578947368421, "no_speech_prob": 1.994712420128053e-06}, {"id": 1031, "seek": 439840, "start": 4413.839999999999, "end": 4419.74, "text": " in this website and the files automatically appear up here in this list.", "tokens": [294, 341, 3144, 293, 264, 7098, 6772, 4204, 493, 510, 294, 341, 1329, 13], "temperature": 0.0, "avg_logprob": -0.12525244162116253, "compression_ratio": 1.631578947368421, "no_speech_prob": 1.994712420128053e-06}, {"id": 1032, "seek": 441974, "start": 4419.74, "end": 4429.0, "text": " So if you look at my index that's the home page the entire file is just this.", "tokens": [407, 498, 291, 574, 412, 452, 8186, 300, 311, 264, 1280, 3028, 264, 2302, 3991, 307, 445, 341, 13], "temperature": 0.0, "avg_logprob": -0.11966969327228826, "compression_ratio": 1.5505050505050506, "no_speech_prob": 2.52155427915568e-06}, {"id": 1033, "seek": 441974, "start": 4429.0, "end": 4434.7, "text": " The only slightly weird thing is at the top of every GitHub pages file you have to have", "tokens": [440, 787, 4748, 3657, 551, 307, 412, 264, 1192, 295, 633, 23331, 7183, 3991, 291, 362, 281, 362], "temperature": 0.0, "avg_logprob": -0.11966969327228826, "compression_ratio": 1.5505050505050506, "no_speech_prob": 2.52155427915568e-06}, {"id": 1034, "seek": 441974, "start": 4434.7, "end": 4439.28, "text": " three dashes title and layout and three dashes.", "tokens": [1045, 8240, 279, 4876, 293, 13333, 293, 1045, 8240, 279, 13], "temperature": 0.0, "avg_logprob": -0.11966969327228826, "compression_ratio": 1.5505050505050506, "no_speech_prob": 2.52155427915568e-06}, {"id": 1035, "seek": 441974, "start": 4439.28, "end": 4442.28, "text": " It's called front matter.", "tokens": [467, 311, 1219, 1868, 1871, 13], "temperature": 0.0, "avg_logprob": -0.11966969327228826, "compression_ratio": 1.5505050505050506, "no_speech_prob": 2.52155427915568e-06}, {"id": 1036, "seek": 441974, "start": 4442.28, "end": 4449.28, "text": " And so once you do that and save it it will appear in your website.", "tokens": [400, 370, 1564, 291, 360, 300, 293, 3155, 309, 309, 486, 4204, 294, 428, 3144, 13], "temperature": 0.0, "avg_logprob": -0.11966969327228826, "compression_ratio": 1.5505050505050506, "no_speech_prob": 2.52155427915568e-06}, {"id": 1037, "seek": 444928, "start": 4449.28, "end": 4454.04, "text": " So something else I did then I was like okay well that's all very well that Fastai has", "tokens": [407, 746, 1646, 286, 630, 550, 286, 390, 411, 1392, 731, 300, 311, 439, 588, 731, 300, 15968, 1301, 575], "temperature": 0.0, "avg_logprob": -0.18354131354660283, "compression_ratio": 1.6704545454545454, "no_speech_prob": 1.07830528577324e-05}, {"id": 1038, "seek": 444928, "start": 4454.04, "end": 4456.92, "text": " created this website but I don't really like what it looks like.", "tokens": [2942, 341, 3144, 457, 286, 500, 380, 534, 411, 437, 309, 1542, 411, 13], "temperature": 0.0, "avg_logprob": -0.18354131354660283, "compression_ratio": 1.6704545454545454, "no_speech_prob": 1.07830528577324e-05}, {"id": 1039, "seek": 444928, "start": 4456.92, "end": 4458.5599999999995, "text": " I want to create a different version.", "tokens": [286, 528, 281, 1884, 257, 819, 3037, 13], "temperature": 0.0, "avg_logprob": -0.18354131354660283, "compression_ratio": 1.6704545454545454, "no_speech_prob": 1.07830528577324e-05}, {"id": 1040, "seek": 444928, "start": 4458.5599999999995, "end": 4459.88, "text": " No worries.", "tokens": [883, 16340, 13], "temperature": 0.0, "avg_logprob": -0.18354131354660283, "compression_ratio": 1.6704545454545454, "no_speech_prob": 1.07830528577324e-05}, {"id": 1041, "seek": 444928, "start": 4459.88, "end": 4464.32, "text": " You can go to Fastai tiny pets and click fork.", "tokens": [509, 393, 352, 281, 15968, 1301, 5870, 19897, 293, 2052, 17716, 13], "temperature": 0.0, "avg_logprob": -0.18354131354660283, "compression_ratio": 1.6704545454545454, "no_speech_prob": 1.07830528577324e-05}, {"id": 1042, "seek": 444928, "start": 4464.32, "end": 4467.8, "text": " And when you click fork it's going to create your own copy.", "tokens": [400, 562, 291, 2052, 17716, 309, 311, 516, 281, 1884, 428, 1065, 5055, 13], "temperature": 0.0, "avg_logprob": -0.18354131354660283, "compression_ratio": 1.6704545454545454, "no_speech_prob": 1.07830528577324e-05}, {"id": 1043, "seek": 444928, "start": 4467.8, "end": 4474.38, "text": " So I did that under my personal account which is JP H00.", "tokens": [407, 286, 630, 300, 833, 452, 2973, 2696, 597, 307, 34336, 389, 628, 13], "temperature": 0.0, "avg_logprob": -0.18354131354660283, "compression_ratio": 1.6704545454545454, "no_speech_prob": 1.07830528577324e-05}, {"id": 1044, "seek": 444928, "start": 4474.38, "end": 4478.42, "text": " And look now I've got my own version of it and now I can make changes here.", "tokens": [400, 574, 586, 286, 600, 658, 452, 1065, 3037, 295, 309, 293, 586, 286, 393, 652, 2962, 510, 13], "temperature": 0.0, "avg_logprob": -0.18354131354660283, "compression_ratio": 1.6704545454545454, "no_speech_prob": 1.07830528577324e-05}, {"id": 1045, "seek": 447842, "start": 4478.42, "end": 4479.58, "text": " So I made a few changes.", "tokens": [407, 286, 1027, 257, 1326, 2962, 13], "temperature": 0.0, "avg_logprob": -0.15249395370483398, "compression_ratio": 1.744360902255639, "no_speech_prob": 4.0294085010827985e-06}, {"id": 1046, "seek": 447842, "start": 4479.58, "end": 4488.08, "text": " One change I made was I went to config.yml and I changed the theme to pages themes hacker.", "tokens": [1485, 1319, 286, 1027, 390, 286, 1437, 281, 6662, 13, 4199, 75, 293, 286, 3105, 264, 6314, 281, 7183, 13544, 38155, 13], "temperature": 0.0, "avg_logprob": -0.15249395370483398, "compression_ratio": 1.744360902255639, "no_speech_prob": 4.0294085010827985e-06}, {"id": 1047, "seek": 447842, "start": 4488.08, "end": 4492.8, "text": " So once you fork one thing you have to do which normally FastPages does for you is you", "tokens": [407, 1564, 291, 17716, 472, 551, 291, 362, 281, 360, 597, 5646, 15968, 47, 1660, 775, 337, 291, 307, 291], "temperature": 0.0, "avg_logprob": -0.15249395370483398, "compression_ratio": 1.744360902255639, "no_speech_prob": 4.0294085010827985e-06}, {"id": 1048, "seek": 447842, "start": 4492.8, "end": 4499.0, "text": " do have to go to settings and click pages and actually enable GitHub pages.", "tokens": [360, 362, 281, 352, 281, 6257, 293, 2052, 7183, 293, 767, 9528, 23331, 7183, 13], "temperature": 0.0, "avg_logprob": -0.15249395370483398, "compression_ratio": 1.744360902255639, "no_speech_prob": 4.0294085010827985e-06}, {"id": 1049, "seek": 447842, "start": 4499.0, "end": 4501.58, "text": " So you basically have to by default it's turned off.", "tokens": [407, 291, 1936, 362, 281, 538, 7576, 309, 311, 3574, 766, 13], "temperature": 0.0, "avg_logprob": -0.15249395370483398, "compression_ratio": 1.744360902255639, "no_speech_prob": 4.0294085010827985e-06}, {"id": 1050, "seek": 447842, "start": 4501.58, "end": 4503.14, "text": " So here you'll just have to turn it on.", "tokens": [407, 510, 291, 603, 445, 362, 281, 1261, 309, 322, 13], "temperature": 0.0, "avg_logprob": -0.15249395370483398, "compression_ratio": 1.744360902255639, "no_speech_prob": 4.0294085010827985e-06}, {"id": 1051, "seek": 447842, "start": 4503.14, "end": 4508.4, "text": " So use the master branch root save and then it'll say no worries it's ready to be published.", "tokens": [407, 764, 264, 4505, 9819, 5593, 3155, 293, 550, 309, 603, 584, 572, 16340, 309, 311, 1919, 281, 312, 6572, 13], "temperature": 0.0, "avg_logprob": -0.15249395370483398, "compression_ratio": 1.744360902255639, "no_speech_prob": 4.0294085010827985e-06}, {"id": 1052, "seek": 450840, "start": 4508.4, "end": 4512.719999999999, "text": " And so I changed the config.yml file to point at a different theme.", "tokens": [400, 370, 286, 3105, 264, 6662, 13, 4199, 75, 3991, 281, 935, 412, 257, 819, 6314, 13], "temperature": 0.0, "avg_logprob": -0.15410792950502375, "compression_ratio": 1.53125, "no_speech_prob": 6.048853265383514e-06}, {"id": 1053, "seek": 450840, "start": 4512.719999999999, "end": 4518.0, "text": " And so if you look at now the JP H's tiny pets it's different.", "tokens": [400, 370, 498, 291, 574, 412, 586, 264, 34336, 389, 311, 5870, 19897, 309, 311, 819, 13], "temperature": 0.0, "avg_logprob": -0.15410792950502375, "compression_ratio": 1.53125, "no_speech_prob": 6.048853265383514e-06}, {"id": 1054, "seek": 450840, "start": 4518.0, "end": 4519.0, "text": " OK.", "tokens": [2264, 13], "temperature": 0.0, "avg_logprob": -0.15410792950502375, "compression_ratio": 1.53125, "no_speech_prob": 6.048853265383514e-06}, {"id": 1055, "seek": 450840, "start": 4519.0, "end": 4525.36, "text": " It's got the same info but it's much more hackerish because JP H00 is a serious hacker", "tokens": [467, 311, 658, 264, 912, 13614, 457, 309, 311, 709, 544, 38155, 742, 570, 34336, 389, 628, 307, 257, 3156, 38155], "temperature": 0.0, "avg_logprob": -0.15410792950502375, "compression_ratio": 1.53125, "no_speech_prob": 6.048853265383514e-06}, {"id": 1056, "seek": 450840, "start": 4525.36, "end": 4530.16, "text": " as you can tell from his website.", "tokens": [382, 291, 393, 980, 490, 702, 3144, 13], "temperature": 0.0, "avg_logprob": -0.15410792950502375, "compression_ratio": 1.53125, "no_speech_prob": 6.048853265383514e-06}, {"id": 1057, "seek": 450840, "start": 4530.16, "end": 4537.5199999999995, "text": " So anyway look it's a very brief taste of this kind of world of JavaScript and websites", "tokens": [407, 4033, 574, 309, 311, 257, 588, 5353, 3939, 295, 341, 733, 295, 1002, 295, 15778, 293, 12891], "temperature": 0.0, "avg_logprob": -0.15410792950502375, "compression_ratio": 1.53125, "no_speech_prob": 6.048853265383514e-06}, {"id": 1058, "seek": 453752, "start": 4537.52, "end": 4538.72, "text": " and so forth.", "tokens": [293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.17325354557411343, "compression_ratio": 1.663594470046083, "no_speech_prob": 5.255274118098896e-06}, {"id": 1059, "seek": 453752, "start": 4538.72, "end": 4545.72, "text": " But I wanted to give you a sense of like you know you don't need any money.", "tokens": [583, 286, 1415, 281, 976, 291, 257, 2020, 295, 411, 291, 458, 291, 500, 380, 643, 604, 1460, 13], "temperature": 0.0, "avg_logprob": -0.17325354557411343, "compression_ratio": 1.663594470046083, "no_speech_prob": 5.255274118098896e-06}, {"id": 1060, "seek": 453752, "start": 4545.72, "end": 4549.240000000001, "text": " You don't need any ideas.", "tokens": [509, 500, 380, 643, 604, 3487, 13], "temperature": 0.0, "avg_logprob": -0.17325354557411343, "compression_ratio": 1.663594470046083, "no_speech_prob": 5.255274118098896e-06}, {"id": 1061, "seek": 453752, "start": 4549.240000000001, "end": 4556.92, "text": " You know you don't really need much code to get started with writing your own web apps.", "tokens": [509, 458, 291, 500, 380, 534, 643, 709, 3089, 281, 483, 1409, 365, 3579, 428, 1065, 3670, 7733, 13], "temperature": 0.0, "avg_logprob": -0.17325354557411343, "compression_ratio": 1.663594470046083, "no_speech_prob": 5.255274118098896e-06}, {"id": 1062, "seek": 453752, "start": 4556.92, "end": 4561.360000000001, "text": " And thanks to hugging face spaces you know they'll even host your model for you.", "tokens": [400, 3231, 281, 41706, 1851, 7673, 291, 458, 436, 603, 754, 3975, 428, 2316, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.17325354557411343, "compression_ratio": 1.663594470046083, "no_speech_prob": 5.255274118098896e-06}, {"id": 1063, "seek": 453752, "start": 4561.360000000001, "end": 4566.160000000001, "text": " And all you need to do is just have the magic string as a thing to call.", "tokens": [400, 439, 291, 643, 281, 360, 307, 445, 362, 264, 5585, 6798, 382, 257, 551, 281, 818, 13], "temperature": 0.0, "avg_logprob": -0.17325354557411343, "compression_ratio": 1.663594470046083, "no_speech_prob": 5.255274118098896e-06}, {"id": 1064, "seek": 453752, "start": 4566.160000000001, "end": 4567.400000000001, "text": " OK.", "tokens": [2264, 13], "temperature": 0.0, "avg_logprob": -0.17325354557411343, "compression_ratio": 1.663594470046083, "no_speech_prob": 5.255274118098896e-06}, {"id": 1065, "seek": 456740, "start": 4567.4, "end": 4572.16, "text": " So signing out hacker Jeremy Howard.", "tokens": [407, 13393, 484, 38155, 17809, 17626, 13], "temperature": 0.0, "avg_logprob": -0.13532055125517004, "compression_ratio": 1.8134328358208955, "no_speech_prob": 1.3630772627948318e-05}, {"id": 1066, "seek": 456740, "start": 4572.16, "end": 4574.4, "text": " Thanks very much for watching.", "tokens": [2561, 588, 709, 337, 1976, 13], "temperature": 0.0, "avg_logprob": -0.13532055125517004, "compression_ratio": 1.8134328358208955, "no_speech_prob": 1.3630772627948318e-05}, {"id": 1067, "seek": 456740, "start": 4574.4, "end": 4578.24, "text": " And in the next lesson we're going to be digging into some natural language processing.", "tokens": [400, 294, 264, 958, 6898, 321, 434, 516, 281, 312, 17343, 666, 512, 3303, 2856, 9007, 13], "temperature": 0.0, "avg_logprob": -0.13532055125517004, "compression_ratio": 1.8134328358208955, "no_speech_prob": 1.3630772627948318e-05}, {"id": 1068, "seek": 456740, "start": 4578.24, "end": 4582.0, "text": " We're going to be doing some of the same stuff but we're going to be doing it with language", "tokens": [492, 434, 516, 281, 312, 884, 512, 295, 264, 912, 1507, 457, 321, 434, 516, 281, 312, 884, 309, 365, 2856], "temperature": 0.0, "avg_logprob": -0.13532055125517004, "compression_ratio": 1.8134328358208955, "no_speech_prob": 1.3630772627948318e-05}, {"id": 1069, "seek": 456740, "start": 4582.0, "end": 4583.5599999999995, "text": " rather than pictures.", "tokens": [2831, 813, 5242, 13], "temperature": 0.0, "avg_logprob": -0.13532055125517004, "compression_ratio": 1.8134328358208955, "no_speech_prob": 1.3630772627948318e-05}, {"id": 1070, "seek": 456740, "start": 4583.5599999999995, "end": 4589.0, "text": " And we're going to be diving under the hood to see how these models actually work.", "tokens": [400, 321, 434, 516, 281, 312, 20241, 833, 264, 13376, 281, 536, 577, 613, 5245, 767, 589, 13], "temperature": 0.0, "avg_logprob": -0.13532055125517004, "compression_ratio": 1.8134328358208955, "no_speech_prob": 1.3630772627948318e-05}, {"id": 1071, "seek": 456740, "start": 4589.0, "end": 4593.5199999999995, "text": " We're going to learn about things like stochastic gradient descent and we might even be having", "tokens": [492, 434, 516, 281, 1466, 466, 721, 411, 342, 8997, 2750, 16235, 23475, 293, 321, 1062, 754, 312, 1419], "temperature": 0.0, "avg_logprob": -0.13532055125517004, "compression_ratio": 1.8134328358208955, "no_speech_prob": 1.3630772627948318e-05}, {"id": 1072, "seek": 456740, "start": 4593.5199999999995, "end": 4596.16, "text": " to brush off a little bit of calculus.", "tokens": [281, 5287, 766, 257, 707, 857, 295, 33400, 13], "temperature": 0.0, "avg_logprob": -0.13532055125517004, "compression_ratio": 1.8134328358208955, "no_speech_prob": 1.3630772627948318e-05}, {"id": 1073, "seek": 459616, "start": 4596.16, "end": 4598.84, "text": " I hope I haven't put you off by saying the C word.", "tokens": [286, 1454, 286, 2378, 380, 829, 291, 766, 538, 1566, 264, 383, 1349, 13], "temperature": 0.0, "avg_logprob": -0.1839536803109305, "compression_ratio": 1.0568181818181819, "no_speech_prob": 0.00010040541383204982}, {"id": 1074, "seek": 459616, "start": 4598.84, "end": 4600.48, "text": " I will see you next time.", "tokens": [286, 486, 536, 291, 958, 565, 13], "temperature": 0.0, "avg_logprob": -0.1839536803109305, "compression_ratio": 1.0568181818181819, "no_speech_prob": 0.00010040541383204982}, {"id": 1075, "seek": 459616, "start": 4600.48, "end": 4601.48, "text": " Thanks all.", "tokens": [2561, 439, 13], "temperature": 0.0, "avg_logprob": -0.1839536803109305, "compression_ratio": 1.0568181818181819, "no_speech_prob": 0.00010040541383204982}, {"id": 1076, "seek": 460148, "start": 4601.48, "end": 4628.12, "text": "aked", "tokens": [50364, 7301, 51696], "temperature": 1.0, "avg_logprob": -4.708522319793701, "compression_ratio": 0.3333333333333333, "no_speech_prob": 0.0045666201040148735}], "language": "en"}