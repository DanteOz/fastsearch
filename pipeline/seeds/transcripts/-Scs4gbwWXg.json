{"text": " Okay, yeah, you've got a question. Yeah. Okay, so I've got my paper space open. And I can see. Start machine. Yeah, my machine is not actually starting, which is not correct. So I can see a list of the directors like goes clear image tools. Got a one or two blah blah blah. How do we get over to the 20, the course to the current course cost 222. So you would get cloned it. So you would open so you would open up Jupiter lab. Right. You go into a terminal, and you would see data slash docs, sorry CD to slash notebooks, and then get cloned and copy and paste the get URL from GitHub. Okay. See what happens. Okay. Thank you. No worries. Does anybody having any trouble getting things working smoothly. I you got. Please go. Just a question I tried to look at my history my bash history I tried to similar I can see it that in my home, but I don't seem to be able to history from my previous session. Yeah, so the bash underscore history file dot bash history file is only created when you close the terminal. So you can't seem link it to it till it exists. And so to make it exist there's two things you could do the first is you could open a terminal, run a command like LS, and then close the terminal and open a terminal again. And now the bash history file will be there because you've done something, and you can seem like to it. Or you can just create it. And to create an empty file in unix you just type, touch space, and then the file name to dot bash underscore history. Okay. And somebody else had a question or comment. Yes. So my question is about with Kaggle part from last lesson. So, what I understand from, from the lesson. You need to either do everything in the Kaggle website, the training and inference, let's say, or it's possible to train your model in local machine or gradient, let's say. Somehow transfer it to the Kaggle. Somehow, right, so which we're going to do today. Yeah. Great. What is the proper way. That was my question. Yeah, yeah, yeah. Great. Perfect. I love it when people ask the question that I want to solve today because that's a fine that it's a question worth asking, answering, I should say. Okay. Well, neither of my gradient machines are starting setting up instance. That's not good. All right, I guess we're going to use the terminal. Now the bad news is that I'm on my Mac, I don't think I've got anything set up. You can use your fancy setup. Thanks, script. Yeah, I mean, I mean, yeah, it's it's kind of slightly set up, I don't have the Kaggle stuff downloaded. So, all right, well, it's always good to revise anyway isn't it share screen portion share. Right. Okay, you guys can see that hopefully. I wonder if we should maybe make this a little bigger as well. Okay, now you guys can see that. Okay. Is that reasonably visible. Yes, it is. Great. Yes. All right. So, so this is T marks, obviously, running in a terminal. And because I'm sharing my screen I'm using this, you know, slightly lower resolution kind of area that usual so particularly good idea to to zoom into one of these pains. So I'll just get control BZ to zoom into the pain. And let's see if on this machine I already okay so this machine does not currently have a Kaggle directory so store. I'll just use the Kaggle. I have SSH config setup. Oh yeah that's right I think Max got some really old version of SCP that doesn't know how to do much so I might have to. Normally, with current version of open SSH, SCP you could tap complete, even to get remote files, which is quite great. And I think I noticed that Mac tends to ship really old versions of a lot of the unique software which is a shame. So we have to do it the slow way so we're going to copy dot Kaggle slash Kaggle dot JSON to here. Why am I putting in SSH move Kaggle dot JSON into dot Kaggle. And that should now work. Great. So, let's download the data. All right. So we're going to go competitions, Patty, and data, and copy, and paste. All right, and then we're going to see if we got Jupiter running. A bit crazy over there. So let's open up Jupiter. And if anybody keeps an eye on paper space let me know if paper space seems to start working. Right here's Patty. And I'll name Patty. Okay. From fastai.vision.all import star. Looks like this is finished so we can now unzip minus q. Not that, unzip minus q. Patty disease classification. There we go. Oh, you know what we could do. We could run this on my GPU server because of course running this on the Macs is a bit of a dumb idea anyway. So, I don't think we've talked about how to do that before so this might be slightly obscure. That's okay. I'll learn something in the process. So, all right, I'm switching over now. Actually, let's jump out of Tmux because running Tmux in Tmux is always a little bit weird. So, this is my GPU server. Okay. And it is running Jupiter. And it says, oh, you can go to localhost 8888 to use this, but I can't, because that's not localhost refers to the machine I'm currently on and I'm not on this machine I'm SSHing into a remote machine. But what I've done, and as I say this is like something that not everybody needs to know about for sure but for those who are interested, what I've done is when I SSH to this particular machine called local, it forwards in a thing that I use on my local machines port 8888 to the remote machines 8888. That means that I can use localhost 8888, and it will actually forward those packets to the remote machine and forward remote machine packets back to here. So this is called SSH forwarding. You know, FYI. So if I go back here. Oh, and the other thing we should probably do is make sure that Jupiter is not running on the local machine. I'll cancel that. And so, to exit out of this, I could create another window or another tab or whatever but I can just hit Ctrl BD to detach from TMUX so that stuff's all running still in the background. And then I can SSH to my machine. And let's see if that's all working. There we go. Okay. That's great. And so here we actually have that going so we can create a new notebook. And it's easy enough by the way if you do like, buy a machine with a GPU, which it's not a terrible idea, especially if eventually GPU prices start to come back down to reasonable levels at some point. But, you know, it doesn't need to be a notebook or anything you can check it anywhere in the house, just like I've done and as you can see log into it from your computer. Now, I can only log into mine, right, you know, by default, I don't even log in from home. If you want to be able to log in when you're not at home, you would have to go into your routers settings and say forward port 22, which is the SSH port to your GPU server. And you'd also have to know the IP address that your house's Wi Fi is on and that tends to change. So you can use something called dynamic DNS. There's lots of different providers of dynamic DNS so you something called din.com just because they've been around forever. And so, yeah, so I can log into my machine from from anywhere, which is, which is very nice. Okay, so let's try this again from fast AI dot vision dot all imports. All right. And so path equals. So we can do LS in bash like so here we go. Great. So you can just use the current path. Is where our data is. And so our training path. And we're also. And then our training path. Is path slash train images. I'm sure we've really talked much about path lip before us so this this path object comes from a Python class called path lip. And it's imported by default with pretty much any fast core or fast AI thing you use. So to learn about it, obviously you can just Google for path lip. And basically, yeah, it lets you create a part this is you know path in your current working directory, or you can do something here to go to a relative directory, or you can go to a absolute directory, and then you can, you know, it's kind of got this somewhat neat use of the slash operator to mean, you know, go to a sub directory. Ls doesn't come with it by default fast core adds that to list things, as you say. Yeah, so that's pretty cool. And so the other thing we did yesterday was we looked at the files. Get image files inside. Let's have a look at the ones inside the training path. And so we can create an image. We can look at it. Size. Which is a property. Okay, so there's a few things we can do that kind of gets us back to where we were yesterday. So a question I saw in the forum was, how would I get like the sizes of all of the files that actually showed how to do it. So we can do it kind of slightly slow way, which is, let's do it the slightly slow way and time how long it takes sizes equals. Paste that here. Yes. Sarah question. So to do this in parallel, which would obviously be faster. One would expect I mean, it would depend if the most of the time is spent reading this from the disk, then doing this in parallel won't be any faster. Most of the time is being spent decoding the JPEG, then doing this in parallel will be faster. And which of those is true will depend on whether we're using an SSD or not. So anyway, I'll show you how to do it. So if you import fastcord.parallel. Which is a module. That module contains a function. For parallel. Which applies this function to these items. So the function we want to apply. Is. And let's look at the doc for it. Show in docs. So here's an example of parallel. That should be. Ordered, but never mind. So here's something that takes two things, X and A, and adds something to each one. And so here's how we use parallel. The docs for fast.ai libraries are a bit different to some in that the tests and the docs are all one thing. So to read this, this is saying if you call parallel, passing it this function, which is just X plus A where A defaults to one. And you do it on this input, which is range 50. Then you would expect to get this output, which is the range from one to 51. So this kind of is showing you lots of examples of using the function and telling you what you would expect to get for each one. So if we want a function, which is going to take some file. And it's going to return. This. And so if we want to run that in parallel. Then we can say parallel. And the function we want to run is this function and the files we want to run on is files, and there's lots of other things we could pass in. So like let's say we want to do it on four parallel workers, see if that ends up any faster. So as you can see running stuff in parallel, when you use fast core is actually pretty fast and easy. But as I say it doesn't necessarily result in a speed up. If the main thing that's taking time is getting stuff off the disk, then it won't be faster. So in this case, it was a bit faster. I think they use really slow disks on. Actually this is my disk. This is a good disk. So that ought to be fast. So we could see if increasing it further is faster still. I guess we've probably. We'll see. So Jeremy, quick question about this. Does this use CPU cores, or is it using the GPU or running it in parallel? CPU. So the GPU is only used for models, basically. Pretty much everything else is going to be done on CPU. Okay. Okay. So that's definitely worth the speed up. Now, I don't normally create a function to do one thing like this. What I would normally do instead is to use a lambda expression. And so to use a lambda expression, it's just basically it's a function you define in line. You just type lambda and you say the argument and you don't have to say return. And so then we can get rid of the definition. And run. Oh, okay. So that's interesting. So we can't use a lambda with parallel. Oh, I guess I didn't know that. Now I think about it. All right. That's fine. We won't use it then. Parallel processing on Python is notoriously crappy. So, yeah, it's it's a it has a lot of limitations, including now I think about it not being able to use lambdas. Okay. So then we created our data loaders. Image data loaders dot from path. Folder. And we passed in the training path. And. Ballot percent. And I think we want some resize transform as well. Right. Cool. So. And so then we created a model. So last time we used ResNet 34. But what I'd be inclined to do. Is to head over to Kaggle. And look at the which image models are best. And see if there's something we might want to use. It's better than ResNet 34. So this is showing speed in a log scale. And this is showing accuracy on ImageNet. And the different colors of various different kind of families. ResNet is this family here. And things like ResNet 34 are not particularly great, as you can see. So let's try using conv next base in 22 blah blah blah instead. Okay. So Vision Learner we first passed in the data loaders. And so these image models here are from a library called Tim. Which to use it, you need it installed. Which I probably have installed, but just to check. Yep, that's already installed. And you can check out things on Tim such as, so if you import it. Then you can say Tim dot list models. And you can pass in basically a glob. So I want to look at conv next models. See what options there are. Mainly because I just want to copy and paste. And so, okay, so there's base, there's small as well. Small. Now why is small? Conv next. If you double click, you'll get this. There's base, large, extra large. That's weird. For some reason, the small one's not appearing. And there's also a tiny. Hi, Jeremy. Yeah. I think they added the small and the tiny one in the last version of Tim that is not in the Pitten style version. Right. Okay, so. Oh, yeah, we need to install the dev version of it. Correct. Yes, thank you. So Ross, who creates Tim. Created a pre-released version, 0.62. And so to install that, we would need to call minus U for upgrade. Oh, just one moment, please. My daughter's having computer problems. All right, Tim, greater than. 0.6. 0.6.2. I think something like this. I'm not quite sure. Oh, it says I've already got 0.6.2 dev installed. Oh, I see. So I've got it on my machine, but it wasn't on the Kaggle machine because I didn't install that version on Kaggle. Okay. Cool. Well, we might as well fix it on Kaggle, just so you see how these things work as well. Because there's actually something quite nifty here. So we'll click edit on Kaggle. And we'll get it here somehow, but it's okay. Copy. Oh, all right. It's he doesn't have it in his data, I guess. All right. So not much we can do about that. All right. Well, I think we should just go ahead and try one of the smaller ones. So let's try small. That has to be a string when he used Tim. See what happens. So when you use a pre trained model, it needs the weights. And so the first time you use it, it downloads the weights. Depending on how much space, if you're using paper space, depending on how much space you have and how long this takes on paper space, you may want to consider Sim linking your home directories dot cash slash torch into slash George. And that way you won't have to download these. Not that it seems to take too long. I don't know if you care or not. You know, one thing we might want to do. Well, let's let's just try to find you in it, shall we? That's good. Seems to be working. Let me know if anybody's got any questions or thoughts along the way. So when it fine tunes. Oh, OK. So the other thing we want to do is tell it that we want to keep track of the error rate. OK. That's going to help. So yeah, so when we fine tune. Just create another copy. OK. So we can look at the source code to see exactly what it's doing. When we call fine tune. So what it's actually doing. Is it's calling freeze. What that does is it says all of the weights, except for the very last layer. You're the optimizers not allowed to change. So if you think back to that Sila and Fergus paper we saw with the different layers and the different like, you know, the later layers were more and more specific. So initially, we just want to fit fit the last layer. So it calls fit on the last layer only. And then it decreases the learning rate and then unfreezes. So then it says, OK, you know, you can train the whole thing and then it trains the whole model for however many epochs we asked for. So we can see. So generally speaking, fast AI methods, I mean, pretty much any methods I write tend to be very small. So they're designed to be reasonably easy to read the source code and see what's going on. At least if you're reasonably comfortable with Python. Oh, and I just sort of something else we should do. Which is if you are using a GPU released in the last, I don't know, four years or so. It's very likely that it'll be much faster using what's called half precision floating point, which is basically like less less precise numbers. It'll be way, way, way faster. Most of the time on Colab and Kaggle, you're not going to get one of those more up to date GPUs. But having said that, there's really never any harm in using half precision floating point. And even if you use an older GPU, it's still going to save memory. So actually to ask fast AI to do that for you, you can add two floating point 16 as in 16 bit at the end of your learner command. So, yeah, so when this finishes, we might try rerunning it with this instead. Excuse me, Jeremy, I'm just following along on purpose post. And if I don't want to bother with importing Tim, just to keep up, what instead of using ConvNext, what would be a good default to use? I mean, why not use ConvNext? I guess because I would I would need to import Tim. I think I missed that bit. So pip install Tim. Yep. Or if you want to get the more recent models, such as the one we're using, then pip install. Let me copy this for you. So that's the command there. And I'll put it in the Zoom chat for you. Great, thanks. And Jeremy, just while we're talking about fine tuning and as that's going on, I don't know if anyone else would find it helpful. But I mean, obviously, like conceptually understand what's happening with fine tuning. But I don't know if anyone else kind of feels like trying to understand better what's actually going under the hood with fine tuning. Like what's actually being altered within the model, like more than just like a kind of at a high level. I'm just trying to get a bit of a better grip on what exactly we're fine tuning and how it's going about fine tuning it, I guess, just under that first surface level. Just want to make sure I understand it better. Yeah. So we just looked at the source code for it. So what did you want to go? Which bit of this did you want to look deeper at? Or like what did you? Yeah, tell me more what you want to know. Yeah, I guess so. Yeah, like stepping through it. So like we've got it frozen at a particular point, right? And then this fit one cycle. So just going through the definition of fit one cycle again. Oh, so we haven't done that yet in the course, I don't think. Yeah. So yeah, so we can certainly talk about that. I don't want to hijack things. If other people want to kind of move on, that's fine. I can follow it up later. But I just I just kind of wanted to get a bit of an overview of what's actually going on there. Let's take a look. Hit one cycle. So. So we look at the docs. Here it is. So what does fit one cycle do? So actually, there's a paper you can read if you want to know exactly what it does. But there's a picture here which tells you what it does. And what this picture is, is so fit one cycle uses something called a scheduler. And a scheduler is something which actually changes the learning rate during the training. So remember, the learning rate is the thing we multiply the gradients by before we subtract from the parameters. When you have a randomly initialized model or even a pre trained model, we actually randomly initialize the last layer of weights. So at first, even a pre trained model that we're fine tuning can't do anything. It's still giving random answers. And so that means we want to use a really small learning rate because it's very it's very difficult to get to a point where it's starting to learn something slightly useful. And so when we start training the first year batches use a tiny learning rate. And then as it gets better and better at like doing something useful, you can increase the learning rate, because it's it's got to a point where it's like, yeah, it's kind of knows vaguely what it's doing. And so as it trains the learning rate goes up and up and up and up and up. And then as you start to get close to the answer. You need to decrease the learning rate again. And the reason for that is that you're really fine going to small little steps you're really, really close now. So when you say, yep. So you say now. So as you get closer to the answer, like are we saying that that's in comparison to the validation set so that we're so that we're moving away from overfit where those might, yeah, I guess where the training or anything. This is just so this is just a plot of, of the curve of batch number against learning rate. So this is this is the shape the exact shape that is used. There's nothing clever going on it just it just follows this exact curve. Okay, so there's no there's that's not interacting with anything else to derive those numbers it's just doing that. Okay. All right. Understood. So in fact, if we look at the source code for it. What it does is it calls them in code combined cause. I'll get the definition of, which is something that uses to cosine schedules, and so a cosine schedule is one that. So it's also known as an annealer it's called learning rate and dealing is something that literally uses cosine. That's it. Yep. Okay, that that's helpful so I get now kind of where that's mapping to that, that idea. Yeah. And you know for people who are interested in going deeper and understanding like, what is fast AI do and why and what, what actually makes what's important in deep learning and stuff this is how exploring the documentation and source code of fast AI, whether you're at a point where you feel this is what you're ready to do can be super useful because you know the documentation can tell us you what paper is being implemented and why and shows you pictures of what it's doing and the source code is something that you can copy and paste into your notebook and try it yourself and so forth. Yes, did you have any other questions about this. No, that's, that's fine. Most of us. All right. I just wanted to comment that Sylvan had a very good blog post explaining those it one cycle policy. Yes, he does. Perfect. And so there's other policies you can use like the triangular version. So this is actually what we originally did I think for one cycle as you can see it ends up being pretty similar. And what would be, I guess the criteria for where you would change that policy like, like what would, I guess what's the basis for the decision you make about changing that policy. I mean, you don't basically it works fine and you just do it. Yeah. Okay. All right. So it's pretty arbitrary. Yeah, I mean, it's, it's not arbitrary. It's something that lots of experimentation has found that this works well. Okay, right. Like, like everything pretty much in fast AI we try to find the things that work well, and the things that need changing we generally tell people all about them, but this is generally something that doesn't need changing too much. Okay. So is this related to learning trade finder. Okay, so let's talk about learning. I'm finding it quite confusing, which actually which which number is correct with learning right finder. So I'm just before you do that I just point out so that the mixed precision version on my RTX card, which is a consumer GPU. The speeds gone from a minute 41 to 28 seconds. So you can see it really does make a huge difference to use fp 16. So, this question about something called the learning rate finder. So the learning rate finder does something very similar to one cycle. The one cycle scheduler or one cycle in yelling, which is it gradually increases the learning rate. While it trains, but it actually only does up to 100 batches. So, generally speaking, far less than even an epoch. And it doesn't increase the learning rate and then decreases again, it just keeps increasing the learning rate until the whole thing falls apart. And so, this is a graph of the learning rate. And remember, it increases that logarithmically increasing every batch. So this is also kind of a graph of time of batch number. And then it shows you what loss it got so for batch, the first few batches it got a loss of about four. And until it got up to a learning rate of about 10 to the negative four, nothing really improved. So clearly learning rates of less than 10 to the negative four aren't very useful. And then as it increased the learning rate, you can see the slope started to get steeper and steeper. And so this area here is where it's learning the most quickly. And then it gets to a point up here where it's too high. And when it gets too high. Initially it just doesn't really improve at all. And then it gets really too high it jumps past the answer. Right. And starts getting much worse. So the. Yeah, so I generally just pick somewhere visually around the middle. Or you can, you know, see it says something around. So is the magnitude of that thing or what why why we wouldn't choose the minimum here. I mean, I, okay. Sorry that that's what I felt like. Yeah, this would be a really bad spot right because at this point it's not learning. So what you want to look at is the is the gradient you want to look at the slope of this line because the slope is how quickly is it improving. So at this point here that this learning rate, it doesn't improve at all. So if we use this learning rate. So would that make sense to use gradient actually for this. Yes, to see what's the minimum. I mean, rather than doing that visually. Well, not necessarily because you see here there's a really steep gradient but that's definitely a bad spot. True. Okay, sorry. So, I mean, don't be sorry it's a great question like it's surprisingly difficult to come up algorithmically with the thing that our eyes do when we say like oh we're about somewhere around here. And wouldn't that be local meaning minimum. No, sorry, because the minimum is down here, which is definitely not what we want. And the minimum gradient would be here, which is definitely not what we want. I'm not saying it's possible it's totally possible but the learning rate finder. So Zach Miller actually spent a lot of time trying different things and wrote a whole blog post for his company and came up with four different approaches, all of which actually don't work too bad. And you can actually look at all of them by saying what suggestion functions do you want to use. Oh, and I wonder if there's a link to Zach's blog post in the docs, because that would be quite helpful. Yeah, so you can see minimum. It's actually one of the suggestion functions but I don't actually know why it's even there because you'd never use it. And, so, minimum will put the plot will be in the minimum but the suggestion value is still like it's like 10, like divided by 10. Oh, is that what happens. Yeah, so it finds the minimum divided by 10. I got it. So we should probably plot that then on the minimum rather than wanting what's effectively 10 times that day. Thanks for explaining. Yeah, so you can see all these numbers are all in the same order of magnitude. And default is point or two. Is that something that fast I use these underneath the hood, or I mean like what's the benefit of changing this learning create manually, or trying to find some, so most of the time our default works perfectly fine, which is why I don't talk about this as much as I used to actually. But you know sometimes some data will particularly like for a tabular data set the learning rate can be almost anything. It really does depend on the model. I find most data division models seem to have pretty similar learning rates that are useful so the defaults generally work pretty well. But yeah if you try something and it doesn't seem to be training quickly well. Just try running the, you know, the first thing I try would be try running LR find just in case the default learning rates, nowhere near the recommended values. And then you could try. Yeah, you could just try. But yeah, these are all very close to route throughout default anyway, so I wouldn't bother in this particular case. Thank you. Yeah, no worries. These are questions. Okay, we've got a model. I'm actually going to have to try it again because I just created a new learner for the purpose of that. And so the next thing we're going to have to do is to apply it to our training set, sorry to our to our test set in order to submit to Kaggle. So the test set it's always good to have two windows, two tabs going on because that way we can start working on the next thing while this is training right and you can see it's still training because the little hourglass icon is there in the icon. So there's something called test underscore. DL, which for some reason is not appearing. It must be from some different part of the library. I test. Data core. Fastai.data.or import star. Something silly. Oh, it's a method. Okay. My bad. DLs.testDL. Okay. All right, so this creates a test data loader. So a test data loader is a data loader used for inference of a bunch of things at once, basically. So this should be an example down here. Okay, so test DL is something that we pass some items to. So I don't know like Radact, Nishik, anybody else? You know, I'm thinking I'm just gonna call get image files on the test set and pass it to test DL. Is that what you guys would do or you have a better way to do this? Yeah, I think that should work. Okay. I don't know like what people, you know, I don't do nearly as much inference stuff as most people, so I never quite know what the fast AI communities preferred idiomatic approaches. Test images. Test images. Test files. Okay, so we've got 3,460 unknown files to apply this to. So, we could create a test data loader, and that's going to be DLs.testDL with those files, I guess. And we should be able to go testDL.showBatch. So I do always like to see what I'm doing, you know. So that looks hopeful. And so a test data loader, the key difference is that it doesn't have labels. Right, so there's no dependent variable. All right, so then I guess we would go learn. Is it.getPreds or.predict? I never quite remember. That's an item, so I guess it's.getPreds. We need better names for these. Okay, and then DL data loader equals the test data loader. Oh, and I should have assigned that to something, obviously. That was a bit silly of me. And also we should look at the documentation for it. Do not use to map keyboard shortcuts. Get the predictions with some particular data loader. It can optionally return the input. It can optionally return the loss. We don't need any of that. Okay, so what I think we should do is we should look at Kaggle at this point. And actually we don't even need to look at Kaggle. What Kaggle normally does is they provide us with a sample submission. Here's one here, right? So let's look at the submission. Sample submission equals pd.readcsv. And notice in Jupyter, if you start quotes and you press tab, it will tab complete file names, which is nice. Okay. Not a very useful sample submission. Unless there's something wrong with Python. This is not a great sample submission, but they just want the name of the class. I see. What a terrible sample submission, particularly for a training one, you would think they would be more helpful. So they just want the text of the name of the class, do they? Yes. I mean, obviously, we could actually look it up and find out rather than guessing when you don't have Radik on the line to show you the answer. Or maybe you can just call Radik and ask him. So data evaluation. Yes. See they've actually got a sample here. Yeah. All right. Let's try that. So preds equals and so by default it's going to return the probability of every class, which we can certainly turn into what we want. I think if we call with decoded, that will do it for us. Does that sound right? See, I'm so out of practice with this. Okay. It's pretty close, right? It's given us the indexes of each one. So this is actually going to be a really good exercise. So in terms of like what's in there, there's three things. There's the probabilities. There's something we don't care about. And there's the indexes. So these are the indexes. So what are these indexes of? They're indexes into the vocab. So if you remember the vocab is the tells you what's what. So we need to convert these predictions into these pieces of these strings. So the first thing I'd probably be inclined to do is to maybe turn that into a pandas series. And so I guess we should. Do you give it a name as well? Okay. So let's call these indexes. Oopsie dozzy. Okay. So there's a pandas series. And I always find pandas the pandas API difficult to remember. I don't find it particularly consistent or intuitive. But there is a dot map function which I think we can look up in to dl.vocab. Category now is not callable. Fair enough. So dl.vocab is although it looks like a list. I guess it's not one, but we might be able to turn it into a list. Normally you can turn things into lists like so. Yes, we can. Let's see if that works. Oh, okay. That's annoying. So I'm pretty sure that you can pass a dictionary. Yes, you can. And I thought a list would count as a dictionary, but apparently it doesn't, which is, I mean, mapping. So a mapping just basically refers to something that behaves like a dictionary. So we actually have to create a dictionary which maps from the index to the name, which is a bit of a pointless thing to do in a sense, but that's okay. So for k, v in. So if we enumerate through that. Okay, so that's what a mapping looks like. So I could say mapping equals. And then here we'll say mapping. There we go. So that's what we want. So, this is basically our results, right? So I was thinking like an alternative way, like mapping also map function also takes functions, correct? I was avoiding that because that I mean I know it doesn't matter here but it's really slow. So we, we could also pass in a function. I was just thinking like you could just have a function that just indexes into the into the list or something like that. Correct. Like a lambda function or something like that. Exactly. Let's go ahead and do that to see what it looks like. Almost nobody knows that you can use a dictionary or a mapping so almost everybody on cackle uses a function and often it can take a very, very, very long time to run at, you know, on big data sets. So yeah, you could also have a lambda. And so that's going to be passed in each index, and you would just want to return the DLs. dot go cab at I. So that does the same thing now obviously this is tiny so it doesn't actually matter but I thought I'd try to show the neat trick which almost nobody knows about, which is the mapping. Okay, so we basically want to use that as our labels. So I think we can go SS label equals. There we go. Okay. So, you know, normally at this point, I would like visually check some results and the easiest way to visually check some results is to go learn dot show results. And this is showing me the actual and the predicted and the accuracy is very high so it's all correct the problem is I don't know which of these are right which are wrong so I have no idea what to look for. So, I don't have that ability to do my normal checking. Okay, so we can say this is a CSV submission. There we go. Okay. So there's a few things we could do here I guess probably the easiest one would be to use the Kaggle CLI. I was going to note something of the submission, or for the two CSV. I think you might have to do index equals false because you're right. Normally, what I always do after that, except this time which I forgot is to do exclamation mark head to show me the first few lines and yeah so now we would see as Tunisic says, we've got this extra column out the front, which is because the default is that it shows kind of the row number. Thanks Tunisic. And so that will fix it. And if we compare that to their sample. Yeah, it looks nice and similar. So that's good. So these are all kind of steps in the same thing so I pop these all together. And then we might item and DD to get Patty, and is that submission file. Okay, so generally minus help or minus minus sorry minus h or minus minus help normally gives you a quick version of help. And so, I want to do something with competitions competitions. Okay, there we go. And so we're going to do a submission. All right, we need a file for upload. And we're going to need the competition. And so I could go Kaggle competitions list pipe grip Patty. That way I don't even have to. Oh, that's not what I expected to happen. Oh, I bet that pages it. Okay, so rather than grip, we should use minus s. And it's going to use a regular expression or something. Look at a few examples. Okay, so there's definitely something called spaceship spaceship. All right. I will go to here after all. And this is what it's called. So, I don't know. Is it not active, probably. That's why it's active. Yeah, exactly. Is it because it's like not really actually organized by Kaggle maybe the only list. Or we need a copy of the category all. It's fine. But maybe it's group. Maybe this is considered in class. Yeah. Anywho, so we were going to do a submission. And so we need to provide the file name, minus F. Oh, full path. That's a bit weird. Okay. And a message, minus m. Initial conv next, small to epoch spaceship. Okay. And then the competition. And go. Took a while for a 70k file, but so be it. Okay. So let's see if it's there. It is. How did we do? Oh, I see. Oh, yeah. Jump to your leaderboard position. Out of. So I'm guessing that there's a problem with our submission because it's, I think maybe what happened was the test files were not like. They got shuffled somehow. So we're not sure. Yeah. Sometimes that might happen. Yeah. That's a good question. Yeah. Yeah. Yeah. That looks like that seems very likely. So we didn't do a lot of checking as we went. Yes. So they were expecting that 2001 would be first and we have 2000, 300,000, 119 first. So that is not. Ideal. It would be nice if get image files by default. Return things in a more sensible order. Anyway, it's good to see these problems. You know, we could just sort it right. But it looks like it works. It probably does work as long as they've got exactly the same number, as long as they're all 123456 digits. If some of them are different numbers of digits, we can't sort it because this is sorting in string order. But, yeah, maybe that's OK. Says start tail. Does it have a tail? Yes. 203469. 203469. Yeah. OK. Maybe we're fine then. So sorted. Whoopsie daisy. Sorted returns the sorted version or sort sorts in place. OK. Those. And so it's very nice to have things set up, you know, that you're doing things from the command line and notebooks and stuff so that when you screw up, which, you know, if you're anything like me, you always screw up. You can pretty quickly repeat the process. So I just hit up arrow. And just add sorted to our message, both literally and figuratively, hopefully. Welcome to the leaderboard. Not a very successful welcome. It's better, isn't it? Point nine one. There we go. Good start. About in the middle. All right. So does anybody have questions about Yeah, this process of seeing my question in the chat and I had the same problem. When installing Tim. We, this is in paper space. Assume Mike was the same. We can see the list of models, but it doesn't actually create the learner. It says Tim is not defined. So you need to. So, yeah. So, actually, I was able to do that. Matt, I just restarted my kernel. Yeah. So just to explain when you see in Python, something is not defined. That symbol. Python doesn't know what it is. And so there are two ways basically to define a symbol to create a symbol. One is to say something like a equals one that defines a symbol called a right or another is to do something like the death. And that defines a symbol called F. Right. And the other way to define symbols is to import them. So in this case, Tim is not defined means you have not imported Tim. And that will solve your problem. So make sense. Yes, it does. But I. Importing Tim and running that commander, I think it was still it will definitely work if you say import Tim this will definitely work. So I'd say you might have reset your kernel or something and hadn't really run that cell. Yeah, if you say if you restart the kernel, it works. Yeah, if you say import module. Yeah. Oh yeah, I mean the other possibility is that you might have got a different message which is something like this module not found and module not found means yeah either you haven't installed it, or if you have installed it, you might need to restart your kernel by clicking kernel restart. So it can like recheck what modules you have since you just installed it. That's what I had. Great. Thank you. All right, well that was pretty successful, even if paper space wasn't. Thanks guys. And see you. Yeah, see you tomorrow. Thanks joining. Thanks Jeremy. Thank you. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 3.0, "text": " Okay, yeah, you've got a question.", "tokens": [1033, 11, 1338, 11, 291, 600, 658, 257, 1168, 13], "temperature": 0.0, "avg_logprob": -0.42404304610358345, "compression_ratio": 1.4814814814814814, "no_speech_prob": 0.08485592901706696}, {"id": 1, "seek": 0, "start": 3.0, "end": 7.0, "text": " Yeah. Okay, so I've got my paper space open.", "tokens": [865, 13, 1033, 11, 370, 286, 600, 658, 452, 3035, 1901, 1269, 13], "temperature": 0.0, "avg_logprob": -0.42404304610358345, "compression_ratio": 1.4814814814814814, "no_speech_prob": 0.08485592901706696}, {"id": 2, "seek": 0, "start": 7.0, "end": 11.0, "text": " And I can see.", "tokens": [400, 286, 393, 536, 13], "temperature": 0.0, "avg_logprob": -0.42404304610358345, "compression_ratio": 1.4814814814814814, "no_speech_prob": 0.08485592901706696}, {"id": 3, "seek": 0, "start": 11.0, "end": 13.0, "text": " Start machine.", "tokens": [6481, 3479, 13], "temperature": 0.0, "avg_logprob": -0.42404304610358345, "compression_ratio": 1.4814814814814814, "no_speech_prob": 0.08485592901706696}, {"id": 4, "seek": 0, "start": 13.0, "end": 23.0, "text": " Yeah, my machine is not actually starting, which is not correct. So I can see a list of the directors like goes clear image tools.", "tokens": [865, 11, 452, 3479, 307, 406, 767, 2891, 11, 597, 307, 406, 3006, 13, 407, 286, 393, 536, 257, 1329, 295, 264, 17307, 411, 1709, 1850, 3256, 3873, 13], "temperature": 0.0, "avg_logprob": -0.42404304610358345, "compression_ratio": 1.4814814814814814, "no_speech_prob": 0.08485592901706696}, {"id": 5, "seek": 2300, "start": 23.0, "end": 36.0, "text": " Got a one or two blah blah blah. How do we get over to the 20, the course to the current course cost 222. So you would get cloned it.", "tokens": [5803, 257, 472, 420, 732, 12288, 12288, 12288, 13, 1012, 360, 321, 483, 670, 281, 264, 945, 11, 264, 1164, 281, 264, 2190, 1164, 2063, 5853, 17, 13, 407, 291, 576, 483, 596, 19009, 309, 13], "temperature": 0.0, "avg_logprob": -0.43658614980763405, "compression_ratio": 1.536, "no_speech_prob": 9.748381853569299e-05}, {"id": 6, "seek": 2300, "start": 36.0, "end": 40.0, "text": " So you would open so you would open up Jupiter lab.", "tokens": [407, 291, 576, 1269, 370, 291, 576, 1269, 493, 24567, 2715, 13], "temperature": 0.0, "avg_logprob": -0.43658614980763405, "compression_ratio": 1.536, "no_speech_prob": 9.748381853569299e-05}, {"id": 7, "seek": 2300, "start": 40.0, "end": 42.0, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.43658614980763405, "compression_ratio": 1.536, "no_speech_prob": 9.748381853569299e-05}, {"id": 8, "seek": 4200, "start": 42.0, "end": 56.0, "text": " You go into a terminal, and you would see data slash docs, sorry CD to slash notebooks, and then get cloned and copy and paste the get URL from GitHub.", "tokens": [509, 352, 666, 257, 14709, 11, 293, 291, 576, 536, 1412, 17330, 45623, 11, 2597, 6743, 281, 17330, 43782, 11, 293, 550, 483, 596, 19009, 293, 5055, 293, 9163, 264, 483, 12905, 490, 23331, 13], "temperature": 0.0, "avg_logprob": -0.2444588708095863, "compression_ratio": 1.3246753246753247, "no_speech_prob": 4.397694647195749e-05}, {"id": 9, "seek": 4200, "start": 56.0, "end": 58.0, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.2444588708095863, "compression_ratio": 1.3246753246753247, "no_speech_prob": 4.397694647195749e-05}, {"id": 10, "seek": 4200, "start": 58.0, "end": 60.0, "text": " See what happens.", "tokens": [3008, 437, 2314, 13], "temperature": 0.0, "avg_logprob": -0.2444588708095863, "compression_ratio": 1.3246753246753247, "no_speech_prob": 4.397694647195749e-05}, {"id": 11, "seek": 4200, "start": 60.0, "end": 62.0, "text": " Okay. Thank you.", "tokens": [1033, 13, 1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.2444588708095863, "compression_ratio": 1.3246753246753247, "no_speech_prob": 4.397694647195749e-05}, {"id": 12, "seek": 4200, "start": 62.0, "end": 67.0, "text": " No worries.", "tokens": [883, 16340, 13], "temperature": 0.0, "avg_logprob": -0.2444588708095863, "compression_ratio": 1.3246753246753247, "no_speech_prob": 4.397694647195749e-05}, {"id": 13, "seek": 6700, "start": 67.0, "end": 76.0, "text": " Does anybody having any trouble getting things working smoothly.", "tokens": [4402, 4472, 1419, 604, 5253, 1242, 721, 1364, 19565, 13], "temperature": 0.0, "avg_logprob": -0.46011885890254267, "compression_ratio": 1.0617283950617284, "no_speech_prob": 4.000767148681916e-05}, {"id": 14, "seek": 6700, "start": 76.0, "end": 79.0, "text": " I", "tokens": [286], "temperature": 0.0, "avg_logprob": -0.46011885890254267, "compression_ratio": 1.0617283950617284, "no_speech_prob": 4.000767148681916e-05}, {"id": 15, "seek": 6700, "start": 79.0, "end": 81.0, "text": " you got.", "tokens": [291, 658, 13], "temperature": 0.0, "avg_logprob": -0.46011885890254267, "compression_ratio": 1.0617283950617284, "no_speech_prob": 4.000767148681916e-05}, {"id": 16, "seek": 6700, "start": 81.0, "end": 83.0, "text": " Please go.", "tokens": [2555, 352, 13], "temperature": 0.0, "avg_logprob": -0.46011885890254267, "compression_ratio": 1.0617283950617284, "no_speech_prob": 4.000767148681916e-05}, {"id": 17, "seek": 8300, "start": 83.0, "end": 99.0, "text": " Just a question I tried to look at my history my bash history I tried to similar I can see it that in my home, but I don't seem to be able to history from my previous session.", "tokens": [1449, 257, 1168, 286, 3031, 281, 574, 412, 452, 2503, 452, 46183, 2503, 286, 3031, 281, 2531, 286, 393, 536, 309, 300, 294, 452, 1280, 11, 457, 286, 500, 380, 1643, 281, 312, 1075, 281, 2503, 490, 452, 3894, 5481, 13], "temperature": 0.0, "avg_logprob": -0.19929066826315486, "compression_ratio": 1.6416184971098267, "no_speech_prob": 2.7102914827992208e-05}, {"id": 18, "seek": 8300, "start": 99.0, "end": 107.0, "text": " Yeah, so the bash underscore history file dot bash history file is only created when you close the terminal.", "tokens": [865, 11, 370, 264, 46183, 37556, 2503, 3991, 5893, 46183, 2503, 3991, 307, 787, 2942, 562, 291, 1998, 264, 14709, 13], "temperature": 0.0, "avg_logprob": -0.19929066826315486, "compression_ratio": 1.6416184971098267, "no_speech_prob": 2.7102914827992208e-05}, {"id": 19, "seek": 10700, "start": 107.0, "end": 121.0, "text": " So you can't seem link it to it till it exists. And so to make it exist there's two things you could do the first is you could open a terminal, run a command like LS, and then close the terminal and open a terminal again.", "tokens": [407, 291, 393, 380, 1643, 2113, 309, 281, 309, 4288, 309, 8198, 13, 400, 370, 281, 652, 309, 2514, 456, 311, 732, 721, 291, 727, 360, 264, 700, 307, 291, 727, 1269, 257, 14709, 11, 1190, 257, 5622, 411, 36657, 11, 293, 550, 1998, 264, 14709, 293, 1269, 257, 14709, 797, 13], "temperature": 0.0, "avg_logprob": -0.18735666628237124, "compression_ratio": 1.7015706806282722, "no_speech_prob": 1.669966513873078e-05}, {"id": 20, "seek": 10700, "start": 121.0, "end": 126.0, "text": " And now the bash history file will be there because you've done something, and you can seem like to it.", "tokens": [400, 586, 264, 46183, 2503, 3991, 486, 312, 456, 570, 291, 600, 1096, 746, 11, 293, 291, 393, 1643, 411, 281, 309, 13], "temperature": 0.0, "avg_logprob": -0.18735666628237124, "compression_ratio": 1.7015706806282722, "no_speech_prob": 1.669966513873078e-05}, {"id": 21, "seek": 12600, "start": 126.0, "end": 139.0, "text": " Or you can just create it. And to create an empty file in unix you just type, touch space, and then the file name to dot bash underscore history.", "tokens": [1610, 291, 393, 445, 1884, 309, 13, 400, 281, 1884, 364, 6707, 3991, 294, 517, 970, 291, 445, 2010, 11, 2557, 1901, 11, 293, 550, 264, 3991, 1315, 281, 5893, 46183, 37556, 2503, 13], "temperature": 0.0, "avg_logprob": -0.2297463677146218, "compression_ratio": 1.4055944055944056, "no_speech_prob": 1.183993572340114e-05}, {"id": 22, "seek": 12600, "start": 139.0, "end": 144.0, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.2297463677146218, "compression_ratio": 1.4055944055944056, "no_speech_prob": 1.183993572340114e-05}, {"id": 23, "seek": 12600, "start": 144.0, "end": 148.0, "text": " And somebody else had a question or comment. Yes.", "tokens": [400, 2618, 1646, 632, 257, 1168, 420, 2871, 13, 1079, 13], "temperature": 0.0, "avg_logprob": -0.2297463677146218, "compression_ratio": 1.4055944055944056, "no_speech_prob": 1.183993572340114e-05}, {"id": 24, "seek": 14800, "start": 148.0, "end": 159.0, "text": " So my question is about with Kaggle part from last lesson. So, what I understand from, from the lesson.", "tokens": [407, 452, 1168, 307, 466, 365, 48751, 22631, 644, 490, 1036, 6898, 13, 407, 11, 437, 286, 1223, 490, 11, 490, 264, 6898, 13], "temperature": 0.0, "avg_logprob": -0.12037398849708447, "compression_ratio": 1.5333333333333334, "no_speech_prob": 8.528693797416054e-06}, {"id": 25, "seek": 14800, "start": 159.0, "end": 177.0, "text": " You need to either do everything in the Kaggle website, the training and inference, let's say, or it's possible to train your model in local machine or gradient, let's say.", "tokens": [509, 643, 281, 2139, 360, 1203, 294, 264, 48751, 22631, 3144, 11, 264, 3097, 293, 38253, 11, 718, 311, 584, 11, 420, 309, 311, 1944, 281, 3847, 428, 2316, 294, 2654, 3479, 420, 16235, 11, 718, 311, 584, 13], "temperature": 0.0, "avg_logprob": -0.12037398849708447, "compression_ratio": 1.5333333333333334, "no_speech_prob": 8.528693797416054e-06}, {"id": 26, "seek": 17700, "start": 177.0, "end": 182.0, "text": " Somehow transfer it to the Kaggle.", "tokens": [28357, 5003, 309, 281, 264, 48751, 22631, 13], "temperature": 0.0, "avg_logprob": -0.2287066667929463, "compression_ratio": 1.5776699029126213, "no_speech_prob": 3.590697451727465e-05}, {"id": 27, "seek": 17700, "start": 182.0, "end": 185.0, "text": " Somehow, right, so which we're going to do today.", "tokens": [28357, 11, 558, 11, 370, 597, 321, 434, 516, 281, 360, 965, 13], "temperature": 0.0, "avg_logprob": -0.2287066667929463, "compression_ratio": 1.5776699029126213, "no_speech_prob": 3.590697451727465e-05}, {"id": 28, "seek": 17700, "start": 185.0, "end": 206.0, "text": " Yeah. Great. What is the proper way. That was my question. Yeah, yeah, yeah. Great. Perfect. I love it when people ask the question that I want to solve today because that's a fine that it's a question worth asking, answering, I should say.", "tokens": [865, 13, 3769, 13, 708, 307, 264, 2296, 636, 13, 663, 390, 452, 1168, 13, 865, 11, 1338, 11, 1338, 13, 3769, 13, 10246, 13, 286, 959, 309, 562, 561, 1029, 264, 1168, 300, 286, 528, 281, 5039, 965, 570, 300, 311, 257, 2489, 300, 309, 311, 257, 1168, 3163, 3365, 11, 13430, 11, 286, 820, 584, 13], "temperature": 0.0, "avg_logprob": -0.2287066667929463, "compression_ratio": 1.5776699029126213, "no_speech_prob": 3.590697451727465e-05}, {"id": 29, "seek": 20600, "start": 206.0, "end": 222.0, "text": " Okay. Well, neither of my gradient machines are starting", "tokens": [1033, 13, 1042, 11, 9662, 295, 452, 16235, 8379, 366, 2891], "temperature": 0.0, "avg_logprob": -0.07742696030195369, "compression_ratio": 1.1967213114754098, "no_speech_prob": 1.3630010471388232e-05}, {"id": 30, "seek": 20600, "start": 222.0, "end": 224.0, "text": " setting up instance.", "tokens": [3287, 493, 5197, 13], "temperature": 0.0, "avg_logprob": -0.07742696030195369, "compression_ratio": 1.1967213114754098, "no_speech_prob": 1.3630010471388232e-05}, {"id": 31, "seek": 20600, "start": 224.0, "end": 225.0, "text": " That's not good.", "tokens": [663, 311, 406, 665, 13], "temperature": 0.0, "avg_logprob": -0.07742696030195369, "compression_ratio": 1.1967213114754098, "no_speech_prob": 1.3630010471388232e-05}, {"id": 32, "seek": 20600, "start": 225.0, "end": 234.0, "text": " All right, I guess we're going to use the terminal.", "tokens": [1057, 558, 11, 286, 2041, 321, 434, 516, 281, 764, 264, 14709, 13], "temperature": 0.0, "avg_logprob": -0.07742696030195369, "compression_ratio": 1.1967213114754098, "no_speech_prob": 1.3630010471388232e-05}, {"id": 33, "seek": 23400, "start": 234.0, "end": 243.0, "text": " Now the bad news is that I'm on my Mac, I don't think I've got anything set up.", "tokens": [823, 264, 1578, 2583, 307, 300, 286, 478, 322, 452, 5707, 11, 286, 500, 380, 519, 286, 600, 658, 1340, 992, 493, 13], "temperature": 0.0, "avg_logprob": -0.16470211082034641, "compression_ratio": 1.4285714285714286, "no_speech_prob": 1.4061407455301378e-05}, {"id": 34, "seek": 23400, "start": 243.0, "end": 246.0, "text": " You can use your fancy setup.", "tokens": [509, 393, 764, 428, 10247, 8657, 13], "temperature": 0.0, "avg_logprob": -0.16470211082034641, "compression_ratio": 1.4285714285714286, "no_speech_prob": 1.4061407455301378e-05}, {"id": 35, "seek": 23400, "start": 246.0, "end": 255.0, "text": " Thanks, script. Yeah, I mean, I mean, yeah, it's it's kind of slightly set up, I don't have the Kaggle stuff downloaded.", "tokens": [2561, 11, 5755, 13, 865, 11, 286, 914, 11, 286, 914, 11, 1338, 11, 309, 311, 309, 311, 733, 295, 4748, 992, 493, 11, 286, 500, 380, 362, 264, 48751, 22631, 1507, 21748, 13], "temperature": 0.0, "avg_logprob": -0.16470211082034641, "compression_ratio": 1.4285714285714286, "no_speech_prob": 1.4061407455301378e-05}, {"id": 36, "seek": 25500, "start": 255.0, "end": 279.0, "text": " So, all right, well, it's always good to revise anyway isn't it share screen portion share.", "tokens": [407, 11, 439, 558, 11, 731, 11, 309, 311, 1009, 665, 281, 44252, 4033, 1943, 380, 309, 2073, 2568, 8044, 2073, 13], "temperature": 0.0, "avg_logprob": -0.16543612480163575, "compression_ratio": 1.1264367816091954, "no_speech_prob": 6.048278919479344e-06}, {"id": 37, "seek": 25500, "start": 279.0, "end": 281.0, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.16543612480163575, "compression_ratio": 1.1264367816091954, "no_speech_prob": 6.048278919479344e-06}, {"id": 38, "seek": 28100, "start": 281.0, "end": 287.0, "text": " Okay, you guys can see that hopefully.", "tokens": [1033, 11, 291, 1074, 393, 536, 300, 4696, 13], "temperature": 0.0, "avg_logprob": -0.12906769783266128, "compression_ratio": 1.443609022556391, "no_speech_prob": 7.71422273828648e-05}, {"id": 39, "seek": 28100, "start": 287.0, "end": 292.0, "text": " I wonder if we should maybe make this a little bigger as well.", "tokens": [286, 2441, 498, 321, 820, 1310, 652, 341, 257, 707, 3801, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.12906769783266128, "compression_ratio": 1.443609022556391, "no_speech_prob": 7.71422273828648e-05}, {"id": 40, "seek": 28100, "start": 292.0, "end": 297.0, "text": " Okay, now you guys can see that.", "tokens": [1033, 11, 586, 291, 1074, 393, 536, 300, 13], "temperature": 0.0, "avg_logprob": -0.12906769783266128, "compression_ratio": 1.443609022556391, "no_speech_prob": 7.71422273828648e-05}, {"id": 41, "seek": 28100, "start": 297.0, "end": 298.0, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.12906769783266128, "compression_ratio": 1.443609022556391, "no_speech_prob": 7.71422273828648e-05}, {"id": 42, "seek": 28100, "start": 298.0, "end": 305.0, "text": " Is that reasonably visible.", "tokens": [1119, 300, 23551, 8974, 13], "temperature": 0.0, "avg_logprob": -0.12906769783266128, "compression_ratio": 1.443609022556391, "no_speech_prob": 7.71422273828648e-05}, {"id": 43, "seek": 28100, "start": 305.0, "end": 308.0, "text": " Yes, it is. Great. Yes.", "tokens": [1079, 11, 309, 307, 13, 3769, 13, 1079, 13], "temperature": 0.0, "avg_logprob": -0.12906769783266128, "compression_ratio": 1.443609022556391, "no_speech_prob": 7.71422273828648e-05}, {"id": 44, "seek": 30800, "start": 308.0, "end": 311.0, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.2007290766789363, "compression_ratio": 1.4846938775510203, "no_speech_prob": 1.3006212611799128e-05}, {"id": 45, "seek": 30800, "start": 311.0, "end": 318.0, "text": " So, so this is T marks, obviously, running in a terminal.", "tokens": [407, 11, 370, 341, 307, 314, 10640, 11, 2745, 11, 2614, 294, 257, 14709, 13], "temperature": 0.0, "avg_logprob": -0.2007290766789363, "compression_ratio": 1.4846938775510203, "no_speech_prob": 1.3006212611799128e-05}, {"id": 46, "seek": 30800, "start": 318.0, "end": 330.0, "text": " And because I'm sharing my screen I'm using this, you know, slightly lower resolution kind of area that usual so particularly good idea to to zoom into one of these pains.", "tokens": [400, 570, 286, 478, 5414, 452, 2568, 286, 478, 1228, 341, 11, 291, 458, 11, 4748, 3126, 8669, 733, 295, 1859, 300, 7713, 370, 4098, 665, 1558, 281, 281, 8863, 666, 472, 295, 613, 29774, 13], "temperature": 0.0, "avg_logprob": -0.2007290766789363, "compression_ratio": 1.4846938775510203, "no_speech_prob": 1.3006212611799128e-05}, {"id": 47, "seek": 30800, "start": 330.0, "end": 335.0, "text": " So I'll just get control BZ to zoom into the pain.", "tokens": [407, 286, 603, 445, 483, 1969, 363, 57, 281, 8863, 666, 264, 1822, 13], "temperature": 0.0, "avg_logprob": -0.2007290766789363, "compression_ratio": 1.4846938775510203, "no_speech_prob": 1.3006212611799128e-05}, {"id": 48, "seek": 33500, "start": 335.0, "end": 346.0, "text": " And let's see if on this machine I already okay so this machine does not currently have a Kaggle directory so store.", "tokens": [400, 718, 311, 536, 498, 322, 341, 3479, 286, 1217, 1392, 370, 341, 3479, 775, 406, 4362, 362, 257, 48751, 22631, 21120, 370, 3531, 13], "temperature": 0.0, "avg_logprob": -0.275856823756777, "compression_ratio": 1.2210526315789474, "no_speech_prob": 5.014566795580322e-06}, {"id": 49, "seek": 34600, "start": 346.0, "end": 366.0, "text": " I'll just use the Kaggle.", "tokens": [286, 603, 445, 764, 264, 48751, 22631, 13], "temperature": 0.0, "avg_logprob": -0.9622658093770345, "compression_ratio": 0.7575757575757576, "no_speech_prob": 6.853311788290739e-06}, {"id": 50, "seek": 36600, "start": 366.0, "end": 378.0, "text": " I have SSH config setup.", "tokens": [286, 362, 12238, 39, 6662, 8657, 13], "temperature": 0.0, "avg_logprob": -0.1714726516178676, "compression_ratio": 1.1885245901639345, "no_speech_prob": 2.1567384465015493e-06}, {"id": 51, "seek": 36600, "start": 378.0, "end": 386.0, "text": " Oh yeah that's right I think Max got some really old version of SCP that doesn't know how to do much so I might have to.", "tokens": [876, 1338, 300, 311, 558, 286, 519, 7402, 658, 512, 534, 1331, 3037, 295, 18489, 300, 1177, 380, 458, 577, 281, 360, 709, 370, 286, 1062, 362, 281, 13], "temperature": 0.0, "avg_logprob": -0.1714726516178676, "compression_ratio": 1.1885245901639345, "no_speech_prob": 2.1567384465015493e-06}, {"id": 52, "seek": 38600, "start": 386.0, "end": 401.0, "text": " Normally, with current version of open SSH, SCP you could tap complete, even to get remote files, which is quite great. And I think I noticed that Mac tends to ship really old versions of a lot of the unique software which is a shame.", "tokens": [17424, 11, 365, 2190, 3037, 295, 1269, 12238, 39, 11, 18489, 291, 727, 5119, 3566, 11, 754, 281, 483, 8607, 7098, 11, 597, 307, 1596, 869, 13, 400, 286, 519, 286, 5694, 300, 5707, 12258, 281, 5374, 534, 1331, 9606, 295, 257, 688, 295, 264, 3845, 4722, 597, 307, 257, 10069, 13], "temperature": 0.0, "avg_logprob": -0.1662635122026716, "compression_ratio": 1.368421052631579, "no_speech_prob": 4.637664460460655e-06}, {"id": 53, "seek": 40100, "start": 401.0, "end": 418.0, "text": " So we have to do it the slow way so we're going to copy dot Kaggle slash Kaggle dot JSON to here. Why am I putting in SSH move Kaggle dot JSON into dot Kaggle.", "tokens": [407, 321, 362, 281, 360, 309, 264, 2964, 636, 370, 321, 434, 516, 281, 5055, 5893, 48751, 22631, 17330, 48751, 22631, 5893, 31828, 281, 510, 13, 1545, 669, 286, 3372, 294, 12238, 39, 1286, 48751, 22631, 5893, 31828, 666, 5893, 48751, 22631, 13], "temperature": 0.0, "avg_logprob": -0.2071458628920258, "compression_ratio": 1.4, "no_speech_prob": 3.041438048967393e-06}, {"id": 54, "seek": 40100, "start": 418.0, "end": 420.0, "text": " And that should now work.", "tokens": [400, 300, 820, 586, 589, 13], "temperature": 0.0, "avg_logprob": -0.2071458628920258, "compression_ratio": 1.4, "no_speech_prob": 3.041438048967393e-06}, {"id": 55, "seek": 40100, "start": 420.0, "end": 424.0, "text": " Great. So,", "tokens": [3769, 13, 407, 11], "temperature": 0.0, "avg_logprob": -0.2071458628920258, "compression_ratio": 1.4, "no_speech_prob": 3.041438048967393e-06}, {"id": 56, "seek": 42400, "start": 424.0, "end": 439.0, "text": " let's download the data.", "tokens": [718, 311, 5484, 264, 1412, 13], "temperature": 0.0, "avg_logprob": -0.16357442140579223, "compression_ratio": 0.75, "no_speech_prob": 0.00020289330859668553}, {"id": 57, "seek": 43900, "start": 439.0, "end": 466.0, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.42174839973449707, "compression_ratio": 0.5555555555555556, "no_speech_prob": 5.137987682246603e-05}, {"id": 58, "seek": 46600, "start": 466.0, "end": 488.0, "text": " So we're going to go competitions, Patty, and data, and copy, and paste.", "tokens": [407, 321, 434, 516, 281, 352, 26185, 11, 44116, 11, 293, 1412, 11, 293, 5055, 11, 293, 9163, 13], "temperature": 0.0, "avg_logprob": -0.35545929618503735, "compression_ratio": 1.0434782608695652, "no_speech_prob": 8.26728228275897e-06}, {"id": 59, "seek": 48800, "start": 488.0, "end": 497.0, "text": " All right, and then we're going to see if we got Jupiter running.", "tokens": [1057, 558, 11, 293, 550, 321, 434, 516, 281, 536, 498, 321, 658, 24567, 2614, 13], "temperature": 0.0, "avg_logprob": -0.1878349275299997, "compression_ratio": 1.4223602484472049, "no_speech_prob": 2.72640659204626e-06}, {"id": 60, "seek": 48800, "start": 497.0, "end": 500.0, "text": " A bit crazy over there.", "tokens": [316, 857, 3219, 670, 456, 13], "temperature": 0.0, "avg_logprob": -0.1878349275299997, "compression_ratio": 1.4223602484472049, "no_speech_prob": 2.72640659204626e-06}, {"id": 61, "seek": 48800, "start": 500.0, "end": 504.0, "text": " So let's open up Jupiter.", "tokens": [407, 718, 311, 1269, 493, 24567, 13], "temperature": 0.0, "avg_logprob": -0.1878349275299997, "compression_ratio": 1.4223602484472049, "no_speech_prob": 2.72640659204626e-06}, {"id": 62, "seek": 48800, "start": 504.0, "end": 510.0, "text": " And if anybody keeps an eye on paper space let me know if paper space seems to start working.", "tokens": [400, 498, 4472, 5965, 364, 3313, 322, 3035, 1901, 718, 385, 458, 498, 3035, 1901, 2544, 281, 722, 1364, 13], "temperature": 0.0, "avg_logprob": -0.1878349275299997, "compression_ratio": 1.4223602484472049, "no_speech_prob": 2.72640659204626e-06}, {"id": 63, "seek": 48800, "start": 510.0, "end": 513.0, "text": " Right here's Patty.", "tokens": [1779, 510, 311, 44116, 13], "temperature": 0.0, "avg_logprob": -0.1878349275299997, "compression_ratio": 1.4223602484472049, "no_speech_prob": 2.72640659204626e-06}, {"id": 64, "seek": 51300, "start": 513.0, "end": 522.0, "text": " And I'll name Patty.", "tokens": [400, 286, 603, 1315, 44116, 13], "temperature": 0.0, "avg_logprob": -0.30312192851099473, "compression_ratio": 1.3181818181818181, "no_speech_prob": 2.9944205834908644e-06}, {"id": 65, "seek": 51300, "start": 522.0, "end": 525.0, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.30312192851099473, "compression_ratio": 1.3181818181818181, "no_speech_prob": 2.9944205834908644e-06}, {"id": 66, "seek": 51300, "start": 525.0, "end": 530.0, "text": " From fastai.vision.all import star.", "tokens": [3358, 2370, 1301, 13, 6763, 13, 336, 974, 3543, 13], "temperature": 0.0, "avg_logprob": -0.30312192851099473, "compression_ratio": 1.3181818181818181, "no_speech_prob": 2.9944205834908644e-06}, {"id": 67, "seek": 51300, "start": 530.0, "end": 535.0, "text": " Looks like this is finished so we can now unzip minus q.", "tokens": [10027, 411, 341, 307, 4335, 370, 321, 393, 586, 517, 27268, 3175, 9505, 13], "temperature": 0.0, "avg_logprob": -0.30312192851099473, "compression_ratio": 1.3181818181818181, "no_speech_prob": 2.9944205834908644e-06}, {"id": 68, "seek": 51300, "start": 535.0, "end": 539.0, "text": " Not that, unzip minus q.", "tokens": [1726, 300, 11, 517, 27268, 3175, 9505, 13], "temperature": 0.0, "avg_logprob": -0.30312192851099473, "compression_ratio": 1.3181818181818181, "no_speech_prob": 2.9944205834908644e-06}, {"id": 69, "seek": 51300, "start": 539.0, "end": 542.0, "text": " Patty disease classification.", "tokens": [44116, 4752, 21538, 13], "temperature": 0.0, "avg_logprob": -0.30312192851099473, "compression_ratio": 1.3181818181818181, "no_speech_prob": 2.9944205834908644e-06}, {"id": 70, "seek": 54200, "start": 542.0, "end": 547.0, "text": " There we go.", "tokens": [821, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.1230672513934928, "compression_ratio": 1.4195402298850575, "no_speech_prob": 9.222972039424349e-06}, {"id": 71, "seek": 54200, "start": 547.0, "end": 550.0, "text": " Oh, you know what we could do.", "tokens": [876, 11, 291, 458, 437, 321, 727, 360, 13], "temperature": 0.0, "avg_logprob": -0.1230672513934928, "compression_ratio": 1.4195402298850575, "no_speech_prob": 9.222972039424349e-06}, {"id": 72, "seek": 54200, "start": 550.0, "end": 558.0, "text": " We could run this on my GPU server because of course running this on the Macs is a bit of a dumb idea anyway.", "tokens": [492, 727, 1190, 341, 322, 452, 18407, 7154, 570, 295, 1164, 2614, 341, 322, 264, 5707, 82, 307, 257, 857, 295, 257, 10316, 1558, 4033, 13], "temperature": 0.0, "avg_logprob": -0.1230672513934928, "compression_ratio": 1.4195402298850575, "no_speech_prob": 9.222972039424349e-06}, {"id": 73, "seek": 54200, "start": 558.0, "end": 571.0, "text": " So, I don't think we've talked about how to do that before so this might be slightly obscure.", "tokens": [407, 11, 286, 500, 380, 519, 321, 600, 2825, 466, 577, 281, 360, 300, 949, 370, 341, 1062, 312, 4748, 34443, 13], "temperature": 0.0, "avg_logprob": -0.1230672513934928, "compression_ratio": 1.4195402298850575, "no_speech_prob": 9.222972039424349e-06}, {"id": 74, "seek": 57100, "start": 571.0, "end": 574.0, "text": " That's okay.", "tokens": [663, 311, 1392, 13], "temperature": 0.0, "avg_logprob": -0.15572000432897498, "compression_ratio": 1.3872832369942196, "no_speech_prob": 1.130023883888498e-05}, {"id": 75, "seek": 57100, "start": 574.0, "end": 579.0, "text": " I'll learn something in the process.", "tokens": [286, 603, 1466, 746, 294, 264, 1399, 13], "temperature": 0.0, "avg_logprob": -0.15572000432897498, "compression_ratio": 1.3872832369942196, "no_speech_prob": 1.130023883888498e-05}, {"id": 76, "seek": 57100, "start": 579.0, "end": 583.0, "text": " So, all right, I'm switching over now.", "tokens": [407, 11, 439, 558, 11, 286, 478, 16493, 670, 586, 13], "temperature": 0.0, "avg_logprob": -0.15572000432897498, "compression_ratio": 1.3872832369942196, "no_speech_prob": 1.130023883888498e-05}, {"id": 77, "seek": 57100, "start": 583.0, "end": 589.0, "text": " Actually, let's jump out of Tmux because running Tmux in Tmux is always a little bit weird.", "tokens": [5135, 11, 718, 311, 3012, 484, 295, 314, 76, 2449, 570, 2614, 314, 76, 2449, 294, 314, 76, 2449, 307, 1009, 257, 707, 857, 3657, 13], "temperature": 0.0, "avg_logprob": -0.15572000432897498, "compression_ratio": 1.3872832369942196, "no_speech_prob": 1.130023883888498e-05}, {"id": 78, "seek": 57100, "start": 589.0, "end": 594.0, "text": " So, this is my GPU server.", "tokens": [407, 11, 341, 307, 452, 18407, 7154, 13], "temperature": 0.0, "avg_logprob": -0.15572000432897498, "compression_ratio": 1.3872832369942196, "no_speech_prob": 1.130023883888498e-05}, {"id": 79, "seek": 57100, "start": 594.0, "end": 595.0, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.15572000432897498, "compression_ratio": 1.3872832369942196, "no_speech_prob": 1.130023883888498e-05}, {"id": 80, "seek": 57100, "start": 595.0, "end": 598.0, "text": " And it is running Jupiter.", "tokens": [400, 309, 307, 2614, 24567, 13], "temperature": 0.0, "avg_logprob": -0.15572000432897498, "compression_ratio": 1.3872832369942196, "no_speech_prob": 1.130023883888498e-05}, {"id": 81, "seek": 59800, "start": 598.0, "end": 615.0, "text": " And it says, oh, you can go to localhost 8888 to use this, but I can't, because that's not localhost refers to the machine I'm currently on and I'm not on this machine I'm SSHing into a remote machine.", "tokens": [400, 309, 1619, 11, 1954, 11, 291, 393, 352, 281, 2654, 6037, 1649, 16919, 23, 281, 764, 341, 11, 457, 286, 393, 380, 11, 570, 300, 311, 406, 2654, 6037, 14942, 281, 264, 3479, 286, 478, 4362, 322, 293, 286, 478, 406, 322, 341, 3479, 286, 478, 12238, 39, 278, 666, 257, 8607, 3479, 13], "temperature": 0.0, "avg_logprob": -0.11679726940090374, "compression_ratio": 1.4779411764705883, "no_speech_prob": 6.339055744319921e-06}, {"id": 82, "seek": 61500, "start": 615.0, "end": 634.0, "text": " But what I've done, and as I say this is like something that not everybody needs to know about for sure but for those who are interested, what I've done is when I SSH to this particular machine called local, it forwards in a thing that I use on my local", "tokens": [583, 437, 286, 600, 1096, 11, 293, 382, 286, 584, 341, 307, 411, 746, 300, 406, 2201, 2203, 281, 458, 466, 337, 988, 457, 337, 729, 567, 366, 3102, 11, 437, 286, 600, 1096, 307, 562, 286, 12238, 39, 281, 341, 1729, 3479, 1219, 2654, 11, 309, 30126, 294, 257, 551, 300, 286, 764, 322, 452, 2654], "temperature": 0.0, "avg_logprob": -0.1342278656206633, "compression_ratio": 1.601063829787234, "no_speech_prob": 8.139098099491093e-06}, {"id": 83, "seek": 61500, "start": 634.0, "end": 642.0, "text": " machines port 8888 to the remote machines 8888.", "tokens": [8379, 2436, 1649, 16919, 23, 281, 264, 8607, 8379, 1649, 16919, 23, 13], "temperature": 0.0, "avg_logprob": -0.1342278656206633, "compression_ratio": 1.601063829787234, "no_speech_prob": 8.139098099491093e-06}, {"id": 84, "seek": 64200, "start": 642.0, "end": 659.0, "text": " That means that I can use localhost 8888, and it will actually forward those packets to the remote machine and forward remote machine packets back to here. So this is called SSH forwarding.", "tokens": [663, 1355, 300, 286, 393, 764, 2654, 6037, 1649, 16919, 23, 11, 293, 309, 486, 767, 2128, 729, 30364, 281, 264, 8607, 3479, 293, 2128, 8607, 3479, 30364, 646, 281, 510, 13, 407, 341, 307, 1219, 12238, 39, 2128, 278, 13], "temperature": 0.0, "avg_logprob": -0.10234075579149969, "compression_ratio": 1.4620689655172414, "no_speech_prob": 1.4738287973159458e-05}, {"id": 85, "seek": 64200, "start": 659.0, "end": 662.0, "text": " You know, FYI.", "tokens": [509, 458, 11, 42730, 40, 13], "temperature": 0.0, "avg_logprob": -0.10234075579149969, "compression_ratio": 1.4620689655172414, "no_speech_prob": 1.4738287973159458e-05}, {"id": 86, "seek": 64200, "start": 662.0, "end": 670.0, "text": " So if I", "tokens": [407, 498, 286], "temperature": 0.0, "avg_logprob": -0.10234075579149969, "compression_ratio": 1.4620689655172414, "no_speech_prob": 1.4738287973159458e-05}, {"id": 87, "seek": 67000, "start": 670.0, "end": 674.0, "text": " go back here.", "tokens": [352, 646, 510, 13], "temperature": 0.0, "avg_logprob": -0.1807708263397217, "compression_ratio": 1.2280701754385965, "no_speech_prob": 1.2606608834175859e-05}, {"id": 88, "seek": 67000, "start": 674.0, "end": 683.0, "text": " Oh, and the other thing we should probably do is make sure that Jupiter is not running on the local machine.", "tokens": [876, 11, 293, 264, 661, 551, 321, 820, 1391, 360, 307, 652, 988, 300, 24567, 307, 406, 2614, 322, 264, 2654, 3479, 13], "temperature": 0.0, "avg_logprob": -0.1807708263397217, "compression_ratio": 1.2280701754385965, "no_speech_prob": 1.2606608834175859e-05}, {"id": 89, "seek": 67000, "start": 683.0, "end": 685.0, "text": " I'll cancel that.", "tokens": [286, 603, 10373, 300, 13], "temperature": 0.0, "avg_logprob": -0.1807708263397217, "compression_ratio": 1.2280701754385965, "no_speech_prob": 1.2606608834175859e-05}, {"id": 90, "seek": 68500, "start": 685.0, "end": 700.0, "text": " And so, to exit out of this, I could create another window or another tab or whatever but I can just hit Ctrl BD to detach from TMUX so that stuff's all running still in the background.", "tokens": [400, 370, 11, 281, 11043, 484, 295, 341, 11, 286, 727, 1884, 1071, 4910, 420, 1071, 4421, 420, 2035, 457, 286, 393, 445, 2045, 35233, 363, 35, 281, 43245, 490, 33550, 52, 55, 370, 300, 1507, 311, 439, 2614, 920, 294, 264, 3678, 13], "temperature": 0.0, "avg_logprob": -0.1166847791427221, "compression_ratio": 1.4102564102564104, "no_speech_prob": 1.1726189086402883e-06}, {"id": 91, "seek": 68500, "start": 700.0, "end": 703.0, "text": " And then I can SSH to my machine.", "tokens": [400, 550, 286, 393, 12238, 39, 281, 452, 3479, 13], "temperature": 0.0, "avg_logprob": -0.1166847791427221, "compression_ratio": 1.4102564102564104, "no_speech_prob": 1.1726189086402883e-06}, {"id": 92, "seek": 68500, "start": 703.0, "end": 710.0, "text": " And let's see if that's all working. There we go. Okay.", "tokens": [400, 718, 311, 536, 498, 300, 311, 439, 1364, 13, 821, 321, 352, 13, 1033, 13], "temperature": 0.0, "avg_logprob": -0.1166847791427221, "compression_ratio": 1.4102564102564104, "no_speech_prob": 1.1726189086402883e-06}, {"id": 93, "seek": 71000, "start": 710.0, "end": 720.0, "text": " That's great. And so here we actually have that going so we can create a new notebook.", "tokens": [663, 311, 869, 13, 400, 370, 510, 321, 767, 362, 300, 516, 370, 321, 393, 1884, 257, 777, 21060, 13], "temperature": 0.0, "avg_logprob": -0.0574499721258459, "compression_ratio": 1.476923076923077, "no_speech_prob": 1.8738453491096152e-06}, {"id": 94, "seek": 71000, "start": 720.0, "end": 739.0, "text": " And it's easy enough by the way if you do like, buy a machine with a GPU, which it's not a terrible idea, especially if eventually GPU prices start to come back down to reasonable levels at some point.", "tokens": [400, 309, 311, 1858, 1547, 538, 264, 636, 498, 291, 360, 411, 11, 2256, 257, 3479, 365, 257, 18407, 11, 597, 309, 311, 406, 257, 6237, 1558, 11, 2318, 498, 4728, 18407, 7901, 722, 281, 808, 646, 760, 281, 10585, 4358, 412, 512, 935, 13], "temperature": 0.0, "avg_logprob": -0.0574499721258459, "compression_ratio": 1.476923076923077, "no_speech_prob": 1.8738453491096152e-06}, {"id": 95, "seek": 73900, "start": 739.0, "end": 748.0, "text": " But, you know, it doesn't need to be a notebook or anything you can check it anywhere in the house, just like I've done and as you can see log into it from your computer.", "tokens": [583, 11, 291, 458, 11, 309, 1177, 380, 643, 281, 312, 257, 21060, 420, 1340, 291, 393, 1520, 309, 4992, 294, 264, 1782, 11, 445, 411, 286, 600, 1096, 293, 382, 291, 393, 536, 3565, 666, 309, 490, 428, 3820, 13], "temperature": 0.0, "avg_logprob": -0.11870384216308594, "compression_ratio": 1.544378698224852, "no_speech_prob": 5.954721473244717e-06}, {"id": 96, "seek": 73900, "start": 748.0, "end": 757.0, "text": " Now, I can only log into mine, right, you know, by default, I don't even log in from home.", "tokens": [823, 11, 286, 393, 787, 3565, 666, 3892, 11, 558, 11, 291, 458, 11, 538, 7576, 11, 286, 500, 380, 754, 3565, 294, 490, 1280, 13], "temperature": 0.0, "avg_logprob": -0.11870384216308594, "compression_ratio": 1.544378698224852, "no_speech_prob": 5.954721473244717e-06}, {"id": 97, "seek": 75700, "start": 757.0, "end": 770.0, "text": " If you want to be able to log in when you're not at home, you would have to go into your routers settings and say forward port 22, which is the SSH port to your GPU server.", "tokens": [759, 291, 528, 281, 312, 1075, 281, 3565, 294, 562, 291, 434, 406, 412, 1280, 11, 291, 576, 362, 281, 352, 666, 428, 4020, 433, 6257, 293, 584, 2128, 2436, 5853, 11, 597, 307, 264, 12238, 39, 2436, 281, 428, 18407, 7154, 13], "temperature": 0.0, "avg_logprob": -0.09129810333251953, "compression_ratio": 1.4728260869565217, "no_speech_prob": 4.860256467509316e-06}, {"id": 98, "seek": 75700, "start": 770.0, "end": 779.0, "text": " And you'd also have to know the IP address that your house's Wi Fi is on and that tends to change.", "tokens": [400, 291, 1116, 611, 362, 281, 458, 264, 8671, 2985, 300, 428, 1782, 311, 14035, 38245, 307, 322, 293, 300, 12258, 281, 1319, 13], "temperature": 0.0, "avg_logprob": -0.09129810333251953, "compression_ratio": 1.4728260869565217, "no_speech_prob": 4.860256467509316e-06}, {"id": 99, "seek": 77900, "start": 779.0, "end": 789.0, "text": " So you can use something called dynamic DNS. There's lots of different providers of dynamic DNS so you something called din.com just because they've been around forever.", "tokens": [407, 291, 393, 764, 746, 1219, 8546, 35153, 13, 821, 311, 3195, 295, 819, 11330, 295, 8546, 35153, 370, 291, 746, 1219, 3791, 13, 1112, 445, 570, 436, 600, 668, 926, 5680, 13], "temperature": 0.0, "avg_logprob": -0.14351905264505527, "compression_ratio": 1.5633802816901408, "no_speech_prob": 3.3930084555322537e-06}, {"id": 100, "seek": 77900, "start": 789.0, "end": 798.0, "text": " And so, yeah, so I can log into my machine from from anywhere, which is, which is very nice.", "tokens": [400, 370, 11, 1338, 11, 370, 286, 393, 3565, 666, 452, 3479, 490, 490, 4992, 11, 597, 307, 11, 597, 307, 588, 1481, 13], "temperature": 0.0, "avg_logprob": -0.14351905264505527, "compression_ratio": 1.5633802816901408, "no_speech_prob": 3.3930084555322537e-06}, {"id": 101, "seek": 77900, "start": 798.0, "end": 808.0, "text": " Okay, so let's try this again from fast AI dot vision dot all imports.", "tokens": [1033, 11, 370, 718, 311, 853, 341, 797, 490, 2370, 7318, 5893, 5201, 5893, 439, 41596, 13], "temperature": 0.0, "avg_logprob": -0.14351905264505527, "compression_ratio": 1.5633802816901408, "no_speech_prob": 3.3930084555322537e-06}, {"id": 102, "seek": 80800, "start": 808.0, "end": 817.0, "text": " All right. And so path equals.", "tokens": [1057, 558, 13, 400, 370, 3100, 6915, 13], "temperature": 0.0, "avg_logprob": -0.19537474427904403, "compression_ratio": 1.3577235772357723, "no_speech_prob": 5.594231879513245e-06}, {"id": 103, "seek": 80800, "start": 817.0, "end": 822.0, "text": " So we can do LS in bash like so here we go.", "tokens": [407, 321, 393, 360, 36657, 294, 46183, 411, 370, 510, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.19537474427904403, "compression_ratio": 1.3577235772357723, "no_speech_prob": 5.594231879513245e-06}, {"id": 104, "seek": 80800, "start": 822.0, "end": 828.0, "text": " Great. So you can just use the current path.", "tokens": [3769, 13, 407, 291, 393, 445, 764, 264, 2190, 3100, 13], "temperature": 0.0, "avg_logprob": -0.19537474427904403, "compression_ratio": 1.3577235772357723, "no_speech_prob": 5.594231879513245e-06}, {"id": 105, "seek": 80800, "start": 828.0, "end": 831.0, "text": " Is where our data is.", "tokens": [1119, 689, 527, 1412, 307, 13], "temperature": 0.0, "avg_logprob": -0.19537474427904403, "compression_ratio": 1.3577235772357723, "no_speech_prob": 5.594231879513245e-06}, {"id": 106, "seek": 80800, "start": 831.0, "end": 835.0, "text": " And so our training path.", "tokens": [400, 370, 527, 3097, 3100, 13], "temperature": 0.0, "avg_logprob": -0.19537474427904403, "compression_ratio": 1.3577235772357723, "no_speech_prob": 5.594231879513245e-06}, {"id": 107, "seek": 83500, "start": 835.0, "end": 842.0, "text": " And we're also.", "tokens": [400, 321, 434, 611, 13], "temperature": 0.0, "avg_logprob": -0.2685955475116598, "compression_ratio": 1.0142857142857142, "no_speech_prob": 4.860374247073196e-06}, {"id": 108, "seek": 83500, "start": 842.0, "end": 848.0, "text": " And then our training path.", "tokens": [400, 550, 527, 3097, 3100, 13], "temperature": 0.0, "avg_logprob": -0.2685955475116598, "compression_ratio": 1.0142857142857142, "no_speech_prob": 4.860374247073196e-06}, {"id": 109, "seek": 83500, "start": 848.0, "end": 851.0, "text": " Is", "tokens": [1119], "temperature": 0.0, "avg_logprob": -0.2685955475116598, "compression_ratio": 1.0142857142857142, "no_speech_prob": 4.860374247073196e-06}, {"id": 110, "seek": 83500, "start": 851.0, "end": 855.0, "text": " path slash", "tokens": [3100, 17330], "temperature": 0.0, "avg_logprob": -0.2685955475116598, "compression_ratio": 1.0142857142857142, "no_speech_prob": 4.860374247073196e-06}, {"id": 111, "seek": 83500, "start": 855.0, "end": 857.0, "text": " train images.", "tokens": [3847, 5267, 13], "temperature": 0.0, "avg_logprob": -0.2685955475116598, "compression_ratio": 1.0142857142857142, "no_speech_prob": 4.860374247073196e-06}, {"id": 112, "seek": 85700, "start": 857.0, "end": 867.0, "text": " I'm sure we've really talked much about path lip before us so this this path object comes from a Python class called path lip.", "tokens": [286, 478, 988, 321, 600, 534, 2825, 709, 466, 3100, 8280, 949, 505, 370, 341, 341, 3100, 2657, 1487, 490, 257, 15329, 1508, 1219, 3100, 8280, 13], "temperature": 0.0, "avg_logprob": -0.19245677207832906, "compression_ratio": 1.4787234042553192, "no_speech_prob": 5.5073951443773694e-06}, {"id": 113, "seek": 85700, "start": 867.0, "end": 877.0, "text": " And it's imported by default with pretty much any fast core or fast AI thing you use. So to learn about it, obviously you can just Google for path lip.", "tokens": [400, 309, 311, 25524, 538, 7576, 365, 1238, 709, 604, 2370, 4965, 420, 2370, 7318, 551, 291, 764, 13, 407, 281, 1466, 466, 309, 11, 2745, 291, 393, 445, 3329, 337, 3100, 8280, 13], "temperature": 0.0, "avg_logprob": -0.19245677207832906, "compression_ratio": 1.4787234042553192, "no_speech_prob": 5.5073951443773694e-06}, {"id": 114, "seek": 87700, "start": 877.0, "end": 899.0, "text": " And basically, yeah, it lets you create a part this is you know path in your current working directory, or you can do something here to go to a relative directory, or you can go to a absolute directory, and then you can, you know, it's kind of got this somewhat neat", "tokens": [400, 1936, 11, 1338, 11, 309, 6653, 291, 1884, 257, 644, 341, 307, 291, 458, 3100, 294, 428, 2190, 1364, 21120, 11, 420, 291, 393, 360, 746, 510, 281, 352, 281, 257, 4972, 21120, 11, 420, 291, 393, 352, 281, 257, 8236, 21120, 11, 293, 550, 291, 393, 11, 291, 458, 11, 309, 311, 733, 295, 658, 341, 8344, 10654], "temperature": 0.0, "avg_logprob": -0.14862331748008728, "compression_ratio": 1.694267515923567, "no_speech_prob": 4.425392489793012e-06}, {"id": 115, "seek": 89900, "start": 899.0, "end": 914.0, "text": " use of the slash operator to mean, you know, go to a sub directory. Ls doesn't come with it by default fast core adds that to list things, as you say.", "tokens": [764, 295, 264, 17330, 12973, 281, 914, 11, 291, 458, 11, 352, 281, 257, 1422, 21120, 13, 441, 82, 1177, 380, 808, 365, 309, 538, 7576, 2370, 4965, 10860, 300, 281, 1329, 721, 11, 382, 291, 584, 13], "temperature": 0.0, "avg_logprob": -0.16432138391443202, "compression_ratio": 1.4728260869565217, "no_speech_prob": 5.5074910960684065e-06}, {"id": 116, "seek": 89900, "start": 914.0, "end": 920.0, "text": " Yeah, so that's pretty cool. And so the other thing we did yesterday was we looked at the files.", "tokens": [865, 11, 370, 300, 311, 1238, 1627, 13, 400, 370, 264, 661, 551, 321, 630, 5186, 390, 321, 2956, 412, 264, 7098, 13], "temperature": 0.0, "avg_logprob": -0.16432138391443202, "compression_ratio": 1.4728260869565217, "no_speech_prob": 5.5074910960684065e-06}, {"id": 117, "seek": 89900, "start": 920.0, "end": 926.0, "text": " Get image files inside.", "tokens": [3240, 3256, 7098, 1854, 13], "temperature": 0.0, "avg_logprob": -0.16432138391443202, "compression_ratio": 1.4728260869565217, "no_speech_prob": 5.5074910960684065e-06}, {"id": 118, "seek": 92600, "start": 926.0, "end": 931.0, "text": " Let's have a look at the ones inside the training path.", "tokens": [961, 311, 362, 257, 574, 412, 264, 2306, 1854, 264, 3097, 3100, 13], "temperature": 0.0, "avg_logprob": -0.14920673614893204, "compression_ratio": 1.1808510638297873, "no_speech_prob": 6.747765382897342e-06}, {"id": 119, "seek": 92600, "start": 931.0, "end": 943.0, "text": " And so we can create an image.", "tokens": [400, 370, 321, 393, 1884, 364, 3256, 13], "temperature": 0.0, "avg_logprob": -0.14920673614893204, "compression_ratio": 1.1808510638297873, "no_speech_prob": 6.747765382897342e-06}, {"id": 120, "seek": 92600, "start": 943.0, "end": 947.0, "text": " We can look at it.", "tokens": [492, 393, 574, 412, 309, 13], "temperature": 0.0, "avg_logprob": -0.14920673614893204, "compression_ratio": 1.1808510638297873, "no_speech_prob": 6.747765382897342e-06}, {"id": 121, "seek": 92600, "start": 947.0, "end": 950.0, "text": " Size.", "tokens": [35818, 13], "temperature": 0.0, "avg_logprob": -0.14920673614893204, "compression_ratio": 1.1808510638297873, "no_speech_prob": 6.747765382897342e-06}, {"id": 122, "seek": 95000, "start": 950.0, "end": 957.0, "text": " Which is a property. Okay, so there's a few things we can do that kind of gets us back to where we were yesterday.", "tokens": [3013, 307, 257, 4707, 13, 1033, 11, 370, 456, 311, 257, 1326, 721, 321, 393, 360, 300, 733, 295, 2170, 505, 646, 281, 689, 321, 645, 5186, 13], "temperature": 0.0, "avg_logprob": -0.13157477974891663, "compression_ratio": 1.4424242424242424, "no_speech_prob": 5.7717261370271444e-06}, {"id": 123, "seek": 95000, "start": 957.0, "end": 969.0, "text": " So a question I saw in the forum was, how would I get like the sizes of all of the files that actually showed how to do it.", "tokens": [407, 257, 1168, 286, 1866, 294, 264, 17542, 390, 11, 577, 576, 286, 483, 411, 264, 11602, 295, 439, 295, 264, 7098, 300, 767, 4712, 577, 281, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.13157477974891663, "compression_ratio": 1.4424242424242424, "no_speech_prob": 5.7717261370271444e-06}, {"id": 124, "seek": 96900, "start": 969.0, "end": 983.0, "text": " So we can do it kind of slightly slow way, which is, let's do it the slightly slow way and time how long it takes sizes equals.", "tokens": [407, 321, 393, 360, 309, 733, 295, 4748, 2964, 636, 11, 597, 307, 11, 718, 311, 360, 309, 264, 4748, 2964, 636, 293, 565, 577, 938, 309, 2516, 11602, 6915, 13], "temperature": 0.0, "avg_logprob": -0.2836825507027762, "compression_ratio": 1.3229166666666667, "no_speech_prob": 6.854063485661754e-06}, {"id": 125, "seek": 98300, "start": 983.0, "end": 1002.0, "text": " Paste that here.", "tokens": [43827, 300, 510, 13], "temperature": 0.0, "avg_logprob": -0.31319451332092285, "compression_ratio": 0.7241379310344828, "no_speech_prob": 1.078103650797857e-05}, {"id": 126, "seek": 98300, "start": 1002.0, "end": 1005.0, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.31319451332092285, "compression_ratio": 0.7241379310344828, "no_speech_prob": 1.078103650797857e-05}, {"id": 127, "seek": 100500, "start": 1005.0, "end": 1013.0, "text": " Sarah question.", "tokens": [9519, 1168, 13], "temperature": 0.0, "avg_logprob": -0.13909404031161604, "compression_ratio": 1.486842105263158, "no_speech_prob": 5.507412879524054e-06}, {"id": 128, "seek": 100500, "start": 1013.0, "end": 1017.0, "text": " So to do this in parallel, which would obviously be faster.", "tokens": [407, 281, 360, 341, 294, 8952, 11, 597, 576, 2745, 312, 4663, 13], "temperature": 0.0, "avg_logprob": -0.13909404031161604, "compression_ratio": 1.486842105263158, "no_speech_prob": 5.507412879524054e-06}, {"id": 129, "seek": 100500, "start": 1017.0, "end": 1025.0, "text": " One would expect I mean, it would depend if the most of the time is spent reading this from the disk, then doing this in parallel won't be any faster.", "tokens": [1485, 576, 2066, 286, 914, 11, 309, 576, 5672, 498, 264, 881, 295, 264, 565, 307, 4418, 3760, 341, 490, 264, 12355, 11, 550, 884, 341, 294, 8952, 1582, 380, 312, 604, 4663, 13], "temperature": 0.0, "avg_logprob": -0.13909404031161604, "compression_ratio": 1.486842105263158, "no_speech_prob": 5.507412879524054e-06}, {"id": 130, "seek": 102500, "start": 1025.0, "end": 1036.0, "text": " Most of the time is being spent decoding the JPEG, then doing this in parallel will be faster. And which of those is true will depend on whether we're using an SSD or not.", "tokens": [4534, 295, 264, 565, 307, 885, 4418, 979, 8616, 264, 508, 5208, 38, 11, 550, 884, 341, 294, 8952, 486, 312, 4663, 13, 400, 597, 295, 729, 307, 2074, 486, 5672, 322, 1968, 321, 434, 1228, 364, 30262, 420, 406, 13], "temperature": 0.0, "avg_logprob": -0.16878342890477444, "compression_ratio": 1.5219512195121951, "no_speech_prob": 1.653671461099293e-06}, {"id": 131, "seek": 102500, "start": 1036.0, "end": 1045.0, "text": " So anyway, I'll show you how to do it. So if you import fastcord.parallel.", "tokens": [407, 4033, 11, 286, 603, 855, 291, 577, 281, 360, 309, 13, 407, 498, 291, 974, 2370, 66, 765, 13, 2181, 336, 338, 13], "temperature": 0.0, "avg_logprob": -0.16878342890477444, "compression_ratio": 1.5219512195121951, "no_speech_prob": 1.653671461099293e-06}, {"id": 132, "seek": 102500, "start": 1045.0, "end": 1047.0, "text": " Which is a module.", "tokens": [3013, 307, 257, 10088, 13], "temperature": 0.0, "avg_logprob": -0.16878342890477444, "compression_ratio": 1.5219512195121951, "no_speech_prob": 1.653671461099293e-06}, {"id": 133, "seek": 102500, "start": 1047.0, "end": 1050.0, "text": " That module contains a function.", "tokens": [663, 10088, 8306, 257, 2445, 13], "temperature": 0.0, "avg_logprob": -0.16878342890477444, "compression_ratio": 1.5219512195121951, "no_speech_prob": 1.653671461099293e-06}, {"id": 134, "seek": 102500, "start": 1050.0, "end": 1053.0, "text": " For parallel.", "tokens": [1171, 8952, 13], "temperature": 0.0, "avg_logprob": -0.16878342890477444, "compression_ratio": 1.5219512195121951, "no_speech_prob": 1.653671461099293e-06}, {"id": 135, "seek": 105300, "start": 1053.0, "end": 1058.0, "text": " Which applies this function to these items.", "tokens": [3013, 13165, 341, 2445, 281, 613, 4754, 13], "temperature": 0.0, "avg_logprob": -0.16555017774755304, "compression_ratio": 1.2647058823529411, "no_speech_prob": 5.771808446297655e-06}, {"id": 136, "seek": 105300, "start": 1058.0, "end": 1063.0, "text": " So the function we want to apply.", "tokens": [407, 264, 2445, 321, 528, 281, 3079, 13], "temperature": 0.0, "avg_logprob": -0.16555017774755304, "compression_ratio": 1.2647058823529411, "no_speech_prob": 5.771808446297655e-06}, {"id": 137, "seek": 105300, "start": 1063.0, "end": 1066.0, "text": " Is.", "tokens": [1119, 13], "temperature": 0.0, "avg_logprob": -0.16555017774755304, "compression_ratio": 1.2647058823529411, "no_speech_prob": 5.771808446297655e-06}, {"id": 138, "seek": 105300, "start": 1066.0, "end": 1072.0, "text": " And let's look at the doc for it.", "tokens": [400, 718, 311, 574, 412, 264, 3211, 337, 309, 13], "temperature": 0.0, "avg_logprob": -0.16555017774755304, "compression_ratio": 1.2647058823529411, "no_speech_prob": 5.771808446297655e-06}, {"id": 139, "seek": 105300, "start": 1072.0, "end": 1078.0, "text": " Show in docs.", "tokens": [6895, 294, 45623, 13], "temperature": 0.0, "avg_logprob": -0.16555017774755304, "compression_ratio": 1.2647058823529411, "no_speech_prob": 5.771808446297655e-06}, {"id": 140, "seek": 107800, "start": 1078.0, "end": 1083.0, "text": " So here's an example of parallel.", "tokens": [407, 510, 311, 364, 1365, 295, 8952, 13], "temperature": 0.0, "avg_logprob": -0.21985506189280543, "compression_ratio": 1.4402985074626866, "no_speech_prob": 1.3006696462980472e-05}, {"id": 141, "seek": 107800, "start": 1083.0, "end": 1085.0, "text": " That should be.", "tokens": [663, 820, 312, 13], "temperature": 0.0, "avg_logprob": -0.21985506189280543, "compression_ratio": 1.4402985074626866, "no_speech_prob": 1.3006696462980472e-05}, {"id": 142, "seek": 107800, "start": 1085.0, "end": 1095.0, "text": " Ordered, but never mind. So here's something that takes two things, X and A, and adds something to each one.", "tokens": [16321, 292, 11, 457, 1128, 1575, 13, 407, 510, 311, 746, 300, 2516, 732, 721, 11, 1783, 293, 316, 11, 293, 10860, 746, 281, 1184, 472, 13], "temperature": 0.0, "avg_logprob": -0.21985506189280543, "compression_ratio": 1.4402985074626866, "no_speech_prob": 1.3006696462980472e-05}, {"id": 143, "seek": 107800, "start": 1095.0, "end": 1098.0, "text": " And so here's how we use parallel.", "tokens": [400, 370, 510, 311, 577, 321, 764, 8952, 13], "temperature": 0.0, "avg_logprob": -0.21985506189280543, "compression_ratio": 1.4402985074626866, "no_speech_prob": 1.3006696462980472e-05}, {"id": 144, "seek": 109800, "start": 1098.0, "end": 1113.0, "text": " The docs for fast.ai libraries are a bit different to some in that the tests and the docs are all one thing. So to read this, this is saying if you call parallel, passing it this function, which is just", "tokens": [440, 45623, 337, 2370, 13, 1301, 15148, 366, 257, 857, 819, 281, 512, 294, 300, 264, 6921, 293, 264, 45623, 366, 439, 472, 551, 13, 407, 281, 1401, 341, 11, 341, 307, 1566, 498, 291, 818, 8952, 11, 8437, 309, 341, 2445, 11, 597, 307, 445], "temperature": 0.0, "avg_logprob": -0.14524759565080916, "compression_ratio": 1.5351351351351352, "no_speech_prob": 2.1233361167105613e-06}, {"id": 145, "seek": 109800, "start": 1113.0, "end": 1117.0, "text": " X plus A where A defaults to one.", "tokens": [1783, 1804, 316, 689, 316, 7576, 82, 281, 472, 13], "temperature": 0.0, "avg_logprob": -0.14524759565080916, "compression_ratio": 1.5351351351351352, "no_speech_prob": 2.1233361167105613e-06}, {"id": 146, "seek": 109800, "start": 1117.0, "end": 1122.0, "text": " And you do it on this input, which is range 50.", "tokens": [400, 291, 360, 309, 322, 341, 4846, 11, 597, 307, 3613, 2625, 13], "temperature": 0.0, "avg_logprob": -0.14524759565080916, "compression_ratio": 1.5351351351351352, "no_speech_prob": 2.1233361167105613e-06}, {"id": 147, "seek": 112200, "start": 1122.0, "end": 1128.0, "text": " Then you would expect to get this output, which is the range from one to 51.", "tokens": [1396, 291, 576, 2066, 281, 483, 341, 5598, 11, 597, 307, 264, 3613, 490, 472, 281, 18485, 13], "temperature": 0.0, "avg_logprob": -0.09400657554725547, "compression_ratio": 1.6628571428571428, "no_speech_prob": 2.6015993626060663e-06}, {"id": 148, "seek": 112200, "start": 1128.0, "end": 1137.0, "text": " So this kind of is showing you lots of examples of using the function and telling you what you would expect to get for each one.", "tokens": [407, 341, 733, 295, 307, 4099, 291, 3195, 295, 5110, 295, 1228, 264, 2445, 293, 3585, 291, 437, 291, 576, 2066, 281, 483, 337, 1184, 472, 13], "temperature": 0.0, "avg_logprob": -0.09400657554725547, "compression_ratio": 1.6628571428571428, "no_speech_prob": 2.6015993626060663e-06}, {"id": 149, "seek": 112200, "start": 1137.0, "end": 1143.0, "text": " So if we want a function, which is going to take some file.", "tokens": [407, 498, 321, 528, 257, 2445, 11, 597, 307, 516, 281, 747, 512, 3991, 13], "temperature": 0.0, "avg_logprob": -0.09400657554725547, "compression_ratio": 1.6628571428571428, "no_speech_prob": 2.6015993626060663e-06}, {"id": 150, "seek": 112200, "start": 1143.0, "end": 1147.0, "text": " And it's going to return.", "tokens": [400, 309, 311, 516, 281, 2736, 13], "temperature": 0.0, "avg_logprob": -0.09400657554725547, "compression_ratio": 1.6628571428571428, "no_speech_prob": 2.6015993626060663e-06}, {"id": 151, "seek": 114700, "start": 1147.0, "end": 1154.0, "text": " This.", "tokens": [639, 13], "temperature": 0.0, "avg_logprob": -0.15179457203034433, "compression_ratio": 1.6744186046511629, "no_speech_prob": 9.22332674235804e-06}, {"id": 152, "seek": 114700, "start": 1154.0, "end": 1158.0, "text": " And so if we want to run that in parallel.", "tokens": [400, 370, 498, 321, 528, 281, 1190, 300, 294, 8952, 13], "temperature": 0.0, "avg_logprob": -0.15179457203034433, "compression_ratio": 1.6744186046511629, "no_speech_prob": 9.22332674235804e-06}, {"id": 153, "seek": 114700, "start": 1158.0, "end": 1164.0, "text": " Then we can say parallel.", "tokens": [1396, 321, 393, 584, 8952, 13], "temperature": 0.0, "avg_logprob": -0.15179457203034433, "compression_ratio": 1.6744186046511629, "no_speech_prob": 9.22332674235804e-06}, {"id": 154, "seek": 114700, "start": 1164.0, "end": 1171.0, "text": " And the function we want to run is this function and the files we want to run on is files, and there's lots of other things we could pass in.", "tokens": [400, 264, 2445, 321, 528, 281, 1190, 307, 341, 2445, 293, 264, 7098, 321, 528, 281, 1190, 322, 307, 7098, 11, 293, 456, 311, 3195, 295, 661, 721, 321, 727, 1320, 294, 13], "temperature": 0.0, "avg_logprob": -0.15179457203034433, "compression_ratio": 1.6744186046511629, "no_speech_prob": 9.22332674235804e-06}, {"id": 155, "seek": 117100, "start": 1171.0, "end": 1179.0, "text": " So like let's say we want to do it on four parallel workers, see if that ends up any faster.", "tokens": [407, 411, 718, 311, 584, 321, 528, 281, 360, 309, 322, 1451, 8952, 5600, 11, 536, 498, 300, 5314, 493, 604, 4663, 13], "temperature": 0.0, "avg_logprob": -0.10922671971696146, "compression_ratio": 1.619718309859155, "no_speech_prob": 1.4367313951879623e-06}, {"id": 156, "seek": 117100, "start": 1179.0, "end": 1187.0, "text": " So as you can see running stuff in parallel, when you use fast core is actually pretty fast and easy.", "tokens": [407, 382, 291, 393, 536, 2614, 1507, 294, 8952, 11, 562, 291, 764, 2370, 4965, 307, 767, 1238, 2370, 293, 1858, 13], "temperature": 0.0, "avg_logprob": -0.10922671971696146, "compression_ratio": 1.619718309859155, "no_speech_prob": 1.4367313951879623e-06}, {"id": 157, "seek": 117100, "start": 1187.0, "end": 1198.0, "text": " But as I say it doesn't necessarily result in a speed up. If the main thing that's taking time is getting stuff off the disk, then it won't be faster.", "tokens": [583, 382, 286, 584, 309, 1177, 380, 4725, 1874, 294, 257, 3073, 493, 13, 759, 264, 2135, 551, 300, 311, 1940, 565, 307, 1242, 1507, 766, 264, 12355, 11, 550, 309, 1582, 380, 312, 4663, 13], "temperature": 0.0, "avg_logprob": -0.10922671971696146, "compression_ratio": 1.619718309859155, "no_speech_prob": 1.4367313951879623e-06}, {"id": 158, "seek": 119800, "start": 1198.0, "end": 1202.0, "text": " So in this case, it was a bit faster.", "tokens": [407, 294, 341, 1389, 11, 309, 390, 257, 857, 4663, 13], "temperature": 0.0, "avg_logprob": -0.20694222507706608, "compression_ratio": 1.4973262032085561, "no_speech_prob": 3.9667693272349425e-06}, {"id": 159, "seek": 119800, "start": 1202.0, "end": 1210.0, "text": " I think they use really slow disks on. Actually this is my disk. This is a good disk. So that ought to be fast.", "tokens": [286, 519, 436, 764, 534, 2964, 41617, 322, 13, 5135, 341, 307, 452, 12355, 13, 639, 307, 257, 665, 12355, 13, 407, 300, 13416, 281, 312, 2370, 13], "temperature": 0.0, "avg_logprob": -0.20694222507706608, "compression_ratio": 1.4973262032085561, "no_speech_prob": 3.9667693272349425e-06}, {"id": 160, "seek": 119800, "start": 1210.0, "end": 1215.0, "text": " So we could see if increasing it further is faster still.", "tokens": [407, 321, 727, 536, 498, 5662, 309, 3052, 307, 4663, 920, 13], "temperature": 0.0, "avg_logprob": -0.20694222507706608, "compression_ratio": 1.4973262032085561, "no_speech_prob": 3.9667693272349425e-06}, {"id": 161, "seek": 119800, "start": 1215.0, "end": 1219.0, "text": " I guess we've probably.", "tokens": [286, 2041, 321, 600, 1391, 13], "temperature": 0.0, "avg_logprob": -0.20694222507706608, "compression_ratio": 1.4973262032085561, "no_speech_prob": 3.9667693272349425e-06}, {"id": 162, "seek": 119800, "start": 1219.0, "end": 1221.0, "text": " We'll see.", "tokens": [492, 603, 536, 13], "temperature": 0.0, "avg_logprob": -0.20694222507706608, "compression_ratio": 1.4973262032085561, "no_speech_prob": 3.9667693272349425e-06}, {"id": 163, "seek": 119800, "start": 1221.0, "end": 1224.0, "text": " So Jeremy, quick question about this.", "tokens": [407, 17809, 11, 1702, 1168, 466, 341, 13], "temperature": 0.0, "avg_logprob": -0.20694222507706608, "compression_ratio": 1.4973262032085561, "no_speech_prob": 3.9667693272349425e-06}, {"id": 164, "seek": 122400, "start": 1224.0, "end": 1231.0, "text": " Does this use CPU cores, or is it using the GPU or running it in parallel?", "tokens": [4402, 341, 764, 13199, 24826, 11, 420, 307, 309, 1228, 264, 18407, 420, 2614, 309, 294, 8952, 30], "temperature": 0.0, "avg_logprob": -0.18115806579589844, "compression_ratio": 1.3661971830985915, "no_speech_prob": 4.425291535881115e-06}, {"id": 165, "seek": 122400, "start": 1231.0, "end": 1239.0, "text": " CPU. So the GPU is only used for models, basically.", "tokens": [13199, 13, 407, 264, 18407, 307, 787, 1143, 337, 5245, 11, 1936, 13], "temperature": 0.0, "avg_logprob": -0.18115806579589844, "compression_ratio": 1.3661971830985915, "no_speech_prob": 4.425291535881115e-06}, {"id": 166, "seek": 122400, "start": 1239.0, "end": 1242.0, "text": " Pretty much everything else is going to be done on CPU.", "tokens": [10693, 709, 1203, 1646, 307, 516, 281, 312, 1096, 322, 13199, 13], "temperature": 0.0, "avg_logprob": -0.18115806579589844, "compression_ratio": 1.3661971830985915, "no_speech_prob": 4.425291535881115e-06}, {"id": 167, "seek": 122400, "start": 1242.0, "end": 1246.0, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.18115806579589844, "compression_ratio": 1.3661971830985915, "no_speech_prob": 4.425291535881115e-06}, {"id": 168, "seek": 122400, "start": 1246.0, "end": 1249.0, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.18115806579589844, "compression_ratio": 1.3661971830985915, "no_speech_prob": 4.425291535881115e-06}, {"id": 169, "seek": 124900, "start": 1249.0, "end": 1261.0, "text": " So that's definitely worth the speed up. Now, I don't normally create a function to do one thing like this.", "tokens": [407, 300, 311, 2138, 3163, 264, 3073, 493, 13, 823, 11, 286, 500, 380, 5646, 1884, 257, 2445, 281, 360, 472, 551, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.08989669315850557, "compression_ratio": 1.5783132530120483, "no_speech_prob": 5.093600975669688e-06}, {"id": 170, "seek": 124900, "start": 1261.0, "end": 1267.0, "text": " What I would normally do instead is to use a lambda expression.", "tokens": [708, 286, 576, 5646, 360, 2602, 307, 281, 764, 257, 13607, 6114, 13], "temperature": 0.0, "avg_logprob": -0.08989669315850557, "compression_ratio": 1.5783132530120483, "no_speech_prob": 5.093600975669688e-06}, {"id": 171, "seek": 124900, "start": 1267.0, "end": 1273.0, "text": " And so to use a lambda expression, it's just basically it's a function you define in line.", "tokens": [400, 370, 281, 764, 257, 13607, 6114, 11, 309, 311, 445, 1936, 309, 311, 257, 2445, 291, 6964, 294, 1622, 13], "temperature": 0.0, "avg_logprob": -0.08989669315850557, "compression_ratio": 1.5783132530120483, "no_speech_prob": 5.093600975669688e-06}, {"id": 172, "seek": 127300, "start": 1273.0, "end": 1279.0, "text": " You just type lambda and you say the argument and you don't have to say return.", "tokens": [509, 445, 2010, 13607, 293, 291, 584, 264, 6770, 293, 291, 500, 380, 362, 281, 584, 2736, 13], "temperature": 0.0, "avg_logprob": -0.10355339421854391, "compression_ratio": 1.4914285714285713, "no_speech_prob": 1.8738554672381724e-06}, {"id": 173, "seek": 127300, "start": 1279.0, "end": 1285.0, "text": " And so then we can get rid of the definition.", "tokens": [400, 370, 550, 321, 393, 483, 3973, 295, 264, 7123, 13], "temperature": 0.0, "avg_logprob": -0.10355339421854391, "compression_ratio": 1.4914285714285713, "no_speech_prob": 1.8738554672381724e-06}, {"id": 174, "seek": 127300, "start": 1285.0, "end": 1290.0, "text": " And run.", "tokens": [400, 1190, 13], "temperature": 0.0, "avg_logprob": -0.10355339421854391, "compression_ratio": 1.4914285714285713, "no_speech_prob": 1.8738554672381724e-06}, {"id": 175, "seek": 127300, "start": 1290.0, "end": 1298.0, "text": " Oh, okay. So that's interesting. So we can't use a lambda with parallel. Oh, I guess I didn't know that. Now I think about it.", "tokens": [876, 11, 1392, 13, 407, 300, 311, 1880, 13, 407, 321, 393, 380, 764, 257, 13607, 365, 8952, 13, 876, 11, 286, 2041, 286, 994, 380, 458, 300, 13, 823, 286, 519, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.10355339421854391, "compression_ratio": 1.4914285714285713, "no_speech_prob": 1.8738554672381724e-06}, {"id": 176, "seek": 129800, "start": 1298.0, "end": 1303.0, "text": " All right. That's fine. We won't use it then.", "tokens": [1057, 558, 13, 663, 311, 2489, 13, 492, 1582, 380, 764, 309, 550, 13], "temperature": 0.0, "avg_logprob": -0.11926532859232888, "compression_ratio": 1.3395061728395061, "no_speech_prob": 5.9550493460847065e-06}, {"id": 177, "seek": 129800, "start": 1303.0, "end": 1310.0, "text": " Parallel processing on Python is notoriously crappy.", "tokens": [3457, 336, 338, 9007, 322, 15329, 307, 46772, 8994, 36531, 13], "temperature": 0.0, "avg_logprob": -0.11926532859232888, "compression_ratio": 1.3395061728395061, "no_speech_prob": 5.9550493460847065e-06}, {"id": 178, "seek": 129800, "start": 1310.0, "end": 1321.0, "text": " So, yeah, it's it's a it has a lot of limitations, including now I think about it not being able to use lambdas.", "tokens": [407, 11, 1338, 11, 309, 311, 309, 311, 257, 309, 575, 257, 688, 295, 15705, 11, 3009, 586, 286, 519, 466, 309, 406, 885, 1075, 281, 764, 10097, 27476, 13], "temperature": 0.0, "avg_logprob": -0.11926532859232888, "compression_ratio": 1.3395061728395061, "no_speech_prob": 5.9550493460847065e-06}, {"id": 179, "seek": 129800, "start": 1321.0, "end": 1326.0, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.11926532859232888, "compression_ratio": 1.3395061728395061, "no_speech_prob": 5.9550493460847065e-06}, {"id": 180, "seek": 132600, "start": 1326.0, "end": 1333.0, "text": " So then we created our data loaders.", "tokens": [407, 550, 321, 2942, 527, 1412, 3677, 433, 13], "temperature": 0.0, "avg_logprob": -0.23045939490908668, "compression_ratio": 1.2659574468085106, "no_speech_prob": 7.52734558773227e-06}, {"id": 181, "seek": 132600, "start": 1333.0, "end": 1338.0, "text": " Image data loaders dot from path.", "tokens": [29903, 1412, 3677, 433, 5893, 490, 3100, 13], "temperature": 0.0, "avg_logprob": -0.23045939490908668, "compression_ratio": 1.2659574468085106, "no_speech_prob": 7.52734558773227e-06}, {"id": 182, "seek": 132600, "start": 1338.0, "end": 1340.0, "text": " Folder.", "tokens": [24609, 260, 13], "temperature": 0.0, "avg_logprob": -0.23045939490908668, "compression_ratio": 1.2659574468085106, "no_speech_prob": 7.52734558773227e-06}, {"id": 183, "seek": 132600, "start": 1340.0, "end": 1346.0, "text": " And we passed in the training path.", "tokens": [400, 321, 4678, 294, 264, 3097, 3100, 13], "temperature": 0.0, "avg_logprob": -0.23045939490908668, "compression_ratio": 1.2659574468085106, "no_speech_prob": 7.52734558773227e-06}, {"id": 184, "seek": 132600, "start": 1346.0, "end": 1349.0, "text": " And.", "tokens": [400, 13], "temperature": 0.0, "avg_logprob": -0.23045939490908668, "compression_ratio": 1.2659574468085106, "no_speech_prob": 7.52734558773227e-06}, {"id": 185, "seek": 134900, "start": 1349.0, "end": 1356.0, "text": " Ballot percent.", "tokens": [10744, 310, 3043, 13], "temperature": 0.0, "avg_logprob": -0.2593471278315005, "compression_ratio": 0.9605263157894737, "no_speech_prob": 9.515782949165441e-06}, {"id": 186, "seek": 134900, "start": 1356.0, "end": 1372.0, "text": " And I think we want some resize transform as well. Right.", "tokens": [400, 286, 519, 321, 528, 512, 50069, 4088, 382, 731, 13, 1779, 13], "temperature": 0.0, "avg_logprob": -0.2593471278315005, "compression_ratio": 0.9605263157894737, "no_speech_prob": 9.515782949165441e-06}, {"id": 187, "seek": 137200, "start": 1372.0, "end": 1384.0, "text": " Cool. So. And so then we created a model.", "tokens": [8561, 13, 407, 13, 400, 370, 550, 321, 2942, 257, 2316, 13], "temperature": 0.0, "avg_logprob": -0.19344784816106161, "compression_ratio": 1.1578947368421053, "no_speech_prob": 8.714259820408188e-07}, {"id": 188, "seek": 137200, "start": 1384.0, "end": 1389.0, "text": " So last time we used ResNet 34.", "tokens": [407, 1036, 565, 321, 1143, 5015, 31890, 12790, 13], "temperature": 0.0, "avg_logprob": -0.19344784816106161, "compression_ratio": 1.1578947368421053, "no_speech_prob": 8.714259820408188e-07}, {"id": 189, "seek": 137200, "start": 1389.0, "end": 1392.0, "text": " But what I'd be inclined to do.", "tokens": [583, 437, 286, 1116, 312, 28173, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.19344784816106161, "compression_ratio": 1.1578947368421053, "no_speech_prob": 8.714259820408188e-07}, {"id": 190, "seek": 137200, "start": 1392.0, "end": 1394.0, "text": " Is to head over to Kaggle.", "tokens": [1119, 281, 1378, 670, 281, 48751, 22631, 13], "temperature": 0.0, "avg_logprob": -0.19344784816106161, "compression_ratio": 1.1578947368421053, "no_speech_prob": 8.714259820408188e-07}, {"id": 191, "seek": 139400, "start": 1394.0, "end": 1402.0, "text": " And look at the which image models are best.", "tokens": [400, 574, 412, 264, 597, 3256, 5245, 366, 1151, 13], "temperature": 0.0, "avg_logprob": -0.1163015123141014, "compression_ratio": 1.3918918918918919, "no_speech_prob": 3.3404216992494185e-06}, {"id": 192, "seek": 139400, "start": 1402.0, "end": 1411.0, "text": " And see if there's something we might want to use. It's better than ResNet 34.", "tokens": [400, 536, 498, 456, 311, 746, 321, 1062, 528, 281, 764, 13, 467, 311, 1101, 813, 5015, 31890, 12790, 13], "temperature": 0.0, "avg_logprob": -0.1163015123141014, "compression_ratio": 1.3918918918918919, "no_speech_prob": 3.3404216992494185e-06}, {"id": 193, "seek": 139400, "start": 1411.0, "end": 1415.0, "text": " So this is showing speed in a log scale.", "tokens": [407, 341, 307, 4099, 3073, 294, 257, 3565, 4373, 13], "temperature": 0.0, "avg_logprob": -0.1163015123141014, "compression_ratio": 1.3918918918918919, "no_speech_prob": 3.3404216992494185e-06}, {"id": 194, "seek": 139400, "start": 1415.0, "end": 1418.0, "text": " And this is showing accuracy on ImageNet.", "tokens": [400, 341, 307, 4099, 14170, 322, 29903, 31890, 13], "temperature": 0.0, "avg_logprob": -0.1163015123141014, "compression_ratio": 1.3918918918918919, "no_speech_prob": 3.3404216992494185e-06}, {"id": 195, "seek": 141800, "start": 1418.0, "end": 1425.0, "text": " And the different colors of various different kind of families.", "tokens": [400, 264, 819, 4577, 295, 3683, 819, 733, 295, 4466, 13], "temperature": 0.0, "avg_logprob": -0.08329141707647414, "compression_ratio": 1.319672131147541, "no_speech_prob": 3.9668575482210144e-06}, {"id": 196, "seek": 141800, "start": 1425.0, "end": 1432.0, "text": " ResNet is this family here.", "tokens": [5015, 31890, 307, 341, 1605, 510, 13], "temperature": 0.0, "avg_logprob": -0.08329141707647414, "compression_ratio": 1.319672131147541, "no_speech_prob": 3.9668575482210144e-06}, {"id": 197, "seek": 141800, "start": 1432.0, "end": 1446.0, "text": " And things like ResNet 34 are not particularly great, as you can see.", "tokens": [400, 721, 411, 5015, 31890, 12790, 366, 406, 4098, 869, 11, 382, 291, 393, 536, 13], "temperature": 0.0, "avg_logprob": -0.08329141707647414, "compression_ratio": 1.319672131147541, "no_speech_prob": 3.9668575482210144e-06}, {"id": 198, "seek": 144600, "start": 1446.0, "end": 1456.0, "text": " So let's try using conv next base in 22 blah blah blah instead.", "tokens": [407, 718, 311, 853, 1228, 3754, 958, 3096, 294, 5853, 12288, 12288, 12288, 2602, 13], "temperature": 0.0, "avg_logprob": -0.2303658894130162, "compression_ratio": 1.3696969696969696, "no_speech_prob": 9.080108611669857e-06}, {"id": 199, "seek": 144600, "start": 1456.0, "end": 1460.0, "text": " Okay. So Vision Learner we first passed in the data loaders.", "tokens": [1033, 13, 407, 25170, 17216, 260, 321, 700, 4678, 294, 264, 1412, 3677, 433, 13], "temperature": 0.0, "avg_logprob": -0.2303658894130162, "compression_ratio": 1.3696969696969696, "no_speech_prob": 9.080108611669857e-06}, {"id": 200, "seek": 144600, "start": 1460.0, "end": 1470.0, "text": " And so these image models here are from a library called Tim.", "tokens": [400, 370, 613, 3256, 5245, 510, 366, 490, 257, 6405, 1219, 7172, 13], "temperature": 0.0, "avg_logprob": -0.2303658894130162, "compression_ratio": 1.3696969696969696, "no_speech_prob": 9.080108611669857e-06}, {"id": 201, "seek": 144600, "start": 1470.0, "end": 1475.0, "text": " Which to use it, you need it installed.", "tokens": [3013, 281, 764, 309, 11, 291, 643, 309, 8899, 13], "temperature": 0.0, "avg_logprob": -0.2303658894130162, "compression_ratio": 1.3696969696969696, "no_speech_prob": 9.080108611669857e-06}, {"id": 202, "seek": 147500, "start": 1475.0, "end": 1480.0, "text": " Which I probably have installed, but just to check.", "tokens": [3013, 286, 1391, 362, 8899, 11, 457, 445, 281, 1520, 13], "temperature": 0.0, "avg_logprob": -0.15970569610595703, "compression_ratio": 1.5114942528735633, "no_speech_prob": 1.3631118235934991e-05}, {"id": 203, "seek": 147500, "start": 1480.0, "end": 1483.0, "text": " Yep, that's already installed.", "tokens": [7010, 11, 300, 311, 1217, 8899, 13], "temperature": 0.0, "avg_logprob": -0.15970569610595703, "compression_ratio": 1.5114942528735633, "no_speech_prob": 1.3631118235934991e-05}, {"id": 204, "seek": 147500, "start": 1483.0, "end": 1491.0, "text": " And you can check out things on Tim such as, so if you import it.", "tokens": [400, 291, 393, 1520, 484, 721, 322, 7172, 1270, 382, 11, 370, 498, 291, 974, 309, 13], "temperature": 0.0, "avg_logprob": -0.15970569610595703, "compression_ratio": 1.5114942528735633, "no_speech_prob": 1.3631118235934991e-05}, {"id": 205, "seek": 147500, "start": 1491.0, "end": 1496.0, "text": " Then you can say Tim dot list models.", "tokens": [1396, 291, 393, 584, 7172, 5893, 1329, 5245, 13], "temperature": 0.0, "avg_logprob": -0.15970569610595703, "compression_ratio": 1.5114942528735633, "no_speech_prob": 1.3631118235934991e-05}, {"id": 206, "seek": 147500, "start": 1496.0, "end": 1503.0, "text": " And you can pass in basically a glob. So I want to look at conv next models.", "tokens": [400, 291, 393, 1320, 294, 1936, 257, 16125, 13, 407, 286, 528, 281, 574, 412, 3754, 958, 5245, 13], "temperature": 0.0, "avg_logprob": -0.15970569610595703, "compression_ratio": 1.5114942528735633, "no_speech_prob": 1.3631118235934991e-05}, {"id": 207, "seek": 150300, "start": 1503.0, "end": 1509.0, "text": " See what options there are. Mainly because I just want to copy and paste.", "tokens": [3008, 437, 3956, 456, 366, 13, 47468, 570, 286, 445, 528, 281, 5055, 293, 9163, 13], "temperature": 0.0, "avg_logprob": -0.1749518632888794, "compression_ratio": 1.447674418604651, "no_speech_prob": 1.5206557691271883e-05}, {"id": 208, "seek": 150300, "start": 1509.0, "end": 1514.0, "text": " And so, okay, so there's base, there's small as well. Small.", "tokens": [400, 370, 11, 1392, 11, 370, 456, 311, 3096, 11, 456, 311, 1359, 382, 731, 13, 15287, 13], "temperature": 0.0, "avg_logprob": -0.1749518632888794, "compression_ratio": 1.447674418604651, "no_speech_prob": 1.5206557691271883e-05}, {"id": 209, "seek": 150300, "start": 1514.0, "end": 1519.0, "text": " Now why is small?", "tokens": [823, 983, 307, 1359, 30], "temperature": 0.0, "avg_logprob": -0.1749518632888794, "compression_ratio": 1.447674418604651, "no_speech_prob": 1.5206557691271883e-05}, {"id": 210, "seek": 150300, "start": 1519.0, "end": 1523.0, "text": " Conv next.", "tokens": [2656, 85, 958, 13], "temperature": 0.0, "avg_logprob": -0.1749518632888794, "compression_ratio": 1.447674418604651, "no_speech_prob": 1.5206557691271883e-05}, {"id": 211, "seek": 150300, "start": 1523.0, "end": 1526.0, "text": " If you double click, you'll get this.", "tokens": [759, 291, 3834, 2052, 11, 291, 603, 483, 341, 13], "temperature": 0.0, "avg_logprob": -0.1749518632888794, "compression_ratio": 1.447674418604651, "no_speech_prob": 1.5206557691271883e-05}, {"id": 212, "seek": 150300, "start": 1526.0, "end": 1532.0, "text": " There's base, large, extra large. That's weird.", "tokens": [821, 311, 3096, 11, 2416, 11, 2857, 2416, 13, 663, 311, 3657, 13], "temperature": 0.0, "avg_logprob": -0.1749518632888794, "compression_ratio": 1.447674418604651, "no_speech_prob": 1.5206557691271883e-05}, {"id": 213, "seek": 153200, "start": 1532.0, "end": 1541.0, "text": " For some reason, the small one's not appearing.", "tokens": [1171, 512, 1778, 11, 264, 1359, 472, 311, 406, 19870, 13], "temperature": 0.0, "avg_logprob": -0.26669285514137964, "compression_ratio": 1.4350649350649352, "no_speech_prob": 3.943582487409003e-05}, {"id": 214, "seek": 153200, "start": 1541.0, "end": 1543.0, "text": " And there's also a tiny.", "tokens": [400, 456, 311, 611, 257, 5870, 13], "temperature": 0.0, "avg_logprob": -0.26669285514137964, "compression_ratio": 1.4350649350649352, "no_speech_prob": 3.943582487409003e-05}, {"id": 215, "seek": 153200, "start": 1543.0, "end": 1554.0, "text": " Hi, Jeremy. Yeah. I think they added the small and the tiny one in the last version of Tim that is not in the Pitten style version.", "tokens": [2421, 11, 17809, 13, 865, 13, 286, 519, 436, 3869, 264, 1359, 293, 264, 5870, 472, 294, 264, 1036, 3037, 295, 7172, 300, 307, 406, 294, 264, 430, 2987, 3758, 3037, 13], "temperature": 0.0, "avg_logprob": -0.26669285514137964, "compression_ratio": 1.4350649350649352, "no_speech_prob": 3.943582487409003e-05}, {"id": 216, "seek": 153200, "start": 1554.0, "end": 1557.0, "text": " Right. Okay, so.", "tokens": [1779, 13, 1033, 11, 370, 13], "temperature": 0.0, "avg_logprob": -0.26669285514137964, "compression_ratio": 1.4350649350649352, "no_speech_prob": 3.943582487409003e-05}, {"id": 217, "seek": 155700, "start": 1557.0, "end": 1564.0, "text": " Oh, yeah, we need to install the dev version of it.", "tokens": [876, 11, 1338, 11, 321, 643, 281, 3625, 264, 1905, 3037, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.20129630144904642, "compression_ratio": 1.1864406779661016, "no_speech_prob": 1.1477942280180287e-05}, {"id": 218, "seek": 155700, "start": 1564.0, "end": 1568.0, "text": " Correct. Yes, thank you.", "tokens": [12753, 13, 1079, 11, 1309, 291, 13], "temperature": 0.0, "avg_logprob": -0.20129630144904642, "compression_ratio": 1.1864406779661016, "no_speech_prob": 1.1477942280180287e-05}, {"id": 219, "seek": 155700, "start": 1568.0, "end": 1574.0, "text": " So Ross, who creates Tim.", "tokens": [407, 16140, 11, 567, 7829, 7172, 13], "temperature": 0.0, "avg_logprob": -0.20129630144904642, "compression_ratio": 1.1864406779661016, "no_speech_prob": 1.1477942280180287e-05}, {"id": 220, "seek": 155700, "start": 1574.0, "end": 1579.0, "text": " Created a pre-released version, 0.62.", "tokens": [11972, 292, 257, 659, 12, 265, 41087, 3037, 11, 1958, 13, 28052, 13], "temperature": 0.0, "avg_logprob": -0.20129630144904642, "compression_ratio": 1.1864406779661016, "no_speech_prob": 1.1477942280180287e-05}, {"id": 221, "seek": 157900, "start": 1579.0, "end": 1589.0, "text": " And so to install that, we would need to call minus U for upgrade.", "tokens": [400, 370, 281, 3625, 300, 11, 321, 576, 643, 281, 818, 3175, 624, 337, 11484, 13], "temperature": 0.0, "avg_logprob": -0.1657523036003113, "compression_ratio": 0.9565217391304348, "no_speech_prob": 2.429349478916265e-05}, {"id": 222, "seek": 158900, "start": 1589.0, "end": 1610.0, "text": " Oh, just one moment, please.", "tokens": [876, 11, 445, 472, 1623, 11, 1767, 13], "temperature": 0.0, "avg_logprob": -0.11525641168866839, "compression_ratio": 0.9315068493150684, "no_speech_prob": 2.177936039515771e-05}, {"id": 223, "seek": 158900, "start": 1610.0, "end": 1613.0, "text": " My daughter's having computer problems.", "tokens": [1222, 4653, 311, 1419, 3820, 2740, 13], "temperature": 0.0, "avg_logprob": -0.11525641168866839, "compression_ratio": 0.9315068493150684, "no_speech_prob": 2.177936039515771e-05}, {"id": 224, "seek": 161300, "start": 1613.0, "end": 1621.0, "text": " All right, Tim, greater than.", "tokens": [1057, 558, 11, 7172, 11, 5044, 813, 13], "temperature": 0.0, "avg_logprob": -0.24185543060302733, "compression_ratio": 1.0588235294117647, "no_speech_prob": 1.892407817649655e-05}, {"id": 225, "seek": 161300, "start": 1621.0, "end": 1624.0, "text": " 0.6.", "tokens": [1958, 13, 21, 13], "temperature": 0.0, "avg_logprob": -0.24185543060302733, "compression_ratio": 1.0588235294117647, "no_speech_prob": 1.892407817649655e-05}, {"id": 226, "seek": 161300, "start": 1624.0, "end": 1628.0, "text": " 0.6.2.", "tokens": [1958, 13, 21, 13, 17, 13], "temperature": 0.0, "avg_logprob": -0.24185543060302733, "compression_ratio": 1.0588235294117647, "no_speech_prob": 1.892407817649655e-05}, {"id": 227, "seek": 161300, "start": 1628.0, "end": 1638.0, "text": " I think something like this. I'm not quite sure.", "tokens": [286, 519, 746, 411, 341, 13, 286, 478, 406, 1596, 988, 13], "temperature": 0.0, "avg_logprob": -0.24185543060302733, "compression_ratio": 1.0588235294117647, "no_speech_prob": 1.892407817649655e-05}, {"id": 228, "seek": 163800, "start": 1638.0, "end": 1645.0, "text": " Oh, it says I've already got 0.6.2 dev installed.", "tokens": [876, 11, 309, 1619, 286, 600, 1217, 658, 1958, 13, 21, 13, 17, 1905, 8899, 13], "temperature": 0.0, "avg_logprob": -0.09823635371044429, "compression_ratio": 1.5255813953488373, "no_speech_prob": 1.2028968740196433e-05}, {"id": 229, "seek": 163800, "start": 1645.0, "end": 1652.0, "text": " Oh, I see. So I've got it on my machine, but it wasn't on the Kaggle machine because I didn't install that version on Kaggle.", "tokens": [876, 11, 286, 536, 13, 407, 286, 600, 658, 309, 322, 452, 3479, 11, 457, 309, 2067, 380, 322, 264, 48751, 22631, 3479, 570, 286, 994, 380, 3625, 300, 3037, 322, 48751, 22631, 13], "temperature": 0.0, "avg_logprob": -0.09823635371044429, "compression_ratio": 1.5255813953488373, "no_speech_prob": 1.2028968740196433e-05}, {"id": 230, "seek": 163800, "start": 1652.0, "end": 1655.0, "text": " Okay. Cool.", "tokens": [1033, 13, 8561, 13], "temperature": 0.0, "avg_logprob": -0.09823635371044429, "compression_ratio": 1.5255813953488373, "no_speech_prob": 1.2028968740196433e-05}, {"id": 231, "seek": 163800, "start": 1655.0, "end": 1660.0, "text": " Well, we might as well fix it on Kaggle, just so you see how these things work as well.", "tokens": [1042, 11, 321, 1062, 382, 731, 3191, 309, 322, 48751, 22631, 11, 445, 370, 291, 536, 577, 613, 721, 589, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.09823635371044429, "compression_ratio": 1.5255813953488373, "no_speech_prob": 1.2028968740196433e-05}, {"id": 232, "seek": 163800, "start": 1660.0, "end": 1665.0, "text": " Because there's actually something quite nifty here.", "tokens": [1436, 456, 311, 767, 746, 1596, 297, 37177, 510, 13], "temperature": 0.0, "avg_logprob": -0.09823635371044429, "compression_ratio": 1.5255813953488373, "no_speech_prob": 1.2028968740196433e-05}, {"id": 233, "seek": 166500, "start": 1665.0, "end": 1669.0, "text": " So we'll click edit on Kaggle.", "tokens": [407, 321, 603, 2052, 8129, 322, 48751, 22631, 13], "temperature": 0.0, "avg_logprob": -0.25442785375258503, "compression_ratio": 1.0123456790123457, "no_speech_prob": 3.119890243397094e-05}, {"id": 234, "seek": 166500, "start": 1669.0, "end": 1673.0, "text": " And we'll", "tokens": [400, 321, 603], "temperature": 0.0, "avg_logprob": -0.25442785375258503, "compression_ratio": 1.0123456790123457, "no_speech_prob": 3.119890243397094e-05}, {"id": 235, "seek": 166500, "start": 1673.0, "end": 1682.0, "text": " get it here somehow, but it's okay.", "tokens": [483, 309, 510, 6063, 11, 457, 309, 311, 1392, 13], "temperature": 0.0, "avg_logprob": -0.25442785375258503, "compression_ratio": 1.0123456790123457, "no_speech_prob": 3.119890243397094e-05}, {"id": 236, "seek": 166500, "start": 1682.0, "end": 1690.0, "text": " Copy.", "tokens": [25653, 13], "temperature": 0.0, "avg_logprob": -0.25442785375258503, "compression_ratio": 1.0123456790123457, "no_speech_prob": 3.119890243397094e-05}, {"id": 237, "seek": 169000, "start": 1690.0, "end": 1696.0, "text": " Oh, all right. It's he doesn't have it in his data, I guess.", "tokens": [876, 11, 439, 558, 13, 467, 311, 415, 1177, 380, 362, 309, 294, 702, 1412, 11, 286, 2041, 13], "temperature": 0.0, "avg_logprob": -0.09371652603149414, "compression_ratio": 1.4821428571428572, "no_speech_prob": 1.0451012713019736e-05}, {"id": 238, "seek": 169000, "start": 1696.0, "end": 1699.0, "text": " All right. So not much we can do about that.", "tokens": [1057, 558, 13, 407, 406, 709, 321, 393, 360, 466, 300, 13], "temperature": 0.0, "avg_logprob": -0.09371652603149414, "compression_ratio": 1.4821428571428572, "no_speech_prob": 1.0451012713019736e-05}, {"id": 239, "seek": 169000, "start": 1699.0, "end": 1704.0, "text": " All right. Well, I think we should just go ahead and try one of the smaller ones.", "tokens": [1057, 558, 13, 1042, 11, 286, 519, 321, 820, 445, 352, 2286, 293, 853, 472, 295, 264, 4356, 2306, 13], "temperature": 0.0, "avg_logprob": -0.09371652603149414, "compression_ratio": 1.4821428571428572, "no_speech_prob": 1.0451012713019736e-05}, {"id": 240, "seek": 169000, "start": 1704.0, "end": 1711.0, "text": " So let's try small.", "tokens": [407, 718, 311, 853, 1359, 13], "temperature": 0.0, "avg_logprob": -0.09371652603149414, "compression_ratio": 1.4821428571428572, "no_speech_prob": 1.0451012713019736e-05}, {"id": 241, "seek": 169000, "start": 1711.0, "end": 1716.0, "text": " That has to be a string when he used Tim.", "tokens": [663, 575, 281, 312, 257, 6798, 562, 415, 1143, 7172, 13], "temperature": 0.0, "avg_logprob": -0.09371652603149414, "compression_ratio": 1.4821428571428572, "no_speech_prob": 1.0451012713019736e-05}, {"id": 242, "seek": 171600, "start": 1716.0, "end": 1725.0, "text": " See what happens. So when you use a pre trained model, it needs the weights.", "tokens": [3008, 437, 2314, 13, 407, 562, 291, 764, 257, 659, 8895, 2316, 11, 309, 2203, 264, 17443, 13], "temperature": 0.0, "avg_logprob": -0.08894927437241013, "compression_ratio": 1.7011494252873562, "no_speech_prob": 1.0676802730813506e-06}, {"id": 243, "seek": 171600, "start": 1725.0, "end": 1734.0, "text": " And so the first time you use it, it downloads the weights.", "tokens": [400, 370, 264, 700, 565, 291, 764, 309, 11, 309, 36553, 264, 17443, 13], "temperature": 0.0, "avg_logprob": -0.08894927437241013, "compression_ratio": 1.7011494252873562, "no_speech_prob": 1.0676802730813506e-06}, {"id": 244, "seek": 171600, "start": 1734.0, "end": 1742.0, "text": " Depending on how much space, if you're using paper space, depending on how much space you have and how long this takes on paper space, you may want to consider", "tokens": [22539, 322, 577, 709, 1901, 11, 498, 291, 434, 1228, 3035, 1901, 11, 5413, 322, 577, 709, 1901, 291, 362, 293, 577, 938, 341, 2516, 322, 3035, 1901, 11, 291, 815, 528, 281, 1949], "temperature": 0.0, "avg_logprob": -0.08894927437241013, "compression_ratio": 1.7011494252873562, "no_speech_prob": 1.0676802730813506e-06}, {"id": 245, "seek": 174200, "start": 1742.0, "end": 1749.0, "text": " Sim linking your home directories dot cash slash torch", "tokens": [3998, 25775, 428, 1280, 5391, 530, 5893, 6388, 17330, 27822], "temperature": 0.0, "avg_logprob": -0.14318538078894982, "compression_ratio": 1.4375, "no_speech_prob": 4.4950147639610805e-06}, {"id": 246, "seek": 174200, "start": 1749.0, "end": 1752.0, "text": " into slash George.", "tokens": [666, 17330, 7136, 13], "temperature": 0.0, "avg_logprob": -0.14318538078894982, "compression_ratio": 1.4375, "no_speech_prob": 4.4950147639610805e-06}, {"id": 247, "seek": 174200, "start": 1752.0, "end": 1760.0, "text": " And that way you won't have to download these. Not that it seems to take too long. I don't know if you care or not.", "tokens": [400, 300, 636, 291, 1582, 380, 362, 281, 5484, 613, 13, 1726, 300, 309, 2544, 281, 747, 886, 938, 13, 286, 500, 380, 458, 498, 291, 1127, 420, 406, 13], "temperature": 0.0, "avg_logprob": -0.14318538078894982, "compression_ratio": 1.4375, "no_speech_prob": 4.4950147639610805e-06}, {"id": 248, "seek": 174200, "start": 1760.0, "end": 1764.0, "text": " You know, one thing we might want to do.", "tokens": [509, 458, 11, 472, 551, 321, 1062, 528, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.14318538078894982, "compression_ratio": 1.4375, "no_speech_prob": 4.4950147639610805e-06}, {"id": 249, "seek": 176400, "start": 1764.0, "end": 1778.0, "text": " Well, let's let's just try to find you in it, shall we?", "tokens": [1042, 11, 718, 311, 718, 311, 445, 853, 281, 915, 291, 294, 309, 11, 4393, 321, 30], "temperature": 0.0, "avg_logprob": -0.21301740407943726, "compression_ratio": 1.0595238095238095, "no_speech_prob": 1.6440912077086978e-05}, {"id": 250, "seek": 176400, "start": 1778.0, "end": 1787.0, "text": " That's good. Seems to be working.", "tokens": [663, 311, 665, 13, 22524, 281, 312, 1364, 13], "temperature": 0.0, "avg_logprob": -0.21301740407943726, "compression_ratio": 1.0595238095238095, "no_speech_prob": 1.6440912077086978e-05}, {"id": 251, "seek": 178700, "start": 1787.0, "end": 1800.0, "text": " Let me know if anybody's got any questions or thoughts along the way.", "tokens": [961, 385, 458, 498, 4472, 311, 658, 604, 1651, 420, 4598, 2051, 264, 636, 13], "temperature": 0.0, "avg_logprob": -0.10670765390935934, "compression_ratio": 1.3868613138686132, "no_speech_prob": 3.7265328955982113e-06}, {"id": 252, "seek": 178700, "start": 1800.0, "end": 1814.0, "text": " So when it fine tunes. Oh, OK. So the other thing we want to do is tell it that we want to keep track of the error rate.", "tokens": [407, 562, 309, 2489, 38498, 13, 876, 11, 2264, 13, 407, 264, 661, 551, 321, 528, 281, 360, 307, 980, 309, 300, 321, 528, 281, 1066, 2837, 295, 264, 6713, 3314, 13], "temperature": 0.0, "avg_logprob": -0.10670765390935934, "compression_ratio": 1.3868613138686132, "no_speech_prob": 3.7265328955982113e-06}, {"id": 253, "seek": 181400, "start": 1814.0, "end": 1818.0, "text": " OK.", "tokens": [2264, 13], "temperature": 0.0, "avg_logprob": -0.2806031545003255, "compression_ratio": 1.0123456790123457, "no_speech_prob": 3.6113040096097393e-06}, {"id": 254, "seek": 181400, "start": 1818.0, "end": 1824.0, "text": " That's going to help. So yeah, so when we fine tune.", "tokens": [663, 311, 516, 281, 854, 13, 407, 1338, 11, 370, 562, 321, 2489, 10864, 13], "temperature": 0.0, "avg_logprob": -0.2806031545003255, "compression_ratio": 1.0123456790123457, "no_speech_prob": 3.6113040096097393e-06}, {"id": 255, "seek": 181400, "start": 1824.0, "end": 1829.0, "text": " Just create another copy.", "tokens": [1449, 1884, 1071, 5055, 13], "temperature": 0.0, "avg_logprob": -0.2806031545003255, "compression_ratio": 1.0123456790123457, "no_speech_prob": 3.6113040096097393e-06}, {"id": 256, "seek": 182900, "start": 1829.0, "end": 1857.0, "text": " OK.", "tokens": [2264, 13], "temperature": 0.0, "avg_logprob": -0.11425372958183289, "compression_ratio": 0.2727272727272727, "no_speech_prob": 2.138848685717676e-05}, {"id": 257, "seek": 185700, "start": 1857.0, "end": 1861.0, "text": " So we can look at the source code to see exactly what it's doing.", "tokens": [407, 321, 393, 574, 412, 264, 4009, 3089, 281, 536, 2293, 437, 309, 311, 884, 13], "temperature": 0.0, "avg_logprob": -0.12225164685930524, "compression_ratio": 1.5229885057471264, "no_speech_prob": 1.2482272495617508e-06}, {"id": 258, "seek": 185700, "start": 1861.0, "end": 1865.0, "text": " When we call fine tune.", "tokens": [1133, 321, 818, 2489, 10864, 13], "temperature": 0.0, "avg_logprob": -0.12225164685930524, "compression_ratio": 1.5229885057471264, "no_speech_prob": 1.2482272495617508e-06}, {"id": 259, "seek": 185700, "start": 1865.0, "end": 1868.0, "text": " So what it's actually doing.", "tokens": [407, 437, 309, 311, 767, 884, 13], "temperature": 0.0, "avg_logprob": -0.12225164685930524, "compression_ratio": 1.5229885057471264, "no_speech_prob": 1.2482272495617508e-06}, {"id": 260, "seek": 185700, "start": 1868.0, "end": 1873.0, "text": " Is it's calling freeze. What that does is it", "tokens": [1119, 309, 311, 5141, 15959, 13, 708, 300, 775, 307, 309], "temperature": 0.0, "avg_logprob": -0.12225164685930524, "compression_ratio": 1.5229885057471264, "no_speech_prob": 1.2482272495617508e-06}, {"id": 261, "seek": 185700, "start": 1873.0, "end": 1878.0, "text": " says all of the weights, except for the very last layer.", "tokens": [1619, 439, 295, 264, 17443, 11, 3993, 337, 264, 588, 1036, 4583, 13], "temperature": 0.0, "avg_logprob": -0.12225164685930524, "compression_ratio": 1.5229885057471264, "no_speech_prob": 1.2482272495617508e-06}, {"id": 262, "seek": 185700, "start": 1878.0, "end": 1881.0, "text": " You're the optimizers not allowed to change.", "tokens": [509, 434, 264, 5028, 22525, 406, 4350, 281, 1319, 13], "temperature": 0.0, "avg_logprob": -0.12225164685930524, "compression_ratio": 1.5229885057471264, "no_speech_prob": 1.2482272495617508e-06}, {"id": 263, "seek": 188100, "start": 1881.0, "end": 1890.0, "text": " So if you think back to that Sila and Fergus paper we saw with the different layers and the different like, you know, the later layers were more and more specific.", "tokens": [407, 498, 291, 519, 646, 281, 300, 6943, 64, 293, 36790, 3035, 321, 1866, 365, 264, 819, 7914, 293, 264, 819, 411, 11, 291, 458, 11, 264, 1780, 7914, 645, 544, 293, 544, 2685, 13], "temperature": 0.0, "avg_logprob": -0.15050605580776552, "compression_ratio": 1.6772486772486772, "no_speech_prob": 3.393045972188702e-06}, {"id": 264, "seek": 188100, "start": 1890.0, "end": 1899.0, "text": " So initially, we just want to fit fit the last layer. So it calls fit on the last layer only.", "tokens": [407, 9105, 11, 321, 445, 528, 281, 3318, 3318, 264, 1036, 4583, 13, 407, 309, 5498, 3318, 322, 264, 1036, 4583, 787, 13], "temperature": 0.0, "avg_logprob": -0.15050605580776552, "compression_ratio": 1.6772486772486772, "no_speech_prob": 3.393045972188702e-06}, {"id": 265, "seek": 188100, "start": 1899.0, "end": 1903.0, "text": " And then it decreases the learning rate and then unfreezes.", "tokens": [400, 550, 309, 24108, 264, 2539, 3314, 293, 550, 3971, 701, 12214, 13], "temperature": 0.0, "avg_logprob": -0.15050605580776552, "compression_ratio": 1.6772486772486772, "no_speech_prob": 3.393045972188702e-06}, {"id": 266, "seek": 190300, "start": 1903.0, "end": 1911.0, "text": " So then it says, OK, you know, you can train the whole thing and then it trains the whole model for however many epochs we asked for.", "tokens": [407, 550, 309, 1619, 11, 2264, 11, 291, 458, 11, 291, 393, 3847, 264, 1379, 551, 293, 550, 309, 16329, 264, 1379, 2316, 337, 4461, 867, 30992, 28346, 321, 2351, 337, 13], "temperature": 0.0, "avg_logprob": -0.10596183776855468, "compression_ratio": 1.640495867768595, "no_speech_prob": 1.7330133914583712e-06}, {"id": 267, "seek": 190300, "start": 1911.0, "end": 1919.0, "text": " So we can see. So generally speaking, fast AI methods, I mean, pretty much any methods I write tend to be very small.", "tokens": [407, 321, 393, 536, 13, 407, 5101, 4124, 11, 2370, 7318, 7150, 11, 286, 914, 11, 1238, 709, 604, 7150, 286, 2464, 3928, 281, 312, 588, 1359, 13], "temperature": 0.0, "avg_logprob": -0.10596183776855468, "compression_ratio": 1.640495867768595, "no_speech_prob": 1.7330133914583712e-06}, {"id": 268, "seek": 190300, "start": 1919.0, "end": 1925.0, "text": " So they're designed to be reasonably easy to read the source code and see what's going on.", "tokens": [407, 436, 434, 4761, 281, 312, 23551, 1858, 281, 1401, 264, 4009, 3089, 293, 536, 437, 311, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.10596183776855468, "compression_ratio": 1.640495867768595, "no_speech_prob": 1.7330133914583712e-06}, {"id": 269, "seek": 190300, "start": 1925.0, "end": 1931.0, "text": " At least if you're reasonably comfortable with Python.", "tokens": [1711, 1935, 498, 291, 434, 23551, 4619, 365, 15329, 13], "temperature": 0.0, "avg_logprob": -0.10596183776855468, "compression_ratio": 1.640495867768595, "no_speech_prob": 1.7330133914583712e-06}, {"id": 270, "seek": 193100, "start": 1931.0, "end": 1935.0, "text": " Oh, and I just sort of something else we should do.", "tokens": [876, 11, 293, 286, 445, 1333, 295, 746, 1646, 321, 820, 360, 13], "temperature": 0.0, "avg_logprob": -0.08241505236239047, "compression_ratio": 1.46875, "no_speech_prob": 1.834177783166524e-05}, {"id": 271, "seek": 193100, "start": 1935.0, "end": 1944.0, "text": " Which is if you are using a GPU released in the last, I don't know, four years or so.", "tokens": [3013, 307, 498, 291, 366, 1228, 257, 18407, 4736, 294, 264, 1036, 11, 286, 500, 380, 458, 11, 1451, 924, 420, 370, 13], "temperature": 0.0, "avg_logprob": -0.08241505236239047, "compression_ratio": 1.46875, "no_speech_prob": 1.834177783166524e-05}, {"id": 272, "seek": 193100, "start": 1944.0, "end": 1950.0, "text": " It's very likely that it'll be much faster using", "tokens": [467, 311, 588, 3700, 300, 309, 603, 312, 709, 4663, 1228], "temperature": 0.0, "avg_logprob": -0.08241505236239047, "compression_ratio": 1.46875, "no_speech_prob": 1.834177783166524e-05}, {"id": 273, "seek": 193100, "start": 1950.0, "end": 1957.0, "text": " what's called half precision floating point, which is basically like less less precise numbers.", "tokens": [437, 311, 1219, 1922, 18356, 12607, 935, 11, 597, 307, 1936, 411, 1570, 1570, 13600, 3547, 13], "temperature": 0.0, "avg_logprob": -0.08241505236239047, "compression_ratio": 1.46875, "no_speech_prob": 1.834177783166524e-05}, {"id": 274, "seek": 195700, "start": 1957.0, "end": 1962.0, "text": " It'll be way, way, way faster.", "tokens": [467, 603, 312, 636, 11, 636, 11, 636, 4663, 13], "temperature": 0.0, "avg_logprob": -0.07376530495556918, "compression_ratio": 1.4593301435406698, "no_speech_prob": 5.682185019395547e-06}, {"id": 275, "seek": 195700, "start": 1962.0, "end": 1967.0, "text": " Most of the time on Colab and Kaggle, you're not going to get", "tokens": [4534, 295, 264, 565, 322, 4004, 455, 293, 48751, 22631, 11, 291, 434, 406, 516, 281, 483], "temperature": 0.0, "avg_logprob": -0.07376530495556918, "compression_ratio": 1.4593301435406698, "no_speech_prob": 5.682185019395547e-06}, {"id": 276, "seek": 195700, "start": 1967.0, "end": 1976.0, "text": " one of those more up to date GPUs. But having said that, there's really never any harm in using half precision floating point.", "tokens": [472, 295, 729, 544, 493, 281, 4002, 18407, 82, 13, 583, 1419, 848, 300, 11, 456, 311, 534, 1128, 604, 6491, 294, 1228, 1922, 18356, 12607, 935, 13], "temperature": 0.0, "avg_logprob": -0.07376530495556918, "compression_ratio": 1.4593301435406698, "no_speech_prob": 5.682185019395547e-06}, {"id": 277, "seek": 195700, "start": 1976.0, "end": 1981.0, "text": " And even if you use an older GPU, it's still going to save memory.", "tokens": [400, 754, 498, 291, 764, 364, 4906, 18407, 11, 309, 311, 920, 516, 281, 3155, 4675, 13], "temperature": 0.0, "avg_logprob": -0.07376530495556918, "compression_ratio": 1.4593301435406698, "no_speech_prob": 5.682185019395547e-06}, {"id": 278, "seek": 195700, "start": 1981.0, "end": 1984.0, "text": " So actually to ask", "tokens": [407, 767, 281, 1029], "temperature": 0.0, "avg_logprob": -0.07376530495556918, "compression_ratio": 1.4593301435406698, "no_speech_prob": 5.682185019395547e-06}, {"id": 279, "seek": 198400, "start": 1984.0, "end": 1991.0, "text": " fast AI to do that for you, you can add two floating point 16 as in 16 bit", "tokens": [2370, 7318, 281, 360, 300, 337, 291, 11, 291, 393, 909, 732, 12607, 935, 3165, 382, 294, 3165, 857], "temperature": 0.0, "avg_logprob": -0.17915892601013184, "compression_ratio": 1.3910614525139664, "no_speech_prob": 1.7229929653694853e-05}, {"id": 280, "seek": 198400, "start": 1991.0, "end": 1995.0, "text": " at the end of your learner command.", "tokens": [412, 264, 917, 295, 428, 33347, 5622, 13], "temperature": 0.0, "avg_logprob": -0.17915892601013184, "compression_ratio": 1.3910614525139664, "no_speech_prob": 1.7229929653694853e-05}, {"id": 281, "seek": 198400, "start": 1995.0, "end": 2004.0, "text": " So, yeah, so when this finishes, we might try rerunning it", "tokens": [407, 11, 1338, 11, 370, 562, 341, 23615, 11, 321, 1062, 853, 43819, 25589, 309], "temperature": 0.0, "avg_logprob": -0.17915892601013184, "compression_ratio": 1.3910614525139664, "no_speech_prob": 1.7229929653694853e-05}, {"id": 282, "seek": 198400, "start": 2004.0, "end": 2007.0, "text": " with this instead.", "tokens": [365, 341, 2602, 13], "temperature": 0.0, "avg_logprob": -0.17915892601013184, "compression_ratio": 1.3910614525139664, "no_speech_prob": 1.7229929653694853e-05}, {"id": 283, "seek": 198400, "start": 2007.0, "end": 2011.0, "text": " Excuse me, Jeremy, I'm just following along on purpose post.", "tokens": [11359, 385, 11, 17809, 11, 286, 478, 445, 3480, 2051, 322, 4334, 2183, 13], "temperature": 0.0, "avg_logprob": -0.17915892601013184, "compression_ratio": 1.3910614525139664, "no_speech_prob": 1.7229929653694853e-05}, {"id": 284, "seek": 201100, "start": 2011.0, "end": 2023.0, "text": " And if I don't want to bother with importing Tim, just to keep up, what instead of using ConvNext, what would be a good default to use?", "tokens": [400, 498, 286, 500, 380, 528, 281, 8677, 365, 43866, 7172, 11, 445, 281, 1066, 493, 11, 437, 2602, 295, 1228, 2656, 85, 31002, 11, 437, 576, 312, 257, 665, 7576, 281, 764, 30], "temperature": 0.0, "avg_logprob": -0.18480149507522584, "compression_ratio": 1.5260115606936415, "no_speech_prob": 2.2463274945039302e-05}, {"id": 285, "seek": 201100, "start": 2023.0, "end": 2028.0, "text": " I mean, why not use ConvNext?", "tokens": [286, 914, 11, 983, 406, 764, 2656, 85, 31002, 30], "temperature": 0.0, "avg_logprob": -0.18480149507522584, "compression_ratio": 1.5260115606936415, "no_speech_prob": 2.2463274945039302e-05}, {"id": 286, "seek": 201100, "start": 2028.0, "end": 2032.0, "text": " I guess because I would", "tokens": [286, 2041, 570, 286, 576], "temperature": 0.0, "avg_logprob": -0.18480149507522584, "compression_ratio": 1.5260115606936415, "no_speech_prob": 2.2463274945039302e-05}, {"id": 287, "seek": 201100, "start": 2032.0, "end": 2036.0, "text": " I would need to import Tim. I think I missed that bit.", "tokens": [286, 576, 643, 281, 974, 7172, 13, 286, 519, 286, 6721, 300, 857, 13], "temperature": 0.0, "avg_logprob": -0.18480149507522584, "compression_ratio": 1.5260115606936415, "no_speech_prob": 2.2463274945039302e-05}, {"id": 288, "seek": 201100, "start": 2036.0, "end": 2038.0, "text": " So pip install Tim.", "tokens": [407, 8489, 3625, 7172, 13], "temperature": 0.0, "avg_logprob": -0.18480149507522584, "compression_ratio": 1.5260115606936415, "no_speech_prob": 2.2463274945039302e-05}, {"id": 289, "seek": 203800, "start": 2038.0, "end": 2043.0, "text": " Yep. Or if you want to get the more recent", "tokens": [7010, 13, 1610, 498, 291, 528, 281, 483, 264, 544, 5162], "temperature": 0.0, "avg_logprob": -0.1324518918991089, "compression_ratio": 1.1714285714285715, "no_speech_prob": 2.045866676780861e-05}, {"id": 290, "seek": 203800, "start": 2043.0, "end": 2052.0, "text": " models, such as the one we're using, then", "tokens": [5245, 11, 1270, 382, 264, 472, 321, 434, 1228, 11, 550], "temperature": 0.0, "avg_logprob": -0.1324518918991089, "compression_ratio": 1.1714285714285715, "no_speech_prob": 2.045866676780861e-05}, {"id": 291, "seek": 203800, "start": 2052.0, "end": 2063.0, "text": " pip install. Let me copy this for you.", "tokens": [8489, 3625, 13, 961, 385, 5055, 341, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.1324518918991089, "compression_ratio": 1.1714285714285715, "no_speech_prob": 2.045866676780861e-05}, {"id": 292, "seek": 206300, "start": 2063.0, "end": 2069.0, "text": " So that's the command there. And I'll put it in the Zoom chat for you.", "tokens": [407, 300, 311, 264, 5622, 456, 13, 400, 286, 603, 829, 309, 294, 264, 13453, 5081, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.1655665756365575, "compression_ratio": 1.779591836734694, "no_speech_prob": 2.9768752938252874e-05}, {"id": 293, "seek": 206300, "start": 2069.0, "end": 2071.0, "text": " Great, thanks.", "tokens": [3769, 11, 3231, 13], "temperature": 0.0, "avg_logprob": -0.1655665756365575, "compression_ratio": 1.779591836734694, "no_speech_prob": 2.9768752938252874e-05}, {"id": 294, "seek": 206300, "start": 2071.0, "end": 2080.0, "text": " And Jeremy, just while we're talking about fine tuning and as that's going on, I don't know if anyone else would find it helpful.", "tokens": [400, 17809, 11, 445, 1339, 321, 434, 1417, 466, 2489, 15164, 293, 382, 300, 311, 516, 322, 11, 286, 500, 380, 458, 498, 2878, 1646, 576, 915, 309, 4961, 13], "temperature": 0.0, "avg_logprob": -0.1655665756365575, "compression_ratio": 1.779591836734694, "no_speech_prob": 2.9768752938252874e-05}, {"id": 295, "seek": 206300, "start": 2080.0, "end": 2084.0, "text": " But I mean, obviously, like conceptually understand what's happening with fine tuning.", "tokens": [583, 286, 914, 11, 2745, 11, 411, 3410, 671, 1223, 437, 311, 2737, 365, 2489, 15164, 13], "temperature": 0.0, "avg_logprob": -0.1655665756365575, "compression_ratio": 1.779591836734694, "no_speech_prob": 2.9768752938252874e-05}, {"id": 296, "seek": 206300, "start": 2084.0, "end": 2091.0, "text": " But I don't know if anyone else kind of feels like trying to understand better what's actually going under the hood with fine tuning.", "tokens": [583, 286, 500, 380, 458, 498, 2878, 1646, 733, 295, 3417, 411, 1382, 281, 1223, 1101, 437, 311, 767, 516, 833, 264, 13376, 365, 2489, 15164, 13], "temperature": 0.0, "avg_logprob": -0.1655665756365575, "compression_ratio": 1.779591836734694, "no_speech_prob": 2.9768752938252874e-05}, {"id": 297, "seek": 209100, "start": 2091.0, "end": 2098.0, "text": " Like what's actually being altered within the model, like more than just like a kind of at a high level.", "tokens": [1743, 437, 311, 767, 885, 28783, 1951, 264, 2316, 11, 411, 544, 813, 445, 411, 257, 733, 295, 412, 257, 1090, 1496, 13], "temperature": 0.0, "avg_logprob": -0.17297345211631374, "compression_ratio": 1.618421052631579, "no_speech_prob": 1.5205660020001233e-05}, {"id": 298, "seek": 209100, "start": 2098.0, "end": 2107.0, "text": " I'm just trying to get a bit of a better grip on what exactly we're fine tuning and how it's going about fine tuning it, I guess, just under that first surface level.", "tokens": [286, 478, 445, 1382, 281, 483, 257, 857, 295, 257, 1101, 12007, 322, 437, 2293, 321, 434, 2489, 15164, 293, 577, 309, 311, 516, 466, 2489, 15164, 309, 11, 286, 2041, 11, 445, 833, 300, 700, 3753, 1496, 13], "temperature": 0.0, "avg_logprob": -0.17297345211631374, "compression_ratio": 1.618421052631579, "no_speech_prob": 1.5205660020001233e-05}, {"id": 299, "seek": 209100, "start": 2107.0, "end": 2109.0, "text": " Just want to make sure I understand it better.", "tokens": [1449, 528, 281, 652, 988, 286, 1223, 309, 1101, 13], "temperature": 0.0, "avg_logprob": -0.17297345211631374, "compression_ratio": 1.618421052631579, "no_speech_prob": 1.5205660020001233e-05}, {"id": 300, "seek": 209100, "start": 2109.0, "end": 2113.0, "text": " Yeah. So we just looked at the source code for it.", "tokens": [865, 13, 407, 321, 445, 2956, 412, 264, 4009, 3089, 337, 309, 13], "temperature": 0.0, "avg_logprob": -0.17297345211631374, "compression_ratio": 1.618421052631579, "no_speech_prob": 1.5205660020001233e-05}, {"id": 301, "seek": 211300, "start": 2113.0, "end": 2123.0, "text": " So what did you want to go? Which bit of this did you want to look deeper at? Or like what did you?", "tokens": [407, 437, 630, 291, 528, 281, 352, 30, 3013, 857, 295, 341, 630, 291, 528, 281, 574, 7731, 412, 30, 1610, 411, 437, 630, 291, 30], "temperature": 0.0, "avg_logprob": -0.2090182150563886, "compression_ratio": 1.6699507389162562, "no_speech_prob": 1.7229869627044536e-05}, {"id": 302, "seek": 211300, "start": 2123.0, "end": 2126.0, "text": " Yeah, tell me more what you want to know.", "tokens": [865, 11, 980, 385, 544, 437, 291, 528, 281, 458, 13], "temperature": 0.0, "avg_logprob": -0.2090182150563886, "compression_ratio": 1.6699507389162562, "no_speech_prob": 1.7229869627044536e-05}, {"id": 303, "seek": 211300, "start": 2126.0, "end": 2133.0, "text": " Yeah, I guess so. Yeah, like stepping through it. So like we've got it frozen at a particular point, right? And then this fit one cycle.", "tokens": [865, 11, 286, 2041, 370, 13, 865, 11, 411, 16821, 807, 309, 13, 407, 411, 321, 600, 658, 309, 12496, 412, 257, 1729, 935, 11, 558, 30, 400, 550, 341, 3318, 472, 6586, 13], "temperature": 0.0, "avg_logprob": -0.2090182150563886, "compression_ratio": 1.6699507389162562, "no_speech_prob": 1.7229869627044536e-05}, {"id": 304, "seek": 211300, "start": 2133.0, "end": 2137.0, "text": " So just going through the definition of fit one cycle again.", "tokens": [407, 445, 516, 807, 264, 7123, 295, 3318, 472, 6586, 797, 13], "temperature": 0.0, "avg_logprob": -0.2090182150563886, "compression_ratio": 1.6699507389162562, "no_speech_prob": 1.7229869627044536e-05}, {"id": 305, "seek": 213700, "start": 2137.0, "end": 2144.0, "text": " Oh, so we haven't done that yet in the course, I don't think. Yeah. So yeah, so we can certainly talk about that.", "tokens": [876, 11, 370, 321, 2378, 380, 1096, 300, 1939, 294, 264, 1164, 11, 286, 500, 380, 519, 13, 865, 13, 407, 1338, 11, 370, 321, 393, 3297, 751, 466, 300, 13], "temperature": 0.0, "avg_logprob": -0.1246849275984854, "compression_ratio": 1.5911111111111111, "no_speech_prob": 1.696337858447805e-05}, {"id": 306, "seek": 213700, "start": 2144.0, "end": 2148.0, "text": " I don't want to hijack things. If other people want to kind of move on, that's fine. I can follow it up later.", "tokens": [286, 500, 380, 528, 281, 10625, 501, 721, 13, 759, 661, 561, 528, 281, 733, 295, 1286, 322, 11, 300, 311, 2489, 13, 286, 393, 1524, 309, 493, 1780, 13], "temperature": 0.0, "avg_logprob": -0.1246849275984854, "compression_ratio": 1.5911111111111111, "no_speech_prob": 1.696337858447805e-05}, {"id": 307, "seek": 213700, "start": 2148.0, "end": 2153.0, "text": " But I just I just kind of wanted to get a bit of an overview of what's actually going on there.", "tokens": [583, 286, 445, 286, 445, 733, 295, 1415, 281, 483, 257, 857, 295, 364, 12492, 295, 437, 311, 767, 516, 322, 456, 13], "temperature": 0.0, "avg_logprob": -0.1246849275984854, "compression_ratio": 1.5911111111111111, "no_speech_prob": 1.696337858447805e-05}, {"id": 308, "seek": 213700, "start": 2153.0, "end": 2162.0, "text": " Let's take a look. Hit one cycle. So.", "tokens": [961, 311, 747, 257, 574, 13, 9217, 472, 6586, 13, 407, 13], "temperature": 0.0, "avg_logprob": -0.1246849275984854, "compression_ratio": 1.5911111111111111, "no_speech_prob": 1.696337858447805e-05}, {"id": 309, "seek": 216200, "start": 2162.0, "end": 2170.0, "text": " So we look at the docs. Here it is.", "tokens": [407, 321, 574, 412, 264, 45623, 13, 1692, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.11979039510091145, "compression_ratio": 1.6551724137931034, "no_speech_prob": 5.014543148718076e-06}, {"id": 310, "seek": 216200, "start": 2170.0, "end": 2179.0, "text": " So what does fit one cycle do? So actually, there's a paper you can read if you want to know exactly what it does.", "tokens": [407, 437, 775, 3318, 472, 6586, 360, 30, 407, 767, 11, 456, 311, 257, 3035, 291, 393, 1401, 498, 291, 528, 281, 458, 2293, 437, 309, 775, 13], "temperature": 0.0, "avg_logprob": -0.11979039510091145, "compression_ratio": 1.6551724137931034, "no_speech_prob": 5.014543148718076e-06}, {"id": 311, "seek": 216200, "start": 2179.0, "end": 2188.0, "text": " But there's a picture here which tells you what it does. And what this picture is, is so fit one cycle uses something called a scheduler.", "tokens": [583, 456, 311, 257, 3036, 510, 597, 5112, 291, 437, 309, 775, 13, 400, 437, 341, 3036, 307, 11, 307, 370, 3318, 472, 6586, 4960, 746, 1219, 257, 12000, 260, 13], "temperature": 0.0, "avg_logprob": -0.11979039510091145, "compression_ratio": 1.6551724137931034, "no_speech_prob": 5.014543148718076e-06}, {"id": 312, "seek": 218800, "start": 2188.0, "end": 2195.0, "text": " And a scheduler is something which actually changes the learning rate during the training.", "tokens": [400, 257, 12000, 260, 307, 746, 597, 767, 2962, 264, 2539, 3314, 1830, 264, 3097, 13], "temperature": 0.0, "avg_logprob": -0.06156884299384223, "compression_ratio": 1.6868686868686869, "no_speech_prob": 5.0141957217419986e-06}, {"id": 313, "seek": 218800, "start": 2195.0, "end": 2204.0, "text": " So remember, the learning rate is the thing we multiply the gradients by before we subtract from the parameters.", "tokens": [407, 1604, 11, 264, 2539, 3314, 307, 264, 551, 321, 12972, 264, 2771, 2448, 538, 949, 321, 16390, 490, 264, 9834, 13], "temperature": 0.0, "avg_logprob": -0.06156884299384223, "compression_ratio": 1.6868686868686869, "no_speech_prob": 5.0141957217419986e-06}, {"id": 314, "seek": 218800, "start": 2204.0, "end": 2212.0, "text": " When you have a randomly initialized model or even a pre trained model, we actually randomly initialize the last layer of weights.", "tokens": [1133, 291, 362, 257, 16979, 5883, 1602, 2316, 420, 754, 257, 659, 8895, 2316, 11, 321, 767, 16979, 5883, 1125, 264, 1036, 4583, 295, 17443, 13], "temperature": 0.0, "avg_logprob": -0.06156884299384223, "compression_ratio": 1.6868686868686869, "no_speech_prob": 5.0141957217419986e-06}, {"id": 315, "seek": 221200, "start": 2212.0, "end": 2221.0, "text": " So at first, even a pre trained model that we're fine tuning can't do anything. It's still giving random answers.", "tokens": [407, 412, 700, 11, 754, 257, 659, 8895, 2316, 300, 321, 434, 2489, 15164, 393, 380, 360, 1340, 13, 467, 311, 920, 2902, 4974, 6338, 13], "temperature": 0.0, "avg_logprob": -0.10200355792867727, "compression_ratio": 1.6788990825688073, "no_speech_prob": 8.578556389693404e-07}, {"id": 316, "seek": 221200, "start": 2221.0, "end": 2232.0, "text": " And so that means we want to use a really small learning rate because it's very it's very difficult to get to a point where it's starting to learn something slightly useful.", "tokens": [400, 370, 300, 1355, 321, 528, 281, 764, 257, 534, 1359, 2539, 3314, 570, 309, 311, 588, 309, 311, 588, 2252, 281, 483, 281, 257, 935, 689, 309, 311, 2891, 281, 1466, 746, 4748, 4420, 13], "temperature": 0.0, "avg_logprob": -0.10200355792867727, "compression_ratio": 1.6788990825688073, "no_speech_prob": 8.578556389693404e-07}, {"id": 317, "seek": 221200, "start": 2232.0, "end": 2239.0, "text": " And so when we start training the first year batches use a tiny learning rate.", "tokens": [400, 370, 562, 321, 722, 3097, 264, 700, 1064, 15245, 279, 764, 257, 5870, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.10200355792867727, "compression_ratio": 1.6788990825688073, "no_speech_prob": 8.578556389693404e-07}, {"id": 318, "seek": 223900, "start": 2239.0, "end": 2251.0, "text": " And then as it gets better and better at like doing something useful, you can increase the learning rate, because it's it's got to a point where it's like, yeah, it's kind of knows vaguely what it's doing.", "tokens": [400, 550, 382, 309, 2170, 1101, 293, 1101, 412, 411, 884, 746, 4420, 11, 291, 393, 3488, 264, 2539, 3314, 11, 570, 309, 311, 309, 311, 658, 281, 257, 935, 689, 309, 311, 411, 11, 1338, 11, 309, 311, 733, 295, 3255, 13501, 48863, 437, 309, 311, 884, 13], "temperature": 0.0, "avg_logprob": -0.06334589466904149, "compression_ratio": 1.88, "no_speech_prob": 1.8737783875621972e-06}, {"id": 319, "seek": 223900, "start": 2251.0, "end": 2255.0, "text": " And so as it trains the learning rate goes up and up and up and up and up.", "tokens": [400, 370, 382, 309, 16329, 264, 2539, 3314, 1709, 493, 293, 493, 293, 493, 293, 493, 293, 493, 13], "temperature": 0.0, "avg_logprob": -0.06334589466904149, "compression_ratio": 1.88, "no_speech_prob": 1.8737783875621972e-06}, {"id": 320, "seek": 223900, "start": 2255.0, "end": 2260.0, "text": " And then as you start to get close to the answer.", "tokens": [400, 550, 382, 291, 722, 281, 483, 1998, 281, 264, 1867, 13], "temperature": 0.0, "avg_logprob": -0.06334589466904149, "compression_ratio": 1.88, "no_speech_prob": 1.8737783875621972e-06}, {"id": 321, "seek": 223900, "start": 2260.0, "end": 2263.0, "text": " You need to decrease the learning rate again.", "tokens": [509, 643, 281, 11514, 264, 2539, 3314, 797, 13], "temperature": 0.0, "avg_logprob": -0.06334589466904149, "compression_ratio": 1.88, "no_speech_prob": 1.8737783875621972e-06}, {"id": 322, "seek": 226300, "start": 2263.0, "end": 2272.0, "text": " And the reason for that is that you're really fine going to small little steps you're really, really close now.", "tokens": [400, 264, 1778, 337, 300, 307, 300, 291, 434, 534, 2489, 516, 281, 1359, 707, 4439, 291, 434, 534, 11, 534, 1998, 586, 13], "temperature": 0.0, "avg_logprob": -0.2407457198219738, "compression_ratio": 1.7575757575757576, "no_speech_prob": 2.318602128070779e-05}, {"id": 323, "seek": 226300, "start": 2272.0, "end": 2287.0, "text": " So when you say, yep. So you say now. So as you get closer to the answer, like are we saying that that's in comparison to the validation set so that we're so that we're moving away from overfit where those might, yeah, I guess where the", "tokens": [407, 562, 291, 584, 11, 18633, 13, 407, 291, 584, 586, 13, 407, 382, 291, 483, 4966, 281, 264, 1867, 11, 411, 366, 321, 1566, 300, 300, 311, 294, 9660, 281, 264, 24071, 992, 370, 300, 321, 434, 370, 300, 321, 434, 2684, 1314, 490, 670, 6845, 689, 729, 1062, 11, 1338, 11, 286, 2041, 689, 264], "temperature": 0.0, "avg_logprob": -0.2407457198219738, "compression_ratio": 1.7575757575757576, "no_speech_prob": 2.318602128070779e-05}, {"id": 324, "seek": 228700, "start": 2287.0, "end": 2299.0, "text": " training or anything. This is just so this is just a plot of, of the curve of batch number against learning rate.", "tokens": [3097, 420, 1340, 13, 639, 307, 445, 370, 341, 307, 445, 257, 7542, 295, 11, 295, 264, 7605, 295, 15245, 1230, 1970, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.21317550983834774, "compression_ratio": 1.813397129186603, "no_speech_prob": 8.397923920711037e-06}, {"id": 325, "seek": 228700, "start": 2299.0, "end": 2303.0, "text": " So this is this is the shape the exact shape that is used.", "tokens": [407, 341, 307, 341, 307, 264, 3909, 264, 1900, 3909, 300, 307, 1143, 13], "temperature": 0.0, "avg_logprob": -0.21317550983834774, "compression_ratio": 1.813397129186603, "no_speech_prob": 8.397923920711037e-06}, {"id": 326, "seek": 228700, "start": 2303.0, "end": 2307.0, "text": " There's nothing clever going on it just it just follows this exact curve.", "tokens": [821, 311, 1825, 13494, 516, 322, 309, 445, 309, 445, 10002, 341, 1900, 7605, 13], "temperature": 0.0, "avg_logprob": -0.21317550983834774, "compression_ratio": 1.813397129186603, "no_speech_prob": 8.397923920711037e-06}, {"id": 327, "seek": 228700, "start": 2307.0, "end": 2313.0, "text": " Okay, so there's no there's that's not interacting with anything else to derive those numbers it's just doing that. Okay. All right.", "tokens": [1033, 11, 370, 456, 311, 572, 456, 311, 300, 311, 406, 18017, 365, 1340, 1646, 281, 28446, 729, 3547, 309, 311, 445, 884, 300, 13, 1033, 13, 1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.21317550983834774, "compression_ratio": 1.813397129186603, "no_speech_prob": 8.397923920711037e-06}, {"id": 328, "seek": 231300, "start": 2313.0, "end": 2322.0, "text": " Understood. So in fact, if we look at the source code for it.", "tokens": [42832, 13, 407, 294, 1186, 11, 498, 321, 574, 412, 264, 4009, 3089, 337, 309, 13], "temperature": 0.0, "avg_logprob": -0.2488311057867006, "compression_ratio": 1.2654867256637168, "no_speech_prob": 3.392972075744183e-06}, {"id": 329, "seek": 231300, "start": 2322.0, "end": 2328.0, "text": " What it does is it calls them in code combined cause.", "tokens": [708, 309, 775, 307, 309, 5498, 552, 294, 3089, 9354, 3082, 13], "temperature": 0.0, "avg_logprob": -0.2488311057867006, "compression_ratio": 1.2654867256637168, "no_speech_prob": 3.392972075744183e-06}, {"id": 330, "seek": 231300, "start": 2328.0, "end": 2332.0, "text": " I'll get the definition of,", "tokens": [286, 603, 483, 264, 7123, 295, 11], "temperature": 0.0, "avg_logprob": -0.2488311057867006, "compression_ratio": 1.2654867256637168, "no_speech_prob": 3.392972075744183e-06}, {"id": 331, "seek": 233200, "start": 2332.0, "end": 2348.0, "text": " which is something that uses to cosine schedules, and so a cosine schedule is one that. So it's also known as an annealer it's called learning rate and dealing is something that literally uses cosine.", "tokens": [597, 307, 746, 300, 4960, 281, 23565, 28078, 11, 293, 370, 257, 23565, 7567, 307, 472, 300, 13, 407, 309, 311, 611, 2570, 382, 364, 22256, 17148, 309, 311, 1219, 2539, 3314, 293, 6260, 307, 746, 300, 3736, 4960, 23565, 13], "temperature": 0.0, "avg_logprob": -0.16446024411684507, "compression_ratio": 1.6795580110497237, "no_speech_prob": 3.84479426429607e-06}, {"id": 332, "seek": 233200, "start": 2348.0, "end": 2350.0, "text": " That's it. Yep.", "tokens": [663, 311, 309, 13, 7010, 13], "temperature": 0.0, "avg_logprob": -0.16446024411684507, "compression_ratio": 1.6795580110497237, "no_speech_prob": 3.84479426429607e-06}, {"id": 333, "seek": 233200, "start": 2350.0, "end": 2357.0, "text": " Okay, that that's helpful so I get now kind of where that's mapping to that, that idea.", "tokens": [1033, 11, 300, 300, 311, 4961, 370, 286, 483, 586, 733, 295, 689, 300, 311, 18350, 281, 300, 11, 300, 1558, 13], "temperature": 0.0, "avg_logprob": -0.16446024411684507, "compression_ratio": 1.6795580110497237, "no_speech_prob": 3.84479426429607e-06}, {"id": 334, "seek": 235700, "start": 2357.0, "end": 2377.0, "text": " Yeah. And you know for people who are interested in going deeper and understanding like, what is fast AI do and why and what, what actually makes what's important in deep learning and stuff this is how exploring the documentation and source code of fast AI,", "tokens": [865, 13, 400, 291, 458, 337, 561, 567, 366, 3102, 294, 516, 7731, 293, 3701, 411, 11, 437, 307, 2370, 7318, 360, 293, 983, 293, 437, 11, 437, 767, 1669, 437, 311, 1021, 294, 2452, 2539, 293, 1507, 341, 307, 577, 12736, 264, 14333, 293, 4009, 3089, 295, 2370, 7318, 11], "temperature": 0.0, "avg_logprob": -0.18480085893110795, "compression_ratio": 1.4855491329479769, "no_speech_prob": 5.337715265341103e-06}, {"id": 335, "seek": 237700, "start": 2377.0, "end": 2390.0, "text": " whether you're at a point where you feel this is what you're ready to do can be super useful because you know the documentation can tell us you what paper is being implemented and why and shows you pictures of what it's doing and the source code is something that you can copy", "tokens": [1968, 291, 434, 412, 257, 935, 689, 291, 841, 341, 307, 437, 291, 434, 1919, 281, 360, 393, 312, 1687, 4420, 570, 291, 458, 264, 14333, 393, 980, 505, 291, 437, 3035, 307, 885, 12270, 293, 983, 293, 3110, 291, 5242, 295, 437, 309, 311, 884, 293, 264, 4009, 3089, 307, 746, 300, 291, 393, 5055], "temperature": 0.0, "avg_logprob": -0.15333079328440657, "compression_ratio": 1.7037037037037037, "no_speech_prob": 8.529716069460846e-06}, {"id": 336, "seek": 237700, "start": 2390.0, "end": 2394.0, "text": " and paste into your notebook and try it yourself and so forth.", "tokens": [293, 9163, 666, 428, 21060, 293, 853, 309, 1803, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.15333079328440657, "compression_ratio": 1.7037037037037037, "no_speech_prob": 8.529716069460846e-06}, {"id": 337, "seek": 237700, "start": 2394.0, "end": 2399.0, "text": " Yes, did you have any other questions about this.", "tokens": [1079, 11, 630, 291, 362, 604, 661, 1651, 466, 341, 13], "temperature": 0.0, "avg_logprob": -0.15333079328440657, "compression_ratio": 1.7037037037037037, "no_speech_prob": 8.529716069460846e-06}, {"id": 338, "seek": 237700, "start": 2399.0, "end": 2401.0, "text": " No, that's, that's fine.", "tokens": [883, 11, 300, 311, 11, 300, 311, 2489, 13], "temperature": 0.0, "avg_logprob": -0.15333079328440657, "compression_ratio": 1.7037037037037037, "no_speech_prob": 8.529716069460846e-06}, {"id": 339, "seek": 240100, "start": 2401.0, "end": 2420.0, "text": " Most of us. All right. I just wanted to comment that Sylvan had a very good blog post explaining those it one cycle policy. Yes, he does.", "tokens": [4534, 295, 505, 13, 1057, 558, 13, 286, 445, 1415, 281, 2871, 300, 33349, 9768, 632, 257, 588, 665, 6968, 2183, 13468, 729, 309, 472, 6586, 3897, 13, 1079, 11, 415, 775, 13], "temperature": 0.0, "avg_logprob": -0.2910306279252215, "compression_ratio": 1.1869918699186992, "no_speech_prob": 5.0144521992478985e-06}, {"id": 340, "seek": 240100, "start": 2420.0, "end": 2423.0, "text": " Perfect.", "tokens": [10246, 13], "temperature": 0.0, "avg_logprob": -0.2910306279252215, "compression_ratio": 1.1869918699186992, "no_speech_prob": 5.0144521992478985e-06}, {"id": 341, "seek": 242300, "start": 2423.0, "end": 2433.0, "text": " And so there's other policies you can use like the triangular version. So this is actually what we originally did I think for one cycle as you can see it ends up being pretty similar.", "tokens": [400, 370, 456, 311, 661, 7657, 291, 393, 764, 411, 264, 38190, 3037, 13, 407, 341, 307, 767, 437, 321, 7993, 630, 286, 519, 337, 472, 6586, 382, 291, 393, 536, 309, 5314, 493, 885, 1238, 2531, 13], "temperature": 0.0, "avg_logprob": -0.08839687017294076, "compression_ratio": 1.7176470588235293, "no_speech_prob": 1.3211048099037725e-05}, {"id": 342, "seek": 242300, "start": 2433.0, "end": 2444.0, "text": " And what would be, I guess the criteria for where you would change that policy like, like what would, I guess what's the basis for the decision you make about changing that policy.", "tokens": [400, 437, 576, 312, 11, 286, 2041, 264, 11101, 337, 689, 291, 576, 1319, 300, 3897, 411, 11, 411, 437, 576, 11, 286, 2041, 437, 311, 264, 5143, 337, 264, 3537, 291, 652, 466, 4473, 300, 3897, 13], "temperature": 0.0, "avg_logprob": -0.08839687017294076, "compression_ratio": 1.7176470588235293, "no_speech_prob": 1.3211048099037725e-05}, {"id": 343, "seek": 242300, "start": 2444.0, "end": 2448.0, "text": " I mean, you don't basically it works fine and you just do it. Yeah. Okay.", "tokens": [286, 914, 11, 291, 500, 380, 1936, 309, 1985, 2489, 293, 291, 445, 360, 309, 13, 865, 13, 1033, 13], "temperature": 0.0, "avg_logprob": -0.08839687017294076, "compression_ratio": 1.7176470588235293, "no_speech_prob": 1.3211048099037725e-05}, {"id": 344, "seek": 244800, "start": 2448.0, "end": 2456.0, "text": " All right. So it's pretty arbitrary. Yeah, I mean, it's, it's not arbitrary. It's something that lots of experimentation has found that this works well.", "tokens": [1057, 558, 13, 407, 309, 311, 1238, 23211, 13, 865, 11, 286, 914, 11, 309, 311, 11, 309, 311, 406, 23211, 13, 467, 311, 746, 300, 3195, 295, 37142, 575, 1352, 300, 341, 1985, 731, 13], "temperature": 0.0, "avg_logprob": -0.15183415015538534, "compression_ratio": 1.7866666666666666, "no_speech_prob": 1.260574754269328e-05}, {"id": 345, "seek": 244800, "start": 2456.0, "end": 2471.0, "text": " Okay, right. Like, like everything pretty much in fast AI we try to find the things that work well, and the things that need changing we generally tell people all about them, but this is generally something that doesn't need changing too much.", "tokens": [1033, 11, 558, 13, 1743, 11, 411, 1203, 1238, 709, 294, 2370, 7318, 321, 853, 281, 915, 264, 721, 300, 589, 731, 11, 293, 264, 721, 300, 643, 4473, 321, 5101, 980, 561, 439, 466, 552, 11, 457, 341, 307, 5101, 746, 300, 1177, 380, 643, 4473, 886, 709, 13], "temperature": 0.0, "avg_logprob": -0.15183415015538534, "compression_ratio": 1.7866666666666666, "no_speech_prob": 1.260574754269328e-05}, {"id": 346, "seek": 244800, "start": 2471.0, "end": 2473.0, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.15183415015538534, "compression_ratio": 1.7866666666666666, "no_speech_prob": 1.260574754269328e-05}, {"id": 347, "seek": 247300, "start": 2473.0, "end": 2478.0, "text": " So is this related to learning trade finder.", "tokens": [407, 307, 341, 4077, 281, 2539, 4923, 915, 260, 13], "temperature": 0.0, "avg_logprob": -0.2007500871698907, "compression_ratio": 1.4122137404580153, "no_speech_prob": 1.6438745660707355e-05}, {"id": 348, "seek": 247300, "start": 2478.0, "end": 2481.0, "text": " Okay, so let's talk about learning.", "tokens": [1033, 11, 370, 718, 311, 751, 466, 2539, 13], "temperature": 0.0, "avg_logprob": -0.2007500871698907, "compression_ratio": 1.4122137404580153, "no_speech_prob": 1.6438745660707355e-05}, {"id": 349, "seek": 247300, "start": 2481.0, "end": 2489.0, "text": " I'm finding it quite confusing, which actually which which number is correct with learning right finder.", "tokens": [286, 478, 5006, 309, 1596, 13181, 11, 597, 767, 597, 597, 1230, 307, 3006, 365, 2539, 558, 915, 260, 13], "temperature": 0.0, "avg_logprob": -0.2007500871698907, "compression_ratio": 1.4122137404580153, "no_speech_prob": 1.6438745660707355e-05}, {"id": 350, "seek": 248900, "start": 2489.0, "end": 2503.0, "text": " So I'm just before you do that I just point out so that the mixed precision version on my RTX card, which is a consumer GPU. The speeds gone from a minute 41 to 28 seconds.", "tokens": [407, 286, 478, 445, 949, 291, 360, 300, 286, 445, 935, 484, 370, 300, 264, 7467, 18356, 3037, 322, 452, 44573, 2920, 11, 597, 307, 257, 9711, 18407, 13, 440, 16411, 2780, 490, 257, 3456, 18173, 281, 7562, 3949, 13], "temperature": 0.0, "avg_logprob": -0.14109445229554787, "compression_ratio": 1.4101382488479262, "no_speech_prob": 7.888820618973114e-06}, {"id": 351, "seek": 248900, "start": 2503.0, "end": 2509.0, "text": " So you can see it really does make a huge difference to use fp 16.", "tokens": [407, 291, 393, 536, 309, 534, 775, 652, 257, 2603, 2649, 281, 764, 283, 79, 3165, 13], "temperature": 0.0, "avg_logprob": -0.14109445229554787, "compression_ratio": 1.4101382488479262, "no_speech_prob": 7.888820618973114e-06}, {"id": 352, "seek": 248900, "start": 2509.0, "end": 2518.0, "text": " So, this question about something called the learning rate finder.", "tokens": [407, 11, 341, 1168, 466, 746, 1219, 264, 2539, 3314, 915, 260, 13], "temperature": 0.0, "avg_logprob": -0.14109445229554787, "compression_ratio": 1.4101382488479262, "no_speech_prob": 7.888820618973114e-06}, {"id": 353, "seek": 251800, "start": 2518.0, "end": 2525.0, "text": " So the learning rate finder does something very similar to one cycle.", "tokens": [407, 264, 2539, 3314, 915, 260, 775, 746, 588, 2531, 281, 472, 6586, 13], "temperature": 0.0, "avg_logprob": -0.13328489576067243, "compression_ratio": 1.5604395604395604, "no_speech_prob": 5.093423169455491e-06}, {"id": 354, "seek": 251800, "start": 2525.0, "end": 2531.0, "text": " The one cycle scheduler or one cycle in yelling, which is it gradually increases the learning rate.", "tokens": [440, 472, 6586, 12000, 260, 420, 472, 6586, 294, 18381, 11, 597, 307, 309, 13145, 8637, 264, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.13328489576067243, "compression_ratio": 1.5604395604395604, "no_speech_prob": 5.093423169455491e-06}, {"id": 355, "seek": 251800, "start": 2531.0, "end": 2542.0, "text": " While it trains, but it actually only does up to 100 batches. So, generally speaking, far less than even an epoch.", "tokens": [3987, 309, 16329, 11, 457, 309, 767, 787, 775, 493, 281, 2319, 15245, 279, 13, 407, 11, 5101, 4124, 11, 1400, 1570, 813, 754, 364, 30992, 339, 13], "temperature": 0.0, "avg_logprob": -0.13328489576067243, "compression_ratio": 1.5604395604395604, "no_speech_prob": 5.093423169455491e-06}, {"id": 356, "seek": 254200, "start": 2542.0, "end": 2550.0, "text": " And it doesn't increase the learning rate and then decreases again, it just keeps increasing the learning rate until the whole thing falls apart.", "tokens": [400, 309, 1177, 380, 3488, 264, 2539, 3314, 293, 550, 24108, 797, 11, 309, 445, 5965, 5662, 264, 2539, 3314, 1826, 264, 1379, 551, 8804, 4936, 13], "temperature": 0.0, "avg_logprob": -0.12489510536193847, "compression_ratio": 1.8761061946902655, "no_speech_prob": 5.682170012732968e-06}, {"id": 357, "seek": 254200, "start": 2550.0, "end": 2562.0, "text": " And so, this is a graph of the learning rate. And remember, it increases that logarithmically increasing every batch. So this is also kind of a graph of time of batch number.", "tokens": [400, 370, 11, 341, 307, 257, 4295, 295, 264, 2539, 3314, 13, 400, 1604, 11, 309, 8637, 300, 41473, 32674, 984, 5662, 633, 15245, 13, 407, 341, 307, 611, 733, 295, 257, 4295, 295, 565, 295, 15245, 1230, 13], "temperature": 0.0, "avg_logprob": -0.12489510536193847, "compression_ratio": 1.8761061946902655, "no_speech_prob": 5.682170012732968e-06}, {"id": 358, "seek": 254200, "start": 2562.0, "end": 2568.0, "text": " And then it shows you what loss it got so for batch, the first few batches it got a loss of about four.", "tokens": [400, 550, 309, 3110, 291, 437, 4470, 309, 658, 370, 337, 15245, 11, 264, 700, 1326, 15245, 279, 309, 658, 257, 4470, 295, 466, 1451, 13], "temperature": 0.0, "avg_logprob": -0.12489510536193847, "compression_ratio": 1.8761061946902655, "no_speech_prob": 5.682170012732968e-06}, {"id": 359, "seek": 256800, "start": 2568.0, "end": 2573.0, "text": " And until it got up to a learning rate of about 10 to the negative four, nothing really improved.", "tokens": [400, 1826, 309, 658, 493, 281, 257, 2539, 3314, 295, 466, 1266, 281, 264, 3671, 1451, 11, 1825, 534, 9689, 13], "temperature": 0.0, "avg_logprob": -0.12439602122587316, "compression_ratio": 1.725, "no_speech_prob": 3.187496076861862e-06}, {"id": 360, "seek": 256800, "start": 2573.0, "end": 2578.0, "text": " So clearly learning rates of less than 10 to the negative four aren't very useful.", "tokens": [407, 4448, 2539, 6846, 295, 1570, 813, 1266, 281, 264, 3671, 1451, 3212, 380, 588, 4420, 13], "temperature": 0.0, "avg_logprob": -0.12439602122587316, "compression_ratio": 1.725, "no_speech_prob": 3.187496076861862e-06}, {"id": 361, "seek": 256800, "start": 2578.0, "end": 2585.0, "text": " And then as it increased the learning rate, you can see the slope started to get steeper and steeper.", "tokens": [400, 550, 382, 309, 6505, 264, 2539, 3314, 11, 291, 393, 536, 264, 13525, 1409, 281, 483, 16841, 260, 293, 16841, 260, 13], "temperature": 0.0, "avg_logprob": -0.12439602122587316, "compression_ratio": 1.725, "no_speech_prob": 3.187496076861862e-06}, {"id": 362, "seek": 256800, "start": 2585.0, "end": 2589.0, "text": " And so this area here is where it's learning the most quickly.", "tokens": [400, 370, 341, 1859, 510, 307, 689, 309, 311, 2539, 264, 881, 2661, 13], "temperature": 0.0, "avg_logprob": -0.12439602122587316, "compression_ratio": 1.725, "no_speech_prob": 3.187496076861862e-06}, {"id": 363, "seek": 258900, "start": 2589.0, "end": 2598.0, "text": " And then it gets to a point up here where it's too high. And when it gets too high.", "tokens": [400, 550, 309, 2170, 281, 257, 935, 493, 510, 689, 309, 311, 886, 1090, 13, 400, 562, 309, 2170, 886, 1090, 13], "temperature": 0.0, "avg_logprob": -0.12351907309839281, "compression_ratio": 1.624113475177305, "no_speech_prob": 2.947944039988215e-06}, {"id": 364, "seek": 258900, "start": 2598.0, "end": 2606.0, "text": " Initially it just doesn't really improve at all. And then it gets really too high it jumps past the answer. Right. And starts getting much worse.", "tokens": [29446, 309, 445, 1177, 380, 534, 3470, 412, 439, 13, 400, 550, 309, 2170, 534, 886, 1090, 309, 16704, 1791, 264, 1867, 13, 1779, 13, 400, 3719, 1242, 709, 5324, 13], "temperature": 0.0, "avg_logprob": -0.12351907309839281, "compression_ratio": 1.624113475177305, "no_speech_prob": 2.947944039988215e-06}, {"id": 365, "seek": 260600, "start": 2606.0, "end": 2622.0, "text": " So the. Yeah, so I generally just pick somewhere visually around the middle. Or you can, you know, see it says something around.", "tokens": [407, 264, 13, 865, 11, 370, 286, 5101, 445, 1888, 4079, 19622, 926, 264, 2808, 13, 1610, 291, 393, 11, 291, 458, 11, 536, 309, 1619, 746, 926, 13], "temperature": 0.0, "avg_logprob": -0.23626125835981526, "compression_ratio": 1.4409937888198758, "no_speech_prob": 7.646126505278517e-06}, {"id": 366, "seek": 260600, "start": 2622.0, "end": 2630.0, "text": " So is the magnitude of that thing or what why why we wouldn't choose the minimum here. I mean, I, okay.", "tokens": [407, 307, 264, 15668, 295, 300, 551, 420, 437, 983, 983, 321, 2759, 380, 2826, 264, 7285, 510, 13, 286, 914, 11, 286, 11, 1392, 13], "temperature": 0.0, "avg_logprob": -0.23626125835981526, "compression_ratio": 1.4409937888198758, "no_speech_prob": 7.646126505278517e-06}, {"id": 367, "seek": 263000, "start": 2630.0, "end": 2638.0, "text": " Sorry that that's what I felt like. Yeah, this would be a really bad spot right because at this point it's not learning.", "tokens": [4919, 300, 300, 311, 437, 286, 2762, 411, 13, 865, 11, 341, 576, 312, 257, 534, 1578, 4008, 558, 570, 412, 341, 935, 309, 311, 406, 2539, 13], "temperature": 0.0, "avg_logprob": -0.1565622954533018, "compression_ratio": 1.9291666666666667, "no_speech_prob": 1.777780744305346e-05}, {"id": 368, "seek": 263000, "start": 2638.0, "end": 2644.0, "text": " So what you want to look at is the is the gradient you want to look at the slope of this line because the slope is how quickly is it improving.", "tokens": [407, 437, 291, 528, 281, 574, 412, 307, 264, 307, 264, 16235, 291, 528, 281, 574, 412, 264, 13525, 295, 341, 1622, 570, 264, 13525, 307, 577, 2661, 307, 309, 11470, 13], "temperature": 0.0, "avg_logprob": -0.1565622954533018, "compression_ratio": 1.9291666666666667, "no_speech_prob": 1.777780744305346e-05}, {"id": 369, "seek": 263000, "start": 2644.0, "end": 2652.0, "text": " So at this point here that this learning rate, it doesn't improve at all. So if we use this learning rate. So would that make sense to use gradient actually for this.", "tokens": [407, 412, 341, 935, 510, 300, 341, 2539, 3314, 11, 309, 1177, 380, 3470, 412, 439, 13, 407, 498, 321, 764, 341, 2539, 3314, 13, 407, 576, 300, 652, 2020, 281, 764, 16235, 767, 337, 341, 13], "temperature": 0.0, "avg_logprob": -0.1565622954533018, "compression_ratio": 1.9291666666666667, "no_speech_prob": 1.777780744305346e-05}, {"id": 370, "seek": 263000, "start": 2652.0, "end": 2654.0, "text": " Yes, to see what's the minimum.", "tokens": [1079, 11, 281, 536, 437, 311, 264, 7285, 13], "temperature": 0.0, "avg_logprob": -0.1565622954533018, "compression_ratio": 1.9291666666666667, "no_speech_prob": 1.777780744305346e-05}, {"id": 371, "seek": 265400, "start": 2654.0, "end": 2662.0, "text": " I mean, rather than doing that visually. Well, not necessarily because you see here there's a really steep gradient but that's definitely a bad spot.", "tokens": [286, 914, 11, 2831, 813, 884, 300, 19622, 13, 1042, 11, 406, 4725, 570, 291, 536, 510, 456, 311, 257, 534, 16841, 16235, 457, 300, 311, 2138, 257, 1578, 4008, 13], "temperature": 0.0, "avg_logprob": -0.14624569027922873, "compression_ratio": 1.5423728813559323, "no_speech_prob": 4.264242670615204e-05}, {"id": 372, "seek": 265400, "start": 2662.0, "end": 2677.0, "text": " True. Okay, sorry. So, I mean, don't be sorry it's a great question like it's surprisingly difficult to come up algorithmically with the thing that our eyes do when we say like oh we're about somewhere around here.", "tokens": [13587, 13, 1033, 11, 2597, 13, 407, 11, 286, 914, 11, 500, 380, 312, 2597, 309, 311, 257, 869, 1168, 411, 309, 311, 17600, 2252, 281, 808, 493, 9284, 984, 365, 264, 551, 300, 527, 2575, 360, 562, 321, 584, 411, 1954, 321, 434, 466, 4079, 926, 510, 13], "temperature": 0.0, "avg_logprob": -0.14624569027922873, "compression_ratio": 1.5423728813559323, "no_speech_prob": 4.264242670615204e-05}, {"id": 373, "seek": 267700, "start": 2677.0, "end": 2689.0, "text": " And wouldn't that be local meaning minimum. No, sorry, because the minimum is down here, which is definitely not what we want. And the minimum gradient would be here, which is definitely not what we want.", "tokens": [400, 2759, 380, 300, 312, 2654, 3620, 7285, 13, 883, 11, 2597, 11, 570, 264, 7285, 307, 760, 510, 11, 597, 307, 2138, 406, 437, 321, 528, 13, 400, 264, 7285, 16235, 576, 312, 510, 11, 597, 307, 2138, 406, 437, 321, 528, 13], "temperature": 0.0, "avg_logprob": -0.16890660454245174, "compression_ratio": 1.7378048780487805, "no_speech_prob": 1.9524444724083878e-05}, {"id": 374, "seek": 267700, "start": 2689.0, "end": 2704.0, "text": " I'm not saying it's possible it's totally possible but the learning rate finder.", "tokens": [286, 478, 406, 1566, 309, 311, 1944, 309, 311, 3879, 1944, 457, 264, 2539, 3314, 915, 260, 13], "temperature": 0.0, "avg_logprob": -0.16890660454245174, "compression_ratio": 1.7378048780487805, "no_speech_prob": 1.9524444724083878e-05}, {"id": 375, "seek": 270400, "start": 2704.0, "end": 2715.0, "text": " So Zach Miller actually spent a lot of time trying different things and wrote a whole blog post for his company and came up with four different approaches, all of which actually don't work too bad.", "tokens": [407, 21028, 16932, 767, 4418, 257, 688, 295, 565, 1382, 819, 721, 293, 4114, 257, 1379, 6968, 2183, 337, 702, 2237, 293, 1361, 493, 365, 1451, 819, 11587, 11, 439, 295, 597, 767, 500, 380, 589, 886, 1578, 13], "temperature": 0.0, "avg_logprob": -0.12242032479548799, "compression_ratio": 1.5204081632653061, "no_speech_prob": 1.568786865391303e-05}, {"id": 376, "seek": 270400, "start": 2715.0, "end": 2722.0, "text": " And you can actually look at all of them by saying what suggestion functions do you want to use.", "tokens": [400, 291, 393, 767, 574, 412, 439, 295, 552, 538, 1566, 437, 16541, 6828, 360, 291, 528, 281, 764, 13], "temperature": 0.0, "avg_logprob": -0.12242032479548799, "compression_ratio": 1.5204081632653061, "no_speech_prob": 1.568786865391303e-05}, {"id": 377, "seek": 270400, "start": 2722.0, "end": 2731.0, "text": " Oh,", "tokens": [876, 11], "temperature": 0.0, "avg_logprob": -0.12242032479548799, "compression_ratio": 1.5204081632653061, "no_speech_prob": 1.568786865391303e-05}, {"id": 378, "seek": 273100, "start": 2731.0, "end": 2741.0, "text": " and I wonder if there's", "tokens": [293, 286, 2441, 498, 456, 311], "temperature": 0.0, "avg_logprob": -0.24796547889709472, "compression_ratio": 0.7419354838709677, "no_speech_prob": 5.3864547226112336e-05}, {"id": 379, "seek": 274100, "start": 2741.0, "end": 2763.0, "text": " a link to Zach's blog post in the docs, because that would be quite helpful.", "tokens": [257, 2113, 281, 21028, 311, 6968, 2183, 294, 264, 45623, 11, 570, 300, 576, 312, 1596, 4961, 13], "temperature": 0.0, "avg_logprob": -0.21082511815157803, "compression_ratio": 1.027027027027027, "no_speech_prob": 7.070721494528698e-06}, {"id": 380, "seek": 276300, "start": 2763.0, "end": 2772.0, "text": " Yeah, so you can see minimum. It's actually one of the suggestion functions but I don't actually know why it's even there because you'd never use it.", "tokens": [865, 11, 370, 291, 393, 536, 7285, 13, 467, 311, 767, 472, 295, 264, 16541, 6828, 457, 286, 500, 380, 767, 458, 983, 309, 311, 754, 456, 570, 291, 1116, 1128, 764, 309, 13], "temperature": 0.0, "avg_logprob": -0.1522018271432796, "compression_ratio": 1.6091954022988506, "no_speech_prob": 3.0413398235396016e-06}, {"id": 381, "seek": 276300, "start": 2772.0, "end": 2784.0, "text": " And, so, minimum will put the plot will be in the minimum but the suggestion value is still like it's like 10, like divided by 10.", "tokens": [400, 11, 370, 11, 7285, 486, 829, 264, 7542, 486, 312, 294, 264, 7285, 457, 264, 16541, 2158, 307, 920, 411, 309, 311, 411, 1266, 11, 411, 6666, 538, 1266, 13], "temperature": 0.0, "avg_logprob": -0.1522018271432796, "compression_ratio": 1.6091954022988506, "no_speech_prob": 3.0413398235396016e-06}, {"id": 382, "seek": 278400, "start": 2784.0, "end": 2800.0, "text": " Oh, is that what happens. Yeah, so it finds the minimum divided by 10. I got it. So we should probably plot that then on the minimum rather than wanting what's effectively 10 times that day. Thanks for explaining.", "tokens": [876, 11, 307, 300, 437, 2314, 13, 865, 11, 370, 309, 10704, 264, 7285, 6666, 538, 1266, 13, 286, 658, 309, 13, 407, 321, 820, 1391, 7542, 300, 550, 322, 264, 7285, 2831, 813, 7935, 437, 311, 8659, 1266, 1413, 300, 786, 13, 2561, 337, 13468, 13], "temperature": 0.0, "avg_logprob": -0.1747771786971831, "compression_ratio": 1.5051546391752577, "no_speech_prob": 4.356475528766168e-06}, {"id": 383, "seek": 278400, "start": 2800.0, "end": 2808.0, "text": " Yeah, so you can see all these numbers are all in the same order of magnitude.", "tokens": [865, 11, 370, 291, 393, 536, 439, 613, 3547, 366, 439, 294, 264, 912, 1668, 295, 15668, 13], "temperature": 0.0, "avg_logprob": -0.1747771786971831, "compression_ratio": 1.5051546391752577, "no_speech_prob": 4.356475528766168e-06}, {"id": 384, "seek": 280800, "start": 2808.0, "end": 2817.0, "text": " And", "tokens": [400], "temperature": 0.0, "avg_logprob": -0.21773679439838117, "compression_ratio": 0.7777777777777778, "no_speech_prob": 1.4060308785701636e-05}, {"id": 385, "seek": 280800, "start": 2817.0, "end": 2821.0, "text": " default is point or two.", "tokens": [7576, 307, 935, 420, 732, 13], "temperature": 0.0, "avg_logprob": -0.21773679439838117, "compression_ratio": 0.7777777777777778, "no_speech_prob": 1.4060308785701636e-05}, {"id": 386, "seek": 282100, "start": 2821.0, "end": 2839.0, "text": " Is that something that fast I use these underneath the hood, or I mean like what's the benefit of changing this learning create manually, or trying to find some, so most of the time our default works perfectly fine, which is why I", "tokens": [1119, 300, 746, 300, 2370, 286, 764, 613, 7223, 264, 13376, 11, 420, 286, 914, 411, 437, 311, 264, 5121, 295, 4473, 341, 2539, 1884, 16945, 11, 420, 1382, 281, 915, 512, 11, 370, 881, 295, 264, 565, 527, 7576, 1985, 6239, 2489, 11, 597, 307, 983, 286], "temperature": 0.0, "avg_logprob": -0.2958949896005484, "compression_ratio": 1.4935064935064934, "no_speech_prob": 4.494062068260973e-06}, {"id": 387, "seek": 283900, "start": 2839.0, "end": 2852.0, "text": " don't talk about this as much as I used to actually. But you know sometimes some data will particularly like for a tabular data set the learning rate can be almost anything.", "tokens": [500, 380, 751, 466, 341, 382, 709, 382, 286, 1143, 281, 767, 13, 583, 291, 458, 2171, 512, 1412, 486, 4098, 411, 337, 257, 4421, 1040, 1412, 992, 264, 2539, 3314, 393, 312, 1920, 1340, 13], "temperature": 0.0, "avg_logprob": -0.129935714433778, "compression_ratio": 1.4350649350649352, "no_speech_prob": 3.1380127438751515e-06}, {"id": 388, "seek": 283900, "start": 2852.0, "end": 2859.0, "text": " It really does depend on the model. I find most", "tokens": [467, 534, 775, 5672, 322, 264, 2316, 13, 286, 915, 881], "temperature": 0.0, "avg_logprob": -0.129935714433778, "compression_ratio": 1.4350649350649352, "no_speech_prob": 3.1380127438751515e-06}, {"id": 389, "seek": 285900, "start": 2859.0, "end": 2869.0, "text": " data division models seem to have pretty similar learning rates that are useful so the defaults generally work pretty well. But yeah if you try something and it doesn't seem to be training quickly well.", "tokens": [1412, 10044, 5245, 1643, 281, 362, 1238, 2531, 2539, 6846, 300, 366, 4420, 370, 264, 7576, 82, 5101, 589, 1238, 731, 13, 583, 1338, 498, 291, 853, 746, 293, 309, 1177, 380, 1643, 281, 312, 3097, 2661, 731, 13], "temperature": 0.0, "avg_logprob": -0.15002843166919463, "compression_ratio": 1.7208333333333334, "no_speech_prob": 5.954747393843718e-06}, {"id": 390, "seek": 285900, "start": 2869.0, "end": 2879.0, "text": " Just try running the, you know, the first thing I try would be try running LR find just in case the default learning rates, nowhere near the recommended values.", "tokens": [1449, 853, 2614, 264, 11, 291, 458, 11, 264, 700, 551, 286, 853, 576, 312, 853, 2614, 441, 49, 915, 445, 294, 1389, 264, 7576, 2539, 6846, 11, 11159, 2651, 264, 9628, 4190, 13], "temperature": 0.0, "avg_logprob": -0.15002843166919463, "compression_ratio": 1.7208333333333334, "no_speech_prob": 5.954747393843718e-06}, {"id": 391, "seek": 285900, "start": 2879.0, "end": 2883.0, "text": " And then you could try. Yeah, you could just try.", "tokens": [400, 550, 291, 727, 853, 13, 865, 11, 291, 727, 445, 853, 13], "temperature": 0.0, "avg_logprob": -0.15002843166919463, "compression_ratio": 1.7208333333333334, "no_speech_prob": 5.954747393843718e-06}, {"id": 392, "seek": 288300, "start": 2883.0, "end": 2895.0, "text": " But yeah, these are all very close to route throughout default anyway, so I wouldn't bother in this particular case.", "tokens": [583, 1338, 11, 613, 366, 439, 588, 1998, 281, 7955, 3710, 7576, 4033, 11, 370, 286, 2759, 380, 8677, 294, 341, 1729, 1389, 13], "temperature": 0.0, "avg_logprob": -0.10809509475509842, "compression_ratio": 1.429951690821256, "no_speech_prob": 1.4735720469616354e-05}, {"id": 393, "seek": 288300, "start": 2895.0, "end": 2898.0, "text": " Thank you. Yeah, no worries.", "tokens": [1044, 291, 13, 865, 11, 572, 16340, 13], "temperature": 0.0, "avg_logprob": -0.10809509475509842, "compression_ratio": 1.429951690821256, "no_speech_prob": 1.4735720469616354e-05}, {"id": 394, "seek": 288300, "start": 2898.0, "end": 2902.0, "text": " These are questions.", "tokens": [1981, 366, 1651, 13], "temperature": 0.0, "avg_logprob": -0.10809509475509842, "compression_ratio": 1.429951690821256, "no_speech_prob": 1.4735720469616354e-05}, {"id": 395, "seek": 288300, "start": 2902.0, "end": 2912.0, "text": " Okay, we've got a model. I'm actually going to have to try it again because I just created a new learner for the purpose of that.", "tokens": [1033, 11, 321, 600, 658, 257, 2316, 13, 286, 478, 767, 516, 281, 362, 281, 853, 309, 797, 570, 286, 445, 2942, 257, 777, 33347, 337, 264, 4334, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.10809509475509842, "compression_ratio": 1.429951690821256, "no_speech_prob": 1.4735720469616354e-05}, {"id": 396, "seek": 291200, "start": 2912.0, "end": 2926.0, "text": " And so the next thing we're going to have to do is to apply it to our training set, sorry to our to our test set in order to submit to Kaggle.", "tokens": [400, 370, 264, 958, 551, 321, 434, 516, 281, 362, 281, 360, 307, 281, 3079, 309, 281, 527, 3097, 992, 11, 2597, 281, 527, 281, 527, 1500, 992, 294, 1668, 281, 10315, 281, 48751, 22631, 13], "temperature": 0.0, "avg_logprob": -0.1439665342632093, "compression_ratio": 1.8271028037383177, "no_speech_prob": 1.952407183125615e-05}, {"id": 397, "seek": 291200, "start": 2926.0, "end": 2940.0, "text": " So the test set it's always good to have two windows, two tabs going on because that way we can start working on the next thing while this is training right and you can see it's still training because the little hourglass icon is there in the icon.", "tokens": [407, 264, 1500, 992, 309, 311, 1009, 665, 281, 362, 732, 9309, 11, 732, 20743, 516, 322, 570, 300, 636, 321, 393, 722, 1364, 322, 264, 958, 551, 1339, 341, 307, 3097, 558, 293, 291, 393, 536, 309, 311, 920, 3097, 570, 264, 707, 1773, 28851, 6528, 307, 456, 294, 264, 6528, 13], "temperature": 0.0, "avg_logprob": -0.1439665342632093, "compression_ratio": 1.8271028037383177, "no_speech_prob": 1.952407183125615e-05}, {"id": 398, "seek": 294000, "start": 2940.0, "end": 2946.0, "text": " So there's something called test underscore.", "tokens": [407, 456, 311, 746, 1219, 1500, 37556, 13], "temperature": 0.0, "avg_logprob": -0.244043723396633, "compression_ratio": 1.2926829268292683, "no_speech_prob": 1.2410046110744588e-05}, {"id": 399, "seek": 294000, "start": 2946.0, "end": 2953.0, "text": " DL, which for some reason is not appearing. It must be from some different part of the library.", "tokens": [413, 43, 11, 597, 337, 512, 1778, 307, 406, 19870, 13, 467, 1633, 312, 490, 512, 819, 644, 295, 264, 6405, 13], "temperature": 0.0, "avg_logprob": -0.244043723396633, "compression_ratio": 1.2926829268292683, "no_speech_prob": 1.2410046110744588e-05}, {"id": 400, "seek": 294000, "start": 2953.0, "end": 2956.0, "text": " I test.", "tokens": [286, 1500, 13], "temperature": 0.0, "avg_logprob": -0.244043723396633, "compression_ratio": 1.2926829268292683, "no_speech_prob": 1.2410046110744588e-05}, {"id": 401, "seek": 294000, "start": 2956.0, "end": 2962.0, "text": " Data core.", "tokens": [11888, 4965, 13], "temperature": 0.0, "avg_logprob": -0.244043723396633, "compression_ratio": 1.2926829268292683, "no_speech_prob": 1.2410046110744588e-05}, {"id": 402, "seek": 296200, "start": 2962.0, "end": 2972.0, "text": " Fastai.data.or import star.", "tokens": [15968, 1301, 13, 67, 3274, 13, 284, 974, 3543, 13], "temperature": 0.0, "avg_logprob": -0.4155705346001519, "compression_ratio": 1.0348837209302326, "no_speech_prob": 4.330604133429006e-05}, {"id": 403, "seek": 296200, "start": 2972.0, "end": 2978.0, "text": " Something silly.", "tokens": [6595, 11774, 13], "temperature": 0.0, "avg_logprob": -0.4155705346001519, "compression_ratio": 1.0348837209302326, "no_speech_prob": 4.330604133429006e-05}, {"id": 404, "seek": 296200, "start": 2978.0, "end": 2980.0, "text": " Oh, it's a method. Okay.", "tokens": [876, 11, 309, 311, 257, 3170, 13, 1033, 13], "temperature": 0.0, "avg_logprob": -0.4155705346001519, "compression_ratio": 1.0348837209302326, "no_speech_prob": 4.330604133429006e-05}, {"id": 405, "seek": 296200, "start": 2980.0, "end": 2982.0, "text": " My bad.", "tokens": [1222, 1578, 13], "temperature": 0.0, "avg_logprob": -0.4155705346001519, "compression_ratio": 1.0348837209302326, "no_speech_prob": 4.330604133429006e-05}, {"id": 406, "seek": 296200, "start": 2982.0, "end": 2986.0, "text": " DLs.testDL.", "tokens": [413, 43, 82, 13, 31636, 35, 43, 13], "temperature": 0.0, "avg_logprob": -0.4155705346001519, "compression_ratio": 1.0348837209302326, "no_speech_prob": 4.330604133429006e-05}, {"id": 407, "seek": 298600, "start": 2986.0, "end": 2999.0, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.34393322467803955, "compression_ratio": 0.38461538461538464, "no_speech_prob": 8.939124199969228e-06}, {"id": 408, "seek": 299900, "start": 2999.0, "end": 3016.0, "text": " All right, so this creates a test data loader. So a test data loader is a data loader used for inference of a bunch of things at once, basically. So this should be an example down here. Okay, so test DL is something that we pass some items to.", "tokens": [1057, 558, 11, 370, 341, 7829, 257, 1500, 1412, 3677, 260, 13, 407, 257, 1500, 1412, 3677, 260, 307, 257, 1412, 3677, 260, 1143, 337, 38253, 295, 257, 3840, 295, 721, 412, 1564, 11, 1936, 13, 407, 341, 820, 312, 364, 1365, 760, 510, 13, 1033, 11, 370, 1500, 413, 43, 307, 746, 300, 321, 1320, 512, 4754, 281, 13], "temperature": 0.0, "avg_logprob": -0.14082841575145721, "compression_ratio": 1.51875, "no_speech_prob": 5.014463567931671e-06}, {"id": 409, "seek": 301600, "start": 3016.0, "end": 3036.0, "text": " So I don't know like Radact, Nishik, anybody else? You know, I'm thinking I'm just gonna call get image files on the test set and pass it to test DL. Is that what you guys would do or you have a better way to do this?", "tokens": [407, 286, 500, 380, 458, 411, 9654, 578, 11, 426, 742, 1035, 11, 4472, 1646, 30, 509, 458, 11, 286, 478, 1953, 286, 478, 445, 799, 818, 483, 3256, 7098, 322, 264, 1500, 992, 293, 1320, 309, 281, 1500, 413, 43, 13, 1119, 300, 437, 291, 1074, 576, 360, 420, 291, 362, 257, 1101, 636, 281, 360, 341, 30], "temperature": 0.0, "avg_logprob": -0.16327237429684155, "compression_ratio": 1.398876404494382, "no_speech_prob": 8.266612894658465e-06}, {"id": 410, "seek": 301600, "start": 3036.0, "end": 3039.0, "text": " Yeah, I think that should work.", "tokens": [865, 11, 286, 519, 300, 820, 589, 13], "temperature": 0.0, "avg_logprob": -0.16327237429684155, "compression_ratio": 1.398876404494382, "no_speech_prob": 8.266612894658465e-06}, {"id": 411, "seek": 303900, "start": 3039.0, "end": 3053.0, "text": " Okay. I don't know like what people, you know, I don't do nearly as much inference stuff as most people, so I never quite know what the fast AI communities preferred idiomatic approaches.", "tokens": [1033, 13, 286, 500, 380, 458, 411, 437, 561, 11, 291, 458, 11, 286, 500, 380, 360, 6217, 382, 709, 38253, 1507, 382, 881, 561, 11, 370, 286, 1128, 1596, 458, 437, 264, 2370, 7318, 4456, 16494, 18014, 13143, 11587, 13], "temperature": 0.0, "avg_logprob": -0.14862268765767414, "compression_ratio": 1.530612244897959, "no_speech_prob": 4.860017725150101e-06}, {"id": 412, "seek": 303900, "start": 3053.0, "end": 3058.0, "text": " Test images.", "tokens": [9279, 5267, 13], "temperature": 0.0, "avg_logprob": -0.14862268765767414, "compression_ratio": 1.530612244897959, "no_speech_prob": 4.860017725150101e-06}, {"id": 413, "seek": 303900, "start": 3058.0, "end": 3061.0, "text": " Test images.", "tokens": [9279, 5267, 13], "temperature": 0.0, "avg_logprob": -0.14862268765767414, "compression_ratio": 1.530612244897959, "no_speech_prob": 4.860017725150101e-06}, {"id": 414, "seek": 303900, "start": 3061.0, "end": 3063.0, "text": " Test files.", "tokens": [9279, 7098, 13], "temperature": 0.0, "avg_logprob": -0.14862268765767414, "compression_ratio": 1.530612244897959, "no_speech_prob": 4.860017725150101e-06}, {"id": 415, "seek": 306300, "start": 3063.0, "end": 3071.0, "text": " Okay, so we've got 3,460 unknown files to apply this to. So,", "tokens": [1033, 11, 370, 321, 600, 658, 805, 11, 19, 4550, 9841, 7098, 281, 3079, 341, 281, 13, 407, 11], "temperature": 0.0, "avg_logprob": -0.13285991880628797, "compression_ratio": 1.3763440860215055, "no_speech_prob": 9.223042980011087e-06}, {"id": 416, "seek": 306300, "start": 3071.0, "end": 3075.0, "text": " we could create a test data loader,", "tokens": [321, 727, 1884, 257, 1500, 1412, 3677, 260, 11], "temperature": 0.0, "avg_logprob": -0.13285991880628797, "compression_ratio": 1.3763440860215055, "no_speech_prob": 9.223042980011087e-06}, {"id": 417, "seek": 306300, "start": 3075.0, "end": 3079.0, "text": " and that's going to be DLs.testDL", "tokens": [293, 300, 311, 516, 281, 312, 413, 43, 82, 13, 31636, 35, 43], "temperature": 0.0, "avg_logprob": -0.13285991880628797, "compression_ratio": 1.3763440860215055, "no_speech_prob": 9.223042980011087e-06}, {"id": 418, "seek": 306300, "start": 3079.0, "end": 3083.0, "text": " with those files, I guess.", "tokens": [365, 729, 7098, 11, 286, 2041, 13], "temperature": 0.0, "avg_logprob": -0.13285991880628797, "compression_ratio": 1.3763440860215055, "no_speech_prob": 9.223042980011087e-06}, {"id": 419, "seek": 306300, "start": 3083.0, "end": 3091.0, "text": " And we should be able to go testDL.showBatch. So I do always like to see what I'm doing, you know.", "tokens": [400, 321, 820, 312, 1075, 281, 352, 1500, 35, 43, 13, 34436, 33, 852, 13, 407, 286, 360, 1009, 411, 281, 536, 437, 286, 478, 884, 11, 291, 458, 13], "temperature": 0.0, "avg_logprob": -0.13285991880628797, "compression_ratio": 1.3763440860215055, "no_speech_prob": 9.223042980011087e-06}, {"id": 420, "seek": 309100, "start": 3091.0, "end": 3100.0, "text": " So that looks hopeful. And so a test data loader, the key difference is that it doesn't have labels.", "tokens": [407, 300, 1542, 20531, 13, 400, 370, 257, 1500, 1412, 3677, 260, 11, 264, 2141, 2649, 307, 300, 309, 1177, 380, 362, 16949, 13], "temperature": 0.0, "avg_logprob": -0.17562750975290933, "compression_ratio": 1.4, "no_speech_prob": 7.411037131532794e-06}, {"id": 421, "seek": 309100, "start": 3100.0, "end": 3105.0, "text": " Right, so there's no dependent variable.", "tokens": [1779, 11, 370, 456, 311, 572, 12334, 7006, 13], "temperature": 0.0, "avg_logprob": -0.17562750975290933, "compression_ratio": 1.4, "no_speech_prob": 7.411037131532794e-06}, {"id": 422, "seek": 309100, "start": 3105.0, "end": 3109.0, "text": " All right, so then I guess we would go", "tokens": [1057, 558, 11, 370, 550, 286, 2041, 321, 576, 352], "temperature": 0.0, "avg_logprob": -0.17562750975290933, "compression_ratio": 1.4, "no_speech_prob": 7.411037131532794e-06}, {"id": 423, "seek": 309100, "start": 3109.0, "end": 3115.0, "text": " learn. Is it.getPreds or.predict? I never quite remember.", "tokens": [1466, 13, 1119, 309, 2411, 847, 47, 986, 82, 420, 2411, 79, 24945, 30, 286, 1128, 1596, 1604, 13], "temperature": 0.0, "avg_logprob": -0.17562750975290933, "compression_ratio": 1.4, "no_speech_prob": 7.411037131532794e-06}, {"id": 424, "seek": 311500, "start": 3115.0, "end": 3123.0, "text": " That's an item, so I guess it's.getPreds. We need better names for these.", "tokens": [663, 311, 364, 3174, 11, 370, 286, 2041, 309, 311, 2411, 847, 47, 986, 82, 13, 492, 643, 1101, 5288, 337, 613, 13], "temperature": 0.0, "avg_logprob": -0.11784586494351611, "compression_ratio": 1.4421052631578948, "no_speech_prob": 1.2411028365022503e-05}, {"id": 425, "seek": 311500, "start": 3123.0, "end": 3131.0, "text": " Okay, and then DL data loader equals the test data loader.", "tokens": [1033, 11, 293, 550, 413, 43, 1412, 3677, 260, 6915, 264, 1500, 1412, 3677, 260, 13], "temperature": 0.0, "avg_logprob": -0.11784586494351611, "compression_ratio": 1.4421052631578948, "no_speech_prob": 1.2411028365022503e-05}, {"id": 426, "seek": 311500, "start": 3131.0, "end": 3137.0, "text": " Oh, and I should have assigned that to something, obviously. That was a bit silly of me.", "tokens": [876, 11, 293, 286, 820, 362, 13279, 300, 281, 746, 11, 2745, 13, 663, 390, 257, 857, 11774, 295, 385, 13], "temperature": 0.0, "avg_logprob": -0.11784586494351611, "compression_ratio": 1.4421052631578948, "no_speech_prob": 1.2411028365022503e-05}, {"id": 427, "seek": 311500, "start": 3137.0, "end": 3142.0, "text": " And also we should look at the documentation for it.", "tokens": [400, 611, 321, 820, 574, 412, 264, 14333, 337, 309, 13], "temperature": 0.0, "avg_logprob": -0.11784586494351611, "compression_ratio": 1.4421052631578948, "no_speech_prob": 1.2411028365022503e-05}, {"id": 428, "seek": 314200, "start": 3142.0, "end": 3152.0, "text": " Do not use to map keyboard shortcuts.", "tokens": [1144, 406, 764, 281, 4471, 10186, 34620, 13], "temperature": 0.0, "avg_logprob": -0.16903532028198243, "compression_ratio": 1.4765625, "no_speech_prob": 1.2804616744688246e-05}, {"id": 429, "seek": 314200, "start": 3152.0, "end": 3156.0, "text": " Get the predictions with some particular data loader.", "tokens": [3240, 264, 21264, 365, 512, 1729, 1412, 3677, 260, 13], "temperature": 0.0, "avg_logprob": -0.16903532028198243, "compression_ratio": 1.4765625, "no_speech_prob": 1.2804616744688246e-05}, {"id": 430, "seek": 314200, "start": 3156.0, "end": 3166.0, "text": " It can optionally return the input. It can optionally return the loss. We don't need any of that.", "tokens": [467, 393, 3614, 379, 2736, 264, 4846, 13, 467, 393, 3614, 379, 2736, 264, 4470, 13, 492, 500, 380, 643, 604, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.16903532028198243, "compression_ratio": 1.4765625, "no_speech_prob": 1.2804616744688246e-05}, {"id": 431, "seek": 316600, "start": 3166.0, "end": 3176.0, "text": " Okay, so what I think we should do is we should look at Kaggle at this point. And actually we don't even need to look at Kaggle. What Kaggle normally does is they provide us with a sample submission.", "tokens": [1033, 11, 370, 437, 286, 519, 321, 820, 360, 307, 321, 820, 574, 412, 48751, 22631, 412, 341, 935, 13, 400, 767, 321, 500, 380, 754, 643, 281, 574, 412, 48751, 22631, 13, 708, 48751, 22631, 5646, 775, 307, 436, 2893, 505, 365, 257, 6889, 23689, 13], "temperature": 0.0, "avg_logprob": -0.11413980413366247, "compression_ratio": 1.665289256198347, "no_speech_prob": 1.0952733646263368e-05}, {"id": 432, "seek": 316600, "start": 3176.0, "end": 3184.0, "text": " Here's one here, right? So let's look at the submission. Sample submission equals pd.readcsv.", "tokens": [1692, 311, 472, 510, 11, 558, 30, 407, 718, 311, 574, 412, 264, 23689, 13, 4832, 781, 23689, 6915, 280, 67, 13, 2538, 14368, 85, 13], "temperature": 0.0, "avg_logprob": -0.11413980413366247, "compression_ratio": 1.665289256198347, "no_speech_prob": 1.0952733646263368e-05}, {"id": 433, "seek": 316600, "start": 3184.0, "end": 3195.0, "text": " And notice in Jupyter, if you start quotes and you press tab, it will tab complete file names, which is nice.", "tokens": [400, 3449, 294, 22125, 88, 391, 11, 498, 291, 722, 19963, 293, 291, 1886, 4421, 11, 309, 486, 4421, 3566, 3991, 5288, 11, 597, 307, 1481, 13], "temperature": 0.0, "avg_logprob": -0.11413980413366247, "compression_ratio": 1.665289256198347, "no_speech_prob": 1.0952733646263368e-05}, {"id": 434, "seek": 319500, "start": 3195.0, "end": 3197.0, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.10626886082791734, "compression_ratio": 1.7313432835820894, "no_speech_prob": 9.312101610703394e-05}, {"id": 435, "seek": 319500, "start": 3197.0, "end": 3201.0, "text": " Not a very useful sample submission.", "tokens": [1726, 257, 588, 4420, 6889, 23689, 13], "temperature": 0.0, "avg_logprob": -0.10626886082791734, "compression_ratio": 1.7313432835820894, "no_speech_prob": 9.312101610703394e-05}, {"id": 436, "seek": 319500, "start": 3201.0, "end": 3204.0, "text": " Unless there's something wrong with Python.", "tokens": [16581, 456, 311, 746, 2085, 365, 15329, 13], "temperature": 0.0, "avg_logprob": -0.10626886082791734, "compression_ratio": 1.7313432835820894, "no_speech_prob": 9.312101610703394e-05}, {"id": 437, "seek": 319500, "start": 3204.0, "end": 3210.0, "text": " This is not a great sample submission, but they just want the name of the class.", "tokens": [639, 307, 406, 257, 869, 6889, 23689, 11, 457, 436, 445, 528, 264, 1315, 295, 264, 1508, 13], "temperature": 0.0, "avg_logprob": -0.10626886082791734, "compression_ratio": 1.7313432835820894, "no_speech_prob": 9.312101610703394e-05}, {"id": 438, "seek": 319500, "start": 3210.0, "end": 3218.0, "text": " I see. What a terrible sample submission, particularly for a training one, you would think they would be more helpful. So they just want the text of the name of the class, do they?", "tokens": [286, 536, 13, 708, 257, 6237, 6889, 23689, 11, 4098, 337, 257, 3097, 472, 11, 291, 576, 519, 436, 576, 312, 544, 4961, 13, 407, 436, 445, 528, 264, 2487, 295, 264, 1315, 295, 264, 1508, 11, 360, 436, 30], "temperature": 0.0, "avg_logprob": -0.10626886082791734, "compression_ratio": 1.7313432835820894, "no_speech_prob": 9.312101610703394e-05}, {"id": 439, "seek": 321800, "start": 3218.0, "end": 3229.0, "text": " Yes. I mean, obviously, we could actually look it up and find out rather than guessing when you don't have Radik on the line to show you the answer.", "tokens": [1079, 13, 286, 914, 11, 2745, 11, 321, 727, 767, 574, 309, 493, 293, 915, 484, 2831, 813, 17939, 562, 291, 500, 380, 362, 9654, 1035, 322, 264, 1622, 281, 855, 291, 264, 1867, 13], "temperature": 0.0, "avg_logprob": -0.16456379388508044, "compression_ratio": 1.4324324324324325, "no_speech_prob": 3.237538294342812e-06}, {"id": 440, "seek": 321800, "start": 3229.0, "end": 3233.0, "text": " Or maybe you can just call Radik and ask him.", "tokens": [1610, 1310, 291, 393, 445, 818, 9654, 1035, 293, 1029, 796, 13], "temperature": 0.0, "avg_logprob": -0.16456379388508044, "compression_ratio": 1.4324324324324325, "no_speech_prob": 3.237538294342812e-06}, {"id": 441, "seek": 321800, "start": 3233.0, "end": 3237.0, "text": " So data evaluation. Yes.", "tokens": [407, 1412, 13344, 13, 1079, 13], "temperature": 0.0, "avg_logprob": -0.16456379388508044, "compression_ratio": 1.4324324324324325, "no_speech_prob": 3.237538294342812e-06}, {"id": 442, "seek": 321800, "start": 3237.0, "end": 3240.0, "text": " See they've actually got a sample here.", "tokens": [3008, 436, 600, 767, 658, 257, 6889, 510, 13], "temperature": 0.0, "avg_logprob": -0.16456379388508044, "compression_ratio": 1.4324324324324325, "no_speech_prob": 3.237538294342812e-06}, {"id": 443, "seek": 321800, "start": 3240.0, "end": 3241.0, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.16456379388508044, "compression_ratio": 1.4324324324324325, "no_speech_prob": 3.237538294342812e-06}, {"id": 444, "seek": 324100, "start": 3241.0, "end": 3248.0, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.19213345845540364, "compression_ratio": 1.2698412698412698, "no_speech_prob": 1.933280373123125e-06}, {"id": 445, "seek": 324100, "start": 3248.0, "end": 3252.0, "text": " Let's try that.", "tokens": [961, 311, 853, 300, 13], "temperature": 0.0, "avg_logprob": -0.19213345845540364, "compression_ratio": 1.2698412698412698, "no_speech_prob": 1.933280373123125e-06}, {"id": 446, "seek": 324100, "start": 3252.0, "end": 3263.0, "text": " So preds equals and so by default it's going to return the probability of every class, which we can certainly turn into what we want.", "tokens": [407, 3852, 82, 6915, 293, 370, 538, 7576, 309, 311, 516, 281, 2736, 264, 8482, 295, 633, 1508, 11, 597, 321, 393, 3297, 1261, 666, 437, 321, 528, 13], "temperature": 0.0, "avg_logprob": -0.19213345845540364, "compression_ratio": 1.2698412698412698, "no_speech_prob": 1.933280373123125e-06}, {"id": 447, "seek": 326300, "start": 3263.0, "end": 3274.0, "text": " I think if we call with decoded, that will do it for us. Does that sound right?", "tokens": [286, 519, 498, 321, 818, 365, 979, 12340, 11, 300, 486, 360, 309, 337, 505, 13, 4402, 300, 1626, 558, 30], "temperature": 0.0, "avg_logprob": -0.12777542800046085, "compression_ratio": 1.52, "no_speech_prob": 1.9033520857192343e-06}, {"id": 448, "seek": 326300, "start": 3274.0, "end": 3283.0, "text": " See, I'm so out of practice with this. Okay. It's pretty close, right? It's given us the indexes of each one.", "tokens": [3008, 11, 286, 478, 370, 484, 295, 3124, 365, 341, 13, 1033, 13, 467, 311, 1238, 1998, 11, 558, 30, 467, 311, 2212, 505, 264, 8186, 279, 295, 1184, 472, 13], "temperature": 0.0, "avg_logprob": -0.12777542800046085, "compression_ratio": 1.52, "no_speech_prob": 1.9033520857192343e-06}, {"id": 449, "seek": 326300, "start": 3283.0, "end": 3287.0, "text": " So this is actually going to be a really good exercise.", "tokens": [407, 341, 307, 767, 516, 281, 312, 257, 534, 665, 5380, 13], "temperature": 0.0, "avg_logprob": -0.12777542800046085, "compression_ratio": 1.52, "no_speech_prob": 1.9033520857192343e-06}, {"id": 450, "seek": 326300, "start": 3287.0, "end": 3290.0, "text": " So in terms of like what's in there, there's three things.", "tokens": [407, 294, 2115, 295, 411, 437, 311, 294, 456, 11, 456, 311, 1045, 721, 13], "temperature": 0.0, "avg_logprob": -0.12777542800046085, "compression_ratio": 1.52, "no_speech_prob": 1.9033520857192343e-06}, {"id": 451, "seek": 329000, "start": 3290.0, "end": 3294.0, "text": " There's the probabilities. There's something we don't care about.", "tokens": [821, 311, 264, 33783, 13, 821, 311, 746, 321, 500, 380, 1127, 466, 13], "temperature": 0.0, "avg_logprob": -0.08619670867919922, "compression_ratio": 1.6573426573426573, "no_speech_prob": 8.267668818007223e-06}, {"id": 452, "seek": 329000, "start": 3294.0, "end": 3300.0, "text": " And there's the indexes.", "tokens": [400, 456, 311, 264, 8186, 279, 13], "temperature": 0.0, "avg_logprob": -0.08619670867919922, "compression_ratio": 1.6573426573426573, "no_speech_prob": 8.267668818007223e-06}, {"id": 453, "seek": 329000, "start": 3300.0, "end": 3307.0, "text": " So these are the indexes. So what are these indexes of? They're indexes into the vocab.", "tokens": [407, 613, 366, 264, 8186, 279, 13, 407, 437, 366, 613, 8186, 279, 295, 30, 814, 434, 8186, 279, 666, 264, 2329, 455, 13], "temperature": 0.0, "avg_logprob": -0.08619670867919922, "compression_ratio": 1.6573426573426573, "no_speech_prob": 8.267668818007223e-06}, {"id": 454, "seek": 329000, "start": 3307.0, "end": 3315.0, "text": " So if you remember the vocab is the tells you what's what.", "tokens": [407, 498, 291, 1604, 264, 2329, 455, 307, 264, 5112, 291, 437, 311, 437, 13], "temperature": 0.0, "avg_logprob": -0.08619670867919922, "compression_ratio": 1.6573426573426573, "no_speech_prob": 8.267668818007223e-06}, {"id": 455, "seek": 331500, "start": 3315.0, "end": 3327.0, "text": " So we need to convert these predictions into these pieces of these strings.", "tokens": [407, 321, 643, 281, 7620, 613, 21264, 666, 613, 3755, 295, 613, 13985, 13], "temperature": 0.0, "avg_logprob": -0.09180252324967157, "compression_ratio": 1.396694214876033, "no_speech_prob": 3.844706498057349e-06}, {"id": 456, "seek": 331500, "start": 3327.0, "end": 3338.0, "text": " So the first thing I'd probably be inclined to do is to maybe turn that into a pandas series.", "tokens": [407, 264, 700, 551, 286, 1116, 1391, 312, 28173, 281, 360, 307, 281, 1310, 1261, 300, 666, 257, 4565, 296, 2638, 13], "temperature": 0.0, "avg_logprob": -0.09180252324967157, "compression_ratio": 1.396694214876033, "no_speech_prob": 3.844706498057349e-06}, {"id": 457, "seek": 333800, "start": 3338.0, "end": 3351.0, "text": " And so I guess we should.", "tokens": [400, 370, 286, 2041, 321, 820, 13], "temperature": 0.0, "avg_logprob": -0.22782819411333868, "compression_ratio": 1.0224719101123596, "no_speech_prob": 3.119859684375115e-05}, {"id": 458, "seek": 333800, "start": 3351.0, "end": 3354.0, "text": " Do you give it a name as well?", "tokens": [1144, 291, 976, 309, 257, 1315, 382, 731, 30], "temperature": 0.0, "avg_logprob": -0.22782819411333868, "compression_ratio": 1.0224719101123596, "no_speech_prob": 3.119859684375115e-05}, {"id": 459, "seek": 333800, "start": 3354.0, "end": 3359.0, "text": " Okay. So let's call these indexes.", "tokens": [1033, 13, 407, 718, 311, 818, 613, 8186, 279, 13], "temperature": 0.0, "avg_logprob": -0.22782819411333868, "compression_ratio": 1.0224719101123596, "no_speech_prob": 3.119859684375115e-05}, {"id": 460, "seek": 335900, "start": 3359.0, "end": 3376.0, "text": " Oopsie dozzy.", "tokens": [21726, 414, 360, 89, 1229, 13], "temperature": 0.0, "avg_logprob": -0.36342885277488013, "compression_ratio": 0.8867924528301887, "no_speech_prob": 8.66433128976496e-06}, {"id": 461, "seek": 335900, "start": 3376.0, "end": 3379.0, "text": " Okay. So there's a pandas series.", "tokens": [1033, 13, 407, 456, 311, 257, 4565, 296, 2638, 13], "temperature": 0.0, "avg_logprob": -0.36342885277488013, "compression_ratio": 0.8867924528301887, "no_speech_prob": 8.66433128976496e-06}, {"id": 462, "seek": 337900, "start": 3379.0, "end": 3389.0, "text": " And I always find pandas the pandas API difficult to remember.", "tokens": [400, 286, 1009, 915, 4565, 296, 264, 4565, 296, 9362, 2252, 281, 1604, 13], "temperature": 0.0, "avg_logprob": -0.1245997667312622, "compression_ratio": 1.16, "no_speech_prob": 8.939377948991023e-06}, {"id": 463, "seek": 337900, "start": 3389.0, "end": 3393.0, "text": " I don't find it particularly consistent or intuitive.", "tokens": [286, 500, 380, 915, 309, 4098, 8398, 420, 21769, 13], "temperature": 0.0, "avg_logprob": -0.1245997667312622, "compression_ratio": 1.16, "no_speech_prob": 8.939377948991023e-06}, {"id": 464, "seek": 339300, "start": 3393.0, "end": 3415.0, "text": " But there is a dot map function which I think we can look up in to dl.vocab.", "tokens": [583, 456, 307, 257, 5893, 4471, 2445, 597, 286, 519, 321, 393, 574, 493, 294, 281, 37873, 13, 20836, 455, 13], "temperature": 0.0, "avg_logprob": -0.2396075600071957, "compression_ratio": 1.1666666666666667, "no_speech_prob": 1.7330164610029897e-06}, {"id": 465, "seek": 339300, "start": 3415.0, "end": 3417.0, "text": " Category now is not callable. Fair enough.", "tokens": [383, 48701, 586, 307, 406, 818, 712, 13, 12157, 1547, 13], "temperature": 0.0, "avg_logprob": -0.2396075600071957, "compression_ratio": 1.1666666666666667, "no_speech_prob": 1.7330164610029897e-06}, {"id": 466, "seek": 341700, "start": 3417.0, "end": 3428.0, "text": " So dl.vocab is although it looks like a list.", "tokens": [407, 37873, 13, 20836, 455, 307, 4878, 309, 1542, 411, 257, 1329, 13], "temperature": 0.0, "avg_logprob": -0.11151423305273056, "compression_ratio": 1.4014084507042253, "no_speech_prob": 1.280516789847752e-05}, {"id": 467, "seek": 341700, "start": 3428.0, "end": 3432.0, "text": " I guess it's not one, but we might be able to turn it into a list.", "tokens": [286, 2041, 309, 311, 406, 472, 11, 457, 321, 1062, 312, 1075, 281, 1261, 309, 666, 257, 1329, 13], "temperature": 0.0, "avg_logprob": -0.11151423305273056, "compression_ratio": 1.4014084507042253, "no_speech_prob": 1.280516789847752e-05}, {"id": 468, "seek": 341700, "start": 3432.0, "end": 3436.0, "text": " Normally you can turn things into lists like so. Yes, we can.", "tokens": [17424, 291, 393, 1261, 721, 666, 14511, 411, 370, 13, 1079, 11, 321, 393, 13], "temperature": 0.0, "avg_logprob": -0.11151423305273056, "compression_ratio": 1.4014084507042253, "no_speech_prob": 1.280516789847752e-05}, {"id": 469, "seek": 341700, "start": 3436.0, "end": 3443.0, "text": " Let's see if that works.", "tokens": [961, 311, 536, 498, 300, 1985, 13], "temperature": 0.0, "avg_logprob": -0.11151423305273056, "compression_ratio": 1.4014084507042253, "no_speech_prob": 1.280516789847752e-05}, {"id": 470, "seek": 344300, "start": 3443.0, "end": 3448.0, "text": " Oh, okay. That's annoying.", "tokens": [876, 11, 1392, 13, 663, 311, 11304, 13], "temperature": 0.0, "avg_logprob": -0.11048810105574758, "compression_ratio": 1.5193370165745856, "no_speech_prob": 9.222774679074064e-06}, {"id": 471, "seek": 344300, "start": 3448.0, "end": 3454.0, "text": " So I'm pretty sure that you can pass a dictionary.", "tokens": [407, 286, 478, 1238, 988, 300, 291, 393, 1320, 257, 25890, 13], "temperature": 0.0, "avg_logprob": -0.11048810105574758, "compression_ratio": 1.5193370165745856, "no_speech_prob": 9.222774679074064e-06}, {"id": 472, "seek": 344300, "start": 3454.0, "end": 3456.0, "text": " Yes, you can.", "tokens": [1079, 11, 291, 393, 13], "temperature": 0.0, "avg_logprob": -0.11048810105574758, "compression_ratio": 1.5193370165745856, "no_speech_prob": 9.222774679074064e-06}, {"id": 473, "seek": 344300, "start": 3456.0, "end": 3463.0, "text": " And I thought a list would count as a dictionary, but apparently it doesn't, which is, I mean, mapping.", "tokens": [400, 286, 1194, 257, 1329, 576, 1207, 382, 257, 25890, 11, 457, 7970, 309, 1177, 380, 11, 597, 307, 11, 286, 914, 11, 18350, 13], "temperature": 0.0, "avg_logprob": -0.11048810105574758, "compression_ratio": 1.5193370165745856, "no_speech_prob": 9.222774679074064e-06}, {"id": 474, "seek": 344300, "start": 3463.0, "end": 3467.0, "text": " So a mapping just basically refers to something that behaves like a dictionary.", "tokens": [407, 257, 18350, 445, 1936, 14942, 281, 746, 300, 36896, 411, 257, 25890, 13], "temperature": 0.0, "avg_logprob": -0.11048810105574758, "compression_ratio": 1.5193370165745856, "no_speech_prob": 9.222774679074064e-06}, {"id": 475, "seek": 346700, "start": 3467.0, "end": 3480.0, "text": " So we actually have to create a dictionary which maps from the index to the name, which is a bit of a pointless thing to do in a sense, but that's okay.", "tokens": [407, 321, 767, 362, 281, 1884, 257, 25890, 597, 11317, 490, 264, 8186, 281, 264, 1315, 11, 597, 307, 257, 857, 295, 257, 32824, 551, 281, 360, 294, 257, 2020, 11, 457, 300, 311, 1392, 13], "temperature": 0.0, "avg_logprob": -0.1199625571568807, "compression_ratio": 1.4055944055944056, "no_speech_prob": 1.3419454262475483e-05}, {"id": 476, "seek": 346700, "start": 3480.0, "end": 3488.0, "text": " So for k, v in.", "tokens": [407, 337, 350, 11, 371, 294, 13], "temperature": 0.0, "avg_logprob": -0.1199625571568807, "compression_ratio": 1.4055944055944056, "no_speech_prob": 1.3419454262475483e-05}, {"id": 477, "seek": 346700, "start": 3488.0, "end": 3496.0, "text": " So if we enumerate through that.", "tokens": [407, 498, 321, 465, 15583, 473, 807, 300, 13], "temperature": 0.0, "avg_logprob": -0.1199625571568807, "compression_ratio": 1.4055944055944056, "no_speech_prob": 1.3419454262475483e-05}, {"id": 478, "seek": 349600, "start": 3496.0, "end": 3500.0, "text": " Okay, so that's what a mapping looks like.", "tokens": [1033, 11, 370, 300, 311, 437, 257, 18350, 1542, 411, 13], "temperature": 0.0, "avg_logprob": -0.1373694149064429, "compression_ratio": 1.576086956521739, "no_speech_prob": 4.356730642030016e-06}, {"id": 479, "seek": 349600, "start": 3500.0, "end": 3504.0, "text": " So I could say mapping equals.", "tokens": [407, 286, 727, 584, 18350, 6915, 13], "temperature": 0.0, "avg_logprob": -0.1373694149064429, "compression_ratio": 1.576086956521739, "no_speech_prob": 4.356730642030016e-06}, {"id": 480, "seek": 349600, "start": 3504.0, "end": 3508.0, "text": " And then here we'll say mapping.", "tokens": [400, 550, 510, 321, 603, 584, 18350, 13], "temperature": 0.0, "avg_logprob": -0.1373694149064429, "compression_ratio": 1.576086956521739, "no_speech_prob": 4.356730642030016e-06}, {"id": 481, "seek": 349600, "start": 3508.0, "end": 3510.0, "text": " There we go. So that's what we want.", "tokens": [821, 321, 352, 13, 407, 300, 311, 437, 321, 528, 13], "temperature": 0.0, "avg_logprob": -0.1373694149064429, "compression_ratio": 1.576086956521739, "no_speech_prob": 4.356730642030016e-06}, {"id": 482, "seek": 349600, "start": 3510.0, "end": 3516.0, "text": " So, this is basically our results, right?", "tokens": [407, 11, 341, 307, 1936, 527, 3542, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1373694149064429, "compression_ratio": 1.576086956521739, "no_speech_prob": 4.356730642030016e-06}, {"id": 483, "seek": 349600, "start": 3516.0, "end": 3522.0, "text": " So I was thinking like an alternative way, like mapping also map function also takes functions, correct?", "tokens": [407, 286, 390, 1953, 411, 364, 8535, 636, 11, 411, 18350, 611, 4471, 2445, 611, 2516, 6828, 11, 3006, 30], "temperature": 0.0, "avg_logprob": -0.1373694149064429, "compression_ratio": 1.576086956521739, "no_speech_prob": 4.356730642030016e-06}, {"id": 484, "seek": 352200, "start": 3522.0, "end": 3527.0, "text": " I was avoiding that because that I mean I know it doesn't matter here but it's really slow.", "tokens": [286, 390, 20220, 300, 570, 300, 286, 914, 286, 458, 309, 1177, 380, 1871, 510, 457, 309, 311, 534, 2964, 13], "temperature": 0.0, "avg_logprob": -0.1302609145641327, "compression_ratio": 1.6986301369863013, "no_speech_prob": 9.665701327321585e-06}, {"id": 485, "seek": 352200, "start": 3527.0, "end": 3531.0, "text": " So we, we could also pass in a function.", "tokens": [407, 321, 11, 321, 727, 611, 1320, 294, 257, 2445, 13], "temperature": 0.0, "avg_logprob": -0.1302609145641327, "compression_ratio": 1.6986301369863013, "no_speech_prob": 9.665701327321585e-06}, {"id": 486, "seek": 352200, "start": 3531.0, "end": 3536.0, "text": " I was just thinking like you could just have a function that just indexes into the into the list or something like that.", "tokens": [286, 390, 445, 1953, 411, 291, 727, 445, 362, 257, 2445, 300, 445, 8186, 279, 666, 264, 666, 264, 1329, 420, 746, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.1302609145641327, "compression_ratio": 1.6986301369863013, "no_speech_prob": 9.665701327321585e-06}, {"id": 487, "seek": 352200, "start": 3536.0, "end": 3538.0, "text": " Correct. Like a lambda function or something like that.", "tokens": [12753, 13, 1743, 257, 13607, 2445, 420, 746, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.1302609145641327, "compression_ratio": 1.6986301369863013, "no_speech_prob": 9.665701327321585e-06}, {"id": 488, "seek": 352200, "start": 3538.0, "end": 3543.0, "text": " Exactly. Let's go ahead and do that to see what it looks like.", "tokens": [7587, 13, 961, 311, 352, 2286, 293, 360, 300, 281, 536, 437, 309, 1542, 411, 13], "temperature": 0.0, "avg_logprob": -0.1302609145641327, "compression_ratio": 1.6986301369863013, "no_speech_prob": 9.665701327321585e-06}, {"id": 489, "seek": 354300, "start": 3543.0, "end": 3557.0, "text": " Almost nobody knows that you can use a dictionary or a mapping so almost everybody on cackle uses a function and often it can take a very, very, very long time to run at, you know, on big data sets.", "tokens": [12627, 5079, 3255, 300, 291, 393, 764, 257, 25890, 420, 257, 18350, 370, 1920, 2201, 322, 269, 501, 306, 4960, 257, 2445, 293, 2049, 309, 393, 747, 257, 588, 11, 588, 11, 588, 938, 565, 281, 1190, 412, 11, 291, 458, 11, 322, 955, 1412, 6352, 13], "temperature": 0.0, "avg_logprob": -0.18276329698233768, "compression_ratio": 1.5545023696682465, "no_speech_prob": 9.223038432537578e-06}, {"id": 490, "seek": 354300, "start": 3557.0, "end": 3567.0, "text": " So yeah, you could also have a lambda. And so that's going to be passed in each index, and you would just want to return the DLs.", "tokens": [407, 1338, 11, 291, 727, 611, 362, 257, 13607, 13, 400, 370, 300, 311, 516, 281, 312, 4678, 294, 1184, 8186, 11, 293, 291, 576, 445, 528, 281, 2736, 264, 413, 43, 82, 13], "temperature": 0.0, "avg_logprob": -0.18276329698233768, "compression_ratio": 1.5545023696682465, "no_speech_prob": 9.223038432537578e-06}, {"id": 491, "seek": 356700, "start": 3567.0, "end": 3573.0, "text": " dot go cab at I.", "tokens": [5893, 352, 5487, 412, 286, 13], "temperature": 0.0, "avg_logprob": -0.19188804626464845, "compression_ratio": 1.4259259259259258, "no_speech_prob": 8.397631063417066e-06}, {"id": 492, "seek": 356700, "start": 3573.0, "end": 3583.0, "text": " So that does the same thing now obviously this is tiny so it doesn't actually matter but I thought I'd try to show the neat trick which almost nobody knows about, which is the mapping.", "tokens": [407, 300, 775, 264, 912, 551, 586, 2745, 341, 307, 5870, 370, 309, 1177, 380, 767, 1871, 457, 286, 1194, 286, 1116, 853, 281, 855, 264, 10654, 4282, 597, 1920, 5079, 3255, 466, 11, 597, 307, 264, 18350, 13], "temperature": 0.0, "avg_logprob": -0.19188804626464845, "compression_ratio": 1.4259259259259258, "no_speech_prob": 8.397631063417066e-06}, {"id": 493, "seek": 356700, "start": 3583.0, "end": 3591.0, "text": " Okay, so we basically want to", "tokens": [1033, 11, 370, 321, 1936, 528, 281], "temperature": 0.0, "avg_logprob": -0.19188804626464845, "compression_ratio": 1.4259259259259258, "no_speech_prob": 8.397631063417066e-06}, {"id": 494, "seek": 359100, "start": 3591.0, "end": 3599.0, "text": " use that as our labels.", "tokens": [764, 300, 382, 527, 16949, 13], "temperature": 0.0, "avg_logprob": -0.1315139041227453, "compression_ratio": 1.0526315789473684, "no_speech_prob": 8.530050763511099e-06}, {"id": 495, "seek": 359100, "start": 3599.0, "end": 3607.0, "text": " So I think we can go SS label", "tokens": [407, 286, 519, 321, 393, 352, 12238, 7645], "temperature": 0.0, "avg_logprob": -0.1315139041227453, "compression_ratio": 1.0526315789473684, "no_speech_prob": 8.530050763511099e-06}, {"id": 496, "seek": 359100, "start": 3607.0, "end": 3613.0, "text": " equals.", "tokens": [6915, 13], "temperature": 0.0, "avg_logprob": -0.1315139041227453, "compression_ratio": 1.0526315789473684, "no_speech_prob": 8.530050763511099e-06}, {"id": 497, "seek": 359100, "start": 3613.0, "end": 3615.0, "text": " There we go.", "tokens": [821, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.1315139041227453, "compression_ratio": 1.0526315789473684, "no_speech_prob": 8.530050763511099e-06}, {"id": 498, "seek": 359100, "start": 3615.0, "end": 3617.0, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.1315139041227453, "compression_ratio": 1.0526315789473684, "no_speech_prob": 8.530050763511099e-06}, {"id": 499, "seek": 361700, "start": 3617.0, "end": 3632.0, "text": " So, you know, normally at this point, I would like visually check some results and the easiest way to visually check some results is to go learn dot show results.", "tokens": [407, 11, 291, 458, 11, 5646, 412, 341, 935, 11, 286, 576, 411, 19622, 1520, 512, 3542, 293, 264, 12889, 636, 281, 19622, 1520, 512, 3542, 307, 281, 352, 1466, 5893, 855, 3542, 13], "temperature": 0.0, "avg_logprob": -0.09813281736875835, "compression_ratio": 1.4727272727272727, "no_speech_prob": 3.5557966384658357e-06}, {"id": 500, "seek": 363200, "start": 3632.0, "end": 3649.0, "text": " And this is showing me the actual and the predicted and the accuracy is very high so it's all correct the problem is I don't know which of these are right which are wrong so I have no idea what to look for. So, I don't have that ability to do my normal checking.", "tokens": [400, 341, 307, 4099, 385, 264, 3539, 293, 264, 19147, 293, 264, 14170, 307, 588, 1090, 370, 309, 311, 439, 3006, 264, 1154, 307, 286, 500, 380, 458, 597, 295, 613, 366, 558, 597, 366, 2085, 370, 286, 362, 572, 1558, 437, 281, 574, 337, 13, 407, 11, 286, 500, 380, 362, 300, 3485, 281, 360, 452, 2710, 8568, 13], "temperature": 0.0, "avg_logprob": -0.1252005650446965, "compression_ratio": 1.5247524752475248, "no_speech_prob": 4.6376517275348306e-06}, {"id": 501, "seek": 363200, "start": 3649.0, "end": 3658.0, "text": " Okay, so we can say this is a CSV submission.", "tokens": [1033, 11, 370, 321, 393, 584, 341, 307, 257, 48814, 23689, 13], "temperature": 0.0, "avg_logprob": -0.1252005650446965, "compression_ratio": 1.5247524752475248, "no_speech_prob": 4.6376517275348306e-06}, {"id": 502, "seek": 365800, "start": 3658.0, "end": 3662.0, "text": " There we go. Okay.", "tokens": [821, 321, 352, 13, 1033, 13], "temperature": 0.0, "avg_logprob": -0.17178476360482228, "compression_ratio": 1.445054945054945, "no_speech_prob": 1.2217636140121613e-05}, {"id": 503, "seek": 365800, "start": 3662.0, "end": 3675.0, "text": " So there's a few things we could do here I guess probably the easiest one would be to use the Kaggle CLI.", "tokens": [407, 456, 311, 257, 1326, 721, 321, 727, 360, 510, 286, 2041, 1391, 264, 12889, 472, 576, 312, 281, 764, 264, 48751, 22631, 12855, 40, 13], "temperature": 0.0, "avg_logprob": -0.17178476360482228, "compression_ratio": 1.445054945054945, "no_speech_prob": 1.2217636140121613e-05}, {"id": 504, "seek": 365800, "start": 3675.0, "end": 3683.0, "text": " I was going to note something of the submission, or for the two CSV. I think you might have to do index equals false because you're right.", "tokens": [286, 390, 516, 281, 3637, 746, 295, 264, 23689, 11, 420, 337, 264, 732, 48814, 13, 286, 519, 291, 1062, 362, 281, 360, 8186, 6915, 7908, 570, 291, 434, 558, 13], "temperature": 0.0, "avg_logprob": -0.17178476360482228, "compression_ratio": 1.445054945054945, "no_speech_prob": 1.2217636140121613e-05}, {"id": 505, "seek": 368300, "start": 3683.0, "end": 3701.0, "text": " Normally, what I always do after that, except this time which I forgot is to do exclamation mark head to show me the first few lines and yeah so now we would see as Tunisic says, we've got this extra column out the front, which is because the default is that it shows kind of the row number.", "tokens": [17424, 11, 437, 286, 1009, 360, 934, 300, 11, 3993, 341, 565, 597, 286, 5298, 307, 281, 360, 1624, 43233, 1491, 1378, 281, 855, 385, 264, 700, 1326, 3876, 293, 1338, 370, 586, 321, 576, 536, 382, 21363, 271, 299, 1619, 11, 321, 600, 658, 341, 2857, 7738, 484, 264, 1868, 11, 597, 307, 570, 264, 7576, 307, 300, 309, 3110, 733, 295, 264, 5386, 1230, 13], "temperature": 0.0, "avg_logprob": -0.1802316095637179, "compression_ratio": 1.5514018691588785, "no_speech_prob": 7.76660908741178e-06}, {"id": 506, "seek": 368300, "start": 3701.0, "end": 3704.0, "text": " Thanks Tunisic.", "tokens": [2561, 21363, 271, 299, 13], "temperature": 0.0, "avg_logprob": -0.1802316095637179, "compression_ratio": 1.5514018691588785, "no_speech_prob": 7.76660908741178e-06}, {"id": 507, "seek": 368300, "start": 3704.0, "end": 3706.0, "text": " And so that will fix it.", "tokens": [400, 370, 300, 486, 3191, 309, 13], "temperature": 0.0, "avg_logprob": -0.1802316095637179, "compression_ratio": 1.5514018691588785, "no_speech_prob": 7.76660908741178e-06}, {"id": 508, "seek": 370600, "start": 3706.0, "end": 3716.0, "text": " And if we compare that to their sample.", "tokens": [400, 498, 321, 6794, 300, 281, 641, 6889, 13], "temperature": 0.0, "avg_logprob": -0.12437533449243617, "compression_ratio": 1.3834586466165413, "no_speech_prob": 5.01447357237339e-06}, {"id": 509, "seek": 370600, "start": 3716.0, "end": 3724.0, "text": " Yeah, it looks nice and similar. So that's good.", "tokens": [865, 11, 309, 1542, 1481, 293, 2531, 13, 407, 300, 311, 665, 13], "temperature": 0.0, "avg_logprob": -0.12437533449243617, "compression_ratio": 1.3834586466165413, "no_speech_prob": 5.01447357237339e-06}, {"id": 510, "seek": 370600, "start": 3724.0, "end": 3729.0, "text": " So these are all kind of steps in the same thing so I pop these all together.", "tokens": [407, 613, 366, 439, 733, 295, 4439, 294, 264, 912, 551, 370, 286, 1665, 613, 439, 1214, 13], "temperature": 0.0, "avg_logprob": -0.12437533449243617, "compression_ratio": 1.3834586466165413, "no_speech_prob": 5.01447357237339e-06}, {"id": 511, "seek": 370600, "start": 3729.0, "end": 3733.0, "text": " And then we might", "tokens": [400, 550, 321, 1062], "temperature": 0.0, "avg_logprob": -0.12437533449243617, "compression_ratio": 1.3834586466165413, "no_speech_prob": 5.01447357237339e-06}, {"id": 512, "seek": 373300, "start": 3733.0, "end": 3740.0, "text": " item and", "tokens": [3174, 293], "temperature": 0.0, "avg_logprob": -0.6581781506538391, "compression_ratio": 0.8285714285714286, "no_speech_prob": 3.704709524754435e-05}, {"id": 513, "seek": 373300, "start": 3740.0, "end": 3746.0, "text": " DD to get Patty,", "tokens": [30778, 281, 483, 44116, 11], "temperature": 0.0, "avg_logprob": -0.6581781506538391, "compression_ratio": 0.8285714285714286, "no_speech_prob": 3.704709524754435e-05}, {"id": 514, "seek": 373300, "start": 3746.0, "end": 3750.0, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.6581781506538391, "compression_ratio": 0.8285714285714286, "no_speech_prob": 3.704709524754435e-05}, {"id": 515, "seek": 375000, "start": 3750.0, "end": 3764.0, "text": " is that submission file.", "tokens": [307, 300, 23689, 3991, 13], "temperature": 0.0, "avg_logprob": -0.1951318316989475, "compression_ratio": 1.4554455445544554, "no_speech_prob": 1.7777087123249657e-05}, {"id": 516, "seek": 375000, "start": 3764.0, "end": 3773.0, "text": " Okay, so generally minus help or minus minus sorry minus h or minus minus help normally gives you a quick version of help.", "tokens": [1033, 11, 370, 5101, 3175, 854, 420, 3175, 3175, 2597, 3175, 276, 420, 3175, 3175, 854, 5646, 2709, 291, 257, 1702, 3037, 295, 854, 13], "temperature": 0.0, "avg_logprob": -0.1951318316989475, "compression_ratio": 1.4554455445544554, "no_speech_prob": 1.7777087123249657e-05}, {"id": 517, "seek": 377300, "start": 3773.0, "end": 3782.0, "text": " And so, I want to do something with competitions competitions.", "tokens": [400, 370, 11, 286, 528, 281, 360, 746, 365, 26185, 26185, 13], "temperature": 0.0, "avg_logprob": -0.17788410186767578, "compression_ratio": 1.5307692307692307, "no_speech_prob": 1.1299958714516833e-05}, {"id": 518, "seek": 377300, "start": 3782.0, "end": 3788.0, "text": " Okay, there we go. And so we're going to do a submission.", "tokens": [1033, 11, 456, 321, 352, 13, 400, 370, 321, 434, 516, 281, 360, 257, 23689, 13], "temperature": 0.0, "avg_logprob": -0.17788410186767578, "compression_ratio": 1.5307692307692307, "no_speech_prob": 1.1299958714516833e-05}, {"id": 519, "seek": 377300, "start": 3788.0, "end": 3792.0, "text": " All right, we need a file for upload.", "tokens": [1057, 558, 11, 321, 643, 257, 3991, 337, 6580, 13], "temperature": 0.0, "avg_logprob": -0.17788410186767578, "compression_ratio": 1.5307692307692307, "no_speech_prob": 1.1299958714516833e-05}, {"id": 520, "seek": 377300, "start": 3792.0, "end": 3796.0, "text": " And we're going to need the competition.", "tokens": [400, 321, 434, 516, 281, 643, 264, 6211, 13], "temperature": 0.0, "avg_logprob": -0.17788410186767578, "compression_ratio": 1.5307692307692307, "no_speech_prob": 1.1299958714516833e-05}, {"id": 521, "seek": 379600, "start": 3796.0, "end": 3803.0, "text": " And so I could go Kaggle competitions list pipe grip Patty.", "tokens": [400, 370, 286, 727, 352, 48751, 22631, 26185, 1329, 11240, 12007, 44116, 13], "temperature": 0.0, "avg_logprob": -0.18670115178945113, "compression_ratio": 1.2661290322580645, "no_speech_prob": 1.75023506017169e-05}, {"id": 522, "seek": 379600, "start": 3803.0, "end": 3811.0, "text": " That way I don't even have to. Oh, that's not what I expected to happen.", "tokens": [663, 636, 286, 500, 380, 754, 362, 281, 13, 876, 11, 300, 311, 406, 437, 286, 5176, 281, 1051, 13], "temperature": 0.0, "avg_logprob": -0.18670115178945113, "compression_ratio": 1.2661290322580645, "no_speech_prob": 1.75023506017169e-05}, {"id": 523, "seek": 379600, "start": 3811.0, "end": 3818.0, "text": " Oh, I bet that pages it.", "tokens": [876, 11, 286, 778, 300, 7183, 309, 13], "temperature": 0.0, "avg_logprob": -0.18670115178945113, "compression_ratio": 1.2661290322580645, "no_speech_prob": 1.75023506017169e-05}, {"id": 524, "seek": 381800, "start": 3818.0, "end": 3826.0, "text": " Okay, so rather than grip, we should use minus s.", "tokens": [1033, 11, 370, 2831, 813, 12007, 11, 321, 820, 764, 3175, 262, 13], "temperature": 0.0, "avg_logprob": -0.13874344948010567, "compression_ratio": 1.1818181818181819, "no_speech_prob": 7.410740636260016e-06}, {"id": 525, "seek": 381800, "start": 3826.0, "end": 3841.0, "text": " And it's going to use a regular expression or something.", "tokens": [400, 309, 311, 516, 281, 764, 257, 3890, 6114, 420, 746, 13], "temperature": 0.0, "avg_logprob": -0.13874344948010567, "compression_ratio": 1.1818181818181819, "no_speech_prob": 7.410740636260016e-06}, {"id": 526, "seek": 381800, "start": 3841.0, "end": 3845.0, "text": " Look at a few examples.", "tokens": [2053, 412, 257, 1326, 5110, 13], "temperature": 0.0, "avg_logprob": -0.13874344948010567, "compression_ratio": 1.1818181818181819, "no_speech_prob": 7.410740636260016e-06}, {"id": 527, "seek": 384500, "start": 3845.0, "end": 3852.0, "text": " Okay, so there's definitely something called spaceship spaceship.", "tokens": [1033, 11, 370, 456, 311, 2138, 746, 1219, 39185, 39185, 13], "temperature": 0.0, "avg_logprob": -0.17624078478131974, "compression_ratio": 1.385135135135135, "no_speech_prob": 5.5071463975764345e-06}, {"id": 528, "seek": 384500, "start": 3852.0, "end": 3854.0, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.17624078478131974, "compression_ratio": 1.385135135135135, "no_speech_prob": 5.5071463975764345e-06}, {"id": 529, "seek": 384500, "start": 3854.0, "end": 3858.0, "text": " I will go to here after all.", "tokens": [286, 486, 352, 281, 510, 934, 439, 13], "temperature": 0.0, "avg_logprob": -0.17624078478131974, "compression_ratio": 1.385135135135135, "no_speech_prob": 5.5071463975764345e-06}, {"id": 530, "seek": 384500, "start": 3858.0, "end": 3863.0, "text": " And this is what it's called. So, I don't know.", "tokens": [400, 341, 307, 437, 309, 311, 1219, 13, 407, 11, 286, 500, 380, 458, 13], "temperature": 0.0, "avg_logprob": -0.17624078478131974, "compression_ratio": 1.385135135135135, "no_speech_prob": 5.5071463975764345e-06}, {"id": 531, "seek": 384500, "start": 3863.0, "end": 3869.0, "text": " Is it not active, probably. That's why it's active.", "tokens": [1119, 309, 406, 4967, 11, 1391, 13, 663, 311, 983, 309, 311, 4967, 13], "temperature": 0.0, "avg_logprob": -0.17624078478131974, "compression_ratio": 1.385135135135135, "no_speech_prob": 5.5071463975764345e-06}, {"id": 532, "seek": 386900, "start": 3869.0, "end": 3875.0, "text": " Yeah, exactly.", "tokens": [865, 11, 2293, 13], "temperature": 0.0, "avg_logprob": -0.27446568598512744, "compression_ratio": 1.3733333333333333, "no_speech_prob": 7.645993719052058e-06}, {"id": 533, "seek": 386900, "start": 3875.0, "end": 3881.0, "text": " Is it because it's like not really actually organized by Kaggle maybe the only list.", "tokens": [1119, 309, 570, 309, 311, 411, 406, 534, 767, 9983, 538, 48751, 22631, 1310, 264, 787, 1329, 13], "temperature": 0.0, "avg_logprob": -0.27446568598512744, "compression_ratio": 1.3733333333333333, "no_speech_prob": 7.645993719052058e-06}, {"id": 534, "seek": 386900, "start": 3881.0, "end": 3884.0, "text": " Or we need a copy of the category all.", "tokens": [1610, 321, 643, 257, 5055, 295, 264, 7719, 439, 13], "temperature": 0.0, "avg_logprob": -0.27446568598512744, "compression_ratio": 1.3733333333333333, "no_speech_prob": 7.645993719052058e-06}, {"id": 535, "seek": 386900, "start": 3884.0, "end": 3887.0, "text": " It's fine. But maybe it's group.", "tokens": [467, 311, 2489, 13, 583, 1310, 309, 311, 1594, 13], "temperature": 0.0, "avg_logprob": -0.27446568598512744, "compression_ratio": 1.3733333333333333, "no_speech_prob": 7.645993719052058e-06}, {"id": 536, "seek": 386900, "start": 3887.0, "end": 3890.0, "text": " Maybe this is considered in class.", "tokens": [2704, 341, 307, 4888, 294, 1508, 13], "temperature": 0.0, "avg_logprob": -0.27446568598512744, "compression_ratio": 1.3733333333333333, "no_speech_prob": 7.645993719052058e-06}, {"id": 537, "seek": 389000, "start": 3890.0, "end": 3899.0, "text": " Yeah. Anywho, so we were going to do a submission.", "tokens": [865, 13, 2639, 13506, 11, 370, 321, 645, 516, 281, 360, 257, 23689, 13], "temperature": 0.0, "avg_logprob": -0.20493032621300739, "compression_ratio": 1.1637931034482758, "no_speech_prob": 2.0783500076504424e-05}, {"id": 538, "seek": 389000, "start": 3899.0, "end": 3904.0, "text": " And so we need to provide the file name, minus F.", "tokens": [400, 370, 321, 643, 281, 2893, 264, 3991, 1315, 11, 3175, 479, 13], "temperature": 0.0, "avg_logprob": -0.20493032621300739, "compression_ratio": 1.1637931034482758, "no_speech_prob": 2.0783500076504424e-05}, {"id": 539, "seek": 389000, "start": 3904.0, "end": 3910.0, "text": " Oh, full path. That's a bit weird.", "tokens": [876, 11, 1577, 3100, 13, 663, 311, 257, 857, 3657, 13], "temperature": 0.0, "avg_logprob": -0.20493032621300739, "compression_ratio": 1.1637931034482758, "no_speech_prob": 2.0783500076504424e-05}, {"id": 540, "seek": 391000, "start": 3910.0, "end": 3920.0, "text": " Okay. And a message, minus m.", "tokens": [1033, 13, 400, 257, 3636, 11, 3175, 275, 13], "temperature": 0.0, "avg_logprob": -0.44814608647273135, "compression_ratio": 0.9736842105263158, "no_speech_prob": 9.368198334414046e-06}, {"id": 541, "seek": 391000, "start": 3920.0, "end": 3930.0, "text": " Initial conv next, small to epoch spaceship.", "tokens": [22937, 831, 3754, 958, 11, 1359, 281, 30992, 339, 39185, 13], "temperature": 0.0, "avg_logprob": -0.44814608647273135, "compression_ratio": 0.9736842105263158, "no_speech_prob": 9.368198334414046e-06}, {"id": 542, "seek": 393000, "start": 3930.0, "end": 3948.0, "text": " Okay. And then the competition. And go.", "tokens": [1033, 13, 400, 550, 264, 6211, 13, 400, 352, 13], "temperature": 0.0, "avg_logprob": -0.13621650740157726, "compression_ratio": 1.1485148514851484, "no_speech_prob": 1.0129196198249701e-05}, {"id": 543, "seek": 393000, "start": 3948.0, "end": 3951.0, "text": " Took a while for a 70k file, but so be it.", "tokens": [38288, 257, 1339, 337, 257, 5285, 74, 3991, 11, 457, 370, 312, 309, 13], "temperature": 0.0, "avg_logprob": -0.13621650740157726, "compression_ratio": 1.1485148514851484, "no_speech_prob": 1.0129196198249701e-05}, {"id": 544, "seek": 393000, "start": 3951.0, "end": 3956.0, "text": " Okay. So let's see if it's there.", "tokens": [1033, 13, 407, 718, 311, 536, 498, 309, 311, 456, 13], "temperature": 0.0, "avg_logprob": -0.13621650740157726, "compression_ratio": 1.1485148514851484, "no_speech_prob": 1.0129196198249701e-05}, {"id": 545, "seek": 395600, "start": 3956.0, "end": 3960.0, "text": " It is.", "tokens": [467, 307, 13], "temperature": 0.0, "avg_logprob": -0.39518385463290745, "compression_ratio": 0.9871794871794872, "no_speech_prob": 3.268813816248439e-05}, {"id": 546, "seek": 395600, "start": 3960.0, "end": 3965.0, "text": " How did we do?", "tokens": [1012, 630, 321, 360, 30], "temperature": 0.0, "avg_logprob": -0.39518385463290745, "compression_ratio": 0.9871794871794872, "no_speech_prob": 3.268813816248439e-05}, {"id": 547, "seek": 395600, "start": 3965.0, "end": 3967.0, "text": " Oh, I see.", "tokens": [876, 11, 286, 536, 13], "temperature": 0.0, "avg_logprob": -0.39518385463290745, "compression_ratio": 0.9871794871794872, "no_speech_prob": 3.268813816248439e-05}, {"id": 548, "seek": 395600, "start": 3967.0, "end": 3968.0, "text": " Oh, yeah.", "tokens": [876, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.39518385463290745, "compression_ratio": 0.9871794871794872, "no_speech_prob": 3.268813816248439e-05}, {"id": 549, "seek": 395600, "start": 3968.0, "end": 3972.0, "text": " Jump to your leaderboard position.", "tokens": [18697, 281, 428, 5263, 3787, 2535, 13], "temperature": 0.0, "avg_logprob": -0.39518385463290745, "compression_ratio": 0.9871794871794872, "no_speech_prob": 3.268813816248439e-05}, {"id": 550, "seek": 397200, "start": 3972.0, "end": 3987.0, "text": " Out of.", "tokens": [5925, 295, 13], "temperature": 0.0, "avg_logprob": -0.2695005043693211, "compression_ratio": 1.2611940298507462, "no_speech_prob": 9.222418157150969e-06}, {"id": 551, "seek": 397200, "start": 3987.0, "end": 3995.0, "text": " So I'm guessing that there's a problem with our submission because it's, I think maybe what happened was the test files were not like.", "tokens": [407, 286, 478, 17939, 300, 456, 311, 257, 1154, 365, 527, 23689, 570, 309, 311, 11, 286, 519, 1310, 437, 2011, 390, 264, 1500, 7098, 645, 406, 411, 13], "temperature": 0.0, "avg_logprob": -0.2695005043693211, "compression_ratio": 1.2611940298507462, "no_speech_prob": 9.222418157150969e-06}, {"id": 552, "seek": 397200, "start": 3995.0, "end": 3999.0, "text": " They got shuffled somehow.", "tokens": [814, 658, 402, 33974, 6063, 13], "temperature": 0.0, "avg_logprob": -0.2695005043693211, "compression_ratio": 1.2611940298507462, "no_speech_prob": 9.222418157150969e-06}, {"id": 553, "seek": 399900, "start": 3999.0, "end": 4002.0, "text": " So we're not sure.", "tokens": [407, 321, 434, 406, 988, 13], "temperature": 0.0, "avg_logprob": -0.4006347289452186, "compression_ratio": 1.52020202020202, "no_speech_prob": 5.592577963398071e-06}, {"id": 554, "seek": 399900, "start": 4002.0, "end": 4003.0, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.4006347289452186, "compression_ratio": 1.52020202020202, "no_speech_prob": 5.592577963398071e-06}, {"id": 555, "seek": 399900, "start": 4002.0, "end": 4004.0, "text": " Sometimes that might happen.", "tokens": [4803, 300, 1062, 1051, 13], "temperature": 0.0, "avg_logprob": -0.4006347289452186, "compression_ratio": 1.52020202020202, "no_speech_prob": 5.592577963398071e-06}, {"id": 556, "seek": 399900, "start": 4004.0, "end": 4006.0, "text": " Yeah. That's a good question.", "tokens": [865, 13, 663, 311, 257, 665, 1168, 13], "temperature": 0.0, "avg_logprob": -0.4006347289452186, "compression_ratio": 1.52020202020202, "no_speech_prob": 5.592577963398071e-06}, {"id": 557, "seek": 399900, "start": 4006.0, "end": 4007.0, "text": " Yeah. Yeah. Yeah.", "tokens": [865, 13, 865, 13, 865, 13], "temperature": 0.0, "avg_logprob": -0.4006347289452186, "compression_ratio": 1.52020202020202, "no_speech_prob": 5.592577963398071e-06}, {"id": 558, "seek": 399900, "start": 4007.0, "end": 4010.0, "text": " That looks like that seems very likely.", "tokens": [663, 1542, 411, 300, 2544, 588, 3700, 13], "temperature": 0.0, "avg_logprob": -0.4006347289452186, "compression_ratio": 1.52020202020202, "no_speech_prob": 5.592577963398071e-06}, {"id": 559, "seek": 399900, "start": 4010.0, "end": 4013.0, "text": " So we didn't do a lot of checking as we went.", "tokens": [407, 321, 994, 380, 360, 257, 688, 295, 8568, 382, 321, 1437, 13], "temperature": 0.0, "avg_logprob": -0.4006347289452186, "compression_ratio": 1.52020202020202, "no_speech_prob": 5.592577963398071e-06}, {"id": 560, "seek": 399900, "start": 4013.0, "end": 4015.0, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.4006347289452186, "compression_ratio": 1.52020202020202, "no_speech_prob": 5.592577963398071e-06}, {"id": 561, "seek": 399900, "start": 4015.0, "end": 4022.0, "text": " So they were expecting that 2001 would be first and we have 2000, 300,000, 119 first.", "tokens": [407, 436, 645, 9650, 300, 16382, 576, 312, 700, 293, 321, 362, 8132, 11, 6641, 11, 1360, 11, 2975, 24, 700, 13], "temperature": 0.0, "avg_logprob": -0.4006347289452186, "compression_ratio": 1.52020202020202, "no_speech_prob": 5.592577963398071e-06}, {"id": 562, "seek": 399900, "start": 4022.0, "end": 4024.0, "text": " So that is not.", "tokens": [407, 300, 307, 406, 13], "temperature": 0.0, "avg_logprob": -0.4006347289452186, "compression_ratio": 1.52020202020202, "no_speech_prob": 5.592577963398071e-06}, {"id": 563, "seek": 399900, "start": 4024.0, "end": 4027.0, "text": " Ideal.", "tokens": [13090, 304, 13], "temperature": 0.0, "avg_logprob": -0.4006347289452186, "compression_ratio": 1.52020202020202, "no_speech_prob": 5.592577963398071e-06}, {"id": 564, "seek": 402700, "start": 4027.0, "end": 4031.0, "text": " It would be nice if get image files by default.", "tokens": [467, 576, 312, 1481, 498, 483, 3256, 7098, 538, 7576, 13], "temperature": 0.0, "avg_logprob": -0.15165040930923152, "compression_ratio": 1.2651515151515151, "no_speech_prob": 1.5205024283204693e-05}, {"id": 565, "seek": 402700, "start": 4031.0, "end": 4038.0, "text": " Return things in a more sensible order.", "tokens": [24350, 721, 294, 257, 544, 25380, 1668, 13], "temperature": 0.0, "avg_logprob": -0.15165040930923152, "compression_ratio": 1.2651515151515151, "no_speech_prob": 1.5205024283204693e-05}, {"id": 566, "seek": 402700, "start": 4038.0, "end": 4043.0, "text": " Anyway, it's good to see these problems.", "tokens": [5684, 11, 309, 311, 665, 281, 536, 613, 2740, 13], "temperature": 0.0, "avg_logprob": -0.15165040930923152, "compression_ratio": 1.2651515151515151, "no_speech_prob": 1.5205024283204693e-05}, {"id": 567, "seek": 402700, "start": 4043.0, "end": 4051.0, "text": " You know, we could just sort it right.", "tokens": [509, 458, 11, 321, 727, 445, 1333, 309, 558, 13], "temperature": 0.0, "avg_logprob": -0.15165040930923152, "compression_ratio": 1.2651515151515151, "no_speech_prob": 1.5205024283204693e-05}, {"id": 568, "seek": 405100, "start": 4051.0, "end": 4062.0, "text": " But it looks like it works.", "tokens": [583, 309, 1542, 411, 309, 1985, 13], "temperature": 0.0, "avg_logprob": -0.1014801561832428, "compression_ratio": 1.475609756097561, "no_speech_prob": 5.50726190340356e-06}, {"id": 569, "seek": 405100, "start": 4062.0, "end": 4068.0, "text": " It probably does work as long as they've got exactly the same number, as long as they're all 123456 digits.", "tokens": [467, 1391, 775, 589, 382, 938, 382, 436, 600, 658, 2293, 264, 912, 1230, 11, 382, 938, 382, 436, 434, 439, 34466, 8465, 21, 27011, 13], "temperature": 0.0, "avg_logprob": -0.1014801561832428, "compression_ratio": 1.475609756097561, "no_speech_prob": 5.50726190340356e-06}, {"id": 570, "seek": 405100, "start": 4068.0, "end": 4079.0, "text": " If some of them are different numbers of digits, we can't sort it because this is sorting in string order.", "tokens": [759, 512, 295, 552, 366, 819, 3547, 295, 27011, 11, 321, 393, 380, 1333, 309, 570, 341, 307, 32411, 294, 6798, 1668, 13], "temperature": 0.0, "avg_logprob": -0.1014801561832428, "compression_ratio": 1.475609756097561, "no_speech_prob": 5.50726190340356e-06}, {"id": 571, "seek": 407900, "start": 4079.0, "end": 4084.0, "text": " But, yeah, maybe that's OK.", "tokens": [583, 11, 1338, 11, 1310, 300, 311, 2264, 13], "temperature": 0.0, "avg_logprob": -0.32096431873462816, "compression_ratio": 1.3421052631578947, "no_speech_prob": 7.646105586900376e-06}, {"id": 572, "seek": 407900, "start": 4084.0, "end": 4085.0, "text": " Says start tail.", "tokens": [36780, 722, 6838, 13], "temperature": 0.0, "avg_logprob": -0.32096431873462816, "compression_ratio": 1.3421052631578947, "no_speech_prob": 7.646105586900376e-06}, {"id": 573, "seek": 407900, "start": 4085.0, "end": 4086.0, "text": " Does it have a tail?", "tokens": [4402, 309, 362, 257, 6838, 30], "temperature": 0.0, "avg_logprob": -0.32096431873462816, "compression_ratio": 1.3421052631578947, "no_speech_prob": 7.646105586900376e-06}, {"id": 574, "seek": 407900, "start": 4086.0, "end": 4087.0, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.32096431873462816, "compression_ratio": 1.3421052631578947, "no_speech_prob": 7.646105586900376e-06}, {"id": 575, "seek": 407900, "start": 4087.0, "end": 4090.0, "text": " 203469.", "tokens": [945, 12249, 30908, 13], "temperature": 0.0, "avg_logprob": -0.32096431873462816, "compression_ratio": 1.3421052631578947, "no_speech_prob": 7.646105586900376e-06}, {"id": 576, "seek": 407900, "start": 4090.0, "end": 4092.0, "text": " 203469. Yeah. OK.", "tokens": [945, 12249, 30908, 13, 865, 13, 2264, 13], "temperature": 0.0, "avg_logprob": -0.32096431873462816, "compression_ratio": 1.3421052631578947, "no_speech_prob": 7.646105586900376e-06}, {"id": 577, "seek": 407900, "start": 4092.0, "end": 4097.0, "text": " Maybe we're fine then.", "tokens": [2704, 321, 434, 2489, 550, 13], "temperature": 0.0, "avg_logprob": -0.32096431873462816, "compression_ratio": 1.3421052631578947, "no_speech_prob": 7.646105586900376e-06}, {"id": 578, "seek": 407900, "start": 4097.0, "end": 4098.0, "text": " So sorted.", "tokens": [407, 25462, 13], "temperature": 0.0, "avg_logprob": -0.32096431873462816, "compression_ratio": 1.3421052631578947, "no_speech_prob": 7.646105586900376e-06}, {"id": 579, "seek": 407900, "start": 4098.0, "end": 4099.0, "text": " Whoopsie daisy.", "tokens": [45263, 414, 1120, 14169, 13], "temperature": 0.0, "avg_logprob": -0.32096431873462816, "compression_ratio": 1.3421052631578947, "no_speech_prob": 7.646105586900376e-06}, {"id": 580, "seek": 407900, "start": 4099.0, "end": 4106.0, "text": " Sorted returns the sorted version or sort sorts in place.", "tokens": [318, 14813, 11247, 264, 25462, 3037, 420, 1333, 7527, 294, 1081, 13], "temperature": 0.0, "avg_logprob": -0.32096431873462816, "compression_ratio": 1.3421052631578947, "no_speech_prob": 7.646105586900376e-06}, {"id": 581, "seek": 410600, "start": 4106.0, "end": 4116.0, "text": " OK.", "tokens": [2264, 13], "temperature": 0.0, "avg_logprob": -0.430048664410909, "compression_ratio": 0.2727272727272727, "no_speech_prob": 0.000144070407259278}, {"id": 582, "seek": 411600, "start": 4116.0, "end": 4139.0, "text": " Those.", "tokens": [3950, 13], "temperature": 0.0, "avg_logprob": -0.5349469582239786, "compression_ratio": 0.42857142857142855, "no_speech_prob": 2.9479549539246364e-06}, {"id": 583, "seek": 413900, "start": 4139.0, "end": 4153.0, "text": " And so it's very nice to have things set up, you know, that you're doing things from the command line and notebooks and stuff so that when you screw up, which, you know, if you're anything like me, you always screw up.", "tokens": [400, 370, 309, 311, 588, 1481, 281, 362, 721, 992, 493, 11, 291, 458, 11, 300, 291, 434, 884, 721, 490, 264, 5622, 1622, 293, 43782, 293, 1507, 370, 300, 562, 291, 5630, 493, 11, 597, 11, 291, 458, 11, 498, 291, 434, 1340, 411, 385, 11, 291, 1009, 5630, 493, 13], "temperature": 0.0, "avg_logprob": -0.09759133021036784, "compression_ratio": 1.5405405405405406, "no_speech_prob": 2.902137566707097e-06}, {"id": 584, "seek": 413900, "start": 4153.0, "end": 4157.0, "text": " You can pretty quickly repeat the process.", "tokens": [509, 393, 1238, 2661, 7149, 264, 1399, 13], "temperature": 0.0, "avg_logprob": -0.09759133021036784, "compression_ratio": 1.5405405405405406, "no_speech_prob": 2.902137566707097e-06}, {"id": 585, "seek": 413900, "start": 4157.0, "end": 4162.0, "text": " So I just hit up arrow.", "tokens": [407, 286, 445, 2045, 493, 11610, 13], "temperature": 0.0, "avg_logprob": -0.09759133021036784, "compression_ratio": 1.5405405405405406, "no_speech_prob": 2.902137566707097e-06}, {"id": 586, "seek": 416200, "start": 4162.0, "end": 4180.0, "text": " And just add sorted to our message, both literally and figuratively, hopefully.", "tokens": [400, 445, 909, 25462, 281, 527, 3636, 11, 1293, 3736, 293, 31094, 19020, 11, 4696, 13], "temperature": 0.0, "avg_logprob": -0.1654130352867974, "compression_ratio": 1.2105263157894737, "no_speech_prob": 4.9064703489420936e-05}, {"id": 587, "seek": 416200, "start": 4180.0, "end": 4182.0, "text": " Welcome to the leaderboard.", "tokens": [4027, 281, 264, 5263, 3787, 13], "temperature": 0.0, "avg_logprob": -0.1654130352867974, "compression_ratio": 1.2105263157894737, "no_speech_prob": 4.9064703489420936e-05}, {"id": 588, "seek": 416200, "start": 4182.0, "end": 4191.0, "text": " Not a very successful welcome.", "tokens": [1726, 257, 588, 4406, 2928, 13], "temperature": 0.0, "avg_logprob": -0.1654130352867974, "compression_ratio": 1.2105263157894737, "no_speech_prob": 4.9064703489420936e-05}, {"id": 589, "seek": 419100, "start": 4191.0, "end": 4193.0, "text": " It's better, isn't it?", "tokens": [467, 311, 1101, 11, 1943, 380, 309, 30], "temperature": 0.0, "avg_logprob": -0.22340258773492308, "compression_ratio": 1.168141592920354, "no_speech_prob": 2.0782823412446305e-05}, {"id": 590, "seek": 419100, "start": 4193.0, "end": 4197.0, "text": " Point nine one.", "tokens": [12387, 4949, 472, 13], "temperature": 0.0, "avg_logprob": -0.22340258773492308, "compression_ratio": 1.168141592920354, "no_speech_prob": 2.0782823412446305e-05}, {"id": 591, "seek": 419100, "start": 4197.0, "end": 4202.0, "text": " There we go.", "tokens": [821, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.22340258773492308, "compression_ratio": 1.168141592920354, "no_speech_prob": 2.0782823412446305e-05}, {"id": 592, "seek": 419100, "start": 4202.0, "end": 4206.0, "text": " Good start.", "tokens": [2205, 722, 13], "temperature": 0.0, "avg_logprob": -0.22340258773492308, "compression_ratio": 1.168141592920354, "no_speech_prob": 2.0782823412446305e-05}, {"id": 593, "seek": 419100, "start": 4206.0, "end": 4209.0, "text": " About in the middle.", "tokens": [7769, 294, 264, 2808, 13], "temperature": 0.0, "avg_logprob": -0.22340258773492308, "compression_ratio": 1.168141592920354, "no_speech_prob": 2.0782823412446305e-05}, {"id": 594, "seek": 419100, "start": 4209.0, "end": 4211.0, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.22340258773492308, "compression_ratio": 1.168141592920354, "no_speech_prob": 2.0782823412446305e-05}, {"id": 595, "seek": 419100, "start": 4211.0, "end": 4216.0, "text": " So does anybody have questions about", "tokens": [407, 775, 4472, 362, 1651, 466], "temperature": 0.0, "avg_logprob": -0.22340258773492308, "compression_ratio": 1.168141592920354, "no_speech_prob": 2.0782823412446305e-05}, {"id": 596, "seek": 421600, "start": 4216.0, "end": 4221.0, "text": " Yeah, this process of", "tokens": [865, 11, 341, 1399, 295], "temperature": 0.0, "avg_logprob": -0.34704321257922116, "compression_ratio": 1.3025210084033614, "no_speech_prob": 2.2819773221272044e-05}, {"id": 597, "seek": 421600, "start": 4221.0, "end": 4226.0, "text": " seeing my question in the chat and I had the same problem.", "tokens": [2577, 452, 1168, 294, 264, 5081, 293, 286, 632, 264, 912, 1154, 13], "temperature": 0.0, "avg_logprob": -0.34704321257922116, "compression_ratio": 1.3025210084033614, "no_speech_prob": 2.2819773221272044e-05}, {"id": 598, "seek": 421600, "start": 4226.0, "end": 4230.0, "text": " When installing Tim.", "tokens": [1133, 20762, 7172, 13], "temperature": 0.0, "avg_logprob": -0.34704321257922116, "compression_ratio": 1.3025210084033614, "no_speech_prob": 2.2819773221272044e-05}, {"id": 599, "seek": 421600, "start": 4230.0, "end": 4233.0, "text": " We, this is in paper space.", "tokens": [492, 11, 341, 307, 294, 3035, 1901, 13], "temperature": 0.0, "avg_logprob": -0.34704321257922116, "compression_ratio": 1.3025210084033614, "no_speech_prob": 2.2819773221272044e-05}, {"id": 600, "seek": 421600, "start": 4233.0, "end": 4236.0, "text": " Assume Mike was the same.", "tokens": [6281, 2540, 6602, 390, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.34704321257922116, "compression_ratio": 1.3025210084033614, "no_speech_prob": 2.2819773221272044e-05}, {"id": 601, "seek": 423600, "start": 4236.0, "end": 4246.0, "text": " We can see the list of models, but it doesn't actually create the learner.", "tokens": [492, 393, 536, 264, 1329, 295, 5245, 11, 457, 309, 1177, 380, 767, 1884, 264, 33347, 13], "temperature": 0.0, "avg_logprob": -0.20087124580560728, "compression_ratio": 1.5054945054945055, "no_speech_prob": 1.602670408828999e-06}, {"id": 602, "seek": 423600, "start": 4246.0, "end": 4248.0, "text": " It says Tim is not defined.", "tokens": [467, 1619, 7172, 307, 406, 7642, 13], "temperature": 0.0, "avg_logprob": -0.20087124580560728, "compression_ratio": 1.5054945054945055, "no_speech_prob": 1.602670408828999e-06}, {"id": 603, "seek": 423600, "start": 4248.0, "end": 4250.0, "text": " So you need to.", "tokens": [407, 291, 643, 281, 13], "temperature": 0.0, "avg_logprob": -0.20087124580560728, "compression_ratio": 1.5054945054945055, "no_speech_prob": 1.602670408828999e-06}, {"id": 604, "seek": 423600, "start": 4250.0, "end": 4252.0, "text": " So, yeah.", "tokens": [407, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.20087124580560728, "compression_ratio": 1.5054945054945055, "no_speech_prob": 1.602670408828999e-06}, {"id": 605, "seek": 423600, "start": 4252.0, "end": 4257.0, "text": " So, actually, I was able to do that.", "tokens": [407, 11, 767, 11, 286, 390, 1075, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.20087124580560728, "compression_ratio": 1.5054945054945055, "no_speech_prob": 1.602670408828999e-06}, {"id": 606, "seek": 423600, "start": 4257.0, "end": 4260.0, "text": " Matt, I just restarted my kernel.", "tokens": [7397, 11, 286, 445, 21022, 292, 452, 28256, 13], "temperature": 0.0, "avg_logprob": -0.20087124580560728, "compression_ratio": 1.5054945054945055, "no_speech_prob": 1.602670408828999e-06}, {"id": 607, "seek": 423600, "start": 4260.0, "end": 4265.0, "text": " Yeah. So just to explain when you see in Python, something is not defined.", "tokens": [865, 13, 407, 445, 281, 2903, 562, 291, 536, 294, 15329, 11, 746, 307, 406, 7642, 13], "temperature": 0.0, "avg_logprob": -0.20087124580560728, "compression_ratio": 1.5054945054945055, "no_speech_prob": 1.602670408828999e-06}, {"id": 608, "seek": 426500, "start": 4265.0, "end": 4267.0, "text": " That symbol.", "tokens": [663, 5986, 13], "temperature": 0.0, "avg_logprob": -0.21013079061136616, "compression_ratio": 1.7393939393939395, "no_speech_prob": 8.013197657419369e-06}, {"id": 609, "seek": 426500, "start": 4267.0, "end": 4269.0, "text": " Python doesn't know what it is.", "tokens": [15329, 1177, 380, 458, 437, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.21013079061136616, "compression_ratio": 1.7393939393939395, "no_speech_prob": 8.013197657419369e-06}, {"id": 610, "seek": 426500, "start": 4269.0, "end": 4275.0, "text": " And so there are two ways basically to define a symbol to create a symbol.", "tokens": [400, 370, 456, 366, 732, 2098, 1936, 281, 6964, 257, 5986, 281, 1884, 257, 5986, 13], "temperature": 0.0, "avg_logprob": -0.21013079061136616, "compression_ratio": 1.7393939393939395, "no_speech_prob": 8.013197657419369e-06}, {"id": 611, "seek": 426500, "start": 4275.0, "end": 4285.0, "text": " One is to say something like a equals one that defines a symbol called a right or another is to do something like the death.", "tokens": [1485, 307, 281, 584, 746, 411, 257, 6915, 472, 300, 23122, 257, 5986, 1219, 257, 558, 420, 1071, 307, 281, 360, 746, 411, 264, 2966, 13], "temperature": 0.0, "avg_logprob": -0.21013079061136616, "compression_ratio": 1.7393939393939395, "no_speech_prob": 8.013197657419369e-06}, {"id": 612, "seek": 426500, "start": 4285.0, "end": 4288.0, "text": " And that defines a symbol called F.", "tokens": [400, 300, 23122, 257, 5986, 1219, 479, 13], "temperature": 0.0, "avg_logprob": -0.21013079061136616, "compression_ratio": 1.7393939393939395, "no_speech_prob": 8.013197657419369e-06}, {"id": 613, "seek": 426500, "start": 4288.0, "end": 4289.0, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.21013079061136616, "compression_ratio": 1.7393939393939395, "no_speech_prob": 8.013197657419369e-06}, {"id": 614, "seek": 428900, "start": 4289.0, "end": 4299.0, "text": " And the other way to define symbols is to import them. So in this case, Tim is not defined means you have not imported Tim.", "tokens": [400, 264, 661, 636, 281, 6964, 16944, 307, 281, 974, 552, 13, 407, 294, 341, 1389, 11, 7172, 307, 406, 7642, 1355, 291, 362, 406, 25524, 7172, 13], "temperature": 0.0, "avg_logprob": -0.24177658348752742, "compression_ratio": 1.3785714285714286, "no_speech_prob": 2.9767084924969822e-05}, {"id": 615, "seek": 428900, "start": 4299.0, "end": 4303.0, "text": " And that will solve your problem.", "tokens": [400, 300, 486, 5039, 428, 1154, 13], "temperature": 0.0, "avg_logprob": -0.24177658348752742, "compression_ratio": 1.3785714285714286, "no_speech_prob": 2.9767084924969822e-05}, {"id": 616, "seek": 428900, "start": 4303.0, "end": 4305.0, "text": " So make sense.", "tokens": [407, 652, 2020, 13], "temperature": 0.0, "avg_logprob": -0.24177658348752742, "compression_ratio": 1.3785714285714286, "no_speech_prob": 2.9767084924969822e-05}, {"id": 617, "seek": 428900, "start": 4305.0, "end": 4309.0, "text": " Yes, it does. But I.", "tokens": [1079, 11, 309, 775, 13, 583, 286, 13], "temperature": 0.0, "avg_logprob": -0.24177658348752742, "compression_ratio": 1.3785714285714286, "no_speech_prob": 2.9767084924969822e-05}, {"id": 618, "seek": 430900, "start": 4309.0, "end": 4324.0, "text": " Importing Tim and running that commander, I think it was still it will definitely work if you say import Tim this will definitely work. So I'd say you might have reset your kernel or something and hadn't really run that cell.", "tokens": [26391, 278, 7172, 293, 2614, 300, 17885, 11, 286, 519, 309, 390, 920, 309, 486, 2138, 589, 498, 291, 584, 974, 7172, 341, 486, 2138, 589, 13, 407, 286, 1116, 584, 291, 1062, 362, 14322, 428, 28256, 420, 746, 293, 8782, 380, 534, 1190, 300, 2815, 13], "temperature": 0.0, "avg_logprob": -0.2777971787886186, "compression_ratio": 1.7513812154696133, "no_speech_prob": 2.0143555957474746e-05}, {"id": 619, "seek": 430900, "start": 4324.0, "end": 4331.0, "text": " Yeah, if you say if you restart the kernel, it works. Yeah, if you say import module. Yeah.", "tokens": [865, 11, 498, 291, 584, 498, 291, 21022, 264, 28256, 11, 309, 1985, 13, 865, 11, 498, 291, 584, 974, 10088, 13, 865, 13], "temperature": 0.0, "avg_logprob": -0.2777971787886186, "compression_ratio": 1.7513812154696133, "no_speech_prob": 2.0143555957474746e-05}, {"id": 620, "seek": 433100, "start": 4331.0, "end": 4346.0, "text": " Oh yeah, I mean the other possibility is that you might have got a different message which is something like this module not found and module not found means yeah either you haven't installed it, or if you have installed it, you might need to restart your", "tokens": [876, 1338, 11, 286, 914, 264, 661, 7959, 307, 300, 291, 1062, 362, 658, 257, 819, 3636, 597, 307, 746, 411, 341, 10088, 406, 1352, 293, 10088, 406, 1352, 1355, 1338, 2139, 291, 2378, 380, 8899, 309, 11, 420, 498, 291, 362, 8899, 309, 11, 291, 1062, 643, 281, 21022, 428], "temperature": 0.0, "avg_logprob": -0.1945984015304051, "compression_ratio": 1.7488584474885844, "no_speech_prob": 3.844648745143786e-06}, {"id": 621, "seek": 433100, "start": 4346.0, "end": 4351.0, "text": " kernel by clicking kernel restart.", "tokens": [28256, 538, 9697, 28256, 21022, 13], "temperature": 0.0, "avg_logprob": -0.1945984015304051, "compression_ratio": 1.7488584474885844, "no_speech_prob": 3.844648745143786e-06}, {"id": 622, "seek": 433100, "start": 4351.0, "end": 4356.0, "text": " So it can like recheck what modules you have since you just installed it.", "tokens": [407, 309, 393, 411, 319, 15723, 437, 16679, 291, 362, 1670, 291, 445, 8899, 309, 13], "temperature": 0.0, "avg_logprob": -0.1945984015304051, "compression_ratio": 1.7488584474885844, "no_speech_prob": 3.844648745143786e-06}, {"id": 623, "seek": 433100, "start": 4356.0, "end": 4358.0, "text": " That's what I had.", "tokens": [663, 311, 437, 286, 632, 13], "temperature": 0.0, "avg_logprob": -0.1945984015304051, "compression_ratio": 1.7488584474885844, "no_speech_prob": 3.844648745143786e-06}, {"id": 624, "seek": 435800, "start": 4358.0, "end": 4363.0, "text": " Great.", "tokens": [3769, 13], "temperature": 0.0, "avg_logprob": -0.22224197387695313, "compression_ratio": 1.4732824427480915, "no_speech_prob": 1.519752277090447e-05}, {"id": 625, "seek": 435800, "start": 4363.0, "end": 4366.0, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.22224197387695313, "compression_ratio": 1.4732824427480915, "no_speech_prob": 1.519752277090447e-05}, {"id": 626, "seek": 435800, "start": 4366.0, "end": 4372.0, "text": " All right, well that was pretty successful, even if paper space wasn't.", "tokens": [1057, 558, 11, 731, 300, 390, 1238, 4406, 11, 754, 498, 3035, 1901, 2067, 380, 13], "temperature": 0.0, "avg_logprob": -0.22224197387695313, "compression_ratio": 1.4732824427480915, "no_speech_prob": 1.519752277090447e-05}, {"id": 627, "seek": 435800, "start": 4372.0, "end": 4378.0, "text": " Thanks guys. And see you. Yeah, see you tomorrow.", "tokens": [2561, 1074, 13, 400, 536, 291, 13, 865, 11, 536, 291, 4153, 13], "temperature": 0.0, "avg_logprob": -0.22224197387695313, "compression_ratio": 1.4732824427480915, "no_speech_prob": 1.519752277090447e-05}, {"id": 628, "seek": 435800, "start": 4378.0, "end": 4380.0, "text": " Thanks joining.", "tokens": [2561, 5549, 13], "temperature": 0.0, "avg_logprob": -0.22224197387695313, "compression_ratio": 1.4732824427480915, "no_speech_prob": 1.519752277090447e-05}, {"id": 629, "seek": 438000, "start": 4380.0, "end": 4388.0, "text": " Thanks Jeremy. Thank you. Thank you.", "tokens": [50364, 2561, 17809, 13, 1044, 291, 13, 1044, 291, 13, 50764], "temperature": 0.0, "avg_logprob": -0.30017610390981037, "compression_ratio": 1.1612903225806452, "no_speech_prob": 9.292345930589363e-05}], "language": "en"}