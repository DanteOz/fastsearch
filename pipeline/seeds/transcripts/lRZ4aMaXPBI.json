{"text": " Let's get started. And I wanted to start today by revisiting some of the stuff we talked about last time, both to review and to kind of fill out some details of things I had mentioned, but that we didn't get into. So this is the Notebook 2b, odds and ends, from the GitHub repo. So we talked last time about this kind of debate within linguistics or within computational linguistics. And I linked to a blog post by Peter Norvig, Google's hud of research, who was disagreeing with Noam Chomsky, the father of linguistics. And he referenced a paper by Leo Breiman called The Two Cultures. But talking about this tension between modeling the underlying mechanism of a phenomena and using machine learning to predict outputs, but knowing that the way you come up with those outputs don't necessarily model what's happening within the system you're looking at. And I gave, as an example, I mentioned, I just wanted to show the diagram from it, some research I had done a long time ago looking at, this is a process that happens within the cell, one carbon cell metabolism. And here, the rectangles are representing different substrates. The ovals represent different enzymes. You have all these different reactions. And this school of research was building computer models that have a bunch of PDEs showing how these reactions happen. And then to give one example from the paper, this could be the velocity for one reaction. It's this complicated equation. And there could be a whole paper by biologists identifying what these different constants and values are. But in the end, we were trying to see if we could replicate some clinical results in children with autism and Down syndrome. And so it was this very try to model what we think the underlying mechanism is to see if we can match these outputs. And this is very different than the approach you would take in machine learning, which would be to choose a model, perhaps a neural net, and say, OK, we know these are the inputs. We know these are the outputs. We want to see if we can recreate that final output without necessarily having the mechanism. Are there any questions on that distinction between these two approaches? And then also, I said last time, they're not necessarily mutually exclusive. There are ways to incorporate structure. So for instance, convolutions in a CNN, a convolutional neural network, are a way of putting some structure of what we think about recognizing pictures into your underlying model. The next step, while we were talking about stop words, I thought of this map I had seen on Twitter showing the most popular word in each state. You can see it's the in every state. I just thought this is a little bit funny. It's a joke. The here, I thought about this in the context of stop words and said, OK, this is a case where you would want to remove them. Although really, you would also probably be wanting to look at which words are disproportionately common in a state compared to being uncommon in other states. And then I'll ask you a question. Does anyone remember? Actually, yeah, what are stemming and lemmatization? Review from last time. Yes, and do you want the catch box? Up to you. OK, I'll come over. Stemming is pretty much like getting the roots of words, right? Correct, yes. Lemmatization is kind of, in a way, stemming, but not quite there. Well, which one is fancier? Lemmatization. Right, lemmatization is fancier. We had the quote, stemming is the poor man's lemmatization. And so really, I guess you could think of both of them as getting roots for words. Stemming is more kind of chopping off the end. And we saw one of the examples I had you do last time was looking at the words organizing, organizer, organize. Do you remember what the stem of those was? Organ. Organ, right, which is kind of a completely different word. Thank you. Yeah. OK, yeah, so we saw stemming and lemmatization. Does anyone remember when you want to use stemming or lemmatization versus not? Quinn? Stemming is a little less like computationally expensive. It's really good, more like the lemmatization needs a lot more rules, because you can't just have to remember that it's not lemmatization. Yes. Yeah, and so to repeat this for the microphone, lemmatization is more computationally expensive than stemming, which is true. Stemming is quicker and easier. In terms of using these kind of preprocessing and removing stop words and doing some form of stemming or lemmatization, when do you want to do that versus not at all? Or do you always want to do that type of preprocessing? Do you have less data? Yes, so the answer is when you have less data. You want to do these also related? Any other? Yes? Exactly. Do you think your model can handle the complexity? And so if you have a model that is more complex and can handle more complexity, like a neural network, you probably don't want to remove stop words or use stemming or lemmatization, because that's throwing away information. However, if you have a simpler model, you probably don't want to use the same model. So you want to use a more complex model. However, if you have a simpler model, you can't really learn as much detail or complexity. And so you do want to do this preprocessing. Cool. So next step, something I wish it set up more last time, is I want to talk to you about how factorization is analogous to matrix decomposition. So I kind of jumped into we used SVD or NMF to decompose our matrices. But I wanted to just provide a little bit more motivation of why that's something we do. And so looking at a much simpler case, which is actually make this a little bit bigger for you, which is multiplication. You can multiply numbers together. So 2 times 2 times 3 times 3 times 2 times 2 is 144. We've learned about multiplication a very long time ago. And then the reverse is prime factorization. So you could take 144 and say, I want to know what its prime factors are. And you can think of this as a way of kind of decomposing an integer into something else. Or you could say factorization is the, quote, opposite or inverse of multiplication. And here, the benefit of doing that is that the factors have this nice property of being prime. So 144 is not prime, but you've found a way to represent it using prime factors. Factorization is much harder than multiplication, which is good because this is the heart of encryption techniques. But you can see, even though we're kind of doing something that's, OK, this is like the opposite of multiplication, it's a much harder process to do. Any questions about that? OK, and so then this is very analogous to what we want to do in a matrix decomposition, which is you can think of a matrix decomposition as a way of taking apart a matrix. And it's the opposite of either matrix multiplication or, in some cases, matrix addition. It's much harder than matrix multiplication to come up with these components. And the reason it's nice is that typically, the matrices you're decomposing into have nice properties. So above, in this case of factorization, we had this nice property of being prime. And 144 is not prime, but we can represent it as a product of prime numbers. And so that's the same thing going on in a matrix decomposition is let's get some nice properties for these matrices. I showed another example from computer vision of here, you have this matrix on the left of people walking through a video. And you want to identify what's the background, what are the people. And so the goal is to decompose it into two different matrices that, in this case, you're going to sum together. But these have nice properties. So in this case, this matrix here of the background is said to be low rank. Oh, OK, this is a little misleading. Really, you would have the whole video, and this would represent a single row. But this one on the far right would give you a sparse matrix. So what are the nice properties that the matrices we get from SVD are? So let's start with you. What nice property does you have? It's orthonormal? Yeah, it's orthonormal or the depending if it's the column. I always have to look up if it's the columns or the rows or orthonormal to each other. And what does it mean to be orthonormal? The columns are orthogonal to each other. Yeah, so the answer is that the columns are orthogonal to each other and pairwise normalized. So that means if you take the dot product of two different columns, you get 0. If you take the dot product of a column with itself, you get 1. How about S, the middle matrix? Yes, S is diagonal is the key here. What other properties? And just to remind you, being diagonal means that everything off the diagonal is 0. Anything you can say about the numbers on the diagonal? They're like the principal components of the original matrix. So if they encode for something important about the original matrix. They're kind of capturing an idea of importance. And so they're called singular values. They're in descending order, so you kind of have the biggest ones first. They're non-negative. But yeah, they've captured some importance. Because you'll remember the scale of u and v is both 1 and that they're normalized. I guess that was a little bit of a spoiler about v. V has the same properties as u, only on the transpose of the rows being the same. So the transpose of the rows being orthonormal to each other. Any questions about this? And then nmf, as the name suggests, what's the special property of the matrices you get there in your decomposition? Anyone want to shout it out? What does nmf stand for? Yeah, non-negative matrix factorization. So the matrices you get are non-negative. And they also, in practice, often end up being sparse. And we'll talk a little bit more about sparse matrices today. But that means that many of the values are 0. And so just keep in mind this analogy with prime factorization is something that we're doing in a kind of more complicated way with matrices. Next, I wanted to just review a little bit of linear algebra, since I know it may have been a while, since you've seen some linear algebra. This is a link I like just for matrix vector multiplication. And it's matrixmultiplication.xyz. But here, and you can change these numbers if you wanted them to be something different or change the size of your matrices that you're multiplying. Actually, I guess I said matrix vector. This kind of shows you step by step kind of how you're sliding this vector along. And this fits with the interpretation I had mentioned last time about thinking of multiplying a matrix by a vector is like taking a linear combination of the columns in that matrix. And here, we've multiplied 3 by the first column, 6 by the second column, 1 by the third column. And then we can also see that with two matrices, how we're taking a linear combination of 3 by the first column, 6 by the second, 1 by the third. And we're taking a second linear combination of 1, 1, 1, times the three columns. So I thought that was just a nice way to see it visually. And then last time, I showed this example that comes from computer vision, not NLP, but of having a bunch of pictures of faces and decomposing them into their facial features. Here, NMF has been used. And then so one of the matrices would represent the different faces. And then the other one is the matrix vector. And matrices would represent the different features you could have. In this case, it's kind of like a particular bridge of your nose and under your eyes. This one looks like just the tip of a nose. This is someone's brows. And in general, you'd have a lot more of these. And then you could take a linear combination. And you could think of that as giving you kind of the different importances or weights of these different features. And then who here has watched some 3Blue1Brown videos? OK, good number, but not everybody, which is just sad because I love 3Blue1Brown. Grant Sanderson has done a bunch of videos on a wide variety of math and computer science context. I wanted to show you one today. This is from the Essence of Linear Algebra series. And it's called the Essence of Linear Algebra Series. Which I wanted to make a plug for. It gives a very visual and geometric perspective on linear algebra that I think is very different from how linear algebra is typically taught. And something I like about these videos is I think they would be great if you're a linear algebra beginner or you're feeling very rusty about linear algebra. But also even if someone who knows a lot of linear algebra, I think they can give you a new perspective. And so I just wanted us to watch one of the videos together. All right, so I just wanted to share that as some linear algebra review. If you're interested in kind of thinking about how this relates to SVD, I definitely recommend looking at the Change of Basis video as well. And I'll see if we have kind of time to get more into that later in the course or not. For now I wanted to bring up the spreadsheet again. There was a question last time about how I generated all the numbers for the spreadsheet. And just to remind you, this figure. Okay. This was to give you kind of a more visual way of seeing what's happening with matrix decompositions like SVD or NMF. And here I was using data from a collection of British novels with just I think 64 vocabulary words representing them as a term document matrix. So here each row stands for one book. They're named by the author's last name and then the beginning of the book title. This is Jane Austen's Pride and Prejudice. And yeah, showing. And here they've been normalized using something called TFIDF. And that I'll ask you to compute in the homework. So kind of a way for representing the book is this one matrix. Then we can use SVD to decompose this into three matrices. And with SVD, we would actually get, I guess, 64 singular values. S would be a 64 by 64 matrix. But I've just chosen the top 10. The nice thing about having these singular values in descending order is that kind of we're getting the most important ones whenever we pick off the top part. And because of our application, we can think of these as corresponding to various topics and how important each topic is. And the matrix on the left, U, is the various books by topics and then the matrix on the right, V, is giving us the topics by words. And we can see something. Let's look at maybe Catherine. Is Catherine large anywhere? Topic six looks like where Catherine has the biggest magnitude. If we go back over to topic six. So not the most important, but still up there. And we can see what books have a lot of topics six. This Life and Times of Tristram Shandy. Here, this is pretty large, 0.5665 for Emily Bronte's Weathering Heights. Catherine's a character in there. So the word is showing up more. So this is kind of just to give you a sense of how you can interpret these matrices. First, I'll ask, are there any questions just about the Excel spreadsheet? Okay. And so then in the notebook, per your request, I've included the code that I used to create this. So I used Python to actually process the data and create these matrices. And you can go through this on your own. It should be pretty straightforward. I exported them to CSVs, and then that's what I was looking at in Excel. But if you want to see where those numbers came from, this is the code to get you there, both for NMF and for SVD. Actually, first I should, yes? So for the topics, the best way to understand them is just to look at the top vocabulary for topics? Yes. Yeah. So the question was, for the topics, the best way to understand them is by looking at the vocabulary words. And I would say yes, because that's really kind of all you have to go on. I mean, I guess potentially the other thing you could do would be to look at different books and say, okay, this book has a lot of this topic. Is it similar to these other books? Actually, let's look at that. Let's see if this, yeah, in this case, I think that's a little bit trickier to do because it looks like there are a lot of these that have around 0.2 of topic one. So to me, this seems less informative. I don't know if any of the later ones, you might get something a little more indicative. Okay, here, sentence sensibility has a lot of topic four. Let's see if any others do. I guess Emma also. So Emma and sentence sensibility are biggest on topic four. So that could raise you to ask, like, okay, what do Emma and sentence sensibility have in common that these other books don't? Although even then, I think I'd probably still want to kind of confirm that bit by saying, like, okay, what words do those involve? In this case, let's see. Eleanor. So I would guess that Eleanor is a character in both, although I would have to have to look that up. Yeah, that's kind of what's jumping out most. Also Marion. Yeah, and in some ways, this is a bit primitive. You know, this is a bag of words approach. We're kind of representing these books just as bags of words. So it's not, you know, we're not going to get anything about, like, the structure of the plot making these books similar. It's really just looking at vocabulary words. All right, then, and actually, let me just ask you. So what did we see last time with, I don't know, advantages, disadvantages of SVD versus NMF? Yes? One advantage with NMF is you get it because you get non-negative values, it's a little bit more interpretable. Okay, yeah. So to repeat that, NMF is their non-negative values. Those can be more interpretable. Because here, you know, with SVD, we are seeing, like, you could have negative values for the topic. And so what does it mean for this to, yeah, be negative above its topic? Yes? SVD was an exact decomposition. Great point. Yes, SVD is an exact decomposition. So you can fully represent your input matrices, whereas non-negative factorization is not exact. Any others? So one thing about NMF, actually, I don't know if this is a positive or a negative. You have to set the number of topics that you wanted. That's a hyperparameter that you're choosing. With SVD, if you do kind of the full decomposition, well, I should say, if you do a traditional SVD, you're getting as many singular values as you had documents or whatever your smaller dimension is. But I'm assuming you have fewer documents than you have vocabulary words. So SVD, there is the opportunity to look at the singular values and see, like, okay, when they get really little, these topics might be so unimportant that I can ignore them and chop them off. But that also means with SVD that you're kind of doing extra work. So it's both giving you more information and it's extra work. So something we talked about then towards the end, but didn't get too into detail on. Well, one, SVD on a big matrix is slow. So here, and this is from a talk I gave at Pi Bay two years ago. I ran some timing tests for the speed of running SVD. And it, I think this is in seconds, very quickly. So here with a 10,000 by 10,000 matrix, it's just so slow. It's not something you really want to be doing. And one way to address this is randomized SVD. And basically randomized SVD is using, it's a little bit more complex under the hood, but part of it is taking this, just multiplying by a random matrix that has a smaller dimension to get a matrix with a smaller dimension that you'll work on. And that surprisingly, you still keep a lot of the information you need and it can be very effective. And so here I've plotted what the air is when reconstructing the matrix. And so, and actually let me maybe draw this. So with SVD, you've got some matrix that you start with, and then you're decomposing that into matrix U, S, and V. And, you know, maybe these start with large singular values, five, I don't know, 4.5. And then down here, you're getting like really little ones, 0.1, 0.01, that may not have that much information. So you could just say, okay, let me chop these off, not use them. Correspondingly, that will also mean you're going to be chopping off, I guess, these last few columns of U. Is this right? Yeah. And then last few rows of V. No. So I'm just thinking about to get the matrix dimensions to line up. So A has the shape in this case. A has the shape like this. So you still want to get back that wider shape. Okay. So V, I think you could keep. But you're throwing away information from U and S. And so if you do that, you're no longer getting A is exactly equal to this, but we've thrown away some information. And so now let's say this is just approximately equal to A. But what you've thrown away had pretty small magnitude since the singular values were a small magnitude. Francesca, is there a question around this? So what I was doing in this chart here is looking K is the number of singular values I kept. And so I was saying, you know, if you throw away K, how close are you to reconstructing your original matrix A? And so here the air is these dotted lines and you can see, okay, if I only kept 100 singular values, the air was much higher. But as I was getting out there, and I think, yeah, I'd have to check. I would have to look up what the original size was. Kind of you reach a point where you stop really seeing improvements. And the neat thing, though, is that this was the same both for randomized SVD and regular SVD. So like randomized SVD, the air is red, SVD it's in purple. It's a little bit hard to see. You have slightly lower air for the regular SVD, but not by a lot. But you're getting a huge speed up by doing randomized SVD. So the speed for randomized SVD is here in blue. Green is the speed for the regular one. So a nice way to kind of get a speed up without losing that much information. And in practice, you often are not using all your singular values, particularly the small ones. And this is a topic that maybe if we have time more later in the course, we might come back to. So I kind of have to see how long everything takes. Then I also wanted to address full versus reduced SVD briefly. Just when we were using NumPy's SVD last time, we would call np.linouge.svd. We were passing in full matrices equals false. And I wanted to show you what difference that makes. And this is for traditional SVD. But the full SVD is going to have U and V both be square matrices. And so that involves basically making up some columns for U that don't directly depend on the data in A or whatever matrix you're decomposing, because you're also adding some rows of pure zeros to your singular values, which will zero out with here. But the way that U is being made into a square matrix is to have it form an orthonormal basis. And so that means spanning the whole space. And I talked a little bit about spanning spaces in the 3Blue1Brown video we just watched. But if you want to completely fill or be able to represent anything in n-dimensional space, or I guess in this case, m-dimensional space, you would have to kind of fill out those spaces. And so I just wanted to let you know that this is a thing in practice. I'd say I think you're usually going to be using reduced SVD. It's quicker to calculate, and you're often not needing to use to turn you into an orthonormal basis. So that's the meaning of that parameter here, full matrices. Let me check, I'm going on time. All right. Any final questions on topic modeling before we move on to logistic regression? The naive phase. Okay, so let's start Notebook 3.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 2.8000000000000003, "text": " Let's get started.", "tokens": [961, 311, 483, 1409, 13], "temperature": 0.0, "avg_logprob": -0.21965378209164269, "compression_ratio": 1.5797101449275361, "no_speech_prob": 0.002630392089486122}, {"id": 1, "seek": 0, "start": 2.8000000000000003, "end": 7.2, "text": " And I wanted to start today by revisiting some of the stuff we", "tokens": [400, 286, 1415, 281, 722, 965, 538, 20767, 1748, 512, 295, 264, 1507, 321], "temperature": 0.0, "avg_logprob": -0.21965378209164269, "compression_ratio": 1.5797101449275361, "no_speech_prob": 0.002630392089486122}, {"id": 2, "seek": 0, "start": 7.2, "end": 10.68, "text": " talked about last time, both to review and to kind of fill out", "tokens": [2825, 466, 1036, 565, 11, 1293, 281, 3131, 293, 281, 733, 295, 2836, 484], "temperature": 0.0, "avg_logprob": -0.21965378209164269, "compression_ratio": 1.5797101449275361, "no_speech_prob": 0.002630392089486122}, {"id": 3, "seek": 0, "start": 10.68, "end": 13.040000000000001, "text": " some details of things I had mentioned,", "tokens": [512, 4365, 295, 721, 286, 632, 2835, 11], "temperature": 0.0, "avg_logprob": -0.21965378209164269, "compression_ratio": 1.5797101449275361, "no_speech_prob": 0.002630392089486122}, {"id": 4, "seek": 0, "start": 13.040000000000001, "end": 16.240000000000002, "text": " but that we didn't get into.", "tokens": [457, 300, 321, 994, 380, 483, 666, 13], "temperature": 0.0, "avg_logprob": -0.21965378209164269, "compression_ratio": 1.5797101449275361, "no_speech_prob": 0.002630392089486122}, {"id": 5, "seek": 0, "start": 16.240000000000002, "end": 20.2, "text": " So this is the Notebook 2b, odds and ends,", "tokens": [407, 341, 307, 264, 11633, 2939, 568, 65, 11, 17439, 293, 5314, 11], "temperature": 0.0, "avg_logprob": -0.21965378209164269, "compression_ratio": 1.5797101449275361, "no_speech_prob": 0.002630392089486122}, {"id": 6, "seek": 0, "start": 20.2, "end": 23.56, "text": " from the GitHub repo.", "tokens": [490, 264, 23331, 49040, 13], "temperature": 0.0, "avg_logprob": -0.21965378209164269, "compression_ratio": 1.5797101449275361, "no_speech_prob": 0.002630392089486122}, {"id": 7, "seek": 0, "start": 23.56, "end": 27.36, "text": " So we talked last time about this kind of debate", "tokens": [407, 321, 2825, 1036, 565, 466, 341, 733, 295, 7958], "temperature": 0.0, "avg_logprob": -0.21965378209164269, "compression_ratio": 1.5797101449275361, "no_speech_prob": 0.002630392089486122}, {"id": 8, "seek": 2736, "start": 27.36, "end": 32.44, "text": " within linguistics or within computational linguistics.", "tokens": [1951, 21766, 6006, 420, 1951, 28270, 21766, 6006, 13], "temperature": 0.0, "avg_logprob": -0.14375451632908412, "compression_ratio": 1.4782608695652173, "no_speech_prob": 0.00012699406943283975}, {"id": 9, "seek": 2736, "start": 32.44, "end": 36.72, "text": " And I linked to a blog post by Peter Norvig,", "tokens": [400, 286, 9408, 281, 257, 6968, 2183, 538, 6508, 6966, 85, 328, 11], "temperature": 0.0, "avg_logprob": -0.14375451632908412, "compression_ratio": 1.4782608695652173, "no_speech_prob": 0.00012699406943283975}, {"id": 10, "seek": 2736, "start": 36.72, "end": 40.879999999999995, "text": " Google's hud of research, who was disagreeing", "tokens": [3329, 311, 276, 532, 295, 2132, 11, 567, 390, 14091, 278], "temperature": 0.0, "avg_logprob": -0.14375451632908412, "compression_ratio": 1.4782608695652173, "no_speech_prob": 0.00012699406943283975}, {"id": 11, "seek": 2736, "start": 40.879999999999995, "end": 43.72, "text": " with Noam Chomsky, the father of linguistics.", "tokens": [365, 883, 335, 761, 4785, 4133, 11, 264, 3086, 295, 21766, 6006, 13], "temperature": 0.0, "avg_logprob": -0.14375451632908412, "compression_ratio": 1.4782608695652173, "no_speech_prob": 0.00012699406943283975}, {"id": 12, "seek": 2736, "start": 43.72, "end": 49.04, "text": " And he referenced a paper by Leo Breiman", "tokens": [400, 415, 32734, 257, 3035, 538, 19344, 7090, 25504], "temperature": 0.0, "avg_logprob": -0.14375451632908412, "compression_ratio": 1.4782608695652173, "no_speech_prob": 0.00012699406943283975}, {"id": 13, "seek": 2736, "start": 49.04, "end": 51.36, "text": " called The Two Cultures.", "tokens": [1219, 440, 4453, 41550, 1303, 13], "temperature": 0.0, "avg_logprob": -0.14375451632908412, "compression_ratio": 1.4782608695652173, "no_speech_prob": 0.00012699406943283975}, {"id": 14, "seek": 2736, "start": 51.36, "end": 54.68, "text": " But talking about this tension between modeling", "tokens": [583, 1417, 466, 341, 8980, 1296, 15983], "temperature": 0.0, "avg_logprob": -0.14375451632908412, "compression_ratio": 1.4782608695652173, "no_speech_prob": 0.00012699406943283975}, {"id": 15, "seek": 5468, "start": 54.68, "end": 57.32, "text": " the underlying mechanism of a phenomena", "tokens": [264, 14217, 7513, 295, 257, 22004], "temperature": 0.0, "avg_logprob": -0.1053849220275879, "compression_ratio": 1.6425855513307985, "no_speech_prob": 3.372541686985642e-05}, {"id": 16, "seek": 5468, "start": 57.32, "end": 60.84, "text": " and using machine learning to predict outputs,", "tokens": [293, 1228, 3479, 2539, 281, 6069, 23930, 11], "temperature": 0.0, "avg_logprob": -0.1053849220275879, "compression_ratio": 1.6425855513307985, "no_speech_prob": 3.372541686985642e-05}, {"id": 17, "seek": 5468, "start": 60.84, "end": 63.24, "text": " but knowing that the way you come up with those outputs", "tokens": [457, 5276, 300, 264, 636, 291, 808, 493, 365, 729, 23930], "temperature": 0.0, "avg_logprob": -0.1053849220275879, "compression_ratio": 1.6425855513307985, "no_speech_prob": 3.372541686985642e-05}, {"id": 18, "seek": 5468, "start": 63.24, "end": 67.52, "text": " don't necessarily model what's happening within the system", "tokens": [500, 380, 4725, 2316, 437, 311, 2737, 1951, 264, 1185], "temperature": 0.0, "avg_logprob": -0.1053849220275879, "compression_ratio": 1.6425855513307985, "no_speech_prob": 3.372541686985642e-05}, {"id": 19, "seek": 5468, "start": 67.52, "end": 69.0, "text": " you're looking at.", "tokens": [291, 434, 1237, 412, 13], "temperature": 0.0, "avg_logprob": -0.1053849220275879, "compression_ratio": 1.6425855513307985, "no_speech_prob": 3.372541686985642e-05}, {"id": 20, "seek": 5468, "start": 69.0, "end": 71.03999999999999, "text": " And I gave, as an example, I mentioned,", "tokens": [400, 286, 2729, 11, 382, 364, 1365, 11, 286, 2835, 11], "temperature": 0.0, "avg_logprob": -0.1053849220275879, "compression_ratio": 1.6425855513307985, "no_speech_prob": 3.372541686985642e-05}, {"id": 21, "seek": 5468, "start": 71.03999999999999, "end": 73.32, "text": " I just wanted to show the diagram from it,", "tokens": [286, 445, 1415, 281, 855, 264, 10686, 490, 309, 11], "temperature": 0.0, "avg_logprob": -0.1053849220275879, "compression_ratio": 1.6425855513307985, "no_speech_prob": 3.372541686985642e-05}, {"id": 22, "seek": 5468, "start": 73.32, "end": 76.76, "text": " some research I had done a long time ago looking at,", "tokens": [512, 2132, 286, 632, 1096, 257, 938, 565, 2057, 1237, 412, 11], "temperature": 0.0, "avg_logprob": -0.1053849220275879, "compression_ratio": 1.6425855513307985, "no_speech_prob": 3.372541686985642e-05}, {"id": 23, "seek": 5468, "start": 76.76, "end": 81.8, "text": " this is a process that happens within the cell, one carbon", "tokens": [341, 307, 257, 1399, 300, 2314, 1951, 264, 2815, 11, 472, 5954], "temperature": 0.0, "avg_logprob": -0.1053849220275879, "compression_ratio": 1.6425855513307985, "no_speech_prob": 3.372541686985642e-05}, {"id": 24, "seek": 5468, "start": 81.8, "end": 84.32, "text": " cell metabolism.", "tokens": [2815, 31190, 13], "temperature": 0.0, "avg_logprob": -0.1053849220275879, "compression_ratio": 1.6425855513307985, "no_speech_prob": 3.372541686985642e-05}, {"id": 25, "seek": 8432, "start": 84.32, "end": 86.32, "text": " And here, the rectangles are representing", "tokens": [400, 510, 11, 264, 24077, 904, 366, 13460], "temperature": 0.0, "avg_logprob": -0.11615558423494038, "compression_ratio": 1.658008658008658, "no_speech_prob": 4.06799626944121e-05}, {"id": 26, "seek": 8432, "start": 86.32, "end": 87.63999999999999, "text": " different substrates.", "tokens": [819, 4594, 12507, 13], "temperature": 0.0, "avg_logprob": -0.11615558423494038, "compression_ratio": 1.658008658008658, "no_speech_prob": 4.06799626944121e-05}, {"id": 27, "seek": 8432, "start": 87.63999999999999, "end": 90.39999999999999, "text": " The ovals represent different enzymes.", "tokens": [440, 14187, 1124, 2906, 819, 29299, 13], "temperature": 0.0, "avg_logprob": -0.11615558423494038, "compression_ratio": 1.658008658008658, "no_speech_prob": 4.06799626944121e-05}, {"id": 28, "seek": 8432, "start": 90.39999999999999, "end": 92.75999999999999, "text": " You have all these different reactions.", "tokens": [509, 362, 439, 613, 819, 12215, 13], "temperature": 0.0, "avg_logprob": -0.11615558423494038, "compression_ratio": 1.658008658008658, "no_speech_prob": 4.06799626944121e-05}, {"id": 29, "seek": 8432, "start": 92.75999999999999, "end": 98.39999999999999, "text": " And this school of research was building computer models", "tokens": [400, 341, 1395, 295, 2132, 390, 2390, 3820, 5245], "temperature": 0.0, "avg_logprob": -0.11615558423494038, "compression_ratio": 1.658008658008658, "no_speech_prob": 4.06799626944121e-05}, {"id": 30, "seek": 8432, "start": 98.39999999999999, "end": 100.67999999999999, "text": " that have a bunch of PDEs showing", "tokens": [300, 362, 257, 3840, 295, 10464, 20442, 4099], "temperature": 0.0, "avg_logprob": -0.11615558423494038, "compression_ratio": 1.658008658008658, "no_speech_prob": 4.06799626944121e-05}, {"id": 31, "seek": 8432, "start": 100.67999999999999, "end": 103.11999999999999, "text": " how these reactions happen.", "tokens": [577, 613, 12215, 1051, 13], "temperature": 0.0, "avg_logprob": -0.11615558423494038, "compression_ratio": 1.658008658008658, "no_speech_prob": 4.06799626944121e-05}, {"id": 32, "seek": 8432, "start": 103.11999999999999, "end": 106.83999999999999, "text": " And then to give one example from the paper,", "tokens": [400, 550, 281, 976, 472, 1365, 490, 264, 3035, 11], "temperature": 0.0, "avg_logprob": -0.11615558423494038, "compression_ratio": 1.658008658008658, "no_speech_prob": 4.06799626944121e-05}, {"id": 33, "seek": 8432, "start": 106.83999999999999, "end": 108.88, "text": " this could be the velocity for one reaction.", "tokens": [341, 727, 312, 264, 9269, 337, 472, 5480, 13], "temperature": 0.0, "avg_logprob": -0.11615558423494038, "compression_ratio": 1.658008658008658, "no_speech_prob": 4.06799626944121e-05}, {"id": 34, "seek": 8432, "start": 108.88, "end": 111.6, "text": " It's this complicated equation.", "tokens": [467, 311, 341, 6179, 5367, 13], "temperature": 0.0, "avg_logprob": -0.11615558423494038, "compression_ratio": 1.658008658008658, "no_speech_prob": 4.06799626944121e-05}, {"id": 35, "seek": 11160, "start": 111.6, "end": 114.55999999999999, "text": " And there could be a whole paper by biologists", "tokens": [400, 456, 727, 312, 257, 1379, 3035, 538, 3228, 12256], "temperature": 0.0, "avg_logprob": -0.11533067246113927, "compression_ratio": 1.7224199288256228, "no_speech_prob": 1.5930938388919458e-05}, {"id": 36, "seek": 11160, "start": 114.55999999999999, "end": 117.72, "text": " identifying what these different constants and values are.", "tokens": [16696, 437, 613, 819, 35870, 293, 4190, 366, 13], "temperature": 0.0, "avg_logprob": -0.11533067246113927, "compression_ratio": 1.7224199288256228, "no_speech_prob": 1.5930938388919458e-05}, {"id": 37, "seek": 11160, "start": 117.72, "end": 119.22, "text": " But in the end, we were trying to see", "tokens": [583, 294, 264, 917, 11, 321, 645, 1382, 281, 536], "temperature": 0.0, "avg_logprob": -0.11533067246113927, "compression_ratio": 1.7224199288256228, "no_speech_prob": 1.5930938388919458e-05}, {"id": 38, "seek": 11160, "start": 119.22, "end": 122.39999999999999, "text": " if we could replicate some clinical results in children", "tokens": [498, 321, 727, 25356, 512, 9115, 3542, 294, 2227], "temperature": 0.0, "avg_logprob": -0.11533067246113927, "compression_ratio": 1.7224199288256228, "no_speech_prob": 1.5930938388919458e-05}, {"id": 39, "seek": 11160, "start": 122.39999999999999, "end": 124.47999999999999, "text": " with autism and Down syndrome.", "tokens": [365, 21471, 293, 9506, 19371, 13], "temperature": 0.0, "avg_logprob": -0.11533067246113927, "compression_ratio": 1.7224199288256228, "no_speech_prob": 1.5930938388919458e-05}, {"id": 40, "seek": 11160, "start": 124.47999999999999, "end": 128.24, "text": " And so it was this very try to model", "tokens": [400, 370, 309, 390, 341, 588, 853, 281, 2316], "temperature": 0.0, "avg_logprob": -0.11533067246113927, "compression_ratio": 1.7224199288256228, "no_speech_prob": 1.5930938388919458e-05}, {"id": 41, "seek": 11160, "start": 128.24, "end": 130.79999999999998, "text": " what we think the underlying mechanism is to see", "tokens": [437, 321, 519, 264, 14217, 7513, 307, 281, 536], "temperature": 0.0, "avg_logprob": -0.11533067246113927, "compression_ratio": 1.7224199288256228, "no_speech_prob": 1.5930938388919458e-05}, {"id": 42, "seek": 11160, "start": 130.79999999999998, "end": 132.44, "text": " if we can match these outputs.", "tokens": [498, 321, 393, 2995, 613, 23930, 13], "temperature": 0.0, "avg_logprob": -0.11533067246113927, "compression_ratio": 1.7224199288256228, "no_speech_prob": 1.5930938388919458e-05}, {"id": 43, "seek": 11160, "start": 132.44, "end": 134.32, "text": " And this is very different than the approach", "tokens": [400, 341, 307, 588, 819, 813, 264, 3109], "temperature": 0.0, "avg_logprob": -0.11533067246113927, "compression_ratio": 1.7224199288256228, "no_speech_prob": 1.5930938388919458e-05}, {"id": 44, "seek": 11160, "start": 134.32, "end": 136.56, "text": " you would take in machine learning, which", "tokens": [291, 576, 747, 294, 3479, 2539, 11, 597], "temperature": 0.0, "avg_logprob": -0.11533067246113927, "compression_ratio": 1.7224199288256228, "no_speech_prob": 1.5930938388919458e-05}, {"id": 45, "seek": 11160, "start": 136.56, "end": 140.44, "text": " would be to choose a model, perhaps a neural net,", "tokens": [576, 312, 281, 2826, 257, 2316, 11, 4317, 257, 18161, 2533, 11], "temperature": 0.0, "avg_logprob": -0.11533067246113927, "compression_ratio": 1.7224199288256228, "no_speech_prob": 1.5930938388919458e-05}, {"id": 46, "seek": 14044, "start": 140.44, "end": 142.2, "text": " and say, OK, we know these are the inputs.", "tokens": [293, 584, 11, 2264, 11, 321, 458, 613, 366, 264, 15743, 13], "temperature": 0.0, "avg_logprob": -0.14817720140729632, "compression_ratio": 1.6352459016393444, "no_speech_prob": 1.921632247103844e-05}, {"id": 47, "seek": 14044, "start": 142.2, "end": 144.07999999999998, "text": " We know these are the outputs.", "tokens": [492, 458, 613, 366, 264, 23930, 13], "temperature": 0.0, "avg_logprob": -0.14817720140729632, "compression_ratio": 1.6352459016393444, "no_speech_prob": 1.921632247103844e-05}, {"id": 48, "seek": 14044, "start": 144.07999999999998, "end": 146.2, "text": " We want to see if we can recreate", "tokens": [492, 528, 281, 536, 498, 321, 393, 25833], "temperature": 0.0, "avg_logprob": -0.14817720140729632, "compression_ratio": 1.6352459016393444, "no_speech_prob": 1.921632247103844e-05}, {"id": 49, "seek": 14044, "start": 146.2, "end": 149.0, "text": " that final output without necessarily", "tokens": [300, 2572, 5598, 1553, 4725], "temperature": 0.0, "avg_logprob": -0.14817720140729632, "compression_ratio": 1.6352459016393444, "no_speech_prob": 1.921632247103844e-05}, {"id": 50, "seek": 14044, "start": 149.0, "end": 151.52, "text": " having the mechanism.", "tokens": [1419, 264, 7513, 13], "temperature": 0.0, "avg_logprob": -0.14817720140729632, "compression_ratio": 1.6352459016393444, "no_speech_prob": 1.921632247103844e-05}, {"id": 51, "seek": 14044, "start": 151.52, "end": 153.92, "text": " Are there any questions on that distinction", "tokens": [2014, 456, 604, 1651, 322, 300, 16844], "temperature": 0.0, "avg_logprob": -0.14817720140729632, "compression_ratio": 1.6352459016393444, "no_speech_prob": 1.921632247103844e-05}, {"id": 52, "seek": 14044, "start": 153.92, "end": 157.68, "text": " between these two approaches?", "tokens": [1296, 613, 732, 11587, 30], "temperature": 0.0, "avg_logprob": -0.14817720140729632, "compression_ratio": 1.6352459016393444, "no_speech_prob": 1.921632247103844e-05}, {"id": 53, "seek": 14044, "start": 157.68, "end": 162.04, "text": " And then also, I said last time, they're not necessarily", "tokens": [400, 550, 611, 11, 286, 848, 1036, 565, 11, 436, 434, 406, 4725], "temperature": 0.0, "avg_logprob": -0.14817720140729632, "compression_ratio": 1.6352459016393444, "no_speech_prob": 1.921632247103844e-05}, {"id": 54, "seek": 14044, "start": 162.04, "end": 163.72, "text": " mutually exclusive.", "tokens": [39144, 13005, 13], "temperature": 0.0, "avg_logprob": -0.14817720140729632, "compression_ratio": 1.6352459016393444, "no_speech_prob": 1.921632247103844e-05}, {"id": 55, "seek": 14044, "start": 163.72, "end": 166.44, "text": " There are ways to incorporate structure.", "tokens": [821, 366, 2098, 281, 16091, 3877, 13], "temperature": 0.0, "avg_logprob": -0.14817720140729632, "compression_ratio": 1.6352459016393444, "no_speech_prob": 1.921632247103844e-05}, {"id": 56, "seek": 14044, "start": 166.44, "end": 170.07999999999998, "text": " So for instance, convolutions in a CNN,", "tokens": [407, 337, 5197, 11, 3754, 15892, 294, 257, 24859, 11], "temperature": 0.0, "avg_logprob": -0.14817720140729632, "compression_ratio": 1.6352459016393444, "no_speech_prob": 1.921632247103844e-05}, {"id": 57, "seek": 17008, "start": 170.08, "end": 172.32000000000002, "text": " a convolutional neural network, are a way", "tokens": [257, 45216, 304, 18161, 3209, 11, 366, 257, 636], "temperature": 0.0, "avg_logprob": -0.16723398367563883, "compression_ratio": 1.5714285714285714, "no_speech_prob": 6.240156835701782e-06}, {"id": 58, "seek": 17008, "start": 172.32000000000002, "end": 175.16000000000003, "text": " of putting some structure of what", "tokens": [295, 3372, 512, 3877, 295, 437], "temperature": 0.0, "avg_logprob": -0.16723398367563883, "compression_ratio": 1.5714285714285714, "no_speech_prob": 6.240156835701782e-06}, {"id": 59, "seek": 17008, "start": 175.16000000000003, "end": 177.48000000000002, "text": " we think about recognizing pictures", "tokens": [321, 519, 466, 18538, 5242], "temperature": 0.0, "avg_logprob": -0.16723398367563883, "compression_ratio": 1.5714285714285714, "no_speech_prob": 6.240156835701782e-06}, {"id": 60, "seek": 17008, "start": 177.48000000000002, "end": 179.4, "text": " into your underlying model.", "tokens": [666, 428, 14217, 2316, 13], "temperature": 0.0, "avg_logprob": -0.16723398367563883, "compression_ratio": 1.5714285714285714, "no_speech_prob": 6.240156835701782e-06}, {"id": 61, "seek": 17008, "start": 183.08, "end": 188.20000000000002, "text": " The next step, while we were talking about stop words,", "tokens": [440, 958, 1823, 11, 1339, 321, 645, 1417, 466, 1590, 2283, 11], "temperature": 0.0, "avg_logprob": -0.16723398367563883, "compression_ratio": 1.5714285714285714, "no_speech_prob": 6.240156835701782e-06}, {"id": 62, "seek": 17008, "start": 188.20000000000002, "end": 191.24, "text": " I thought of this map I had seen on Twitter showing", "tokens": [286, 1194, 295, 341, 4471, 286, 632, 1612, 322, 5794, 4099], "temperature": 0.0, "avg_logprob": -0.16723398367563883, "compression_ratio": 1.5714285714285714, "no_speech_prob": 6.240156835701782e-06}, {"id": 63, "seek": 17008, "start": 191.24, "end": 193.96, "text": " the most popular word in each state.", "tokens": [264, 881, 3743, 1349, 294, 1184, 1785, 13], "temperature": 0.0, "avg_logprob": -0.16723398367563883, "compression_ratio": 1.5714285714285714, "no_speech_prob": 6.240156835701782e-06}, {"id": 64, "seek": 17008, "start": 193.96, "end": 196.64000000000001, "text": " You can see it's the in every state.", "tokens": [509, 393, 536, 309, 311, 264, 294, 633, 1785, 13], "temperature": 0.0, "avg_logprob": -0.16723398367563883, "compression_ratio": 1.5714285714285714, "no_speech_prob": 6.240156835701782e-06}, {"id": 65, "seek": 17008, "start": 196.64000000000001, "end": 198.8, "text": " I just thought this is a little bit funny.", "tokens": [286, 445, 1194, 341, 307, 257, 707, 857, 4074, 13], "temperature": 0.0, "avg_logprob": -0.16723398367563883, "compression_ratio": 1.5714285714285714, "no_speech_prob": 6.240156835701782e-06}, {"id": 66, "seek": 19880, "start": 198.8, "end": 201.16000000000003, "text": " It's a joke.", "tokens": [467, 311, 257, 7647, 13], "temperature": 0.0, "avg_logprob": -0.18930310619120694, "compression_ratio": 1.5594713656387664, "no_speech_prob": 2.3916383725008927e-05}, {"id": 67, "seek": 19880, "start": 201.16000000000003, "end": 206.24, "text": " The here, I thought about this in the context of stop words", "tokens": [440, 510, 11, 286, 1194, 466, 341, 294, 264, 4319, 295, 1590, 2283], "temperature": 0.0, "avg_logprob": -0.18930310619120694, "compression_ratio": 1.5594713656387664, "no_speech_prob": 2.3916383725008927e-05}, {"id": 68, "seek": 19880, "start": 206.24, "end": 207.68, "text": " and said, OK, this is a case where", "tokens": [293, 848, 11, 2264, 11, 341, 307, 257, 1389, 689], "temperature": 0.0, "avg_logprob": -0.18930310619120694, "compression_ratio": 1.5594713656387664, "no_speech_prob": 2.3916383725008927e-05}, {"id": 69, "seek": 19880, "start": 207.68, "end": 209.48000000000002, "text": " you would want to remove them.", "tokens": [291, 576, 528, 281, 4159, 552, 13], "temperature": 0.0, "avg_logprob": -0.18930310619120694, "compression_ratio": 1.5594713656387664, "no_speech_prob": 2.3916383725008927e-05}, {"id": 70, "seek": 19880, "start": 209.48000000000002, "end": 211.16000000000003, "text": " Although really, you would also probably", "tokens": [5780, 534, 11, 291, 576, 611, 1391], "temperature": 0.0, "avg_logprob": -0.18930310619120694, "compression_ratio": 1.5594713656387664, "no_speech_prob": 2.3916383725008927e-05}, {"id": 71, "seek": 19880, "start": 211.16000000000003, "end": 215.88000000000002, "text": " be wanting to look at which words are disproportionately", "tokens": [312, 7935, 281, 574, 412, 597, 2283, 366, 43397], "temperature": 0.0, "avg_logprob": -0.18930310619120694, "compression_ratio": 1.5594713656387664, "no_speech_prob": 2.3916383725008927e-05}, {"id": 72, "seek": 19880, "start": 215.88000000000002, "end": 220.12, "text": " common in a state compared to being", "tokens": [2689, 294, 257, 1785, 5347, 281, 885], "temperature": 0.0, "avg_logprob": -0.18930310619120694, "compression_ratio": 1.5594713656387664, "no_speech_prob": 2.3916383725008927e-05}, {"id": 73, "seek": 19880, "start": 220.12, "end": 223.36, "text": " uncommon in other states.", "tokens": [29289, 294, 661, 4368, 13], "temperature": 0.0, "avg_logprob": -0.18930310619120694, "compression_ratio": 1.5594713656387664, "no_speech_prob": 2.3916383725008927e-05}, {"id": 74, "seek": 19880, "start": 223.36, "end": 224.68, "text": " And then I'll ask you a question.", "tokens": [400, 550, 286, 603, 1029, 291, 257, 1168, 13], "temperature": 0.0, "avg_logprob": -0.18930310619120694, "compression_ratio": 1.5594713656387664, "no_speech_prob": 2.3916383725008927e-05}, {"id": 75, "seek": 19880, "start": 224.68, "end": 226.96, "text": " Does anyone remember?", "tokens": [4402, 2878, 1604, 30], "temperature": 0.0, "avg_logprob": -0.18930310619120694, "compression_ratio": 1.5594713656387664, "no_speech_prob": 2.3916383725008927e-05}, {"id": 76, "seek": 22696, "start": 226.96, "end": 231.08, "text": " Actually, yeah, what are stemming and lemmatization?", "tokens": [5135, 11, 1338, 11, 437, 366, 12312, 2810, 293, 7495, 15677, 2144, 30], "temperature": 0.0, "avg_logprob": -0.349558152650532, "compression_ratio": 1.301775147928994, "no_speech_prob": 1.2410204362822697e-05}, {"id": 77, "seek": 22696, "start": 231.08, "end": 232.84, "text": " Review from last time.", "tokens": [19954, 490, 1036, 565, 13], "temperature": 0.0, "avg_logprob": -0.349558152650532, "compression_ratio": 1.301775147928994, "no_speech_prob": 1.2410204362822697e-05}, {"id": 78, "seek": 22696, "start": 237.56, "end": 239.28, "text": " Yes, and do you want the catch box?", "tokens": [1079, 11, 293, 360, 291, 528, 264, 3745, 2424, 30], "temperature": 0.0, "avg_logprob": -0.349558152650532, "compression_ratio": 1.301775147928994, "no_speech_prob": 1.2410204362822697e-05}, {"id": 79, "seek": 22696, "start": 239.28, "end": 241.4, "text": " Up to you.", "tokens": [5858, 281, 291, 13], "temperature": 0.0, "avg_logprob": -0.349558152650532, "compression_ratio": 1.301775147928994, "no_speech_prob": 1.2410204362822697e-05}, {"id": 80, "seek": 22696, "start": 241.4, "end": 242.64000000000001, "text": " OK, I'll come over.", "tokens": [2264, 11, 286, 603, 808, 670, 13], "temperature": 0.0, "avg_logprob": -0.349558152650532, "compression_ratio": 1.301775147928994, "no_speech_prob": 1.2410204362822697e-05}, {"id": 81, "seek": 22696, "start": 249.12, "end": 252.60000000000002, "text": " Stemming is pretty much like getting the roots of words,", "tokens": [745, 443, 2810, 307, 1238, 709, 411, 1242, 264, 10669, 295, 2283, 11], "temperature": 0.0, "avg_logprob": -0.349558152650532, "compression_ratio": 1.301775147928994, "no_speech_prob": 1.2410204362822697e-05}, {"id": 82, "seek": 22696, "start": 252.60000000000002, "end": 253.08, "text": " right?", "tokens": [558, 30], "temperature": 0.0, "avg_logprob": -0.349558152650532, "compression_ratio": 1.301775147928994, "no_speech_prob": 1.2410204362822697e-05}, {"id": 83, "seek": 22696, "start": 253.08, "end": 254.4, "text": " Correct, yes.", "tokens": [12753, 11, 2086, 13], "temperature": 0.0, "avg_logprob": -0.349558152650532, "compression_ratio": 1.301775147928994, "no_speech_prob": 1.2410204362822697e-05}, {"id": 84, "seek": 25440, "start": 254.4, "end": 261.56, "text": " Lemmatization is kind of, in a way, stemming,", "tokens": [16905, 15677, 2144, 307, 733, 295, 11, 294, 257, 636, 11, 12312, 2810, 11], "temperature": 0.0, "avg_logprob": -0.16608464846047022, "compression_ratio": 1.5833333333333333, "no_speech_prob": 2.9760345569229685e-05}, {"id": 85, "seek": 25440, "start": 261.56, "end": 266.36, "text": " but not quite there.", "tokens": [457, 406, 1596, 456, 13], "temperature": 0.0, "avg_logprob": -0.16608464846047022, "compression_ratio": 1.5833333333333333, "no_speech_prob": 2.9760345569229685e-05}, {"id": 86, "seek": 25440, "start": 266.36, "end": 269.92, "text": " Well, which one is fancier?", "tokens": [1042, 11, 597, 472, 307, 3429, 27674, 30], "temperature": 0.0, "avg_logprob": -0.16608464846047022, "compression_ratio": 1.5833333333333333, "no_speech_prob": 2.9760345569229685e-05}, {"id": 87, "seek": 25440, "start": 269.92, "end": 270.8, "text": " Lemmatization.", "tokens": [16905, 15677, 2144, 13], "temperature": 0.0, "avg_logprob": -0.16608464846047022, "compression_ratio": 1.5833333333333333, "no_speech_prob": 2.9760345569229685e-05}, {"id": 88, "seek": 25440, "start": 270.8, "end": 273.48, "text": " Right, lemmatization is fancier.", "tokens": [1779, 11, 7495, 15677, 2144, 307, 3429, 27674, 13], "temperature": 0.0, "avg_logprob": -0.16608464846047022, "compression_ratio": 1.5833333333333333, "no_speech_prob": 2.9760345569229685e-05}, {"id": 89, "seek": 25440, "start": 273.48, "end": 276.52, "text": " We had the quote, stemming is the poor man's lemmatization.", "tokens": [492, 632, 264, 6513, 11, 12312, 2810, 307, 264, 4716, 587, 311, 7495, 15677, 2144, 13], "temperature": 0.0, "avg_logprob": -0.16608464846047022, "compression_ratio": 1.5833333333333333, "no_speech_prob": 2.9760345569229685e-05}, {"id": 90, "seek": 25440, "start": 276.52, "end": 278.6, "text": " And so really, I guess you could think of both of them", "tokens": [400, 370, 534, 11, 286, 2041, 291, 727, 519, 295, 1293, 295, 552], "temperature": 0.0, "avg_logprob": -0.16608464846047022, "compression_ratio": 1.5833333333333333, "no_speech_prob": 2.9760345569229685e-05}, {"id": 91, "seek": 25440, "start": 278.6, "end": 281.16, "text": " as getting roots for words.", "tokens": [382, 1242, 10669, 337, 2283, 13], "temperature": 0.0, "avg_logprob": -0.16608464846047022, "compression_ratio": 1.5833333333333333, "no_speech_prob": 2.9760345569229685e-05}, {"id": 92, "seek": 28116, "start": 281.16, "end": 285.08000000000004, "text": " Stemming is more kind of chopping off the end.", "tokens": [745, 443, 2810, 307, 544, 733, 295, 35205, 766, 264, 917, 13], "temperature": 0.0, "avg_logprob": -0.23420815939431663, "compression_ratio": 1.5255102040816326, "no_speech_prob": 2.2817772332928143e-05}, {"id": 93, "seek": 28116, "start": 285.08000000000004, "end": 288.20000000000005, "text": " And we saw one of the examples I had you do last time", "tokens": [400, 321, 1866, 472, 295, 264, 5110, 286, 632, 291, 360, 1036, 565], "temperature": 0.0, "avg_logprob": -0.23420815939431663, "compression_ratio": 1.5255102040816326, "no_speech_prob": 2.2817772332928143e-05}, {"id": 94, "seek": 28116, "start": 288.20000000000005, "end": 291.52000000000004, "text": " was looking at the words organizing, organizer,", "tokens": [390, 1237, 412, 264, 2283, 17608, 11, 41363, 11], "temperature": 0.0, "avg_logprob": -0.23420815939431663, "compression_ratio": 1.5255102040816326, "no_speech_prob": 2.2817772332928143e-05}, {"id": 95, "seek": 28116, "start": 291.52000000000004, "end": 292.92, "text": " organize.", "tokens": [13859, 13], "temperature": 0.0, "avg_logprob": -0.23420815939431663, "compression_ratio": 1.5255102040816326, "no_speech_prob": 2.2817772332928143e-05}, {"id": 96, "seek": 28116, "start": 292.92, "end": 295.40000000000003, "text": " Do you remember what the stem of those was?", "tokens": [1144, 291, 1604, 437, 264, 12312, 295, 729, 390, 30], "temperature": 0.0, "avg_logprob": -0.23420815939431663, "compression_ratio": 1.5255102040816326, "no_speech_prob": 2.2817772332928143e-05}, {"id": 97, "seek": 28116, "start": 295.40000000000003, "end": 296.76000000000005, "text": " Organ.", "tokens": [12538, 13], "temperature": 0.0, "avg_logprob": -0.23420815939431663, "compression_ratio": 1.5255102040816326, "no_speech_prob": 2.2817772332928143e-05}, {"id": 98, "seek": 28116, "start": 296.76000000000005, "end": 299.8, "text": " Organ, right, which is kind of a completely different word.", "tokens": [12538, 11, 558, 11, 597, 307, 733, 295, 257, 2584, 819, 1349, 13], "temperature": 0.0, "avg_logprob": -0.23420815939431663, "compression_ratio": 1.5255102040816326, "no_speech_prob": 2.2817772332928143e-05}, {"id": 99, "seek": 28116, "start": 299.8, "end": 300.36, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.23420815939431663, "compression_ratio": 1.5255102040816326, "no_speech_prob": 2.2817772332928143e-05}, {"id": 100, "seek": 28116, "start": 300.36, "end": 300.86, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.23420815939431663, "compression_ratio": 1.5255102040816326, "no_speech_prob": 2.2817772332928143e-05}, {"id": 101, "seek": 30086, "start": 300.86, "end": 315.7, "text": " OK, yeah, so we saw stemming and lemmatization.", "tokens": [2264, 11, 1338, 11, 370, 321, 1866, 12312, 2810, 293, 7495, 15677, 2144, 13], "temperature": 0.0, "avg_logprob": -0.3574924247209416, "compression_ratio": 1.276190476190476, "no_speech_prob": 5.1432311011012644e-05}, {"id": 102, "seek": 30086, "start": 315.7, "end": 318.94, "text": " Does anyone remember when you want to use stemming", "tokens": [4402, 2878, 1604, 562, 291, 528, 281, 764, 12312, 2810], "temperature": 0.0, "avg_logprob": -0.3574924247209416, "compression_ratio": 1.276190476190476, "no_speech_prob": 5.1432311011012644e-05}, {"id": 103, "seek": 30086, "start": 318.94, "end": 321.14, "text": " or lemmatization versus not?", "tokens": [420, 7495, 15677, 2144, 5717, 406, 30], "temperature": 0.0, "avg_logprob": -0.3574924247209416, "compression_ratio": 1.276190476190476, "no_speech_prob": 5.1432311011012644e-05}, {"id": 104, "seek": 30086, "start": 326.58000000000004, "end": 327.58000000000004, "text": " Quinn?", "tokens": [36723, 30], "temperature": 0.0, "avg_logprob": -0.3574924247209416, "compression_ratio": 1.276190476190476, "no_speech_prob": 5.1432311011012644e-05}, {"id": 105, "seek": 32758, "start": 327.58, "end": 331.02, "text": " Stemming is a little less like computationally expensive.", "tokens": [745, 443, 2810, 307, 257, 707, 1570, 411, 24903, 379, 5124, 13], "temperature": 0.0, "avg_logprob": -0.39624442123785253, "compression_ratio": 1.756, "no_speech_prob": 2.1774474589619786e-05}, {"id": 106, "seek": 32758, "start": 331.02, "end": 332.97999999999996, "text": " It's really good, more like the lemmatization", "tokens": [467, 311, 534, 665, 11, 544, 411, 264, 7495, 15677, 2144], "temperature": 0.0, "avg_logprob": -0.39624442123785253, "compression_ratio": 1.756, "no_speech_prob": 2.1774474589619786e-05}, {"id": 107, "seek": 32758, "start": 332.97999999999996, "end": 334.85999999999996, "text": " needs a lot more rules, because you can't just", "tokens": [2203, 257, 688, 544, 4474, 11, 570, 291, 393, 380, 445], "temperature": 0.0, "avg_logprob": -0.39624442123785253, "compression_ratio": 1.756, "no_speech_prob": 2.1774474589619786e-05}, {"id": 108, "seek": 32758, "start": 334.85999999999996, "end": 336.62, "text": " have to remember that it's not lemmatization.", "tokens": [362, 281, 1604, 300, 309, 311, 406, 7495, 15677, 2144, 13], "temperature": 0.0, "avg_logprob": -0.39624442123785253, "compression_ratio": 1.756, "no_speech_prob": 2.1774474589619786e-05}, {"id": 109, "seek": 32758, "start": 336.62, "end": 337.41999999999996, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.39624442123785253, "compression_ratio": 1.756, "no_speech_prob": 2.1774474589619786e-05}, {"id": 110, "seek": 32758, "start": 337.41999999999996, "end": 340.21999999999997, "text": " Yeah, and so to repeat this for the microphone,", "tokens": [865, 11, 293, 370, 281, 7149, 341, 337, 264, 10952, 11], "temperature": 0.0, "avg_logprob": -0.39624442123785253, "compression_ratio": 1.756, "no_speech_prob": 2.1774474589619786e-05}, {"id": 111, "seek": 32758, "start": 340.21999999999997, "end": 342.94, "text": " lemmatization is more computationally expensive", "tokens": [7495, 15677, 2144, 307, 544, 24903, 379, 5124], "temperature": 0.0, "avg_logprob": -0.39624442123785253, "compression_ratio": 1.756, "no_speech_prob": 2.1774474589619786e-05}, {"id": 112, "seek": 32758, "start": 342.94, "end": 344.82, "text": " than stemming, which is true.", "tokens": [813, 12312, 2810, 11, 597, 307, 2074, 13], "temperature": 0.0, "avg_logprob": -0.39624442123785253, "compression_ratio": 1.756, "no_speech_prob": 2.1774474589619786e-05}, {"id": 113, "seek": 32758, "start": 344.82, "end": 348.14, "text": " Stemming is quicker and easier.", "tokens": [745, 443, 2810, 307, 16255, 293, 3571, 13], "temperature": 0.0, "avg_logprob": -0.39624442123785253, "compression_ratio": 1.756, "no_speech_prob": 2.1774474589619786e-05}, {"id": 114, "seek": 32758, "start": 348.14, "end": 353.82, "text": " In terms of using these kind of preprocessing", "tokens": [682, 2115, 295, 1228, 613, 733, 295, 2666, 340, 780, 278], "temperature": 0.0, "avg_logprob": -0.39624442123785253, "compression_ratio": 1.756, "no_speech_prob": 2.1774474589619786e-05}, {"id": 115, "seek": 32758, "start": 353.82, "end": 356.78, "text": " and removing stop words and doing", "tokens": [293, 12720, 1590, 2283, 293, 884], "temperature": 0.0, "avg_logprob": -0.39624442123785253, "compression_ratio": 1.756, "no_speech_prob": 2.1774474589619786e-05}, {"id": 116, "seek": 35678, "start": 356.78, "end": 358.9, "text": " some form of stemming or lemmatization,", "tokens": [512, 1254, 295, 12312, 2810, 420, 7495, 15677, 2144, 11], "temperature": 0.0, "avg_logprob": -0.2658889321719899, "compression_ratio": 1.64375, "no_speech_prob": 3.169061164953746e-05}, {"id": 117, "seek": 35678, "start": 358.9, "end": 361.09999999999997, "text": " when do you want to do that versus not at all?", "tokens": [562, 360, 291, 528, 281, 360, 300, 5717, 406, 412, 439, 30], "temperature": 0.0, "avg_logprob": -0.2658889321719899, "compression_ratio": 1.64375, "no_speech_prob": 3.169061164953746e-05}, {"id": 118, "seek": 35678, "start": 363.97999999999996, "end": 367.05999999999995, "text": " Or do you always want to do that type of preprocessing?", "tokens": [1610, 360, 291, 1009, 528, 281, 360, 300, 2010, 295, 2666, 340, 780, 278, 30], "temperature": 0.0, "avg_logprob": -0.2658889321719899, "compression_ratio": 1.64375, "no_speech_prob": 3.169061164953746e-05}, {"id": 119, "seek": 35678, "start": 370.26, "end": 372.78, "text": " Do you have less data?", "tokens": [1144, 291, 362, 1570, 1412, 30], "temperature": 0.0, "avg_logprob": -0.2658889321719899, "compression_ratio": 1.64375, "no_speech_prob": 3.169061164953746e-05}, {"id": 120, "seek": 35678, "start": 372.78, "end": 375.14, "text": " Yes, so the answer is when you have less data.", "tokens": [1079, 11, 370, 264, 1867, 307, 562, 291, 362, 1570, 1412, 13], "temperature": 0.0, "avg_logprob": -0.2658889321719899, "compression_ratio": 1.64375, "no_speech_prob": 3.169061164953746e-05}, {"id": 121, "seek": 35678, "start": 375.14, "end": 378.5, "text": " You want to do these also related?", "tokens": [509, 528, 281, 360, 613, 611, 4077, 30], "temperature": 0.0, "avg_logprob": -0.2658889321719899, "compression_ratio": 1.64375, "no_speech_prob": 3.169061164953746e-05}, {"id": 122, "seek": 35678, "start": 378.5, "end": 378.97999999999996, "text": " Any other?", "tokens": [2639, 661, 30], "temperature": 0.0, "avg_logprob": -0.2658889321719899, "compression_ratio": 1.64375, "no_speech_prob": 3.169061164953746e-05}, {"id": 123, "seek": 37898, "start": 378.98, "end": 383.3, "text": " Yes?", "tokens": [1079, 30], "temperature": 0.0, "avg_logprob": -0.47320977915888246, "compression_ratio": 1.8237885462555066, "no_speech_prob": 8.938557584770024e-06}, {"id": 124, "seek": 37898, "start": 383.3, "end": 384.18, "text": " Exactly.", "tokens": [7587, 13], "temperature": 0.0, "avg_logprob": -0.47320977915888246, "compression_ratio": 1.8237885462555066, "no_speech_prob": 8.938557584770024e-06}, {"id": 125, "seek": 37898, "start": 384.18, "end": 386.70000000000005, "text": " Do you think your model can handle the complexity?", "tokens": [1144, 291, 519, 428, 2316, 393, 4813, 264, 14024, 30], "temperature": 0.0, "avg_logprob": -0.47320977915888246, "compression_ratio": 1.8237885462555066, "no_speech_prob": 8.938557584770024e-06}, {"id": 126, "seek": 37898, "start": 386.70000000000005, "end": 390.90000000000003, "text": " And so if you have a model that is more complex", "tokens": [400, 370, 498, 291, 362, 257, 2316, 300, 307, 544, 3997], "temperature": 0.0, "avg_logprob": -0.47320977915888246, "compression_ratio": 1.8237885462555066, "no_speech_prob": 8.938557584770024e-06}, {"id": 127, "seek": 37898, "start": 390.90000000000003, "end": 393.78000000000003, "text": " and can handle more complexity, like a neural network,", "tokens": [293, 393, 4813, 544, 14024, 11, 411, 257, 18161, 3209, 11], "temperature": 0.0, "avg_logprob": -0.47320977915888246, "compression_ratio": 1.8237885462555066, "no_speech_prob": 8.938557584770024e-06}, {"id": 128, "seek": 37898, "start": 393.78000000000003, "end": 396.14000000000004, "text": " you probably don't want to remove stop words", "tokens": [291, 1391, 500, 380, 528, 281, 4159, 1590, 2283], "temperature": 0.0, "avg_logprob": -0.47320977915888246, "compression_ratio": 1.8237885462555066, "no_speech_prob": 8.938557584770024e-06}, {"id": 129, "seek": 37898, "start": 396.14000000000004, "end": 398.02000000000004, "text": " or use stemming or lemmatization,", "tokens": [420, 764, 12312, 2810, 420, 7495, 15677, 2144, 11], "temperature": 0.0, "avg_logprob": -0.47320977915888246, "compression_ratio": 1.8237885462555066, "no_speech_prob": 8.938557584770024e-06}, {"id": 130, "seek": 37898, "start": 398.02000000000004, "end": 400.42, "text": " because that's throwing away information.", "tokens": [570, 300, 311, 10238, 1314, 1589, 13], "temperature": 0.0, "avg_logprob": -0.47320977915888246, "compression_ratio": 1.8237885462555066, "no_speech_prob": 8.938557584770024e-06}, {"id": 131, "seek": 37898, "start": 400.42, "end": 402.38, "text": " However, if you have a simpler model,", "tokens": [2908, 11, 498, 291, 362, 257, 18587, 2316, 11], "temperature": 0.0, "avg_logprob": -0.47320977915888246, "compression_ratio": 1.8237885462555066, "no_speech_prob": 8.938557584770024e-06}, {"id": 132, "seek": 37898, "start": 402.38, "end": 404.70000000000005, "text": " you probably don't want to use the same model.", "tokens": [291, 1391, 500, 380, 528, 281, 764, 264, 912, 2316, 13], "temperature": 0.0, "avg_logprob": -0.47320977915888246, "compression_ratio": 1.8237885462555066, "no_speech_prob": 8.938557584770024e-06}, {"id": 133, "seek": 37898, "start": 404.70000000000005, "end": 407.06, "text": " So you want to use a more complex model.", "tokens": [407, 291, 528, 281, 764, 257, 544, 3997, 2316, 13], "temperature": 0.0, "avg_logprob": -0.47320977915888246, "compression_ratio": 1.8237885462555066, "no_speech_prob": 8.938557584770024e-06}, {"id": 134, "seek": 40706, "start": 407.06, "end": 409.1, "text": " However, if you have a simpler model,", "tokens": [2908, 11, 498, 291, 362, 257, 18587, 2316, 11], "temperature": 0.0, "avg_logprob": -0.17572096622351444, "compression_ratio": 1.4557522123893805, "no_speech_prob": 2.8569691039592726e-06}, {"id": 135, "seek": 40706, "start": 409.1, "end": 413.7, "text": " you can't really learn as much detail or complexity.", "tokens": [291, 393, 380, 534, 1466, 382, 709, 2607, 420, 14024, 13], "temperature": 0.0, "avg_logprob": -0.17572096622351444, "compression_ratio": 1.4557522123893805, "no_speech_prob": 2.8569691039592726e-06}, {"id": 136, "seek": 40706, "start": 413.7, "end": 416.14, "text": " And so you do want to do this preprocessing.", "tokens": [400, 370, 291, 360, 528, 281, 360, 341, 2666, 340, 780, 278, 13], "temperature": 0.0, "avg_logprob": -0.17572096622351444, "compression_ratio": 1.4557522123893805, "no_speech_prob": 2.8569691039592726e-06}, {"id": 137, "seek": 40706, "start": 419.66, "end": 420.14, "text": " Cool.", "tokens": [8561, 13], "temperature": 0.0, "avg_logprob": -0.17572096622351444, "compression_ratio": 1.4557522123893805, "no_speech_prob": 2.8569691039592726e-06}, {"id": 138, "seek": 40706, "start": 423.14, "end": 427.18, "text": " So next step, something I wish it set up more last time,", "tokens": [407, 958, 1823, 11, 746, 286, 3172, 309, 992, 493, 544, 1036, 565, 11], "temperature": 0.0, "avg_logprob": -0.17572096622351444, "compression_ratio": 1.4557522123893805, "no_speech_prob": 2.8569691039592726e-06}, {"id": 139, "seek": 40706, "start": 427.18, "end": 429.62, "text": " is I want to talk to you about how factorization is", "tokens": [307, 286, 528, 281, 751, 281, 291, 466, 577, 5952, 2144, 307], "temperature": 0.0, "avg_logprob": -0.17572096622351444, "compression_ratio": 1.4557522123893805, "no_speech_prob": 2.8569691039592726e-06}, {"id": 140, "seek": 40706, "start": 429.62, "end": 431.98, "text": " analogous to matrix decomposition.", "tokens": [16660, 563, 281, 8141, 48356, 13], "temperature": 0.0, "avg_logprob": -0.17572096622351444, "compression_ratio": 1.4557522123893805, "no_speech_prob": 2.8569691039592726e-06}, {"id": 141, "seek": 40706, "start": 431.98, "end": 435.9, "text": " So I kind of jumped into we used SVD or NMF", "tokens": [407, 286, 733, 295, 13864, 666, 321, 1143, 31910, 35, 420, 426, 44, 37], "temperature": 0.0, "avg_logprob": -0.17572096622351444, "compression_ratio": 1.4557522123893805, "no_speech_prob": 2.8569691039592726e-06}, {"id": 142, "seek": 43590, "start": 435.9, "end": 438.41999999999996, "text": " to decompose our matrices.", "tokens": [281, 22867, 541, 527, 32284, 13], "temperature": 0.0, "avg_logprob": -0.14229993636791521, "compression_ratio": 1.6351931330472103, "no_speech_prob": 3.647033736342564e-05}, {"id": 143, "seek": 43590, "start": 438.41999999999996, "end": 440.9, "text": " But I wanted to just provide a little bit more motivation", "tokens": [583, 286, 1415, 281, 445, 2893, 257, 707, 857, 544, 12335], "temperature": 0.0, "avg_logprob": -0.14229993636791521, "compression_ratio": 1.6351931330472103, "no_speech_prob": 3.647033736342564e-05}, {"id": 144, "seek": 43590, "start": 440.9, "end": 443.41999999999996, "text": " of why that's something we do.", "tokens": [295, 983, 300, 311, 746, 321, 360, 13], "temperature": 0.0, "avg_logprob": -0.14229993636791521, "compression_ratio": 1.6351931330472103, "no_speech_prob": 3.647033736342564e-05}, {"id": 145, "seek": 43590, "start": 443.41999999999996, "end": 447.41999999999996, "text": " And so looking at a much simpler case, which is actually", "tokens": [400, 370, 1237, 412, 257, 709, 18587, 1389, 11, 597, 307, 767], "temperature": 0.0, "avg_logprob": -0.14229993636791521, "compression_ratio": 1.6351931330472103, "no_speech_prob": 3.647033736342564e-05}, {"id": 146, "seek": 43590, "start": 447.41999999999996, "end": 450.97999999999996, "text": " make this a little bit bigger for you,", "tokens": [652, 341, 257, 707, 857, 3801, 337, 291, 11], "temperature": 0.0, "avg_logprob": -0.14229993636791521, "compression_ratio": 1.6351931330472103, "no_speech_prob": 3.647033736342564e-05}, {"id": 147, "seek": 43590, "start": 450.97999999999996, "end": 452.78, "text": " which is multiplication.", "tokens": [597, 307, 27290, 13], "temperature": 0.0, "avg_logprob": -0.14229993636791521, "compression_ratio": 1.6351931330472103, "no_speech_prob": 3.647033736342564e-05}, {"id": 148, "seek": 43590, "start": 452.78, "end": 455.09999999999997, "text": " You can multiply numbers together.", "tokens": [509, 393, 12972, 3547, 1214, 13], "temperature": 0.0, "avg_logprob": -0.14229993636791521, "compression_ratio": 1.6351931330472103, "no_speech_prob": 3.647033736342564e-05}, {"id": 149, "seek": 43590, "start": 455.09999999999997, "end": 460.97999999999996, "text": " So 2 times 2 times 3 times 3 times 2 times 2 is 144.", "tokens": [407, 568, 1413, 568, 1413, 805, 1413, 805, 1413, 568, 1413, 568, 307, 45218, 13], "temperature": 0.0, "avg_logprob": -0.14229993636791521, "compression_ratio": 1.6351931330472103, "no_speech_prob": 3.647033736342564e-05}, {"id": 150, "seek": 43590, "start": 460.97999999999996, "end": 462.73999999999995, "text": " We've learned about multiplication", "tokens": [492, 600, 3264, 466, 27290], "temperature": 0.0, "avg_logprob": -0.14229993636791521, "compression_ratio": 1.6351931330472103, "no_speech_prob": 3.647033736342564e-05}, {"id": 151, "seek": 43590, "start": 462.73999999999995, "end": 464.53999999999996, "text": " a very long time ago.", "tokens": [257, 588, 938, 565, 2057, 13], "temperature": 0.0, "avg_logprob": -0.14229993636791521, "compression_ratio": 1.6351931330472103, "no_speech_prob": 3.647033736342564e-05}, {"id": 152, "seek": 46454, "start": 464.54, "end": 468.06, "text": " And then the reverse is prime factorization.", "tokens": [400, 550, 264, 9943, 307, 5835, 5952, 2144, 13], "temperature": 0.0, "avg_logprob": -0.12293775202864307, "compression_ratio": 1.7182539682539681, "no_speech_prob": 8.012219950614963e-06}, {"id": 153, "seek": 46454, "start": 468.06, "end": 469.94, "text": " So you could take 144 and say, I want", "tokens": [407, 291, 727, 747, 45218, 293, 584, 11, 286, 528], "temperature": 0.0, "avg_logprob": -0.12293775202864307, "compression_ratio": 1.7182539682539681, "no_speech_prob": 8.012219950614963e-06}, {"id": 154, "seek": 46454, "start": 469.94, "end": 472.06, "text": " to know what its prime factors are.", "tokens": [281, 458, 437, 1080, 5835, 6771, 366, 13], "temperature": 0.0, "avg_logprob": -0.12293775202864307, "compression_ratio": 1.7182539682539681, "no_speech_prob": 8.012219950614963e-06}, {"id": 155, "seek": 46454, "start": 472.06, "end": 475.70000000000005, "text": " And you can think of this as a way of kind of decomposing", "tokens": [400, 291, 393, 519, 295, 341, 382, 257, 636, 295, 733, 295, 22867, 6110], "temperature": 0.0, "avg_logprob": -0.12293775202864307, "compression_ratio": 1.7182539682539681, "no_speech_prob": 8.012219950614963e-06}, {"id": 156, "seek": 46454, "start": 475.70000000000005, "end": 478.5, "text": " an integer into something else.", "tokens": [364, 24922, 666, 746, 1646, 13], "temperature": 0.0, "avg_logprob": -0.12293775202864307, "compression_ratio": 1.7182539682539681, "no_speech_prob": 8.012219950614963e-06}, {"id": 157, "seek": 46454, "start": 478.5, "end": 481.38, "text": " Or you could say factorization is the, quote, opposite", "tokens": [1610, 291, 727, 584, 5952, 2144, 307, 264, 11, 6513, 11, 6182], "temperature": 0.0, "avg_logprob": -0.12293775202864307, "compression_ratio": 1.7182539682539681, "no_speech_prob": 8.012219950614963e-06}, {"id": 158, "seek": 46454, "start": 481.38, "end": 484.82000000000005, "text": " or inverse of multiplication.", "tokens": [420, 17340, 295, 27290, 13], "temperature": 0.0, "avg_logprob": -0.12293775202864307, "compression_ratio": 1.7182539682539681, "no_speech_prob": 8.012219950614963e-06}, {"id": 159, "seek": 46454, "start": 484.82000000000005, "end": 486.62, "text": " And here, the benefit of doing that", "tokens": [400, 510, 11, 264, 5121, 295, 884, 300], "temperature": 0.0, "avg_logprob": -0.12293775202864307, "compression_ratio": 1.7182539682539681, "no_speech_prob": 8.012219950614963e-06}, {"id": 160, "seek": 46454, "start": 486.62, "end": 490.14000000000004, "text": " is that the factors have this nice property of being prime.", "tokens": [307, 300, 264, 6771, 362, 341, 1481, 4707, 295, 885, 5835, 13], "temperature": 0.0, "avg_logprob": -0.12293775202864307, "compression_ratio": 1.7182539682539681, "no_speech_prob": 8.012219950614963e-06}, {"id": 161, "seek": 46454, "start": 490.14000000000004, "end": 493.14000000000004, "text": " So 144 is not prime, but you've found a way", "tokens": [407, 45218, 307, 406, 5835, 11, 457, 291, 600, 1352, 257, 636], "temperature": 0.0, "avg_logprob": -0.12293775202864307, "compression_ratio": 1.7182539682539681, "no_speech_prob": 8.012219950614963e-06}, {"id": 162, "seek": 49314, "start": 493.14, "end": 494.82, "text": " to represent it using prime factors.", "tokens": [281, 2906, 309, 1228, 5835, 6771, 13], "temperature": 0.0, "avg_logprob": -0.18372496436623967, "compression_ratio": 1.6233766233766234, "no_speech_prob": 1.0288943485647906e-05}, {"id": 163, "seek": 49314, "start": 497.58, "end": 500.5, "text": " Factorization is much harder than multiplication,", "tokens": [479, 15104, 2144, 307, 709, 6081, 813, 27290, 11], "temperature": 0.0, "avg_logprob": -0.18372496436623967, "compression_ratio": 1.6233766233766234, "no_speech_prob": 1.0288943485647906e-05}, {"id": 164, "seek": 49314, "start": 500.5, "end": 503.21999999999997, "text": " which is good because this is the heart of encryption", "tokens": [597, 307, 665, 570, 341, 307, 264, 1917, 295, 29575], "temperature": 0.0, "avg_logprob": -0.18372496436623967, "compression_ratio": 1.6233766233766234, "no_speech_prob": 1.0288943485647906e-05}, {"id": 165, "seek": 49314, "start": 503.21999999999997, "end": 504.58, "text": " techniques.", "tokens": [7512, 13], "temperature": 0.0, "avg_logprob": -0.18372496436623967, "compression_ratio": 1.6233766233766234, "no_speech_prob": 1.0288943485647906e-05}, {"id": 166, "seek": 49314, "start": 504.58, "end": 505.97999999999996, "text": " But you can see, even though we're", "tokens": [583, 291, 393, 536, 11, 754, 1673, 321, 434], "temperature": 0.0, "avg_logprob": -0.18372496436623967, "compression_ratio": 1.6233766233766234, "no_speech_prob": 1.0288943485647906e-05}, {"id": 167, "seek": 49314, "start": 505.97999999999996, "end": 507.65999999999997, "text": " kind of doing something that's, OK, this", "tokens": [733, 295, 884, 746, 300, 311, 11, 2264, 11, 341], "temperature": 0.0, "avg_logprob": -0.18372496436623967, "compression_ratio": 1.6233766233766234, "no_speech_prob": 1.0288943485647906e-05}, {"id": 168, "seek": 49314, "start": 507.65999999999997, "end": 509.65999999999997, "text": " is like the opposite of multiplication,", "tokens": [307, 411, 264, 6182, 295, 27290, 11], "temperature": 0.0, "avg_logprob": -0.18372496436623967, "compression_ratio": 1.6233766233766234, "no_speech_prob": 1.0288943485647906e-05}, {"id": 169, "seek": 49314, "start": 509.65999999999997, "end": 513.78, "text": " it's a much harder process to do.", "tokens": [309, 311, 257, 709, 6081, 1399, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.18372496436623967, "compression_ratio": 1.6233766233766234, "no_speech_prob": 1.0288943485647906e-05}, {"id": 170, "seek": 49314, "start": 513.78, "end": 515.02, "text": " Any questions about that?", "tokens": [2639, 1651, 466, 300, 30], "temperature": 0.0, "avg_logprob": -0.18372496436623967, "compression_ratio": 1.6233766233766234, "no_speech_prob": 1.0288943485647906e-05}, {"id": 171, "seek": 49314, "start": 518.26, "end": 520.7, "text": " OK, and so then this is very analogous to what", "tokens": [2264, 11, 293, 370, 550, 341, 307, 588, 16660, 563, 281, 437], "temperature": 0.0, "avg_logprob": -0.18372496436623967, "compression_ratio": 1.6233766233766234, "no_speech_prob": 1.0288943485647906e-05}, {"id": 172, "seek": 52070, "start": 520.7, "end": 525.1400000000001, "text": " we want to do in a matrix decomposition, which", "tokens": [321, 528, 281, 360, 294, 257, 8141, 48356, 11, 597], "temperature": 0.0, "avg_logprob": -0.10924935340881348, "compression_ratio": 1.811926605504587, "no_speech_prob": 4.565879407891771e-06}, {"id": 173, "seek": 52070, "start": 525.1400000000001, "end": 528.74, "text": " is you can think of a matrix decomposition as a way", "tokens": [307, 291, 393, 519, 295, 257, 8141, 48356, 382, 257, 636], "temperature": 0.0, "avg_logprob": -0.10924935340881348, "compression_ratio": 1.811926605504587, "no_speech_prob": 4.565879407891771e-06}, {"id": 174, "seek": 52070, "start": 528.74, "end": 530.82, "text": " of taking apart a matrix.", "tokens": [295, 1940, 4936, 257, 8141, 13], "temperature": 0.0, "avg_logprob": -0.10924935340881348, "compression_ratio": 1.811926605504587, "no_speech_prob": 4.565879407891771e-06}, {"id": 175, "seek": 52070, "start": 530.82, "end": 534.7, "text": " And it's the opposite of either matrix multiplication", "tokens": [400, 309, 311, 264, 6182, 295, 2139, 8141, 27290], "temperature": 0.0, "avg_logprob": -0.10924935340881348, "compression_ratio": 1.811926605504587, "no_speech_prob": 4.565879407891771e-06}, {"id": 176, "seek": 52070, "start": 534.7, "end": 537.0600000000001, "text": " or, in some cases, matrix addition.", "tokens": [420, 11, 294, 512, 3331, 11, 8141, 4500, 13], "temperature": 0.0, "avg_logprob": -0.10924935340881348, "compression_ratio": 1.811926605504587, "no_speech_prob": 4.565879407891771e-06}, {"id": 177, "seek": 52070, "start": 537.0600000000001, "end": 539.58, "text": " It's much harder than matrix multiplication", "tokens": [467, 311, 709, 6081, 813, 8141, 27290], "temperature": 0.0, "avg_logprob": -0.10924935340881348, "compression_ratio": 1.811926605504587, "no_speech_prob": 4.565879407891771e-06}, {"id": 178, "seek": 52070, "start": 539.58, "end": 541.7800000000001, "text": " to come up with these components.", "tokens": [281, 808, 493, 365, 613, 6677, 13], "temperature": 0.0, "avg_logprob": -0.10924935340881348, "compression_ratio": 1.811926605504587, "no_speech_prob": 4.565879407891771e-06}, {"id": 179, "seek": 52070, "start": 541.7800000000001, "end": 544.98, "text": " And the reason it's nice is that typically,", "tokens": [400, 264, 1778, 309, 311, 1481, 307, 300, 5850, 11], "temperature": 0.0, "avg_logprob": -0.10924935340881348, "compression_ratio": 1.811926605504587, "no_speech_prob": 4.565879407891771e-06}, {"id": 180, "seek": 52070, "start": 544.98, "end": 548.58, "text": " the matrices you're decomposing into have nice properties.", "tokens": [264, 32284, 291, 434, 22867, 6110, 666, 362, 1481, 7221, 13], "temperature": 0.0, "avg_logprob": -0.10924935340881348, "compression_ratio": 1.811926605504587, "no_speech_prob": 4.565879407891771e-06}, {"id": 181, "seek": 54858, "start": 548.58, "end": 550.74, "text": " So above, in this case of factorization,", "tokens": [407, 3673, 11, 294, 341, 1389, 295, 5952, 2144, 11], "temperature": 0.0, "avg_logprob": -0.10873954095573068, "compression_ratio": 1.6129032258064515, "no_speech_prob": 1.6440655599581078e-05}, {"id": 182, "seek": 54858, "start": 550.74, "end": 553.22, "text": " we had this nice property of being prime.", "tokens": [321, 632, 341, 1481, 4707, 295, 885, 5835, 13], "temperature": 0.0, "avg_logprob": -0.10873954095573068, "compression_ratio": 1.6129032258064515, "no_speech_prob": 1.6440655599581078e-05}, {"id": 183, "seek": 54858, "start": 553.22, "end": 555.86, "text": " And 144 is not prime, but we can represent it", "tokens": [400, 45218, 307, 406, 5835, 11, 457, 321, 393, 2906, 309], "temperature": 0.0, "avg_logprob": -0.10873954095573068, "compression_ratio": 1.6129032258064515, "no_speech_prob": 1.6440655599581078e-05}, {"id": 184, "seek": 54858, "start": 555.86, "end": 558.62, "text": " as a product of prime numbers.", "tokens": [382, 257, 1674, 295, 5835, 3547, 13], "temperature": 0.0, "avg_logprob": -0.10873954095573068, "compression_ratio": 1.6129032258064515, "no_speech_prob": 1.6440655599581078e-05}, {"id": 185, "seek": 54858, "start": 558.62, "end": 561.1800000000001, "text": " And so that's the same thing going on in a matrix", "tokens": [400, 370, 300, 311, 264, 912, 551, 516, 322, 294, 257, 8141], "temperature": 0.0, "avg_logprob": -0.10873954095573068, "compression_ratio": 1.6129032258064515, "no_speech_prob": 1.6440655599581078e-05}, {"id": 186, "seek": 54858, "start": 561.1800000000001, "end": 564.38, "text": " decomposition is let's get some nice properties", "tokens": [48356, 307, 718, 311, 483, 512, 1481, 7221], "temperature": 0.0, "avg_logprob": -0.10873954095573068, "compression_ratio": 1.6129032258064515, "no_speech_prob": 1.6440655599581078e-05}, {"id": 187, "seek": 54858, "start": 564.38, "end": 567.82, "text": " for these matrices.", "tokens": [337, 613, 32284, 13], "temperature": 0.0, "avg_logprob": -0.10873954095573068, "compression_ratio": 1.6129032258064515, "no_speech_prob": 1.6440655599581078e-05}, {"id": 188, "seek": 54858, "start": 567.82, "end": 571.46, "text": " I showed another example from computer vision of here,", "tokens": [286, 4712, 1071, 1365, 490, 3820, 5201, 295, 510, 11], "temperature": 0.0, "avg_logprob": -0.10873954095573068, "compression_ratio": 1.6129032258064515, "no_speech_prob": 1.6440655599581078e-05}, {"id": 189, "seek": 54858, "start": 571.46, "end": 576.0200000000001, "text": " you have this matrix on the left of people", "tokens": [291, 362, 341, 8141, 322, 264, 1411, 295, 561], "temperature": 0.0, "avg_logprob": -0.10873954095573068, "compression_ratio": 1.6129032258064515, "no_speech_prob": 1.6440655599581078e-05}, {"id": 190, "seek": 54858, "start": 576.0200000000001, "end": 577.6600000000001, "text": " walking through a video.", "tokens": [4494, 807, 257, 960, 13], "temperature": 0.0, "avg_logprob": -0.10873954095573068, "compression_ratio": 1.6129032258064515, "no_speech_prob": 1.6440655599581078e-05}, {"id": 191, "seek": 57766, "start": 577.66, "end": 580.78, "text": " And you want to identify what's the background, what", "tokens": [400, 291, 528, 281, 5876, 437, 311, 264, 3678, 11, 437], "temperature": 0.0, "avg_logprob": -0.11350947215144795, "compression_ratio": 1.6920289855072463, "no_speech_prob": 1.4284129065345041e-05}, {"id": 192, "seek": 57766, "start": 580.78, "end": 581.9, "text": " are the people.", "tokens": [366, 264, 561, 13], "temperature": 0.0, "avg_logprob": -0.11350947215144795, "compression_ratio": 1.6920289855072463, "no_speech_prob": 1.4284129065345041e-05}, {"id": 193, "seek": 57766, "start": 581.9, "end": 583.9, "text": " And so the goal is to decompose it", "tokens": [400, 370, 264, 3387, 307, 281, 22867, 541, 309], "temperature": 0.0, "avg_logprob": -0.11350947215144795, "compression_ratio": 1.6920289855072463, "no_speech_prob": 1.4284129065345041e-05}, {"id": 194, "seek": 57766, "start": 583.9, "end": 586.2199999999999, "text": " into two different matrices that, in this case,", "tokens": [666, 732, 819, 32284, 300, 11, 294, 341, 1389, 11], "temperature": 0.0, "avg_logprob": -0.11350947215144795, "compression_ratio": 1.6920289855072463, "no_speech_prob": 1.4284129065345041e-05}, {"id": 195, "seek": 57766, "start": 586.2199999999999, "end": 587.9399999999999, "text": " you're going to sum together.", "tokens": [291, 434, 516, 281, 2408, 1214, 13], "temperature": 0.0, "avg_logprob": -0.11350947215144795, "compression_ratio": 1.6920289855072463, "no_speech_prob": 1.4284129065345041e-05}, {"id": 196, "seek": 57766, "start": 587.9399999999999, "end": 590.2199999999999, "text": " But these have nice properties.", "tokens": [583, 613, 362, 1481, 7221, 13], "temperature": 0.0, "avg_logprob": -0.11350947215144795, "compression_ratio": 1.6920289855072463, "no_speech_prob": 1.4284129065345041e-05}, {"id": 197, "seek": 57766, "start": 590.2199999999999, "end": 592.6999999999999, "text": " So in this case, this matrix here of the background", "tokens": [407, 294, 341, 1389, 11, 341, 8141, 510, 295, 264, 3678], "temperature": 0.0, "avg_logprob": -0.11350947215144795, "compression_ratio": 1.6920289855072463, "no_speech_prob": 1.4284129065345041e-05}, {"id": 198, "seek": 57766, "start": 592.6999999999999, "end": 594.54, "text": " is said to be low rank.", "tokens": [307, 848, 281, 312, 2295, 6181, 13], "temperature": 0.0, "avg_logprob": -0.11350947215144795, "compression_ratio": 1.6920289855072463, "no_speech_prob": 1.4284129065345041e-05}, {"id": 199, "seek": 57766, "start": 594.54, "end": 596.98, "text": " Oh, OK, this is a little misleading.", "tokens": [876, 11, 2264, 11, 341, 307, 257, 707, 36429, 13], "temperature": 0.0, "avg_logprob": -0.11350947215144795, "compression_ratio": 1.6920289855072463, "no_speech_prob": 1.4284129065345041e-05}, {"id": 200, "seek": 57766, "start": 596.98, "end": 598.62, "text": " Really, you would have the whole video,", "tokens": [4083, 11, 291, 576, 362, 264, 1379, 960, 11], "temperature": 0.0, "avg_logprob": -0.11350947215144795, "compression_ratio": 1.6920289855072463, "no_speech_prob": 1.4284129065345041e-05}, {"id": 201, "seek": 57766, "start": 598.62, "end": 601.86, "text": " and this would represent a single row.", "tokens": [293, 341, 576, 2906, 257, 2167, 5386, 13], "temperature": 0.0, "avg_logprob": -0.11350947215144795, "compression_ratio": 1.6920289855072463, "no_speech_prob": 1.4284129065345041e-05}, {"id": 202, "seek": 57766, "start": 601.86, "end": 607.54, "text": " But this one on the far right would give you a sparse matrix.", "tokens": [583, 341, 472, 322, 264, 1400, 558, 576, 976, 291, 257, 637, 11668, 8141, 13], "temperature": 0.0, "avg_logprob": -0.11350947215144795, "compression_ratio": 1.6920289855072463, "no_speech_prob": 1.4284129065345041e-05}, {"id": 203, "seek": 60754, "start": 607.54, "end": 611.4599999999999, "text": " So what are the nice properties that the matrices", "tokens": [407, 437, 366, 264, 1481, 7221, 300, 264, 32284], "temperature": 0.0, "avg_logprob": -0.3035565867568507, "compression_ratio": 1.4375, "no_speech_prob": 4.49495792054222e-06}, {"id": 204, "seek": 60754, "start": 611.4599999999999, "end": 613.54, "text": " we get from SVD are?", "tokens": [321, 483, 490, 31910, 35, 366, 30], "temperature": 0.0, "avg_logprob": -0.3035565867568507, "compression_ratio": 1.4375, "no_speech_prob": 4.49495792054222e-06}, {"id": 205, "seek": 60754, "start": 613.54, "end": 614.98, "text": " So let's start with you.", "tokens": [407, 718, 311, 722, 365, 291, 13], "temperature": 0.0, "avg_logprob": -0.3035565867568507, "compression_ratio": 1.4375, "no_speech_prob": 4.49495792054222e-06}, {"id": 206, "seek": 60754, "start": 614.98, "end": 616.8199999999999, "text": " What nice property does you have?", "tokens": [708, 1481, 4707, 775, 291, 362, 30], "temperature": 0.0, "avg_logprob": -0.3035565867568507, "compression_ratio": 1.4375, "no_speech_prob": 4.49495792054222e-06}, {"id": 207, "seek": 60754, "start": 623.5799999999999, "end": 625.78, "text": " It's orthonormal?", "tokens": [467, 311, 420, 11943, 24440, 30], "temperature": 0.0, "avg_logprob": -0.3035565867568507, "compression_ratio": 1.4375, "no_speech_prob": 4.49495792054222e-06}, {"id": 208, "seek": 60754, "start": 625.78, "end": 634.9, "text": " Yeah, it's orthonormal or the depending if it's the column.", "tokens": [865, 11, 309, 311, 420, 11943, 24440, 420, 264, 5413, 498, 309, 311, 264, 7738, 13], "temperature": 0.0, "avg_logprob": -0.3035565867568507, "compression_ratio": 1.4375, "no_speech_prob": 4.49495792054222e-06}, {"id": 209, "seek": 63490, "start": 634.9, "end": 637.8199999999999, "text": " I always have to look up if it's the columns or the rows", "tokens": [286, 1009, 362, 281, 574, 493, 498, 309, 311, 264, 13766, 420, 264, 13241], "temperature": 0.0, "avg_logprob": -0.25189772213206574, "compression_ratio": 1.7941176470588236, "no_speech_prob": 8.80050993146142e-06}, {"id": 210, "seek": 63490, "start": 637.8199999999999, "end": 640.8199999999999, "text": " or orthonormal to each other.", "tokens": [420, 420, 11943, 24440, 281, 1184, 661, 13], "temperature": 0.0, "avg_logprob": -0.25189772213206574, "compression_ratio": 1.7941176470588236, "no_speech_prob": 8.80050993146142e-06}, {"id": 211, "seek": 63490, "start": 640.8199999999999, "end": 643.02, "text": " And what does it mean to be orthonormal?", "tokens": [400, 437, 775, 309, 914, 281, 312, 420, 11943, 24440, 30], "temperature": 0.0, "avg_logprob": -0.25189772213206574, "compression_ratio": 1.7941176470588236, "no_speech_prob": 8.80050993146142e-06}, {"id": 212, "seek": 63490, "start": 650.8199999999999, "end": 655.38, "text": " The columns are orthogonal to each other.", "tokens": [440, 13766, 366, 41488, 281, 1184, 661, 13], "temperature": 0.0, "avg_logprob": -0.25189772213206574, "compression_ratio": 1.7941176470588236, "no_speech_prob": 8.80050993146142e-06}, {"id": 213, "seek": 63490, "start": 655.38, "end": 657.74, "text": " Yeah, so the answer is that the columns", "tokens": [865, 11, 370, 264, 1867, 307, 300, 264, 13766], "temperature": 0.0, "avg_logprob": -0.25189772213206574, "compression_ratio": 1.7941176470588236, "no_speech_prob": 8.80050993146142e-06}, {"id": 214, "seek": 63490, "start": 657.74, "end": 661.6999999999999, "text": " are orthogonal to each other and pairwise normalized.", "tokens": [366, 41488, 281, 1184, 661, 293, 6119, 3711, 48704, 13], "temperature": 0.0, "avg_logprob": -0.25189772213206574, "compression_ratio": 1.7941176470588236, "no_speech_prob": 8.80050993146142e-06}, {"id": 215, "seek": 63490, "start": 661.6999999999999, "end": 663.38, "text": " So that means if you take the dot product", "tokens": [407, 300, 1355, 498, 291, 747, 264, 5893, 1674], "temperature": 0.0, "avg_logprob": -0.25189772213206574, "compression_ratio": 1.7941176470588236, "no_speech_prob": 8.80050993146142e-06}, {"id": 216, "seek": 66338, "start": 663.38, "end": 666.22, "text": " of two different columns, you get 0.", "tokens": [295, 732, 819, 13766, 11, 291, 483, 1958, 13], "temperature": 0.0, "avg_logprob": -0.16694343090057373, "compression_ratio": 1.5027322404371584, "no_speech_prob": 6.144020517240278e-06}, {"id": 217, "seek": 66338, "start": 666.22, "end": 669.02, "text": " If you take the dot product of a column with itself, you get 1.", "tokens": [759, 291, 747, 264, 5893, 1674, 295, 257, 7738, 365, 2564, 11, 291, 483, 502, 13], "temperature": 0.0, "avg_logprob": -0.16694343090057373, "compression_ratio": 1.5027322404371584, "no_speech_prob": 6.144020517240278e-06}, {"id": 218, "seek": 66338, "start": 671.5, "end": 676.1, "text": " How about S, the middle matrix?", "tokens": [1012, 466, 318, 11, 264, 2808, 8141, 30], "temperature": 0.0, "avg_logprob": -0.16694343090057373, "compression_ratio": 1.5027322404371584, "no_speech_prob": 6.144020517240278e-06}, {"id": 219, "seek": 66338, "start": 676.1, "end": 681.18, "text": " Yes, S is diagonal is the key here.", "tokens": [1079, 11, 318, 307, 21539, 307, 264, 2141, 510, 13], "temperature": 0.0, "avg_logprob": -0.16694343090057373, "compression_ratio": 1.5027322404371584, "no_speech_prob": 6.144020517240278e-06}, {"id": 220, "seek": 66338, "start": 681.18, "end": 682.3, "text": " What other properties?", "tokens": [708, 661, 7221, 30], "temperature": 0.0, "avg_logprob": -0.16694343090057373, "compression_ratio": 1.5027322404371584, "no_speech_prob": 6.144020517240278e-06}, {"id": 221, "seek": 66338, "start": 682.3, "end": 685.06, "text": " And just to remind you, being diagonal", "tokens": [400, 445, 281, 4160, 291, 11, 885, 21539], "temperature": 0.0, "avg_logprob": -0.16694343090057373, "compression_ratio": 1.5027322404371584, "no_speech_prob": 6.144020517240278e-06}, {"id": 222, "seek": 66338, "start": 685.06, "end": 687.82, "text": " means that everything off the diagonal is 0.", "tokens": [1355, 300, 1203, 766, 264, 21539, 307, 1958, 13], "temperature": 0.0, "avg_logprob": -0.16694343090057373, "compression_ratio": 1.5027322404371584, "no_speech_prob": 6.144020517240278e-06}, {"id": 223, "seek": 68782, "start": 687.82, "end": 690.9000000000001, "text": " Anything you can say about the numbers on the diagonal?", "tokens": [11998, 291, 393, 584, 466, 264, 3547, 322, 264, 21539, 30], "temperature": 0.0, "avg_logprob": -0.49829333761463995, "compression_ratio": 1.5930232558139534, "no_speech_prob": 3.535420910338871e-05}, {"id": 224, "seek": 68782, "start": 697.3000000000001, "end": 700.0200000000001, "text": " They're like the principal components", "tokens": [814, 434, 411, 264, 9716, 6677], "temperature": 0.0, "avg_logprob": -0.49829333761463995, "compression_ratio": 1.5930232558139534, "no_speech_prob": 3.535420910338871e-05}, {"id": 225, "seek": 68782, "start": 700.0200000000001, "end": 703.6600000000001, "text": " of the original matrix.", "tokens": [295, 264, 3380, 8141, 13], "temperature": 0.0, "avg_logprob": -0.49829333761463995, "compression_ratio": 1.5930232558139534, "no_speech_prob": 3.535420910338871e-05}, {"id": 226, "seek": 68782, "start": 703.6600000000001, "end": 708.74, "text": " So if they encode for something important about the original", "tokens": [407, 498, 436, 2058, 1429, 337, 746, 1021, 466, 264, 3380], "temperature": 0.0, "avg_logprob": -0.49829333761463995, "compression_ratio": 1.5930232558139534, "no_speech_prob": 3.535420910338871e-05}, {"id": 227, "seek": 68782, "start": 708.74, "end": 709.2600000000001, "text": " matrix.", "tokens": [8141, 13], "temperature": 0.0, "avg_logprob": -0.49829333761463995, "compression_ratio": 1.5930232558139534, "no_speech_prob": 3.535420910338871e-05}, {"id": 228, "seek": 68782, "start": 709.2600000000001, "end": 712.4200000000001, "text": " They're kind of capturing an idea of importance.", "tokens": [814, 434, 733, 295, 23384, 364, 1558, 295, 7379, 13], "temperature": 0.0, "avg_logprob": -0.49829333761463995, "compression_ratio": 1.5930232558139534, "no_speech_prob": 3.535420910338871e-05}, {"id": 229, "seek": 68782, "start": 712.4200000000001, "end": 714.9000000000001, "text": " And so they're called singular values.", "tokens": [400, 370, 436, 434, 1219, 20010, 4190, 13], "temperature": 0.0, "avg_logprob": -0.49829333761463995, "compression_ratio": 1.5930232558139534, "no_speech_prob": 3.535420910338871e-05}, {"id": 230, "seek": 71490, "start": 714.9, "end": 718.02, "text": " They're in descending order, so you kind of have the biggest", "tokens": [814, 434, 294, 40182, 1668, 11, 370, 291, 733, 295, 362, 264, 3880], "temperature": 0.0, "avg_logprob": -0.334409065616941, "compression_ratio": 1.5344827586206897, "no_speech_prob": 5.649041850119829e-05}, {"id": 231, "seek": 71490, "start": 718.02, "end": 718.9399999999999, "text": " ones first.", "tokens": [2306, 700, 13], "temperature": 0.0, "avg_logprob": -0.334409065616941, "compression_ratio": 1.5344827586206897, "no_speech_prob": 5.649041850119829e-05}, {"id": 232, "seek": 71490, "start": 718.9399999999999, "end": 720.9399999999999, "text": " They're non-negative.", "tokens": [814, 434, 2107, 12, 28561, 1166, 13], "temperature": 0.0, "avg_logprob": -0.334409065616941, "compression_ratio": 1.5344827586206897, "no_speech_prob": 5.649041850119829e-05}, {"id": 233, "seek": 71490, "start": 720.9399999999999, "end": 723.8199999999999, "text": " But yeah, they've captured some importance.", "tokens": [583, 1338, 11, 436, 600, 11828, 512, 7379, 13], "temperature": 0.0, "avg_logprob": -0.334409065616941, "compression_ratio": 1.5344827586206897, "no_speech_prob": 5.649041850119829e-05}, {"id": 234, "seek": 71490, "start": 723.8199999999999, "end": 727.78, "text": " Because you'll remember the scale of u and v is both 1", "tokens": [1436, 291, 603, 1604, 264, 4373, 295, 344, 293, 371, 307, 1293, 502], "temperature": 0.0, "avg_logprob": -0.334409065616941, "compression_ratio": 1.5344827586206897, "no_speech_prob": 5.649041850119829e-05}, {"id": 235, "seek": 71490, "start": 727.78, "end": 731.06, "text": " and that they're normalized.", "tokens": [293, 300, 436, 434, 48704, 13], "temperature": 0.0, "avg_logprob": -0.334409065616941, "compression_ratio": 1.5344827586206897, "no_speech_prob": 5.649041850119829e-05}, {"id": 236, "seek": 71490, "start": 731.06, "end": 733.98, "text": " I guess that was a little bit of a spoiler about v.", "tokens": [286, 2041, 300, 390, 257, 707, 857, 295, 257, 26927, 466, 371, 13], "temperature": 0.0, "avg_logprob": -0.334409065616941, "compression_ratio": 1.5344827586206897, "no_speech_prob": 5.649041850119829e-05}, {"id": 237, "seek": 71490, "start": 733.98, "end": 738.78, "text": " V has the same properties as u, only", "tokens": [691, 575, 264, 912, 7221, 382, 344, 11, 787], "temperature": 0.0, "avg_logprob": -0.334409065616941, "compression_ratio": 1.5344827586206897, "no_speech_prob": 5.649041850119829e-05}, {"id": 238, "seek": 71490, "start": 738.78, "end": 743.22, "text": " on the transpose of the rows being the same.", "tokens": [322, 264, 25167, 295, 264, 13241, 885, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.334409065616941, "compression_ratio": 1.5344827586206897, "no_speech_prob": 5.649041850119829e-05}, {"id": 239, "seek": 74322, "start": 743.22, "end": 747.14, "text": " So the transpose of the rows being orthonormal to each other.", "tokens": [407, 264, 25167, 295, 264, 13241, 885, 420, 11943, 24440, 281, 1184, 661, 13], "temperature": 0.0, "avg_logprob": -0.29345832237830527, "compression_ratio": 1.4472049689440993, "no_speech_prob": 1.4509492757497355e-05}, {"id": 240, "seek": 74322, "start": 750.34, "end": 753.58, "text": " Any questions about this?", "tokens": [2639, 1651, 466, 341, 30], "temperature": 0.0, "avg_logprob": -0.29345832237830527, "compression_ratio": 1.4472049689440993, "no_speech_prob": 1.4509492757497355e-05}, {"id": 241, "seek": 74322, "start": 758.14, "end": 760.46, "text": " And then nmf, as the name suggests,", "tokens": [400, 550, 297, 76, 69, 11, 382, 264, 1315, 13409, 11], "temperature": 0.0, "avg_logprob": -0.29345832237830527, "compression_ratio": 1.4472049689440993, "no_speech_prob": 1.4509492757497355e-05}, {"id": 242, "seek": 74322, "start": 760.46, "end": 762.7, "text": " what's the special property of the matrices", "tokens": [437, 311, 264, 2121, 4707, 295, 264, 32284], "temperature": 0.0, "avg_logprob": -0.29345832237830527, "compression_ratio": 1.4472049689440993, "no_speech_prob": 1.4509492757497355e-05}, {"id": 243, "seek": 74322, "start": 762.7, "end": 764.7, "text": " you get there in your decomposition?", "tokens": [291, 483, 456, 294, 428, 48356, 30], "temperature": 0.0, "avg_logprob": -0.29345832237830527, "compression_ratio": 1.4472049689440993, "no_speech_prob": 1.4509492757497355e-05}, {"id": 244, "seek": 76470, "start": 764.7, "end": 769.7800000000001, "text": " Anyone want to shout it out?", "tokens": [14643, 528, 281, 8043, 309, 484, 30], "temperature": 0.0, "avg_logprob": -0.20209429340977822, "compression_ratio": 1.3546099290780143, "no_speech_prob": 2.668711204023566e-05}, {"id": 245, "seek": 76470, "start": 778.1400000000001, "end": 781.86, "text": " What does nmf stand for?", "tokens": [708, 775, 297, 76, 69, 1463, 337, 30], "temperature": 0.0, "avg_logprob": -0.20209429340977822, "compression_ratio": 1.3546099290780143, "no_speech_prob": 2.668711204023566e-05}, {"id": 246, "seek": 76470, "start": 781.86, "end": 784.34, "text": " Yeah, non-negative matrix factorization.", "tokens": [865, 11, 2107, 12, 28561, 1166, 8141, 5952, 2144, 13], "temperature": 0.0, "avg_logprob": -0.20209429340977822, "compression_ratio": 1.3546099290780143, "no_speech_prob": 2.668711204023566e-05}, {"id": 247, "seek": 76470, "start": 784.34, "end": 788.5400000000001, "text": " So the matrices you get are non-negative.", "tokens": [407, 264, 32284, 291, 483, 366, 2107, 12, 28561, 1166, 13], "temperature": 0.0, "avg_logprob": -0.20209429340977822, "compression_ratio": 1.3546099290780143, "no_speech_prob": 2.668711204023566e-05}, {"id": 248, "seek": 76470, "start": 788.5400000000001, "end": 792.26, "text": " And they also, in practice, often end up being sparse.", "tokens": [400, 436, 611, 11, 294, 3124, 11, 2049, 917, 493, 885, 637, 11668, 13], "temperature": 0.0, "avg_logprob": -0.20209429340977822, "compression_ratio": 1.3546099290780143, "no_speech_prob": 2.668711204023566e-05}, {"id": 249, "seek": 79226, "start": 792.26, "end": 795.9399999999999, "text": " And we'll talk a little bit more about sparse matrices today.", "tokens": [400, 321, 603, 751, 257, 707, 857, 544, 466, 637, 11668, 32284, 965, 13], "temperature": 0.0, "avg_logprob": -0.16384991237095425, "compression_ratio": 1.641350210970464, "no_speech_prob": 1.8924147298093885e-05}, {"id": 250, "seek": 79226, "start": 795.9399999999999, "end": 798.22, "text": " But that means that many of the values are 0.", "tokens": [583, 300, 1355, 300, 867, 295, 264, 4190, 366, 1958, 13], "temperature": 0.0, "avg_logprob": -0.16384991237095425, "compression_ratio": 1.641350210970464, "no_speech_prob": 1.8924147298093885e-05}, {"id": 251, "seek": 79226, "start": 801.34, "end": 803.78, "text": " And so just keep in mind this analogy", "tokens": [400, 370, 445, 1066, 294, 1575, 341, 21663], "temperature": 0.0, "avg_logprob": -0.16384991237095425, "compression_ratio": 1.641350210970464, "no_speech_prob": 1.8924147298093885e-05}, {"id": 252, "seek": 79226, "start": 803.78, "end": 806.46, "text": " with prime factorization is something", "tokens": [365, 5835, 5952, 2144, 307, 746], "temperature": 0.0, "avg_logprob": -0.16384991237095425, "compression_ratio": 1.641350210970464, "no_speech_prob": 1.8924147298093885e-05}, {"id": 253, "seek": 79226, "start": 806.46, "end": 808.86, "text": " that we're doing in a kind of more complicated way", "tokens": [300, 321, 434, 884, 294, 257, 733, 295, 544, 6179, 636], "temperature": 0.0, "avg_logprob": -0.16384991237095425, "compression_ratio": 1.641350210970464, "no_speech_prob": 1.8924147298093885e-05}, {"id": 254, "seek": 79226, "start": 808.86, "end": 809.9, "text": " with matrices.", "tokens": [365, 32284, 13], "temperature": 0.0, "avg_logprob": -0.16384991237095425, "compression_ratio": 1.641350210970464, "no_speech_prob": 1.8924147298093885e-05}, {"id": 255, "seek": 79226, "start": 813.02, "end": 816.42, "text": " Next, I wanted to just review a little bit of linear algebra,", "tokens": [3087, 11, 286, 1415, 281, 445, 3131, 257, 707, 857, 295, 8213, 21989, 11], "temperature": 0.0, "avg_logprob": -0.16384991237095425, "compression_ratio": 1.641350210970464, "no_speech_prob": 1.8924147298093885e-05}, {"id": 256, "seek": 79226, "start": 816.42, "end": 818.62, "text": " since I know it may have been a while, since you've", "tokens": [1670, 286, 458, 309, 815, 362, 668, 257, 1339, 11, 1670, 291, 600], "temperature": 0.0, "avg_logprob": -0.16384991237095425, "compression_ratio": 1.641350210970464, "no_speech_prob": 1.8924147298093885e-05}, {"id": 257, "seek": 79226, "start": 818.62, "end": 821.42, "text": " seen some linear algebra.", "tokens": [1612, 512, 8213, 21989, 13], "temperature": 0.0, "avg_logprob": -0.16384991237095425, "compression_ratio": 1.641350210970464, "no_speech_prob": 1.8924147298093885e-05}, {"id": 258, "seek": 82142, "start": 821.42, "end": 826.18, "text": " This is a link I like just for matrix vector multiplication.", "tokens": [639, 307, 257, 2113, 286, 411, 445, 337, 8141, 8062, 27290, 13], "temperature": 0.0, "avg_logprob": -0.1318639690956373, "compression_ratio": 1.592039800995025, "no_speech_prob": 8.748588879825547e-05}, {"id": 259, "seek": 82142, "start": 826.18, "end": 831.6999999999999, "text": " And it's matrixmultiplication.xyz.", "tokens": [400, 309, 311, 8141, 76, 723, 72, 4770, 399, 13, 12876, 89, 13], "temperature": 0.0, "avg_logprob": -0.1318639690956373, "compression_ratio": 1.592039800995025, "no_speech_prob": 8.748588879825547e-05}, {"id": 260, "seek": 82142, "start": 831.6999999999999, "end": 834.42, "text": " But here, and you can change these numbers", "tokens": [583, 510, 11, 293, 291, 393, 1319, 613, 3547], "temperature": 0.0, "avg_logprob": -0.1318639690956373, "compression_ratio": 1.592039800995025, "no_speech_prob": 8.748588879825547e-05}, {"id": 261, "seek": 82142, "start": 834.42, "end": 837.62, "text": " if you wanted them to be something different", "tokens": [498, 291, 1415, 552, 281, 312, 746, 819], "temperature": 0.0, "avg_logprob": -0.1318639690956373, "compression_ratio": 1.592039800995025, "no_speech_prob": 8.748588879825547e-05}, {"id": 262, "seek": 82142, "start": 837.62, "end": 842.38, "text": " or change the size of your matrices", "tokens": [420, 1319, 264, 2744, 295, 428, 32284], "temperature": 0.0, "avg_logprob": -0.1318639690956373, "compression_ratio": 1.592039800995025, "no_speech_prob": 8.748588879825547e-05}, {"id": 263, "seek": 82142, "start": 842.38, "end": 843.6999999999999, "text": " that you're multiplying.", "tokens": [300, 291, 434, 30955, 13], "temperature": 0.0, "avg_logprob": -0.1318639690956373, "compression_ratio": 1.592039800995025, "no_speech_prob": 8.748588879825547e-05}, {"id": 264, "seek": 82142, "start": 843.6999999999999, "end": 846.6999999999999, "text": " Actually, I guess I said matrix vector.", "tokens": [5135, 11, 286, 2041, 286, 848, 8141, 8062, 13], "temperature": 0.0, "avg_logprob": -0.1318639690956373, "compression_ratio": 1.592039800995025, "no_speech_prob": 8.748588879825547e-05}, {"id": 265, "seek": 82142, "start": 846.6999999999999, "end": 850.38, "text": " This kind of shows you step by step", "tokens": [639, 733, 295, 3110, 291, 1823, 538, 1823], "temperature": 0.0, "avg_logprob": -0.1318639690956373, "compression_ratio": 1.592039800995025, "no_speech_prob": 8.748588879825547e-05}, {"id": 266, "seek": 85038, "start": 850.38, "end": 854.7, "text": " kind of how you're sliding this vector along.", "tokens": [733, 295, 577, 291, 434, 21169, 341, 8062, 2051, 13], "temperature": 0.0, "avg_logprob": -0.11756861069623162, "compression_ratio": 1.6363636363636365, "no_speech_prob": 2.5464541977271438e-05}, {"id": 267, "seek": 85038, "start": 854.7, "end": 856.7, "text": " And this fits with the interpretation", "tokens": [400, 341, 9001, 365, 264, 14174], "temperature": 0.0, "avg_logprob": -0.11756861069623162, "compression_ratio": 1.6363636363636365, "no_speech_prob": 2.5464541977271438e-05}, {"id": 268, "seek": 85038, "start": 856.7, "end": 859.14, "text": " I had mentioned last time about thinking", "tokens": [286, 632, 2835, 1036, 565, 466, 1953], "temperature": 0.0, "avg_logprob": -0.11756861069623162, "compression_ratio": 1.6363636363636365, "no_speech_prob": 2.5464541977271438e-05}, {"id": 269, "seek": 85038, "start": 859.14, "end": 861.14, "text": " of multiplying a matrix by a vector", "tokens": [295, 30955, 257, 8141, 538, 257, 8062], "temperature": 0.0, "avg_logprob": -0.11756861069623162, "compression_ratio": 1.6363636363636365, "no_speech_prob": 2.5464541977271438e-05}, {"id": 270, "seek": 85038, "start": 861.14, "end": 864.42, "text": " is like taking a linear combination of the columns", "tokens": [307, 411, 1940, 257, 8213, 6562, 295, 264, 13766], "temperature": 0.0, "avg_logprob": -0.11756861069623162, "compression_ratio": 1.6363636363636365, "no_speech_prob": 2.5464541977271438e-05}, {"id": 271, "seek": 85038, "start": 864.42, "end": 867.66, "text": " in that matrix.", "tokens": [294, 300, 8141, 13], "temperature": 0.0, "avg_logprob": -0.11756861069623162, "compression_ratio": 1.6363636363636365, "no_speech_prob": 2.5464541977271438e-05}, {"id": 272, "seek": 85038, "start": 867.66, "end": 870.66, "text": " And here, we've multiplied 3 by the first column,", "tokens": [400, 510, 11, 321, 600, 17207, 805, 538, 264, 700, 7738, 11], "temperature": 0.0, "avg_logprob": -0.11756861069623162, "compression_ratio": 1.6363636363636365, "no_speech_prob": 2.5464541977271438e-05}, {"id": 273, "seek": 85038, "start": 870.66, "end": 874.1, "text": " 6 by the second column, 1 by the third column.", "tokens": [1386, 538, 264, 1150, 7738, 11, 502, 538, 264, 2636, 7738, 13], "temperature": 0.0, "avg_logprob": -0.11756861069623162, "compression_ratio": 1.6363636363636365, "no_speech_prob": 2.5464541977271438e-05}, {"id": 274, "seek": 87410, "start": 874.1, "end": 882.1, "text": " And then we can also see that with two matrices,", "tokens": [400, 550, 321, 393, 611, 536, 300, 365, 732, 32284, 11], "temperature": 0.0, "avg_logprob": -0.2961864241634507, "compression_ratio": 1.6473988439306357, "no_speech_prob": 1.1300550795567688e-05}, {"id": 275, "seek": 87410, "start": 882.1, "end": 884.94, "text": " how we're taking a linear combination of 3", "tokens": [577, 321, 434, 1940, 257, 8213, 6562, 295, 805], "temperature": 0.0, "avg_logprob": -0.2961864241634507, "compression_ratio": 1.6473988439306357, "no_speech_prob": 1.1300550795567688e-05}, {"id": 276, "seek": 87410, "start": 884.94, "end": 888.14, "text": " by the first column, 6 by the second, 1 by the third.", "tokens": [538, 264, 700, 7738, 11, 1386, 538, 264, 1150, 11, 502, 538, 264, 2636, 13], "temperature": 0.0, "avg_logprob": -0.2961864241634507, "compression_ratio": 1.6473988439306357, "no_speech_prob": 1.1300550795567688e-05}, {"id": 277, "seek": 87410, "start": 888.14, "end": 893.34, "text": " And we're taking a second linear combination of 1, 1, 1,", "tokens": [400, 321, 434, 1940, 257, 1150, 8213, 6562, 295, 502, 11, 502, 11, 502, 11], "temperature": 0.0, "avg_logprob": -0.2961864241634507, "compression_ratio": 1.6473988439306357, "no_speech_prob": 1.1300550795567688e-05}, {"id": 278, "seek": 87410, "start": 893.34, "end": 894.94, "text": " times the three columns.", "tokens": [1413, 264, 1045, 13766, 13], "temperature": 0.0, "avg_logprob": -0.2961864241634507, "compression_ratio": 1.6473988439306357, "no_speech_prob": 1.1300550795567688e-05}, {"id": 279, "seek": 87410, "start": 898.26, "end": 901.6600000000001, "text": " So I thought that was just a nice way to see it visually.", "tokens": [407, 286, 1194, 300, 390, 445, 257, 1481, 636, 281, 536, 309, 19622, 13], "temperature": 0.0, "avg_logprob": -0.2961864241634507, "compression_ratio": 1.6473988439306357, "no_speech_prob": 1.1300550795567688e-05}, {"id": 280, "seek": 90166, "start": 901.66, "end": 906.66, "text": " And then last time, I showed this example", "tokens": [400, 550, 1036, 565, 11, 286, 4712, 341, 1365], "temperature": 0.0, "avg_logprob": -0.5000562996699892, "compression_ratio": 1.5467980295566504, "no_speech_prob": 1.618433088879101e-05}, {"id": 281, "seek": 90166, "start": 906.66, "end": 910.42, "text": " that comes from computer vision, not NLP,", "tokens": [300, 1487, 490, 3820, 5201, 11, 406, 426, 45196, 11], "temperature": 0.0, "avg_logprob": -0.5000562996699892, "compression_ratio": 1.5467980295566504, "no_speech_prob": 1.618433088879101e-05}, {"id": 282, "seek": 90166, "start": 910.42, "end": 914.78, "text": " but of having a bunch of pictures of faces", "tokens": [457, 295, 1419, 257, 3840, 295, 5242, 295, 8475], "temperature": 0.0, "avg_logprob": -0.5000562996699892, "compression_ratio": 1.5467980295566504, "no_speech_prob": 1.618433088879101e-05}, {"id": 283, "seek": 90166, "start": 914.78, "end": 919.38, "text": " and decomposing them into their facial features.", "tokens": [293, 22867, 6110, 552, 666, 641, 15642, 4122, 13], "temperature": 0.0, "avg_logprob": -0.5000562996699892, "compression_ratio": 1.5467980295566504, "no_speech_prob": 1.618433088879101e-05}, {"id": 284, "seek": 90166, "start": 919.38, "end": 922.06, "text": " Here, NMF has been used.", "tokens": [1692, 11, 426, 44, 37, 575, 668, 1143, 13], "temperature": 0.0, "avg_logprob": -0.5000562996699892, "compression_ratio": 1.5467980295566504, "no_speech_prob": 1.618433088879101e-05}, {"id": 285, "seek": 90166, "start": 922.06, "end": 924.26, "text": " And then so one of the matrices would", "tokens": [400, 550, 370, 472, 295, 264, 32284, 576], "temperature": 0.0, "avg_logprob": -0.5000562996699892, "compression_ratio": 1.5467980295566504, "no_speech_prob": 1.618433088879101e-05}, {"id": 286, "seek": 90166, "start": 924.26, "end": 926.26, "text": " represent the different faces.", "tokens": [2906, 264, 819, 8475, 13], "temperature": 0.0, "avg_logprob": -0.5000562996699892, "compression_ratio": 1.5467980295566504, "no_speech_prob": 1.618433088879101e-05}, {"id": 287, "seek": 90166, "start": 926.26, "end": 929.66, "text": " And then the other one is the matrix vector.", "tokens": [400, 550, 264, 661, 472, 307, 264, 8141, 8062, 13], "temperature": 0.0, "avg_logprob": -0.5000562996699892, "compression_ratio": 1.5467980295566504, "no_speech_prob": 1.618433088879101e-05}, {"id": 288, "seek": 92966, "start": 929.66, "end": 932.2199999999999, "text": " And matrices would represent the different features", "tokens": [400, 32284, 576, 2906, 264, 819, 4122], "temperature": 0.0, "avg_logprob": -0.13616425415565228, "compression_ratio": 1.7925311203319503, "no_speech_prob": 2.078302168229129e-05}, {"id": 289, "seek": 92966, "start": 932.2199999999999, "end": 933.2199999999999, "text": " you could have.", "tokens": [291, 727, 362, 13], "temperature": 0.0, "avg_logprob": -0.13616425415565228, "compression_ratio": 1.7925311203319503, "no_speech_prob": 2.078302168229129e-05}, {"id": 290, "seek": 92966, "start": 933.2199999999999, "end": 936.1, "text": " In this case, it's kind of like a particular bridge", "tokens": [682, 341, 1389, 11, 309, 311, 733, 295, 411, 257, 1729, 7283], "temperature": 0.0, "avg_logprob": -0.13616425415565228, "compression_ratio": 1.7925311203319503, "no_speech_prob": 2.078302168229129e-05}, {"id": 291, "seek": 92966, "start": 936.1, "end": 938.54, "text": " of your nose and under your eyes.", "tokens": [295, 428, 6690, 293, 833, 428, 2575, 13], "temperature": 0.0, "avg_logprob": -0.13616425415565228, "compression_ratio": 1.7925311203319503, "no_speech_prob": 2.078302168229129e-05}, {"id": 292, "seek": 92966, "start": 938.54, "end": 941.26, "text": " This one looks like just the tip of a nose.", "tokens": [639, 472, 1542, 411, 445, 264, 4125, 295, 257, 6690, 13], "temperature": 0.0, "avg_logprob": -0.13616425415565228, "compression_ratio": 1.7925311203319503, "no_speech_prob": 2.078302168229129e-05}, {"id": 293, "seek": 92966, "start": 941.26, "end": 943.2199999999999, "text": " This is someone's brows.", "tokens": [639, 307, 1580, 311, 8333, 13], "temperature": 0.0, "avg_logprob": -0.13616425415565228, "compression_ratio": 1.7925311203319503, "no_speech_prob": 2.078302168229129e-05}, {"id": 294, "seek": 92966, "start": 943.2199999999999, "end": 945.6999999999999, "text": " And in general, you'd have a lot more of these.", "tokens": [400, 294, 2674, 11, 291, 1116, 362, 257, 688, 544, 295, 613, 13], "temperature": 0.0, "avg_logprob": -0.13616425415565228, "compression_ratio": 1.7925311203319503, "no_speech_prob": 2.078302168229129e-05}, {"id": 295, "seek": 92966, "start": 945.6999999999999, "end": 947.86, "text": " And then you could take a linear combination.", "tokens": [400, 550, 291, 727, 747, 257, 8213, 6562, 13], "temperature": 0.0, "avg_logprob": -0.13616425415565228, "compression_ratio": 1.7925311203319503, "no_speech_prob": 2.078302168229129e-05}, {"id": 296, "seek": 92966, "start": 947.86, "end": 949.78, "text": " And you could think of that as giving you", "tokens": [400, 291, 727, 519, 295, 300, 382, 2902, 291], "temperature": 0.0, "avg_logprob": -0.13616425415565228, "compression_ratio": 1.7925311203319503, "no_speech_prob": 2.078302168229129e-05}, {"id": 297, "seek": 92966, "start": 949.78, "end": 952.9, "text": " kind of the different importances or weights", "tokens": [733, 295, 264, 819, 974, 2676, 420, 17443], "temperature": 0.0, "avg_logprob": -0.13616425415565228, "compression_ratio": 1.7925311203319503, "no_speech_prob": 2.078302168229129e-05}, {"id": 298, "seek": 92966, "start": 952.9, "end": 954.38, "text": " of these different features.", "tokens": [295, 613, 819, 4122, 13], "temperature": 0.0, "avg_logprob": -0.13616425415565228, "compression_ratio": 1.7925311203319503, "no_speech_prob": 2.078302168229129e-05}, {"id": 299, "seek": 95438, "start": 954.38, "end": 959.38, "text": " And then who here has watched some 3Blue1Brown videos?", "tokens": [400, 550, 567, 510, 575, 6337, 512, 805, 45231, 16, 22170, 648, 2145, 30], "temperature": 0.0, "avg_logprob": -0.3033332124762579, "compression_ratio": 1.5564853556485356, "no_speech_prob": 3.763369386433624e-05}, {"id": 300, "seek": 95438, "start": 959.38, "end": 961.38, "text": " OK, good number, but not everybody,", "tokens": [2264, 11, 665, 1230, 11, 457, 406, 2201, 11], "temperature": 0.0, "avg_logprob": -0.3033332124762579, "compression_ratio": 1.5564853556485356, "no_speech_prob": 3.763369386433624e-05}, {"id": 301, "seek": 95438, "start": 961.38, "end": 966.38, "text": " which is just sad because I love 3Blue1Brown.", "tokens": [597, 307, 445, 4227, 570, 286, 959, 805, 45231, 16, 22170, 648, 13], "temperature": 0.0, "avg_logprob": -0.3033332124762579, "compression_ratio": 1.5564853556485356, "no_speech_prob": 3.763369386433624e-05}, {"id": 302, "seek": 95438, "start": 966.38, "end": 969.38, "text": " Grant Sanderson has done a bunch of videos", "tokens": [17529, 7985, 3953, 575, 1096, 257, 3840, 295, 2145], "temperature": 0.0, "avg_logprob": -0.3033332124762579, "compression_ratio": 1.5564853556485356, "no_speech_prob": 3.763369386433624e-05}, {"id": 303, "seek": 95438, "start": 969.38, "end": 975.38, "text": " on a wide variety of math and computer science context.", "tokens": [322, 257, 4874, 5673, 295, 5221, 293, 3820, 3497, 4319, 13], "temperature": 0.0, "avg_logprob": -0.3033332124762579, "compression_ratio": 1.5564853556485356, "no_speech_prob": 3.763369386433624e-05}, {"id": 304, "seek": 95438, "start": 975.38, "end": 977.38, "text": " I wanted to show you one today.", "tokens": [286, 1415, 281, 855, 291, 472, 965, 13], "temperature": 0.0, "avg_logprob": -0.3033332124762579, "compression_ratio": 1.5564853556485356, "no_speech_prob": 3.763369386433624e-05}, {"id": 305, "seek": 95438, "start": 977.38, "end": 980.38, "text": " This is from the Essence of Linear Algebra series.", "tokens": [639, 307, 490, 264, 14357, 655, 295, 14670, 289, 967, 19983, 2638, 13], "temperature": 0.0, "avg_logprob": -0.3033332124762579, "compression_ratio": 1.5564853556485356, "no_speech_prob": 3.763369386433624e-05}, {"id": 306, "seek": 95438, "start": 980.38, "end": 983.38, "text": " And it's called the Essence of Linear Algebra Series.", "tokens": [400, 309, 311, 1219, 264, 14357, 655, 295, 14670, 289, 967, 19983, 13934, 13], "temperature": 0.0, "avg_logprob": -0.3033332124762579, "compression_ratio": 1.5564853556485356, "no_speech_prob": 3.763369386433624e-05}, {"id": 307, "seek": 98338, "start": 983.38, "end": 986.38, "text": " Which I wanted to make a plug for.", "tokens": [3013, 286, 1415, 281, 652, 257, 5452, 337, 13], "temperature": 0.0, "avg_logprob": -0.11532492022360524, "compression_ratio": 1.8458646616541354, "no_speech_prob": 6.604020745726302e-05}, {"id": 308, "seek": 98338, "start": 986.38, "end": 989.38, "text": " It gives a very visual and geometric perspective", "tokens": [467, 2709, 257, 588, 5056, 293, 33246, 4585], "temperature": 0.0, "avg_logprob": -0.11532492022360524, "compression_ratio": 1.8458646616541354, "no_speech_prob": 6.604020745726302e-05}, {"id": 309, "seek": 98338, "start": 989.38, "end": 992.38, "text": " on linear algebra that I think is very different from how", "tokens": [322, 8213, 21989, 300, 286, 519, 307, 588, 819, 490, 577], "temperature": 0.0, "avg_logprob": -0.11532492022360524, "compression_ratio": 1.8458646616541354, "no_speech_prob": 6.604020745726302e-05}, {"id": 310, "seek": 98338, "start": 992.38, "end": 995.38, "text": " linear algebra is typically taught.", "tokens": [8213, 21989, 307, 5850, 5928, 13], "temperature": 0.0, "avg_logprob": -0.11532492022360524, "compression_ratio": 1.8458646616541354, "no_speech_prob": 6.604020745726302e-05}, {"id": 311, "seek": 98338, "start": 995.38, "end": 997.38, "text": " And something I like about these videos", "tokens": [400, 746, 286, 411, 466, 613, 2145], "temperature": 0.0, "avg_logprob": -0.11532492022360524, "compression_ratio": 1.8458646616541354, "no_speech_prob": 6.604020745726302e-05}, {"id": 312, "seek": 98338, "start": 997.38, "end": 1000.38, "text": " is I think they would be great if you're", "tokens": [307, 286, 519, 436, 576, 312, 869, 498, 291, 434], "temperature": 0.0, "avg_logprob": -0.11532492022360524, "compression_ratio": 1.8458646616541354, "no_speech_prob": 6.604020745726302e-05}, {"id": 313, "seek": 98338, "start": 1000.38, "end": 1002.38, "text": " a linear algebra beginner or you're feeling", "tokens": [257, 8213, 21989, 22080, 420, 291, 434, 2633], "temperature": 0.0, "avg_logprob": -0.11532492022360524, "compression_ratio": 1.8458646616541354, "no_speech_prob": 6.604020745726302e-05}, {"id": 314, "seek": 98338, "start": 1002.38, "end": 1004.38, "text": " very rusty about linear algebra.", "tokens": [588, 45394, 466, 8213, 21989, 13], "temperature": 0.0, "avg_logprob": -0.11532492022360524, "compression_ratio": 1.8458646616541354, "no_speech_prob": 6.604020745726302e-05}, {"id": 315, "seek": 98338, "start": 1004.38, "end": 1007.38, "text": " But also even if someone who knows a lot of linear algebra,", "tokens": [583, 611, 754, 498, 1580, 567, 3255, 257, 688, 295, 8213, 21989, 11], "temperature": 0.0, "avg_logprob": -0.11532492022360524, "compression_ratio": 1.8458646616541354, "no_speech_prob": 6.604020745726302e-05}, {"id": 316, "seek": 98338, "start": 1007.38, "end": 1010.38, "text": " I think they can give you a new perspective.", "tokens": [286, 519, 436, 393, 976, 291, 257, 777, 4585, 13], "temperature": 0.0, "avg_logprob": -0.11532492022360524, "compression_ratio": 1.8458646616541354, "no_speech_prob": 6.604020745726302e-05}, {"id": 317, "seek": 98338, "start": 1010.38, "end": 1012.38, "text": " And so I just wanted us to watch one of the videos", "tokens": [400, 370, 286, 445, 1415, 505, 281, 1159, 472, 295, 264, 2145], "temperature": 0.0, "avg_logprob": -0.11532492022360524, "compression_ratio": 1.8458646616541354, "no_speech_prob": 6.604020745726302e-05}, {"id": 318, "seek": 101238, "start": 1012.38, "end": 1014.38, "text": " together.", "tokens": [1214, 13], "temperature": 0.0, "avg_logprob": -0.14386543360623447, "compression_ratio": 1.4903846153846154, "no_speech_prob": 6.602204666705802e-05}, {"id": 319, "seek": 101238, "start": 1021.38, "end": 1023.38, "text": " All right, so I just wanted to share that", "tokens": [1057, 558, 11, 370, 286, 445, 1415, 281, 2073, 300], "temperature": 0.0, "avg_logprob": -0.14386543360623447, "compression_ratio": 1.4903846153846154, "no_speech_prob": 6.602204666705802e-05}, {"id": 320, "seek": 101238, "start": 1023.38, "end": 1025.38, "text": " as some linear algebra review.", "tokens": [382, 512, 8213, 21989, 3131, 13], "temperature": 0.0, "avg_logprob": -0.14386543360623447, "compression_ratio": 1.4903846153846154, "no_speech_prob": 6.602204666705802e-05}, {"id": 321, "seek": 101238, "start": 1025.38, "end": 1028.38, "text": " If you're interested in kind of thinking about how this", "tokens": [759, 291, 434, 3102, 294, 733, 295, 1953, 466, 577, 341], "temperature": 0.0, "avg_logprob": -0.14386543360623447, "compression_ratio": 1.4903846153846154, "no_speech_prob": 6.602204666705802e-05}, {"id": 322, "seek": 101238, "start": 1028.38, "end": 1031.38, "text": " relates to SVD, I definitely recommend looking at the Change", "tokens": [16155, 281, 31910, 35, 11, 286, 2138, 2748, 1237, 412, 264, 15060], "temperature": 0.0, "avg_logprob": -0.14386543360623447, "compression_ratio": 1.4903846153846154, "no_speech_prob": 6.602204666705802e-05}, {"id": 323, "seek": 101238, "start": 1031.38, "end": 1034.38, "text": " of Basis video as well.", "tokens": [295, 5859, 271, 960, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.14386543360623447, "compression_ratio": 1.4903846153846154, "no_speech_prob": 6.602204666705802e-05}, {"id": 324, "seek": 101238, "start": 1034.38, "end": 1037.38, "text": " And I'll see if we have kind of time to get more into that", "tokens": [400, 286, 603, 536, 498, 321, 362, 733, 295, 565, 281, 483, 544, 666, 300], "temperature": 0.0, "avg_logprob": -0.14386543360623447, "compression_ratio": 1.4903846153846154, "no_speech_prob": 6.602204666705802e-05}, {"id": 325, "seek": 101238, "start": 1037.38, "end": 1039.38, "text": " later in the course or not.", "tokens": [1780, 294, 264, 1164, 420, 406, 13], "temperature": 0.0, "avg_logprob": -0.14386543360623447, "compression_ratio": 1.4903846153846154, "no_speech_prob": 6.602204666705802e-05}, {"id": 326, "seek": 103938, "start": 1039.38, "end": 1045.38, "text": " For now I wanted to bring up the spreadsheet again.", "tokens": [1171, 586, 286, 1415, 281, 1565, 493, 264, 27733, 797, 13], "temperature": 0.0, "avg_logprob": -0.18448692379575787, "compression_ratio": 1.4518072289156627, "no_speech_prob": 8.217739377869293e-05}, {"id": 327, "seek": 103938, "start": 1045.38, "end": 1048.38, "text": " There was a question last time about how I generated all the", "tokens": [821, 390, 257, 1168, 1036, 565, 466, 577, 286, 10833, 439, 264], "temperature": 0.0, "avg_logprob": -0.18448692379575787, "compression_ratio": 1.4518072289156627, "no_speech_prob": 8.217739377869293e-05}, {"id": 328, "seek": 103938, "start": 1048.38, "end": 1050.38, "text": " numbers for the spreadsheet.", "tokens": [3547, 337, 264, 27733, 13], "temperature": 0.0, "avg_logprob": -0.18448692379575787, "compression_ratio": 1.4518072289156627, "no_speech_prob": 8.217739377869293e-05}, {"id": 329, "seek": 103938, "start": 1050.38, "end": 1054.38, "text": " And just to remind you, this figure.", "tokens": [400, 445, 281, 4160, 291, 11, 341, 2573, 13], "temperature": 0.0, "avg_logprob": -0.18448692379575787, "compression_ratio": 1.4518072289156627, "no_speech_prob": 8.217739377869293e-05}, {"id": 330, "seek": 103938, "start": 1062.38, "end": 1064.38, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.18448692379575787, "compression_ratio": 1.4518072289156627, "no_speech_prob": 8.217739377869293e-05}, {"id": 331, "seek": 103938, "start": 1064.38, "end": 1067.38, "text": " This was to give you kind of a more visual way of seeing", "tokens": [639, 390, 281, 976, 291, 733, 295, 257, 544, 5056, 636, 295, 2577], "temperature": 0.0, "avg_logprob": -0.18448692379575787, "compression_ratio": 1.4518072289156627, "no_speech_prob": 8.217739377869293e-05}, {"id": 332, "seek": 106738, "start": 1067.38, "end": 1070.38, "text": " what's happening with matrix decompositions like SVD or", "tokens": [437, 311, 2737, 365, 8141, 22867, 329, 2451, 411, 31910, 35, 420], "temperature": 0.0, "avg_logprob": -0.10248149525035512, "compression_ratio": 1.4508928571428572, "no_speech_prob": 0.00019395175331737846}, {"id": 333, "seek": 106738, "start": 1070.38, "end": 1072.38, "text": " NMF.", "tokens": [426, 44, 37, 13], "temperature": 0.0, "avg_logprob": -0.10248149525035512, "compression_ratio": 1.4508928571428572, "no_speech_prob": 0.00019395175331737846}, {"id": 334, "seek": 106738, "start": 1072.38, "end": 1078.38, "text": " And here I was using data from a collection of British novels", "tokens": [400, 510, 286, 390, 1228, 1412, 490, 257, 5765, 295, 6221, 24574], "temperature": 0.0, "avg_logprob": -0.10248149525035512, "compression_ratio": 1.4508928571428572, "no_speech_prob": 0.00019395175331737846}, {"id": 335, "seek": 106738, "start": 1078.38, "end": 1084.38, "text": " with just I think 64 vocabulary words representing them as a", "tokens": [365, 445, 286, 519, 12145, 19864, 2283, 13460, 552, 382, 257], "temperature": 0.0, "avg_logprob": -0.10248149525035512, "compression_ratio": 1.4508928571428572, "no_speech_prob": 0.00019395175331737846}, {"id": 336, "seek": 106738, "start": 1084.38, "end": 1086.38, "text": " term document matrix.", "tokens": [1433, 4166, 8141, 13], "temperature": 0.0, "avg_logprob": -0.10248149525035512, "compression_ratio": 1.4508928571428572, "no_speech_prob": 0.00019395175331737846}, {"id": 337, "seek": 106738, "start": 1086.38, "end": 1090.38, "text": " So here each row stands for one book.", "tokens": [407, 510, 1184, 5386, 7382, 337, 472, 1446, 13], "temperature": 0.0, "avg_logprob": -0.10248149525035512, "compression_ratio": 1.4508928571428572, "no_speech_prob": 0.00019395175331737846}, {"id": 338, "seek": 106738, "start": 1090.38, "end": 1094.38, "text": " They're named by the author's last name and then the beginning", "tokens": [814, 434, 4926, 538, 264, 3793, 311, 1036, 1315, 293, 550, 264, 2863], "temperature": 0.0, "avg_logprob": -0.10248149525035512, "compression_ratio": 1.4508928571428572, "no_speech_prob": 0.00019395175331737846}, {"id": 339, "seek": 106738, "start": 1094.38, "end": 1095.38, "text": " of the book title.", "tokens": [295, 264, 1446, 4876, 13], "temperature": 0.0, "avg_logprob": -0.10248149525035512, "compression_ratio": 1.4508928571428572, "no_speech_prob": 0.00019395175331737846}, {"id": 340, "seek": 109538, "start": 1095.38, "end": 1099.38, "text": " This is Jane Austen's Pride and Prejudice.", "tokens": [639, 307, 13048, 4126, 268, 311, 30319, 293, 6001, 9218, 573, 13], "temperature": 0.0, "avg_logprob": -0.12223551994146303, "compression_ratio": 1.4230769230769231, "no_speech_prob": 1.618563874217216e-05}, {"id": 341, "seek": 109538, "start": 1099.38, "end": 1100.38, "text": " And yeah, showing.", "tokens": [400, 1338, 11, 4099, 13], "temperature": 0.0, "avg_logprob": -0.12223551994146303, "compression_ratio": 1.4230769230769231, "no_speech_prob": 1.618563874217216e-05}, {"id": 342, "seek": 109538, "start": 1100.38, "end": 1104.38, "text": " And here they've been normalized using something called TFIDF.", "tokens": [400, 510, 436, 600, 668, 48704, 1228, 746, 1219, 40964, 2777, 37, 13], "temperature": 0.0, "avg_logprob": -0.12223551994146303, "compression_ratio": 1.4230769230769231, "no_speech_prob": 1.618563874217216e-05}, {"id": 343, "seek": 109538, "start": 1104.38, "end": 1109.38, "text": " And that I'll ask you to compute in the homework.", "tokens": [400, 300, 286, 603, 1029, 291, 281, 14722, 294, 264, 14578, 13], "temperature": 0.0, "avg_logprob": -0.12223551994146303, "compression_ratio": 1.4230769230769231, "no_speech_prob": 1.618563874217216e-05}, {"id": 344, "seek": 109538, "start": 1109.38, "end": 1112.38, "text": " So kind of a way for representing the book is this one", "tokens": [407, 733, 295, 257, 636, 337, 13460, 264, 1446, 307, 341, 472], "temperature": 0.0, "avg_logprob": -0.12223551994146303, "compression_ratio": 1.4230769230769231, "no_speech_prob": 1.618563874217216e-05}, {"id": 345, "seek": 109538, "start": 1112.38, "end": 1113.38, "text": " matrix.", "tokens": [8141, 13], "temperature": 0.0, "avg_logprob": -0.12223551994146303, "compression_ratio": 1.4230769230769231, "no_speech_prob": 1.618563874217216e-05}, {"id": 346, "seek": 109538, "start": 1113.38, "end": 1119.38, "text": " Then we can use SVD to decompose this into three matrices.", "tokens": [1396, 321, 393, 764, 31910, 35, 281, 22867, 541, 341, 666, 1045, 32284, 13], "temperature": 0.0, "avg_logprob": -0.12223551994146303, "compression_ratio": 1.4230769230769231, "no_speech_prob": 1.618563874217216e-05}, {"id": 347, "seek": 111938, "start": 1119.38, "end": 1126.38, "text": " And with SVD, we would actually get, I guess, 64 singular values.", "tokens": [400, 365, 31910, 35, 11, 321, 576, 767, 483, 11, 286, 2041, 11, 12145, 20010, 4190, 13], "temperature": 0.0, "avg_logprob": -0.08330482702988845, "compression_ratio": 1.58203125, "no_speech_prob": 6.603744986932725e-05}, {"id": 348, "seek": 111938, "start": 1126.38, "end": 1129.38, "text": " S would be a 64 by 64 matrix.", "tokens": [318, 576, 312, 257, 12145, 538, 12145, 8141, 13], "temperature": 0.0, "avg_logprob": -0.08330482702988845, "compression_ratio": 1.58203125, "no_speech_prob": 6.603744986932725e-05}, {"id": 349, "seek": 111938, "start": 1129.38, "end": 1133.38, "text": " But I've just chosen the top 10.", "tokens": [583, 286, 600, 445, 8614, 264, 1192, 1266, 13], "temperature": 0.0, "avg_logprob": -0.08330482702988845, "compression_ratio": 1.58203125, "no_speech_prob": 6.603744986932725e-05}, {"id": 350, "seek": 111938, "start": 1133.38, "end": 1137.38, "text": " The nice thing about having these singular values in descending", "tokens": [440, 1481, 551, 466, 1419, 613, 20010, 4190, 294, 40182], "temperature": 0.0, "avg_logprob": -0.08330482702988845, "compression_ratio": 1.58203125, "no_speech_prob": 6.603744986932725e-05}, {"id": 351, "seek": 111938, "start": 1137.38, "end": 1139.38, "text": " order is that kind of we're getting the most important ones", "tokens": [1668, 307, 300, 733, 295, 321, 434, 1242, 264, 881, 1021, 2306], "temperature": 0.0, "avg_logprob": -0.08330482702988845, "compression_ratio": 1.58203125, "no_speech_prob": 6.603744986932725e-05}, {"id": 352, "seek": 111938, "start": 1139.38, "end": 1142.38, "text": " whenever we pick off the top part.", "tokens": [5699, 321, 1888, 766, 264, 1192, 644, 13], "temperature": 0.0, "avg_logprob": -0.08330482702988845, "compression_ratio": 1.58203125, "no_speech_prob": 6.603744986932725e-05}, {"id": 353, "seek": 111938, "start": 1142.38, "end": 1145.38, "text": " And because of our application, we can think of these as", "tokens": [400, 570, 295, 527, 3861, 11, 321, 393, 519, 295, 613, 382], "temperature": 0.0, "avg_logprob": -0.08330482702988845, "compression_ratio": 1.58203125, "no_speech_prob": 6.603744986932725e-05}, {"id": 354, "seek": 111938, "start": 1145.38, "end": 1148.38, "text": " corresponding to various topics and how important each topic", "tokens": [11760, 281, 3683, 8378, 293, 577, 1021, 1184, 4829], "temperature": 0.0, "avg_logprob": -0.08330482702988845, "compression_ratio": 1.58203125, "no_speech_prob": 6.603744986932725e-05}, {"id": 355, "seek": 114838, "start": 1148.38, "end": 1149.38, "text": " is.", "tokens": [307, 13], "temperature": 0.0, "avg_logprob": -0.11369114611522262, "compression_ratio": 1.6067415730337078, "no_speech_prob": 2.078457509924192e-05}, {"id": 356, "seek": 114838, "start": 1149.38, "end": 1156.38, "text": " And the matrix on the left, U, is the various books by topics", "tokens": [400, 264, 8141, 322, 264, 1411, 11, 624, 11, 307, 264, 3683, 3642, 538, 8378], "temperature": 0.0, "avg_logprob": -0.11369114611522262, "compression_ratio": 1.6067415730337078, "no_speech_prob": 2.078457509924192e-05}, {"id": 357, "seek": 114838, "start": 1156.38, "end": 1161.38, "text": " and then the matrix on the right, V, is giving us the topics by", "tokens": [293, 550, 264, 8141, 322, 264, 558, 11, 691, 11, 307, 2902, 505, 264, 8378, 538], "temperature": 0.0, "avg_logprob": -0.11369114611522262, "compression_ratio": 1.6067415730337078, "no_speech_prob": 2.078457509924192e-05}, {"id": 358, "seek": 114838, "start": 1161.38, "end": 1162.38, "text": " words.", "tokens": [2283, 13], "temperature": 0.0, "avg_logprob": -0.11369114611522262, "compression_ratio": 1.6067415730337078, "no_speech_prob": 2.078457509924192e-05}, {"id": 359, "seek": 114838, "start": 1162.38, "end": 1166.38, "text": " And we can see something.", "tokens": [400, 321, 393, 536, 746, 13], "temperature": 0.0, "avg_logprob": -0.11369114611522262, "compression_ratio": 1.6067415730337078, "no_speech_prob": 2.078457509924192e-05}, {"id": 360, "seek": 114838, "start": 1166.38, "end": 1169.38, "text": " Let's look at maybe Catherine.", "tokens": [961, 311, 574, 412, 1310, 23098, 13], "temperature": 0.0, "avg_logprob": -0.11369114611522262, "compression_ratio": 1.6067415730337078, "no_speech_prob": 2.078457509924192e-05}, {"id": 361, "seek": 114838, "start": 1169.38, "end": 1171.38, "text": " Is Catherine large anywhere?", "tokens": [1119, 23098, 2416, 4992, 30], "temperature": 0.0, "avg_logprob": -0.11369114611522262, "compression_ratio": 1.6067415730337078, "no_speech_prob": 2.078457509924192e-05}, {"id": 362, "seek": 114838, "start": 1171.38, "end": 1177.38, "text": " Topic six looks like where Catherine has the biggest magnitude.", "tokens": [8840, 299, 2309, 1542, 411, 689, 23098, 575, 264, 3880, 15668, 13], "temperature": 0.0, "avg_logprob": -0.11369114611522262, "compression_ratio": 1.6067415730337078, "no_speech_prob": 2.078457509924192e-05}, {"id": 363, "seek": 117738, "start": 1177.38, "end": 1181.38, "text": " If we go back over to topic six.", "tokens": [759, 321, 352, 646, 670, 281, 4829, 2309, 13], "temperature": 0.0, "avg_logprob": -0.13315418633547696, "compression_ratio": 1.4020100502512562, "no_speech_prob": 1.8057378838420846e-05}, {"id": 364, "seek": 117738, "start": 1181.38, "end": 1186.38, "text": " So not the most important, but still up there.", "tokens": [407, 406, 264, 881, 1021, 11, 457, 920, 493, 456, 13], "temperature": 0.0, "avg_logprob": -0.13315418633547696, "compression_ratio": 1.4020100502512562, "no_speech_prob": 1.8057378838420846e-05}, {"id": 365, "seek": 117738, "start": 1186.38, "end": 1192.38, "text": " And we can see what books have a lot of topics six.", "tokens": [400, 321, 393, 536, 437, 3642, 362, 257, 688, 295, 8378, 2309, 13], "temperature": 0.0, "avg_logprob": -0.13315418633547696, "compression_ratio": 1.4020100502512562, "no_speech_prob": 1.8057378838420846e-05}, {"id": 366, "seek": 117738, "start": 1192.38, "end": 1197.38, "text": " This Life and Times of Tristram Shandy.", "tokens": [639, 7720, 293, 11366, 295, 1765, 468, 2356, 1160, 11425, 13], "temperature": 0.0, "avg_logprob": -0.13315418633547696, "compression_ratio": 1.4020100502512562, "no_speech_prob": 1.8057378838420846e-05}, {"id": 367, "seek": 117738, "start": 1197.38, "end": 1203.38, "text": " Here, this is pretty large, 0.5665 for Emily Bronte's", "tokens": [1692, 11, 341, 307, 1238, 2416, 11, 1958, 13, 20, 15237, 20, 337, 15034, 1603, 10219, 311], "temperature": 0.0, "avg_logprob": -0.13315418633547696, "compression_ratio": 1.4020100502512562, "no_speech_prob": 1.8057378838420846e-05}, {"id": 368, "seek": 117738, "start": 1203.38, "end": 1204.38, "text": " Weathering Heights.", "tokens": [34441, 278, 44039, 13], "temperature": 0.0, "avg_logprob": -0.13315418633547696, "compression_ratio": 1.4020100502512562, "no_speech_prob": 1.8057378838420846e-05}, {"id": 369, "seek": 117738, "start": 1204.38, "end": 1206.38, "text": " Catherine's a character in there.", "tokens": [23098, 311, 257, 2517, 294, 456, 13], "temperature": 0.0, "avg_logprob": -0.13315418633547696, "compression_ratio": 1.4020100502512562, "no_speech_prob": 1.8057378838420846e-05}, {"id": 370, "seek": 120638, "start": 1206.38, "end": 1208.38, "text": " So the word is showing up more.", "tokens": [407, 264, 1349, 307, 4099, 493, 544, 13], "temperature": 0.0, "avg_logprob": -0.101416853758005, "compression_ratio": 1.6, "no_speech_prob": 0.00011957308015553281}, {"id": 371, "seek": 120638, "start": 1208.38, "end": 1211.38, "text": " So this is kind of just to give you a sense of how you can", "tokens": [407, 341, 307, 733, 295, 445, 281, 976, 291, 257, 2020, 295, 577, 291, 393], "temperature": 0.0, "avg_logprob": -0.101416853758005, "compression_ratio": 1.6, "no_speech_prob": 0.00011957308015553281}, {"id": 372, "seek": 120638, "start": 1211.38, "end": 1213.38, "text": " interpret these matrices.", "tokens": [7302, 613, 32284, 13], "temperature": 0.0, "avg_logprob": -0.101416853758005, "compression_ratio": 1.6, "no_speech_prob": 0.00011957308015553281}, {"id": 373, "seek": 120638, "start": 1213.38, "end": 1216.38, "text": " First, I'll ask, are there any questions just about the Excel", "tokens": [2386, 11, 286, 603, 1029, 11, 366, 456, 604, 1651, 445, 466, 264, 19060], "temperature": 0.0, "avg_logprob": -0.101416853758005, "compression_ratio": 1.6, "no_speech_prob": 0.00011957308015553281}, {"id": 374, "seek": 120638, "start": 1216.38, "end": 1219.38, "text": " spreadsheet?", "tokens": [27733, 30], "temperature": 0.0, "avg_logprob": -0.101416853758005, "compression_ratio": 1.6, "no_speech_prob": 0.00011957308015553281}, {"id": 375, "seek": 120638, "start": 1219.38, "end": 1220.38, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.101416853758005, "compression_ratio": 1.6, "no_speech_prob": 0.00011957308015553281}, {"id": 376, "seek": 120638, "start": 1220.38, "end": 1225.38, "text": " And so then in the notebook, per your request, I've included", "tokens": [400, 370, 550, 294, 264, 21060, 11, 680, 428, 5308, 11, 286, 600, 5556], "temperature": 0.0, "avg_logprob": -0.101416853758005, "compression_ratio": 1.6, "no_speech_prob": 0.00011957308015553281}, {"id": 377, "seek": 120638, "start": 1225.38, "end": 1227.38, "text": " the code that I used to create this.", "tokens": [264, 3089, 300, 286, 1143, 281, 1884, 341, 13], "temperature": 0.0, "avg_logprob": -0.101416853758005, "compression_ratio": 1.6, "no_speech_prob": 0.00011957308015553281}, {"id": 378, "seek": 120638, "start": 1227.38, "end": 1232.38, "text": " So I used Python to actually process the data and create", "tokens": [407, 286, 1143, 15329, 281, 767, 1399, 264, 1412, 293, 1884], "temperature": 0.0, "avg_logprob": -0.101416853758005, "compression_ratio": 1.6, "no_speech_prob": 0.00011957308015553281}, {"id": 379, "seek": 120638, "start": 1232.38, "end": 1233.38, "text": " these matrices.", "tokens": [613, 32284, 13], "temperature": 0.0, "avg_logprob": -0.101416853758005, "compression_ratio": 1.6, "no_speech_prob": 0.00011957308015553281}, {"id": 380, "seek": 123338, "start": 1233.38, "end": 1236.38, "text": " And you can go through this on your own.", "tokens": [400, 291, 393, 352, 807, 341, 322, 428, 1065, 13], "temperature": 0.0, "avg_logprob": -0.0932888150215149, "compression_ratio": 1.394736842105263, "no_speech_prob": 7.030712004052475e-05}, {"id": 381, "seek": 123338, "start": 1236.38, "end": 1239.38, "text": " It should be pretty straightforward.", "tokens": [467, 820, 312, 1238, 15325, 13], "temperature": 0.0, "avg_logprob": -0.0932888150215149, "compression_ratio": 1.394736842105263, "no_speech_prob": 7.030712004052475e-05}, {"id": 382, "seek": 123338, "start": 1239.38, "end": 1244.38, "text": " I exported them to CSVs, and then that's what I was looking at", "tokens": [286, 42055, 552, 281, 48814, 82, 11, 293, 550, 300, 311, 437, 286, 390, 1237, 412], "temperature": 0.0, "avg_logprob": -0.0932888150215149, "compression_ratio": 1.394736842105263, "no_speech_prob": 7.030712004052475e-05}, {"id": 383, "seek": 123338, "start": 1244.38, "end": 1245.38, "text": " in Excel.", "tokens": [294, 19060, 13], "temperature": 0.0, "avg_logprob": -0.0932888150215149, "compression_ratio": 1.394736842105263, "no_speech_prob": 7.030712004052475e-05}, {"id": 384, "seek": 123338, "start": 1245.38, "end": 1249.38, "text": " But if you want to see where those numbers came from, this is", "tokens": [583, 498, 291, 528, 281, 536, 689, 729, 3547, 1361, 490, 11, 341, 307], "temperature": 0.0, "avg_logprob": -0.0932888150215149, "compression_ratio": 1.394736842105263, "no_speech_prob": 7.030712004052475e-05}, {"id": 385, "seek": 123338, "start": 1249.38, "end": 1262.38, "text": " the code to get you there, both for NMF and for SVD.", "tokens": [264, 3089, 281, 483, 291, 456, 11, 1293, 337, 426, 44, 37, 293, 337, 31910, 35, 13], "temperature": 0.0, "avg_logprob": -0.0932888150215149, "compression_ratio": 1.394736842105263, "no_speech_prob": 7.030712004052475e-05}, {"id": 386, "seek": 126238, "start": 1262.38, "end": 1264.38, "text": " Actually, first I should, yes?", "tokens": [5135, 11, 700, 286, 820, 11, 2086, 30], "temperature": 0.0, "avg_logprob": -0.11488020009007947, "compression_ratio": 1.8909774436090225, "no_speech_prob": 4.3994739826302975e-05}, {"id": 387, "seek": 126238, "start": 1264.38, "end": 1267.38, "text": " So for the topics, the best way to understand them is just to", "tokens": [407, 337, 264, 8378, 11, 264, 1151, 636, 281, 1223, 552, 307, 445, 281], "temperature": 0.0, "avg_logprob": -0.11488020009007947, "compression_ratio": 1.8909774436090225, "no_speech_prob": 4.3994739826302975e-05}, {"id": 388, "seek": 126238, "start": 1267.38, "end": 1270.38, "text": " look at the top vocabulary for topics?", "tokens": [574, 412, 264, 1192, 19864, 337, 8378, 30], "temperature": 0.0, "avg_logprob": -0.11488020009007947, "compression_ratio": 1.8909774436090225, "no_speech_prob": 4.3994739826302975e-05}, {"id": 389, "seek": 126238, "start": 1270.38, "end": 1271.38, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.11488020009007947, "compression_ratio": 1.8909774436090225, "no_speech_prob": 4.3994739826302975e-05}, {"id": 390, "seek": 126238, "start": 1271.38, "end": 1272.38, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.11488020009007947, "compression_ratio": 1.8909774436090225, "no_speech_prob": 4.3994739826302975e-05}, {"id": 391, "seek": 126238, "start": 1272.38, "end": 1274.38, "text": " So the question was, for the topics, the best way to", "tokens": [407, 264, 1168, 390, 11, 337, 264, 8378, 11, 264, 1151, 636, 281], "temperature": 0.0, "avg_logprob": -0.11488020009007947, "compression_ratio": 1.8909774436090225, "no_speech_prob": 4.3994739826302975e-05}, {"id": 392, "seek": 126238, "start": 1274.38, "end": 1276.38, "text": " understand them is by looking at the vocabulary words.", "tokens": [1223, 552, 307, 538, 1237, 412, 264, 19864, 2283, 13], "temperature": 0.0, "avg_logprob": -0.11488020009007947, "compression_ratio": 1.8909774436090225, "no_speech_prob": 4.3994739826302975e-05}, {"id": 393, "seek": 126238, "start": 1276.38, "end": 1279.38, "text": " And I would say yes, because that's really kind of all you", "tokens": [400, 286, 576, 584, 2086, 11, 570, 300, 311, 534, 733, 295, 439, 291], "temperature": 0.0, "avg_logprob": -0.11488020009007947, "compression_ratio": 1.8909774436090225, "no_speech_prob": 4.3994739826302975e-05}, {"id": 394, "seek": 126238, "start": 1279.38, "end": 1281.38, "text": " have to go on.", "tokens": [362, 281, 352, 322, 13], "temperature": 0.0, "avg_logprob": -0.11488020009007947, "compression_ratio": 1.8909774436090225, "no_speech_prob": 4.3994739826302975e-05}, {"id": 395, "seek": 126238, "start": 1281.38, "end": 1284.38, "text": " I mean, I guess potentially the other thing you could do would", "tokens": [286, 914, 11, 286, 2041, 7263, 264, 661, 551, 291, 727, 360, 576], "temperature": 0.0, "avg_logprob": -0.11488020009007947, "compression_ratio": 1.8909774436090225, "no_speech_prob": 4.3994739826302975e-05}, {"id": 396, "seek": 126238, "start": 1284.38, "end": 1287.38, "text": " be to look at different books and say, okay, this book has a", "tokens": [312, 281, 574, 412, 819, 3642, 293, 584, 11, 1392, 11, 341, 1446, 575, 257], "temperature": 0.0, "avg_logprob": -0.11488020009007947, "compression_ratio": 1.8909774436090225, "no_speech_prob": 4.3994739826302975e-05}, {"id": 397, "seek": 126238, "start": 1287.38, "end": 1288.38, "text": " lot of this topic.", "tokens": [688, 295, 341, 4829, 13], "temperature": 0.0, "avg_logprob": -0.11488020009007947, "compression_ratio": 1.8909774436090225, "no_speech_prob": 4.3994739826302975e-05}, {"id": 398, "seek": 126238, "start": 1288.38, "end": 1291.38, "text": " Is it similar to these other books?", "tokens": [1119, 309, 2531, 281, 613, 661, 3642, 30], "temperature": 0.0, "avg_logprob": -0.11488020009007947, "compression_ratio": 1.8909774436090225, "no_speech_prob": 4.3994739826302975e-05}, {"id": 399, "seek": 129138, "start": 1291.38, "end": 1295.38, "text": " Actually, let's look at that.", "tokens": [5135, 11, 718, 311, 574, 412, 300, 13], "temperature": 0.0, "avg_logprob": -0.11146341902869088, "compression_ratio": 1.59915611814346, "no_speech_prob": 2.6687421268434264e-05}, {"id": 400, "seek": 129138, "start": 1295.38, "end": 1300.38, "text": " Let's see if this, yeah, in this case, I think that's a little", "tokens": [961, 311, 536, 498, 341, 11, 1338, 11, 294, 341, 1389, 11, 286, 519, 300, 311, 257, 707], "temperature": 0.0, "avg_logprob": -0.11146341902869088, "compression_ratio": 1.59915611814346, "no_speech_prob": 2.6687421268434264e-05}, {"id": 401, "seek": 129138, "start": 1300.38, "end": 1302.38, "text": " bit trickier to do because it looks like there are a lot of", "tokens": [857, 4282, 811, 281, 360, 570, 309, 1542, 411, 456, 366, 257, 688, 295], "temperature": 0.0, "avg_logprob": -0.11146341902869088, "compression_ratio": 1.59915611814346, "no_speech_prob": 2.6687421268434264e-05}, {"id": 402, "seek": 129138, "start": 1302.38, "end": 1305.38, "text": " these that have around 0.2 of topic one.", "tokens": [613, 300, 362, 926, 1958, 13, 17, 295, 4829, 472, 13], "temperature": 0.0, "avg_logprob": -0.11146341902869088, "compression_ratio": 1.59915611814346, "no_speech_prob": 2.6687421268434264e-05}, {"id": 403, "seek": 129138, "start": 1305.38, "end": 1309.38, "text": " So to me, this seems less informative.", "tokens": [407, 281, 385, 11, 341, 2544, 1570, 27759, 13], "temperature": 0.0, "avg_logprob": -0.11146341902869088, "compression_ratio": 1.59915611814346, "no_speech_prob": 2.6687421268434264e-05}, {"id": 404, "seek": 129138, "start": 1309.38, "end": 1313.38, "text": " I don't know if any of the later ones, you might get something", "tokens": [286, 500, 380, 458, 498, 604, 295, 264, 1780, 2306, 11, 291, 1062, 483, 746], "temperature": 0.0, "avg_logprob": -0.11146341902869088, "compression_ratio": 1.59915611814346, "no_speech_prob": 2.6687421268434264e-05}, {"id": 405, "seek": 129138, "start": 1313.38, "end": 1315.38, "text": " a little more indicative.", "tokens": [257, 707, 544, 47513, 13], "temperature": 0.0, "avg_logprob": -0.11146341902869088, "compression_ratio": 1.59915611814346, "no_speech_prob": 2.6687421268434264e-05}, {"id": 406, "seek": 129138, "start": 1315.38, "end": 1320.38, "text": " Okay, here, sentence sensibility has a lot of topic four.", "tokens": [1033, 11, 510, 11, 8174, 2923, 2841, 575, 257, 688, 295, 4829, 1451, 13], "temperature": 0.0, "avg_logprob": -0.11146341902869088, "compression_ratio": 1.59915611814346, "no_speech_prob": 2.6687421268434264e-05}, {"id": 407, "seek": 132038, "start": 1320.38, "end": 1328.38, "text": " Let's see if any others do.", "tokens": [961, 311, 536, 498, 604, 2357, 360, 13], "temperature": 0.0, "avg_logprob": -0.11089578041663536, "compression_ratio": 1.6294642857142858, "no_speech_prob": 8.267657904070802e-06}, {"id": 408, "seek": 132038, "start": 1328.38, "end": 1330.38, "text": " I guess Emma also.", "tokens": [286, 2041, 17124, 611, 13], "temperature": 0.0, "avg_logprob": -0.11089578041663536, "compression_ratio": 1.6294642857142858, "no_speech_prob": 8.267657904070802e-06}, {"id": 409, "seek": 132038, "start": 1330.38, "end": 1334.38, "text": " So Emma and sentence sensibility are biggest on topic four.", "tokens": [407, 17124, 293, 8174, 2923, 2841, 366, 3880, 322, 4829, 1451, 13], "temperature": 0.0, "avg_logprob": -0.11089578041663536, "compression_ratio": 1.6294642857142858, "no_speech_prob": 8.267657904070802e-06}, {"id": 410, "seek": 132038, "start": 1334.38, "end": 1337.38, "text": " So that could raise you to ask, like, okay, what do Emma and", "tokens": [407, 300, 727, 5300, 291, 281, 1029, 11, 411, 11, 1392, 11, 437, 360, 17124, 293], "temperature": 0.0, "avg_logprob": -0.11089578041663536, "compression_ratio": 1.6294642857142858, "no_speech_prob": 8.267657904070802e-06}, {"id": 411, "seek": 132038, "start": 1337.38, "end": 1340.38, "text": " sentence sensibility have in common that these other books", "tokens": [8174, 2923, 2841, 362, 294, 2689, 300, 613, 661, 3642], "temperature": 0.0, "avg_logprob": -0.11089578041663536, "compression_ratio": 1.6294642857142858, "no_speech_prob": 8.267657904070802e-06}, {"id": 412, "seek": 132038, "start": 1340.38, "end": 1341.38, "text": " don't?", "tokens": [500, 380, 30], "temperature": 0.0, "avg_logprob": -0.11089578041663536, "compression_ratio": 1.6294642857142858, "no_speech_prob": 8.267657904070802e-06}, {"id": 413, "seek": 132038, "start": 1341.38, "end": 1343.38, "text": " Although even then, I think I'd probably still want to kind of", "tokens": [5780, 754, 550, 11, 286, 519, 286, 1116, 1391, 920, 528, 281, 733, 295], "temperature": 0.0, "avg_logprob": -0.11089578041663536, "compression_ratio": 1.6294642857142858, "no_speech_prob": 8.267657904070802e-06}, {"id": 414, "seek": 132038, "start": 1343.38, "end": 1346.38, "text": " confirm that bit by saying, like, okay, what words do those", "tokens": [9064, 300, 857, 538, 1566, 11, 411, 11, 1392, 11, 437, 2283, 360, 729], "temperature": 0.0, "avg_logprob": -0.11089578041663536, "compression_ratio": 1.6294642857142858, "no_speech_prob": 8.267657904070802e-06}, {"id": 415, "seek": 132038, "start": 1346.38, "end": 1348.38, "text": " involve?", "tokens": [9494, 30], "temperature": 0.0, "avg_logprob": -0.11089578041663536, "compression_ratio": 1.6294642857142858, "no_speech_prob": 8.267657904070802e-06}, {"id": 416, "seek": 134838, "start": 1348.38, "end": 1351.38, "text": " In this case, let's see.", "tokens": [682, 341, 1389, 11, 718, 311, 536, 13], "temperature": 0.0, "avg_logprob": -0.13714374666628631, "compression_ratio": 1.532258064516129, "no_speech_prob": 1.9831750250887126e-05}, {"id": 417, "seek": 134838, "start": 1351.38, "end": 1353.38, "text": " Eleanor.", "tokens": [8024, 29330, 13], "temperature": 0.0, "avg_logprob": -0.13714374666628631, "compression_ratio": 1.532258064516129, "no_speech_prob": 1.9831750250887126e-05}, {"id": 418, "seek": 134838, "start": 1353.38, "end": 1358.38, "text": " So I would guess that Eleanor is a character in both, although I", "tokens": [407, 286, 576, 2041, 300, 8024, 29330, 307, 257, 2517, 294, 1293, 11, 4878, 286], "temperature": 0.0, "avg_logprob": -0.13714374666628631, "compression_ratio": 1.532258064516129, "no_speech_prob": 1.9831750250887126e-05}, {"id": 419, "seek": 134838, "start": 1358.38, "end": 1363.38, "text": " would have to have to look that up.", "tokens": [576, 362, 281, 362, 281, 574, 300, 493, 13], "temperature": 0.0, "avg_logprob": -0.13714374666628631, "compression_ratio": 1.532258064516129, "no_speech_prob": 1.9831750250887126e-05}, {"id": 420, "seek": 134838, "start": 1363.38, "end": 1366.38, "text": " Yeah, that's kind of what's jumping out most.", "tokens": [865, 11, 300, 311, 733, 295, 437, 311, 11233, 484, 881, 13], "temperature": 0.0, "avg_logprob": -0.13714374666628631, "compression_ratio": 1.532258064516129, "no_speech_prob": 1.9831750250887126e-05}, {"id": 421, "seek": 134838, "start": 1366.38, "end": 1369.38, "text": " Also Marion.", "tokens": [2743, 49270, 13], "temperature": 0.0, "avg_logprob": -0.13714374666628631, "compression_ratio": 1.532258064516129, "no_speech_prob": 1.9831750250887126e-05}, {"id": 422, "seek": 134838, "start": 1369.38, "end": 1375.38, "text": " Yeah, and in some ways, this is a bit primitive.", "tokens": [865, 11, 293, 294, 512, 2098, 11, 341, 307, 257, 857, 28540, 13], "temperature": 0.0, "avg_logprob": -0.13714374666628631, "compression_ratio": 1.532258064516129, "no_speech_prob": 1.9831750250887126e-05}, {"id": 423, "seek": 134838, "start": 1375.38, "end": 1377.38, "text": " You know, this is a bag of words approach.", "tokens": [509, 458, 11, 341, 307, 257, 3411, 295, 2283, 3109, 13], "temperature": 0.0, "avg_logprob": -0.13714374666628631, "compression_ratio": 1.532258064516129, "no_speech_prob": 1.9831750250887126e-05}, {"id": 424, "seek": 137738, "start": 1377.38, "end": 1380.38, "text": " We're kind of representing these books just as bags of words.", "tokens": [492, 434, 733, 295, 13460, 613, 3642, 445, 382, 10405, 295, 2283, 13], "temperature": 0.0, "avg_logprob": -0.11700209085043374, "compression_ratio": 1.5524861878453038, "no_speech_prob": 8.139380952343345e-06}, {"id": 425, "seek": 137738, "start": 1380.38, "end": 1385.38, "text": " So it's not, you know, we're not going to get anything about,", "tokens": [407, 309, 311, 406, 11, 291, 458, 11, 321, 434, 406, 516, 281, 483, 1340, 466, 11], "temperature": 0.0, "avg_logprob": -0.11700209085043374, "compression_ratio": 1.5524861878453038, "no_speech_prob": 8.139380952343345e-06}, {"id": 426, "seek": 137738, "start": 1385.38, "end": 1388.38, "text": " like, the structure of the plot making these books similar.", "tokens": [411, 11, 264, 3877, 295, 264, 7542, 1455, 613, 3642, 2531, 13], "temperature": 0.0, "avg_logprob": -0.11700209085043374, "compression_ratio": 1.5524861878453038, "no_speech_prob": 8.139380952343345e-06}, {"id": 427, "seek": 137738, "start": 1388.38, "end": 1397.38, "text": " It's really just looking at vocabulary words.", "tokens": [467, 311, 534, 445, 1237, 412, 19864, 2283, 13], "temperature": 0.0, "avg_logprob": -0.11700209085043374, "compression_ratio": 1.5524861878453038, "no_speech_prob": 8.139380952343345e-06}, {"id": 428, "seek": 137738, "start": 1397.38, "end": 1402.38, "text": " All right, then, and actually, let me just ask you.", "tokens": [1057, 558, 11, 550, 11, 293, 767, 11, 718, 385, 445, 1029, 291, 13], "temperature": 0.0, "avg_logprob": -0.11700209085043374, "compression_ratio": 1.5524861878453038, "no_speech_prob": 8.139380952343345e-06}, {"id": 429, "seek": 140238, "start": 1402.38, "end": 1409.38, "text": " So what did we see last time with, I don't know, advantages,", "tokens": [407, 437, 630, 321, 536, 1036, 565, 365, 11, 286, 500, 380, 458, 11, 14906, 11], "temperature": 0.0, "avg_logprob": -0.13663267135620116, "compression_ratio": 1.5876288659793814, "no_speech_prob": 4.757305941893719e-05}, {"id": 430, "seek": 140238, "start": 1409.38, "end": 1417.38, "text": " disadvantages of SVD versus NMF?", "tokens": [37431, 295, 31910, 35, 5717, 426, 44, 37, 30], "temperature": 0.0, "avg_logprob": -0.13663267135620116, "compression_ratio": 1.5876288659793814, "no_speech_prob": 4.757305941893719e-05}, {"id": 431, "seek": 140238, "start": 1417.38, "end": 1418.38, "text": " Yes?", "tokens": [1079, 30], "temperature": 0.0, "avg_logprob": -0.13663267135620116, "compression_ratio": 1.5876288659793814, "no_speech_prob": 4.757305941893719e-05}, {"id": 432, "seek": 140238, "start": 1418.38, "end": 1422.38, "text": " One advantage with NMF is you get it because you get non-negative", "tokens": [1485, 5002, 365, 426, 44, 37, 307, 291, 483, 309, 570, 291, 483, 2107, 12, 28561, 1166], "temperature": 0.0, "avg_logprob": -0.13663267135620116, "compression_ratio": 1.5876288659793814, "no_speech_prob": 4.757305941893719e-05}, {"id": 433, "seek": 140238, "start": 1422.38, "end": 1424.38, "text": " values, it's a little bit more interpretable.", "tokens": [4190, 11, 309, 311, 257, 707, 857, 544, 7302, 712, 13], "temperature": 0.0, "avg_logprob": -0.13663267135620116, "compression_ratio": 1.5876288659793814, "no_speech_prob": 4.757305941893719e-05}, {"id": 434, "seek": 140238, "start": 1424.38, "end": 1425.38, "text": " Okay, yeah.", "tokens": [1033, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.13663267135620116, "compression_ratio": 1.5876288659793814, "no_speech_prob": 4.757305941893719e-05}, {"id": 435, "seek": 140238, "start": 1425.38, "end": 1429.38, "text": " So to repeat that, NMF is their non-negative values.", "tokens": [407, 281, 7149, 300, 11, 426, 44, 37, 307, 641, 2107, 12, 28561, 1166, 4190, 13], "temperature": 0.0, "avg_logprob": -0.13663267135620116, "compression_ratio": 1.5876288659793814, "no_speech_prob": 4.757305941893719e-05}, {"id": 436, "seek": 140238, "start": 1429.38, "end": 1431.38, "text": " Those can be more interpretable.", "tokens": [3950, 393, 312, 544, 7302, 712, 13], "temperature": 0.0, "avg_logprob": -0.13663267135620116, "compression_ratio": 1.5876288659793814, "no_speech_prob": 4.757305941893719e-05}, {"id": 437, "seek": 143138, "start": 1431.38, "end": 1434.38, "text": " Because here, you know, with SVD, we are seeing, like, you could", "tokens": [1436, 510, 11, 291, 458, 11, 365, 31910, 35, 11, 321, 366, 2577, 11, 411, 11, 291, 727], "temperature": 0.0, "avg_logprob": -0.18058296044667563, "compression_ratio": 1.5845410628019323, "no_speech_prob": 6.708194996463135e-05}, {"id": 438, "seek": 143138, "start": 1434.38, "end": 1436.38, "text": " have negative values for the topic.", "tokens": [362, 3671, 4190, 337, 264, 4829, 13], "temperature": 0.0, "avg_logprob": -0.18058296044667563, "compression_ratio": 1.5845410628019323, "no_speech_prob": 6.708194996463135e-05}, {"id": 439, "seek": 143138, "start": 1436.38, "end": 1442.38, "text": " And so what does it mean for this to, yeah, be negative above its topic?", "tokens": [400, 370, 437, 775, 309, 914, 337, 341, 281, 11, 1338, 11, 312, 3671, 3673, 1080, 4829, 30], "temperature": 0.0, "avg_logprob": -0.18058296044667563, "compression_ratio": 1.5845410628019323, "no_speech_prob": 6.708194996463135e-05}, {"id": 440, "seek": 143138, "start": 1442.38, "end": 1443.38, "text": " Yes?", "tokens": [1079, 30], "temperature": 0.0, "avg_logprob": -0.18058296044667563, "compression_ratio": 1.5845410628019323, "no_speech_prob": 6.708194996463135e-05}, {"id": 441, "seek": 143138, "start": 1443.38, "end": 1448.38, "text": " SVD was an exact decomposition.", "tokens": [31910, 35, 390, 364, 1900, 48356, 13], "temperature": 0.0, "avg_logprob": -0.18058296044667563, "compression_ratio": 1.5845410628019323, "no_speech_prob": 6.708194996463135e-05}, {"id": 442, "seek": 143138, "start": 1448.38, "end": 1449.38, "text": " Great point.", "tokens": [3769, 935, 13], "temperature": 0.0, "avg_logprob": -0.18058296044667563, "compression_ratio": 1.5845410628019323, "no_speech_prob": 6.708194996463135e-05}, {"id": 443, "seek": 143138, "start": 1449.38, "end": 1451.38, "text": " Yes, SVD is an exact decomposition.", "tokens": [1079, 11, 31910, 35, 307, 364, 1900, 48356, 13], "temperature": 0.0, "avg_logprob": -0.18058296044667563, "compression_ratio": 1.5845410628019323, "no_speech_prob": 6.708194996463135e-05}, {"id": 444, "seek": 143138, "start": 1451.38, "end": 1458.38, "text": " So you can fully represent your input matrices, whereas non-negative", "tokens": [407, 291, 393, 4498, 2906, 428, 4846, 32284, 11, 9735, 2107, 12, 28561, 1166], "temperature": 0.0, "avg_logprob": -0.18058296044667563, "compression_ratio": 1.5845410628019323, "no_speech_prob": 6.708194996463135e-05}, {"id": 445, "seek": 145838, "start": 1458.38, "end": 1467.38, "text": " factorization is not exact.", "tokens": [5952, 2144, 307, 406, 1900, 13], "temperature": 0.0, "avg_logprob": -0.06882013593401227, "compression_ratio": 1.3515151515151516, "no_speech_prob": 3.882621604134329e-05}, {"id": 446, "seek": 145838, "start": 1467.38, "end": 1473.38, "text": " Any others?", "tokens": [2639, 2357, 30], "temperature": 0.0, "avg_logprob": -0.06882013593401227, "compression_ratio": 1.3515151515151516, "no_speech_prob": 3.882621604134329e-05}, {"id": 447, "seek": 145838, "start": 1473.38, "end": 1478.38, "text": " So one thing about NMF, actually, I don't know if this is a positive", "tokens": [407, 472, 551, 466, 426, 44, 37, 11, 767, 11, 286, 500, 380, 458, 498, 341, 307, 257, 3353], "temperature": 0.0, "avg_logprob": -0.06882013593401227, "compression_ratio": 1.3515151515151516, "no_speech_prob": 3.882621604134329e-05}, {"id": 448, "seek": 145838, "start": 1478.38, "end": 1479.38, "text": " or a negative.", "tokens": [420, 257, 3671, 13], "temperature": 0.0, "avg_logprob": -0.06882013593401227, "compression_ratio": 1.3515151515151516, "no_speech_prob": 3.882621604134329e-05}, {"id": 449, "seek": 145838, "start": 1479.38, "end": 1483.38, "text": " You have to set the number of topics that you wanted.", "tokens": [509, 362, 281, 992, 264, 1230, 295, 8378, 300, 291, 1415, 13], "temperature": 0.0, "avg_logprob": -0.06882013593401227, "compression_ratio": 1.3515151515151516, "no_speech_prob": 3.882621604134329e-05}, {"id": 450, "seek": 145838, "start": 1483.38, "end": 1486.38, "text": " That's a hyperparameter that you're choosing.", "tokens": [663, 311, 257, 9848, 2181, 335, 2398, 300, 291, 434, 10875, 13], "temperature": 0.0, "avg_logprob": -0.06882013593401227, "compression_ratio": 1.3515151515151516, "no_speech_prob": 3.882621604134329e-05}, {"id": 451, "seek": 148638, "start": 1486.38, "end": 1492.38, "text": " With SVD, if you do kind of the full decomposition, well, I should say,", "tokens": [2022, 31910, 35, 11, 498, 291, 360, 733, 295, 264, 1577, 48356, 11, 731, 11, 286, 820, 584, 11], "temperature": 0.0, "avg_logprob": -0.06604053372534636, "compression_ratio": 1.6414342629482073, "no_speech_prob": 8.267247721960302e-06}, {"id": 452, "seek": 148638, "start": 1492.38, "end": 1497.38, "text": " if you do a traditional SVD, you're getting as many singular values as", "tokens": [498, 291, 360, 257, 5164, 31910, 35, 11, 291, 434, 1242, 382, 867, 20010, 4190, 382], "temperature": 0.0, "avg_logprob": -0.06604053372534636, "compression_ratio": 1.6414342629482073, "no_speech_prob": 8.267247721960302e-06}, {"id": 453, "seek": 148638, "start": 1497.38, "end": 1502.38, "text": " you had documents or whatever your smaller dimension is.", "tokens": [291, 632, 8512, 420, 2035, 428, 4356, 10139, 307, 13], "temperature": 0.0, "avg_logprob": -0.06604053372534636, "compression_ratio": 1.6414342629482073, "no_speech_prob": 8.267247721960302e-06}, {"id": 454, "seek": 148638, "start": 1502.38, "end": 1504.38, "text": " But I'm assuming you have fewer documents than you have vocabulary", "tokens": [583, 286, 478, 11926, 291, 362, 13366, 8512, 813, 291, 362, 19864], "temperature": 0.0, "avg_logprob": -0.06604053372534636, "compression_ratio": 1.6414342629482073, "no_speech_prob": 8.267247721960302e-06}, {"id": 455, "seek": 148638, "start": 1504.38, "end": 1505.38, "text": " words.", "tokens": [2283, 13], "temperature": 0.0, "avg_logprob": -0.06604053372534636, "compression_ratio": 1.6414342629482073, "no_speech_prob": 8.267247721960302e-06}, {"id": 456, "seek": 148638, "start": 1505.38, "end": 1509.38, "text": " So SVD, there is the opportunity to look at the singular values and see,", "tokens": [407, 31910, 35, 11, 456, 307, 264, 2650, 281, 574, 412, 264, 20010, 4190, 293, 536, 11], "temperature": 0.0, "avg_logprob": -0.06604053372534636, "compression_ratio": 1.6414342629482073, "no_speech_prob": 8.267247721960302e-06}, {"id": 457, "seek": 148638, "start": 1509.38, "end": 1513.38, "text": " like, okay, when they get really little, these topics might be so", "tokens": [411, 11, 1392, 11, 562, 436, 483, 534, 707, 11, 613, 8378, 1062, 312, 370], "temperature": 0.0, "avg_logprob": -0.06604053372534636, "compression_ratio": 1.6414342629482073, "no_speech_prob": 8.267247721960302e-06}, {"id": 458, "seek": 151338, "start": 1513.38, "end": 1517.38, "text": " unimportant that I can ignore them and chop them off.", "tokens": [517, 41654, 300, 286, 393, 11200, 552, 293, 7931, 552, 766, 13], "temperature": 0.0, "avg_logprob": -0.07948107069188898, "compression_ratio": 1.5247524752475248, "no_speech_prob": 1.7777942048269324e-05}, {"id": 459, "seek": 151338, "start": 1517.38, "end": 1521.38, "text": " But that also means with SVD that you're kind of doing extra work.", "tokens": [583, 300, 611, 1355, 365, 31910, 35, 300, 291, 434, 733, 295, 884, 2857, 589, 13], "temperature": 0.0, "avg_logprob": -0.07948107069188898, "compression_ratio": 1.5247524752475248, "no_speech_prob": 1.7777942048269324e-05}, {"id": 460, "seek": 151338, "start": 1521.38, "end": 1527.38, "text": " So it's both giving you more information and it's extra work.", "tokens": [407, 309, 311, 1293, 2902, 291, 544, 1589, 293, 309, 311, 2857, 589, 13], "temperature": 0.0, "avg_logprob": -0.07948107069188898, "compression_ratio": 1.5247524752475248, "no_speech_prob": 1.7777942048269324e-05}, {"id": 461, "seek": 151338, "start": 1527.38, "end": 1533.38, "text": " So something we talked about then towards the end, but didn't get too", "tokens": [407, 746, 321, 2825, 466, 550, 3030, 264, 917, 11, 457, 994, 380, 483, 886], "temperature": 0.0, "avg_logprob": -0.07948107069188898, "compression_ratio": 1.5247524752475248, "no_speech_prob": 1.7777942048269324e-05}, {"id": 462, "seek": 151338, "start": 1533.38, "end": 1536.38, "text": " into detail on.", "tokens": [666, 2607, 322, 13], "temperature": 0.0, "avg_logprob": -0.07948107069188898, "compression_ratio": 1.5247524752475248, "no_speech_prob": 1.7777942048269324e-05}, {"id": 463, "seek": 151338, "start": 1536.38, "end": 1539.38, "text": " Well, one, SVD on a big matrix is slow.", "tokens": [1042, 11, 472, 11, 31910, 35, 322, 257, 955, 8141, 307, 2964, 13], "temperature": 0.0, "avg_logprob": -0.07948107069188898, "compression_ratio": 1.5247524752475248, "no_speech_prob": 1.7777942048269324e-05}, {"id": 464, "seek": 153938, "start": 1539.38, "end": 1544.38, "text": " So here, and this is from a talk I gave at Pi Bay two years ago.", "tokens": [407, 510, 11, 293, 341, 307, 490, 257, 751, 286, 2729, 412, 17741, 7840, 732, 924, 2057, 13], "temperature": 0.0, "avg_logprob": -0.13755846505213265, "compression_ratio": 1.4770642201834863, "no_speech_prob": 1.9525141397025436e-05}, {"id": 465, "seek": 153938, "start": 1544.38, "end": 1549.38, "text": " I ran some timing tests for the speed of running SVD.", "tokens": [286, 5872, 512, 10822, 6921, 337, 264, 3073, 295, 2614, 31910, 35, 13], "temperature": 0.0, "avg_logprob": -0.13755846505213265, "compression_ratio": 1.4770642201834863, "no_speech_prob": 1.9525141397025436e-05}, {"id": 466, "seek": 153938, "start": 1549.38, "end": 1554.38, "text": " And it, I think this is in seconds, very quickly.", "tokens": [400, 309, 11, 286, 519, 341, 307, 294, 3949, 11, 588, 2661, 13], "temperature": 0.0, "avg_logprob": -0.13755846505213265, "compression_ratio": 1.4770642201834863, "no_speech_prob": 1.9525141397025436e-05}, {"id": 467, "seek": 153938, "start": 1554.38, "end": 1558.38, "text": " So here with a 10,000 by 10,000 matrix, it's just so slow.", "tokens": [407, 510, 365, 257, 1266, 11, 1360, 538, 1266, 11, 1360, 8141, 11, 309, 311, 445, 370, 2964, 13], "temperature": 0.0, "avg_logprob": -0.13755846505213265, "compression_ratio": 1.4770642201834863, "no_speech_prob": 1.9525141397025436e-05}, {"id": 468, "seek": 153938, "start": 1558.38, "end": 1561.38, "text": " It's not something you really want to be doing.", "tokens": [467, 311, 406, 746, 291, 534, 528, 281, 312, 884, 13], "temperature": 0.0, "avg_logprob": -0.13755846505213265, "compression_ratio": 1.4770642201834863, "no_speech_prob": 1.9525141397025436e-05}, {"id": 469, "seek": 153938, "start": 1561.38, "end": 1566.38, "text": " And one way to address this is randomized SVD.", "tokens": [400, 472, 636, 281, 2985, 341, 307, 38513, 31910, 35, 13], "temperature": 0.0, "avg_logprob": -0.13755846505213265, "compression_ratio": 1.4770642201834863, "no_speech_prob": 1.9525141397025436e-05}, {"id": 470, "seek": 156638, "start": 1566.38, "end": 1573.38, "text": " And basically randomized SVD is using, it's a little bit more complex", "tokens": [400, 1936, 38513, 31910, 35, 307, 1228, 11, 309, 311, 257, 707, 857, 544, 3997], "temperature": 0.0, "avg_logprob": -0.10506886449353449, "compression_ratio": 1.5681818181818181, "no_speech_prob": 1.5689085557824e-05}, {"id": 471, "seek": 156638, "start": 1573.38, "end": 1577.38, "text": " under the hood, but part of it is taking this, just multiplying by a", "tokens": [833, 264, 13376, 11, 457, 644, 295, 309, 307, 1940, 341, 11, 445, 30955, 538, 257], "temperature": 0.0, "avg_logprob": -0.10506886449353449, "compression_ratio": 1.5681818181818181, "no_speech_prob": 1.5689085557824e-05}, {"id": 472, "seek": 156638, "start": 1577.38, "end": 1582.38, "text": " random matrix that has a smaller dimension to get a matrix with a", "tokens": [4974, 8141, 300, 575, 257, 4356, 10139, 281, 483, 257, 8141, 365, 257], "temperature": 0.0, "avg_logprob": -0.10506886449353449, "compression_ratio": 1.5681818181818181, "no_speech_prob": 1.5689085557824e-05}, {"id": 473, "seek": 156638, "start": 1582.38, "end": 1584.38, "text": " smaller dimension that you'll work on.", "tokens": [4356, 10139, 300, 291, 603, 589, 322, 13], "temperature": 0.0, "avg_logprob": -0.10506886449353449, "compression_ratio": 1.5681818181818181, "no_speech_prob": 1.5689085557824e-05}, {"id": 474, "seek": 156638, "start": 1584.38, "end": 1588.38, "text": " And that surprisingly, you still keep a lot of the information you need", "tokens": [400, 300, 17600, 11, 291, 920, 1066, 257, 688, 295, 264, 1589, 291, 643], "temperature": 0.0, "avg_logprob": -0.10506886449353449, "compression_ratio": 1.5681818181818181, "no_speech_prob": 1.5689085557824e-05}, {"id": 475, "seek": 156638, "start": 1588.38, "end": 1590.38, "text": " and it can be very effective.", "tokens": [293, 309, 393, 312, 588, 4942, 13], "temperature": 0.0, "avg_logprob": -0.10506886449353449, "compression_ratio": 1.5681818181818181, "no_speech_prob": 1.5689085557824e-05}, {"id": 476, "seek": 159038, "start": 1590.38, "end": 1601.38, "text": " And so here I've plotted what the air is when reconstructing the matrix.", "tokens": [400, 370, 510, 286, 600, 43288, 437, 264, 1988, 307, 562, 31499, 278, 264, 8141, 13], "temperature": 0.0, "avg_logprob": -0.1932089477777481, "compression_ratio": 1.1584158415841583, "no_speech_prob": 1.5687517588958144e-05}, {"id": 477, "seek": 160138, "start": 1601.38, "end": 1622.38, "text": " And so, and actually let me maybe draw this.", "tokens": [400, 370, 11, 293, 767, 718, 385, 1310, 2642, 341, 13], "temperature": 0.0, "avg_logprob": -0.1216347614924113, "compression_ratio": 1.1938775510204083, "no_speech_prob": 3.555836201485363e-06}, {"id": 478, "seek": 160138, "start": 1622.38, "end": 1628.38, "text": " So with SVD, you've got some matrix that you start with, and then you're", "tokens": [407, 365, 31910, 35, 11, 291, 600, 658, 512, 8141, 300, 291, 722, 365, 11, 293, 550, 291, 434], "temperature": 0.0, "avg_logprob": -0.1216347614924113, "compression_ratio": 1.1938775510204083, "no_speech_prob": 3.555836201485363e-06}, {"id": 479, "seek": 162838, "start": 1628.38, "end": 1643.38, "text": " decomposing that into matrix U, S, and V.", "tokens": [22867, 6110, 300, 666, 8141, 624, 11, 318, 11, 293, 691, 13], "temperature": 0.0, "avg_logprob": -0.11847856447294161, "compression_ratio": 1.3815028901734103, "no_speech_prob": 9.516160389466677e-06}, {"id": 480, "seek": 162838, "start": 1643.38, "end": 1648.38, "text": " And, you know, maybe these start with large singular values, five, I don't", "tokens": [400, 11, 291, 458, 11, 1310, 613, 722, 365, 2416, 20010, 4190, 11, 1732, 11, 286, 500, 380], "temperature": 0.0, "avg_logprob": -0.11847856447294161, "compression_ratio": 1.3815028901734103, "no_speech_prob": 9.516160389466677e-06}, {"id": 481, "seek": 162838, "start": 1648.38, "end": 1650.38, "text": " know, 4.5.", "tokens": [458, 11, 1017, 13, 20, 13], "temperature": 0.0, "avg_logprob": -0.11847856447294161, "compression_ratio": 1.3815028901734103, "no_speech_prob": 9.516160389466677e-06}, {"id": 482, "seek": 162838, "start": 1650.38, "end": 1654.38, "text": " And then down here, you're getting like really little ones, 0.1, 0.01,", "tokens": [400, 550, 760, 510, 11, 291, 434, 1242, 411, 534, 707, 2306, 11, 1958, 13, 16, 11, 1958, 13, 10607, 11], "temperature": 0.0, "avg_logprob": -0.11847856447294161, "compression_ratio": 1.3815028901734103, "no_speech_prob": 9.516160389466677e-06}, {"id": 483, "seek": 162838, "start": 1654.38, "end": 1656.38, "text": " that may not have that much information.", "tokens": [300, 815, 406, 362, 300, 709, 1589, 13], "temperature": 0.0, "avg_logprob": -0.11847856447294161, "compression_ratio": 1.3815028901734103, "no_speech_prob": 9.516160389466677e-06}, {"id": 484, "seek": 165638, "start": 1656.38, "end": 1662.38, "text": " So you could just say, okay, let me chop these off, not use them.", "tokens": [407, 291, 727, 445, 584, 11, 1392, 11, 718, 385, 7931, 613, 766, 11, 406, 764, 552, 13], "temperature": 0.0, "avg_logprob": -0.09835845232009888, "compression_ratio": 1.376543209876543, "no_speech_prob": 3.2191815989790484e-05}, {"id": 485, "seek": 165638, "start": 1662.38, "end": 1666.38, "text": " Correspondingly, that will also mean you're going to be chopping off, I", "tokens": [3925, 6663, 12163, 11, 300, 486, 611, 914, 291, 434, 516, 281, 312, 35205, 766, 11, 286], "temperature": 0.0, "avg_logprob": -0.09835845232009888, "compression_ratio": 1.376543209876543, "no_speech_prob": 3.2191815989790484e-05}, {"id": 486, "seek": 165638, "start": 1666.38, "end": 1673.38, "text": " guess, these last few columns of U.", "tokens": [2041, 11, 613, 1036, 1326, 13766, 295, 624, 13], "temperature": 0.0, "avg_logprob": -0.09835845232009888, "compression_ratio": 1.376543209876543, "no_speech_prob": 3.2191815989790484e-05}, {"id": 487, "seek": 165638, "start": 1673.38, "end": 1676.38, "text": " Is this right?", "tokens": [1119, 341, 558, 30], "temperature": 0.0, "avg_logprob": -0.09835845232009888, "compression_ratio": 1.376543209876543, "no_speech_prob": 3.2191815989790484e-05}, {"id": 488, "seek": 165638, "start": 1676.38, "end": 1677.38, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.09835845232009888, "compression_ratio": 1.376543209876543, "no_speech_prob": 3.2191815989790484e-05}, {"id": 489, "seek": 165638, "start": 1677.38, "end": 1683.38, "text": " And then last few rows of V.", "tokens": [400, 550, 1036, 1326, 13241, 295, 691, 13], "temperature": 0.0, "avg_logprob": -0.09835845232009888, "compression_ratio": 1.376543209876543, "no_speech_prob": 3.2191815989790484e-05}, {"id": 490, "seek": 168338, "start": 1683.38, "end": 1699.38, "text": " No.", "tokens": [883, 13], "temperature": 0.0, "avg_logprob": -0.07737546188886775, "compression_ratio": 1.336734693877551, "no_speech_prob": 9.08037145563867e-06}, {"id": 491, "seek": 168338, "start": 1699.38, "end": 1703.38, "text": " So I'm just thinking about to get the matrix dimensions to line up.", "tokens": [407, 286, 478, 445, 1953, 466, 281, 483, 264, 8141, 12819, 281, 1622, 493, 13], "temperature": 0.0, "avg_logprob": -0.07737546188886775, "compression_ratio": 1.336734693877551, "no_speech_prob": 9.08037145563867e-06}, {"id": 492, "seek": 168338, "start": 1703.38, "end": 1707.38, "text": " So A has the shape in this case.", "tokens": [407, 316, 575, 264, 3909, 294, 341, 1389, 13], "temperature": 0.0, "avg_logprob": -0.07737546188886775, "compression_ratio": 1.336734693877551, "no_speech_prob": 9.08037145563867e-06}, {"id": 493, "seek": 168338, "start": 1707.38, "end": 1710.38, "text": " A has the shape like this.", "tokens": [316, 575, 264, 3909, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.07737546188886775, "compression_ratio": 1.336734693877551, "no_speech_prob": 9.08037145563867e-06}, {"id": 494, "seek": 171038, "start": 1710.38, "end": 1713.38, "text": " So you still want to get back that wider shape.", "tokens": [407, 291, 920, 528, 281, 483, 646, 300, 11842, 3909, 13], "temperature": 0.0, "avg_logprob": -0.08466744952731663, "compression_ratio": 1.5422885572139304, "no_speech_prob": 8.013405931706075e-06}, {"id": 495, "seek": 171038, "start": 1713.38, "end": 1714.38, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.08466744952731663, "compression_ratio": 1.5422885572139304, "no_speech_prob": 8.013405931706075e-06}, {"id": 496, "seek": 171038, "start": 1714.38, "end": 1717.38, "text": " So V, I think you could keep.", "tokens": [407, 691, 11, 286, 519, 291, 727, 1066, 13], "temperature": 0.0, "avg_logprob": -0.08466744952731663, "compression_ratio": 1.5422885572139304, "no_speech_prob": 8.013405931706075e-06}, {"id": 497, "seek": 171038, "start": 1717.38, "end": 1720.38, "text": " But you're throwing away information from U and S.", "tokens": [583, 291, 434, 10238, 1314, 1589, 490, 624, 293, 318, 13], "temperature": 0.0, "avg_logprob": -0.08466744952731663, "compression_ratio": 1.5422885572139304, "no_speech_prob": 8.013405931706075e-06}, {"id": 498, "seek": 171038, "start": 1720.38, "end": 1725.38, "text": " And so if you do that, you're no longer getting A is exactly equal to this,", "tokens": [400, 370, 498, 291, 360, 300, 11, 291, 434, 572, 2854, 1242, 316, 307, 2293, 2681, 281, 341, 11], "temperature": 0.0, "avg_logprob": -0.08466744952731663, "compression_ratio": 1.5422885572139304, "no_speech_prob": 8.013405931706075e-06}, {"id": 499, "seek": 171038, "start": 1725.38, "end": 1727.38, "text": " but we've thrown away some information.", "tokens": [457, 321, 600, 11732, 1314, 512, 1589, 13], "temperature": 0.0, "avg_logprob": -0.08466744952731663, "compression_ratio": 1.5422885572139304, "no_speech_prob": 8.013405931706075e-06}, {"id": 500, "seek": 171038, "start": 1727.38, "end": 1739.38, "text": " And so now let's say this is just approximately equal to A.", "tokens": [400, 370, 586, 718, 311, 584, 341, 307, 445, 10447, 2681, 281, 316, 13], "temperature": 0.0, "avg_logprob": -0.08466744952731663, "compression_ratio": 1.5422885572139304, "no_speech_prob": 8.013405931706075e-06}, {"id": 501, "seek": 173938, "start": 1739.38, "end": 1744.38, "text": " But what you've thrown away had pretty small magnitude since the singular", "tokens": [583, 437, 291, 600, 11732, 1314, 632, 1238, 1359, 15668, 1670, 264, 20010], "temperature": 0.0, "avg_logprob": -0.10845247904459636, "compression_ratio": 1.6269430051813472, "no_speech_prob": 4.860402896156302e-06}, {"id": 502, "seek": 173938, "start": 1744.38, "end": 1746.38, "text": " values were a small magnitude.", "tokens": [4190, 645, 257, 1359, 15668, 13], "temperature": 0.0, "avg_logprob": -0.10845247904459636, "compression_ratio": 1.6269430051813472, "no_speech_prob": 4.860402896156302e-06}, {"id": 503, "seek": 173938, "start": 1746.38, "end": 1752.38, "text": " Francesca, is there a question around this?", "tokens": [31441, 496, 11, 307, 456, 257, 1168, 926, 341, 30], "temperature": 0.0, "avg_logprob": -0.10845247904459636, "compression_ratio": 1.6269430051813472, "no_speech_prob": 4.860402896156302e-06}, {"id": 504, "seek": 173938, "start": 1752.38, "end": 1760.38, "text": " So what I was doing in this chart here is looking K is the number of singular", "tokens": [407, 437, 286, 390, 884, 294, 341, 6927, 510, 307, 1237, 591, 307, 264, 1230, 295, 20010], "temperature": 0.0, "avg_logprob": -0.10845247904459636, "compression_ratio": 1.6269430051813472, "no_speech_prob": 4.860402896156302e-06}, {"id": 505, "seek": 173938, "start": 1760.38, "end": 1762.38, "text": " values I kept.", "tokens": [4190, 286, 4305, 13], "temperature": 0.0, "avg_logprob": -0.10845247904459636, "compression_ratio": 1.6269430051813472, "no_speech_prob": 4.860402896156302e-06}, {"id": 506, "seek": 173938, "start": 1762.38, "end": 1766.38, "text": " And so I was saying, you know, if you throw away K, how close are you to", "tokens": [400, 370, 286, 390, 1566, 11, 291, 458, 11, 498, 291, 3507, 1314, 591, 11, 577, 1998, 366, 291, 281], "temperature": 0.0, "avg_logprob": -0.10845247904459636, "compression_ratio": 1.6269430051813472, "no_speech_prob": 4.860402896156302e-06}, {"id": 507, "seek": 176638, "start": 1766.38, "end": 1769.38, "text": " reconstructing your original matrix A?", "tokens": [31499, 278, 428, 3380, 8141, 316, 30], "temperature": 0.0, "avg_logprob": -0.12552911043167114, "compression_ratio": 1.5434782608695652, "no_speech_prob": 2.178189424739685e-05}, {"id": 508, "seek": 176638, "start": 1769.38, "end": 1777.38, "text": " And so here the air is these dotted lines and you can see, okay,", "tokens": [400, 370, 510, 264, 1988, 307, 613, 37459, 3876, 293, 291, 393, 536, 11, 1392, 11], "temperature": 0.0, "avg_logprob": -0.12552911043167114, "compression_ratio": 1.5434782608695652, "no_speech_prob": 2.178189424739685e-05}, {"id": 509, "seek": 176638, "start": 1777.38, "end": 1782.38, "text": " if I only kept 100 singular values, the air was much higher.", "tokens": [498, 286, 787, 4305, 2319, 20010, 4190, 11, 264, 1988, 390, 709, 2946, 13], "temperature": 0.0, "avg_logprob": -0.12552911043167114, "compression_ratio": 1.5434782608695652, "no_speech_prob": 2.178189424739685e-05}, {"id": 510, "seek": 176638, "start": 1782.38, "end": 1789.38, "text": " But as I was getting out there, and I think, yeah, I'd have to check.", "tokens": [583, 382, 286, 390, 1242, 484, 456, 11, 293, 286, 519, 11, 1338, 11, 286, 1116, 362, 281, 1520, 13], "temperature": 0.0, "avg_logprob": -0.12552911043167114, "compression_ratio": 1.5434782608695652, "no_speech_prob": 2.178189424739685e-05}, {"id": 511, "seek": 176638, "start": 1789.38, "end": 1791.38, "text": " I would have to look up what the original size was.", "tokens": [286, 576, 362, 281, 574, 493, 437, 264, 3380, 2744, 390, 13], "temperature": 0.0, "avg_logprob": -0.12552911043167114, "compression_ratio": 1.5434782608695652, "no_speech_prob": 2.178189424739685e-05}, {"id": 512, "seek": 176638, "start": 1791.38, "end": 1795.38, "text": " Kind of you reach a point where you stop really seeing improvements.", "tokens": [9242, 295, 291, 2524, 257, 935, 689, 291, 1590, 534, 2577, 13797, 13], "temperature": 0.0, "avg_logprob": -0.12552911043167114, "compression_ratio": 1.5434782608695652, "no_speech_prob": 2.178189424739685e-05}, {"id": 513, "seek": 179538, "start": 1795.38, "end": 1800.38, "text": " And the neat thing, though, is that this was the same both for randomized", "tokens": [400, 264, 10654, 551, 11, 1673, 11, 307, 300, 341, 390, 264, 912, 1293, 337, 38513], "temperature": 0.0, "avg_logprob": -0.07023776780574693, "compression_ratio": 1.6948356807511737, "no_speech_prob": 2.2124793758848682e-05}, {"id": 514, "seek": 179538, "start": 1800.38, "end": 1802.38, "text": " SVD and regular SVD.", "tokens": [31910, 35, 293, 3890, 31910, 35, 13], "temperature": 0.0, "avg_logprob": -0.07023776780574693, "compression_ratio": 1.6948356807511737, "no_speech_prob": 2.2124793758848682e-05}, {"id": 515, "seek": 179538, "start": 1802.38, "end": 1807.38, "text": " So like randomized SVD, the air is red, SVD it's in purple.", "tokens": [407, 411, 38513, 31910, 35, 11, 264, 1988, 307, 2182, 11, 31910, 35, 309, 311, 294, 9656, 13], "temperature": 0.0, "avg_logprob": -0.07023776780574693, "compression_ratio": 1.6948356807511737, "no_speech_prob": 2.2124793758848682e-05}, {"id": 516, "seek": 179538, "start": 1807.38, "end": 1809.38, "text": " It's a little bit hard to see.", "tokens": [467, 311, 257, 707, 857, 1152, 281, 536, 13], "temperature": 0.0, "avg_logprob": -0.07023776780574693, "compression_ratio": 1.6948356807511737, "no_speech_prob": 2.2124793758848682e-05}, {"id": 517, "seek": 179538, "start": 1809.38, "end": 1815.38, "text": " You have slightly lower air for the regular SVD, but not by a lot.", "tokens": [509, 362, 4748, 3126, 1988, 337, 264, 3890, 31910, 35, 11, 457, 406, 538, 257, 688, 13], "temperature": 0.0, "avg_logprob": -0.07023776780574693, "compression_ratio": 1.6948356807511737, "no_speech_prob": 2.2124793758848682e-05}, {"id": 518, "seek": 179538, "start": 1815.38, "end": 1819.38, "text": " But you're getting a huge speed up by doing randomized SVD.", "tokens": [583, 291, 434, 1242, 257, 2603, 3073, 493, 538, 884, 38513, 31910, 35, 13], "temperature": 0.0, "avg_logprob": -0.07023776780574693, "compression_ratio": 1.6948356807511737, "no_speech_prob": 2.2124793758848682e-05}, {"id": 519, "seek": 179538, "start": 1819.38, "end": 1823.38, "text": " So the speed for randomized SVD is here in blue.", "tokens": [407, 264, 3073, 337, 38513, 31910, 35, 307, 510, 294, 3344, 13], "temperature": 0.0, "avg_logprob": -0.07023776780574693, "compression_ratio": 1.6948356807511737, "no_speech_prob": 2.2124793758848682e-05}, {"id": 520, "seek": 182338, "start": 1823.38, "end": 1828.38, "text": " Green is the speed for the regular one.", "tokens": [6969, 307, 264, 3073, 337, 264, 3890, 472, 13], "temperature": 0.0, "avg_logprob": -0.07609662777040063, "compression_ratio": 1.5527638190954773, "no_speech_prob": 1.8342323528486304e-05}, {"id": 521, "seek": 182338, "start": 1828.38, "end": 1836.38, "text": " So a nice way to kind of get a speed up without losing that much information.", "tokens": [407, 257, 1481, 636, 281, 733, 295, 483, 257, 3073, 493, 1553, 7027, 300, 709, 1589, 13], "temperature": 0.0, "avg_logprob": -0.07609662777040063, "compression_ratio": 1.5527638190954773, "no_speech_prob": 1.8342323528486304e-05}, {"id": 522, "seek": 182338, "start": 1836.38, "end": 1840.38, "text": " And in practice, you often are not using all your singular values,", "tokens": [400, 294, 3124, 11, 291, 2049, 366, 406, 1228, 439, 428, 20010, 4190, 11], "temperature": 0.0, "avg_logprob": -0.07609662777040063, "compression_ratio": 1.5527638190954773, "no_speech_prob": 1.8342323528486304e-05}, {"id": 523, "seek": 182338, "start": 1840.38, "end": 1843.38, "text": " particularly the small ones.", "tokens": [4098, 264, 1359, 2306, 13], "temperature": 0.0, "avg_logprob": -0.07609662777040063, "compression_ratio": 1.5527638190954773, "no_speech_prob": 1.8342323528486304e-05}, {"id": 524, "seek": 182338, "start": 1843.38, "end": 1847.38, "text": " And this is a topic that maybe if we have time more later in the course,", "tokens": [400, 341, 307, 257, 4829, 300, 1310, 498, 321, 362, 565, 544, 1780, 294, 264, 1164, 11], "temperature": 0.0, "avg_logprob": -0.07609662777040063, "compression_ratio": 1.5527638190954773, "no_speech_prob": 1.8342323528486304e-05}, {"id": 525, "seek": 182338, "start": 1847.38, "end": 1849.38, "text": " we might come back to.", "tokens": [321, 1062, 808, 646, 281, 13], "temperature": 0.0, "avg_logprob": -0.07609662777040063, "compression_ratio": 1.5527638190954773, "no_speech_prob": 1.8342323528486304e-05}, {"id": 526, "seek": 184938, "start": 1849.38, "end": 1855.38, "text": " So I kind of have to see how long everything takes.", "tokens": [407, 286, 733, 295, 362, 281, 536, 577, 938, 1203, 2516, 13], "temperature": 0.0, "avg_logprob": -0.12144834654671806, "compression_ratio": 1.4264705882352942, "no_speech_prob": 6.502430915134028e-05}, {"id": 527, "seek": 184938, "start": 1855.38, "end": 1860.38, "text": " Then I also wanted to address full versus reduced SVD briefly.", "tokens": [1396, 286, 611, 1415, 281, 2985, 1577, 5717, 9212, 31910, 35, 10515, 13], "temperature": 0.0, "avg_logprob": -0.12144834654671806, "compression_ratio": 1.4264705882352942, "no_speech_prob": 6.502430915134028e-05}, {"id": 528, "seek": 184938, "start": 1860.38, "end": 1868.38, "text": " Just when we were using NumPy's SVD last time, we would call np.linouge.svd.", "tokens": [1449, 562, 321, 645, 1228, 22592, 47, 88, 311, 31910, 35, 1036, 565, 11, 321, 576, 818, 33808, 13, 5045, 263, 432, 13, 82, 85, 67, 13], "temperature": 0.0, "avg_logprob": -0.12144834654671806, "compression_ratio": 1.4264705882352942, "no_speech_prob": 6.502430915134028e-05}, {"id": 529, "seek": 184938, "start": 1868.38, "end": 1871.38, "text": " We were passing in full matrices equals false.", "tokens": [492, 645, 8437, 294, 1577, 32284, 6915, 7908, 13], "temperature": 0.0, "avg_logprob": -0.12144834654671806, "compression_ratio": 1.4264705882352942, "no_speech_prob": 6.502430915134028e-05}, {"id": 530, "seek": 184938, "start": 1871.38, "end": 1873.38, "text": " And I wanted to show you what difference that makes.", "tokens": [400, 286, 1415, 281, 855, 291, 437, 2649, 300, 1669, 13], "temperature": 0.0, "avg_logprob": -0.12144834654671806, "compression_ratio": 1.4264705882352942, "no_speech_prob": 6.502430915134028e-05}, {"id": 531, "seek": 187338, "start": 1873.38, "end": 1880.38, "text": " And this is for traditional SVD.", "tokens": [400, 341, 307, 337, 5164, 31910, 35, 13], "temperature": 0.0, "avg_logprob": -0.0896420031785965, "compression_ratio": 1.4069767441860466, "no_speech_prob": 2.9771810659440234e-05}, {"id": 532, "seek": 187338, "start": 1880.38, "end": 1887.38, "text": " But the full SVD is going to have U and V both be square matrices.", "tokens": [583, 264, 1577, 31910, 35, 307, 516, 281, 362, 624, 293, 691, 1293, 312, 3732, 32284, 13], "temperature": 0.0, "avg_logprob": -0.0896420031785965, "compression_ratio": 1.4069767441860466, "no_speech_prob": 2.9771810659440234e-05}, {"id": 533, "seek": 187338, "start": 1887.38, "end": 1893.38, "text": " And so that involves basically making up some columns for U", "tokens": [400, 370, 300, 11626, 1936, 1455, 493, 512, 13766, 337, 624], "temperature": 0.0, "avg_logprob": -0.0896420031785965, "compression_ratio": 1.4069767441860466, "no_speech_prob": 2.9771810659440234e-05}, {"id": 534, "seek": 187338, "start": 1893.38, "end": 1899.38, "text": " that don't directly depend on the data in A or whatever matrix you're decomposing,", "tokens": [300, 500, 380, 3838, 5672, 322, 264, 1412, 294, 316, 420, 2035, 8141, 291, 434, 22867, 6110, 11], "temperature": 0.0, "avg_logprob": -0.0896420031785965, "compression_ratio": 1.4069767441860466, "no_speech_prob": 2.9771810659440234e-05}, {"id": 535, "seek": 189938, "start": 1899.38, "end": 1906.38, "text": " because you're also adding some rows of pure zeros to your singular values,", "tokens": [570, 291, 434, 611, 5127, 512, 13241, 295, 6075, 35193, 281, 428, 20010, 4190, 11], "temperature": 0.0, "avg_logprob": -0.06081438612663883, "compression_ratio": 1.5158371040723981, "no_speech_prob": 7.253660442074761e-05}, {"id": 536, "seek": 189938, "start": 1906.38, "end": 1909.38, "text": " which will zero out with here.", "tokens": [597, 486, 4018, 484, 365, 510, 13], "temperature": 0.0, "avg_logprob": -0.06081438612663883, "compression_ratio": 1.5158371040723981, "no_speech_prob": 7.253660442074761e-05}, {"id": 537, "seek": 189938, "start": 1909.38, "end": 1916.38, "text": " But the way that U is being made into a square matrix is to have it form an orthonormal basis.", "tokens": [583, 264, 636, 300, 624, 307, 885, 1027, 666, 257, 3732, 8141, 307, 281, 362, 309, 1254, 364, 420, 11943, 24440, 5143, 13], "temperature": 0.0, "avg_logprob": -0.06081438612663883, "compression_ratio": 1.5158371040723981, "no_speech_prob": 7.253660442074761e-05}, {"id": 538, "seek": 189938, "start": 1916.38, "end": 1919.38, "text": " And so that means spanning the whole space.", "tokens": [400, 370, 300, 1355, 47626, 264, 1379, 1901, 13], "temperature": 0.0, "avg_logprob": -0.06081438612663883, "compression_ratio": 1.5158371040723981, "no_speech_prob": 7.253660442074761e-05}, {"id": 539, "seek": 189938, "start": 1919.38, "end": 1925.38, "text": " And I talked a little bit about spanning spaces in the 3Blue1Brown video we just watched.", "tokens": [400, 286, 2825, 257, 707, 857, 466, 47626, 7673, 294, 264, 805, 45231, 16, 22170, 648, 960, 321, 445, 6337, 13], "temperature": 0.0, "avg_logprob": -0.06081438612663883, "compression_ratio": 1.5158371040723981, "no_speech_prob": 7.253660442074761e-05}, {"id": 540, "seek": 192538, "start": 1925.38, "end": 1934.38, "text": " But if you want to completely fill or be able to represent anything in n-dimensional space,", "tokens": [583, 498, 291, 528, 281, 2584, 2836, 420, 312, 1075, 281, 2906, 1340, 294, 297, 12, 18759, 1901, 11], "temperature": 0.0, "avg_logprob": -0.10469146055333754, "compression_ratio": 1.5714285714285714, "no_speech_prob": 2.24729701585602e-05}, {"id": 541, "seek": 192538, "start": 1934.38, "end": 1939.38, "text": " or I guess in this case, m-dimensional space, you would have to kind of fill out those spaces.", "tokens": [420, 286, 2041, 294, 341, 1389, 11, 275, 12, 18759, 1901, 11, 291, 576, 362, 281, 733, 295, 2836, 484, 729, 7673, 13], "temperature": 0.0, "avg_logprob": -0.10469146055333754, "compression_ratio": 1.5714285714285714, "no_speech_prob": 2.24729701585602e-05}, {"id": 542, "seek": 192538, "start": 1939.38, "end": 1944.38, "text": " And so I just wanted to let you know that this is a thing in practice.", "tokens": [400, 370, 286, 445, 1415, 281, 718, 291, 458, 300, 341, 307, 257, 551, 294, 3124, 13], "temperature": 0.0, "avg_logprob": -0.10469146055333754, "compression_ratio": 1.5714285714285714, "no_speech_prob": 2.24729701585602e-05}, {"id": 543, "seek": 192538, "start": 1944.38, "end": 1948.38, "text": " I'd say I think you're usually going to be using reduced SVD.", "tokens": [286, 1116, 584, 286, 519, 291, 434, 2673, 516, 281, 312, 1228, 9212, 31910, 35, 13], "temperature": 0.0, "avg_logprob": -0.10469146055333754, "compression_ratio": 1.5714285714285714, "no_speech_prob": 2.24729701585602e-05}, {"id": 544, "seek": 194838, "start": 1948.38, "end": 1958.38, "text": " It's quicker to calculate, and you're often not needing to use to turn you into an orthonormal basis.", "tokens": [467, 311, 16255, 281, 8873, 11, 293, 291, 434, 2049, 406, 18006, 281, 764, 281, 1261, 291, 666, 364, 420, 11943, 24440, 5143, 13], "temperature": 0.0, "avg_logprob": -0.12777310689290364, "compression_ratio": 1.4517766497461928, "no_speech_prob": 8.39787389850244e-06}, {"id": 545, "seek": 194838, "start": 1958.38, "end": 1965.38, "text": " So that's the meaning of that parameter here, full matrices.", "tokens": [407, 300, 311, 264, 3620, 295, 300, 13075, 510, 11, 1577, 32284, 13], "temperature": 0.0, "avg_logprob": -0.12777310689290364, "compression_ratio": 1.4517766497461928, "no_speech_prob": 8.39787389850244e-06}, {"id": 546, "seek": 194838, "start": 1965.38, "end": 1968.38, "text": " Let me check, I'm going on time.", "tokens": [961, 385, 1520, 11, 286, 478, 516, 322, 565, 13], "temperature": 0.0, "avg_logprob": -0.12777310689290364, "compression_ratio": 1.4517766497461928, "no_speech_prob": 8.39787389850244e-06}, {"id": 547, "seek": 194838, "start": 1968.38, "end": 1977.38, "text": " All right. Any final questions on topic modeling before we move on to logistic regression?", "tokens": [1057, 558, 13, 2639, 2572, 1651, 322, 4829, 15983, 949, 321, 1286, 322, 281, 3565, 3142, 24590, 30], "temperature": 0.0, "avg_logprob": -0.12777310689290364, "compression_ratio": 1.4517766497461928, "no_speech_prob": 8.39787389850244e-06}, {"id": 548, "seek": 197738, "start": 1977.38, "end": 1982.38, "text": " The naive phase.", "tokens": [440, 29052, 5574, 13], "temperature": 0.0, "avg_logprob": -0.47369635105133057, "compression_ratio": 0.6666666666666666, "no_speech_prob": 8.74359393492341e-05}, {"id": 549, "seek": 198238, "start": 1982.38, "end": 2011.38, "text": " Okay, so let's start Notebook 3.", "tokens": [1033, 11, 370, 718, 311, 722, 11633, 2939, 805, 13], "temperature": 0.0, "avg_logprob": -0.278928313936506, "compression_ratio": 0.8, "no_speech_prob": 0.00010210965410806239}], "language": "en"}