{"text": " I am recording now, so, but please keep talking. Don't be shy, Serata, it's okay. Oh, getting a bit noisy out here with street cleaning something. I think your headset is very good because it's filtering it out. Oh, good. I didn't hear anything. So, I mean, I say street cleaning, it's more like footpath cleaning. We have a walking path along the front of our house. Oh, come on, I start pressing the recording button and everybody stops talking. Well, you know. People don't want to just hear my voice all the time on these recordings, guys. There are things I wanted to cover in today's session, but then the responsible part of me says I probably ought to create a lesson before Tuesday's class. So maybe we'll do that. I've got a question, Jeremy. I had to leave before you finished that code change yesterday. Did you? Was that actually, do you want to recap on where we got to with waiting with data? Probably not, because you can just watch the video. And so, like, otherwise, I guess we're just doing it twice. So is it working now? Yeah, yeah, it's all good. You know, I mean, it's the concept is working correctly in terms of the code. We didn't get a better score, but I didn't particularly expect to either. You know, maybe after next Tuesday's lesson, we will revisit it because I actually think the main thing it might be useful for is what's called curriculum learning, which is basically focusing on the hard bits. Looks like Nick's internet still isn't working, but Nick was saying the other day that he looked at which ones we're having the errors on, which is like what we look at in the book. It's like looking at the classification interpretation and looking at like plot top losses and stuff. And he said like, yeah, all the ones that we're making that we're getting wrong are basically from the same one or two classes. So I haven't done much with curriculum learning in practice. Like I like all it means in theory is that we use our weighted data loader to weight the ones that we're getting wrong higher. And whether that will actually give us a better result or not, I'm not sure. But that I think that's more likely to be a useful path than simply reweighting things to be more balanced, because we don't want things to be more balanced because the ones that we observe the most often in the test set are actually the ones we want to be the best at. I will say I didn't check whether the distribution of the test set is the same as the training set. If it's randomly selected, then it will be and if it's not, then that would be a reason to use a weighted data loader as well. Yeah. Okay, so. What's the difference? Yeah, I guess like what's the is it is a curriculum learning kind of related to boosting and conceptually. Not really. I mean, maybe. So boosting is where you calculate the difference between the actuals and your predictions to get residuals, and then you create a model that tries to predict the residuals. And then you can add those two predictions together, which is, if not done carefully as a recipe for overfitting. But if done carefully can be very effective. Yeah, well, we're talking about something which is conceptually very different, which is saying like, oh, we're like really bad at recognizing this category. So let's show that category more often during training. Of kind of focusing on examples you're getting it wrong. More conceptually doing something similar. I was just going to ask, are the labels ever wrong, like by accident, or intentionally in Kaggle? Of course, absolutely. So both intentionally as well? No, not intentionally. I mean, not normally. Like sometimes there might be a competition where they say like, oh, this is a synthetically generated data set and some of the data is wrong because we're trying to do something like what happens in practice, but we can't share the real data. So is there any advantage in trying something like some uncertainty values from something like MC dropout, try to find like a threshold of things that are too difficult and then potentially they're wrongly labeled? I'm not sure you would need that. Like the thing we use in the book and the course is simply to find the things that we are confident of, but we know we're wrong, but turned out to be wrong and then just look at the pictures. So the raw software max value is enough, you think, to basically know whether or not it's true? I do, yeah. I mean, that seems to work pretty well. I mean, the only thing is you would need to be able to recognize these things in photos. But I'm sure if you spent an hour reading on the internet about what these different diseases are and how they look, you would be able to pick it up fast enough. And then, you know, just like we did in chapter two, if you're recognizing the things that aren't black and brown teddy bears. Okay. But so plausibly even just knocking out some of the extremely difficult examples might get you higher on the leadable purely by virtue of them misleading the model. Not by knocking out the hard ones, but by knocking out the wrong ones. Yes. Unless the test set is mislabeled consistently with the training set, in which case you would not want to knock them out because you would want to be able to correctly predict the things which people are incorrectly recognizing as the wrong disease. Something to try there. Yeah. Yeah. So I would do exactly what we did in chapter two. You know, you can use exactly the same widget. But as I say, you'd have to probably spend an hour learning about rice disease, which would be a reasonably interesting thing to do anyway. I just saw a link. There's a discussion in the petty. Some people identify there's some mislabeling at least over 20 already. Okay. Yes, so definitely happened. Says we have manually annotated every image with the help of agricultural experts, but there could be errors. Wow, this person knows more about rice than I do. I think the images in the tongue grow have charsive issues his symptoms can easily be confused with potassium deficiency. Fair enough. Is that an example of what you're talking about where if if layman or sorry for semi expert gets confused then the labeling and the test sets probably the same. Yeah, exactly. So you're probably fixing these would probably screw up your model, because assuming that the test set was labeled used by the same people in the same way. Sometimes test sets. The test set is more of a gold standard they'll make more effort to talk to like a larger number of high quality experts and have them vote or something. Honestly, this competition seems like it doesn't even have any prize money attached so I'd like I think it's really low, low investment probably. And so I doubt they did that, but, but that can happen. Yeah, that the test set could have, I mean, it makes sense to invest in getting really good labels for the test set. Actually, I was looking at one of the other competitions on unifes the x rays. I think there there was one, somebody had identified their wrist was wrongly labeled as a carrot one. It is a guess yeah it's not a, there's no money again but it's been running for a while. What's it called a unifes UNIFESP. Oh right it's another community competition I, yeah, gosh it's not very popular why is there only 74 teams. Yeah sorry go ahead. I was just looking around and it looked interesting so I'm number 15 at the moment. It is a slightly weird one because well, it's, it's interesting because some of the x rays have multiple labels, but the labels are just concatenated. So there's interesting discussion on how you'd analyze that would you treat a combination as a distinct classification, whether it was like a neck and a chest or something or do you look at each of them individually and then try and label a multiple one from the different things so some. Okay, so I'm just having a look at this competition so when does it close, it's a month to go but I don't know exactly when that is. Normally there's July 31. Okay, where do you see that. When you go to like to the bottom of, like, on the overview and it says there's a whole timeline so then you just hover over that. Oh my god I see it says close in a month but you actually have to get a tooltip by hovering. Okay, thanks to Nish. That's strange UX. Yeah, so we've actually got more than a month so maybe next week we could have a look at this one, because it would be a good opportunity to play around with medical image stuff because they're using DICOM, I think. Yeah, there's somebody is also which I used supplied a library of PNGs, which made it easier to use but I don't know what you lose in using that rather than the DICOM images. It rather depends. So DICOM is a very generic file format that can contain lots of different things. One of the things DICOM contain is, is higher bit depth images than a PNG allows. So if they've, yes, they might, they might have gotten rid of that. Fastai has a nice medical imaging. It's pretty small but like has some useful stuff medical imaging library, which I think is fastai.vision.medical, which can handle DICOM directly. And I see there's a fastai entry as well. That'd be fun we should try this next week. I see there's a PNGs. I think the DICOMs come to about 27 gigabytes. Oh my god. Okay. So the, the PNG was quite attractive from that point. Yeah. So one thing that you can do with DICOM is to compress them, particularly using JPEG 2000 which is a really good compression. But yeah, people often don't, for some reason. So probably the first thing I'd look at in that competition is to see, look at DICOM and see is it storing 16 bit data or not. And if it is I would try to find a way to resave that without losing the extra information, which I think we've got examples of in our medical imaging tutorial. So we can look at that. All right, I'm going to share my screen. Even though I don't know what I'm doing. I'm going to have to drop in a few minutes but I'll catch the rest on the record. Thanks for this. Nice to see you. By the way I was looking at this. Conf next paper. Gosh, everybody. Congratulates transformers on everything. Vision Transformers bring new ideas like the Adam W optimizer. I guess who actually wrote the first thing saying we should always use the Adam W optimizer. I would be silver in fast AI. I think that was years before the vision transformers. Adam W. I read that paper last night. I was thinking they talk about how all of these things were already there. They just rediscovered them like slightly larger kernel size and things like that. Which begs the question why hasn't no one just done experiments to tweak these things together. I mean it's. Nobody takes any notice because they're not written in PDFs, you know. Is it. I mean, these benchmarks they're like, the thing is that like a lot of researchers aren't good practitioners. So they just, they're not very good at training accurate networks and they don't know these tricks, you know, and they don't hang out on Kaggle and learn about what actually works. But then the thing is like, it's not always easy to publish, like even if you did stick it into a PDF and submit it to Europe's there's no particular high likelihood that they're going to accept it because the field research wise is very focused on theory results and you know things with lots of Greek letters in them. Does that mean that the part of the problem is that the data sets the benchmarks are just too inaccessible to the average person. So, no, I wouldn't say that for ImageNet 1K. No, I wouldn't say that the issue is I think the culture of research is not particularly interested in experimental results, you know. But in a limited experience, I will say it's very hard to find the viewer as well, especially you have a very strong domain, not just one thing or the sample data set you can find in open source. But you have a very strong domain and then a lot of peer reviewers, they're just not picking up to review it, even if we pay for the reviewer we're using, so people can get it for free. And it takes us three months just to find the viewer. Yeah. Topic of papers, when do you know when a paper is worth reading, given the situation. I mean, I don't like until, I mean, I'm very fond of papers that describe things which did very well in an actual competition, you know, that then we know this is something that actually predicts things accurately. And they can get similar results if they've got a good, you know, just table of results. So generally speaking I like things that actually have good results, particularly if they show like how long it took to train and how much data they trained on. And, yeah, so are they getting good results, using less data and less time than you might expect from the same thing. And yeah I certainly wouldn't focus only on those that get good results on really big data sets that's not necessarily more interesting. I'm very interested in things that show good results using transfer learning. So look for things that are like practically useful, I don't train that much from random. So I'm very interested in things that do well in transfer learning. Also like, look for people who you've liked their work before, you know, and, and in particular that doesn't mean like always reading the latest papers. You know if you come across a paper from somebody that you find useful. Go back and look at their Google Scholar and look read the older papers. See who they collaborate with and read their papers. So for example, I really like Kwok Lee and Google Brain. His, him and his team do a lot of good work it tends to be, you know, very practical and high quality results and so we know when his team releases a paper I. I also know like he seems to have similar interests to mine like he tends to do stuff involving transfer learning and getting good results in less epochs and stuff like that. So, if I see he's got a new paper out I'm pretty likely to read it. I have a question. I mean, I mean, for for the the carrier competitions and like, like in a lab type of environment is, I mean, when to the question that I have is when to stop iterating on on a model on a model that you have is, is, I have the. I mean, when is enough enough to do the training on the data that you have. When is enough. So that question. I mean, there's some reason you're doing this work right so like you hopefully know when it does what you want it to do. And the thing that happens, all that happens, especially to me all the time is that he trained the model, and it works perfectly fine on the lab. When we're doing it and then as soon as we throw a couple of images that they are not part of the set. I mean that goes nuts and okay. Because, like, or more light, or the temperature or stuff like that. So that's a different problem right so that that means your problem is that you're, you're not using the, you know, the right data to train on. So like you need to you. You need to be thinking about how you're going to deploy this thing. When you train it. And if you train it with data that's different to, you know, how you're going to deploy it. It's not going to work. Yeah, so that's that's what that means, and it might be difficult to get data enough data of the kind you're going to deploy it on but like, at some point you're going to be deploying this thing, which means by definition you've got some way of getting that data you're going to deploy it with. So like do the exact thing you're going to use to deploy it but don't deploy it just capture that data until you've got some actual data from the actual environment you want to deploy the model in. And also take advantage of semi supervised learning techniques to then, you know, and transfer learning to maximize the amount of juice you get from that data that you've collected. And finally I'd say, like that's a for medical imaging, like, okay you want to deploy a model to like a new hospital they've got a different brand of MRI machine you haven't seen before. I would take advantage of fine tuning, you know, each time I deployed it to some different environment where things a bit different, I would expect to have to go through a fine tuning process to train it to recognize that particular MRI machines, images, but you know each time you do that fine tuning it shouldn't take very much data or very much time because it's your models already learned the key features. And you're just asking it to learn to recognize slightly different ways of seeing those features. Yeah, I don't think you'll solve this by training for longer. You know, you'll solve it by figuring out your, your data pipeline your data labeling and your rollout strategy. Usually the issues that we're having is that we don't have enough data of certain category. But, I mean, the thing that you did yesterday. It resolves a little bit of that problem I think we're going to start using. Yeah. Well also like, if you don't have enough data of some category, don't use the model for that category. Well, so like, you know, rather than using softmax use binary sigmoid, you know, as your last layer and so then you've kind of got like a probability that X appears in this image and so then you can, you can recognize when none of the things that you can predict well appear in the image. And so, then have a, you know, you always want a human in the loop anyway. So when you didn't find any of the categories of things you've got enough data to be able to find, then triage those to the human review. And one thing that we did is, I mean we have like 50 something categories. Just one moment, hang on. Sorry about that. We had like 50 categories and some of them are like they have a lot like 10 of them have a lot of items. So we end up doing like in a three step kind of process, like the ones with a lot, the ones with medium number, with a smaller number. And it looks like you resolve the problem a little bit. Cool. But, but this was to classify metadata coming from, from other systems and classify for legal purposes for legal retention. I see. I had a question actually, you tried the data a lot of sites so I think you haven't submitted that to Kaggle notebook. So, did you do any validation locally first before submitting to Kaggle, something like that. No, I mean you saw what I did right. And when I did it, so I just, yeah, I just, I'm like I was intentionally using a very mechanistic approach. So it's kind of like this. Yeah, showing like his, the basic steps of pretty much any computer vision model which is entirely mechanical and doesn't require any domain expertise. So yeah, my question more was like, shouldn't we always treat the public leaderboard like a good or like should we take a hold out local data sets first to validate. I mean, I always have a validation set. Yeah. Which we saw in this. And this I just used a random splitter, because as far as I know the test set and the Kaggle competition is a randomly split validation set. Yeah, so like, whether it be for Kaggle or anything. I think creating a validation set that is closely as possible represents the data you expect to get in deployment or in your test set is really important. And yeah, I actually didn't spend the time doing that on this paddy competition. Normally on Kaggle if somebody does and notices there's a difference between the private leaderboard and the public leaderboard and like the test set and the training set normally it'll appear, you know, in discussions or on a Kaggle kernel or something. So, partly why I didn't look into it but yeah I mean you should probably check. Do they have the same sizes. And for me if I said as I see any difference between the test set and the training set that puts my alarm bells on right because now I know that it's not randomly selected. And if you know it's not randomly selected then you immediately have to think okay they're trying to trick us. So, I would then look everything I could for differences. Because it takes effort to not randomly select a test set so they must be doing it very intentionally for some reason. Quite often for wrong reasons. I think so, like I don't think a Kaggle competition should ever silently give you a systematically different test set. I think that's great reasons to create a systematically different test set, but there's never a reason not to tell people. So if it's like medical imaging is a different hospital you should say this is a different hospital or if it's fishing you should say these are different boats, or, you know, because like you want people to do well on your data. So if you tell them, then they can use that information to give you better models. And then, like, going back to what you asked about. There's this validation and training then there's this, whether your local validation maps to what's happening on the leaderboard with the score on the hidden test set. But there's one other scenario that I encountered recently. And maybe it would be interesting for someone when you're working on a competition. You might have missed something in your code or the prediction, you know your model is doing something useful but you're failing to output a correctly formatted submission file, and not in the sense that the submission phase on Kaggle, but some predictions are not aligned, or should be or, you know, they're for a different customer ID or stuff like that. So, once you have one good submission file, relatively good, you can just store it locally, and then see, you know, run a check the correlation between your new submission, and the one that you know that this, okay, and you know the correlation should be upwards of 0.9. And then you know yeah okay so I didn't mess up anything with the technical aspect of putting the prediction I mean, it's not a great trick but, you know, I was like, putting my hair out, why is this not working it's a better model. So this was like a sanity check step. Maybe at some point. Thanks, cool. Alright, so let me share my screen. It's fine zoom, zoom, share screen. Oh, that's not the right button. Control shift H. Okay. Where did we get to in the last lesson. We finished random forests right. And, Oh, that's right and I haven't posted that video yet. Last year life. Okay, so we could get small models. And to get to the end of this. Okay, so that basically. So we basically finished the second one of, of our Kaggle things. So next week. See what's in part three. Gradient accumulation. I think that's worth covering. So one thing that somebody pointed out on Kaggle is I've actually, I'm using gradient accumulation wrong. I was passing in two here to mean to make create two batch, like do two batches before you accumulate. But actually what I meant to be putting in here is the kind of target batch size I want. So that would be actually, I should be putting 64 here. So I feel a bit stupid. What I've been doing is I've been actually not using gradient graded correlation at all. I guess it's been doing a batch and saying that's over. I'm saying my maximum batch size should be two. Okay. So this has actually been not working at all. That's interesting. Oops. So it's been using a batch size of 32. And not accumulating. Ah, okay. So that's one thing to note. So when I get Kaggle GPU time again, we'll need to rerun this. Actually it only took 4,000 seconds. So I guess we should, we could just get it running right now, couldn't we? So that should be 64. Gravity of Paths defines how large the effective batch size you want is. So if we just remove the batch size, we can just remove the number of batches. Oh, we can just remove this sentence entirely. Oh no, that's right. We need to remove the batch size by some number based on how small we need it to be for our GPUs per M. So And on Kaggle, I think these were all smaller. I don't know why, but the Kaggle GPUs use less memory than my GPU for some reason. Okay. So we're now Let's try running it. Jeremy, you would increase that number until we no longer get CUDA out of memory. And you could be able to pretty much guess it by looking at like, I mean, you can just, once you found a batch size that fits, you know, so the default batch size, I believe is 32. Once you find a batch size that fits, sorry, 64 is the default batch size that fits, you're just like, okay, well, if it fits in 32, then I just need to set it to two because 64 divided by two is enough. And the key thing I do here is, you know, so I've got this report GPU function. So what I did at home was I just, you know, changed this until it got less than 16 gig. And as you can see, I'm just doing like a single APOC on small images. So this ran in, I don't know, 15 seconds or something. Yeah, batch size 64 by default. Yeah, so then I just went through checking the memory use of ConvNextLarge with different image sizes, again, just keeping on using just one APOC. And that's how I figured out what I could do to set the QM to work. All right, so that should be right to save and run. And then turn off this one. So when you're running something like you click save version, and you click run, you'll then see it down here. And that runs in the background, you don't have to leave this open. And so you can go back to it later. So if I just copy that, you can close it. And if I go to my notebook in Kaggle, this shows me version three or four because version four hasn't finished running yet. So if I click here, I can go to version four and it says, oh, that's still running. And I can see here, it's been running for about a minute. And it shows me anything that you print out will appear, including warnings. So that's what happens in Kaggle. So if we also do the multi objective loss function thing, that would be cool. So I thought like next time in our next lesson, broadly speaking, gosh, this is taking a long time. I kind of want to cover like what the inputs to a model look like and what the outputs to a model look like. So like in terms of inputs. Really the key thing is embeddings. That's the key thing we haven't seen yet in terms of what model inputs look like. For model outputs, I think we need to look at softmax. Softmax. Cross entropy loss. Entropy loss. And then, you know, our multi target loss, which we could do first, kind of a segue. So maybe in terms of the ordering, the segue would be like doing multi target loss first. And we could talk about softmax and cross entropy, which would then lead us potentially to like looking at the bear classifier. What if there's no bears? So we can just use the binary sigmoid. So then for embeddings, I guess that's where we'd cover the collaborative filtering, collaborative filtering, because that's like a really nice version of embeddings. So I guess the question is, for those who have done the course before, are there any other topics, I guess like time permitting, it would be nice to look at like the confnet, what a confnet is. Just kind of so that's all right. Then we've got like the outputs, the inputs, and then the middle. What about more NLP stuff? Like what? Well, I've heard that I can face is getting integrated with past AI, maybe looking at that how it works. Well, it's not done yet. So we can't do that yet. But definitely in part two. I've got a question. I don't know if it's helpful, but there's a lot of emphasis on outputs and inputs. But like in the middle, just understanding like the outputs of a hidden layer, whether they're going awry or not, how do you debug that? How do you understand, you know, when to kind of look at that? Yeah, very helpful. Last time we did a part two, we did a very deep dive into that. And I think we should do that again in our part two, because like most people won't have to debug that because if you're using an off the shelf model, you know, like it's, you know, with off the shelf initializations, that shouldn't happen. So it's probably more of an advanced debugging technique, I would say. But yeah, if you're interested in looking at it now, definitely check out our previous part two. Because we did a very deep dive into that and developed the so forth, colorful dimension plot, which is absolutely great for that. Jeremy? What about deep learning on tabular data? Yeah, so that would exactly so collaborative filtering would lead us exactly into that. Thank you. Yeah, sorry, Serata. Do you mind to spend five minutes talking about the importance of the ethical side? At least you point to the resources Rachel prepared before. So I think that would make people, because it's so easy to build a model, but how to apply is getting more scary now. Yes. Yes, I mentioned in lesson one, the data ethics course but you're right, it would be nice to kind of like touch on something there wouldn't it. By Rachel from part one before. Yeah, I mean that, I mean, okay, I mean that actually would be a great thing just to talk about, you know, that that lecture is not at all out of date. So, yes. So maybe touch on it in this one, and also talk link to, you know, for varying levels of interest. The two hour version would be Rachel's talk in the 2020 lecture, and then deeper interest still would be the, yes, the full ethics course, that's a great point. Thank you. So then, for, for actually pretty much all of these things. We have Excel spreadsheets, which is fun. So there's, let's have a look, collaborative filtering. Oh, looks like I've already downloaded that. Sure, I will encourage you to continue teaching in Excel. Yesterday I on the panel in a data science conference and when I mentioned, I start with Excel actually inspire a lot of people, they want to have a go with data science and learning it. So, good feedback. Yeah, because there's certainly some people who don't find it useful at all. And they tend to be quite loud about it so it's certainly nice to hear that that feedback. Sorry. So I thought you didn't let those people get to you. Well, I only pretend that anybody doesn't get to me. I was going to say that's that was really great to see. I've only seen it done once before. And that was in a physicist in Belgium who explained relative transfer modeling using Excel, and it was just so nice to see the clarity. Okay, great. Okay, thank you. I will. Let's see, so we've got. So I think these are actually from the 2019 course faster I one courses to one. So I'm just going to grab them all. I don't think we're going to cover this year. This part one that we will cover part two is like different optimizers like momentum and Adam and stuff, but I think that's okay because I feel like nowadays. Just use the default Adam W and it works. So I don't. I think it's fine. Not to know too much more than that. It's, it's a little bit of a technicality nowadays. Yeah. It used to be something we did in one of the first lessons you know but that was when you kind of had to know it right because yours fiddled around with momentum and blah blah blah. And to me, always like the biggest thing when starting with something is to have to, you know, once I figure out how to read in the data, then things. I'm really grateful that there's such an emphasis in this edition of the course on the reading, you know, data, and you know, with similar data, that is something that I would also stay on the lookout for just understanding better reading the data. Great. I don't think we did this one anymore, because we kind of have better versions in in Jupiter, with iPod widgets so we've got this fun. Convolutions example. Which I think is still valuable. Okay, we've got soft max and cross entropy examples. And we've got collaborative filtering. That sounds interesting. Wonder what that is. And then, also we've got word embeddings. All right, and but these are such a cool and important subject. And it's something that we haven't discussed that much in this course. No, we haven't touched them at all. All right. It feels like a lot to cover. We will. We will do our best. Okay. I think we're up to our hour so thanks everybody. Nice chat today, and I will get to work on putting this together. Have a nice weekend. Thank you so much. Thanks. Remind everyone. This is a waste and bias video today. I think 6 o'clock this one time so with anyone interest. The guy mentioned here, Thomas mentioned you're going to have another US session as well, but you can join. Yes I think there's details on the forum. Thanks. Bye.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 5.0, "text": " I am recording now, so, but please keep talking.", "tokens": [286, 669, 6613, 586, 11, 370, 11, 457, 1767, 1066, 1417, 13], "temperature": 0.0, "avg_logprob": -0.33887955382630064, "compression_ratio": 1.5778894472361809, "no_speech_prob": 0.08483339846134186}, {"id": 1, "seek": 0, "start": 5.0, "end": 7.0, "text": " Don't be shy, Serata, it's okay.", "tokens": [1468, 380, 312, 12685, 11, 4210, 3274, 11, 309, 311, 1392, 13], "temperature": 0.0, "avg_logprob": -0.33887955382630064, "compression_ratio": 1.5778894472361809, "no_speech_prob": 0.08483339846134186}, {"id": 2, "seek": 0, "start": 7.0, "end": 16.0, "text": " Oh, getting a bit noisy out here with street cleaning something.", "tokens": [876, 11, 1242, 257, 857, 24518, 484, 510, 365, 4838, 8924, 746, 13], "temperature": 0.0, "avg_logprob": -0.33887955382630064, "compression_ratio": 1.5778894472361809, "no_speech_prob": 0.08483339846134186}, {"id": 3, "seek": 0, "start": 16.0, "end": 23.0, "text": " I think your headset is very good because it's filtering it out.", "tokens": [286, 519, 428, 26850, 307, 588, 665, 570, 309, 311, 30822, 309, 484, 13], "temperature": 0.0, "avg_logprob": -0.33887955382630064, "compression_ratio": 1.5778894472361809, "no_speech_prob": 0.08483339846134186}, {"id": 4, "seek": 0, "start": 23.0, "end": 29.0, "text": " Oh, good. I didn't hear anything. So, I mean, I say street cleaning, it's more like footpath cleaning.", "tokens": [876, 11, 665, 13, 286, 994, 380, 1568, 1340, 13, 407, 11, 286, 914, 11, 286, 584, 4838, 8924, 11, 309, 311, 544, 411, 2671, 31852, 8924, 13], "temperature": 0.0, "avg_logprob": -0.33887955382630064, "compression_ratio": 1.5778894472361809, "no_speech_prob": 0.08483339846134186}, {"id": 5, "seek": 2900, "start": 29.0, "end": 37.0, "text": " We have a walking path along the front of our house.", "tokens": [492, 362, 257, 4494, 3100, 2051, 264, 1868, 295, 527, 1782, 13], "temperature": 0.0, "avg_logprob": -0.14113683700561525, "compression_ratio": 1.5450643776824033, "no_speech_prob": 0.00021595442376565188}, {"id": 6, "seek": 2900, "start": 37.0, "end": 40.0, "text": " Oh, come on, I start pressing the recording button and everybody stops talking.", "tokens": [876, 11, 808, 322, 11, 286, 722, 12417, 264, 6613, 2960, 293, 2201, 10094, 1417, 13], "temperature": 0.0, "avg_logprob": -0.14113683700561525, "compression_ratio": 1.5450643776824033, "no_speech_prob": 0.00021595442376565188}, {"id": 7, "seek": 2900, "start": 40.0, "end": 45.0, "text": " Well, you know.", "tokens": [1042, 11, 291, 458, 13], "temperature": 0.0, "avg_logprob": -0.14113683700561525, "compression_ratio": 1.5450643776824033, "no_speech_prob": 0.00021595442376565188}, {"id": 8, "seek": 2900, "start": 45.0, "end": 52.0, "text": " People don't want to just hear my voice all the time on these recordings, guys.", "tokens": [3432, 500, 380, 528, 281, 445, 1568, 452, 3177, 439, 264, 565, 322, 613, 25162, 11, 1074, 13], "temperature": 0.0, "avg_logprob": -0.14113683700561525, "compression_ratio": 1.5450643776824033, "no_speech_prob": 0.00021595442376565188}, {"id": 9, "seek": 2900, "start": 52.0, "end": 58.0, "text": " There are things I wanted to cover in today's session, but then the responsible part of me says I probably ought to create a lesson", "tokens": [821, 366, 721, 286, 1415, 281, 2060, 294, 965, 311, 5481, 11, 457, 550, 264, 6250, 644, 295, 385, 1619, 286, 1391, 13416, 281, 1884, 257, 6898], "temperature": 0.0, "avg_logprob": -0.14113683700561525, "compression_ratio": 1.5450643776824033, "no_speech_prob": 0.00021595442376565188}, {"id": 10, "seek": 5800, "start": 58.0, "end": 64.0, "text": " before Tuesday's class. So maybe we'll do that.", "tokens": [949, 10017, 311, 1508, 13, 407, 1310, 321, 603, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.17240020206996373, "compression_ratio": 1.4848484848484849, "no_speech_prob": 0.0002904972934629768}, {"id": 11, "seek": 5800, "start": 64.0, "end": 72.0, "text": " I've got a question, Jeremy. I had to leave before you finished that code change yesterday. Did you?", "tokens": [286, 600, 658, 257, 1168, 11, 17809, 13, 286, 632, 281, 1856, 949, 291, 4335, 300, 3089, 1319, 5186, 13, 2589, 291, 30], "temperature": 0.0, "avg_logprob": -0.17240020206996373, "compression_ratio": 1.4848484848484849, "no_speech_prob": 0.0002904972934629768}, {"id": 12, "seek": 5800, "start": 72.0, "end": 79.0, "text": " Was that actually, do you want to recap on where we got to with waiting with data?", "tokens": [3027, 300, 767, 11, 360, 291, 528, 281, 20928, 322, 689, 321, 658, 281, 365, 3806, 365, 1412, 30], "temperature": 0.0, "avg_logprob": -0.17240020206996373, "compression_ratio": 1.4848484848484849, "no_speech_prob": 0.0002904972934629768}, {"id": 13, "seek": 5800, "start": 79.0, "end": 84.0, "text": " Probably not, because you can just watch the video. And so, like, otherwise, I guess we're just doing it twice.", "tokens": [9210, 406, 11, 570, 291, 393, 445, 1159, 264, 960, 13, 400, 370, 11, 411, 11, 5911, 11, 286, 2041, 321, 434, 445, 884, 309, 6091, 13], "temperature": 0.0, "avg_logprob": -0.17240020206996373, "compression_ratio": 1.4848484848484849, "no_speech_prob": 0.0002904972934629768}, {"id": 14, "seek": 8400, "start": 84.0, "end": 92.0, "text": " So is it working now? Yeah, yeah, it's all good. You know, I mean, it's", "tokens": [407, 307, 309, 1364, 586, 30, 865, 11, 1338, 11, 309, 311, 439, 665, 13, 509, 458, 11, 286, 914, 11, 309, 311], "temperature": 0.0, "avg_logprob": -0.1145148602398959, "compression_ratio": 1.5348837209302326, "no_speech_prob": 1.1657371942419559e-05}, {"id": 15, "seek": 8400, "start": 92.0, "end": 98.0, "text": " the concept is working correctly in terms of the code.", "tokens": [264, 3410, 307, 1364, 8944, 294, 2115, 295, 264, 3089, 13], "temperature": 0.0, "avg_logprob": -0.1145148602398959, "compression_ratio": 1.5348837209302326, "no_speech_prob": 1.1657371942419559e-05}, {"id": 16, "seek": 8400, "start": 98.0, "end": 104.0, "text": " We didn't get a better score, but I didn't particularly expect to either.", "tokens": [492, 994, 380, 483, 257, 1101, 6175, 11, 457, 286, 994, 380, 4098, 2066, 281, 2139, 13], "temperature": 0.0, "avg_logprob": -0.1145148602398959, "compression_ratio": 1.5348837209302326, "no_speech_prob": 1.1657371942419559e-05}, {"id": 17, "seek": 8400, "start": 104.0, "end": 111.0, "text": " You know, maybe after next Tuesday's lesson, we will revisit it because I actually think the main thing it might be useful for is", "tokens": [509, 458, 11, 1310, 934, 958, 10017, 311, 6898, 11, 321, 486, 32676, 309, 570, 286, 767, 519, 264, 2135, 551, 309, 1062, 312, 4420, 337, 307], "temperature": 0.0, "avg_logprob": -0.1145148602398959, "compression_ratio": 1.5348837209302326, "no_speech_prob": 1.1657371942419559e-05}, {"id": 18, "seek": 11100, "start": 111.0, "end": 117.0, "text": " what's called curriculum learning, which is basically focusing on the hard bits.", "tokens": [437, 311, 1219, 14302, 2539, 11, 597, 307, 1936, 8416, 322, 264, 1152, 9239, 13], "temperature": 0.0, "avg_logprob": -0.1315390836624872, "compression_ratio": 1.6851851851851851, "no_speech_prob": 7.47994490666315e-05}, {"id": 19, "seek": 11100, "start": 117.0, "end": 123.0, "text": " Looks like Nick's internet still isn't working, but Nick was saying the other day that he", "tokens": [10027, 411, 9449, 311, 4705, 920, 1943, 380, 1364, 11, 457, 9449, 390, 1566, 264, 661, 786, 300, 415], "temperature": 0.0, "avg_logprob": -0.1315390836624872, "compression_ratio": 1.6851851851851851, "no_speech_prob": 7.47994490666315e-05}, {"id": 20, "seek": 11100, "start": 123.0, "end": 128.0, "text": " looked at which ones we're having the errors on, which is like what we look at in the book.", "tokens": [2956, 412, 597, 2306, 321, 434, 1419, 264, 13603, 322, 11, 597, 307, 411, 437, 321, 574, 412, 294, 264, 1446, 13], "temperature": 0.0, "avg_logprob": -0.1315390836624872, "compression_ratio": 1.6851851851851851, "no_speech_prob": 7.47994490666315e-05}, {"id": 21, "seek": 11100, "start": 128.0, "end": 134.0, "text": " It's like looking at the classification interpretation and looking at like plot top losses and stuff.", "tokens": [467, 311, 411, 1237, 412, 264, 21538, 14174, 293, 1237, 412, 411, 7542, 1192, 15352, 293, 1507, 13], "temperature": 0.0, "avg_logprob": -0.1315390836624872, "compression_ratio": 1.6851851851851851, "no_speech_prob": 7.47994490666315e-05}, {"id": 22, "seek": 13400, "start": 134.0, "end": 141.0, "text": " And he said like, yeah, all the ones that we're making that we're getting wrong are basically from the same one or two classes.", "tokens": [400, 415, 848, 411, 11, 1338, 11, 439, 264, 2306, 300, 321, 434, 1455, 300, 321, 434, 1242, 2085, 366, 1936, 490, 264, 912, 472, 420, 732, 5359, 13], "temperature": 0.0, "avg_logprob": -0.1069384407751339, "compression_ratio": 1.6824034334763949, "no_speech_prob": 9.36737433221424e-06}, {"id": 23, "seek": 13400, "start": 141.0, "end": 148.0, "text": " So I haven't done much with curriculum learning in practice.", "tokens": [407, 286, 2378, 380, 1096, 709, 365, 14302, 2539, 294, 3124, 13], "temperature": 0.0, "avg_logprob": -0.1069384407751339, "compression_ratio": 1.6824034334763949, "no_speech_prob": 9.36737433221424e-06}, {"id": 24, "seek": 13400, "start": 148.0, "end": 156.0, "text": " Like I like all it means in theory is that we use our weighted data loader to weight the ones that we're getting wrong higher.", "tokens": [1743, 286, 411, 439, 309, 1355, 294, 5261, 307, 300, 321, 764, 527, 32807, 1412, 3677, 260, 281, 3364, 264, 2306, 300, 321, 434, 1242, 2085, 2946, 13], "temperature": 0.0, "avg_logprob": -0.1069384407751339, "compression_ratio": 1.6824034334763949, "no_speech_prob": 9.36737433221424e-06}, {"id": 25, "seek": 13400, "start": 156.0, "end": 162.0, "text": " And whether that will actually give us a better result or not, I'm not sure.", "tokens": [400, 1968, 300, 486, 767, 976, 505, 257, 1101, 1874, 420, 406, 11, 286, 478, 406, 988, 13], "temperature": 0.0, "avg_logprob": -0.1069384407751339, "compression_ratio": 1.6824034334763949, "no_speech_prob": 9.36737433221424e-06}, {"id": 26, "seek": 16200, "start": 162.0, "end": 173.0, "text": " But that I think that's more likely to be a useful path than simply reweighting things to be more balanced,", "tokens": [583, 300, 286, 519, 300, 311, 544, 3700, 281, 312, 257, 4420, 3100, 813, 2935, 319, 12329, 278, 721, 281, 312, 544, 13902, 11], "temperature": 0.0, "avg_logprob": -0.08063818124624399, "compression_ratio": 1.7025316455696202, "no_speech_prob": 1.5935014744172804e-05}, {"id": 27, "seek": 16200, "start": 173.0, "end": 189.0, "text": " because we don't want things to be more balanced because the ones that we observe the most often in the test set are actually the ones we want to be the best at.", "tokens": [570, 321, 500, 380, 528, 721, 281, 312, 544, 13902, 570, 264, 2306, 300, 321, 11441, 264, 881, 2049, 294, 264, 1500, 992, 366, 767, 264, 2306, 321, 528, 281, 312, 264, 1151, 412, 13], "temperature": 0.0, "avg_logprob": -0.08063818124624399, "compression_ratio": 1.7025316455696202, "no_speech_prob": 1.5935014744172804e-05}, {"id": 28, "seek": 18900, "start": 189.0, "end": 194.0, "text": " I will say I didn't check whether the distribution of the test set is the same as the training set.", "tokens": [286, 486, 584, 286, 994, 380, 1520, 1968, 264, 7316, 295, 264, 1500, 992, 307, 264, 912, 382, 264, 3097, 992, 13], "temperature": 0.0, "avg_logprob": -0.08980601174490792, "compression_ratio": 1.4938271604938271, "no_speech_prob": 1.473703196097631e-05}, {"id": 29, "seek": 18900, "start": 194.0, "end": 207.0, "text": " If it's randomly selected, then it will be and if it's not, then that would be a reason to use a weighted data loader as well.", "tokens": [759, 309, 311, 16979, 8209, 11, 550, 309, 486, 312, 293, 498, 309, 311, 406, 11, 550, 300, 576, 312, 257, 1778, 281, 764, 257, 32807, 1412, 3677, 260, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.08980601174490792, "compression_ratio": 1.4938271604938271, "no_speech_prob": 1.473703196097631e-05}, {"id": 30, "seek": 18900, "start": 207.0, "end": 214.0, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.08980601174490792, "compression_ratio": 1.4938271604938271, "no_speech_prob": 1.473703196097631e-05}, {"id": 31, "seek": 18900, "start": 214.0, "end": 216.0, "text": " Okay, so.", "tokens": [1033, 11, 370, 13], "temperature": 0.0, "avg_logprob": -0.08980601174490792, "compression_ratio": 1.4938271604938271, "no_speech_prob": 1.473703196097631e-05}, {"id": 32, "seek": 21600, "start": 216.0, "end": 226.0, "text": " What's the difference? Yeah, I guess like what's the is it is a curriculum learning kind of related to boosting and conceptually.", "tokens": [708, 311, 264, 2649, 30, 865, 11, 286, 2041, 411, 437, 311, 264, 307, 309, 307, 257, 14302, 2539, 733, 295, 4077, 281, 43117, 293, 3410, 671, 13], "temperature": 0.0, "avg_logprob": -0.1679523296845265, "compression_ratio": 1.625615763546798, "no_speech_prob": 5.141855945112184e-05}, {"id": 33, "seek": 21600, "start": 226.0, "end": 235.0, "text": " Not really. I mean, maybe. So boosting is where you calculate the difference between the actuals and your predictions to get residuals,", "tokens": [1726, 534, 13, 286, 914, 11, 1310, 13, 407, 43117, 307, 689, 291, 8873, 264, 2649, 1296, 264, 3539, 82, 293, 428, 21264, 281, 483, 27980, 82, 11], "temperature": 0.0, "avg_logprob": -0.1679523296845265, "compression_ratio": 1.625615763546798, "no_speech_prob": 5.141855945112184e-05}, {"id": 34, "seek": 21600, "start": 235.0, "end": 238.0, "text": " and then you create a model that tries to predict the residuals.", "tokens": [293, 550, 291, 1884, 257, 2316, 300, 9898, 281, 6069, 264, 27980, 82, 13], "temperature": 0.0, "avg_logprob": -0.1679523296845265, "compression_ratio": 1.625615763546798, "no_speech_prob": 5.141855945112184e-05}, {"id": 35, "seek": 23800, "start": 238.0, "end": 248.0, "text": " And then you can add those two predictions together, which is, if not done carefully as a recipe for overfitting.", "tokens": [400, 550, 291, 393, 909, 729, 732, 21264, 1214, 11, 597, 307, 11, 498, 406, 1096, 7500, 382, 257, 6782, 337, 670, 69, 2414, 13], "temperature": 0.0, "avg_logprob": -0.1114602279663086, "compression_ratio": 1.5879396984924623, "no_speech_prob": 5.862241323484341e-06}, {"id": 36, "seek": 23800, "start": 248.0, "end": 252.0, "text": " But if done carefully can be very effective.", "tokens": [583, 498, 1096, 7500, 393, 312, 588, 4942, 13], "temperature": 0.0, "avg_logprob": -0.1114602279663086, "compression_ratio": 1.5879396984924623, "no_speech_prob": 5.862241323484341e-06}, {"id": 37, "seek": 23800, "start": 252.0, "end": 260.0, "text": " Yeah, well, we're talking about something which is conceptually very different, which is saying like, oh, we're like really bad at recognizing this category.", "tokens": [865, 11, 731, 11, 321, 434, 1417, 466, 746, 597, 307, 3410, 671, 588, 819, 11, 597, 307, 1566, 411, 11, 1954, 11, 321, 434, 411, 534, 1578, 412, 18538, 341, 7719, 13], "temperature": 0.0, "avg_logprob": -0.1114602279663086, "compression_ratio": 1.5879396984924623, "no_speech_prob": 5.862241323484341e-06}, {"id": 38, "seek": 26000, "start": 260.0, "end": 268.0, "text": " So let's show that category more often during training.", "tokens": [407, 718, 311, 855, 300, 7719, 544, 2049, 1830, 3097, 13], "temperature": 0.0, "avg_logprob": -0.22232765139955463, "compression_ratio": 1.422680412371134, "no_speech_prob": 9.080117706616875e-06}, {"id": 39, "seek": 26000, "start": 268.0, "end": 278.0, "text": " Of kind of focusing on examples you're getting it wrong. More conceptually doing something similar.", "tokens": [2720, 733, 295, 8416, 322, 5110, 291, 434, 1242, 309, 2085, 13, 5048, 3410, 671, 884, 746, 2531, 13], "temperature": 0.0, "avg_logprob": -0.22232765139955463, "compression_ratio": 1.422680412371134, "no_speech_prob": 9.080117706616875e-06}, {"id": 40, "seek": 26000, "start": 278.0, "end": 285.0, "text": " I was just going to ask, are the labels ever wrong, like by accident, or intentionally in Kaggle? Of course, absolutely.", "tokens": [286, 390, 445, 516, 281, 1029, 11, 366, 264, 16949, 1562, 2085, 11, 411, 538, 6398, 11, 420, 22062, 294, 48751, 22631, 30, 2720, 1164, 11, 3122, 13], "temperature": 0.0, "avg_logprob": -0.22232765139955463, "compression_ratio": 1.422680412371134, "no_speech_prob": 9.080117706616875e-06}, {"id": 41, "seek": 28500, "start": 285.0, "end": 290.0, "text": " So both intentionally as well? No, not intentionally. I mean, not normally.", "tokens": [407, 1293, 22062, 382, 731, 30, 883, 11, 406, 22062, 13, 286, 914, 11, 406, 5646, 13], "temperature": 0.0, "avg_logprob": -0.11855543892959068, "compression_ratio": 1.7740863787375416, "no_speech_prob": 2.247220436402131e-05}, {"id": 42, "seek": 28500, "start": 290.0, "end": 302.0, "text": " Like sometimes there might be a competition where they say like, oh, this is a synthetically generated data set and some of the data is wrong because we're trying to do something like what happens in practice, but we can't share the real data.", "tokens": [1743, 2171, 456, 1062, 312, 257, 6211, 689, 436, 584, 411, 11, 1954, 11, 341, 307, 257, 10657, 22652, 10833, 1412, 992, 293, 512, 295, 264, 1412, 307, 2085, 570, 321, 434, 1382, 281, 360, 746, 411, 437, 2314, 294, 3124, 11, 457, 321, 393, 380, 2073, 264, 957, 1412, 13], "temperature": 0.0, "avg_logprob": -0.11855543892959068, "compression_ratio": 1.7740863787375416, "no_speech_prob": 2.247220436402131e-05}, {"id": 43, "seek": 28500, "start": 302.0, "end": 313.0, "text": " So is there any advantage in trying something like some uncertainty values from something like MC dropout, try to find like a threshold of things that are too difficult and then potentially they're wrongly labeled?", "tokens": [407, 307, 456, 604, 5002, 294, 1382, 746, 411, 512, 15697, 4190, 490, 746, 411, 8797, 3270, 346, 11, 853, 281, 915, 411, 257, 14678, 295, 721, 300, 366, 886, 2252, 293, 550, 7263, 436, 434, 2085, 356, 21335, 30], "temperature": 0.0, "avg_logprob": -0.11855543892959068, "compression_ratio": 1.7740863787375416, "no_speech_prob": 2.247220436402131e-05}, {"id": 44, "seek": 31300, "start": 313.0, "end": 328.0, "text": " I'm not sure you would need that. Like the thing we use in the book and the course is simply to find the things that we are confident of, but we know we're wrong, but turned out to be wrong and then just look at the pictures.", "tokens": [286, 478, 406, 988, 291, 576, 643, 300, 13, 1743, 264, 551, 321, 764, 294, 264, 1446, 293, 264, 1164, 307, 2935, 281, 915, 264, 721, 300, 321, 366, 6679, 295, 11, 457, 321, 458, 321, 434, 2085, 11, 457, 3574, 484, 281, 312, 2085, 293, 550, 445, 574, 412, 264, 5242, 13], "temperature": 0.0, "avg_logprob": -0.1505112405550682, "compression_ratio": 1.7433962264150944, "no_speech_prob": 2.2120624635135755e-05}, {"id": 45, "seek": 31300, "start": 328.0, "end": 332.0, "text": " So the raw software max value is enough, you think, to basically know whether or not it's true?", "tokens": [407, 264, 8936, 4722, 11469, 2158, 307, 1547, 11, 291, 519, 11, 281, 1936, 458, 1968, 420, 406, 309, 311, 2074, 30], "temperature": 0.0, "avg_logprob": -0.1505112405550682, "compression_ratio": 1.7433962264150944, "no_speech_prob": 2.2120624635135755e-05}, {"id": 46, "seek": 31300, "start": 332.0, "end": 339.0, "text": " I do, yeah. I mean, that seems to work pretty well. I mean, the only thing is you would need to be able to recognize these things in photos.", "tokens": [286, 360, 11, 1338, 13, 286, 914, 11, 300, 2544, 281, 589, 1238, 731, 13, 286, 914, 11, 264, 787, 551, 307, 291, 576, 643, 281, 312, 1075, 281, 5521, 613, 721, 294, 5787, 13], "temperature": 0.0, "avg_logprob": -0.1505112405550682, "compression_ratio": 1.7433962264150944, "no_speech_prob": 2.2120624635135755e-05}, {"id": 47, "seek": 33900, "start": 339.0, "end": 348.0, "text": " But I'm sure if you spent an hour reading on the internet about what these different diseases are and how they look, you would be able to pick it up fast enough.", "tokens": [583, 286, 478, 988, 498, 291, 4418, 364, 1773, 3760, 322, 264, 4705, 466, 437, 613, 819, 11044, 366, 293, 577, 436, 574, 11, 291, 576, 312, 1075, 281, 1888, 309, 493, 2370, 1547, 13], "temperature": 0.0, "avg_logprob": -0.12162318456740606, "compression_ratio": 1.59375, "no_speech_prob": 3.882903547491878e-05}, {"id": 48, "seek": 33900, "start": 348.0, "end": 354.0, "text": " And then, you know, just like we did in chapter two, if you're recognizing the things that aren't black and brown teddy bears.", "tokens": [400, 550, 11, 291, 458, 11, 445, 411, 321, 630, 294, 7187, 732, 11, 498, 291, 434, 18538, 264, 721, 300, 3212, 380, 2211, 293, 6292, 45116, 17276, 13], "temperature": 0.0, "avg_logprob": -0.12162318456740606, "compression_ratio": 1.59375, "no_speech_prob": 3.882903547491878e-05}, {"id": 49, "seek": 33900, "start": 354.0, "end": 363.0, "text": " Okay. But so plausibly even just knocking out some of the extremely difficult examples might get you higher on the leadable purely by virtue of them misleading the model.", "tokens": [1033, 13, 583, 370, 34946, 3545, 754, 445, 24085, 484, 512, 295, 264, 4664, 2252, 5110, 1062, 483, 291, 2946, 322, 264, 1477, 712, 17491, 538, 20816, 295, 552, 36429, 264, 2316, 13], "temperature": 0.0, "avg_logprob": -0.12162318456740606, "compression_ratio": 1.59375, "no_speech_prob": 3.882903547491878e-05}, {"id": 50, "seek": 36300, "start": 363.0, "end": 369.0, "text": " Not by knocking out the hard ones, but by knocking out the wrong ones. Yes.", "tokens": [1726, 538, 24085, 484, 264, 1152, 2306, 11, 457, 538, 24085, 484, 264, 2085, 2306, 13, 1079, 13], "temperature": 0.0, "avg_logprob": -0.10918827298321301, "compression_ratio": 1.7277227722772277, "no_speech_prob": 2.586527625680901e-05}, {"id": 51, "seek": 36300, "start": 369.0, "end": 383.0, "text": " Unless the test set is mislabeled consistently with the training set, in which case you would not want to knock them out because you would want to be able to correctly predict the things which people are incorrectly recognizing as the wrong disease.", "tokens": [16581, 264, 1500, 992, 307, 3346, 75, 18657, 292, 14961, 365, 264, 3097, 992, 11, 294, 597, 1389, 291, 576, 406, 528, 281, 6728, 552, 484, 570, 291, 576, 528, 281, 312, 1075, 281, 8944, 6069, 264, 721, 597, 561, 366, 42892, 18538, 382, 264, 2085, 4752, 13], "temperature": 0.0, "avg_logprob": -0.10918827298321301, "compression_ratio": 1.7277227722772277, "no_speech_prob": 2.586527625680901e-05}, {"id": 52, "seek": 36300, "start": 383.0, "end": 384.0, "text": " Something to try there.", "tokens": [6595, 281, 853, 456, 13], "temperature": 0.0, "avg_logprob": -0.10918827298321301, "compression_ratio": 1.7277227722772277, "no_speech_prob": 2.586527625680901e-05}, {"id": 53, "seek": 38400, "start": 384.0, "end": 393.0, "text": " Yeah. Yeah. So I would do exactly what we did in chapter two. You know, you can use exactly the same widget.", "tokens": [865, 13, 865, 13, 407, 286, 576, 360, 2293, 437, 321, 630, 294, 7187, 732, 13, 509, 458, 11, 291, 393, 764, 2293, 264, 912, 34047, 13], "temperature": 0.0, "avg_logprob": -0.1566812672565893, "compression_ratio": 1.5454545454545454, "no_speech_prob": 6.400296842912212e-05}, {"id": 54, "seek": 38400, "start": 393.0, "end": 401.0, "text": " But as I say, you'd have to probably spend an hour learning about rice disease, which would be a reasonably interesting thing to do anyway.", "tokens": [583, 382, 286, 584, 11, 291, 1116, 362, 281, 1391, 3496, 364, 1773, 2539, 466, 5090, 4752, 11, 597, 576, 312, 257, 23551, 1880, 551, 281, 360, 4033, 13], "temperature": 0.0, "avg_logprob": -0.1566812672565893, "compression_ratio": 1.5454545454545454, "no_speech_prob": 6.400296842912212e-05}, {"id": 55, "seek": 38400, "start": 401.0, "end": 403.0, "text": " I just saw a link.", "tokens": [286, 445, 1866, 257, 2113, 13], "temperature": 0.0, "avg_logprob": -0.1566812672565893, "compression_ratio": 1.5454545454545454, "no_speech_prob": 6.400296842912212e-05}, {"id": 56, "seek": 38400, "start": 403.0, "end": 406.0, "text": " There's a discussion in the petty.", "tokens": [821, 311, 257, 5017, 294, 264, 39334, 13], "temperature": 0.0, "avg_logprob": -0.1566812672565893, "compression_ratio": 1.5454545454545454, "no_speech_prob": 6.400296842912212e-05}, {"id": 57, "seek": 38400, "start": 406.0, "end": 412.0, "text": " Some people identify there's some mislabeling at least over 20 already.", "tokens": [2188, 561, 5876, 456, 311, 512, 3346, 44990, 11031, 412, 1935, 670, 945, 1217, 13], "temperature": 0.0, "avg_logprob": -0.1566812672565893, "compression_ratio": 1.5454545454545454, "no_speech_prob": 6.400296842912212e-05}, {"id": 58, "seek": 41200, "start": 412.0, "end": 415.0, "text": " Okay. Yes, so definitely happened.", "tokens": [1033, 13, 1079, 11, 370, 2138, 2011, 13], "temperature": 0.0, "avg_logprob": -0.21217096174085462, "compression_ratio": 1.4766355140186915, "no_speech_prob": 0.00014171608199831098}, {"id": 59, "seek": 41200, "start": 415.0, "end": 427.0, "text": " Says we have manually annotated every image with the help of agricultural experts, but there could be errors.", "tokens": [36780, 321, 362, 16945, 25339, 770, 633, 3256, 365, 264, 854, 295, 19587, 8572, 11, 457, 456, 727, 312, 13603, 13], "temperature": 0.0, "avg_logprob": -0.21217096174085462, "compression_ratio": 1.4766355140186915, "no_speech_prob": 0.00014171608199831098}, {"id": 60, "seek": 41200, "start": 427.0, "end": 430.0, "text": " Wow, this person knows more about rice than I do.", "tokens": [3153, 11, 341, 954, 3255, 544, 466, 5090, 813, 286, 360, 13], "temperature": 0.0, "avg_logprob": -0.21217096174085462, "compression_ratio": 1.4766355140186915, "no_speech_prob": 0.00014171608199831098}, {"id": 61, "seek": 41200, "start": 430.0, "end": 438.0, "text": " I think the images in the tongue grow have charsive issues his symptoms can easily be confused with potassium deficiency.", "tokens": [286, 519, 264, 5267, 294, 264, 10601, 1852, 362, 417, 685, 488, 2663, 702, 8332, 393, 3612, 312, 9019, 365, 29547, 37500, 13], "temperature": 0.0, "avg_logprob": -0.21217096174085462, "compression_ratio": 1.4766355140186915, "no_speech_prob": 0.00014171608199831098}, {"id": 62, "seek": 43800, "start": 438.0, "end": 446.0, "text": " Fair enough.", "tokens": [12157, 1547, 13], "temperature": 0.0, "avg_logprob": -0.15646035158181493, "compression_ratio": 1.6618357487922706, "no_speech_prob": 7.071516847645398e-06}, {"id": 63, "seek": 43800, "start": 446.0, "end": 454.0, "text": " Is that an example of what you're talking about where if if layman or sorry for semi expert gets confused then the labeling and the test sets probably the same.", "tokens": [1119, 300, 364, 1365, 295, 437, 291, 434, 1417, 466, 689, 498, 498, 2360, 1601, 420, 2597, 337, 12909, 5844, 2170, 9019, 550, 264, 40244, 293, 264, 1500, 6352, 1391, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.15646035158181493, "compression_ratio": 1.6618357487922706, "no_speech_prob": 7.071516847645398e-06}, {"id": 64, "seek": 43800, "start": 454.0, "end": 464.0, "text": " Yeah, exactly. So you're probably fixing these would probably screw up your model, because assuming that the test set was labeled used by the same people in the same way.", "tokens": [865, 11, 2293, 13, 407, 291, 434, 1391, 19442, 613, 576, 1391, 5630, 493, 428, 2316, 11, 570, 11926, 300, 264, 1500, 992, 390, 21335, 1143, 538, 264, 912, 561, 294, 264, 912, 636, 13], "temperature": 0.0, "avg_logprob": -0.15646035158181493, "compression_ratio": 1.6618357487922706, "no_speech_prob": 7.071516847645398e-06}, {"id": 65, "seek": 46400, "start": 464.0, "end": 475.0, "text": " Sometimes test sets. The test set is more of a gold standard they'll make more effort to talk to like a larger number of high quality experts and have them vote or something.", "tokens": [4803, 1500, 6352, 13, 440, 1500, 992, 307, 544, 295, 257, 3821, 3832, 436, 603, 652, 544, 4630, 281, 751, 281, 411, 257, 4833, 1230, 295, 1090, 3125, 8572, 293, 362, 552, 4740, 420, 746, 13], "temperature": 0.0, "avg_logprob": -0.08813845449023777, "compression_ratio": 1.5165876777251184, "no_speech_prob": 8.397634701395873e-06}, {"id": 66, "seek": 46400, "start": 475.0, "end": 484.0, "text": " Honestly, this competition seems like it doesn't even have any prize money attached so I'd like I think it's really low, low investment probably.", "tokens": [12348, 11, 341, 6211, 2544, 411, 309, 1177, 380, 754, 362, 604, 12818, 1460, 8570, 370, 286, 1116, 411, 286, 519, 309, 311, 534, 2295, 11, 2295, 6078, 1391, 13], "temperature": 0.0, "avg_logprob": -0.08813845449023777, "compression_ratio": 1.5165876777251184, "no_speech_prob": 8.397634701395873e-06}, {"id": 67, "seek": 48400, "start": 484.0, "end": 497.0, "text": " And so I doubt they did that, but, but that can happen. Yeah, that the test set could have, I mean, it makes sense to invest in getting really good labels for the test set.", "tokens": [400, 370, 286, 6385, 436, 630, 300, 11, 457, 11, 457, 300, 393, 1051, 13, 865, 11, 300, 264, 1500, 992, 727, 362, 11, 286, 914, 11, 309, 1669, 2020, 281, 1963, 294, 1242, 534, 665, 16949, 337, 264, 1500, 992, 13], "temperature": 0.0, "avg_logprob": -0.11242938995361328, "compression_ratio": 1.3893129770992367, "no_speech_prob": 1.5679237549193203e-05}, {"id": 68, "seek": 48400, "start": 497.0, "end": 504.0, "text": " Actually,", "tokens": [5135, 11], "temperature": 0.0, "avg_logprob": -0.11242938995361328, "compression_ratio": 1.3893129770992367, "no_speech_prob": 1.5679237549193203e-05}, {"id": 69, "seek": 50400, "start": 504.0, "end": 516.0, "text": " I was looking at one of the other competitions on unifes the x rays. I think there there was one, somebody had identified their wrist was wrongly labeled as a carrot one.", "tokens": [286, 390, 1237, 412, 472, 295, 264, 661, 26185, 322, 517, 351, 279, 264, 2031, 24417, 13, 286, 519, 456, 456, 390, 472, 11, 2618, 632, 9234, 641, 15043, 390, 2085, 356, 21335, 382, 257, 22767, 472, 13], "temperature": 0.0, "avg_logprob": -0.3035598331027561, "compression_ratio": 1.5, "no_speech_prob": 2.667639091669116e-05}, {"id": 70, "seek": 50400, "start": 516.0, "end": 530.0, "text": " It is a guess yeah it's not a, there's no money again but it's been running for a while. What's it called a unifes UNIFESP.", "tokens": [467, 307, 257, 2041, 1338, 309, 311, 406, 257, 11, 456, 311, 572, 1460, 797, 457, 309, 311, 668, 2614, 337, 257, 1339, 13, 708, 311, 309, 1219, 257, 517, 351, 279, 8229, 12775, 2358, 47, 13], "temperature": 0.0, "avg_logprob": -0.3035598331027561, "compression_ratio": 1.5, "no_speech_prob": 2.667639091669116e-05}, {"id": 71, "seek": 53000, "start": 530.0, "end": 538.0, "text": " Oh right it's another community competition I, yeah, gosh it's not very popular why is there only 74 teams.", "tokens": [876, 558, 309, 311, 1071, 1768, 6211, 286, 11, 1338, 11, 6502, 309, 311, 406, 588, 3743, 983, 307, 456, 787, 28868, 5491, 13], "temperature": 0.0, "avg_logprob": -0.2786110032279536, "compression_ratio": 1.3417721518987342, "no_speech_prob": 6.202514487085864e-05}, {"id": 72, "seek": 53000, "start": 538.0, "end": 545.0, "text": " Yeah sorry go ahead. I was just looking around and it looked interesting so I'm number 15 at the moment.", "tokens": [865, 2597, 352, 2286, 13, 286, 390, 445, 1237, 926, 293, 309, 2956, 1880, 370, 286, 478, 1230, 2119, 412, 264, 1623, 13], "temperature": 0.0, "avg_logprob": -0.2786110032279536, "compression_ratio": 1.3417721518987342, "no_speech_prob": 6.202514487085864e-05}, {"id": 73, "seek": 54500, "start": 545.0, "end": 563.0, "text": " It is a slightly weird one because well, it's, it's interesting because some of the x rays have multiple labels, but the labels are just concatenated. So there's interesting discussion on how you'd analyze that would you treat a combination", "tokens": [467, 307, 257, 4748, 3657, 472, 570, 731, 11, 309, 311, 11, 309, 311, 1880, 570, 512, 295, 264, 2031, 24417, 362, 3866, 16949, 11, 457, 264, 16949, 366, 445, 1588, 7186, 770, 13, 407, 456, 311, 1880, 5017, 322, 577, 291, 1116, 12477, 300, 576, 291, 2387, 257, 6562], "temperature": 0.0, "avg_logprob": -0.12442576443707501, "compression_ratio": 1.5, "no_speech_prob": 4.824881034437567e-05}, {"id": 74, "seek": 56300, "start": 563.0, "end": 576.0, "text": " as a distinct classification, whether it was like a neck and a chest or something or do you look at each of them individually and then try and label a multiple one from the different things so some.", "tokens": [382, 257, 10644, 21538, 11, 1968, 309, 390, 411, 257, 6189, 293, 257, 7443, 420, 746, 420, 360, 291, 574, 412, 1184, 295, 552, 16652, 293, 550, 853, 293, 7645, 257, 3866, 472, 490, 264, 819, 721, 370, 512, 13], "temperature": 0.0, "avg_logprob": -0.08032220060175116, "compression_ratio": 1.4452554744525548, "no_speech_prob": 5.4727242968510836e-05}, {"id": 75, "seek": 57600, "start": 576.0, "end": 593.0, "text": " Okay, so I'm just having a look at this competition so when does it close, it's a month to go but I don't know exactly when that is.", "tokens": [1033, 11, 370, 286, 478, 445, 1419, 257, 574, 412, 341, 6211, 370, 562, 775, 309, 1998, 11, 309, 311, 257, 1618, 281, 352, 457, 286, 500, 380, 458, 2293, 562, 300, 307, 13], "temperature": 0.0, "avg_logprob": -0.1846570090243691, "compression_ratio": 1.2110091743119267, "no_speech_prob": 7.527507932536537e-06}, {"id": 76, "seek": 59300, "start": 593.0, "end": 606.0, "text": " Normally there's July 31. Okay, where do you see that. When you go to like to the bottom of, like, on the overview and it says there's a whole timeline so then you just hover over that.", "tokens": [17424, 456, 311, 7370, 10353, 13, 1033, 11, 689, 360, 291, 536, 300, 13, 1133, 291, 352, 281, 411, 281, 264, 2767, 295, 11, 411, 11, 322, 264, 12492, 293, 309, 1619, 456, 311, 257, 1379, 12933, 370, 550, 291, 445, 20076, 670, 300, 13], "temperature": 0.0, "avg_logprob": -0.21255411726705145, "compression_ratio": 1.5560975609756098, "no_speech_prob": 9.972205589292571e-06}, {"id": 77, "seek": 59300, "start": 606.0, "end": 613.0, "text": " Oh my god I see it says close in a month but you actually have to get a tooltip by hovering. Okay, thanks to Nish.", "tokens": [876, 452, 3044, 286, 536, 309, 1619, 1998, 294, 257, 1618, 457, 291, 767, 362, 281, 483, 257, 2290, 83, 647, 538, 44923, 13, 1033, 11, 3231, 281, 426, 742, 13], "temperature": 0.0, "avg_logprob": -0.21255411726705145, "compression_ratio": 1.5560975609756098, "no_speech_prob": 9.972205589292571e-06}, {"id": 78, "seek": 59300, "start": 613.0, "end": 615.0, "text": " That's strange UX.", "tokens": [663, 311, 5861, 40176, 13], "temperature": 0.0, "avg_logprob": -0.21255411726705145, "compression_ratio": 1.5560975609756098, "no_speech_prob": 9.972205589292571e-06}, {"id": 79, "seek": 61500, "start": 615.0, "end": 626.0, "text": " Yeah, so we've actually got more than a month so maybe next week we could have a look at this one, because it would be a good opportunity to play around with medical image stuff because they're using DICOM, I think.", "tokens": [865, 11, 370, 321, 600, 767, 658, 544, 813, 257, 1618, 370, 1310, 958, 1243, 321, 727, 362, 257, 574, 412, 341, 472, 11, 570, 309, 576, 312, 257, 665, 2650, 281, 862, 926, 365, 4625, 3256, 1507, 570, 436, 434, 1228, 413, 2532, 5251, 11, 286, 519, 13], "temperature": 0.0, "avg_logprob": -0.13133303973139548, "compression_ratio": 1.541501976284585, "no_speech_prob": 2.4434614260826493e-06}, {"id": 80, "seek": 61500, "start": 626.0, "end": 639.0, "text": " Yeah, there's somebody is also which I used supplied a library of PNGs, which made it easier to use but I don't know what you lose in using that rather than the DICOM images.", "tokens": [865, 11, 456, 311, 2618, 307, 611, 597, 286, 1143, 27625, 257, 6405, 295, 430, 30237, 82, 11, 597, 1027, 309, 3571, 281, 764, 457, 286, 500, 380, 458, 437, 291, 3624, 294, 1228, 300, 2831, 813, 264, 413, 2532, 5251, 5267, 13], "temperature": 0.0, "avg_logprob": -0.13133303973139548, "compression_ratio": 1.541501976284585, "no_speech_prob": 2.4434614260826493e-06}, {"id": 81, "seek": 63900, "start": 639.0, "end": 647.0, "text": " It rather depends. So DICOM is a very generic file format that can contain lots of different things.", "tokens": [467, 2831, 5946, 13, 407, 413, 2532, 5251, 307, 257, 588, 19577, 3991, 7877, 300, 393, 5304, 3195, 295, 819, 721, 13], "temperature": 0.0, "avg_logprob": -0.15007754937926335, "compression_ratio": 1.4880952380952381, "no_speech_prob": 7.887384526839014e-06}, {"id": 82, "seek": 63900, "start": 647.0, "end": 658.0, "text": " One of the things DICOM contain is, is higher bit depth images than a PNG allows. So if they've, yes, they might, they might have gotten rid of that.", "tokens": [1485, 295, 264, 721, 413, 2532, 5251, 5304, 307, 11, 307, 2946, 857, 7161, 5267, 813, 257, 430, 30237, 4045, 13, 407, 498, 436, 600, 11, 2086, 11, 436, 1062, 11, 436, 1062, 362, 5768, 3973, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.15007754937926335, "compression_ratio": 1.4880952380952381, "no_speech_prob": 7.887384526839014e-06}, {"id": 83, "seek": 65800, "start": 658.0, "end": 674.0, "text": " Fastai has a nice medical imaging. It's pretty small but like has some useful stuff medical imaging library, which I think is fastai.vision.medical, which can handle DICOM directly.", "tokens": [15968, 1301, 575, 257, 1481, 4625, 25036, 13, 467, 311, 1238, 1359, 457, 411, 575, 512, 4420, 1507, 4625, 25036, 6405, 11, 597, 286, 519, 307, 2370, 1301, 13, 6763, 13, 1912, 804, 11, 597, 393, 4813, 413, 2532, 5251, 3838, 13], "temperature": 0.0, "avg_logprob": -0.124531772038708, "compression_ratio": 1.451086956521739, "no_speech_prob": 1.0613115591695532e-05}, {"id": 84, "seek": 65800, "start": 674.0, "end": 680.0, "text": " And I see there's a fastai entry as well.", "tokens": [400, 286, 536, 456, 311, 257, 2370, 1301, 8729, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.124531772038708, "compression_ratio": 1.451086956521739, "no_speech_prob": 1.0613115591695532e-05}, {"id": 85, "seek": 65800, "start": 680.0, "end": 687.0, "text": " That'd be fun we should try this next week.", "tokens": [663, 1116, 312, 1019, 321, 820, 853, 341, 958, 1243, 13], "temperature": 0.0, "avg_logprob": -0.124531772038708, "compression_ratio": 1.451086956521739, "no_speech_prob": 1.0613115591695532e-05}, {"id": 86, "seek": 68700, "start": 687.0, "end": 692.0, "text": " I see there's a PNGs.", "tokens": [286, 536, 456, 311, 257, 430, 30237, 82, 13], "temperature": 0.0, "avg_logprob": -0.14617970649232256, "compression_ratio": 1.4133333333333333, "no_speech_prob": 4.466074096853845e-05}, {"id": 87, "seek": 68700, "start": 692.0, "end": 701.0, "text": " I think the DICOMs come to about 27 gigabytes. Oh my god. Okay. So the, the PNG was quite attractive from that point. Yeah.", "tokens": [286, 519, 264, 413, 2532, 5251, 82, 808, 281, 466, 7634, 42741, 13, 876, 452, 3044, 13, 1033, 13, 407, 264, 11, 264, 430, 30237, 390, 1596, 12609, 490, 300, 935, 13, 865, 13], "temperature": 0.0, "avg_logprob": -0.14617970649232256, "compression_ratio": 1.4133333333333333, "no_speech_prob": 4.466074096853845e-05}, {"id": 88, "seek": 68700, "start": 701.0, "end": 709.0, "text": " So one thing that you can do with DICOM is to compress them, particularly using JPEG 2000 which is a really good compression.", "tokens": [407, 472, 551, 300, 291, 393, 360, 365, 413, 2532, 5251, 307, 281, 14778, 552, 11, 4098, 1228, 508, 5208, 38, 8132, 597, 307, 257, 534, 665, 19355, 13], "temperature": 0.0, "avg_logprob": -0.14617970649232256, "compression_ratio": 1.4133333333333333, "no_speech_prob": 4.466074096853845e-05}, {"id": 89, "seek": 68700, "start": 709.0, "end": 713.0, "text": " But yeah, people often don't, for some reason.", "tokens": [583, 1338, 11, 561, 2049, 500, 380, 11, 337, 512, 1778, 13], "temperature": 0.0, "avg_logprob": -0.14617970649232256, "compression_ratio": 1.4133333333333333, "no_speech_prob": 4.466074096853845e-05}, {"id": 90, "seek": 71300, "start": 713.0, "end": 722.0, "text": " So probably the first thing I'd look at in that competition is to see, look at DICOM and see is it storing 16 bit data or not.", "tokens": [407, 1391, 264, 700, 551, 286, 1116, 574, 412, 294, 300, 6211, 307, 281, 536, 11, 574, 412, 413, 2532, 5251, 293, 536, 307, 309, 26085, 3165, 857, 1412, 420, 406, 13], "temperature": 0.0, "avg_logprob": -0.09011297861735026, "compression_ratio": 1.435, "no_speech_prob": 1.2212885849294253e-05}, {"id": 91, "seek": 71300, "start": 722.0, "end": 733.0, "text": " And if it is I would try to find a way to resave that without losing the extra information,", "tokens": [400, 498, 309, 307, 286, 576, 853, 281, 915, 257, 636, 281, 725, 946, 300, 1553, 7027, 264, 2857, 1589, 11], "temperature": 0.0, "avg_logprob": -0.09011297861735026, "compression_ratio": 1.435, "no_speech_prob": 1.2212885849294253e-05}, {"id": 92, "seek": 71300, "start": 733.0, "end": 739.0, "text": " which I think we've got examples of in our medical imaging tutorial.", "tokens": [597, 286, 519, 321, 600, 658, 5110, 295, 294, 527, 4625, 25036, 7073, 13], "temperature": 0.0, "avg_logprob": -0.09011297861735026, "compression_ratio": 1.435, "no_speech_prob": 1.2212885849294253e-05}, {"id": 93, "seek": 73900, "start": 739.0, "end": 743.0, "text": " So we can look at that.", "tokens": [407, 321, 393, 574, 412, 300, 13], "temperature": 0.0, "avg_logprob": -0.19125676709552145, "compression_ratio": 1.4545454545454546, "no_speech_prob": 3.0667742976220325e-05}, {"id": 94, "seek": 73900, "start": 743.0, "end": 746.0, "text": " All right, I'm going to share my screen.", "tokens": [1057, 558, 11, 286, 478, 516, 281, 2073, 452, 2568, 13], "temperature": 0.0, "avg_logprob": -0.19125676709552145, "compression_ratio": 1.4545454545454546, "no_speech_prob": 3.0667742976220325e-05}, {"id": 95, "seek": 73900, "start": 746.0, "end": 748.0, "text": " Even though I don't know what I'm doing.", "tokens": [2754, 1673, 286, 500, 380, 458, 437, 286, 478, 884, 13], "temperature": 0.0, "avg_logprob": -0.19125676709552145, "compression_ratio": 1.4545454545454546, "no_speech_prob": 3.0667742976220325e-05}, {"id": 96, "seek": 73900, "start": 748.0, "end": 759.0, "text": " I'm going to have to drop in a few minutes but I'll catch the rest on the record. Thanks for this. Nice to see you.", "tokens": [286, 478, 516, 281, 362, 281, 3270, 294, 257, 1326, 2077, 457, 286, 603, 3745, 264, 1472, 322, 264, 2136, 13, 2561, 337, 341, 13, 5490, 281, 536, 291, 13], "temperature": 0.0, "avg_logprob": -0.19125676709552145, "compression_ratio": 1.4545454545454546, "no_speech_prob": 3.0667742976220325e-05}, {"id": 97, "seek": 73900, "start": 759.0, "end": 762.0, "text": " By the way I was looking at this.", "tokens": [3146, 264, 636, 286, 390, 1237, 412, 341, 13], "temperature": 0.0, "avg_logprob": -0.19125676709552145, "compression_ratio": 1.4545454545454546, "no_speech_prob": 3.0667742976220325e-05}, {"id": 98, "seek": 73900, "start": 762.0, "end": 766.0, "text": " Conf next paper.", "tokens": [11701, 958, 3035, 13], "temperature": 0.0, "avg_logprob": -0.19125676709552145, "compression_ratio": 1.4545454545454546, "no_speech_prob": 3.0667742976220325e-05}, {"id": 99, "seek": 76600, "start": 766.0, "end": 769.0, "text": " Gosh, everybody.", "tokens": [19185, 11, 2201, 13], "temperature": 0.0, "avg_logprob": -0.23786604404449463, "compression_ratio": 1.6329787234042554, "no_speech_prob": 3.533954804879613e-05}, {"id": 100, "seek": 76600, "start": 769.0, "end": 772.0, "text": " Congratulates transformers on everything.", "tokens": [4280, 4481, 26192, 4088, 433, 322, 1203, 13], "temperature": 0.0, "avg_logprob": -0.23786604404449463, "compression_ratio": 1.6329787234042554, "no_speech_prob": 3.533954804879613e-05}, {"id": 101, "seek": 76600, "start": 772.0, "end": 777.0, "text": " Vision Transformers bring new ideas like the Adam W optimizer.", "tokens": [25170, 27938, 433, 1565, 777, 3487, 411, 264, 7938, 343, 5028, 6545, 13], "temperature": 0.0, "avg_logprob": -0.23786604404449463, "compression_ratio": 1.6329787234042554, "no_speech_prob": 3.533954804879613e-05}, {"id": 102, "seek": 76600, "start": 777.0, "end": 782.0, "text": " I guess who actually wrote the first thing saying we should always use the Adam W optimizer.", "tokens": [286, 2041, 567, 767, 4114, 264, 700, 551, 1566, 321, 820, 1009, 764, 264, 7938, 343, 5028, 6545, 13], "temperature": 0.0, "avg_logprob": -0.23786604404449463, "compression_ratio": 1.6329787234042554, "no_speech_prob": 3.533954804879613e-05}, {"id": 103, "seek": 76600, "start": 782.0, "end": 790.0, "text": " I would be silver in fast AI. I think that was years before the vision transformers.", "tokens": [286, 576, 312, 8753, 294, 2370, 7318, 13, 286, 519, 300, 390, 924, 949, 264, 5201, 4088, 433, 13], "temperature": 0.0, "avg_logprob": -0.23786604404449463, "compression_ratio": 1.6329787234042554, "no_speech_prob": 3.533954804879613e-05}, {"id": 104, "seek": 76600, "start": 790.0, "end": 792.0, "text": " Adam W.", "tokens": [7938, 343, 13], "temperature": 0.0, "avg_logprob": -0.23786604404449463, "compression_ratio": 1.6329787234042554, "no_speech_prob": 3.533954804879613e-05}, {"id": 105, "seek": 79200, "start": 792.0, "end": 800.0, "text": " I read that paper last night.", "tokens": [286, 1401, 300, 3035, 1036, 1818, 13], "temperature": 0.0, "avg_logprob": -0.2762043796368499, "compression_ratio": 1.5297297297297296, "no_speech_prob": 2.586115624580998e-05}, {"id": 106, "seek": 79200, "start": 800.0, "end": 805.0, "text": " I was thinking they talk about how all of these things were already there.", "tokens": [286, 390, 1953, 436, 751, 466, 577, 439, 295, 613, 721, 645, 1217, 456, 13], "temperature": 0.0, "avg_logprob": -0.2762043796368499, "compression_ratio": 1.5297297297297296, "no_speech_prob": 2.586115624580998e-05}, {"id": 107, "seek": 79200, "start": 805.0, "end": 812.0, "text": " They just rediscovered them like slightly larger kernel size and things like that.", "tokens": [814, 445, 2182, 40080, 292, 552, 411, 4748, 4833, 28256, 2744, 293, 721, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.2762043796368499, "compression_ratio": 1.5297297297297296, "no_speech_prob": 2.586115624580998e-05}, {"id": 108, "seek": 79200, "start": 812.0, "end": 819.0, "text": " Which begs the question why hasn't no one just done experiments to tweak these things together.", "tokens": [3013, 4612, 82, 264, 1168, 983, 6132, 380, 572, 472, 445, 1096, 12050, 281, 29879, 613, 721, 1214, 13], "temperature": 0.0, "avg_logprob": -0.2762043796368499, "compression_ratio": 1.5297297297297296, "no_speech_prob": 2.586115624580998e-05}, {"id": 109, "seek": 81900, "start": 819.0, "end": 822.0, "text": " I mean it's.", "tokens": [286, 914, 309, 311, 13], "temperature": 0.0, "avg_logprob": -0.1671111366965554, "compression_ratio": 1.6637554585152838, "no_speech_prob": 9.022454469231889e-05}, {"id": 110, "seek": 81900, "start": 822.0, "end": 828.0, "text": " Nobody takes any notice because they're not written in PDFs, you know.", "tokens": [9297, 2516, 604, 3449, 570, 436, 434, 406, 3720, 294, 17752, 82, 11, 291, 458, 13], "temperature": 0.0, "avg_logprob": -0.1671111366965554, "compression_ratio": 1.6637554585152838, "no_speech_prob": 9.022454469231889e-05}, {"id": 111, "seek": 81900, "start": 828.0, "end": 830.0, "text": " Is it.", "tokens": [1119, 309, 13], "temperature": 0.0, "avg_logprob": -0.1671111366965554, "compression_ratio": 1.6637554585152838, "no_speech_prob": 9.022454469231889e-05}, {"id": 112, "seek": 81900, "start": 830.0, "end": 837.0, "text": " I mean, these benchmarks they're like, the thing is that like a lot of researchers aren't good practitioners.", "tokens": [286, 914, 11, 613, 43751, 436, 434, 411, 11, 264, 551, 307, 300, 411, 257, 688, 295, 10309, 3212, 380, 665, 25742, 13], "temperature": 0.0, "avg_logprob": -0.1671111366965554, "compression_ratio": 1.6637554585152838, "no_speech_prob": 9.022454469231889e-05}, {"id": 113, "seek": 81900, "start": 837.0, "end": 847.0, "text": " So they just, they're not very good at training accurate networks and they don't know these tricks, you know, and they don't hang out on Kaggle and learn about what actually works.", "tokens": [407, 436, 445, 11, 436, 434, 406, 588, 665, 412, 3097, 8559, 9590, 293, 436, 500, 380, 458, 613, 11733, 11, 291, 458, 11, 293, 436, 500, 380, 3967, 484, 322, 48751, 22631, 293, 1466, 466, 437, 767, 1985, 13], "temperature": 0.0, "avg_logprob": -0.1671111366965554, "compression_ratio": 1.6637554585152838, "no_speech_prob": 9.022454469231889e-05}, {"id": 114, "seek": 84700, "start": 847.0, "end": 867.0, "text": " But then the thing is like, it's not always easy to publish, like even if you did stick it into a PDF and submit it to Europe's there's no particular high likelihood that they're going to accept it because the field research wise is very focused on theory results and you know things with lots of Greek letters in them.", "tokens": [583, 550, 264, 551, 307, 411, 11, 309, 311, 406, 1009, 1858, 281, 11374, 11, 411, 754, 498, 291, 630, 2897, 309, 666, 257, 17752, 293, 10315, 309, 281, 3315, 311, 456, 311, 572, 1729, 1090, 22119, 300, 436, 434, 516, 281, 3241, 309, 570, 264, 2519, 2132, 10829, 307, 588, 5178, 322, 5261, 3542, 293, 291, 458, 721, 365, 3195, 295, 10281, 7825, 294, 552, 13], "temperature": 0.0, "avg_logprob": -0.11910245895385742, "compression_ratio": 1.650735294117647, "no_speech_prob": 1.16575984066003e-05}, {"id": 115, "seek": 84700, "start": 867.0, "end": 873.0, "text": " Does that mean that the part of the problem is that the data sets the benchmarks are just too inaccessible to the average person.", "tokens": [4402, 300, 914, 300, 264, 644, 295, 264, 1154, 307, 300, 264, 1412, 6352, 264, 43751, 366, 445, 886, 33230, 780, 964, 281, 264, 4274, 954, 13], "temperature": 0.0, "avg_logprob": -0.11910245895385742, "compression_ratio": 1.650735294117647, "no_speech_prob": 1.16575984066003e-05}, {"id": 116, "seek": 87300, "start": 873.0, "end": 891.0, "text": " So, no, I wouldn't say that for ImageNet 1K. No, I wouldn't say that the issue is I think the culture of research is not particularly interested in experimental results, you know.", "tokens": [407, 11, 572, 11, 286, 2759, 380, 584, 300, 337, 29903, 31890, 502, 42, 13, 883, 11, 286, 2759, 380, 584, 300, 264, 2734, 307, 286, 519, 264, 3713, 295, 2132, 307, 406, 4098, 3102, 294, 17069, 3542, 11, 291, 458, 13], "temperature": 0.0, "avg_logprob": -0.1909507875857146, "compression_ratio": 1.376923076923077, "no_speech_prob": 8.478883682982996e-05}, {"id": 117, "seek": 89100, "start": 891.0, "end": 907.0, "text": " But in a limited experience, I will say it's very hard to find the viewer as well, especially you have a very strong domain, not just one thing or the sample data set you can find in open source.", "tokens": [583, 294, 257, 5567, 1752, 11, 286, 486, 584, 309, 311, 588, 1152, 281, 915, 264, 16767, 382, 731, 11, 2318, 291, 362, 257, 588, 2068, 9274, 11, 406, 445, 472, 551, 420, 264, 6889, 1412, 992, 291, 393, 915, 294, 1269, 4009, 13], "temperature": 0.0, "avg_logprob": -0.23647634188334146, "compression_ratio": 1.3732394366197183, "no_speech_prob": 0.0005516979726962745}, {"id": 118, "seek": 90700, "start": 907.0, "end": 921.0, "text": " But you have a very strong domain and then a lot of peer reviewers, they're just not picking up to review it, even if we pay for the reviewer we're using, so people can get it for free.", "tokens": [583, 291, 362, 257, 588, 2068, 9274, 293, 550, 257, 688, 295, 15108, 45837, 11, 436, 434, 445, 406, 8867, 493, 281, 3131, 309, 11, 754, 498, 321, 1689, 337, 264, 3131, 260, 321, 434, 1228, 11, 370, 561, 393, 483, 309, 337, 1737, 13], "temperature": 0.0, "avg_logprob": -0.34157528952946736, "compression_ratio": 1.5126582278481013, "no_speech_prob": 0.0006442715530283749}, {"id": 119, "seek": 90700, "start": 921.0, "end": 928.0, "text": " And it takes us three months just to find the viewer.", "tokens": [400, 309, 2516, 505, 1045, 2493, 445, 281, 915, 264, 16767, 13], "temperature": 0.0, "avg_logprob": -0.34157528952946736, "compression_ratio": 1.5126582278481013, "no_speech_prob": 0.0006442715530283749}, {"id": 120, "seek": 92800, "start": 928.0, "end": 930.0, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.3092401677911932, "compression_ratio": 1.1379310344827587, "no_speech_prob": 0.00010872670827666298}, {"id": 121, "seek": 92800, "start": 930.0, "end": 941.0, "text": " Topic of papers, when do you know when a paper is worth reading, given the situation.", "tokens": [8840, 299, 295, 10577, 11, 562, 360, 291, 458, 562, 257, 3035, 307, 3163, 3760, 11, 2212, 264, 2590, 13], "temperature": 0.0, "avg_logprob": -0.3092401677911932, "compression_ratio": 1.1379310344827587, "no_speech_prob": 0.00010872670827666298}, {"id": 122, "seek": 92800, "start": 941.0, "end": 944.0, "text": " I mean,", "tokens": [286, 914, 11], "temperature": 0.0, "avg_logprob": -0.3092401677911932, "compression_ratio": 1.1379310344827587, "no_speech_prob": 0.00010872670827666298}, {"id": 123, "seek": 94400, "start": 944.0, "end": 963.0, "text": " I don't like until, I mean, I'm very fond of papers that describe things which did very well in an actual competition, you know, that then we know this is something that actually predicts things accurately.", "tokens": [286, 500, 380, 411, 1826, 11, 286, 914, 11, 286, 478, 588, 9557, 295, 10577, 300, 6786, 721, 597, 630, 588, 731, 294, 364, 3539, 6211, 11, 291, 458, 11, 300, 550, 321, 458, 341, 307, 746, 300, 767, 6069, 82, 721, 20095, 13], "temperature": 0.0, "avg_logprob": -0.13430880506833395, "compression_ratio": 1.4507042253521127, "no_speech_prob": 3.7617774069076404e-05}, {"id": 124, "seek": 96300, "start": 963.0, "end": 978.0, "text": " And they can get similar results if they've got a good, you know, just table of results. So generally speaking I like things that actually have good results, particularly if they show like how long it took to train and how much data they trained on.", "tokens": [400, 436, 393, 483, 2531, 3542, 498, 436, 600, 658, 257, 665, 11, 291, 458, 11, 445, 3199, 295, 3542, 13, 407, 5101, 4124, 286, 411, 721, 300, 767, 362, 665, 3542, 11, 4098, 498, 436, 855, 411, 577, 938, 309, 1890, 281, 3847, 293, 577, 709, 1412, 436, 8895, 322, 13], "temperature": 0.0, "avg_logprob": -0.12103164763677687, "compression_ratio": 1.6757990867579908, "no_speech_prob": 4.425051884027198e-06}, {"id": 125, "seek": 96300, "start": 978.0, "end": 988.0, "text": " And, yeah, so are they getting good results, using less data and less time than you might expect from the same thing.", "tokens": [400, 11, 1338, 11, 370, 366, 436, 1242, 665, 3542, 11, 1228, 1570, 1412, 293, 1570, 565, 813, 291, 1062, 2066, 490, 264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.12103164763677687, "compression_ratio": 1.6757990867579908, "no_speech_prob": 4.425051884027198e-06}, {"id": 126, "seek": 98800, "start": 988.0, "end": 1000.0, "text": " And yeah I certainly wouldn't focus only on those that get good results on really big data sets that's not necessarily more interesting. I'm very interested in things that show good results using transfer learning.", "tokens": [400, 1338, 286, 3297, 2759, 380, 1879, 787, 322, 729, 300, 483, 665, 3542, 322, 534, 955, 1412, 6352, 300, 311, 406, 4725, 544, 1880, 13, 286, 478, 588, 3102, 294, 721, 300, 855, 665, 3542, 1228, 5003, 2539, 13], "temperature": 0.0, "avg_logprob": -0.09974757327309138, "compression_ratio": 1.8058252427184467, "no_speech_prob": 4.710378561867401e-06}, {"id": 127, "seek": 98800, "start": 1000.0, "end": 1009.0, "text": " So look for things that are like practically useful, I don't train that much from random. So I'm very interested in things that do well in transfer learning.", "tokens": [407, 574, 337, 721, 300, 366, 411, 15667, 4420, 11, 286, 500, 380, 3847, 300, 709, 490, 4974, 13, 407, 286, 478, 588, 3102, 294, 721, 300, 360, 731, 294, 5003, 2539, 13], "temperature": 0.0, "avg_logprob": -0.09974757327309138, "compression_ratio": 1.8058252427184467, "no_speech_prob": 4.710378561867401e-06}, {"id": 128, "seek": 100900, "start": 1009.0, "end": 1022.0, "text": " Also like, look for people who you've liked their work before, you know, and, and in particular that doesn't mean like always reading the latest papers.", "tokens": [2743, 411, 11, 574, 337, 561, 567, 291, 600, 4501, 641, 589, 949, 11, 291, 458, 11, 293, 11, 293, 294, 1729, 300, 1177, 380, 914, 411, 1009, 3760, 264, 6792, 10577, 13], "temperature": 0.0, "avg_logprob": -0.10938300869681618, "compression_ratio": 1.6561085972850678, "no_speech_prob": 9.663674973126035e-06}, {"id": 129, "seek": 100900, "start": 1022.0, "end": 1027.0, "text": " You know if you come across a paper from somebody that you find useful.", "tokens": [509, 458, 498, 291, 808, 2108, 257, 3035, 490, 2618, 300, 291, 915, 4420, 13], "temperature": 0.0, "avg_logprob": -0.10938300869681618, "compression_ratio": 1.6561085972850678, "no_speech_prob": 9.663674973126035e-06}, {"id": 130, "seek": 100900, "start": 1027.0, "end": 1032.0, "text": " Go back and look at their Google Scholar and look read the older papers.", "tokens": [1037, 646, 293, 574, 412, 641, 3329, 2065, 15276, 293, 574, 1401, 264, 4906, 10577, 13], "temperature": 0.0, "avg_logprob": -0.10938300869681618, "compression_ratio": 1.6561085972850678, "no_speech_prob": 9.663674973126035e-06}, {"id": 131, "seek": 100900, "start": 1032.0, "end": 1037.0, "text": " See who they collaborate with and read their papers. So for example,", "tokens": [3008, 567, 436, 18338, 365, 293, 1401, 641, 10577, 13, 407, 337, 1365, 11], "temperature": 0.0, "avg_logprob": -0.10938300869681618, "compression_ratio": 1.6561085972850678, "no_speech_prob": 9.663674973126035e-06}, {"id": 132, "seek": 103700, "start": 1037.0, "end": 1041.0, "text": " I really like Kwok Lee and Google Brain.", "tokens": [286, 534, 411, 43432, 453, 6957, 293, 3329, 29783, 13], "temperature": 0.0, "avg_logprob": -0.17548531073111076, "compression_ratio": 1.3819444444444444, "no_speech_prob": 1.4734227988810744e-05}, {"id": 133, "seek": 103700, "start": 1041.0, "end": 1054.0, "text": " His, him and his team do a lot of good work it tends to be, you know, very practical and high quality results and so we know when his team releases a paper I.", "tokens": [2812, 11, 796, 293, 702, 1469, 360, 257, 688, 295, 665, 589, 309, 12258, 281, 312, 11, 291, 458, 11, 588, 8496, 293, 1090, 3125, 3542, 293, 370, 321, 458, 562, 702, 1469, 16952, 257, 3035, 286, 13], "temperature": 0.0, "avg_logprob": -0.17548531073111076, "compression_ratio": 1.3819444444444444, "no_speech_prob": 1.4734227988810744e-05}, {"id": 134, "seek": 105400, "start": 1054.0, "end": 1070.0, "text": " I also know like he seems to have similar interests to mine like he tends to do stuff involving transfer learning and getting good results in less epochs and stuff like that. So, if I see he's got a new paper out I'm pretty likely to read it.", "tokens": [286, 611, 458, 411, 415, 2544, 281, 362, 2531, 8847, 281, 3892, 411, 415, 12258, 281, 360, 1507, 17030, 5003, 2539, 293, 1242, 665, 3542, 294, 1570, 30992, 28346, 293, 1507, 411, 300, 13, 407, 11, 498, 286, 536, 415, 311, 658, 257, 777, 3035, 484, 286, 478, 1238, 3700, 281, 1401, 309, 13], "temperature": 0.0, "avg_logprob": -0.1205164542564979, "compression_ratio": 1.5086705202312138, "no_speech_prob": 1.3203482922108378e-05}, {"id": 135, "seek": 105400, "start": 1070.0, "end": 1072.0, "text": " I have a question.", "tokens": [286, 362, 257, 1168, 13], "temperature": 0.0, "avg_logprob": -0.1205164542564979, "compression_ratio": 1.5086705202312138, "no_speech_prob": 1.3203482922108378e-05}, {"id": 136, "seek": 107200, "start": 1072.0, "end": 1093.0, "text": " I mean, I mean, for for the the carrier competitions and like, like in a lab type of environment is, I mean, when to the question that I have is when to stop iterating on on a model on a model that you have is, is, I have the.", "tokens": [286, 914, 11, 286, 914, 11, 337, 337, 264, 264, 17574, 26185, 293, 411, 11, 411, 294, 257, 2715, 2010, 295, 2823, 307, 11, 286, 914, 11, 562, 281, 264, 1168, 300, 286, 362, 307, 562, 281, 1590, 17138, 990, 322, 322, 257, 2316, 322, 257, 2316, 300, 291, 362, 307, 11, 307, 11, 286, 362, 264, 13], "temperature": 0.0, "avg_logprob": -0.25508726796796244, "compression_ratio": 1.6496350364963503, "no_speech_prob": 3.368732723174617e-05}, {"id": 137, "seek": 109300, "start": 1093.0, "end": 1103.0, "text": " I mean, when is enough enough to do the training on the data that you have. When is enough.", "tokens": [286, 914, 11, 562, 307, 1547, 1547, 281, 360, 264, 3097, 322, 264, 1412, 300, 291, 362, 13, 1133, 307, 1547, 13], "temperature": 0.0, "avg_logprob": -0.12137332558631897, "compression_ratio": 1.5333333333333334, "no_speech_prob": 4.7532714233966544e-05}, {"id": 138, "seek": 109300, "start": 1103.0, "end": 1105.0, "text": " So that question.", "tokens": [407, 300, 1168, 13], "temperature": 0.0, "avg_logprob": -0.12137332558631897, "compression_ratio": 1.5333333333333334, "no_speech_prob": 4.7532714233966544e-05}, {"id": 139, "seek": 109300, "start": 1105.0, "end": 1110.0, "text": " I mean,", "tokens": [286, 914, 11], "temperature": 0.0, "avg_logprob": -0.12137332558631897, "compression_ratio": 1.5333333333333334, "no_speech_prob": 4.7532714233966544e-05}, {"id": 140, "seek": 109300, "start": 1110.0, "end": 1119.0, "text": " there's some reason you're doing this work right so like you hopefully know when it does what you want it to do.", "tokens": [456, 311, 512, 1778, 291, 434, 884, 341, 589, 558, 370, 411, 291, 4696, 458, 562, 309, 775, 437, 291, 528, 309, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.12137332558631897, "compression_ratio": 1.5333333333333334, "no_speech_prob": 4.7532714233966544e-05}, {"id": 141, "seek": 111900, "start": 1119.0, "end": 1130.0, "text": " And the thing that happens, all that happens, especially to me all the time is that he trained the model, and it works perfectly fine on the lab.", "tokens": [400, 264, 551, 300, 2314, 11, 439, 300, 2314, 11, 2318, 281, 385, 439, 264, 565, 307, 300, 415, 8895, 264, 2316, 11, 293, 309, 1985, 6239, 2489, 322, 264, 2715, 13], "temperature": 0.0, "avg_logprob": -0.1537988152302487, "compression_ratio": 1.6091954022988506, "no_speech_prob": 2.2467145754490048e-05}, {"id": 142, "seek": 111900, "start": 1130.0, "end": 1138.0, "text": " When we're doing it and then as soon as we throw a couple of images that they are not part of the set. I mean that goes nuts and okay.", "tokens": [1133, 321, 434, 884, 309, 293, 550, 382, 2321, 382, 321, 3507, 257, 1916, 295, 5267, 300, 436, 366, 406, 644, 295, 264, 992, 13, 286, 914, 300, 1709, 10483, 293, 1392, 13], "temperature": 0.0, "avg_logprob": -0.1537988152302487, "compression_ratio": 1.6091954022988506, "no_speech_prob": 2.2467145754490048e-05}, {"id": 143, "seek": 113800, "start": 1138.0, "end": 1157.0, "text": " Because, like, or more light, or the temperature or stuff like that. So that's a different problem right so that that means your problem is that you're, you're not using", "tokens": [1436, 11, 411, 11, 420, 544, 1442, 11, 420, 264, 4292, 420, 1507, 411, 300, 13, 407, 300, 311, 257, 819, 1154, 558, 370, 300, 300, 1355, 428, 1154, 307, 300, 291, 434, 11, 291, 434, 406, 1228], "temperature": 0.0, "avg_logprob": -0.20797007424490793, "compression_ratio": 1.536231884057971, "no_speech_prob": 1.2481549447329598e-06}, {"id": 144, "seek": 113800, "start": 1157.0, "end": 1161.0, "text": " the, you know, the right data to train on.", "tokens": [264, 11, 291, 458, 11, 264, 558, 1412, 281, 3847, 322, 13], "temperature": 0.0, "avg_logprob": -0.20797007424490793, "compression_ratio": 1.536231884057971, "no_speech_prob": 1.2481549447329598e-06}, {"id": 145, "seek": 116100, "start": 1161.0, "end": 1165.0, "text": " So like you need to you.", "tokens": [407, 411, 291, 643, 281, 291, 13], "temperature": 0.0, "avg_logprob": -0.09086137481882603, "compression_ratio": 1.7215189873417722, "no_speech_prob": 2.4438725176878506e-06}, {"id": 146, "seek": 116100, "start": 1165.0, "end": 1169.0, "text": " You need to be thinking about how you're going to deploy this thing.", "tokens": [509, 643, 281, 312, 1953, 466, 577, 291, 434, 516, 281, 7274, 341, 551, 13], "temperature": 0.0, "avg_logprob": -0.09086137481882603, "compression_ratio": 1.7215189873417722, "no_speech_prob": 2.4438725176878506e-06}, {"id": 147, "seek": 116100, "start": 1169.0, "end": 1180.0, "text": " When you train it. And if you train it with data that's different to, you know, how you're going to deploy it. It's not going to work.", "tokens": [1133, 291, 3847, 309, 13, 400, 498, 291, 3847, 309, 365, 1412, 300, 311, 819, 281, 11, 291, 458, 11, 577, 291, 434, 516, 281, 7274, 309, 13, 467, 311, 406, 516, 281, 589, 13], "temperature": 0.0, "avg_logprob": -0.09086137481882603, "compression_ratio": 1.7215189873417722, "no_speech_prob": 2.4438725176878506e-06}, {"id": 148, "seek": 116100, "start": 1180.0, "end": 1187.0, "text": " Yeah, so that's that's what that means, and", "tokens": [865, 11, 370, 300, 311, 300, 311, 437, 300, 1355, 11, 293], "temperature": 0.0, "avg_logprob": -0.09086137481882603, "compression_ratio": 1.7215189873417722, "no_speech_prob": 2.4438725176878506e-06}, {"id": 149, "seek": 118700, "start": 1187.0, "end": 1199.0, "text": " it might be difficult to get data enough data of the kind you're going to deploy it on but like, at some point you're going to be deploying this thing, which means by definition you've got some way of getting that data you're going to deploy it with.", "tokens": [309, 1062, 312, 2252, 281, 483, 1412, 1547, 1412, 295, 264, 733, 291, 434, 516, 281, 7274, 309, 322, 457, 411, 11, 412, 512, 935, 291, 434, 516, 281, 312, 34198, 341, 551, 11, 597, 1355, 538, 7123, 291, 600, 658, 512, 636, 295, 1242, 300, 1412, 291, 434, 516, 281, 7274, 309, 365, 13], "temperature": 0.0, "avg_logprob": -0.06842066014854653, "compression_ratio": 2.0271493212669682, "no_speech_prob": 1.7776686945580877e-05}, {"id": 150, "seek": 118700, "start": 1199.0, "end": 1213.0, "text": " So like do the exact thing you're going to use to deploy it but don't deploy it just capture that data until you've got some actual data from the actual environment you want to deploy the model in.", "tokens": [407, 411, 360, 264, 1900, 551, 291, 434, 516, 281, 764, 281, 7274, 309, 457, 500, 380, 7274, 309, 445, 7983, 300, 1412, 1826, 291, 600, 658, 512, 3539, 1412, 490, 264, 3539, 2823, 291, 528, 281, 7274, 264, 2316, 294, 13], "temperature": 0.0, "avg_logprob": -0.06842066014854653, "compression_ratio": 2.0271493212669682, "no_speech_prob": 1.7776686945580877e-05}, {"id": 151, "seek": 121300, "start": 1213.0, "end": 1230.0, "text": " And also take advantage of semi supervised learning techniques to then, you know, and transfer learning to maximize the amount of juice you get from that data that you've collected.", "tokens": [400, 611, 747, 5002, 295, 12909, 46533, 2539, 7512, 281, 550, 11, 291, 458, 11, 293, 5003, 2539, 281, 19874, 264, 2372, 295, 8544, 291, 483, 490, 300, 1412, 300, 291, 600, 11087, 13], "temperature": 0.0, "avg_logprob": -0.06979798015795256, "compression_ratio": 1.3609022556390977, "no_speech_prob": 1.9946105567214545e-06}, {"id": 152, "seek": 123000, "start": 1230.0, "end": 1244.0, "text": " And finally I'd say, like that's a for medical imaging, like, okay you want to deploy a model to like a new hospital they've got a different brand of MRI machine you haven't seen before.", "tokens": [400, 2721, 286, 1116, 584, 11, 411, 300, 311, 257, 337, 4625, 25036, 11, 411, 11, 1392, 291, 528, 281, 7274, 257, 2316, 281, 411, 257, 777, 4530, 436, 600, 658, 257, 819, 3360, 295, 32812, 3479, 291, 2378, 380, 1612, 949, 13], "temperature": 0.0, "avg_logprob": -0.12733311348773063, "compression_ratio": 1.3098591549295775, "no_speech_prob": 1.8161144907935522e-06}, {"id": 153, "seek": 124400, "start": 1244.0, "end": 1260.0, "text": " I would take advantage of fine tuning, you know, each time I deployed it to some different environment where things a bit different, I would expect to have to go through a fine tuning process to train it to recognize that particular MRI machines, images,", "tokens": [286, 576, 747, 5002, 295, 2489, 15164, 11, 291, 458, 11, 1184, 565, 286, 17826, 309, 281, 512, 819, 2823, 689, 721, 257, 857, 819, 11, 286, 576, 2066, 281, 362, 281, 352, 807, 257, 2489, 15164, 1399, 281, 3847, 309, 281, 5521, 300, 1729, 32812, 8379, 11, 5267, 11], "temperature": 0.0, "avg_logprob": -0.10891113497994163, "compression_ratio": 1.691358024691358, "no_speech_prob": 8.938594874052797e-06}, {"id": 154, "seek": 124400, "start": 1260.0, "end": 1271.0, "text": " but you know each time you do that fine tuning it shouldn't take very much data or very much time because it's your models already learned the key features.", "tokens": [457, 291, 458, 1184, 565, 291, 360, 300, 2489, 15164, 309, 4659, 380, 747, 588, 709, 1412, 420, 588, 709, 565, 570, 309, 311, 428, 5245, 1217, 3264, 264, 2141, 4122, 13], "temperature": 0.0, "avg_logprob": -0.10891113497994163, "compression_ratio": 1.691358024691358, "no_speech_prob": 8.938594874052797e-06}, {"id": 155, "seek": 127100, "start": 1271.0, "end": 1278.0, "text": " And you're just asking it to learn to recognize slightly different ways of seeing those features.", "tokens": [400, 291, 434, 445, 3365, 309, 281, 1466, 281, 5521, 4748, 819, 2098, 295, 2577, 729, 4122, 13], "temperature": 0.0, "avg_logprob": -0.08020004125741811, "compression_ratio": 1.5280898876404494, "no_speech_prob": 1.2218330994073767e-05}, {"id": 156, "seek": 127100, "start": 1278.0, "end": 1282.0, "text": " Yeah, I don't think you'll solve this by training for longer.", "tokens": [865, 11, 286, 500, 380, 519, 291, 603, 5039, 341, 538, 3097, 337, 2854, 13], "temperature": 0.0, "avg_logprob": -0.08020004125741811, "compression_ratio": 1.5280898876404494, "no_speech_prob": 1.2218330994073767e-05}, {"id": 157, "seek": 127100, "start": 1282.0, "end": 1293.0, "text": " You know, you'll solve it by figuring out your, your data pipeline your data labeling and your rollout strategy.", "tokens": [509, 458, 11, 291, 603, 5039, 309, 538, 15213, 484, 428, 11, 428, 1412, 15517, 428, 1412, 40244, 293, 428, 3373, 346, 5206, 13], "temperature": 0.0, "avg_logprob": -0.08020004125741811, "compression_ratio": 1.5280898876404494, "no_speech_prob": 1.2218330994073767e-05}, {"id": 158, "seek": 129300, "start": 1293.0, "end": 1305.0, "text": " Usually the issues that we're having is that we don't have enough data of certain category. But, I mean, the thing that you did yesterday.", "tokens": [11419, 264, 2663, 300, 321, 434, 1419, 307, 300, 321, 500, 380, 362, 1547, 1412, 295, 1629, 7719, 13, 583, 11, 286, 914, 11, 264, 551, 300, 291, 630, 5186, 13], "temperature": 0.0, "avg_logprob": -0.1511333018173406, "compression_ratio": 1.6701030927835052, "no_speech_prob": 7.766618182358798e-06}, {"id": 159, "seek": 129300, "start": 1305.0, "end": 1315.0, "text": " It resolves a little bit of that problem I think we're going to start using. Yeah. Well also like, if you don't have enough data of some category, don't use the model for that category.", "tokens": [467, 7923, 977, 257, 707, 857, 295, 300, 1154, 286, 519, 321, 434, 516, 281, 722, 1228, 13, 865, 13, 1042, 611, 411, 11, 498, 291, 500, 380, 362, 1547, 1412, 295, 512, 7719, 11, 500, 380, 764, 264, 2316, 337, 300, 7719, 13], "temperature": 0.0, "avg_logprob": -0.1511333018173406, "compression_ratio": 1.6701030927835052, "no_speech_prob": 7.766618182358798e-06}, {"id": 160, "seek": 131500, "start": 1315.0, "end": 1336.0, "text": " Well, so like, you know, rather than using softmax use binary sigmoid, you know, as your last layer and so then you've kind of got like a probability that X appears in this image and so then you can, you can recognize when none of the things", "tokens": [1042, 11, 370, 411, 11, 291, 458, 11, 2831, 813, 1228, 2787, 41167, 764, 17434, 4556, 3280, 327, 11, 291, 458, 11, 382, 428, 1036, 4583, 293, 370, 550, 291, 600, 733, 295, 658, 411, 257, 8482, 300, 1783, 7038, 294, 341, 3256, 293, 370, 550, 291, 393, 11, 291, 393, 5521, 562, 6022, 295, 264, 721], "temperature": 0.0, "avg_logprob": -0.14794788985955912, "compression_ratio": 1.5649350649350648, "no_speech_prob": 9.367586244479753e-06}, {"id": 161, "seek": 133600, "start": 1336.0, "end": 1346.0, "text": " that you can predict well appear in the image. And so, then have a, you know, you always want a human in the loop anyway.", "tokens": [300, 291, 393, 6069, 731, 4204, 294, 264, 3256, 13, 400, 370, 11, 550, 362, 257, 11, 291, 458, 11, 291, 1009, 528, 257, 1952, 294, 264, 6367, 4033, 13], "temperature": 0.0, "avg_logprob": -0.10141620916478775, "compression_ratio": 1.5416666666666667, "no_speech_prob": 1.9219798559788615e-05}, {"id": 162, "seek": 133600, "start": 1346.0, "end": 1359.0, "text": " So when you didn't find any of the categories of things you've got enough data to be able to find, then triage those to the human review.", "tokens": [407, 562, 291, 994, 380, 915, 604, 295, 264, 10479, 295, 721, 291, 600, 658, 1547, 1412, 281, 312, 1075, 281, 915, 11, 550, 1376, 609, 729, 281, 264, 1952, 3131, 13], "temperature": 0.0, "avg_logprob": -0.10141620916478775, "compression_ratio": 1.5416666666666667, "no_speech_prob": 1.9219798559788615e-05}, {"id": 163, "seek": 135900, "start": 1359.0, "end": 1387.0, "text": " And one thing that we did is, I mean we have like 50 something categories. Just one moment, hang on.", "tokens": [400, 472, 551, 300, 321, 630, 307, 11, 286, 914, 321, 362, 411, 2625, 746, 10479, 13, 1449, 472, 1623, 11, 3967, 322, 13], "temperature": 0.0, "avg_logprob": -0.1987010751451765, "compression_ratio": 1.1111111111111112, "no_speech_prob": 4.9033758841687813e-05}, {"id": 164, "seek": 138700, "start": 1387.0, "end": 1390.0, "text": " Sorry about that.", "tokens": [4919, 466, 300, 13], "temperature": 0.0, "avg_logprob": -0.14175550261540198, "compression_ratio": 1.6369426751592357, "no_speech_prob": 4.538493885775097e-05}, {"id": 165, "seek": 138700, "start": 1390.0, "end": 1406.0, "text": " We had like 50 categories and some of them are like they have a lot like 10 of them have a lot of items. So we end up doing like in a three step kind of process, like the ones with a lot, the ones with medium number, with a smaller number.", "tokens": [492, 632, 411, 2625, 10479, 293, 512, 295, 552, 366, 411, 436, 362, 257, 688, 411, 1266, 295, 552, 362, 257, 688, 295, 4754, 13, 407, 321, 917, 493, 884, 411, 294, 257, 1045, 1823, 733, 295, 1399, 11, 411, 264, 2306, 365, 257, 688, 11, 264, 2306, 365, 6399, 1230, 11, 365, 257, 4356, 1230, 13], "temperature": 0.0, "avg_logprob": -0.14175550261540198, "compression_ratio": 1.6369426751592357, "no_speech_prob": 4.538493885775097e-05}, {"id": 166, "seek": 140600, "start": 1406.0, "end": 1426.0, "text": " And it looks like you resolve the problem a little bit. Cool. But, but this was to classify metadata coming from, from other systems and classify for legal purposes for legal retention.", "tokens": [400, 309, 1542, 411, 291, 14151, 264, 1154, 257, 707, 857, 13, 8561, 13, 583, 11, 457, 341, 390, 281, 33872, 26603, 1348, 490, 11, 490, 661, 3652, 293, 33872, 337, 5089, 9932, 337, 5089, 22871, 13], "temperature": 0.0, "avg_logprob": -0.20422609992649243, "compression_ratio": 1.381294964028777, "no_speech_prob": 1.4052745427761693e-05}, {"id": 167, "seek": 140600, "start": 1426.0, "end": 1433.0, "text": " I see.", "tokens": [286, 536, 13], "temperature": 0.0, "avg_logprob": -0.20422609992649243, "compression_ratio": 1.381294964028777, "no_speech_prob": 1.4052745427761693e-05}, {"id": 168, "seek": 143300, "start": 1433.0, "end": 1448.0, "text": " I had a question actually, you tried the data a lot of sites so I think you haven't submitted that to Kaggle notebook. So, did you do any validation locally first before submitting to Kaggle, something like that.", "tokens": [286, 632, 257, 1168, 767, 11, 291, 3031, 264, 1412, 257, 688, 295, 7533, 370, 286, 519, 291, 2378, 380, 14405, 300, 281, 48751, 22631, 21060, 13, 407, 11, 630, 291, 360, 604, 24071, 16143, 700, 949, 31836, 281, 48751, 22631, 11, 746, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.3179960060119629, "compression_ratio": 1.4421768707482994, "no_speech_prob": 1.6697917089913972e-05}, {"id": 169, "seek": 144800, "start": 1448.0, "end": 1465.0, "text": " No, I mean you saw what I did right. And when I did it, so I just, yeah, I just, I'm like I was intentionally using a very mechanistic approach.", "tokens": [883, 11, 286, 914, 291, 1866, 437, 286, 630, 558, 13, 400, 562, 286, 630, 309, 11, 370, 286, 445, 11, 1338, 11, 286, 445, 11, 286, 478, 411, 286, 390, 22062, 1228, 257, 588, 4236, 3142, 3109, 13], "temperature": 0.0, "avg_logprob": -0.1395079590553461, "compression_ratio": 1.2521739130434784, "no_speech_prob": 2.123201056747348e-06}, {"id": 170, "seek": 146500, "start": 1465.0, "end": 1479.0, "text": " So it's kind of like this. Yeah, showing like his, the basic steps of pretty much any computer vision model which is entirely mechanical and doesn't require any domain expertise.", "tokens": [407, 309, 311, 733, 295, 411, 341, 13, 865, 11, 4099, 411, 702, 11, 264, 3875, 4439, 295, 1238, 709, 604, 3820, 5201, 2316, 597, 307, 7696, 12070, 293, 1177, 380, 3651, 604, 9274, 11769, 13], "temperature": 0.0, "avg_logprob": -0.22800139891795623, "compression_ratio": 1.5852534562211982, "no_speech_prob": 3.966686563217081e-06}, {"id": 171, "seek": 146500, "start": 1479.0, "end": 1490.0, "text": " So yeah, my question more was like, shouldn't we always treat the public leaderboard like a good or like should we take a hold out local data sets first to validate.", "tokens": [407, 1338, 11, 452, 1168, 544, 390, 411, 11, 4659, 380, 321, 1009, 2387, 264, 1908, 5263, 3787, 411, 257, 665, 420, 411, 820, 321, 747, 257, 1797, 484, 2654, 1412, 6352, 700, 281, 29562, 13], "temperature": 0.0, "avg_logprob": -0.22800139891795623, "compression_ratio": 1.5852534562211982, "no_speech_prob": 3.966686563217081e-06}, {"id": 172, "seek": 149000, "start": 1490.0, "end": 1496.0, "text": " I mean, I always have a validation set. Yeah.", "tokens": [286, 914, 11, 286, 1009, 362, 257, 24071, 992, 13, 865, 13], "temperature": 0.0, "avg_logprob": -0.1273705388458682, "compression_ratio": 1.535294117647059, "no_speech_prob": 9.817046702664811e-06}, {"id": 173, "seek": 149000, "start": 1496.0, "end": 1499.0, "text": " Which we saw in this.", "tokens": [3013, 321, 1866, 294, 341, 13], "temperature": 0.0, "avg_logprob": -0.1273705388458682, "compression_ratio": 1.535294117647059, "no_speech_prob": 9.817046702664811e-06}, {"id": 174, "seek": 149000, "start": 1499.0, "end": 1512.0, "text": " And this I just used a random splitter, because as far as I know the test set and the Kaggle competition is a randomly split validation set. Yeah, so like, whether it be for Kaggle or anything.", "tokens": [400, 341, 286, 445, 1143, 257, 4974, 4732, 3904, 11, 570, 382, 1400, 382, 286, 458, 264, 1500, 992, 293, 264, 48751, 22631, 6211, 307, 257, 16979, 7472, 24071, 992, 13, 865, 11, 370, 411, 11, 1968, 309, 312, 337, 48751, 22631, 420, 1340, 13], "temperature": 0.0, "avg_logprob": -0.1273705388458682, "compression_ratio": 1.535294117647059, "no_speech_prob": 9.817046702664811e-06}, {"id": 175, "seek": 151200, "start": 1512.0, "end": 1527.0, "text": " I think creating a validation set that is closely as possible represents the data you expect to get in deployment or in your test set is really important.", "tokens": [286, 519, 4084, 257, 24071, 992, 300, 307, 8185, 382, 1944, 8855, 264, 1412, 291, 2066, 281, 483, 294, 19317, 420, 294, 428, 1500, 992, 307, 534, 1021, 13], "temperature": 0.0, "avg_logprob": -0.12021375152299989, "compression_ratio": 1.4779874213836477, "no_speech_prob": 3.237487362639513e-06}, {"id": 176, "seek": 151200, "start": 1527.0, "end": 1534.0, "text": " And yeah, I actually didn't spend the time doing that on this paddy competition.", "tokens": [400, 1338, 11, 286, 767, 994, 380, 3496, 264, 565, 884, 300, 322, 341, 6887, 3173, 6211, 13], "temperature": 0.0, "avg_logprob": -0.12021375152299989, "compression_ratio": 1.4779874213836477, "no_speech_prob": 3.237487362639513e-06}, {"id": 177, "seek": 153400, "start": 1534.0, "end": 1549.0, "text": " Normally on Kaggle if somebody does and notices there's a difference between the private leaderboard and the public leaderboard and like the test set and the training set normally it'll appear, you know, in discussions or on a Kaggle kernel or something.", "tokens": [17424, 322, 48751, 22631, 498, 2618, 775, 293, 32978, 456, 311, 257, 2649, 1296, 264, 4551, 5263, 3787, 293, 264, 1908, 5263, 3787, 293, 411, 264, 1500, 992, 293, 264, 3097, 992, 5646, 309, 603, 4204, 11, 291, 458, 11, 294, 11088, 420, 322, 257, 48751, 22631, 28256, 420, 746, 13], "temperature": 0.0, "avg_logprob": -0.16748183857310903, "compression_ratio": 1.5776397515527951, "no_speech_prob": 9.079203664441593e-06}, {"id": 178, "seek": 154900, "start": 1549.0, "end": 1565.0, "text": " So, partly why I didn't look into it but yeah I mean you should probably check.", "tokens": [407, 11, 17031, 983, 286, 994, 380, 574, 666, 309, 457, 1338, 286, 914, 291, 820, 1391, 1520, 13], "temperature": 0.0, "avg_logprob": -0.24479007720947266, "compression_ratio": 1.1020408163265305, "no_speech_prob": 2.4059897896222537e-06}, {"id": 179, "seek": 154900, "start": 1565.0, "end": 1568.0, "text": " Do they have the same sizes.", "tokens": [1144, 436, 362, 264, 912, 11602, 13], "temperature": 0.0, "avg_logprob": -0.24479007720947266, "compression_ratio": 1.1020408163265305, "no_speech_prob": 2.4059897896222537e-06}, {"id": 180, "seek": 156800, "start": 1568.0, "end": 1580.0, "text": " And for me if I said as I see any difference between the test set and the training set that puts my alarm bells on right because now I know that it's not randomly selected.", "tokens": [400, 337, 385, 498, 286, 848, 382, 286, 536, 604, 2649, 1296, 264, 1500, 992, 293, 264, 3097, 992, 300, 8137, 452, 14183, 25474, 322, 558, 570, 586, 286, 458, 300, 309, 311, 406, 16979, 8209, 13], "temperature": 0.0, "avg_logprob": -0.10202609300613404, "compression_ratio": 1.6634146341463414, "no_speech_prob": 8.139187229971867e-06}, {"id": 181, "seek": 156800, "start": 1580.0, "end": 1585.0, "text": " And if you know it's not randomly selected then you immediately have to think okay they're trying to trick us.", "tokens": [400, 498, 291, 458, 309, 311, 406, 16979, 8209, 550, 291, 4258, 362, 281, 519, 1392, 436, 434, 1382, 281, 4282, 505, 13], "temperature": 0.0, "avg_logprob": -0.10202609300613404, "compression_ratio": 1.6634146341463414, "no_speech_prob": 8.139187229971867e-06}, {"id": 182, "seek": 156800, "start": 1585.0, "end": 1591.0, "text": " So, I would then look everything I could for differences.", "tokens": [407, 11, 286, 576, 550, 574, 1203, 286, 727, 337, 7300, 13], "temperature": 0.0, "avg_logprob": -0.10202609300613404, "compression_ratio": 1.6634146341463414, "no_speech_prob": 8.139187229971867e-06}, {"id": 183, "seek": 159100, "start": 1591.0, "end": 1600.0, "text": " Because it takes effort to not randomly select a test set so they must be doing it very intentionally for some reason.", "tokens": [1436, 309, 2516, 4630, 281, 406, 16979, 3048, 257, 1500, 992, 370, 436, 1633, 312, 884, 309, 588, 22062, 337, 512, 1778, 13], "temperature": 0.0, "avg_logprob": -0.08372480760921132, "compression_ratio": 1.7455357142857142, "no_speech_prob": 1.6696501916158013e-05}, {"id": 184, "seek": 159100, "start": 1600.0, "end": 1603.0, "text": " Quite often for wrong reasons.", "tokens": [20464, 2049, 337, 2085, 4112, 13], "temperature": 0.0, "avg_logprob": -0.08372480760921132, "compression_ratio": 1.7455357142857142, "no_speech_prob": 1.6696501916158013e-05}, {"id": 185, "seek": 159100, "start": 1603.0, "end": 1612.0, "text": " I think so, like I don't think a Kaggle competition should ever silently give you a systematically different test set.", "tokens": [286, 519, 370, 11, 411, 286, 500, 380, 519, 257, 48751, 22631, 6211, 820, 1562, 40087, 976, 291, 257, 39531, 819, 1500, 992, 13], "temperature": 0.0, "avg_logprob": -0.08372480760921132, "compression_ratio": 1.7455357142857142, "no_speech_prob": 1.6696501916158013e-05}, {"id": 186, "seek": 159100, "start": 1612.0, "end": 1617.0, "text": " I think that's great reasons to create a systematically different test set, but there's never a reason not to tell people.", "tokens": [286, 519, 300, 311, 869, 4112, 281, 1884, 257, 39531, 819, 1500, 992, 11, 457, 456, 311, 1128, 257, 1778, 406, 281, 980, 561, 13], "temperature": 0.0, "avg_logprob": -0.08372480760921132, "compression_ratio": 1.7455357142857142, "no_speech_prob": 1.6696501916158013e-05}, {"id": 187, "seek": 161700, "start": 1617.0, "end": 1629.0, "text": " So if it's like medical imaging is a different hospital you should say this is a different hospital or if it's fishing you should say these are different boats, or, you know, because like you want people to do well on your data.", "tokens": [407, 498, 309, 311, 411, 4625, 25036, 307, 257, 819, 4530, 291, 820, 584, 341, 307, 257, 819, 4530, 420, 498, 309, 311, 10180, 291, 820, 584, 613, 366, 819, 17772, 11, 420, 11, 291, 458, 11, 570, 411, 291, 528, 561, 281, 360, 731, 322, 428, 1412, 13], "temperature": 0.0, "avg_logprob": -0.08768654522830494, "compression_ratio": 1.7087912087912087, "no_speech_prob": 9.738892549648881e-05}, {"id": 188, "seek": 161700, "start": 1629.0, "end": 1639.0, "text": " So if you tell them, then they can use that information to give you better models.", "tokens": [407, 498, 291, 980, 552, 11, 550, 436, 393, 764, 300, 1589, 281, 976, 291, 1101, 5245, 13], "temperature": 0.0, "avg_logprob": -0.08768654522830494, "compression_ratio": 1.7087912087912087, "no_speech_prob": 9.738892549648881e-05}, {"id": 189, "seek": 163900, "start": 1639.0, "end": 1652.0, "text": " And then, like, going back to what you asked about. There's this validation and training then there's this, whether your local validation maps to what's happening on the leaderboard with the score on the hidden test set.", "tokens": [400, 550, 11, 411, 11, 516, 646, 281, 437, 291, 2351, 466, 13, 821, 311, 341, 24071, 293, 3097, 550, 456, 311, 341, 11, 1968, 428, 2654, 24071, 11317, 281, 437, 311, 2737, 322, 264, 5263, 3787, 365, 264, 6175, 322, 264, 7633, 1500, 992, 13], "temperature": 0.0, "avg_logprob": -0.17259287532371811, "compression_ratio": 1.6470588235294117, "no_speech_prob": 4.131246168981306e-05}, {"id": 190, "seek": 163900, "start": 1652.0, "end": 1660.0, "text": " But there's one other scenario that I encountered recently. And maybe it would be interesting for someone when you're working on a competition.", "tokens": [583, 456, 311, 472, 661, 9005, 300, 286, 20381, 3938, 13, 400, 1310, 309, 576, 312, 1880, 337, 1580, 562, 291, 434, 1364, 322, 257, 6211, 13], "temperature": 0.0, "avg_logprob": -0.17259287532371811, "compression_ratio": 1.6470588235294117, "no_speech_prob": 4.131246168981306e-05}, {"id": 191, "seek": 166000, "start": 1660.0, "end": 1675.0, "text": " You might have missed something in your code or the prediction, you know your model is doing something useful but you're failing to output a correctly formatted submission file, and not in the sense that the submission phase on Kaggle, but some predictions are not aligned,", "tokens": [509, 1062, 362, 6721, 746, 294, 428, 3089, 420, 264, 17630, 11, 291, 458, 428, 2316, 307, 884, 746, 4420, 457, 291, 434, 18223, 281, 5598, 257, 8944, 1254, 32509, 23689, 3991, 11, 293, 406, 294, 264, 2020, 300, 264, 23689, 5574, 322, 48751, 22631, 11, 457, 512, 21264, 366, 406, 17962, 11], "temperature": 0.0, "avg_logprob": -0.277173778467011, "compression_ratio": 1.6058823529411765, "no_speech_prob": 2.5859857487375848e-05}, {"id": 192, "seek": 167500, "start": 1675.0, "end": 1695.0, "text": " or should be or, you know, they're for a different customer ID or stuff like that. So, once you have one good submission file, relatively good, you can just store it locally, and then see, you know, run a check the correlation between your new submission,", "tokens": [420, 820, 312, 420, 11, 291, 458, 11, 436, 434, 337, 257, 819, 5474, 7348, 420, 1507, 411, 300, 13, 407, 11, 1564, 291, 362, 472, 665, 23689, 3991, 11, 7226, 665, 11, 291, 393, 445, 3531, 309, 16143, 11, 293, 550, 536, 11, 291, 458, 11, 1190, 257, 1520, 264, 20009, 1296, 428, 777, 23689, 11], "temperature": 0.0, "avg_logprob": -0.2002725601196289, "compression_ratio": 1.5178571428571428, "no_speech_prob": 6.915086123626679e-05}, {"id": 193, "seek": 169500, "start": 1695.0, "end": 1712.0, "text": " and the one that you know that this, okay, and you know the correlation should be upwards of 0.9. And then you know yeah okay so I didn't mess up anything with the technical aspect of putting the prediction I mean, it's not a great trick but, you know, I was like, putting my hair out, why is this not working", "tokens": [293, 264, 472, 300, 291, 458, 300, 341, 11, 1392, 11, 293, 291, 458, 264, 20009, 820, 312, 22167, 295, 1958, 13, 24, 13, 400, 550, 291, 458, 1338, 1392, 370, 286, 994, 380, 2082, 493, 1340, 365, 264, 6191, 4171, 295, 3372, 264, 17630, 286, 914, 11, 309, 311, 406, 257, 869, 4282, 457, 11, 291, 458, 11, 286, 390, 411, 11, 3372, 452, 2578, 484, 11, 983, 307, 341, 406, 1364], "temperature": 0.0, "avg_logprob": -0.2239961527814769, "compression_ratio": 1.634453781512605, "no_speech_prob": 5.556783071369864e-05}, {"id": 194, "seek": 169500, "start": 1712.0, "end": 1723.0, "text": " it's a better model. So this was like a sanity check step. Maybe at some point.", "tokens": [309, 311, 257, 1101, 2316, 13, 407, 341, 390, 411, 257, 47892, 1520, 1823, 13, 2704, 412, 512, 935, 13], "temperature": 0.0, "avg_logprob": -0.2239961527814769, "compression_ratio": 1.634453781512605, "no_speech_prob": 5.556783071369864e-05}, {"id": 195, "seek": 172300, "start": 1723.0, "end": 1730.0, "text": " Thanks, cool.", "tokens": [2561, 11, 1627, 13], "temperature": 0.0, "avg_logprob": -0.2512875965663365, "compression_ratio": 0.8596491228070176, "no_speech_prob": 5.062486161477864e-05}, {"id": 196, "seek": 172300, "start": 1730.0, "end": 1734.0, "text": " Alright, so", "tokens": [2798, 11, 370], "temperature": 0.0, "avg_logprob": -0.2512875965663365, "compression_ratio": 0.8596491228070176, "no_speech_prob": 5.062486161477864e-05}, {"id": 197, "seek": 172300, "start": 1734.0, "end": 1742.0, "text": " let me share my screen.", "tokens": [718, 385, 2073, 452, 2568, 13], "temperature": 0.0, "avg_logprob": -0.2512875965663365, "compression_ratio": 0.8596491228070176, "no_speech_prob": 5.062486161477864e-05}, {"id": 198, "seek": 174200, "start": 1742.0, "end": 1758.0, "text": " It's fine zoom, zoom, share screen.", "tokens": [467, 311, 2489, 8863, 11, 8863, 11, 2073, 2568, 13], "temperature": 0.0, "avg_logprob": -0.22787719784360944, "compression_ratio": 1.058139534883721, "no_speech_prob": 1.0774199836305343e-05}, {"id": 199, "seek": 174200, "start": 1758.0, "end": 1763.0, "text": " Oh, that's not the right button.", "tokens": [876, 11, 300, 311, 406, 264, 558, 2960, 13], "temperature": 0.0, "avg_logprob": -0.22787719784360944, "compression_ratio": 1.058139534883721, "no_speech_prob": 1.0774199836305343e-05}, {"id": 200, "seek": 174200, "start": 1763.0, "end": 1771.0, "text": " Control shift H. Okay.", "tokens": [12912, 5513, 389, 13, 1033, 13], "temperature": 0.0, "avg_logprob": -0.22787719784360944, "compression_ratio": 1.058139534883721, "no_speech_prob": 1.0774199836305343e-05}, {"id": 201, "seek": 177100, "start": 1771.0, "end": 1781.0, "text": " Where did we get to in the last lesson.", "tokens": [2305, 630, 321, 483, 281, 294, 264, 1036, 6898, 13], "temperature": 0.0, "avg_logprob": -0.20119098516610953, "compression_ratio": 1.0684931506849316, "no_speech_prob": 1.044948567141546e-05}, {"id": 202, "seek": 177100, "start": 1781.0, "end": 1786.0, "text": " We finished random forests right.", "tokens": [492, 4335, 4974, 21700, 558, 13], "temperature": 0.0, "avg_logprob": -0.20119098516610953, "compression_ratio": 1.0684931506849316, "no_speech_prob": 1.044948567141546e-05}, {"id": 203, "seek": 177100, "start": 1786.0, "end": 1791.0, "text": " And,", "tokens": [400, 11], "temperature": 0.0, "avg_logprob": -0.20119098516610953, "compression_ratio": 1.0684931506849316, "no_speech_prob": 1.044948567141546e-05}, {"id": 204, "seek": 179100, "start": 1791.0, "end": 1802.0, "text": " Oh, that's right and I haven't posted that video yet.", "tokens": [876, 11, 300, 311, 558, 293, 286, 2378, 380, 9437, 300, 960, 1939, 13], "temperature": 0.0, "avg_logprob": -0.17575007014804417, "compression_ratio": 0.9137931034482759, "no_speech_prob": 2.6675414119381458e-05}, {"id": 205, "seek": 180200, "start": 1802.0, "end": 1826.0, "text": " Last year life.", "tokens": [5264, 1064, 993, 13], "temperature": 0.0, "avg_logprob": -0.5586952567100525, "compression_ratio": 0.6521739130434783, "no_speech_prob": 2.75087368208915e-05}, {"id": 206, "seek": 182600, "start": 1826.0, "end": 1832.0, "text": " Okay, so we could get small models.", "tokens": [1033, 11, 370, 321, 727, 483, 1359, 5245, 13], "temperature": 0.0, "avg_logprob": -0.248280331492424, "compression_ratio": 1.0963855421686748, "no_speech_prob": 9.077472896024119e-06}, {"id": 207, "seek": 182600, "start": 1832.0, "end": 1844.0, "text": " And to get to the end of this.", "tokens": [400, 281, 483, 281, 264, 917, 295, 341, 13], "temperature": 0.0, "avg_logprob": -0.248280331492424, "compression_ratio": 1.0963855421686748, "no_speech_prob": 9.077472896024119e-06}, {"id": 208, "seek": 182600, "start": 1844.0, "end": 1849.0, "text": " Okay, so that basically.", "tokens": [1033, 11, 370, 300, 1936, 13], "temperature": 0.0, "avg_logprob": -0.248280331492424, "compression_ratio": 1.0963855421686748, "no_speech_prob": 9.077472896024119e-06}, {"id": 209, "seek": 184900, "start": 1849.0, "end": 1859.0, "text": " So we basically finished the second one of, of our Kaggle things.", "tokens": [407, 321, 1936, 4335, 264, 1150, 472, 295, 11, 295, 527, 48751, 22631, 721, 13], "temperature": 0.0, "avg_logprob": -0.2158064842224121, "compression_ratio": 1.0, "no_speech_prob": 7.766107046336401e-06}, {"id": 210, "seek": 184900, "start": 1859.0, "end": 1867.0, "text": " So next week.", "tokens": [407, 958, 1243, 13], "temperature": 0.0, "avg_logprob": -0.2158064842224121, "compression_ratio": 1.0, "no_speech_prob": 7.766107046336401e-06}, {"id": 211, "seek": 186700, "start": 1867.0, "end": 1879.0, "text": " See what's in part three.", "tokens": [3008, 437, 311, 294, 644, 1045, 13], "temperature": 0.0, "avg_logprob": -0.1048173075136931, "compression_ratio": 1.3805970149253732, "no_speech_prob": 5.862519174115732e-06}, {"id": 212, "seek": 186700, "start": 1879.0, "end": 1890.0, "text": " Gradient accumulation. I think that's worth covering. So one thing that somebody pointed out on Kaggle is I've actually, I'm using gradient accumulation wrong.", "tokens": [16710, 1196, 35647, 13, 286, 519, 300, 311, 3163, 10322, 13, 407, 472, 551, 300, 2618, 10932, 484, 322, 48751, 22631, 307, 286, 600, 767, 11, 286, 478, 1228, 16235, 35647, 2085, 13], "temperature": 0.0, "avg_logprob": -0.1048173075136931, "compression_ratio": 1.3805970149253732, "no_speech_prob": 5.862519174115732e-06}, {"id": 213, "seek": 189000, "start": 1890.0, "end": 1899.0, "text": " I was passing in two here to mean to make create two batch, like do two batches before you accumulate.", "tokens": [286, 390, 8437, 294, 732, 510, 281, 914, 281, 652, 1884, 732, 15245, 11, 411, 360, 732, 15245, 279, 949, 291, 33384, 13], "temperature": 0.0, "avg_logprob": -0.16708805191684778, "compression_ratio": 1.5755813953488371, "no_speech_prob": 1.2604255971382372e-05}, {"id": 214, "seek": 189000, "start": 1899.0, "end": 1910.0, "text": " But actually what I meant to be putting in here is the kind of target batch size I want. So that would be actually, I should be putting 64 here.", "tokens": [583, 767, 437, 286, 4140, 281, 312, 3372, 294, 510, 307, 264, 733, 295, 3779, 15245, 2744, 286, 528, 13, 407, 300, 576, 312, 767, 11, 286, 820, 312, 3372, 12145, 510, 13], "temperature": 0.0, "avg_logprob": -0.16708805191684778, "compression_ratio": 1.5755813953488371, "no_speech_prob": 1.2604255971382372e-05}, {"id": 215, "seek": 189000, "start": 1910.0, "end": 1912.0, "text": " So I feel a bit stupid.", "tokens": [407, 286, 841, 257, 857, 6631, 13], "temperature": 0.0, "avg_logprob": -0.16708805191684778, "compression_ratio": 1.5755813953488371, "no_speech_prob": 1.2604255971382372e-05}, {"id": 216, "seek": 191200, "start": 1912.0, "end": 1921.0, "text": " What I've been doing is I've been actually not using gradient graded correlation at all. I guess it's been doing a batch and saying that's over.", "tokens": [708, 286, 600, 668, 884, 307, 286, 600, 668, 767, 406, 1228, 16235, 2771, 292, 20009, 412, 439, 13, 286, 2041, 309, 311, 668, 884, 257, 15245, 293, 1566, 300, 311, 670, 13], "temperature": 0.0, "avg_logprob": -0.19759980491969897, "compression_ratio": 1.655, "no_speech_prob": 5.954653715889435e-06}, {"id": 217, "seek": 191200, "start": 1921.0, "end": 1925.0, "text": " I'm saying my maximum batch size should be two.", "tokens": [286, 478, 1566, 452, 6674, 15245, 2744, 820, 312, 732, 13], "temperature": 0.0, "avg_logprob": -0.19759980491969897, "compression_ratio": 1.655, "no_speech_prob": 5.954653715889435e-06}, {"id": 218, "seek": 191200, "start": 1925.0, "end": 1930.0, "text": " Okay. So this has actually been not working at all. That's interesting.", "tokens": [1033, 13, 407, 341, 575, 767, 668, 406, 1364, 412, 439, 13, 663, 311, 1880, 13], "temperature": 0.0, "avg_logprob": -0.19759980491969897, "compression_ratio": 1.655, "no_speech_prob": 5.954653715889435e-06}, {"id": 219, "seek": 191200, "start": 1930.0, "end": 1933.0, "text": " Oops.", "tokens": [21726, 13], "temperature": 0.0, "avg_logprob": -0.19759980491969897, "compression_ratio": 1.655, "no_speech_prob": 5.954653715889435e-06}, {"id": 220, "seek": 191200, "start": 1933.0, "end": 1936.0, "text": " So it's been using a batch size of 32.", "tokens": [407, 309, 311, 668, 1228, 257, 15245, 2744, 295, 8858, 13], "temperature": 0.0, "avg_logprob": -0.19759980491969897, "compression_ratio": 1.655, "no_speech_prob": 5.954653715889435e-06}, {"id": 221, "seek": 191200, "start": 1936.0, "end": 1939.0, "text": " And not accumulating.", "tokens": [400, 406, 12989, 12162, 13], "temperature": 0.0, "avg_logprob": -0.19759980491969897, "compression_ratio": 1.655, "no_speech_prob": 5.954653715889435e-06}, {"id": 222, "seek": 193900, "start": 1939.0, "end": 1943.0, "text": " Ah, okay.", "tokens": [2438, 11, 1392, 13], "temperature": 0.0, "avg_logprob": -0.12840942626303814, "compression_ratio": 1.140495867768595, "no_speech_prob": 1.3419053175312001e-05}, {"id": 223, "seek": 193900, "start": 1943.0, "end": 1951.0, "text": " So that's one thing to note. So when I get Kaggle GPU time again, we'll need to rerun this.", "tokens": [407, 300, 311, 472, 551, 281, 3637, 13, 407, 562, 286, 483, 48751, 22631, 18407, 565, 797, 11, 321, 603, 643, 281, 43819, 409, 341, 13], "temperature": 0.0, "avg_logprob": -0.12840942626303814, "compression_ratio": 1.140495867768595, "no_speech_prob": 1.3419053175312001e-05}, {"id": 224, "seek": 193900, "start": 1951.0, "end": 1958.0, "text": " Actually it only took 4,000 seconds.", "tokens": [5135, 309, 787, 1890, 1017, 11, 1360, 3949, 13], "temperature": 0.0, "avg_logprob": -0.12840942626303814, "compression_ratio": 1.140495867768595, "no_speech_prob": 1.3419053175312001e-05}, {"id": 225, "seek": 195800, "start": 1958.0, "end": 1976.0, "text": " So I guess we should, we could just get it running right now, couldn't we?", "tokens": [407, 286, 2041, 321, 820, 11, 321, 727, 445, 483, 309, 2614, 558, 586, 11, 2809, 380, 321, 30], "temperature": 0.0, "avg_logprob": -0.1062314693744366, "compression_ratio": 1.0266666666666666, "no_speech_prob": 1.3842727639712393e-05}, {"id": 226, "seek": 195800, "start": 1976.0, "end": 1985.0, "text": " So", "tokens": [407], "temperature": 0.0, "avg_logprob": -0.1062314693744366, "compression_ratio": 1.0266666666666666, "no_speech_prob": 1.3842727639712393e-05}, {"id": 227, "seek": 198500, "start": 1985.0, "end": 1995.0, "text": " that should be 64.", "tokens": [300, 820, 312, 12145, 13], "temperature": 0.0, "avg_logprob": -0.2035494584303636, "compression_ratio": 1.0705882352941176, "no_speech_prob": 4.985213672625832e-05}, {"id": 228, "seek": 198500, "start": 1995.0, "end": 2013.0, "text": " Gravity of Paths defines how large the effective batch size you want is.", "tokens": [49478, 295, 21914, 82, 23122, 577, 2416, 264, 4942, 15245, 2744, 291, 528, 307, 13], "temperature": 0.0, "avg_logprob": -0.2035494584303636, "compression_ratio": 1.0705882352941176, "no_speech_prob": 4.985213672625832e-05}, {"id": 229, "seek": 201300, "start": 2013.0, "end": 2020.0, "text": " So if we just remove the batch size, we can just remove the number of batches.", "tokens": [407, 498, 321, 445, 4159, 264, 15245, 2744, 11, 321, 393, 445, 4159, 264, 1230, 295, 15245, 279, 13], "temperature": 0.0, "avg_logprob": -0.6586903658780184, "compression_ratio": 1.5051546391752577, "no_speech_prob": 9.515024430584162e-06}, {"id": 230, "seek": 201300, "start": 2020.0, "end": 2023.0, "text": " Oh, we can just remove this sentence entirely.", "tokens": [876, 11, 321, 393, 445, 4159, 341, 8174, 7696, 13], "temperature": 0.0, "avg_logprob": -0.6586903658780184, "compression_ratio": 1.5051546391752577, "no_speech_prob": 9.515024430584162e-06}, {"id": 231, "seek": 201300, "start": 2023.0, "end": 2032.0, "text": " Oh no, that's right.", "tokens": [876, 572, 11, 300, 311, 558, 13], "temperature": 0.0, "avg_logprob": -0.6586903658780184, "compression_ratio": 1.5051546391752577, "no_speech_prob": 9.515024430584162e-06}, {"id": 232, "seek": 203200, "start": 2032.0, "end": 2060.0, "text": " We need to remove the batch size by some number based on how small we need it to be for our GPUs per M.", "tokens": [492, 643, 281, 4159, 264, 15245, 2744, 538, 512, 1230, 2361, 322, 577, 1359, 321, 643, 309, 281, 312, 337, 527, 18407, 82, 680, 376, 13], "temperature": 0.0, "avg_logprob": -0.2815922101338704, "compression_ratio": 1.10752688172043, "no_speech_prob": 1.669705488893669e-05}, {"id": 233, "seek": 206000, "start": 2060.0, "end": 2073.0, "text": " So", "tokens": [407], "temperature": 0.0, "avg_logprob": -0.3279104232788086, "compression_ratio": 0.2, "no_speech_prob": 8.663730113767087e-06}, {"id": 234, "seek": 207300, "start": 2073.0, "end": 2090.0, "text": " And on Kaggle, I think these were all smaller. I don't know why, but the Kaggle GPUs use less memory than my GPU for some reason.", "tokens": [400, 322, 48751, 22631, 11, 286, 519, 613, 645, 439, 4356, 13, 286, 500, 380, 458, 983, 11, 457, 264, 48751, 22631, 18407, 82, 764, 1570, 4675, 813, 452, 18407, 337, 512, 1778, 13], "temperature": 0.0, "avg_logprob": -0.12697259585062662, "compression_ratio": 1.2032520325203253, "no_speech_prob": 6.240582933969563e-06}, {"id": 235, "seek": 207300, "start": 2090.0, "end": 2092.0, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.12697259585062662, "compression_ratio": 1.2032520325203253, "no_speech_prob": 6.240582933969563e-06}, {"id": 236, "seek": 207300, "start": 2092.0, "end": 2101.0, "text": " So we're now", "tokens": [407, 321, 434, 586], "temperature": 0.0, "avg_logprob": -0.12697259585062662, "compression_ratio": 1.2032520325203253, "no_speech_prob": 6.240582933969563e-06}, {"id": 237, "seek": 210100, "start": 2101.0, "end": 2122.0, "text": " Let's try running it.", "tokens": [961, 311, 853, 2614, 309, 13], "temperature": 0.0, "avg_logprob": -0.32412103017171223, "compression_ratio": 1.0729166666666667, "no_speech_prob": 2.1772046238766052e-05}, {"id": 238, "seek": 210100, "start": 2122.0, "end": 2129.0, "text": " Jeremy, you would increase that number until we no longer get CUDA out of memory.", "tokens": [17809, 11, 291, 576, 3488, 300, 1230, 1826, 321, 572, 2854, 483, 29777, 7509, 484, 295, 4675, 13], "temperature": 0.0, "avg_logprob": -0.32412103017171223, "compression_ratio": 1.0729166666666667, "no_speech_prob": 2.1772046238766052e-05}, {"id": 239, "seek": 212900, "start": 2129.0, "end": 2137.0, "text": " And you could be able to pretty much guess it by looking at like, I mean, you can just,", "tokens": [400, 291, 727, 312, 1075, 281, 1238, 709, 2041, 309, 538, 1237, 412, 411, 11, 286, 914, 11, 291, 393, 445, 11], "temperature": 0.0, "avg_logprob": -0.15372784932454428, "compression_ratio": 1.3533834586466165, "no_speech_prob": 1.8338008885621093e-05}, {"id": 240, "seek": 212900, "start": 2137.0, "end": 2145.0, "text": " once you found a batch size that fits, you know, so the default batch size, I believe is 32.", "tokens": [1564, 291, 1352, 257, 15245, 2744, 300, 9001, 11, 291, 458, 11, 370, 264, 7576, 15245, 2744, 11, 286, 1697, 307, 8858, 13], "temperature": 0.0, "avg_logprob": -0.15372784932454428, "compression_ratio": 1.3533834586466165, "no_speech_prob": 1.8338008885621093e-05}, {"id": 241, "seek": 214500, "start": 2145.0, "end": 2159.0, "text": " Once you find a batch size that fits, sorry, 64 is the default batch size that fits, you're just like, okay, well, if it fits in 32, then I just need to set it to two because 64 divided by two is enough.", "tokens": [3443, 291, 915, 257, 15245, 2744, 300, 9001, 11, 2597, 11, 12145, 307, 264, 7576, 15245, 2744, 300, 9001, 11, 291, 434, 445, 411, 11, 1392, 11, 731, 11, 498, 309, 9001, 294, 8858, 11, 550, 286, 445, 643, 281, 992, 309, 281, 732, 570, 12145, 6666, 538, 732, 307, 1547, 13], "temperature": 0.0, "avg_logprob": -0.11744443108053769, "compression_ratio": 1.5720338983050848, "no_speech_prob": 1.1124321645183954e-05}, {"id": 242, "seek": 214500, "start": 2159.0, "end": 2173.0, "text": " And the key thing I do here is, you know, so I've got this report GPU function. So what I did at home was I just, you know, changed this until it got less than 16 gig.", "tokens": [400, 264, 2141, 551, 286, 360, 510, 307, 11, 291, 458, 11, 370, 286, 600, 658, 341, 2275, 18407, 2445, 13, 407, 437, 286, 630, 412, 1280, 390, 286, 445, 11, 291, 458, 11, 3105, 341, 1826, 309, 658, 1570, 813, 3165, 8741, 13], "temperature": 0.0, "avg_logprob": -0.11744443108053769, "compression_ratio": 1.5720338983050848, "no_speech_prob": 1.1124321645183954e-05}, {"id": 243, "seek": 217300, "start": 2173.0, "end": 2193.0, "text": " And as you can see, I'm just doing like a single APOC on small images. So this ran in, I don't know, 15 seconds or something.", "tokens": [400, 382, 291, 393, 536, 11, 286, 478, 445, 884, 411, 257, 2167, 5372, 30087, 322, 1359, 5267, 13, 407, 341, 5872, 294, 11, 286, 500, 380, 458, 11, 2119, 3949, 420, 746, 13], "temperature": 0.0, "avg_logprob": -0.07749098853061073, "compression_ratio": 1.1061946902654867, "no_speech_prob": 1.593427441548556e-05}, {"id": 244, "seek": 219300, "start": 2193.0, "end": 2207.0, "text": " Yeah, batch size 64 by default.", "tokens": [865, 11, 15245, 2744, 12145, 538, 7576, 13], "temperature": 0.0, "avg_logprob": -0.1443202154976981, "compression_ratio": 1.2535211267605635, "no_speech_prob": 3.500616003293544e-06}, {"id": 245, "seek": 219300, "start": 2207.0, "end": 2220.0, "text": " Yeah, so then I just went through checking the memory use of ConvNextLarge with different image sizes, again, just keeping on using just one APOC.", "tokens": [865, 11, 370, 550, 286, 445, 1437, 807, 8568, 264, 4675, 764, 295, 2656, 85, 31002, 43, 289, 432, 365, 819, 3256, 11602, 11, 797, 11, 445, 5145, 322, 1228, 445, 472, 5372, 30087, 13], "temperature": 0.0, "avg_logprob": -0.1443202154976981, "compression_ratio": 1.2535211267605635, "no_speech_prob": 3.500616003293544e-06}, {"id": 246, "seek": 222000, "start": 2220.0, "end": 2235.0, "text": " And that's how I figured out what I could do to set the QM to work.", "tokens": [400, 300, 311, 577, 286, 8932, 484, 437, 286, 727, 360, 281, 992, 264, 1249, 44, 281, 589, 13], "temperature": 0.0, "avg_logprob": -0.2321504040768272, "compression_ratio": 1.202020202020202, "no_speech_prob": 1.544305996503681e-05}, {"id": 247, "seek": 222000, "start": 2235.0, "end": 2243.0, "text": " All right, so that should be right to save and run.", "tokens": [1057, 558, 11, 370, 300, 820, 312, 558, 281, 3155, 293, 1190, 13], "temperature": 0.0, "avg_logprob": -0.2321504040768272, "compression_ratio": 1.202020202020202, "no_speech_prob": 1.544305996503681e-05}, {"id": 248, "seek": 224300, "start": 2243.0, "end": 2252.0, "text": " And then", "tokens": [400, 550], "temperature": 0.0, "avg_logprob": -0.11753102540969848, "compression_ratio": 1.2777777777777777, "no_speech_prob": 1.4508816093439236e-05}, {"id": 249, "seek": 224300, "start": 2252.0, "end": 2255.0, "text": " turn off this one.", "tokens": [1261, 766, 341, 472, 13], "temperature": 0.0, "avg_logprob": -0.11753102540969848, "compression_ratio": 1.2777777777777777, "no_speech_prob": 1.4508816093439236e-05}, {"id": 250, "seek": 224300, "start": 2255.0, "end": 2264.0, "text": " So when you're running something like you click save version, and you click run, you'll then see it down here.", "tokens": [407, 562, 291, 434, 2614, 746, 411, 291, 2052, 3155, 3037, 11, 293, 291, 2052, 1190, 11, 291, 603, 550, 536, 309, 760, 510, 13], "temperature": 0.0, "avg_logprob": -0.11753102540969848, "compression_ratio": 1.2777777777777777, "no_speech_prob": 1.4508816093439236e-05}, {"id": 251, "seek": 226400, "start": 2264.0, "end": 2274.0, "text": " And that runs in the background, you don't have to leave this open. And so you can go back to it later. So if I just copy that, you can close it.", "tokens": [400, 300, 6676, 294, 264, 3678, 11, 291, 500, 380, 362, 281, 1856, 341, 1269, 13, 400, 370, 291, 393, 352, 646, 281, 309, 1780, 13, 407, 498, 286, 445, 5055, 300, 11, 291, 393, 1998, 309, 13], "temperature": 0.0, "avg_logprob": -0.09236588044600054, "compression_ratio": 1.3656716417910448, "no_speech_prob": 4.637010533770081e-06}, {"id": 252, "seek": 226400, "start": 2274.0, "end": 2283.0, "text": " And if I go to my notebook in Kaggle,", "tokens": [400, 498, 286, 352, 281, 452, 21060, 294, 48751, 22631, 11], "temperature": 0.0, "avg_logprob": -0.09236588044600054, "compression_ratio": 1.3656716417910448, "no_speech_prob": 4.637010533770081e-06}, {"id": 253, "seek": 228300, "start": 2283.0, "end": 2297.0, "text": " this shows me version three or four because version four hasn't finished running yet. So if I click here, I can go to version four and it says, oh, that's still running.", "tokens": [341, 3110, 385, 3037, 1045, 420, 1451, 570, 3037, 1451, 6132, 380, 4335, 2614, 1939, 13, 407, 498, 286, 2052, 510, 11, 286, 393, 352, 281, 3037, 1451, 293, 309, 1619, 11, 1954, 11, 300, 311, 920, 2614, 13], "temperature": 0.0, "avg_logprob": -0.1103752309625799, "compression_ratio": 1.625668449197861, "no_speech_prob": 4.222541519993683e-06}, {"id": 254, "seek": 228300, "start": 2297.0, "end": 2301.0, "text": " And I can see here, it's been running for about a minute.", "tokens": [400, 286, 393, 536, 510, 11, 309, 311, 668, 2614, 337, 466, 257, 3456, 13], "temperature": 0.0, "avg_logprob": -0.1103752309625799, "compression_ratio": 1.625668449197861, "no_speech_prob": 4.222541519993683e-06}, {"id": 255, "seek": 228300, "start": 2301.0, "end": 2309.0, "text": " And it shows me anything that you print out will appear, including warnings.", "tokens": [400, 309, 3110, 385, 1340, 300, 291, 4482, 484, 486, 4204, 11, 3009, 30009, 13], "temperature": 0.0, "avg_logprob": -0.1103752309625799, "compression_ratio": 1.625668449197861, "no_speech_prob": 4.222541519993683e-06}, {"id": 256, "seek": 230900, "start": 2309.0, "end": 2316.0, "text": " So that's what happens in Kaggle.", "tokens": [407, 300, 311, 437, 2314, 294, 48751, 22631, 13], "temperature": 0.0, "avg_logprob": -0.12282011872631009, "compression_ratio": 1.4066666666666667, "no_speech_prob": 6.338332696032012e-06}, {"id": 257, "seek": 230900, "start": 2316.0, "end": 2327.0, "text": " So if we also do the multi objective loss function thing, that would be cool.", "tokens": [407, 498, 321, 611, 360, 264, 4825, 10024, 4470, 2445, 551, 11, 300, 576, 312, 1627, 13], "temperature": 0.0, "avg_logprob": -0.12282011872631009, "compression_ratio": 1.4066666666666667, "no_speech_prob": 6.338332696032012e-06}, {"id": 258, "seek": 230900, "start": 2327.0, "end": 2334.0, "text": " So I thought like next time in our next lesson, broadly speaking,", "tokens": [407, 286, 1194, 411, 958, 565, 294, 527, 958, 6898, 11, 19511, 4124, 11], "temperature": 0.0, "avg_logprob": -0.12282011872631009, "compression_ratio": 1.4066666666666667, "no_speech_prob": 6.338332696032012e-06}, {"id": 259, "seek": 230900, "start": 2334.0, "end": 2337.0, "text": " gosh, this is taking a long time.", "tokens": [6502, 11, 341, 307, 1940, 257, 938, 565, 13], "temperature": 0.0, "avg_logprob": -0.12282011872631009, "compression_ratio": 1.4066666666666667, "no_speech_prob": 6.338332696032012e-06}, {"id": 260, "seek": 233700, "start": 2337.0, "end": 2344.0, "text": " I kind of want to cover like what the inputs to a model look like and what the outputs to a model look like.", "tokens": [286, 733, 295, 528, 281, 2060, 411, 437, 264, 15743, 281, 257, 2316, 574, 411, 293, 437, 264, 23930, 281, 257, 2316, 574, 411, 13], "temperature": 0.0, "avg_logprob": -0.090656660605168, "compression_ratio": 1.7638888888888888, "no_speech_prob": 1.1477049156383146e-05}, {"id": 261, "seek": 233700, "start": 2344.0, "end": 2347.0, "text": " So like in terms of inputs.", "tokens": [407, 411, 294, 2115, 295, 15743, 13], "temperature": 0.0, "avg_logprob": -0.090656660605168, "compression_ratio": 1.7638888888888888, "no_speech_prob": 1.1477049156383146e-05}, {"id": 262, "seek": 233700, "start": 2347.0, "end": 2351.0, "text": " Really the key thing is embeddings.", "tokens": [4083, 264, 2141, 551, 307, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.090656660605168, "compression_ratio": 1.7638888888888888, "no_speech_prob": 1.1477049156383146e-05}, {"id": 263, "seek": 233700, "start": 2351.0, "end": 2358.0, "text": " That's the key thing we haven't seen yet in terms of what model inputs look like.", "tokens": [663, 311, 264, 2141, 551, 321, 2378, 380, 1612, 1939, 294, 2115, 295, 437, 2316, 15743, 574, 411, 13], "temperature": 0.0, "avg_logprob": -0.090656660605168, "compression_ratio": 1.7638888888888888, "no_speech_prob": 1.1477049156383146e-05}, {"id": 264, "seek": 235800, "start": 2358.0, "end": 2372.0, "text": " For model outputs, I think we need to look at softmax.", "tokens": [1171, 2316, 23930, 11, 286, 519, 321, 643, 281, 574, 412, 2787, 41167, 13], "temperature": 0.0, "avg_logprob": -0.21017566408429827, "compression_ratio": 1.1411764705882352, "no_speech_prob": 3.44625232173712e-06}, {"id": 265, "seek": 235800, "start": 2372.0, "end": 2375.0, "text": " Softmax.", "tokens": [16985, 41167, 13], "temperature": 0.0, "avg_logprob": -0.21017566408429827, "compression_ratio": 1.1411764705882352, "no_speech_prob": 3.44625232173712e-06}, {"id": 266, "seek": 235800, "start": 2375.0, "end": 2378.0, "text": " Cross entropy loss.", "tokens": [11623, 30867, 4470, 13], "temperature": 0.0, "avg_logprob": -0.21017566408429827, "compression_ratio": 1.1411764705882352, "no_speech_prob": 3.44625232173712e-06}, {"id": 267, "seek": 235800, "start": 2378.0, "end": 2384.0, "text": " Entropy loss.", "tokens": [3951, 27514, 4470, 13], "temperature": 0.0, "avg_logprob": -0.21017566408429827, "compression_ratio": 1.1411764705882352, "no_speech_prob": 3.44625232173712e-06}, {"id": 268, "seek": 238400, "start": 2384.0, "end": 2390.0, "text": " And then, you know, our multi target loss,", "tokens": [400, 550, 11, 291, 458, 11, 527, 4825, 3779, 4470, 11], "temperature": 0.0, "avg_logprob": -0.11439664023263114, "compression_ratio": 1.4262295081967213, "no_speech_prob": 5.862018952029757e-06}, {"id": 269, "seek": 238400, "start": 2390.0, "end": 2395.0, "text": " which we could do first, kind of a segue.", "tokens": [597, 321, 727, 360, 700, 11, 733, 295, 257, 33850, 13], "temperature": 0.0, "avg_logprob": -0.11439664023263114, "compression_ratio": 1.4262295081967213, "no_speech_prob": 5.862018952029757e-06}, {"id": 270, "seek": 238400, "start": 2395.0, "end": 2406.0, "text": " So maybe in terms of the ordering, the segue would be like doing multi target loss first.", "tokens": [407, 1310, 294, 2115, 295, 264, 21739, 11, 264, 33850, 576, 312, 411, 884, 4825, 3779, 4470, 700, 13], "temperature": 0.0, "avg_logprob": -0.11439664023263114, "compression_ratio": 1.4262295081967213, "no_speech_prob": 5.862018952029757e-06}, {"id": 271, "seek": 240600, "start": 2406.0, "end": 2418.0, "text": " And we could talk about softmax and cross entropy, which would then lead us potentially to like looking at the bear classifier.", "tokens": [400, 321, 727, 751, 466, 2787, 41167, 293, 3278, 30867, 11, 597, 576, 550, 1477, 505, 7263, 281, 411, 1237, 412, 264, 6155, 1508, 9902, 13], "temperature": 0.0, "avg_logprob": -0.06821522345909706, "compression_ratio": 1.352112676056338, "no_speech_prob": 3.237514647480566e-06}, {"id": 272, "seek": 240600, "start": 2418.0, "end": 2423.0, "text": " What if there's no bears?", "tokens": [708, 498, 456, 311, 572, 17276, 30], "temperature": 0.0, "avg_logprob": -0.06821522345909706, "compression_ratio": 1.352112676056338, "no_speech_prob": 3.237514647480566e-06}, {"id": 273, "seek": 240600, "start": 2423.0, "end": 2429.0, "text": " So we can just use the binary sigmoid.", "tokens": [407, 321, 393, 445, 764, 264, 17434, 4556, 3280, 327, 13], "temperature": 0.0, "avg_logprob": -0.06821522345909706, "compression_ratio": 1.352112676056338, "no_speech_prob": 3.237514647480566e-06}, {"id": 274, "seek": 242900, "start": 2429.0, "end": 2445.0, "text": " So then for embeddings, I guess that's where we'd cover the collaborative filtering, collaborative filtering, because that's like a really nice version of embeddings.", "tokens": [407, 550, 337, 12240, 29432, 11, 286, 2041, 300, 311, 689, 321, 1116, 2060, 264, 16555, 30822, 11, 16555, 30822, 11, 570, 300, 311, 411, 257, 534, 1481, 3037, 295, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.116118032282049, "compression_ratio": 1.6232876712328768, "no_speech_prob": 5.506822617462603e-06}, {"id": 275, "seek": 242900, "start": 2445.0, "end": 2450.0, "text": " So I guess the question is, for those who have done the course before,", "tokens": [407, 286, 2041, 264, 1168, 307, 11, 337, 729, 567, 362, 1096, 264, 1164, 949, 11], "temperature": 0.0, "avg_logprob": -0.116118032282049, "compression_ratio": 1.6232876712328768, "no_speech_prob": 5.506822617462603e-06}, {"id": 276, "seek": 245000, "start": 2450.0, "end": 2460.0, "text": " are there any other topics, I guess like time permitting, it would be nice to look at like the confnet, what a confnet is.", "tokens": [366, 456, 604, 661, 8378, 11, 286, 2041, 411, 565, 4784, 2414, 11, 309, 576, 312, 1481, 281, 574, 412, 411, 264, 1497, 7129, 11, 437, 257, 1497, 7129, 307, 13], "temperature": 0.0, "avg_logprob": -0.16670180621900058, "compression_ratio": 1.452513966480447, "no_speech_prob": 2.110845707647968e-05}, {"id": 277, "seek": 245000, "start": 2460.0, "end": 2473.0, "text": " Just kind of so that's all right. Then we've got like the outputs, the inputs, and then the middle.", "tokens": [1449, 733, 295, 370, 300, 311, 439, 558, 13, 1396, 321, 600, 658, 411, 264, 23930, 11, 264, 15743, 11, 293, 550, 264, 2808, 13], "temperature": 0.0, "avg_logprob": -0.16670180621900058, "compression_ratio": 1.452513966480447, "no_speech_prob": 2.110845707647968e-05}, {"id": 278, "seek": 245000, "start": 2473.0, "end": 2476.0, "text": " What about more NLP stuff?", "tokens": [708, 466, 544, 426, 45196, 1507, 30], "temperature": 0.0, "avg_logprob": -0.16670180621900058, "compression_ratio": 1.452513966480447, "no_speech_prob": 2.110845707647968e-05}, {"id": 279, "seek": 245000, "start": 2476.0, "end": 2478.0, "text": " Like what?", "tokens": [1743, 437, 30], "temperature": 0.0, "avg_logprob": -0.16670180621900058, "compression_ratio": 1.452513966480447, "no_speech_prob": 2.110845707647968e-05}, {"id": 280, "seek": 247800, "start": 2478.0, "end": 2486.0, "text": " Well, I've heard that I can face is getting integrated with past AI, maybe looking at that how it works.", "tokens": [1042, 11, 286, 600, 2198, 300, 286, 393, 1851, 307, 1242, 10919, 365, 1791, 7318, 11, 1310, 1237, 412, 300, 577, 309, 1985, 13], "temperature": 0.0, "avg_logprob": -0.17859973436520424, "compression_ratio": 1.4818652849740932, "no_speech_prob": 5.224788037594408e-05}, {"id": 281, "seek": 247800, "start": 2486.0, "end": 2493.0, "text": " Well, it's not done yet. So we can't do that yet. But definitely in part two.", "tokens": [1042, 11, 309, 311, 406, 1096, 1939, 13, 407, 321, 393, 380, 360, 300, 1939, 13, 583, 2138, 294, 644, 732, 13], "temperature": 0.0, "avg_logprob": -0.17859973436520424, "compression_ratio": 1.4818652849740932, "no_speech_prob": 5.224788037594408e-05}, {"id": 282, "seek": 247800, "start": 2493.0, "end": 2499.0, "text": " I've got a question. I don't know if it's helpful, but there's a lot of emphasis on outputs and inputs.", "tokens": [286, 600, 658, 257, 1168, 13, 286, 500, 380, 458, 498, 309, 311, 4961, 11, 457, 456, 311, 257, 688, 295, 16271, 322, 23930, 293, 15743, 13], "temperature": 0.0, "avg_logprob": -0.17859973436520424, "compression_ratio": 1.4818652849740932, "no_speech_prob": 5.224788037594408e-05}, {"id": 283, "seek": 249900, "start": 2499.0, "end": 2511.0, "text": " But like in the middle, just understanding like the outputs of a hidden layer, whether they're going awry or not, how do you debug that? How do you understand, you know, when to kind of look at that?", "tokens": [583, 411, 294, 264, 2808, 11, 445, 3701, 411, 264, 23930, 295, 257, 7633, 4583, 11, 1968, 436, 434, 516, 1714, 627, 420, 406, 11, 577, 360, 291, 24083, 300, 30, 1012, 360, 291, 1223, 11, 291, 458, 11, 562, 281, 733, 295, 574, 412, 300, 30], "temperature": 0.0, "avg_logprob": -0.11421283431675124, "compression_ratio": 1.608294930875576, "no_speech_prob": 4.222527877573157e-06}, {"id": 284, "seek": 249900, "start": 2511.0, "end": 2515.0, "text": " Yeah, very helpful.", "tokens": [865, 11, 588, 4961, 13], "temperature": 0.0, "avg_logprob": -0.11421283431675124, "compression_ratio": 1.608294930875576, "no_speech_prob": 4.222527877573157e-06}, {"id": 285, "seek": 249900, "start": 2515.0, "end": 2523.0, "text": " Last time we did a part two, we did a very deep dive into that. And I think we should do that again in our part two, because like", "tokens": [5264, 565, 321, 630, 257, 644, 732, 11, 321, 630, 257, 588, 2452, 9192, 666, 300, 13, 400, 286, 519, 321, 820, 360, 300, 797, 294, 527, 644, 732, 11, 570, 411], "temperature": 0.0, "avg_logprob": -0.11421283431675124, "compression_ratio": 1.608294930875576, "no_speech_prob": 4.222527877573157e-06}, {"id": 286, "seek": 252300, "start": 2523.0, "end": 2538.0, "text": " most people won't have to debug that because if you're using an off the shelf model, you know, like it's, you know, with off the shelf initializations, that shouldn't happen.", "tokens": [881, 561, 1582, 380, 362, 281, 24083, 300, 570, 498, 291, 434, 1228, 364, 766, 264, 15222, 2316, 11, 291, 458, 11, 411, 309, 311, 11, 291, 458, 11, 365, 766, 264, 15222, 5883, 14455, 11, 300, 4659, 380, 1051, 13], "temperature": 0.0, "avg_logprob": -0.08407891217400046, "compression_ratio": 1.5906976744186045, "no_speech_prob": 5.771458290837472e-06}, {"id": 287, "seek": 252300, "start": 2538.0, "end": 2543.0, "text": " So it's probably more of an advanced debugging technique, I would say.", "tokens": [407, 309, 311, 1391, 544, 295, 364, 7339, 45592, 6532, 11, 286, 576, 584, 13], "temperature": 0.0, "avg_logprob": -0.08407891217400046, "compression_ratio": 1.5906976744186045, "no_speech_prob": 5.771458290837472e-06}, {"id": 288, "seek": 252300, "start": 2543.0, "end": 2549.0, "text": " But yeah, if you're interested in looking at it now, definitely check out our previous part two.", "tokens": [583, 1338, 11, 498, 291, 434, 3102, 294, 1237, 412, 309, 586, 11, 2138, 1520, 484, 527, 3894, 644, 732, 13], "temperature": 0.0, "avg_logprob": -0.08407891217400046, "compression_ratio": 1.5906976744186045, "no_speech_prob": 5.771458290837472e-06}, {"id": 289, "seek": 254900, "start": 2549.0, "end": 2561.0, "text": " Because we did a very deep dive into that and developed the so forth, colorful dimension plot, which is absolutely great for that.", "tokens": [1436, 321, 630, 257, 588, 2452, 9192, 666, 300, 293, 4743, 264, 370, 5220, 11, 18506, 10139, 7542, 11, 597, 307, 3122, 869, 337, 300, 13], "temperature": 0.0, "avg_logprob": -0.19942143845231566, "compression_ratio": 1.5228426395939085, "no_speech_prob": 4.610137693816796e-05}, {"id": 290, "seek": 254900, "start": 2561.0, "end": 2566.0, "text": " Jeremy? What about deep learning on tabular data?", "tokens": [17809, 30, 708, 466, 2452, 2539, 322, 4421, 1040, 1412, 30], "temperature": 0.0, "avg_logprob": -0.19942143845231566, "compression_ratio": 1.5228426395939085, "no_speech_prob": 4.610137693816796e-05}, {"id": 291, "seek": 254900, "start": 2566.0, "end": 2572.0, "text": " Yeah, so that would exactly so collaborative filtering would lead us exactly into that. Thank you.", "tokens": [865, 11, 370, 300, 576, 2293, 370, 16555, 30822, 576, 1477, 505, 2293, 666, 300, 13, 1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.19942143845231566, "compression_ratio": 1.5228426395939085, "no_speech_prob": 4.610137693816796e-05}, {"id": 292, "seek": 254900, "start": 2572.0, "end": 2575.0, "text": " Yeah, sorry, Serata.", "tokens": [865, 11, 2597, 11, 4210, 3274, 13], "temperature": 0.0, "avg_logprob": -0.19942143845231566, "compression_ratio": 1.5228426395939085, "no_speech_prob": 4.610137693816796e-05}, {"id": 293, "seek": 257500, "start": 2575.0, "end": 2585.0, "text": " Do you mind to spend five minutes talking about the importance of the ethical side? At least you point to the resources Rachel prepared before.", "tokens": [1144, 291, 1575, 281, 3496, 1732, 2077, 1417, 466, 264, 7379, 295, 264, 18890, 1252, 30, 1711, 1935, 291, 935, 281, 264, 3593, 14246, 4927, 949, 13], "temperature": 0.0, "avg_logprob": -0.20006439502422627, "compression_ratio": 1.4, "no_speech_prob": 8.610359509475529e-05}, {"id": 294, "seek": 257500, "start": 2585.0, "end": 2593.0, "text": " So I think that would make people, because it's so easy to build a model, but how to apply is getting more scary now.", "tokens": [407, 286, 519, 300, 576, 652, 561, 11, 570, 309, 311, 370, 1858, 281, 1322, 257, 2316, 11, 457, 577, 281, 3079, 307, 1242, 544, 6958, 586, 13], "temperature": 0.0, "avg_logprob": -0.20006439502422627, "compression_ratio": 1.4, "no_speech_prob": 8.610359509475529e-05}, {"id": 295, "seek": 257500, "start": 2593.0, "end": 2595.0, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.20006439502422627, "compression_ratio": 1.4, "no_speech_prob": 8.610359509475529e-05}, {"id": 296, "seek": 259500, "start": 2595.0, "end": 2607.0, "text": " Yes, I mentioned in lesson one, the data ethics course but you're right, it would be nice to kind of like touch on something there wouldn't it.", "tokens": [1079, 11, 286, 2835, 294, 6898, 472, 11, 264, 1412, 19769, 1164, 457, 291, 434, 558, 11, 309, 576, 312, 1481, 281, 733, 295, 411, 2557, 322, 746, 456, 2759, 380, 309, 13], "temperature": 0.0, "avg_logprob": -0.18232724977576215, "compression_ratio": 1.3059701492537314, "no_speech_prob": 1.4966477465350181e-05}, {"id": 297, "seek": 259500, "start": 2607.0, "end": 2613.0, "text": " By Rachel from part one before.", "tokens": [3146, 14246, 490, 644, 472, 949, 13], "temperature": 0.0, "avg_logprob": -0.18232724977576215, "compression_ratio": 1.3059701492537314, "no_speech_prob": 1.4966477465350181e-05}, {"id": 298, "seek": 261300, "start": 2613.0, "end": 2630.0, "text": " Yeah, I mean that, I mean, okay, I mean that actually would be a great thing just to talk about, you know, that that lecture is not at all out of date. So,", "tokens": [865, 11, 286, 914, 300, 11, 286, 914, 11, 1392, 11, 286, 914, 300, 767, 576, 312, 257, 869, 551, 445, 281, 751, 466, 11, 291, 458, 11, 300, 300, 7991, 307, 406, 412, 439, 484, 295, 4002, 13, 407, 11], "temperature": 0.0, "avg_logprob": -0.18047523498535156, "compression_ratio": 1.3559322033898304, "no_speech_prob": 2.7504023819346912e-05}, {"id": 299, "seek": 261300, "start": 2630.0, "end": 2632.0, "text": " yes.", "tokens": [2086, 13], "temperature": 0.0, "avg_logprob": -0.18047523498535156, "compression_ratio": 1.3559322033898304, "no_speech_prob": 2.7504023819346912e-05}, {"id": 300, "seek": 263200, "start": 2632.0, "end": 2644.0, "text": " So maybe touch on it in this one, and also talk link to, you know, for varying levels of interest.", "tokens": [407, 1310, 2557, 322, 309, 294, 341, 472, 11, 293, 611, 751, 2113, 281, 11, 291, 458, 11, 337, 22984, 4358, 295, 1179, 13], "temperature": 0.0, "avg_logprob": -0.16967183265133182, "compression_ratio": 1.4917127071823204, "no_speech_prob": 2.7500880605657585e-05}, {"id": 301, "seek": 263200, "start": 2644.0, "end": 2661.0, "text": " The two hour version would be Rachel's talk in the 2020 lecture, and then deeper interest still would be the, yes, the full ethics course, that's a great point. Thank you.", "tokens": [440, 732, 1773, 3037, 576, 312, 14246, 311, 751, 294, 264, 4808, 7991, 11, 293, 550, 7731, 1179, 920, 576, 312, 264, 11, 2086, 11, 264, 1577, 19769, 1164, 11, 300, 311, 257, 869, 935, 13, 1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.16967183265133182, "compression_ratio": 1.4917127071823204, "no_speech_prob": 2.7500880605657585e-05}, {"id": 302, "seek": 266100, "start": 2661.0, "end": 2671.0, "text": " So then, for, for actually pretty much all of these things.", "tokens": [407, 550, 11, 337, 11, 337, 767, 1238, 709, 439, 295, 613, 721, 13], "temperature": 0.0, "avg_logprob": -0.1537868082523346, "compression_ratio": 1.10989010989011, "no_speech_prob": 3.319663301226683e-05}, {"id": 303, "seek": 266100, "start": 2671.0, "end": 2680.0, "text": " We have Excel spreadsheets,", "tokens": [492, 362, 19060, 23651, 1385, 11], "temperature": 0.0, "avg_logprob": -0.1537868082523346, "compression_ratio": 1.10989010989011, "no_speech_prob": 3.319663301226683e-05}, {"id": 304, "seek": 266100, "start": 2680.0, "end": 2688.0, "text": " which is fun.", "tokens": [597, 307, 1019, 13], "temperature": 0.0, "avg_logprob": -0.1537868082523346, "compression_ratio": 1.10989010989011, "no_speech_prob": 3.319663301226683e-05}, {"id": 305, "seek": 268800, "start": 2688.0, "end": 2699.0, "text": " So there's, let's have a look, collaborative filtering.", "tokens": [407, 456, 311, 11, 718, 311, 362, 257, 574, 11, 16555, 30822, 13], "temperature": 0.0, "avg_logprob": -0.14930675769674367, "compression_ratio": 1.0869565217391304, "no_speech_prob": 4.830840043723583e-05}, {"id": 306, "seek": 268800, "start": 2699.0, "end": 2706.0, "text": " Oh, looks like I've already downloaded that.", "tokens": [876, 11, 1542, 411, 286, 600, 1217, 21748, 300, 13], "temperature": 0.0, "avg_logprob": -0.14930675769674367, "compression_ratio": 1.0869565217391304, "no_speech_prob": 4.830840043723583e-05}, {"id": 307, "seek": 270600, "start": 2706.0, "end": 2724.0, "text": " Sure, I will encourage you to continue teaching in Excel. Yesterday I on the panel in a data science conference and when I mentioned, I start with Excel actually inspire a lot of people, they want to have a go with data science and learning it.", "tokens": [4894, 11, 286, 486, 5373, 291, 281, 2354, 4571, 294, 19060, 13, 19765, 286, 322, 264, 4831, 294, 257, 1412, 3497, 7586, 293, 562, 286, 2835, 11, 286, 722, 365, 19060, 767, 15638, 257, 688, 295, 561, 11, 436, 528, 281, 362, 257, 352, 365, 1412, 3497, 293, 2539, 309, 13], "temperature": 0.0, "avg_logprob": -0.27314605712890627, "compression_ratio": 1.4787878787878788, "no_speech_prob": 4.610347968991846e-05}, {"id": 308, "seek": 272400, "start": 2724.0, "end": 2745.0, "text": " So, good feedback. Yeah, because there's certainly some people who don't find it useful at all. And they tend to be quite loud about it so it's certainly nice to hear that that feedback.", "tokens": [407, 11, 665, 5824, 13, 865, 11, 570, 456, 311, 3297, 512, 561, 567, 500, 380, 915, 309, 4420, 412, 439, 13, 400, 436, 3928, 281, 312, 1596, 6588, 466, 309, 370, 309, 311, 3297, 1481, 281, 1568, 300, 300, 5824, 13], "temperature": 0.0, "avg_logprob": -0.2174325649554913, "compression_ratio": 1.50920245398773, "no_speech_prob": 2.5060857296921313e-05}, {"id": 309, "seek": 272400, "start": 2745.0, "end": 2748.0, "text": " Sorry.", "tokens": [4919, 13], "temperature": 0.0, "avg_logprob": -0.2174325649554913, "compression_ratio": 1.50920245398773, "no_speech_prob": 2.5060857296921313e-05}, {"id": 310, "seek": 272400, "start": 2748.0, "end": 2751.0, "text": " So I thought you didn't let those people get to you.", "tokens": [407, 286, 1194, 291, 994, 380, 718, 729, 561, 483, 281, 291, 13], "temperature": 0.0, "avg_logprob": -0.2174325649554913, "compression_ratio": 1.50920245398773, "no_speech_prob": 2.5060857296921313e-05}, {"id": 311, "seek": 275100, "start": 2751.0, "end": 2758.0, "text": " Well, I only pretend that anybody doesn't get to me.", "tokens": [1042, 11, 286, 787, 11865, 300, 4472, 1177, 380, 483, 281, 385, 13], "temperature": 0.0, "avg_logprob": -0.21643374717398867, "compression_ratio": 1.4946808510638299, "no_speech_prob": 7.595507486257702e-05}, {"id": 312, "seek": 275100, "start": 2758.0, "end": 2762.0, "text": " I was going to say that's that was really great to see.", "tokens": [286, 390, 516, 281, 584, 300, 311, 300, 390, 534, 869, 281, 536, 13], "temperature": 0.0, "avg_logprob": -0.21643374717398867, "compression_ratio": 1.4946808510638299, "no_speech_prob": 7.595507486257702e-05}, {"id": 313, "seek": 275100, "start": 2762.0, "end": 2765.0, "text": " I've only seen it done once before.", "tokens": [286, 600, 787, 1612, 309, 1096, 1564, 949, 13], "temperature": 0.0, "avg_logprob": -0.21643374717398867, "compression_ratio": 1.4946808510638299, "no_speech_prob": 7.595507486257702e-05}, {"id": 314, "seek": 275100, "start": 2765.0, "end": 2776.0, "text": " And that was in a physicist in Belgium who explained relative transfer modeling using Excel, and it was just so nice to see the clarity.", "tokens": [400, 300, 390, 294, 257, 42466, 294, 28094, 567, 8825, 4972, 5003, 15983, 1228, 19060, 11, 293, 309, 390, 445, 370, 1481, 281, 536, 264, 16992, 13], "temperature": 0.0, "avg_logprob": -0.21643374717398867, "compression_ratio": 1.4946808510638299, "no_speech_prob": 7.595507486257702e-05}, {"id": 315, "seek": 277600, "start": 2776.0, "end": 2784.0, "text": " Okay, great. Okay, thank you. I will.", "tokens": [1033, 11, 869, 13, 1033, 11, 1309, 291, 13, 286, 486, 13], "temperature": 0.0, "avg_logprob": -0.17895427243462925, "compression_ratio": 1.2805755395683454, "no_speech_prob": 1.9208542653359473e-05}, {"id": 316, "seek": 277600, "start": 2784.0, "end": 2791.0, "text": " Let's see, so we've got.", "tokens": [961, 311, 536, 11, 370, 321, 600, 658, 13], "temperature": 0.0, "avg_logprob": -0.17895427243462925, "compression_ratio": 1.2805755395683454, "no_speech_prob": 1.9208542653359473e-05}, {"id": 317, "seek": 277600, "start": 2791.0, "end": 2800.0, "text": " So I think these are actually from the 2019 course faster I one courses to one.", "tokens": [407, 286, 519, 613, 366, 767, 490, 264, 6071, 1164, 4663, 286, 472, 7712, 281, 472, 13], "temperature": 0.0, "avg_logprob": -0.17895427243462925, "compression_ratio": 1.2805755395683454, "no_speech_prob": 1.9208542653359473e-05}, {"id": 318, "seek": 277600, "start": 2800.0, "end": 2805.0, "text": " So I'm just going to grab them all.", "tokens": [407, 286, 478, 445, 516, 281, 4444, 552, 439, 13], "temperature": 0.0, "avg_logprob": -0.17895427243462925, "compression_ratio": 1.2805755395683454, "no_speech_prob": 1.9208542653359473e-05}, {"id": 319, "seek": 280500, "start": 2805.0, "end": 2808.0, "text": " I don't think we're going to cover this year.", "tokens": [286, 500, 380, 519, 321, 434, 516, 281, 2060, 341, 1064, 13], "temperature": 0.0, "avg_logprob": -0.11466542605696053, "compression_ratio": 1.5392156862745099, "no_speech_prob": 0.0002623648033477366}, {"id": 320, "seek": 280500, "start": 2808.0, "end": 2820.0, "text": " This part one that we will cover part two is like different optimizers like momentum and Adam and stuff, but I think that's okay because I feel like nowadays.", "tokens": [639, 644, 472, 300, 321, 486, 2060, 644, 732, 307, 411, 819, 5028, 22525, 411, 11244, 293, 7938, 293, 1507, 11, 457, 286, 519, 300, 311, 1392, 570, 286, 841, 411, 13434, 13], "temperature": 0.0, "avg_logprob": -0.11466542605696053, "compression_ratio": 1.5392156862745099, "no_speech_prob": 0.0002623648033477366}, {"id": 321, "seek": 280500, "start": 2820.0, "end": 2823.0, "text": " Just use the default Adam W and it works.", "tokens": [1449, 764, 264, 7576, 7938, 343, 293, 309, 1985, 13], "temperature": 0.0, "avg_logprob": -0.11466542605696053, "compression_ratio": 1.5392156862745099, "no_speech_prob": 0.0002623648033477366}, {"id": 322, "seek": 280500, "start": 2823.0, "end": 2826.0, "text": " So I don't. I think it's fine.", "tokens": [407, 286, 500, 380, 13, 286, 519, 309, 311, 2489, 13], "temperature": 0.0, "avg_logprob": -0.11466542605696053, "compression_ratio": 1.5392156862745099, "no_speech_prob": 0.0002623648033477366}, {"id": 323, "seek": 280500, "start": 2826.0, "end": 2831.0, "text": " Not to know too much more than that.", "tokens": [1726, 281, 458, 886, 709, 544, 813, 300, 13], "temperature": 0.0, "avg_logprob": -0.11466542605696053, "compression_ratio": 1.5392156862745099, "no_speech_prob": 0.0002623648033477366}, {"id": 324, "seek": 283100, "start": 2831.0, "end": 2839.0, "text": " It's, it's a little bit of a technicality nowadays. Yeah.", "tokens": [467, 311, 11, 309, 311, 257, 707, 857, 295, 257, 6191, 507, 13434, 13, 865, 13], "temperature": 0.0, "avg_logprob": -0.13175841300718247, "compression_ratio": 1.484472049689441, "no_speech_prob": 6.203480006661266e-05}, {"id": 325, "seek": 283100, "start": 2839.0, "end": 2851.0, "text": " It used to be something we did in one of the first lessons you know but that was when you kind of had to know it right because yours fiddled around with momentum and blah blah blah.", "tokens": [467, 1143, 281, 312, 746, 321, 630, 294, 472, 295, 264, 700, 8820, 291, 458, 457, 300, 390, 562, 291, 733, 295, 632, 281, 458, 309, 558, 570, 6342, 283, 14273, 1493, 926, 365, 11244, 293, 12288, 12288, 12288, 13], "temperature": 0.0, "avg_logprob": -0.13175841300718247, "compression_ratio": 1.484472049689441, "no_speech_prob": 6.203480006661266e-05}, {"id": 326, "seek": 285100, "start": 2851.0, "end": 2864.0, "text": " And to me, always like the biggest thing when starting with something is to have to, you know, once I figure out how to read in the data, then things.", "tokens": [400, 281, 385, 11, 1009, 411, 264, 3880, 551, 562, 2891, 365, 746, 307, 281, 362, 281, 11, 291, 458, 11, 1564, 286, 2573, 484, 577, 281, 1401, 294, 264, 1412, 11, 550, 721, 13], "temperature": 0.0, "avg_logprob": -0.24080413427108374, "compression_ratio": 1.3274336283185841, "no_speech_prob": 5.823327592224814e-05}, {"id": 327, "seek": 286400, "start": 2864.0, "end": 2885.0, "text": " I'm really grateful that there's such an emphasis in this edition of the course on the reading, you know, data, and you know, with similar data, that is something that I would also stay on the lookout for just understanding better reading the data.", "tokens": [286, 478, 534, 7941, 300, 456, 311, 1270, 364, 16271, 294, 341, 11377, 295, 264, 1164, 322, 264, 3760, 11, 291, 458, 11, 1412, 11, 293, 291, 458, 11, 365, 2531, 1412, 11, 300, 307, 746, 300, 286, 576, 611, 1754, 322, 264, 41025, 337, 445, 3701, 1101, 3760, 264, 1412, 13], "temperature": 0.0, "avg_logprob": -0.21186017990112305, "compression_ratio": 1.5308641975308641, "no_speech_prob": 5.643403346766718e-05}, {"id": 328, "seek": 288500, "start": 2885.0, "end": 2901.0, "text": " Great. I don't think we did this one anymore, because we kind of have better versions in in Jupiter, with iPod widgets so we've got this fun.", "tokens": [3769, 13, 286, 500, 380, 519, 321, 630, 341, 472, 3602, 11, 570, 321, 733, 295, 362, 1101, 9606, 294, 294, 24567, 11, 365, 5180, 378, 43355, 370, 321, 600, 658, 341, 1019, 13], "temperature": 0.0, "avg_logprob": -0.1733950862178096, "compression_ratio": 1.3066666666666666, "no_speech_prob": 8.011780664674006e-06}, {"id": 329, "seek": 288500, "start": 2901.0, "end": 2906.0, "text": " Convolutions example.", "tokens": [2656, 85, 15892, 1365, 13], "temperature": 0.0, "avg_logprob": -0.1733950862178096, "compression_ratio": 1.3066666666666666, "no_speech_prob": 8.011780664674006e-06}, {"id": 330, "seek": 288500, "start": 2906.0, "end": 2911.0, "text": " Which I think is still valuable.", "tokens": [3013, 286, 519, 307, 920, 8263, 13], "temperature": 0.0, "avg_logprob": -0.1733950862178096, "compression_ratio": 1.3066666666666666, "no_speech_prob": 8.011780664674006e-06}, {"id": 331, "seek": 291100, "start": 2911.0, "end": 2921.0, "text": " Okay, we've got soft max and cross entropy examples.", "tokens": [1033, 11, 321, 600, 658, 2787, 11469, 293, 3278, 30867, 5110, 13], "temperature": 0.0, "avg_logprob": -0.21561810705396864, "compression_ratio": 1.2123893805309736, "no_speech_prob": 3.267181818955578e-05}, {"id": 332, "seek": 291100, "start": 2921.0, "end": 2927.0, "text": " And we've got collaborative filtering.", "tokens": [400, 321, 600, 658, 16555, 30822, 13], "temperature": 0.0, "avg_logprob": -0.21561810705396864, "compression_ratio": 1.2123893805309736, "no_speech_prob": 3.267181818955578e-05}, {"id": 333, "seek": 291100, "start": 2927.0, "end": 2932.0, "text": " That sounds interesting. Wonder what that is.", "tokens": [663, 3263, 1880, 13, 13224, 437, 300, 307, 13], "temperature": 0.0, "avg_logprob": -0.21561810705396864, "compression_ratio": 1.2123893805309736, "no_speech_prob": 3.267181818955578e-05}, {"id": 334, "seek": 293200, "start": 2932.0, "end": 2945.0, "text": " And then, also we've got word embeddings.", "tokens": [400, 550, 11, 611, 321, 600, 658, 1349, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.2000877939421555, "compression_ratio": 1.452054794520548, "no_speech_prob": 2.0460120140342042e-05}, {"id": 335, "seek": 293200, "start": 2945.0, "end": 2950.0, "text": " All right, and but these are such a cool and important subject.", "tokens": [1057, 558, 11, 293, 457, 613, 366, 1270, 257, 1627, 293, 1021, 3983, 13], "temperature": 0.0, "avg_logprob": -0.2000877939421555, "compression_ratio": 1.452054794520548, "no_speech_prob": 2.0460120140342042e-05}, {"id": 336, "seek": 293200, "start": 2950.0, "end": 2959.0, "text": " And it's something that we haven't discussed that much in this course. No, we haven't touched them at all.", "tokens": [400, 309, 311, 746, 300, 321, 2378, 380, 7152, 300, 709, 294, 341, 1164, 13, 883, 11, 321, 2378, 380, 9828, 552, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.2000877939421555, "compression_ratio": 1.452054794520548, "no_speech_prob": 2.0460120140342042e-05}, {"id": 337, "seek": 295900, "start": 2959.0, "end": 2962.0, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.09929515293666295, "compression_ratio": 1.3831168831168832, "no_speech_prob": 5.911143307457678e-05}, {"id": 338, "seek": 295900, "start": 2962.0, "end": 2967.0, "text": " It feels like a lot to cover.", "tokens": [467, 3417, 411, 257, 688, 281, 2060, 13], "temperature": 0.0, "avg_logprob": -0.09929515293666295, "compression_ratio": 1.3831168831168832, "no_speech_prob": 5.911143307457678e-05}, {"id": 339, "seek": 295900, "start": 2967.0, "end": 2970.0, "text": " We will.", "tokens": [492, 486, 13], "temperature": 0.0, "avg_logprob": -0.09929515293666295, "compression_ratio": 1.3831168831168832, "no_speech_prob": 5.911143307457678e-05}, {"id": 340, "seek": 295900, "start": 2970.0, "end": 2973.0, "text": " We will do our best.", "tokens": [492, 486, 360, 527, 1151, 13], "temperature": 0.0, "avg_logprob": -0.09929515293666295, "compression_ratio": 1.3831168831168832, "no_speech_prob": 5.911143307457678e-05}, {"id": 341, "seek": 295900, "start": 2973.0, "end": 2974.0, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.09929515293666295, "compression_ratio": 1.3831168831168832, "no_speech_prob": 5.911143307457678e-05}, {"id": 342, "seek": 295900, "start": 2974.0, "end": 2985.0, "text": " I think we're up to our hour so thanks everybody. Nice chat today, and I will get to work on putting this together.", "tokens": [286, 519, 321, 434, 493, 281, 527, 1773, 370, 3231, 2201, 13, 5490, 5081, 965, 11, 293, 286, 486, 483, 281, 589, 322, 3372, 341, 1214, 13], "temperature": 0.0, "avg_logprob": -0.09929515293666295, "compression_ratio": 1.3831168831168832, "no_speech_prob": 5.911143307457678e-05}, {"id": 343, "seek": 295900, "start": 2985.0, "end": 2987.0, "text": " Have a nice weekend.", "tokens": [3560, 257, 1481, 6711, 13], "temperature": 0.0, "avg_logprob": -0.09929515293666295, "compression_ratio": 1.3831168831168832, "no_speech_prob": 5.911143307457678e-05}, {"id": 344, "seek": 298700, "start": 2987.0, "end": 2990.0, "text": " Thank you so much.", "tokens": [1044, 291, 370, 709, 13], "temperature": 0.4, "avg_logprob": -0.3944981294519761, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.00024092105741146952}, {"id": 345, "seek": 298700, "start": 2990.0, "end": 2991.0, "text": " Thanks.", "tokens": [2561, 13], "temperature": 0.4, "avg_logprob": -0.3944981294519761, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.00024092105741146952}, {"id": 346, "seek": 298700, "start": 2991.0, "end": 2993.0, "text": " Remind everyone.", "tokens": [4080, 471, 1518, 13], "temperature": 0.4, "avg_logprob": -0.3944981294519761, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.00024092105741146952}, {"id": 347, "seek": 298700, "start": 2993.0, "end": 2998.0, "text": " This is a waste and bias video today.", "tokens": [639, 307, 257, 5964, 293, 12577, 960, 965, 13], "temperature": 0.4, "avg_logprob": -0.3944981294519761, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.00024092105741146952}, {"id": 348, "seek": 298700, "start": 2998.0, "end": 3003.0, "text": " I think 6 o'clock this one time so with anyone interest.", "tokens": [286, 519, 1386, 277, 6, 9023, 341, 472, 565, 370, 365, 2878, 1179, 13], "temperature": 0.4, "avg_logprob": -0.3944981294519761, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.00024092105741146952}, {"id": 349, "seek": 298700, "start": 3003.0, "end": 3012.0, "text": " The guy mentioned here, Thomas mentioned you're going to have another US session as well, but you can join. Yes I think there's details on the forum.", "tokens": [440, 2146, 2835, 510, 11, 8500, 2835, 291, 434, 516, 281, 362, 1071, 2546, 5481, 382, 731, 11, 457, 291, 393, 3917, 13, 1079, 286, 519, 456, 311, 4365, 322, 264, 17542, 13], "temperature": 0.4, "avg_logprob": -0.3944981294519761, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.00024092105741146952}, {"id": 350, "seek": 298700, "start": 3012.0, "end": 3014.0, "text": " Thanks.", "tokens": [2561, 13], "temperature": 0.4, "avg_logprob": -0.3944981294519761, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.00024092105741146952}, {"id": 351, "seek": 301400, "start": 3014.0, "end": 3018.0, "text": " Bye.", "tokens": [50364, 4621, 13, 50564], "temperature": 0.0, "avg_logprob": -0.5742990016937256, "compression_ratio": 0.3333333333333333, "no_speech_prob": 0.0002751804713625461}], "language": "en"}