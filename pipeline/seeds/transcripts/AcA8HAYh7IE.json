{"text": " Welcome back to lesson nine, part two, how to train your model. Before we talk about training our model though, I wanted to revisit a couple of things that came up last week. And the reason I really wanted to revisit them is because I wanted to kind of give you an insight into how I do research. I mean, a lot of this course really will be me showing you how I do research and how I do software development in the hope that that is somewhat helpful to you. So one of the questions that came up last week was we looked at, how's the size of that? We looked at, we looked inside the nn.conf2d that comes with PyTorch to see how it goes about initializing parameters. And we found that inside here, convend.reset parameters, we found the way that it does initialization. And we found this math.square root 5 without any commentary, which was quite mysterious. So I decided to do some research into, you know, kind of dual research. One is like, what's the impact of this math.square root 5? And then at the same time, trying to get in touch with the PyTorch team about asking them where this math.square root 5 comes from. So let me show you how I went about doing that research. So I loaded up just what we had from last week, which was the ability to download the MNIST data and open it up, and then the function to normalize it, which I thought we'd export if we haven't already. And then we'd grab the data and we'd normalize it. And because we're going to be talking a lot about convolutions towards the end of today's lesson and particularly next lesson, I suspect, so I'll skip over some of the details about convolutions for now. But basically to do a convolution, as you know, we need a square or rectangular input, and our MNIST input, remember, was just a single vector per image 768 long. So I resized them all to 28 by 28 one-channel images so that we could test out the impact of this conv2d in it in PyTorch and set up the various variables that we wanted to have. And then I created a conv2d layer. So we have one input because it's just one channel, NH, which is number hidden, which is 32 outputs, and let's do a 5 by 5 kernel. And we'll talk more about why 5 by 5 might be suitable. And just for testing, let's just grab the first 100 elements of the validation set. So we've now got a tensor of 100 by 1 by 28 by 28. So it's a really good idea when you're playing with anything in software development, but including notebooks, to refactor things. So I'm going to be wanting to look at the mean and standard deviation of a bunch of things. So let's create a little function called stats to do that. And I never plan ahead what I'm going to do. When you see this in a notebook, it always means that I've written out that by hand, and then I copied it, and then I'm like, OK, I'm using it twice. I'll chuck it in a function. So then I go back and create the function. So here I've got the mean and standard deviation of my L1, which is a conv2d layer. And so a conv2d layer contains a weight tensor parameter and a bias tensor parameter. So just to remind you, L1.weight.shape is 32 output filters, because that's what number hidden was. And then we have an input filter, because we only have one channel, and then 5 by 5. So that's the size of our tensor. And if you've forgotten why that's the size of a tensor, you can go back to the Excel directory, for example, from part one, where you can find the conv example spreadsheet. And in the conv example spreadsheet, you can see what each of those parameters does. So we basically had a filter for each input channel and for each output channel. So that's kind of what it looked like. And so, for the next layer, we now have a four-dimensional tensor, a rank four tensor. We've got the 3 by 3. We've got it for each input and for each output. So that's the 32 by 1 by 5 by 5. So the mean and standard deviation of the weights, 0 and 0.11, and this is because we know that behind the scenes, it's called this function to initialize. So the bias is initialized with a uniform random number between negative of this and positive of this. And then the weights are initialized with chiming uniform with this odd math dot square root 5 thing. So that's fine. That's not particularly interesting. What's more interesting is to take our input tensor of MNIST numbers and put it through this layer, which we called L1, which remember is a conv2D layer, so layer 1. And let's create an output T, and let's look at the stats of T. So this is the stats of the output of this layer. We would like it to be a mean of 0 and a standard deviation or a variance of 1. The mean of 0 is there, but the standard deviation of 1 is not there. So that looks like a problem. Let's compare this to the normal chiming in it. So the normal chiming in it, remember, is designed to be used after a ReLU layer or more generally a leaky ReLU layer. And recall that a leaky ReLU layer has the y equals x here, and here the gradient of this is called A or leak or whatever. And now in our case, we're just looking at a conv layer, so we don't have anything kind of going on there. In fact, it's straight here as well. So effectively, we have like a leak, if you like, of 1 or an A of 1. So to use chiming in it with no ReLU, we can just put A equals 1. And if we do that, then we get a mean of 0 and a variance of 1. So chiming in it seems to be working nicely. So let's now try it with ReLU. So let's now define a function, which is the function for layer 1, which is to pass it through our layer 1 conv and then do a ReLU with some A, with some leak amount, which we'll set to 0 by default. So this will be just a regular ReLU. And you can see that if we now run that with chiming initialization, we get a variance of 1, which is good. And the mean is no longer 0, as we discussed last week. It's about a half. But if we go back and reinitialize the conv2d with this default PyTorch, this is not looking good at all. With ReLU, it's even worse. Because remember, they don't have anything kind of handling that ReLU case in the default conv. So this looks like a problem. So a variance of 0.35. It may not sound a lot lower than 1, but let's take a look at what that means. So I forgot to mention where we are. This is the 0 to a y square root 5 notebook. So 0 to a notebook. So in order to explore this, I decided that I would try and write my own chiming init function. And so normally with the chiming init function, if we were working with just a regular fully connected matrix multiplication, we would basically be saying how many output filters are there. So if this is the weight matrix, then what's the width of the weight matrix? For a convolutional layer, it's a little bit different. Because what we actually want to know is each time, like in this case, we're basically multiplying all these together with some set of inputs and then adding them all up. That's basically what a single step of a matrix multiplication is. In a convolution, we're also multiplying a bunch of things together and adding them up. But what we're actually adding together is, if it was 3 by 3, is we're multiplying together each of the 3 by 3 elements. And also the channel dimension. We actually multiply all of those together and add them all up. So because convolution and matrix multiplication are kind of one and the same thing, as we know, with some weight tying and with some zeros. So in order to calculate the total number of multiplications and additions going on for a convolutional layer, we need to basically take the kernel size, which in this case is 5 by 5, and multiply it by the number of filters. So the general way to get that 5 by 5 piece is we can just grab any one piece of this weight tensor and that will return a 5 by 5 kernel. And then say how many elements are in that part of the weight tensor. And that's going to be the receptive field size. So the receptive field size for just the immediate layer before is how many elements are in that kernel. So for this, it's 25. It's 5 by 5. And so if we then say, OK, let's grab the shape of the weight matrix. And it gives us the number of filters out, 32. And then the number of filters in, 1. And then I'll skip the rest because they're the only two things I want. So now for the kaming her in it, we can calculate fan in is the number of input filters times the receptive field size. So that's 1 times 25. Fan out is 32 by 25. So there you can see this is how we calculate the effective fan in and fan out for a convolutional layer. So we can do all that by hand. And then the kaming in it formula, you need to then, for leaky value, you need to multiply by root 2. Or if there's a leaky part in it, so if the a is not equal to 0, then it's actually root 2 divided by 1 plus a squared. So that's just the formula for the kaming in it. And that's often called the gain for the in it. And so there's the formula for the gain. So you can see that if the gain is 1, then that's just linear. There's no nonlinearity at all. So there's no change to the calculation of how you do the initialization. On the other hand, if it's a standard ReLU, then you've got the root 2, which we saw last week from the kaming paper. With a gain of 0.01, it's about root 2 as well. It's pretty close. And this is kind of a common leaky ReLU amount. But what about in the case of the PyTorch in it? In the case of the PyTorch in it, it's root 5, which is 0.577, which sounds like an odd number. It's a long way away from what we were expecting to see. So that's a bit concerning. But one thing we have to account for here is that the initialization that they use for PyTorch is not kaming normal. It's kaming uniform. And so normally distributed random numbers look like that. But uniform random numbers look like that. And so the uniform random numbers they were using as their kind of starting point were between minus 1 and 1. And so the standard deviation of that obviously is not 1. The standard deviation is obviously less than 1. And so you can Google for the standard deviation of a uniform distribution. Or you could jump into Excel or Python and just grab a bunch of random numbers and find out what the standard deviation is. And you'll find that you can, I've done it here actually, I've grabbed 10,000 random numbers in that uniform distribution and asked for their standard deviation. And it turns out that it's 1 over root 3. So part of the reason for this difference actually is that they need a gain to handle uniform random numbers rather than just normal random numbers. But it still doesn't quite account for the difference. So let's take a look. So here's my version of kaming in which I've just grabbed all of the previous lines of code and merged them together. And then I've just added this thing to multiply it by root 3 because of the uniform random number. And so then if I run this, timing 2 on my weights and get the stats of that, nice. I again get a variance of about 1. And again confirming that if I, well this is interesting, if I do it with a equals math dot square root 5, I would expect to get the same result as the PyTorch default, which I do. So that's about 0.4, which is what we found back here, 0.35. So it seems like we've successfully, you know, re-implemented what they had. So at this point I was like, okay, well what does this do? What does this mean? So to see kind of what this looks like, I threw together a quick confnet and I grabbed the first 100 dependent variables. And so then I took my input and I ran it through the whole confnet to get the stats out of the results. So this is now telling me what happens when I use the default PyTorch in it and put it through a four-layer confnet. And the answer is I end up with a variance of 0.006. And that sounds likely to be a really big problem, right, because there's so little variation going on that last layer. And also there's a huge difference between the first layer and the last layer. That's the really big issue. The first layer had a standard deviation of 0.4 and the last layer had a standard deviation of, well, the input layer is 1. The first hidden layer is 0.4 and the last layer is 0.006. So these are all going at like totally different rates. And then what we could do is we could grab that prediction and put it through mean squared error. This is the function we created last week. Run backward and get the stats on the gradients for the first layer weights. So this has now gone all the way forward and all the way back again. And again, standard deviation is nowhere near 1. So that sounds like a big problem. So let's try using chiming uniform instead. And if you look at the chiming uniform source code, you'll see that it's got this, it's got the steps that we saw, gain over root of the fan, and here is the square root of 3 because it's uniform. And so we can confirm. Let's go through and go through every layer. And if it's a convolutional layer, then let's call chiming uniform on the weights and set the biases to 0. So we'll initialize it ourselves. And then we'll grab T. And it's not 1, but it's a lot better than 0.008. So this is pretty encouraging that we can get through four layers. We wouldn't want to probably have a 40-layer neural network, which is losing this much variance, but it should be fine, pretty good enough for our four-layer network. And then let's also confirm on the backward. On the backward, the first layer's gradient is 0.5. So that was my kind of starting point for the research here. And at the end of this, I kind of thought this is pretty concerning. And why did I think it was concerning? We'll be seeing a lot more about why it's concerning, but let's quickly look at 2b initializing. So Sylvain put this together today, and he called this why you need a good init. And he's pointed out here that if you grab a random vector, X, and a random matrix, A, which is normally distributed, mean 0 and a standard deviation of 1, then 100 times, you basically go X is A times X. And then you go, so you're basically multiplying again and again and again. After 100 iterations, your standard deviation and mean are not a number. So basically, the issue is that when you multiply by a matrix lots and lots of times, you explode out to the point that your computer can't even keep track. So what Sylvain did next was he actually put something in a loop to check whether it's not a number, and he found it was only 28 iterations before it died. So it didn't take very long to explode. Now on the other hand, what if we take the random numbers with a standard deviation of 0.01 instead of 1, and we do that 100 times, then it disappears to 0. So you can see if you've got 100 layer neural net, because that's what it's doing, it's doing 100 matrix multipliers on itself, on the output of each previous one, you've got to be super careful to find some set of weights. Because if this is your starting set of weights, if it's 0.01 or if it's 1 standard deviation, you can't ever learn anything because there are no gradients. The gradients are either 0 or nan. So you actually have to have a reasonable starting point. This is really why for decades people weren't able to train deep neural networks, because people hadn't figured out how to initialize them. So instead we have to use some better in it. We'll talk about that in a moment. For those who are interested, Sylvain's then gone on to describe why it is that you have to divide by the square root of the fan. And so feel free to keep reading that if you're interested. It's cool. But we don't need to know it for now. It's just some derivation and further discussion. So in parallel I also asked the PyTorch team about this, and I sent these results to them. And I said, what's going on? And so Sumath finally appeared, and he said it was a historical accident. It was 15 years ago, or for 15 years before PyTorch appeared, there was a product called Torch, a neural network product in Lua. And they did it that way. And so then on Google+, in 2014, he started talking to Saunders Dielerman, who's now at DeepMind and about it this time, maybe a bit before he was our intern, actually. And Saunders said this is at Inletic. And Saunders said this Route 5 thing looks weird. And Sumath said, no, no, go look at the paper. And Saunders said, no, that's not what the paper said. And Sumath said, oh, it's a bug. But it's a good bug, because somebody went and checked it out, and they thought that they were getting better results with this thing. So then I talked to Sumath, and he was already aware of this issue to some extent. And within a couple of hours, PyTorch team had created an issue saying they're going to update their innets. So this is super cool. So this is like partly to say, well, this is an awesome team, super responsive. And this is why PyTorch works so well, is that they see issues and they fix them. But it's also to say, when you see something in a library, don't assume it's right or that it makes sense. When it comes to deep learning, none of us know what we're doing. And you can see it doesn't take too much to dig into something. And then you can raise an issue and say, here's an analysis that I did. There's a fantastic extension called Gistit, G-I-S-T, Gistit, for Jupyter Notebooks that lets you take your little research notebook, press a single button, and it turns it into a shareable gist that you can then put a link to say, here's the analysis that I did. And so, yeah, that's a little bit of fun little bit of research I did into answering this question from last week. There are lots of interesting initialization approaches you can use. We've already talked about the Loro and Benjio paper. We've already talked about the Kai Ming He paper. There's an interesting paper called All You Need is a Good Init, which describes how you can kind of iteratively go through your network and set one layer of weights at a time to like literally like kind of do a little optimize to find out which set of parameters gets you a unit variance at every point. There's another cool paper which talks about something called orthogonal initialization. If you've done some linear algebra, particularly if you've done Rachel's computational linear algebra course, you'll know about the idea of orthogonal matrices, and they make good innits. We talked briefly last week about fix-up initialization, and then there's also something called self-normalizing neural networks. Fix-up and self-normalizing neural networks are both interesting papers because they describe how to try to set a combination of kind of activation functions and in it such that you are guaranteed a unit variance as deep as you like. And both of those two papers went to something like a thousand layers deep and trained them successfully. In both cases, the fix-up is much more recent, but in both cases people have kind of hailed them as reasons we can get rid of batch norm. I think it's very unlikely to be true. Very few people use this SELU thing now because in both cases they're incredibly fiddly. So, for example, in the self-normalizing neural networks case, if you put in dropout, you need to put a correction. If you do anything different, you need to put in a correction because as soon as you've seen, as soon as something changes like the amount of leakiness in your activation function or whatever, all of your assumptions about what your variance will be in the next layer disappear. And for this SELU paper, it was a particular problem because it relied on two specific numbers that were calculated in a famous 96-page long appendix of math in the SELU paper. And so if you wanted to do a slightly different architecture in any way, and they only showed this a fully connected network, so even if you want to do convolutions, what are you going to do? Redo that 96 pages of math. So that 96 pages of math is now so famous that it has its own Twitter handle, the SELU appendix, which has the pin tweet, why does nobody want to read me? And this is like literally what the entire 96 pages of the appendix looks like. I will mention that in my opinion, this is kind of a dumb way of finding those two numbers. All you need is a good in it paper is a much better approach to kind of doing these things in my opinion, which is like if you've got a couple of parameters you need to set, then why not kind of set them using a quick little loop or something. So if you need those, if you want to find two kind of SELU parameters that work for your architecture, you can find them empirically pretty quickly and pretty easily. Okay. So that's a little bit about in it. We'll come back to more of that very shortly. There was one other question from last week, which was we noticed that the shape of the kind of manual linear layer we created and the shape of the PyTorch one were transposed, and the question was why. And so again, I did some digging into this until eventually Sumit from the PyTorch team pointed out to me this commit from seven years ago in the old Lua Torch code where this actually happened. And that basically it's because that old Lua library couldn't handle batch matrix multiplication without doing it in this transposed way, and that's why still to this day PyTorch does it kind of upside down, which is fine. Like it's not slower, it's not a problem. But again, it's kind of an interesting case of like I find this happens all the time in deep learning. Something's done a particular way forever, and then everybody does it that way forever, and nobody goes back and says why. Now in this particular case, it really doesn't matter, I don't think. But often it does, right? So like things like how do we initialize neural networks and how many layers should they have and stuff like that. And nobody really challenged the normal practices for years. So I'm hoping that with this really ground up approach, you can see what the assumptions we're making are and see how to question them and see that to me PyTorch is the best library around at the moment, and even PyTorch has these weird kind of archaic edges to it. Okay, so that was a little diversion to start with, but a fun diversion because that's something I spent a couple of days this week on and think it's pretty interesting. So to go back to how do we implement a basic modern CNN model, we got to this point. So we've done a matrix multiplication, so that's our affine function. We've done ReLU, so that's our non-linearity, and so a fully connected network forward is simply layering together those two things. So we did that, and then we did the backward pass and we kind of refactored that nicely and it turned out that it looked pretty similar to PyTorch's way of doing things. And so now we're ready to train our model, and that's where we're up to. So here we are, 03 MiniBatch training, and we're going to train our model. So we can start by grabbing our MNIST data. So again, we're just importing the stuff that we just exported from the previous class. Here's the model we created in the previous class. And so let's get some predictions from that model, and we'll call them Pred. And so now to train our model, the first thing we need to do is we need a loss function, because without a loss function, we can't train it. Now previously we used mean squared error, which I said was a total cheat. Now that we've decided to trust PyTorch's autograd, we can use many more things because you don't have to write our own gradients, and I'm too lazy to do that. So let's go ahead and use cross entropy, because cross entropy makes a lot more sense. To remind you from the last class, there is an entropy example notebook where we learned, first of all, that cross entropy requires doing two things. First of all, softmax. Well, in the case of this multi-class categorical cross entropy, you first do softmax, and then you do the negative log likelihood. So the softmax was if we have a bunch of different possible predictions, and we got some output for each one from our model, then we take e to the power of that output, we sum them all up, and then we take the e to the power of divided by the sum of e to the power of. And that was our softmax. So there it is in math form. There it is in summation math form, and here it is in code form. So e to the x divided by x exp sum, and then the whole thing we do a dot log, and that's because in PyTorch, negative log likelihood expects a log softmax, not just a softmax. And we'll see why in a moment. So we pop a log in the end. So here's our log of softmax function. So now we can go ahead and create our softmax predictions by passing preds to log softmax. Now that we've done that, we can calculate cross entropy loss. And cross entropy loss is generally expressed in this form, which is this form, sum of actual times the log of the probability of that actual. So in other words, if we have is cat and is dog, then here's our actuals. So it's one hot encoded. Is cat yes? Is dog no? We have our predictions from our model, from our softmax. We can then say, well, what's the log of the probability it's a cat? So log of this. What's the log of the probability it's a dog? So log of 1 minus that. And so then our negative log likelihood is simply b times e plus c times f. And then take the negative of all that. That's negative log likelihood, which is what this is. But remember, and I know I keep saying this because people keep forgetting, not you guys, but people out in the world keep forgetting that when you're multiplying by stuff which is mainly 0 and one hot encoded categorical classification, most of your categories are 0. Every time you multiply by 0, you're doing nothing at all, but you're doing it very slowly. So rather than multiplying by 0 and then adding up the 1, 1 that you have, a much faster way as we know to multiply by a one hot encoded thing is to first of all simply say, what's the location of the 1 here? So in this case, it's location 2. In this case, it's location 1, if we index from 1. And then we just look up into our array of probabilities directly offset by this amount. Or to put it in math terms, for one hot encoded x's, the above is simply log of p i, where i is the index of our prediction. Sorry, not our prediction, the actual. So the index into here of the actual. So how do we write this in PyTorch? And I'll show you a really cool trick. This is what we're going to end up with. This is our negative log likelihood implementation. And it's incredibly fast and it's incredibly concise. And I'll show you how we do it. Let's look at our dependent variable. So let's just look at the first three values. So 5, 0, 4. So that's the first three elements of the dependent variable. And so what we want to do is we want to find what is the probability associated with 5 in our predictions and with 0 and with 4. So our softmax predictions, remember, 50,000 by 10. Okay, and so if we take the very first of those, they're all there. And so it said that the actual answer should be 5. So if we go into this 0, 1, 2, 3, 4, 5, that's the answer that we're going to want. Okay, so here's how we can grab all three of those at once. We can index into our array with the whole thing, 5, 0, 4. And then for the first bit, we pass in just the contiguous integers, 0, 1, 2. Why does this work? This works because PyTorch supports all of the advanced indexing support from NumPy. And so if you click on this link, one of the many things that types of indexing that NumPy and therefore PyTorch supports is integer array indexing. And what this is is that you pass a list for each dimension. So in this case, we have two dimensions. So we need to pass two lists. And the first is the list of all of the row indexes you want. And the second is the list of all of the column indexes you want. So this is going to end up returning 0, 5, 1, 0, and 2, 4, which is the exact numbers that we wanted. So for example, 0, 5 is minus 2.49. So to grab the entire list of the exact things that we want for our negative log likelihood, and we basically say, OK, let's look in our predictions. And then for our row indexes, it's every single row index. So range of target.shape0. So target.shape0 is the number of rows. So range of that is all of the numbers from 0 to the number of rows. So 0, 1, 2, 3, blah, blah, blah, 50,000, or 49,999. And then which columns do we want for each of those rows? Well, whatever our target is, whatever the actual value. So in this case, 5, 0, 4, et cetera. So that returns all of the values we need. We then take minus, because it's negative log likelihood, and take them in. So that's all it takes to do negative log likelihood in PyTorch, which is super wonderfully easy. So now we can calculate our loss, which is the negative log likelihood of the softmax predictions. That's what we had up here compared to our actual y trading. And so there it is. Now this was our softmax formula, which is e to the x over sum of e to the x's. So we have a, and then it's all logged. So we've got a log of a over b. And remember, I keep telling you that one thing you want to remember from high school math is how logs work. So I do want you to try to recall that log of a over b is log of a minus log of b. And so we can rewrite this as log of e to the x minus log of all that. And of course, e to the something and log are opposites of each other. So log of e to the x is just x. So that ends up being x minus x dot x dot sum dot log. So this is useful. And let's just check that that actually works. So as I kind of keep refactoring these things, as even as I'd like to be, these mathematical manipulations are just refactoring. So just refactoring the math. So you keep checking along. So we created test near last time. So let's use it to make sure that it's the same as our loss. Now you'll see here this is taking the log of the sum of the x. And there's a trick called log sum x. The reason we need this trick is that when you go e to the power of something, you can get ridiculously big numbers. And if you've done Rachel's computational linear algebra course, then you'll know that very big numbers in floating point on a computer are really inaccurate. Basically, the further you get away from zero, the less kind of fine grained they are. It gets to the point where two numbers, a thousand apart, the computer thinks they're the same number. So you don't want big numbers, particularly when you're calculating gradients. So anywhere you see an e to the x, we get nervous. We don't want x to be big. But it turns out that if you do this little mathematical substitution, you can actually subtract a number from your x's and add them back at the front, and you get the same answer. So what you could do is you can find the maximum of all of your x's. You can subtract it from all of your x's and then add it back afterwards outside the x. And you get exactly the same answer. So in other words, let's find the maximum. Let's subtract it from all of our x's. And then let's do log sum exp. And then at the end, we'll add it back again. And that gives you exactly the same number, but without this numerical problem. So when people talk about numerical stability tricks, they're talking about stuff like this. And this is a really helpful numerical stability trick. So this is how you do log sum exp in real life. We can check that this one here is the same as, and look, in fact, log sum exp is already a method in PyTorch. It's such an important and useful thing. You can just actually use PyTorches, and you'll get the same result as the one we just wrote. So now we can use it. So log softmax is now just x minus x.logsumexp. And let's check. Yep, still the same. So now that that's all working, we may as well just use PyTorch's log softmaxes and PyTorch's NLL loss. But actually, NLL loss of log softmax is called cross entropy. So finally, we get test near f.crossentropy is the same as loss, and it is. So we've now recreated PyTorch's cross entropy. So we're allowed to use it according to our rules. Okay, so now that we have a loss function, we can use it to train. And we may as well also define a metric, because it's nice to see accuracy to see how we're going. It's just much more interpretable. And remember from part one that the accuracy is simply grab the argmax to find out which of the numbers in our softmax is the highest, and the index of that is our prediction. And then check whether that's equal to the actual, and then we want to take the mean. But in PyTorch, you can't take the mean of ints. You have to take the mean of floats, which makes some sense. So turn it into a float first. So there's our accuracy. So let's just check. Let's grab a batch size of 64, and let's grab our first x batch. This is our first playing around with mini batches, right? So our first x batch is going to be our training set from zero up to batch size. So our predictions is we're just going to run our model. And remember our model was linear, ReLU linear. So it's still using a super simple model. So let's calculate some predictions, and let's have a look at them. And here's some predictions. And it's 64 by 10, as you'd expect, batch size 64 and 10 possible probabilities, right? So now we can grab our first batch of dependent variables and calculate our loss. Okay, and it's 2.3, and calculate our accuracy. And as you'd expect, it's about 10% because we haven't trained our model. Okay, so we've got a model that's giving basically random answers. So let's train it. So we need a learning rate. We need to pick a number of epochs, and we need a training loop. So our training loop, if you remember from part one, remember lesson two SGD? Our training loop looks like this. Calculate your predictions, calculate your loss, do backward, subtract learning rate times gradients, and zero the gradients. Okay, so let's do exactly the same thing. So we're gonna go through with GPOC and go through i up until n, which is 50,000. That's the number of rows. But integer divide by batch size, cuz we're gonna do a batch at a time. And so then we'll grab everything starting at i times batch size and ending at that plus batch size. So this is gonna be our first, this is gonna be our i-th mini batch. And so let's grab one x mini batch, one y mini batch, and pass that through the model and our loss function. And then do backward. And then we're gonna do our update, which remember we have to do with no grad, cuz this is not part of the gradient calculation. This is the result of it. But now we can't just go A dot subtract learning rate times gradient. We have to do that for every single one of our parameters. So our model has three layers. The ReLU has no parameters in it. So the linear layer has weight and bias, and this linear layer has weight and bias. So we've basically got four tensors to deal with. So we're gonna go through all of our layers. And let's just check whether that layer has an attribute called weight or not. That's a bit kind of more flexible than hard coding things. And if it does, then let's update the weight to minus equals the gradient of that by the learning rate, the bias to the bias gradient by the learning rate, and then zero those gradients when we're done. So let's run it. And then let's check the loss function and the accuracy. And the loss has gone down from 2.3 to 0.05, and the accuracy has gone up from 0.12 to 1. Notice that this accuracy is for only a single mini-batch, and it's a mini-batch from the training set. So it doesn't mean too much, but obviously our model is learning something. So this is good. So this is our, you know, we're now, well, we haven't really done conv. I guess we've got a basic training loop. We're now here. We have a basic training loop, which is great. So we kind of got all the pieces. So let's try to make this simpler, because this is too much code, right, and it's too hard to fiddle around with. So the first bit we'll do is we're going to try and get rid of this mess, and we're going to replace it with this. And so the difference here is rather than manually going through weight and bias for each one, we're going to loop through something called model.parameters. So we're not even going to loop through the layers. We're just going to loop directly through model.parameters. And for each parameter, we'll say that parameter minus equals gradient times learning rate. So somehow we need to be able to get all of the parameters of our model, because if we could do that, we could greatly simplify this part of the loop and also make it much more flexible. So to do that, we could create something like this. I'm just calling this dummy module. And in dummy module, what I'm going to do is I'm going to say every time I set an attribute like L1 or L2, in this case to linear, I want to update a list called underscore modules with a list of all of the modules I have. So in other words, after I create this dummy module, I want to be able to print out, here's my representation, I want to be able to print out the list of those modules and see the modules that are there, because then I can define a method called parameters that will go through everything in my underscore modules list and then go through all of their parameters. And that's what I'll be able to do. See, I could do here model.parameters. So how did I create this? You'll see it's not inheriting from something. This is all written in pure Python. How did I make it so that as soon as I said here's an attribute in my in it, that somehow it magically appeared in this underscore modules list so that I could then create these parameters so that I could then do this refactoring? And the trick is that Python has a special Dunder setAtra method. And every time you assign to anything inside self inside Python, it will call this method if you've got one. And so this method just checks that my the key, so in other words, the attribute name, doesn't start with underscore, because if it does, it might be underscore modules and then it's going to be like a recursive loop. And also, Python's got all kinds of internal stuff that starts with underscore. So as long as it's not some internal private stuff, put that value inside my modules dictionary and call it k. That's it. And then after you've done that, do whatever the superclass does when it sets attributes. And in this case, the superclass is object. If you don't say what it is, then it's just the Python highest level object. So now we have something that has all of the stuff we need to do this refactoring. But the good news is PyTorch also has something that does that and it's called nn.module. So we can do the exact same thing rather than implementing that set attribute stuff ourselves. We can just call just inherit from nn.module and it does it for us. And so this is now you know why you have to call super Dunder in it first, because it has to set up its equivalent of this underscore modules dictionary. So that's why you have to call super in it first. And then after you've done that in PyTorch, it's exactly the same as what I just showed you. It now creates something which you can access through named children. And you can see here if I print out the name and the layer, there is the name and the layer. So this is how PyTorch does the exact same thing. Just like I created a Dunder repra, PyTorch also has a Dunder repra. So if you just print out model, it prints it out like so. You can grab the attributes just in the normal Pythonic way. It's just a normal Python class. It has a bit of this extra behavior. So now we can run it with this refactoring. Make sure everything works. And there we go. Okay. So this is doing exactly the same thing as before, but a little bit more conveniently. Not convenient enough for my liking. So one thing we could try to do is to get rid of the need to write every layer separately, maybe go back to having it as a list again. So if we made it a list of layers, right, and then we want to be able to pass that to some model class, passing the layers, this is not enough to make them available as parameters, right, because the only thing that actually that PyTorch is going to make available as parameters are things that it knows are proper nn.modules. So but here's the cool thing. You can just go through all of those layers and call self.addModule. That's just the equivalent of what I did here when I said self.underscoreModules, blah, blah, right? So in PyTorch, you can just call self.addModule, and just like I did, you give it a name, and then you pass in the layer. And so if you do that, then you end up with the same thing. So that's one thing you could do. But this is kind of clunky, so it would be nice if PyTorch would do it for you, and it does. That's what nn.modules list does. So if you use an nn.module list, then it just basically calls that line of code for you. So you can see us doing it here. We've got to create something called sequential model, which just sets self.layers to that module list. And then when we call it, it just goes through each layer, x equals that layer of x, and returns it. And there's that. Okay? Even this is a little bit on the clunky side. Why would we have to write it ourselves? We don't. PyTorch has that code already. It's called nn.sequential. Okay? So we've now recreated nn.sequential. And there it is doing the same thing. So again, we're not creating dumbed-down versions. If you look at nn.sequential and you look at the source code and you look at forward, it's just... Look, it's even the same name. So go through each module itself, dot underscore modules, dot values, input equals module input, return input. Right? So that's their version. And remember, our version was basically the same. And we could even put it in something called underscore modules. So yeah. That's all nn.sequential is doing for you. Okay? So we're making some progress. It's less ugly than it used to be. Still more ugly than we would like. This is where we got our fit function up to. So let's try and simplify it a bit more. Let's replace all this torch.nograd for p and module.parameters, blah, blah, blah, with something where we can just write those two lines of code. That would be nice. So let's create a class called optimizer. We're gonna pass in some parameters and store them away. And we're gonna pass in the learning rate and we're gonna store that away. And so if we're gonna be able to go opt.step, opt.step has to do this. So here is step with torch.nograd. Go through each parameter and do that. Okay, so let's just factor that out. And zero grad, we probably shouldn't actually go model.zerograd because it's actually possible for the user, as you know, to say I don't want to include certain parameters in my optimizer. So when we're doing gradual unfreezing and stuff. So really zero grad should actually do this. It should go through the list of parameters that you asked to optimize and zero those gradients. So here we've now created something called optimizer and we can now grab our model. And so remember that the model now we've created something called dot parameters. So we can pass that to our optimizer. And then we can now just go up dot step, up dot zero grad. And let's test it and it works. Okay, now of course, luckily for us, PyTorch already has these lines of code. It's called optim.sgd. Now optim.sgd does do a few more things, weight decay, momentum, stuff like that. So let's have a look. Here's optim.sgd and here's its step function. So it's got weight decay, momentum, dampening, nest drop. We're gonna see all these things very shortly. But basically all it does is it goes through each layer group and it does the exact thing that we just see. Okay, so once you remove the momentum and stuff, and we're gonna be implementing this in a much better way than PyTorch in very soon. So once you remove all that, there optim.sgd is exactly the same as our optimizer. So let's go ahead and use that instead. And so it's kind of nice then if we're gonna use all the parameters of the model, let's just create a get model which creates our model and returns it as well as a SGD optimizer with all the parameters. And okay, there's our training loop. And seems to be working. It's nice to put tests in from time to time. And I like to put these tests in like, hey, my accuracy should be significantly better than 50%. You know, note that these kind of stochastic tests are highly imperfect in many ways. It's theoretically possible it could fail because you got really unlucky. I know though that this is really, it's vanishingly unlikely to happen. It's always much more than 90%. It's also possible that your code could be failing in a way that causes the accuracy to be a bit lower than it should be, but not this low. But I still think it's a great idea to have these kinds of tests when you're doing machine learning because they give you a hint when something's going wrong. And you'll notice I don't set a random seed at any point. This is very intentional. I really like it that if there's variation going on when I run my model at different times, I wanna see it. I don't want it to be hidden away behind a fixed seed. So there's a big push in science for reproducible science, which is great for many reasons, but it's not how you should develop your models. When you're developing your models, you wanna have a kind of good intuitive sense of what bits are stable and what bits are unstable and how much variation do you expect. And so if you have a test which fails one every 100 times, it's good to know that. And so in the fast AI code, there's lots of tests like that. And so then sometimes there'll be a test that fails. It's nothing particularly to do with the push that just happened, but it's really helpful for us, cuz then we can look at it and be like, this thing we thought should pretty much always be true, sometimes isn't true. And then we'll go back and we'll deeply study why that is and figure out how to make it more stable and how to make it reliably pass that test. So this is a kind of a controversial kind of test to have, but something that I found in practice is very, very, very helpful. It's not complete and it's not totally automated and it's imperfect in many ways, but it's nonetheless helpful. Okay, let's get rid of these two lines of code. These were the lines of code that grabbed our x mini batch from the training set and the y mini batch from the training set. Let's do them both in one line of code. So it'd be nice to have one line of code where we have some kind of object where we can pass in the indexes we want and get back both x and y. And that's called a data set, as you know. So here's our data set class. Again, not inheriting from anything, it's all from scratch, pluripython. We initialize it by passing in the x and the y, we'll store them away. It's very handy to have a length. Hopefully you know by now. If you don't, then now's a good time to realize that Dunderlen is the thing that lets you go len of something in Python and have it work. That's what len will call. So now we've got the length of our data set. And Dundergetitem is a thing that when you index into it, it will return that. And so we just return the tuple of x i and y i. So let's go ahead and create a data set for our training set and our validation set. Check that the lengths are right. Check the first few values, make sure they all seem sane. Now we'll grab our model. And as I said, we will replace those two lines of code with one. And so at this point, our training loop's getting quite neat. It's not as neat as it could be, but it's getting quite neat. So the next thing we're going to do, so that's a data set. Next thing we're going to do is create a data loader. This is what the start of our training loop looked like before. And let's replace it with this single line of code. So to do that, we're going to have a class that takes a data set and a batch size and stores them away. And when you go for blah in blah, behind the scenes in Python, it calls Dunderitter. And so what we're going to do is we're going to loop through range from zero up to the size of the data set, jumping up batch size at a time. So 0, 64, 128, et cetera, up to 50,000. And each time we go through, we will yield our data set at an index starting at i and ending at i plus self.batch size. Probably quite a lot of you haven't seen yield before. It's an incredibly useful concept. If you're really interested, it's something called a coroutine. It's basically this really interesting idea that you can have a function that doesn't return just one thing once, but can return lots of things, and you can kind of ask for it lots of times. So the way these iterators work in Python is that when you call this, it basically returns something which you can then call next on lots of times, and each time you call next, it will return the next thing that is yielded. So I don't have time to explain coroutines in detail here, but it's really worth looking up and learning about. We'll be using them lots. They're a super valuable thing. And it's not just for data science. They're really handy for things like network programming, web apps, stuff like that as well, so well worth being familiar with yield in Python. And nowadays, most programming languages have something like this, so you'll be able to take it to wherever you go. So now we have a data loader. We can create a training one and a validation one, and this is how we do it. Iter, valid dl, is the thing that basically kind of generates our coroutine for us, and then next is the thing that grabs the next thing yielded out of that coroutine. So this is a very common thing. You'll be doing lots. Next, itter, blah. You probably did it a whole lot of times in part one, because we kind of did it without diving in very deeply into what's going on. And that returns one thing from our data set, and the data set returns two things, because that's what we put in it. So we expect to get two things back, and we can check that those two things are the right size. So that's our data loader. And so let's double check. There it is. Good stuff. So now there's our fitness function. Let's call the fitness function looking good. So this is about as neat as we're going to get. That's quite beautiful, right? It's kind of all the steps you can think of if you set it in English there. Go through each epoch. Go through each batch, grabbing the independent variable. Calculate the predictions. Calculate the losses. Calculate the gradients. Update with the learning rate. Reset the gradients. So that's kind of where you want to get, is to a point where you can read your code in a very kind of intuitive way to a domain expert. And until you get to that point, it's very hard really, I find, to really maintain the code and understand the code. And this is the trick for doing research as well. This is not just for hardcore software engineering. A researcher that can't do those things to their code can't do research properly, right? Because if you think of something you want to try, you don't know how to do it, or it takes weeks, or if there are bugs in it, you don't know. So you want your code to be quite beautiful. And I think this is beautiful code. And this is, at this point, you know, this data set and this data loader are the same abstractions that PyTorch uses. So let's dig into this a little bit more. We do have a problem, which is that we're always looping through our training set in order. And that's very problematic because we lose the randomness of kind of shuffling it each time, particularly if our training set was already, like, ordered by dependent variable, then every batch is going to be exactly the same dependent variable. So we really want to shuffle it. So let's try random sampling. So for random sampling, I'm going to create a sampler class, and we're going to pass into it a data set to sample and a batch size and something that says whether to shuffle or not, right? And as per usual, we just store those away. I don't actually store away the data set. I just store away the length of the data set so that we know how many items to sample. Okay. And then here's our dunder error, right? So remember, this is the thing that we can call next on lots of times. And so if we are shuffling, then let's grab a random permutation of the numbers from 0 to n minus 1. And if we're not shuffling, then let's grab all of the integers in order from 0 to n minus 1. And then, this is the same we had before, go through that range and yield the indexes. So what does that look like? Here's a sampler with shuffle equals false and a batch size of 3, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9. And here it is with shuffle equals true, 5, 4, 3, 7, 6, 2, 8, 9, 0, 1. So now that we've got these, we can now replace our data loader with something where we pass it a sampler. And we then loop through for s in sampler. So it's gonna loop through each of these, right? And the cool thing is that because we used yield, these are only gonna be calculated when we ask for them. They're not all calculated up front. So we can use these on really big data sets, no problem. And so it's kind of, this is a common thing is where you're actually looping through something which is itself a coroutine and then yielding something which does other things to that. So this is like a really nice way of doing streaming computations. It's being done lazily. You're not gonna run out of memory. It's a really neat way to do things. And so then we're gonna grab all of the indexes in that sample. And we're gonna grab the data set at that index. So now we've got a list of tensors. And then we need some way to collate them all together into a single pair of tensors. And so we've created a function called collate, which just grabs the Xs and the Ys and stacks them up. So collage.stack just grabs a bunch of tensors and glues them together on a new axis. You might wanna do different things like add some padding or stuff like that. So you can pass in a different collate function if you want to. And it'll store it away and use it. So now we can create our two samplers. We can create our two data loaders with those samplers. So the training one is shuffling, the valid one's not shuffling. So let's check, there's the validation data loader. And the training data loader, if we call it twice with exactly the same index, we get different things. In this case, we got two eights, but they're different eights. Call it another two times, we're getting different numbers. So it is shuffling as we hoped. And so again, we can train our model and that's fine. So the PyTorch data loader does exactly that. So let's import the PyTorch data loader. And you can see it takes exactly the same arguments. Okay, and we can even pass in the exact collate function that we just wrote. It doesn't have a single sampler that you pass shuffle equals true or false to. It has two samplers, one called random, one called sequential. So slightly different to the API we just wrote, but does exactly the same thing. And so you can create those data loaders and works exactly the same. So that's what a PyTorch data loader does. Most of the time you don't need the flexibility of writing your own sampler and your own collation function. So you can just pass in shuffle and it will use the default sampler and collation function that work the way we just showed. Something that we did not implement in PyTorch's data loader is that in PyTorch's data loader you can pass in an extra parameter called num workers. And that will fire off that many processes. And each one will separately grab stuff out of your data set and then it will collate them together afterwards. And so if your data set's doing things like opening big JPEG files and doing all kinds of image transformations, that's a really good idea. So we won't implement that. All right, so finally for this section we should add validation. So to know if we're overfitting we need to have a separate validation set. So here's the same loop that we had before. And here's the same loop pretty much again, but with torch.nograd going through the validation set. So for this we grab the predictions and the loss as before, but we don't call backward and we don't step the optimizer because it's just the validation. Instead we just keep track of the loss and we also keep track of the accuracy. The only other difference is that we've added here model.train and here model.neval. What does that do? Well actually all it does is it sets an internal attribute called.training to true or false. So let's try it. If I print model.training after each one and train this, let's see, true, false, true, false, true, false. And so why does it set this thing called model.training to true or false? Because some kinds of layers need to have different behavior depending on whether it's training or evaluation or validation. For example batch norm only updates its running statistics if it's training. Dropout only does randomized dropout if it's training. They're the two main ones. So that's why you always want to have train and eval. And if you forget to put something into eval mode when you're done training, you'll often be surprised because you'll be getting worse results than you expected. Okay so that's our fit loop. One thing to note, are these validation results correct if the batch size varies? Let's spell that correctly. If the batch size varies. Because what we're doing here is we're adding up the loss and we're adding up the accuracy. And then at the end we see how big is our data loader, how many batches are there, and we divide. But if you think about it, if you had one mini batch of size 1000 and one mini batch of size 1, you can't actually just do that, right? You actually need to do a weighted average, weighted by the size of the mini batch. So this incorrect way is how nearly every library does it. Fast AI does it the proper way and the next time we do this we're going to do it the proper way. But for now here's what most people do and it does not work correctly when your batch size varies. So it's handy to have something that we can basically pass in a training data set and a validation data set and a batch size to and just grab the data loaders. The training data set will be shuffled, validation won't be shuffled. Also the validation data set, we don't need to do the backward pass, so we don't need to store the gradients. So that means that we have twice as much room. So we can make it twice the batch size. So it's another nice thing to refactor out, you don't have to type it anymore and also it means that you won't accidentally make a mistake. And so now we can go ahead and fit and let's do five epochs and so now these are actual validation accuracies. Okay, great. So we've successfully built a training loop. Let's have a six minute break, come back at 7.55 and talk about callbacks. Before we continue, Rachel, any questions? Okay. So why do we have to zero out our gradients in PyTorch? Why do you have to zero out your gradients in PyTorch? So yeah, the way we... Let's go back to... So here's our optimizer, right? Or let's go back even further. Here's our first version. So this is just with no additional help from PyTorch at all. If we didn't go grad.zero here, then what's gonna happen the next time we go through and say lost.backward is it's gonna add the new gradients to those existing gradients. Now, why does that happen? Well, that happens because we often have lots of sources of gradients. There's lots of different modules all connected together and so they're getting their gradients from lots of different places and they all have to be added up. So when we call backward, we wouldn't want backward to zero the gradients because then we would lose this ability to plug in lots of things together and just have them work. So that's why we need the grad.zero here. So then that's part one of the answer. Part two of the answer is why did we write our optimizer so that it was one thing called step and one thing called zero grad? Because what we could have done is we could have removed these lines and pushed this up here and so that step could have done both. And then since we've actually got this kind of twice now, we could put it all inside the for loop. So we could certainly have written our optimizer like this as it goes through each parameter and does the update and sets the gradient to zero and then we would be able to remove this line. So the problem with that is that we then remove the ability to not zero the gradients here and that means any time that we don't want to zero the gradients, we now can't use the optimizer. So for example, what if you are working with some pretty big objects, so like if you're doing super resolution and you're trying to create a 2K output, you know, your batch size, you can only fit two images on the GPU at a time and the stability of the gradients that you get from a batch size of two is so poor that you need to use a larger batch size. So well, that would be really easy to do if you did it like this, right, because we could say if I percent two, then, right, and so this is now going to only run these things every two iterations and so that means that our effective batch size is now double. So that's handy, right, that's called gradient accumulation. The gradient accumulation is where you change your training loop so that your optimizer step and your zero grads only happen occasionally. So that's really the reason is that there might be times you don't want to zero the gradients every time you do a step and if there's nowhere to do that, that's a problem. You could argue that, I can't think of a reason that this isn't a good idea, we could make our optimizer, we could say kind of like auto zero equals true, say, and then we could have something in here which kind of says like if self.auto zero, then self.zero grad, right, something like that and then that could even be the default and then you wouldn't have to worry about it unless you explicitly wanted to do gradient accumulation. I think that would really be a better API design, maybe, but that's not what they've done but it's so easy to write your own optimizers, you could totally do that. But I mean, you know, the upside is removing a single line of code which isn't a huge upside anyway. Any other questions, Rachel? Okay. Okay, so that's our training loop. But it's not quite where we want it to be and I'm stealing some slides here from Sylvain who had a really cool talk recently called an infinitely customizable training loop so I'll steal his slides. Before I do, I would like to do a big thank you to Sylvain. He has been working full time with Fast AI for well over a year now, I guess, and a huge amount of what you see in the Fast AI library and research and courses is him. So massive thank you to Sylvain who's the most awesome person I've worked with in my whole life so that's pretty cool. But also thank you to lots of other people. Huge thanks to Staz who a lot of you all have come across in the forum and he's done a lot of the stuff that makes Fast AI work well and he's entirely a volunteer so like super grateful to him. The stuff that lets you check whether your installation works properly, that lets you quickly check whether your performance is what it should be. Also like organizing lots of helpful projects through the forums. He's been fantastic. Lots of other folks as well. Andrew Shaw wrote a lot of the original documentation stuff that we have. Fred Munro has been helpful in thousands of ways and is just incredibly generous. Jason, a lot of you will already be aware of, who helped a lot with the final lesson of the last course and is hard at work now on taking it even further to doing some stuff that's going to blow you away. I particularly want to point out RADIC because this is the list of the, I can't quite count, the 20 most helpful people on the forum as ranked by number of likes. When somebody clicks that like button it means they're saying, you've helped me. More people have said that about RADIC than anybody else. It's not surprising because RADIC is not just an incredibly helpful person but extremely thoughtful. Gosh, when he started as a fast AI student he considered himself, if I remember correctly, basically a failed ML student. He had tried a number of times to learn ML and hadn't succeeded but he's just applied himself so well for the last couple of years and he's now a Kaggle winner, a world recognized deep learning practitioner. Thank you to all of these people and everybody else who's contributed in so many ways. And of course Rachel who's sitting right next to me. This is the fit function that we just wrote or the one, the slightly more elegant one before we added validation to it. So go through each epoch, go through each set of mini batch, get the prediction, the loss, backward pass, update your parameters and then zero the gradients. So that's basically what we're doing. Model, predictions, loss, gradients, step. And each time we grab a bit more training data. But that's not really all we want to do in a training loop. We might want to add the beautiful fast progress bars and animations that Sylvain created in his fast progress library or TensorBoard or whatever. And thanks to Jason actually we now have TensorBoard integration in Fast AI. So be sure to check that out if you want extra pretty graphs like these. Hyperparameter scheduling. You might want to add all kinds of different regularization techniques. These are all examples of regularization techniques that we support in Fast AI and many more. Mixed precision training. You know, so take advantage of the tensor cores in a Volta GPU to train much faster. There's more tweaks you might want to do to the training loop than we could possibly think of. And even if we did think of all of the ones that exist now, somebody will come up with a new one tomorrow. So you've got some possible ways you could solve this problem. Some of the things we're talking about are even things like how do you add GANs, more complex stuff. So one approach is write a training loop for every possible way you want to train. And this is particularly problematic when you start to like want to combine multiple different tweaks, right? As you're like cutting and pasting or whatever. So that's certainly not going to work for Fast AI. There's what I tried for Fast AI 0.7. This is my training loop. The last time I tried this. Which is like throw in every damn thing. And it just got, you know, so every time somebody would say like, oh, a new paper's come out. Can you please implement? And I'd just be like, no. I couldn't bear it. So now we have something better. Callbacks. And callbacks are something which like every library has callbacks, but nobody else have callbacks, anything like our callbacks. And you'll see what I mean. Our callbacks let you not only look at but fully customize every one of these steps. And so here's our starting training loop. Is the Fast AI version one training loop. It's the same, right? There's the exact same lines of code. Plus a bunch of calls to callbacks. And so each one basically says, you know, before I do a step, on step begin. After I do a step, on step end. After I do a batch, on batch end. After I do an epoch, on epoch end. After I finish training, on training end. Right? And they have the ability to also change things or even they have the ability to say, please skip the next step by returning a Boolean. So with this, we can create and have created all kinds of things in Fast AI, like learning rate schedulers and early stopping and parallel trainer. This is literally when I wrote parallel trainer, this is the entire callback I wrote. This is the entire gradient clipping callback. After you do the backward pass, clip the gradients. So you can do a lot with a little. And then you can mix them all together. Because all of the callbacks work with all of the other callbacks. So these are some of the callbacks that we have in Fast AI right now. So for example, how did we do GANs last course? So what we did behind the scenes was we created a GAN module. It was ridiculously simple. We created a GAN module that had a forward method that just said, what's your generator mode? Is it, sorry, are you in generator mode or not? Where not means discriminator mode. If you're in generator mode, call the generator, otherwise call the critic. And then there's a function called switch that just changed generator mode, backwards and forwards, between generator and discriminator. Same thing if we created a loss function where there was a generator loss and a critic loss. And so then we created a callback, right, which had a switch that just switched the generator mode on and off and passed that along to the model I just showed you and the loss function I just showed you. And then it would set requires grad to the generator or discriminator as appropriate. And then would have on train begin, on train end, on batch blah, blah, blah, callbacks to do the right thing at the right time. So most importantly, at the start of an epoch, set your generator mode. And at the end of training, set your generator mode. So if you look at other libraries' implementation of GANs, they're basically a whole new training loop, whole new data loaders, whole new everything. And it was really cool. With REST AI, we were able to create a GAN in this incredibly small amount of code for such a complex task. So let's do that ourselves, right, because we've got a training loop. If we add callbacks, we should now then be able to do everything. So let's start out by grabbing our data as before. So we've got the number of hidden is 50, batch size 64, loss function is cross entropy. This is the signature of our fit function before. And I get very nervous when I see functions with lots of things being passed to it. And it makes me think, do we really need to pass all those things to it, or can some of them be packaged up together? There's a lot of benefits to packaging up things together. And you can package up things together where they're kind of alike things. You can pass them around to everything that needs them together. You can create them using kind of factory methods that create them together. And you can do smart things, like look at the combination of them and make smart decisions for your users, rather than having to have them set everything themselves. So there's lots of reasons that I would prefer to keep epochs, right, but I'd like to put all these other things into a single object. And specifically, we can do that in two steps. First of all, let's take this data and say training and valid data conceptually should be one thing. It's my data, right? Maybe there's test data there as well. So let's create a class called data bunch that we're going to pass in training data and validation data, and we'll store them away. And that's the entirety. There's no logic here. But for convenience, let's make it easy to grab the data set out of them as well. And remember, we're now using... You can either use the handmade data loader that we built in the last one, or you can use the PyTorch data loader. They're both providing exactly the same API at this point, except for the numWorkers issue. So remember that we passed these data loaders a data set that you can access. And then it would be nice if we could create a getModel function, which could create our model but automatically set the last layer to have the correct number of activations, because the data knows how many activations it needs. So let's also optionally make it that you can pass in C, which is going to get stored away, so that then when we create our data, we can pass in C, which remember we set to our maximum Y value. And so that way, we never have to think about that again. So that's our data bunch class. So there's our getModel. So it's just going to create a model with the number of inputs is the size of the input data. Number of hidden is whatever we had earlier, whatever we pass in, then a value, and then a linear from hidden to data.c, and return the model and an optimizer. And we all know all about dot parameters now. So then the other rest of the stuff, model, lossfunk, opt, and data, let's store them in something. Model, opt, lossfunk, data, and we'll just store them away. And that thing we'll call a learner. So notice our learner class has no logic at all. It's just a storage device for these four things. So now we can create a learner passing in the model and the optimizer. Since they're returned in this order from getModel, we can just say star getModel. So that's going to pass in the model and the optimizer. And we've got our loss function already. At the top here, we set it to cross entropy. And we've got our data, because it's that data bunch we just created. So there's nothing magic going on with data bunches and learners. They're just wrappers for the information that we need. So now we'll take the fit function we had before, and I just pasted it here. But every time I had model, I replaced it with learn.model. Every time I had data, I replaced it with learn.data, and so forth. So there's the exact same thing that we had before, still working fine. And so now, let's add callbacks. So our fit function before basically said for epoch, in range epochs, for batch, in train dl, and then it had these contents, right? Predictions, loss, backwards, step, zero grad. I factored out the contents into something called one batch. Okay, and then I added all these callbacks. All right, cb.afterbackward, cd.afterstep. I did one other refactoring, which is that the training loop has to loop through every batch, and the validation loop has to loop through every batch. So I just created something called all batches. Okay, so this is my fit loop, right? Begin fit, the epoch in epochs, begin epoch, all batches with a training set. Begin validate, no grad, all batches with a validation set, after epoch, after fit. Okay, so that's that. So here's a callback, right, which has all the stuff. And so then we need a callback handler, and that's gonna be something which you just say, here's all my callbacks. And basically, it's just gonna go through for each thing and say, go through every callback and call it. And keep track of whether we've received a false yet or not. False means don't keep going anymore, and then return it. So we do that for begin fit, after fit, begin epoch, begin validate, after epoch, begin batch, after loss, after backward, after step. So here's an example of a little callback we could create. And it's one that's going to, at the start of the fit, it'll set number of iterations to zero. And then after every step, it'll say number of iterations plus equals one, and print that out. And if we get past ten iterations, then it'll tell the learner to stop. Because we have this little thing called do stop, that gets checked at the end. So let's test it. There we go. And so it called fit, and it only did ten batches. And this is actually a really handy callback, because quite often you want to just run a few batches to make sure things seem to be working. You don't want to run a whole epoch. So here's a quick way you can do something like that. This is basically what fast AI V1 looks like right now. It does have a little bit of extra stuff that lets you pass back a different loss and different data, but it's nearly exactly the same. But I really like rewriting stuff, because when I rewrite stuff, it lets me kind of look and see what I've written. And when I looked back at this, I saw cb, cb, cb, cb, cb. There's this object, the cb is the callback handler, that's being passed everywhere. And that's a code smell. That code smell says something should have that state. And specifically, these three functions should be the methods of something that has this state. So after I kind of wrote this part of the lesson, I suddenly realized, fast AI is doing it the dumb way. So let's fix it. So, and this is likely to appear in a future version of fast AI. I created a new thing called runner. And so runner is a new class that contains the three things I just said. One batch, all batches, and fit. And the runner, so here's fit, right? It's incredibly simple. We're gonna keep track of how many epochs we're doing. We're gonna keep track of the learner that we're running. And remember, the learner has no logic in it, the stores for things, okay? And then we tell each of our callbacks what runner they're currently working with, and then we call begin fit. And then we go through each epoch, set the epoch, we call begin epoch, we call all batches. And then with no grad, we call begin validate, and then we call all batches. And then we call after epoch, and then we call after fit. That's it. Now, this self string might look a bit weird, but look at what we had before. Again, horrible code smell is lots of duplicate code. Res equals true for callback, blah, blah, blah, blah, blah. Begin epoch. Res equals true for callback, blah, blah, blah, blah, blah, begin validate. So that's bad, right? Code duplication means cognitive overhead to understand what's going on. Lots of opportunities to accidentally have one or instead of an and. Lots of places you have to change if you need to edit something. So basically I took that out and I factored it out into done to call. So done to call is the thing that we've seen it before. It's the thing that lets you treat an object as if it was a function. So I could have called this lots of things. I could have called it self.runCallback or whatever, right? But it's the thing that happens absolutely everywhere. And so my kind of rule of thumb is if you do something lots of times, make it small. So done to call is the smallest possible way you can call something. You don't have to give it a name at all when you call it. So we say call the callback called after epoch. It also makes sense, right? We're calling a callback, so why not use done to call to call a callback? So after epoch, I gotta go through all of my callbacks. I'll talk about this sorted in a moment. And then the other thing I didn't like before is that all of my callbacks had to inherit from this callback superclass. Cuz if they didn't, then they would have been missing one of these methods. And so then when it tried to call the method, there would have been an exception. And I don't like forcing people to have to inherit from something. They should be able to do whatever they like. So what we did here was we used get attribute, which is the Python thing which says look inside this object and try to find something of this name, eg begin validate. And default to none if you can't find it, right? So it tries to find that callback and there'll be none if the callback doesn't exist. And if you find it, then you can call it, right? So this is a nice way to call any callback. But when you implement a callback, as you can see, look how much easier our test callback is now, right? It's just super simple, just implement what you need. And we inherit from a new callback class, but we don't have to anymore, right? The main reason why is that our callback class now has an underscore order, which we can use to choose what order callbacks run in. We'll talk about that after we handle this question. What is the difference between hooks and PyTorch and callbacks in FastAI? We're gonna do hooks very shortly. But if you think about it, if I want to kind of add a callback after I calculate the forward pass of the second layer of my model, there's no way for me to do that, right? Because the point at which I do the forward pass looks like this. Self.model, right? Or if I want to hook into the point at which I've just called the backward pass of my penultimate layer, I can't do that either, because the whole thing appears here as self.lost or backward, okay? So, PyTorch hooks are callbacks that you can add to specific PyTorch modules. And we're gonna see them in very shortly. Well, it might be next class. We'll see how we go. Okay, so. Very often you wanna be able to inject behavior into something, but the different things can influence each other. For example, transformations, we're gonna be seeing this when we do data augmentation. So quite often you'll need things to run in a particular order. So when I add this kind of injectable behavior like callbacks, I like to just add something, which is what order should it run in? You don't have to put this here. You might have noticed that what I do when I call this is I, oh, this currently does, sorry. Actually, when we look at transformations, it won't require order. This one does require an order. So, yeah, okay, so your callbacks need to be something that have an underscore order attribute in them. And this way we can make sure that some things run after other things. So for example, you might have noticed that our runner in the fit function never calls model.eval, never calls model.train. So it literally doesn't do anything. It just says these are the steps I have to run and the callbacks do the running. So I created a train eval callback that at the beginning of an epoch, calls model.train and at the beginning of validation, calls model.neval. And I also added stuff to keep track of how many epochs has it done. And this is quite nice. It actually does it as a floating point, not just as an int. So you could be like 2.3 epochs in. It also keeps track of how many iterations do you want. So now we have this thing keeping track of iterations. Our test callback that should stop training after ten iterations. Rather than keeping track of n itters itself, it should just use the n itter that was defined in this callback. So what we can do is we can say, all right, well train eval callback has an order of zero because it inherits. So what we can just do here is make sure that this is later, underscore order equals one. And so that way we can now refer to stuff that's inside the train eval callback like n itter. Sorry, well actually we don't even need to do that because it's putting n itter inside self.run. So we can just go self.n itter. If this ran before train eval callback, that would be a problem because n itter might not have been updated yet. So that's what the order's for. Another nice thing about runner, sorry, a nice thing about class callback is that I've defined dunder getAtra. And I've defined it to say return getAtra self.run, k. An important thing to know about dunder getAtra is that it is only called by Python if it can't find the attribute that you've asked for. So if something asks for self.name, well I have self.name, so it's never gonna get to here. So if you get to here, it means Python looked for this attribute and it couldn't find it. And so very, very often the thing you actually want in the callback is actually inside the runner, which we store away as self.run. So this means that in all of our callbacks, let's look at one, you can basically just use self. pretty much everything. And it will grab what you want, even though most of the stuff you want is inside the runner. So you'll see this pattern in fast AI a lot is that when one object contains another object or composes another object, we very often delegate get attribute to the other object. So for example, if you're looking at a data set, then I think we delegate to x. If you're looking at stuff in the data box API, it'll often delegate to stuff lower in the data box API and so forth. So I find this pretty handy. Okay, so we have a callback that as you see, there's very little to it. One interesting thing you might notice is that a callback has a name property. And the name property works like this. If you have a property called train eval callback, then we've got a function called camel to snake. This is called camel case means you've got uppercase and lowercase letters like a camel and snake case looks like this. So camel to snake turns this into a snake. And then what we do here is we remove callback from the end. And that's its name. So train eval callback has a name, which is just train eval with an underscore. And then in the runner, any callback functions that you pass in, which it uses to create new callbacks, it actually assigns them to an attribute with that name. So we now have something called runner.trainEval, for example. So we do this in the fast AI library. When you say learn.recorder, we didn't actually add an attribute called recorder to learner. It just automatically sets that because there's a learner callback. So let's see how to use this. There's a question. Okay, let's do that in a moment. So let's use this to add metrics, because it's no fun having a training loop where we can't actually see how we're going. And part of the whole point of this is that our actual training loop is now so incredibly tight and neat and easy. But we actually want to do all the stuff we want to do. So what if we create a little callback called averageStatsCallback, right? Where we're going to stick into a couple of objects to keep track of our loss and metrics, one for training, one for valid. And at the start of an epoch, we'll reset the statistics. At the end of an epoch, we'll print out the statistics. And after we've got the loss calculated, we will accumulate the statistics. So then all we need is an object that has an accumulate method. So let's create a class that does that. And here's our accumulate method. It's going to add up the total loss. And for each metric, it'll add up the total metrics. And then we'll give it a property called averageStats that will go through all of those losses and metrics and return the average. And you might notice here I've fixed the problem of having different batch sizes in the average. We're actually adding loss times the size of the batch and count plus the size of the batch and metrics times the size of the batch. And so then we're dividing here by the total batch size. So this is going to keep track of our stats. We'll add a Dundar repra so that it prints out those statistics in a nice way. And so now we can create our learner, add our averageStats callback. And when we call fit, it prints out how we're going. And so that's the entirety of what it talked to add metrics and loss tracking to our minimal training loop. Yes, Rachel. Runner Dundar call exits early when the first callback returns true. Why is that? So one of the things I noticed was really annoying in the first way I wrote the callback handler was I had it so that I could get the callback to work. So something had to return true to mean keep going. So basically false meant stop. And that was really awkward because if you don't add a return in Python, then it actually returns none. And none is false. And I thought, oh, if I forget to return something, that should mean keep going. That should be like the default. So the first thing to point out is that the basic loop now actually says if not rather than if. Right? So if not, begin epoch. So in other words, if your callback handler returns false, then keep going. And so that means that basically none of my callbacks need to return anything most of the time except for test callback, which returns true. So true means cancel. It means stop. So if one of my callbacks says stop, then I mean, I could certainly imagine an argument in either way. But the way I thought it, if it says stop, let's just stop right now. You know? Why do we need to run the other callbacks? So if it says stop, then it returns stop. It says we don't want to go anymore. And then we can, depending on where you are, so if after epoch returns stop, then it's actually going to stop the loop entirely. So that's why. Yeah, so this is a little awkward. We had to construct our average stats callback. And then we had to pass that to run. And then later on, we can refer to stats.validStats.averageStats. Because remember, averageStats was where we grabbed this. So that's okay, but it's a little awkward. So instead, what I do is I create a accuracy callback function. So that is the averageStats callback constructor passing in accuracy, but with partial. So partial is a function that returns a function. And so this is now a function which can create a callback. And so I can pass this to cbfuncs. And now I don't have to store it away, because the runner is going to be able to, this is what we saw before. The runner will go through hcbfuncs, it will call that function to create the callback, and then it will stick that callback inside the runner, giving it this name as the attribute. So this way, we can say, this is our callback function, this is our runner, fit, and now it's automatically available inside run.averageStats. So this is what FastAI v1 does, except it puts them inside a learner, because we don't have a runner concept. So I think that's pretty handy. It's kind of like, it looks a little bit awkward the first time you do it, but you can kind of create a standard set of callback functions that you want to use for particular types of models, and then you can just store them away in a list, and you don't have to think about them again, which is what you'll see we'll do lots of times. So, like a lot of things in this part two of the course, you can choose how deep to go on different things. I think our approach to callbacks is super interesting, and if you do too, you might want to go deep here and really look into what kind of callbacks you can build, and what things you can do with them that we haven't done yet. But then a lot of these details around exactly how I do this, if you're not as interested in the details of software engineering, this might be something you care less about, which is fine. The main thing that everybody should take away is that. That's our training loop. Okay? So the other stuff about exactly how do we create our average stats callback, and exactly what does Dundalk call do, are fairly minor details, but you should recognize that the fit function stores how many epochs we're doing, what learner we're working with, calls each of the different callbacks at each time, right? And like I never remember which ones are at which place. If you go to docs.fast.ai, the callbacks documentation will show you. Personally, I just always look at the source code, because it's just so easy to see exactly what happens at each time, and exactly what's available at each time. So let's use this. And let's use this to do one cycle training, because it's pretty hard when you have to have a constant learning rate all the time, particularly because I was really wanting to show you like a deep dive, which you're about to see using hooks, a deep dive into how the mechanics or kind of how the dynamics of training models looks like. And what we'll learn is that the first batches, everything, if you can get the first batches working well, then things will tend to be, then things will tend to be good. And this is how you can get super convergence. So if you want your first batches to be good, it turns out that good annealing is critical. So let's do that right away. Let's set up good annealing, because we have the mechanics we need, because we have callbacks. So we're inside 05 anneal. We'll get our data. This is all the same as before. Here's something to create a learner with one line. So let's create a learner with that same little model we had before and loss function our data. And we'll create a runner with our average stats callback. This defaulted to a learning rate of 0.5. Maybe we could try it with learning rate of 0.3. It's pretty handy being able to like quickly create things with different learning rates. So let's create a function that's just going to be partial get model with a learning rate. And so now we can just call get model func and pass the learning rate in. And we'll immediately have something with a different learning rate. Yes, tell me the question. So what is your typical debugging process? My debugging process is to use the debugger. So if I got an exception while I was running a cell, then I just go into the next cell and type %debug. And that pops open the debugger. If things aren't working the way I expected, but it wasn't an exception, then I'll just add set underscore trace somewhere around the point I care about. That's about it. Yeah, I find that works pretty well. Most of the time, then it's just a case of looking at what's the shape of everything. And what does everything contain? Like a couple of objects in the batch. I normally find something's got nans or zeros or whatever. Yeah, it's really rare that using the debugger that I find debugging is that difficult. If it is, then it's just a case of stepping away and questioning your assumptions. But with the help of a debugger, all of the states right there in front of you, which is one of the great things about PyTorch, is that it supports this kind of development. Okay. All right. So we're going to create a callback that's going to do hyperparameter scheduling. And so for this notebook, we're just going to do learning rate as a hyperparameter. But in the last 12 months, one of the really successful areas of research have been people pointing out that you can and should schedule everything. Your dropout amount, what kind of data augmentation you do, weight decay, learning rate, momentum, everything. Which makes sense, right? Because the other thing that we've been learning a lot about in the last 12 months is how as you train a model, it kind of goes through these different phases of the kind of weight landscapes, sorry, the loss function, the loss landscapes of neural nets look very different at the start, in the middle, and at the end. And so it's very unlikely that you would want the same hyperparameters throughout. So being able to schedule anything is super handy. So we'll create a parameter scheduler callback, and you're just going to pass in a function, right, and a parameter to schedule. So we're going to be passing in LR, because LR is what PyTorch calls learning rate. And then this function will be something which takes a single argument which is number of epochs divided by total epochs. Remember I told you that that train eval callback we added is going to set this to be a float. So this will be the number, this will be like epoch number 2.35 out of 6. So this will be a float of exactly how far through training are we. And we'll pass that to some function that we're going to write, and the result of that function will be used to set the hyperparameter. In this case, learning rate. As you know from part one, you don't necessarily want to have the same value of a hyperparameter for all of your layers. So PyTorch has something called parameter groups, which we use in abstraction we call layer groups in fast AI, but they're basically the same thing. And so a PyTorch optimizer contains a number of parameter groups. Unless you explicitly create more than one, it'll all be in one. But anytime we do stuff with hyperparameters, you have to loop through PG in self.opt.parameter groups. So then parameter group, so learning rate for this parameter group, this layer group, is equal to the result of this function. And then every time we start a new batch, if we're training, then we'll run our scheduler. Pretty hard to know if our scheduler is working, if we can't actually see what's happening to the learning rate as we go. So let's create another callback called recorder that at the start of fit fitting sets the LRs and losses arrays to being empty. And then after each batch, as long as you're training, it appends the current learning rate and the current loss. Now there's actually lots of learning rates potentially because there's lots of layer groups. So in fast AI, we tend to use the final layer group as the learning rate we actually print out. But you don't have to do it that way. And then we'll add something to plot the learning rates and we'll add something to plot the losses. So hopefully this looks like something that we can use in the future. So hopefully this looks pretty familiar compared to the recorder in fast AI v1. So with that in place, we now need to create a function that takes the percentage through the learning, which we're going to call pause for position, and returns the value of learning rate. And so let's create one for linear schedules. So what we want to be able to pass this is a starting learning rate and an ending learning rate. So we might pass it 10 and 1, and it would start at a learning rate of 10 and go down to 1. That would be ridiculously high, but whatever. But we need a function that just takes position. So this is a function that's going to return a function. So here's a function that takes a start learning rate and an end learning rate and a position and returns the learning rate. So to start plus position times difference. So to convert that function into one which only takes position, we do partial, passing in that function and the start and the end we were given. So now this function just takes position because that's the only thing from inner that we haven't set. So that's going to work fine, but it's inconvenient because we're going to create lots of different schedulers, and I don't want to have to write all this every time. So we can simplify the way that you can create these by using a decorator. Here's the version with a decorator. With a decorator, you create linear scheduler in the natural way. It's something that takes a start learning rate and end learning rate and a position and returns this. And then we add an annealer decorator, and the annealer decorator is the thing that does all this inner partial nonsense. What's a decorator? A decorator is a function that returns a function, and what Python does is if it sees the name of a function here with an at sign before it, then it takes this function, passes it into this function and replaces the definition of this function with whatever this returns. So it's going to take this. It's going to pass it over here. And then it's going to say return inner where inner is partial as we described before. So let's see that. So now shed Lin we wrote it as taking start and end and pause, but if I hit shift tab. This says it only takes start and end. Why is that because we've replaced this function with this function. And this function just takes that and end. And this is where Jupiter is going to give you a lot more happy times and pretty much any IDE because this kind of dynamic code generation is going to be a lot more important. Code generation it's pretty hard for an IDE to do that for you or else in Jupiter. It's actually running the code in an actual Python process, so it knows exactly what shed Lin means. OK, so this is now created a function that takes start. And end and returns a function. Which takes. Pause which is what we need. For our scheduler. So let's try it. Let's say F equals shed Lin 1, 2, so this is a scheduler that starts at learning rate 1 ends at learning rate 2 and then we'll say hey what should that be 30% of the way through training. And again if I hit shift tab here. It knows that F is something that takes pause right so it's it's really nice in Jupiter you can you can take advantage of. Python's dynamic nature. And like. There's no point using a dynamic language if you're not taking advantage of his dynamic nature right so things like. Decorators are super convenient way to do this stuff. There are other languages like Julia that can do similar things with macros like it's this is not the only way to get this kind of nice. A very expressive ability but it's one good way to do it. So now we can just go ahead and define all of our different schedule is by passing it each is start and pause so, for example, no schedule is something which always return start or cosine scheduling. Exponential scheduling. So. So. Let's define those and then let's try to plot them and it doesn't work why doesn't it work because you can't plot pie torch tensors. But it turns out the only reason you can't plot pie torch tensors is because tensors don't have an end in attribute which tells Matt plot lib how many dimensions there are so watch this. Torch dot tensor dot m dim equals a property that is the length of the shape. This is now replaced the definition again using the dynamic features of Python replace the death replace actually insert into the definition of tensor and your property called end in. And now we can plot tensors right so like the nice thing about Python is you never have to be like oh this isn't supported because you can change everything you can insert things you can replace things whatever so. Here we've now got a nice print out of our four different schedules. Which isn't really enough because if you want to do one cycle scheduling then and fattened but you know most of the time nowadays you want some kind of warm up and some kind of cool down or if you're doing something like sgdr you've got like multiple. Call downs so we really need to be able to paste some of these schedule is together. So let's create a nother function called combine schedule is and it's going to look like this. We're going to pass in. We're going to pass in we're going to pass in the kind of the phases we want so phase one will be a cosine schedule from a learning rate of point three to point six phase two will be a learning rate as cosine schedule with the learning rate going from point six to point two. And phase one will take up 30% of our batches and phase two will take up 70% so that's what we're going to pass in how long is each phase and what's the schedule in each phase. So. Here's how we do that I don't think I need to go through the code it's there's nothing interesting about it. But what we do once we have that is that we can then plot that schedule and you can kind of see why we're very fond of these. Cosine one cycle schedules I don't think this has ever been published anywhere but it's what fast AI users by default nowadays. Is you kind of get a nice gentle warm up at the start, this is the time when things are just super sensitive and fall apart really quickly. But it doesn't take long as you'll see in next week's lesson when we do a deep dive into into stuff using hooks. It doesn't take long for it to get into a decent part of the lost landscape and so you can quite quickly increase the learning rate. And then something that people have and we'll start looking at papers next week for this something that people have realized in the last. Four months or so although Leslie Smith really kind of showed us this two years ago, but it's only been the last four months or so that people have really understood this in the wider academic literature. You need to train at a high learning rate for a long time and so with this kind of coach cosine schedule we keep it up high for a long time. But then you also need to fine tune at a very low learning rate for a long time, so this has all of the kind of nice features that we want so cosine one cycle schedules are terrific and we now can build them from scratch. So let's try trading like this so with let's create a list of callback functions that has a recorder in it and average stats callback with accuracy in it and a parameter scheduler that schedules the learning rate using this schedule. And then fit. That's looking pretty good we're getting up towards 94% pretty quickly and we can now go plot LR. And it's the shape that we hoped for and we can even say plot loss. Okay. So. We now have. Really all of the pieces we need. To. To kind of try out lots of different ways of of training neural nets we still haven't looked at convolutions really will do that next week and a lot more. But you kind of have the ability now to hopefully think of lots of things that you might want to try and and try them out. So. So next week we're going to be starting with. Conf nets. We're going to be. Kind of and we're going to be finally using our GPU because once we because once we start creating confidence of this size it starts taking a little bit too long. But just to read ahead a little bit how what's it going to take to put stuff on the GPU this is the entirety of the callback. So we've now got the mechanics we need to do things unbelievably quickly. And then we will be able to oh and also we'll be wanting to add some transformations this is the one we're going to be using. Also we'll be wanting to add some transformations this is the entirety of what it takes to do batch wise transformations. Without callback. As we discussed though we can't add callbacks between layers so we will add callbacks between layers initially manually. And then using pie torch hooks and that way we're going to be able to plot and see exactly what's going on inside our models as they train. And we'll find ways to train them. Much much more nicely so that by the end of next by the end of the next notebook will be up over 98% accuracy. And that's going to be super cool. And then we're going to do a deep dive into batch norm. Data blocks API. Optimizes and transforms and at that point I think we'll have basically all the mechanics we need to go into some more advanced architectures and training methods and see how we did some of the. Call stuff that we did in part one so I'll see you next week.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 8.94, "text": " Welcome back to lesson nine, part two, how to train your model.", "tokens": [4027, 646, 281, 6898, 4949, 11, 644, 732, 11, 577, 281, 3847, 428, 2316, 13], "temperature": 0.0, "avg_logprob": -0.1446794452089252, "compression_ratio": 1.7174887892376682, "no_speech_prob": 0.019699577242136}, {"id": 1, "seek": 0, "start": 8.94, "end": 13.040000000000001, "text": " Before we talk about training our model though, I wanted to revisit a couple of things that", "tokens": [4546, 321, 751, 466, 3097, 527, 2316, 1673, 11, 286, 1415, 281, 32676, 257, 1916, 295, 721, 300], "temperature": 0.0, "avg_logprob": -0.1446794452089252, "compression_ratio": 1.7174887892376682, "no_speech_prob": 0.019699577242136}, {"id": 2, "seek": 0, "start": 13.040000000000001, "end": 15.76, "text": " came up last week.", "tokens": [1361, 493, 1036, 1243, 13], "temperature": 0.0, "avg_logprob": -0.1446794452089252, "compression_ratio": 1.7174887892376682, "no_speech_prob": 0.019699577242136}, {"id": 3, "seek": 0, "start": 15.76, "end": 23.400000000000002, "text": " And the reason I really wanted to revisit them is because I wanted to kind of give you", "tokens": [400, 264, 1778, 286, 534, 1415, 281, 32676, 552, 307, 570, 286, 1415, 281, 733, 295, 976, 291], "temperature": 0.0, "avg_logprob": -0.1446794452089252, "compression_ratio": 1.7174887892376682, "no_speech_prob": 0.019699577242136}, {"id": 4, "seek": 0, "start": 23.400000000000002, "end": 24.84, "text": " an insight into how I do research.", "tokens": [364, 11269, 666, 577, 286, 360, 2132, 13], "temperature": 0.0, "avg_logprob": -0.1446794452089252, "compression_ratio": 1.7174887892376682, "no_speech_prob": 0.019699577242136}, {"id": 5, "seek": 0, "start": 24.84, "end": 29.0, "text": " I mean, a lot of this course really will be me showing you how I do research and how I", "tokens": [286, 914, 11, 257, 688, 295, 341, 1164, 534, 486, 312, 385, 4099, 291, 577, 286, 360, 2132, 293, 577, 286], "temperature": 0.0, "avg_logprob": -0.1446794452089252, "compression_ratio": 1.7174887892376682, "no_speech_prob": 0.019699577242136}, {"id": 6, "seek": 2900, "start": 29.0, "end": 33.64, "text": " do software development in the hope that that is somewhat helpful to you.", "tokens": [360, 4722, 3250, 294, 264, 1454, 300, 300, 307, 8344, 4961, 281, 291, 13], "temperature": 0.0, "avg_logprob": -0.1516528439212155, "compression_ratio": 1.5026737967914439, "no_speech_prob": 4.329062721808441e-05}, {"id": 7, "seek": 2900, "start": 33.64, "end": 44.64, "text": " So one of the questions that came up last week was we looked at, how's the size of that?", "tokens": [407, 472, 295, 264, 1651, 300, 1361, 493, 1036, 1243, 390, 321, 2956, 412, 11, 577, 311, 264, 2744, 295, 300, 30], "temperature": 0.0, "avg_logprob": -0.1516528439212155, "compression_ratio": 1.5026737967914439, "no_speech_prob": 4.329062721808441e-05}, {"id": 8, "seek": 2900, "start": 44.64, "end": 54.400000000000006, "text": " We looked at, we looked inside the nn.conf2d that comes with PyTorch to see how it goes", "tokens": [492, 2956, 412, 11, 321, 2956, 1854, 264, 297, 77, 13, 24697, 17, 67, 300, 1487, 365, 9953, 51, 284, 339, 281, 536, 577, 309, 1709], "temperature": 0.0, "avg_logprob": -0.1516528439212155, "compression_ratio": 1.5026737967914439, "no_speech_prob": 4.329062721808441e-05}, {"id": 9, "seek": 2900, "start": 54.400000000000006, "end": 56.56, "text": " about initializing parameters.", "tokens": [466, 5883, 3319, 9834, 13], "temperature": 0.0, "avg_logprob": -0.1516528439212155, "compression_ratio": 1.5026737967914439, "no_speech_prob": 4.329062721808441e-05}, {"id": 10, "seek": 5656, "start": 56.56, "end": 62.28, "text": " And we found that inside here, convend.reset parameters, we found the way that it does", "tokens": [400, 321, 1352, 300, 1854, 510, 11, 3754, 521, 13, 495, 302, 9834, 11, 321, 1352, 264, 636, 300, 309, 775], "temperature": 0.0, "avg_logprob": -0.14264857874507397, "compression_ratio": 1.6612903225806452, "no_speech_prob": 2.0457391656236723e-05}, {"id": 11, "seek": 5656, "start": 62.28, "end": 63.28, "text": " initialization.", "tokens": [5883, 2144, 13], "temperature": 0.0, "avg_logprob": -0.14264857874507397, "compression_ratio": 1.6612903225806452, "no_speech_prob": 2.0457391656236723e-05}, {"id": 12, "seek": 5656, "start": 63.28, "end": 72.22, "text": " And we found this math.square root 5 without any commentary, which was quite mysterious.", "tokens": [400, 321, 1352, 341, 5221, 13, 33292, 543, 5593, 1025, 1553, 604, 23527, 11, 597, 390, 1596, 13831, 13], "temperature": 0.0, "avg_logprob": -0.14264857874507397, "compression_ratio": 1.6612903225806452, "no_speech_prob": 2.0457391656236723e-05}, {"id": 13, "seek": 5656, "start": 72.22, "end": 77.36, "text": " So I decided to do some research into, you know, kind of dual research.", "tokens": [407, 286, 3047, 281, 360, 512, 2132, 666, 11, 291, 458, 11, 733, 295, 11848, 2132, 13], "temperature": 0.0, "avg_logprob": -0.14264857874507397, "compression_ratio": 1.6612903225806452, "no_speech_prob": 2.0457391656236723e-05}, {"id": 14, "seek": 5656, "start": 77.36, "end": 81.08, "text": " One is like, what's the impact of this math.square root 5?", "tokens": [1485, 307, 411, 11, 437, 311, 264, 2712, 295, 341, 5221, 13, 33292, 543, 5593, 1025, 30], "temperature": 0.0, "avg_logprob": -0.14264857874507397, "compression_ratio": 1.6612903225806452, "no_speech_prob": 2.0457391656236723e-05}, {"id": 15, "seek": 5656, "start": 81.08, "end": 85.32000000000001, "text": " And then at the same time, trying to get in touch with the PyTorch team about asking them", "tokens": [400, 550, 412, 264, 912, 565, 11, 1382, 281, 483, 294, 2557, 365, 264, 9953, 51, 284, 339, 1469, 466, 3365, 552], "temperature": 0.0, "avg_logprob": -0.14264857874507397, "compression_ratio": 1.6612903225806452, "no_speech_prob": 2.0457391656236723e-05}, {"id": 16, "seek": 8532, "start": 85.32, "end": 88.36, "text": " where this math.square root 5 comes from.", "tokens": [689, 341, 5221, 13, 33292, 543, 5593, 1025, 1487, 490, 13], "temperature": 0.0, "avg_logprob": -0.10999451431573606, "compression_ratio": 1.362962962962963, "no_speech_prob": 1.3844626664649695e-05}, {"id": 17, "seek": 8532, "start": 88.36, "end": 97.75999999999999, "text": " So let me show you how I went about doing that research.", "tokens": [407, 718, 385, 855, 291, 577, 286, 1437, 466, 884, 300, 2132, 13], "temperature": 0.0, "avg_logprob": -0.10999451431573606, "compression_ratio": 1.362962962962963, "no_speech_prob": 1.3844626664649695e-05}, {"id": 18, "seek": 8532, "start": 97.75999999999999, "end": 106.13999999999999, "text": " So I loaded up just what we had from last week, which was the ability to download the", "tokens": [407, 286, 13210, 493, 445, 437, 321, 632, 490, 1036, 1243, 11, 597, 390, 264, 3485, 281, 5484, 264], "temperature": 0.0, "avg_logprob": -0.10999451431573606, "compression_ratio": 1.362962962962963, "no_speech_prob": 1.3844626664649695e-05}, {"id": 19, "seek": 10614, "start": 106.14, "end": 119.2, "text": " MNIST data and open it up, and then the function to normalize it, which I thought we'd export", "tokens": [376, 45, 19756, 1412, 293, 1269, 309, 493, 11, 293, 550, 264, 2445, 281, 2710, 1125, 309, 11, 597, 286, 1194, 321, 1116, 10725], "temperature": 0.0, "avg_logprob": -0.1269999159143326, "compression_ratio": 1.579185520361991, "no_speech_prob": 1.3006148947170004e-05}, {"id": 20, "seek": 10614, "start": 119.2, "end": 121.6, "text": " if we haven't already.", "tokens": [498, 321, 2378, 380, 1217, 13], "temperature": 0.0, "avg_logprob": -0.1269999159143326, "compression_ratio": 1.579185520361991, "no_speech_prob": 1.3006148947170004e-05}, {"id": 21, "seek": 10614, "start": 121.6, "end": 124.28, "text": " And then we'd grab the data and we'd normalize it.", "tokens": [400, 550, 321, 1116, 4444, 264, 1412, 293, 321, 1116, 2710, 1125, 309, 13], "temperature": 0.0, "avg_logprob": -0.1269999159143326, "compression_ratio": 1.579185520361991, "no_speech_prob": 1.3006148947170004e-05}, {"id": 22, "seek": 10614, "start": 124.28, "end": 129.52, "text": " And because we're going to be talking a lot about convolutions towards the end of today's", "tokens": [400, 570, 321, 434, 516, 281, 312, 1417, 257, 688, 466, 3754, 15892, 3030, 264, 917, 295, 965, 311], "temperature": 0.0, "avg_logprob": -0.1269999159143326, "compression_ratio": 1.579185520361991, "no_speech_prob": 1.3006148947170004e-05}, {"id": 23, "seek": 10614, "start": 129.52, "end": 134.68, "text": " lesson and particularly next lesson, I suspect, so I'll skip over some of the details about", "tokens": [6898, 293, 4098, 958, 6898, 11, 286, 9091, 11, 370, 286, 603, 10023, 670, 512, 295, 264, 4365, 466], "temperature": 0.0, "avg_logprob": -0.1269999159143326, "compression_ratio": 1.579185520361991, "no_speech_prob": 1.3006148947170004e-05}, {"id": 24, "seek": 13468, "start": 134.68, "end": 136.44, "text": " convolutions for now.", "tokens": [3754, 15892, 337, 586, 13], "temperature": 0.0, "avg_logprob": -0.12283750182216607, "compression_ratio": 1.5062240663900415, "no_speech_prob": 1.9220362446503714e-05}, {"id": 25, "seek": 13468, "start": 136.44, "end": 142.6, "text": " But basically to do a convolution, as you know, we need a square or rectangular input,", "tokens": [583, 1936, 281, 360, 257, 45216, 11, 382, 291, 458, 11, 321, 643, 257, 3732, 420, 31167, 4846, 11], "temperature": 0.0, "avg_logprob": -0.12283750182216607, "compression_ratio": 1.5062240663900415, "no_speech_prob": 1.9220362446503714e-05}, {"id": 26, "seek": 13468, "start": 142.6, "end": 147.52, "text": " and our MNIST input, remember, was just a single vector per image 768 long.", "tokens": [293, 527, 376, 45, 19756, 4846, 11, 1604, 11, 390, 445, 257, 2167, 8062, 680, 3256, 24733, 23, 938, 13], "temperature": 0.0, "avg_logprob": -0.12283750182216607, "compression_ratio": 1.5062240663900415, "no_speech_prob": 1.9220362446503714e-05}, {"id": 27, "seek": 13468, "start": 147.52, "end": 154.76000000000002, "text": " So I resized them all to 28 by 28 one-channel images so that we could test out the impact", "tokens": [407, 286, 725, 1602, 552, 439, 281, 7562, 538, 7562, 472, 12, 339, 11444, 5267, 370, 300, 321, 727, 1500, 484, 264, 2712], "temperature": 0.0, "avg_logprob": -0.12283750182216607, "compression_ratio": 1.5062240663900415, "no_speech_prob": 1.9220362446503714e-05}, {"id": 28, "seek": 13468, "start": 154.76000000000002, "end": 163.52, "text": " of this conv2d in it in PyTorch and set up the various variables that we wanted to have.", "tokens": [295, 341, 3754, 17, 67, 294, 309, 294, 9953, 51, 284, 339, 293, 992, 493, 264, 3683, 9102, 300, 321, 1415, 281, 362, 13], "temperature": 0.0, "avg_logprob": -0.12283750182216607, "compression_ratio": 1.5062240663900415, "no_speech_prob": 1.9220362446503714e-05}, {"id": 29, "seek": 16352, "start": 163.52, "end": 166.24, "text": " And then I created a conv2d layer.", "tokens": [400, 550, 286, 2942, 257, 3754, 17, 67, 4583, 13], "temperature": 0.0, "avg_logprob": -0.12657058806646437, "compression_ratio": 1.5649122807017544, "no_speech_prob": 5.862544185220031e-06}, {"id": 30, "seek": 16352, "start": 166.24, "end": 170.48000000000002, "text": " So we have one input because it's just one channel, NH, which is number hidden, which", "tokens": [407, 321, 362, 472, 4846, 570, 309, 311, 445, 472, 2269, 11, 31118, 11, 597, 307, 1230, 7633, 11, 597], "temperature": 0.0, "avg_logprob": -0.12657058806646437, "compression_ratio": 1.5649122807017544, "no_speech_prob": 5.862544185220031e-06}, {"id": 31, "seek": 16352, "start": 170.48000000000002, "end": 173.76000000000002, "text": " is 32 outputs, and let's do a 5 by 5 kernel.", "tokens": [307, 8858, 23930, 11, 293, 718, 311, 360, 257, 1025, 538, 1025, 28256, 13], "temperature": 0.0, "avg_logprob": -0.12657058806646437, "compression_ratio": 1.5649122807017544, "no_speech_prob": 5.862544185220031e-06}, {"id": 32, "seek": 16352, "start": 173.76000000000002, "end": 177.52, "text": " And we'll talk more about why 5 by 5 might be suitable.", "tokens": [400, 321, 603, 751, 544, 466, 983, 1025, 538, 1025, 1062, 312, 12873, 13], "temperature": 0.0, "avg_logprob": -0.12657058806646437, "compression_ratio": 1.5649122807017544, "no_speech_prob": 5.862544185220031e-06}, {"id": 33, "seek": 16352, "start": 177.52, "end": 182.46, "text": " And just for testing, let's just grab the first 100 elements of the validation set.", "tokens": [400, 445, 337, 4997, 11, 718, 311, 445, 4444, 264, 700, 2319, 4959, 295, 264, 24071, 992, 13], "temperature": 0.0, "avg_logprob": -0.12657058806646437, "compression_ratio": 1.5649122807017544, "no_speech_prob": 5.862544185220031e-06}, {"id": 34, "seek": 16352, "start": 182.46, "end": 188.06, "text": " So we've now got a tensor of 100 by 1 by 28 by 28.", "tokens": [407, 321, 600, 586, 658, 257, 40863, 295, 2319, 538, 502, 538, 7562, 538, 7562, 13], "temperature": 0.0, "avg_logprob": -0.12657058806646437, "compression_ratio": 1.5649122807017544, "no_speech_prob": 5.862544185220031e-06}, {"id": 35, "seek": 16352, "start": 188.06, "end": 193.48000000000002, "text": " So it's a really good idea when you're playing with anything in software development, but", "tokens": [407, 309, 311, 257, 534, 665, 1558, 562, 291, 434, 2433, 365, 1340, 294, 4722, 3250, 11, 457], "temperature": 0.0, "avg_logprob": -0.12657058806646437, "compression_ratio": 1.5649122807017544, "no_speech_prob": 5.862544185220031e-06}, {"id": 36, "seek": 19348, "start": 193.48, "end": 198.16, "text": " including notebooks, to refactor things.", "tokens": [3009, 43782, 11, 281, 1895, 15104, 721, 13], "temperature": 0.0, "avg_logprob": -0.12379529259421608, "compression_ratio": 1.7615384615384615, "no_speech_prob": 2.586652772151865e-05}, {"id": 37, "seek": 19348, "start": 198.16, "end": 202.51999999999998, "text": " So I'm going to be wanting to look at the mean and standard deviation of a bunch of", "tokens": [407, 286, 478, 516, 281, 312, 7935, 281, 574, 412, 264, 914, 293, 3832, 25163, 295, 257, 3840, 295], "temperature": 0.0, "avg_logprob": -0.12379529259421608, "compression_ratio": 1.7615384615384615, "no_speech_prob": 2.586652772151865e-05}, {"id": 38, "seek": 19348, "start": 202.51999999999998, "end": 203.51999999999998, "text": " things.", "tokens": [721, 13], "temperature": 0.0, "avg_logprob": -0.12379529259421608, "compression_ratio": 1.7615384615384615, "no_speech_prob": 2.586652772151865e-05}, {"id": 39, "seek": 19348, "start": 203.51999999999998, "end": 206.44, "text": " So let's create a little function called stats to do that.", "tokens": [407, 718, 311, 1884, 257, 707, 2445, 1219, 18152, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.12379529259421608, "compression_ratio": 1.7615384615384615, "no_speech_prob": 2.586652772151865e-05}, {"id": 40, "seek": 19348, "start": 206.44, "end": 208.79999999999998, "text": " And I never plan ahead what I'm going to do.", "tokens": [400, 286, 1128, 1393, 2286, 437, 286, 478, 516, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.12379529259421608, "compression_ratio": 1.7615384615384615, "no_speech_prob": 2.586652772151865e-05}, {"id": 41, "seek": 19348, "start": 208.79999999999998, "end": 212.76, "text": " When you see this in a notebook, it always means that I've written out that by hand,", "tokens": [1133, 291, 536, 341, 294, 257, 21060, 11, 309, 1009, 1355, 300, 286, 600, 3720, 484, 300, 538, 1011, 11], "temperature": 0.0, "avg_logprob": -0.12379529259421608, "compression_ratio": 1.7615384615384615, "no_speech_prob": 2.586652772151865e-05}, {"id": 42, "seek": 19348, "start": 212.76, "end": 215.28, "text": " and then I copied it, and then I'm like, OK, I'm using it twice.", "tokens": [293, 550, 286, 25365, 309, 11, 293, 550, 286, 478, 411, 11, 2264, 11, 286, 478, 1228, 309, 6091, 13], "temperature": 0.0, "avg_logprob": -0.12379529259421608, "compression_ratio": 1.7615384615384615, "no_speech_prob": 2.586652772151865e-05}, {"id": 43, "seek": 19348, "start": 215.28, "end": 216.28, "text": " I'll chuck it in a function.", "tokens": [286, 603, 20870, 309, 294, 257, 2445, 13], "temperature": 0.0, "avg_logprob": -0.12379529259421608, "compression_ratio": 1.7615384615384615, "no_speech_prob": 2.586652772151865e-05}, {"id": 44, "seek": 19348, "start": 216.28, "end": 218.44, "text": " So then I go back and create the function.", "tokens": [407, 550, 286, 352, 646, 293, 1884, 264, 2445, 13], "temperature": 0.0, "avg_logprob": -0.12379529259421608, "compression_ratio": 1.7615384615384615, "no_speech_prob": 2.586652772151865e-05}, {"id": 45, "seek": 21844, "start": 218.44, "end": 226.35999999999999, "text": " So here I've got the mean and standard deviation of my L1, which is a conv2d layer.", "tokens": [407, 510, 286, 600, 658, 264, 914, 293, 3832, 25163, 295, 452, 441, 16, 11, 597, 307, 257, 3754, 17, 67, 4583, 13], "temperature": 0.0, "avg_logprob": -0.10286179455843839, "compression_ratio": 1.469945355191257, "no_speech_prob": 6.339144874800695e-06}, {"id": 46, "seek": 21844, "start": 226.35999999999999, "end": 233.35999999999999, "text": " And so a conv2d layer contains a weight tensor parameter and a bias tensor parameter.", "tokens": [400, 370, 257, 3754, 17, 67, 4583, 8306, 257, 3364, 40863, 13075, 293, 257, 12577, 40863, 13075, 13], "temperature": 0.0, "avg_logprob": -0.10286179455843839, "compression_ratio": 1.469945355191257, "no_speech_prob": 6.339144874800695e-06}, {"id": 47, "seek": 21844, "start": 233.35999999999999, "end": 245.66, "text": " So just to remind you, L1.weight.shape is 32 output filters, because that's what number", "tokens": [407, 445, 281, 4160, 291, 11, 441, 16, 13, 12329, 13, 82, 42406, 307, 8858, 5598, 15995, 11, 570, 300, 311, 437, 1230], "temperature": 0.0, "avg_logprob": -0.10286179455843839, "compression_ratio": 1.469945355191257, "no_speech_prob": 6.339144874800695e-06}, {"id": 48, "seek": 21844, "start": 245.66, "end": 246.66, "text": " hidden was.", "tokens": [7633, 390, 13], "temperature": 0.0, "avg_logprob": -0.10286179455843839, "compression_ratio": 1.469945355191257, "no_speech_prob": 6.339144874800695e-06}, {"id": 49, "seek": 24666, "start": 246.66, "end": 250.96, "text": " And then we have an input filter, because we only have one channel, and then 5 by 5.", "tokens": [400, 550, 321, 362, 364, 4846, 6608, 11, 570, 321, 787, 362, 472, 2269, 11, 293, 550, 1025, 538, 1025, 13], "temperature": 0.0, "avg_logprob": -0.19910829278487194, "compression_ratio": 1.5698924731182795, "no_speech_prob": 6.438881428039167e-06}, {"id": 50, "seek": 24666, "start": 250.96, "end": 253.74, "text": " So that's the size of our tensor.", "tokens": [407, 300, 311, 264, 2744, 295, 527, 40863, 13], "temperature": 0.0, "avg_logprob": -0.19910829278487194, "compression_ratio": 1.5698924731182795, "no_speech_prob": 6.438881428039167e-06}, {"id": 51, "seek": 24666, "start": 253.74, "end": 260.76, "text": " And if you've forgotten why that's the size of a tensor, you can go back to the Excel", "tokens": [400, 498, 291, 600, 11832, 983, 300, 311, 264, 2744, 295, 257, 40863, 11, 291, 393, 352, 646, 281, 264, 19060], "temperature": 0.0, "avg_logprob": -0.19910829278487194, "compression_ratio": 1.5698924731182795, "no_speech_prob": 6.438881428039167e-06}, {"id": 52, "seek": 24666, "start": 260.76, "end": 271.15999999999997, "text": " directory, for example, from part one, where you can find the conv example spreadsheet.", "tokens": [21120, 11, 337, 1365, 11, 490, 644, 472, 11, 689, 291, 393, 915, 264, 3754, 1365, 27733, 13], "temperature": 0.0, "avg_logprob": -0.19910829278487194, "compression_ratio": 1.5698924731182795, "no_speech_prob": 6.438881428039167e-06}, {"id": 53, "seek": 27116, "start": 271.16, "end": 279.36, "text": " And in the conv example spreadsheet, you can see what each of those parameters does.", "tokens": [400, 294, 264, 3754, 1365, 27733, 11, 291, 393, 536, 437, 1184, 295, 729, 9834, 775, 13], "temperature": 0.0, "avg_logprob": -0.1873236592610677, "compression_ratio": 1.5396825396825398, "no_speech_prob": 1.6027514675442944e-06}, {"id": 54, "seek": 27116, "start": 279.36, "end": 288.16, "text": " So we basically had a filter for each input channel and for each output channel.", "tokens": [407, 321, 1936, 632, 257, 6608, 337, 1184, 4846, 2269, 293, 337, 1184, 5598, 2269, 13], "temperature": 0.0, "avg_logprob": -0.1873236592610677, "compression_ratio": 1.5396825396825398, "no_speech_prob": 1.6027514675442944e-06}, {"id": 55, "seek": 27116, "start": 288.16, "end": 289.28000000000003, "text": " So that's kind of what it looked like.", "tokens": [407, 300, 311, 733, 295, 437, 309, 2956, 411, 13], "temperature": 0.0, "avg_logprob": -0.1873236592610677, "compression_ratio": 1.5396825396825398, "no_speech_prob": 1.6027514675442944e-06}, {"id": 56, "seek": 27116, "start": 289.28000000000003, "end": 300.6, "text": " And so, for the next layer, we now have a four-dimensional tensor, a rank four tensor.", "tokens": [400, 370, 11, 337, 264, 958, 4583, 11, 321, 586, 362, 257, 1451, 12, 18759, 40863, 11, 257, 6181, 1451, 40863, 13], "temperature": 0.0, "avg_logprob": -0.1873236592610677, "compression_ratio": 1.5396825396825398, "no_speech_prob": 1.6027514675442944e-06}, {"id": 57, "seek": 30060, "start": 300.6, "end": 302.76000000000005, "text": " We've got the 3 by 3.", "tokens": [492, 600, 658, 264, 805, 538, 805, 13], "temperature": 0.0, "avg_logprob": -0.1409629640125093, "compression_ratio": 1.4427480916030535, "no_speech_prob": 9.818135367822833e-06}, {"id": 58, "seek": 30060, "start": 302.76000000000005, "end": 309.72, "text": " We've got it for each input and for each output.", "tokens": [492, 600, 658, 309, 337, 1184, 4846, 293, 337, 1184, 5598, 13], "temperature": 0.0, "avg_logprob": -0.1409629640125093, "compression_ratio": 1.4427480916030535, "no_speech_prob": 9.818135367822833e-06}, {"id": 59, "seek": 30060, "start": 309.72, "end": 314.20000000000005, "text": " So that's the 32 by 1 by 5 by 5.", "tokens": [407, 300, 311, 264, 8858, 538, 502, 538, 1025, 538, 1025, 13], "temperature": 0.0, "avg_logprob": -0.1409629640125093, "compression_ratio": 1.4427480916030535, "no_speech_prob": 9.818135367822833e-06}, {"id": 60, "seek": 30060, "start": 314.20000000000005, "end": 323.20000000000005, "text": " So the mean and standard deviation of the weights, 0 and 0.11, and this is because we", "tokens": [407, 264, 914, 293, 3832, 25163, 295, 264, 17443, 11, 1958, 293, 1958, 13, 5348, 11, 293, 341, 307, 570, 321], "temperature": 0.0, "avg_logprob": -0.1409629640125093, "compression_ratio": 1.4427480916030535, "no_speech_prob": 9.818135367822833e-06}, {"id": 61, "seek": 32320, "start": 323.2, "end": 332.24, "text": " know that behind the scenes, it's called this function to initialize.", "tokens": [458, 300, 2261, 264, 8026, 11, 309, 311, 1219, 341, 2445, 281, 5883, 1125, 13], "temperature": 0.0, "avg_logprob": -0.10681401677878506, "compression_ratio": 1.6598984771573604, "no_speech_prob": 8.800790965324268e-06}, {"id": 62, "seek": 32320, "start": 332.24, "end": 340.24, "text": " So the bias is initialized with a uniform random number between negative of this and", "tokens": [407, 264, 12577, 307, 5883, 1602, 365, 257, 9452, 4974, 1230, 1296, 3671, 295, 341, 293], "temperature": 0.0, "avg_logprob": -0.10681401677878506, "compression_ratio": 1.6598984771573604, "no_speech_prob": 8.800790965324268e-06}, {"id": 63, "seek": 32320, "start": 340.24, "end": 341.71999999999997, "text": " positive of this.", "tokens": [3353, 295, 341, 13], "temperature": 0.0, "avg_logprob": -0.10681401677878506, "compression_ratio": 1.6598984771573604, "no_speech_prob": 8.800790965324268e-06}, {"id": 64, "seek": 32320, "start": 341.71999999999997, "end": 347.68, "text": " And then the weights are initialized with chiming uniform with this odd math dot square", "tokens": [400, 550, 264, 17443, 366, 5883, 1602, 365, 18375, 278, 9452, 365, 341, 7401, 5221, 5893, 3732], "temperature": 0.0, "avg_logprob": -0.10681401677878506, "compression_ratio": 1.6598984771573604, "no_speech_prob": 8.800790965324268e-06}, {"id": 65, "seek": 32320, "start": 347.68, "end": 350.88, "text": " root 5 thing.", "tokens": [5593, 1025, 551, 13], "temperature": 0.0, "avg_logprob": -0.10681401677878506, "compression_ratio": 1.6598984771573604, "no_speech_prob": 8.800790965324268e-06}, {"id": 66, "seek": 32320, "start": 350.88, "end": 351.88, "text": " So that's fine.", "tokens": [407, 300, 311, 2489, 13], "temperature": 0.0, "avg_logprob": -0.10681401677878506, "compression_ratio": 1.6598984771573604, "no_speech_prob": 8.800790965324268e-06}, {"id": 67, "seek": 32320, "start": 351.88, "end": 352.88, "text": " That's not particularly interesting.", "tokens": [663, 311, 406, 4098, 1880, 13], "temperature": 0.0, "avg_logprob": -0.10681401677878506, "compression_ratio": 1.6598984771573604, "no_speech_prob": 8.800790965324268e-06}, {"id": 68, "seek": 35288, "start": 352.88, "end": 358.0, "text": " What's more interesting is to take our input tensor of MNIST numbers and put it through", "tokens": [708, 311, 544, 1880, 307, 281, 747, 527, 4846, 40863, 295, 376, 45, 19756, 3547, 293, 829, 309, 807], "temperature": 0.0, "avg_logprob": -0.14535776156823613, "compression_ratio": 1.5884955752212389, "no_speech_prob": 2.796833541651722e-05}, {"id": 69, "seek": 35288, "start": 358.0, "end": 365.08, "text": " this layer, which we called L1, which remember is a conv2D layer, so layer 1.", "tokens": [341, 4583, 11, 597, 321, 1219, 441, 16, 11, 597, 1604, 307, 257, 3754, 17, 35, 4583, 11, 370, 4583, 502, 13], "temperature": 0.0, "avg_logprob": -0.14535776156823613, "compression_ratio": 1.5884955752212389, "no_speech_prob": 2.796833541651722e-05}, {"id": 70, "seek": 35288, "start": 365.08, "end": 369.48, "text": " And let's create an output T, and let's look at the stats of T.", "tokens": [400, 718, 311, 1884, 364, 5598, 314, 11, 293, 718, 311, 574, 412, 264, 18152, 295, 314, 13], "temperature": 0.0, "avg_logprob": -0.14535776156823613, "compression_ratio": 1.5884955752212389, "no_speech_prob": 2.796833541651722e-05}, {"id": 71, "seek": 35288, "start": 369.48, "end": 372.44, "text": " So this is the stats of the output of this layer.", "tokens": [407, 341, 307, 264, 18152, 295, 264, 5598, 295, 341, 4583, 13], "temperature": 0.0, "avg_logprob": -0.14535776156823613, "compression_ratio": 1.5884955752212389, "no_speech_prob": 2.796833541651722e-05}, {"id": 72, "seek": 35288, "start": 372.44, "end": 378.88, "text": " We would like it to be a mean of 0 and a standard deviation or a variance of 1.", "tokens": [492, 576, 411, 309, 281, 312, 257, 914, 295, 1958, 293, 257, 3832, 25163, 420, 257, 21977, 295, 502, 13], "temperature": 0.0, "avg_logprob": -0.14535776156823613, "compression_ratio": 1.5884955752212389, "no_speech_prob": 2.796833541651722e-05}, {"id": 73, "seek": 37888, "start": 378.88, "end": 384.48, "text": " The mean of 0 is there, but the standard deviation of 1 is not there.", "tokens": [440, 914, 295, 1958, 307, 456, 11, 457, 264, 3832, 25163, 295, 502, 307, 406, 456, 13], "temperature": 0.0, "avg_logprob": -0.10762015581130982, "compression_ratio": 1.5375722543352601, "no_speech_prob": 1.5779488649059203e-06}, {"id": 74, "seek": 37888, "start": 384.48, "end": 387.2, "text": " So that looks like a problem.", "tokens": [407, 300, 1542, 411, 257, 1154, 13], "temperature": 0.0, "avg_logprob": -0.10762015581130982, "compression_ratio": 1.5375722543352601, "no_speech_prob": 1.5779488649059203e-06}, {"id": 75, "seek": 37888, "start": 387.2, "end": 391.8, "text": " Let's compare this to the normal chiming in it.", "tokens": [961, 311, 6794, 341, 281, 264, 2710, 18375, 278, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.10762015581130982, "compression_ratio": 1.5375722543352601, "no_speech_prob": 1.5779488649059203e-06}, {"id": 76, "seek": 37888, "start": 391.8, "end": 399.28, "text": " So the normal chiming in it, remember, is designed to be used after a ReLU layer or", "tokens": [407, 264, 2710, 18375, 278, 294, 309, 11, 1604, 11, 307, 4761, 281, 312, 1143, 934, 257, 1300, 43, 52, 4583, 420], "temperature": 0.0, "avg_logprob": -0.10762015581130982, "compression_ratio": 1.5375722543352601, "no_speech_prob": 1.5779488649059203e-06}, {"id": 77, "seek": 37888, "start": 399.28, "end": 405.96, "text": " more generally a leaky ReLU layer.", "tokens": [544, 5101, 257, 476, 15681, 1300, 43, 52, 4583, 13], "temperature": 0.0, "avg_logprob": -0.10762015581130982, "compression_ratio": 1.5375722543352601, "no_speech_prob": 1.5779488649059203e-06}, {"id": 78, "seek": 40596, "start": 405.96, "end": 418.2, "text": " And recall that a leaky ReLU layer has the y equals x here, and here the gradient of", "tokens": [400, 9901, 300, 257, 476, 15681, 1300, 43, 52, 4583, 575, 264, 288, 6915, 2031, 510, 11, 293, 510, 264, 16235, 295], "temperature": 0.0, "avg_logprob": -0.12446441369898179, "compression_ratio": 1.4223602484472049, "no_speech_prob": 6.747934548911871e-06}, {"id": 79, "seek": 40596, "start": 418.2, "end": 425.52, "text": " this is called A or leak or whatever.", "tokens": [341, 307, 1219, 316, 420, 17143, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.12446441369898179, "compression_ratio": 1.4223602484472049, "no_speech_prob": 6.747934548911871e-06}, {"id": 80, "seek": 40596, "start": 425.52, "end": 432.96, "text": " And now in our case, we're just looking at a conv layer, so we don't have anything kind", "tokens": [400, 586, 294, 527, 1389, 11, 321, 434, 445, 1237, 412, 257, 3754, 4583, 11, 370, 321, 500, 380, 362, 1340, 733], "temperature": 0.0, "avg_logprob": -0.12446441369898179, "compression_ratio": 1.4223602484472049, "no_speech_prob": 6.747934548911871e-06}, {"id": 81, "seek": 40596, "start": 432.96, "end": 433.96, "text": " of going on there.", "tokens": [295, 516, 322, 456, 13], "temperature": 0.0, "avg_logprob": -0.12446441369898179, "compression_ratio": 1.4223602484472049, "no_speech_prob": 6.747934548911871e-06}, {"id": 82, "seek": 43396, "start": 433.96, "end": 436.84, "text": " In fact, it's straight here as well.", "tokens": [682, 1186, 11, 309, 311, 2997, 510, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.10769137266640351, "compression_ratio": 1.5320197044334976, "no_speech_prob": 1.5206548596324865e-05}, {"id": 83, "seek": 43396, "start": 436.84, "end": 442.08, "text": " So effectively, we have like a leak, if you like, of 1 or an A of 1.", "tokens": [407, 8659, 11, 321, 362, 411, 257, 17143, 11, 498, 291, 411, 11, 295, 502, 420, 364, 316, 295, 502, 13], "temperature": 0.0, "avg_logprob": -0.10769137266640351, "compression_ratio": 1.5320197044334976, "no_speech_prob": 1.5206548596324865e-05}, {"id": 84, "seek": 43396, "start": 442.08, "end": 447.21999999999997, "text": " So to use chiming in it with no ReLU, we can just put A equals 1.", "tokens": [407, 281, 764, 18375, 278, 294, 309, 365, 572, 1300, 43, 52, 11, 321, 393, 445, 829, 316, 6915, 502, 13], "temperature": 0.0, "avg_logprob": -0.10769137266640351, "compression_ratio": 1.5320197044334976, "no_speech_prob": 1.5206548596324865e-05}, {"id": 85, "seek": 43396, "start": 447.21999999999997, "end": 452.15999999999997, "text": " And if we do that, then we get a mean of 0 and a variance of 1.", "tokens": [400, 498, 321, 360, 300, 11, 550, 321, 483, 257, 914, 295, 1958, 293, 257, 21977, 295, 502, 13], "temperature": 0.0, "avg_logprob": -0.10769137266640351, "compression_ratio": 1.5320197044334976, "no_speech_prob": 1.5206548596324865e-05}, {"id": 86, "seek": 43396, "start": 452.15999999999997, "end": 459.15999999999997, "text": " So chiming in it seems to be working nicely.", "tokens": [407, 18375, 278, 294, 309, 2544, 281, 312, 1364, 9594, 13], "temperature": 0.0, "avg_logprob": -0.10769137266640351, "compression_ratio": 1.5320197044334976, "no_speech_prob": 1.5206548596324865e-05}, {"id": 87, "seek": 43396, "start": 459.15999999999997, "end": 461.35999999999996, "text": " So let's now try it with ReLU.", "tokens": [407, 718, 311, 586, 853, 309, 365, 1300, 43, 52, 13], "temperature": 0.0, "avg_logprob": -0.10769137266640351, "compression_ratio": 1.5320197044334976, "no_speech_prob": 1.5206548596324865e-05}, {"id": 88, "seek": 46136, "start": 461.36, "end": 465.76, "text": " So let's now define a function, which is the function for layer 1, which is to pass it", "tokens": [407, 718, 311, 586, 6964, 257, 2445, 11, 597, 307, 264, 2445, 337, 4583, 502, 11, 597, 307, 281, 1320, 309], "temperature": 0.0, "avg_logprob": -0.13326312125997342, "compression_ratio": 1.575609756097561, "no_speech_prob": 1.7329997490378446e-06}, {"id": 89, "seek": 46136, "start": 465.76, "end": 472.28000000000003, "text": " through our layer 1 conv and then do a ReLU with some A, with some leak amount, which", "tokens": [807, 527, 4583, 502, 3754, 293, 550, 360, 257, 1300, 43, 52, 365, 512, 316, 11, 365, 512, 17143, 2372, 11, 597], "temperature": 0.0, "avg_logprob": -0.13326312125997342, "compression_ratio": 1.575609756097561, "no_speech_prob": 1.7329997490378446e-06}, {"id": 90, "seek": 46136, "start": 472.28000000000003, "end": 473.8, "text": " we'll set to 0 by default.", "tokens": [321, 603, 992, 281, 1958, 538, 7576, 13], "temperature": 0.0, "avg_logprob": -0.13326312125997342, "compression_ratio": 1.575609756097561, "no_speech_prob": 1.7329997490378446e-06}, {"id": 91, "seek": 46136, "start": 473.8, "end": 478.48, "text": " So this will be just a regular ReLU.", "tokens": [407, 341, 486, 312, 445, 257, 3890, 1300, 43, 52, 13], "temperature": 0.0, "avg_logprob": -0.13326312125997342, "compression_ratio": 1.575609756097561, "no_speech_prob": 1.7329997490378446e-06}, {"id": 92, "seek": 46136, "start": 478.48, "end": 489.6, "text": " And you can see that if we now run that with chiming initialization, we get a variance", "tokens": [400, 291, 393, 536, 300, 498, 321, 586, 1190, 300, 365, 18375, 278, 5883, 2144, 11, 321, 483, 257, 21977], "temperature": 0.0, "avg_logprob": -0.13326312125997342, "compression_ratio": 1.575609756097561, "no_speech_prob": 1.7329997490378446e-06}, {"id": 93, "seek": 48960, "start": 489.6, "end": 491.84000000000003, "text": " of 1, which is good.", "tokens": [295, 502, 11, 597, 307, 665, 13], "temperature": 0.0, "avg_logprob": -0.12489651489257812, "compression_ratio": 1.5059760956175299, "no_speech_prob": 2.482424406480277e-06}, {"id": 94, "seek": 48960, "start": 491.84000000000003, "end": 494.40000000000003, "text": " And the mean is no longer 0, as we discussed last week.", "tokens": [400, 264, 914, 307, 572, 2854, 1958, 11, 382, 321, 7152, 1036, 1243, 13], "temperature": 0.0, "avg_logprob": -0.12489651489257812, "compression_ratio": 1.5059760956175299, "no_speech_prob": 2.482424406480277e-06}, {"id": 95, "seek": 48960, "start": 494.40000000000003, "end": 497.68, "text": " It's about a half.", "tokens": [467, 311, 466, 257, 1922, 13], "temperature": 0.0, "avg_logprob": -0.12489651489257812, "compression_ratio": 1.5059760956175299, "no_speech_prob": 2.482424406480277e-06}, {"id": 96, "seek": 48960, "start": 497.68, "end": 504.12, "text": " But if we go back and reinitialize the conv2d with this default PyTorch, this is not looking", "tokens": [583, 498, 321, 352, 646, 293, 6561, 270, 831, 1125, 264, 3754, 17, 67, 365, 341, 7576, 9953, 51, 284, 339, 11, 341, 307, 406, 1237], "temperature": 0.0, "avg_logprob": -0.12489651489257812, "compression_ratio": 1.5059760956175299, "no_speech_prob": 2.482424406480277e-06}, {"id": 97, "seek": 48960, "start": 504.12, "end": 505.12, "text": " good at all.", "tokens": [665, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.12489651489257812, "compression_ratio": 1.5059760956175299, "no_speech_prob": 2.482424406480277e-06}, {"id": 98, "seek": 48960, "start": 505.12, "end": 506.28000000000003, "text": " With ReLU, it's even worse.", "tokens": [2022, 1300, 43, 52, 11, 309, 311, 754, 5324, 13], "temperature": 0.0, "avg_logprob": -0.12489651489257812, "compression_ratio": 1.5059760956175299, "no_speech_prob": 2.482424406480277e-06}, {"id": 99, "seek": 48960, "start": 506.28000000000003, "end": 513.52, "text": " Because remember, they don't have anything kind of handling that ReLU case in the default", "tokens": [1436, 1604, 11, 436, 500, 380, 362, 1340, 733, 295, 13175, 300, 1300, 43, 52, 1389, 294, 264, 7576], "temperature": 0.0, "avg_logprob": -0.12489651489257812, "compression_ratio": 1.5059760956175299, "no_speech_prob": 2.482424406480277e-06}, {"id": 100, "seek": 48960, "start": 513.52, "end": 514.52, "text": " conv.", "tokens": [3754, 13], "temperature": 0.0, "avg_logprob": -0.12489651489257812, "compression_ratio": 1.5059760956175299, "no_speech_prob": 2.482424406480277e-06}, {"id": 101, "seek": 48960, "start": 514.52, "end": 516.1600000000001, "text": " So this looks like a problem.", "tokens": [407, 341, 1542, 411, 257, 1154, 13], "temperature": 0.0, "avg_logprob": -0.12489651489257812, "compression_ratio": 1.5059760956175299, "no_speech_prob": 2.482424406480277e-06}, {"id": 102, "seek": 48960, "start": 516.1600000000001, "end": 519.0400000000001, "text": " So a variance of 0.35.", "tokens": [407, 257, 21977, 295, 1958, 13, 8794, 13], "temperature": 0.0, "avg_logprob": -0.12489651489257812, "compression_ratio": 1.5059760956175299, "no_speech_prob": 2.482424406480277e-06}, {"id": 103, "seek": 51904, "start": 519.04, "end": 526.0799999999999, "text": " It may not sound a lot lower than 1, but let's take a look at what that means.", "tokens": [467, 815, 406, 1626, 257, 688, 3126, 813, 502, 11, 457, 718, 311, 747, 257, 574, 412, 437, 300, 1355, 13], "temperature": 0.0, "avg_logprob": -0.11435940152122862, "compression_ratio": 1.4838709677419355, "no_speech_prob": 7.64646210882347e-06}, {"id": 104, "seek": 51904, "start": 526.0799999999999, "end": 527.4, "text": " So I forgot to mention where we are.", "tokens": [407, 286, 5298, 281, 2152, 689, 321, 366, 13], "temperature": 0.0, "avg_logprob": -0.11435940152122862, "compression_ratio": 1.4838709677419355, "no_speech_prob": 7.64646210882347e-06}, {"id": 105, "seek": 51904, "start": 527.4, "end": 532.1999999999999, "text": " This is the 0 to a y square root 5 notebook.", "tokens": [639, 307, 264, 1958, 281, 257, 288, 3732, 5593, 1025, 21060, 13], "temperature": 0.0, "avg_logprob": -0.11435940152122862, "compression_ratio": 1.4838709677419355, "no_speech_prob": 7.64646210882347e-06}, {"id": 106, "seek": 51904, "start": 532.1999999999999, "end": 536.92, "text": " So 0 to a notebook.", "tokens": [407, 1958, 281, 257, 21060, 13], "temperature": 0.0, "avg_logprob": -0.11435940152122862, "compression_ratio": 1.4838709677419355, "no_speech_prob": 7.64646210882347e-06}, {"id": 107, "seek": 51904, "start": 536.92, "end": 542.7199999999999, "text": " So in order to explore this, I decided that I would try and write my own chiming init", "tokens": [407, 294, 1668, 281, 6839, 341, 11, 286, 3047, 300, 286, 576, 853, 293, 2464, 452, 1065, 18375, 278, 3157], "temperature": 0.0, "avg_logprob": -0.11435940152122862, "compression_ratio": 1.4838709677419355, "no_speech_prob": 7.64646210882347e-06}, {"id": 108, "seek": 51904, "start": 542.7199999999999, "end": 545.76, "text": " function.", "tokens": [2445, 13], "temperature": 0.0, "avg_logprob": -0.11435940152122862, "compression_ratio": 1.4838709677419355, "no_speech_prob": 7.64646210882347e-06}, {"id": 109, "seek": 54576, "start": 545.76, "end": 552.64, "text": " And so normally with the chiming init function, if we were working with just a regular fully", "tokens": [400, 370, 5646, 365, 264, 18375, 278, 3157, 2445, 11, 498, 321, 645, 1364, 365, 445, 257, 3890, 4498], "temperature": 0.0, "avg_logprob": -0.09507505098978679, "compression_ratio": 1.5480769230769231, "no_speech_prob": 5.77175478611025e-06}, {"id": 110, "seek": 54576, "start": 552.64, "end": 564.3199999999999, "text": " connected matrix multiplication, we would basically be saying how many output filters", "tokens": [4582, 8141, 27290, 11, 321, 576, 1936, 312, 1566, 577, 867, 5598, 15995], "temperature": 0.0, "avg_logprob": -0.09507505098978679, "compression_ratio": 1.5480769230769231, "no_speech_prob": 5.77175478611025e-06}, {"id": 111, "seek": 54576, "start": 564.3199999999999, "end": 565.3199999999999, "text": " are there.", "tokens": [366, 456, 13], "temperature": 0.0, "avg_logprob": -0.09507505098978679, "compression_ratio": 1.5480769230769231, "no_speech_prob": 5.77175478611025e-06}, {"id": 112, "seek": 54576, "start": 565.3199999999999, "end": 569.68, "text": " So if this is the weight matrix, then what's the width of the weight matrix?", "tokens": [407, 498, 341, 307, 264, 3364, 8141, 11, 550, 437, 311, 264, 11402, 295, 264, 3364, 8141, 30], "temperature": 0.0, "avg_logprob": -0.09507505098978679, "compression_ratio": 1.5480769230769231, "no_speech_prob": 5.77175478611025e-06}, {"id": 113, "seek": 54576, "start": 569.68, "end": 575.6, "text": " For a convolutional layer, it's a little bit different.", "tokens": [1171, 257, 45216, 304, 4583, 11, 309, 311, 257, 707, 857, 819, 13], "temperature": 0.0, "avg_logprob": -0.09507505098978679, "compression_ratio": 1.5480769230769231, "no_speech_prob": 5.77175478611025e-06}, {"id": 114, "seek": 57560, "start": 575.6, "end": 582.48, "text": " Because what we actually want to know is each time, like in this case, we're basically multiplying", "tokens": [1436, 437, 321, 767, 528, 281, 458, 307, 1184, 565, 11, 411, 294, 341, 1389, 11, 321, 434, 1936, 30955], "temperature": 0.0, "avg_logprob": -0.09679992045831243, "compression_ratio": 1.8506224066390042, "no_speech_prob": 9.368418432131875e-06}, {"id": 115, "seek": 57560, "start": 582.48, "end": 587.16, "text": " all these together with some set of inputs and then adding them all up.", "tokens": [439, 613, 1214, 365, 512, 992, 295, 15743, 293, 550, 5127, 552, 439, 493, 13], "temperature": 0.0, "avg_logprob": -0.09679992045831243, "compression_ratio": 1.8506224066390042, "no_speech_prob": 9.368418432131875e-06}, {"id": 116, "seek": 57560, "start": 587.16, "end": 590.9200000000001, "text": " That's basically what a single step of a matrix multiplication is.", "tokens": [663, 311, 1936, 437, 257, 2167, 1823, 295, 257, 8141, 27290, 307, 13], "temperature": 0.0, "avg_logprob": -0.09679992045831243, "compression_ratio": 1.8506224066390042, "no_speech_prob": 9.368418432131875e-06}, {"id": 117, "seek": 57560, "start": 590.9200000000001, "end": 595.8000000000001, "text": " In a convolution, we're also multiplying a bunch of things together and adding them up.", "tokens": [682, 257, 45216, 11, 321, 434, 611, 30955, 257, 3840, 295, 721, 1214, 293, 5127, 552, 493, 13], "temperature": 0.0, "avg_logprob": -0.09679992045831243, "compression_ratio": 1.8506224066390042, "no_speech_prob": 9.368418432131875e-06}, {"id": 118, "seek": 57560, "start": 595.8000000000001, "end": 600.6800000000001, "text": " But what we're actually adding together is, if it was 3 by 3, is we're multiplying together", "tokens": [583, 437, 321, 434, 767, 5127, 1214, 307, 11, 498, 309, 390, 805, 538, 805, 11, 307, 321, 434, 30955, 1214], "temperature": 0.0, "avg_logprob": -0.09679992045831243, "compression_ratio": 1.8506224066390042, "no_speech_prob": 9.368418432131875e-06}, {"id": 119, "seek": 57560, "start": 600.6800000000001, "end": 604.1, "text": " each of the 3 by 3 elements.", "tokens": [1184, 295, 264, 805, 538, 805, 4959, 13], "temperature": 0.0, "avg_logprob": -0.09679992045831243, "compression_ratio": 1.8506224066390042, "no_speech_prob": 9.368418432131875e-06}, {"id": 120, "seek": 60410, "start": 604.1, "end": 607.32, "text": " And also the channel dimension.", "tokens": [400, 611, 264, 2269, 10139, 13], "temperature": 0.0, "avg_logprob": -0.10874109010438661, "compression_ratio": 1.641025641025641, "no_speech_prob": 1.2679125802605995e-06}, {"id": 121, "seek": 60410, "start": 607.32, "end": 610.76, "text": " We actually multiply all of those together and add them all up.", "tokens": [492, 767, 12972, 439, 295, 729, 1214, 293, 909, 552, 439, 493, 13], "temperature": 0.0, "avg_logprob": -0.10874109010438661, "compression_ratio": 1.641025641025641, "no_speech_prob": 1.2679125802605995e-06}, {"id": 122, "seek": 60410, "start": 610.76, "end": 617.8000000000001, "text": " So because convolution and matrix multiplication are kind of one and the same thing, as we", "tokens": [407, 570, 45216, 293, 8141, 27290, 366, 733, 295, 472, 293, 264, 912, 551, 11, 382, 321], "temperature": 0.0, "avg_logprob": -0.10874109010438661, "compression_ratio": 1.641025641025641, "no_speech_prob": 1.2679125802605995e-06}, {"id": 123, "seek": 60410, "start": 617.8000000000001, "end": 621.88, "text": " know, with some weight tying and with some zeros.", "tokens": [458, 11, 365, 512, 3364, 32405, 293, 365, 512, 35193, 13], "temperature": 0.0, "avg_logprob": -0.10874109010438661, "compression_ratio": 1.641025641025641, "no_speech_prob": 1.2679125802605995e-06}, {"id": 124, "seek": 60410, "start": 621.88, "end": 627.48, "text": " So in order to calculate the total number of multiplications and additions going on", "tokens": [407, 294, 1668, 281, 8873, 264, 3217, 1230, 295, 17596, 763, 293, 35113, 516, 322], "temperature": 0.0, "avg_logprob": -0.10874109010438661, "compression_ratio": 1.641025641025641, "no_speech_prob": 1.2679125802605995e-06}, {"id": 125, "seek": 62748, "start": 627.48, "end": 635.88, "text": " for a convolutional layer, we need to basically take the kernel size, which in this case is", "tokens": [337, 257, 45216, 304, 4583, 11, 321, 643, 281, 1936, 747, 264, 28256, 2744, 11, 597, 294, 341, 1389, 307], "temperature": 0.0, "avg_logprob": -0.1019494847247475, "compression_ratio": 1.516304347826087, "no_speech_prob": 1.6027779565774836e-06}, {"id": 126, "seek": 62748, "start": 635.88, "end": 644.1800000000001, "text": " 5 by 5, and multiply it by the number of filters.", "tokens": [1025, 538, 1025, 11, 293, 12972, 309, 538, 264, 1230, 295, 15995, 13], "temperature": 0.0, "avg_logprob": -0.1019494847247475, "compression_ratio": 1.516304347826087, "no_speech_prob": 1.6027779565774836e-06}, {"id": 127, "seek": 62748, "start": 644.1800000000001, "end": 649.78, "text": " So the general way to get that 5 by 5 piece is we can just grab any one piece of this", "tokens": [407, 264, 2674, 636, 281, 483, 300, 1025, 538, 1025, 2522, 307, 321, 393, 445, 4444, 604, 472, 2522, 295, 341], "temperature": 0.0, "avg_logprob": -0.1019494847247475, "compression_ratio": 1.516304347826087, "no_speech_prob": 1.6027779565774836e-06}, {"id": 128, "seek": 62748, "start": 649.78, "end": 654.48, "text": " weight tensor and that will return a 5 by 5 kernel.", "tokens": [3364, 40863, 293, 300, 486, 2736, 257, 1025, 538, 1025, 28256, 13], "temperature": 0.0, "avg_logprob": -0.1019494847247475, "compression_ratio": 1.516304347826087, "no_speech_prob": 1.6027779565774836e-06}, {"id": 129, "seek": 65448, "start": 654.48, "end": 658.5600000000001, "text": " And then say how many elements are in that part of the weight tensor.", "tokens": [400, 550, 584, 577, 867, 4959, 366, 294, 300, 644, 295, 264, 3364, 40863, 13], "temperature": 0.0, "avg_logprob": -0.11795245179342567, "compression_ratio": 1.7201834862385321, "no_speech_prob": 2.2124851966509596e-05}, {"id": 130, "seek": 65448, "start": 658.5600000000001, "end": 661.6800000000001, "text": " And that's going to be the receptive field size.", "tokens": [400, 300, 311, 516, 281, 312, 264, 45838, 2519, 2744, 13], "temperature": 0.0, "avg_logprob": -0.11795245179342567, "compression_ratio": 1.7201834862385321, "no_speech_prob": 2.2124851966509596e-05}, {"id": 131, "seek": 65448, "start": 661.6800000000001, "end": 670.5600000000001, "text": " So the receptive field size for just the immediate layer before is how many elements are in that", "tokens": [407, 264, 45838, 2519, 2744, 337, 445, 264, 11629, 4583, 949, 307, 577, 867, 4959, 366, 294, 300], "temperature": 0.0, "avg_logprob": -0.11795245179342567, "compression_ratio": 1.7201834862385321, "no_speech_prob": 2.2124851966509596e-05}, {"id": 132, "seek": 65448, "start": 670.5600000000001, "end": 671.5600000000001, "text": " kernel.", "tokens": [28256, 13], "temperature": 0.0, "avg_logprob": -0.11795245179342567, "compression_ratio": 1.7201834862385321, "no_speech_prob": 2.2124851966509596e-05}, {"id": 133, "seek": 65448, "start": 671.5600000000001, "end": 673.0, "text": " So for this, it's 25.", "tokens": [407, 337, 341, 11, 309, 311, 3552, 13], "temperature": 0.0, "avg_logprob": -0.11795245179342567, "compression_ratio": 1.7201834862385321, "no_speech_prob": 2.2124851966509596e-05}, {"id": 134, "seek": 65448, "start": 673.0, "end": 674.84, "text": " It's 5 by 5.", "tokens": [467, 311, 1025, 538, 1025, 13], "temperature": 0.0, "avg_logprob": -0.11795245179342567, "compression_ratio": 1.7201834862385321, "no_speech_prob": 2.2124851966509596e-05}, {"id": 135, "seek": 65448, "start": 674.84, "end": 680.08, "text": " And so if we then say, OK, let's grab the shape of the weight matrix.", "tokens": [400, 370, 498, 321, 550, 584, 11, 2264, 11, 718, 311, 4444, 264, 3909, 295, 264, 3364, 8141, 13], "temperature": 0.0, "avg_logprob": -0.11795245179342567, "compression_ratio": 1.7201834862385321, "no_speech_prob": 2.2124851966509596e-05}, {"id": 136, "seek": 65448, "start": 680.08, "end": 683.78, "text": " And it gives us the number of filters out, 32.", "tokens": [400, 309, 2709, 505, 264, 1230, 295, 15995, 484, 11, 8858, 13], "temperature": 0.0, "avg_logprob": -0.11795245179342567, "compression_ratio": 1.7201834862385321, "no_speech_prob": 2.2124851966509596e-05}, {"id": 137, "seek": 68378, "start": 683.78, "end": 686.28, "text": " And then the number of filters in, 1.", "tokens": [400, 550, 264, 1230, 295, 15995, 294, 11, 502, 13], "temperature": 0.0, "avg_logprob": -0.12484952944134353, "compression_ratio": 1.6293103448275863, "no_speech_prob": 4.425452061695978e-06}, {"id": 138, "seek": 68378, "start": 686.28, "end": 690.0, "text": " And then I'll skip the rest because they're the only two things I want.", "tokens": [400, 550, 286, 603, 10023, 264, 1472, 570, 436, 434, 264, 787, 732, 721, 286, 528, 13], "temperature": 0.0, "avg_logprob": -0.12484952944134353, "compression_ratio": 1.6293103448275863, "no_speech_prob": 4.425452061695978e-06}, {"id": 139, "seek": 68378, "start": 690.0, "end": 697.5, "text": " So now for the kaming her in it, we can calculate fan in is the number of input filters times", "tokens": [407, 586, 337, 264, 350, 5184, 720, 294, 309, 11, 321, 393, 8873, 3429, 294, 307, 264, 1230, 295, 4846, 15995, 1413], "temperature": 0.0, "avg_logprob": -0.12484952944134353, "compression_ratio": 1.6293103448275863, "no_speech_prob": 4.425452061695978e-06}, {"id": 140, "seek": 68378, "start": 697.5, "end": 699.12, "text": " the receptive field size.", "tokens": [264, 45838, 2519, 2744, 13], "temperature": 0.0, "avg_logprob": -0.12484952944134353, "compression_ratio": 1.6293103448275863, "no_speech_prob": 4.425452061695978e-06}, {"id": 141, "seek": 68378, "start": 699.12, "end": 701.42, "text": " So that's 1 times 25.", "tokens": [407, 300, 311, 502, 1413, 3552, 13], "temperature": 0.0, "avg_logprob": -0.12484952944134353, "compression_ratio": 1.6293103448275863, "no_speech_prob": 4.425452061695978e-06}, {"id": 142, "seek": 68378, "start": 701.42, "end": 705.52, "text": " Fan out is 32 by 25.", "tokens": [18564, 484, 307, 8858, 538, 3552, 13], "temperature": 0.0, "avg_logprob": -0.12484952944134353, "compression_ratio": 1.6293103448275863, "no_speech_prob": 4.425452061695978e-06}, {"id": 143, "seek": 68378, "start": 705.52, "end": 711.04, "text": " So there you can see this is how we calculate the effective fan in and fan out for a convolutional", "tokens": [407, 456, 291, 393, 536, 341, 307, 577, 321, 8873, 264, 4942, 3429, 294, 293, 3429, 484, 337, 257, 45216, 304], "temperature": 0.0, "avg_logprob": -0.12484952944134353, "compression_ratio": 1.6293103448275863, "no_speech_prob": 4.425452061695978e-06}, {"id": 144, "seek": 68378, "start": 711.04, "end": 712.24, "text": " layer.", "tokens": [4583, 13], "temperature": 0.0, "avg_logprob": -0.12484952944134353, "compression_ratio": 1.6293103448275863, "no_speech_prob": 4.425452061695978e-06}, {"id": 145, "seek": 71224, "start": 712.24, "end": 719.32, "text": " So we can do all that by hand.", "tokens": [407, 321, 393, 360, 439, 300, 538, 1011, 13], "temperature": 0.0, "avg_logprob": -0.10840391531223204, "compression_ratio": 1.5272727272727273, "no_speech_prob": 2.368621153436834e-06}, {"id": 146, "seek": 71224, "start": 719.32, "end": 728.96, "text": " And then the kaming in it formula, you need to then, for leaky value, you need to multiply", "tokens": [400, 550, 264, 350, 5184, 294, 309, 8513, 11, 291, 643, 281, 550, 11, 337, 476, 15681, 2158, 11, 291, 643, 281, 12972], "temperature": 0.0, "avg_logprob": -0.10840391531223204, "compression_ratio": 1.5272727272727273, "no_speech_prob": 2.368621153436834e-06}, {"id": 147, "seek": 71224, "start": 728.96, "end": 732.26, "text": " by root 2.", "tokens": [538, 5593, 568, 13], "temperature": 0.0, "avg_logprob": -0.10840391531223204, "compression_ratio": 1.5272727272727273, "no_speech_prob": 2.368621153436834e-06}, {"id": 148, "seek": 71224, "start": 732.26, "end": 738.6, "text": " Or if there's a leaky part in it, so if the a is not equal to 0, then it's actually root", "tokens": [1610, 498, 456, 311, 257, 476, 15681, 644, 294, 309, 11, 370, 498, 264, 257, 307, 406, 2681, 281, 1958, 11, 550, 309, 311, 767, 5593], "temperature": 0.0, "avg_logprob": -0.10840391531223204, "compression_ratio": 1.5272727272727273, "no_speech_prob": 2.368621153436834e-06}, {"id": 149, "seek": 71224, "start": 738.6, "end": 741.58, "text": " 2 divided by 1 plus a squared.", "tokens": [568, 6666, 538, 502, 1804, 257, 8889, 13], "temperature": 0.0, "avg_logprob": -0.10840391531223204, "compression_ratio": 1.5272727272727273, "no_speech_prob": 2.368621153436834e-06}, {"id": 150, "seek": 74158, "start": 741.58, "end": 745.72, "text": " So that's just the formula for the kaming in it.", "tokens": [407, 300, 311, 445, 264, 8513, 337, 264, 350, 5184, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.08940869018811137, "compression_ratio": 1.7918552036199096, "no_speech_prob": 1.6280359886877704e-06}, {"id": 151, "seek": 74158, "start": 745.72, "end": 750.4000000000001, "text": " And that's often called the gain for the in it.", "tokens": [400, 300, 311, 2049, 1219, 264, 6052, 337, 264, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.08940869018811137, "compression_ratio": 1.7918552036199096, "no_speech_prob": 1.6280359886877704e-06}, {"id": 152, "seek": 74158, "start": 750.4000000000001, "end": 752.46, "text": " And so there's the formula for the gain.", "tokens": [400, 370, 456, 311, 264, 8513, 337, 264, 6052, 13], "temperature": 0.0, "avg_logprob": -0.08940869018811137, "compression_ratio": 1.7918552036199096, "no_speech_prob": 1.6280359886877704e-06}, {"id": 153, "seek": 74158, "start": 752.46, "end": 758.2800000000001, "text": " So you can see that if the gain is 1, then that's just linear.", "tokens": [407, 291, 393, 536, 300, 498, 264, 6052, 307, 502, 11, 550, 300, 311, 445, 8213, 13], "temperature": 0.0, "avg_logprob": -0.08940869018811137, "compression_ratio": 1.7918552036199096, "no_speech_prob": 1.6280359886877704e-06}, {"id": 154, "seek": 74158, "start": 758.2800000000001, "end": 760.26, "text": " There's no nonlinearity at all.", "tokens": [821, 311, 572, 2107, 1889, 17409, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.08940869018811137, "compression_ratio": 1.7918552036199096, "no_speech_prob": 1.6280359886877704e-06}, {"id": 155, "seek": 74158, "start": 760.26, "end": 766.4000000000001, "text": " So there's no change to the calculation of how you do the initialization.", "tokens": [407, 456, 311, 572, 1319, 281, 264, 17108, 295, 577, 291, 360, 264, 5883, 2144, 13], "temperature": 0.0, "avg_logprob": -0.08940869018811137, "compression_ratio": 1.7918552036199096, "no_speech_prob": 1.6280359886877704e-06}, {"id": 156, "seek": 74158, "start": 766.4000000000001, "end": 770.9000000000001, "text": " On the other hand, if it's a standard ReLU, then you've got the root 2, which we saw last", "tokens": [1282, 264, 661, 1011, 11, 498, 309, 311, 257, 3832, 1300, 43, 52, 11, 550, 291, 600, 658, 264, 5593, 568, 11, 597, 321, 1866, 1036], "temperature": 0.0, "avg_logprob": -0.08940869018811137, "compression_ratio": 1.7918552036199096, "no_speech_prob": 1.6280359886877704e-06}, {"id": 157, "seek": 77090, "start": 770.9, "end": 775.48, "text": " week from the kaming paper.", "tokens": [1243, 490, 264, 350, 5184, 3035, 13], "temperature": 0.0, "avg_logprob": -0.12304677168528239, "compression_ratio": 1.5636363636363637, "no_speech_prob": 5.338085429684725e-06}, {"id": 158, "seek": 77090, "start": 775.48, "end": 778.24, "text": " With a gain of 0.01, it's about root 2 as well.", "tokens": [2022, 257, 6052, 295, 1958, 13, 10607, 11, 309, 311, 466, 5593, 568, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.12304677168528239, "compression_ratio": 1.5636363636363637, "no_speech_prob": 5.338085429684725e-06}, {"id": 159, "seek": 77090, "start": 778.24, "end": 779.24, "text": " It's pretty close.", "tokens": [467, 311, 1238, 1998, 13], "temperature": 0.0, "avg_logprob": -0.12304677168528239, "compression_ratio": 1.5636363636363637, "no_speech_prob": 5.338085429684725e-06}, {"id": 160, "seek": 77090, "start": 779.24, "end": 781.92, "text": " And this is kind of a common leaky ReLU amount.", "tokens": [400, 341, 307, 733, 295, 257, 2689, 476, 15681, 1300, 43, 52, 2372, 13], "temperature": 0.0, "avg_logprob": -0.12304677168528239, "compression_ratio": 1.5636363636363637, "no_speech_prob": 5.338085429684725e-06}, {"id": 161, "seek": 77090, "start": 781.92, "end": 784.0799999999999, "text": " But what about in the case of the PyTorch in it?", "tokens": [583, 437, 466, 294, 264, 1389, 295, 264, 9953, 51, 284, 339, 294, 309, 30], "temperature": 0.0, "avg_logprob": -0.12304677168528239, "compression_ratio": 1.5636363636363637, "no_speech_prob": 5.338085429684725e-06}, {"id": 162, "seek": 77090, "start": 784.0799999999999, "end": 791.88, "text": " In the case of the PyTorch in it, it's root 5, which is 0.577, which sounds like an odd", "tokens": [682, 264, 1389, 295, 264, 9953, 51, 284, 339, 294, 309, 11, 309, 311, 5593, 1025, 11, 597, 307, 1958, 13, 20, 17512, 11, 597, 3263, 411, 364, 7401], "temperature": 0.0, "avg_logprob": -0.12304677168528239, "compression_ratio": 1.5636363636363637, "no_speech_prob": 5.338085429684725e-06}, {"id": 163, "seek": 77090, "start": 791.88, "end": 793.36, "text": " number.", "tokens": [1230, 13], "temperature": 0.0, "avg_logprob": -0.12304677168528239, "compression_ratio": 1.5636363636363637, "no_speech_prob": 5.338085429684725e-06}, {"id": 164, "seek": 77090, "start": 793.36, "end": 796.88, "text": " It's a long way away from what we were expecting to see.", "tokens": [467, 311, 257, 938, 636, 1314, 490, 437, 321, 645, 9650, 281, 536, 13], "temperature": 0.0, "avg_logprob": -0.12304677168528239, "compression_ratio": 1.5636363636363637, "no_speech_prob": 5.338085429684725e-06}, {"id": 165, "seek": 79688, "start": 796.88, "end": 803.6, "text": " So that's a bit concerning.", "tokens": [407, 300, 311, 257, 857, 18087, 13], "temperature": 0.0, "avg_logprob": -0.11610103120990828, "compression_ratio": 1.32, "no_speech_prob": 6.375503289746121e-07}, {"id": 166, "seek": 79688, "start": 803.6, "end": 812.08, "text": " But one thing we have to account for here is that the initialization that they use for", "tokens": [583, 472, 551, 321, 362, 281, 2696, 337, 510, 307, 300, 264, 5883, 2144, 300, 436, 764, 337], "temperature": 0.0, "avg_logprob": -0.11610103120990828, "compression_ratio": 1.32, "no_speech_prob": 6.375503289746121e-07}, {"id": 167, "seek": 79688, "start": 812.08, "end": 816.36, "text": " PyTorch is not kaming normal.", "tokens": [9953, 51, 284, 339, 307, 406, 350, 5184, 2710, 13], "temperature": 0.0, "avg_logprob": -0.11610103120990828, "compression_ratio": 1.32, "no_speech_prob": 6.375503289746121e-07}, {"id": 168, "seek": 79688, "start": 816.36, "end": 820.04, "text": " It's kaming uniform.", "tokens": [467, 311, 350, 5184, 9452, 13], "temperature": 0.0, "avg_logprob": -0.11610103120990828, "compression_ratio": 1.32, "no_speech_prob": 6.375503289746121e-07}, {"id": 169, "seek": 82004, "start": 820.04, "end": 832.0799999999999, "text": " And so normally distributed random numbers look like that.", "tokens": [400, 370, 5646, 12631, 4974, 3547, 574, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.13924028759910947, "compression_ratio": 1.709090909090909, "no_speech_prob": 1.9947065084124915e-06}, {"id": 170, "seek": 82004, "start": 832.0799999999999, "end": 840.3199999999999, "text": " But uniform random numbers look like that.", "tokens": [583, 9452, 4974, 3547, 574, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.13924028759910947, "compression_ratio": 1.709090909090909, "no_speech_prob": 1.9947065084124915e-06}, {"id": 171, "seek": 82004, "start": 840.3199999999999, "end": 846.0, "text": " And so the uniform random numbers they were using as their kind of starting point were", "tokens": [400, 370, 264, 9452, 4974, 3547, 436, 645, 1228, 382, 641, 733, 295, 2891, 935, 645], "temperature": 0.0, "avg_logprob": -0.13924028759910947, "compression_ratio": 1.709090909090909, "no_speech_prob": 1.9947065084124915e-06}, {"id": 172, "seek": 84600, "start": 846.0, "end": 850.78, "text": " between minus 1 and 1.", "tokens": [1296, 3175, 502, 293, 502, 13], "temperature": 0.0, "avg_logprob": -0.11550081968307495, "compression_ratio": 1.7688172043010753, "no_speech_prob": 4.4253620217205025e-06}, {"id": 173, "seek": 84600, "start": 850.78, "end": 855.12, "text": " And so the standard deviation of that obviously is not 1.", "tokens": [400, 370, 264, 3832, 25163, 295, 300, 2745, 307, 406, 502, 13], "temperature": 0.0, "avg_logprob": -0.11550081968307495, "compression_ratio": 1.7688172043010753, "no_speech_prob": 4.4253620217205025e-06}, {"id": 174, "seek": 84600, "start": 855.12, "end": 858.76, "text": " The standard deviation is obviously less than 1.", "tokens": [440, 3832, 25163, 307, 2745, 1570, 813, 502, 13], "temperature": 0.0, "avg_logprob": -0.11550081968307495, "compression_ratio": 1.7688172043010753, "no_speech_prob": 4.4253620217205025e-06}, {"id": 175, "seek": 84600, "start": 858.76, "end": 864.22, "text": " And so you can Google for the standard deviation of a uniform distribution.", "tokens": [400, 370, 291, 393, 3329, 337, 264, 3832, 25163, 295, 257, 9452, 7316, 13], "temperature": 0.0, "avg_logprob": -0.11550081968307495, "compression_ratio": 1.7688172043010753, "no_speech_prob": 4.4253620217205025e-06}, {"id": 176, "seek": 84600, "start": 864.22, "end": 869.2, "text": " Or you could jump into Excel or Python and just grab a bunch of random numbers and find", "tokens": [1610, 291, 727, 3012, 666, 19060, 420, 15329, 293, 445, 4444, 257, 3840, 295, 4974, 3547, 293, 915], "temperature": 0.0, "avg_logprob": -0.11550081968307495, "compression_ratio": 1.7688172043010753, "no_speech_prob": 4.4253620217205025e-06}, {"id": 177, "seek": 84600, "start": 869.2, "end": 872.06, "text": " out what the standard deviation is.", "tokens": [484, 437, 264, 3832, 25163, 307, 13], "temperature": 0.0, "avg_logprob": -0.11550081968307495, "compression_ratio": 1.7688172043010753, "no_speech_prob": 4.4253620217205025e-06}, {"id": 178, "seek": 87206, "start": 872.06, "end": 879.2399999999999, "text": " And you'll find that you can, I've done it here actually, I've grabbed 10,000 random", "tokens": [400, 291, 603, 915, 300, 291, 393, 11, 286, 600, 1096, 309, 510, 767, 11, 286, 600, 18607, 1266, 11, 1360, 4974], "temperature": 0.0, "avg_logprob": -0.1463687975112706, "compression_ratio": 1.4896907216494846, "no_speech_prob": 1.9637911918835016e-06}, {"id": 179, "seek": 87206, "start": 879.2399999999999, "end": 883.4399999999999, "text": " numbers in that uniform distribution and asked for their standard deviation.", "tokens": [3547, 294, 300, 9452, 7316, 293, 2351, 337, 641, 3832, 25163, 13], "temperature": 0.0, "avg_logprob": -0.1463687975112706, "compression_ratio": 1.4896907216494846, "no_speech_prob": 1.9637911918835016e-06}, {"id": 180, "seek": 87206, "start": 883.4399999999999, "end": 887.9599999999999, "text": " And it turns out that it's 1 over root 3.", "tokens": [400, 309, 4523, 484, 300, 309, 311, 502, 670, 5593, 805, 13], "temperature": 0.0, "avg_logprob": -0.1463687975112706, "compression_ratio": 1.4896907216494846, "no_speech_prob": 1.9637911918835016e-06}, {"id": 181, "seek": 87206, "start": 887.9599999999999, "end": 897.1999999999999, "text": " So part of the reason for this difference actually is that they need a gain to handle", "tokens": [407, 644, 295, 264, 1778, 337, 341, 2649, 767, 307, 300, 436, 643, 257, 6052, 281, 4813], "temperature": 0.0, "avg_logprob": -0.1463687975112706, "compression_ratio": 1.4896907216494846, "no_speech_prob": 1.9637911918835016e-06}, {"id": 182, "seek": 89720, "start": 897.2, "end": 902.5200000000001, "text": " uniform random numbers rather than just normal random numbers.", "tokens": [9452, 4974, 3547, 2831, 813, 445, 2710, 4974, 3547, 13], "temperature": 0.0, "avg_logprob": -0.08417198160192468, "compression_ratio": 1.640552995391705, "no_speech_prob": 4.637660822481848e-06}, {"id": 183, "seek": 89720, "start": 902.5200000000001, "end": 905.6400000000001, "text": " But it still doesn't quite account for the difference.", "tokens": [583, 309, 920, 1177, 380, 1596, 2696, 337, 264, 2649, 13], "temperature": 0.0, "avg_logprob": -0.08417198160192468, "compression_ratio": 1.640552995391705, "no_speech_prob": 4.637660822481848e-06}, {"id": 184, "seek": 89720, "start": 905.6400000000001, "end": 906.6400000000001, "text": " So let's take a look.", "tokens": [407, 718, 311, 747, 257, 574, 13], "temperature": 0.0, "avg_logprob": -0.08417198160192468, "compression_ratio": 1.640552995391705, "no_speech_prob": 4.637660822481848e-06}, {"id": 185, "seek": 89720, "start": 906.6400000000001, "end": 912.5200000000001, "text": " So here's my version of kaming in which I've just grabbed all of the previous lines of", "tokens": [407, 510, 311, 452, 3037, 295, 350, 5184, 294, 597, 286, 600, 445, 18607, 439, 295, 264, 3894, 3876, 295], "temperature": 0.0, "avg_logprob": -0.08417198160192468, "compression_ratio": 1.640552995391705, "no_speech_prob": 4.637660822481848e-06}, {"id": 186, "seek": 89720, "start": 912.5200000000001, "end": 916.1600000000001, "text": " code and merged them together.", "tokens": [3089, 293, 36427, 552, 1214, 13], "temperature": 0.0, "avg_logprob": -0.08417198160192468, "compression_ratio": 1.640552995391705, "no_speech_prob": 4.637660822481848e-06}, {"id": 187, "seek": 89720, "start": 916.1600000000001, "end": 921.48, "text": " And then I've just added this thing to multiply it by root 3 because of the uniform random", "tokens": [400, 550, 286, 600, 445, 3869, 341, 551, 281, 12972, 309, 538, 5593, 805, 570, 295, 264, 9452, 4974], "temperature": 0.0, "avg_logprob": -0.08417198160192468, "compression_ratio": 1.640552995391705, "no_speech_prob": 4.637660822481848e-06}, {"id": 188, "seek": 89720, "start": 921.48, "end": 922.8000000000001, "text": " number.", "tokens": [1230, 13], "temperature": 0.0, "avg_logprob": -0.08417198160192468, "compression_ratio": 1.640552995391705, "no_speech_prob": 4.637660822481848e-06}, {"id": 189, "seek": 92280, "start": 922.8, "end": 932.0, "text": " And so then if I run this, timing 2 on my weights and get the stats of that, nice.", "tokens": [400, 370, 550, 498, 286, 1190, 341, 11, 10822, 568, 322, 452, 17443, 293, 483, 264, 18152, 295, 300, 11, 1481, 13], "temperature": 0.0, "avg_logprob": -0.15687195821241898, "compression_ratio": 1.505050505050505, "no_speech_prob": 4.4948706090508495e-06}, {"id": 190, "seek": 92280, "start": 932.0, "end": 935.5999999999999, "text": " I again get a variance of about 1.", "tokens": [286, 797, 483, 257, 21977, 295, 466, 502, 13], "temperature": 0.0, "avg_logprob": -0.15687195821241898, "compression_ratio": 1.505050505050505, "no_speech_prob": 4.4948706090508495e-06}, {"id": 191, "seek": 92280, "start": 935.5999999999999, "end": 943.28, "text": " And again confirming that if I, well this is interesting, if I do it with a equals math", "tokens": [400, 797, 42861, 300, 498, 286, 11, 731, 341, 307, 1880, 11, 498, 286, 360, 309, 365, 257, 6915, 5221], "temperature": 0.0, "avg_logprob": -0.15687195821241898, "compression_ratio": 1.505050505050505, "no_speech_prob": 4.4948706090508495e-06}, {"id": 192, "seek": 92280, "start": 943.28, "end": 949.4399999999999, "text": " dot square root 5, I would expect to get the same result as the PyTorch default, which", "tokens": [5893, 3732, 5593, 1025, 11, 286, 576, 2066, 281, 483, 264, 912, 1874, 382, 264, 9953, 51, 284, 339, 7576, 11, 597], "temperature": 0.0, "avg_logprob": -0.15687195821241898, "compression_ratio": 1.505050505050505, "no_speech_prob": 4.4948706090508495e-06}, {"id": 193, "seek": 92280, "start": 949.4399999999999, "end": 950.4399999999999, "text": " I do.", "tokens": [286, 360, 13], "temperature": 0.0, "avg_logprob": -0.15687195821241898, "compression_ratio": 1.505050505050505, "no_speech_prob": 4.4948706090508495e-06}, {"id": 194, "seek": 95044, "start": 950.44, "end": 955.0, "text": " So that's about 0.4, which is what we found back here, 0.35.", "tokens": [407, 300, 311, 466, 1958, 13, 19, 11, 597, 307, 437, 321, 1352, 646, 510, 11, 1958, 13, 8794, 13], "temperature": 0.0, "avg_logprob": -0.199530289904906, "compression_ratio": 1.5133928571428572, "no_speech_prob": 4.7849475777184125e-06}, {"id": 195, "seek": 95044, "start": 955.0, "end": 960.8000000000001, "text": " So it seems like we've successfully, you know, re-implemented what they had.", "tokens": [407, 309, 2544, 411, 321, 600, 10727, 11, 291, 458, 11, 319, 12, 332, 781, 14684, 437, 436, 632, 13], "temperature": 0.0, "avg_logprob": -0.199530289904906, "compression_ratio": 1.5133928571428572, "no_speech_prob": 4.7849475777184125e-06}, {"id": 196, "seek": 95044, "start": 960.8000000000001, "end": 963.5200000000001, "text": " So at this point I was like, okay, well what does this do?", "tokens": [407, 412, 341, 935, 286, 390, 411, 11, 1392, 11, 731, 437, 775, 341, 360, 30], "temperature": 0.0, "avg_logprob": -0.199530289904906, "compression_ratio": 1.5133928571428572, "no_speech_prob": 4.7849475777184125e-06}, {"id": 197, "seek": 95044, "start": 963.5200000000001, "end": 965.48, "text": " What does this mean?", "tokens": [708, 775, 341, 914, 30], "temperature": 0.0, "avg_logprob": -0.199530289904906, "compression_ratio": 1.5133928571428572, "no_speech_prob": 4.7849475777184125e-06}, {"id": 198, "seek": 95044, "start": 965.48, "end": 973.6400000000001, "text": " So to see kind of what this looks like, I threw together a quick confnet and I grabbed", "tokens": [407, 281, 536, 733, 295, 437, 341, 1542, 411, 11, 286, 11918, 1214, 257, 1702, 1497, 7129, 293, 286, 18607], "temperature": 0.0, "avg_logprob": -0.199530289904906, "compression_ratio": 1.5133928571428572, "no_speech_prob": 4.7849475777184125e-06}, {"id": 199, "seek": 95044, "start": 973.6400000000001, "end": 977.36, "text": " the first 100 dependent variables.", "tokens": [264, 700, 2319, 12334, 9102, 13], "temperature": 0.0, "avg_logprob": -0.199530289904906, "compression_ratio": 1.5133928571428572, "no_speech_prob": 4.7849475777184125e-06}, {"id": 200, "seek": 97736, "start": 977.36, "end": 982.72, "text": " And so then I took my input and I ran it through the whole confnet to get the stats out of", "tokens": [400, 370, 550, 286, 1890, 452, 4846, 293, 286, 5872, 309, 807, 264, 1379, 1497, 7129, 281, 483, 264, 18152, 484, 295], "temperature": 0.0, "avg_logprob": -0.0641134713442271, "compression_ratio": 1.6797153024911031, "no_speech_prob": 2.2252704638958676e-06}, {"id": 201, "seek": 97736, "start": 982.72, "end": 983.72, "text": " the results.", "tokens": [264, 3542, 13], "temperature": 0.0, "avg_logprob": -0.0641134713442271, "compression_ratio": 1.6797153024911031, "no_speech_prob": 2.2252704638958676e-06}, {"id": 202, "seek": 97736, "start": 983.72, "end": 988.24, "text": " So this is now telling me what happens when I use the default PyTorch in it and put it", "tokens": [407, 341, 307, 586, 3585, 385, 437, 2314, 562, 286, 764, 264, 7576, 9953, 51, 284, 339, 294, 309, 293, 829, 309], "temperature": 0.0, "avg_logprob": -0.0641134713442271, "compression_ratio": 1.6797153024911031, "no_speech_prob": 2.2252704638958676e-06}, {"id": 203, "seek": 97736, "start": 988.24, "end": 991.94, "text": " through a four-layer confnet.", "tokens": [807, 257, 1451, 12, 8376, 260, 1497, 7129, 13], "temperature": 0.0, "avg_logprob": -0.0641134713442271, "compression_ratio": 1.6797153024911031, "no_speech_prob": 2.2252704638958676e-06}, {"id": 204, "seek": 97736, "start": 991.94, "end": 996.24, "text": " And the answer is I end up with a variance of 0.006.", "tokens": [400, 264, 1867, 307, 286, 917, 493, 365, 257, 21977, 295, 1958, 13, 628, 21, 13], "temperature": 0.0, "avg_logprob": -0.0641134713442271, "compression_ratio": 1.6797153024911031, "no_speech_prob": 2.2252704638958676e-06}, {"id": 205, "seek": 97736, "start": 996.24, "end": 1000.8000000000001, "text": " And that sounds likely to be a really big problem, right, because there's so little", "tokens": [400, 300, 3263, 3700, 281, 312, 257, 534, 955, 1154, 11, 558, 11, 570, 456, 311, 370, 707], "temperature": 0.0, "avg_logprob": -0.0641134713442271, "compression_ratio": 1.6797153024911031, "no_speech_prob": 2.2252704638958676e-06}, {"id": 206, "seek": 97736, "start": 1000.8000000000001, "end": 1003.08, "text": " variation going on that last layer.", "tokens": [12990, 516, 322, 300, 1036, 4583, 13], "temperature": 0.0, "avg_logprob": -0.0641134713442271, "compression_ratio": 1.6797153024911031, "no_speech_prob": 2.2252704638958676e-06}, {"id": 207, "seek": 97736, "start": 1003.08, "end": 1006.7, "text": " And also there's a huge difference between the first layer and the last layer.", "tokens": [400, 611, 456, 311, 257, 2603, 2649, 1296, 264, 700, 4583, 293, 264, 1036, 4583, 13], "temperature": 0.0, "avg_logprob": -0.0641134713442271, "compression_ratio": 1.6797153024911031, "no_speech_prob": 2.2252704638958676e-06}, {"id": 208, "seek": 100670, "start": 1006.7, "end": 1007.7, "text": " That's the really big issue.", "tokens": [663, 311, 264, 534, 955, 2734, 13], "temperature": 0.0, "avg_logprob": -0.14730334281921387, "compression_ratio": 1.8133333333333332, "no_speech_prob": 3.1875367767497664e-06}, {"id": 209, "seek": 100670, "start": 1007.7, "end": 1013.0600000000001, "text": " The first layer had a standard deviation of 0.4 and the last layer had a standard deviation", "tokens": [440, 700, 4583, 632, 257, 3832, 25163, 295, 1958, 13, 19, 293, 264, 1036, 4583, 632, 257, 3832, 25163], "temperature": 0.0, "avg_logprob": -0.14730334281921387, "compression_ratio": 1.8133333333333332, "no_speech_prob": 3.1875367767497664e-06}, {"id": 210, "seek": 100670, "start": 1013.0600000000001, "end": 1016.32, "text": " of, well, the input layer is 1.", "tokens": [295, 11, 731, 11, 264, 4846, 4583, 307, 502, 13], "temperature": 0.0, "avg_logprob": -0.14730334281921387, "compression_ratio": 1.8133333333333332, "no_speech_prob": 3.1875367767497664e-06}, {"id": 211, "seek": 100670, "start": 1016.32, "end": 1020.5200000000001, "text": " The first hidden layer is 0.4 and the last layer is 0.006.", "tokens": [440, 700, 7633, 4583, 307, 1958, 13, 19, 293, 264, 1036, 4583, 307, 1958, 13, 628, 21, 13], "temperature": 0.0, "avg_logprob": -0.14730334281921387, "compression_ratio": 1.8133333333333332, "no_speech_prob": 3.1875367767497664e-06}, {"id": 212, "seek": 100670, "start": 1020.5200000000001, "end": 1024.24, "text": " So these are all going at like totally different rates.", "tokens": [407, 613, 366, 439, 516, 412, 411, 3879, 819, 6846, 13], "temperature": 0.0, "avg_logprob": -0.14730334281921387, "compression_ratio": 1.8133333333333332, "no_speech_prob": 3.1875367767497664e-06}, {"id": 213, "seek": 100670, "start": 1024.24, "end": 1028.64, "text": " And then what we could do is we could grab that prediction and put it through mean squared", "tokens": [400, 550, 437, 321, 727, 360, 307, 321, 727, 4444, 300, 17630, 293, 829, 309, 807, 914, 8889], "temperature": 0.0, "avg_logprob": -0.14730334281921387, "compression_ratio": 1.8133333333333332, "no_speech_prob": 3.1875367767497664e-06}, {"id": 214, "seek": 100670, "start": 1028.64, "end": 1029.64, "text": " error.", "tokens": [6713, 13], "temperature": 0.0, "avg_logprob": -0.14730334281921387, "compression_ratio": 1.8133333333333332, "no_speech_prob": 3.1875367767497664e-06}, {"id": 215, "seek": 100670, "start": 1029.64, "end": 1031.76, "text": " This is the function we created last week.", "tokens": [639, 307, 264, 2445, 321, 2942, 1036, 1243, 13], "temperature": 0.0, "avg_logprob": -0.14730334281921387, "compression_ratio": 1.8133333333333332, "no_speech_prob": 3.1875367767497664e-06}, {"id": 216, "seek": 103176, "start": 1031.76, "end": 1038.2, "text": " Run backward and get the stats on the gradients for the first layer weights.", "tokens": [8950, 23897, 293, 483, 264, 18152, 322, 264, 2771, 2448, 337, 264, 700, 4583, 17443, 13], "temperature": 0.0, "avg_logprob": -0.1272459226785247, "compression_ratio": 1.6712962962962963, "no_speech_prob": 2.857295612557209e-06}, {"id": 217, "seek": 103176, "start": 1038.2, "end": 1041.72, "text": " So this has now gone all the way forward and all the way back again.", "tokens": [407, 341, 575, 586, 2780, 439, 264, 636, 2128, 293, 439, 264, 636, 646, 797, 13], "temperature": 0.0, "avg_logprob": -0.1272459226785247, "compression_ratio": 1.6712962962962963, "no_speech_prob": 2.857295612557209e-06}, {"id": 218, "seek": 103176, "start": 1041.72, "end": 1045.22, "text": " And again, standard deviation is nowhere near 1.", "tokens": [400, 797, 11, 3832, 25163, 307, 11159, 2651, 502, 13], "temperature": 0.0, "avg_logprob": -0.1272459226785247, "compression_ratio": 1.6712962962962963, "no_speech_prob": 2.857295612557209e-06}, {"id": 219, "seek": 103176, "start": 1045.22, "end": 1050.4, "text": " So that sounds like a big problem.", "tokens": [407, 300, 3263, 411, 257, 955, 1154, 13], "temperature": 0.0, "avg_logprob": -0.1272459226785247, "compression_ratio": 1.6712962962962963, "no_speech_prob": 2.857295612557209e-06}, {"id": 220, "seek": 103176, "start": 1050.4, "end": 1055.24, "text": " So let's try using chiming uniform instead.", "tokens": [407, 718, 311, 853, 1228, 18375, 278, 9452, 2602, 13], "temperature": 0.0, "avg_logprob": -0.1272459226785247, "compression_ratio": 1.6712962962962963, "no_speech_prob": 2.857295612557209e-06}, {"id": 221, "seek": 103176, "start": 1055.24, "end": 1060.64, "text": " And if you look at the chiming uniform source code, you'll see that it's got this, it's", "tokens": [400, 498, 291, 574, 412, 264, 18375, 278, 9452, 4009, 3089, 11, 291, 603, 536, 300, 309, 311, 658, 341, 11, 309, 311], "temperature": 0.0, "avg_logprob": -0.1272459226785247, "compression_ratio": 1.6712962962962963, "no_speech_prob": 2.857295612557209e-06}, {"id": 222, "seek": 106064, "start": 1060.64, "end": 1065.64, "text": " got the steps that we saw, gain over root of the fan, and here is the square root of", "tokens": [658, 264, 4439, 300, 321, 1866, 11, 6052, 670, 5593, 295, 264, 3429, 11, 293, 510, 307, 264, 3732, 5593, 295], "temperature": 0.0, "avg_logprob": -0.12325521694716587, "compression_ratio": 1.585, "no_speech_prob": 3.041561058125808e-06}, {"id": 223, "seek": 106064, "start": 1065.64, "end": 1069.0400000000002, "text": " 3 because it's uniform.", "tokens": [805, 570, 309, 311, 9452, 13], "temperature": 0.0, "avg_logprob": -0.12325521694716587, "compression_ratio": 1.585, "no_speech_prob": 3.041561058125808e-06}, {"id": 224, "seek": 106064, "start": 1069.0400000000002, "end": 1070.96, "text": " And so we can confirm.", "tokens": [400, 370, 321, 393, 9064, 13], "temperature": 0.0, "avg_logprob": -0.12325521694716587, "compression_ratio": 1.585, "no_speech_prob": 3.041561058125808e-06}, {"id": 225, "seek": 106064, "start": 1070.96, "end": 1074.0, "text": " Let's go through and go through every layer.", "tokens": [961, 311, 352, 807, 293, 352, 807, 633, 4583, 13], "temperature": 0.0, "avg_logprob": -0.12325521694716587, "compression_ratio": 1.585, "no_speech_prob": 3.041561058125808e-06}, {"id": 226, "seek": 106064, "start": 1074.0, "end": 1079.3600000000001, "text": " And if it's a convolutional layer, then let's call chiming uniform on the weights and set", "tokens": [400, 498, 309, 311, 257, 45216, 304, 4583, 11, 550, 718, 311, 818, 18375, 278, 9452, 322, 264, 17443, 293, 992], "temperature": 0.0, "avg_logprob": -0.12325521694716587, "compression_ratio": 1.585, "no_speech_prob": 3.041561058125808e-06}, {"id": 227, "seek": 106064, "start": 1079.3600000000001, "end": 1081.88, "text": " the biases to 0.", "tokens": [264, 32152, 281, 1958, 13], "temperature": 0.0, "avg_logprob": -0.12325521694716587, "compression_ratio": 1.585, "no_speech_prob": 3.041561058125808e-06}, {"id": 228, "seek": 106064, "start": 1081.88, "end": 1087.72, "text": " So we'll initialize it ourselves.", "tokens": [407, 321, 603, 5883, 1125, 309, 4175, 13], "temperature": 0.0, "avg_logprob": -0.12325521694716587, "compression_ratio": 1.585, "no_speech_prob": 3.041561058125808e-06}, {"id": 229, "seek": 108772, "start": 1087.72, "end": 1097.64, "text": " And then we'll grab T. And it's not 1, but it's a lot better than 0.008.", "tokens": [400, 550, 321, 603, 4444, 314, 13, 400, 309, 311, 406, 502, 11, 457, 309, 311, 257, 688, 1101, 813, 1958, 13, 628, 23, 13], "temperature": 0.0, "avg_logprob": -0.12273841434054905, "compression_ratio": 1.5462555066079295, "no_speech_prob": 4.565877134155016e-06}, {"id": 230, "seek": 108772, "start": 1097.64, "end": 1101.6000000000001, "text": " So this is pretty encouraging that we can get through four layers.", "tokens": [407, 341, 307, 1238, 14580, 300, 321, 393, 483, 807, 1451, 7914, 13], "temperature": 0.0, "avg_logprob": -0.12273841434054905, "compression_ratio": 1.5462555066079295, "no_speech_prob": 4.565877134155016e-06}, {"id": 231, "seek": 108772, "start": 1101.6000000000001, "end": 1106.52, "text": " We wouldn't want to probably have a 40-layer neural network, which is losing this much", "tokens": [492, 2759, 380, 528, 281, 1391, 362, 257, 3356, 12, 8376, 260, 18161, 3209, 11, 597, 307, 7027, 341, 709], "temperature": 0.0, "avg_logprob": -0.12273841434054905, "compression_ratio": 1.5462555066079295, "no_speech_prob": 4.565877134155016e-06}, {"id": 232, "seek": 108772, "start": 1106.52, "end": 1110.48, "text": " variance, but it should be fine, pretty good enough for our four-layer network.", "tokens": [21977, 11, 457, 309, 820, 312, 2489, 11, 1238, 665, 1547, 337, 527, 1451, 12, 8376, 260, 3209, 13], "temperature": 0.0, "avg_logprob": -0.12273841434054905, "compression_ratio": 1.5462555066079295, "no_speech_prob": 4.565877134155016e-06}, {"id": 233, "seek": 108772, "start": 1110.48, "end": 1112.16, "text": " And then let's also confirm on the backward.", "tokens": [400, 550, 718, 311, 611, 9064, 322, 264, 23897, 13], "temperature": 0.0, "avg_logprob": -0.12273841434054905, "compression_ratio": 1.5462555066079295, "no_speech_prob": 4.565877134155016e-06}, {"id": 234, "seek": 111216, "start": 1112.16, "end": 1118.0800000000002, "text": " On the backward, the first layer's gradient is 0.5.", "tokens": [1282, 264, 23897, 11, 264, 700, 4583, 311, 16235, 307, 1958, 13, 20, 13], "temperature": 0.0, "avg_logprob": -0.13470424305308948, "compression_ratio": 1.5388349514563107, "no_speech_prob": 7.224364253488602e-07}, {"id": 235, "seek": 111216, "start": 1118.0800000000002, "end": 1122.64, "text": " So that was my kind of starting point for the research here.", "tokens": [407, 300, 390, 452, 733, 295, 2891, 935, 337, 264, 2132, 510, 13], "temperature": 0.0, "avg_logprob": -0.13470424305308948, "compression_ratio": 1.5388349514563107, "no_speech_prob": 7.224364253488602e-07}, {"id": 236, "seek": 111216, "start": 1122.64, "end": 1129.2, "text": " And at the end of this, I kind of thought this is pretty concerning.", "tokens": [400, 412, 264, 917, 295, 341, 11, 286, 733, 295, 1194, 341, 307, 1238, 18087, 13], "temperature": 0.0, "avg_logprob": -0.13470424305308948, "compression_ratio": 1.5388349514563107, "no_speech_prob": 7.224364253488602e-07}, {"id": 237, "seek": 111216, "start": 1129.2, "end": 1132.52, "text": " And why did I think it was concerning?", "tokens": [400, 983, 630, 286, 519, 309, 390, 18087, 30], "temperature": 0.0, "avg_logprob": -0.13470424305308948, "compression_ratio": 1.5388349514563107, "no_speech_prob": 7.224364253488602e-07}, {"id": 238, "seek": 111216, "start": 1132.52, "end": 1139.0, "text": " We'll be seeing a lot more about why it's concerning, but let's quickly look at 2b initializing.", "tokens": [492, 603, 312, 2577, 257, 688, 544, 466, 983, 309, 311, 18087, 11, 457, 718, 311, 2661, 574, 412, 568, 65, 5883, 3319, 13], "temperature": 0.0, "avg_logprob": -0.13470424305308948, "compression_ratio": 1.5388349514563107, "no_speech_prob": 7.224364253488602e-07}, {"id": 239, "seek": 113900, "start": 1139.0, "end": 1143.76, "text": " So Sylvain put this together today, and he called this why you need a good init.", "tokens": [407, 3902, 14574, 491, 829, 341, 1214, 965, 11, 293, 415, 1219, 341, 983, 291, 643, 257, 665, 3157, 13], "temperature": 0.0, "avg_logprob": -0.18368158340454102, "compression_ratio": 1.4921465968586387, "no_speech_prob": 2.212492108810693e-05}, {"id": 240, "seek": 113900, "start": 1143.76, "end": 1152.92, "text": " And he's pointed out here that if you grab a random vector, X, and a random matrix, A,", "tokens": [400, 415, 311, 10932, 484, 510, 300, 498, 291, 4444, 257, 4974, 8062, 11, 1783, 11, 293, 257, 4974, 8141, 11, 316, 11], "temperature": 0.0, "avg_logprob": -0.18368158340454102, "compression_ratio": 1.4921465968586387, "no_speech_prob": 2.212492108810693e-05}, {"id": 241, "seek": 113900, "start": 1152.92, "end": 1163.62, "text": " which is normally distributed, mean 0 and a standard deviation of 1, then 100 times,", "tokens": [597, 307, 5646, 12631, 11, 914, 1958, 293, 257, 3832, 25163, 295, 502, 11, 550, 2319, 1413, 11], "temperature": 0.0, "avg_logprob": -0.18368158340454102, "compression_ratio": 1.4921465968586387, "no_speech_prob": 2.212492108810693e-05}, {"id": 242, "seek": 113900, "start": 1163.62, "end": 1168.4, "text": " you basically go X is A times X.", "tokens": [291, 1936, 352, 1783, 307, 316, 1413, 1783, 13], "temperature": 0.0, "avg_logprob": -0.18368158340454102, "compression_ratio": 1.4921465968586387, "no_speech_prob": 2.212492108810693e-05}, {"id": 243, "seek": 116840, "start": 1168.4, "end": 1172.76, "text": " And then you go, so you're basically multiplying again and again and again.", "tokens": [400, 550, 291, 352, 11, 370, 291, 434, 1936, 30955, 797, 293, 797, 293, 797, 13], "temperature": 0.0, "avg_logprob": -0.09192037582397461, "compression_ratio": 1.6470588235294117, "no_speech_prob": 1.4284983990364708e-05}, {"id": 244, "seek": 116840, "start": 1172.76, "end": 1179.24, "text": " After 100 iterations, your standard deviation and mean are not a number.", "tokens": [2381, 2319, 36540, 11, 428, 3832, 25163, 293, 914, 366, 406, 257, 1230, 13], "temperature": 0.0, "avg_logprob": -0.09192037582397461, "compression_ratio": 1.6470588235294117, "no_speech_prob": 1.4284983990364708e-05}, {"id": 245, "seek": 116840, "start": 1179.24, "end": 1186.2800000000002, "text": " So basically, the issue is that when you multiply by a matrix lots and lots of times, you explode", "tokens": [407, 1936, 11, 264, 2734, 307, 300, 562, 291, 12972, 538, 257, 8141, 3195, 293, 3195, 295, 1413, 11, 291, 21411], "temperature": 0.0, "avg_logprob": -0.09192037582397461, "compression_ratio": 1.6470588235294117, "no_speech_prob": 1.4284983990364708e-05}, {"id": 246, "seek": 116840, "start": 1186.2800000000002, "end": 1189.4, "text": " out to the point that your computer can't even keep track.", "tokens": [484, 281, 264, 935, 300, 428, 3820, 393, 380, 754, 1066, 2837, 13], "temperature": 0.0, "avg_logprob": -0.09192037582397461, "compression_ratio": 1.6470588235294117, "no_speech_prob": 1.4284983990364708e-05}, {"id": 247, "seek": 116840, "start": 1189.4, "end": 1196.0, "text": " So what Sylvain did next was he actually put something in a loop to check whether it's", "tokens": [407, 437, 3902, 14574, 491, 630, 958, 390, 415, 767, 829, 746, 294, 257, 6367, 281, 1520, 1968, 309, 311], "temperature": 0.0, "avg_logprob": -0.09192037582397461, "compression_ratio": 1.6470588235294117, "no_speech_prob": 1.4284983990364708e-05}, {"id": 248, "seek": 119600, "start": 1196.0, "end": 1204.96, "text": " not a number, and he found it was only 28 iterations before it died.", "tokens": [406, 257, 1230, 11, 293, 415, 1352, 309, 390, 787, 7562, 36540, 949, 309, 4539, 13], "temperature": 0.0, "avg_logprob": -0.13385387369104335, "compression_ratio": 1.4722222222222223, "no_speech_prob": 6.4389719227619935e-06}, {"id": 249, "seek": 119600, "start": 1204.96, "end": 1210.52, "text": " So it didn't take very long to explode.", "tokens": [407, 309, 994, 380, 747, 588, 938, 281, 21411, 13], "temperature": 0.0, "avg_logprob": -0.13385387369104335, "compression_ratio": 1.4722222222222223, "no_speech_prob": 6.4389719227619935e-06}, {"id": 250, "seek": 119600, "start": 1210.52, "end": 1214.56, "text": " Now on the other hand, what if we take the random numbers with a standard deviation of", "tokens": [823, 322, 264, 661, 1011, 11, 437, 498, 321, 747, 264, 4974, 3547, 365, 257, 3832, 25163, 295], "temperature": 0.0, "avg_logprob": -0.13385387369104335, "compression_ratio": 1.4722222222222223, "no_speech_prob": 6.4389719227619935e-06}, {"id": 251, "seek": 119600, "start": 1214.56, "end": 1222.88, "text": " 0.01 instead of 1, and we do that 100 times, then it disappears to 0.", "tokens": [1958, 13, 10607, 2602, 295, 502, 11, 293, 321, 360, 300, 2319, 1413, 11, 550, 309, 25527, 281, 1958, 13], "temperature": 0.0, "avg_logprob": -0.13385387369104335, "compression_ratio": 1.4722222222222223, "no_speech_prob": 6.4389719227619935e-06}, {"id": 252, "seek": 122288, "start": 1222.88, "end": 1227.1200000000001, "text": " So you can see if you've got 100 layer neural net, because that's what it's doing, it's", "tokens": [407, 291, 393, 536, 498, 291, 600, 658, 2319, 4583, 18161, 2533, 11, 570, 300, 311, 437, 309, 311, 884, 11, 309, 311], "temperature": 0.0, "avg_logprob": -0.11409035404171564, "compression_ratio": 1.7148760330578512, "no_speech_prob": 3.6687845295091392e-06}, {"id": 253, "seek": 122288, "start": 1227.1200000000001, "end": 1232.0400000000002, "text": " doing 100 matrix multipliers on itself, on the output of each previous one, you've got", "tokens": [884, 2319, 8141, 12788, 4890, 322, 2564, 11, 322, 264, 5598, 295, 1184, 3894, 472, 11, 291, 600, 658], "temperature": 0.0, "avg_logprob": -0.11409035404171564, "compression_ratio": 1.7148760330578512, "no_speech_prob": 3.6687845295091392e-06}, {"id": 254, "seek": 122288, "start": 1232.0400000000002, "end": 1236.88, "text": " to be super careful to find some set of weights.", "tokens": [281, 312, 1687, 5026, 281, 915, 512, 992, 295, 17443, 13], "temperature": 0.0, "avg_logprob": -0.11409035404171564, "compression_ratio": 1.7148760330578512, "no_speech_prob": 3.6687845295091392e-06}, {"id": 255, "seek": 122288, "start": 1236.88, "end": 1244.8000000000002, "text": " Because if this is your starting set of weights, if it's 0.01 or if it's 1 standard deviation,", "tokens": [1436, 498, 341, 307, 428, 2891, 992, 295, 17443, 11, 498, 309, 311, 1958, 13, 10607, 420, 498, 309, 311, 502, 3832, 25163, 11], "temperature": 0.0, "avg_logprob": -0.11409035404171564, "compression_ratio": 1.7148760330578512, "no_speech_prob": 3.6687845295091392e-06}, {"id": 256, "seek": 122288, "start": 1244.8000000000002, "end": 1248.1200000000001, "text": " you can't ever learn anything because there are no gradients.", "tokens": [291, 393, 380, 1562, 1466, 1340, 570, 456, 366, 572, 2771, 2448, 13], "temperature": 0.0, "avg_logprob": -0.11409035404171564, "compression_ratio": 1.7148760330578512, "no_speech_prob": 3.6687845295091392e-06}, {"id": 257, "seek": 122288, "start": 1248.1200000000001, "end": 1250.88, "text": " The gradients are either 0 or nan.", "tokens": [440, 2771, 2448, 366, 2139, 1958, 420, 14067, 13], "temperature": 0.0, "avg_logprob": -0.11409035404171564, "compression_ratio": 1.7148760330578512, "no_speech_prob": 3.6687845295091392e-06}, {"id": 258, "seek": 125088, "start": 1250.88, "end": 1253.5600000000002, "text": " So you actually have to have a reasonable starting point.", "tokens": [407, 291, 767, 362, 281, 362, 257, 10585, 2891, 935, 13], "temperature": 0.0, "avg_logprob": -0.16214600015193859, "compression_ratio": 1.5708154506437768, "no_speech_prob": 2.090427642542636e-06}, {"id": 259, "seek": 125088, "start": 1253.5600000000002, "end": 1260.92, "text": " This is really why for decades people weren't able to train deep neural networks, because", "tokens": [639, 307, 534, 983, 337, 7878, 561, 4999, 380, 1075, 281, 3847, 2452, 18161, 9590, 11, 570], "temperature": 0.0, "avg_logprob": -0.16214600015193859, "compression_ratio": 1.5708154506437768, "no_speech_prob": 2.090427642542636e-06}, {"id": 260, "seek": 125088, "start": 1260.92, "end": 1266.2600000000002, "text": " people hadn't figured out how to initialize them.", "tokens": [561, 8782, 380, 8932, 484, 577, 281, 5883, 1125, 552, 13], "temperature": 0.0, "avg_logprob": -0.16214600015193859, "compression_ratio": 1.5708154506437768, "no_speech_prob": 2.090427642542636e-06}, {"id": 261, "seek": 125088, "start": 1266.2600000000002, "end": 1272.0800000000002, "text": " So instead we have to use some better in it.", "tokens": [407, 2602, 321, 362, 281, 764, 512, 1101, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.16214600015193859, "compression_ratio": 1.5708154506437768, "no_speech_prob": 2.090427642542636e-06}, {"id": 262, "seek": 125088, "start": 1272.0800000000002, "end": 1274.24, "text": " We'll talk about that in a moment.", "tokens": [492, 603, 751, 466, 300, 294, 257, 1623, 13], "temperature": 0.0, "avg_logprob": -0.16214600015193859, "compression_ratio": 1.5708154506437768, "no_speech_prob": 2.090427642542636e-06}, {"id": 263, "seek": 125088, "start": 1274.24, "end": 1277.68, "text": " For those who are interested, Sylvain's then gone on to describe why it is that you have", "tokens": [1171, 729, 567, 366, 3102, 11, 3902, 14574, 491, 311, 550, 2780, 322, 281, 6786, 983, 309, 307, 300, 291, 362], "temperature": 0.0, "avg_logprob": -0.16214600015193859, "compression_ratio": 1.5708154506437768, "no_speech_prob": 2.090427642542636e-06}, {"id": 264, "seek": 127768, "start": 1277.68, "end": 1281.92, "text": " to divide by the square root of the fan.", "tokens": [281, 9845, 538, 264, 3732, 5593, 295, 264, 3429, 13], "temperature": 0.0, "avg_logprob": -0.1575856794390762, "compression_ratio": 1.53515625, "no_speech_prob": 6.747479801560985e-06}, {"id": 265, "seek": 127768, "start": 1281.92, "end": 1284.8400000000001, "text": " And so feel free to keep reading that if you're interested.", "tokens": [400, 370, 841, 1737, 281, 1066, 3760, 300, 498, 291, 434, 3102, 13], "temperature": 0.0, "avg_logprob": -0.1575856794390762, "compression_ratio": 1.53515625, "no_speech_prob": 6.747479801560985e-06}, {"id": 266, "seek": 127768, "start": 1284.8400000000001, "end": 1286.28, "text": " It's cool.", "tokens": [467, 311, 1627, 13], "temperature": 0.0, "avg_logprob": -0.1575856794390762, "compression_ratio": 1.53515625, "no_speech_prob": 6.747479801560985e-06}, {"id": 267, "seek": 127768, "start": 1286.28, "end": 1287.76, "text": " But we don't need to know it for now.", "tokens": [583, 321, 500, 380, 643, 281, 458, 309, 337, 586, 13], "temperature": 0.0, "avg_logprob": -0.1575856794390762, "compression_ratio": 1.53515625, "no_speech_prob": 6.747479801560985e-06}, {"id": 268, "seek": 127768, "start": 1287.76, "end": 1292.1200000000001, "text": " It's just some derivation and further discussion.", "tokens": [467, 311, 445, 512, 10151, 399, 293, 3052, 5017, 13], "temperature": 0.0, "avg_logprob": -0.1575856794390762, "compression_ratio": 1.53515625, "no_speech_prob": 6.747479801560985e-06}, {"id": 269, "seek": 127768, "start": 1292.1200000000001, "end": 1297.8, "text": " So in parallel I also asked the PyTorch team about this, and I sent these results to them.", "tokens": [407, 294, 8952, 286, 611, 2351, 264, 9953, 51, 284, 339, 1469, 466, 341, 11, 293, 286, 2279, 613, 3542, 281, 552, 13], "temperature": 0.0, "avg_logprob": -0.1575856794390762, "compression_ratio": 1.53515625, "no_speech_prob": 6.747479801560985e-06}, {"id": 270, "seek": 127768, "start": 1297.8, "end": 1299.8200000000002, "text": " And I said, what's going on?", "tokens": [400, 286, 848, 11, 437, 311, 516, 322, 30], "temperature": 0.0, "avg_logprob": -0.1575856794390762, "compression_ratio": 1.53515625, "no_speech_prob": 6.747479801560985e-06}, {"id": 271, "seek": 127768, "start": 1299.8200000000002, "end": 1306.72, "text": " And so Sumath finally appeared, and he said it was a historical accident.", "tokens": [400, 370, 8626, 998, 2721, 8516, 11, 293, 415, 848, 309, 390, 257, 8584, 6398, 13], "temperature": 0.0, "avg_logprob": -0.1575856794390762, "compression_ratio": 1.53515625, "no_speech_prob": 6.747479801560985e-06}, {"id": 272, "seek": 130672, "start": 1306.72, "end": 1311.68, "text": " It was 15 years ago, or for 15 years before PyTorch appeared, there was a product called", "tokens": [467, 390, 2119, 924, 2057, 11, 420, 337, 2119, 924, 949, 9953, 51, 284, 339, 8516, 11, 456, 390, 257, 1674, 1219], "temperature": 0.0, "avg_logprob": -0.23222165541215375, "compression_ratio": 1.5493562231759657, "no_speech_prob": 5.5063533181964885e-06}, {"id": 273, "seek": 130672, "start": 1311.68, "end": 1314.6000000000001, "text": " Torch, a neural network product in Lua.", "tokens": [7160, 339, 11, 257, 18161, 3209, 1674, 294, 441, 4398, 13], "temperature": 0.0, "avg_logprob": -0.23222165541215375, "compression_ratio": 1.5493562231759657, "no_speech_prob": 5.5063533181964885e-06}, {"id": 274, "seek": 130672, "start": 1314.6000000000001, "end": 1318.0, "text": " And they did it that way.", "tokens": [400, 436, 630, 309, 300, 636, 13], "temperature": 0.0, "avg_logprob": -0.23222165541215375, "compression_ratio": 1.5493562231759657, "no_speech_prob": 5.5063533181964885e-06}, {"id": 275, "seek": 130672, "start": 1318.0, "end": 1322.92, "text": " And so then on Google+, in 2014, he started talking to Saunders Dielerman, who's now at", "tokens": [400, 370, 550, 322, 3329, 46797, 294, 8227, 11, 415, 1409, 1417, 281, 6299, 997, 433, 413, 1187, 11821, 11, 567, 311, 586, 412], "temperature": 0.0, "avg_logprob": -0.23222165541215375, "compression_ratio": 1.5493562231759657, "no_speech_prob": 5.5063533181964885e-06}, {"id": 276, "seek": 130672, "start": 1322.92, "end": 1328.6000000000001, "text": " DeepMind and about it this time, maybe a bit before he was our intern, actually.", "tokens": [14895, 44, 471, 293, 466, 309, 341, 565, 11, 1310, 257, 857, 949, 415, 390, 527, 2154, 11, 767, 13], "temperature": 0.0, "avg_logprob": -0.23222165541215375, "compression_ratio": 1.5493562231759657, "no_speech_prob": 5.5063533181964885e-06}, {"id": 277, "seek": 130672, "start": 1328.6000000000001, "end": 1334.6000000000001, "text": " And Saunders said this is at Inletic.", "tokens": [400, 6299, 997, 433, 848, 341, 307, 412, 682, 2631, 299, 13], "temperature": 0.0, "avg_logprob": -0.23222165541215375, "compression_ratio": 1.5493562231759657, "no_speech_prob": 5.5063533181964885e-06}, {"id": 278, "seek": 133460, "start": 1334.6, "end": 1340.6399999999999, "text": " And Saunders said this Route 5 thing looks weird.", "tokens": [400, 6299, 997, 433, 848, 341, 39142, 1025, 551, 1542, 3657, 13], "temperature": 0.0, "avg_logprob": -0.13620432685403264, "compression_ratio": 1.7402597402597402, "no_speech_prob": 8.394325959670823e-06}, {"id": 279, "seek": 133460, "start": 1340.6399999999999, "end": 1342.6799999999998, "text": " And Sumath said, no, no, go look at the paper.", "tokens": [400, 8626, 998, 848, 11, 572, 11, 572, 11, 352, 574, 412, 264, 3035, 13], "temperature": 0.0, "avg_logprob": -0.13620432685403264, "compression_ratio": 1.7402597402597402, "no_speech_prob": 8.394325959670823e-06}, {"id": 280, "seek": 133460, "start": 1342.6799999999998, "end": 1344.52, "text": " And Saunders said, no, that's not what the paper said.", "tokens": [400, 6299, 997, 433, 848, 11, 572, 11, 300, 311, 406, 437, 264, 3035, 848, 13], "temperature": 0.0, "avg_logprob": -0.13620432685403264, "compression_ratio": 1.7402597402597402, "no_speech_prob": 8.394325959670823e-06}, {"id": 281, "seek": 133460, "start": 1344.52, "end": 1347.1999999999998, "text": " And Sumath said, oh, it's a bug.", "tokens": [400, 8626, 998, 848, 11, 1954, 11, 309, 311, 257, 7426, 13], "temperature": 0.0, "avg_logprob": -0.13620432685403264, "compression_ratio": 1.7402597402597402, "no_speech_prob": 8.394325959670823e-06}, {"id": 282, "seek": 133460, "start": 1347.1999999999998, "end": 1352.08, "text": " But it's a good bug, because somebody went and checked it out, and they thought that", "tokens": [583, 309, 311, 257, 665, 7426, 11, 570, 2618, 1437, 293, 10033, 309, 484, 11, 293, 436, 1194, 300], "temperature": 0.0, "avg_logprob": -0.13620432685403264, "compression_ratio": 1.7402597402597402, "no_speech_prob": 8.394325959670823e-06}, {"id": 283, "seek": 133460, "start": 1352.08, "end": 1355.6799999999998, "text": " they were getting better results with this thing.", "tokens": [436, 645, 1242, 1101, 3542, 365, 341, 551, 13], "temperature": 0.0, "avg_logprob": -0.13620432685403264, "compression_ratio": 1.7402597402597402, "no_speech_prob": 8.394325959670823e-06}, {"id": 284, "seek": 133460, "start": 1355.6799999999998, "end": 1362.12, "text": " So then I talked to Sumath, and he was already aware of this issue to some extent.", "tokens": [407, 550, 286, 2825, 281, 8626, 998, 11, 293, 415, 390, 1217, 3650, 295, 341, 2734, 281, 512, 8396, 13], "temperature": 0.0, "avg_logprob": -0.13620432685403264, "compression_ratio": 1.7402597402597402, "no_speech_prob": 8.394325959670823e-06}, {"id": 285, "seek": 136212, "start": 1362.12, "end": 1370.6, "text": " And within a couple of hours, PyTorch team had created an issue saying they're going", "tokens": [400, 1951, 257, 1916, 295, 2496, 11, 9953, 51, 284, 339, 1469, 632, 2942, 364, 2734, 1566, 436, 434, 516], "temperature": 0.0, "avg_logprob": -0.1572530185475069, "compression_ratio": 1.5837837837837838, "no_speech_prob": 1.1658484254439827e-05}, {"id": 286, "seek": 136212, "start": 1370.6, "end": 1372.84, "text": " to update their innets.", "tokens": [281, 5623, 641, 7714, 1385, 13], "temperature": 0.0, "avg_logprob": -0.1572530185475069, "compression_ratio": 1.5837837837837838, "no_speech_prob": 1.1658484254439827e-05}, {"id": 287, "seek": 136212, "start": 1372.84, "end": 1374.36, "text": " So this is super cool.", "tokens": [407, 341, 307, 1687, 1627, 13], "temperature": 0.0, "avg_logprob": -0.1572530185475069, "compression_ratio": 1.5837837837837838, "no_speech_prob": 1.1658484254439827e-05}, {"id": 288, "seek": 136212, "start": 1374.36, "end": 1379.0, "text": " So this is like partly to say, well, this is an awesome team, super responsive.", "tokens": [407, 341, 307, 411, 17031, 281, 584, 11, 731, 11, 341, 307, 364, 3476, 1469, 11, 1687, 21826, 13], "temperature": 0.0, "avg_logprob": -0.1572530185475069, "compression_ratio": 1.5837837837837838, "no_speech_prob": 1.1658484254439827e-05}, {"id": 289, "seek": 136212, "start": 1379.0, "end": 1384.2399999999998, "text": " And this is why PyTorch works so well, is that they see issues and they fix them.", "tokens": [400, 341, 307, 983, 9953, 51, 284, 339, 1985, 370, 731, 11, 307, 300, 436, 536, 2663, 293, 436, 3191, 552, 13], "temperature": 0.0, "avg_logprob": -0.1572530185475069, "compression_ratio": 1.5837837837837838, "no_speech_prob": 1.1658484254439827e-05}, {"id": 290, "seek": 138424, "start": 1384.24, "end": 1392.88, "text": " But it's also to say, when you see something in a library, don't assume it's right or", "tokens": [583, 309, 311, 611, 281, 584, 11, 562, 291, 536, 746, 294, 257, 6405, 11, 500, 380, 6552, 309, 311, 558, 420], "temperature": 0.0, "avg_logprob": -0.10966321555050937, "compression_ratio": 1.5794871794871794, "no_speech_prob": 7.76483921072213e-06}, {"id": 291, "seek": 138424, "start": 1392.88, "end": 1394.88, "text": " that it makes sense.", "tokens": [300, 309, 1669, 2020, 13], "temperature": 0.0, "avg_logprob": -0.10966321555050937, "compression_ratio": 1.5794871794871794, "no_speech_prob": 7.76483921072213e-06}, {"id": 292, "seek": 138424, "start": 1394.88, "end": 1398.0, "text": " When it comes to deep learning, none of us know what we're doing.", "tokens": [1133, 309, 1487, 281, 2452, 2539, 11, 6022, 295, 505, 458, 437, 321, 434, 884, 13], "temperature": 0.0, "avg_logprob": -0.10966321555050937, "compression_ratio": 1.5794871794871794, "no_speech_prob": 7.76483921072213e-06}, {"id": 293, "seek": 138424, "start": 1398.0, "end": 1403.0, "text": " And you can see it doesn't take too much to dig into something.", "tokens": [400, 291, 393, 536, 309, 1177, 380, 747, 886, 709, 281, 2528, 666, 746, 13], "temperature": 0.0, "avg_logprob": -0.10966321555050937, "compression_ratio": 1.5794871794871794, "no_speech_prob": 7.76483921072213e-06}, {"id": 294, "seek": 138424, "start": 1403.0, "end": 1409.84, "text": " And then you can raise an issue and say, here's an analysis that I did.", "tokens": [400, 550, 291, 393, 5300, 364, 2734, 293, 584, 11, 510, 311, 364, 5215, 300, 286, 630, 13], "temperature": 0.0, "avg_logprob": -0.10966321555050937, "compression_ratio": 1.5794871794871794, "no_speech_prob": 7.76483921072213e-06}, {"id": 295, "seek": 140984, "start": 1409.84, "end": 1415.28, "text": " There's a fantastic extension called Gistit, G-I-S-T, Gistit, for Jupyter Notebooks that", "tokens": [821, 311, 257, 5456, 10320, 1219, 460, 468, 270, 11, 460, 12, 40, 12, 50, 12, 51, 11, 460, 468, 270, 11, 337, 22125, 88, 391, 11633, 15170, 300], "temperature": 0.0, "avg_logprob": -0.12511250230132556, "compression_ratio": 1.6679104477611941, "no_speech_prob": 1.6185227650566958e-05}, {"id": 296, "seek": 140984, "start": 1415.28, "end": 1419.36, "text": " lets you take your little research notebook, press a single button, and it turns it into", "tokens": [6653, 291, 747, 428, 707, 2132, 21060, 11, 1886, 257, 2167, 2960, 11, 293, 309, 4523, 309, 666], "temperature": 0.0, "avg_logprob": -0.12511250230132556, "compression_ratio": 1.6679104477611941, "no_speech_prob": 1.6185227650566958e-05}, {"id": 297, "seek": 140984, "start": 1419.36, "end": 1425.4199999999998, "text": " a shareable gist that you can then put a link to say, here's the analysis that I did.", "tokens": [257, 2073, 712, 290, 468, 300, 291, 393, 550, 829, 257, 2113, 281, 584, 11, 510, 311, 264, 5215, 300, 286, 630, 13], "temperature": 0.0, "avg_logprob": -0.12511250230132556, "compression_ratio": 1.6679104477611941, "no_speech_prob": 1.6185227650566958e-05}, {"id": 298, "seek": 140984, "start": 1425.4199999999998, "end": 1430.9599999999998, "text": " And so, yeah, that's a little bit of fun little bit of research I did into answering this", "tokens": [400, 370, 11, 1338, 11, 300, 311, 257, 707, 857, 295, 1019, 707, 857, 295, 2132, 286, 630, 666, 13430, 341], "temperature": 0.0, "avg_logprob": -0.12511250230132556, "compression_ratio": 1.6679104477611941, "no_speech_prob": 1.6185227650566958e-05}, {"id": 299, "seek": 140984, "start": 1430.9599999999998, "end": 1434.6799999999998, "text": " question from last week.", "tokens": [1168, 490, 1036, 1243, 13], "temperature": 0.0, "avg_logprob": -0.12511250230132556, "compression_ratio": 1.6679104477611941, "no_speech_prob": 1.6185227650566958e-05}, {"id": 300, "seek": 140984, "start": 1434.6799999999998, "end": 1437.84, "text": " There are lots of interesting initialization approaches you can use.", "tokens": [821, 366, 3195, 295, 1880, 5883, 2144, 11587, 291, 393, 764, 13], "temperature": 0.0, "avg_logprob": -0.12511250230132556, "compression_ratio": 1.6679104477611941, "no_speech_prob": 1.6185227650566958e-05}, {"id": 301, "seek": 143784, "start": 1437.84, "end": 1441.4399999999998, "text": " We've already talked about the Loro and Benjio paper.", "tokens": [492, 600, 1217, 2825, 466, 264, 441, 10780, 293, 3964, 73, 1004, 3035, 13], "temperature": 0.0, "avg_logprob": -0.18789093167174095, "compression_ratio": 1.6254980079681276, "no_speech_prob": 2.4679256966919638e-05}, {"id": 302, "seek": 143784, "start": 1441.4399999999998, "end": 1445.04, "text": " We've already talked about the Kai Ming He paper.", "tokens": [492, 600, 1217, 2825, 466, 264, 20753, 19352, 634, 3035, 13], "temperature": 0.0, "avg_logprob": -0.18789093167174095, "compression_ratio": 1.6254980079681276, "no_speech_prob": 2.4679256966919638e-05}, {"id": 303, "seek": 143784, "start": 1445.04, "end": 1449.8, "text": " There's an interesting paper called All You Need is a Good Init, which describes how you", "tokens": [821, 311, 364, 1880, 3035, 1219, 1057, 509, 16984, 307, 257, 2205, 682, 270, 11, 597, 15626, 577, 291], "temperature": 0.0, "avg_logprob": -0.18789093167174095, "compression_ratio": 1.6254980079681276, "no_speech_prob": 2.4679256966919638e-05}, {"id": 304, "seek": 143784, "start": 1449.8, "end": 1455.84, "text": " can kind of iteratively go through your network and set one layer of weights at a time to", "tokens": [393, 733, 295, 17138, 19020, 352, 807, 428, 3209, 293, 992, 472, 4583, 295, 17443, 412, 257, 565, 281], "temperature": 0.0, "avg_logprob": -0.18789093167174095, "compression_ratio": 1.6254980079681276, "no_speech_prob": 2.4679256966919638e-05}, {"id": 305, "seek": 143784, "start": 1455.84, "end": 1460.48, "text": " like literally like kind of do a little optimize to find out which set of parameters gets you", "tokens": [411, 3736, 411, 733, 295, 360, 257, 707, 19719, 281, 915, 484, 597, 992, 295, 9834, 2170, 291], "temperature": 0.0, "avg_logprob": -0.18789093167174095, "compression_ratio": 1.6254980079681276, "no_speech_prob": 2.4679256966919638e-05}, {"id": 306, "seek": 143784, "start": 1460.48, "end": 1464.28, "text": " a unit variance at every point.", "tokens": [257, 4985, 21977, 412, 633, 935, 13], "temperature": 0.0, "avg_logprob": -0.18789093167174095, "compression_ratio": 1.6254980079681276, "no_speech_prob": 2.4679256966919638e-05}, {"id": 307, "seek": 146428, "start": 1464.28, "end": 1469.6399999999999, "text": " There's another cool paper which talks about something called orthogonal initialization.", "tokens": [821, 311, 1071, 1627, 3035, 597, 6686, 466, 746, 1219, 41488, 5883, 2144, 13], "temperature": 0.0, "avg_logprob": -0.11937797719782049, "compression_ratio": 1.845018450184502, "no_speech_prob": 1.2218600204505492e-05}, {"id": 308, "seek": 146428, "start": 1469.6399999999999, "end": 1474.84, "text": " If you've done some linear algebra, particularly if you've done Rachel's computational linear", "tokens": [759, 291, 600, 1096, 512, 8213, 21989, 11, 4098, 498, 291, 600, 1096, 14246, 311, 28270, 8213], "temperature": 0.0, "avg_logprob": -0.11937797719782049, "compression_ratio": 1.845018450184502, "no_speech_prob": 1.2218600204505492e-05}, {"id": 309, "seek": 146428, "start": 1474.84, "end": 1479.44, "text": " algebra course, you'll know about the idea of orthogonal matrices, and they make good", "tokens": [21989, 1164, 11, 291, 603, 458, 466, 264, 1558, 295, 41488, 32284, 11, 293, 436, 652, 665], "temperature": 0.0, "avg_logprob": -0.11937797719782049, "compression_ratio": 1.845018450184502, "no_speech_prob": 1.2218600204505492e-05}, {"id": 310, "seek": 146428, "start": 1479.44, "end": 1480.7, "text": " innits.", "tokens": [7714, 1208, 13], "temperature": 0.0, "avg_logprob": -0.11937797719782049, "compression_ratio": 1.845018450184502, "no_speech_prob": 1.2218600204505492e-05}, {"id": 311, "seek": 146428, "start": 1480.7, "end": 1486.92, "text": " We talked briefly last week about fix-up initialization, and then there's also something called self-normalizing", "tokens": [492, 2825, 10515, 1036, 1243, 466, 3191, 12, 1010, 5883, 2144, 11, 293, 550, 456, 311, 611, 746, 1219, 2698, 12, 23157, 3319], "temperature": 0.0, "avg_logprob": -0.11937797719782049, "compression_ratio": 1.845018450184502, "no_speech_prob": 1.2218600204505492e-05}, {"id": 312, "seek": 146428, "start": 1486.92, "end": 1489.08, "text": " neural networks.", "tokens": [18161, 9590, 13], "temperature": 0.0, "avg_logprob": -0.11937797719782049, "compression_ratio": 1.845018450184502, "no_speech_prob": 1.2218600204505492e-05}, {"id": 313, "seek": 146428, "start": 1489.08, "end": 1493.32, "text": " Fix-up and self-normalizing neural networks are both interesting papers because they describe", "tokens": [25538, 12, 1010, 293, 2698, 12, 23157, 3319, 18161, 9590, 366, 1293, 1880, 10577, 570, 436, 6786], "temperature": 0.0, "avg_logprob": -0.11937797719782049, "compression_ratio": 1.845018450184502, "no_speech_prob": 1.2218600204505492e-05}, {"id": 314, "seek": 149332, "start": 1493.32, "end": 1501.8799999999999, "text": " how to try to set a combination of kind of activation functions and in it such that you", "tokens": [577, 281, 853, 281, 992, 257, 6562, 295, 733, 295, 24433, 6828, 293, 294, 309, 1270, 300, 291], "temperature": 0.0, "avg_logprob": -0.14830475052197775, "compression_ratio": 1.6508620689655173, "no_speech_prob": 3.4464983400539495e-06}, {"id": 315, "seek": 149332, "start": 1501.8799999999999, "end": 1505.4399999999998, "text": " are guaranteed a unit variance as deep as you like.", "tokens": [366, 18031, 257, 4985, 21977, 382, 2452, 382, 291, 411, 13], "temperature": 0.0, "avg_logprob": -0.14830475052197775, "compression_ratio": 1.6508620689655173, "no_speech_prob": 3.4464983400539495e-06}, {"id": 316, "seek": 149332, "start": 1505.4399999999998, "end": 1510.9199999999998, "text": " And both of those two papers went to something like a thousand layers deep and trained them", "tokens": [400, 1293, 295, 729, 732, 10577, 1437, 281, 746, 411, 257, 4714, 7914, 2452, 293, 8895, 552], "temperature": 0.0, "avg_logprob": -0.14830475052197775, "compression_ratio": 1.6508620689655173, "no_speech_prob": 3.4464983400539495e-06}, {"id": 317, "seek": 149332, "start": 1510.9199999999998, "end": 1513.04, "text": " successfully.", "tokens": [10727, 13], "temperature": 0.0, "avg_logprob": -0.14830475052197775, "compression_ratio": 1.6508620689655173, "no_speech_prob": 3.4464983400539495e-06}, {"id": 318, "seek": 149332, "start": 1513.04, "end": 1517.84, "text": " In both cases, the fix-up is much more recent, but in both cases people have kind of hailed", "tokens": [682, 1293, 3331, 11, 264, 3191, 12, 1010, 307, 709, 544, 5162, 11, 457, 294, 1293, 3331, 561, 362, 733, 295, 324, 7292], "temperature": 0.0, "avg_logprob": -0.14830475052197775, "compression_ratio": 1.6508620689655173, "no_speech_prob": 3.4464983400539495e-06}, {"id": 319, "seek": 149332, "start": 1517.84, "end": 1522.36, "text": " them as reasons we can get rid of batch norm.", "tokens": [552, 382, 4112, 321, 393, 483, 3973, 295, 15245, 2026, 13], "temperature": 0.0, "avg_logprob": -0.14830475052197775, "compression_ratio": 1.6508620689655173, "no_speech_prob": 3.4464983400539495e-06}, {"id": 320, "seek": 152236, "start": 1522.36, "end": 1524.32, "text": " I think it's very unlikely to be true.", "tokens": [286, 519, 309, 311, 588, 17518, 281, 312, 2074, 13], "temperature": 0.0, "avg_logprob": -0.15759002330691316, "compression_ratio": 1.7322033898305085, "no_speech_prob": 1.8341106624575332e-05}, {"id": 321, "seek": 152236, "start": 1524.32, "end": 1529.7199999999998, "text": " Very few people use this SELU thing now because in both cases they're incredibly fiddly.", "tokens": [4372, 1326, 561, 764, 341, 318, 3158, 52, 551, 586, 570, 294, 1293, 3331, 436, 434, 6252, 283, 14273, 356, 13], "temperature": 0.0, "avg_logprob": -0.15759002330691316, "compression_ratio": 1.7322033898305085, "no_speech_prob": 1.8341106624575332e-05}, {"id": 322, "seek": 152236, "start": 1529.7199999999998, "end": 1535.3999999999999, "text": " So, for example, in the self-normalizing neural networks case, if you put in dropout, you", "tokens": [407, 11, 337, 1365, 11, 294, 264, 2698, 12, 23157, 3319, 18161, 9590, 1389, 11, 498, 291, 829, 294, 3270, 346, 11, 291], "temperature": 0.0, "avg_logprob": -0.15759002330691316, "compression_ratio": 1.7322033898305085, "no_speech_prob": 1.8341106624575332e-05}, {"id": 323, "seek": 152236, "start": 1535.3999999999999, "end": 1536.84, "text": " need to put a correction.", "tokens": [643, 281, 829, 257, 19984, 13], "temperature": 0.0, "avg_logprob": -0.15759002330691316, "compression_ratio": 1.7322033898305085, "no_speech_prob": 1.8341106624575332e-05}, {"id": 324, "seek": 152236, "start": 1536.84, "end": 1540.4399999999998, "text": " If you do anything different, you need to put in a correction because as soon as you've", "tokens": [759, 291, 360, 1340, 819, 11, 291, 643, 281, 829, 294, 257, 19984, 570, 382, 2321, 382, 291, 600], "temperature": 0.0, "avg_logprob": -0.15759002330691316, "compression_ratio": 1.7322033898305085, "no_speech_prob": 1.8341106624575332e-05}, {"id": 325, "seek": 152236, "start": 1540.4399999999998, "end": 1545.08, "text": " seen, as soon as something changes like the amount of leakiness in your activation function", "tokens": [1612, 11, 382, 2321, 382, 746, 2962, 411, 264, 2372, 295, 17143, 1324, 294, 428, 24433, 2445], "temperature": 0.0, "avg_logprob": -0.15759002330691316, "compression_ratio": 1.7322033898305085, "no_speech_prob": 1.8341106624575332e-05}, {"id": 326, "seek": 152236, "start": 1545.08, "end": 1550.6, "text": " or whatever, all of your assumptions about what your variance will be in the next layer", "tokens": [420, 2035, 11, 439, 295, 428, 17695, 466, 437, 428, 21977, 486, 312, 294, 264, 958, 4583], "temperature": 0.0, "avg_logprob": -0.15759002330691316, "compression_ratio": 1.7322033898305085, "no_speech_prob": 1.8341106624575332e-05}, {"id": 327, "seek": 155060, "start": 1550.6, "end": 1553.52, "text": " disappear.", "tokens": [11596, 13], "temperature": 0.0, "avg_logprob": -0.14136244397644604, "compression_ratio": 1.5714285714285714, "no_speech_prob": 5.1737397370743565e-06}, {"id": 328, "seek": 155060, "start": 1553.52, "end": 1558.54, "text": " And for this SELU paper, it was a particular problem because it relied on two specific", "tokens": [400, 337, 341, 318, 3158, 52, 3035, 11, 309, 390, 257, 1729, 1154, 570, 309, 35463, 322, 732, 2685], "temperature": 0.0, "avg_logprob": -0.14136244397644604, "compression_ratio": 1.5714285714285714, "no_speech_prob": 5.1737397370743565e-06}, {"id": 329, "seek": 155060, "start": 1558.54, "end": 1566.9599999999998, "text": " numbers that were calculated in a famous 96-page long appendix of math in the SELU paper.", "tokens": [3547, 300, 645, 15598, 294, 257, 4618, 24124, 12, 15161, 938, 34116, 970, 295, 5221, 294, 264, 318, 3158, 52, 3035, 13], "temperature": 0.0, "avg_logprob": -0.14136244397644604, "compression_ratio": 1.5714285714285714, "no_speech_prob": 5.1737397370743565e-06}, {"id": 330, "seek": 155060, "start": 1566.9599999999998, "end": 1572.7199999999998, "text": " And so if you wanted to do a slightly different architecture in any way, and they only showed", "tokens": [400, 370, 498, 291, 1415, 281, 360, 257, 4748, 819, 9482, 294, 604, 636, 11, 293, 436, 787, 4712], "temperature": 0.0, "avg_logprob": -0.14136244397644604, "compression_ratio": 1.5714285714285714, "no_speech_prob": 5.1737397370743565e-06}, {"id": 331, "seek": 155060, "start": 1572.7199999999998, "end": 1576.08, "text": " this a fully connected network, so even if you want to do convolutions, what are you", "tokens": [341, 257, 4498, 4582, 3209, 11, 370, 754, 498, 291, 528, 281, 360, 3754, 15892, 11, 437, 366, 291], "temperature": 0.0, "avg_logprob": -0.14136244397644604, "compression_ratio": 1.5714285714285714, "no_speech_prob": 5.1737397370743565e-06}, {"id": 332, "seek": 155060, "start": 1576.08, "end": 1577.08, "text": " going to do?", "tokens": [516, 281, 360, 30], "temperature": 0.0, "avg_logprob": -0.14136244397644604, "compression_ratio": 1.5714285714285714, "no_speech_prob": 5.1737397370743565e-06}, {"id": 333, "seek": 155060, "start": 1577.08, "end": 1579.12, "text": " Redo that 96 pages of math.", "tokens": [4477, 78, 300, 24124, 7183, 295, 5221, 13], "temperature": 0.0, "avg_logprob": -0.14136244397644604, "compression_ratio": 1.5714285714285714, "no_speech_prob": 5.1737397370743565e-06}, {"id": 334, "seek": 157912, "start": 1579.12, "end": 1583.8, "text": " So that 96 pages of math is now so famous that it has its own Twitter handle, the SELU", "tokens": [407, 300, 24124, 7183, 295, 5221, 307, 586, 370, 4618, 300, 309, 575, 1080, 1065, 5794, 4813, 11, 264, 318, 3158, 52], "temperature": 0.0, "avg_logprob": -0.14256022392062967, "compression_ratio": 1.62890625, "no_speech_prob": 1.7060854133887915e-06}, {"id": 335, "seek": 157912, "start": 1583.8, "end": 1590.8799999999999, "text": " appendix, which has the pin tweet, why does nobody want to read me?", "tokens": [34116, 970, 11, 597, 575, 264, 5447, 15258, 11, 983, 775, 5079, 528, 281, 1401, 385, 30], "temperature": 0.0, "avg_logprob": -0.14256022392062967, "compression_ratio": 1.62890625, "no_speech_prob": 1.7060854133887915e-06}, {"id": 336, "seek": 157912, "start": 1590.8799999999999, "end": 1595.7199999999998, "text": " And this is like literally what the entire 96 pages of the appendix looks like.", "tokens": [400, 341, 307, 411, 3736, 437, 264, 2302, 24124, 7183, 295, 264, 34116, 970, 1542, 411, 13], "temperature": 0.0, "avg_logprob": -0.14256022392062967, "compression_ratio": 1.62890625, "no_speech_prob": 1.7060854133887915e-06}, {"id": 337, "seek": 157912, "start": 1595.7199999999998, "end": 1603.0, "text": " I will mention that in my opinion, this is kind of a dumb way of finding those two numbers.", "tokens": [286, 486, 2152, 300, 294, 452, 4800, 11, 341, 307, 733, 295, 257, 10316, 636, 295, 5006, 729, 732, 3547, 13], "temperature": 0.0, "avg_logprob": -0.14256022392062967, "compression_ratio": 1.62890625, "no_speech_prob": 1.7060854133887915e-06}, {"id": 338, "seek": 157912, "start": 1603.0, "end": 1606.9799999999998, "text": " All you need is a good in it paper is a much better approach to kind of doing these things", "tokens": [1057, 291, 643, 307, 257, 665, 294, 309, 3035, 307, 257, 709, 1101, 3109, 281, 733, 295, 884, 613, 721], "temperature": 0.0, "avg_logprob": -0.14256022392062967, "compression_ratio": 1.62890625, "no_speech_prob": 1.7060854133887915e-06}, {"id": 339, "seek": 160698, "start": 1606.98, "end": 1611.48, "text": " in my opinion, which is like if you've got a couple of parameters you need to set, then", "tokens": [294, 452, 4800, 11, 597, 307, 411, 498, 291, 600, 658, 257, 1916, 295, 9834, 291, 643, 281, 992, 11, 550], "temperature": 0.0, "avg_logprob": -0.14404105273160067, "compression_ratio": 1.6345381526104417, "no_speech_prob": 8.530074410373345e-06}, {"id": 340, "seek": 160698, "start": 1611.48, "end": 1616.0, "text": " why not kind of set them using a quick little loop or something.", "tokens": [983, 406, 733, 295, 992, 552, 1228, 257, 1702, 707, 6367, 420, 746, 13], "temperature": 0.0, "avg_logprob": -0.14404105273160067, "compression_ratio": 1.6345381526104417, "no_speech_prob": 8.530074410373345e-06}, {"id": 341, "seek": 160698, "start": 1616.0, "end": 1620.48, "text": " So if you need those, if you want to find two kind of SELU parameters that work for", "tokens": [407, 498, 291, 643, 729, 11, 498, 291, 528, 281, 915, 732, 733, 295, 318, 3158, 52, 9834, 300, 589, 337], "temperature": 0.0, "avg_logprob": -0.14404105273160067, "compression_ratio": 1.6345381526104417, "no_speech_prob": 8.530074410373345e-06}, {"id": 342, "seek": 160698, "start": 1620.48, "end": 1626.16, "text": " your architecture, you can find them empirically pretty quickly and pretty easily.", "tokens": [428, 9482, 11, 291, 393, 915, 552, 25790, 984, 1238, 2661, 293, 1238, 3612, 13], "temperature": 0.0, "avg_logprob": -0.14404105273160067, "compression_ratio": 1.6345381526104417, "no_speech_prob": 8.530074410373345e-06}, {"id": 343, "seek": 160698, "start": 1626.16, "end": 1628.54, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.14404105273160067, "compression_ratio": 1.6345381526104417, "no_speech_prob": 8.530074410373345e-06}, {"id": 344, "seek": 160698, "start": 1628.54, "end": 1630.6, "text": " So that's a little bit about in it.", "tokens": [407, 300, 311, 257, 707, 857, 466, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.14404105273160067, "compression_ratio": 1.6345381526104417, "no_speech_prob": 8.530074410373345e-06}, {"id": 345, "seek": 160698, "start": 1630.6, "end": 1634.92, "text": " We'll come back to more of that very shortly.", "tokens": [492, 603, 808, 646, 281, 544, 295, 300, 588, 13392, 13], "temperature": 0.0, "avg_logprob": -0.14404105273160067, "compression_ratio": 1.6345381526104417, "no_speech_prob": 8.530074410373345e-06}, {"id": 346, "seek": 163492, "start": 1634.92, "end": 1642.68, "text": " There was one other question from last week, which was we noticed that the shape of the", "tokens": [821, 390, 472, 661, 1168, 490, 1036, 1243, 11, 597, 390, 321, 5694, 300, 264, 3909, 295, 264], "temperature": 0.0, "avg_logprob": -0.16113163295545077, "compression_ratio": 1.5846994535519126, "no_speech_prob": 1.568727930134628e-05}, {"id": 347, "seek": 163492, "start": 1642.68, "end": 1650.5600000000002, "text": " kind of manual linear layer we created and the shape of the PyTorch one were transposed,", "tokens": [733, 295, 9688, 8213, 4583, 321, 2942, 293, 264, 3909, 295, 264, 9953, 51, 284, 339, 472, 645, 7132, 1744, 11], "temperature": 0.0, "avg_logprob": -0.16113163295545077, "compression_ratio": 1.5846994535519126, "no_speech_prob": 1.568727930134628e-05}, {"id": 348, "seek": 163492, "start": 1650.5600000000002, "end": 1653.1000000000001, "text": " and the question was why.", "tokens": [293, 264, 1168, 390, 983, 13], "temperature": 0.0, "avg_logprob": -0.16113163295545077, "compression_ratio": 1.5846994535519126, "no_speech_prob": 1.568727930134628e-05}, {"id": 349, "seek": 163492, "start": 1653.1000000000001, "end": 1658.0, "text": " And so again, I did some digging into this until eventually Sumit from the PyTorch team", "tokens": [400, 370, 797, 11, 286, 630, 512, 17343, 666, 341, 1826, 4728, 8626, 270, 490, 264, 9953, 51, 284, 339, 1469], "temperature": 0.0, "avg_logprob": -0.16113163295545077, "compression_ratio": 1.5846994535519126, "no_speech_prob": 1.568727930134628e-05}, {"id": 350, "seek": 165800, "start": 1658.0, "end": 1667.2, "text": " pointed out to me this commit from seven years ago in the old Lua Torch code where this", "tokens": [10932, 484, 281, 385, 341, 5599, 490, 3407, 924, 2057, 294, 264, 1331, 441, 4398, 7160, 339, 3089, 689, 341], "temperature": 0.0, "avg_logprob": -0.14362726406175264, "compression_ratio": 1.578723404255319, "no_speech_prob": 2.4823830244713463e-06}, {"id": 351, "seek": 165800, "start": 1667.2, "end": 1668.44, "text": " actually happened.", "tokens": [767, 2011, 13], "temperature": 0.0, "avg_logprob": -0.14362726406175264, "compression_ratio": 1.578723404255319, "no_speech_prob": 2.4823830244713463e-06}, {"id": 352, "seek": 165800, "start": 1668.44, "end": 1674.92, "text": " And that basically it's because that old Lua library couldn't handle batch matrix multiplication", "tokens": [400, 300, 1936, 309, 311, 570, 300, 1331, 441, 4398, 6405, 2809, 380, 4813, 15245, 8141, 27290], "temperature": 0.0, "avg_logprob": -0.14362726406175264, "compression_ratio": 1.578723404255319, "no_speech_prob": 2.4823830244713463e-06}, {"id": 353, "seek": 165800, "start": 1674.92, "end": 1679.6, "text": " without doing it in this transposed way, and that's why still to this day PyTorch does", "tokens": [1553, 884, 309, 294, 341, 7132, 1744, 636, 11, 293, 300, 311, 983, 920, 281, 341, 786, 9953, 51, 284, 339, 775], "temperature": 0.0, "avg_logprob": -0.14362726406175264, "compression_ratio": 1.578723404255319, "no_speech_prob": 2.4823830244713463e-06}, {"id": 354, "seek": 165800, "start": 1679.6, "end": 1682.52, "text": " it kind of upside down, which is fine.", "tokens": [309, 733, 295, 14119, 760, 11, 597, 307, 2489, 13], "temperature": 0.0, "avg_logprob": -0.14362726406175264, "compression_ratio": 1.578723404255319, "no_speech_prob": 2.4823830244713463e-06}, {"id": 355, "seek": 165800, "start": 1682.52, "end": 1684.36, "text": " Like it's not slower, it's not a problem.", "tokens": [1743, 309, 311, 406, 14009, 11, 309, 311, 406, 257, 1154, 13], "temperature": 0.0, "avg_logprob": -0.14362726406175264, "compression_ratio": 1.578723404255319, "no_speech_prob": 2.4823830244713463e-06}, {"id": 356, "seek": 168436, "start": 1684.36, "end": 1691.1999999999998, "text": " But again, it's kind of an interesting case of like I find this happens all the time in", "tokens": [583, 797, 11, 309, 311, 733, 295, 364, 1880, 1389, 295, 411, 286, 915, 341, 2314, 439, 264, 565, 294], "temperature": 0.0, "avg_logprob": -0.18260864662913096, "compression_ratio": 1.6641221374045803, "no_speech_prob": 5.0145281420554966e-06}, {"id": 357, "seek": 168436, "start": 1691.1999999999998, "end": 1692.1999999999998, "text": " deep learning.", "tokens": [2452, 2539, 13], "temperature": 0.0, "avg_logprob": -0.18260864662913096, "compression_ratio": 1.6641221374045803, "no_speech_prob": 5.0145281420554966e-06}, {"id": 358, "seek": 168436, "start": 1692.1999999999998, "end": 1696.12, "text": " Something's done a particular way forever, and then everybody does it that way forever,", "tokens": [6595, 311, 1096, 257, 1729, 636, 5680, 11, 293, 550, 2201, 775, 309, 300, 636, 5680, 11], "temperature": 0.0, "avg_logprob": -0.18260864662913096, "compression_ratio": 1.6641221374045803, "no_speech_prob": 5.0145281420554966e-06}, {"id": 359, "seek": 168436, "start": 1696.12, "end": 1697.6399999999999, "text": " and nobody goes back and says why.", "tokens": [293, 5079, 1709, 646, 293, 1619, 983, 13], "temperature": 0.0, "avg_logprob": -0.18260864662913096, "compression_ratio": 1.6641221374045803, "no_speech_prob": 5.0145281420554966e-06}, {"id": 360, "seek": 168436, "start": 1697.6399999999999, "end": 1702.4199999999998, "text": " Now in this particular case, it really doesn't matter, I don't think.", "tokens": [823, 294, 341, 1729, 1389, 11, 309, 534, 1177, 380, 1871, 11, 286, 500, 380, 519, 13], "temperature": 0.0, "avg_logprob": -0.18260864662913096, "compression_ratio": 1.6641221374045803, "no_speech_prob": 5.0145281420554966e-06}, {"id": 361, "seek": 168436, "start": 1702.4199999999998, "end": 1704.02, "text": " But often it does, right?", "tokens": [583, 2049, 309, 775, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.18260864662913096, "compression_ratio": 1.6641221374045803, "no_speech_prob": 5.0145281420554966e-06}, {"id": 362, "seek": 168436, "start": 1704.02, "end": 1707.9599999999998, "text": " So like things like how do we initialize neural networks and how many layers should they have", "tokens": [407, 411, 721, 411, 577, 360, 321, 5883, 1125, 18161, 9590, 293, 577, 867, 7914, 820, 436, 362], "temperature": 0.0, "avg_logprob": -0.18260864662913096, "compression_ratio": 1.6641221374045803, "no_speech_prob": 5.0145281420554966e-06}, {"id": 363, "seek": 168436, "start": 1707.9599999999998, "end": 1708.9599999999998, "text": " and stuff like that.", "tokens": [293, 1507, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.18260864662913096, "compression_ratio": 1.6641221374045803, "no_speech_prob": 5.0145281420554966e-06}, {"id": 364, "seek": 170896, "start": 1708.96, "end": 1714.4, "text": " And nobody really challenged the normal practices for years.", "tokens": [400, 5079, 534, 17737, 264, 2710, 7525, 337, 924, 13], "temperature": 0.0, "avg_logprob": -0.15962009202866329, "compression_ratio": 1.5450236966824644, "no_speech_prob": 1.7776110325939953e-05}, {"id": 365, "seek": 170896, "start": 1714.4, "end": 1721.08, "text": " So I'm hoping that with this really ground up approach, you can see what the assumptions", "tokens": [407, 286, 478, 7159, 300, 365, 341, 534, 2727, 493, 3109, 11, 291, 393, 536, 437, 264, 17695], "temperature": 0.0, "avg_logprob": -0.15962009202866329, "compression_ratio": 1.5450236966824644, "no_speech_prob": 1.7776110325939953e-05}, {"id": 366, "seek": 170896, "start": 1721.08, "end": 1727.3600000000001, "text": " we're making are and see how to question them and see that to me PyTorch is the best library", "tokens": [321, 434, 1455, 366, 293, 536, 577, 281, 1168, 552, 293, 536, 300, 281, 385, 9953, 51, 284, 339, 307, 264, 1151, 6405], "temperature": 0.0, "avg_logprob": -0.15962009202866329, "compression_ratio": 1.5450236966824644, "no_speech_prob": 1.7776110325939953e-05}, {"id": 367, "seek": 170896, "start": 1727.3600000000001, "end": 1734.72, "text": " around at the moment, and even PyTorch has these weird kind of archaic edges to it.", "tokens": [926, 412, 264, 1623, 11, 293, 754, 9953, 51, 284, 339, 575, 613, 3657, 733, 295, 3912, 64, 299, 8819, 281, 309, 13], "temperature": 0.0, "avg_logprob": -0.15962009202866329, "compression_ratio": 1.5450236966824644, "no_speech_prob": 1.7776110325939953e-05}, {"id": 368, "seek": 173472, "start": 1734.72, "end": 1742.0, "text": " Okay, so that was a little diversion to start with, but a fun diversion because that's something", "tokens": [1033, 11, 370, 300, 390, 257, 707, 49422, 281, 722, 365, 11, 457, 257, 1019, 49422, 570, 300, 311, 746], "temperature": 0.0, "avg_logprob": -0.10976747719638319, "compression_ratio": 1.5211267605633803, "no_speech_prob": 1.696320941846352e-05}, {"id": 369, "seek": 173472, "start": 1742.0, "end": 1745.32, "text": " I spent a couple of days this week on and think it's pretty interesting.", "tokens": [286, 4418, 257, 1916, 295, 1708, 341, 1243, 322, 293, 519, 309, 311, 1238, 1880, 13], "temperature": 0.0, "avg_logprob": -0.10976747719638319, "compression_ratio": 1.5211267605633803, "no_speech_prob": 1.696320941846352e-05}, {"id": 370, "seek": 173472, "start": 1745.32, "end": 1755.08, "text": " So to go back to how do we implement a basic modern CNN model, we got to this point.", "tokens": [407, 281, 352, 646, 281, 577, 360, 321, 4445, 257, 3875, 4363, 24859, 2316, 11, 321, 658, 281, 341, 935, 13], "temperature": 0.0, "avg_logprob": -0.10976747719638319, "compression_ratio": 1.5211267605633803, "no_speech_prob": 1.696320941846352e-05}, {"id": 371, "seek": 173472, "start": 1755.08, "end": 1759.16, "text": " So we've done a matrix multiplication, so that's our affine function.", "tokens": [407, 321, 600, 1096, 257, 8141, 27290, 11, 370, 300, 311, 527, 2096, 533, 2445, 13], "temperature": 0.0, "avg_logprob": -0.10976747719638319, "compression_ratio": 1.5211267605633803, "no_speech_prob": 1.696320941846352e-05}, {"id": 372, "seek": 175916, "start": 1759.16, "end": 1767.52, "text": " We've done ReLU, so that's our non-linearity, and so a fully connected network forward is", "tokens": [492, 600, 1096, 1300, 43, 52, 11, 370, 300, 311, 527, 2107, 12, 1889, 17409, 11, 293, 370, 257, 4498, 4582, 3209, 2128, 307], "temperature": 0.0, "avg_logprob": -0.11849756424243633, "compression_ratio": 1.609442060085837, "no_speech_prob": 3.555911234798259e-06}, {"id": 373, "seek": 175916, "start": 1767.52, "end": 1771.76, "text": " simply layering together those two things.", "tokens": [2935, 40754, 1214, 729, 732, 721, 13], "temperature": 0.0, "avg_logprob": -0.11849756424243633, "compression_ratio": 1.609442060085837, "no_speech_prob": 3.555911234798259e-06}, {"id": 374, "seek": 175916, "start": 1771.76, "end": 1776.3600000000001, "text": " So we did that, and then we did the backward pass and we kind of refactored that nicely", "tokens": [407, 321, 630, 300, 11, 293, 550, 321, 630, 264, 23897, 1320, 293, 321, 733, 295, 1895, 578, 2769, 300, 9594], "temperature": 0.0, "avg_logprob": -0.11849756424243633, "compression_ratio": 1.609442060085837, "no_speech_prob": 3.555911234798259e-06}, {"id": 375, "seek": 175916, "start": 1776.3600000000001, "end": 1781.96, "text": " and it turned out that it looked pretty similar to PyTorch's way of doing things.", "tokens": [293, 309, 3574, 484, 300, 309, 2956, 1238, 2531, 281, 9953, 51, 284, 339, 311, 636, 295, 884, 721, 13], "temperature": 0.0, "avg_logprob": -0.11849756424243633, "compression_ratio": 1.609442060085837, "no_speech_prob": 3.555911234798259e-06}, {"id": 376, "seek": 175916, "start": 1781.96, "end": 1787.16, "text": " And so now we're ready to train our model, and that's where we're up to.", "tokens": [400, 370, 586, 321, 434, 1919, 281, 3847, 527, 2316, 11, 293, 300, 311, 689, 321, 434, 493, 281, 13], "temperature": 0.0, "avg_logprob": -0.11849756424243633, "compression_ratio": 1.609442060085837, "no_speech_prob": 3.555911234798259e-06}, {"id": 377, "seek": 178716, "start": 1787.16, "end": 1793.38, "text": " So here we are, 03 MiniBatch training, and we're going to train our model.", "tokens": [407, 510, 321, 366, 11, 43677, 18239, 33, 852, 3097, 11, 293, 321, 434, 516, 281, 3847, 527, 2316, 13], "temperature": 0.0, "avg_logprob": -0.15308800372448597, "compression_ratio": 1.5829383886255923, "no_speech_prob": 4.425367933436064e-06}, {"id": 378, "seek": 178716, "start": 1793.38, "end": 1798.3000000000002, "text": " So we can start by grabbing our MNIST data.", "tokens": [407, 321, 393, 722, 538, 23771, 527, 376, 45, 19756, 1412, 13], "temperature": 0.0, "avg_logprob": -0.15308800372448597, "compression_ratio": 1.5829383886255923, "no_speech_prob": 4.425367933436064e-06}, {"id": 379, "seek": 178716, "start": 1798.3000000000002, "end": 1804.8000000000002, "text": " So again, we're just importing the stuff that we just exported from the previous class.", "tokens": [407, 797, 11, 321, 434, 445, 43866, 264, 1507, 300, 321, 445, 42055, 490, 264, 3894, 1508, 13], "temperature": 0.0, "avg_logprob": -0.15308800372448597, "compression_ratio": 1.5829383886255923, "no_speech_prob": 4.425367933436064e-06}, {"id": 380, "seek": 178716, "start": 1804.8000000000002, "end": 1809.5, "text": " Here's the model we created in the previous class.", "tokens": [1692, 311, 264, 2316, 321, 2942, 294, 264, 3894, 1508, 13], "temperature": 0.0, "avg_logprob": -0.15308800372448597, "compression_ratio": 1.5829383886255923, "no_speech_prob": 4.425367933436064e-06}, {"id": 381, "seek": 178716, "start": 1809.5, "end": 1814.8600000000001, "text": " And so let's get some predictions from that model, and we'll call them Pred.", "tokens": [400, 370, 718, 311, 483, 512, 21264, 490, 300, 2316, 11, 293, 321, 603, 818, 552, 32969, 13], "temperature": 0.0, "avg_logprob": -0.15308800372448597, "compression_ratio": 1.5829383886255923, "no_speech_prob": 4.425367933436064e-06}, {"id": 382, "seek": 181486, "start": 1814.86, "end": 1820.6799999999998, "text": " And so now to train our model, the first thing we need to do is we need a loss function,", "tokens": [400, 370, 586, 281, 3847, 527, 2316, 11, 264, 700, 551, 321, 643, 281, 360, 307, 321, 643, 257, 4470, 2445, 11], "temperature": 0.0, "avg_logprob": -0.09459029606410435, "compression_ratio": 1.6233766233766234, "no_speech_prob": 8.397672900173347e-06}, {"id": 383, "seek": 181486, "start": 1820.6799999999998, "end": 1823.36, "text": " because without a loss function, we can't train it.", "tokens": [570, 1553, 257, 4470, 2445, 11, 321, 393, 380, 3847, 309, 13], "temperature": 0.0, "avg_logprob": -0.09459029606410435, "compression_ratio": 1.6233766233766234, "no_speech_prob": 8.397672900173347e-06}, {"id": 384, "seek": 181486, "start": 1823.36, "end": 1829.0, "text": " Now previously we used mean squared error, which I said was a total cheat.", "tokens": [823, 8046, 321, 1143, 914, 8889, 6713, 11, 597, 286, 848, 390, 257, 3217, 17470, 13], "temperature": 0.0, "avg_logprob": -0.09459029606410435, "compression_ratio": 1.6233766233766234, "no_speech_prob": 8.397672900173347e-06}, {"id": 385, "seek": 181486, "start": 1829.0, "end": 1835.4399999999998, "text": " Now that we've decided to trust PyTorch's autograd, we can use many more things because", "tokens": [823, 300, 321, 600, 3047, 281, 3361, 9953, 51, 284, 339, 311, 1476, 664, 6206, 11, 321, 393, 764, 867, 544, 721, 570], "temperature": 0.0, "avg_logprob": -0.09459029606410435, "compression_ratio": 1.6233766233766234, "no_speech_prob": 8.397672900173347e-06}, {"id": 386, "seek": 181486, "start": 1835.4399999999998, "end": 1839.52, "text": " you don't have to write our own gradients, and I'm too lazy to do that.", "tokens": [291, 500, 380, 362, 281, 2464, 527, 1065, 2771, 2448, 11, 293, 286, 478, 886, 14847, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.09459029606410435, "compression_ratio": 1.6233766233766234, "no_speech_prob": 8.397672900173347e-06}, {"id": 387, "seek": 183952, "start": 1839.52, "end": 1845.04, "text": " So let's go ahead and use cross entropy, because cross entropy makes a lot more sense.", "tokens": [407, 718, 311, 352, 2286, 293, 764, 3278, 30867, 11, 570, 3278, 30867, 1669, 257, 688, 544, 2020, 13], "temperature": 0.0, "avg_logprob": -0.1632523727416992, "compression_ratio": 1.7300884955752212, "no_speech_prob": 8.939324288803618e-06}, {"id": 388, "seek": 183952, "start": 1845.04, "end": 1854.6399999999999, "text": " To remind you from the last class, there is an entropy example notebook where we learned,", "tokens": [1407, 4160, 291, 490, 264, 1036, 1508, 11, 456, 307, 364, 30867, 1365, 21060, 689, 321, 3264, 11], "temperature": 0.0, "avg_logprob": -0.1632523727416992, "compression_ratio": 1.7300884955752212, "no_speech_prob": 8.939324288803618e-06}, {"id": 389, "seek": 183952, "start": 1854.6399999999999, "end": 1858.08, "text": " first of all, that cross entropy requires doing two things.", "tokens": [700, 295, 439, 11, 300, 3278, 30867, 7029, 884, 732, 721, 13], "temperature": 0.0, "avg_logprob": -0.1632523727416992, "compression_ratio": 1.7300884955752212, "no_speech_prob": 8.939324288803618e-06}, {"id": 390, "seek": 183952, "start": 1858.08, "end": 1860.08, "text": " First of all, softmax.", "tokens": [2386, 295, 439, 11, 2787, 41167, 13], "temperature": 0.0, "avg_logprob": -0.1632523727416992, "compression_ratio": 1.7300884955752212, "no_speech_prob": 8.939324288803618e-06}, {"id": 391, "seek": 183952, "start": 1860.08, "end": 1866.52, "text": " Well, in the case of this multi-class categorical cross entropy, you first do softmax, and then", "tokens": [1042, 11, 294, 264, 1389, 295, 341, 4825, 12, 11665, 19250, 804, 3278, 30867, 11, 291, 700, 360, 2787, 41167, 11, 293, 550], "temperature": 0.0, "avg_logprob": -0.1632523727416992, "compression_ratio": 1.7300884955752212, "no_speech_prob": 8.939324288803618e-06}, {"id": 392, "seek": 183952, "start": 1866.52, "end": 1868.2, "text": " you do the negative log likelihood.", "tokens": [291, 360, 264, 3671, 3565, 22119, 13], "temperature": 0.0, "avg_logprob": -0.1632523727416992, "compression_ratio": 1.7300884955752212, "no_speech_prob": 8.939324288803618e-06}, {"id": 393, "seek": 186820, "start": 1868.2, "end": 1875.16, "text": " So the softmax was if we have a bunch of different possible predictions, and we got some output", "tokens": [407, 264, 2787, 41167, 390, 498, 321, 362, 257, 3840, 295, 819, 1944, 21264, 11, 293, 321, 658, 512, 5598], "temperature": 0.0, "avg_logprob": -0.08084883378899616, "compression_ratio": 1.7272727272727273, "no_speech_prob": 6.643128472205717e-06}, {"id": 394, "seek": 186820, "start": 1875.16, "end": 1881.0, "text": " for each one from our model, then we take e to the power of that output, we sum them", "tokens": [337, 1184, 472, 490, 527, 2316, 11, 550, 321, 747, 308, 281, 264, 1347, 295, 300, 5598, 11, 321, 2408, 552], "temperature": 0.0, "avg_logprob": -0.08084883378899616, "compression_ratio": 1.7272727272727273, "no_speech_prob": 6.643128472205717e-06}, {"id": 395, "seek": 186820, "start": 1881.0, "end": 1887.88, "text": " all up, and then we take the e to the power of divided by the sum of e to the power of.", "tokens": [439, 493, 11, 293, 550, 321, 747, 264, 308, 281, 264, 1347, 295, 6666, 538, 264, 2408, 295, 308, 281, 264, 1347, 295, 13], "temperature": 0.0, "avg_logprob": -0.08084883378899616, "compression_ratio": 1.7272727272727273, "no_speech_prob": 6.643128472205717e-06}, {"id": 396, "seek": 186820, "start": 1887.88, "end": 1890.48, "text": " And that was our softmax.", "tokens": [400, 300, 390, 527, 2787, 41167, 13], "temperature": 0.0, "avg_logprob": -0.08084883378899616, "compression_ratio": 1.7272727272727273, "no_speech_prob": 6.643128472205717e-06}, {"id": 397, "seek": 186820, "start": 1890.48, "end": 1895.88, "text": " So there it is in math form.", "tokens": [407, 456, 309, 307, 294, 5221, 1254, 13], "temperature": 0.0, "avg_logprob": -0.08084883378899616, "compression_ratio": 1.7272727272727273, "no_speech_prob": 6.643128472205717e-06}, {"id": 398, "seek": 189588, "start": 1895.88, "end": 1905.2800000000002, "text": " There it is in summation math form, and here it is in code form.", "tokens": [821, 309, 307, 294, 28811, 5221, 1254, 11, 293, 510, 309, 307, 294, 3089, 1254, 13], "temperature": 0.0, "avg_logprob": -0.20159046624296456, "compression_ratio": 1.549222797927461, "no_speech_prob": 1.7603152855372173e-06}, {"id": 399, "seek": 189588, "start": 1905.2800000000002, "end": 1914.4, "text": " So e to the x divided by x exp sum, and then the whole thing we do a dot log, and that's", "tokens": [407, 308, 281, 264, 2031, 6666, 538, 2031, 1278, 2408, 11, 293, 550, 264, 1379, 551, 321, 360, 257, 5893, 3565, 11, 293, 300, 311], "temperature": 0.0, "avg_logprob": -0.20159046624296456, "compression_ratio": 1.549222797927461, "no_speech_prob": 1.7603152855372173e-06}, {"id": 400, "seek": 189588, "start": 1914.4, "end": 1922.48, "text": " because in PyTorch, negative log likelihood expects a log softmax, not just a softmax.", "tokens": [570, 294, 9953, 51, 284, 339, 11, 3671, 3565, 22119, 33280, 257, 3565, 2787, 41167, 11, 406, 445, 257, 2787, 41167, 13], "temperature": 0.0, "avg_logprob": -0.20159046624296456, "compression_ratio": 1.549222797927461, "no_speech_prob": 1.7603152855372173e-06}, {"id": 401, "seek": 189588, "start": 1922.48, "end": 1923.8000000000002, "text": " And we'll see why in a moment.", "tokens": [400, 321, 603, 536, 983, 294, 257, 1623, 13], "temperature": 0.0, "avg_logprob": -0.20159046624296456, "compression_ratio": 1.549222797927461, "no_speech_prob": 1.7603152855372173e-06}, {"id": 402, "seek": 189588, "start": 1923.8000000000002, "end": 1925.6000000000001, "text": " So we pop a log in the end.", "tokens": [407, 321, 1665, 257, 3565, 294, 264, 917, 13], "temperature": 0.0, "avg_logprob": -0.20159046624296456, "compression_ratio": 1.549222797927461, "no_speech_prob": 1.7603152855372173e-06}, {"id": 403, "seek": 192560, "start": 1925.6, "end": 1929.1599999999999, "text": " So here's our log of softmax function.", "tokens": [407, 510, 311, 527, 3565, 295, 2787, 41167, 2445, 13], "temperature": 0.0, "avg_logprob": -0.10162876747749947, "compression_ratio": 1.5977653631284916, "no_speech_prob": 1.3007007510168478e-05}, {"id": 404, "seek": 192560, "start": 1929.1599999999999, "end": 1936.6399999999999, "text": " So now we can go ahead and create our softmax predictions by passing preds to log softmax.", "tokens": [407, 586, 321, 393, 352, 2286, 293, 1884, 527, 2787, 41167, 21264, 538, 8437, 3852, 82, 281, 3565, 2787, 41167, 13], "temperature": 0.0, "avg_logprob": -0.10162876747749947, "compression_ratio": 1.5977653631284916, "no_speech_prob": 1.3007007510168478e-05}, {"id": 405, "seek": 192560, "start": 1936.6399999999999, "end": 1942.1399999999999, "text": " Now that we've done that, we can calculate cross entropy loss.", "tokens": [823, 300, 321, 600, 1096, 300, 11, 321, 393, 8873, 3278, 30867, 4470, 13], "temperature": 0.0, "avg_logprob": -0.10162876747749947, "compression_ratio": 1.5977653631284916, "no_speech_prob": 1.3007007510168478e-05}, {"id": 406, "seek": 192560, "start": 1942.1399999999999, "end": 1952.6399999999999, "text": " And cross entropy loss is generally expressed in this form, which is this form, sum of actual", "tokens": [400, 3278, 30867, 4470, 307, 5101, 12675, 294, 341, 1254, 11, 597, 307, 341, 1254, 11, 2408, 295, 3539], "temperature": 0.0, "avg_logprob": -0.10162876747749947, "compression_ratio": 1.5977653631284916, "no_speech_prob": 1.3007007510168478e-05}, {"id": 407, "seek": 195264, "start": 1952.64, "end": 1957.3600000000001, "text": " times the log of the probability of that actual.", "tokens": [1413, 264, 3565, 295, 264, 8482, 295, 300, 3539, 13], "temperature": 0.0, "avg_logprob": -0.10463240228850267, "compression_ratio": 1.8894736842105264, "no_speech_prob": 3.0894582323526265e-06}, {"id": 408, "seek": 195264, "start": 1957.3600000000001, "end": 1962.8000000000002, "text": " So in other words, if we have is cat and is dog, then here's our actuals.", "tokens": [407, 294, 661, 2283, 11, 498, 321, 362, 307, 3857, 293, 307, 3000, 11, 550, 510, 311, 527, 3539, 82, 13], "temperature": 0.0, "avg_logprob": -0.10463240228850267, "compression_ratio": 1.8894736842105264, "no_speech_prob": 3.0894582323526265e-06}, {"id": 409, "seek": 195264, "start": 1962.8000000000002, "end": 1964.2, "text": " So it's one hot encoded.", "tokens": [407, 309, 311, 472, 2368, 2058, 12340, 13], "temperature": 0.0, "avg_logprob": -0.10463240228850267, "compression_ratio": 1.8894736842105264, "no_speech_prob": 3.0894582323526265e-06}, {"id": 410, "seek": 195264, "start": 1964.2, "end": 1965.2800000000002, "text": " Is cat yes?", "tokens": [1119, 3857, 2086, 30], "temperature": 0.0, "avg_logprob": -0.10463240228850267, "compression_ratio": 1.8894736842105264, "no_speech_prob": 3.0894582323526265e-06}, {"id": 411, "seek": 195264, "start": 1965.2800000000002, "end": 1967.0800000000002, "text": " Is dog no?", "tokens": [1119, 3000, 572, 30], "temperature": 0.0, "avg_logprob": -0.10463240228850267, "compression_ratio": 1.8894736842105264, "no_speech_prob": 3.0894582323526265e-06}, {"id": 412, "seek": 195264, "start": 1967.0800000000002, "end": 1973.5600000000002, "text": " We have our predictions from our model, from our softmax.", "tokens": [492, 362, 527, 21264, 490, 527, 2316, 11, 490, 527, 2787, 41167, 13], "temperature": 0.0, "avg_logprob": -0.10463240228850267, "compression_ratio": 1.8894736842105264, "no_speech_prob": 3.0894582323526265e-06}, {"id": 413, "seek": 195264, "start": 1973.5600000000002, "end": 1977.48, "text": " We can then say, well, what's the log of the probability it's a cat?", "tokens": [492, 393, 550, 584, 11, 731, 11, 437, 311, 264, 3565, 295, 264, 8482, 309, 311, 257, 3857, 30], "temperature": 0.0, "avg_logprob": -0.10463240228850267, "compression_ratio": 1.8894736842105264, "no_speech_prob": 3.0894582323526265e-06}, {"id": 414, "seek": 195264, "start": 1977.48, "end": 1978.96, "text": " So log of this.", "tokens": [407, 3565, 295, 341, 13], "temperature": 0.0, "avg_logprob": -0.10463240228850267, "compression_ratio": 1.8894736842105264, "no_speech_prob": 3.0894582323526265e-06}, {"id": 415, "seek": 195264, "start": 1978.96, "end": 1980.8200000000002, "text": " What's the log of the probability it's a dog?", "tokens": [708, 311, 264, 3565, 295, 264, 8482, 309, 311, 257, 3000, 30], "temperature": 0.0, "avg_logprob": -0.10463240228850267, "compression_ratio": 1.8894736842105264, "no_speech_prob": 3.0894582323526265e-06}, {"id": 416, "seek": 198082, "start": 1980.82, "end": 1983.3999999999999, "text": " So log of 1 minus that.", "tokens": [407, 3565, 295, 502, 3175, 300, 13], "temperature": 0.0, "avg_logprob": -0.11771409055019946, "compression_ratio": 1.7064220183486238, "no_speech_prob": 6.475866030086763e-07}, {"id": 417, "seek": 198082, "start": 1983.3999999999999, "end": 1989.8799999999999, "text": " And so then our negative log likelihood is simply b times e plus c times f.", "tokens": [400, 370, 550, 527, 3671, 3565, 22119, 307, 2935, 272, 1413, 308, 1804, 269, 1413, 283, 13], "temperature": 0.0, "avg_logprob": -0.11771409055019946, "compression_ratio": 1.7064220183486238, "no_speech_prob": 6.475866030086763e-07}, {"id": 418, "seek": 198082, "start": 1989.8799999999999, "end": 1991.96, "text": " And then take the negative of all that.", "tokens": [400, 550, 747, 264, 3671, 295, 439, 300, 13], "temperature": 0.0, "avg_logprob": -0.11771409055019946, "compression_ratio": 1.7064220183486238, "no_speech_prob": 6.475866030086763e-07}, {"id": 419, "seek": 198082, "start": 1991.96, "end": 1997.0, "text": " That's negative log likelihood, which is what this is.", "tokens": [663, 311, 3671, 3565, 22119, 11, 597, 307, 437, 341, 307, 13], "temperature": 0.0, "avg_logprob": -0.11771409055019946, "compression_ratio": 1.7064220183486238, "no_speech_prob": 6.475866030086763e-07}, {"id": 420, "seek": 198082, "start": 1997.0, "end": 2001.28, "text": " But remember, and I know I keep saying this because people keep forgetting, not you guys,", "tokens": [583, 1604, 11, 293, 286, 458, 286, 1066, 1566, 341, 570, 561, 1066, 25428, 11, 406, 291, 1074, 11], "temperature": 0.0, "avg_logprob": -0.11771409055019946, "compression_ratio": 1.7064220183486238, "no_speech_prob": 6.475866030086763e-07}, {"id": 421, "seek": 198082, "start": 2001.28, "end": 2004.7, "text": " but people out in the world keep forgetting that when you're multiplying by stuff which", "tokens": [457, 561, 484, 294, 264, 1002, 1066, 25428, 300, 562, 291, 434, 30955, 538, 1507, 597], "temperature": 0.0, "avg_logprob": -0.11771409055019946, "compression_ratio": 1.7064220183486238, "no_speech_prob": 6.475866030086763e-07}, {"id": 422, "seek": 200470, "start": 2004.7, "end": 2011.8400000000001, "text": " is mainly 0 and one hot encoded categorical classification, most of your categories are", "tokens": [307, 8704, 1958, 293, 472, 2368, 2058, 12340, 19250, 804, 21538, 11, 881, 295, 428, 10479, 366], "temperature": 0.0, "avg_logprob": -0.14426801789481686, "compression_ratio": 1.7149122807017543, "no_speech_prob": 2.058014388239826e-06}, {"id": 423, "seek": 200470, "start": 2011.8400000000001, "end": 2012.8400000000001, "text": " 0.", "tokens": [1958, 13], "temperature": 0.0, "avg_logprob": -0.14426801789481686, "compression_ratio": 1.7149122807017543, "no_speech_prob": 2.058014388239826e-06}, {"id": 424, "seek": 200470, "start": 2012.8400000000001, "end": 2017.6000000000001, "text": " Every time you multiply by 0, you're doing nothing at all, but you're doing it very slowly.", "tokens": [2048, 565, 291, 12972, 538, 1958, 11, 291, 434, 884, 1825, 412, 439, 11, 457, 291, 434, 884, 309, 588, 5692, 13], "temperature": 0.0, "avg_logprob": -0.14426801789481686, "compression_ratio": 1.7149122807017543, "no_speech_prob": 2.058014388239826e-06}, {"id": 425, "seek": 200470, "start": 2017.6000000000001, "end": 2024.8400000000001, "text": " So rather than multiplying by 0 and then adding up the 1, 1 that you have, a much faster way", "tokens": [407, 2831, 813, 30955, 538, 1958, 293, 550, 5127, 493, 264, 502, 11, 502, 300, 291, 362, 11, 257, 709, 4663, 636], "temperature": 0.0, "avg_logprob": -0.14426801789481686, "compression_ratio": 1.7149122807017543, "no_speech_prob": 2.058014388239826e-06}, {"id": 426, "seek": 200470, "start": 2024.8400000000001, "end": 2031.44, "text": " as we know to multiply by a one hot encoded thing is to first of all simply say, what's", "tokens": [382, 321, 458, 281, 12972, 538, 257, 472, 2368, 2058, 12340, 551, 307, 281, 700, 295, 439, 2935, 584, 11, 437, 311], "temperature": 0.0, "avg_logprob": -0.14426801789481686, "compression_ratio": 1.7149122807017543, "no_speech_prob": 2.058014388239826e-06}, {"id": 427, "seek": 200470, "start": 2031.44, "end": 2034.28, "text": " the location of the 1 here?", "tokens": [264, 4914, 295, 264, 502, 510, 30], "temperature": 0.0, "avg_logprob": -0.14426801789481686, "compression_ratio": 1.7149122807017543, "no_speech_prob": 2.058014388239826e-06}, {"id": 428, "seek": 203428, "start": 2034.28, "end": 2035.8, "text": " So in this case, it's location 2.", "tokens": [407, 294, 341, 1389, 11, 309, 311, 4914, 568, 13], "temperature": 0.0, "avg_logprob": -0.22054140495531488, "compression_ratio": 1.61244019138756, "no_speech_prob": 8.939286999520846e-06}, {"id": 429, "seek": 203428, "start": 2035.8, "end": 2040.68, "text": " In this case, it's location 1, if we index from 1.", "tokens": [682, 341, 1389, 11, 309, 311, 4914, 502, 11, 498, 321, 8186, 490, 502, 13], "temperature": 0.0, "avg_logprob": -0.22054140495531488, "compression_ratio": 1.61244019138756, "no_speech_prob": 8.939286999520846e-06}, {"id": 430, "seek": 203428, "start": 2040.68, "end": 2046.8, "text": " And then we just look up into our array of probabilities directly offset by this amount.", "tokens": [400, 550, 321, 445, 574, 493, 666, 527, 10225, 295, 33783, 3838, 18687, 538, 341, 2372, 13], "temperature": 0.0, "avg_logprob": -0.22054140495531488, "compression_ratio": 1.61244019138756, "no_speech_prob": 8.939286999520846e-06}, {"id": 431, "seek": 203428, "start": 2046.8, "end": 2054.0, "text": " Or to put it in math terms, for one hot encoded x's, the above is simply log of p i, where", "tokens": [1610, 281, 829, 309, 294, 5221, 2115, 11, 337, 472, 2368, 2058, 12340, 2031, 311, 11, 264, 3673, 307, 2935, 3565, 295, 280, 741, 11, 689], "temperature": 0.0, "avg_logprob": -0.22054140495531488, "compression_ratio": 1.61244019138756, "no_speech_prob": 8.939286999520846e-06}, {"id": 432, "seek": 203428, "start": 2054.0, "end": 2057.92, "text": " i is the index of our prediction.", "tokens": [741, 307, 264, 8186, 295, 527, 17630, 13], "temperature": 0.0, "avg_logprob": -0.22054140495531488, "compression_ratio": 1.61244019138756, "no_speech_prob": 8.939286999520846e-06}, {"id": 433, "seek": 203428, "start": 2057.92, "end": 2062.72, "text": " Sorry, not our prediction, the actual.", "tokens": [4919, 11, 406, 527, 17630, 11, 264, 3539, 13], "temperature": 0.0, "avg_logprob": -0.22054140495531488, "compression_ratio": 1.61244019138756, "no_speech_prob": 8.939286999520846e-06}, {"id": 434, "seek": 206272, "start": 2062.72, "end": 2066.3999999999996, "text": " So the index into here of the actual.", "tokens": [407, 264, 8186, 666, 510, 295, 264, 3539, 13], "temperature": 0.0, "avg_logprob": -0.1559737812389027, "compression_ratio": 1.6375545851528384, "no_speech_prob": 4.425405677466188e-06}, {"id": 435, "seek": 206272, "start": 2066.3999999999996, "end": 2071.2, "text": " So how do we write this in PyTorch?", "tokens": [407, 577, 360, 321, 2464, 341, 294, 9953, 51, 284, 339, 30], "temperature": 0.0, "avg_logprob": -0.1559737812389027, "compression_ratio": 1.6375545851528384, "no_speech_prob": 4.425405677466188e-06}, {"id": 436, "seek": 206272, "start": 2071.2, "end": 2075.9599999999996, "text": " And I'll show you a really cool trick.", "tokens": [400, 286, 603, 855, 291, 257, 534, 1627, 4282, 13], "temperature": 0.0, "avg_logprob": -0.1559737812389027, "compression_ratio": 1.6375545851528384, "no_speech_prob": 4.425405677466188e-06}, {"id": 437, "seek": 206272, "start": 2075.9599999999996, "end": 2077.3599999999997, "text": " This is what we're going to end up with.", "tokens": [639, 307, 437, 321, 434, 516, 281, 917, 493, 365, 13], "temperature": 0.0, "avg_logprob": -0.1559737812389027, "compression_ratio": 1.6375545851528384, "no_speech_prob": 4.425405677466188e-06}, {"id": 438, "seek": 206272, "start": 2077.3599999999997, "end": 2081.64, "text": " This is our negative log likelihood implementation.", "tokens": [639, 307, 527, 3671, 3565, 22119, 11420, 13], "temperature": 0.0, "avg_logprob": -0.1559737812389027, "compression_ratio": 1.6375545851528384, "no_speech_prob": 4.425405677466188e-06}, {"id": 439, "seek": 206272, "start": 2081.64, "end": 2084.0, "text": " And it's incredibly fast and it's incredibly concise.", "tokens": [400, 309, 311, 6252, 2370, 293, 309, 311, 6252, 44882, 13], "temperature": 0.0, "avg_logprob": -0.1559737812389027, "compression_ratio": 1.6375545851528384, "no_speech_prob": 4.425405677466188e-06}, {"id": 440, "seek": 206272, "start": 2084.0, "end": 2086.3999999999996, "text": " And I'll show you how we do it.", "tokens": [400, 286, 603, 855, 291, 577, 321, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.1559737812389027, "compression_ratio": 1.6375545851528384, "no_speech_prob": 4.425405677466188e-06}, {"id": 441, "seek": 206272, "start": 2086.3999999999996, "end": 2089.6, "text": " Let's look at our dependent variable.", "tokens": [961, 311, 574, 412, 527, 12334, 7006, 13], "temperature": 0.0, "avg_logprob": -0.1559737812389027, "compression_ratio": 1.6375545851528384, "no_speech_prob": 4.425405677466188e-06}, {"id": 442, "seek": 206272, "start": 2089.6, "end": 2091.3199999999997, "text": " So let's just look at the first three values.", "tokens": [407, 718, 311, 445, 574, 412, 264, 700, 1045, 4190, 13], "temperature": 0.0, "avg_logprob": -0.1559737812389027, "compression_ratio": 1.6375545851528384, "no_speech_prob": 4.425405677466188e-06}, {"id": 443, "seek": 209132, "start": 2091.32, "end": 2094.28, "text": " So 5, 0, 4.", "tokens": [407, 1025, 11, 1958, 11, 1017, 13], "temperature": 0.0, "avg_logprob": -0.18044493649456952, "compression_ratio": 1.5393939393939393, "no_speech_prob": 7.646427548024803e-06}, {"id": 444, "seek": 209132, "start": 2094.28, "end": 2099.4, "text": " So that's the first three elements of the dependent variable.", "tokens": [407, 300, 311, 264, 700, 1045, 4959, 295, 264, 12334, 7006, 13], "temperature": 0.0, "avg_logprob": -0.18044493649456952, "compression_ratio": 1.5393939393939393, "no_speech_prob": 7.646427548024803e-06}, {"id": 445, "seek": 209132, "start": 2099.4, "end": 2107.04, "text": " And so what we want to do is we want to find what is the probability associated with 5", "tokens": [400, 370, 437, 321, 528, 281, 360, 307, 321, 528, 281, 915, 437, 307, 264, 8482, 6615, 365, 1025], "temperature": 0.0, "avg_logprob": -0.18044493649456952, "compression_ratio": 1.5393939393939393, "no_speech_prob": 7.646427548024803e-06}, {"id": 446, "seek": 209132, "start": 2107.04, "end": 2111.5, "text": " in our predictions and with 0 and with 4.", "tokens": [294, 527, 21264, 293, 365, 1958, 293, 365, 1017, 13], "temperature": 0.0, "avg_logprob": -0.18044493649456952, "compression_ratio": 1.5393939393939393, "no_speech_prob": 7.646427548024803e-06}, {"id": 447, "seek": 209132, "start": 2111.5, "end": 2120.96, "text": " So our softmax predictions, remember, 50,000 by 10.", "tokens": [407, 527, 2787, 41167, 21264, 11, 1604, 11, 2625, 11, 1360, 538, 1266, 13], "temperature": 0.0, "avg_logprob": -0.18044493649456952, "compression_ratio": 1.5393939393939393, "no_speech_prob": 7.646427548024803e-06}, {"id": 448, "seek": 212096, "start": 2120.96, "end": 2128.08, "text": " Okay, and so if we take the very first of those, they're all there.", "tokens": [1033, 11, 293, 370, 498, 321, 747, 264, 588, 700, 295, 729, 11, 436, 434, 439, 456, 13], "temperature": 0.0, "avg_logprob": -0.22454858182081536, "compression_ratio": 1.3741496598639455, "no_speech_prob": 8.267101293313317e-06}, {"id": 449, "seek": 212096, "start": 2128.08, "end": 2136.2, "text": " And so it said that the actual answer should be 5.", "tokens": [400, 370, 309, 848, 300, 264, 3539, 1867, 820, 312, 1025, 13], "temperature": 0.0, "avg_logprob": -0.22454858182081536, "compression_ratio": 1.3741496598639455, "no_speech_prob": 8.267101293313317e-06}, {"id": 450, "seek": 212096, "start": 2136.2, "end": 2147.76, "text": " So if we go into this 0, 1, 2, 3, 4, 5, that's the answer that we're going to want.", "tokens": [407, 498, 321, 352, 666, 341, 1958, 11, 502, 11, 568, 11, 805, 11, 1017, 11, 1025, 11, 300, 311, 264, 1867, 300, 321, 434, 516, 281, 528, 13], "temperature": 0.0, "avg_logprob": -0.22454858182081536, "compression_ratio": 1.3741496598639455, "no_speech_prob": 8.267101293313317e-06}, {"id": 451, "seek": 214776, "start": 2147.76, "end": 2155.32, "text": " Okay, so here's how we can grab all three of those at once.", "tokens": [1033, 11, 370, 510, 311, 577, 321, 393, 4444, 439, 1045, 295, 729, 412, 1564, 13], "temperature": 0.0, "avg_logprob": -0.09314612720323645, "compression_ratio": 1.4401913875598087, "no_speech_prob": 4.22279117628932e-06}, {"id": 452, "seek": 214776, "start": 2155.32, "end": 2162.48, "text": " We can index into our array with the whole thing, 5, 0, 4.", "tokens": [492, 393, 8186, 666, 527, 10225, 365, 264, 1379, 551, 11, 1025, 11, 1958, 11, 1017, 13], "temperature": 0.0, "avg_logprob": -0.09314612720323645, "compression_ratio": 1.4401913875598087, "no_speech_prob": 4.22279117628932e-06}, {"id": 453, "seek": 214776, "start": 2162.48, "end": 2169.44, "text": " And then for the first bit, we pass in just the contiguous integers, 0, 1, 2.", "tokens": [400, 550, 337, 264, 700, 857, 11, 321, 1320, 294, 445, 264, 660, 30525, 41674, 11, 1958, 11, 502, 11, 568, 13], "temperature": 0.0, "avg_logprob": -0.09314612720323645, "compression_ratio": 1.4401913875598087, "no_speech_prob": 4.22279117628932e-06}, {"id": 454, "seek": 214776, "start": 2169.44, "end": 2170.76, "text": " Why does this work?", "tokens": [1545, 775, 341, 589, 30], "temperature": 0.0, "avg_logprob": -0.09314612720323645, "compression_ratio": 1.4401913875598087, "no_speech_prob": 4.22279117628932e-06}, {"id": 455, "seek": 214776, "start": 2170.76, "end": 2177.6400000000003, "text": " This works because PyTorch supports all of the advanced indexing support from NumPy.", "tokens": [639, 1985, 570, 9953, 51, 284, 339, 9346, 439, 295, 264, 7339, 8186, 278, 1406, 490, 22592, 47, 88, 13], "temperature": 0.0, "avg_logprob": -0.09314612720323645, "compression_ratio": 1.4401913875598087, "no_speech_prob": 4.22279117628932e-06}, {"id": 456, "seek": 217764, "start": 2177.64, "end": 2184.24, "text": " And so if you click on this link, one of the many things that types of indexing that NumPy", "tokens": [400, 370, 498, 291, 2052, 322, 341, 2113, 11, 472, 295, 264, 867, 721, 300, 3467, 295, 8186, 278, 300, 22592, 47, 88], "temperature": 0.0, "avg_logprob": -0.08755978222551017, "compression_ratio": 1.8214285714285714, "no_speech_prob": 5.014680027670693e-06}, {"id": 457, "seek": 217764, "start": 2184.24, "end": 2188.0, "text": " and therefore PyTorch supports is integer array indexing.", "tokens": [293, 4412, 9953, 51, 284, 339, 9346, 307, 24922, 10225, 8186, 278, 13], "temperature": 0.0, "avg_logprob": -0.08755978222551017, "compression_ratio": 1.8214285714285714, "no_speech_prob": 5.014680027670693e-06}, {"id": 458, "seek": 217764, "start": 2188.0, "end": 2195.04, "text": " And what this is is that you pass a list for each dimension.", "tokens": [400, 437, 341, 307, 307, 300, 291, 1320, 257, 1329, 337, 1184, 10139, 13], "temperature": 0.0, "avg_logprob": -0.08755978222551017, "compression_ratio": 1.8214285714285714, "no_speech_prob": 5.014680027670693e-06}, {"id": 459, "seek": 217764, "start": 2195.04, "end": 2198.56, "text": " So in this case, we have two dimensions.", "tokens": [407, 294, 341, 1389, 11, 321, 362, 732, 12819, 13], "temperature": 0.0, "avg_logprob": -0.08755978222551017, "compression_ratio": 1.8214285714285714, "no_speech_prob": 5.014680027670693e-06}, {"id": 460, "seek": 217764, "start": 2198.56, "end": 2200.48, "text": " So we need to pass two lists.", "tokens": [407, 321, 643, 281, 1320, 732, 14511, 13], "temperature": 0.0, "avg_logprob": -0.08755978222551017, "compression_ratio": 1.8214285714285714, "no_speech_prob": 5.014680027670693e-06}, {"id": 461, "seek": 217764, "start": 2200.48, "end": 2204.02, "text": " And the first is the list of all of the row indexes you want.", "tokens": [400, 264, 700, 307, 264, 1329, 295, 439, 295, 264, 5386, 8186, 279, 291, 528, 13], "temperature": 0.0, "avg_logprob": -0.08755978222551017, "compression_ratio": 1.8214285714285714, "no_speech_prob": 5.014680027670693e-06}, {"id": 462, "seek": 217764, "start": 2204.02, "end": 2207.0, "text": " And the second is the list of all of the column indexes you want.", "tokens": [400, 264, 1150, 307, 264, 1329, 295, 439, 295, 264, 7738, 8186, 279, 291, 528, 13], "temperature": 0.0, "avg_logprob": -0.08755978222551017, "compression_ratio": 1.8214285714285714, "no_speech_prob": 5.014680027670693e-06}, {"id": 463, "seek": 220700, "start": 2207.0, "end": 2216.52, "text": " So this is going to end up returning 0, 5, 1, 0, and 2, 4, which is the exact numbers", "tokens": [407, 341, 307, 516, 281, 917, 493, 12678, 1958, 11, 1025, 11, 502, 11, 1958, 11, 293, 568, 11, 1017, 11, 597, 307, 264, 1900, 3547], "temperature": 0.0, "avg_logprob": -0.0671944949362013, "compression_ratio": 1.4743589743589745, "no_speech_prob": 6.144115104689263e-06}, {"id": 464, "seek": 220700, "start": 2216.52, "end": 2217.52, "text": " that we wanted.", "tokens": [300, 321, 1415, 13], "temperature": 0.0, "avg_logprob": -0.0671944949362013, "compression_ratio": 1.4743589743589745, "no_speech_prob": 6.144115104689263e-06}, {"id": 465, "seek": 220700, "start": 2217.52, "end": 2222.16, "text": " So for example, 0, 5 is minus 2.49.", "tokens": [407, 337, 1365, 11, 1958, 11, 1025, 307, 3175, 568, 13, 14938, 13], "temperature": 0.0, "avg_logprob": -0.0671944949362013, "compression_ratio": 1.4743589743589745, "no_speech_prob": 6.144115104689263e-06}, {"id": 466, "seek": 220700, "start": 2222.16, "end": 2231.68, "text": " So to grab the entire list of the exact things that we want for our negative log likelihood,", "tokens": [407, 281, 4444, 264, 2302, 1329, 295, 264, 1900, 721, 300, 321, 528, 337, 527, 3671, 3565, 22119, 11], "temperature": 0.0, "avg_logprob": -0.0671944949362013, "compression_ratio": 1.4743589743589745, "no_speech_prob": 6.144115104689263e-06}, {"id": 467, "seek": 223168, "start": 2231.68, "end": 2237.8399999999997, "text": " and we basically say, OK, let's look in our predictions.", "tokens": [293, 321, 1936, 584, 11, 2264, 11, 718, 311, 574, 294, 527, 21264, 13], "temperature": 0.0, "avg_logprob": -0.10905861627487909, "compression_ratio": 1.6378378378378378, "no_speech_prob": 2.1907189875491895e-06}, {"id": 468, "seek": 223168, "start": 2237.8399999999997, "end": 2242.9199999999996, "text": " And then for our row indexes, it's every single row index.", "tokens": [400, 550, 337, 527, 5386, 8186, 279, 11, 309, 311, 633, 2167, 5386, 8186, 13], "temperature": 0.0, "avg_logprob": -0.10905861627487909, "compression_ratio": 1.6378378378378378, "no_speech_prob": 2.1907189875491895e-06}, {"id": 469, "seek": 223168, "start": 2242.9199999999996, "end": 2245.68, "text": " So range of target.shape0.", "tokens": [407, 3613, 295, 3779, 13, 82, 42406, 15, 13], "temperature": 0.0, "avg_logprob": -0.10905861627487909, "compression_ratio": 1.6378378378378378, "no_speech_prob": 2.1907189875491895e-06}, {"id": 470, "seek": 223168, "start": 2245.68, "end": 2249.18, "text": " So target.shape0 is the number of rows.", "tokens": [407, 3779, 13, 82, 42406, 15, 307, 264, 1230, 295, 13241, 13], "temperature": 0.0, "avg_logprob": -0.10905861627487909, "compression_ratio": 1.6378378378378378, "no_speech_prob": 2.1907189875491895e-06}, {"id": 471, "seek": 223168, "start": 2249.18, "end": 2253.08, "text": " So range of that is all of the numbers from 0 to the number of rows.", "tokens": [407, 3613, 295, 300, 307, 439, 295, 264, 3547, 490, 1958, 281, 264, 1230, 295, 13241, 13], "temperature": 0.0, "avg_logprob": -0.10905861627487909, "compression_ratio": 1.6378378378378378, "no_speech_prob": 2.1907189875491895e-06}, {"id": 472, "seek": 223168, "start": 2253.08, "end": 2258.24, "text": " So 0, 1, 2, 3, blah, blah, blah, 50,000, or 49,999.", "tokens": [407, 1958, 11, 502, 11, 568, 11, 805, 11, 12288, 11, 12288, 11, 12288, 11, 2625, 11, 1360, 11, 420, 16513, 11, 49017, 13], "temperature": 0.0, "avg_logprob": -0.10905861627487909, "compression_ratio": 1.6378378378378378, "no_speech_prob": 2.1907189875491895e-06}, {"id": 473, "seek": 225824, "start": 2258.24, "end": 2261.7999999999997, "text": " And then which columns do we want for each of those rows?", "tokens": [400, 550, 597, 13766, 360, 321, 528, 337, 1184, 295, 729, 13241, 30], "temperature": 0.0, "avg_logprob": -0.10679592859177363, "compression_ratio": 1.6283185840707965, "no_speech_prob": 2.2958952285989653e-06}, {"id": 474, "seek": 225824, "start": 2261.7999999999997, "end": 2266.0, "text": " Well, whatever our target is, whatever the actual value.", "tokens": [1042, 11, 2035, 527, 3779, 307, 11, 2035, 264, 3539, 2158, 13], "temperature": 0.0, "avg_logprob": -0.10679592859177363, "compression_ratio": 1.6283185840707965, "no_speech_prob": 2.2958952285989653e-06}, {"id": 475, "seek": 225824, "start": 2266.0, "end": 2269.66, "text": " So in this case, 5, 0, 4, et cetera.", "tokens": [407, 294, 341, 1389, 11, 1025, 11, 1958, 11, 1017, 11, 1030, 11458, 13], "temperature": 0.0, "avg_logprob": -0.10679592859177363, "compression_ratio": 1.6283185840707965, "no_speech_prob": 2.2958952285989653e-06}, {"id": 476, "seek": 225824, "start": 2269.66, "end": 2274.24, "text": " So that returns all of the values we need.", "tokens": [407, 300, 11247, 439, 295, 264, 4190, 321, 643, 13], "temperature": 0.0, "avg_logprob": -0.10679592859177363, "compression_ratio": 1.6283185840707965, "no_speech_prob": 2.2958952285989653e-06}, {"id": 477, "seek": 225824, "start": 2274.24, "end": 2277.8399999999997, "text": " We then take minus, because it's negative log likelihood, and take them in.", "tokens": [492, 550, 747, 3175, 11, 570, 309, 311, 3671, 3565, 22119, 11, 293, 747, 552, 294, 13], "temperature": 0.0, "avg_logprob": -0.10679592859177363, "compression_ratio": 1.6283185840707965, "no_speech_prob": 2.2958952285989653e-06}, {"id": 478, "seek": 225824, "start": 2277.8399999999997, "end": 2285.0, "text": " So that's all it takes to do negative log likelihood in PyTorch, which is super wonderfully", "tokens": [407, 300, 311, 439, 309, 2516, 281, 360, 3671, 3565, 22119, 294, 9953, 51, 284, 339, 11, 597, 307, 1687, 38917], "temperature": 0.0, "avg_logprob": -0.10679592859177363, "compression_ratio": 1.6283185840707965, "no_speech_prob": 2.2958952285989653e-06}, {"id": 479, "seek": 225824, "start": 2285.0, "end": 2287.3799999999997, "text": " easy.", "tokens": [1858, 13], "temperature": 0.0, "avg_logprob": -0.10679592859177363, "compression_ratio": 1.6283185840707965, "no_speech_prob": 2.2958952285989653e-06}, {"id": 480, "seek": 228738, "start": 2287.38, "end": 2292.28, "text": " So now we can calculate our loss, which is the negative log likelihood of the softmax", "tokens": [407, 586, 321, 393, 8873, 527, 4470, 11, 597, 307, 264, 3671, 3565, 22119, 295, 264, 2787, 41167], "temperature": 0.0, "avg_logprob": -0.14970259348551432, "compression_ratio": 1.4971098265895955, "no_speech_prob": 2.3687871362199076e-06}, {"id": 481, "seek": 228738, "start": 2292.28, "end": 2294.28, "text": " predictions.", "tokens": [21264, 13], "temperature": 0.0, "avg_logprob": -0.14970259348551432, "compression_ratio": 1.4971098265895955, "no_speech_prob": 2.3687871362199076e-06}, {"id": 482, "seek": 228738, "start": 2294.28, "end": 2301.3, "text": " That's what we had up here compared to our actual y trading.", "tokens": [663, 311, 437, 321, 632, 493, 510, 5347, 281, 527, 3539, 288, 9529, 13], "temperature": 0.0, "avg_logprob": -0.14970259348551432, "compression_ratio": 1.4971098265895955, "no_speech_prob": 2.3687871362199076e-06}, {"id": 483, "seek": 228738, "start": 2301.3, "end": 2305.92, "text": " And so there it is.", "tokens": [400, 370, 456, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.14970259348551432, "compression_ratio": 1.4971098265895955, "no_speech_prob": 2.3687871362199076e-06}, {"id": 484, "seek": 228738, "start": 2305.92, "end": 2316.46, "text": " Now this was our softmax formula, which is e to the x over sum of e to the x's.", "tokens": [823, 341, 390, 527, 2787, 41167, 8513, 11, 597, 307, 308, 281, 264, 2031, 670, 2408, 295, 308, 281, 264, 2031, 311, 13], "temperature": 0.0, "avg_logprob": -0.14970259348551432, "compression_ratio": 1.4971098265895955, "no_speech_prob": 2.3687871362199076e-06}, {"id": 485, "seek": 231646, "start": 2316.46, "end": 2319.0, "text": " So we have a, and then it's all logged.", "tokens": [407, 321, 362, 257, 11, 293, 550, 309, 311, 439, 27231, 13], "temperature": 0.0, "avg_logprob": -0.06830274081621014, "compression_ratio": 1.8409090909090908, "no_speech_prob": 4.157327566645108e-06}, {"id": 486, "seek": 231646, "start": 2319.0, "end": 2321.02, "text": " So we've got a log of a over b.", "tokens": [407, 321, 600, 658, 257, 3565, 295, 257, 670, 272, 13], "temperature": 0.0, "avg_logprob": -0.06830274081621014, "compression_ratio": 1.8409090909090908, "no_speech_prob": 4.157327566645108e-06}, {"id": 487, "seek": 231646, "start": 2321.02, "end": 2324.16, "text": " And remember, I keep telling you that one thing you want to remember from high school", "tokens": [400, 1604, 11, 286, 1066, 3585, 291, 300, 472, 551, 291, 528, 281, 1604, 490, 1090, 1395], "temperature": 0.0, "avg_logprob": -0.06830274081621014, "compression_ratio": 1.8409090909090908, "no_speech_prob": 4.157327566645108e-06}, {"id": 488, "seek": 231646, "start": 2324.16, "end": 2325.76, "text": " math is how logs work.", "tokens": [5221, 307, 577, 20820, 589, 13], "temperature": 0.0, "avg_logprob": -0.06830274081621014, "compression_ratio": 1.8409090909090908, "no_speech_prob": 4.157327566645108e-06}, {"id": 489, "seek": 231646, "start": 2325.76, "end": 2331.6, "text": " So I do want you to try to recall that log of a over b is log of a minus log of b.", "tokens": [407, 286, 360, 528, 291, 281, 853, 281, 9901, 300, 3565, 295, 257, 670, 272, 307, 3565, 295, 257, 3175, 3565, 295, 272, 13], "temperature": 0.0, "avg_logprob": -0.06830274081621014, "compression_ratio": 1.8409090909090908, "no_speech_prob": 4.157327566645108e-06}, {"id": 490, "seek": 231646, "start": 2331.6, "end": 2341.12, "text": " And so we can rewrite this as log of e to the x minus log of all that.", "tokens": [400, 370, 321, 393, 28132, 341, 382, 3565, 295, 308, 281, 264, 2031, 3175, 3565, 295, 439, 300, 13], "temperature": 0.0, "avg_logprob": -0.06830274081621014, "compression_ratio": 1.8409090909090908, "no_speech_prob": 4.157327566645108e-06}, {"id": 491, "seek": 231646, "start": 2341.12, "end": 2345.36, "text": " And of course, e to the something and log are opposites of each other.", "tokens": [400, 295, 1164, 11, 308, 281, 264, 746, 293, 3565, 366, 4665, 3324, 295, 1184, 661, 13], "temperature": 0.0, "avg_logprob": -0.06830274081621014, "compression_ratio": 1.8409090909090908, "no_speech_prob": 4.157327566645108e-06}, {"id": 492, "seek": 234536, "start": 2345.36, "end": 2348.6, "text": " So log of e to the x is just x.", "tokens": [407, 3565, 295, 308, 281, 264, 2031, 307, 445, 2031, 13], "temperature": 0.0, "avg_logprob": -0.14381326708877296, "compression_ratio": 1.7395348837209301, "no_speech_prob": 5.338109531294322e-06}, {"id": 493, "seek": 234536, "start": 2348.6, "end": 2356.08, "text": " So that ends up being x minus x dot x dot sum dot log.", "tokens": [407, 300, 5314, 493, 885, 2031, 3175, 2031, 5893, 2031, 5893, 2408, 5893, 3565, 13], "temperature": 0.0, "avg_logprob": -0.14381326708877296, "compression_ratio": 1.7395348837209301, "no_speech_prob": 5.338109531294322e-06}, {"id": 494, "seek": 234536, "start": 2356.08, "end": 2358.8, "text": " So this is useful.", "tokens": [407, 341, 307, 4420, 13], "temperature": 0.0, "avg_logprob": -0.14381326708877296, "compression_ratio": 1.7395348837209301, "no_speech_prob": 5.338109531294322e-06}, {"id": 495, "seek": 234536, "start": 2358.8, "end": 2360.76, "text": " And let's just check that that actually works.", "tokens": [400, 718, 311, 445, 1520, 300, 300, 767, 1985, 13], "temperature": 0.0, "avg_logprob": -0.14381326708877296, "compression_ratio": 1.7395348837209301, "no_speech_prob": 5.338109531294322e-06}, {"id": 496, "seek": 234536, "start": 2360.76, "end": 2365.6800000000003, "text": " So as I kind of keep refactoring these things, as even as I'd like to be, these mathematical", "tokens": [407, 382, 286, 733, 295, 1066, 1895, 578, 3662, 613, 721, 11, 382, 754, 382, 286, 1116, 411, 281, 312, 11, 613, 18894], "temperature": 0.0, "avg_logprob": -0.14381326708877296, "compression_ratio": 1.7395348837209301, "no_speech_prob": 5.338109531294322e-06}, {"id": 497, "seek": 234536, "start": 2365.6800000000003, "end": 2367.32, "text": " manipulations are just refactoring.", "tokens": [9258, 4136, 366, 445, 1895, 578, 3662, 13], "temperature": 0.0, "avg_logprob": -0.14381326708877296, "compression_ratio": 1.7395348837209301, "no_speech_prob": 5.338109531294322e-06}, {"id": 498, "seek": 234536, "start": 2367.32, "end": 2369.2400000000002, "text": " So just refactoring the math.", "tokens": [407, 445, 1895, 578, 3662, 264, 5221, 13], "temperature": 0.0, "avg_logprob": -0.14381326708877296, "compression_ratio": 1.7395348837209301, "no_speech_prob": 5.338109531294322e-06}, {"id": 499, "seek": 234536, "start": 2369.2400000000002, "end": 2370.32, "text": " So you keep checking along.", "tokens": [407, 291, 1066, 8568, 2051, 13], "temperature": 0.0, "avg_logprob": -0.14381326708877296, "compression_ratio": 1.7395348837209301, "no_speech_prob": 5.338109531294322e-06}, {"id": 500, "seek": 234536, "start": 2370.32, "end": 2372.32, "text": " So we created test near last time.", "tokens": [407, 321, 2942, 1500, 2651, 1036, 565, 13], "temperature": 0.0, "avg_logprob": -0.14381326708877296, "compression_ratio": 1.7395348837209301, "no_speech_prob": 5.338109531294322e-06}, {"id": 501, "seek": 237232, "start": 2372.32, "end": 2377.92, "text": " So let's use it to make sure that it's the same as our loss.", "tokens": [407, 718, 311, 764, 309, 281, 652, 988, 300, 309, 311, 264, 912, 382, 527, 4470, 13], "temperature": 0.0, "avg_logprob": -0.12432194933479215, "compression_ratio": 1.5245901639344261, "no_speech_prob": 2.5215492769348202e-06}, {"id": 502, "seek": 237232, "start": 2377.92, "end": 2387.96, "text": " Now you'll see here this is taking the log of the sum of the x.", "tokens": [823, 291, 603, 536, 510, 341, 307, 1940, 264, 3565, 295, 264, 2408, 295, 264, 2031, 13], "temperature": 0.0, "avg_logprob": -0.12432194933479215, "compression_ratio": 1.5245901639344261, "no_speech_prob": 2.5215492769348202e-06}, {"id": 503, "seek": 237232, "start": 2387.96, "end": 2391.56, "text": " And there's a trick called log sum x.", "tokens": [400, 456, 311, 257, 4282, 1219, 3565, 2408, 2031, 13], "temperature": 0.0, "avg_logprob": -0.12432194933479215, "compression_ratio": 1.5245901639344261, "no_speech_prob": 2.5215492769348202e-06}, {"id": 504, "seek": 237232, "start": 2391.56, "end": 2397.0, "text": " The reason we need this trick is that when you go e to the power of something, you can", "tokens": [440, 1778, 321, 643, 341, 4282, 307, 300, 562, 291, 352, 308, 281, 264, 1347, 295, 746, 11, 291, 393], "temperature": 0.0, "avg_logprob": -0.12432194933479215, "compression_ratio": 1.5245901639344261, "no_speech_prob": 2.5215492769348202e-06}, {"id": 505, "seek": 237232, "start": 2397.0, "end": 2399.36, "text": " get ridiculously big numbers.", "tokens": [483, 41358, 955, 3547, 13], "temperature": 0.0, "avg_logprob": -0.12432194933479215, "compression_ratio": 1.5245901639344261, "no_speech_prob": 2.5215492769348202e-06}, {"id": 506, "seek": 239936, "start": 2399.36, "end": 2404.6800000000003, "text": " And if you've done Rachel's computational linear algebra course, then you'll know that", "tokens": [400, 498, 291, 600, 1096, 14246, 311, 28270, 8213, 21989, 1164, 11, 550, 291, 603, 458, 300], "temperature": 0.0, "avg_logprob": -0.15312712071305615, "compression_ratio": 1.707142857142857, "no_speech_prob": 9.516089448879939e-06}, {"id": 507, "seek": 239936, "start": 2404.6800000000003, "end": 2409.2000000000003, "text": " very big numbers in floating point on a computer are really inaccurate.", "tokens": [588, 955, 3547, 294, 12607, 935, 322, 257, 3820, 366, 534, 46443, 13], "temperature": 0.0, "avg_logprob": -0.15312712071305615, "compression_ratio": 1.707142857142857, "no_speech_prob": 9.516089448879939e-06}, {"id": 508, "seek": 239936, "start": 2409.2000000000003, "end": 2414.0, "text": " Basically, the further you get away from zero, the less kind of fine grained they are.", "tokens": [8537, 11, 264, 3052, 291, 483, 1314, 490, 4018, 11, 264, 1570, 733, 295, 2489, 1295, 2001, 436, 366, 13], "temperature": 0.0, "avg_logprob": -0.15312712071305615, "compression_ratio": 1.707142857142857, "no_speech_prob": 9.516089448879939e-06}, {"id": 509, "seek": 239936, "start": 2414.0, "end": 2418.28, "text": " It gets to the point where two numbers, a thousand apart, the computer thinks they're", "tokens": [467, 2170, 281, 264, 935, 689, 732, 3547, 11, 257, 4714, 4936, 11, 264, 3820, 7309, 436, 434], "temperature": 0.0, "avg_logprob": -0.15312712071305615, "compression_ratio": 1.707142857142857, "no_speech_prob": 9.516089448879939e-06}, {"id": 510, "seek": 239936, "start": 2418.28, "end": 2419.44, "text": " the same number.", "tokens": [264, 912, 1230, 13], "temperature": 0.0, "avg_logprob": -0.15312712071305615, "compression_ratio": 1.707142857142857, "no_speech_prob": 9.516089448879939e-06}, {"id": 511, "seek": 239936, "start": 2419.44, "end": 2424.1600000000003, "text": " So you don't want big numbers, particularly when you're calculating gradients.", "tokens": [407, 291, 500, 380, 528, 955, 3547, 11, 4098, 562, 291, 434, 28258, 2771, 2448, 13], "temperature": 0.0, "avg_logprob": -0.15312712071305615, "compression_ratio": 1.707142857142857, "no_speech_prob": 9.516089448879939e-06}, {"id": 512, "seek": 239936, "start": 2424.1600000000003, "end": 2427.7400000000002, "text": " So anywhere you see an e to the x, we get nervous.", "tokens": [407, 4992, 291, 536, 364, 308, 281, 264, 2031, 11, 321, 483, 6296, 13], "temperature": 0.0, "avg_logprob": -0.15312712071305615, "compression_ratio": 1.707142857142857, "no_speech_prob": 9.516089448879939e-06}, {"id": 513, "seek": 242774, "start": 2427.74, "end": 2430.0, "text": " We don't want x to be big.", "tokens": [492, 500, 380, 528, 2031, 281, 312, 955, 13], "temperature": 0.0, "avg_logprob": -0.09264208606837951, "compression_ratio": 1.6930232558139535, "no_speech_prob": 2.8408530852175318e-05}, {"id": 514, "seek": 242774, "start": 2430.0, "end": 2437.24, "text": " But it turns out that if you do this little mathematical substitution, you can actually", "tokens": [583, 309, 4523, 484, 300, 498, 291, 360, 341, 707, 18894, 35827, 11, 291, 393, 767], "temperature": 0.0, "avg_logprob": -0.09264208606837951, "compression_ratio": 1.6930232558139535, "no_speech_prob": 2.8408530852175318e-05}, {"id": 515, "seek": 242774, "start": 2437.24, "end": 2444.64, "text": " subtract a number from your x's and add them back at the front, and you get the same answer.", "tokens": [16390, 257, 1230, 490, 428, 2031, 311, 293, 909, 552, 646, 412, 264, 1868, 11, 293, 291, 483, 264, 912, 1867, 13], "temperature": 0.0, "avg_logprob": -0.09264208606837951, "compression_ratio": 1.6930232558139535, "no_speech_prob": 2.8408530852175318e-05}, {"id": 516, "seek": 242774, "start": 2444.64, "end": 2450.52, "text": " So what you could do is you can find the maximum of all of your x's.", "tokens": [407, 437, 291, 727, 360, 307, 291, 393, 915, 264, 6674, 295, 439, 295, 428, 2031, 311, 13], "temperature": 0.0, "avg_logprob": -0.09264208606837951, "compression_ratio": 1.6930232558139535, "no_speech_prob": 2.8408530852175318e-05}, {"id": 517, "seek": 242774, "start": 2450.52, "end": 2456.68, "text": " You can subtract it from all of your x's and then add it back afterwards outside the x.", "tokens": [509, 393, 16390, 309, 490, 439, 295, 428, 2031, 311, 293, 550, 909, 309, 646, 10543, 2380, 264, 2031, 13], "temperature": 0.0, "avg_logprob": -0.09264208606837951, "compression_ratio": 1.6930232558139535, "no_speech_prob": 2.8408530852175318e-05}, {"id": 518, "seek": 245668, "start": 2456.68, "end": 2458.8799999999997, "text": " And you get exactly the same answer.", "tokens": [400, 291, 483, 2293, 264, 912, 1867, 13], "temperature": 0.0, "avg_logprob": -0.11098721197673253, "compression_ratio": 1.7593360995850622, "no_speech_prob": 2.3187079932540655e-05}, {"id": 519, "seek": 245668, "start": 2458.8799999999997, "end": 2462.56, "text": " So in other words, let's find the maximum.", "tokens": [407, 294, 661, 2283, 11, 718, 311, 915, 264, 6674, 13], "temperature": 0.0, "avg_logprob": -0.11098721197673253, "compression_ratio": 1.7593360995850622, "no_speech_prob": 2.3187079932540655e-05}, {"id": 520, "seek": 245668, "start": 2462.56, "end": 2465.3999999999996, "text": " Let's subtract it from all of our x's.", "tokens": [961, 311, 16390, 309, 490, 439, 295, 527, 2031, 311, 13], "temperature": 0.0, "avg_logprob": -0.11098721197673253, "compression_ratio": 1.7593360995850622, "no_speech_prob": 2.3187079932540655e-05}, {"id": 521, "seek": 245668, "start": 2465.3999999999996, "end": 2467.64, "text": " And then let's do log sum exp.", "tokens": [400, 550, 718, 311, 360, 3565, 2408, 1278, 13], "temperature": 0.0, "avg_logprob": -0.11098721197673253, "compression_ratio": 1.7593360995850622, "no_speech_prob": 2.3187079932540655e-05}, {"id": 522, "seek": 245668, "start": 2467.64, "end": 2469.52, "text": " And then at the end, we'll add it back again.", "tokens": [400, 550, 412, 264, 917, 11, 321, 603, 909, 309, 646, 797, 13], "temperature": 0.0, "avg_logprob": -0.11098721197673253, "compression_ratio": 1.7593360995850622, "no_speech_prob": 2.3187079932540655e-05}, {"id": 523, "seek": 245668, "start": 2469.52, "end": 2474.3999999999996, "text": " And that gives you exactly the same number, but without this numerical problem.", "tokens": [400, 300, 2709, 291, 2293, 264, 912, 1230, 11, 457, 1553, 341, 29054, 1154, 13], "temperature": 0.0, "avg_logprob": -0.11098721197673253, "compression_ratio": 1.7593360995850622, "no_speech_prob": 2.3187079932540655e-05}, {"id": 524, "seek": 245668, "start": 2474.3999999999996, "end": 2479.44, "text": " So when people talk about numerical stability tricks, they're talking about stuff like this.", "tokens": [407, 562, 561, 751, 466, 29054, 11826, 11733, 11, 436, 434, 1417, 466, 1507, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.11098721197673253, "compression_ratio": 1.7593360995850622, "no_speech_prob": 2.3187079932540655e-05}, {"id": 525, "seek": 245668, "start": 2479.44, "end": 2483.1, "text": " And this is a really helpful numerical stability trick.", "tokens": [400, 341, 307, 257, 534, 4961, 29054, 11826, 4282, 13], "temperature": 0.0, "avg_logprob": -0.11098721197673253, "compression_ratio": 1.7593360995850622, "no_speech_prob": 2.3187079932540655e-05}, {"id": 526, "seek": 248310, "start": 2483.1, "end": 2486.92, "text": " So this is how you do log sum exp in real life.", "tokens": [407, 341, 307, 577, 291, 360, 3565, 2408, 1278, 294, 957, 993, 13], "temperature": 0.0, "avg_logprob": -0.0845807854455846, "compression_ratio": 1.6502057613168724, "no_speech_prob": 3.2697691494831815e-05}, {"id": 527, "seek": 248310, "start": 2486.92, "end": 2494.24, "text": " We can check that this one here is the same as, and look, in fact, log sum exp is already", "tokens": [492, 393, 1520, 300, 341, 472, 510, 307, 264, 912, 382, 11, 293, 574, 11, 294, 1186, 11, 3565, 2408, 1278, 307, 1217], "temperature": 0.0, "avg_logprob": -0.0845807854455846, "compression_ratio": 1.6502057613168724, "no_speech_prob": 3.2697691494831815e-05}, {"id": 528, "seek": 248310, "start": 2494.24, "end": 2496.2, "text": " a method in PyTorch.", "tokens": [257, 3170, 294, 9953, 51, 284, 339, 13], "temperature": 0.0, "avg_logprob": -0.0845807854455846, "compression_ratio": 1.6502057613168724, "no_speech_prob": 3.2697691494831815e-05}, {"id": 529, "seek": 248310, "start": 2496.2, "end": 2498.56, "text": " It's such an important and useful thing.", "tokens": [467, 311, 1270, 364, 1021, 293, 4420, 551, 13], "temperature": 0.0, "avg_logprob": -0.0845807854455846, "compression_ratio": 1.6502057613168724, "no_speech_prob": 3.2697691494831815e-05}, {"id": 530, "seek": 248310, "start": 2498.56, "end": 2503.52, "text": " You can just actually use PyTorches, and you'll get the same result as the one we just wrote.", "tokens": [509, 393, 445, 767, 764, 9953, 51, 284, 3781, 11, 293, 291, 603, 483, 264, 912, 1874, 382, 264, 472, 321, 445, 4114, 13], "temperature": 0.0, "avg_logprob": -0.0845807854455846, "compression_ratio": 1.6502057613168724, "no_speech_prob": 3.2697691494831815e-05}, {"id": 531, "seek": 248310, "start": 2503.52, "end": 2504.92, "text": " So now we can use it.", "tokens": [407, 586, 321, 393, 764, 309, 13], "temperature": 0.0, "avg_logprob": -0.0845807854455846, "compression_ratio": 1.6502057613168724, "no_speech_prob": 3.2697691494831815e-05}, {"id": 532, "seek": 248310, "start": 2504.92, "end": 2510.16, "text": " So log softmax is now just x minus x.logsumexp.", "tokens": [407, 3565, 2787, 41167, 307, 586, 445, 2031, 3175, 2031, 13, 4987, 82, 449, 15952, 13], "temperature": 0.0, "avg_logprob": -0.0845807854455846, "compression_ratio": 1.6502057613168724, "no_speech_prob": 3.2697691494831815e-05}, {"id": 533, "seek": 248310, "start": 2510.16, "end": 2511.16, "text": " And let's check.", "tokens": [400, 718, 311, 1520, 13], "temperature": 0.0, "avg_logprob": -0.0845807854455846, "compression_ratio": 1.6502057613168724, "no_speech_prob": 3.2697691494831815e-05}, {"id": 534, "seek": 248310, "start": 2511.16, "end": 2512.2, "text": " Yep, still the same.", "tokens": [7010, 11, 920, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.0845807854455846, "compression_ratio": 1.6502057613168724, "no_speech_prob": 3.2697691494831815e-05}, {"id": 535, "seek": 251220, "start": 2512.2, "end": 2517.3199999999997, "text": " So now that that's all working, we may as well just use PyTorch's log softmaxes and", "tokens": [407, 586, 300, 300, 311, 439, 1364, 11, 321, 815, 382, 731, 445, 764, 9953, 51, 284, 339, 311, 3565, 2787, 41167, 279, 293], "temperature": 0.0, "avg_logprob": -0.1614276019009677, "compression_ratio": 1.6798029556650247, "no_speech_prob": 4.936931418342283e-06}, {"id": 536, "seek": 251220, "start": 2517.3199999999997, "end": 2520.7599999999998, "text": " PyTorch's NLL loss.", "tokens": [9953, 51, 284, 339, 311, 426, 24010, 4470, 13], "temperature": 0.0, "avg_logprob": -0.1614276019009677, "compression_ratio": 1.6798029556650247, "no_speech_prob": 4.936931418342283e-06}, {"id": 537, "seek": 251220, "start": 2520.7599999999998, "end": 2529.3399999999997, "text": " But actually, NLL loss of log softmax is called cross entropy.", "tokens": [583, 767, 11, 426, 24010, 4470, 295, 3565, 2787, 41167, 307, 1219, 3278, 30867, 13], "temperature": 0.0, "avg_logprob": -0.1614276019009677, "compression_ratio": 1.6798029556650247, "no_speech_prob": 4.936931418342283e-06}, {"id": 538, "seek": 251220, "start": 2529.3399999999997, "end": 2535.2799999999997, "text": " So finally, we get test near f.crossentropy is the same as loss, and it is.", "tokens": [407, 2721, 11, 321, 483, 1500, 2651, 283, 13, 35418, 317, 27514, 307, 264, 912, 382, 4470, 11, 293, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.1614276019009677, "compression_ratio": 1.6798029556650247, "no_speech_prob": 4.936931418342283e-06}, {"id": 539, "seek": 251220, "start": 2535.2799999999997, "end": 2538.4399999999996, "text": " So we've now recreated PyTorch's cross entropy.", "tokens": [407, 321, 600, 586, 850, 26559, 9953, 51, 284, 339, 311, 3278, 30867, 13], "temperature": 0.0, "avg_logprob": -0.1614276019009677, "compression_ratio": 1.6798029556650247, "no_speech_prob": 4.936931418342283e-06}, {"id": 540, "seek": 251220, "start": 2538.4399999999996, "end": 2541.2, "text": " So we're allowed to use it according to our rules.", "tokens": [407, 321, 434, 4350, 281, 764, 309, 4650, 281, 527, 4474, 13], "temperature": 0.0, "avg_logprob": -0.1614276019009677, "compression_ratio": 1.6798029556650247, "no_speech_prob": 4.936931418342283e-06}, {"id": 541, "seek": 254120, "start": 2541.2, "end": 2548.24, "text": " Okay, so now that we have a loss function, we can use it to train.", "tokens": [1033, 11, 370, 586, 300, 321, 362, 257, 4470, 2445, 11, 321, 393, 764, 309, 281, 3847, 13], "temperature": 0.0, "avg_logprob": -0.11228653963874369, "compression_ratio": 1.6, "no_speech_prob": 4.092867129656952e-06}, {"id": 542, "seek": 254120, "start": 2548.24, "end": 2552.4399999999996, "text": " And we may as well also define a metric, because it's nice to see accuracy to see how we're", "tokens": [400, 321, 815, 382, 731, 611, 6964, 257, 20678, 11, 570, 309, 311, 1481, 281, 536, 14170, 281, 536, 577, 321, 434], "temperature": 0.0, "avg_logprob": -0.11228653963874369, "compression_ratio": 1.6, "no_speech_prob": 4.092867129656952e-06}, {"id": 543, "seek": 254120, "start": 2552.4399999999996, "end": 2553.4399999999996, "text": " going.", "tokens": [516, 13], "temperature": 0.0, "avg_logprob": -0.11228653963874369, "compression_ratio": 1.6, "no_speech_prob": 4.092867129656952e-06}, {"id": 544, "seek": 254120, "start": 2553.4399999999996, "end": 2554.56, "text": " It's just much more interpretable.", "tokens": [467, 311, 445, 709, 544, 7302, 712, 13], "temperature": 0.0, "avg_logprob": -0.11228653963874369, "compression_ratio": 1.6, "no_speech_prob": 4.092867129656952e-06}, {"id": 545, "seek": 254120, "start": 2554.56, "end": 2562.6, "text": " And remember from part one that the accuracy is simply grab the argmax to find out which", "tokens": [400, 1604, 490, 644, 472, 300, 264, 14170, 307, 2935, 4444, 264, 3882, 41167, 281, 915, 484, 597], "temperature": 0.0, "avg_logprob": -0.11228653963874369, "compression_ratio": 1.6, "no_speech_prob": 4.092867129656952e-06}, {"id": 546, "seek": 254120, "start": 2562.6, "end": 2567.8399999999997, "text": " of the numbers in our softmax is the highest, and the index of that is our prediction.", "tokens": [295, 264, 3547, 294, 527, 2787, 41167, 307, 264, 6343, 11, 293, 264, 8186, 295, 300, 307, 527, 17630, 13], "temperature": 0.0, "avg_logprob": -0.11228653963874369, "compression_ratio": 1.6, "no_speech_prob": 4.092867129656952e-06}, {"id": 547, "seek": 256784, "start": 2567.84, "end": 2573.2400000000002, "text": " And then check whether that's equal to the actual, and then we want to take the mean.", "tokens": [400, 550, 1520, 1968, 300, 311, 2681, 281, 264, 3539, 11, 293, 550, 321, 528, 281, 747, 264, 914, 13], "temperature": 0.0, "avg_logprob": -0.10147668283881871, "compression_ratio": 1.7932330827067668, "no_speech_prob": 1.80579190782737e-05}, {"id": 548, "seek": 256784, "start": 2573.2400000000002, "end": 2576.28, "text": " But in PyTorch, you can't take the mean of ints.", "tokens": [583, 294, 9953, 51, 284, 339, 11, 291, 393, 380, 747, 264, 914, 295, 560, 82, 13], "temperature": 0.0, "avg_logprob": -0.10147668283881871, "compression_ratio": 1.7932330827067668, "no_speech_prob": 1.80579190782737e-05}, {"id": 549, "seek": 256784, "start": 2576.28, "end": 2578.7200000000003, "text": " You have to take the mean of floats, which makes some sense.", "tokens": [509, 362, 281, 747, 264, 914, 295, 37878, 11, 597, 1669, 512, 2020, 13], "temperature": 0.0, "avg_logprob": -0.10147668283881871, "compression_ratio": 1.7932330827067668, "no_speech_prob": 1.80579190782737e-05}, {"id": 550, "seek": 256784, "start": 2578.7200000000003, "end": 2580.36, "text": " So turn it into a float first.", "tokens": [407, 1261, 309, 666, 257, 15706, 700, 13], "temperature": 0.0, "avg_logprob": -0.10147668283881871, "compression_ratio": 1.7932330827067668, "no_speech_prob": 1.80579190782737e-05}, {"id": 551, "seek": 256784, "start": 2580.36, "end": 2583.0, "text": " So there's our accuracy.", "tokens": [407, 456, 311, 527, 14170, 13], "temperature": 0.0, "avg_logprob": -0.10147668283881871, "compression_ratio": 1.7932330827067668, "no_speech_prob": 1.80579190782737e-05}, {"id": 552, "seek": 256784, "start": 2583.0, "end": 2584.0, "text": " So let's just check.", "tokens": [407, 718, 311, 445, 1520, 13], "temperature": 0.0, "avg_logprob": -0.10147668283881871, "compression_ratio": 1.7932330827067668, "no_speech_prob": 1.80579190782737e-05}, {"id": 553, "seek": 256784, "start": 2584.0, "end": 2588.32, "text": " Let's grab a batch size of 64, and let's grab our first x batch.", "tokens": [961, 311, 4444, 257, 15245, 2744, 295, 12145, 11, 293, 718, 311, 4444, 527, 700, 2031, 15245, 13], "temperature": 0.0, "avg_logprob": -0.10147668283881871, "compression_ratio": 1.7932330827067668, "no_speech_prob": 1.80579190782737e-05}, {"id": 554, "seek": 256784, "start": 2588.32, "end": 2591.06, "text": " This is our first playing around with mini batches, right?", "tokens": [639, 307, 527, 700, 2433, 926, 365, 8382, 15245, 279, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.10147668283881871, "compression_ratio": 1.7932330827067668, "no_speech_prob": 1.80579190782737e-05}, {"id": 555, "seek": 256784, "start": 2591.06, "end": 2597.04, "text": " So our first x batch is going to be our training set from zero up to batch size.", "tokens": [407, 527, 700, 2031, 15245, 307, 516, 281, 312, 527, 3097, 992, 490, 4018, 493, 281, 15245, 2744, 13], "temperature": 0.0, "avg_logprob": -0.10147668283881871, "compression_ratio": 1.7932330827067668, "no_speech_prob": 1.80579190782737e-05}, {"id": 556, "seek": 259704, "start": 2597.04, "end": 2599.82, "text": " So our predictions is we're just going to run our model.", "tokens": [407, 527, 21264, 307, 321, 434, 445, 516, 281, 1190, 527, 2316, 13], "temperature": 0.0, "avg_logprob": -0.24523353576660156, "compression_ratio": 1.5687203791469195, "no_speech_prob": 3.5559428397391457e-06}, {"id": 557, "seek": 259704, "start": 2599.82, "end": 2606.4, "text": " And remember our model was linear, ReLU linear.", "tokens": [400, 1604, 527, 2316, 390, 8213, 11, 1300, 43, 52, 8213, 13], "temperature": 0.0, "avg_logprob": -0.24523353576660156, "compression_ratio": 1.5687203791469195, "no_speech_prob": 3.5559428397391457e-06}, {"id": 558, "seek": 259704, "start": 2606.4, "end": 2610.2, "text": " So it's still using a super simple model.", "tokens": [407, 309, 311, 920, 1228, 257, 1687, 2199, 2316, 13], "temperature": 0.0, "avg_logprob": -0.24523353576660156, "compression_ratio": 1.5687203791469195, "no_speech_prob": 3.5559428397391457e-06}, {"id": 559, "seek": 259704, "start": 2610.2, "end": 2613.8, "text": " So let's calculate some predictions, and let's have a look at them.", "tokens": [407, 718, 311, 8873, 512, 21264, 11, 293, 718, 311, 362, 257, 574, 412, 552, 13], "temperature": 0.0, "avg_logprob": -0.24523353576660156, "compression_ratio": 1.5687203791469195, "no_speech_prob": 3.5559428397391457e-06}, {"id": 560, "seek": 259704, "start": 2613.8, "end": 2615.44, "text": " And here's some predictions.", "tokens": [400, 510, 311, 512, 21264, 13], "temperature": 0.0, "avg_logprob": -0.24523353576660156, "compression_ratio": 1.5687203791469195, "no_speech_prob": 3.5559428397391457e-06}, {"id": 561, "seek": 259704, "start": 2615.44, "end": 2623.02, "text": " And it's 64 by 10, as you'd expect, batch size 64 and 10 possible probabilities, right?", "tokens": [400, 309, 311, 12145, 538, 1266, 11, 382, 291, 1116, 2066, 11, 15245, 2744, 12145, 293, 1266, 1944, 33783, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.24523353576660156, "compression_ratio": 1.5687203791469195, "no_speech_prob": 3.5559428397391457e-06}, {"id": 562, "seek": 262302, "start": 2623.02, "end": 2629.56, "text": " So now we can grab our first batch of dependent variables and calculate our loss.", "tokens": [407, 586, 321, 393, 4444, 527, 700, 15245, 295, 12334, 9102, 293, 8873, 527, 4470, 13], "temperature": 0.0, "avg_logprob": -0.1457486932927912, "compression_ratio": 1.598326359832636, "no_speech_prob": 4.0928889575297944e-06}, {"id": 563, "seek": 262302, "start": 2629.56, "end": 2633.88, "text": " Okay, and it's 2.3, and calculate our accuracy.", "tokens": [1033, 11, 293, 309, 311, 568, 13, 18, 11, 293, 8873, 527, 14170, 13], "temperature": 0.0, "avg_logprob": -0.1457486932927912, "compression_ratio": 1.598326359832636, "no_speech_prob": 4.0928889575297944e-06}, {"id": 564, "seek": 262302, "start": 2633.88, "end": 2636.7599999999998, "text": " And as you'd expect, it's about 10% because we haven't trained our model.", "tokens": [400, 382, 291, 1116, 2066, 11, 309, 311, 466, 1266, 4, 570, 321, 2378, 380, 8895, 527, 2316, 13], "temperature": 0.0, "avg_logprob": -0.1457486932927912, "compression_ratio": 1.598326359832636, "no_speech_prob": 4.0928889575297944e-06}, {"id": 565, "seek": 262302, "start": 2636.7599999999998, "end": 2642.88, "text": " Okay, so we've got a model that's giving basically random answers.", "tokens": [1033, 11, 370, 321, 600, 658, 257, 2316, 300, 311, 2902, 1936, 4974, 6338, 13], "temperature": 0.0, "avg_logprob": -0.1457486932927912, "compression_ratio": 1.598326359832636, "no_speech_prob": 4.0928889575297944e-06}, {"id": 566, "seek": 262302, "start": 2642.88, "end": 2643.92, "text": " So let's train it.", "tokens": [407, 718, 311, 3847, 309, 13], "temperature": 0.0, "avg_logprob": -0.1457486932927912, "compression_ratio": 1.598326359832636, "no_speech_prob": 4.0928889575297944e-06}, {"id": 567, "seek": 262302, "start": 2643.92, "end": 2644.92, "text": " So we need a learning rate.", "tokens": [407, 321, 643, 257, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.1457486932927912, "compression_ratio": 1.598326359832636, "no_speech_prob": 4.0928889575297944e-06}, {"id": 568, "seek": 262302, "start": 2644.92, "end": 2650.28, "text": " We need to pick a number of epochs, and we need a training loop.", "tokens": [492, 643, 281, 1888, 257, 1230, 295, 30992, 28346, 11, 293, 321, 643, 257, 3097, 6367, 13], "temperature": 0.0, "avg_logprob": -0.1457486932927912, "compression_ratio": 1.598326359832636, "no_speech_prob": 4.0928889575297944e-06}, {"id": 569, "seek": 265028, "start": 2650.28, "end": 2657.48, "text": " So our training loop, if you remember from part one, remember lesson two SGD?", "tokens": [407, 527, 3097, 6367, 11, 498, 291, 1604, 490, 644, 472, 11, 1604, 6898, 732, 34520, 35, 30], "temperature": 0.0, "avg_logprob": -0.19168173300253377, "compression_ratio": 1.5384615384615385, "no_speech_prob": 4.78506444778759e-06}, {"id": 570, "seek": 265028, "start": 2657.48, "end": 2661.5600000000004, "text": " Our training loop looks like this.", "tokens": [2621, 3097, 6367, 1542, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.19168173300253377, "compression_ratio": 1.5384615384615385, "no_speech_prob": 4.78506444778759e-06}, {"id": 571, "seek": 265028, "start": 2661.5600000000004, "end": 2668.6800000000003, "text": " Calculate your predictions, calculate your loss, do backward, subtract learning rate", "tokens": [3511, 2444, 473, 428, 21264, 11, 8873, 428, 4470, 11, 360, 23897, 11, 16390, 2539, 3314], "temperature": 0.0, "avg_logprob": -0.19168173300253377, "compression_ratio": 1.5384615384615385, "no_speech_prob": 4.78506444778759e-06}, {"id": 572, "seek": 265028, "start": 2668.6800000000003, "end": 2671.1200000000003, "text": " times gradients, and zero the gradients.", "tokens": [1413, 2771, 2448, 11, 293, 4018, 264, 2771, 2448, 13], "temperature": 0.0, "avg_logprob": -0.19168173300253377, "compression_ratio": 1.5384615384615385, "no_speech_prob": 4.78506444778759e-06}, {"id": 573, "seek": 265028, "start": 2671.1200000000003, "end": 2673.4, "text": " Okay, so let's do exactly the same thing.", "tokens": [1033, 11, 370, 718, 311, 360, 2293, 264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.19168173300253377, "compression_ratio": 1.5384615384615385, "no_speech_prob": 4.78506444778759e-06}, {"id": 574, "seek": 267340, "start": 2673.4, "end": 2681.56, "text": " So we're gonna go through with GPOC and go through i up until n, which is 50,000.", "tokens": [407, 321, 434, 799, 352, 807, 365, 26039, 30087, 293, 352, 807, 741, 493, 1826, 297, 11, 597, 307, 2625, 11, 1360, 13], "temperature": 0.0, "avg_logprob": -0.1491282958984375, "compression_ratio": 1.779591836734694, "no_speech_prob": 3.5354478313820437e-05}, {"id": 575, "seek": 267340, "start": 2681.56, "end": 2684.04, "text": " That's the number of rows.", "tokens": [663, 311, 264, 1230, 295, 13241, 13], "temperature": 0.0, "avg_logprob": -0.1491282958984375, "compression_ratio": 1.779591836734694, "no_speech_prob": 3.5354478313820437e-05}, {"id": 576, "seek": 267340, "start": 2684.04, "end": 2687.64, "text": " But integer divide by batch size, cuz we're gonna do a batch at a time.", "tokens": [583, 24922, 9845, 538, 15245, 2744, 11, 11910, 321, 434, 799, 360, 257, 15245, 412, 257, 565, 13], "temperature": 0.0, "avg_logprob": -0.1491282958984375, "compression_ratio": 1.779591836734694, "no_speech_prob": 3.5354478313820437e-05}, {"id": 577, "seek": 267340, "start": 2687.64, "end": 2692.7000000000003, "text": " And so then we'll grab everything starting at i times batch size and ending at that plus", "tokens": [400, 370, 550, 321, 603, 4444, 1203, 2891, 412, 741, 1413, 15245, 2744, 293, 8121, 412, 300, 1804], "temperature": 0.0, "avg_logprob": -0.1491282958984375, "compression_ratio": 1.779591836734694, "no_speech_prob": 3.5354478313820437e-05}, {"id": 578, "seek": 267340, "start": 2692.7000000000003, "end": 2693.8, "text": " batch size.", "tokens": [15245, 2744, 13], "temperature": 0.0, "avg_logprob": -0.1491282958984375, "compression_ratio": 1.779591836734694, "no_speech_prob": 3.5354478313820437e-05}, {"id": 579, "seek": 267340, "start": 2693.8, "end": 2697.4, "text": " So this is gonna be our first, this is gonna be our i-th mini batch.", "tokens": [407, 341, 307, 799, 312, 527, 700, 11, 341, 307, 799, 312, 527, 741, 12, 392, 8382, 15245, 13], "temperature": 0.0, "avg_logprob": -0.1491282958984375, "compression_ratio": 1.779591836734694, "no_speech_prob": 3.5354478313820437e-05}, {"id": 580, "seek": 267340, "start": 2697.4, "end": 2703.1600000000003, "text": " And so let's grab one x mini batch, one y mini batch, and pass that through the model", "tokens": [400, 370, 718, 311, 4444, 472, 2031, 8382, 15245, 11, 472, 288, 8382, 15245, 11, 293, 1320, 300, 807, 264, 2316], "temperature": 0.0, "avg_logprob": -0.1491282958984375, "compression_ratio": 1.779591836734694, "no_speech_prob": 3.5354478313820437e-05}, {"id": 581, "seek": 270316, "start": 2703.16, "end": 2705.96, "text": " and our loss function.", "tokens": [293, 527, 4470, 2445, 13], "temperature": 0.0, "avg_logprob": -0.14740280189899482, "compression_ratio": 1.6026785714285714, "no_speech_prob": 8.939552571973763e-06}, {"id": 582, "seek": 270316, "start": 2705.96, "end": 2708.68, "text": " And then do backward.", "tokens": [400, 550, 360, 23897, 13], "temperature": 0.0, "avg_logprob": -0.14740280189899482, "compression_ratio": 1.6026785714285714, "no_speech_prob": 8.939552571973763e-06}, {"id": 583, "seek": 270316, "start": 2708.68, "end": 2711.8799999999997, "text": " And then we're gonna do our update, which remember we have to do with no grad, cuz this", "tokens": [400, 550, 321, 434, 799, 360, 527, 5623, 11, 597, 1604, 321, 362, 281, 360, 365, 572, 2771, 11, 11910, 341], "temperature": 0.0, "avg_logprob": -0.14740280189899482, "compression_ratio": 1.6026785714285714, "no_speech_prob": 8.939552571973763e-06}, {"id": 584, "seek": 270316, "start": 2711.8799999999997, "end": 2713.64, "text": " is not part of the gradient calculation.", "tokens": [307, 406, 644, 295, 264, 16235, 17108, 13], "temperature": 0.0, "avg_logprob": -0.14740280189899482, "compression_ratio": 1.6026785714285714, "no_speech_prob": 8.939552571973763e-06}, {"id": 585, "seek": 270316, "start": 2713.64, "end": 2715.72, "text": " This is the result of it.", "tokens": [639, 307, 264, 1874, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.14740280189899482, "compression_ratio": 1.6026785714285714, "no_speech_prob": 8.939552571973763e-06}, {"id": 586, "seek": 270316, "start": 2715.72, "end": 2721.3599999999997, "text": " But now we can't just go A dot subtract learning rate times gradient.", "tokens": [583, 586, 321, 393, 380, 445, 352, 316, 5893, 16390, 2539, 3314, 1413, 16235, 13], "temperature": 0.0, "avg_logprob": -0.14740280189899482, "compression_ratio": 1.6026785714285714, "no_speech_prob": 8.939552571973763e-06}, {"id": 587, "seek": 270316, "start": 2721.3599999999997, "end": 2725.3999999999996, "text": " We have to do that for every single one of our parameters.", "tokens": [492, 362, 281, 360, 300, 337, 633, 2167, 472, 295, 527, 9834, 13], "temperature": 0.0, "avg_logprob": -0.14740280189899482, "compression_ratio": 1.6026785714285714, "no_speech_prob": 8.939552571973763e-06}, {"id": 588, "seek": 270316, "start": 2725.3999999999996, "end": 2731.04, "text": " So our model has three layers.", "tokens": [407, 527, 2316, 575, 1045, 7914, 13], "temperature": 0.0, "avg_logprob": -0.14740280189899482, "compression_ratio": 1.6026785714285714, "no_speech_prob": 8.939552571973763e-06}, {"id": 589, "seek": 273104, "start": 2731.04, "end": 2733.68, "text": " The ReLU has no parameters in it.", "tokens": [440, 1300, 43, 52, 575, 572, 9834, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.11750603544301, "compression_ratio": 1.7254901960784315, "no_speech_prob": 2.8572933388204547e-06}, {"id": 590, "seek": 273104, "start": 2733.68, "end": 2738.68, "text": " So the linear layer has weight and bias, and this linear layer has weight and bias.", "tokens": [407, 264, 8213, 4583, 575, 3364, 293, 12577, 11, 293, 341, 8213, 4583, 575, 3364, 293, 12577, 13], "temperature": 0.0, "avg_logprob": -0.11750603544301, "compression_ratio": 1.7254901960784315, "no_speech_prob": 2.8572933388204547e-06}, {"id": 591, "seek": 273104, "start": 2738.68, "end": 2743.94, "text": " So we've basically got four tensors to deal with.", "tokens": [407, 321, 600, 1936, 658, 1451, 10688, 830, 281, 2028, 365, 13], "temperature": 0.0, "avg_logprob": -0.11750603544301, "compression_ratio": 1.7254901960784315, "no_speech_prob": 2.8572933388204547e-06}, {"id": 592, "seek": 273104, "start": 2743.94, "end": 2746.34, "text": " So we're gonna go through all of our layers.", "tokens": [407, 321, 434, 799, 352, 807, 439, 295, 527, 7914, 13], "temperature": 0.0, "avg_logprob": -0.11750603544301, "compression_ratio": 1.7254901960784315, "no_speech_prob": 2.8572933388204547e-06}, {"id": 593, "seek": 273104, "start": 2746.34, "end": 2750.34, "text": " And let's just check whether that layer has an attribute called weight or not.", "tokens": [400, 718, 311, 445, 1520, 1968, 300, 4583, 575, 364, 19667, 1219, 3364, 420, 406, 13], "temperature": 0.0, "avg_logprob": -0.11750603544301, "compression_ratio": 1.7254901960784315, "no_speech_prob": 2.8572933388204547e-06}, {"id": 594, "seek": 273104, "start": 2750.34, "end": 2754.5, "text": " That's a bit kind of more flexible than hard coding things.", "tokens": [663, 311, 257, 857, 733, 295, 544, 11358, 813, 1152, 17720, 721, 13], "temperature": 0.0, "avg_logprob": -0.11750603544301, "compression_ratio": 1.7254901960784315, "no_speech_prob": 2.8572933388204547e-06}, {"id": 595, "seek": 273104, "start": 2754.5, "end": 2759.14, "text": " And if it does, then let's update the weight to minus equals the gradient of that by the", "tokens": [400, 498, 309, 775, 11, 550, 718, 311, 5623, 264, 3364, 281, 3175, 6915, 264, 16235, 295, 300, 538, 264], "temperature": 0.0, "avg_logprob": -0.11750603544301, "compression_ratio": 1.7254901960784315, "no_speech_prob": 2.8572933388204547e-06}, {"id": 596, "seek": 275914, "start": 2759.14, "end": 2764.9, "text": " learning rate, the bias to the bias gradient by the learning rate, and then zero those", "tokens": [2539, 3314, 11, 264, 12577, 281, 264, 12577, 16235, 538, 264, 2539, 3314, 11, 293, 550, 4018, 729], "temperature": 0.0, "avg_logprob": -0.11577135118944891, "compression_ratio": 1.7971698113207548, "no_speech_prob": 6.540376034536166e-06}, {"id": 597, "seek": 275914, "start": 2764.9, "end": 2766.8199999999997, "text": " gradients when we're done.", "tokens": [2771, 2448, 562, 321, 434, 1096, 13], "temperature": 0.0, "avg_logprob": -0.11577135118944891, "compression_ratio": 1.7971698113207548, "no_speech_prob": 6.540376034536166e-06}, {"id": 598, "seek": 275914, "start": 2766.8199999999997, "end": 2769.3799999999997, "text": " So let's run it.", "tokens": [407, 718, 311, 1190, 309, 13], "temperature": 0.0, "avg_logprob": -0.11577135118944891, "compression_ratio": 1.7971698113207548, "no_speech_prob": 6.540376034536166e-06}, {"id": 599, "seek": 275914, "start": 2769.3799999999997, "end": 2773.7799999999997, "text": " And then let's check the loss function and the accuracy.", "tokens": [400, 550, 718, 311, 1520, 264, 4470, 2445, 293, 264, 14170, 13], "temperature": 0.0, "avg_logprob": -0.11577135118944891, "compression_ratio": 1.7971698113207548, "no_speech_prob": 6.540376034536166e-06}, {"id": 600, "seek": 275914, "start": 2773.7799999999997, "end": 2781.7599999999998, "text": " And the loss has gone down from 2.3 to 0.05, and the accuracy has gone up from 0.12 to", "tokens": [400, 264, 4470, 575, 2780, 760, 490, 568, 13, 18, 281, 1958, 13, 13328, 11, 293, 264, 14170, 575, 2780, 493, 490, 1958, 13, 4762, 281], "temperature": 0.0, "avg_logprob": -0.11577135118944891, "compression_ratio": 1.7971698113207548, "no_speech_prob": 6.540376034536166e-06}, {"id": 601, "seek": 275914, "start": 2781.7599999999998, "end": 2783.12, "text": " 1.", "tokens": [502, 13], "temperature": 0.0, "avg_logprob": -0.11577135118944891, "compression_ratio": 1.7971698113207548, "no_speech_prob": 6.540376034536166e-06}, {"id": 602, "seek": 275914, "start": 2783.12, "end": 2788.08, "text": " Notice that this accuracy is for only a single mini-batch, and it's a mini-batch from the", "tokens": [13428, 300, 341, 14170, 307, 337, 787, 257, 2167, 8382, 12, 65, 852, 11, 293, 309, 311, 257, 8382, 12, 65, 852, 490, 264], "temperature": 0.0, "avg_logprob": -0.11577135118944891, "compression_ratio": 1.7971698113207548, "no_speech_prob": 6.540376034536166e-06}, {"id": 603, "seek": 275914, "start": 2788.08, "end": 2789.08, "text": " training set.", "tokens": [3097, 992, 13], "temperature": 0.0, "avg_logprob": -0.11577135118944891, "compression_ratio": 1.7971698113207548, "no_speech_prob": 6.540376034536166e-06}, {"id": 604, "seek": 278908, "start": 2789.08, "end": 2792.66, "text": " So it doesn't mean too much, but obviously our model is learning something.", "tokens": [407, 309, 1177, 380, 914, 886, 709, 11, 457, 2745, 527, 2316, 307, 2539, 746, 13], "temperature": 0.0, "avg_logprob": -0.15678629799494667, "compression_ratio": 1.7119341563786008, "no_speech_prob": 1.7231399397132918e-05}, {"id": 605, "seek": 278908, "start": 2792.66, "end": 2793.66, "text": " So this is good.", "tokens": [407, 341, 307, 665, 13], "temperature": 0.0, "avg_logprob": -0.15678629799494667, "compression_ratio": 1.7119341563786008, "no_speech_prob": 1.7231399397132918e-05}, {"id": 606, "seek": 278908, "start": 2793.66, "end": 2802.14, "text": " So this is our, you know, we're now, well, we haven't really done conv.", "tokens": [407, 341, 307, 527, 11, 291, 458, 11, 321, 434, 586, 11, 731, 11, 321, 2378, 380, 534, 1096, 3754, 13], "temperature": 0.0, "avg_logprob": -0.15678629799494667, "compression_ratio": 1.7119341563786008, "no_speech_prob": 1.7231399397132918e-05}, {"id": 607, "seek": 278908, "start": 2802.14, "end": 2803.58, "text": " I guess we've got a basic training loop.", "tokens": [286, 2041, 321, 600, 658, 257, 3875, 3097, 6367, 13], "temperature": 0.0, "avg_logprob": -0.15678629799494667, "compression_ratio": 1.7119341563786008, "no_speech_prob": 1.7231399397132918e-05}, {"id": 608, "seek": 278908, "start": 2803.58, "end": 2804.58, "text": " We're now here.", "tokens": [492, 434, 586, 510, 13], "temperature": 0.0, "avg_logprob": -0.15678629799494667, "compression_ratio": 1.7119341563786008, "no_speech_prob": 1.7231399397132918e-05}, {"id": 609, "seek": 278908, "start": 2804.58, "end": 2809.5, "text": " We have a basic training loop, which is great.", "tokens": [492, 362, 257, 3875, 3097, 6367, 11, 597, 307, 869, 13], "temperature": 0.0, "avg_logprob": -0.15678629799494667, "compression_ratio": 1.7119341563786008, "no_speech_prob": 1.7231399397132918e-05}, {"id": 610, "seek": 278908, "start": 2809.5, "end": 2812.62, "text": " So we kind of got all the pieces.", "tokens": [407, 321, 733, 295, 658, 439, 264, 3755, 13], "temperature": 0.0, "avg_logprob": -0.15678629799494667, "compression_ratio": 1.7119341563786008, "no_speech_prob": 1.7231399397132918e-05}, {"id": 611, "seek": 278908, "start": 2812.62, "end": 2816.42, "text": " So let's try to make this simpler, because this is too much code, right, and it's too", "tokens": [407, 718, 311, 853, 281, 652, 341, 18587, 11, 570, 341, 307, 886, 709, 3089, 11, 558, 11, 293, 309, 311, 886], "temperature": 0.0, "avg_logprob": -0.15678629799494667, "compression_ratio": 1.7119341563786008, "no_speech_prob": 1.7231399397132918e-05}, {"id": 612, "seek": 278908, "start": 2816.42, "end": 2818.66, "text": " hard to fiddle around with.", "tokens": [1152, 281, 24553, 2285, 926, 365, 13], "temperature": 0.0, "avg_logprob": -0.15678629799494667, "compression_ratio": 1.7119341563786008, "no_speech_prob": 1.7231399397132918e-05}, {"id": 613, "seek": 281866, "start": 2818.66, "end": 2827.8199999999997, "text": " So the first bit we'll do is we're going to try and get rid of this mess, and we're going", "tokens": [407, 264, 700, 857, 321, 603, 360, 307, 321, 434, 516, 281, 853, 293, 483, 3973, 295, 341, 2082, 11, 293, 321, 434, 516], "temperature": 0.0, "avg_logprob": -0.10612313887652229, "compression_ratio": 1.8731707317073172, "no_speech_prob": 6.962205134186661e-06}, {"id": 614, "seek": 281866, "start": 2827.8199999999997, "end": 2832.12, "text": " to replace it with this.", "tokens": [281, 7406, 309, 365, 341, 13], "temperature": 0.0, "avg_logprob": -0.10612313887652229, "compression_ratio": 1.8731707317073172, "no_speech_prob": 6.962205134186661e-06}, {"id": 615, "seek": 281866, "start": 2832.12, "end": 2836.94, "text": " And so the difference here is rather than manually going through weight and bias for", "tokens": [400, 370, 264, 2649, 510, 307, 2831, 813, 16945, 516, 807, 3364, 293, 12577, 337], "temperature": 0.0, "avg_logprob": -0.10612313887652229, "compression_ratio": 1.8731707317073172, "no_speech_prob": 6.962205134186661e-06}, {"id": 616, "seek": 281866, "start": 2836.94, "end": 2842.54, "text": " each one, we're going to loop through something called model.parameters.", "tokens": [1184, 472, 11, 321, 434, 516, 281, 6367, 807, 746, 1219, 2316, 13, 2181, 335, 6202, 13], "temperature": 0.0, "avg_logprob": -0.10612313887652229, "compression_ratio": 1.8731707317073172, "no_speech_prob": 6.962205134186661e-06}, {"id": 617, "seek": 281866, "start": 2842.54, "end": 2844.02, "text": " So we're not even going to loop through the layers.", "tokens": [407, 321, 434, 406, 754, 516, 281, 6367, 807, 264, 7914, 13], "temperature": 0.0, "avg_logprob": -0.10612313887652229, "compression_ratio": 1.8731707317073172, "no_speech_prob": 6.962205134186661e-06}, {"id": 618, "seek": 281866, "start": 2844.02, "end": 2846.8799999999997, "text": " We're just going to loop directly through model.parameters.", "tokens": [492, 434, 445, 516, 281, 6367, 3838, 807, 2316, 13, 2181, 335, 6202, 13], "temperature": 0.0, "avg_logprob": -0.10612313887652229, "compression_ratio": 1.8731707317073172, "no_speech_prob": 6.962205134186661e-06}, {"id": 619, "seek": 284688, "start": 2846.88, "end": 2852.42, "text": " And for each parameter, we'll say that parameter minus equals gradient times learning rate.", "tokens": [400, 337, 1184, 13075, 11, 321, 603, 584, 300, 13075, 3175, 6915, 16235, 1413, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.14379616405652917, "compression_ratio": 1.6712328767123288, "no_speech_prob": 1.392540980305057e-06}, {"id": 620, "seek": 284688, "start": 2852.42, "end": 2856.58, "text": " So somehow we need to be able to get all of the parameters of our model, because if we", "tokens": [407, 6063, 321, 643, 281, 312, 1075, 281, 483, 439, 295, 264, 9834, 295, 527, 2316, 11, 570, 498, 321], "temperature": 0.0, "avg_logprob": -0.14379616405652917, "compression_ratio": 1.6712328767123288, "no_speech_prob": 1.392540980305057e-06}, {"id": 621, "seek": 284688, "start": 2856.58, "end": 2861.9, "text": " could do that, we could greatly simplify this part of the loop and also make it much more", "tokens": [727, 360, 300, 11, 321, 727, 14147, 20460, 341, 644, 295, 264, 6367, 293, 611, 652, 309, 709, 544], "temperature": 0.0, "avg_logprob": -0.14379616405652917, "compression_ratio": 1.6712328767123288, "no_speech_prob": 1.392540980305057e-06}, {"id": 622, "seek": 284688, "start": 2861.9, "end": 2863.7400000000002, "text": " flexible.", "tokens": [11358, 13], "temperature": 0.0, "avg_logprob": -0.14379616405652917, "compression_ratio": 1.6712328767123288, "no_speech_prob": 1.392540980305057e-06}, {"id": 623, "seek": 284688, "start": 2863.7400000000002, "end": 2873.1400000000003, "text": " So to do that, we could create something like this.", "tokens": [407, 281, 360, 300, 11, 321, 727, 1884, 746, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.14379616405652917, "compression_ratio": 1.6712328767123288, "no_speech_prob": 1.392540980305057e-06}, {"id": 624, "seek": 284688, "start": 2873.1400000000003, "end": 2875.78, "text": " I'm just calling this dummy module.", "tokens": [286, 478, 445, 5141, 341, 35064, 10088, 13], "temperature": 0.0, "avg_logprob": -0.14379616405652917, "compression_ratio": 1.6712328767123288, "no_speech_prob": 1.392540980305057e-06}, {"id": 625, "seek": 287578, "start": 2875.78, "end": 2881.38, "text": " And in dummy module, what I'm going to do is I'm going to say every time I set an attribute", "tokens": [400, 294, 35064, 10088, 11, 437, 286, 478, 516, 281, 360, 307, 286, 478, 516, 281, 584, 633, 565, 286, 992, 364, 19667], "temperature": 0.0, "avg_logprob": -0.09325282356955789, "compression_ratio": 1.7946428571428572, "no_speech_prob": 3.90545528716757e-06}, {"id": 626, "seek": 287578, "start": 2881.38, "end": 2891.7400000000002, "text": " like L1 or L2, in this case to linear, I want to update a list called underscore modules", "tokens": [411, 441, 16, 420, 441, 17, 11, 294, 341, 1389, 281, 8213, 11, 286, 528, 281, 5623, 257, 1329, 1219, 37556, 16679], "temperature": 0.0, "avg_logprob": -0.09325282356955789, "compression_ratio": 1.7946428571428572, "no_speech_prob": 3.90545528716757e-06}, {"id": 627, "seek": 287578, "start": 2891.7400000000002, "end": 2893.78, "text": " with a list of all of the modules I have.", "tokens": [365, 257, 1329, 295, 439, 295, 264, 16679, 286, 362, 13], "temperature": 0.0, "avg_logprob": -0.09325282356955789, "compression_ratio": 1.7946428571428572, "no_speech_prob": 3.90545528716757e-06}, {"id": 628, "seek": 287578, "start": 2893.78, "end": 2899.46, "text": " So in other words, after I create this dummy module, I want to be able to print out, here's", "tokens": [407, 294, 661, 2283, 11, 934, 286, 1884, 341, 35064, 10088, 11, 286, 528, 281, 312, 1075, 281, 4482, 484, 11, 510, 311], "temperature": 0.0, "avg_logprob": -0.09325282356955789, "compression_ratio": 1.7946428571428572, "no_speech_prob": 3.90545528716757e-06}, {"id": 629, "seek": 287578, "start": 2899.46, "end": 2903.2400000000002, "text": " my representation, I want to be able to print out the list of those modules and see the", "tokens": [452, 10290, 11, 286, 528, 281, 312, 1075, 281, 4482, 484, 264, 1329, 295, 729, 16679, 293, 536, 264], "temperature": 0.0, "avg_logprob": -0.09325282356955789, "compression_ratio": 1.7946428571428572, "no_speech_prob": 3.90545528716757e-06}, {"id": 630, "seek": 290324, "start": 2903.24, "end": 2909.22, "text": " modules that are there, because then I can define a method called parameters that will", "tokens": [16679, 300, 366, 456, 11, 570, 550, 286, 393, 6964, 257, 3170, 1219, 9834, 300, 486], "temperature": 0.0, "avg_logprob": -0.1690894535609654, "compression_ratio": 1.6238938053097345, "no_speech_prob": 7.411205842799973e-06}, {"id": 631, "seek": 290324, "start": 2909.22, "end": 2915.4599999999996, "text": " go through everything in my underscore modules list and then go through all of their parameters.", "tokens": [352, 807, 1203, 294, 452, 37556, 16679, 1329, 293, 550, 352, 807, 439, 295, 641, 9834, 13], "temperature": 0.0, "avg_logprob": -0.1690894535609654, "compression_ratio": 1.6238938053097345, "no_speech_prob": 7.411205842799973e-06}, {"id": 632, "seek": 290324, "start": 2915.4599999999996, "end": 2917.2599999999998, "text": " And that's what I'll be able to do.", "tokens": [400, 300, 311, 437, 286, 603, 312, 1075, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.1690894535609654, "compression_ratio": 1.6238938053097345, "no_speech_prob": 7.411205842799973e-06}, {"id": 633, "seek": 290324, "start": 2917.2599999999998, "end": 2920.9399999999996, "text": " See, I could do here model.parameters.", "tokens": [3008, 11, 286, 727, 360, 510, 2316, 13, 2181, 335, 6202, 13], "temperature": 0.0, "avg_logprob": -0.1690894535609654, "compression_ratio": 1.6238938053097345, "no_speech_prob": 7.411205842799973e-06}, {"id": 634, "seek": 290324, "start": 2920.9399999999996, "end": 2922.7, "text": " So how did I create this?", "tokens": [407, 577, 630, 286, 1884, 341, 30], "temperature": 0.0, "avg_logprob": -0.1690894535609654, "compression_ratio": 1.6238938053097345, "no_speech_prob": 7.411205842799973e-06}, {"id": 635, "seek": 290324, "start": 2922.7, "end": 2924.58, "text": " You'll see it's not inheriting from something.", "tokens": [509, 603, 536, 309, 311, 406, 9484, 1748, 490, 746, 13], "temperature": 0.0, "avg_logprob": -0.1690894535609654, "compression_ratio": 1.6238938053097345, "no_speech_prob": 7.411205842799973e-06}, {"id": 636, "seek": 290324, "start": 2924.58, "end": 2927.9399999999996, "text": " This is all written in pure Python.", "tokens": [639, 307, 439, 3720, 294, 6075, 15329, 13], "temperature": 0.0, "avg_logprob": -0.1690894535609654, "compression_ratio": 1.6238938053097345, "no_speech_prob": 7.411205842799973e-06}, {"id": 637, "seek": 292794, "start": 2927.94, "end": 2935.66, "text": " How did I make it so that as soon as I said here's an attribute in my in it, that somehow", "tokens": [1012, 630, 286, 652, 309, 370, 300, 382, 2321, 382, 286, 848, 510, 311, 364, 19667, 294, 452, 294, 309, 11, 300, 6063], "temperature": 0.0, "avg_logprob": -0.156817636991802, "compression_ratio": 1.574468085106383, "no_speech_prob": 2.4439812023047125e-06}, {"id": 638, "seek": 292794, "start": 2935.66, "end": 2941.38, "text": " it magically appeared in this underscore modules list so that I could then create these parameters", "tokens": [309, 39763, 8516, 294, 341, 37556, 16679, 1329, 370, 300, 286, 727, 550, 1884, 613, 9834], "temperature": 0.0, "avg_logprob": -0.156817636991802, "compression_ratio": 1.574468085106383, "no_speech_prob": 2.4439812023047125e-06}, {"id": 639, "seek": 292794, "start": 2941.38, "end": 2945.18, "text": " so that I could then do this refactoring?", "tokens": [370, 300, 286, 727, 550, 360, 341, 1895, 578, 3662, 30], "temperature": 0.0, "avg_logprob": -0.156817636991802, "compression_ratio": 1.574468085106383, "no_speech_prob": 2.4439812023047125e-06}, {"id": 640, "seek": 292794, "start": 2945.18, "end": 2951.04, "text": " And the trick is that Python has a special Dunder setAtra method.", "tokens": [400, 264, 4282, 307, 300, 15329, 575, 257, 2121, 413, 6617, 992, 18684, 424, 3170, 13], "temperature": 0.0, "avg_logprob": -0.156817636991802, "compression_ratio": 1.574468085106383, "no_speech_prob": 2.4439812023047125e-06}, {"id": 641, "seek": 295104, "start": 2951.04, "end": 2958.64, "text": " And every time you assign to anything inside self inside Python, it will call this method", "tokens": [400, 633, 565, 291, 6269, 281, 1340, 1854, 2698, 1854, 15329, 11, 309, 486, 818, 341, 3170], "temperature": 0.0, "avg_logprob": -0.13344059738458372, "compression_ratio": 1.6846473029045643, "no_speech_prob": 3.1381230201077415e-06}, {"id": 642, "seek": 295104, "start": 2958.64, "end": 2961.18, "text": " if you've got one.", "tokens": [498, 291, 600, 658, 472, 13], "temperature": 0.0, "avg_logprob": -0.13344059738458372, "compression_ratio": 1.6846473029045643, "no_speech_prob": 3.1381230201077415e-06}, {"id": 643, "seek": 295104, "start": 2961.18, "end": 2966.62, "text": " And so this method just checks that my the key, so in other words, the attribute name,", "tokens": [400, 370, 341, 3170, 445, 13834, 300, 452, 264, 2141, 11, 370, 294, 661, 2283, 11, 264, 19667, 1315, 11], "temperature": 0.0, "avg_logprob": -0.13344059738458372, "compression_ratio": 1.6846473029045643, "no_speech_prob": 3.1381230201077415e-06}, {"id": 644, "seek": 295104, "start": 2966.62, "end": 2970.1, "text": " doesn't start with underscore, because if it does, it might be underscore modules and", "tokens": [1177, 380, 722, 365, 37556, 11, 570, 498, 309, 775, 11, 309, 1062, 312, 37556, 16679, 293], "temperature": 0.0, "avg_logprob": -0.13344059738458372, "compression_ratio": 1.6846473029045643, "no_speech_prob": 3.1381230201077415e-06}, {"id": 645, "seek": 295104, "start": 2970.1, "end": 2972.1, "text": " then it's going to be like a recursive loop.", "tokens": [550, 309, 311, 516, 281, 312, 411, 257, 20560, 488, 6367, 13], "temperature": 0.0, "avg_logprob": -0.13344059738458372, "compression_ratio": 1.6846473029045643, "no_speech_prob": 3.1381230201077415e-06}, {"id": 646, "seek": 295104, "start": 2972.1, "end": 2976.32, "text": " And also, Python's got all kinds of internal stuff that starts with underscore.", "tokens": [400, 611, 11, 15329, 311, 658, 439, 3685, 295, 6920, 1507, 300, 3719, 365, 37556, 13], "temperature": 0.0, "avg_logprob": -0.13344059738458372, "compression_ratio": 1.6846473029045643, "no_speech_prob": 3.1381230201077415e-06}, {"id": 647, "seek": 297632, "start": 2976.32, "end": 2982.32, "text": " So as long as it's not some internal private stuff, put that value inside my modules dictionary", "tokens": [407, 382, 938, 382, 309, 311, 406, 512, 6920, 4551, 1507, 11, 829, 300, 2158, 1854, 452, 16679, 25890], "temperature": 0.0, "avg_logprob": -0.10174261785186497, "compression_ratio": 1.6274509803921569, "no_speech_prob": 1.7330319224129198e-06}, {"id": 648, "seek": 297632, "start": 2982.32, "end": 2984.46, "text": " and call it k.", "tokens": [293, 818, 309, 350, 13], "temperature": 0.0, "avg_logprob": -0.10174261785186497, "compression_ratio": 1.6274509803921569, "no_speech_prob": 1.7330319224129198e-06}, {"id": 649, "seek": 297632, "start": 2984.46, "end": 2985.46, "text": " That's it.", "tokens": [663, 311, 309, 13], "temperature": 0.0, "avg_logprob": -0.10174261785186497, "compression_ratio": 1.6274509803921569, "no_speech_prob": 1.7330319224129198e-06}, {"id": 650, "seek": 297632, "start": 2985.46, "end": 2990.46, "text": " And then after you've done that, do whatever the superclass does when it sets attributes.", "tokens": [400, 550, 934, 291, 600, 1096, 300, 11, 360, 2035, 264, 1687, 11665, 775, 562, 309, 6352, 17212, 13], "temperature": 0.0, "avg_logprob": -0.10174261785186497, "compression_ratio": 1.6274509803921569, "no_speech_prob": 1.7330319224129198e-06}, {"id": 651, "seek": 297632, "start": 2990.46, "end": 2993.1400000000003, "text": " And in this case, the superclass is object.", "tokens": [400, 294, 341, 1389, 11, 264, 1687, 11665, 307, 2657, 13], "temperature": 0.0, "avg_logprob": -0.10174261785186497, "compression_ratio": 1.6274509803921569, "no_speech_prob": 1.7330319224129198e-06}, {"id": 652, "seek": 297632, "start": 2993.1400000000003, "end": 2997.1600000000003, "text": " If you don't say what it is, then it's just the Python highest level object.", "tokens": [759, 291, 500, 380, 584, 437, 309, 307, 11, 550, 309, 311, 445, 264, 15329, 6343, 1496, 2657, 13], "temperature": 0.0, "avg_logprob": -0.10174261785186497, "compression_ratio": 1.6274509803921569, "no_speech_prob": 1.7330319224129198e-06}, {"id": 653, "seek": 297632, "start": 2997.1600000000003, "end": 3005.34, "text": " So now we have something that has all of the stuff we need to do this refactoring.", "tokens": [407, 586, 321, 362, 746, 300, 575, 439, 295, 264, 1507, 321, 643, 281, 360, 341, 1895, 578, 3662, 13], "temperature": 0.0, "avg_logprob": -0.10174261785186497, "compression_ratio": 1.6274509803921569, "no_speech_prob": 1.7330319224129198e-06}, {"id": 654, "seek": 300534, "start": 3005.34, "end": 3011.7400000000002, "text": " But the good news is PyTorch also has something that does that and it's called nn.module.", "tokens": [583, 264, 665, 2583, 307, 9953, 51, 284, 339, 611, 575, 746, 300, 775, 300, 293, 309, 311, 1219, 297, 77, 13, 8014, 2271, 13], "temperature": 0.0, "avg_logprob": -0.12000967421621647, "compression_ratio": 1.6504065040650406, "no_speech_prob": 3.237737701056176e-06}, {"id": 655, "seek": 300534, "start": 3011.7400000000002, "end": 3017.1800000000003, "text": " So we can do the exact same thing rather than implementing that set attribute stuff ourselves.", "tokens": [407, 321, 393, 360, 264, 1900, 912, 551, 2831, 813, 18114, 300, 992, 19667, 1507, 4175, 13], "temperature": 0.0, "avg_logprob": -0.12000967421621647, "compression_ratio": 1.6504065040650406, "no_speech_prob": 3.237737701056176e-06}, {"id": 656, "seek": 300534, "start": 3017.1800000000003, "end": 3023.1400000000003, "text": " We can just call just inherit from nn.module and it does it for us.", "tokens": [492, 393, 445, 818, 445, 21389, 490, 297, 77, 13, 8014, 2271, 293, 309, 775, 309, 337, 505, 13], "temperature": 0.0, "avg_logprob": -0.12000967421621647, "compression_ratio": 1.6504065040650406, "no_speech_prob": 3.237737701056176e-06}, {"id": 657, "seek": 300534, "start": 3023.1400000000003, "end": 3028.9, "text": " And so this is now you know why you have to call super Dunder in it first, because it", "tokens": [400, 370, 341, 307, 586, 291, 458, 983, 291, 362, 281, 818, 1687, 413, 6617, 294, 309, 700, 11, 570, 309], "temperature": 0.0, "avg_logprob": -0.12000967421621647, "compression_ratio": 1.6504065040650406, "no_speech_prob": 3.237737701056176e-06}, {"id": 658, "seek": 300534, "start": 3028.9, "end": 3034.34, "text": " has to set up its equivalent of this underscore modules dictionary.", "tokens": [575, 281, 992, 493, 1080, 10344, 295, 341, 37556, 16679, 25890, 13], "temperature": 0.0, "avg_logprob": -0.12000967421621647, "compression_ratio": 1.6504065040650406, "no_speech_prob": 3.237737701056176e-06}, {"id": 659, "seek": 303434, "start": 3034.34, "end": 3037.86, "text": " So that's why you have to call super in it first.", "tokens": [407, 300, 311, 983, 291, 362, 281, 818, 1687, 294, 309, 700, 13], "temperature": 0.0, "avg_logprob": -0.08234956359863281, "compression_ratio": 1.7028112449799198, "no_speech_prob": 8.013334991119336e-06}, {"id": 660, "seek": 303434, "start": 3037.86, "end": 3043.02, "text": " And then after you've done that in PyTorch, it's exactly the same as what I just showed", "tokens": [400, 550, 934, 291, 600, 1096, 300, 294, 9953, 51, 284, 339, 11, 309, 311, 2293, 264, 912, 382, 437, 286, 445, 4712], "temperature": 0.0, "avg_logprob": -0.08234956359863281, "compression_ratio": 1.7028112449799198, "no_speech_prob": 8.013334991119336e-06}, {"id": 661, "seek": 303434, "start": 3043.02, "end": 3044.02, "text": " you.", "tokens": [291, 13], "temperature": 0.0, "avg_logprob": -0.08234956359863281, "compression_ratio": 1.7028112449799198, "no_speech_prob": 8.013334991119336e-06}, {"id": 662, "seek": 303434, "start": 3044.02, "end": 3048.78, "text": " It now creates something which you can access through named children.", "tokens": [467, 586, 7829, 746, 597, 291, 393, 2105, 807, 4926, 2227, 13], "temperature": 0.0, "avg_logprob": -0.08234956359863281, "compression_ratio": 1.7028112449799198, "no_speech_prob": 8.013334991119336e-06}, {"id": 663, "seek": 303434, "start": 3048.78, "end": 3053.7400000000002, "text": " And you can see here if I print out the name and the layer, there is the name and the layer.", "tokens": [400, 291, 393, 536, 510, 498, 286, 4482, 484, 264, 1315, 293, 264, 4583, 11, 456, 307, 264, 1315, 293, 264, 4583, 13], "temperature": 0.0, "avg_logprob": -0.08234956359863281, "compression_ratio": 1.7028112449799198, "no_speech_prob": 8.013334991119336e-06}, {"id": 664, "seek": 303434, "start": 3053.7400000000002, "end": 3057.8, "text": " So this is how PyTorch does the exact same thing.", "tokens": [407, 341, 307, 577, 9953, 51, 284, 339, 775, 264, 1900, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.08234956359863281, "compression_ratio": 1.7028112449799198, "no_speech_prob": 8.013334991119336e-06}, {"id": 665, "seek": 303434, "start": 3057.8, "end": 3063.7200000000003, "text": " Just like I created a Dunder repra, PyTorch also has a Dunder repra.", "tokens": [1449, 411, 286, 2942, 257, 413, 6617, 1085, 424, 11, 9953, 51, 284, 339, 611, 575, 257, 413, 6617, 1085, 424, 13], "temperature": 0.0, "avg_logprob": -0.08234956359863281, "compression_ratio": 1.7028112449799198, "no_speech_prob": 8.013334991119336e-06}, {"id": 666, "seek": 306372, "start": 3063.72, "end": 3068.18, "text": " So if you just print out model, it prints it out like so.", "tokens": [407, 498, 291, 445, 4482, 484, 2316, 11, 309, 22305, 309, 484, 411, 370, 13], "temperature": 0.0, "avg_logprob": -0.16904122741134078, "compression_ratio": 1.5879828326180256, "no_speech_prob": 4.289297976356465e-06}, {"id": 667, "seek": 306372, "start": 3068.18, "end": 3071.74, "text": " You can grab the attributes just in the normal Pythonic way.", "tokens": [509, 393, 4444, 264, 17212, 445, 294, 264, 2710, 9953, 392, 11630, 636, 13], "temperature": 0.0, "avg_logprob": -0.16904122741134078, "compression_ratio": 1.5879828326180256, "no_speech_prob": 4.289297976356465e-06}, {"id": 668, "seek": 306372, "start": 3071.74, "end": 3074.02, "text": " It's just a normal Python class.", "tokens": [467, 311, 445, 257, 2710, 15329, 1508, 13], "temperature": 0.0, "avg_logprob": -0.16904122741134078, "compression_ratio": 1.5879828326180256, "no_speech_prob": 4.289297976356465e-06}, {"id": 669, "seek": 306372, "start": 3074.02, "end": 3077.5, "text": " It has a bit of this extra behavior.", "tokens": [467, 575, 257, 857, 295, 341, 2857, 5223, 13], "temperature": 0.0, "avg_logprob": -0.16904122741134078, "compression_ratio": 1.5879828326180256, "no_speech_prob": 4.289297976356465e-06}, {"id": 670, "seek": 306372, "start": 3077.5, "end": 3085.2, "text": " So now we can run it with this refactoring.", "tokens": [407, 586, 321, 393, 1190, 309, 365, 341, 1895, 578, 3662, 13], "temperature": 0.0, "avg_logprob": -0.16904122741134078, "compression_ratio": 1.5879828326180256, "no_speech_prob": 4.289297976356465e-06}, {"id": 671, "seek": 306372, "start": 3085.2, "end": 3087.8399999999997, "text": " Make sure everything works.", "tokens": [4387, 988, 1203, 1985, 13], "temperature": 0.0, "avg_logprob": -0.16904122741134078, "compression_ratio": 1.5879828326180256, "no_speech_prob": 4.289297976356465e-06}, {"id": 672, "seek": 306372, "start": 3087.8399999999997, "end": 3088.8399999999997, "text": " And there we go.", "tokens": [400, 456, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.16904122741134078, "compression_ratio": 1.5879828326180256, "no_speech_prob": 4.289297976356465e-06}, {"id": 673, "seek": 306372, "start": 3088.8399999999997, "end": 3089.8399999999997, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.16904122741134078, "compression_ratio": 1.5879828326180256, "no_speech_prob": 4.289297976356465e-06}, {"id": 674, "seek": 306372, "start": 3089.8399999999997, "end": 3093.62, "text": " So this is doing exactly the same thing as before, but a little bit more conveniently.", "tokens": [407, 341, 307, 884, 2293, 264, 912, 551, 382, 949, 11, 457, 257, 707, 857, 544, 44375, 13], "temperature": 0.0, "avg_logprob": -0.16904122741134078, "compression_ratio": 1.5879828326180256, "no_speech_prob": 4.289297976356465e-06}, {"id": 675, "seek": 309362, "start": 3093.62, "end": 3096.7, "text": " Not convenient enough for my liking.", "tokens": [1726, 10851, 1547, 337, 452, 16933, 13], "temperature": 0.0, "avg_logprob": -0.1195984399446877, "compression_ratio": 1.644859813084112, "no_speech_prob": 1.8631399143487215e-05}, {"id": 676, "seek": 309362, "start": 3096.7, "end": 3104.66, "text": " So one thing we could try to do is to get rid of the need to write every layer separately,", "tokens": [407, 472, 551, 321, 727, 853, 281, 360, 307, 281, 483, 3973, 295, 264, 643, 281, 2464, 633, 4583, 14759, 11], "temperature": 0.0, "avg_logprob": -0.1195984399446877, "compression_ratio": 1.644859813084112, "no_speech_prob": 1.8631399143487215e-05}, {"id": 677, "seek": 309362, "start": 3104.66, "end": 3106.94, "text": " maybe go back to having it as a list again.", "tokens": [1310, 352, 646, 281, 1419, 309, 382, 257, 1329, 797, 13], "temperature": 0.0, "avg_logprob": -0.1195984399446877, "compression_ratio": 1.644859813084112, "no_speech_prob": 1.8631399143487215e-05}, {"id": 678, "seek": 309362, "start": 3106.94, "end": 3114.06, "text": " So if we made it a list of layers, right, and then we want to be able to pass that to", "tokens": [407, 498, 321, 1027, 309, 257, 1329, 295, 7914, 11, 558, 11, 293, 550, 321, 528, 281, 312, 1075, 281, 1320, 300, 281], "temperature": 0.0, "avg_logprob": -0.1195984399446877, "compression_ratio": 1.644859813084112, "no_speech_prob": 1.8631399143487215e-05}, {"id": 679, "seek": 309362, "start": 3114.06, "end": 3122.7, "text": " some model class, passing the layers, this is not enough to make them available as parameters,", "tokens": [512, 2316, 1508, 11, 8437, 264, 7914, 11, 341, 307, 406, 1547, 281, 652, 552, 2435, 382, 9834, 11], "temperature": 0.0, "avg_logprob": -0.1195984399446877, "compression_ratio": 1.644859813084112, "no_speech_prob": 1.8631399143487215e-05}, {"id": 680, "seek": 312270, "start": 3122.7, "end": 3126.14, "text": " right, because the only thing that actually that PyTorch is going to make available as", "tokens": [558, 11, 570, 264, 787, 551, 300, 767, 300, 9953, 51, 284, 339, 307, 516, 281, 652, 2435, 382], "temperature": 0.0, "avg_logprob": -0.14444130570141236, "compression_ratio": 1.71484375, "no_speech_prob": 6.048794148227898e-06}, {"id": 681, "seek": 312270, "start": 3126.14, "end": 3132.4199999999996, "text": " parameters are things that it knows are proper nn.modules.", "tokens": [9834, 366, 721, 300, 309, 3255, 366, 2296, 297, 77, 13, 8014, 3473, 13], "temperature": 0.0, "avg_logprob": -0.14444130570141236, "compression_ratio": 1.71484375, "no_speech_prob": 6.048794148227898e-06}, {"id": 682, "seek": 312270, "start": 3132.4199999999996, "end": 3134.08, "text": " So but here's the cool thing.", "tokens": [407, 457, 510, 311, 264, 1627, 551, 13], "temperature": 0.0, "avg_logprob": -0.14444130570141236, "compression_ratio": 1.71484375, "no_speech_prob": 6.048794148227898e-06}, {"id": 683, "seek": 312270, "start": 3134.08, "end": 3140.3799999999997, "text": " You can just go through all of those layers and call self.addModule.", "tokens": [509, 393, 445, 352, 807, 439, 295, 729, 7914, 293, 818, 2698, 13, 25224, 44, 378, 2271, 13], "temperature": 0.0, "avg_logprob": -0.14444130570141236, "compression_ratio": 1.71484375, "no_speech_prob": 6.048794148227898e-06}, {"id": 684, "seek": 312270, "start": 3140.3799999999997, "end": 3144.8999999999996, "text": " That's just the equivalent of what I did here when I said self.underscoreModules, blah,", "tokens": [663, 311, 445, 264, 10344, 295, 437, 286, 630, 510, 562, 286, 848, 2698, 13, 997, 433, 12352, 44, 378, 3473, 11, 12288, 11], "temperature": 0.0, "avg_logprob": -0.14444130570141236, "compression_ratio": 1.71484375, "no_speech_prob": 6.048794148227898e-06}, {"id": 685, "seek": 312270, "start": 3144.8999999999996, "end": 3145.8999999999996, "text": " blah, right?", "tokens": [12288, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.14444130570141236, "compression_ratio": 1.71484375, "no_speech_prob": 6.048794148227898e-06}, {"id": 686, "seek": 312270, "start": 3145.8999999999996, "end": 3151.8199999999997, "text": " So in PyTorch, you can just call self.addModule, and just like I did, you give it a name, and", "tokens": [407, 294, 9953, 51, 284, 339, 11, 291, 393, 445, 818, 2698, 13, 25224, 44, 378, 2271, 11, 293, 445, 411, 286, 630, 11, 291, 976, 309, 257, 1315, 11, 293], "temperature": 0.0, "avg_logprob": -0.14444130570141236, "compression_ratio": 1.71484375, "no_speech_prob": 6.048794148227898e-06}, {"id": 687, "seek": 315182, "start": 3151.82, "end": 3153.78, "text": " then you pass in the layer.", "tokens": [550, 291, 1320, 294, 264, 4583, 13], "temperature": 0.0, "avg_logprob": -0.15154915977926814, "compression_ratio": 1.5308641975308641, "no_speech_prob": 9.51604397414485e-06}, {"id": 688, "seek": 315182, "start": 3153.78, "end": 3163.5800000000004, "text": " And so if you do that, then you end up with the same thing.", "tokens": [400, 370, 498, 291, 360, 300, 11, 550, 291, 917, 493, 365, 264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.15154915977926814, "compression_ratio": 1.5308641975308641, "no_speech_prob": 9.51604397414485e-06}, {"id": 689, "seek": 315182, "start": 3163.5800000000004, "end": 3165.98, "text": " So that's one thing you could do.", "tokens": [407, 300, 311, 472, 551, 291, 727, 360, 13], "temperature": 0.0, "avg_logprob": -0.15154915977926814, "compression_ratio": 1.5308641975308641, "no_speech_prob": 9.51604397414485e-06}, {"id": 690, "seek": 315182, "start": 3165.98, "end": 3169.7400000000002, "text": " But this is kind of clunky, so it would be nice if PyTorch would do it for you, and it", "tokens": [583, 341, 307, 733, 295, 596, 25837, 11, 370, 309, 576, 312, 1481, 498, 9953, 51, 284, 339, 576, 360, 309, 337, 291, 11, 293, 309], "temperature": 0.0, "avg_logprob": -0.15154915977926814, "compression_ratio": 1.5308641975308641, "no_speech_prob": 9.51604397414485e-06}, {"id": 691, "seek": 315182, "start": 3169.7400000000002, "end": 3170.86, "text": " does.", "tokens": [775, 13], "temperature": 0.0, "avg_logprob": -0.15154915977926814, "compression_ratio": 1.5308641975308641, "no_speech_prob": 9.51604397414485e-06}, {"id": 692, "seek": 315182, "start": 3170.86, "end": 3173.1000000000004, "text": " That's what nn.modules list does.", "tokens": [663, 311, 437, 297, 77, 13, 8014, 3473, 1329, 775, 13], "temperature": 0.0, "avg_logprob": -0.15154915977926814, "compression_ratio": 1.5308641975308641, "no_speech_prob": 9.51604397414485e-06}, {"id": 693, "seek": 317310, "start": 3173.1, "end": 3184.58, "text": " So if you use an nn.module list, then it just basically calls that line of code for you.", "tokens": [407, 498, 291, 764, 364, 297, 77, 13, 8014, 2271, 1329, 11, 550, 309, 445, 1936, 5498, 300, 1622, 295, 3089, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.14116768075638458, "compression_ratio": 1.6363636363636365, "no_speech_prob": 1.9637900550151244e-06}, {"id": 694, "seek": 317310, "start": 3184.58, "end": 3185.9, "text": " So you can see us doing it here.", "tokens": [407, 291, 393, 536, 505, 884, 309, 510, 13], "temperature": 0.0, "avg_logprob": -0.14116768075638458, "compression_ratio": 1.6363636363636365, "no_speech_prob": 1.9637900550151244e-06}, {"id": 695, "seek": 317310, "start": 3185.9, "end": 3190.66, "text": " We've got to create something called sequential model, which just sets self.layers to that", "tokens": [492, 600, 658, 281, 1884, 746, 1219, 42881, 2316, 11, 597, 445, 6352, 2698, 13, 8376, 433, 281, 300], "temperature": 0.0, "avg_logprob": -0.14116768075638458, "compression_ratio": 1.6363636363636365, "no_speech_prob": 1.9637900550151244e-06}, {"id": 696, "seek": 317310, "start": 3190.66, "end": 3191.98, "text": " module list.", "tokens": [10088, 1329, 13], "temperature": 0.0, "avg_logprob": -0.14116768075638458, "compression_ratio": 1.6363636363636365, "no_speech_prob": 1.9637900550151244e-06}, {"id": 697, "seek": 317310, "start": 3191.98, "end": 3196.2999999999997, "text": " And then when we call it, it just goes through each layer, x equals that layer of x, and", "tokens": [400, 550, 562, 321, 818, 309, 11, 309, 445, 1709, 807, 1184, 4583, 11, 2031, 6915, 300, 4583, 295, 2031, 11, 293], "temperature": 0.0, "avg_logprob": -0.14116768075638458, "compression_ratio": 1.6363636363636365, "no_speech_prob": 1.9637900550151244e-06}, {"id": 698, "seek": 317310, "start": 3196.2999999999997, "end": 3198.74, "text": " returns it.", "tokens": [11247, 309, 13], "temperature": 0.0, "avg_logprob": -0.14116768075638458, "compression_ratio": 1.6363636363636365, "no_speech_prob": 1.9637900550151244e-06}, {"id": 699, "seek": 317310, "start": 3198.74, "end": 3199.74, "text": " And there's that.", "tokens": [400, 456, 311, 300, 13], "temperature": 0.0, "avg_logprob": -0.14116768075638458, "compression_ratio": 1.6363636363636365, "no_speech_prob": 1.9637900550151244e-06}, {"id": 700, "seek": 317310, "start": 3199.74, "end": 3200.74, "text": " Okay?", "tokens": [1033, 30], "temperature": 0.0, "avg_logprob": -0.14116768075638458, "compression_ratio": 1.6363636363636365, "no_speech_prob": 1.9637900550151244e-06}, {"id": 701, "seek": 317310, "start": 3200.74, "end": 3202.66, "text": " Even this is a little bit on the clunky side.", "tokens": [2754, 341, 307, 257, 707, 857, 322, 264, 596, 25837, 1252, 13], "temperature": 0.0, "avg_logprob": -0.14116768075638458, "compression_ratio": 1.6363636363636365, "no_speech_prob": 1.9637900550151244e-06}, {"id": 702, "seek": 320266, "start": 3202.66, "end": 3205.74, "text": " Why would we have to write it ourselves?", "tokens": [1545, 576, 321, 362, 281, 2464, 309, 4175, 30], "temperature": 0.0, "avg_logprob": -0.15405741192045666, "compression_ratio": 1.6343612334801763, "no_speech_prob": 6.3390393734152894e-06}, {"id": 703, "seek": 320266, "start": 3205.74, "end": 3206.74, "text": " We don't.", "tokens": [492, 500, 380, 13], "temperature": 0.0, "avg_logprob": -0.15405741192045666, "compression_ratio": 1.6343612334801763, "no_speech_prob": 6.3390393734152894e-06}, {"id": 704, "seek": 320266, "start": 3206.74, "end": 3208.98, "text": " PyTorch has that code already.", "tokens": [9953, 51, 284, 339, 575, 300, 3089, 1217, 13], "temperature": 0.0, "avg_logprob": -0.15405741192045666, "compression_ratio": 1.6343612334801763, "no_speech_prob": 6.3390393734152894e-06}, {"id": 705, "seek": 320266, "start": 3208.98, "end": 3209.98, "text": " It's called nn.sequential.", "tokens": [467, 311, 1219, 297, 77, 13, 11834, 2549, 13], "temperature": 0.0, "avg_logprob": -0.15405741192045666, "compression_ratio": 1.6343612334801763, "no_speech_prob": 6.3390393734152894e-06}, {"id": 706, "seek": 320266, "start": 3209.98, "end": 3210.98, "text": " Okay?", "tokens": [1033, 30], "temperature": 0.0, "avg_logprob": -0.15405741192045666, "compression_ratio": 1.6343612334801763, "no_speech_prob": 6.3390393734152894e-06}, {"id": 707, "seek": 320266, "start": 3210.98, "end": 3216.94, "text": " So we've now recreated nn.sequential.", "tokens": [407, 321, 600, 586, 850, 26559, 297, 77, 13, 11834, 2549, 13], "temperature": 0.0, "avg_logprob": -0.15405741192045666, "compression_ratio": 1.6343612334801763, "no_speech_prob": 6.3390393734152894e-06}, {"id": 708, "seek": 320266, "start": 3216.94, "end": 3219.8199999999997, "text": " And there it is doing the same thing.", "tokens": [400, 456, 309, 307, 884, 264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.15405741192045666, "compression_ratio": 1.6343612334801763, "no_speech_prob": 6.3390393734152894e-06}, {"id": 709, "seek": 320266, "start": 3219.8199999999997, "end": 3224.2999999999997, "text": " So again, we're not creating dumbed-down versions.", "tokens": [407, 797, 11, 321, 434, 406, 4084, 10316, 292, 12, 5093, 9606, 13], "temperature": 0.0, "avg_logprob": -0.15405741192045666, "compression_ratio": 1.6343612334801763, "no_speech_prob": 6.3390393734152894e-06}, {"id": 710, "seek": 320266, "start": 3224.2999999999997, "end": 3229.14, "text": " If you look at nn.sequential and you look at the source code and you look at forward,", "tokens": [759, 291, 574, 412, 297, 77, 13, 11834, 2549, 293, 291, 574, 412, 264, 4009, 3089, 293, 291, 574, 412, 2128, 11], "temperature": 0.0, "avg_logprob": -0.15405741192045666, "compression_ratio": 1.6343612334801763, "no_speech_prob": 6.3390393734152894e-06}, {"id": 711, "seek": 320266, "start": 3229.14, "end": 3230.14, "text": " it's just...", "tokens": [309, 311, 445, 485], "temperature": 0.0, "avg_logprob": -0.15405741192045666, "compression_ratio": 1.6343612334801763, "no_speech_prob": 6.3390393734152894e-06}, {"id": 712, "seek": 320266, "start": 3230.14, "end": 3232.22, "text": " Look, it's even the same name.", "tokens": [2053, 11, 309, 311, 754, 264, 912, 1315, 13], "temperature": 0.0, "avg_logprob": -0.15405741192045666, "compression_ratio": 1.6343612334801763, "no_speech_prob": 6.3390393734152894e-06}, {"id": 713, "seek": 323222, "start": 3232.22, "end": 3237.62, "text": " So go through each module itself, dot underscore modules, dot values, input equals module input,", "tokens": [407, 352, 807, 1184, 10088, 2564, 11, 5893, 37556, 16679, 11, 5893, 4190, 11, 4846, 6915, 10088, 4846, 11], "temperature": 0.0, "avg_logprob": -0.23456255594889322, "compression_ratio": 1.575, "no_speech_prob": 1.2606749805854633e-05}, {"id": 714, "seek": 323222, "start": 3237.62, "end": 3238.62, "text": " return input.", "tokens": [2736, 4846, 13], "temperature": 0.0, "avg_logprob": -0.23456255594889322, "compression_ratio": 1.575, "no_speech_prob": 1.2606749805854633e-05}, {"id": 715, "seek": 323222, "start": 3238.62, "end": 3239.62, "text": " Right?", "tokens": [1779, 30], "temperature": 0.0, "avg_logprob": -0.23456255594889322, "compression_ratio": 1.575, "no_speech_prob": 1.2606749805854633e-05}, {"id": 716, "seek": 323222, "start": 3239.62, "end": 3241.18, "text": " So that's their version.", "tokens": [407, 300, 311, 641, 3037, 13], "temperature": 0.0, "avg_logprob": -0.23456255594889322, "compression_ratio": 1.575, "no_speech_prob": 1.2606749805854633e-05}, {"id": 717, "seek": 323222, "start": 3241.18, "end": 3250.3799999999997, "text": " And remember, our version was basically the same.", "tokens": [400, 1604, 11, 527, 3037, 390, 1936, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.23456255594889322, "compression_ratio": 1.575, "no_speech_prob": 1.2606749805854633e-05}, {"id": 718, "seek": 323222, "start": 3250.3799999999997, "end": 3253.14, "text": " And we could even put it in something called underscore modules.", "tokens": [400, 321, 727, 754, 829, 309, 294, 746, 1219, 37556, 16679, 13], "temperature": 0.0, "avg_logprob": -0.23456255594889322, "compression_ratio": 1.575, "no_speech_prob": 1.2606749805854633e-05}, {"id": 719, "seek": 323222, "start": 3253.14, "end": 3255.22, "text": " So yeah.", "tokens": [407, 1338, 13], "temperature": 0.0, "avg_logprob": -0.23456255594889322, "compression_ratio": 1.575, "no_speech_prob": 1.2606749805854633e-05}, {"id": 720, "seek": 323222, "start": 3255.22, "end": 3258.7, "text": " That's all nn.sequential is doing for you.", "tokens": [663, 311, 439, 297, 77, 13, 11834, 2549, 307, 884, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.23456255594889322, "compression_ratio": 1.575, "no_speech_prob": 1.2606749805854633e-05}, {"id": 721, "seek": 323222, "start": 3258.7, "end": 3260.18, "text": " Okay?", "tokens": [1033, 30], "temperature": 0.0, "avg_logprob": -0.23456255594889322, "compression_ratio": 1.575, "no_speech_prob": 1.2606749805854633e-05}, {"id": 722, "seek": 326018, "start": 3260.18, "end": 3262.62, "text": " So we're making some progress.", "tokens": [407, 321, 434, 1455, 512, 4205, 13], "temperature": 0.0, "avg_logprob": -0.13302212894553006, "compression_ratio": 1.5462962962962963, "no_speech_prob": 1.0952942830044776e-05}, {"id": 723, "seek": 326018, "start": 3262.62, "end": 3265.02, "text": " It's less ugly than it used to be.", "tokens": [467, 311, 1570, 12246, 813, 309, 1143, 281, 312, 13], "temperature": 0.0, "avg_logprob": -0.13302212894553006, "compression_ratio": 1.5462962962962963, "no_speech_prob": 1.0952942830044776e-05}, {"id": 724, "seek": 326018, "start": 3265.02, "end": 3269.74, "text": " Still more ugly than we would like.", "tokens": [8291, 544, 12246, 813, 321, 576, 411, 13], "temperature": 0.0, "avg_logprob": -0.13302212894553006, "compression_ratio": 1.5462962962962963, "no_speech_prob": 1.0952942830044776e-05}, {"id": 725, "seek": 326018, "start": 3269.74, "end": 3277.22, "text": " This is where we got our fit function up to.", "tokens": [639, 307, 689, 321, 658, 527, 3318, 2445, 493, 281, 13], "temperature": 0.0, "avg_logprob": -0.13302212894553006, "compression_ratio": 1.5462962962962963, "no_speech_prob": 1.0952942830044776e-05}, {"id": 726, "seek": 326018, "start": 3277.22, "end": 3281.52, "text": " So let's try and simplify it a bit more.", "tokens": [407, 718, 311, 853, 293, 20460, 309, 257, 857, 544, 13], "temperature": 0.0, "avg_logprob": -0.13302212894553006, "compression_ratio": 1.5462962962962963, "no_speech_prob": 1.0952942830044776e-05}, {"id": 727, "seek": 326018, "start": 3281.52, "end": 3286.74, "text": " Let's replace all this torch.nograd for p and module.parameters, blah, blah, blah, with", "tokens": [961, 311, 7406, 439, 341, 27822, 13, 77, 664, 6206, 337, 280, 293, 10088, 13, 2181, 335, 6202, 11, 12288, 11, 12288, 11, 12288, 11, 365], "temperature": 0.0, "avg_logprob": -0.13302212894553006, "compression_ratio": 1.5462962962962963, "no_speech_prob": 1.0952942830044776e-05}, {"id": 728, "seek": 326018, "start": 3286.74, "end": 3289.06, "text": " something where we can just write those two lines of code.", "tokens": [746, 689, 321, 393, 445, 2464, 729, 732, 3876, 295, 3089, 13], "temperature": 0.0, "avg_logprob": -0.13302212894553006, "compression_ratio": 1.5462962962962963, "no_speech_prob": 1.0952942830044776e-05}, {"id": 729, "seek": 328906, "start": 3289.06, "end": 3290.74, "text": " That would be nice.", "tokens": [663, 576, 312, 1481, 13], "temperature": 0.0, "avg_logprob": -0.1166502603209845, "compression_ratio": 1.7244897959183674, "no_speech_prob": 2.9480038392648567e-06}, {"id": 730, "seek": 328906, "start": 3290.74, "end": 3294.1, "text": " So let's create a class called optimizer.", "tokens": [407, 718, 311, 1884, 257, 1508, 1219, 5028, 6545, 13], "temperature": 0.0, "avg_logprob": -0.1166502603209845, "compression_ratio": 1.7244897959183674, "no_speech_prob": 2.9480038392648567e-06}, {"id": 731, "seek": 328906, "start": 3294.1, "end": 3300.46, "text": " We're gonna pass in some parameters and store them away.", "tokens": [492, 434, 799, 1320, 294, 512, 9834, 293, 3531, 552, 1314, 13], "temperature": 0.0, "avg_logprob": -0.1166502603209845, "compression_ratio": 1.7244897959183674, "no_speech_prob": 2.9480038392648567e-06}, {"id": 732, "seek": 328906, "start": 3300.46, "end": 3306.1, "text": " And we're gonna pass in the learning rate and we're gonna store that away.", "tokens": [400, 321, 434, 799, 1320, 294, 264, 2539, 3314, 293, 321, 434, 799, 3531, 300, 1314, 13], "temperature": 0.0, "avg_logprob": -0.1166502603209845, "compression_ratio": 1.7244897959183674, "no_speech_prob": 2.9480038392648567e-06}, {"id": 733, "seek": 328906, "start": 3306.1, "end": 3312.12, "text": " And so if we're gonna be able to go opt.step, opt.step has to do this.", "tokens": [400, 370, 498, 321, 434, 799, 312, 1075, 281, 352, 2427, 13, 16792, 11, 2427, 13, 16792, 575, 281, 360, 341, 13], "temperature": 0.0, "avg_logprob": -0.1166502603209845, "compression_ratio": 1.7244897959183674, "no_speech_prob": 2.9480038392648567e-06}, {"id": 734, "seek": 328906, "start": 3312.12, "end": 3315.86, "text": " So here is step with torch.nograd.", "tokens": [407, 510, 307, 1823, 365, 27822, 13, 77, 664, 6206, 13], "temperature": 0.0, "avg_logprob": -0.1166502603209845, "compression_ratio": 1.7244897959183674, "no_speech_prob": 2.9480038392648567e-06}, {"id": 735, "seek": 328906, "start": 3315.86, "end": 3318.1, "text": " Go through each parameter and do that.", "tokens": [1037, 807, 1184, 13075, 293, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.1166502603209845, "compression_ratio": 1.7244897959183674, "no_speech_prob": 2.9480038392648567e-06}, {"id": 736, "seek": 331810, "start": 3318.1, "end": 3320.2999999999997, "text": " Okay, so let's just factor that out.", "tokens": [1033, 11, 370, 718, 311, 445, 5952, 300, 484, 13], "temperature": 0.0, "avg_logprob": -0.15520166467737267, "compression_ratio": 1.6814516129032258, "no_speech_prob": 2.4060902887867996e-06}, {"id": 737, "seek": 331810, "start": 3320.2999999999997, "end": 3325.98, "text": " And zero grad, we probably shouldn't actually go model.zerograd because it's actually possible", "tokens": [400, 4018, 2771, 11, 321, 1391, 4659, 380, 767, 352, 2316, 13, 32226, 7165, 570, 309, 311, 767, 1944], "temperature": 0.0, "avg_logprob": -0.15520166467737267, "compression_ratio": 1.6814516129032258, "no_speech_prob": 2.4060902887867996e-06}, {"id": 738, "seek": 331810, "start": 3325.98, "end": 3332.8399999999997, "text": " for the user, as you know, to say I don't want to include certain parameters in my optimizer.", "tokens": [337, 264, 4195, 11, 382, 291, 458, 11, 281, 584, 286, 500, 380, 528, 281, 4090, 1629, 9834, 294, 452, 5028, 6545, 13], "temperature": 0.0, "avg_logprob": -0.15520166467737267, "compression_ratio": 1.6814516129032258, "no_speech_prob": 2.4060902887867996e-06}, {"id": 739, "seek": 331810, "start": 3332.8399999999997, "end": 3336.48, "text": " So when we're doing gradual unfreezing and stuff.", "tokens": [407, 562, 321, 434, 884, 32890, 3971, 701, 8781, 293, 1507, 13], "temperature": 0.0, "avg_logprob": -0.15520166467737267, "compression_ratio": 1.6814516129032258, "no_speech_prob": 2.4060902887867996e-06}, {"id": 740, "seek": 331810, "start": 3336.48, "end": 3338.22, "text": " So really zero grad should actually do this.", "tokens": [407, 534, 4018, 2771, 820, 767, 360, 341, 13], "temperature": 0.0, "avg_logprob": -0.15520166467737267, "compression_ratio": 1.6814516129032258, "no_speech_prob": 2.4060902887867996e-06}, {"id": 741, "seek": 331810, "start": 3338.22, "end": 3342.7799999999997, "text": " It should go through the list of parameters that you asked to optimize and zero those", "tokens": [467, 820, 352, 807, 264, 1329, 295, 9834, 300, 291, 2351, 281, 19719, 293, 4018, 729], "temperature": 0.0, "avg_logprob": -0.15520166467737267, "compression_ratio": 1.6814516129032258, "no_speech_prob": 2.4060902887867996e-06}, {"id": 742, "seek": 331810, "start": 3342.7799999999997, "end": 3343.7799999999997, "text": " gradients.", "tokens": [2771, 2448, 13], "temperature": 0.0, "avg_logprob": -0.15520166467737267, "compression_ratio": 1.6814516129032258, "no_speech_prob": 2.4060902887867996e-06}, {"id": 743, "seek": 334378, "start": 3343.78, "end": 3349.82, "text": " So here we've now created something called optimizer and we can now grab our model.", "tokens": [407, 510, 321, 600, 586, 2942, 746, 1219, 5028, 6545, 293, 321, 393, 586, 4444, 527, 2316, 13], "temperature": 0.0, "avg_logprob": -0.19142610124013956, "compression_ratio": 1.7142857142857142, "no_speech_prob": 5.422145022748737e-06}, {"id": 744, "seek": 334378, "start": 3349.82, "end": 3353.7400000000002, "text": " And so remember that the model now we've created something called dot parameters.", "tokens": [400, 370, 1604, 300, 264, 2316, 586, 321, 600, 2942, 746, 1219, 5893, 9834, 13], "temperature": 0.0, "avg_logprob": -0.19142610124013956, "compression_ratio": 1.7142857142857142, "no_speech_prob": 5.422145022748737e-06}, {"id": 745, "seek": 334378, "start": 3353.7400000000002, "end": 3356.1000000000004, "text": " So we can pass that to our optimizer.", "tokens": [407, 321, 393, 1320, 300, 281, 527, 5028, 6545, 13], "temperature": 0.0, "avg_logprob": -0.19142610124013956, "compression_ratio": 1.7142857142857142, "no_speech_prob": 5.422145022748737e-06}, {"id": 746, "seek": 334378, "start": 3356.1000000000004, "end": 3360.36, "text": " And then we can now just go up dot step, up dot zero grad.", "tokens": [400, 550, 321, 393, 586, 445, 352, 493, 5893, 1823, 11, 493, 5893, 4018, 2771, 13], "temperature": 0.0, "avg_logprob": -0.19142610124013956, "compression_ratio": 1.7142857142857142, "no_speech_prob": 5.422145022748737e-06}, {"id": 747, "seek": 334378, "start": 3360.36, "end": 3362.9, "text": " And let's test it and it works.", "tokens": [400, 718, 311, 1500, 309, 293, 309, 1985, 13], "temperature": 0.0, "avg_logprob": -0.19142610124013956, "compression_ratio": 1.7142857142857142, "no_speech_prob": 5.422145022748737e-06}, {"id": 748, "seek": 334378, "start": 3362.9, "end": 3370.34, "text": " Okay, now of course, luckily for us, PyTorch already has these lines of code.", "tokens": [1033, 11, 586, 295, 1164, 11, 22880, 337, 505, 11, 9953, 51, 284, 339, 1217, 575, 613, 3876, 295, 3089, 13], "temperature": 0.0, "avg_logprob": -0.19142610124013956, "compression_ratio": 1.7142857142857142, "no_speech_prob": 5.422145022748737e-06}, {"id": 749, "seek": 337034, "start": 3370.34, "end": 3374.82, "text": " It's called optim.sgd.", "tokens": [467, 311, 1219, 2427, 332, 13, 82, 70, 67, 13], "temperature": 0.0, "avg_logprob": -0.18091260341175816, "compression_ratio": 1.6355555555555557, "no_speech_prob": 2.355149081267882e-05}, {"id": 750, "seek": 337034, "start": 3374.82, "end": 3383.4, "text": " Now optim.sgd does do a few more things, weight decay, momentum, stuff like that.", "tokens": [823, 2427, 332, 13, 82, 70, 67, 775, 360, 257, 1326, 544, 721, 11, 3364, 21039, 11, 11244, 11, 1507, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.18091260341175816, "compression_ratio": 1.6355555555555557, "no_speech_prob": 2.355149081267882e-05}, {"id": 751, "seek": 337034, "start": 3383.4, "end": 3384.4, "text": " So let's have a look.", "tokens": [407, 718, 311, 362, 257, 574, 13], "temperature": 0.0, "avg_logprob": -0.18091260341175816, "compression_ratio": 1.6355555555555557, "no_speech_prob": 2.355149081267882e-05}, {"id": 752, "seek": 337034, "start": 3384.4, "end": 3387.3, "text": " Here's optim.sgd and here's its step function.", "tokens": [1692, 311, 2427, 332, 13, 82, 70, 67, 293, 510, 311, 1080, 1823, 2445, 13], "temperature": 0.0, "avg_logprob": -0.18091260341175816, "compression_ratio": 1.6355555555555557, "no_speech_prob": 2.355149081267882e-05}, {"id": 753, "seek": 337034, "start": 3387.3, "end": 3390.8, "text": " So it's got weight decay, momentum, dampening, nest drop.", "tokens": [407, 309, 311, 658, 3364, 21039, 11, 11244, 11, 19498, 4559, 11, 15646, 3270, 13], "temperature": 0.0, "avg_logprob": -0.18091260341175816, "compression_ratio": 1.6355555555555557, "no_speech_prob": 2.355149081267882e-05}, {"id": 754, "seek": 337034, "start": 3390.8, "end": 3393.06, "text": " We're gonna see all these things very shortly.", "tokens": [492, 434, 799, 536, 439, 613, 721, 588, 13392, 13], "temperature": 0.0, "avg_logprob": -0.18091260341175816, "compression_ratio": 1.6355555555555557, "no_speech_prob": 2.355149081267882e-05}, {"id": 755, "seek": 337034, "start": 3393.06, "end": 3400.2200000000003, "text": " But basically all it does is it goes through each layer group and it does the exact thing", "tokens": [583, 1936, 439, 309, 775, 307, 309, 1709, 807, 1184, 4583, 1594, 293, 309, 775, 264, 1900, 551], "temperature": 0.0, "avg_logprob": -0.18091260341175816, "compression_ratio": 1.6355555555555557, "no_speech_prob": 2.355149081267882e-05}, {"id": 756, "seek": 340022, "start": 3400.22, "end": 3401.22, "text": " that we just see.", "tokens": [300, 321, 445, 536, 13], "temperature": 0.0, "avg_logprob": -0.1296792840057949, "compression_ratio": 1.6380090497737556, "no_speech_prob": 4.029362571600359e-06}, {"id": 757, "seek": 340022, "start": 3401.22, "end": 3405.2999999999997, "text": " Okay, so once you remove the momentum and stuff, and we're gonna be implementing this", "tokens": [1033, 11, 370, 1564, 291, 4159, 264, 11244, 293, 1507, 11, 293, 321, 434, 799, 312, 18114, 341], "temperature": 0.0, "avg_logprob": -0.1296792840057949, "compression_ratio": 1.6380090497737556, "no_speech_prob": 4.029362571600359e-06}, {"id": 758, "seek": 340022, "start": 3405.2999999999997, "end": 3409.1, "text": " in a much better way than PyTorch in very soon.", "tokens": [294, 257, 709, 1101, 636, 813, 9953, 51, 284, 339, 294, 588, 2321, 13], "temperature": 0.0, "avg_logprob": -0.1296792840057949, "compression_ratio": 1.6380090497737556, "no_speech_prob": 4.029362571600359e-06}, {"id": 759, "seek": 340022, "start": 3409.1, "end": 3418.12, "text": " So once you remove all that, there optim.sgd is exactly the same as our optimizer.", "tokens": [407, 1564, 291, 4159, 439, 300, 11, 456, 2427, 332, 13, 82, 70, 67, 307, 2293, 264, 912, 382, 527, 5028, 6545, 13], "temperature": 0.0, "avg_logprob": -0.1296792840057949, "compression_ratio": 1.6380090497737556, "no_speech_prob": 4.029362571600359e-06}, {"id": 760, "seek": 340022, "start": 3418.12, "end": 3421.06, "text": " So let's go ahead and use that instead.", "tokens": [407, 718, 311, 352, 2286, 293, 764, 300, 2602, 13], "temperature": 0.0, "avg_logprob": -0.1296792840057949, "compression_ratio": 1.6380090497737556, "no_speech_prob": 4.029362571600359e-06}, {"id": 761, "seek": 340022, "start": 3421.06, "end": 3425.98, "text": " And so it's kind of nice then if we're gonna use all the parameters of the model, let's", "tokens": [400, 370, 309, 311, 733, 295, 1481, 550, 498, 321, 434, 799, 764, 439, 264, 9834, 295, 264, 2316, 11, 718, 311], "temperature": 0.0, "avg_logprob": -0.1296792840057949, "compression_ratio": 1.6380090497737556, "no_speech_prob": 4.029362571600359e-06}, {"id": 762, "seek": 342598, "start": 3425.98, "end": 3431.7400000000002, "text": " just create a get model which creates our model and returns it as well as a SGD optimizer", "tokens": [445, 1884, 257, 483, 2316, 597, 7829, 527, 2316, 293, 11247, 309, 382, 731, 382, 257, 34520, 35, 5028, 6545], "temperature": 0.0, "avg_logprob": -0.15651770388142447, "compression_ratio": 1.5288461538461537, "no_speech_prob": 2.9022721719229594e-06}, {"id": 763, "seek": 342598, "start": 3431.7400000000002, "end": 3434.54, "text": " with all the parameters.", "tokens": [365, 439, 264, 9834, 13], "temperature": 0.0, "avg_logprob": -0.15651770388142447, "compression_ratio": 1.5288461538461537, "no_speech_prob": 2.9022721719229594e-06}, {"id": 764, "seek": 342598, "start": 3434.54, "end": 3440.22, "text": " And okay, there's our training loop.", "tokens": [400, 1392, 11, 456, 311, 527, 3097, 6367, 13], "temperature": 0.0, "avg_logprob": -0.15651770388142447, "compression_ratio": 1.5288461538461537, "no_speech_prob": 2.9022721719229594e-06}, {"id": 765, "seek": 342598, "start": 3440.22, "end": 3443.26, "text": " And seems to be working.", "tokens": [400, 2544, 281, 312, 1364, 13], "temperature": 0.0, "avg_logprob": -0.15651770388142447, "compression_ratio": 1.5288461538461537, "no_speech_prob": 2.9022721719229594e-06}, {"id": 766, "seek": 342598, "start": 3443.26, "end": 3445.86, "text": " It's nice to put tests in from time to time.", "tokens": [467, 311, 1481, 281, 829, 6921, 294, 490, 565, 281, 565, 13], "temperature": 0.0, "avg_logprob": -0.15651770388142447, "compression_ratio": 1.5288461538461537, "no_speech_prob": 2.9022721719229594e-06}, {"id": 767, "seek": 342598, "start": 3445.86, "end": 3451.08, "text": " And I like to put these tests in like, hey, my accuracy should be significantly better", "tokens": [400, 286, 411, 281, 829, 613, 6921, 294, 411, 11, 4177, 11, 452, 14170, 820, 312, 10591, 1101], "temperature": 0.0, "avg_logprob": -0.15651770388142447, "compression_ratio": 1.5288461538461537, "no_speech_prob": 2.9022721719229594e-06}, {"id": 768, "seek": 342598, "start": 3451.08, "end": 3452.08, "text": " than 50%.", "tokens": [813, 2625, 6856], "temperature": 0.0, "avg_logprob": -0.15651770388142447, "compression_ratio": 1.5288461538461537, "no_speech_prob": 2.9022721719229594e-06}, {"id": 769, "seek": 345208, "start": 3452.08, "end": 3459.1, "text": " You know, note that these kind of stochastic tests are highly imperfect in many ways.", "tokens": [509, 458, 11, 3637, 300, 613, 733, 295, 342, 8997, 2750, 6921, 366, 5405, 26714, 294, 867, 2098, 13], "temperature": 0.0, "avg_logprob": -0.11751123076503717, "compression_ratio": 1.6721991701244814, "no_speech_prob": 5.255203177512158e-06}, {"id": 770, "seek": 345208, "start": 3459.1, "end": 3463.2599999999998, "text": " It's theoretically possible it could fail because you got really unlucky.", "tokens": [467, 311, 29400, 1944, 309, 727, 3061, 570, 291, 658, 534, 38838, 13], "temperature": 0.0, "avg_logprob": -0.11751123076503717, "compression_ratio": 1.6721991701244814, "no_speech_prob": 5.255203177512158e-06}, {"id": 771, "seek": 345208, "start": 3463.2599999999998, "end": 3468.06, "text": " I know though that this is really, it's vanishingly unlikely to happen.", "tokens": [286, 458, 1673, 300, 341, 307, 534, 11, 309, 311, 3161, 3807, 356, 17518, 281, 1051, 13], "temperature": 0.0, "avg_logprob": -0.11751123076503717, "compression_ratio": 1.6721991701244814, "no_speech_prob": 5.255203177512158e-06}, {"id": 772, "seek": 345208, "start": 3468.06, "end": 3472.14, "text": " It's always much more than 90%.", "tokens": [467, 311, 1009, 709, 544, 813, 4289, 6856], "temperature": 0.0, "avg_logprob": -0.11751123076503717, "compression_ratio": 1.6721991701244814, "no_speech_prob": 5.255203177512158e-06}, {"id": 773, "seek": 345208, "start": 3472.14, "end": 3476.9, "text": " It's also possible that your code could be failing in a way that causes the accuracy", "tokens": [467, 311, 611, 1944, 300, 428, 3089, 727, 312, 18223, 294, 257, 636, 300, 7700, 264, 14170], "temperature": 0.0, "avg_logprob": -0.11751123076503717, "compression_ratio": 1.6721991701244814, "no_speech_prob": 5.255203177512158e-06}, {"id": 774, "seek": 345208, "start": 3476.9, "end": 3481.9, "text": " to be a bit lower than it should be, but not this low.", "tokens": [281, 312, 257, 857, 3126, 813, 309, 820, 312, 11, 457, 406, 341, 2295, 13], "temperature": 0.0, "avg_logprob": -0.11751123076503717, "compression_ratio": 1.6721991701244814, "no_speech_prob": 5.255203177512158e-06}, {"id": 775, "seek": 348190, "start": 3481.9, "end": 3488.1800000000003, "text": " But I still think it's a great idea to have these kinds of tests when you're doing machine", "tokens": [583, 286, 920, 519, 309, 311, 257, 869, 1558, 281, 362, 613, 3685, 295, 6921, 562, 291, 434, 884, 3479], "temperature": 0.0, "avg_logprob": -0.08471926936396847, "compression_ratio": 1.6007905138339922, "no_speech_prob": 7.4111026151513215e-06}, {"id": 776, "seek": 348190, "start": 3488.1800000000003, "end": 3493.9, "text": " learning because they give you a hint when something's going wrong.", "tokens": [2539, 570, 436, 976, 291, 257, 12075, 562, 746, 311, 516, 2085, 13], "temperature": 0.0, "avg_logprob": -0.08471926936396847, "compression_ratio": 1.6007905138339922, "no_speech_prob": 7.4111026151513215e-06}, {"id": 777, "seek": 348190, "start": 3493.9, "end": 3498.62, "text": " And you'll notice I don't set a random seed at any point.", "tokens": [400, 291, 603, 3449, 286, 500, 380, 992, 257, 4974, 8871, 412, 604, 935, 13], "temperature": 0.0, "avg_logprob": -0.08471926936396847, "compression_ratio": 1.6007905138339922, "no_speech_prob": 7.4111026151513215e-06}, {"id": 778, "seek": 348190, "start": 3498.62, "end": 3500.38, "text": " This is very intentional.", "tokens": [639, 307, 588, 21935, 13], "temperature": 0.0, "avg_logprob": -0.08471926936396847, "compression_ratio": 1.6007905138339922, "no_speech_prob": 7.4111026151513215e-06}, {"id": 779, "seek": 348190, "start": 3500.38, "end": 3506.26, "text": " I really like it that if there's variation going on when I run my model at different", "tokens": [286, 534, 411, 309, 300, 498, 456, 311, 12990, 516, 322, 562, 286, 1190, 452, 2316, 412, 819], "temperature": 0.0, "avg_logprob": -0.08471926936396847, "compression_ratio": 1.6007905138339922, "no_speech_prob": 7.4111026151513215e-06}, {"id": 780, "seek": 348190, "start": 3506.26, "end": 3508.1800000000003, "text": " times, I wanna see it.", "tokens": [1413, 11, 286, 1948, 536, 309, 13], "temperature": 0.0, "avg_logprob": -0.08471926936396847, "compression_ratio": 1.6007905138339922, "no_speech_prob": 7.4111026151513215e-06}, {"id": 781, "seek": 348190, "start": 3508.1800000000003, "end": 3511.1600000000003, "text": " I don't want it to be hidden away behind a fixed seed.", "tokens": [286, 500, 380, 528, 309, 281, 312, 7633, 1314, 2261, 257, 6806, 8871, 13], "temperature": 0.0, "avg_logprob": -0.08471926936396847, "compression_ratio": 1.6007905138339922, "no_speech_prob": 7.4111026151513215e-06}, {"id": 782, "seek": 351116, "start": 3511.16, "end": 3516.98, "text": " So there's a big push in science for reproducible science, which is great for many reasons,", "tokens": [407, 456, 311, 257, 955, 2944, 294, 3497, 337, 11408, 32128, 3497, 11, 597, 307, 869, 337, 867, 4112, 11], "temperature": 0.0, "avg_logprob": -0.11638145057522521, "compression_ratio": 1.6694915254237288, "no_speech_prob": 1.6536440625714022e-06}, {"id": 783, "seek": 351116, "start": 3516.98, "end": 3519.18, "text": " but it's not how you should develop your models.", "tokens": [457, 309, 311, 406, 577, 291, 820, 1499, 428, 5245, 13], "temperature": 0.0, "avg_logprob": -0.11638145057522521, "compression_ratio": 1.6694915254237288, "no_speech_prob": 1.6536440625714022e-06}, {"id": 784, "seek": 351116, "start": 3519.18, "end": 3524.8999999999996, "text": " When you're developing your models, you wanna have a kind of good intuitive sense of what", "tokens": [1133, 291, 434, 6416, 428, 5245, 11, 291, 1948, 362, 257, 733, 295, 665, 21769, 2020, 295, 437], "temperature": 0.0, "avg_logprob": -0.11638145057522521, "compression_ratio": 1.6694915254237288, "no_speech_prob": 1.6536440625714022e-06}, {"id": 785, "seek": 351116, "start": 3524.8999999999996, "end": 3528.7999999999997, "text": " bits are stable and what bits are unstable and how much variation do you expect.", "tokens": [9239, 366, 8351, 293, 437, 9239, 366, 23742, 293, 577, 709, 12990, 360, 291, 2066, 13], "temperature": 0.0, "avg_logprob": -0.11638145057522521, "compression_ratio": 1.6694915254237288, "no_speech_prob": 1.6536440625714022e-06}, {"id": 786, "seek": 351116, "start": 3528.7999999999997, "end": 3538.3199999999997, "text": " And so if you have a test which fails one every 100 times, it's good to know that.", "tokens": [400, 370, 498, 291, 362, 257, 1500, 597, 18199, 472, 633, 2319, 1413, 11, 309, 311, 665, 281, 458, 300, 13], "temperature": 0.0, "avg_logprob": -0.11638145057522521, "compression_ratio": 1.6694915254237288, "no_speech_prob": 1.6536440625714022e-06}, {"id": 787, "seek": 353832, "start": 3538.32, "end": 3541.6800000000003, "text": " And so in the fast AI code, there's lots of tests like that.", "tokens": [400, 370, 294, 264, 2370, 7318, 3089, 11, 456, 311, 3195, 295, 6921, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.10969175720214844, "compression_ratio": 1.7173913043478262, "no_speech_prob": 7.296085641428363e-06}, {"id": 788, "seek": 353832, "start": 3541.6800000000003, "end": 3545.38, "text": " And so then sometimes there'll be a test that fails.", "tokens": [400, 370, 550, 2171, 456, 603, 312, 257, 1500, 300, 18199, 13], "temperature": 0.0, "avg_logprob": -0.10969175720214844, "compression_ratio": 1.7173913043478262, "no_speech_prob": 7.296085641428363e-06}, {"id": 789, "seek": 353832, "start": 3545.38, "end": 3548.9, "text": " It's nothing particularly to do with the push that just happened, but it's really helpful", "tokens": [467, 311, 1825, 4098, 281, 360, 365, 264, 2944, 300, 445, 2011, 11, 457, 309, 311, 534, 4961], "temperature": 0.0, "avg_logprob": -0.10969175720214844, "compression_ratio": 1.7173913043478262, "no_speech_prob": 7.296085641428363e-06}, {"id": 790, "seek": 353832, "start": 3548.9, "end": 3553.52, "text": " for us, cuz then we can look at it and be like, this thing we thought should pretty", "tokens": [337, 505, 11, 11910, 550, 321, 393, 574, 412, 309, 293, 312, 411, 11, 341, 551, 321, 1194, 820, 1238], "temperature": 0.0, "avg_logprob": -0.10969175720214844, "compression_ratio": 1.7173913043478262, "no_speech_prob": 7.296085641428363e-06}, {"id": 791, "seek": 353832, "start": 3553.52, "end": 3555.9, "text": " much always be true, sometimes isn't true.", "tokens": [709, 1009, 312, 2074, 11, 2171, 1943, 380, 2074, 13], "temperature": 0.0, "avg_logprob": -0.10969175720214844, "compression_ratio": 1.7173913043478262, "no_speech_prob": 7.296085641428363e-06}, {"id": 792, "seek": 353832, "start": 3555.9, "end": 3559.98, "text": " And then we'll go back and we'll deeply study why that is and figure out how to make it", "tokens": [400, 550, 321, 603, 352, 646, 293, 321, 603, 8760, 2979, 983, 300, 307, 293, 2573, 484, 577, 281, 652, 309], "temperature": 0.0, "avg_logprob": -0.10969175720214844, "compression_ratio": 1.7173913043478262, "no_speech_prob": 7.296085641428363e-06}, {"id": 793, "seek": 353832, "start": 3559.98, "end": 3563.76, "text": " more stable and how to make it reliably pass that test.", "tokens": [544, 8351, 293, 577, 281, 652, 309, 49927, 1320, 300, 1500, 13], "temperature": 0.0, "avg_logprob": -0.10969175720214844, "compression_ratio": 1.7173913043478262, "no_speech_prob": 7.296085641428363e-06}, {"id": 794, "seek": 356376, "start": 3563.76, "end": 3569.94, "text": " So this is a kind of a controversial kind of test to have, but something that I found", "tokens": [407, 341, 307, 257, 733, 295, 257, 17323, 733, 295, 1500, 281, 362, 11, 457, 746, 300, 286, 1352], "temperature": 0.0, "avg_logprob": -0.1344070592202431, "compression_ratio": 1.825910931174089, "no_speech_prob": 9.97235929389717e-06}, {"id": 795, "seek": 356376, "start": 3569.94, "end": 3573.1800000000003, "text": " in practice is very, very, very helpful.", "tokens": [294, 3124, 307, 588, 11, 588, 11, 588, 4961, 13], "temperature": 0.0, "avg_logprob": -0.1344070592202431, "compression_ratio": 1.825910931174089, "no_speech_prob": 9.97235929389717e-06}, {"id": 796, "seek": 356376, "start": 3573.1800000000003, "end": 3577.1800000000003, "text": " It's not complete and it's not totally automated and it's imperfect in many ways, but it's", "tokens": [467, 311, 406, 3566, 293, 309, 311, 406, 3879, 18473, 293, 309, 311, 26714, 294, 867, 2098, 11, 457, 309, 311], "temperature": 0.0, "avg_logprob": -0.1344070592202431, "compression_ratio": 1.825910931174089, "no_speech_prob": 9.97235929389717e-06}, {"id": 797, "seek": 356376, "start": 3577.1800000000003, "end": 3578.1800000000003, "text": " nonetheless helpful.", "tokens": [26756, 4961, 13], "temperature": 0.0, "avg_logprob": -0.1344070592202431, "compression_ratio": 1.825910931174089, "no_speech_prob": 9.97235929389717e-06}, {"id": 798, "seek": 356376, "start": 3578.1800000000003, "end": 3582.42, "text": " Okay, let's get rid of these two lines of code.", "tokens": [1033, 11, 718, 311, 483, 3973, 295, 613, 732, 3876, 295, 3089, 13], "temperature": 0.0, "avg_logprob": -0.1344070592202431, "compression_ratio": 1.825910931174089, "no_speech_prob": 9.97235929389717e-06}, {"id": 799, "seek": 356376, "start": 3582.42, "end": 3586.98, "text": " These were the lines of code that grabbed our x mini batch from the training set and", "tokens": [1981, 645, 264, 3876, 295, 3089, 300, 18607, 527, 2031, 8382, 15245, 490, 264, 3097, 992, 293], "temperature": 0.0, "avg_logprob": -0.1344070592202431, "compression_ratio": 1.825910931174089, "no_speech_prob": 9.97235929389717e-06}, {"id": 800, "seek": 356376, "start": 3586.98, "end": 3589.84, "text": " the y mini batch from the training set.", "tokens": [264, 288, 8382, 15245, 490, 264, 3097, 992, 13], "temperature": 0.0, "avg_logprob": -0.1344070592202431, "compression_ratio": 1.825910931174089, "no_speech_prob": 9.97235929389717e-06}, {"id": 801, "seek": 356376, "start": 3589.84, "end": 3592.28, "text": " Let's do them both in one line of code.", "tokens": [961, 311, 360, 552, 1293, 294, 472, 1622, 295, 3089, 13], "temperature": 0.0, "avg_logprob": -0.1344070592202431, "compression_ratio": 1.825910931174089, "no_speech_prob": 9.97235929389717e-06}, {"id": 802, "seek": 359228, "start": 3592.28, "end": 3595.26, "text": " So it'd be nice to have one line of code where we have some kind of object where we can pass", "tokens": [407, 309, 1116, 312, 1481, 281, 362, 472, 1622, 295, 3089, 689, 321, 362, 512, 733, 295, 2657, 689, 321, 393, 1320], "temperature": 0.0, "avg_logprob": -0.158865336499183, "compression_ratio": 1.7, "no_speech_prob": 6.643059805355733e-06}, {"id": 803, "seek": 359228, "start": 3595.26, "end": 3600.2000000000003, "text": " in the indexes we want and get back both x and y.", "tokens": [294, 264, 8186, 279, 321, 528, 293, 483, 646, 1293, 2031, 293, 288, 13], "temperature": 0.0, "avg_logprob": -0.158865336499183, "compression_ratio": 1.7, "no_speech_prob": 6.643059805355733e-06}, {"id": 804, "seek": 359228, "start": 3600.2000000000003, "end": 3602.82, "text": " And that's called a data set, as you know.", "tokens": [400, 300, 311, 1219, 257, 1412, 992, 11, 382, 291, 458, 13], "temperature": 0.0, "avg_logprob": -0.158865336499183, "compression_ratio": 1.7, "no_speech_prob": 6.643059805355733e-06}, {"id": 805, "seek": 359228, "start": 3602.82, "end": 3604.38, "text": " So here's our data set class.", "tokens": [407, 510, 311, 527, 1412, 992, 1508, 13], "temperature": 0.0, "avg_logprob": -0.158865336499183, "compression_ratio": 1.7, "no_speech_prob": 6.643059805355733e-06}, {"id": 806, "seek": 359228, "start": 3604.38, "end": 3609.2200000000003, "text": " Again, not inheriting from anything, it's all from scratch, pluripython.", "tokens": [3764, 11, 406, 9484, 1748, 490, 1340, 11, 309, 311, 439, 490, 8459, 11, 499, 374, 647, 88, 11943, 13], "temperature": 0.0, "avg_logprob": -0.158865336499183, "compression_ratio": 1.7, "no_speech_prob": 6.643059805355733e-06}, {"id": 807, "seek": 359228, "start": 3609.2200000000003, "end": 3613.6600000000003, "text": " We initialize it by passing in the x and the y, we'll store them away.", "tokens": [492, 5883, 1125, 309, 538, 8437, 294, 264, 2031, 293, 264, 288, 11, 321, 603, 3531, 552, 1314, 13], "temperature": 0.0, "avg_logprob": -0.158865336499183, "compression_ratio": 1.7, "no_speech_prob": 6.643059805355733e-06}, {"id": 808, "seek": 359228, "start": 3613.6600000000003, "end": 3616.1200000000003, "text": " It's very handy to have a length.", "tokens": [467, 311, 588, 13239, 281, 362, 257, 4641, 13], "temperature": 0.0, "avg_logprob": -0.158865336499183, "compression_ratio": 1.7, "no_speech_prob": 6.643059805355733e-06}, {"id": 809, "seek": 359228, "start": 3616.1200000000003, "end": 3617.34, "text": " Hopefully you know by now.", "tokens": [10429, 291, 458, 538, 586, 13], "temperature": 0.0, "avg_logprob": -0.158865336499183, "compression_ratio": 1.7, "no_speech_prob": 6.643059805355733e-06}, {"id": 810, "seek": 359228, "start": 3617.34, "end": 3622.0, "text": " If you don't, then now's a good time to realize that Dunderlen is the thing that lets you", "tokens": [759, 291, 500, 380, 11, 550, 586, 311, 257, 665, 565, 281, 4325, 300, 413, 6617, 6698, 307, 264, 551, 300, 6653, 291], "temperature": 0.0, "avg_logprob": -0.158865336499183, "compression_ratio": 1.7, "no_speech_prob": 6.643059805355733e-06}, {"id": 811, "seek": 362200, "start": 3622.0, "end": 3626.06, "text": " go len of something in Python and have it work.", "tokens": [352, 40116, 295, 746, 294, 15329, 293, 362, 309, 589, 13], "temperature": 0.0, "avg_logprob": -0.12618464010733146, "compression_ratio": 1.6294642857142858, "no_speech_prob": 9.368314749735873e-06}, {"id": 812, "seek": 362200, "start": 3626.06, "end": 3627.14, "text": " That's what len will call.", "tokens": [663, 311, 437, 40116, 486, 818, 13], "temperature": 0.0, "avg_logprob": -0.12618464010733146, "compression_ratio": 1.6294642857142858, "no_speech_prob": 9.368314749735873e-06}, {"id": 813, "seek": 362200, "start": 3627.14, "end": 3629.94, "text": " So now we've got the length of our data set.", "tokens": [407, 586, 321, 600, 658, 264, 4641, 295, 527, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.12618464010733146, "compression_ratio": 1.6294642857142858, "no_speech_prob": 9.368314749735873e-06}, {"id": 814, "seek": 362200, "start": 3629.94, "end": 3635.06, "text": " And Dundergetitem is a thing that when you index into it, it will return that.", "tokens": [400, 413, 6617, 847, 270, 443, 307, 257, 551, 300, 562, 291, 8186, 666, 309, 11, 309, 486, 2736, 300, 13], "temperature": 0.0, "avg_logprob": -0.12618464010733146, "compression_ratio": 1.6294642857142858, "no_speech_prob": 9.368314749735873e-06}, {"id": 815, "seek": 362200, "start": 3635.06, "end": 3640.18, "text": " And so we just return the tuple of x i and y i.", "tokens": [400, 370, 321, 445, 2736, 264, 2604, 781, 295, 2031, 741, 293, 288, 741, 13], "temperature": 0.0, "avg_logprob": -0.12618464010733146, "compression_ratio": 1.6294642857142858, "no_speech_prob": 9.368314749735873e-06}, {"id": 816, "seek": 362200, "start": 3640.18, "end": 3647.26, "text": " So let's go ahead and create a data set for our training set and our validation set.", "tokens": [407, 718, 311, 352, 2286, 293, 1884, 257, 1412, 992, 337, 527, 3097, 992, 293, 527, 24071, 992, 13], "temperature": 0.0, "avg_logprob": -0.12618464010733146, "compression_ratio": 1.6294642857142858, "no_speech_prob": 9.368314749735873e-06}, {"id": 817, "seek": 362200, "start": 3647.26, "end": 3650.24, "text": " Check that the lengths are right.", "tokens": [6881, 300, 264, 26329, 366, 558, 13], "temperature": 0.0, "avg_logprob": -0.12618464010733146, "compression_ratio": 1.6294642857142858, "no_speech_prob": 9.368314749735873e-06}, {"id": 818, "seek": 365024, "start": 3650.24, "end": 3656.8199999999997, "text": " Check the first few values, make sure they all seem sane.", "tokens": [6881, 264, 700, 1326, 4190, 11, 652, 988, 436, 439, 1643, 45610, 13], "temperature": 0.0, "avg_logprob": -0.11882577862655908, "compression_ratio": 1.696035242290749, "no_speech_prob": 9.223281267622951e-06}, {"id": 819, "seek": 365024, "start": 3656.8199999999997, "end": 3658.8599999999997, "text": " Now we'll grab our model.", "tokens": [823, 321, 603, 4444, 527, 2316, 13], "temperature": 0.0, "avg_logprob": -0.11882577862655908, "compression_ratio": 1.696035242290749, "no_speech_prob": 9.223281267622951e-06}, {"id": 820, "seek": 365024, "start": 3658.8599999999997, "end": 3663.18, "text": " And as I said, we will replace those two lines of code with one.", "tokens": [400, 382, 286, 848, 11, 321, 486, 7406, 729, 732, 3876, 295, 3089, 365, 472, 13], "temperature": 0.0, "avg_logprob": -0.11882577862655908, "compression_ratio": 1.696035242290749, "no_speech_prob": 9.223281267622951e-06}, {"id": 821, "seek": 365024, "start": 3663.18, "end": 3665.74, "text": " And so at this point, our training loop's getting quite neat.", "tokens": [400, 370, 412, 341, 935, 11, 527, 3097, 6367, 311, 1242, 1596, 10654, 13], "temperature": 0.0, "avg_logprob": -0.11882577862655908, "compression_ratio": 1.696035242290749, "no_speech_prob": 9.223281267622951e-06}, {"id": 822, "seek": 365024, "start": 3665.74, "end": 3670.8999999999996, "text": " It's not as neat as it could be, but it's getting quite neat.", "tokens": [467, 311, 406, 382, 10654, 382, 309, 727, 312, 11, 457, 309, 311, 1242, 1596, 10654, 13], "temperature": 0.0, "avg_logprob": -0.11882577862655908, "compression_ratio": 1.696035242290749, "no_speech_prob": 9.223281267622951e-06}, {"id": 823, "seek": 365024, "start": 3670.8999999999996, "end": 3675.02, "text": " So the next thing we're going to do, so that's a data set.", "tokens": [407, 264, 958, 551, 321, 434, 516, 281, 360, 11, 370, 300, 311, 257, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.11882577862655908, "compression_ratio": 1.696035242290749, "no_speech_prob": 9.223281267622951e-06}, {"id": 824, "seek": 365024, "start": 3675.02, "end": 3677.7799999999997, "text": " Next thing we're going to do is create a data loader.", "tokens": [3087, 551, 321, 434, 516, 281, 360, 307, 1884, 257, 1412, 3677, 260, 13], "temperature": 0.0, "avg_logprob": -0.11882577862655908, "compression_ratio": 1.696035242290749, "no_speech_prob": 9.223281267622951e-06}, {"id": 825, "seek": 367778, "start": 3677.78, "end": 3681.6200000000003, "text": " This is what the start of our training loop looked like before.", "tokens": [639, 307, 437, 264, 722, 295, 527, 3097, 6367, 2956, 411, 949, 13], "temperature": 0.0, "avg_logprob": -0.12748032524472191, "compression_ratio": 1.4975369458128078, "no_speech_prob": 2.6015923140221275e-06}, {"id": 826, "seek": 367778, "start": 3681.6200000000003, "end": 3686.5, "text": " And let's replace it with this single line of code.", "tokens": [400, 718, 311, 7406, 309, 365, 341, 2167, 1622, 295, 3089, 13], "temperature": 0.0, "avg_logprob": -0.12748032524472191, "compression_ratio": 1.4975369458128078, "no_speech_prob": 2.6015923140221275e-06}, {"id": 827, "seek": 367778, "start": 3686.5, "end": 3693.1800000000003, "text": " So to do that, we're going to have a class that takes a data set and a batch size and", "tokens": [407, 281, 360, 300, 11, 321, 434, 516, 281, 362, 257, 1508, 300, 2516, 257, 1412, 992, 293, 257, 15245, 2744, 293], "temperature": 0.0, "avg_logprob": -0.12748032524472191, "compression_ratio": 1.4975369458128078, "no_speech_prob": 2.6015923140221275e-06}, {"id": 828, "seek": 367778, "start": 3693.1800000000003, "end": 3695.2200000000003, "text": " stores them away.", "tokens": [9512, 552, 1314, 13], "temperature": 0.0, "avg_logprob": -0.12748032524472191, "compression_ratio": 1.4975369458128078, "no_speech_prob": 2.6015923140221275e-06}, {"id": 829, "seek": 367778, "start": 3695.2200000000003, "end": 3702.7400000000002, "text": " And when you go for blah in blah, behind the scenes in Python, it calls Dunderitter.", "tokens": [400, 562, 291, 352, 337, 12288, 294, 12288, 11, 2261, 264, 8026, 294, 15329, 11, 309, 5498, 413, 6617, 3904, 13], "temperature": 0.0, "avg_logprob": -0.12748032524472191, "compression_ratio": 1.4975369458128078, "no_speech_prob": 2.6015923140221275e-06}, {"id": 830, "seek": 370274, "start": 3702.74, "end": 3707.9399999999996, "text": " And so what we're going to do is we're going to loop through range from zero up to the", "tokens": [400, 370, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 6367, 807, 3613, 490, 4018, 493, 281, 264], "temperature": 0.0, "avg_logprob": -0.10816864655396649, "compression_ratio": 1.575221238938053, "no_speech_prob": 5.093655545351794e-06}, {"id": 831, "seek": 370274, "start": 3707.9399999999996, "end": 3712.18, "text": " size of the data set, jumping up batch size at a time.", "tokens": [2744, 295, 264, 1412, 992, 11, 11233, 493, 15245, 2744, 412, 257, 565, 13], "temperature": 0.0, "avg_logprob": -0.10816864655396649, "compression_ratio": 1.575221238938053, "no_speech_prob": 5.093655545351794e-06}, {"id": 832, "seek": 370274, "start": 3712.18, "end": 3716.1, "text": " So 0, 64, 128, et cetera, up to 50,000.", "tokens": [407, 1958, 11, 12145, 11, 29810, 11, 1030, 11458, 11, 493, 281, 2625, 11, 1360, 13], "temperature": 0.0, "avg_logprob": -0.10816864655396649, "compression_ratio": 1.575221238938053, "no_speech_prob": 5.093655545351794e-06}, {"id": 833, "seek": 370274, "start": 3716.1, "end": 3722.8999999999996, "text": " And each time we go through, we will yield our data set at an index starting at i and", "tokens": [400, 1184, 565, 321, 352, 807, 11, 321, 486, 11257, 527, 1412, 992, 412, 364, 8186, 2891, 412, 741, 293], "temperature": 0.0, "avg_logprob": -0.10816864655396649, "compression_ratio": 1.575221238938053, "no_speech_prob": 5.093655545351794e-06}, {"id": 834, "seek": 370274, "start": 3722.8999999999996, "end": 3727.8599999999997, "text": " ending at i plus self.batch size.", "tokens": [8121, 412, 741, 1804, 2698, 13, 65, 852, 2744, 13], "temperature": 0.0, "avg_logprob": -0.10816864655396649, "compression_ratio": 1.575221238938053, "no_speech_prob": 5.093655545351794e-06}, {"id": 835, "seek": 370274, "start": 3727.8599999999997, "end": 3730.02, "text": " Probably quite a lot of you haven't seen yield before.", "tokens": [9210, 1596, 257, 688, 295, 291, 2378, 380, 1612, 11257, 949, 13], "temperature": 0.0, "avg_logprob": -0.10816864655396649, "compression_ratio": 1.575221238938053, "no_speech_prob": 5.093655545351794e-06}, {"id": 836, "seek": 373002, "start": 3730.02, "end": 3736.18, "text": " It's an incredibly useful concept.", "tokens": [467, 311, 364, 6252, 4420, 3410, 13], "temperature": 0.0, "avg_logprob": -0.09893767708226255, "compression_ratio": 1.7117117117117118, "no_speech_prob": 6.853641025372781e-06}, {"id": 837, "seek": 373002, "start": 3736.18, "end": 3738.98, "text": " If you're really interested, it's something called a coroutine.", "tokens": [759, 291, 434, 534, 3102, 11, 309, 311, 746, 1219, 257, 1181, 45075, 13], "temperature": 0.0, "avg_logprob": -0.09893767708226255, "compression_ratio": 1.7117117117117118, "no_speech_prob": 6.853641025372781e-06}, {"id": 838, "seek": 373002, "start": 3738.98, "end": 3742.66, "text": " It's basically this really interesting idea that you can have a function that doesn't", "tokens": [467, 311, 1936, 341, 534, 1880, 1558, 300, 291, 393, 362, 257, 2445, 300, 1177, 380], "temperature": 0.0, "avg_logprob": -0.09893767708226255, "compression_ratio": 1.7117117117117118, "no_speech_prob": 6.853641025372781e-06}, {"id": 839, "seek": 373002, "start": 3742.66, "end": 3747.7, "text": " return just one thing once, but can return lots of things, and you can kind of ask for", "tokens": [2736, 445, 472, 551, 1564, 11, 457, 393, 2736, 3195, 295, 721, 11, 293, 291, 393, 733, 295, 1029, 337], "temperature": 0.0, "avg_logprob": -0.09893767708226255, "compression_ratio": 1.7117117117117118, "no_speech_prob": 6.853641025372781e-06}, {"id": 840, "seek": 373002, "start": 3747.7, "end": 3748.7, "text": " it lots of times.", "tokens": [309, 3195, 295, 1413, 13], "temperature": 0.0, "avg_logprob": -0.09893767708226255, "compression_ratio": 1.7117117117117118, "no_speech_prob": 6.853641025372781e-06}, {"id": 841, "seek": 373002, "start": 3748.7, "end": 3756.5, "text": " So the way these iterators work in Python is that when you call this, it basically returns", "tokens": [407, 264, 636, 613, 17138, 3391, 589, 294, 15329, 307, 300, 562, 291, 818, 341, 11, 309, 1936, 11247], "temperature": 0.0, "avg_logprob": -0.09893767708226255, "compression_ratio": 1.7117117117117118, "no_speech_prob": 6.853641025372781e-06}, {"id": 842, "seek": 375650, "start": 3756.5, "end": 3761.94, "text": " something which you can then call next on lots of times, and each time you call next,", "tokens": [746, 597, 291, 393, 550, 818, 958, 322, 3195, 295, 1413, 11, 293, 1184, 565, 291, 818, 958, 11], "temperature": 0.0, "avg_logprob": -0.11582166270205849, "compression_ratio": 1.6536964980544746, "no_speech_prob": 9.817057616601232e-06}, {"id": 843, "seek": 375650, "start": 3761.94, "end": 3766.98, "text": " it will return the next thing that is yielded.", "tokens": [309, 486, 2736, 264, 958, 551, 300, 307, 11257, 292, 13], "temperature": 0.0, "avg_logprob": -0.11582166270205849, "compression_ratio": 1.6536964980544746, "no_speech_prob": 9.817057616601232e-06}, {"id": 844, "seek": 375650, "start": 3766.98, "end": 3773.8, "text": " So I don't have time to explain coroutines in detail here, but it's really worth looking", "tokens": [407, 286, 500, 380, 362, 565, 281, 2903, 1181, 346, 1652, 294, 2607, 510, 11, 457, 309, 311, 534, 3163, 1237], "temperature": 0.0, "avg_logprob": -0.11582166270205849, "compression_ratio": 1.6536964980544746, "no_speech_prob": 9.817057616601232e-06}, {"id": 845, "seek": 375650, "start": 3773.8, "end": 3774.8, "text": " up and learning about.", "tokens": [493, 293, 2539, 466, 13], "temperature": 0.0, "avg_logprob": -0.11582166270205849, "compression_ratio": 1.6536964980544746, "no_speech_prob": 9.817057616601232e-06}, {"id": 846, "seek": 375650, "start": 3774.8, "end": 3776.3, "text": " We'll be using them lots.", "tokens": [492, 603, 312, 1228, 552, 3195, 13], "temperature": 0.0, "avg_logprob": -0.11582166270205849, "compression_ratio": 1.6536964980544746, "no_speech_prob": 9.817057616601232e-06}, {"id": 847, "seek": 375650, "start": 3776.3, "end": 3778.86, "text": " They're a super valuable thing.", "tokens": [814, 434, 257, 1687, 8263, 551, 13], "temperature": 0.0, "avg_logprob": -0.11582166270205849, "compression_ratio": 1.6536964980544746, "no_speech_prob": 9.817057616601232e-06}, {"id": 848, "seek": 375650, "start": 3778.86, "end": 3780.66, "text": " And it's not just for data science.", "tokens": [400, 309, 311, 406, 445, 337, 1412, 3497, 13], "temperature": 0.0, "avg_logprob": -0.11582166270205849, "compression_ratio": 1.6536964980544746, "no_speech_prob": 9.817057616601232e-06}, {"id": 849, "seek": 375650, "start": 3780.66, "end": 3785.5, "text": " They're really handy for things like network programming, web apps, stuff like that as", "tokens": [814, 434, 534, 13239, 337, 721, 411, 3209, 9410, 11, 3670, 7733, 11, 1507, 411, 300, 382], "temperature": 0.0, "avg_logprob": -0.11582166270205849, "compression_ratio": 1.6536964980544746, "no_speech_prob": 9.817057616601232e-06}, {"id": 850, "seek": 378550, "start": 3785.5, "end": 3789.46, "text": " well, so well worth being familiar with yield in Python.", "tokens": [731, 11, 370, 731, 3163, 885, 4963, 365, 11257, 294, 15329, 13], "temperature": 0.0, "avg_logprob": -0.1582906125771879, "compression_ratio": 1.5683760683760684, "no_speech_prob": 1.0782698154798709e-05}, {"id": 851, "seek": 378550, "start": 3789.46, "end": 3794.26, "text": " And nowadays, most programming languages have something like this, so you'll be able to", "tokens": [400, 13434, 11, 881, 9410, 8650, 362, 746, 411, 341, 11, 370, 291, 603, 312, 1075, 281], "temperature": 0.0, "avg_logprob": -0.1582906125771879, "compression_ratio": 1.5683760683760684, "no_speech_prob": 1.0782698154798709e-05}, {"id": 852, "seek": 378550, "start": 3794.26, "end": 3797.18, "text": " take it to wherever you go.", "tokens": [747, 309, 281, 8660, 291, 352, 13], "temperature": 0.0, "avg_logprob": -0.1582906125771879, "compression_ratio": 1.5683760683760684, "no_speech_prob": 1.0782698154798709e-05}, {"id": 853, "seek": 378550, "start": 3797.18, "end": 3798.86, "text": " So now we have a data loader.", "tokens": [407, 586, 321, 362, 257, 1412, 3677, 260, 13], "temperature": 0.0, "avg_logprob": -0.1582906125771879, "compression_ratio": 1.5683760683760684, "no_speech_prob": 1.0782698154798709e-05}, {"id": 854, "seek": 378550, "start": 3798.86, "end": 3804.74, "text": " We can create a training one and a validation one, and this is how we do it.", "tokens": [492, 393, 1884, 257, 3097, 472, 293, 257, 24071, 472, 11, 293, 341, 307, 577, 321, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.1582906125771879, "compression_ratio": 1.5683760683760684, "no_speech_prob": 1.0782698154798709e-05}, {"id": 855, "seek": 378550, "start": 3804.74, "end": 3813.38, "text": " Iter, valid dl, is the thing that basically kind of generates our coroutine for us, and", "tokens": [286, 391, 11, 7363, 37873, 11, 307, 264, 551, 300, 1936, 733, 295, 23815, 527, 1181, 45075, 337, 505, 11, 293], "temperature": 0.0, "avg_logprob": -0.1582906125771879, "compression_ratio": 1.5683760683760684, "no_speech_prob": 1.0782698154798709e-05}, {"id": 856, "seek": 381338, "start": 3813.38, "end": 3818.1, "text": " then next is the thing that grabs the next thing yielded out of that coroutine.", "tokens": [550, 958, 307, 264, 551, 300, 30028, 264, 958, 551, 11257, 292, 484, 295, 300, 1181, 45075, 13], "temperature": 0.0, "avg_logprob": -0.13107818875994, "compression_ratio": 1.7942238267148014, "no_speech_prob": 6.643072083534207e-06}, {"id": 857, "seek": 381338, "start": 3818.1, "end": 3819.42, "text": " So this is a very common thing.", "tokens": [407, 341, 307, 257, 588, 2689, 551, 13], "temperature": 0.0, "avg_logprob": -0.13107818875994, "compression_ratio": 1.7942238267148014, "no_speech_prob": 6.643072083534207e-06}, {"id": 858, "seek": 381338, "start": 3819.42, "end": 3820.42, "text": " You'll be doing lots.", "tokens": [509, 603, 312, 884, 3195, 13], "temperature": 0.0, "avg_logprob": -0.13107818875994, "compression_ratio": 1.7942238267148014, "no_speech_prob": 6.643072083534207e-06}, {"id": 859, "seek": 381338, "start": 3820.42, "end": 3821.7000000000003, "text": " Next, itter, blah.", "tokens": [3087, 11, 309, 391, 11, 12288, 13], "temperature": 0.0, "avg_logprob": -0.13107818875994, "compression_ratio": 1.7942238267148014, "no_speech_prob": 6.643072083534207e-06}, {"id": 860, "seek": 381338, "start": 3821.7000000000003, "end": 3825.6600000000003, "text": " You probably did it a whole lot of times in part one, because we kind of did it without", "tokens": [509, 1391, 630, 309, 257, 1379, 688, 295, 1413, 294, 644, 472, 11, 570, 321, 733, 295, 630, 309, 1553], "temperature": 0.0, "avg_logprob": -0.13107818875994, "compression_ratio": 1.7942238267148014, "no_speech_prob": 6.643072083534207e-06}, {"id": 861, "seek": 381338, "start": 3825.6600000000003, "end": 3828.5, "text": " diving in very deeply into what's going on.", "tokens": [20241, 294, 588, 8760, 666, 437, 311, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.13107818875994, "compression_ratio": 1.7942238267148014, "no_speech_prob": 6.643072083534207e-06}, {"id": 862, "seek": 381338, "start": 3828.5, "end": 3835.94, "text": " And that returns one thing from our data set, and the data set returns two things, because", "tokens": [400, 300, 11247, 472, 551, 490, 527, 1412, 992, 11, 293, 264, 1412, 992, 11247, 732, 721, 11, 570], "temperature": 0.0, "avg_logprob": -0.13107818875994, "compression_ratio": 1.7942238267148014, "no_speech_prob": 6.643072083534207e-06}, {"id": 863, "seek": 381338, "start": 3835.94, "end": 3836.94, "text": " that's what we put in it.", "tokens": [300, 311, 437, 321, 829, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.13107818875994, "compression_ratio": 1.7942238267148014, "no_speech_prob": 6.643072083534207e-06}, {"id": 864, "seek": 381338, "start": 3836.94, "end": 3841.38, "text": " So we expect to get two things back, and we can check that those two things are the right", "tokens": [407, 321, 2066, 281, 483, 732, 721, 646, 11, 293, 321, 393, 1520, 300, 729, 732, 721, 366, 264, 558], "temperature": 0.0, "avg_logprob": -0.13107818875994, "compression_ratio": 1.7942238267148014, "no_speech_prob": 6.643072083534207e-06}, {"id": 865, "seek": 381338, "start": 3841.38, "end": 3842.94, "text": " size.", "tokens": [2744, 13], "temperature": 0.0, "avg_logprob": -0.13107818875994, "compression_ratio": 1.7942238267148014, "no_speech_prob": 6.643072083534207e-06}, {"id": 866, "seek": 384294, "start": 3842.94, "end": 3845.5, "text": " So that's our data loader.", "tokens": [407, 300, 311, 527, 1412, 3677, 260, 13], "temperature": 0.0, "avg_logprob": -0.16852888071312094, "compression_ratio": 1.5694444444444444, "no_speech_prob": 1.061587408912601e-05}, {"id": 867, "seek": 384294, "start": 3845.5, "end": 3846.78, "text": " And so let's double check.", "tokens": [400, 370, 718, 311, 3834, 1520, 13], "temperature": 0.0, "avg_logprob": -0.16852888071312094, "compression_ratio": 1.5694444444444444, "no_speech_prob": 1.061587408912601e-05}, {"id": 868, "seek": 384294, "start": 3846.78, "end": 3848.42, "text": " There it is.", "tokens": [821, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.16852888071312094, "compression_ratio": 1.5694444444444444, "no_speech_prob": 1.061587408912601e-05}, {"id": 869, "seek": 384294, "start": 3848.42, "end": 3849.42, "text": " Good stuff.", "tokens": [2205, 1507, 13], "temperature": 0.0, "avg_logprob": -0.16852888071312094, "compression_ratio": 1.5694444444444444, "no_speech_prob": 1.061587408912601e-05}, {"id": 870, "seek": 384294, "start": 3849.42, "end": 3852.54, "text": " So now there's our fitness function.", "tokens": [407, 586, 456, 311, 527, 15303, 2445, 13], "temperature": 0.0, "avg_logprob": -0.16852888071312094, "compression_ratio": 1.5694444444444444, "no_speech_prob": 1.061587408912601e-05}, {"id": 871, "seek": 384294, "start": 3852.54, "end": 3856.26, "text": " Let's call the fitness function looking good.", "tokens": [961, 311, 818, 264, 15303, 2445, 1237, 665, 13], "temperature": 0.0, "avg_logprob": -0.16852888071312094, "compression_ratio": 1.5694444444444444, "no_speech_prob": 1.061587408912601e-05}, {"id": 872, "seek": 384294, "start": 3856.26, "end": 3860.54, "text": " So this is about as neat as we're going to get.", "tokens": [407, 341, 307, 466, 382, 10654, 382, 321, 434, 516, 281, 483, 13], "temperature": 0.0, "avg_logprob": -0.16852888071312094, "compression_ratio": 1.5694444444444444, "no_speech_prob": 1.061587408912601e-05}, {"id": 873, "seek": 384294, "start": 3860.54, "end": 3863.54, "text": " That's quite beautiful, right?", "tokens": [663, 311, 1596, 2238, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.16852888071312094, "compression_ratio": 1.5694444444444444, "no_speech_prob": 1.061587408912601e-05}, {"id": 874, "seek": 384294, "start": 3863.54, "end": 3868.14, "text": " It's kind of all the steps you can think of if you set it in English there.", "tokens": [467, 311, 733, 295, 439, 264, 4439, 291, 393, 519, 295, 498, 291, 992, 309, 294, 3669, 456, 13], "temperature": 0.0, "avg_logprob": -0.16852888071312094, "compression_ratio": 1.5694444444444444, "no_speech_prob": 1.061587408912601e-05}, {"id": 875, "seek": 384294, "start": 3868.14, "end": 3870.38, "text": " Go through each epoch.", "tokens": [1037, 807, 1184, 30992, 339, 13], "temperature": 0.0, "avg_logprob": -0.16852888071312094, "compression_ratio": 1.5694444444444444, "no_speech_prob": 1.061587408912601e-05}, {"id": 876, "seek": 387038, "start": 3870.38, "end": 3874.86, "text": " Go through each batch, grabbing the independent variable.", "tokens": [1037, 807, 1184, 15245, 11, 23771, 264, 6695, 7006, 13], "temperature": 0.0, "avg_logprob": -0.12410635533540146, "compression_ratio": 1.6789473684210525, "no_speech_prob": 5.0935250328620896e-06}, {"id": 877, "seek": 387038, "start": 3874.86, "end": 3877.3, "text": " Calculate the predictions.", "tokens": [3511, 2444, 473, 264, 21264, 13], "temperature": 0.0, "avg_logprob": -0.12410635533540146, "compression_ratio": 1.6789473684210525, "no_speech_prob": 5.0935250328620896e-06}, {"id": 878, "seek": 387038, "start": 3877.3, "end": 3879.82, "text": " Calculate the losses.", "tokens": [3511, 2444, 473, 264, 15352, 13], "temperature": 0.0, "avg_logprob": -0.12410635533540146, "compression_ratio": 1.6789473684210525, "no_speech_prob": 5.0935250328620896e-06}, {"id": 879, "seek": 387038, "start": 3879.82, "end": 3882.7000000000003, "text": " Calculate the gradients.", "tokens": [3511, 2444, 473, 264, 2771, 2448, 13], "temperature": 0.0, "avg_logprob": -0.12410635533540146, "compression_ratio": 1.6789473684210525, "no_speech_prob": 5.0935250328620896e-06}, {"id": 880, "seek": 387038, "start": 3882.7000000000003, "end": 3885.58, "text": " Update with the learning rate.", "tokens": [28923, 365, 264, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.12410635533540146, "compression_ratio": 1.6789473684210525, "no_speech_prob": 5.0935250328620896e-06}, {"id": 881, "seek": 387038, "start": 3885.58, "end": 3888.0, "text": " Reset the gradients.", "tokens": [5015, 302, 264, 2771, 2448, 13], "temperature": 0.0, "avg_logprob": -0.12410635533540146, "compression_ratio": 1.6789473684210525, "no_speech_prob": 5.0935250328620896e-06}, {"id": 882, "seek": 387038, "start": 3888.0, "end": 3893.38, "text": " So that's kind of where you want to get, is to a point where you can read your code in", "tokens": [407, 300, 311, 733, 295, 689, 291, 528, 281, 483, 11, 307, 281, 257, 935, 689, 291, 393, 1401, 428, 3089, 294], "temperature": 0.0, "avg_logprob": -0.12410635533540146, "compression_ratio": 1.6789473684210525, "no_speech_prob": 5.0935250328620896e-06}, {"id": 883, "seek": 387038, "start": 3893.38, "end": 3896.6800000000003, "text": " a very kind of intuitive way to a domain expert.", "tokens": [257, 588, 733, 295, 21769, 636, 281, 257, 9274, 5844, 13], "temperature": 0.0, "avg_logprob": -0.12410635533540146, "compression_ratio": 1.6789473684210525, "no_speech_prob": 5.0935250328620896e-06}, {"id": 884, "seek": 389668, "start": 3896.68, "end": 3900.4199999999996, "text": " And until you get to that point, it's very hard really, I find, to really maintain the", "tokens": [400, 1826, 291, 483, 281, 300, 935, 11, 309, 311, 588, 1152, 534, 11, 286, 915, 11, 281, 534, 6909, 264], "temperature": 0.0, "avg_logprob": -0.12160661003806374, "compression_ratio": 1.7741935483870968, "no_speech_prob": 2.368735977142933e-06}, {"id": 885, "seek": 389668, "start": 3900.4199999999996, "end": 3902.8199999999997, "text": " code and understand the code.", "tokens": [3089, 293, 1223, 264, 3089, 13], "temperature": 0.0, "avg_logprob": -0.12160661003806374, "compression_ratio": 1.7741935483870968, "no_speech_prob": 2.368735977142933e-06}, {"id": 886, "seek": 389668, "start": 3902.8199999999997, "end": 3906.18, "text": " And this is the trick for doing research as well.", "tokens": [400, 341, 307, 264, 4282, 337, 884, 2132, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.12160661003806374, "compression_ratio": 1.7741935483870968, "no_speech_prob": 2.368735977142933e-06}, {"id": 887, "seek": 389668, "start": 3906.18, "end": 3909.18, "text": " This is not just for hardcore software engineering.", "tokens": [639, 307, 406, 445, 337, 28196, 4722, 7043, 13], "temperature": 0.0, "avg_logprob": -0.12160661003806374, "compression_ratio": 1.7741935483870968, "no_speech_prob": 2.368735977142933e-06}, {"id": 888, "seek": 389668, "start": 3909.18, "end": 3914.94, "text": " A researcher that can't do those things to their code can't do research properly, right?", "tokens": [316, 21751, 300, 393, 380, 360, 729, 721, 281, 641, 3089, 393, 380, 360, 2132, 6108, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.12160661003806374, "compression_ratio": 1.7741935483870968, "no_speech_prob": 2.368735977142933e-06}, {"id": 889, "seek": 389668, "start": 3914.94, "end": 3919.62, "text": " Because if you think of something you want to try, you don't know how to do it, or it", "tokens": [1436, 498, 291, 519, 295, 746, 291, 528, 281, 853, 11, 291, 500, 380, 458, 577, 281, 360, 309, 11, 420, 309], "temperature": 0.0, "avg_logprob": -0.12160661003806374, "compression_ratio": 1.7741935483870968, "no_speech_prob": 2.368735977142933e-06}, {"id": 890, "seek": 389668, "start": 3919.62, "end": 3922.52, "text": " takes weeks, or if there are bugs in it, you don't know.", "tokens": [2516, 3259, 11, 420, 498, 456, 366, 15120, 294, 309, 11, 291, 500, 380, 458, 13], "temperature": 0.0, "avg_logprob": -0.12160661003806374, "compression_ratio": 1.7741935483870968, "no_speech_prob": 2.368735977142933e-06}, {"id": 891, "seek": 389668, "start": 3922.52, "end": 3925.1, "text": " So you want your code to be quite beautiful.", "tokens": [407, 291, 528, 428, 3089, 281, 312, 1596, 2238, 13], "temperature": 0.0, "avg_logprob": -0.12160661003806374, "compression_ratio": 1.7741935483870968, "no_speech_prob": 2.368735977142933e-06}, {"id": 892, "seek": 392510, "start": 3925.1, "end": 3930.38, "text": " And I think this is beautiful code.", "tokens": [400, 286, 519, 341, 307, 2238, 3089, 13], "temperature": 0.0, "avg_logprob": -0.10968542098999023, "compression_ratio": 1.5889830508474576, "no_speech_prob": 7.527604793722276e-06}, {"id": 893, "seek": 392510, "start": 3930.38, "end": 3936.66, "text": " And this is, at this point, you know, this data set and this data loader are the same", "tokens": [400, 341, 307, 11, 412, 341, 935, 11, 291, 458, 11, 341, 1412, 992, 293, 341, 1412, 3677, 260, 366, 264, 912], "temperature": 0.0, "avg_logprob": -0.10968542098999023, "compression_ratio": 1.5889830508474576, "no_speech_prob": 7.527604793722276e-06}, {"id": 894, "seek": 392510, "start": 3936.66, "end": 3941.1, "text": " abstractions that PyTorch uses.", "tokens": [12649, 626, 300, 9953, 51, 284, 339, 4960, 13], "temperature": 0.0, "avg_logprob": -0.10968542098999023, "compression_ratio": 1.5889830508474576, "no_speech_prob": 7.527604793722276e-06}, {"id": 895, "seek": 392510, "start": 3941.1, "end": 3943.18, "text": " So let's dig into this a little bit more.", "tokens": [407, 718, 311, 2528, 666, 341, 257, 707, 857, 544, 13], "temperature": 0.0, "avg_logprob": -0.10968542098999023, "compression_ratio": 1.5889830508474576, "no_speech_prob": 7.527604793722276e-06}, {"id": 896, "seek": 392510, "start": 3943.18, "end": 3947.86, "text": " We do have a problem, which is that we're always looping through our training set in", "tokens": [492, 360, 362, 257, 1154, 11, 597, 307, 300, 321, 434, 1009, 6367, 278, 807, 527, 3097, 992, 294], "temperature": 0.0, "avg_logprob": -0.10968542098999023, "compression_ratio": 1.5889830508474576, "no_speech_prob": 7.527604793722276e-06}, {"id": 897, "seek": 392510, "start": 3947.86, "end": 3949.72, "text": " order.", "tokens": [1668, 13], "temperature": 0.0, "avg_logprob": -0.10968542098999023, "compression_ratio": 1.5889830508474576, "no_speech_prob": 7.527604793722276e-06}, {"id": 898, "seek": 392510, "start": 3949.72, "end": 3954.7, "text": " And that's very problematic because we lose the randomness of kind of shuffling it each", "tokens": [400, 300, 311, 588, 19011, 570, 321, 3624, 264, 4974, 1287, 295, 733, 295, 402, 1245, 1688, 309, 1184], "temperature": 0.0, "avg_logprob": -0.10968542098999023, "compression_ratio": 1.5889830508474576, "no_speech_prob": 7.527604793722276e-06}, {"id": 899, "seek": 395470, "start": 3954.7, "end": 3959.54, "text": " time, particularly if our training set was already, like, ordered by dependent variable,", "tokens": [565, 11, 4098, 498, 527, 3097, 992, 390, 1217, 11, 411, 11, 8866, 538, 12334, 7006, 11], "temperature": 0.0, "avg_logprob": -0.13089495190119338, "compression_ratio": 1.749034749034749, "no_speech_prob": 1.4285365978139453e-05}, {"id": 900, "seek": 395470, "start": 3959.54, "end": 3964.4199999999996, "text": " then every batch is going to be exactly the same dependent variable.", "tokens": [550, 633, 15245, 307, 516, 281, 312, 2293, 264, 912, 12334, 7006, 13], "temperature": 0.0, "avg_logprob": -0.13089495190119338, "compression_ratio": 1.749034749034749, "no_speech_prob": 1.4285365978139453e-05}, {"id": 901, "seek": 395470, "start": 3964.4199999999996, "end": 3966.46, "text": " So we really want to shuffle it.", "tokens": [407, 321, 534, 528, 281, 39426, 309, 13], "temperature": 0.0, "avg_logprob": -0.13089495190119338, "compression_ratio": 1.749034749034749, "no_speech_prob": 1.4285365978139453e-05}, {"id": 902, "seek": 395470, "start": 3966.46, "end": 3968.18, "text": " So let's try random sampling.", "tokens": [407, 718, 311, 853, 4974, 21179, 13], "temperature": 0.0, "avg_logprob": -0.13089495190119338, "compression_ratio": 1.749034749034749, "no_speech_prob": 1.4285365978139453e-05}, {"id": 903, "seek": 395470, "start": 3968.18, "end": 3973.8199999999997, "text": " So for random sampling, I'm going to create a sampler class, and we're going to pass into", "tokens": [407, 337, 4974, 21179, 11, 286, 478, 516, 281, 1884, 257, 3247, 22732, 1508, 11, 293, 321, 434, 516, 281, 1320, 666], "temperature": 0.0, "avg_logprob": -0.13089495190119338, "compression_ratio": 1.749034749034749, "no_speech_prob": 1.4285365978139453e-05}, {"id": 904, "seek": 395470, "start": 3973.8199999999997, "end": 3980.2999999999997, "text": " it a data set to sample and a batch size and something that says whether to shuffle or", "tokens": [309, 257, 1412, 992, 281, 6889, 293, 257, 15245, 2744, 293, 746, 300, 1619, 1968, 281, 39426, 420], "temperature": 0.0, "avg_logprob": -0.13089495190119338, "compression_ratio": 1.749034749034749, "no_speech_prob": 1.4285365978139453e-05}, {"id": 905, "seek": 395470, "start": 3980.2999999999997, "end": 3981.2999999999997, "text": " not, right?", "tokens": [406, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.13089495190119338, "compression_ratio": 1.749034749034749, "no_speech_prob": 1.4285365978139453e-05}, {"id": 906, "seek": 395470, "start": 3981.2999999999997, "end": 3983.02, "text": " And as per usual, we just store those away.", "tokens": [400, 382, 680, 7713, 11, 321, 445, 3531, 729, 1314, 13], "temperature": 0.0, "avg_logprob": -0.13089495190119338, "compression_ratio": 1.749034749034749, "no_speech_prob": 1.4285365978139453e-05}, {"id": 907, "seek": 398302, "start": 3983.02, "end": 3984.74, "text": " I don't actually store away the data set.", "tokens": [286, 500, 380, 767, 3531, 1314, 264, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.18232856194178262, "compression_ratio": 1.570754716981132, "no_speech_prob": 5.255328687781002e-06}, {"id": 908, "seek": 398302, "start": 3984.74, "end": 3991.14, "text": " I just store away the length of the data set so that we know how many items to sample.", "tokens": [286, 445, 3531, 1314, 264, 4641, 295, 264, 1412, 992, 370, 300, 321, 458, 577, 867, 4754, 281, 6889, 13], "temperature": 0.0, "avg_logprob": -0.18232856194178262, "compression_ratio": 1.570754716981132, "no_speech_prob": 5.255328687781002e-06}, {"id": 909, "seek": 398302, "start": 3991.14, "end": 3992.46, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.18232856194178262, "compression_ratio": 1.570754716981132, "no_speech_prob": 5.255328687781002e-06}, {"id": 910, "seek": 398302, "start": 3992.46, "end": 3994.74, "text": " And then here's our dunder error, right?", "tokens": [400, 550, 510, 311, 527, 274, 6617, 6713, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.18232856194178262, "compression_ratio": 1.570754716981132, "no_speech_prob": 5.255328687781002e-06}, {"id": 911, "seek": 398302, "start": 3994.74, "end": 4000.86, "text": " So remember, this is the thing that we can call next on lots of times.", "tokens": [407, 1604, 11, 341, 307, 264, 551, 300, 321, 393, 818, 958, 322, 3195, 295, 1413, 13], "temperature": 0.0, "avg_logprob": -0.18232856194178262, "compression_ratio": 1.570754716981132, "no_speech_prob": 5.255328687781002e-06}, {"id": 912, "seek": 398302, "start": 4000.86, "end": 4011.58, "text": " And so if we are shuffling, then let's grab a random permutation of the numbers from 0", "tokens": [400, 370, 498, 321, 366, 402, 1245, 1688, 11, 550, 718, 311, 4444, 257, 4974, 4784, 11380, 295, 264, 3547, 490, 1958], "temperature": 0.0, "avg_logprob": -0.18232856194178262, "compression_ratio": 1.570754716981132, "no_speech_prob": 5.255328687781002e-06}, {"id": 913, "seek": 401158, "start": 4011.58, "end": 4013.74, "text": " to n minus 1.", "tokens": [281, 297, 3175, 502, 13], "temperature": 0.0, "avg_logprob": -0.13270565410992047, "compression_ratio": 1.5118483412322274, "no_speech_prob": 1.4285358702181838e-05}, {"id": 914, "seek": 401158, "start": 4013.74, "end": 4019.42, "text": " And if we're not shuffling, then let's grab all of the integers in order from 0 to n minus", "tokens": [400, 498, 321, 434, 406, 402, 1245, 1688, 11, 550, 718, 311, 4444, 439, 295, 264, 41674, 294, 1668, 490, 1958, 281, 297, 3175], "temperature": 0.0, "avg_logprob": -0.13270565410992047, "compression_ratio": 1.5118483412322274, "no_speech_prob": 1.4285358702181838e-05}, {"id": 915, "seek": 401158, "start": 4019.42, "end": 4020.42, "text": " 1.", "tokens": [502, 13], "temperature": 0.0, "avg_logprob": -0.13270565410992047, "compression_ratio": 1.5118483412322274, "no_speech_prob": 1.4285358702181838e-05}, {"id": 916, "seek": 401158, "start": 4020.42, "end": 4026.46, "text": " And then, this is the same we had before, go through that range and yield the indexes.", "tokens": [400, 550, 11, 341, 307, 264, 912, 321, 632, 949, 11, 352, 807, 300, 3613, 293, 11257, 264, 8186, 279, 13], "temperature": 0.0, "avg_logprob": -0.13270565410992047, "compression_ratio": 1.5118483412322274, "no_speech_prob": 1.4285358702181838e-05}, {"id": 917, "seek": 401158, "start": 4026.46, "end": 4029.42, "text": " So what does that look like?", "tokens": [407, 437, 775, 300, 574, 411, 30], "temperature": 0.0, "avg_logprob": -0.13270565410992047, "compression_ratio": 1.5118483412322274, "no_speech_prob": 1.4285358702181838e-05}, {"id": 918, "seek": 401158, "start": 4029.42, "end": 4036.5, "text": " Here's a sampler with shuffle equals false and a batch size of 3, 0, 1, 2, 3, 4, 5, 6,", "tokens": [1692, 311, 257, 3247, 22732, 365, 39426, 6915, 7908, 293, 257, 15245, 2744, 295, 805, 11, 1958, 11, 502, 11, 568, 11, 805, 11, 1017, 11, 1025, 11, 1386, 11], "temperature": 0.0, "avg_logprob": -0.13270565410992047, "compression_ratio": 1.5118483412322274, "no_speech_prob": 1.4285358702181838e-05}, {"id": 919, "seek": 401158, "start": 4036.5, "end": 4037.9, "text": " 7, 8, 9.", "tokens": [1614, 11, 1649, 11, 1722, 13], "temperature": 0.0, "avg_logprob": -0.13270565410992047, "compression_ratio": 1.5118483412322274, "no_speech_prob": 1.4285358702181838e-05}, {"id": 920, "seek": 403790, "start": 4037.9, "end": 4047.26, "text": " And here it is with shuffle equals true, 5, 4, 3, 7, 6, 2, 8, 9, 0, 1.", "tokens": [400, 510, 309, 307, 365, 39426, 6915, 2074, 11, 1025, 11, 1017, 11, 805, 11, 1614, 11, 1386, 11, 568, 11, 1649, 11, 1722, 11, 1958, 11, 502, 13], "temperature": 0.0, "avg_logprob": -0.10186057954322635, "compression_ratio": 1.625984251968504, "no_speech_prob": 1.8738710423349403e-06}, {"id": 921, "seek": 403790, "start": 4047.26, "end": 4052.7000000000003, "text": " So now that we've got these, we can now replace our data loader with something where we pass", "tokens": [407, 586, 300, 321, 600, 658, 613, 11, 321, 393, 586, 7406, 527, 1412, 3677, 260, 365, 746, 689, 321, 1320], "temperature": 0.0, "avg_logprob": -0.10186057954322635, "compression_ratio": 1.625984251968504, "no_speech_prob": 1.8738710423349403e-06}, {"id": 922, "seek": 403790, "start": 4052.7000000000003, "end": 4055.2200000000003, "text": " it a sampler.", "tokens": [309, 257, 3247, 22732, 13], "temperature": 0.0, "avg_logprob": -0.10186057954322635, "compression_ratio": 1.625984251968504, "no_speech_prob": 1.8738710423349403e-06}, {"id": 923, "seek": 403790, "start": 4055.2200000000003, "end": 4058.1, "text": " And we then loop through for s in sampler.", "tokens": [400, 321, 550, 6367, 807, 337, 262, 294, 3247, 22732, 13], "temperature": 0.0, "avg_logprob": -0.10186057954322635, "compression_ratio": 1.625984251968504, "no_speech_prob": 1.8738710423349403e-06}, {"id": 924, "seek": 403790, "start": 4058.1, "end": 4060.86, "text": " So it's gonna loop through each of these, right?", "tokens": [407, 309, 311, 799, 6367, 807, 1184, 295, 613, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.10186057954322635, "compression_ratio": 1.625984251968504, "no_speech_prob": 1.8738710423349403e-06}, {"id": 925, "seek": 403790, "start": 4060.86, "end": 4064.98, "text": " And the cool thing is that because we used yield, these are only gonna be calculated", "tokens": [400, 264, 1627, 551, 307, 300, 570, 321, 1143, 11257, 11, 613, 366, 787, 799, 312, 15598], "temperature": 0.0, "avg_logprob": -0.10186057954322635, "compression_ratio": 1.625984251968504, "no_speech_prob": 1.8738710423349403e-06}, {"id": 926, "seek": 403790, "start": 4064.98, "end": 4065.98, "text": " when we ask for them.", "tokens": [562, 321, 1029, 337, 552, 13], "temperature": 0.0, "avg_logprob": -0.10186057954322635, "compression_ratio": 1.625984251968504, "no_speech_prob": 1.8738710423349403e-06}, {"id": 927, "seek": 403790, "start": 4065.98, "end": 4067.5, "text": " They're not all calculated up front.", "tokens": [814, 434, 406, 439, 15598, 493, 1868, 13], "temperature": 0.0, "avg_logprob": -0.10186057954322635, "compression_ratio": 1.625984251968504, "no_speech_prob": 1.8738710423349403e-06}, {"id": 928, "seek": 406750, "start": 4067.5, "end": 4071.06, "text": " So we can use these on really big data sets, no problem.", "tokens": [407, 321, 393, 764, 613, 322, 534, 955, 1412, 6352, 11, 572, 1154, 13], "temperature": 0.0, "avg_logprob": -0.11336946049961474, "compression_ratio": 1.6833333333333333, "no_speech_prob": 2.0145011148997582e-05}, {"id": 929, "seek": 406750, "start": 4071.06, "end": 4077.14, "text": " And so it's kind of, this is a common thing is where you're actually looping through something", "tokens": [400, 370, 309, 311, 733, 295, 11, 341, 307, 257, 2689, 551, 307, 689, 291, 434, 767, 6367, 278, 807, 746], "temperature": 0.0, "avg_logprob": -0.11336946049961474, "compression_ratio": 1.6833333333333333, "no_speech_prob": 2.0145011148997582e-05}, {"id": 930, "seek": 406750, "start": 4077.14, "end": 4081.38, "text": " which is itself a coroutine and then yielding something which does other things to that.", "tokens": [597, 307, 2564, 257, 1181, 45075, 293, 550, 11257, 278, 746, 597, 775, 661, 721, 281, 300, 13], "temperature": 0.0, "avg_logprob": -0.11336946049961474, "compression_ratio": 1.6833333333333333, "no_speech_prob": 2.0145011148997582e-05}, {"id": 931, "seek": 406750, "start": 4081.38, "end": 4085.9, "text": " So this is like a really nice way of doing streaming computations.", "tokens": [407, 341, 307, 411, 257, 534, 1481, 636, 295, 884, 11791, 2807, 763, 13], "temperature": 0.0, "avg_logprob": -0.11336946049961474, "compression_ratio": 1.6833333333333333, "no_speech_prob": 2.0145011148997582e-05}, {"id": 932, "seek": 406750, "start": 4085.9, "end": 4086.9, "text": " It's being done lazily.", "tokens": [467, 311, 885, 1096, 19320, 953, 13], "temperature": 0.0, "avg_logprob": -0.11336946049961474, "compression_ratio": 1.6833333333333333, "no_speech_prob": 2.0145011148997582e-05}, {"id": 933, "seek": 406750, "start": 4086.9, "end": 4090.06, "text": " You're not gonna run out of memory.", "tokens": [509, 434, 406, 799, 1190, 484, 295, 4675, 13], "temperature": 0.0, "avg_logprob": -0.11336946049961474, "compression_ratio": 1.6833333333333333, "no_speech_prob": 2.0145011148997582e-05}, {"id": 934, "seek": 406750, "start": 4090.06, "end": 4091.66, "text": " It's a really neat way to do things.", "tokens": [467, 311, 257, 534, 10654, 636, 281, 360, 721, 13], "temperature": 0.0, "avg_logprob": -0.11336946049961474, "compression_ratio": 1.6833333333333333, "no_speech_prob": 2.0145011148997582e-05}, {"id": 935, "seek": 409166, "start": 4091.66, "end": 4098.099999999999, "text": " And so then we're gonna grab all of the indexes in that sample.", "tokens": [400, 370, 550, 321, 434, 799, 4444, 439, 295, 264, 8186, 279, 294, 300, 6889, 13], "temperature": 0.0, "avg_logprob": -0.07969927546953914, "compression_ratio": 1.7371134020618557, "no_speech_prob": 4.4950784285902046e-06}, {"id": 936, "seek": 409166, "start": 4098.099999999999, "end": 4101.9, "text": " And we're gonna grab the data set at that index.", "tokens": [400, 321, 434, 799, 4444, 264, 1412, 992, 412, 300, 8186, 13], "temperature": 0.0, "avg_logprob": -0.07969927546953914, "compression_ratio": 1.7371134020618557, "no_speech_prob": 4.4950784285902046e-06}, {"id": 937, "seek": 409166, "start": 4101.9, "end": 4104.66, "text": " So now we've got a list of tensors.", "tokens": [407, 586, 321, 600, 658, 257, 1329, 295, 10688, 830, 13], "temperature": 0.0, "avg_logprob": -0.07969927546953914, "compression_ratio": 1.7371134020618557, "no_speech_prob": 4.4950784285902046e-06}, {"id": 938, "seek": 409166, "start": 4104.66, "end": 4110.5599999999995, "text": " And then we need some way to collate them all together into a single pair of tensors.", "tokens": [400, 550, 321, 643, 512, 636, 281, 1263, 473, 552, 439, 1214, 666, 257, 2167, 6119, 295, 10688, 830, 13], "temperature": 0.0, "avg_logprob": -0.07969927546953914, "compression_ratio": 1.7371134020618557, "no_speech_prob": 4.4950784285902046e-06}, {"id": 939, "seek": 409166, "start": 4110.5599999999995, "end": 4116.84, "text": " And so we've created a function called collate, which just grabs the Xs and the Ys and stacks", "tokens": [400, 370, 321, 600, 2942, 257, 2445, 1219, 1263, 473, 11, 597, 445, 30028, 264, 1783, 82, 293, 264, 398, 82, 293, 30792], "temperature": 0.0, "avg_logprob": -0.07969927546953914, "compression_ratio": 1.7371134020618557, "no_speech_prob": 4.4950784285902046e-06}, {"id": 940, "seek": 409166, "start": 4116.84, "end": 4117.84, "text": " them up.", "tokens": [552, 493, 13], "temperature": 0.0, "avg_logprob": -0.07969927546953914, "compression_ratio": 1.7371134020618557, "no_speech_prob": 4.4950784285902046e-06}, {"id": 941, "seek": 411784, "start": 4117.84, "end": 4124.62, "text": " So collage.stack just grabs a bunch of tensors and glues them together on a new axis.", "tokens": [407, 1263, 609, 13, 372, 501, 445, 30028, 257, 3840, 295, 10688, 830, 293, 1563, 1247, 552, 1214, 322, 257, 777, 10298, 13], "temperature": 0.0, "avg_logprob": -0.1411607148217373, "compression_ratio": 1.7056451612903225, "no_speech_prob": 4.6644342432955455e-07}, {"id": 942, "seek": 411784, "start": 4124.62, "end": 4129.42, "text": " You might wanna do different things like add some padding or stuff like that.", "tokens": [509, 1062, 1948, 360, 819, 721, 411, 909, 512, 39562, 420, 1507, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.1411607148217373, "compression_ratio": 1.7056451612903225, "no_speech_prob": 4.6644342432955455e-07}, {"id": 943, "seek": 411784, "start": 4129.42, "end": 4132.22, "text": " So you can pass in a different collate function if you want to.", "tokens": [407, 291, 393, 1320, 294, 257, 819, 1263, 473, 2445, 498, 291, 528, 281, 13], "temperature": 0.0, "avg_logprob": -0.1411607148217373, "compression_ratio": 1.7056451612903225, "no_speech_prob": 4.6644342432955455e-07}, {"id": 944, "seek": 411784, "start": 4132.22, "end": 4135.66, "text": " And it'll store it away and use it.", "tokens": [400, 309, 603, 3531, 309, 1314, 293, 764, 309, 13], "temperature": 0.0, "avg_logprob": -0.1411607148217373, "compression_ratio": 1.7056451612903225, "no_speech_prob": 4.6644342432955455e-07}, {"id": 945, "seek": 411784, "start": 4135.66, "end": 4140.2, "text": " So now we can create our two samplers.", "tokens": [407, 586, 321, 393, 1884, 527, 732, 3247, 564, 433, 13], "temperature": 0.0, "avg_logprob": -0.1411607148217373, "compression_ratio": 1.7056451612903225, "no_speech_prob": 4.6644342432955455e-07}, {"id": 946, "seek": 411784, "start": 4140.2, "end": 4142.900000000001, "text": " We can create our two data loaders with those samplers.", "tokens": [492, 393, 1884, 527, 732, 1412, 3677, 433, 365, 729, 3247, 564, 433, 13], "temperature": 0.0, "avg_logprob": -0.1411607148217373, "compression_ratio": 1.7056451612903225, "no_speech_prob": 4.6644342432955455e-07}, {"id": 947, "seek": 411784, "start": 4142.900000000001, "end": 4146.64, "text": " So the training one is shuffling, the valid one's not shuffling.", "tokens": [407, 264, 3097, 472, 307, 402, 1245, 1688, 11, 264, 7363, 472, 311, 406, 402, 1245, 1688, 13], "temperature": 0.0, "avg_logprob": -0.1411607148217373, "compression_ratio": 1.7056451612903225, "no_speech_prob": 4.6644342432955455e-07}, {"id": 948, "seek": 414664, "start": 4146.64, "end": 4150.18, "text": " So let's check, there's the validation data loader.", "tokens": [407, 718, 311, 1520, 11, 456, 311, 264, 24071, 1412, 3677, 260, 13], "temperature": 0.0, "avg_logprob": -0.14336208161853609, "compression_ratio": 1.6545454545454545, "no_speech_prob": 1.5294058357540052e-06}, {"id": 949, "seek": 414664, "start": 4150.18, "end": 4156.26, "text": " And the training data loader, if we call it twice with exactly the same index, we get", "tokens": [400, 264, 3097, 1412, 3677, 260, 11, 498, 321, 818, 309, 6091, 365, 2293, 264, 912, 8186, 11, 321, 483], "temperature": 0.0, "avg_logprob": -0.14336208161853609, "compression_ratio": 1.6545454545454545, "no_speech_prob": 1.5294058357540052e-06}, {"id": 950, "seek": 414664, "start": 4156.26, "end": 4157.26, "text": " different things.", "tokens": [819, 721, 13], "temperature": 0.0, "avg_logprob": -0.14336208161853609, "compression_ratio": 1.6545454545454545, "no_speech_prob": 1.5294058357540052e-06}, {"id": 951, "seek": 414664, "start": 4157.26, "end": 4159.34, "text": " In this case, we got two eights, but they're different eights.", "tokens": [682, 341, 1389, 11, 321, 658, 732, 3180, 82, 11, 457, 436, 434, 819, 3180, 82, 13], "temperature": 0.0, "avg_logprob": -0.14336208161853609, "compression_ratio": 1.6545454545454545, "no_speech_prob": 1.5294058357540052e-06}, {"id": 952, "seek": 414664, "start": 4159.34, "end": 4162.860000000001, "text": " Call it another two times, we're getting different numbers.", "tokens": [7807, 309, 1071, 732, 1413, 11, 321, 434, 1242, 819, 3547, 13], "temperature": 0.0, "avg_logprob": -0.14336208161853609, "compression_ratio": 1.6545454545454545, "no_speech_prob": 1.5294058357540052e-06}, {"id": 953, "seek": 414664, "start": 4162.860000000001, "end": 4166.42, "text": " So it is shuffling as we hoped.", "tokens": [407, 309, 307, 402, 1245, 1688, 382, 321, 19737, 13], "temperature": 0.0, "avg_logprob": -0.14336208161853609, "compression_ratio": 1.6545454545454545, "no_speech_prob": 1.5294058357540052e-06}, {"id": 954, "seek": 414664, "start": 4166.42, "end": 4172.660000000001, "text": " And so again, we can train our model and that's fine.", "tokens": [400, 370, 797, 11, 321, 393, 3847, 527, 2316, 293, 300, 311, 2489, 13], "temperature": 0.0, "avg_logprob": -0.14336208161853609, "compression_ratio": 1.6545454545454545, "no_speech_prob": 1.5294058357540052e-06}, {"id": 955, "seek": 417266, "start": 4172.66, "end": 4176.7, "text": " So the PyTorch data loader does exactly that.", "tokens": [407, 264, 9953, 51, 284, 339, 1412, 3677, 260, 775, 2293, 300, 13], "temperature": 0.0, "avg_logprob": -0.12448056276179542, "compression_ratio": 1.7886178861788617, "no_speech_prob": 5.507519745151512e-06}, {"id": 956, "seek": 417266, "start": 4176.7, "end": 4179.46, "text": " So let's import the PyTorch data loader.", "tokens": [407, 718, 311, 974, 264, 9953, 51, 284, 339, 1412, 3677, 260, 13], "temperature": 0.0, "avg_logprob": -0.12448056276179542, "compression_ratio": 1.7886178861788617, "no_speech_prob": 5.507519745151512e-06}, {"id": 957, "seek": 417266, "start": 4179.46, "end": 4181.7, "text": " And you can see it takes exactly the same arguments.", "tokens": [400, 291, 393, 536, 309, 2516, 2293, 264, 912, 12869, 13], "temperature": 0.0, "avg_logprob": -0.12448056276179542, "compression_ratio": 1.7886178861788617, "no_speech_prob": 5.507519745151512e-06}, {"id": 958, "seek": 417266, "start": 4181.7, "end": 4187.94, "text": " Okay, and we can even pass in the exact collate function that we just wrote.", "tokens": [1033, 11, 293, 321, 393, 754, 1320, 294, 264, 1900, 1263, 473, 2445, 300, 321, 445, 4114, 13], "temperature": 0.0, "avg_logprob": -0.12448056276179542, "compression_ratio": 1.7886178861788617, "no_speech_prob": 5.507519745151512e-06}, {"id": 959, "seek": 417266, "start": 4187.94, "end": 4191.44, "text": " It doesn't have a single sampler that you pass shuffle equals true or false to.", "tokens": [467, 1177, 380, 362, 257, 2167, 3247, 22732, 300, 291, 1320, 39426, 6915, 2074, 420, 7908, 281, 13], "temperature": 0.0, "avg_logprob": -0.12448056276179542, "compression_ratio": 1.7886178861788617, "no_speech_prob": 5.507519745151512e-06}, {"id": 960, "seek": 417266, "start": 4191.44, "end": 4194.54, "text": " It has two samplers, one called random, one called sequential.", "tokens": [467, 575, 732, 3247, 564, 433, 11, 472, 1219, 4974, 11, 472, 1219, 42881, 13], "temperature": 0.0, "avg_logprob": -0.12448056276179542, "compression_ratio": 1.7886178861788617, "no_speech_prob": 5.507519745151512e-06}, {"id": 961, "seek": 417266, "start": 4194.54, "end": 4199.44, "text": " So slightly different to the API we just wrote, but does exactly the same thing.", "tokens": [407, 4748, 819, 281, 264, 9362, 321, 445, 4114, 11, 457, 775, 2293, 264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.12448056276179542, "compression_ratio": 1.7886178861788617, "no_speech_prob": 5.507519745151512e-06}, {"id": 962, "seek": 419944, "start": 4199.44, "end": 4205.54, "text": " And so you can create those data loaders and works exactly the same.", "tokens": [400, 370, 291, 393, 1884, 729, 1412, 3677, 433, 293, 1985, 2293, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.10668914548812373, "compression_ratio": 1.6995073891625616, "no_speech_prob": 4.2227370613545645e-06}, {"id": 963, "seek": 419944, "start": 4205.54, "end": 4208.74, "text": " So that's what a PyTorch data loader does.", "tokens": [407, 300, 311, 437, 257, 9953, 51, 284, 339, 1412, 3677, 260, 775, 13], "temperature": 0.0, "avg_logprob": -0.10668914548812373, "compression_ratio": 1.6995073891625616, "no_speech_prob": 4.2227370613545645e-06}, {"id": 964, "seek": 419944, "start": 4208.74, "end": 4212.9, "text": " Most of the time you don't need the flexibility of writing your own sampler and your own collation", "tokens": [4534, 295, 264, 565, 291, 500, 380, 643, 264, 12635, 295, 3579, 428, 1065, 3247, 22732, 293, 428, 1065, 1263, 399], "temperature": 0.0, "avg_logprob": -0.10668914548812373, "compression_ratio": 1.6995073891625616, "no_speech_prob": 4.2227370613545645e-06}, {"id": 965, "seek": 419944, "start": 4212.9, "end": 4213.98, "text": " function.", "tokens": [2445, 13], "temperature": 0.0, "avg_logprob": -0.10668914548812373, "compression_ratio": 1.6995073891625616, "no_speech_prob": 4.2227370613545645e-06}, {"id": 966, "seek": 419944, "start": 4213.98, "end": 4221.219999999999, "text": " So you can just pass in shuffle and it will use the default sampler and collation function", "tokens": [407, 291, 393, 445, 1320, 294, 39426, 293, 309, 486, 764, 264, 7576, 3247, 22732, 293, 1263, 399, 2445], "temperature": 0.0, "avg_logprob": -0.10668914548812373, "compression_ratio": 1.6995073891625616, "no_speech_prob": 4.2227370613545645e-06}, {"id": 967, "seek": 419944, "start": 4221.219999999999, "end": 4225.98, "text": " that work the way we just showed.", "tokens": [300, 589, 264, 636, 321, 445, 4712, 13], "temperature": 0.0, "avg_logprob": -0.10668914548812373, "compression_ratio": 1.6995073891625616, "no_speech_prob": 4.2227370613545645e-06}, {"id": 968, "seek": 422598, "start": 4225.98, "end": 4230.459999999999, "text": " Something that we did not implement in PyTorch's data loader is that in PyTorch's data loader", "tokens": [6595, 300, 321, 630, 406, 4445, 294, 9953, 51, 284, 339, 311, 1412, 3677, 260, 307, 300, 294, 9953, 51, 284, 339, 311, 1412, 3677, 260], "temperature": 0.0, "avg_logprob": -0.13248851610266643, "compression_ratio": 1.727626459143969, "no_speech_prob": 1.078239620255772e-05}, {"id": 969, "seek": 422598, "start": 4230.459999999999, "end": 4234.0599999999995, "text": " you can pass in an extra parameter called num workers.", "tokens": [291, 393, 1320, 294, 364, 2857, 13075, 1219, 1031, 5600, 13], "temperature": 0.0, "avg_logprob": -0.13248851610266643, "compression_ratio": 1.727626459143969, "no_speech_prob": 1.078239620255772e-05}, {"id": 970, "seek": 422598, "start": 4234.0599999999995, "end": 4237.94, "text": " And that will fire off that many processes.", "tokens": [400, 300, 486, 2610, 766, 300, 867, 7555, 13], "temperature": 0.0, "avg_logprob": -0.13248851610266643, "compression_ratio": 1.727626459143969, "no_speech_prob": 1.078239620255772e-05}, {"id": 971, "seek": 422598, "start": 4237.94, "end": 4243.78, "text": " And each one will separately grab stuff out of your data set and then it will collate", "tokens": [400, 1184, 472, 486, 14759, 4444, 1507, 484, 295, 428, 1412, 992, 293, 550, 309, 486, 1263, 473], "temperature": 0.0, "avg_logprob": -0.13248851610266643, "compression_ratio": 1.727626459143969, "no_speech_prob": 1.078239620255772e-05}, {"id": 972, "seek": 422598, "start": 4243.78, "end": 4245.339999999999, "text": " them together afterwards.", "tokens": [552, 1214, 10543, 13], "temperature": 0.0, "avg_logprob": -0.13248851610266643, "compression_ratio": 1.727626459143969, "no_speech_prob": 1.078239620255772e-05}, {"id": 973, "seek": 422598, "start": 4245.339999999999, "end": 4250.639999999999, "text": " And so if your data set's doing things like opening big JPEG files and doing all kinds", "tokens": [400, 370, 498, 428, 1412, 992, 311, 884, 721, 411, 5193, 955, 508, 5208, 38, 7098, 293, 884, 439, 3685], "temperature": 0.0, "avg_logprob": -0.13248851610266643, "compression_ratio": 1.727626459143969, "no_speech_prob": 1.078239620255772e-05}, {"id": 974, "seek": 422598, "start": 4250.639999999999, "end": 4255.04, "text": " of image transformations, that's a really good idea.", "tokens": [295, 3256, 34852, 11, 300, 311, 257, 534, 665, 1558, 13], "temperature": 0.0, "avg_logprob": -0.13248851610266643, "compression_ratio": 1.727626459143969, "no_speech_prob": 1.078239620255772e-05}, {"id": 975, "seek": 425504, "start": 4255.04, "end": 4256.5, "text": " So we won't implement that.", "tokens": [407, 321, 1582, 380, 4445, 300, 13], "temperature": 0.0, "avg_logprob": -0.15978682913431308, "compression_ratio": 1.6105263157894736, "no_speech_prob": 3.0894627798261354e-06}, {"id": 976, "seek": 425504, "start": 4256.5, "end": 4264.0, "text": " All right, so finally for this section we should add validation.", "tokens": [1057, 558, 11, 370, 2721, 337, 341, 3541, 321, 820, 909, 24071, 13], "temperature": 0.0, "avg_logprob": -0.15978682913431308, "compression_ratio": 1.6105263157894736, "no_speech_prob": 3.0894627798261354e-06}, {"id": 977, "seek": 425504, "start": 4264.0, "end": 4273.5199999999995, "text": " So to know if we're overfitting we need to have a separate validation set.", "tokens": [407, 281, 458, 498, 321, 434, 670, 69, 2414, 321, 643, 281, 362, 257, 4994, 24071, 992, 13], "temperature": 0.0, "avg_logprob": -0.15978682913431308, "compression_ratio": 1.6105263157894736, "no_speech_prob": 3.0894627798261354e-06}, {"id": 978, "seek": 425504, "start": 4273.5199999999995, "end": 4277.94, "text": " So here's the same loop that we had before.", "tokens": [407, 510, 311, 264, 912, 6367, 300, 321, 632, 949, 13], "temperature": 0.0, "avg_logprob": -0.15978682913431308, "compression_ratio": 1.6105263157894736, "no_speech_prob": 3.0894627798261354e-06}, {"id": 979, "seek": 425504, "start": 4277.94, "end": 4283.78, "text": " And here's the same loop pretty much again, but with torch.nograd going through the validation", "tokens": [400, 510, 311, 264, 912, 6367, 1238, 709, 797, 11, 457, 365, 27822, 13, 77, 664, 6206, 516, 807, 264, 24071], "temperature": 0.0, "avg_logprob": -0.15978682913431308, "compression_ratio": 1.6105263157894736, "no_speech_prob": 3.0894627798261354e-06}, {"id": 980, "seek": 428378, "start": 4283.78, "end": 4285.38, "text": " set.", "tokens": [992, 13], "temperature": 0.0, "avg_logprob": -0.12386105650214739, "compression_ratio": 1.683168316831683, "no_speech_prob": 1.3211690202297177e-05}, {"id": 981, "seek": 428378, "start": 4285.38, "end": 4290.92, "text": " So for this we grab the predictions and the loss as before, but we don't call backward", "tokens": [407, 337, 341, 321, 4444, 264, 21264, 293, 264, 4470, 382, 949, 11, 457, 321, 500, 380, 818, 23897], "temperature": 0.0, "avg_logprob": -0.12386105650214739, "compression_ratio": 1.683168316831683, "no_speech_prob": 1.3211690202297177e-05}, {"id": 982, "seek": 428378, "start": 4290.92, "end": 4294.34, "text": " and we don't step the optimizer because it's just the validation.", "tokens": [293, 321, 500, 380, 1823, 264, 5028, 6545, 570, 309, 311, 445, 264, 24071, 13], "temperature": 0.0, "avg_logprob": -0.12386105650214739, "compression_ratio": 1.683168316831683, "no_speech_prob": 1.3211690202297177e-05}, {"id": 983, "seek": 428378, "start": 4294.34, "end": 4300.74, "text": " Instead we just keep track of the loss and we also keep track of the accuracy.", "tokens": [7156, 321, 445, 1066, 2837, 295, 264, 4470, 293, 321, 611, 1066, 2837, 295, 264, 14170, 13], "temperature": 0.0, "avg_logprob": -0.12386105650214739, "compression_ratio": 1.683168316831683, "no_speech_prob": 1.3211690202297177e-05}, {"id": 984, "seek": 428378, "start": 4300.74, "end": 4307.82, "text": " The only other difference is that we've added here model.train and here model.neval.", "tokens": [440, 787, 661, 2649, 307, 300, 321, 600, 3869, 510, 2316, 13, 83, 7146, 293, 510, 2316, 13, 716, 3337, 13], "temperature": 0.0, "avg_logprob": -0.12386105650214739, "compression_ratio": 1.683168316831683, "no_speech_prob": 1.3211690202297177e-05}, {"id": 985, "seek": 428378, "start": 4307.82, "end": 4308.82, "text": " What does that do?", "tokens": [708, 775, 300, 360, 30], "temperature": 0.0, "avg_logprob": -0.12386105650214739, "compression_ratio": 1.683168316831683, "no_speech_prob": 1.3211690202297177e-05}, {"id": 986, "seek": 430882, "start": 4308.82, "end": 4315.94, "text": " Well actually all it does is it sets an internal attribute called.training to true or false.", "tokens": [1042, 767, 439, 309, 775, 307, 309, 6352, 364, 6920, 19667, 1219, 2411, 17227, 1760, 281, 2074, 420, 7908, 13], "temperature": 0.0, "avg_logprob": -0.1595497872065572, "compression_ratio": 1.7534883720930232, "no_speech_prob": 3.0894725568941794e-06}, {"id": 987, "seek": 430882, "start": 4315.94, "end": 4316.94, "text": " So let's try it.", "tokens": [407, 718, 311, 853, 309, 13], "temperature": 0.0, "avg_logprob": -0.1595497872065572, "compression_ratio": 1.7534883720930232, "no_speech_prob": 3.0894725568941794e-06}, {"id": 988, "seek": 430882, "start": 4316.94, "end": 4325.42, "text": " If I print model.training after each one and train this, let's see, true, false, true,", "tokens": [759, 286, 4482, 2316, 13, 17227, 1760, 934, 1184, 472, 293, 3847, 341, 11, 718, 311, 536, 11, 2074, 11, 7908, 11, 2074, 11], "temperature": 0.0, "avg_logprob": -0.1595497872065572, "compression_ratio": 1.7534883720930232, "no_speech_prob": 3.0894725568941794e-06}, {"id": 989, "seek": 430882, "start": 4325.42, "end": 4327.34, "text": " false, true, false.", "tokens": [7908, 11, 2074, 11, 7908, 13], "temperature": 0.0, "avg_logprob": -0.1595497872065572, "compression_ratio": 1.7534883720930232, "no_speech_prob": 3.0894725568941794e-06}, {"id": 990, "seek": 430882, "start": 4327.34, "end": 4332.66, "text": " And so why does it set this thing called model.training to true or false?", "tokens": [400, 370, 983, 775, 309, 992, 341, 551, 1219, 2316, 13, 17227, 1760, 281, 2074, 420, 7908, 30], "temperature": 0.0, "avg_logprob": -0.1595497872065572, "compression_ratio": 1.7534883720930232, "no_speech_prob": 3.0894725568941794e-06}, {"id": 991, "seek": 430882, "start": 4332.66, "end": 4338.4, "text": " Because some kinds of layers need to have different behavior depending on whether it's", "tokens": [1436, 512, 3685, 295, 7914, 643, 281, 362, 819, 5223, 5413, 322, 1968, 309, 311], "temperature": 0.0, "avg_logprob": -0.1595497872065572, "compression_ratio": 1.7534883720930232, "no_speech_prob": 3.0894725568941794e-06}, {"id": 992, "seek": 433840, "start": 4338.4, "end": 4342.7, "text": " training or evaluation or validation.", "tokens": [3097, 420, 13344, 420, 24071, 13], "temperature": 0.0, "avg_logprob": -0.15195778304455326, "compression_ratio": 1.6816326530612244, "no_speech_prob": 1.0451366506458726e-05}, {"id": 993, "seek": 433840, "start": 4342.7, "end": 4349.82, "text": " For example batch norm only updates its running statistics if it's training.", "tokens": [1171, 1365, 15245, 2026, 787, 9205, 1080, 2614, 12523, 498, 309, 311, 3097, 13], "temperature": 0.0, "avg_logprob": -0.15195778304455326, "compression_ratio": 1.6816326530612244, "no_speech_prob": 1.0451366506458726e-05}, {"id": 994, "seek": 433840, "start": 4349.82, "end": 4353.82, "text": " Dropout only does randomized dropout if it's training.", "tokens": [17675, 346, 787, 775, 38513, 3270, 346, 498, 309, 311, 3097, 13], "temperature": 0.0, "avg_logprob": -0.15195778304455326, "compression_ratio": 1.6816326530612244, "no_speech_prob": 1.0451366506458726e-05}, {"id": 995, "seek": 433840, "start": 4353.82, "end": 4356.0199999999995, "text": " They're the two main ones.", "tokens": [814, 434, 264, 732, 2135, 2306, 13], "temperature": 0.0, "avg_logprob": -0.15195778304455326, "compression_ratio": 1.6816326530612244, "no_speech_prob": 1.0451366506458726e-05}, {"id": 996, "seek": 433840, "start": 4356.0199999999995, "end": 4358.96, "text": " So that's why you always want to have train and eval.", "tokens": [407, 300, 311, 983, 291, 1009, 528, 281, 362, 3847, 293, 1073, 304, 13], "temperature": 0.0, "avg_logprob": -0.15195778304455326, "compression_ratio": 1.6816326530612244, "no_speech_prob": 1.0451366506458726e-05}, {"id": 997, "seek": 433840, "start": 4358.96, "end": 4363.42, "text": " And if you forget to put something into eval mode when you're done training, you'll often", "tokens": [400, 498, 291, 2870, 281, 829, 746, 666, 1073, 304, 4391, 562, 291, 434, 1096, 3097, 11, 291, 603, 2049], "temperature": 0.0, "avg_logprob": -0.15195778304455326, "compression_ratio": 1.6816326530612244, "no_speech_prob": 1.0451366506458726e-05}, {"id": 998, "seek": 433840, "start": 4363.42, "end": 4367.7, "text": " be surprised because you'll be getting worse results than you expected.", "tokens": [312, 6100, 570, 291, 603, 312, 1242, 5324, 3542, 813, 291, 5176, 13], "temperature": 0.0, "avg_logprob": -0.15195778304455326, "compression_ratio": 1.6816326530612244, "no_speech_prob": 1.0451366506458726e-05}, {"id": 999, "seek": 436770, "start": 4367.7, "end": 4370.46, "text": " Okay so that's our fit loop.", "tokens": [1033, 370, 300, 311, 527, 3318, 6367, 13], "temperature": 0.0, "avg_logprob": -0.15856856529158775, "compression_ratio": 1.726829268292683, "no_speech_prob": 6.54038831271464e-06}, {"id": 1000, "seek": 436770, "start": 4370.46, "end": 4376.26, "text": " One thing to note, are these validation results correct if the batch size varies?", "tokens": [1485, 551, 281, 3637, 11, 366, 613, 24071, 3542, 3006, 498, 264, 15245, 2744, 21716, 30], "temperature": 0.0, "avg_logprob": -0.15856856529158775, "compression_ratio": 1.726829268292683, "no_speech_prob": 6.54038831271464e-06}, {"id": 1001, "seek": 436770, "start": 4376.26, "end": 4381.9, "text": " Let's spell that correctly.", "tokens": [961, 311, 9827, 300, 8944, 13], "temperature": 0.0, "avg_logprob": -0.15856856529158775, "compression_ratio": 1.726829268292683, "no_speech_prob": 6.54038831271464e-06}, {"id": 1002, "seek": 436770, "start": 4381.9, "end": 4382.98, "text": " If the batch size varies.", "tokens": [759, 264, 15245, 2744, 21716, 13], "temperature": 0.0, "avg_logprob": -0.15856856529158775, "compression_ratio": 1.726829268292683, "no_speech_prob": 6.54038831271464e-06}, {"id": 1003, "seek": 436770, "start": 4382.98, "end": 4388.58, "text": " Because what we're doing here is we're adding up the loss and we're adding up the accuracy.", "tokens": [1436, 437, 321, 434, 884, 510, 307, 321, 434, 5127, 493, 264, 4470, 293, 321, 434, 5127, 493, 264, 14170, 13], "temperature": 0.0, "avg_logprob": -0.15856856529158775, "compression_ratio": 1.726829268292683, "no_speech_prob": 6.54038831271464e-06}, {"id": 1004, "seek": 436770, "start": 4388.58, "end": 4393.179999999999, "text": " And then at the end we see how big is our data loader, how many batches are there, and", "tokens": [400, 550, 412, 264, 917, 321, 536, 577, 955, 307, 527, 1412, 3677, 260, 11, 577, 867, 15245, 279, 366, 456, 11, 293], "temperature": 0.0, "avg_logprob": -0.15856856529158775, "compression_ratio": 1.726829268292683, "no_speech_prob": 6.54038831271464e-06}, {"id": 1005, "seek": 436770, "start": 4393.179999999999, "end": 4395.179999999999, "text": " we divide.", "tokens": [321, 9845, 13], "temperature": 0.0, "avg_logprob": -0.15856856529158775, "compression_ratio": 1.726829268292683, "no_speech_prob": 6.54038831271464e-06}, {"id": 1006, "seek": 439518, "start": 4395.18, "end": 4400.9800000000005, "text": " But if you think about it, if you had one mini batch of size 1000 and one mini batch", "tokens": [583, 498, 291, 519, 466, 309, 11, 498, 291, 632, 472, 8382, 15245, 295, 2744, 9714, 293, 472, 8382, 15245], "temperature": 0.0, "avg_logprob": -0.1597865526793433, "compression_ratio": 1.773076923076923, "no_speech_prob": 1.529396513433312e-06}, {"id": 1007, "seek": 439518, "start": 4400.9800000000005, "end": 4405.46, "text": " of size 1, you can't actually just do that, right?", "tokens": [295, 2744, 502, 11, 291, 393, 380, 767, 445, 360, 300, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1597865526793433, "compression_ratio": 1.773076923076923, "no_speech_prob": 1.529396513433312e-06}, {"id": 1008, "seek": 439518, "start": 4405.46, "end": 4409.3, "text": " You actually need to do a weighted average, weighted by the size of the mini batch.", "tokens": [509, 767, 643, 281, 360, 257, 32807, 4274, 11, 32807, 538, 264, 2744, 295, 264, 8382, 15245, 13], "temperature": 0.0, "avg_logprob": -0.1597865526793433, "compression_ratio": 1.773076923076923, "no_speech_prob": 1.529396513433312e-06}, {"id": 1009, "seek": 439518, "start": 4409.3, "end": 4413.820000000001, "text": " So this incorrect way is how nearly every library does it.", "tokens": [407, 341, 18424, 636, 307, 577, 6217, 633, 6405, 775, 309, 13], "temperature": 0.0, "avg_logprob": -0.1597865526793433, "compression_ratio": 1.773076923076923, "no_speech_prob": 1.529396513433312e-06}, {"id": 1010, "seek": 439518, "start": 4413.820000000001, "end": 4417.42, "text": " Fast AI does it the proper way and the next time we do this we're going to do it the proper", "tokens": [15968, 7318, 775, 309, 264, 2296, 636, 293, 264, 958, 565, 321, 360, 341, 321, 434, 516, 281, 360, 309, 264, 2296], "temperature": 0.0, "avg_logprob": -0.1597865526793433, "compression_ratio": 1.773076923076923, "no_speech_prob": 1.529396513433312e-06}, {"id": 1011, "seek": 439518, "start": 4417.42, "end": 4418.42, "text": " way.", "tokens": [636, 13], "temperature": 0.0, "avg_logprob": -0.1597865526793433, "compression_ratio": 1.773076923076923, "no_speech_prob": 1.529396513433312e-06}, {"id": 1012, "seek": 439518, "start": 4418.42, "end": 4423.780000000001, "text": " But for now here's what most people do and it does not work correctly when your batch", "tokens": [583, 337, 586, 510, 311, 437, 881, 561, 360, 293, 309, 775, 406, 589, 8944, 562, 428, 15245], "temperature": 0.0, "avg_logprob": -0.1597865526793433, "compression_ratio": 1.773076923076923, "no_speech_prob": 1.529396513433312e-06}, {"id": 1013, "seek": 442378, "start": 4423.78, "end": 4427.94, "text": " size varies.", "tokens": [2744, 21716, 13], "temperature": 0.0, "avg_logprob": -0.12154543841326679, "compression_ratio": 1.8472222222222223, "no_speech_prob": 3.218929487047717e-05}, {"id": 1014, "seek": 442378, "start": 4427.94, "end": 4432.38, "text": " So it's handy to have something that we can basically pass in a training data set and", "tokens": [407, 309, 311, 13239, 281, 362, 746, 300, 321, 393, 1936, 1320, 294, 257, 3097, 1412, 992, 293], "temperature": 0.0, "avg_logprob": -0.12154543841326679, "compression_ratio": 1.8472222222222223, "no_speech_prob": 3.218929487047717e-05}, {"id": 1015, "seek": 442378, "start": 4432.38, "end": 4437.219999999999, "text": " a validation data set and a batch size to and just grab the data loaders.", "tokens": [257, 24071, 1412, 992, 293, 257, 15245, 2744, 281, 293, 445, 4444, 264, 1412, 3677, 433, 13], "temperature": 0.0, "avg_logprob": -0.12154543841326679, "compression_ratio": 1.8472222222222223, "no_speech_prob": 3.218929487047717e-05}, {"id": 1016, "seek": 442378, "start": 4437.219999999999, "end": 4442.58, "text": " The training data set will be shuffled, validation won't be shuffled.", "tokens": [440, 3097, 1412, 992, 486, 312, 402, 33974, 11, 24071, 1582, 380, 312, 402, 33974, 13], "temperature": 0.0, "avg_logprob": -0.12154543841326679, "compression_ratio": 1.8472222222222223, "no_speech_prob": 3.218929487047717e-05}, {"id": 1017, "seek": 442378, "start": 4442.58, "end": 4446.86, "text": " Also the validation data set, we don't need to do the backward pass, so we don't need", "tokens": [2743, 264, 24071, 1412, 992, 11, 321, 500, 380, 643, 281, 360, 264, 23897, 1320, 11, 370, 321, 500, 380, 643], "temperature": 0.0, "avg_logprob": -0.12154543841326679, "compression_ratio": 1.8472222222222223, "no_speech_prob": 3.218929487047717e-05}, {"id": 1018, "seek": 442378, "start": 4446.86, "end": 4448.3, "text": " to store the gradients.", "tokens": [281, 3531, 264, 2771, 2448, 13], "temperature": 0.0, "avg_logprob": -0.12154543841326679, "compression_ratio": 1.8472222222222223, "no_speech_prob": 3.218929487047717e-05}, {"id": 1019, "seek": 442378, "start": 4448.3, "end": 4451.219999999999, "text": " So that means that we have twice as much room.", "tokens": [407, 300, 1355, 300, 321, 362, 6091, 382, 709, 1808, 13], "temperature": 0.0, "avg_logprob": -0.12154543841326679, "compression_ratio": 1.8472222222222223, "no_speech_prob": 3.218929487047717e-05}, {"id": 1020, "seek": 445122, "start": 4451.22, "end": 4454.54, "text": " So we can make it twice the batch size.", "tokens": [407, 321, 393, 652, 309, 6091, 264, 15245, 2744, 13], "temperature": 0.0, "avg_logprob": -0.13544470010344514, "compression_ratio": 1.5605381165919283, "no_speech_prob": 5.862645139131928e-06}, {"id": 1021, "seek": 445122, "start": 4454.54, "end": 4458.3, "text": " So it's another nice thing to refactor out, you don't have to type it anymore and also", "tokens": [407, 309, 311, 1071, 1481, 551, 281, 1895, 15104, 484, 11, 291, 500, 380, 362, 281, 2010, 309, 3602, 293, 611], "temperature": 0.0, "avg_logprob": -0.13544470010344514, "compression_ratio": 1.5605381165919283, "no_speech_prob": 5.862645139131928e-06}, {"id": 1022, "seek": 445122, "start": 4458.3, "end": 4462.9400000000005, "text": " it means that you won't accidentally make a mistake.", "tokens": [309, 1355, 300, 291, 1582, 380, 15715, 652, 257, 6146, 13], "temperature": 0.0, "avg_logprob": -0.13544470010344514, "compression_ratio": 1.5605381165919283, "no_speech_prob": 5.862645139131928e-06}, {"id": 1023, "seek": 445122, "start": 4462.9400000000005, "end": 4468.9400000000005, "text": " And so now we can go ahead and fit and let's do five epochs and so now these are actual", "tokens": [400, 370, 586, 321, 393, 352, 2286, 293, 3318, 293, 718, 311, 360, 1732, 30992, 28346, 293, 370, 586, 613, 366, 3539], "temperature": 0.0, "avg_logprob": -0.13544470010344514, "compression_ratio": 1.5605381165919283, "no_speech_prob": 5.862645139131928e-06}, {"id": 1024, "seek": 445122, "start": 4468.9400000000005, "end": 4470.9400000000005, "text": " validation accuracies.", "tokens": [24071, 5771, 20330, 13], "temperature": 0.0, "avg_logprob": -0.13544470010344514, "compression_ratio": 1.5605381165919283, "no_speech_prob": 5.862645139131928e-06}, {"id": 1025, "seek": 445122, "start": 4470.9400000000005, "end": 4473.02, "text": " Okay, great.", "tokens": [1033, 11, 869, 13], "temperature": 0.0, "avg_logprob": -0.13544470010344514, "compression_ratio": 1.5605381165919283, "no_speech_prob": 5.862645139131928e-06}, {"id": 1026, "seek": 445122, "start": 4473.02, "end": 4476.9400000000005, "text": " So we've successfully built a training loop.", "tokens": [407, 321, 600, 10727, 3094, 257, 3097, 6367, 13], "temperature": 0.0, "avg_logprob": -0.13544470010344514, "compression_ratio": 1.5605381165919283, "no_speech_prob": 5.862645139131928e-06}, {"id": 1027, "seek": 447694, "start": 4476.94, "end": 4484.299999999999, "text": " Let's have a six minute break, come back at 7.55 and talk about callbacks.", "tokens": [961, 311, 362, 257, 2309, 3456, 1821, 11, 808, 646, 412, 1614, 13, 13622, 293, 751, 466, 818, 17758, 13], "temperature": 0.0, "avg_logprob": -0.1572248810216, "compression_ratio": 1.481012658227848, "no_speech_prob": 2.8849955924670212e-05}, {"id": 1028, "seek": 447694, "start": 4484.299999999999, "end": 4488.259999999999, "text": " Before we continue, Rachel, any questions?", "tokens": [4546, 321, 2354, 11, 14246, 11, 604, 1651, 30], "temperature": 0.0, "avg_logprob": -0.1572248810216, "compression_ratio": 1.481012658227848, "no_speech_prob": 2.8849955924670212e-05}, {"id": 1029, "seek": 447694, "start": 4488.259999999999, "end": 4491.259999999999, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.1572248810216, "compression_ratio": 1.481012658227848, "no_speech_prob": 2.8849955924670212e-05}, {"id": 1030, "seek": 447694, "start": 4491.259999999999, "end": 4496.94, "text": " So why do we have to zero out our gradients in PyTorch?", "tokens": [407, 983, 360, 321, 362, 281, 4018, 484, 527, 2771, 2448, 294, 9953, 51, 284, 339, 30], "temperature": 0.0, "avg_logprob": -0.1572248810216, "compression_ratio": 1.481012658227848, "no_speech_prob": 2.8849955924670212e-05}, {"id": 1031, "seek": 447694, "start": 4496.94, "end": 4499.7, "text": " Why do you have to zero out your gradients in PyTorch?", "tokens": [1545, 360, 291, 362, 281, 4018, 484, 428, 2771, 2448, 294, 9953, 51, 284, 339, 30], "temperature": 0.0, "avg_logprob": -0.1572248810216, "compression_ratio": 1.481012658227848, "no_speech_prob": 2.8849955924670212e-05}, {"id": 1032, "seek": 449970, "start": 4499.7, "end": 4508.86, "text": " So yeah, the way we...", "tokens": [407, 1338, 11, 264, 636, 321, 485], "temperature": 0.0, "avg_logprob": -0.2237306833267212, "compression_ratio": 1.2242990654205608, "no_speech_prob": 1.6278823977700085e-06}, {"id": 1033, "seek": 449970, "start": 4508.86, "end": 4512.179999999999, "text": " Let's go back to...", "tokens": [961, 311, 352, 646, 281, 485], "temperature": 0.0, "avg_logprob": -0.2237306833267212, "compression_ratio": 1.2242990654205608, "no_speech_prob": 1.6278823977700085e-06}, {"id": 1034, "seek": 449970, "start": 4512.179999999999, "end": 4516.34, "text": " So here's our optimizer, right?", "tokens": [407, 510, 311, 527, 5028, 6545, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2237306833267212, "compression_ratio": 1.2242990654205608, "no_speech_prob": 1.6278823977700085e-06}, {"id": 1035, "seek": 449970, "start": 4516.34, "end": 4522.58, "text": " Or let's go back even further.", "tokens": [1610, 718, 311, 352, 646, 754, 3052, 13], "temperature": 0.0, "avg_logprob": -0.2237306833267212, "compression_ratio": 1.2242990654205608, "no_speech_prob": 1.6278823977700085e-06}, {"id": 1036, "seek": 449970, "start": 4522.58, "end": 4525.0199999999995, "text": " Here's our first version.", "tokens": [1692, 311, 527, 700, 3037, 13], "temperature": 0.0, "avg_logprob": -0.2237306833267212, "compression_ratio": 1.2242990654205608, "no_speech_prob": 1.6278823977700085e-06}, {"id": 1037, "seek": 452502, "start": 4525.02, "end": 4533.14, "text": " So this is just with no additional help from PyTorch at all.", "tokens": [407, 341, 307, 445, 365, 572, 4497, 854, 490, 9953, 51, 284, 339, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.12725573221842448, "compression_ratio": 1.463276836158192, "no_speech_prob": 1.2289024198253173e-06}, {"id": 1038, "seek": 452502, "start": 4533.14, "end": 4540.18, "text": " If we didn't go grad.zero here, then what's gonna happen the next time we go through and", "tokens": [759, 321, 994, 380, 352, 2771, 13, 32226, 510, 11, 550, 437, 311, 799, 1051, 264, 958, 565, 321, 352, 807, 293], "temperature": 0.0, "avg_logprob": -0.12725573221842448, "compression_ratio": 1.463276836158192, "no_speech_prob": 1.2289024198253173e-06}, {"id": 1039, "seek": 452502, "start": 4540.18, "end": 4546.780000000001, "text": " say lost.backward is it's gonna add the new gradients to those existing gradients.", "tokens": [584, 2731, 13, 3207, 1007, 307, 309, 311, 799, 909, 264, 777, 2771, 2448, 281, 729, 6741, 2771, 2448, 13], "temperature": 0.0, "avg_logprob": -0.12725573221842448, "compression_ratio": 1.463276836158192, "no_speech_prob": 1.2289024198253173e-06}, {"id": 1040, "seek": 452502, "start": 4546.780000000001, "end": 4548.540000000001, "text": " Now, why does that happen?", "tokens": [823, 11, 983, 775, 300, 1051, 30], "temperature": 0.0, "avg_logprob": -0.12725573221842448, "compression_ratio": 1.463276836158192, "no_speech_prob": 1.2289024198253173e-06}, {"id": 1041, "seek": 454854, "start": 4548.54, "end": 4555.42, "text": " Well, that happens because we often have lots of sources of gradients.", "tokens": [1042, 11, 300, 2314, 570, 321, 2049, 362, 3195, 295, 7139, 295, 2771, 2448, 13], "temperature": 0.0, "avg_logprob": -0.10217554011243454, "compression_ratio": 1.7797356828193833, "no_speech_prob": 3.611909278333769e-06}, {"id": 1042, "seek": 454854, "start": 4555.42, "end": 4559.9, "text": " There's lots of different modules all connected together and so they're getting their gradients", "tokens": [821, 311, 3195, 295, 819, 16679, 439, 4582, 1214, 293, 370, 436, 434, 1242, 641, 2771, 2448], "temperature": 0.0, "avg_logprob": -0.10217554011243454, "compression_ratio": 1.7797356828193833, "no_speech_prob": 3.611909278333769e-06}, {"id": 1043, "seek": 454854, "start": 4559.9, "end": 4563.46, "text": " from lots of different places and they all have to be added up.", "tokens": [490, 3195, 295, 819, 3190, 293, 436, 439, 362, 281, 312, 3869, 493, 13], "temperature": 0.0, "avg_logprob": -0.10217554011243454, "compression_ratio": 1.7797356828193833, "no_speech_prob": 3.611909278333769e-06}, {"id": 1044, "seek": 454854, "start": 4563.46, "end": 4569.34, "text": " So when we call backward, we wouldn't want backward to zero the gradients because then", "tokens": [407, 562, 321, 818, 23897, 11, 321, 2759, 380, 528, 23897, 281, 4018, 264, 2771, 2448, 570, 550], "temperature": 0.0, "avg_logprob": -0.10217554011243454, "compression_ratio": 1.7797356828193833, "no_speech_prob": 3.611909278333769e-06}, {"id": 1045, "seek": 454854, "start": 4569.34, "end": 4576.46, "text": " we would lose this ability to plug in lots of things together and just have them work.", "tokens": [321, 576, 3624, 341, 3485, 281, 5452, 294, 3195, 295, 721, 1214, 293, 445, 362, 552, 589, 13], "temperature": 0.0, "avg_logprob": -0.10217554011243454, "compression_ratio": 1.7797356828193833, "no_speech_prob": 3.611909278333769e-06}, {"id": 1046, "seek": 457646, "start": 4576.46, "end": 4581.86, "text": " So that's why we need the grad.zero here.", "tokens": [407, 300, 311, 983, 321, 643, 264, 2771, 13, 32226, 510, 13], "temperature": 0.0, "avg_logprob": -0.10541781750354139, "compression_ratio": 1.827027027027027, "no_speech_prob": 3.28868236465496e-06}, {"id": 1047, "seek": 457646, "start": 4581.86, "end": 4585.66, "text": " So then that's part one of the answer.", "tokens": [407, 550, 300, 311, 644, 472, 295, 264, 1867, 13], "temperature": 0.0, "avg_logprob": -0.10541781750354139, "compression_ratio": 1.827027027027027, "no_speech_prob": 3.28868236465496e-06}, {"id": 1048, "seek": 457646, "start": 4585.66, "end": 4591.18, "text": " Part two of the answer is why did we write our optimizer so that it was one thing called", "tokens": [4100, 732, 295, 264, 1867, 307, 983, 630, 321, 2464, 527, 5028, 6545, 370, 300, 309, 390, 472, 551, 1219], "temperature": 0.0, "avg_logprob": -0.10541781750354139, "compression_ratio": 1.827027027027027, "no_speech_prob": 3.28868236465496e-06}, {"id": 1049, "seek": 457646, "start": 4591.18, "end": 4593.58, "text": " step and one thing called zero grad?", "tokens": [1823, 293, 472, 551, 1219, 4018, 2771, 30], "temperature": 0.0, "avg_logprob": -0.10541781750354139, "compression_ratio": 1.827027027027027, "no_speech_prob": 3.28868236465496e-06}, {"id": 1050, "seek": 457646, "start": 4593.58, "end": 4599.1, "text": " Because what we could have done is we could have removed these lines and pushed this up", "tokens": [1436, 437, 321, 727, 362, 1096, 307, 321, 727, 362, 7261, 613, 3876, 293, 9152, 341, 493], "temperature": 0.0, "avg_logprob": -0.10541781750354139, "compression_ratio": 1.827027027027027, "no_speech_prob": 3.28868236465496e-06}, {"id": 1051, "seek": 457646, "start": 4599.1, "end": 4605.26, "text": " here and so that step could have done both.", "tokens": [510, 293, 370, 300, 1823, 727, 362, 1096, 1293, 13], "temperature": 0.0, "avg_logprob": -0.10541781750354139, "compression_ratio": 1.827027027027027, "no_speech_prob": 3.28868236465496e-06}, {"id": 1052, "seek": 460526, "start": 4605.26, "end": 4609.5, "text": " And then since we've actually got this kind of twice now, we could put it all inside the", "tokens": [400, 550, 1670, 321, 600, 767, 658, 341, 733, 295, 6091, 586, 11, 321, 727, 829, 309, 439, 1854, 264], "temperature": 0.0, "avg_logprob": -0.11436270361077296, "compression_ratio": 1.5401069518716577, "no_speech_prob": 2.058018026218633e-06}, {"id": 1053, "seek": 460526, "start": 4609.5, "end": 4613.02, "text": " for loop.", "tokens": [337, 6367, 13], "temperature": 0.0, "avg_logprob": -0.11436270361077296, "compression_ratio": 1.5401069518716577, "no_speech_prob": 2.058018026218633e-06}, {"id": 1054, "seek": 460526, "start": 4613.02, "end": 4617.14, "text": " So we could certainly have written our optimizer like this as it goes through each parameter", "tokens": [407, 321, 727, 3297, 362, 3720, 527, 5028, 6545, 411, 341, 382, 309, 1709, 807, 1184, 13075], "temperature": 0.0, "avg_logprob": -0.11436270361077296, "compression_ratio": 1.5401069518716577, "no_speech_prob": 2.058018026218633e-06}, {"id": 1055, "seek": 460526, "start": 4617.14, "end": 4622.900000000001, "text": " and does the update and sets the gradient to zero and then we would be able to remove", "tokens": [293, 775, 264, 5623, 293, 6352, 264, 16235, 281, 4018, 293, 550, 321, 576, 312, 1075, 281, 4159], "temperature": 0.0, "avg_logprob": -0.11436270361077296, "compression_ratio": 1.5401069518716577, "no_speech_prob": 2.058018026218633e-06}, {"id": 1056, "seek": 460526, "start": 4622.900000000001, "end": 4626.22, "text": " this line.", "tokens": [341, 1622, 13], "temperature": 0.0, "avg_logprob": -0.11436270361077296, "compression_ratio": 1.5401069518716577, "no_speech_prob": 2.058018026218633e-06}, {"id": 1057, "seek": 462622, "start": 4626.22, "end": 4639.780000000001, "text": " So the problem with that is that we then remove the ability to not zero the gradients here", "tokens": [407, 264, 1154, 365, 300, 307, 300, 321, 550, 4159, 264, 3485, 281, 406, 4018, 264, 2771, 2448, 510], "temperature": 0.0, "avg_logprob": -0.09608813878652211, "compression_ratio": 1.6331360946745561, "no_speech_prob": 4.116344882731937e-07}, {"id": 1058, "seek": 462622, "start": 4639.780000000001, "end": 4643.66, "text": " and that means any time that we don't want to zero the gradients, we now can't use the", "tokens": [293, 300, 1355, 604, 565, 300, 321, 500, 380, 528, 281, 4018, 264, 2771, 2448, 11, 321, 586, 393, 380, 764, 264], "temperature": 0.0, "avg_logprob": -0.09608813878652211, "compression_ratio": 1.6331360946745561, "no_speech_prob": 4.116344882731937e-07}, {"id": 1059, "seek": 462622, "start": 4643.66, "end": 4644.9400000000005, "text": " optimizer.", "tokens": [5028, 6545, 13], "temperature": 0.0, "avg_logprob": -0.09608813878652211, "compression_ratio": 1.6331360946745561, "no_speech_prob": 4.116344882731937e-07}, {"id": 1060, "seek": 462622, "start": 4644.9400000000005, "end": 4653.54, "text": " So for example, what if you are working with some pretty big objects, so like if you're", "tokens": [407, 337, 1365, 11, 437, 498, 291, 366, 1364, 365, 512, 1238, 955, 6565, 11, 370, 411, 498, 291, 434], "temperature": 0.0, "avg_logprob": -0.09608813878652211, "compression_ratio": 1.6331360946745561, "no_speech_prob": 4.116344882731937e-07}, {"id": 1061, "seek": 465354, "start": 4653.54, "end": 4660.1, "text": " doing super resolution and you're trying to create a 2K output, you know, your batch size,", "tokens": [884, 1687, 8669, 293, 291, 434, 1382, 281, 1884, 257, 568, 42, 5598, 11, 291, 458, 11, 428, 15245, 2744, 11], "temperature": 0.0, "avg_logprob": -0.09930706024169922, "compression_ratio": 1.5919282511210762, "no_speech_prob": 3.785204853556934e-06}, {"id": 1062, "seek": 465354, "start": 4660.1, "end": 4670.3, "text": " you can only fit two images on the GPU at a time and the stability of the gradients", "tokens": [291, 393, 787, 3318, 732, 5267, 322, 264, 18407, 412, 257, 565, 293, 264, 11826, 295, 264, 2771, 2448], "temperature": 0.0, "avg_logprob": -0.09930706024169922, "compression_ratio": 1.5919282511210762, "no_speech_prob": 3.785204853556934e-06}, {"id": 1063, "seek": 465354, "start": 4670.3, "end": 4675.58, "text": " that you get from a batch size of two is so poor that you need to use a larger batch size.", "tokens": [300, 291, 483, 490, 257, 15245, 2744, 295, 732, 307, 370, 4716, 300, 291, 643, 281, 764, 257, 4833, 15245, 2744, 13], "temperature": 0.0, "avg_logprob": -0.09930706024169922, "compression_ratio": 1.5919282511210762, "no_speech_prob": 3.785204853556934e-06}, {"id": 1064, "seek": 465354, "start": 4675.58, "end": 4683.38, "text": " So well, that would be really easy to do if you did it like this, right, because we could", "tokens": [407, 731, 11, 300, 576, 312, 534, 1858, 281, 360, 498, 291, 630, 309, 411, 341, 11, 558, 11, 570, 321, 727], "temperature": 0.0, "avg_logprob": -0.09930706024169922, "compression_ratio": 1.5919282511210762, "no_speech_prob": 3.785204853556934e-06}, {"id": 1065, "seek": 468338, "start": 4683.38, "end": 4702.54, "text": " say if I percent two, then, right, and so this is now going to only run these things", "tokens": [584, 498, 286, 3043, 732, 11, 550, 11, 558, 11, 293, 370, 341, 307, 586, 516, 281, 787, 1190, 613, 721], "temperature": 0.0, "avg_logprob": -0.16938941423283066, "compression_ratio": 1.3658536585365855, "no_speech_prob": 1.529398559796391e-06}, {"id": 1066, "seek": 468338, "start": 4702.54, "end": 4712.0, "text": " every two iterations and so that means that our effective batch size is now double.", "tokens": [633, 732, 36540, 293, 370, 300, 1355, 300, 527, 4942, 15245, 2744, 307, 586, 3834, 13], "temperature": 0.0, "avg_logprob": -0.16938941423283066, "compression_ratio": 1.3658536585365855, "no_speech_prob": 1.529398559796391e-06}, {"id": 1067, "seek": 471200, "start": 4712.0, "end": 4715.18, "text": " So that's handy, right, that's called gradient accumulation.", "tokens": [407, 300, 311, 13239, 11, 558, 11, 300, 311, 1219, 16235, 35647, 13], "temperature": 0.0, "avg_logprob": -0.12951855161296788, "compression_ratio": 1.6845238095238095, "no_speech_prob": 6.642875177931273e-06}, {"id": 1068, "seek": 471200, "start": 4715.18, "end": 4721.98, "text": " The gradient accumulation is where you change your training loop so that your optimizer", "tokens": [440, 16235, 35647, 307, 689, 291, 1319, 428, 3097, 6367, 370, 300, 428, 5028, 6545], "temperature": 0.0, "avg_logprob": -0.12951855161296788, "compression_ratio": 1.6845238095238095, "no_speech_prob": 6.642875177931273e-06}, {"id": 1069, "seek": 471200, "start": 4721.98, "end": 4726.62, "text": " step and your zero grads only happen occasionally.", "tokens": [1823, 293, 428, 4018, 2771, 82, 787, 1051, 16895, 13], "temperature": 0.0, "avg_logprob": -0.12951855161296788, "compression_ratio": 1.6845238095238095, "no_speech_prob": 6.642875177931273e-06}, {"id": 1070, "seek": 471200, "start": 4726.62, "end": 4734.42, "text": " So that's really the reason is that there might be times you don't want to zero the", "tokens": [407, 300, 311, 534, 264, 1778, 307, 300, 456, 1062, 312, 1413, 291, 500, 380, 528, 281, 4018, 264], "temperature": 0.0, "avg_logprob": -0.12951855161296788, "compression_ratio": 1.6845238095238095, "no_speech_prob": 6.642875177931273e-06}, {"id": 1071, "seek": 473442, "start": 4734.42, "end": 4743.54, "text": " gradients every time you do a step and if there's nowhere to do that, that's a problem.", "tokens": [2771, 2448, 633, 565, 291, 360, 257, 1823, 293, 498, 456, 311, 11159, 281, 360, 300, 11, 300, 311, 257, 1154, 13], "temperature": 0.0, "avg_logprob": -0.110958620707194, "compression_ratio": 1.5254237288135593, "no_speech_prob": 8.664179404149763e-06}, {"id": 1072, "seek": 473442, "start": 4743.54, "end": 4747.74, "text": " You could argue that, I can't think of a reason that this isn't a good idea, we could make", "tokens": [509, 727, 9695, 300, 11, 286, 393, 380, 519, 295, 257, 1778, 300, 341, 1943, 380, 257, 665, 1558, 11, 321, 727, 652], "temperature": 0.0, "avg_logprob": -0.110958620707194, "compression_ratio": 1.5254237288135593, "no_speech_prob": 8.664179404149763e-06}, {"id": 1073, "seek": 473442, "start": 4747.74, "end": 4755.7, "text": " our optimizer, we could say kind of like auto zero equals true, say, and then we could have", "tokens": [527, 5028, 6545, 11, 321, 727, 584, 733, 295, 411, 8399, 4018, 6915, 2074, 11, 584, 11, 293, 550, 321, 727, 362], "temperature": 0.0, "avg_logprob": -0.110958620707194, "compression_ratio": 1.5254237288135593, "no_speech_prob": 8.664179404149763e-06}, {"id": 1074, "seek": 475570, "start": 4755.7, "end": 4768.34, "text": " something in here which kind of says like if self.auto zero, then self.zero grad, right,", "tokens": [746, 294, 510, 597, 733, 295, 1619, 411, 498, 2698, 13, 41988, 4018, 11, 550, 2698, 13, 32226, 2771, 11, 558, 11], "temperature": 0.0, "avg_logprob": -0.1349490910041623, "compression_ratio": 1.5849056603773586, "no_speech_prob": 5.0933849706780165e-06}, {"id": 1075, "seek": 475570, "start": 4768.34, "end": 4772.7, "text": " something like that and then that could even be the default and then you wouldn't have", "tokens": [746, 411, 300, 293, 550, 300, 727, 754, 312, 264, 7576, 293, 550, 291, 2759, 380, 362], "temperature": 0.0, "avg_logprob": -0.1349490910041623, "compression_ratio": 1.5849056603773586, "no_speech_prob": 5.0933849706780165e-06}, {"id": 1076, "seek": 475570, "start": 4772.7, "end": 4776.58, "text": " to worry about it unless you explicitly wanted to do gradient accumulation.", "tokens": [281, 3292, 466, 309, 5969, 291, 20803, 1415, 281, 360, 16235, 35647, 13], "temperature": 0.0, "avg_logprob": -0.1349490910041623, "compression_ratio": 1.5849056603773586, "no_speech_prob": 5.0933849706780165e-06}, {"id": 1077, "seek": 475570, "start": 4776.58, "end": 4782.179999999999, "text": " I think that would really be a better API design, maybe, but that's not what they've", "tokens": [286, 519, 300, 576, 534, 312, 257, 1101, 9362, 1715, 11, 1310, 11, 457, 300, 311, 406, 437, 436, 600], "temperature": 0.0, "avg_logprob": -0.1349490910041623, "compression_ratio": 1.5849056603773586, "no_speech_prob": 5.0933849706780165e-06}, {"id": 1078, "seek": 478218, "start": 4782.18, "end": 4786.700000000001, "text": " done but it's so easy to write your own optimizers, you could totally do that.", "tokens": [1096, 457, 309, 311, 370, 1858, 281, 2464, 428, 1065, 5028, 22525, 11, 291, 727, 3879, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.3174506522513725, "compression_ratio": 1.3910614525139664, "no_speech_prob": 1.4063323760638013e-05}, {"id": 1079, "seek": 478218, "start": 4786.700000000001, "end": 4791.9400000000005, "text": " But I mean, you know, the upside is removing a single line of code which isn't a huge upside", "tokens": [583, 286, 914, 11, 291, 458, 11, 264, 14119, 307, 12720, 257, 2167, 1622, 295, 3089, 597, 1943, 380, 257, 2603, 14119], "temperature": 0.0, "avg_logprob": -0.3174506522513725, "compression_ratio": 1.3910614525139664, "no_speech_prob": 1.4063323760638013e-05}, {"id": 1080, "seek": 478218, "start": 4791.9400000000005, "end": 4795.3, "text": " anyway.", "tokens": [4033, 13], "temperature": 0.0, "avg_logprob": -0.3174506522513725, "compression_ratio": 1.3910614525139664, "no_speech_prob": 1.4063323760638013e-05}, {"id": 1081, "seek": 478218, "start": 4795.3, "end": 4798.02, "text": " Any other questions, Rachel?", "tokens": [2639, 661, 1651, 11, 14246, 30], "temperature": 0.0, "avg_logprob": -0.3174506522513725, "compression_ratio": 1.3910614525139664, "no_speech_prob": 1.4063323760638013e-05}, {"id": 1082, "seek": 478218, "start": 4798.02, "end": 4799.5, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.3174506522513725, "compression_ratio": 1.3910614525139664, "no_speech_prob": 1.4063323760638013e-05}, {"id": 1083, "seek": 478218, "start": 4799.5, "end": 4806.9400000000005, "text": " Okay, so that's our training loop.", "tokens": [1033, 11, 370, 300, 311, 527, 3097, 6367, 13], "temperature": 0.0, "avg_logprob": -0.3174506522513725, "compression_ratio": 1.3910614525139664, "no_speech_prob": 1.4063323760638013e-05}, {"id": 1084, "seek": 480694, "start": 4806.94, "end": 4814.139999999999, "text": " But it's not quite where we want it to be and I'm stealing some slides here from Sylvain", "tokens": [583, 309, 311, 406, 1596, 689, 321, 528, 309, 281, 312, 293, 286, 478, 19757, 512, 9788, 510, 490, 3902, 14574, 491], "temperature": 0.0, "avg_logprob": -0.12751718248639787, "compression_ratio": 1.4827586206896552, "no_speech_prob": 2.0778243197128177e-05}, {"id": 1085, "seek": 480694, "start": 4814.139999999999, "end": 4819.139999999999, "text": " who had a really cool talk recently called an infinitely customizable training loop so", "tokens": [567, 632, 257, 534, 1627, 751, 3938, 1219, 364, 36227, 47922, 3097, 6367, 370], "temperature": 0.0, "avg_logprob": -0.12751718248639787, "compression_ratio": 1.4827586206896552, "no_speech_prob": 2.0778243197128177e-05}, {"id": 1086, "seek": 480694, "start": 4819.139999999999, "end": 4821.46, "text": " I'll steal his slides.", "tokens": [286, 603, 11009, 702, 9788, 13], "temperature": 0.0, "avg_logprob": -0.12751718248639787, "compression_ratio": 1.4827586206896552, "no_speech_prob": 2.0778243197128177e-05}, {"id": 1087, "seek": 480694, "start": 4821.46, "end": 4827.66, "text": " Before I do, I would like to do a big thank you to Sylvain.", "tokens": [4546, 286, 360, 11, 286, 576, 411, 281, 360, 257, 955, 1309, 291, 281, 3902, 14574, 491, 13], "temperature": 0.0, "avg_logprob": -0.12751718248639787, "compression_ratio": 1.4827586206896552, "no_speech_prob": 2.0778243197128177e-05}, {"id": 1088, "seek": 482766, "start": 4827.66, "end": 4837.94, "text": " He has been working full time with Fast AI for well over a year now, I guess, and a huge", "tokens": [634, 575, 668, 1364, 1577, 565, 365, 15968, 7318, 337, 731, 670, 257, 1064, 586, 11, 286, 2041, 11, 293, 257, 2603], "temperature": 0.0, "avg_logprob": -0.14929396382878335, "compression_ratio": 1.5420560747663552, "no_speech_prob": 3.2367527182941558e-06}, {"id": 1089, "seek": 482766, "start": 4837.94, "end": 4844.54, "text": " amount of what you see in the Fast AI library and research and courses is him.", "tokens": [2372, 295, 437, 291, 536, 294, 264, 15968, 7318, 6405, 293, 2132, 293, 7712, 307, 796, 13], "temperature": 0.0, "avg_logprob": -0.14929396382878335, "compression_ratio": 1.5420560747663552, "no_speech_prob": 3.2367527182941558e-06}, {"id": 1090, "seek": 482766, "start": 4844.54, "end": 4851.099999999999, "text": " So massive thank you to Sylvain who's the most awesome person I've worked with in my", "tokens": [407, 5994, 1309, 291, 281, 3902, 14574, 491, 567, 311, 264, 881, 3476, 954, 286, 600, 2732, 365, 294, 452], "temperature": 0.0, "avg_logprob": -0.14929396382878335, "compression_ratio": 1.5420560747663552, "no_speech_prob": 3.2367527182941558e-06}, {"id": 1091, "seek": 482766, "start": 4851.099999999999, "end": 4854.16, "text": " whole life so that's pretty cool.", "tokens": [1379, 993, 370, 300, 311, 1238, 1627, 13], "temperature": 0.0, "avg_logprob": -0.14929396382878335, "compression_ratio": 1.5420560747663552, "no_speech_prob": 3.2367527182941558e-06}, {"id": 1092, "seek": 482766, "start": 4854.16, "end": 4856.54, "text": " But also thank you to lots of other people.", "tokens": [583, 611, 1309, 291, 281, 3195, 295, 661, 561, 13], "temperature": 0.0, "avg_logprob": -0.14929396382878335, "compression_ratio": 1.5420560747663552, "no_speech_prob": 3.2367527182941558e-06}, {"id": 1093, "seek": 485654, "start": 4856.54, "end": 4862.62, "text": " Huge thanks to Staz who a lot of you all have come across in the forum and he's done a lot", "tokens": [37043, 3231, 281, 745, 921, 567, 257, 688, 295, 291, 439, 362, 808, 2108, 294, 264, 17542, 293, 415, 311, 1096, 257, 688], "temperature": 0.0, "avg_logprob": -0.15137471471514022, "compression_ratio": 1.7014925373134329, "no_speech_prob": 3.4231285098940134e-05}, {"id": 1094, "seek": 485654, "start": 4862.62, "end": 4868.9, "text": " of the stuff that makes Fast AI work well and he's entirely a volunteer so like super", "tokens": [295, 264, 1507, 300, 1669, 15968, 7318, 589, 731, 293, 415, 311, 7696, 257, 13835, 370, 411, 1687], "temperature": 0.0, "avg_logprob": -0.15137471471514022, "compression_ratio": 1.7014925373134329, "no_speech_prob": 3.4231285098940134e-05}, {"id": 1095, "seek": 485654, "start": 4868.9, "end": 4871.7, "text": " grateful to him.", "tokens": [7941, 281, 796, 13], "temperature": 0.0, "avg_logprob": -0.15137471471514022, "compression_ratio": 1.7014925373134329, "no_speech_prob": 3.4231285098940134e-05}, {"id": 1096, "seek": 485654, "start": 4871.7, "end": 4875.0199999999995, "text": " The stuff that lets you check whether your installation works properly, that lets you", "tokens": [440, 1507, 300, 6653, 291, 1520, 1968, 428, 13260, 1985, 6108, 11, 300, 6653, 291], "temperature": 0.0, "avg_logprob": -0.15137471471514022, "compression_ratio": 1.7014925373134329, "no_speech_prob": 3.4231285098940134e-05}, {"id": 1097, "seek": 485654, "start": 4875.0199999999995, "end": 4878.8, "text": " quickly check whether your performance is what it should be.", "tokens": [2661, 1520, 1968, 428, 3389, 307, 437, 309, 820, 312, 13], "temperature": 0.0, "avg_logprob": -0.15137471471514022, "compression_ratio": 1.7014925373134329, "no_speech_prob": 3.4231285098940134e-05}, {"id": 1098, "seek": 485654, "start": 4878.8, "end": 4883.3, "text": " Also like organizing lots of helpful projects through the forums.", "tokens": [2743, 411, 17608, 3195, 295, 4961, 4455, 807, 264, 26998, 13], "temperature": 0.0, "avg_logprob": -0.15137471471514022, "compression_ratio": 1.7014925373134329, "no_speech_prob": 3.4231285098940134e-05}, {"id": 1099, "seek": 485654, "start": 4883.3, "end": 4884.3, "text": " He's been fantastic.", "tokens": [634, 311, 668, 5456, 13], "temperature": 0.0, "avg_logprob": -0.15137471471514022, "compression_ratio": 1.7014925373134329, "no_speech_prob": 3.4231285098940134e-05}, {"id": 1100, "seek": 485654, "start": 4884.3, "end": 4886.26, "text": " Lots of other folks as well.", "tokens": [15908, 295, 661, 4024, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.15137471471514022, "compression_ratio": 1.7014925373134329, "no_speech_prob": 3.4231285098940134e-05}, {"id": 1101, "seek": 488626, "start": 4886.26, "end": 4891.780000000001, "text": " Andrew Shaw wrote a lot of the original documentation stuff that we have.", "tokens": [10110, 27132, 4114, 257, 688, 295, 264, 3380, 14333, 1507, 300, 321, 362, 13], "temperature": 0.0, "avg_logprob": -0.19058617374353243, "compression_ratio": 1.5950704225352113, "no_speech_prob": 2.9761873520328663e-05}, {"id": 1102, "seek": 488626, "start": 4891.780000000001, "end": 4896.42, "text": " Fred Munro has been helpful in thousands of ways and is just incredibly generous.", "tokens": [10112, 17050, 340, 575, 668, 4961, 294, 5383, 295, 2098, 293, 307, 445, 6252, 14537, 13], "temperature": 0.0, "avg_logprob": -0.19058617374353243, "compression_ratio": 1.5950704225352113, "no_speech_prob": 2.9761873520328663e-05}, {"id": 1103, "seek": 488626, "start": 4896.42, "end": 4902.18, "text": " Jason, a lot of you will already be aware of, who helped a lot with the final lesson of", "tokens": [11181, 11, 257, 688, 295, 291, 486, 1217, 312, 3650, 295, 11, 567, 4254, 257, 688, 365, 264, 2572, 6898, 295], "temperature": 0.0, "avg_logprob": -0.19058617374353243, "compression_ratio": 1.5950704225352113, "no_speech_prob": 2.9761873520328663e-05}, {"id": 1104, "seek": 488626, "start": 4902.18, "end": 4908.9400000000005, "text": " the last course and is hard at work now on taking it even further to doing some stuff", "tokens": [264, 1036, 1164, 293, 307, 1152, 412, 589, 586, 322, 1940, 309, 754, 3052, 281, 884, 512, 1507], "temperature": 0.0, "avg_logprob": -0.19058617374353243, "compression_ratio": 1.5950704225352113, "no_speech_prob": 2.9761873520328663e-05}, {"id": 1105, "seek": 488626, "start": 4908.9400000000005, "end": 4909.9400000000005, "text": " that's going to blow you away.", "tokens": [300, 311, 516, 281, 6327, 291, 1314, 13], "temperature": 0.0, "avg_logprob": -0.19058617374353243, "compression_ratio": 1.5950704225352113, "no_speech_prob": 2.9761873520328663e-05}, {"id": 1106, "seek": 488626, "start": 4909.9400000000005, "end": 4915.46, "text": " I particularly want to point out RADIC because this is the list of the, I can't quite count,", "tokens": [286, 4098, 528, 281, 935, 484, 497, 6112, 2532, 570, 341, 307, 264, 1329, 295, 264, 11, 286, 393, 380, 1596, 1207, 11], "temperature": 0.0, "avg_logprob": -0.19058617374353243, "compression_ratio": 1.5950704225352113, "no_speech_prob": 2.9761873520328663e-05}, {"id": 1107, "seek": 491546, "start": 4915.46, "end": 4923.38, "text": " the 20 most helpful people on the forum as ranked by number of likes.", "tokens": [264, 945, 881, 4961, 561, 322, 264, 17542, 382, 20197, 538, 1230, 295, 5902, 13], "temperature": 0.0, "avg_logprob": -0.24960580625032125, "compression_ratio": 1.4714285714285715, "no_speech_prob": 1.9212620827602223e-05}, {"id": 1108, "seek": 491546, "start": 4923.38, "end": 4929.78, "text": " When somebody clicks that like button it means they're saying, you've helped me.", "tokens": [1133, 2618, 18521, 300, 411, 2960, 309, 1355, 436, 434, 1566, 11, 291, 600, 4254, 385, 13], "temperature": 0.0, "avg_logprob": -0.24960580625032125, "compression_ratio": 1.4714285714285715, "no_speech_prob": 1.9212620827602223e-05}, {"id": 1109, "seek": 491546, "start": 4929.78, "end": 4931.94, "text": " More people have said that about RADIC than anybody else.", "tokens": [5048, 561, 362, 848, 300, 466, 497, 6112, 2532, 813, 4472, 1646, 13], "temperature": 0.0, "avg_logprob": -0.24960580625032125, "compression_ratio": 1.4714285714285715, "no_speech_prob": 1.9212620827602223e-05}, {"id": 1110, "seek": 491546, "start": 4931.94, "end": 4938.1, "text": " It's not surprising because RADIC is not just an incredibly helpful person but extremely", "tokens": [467, 311, 406, 8830, 570, 497, 6112, 2532, 307, 406, 445, 364, 6252, 4961, 954, 457, 4664], "temperature": 0.0, "avg_logprob": -0.24960580625032125, "compression_ratio": 1.4714285714285715, "no_speech_prob": 1.9212620827602223e-05}, {"id": 1111, "seek": 491546, "start": 4938.1, "end": 4939.46, "text": " thoughtful.", "tokens": [21566, 13], "temperature": 0.0, "avg_logprob": -0.24960580625032125, "compression_ratio": 1.4714285714285715, "no_speech_prob": 1.9212620827602223e-05}, {"id": 1112, "seek": 493946, "start": 4939.46, "end": 4948.78, "text": " Gosh, when he started as a fast AI student he considered himself, if I remember correctly,", "tokens": [19185, 11, 562, 415, 1409, 382, 257, 2370, 7318, 3107, 415, 4888, 3647, 11, 498, 286, 1604, 8944, 11], "temperature": 0.0, "avg_logprob": -0.1780210659827715, "compression_ratio": 1.5091743119266054, "no_speech_prob": 1.592301305208821e-05}, {"id": 1113, "seek": 493946, "start": 4948.78, "end": 4950.82, "text": " basically a failed ML student.", "tokens": [1936, 257, 7612, 21601, 3107, 13], "temperature": 0.0, "avg_logprob": -0.1780210659827715, "compression_ratio": 1.5091743119266054, "no_speech_prob": 1.592301305208821e-05}, {"id": 1114, "seek": 493946, "start": 4950.82, "end": 4955.5, "text": " He had tried a number of times to learn ML and hadn't succeeded but he's just applied", "tokens": [634, 632, 3031, 257, 1230, 295, 1413, 281, 1466, 21601, 293, 8782, 380, 20263, 457, 415, 311, 445, 6456], "temperature": 0.0, "avg_logprob": -0.1780210659827715, "compression_ratio": 1.5091743119266054, "no_speech_prob": 1.592301305208821e-05}, {"id": 1115, "seek": 493946, "start": 4955.5, "end": 4962.66, "text": " himself so well for the last couple of years and he's now a Kaggle winner, a world recognized", "tokens": [3647, 370, 731, 337, 264, 1036, 1916, 295, 924, 293, 415, 311, 586, 257, 48751, 22631, 8507, 11, 257, 1002, 9823], "temperature": 0.0, "avg_logprob": -0.1780210659827715, "compression_ratio": 1.5091743119266054, "no_speech_prob": 1.592301305208821e-05}, {"id": 1116, "seek": 493946, "start": 4962.66, "end": 4965.14, "text": " deep learning practitioner.", "tokens": [2452, 2539, 32125, 13], "temperature": 0.0, "avg_logprob": -0.1780210659827715, "compression_ratio": 1.5091743119266054, "no_speech_prob": 1.592301305208821e-05}, {"id": 1117, "seek": 496514, "start": 4965.14, "end": 4971.34, "text": " Thank you to all of these people and everybody else who's contributed in so many ways.", "tokens": [1044, 291, 281, 439, 295, 613, 561, 293, 2201, 1646, 567, 311, 18434, 294, 370, 867, 2098, 13], "temperature": 0.0, "avg_logprob": -0.16779070887072334, "compression_ratio": 1.5707762557077625, "no_speech_prob": 9.367585334985051e-06}, {"id": 1118, "seek": 496514, "start": 4971.34, "end": 4978.22, "text": " And of course Rachel who's sitting right next to me.", "tokens": [400, 295, 1164, 14246, 567, 311, 3798, 558, 958, 281, 385, 13], "temperature": 0.0, "avg_logprob": -0.16779070887072334, "compression_ratio": 1.5707762557077625, "no_speech_prob": 9.367585334985051e-06}, {"id": 1119, "seek": 496514, "start": 4978.22, "end": 4985.3, "text": " This is the fit function that we just wrote or the one, the slightly more elegant one", "tokens": [639, 307, 264, 3318, 2445, 300, 321, 445, 4114, 420, 264, 472, 11, 264, 4748, 544, 21117, 472], "temperature": 0.0, "avg_logprob": -0.16779070887072334, "compression_ratio": 1.5707762557077625, "no_speech_prob": 9.367585334985051e-06}, {"id": 1120, "seek": 496514, "start": 4985.3, "end": 4988.42, "text": " before we added validation to it.", "tokens": [949, 321, 3869, 24071, 281, 309, 13], "temperature": 0.0, "avg_logprob": -0.16779070887072334, "compression_ratio": 1.5707762557077625, "no_speech_prob": 9.367585334985051e-06}, {"id": 1121, "seek": 496514, "start": 4988.42, "end": 4993.38, "text": " So go through each epoch, go through each set of mini batch, get the prediction, the", "tokens": [407, 352, 807, 1184, 30992, 339, 11, 352, 807, 1184, 992, 295, 8382, 15245, 11, 483, 264, 17630, 11, 264], "temperature": 0.0, "avg_logprob": -0.16779070887072334, "compression_ratio": 1.5707762557077625, "no_speech_prob": 9.367585334985051e-06}, {"id": 1122, "seek": 499338, "start": 4993.38, "end": 5001.02, "text": " loss, backward pass, update your parameters and then zero the gradients.", "tokens": [4470, 11, 23897, 1320, 11, 5623, 428, 9834, 293, 550, 4018, 264, 2771, 2448, 13], "temperature": 0.0, "avg_logprob": -0.1856945377506622, "compression_ratio": 1.4914285714285713, "no_speech_prob": 3.611845613704645e-06}, {"id": 1123, "seek": 499338, "start": 5001.02, "end": 5004.46, "text": " So that's basically what we're doing.", "tokens": [407, 300, 311, 1936, 437, 321, 434, 884, 13], "temperature": 0.0, "avg_logprob": -0.1856945377506622, "compression_ratio": 1.4914285714285713, "no_speech_prob": 3.611845613704645e-06}, {"id": 1124, "seek": 499338, "start": 5004.46, "end": 5009.54, "text": " Model, predictions, loss, gradients, step.", "tokens": [17105, 11, 21264, 11, 4470, 11, 2771, 2448, 11, 1823, 13], "temperature": 0.0, "avg_logprob": -0.1856945377506622, "compression_ratio": 1.4914285714285713, "no_speech_prob": 3.611845613704645e-06}, {"id": 1125, "seek": 499338, "start": 5009.54, "end": 5015.02, "text": " And each time we grab a bit more training data.", "tokens": [400, 1184, 565, 321, 4444, 257, 857, 544, 3097, 1412, 13], "temperature": 0.0, "avg_logprob": -0.1856945377506622, "compression_ratio": 1.4914285714285713, "no_speech_prob": 3.611845613704645e-06}, {"id": 1126, "seek": 499338, "start": 5015.02, "end": 5020.06, "text": " But that's not really all we want to do in a training loop.", "tokens": [583, 300, 311, 406, 534, 439, 321, 528, 281, 360, 294, 257, 3097, 6367, 13], "temperature": 0.0, "avg_logprob": -0.1856945377506622, "compression_ratio": 1.4914285714285713, "no_speech_prob": 3.611845613704645e-06}, {"id": 1127, "seek": 502006, "start": 5020.06, "end": 5027.14, "text": " We might want to add the beautiful fast progress bars and animations that Sylvain created in", "tokens": [492, 1062, 528, 281, 909, 264, 2238, 2370, 4205, 10228, 293, 22868, 300, 3902, 14574, 491, 2942, 294], "temperature": 0.0, "avg_logprob": -0.17770282258378697, "compression_ratio": 1.6056910569105691, "no_speech_prob": 1.112427798943827e-05}, {"id": 1128, "seek": 502006, "start": 5027.14, "end": 5031.42, "text": " his fast progress library or TensorBoard or whatever.", "tokens": [702, 2370, 4205, 6405, 420, 34306, 22493, 515, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.17770282258378697, "compression_ratio": 1.6056910569105691, "no_speech_prob": 1.112427798943827e-05}, {"id": 1129, "seek": 502006, "start": 5031.42, "end": 5035.52, "text": " And thanks to Jason actually we now have TensorBoard integration in Fast AI.", "tokens": [400, 3231, 281, 11181, 767, 321, 586, 362, 34306, 22493, 515, 10980, 294, 15968, 7318, 13], "temperature": 0.0, "avg_logprob": -0.17770282258378697, "compression_ratio": 1.6056910569105691, "no_speech_prob": 1.112427798943827e-05}, {"id": 1130, "seek": 502006, "start": 5035.52, "end": 5041.580000000001, "text": " So be sure to check that out if you want extra pretty graphs like these.", "tokens": [407, 312, 988, 281, 1520, 300, 484, 498, 291, 528, 2857, 1238, 24877, 411, 613, 13], "temperature": 0.0, "avg_logprob": -0.17770282258378697, "compression_ratio": 1.6056910569105691, "no_speech_prob": 1.112427798943827e-05}, {"id": 1131, "seek": 502006, "start": 5041.580000000001, "end": 5043.4800000000005, "text": " Hyperparameter scheduling.", "tokens": [29592, 2181, 335, 2398, 29055, 13], "temperature": 0.0, "avg_logprob": -0.17770282258378697, "compression_ratio": 1.6056910569105691, "no_speech_prob": 1.112427798943827e-05}, {"id": 1132, "seek": 502006, "start": 5043.4800000000005, "end": 5046.54, "text": " You might want to add all kinds of different regularization techniques.", "tokens": [509, 1062, 528, 281, 909, 439, 3685, 295, 819, 3890, 2144, 7512, 13], "temperature": 0.0, "avg_logprob": -0.17770282258378697, "compression_ratio": 1.6056910569105691, "no_speech_prob": 1.112427798943827e-05}, {"id": 1133, "seek": 504654, "start": 5046.54, "end": 5053.26, "text": " These are all examples of regularization techniques that we support in Fast AI and many more.", "tokens": [1981, 366, 439, 5110, 295, 3890, 2144, 7512, 300, 321, 1406, 294, 15968, 7318, 293, 867, 544, 13], "temperature": 0.0, "avg_logprob": -0.14235433248373178, "compression_ratio": 1.5378787878787878, "no_speech_prob": 5.6821509133442305e-06}, {"id": 1134, "seek": 504654, "start": 5053.26, "end": 5054.26, "text": " Mixed precision training.", "tokens": [12769, 292, 18356, 3097, 13], "temperature": 0.0, "avg_logprob": -0.14235433248373178, "compression_ratio": 1.5378787878787878, "no_speech_prob": 5.6821509133442305e-06}, {"id": 1135, "seek": 504654, "start": 5054.26, "end": 5062.0199999999995, "text": " You know, so take advantage of the tensor cores in a Volta GPU to train much faster.", "tokens": [509, 458, 11, 370, 747, 5002, 295, 264, 40863, 24826, 294, 257, 8911, 1328, 18407, 281, 3847, 709, 4663, 13], "temperature": 0.0, "avg_logprob": -0.14235433248373178, "compression_ratio": 1.5378787878787878, "no_speech_prob": 5.6821509133442305e-06}, {"id": 1136, "seek": 504654, "start": 5062.0199999999995, "end": 5066.26, "text": " There's more tweaks you might want to do to the training loop than we could possibly think", "tokens": [821, 311, 544, 46664, 291, 1062, 528, 281, 360, 281, 264, 3097, 6367, 813, 321, 727, 6264, 519], "temperature": 0.0, "avg_logprob": -0.14235433248373178, "compression_ratio": 1.5378787878787878, "no_speech_prob": 5.6821509133442305e-06}, {"id": 1137, "seek": 504654, "start": 5066.26, "end": 5067.26, "text": " of.", "tokens": [295, 13], "temperature": 0.0, "avg_logprob": -0.14235433248373178, "compression_ratio": 1.5378787878787878, "no_speech_prob": 5.6821509133442305e-06}, {"id": 1138, "seek": 504654, "start": 5067.26, "end": 5069.42, "text": " And even if we did think of all of the ones that exist now, somebody will come up with", "tokens": [400, 754, 498, 321, 630, 519, 295, 439, 295, 264, 2306, 300, 2514, 586, 11, 2618, 486, 808, 493, 365], "temperature": 0.0, "avg_logprob": -0.14235433248373178, "compression_ratio": 1.5378787878787878, "no_speech_prob": 5.6821509133442305e-06}, {"id": 1139, "seek": 504654, "start": 5069.42, "end": 5071.42, "text": " a new one tomorrow.", "tokens": [257, 777, 472, 4153, 13], "temperature": 0.0, "avg_logprob": -0.14235433248373178, "compression_ratio": 1.5378787878787878, "no_speech_prob": 5.6821509133442305e-06}, {"id": 1140, "seek": 507142, "start": 5071.42, "end": 5077.54, "text": " So you've got some possible ways you could solve this problem.", "tokens": [407, 291, 600, 658, 512, 1944, 2098, 291, 727, 5039, 341, 1154, 13], "temperature": 0.0, "avg_logprob": -0.18465404510498046, "compression_ratio": 1.6422764227642277, "no_speech_prob": 3.3930505196622107e-06}, {"id": 1141, "seek": 507142, "start": 5077.54, "end": 5082.26, "text": " Some of the things we're talking about are even things like how do you add GANs, more", "tokens": [2188, 295, 264, 721, 321, 434, 1417, 466, 366, 754, 721, 411, 577, 360, 291, 909, 460, 1770, 82, 11, 544], "temperature": 0.0, "avg_logprob": -0.18465404510498046, "compression_ratio": 1.6422764227642277, "no_speech_prob": 3.3930505196622107e-06}, {"id": 1142, "seek": 507142, "start": 5082.26, "end": 5083.38, "text": " complex stuff.", "tokens": [3997, 1507, 13], "temperature": 0.0, "avg_logprob": -0.18465404510498046, "compression_ratio": 1.6422764227642277, "no_speech_prob": 3.3930505196622107e-06}, {"id": 1143, "seek": 507142, "start": 5083.38, "end": 5090.58, "text": " So one approach is write a training loop for every possible way you want to train.", "tokens": [407, 472, 3109, 307, 2464, 257, 3097, 6367, 337, 633, 1944, 636, 291, 528, 281, 3847, 13], "temperature": 0.0, "avg_logprob": -0.18465404510498046, "compression_ratio": 1.6422764227642277, "no_speech_prob": 3.3930505196622107e-06}, {"id": 1144, "seek": 507142, "start": 5090.58, "end": 5096.32, "text": " And this is particularly problematic when you start to like want to combine multiple", "tokens": [400, 341, 307, 4098, 19011, 562, 291, 722, 281, 411, 528, 281, 10432, 3866], "temperature": 0.0, "avg_logprob": -0.18465404510498046, "compression_ratio": 1.6422764227642277, "no_speech_prob": 3.3930505196622107e-06}, {"id": 1145, "seek": 507142, "start": 5096.32, "end": 5097.7, "text": " different tweaks, right?", "tokens": [819, 46664, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.18465404510498046, "compression_ratio": 1.6422764227642277, "no_speech_prob": 3.3930505196622107e-06}, {"id": 1146, "seek": 507142, "start": 5097.7, "end": 5100.2, "text": " As you're like cutting and pasting or whatever.", "tokens": [1018, 291, 434, 411, 6492, 293, 1791, 278, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.18465404510498046, "compression_ratio": 1.6422764227642277, "no_speech_prob": 3.3930505196622107e-06}, {"id": 1147, "seek": 510020, "start": 5100.2, "end": 5105.62, "text": " So that's certainly not going to work for Fast AI.", "tokens": [407, 300, 311, 3297, 406, 516, 281, 589, 337, 15968, 7318, 13], "temperature": 0.0, "avg_logprob": -0.1455663217080606, "compression_ratio": 1.5442477876106195, "no_speech_prob": 3.3929616165551124e-06}, {"id": 1148, "seek": 510020, "start": 5105.62, "end": 5108.34, "text": " There's what I tried for Fast AI 0.7.", "tokens": [821, 311, 437, 286, 3031, 337, 15968, 7318, 1958, 13, 22, 13], "temperature": 0.0, "avg_logprob": -0.1455663217080606, "compression_ratio": 1.5442477876106195, "no_speech_prob": 3.3929616165551124e-06}, {"id": 1149, "seek": 510020, "start": 5108.34, "end": 5110.099999999999, "text": " This is my training loop.", "tokens": [639, 307, 452, 3097, 6367, 13], "temperature": 0.0, "avg_logprob": -0.1455663217080606, "compression_ratio": 1.5442477876106195, "no_speech_prob": 3.3929616165551124e-06}, {"id": 1150, "seek": 510020, "start": 5110.099999999999, "end": 5111.98, "text": " The last time I tried this.", "tokens": [440, 1036, 565, 286, 3031, 341, 13], "temperature": 0.0, "avg_logprob": -0.1455663217080606, "compression_ratio": 1.5442477876106195, "no_speech_prob": 3.3929616165551124e-06}, {"id": 1151, "seek": 510020, "start": 5111.98, "end": 5116.5, "text": " Which is like throw in every damn thing.", "tokens": [3013, 307, 411, 3507, 294, 633, 8151, 551, 13], "temperature": 0.0, "avg_logprob": -0.1455663217080606, "compression_ratio": 1.5442477876106195, "no_speech_prob": 3.3929616165551124e-06}, {"id": 1152, "seek": 510020, "start": 5116.5, "end": 5122.12, "text": " And it just got, you know, so every time somebody would say like, oh, a new paper's come out.", "tokens": [400, 309, 445, 658, 11, 291, 458, 11, 370, 633, 565, 2618, 576, 584, 411, 11, 1954, 11, 257, 777, 3035, 311, 808, 484, 13], "temperature": 0.0, "avg_logprob": -0.1455663217080606, "compression_ratio": 1.5442477876106195, "no_speech_prob": 3.3929616165551124e-06}, {"id": 1153, "seek": 510020, "start": 5122.12, "end": 5123.3, "text": " Can you please implement?", "tokens": [1664, 291, 1767, 4445, 30], "temperature": 0.0, "avg_logprob": -0.1455663217080606, "compression_ratio": 1.5442477876106195, "no_speech_prob": 3.3929616165551124e-06}, {"id": 1154, "seek": 510020, "start": 5123.3, "end": 5126.179999999999, "text": " And I'd just be like, no.", "tokens": [400, 286, 1116, 445, 312, 411, 11, 572, 13], "temperature": 0.0, "avg_logprob": -0.1455663217080606, "compression_ratio": 1.5442477876106195, "no_speech_prob": 3.3929616165551124e-06}, {"id": 1155, "seek": 510020, "start": 5126.179999999999, "end": 5129.94, "text": " I couldn't bear it.", "tokens": [286, 2809, 380, 6155, 309, 13], "temperature": 0.0, "avg_logprob": -0.1455663217080606, "compression_ratio": 1.5442477876106195, "no_speech_prob": 3.3929616165551124e-06}, {"id": 1156, "seek": 512994, "start": 5129.94, "end": 5133.379999999999, "text": " So now we have something better.", "tokens": [407, 586, 321, 362, 746, 1101, 13], "temperature": 0.0, "avg_logprob": -0.19624707890653062, "compression_ratio": 1.694300518134715, "no_speech_prob": 3.1875379136181436e-06}, {"id": 1157, "seek": 512994, "start": 5133.379999999999, "end": 5134.74, "text": " Callbacks.", "tokens": [7807, 17758, 13], "temperature": 0.0, "avg_logprob": -0.19624707890653062, "compression_ratio": 1.694300518134715, "no_speech_prob": 3.1875379136181436e-06}, {"id": 1158, "seek": 512994, "start": 5134.74, "end": 5138.62, "text": " And callbacks are something which like every library has callbacks, but nobody else have", "tokens": [400, 818, 17758, 366, 746, 597, 411, 633, 6405, 575, 818, 17758, 11, 457, 5079, 1646, 362], "temperature": 0.0, "avg_logprob": -0.19624707890653062, "compression_ratio": 1.694300518134715, "no_speech_prob": 3.1875379136181436e-06}, {"id": 1159, "seek": 512994, "start": 5138.62, "end": 5140.94, "text": " callbacks, anything like our callbacks.", "tokens": [818, 17758, 11, 1340, 411, 527, 818, 17758, 13], "temperature": 0.0, "avg_logprob": -0.19624707890653062, "compression_ratio": 1.694300518134715, "no_speech_prob": 3.1875379136181436e-06}, {"id": 1160, "seek": 512994, "start": 5140.94, "end": 5142.219999999999, "text": " And you'll see what I mean.", "tokens": [400, 291, 603, 536, 437, 286, 914, 13], "temperature": 0.0, "avg_logprob": -0.19624707890653062, "compression_ratio": 1.694300518134715, "no_speech_prob": 3.1875379136181436e-06}, {"id": 1161, "seek": 512994, "start": 5142.219999999999, "end": 5152.0599999999995, "text": " Our callbacks let you not only look at but fully customize every one of these steps.", "tokens": [2621, 818, 17758, 718, 291, 406, 787, 574, 412, 457, 4498, 19734, 633, 472, 295, 613, 4439, 13], "temperature": 0.0, "avg_logprob": -0.19624707890653062, "compression_ratio": 1.694300518134715, "no_speech_prob": 3.1875379136181436e-06}, {"id": 1162, "seek": 512994, "start": 5152.0599999999995, "end": 5157.74, "text": " And so here's our starting training loop.", "tokens": [400, 370, 510, 311, 527, 2891, 3097, 6367, 13], "temperature": 0.0, "avg_logprob": -0.19624707890653062, "compression_ratio": 1.694300518134715, "no_speech_prob": 3.1875379136181436e-06}, {"id": 1163, "seek": 515774, "start": 5157.74, "end": 5162.82, "text": " Is the Fast AI version one training loop.", "tokens": [1119, 264, 15968, 7318, 3037, 472, 3097, 6367, 13], "temperature": 0.0, "avg_logprob": -0.18223267335158128, "compression_ratio": 1.721311475409836, "no_speech_prob": 6.339104857033817e-06}, {"id": 1164, "seek": 515774, "start": 5162.82, "end": 5164.98, "text": " It's the same, right?", "tokens": [467, 311, 264, 912, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.18223267335158128, "compression_ratio": 1.721311475409836, "no_speech_prob": 6.339104857033817e-06}, {"id": 1165, "seek": 515774, "start": 5164.98, "end": 5168.86, "text": " There's the exact same lines of code.", "tokens": [821, 311, 264, 1900, 912, 3876, 295, 3089, 13], "temperature": 0.0, "avg_logprob": -0.18223267335158128, "compression_ratio": 1.721311475409836, "no_speech_prob": 6.339104857033817e-06}, {"id": 1166, "seek": 515774, "start": 5168.86, "end": 5174.26, "text": " Plus a bunch of calls to callbacks.", "tokens": [7721, 257, 3840, 295, 5498, 281, 818, 17758, 13], "temperature": 0.0, "avg_logprob": -0.18223267335158128, "compression_ratio": 1.721311475409836, "no_speech_prob": 6.339104857033817e-06}, {"id": 1167, "seek": 515774, "start": 5174.26, "end": 5180.7, "text": " And so each one basically says, you know, before I do a step, on step begin.", "tokens": [400, 370, 1184, 472, 1936, 1619, 11, 291, 458, 11, 949, 286, 360, 257, 1823, 11, 322, 1823, 1841, 13], "temperature": 0.0, "avg_logprob": -0.18223267335158128, "compression_ratio": 1.721311475409836, "no_speech_prob": 6.339104857033817e-06}, {"id": 1168, "seek": 515774, "start": 5180.7, "end": 5183.0599999999995, "text": " After I do a step, on step end.", "tokens": [2381, 286, 360, 257, 1823, 11, 322, 1823, 917, 13], "temperature": 0.0, "avg_logprob": -0.18223267335158128, "compression_ratio": 1.721311475409836, "no_speech_prob": 6.339104857033817e-06}, {"id": 1169, "seek": 515774, "start": 5183.0599999999995, "end": 5184.78, "text": " After I do a batch, on batch end.", "tokens": [2381, 286, 360, 257, 15245, 11, 322, 15245, 917, 13], "temperature": 0.0, "avg_logprob": -0.18223267335158128, "compression_ratio": 1.721311475409836, "no_speech_prob": 6.339104857033817e-06}, {"id": 1170, "seek": 515774, "start": 5184.78, "end": 5186.34, "text": " After I do an epoch, on epoch end.", "tokens": [2381, 286, 360, 364, 30992, 339, 11, 322, 30992, 339, 917, 13], "temperature": 0.0, "avg_logprob": -0.18223267335158128, "compression_ratio": 1.721311475409836, "no_speech_prob": 6.339104857033817e-06}, {"id": 1171, "seek": 518634, "start": 5186.34, "end": 5188.42, "text": " After I finish training, on training end.", "tokens": [2381, 286, 2413, 3097, 11, 322, 3097, 917, 13], "temperature": 0.0, "avg_logprob": -0.13679961057809684, "compression_ratio": 1.7131147540983607, "no_speech_prob": 1.2098429351681261e-06}, {"id": 1172, "seek": 518634, "start": 5188.42, "end": 5189.42, "text": " Right?", "tokens": [1779, 30], "temperature": 0.0, "avg_logprob": -0.13679961057809684, "compression_ratio": 1.7131147540983607, "no_speech_prob": 1.2098429351681261e-06}, {"id": 1173, "seek": 518634, "start": 5189.42, "end": 5194.82, "text": " And they have the ability to also change things or even they have the ability to say, please", "tokens": [400, 436, 362, 264, 3485, 281, 611, 1319, 721, 420, 754, 436, 362, 264, 3485, 281, 584, 11, 1767], "temperature": 0.0, "avg_logprob": -0.13679961057809684, "compression_ratio": 1.7131147540983607, "no_speech_prob": 1.2098429351681261e-06}, {"id": 1174, "seek": 518634, "start": 5194.82, "end": 5199.26, "text": " skip the next step by returning a Boolean.", "tokens": [10023, 264, 958, 1823, 538, 12678, 257, 23351, 28499, 13], "temperature": 0.0, "avg_logprob": -0.13679961057809684, "compression_ratio": 1.7131147540983607, "no_speech_prob": 1.2098429351681261e-06}, {"id": 1175, "seek": 518634, "start": 5199.26, "end": 5205.0, "text": " So with this, we can create and have created all kinds of things in Fast AI, like learning", "tokens": [407, 365, 341, 11, 321, 393, 1884, 293, 362, 2942, 439, 3685, 295, 721, 294, 15968, 7318, 11, 411, 2539], "temperature": 0.0, "avg_logprob": -0.13679961057809684, "compression_ratio": 1.7131147540983607, "no_speech_prob": 1.2098429351681261e-06}, {"id": 1176, "seek": 518634, "start": 5205.0, "end": 5210.5, "text": " rate schedulers and early stopping and parallel trainer.", "tokens": [3314, 12000, 433, 293, 2440, 12767, 293, 8952, 21110, 13], "temperature": 0.0, "avg_logprob": -0.13679961057809684, "compression_ratio": 1.7131147540983607, "no_speech_prob": 1.2098429351681261e-06}, {"id": 1177, "seek": 518634, "start": 5210.5, "end": 5215.66, "text": " This is literally when I wrote parallel trainer, this is the entire callback I wrote.", "tokens": [639, 307, 3736, 562, 286, 4114, 8952, 21110, 11, 341, 307, 264, 2302, 818, 3207, 286, 4114, 13], "temperature": 0.0, "avg_logprob": -0.13679961057809684, "compression_ratio": 1.7131147540983607, "no_speech_prob": 1.2098429351681261e-06}, {"id": 1178, "seek": 521566, "start": 5215.66, "end": 5218.7, "text": " This is the entire gradient clipping callback.", "tokens": [639, 307, 264, 2302, 16235, 49320, 818, 3207, 13], "temperature": 0.0, "avg_logprob": -0.11022292735964753, "compression_ratio": 1.7032967032967032, "no_speech_prob": 5.5942664403119124e-06}, {"id": 1179, "seek": 521566, "start": 5218.7, "end": 5223.42, "text": " After you do the backward pass, clip the gradients.", "tokens": [2381, 291, 360, 264, 23897, 1320, 11, 7353, 264, 2771, 2448, 13], "temperature": 0.0, "avg_logprob": -0.11022292735964753, "compression_ratio": 1.7032967032967032, "no_speech_prob": 5.5942664403119124e-06}, {"id": 1180, "seek": 521566, "start": 5223.42, "end": 5229.0599999999995, "text": " So you can do a lot with a little.", "tokens": [407, 291, 393, 360, 257, 688, 365, 257, 707, 13], "temperature": 0.0, "avg_logprob": -0.11022292735964753, "compression_ratio": 1.7032967032967032, "no_speech_prob": 5.5942664403119124e-06}, {"id": 1181, "seek": 521566, "start": 5229.0599999999995, "end": 5232.9, "text": " And then you can mix them all together.", "tokens": [400, 550, 291, 393, 2890, 552, 439, 1214, 13], "temperature": 0.0, "avg_logprob": -0.11022292735964753, "compression_ratio": 1.7032967032967032, "no_speech_prob": 5.5942664403119124e-06}, {"id": 1182, "seek": 521566, "start": 5232.9, "end": 5236.94, "text": " Because all of the callbacks work with all of the other callbacks.", "tokens": [1436, 439, 295, 264, 818, 17758, 589, 365, 439, 295, 264, 661, 818, 17758, 13], "temperature": 0.0, "avg_logprob": -0.11022292735964753, "compression_ratio": 1.7032967032967032, "no_speech_prob": 5.5942664403119124e-06}, {"id": 1183, "seek": 521566, "start": 5236.94, "end": 5243.26, "text": " So these are some of the callbacks that we have in Fast AI right now.", "tokens": [407, 613, 366, 512, 295, 264, 818, 17758, 300, 321, 362, 294, 15968, 7318, 558, 586, 13], "temperature": 0.0, "avg_logprob": -0.11022292735964753, "compression_ratio": 1.7032967032967032, "no_speech_prob": 5.5942664403119124e-06}, {"id": 1184, "seek": 524326, "start": 5243.26, "end": 5250.1, "text": " So for example, how did we do GANs last course?", "tokens": [407, 337, 1365, 11, 577, 630, 321, 360, 460, 1770, 82, 1036, 1164, 30], "temperature": 0.0, "avg_logprob": -0.17030054658323854, "compression_ratio": 1.5829145728643217, "no_speech_prob": 5.862639227416366e-06}, {"id": 1185, "seek": 524326, "start": 5250.1, "end": 5254.9800000000005, "text": " So what we did behind the scenes was we created a GAN module.", "tokens": [407, 437, 321, 630, 2261, 264, 8026, 390, 321, 2942, 257, 460, 1770, 10088, 13], "temperature": 0.0, "avg_logprob": -0.17030054658323854, "compression_ratio": 1.5829145728643217, "no_speech_prob": 5.862639227416366e-06}, {"id": 1186, "seek": 524326, "start": 5254.9800000000005, "end": 5257.06, "text": " It was ridiculously simple.", "tokens": [467, 390, 41358, 2199, 13], "temperature": 0.0, "avg_logprob": -0.17030054658323854, "compression_ratio": 1.5829145728643217, "no_speech_prob": 5.862639227416366e-06}, {"id": 1187, "seek": 524326, "start": 5257.06, "end": 5262.5, "text": " We created a GAN module that had a forward method that just said, what's your generator", "tokens": [492, 2942, 257, 460, 1770, 10088, 300, 632, 257, 2128, 3170, 300, 445, 848, 11, 437, 311, 428, 19265], "temperature": 0.0, "avg_logprob": -0.17030054658323854, "compression_ratio": 1.5829145728643217, "no_speech_prob": 5.862639227416366e-06}, {"id": 1188, "seek": 524326, "start": 5262.5, "end": 5263.5, "text": " mode?", "tokens": [4391, 30], "temperature": 0.0, "avg_logprob": -0.17030054658323854, "compression_ratio": 1.5829145728643217, "no_speech_prob": 5.862639227416366e-06}, {"id": 1189, "seek": 524326, "start": 5263.5, "end": 5268.42, "text": " Is it, sorry, are you in generator mode or not?", "tokens": [1119, 309, 11, 2597, 11, 366, 291, 294, 19265, 4391, 420, 406, 30], "temperature": 0.0, "avg_logprob": -0.17030054658323854, "compression_ratio": 1.5829145728643217, "no_speech_prob": 5.862639227416366e-06}, {"id": 1190, "seek": 524326, "start": 5268.42, "end": 5269.780000000001, "text": " Where not means discriminator mode.", "tokens": [2305, 406, 1355, 20828, 1639, 4391, 13], "temperature": 0.0, "avg_logprob": -0.17030054658323854, "compression_ratio": 1.5829145728643217, "no_speech_prob": 5.862639227416366e-06}, {"id": 1191, "seek": 526978, "start": 5269.78, "end": 5274.32, "text": " If you're in generator mode, call the generator, otherwise call the critic.", "tokens": [759, 291, 434, 294, 19265, 4391, 11, 818, 264, 19265, 11, 5911, 818, 264, 7850, 13], "temperature": 0.0, "avg_logprob": -0.10672375361124674, "compression_ratio": 1.8883495145631068, "no_speech_prob": 5.255342330201529e-06}, {"id": 1192, "seek": 526978, "start": 5274.32, "end": 5278.219999999999, "text": " And then there's a function called switch that just changed generator mode, backwards", "tokens": [400, 550, 456, 311, 257, 2445, 1219, 3679, 300, 445, 3105, 19265, 4391, 11, 12204], "temperature": 0.0, "avg_logprob": -0.10672375361124674, "compression_ratio": 1.8883495145631068, "no_speech_prob": 5.255342330201529e-06}, {"id": 1193, "seek": 526978, "start": 5278.219999999999, "end": 5282.3, "text": " and forwards, between generator and discriminator.", "tokens": [293, 30126, 11, 1296, 19265, 293, 20828, 1639, 13], "temperature": 0.0, "avg_logprob": -0.10672375361124674, "compression_ratio": 1.8883495145631068, "no_speech_prob": 5.255342330201529e-06}, {"id": 1194, "seek": 526978, "start": 5282.3, "end": 5290.86, "text": " Same thing if we created a loss function where there was a generator loss and a critic loss.", "tokens": [10635, 551, 498, 321, 2942, 257, 4470, 2445, 689, 456, 390, 257, 19265, 4470, 293, 257, 7850, 4470, 13], "temperature": 0.0, "avg_logprob": -0.10672375361124674, "compression_ratio": 1.8883495145631068, "no_speech_prob": 5.255342330201529e-06}, {"id": 1195, "seek": 526978, "start": 5290.86, "end": 5295.82, "text": " And so then we created a callback, right, which had a switch that just switched the", "tokens": [400, 370, 550, 321, 2942, 257, 818, 3207, 11, 558, 11, 597, 632, 257, 3679, 300, 445, 16858, 264], "temperature": 0.0, "avg_logprob": -0.10672375361124674, "compression_ratio": 1.8883495145631068, "no_speech_prob": 5.255342330201529e-06}, {"id": 1196, "seek": 529582, "start": 5295.82, "end": 5302.299999999999, "text": " generator mode on and off and passed that along to the model I just showed you and the", "tokens": [19265, 4391, 322, 293, 766, 293, 4678, 300, 2051, 281, 264, 2316, 286, 445, 4712, 291, 293, 264], "temperature": 0.0, "avg_logprob": -0.0951209685381721, "compression_ratio": 1.7619047619047619, "no_speech_prob": 8.013302249310073e-06}, {"id": 1197, "seek": 529582, "start": 5302.299999999999, "end": 5306.54, "text": " loss function I just showed you.", "tokens": [4470, 2445, 286, 445, 4712, 291, 13], "temperature": 0.0, "avg_logprob": -0.0951209685381721, "compression_ratio": 1.7619047619047619, "no_speech_prob": 8.013302249310073e-06}, {"id": 1198, "seek": 529582, "start": 5306.54, "end": 5314.58, "text": " And then it would set requires grad to the generator or discriminator as appropriate.", "tokens": [400, 550, 309, 576, 992, 7029, 2771, 281, 264, 19265, 420, 20828, 1639, 382, 6854, 13], "temperature": 0.0, "avg_logprob": -0.0951209685381721, "compression_ratio": 1.7619047619047619, "no_speech_prob": 8.013302249310073e-06}, {"id": 1199, "seek": 529582, "start": 5314.58, "end": 5319.299999999999, "text": " And then would have on train begin, on train end, on batch blah, blah, blah, callbacks", "tokens": [400, 550, 576, 362, 322, 3847, 1841, 11, 322, 3847, 917, 11, 322, 15245, 12288, 11, 12288, 11, 12288, 11, 818, 17758], "temperature": 0.0, "avg_logprob": -0.0951209685381721, "compression_ratio": 1.7619047619047619, "no_speech_prob": 8.013302249310073e-06}, {"id": 1200, "seek": 529582, "start": 5319.299999999999, "end": 5320.74, "text": " to do the right thing at the right time.", "tokens": [281, 360, 264, 558, 551, 412, 264, 558, 565, 13], "temperature": 0.0, "avg_logprob": -0.0951209685381721, "compression_ratio": 1.7619047619047619, "no_speech_prob": 8.013302249310073e-06}, {"id": 1201, "seek": 532074, "start": 5320.74, "end": 5327.0199999999995, "text": " So most importantly, at the start of an epoch, set your generator mode.", "tokens": [407, 881, 8906, 11, 412, 264, 722, 295, 364, 30992, 339, 11, 992, 428, 19265, 4391, 13], "temperature": 0.0, "avg_logprob": -0.14095015525817872, "compression_ratio": 1.6555555555555554, "no_speech_prob": 2.57593711694426e-07}, {"id": 1202, "seek": 532074, "start": 5327.0199999999995, "end": 5329.94, "text": " And at the end of training, set your generator mode.", "tokens": [400, 412, 264, 917, 295, 3097, 11, 992, 428, 19265, 4391, 13], "temperature": 0.0, "avg_logprob": -0.14095015525817872, "compression_ratio": 1.6555555555555554, "no_speech_prob": 2.57593711694426e-07}, {"id": 1203, "seek": 532074, "start": 5329.94, "end": 5337.62, "text": " So if you look at other libraries' implementation of GANs, they're basically a whole new training", "tokens": [407, 498, 291, 574, 412, 661, 15148, 6, 11420, 295, 460, 1770, 82, 11, 436, 434, 1936, 257, 1379, 777, 3097], "temperature": 0.0, "avg_logprob": -0.14095015525817872, "compression_ratio": 1.6555555555555554, "no_speech_prob": 2.57593711694426e-07}, {"id": 1204, "seek": 532074, "start": 5337.62, "end": 5341.7, "text": " loop, whole new data loaders, whole new everything.", "tokens": [6367, 11, 1379, 777, 1412, 3677, 433, 11, 1379, 777, 1203, 13], "temperature": 0.0, "avg_logprob": -0.14095015525817872, "compression_ratio": 1.6555555555555554, "no_speech_prob": 2.57593711694426e-07}, {"id": 1205, "seek": 532074, "start": 5341.7, "end": 5342.7, "text": " And it was really cool.", "tokens": [400, 309, 390, 534, 1627, 13], "temperature": 0.0, "avg_logprob": -0.14095015525817872, "compression_ratio": 1.6555555555555554, "no_speech_prob": 2.57593711694426e-07}, {"id": 1206, "seek": 534270, "start": 5342.7, "end": 5350.58, "text": " With REST AI, we were able to create a GAN in this incredibly small amount of code for", "tokens": [2022, 497, 14497, 7318, 11, 321, 645, 1075, 281, 1884, 257, 460, 1770, 294, 341, 6252, 1359, 2372, 295, 3089, 337], "temperature": 0.0, "avg_logprob": -0.1784020389419004, "compression_ratio": 1.4134615384615385, "no_speech_prob": 5.50748791283695e-06}, {"id": 1207, "seek": 534270, "start": 5350.58, "end": 5353.78, "text": " such a complex task.", "tokens": [1270, 257, 3997, 5633, 13], "temperature": 0.0, "avg_logprob": -0.1784020389419004, "compression_ratio": 1.4134615384615385, "no_speech_prob": 5.50748791283695e-06}, {"id": 1208, "seek": 534270, "start": 5353.78, "end": 5357.78, "text": " So let's do that ourselves, right, because we've got a training loop.", "tokens": [407, 718, 311, 360, 300, 4175, 11, 558, 11, 570, 321, 600, 658, 257, 3097, 6367, 13], "temperature": 0.0, "avg_logprob": -0.1784020389419004, "compression_ratio": 1.4134615384615385, "no_speech_prob": 5.50748791283695e-06}, {"id": 1209, "seek": 534270, "start": 5357.78, "end": 5363.5199999999995, "text": " If we add callbacks, we should now then be able to do everything.", "tokens": [759, 321, 909, 818, 17758, 11, 321, 820, 586, 550, 312, 1075, 281, 360, 1203, 13], "temperature": 0.0, "avg_logprob": -0.1784020389419004, "compression_ratio": 1.4134615384615385, "no_speech_prob": 5.50748791283695e-06}, {"id": 1210, "seek": 534270, "start": 5363.5199999999995, "end": 5369.82, "text": " So let's start out by grabbing our data as before.", "tokens": [407, 718, 311, 722, 484, 538, 23771, 527, 1412, 382, 949, 13], "temperature": 0.0, "avg_logprob": -0.1784020389419004, "compression_ratio": 1.4134615384615385, "no_speech_prob": 5.50748791283695e-06}, {"id": 1211, "seek": 536982, "start": 5369.82, "end": 5377.66, "text": " So we've got the number of hidden is 50, batch size 64, loss function is cross entropy.", "tokens": [407, 321, 600, 658, 264, 1230, 295, 7633, 307, 2625, 11, 15245, 2744, 12145, 11, 4470, 2445, 307, 3278, 30867, 13], "temperature": 0.0, "avg_logprob": -0.10459928605162981, "compression_ratio": 1.6378600823045268, "no_speech_prob": 1.72299533005571e-05}, {"id": 1212, "seek": 536982, "start": 5377.66, "end": 5381.259999999999, "text": " This is the signature of our fit function before.", "tokens": [639, 307, 264, 13397, 295, 527, 3318, 2445, 949, 13], "temperature": 0.0, "avg_logprob": -0.10459928605162981, "compression_ratio": 1.6378600823045268, "no_speech_prob": 1.72299533005571e-05}, {"id": 1213, "seek": 536982, "start": 5381.259999999999, "end": 5388.599999999999, "text": " And I get very nervous when I see functions with lots of things being passed to it.", "tokens": [400, 286, 483, 588, 6296, 562, 286, 536, 6828, 365, 3195, 295, 721, 885, 4678, 281, 309, 13], "temperature": 0.0, "avg_logprob": -0.10459928605162981, "compression_ratio": 1.6378600823045268, "no_speech_prob": 1.72299533005571e-05}, {"id": 1214, "seek": 536982, "start": 5388.599999999999, "end": 5391.219999999999, "text": " And it makes me think, do we really need to pass all those things to it, or can some of", "tokens": [400, 309, 1669, 385, 519, 11, 360, 321, 534, 643, 281, 1320, 439, 729, 721, 281, 309, 11, 420, 393, 512, 295], "temperature": 0.0, "avg_logprob": -0.10459928605162981, "compression_ratio": 1.6378600823045268, "no_speech_prob": 1.72299533005571e-05}, {"id": 1215, "seek": 536982, "start": 5391.219999999999, "end": 5392.46, "text": " them be packaged up together?", "tokens": [552, 312, 38162, 493, 1214, 30], "temperature": 0.0, "avg_logprob": -0.10459928605162981, "compression_ratio": 1.6378600823045268, "no_speech_prob": 1.72299533005571e-05}, {"id": 1216, "seek": 536982, "start": 5392.46, "end": 5395.46, "text": " There's a lot of benefits to packaging up things together.", "tokens": [821, 311, 257, 688, 295, 5311, 281, 16836, 493, 721, 1214, 13], "temperature": 0.0, "avg_logprob": -0.10459928605162981, "compression_ratio": 1.6378600823045268, "no_speech_prob": 1.72299533005571e-05}, {"id": 1217, "seek": 539546, "start": 5395.46, "end": 5400.14, "text": " And you can package up things together where they're kind of alike things.", "tokens": [400, 291, 393, 7372, 493, 721, 1214, 689, 436, 434, 733, 295, 20025, 721, 13], "temperature": 0.0, "avg_logprob": -0.09079693013971502, "compression_ratio": 1.8509803921568628, "no_speech_prob": 1.06144843812217e-05}, {"id": 1218, "seek": 539546, "start": 5400.14, "end": 5404.62, "text": " You can pass them around to everything that needs them together.", "tokens": [509, 393, 1320, 552, 926, 281, 1203, 300, 2203, 552, 1214, 13], "temperature": 0.0, "avg_logprob": -0.09079693013971502, "compression_ratio": 1.8509803921568628, "no_speech_prob": 1.06144843812217e-05}, {"id": 1219, "seek": 539546, "start": 5404.62, "end": 5408.82, "text": " You can create them using kind of factory methods that create them together.", "tokens": [509, 393, 1884, 552, 1228, 733, 295, 9265, 7150, 300, 1884, 552, 1214, 13], "temperature": 0.0, "avg_logprob": -0.09079693013971502, "compression_ratio": 1.8509803921568628, "no_speech_prob": 1.06144843812217e-05}, {"id": 1220, "seek": 539546, "start": 5408.82, "end": 5414.14, "text": " And you can do smart things, like look at the combination of them and make smart decisions", "tokens": [400, 291, 393, 360, 4069, 721, 11, 411, 574, 412, 264, 6562, 295, 552, 293, 652, 4069, 5327], "temperature": 0.0, "avg_logprob": -0.09079693013971502, "compression_ratio": 1.8509803921568628, "no_speech_prob": 1.06144843812217e-05}, {"id": 1221, "seek": 539546, "start": 5414.14, "end": 5418.78, "text": " for your users, rather than having to have them set everything themselves.", "tokens": [337, 428, 5022, 11, 2831, 813, 1419, 281, 362, 552, 992, 1203, 2969, 13], "temperature": 0.0, "avg_logprob": -0.09079693013971502, "compression_ratio": 1.8509803921568628, "no_speech_prob": 1.06144843812217e-05}, {"id": 1222, "seek": 539546, "start": 5418.78, "end": 5424.62, "text": " So there's lots of reasons that I would prefer to keep epochs, right, but I'd like to put", "tokens": [407, 456, 311, 3195, 295, 4112, 300, 286, 576, 4382, 281, 1066, 30992, 28346, 11, 558, 11, 457, 286, 1116, 411, 281, 829], "temperature": 0.0, "avg_logprob": -0.09079693013971502, "compression_ratio": 1.8509803921568628, "no_speech_prob": 1.06144843812217e-05}, {"id": 1223, "seek": 542462, "start": 5424.62, "end": 5429.78, "text": " all these other things into a single object.", "tokens": [439, 613, 661, 721, 666, 257, 2167, 2657, 13], "temperature": 0.0, "avg_logprob": -0.11952857340662933, "compression_ratio": 1.6809338521400778, "no_speech_prob": 8.397851161134895e-06}, {"id": 1224, "seek": 542462, "start": 5429.78, "end": 5431.58, "text": " And specifically, we can do that in two steps.", "tokens": [400, 4682, 11, 321, 393, 360, 300, 294, 732, 4439, 13], "temperature": 0.0, "avg_logprob": -0.11952857340662933, "compression_ratio": 1.6809338521400778, "no_speech_prob": 8.397851161134895e-06}, {"id": 1225, "seek": 542462, "start": 5431.58, "end": 5436.62, "text": " First of all, let's take this data and say training and valid data conceptually should", "tokens": [2386, 295, 439, 11, 718, 311, 747, 341, 1412, 293, 584, 3097, 293, 7363, 1412, 3410, 671, 820], "temperature": 0.0, "avg_logprob": -0.11952857340662933, "compression_ratio": 1.6809338521400778, "no_speech_prob": 8.397851161134895e-06}, {"id": 1226, "seek": 542462, "start": 5436.62, "end": 5437.82, "text": " be one thing.", "tokens": [312, 472, 551, 13], "temperature": 0.0, "avg_logprob": -0.11952857340662933, "compression_ratio": 1.6809338521400778, "no_speech_prob": 8.397851161134895e-06}, {"id": 1227, "seek": 542462, "start": 5437.82, "end": 5439.16, "text": " It's my data, right?", "tokens": [467, 311, 452, 1412, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.11952857340662933, "compression_ratio": 1.6809338521400778, "no_speech_prob": 8.397851161134895e-06}, {"id": 1228, "seek": 542462, "start": 5439.16, "end": 5441.94, "text": " Maybe there's test data there as well.", "tokens": [2704, 456, 311, 1500, 1412, 456, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.11952857340662933, "compression_ratio": 1.6809338521400778, "no_speech_prob": 8.397851161134895e-06}, {"id": 1229, "seek": 542462, "start": 5441.94, "end": 5448.08, "text": " So let's create a class called data bunch that we're going to pass in training data", "tokens": [407, 718, 311, 1884, 257, 1508, 1219, 1412, 3840, 300, 321, 434, 516, 281, 1320, 294, 3097, 1412], "temperature": 0.0, "avg_logprob": -0.11952857340662933, "compression_ratio": 1.6809338521400778, "no_speech_prob": 8.397851161134895e-06}, {"id": 1230, "seek": 542462, "start": 5448.08, "end": 5451.46, "text": " and validation data, and we'll store them away.", "tokens": [293, 24071, 1412, 11, 293, 321, 603, 3531, 552, 1314, 13], "temperature": 0.0, "avg_logprob": -0.11952857340662933, "compression_ratio": 1.6809338521400778, "no_speech_prob": 8.397851161134895e-06}, {"id": 1231, "seek": 542462, "start": 5451.46, "end": 5452.46, "text": " And that's the entirety.", "tokens": [400, 300, 311, 264, 31557, 13], "temperature": 0.0, "avg_logprob": -0.11952857340662933, "compression_ratio": 1.6809338521400778, "no_speech_prob": 8.397851161134895e-06}, {"id": 1232, "seek": 542462, "start": 5452.46, "end": 5453.46, "text": " There's no logic here.", "tokens": [821, 311, 572, 9952, 510, 13], "temperature": 0.0, "avg_logprob": -0.11952857340662933, "compression_ratio": 1.6809338521400778, "no_speech_prob": 8.397851161134895e-06}, {"id": 1233, "seek": 545346, "start": 5453.46, "end": 5458.58, "text": " But for convenience, let's make it easy to grab the data set out of them as well.", "tokens": [583, 337, 19283, 11, 718, 311, 652, 309, 1858, 281, 4444, 264, 1412, 992, 484, 295, 552, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.14912162570778384, "compression_ratio": 1.6487603305785123, "no_speech_prob": 5.1736997193074785e-06}, {"id": 1234, "seek": 545346, "start": 5458.58, "end": 5461.9, "text": " And remember, we're now using...", "tokens": [400, 1604, 11, 321, 434, 586, 1228, 485], "temperature": 0.0, "avg_logprob": -0.14912162570778384, "compression_ratio": 1.6487603305785123, "no_speech_prob": 5.1736997193074785e-06}, {"id": 1235, "seek": 545346, "start": 5461.9, "end": 5467.82, "text": " You can either use the handmade data loader that we built in the last one, or you can", "tokens": [509, 393, 2139, 764, 264, 39446, 1412, 3677, 260, 300, 321, 3094, 294, 264, 1036, 472, 11, 420, 291, 393], "temperature": 0.0, "avg_logprob": -0.14912162570778384, "compression_ratio": 1.6487603305785123, "no_speech_prob": 5.1736997193074785e-06}, {"id": 1236, "seek": 545346, "start": 5467.82, "end": 5470.46, "text": " use the PyTorch data loader.", "tokens": [764, 264, 9953, 51, 284, 339, 1412, 3677, 260, 13], "temperature": 0.0, "avg_logprob": -0.14912162570778384, "compression_ratio": 1.6487603305785123, "no_speech_prob": 5.1736997193074785e-06}, {"id": 1237, "seek": 545346, "start": 5470.46, "end": 5478.36, "text": " They're both providing exactly the same API at this point, except for the numWorkers issue.", "tokens": [814, 434, 1293, 6530, 2293, 264, 912, 9362, 412, 341, 935, 11, 3993, 337, 264, 1031, 28846, 433, 2734, 13], "temperature": 0.0, "avg_logprob": -0.14912162570778384, "compression_ratio": 1.6487603305785123, "no_speech_prob": 5.1736997193074785e-06}, {"id": 1238, "seek": 545346, "start": 5478.36, "end": 5482.9, "text": " So remember that we passed these data loaders a data set that you can access.", "tokens": [407, 1604, 300, 321, 4678, 613, 1412, 3677, 433, 257, 1412, 992, 300, 291, 393, 2105, 13], "temperature": 0.0, "avg_logprob": -0.14912162570778384, "compression_ratio": 1.6487603305785123, "no_speech_prob": 5.1736997193074785e-06}, {"id": 1239, "seek": 548290, "start": 5482.9, "end": 5490.5, "text": " And then it would be nice if we could create a getModel function, which could create our", "tokens": [400, 550, 309, 576, 312, 1481, 498, 321, 727, 1884, 257, 483, 44, 41147, 2445, 11, 597, 727, 1884, 527], "temperature": 0.0, "avg_logprob": -0.09425634496352252, "compression_ratio": 1.7587719298245614, "no_speech_prob": 5.093596428196179e-06}, {"id": 1240, "seek": 548290, "start": 5490.5, "end": 5496.179999999999, "text": " model but automatically set the last layer to have the correct number of activations,", "tokens": [2316, 457, 6772, 992, 264, 1036, 4583, 281, 362, 264, 3006, 1230, 295, 2430, 763, 11], "temperature": 0.0, "avg_logprob": -0.09425634496352252, "compression_ratio": 1.7587719298245614, "no_speech_prob": 5.093596428196179e-06}, {"id": 1241, "seek": 548290, "start": 5496.179999999999, "end": 5499.78, "text": " because the data knows how many activations it needs.", "tokens": [570, 264, 1412, 3255, 577, 867, 2430, 763, 309, 2203, 13], "temperature": 0.0, "avg_logprob": -0.09425634496352252, "compression_ratio": 1.7587719298245614, "no_speech_prob": 5.093596428196179e-06}, {"id": 1242, "seek": 548290, "start": 5499.78, "end": 5503.78, "text": " So let's also optionally make it that you can pass in C, which is going to get stored", "tokens": [407, 718, 311, 611, 3614, 379, 652, 309, 300, 291, 393, 1320, 294, 383, 11, 597, 307, 516, 281, 483, 12187], "temperature": 0.0, "avg_logprob": -0.09425634496352252, "compression_ratio": 1.7587719298245614, "no_speech_prob": 5.093596428196179e-06}, {"id": 1243, "seek": 548290, "start": 5503.78, "end": 5510.0599999999995, "text": " away, so that then when we create our data, we can pass in C, which remember we set to", "tokens": [1314, 11, 370, 300, 550, 562, 321, 1884, 527, 1412, 11, 321, 393, 1320, 294, 383, 11, 597, 1604, 321, 992, 281], "temperature": 0.0, "avg_logprob": -0.09425634496352252, "compression_ratio": 1.7587719298245614, "no_speech_prob": 5.093596428196179e-06}, {"id": 1244, "seek": 551006, "start": 5510.06, "end": 5513.9400000000005, "text": " our maximum Y value.", "tokens": [527, 6674, 398, 2158, 13], "temperature": 0.0, "avg_logprob": -0.1347580761976645, "compression_ratio": 1.4774193548387098, "no_speech_prob": 7.4109902925556526e-06}, {"id": 1245, "seek": 551006, "start": 5513.9400000000005, "end": 5519.9800000000005, "text": " And so that way, we never have to think about that again.", "tokens": [400, 370, 300, 636, 11, 321, 1128, 362, 281, 519, 466, 300, 797, 13], "temperature": 0.0, "avg_logprob": -0.1347580761976645, "compression_ratio": 1.4774193548387098, "no_speech_prob": 7.4109902925556526e-06}, {"id": 1246, "seek": 551006, "start": 5519.9800000000005, "end": 5526.580000000001, "text": " So that's our data bunch class.", "tokens": [407, 300, 311, 527, 1412, 3840, 1508, 13], "temperature": 0.0, "avg_logprob": -0.1347580761976645, "compression_ratio": 1.4774193548387098, "no_speech_prob": 7.4109902925556526e-06}, {"id": 1247, "seek": 551006, "start": 5526.580000000001, "end": 5527.660000000001, "text": " So there's our getModel.", "tokens": [407, 456, 311, 527, 483, 44, 41147, 13], "temperature": 0.0, "avg_logprob": -0.1347580761976645, "compression_ratio": 1.4774193548387098, "no_speech_prob": 7.4109902925556526e-06}, {"id": 1248, "seek": 551006, "start": 5527.660000000001, "end": 5535.46, "text": " So it's just going to create a model with the number of inputs is the size of the input", "tokens": [407, 309, 311, 445, 516, 281, 1884, 257, 2316, 365, 264, 1230, 295, 15743, 307, 264, 2744, 295, 264, 4846], "temperature": 0.0, "avg_logprob": -0.1347580761976645, "compression_ratio": 1.4774193548387098, "no_speech_prob": 7.4109902925556526e-06}, {"id": 1249, "seek": 551006, "start": 5535.46, "end": 5537.580000000001, "text": " data.", "tokens": [1412, 13], "temperature": 0.0, "avg_logprob": -0.1347580761976645, "compression_ratio": 1.4774193548387098, "no_speech_prob": 7.4109902925556526e-06}, {"id": 1250, "seek": 553758, "start": 5537.58, "end": 5542.98, "text": " Number of hidden is whatever we had earlier, whatever we pass in, then a value, and then", "tokens": [5118, 295, 7633, 307, 2035, 321, 632, 3071, 11, 2035, 321, 1320, 294, 11, 550, 257, 2158, 11, 293, 550], "temperature": 0.0, "avg_logprob": -0.19566084959796656, "compression_ratio": 1.6820276497695852, "no_speech_prob": 4.495132088777609e-06}, {"id": 1251, "seek": 553758, "start": 5542.98, "end": 5549.98, "text": " a linear from hidden to data.c, and return the model and an optimizer.", "tokens": [257, 8213, 490, 7633, 281, 1412, 13, 66, 11, 293, 2736, 264, 2316, 293, 364, 5028, 6545, 13], "temperature": 0.0, "avg_logprob": -0.19566084959796656, "compression_ratio": 1.6820276497695852, "no_speech_prob": 4.495132088777609e-06}, {"id": 1252, "seek": 553758, "start": 5549.98, "end": 5552.96, "text": " And we all know all about dot parameters now.", "tokens": [400, 321, 439, 458, 439, 466, 5893, 9834, 586, 13], "temperature": 0.0, "avg_logprob": -0.19566084959796656, "compression_ratio": 1.6820276497695852, "no_speech_prob": 4.495132088777609e-06}, {"id": 1253, "seek": 553758, "start": 5552.96, "end": 5561.32, "text": " So then the other rest of the stuff, model, lossfunk, opt, and data, let's store them", "tokens": [407, 550, 264, 661, 1472, 295, 264, 1507, 11, 2316, 11, 4470, 69, 3197, 11, 2427, 11, 293, 1412, 11, 718, 311, 3531, 552], "temperature": 0.0, "avg_logprob": -0.19566084959796656, "compression_ratio": 1.6820276497695852, "no_speech_prob": 4.495132088777609e-06}, {"id": 1254, "seek": 553758, "start": 5561.32, "end": 5562.32, "text": " in something.", "tokens": [294, 746, 13], "temperature": 0.0, "avg_logprob": -0.19566084959796656, "compression_ratio": 1.6820276497695852, "no_speech_prob": 4.495132088777609e-06}, {"id": 1255, "seek": 553758, "start": 5562.32, "end": 5565.96, "text": " Model, opt, lossfunk, data, and we'll just store them away.", "tokens": [17105, 11, 2427, 11, 4470, 69, 3197, 11, 1412, 11, 293, 321, 603, 445, 3531, 552, 1314, 13], "temperature": 0.0, "avg_logprob": -0.19566084959796656, "compression_ratio": 1.6820276497695852, "no_speech_prob": 4.495132088777609e-06}, {"id": 1256, "seek": 556596, "start": 5565.96, "end": 5568.1, "text": " And that thing we'll call a learner.", "tokens": [400, 300, 551, 321, 603, 818, 257, 33347, 13], "temperature": 0.0, "avg_logprob": -0.11809270627030702, "compression_ratio": 1.7545454545454546, "no_speech_prob": 5.955068445473444e-06}, {"id": 1257, "seek": 556596, "start": 5568.1, "end": 5572.3, "text": " So notice our learner class has no logic at all.", "tokens": [407, 3449, 527, 33347, 1508, 575, 572, 9952, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.11809270627030702, "compression_ratio": 1.7545454545454546, "no_speech_prob": 5.955068445473444e-06}, {"id": 1258, "seek": 556596, "start": 5572.3, "end": 5578.6, "text": " It's just a storage device for these four things.", "tokens": [467, 311, 445, 257, 6725, 4302, 337, 613, 1451, 721, 13], "temperature": 0.0, "avg_logprob": -0.11809270627030702, "compression_ratio": 1.7545454545454546, "no_speech_prob": 5.955068445473444e-06}, {"id": 1259, "seek": 556596, "start": 5578.6, "end": 5585.16, "text": " So now we can create a learner passing in the model and the optimizer.", "tokens": [407, 586, 321, 393, 1884, 257, 33347, 8437, 294, 264, 2316, 293, 264, 5028, 6545, 13], "temperature": 0.0, "avg_logprob": -0.11809270627030702, "compression_ratio": 1.7545454545454546, "no_speech_prob": 5.955068445473444e-06}, {"id": 1260, "seek": 556596, "start": 5585.16, "end": 5589.32, "text": " Since they're returned in this order from getModel, we can just say star getModel.", "tokens": [4162, 436, 434, 8752, 294, 341, 1668, 490, 483, 44, 41147, 11, 321, 393, 445, 584, 3543, 483, 44, 41147, 13], "temperature": 0.0, "avg_logprob": -0.11809270627030702, "compression_ratio": 1.7545454545454546, "no_speech_prob": 5.955068445473444e-06}, {"id": 1261, "seek": 556596, "start": 5589.32, "end": 5592.06, "text": " So that's going to pass in the model and the optimizer.", "tokens": [407, 300, 311, 516, 281, 1320, 294, 264, 2316, 293, 264, 5028, 6545, 13], "temperature": 0.0, "avg_logprob": -0.11809270627030702, "compression_ratio": 1.7545454545454546, "no_speech_prob": 5.955068445473444e-06}, {"id": 1262, "seek": 556596, "start": 5592.06, "end": 5593.78, "text": " And we've got our loss function already.", "tokens": [400, 321, 600, 658, 527, 4470, 2445, 1217, 13], "temperature": 0.0, "avg_logprob": -0.11809270627030702, "compression_ratio": 1.7545454545454546, "no_speech_prob": 5.955068445473444e-06}, {"id": 1263, "seek": 559378, "start": 5593.78, "end": 5596.639999999999, "text": " At the top here, we set it to cross entropy.", "tokens": [1711, 264, 1192, 510, 11, 321, 992, 309, 281, 3278, 30867, 13], "temperature": 0.0, "avg_logprob": -0.1022932870047433, "compression_ratio": 1.7710843373493976, "no_speech_prob": 1.4509784705296624e-05}, {"id": 1264, "seek": 559378, "start": 5596.639999999999, "end": 5600.62, "text": " And we've got our data, because it's that data bunch we just created.", "tokens": [400, 321, 600, 658, 527, 1412, 11, 570, 309, 311, 300, 1412, 3840, 321, 445, 2942, 13], "temperature": 0.0, "avg_logprob": -0.1022932870047433, "compression_ratio": 1.7710843373493976, "no_speech_prob": 1.4509784705296624e-05}, {"id": 1265, "seek": 559378, "start": 5600.62, "end": 5604.66, "text": " So there's nothing magic going on with data bunches and learners.", "tokens": [407, 456, 311, 1825, 5585, 516, 322, 365, 1412, 3840, 279, 293, 23655, 13], "temperature": 0.0, "avg_logprob": -0.1022932870047433, "compression_ratio": 1.7710843373493976, "no_speech_prob": 1.4509784705296624e-05}, {"id": 1266, "seek": 559378, "start": 5604.66, "end": 5609.259999999999, "text": " They're just wrappers for the information that we need.", "tokens": [814, 434, 445, 7843, 15226, 337, 264, 1589, 300, 321, 643, 13], "temperature": 0.0, "avg_logprob": -0.1022932870047433, "compression_ratio": 1.7710843373493976, "no_speech_prob": 1.4509784705296624e-05}, {"id": 1267, "seek": 559378, "start": 5609.259999999999, "end": 5613.5199999999995, "text": " So now we'll take the fit function we had before, and I just pasted it here.", "tokens": [407, 586, 321, 603, 747, 264, 3318, 2445, 321, 632, 949, 11, 293, 286, 445, 1791, 292, 309, 510, 13], "temperature": 0.0, "avg_logprob": -0.1022932870047433, "compression_ratio": 1.7710843373493976, "no_speech_prob": 1.4509784705296624e-05}, {"id": 1268, "seek": 559378, "start": 5613.5199999999995, "end": 5616.98, "text": " But every time I had model, I replaced it with learn.model.", "tokens": [583, 633, 565, 286, 632, 2316, 11, 286, 10772, 309, 365, 1466, 13, 8014, 338, 13], "temperature": 0.0, "avg_logprob": -0.1022932870047433, "compression_ratio": 1.7710843373493976, "no_speech_prob": 1.4509784705296624e-05}, {"id": 1269, "seek": 559378, "start": 5616.98, "end": 5621.98, "text": " Every time I had data, I replaced it with learn.data, and so forth.", "tokens": [2048, 565, 286, 632, 1412, 11, 286, 10772, 309, 365, 1466, 13, 67, 3274, 11, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.1022932870047433, "compression_ratio": 1.7710843373493976, "no_speech_prob": 1.4509784705296624e-05}, {"id": 1270, "seek": 562198, "start": 5621.98, "end": 5627.5, "text": " So there's the exact same thing that we had before, still working fine.", "tokens": [407, 456, 311, 264, 1900, 912, 551, 300, 321, 632, 949, 11, 920, 1364, 2489, 13], "temperature": 0.0, "avg_logprob": -0.2752001682917277, "compression_ratio": 1.5681818181818181, "no_speech_prob": 1.933323346747784e-06}, {"id": 1271, "seek": 562198, "start": 5629.099999999999, "end": 5632.7, "text": " And so now, let's add callbacks.", "tokens": [400, 370, 586, 11, 718, 311, 909, 818, 17758, 13], "temperature": 0.0, "avg_logprob": -0.2752001682917277, "compression_ratio": 1.5681818181818181, "no_speech_prob": 1.933323346747784e-06}, {"id": 1272, "seek": 562198, "start": 5635.379999999999, "end": 5640.459999999999, "text": " So our fit function before basically said for epoch, in range epochs,", "tokens": [407, 527, 3318, 2445, 949, 1936, 848, 337, 30992, 339, 11, 294, 3613, 30992, 28346, 11], "temperature": 0.0, "avg_logprob": -0.2752001682917277, "compression_ratio": 1.5681818181818181, "no_speech_prob": 1.933323346747784e-06}, {"id": 1273, "seek": 562198, "start": 5640.459999999999, "end": 5644.419999999999, "text": " for batch, in train dl, and then it had these contents, right?", "tokens": [337, 15245, 11, 294, 3847, 37873, 11, 293, 550, 309, 632, 613, 15768, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2752001682917277, "compression_ratio": 1.5681818181818181, "no_speech_prob": 1.933323346747784e-06}, {"id": 1274, "seek": 562198, "start": 5644.419999999999, "end": 5646.98, "text": " Predictions, loss, backwards, step, zero grad.", "tokens": [32969, 15607, 11, 4470, 11, 12204, 11, 1823, 11, 4018, 2771, 13], "temperature": 0.0, "avg_logprob": -0.2752001682917277, "compression_ratio": 1.5681818181818181, "no_speech_prob": 1.933323346747784e-06}, {"id": 1275, "seek": 562198, "start": 5646.98, "end": 5650.58, "text": " I factored out the contents into something called one batch.", "tokens": [286, 1186, 2769, 484, 264, 15768, 666, 746, 1219, 472, 15245, 13], "temperature": 0.0, "avg_logprob": -0.2752001682917277, "compression_ratio": 1.5681818181818181, "no_speech_prob": 1.933323346747784e-06}, {"id": 1276, "seek": 565058, "start": 5650.58, "end": 5659.26, "text": " Okay, and then I added all these callbacks.", "tokens": [1033, 11, 293, 550, 286, 3869, 439, 613, 818, 17758, 13], "temperature": 0.0, "avg_logprob": -0.2928213190149378, "compression_ratio": 1.6379310344827587, "no_speech_prob": 3.6119381547905505e-06}, {"id": 1277, "seek": 565058, "start": 5659.26, "end": 5662.9, "text": " All right, cb.afterbackward, cd.afterstep.", "tokens": [1057, 558, 11, 269, 65, 13, 18837, 3207, 1007, 11, 269, 67, 13, 18837, 16792, 13], "temperature": 0.0, "avg_logprob": -0.2928213190149378, "compression_ratio": 1.6379310344827587, "no_speech_prob": 3.6119381547905505e-06}, {"id": 1278, "seek": 565058, "start": 5664.26, "end": 5670.66, "text": " I did one other refactoring, which is that the training loop has to loop", "tokens": [286, 630, 472, 661, 1895, 578, 3662, 11, 597, 307, 300, 264, 3097, 6367, 575, 281, 6367], "temperature": 0.0, "avg_logprob": -0.2928213190149378, "compression_ratio": 1.6379310344827587, "no_speech_prob": 3.6119381547905505e-06}, {"id": 1279, "seek": 565058, "start": 5670.66, "end": 5675.38, "text": " through every batch, and the validation loop has to loop through every batch.", "tokens": [807, 633, 15245, 11, 293, 264, 24071, 6367, 575, 281, 6367, 807, 633, 15245, 13], "temperature": 0.0, "avg_logprob": -0.2928213190149378, "compression_ratio": 1.6379310344827587, "no_speech_prob": 3.6119381547905505e-06}, {"id": 1280, "seek": 565058, "start": 5675.38, "end": 5679.0599999999995, "text": " So I just created something called all batches.", "tokens": [407, 286, 445, 2942, 746, 1219, 439, 15245, 279, 13], "temperature": 0.0, "avg_logprob": -0.2928213190149378, "compression_ratio": 1.6379310344827587, "no_speech_prob": 3.6119381547905505e-06}, {"id": 1281, "seek": 567906, "start": 5679.06, "end": 5681.660000000001, "text": " Okay, so this is my fit loop, right?", "tokens": [1033, 11, 370, 341, 307, 452, 3318, 6367, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.23883251710371536, "compression_ratio": 1.621301775147929, "no_speech_prob": 9.972699444915634e-06}, {"id": 1282, "seek": 567906, "start": 5681.660000000001, "end": 5687.700000000001, "text": " Begin fit, the epoch in epochs, begin epoch, all batches with a training set.", "tokens": [20660, 3318, 11, 264, 30992, 339, 294, 30992, 28346, 11, 1841, 30992, 339, 11, 439, 15245, 279, 365, 257, 3097, 992, 13], "temperature": 0.0, "avg_logprob": -0.23883251710371536, "compression_ratio": 1.621301775147929, "no_speech_prob": 9.972699444915634e-06}, {"id": 1283, "seek": 567906, "start": 5687.700000000001, "end": 5694.34, "text": " Begin validate, no grad, all batches with a validation set, after epoch, after fit.", "tokens": [20660, 29562, 11, 572, 2771, 11, 439, 15245, 279, 365, 257, 24071, 992, 11, 934, 30992, 339, 11, 934, 3318, 13], "temperature": 0.0, "avg_logprob": -0.23883251710371536, "compression_ratio": 1.621301775147929, "no_speech_prob": 9.972699444915634e-06}, {"id": 1284, "seek": 567906, "start": 5695.38, "end": 5697.740000000001, "text": " Okay, so that's that.", "tokens": [1033, 11, 370, 300, 311, 300, 13], "temperature": 0.0, "avg_logprob": -0.23883251710371536, "compression_ratio": 1.621301775147929, "no_speech_prob": 9.972699444915634e-06}, {"id": 1285, "seek": 567906, "start": 5699.780000000001, "end": 5704.740000000001, "text": " So here's a callback, right, which has all the stuff.", "tokens": [407, 510, 311, 257, 818, 3207, 11, 558, 11, 597, 575, 439, 264, 1507, 13], "temperature": 0.0, "avg_logprob": -0.23883251710371536, "compression_ratio": 1.621301775147929, "no_speech_prob": 9.972699444915634e-06}, {"id": 1286, "seek": 570474, "start": 5704.74, "end": 5709.0599999999995, "text": " And so then we need a callback handler, and", "tokens": [400, 370, 550, 321, 643, 257, 818, 3207, 41967, 11, 293], "temperature": 0.0, "avg_logprob": -0.2049768304312101, "compression_ratio": 1.6650246305418719, "no_speech_prob": 9.222876542480662e-06}, {"id": 1287, "seek": 570474, "start": 5709.0599999999995, "end": 5716.34, "text": " that's gonna be something which you just say, here's all my callbacks.", "tokens": [300, 311, 799, 312, 746, 597, 291, 445, 584, 11, 510, 311, 439, 452, 818, 17758, 13], "temperature": 0.0, "avg_logprob": -0.2049768304312101, "compression_ratio": 1.6650246305418719, "no_speech_prob": 9.222876542480662e-06}, {"id": 1288, "seek": 570474, "start": 5716.34, "end": 5720.46, "text": " And basically, it's just gonna go through for each thing and say,", "tokens": [400, 1936, 11, 309, 311, 445, 799, 352, 807, 337, 1184, 551, 293, 584, 11], "temperature": 0.0, "avg_logprob": -0.2049768304312101, "compression_ratio": 1.6650246305418719, "no_speech_prob": 9.222876542480662e-06}, {"id": 1289, "seek": 570474, "start": 5720.46, "end": 5723.5, "text": " go through every callback and call it.", "tokens": [352, 807, 633, 818, 3207, 293, 818, 309, 13], "temperature": 0.0, "avg_logprob": -0.2049768304312101, "compression_ratio": 1.6650246305418719, "no_speech_prob": 9.222876542480662e-06}, {"id": 1290, "seek": 570474, "start": 5724.94, "end": 5730.219999999999, "text": " And keep track of whether we've received a false yet or not.", "tokens": [400, 1066, 2837, 295, 1968, 321, 600, 4613, 257, 7908, 1939, 420, 406, 13], "temperature": 0.0, "avg_logprob": -0.2049768304312101, "compression_ratio": 1.6650246305418719, "no_speech_prob": 9.222876542480662e-06}, {"id": 1291, "seek": 570474, "start": 5730.219999999999, "end": 5733.3, "text": " False means don't keep going anymore, and then return it.", "tokens": [50040, 1355, 500, 380, 1066, 516, 3602, 11, 293, 550, 2736, 309, 13], "temperature": 0.0, "avg_logprob": -0.2049768304312101, "compression_ratio": 1.6650246305418719, "no_speech_prob": 9.222876542480662e-06}, {"id": 1292, "seek": 573330, "start": 5733.3, "end": 5738.62, "text": " So we do that for begin fit, after fit, begin epoch, begin validate,", "tokens": [407, 321, 360, 300, 337, 1841, 3318, 11, 934, 3318, 11, 1841, 30992, 339, 11, 1841, 29562, 11], "temperature": 0.0, "avg_logprob": -0.1558305566961115, "compression_ratio": 1.7981220657276995, "no_speech_prob": 7.183182788139675e-06}, {"id": 1293, "seek": 573330, "start": 5738.62, "end": 5741.62, "text": " after epoch, begin batch, after loss, after backward, after step.", "tokens": [934, 30992, 339, 11, 1841, 15245, 11, 934, 4470, 11, 934, 23897, 11, 934, 1823, 13], "temperature": 0.0, "avg_logprob": -0.1558305566961115, "compression_ratio": 1.7981220657276995, "no_speech_prob": 7.183182788139675e-06}, {"id": 1294, "seek": 573330, "start": 5743.78, "end": 5747.54, "text": " So here's an example of a little callback we could create.", "tokens": [407, 510, 311, 364, 1365, 295, 257, 707, 818, 3207, 321, 727, 1884, 13], "temperature": 0.0, "avg_logprob": -0.1558305566961115, "compression_ratio": 1.7981220657276995, "no_speech_prob": 7.183182788139675e-06}, {"id": 1295, "seek": 573330, "start": 5747.54, "end": 5752.46, "text": " And it's one that's going to, at the start of the fit,", "tokens": [400, 309, 311, 472, 300, 311, 516, 281, 11, 412, 264, 722, 295, 264, 3318, 11], "temperature": 0.0, "avg_logprob": -0.1558305566961115, "compression_ratio": 1.7981220657276995, "no_speech_prob": 7.183182788139675e-06}, {"id": 1296, "seek": 573330, "start": 5752.46, "end": 5754.26, "text": " it'll set number of iterations to zero.", "tokens": [309, 603, 992, 1230, 295, 36540, 281, 4018, 13], "temperature": 0.0, "avg_logprob": -0.1558305566961115, "compression_ratio": 1.7981220657276995, "no_speech_prob": 7.183182788139675e-06}, {"id": 1297, "seek": 573330, "start": 5755.46, "end": 5759.5, "text": " And then after every step, it'll say number of iterations plus equals one,", "tokens": [400, 550, 934, 633, 1823, 11, 309, 603, 584, 1230, 295, 36540, 1804, 6915, 472, 11], "temperature": 0.0, "avg_logprob": -0.1558305566961115, "compression_ratio": 1.7981220657276995, "no_speech_prob": 7.183182788139675e-06}, {"id": 1298, "seek": 573330, "start": 5760.66, "end": 5762.34, "text": " and print that out.", "tokens": [293, 4482, 300, 484, 13], "temperature": 0.0, "avg_logprob": -0.1558305566961115, "compression_ratio": 1.7981220657276995, "no_speech_prob": 7.183182788139675e-06}, {"id": 1299, "seek": 576234, "start": 5762.34, "end": 5767.7, "text": " And if we get past ten iterations, then it'll tell the learner to stop.", "tokens": [400, 498, 321, 483, 1791, 2064, 36540, 11, 550, 309, 603, 980, 264, 33347, 281, 1590, 13], "temperature": 0.0, "avg_logprob": -0.21942190366370656, "compression_ratio": 1.626086956521739, "no_speech_prob": 5.682296887243865e-06}, {"id": 1300, "seek": 576234, "start": 5769.62, "end": 5772.22, "text": " Because we have this little thing called do stop,", "tokens": [1436, 321, 362, 341, 707, 551, 1219, 360, 1590, 11], "temperature": 0.0, "avg_logprob": -0.21942190366370656, "compression_ratio": 1.626086956521739, "no_speech_prob": 5.682296887243865e-06}, {"id": 1301, "seek": 576234, "start": 5772.22, "end": 5773.22, "text": " that gets checked at the end.", "tokens": [300, 2170, 10033, 412, 264, 917, 13], "temperature": 0.0, "avg_logprob": -0.21942190366370656, "compression_ratio": 1.626086956521739, "no_speech_prob": 5.682296887243865e-06}, {"id": 1302, "seek": 576234, "start": 5775.1, "end": 5775.860000000001, "text": " So let's test it.", "tokens": [407, 718, 311, 1500, 309, 13], "temperature": 0.0, "avg_logprob": -0.21942190366370656, "compression_ratio": 1.626086956521739, "no_speech_prob": 5.682296887243865e-06}, {"id": 1303, "seek": 576234, "start": 5780.34, "end": 5781.02, "text": " There we go.", "tokens": [821, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.21942190366370656, "compression_ratio": 1.626086956521739, "no_speech_prob": 5.682296887243865e-06}, {"id": 1304, "seek": 576234, "start": 5781.02, "end": 5784.5, "text": " And so it called fit, and it only did ten batches.", "tokens": [400, 370, 309, 1219, 3318, 11, 293, 309, 787, 630, 2064, 15245, 279, 13], "temperature": 0.0, "avg_logprob": -0.21942190366370656, "compression_ratio": 1.626086956521739, "no_speech_prob": 5.682296887243865e-06}, {"id": 1305, "seek": 576234, "start": 5784.5, "end": 5788.74, "text": " And this is actually a really handy callback, because quite often you want to", "tokens": [400, 341, 307, 767, 257, 534, 13239, 818, 3207, 11, 570, 1596, 2049, 291, 528, 281], "temperature": 0.0, "avg_logprob": -0.21942190366370656, "compression_ratio": 1.626086956521739, "no_speech_prob": 5.682296887243865e-06}, {"id": 1306, "seek": 576234, "start": 5788.74, "end": 5791.26, "text": " just run a few batches to make sure things seem to be working.", "tokens": [445, 1190, 257, 1326, 15245, 279, 281, 652, 988, 721, 1643, 281, 312, 1364, 13], "temperature": 0.0, "avg_logprob": -0.21942190366370656, "compression_ratio": 1.626086956521739, "no_speech_prob": 5.682296887243865e-06}, {"id": 1307, "seek": 579126, "start": 5791.26, "end": 5792.900000000001, "text": " You don't want to run a whole epoch.", "tokens": [509, 500, 380, 528, 281, 1190, 257, 1379, 30992, 339, 13], "temperature": 0.0, "avg_logprob": -0.21131548750291176, "compression_ratio": 1.5793650793650793, "no_speech_prob": 8.664206688990816e-06}, {"id": 1308, "seek": 579126, "start": 5792.900000000001, "end": 5795.3, "text": " So here's a quick way you can do something like that.", "tokens": [407, 510, 311, 257, 1702, 636, 291, 393, 360, 746, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.21131548750291176, "compression_ratio": 1.5793650793650793, "no_speech_prob": 8.664206688990816e-06}, {"id": 1309, "seek": 579126, "start": 5797.42, "end": 5802.38, "text": " This is basically what fast AI V1 looks like right now.", "tokens": [639, 307, 1936, 437, 2370, 7318, 691, 16, 1542, 411, 558, 586, 13], "temperature": 0.0, "avg_logprob": -0.21131548750291176, "compression_ratio": 1.5793650793650793, "no_speech_prob": 8.664206688990816e-06}, {"id": 1310, "seek": 579126, "start": 5803.7, "end": 5808.58, "text": " It does have a little bit of extra stuff that lets you pass back a different loss", "tokens": [467, 775, 362, 257, 707, 857, 295, 2857, 1507, 300, 6653, 291, 1320, 646, 257, 819, 4470], "temperature": 0.0, "avg_logprob": -0.21131548750291176, "compression_ratio": 1.5793650793650793, "no_speech_prob": 8.664206688990816e-06}, {"id": 1311, "seek": 579126, "start": 5808.58, "end": 5810.38, "text": " and different data, but it's nearly exactly the same.", "tokens": [293, 819, 1412, 11, 457, 309, 311, 6217, 2293, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.21131548750291176, "compression_ratio": 1.5793650793650793, "no_speech_prob": 8.664206688990816e-06}, {"id": 1312, "seek": 579126, "start": 5811.42, "end": 5816.860000000001, "text": " But I really like rewriting stuff,", "tokens": [583, 286, 534, 411, 319, 19868, 1507, 11], "temperature": 0.0, "avg_logprob": -0.21131548750291176, "compression_ratio": 1.5793650793650793, "no_speech_prob": 8.664206688990816e-06}, {"id": 1313, "seek": 579126, "start": 5816.860000000001, "end": 5820.46, "text": " because when I rewrite stuff, it lets me kind of look and see what I've written.", "tokens": [570, 562, 286, 28132, 1507, 11, 309, 6653, 385, 733, 295, 574, 293, 536, 437, 286, 600, 3720, 13], "temperature": 0.0, "avg_logprob": -0.21131548750291176, "compression_ratio": 1.5793650793650793, "no_speech_prob": 8.664206688990816e-06}, {"id": 1314, "seek": 582046, "start": 5820.46, "end": 5829.06, "text": " And when I looked back at this, I saw cb, cb, cb, cb, cb.", "tokens": [400, 562, 286, 2956, 646, 412, 341, 11, 286, 1866, 269, 65, 11, 269, 65, 11, 269, 65, 11, 269, 65, 11, 269, 65, 13], "temperature": 0.0, "avg_logprob": -0.26469921421360326, "compression_ratio": 1.5, "no_speech_prob": 8.80073548614746e-06}, {"id": 1315, "seek": 582046, "start": 5832.9, "end": 5837.26, "text": " There's this object, the cb is the callback handler,", "tokens": [821, 311, 341, 2657, 11, 264, 269, 65, 307, 264, 818, 3207, 41967, 11], "temperature": 0.0, "avg_logprob": -0.26469921421360326, "compression_ratio": 1.5, "no_speech_prob": 8.80073548614746e-06}, {"id": 1316, "seek": 582046, "start": 5837.26, "end": 5838.46, "text": " that's being passed everywhere.", "tokens": [300, 311, 885, 4678, 5315, 13], "temperature": 0.0, "avg_logprob": -0.26469921421360326, "compression_ratio": 1.5, "no_speech_prob": 8.80073548614746e-06}, {"id": 1317, "seek": 582046, "start": 5839.7, "end": 5841.46, "text": " And that's a code smell.", "tokens": [400, 300, 311, 257, 3089, 4316, 13], "temperature": 0.0, "avg_logprob": -0.26469921421360326, "compression_ratio": 1.5, "no_speech_prob": 8.80073548614746e-06}, {"id": 1318, "seek": 582046, "start": 5841.46, "end": 5845.54, "text": " That code smell says something should have that state.", "tokens": [663, 3089, 4316, 1619, 746, 820, 362, 300, 1785, 13], "temperature": 0.0, "avg_logprob": -0.26469921421360326, "compression_ratio": 1.5, "no_speech_prob": 8.80073548614746e-06}, {"id": 1319, "seek": 584554, "start": 5845.54, "end": 5850.7, "text": " And specifically, these three functions should be", "tokens": [400, 4682, 11, 613, 1045, 6828, 820, 312], "temperature": 0.0, "avg_logprob": -0.19540708089612194, "compression_ratio": 1.5650224215246638, "no_speech_prob": 4.495017947192537e-06}, {"id": 1320, "seek": 584554, "start": 5850.7, "end": 5854.74, "text": " the methods of something that has this state.", "tokens": [264, 7150, 295, 746, 300, 575, 341, 1785, 13], "temperature": 0.0, "avg_logprob": -0.19540708089612194, "compression_ratio": 1.5650224215246638, "no_speech_prob": 4.495017947192537e-06}, {"id": 1321, "seek": 584554, "start": 5854.74, "end": 5859.14, "text": " So after I kind of wrote this part of the lesson, I suddenly realized,", "tokens": [407, 934, 286, 733, 295, 4114, 341, 644, 295, 264, 6898, 11, 286, 5800, 5334, 11], "temperature": 0.0, "avg_logprob": -0.19540708089612194, "compression_ratio": 1.5650224215246638, "no_speech_prob": 4.495017947192537e-06}, {"id": 1322, "seek": 584554, "start": 5859.14, "end": 5861.3, "text": " fast AI is doing it the dumb way.", "tokens": [2370, 7318, 307, 884, 309, 264, 10316, 636, 13], "temperature": 0.0, "avg_logprob": -0.19540708089612194, "compression_ratio": 1.5650224215246638, "no_speech_prob": 4.495017947192537e-06}, {"id": 1323, "seek": 584554, "start": 5861.3, "end": 5862.54, "text": " So let's fix it.", "tokens": [407, 718, 311, 3191, 309, 13], "temperature": 0.0, "avg_logprob": -0.19540708089612194, "compression_ratio": 1.5650224215246638, "no_speech_prob": 4.495017947192537e-06}, {"id": 1324, "seek": 584554, "start": 5862.54, "end": 5867.7, "text": " So, and this is likely to appear in a future version of fast AI.", "tokens": [407, 11, 293, 341, 307, 3700, 281, 4204, 294, 257, 2027, 3037, 295, 2370, 7318, 13], "temperature": 0.0, "avg_logprob": -0.19540708089612194, "compression_ratio": 1.5650224215246638, "no_speech_prob": 4.495017947192537e-06}, {"id": 1325, "seek": 584554, "start": 5867.7, "end": 5870.0199999999995, "text": " I created a new thing called runner.", "tokens": [286, 2942, 257, 777, 551, 1219, 24376, 13], "temperature": 0.0, "avg_logprob": -0.19540708089612194, "compression_ratio": 1.5650224215246638, "no_speech_prob": 4.495017947192537e-06}, {"id": 1326, "seek": 587002, "start": 5870.02, "end": 5879.9800000000005, "text": " And so runner is a new class that contains the three things I just said.", "tokens": [400, 370, 24376, 307, 257, 777, 1508, 300, 8306, 264, 1045, 721, 286, 445, 848, 13], "temperature": 0.0, "avg_logprob": -0.27129075106452494, "compression_ratio": 1.3875, "no_speech_prob": 6.747819952579448e-06}, {"id": 1327, "seek": 587002, "start": 5879.9800000000005, "end": 5883.740000000001, "text": " One batch, all batches, and fit.", "tokens": [1485, 15245, 11, 439, 15245, 279, 11, 293, 3318, 13], "temperature": 0.0, "avg_logprob": -0.27129075106452494, "compression_ratio": 1.3875, "no_speech_prob": 6.747819952579448e-06}, {"id": 1328, "seek": 587002, "start": 5883.740000000001, "end": 5893.700000000001, "text": " And the runner, so here's fit, right?", "tokens": [400, 264, 24376, 11, 370, 510, 311, 3318, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.27129075106452494, "compression_ratio": 1.3875, "no_speech_prob": 6.747819952579448e-06}, {"id": 1329, "seek": 587002, "start": 5893.700000000001, "end": 5895.740000000001, "text": " It's incredibly simple.", "tokens": [467, 311, 6252, 2199, 13], "temperature": 0.0, "avg_logprob": -0.27129075106452494, "compression_ratio": 1.3875, "no_speech_prob": 6.747819952579448e-06}, {"id": 1330, "seek": 587002, "start": 5895.740000000001, "end": 5898.5, "text": " We're gonna keep track of how many epochs we're doing.", "tokens": [492, 434, 799, 1066, 2837, 295, 577, 867, 30992, 28346, 321, 434, 884, 13], "temperature": 0.0, "avg_logprob": -0.27129075106452494, "compression_ratio": 1.3875, "no_speech_prob": 6.747819952579448e-06}, {"id": 1331, "seek": 589850, "start": 5898.5, "end": 5901.58, "text": " We're gonna keep track of the learner that we're running.", "tokens": [492, 434, 799, 1066, 2837, 295, 264, 33347, 300, 321, 434, 2614, 13], "temperature": 0.0, "avg_logprob": -0.15587092254121424, "compression_ratio": 1.95260663507109, "no_speech_prob": 6.643183951382525e-06}, {"id": 1332, "seek": 589850, "start": 5901.58, "end": 5903.74, "text": " And remember, the learner has no logic in it,", "tokens": [400, 1604, 11, 264, 33347, 575, 572, 9952, 294, 309, 11], "temperature": 0.0, "avg_logprob": -0.15587092254121424, "compression_ratio": 1.95260663507109, "no_speech_prob": 6.643183951382525e-06}, {"id": 1333, "seek": 589850, "start": 5903.74, "end": 5905.94, "text": " the stores for things, okay?", "tokens": [264, 9512, 337, 721, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.15587092254121424, "compression_ratio": 1.95260663507109, "no_speech_prob": 6.643183951382525e-06}, {"id": 1334, "seek": 589850, "start": 5907.1, "end": 5913.7, "text": " And then we tell each of our callbacks what runner", "tokens": [400, 550, 321, 980, 1184, 295, 527, 818, 17758, 437, 24376], "temperature": 0.0, "avg_logprob": -0.15587092254121424, "compression_ratio": 1.95260663507109, "no_speech_prob": 6.643183951382525e-06}, {"id": 1335, "seek": 589850, "start": 5913.7, "end": 5917.46, "text": " they're currently working with, and then we call begin fit.", "tokens": [436, 434, 4362, 1364, 365, 11, 293, 550, 321, 818, 1841, 3318, 13], "temperature": 0.0, "avg_logprob": -0.15587092254121424, "compression_ratio": 1.95260663507109, "no_speech_prob": 6.643183951382525e-06}, {"id": 1336, "seek": 589850, "start": 5917.46, "end": 5921.94, "text": " And then we go through each epoch, set the epoch, we call begin epoch,", "tokens": [400, 550, 321, 352, 807, 1184, 30992, 339, 11, 992, 264, 30992, 339, 11, 321, 818, 1841, 30992, 339, 11], "temperature": 0.0, "avg_logprob": -0.15587092254121424, "compression_ratio": 1.95260663507109, "no_speech_prob": 6.643183951382525e-06}, {"id": 1337, "seek": 589850, "start": 5921.94, "end": 5923.66, "text": " we call all batches.", "tokens": [321, 818, 439, 15245, 279, 13], "temperature": 0.0, "avg_logprob": -0.15587092254121424, "compression_ratio": 1.95260663507109, "no_speech_prob": 6.643183951382525e-06}, {"id": 1338, "seek": 589850, "start": 5923.66, "end": 5928.3, "text": " And then with no grad, we call begin validate, and then we call all batches.", "tokens": [400, 550, 365, 572, 2771, 11, 321, 818, 1841, 29562, 11, 293, 550, 321, 818, 439, 15245, 279, 13], "temperature": 0.0, "avg_logprob": -0.15587092254121424, "compression_ratio": 1.95260663507109, "no_speech_prob": 6.643183951382525e-06}, {"id": 1339, "seek": 592830, "start": 5928.3, "end": 5931.26, "text": " And then we call after epoch, and then we call after fit.", "tokens": [400, 550, 321, 818, 934, 30992, 339, 11, 293, 550, 321, 818, 934, 3318, 13], "temperature": 0.0, "avg_logprob": -0.13881898537660256, "compression_ratio": 1.9025641025641025, "no_speech_prob": 3.1875454169494333e-06}, {"id": 1340, "seek": 592830, "start": 5932.42, "end": 5933.54, "text": " That's it.", "tokens": [663, 311, 309, 13], "temperature": 0.0, "avg_logprob": -0.13881898537660256, "compression_ratio": 1.9025641025641025, "no_speech_prob": 3.1875454169494333e-06}, {"id": 1341, "seek": 592830, "start": 5933.54, "end": 5940.18, "text": " Now, this self string might look a bit weird, but", "tokens": [823, 11, 341, 2698, 6798, 1062, 574, 257, 857, 3657, 11, 457], "temperature": 0.0, "avg_logprob": -0.13881898537660256, "compression_ratio": 1.9025641025641025, "no_speech_prob": 3.1875454169494333e-06}, {"id": 1342, "seek": 592830, "start": 5940.18, "end": 5942.1, "text": " look at what we had before.", "tokens": [574, 412, 437, 321, 632, 949, 13], "temperature": 0.0, "avg_logprob": -0.13881898537660256, "compression_ratio": 1.9025641025641025, "no_speech_prob": 3.1875454169494333e-06}, {"id": 1343, "seek": 592830, "start": 5942.1, "end": 5947.5, "text": " Again, horrible code smell is lots of duplicate code.", "tokens": [3764, 11, 9263, 3089, 4316, 307, 3195, 295, 23976, 3089, 13], "temperature": 0.0, "avg_logprob": -0.13881898537660256, "compression_ratio": 1.9025641025641025, "no_speech_prob": 3.1875454169494333e-06}, {"id": 1344, "seek": 592830, "start": 5947.5, "end": 5950.9800000000005, "text": " Res equals true for callback, blah, blah, blah, blah, blah.", "tokens": [5015, 6915, 2074, 337, 818, 3207, 11, 12288, 11, 12288, 11, 12288, 11, 12288, 11, 12288, 13], "temperature": 0.0, "avg_logprob": -0.13881898537660256, "compression_ratio": 1.9025641025641025, "no_speech_prob": 3.1875454169494333e-06}, {"id": 1345, "seek": 592830, "start": 5950.9800000000005, "end": 5952.38, "text": " Begin epoch.", "tokens": [20660, 30992, 339, 13], "temperature": 0.0, "avg_logprob": -0.13881898537660256, "compression_ratio": 1.9025641025641025, "no_speech_prob": 3.1875454169494333e-06}, {"id": 1346, "seek": 592830, "start": 5952.38, "end": 5955.860000000001, "text": " Res equals true for callback, blah, blah, blah, blah, blah, begin validate.", "tokens": [5015, 6915, 2074, 337, 818, 3207, 11, 12288, 11, 12288, 11, 12288, 11, 12288, 11, 12288, 11, 1841, 29562, 13], "temperature": 0.0, "avg_logprob": -0.13881898537660256, "compression_ratio": 1.9025641025641025, "no_speech_prob": 3.1875454169494333e-06}, {"id": 1347, "seek": 592830, "start": 5955.860000000001, "end": 5957.78, "text": " So that's bad, right?", "tokens": [407, 300, 311, 1578, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.13881898537660256, "compression_ratio": 1.9025641025641025, "no_speech_prob": 3.1875454169494333e-06}, {"id": 1348, "seek": 595778, "start": 5957.78, "end": 5963.7, "text": " Code duplication means cognitive overhead to understand what's going on.", "tokens": [15549, 17154, 399, 1355, 15605, 19922, 281, 1223, 437, 311, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.14682372411092123, "compression_ratio": 1.675, "no_speech_prob": 1.8341963368584402e-05}, {"id": 1349, "seek": 595778, "start": 5963.7, "end": 5967.54, "text": " Lots of opportunities to accidentally have one or instead of an and.", "tokens": [15908, 295, 4786, 281, 15715, 362, 472, 420, 2602, 295, 364, 293, 13], "temperature": 0.0, "avg_logprob": -0.14682372411092123, "compression_ratio": 1.675, "no_speech_prob": 1.8341963368584402e-05}, {"id": 1350, "seek": 595778, "start": 5967.54, "end": 5970.74, "text": " Lots of places you have to change if you need to edit something.", "tokens": [15908, 295, 3190, 291, 362, 281, 1319, 498, 291, 643, 281, 8129, 746, 13], "temperature": 0.0, "avg_logprob": -0.14682372411092123, "compression_ratio": 1.675, "no_speech_prob": 1.8341963368584402e-05}, {"id": 1351, "seek": 595778, "start": 5971.74, "end": 5978.42, "text": " So basically I took that out and I factored it out into done to call.", "tokens": [407, 1936, 286, 1890, 300, 484, 293, 286, 1186, 2769, 309, 484, 666, 1096, 281, 818, 13], "temperature": 0.0, "avg_logprob": -0.14682372411092123, "compression_ratio": 1.675, "no_speech_prob": 1.8341963368584402e-05}, {"id": 1352, "seek": 595778, "start": 5978.42, "end": 5982.9, "text": " So done to call is the thing that we've seen it before.", "tokens": [407, 1096, 281, 818, 307, 264, 551, 300, 321, 600, 1612, 309, 949, 13], "temperature": 0.0, "avg_logprob": -0.14682372411092123, "compression_ratio": 1.675, "no_speech_prob": 1.8341963368584402e-05}, {"id": 1353, "seek": 595778, "start": 5982.9, "end": 5986.179999999999, "text": " It's the thing that lets you treat an object as if it was a function.", "tokens": [467, 311, 264, 551, 300, 6653, 291, 2387, 364, 2657, 382, 498, 309, 390, 257, 2445, 13], "temperature": 0.0, "avg_logprob": -0.14682372411092123, "compression_ratio": 1.675, "no_speech_prob": 1.8341963368584402e-05}, {"id": 1354, "seek": 598618, "start": 5986.18, "end": 5989.34, "text": " So I could have called this lots of things.", "tokens": [407, 286, 727, 362, 1219, 341, 3195, 295, 721, 13], "temperature": 0.0, "avg_logprob": -0.2354282684326172, "compression_ratio": 1.7286821705426356, "no_speech_prob": 8.397383680858184e-06}, {"id": 1355, "seek": 598618, "start": 5989.34, "end": 5994.54, "text": " I could have called it self.runCallback or whatever, right?", "tokens": [286, 727, 362, 1219, 309, 2698, 13, 12997, 46113, 3207, 420, 2035, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2354282684326172, "compression_ratio": 1.7286821705426356, "no_speech_prob": 8.397383680858184e-06}, {"id": 1356, "seek": 598618, "start": 5994.54, "end": 5996.820000000001, "text": " But it's the thing that happens absolutely everywhere.", "tokens": [583, 309, 311, 264, 551, 300, 2314, 3122, 5315, 13], "temperature": 0.0, "avg_logprob": -0.2354282684326172, "compression_ratio": 1.7286821705426356, "no_speech_prob": 8.397383680858184e-06}, {"id": 1357, "seek": 598618, "start": 5996.820000000001, "end": 6000.9400000000005, "text": " And so my kind of rule of thumb is if you do something lots of times,", "tokens": [400, 370, 452, 733, 295, 4978, 295, 9298, 307, 498, 291, 360, 746, 3195, 295, 1413, 11], "temperature": 0.0, "avg_logprob": -0.2354282684326172, "compression_ratio": 1.7286821705426356, "no_speech_prob": 8.397383680858184e-06}, {"id": 1358, "seek": 598618, "start": 6000.9400000000005, "end": 6002.22, "text": " make it small.", "tokens": [652, 309, 1359, 13], "temperature": 0.0, "avg_logprob": -0.2354282684326172, "compression_ratio": 1.7286821705426356, "no_speech_prob": 8.397383680858184e-06}, {"id": 1359, "seek": 598618, "start": 6002.22, "end": 6006.22, "text": " So done to call is the smallest possible way you can call something.", "tokens": [407, 1096, 281, 818, 307, 264, 16998, 1944, 636, 291, 393, 818, 746, 13], "temperature": 0.0, "avg_logprob": -0.2354282684326172, "compression_ratio": 1.7286821705426356, "no_speech_prob": 8.397383680858184e-06}, {"id": 1360, "seek": 598618, "start": 6006.22, "end": 6009.3, "text": " You don't have to give it a name at all when you call it.", "tokens": [509, 500, 380, 362, 281, 976, 309, 257, 1315, 412, 439, 562, 291, 818, 309, 13], "temperature": 0.0, "avg_logprob": -0.2354282684326172, "compression_ratio": 1.7286821705426356, "no_speech_prob": 8.397383680858184e-06}, {"id": 1361, "seek": 598618, "start": 6009.3, "end": 6013.26, "text": " So we say call the callback called after epoch.", "tokens": [407, 321, 584, 818, 264, 818, 3207, 1219, 934, 30992, 339, 13], "temperature": 0.0, "avg_logprob": -0.2354282684326172, "compression_ratio": 1.7286821705426356, "no_speech_prob": 8.397383680858184e-06}, {"id": 1362, "seek": 598618, "start": 6013.26, "end": 6014.5, "text": " It also makes sense, right?", "tokens": [467, 611, 1669, 2020, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2354282684326172, "compression_ratio": 1.7286821705426356, "no_speech_prob": 8.397383680858184e-06}, {"id": 1363, "seek": 601450, "start": 6014.5, "end": 6019.82, "text": " We're calling a callback, so why not use done to call to call a callback?", "tokens": [492, 434, 5141, 257, 818, 3207, 11, 370, 983, 406, 764, 1096, 281, 818, 281, 818, 257, 818, 3207, 30], "temperature": 0.0, "avg_logprob": -0.16995722576252464, "compression_ratio": 1.6561085972850678, "no_speech_prob": 5.255219093669439e-06}, {"id": 1364, "seek": 601450, "start": 6019.82, "end": 6023.9, "text": " So after epoch, I gotta go through all of my callbacks.", "tokens": [407, 934, 30992, 339, 11, 286, 3428, 352, 807, 439, 295, 452, 818, 17758, 13], "temperature": 0.0, "avg_logprob": -0.16995722576252464, "compression_ratio": 1.6561085972850678, "no_speech_prob": 5.255219093669439e-06}, {"id": 1365, "seek": 601450, "start": 6025.9, "end": 6028.26, "text": " I'll talk about this sorted in a moment.", "tokens": [286, 603, 751, 466, 341, 25462, 294, 257, 1623, 13], "temperature": 0.0, "avg_logprob": -0.16995722576252464, "compression_ratio": 1.6561085972850678, "no_speech_prob": 5.255219093669439e-06}, {"id": 1366, "seek": 601450, "start": 6028.26, "end": 6032.98, "text": " And then the other thing I didn't like before is that all of my callbacks", "tokens": [400, 550, 264, 661, 551, 286, 994, 380, 411, 949, 307, 300, 439, 295, 452, 818, 17758], "temperature": 0.0, "avg_logprob": -0.16995722576252464, "compression_ratio": 1.6561085972850678, "no_speech_prob": 5.255219093669439e-06}, {"id": 1367, "seek": 601450, "start": 6032.98, "end": 6036.86, "text": " had to inherit from this callback superclass.", "tokens": [632, 281, 21389, 490, 341, 818, 3207, 1687, 11665, 13], "temperature": 0.0, "avg_logprob": -0.16995722576252464, "compression_ratio": 1.6561085972850678, "no_speech_prob": 5.255219093669439e-06}, {"id": 1368, "seek": 601450, "start": 6036.86, "end": 6042.86, "text": " Cuz if they didn't, then they would have been missing one of these methods.", "tokens": [27017, 498, 436, 994, 380, 11, 550, 436, 576, 362, 668, 5361, 472, 295, 613, 7150, 13], "temperature": 0.0, "avg_logprob": -0.16995722576252464, "compression_ratio": 1.6561085972850678, "no_speech_prob": 5.255219093669439e-06}, {"id": 1369, "seek": 604286, "start": 6042.86, "end": 6045.259999999999, "text": " And so then when it tried to call the method,", "tokens": [400, 370, 550, 562, 309, 3031, 281, 818, 264, 3170, 11], "temperature": 0.0, "avg_logprob": -0.1678426530626085, "compression_ratio": 1.607843137254902, "no_speech_prob": 6.643211690970929e-06}, {"id": 1370, "seek": 604286, "start": 6045.259999999999, "end": 6046.7, "text": " there would have been an exception.", "tokens": [456, 576, 362, 668, 364, 11183, 13], "temperature": 0.0, "avg_logprob": -0.1678426530626085, "compression_ratio": 1.607843137254902, "no_speech_prob": 6.643211690970929e-06}, {"id": 1371, "seek": 604286, "start": 6046.7, "end": 6049.58, "text": " And I don't like forcing people to have to inherit from something.", "tokens": [400, 286, 500, 380, 411, 19030, 561, 281, 362, 281, 21389, 490, 746, 13], "temperature": 0.0, "avg_logprob": -0.1678426530626085, "compression_ratio": 1.607843137254902, "no_speech_prob": 6.643211690970929e-06}, {"id": 1372, "seek": 604286, "start": 6049.58, "end": 6051.7, "text": " They should be able to do whatever they like.", "tokens": [814, 820, 312, 1075, 281, 360, 2035, 436, 411, 13], "temperature": 0.0, "avg_logprob": -0.1678426530626085, "compression_ratio": 1.607843137254902, "no_speech_prob": 6.643211690970929e-06}, {"id": 1373, "seek": 604286, "start": 6051.7, "end": 6058.58, "text": " So what we did here was we used get attribute,", "tokens": [407, 437, 321, 630, 510, 390, 321, 1143, 483, 19667, 11], "temperature": 0.0, "avg_logprob": -0.1678426530626085, "compression_ratio": 1.607843137254902, "no_speech_prob": 6.643211690970929e-06}, {"id": 1374, "seek": 604286, "start": 6058.58, "end": 6062.98, "text": " which is the Python thing which says look inside this object and", "tokens": [597, 307, 264, 15329, 551, 597, 1619, 574, 1854, 341, 2657, 293], "temperature": 0.0, "avg_logprob": -0.1678426530626085, "compression_ratio": 1.607843137254902, "no_speech_prob": 6.643211690970929e-06}, {"id": 1375, "seek": 604286, "start": 6062.98, "end": 6067.5, "text": " try to find something of this name, eg begin validate.", "tokens": [853, 281, 915, 746, 295, 341, 1315, 11, 24263, 1841, 29562, 13], "temperature": 0.0, "avg_logprob": -0.1678426530626085, "compression_ratio": 1.607843137254902, "no_speech_prob": 6.643211690970929e-06}, {"id": 1376, "seek": 604286, "start": 6067.5, "end": 6070.66, "text": " And default to none if you can't find it, right?", "tokens": [400, 7576, 281, 6022, 498, 291, 393, 380, 915, 309, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1678426530626085, "compression_ratio": 1.607843137254902, "no_speech_prob": 6.643211690970929e-06}, {"id": 1377, "seek": 607066, "start": 6070.66, "end": 6073.54, "text": " So it tries to find that callback and", "tokens": [407, 309, 9898, 281, 915, 300, 818, 3207, 293], "temperature": 0.0, "avg_logprob": -0.13075693527070603, "compression_ratio": 1.702020202020202, "no_speech_prob": 7.888889740570448e-06}, {"id": 1378, "seek": 607066, "start": 6073.54, "end": 6076.66, "text": " there'll be none if the callback doesn't exist.", "tokens": [456, 603, 312, 6022, 498, 264, 818, 3207, 1177, 380, 2514, 13], "temperature": 0.0, "avg_logprob": -0.13075693527070603, "compression_ratio": 1.702020202020202, "no_speech_prob": 7.888889740570448e-06}, {"id": 1379, "seek": 607066, "start": 6076.66, "end": 6081.0199999999995, "text": " And if you find it, then you can call it, right?", "tokens": [400, 498, 291, 915, 309, 11, 550, 291, 393, 818, 309, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.13075693527070603, "compression_ratio": 1.702020202020202, "no_speech_prob": 7.888889740570448e-06}, {"id": 1380, "seek": 607066, "start": 6081.0199999999995, "end": 6085.78, "text": " So this is a nice way to call any callback.", "tokens": [407, 341, 307, 257, 1481, 636, 281, 818, 604, 818, 3207, 13], "temperature": 0.0, "avg_logprob": -0.13075693527070603, "compression_ratio": 1.702020202020202, "no_speech_prob": 7.888889740570448e-06}, {"id": 1381, "seek": 607066, "start": 6085.78, "end": 6089.18, "text": " But when you implement a callback, as you can see,", "tokens": [583, 562, 291, 4445, 257, 818, 3207, 11, 382, 291, 393, 536, 11], "temperature": 0.0, "avg_logprob": -0.13075693527070603, "compression_ratio": 1.702020202020202, "no_speech_prob": 7.888889740570448e-06}, {"id": 1382, "seek": 607066, "start": 6089.18, "end": 6092.0199999999995, "text": " look how much easier our test callback is now, right?", "tokens": [574, 577, 709, 3571, 527, 1500, 818, 3207, 307, 586, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.13075693527070603, "compression_ratio": 1.702020202020202, "no_speech_prob": 7.888889740570448e-06}, {"id": 1383, "seek": 607066, "start": 6092.0199999999995, "end": 6095.66, "text": " It's just super simple, just implement what you need.", "tokens": [467, 311, 445, 1687, 2199, 11, 445, 4445, 437, 291, 643, 13], "temperature": 0.0, "avg_logprob": -0.13075693527070603, "compression_ratio": 1.702020202020202, "no_speech_prob": 7.888889740570448e-06}, {"id": 1384, "seek": 609566, "start": 6095.66, "end": 6100.9, "text": " And we inherit from a new callback class, but", "tokens": [400, 321, 21389, 490, 257, 777, 818, 3207, 1508, 11, 457], "temperature": 0.0, "avg_logprob": -0.20887783299321713, "compression_ratio": 1.5529953917050692, "no_speech_prob": 4.1572720874683e-06}, {"id": 1385, "seek": 609566, "start": 6100.9, "end": 6102.86, "text": " we don't have to anymore, right?", "tokens": [321, 500, 380, 362, 281, 3602, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.20887783299321713, "compression_ratio": 1.5529953917050692, "no_speech_prob": 4.1572720874683e-06}, {"id": 1386, "seek": 609566, "start": 6102.86, "end": 6107.94, "text": " The main reason why is that our callback class now has an underscore order,", "tokens": [440, 2135, 1778, 983, 307, 300, 527, 818, 3207, 1508, 586, 575, 364, 37556, 1668, 11], "temperature": 0.0, "avg_logprob": -0.20887783299321713, "compression_ratio": 1.5529953917050692, "no_speech_prob": 4.1572720874683e-06}, {"id": 1387, "seek": 609566, "start": 6107.94, "end": 6111.9, "text": " which we can use to choose what order callbacks run in.", "tokens": [597, 321, 393, 764, 281, 2826, 437, 1668, 818, 17758, 1190, 294, 13], "temperature": 0.0, "avg_logprob": -0.20887783299321713, "compression_ratio": 1.5529953917050692, "no_speech_prob": 4.1572720874683e-06}, {"id": 1388, "seek": 609566, "start": 6111.9, "end": 6114.3, "text": " We'll talk about that after we handle this question.", "tokens": [492, 603, 751, 466, 300, 934, 321, 4813, 341, 1168, 13], "temperature": 0.0, "avg_logprob": -0.20887783299321713, "compression_ratio": 1.5529953917050692, "no_speech_prob": 4.1572720874683e-06}, {"id": 1389, "seek": 609566, "start": 6117.78, "end": 6122.3, "text": " What is the difference between hooks and PyTorch and callbacks in FastAI?", "tokens": [708, 307, 264, 2649, 1296, 26485, 293, 9953, 51, 284, 339, 293, 818, 17758, 294, 15968, 48698, 30], "temperature": 0.0, "avg_logprob": -0.20887783299321713, "compression_ratio": 1.5529953917050692, "no_speech_prob": 4.1572720874683e-06}, {"id": 1390, "seek": 612230, "start": 6122.3, "end": 6126.3, "text": " We're gonna do hooks very shortly.", "tokens": [492, 434, 799, 360, 26485, 588, 13392, 13], "temperature": 0.0, "avg_logprob": -0.19674509221857245, "compression_ratio": 1.511111111111111, "no_speech_prob": 1.1300309779471718e-05}, {"id": 1391, "seek": 612230, "start": 6128.74, "end": 6136.38, "text": " But if you think about it, if I want to kind of add a callback", "tokens": [583, 498, 291, 519, 466, 309, 11, 498, 286, 528, 281, 733, 295, 909, 257, 818, 3207], "temperature": 0.0, "avg_logprob": -0.19674509221857245, "compression_ratio": 1.511111111111111, "no_speech_prob": 1.1300309779471718e-05}, {"id": 1392, "seek": 612230, "start": 6136.38, "end": 6142.54, "text": " after I calculate the forward pass of the second layer of my model,", "tokens": [934, 286, 8873, 264, 2128, 1320, 295, 264, 1150, 4583, 295, 452, 2316, 11], "temperature": 0.0, "avg_logprob": -0.19674509221857245, "compression_ratio": 1.511111111111111, "no_speech_prob": 1.1300309779471718e-05}, {"id": 1393, "seek": 612230, "start": 6142.54, "end": 6145.18, "text": " there's no way for me to do that, right?", "tokens": [456, 311, 572, 636, 337, 385, 281, 360, 300, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.19674509221857245, "compression_ratio": 1.511111111111111, "no_speech_prob": 1.1300309779471718e-05}, {"id": 1394, "seek": 612230, "start": 6145.18, "end": 6150.38, "text": " Because the point at which I do the forward pass looks like this.", "tokens": [1436, 264, 935, 412, 597, 286, 360, 264, 2128, 1320, 1542, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.19674509221857245, "compression_ratio": 1.511111111111111, "no_speech_prob": 1.1300309779471718e-05}, {"id": 1395, "seek": 615038, "start": 6150.38, "end": 6153.62, "text": " Self.model, right?", "tokens": [16348, 13, 8014, 338, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2232771801347492, "compression_ratio": 1.5418326693227091, "no_speech_prob": 1.8342112525715493e-05}, {"id": 1396, "seek": 615038, "start": 6153.62, "end": 6158.06, "text": " Or if I want to hook into the point at which I've just called the backward pass", "tokens": [1610, 498, 286, 528, 281, 6328, 666, 264, 935, 412, 597, 286, 600, 445, 1219, 264, 23897, 1320], "temperature": 0.0, "avg_logprob": -0.2232771801347492, "compression_ratio": 1.5418326693227091, "no_speech_prob": 1.8342112525715493e-05}, {"id": 1397, "seek": 615038, "start": 6158.06, "end": 6161.1, "text": " of my penultimate layer, I can't do that either,", "tokens": [295, 452, 3435, 723, 2905, 4583, 11, 286, 393, 380, 360, 300, 2139, 11], "temperature": 0.0, "avg_logprob": -0.2232771801347492, "compression_ratio": 1.5418326693227091, "no_speech_prob": 1.8342112525715493e-05}, {"id": 1398, "seek": 615038, "start": 6161.1, "end": 6164.54, "text": " because the whole thing appears here as self.lost or backward, okay?", "tokens": [570, 264, 1379, 551, 7038, 510, 382, 2698, 13, 75, 555, 420, 23897, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.2232771801347492, "compression_ratio": 1.5418326693227091, "no_speech_prob": 1.8342112525715493e-05}, {"id": 1399, "seek": 615038, "start": 6164.54, "end": 6172.3, "text": " So, PyTorch hooks are callbacks that you can add to specific PyTorch modules.", "tokens": [407, 11, 9953, 51, 284, 339, 26485, 366, 818, 17758, 300, 291, 393, 909, 281, 2685, 9953, 51, 284, 339, 16679, 13], "temperature": 0.0, "avg_logprob": -0.2232771801347492, "compression_ratio": 1.5418326693227091, "no_speech_prob": 1.8342112525715493e-05}, {"id": 1400, "seek": 615038, "start": 6173.66, "end": 6175.9800000000005, "text": " And we're gonna see them in very shortly.", "tokens": [400, 321, 434, 799, 536, 552, 294, 588, 13392, 13], "temperature": 0.0, "avg_logprob": -0.2232771801347492, "compression_ratio": 1.5418326693227091, "no_speech_prob": 1.8342112525715493e-05}, {"id": 1401, "seek": 615038, "start": 6177.3, "end": 6178.62, "text": " Well, it might be next class.", "tokens": [1042, 11, 309, 1062, 312, 958, 1508, 13], "temperature": 0.0, "avg_logprob": -0.2232771801347492, "compression_ratio": 1.5418326693227091, "no_speech_prob": 1.8342112525715493e-05}, {"id": 1402, "seek": 617862, "start": 6178.62, "end": 6181.9, "text": " We'll see how we go.", "tokens": [492, 603, 536, 577, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.20568242073059081, "compression_ratio": 1.48, "no_speech_prob": 1.3006499102630187e-05}, {"id": 1403, "seek": 617862, "start": 6181.9, "end": 6182.9, "text": " Okay, so.", "tokens": [1033, 11, 370, 13], "temperature": 0.0, "avg_logprob": -0.20568242073059081, "compression_ratio": 1.48, "no_speech_prob": 1.3006499102630187e-05}, {"id": 1404, "seek": 617862, "start": 6186.58, "end": 6191.34, "text": " Very often you wanna be able to inject behavior into something,", "tokens": [4372, 2049, 291, 1948, 312, 1075, 281, 10711, 5223, 666, 746, 11], "temperature": 0.0, "avg_logprob": -0.20568242073059081, "compression_ratio": 1.48, "no_speech_prob": 1.3006499102630187e-05}, {"id": 1405, "seek": 617862, "start": 6191.34, "end": 6196.0199999999995, "text": " but the different things can influence each other.", "tokens": [457, 264, 819, 721, 393, 6503, 1184, 661, 13], "temperature": 0.0, "avg_logprob": -0.20568242073059081, "compression_ratio": 1.48, "no_speech_prob": 1.3006499102630187e-05}, {"id": 1406, "seek": 617862, "start": 6196.0199999999995, "end": 6197.94, "text": " For example, transformations,", "tokens": [1171, 1365, 11, 34852, 11], "temperature": 0.0, "avg_logprob": -0.20568242073059081, "compression_ratio": 1.48, "no_speech_prob": 1.3006499102630187e-05}, {"id": 1407, "seek": 617862, "start": 6197.94, "end": 6200.9, "text": " we're gonna be seeing this when we do data augmentation.", "tokens": [321, 434, 799, 312, 2577, 341, 562, 321, 360, 1412, 14501, 19631, 13], "temperature": 0.0, "avg_logprob": -0.20568242073059081, "compression_ratio": 1.48, "no_speech_prob": 1.3006499102630187e-05}, {"id": 1408, "seek": 617862, "start": 6200.9, "end": 6204.58, "text": " So quite often you'll need things to run in a particular order.", "tokens": [407, 1596, 2049, 291, 603, 643, 721, 281, 1190, 294, 257, 1729, 1668, 13], "temperature": 0.0, "avg_logprob": -0.20568242073059081, "compression_ratio": 1.48, "no_speech_prob": 1.3006499102630187e-05}, {"id": 1409, "seek": 620458, "start": 6204.58, "end": 6208.7, "text": " So when I add this kind of injectable behavior like callbacks,", "tokens": [407, 562, 286, 909, 341, 733, 295, 10711, 712, 5223, 411, 818, 17758, 11], "temperature": 0.0, "avg_logprob": -0.2111082973643246, "compression_ratio": 1.68, "no_speech_prob": 2.0461440726649016e-05}, {"id": 1410, "seek": 620458, "start": 6208.7, "end": 6211.94, "text": " I like to just add something, which is what order should it run in?", "tokens": [286, 411, 281, 445, 909, 746, 11, 597, 307, 437, 1668, 820, 309, 1190, 294, 30], "temperature": 0.0, "avg_logprob": -0.2111082973643246, "compression_ratio": 1.68, "no_speech_prob": 2.0461440726649016e-05}, {"id": 1411, "seek": 620458, "start": 6214.42, "end": 6216.1, "text": " You don't have to put this here.", "tokens": [509, 500, 380, 362, 281, 829, 341, 510, 13], "temperature": 0.0, "avg_logprob": -0.2111082973643246, "compression_ratio": 1.68, "no_speech_prob": 2.0461440726649016e-05}, {"id": 1412, "seek": 620458, "start": 6216.1, "end": 6220.98, "text": " You might have noticed that what I do when I call this is I,", "tokens": [509, 1062, 362, 5694, 300, 437, 286, 360, 562, 286, 818, 341, 307, 286, 11], "temperature": 0.0, "avg_logprob": -0.2111082973643246, "compression_ratio": 1.68, "no_speech_prob": 2.0461440726649016e-05}, {"id": 1413, "seek": 620458, "start": 6220.98, "end": 6223.78, "text": " oh, this currently does, sorry.", "tokens": [1954, 11, 341, 4362, 775, 11, 2597, 13], "temperature": 0.0, "avg_logprob": -0.2111082973643246, "compression_ratio": 1.68, "no_speech_prob": 2.0461440726649016e-05}, {"id": 1414, "seek": 620458, "start": 6223.78, "end": 6226.86, "text": " Actually, when we look at transformations, it won't require order.", "tokens": [5135, 11, 562, 321, 574, 412, 34852, 11, 309, 1582, 380, 3651, 1668, 13], "temperature": 0.0, "avg_logprob": -0.2111082973643246, "compression_ratio": 1.68, "no_speech_prob": 2.0461440726649016e-05}, {"id": 1415, "seek": 620458, "start": 6226.86, "end": 6228.1, "text": " This one does require an order.", "tokens": [639, 472, 775, 3651, 364, 1668, 13], "temperature": 0.0, "avg_logprob": -0.2111082973643246, "compression_ratio": 1.68, "no_speech_prob": 2.0461440726649016e-05}, {"id": 1416, "seek": 620458, "start": 6228.1, "end": 6232.82, "text": " So, yeah, okay, so your callbacks need to be something that have", "tokens": [407, 11, 1338, 11, 1392, 11, 370, 428, 818, 17758, 643, 281, 312, 746, 300, 362], "temperature": 0.0, "avg_logprob": -0.2111082973643246, "compression_ratio": 1.68, "no_speech_prob": 2.0461440726649016e-05}, {"id": 1417, "seek": 623282, "start": 6232.82, "end": 6235.34, "text": " an underscore order attribute in them.", "tokens": [364, 37556, 1668, 19667, 294, 552, 13], "temperature": 0.0, "avg_logprob": -0.1756456456285842, "compression_ratio": 1.6401869158878504, "no_speech_prob": 4.356814315542579e-06}, {"id": 1418, "seek": 623282, "start": 6235.34, "end": 6240.78, "text": " And this way we can make sure that some things run after other things.", "tokens": [400, 341, 636, 321, 393, 652, 988, 300, 512, 721, 1190, 934, 661, 721, 13], "temperature": 0.0, "avg_logprob": -0.1756456456285842, "compression_ratio": 1.6401869158878504, "no_speech_prob": 4.356814315542579e-06}, {"id": 1419, "seek": 623282, "start": 6240.78, "end": 6245.98, "text": " So for example, you might have noticed that our runner in", "tokens": [407, 337, 1365, 11, 291, 1062, 362, 5694, 300, 527, 24376, 294], "temperature": 0.0, "avg_logprob": -0.1756456456285842, "compression_ratio": 1.6401869158878504, "no_speech_prob": 4.356814315542579e-06}, {"id": 1420, "seek": 623282, "start": 6245.98, "end": 6250.7, "text": " the fit function never calls model.eval, never calls model.train.", "tokens": [264, 3318, 2445, 1128, 5498, 2316, 13, 68, 3337, 11, 1128, 5498, 2316, 13, 83, 7146, 13], "temperature": 0.0, "avg_logprob": -0.1756456456285842, "compression_ratio": 1.6401869158878504, "no_speech_prob": 4.356814315542579e-06}, {"id": 1421, "seek": 623282, "start": 6250.7, "end": 6253.099999999999, "text": " So it literally doesn't do anything.", "tokens": [407, 309, 3736, 1177, 380, 360, 1340, 13], "temperature": 0.0, "avg_logprob": -0.1756456456285842, "compression_ratio": 1.6401869158878504, "no_speech_prob": 4.356814315542579e-06}, {"id": 1422, "seek": 623282, "start": 6253.099999999999, "end": 6256.5, "text": " It just says these are the steps I have to run and", "tokens": [467, 445, 1619, 613, 366, 264, 4439, 286, 362, 281, 1190, 293], "temperature": 0.0, "avg_logprob": -0.1756456456285842, "compression_ratio": 1.6401869158878504, "no_speech_prob": 4.356814315542579e-06}, {"id": 1423, "seek": 623282, "start": 6256.5, "end": 6258.54, "text": " the callbacks do the running.", "tokens": [264, 818, 17758, 360, 264, 2614, 13], "temperature": 0.0, "avg_logprob": -0.1756456456285842, "compression_ratio": 1.6401869158878504, "no_speech_prob": 4.356814315542579e-06}, {"id": 1424, "seek": 625854, "start": 6258.54, "end": 6264.26, "text": " So I created a train eval callback that at the beginning of an epoch,", "tokens": [407, 286, 2942, 257, 3847, 1073, 304, 818, 3207, 300, 412, 264, 2863, 295, 364, 30992, 339, 11], "temperature": 0.0, "avg_logprob": -0.26454075451554926, "compression_ratio": 1.7399103139013452, "no_speech_prob": 8.397896635869984e-06}, {"id": 1425, "seek": 625854, "start": 6264.26, "end": 6268.46, "text": " calls model.train and at the beginning of validation,", "tokens": [5498, 2316, 13, 83, 7146, 293, 412, 264, 2863, 295, 24071, 11], "temperature": 0.0, "avg_logprob": -0.26454075451554926, "compression_ratio": 1.7399103139013452, "no_speech_prob": 8.397896635869984e-06}, {"id": 1426, "seek": 625854, "start": 6268.46, "end": 6270.82, "text": " calls model.neval.", "tokens": [5498, 2316, 13, 716, 3337, 13], "temperature": 0.0, "avg_logprob": -0.26454075451554926, "compression_ratio": 1.7399103139013452, "no_speech_prob": 8.397896635869984e-06}, {"id": 1427, "seek": 625854, "start": 6270.82, "end": 6276.34, "text": " And I also added stuff to keep track of how many epochs has it done.", "tokens": [400, 286, 611, 3869, 1507, 281, 1066, 2837, 295, 577, 867, 30992, 28346, 575, 309, 1096, 13], "temperature": 0.0, "avg_logprob": -0.26454075451554926, "compression_ratio": 1.7399103139013452, "no_speech_prob": 8.397896635869984e-06}, {"id": 1428, "seek": 625854, "start": 6276.34, "end": 6277.1, "text": " And this is quite nice.", "tokens": [400, 341, 307, 1596, 1481, 13], "temperature": 0.0, "avg_logprob": -0.26454075451554926, "compression_ratio": 1.7399103139013452, "no_speech_prob": 8.397896635869984e-06}, {"id": 1429, "seek": 625854, "start": 6277.1, "end": 6279.82, "text": " It actually does it as a floating point, not just as an int.", "tokens": [467, 767, 775, 309, 382, 257, 12607, 935, 11, 406, 445, 382, 364, 560, 13], "temperature": 0.0, "avg_logprob": -0.26454075451554926, "compression_ratio": 1.7399103139013452, "no_speech_prob": 8.397896635869984e-06}, {"id": 1430, "seek": 625854, "start": 6279.82, "end": 6283.98, "text": " So you could be like 2.3 epochs in.", "tokens": [407, 291, 727, 312, 411, 568, 13, 18, 30992, 28346, 294, 13], "temperature": 0.0, "avg_logprob": -0.26454075451554926, "compression_ratio": 1.7399103139013452, "no_speech_prob": 8.397896635869984e-06}, {"id": 1431, "seek": 625854, "start": 6283.98, "end": 6286.58, "text": " It also keeps track of how many iterations do you want.", "tokens": [467, 611, 5965, 2837, 295, 577, 867, 36540, 360, 291, 528, 13], "temperature": 0.0, "avg_logprob": -0.26454075451554926, "compression_ratio": 1.7399103139013452, "no_speech_prob": 8.397896635869984e-06}, {"id": 1432, "seek": 628658, "start": 6286.58, "end": 6289.62, "text": " So now we have this thing keeping track of iterations.", "tokens": [407, 586, 321, 362, 341, 551, 5145, 2837, 295, 36540, 13], "temperature": 0.0, "avg_logprob": -0.23280176909073538, "compression_ratio": 1.7323232323232323, "no_speech_prob": 4.565830749925226e-06}, {"id": 1433, "seek": 628658, "start": 6289.62, "end": 6294.66, "text": " Our test callback that should stop training after ten iterations.", "tokens": [2621, 1500, 818, 3207, 300, 820, 1590, 3097, 934, 2064, 36540, 13], "temperature": 0.0, "avg_logprob": -0.23280176909073538, "compression_ratio": 1.7323232323232323, "no_speech_prob": 4.565830749925226e-06}, {"id": 1434, "seek": 628658, "start": 6295.7, "end": 6300.18, "text": " Rather than keeping track of n itters itself, it should just use the n itter", "tokens": [16571, 813, 5145, 2837, 295, 297, 309, 1559, 2564, 11, 309, 820, 445, 764, 264, 297, 309, 391], "temperature": 0.0, "avg_logprob": -0.23280176909073538, "compression_ratio": 1.7323232323232323, "no_speech_prob": 4.565830749925226e-06}, {"id": 1435, "seek": 628658, "start": 6302.74, "end": 6304.34, "text": " that was defined in this callback.", "tokens": [300, 390, 7642, 294, 341, 818, 3207, 13], "temperature": 0.0, "avg_logprob": -0.23280176909073538, "compression_ratio": 1.7323232323232323, "no_speech_prob": 4.565830749925226e-06}, {"id": 1436, "seek": 628658, "start": 6305.38, "end": 6310.78, "text": " So what we can do is we can say, all right, well train eval callback", "tokens": [407, 437, 321, 393, 360, 307, 321, 393, 584, 11, 439, 558, 11, 731, 3847, 1073, 304, 818, 3207], "temperature": 0.0, "avg_logprob": -0.23280176909073538, "compression_ratio": 1.7323232323232323, "no_speech_prob": 4.565830749925226e-06}, {"id": 1437, "seek": 628658, "start": 6310.78, "end": 6313.5, "text": " has an order of zero because it inherits.", "tokens": [575, 364, 1668, 295, 4018, 570, 309, 9484, 1208, 13], "temperature": 0.0, "avg_logprob": -0.23280176909073538, "compression_ratio": 1.7323232323232323, "no_speech_prob": 4.565830749925226e-06}, {"id": 1438, "seek": 631350, "start": 6313.5, "end": 6316.9, "text": " So what we can just do here is make sure that this is later,", "tokens": [407, 437, 321, 393, 445, 360, 510, 307, 652, 988, 300, 341, 307, 1780, 11], "temperature": 0.0, "avg_logprob": -0.37617544876901726, "compression_ratio": 1.6082474226804124, "no_speech_prob": 4.785005785379326e-06}, {"id": 1439, "seek": 631350, "start": 6316.9, "end": 6319.26, "text": " underscore order equals one.", "tokens": [37556, 1668, 6915, 472, 13], "temperature": 0.0, "avg_logprob": -0.37617544876901726, "compression_ratio": 1.6082474226804124, "no_speech_prob": 4.785005785379326e-06}, {"id": 1440, "seek": 631350, "start": 6321.42, "end": 6326.7, "text": " And so that way we can now refer to stuff that's inside the train eval", "tokens": [400, 370, 300, 636, 321, 393, 586, 2864, 281, 1507, 300, 311, 1854, 264, 3847, 1073, 304], "temperature": 0.0, "avg_logprob": -0.37617544876901726, "compression_ratio": 1.6082474226804124, "no_speech_prob": 4.785005785379326e-06}, {"id": 1441, "seek": 631350, "start": 6326.7, "end": 6328.58, "text": " callback like n itter.", "tokens": [818, 3207, 411, 297, 309, 391, 13], "temperature": 0.0, "avg_logprob": -0.37617544876901726, "compression_ratio": 1.6082474226804124, "no_speech_prob": 4.785005785379326e-06}, {"id": 1442, "seek": 631350, "start": 6331.62, "end": 6335.02, "text": " Sorry, well actually we don't even need to do that because it's putting n itter", "tokens": [4919, 11, 731, 767, 321, 500, 380, 754, 643, 281, 360, 300, 570, 309, 311, 3372, 297, 309, 391], "temperature": 0.0, "avg_logprob": -0.37617544876901726, "compression_ratio": 1.6082474226804124, "no_speech_prob": 4.785005785379326e-06}, {"id": 1443, "seek": 631350, "start": 6335.02, "end": 6336.06, "text": " inside self.run.", "tokens": [1854, 2698, 13, 12997, 13], "temperature": 0.0, "avg_logprob": -0.37617544876901726, "compression_ratio": 1.6082474226804124, "no_speech_prob": 4.785005785379326e-06}, {"id": 1444, "seek": 631350, "start": 6336.06, "end": 6338.46, "text": " So we can just go self.n itter.", "tokens": [407, 321, 393, 445, 352, 2698, 13, 77, 309, 391, 13], "temperature": 0.0, "avg_logprob": -0.37617544876901726, "compression_ratio": 1.6082474226804124, "no_speech_prob": 4.785005785379326e-06}, {"id": 1445, "seek": 633846, "start": 6338.46, "end": 6344.06, "text": " If this ran before train eval callback, that would be a problem", "tokens": [759, 341, 5872, 949, 3847, 1073, 304, 818, 3207, 11, 300, 576, 312, 257, 1154], "temperature": 0.0, "avg_logprob": -0.3413999113318038, "compression_ratio": 1.4825581395348837, "no_speech_prob": 1.733039198370534e-06}, {"id": 1446, "seek": 633846, "start": 6344.06, "end": 6347.74, "text": " because n itter might not have been updated yet.", "tokens": [570, 297, 309, 391, 1062, 406, 362, 668, 10588, 1939, 13], "temperature": 0.0, "avg_logprob": -0.3413999113318038, "compression_ratio": 1.4825581395348837, "no_speech_prob": 1.733039198370534e-06}, {"id": 1447, "seek": 633846, "start": 6347.74, "end": 6351.18, "text": " So that's what the order's for.", "tokens": [407, 300, 311, 437, 264, 1668, 311, 337, 13], "temperature": 0.0, "avg_logprob": -0.3413999113318038, "compression_ratio": 1.4825581395348837, "no_speech_prob": 1.733039198370534e-06}, {"id": 1448, "seek": 633846, "start": 6351.18, "end": 6364.54, "text": " Another nice thing about runner, sorry, a nice thing about class callback", "tokens": [3996, 1481, 551, 466, 24376, 11, 2597, 11, 257, 1481, 551, 466, 1508, 818, 3207], "temperature": 0.0, "avg_logprob": -0.3413999113318038, "compression_ratio": 1.4825581395348837, "no_speech_prob": 1.733039198370534e-06}, {"id": 1449, "seek": 633846, "start": 6364.54, "end": 6367.82, "text": " is that I've defined dunder getAtra.", "tokens": [307, 300, 286, 600, 7642, 274, 6617, 483, 18684, 424, 13], "temperature": 0.0, "avg_logprob": -0.3413999113318038, "compression_ratio": 1.4825581395348837, "no_speech_prob": 1.733039198370534e-06}, {"id": 1450, "seek": 636782, "start": 6367.82, "end": 6375.58, "text": " And I've defined it to say return getAtra self.run, k.", "tokens": [400, 286, 600, 7642, 309, 281, 584, 2736, 483, 18684, 424, 2698, 13, 12997, 11, 350, 13], "temperature": 0.0, "avg_logprob": -0.17435556703859623, "compression_ratio": 1.7077625570776256, "no_speech_prob": 5.594163667410612e-06}, {"id": 1451, "seek": 636782, "start": 6375.58, "end": 6379.58, "text": " An important thing to know about dunder getAtra is that it is only called", "tokens": [1107, 1021, 551, 281, 458, 466, 274, 6617, 483, 18684, 424, 307, 300, 309, 307, 787, 1219], "temperature": 0.0, "avg_logprob": -0.17435556703859623, "compression_ratio": 1.7077625570776256, "no_speech_prob": 5.594163667410612e-06}, {"id": 1452, "seek": 636782, "start": 6379.58, "end": 6383.42, "text": " by Python if it can't find the attribute that you've asked for.", "tokens": [538, 15329, 498, 309, 393, 380, 915, 264, 19667, 300, 291, 600, 2351, 337, 13], "temperature": 0.0, "avg_logprob": -0.17435556703859623, "compression_ratio": 1.7077625570776256, "no_speech_prob": 5.594163667410612e-06}, {"id": 1453, "seek": 636782, "start": 6384.86, "end": 6389.0199999999995, "text": " So if something asks for self.name, well I have self.name, so", "tokens": [407, 498, 746, 8962, 337, 2698, 13, 16344, 11, 731, 286, 362, 2698, 13, 16344, 11, 370], "temperature": 0.0, "avg_logprob": -0.17435556703859623, "compression_ratio": 1.7077625570776256, "no_speech_prob": 5.594163667410612e-06}, {"id": 1454, "seek": 636782, "start": 6389.0199999999995, "end": 6389.98, "text": " it's never gonna get to here.", "tokens": [309, 311, 1128, 799, 483, 281, 510, 13], "temperature": 0.0, "avg_logprob": -0.17435556703859623, "compression_ratio": 1.7077625570776256, "no_speech_prob": 5.594163667410612e-06}, {"id": 1455, "seek": 636782, "start": 6391.42, "end": 6394.74, "text": " So if you get to here, it means Python looked for this attribute and", "tokens": [407, 498, 291, 483, 281, 510, 11, 309, 1355, 15329, 2956, 337, 341, 19667, 293], "temperature": 0.0, "avg_logprob": -0.17435556703859623, "compression_ratio": 1.7077625570776256, "no_speech_prob": 5.594163667410612e-06}, {"id": 1456, "seek": 636782, "start": 6394.74, "end": 6395.38, "text": " it couldn't find it.", "tokens": [309, 2809, 380, 915, 309, 13], "temperature": 0.0, "avg_logprob": -0.17435556703859623, "compression_ratio": 1.7077625570776256, "no_speech_prob": 5.594163667410612e-06}, {"id": 1457, "seek": 639538, "start": 6395.38, "end": 6402.26, "text": " And so very, very often the thing you actually want in the callback", "tokens": [400, 370, 588, 11, 588, 2049, 264, 551, 291, 767, 528, 294, 264, 818, 3207], "temperature": 0.0, "avg_logprob": -0.1538934810187227, "compression_ratio": 1.6618357487922706, "no_speech_prob": 8.267541488748975e-06}, {"id": 1458, "seek": 639538, "start": 6403.06, "end": 6407.22, "text": " is actually inside the runner, which we store away as self.run.", "tokens": [307, 767, 1854, 264, 24376, 11, 597, 321, 3531, 1314, 382, 2698, 13, 12997, 13], "temperature": 0.0, "avg_logprob": -0.1538934810187227, "compression_ratio": 1.6618357487922706, "no_speech_prob": 8.267541488748975e-06}, {"id": 1459, "seek": 639538, "start": 6407.22, "end": 6411.7, "text": " So this means that in all of our callbacks, let's look at one,", "tokens": [407, 341, 1355, 300, 294, 439, 295, 527, 818, 17758, 11, 718, 311, 574, 412, 472, 11], "temperature": 0.0, "avg_logprob": -0.1538934810187227, "compression_ratio": 1.6618357487922706, "no_speech_prob": 8.267541488748975e-06}, {"id": 1460, "seek": 639538, "start": 6413.86, "end": 6417.86, "text": " you can basically just use self. pretty much everything.", "tokens": [291, 393, 1936, 445, 764, 2698, 13, 1238, 709, 1203, 13], "temperature": 0.0, "avg_logprob": -0.1538934810187227, "compression_ratio": 1.6618357487922706, "no_speech_prob": 8.267541488748975e-06}, {"id": 1461, "seek": 639538, "start": 6417.86, "end": 6422.58, "text": " And it will grab what you want, even though most of the stuff you want", "tokens": [400, 309, 486, 4444, 437, 291, 528, 11, 754, 1673, 881, 295, 264, 1507, 291, 528], "temperature": 0.0, "avg_logprob": -0.1538934810187227, "compression_ratio": 1.6618357487922706, "no_speech_prob": 8.267541488748975e-06}, {"id": 1462, "seek": 639538, "start": 6422.58, "end": 6423.78, "text": " is inside the runner.", "tokens": [307, 1854, 264, 24376, 13], "temperature": 0.0, "avg_logprob": -0.1538934810187227, "compression_ratio": 1.6618357487922706, "no_speech_prob": 8.267541488748975e-06}, {"id": 1463, "seek": 642378, "start": 6423.78, "end": 6431.0599999999995, "text": " So you'll see this pattern in fast AI a lot is that when one object", "tokens": [407, 291, 603, 536, 341, 5102, 294, 2370, 7318, 257, 688, 307, 300, 562, 472, 2657], "temperature": 0.0, "avg_logprob": -0.18694906649382217, "compression_ratio": 1.5389221556886228, "no_speech_prob": 8.266870281659067e-06}, {"id": 1464, "seek": 642378, "start": 6431.86, "end": 6436.179999999999, "text": " contains another object or composes another object, we very often delegate", "tokens": [8306, 1071, 2657, 420, 715, 4201, 1071, 2657, 11, 321, 588, 2049, 40999], "temperature": 0.0, "avg_logprob": -0.18694906649382217, "compression_ratio": 1.5389221556886228, "no_speech_prob": 8.266870281659067e-06}, {"id": 1465, "seek": 642378, "start": 6436.98, "end": 6438.98, "text": " get attribute to the other object.", "tokens": [483, 19667, 281, 264, 661, 2657, 13], "temperature": 0.0, "avg_logprob": -0.18694906649382217, "compression_ratio": 1.5389221556886228, "no_speech_prob": 8.266870281659067e-06}, {"id": 1466, "seek": 642378, "start": 6438.98, "end": 6447.46, "text": " So for example, if you're looking at a data set, then I think we delegate", "tokens": [407, 337, 1365, 11, 498, 291, 434, 1237, 412, 257, 1412, 992, 11, 550, 286, 519, 321, 40999], "temperature": 0.0, "avg_logprob": -0.18694906649382217, "compression_ratio": 1.5389221556886228, "no_speech_prob": 8.266870281659067e-06}, {"id": 1467, "seek": 642378, "start": 6448.58, "end": 6449.62, "text": " to x.", "tokens": [281, 2031, 13], "temperature": 0.0, "avg_logprob": -0.18694906649382217, "compression_ratio": 1.5389221556886228, "no_speech_prob": 8.266870281659067e-06}, {"id": 1468, "seek": 644962, "start": 6449.62, "end": 6454.74, "text": " If you're looking at stuff in the data box API, it'll often delegate to", "tokens": [759, 291, 434, 1237, 412, 1507, 294, 264, 1412, 2424, 9362, 11, 309, 603, 2049, 40999, 281], "temperature": 0.0, "avg_logprob": -0.135019272252133, "compression_ratio": 1.6076555023923444, "no_speech_prob": 1.3211611985752825e-05}, {"id": 1469, "seek": 644962, "start": 6454.74, "end": 6457.86, "text": " stuff lower in the data box API and so forth.", "tokens": [1507, 3126, 294, 264, 1412, 2424, 9362, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.135019272252133, "compression_ratio": 1.6076555023923444, "no_speech_prob": 1.3211611985752825e-05}, {"id": 1470, "seek": 644962, "start": 6458.9, "end": 6460.099999999999, "text": " So I find this pretty handy.", "tokens": [407, 286, 915, 341, 1238, 13239, 13], "temperature": 0.0, "avg_logprob": -0.135019272252133, "compression_ratio": 1.6076555023923444, "no_speech_prob": 1.3211611985752825e-05}, {"id": 1471, "seek": 644962, "start": 6462.5, "end": 6467.94, "text": " Okay, so we have a callback that as you see, there's very little to it.", "tokens": [1033, 11, 370, 321, 362, 257, 818, 3207, 300, 382, 291, 536, 11, 456, 311, 588, 707, 281, 309, 13], "temperature": 0.0, "avg_logprob": -0.135019272252133, "compression_ratio": 1.6076555023923444, "no_speech_prob": 1.3211611985752825e-05}, {"id": 1472, "seek": 644962, "start": 6468.66, "end": 6471.86, "text": " One interesting thing you might notice is that a callback has a name", "tokens": [1485, 1880, 551, 291, 1062, 3449, 307, 300, 257, 818, 3207, 575, 257, 1315], "temperature": 0.0, "avg_logprob": -0.135019272252133, "compression_ratio": 1.6076555023923444, "no_speech_prob": 1.3211611985752825e-05}, {"id": 1473, "seek": 644962, "start": 6471.86, "end": 6472.34, "text": " property.", "tokens": [4707, 13], "temperature": 0.0, "avg_logprob": -0.135019272252133, "compression_ratio": 1.6076555023923444, "no_speech_prob": 1.3211611985752825e-05}, {"id": 1474, "seek": 644962, "start": 6473.22, "end": 6477.38, "text": " And the name property works like this.", "tokens": [400, 264, 1315, 4707, 1985, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.135019272252133, "compression_ratio": 1.6076555023923444, "no_speech_prob": 1.3211611985752825e-05}, {"id": 1475, "seek": 647738, "start": 6477.38, "end": 6482.02, "text": " If you have a property called train eval callback, then we've got a function", "tokens": [759, 291, 362, 257, 4707, 1219, 3847, 1073, 304, 818, 3207, 11, 550, 321, 600, 658, 257, 2445], "temperature": 0.0, "avg_logprob": -0.09843376127340026, "compression_ratio": 1.8354978354978355, "no_speech_prob": 2.1111682144692168e-05}, {"id": 1476, "seek": 647738, "start": 6482.02, "end": 6483.38, "text": " called camel to snake.", "tokens": [1219, 37755, 281, 12650, 13], "temperature": 0.0, "avg_logprob": -0.09843376127340026, "compression_ratio": 1.8354978354978355, "no_speech_prob": 2.1111682144692168e-05}, {"id": 1477, "seek": 647738, "start": 6483.38, "end": 6486.74, "text": " This is called camel case means you've got uppercase and lowercase letters", "tokens": [639, 307, 1219, 37755, 1389, 1355, 291, 600, 658, 11775, 2869, 651, 293, 3126, 9765, 7825], "temperature": 0.0, "avg_logprob": -0.09843376127340026, "compression_ratio": 1.8354978354978355, "no_speech_prob": 2.1111682144692168e-05}, {"id": 1478, "seek": 647738, "start": 6486.74, "end": 6490.5, "text": " like a camel and snake case looks like this.", "tokens": [411, 257, 37755, 293, 12650, 1389, 1542, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.09843376127340026, "compression_ratio": 1.8354978354978355, "no_speech_prob": 2.1111682144692168e-05}, {"id": 1479, "seek": 647738, "start": 6490.5, "end": 6492.82, "text": " So camel to snake turns this into a snake.", "tokens": [407, 37755, 281, 12650, 4523, 341, 666, 257, 12650, 13], "temperature": 0.0, "avg_logprob": -0.09843376127340026, "compression_ratio": 1.8354978354978355, "no_speech_prob": 2.1111682144692168e-05}, {"id": 1480, "seek": 647738, "start": 6493.86, "end": 6499.22, "text": " And then what we do here is we remove callback from the end.", "tokens": [400, 550, 437, 321, 360, 510, 307, 321, 4159, 818, 3207, 490, 264, 917, 13], "temperature": 0.0, "avg_logprob": -0.09843376127340026, "compression_ratio": 1.8354978354978355, "no_speech_prob": 2.1111682144692168e-05}, {"id": 1481, "seek": 647738, "start": 6500.18, "end": 6501.22, "text": " And that's its name.", "tokens": [400, 300, 311, 1080, 1315, 13], "temperature": 0.0, "avg_logprob": -0.09843376127340026, "compression_ratio": 1.8354978354978355, "no_speech_prob": 2.1111682144692168e-05}, {"id": 1482, "seek": 647738, "start": 6501.22, "end": 6505.46, "text": " So train eval callback has a name, which is just train eval with an underscore.", "tokens": [407, 3847, 1073, 304, 818, 3207, 575, 257, 1315, 11, 597, 307, 445, 3847, 1073, 304, 365, 364, 37556, 13], "temperature": 0.0, "avg_logprob": -0.09843376127340026, "compression_ratio": 1.8354978354978355, "no_speech_prob": 2.1111682144692168e-05}, {"id": 1483, "seek": 650546, "start": 6505.46, "end": 6510.9, "text": " And then in the runner, any callback functions that you pass in,", "tokens": [400, 550, 294, 264, 24376, 11, 604, 818, 3207, 6828, 300, 291, 1320, 294, 11], "temperature": 0.0, "avg_logprob": -0.13882715805717136, "compression_ratio": 1.6009615384615385, "no_speech_prob": 4.4508220753414207e-07}, {"id": 1484, "seek": 650546, "start": 6512.02, "end": 6517.14, "text": " which it uses to create new callbacks, it actually assigns them to an", "tokens": [597, 309, 4960, 281, 1884, 777, 818, 17758, 11, 309, 767, 6269, 82, 552, 281, 364], "temperature": 0.0, "avg_logprob": -0.13882715805717136, "compression_ratio": 1.6009615384615385, "no_speech_prob": 4.4508220753414207e-07}, {"id": 1485, "seek": 650546, "start": 6517.14, "end": 6519.06, "text": " attribute with that name.", "tokens": [19667, 365, 300, 1315, 13], "temperature": 0.0, "avg_logprob": -0.13882715805717136, "compression_ratio": 1.6009615384615385, "no_speech_prob": 4.4508220753414207e-07}, {"id": 1486, "seek": 650546, "start": 6519.06, "end": 6527.38, "text": " So we now have something called runner.trainEval, for example.", "tokens": [407, 321, 586, 362, 746, 1219, 24376, 13, 83, 7146, 36, 3337, 11, 337, 1365, 13], "temperature": 0.0, "avg_logprob": -0.13882715805717136, "compression_ratio": 1.6009615384615385, "no_speech_prob": 4.4508220753414207e-07}, {"id": 1487, "seek": 650546, "start": 6527.38, "end": 6529.94, "text": " So we do this in the fast AI library.", "tokens": [407, 321, 360, 341, 294, 264, 2370, 7318, 6405, 13], "temperature": 0.0, "avg_logprob": -0.13882715805717136, "compression_ratio": 1.6009615384615385, "no_speech_prob": 4.4508220753414207e-07}, {"id": 1488, "seek": 650546, "start": 6529.94, "end": 6534.82, "text": " When you say learn.recorder, we didn't actually add an attribute called", "tokens": [1133, 291, 584, 1466, 13, 13867, 4687, 11, 321, 994, 380, 767, 909, 364, 19667, 1219], "temperature": 0.0, "avg_logprob": -0.13882715805717136, "compression_ratio": 1.6009615384615385, "no_speech_prob": 4.4508220753414207e-07}, {"id": 1489, "seek": 653482, "start": 6534.82, "end": 6536.179999999999, "text": " recorder to learner.", "tokens": [37744, 281, 33347, 13], "temperature": 0.0, "avg_logprob": -0.10920703516597241, "compression_ratio": 1.728813559322034, "no_speech_prob": 1.2218764823046513e-05}, {"id": 1490, "seek": 653482, "start": 6536.9, "end": 6539.86, "text": " It just automatically sets that because there's a learner callback.", "tokens": [467, 445, 6772, 6352, 300, 570, 456, 311, 257, 33347, 818, 3207, 13], "temperature": 0.0, "avg_logprob": -0.10920703516597241, "compression_ratio": 1.728813559322034, "no_speech_prob": 1.2218764823046513e-05}, {"id": 1491, "seek": 653482, "start": 6543.54, "end": 6544.66, "text": " So let's see how to use this.", "tokens": [407, 718, 311, 536, 577, 281, 764, 341, 13], "temperature": 0.0, "avg_logprob": -0.10920703516597241, "compression_ratio": 1.728813559322034, "no_speech_prob": 1.2218764823046513e-05}, {"id": 1492, "seek": 653482, "start": 6546.98, "end": 6547.78, "text": " There's a question.", "tokens": [821, 311, 257, 1168, 13], "temperature": 0.0, "avg_logprob": -0.10920703516597241, "compression_ratio": 1.728813559322034, "no_speech_prob": 1.2218764823046513e-05}, {"id": 1493, "seek": 653482, "start": 6548.9, "end": 6550.0199999999995, "text": " Okay, let's do that in a moment.", "tokens": [1033, 11, 718, 311, 360, 300, 294, 257, 1623, 13], "temperature": 0.0, "avg_logprob": -0.10920703516597241, "compression_ratio": 1.728813559322034, "no_speech_prob": 1.2218764823046513e-05}, {"id": 1494, "seek": 653482, "start": 6550.0199999999995, "end": 6555.7, "text": " So let's use this to add metrics, because it's no fun having a training", "tokens": [407, 718, 311, 764, 341, 281, 909, 16367, 11, 570, 309, 311, 572, 1019, 1419, 257, 3097], "temperature": 0.0, "avg_logprob": -0.10920703516597241, "compression_ratio": 1.728813559322034, "no_speech_prob": 1.2218764823046513e-05}, {"id": 1495, "seek": 653482, "start": 6555.7, "end": 6557.299999999999, "text": " loop where we can't actually see how we're going.", "tokens": [6367, 689, 321, 393, 380, 767, 536, 577, 321, 434, 516, 13], "temperature": 0.0, "avg_logprob": -0.10920703516597241, "compression_ratio": 1.728813559322034, "no_speech_prob": 1.2218764823046513e-05}, {"id": 1496, "seek": 653482, "start": 6557.86, "end": 6561.0599999999995, "text": " And part of the whole point of this is that our actual training loop is now", "tokens": [400, 644, 295, 264, 1379, 935, 295, 341, 307, 300, 527, 3539, 3097, 6367, 307, 586], "temperature": 0.0, "avg_logprob": -0.10920703516597241, "compression_ratio": 1.728813559322034, "no_speech_prob": 1.2218764823046513e-05}, {"id": 1497, "seek": 656106, "start": 6561.06, "end": 6564.1, "text": " so incredibly tight and neat and easy.", "tokens": [370, 6252, 4524, 293, 10654, 293, 1858, 13], "temperature": 0.0, "avg_logprob": -0.26081009915000514, "compression_ratio": 1.7393162393162394, "no_speech_prob": 1.520664318377385e-05}, {"id": 1498, "seek": 656106, "start": 6564.740000000001, "end": 6566.9800000000005, "text": " But we actually want to do all the stuff we want to do.", "tokens": [583, 321, 767, 528, 281, 360, 439, 264, 1507, 321, 528, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.26081009915000514, "compression_ratio": 1.7393162393162394, "no_speech_prob": 1.520664318377385e-05}, {"id": 1499, "seek": 656106, "start": 6566.9800000000005, "end": 6571.700000000001, "text": " So what if we create a little callback called averageStatsCallback, right?", "tokens": [407, 437, 498, 321, 1884, 257, 707, 818, 3207, 1219, 4274, 4520, 1720, 46113, 3207, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.26081009915000514, "compression_ratio": 1.7393162393162394, "no_speech_prob": 1.520664318377385e-05}, {"id": 1500, "seek": 656106, "start": 6573.700000000001, "end": 6578.02, "text": " Where we're going to stick into a couple of objects to keep track of our loss", "tokens": [2305, 321, 434, 516, 281, 2897, 666, 257, 1916, 295, 6565, 281, 1066, 2837, 295, 527, 4470], "temperature": 0.0, "avg_logprob": -0.26081009915000514, "compression_ratio": 1.7393162393162394, "no_speech_prob": 1.520664318377385e-05}, {"id": 1501, "seek": 656106, "start": 6578.02, "end": 6580.5, "text": " and metrics, one for training, one for valid.", "tokens": [293, 16367, 11, 472, 337, 3097, 11, 472, 337, 7363, 13], "temperature": 0.0, "avg_logprob": -0.26081009915000514, "compression_ratio": 1.7393162393162394, "no_speech_prob": 1.520664318377385e-05}, {"id": 1502, "seek": 656106, "start": 6581.14, "end": 6583.780000000001, "text": " And at the start of an epoch, we'll reset the statistics.", "tokens": [400, 412, 264, 722, 295, 364, 30992, 339, 11, 321, 603, 14322, 264, 12523, 13], "temperature": 0.0, "avg_logprob": -0.26081009915000514, "compression_ratio": 1.7393162393162394, "no_speech_prob": 1.520664318377385e-05}, {"id": 1503, "seek": 656106, "start": 6584.740000000001, "end": 6587.860000000001, "text": " At the end of an epoch, we'll print out the statistics.", "tokens": [1711, 264, 917, 295, 364, 30992, 339, 11, 321, 603, 4482, 484, 264, 12523, 13], "temperature": 0.0, "avg_logprob": -0.26081009915000514, "compression_ratio": 1.7393162393162394, "no_speech_prob": 1.520664318377385e-05}, {"id": 1504, "seek": 658786, "start": 6587.86, "end": 6594.259999999999, "text": " And after we've got the loss calculated, we will accumulate the statistics.", "tokens": [400, 934, 321, 600, 658, 264, 4470, 15598, 11, 321, 486, 33384, 264, 12523, 13], "temperature": 0.0, "avg_logprob": -0.24445059692975388, "compression_ratio": 1.7660550458715596, "no_speech_prob": 5.338112259778427e-06}, {"id": 1505, "seek": 658786, "start": 6594.259999999999, "end": 6598.099999999999, "text": " So then all we need is an object that has an accumulate method.", "tokens": [407, 550, 439, 321, 643, 307, 364, 2657, 300, 575, 364, 33384, 3170, 13], "temperature": 0.0, "avg_logprob": -0.24445059692975388, "compression_ratio": 1.7660550458715596, "no_speech_prob": 5.338112259778427e-06}, {"id": 1506, "seek": 658786, "start": 6599.0599999999995, "end": 6601.219999999999, "text": " So let's create a class that does that.", "tokens": [407, 718, 311, 1884, 257, 1508, 300, 775, 300, 13], "temperature": 0.0, "avg_logprob": -0.24445059692975388, "compression_ratio": 1.7660550458715596, "no_speech_prob": 5.338112259778427e-06}, {"id": 1507, "seek": 658786, "start": 6601.219999999999, "end": 6602.74, "text": " And here's our accumulate method.", "tokens": [400, 510, 311, 527, 33384, 3170, 13], "temperature": 0.0, "avg_logprob": -0.24445059692975388, "compression_ratio": 1.7660550458715596, "no_speech_prob": 5.338112259778427e-06}, {"id": 1508, "seek": 658786, "start": 6603.7, "end": 6606.0199999999995, "text": " It's going to add up the total loss.", "tokens": [467, 311, 516, 281, 909, 493, 264, 3217, 4470, 13], "temperature": 0.0, "avg_logprob": -0.24445059692975388, "compression_ratio": 1.7660550458715596, "no_speech_prob": 5.338112259778427e-06}, {"id": 1509, "seek": 658786, "start": 6607.0599999999995, "end": 6610.9, "text": " And for each metric, it'll add up the total metrics.", "tokens": [400, 337, 1184, 20678, 11, 309, 603, 909, 493, 264, 3217, 16367, 13], "temperature": 0.0, "avg_logprob": -0.24445059692975388, "compression_ratio": 1.7660550458715596, "no_speech_prob": 5.338112259778427e-06}, {"id": 1510, "seek": 658786, "start": 6612.0199999999995, "end": 6615.38, "text": " And then we'll give it a property called averageStats that will go through all of", "tokens": [400, 550, 321, 603, 976, 309, 257, 4707, 1219, 4274, 4520, 1720, 300, 486, 352, 807, 439, 295], "temperature": 0.0, "avg_logprob": -0.24445059692975388, "compression_ratio": 1.7660550458715596, "no_speech_prob": 5.338112259778427e-06}, {"id": 1511, "seek": 661538, "start": 6615.38, "end": 6619.06, "text": " those losses and metrics and return the average.", "tokens": [729, 15352, 293, 16367, 293, 2736, 264, 4274, 13], "temperature": 0.0, "avg_logprob": -0.24710188309351602, "compression_ratio": 1.8446601941747574, "no_speech_prob": 1.045126646204153e-05}, {"id": 1512, "seek": 661538, "start": 6619.78, "end": 6624.1, "text": " And you might notice here I've fixed the problem of having different batch sizes", "tokens": [400, 291, 1062, 3449, 510, 286, 600, 6806, 264, 1154, 295, 1419, 819, 15245, 11602], "temperature": 0.0, "avg_logprob": -0.24710188309351602, "compression_ratio": 1.8446601941747574, "no_speech_prob": 1.045126646204153e-05}, {"id": 1513, "seek": 661538, "start": 6624.1, "end": 6624.9800000000005, "text": " in the average.", "tokens": [294, 264, 4274, 13], "temperature": 0.0, "avg_logprob": -0.24710188309351602, "compression_ratio": 1.8446601941747574, "no_speech_prob": 1.045126646204153e-05}, {"id": 1514, "seek": 661538, "start": 6624.9800000000005, "end": 6631.86, "text": " We're actually adding loss times the size of the batch and count plus the size of the batch", "tokens": [492, 434, 767, 5127, 4470, 1413, 264, 2744, 295, 264, 15245, 293, 1207, 1804, 264, 2744, 295, 264, 15245], "temperature": 0.0, "avg_logprob": -0.24710188309351602, "compression_ratio": 1.8446601941747574, "no_speech_prob": 1.045126646204153e-05}, {"id": 1515, "seek": 661538, "start": 6631.86, "end": 6634.58, "text": " and metrics times the size of the batch.", "tokens": [293, 16367, 1413, 264, 2744, 295, 264, 15245, 13], "temperature": 0.0, "avg_logprob": -0.24710188309351602, "compression_ratio": 1.8446601941747574, "no_speech_prob": 1.045126646204153e-05}, {"id": 1516, "seek": 661538, "start": 6635.22, "end": 6638.34, "text": " And so then we're dividing here by the total batch size.", "tokens": [400, 370, 550, 321, 434, 26764, 510, 538, 264, 3217, 15245, 2744, 13], "temperature": 0.0, "avg_logprob": -0.24710188309351602, "compression_ratio": 1.8446601941747574, "no_speech_prob": 1.045126646204153e-05}, {"id": 1517, "seek": 661538, "start": 6639.06, "end": 6643.3, "text": " So this is going to keep track of our stats.", "tokens": [407, 341, 307, 516, 281, 1066, 2837, 295, 527, 18152, 13], "temperature": 0.0, "avg_logprob": -0.24710188309351602, "compression_ratio": 1.8446601941747574, "no_speech_prob": 1.045126646204153e-05}, {"id": 1518, "seek": 664330, "start": 6643.3, "end": 6648.18, "text": " We'll add a Dundar repra so that it prints out those statistics in a nice way.", "tokens": [492, 603, 909, 257, 413, 997, 289, 1085, 424, 370, 300, 309, 22305, 484, 729, 12523, 294, 257, 1481, 636, 13], "temperature": 0.0, "avg_logprob": -0.29879767894744874, "compression_ratio": 1.548913043478261, "no_speech_prob": 2.812988668665639e-06}, {"id": 1519, "seek": 664330, "start": 6649.38, "end": 6655.22, "text": " And so now we can create our learner, add our averageStats callback.", "tokens": [400, 370, 586, 321, 393, 1884, 527, 33347, 11, 909, 527, 4274, 4520, 1720, 818, 3207, 13], "temperature": 0.0, "avg_logprob": -0.29879767894744874, "compression_ratio": 1.548913043478261, "no_speech_prob": 2.812988668665639e-06}, {"id": 1520, "seek": 664330, "start": 6655.9400000000005, "end": 6660.58, "text": " And when we call fit, it prints out how we're going.", "tokens": [400, 562, 321, 818, 3318, 11, 309, 22305, 484, 577, 321, 434, 516, 13], "temperature": 0.0, "avg_logprob": -0.29879767894744874, "compression_ratio": 1.548913043478261, "no_speech_prob": 2.812988668665639e-06}, {"id": 1521, "seek": 664330, "start": 6661.54, "end": 6670.58, "text": " And so that's the entirety of what it talked to add metrics and loss tracking to our", "tokens": [400, 370, 300, 311, 264, 31557, 295, 437, 309, 2825, 281, 909, 16367, 293, 4470, 11603, 281, 527], "temperature": 0.0, "avg_logprob": -0.29879767894744874, "compression_ratio": 1.548913043478261, "no_speech_prob": 2.812988668665639e-06}, {"id": 1522, "seek": 667058, "start": 6670.58, "end": 6673.46, "text": " minimal training loop.", "tokens": [13206, 3097, 6367, 13], "temperature": 0.0, "avg_logprob": -0.5441176263909591, "compression_ratio": 1.5112359550561798, "no_speech_prob": 8.66423397383187e-06}, {"id": 1523, "seek": 667058, "start": 6673.46, "end": 6674.26, "text": " Yes, Rachel.", "tokens": [1079, 11, 14246, 13], "temperature": 0.0, "avg_logprob": -0.5441176263909591, "compression_ratio": 1.5112359550561798, "no_speech_prob": 8.66423397383187e-06}, {"id": 1524, "seek": 667058, "start": 6677.78, "end": 6682.9, "text": " Runner Dundar call exits early when the first callback returns true.", "tokens": [50105, 413, 997, 289, 818, 44183, 2440, 562, 264, 700, 818, 3207, 11247, 2074, 13], "temperature": 0.0, "avg_logprob": -0.5441176263909591, "compression_ratio": 1.5112359550561798, "no_speech_prob": 8.66423397383187e-06}, {"id": 1525, "seek": 667058, "start": 6683.7, "end": 6684.5, "text": " Why is that?", "tokens": [1545, 307, 300, 30], "temperature": 0.0, "avg_logprob": -0.5441176263909591, "compression_ratio": 1.5112359550561798, "no_speech_prob": 8.66423397383187e-06}, {"id": 1526, "seek": 667058, "start": 6686.58, "end": 6691.54, "text": " So one of the things I noticed was really annoying in the first way I wrote the callback", "tokens": [407, 472, 295, 264, 721, 286, 5694, 390, 534, 11304, 294, 264, 700, 636, 286, 4114, 264, 818, 3207], "temperature": 0.0, "avg_logprob": -0.5441176263909591, "compression_ratio": 1.5112359550561798, "no_speech_prob": 8.66423397383187e-06}, {"id": 1527, "seek": 667058, "start": 6691.54, "end": 6699.14, "text": " handler was I had it so that I could get the callback to work.", "tokens": [41967, 390, 286, 632, 309, 370, 300, 286, 727, 483, 264, 818, 3207, 281, 589, 13], "temperature": 0.0, "avg_logprob": -0.5441176263909591, "compression_ratio": 1.5112359550561798, "no_speech_prob": 8.66423397383187e-06}, {"id": 1528, "seek": 669914, "start": 6699.14, "end": 6707.46, "text": " So something had to return true to mean keep going.", "tokens": [407, 746, 632, 281, 2736, 2074, 281, 914, 1066, 516, 13], "temperature": 0.0, "avg_logprob": -0.10053629098936569, "compression_ratio": 1.6614583333333333, "no_speech_prob": 5.255237738310825e-06}, {"id": 1529, "seek": 669914, "start": 6707.46, "end": 6709.9400000000005, "text": " So basically false meant stop.", "tokens": [407, 1936, 7908, 4140, 1590, 13], "temperature": 0.0, "avg_logprob": -0.10053629098936569, "compression_ratio": 1.6614583333333333, "no_speech_prob": 5.255237738310825e-06}, {"id": 1530, "seek": 669914, "start": 6709.9400000000005, "end": 6715.9400000000005, "text": " And that was really awkward because if you don't add a return in Python, then it actually", "tokens": [400, 300, 390, 534, 11411, 570, 498, 291, 500, 380, 909, 257, 2736, 294, 15329, 11, 550, 309, 767], "temperature": 0.0, "avg_logprob": -0.10053629098936569, "compression_ratio": 1.6614583333333333, "no_speech_prob": 5.255237738310825e-06}, {"id": 1531, "seek": 669914, "start": 6715.9400000000005, "end": 6716.660000000001, "text": " returns none.", "tokens": [11247, 6022, 13], "temperature": 0.0, "avg_logprob": -0.10053629098936569, "compression_ratio": 1.6614583333333333, "no_speech_prob": 5.255237738310825e-06}, {"id": 1532, "seek": 669914, "start": 6717.3, "end": 6718.34, "text": " And none is false.", "tokens": [400, 6022, 307, 7908, 13], "temperature": 0.0, "avg_logprob": -0.10053629098936569, "compression_ratio": 1.6614583333333333, "no_speech_prob": 5.255237738310825e-06}, {"id": 1533, "seek": 669914, "start": 6718.9800000000005, "end": 6723.14, "text": " And I thought, oh, if I forget to return something, that should mean keep going.", "tokens": [400, 286, 1194, 11, 1954, 11, 498, 286, 2870, 281, 2736, 746, 11, 300, 820, 914, 1066, 516, 13], "temperature": 0.0, "avg_logprob": -0.10053629098936569, "compression_ratio": 1.6614583333333333, "no_speech_prob": 5.255237738310825e-06}, {"id": 1534, "seek": 669914, "start": 6723.14, "end": 6724.42, "text": " That should be like the default.", "tokens": [663, 820, 312, 411, 264, 7576, 13], "temperature": 0.0, "avg_logprob": -0.10053629098936569, "compression_ratio": 1.6614583333333333, "no_speech_prob": 5.255237738310825e-06}, {"id": 1535, "seek": 672442, "start": 6724.42, "end": 6734.9800000000005, "text": " So the first thing to point out is that the basic loop now actually says if not rather", "tokens": [407, 264, 700, 551, 281, 935, 484, 307, 300, 264, 3875, 6367, 586, 767, 1619, 498, 406, 2831], "temperature": 0.0, "avg_logprob": -0.14295606975313985, "compression_ratio": 1.5567567567567568, "no_speech_prob": 2.7263965876045404e-06}, {"id": 1536, "seek": 672442, "start": 6734.9800000000005, "end": 6735.46, "text": " than if.", "tokens": [813, 498, 13], "temperature": 0.0, "avg_logprob": -0.14295606975313985, "compression_ratio": 1.5567567567567568, "no_speech_prob": 2.7263965876045404e-06}, {"id": 1537, "seek": 672442, "start": 6735.46, "end": 6736.26, "text": " Right?", "tokens": [1779, 30], "temperature": 0.0, "avg_logprob": -0.14295606975313985, "compression_ratio": 1.5567567567567568, "no_speech_prob": 2.7263965876045404e-06}, {"id": 1538, "seek": 672442, "start": 6736.26, "end": 6738.58, "text": " So if not, begin epoch.", "tokens": [407, 498, 406, 11, 1841, 30992, 339, 13], "temperature": 0.0, "avg_logprob": -0.14295606975313985, "compression_ratio": 1.5567567567567568, "no_speech_prob": 2.7263965876045404e-06}, {"id": 1539, "seek": 672442, "start": 6738.58, "end": 6744.18, "text": " So in other words, if your callback handler returns false, then keep going.", "tokens": [407, 294, 661, 2283, 11, 498, 428, 818, 3207, 41967, 11247, 7908, 11, 550, 1066, 516, 13], "temperature": 0.0, "avg_logprob": -0.14295606975313985, "compression_ratio": 1.5567567567567568, "no_speech_prob": 2.7263965876045404e-06}, {"id": 1540, "seek": 672442, "start": 6744.74, "end": 6750.9800000000005, "text": " And so that means that basically none of my callbacks need to return anything most of", "tokens": [400, 370, 300, 1355, 300, 1936, 6022, 295, 452, 818, 17758, 643, 281, 2736, 1340, 881, 295], "temperature": 0.0, "avg_logprob": -0.14295606975313985, "compression_ratio": 1.5567567567567568, "no_speech_prob": 2.7263965876045404e-06}, {"id": 1541, "seek": 675098, "start": 6750.98, "end": 6755.86, "text": " the time except for test callback, which returns true.", "tokens": [264, 565, 3993, 337, 1500, 818, 3207, 11, 597, 11247, 2074, 13], "temperature": 0.0, "avg_logprob": -0.11014957917042267, "compression_ratio": 1.5, "no_speech_prob": 4.222760708216811e-06}, {"id": 1542, "seek": 675098, "start": 6755.86, "end": 6757.86, "text": " So true means cancel.", "tokens": [407, 2074, 1355, 10373, 13], "temperature": 0.0, "avg_logprob": -0.11014957917042267, "compression_ratio": 1.5, "no_speech_prob": 4.222760708216811e-06}, {"id": 1543, "seek": 675098, "start": 6757.86, "end": 6758.58, "text": " It means stop.", "tokens": [467, 1355, 1590, 13], "temperature": 0.0, "avg_logprob": -0.11014957917042267, "compression_ratio": 1.5, "no_speech_prob": 4.222760708216811e-06}, {"id": 1544, "seek": 675098, "start": 6763.0599999999995, "end": 6774.419999999999, "text": " So if one of my callbacks says stop, then I mean, I could certainly imagine an argument", "tokens": [407, 498, 472, 295, 452, 818, 17758, 1619, 1590, 11, 550, 286, 914, 11, 286, 727, 3297, 3811, 364, 6770], "temperature": 0.0, "avg_logprob": -0.11014957917042267, "compression_ratio": 1.5, "no_speech_prob": 4.222760708216811e-06}, {"id": 1545, "seek": 675098, "start": 6774.419999999999, "end": 6775.219999999999, "text": " in either way.", "tokens": [294, 2139, 636, 13], "temperature": 0.0, "avg_logprob": -0.11014957917042267, "compression_ratio": 1.5, "no_speech_prob": 4.222760708216811e-06}, {"id": 1546, "seek": 675098, "start": 6775.219999999999, "end": 6780.0199999999995, "text": " But the way I thought it, if it says stop, let's just stop right now.", "tokens": [583, 264, 636, 286, 1194, 309, 11, 498, 309, 1619, 1590, 11, 718, 311, 445, 1590, 558, 586, 13], "temperature": 0.0, "avg_logprob": -0.11014957917042267, "compression_ratio": 1.5, "no_speech_prob": 4.222760708216811e-06}, {"id": 1547, "seek": 678002, "start": 6780.02, "end": 6780.660000000001, "text": " You know?", "tokens": [509, 458, 30], "temperature": 0.0, "avg_logprob": -0.30646749373969684, "compression_ratio": 1.577092511013216, "no_speech_prob": 3.446510845606099e-06}, {"id": 1548, "seek": 678002, "start": 6780.660000000001, "end": 6782.660000000001, "text": " Why do we need to run the other callbacks?", "tokens": [1545, 360, 321, 643, 281, 1190, 264, 661, 818, 17758, 30], "temperature": 0.0, "avg_logprob": -0.30646749373969684, "compression_ratio": 1.577092511013216, "no_speech_prob": 3.446510845606099e-06}, {"id": 1549, "seek": 678002, "start": 6782.660000000001, "end": 6785.860000000001, "text": " So if it says stop, then it returns stop.", "tokens": [407, 498, 309, 1619, 1590, 11, 550, 309, 11247, 1590, 13], "temperature": 0.0, "avg_logprob": -0.30646749373969684, "compression_ratio": 1.577092511013216, "no_speech_prob": 3.446510845606099e-06}, {"id": 1550, "seek": 678002, "start": 6785.860000000001, "end": 6787.860000000001, "text": " It says we don't want to go anymore.", "tokens": [467, 1619, 321, 500, 380, 528, 281, 352, 3602, 13], "temperature": 0.0, "avg_logprob": -0.30646749373969684, "compression_ratio": 1.577092511013216, "no_speech_prob": 3.446510845606099e-06}, {"id": 1551, "seek": 678002, "start": 6787.860000000001, "end": 6793.46, "text": " And then we can, depending on where you are, so if after epoch returns stop, then it's", "tokens": [400, 550, 321, 393, 11, 5413, 322, 689, 291, 366, 11, 370, 498, 934, 30992, 339, 11247, 1590, 11, 550, 309, 311], "temperature": 0.0, "avg_logprob": -0.30646749373969684, "compression_ratio": 1.577092511013216, "no_speech_prob": 3.446510845606099e-06}, {"id": 1552, "seek": 678002, "start": 6793.46, "end": 6797.06, "text": " actually going to stop the loop entirely.", "tokens": [767, 516, 281, 1590, 264, 6367, 7696, 13], "temperature": 0.0, "avg_logprob": -0.30646749373969684, "compression_ratio": 1.577092511013216, "no_speech_prob": 3.446510845606099e-06}, {"id": 1553, "seek": 678002, "start": 6798.26, "end": 6800.26, "text": " So that's why.", "tokens": [407, 300, 311, 983, 13], "temperature": 0.0, "avg_logprob": -0.30646749373969684, "compression_ratio": 1.577092511013216, "no_speech_prob": 3.446510845606099e-06}, {"id": 1554, "seek": 678002, "start": 6802.740000000001, "end": 6804.660000000001, "text": " Yeah, so this is a little awkward.", "tokens": [865, 11, 370, 341, 307, 257, 707, 11411, 13], "temperature": 0.0, "avg_logprob": -0.30646749373969684, "compression_ratio": 1.577092511013216, "no_speech_prob": 3.446510845606099e-06}, {"id": 1555, "seek": 678002, "start": 6804.660000000001, "end": 6808.02, "text": " We had to construct our average stats callback.", "tokens": [492, 632, 281, 7690, 527, 4274, 18152, 818, 3207, 13], "temperature": 0.0, "avg_logprob": -0.30646749373969684, "compression_ratio": 1.577092511013216, "no_speech_prob": 3.446510845606099e-06}, {"id": 1556, "seek": 680802, "start": 6808.02, "end": 6811.620000000001, "text": " And then we had to pass that to run.", "tokens": [400, 550, 321, 632, 281, 1320, 300, 281, 1190, 13], "temperature": 0.0, "avg_logprob": -0.15325997515422543, "compression_ratio": 1.4916201117318435, "no_speech_prob": 2.7264381969871465e-06}, {"id": 1557, "seek": 680802, "start": 6811.620000000001, "end": 6817.780000000001, "text": " And then later on, we can refer to stats.validStats.averageStats.", "tokens": [400, 550, 1780, 322, 11, 321, 393, 2864, 281, 18152, 13, 3337, 327, 4520, 1720, 13, 64, 3623, 4520, 1720, 13], "temperature": 0.0, "avg_logprob": -0.15325997515422543, "compression_ratio": 1.4916201117318435, "no_speech_prob": 2.7264381969871465e-06}, {"id": 1558, "seek": 680802, "start": 6817.780000000001, "end": 6820.580000000001, "text": " Because remember, averageStats was where we grabbed this.", "tokens": [1436, 1604, 11, 4274, 4520, 1720, 390, 689, 321, 18607, 341, 13], "temperature": 0.0, "avg_logprob": -0.15325997515422543, "compression_ratio": 1.4916201117318435, "no_speech_prob": 2.7264381969871465e-06}, {"id": 1559, "seek": 680802, "start": 6822.900000000001, "end": 6825.9400000000005, "text": " So that's okay, but it's a little awkward.", "tokens": [407, 300, 311, 1392, 11, 457, 309, 311, 257, 707, 11411, 13], "temperature": 0.0, "avg_logprob": -0.15325997515422543, "compression_ratio": 1.4916201117318435, "no_speech_prob": 2.7264381969871465e-06}, {"id": 1560, "seek": 680802, "start": 6825.9400000000005, "end": 6831.620000000001, "text": " So instead, what I do is I create a accuracy callback function.", "tokens": [407, 2602, 11, 437, 286, 360, 307, 286, 1884, 257, 14170, 818, 3207, 2445, 13], "temperature": 0.0, "avg_logprob": -0.15325997515422543, "compression_ratio": 1.4916201117318435, "no_speech_prob": 2.7264381969871465e-06}, {"id": 1561, "seek": 683162, "start": 6831.62, "end": 6840.9, "text": " So that is the averageStats callback constructor passing in accuracy, but with partial.", "tokens": [407, 300, 307, 264, 4274, 4520, 1720, 818, 3207, 47479, 8437, 294, 14170, 11, 457, 365, 14641, 13], "temperature": 0.0, "avg_logprob": -0.33891684668404715, "compression_ratio": 1.6344086021505377, "no_speech_prob": 2.4824666979839094e-06}, {"id": 1562, "seek": 683162, "start": 6840.9, "end": 6844.74, "text": " So partial is a function that returns a function.", "tokens": [407, 14641, 307, 257, 2445, 300, 11247, 257, 2445, 13], "temperature": 0.0, "avg_logprob": -0.33891684668404715, "compression_ratio": 1.6344086021505377, "no_speech_prob": 2.4824666979839094e-06}, {"id": 1563, "seek": 683162, "start": 6845.94, "end": 6851.3, "text": " And so this is now a function which can create a callback.", "tokens": [400, 370, 341, 307, 586, 257, 2445, 597, 393, 1884, 257, 818, 3207, 13], "temperature": 0.0, "avg_logprob": -0.33891684668404715, "compression_ratio": 1.6344086021505377, "no_speech_prob": 2.4824666979839094e-06}, {"id": 1564, "seek": 683162, "start": 6851.3, "end": 6854.82, "text": " And so I can pass this to cbfuncs.", "tokens": [400, 370, 286, 393, 1320, 341, 281, 269, 65, 15930, 14368, 13], "temperature": 0.0, "avg_logprob": -0.33891684668404715, "compression_ratio": 1.6344086021505377, "no_speech_prob": 2.4824666979839094e-06}, {"id": 1565, "seek": 683162, "start": 6855.38, "end": 6860.82, "text": " And now I don't have to store it away, because the runner is going to be", "tokens": [400, 586, 286, 500, 380, 362, 281, 3531, 309, 1314, 11, 570, 264, 24376, 307, 516, 281, 312], "temperature": 0.0, "avg_logprob": -0.33891684668404715, "compression_ratio": 1.6344086021505377, "no_speech_prob": 2.4824666979839094e-06}, {"id": 1566, "seek": 686082, "start": 6860.82, "end": 6862.9, "text": " able to, this is what we saw before.", "tokens": [1075, 281, 11, 341, 307, 437, 321, 1866, 949, 13], "temperature": 0.0, "avg_logprob": -0.20454306554312657, "compression_ratio": 1.7783251231527093, "no_speech_prob": 4.592101561229356e-07}, {"id": 1567, "seek": 686082, "start": 6862.9, "end": 6870.0199999999995, "text": " The runner will go through hcbfuncs, it will call that function to create the callback,", "tokens": [440, 24376, 486, 352, 807, 276, 66, 65, 15930, 14368, 11, 309, 486, 818, 300, 2445, 281, 1884, 264, 818, 3207, 11], "temperature": 0.0, "avg_logprob": -0.20454306554312657, "compression_ratio": 1.7783251231527093, "no_speech_prob": 4.592101561229356e-07}, {"id": 1568, "seek": 686082, "start": 6870.0199999999995, "end": 6876.34, "text": " and then it will stick that callback inside the runner, giving it this name as the attribute.", "tokens": [293, 550, 309, 486, 2897, 300, 818, 3207, 1854, 264, 24376, 11, 2902, 309, 341, 1315, 382, 264, 19667, 13], "temperature": 0.0, "avg_logprob": -0.20454306554312657, "compression_ratio": 1.7783251231527093, "no_speech_prob": 4.592101561229356e-07}, {"id": 1569, "seek": 686082, "start": 6877.94, "end": 6887.299999999999, "text": " So this way, we can say, this is our callback function, this is our runner, fit, and now", "tokens": [407, 341, 636, 11, 321, 393, 584, 11, 341, 307, 527, 818, 3207, 2445, 11, 341, 307, 527, 24376, 11, 3318, 11, 293, 586], "temperature": 0.0, "avg_logprob": -0.20454306554312657, "compression_ratio": 1.7783251231527093, "no_speech_prob": 4.592101561229356e-07}, {"id": 1570, "seek": 688730, "start": 6887.3, "end": 6890.900000000001, "text": " it's automatically available inside run.averageStats.", "tokens": [309, 311, 6772, 2435, 1854, 1190, 13, 64, 3623, 4520, 1720, 13], "temperature": 0.0, "avg_logprob": -0.10503933246319111, "compression_ratio": 1.6563573883161513, "no_speech_prob": 6.14404189036577e-06}, {"id": 1571, "seek": 688730, "start": 6890.900000000001, "end": 6896.1, "text": " So this is what FastAI v1 does, except it puts them inside a learner,", "tokens": [407, 341, 307, 437, 15968, 48698, 371, 16, 775, 11, 3993, 309, 8137, 552, 1854, 257, 33347, 11], "temperature": 0.0, "avg_logprob": -0.10503933246319111, "compression_ratio": 1.6563573883161513, "no_speech_prob": 6.14404189036577e-06}, {"id": 1572, "seek": 688730, "start": 6896.1, "end": 6898.18, "text": " because we don't have a runner concept.", "tokens": [570, 321, 500, 380, 362, 257, 24376, 3410, 13], "temperature": 0.0, "avg_logprob": -0.10503933246319111, "compression_ratio": 1.6563573883161513, "no_speech_prob": 6.14404189036577e-06}, {"id": 1573, "seek": 688730, "start": 6900.18, "end": 6901.78, "text": " So I think that's pretty handy.", "tokens": [407, 286, 519, 300, 311, 1238, 13239, 13], "temperature": 0.0, "avg_logprob": -0.10503933246319111, "compression_ratio": 1.6563573883161513, "no_speech_prob": 6.14404189036577e-06}, {"id": 1574, "seek": 688730, "start": 6901.78, "end": 6905.38, "text": " It's kind of like, it looks a little bit awkward the first time you do it,", "tokens": [467, 311, 733, 295, 411, 11, 309, 1542, 257, 707, 857, 11411, 264, 700, 565, 291, 360, 309, 11], "temperature": 0.0, "avg_logprob": -0.10503933246319111, "compression_ratio": 1.6563573883161513, "no_speech_prob": 6.14404189036577e-06}, {"id": 1575, "seek": 688730, "start": 6905.38, "end": 6911.46, "text": " but you can kind of create a standard set of callback functions that you want to use for", "tokens": [457, 291, 393, 733, 295, 1884, 257, 3832, 992, 295, 818, 3207, 6828, 300, 291, 528, 281, 764, 337], "temperature": 0.0, "avg_logprob": -0.10503933246319111, "compression_ratio": 1.6563573883161513, "no_speech_prob": 6.14404189036577e-06}, {"id": 1576, "seek": 688730, "start": 6911.46, "end": 6915.62, "text": " particular types of models, and then you can just store them away in a list,", "tokens": [1729, 3467, 295, 5245, 11, 293, 550, 291, 393, 445, 3531, 552, 1314, 294, 257, 1329, 11], "temperature": 0.0, "avg_logprob": -0.10503933246319111, "compression_ratio": 1.6563573883161513, "no_speech_prob": 6.14404189036577e-06}, {"id": 1577, "seek": 691562, "start": 6915.62, "end": 6919.38, "text": " and you don't have to think about them again, which is what you'll see we'll do lots of times.", "tokens": [293, 291, 500, 380, 362, 281, 519, 466, 552, 797, 11, 597, 307, 437, 291, 603, 536, 321, 603, 360, 3195, 295, 1413, 13], "temperature": 0.0, "avg_logprob": -0.10274616603193612, "compression_ratio": 1.7479674796747968, "no_speech_prob": 1.4283388736657798e-05}, {"id": 1578, "seek": 691562, "start": 6921.3, "end": 6929.14, "text": " So, like a lot of things in this part two of the course, you can choose how deep to go", "tokens": [407, 11, 411, 257, 688, 295, 721, 294, 341, 644, 732, 295, 264, 1164, 11, 291, 393, 2826, 577, 2452, 281, 352], "temperature": 0.0, "avg_logprob": -0.10274616603193612, "compression_ratio": 1.7479674796747968, "no_speech_prob": 1.4283388736657798e-05}, {"id": 1579, "seek": 691562, "start": 6929.14, "end": 6929.94, "text": " on different things.", "tokens": [322, 819, 721, 13], "temperature": 0.0, "avg_logprob": -0.10274616603193612, "compression_ratio": 1.7479674796747968, "no_speech_prob": 1.4283388736657798e-05}, {"id": 1580, "seek": 691562, "start": 6930.98, "end": 6935.3, "text": " I think our approach to callbacks is super interesting, and if you do too,", "tokens": [286, 519, 527, 3109, 281, 818, 17758, 307, 1687, 1880, 11, 293, 498, 291, 360, 886, 11], "temperature": 0.0, "avg_logprob": -0.10274616603193612, "compression_ratio": 1.7479674796747968, "no_speech_prob": 1.4283388736657798e-05}, {"id": 1581, "seek": 691562, "start": 6935.3, "end": 6941.94, "text": " you might want to go deep here and really look into what kind of callbacks you can build,", "tokens": [291, 1062, 528, 281, 352, 2452, 510, 293, 534, 574, 666, 437, 733, 295, 818, 17758, 291, 393, 1322, 11], "temperature": 0.0, "avg_logprob": -0.10274616603193612, "compression_ratio": 1.7479674796747968, "no_speech_prob": 1.4283388736657798e-05}, {"id": 1582, "seek": 691562, "start": 6941.94, "end": 6944.0199999999995, "text": " and what things you can do with them that we haven't done yet.", "tokens": [293, 437, 721, 291, 393, 360, 365, 552, 300, 321, 2378, 380, 1096, 1939, 13], "temperature": 0.0, "avg_logprob": -0.10274616603193612, "compression_ratio": 1.7479674796747968, "no_speech_prob": 1.4283388736657798e-05}, {"id": 1583, "seek": 694402, "start": 6944.02, "end": 6950.5, "text": " But then a lot of these details around exactly how I do this, if you're not as interested in the", "tokens": [583, 550, 257, 688, 295, 613, 4365, 926, 2293, 577, 286, 360, 341, 11, 498, 291, 434, 406, 382, 3102, 294, 264], "temperature": 0.0, "avg_logprob": -0.19768742764933725, "compression_ratio": 1.5434782608695652, "no_speech_prob": 8.664466804475524e-06}, {"id": 1584, "seek": 694402, "start": 6950.5, "end": 6955.14, "text": " details of software engineering, this might be something you care less about, which is fine.", "tokens": [4365, 295, 4722, 7043, 11, 341, 1062, 312, 746, 291, 1127, 1570, 466, 11, 597, 307, 2489, 13], "temperature": 0.0, "avg_logprob": -0.19768742764933725, "compression_ratio": 1.5434782608695652, "no_speech_prob": 8.664466804475524e-06}, {"id": 1585, "seek": 694402, "start": 6956.660000000001, "end": 6961.14, "text": " The main thing that everybody should take away is that.", "tokens": [440, 2135, 551, 300, 2201, 820, 747, 1314, 307, 300, 13], "temperature": 0.0, "avg_logprob": -0.19768742764933725, "compression_ratio": 1.5434782608695652, "no_speech_prob": 8.664466804475524e-06}, {"id": 1586, "seek": 694402, "start": 6961.14, "end": 6962.5, "text": " That's our training loop.", "tokens": [663, 311, 527, 3097, 6367, 13], "temperature": 0.0, "avg_logprob": -0.19768742764933725, "compression_ratio": 1.5434782608695652, "no_speech_prob": 8.664466804475524e-06}, {"id": 1587, "seek": 694402, "start": 6963.780000000001, "end": 6964.280000000001, "text": " Okay?", "tokens": [1033, 30], "temperature": 0.0, "avg_logprob": -0.19768742764933725, "compression_ratio": 1.5434782608695652, "no_speech_prob": 8.664466804475524e-06}, {"id": 1588, "seek": 694402, "start": 6966.34, "end": 6971.9400000000005, "text": " So the other stuff about exactly how do we create our average stats callback,", "tokens": [407, 264, 661, 1507, 466, 2293, 577, 360, 321, 1884, 527, 4274, 18152, 818, 3207, 11], "temperature": 0.0, "avg_logprob": -0.19768742764933725, "compression_ratio": 1.5434782608695652, "no_speech_prob": 8.664466804475524e-06}, {"id": 1589, "seek": 697194, "start": 6971.94, "end": 6978.74, "text": " and exactly what does Dundalk call do, are fairly minor details, but you should recognize", "tokens": [293, 2293, 437, 775, 413, 997, 667, 818, 360, 11, 366, 6457, 6696, 4365, 11, 457, 291, 820, 5521], "temperature": 0.0, "avg_logprob": -0.2784655545208905, "compression_ratio": 1.494949494949495, "no_speech_prob": 3.1196937925415114e-05}, {"id": 1590, "seek": 697194, "start": 6979.139999999999, "end": 6985.379999999999, "text": " that the fit function stores how many epochs we're doing, what learner we're working with,", "tokens": [300, 264, 3318, 2445, 9512, 577, 867, 30992, 28346, 321, 434, 884, 11, 437, 33347, 321, 434, 1364, 365, 11], "temperature": 0.0, "avg_logprob": -0.2784655545208905, "compression_ratio": 1.494949494949495, "no_speech_prob": 3.1196937925415114e-05}, {"id": 1591, "seek": 697194, "start": 6987.0599999999995, "end": 6991.78, "text": " calls each of the different callbacks at each time, right?", "tokens": [5498, 1184, 295, 264, 819, 818, 17758, 412, 1184, 565, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2784655545208905, "compression_ratio": 1.494949494949495, "no_speech_prob": 3.1196937925415114e-05}, {"id": 1592, "seek": 697194, "start": 6991.78, "end": 6996.5, "text": " And like I never remember which ones are at which place.", "tokens": [400, 411, 286, 1128, 1604, 597, 2306, 366, 412, 597, 1081, 13], "temperature": 0.0, "avg_logprob": -0.2784655545208905, "compression_ratio": 1.494949494949495, "no_speech_prob": 3.1196937925415114e-05}, {"id": 1593, "seek": 699650, "start": 6996.5, "end": 7002.1, "text": " If you go to docs.fast.ai, the callbacks documentation will show you.", "tokens": [759, 291, 352, 281, 45623, 13, 7011, 13, 1301, 11, 264, 818, 17758, 14333, 486, 855, 291, 13], "temperature": 0.0, "avg_logprob": -0.2545521570288617, "compression_ratio": 1.6442307692307692, "no_speech_prob": 6.681455602119968e-07}, {"id": 1594, "seek": 699650, "start": 7002.82, "end": 7007.54, "text": " Personally, I just always look at the source code, because it's just so easy to see exactly what", "tokens": [21079, 11, 286, 445, 1009, 574, 412, 264, 4009, 3089, 11, 570, 309, 311, 445, 370, 1858, 281, 536, 2293, 437], "temperature": 0.0, "avg_logprob": -0.2545521570288617, "compression_ratio": 1.6442307692307692, "no_speech_prob": 6.681455602119968e-07}, {"id": 1595, "seek": 699650, "start": 7008.02, "end": 7011.54, "text": " happens at each time, and exactly what's available at each time.", "tokens": [2314, 412, 1184, 565, 11, 293, 2293, 437, 311, 2435, 412, 1184, 565, 13], "temperature": 0.0, "avg_logprob": -0.2545521570288617, "compression_ratio": 1.6442307692307692, "no_speech_prob": 6.681455602119968e-07}, {"id": 1596, "seek": 699650, "start": 7013.54, "end": 7015.06, "text": " So let's use this.", "tokens": [407, 718, 311, 764, 341, 13], "temperature": 0.0, "avg_logprob": -0.2545521570288617, "compression_ratio": 1.6442307692307692, "no_speech_prob": 6.681455602119968e-07}, {"id": 1597, "seek": 699650, "start": 7017.14, "end": 7023.3, "text": " And let's use this to do one cycle training, because it's pretty hard when you have to have", "tokens": [400, 718, 311, 764, 341, 281, 360, 472, 6586, 3097, 11, 570, 309, 311, 1238, 1152, 562, 291, 362, 281, 362], "temperature": 0.0, "avg_logprob": -0.2545521570288617, "compression_ratio": 1.6442307692307692, "no_speech_prob": 6.681455602119968e-07}, {"id": 1598, "seek": 702330, "start": 7023.3, "end": 7030.18, "text": " a constant learning rate all the time, particularly because I was really wanting to show you", "tokens": [257, 5754, 2539, 3314, 439, 264, 565, 11, 4098, 570, 286, 390, 534, 7935, 281, 855, 291], "temperature": 0.0, "avg_logprob": -0.26080092977970204, "compression_ratio": 1.669603524229075, "no_speech_prob": 7.646311132702976e-06}, {"id": 1599, "seek": 702330, "start": 7031.38, "end": 7036.74, "text": " like a deep dive, which you're about to see using hooks, a deep dive into how", "tokens": [411, 257, 2452, 9192, 11, 597, 291, 434, 466, 281, 536, 1228, 26485, 11, 257, 2452, 9192, 666, 577], "temperature": 0.0, "avg_logprob": -0.26080092977970204, "compression_ratio": 1.669603524229075, "no_speech_prob": 7.646311132702976e-06}, {"id": 1600, "seek": 702330, "start": 7037.78, "end": 7042.34, "text": " the mechanics or kind of how the dynamics of training models looks like.", "tokens": [264, 12939, 420, 733, 295, 577, 264, 15679, 295, 3097, 5245, 1542, 411, 13], "temperature": 0.0, "avg_logprob": -0.26080092977970204, "compression_ratio": 1.669603524229075, "no_speech_prob": 7.646311132702976e-06}, {"id": 1601, "seek": 702330, "start": 7042.820000000001, "end": 7047.14, "text": " And what we'll learn is that the first batches, everything,", "tokens": [400, 437, 321, 603, 1466, 307, 300, 264, 700, 15245, 279, 11, 1203, 11], "temperature": 0.0, "avg_logprob": -0.26080092977970204, "compression_ratio": 1.669603524229075, "no_speech_prob": 7.646311132702976e-06}, {"id": 1602, "seek": 702330, "start": 7047.14, "end": 7050.58, "text": " if you can get the first batches working well, then things will tend to be,", "tokens": [498, 291, 393, 483, 264, 700, 15245, 279, 1364, 731, 11, 550, 721, 486, 3928, 281, 312, 11], "temperature": 0.0, "avg_logprob": -0.26080092977970204, "compression_ratio": 1.669603524229075, "no_speech_prob": 7.646311132702976e-06}, {"id": 1603, "seek": 705058, "start": 7050.58, "end": 7053.0599999999995, "text": " then things will tend to be good.", "tokens": [550, 721, 486, 3928, 281, 312, 665, 13], "temperature": 0.0, "avg_logprob": -0.09834867908108619, "compression_ratio": 1.6550387596899225, "no_speech_prob": 1.5206247553578578e-05}, {"id": 1604, "seek": 705058, "start": 7053.0599999999995, "end": 7055.14, "text": " And this is how you can get super convergence.", "tokens": [400, 341, 307, 577, 291, 393, 483, 1687, 32181, 13], "temperature": 0.0, "avg_logprob": -0.09834867908108619, "compression_ratio": 1.6550387596899225, "no_speech_prob": 1.5206247553578578e-05}, {"id": 1605, "seek": 705058, "start": 7057.14, "end": 7063.3, "text": " So if you want your first batches to be good, it turns out that good annealing is critical.", "tokens": [407, 498, 291, 528, 428, 700, 15245, 279, 281, 312, 665, 11, 309, 4523, 484, 300, 665, 22256, 4270, 307, 4924, 13], "temperature": 0.0, "avg_logprob": -0.09834867908108619, "compression_ratio": 1.6550387596899225, "no_speech_prob": 1.5206247553578578e-05}, {"id": 1606, "seek": 705058, "start": 7064.0199999999995, "end": 7066.1, "text": " So let's do that right away.", "tokens": [407, 718, 311, 360, 300, 558, 1314, 13], "temperature": 0.0, "avg_logprob": -0.09834867908108619, "compression_ratio": 1.6550387596899225, "no_speech_prob": 1.5206247553578578e-05}, {"id": 1607, "seek": 705058, "start": 7066.1, "end": 7069.86, "text": " Let's set up good annealing, because we have the mechanics we need, because we have callbacks.", "tokens": [961, 311, 992, 493, 665, 22256, 4270, 11, 570, 321, 362, 264, 12939, 321, 643, 11, 570, 321, 362, 818, 17758, 13], "temperature": 0.0, "avg_logprob": -0.09834867908108619, "compression_ratio": 1.6550387596899225, "no_speech_prob": 1.5206247553578578e-05}, {"id": 1608, "seek": 705058, "start": 7069.86, "end": 7071.3, "text": " So we're inside 05 anneal.", "tokens": [407, 321, 434, 1854, 1958, 20, 22256, 304, 13], "temperature": 0.0, "avg_logprob": -0.09834867908108619, "compression_ratio": 1.6550387596899225, "no_speech_prob": 1.5206247553578578e-05}, {"id": 1609, "seek": 705058, "start": 7073.7, "end": 7074.5, "text": " We'll get our data.", "tokens": [492, 603, 483, 527, 1412, 13], "temperature": 0.0, "avg_logprob": -0.09834867908108619, "compression_ratio": 1.6550387596899225, "no_speech_prob": 1.5206247553578578e-05}, {"id": 1610, "seek": 705058, "start": 7074.5, "end": 7075.7, "text": " This is all the same as before.", "tokens": [639, 307, 439, 264, 912, 382, 949, 13], "temperature": 0.0, "avg_logprob": -0.09834867908108619, "compression_ratio": 1.6550387596899225, "no_speech_prob": 1.5206247553578578e-05}, {"id": 1611, "seek": 705058, "start": 7076.42, "end": 7078.42, "text": " Here's something to create a learner with one line.", "tokens": [1692, 311, 746, 281, 1884, 257, 33347, 365, 472, 1622, 13], "temperature": 0.0, "avg_logprob": -0.09834867908108619, "compression_ratio": 1.6550387596899225, "no_speech_prob": 1.5206247553578578e-05}, {"id": 1612, "seek": 707842, "start": 7078.42, "end": 7084.5, "text": " So let's create a learner with that same little model we had before and loss function our data.", "tokens": [407, 718, 311, 1884, 257, 33347, 365, 300, 912, 707, 2316, 321, 632, 949, 293, 4470, 2445, 527, 1412, 13], "temperature": 0.0, "avg_logprob": -0.21872921343202945, "compression_ratio": 1.7899159663865547, "no_speech_prob": 9.515963029116392e-06}, {"id": 1613, "seek": 707842, "start": 7084.5, "end": 7088.9800000000005, "text": " And we'll create a runner with our average stats callback.", "tokens": [400, 321, 603, 1884, 257, 24376, 365, 527, 4274, 18152, 818, 3207, 13], "temperature": 0.0, "avg_logprob": -0.21872921343202945, "compression_ratio": 1.7899159663865547, "no_speech_prob": 9.515963029116392e-06}, {"id": 1614, "seek": 707842, "start": 7090.1, "end": 7092.82, "text": " This defaulted to a learning rate of 0.5.", "tokens": [639, 7576, 292, 281, 257, 2539, 3314, 295, 1958, 13, 20, 13], "temperature": 0.0, "avg_logprob": -0.21872921343202945, "compression_ratio": 1.7899159663865547, "no_speech_prob": 9.515963029116392e-06}, {"id": 1615, "seek": 707842, "start": 7093.46, "end": 7095.46, "text": " Maybe we could try it with learning rate of 0.3.", "tokens": [2704, 321, 727, 853, 309, 365, 2539, 3314, 295, 1958, 13, 18, 13], "temperature": 0.0, "avg_logprob": -0.21872921343202945, "compression_ratio": 1.7899159663865547, "no_speech_prob": 9.515963029116392e-06}, {"id": 1616, "seek": 707842, "start": 7097.46, "end": 7101.22, "text": " It's pretty handy being able to like quickly create things with different learning rates.", "tokens": [467, 311, 1238, 13239, 885, 1075, 281, 411, 2661, 1884, 721, 365, 819, 2539, 6846, 13], "temperature": 0.0, "avg_logprob": -0.21872921343202945, "compression_ratio": 1.7899159663865547, "no_speech_prob": 9.515963029116392e-06}, {"id": 1617, "seek": 707842, "start": 7101.22, "end": 7107.06, "text": " So let's create a function that's just going to be partial get model with a learning rate.", "tokens": [407, 718, 311, 1884, 257, 2445, 300, 311, 445, 516, 281, 312, 14641, 483, 2316, 365, 257, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.21872921343202945, "compression_ratio": 1.7899159663865547, "no_speech_prob": 9.515963029116392e-06}, {"id": 1618, "seek": 710706, "start": 7107.06, "end": 7111.46, "text": " And so now we can just call get model func and pass the learning rate in.", "tokens": [400, 370, 586, 321, 393, 445, 818, 483, 2316, 1019, 66, 293, 1320, 264, 2539, 3314, 294, 13], "temperature": 0.0, "avg_logprob": -0.1195535659790039, "compression_ratio": 1.55, "no_speech_prob": 4.565838935377542e-06}, {"id": 1619, "seek": 710706, "start": 7111.46, "end": 7113.780000000001, "text": " And we'll immediately have something with a different learning rate.", "tokens": [400, 321, 603, 4258, 362, 746, 365, 257, 819, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.1195535659790039, "compression_ratio": 1.55, "no_speech_prob": 4.565838935377542e-06}, {"id": 1620, "seek": 710706, "start": 7116.5, "end": 7117.780000000001, "text": " Yes, tell me the question.", "tokens": [1079, 11, 980, 385, 264, 1168, 13], "temperature": 0.0, "avg_logprob": -0.1195535659790039, "compression_ratio": 1.55, "no_speech_prob": 4.565838935377542e-06}, {"id": 1621, "seek": 710706, "start": 7121.38, "end": 7123.700000000001, "text": " So what is your typical debugging process?", "tokens": [407, 437, 307, 428, 7476, 45592, 1399, 30], "temperature": 0.0, "avg_logprob": -0.1195535659790039, "compression_ratio": 1.55, "no_speech_prob": 4.565838935377542e-06}, {"id": 1622, "seek": 710706, "start": 7126.820000000001, "end": 7129.3, "text": " My debugging process is to use the debugger.", "tokens": [1222, 45592, 1399, 307, 281, 764, 264, 24083, 1321, 13], "temperature": 0.0, "avg_logprob": -0.1195535659790039, "compression_ratio": 1.55, "no_speech_prob": 4.565838935377542e-06}, {"id": 1623, "seek": 710706, "start": 7129.9400000000005, "end": 7135.46, "text": " So if I got an exception while I was running a cell,", "tokens": [407, 498, 286, 658, 364, 11183, 1339, 286, 390, 2614, 257, 2815, 11], "temperature": 0.0, "avg_logprob": -0.1195535659790039, "compression_ratio": 1.55, "no_speech_prob": 4.565838935377542e-06}, {"id": 1624, "seek": 713546, "start": 7135.46, "end": 7137.7, "text": " then I just go into the next cell and type %debug.", "tokens": [550, 286, 445, 352, 666, 264, 958, 2815, 293, 2010, 14189, 1479, 44455, 13], "temperature": 0.0, "avg_logprob": -0.11118328362180475, "compression_ratio": 1.630952380952381, "no_speech_prob": 1.1124830962216947e-05}, {"id": 1625, "seek": 713546, "start": 7138.34, "end": 7140.1, "text": " And that pops open the debugger.", "tokens": [400, 300, 16795, 1269, 264, 24083, 1321, 13], "temperature": 0.0, "avg_logprob": -0.11118328362180475, "compression_ratio": 1.630952380952381, "no_speech_prob": 1.1124830962216947e-05}, {"id": 1626, "seek": 713546, "start": 7140.1, "end": 7143.22, "text": " If things aren't working the way I expected, but it wasn't an exception,", "tokens": [759, 721, 3212, 380, 1364, 264, 636, 286, 5176, 11, 457, 309, 2067, 380, 364, 11183, 11], "temperature": 0.0, "avg_logprob": -0.11118328362180475, "compression_ratio": 1.630952380952381, "no_speech_prob": 1.1124830962216947e-05}, {"id": 1627, "seek": 713546, "start": 7143.22, "end": 7146.82, "text": " then I'll just add set underscore trace somewhere around the point I care about.", "tokens": [550, 286, 603, 445, 909, 992, 37556, 13508, 4079, 926, 264, 935, 286, 1127, 466, 13], "temperature": 0.0, "avg_logprob": -0.11118328362180475, "compression_ratio": 1.630952380952381, "no_speech_prob": 1.1124830962216947e-05}, {"id": 1628, "seek": 713546, "start": 7149.14, "end": 7150.02, "text": " That's about it.", "tokens": [663, 311, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.11118328362180475, "compression_ratio": 1.630952380952381, "no_speech_prob": 1.1124830962216947e-05}, {"id": 1629, "seek": 713546, "start": 7150.02, "end": 7154.34, "text": " Yeah, I find that works pretty well.", "tokens": [865, 11, 286, 915, 300, 1985, 1238, 731, 13], "temperature": 0.0, "avg_logprob": -0.11118328362180475, "compression_ratio": 1.630952380952381, "no_speech_prob": 1.1124830962216947e-05}, {"id": 1630, "seek": 713546, "start": 7156.34, "end": 7161.7, "text": " Most of the time, then it's just a case of looking at what's the shape of everything.", "tokens": [4534, 295, 264, 565, 11, 550, 309, 311, 445, 257, 1389, 295, 1237, 412, 437, 311, 264, 3909, 295, 1203, 13], "temperature": 0.0, "avg_logprob": -0.11118328362180475, "compression_ratio": 1.630952380952381, "no_speech_prob": 1.1124830962216947e-05}, {"id": 1631, "seek": 713546, "start": 7162.26, "end": 7163.78, "text": " And what does everything contain?", "tokens": [400, 437, 775, 1203, 5304, 30], "temperature": 0.0, "avg_logprob": -0.11118328362180475, "compression_ratio": 1.630952380952381, "no_speech_prob": 1.1124830962216947e-05}, {"id": 1632, "seek": 716378, "start": 7163.78, "end": 7166.58, "text": " Like a couple of objects in the batch.", "tokens": [1743, 257, 1916, 295, 6565, 294, 264, 15245, 13], "temperature": 0.0, "avg_logprob": -0.08838443756103516, "compression_ratio": 1.6097560975609757, "no_speech_prob": 1.0782576282508671e-05}, {"id": 1633, "seek": 716378, "start": 7166.58, "end": 7169.54, "text": " I normally find something's got nans or zeros or whatever.", "tokens": [286, 5646, 915, 746, 311, 658, 297, 599, 420, 35193, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.08838443756103516, "compression_ratio": 1.6097560975609757, "no_speech_prob": 1.0782576282508671e-05}, {"id": 1634, "seek": 716378, "start": 7171.78, "end": 7178.5, "text": " Yeah, it's really rare that using the debugger that I find debugging is that difficult.", "tokens": [865, 11, 309, 311, 534, 5892, 300, 1228, 264, 24083, 1321, 300, 286, 915, 45592, 307, 300, 2252, 13], "temperature": 0.0, "avg_logprob": -0.08838443756103516, "compression_ratio": 1.6097560975609757, "no_speech_prob": 1.0782576282508671e-05}, {"id": 1635, "seek": 716378, "start": 7179.3, "end": 7185.219999999999, "text": " If it is, then it's just a case of stepping away and questioning your assumptions.", "tokens": [759, 309, 307, 11, 550, 309, 311, 445, 257, 1389, 295, 16821, 1314, 293, 21257, 428, 17695, 13], "temperature": 0.0, "avg_logprob": -0.08838443756103516, "compression_ratio": 1.6097560975609757, "no_speech_prob": 1.0782576282508671e-05}, {"id": 1636, "seek": 716378, "start": 7185.219999999999, "end": 7190.74, "text": " But with the help of a debugger, all of the states right there in front of you,", "tokens": [583, 365, 264, 854, 295, 257, 24083, 1321, 11, 439, 295, 264, 4368, 558, 456, 294, 1868, 295, 291, 11], "temperature": 0.0, "avg_logprob": -0.08838443756103516, "compression_ratio": 1.6097560975609757, "no_speech_prob": 1.0782576282508671e-05}, {"id": 1637, "seek": 719074, "start": 7190.74, "end": 7195.94, "text": " which is one of the great things about PyTorch, is that it supports this kind of development.", "tokens": [597, 307, 472, 295, 264, 869, 721, 466, 9953, 51, 284, 339, 11, 307, 300, 309, 9346, 341, 733, 295, 3250, 13], "temperature": 0.0, "avg_logprob": -0.21187118061801843, "compression_ratio": 1.3333333333333333, "no_speech_prob": 8.26747691462515e-06}, {"id": 1638, "seek": 719074, "start": 7200.9, "end": 7201.4, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.21187118061801843, "compression_ratio": 1.3333333333333333, "no_speech_prob": 8.26747691462515e-06}, {"id": 1639, "seek": 719074, "start": 7203.54, "end": 7204.26, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.21187118061801843, "compression_ratio": 1.3333333333333333, "no_speech_prob": 8.26747691462515e-06}, {"id": 1640, "seek": 719074, "start": 7204.26, "end": 7215.46, "text": " So we're going to create a callback that's going to do hyperparameter scheduling.", "tokens": [407, 321, 434, 516, 281, 1884, 257, 818, 3207, 300, 311, 516, 281, 360, 9848, 2181, 335, 2398, 29055, 13], "temperature": 0.0, "avg_logprob": -0.21187118061801843, "compression_ratio": 1.3333333333333333, "no_speech_prob": 8.26747691462515e-06}, {"id": 1641, "seek": 721546, "start": 7215.46, "end": 7220.74, "text": " And so for this notebook, we're just going to do learning rate as a hyperparameter.", "tokens": [400, 370, 337, 341, 21060, 11, 321, 434, 445, 516, 281, 360, 2539, 3314, 382, 257, 9848, 2181, 335, 2398, 13], "temperature": 0.0, "avg_logprob": -0.19142954836609544, "compression_ratio": 1.5569620253164558, "no_speech_prob": 3.6687190458906116e-06}, {"id": 1642, "seek": 721546, "start": 7221.38, "end": 7228.1, "text": " But in the last 12 months, one of the really successful areas of research", "tokens": [583, 294, 264, 1036, 2272, 2493, 11, 472, 295, 264, 534, 4406, 3179, 295, 2132], "temperature": 0.0, "avg_logprob": -0.19142954836609544, "compression_ratio": 1.5569620253164558, "no_speech_prob": 3.6687190458906116e-06}, {"id": 1643, "seek": 721546, "start": 7228.1, "end": 7233.78, "text": " have been people pointing out that you can and should schedule everything.", "tokens": [362, 668, 561, 12166, 484, 300, 291, 393, 293, 820, 7567, 1203, 13], "temperature": 0.0, "avg_logprob": -0.19142954836609544, "compression_ratio": 1.5569620253164558, "no_speech_prob": 3.6687190458906116e-06}, {"id": 1644, "seek": 721546, "start": 7233.78, "end": 7238.82, "text": " Your dropout amount, what kind of data augmentation you do, weight decay,", "tokens": [2260, 3270, 346, 2372, 11, 437, 733, 295, 1412, 14501, 19631, 291, 360, 11, 3364, 21039, 11], "temperature": 0.0, "avg_logprob": -0.19142954836609544, "compression_ratio": 1.5569620253164558, "no_speech_prob": 3.6687190458906116e-06}, {"id": 1645, "seek": 721546, "start": 7239.62, "end": 7242.02, "text": " learning rate, momentum, everything.", "tokens": [2539, 3314, 11, 11244, 11, 1203, 13], "temperature": 0.0, "avg_logprob": -0.19142954836609544, "compression_ratio": 1.5569620253164558, "no_speech_prob": 3.6687190458906116e-06}, {"id": 1646, "seek": 721546, "start": 7242.02, "end": 7243.06, "text": " Which makes sense, right?", "tokens": [3013, 1669, 2020, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.19142954836609544, "compression_ratio": 1.5569620253164558, "no_speech_prob": 3.6687190458906116e-06}, {"id": 1647, "seek": 724306, "start": 7243.06, "end": 7247.700000000001, "text": " Because the other thing that we've been learning a lot about in the last 12 months", "tokens": [1436, 264, 661, 551, 300, 321, 600, 668, 2539, 257, 688, 466, 294, 264, 1036, 2272, 2493], "temperature": 0.0, "avg_logprob": -0.16548940462943834, "compression_ratio": 1.6476683937823835, "no_speech_prob": 2.332015810679877e-06}, {"id": 1648, "seek": 724306, "start": 7247.700000000001, "end": 7251.860000000001, "text": " is how as you train a model, it kind of goes through these different phases", "tokens": [307, 577, 382, 291, 3847, 257, 2316, 11, 309, 733, 295, 1709, 807, 613, 819, 18764], "temperature": 0.0, "avg_logprob": -0.16548940462943834, "compression_ratio": 1.6476683937823835, "no_speech_prob": 2.332015810679877e-06}, {"id": 1649, "seek": 724306, "start": 7252.660000000001, "end": 7259.780000000001, "text": " of the kind of weight landscapes, sorry, the loss function,", "tokens": [295, 264, 733, 295, 3364, 29822, 11, 2597, 11, 264, 4470, 2445, 11], "temperature": 0.0, "avg_logprob": -0.16548940462943834, "compression_ratio": 1.6476683937823835, "no_speech_prob": 2.332015810679877e-06}, {"id": 1650, "seek": 724306, "start": 7260.42, "end": 7267.860000000001, "text": " the loss landscapes of neural nets look very different at the start, in the middle, and at the end.", "tokens": [264, 4470, 29822, 295, 18161, 36170, 574, 588, 819, 412, 264, 722, 11, 294, 264, 2808, 11, 293, 412, 264, 917, 13], "temperature": 0.0, "avg_logprob": -0.16548940462943834, "compression_ratio": 1.6476683937823835, "no_speech_prob": 2.332015810679877e-06}, {"id": 1651, "seek": 726786, "start": 7267.86, "end": 7273.7, "text": " And so it's very unlikely that you would want the same hyperparameters throughout.", "tokens": [400, 370, 309, 311, 588, 17518, 300, 291, 576, 528, 264, 912, 9848, 2181, 335, 6202, 3710, 13], "temperature": 0.0, "avg_logprob": -0.20196095219364874, "compression_ratio": 1.6746987951807228, "no_speech_prob": 9.276312198380765e-07}, {"id": 1652, "seek": 726786, "start": 7273.7, "end": 7277.219999999999, "text": " So being able to schedule anything is super handy.", "tokens": [407, 885, 1075, 281, 7567, 1340, 307, 1687, 13239, 13], "temperature": 0.0, "avg_logprob": -0.20196095219364874, "compression_ratio": 1.6746987951807228, "no_speech_prob": 9.276312198380765e-07}, {"id": 1653, "seek": 726786, "start": 7278.58, "end": 7283.7, "text": " So we'll create a parameter scheduler callback, and you're just going to pass in a function,", "tokens": [407, 321, 603, 1884, 257, 13075, 12000, 260, 818, 3207, 11, 293, 291, 434, 445, 516, 281, 1320, 294, 257, 2445, 11], "temperature": 0.0, "avg_logprob": -0.20196095219364874, "compression_ratio": 1.6746987951807228, "no_speech_prob": 9.276312198380765e-07}, {"id": 1654, "seek": 726786, "start": 7284.66, "end": 7287.219999999999, "text": " right, and a parameter to schedule.", "tokens": [558, 11, 293, 257, 13075, 281, 7567, 13], "temperature": 0.0, "avg_logprob": -0.20196095219364874, "compression_ratio": 1.6746987951807228, "no_speech_prob": 9.276312198380765e-07}, {"id": 1655, "seek": 726786, "start": 7287.219999999999, "end": 7292.0199999999995, "text": " So we're going to be passing in LR, because LR is what PyTorch calls learning rate.", "tokens": [407, 321, 434, 516, 281, 312, 8437, 294, 441, 49, 11, 570, 441, 49, 307, 437, 9953, 51, 284, 339, 5498, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.20196095219364874, "compression_ratio": 1.6746987951807228, "no_speech_prob": 9.276312198380765e-07}, {"id": 1656, "seek": 726786, "start": 7292.0199999999995, "end": 7296.42, "text": " And then this function will be something which takes a single argument", "tokens": [400, 550, 341, 2445, 486, 312, 746, 597, 2516, 257, 2167, 6770], "temperature": 0.0, "avg_logprob": -0.20196095219364874, "compression_ratio": 1.6746987951807228, "no_speech_prob": 9.276312198380765e-07}, {"id": 1657, "seek": 729642, "start": 7296.42, "end": 7300.82, "text": " which is number of epochs divided by total epochs.", "tokens": [597, 307, 1230, 295, 30992, 28346, 6666, 538, 3217, 30992, 28346, 13], "temperature": 0.0, "avg_logprob": -0.23038438650277945, "compression_ratio": 1.7551867219917012, "no_speech_prob": 8.397833880735561e-06}, {"id": 1658, "seek": 729642, "start": 7300.82, "end": 7306.5, "text": " Remember I told you that that train eval callback we added is going to set this to be a float.", "tokens": [5459, 286, 1907, 291, 300, 300, 3847, 1073, 304, 818, 3207, 321, 3869, 307, 516, 281, 992, 341, 281, 312, 257, 15706, 13], "temperature": 0.0, "avg_logprob": -0.23038438650277945, "compression_ratio": 1.7551867219917012, "no_speech_prob": 8.397833880735561e-06}, {"id": 1659, "seek": 729642, "start": 7307.14, "end": 7312.34, "text": " So this will be the number, this will be like epoch number 2.35 out of 6.", "tokens": [407, 341, 486, 312, 264, 1230, 11, 341, 486, 312, 411, 30992, 339, 1230, 568, 13, 8794, 484, 295, 1386, 13], "temperature": 0.0, "avg_logprob": -0.23038438650277945, "compression_ratio": 1.7551867219917012, "no_speech_prob": 8.397833880735561e-06}, {"id": 1660, "seek": 729642, "start": 7312.34, "end": 7316.5, "text": " So this will be a float of exactly how far through training are we.", "tokens": [407, 341, 486, 312, 257, 15706, 295, 2293, 577, 1400, 807, 3097, 366, 321, 13], "temperature": 0.0, "avg_logprob": -0.23038438650277945, "compression_ratio": 1.7551867219917012, "no_speech_prob": 8.397833880735561e-06}, {"id": 1661, "seek": 729642, "start": 7317.06, "end": 7319.54, "text": " And we'll pass that to some function that we're going to write,", "tokens": [400, 321, 603, 1320, 300, 281, 512, 2445, 300, 321, 434, 516, 281, 2464, 11], "temperature": 0.0, "avg_logprob": -0.23038438650277945, "compression_ratio": 1.7551867219917012, "no_speech_prob": 8.397833880735561e-06}, {"id": 1662, "seek": 729642, "start": 7320.1, "end": 7324.58, "text": " and the result of that function will be used to set the hyperparameter.", "tokens": [293, 264, 1874, 295, 300, 2445, 486, 312, 1143, 281, 992, 264, 9848, 2181, 335, 2398, 13], "temperature": 0.0, "avg_logprob": -0.23038438650277945, "compression_ratio": 1.7551867219917012, "no_speech_prob": 8.397833880735561e-06}, {"id": 1663, "seek": 732458, "start": 7324.58, "end": 7327.62, "text": " In this case, learning rate.", "tokens": [682, 341, 1389, 11, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.1607691764831543, "compression_ratio": 1.5360824742268042, "no_speech_prob": 3.6118524349149084e-06}, {"id": 1664, "seek": 732458, "start": 7329.14, "end": 7335.46, "text": " As you know from part one, you don't necessarily want to have the same value of a hyperparameter", "tokens": [1018, 291, 458, 490, 644, 472, 11, 291, 500, 380, 4725, 528, 281, 362, 264, 912, 2158, 295, 257, 9848, 2181, 335, 2398], "temperature": 0.0, "avg_logprob": -0.1607691764831543, "compression_ratio": 1.5360824742268042, "no_speech_prob": 3.6118524349149084e-06}, {"id": 1665, "seek": 732458, "start": 7335.46, "end": 7336.42, "text": " for all of your layers.", "tokens": [337, 439, 295, 428, 7914, 13], "temperature": 0.0, "avg_logprob": -0.1607691764831543, "compression_ratio": 1.5360824742268042, "no_speech_prob": 3.6118524349149084e-06}, {"id": 1666, "seek": 732458, "start": 7337.3, "end": 7342.9, "text": " So PyTorch has something called parameter groups, which we use in abstraction we call", "tokens": [407, 9953, 51, 284, 339, 575, 746, 1219, 13075, 3935, 11, 597, 321, 764, 294, 37765, 321, 818], "temperature": 0.0, "avg_logprob": -0.1607691764831543, "compression_ratio": 1.5360824742268042, "no_speech_prob": 3.6118524349149084e-06}, {"id": 1667, "seek": 732458, "start": 7343.54, "end": 7346.9, "text": " layer groups in fast AI, but they're basically the same thing.", "tokens": [4583, 3935, 294, 2370, 7318, 11, 457, 436, 434, 1936, 264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.1607691764831543, "compression_ratio": 1.5360824742268042, "no_speech_prob": 3.6118524349149084e-06}, {"id": 1668, "seek": 734690, "start": 7346.9, "end": 7354.099999999999, "text": " And so a PyTorch optimizer contains a number of parameter groups.", "tokens": [400, 370, 257, 9953, 51, 284, 339, 5028, 6545, 8306, 257, 1230, 295, 13075, 3935, 13], "temperature": 0.0, "avg_logprob": -0.2618975152774733, "compression_ratio": 1.6559633027522935, "no_speech_prob": 1.733022827465902e-06}, {"id": 1669, "seek": 734690, "start": 7354.099999999999, "end": 7357.46, "text": " Unless you explicitly create more than one, it'll all be in one.", "tokens": [16581, 291, 20803, 1884, 544, 813, 472, 11, 309, 603, 439, 312, 294, 472, 13], "temperature": 0.0, "avg_logprob": -0.2618975152774733, "compression_ratio": 1.6559633027522935, "no_speech_prob": 1.733022827465902e-06}, {"id": 1670, "seek": 734690, "start": 7357.46, "end": 7360.82, "text": " But anytime we do stuff with hyperparameters, you have to loop through", "tokens": [583, 13038, 321, 360, 1507, 365, 9848, 2181, 335, 6202, 11, 291, 362, 281, 6367, 807], "temperature": 0.0, "avg_logprob": -0.2618975152774733, "compression_ratio": 1.6559633027522935, "no_speech_prob": 1.733022827465902e-06}, {"id": 1671, "seek": 734690, "start": 7361.54, "end": 7364.0199999999995, "text": " PG in self.opt.parameter groups.", "tokens": [40975, 294, 2698, 13, 5747, 13, 2181, 335, 2398, 3935, 13], "temperature": 0.0, "avg_logprob": -0.2618975152774733, "compression_ratio": 1.6559633027522935, "no_speech_prob": 1.733022827465902e-06}, {"id": 1672, "seek": 734690, "start": 7364.98, "end": 7370.0199999999995, "text": " So then parameter group, so learning rate for this parameter group, this layer group,", "tokens": [407, 550, 13075, 1594, 11, 370, 2539, 3314, 337, 341, 13075, 1594, 11, 341, 4583, 1594, 11], "temperature": 0.0, "avg_logprob": -0.2618975152774733, "compression_ratio": 1.6559633027522935, "no_speech_prob": 1.733022827465902e-06}, {"id": 1673, "seek": 734690, "start": 7370.74, "end": 7372.179999999999, "text": " is equal to the result of this function.", "tokens": [307, 2681, 281, 264, 1874, 295, 341, 2445, 13], "temperature": 0.0, "avg_logprob": -0.2618975152774733, "compression_ratio": 1.6559633027522935, "no_speech_prob": 1.733022827465902e-06}, {"id": 1674, "seek": 737218, "start": 7372.18, "end": 7380.18, "text": " And then every time we start a new batch, if we're training, then we'll run our scheduler.", "tokens": [400, 550, 633, 565, 321, 722, 257, 777, 15245, 11, 498, 321, 434, 3097, 11, 550, 321, 603, 1190, 527, 12000, 260, 13], "temperature": 0.0, "avg_logprob": -0.2548171661712311, "compression_ratio": 1.5934579439252337, "no_speech_prob": 2.1907703739998396e-06}, {"id": 1675, "seek": 737218, "start": 7381.22, "end": 7385.22, "text": " Pretty hard to know if our scheduler is working, if we can't actually see what's happening to the", "tokens": [10693, 1152, 281, 458, 498, 527, 12000, 260, 307, 1364, 11, 498, 321, 393, 380, 767, 536, 437, 311, 2737, 281, 264], "temperature": 0.0, "avg_logprob": -0.2548171661712311, "compression_ratio": 1.5934579439252337, "no_speech_prob": 2.1907703739998396e-06}, {"id": 1676, "seek": 737218, "start": 7385.22, "end": 7386.900000000001, "text": " learning rate as we go.", "tokens": [2539, 3314, 382, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.2548171661712311, "compression_ratio": 1.5934579439252337, "no_speech_prob": 2.1907703739998396e-06}, {"id": 1677, "seek": 737218, "start": 7387.38, "end": 7394.18, "text": " So let's create another callback called recorder that at the start of fit fitting sets the LRs", "tokens": [407, 718, 311, 1884, 1071, 818, 3207, 1219, 37744, 300, 412, 264, 722, 295, 3318, 15669, 6352, 264, 441, 49, 82], "temperature": 0.0, "avg_logprob": -0.2548171661712311, "compression_ratio": 1.5934579439252337, "no_speech_prob": 2.1907703739998396e-06}, {"id": 1678, "seek": 737218, "start": 7394.18, "end": 7396.34, "text": " and losses arrays to being empty.", "tokens": [293, 15352, 41011, 281, 885, 6707, 13], "temperature": 0.0, "avg_logprob": -0.2548171661712311, "compression_ratio": 1.5934579439252337, "no_speech_prob": 2.1907703739998396e-06}, {"id": 1679, "seek": 739634, "start": 7396.34, "end": 7403.46, "text": " And then after each batch, as long as you're training, it appends the current learning rate", "tokens": [400, 550, 934, 1184, 15245, 11, 382, 938, 382, 291, 434, 3097, 11, 309, 724, 2581, 264, 2190, 2539, 3314], "temperature": 0.0, "avg_logprob": -0.34306962149483816, "compression_ratio": 1.9429657794676807, "no_speech_prob": 5.338034497981425e-06}, {"id": 1680, "seek": 739634, "start": 7404.02, "end": 7405.46, "text": " and the current loss.", "tokens": [293, 264, 2190, 4470, 13], "temperature": 0.0, "avg_logprob": -0.34306962149483816, "compression_ratio": 1.9429657794676807, "no_speech_prob": 5.338034497981425e-06}, {"id": 1681, "seek": 739634, "start": 7405.46, "end": 7409.22, "text": " Now there's actually lots of learning rates potentially because there's lots of layer groups.", "tokens": [823, 456, 311, 767, 3195, 295, 2539, 6846, 7263, 570, 456, 311, 3195, 295, 4583, 3935, 13], "temperature": 0.0, "avg_logprob": -0.34306962149483816, "compression_ratio": 1.9429657794676807, "no_speech_prob": 5.338034497981425e-06}, {"id": 1682, "seek": 739634, "start": 7409.22, "end": 7415.62, "text": " So in fast AI, we tend to use the final layer group as the learning rate we actually print out.", "tokens": [407, 294, 2370, 7318, 11, 321, 3928, 281, 764, 264, 2572, 4583, 1594, 382, 264, 2539, 3314, 321, 767, 4482, 484, 13], "temperature": 0.0, "avg_logprob": -0.34306962149483816, "compression_ratio": 1.9429657794676807, "no_speech_prob": 5.338034497981425e-06}, {"id": 1683, "seek": 739634, "start": 7416.42, "end": 7417.62, "text": " But you don't have to do it that way.", "tokens": [583, 291, 500, 380, 362, 281, 360, 309, 300, 636, 13], "temperature": 0.0, "avg_logprob": -0.34306962149483816, "compression_ratio": 1.9429657794676807, "no_speech_prob": 5.338034497981425e-06}, {"id": 1684, "seek": 739634, "start": 7418.42, "end": 7421.7, "text": " And then we'll add something to plot the learning rates and we'll add something to plot the losses.", "tokens": [400, 550, 321, 603, 909, 746, 281, 7542, 264, 2539, 6846, 293, 321, 603, 909, 746, 281, 7542, 264, 15352, 13], "temperature": 0.0, "avg_logprob": -0.34306962149483816, "compression_ratio": 1.9429657794676807, "no_speech_prob": 5.338034497981425e-06}, {"id": 1685, "seek": 739634, "start": 7422.66, "end": 7425.46, "text": " So hopefully this looks like something that we can use in the future.", "tokens": [407, 4696, 341, 1542, 411, 746, 300, 321, 393, 764, 294, 264, 2027, 13], "temperature": 0.0, "avg_logprob": -0.34306962149483816, "compression_ratio": 1.9429657794676807, "no_speech_prob": 5.338034497981425e-06}, {"id": 1686, "seek": 742546, "start": 7425.46, "end": 7428.9800000000005, "text": " So hopefully this looks pretty familiar compared to the recorder in fast AI v1.", "tokens": [407, 4696, 341, 1542, 1238, 4963, 5347, 281, 264, 37744, 294, 2370, 7318, 371, 16, 13], "temperature": 0.0, "avg_logprob": -0.10449263323908267, "compression_ratio": 1.6271186440677967, "no_speech_prob": 5.954940661467845e-06}, {"id": 1687, "seek": 742546, "start": 7429.78, "end": 7437.3, "text": " So with that in place, we now need to create a function that takes the percentage through", "tokens": [407, 365, 300, 294, 1081, 11, 321, 586, 643, 281, 1884, 257, 2445, 300, 2516, 264, 9668, 807], "temperature": 0.0, "avg_logprob": -0.10449263323908267, "compression_ratio": 1.6271186440677967, "no_speech_prob": 5.954940661467845e-06}, {"id": 1688, "seek": 742546, "start": 7438.66, "end": 7444.74, "text": " the learning, which we're going to call pause for position, and returns the value of learning rate.", "tokens": [264, 2539, 11, 597, 321, 434, 516, 281, 818, 10465, 337, 2535, 11, 293, 11247, 264, 2158, 295, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.10449263323908267, "compression_ratio": 1.6271186440677967, "no_speech_prob": 5.954940661467845e-06}, {"id": 1689, "seek": 742546, "start": 7445.38, "end": 7448.1, "text": " And so let's create one for linear schedules.", "tokens": [400, 370, 718, 311, 1884, 472, 337, 8213, 28078, 13], "temperature": 0.0, "avg_logprob": -0.10449263323908267, "compression_ratio": 1.6271186440677967, "no_speech_prob": 5.954940661467845e-06}, {"id": 1690, "seek": 744810, "start": 7448.1, "end": 7455.38, "text": " So what we want to be able to pass this is a starting learning rate and an ending learning rate.", "tokens": [407, 437, 321, 528, 281, 312, 1075, 281, 1320, 341, 307, 257, 2891, 2539, 3314, 293, 364, 8121, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.2341144773695204, "compression_ratio": 1.7309644670050761, "no_speech_prob": 9.51599758991506e-06}, {"id": 1691, "seek": 744810, "start": 7455.38, "end": 7461.38, "text": " So we might pass it 10 and 1, and it would start at a learning rate of 10 and go down to 1.", "tokens": [407, 321, 1062, 1320, 309, 1266, 293, 502, 11, 293, 309, 576, 722, 412, 257, 2539, 3314, 295, 1266, 293, 352, 760, 281, 502, 13], "temperature": 0.0, "avg_logprob": -0.2341144773695204, "compression_ratio": 1.7309644670050761, "no_speech_prob": 9.51599758991506e-06}, {"id": 1692, "seek": 744810, "start": 7461.38, "end": 7463.38, "text": " That would be ridiculously high, but whatever.", "tokens": [663, 576, 312, 41358, 1090, 11, 457, 2035, 13], "temperature": 0.0, "avg_logprob": -0.2341144773695204, "compression_ratio": 1.7309644670050761, "no_speech_prob": 9.51599758991506e-06}, {"id": 1693, "seek": 744810, "start": 7466.02, "end": 7468.58, "text": " But we need a function that just takes position.", "tokens": [583, 321, 643, 257, 2445, 300, 445, 2516, 2535, 13], "temperature": 0.0, "avg_logprob": -0.2341144773695204, "compression_ratio": 1.7309644670050761, "no_speech_prob": 9.51599758991506e-06}, {"id": 1694, "seek": 744810, "start": 7469.14, "end": 7471.54, "text": " So this is a function that's going to return a function.", "tokens": [407, 341, 307, 257, 2445, 300, 311, 516, 281, 2736, 257, 2445, 13], "temperature": 0.0, "avg_logprob": -0.2341144773695204, "compression_ratio": 1.7309644670050761, "no_speech_prob": 9.51599758991506e-06}, {"id": 1695, "seek": 747154, "start": 7471.54, "end": 7478.42, "text": " So here's a function that takes a start learning rate and an end learning rate and a position", "tokens": [407, 510, 311, 257, 2445, 300, 2516, 257, 722, 2539, 3314, 293, 364, 917, 2539, 3314, 293, 257, 2535], "temperature": 0.0, "avg_logprob": -0.26437761942545573, "compression_ratio": 1.8411764705882352, "no_speech_prob": 4.9369291446055286e-06}, {"id": 1696, "seek": 747154, "start": 7479.3, "end": 7480.58, "text": " and returns the learning rate.", "tokens": [293, 11247, 264, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.26437761942545573, "compression_ratio": 1.8411764705882352, "no_speech_prob": 4.9369291446055286e-06}, {"id": 1697, "seek": 747154, "start": 7480.58, "end": 7482.98, "text": " So to start plus position times difference.", "tokens": [407, 281, 722, 1804, 2535, 1413, 2649, 13], "temperature": 0.0, "avg_logprob": -0.26437761942545573, "compression_ratio": 1.8411764705882352, "no_speech_prob": 4.9369291446055286e-06}, {"id": 1698, "seek": 747154, "start": 7484.98, "end": 7490.18, "text": " So to convert that function into one which only takes position,", "tokens": [407, 281, 7620, 300, 2445, 666, 472, 597, 787, 2516, 2535, 11], "temperature": 0.0, "avg_logprob": -0.26437761942545573, "compression_ratio": 1.8411764705882352, "no_speech_prob": 4.9369291446055286e-06}, {"id": 1699, "seek": 747154, "start": 7491.22, "end": 7496.98, "text": " we do partial, passing in that function and the start and the end we were given.", "tokens": [321, 360, 14641, 11, 8437, 294, 300, 2445, 293, 264, 722, 293, 264, 917, 321, 645, 2212, 13], "temperature": 0.0, "avg_logprob": -0.26437761942545573, "compression_ratio": 1.8411764705882352, "no_speech_prob": 4.9369291446055286e-06}, {"id": 1700, "seek": 749698, "start": 7496.98, "end": 7504.339999999999, "text": " So now this function just takes position because that's the only thing from inner that we haven't set.", "tokens": [407, 586, 341, 2445, 445, 2516, 2535, 570, 300, 311, 264, 787, 551, 490, 7284, 300, 321, 2378, 380, 992, 13], "temperature": 0.0, "avg_logprob": -0.22805428259151497, "compression_ratio": 1.6710526315789473, "no_speech_prob": 4.356831595941912e-06}, {"id": 1701, "seek": 749698, "start": 7507.299999999999, "end": 7512.5, "text": " So that's going to work fine, but it's inconvenient because we're going to create lots of different", "tokens": [407, 300, 311, 516, 281, 589, 2489, 11, 457, 309, 311, 46196, 570, 321, 434, 516, 281, 1884, 3195, 295, 819], "temperature": 0.0, "avg_logprob": -0.22805428259151497, "compression_ratio": 1.6710526315789473, "no_speech_prob": 4.356831595941912e-06}, {"id": 1702, "seek": 749698, "start": 7512.5, "end": 7515.94, "text": " schedulers, and I don't want to have to write all this every time.", "tokens": [12000, 433, 11, 293, 286, 500, 380, 528, 281, 362, 281, 2464, 439, 341, 633, 565, 13], "temperature": 0.0, "avg_logprob": -0.22805428259151497, "compression_ratio": 1.6710526315789473, "no_speech_prob": 4.356831595941912e-06}, {"id": 1703, "seek": 749698, "start": 7516.98, "end": 7522.259999999999, "text": " So we can simplify the way that you can create these by using a decorator.", "tokens": [407, 321, 393, 20460, 264, 636, 300, 291, 393, 1884, 613, 538, 1228, 257, 7919, 1639, 13], "temperature": 0.0, "avg_logprob": -0.22805428259151497, "compression_ratio": 1.6710526315789473, "no_speech_prob": 4.356831595941912e-06}, {"id": 1704, "seek": 749698, "start": 7522.259999999999, "end": 7523.94, "text": " Here's the version with a decorator.", "tokens": [1692, 311, 264, 3037, 365, 257, 7919, 1639, 13], "temperature": 0.0, "avg_logprob": -0.22805428259151497, "compression_ratio": 1.6710526315789473, "no_speech_prob": 4.356831595941912e-06}, {"id": 1705, "seek": 752394, "start": 7523.94, "end": 7531.299999999999, "text": " With a decorator, you create linear scheduler in the natural way.", "tokens": [2022, 257, 7919, 1639, 11, 291, 1884, 8213, 12000, 260, 294, 264, 3303, 636, 13], "temperature": 0.0, "avg_logprob": -0.33393388045461553, "compression_ratio": 1.994871794871795, "no_speech_prob": 1.1478574378998019e-05}, {"id": 1706, "seek": 752394, "start": 7531.299999999999, "end": 7536.099999999999, "text": " It's something that takes a start learning rate and end learning rate and a position and returns this.", "tokens": [467, 311, 746, 300, 2516, 257, 722, 2539, 3314, 293, 917, 2539, 3314, 293, 257, 2535, 293, 11247, 341, 13], "temperature": 0.0, "avg_logprob": -0.33393388045461553, "compression_ratio": 1.994871794871795, "no_speech_prob": 1.1478574378998019e-05}, {"id": 1707, "seek": 752394, "start": 7537.54, "end": 7544.419999999999, "text": " And then we add an annealer decorator, and the annealer decorator is the thing that does all this inner partial nonsense.", "tokens": [400, 550, 321, 909, 364, 22256, 17148, 7919, 1639, 11, 293, 264, 22256, 17148, 7919, 1639, 307, 264, 551, 300, 775, 439, 341, 7284, 14641, 14925, 13], "temperature": 0.0, "avg_logprob": -0.33393388045461553, "compression_ratio": 1.994871794871795, "no_speech_prob": 1.1478574378998019e-05}, {"id": 1708, "seek": 752394, "start": 7545.7, "end": 7546.5, "text": " What's a decorator?", "tokens": [708, 311, 257, 7919, 1639, 30], "temperature": 0.0, "avg_logprob": -0.33393388045461553, "compression_ratio": 1.994871794871795, "no_speech_prob": 1.1478574378998019e-05}, {"id": 1709, "seek": 754650, "start": 7546.5, "end": 7566.02, "text": " A decorator is a function that returns a function, and what Python does is if it sees the name of a function here with an at sign before it, then it takes this function, passes it into this function and replaces the definition of this function with whatever this returns.", "tokens": [316, 7919, 1639, 307, 257, 2445, 300, 11247, 257, 2445, 11, 293, 437, 15329, 775, 307, 498, 309, 8194, 264, 1315, 295, 257, 2445, 510, 365, 364, 412, 1465, 949, 309, 11, 550, 309, 2516, 341, 2445, 11, 11335, 309, 666, 341, 2445, 293, 46734, 264, 7123, 295, 341, 2445, 365, 2035, 341, 11247, 13], "temperature": 0.0, "avg_logprob": -0.25587761402130127, "compression_ratio": 1.8444444444444446, "no_speech_prob": 8.013307706278283e-06}, {"id": 1710, "seek": 754650, "start": 7567.14, "end": 7568.74, "text": " So it's going to take this.", "tokens": [407, 309, 311, 516, 281, 747, 341, 13], "temperature": 0.0, "avg_logprob": -0.25587761402130127, "compression_ratio": 1.8444444444444446, "no_speech_prob": 8.013307706278283e-06}, {"id": 1711, "seek": 754650, "start": 7569.78, "end": 7571.14, "text": " It's going to pass it over here.", "tokens": [467, 311, 516, 281, 1320, 309, 670, 510, 13], "temperature": 0.0, "avg_logprob": -0.25587761402130127, "compression_ratio": 1.8444444444444446, "no_speech_prob": 8.013307706278283e-06}, {"id": 1712, "seek": 757114, "start": 7571.14, "end": 7579.700000000001, "text": " And then it's going to say return inner where inner is partial as we described before.", "tokens": [400, 550, 309, 311, 516, 281, 584, 2736, 7284, 689, 7284, 307, 14641, 382, 321, 7619, 949, 13], "temperature": 0.0, "avg_logprob": -0.3739756364088792, "compression_ratio": 1.4620253164556962, "no_speech_prob": 1.994696503970772e-06}, {"id": 1713, "seek": 757114, "start": 7580.5, "end": 7581.700000000001, "text": " So let's see that.", "tokens": [407, 718, 311, 536, 300, 13], "temperature": 0.0, "avg_logprob": -0.3739756364088792, "compression_ratio": 1.4620253164556962, "no_speech_prob": 1.994696503970772e-06}, {"id": 1714, "seek": 757114, "start": 7583.9400000000005, "end": 7591.46, "text": " So now shed Lin we wrote it as taking start and end and pause, but if I hit shift tab.", "tokens": [407, 586, 14951, 9355, 321, 4114, 309, 382, 1940, 722, 293, 917, 293, 10465, 11, 457, 498, 286, 2045, 5513, 4421, 13], "temperature": 0.0, "avg_logprob": -0.3739756364088792, "compression_ratio": 1.4620253164556962, "no_speech_prob": 1.994696503970772e-06}, {"id": 1715, "seek": 757114, "start": 7593.38, "end": 7595.22, "text": " This says it only takes start and end.", "tokens": [639, 1619, 309, 787, 2516, 722, 293, 917, 13], "temperature": 0.0, "avg_logprob": -0.3739756364088792, "compression_ratio": 1.4620253164556962, "no_speech_prob": 1.994696503970772e-06}, {"id": 1716, "seek": 759522, "start": 7595.22, "end": 7600.66, "text": " Why is that because we've replaced this function with this function.", "tokens": [1545, 307, 300, 570, 321, 600, 10772, 341, 2445, 365, 341, 2445, 13], "temperature": 0.0, "avg_logprob": -0.510908589218602, "compression_ratio": 1.6589595375722543, "no_speech_prob": 5.954776497674175e-06}, {"id": 1717, "seek": 759522, "start": 7602.18, "end": 7604.1, "text": " And this function just takes that and end.", "tokens": [400, 341, 2445, 445, 2516, 300, 293, 917, 13], "temperature": 0.0, "avg_logprob": -0.510908589218602, "compression_ratio": 1.6589595375722543, "no_speech_prob": 5.954776497674175e-06}, {"id": 1718, "seek": 759522, "start": 7604.9800000000005, "end": 7623.38, "text": " And this is where Jupiter is going to give you a lot more happy times and pretty much any IDE because this kind of dynamic code generation is going to be a lot more important.", "tokens": [400, 341, 307, 689, 24567, 307, 516, 281, 976, 291, 257, 688, 544, 2055, 1413, 293, 1238, 709, 604, 40930, 570, 341, 733, 295, 8546, 3089, 5125, 307, 516, 281, 312, 257, 688, 544, 1021, 13], "temperature": 0.0, "avg_logprob": -0.510908589218602, "compression_ratio": 1.6589595375722543, "no_speech_prob": 5.954776497674175e-06}, {"id": 1719, "seek": 762338, "start": 7623.38, "end": 7636.42, "text": " Code generation it's pretty hard for an IDE to do that for you or else in Jupiter. It's actually running the code in an actual Python process, so it knows exactly what shed Lin means.", "tokens": [15549, 5125, 309, 311, 1238, 1152, 337, 364, 40930, 281, 360, 300, 337, 291, 420, 1646, 294, 24567, 13, 467, 311, 767, 2614, 264, 3089, 294, 364, 3539, 15329, 1399, 11, 370, 309, 3255, 2293, 437, 14951, 9355, 1355, 13], "temperature": 0.0, "avg_logprob": -0.2569204180428151, "compression_ratio": 1.537037037037037, "no_speech_prob": 1.3845519788446836e-05}, {"id": 1720, "seek": 762338, "start": 7636.9800000000005, "end": 7641.54, "text": " OK, so this is now created a function that takes start.", "tokens": [2264, 11, 370, 341, 307, 586, 2942, 257, 2445, 300, 2516, 722, 13], "temperature": 0.0, "avg_logprob": -0.2569204180428151, "compression_ratio": 1.537037037037037, "no_speech_prob": 1.3845519788446836e-05}, {"id": 1721, "seek": 762338, "start": 7642.02, "end": 7645.14, "text": " And end and returns a function.", "tokens": [400, 917, 293, 11247, 257, 2445, 13], "temperature": 0.0, "avg_logprob": -0.2569204180428151, "compression_ratio": 1.537037037037037, "no_speech_prob": 1.3845519788446836e-05}, {"id": 1722, "seek": 762338, "start": 7645.62, "end": 7646.66, "text": " Which takes.", "tokens": [3013, 2516, 13], "temperature": 0.0, "avg_logprob": -0.2569204180428151, "compression_ratio": 1.537037037037037, "no_speech_prob": 1.3845519788446836e-05}, {"id": 1723, "seek": 762338, "start": 7647.38, "end": 7649.46, "text": " Pause which is what we need.", "tokens": [31973, 597, 307, 437, 321, 643, 13], "temperature": 0.0, "avg_logprob": -0.2569204180428151, "compression_ratio": 1.537037037037037, "no_speech_prob": 1.3845519788446836e-05}, {"id": 1724, "seek": 762338, "start": 7650.1, "end": 7651.06, "text": " For our scheduler.", "tokens": [1171, 527, 12000, 260, 13], "temperature": 0.0, "avg_logprob": -0.2569204180428151, "compression_ratio": 1.537037037037037, "no_speech_prob": 1.3845519788446836e-05}, {"id": 1725, "seek": 765106, "start": 7651.06, "end": 7652.34, "text": " So let's try it.", "tokens": [407, 718, 311, 853, 309, 13], "temperature": 0.0, "avg_logprob": -0.25886835370744976, "compression_ratio": 1.553648068669528, "no_speech_prob": 5.682154551323038e-06}, {"id": 1726, "seek": 765106, "start": 7653.14, "end": 7664.580000000001, "text": " Let's say F equals shed Lin 1, 2, so this is a scheduler that starts at learning rate 1 ends at learning rate 2 and then we'll say hey what should that be 30% of the way through training.", "tokens": [961, 311, 584, 479, 6915, 14951, 9355, 502, 11, 568, 11, 370, 341, 307, 257, 12000, 260, 300, 3719, 412, 2539, 3314, 502, 5314, 412, 2539, 3314, 568, 293, 550, 321, 603, 584, 4177, 437, 820, 300, 312, 2217, 4, 295, 264, 636, 807, 3097, 13], "temperature": 0.0, "avg_logprob": -0.25886835370744976, "compression_ratio": 1.553648068669528, "no_speech_prob": 5.682154551323038e-06}, {"id": 1727, "seek": 765106, "start": 7665.9400000000005, "end": 7668.9800000000005, "text": " And again if I hit shift tab here.", "tokens": [400, 797, 498, 286, 2045, 5513, 4421, 510, 13], "temperature": 0.0, "avg_logprob": -0.25886835370744976, "compression_ratio": 1.553648068669528, "no_speech_prob": 5.682154551323038e-06}, {"id": 1728, "seek": 765106, "start": 7669.860000000001, "end": 7677.3, "text": " It knows that F is something that takes pause right so it's it's really nice in Jupiter you can you can take advantage of.", "tokens": [467, 3255, 300, 479, 307, 746, 300, 2516, 10465, 558, 370, 309, 311, 309, 311, 534, 1481, 294, 24567, 291, 393, 291, 393, 747, 5002, 295, 13], "temperature": 0.0, "avg_logprob": -0.25886835370744976, "compression_ratio": 1.553648068669528, "no_speech_prob": 5.682154551323038e-06}, {"id": 1729, "seek": 767730, "start": 7677.3, "end": 7682.1, "text": " Python's dynamic nature.", "tokens": [15329, 311, 8546, 3687, 13], "temperature": 0.0, "avg_logprob": -0.27605536965762867, "compression_ratio": 1.6847290640394088, "no_speech_prob": 9.665473953646142e-06}, {"id": 1730, "seek": 767730, "start": 7682.820000000001, "end": 7683.38, "text": " And like.", "tokens": [400, 411, 13], "temperature": 0.0, "avg_logprob": -0.27605536965762867, "compression_ratio": 1.6847290640394088, "no_speech_prob": 9.665473953646142e-06}, {"id": 1731, "seek": 767730, "start": 7684.02, "end": 7689.38, "text": " There's no point using a dynamic language if you're not taking advantage of his dynamic nature right so things like.", "tokens": [821, 311, 572, 935, 1228, 257, 8546, 2856, 498, 291, 434, 406, 1940, 5002, 295, 702, 8546, 3687, 558, 370, 721, 411, 13], "temperature": 0.0, "avg_logprob": -0.27605536965762867, "compression_ratio": 1.6847290640394088, "no_speech_prob": 9.665473953646142e-06}, {"id": 1732, "seek": 767730, "start": 7690.58, "end": 7693.9400000000005, "text": " Decorators are super convenient way to do this stuff.", "tokens": [12427, 284, 3391, 366, 1687, 10851, 636, 281, 360, 341, 1507, 13], "temperature": 0.0, "avg_logprob": -0.27605536965762867, "compression_ratio": 1.6847290640394088, "no_speech_prob": 9.665473953646142e-06}, {"id": 1733, "seek": 767730, "start": 7694.9800000000005, "end": 7702.18, "text": " There are other languages like Julia that can do similar things with macros like it's this is not the only way to get this kind of nice.", "tokens": [821, 366, 661, 8650, 411, 18551, 300, 393, 360, 2531, 721, 365, 7912, 2635, 411, 309, 311, 341, 307, 406, 264, 787, 636, 281, 483, 341, 733, 295, 1481, 13], "temperature": 0.0, "avg_logprob": -0.27605536965762867, "compression_ratio": 1.6847290640394088, "no_speech_prob": 9.665473953646142e-06}, {"id": 1734, "seek": 770218, "start": 7702.18, "end": 7706.820000000001, "text": " A very expressive ability but it's one good way to do it.", "tokens": [316, 588, 40189, 3485, 457, 309, 311, 472, 665, 636, 281, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.26639075777423915, "compression_ratio": 1.5271739130434783, "no_speech_prob": 7.766673661535606e-06}, {"id": 1735, "seek": 770218, "start": 7707.860000000001, "end": 7721.22, "text": " So now we can just go ahead and define all of our different schedule is by passing it each is start and pause so, for example, no schedule is something which always return start or cosine scheduling.", "tokens": [407, 586, 321, 393, 445, 352, 2286, 293, 6964, 439, 295, 527, 819, 7567, 307, 538, 8437, 309, 1184, 307, 722, 293, 10465, 370, 11, 337, 1365, 11, 572, 7567, 307, 746, 597, 1009, 2736, 722, 420, 23565, 29055, 13], "temperature": 0.0, "avg_logprob": -0.26639075777423915, "compression_ratio": 1.5271739130434783, "no_speech_prob": 7.766673661535606e-06}, {"id": 1736, "seek": 770218, "start": 7722.26, "end": 7723.3, "text": " Exponential scheduling.", "tokens": [21391, 266, 2549, 29055, 13], "temperature": 0.0, "avg_logprob": -0.26639075777423915, "compression_ratio": 1.5271739130434783, "no_speech_prob": 7.766673661535606e-06}, {"id": 1737, "seek": 772330, "start": 7723.3, "end": 7727.14, "text": " So.", "tokens": [407, 13], "temperature": 0.0, "avg_logprob": -0.3421842835166238, "compression_ratio": 1.308411214953271, "no_speech_prob": 6.143789050838677e-06}, {"id": 1738, "seek": 772330, "start": 7728.58, "end": 7729.22, "text": " So.", "tokens": [407, 13], "temperature": 0.0, "avg_logprob": -0.3421842835166238, "compression_ratio": 1.308411214953271, "no_speech_prob": 6.143789050838677e-06}, {"id": 1739, "seek": 772330, "start": 7730.02, "end": 7740.34, "text": " Let's define those and then let's try to plot them and it doesn't work why doesn't it work because you can't plot pie torch tensors.", "tokens": [961, 311, 6964, 729, 293, 550, 718, 311, 853, 281, 7542, 552, 293, 309, 1177, 380, 589, 983, 1177, 380, 309, 589, 570, 291, 393, 380, 7542, 1730, 27822, 10688, 830, 13], "temperature": 0.0, "avg_logprob": -0.3421842835166238, "compression_ratio": 1.308411214953271, "no_speech_prob": 6.143789050838677e-06}, {"id": 1740, "seek": 774034, "start": 7740.34, "end": 7752.66, "text": " But it turns out the only reason you can't plot pie torch tensors is because tensors don't have an end in attribute which tells Matt plot lib how many dimensions there are so watch this.", "tokens": [583, 309, 4523, 484, 264, 787, 1778, 291, 393, 380, 7542, 1730, 27822, 10688, 830, 307, 570, 10688, 830, 500, 380, 362, 364, 917, 294, 19667, 597, 5112, 7397, 7542, 22854, 577, 867, 12819, 456, 366, 370, 1159, 341, 13], "temperature": 0.0, "avg_logprob": -0.3093318045139313, "compression_ratio": 1.5260115606936415, "no_speech_prob": 1.095206789614167e-05}, {"id": 1741, "seek": 774034, "start": 7753.46, "end": 7758.9800000000005, "text": " Torch dot tensor dot m dim equals a property that is the length of the shape.", "tokens": [7160, 339, 5893, 40863, 5893, 275, 5013, 6915, 257, 4707, 300, 307, 264, 4641, 295, 264, 3909, 13], "temperature": 0.0, "avg_logprob": -0.3093318045139313, "compression_ratio": 1.5260115606936415, "no_speech_prob": 1.095206789614167e-05}, {"id": 1742, "seek": 775898, "start": 7758.98, "end": 7769.7, "text": " This is now replaced the definition again using the dynamic features of Python replace the death replace actually insert into the definition of tensor and your property called end in.", "tokens": [639, 307, 586, 10772, 264, 7123, 797, 1228, 264, 8546, 4122, 295, 15329, 7406, 264, 2966, 7406, 767, 8969, 666, 264, 7123, 295, 40863, 293, 428, 4707, 1219, 917, 294, 13], "temperature": 0.0, "avg_logprob": -0.26144824028015134, "compression_ratio": 1.7927927927927927, "no_speech_prob": 5.255046289676102e-06}, {"id": 1743, "seek": 775898, "start": 7771.7, "end": 7785.299999999999, "text": " And now we can plot tensors right so like the nice thing about Python is you never have to be like oh this isn't supported because you can change everything you can insert things you can replace things whatever so.", "tokens": [400, 586, 321, 393, 7542, 10688, 830, 558, 370, 411, 264, 1481, 551, 466, 15329, 307, 291, 1128, 362, 281, 312, 411, 1954, 341, 1943, 380, 8104, 570, 291, 393, 1319, 1203, 291, 393, 8969, 721, 291, 393, 7406, 721, 2035, 370, 13], "temperature": 0.0, "avg_logprob": -0.26144824028015134, "compression_ratio": 1.7927927927927927, "no_speech_prob": 5.255046289676102e-06}, {"id": 1744, "seek": 778530, "start": 7785.3, "end": 7788.58, "text": " Here we've now got a nice print out of our four different schedules.", "tokens": [1692, 321, 600, 586, 658, 257, 1481, 4482, 484, 295, 527, 1451, 819, 28078, 13], "temperature": 0.0, "avg_logprob": -0.25946027582341974, "compression_ratio": 1.6, "no_speech_prob": 1.6697482351446524e-05}, {"id": 1745, "seek": 778530, "start": 7793.38, "end": 7807.46, "text": " Which isn't really enough because if you want to do one cycle scheduling then and fattened but you know most of the time nowadays you want some kind of warm up and some kind of cool down or if you're doing something like sgdr you've got like multiple.", "tokens": [3013, 1943, 380, 534, 1547, 570, 498, 291, 528, 281, 360, 472, 6586, 29055, 550, 293, 4046, 1147, 292, 457, 291, 458, 881, 295, 264, 565, 13434, 291, 528, 512, 733, 295, 4561, 493, 293, 512, 733, 295, 1627, 760, 420, 498, 291, 434, 884, 746, 411, 262, 70, 16753, 291, 600, 658, 411, 3866, 13], "temperature": 0.0, "avg_logprob": -0.25946027582341974, "compression_ratio": 1.6, "no_speech_prob": 1.6697482351446524e-05}, {"id": 1746, "seek": 780746, "start": 7807.46, "end": 7815.78, "text": " Call downs so we really need to be able to paste some of these schedule is together.", "tokens": [7807, 21554, 370, 321, 534, 643, 281, 312, 1075, 281, 9163, 512, 295, 613, 7567, 307, 1214, 13], "temperature": 0.0, "avg_logprob": -0.3529206028691045, "compression_ratio": 1.4817518248175183, "no_speech_prob": 9.817659702093806e-06}, {"id": 1747, "seek": 780746, "start": 7817.3, "end": 7823.54, "text": " So let's create a nother function called combine schedule is and it's going to look like this.", "tokens": [407, 718, 311, 1884, 257, 406, 511, 2445, 1219, 10432, 7567, 307, 293, 309, 311, 516, 281, 574, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.3529206028691045, "compression_ratio": 1.4817518248175183, "no_speech_prob": 9.817659702093806e-06}, {"id": 1748, "seek": 780746, "start": 7824.74, "end": 7825.86, "text": " We're going to pass in.", "tokens": [492, 434, 516, 281, 1320, 294, 13], "temperature": 0.0, "avg_logprob": -0.3529206028691045, "compression_ratio": 1.4817518248175183, "no_speech_prob": 9.817659702093806e-06}, {"id": 1749, "seek": 782586, "start": 7825.86, "end": 7841.54, "text": " We're going to pass in we're going to pass in the kind of the phases we want so phase one will be a cosine schedule from a learning rate of point three to point six phase two will be a learning rate as cosine schedule with the learning rate going from point six to point two.", "tokens": [492, 434, 516, 281, 1320, 294, 321, 434, 516, 281, 1320, 294, 264, 733, 295, 264, 18764, 321, 528, 370, 5574, 472, 486, 312, 257, 23565, 7567, 490, 257, 2539, 3314, 295, 935, 1045, 281, 935, 2309, 5574, 732, 486, 312, 257, 2539, 3314, 382, 23565, 7567, 365, 264, 2539, 3314, 516, 490, 935, 2309, 281, 935, 732, 13], "temperature": 0.0, "avg_logprob": -0.1288809557573511, "compression_ratio": 2.1421800947867298, "no_speech_prob": 1.6441310435766354e-05}, {"id": 1750, "seek": 782586, "start": 7842.42, "end": 7853.78, "text": " And phase one will take up 30% of our batches and phase two will take up 70% so that's what we're going to pass in how long is each phase and what's the schedule in each phase.", "tokens": [400, 5574, 472, 486, 747, 493, 2217, 4, 295, 527, 15245, 279, 293, 5574, 732, 486, 747, 493, 5285, 4, 370, 300, 311, 437, 321, 434, 516, 281, 1320, 294, 577, 938, 307, 1184, 5574, 293, 437, 311, 264, 7567, 294, 1184, 5574, 13], "temperature": 0.0, "avg_logprob": -0.1288809557573511, "compression_ratio": 2.1421800947867298, "no_speech_prob": 1.6441310435766354e-05}, {"id": 1751, "seek": 785378, "start": 7853.78, "end": 7856.5, "text": " So.", "tokens": [407, 13], "temperature": 0.0, "avg_logprob": -0.20248043175899622, "compression_ratio": 1.5555555555555556, "no_speech_prob": 2.7532414605957456e-05}, {"id": 1752, "seek": 785378, "start": 7857.86, "end": 7863.62, "text": " Here's how we do that I don't think I need to go through the code it's there's nothing interesting about it.", "tokens": [1692, 311, 577, 321, 360, 300, 286, 500, 380, 519, 286, 643, 281, 352, 807, 264, 3089, 309, 311, 456, 311, 1825, 1880, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.20248043175899622, "compression_ratio": 1.5555555555555556, "no_speech_prob": 2.7532414605957456e-05}, {"id": 1753, "seek": 785378, "start": 7864.5, "end": 7874.58, "text": " But what we do once we have that is that we can then plot that schedule and you can kind of see why we're very fond of these.", "tokens": [583, 437, 321, 360, 1564, 321, 362, 300, 307, 300, 321, 393, 550, 7542, 300, 7567, 293, 291, 393, 733, 295, 536, 983, 321, 434, 588, 9557, 295, 613, 13], "temperature": 0.0, "avg_logprob": -0.20248043175899622, "compression_ratio": 1.5555555555555556, "no_speech_prob": 2.7532414605957456e-05}, {"id": 1754, "seek": 787458, "start": 7874.58, "end": 7883.46, "text": " Cosine one cycle schedules I don't think this has ever been published anywhere but it's what fast AI users by default nowadays.", "tokens": [15855, 533, 472, 6586, 28078, 286, 500, 380, 519, 341, 575, 1562, 668, 6572, 4992, 457, 309, 311, 437, 2370, 7318, 5022, 538, 7576, 13434, 13], "temperature": 0.0, "avg_logprob": -0.21986988612583705, "compression_ratio": 1.5447154471544715, "no_speech_prob": 5.648556907544844e-05}, {"id": 1755, "seek": 787458, "start": 7884.5, "end": 7891.94, "text": " Is you kind of get a nice gentle warm up at the start, this is the time when things are just super sensitive and fall apart really quickly.", "tokens": [1119, 291, 733, 295, 483, 257, 1481, 6424, 4561, 493, 412, 264, 722, 11, 341, 307, 264, 565, 562, 721, 366, 445, 1687, 9477, 293, 2100, 4936, 534, 2661, 13], "temperature": 0.0, "avg_logprob": -0.21986988612583705, "compression_ratio": 1.5447154471544715, "no_speech_prob": 5.648556907544844e-05}, {"id": 1756, "seek": 787458, "start": 7892.34, "end": 7898.66, "text": " But it doesn't take long as you'll see in next week's lesson when we do a deep dive into into stuff using hooks.", "tokens": [583, 309, 1177, 380, 747, 938, 382, 291, 603, 536, 294, 958, 1243, 311, 6898, 562, 321, 360, 257, 2452, 9192, 666, 666, 1507, 1228, 26485, 13], "temperature": 0.0, "avg_logprob": -0.21986988612583705, "compression_ratio": 1.5447154471544715, "no_speech_prob": 5.648556907544844e-05}, {"id": 1757, "seek": 789866, "start": 7898.66, "end": 7905.86, "text": " It doesn't take long for it to get into a decent part of the lost landscape and so you can quite quickly increase the learning rate.", "tokens": [467, 1177, 380, 747, 938, 337, 309, 281, 483, 666, 257, 8681, 644, 295, 264, 2731, 9661, 293, 370, 291, 393, 1596, 2661, 3488, 264, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.15526614469640396, "compression_ratio": 1.7723880597014925, "no_speech_prob": 7.766132512188051e-06}, {"id": 1758, "seek": 789866, "start": 7907.0599999999995, "end": 7912.98, "text": " And then something that people have and we'll start looking at papers next week for this something that people have realized in the last.", "tokens": [400, 550, 746, 300, 561, 362, 293, 321, 603, 722, 1237, 412, 10577, 958, 1243, 337, 341, 746, 300, 561, 362, 5334, 294, 264, 1036, 13], "temperature": 0.0, "avg_logprob": -0.15526614469640396, "compression_ratio": 1.7723880597014925, "no_speech_prob": 7.766132512188051e-06}, {"id": 1759, "seek": 789866, "start": 7914.0199999999995, "end": 7924.9, "text": " Four months or so although Leslie Smith really kind of showed us this two years ago, but it's only been the last four months or so that people have really understood this in the wider academic literature.", "tokens": [7451, 2493, 420, 370, 4878, 28140, 8538, 534, 733, 295, 4712, 505, 341, 732, 924, 2057, 11, 457, 309, 311, 787, 668, 264, 1036, 1451, 2493, 420, 370, 300, 561, 362, 534, 7320, 341, 294, 264, 11842, 7778, 10394, 13], "temperature": 0.0, "avg_logprob": -0.15526614469640396, "compression_ratio": 1.7723880597014925, "no_speech_prob": 7.766132512188051e-06}, {"id": 1760, "seek": 792490, "start": 7924.9, "end": 7934.82, "text": " You need to train at a high learning rate for a long time and so with this kind of coach cosine schedule we keep it up high for a long time.", "tokens": [509, 643, 281, 3847, 412, 257, 1090, 2539, 3314, 337, 257, 938, 565, 293, 370, 365, 341, 733, 295, 6560, 23565, 7567, 321, 1066, 309, 493, 1090, 337, 257, 938, 565, 13], "temperature": 0.0, "avg_logprob": -0.1746127439099689, "compression_ratio": 1.7658536585365854, "no_speech_prob": 9.368091014039237e-06}, {"id": 1761, "seek": 792490, "start": 7935.54, "end": 7949.86, "text": " But then you also need to fine tune at a very low learning rate for a long time, so this has all of the kind of nice features that we want so cosine one cycle schedules are terrific and we now can build them from scratch.", "tokens": [583, 550, 291, 611, 643, 281, 2489, 10864, 412, 257, 588, 2295, 2539, 3314, 337, 257, 938, 565, 11, 370, 341, 575, 439, 295, 264, 733, 295, 1481, 4122, 300, 321, 528, 370, 23565, 472, 6586, 28078, 366, 20899, 293, 321, 586, 393, 1322, 552, 490, 8459, 13], "temperature": 0.0, "avg_logprob": -0.1746127439099689, "compression_ratio": 1.7658536585365854, "no_speech_prob": 9.368091014039237e-06}, {"id": 1762, "seek": 794986, "start": 7949.86, "end": 7963.299999999999, "text": " So let's try trading like this so with let's create a list of callback functions that has a recorder in it and average stats callback with accuracy in it and a parameter scheduler that schedules the learning rate using this schedule.", "tokens": [407, 718, 311, 853, 9529, 411, 341, 370, 365, 718, 311, 1884, 257, 1329, 295, 818, 3207, 6828, 300, 575, 257, 37744, 294, 309, 293, 4274, 18152, 818, 3207, 365, 14170, 294, 309, 293, 257, 13075, 12000, 260, 300, 28078, 264, 2539, 3314, 1228, 341, 7567, 13], "temperature": 0.0, "avg_logprob": -0.27225457168206935, "compression_ratio": 1.5898617511520738, "no_speech_prob": 9.079236406250857e-06}, {"id": 1763, "seek": 794986, "start": 7965.139999999999, "end": 7965.86, "text": " And then fit.", "tokens": [400, 550, 3318, 13], "temperature": 0.0, "avg_logprob": -0.27225457168206935, "compression_ratio": 1.5898617511520738, "no_speech_prob": 9.079236406250857e-06}, {"id": 1764, "seek": 794986, "start": 7968.099999999999, "end": 7976.5, "text": " That's looking pretty good we're getting up towards 94% pretty quickly and we can now go plot LR.", "tokens": [663, 311, 1237, 1238, 665, 321, 434, 1242, 493, 3030, 30849, 4, 1238, 2661, 293, 321, 393, 586, 352, 7542, 441, 49, 13], "temperature": 0.0, "avg_logprob": -0.27225457168206935, "compression_ratio": 1.5898617511520738, "no_speech_prob": 9.079236406250857e-06}, {"id": 1765, "seek": 797650, "start": 7976.5, "end": 7980.02, "text": " And it's the shape that we hoped for and we can even say plot loss.", "tokens": [400, 309, 311, 264, 3909, 300, 321, 19737, 337, 293, 321, 393, 754, 584, 7542, 4470, 13], "temperature": 0.0, "avg_logprob": -0.5337076381761201, "compression_ratio": 1.1851851851851851, "no_speech_prob": 2.4295557523146272e-05}, {"id": 1766, "seek": 797650, "start": 7981.78, "end": 7982.34, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.5337076381761201, "compression_ratio": 1.1851851851851851, "no_speech_prob": 2.4295557523146272e-05}, {"id": 1767, "seek": 797650, "start": 7983.46, "end": 7984.1, "text": " So.", "tokens": [407, 13], "temperature": 0.0, "avg_logprob": -0.5337076381761201, "compression_ratio": 1.1851851851851851, "no_speech_prob": 2.4295557523146272e-05}, {"id": 1768, "seek": 797650, "start": 7988.1, "end": 7989.38, "text": " We now have.", "tokens": [492, 586, 362, 13], "temperature": 0.0, "avg_logprob": -0.5337076381761201, "compression_ratio": 1.1851851851851851, "no_speech_prob": 2.4295557523146272e-05}, {"id": 1769, "seek": 797650, "start": 7992.1, "end": 7995.14, "text": " Really all of the pieces we need.", "tokens": [4083, 439, 295, 264, 3755, 321, 643, 13], "temperature": 0.0, "avg_logprob": -0.5337076381761201, "compression_ratio": 1.1851851851851851, "no_speech_prob": 2.4295557523146272e-05}, {"id": 1770, "seek": 797650, "start": 7996.42, "end": 7997.14, "text": " To.", "tokens": [1407, 13], "temperature": 0.0, "avg_logprob": -0.5337076381761201, "compression_ratio": 1.1851851851851851, "no_speech_prob": 2.4295557523146272e-05}, {"id": 1771, "seek": 799714, "start": 7997.14, "end": 8008.660000000001, "text": " To kind of try out lots of different ways of of training neural nets we still haven't looked at convolutions really will do that next week and a lot more.", "tokens": [1407, 733, 295, 853, 484, 3195, 295, 819, 2098, 295, 295, 3097, 18161, 36170, 321, 920, 2378, 380, 2956, 412, 3754, 15892, 534, 486, 360, 300, 958, 1243, 293, 257, 688, 544, 13], "temperature": 0.0, "avg_logprob": -0.23157776423863002, "compression_ratio": 1.5524861878453038, "no_speech_prob": 5.954203061264707e-06}, {"id": 1772, "seek": 799714, "start": 8010.58, "end": 8020.02, "text": " But you kind of have the ability now to hopefully think of lots of things that you might want to try and and try them out.", "tokens": [583, 291, 733, 295, 362, 264, 3485, 586, 281, 4696, 519, 295, 3195, 295, 721, 300, 291, 1062, 528, 281, 853, 293, 293, 853, 552, 484, 13], "temperature": 0.0, "avg_logprob": -0.23157776423863002, "compression_ratio": 1.5524861878453038, "no_speech_prob": 5.954203061264707e-06}, {"id": 1773, "seek": 799714, "start": 8021.54, "end": 8022.1, "text": " So.", "tokens": [407, 13], "temperature": 0.0, "avg_logprob": -0.23157776423863002, "compression_ratio": 1.5524861878453038, "no_speech_prob": 5.954203061264707e-06}, {"id": 1774, "seek": 802210, "start": 8022.1, "end": 8026.42, "text": " So next week we're going to be starting with.", "tokens": [407, 958, 1243, 321, 434, 516, 281, 312, 2891, 365, 13], "temperature": 0.0, "avg_logprob": -0.3975588177877759, "compression_ratio": 1.6081081081081081, "no_speech_prob": 4.784224074683152e-06}, {"id": 1775, "seek": 802210, "start": 8027.38, "end": 8028.18, "text": " Conf nets.", "tokens": [11701, 36170, 13], "temperature": 0.0, "avg_logprob": -0.3975588177877759, "compression_ratio": 1.6081081081081081, "no_speech_prob": 4.784224074683152e-06}, {"id": 1776, "seek": 802210, "start": 8029.38, "end": 8030.9800000000005, "text": " We're going to be.", "tokens": [492, 434, 516, 281, 312, 13], "temperature": 0.0, "avg_logprob": -0.3975588177877759, "compression_ratio": 1.6081081081081081, "no_speech_prob": 4.784224074683152e-06}, {"id": 1777, "seek": 802210, "start": 8032.18, "end": 8040.42, "text": " Kind of and we're going to be finally using our GPU because once we because once we start creating confidence of this size it starts taking a little bit too long.", "tokens": [9242, 295, 293, 321, 434, 516, 281, 312, 2721, 1228, 527, 18407, 570, 1564, 321, 570, 1564, 321, 722, 4084, 6687, 295, 341, 2744, 309, 3719, 1940, 257, 707, 857, 886, 938, 13], "temperature": 0.0, "avg_logprob": -0.3975588177877759, "compression_ratio": 1.6081081081081081, "no_speech_prob": 4.784224074683152e-06}, {"id": 1778, "seek": 804042, "start": 8040.42, "end": 8051.3, "text": " But just to read ahead a little bit how what's it going to take to put stuff on the GPU this is the entirety of the callback.", "tokens": [583, 445, 281, 1401, 2286, 257, 707, 857, 577, 437, 311, 309, 516, 281, 747, 281, 829, 1507, 322, 264, 18407, 341, 307, 264, 31557, 295, 264, 818, 3207, 13], "temperature": 0.0, "avg_logprob": -0.44712880762611945, "compression_ratio": 1.5625, "no_speech_prob": 1.1477307452878449e-05}, {"id": 1779, "seek": 804042, "start": 8052.58, "end": 8057.38, "text": " So we've now got the mechanics we need to do things unbelievably quickly.", "tokens": [407, 321, 600, 586, 658, 264, 12939, 321, 643, 281, 360, 721, 43593, 2661, 13], "temperature": 0.0, "avg_logprob": -0.44712880762611945, "compression_ratio": 1.5625, "no_speech_prob": 1.1477307452878449e-05}, {"id": 1780, "seek": 804042, "start": 8059.86, "end": 8067.3, "text": " And then we will be able to oh and also we'll be wanting to add some transformations this is the one we're going to be using.", "tokens": [400, 550, 321, 486, 312, 1075, 281, 1954, 293, 611, 321, 603, 312, 7935, 281, 909, 512, 34852, 341, 307, 264, 472, 321, 434, 516, 281, 312, 1228, 13], "temperature": 0.0, "avg_logprob": -0.44712880762611945, "compression_ratio": 1.5625, "no_speech_prob": 1.1477307452878449e-05}, {"id": 1781, "seek": 806730, "start": 8067.3, "end": 8073.3, "text": " Also we'll be wanting to add some transformations this is the entirety of what it takes to do batch wise transformations.", "tokens": [2743, 321, 603, 312, 7935, 281, 909, 512, 34852, 341, 307, 264, 31557, 295, 437, 309, 2516, 281, 360, 15245, 10829, 34852, 13], "temperature": 0.0, "avg_logprob": -0.18231207748939252, "compression_ratio": 1.6560509554140128, "no_speech_prob": 6.048554951121332e-06}, {"id": 1782, "seek": 806730, "start": 8074.5, "end": 8075.22, "text": " Without callback.", "tokens": [9129, 818, 3207, 13], "temperature": 0.0, "avg_logprob": -0.18231207748939252, "compression_ratio": 1.6560509554140128, "no_speech_prob": 6.048554951121332e-06}, {"id": 1783, "seek": 806730, "start": 8077.3, "end": 8087.9400000000005, "text": " As we discussed though we can't add callbacks between layers so we will add callbacks between layers initially manually.", "tokens": [1018, 321, 7152, 1673, 321, 393, 380, 909, 818, 17758, 1296, 7914, 370, 321, 486, 909, 818, 17758, 1296, 7914, 9105, 16945, 13], "temperature": 0.0, "avg_logprob": -0.18231207748939252, "compression_ratio": 1.6560509554140128, "no_speech_prob": 6.048554951121332e-06}, {"id": 1784, "seek": 808794, "start": 8087.94, "end": 8098.259999999999, "text": " And then using pie torch hooks and that way we're going to be able to plot and see exactly what's going on inside our models as they train.", "tokens": [400, 550, 1228, 1730, 27822, 26485, 293, 300, 636, 321, 434, 516, 281, 312, 1075, 281, 7542, 293, 536, 2293, 437, 311, 516, 322, 1854, 527, 5245, 382, 436, 3847, 13], "temperature": 0.0, "avg_logprob": -0.1704650317921358, "compression_ratio": 1.6161616161616161, "no_speech_prob": 5.092892024549656e-06}, {"id": 1785, "seek": 808794, "start": 8099.219999999999, "end": 8101.46, "text": " And we'll find ways to train them.", "tokens": [400, 321, 603, 915, 2098, 281, 3847, 552, 13], "temperature": 0.0, "avg_logprob": -0.1704650317921358, "compression_ratio": 1.6161616161616161, "no_speech_prob": 5.092892024549656e-06}, {"id": 1786, "seek": 808794, "start": 8103.0599999999995, "end": 8110.339999999999, "text": " Much much more nicely so that by the end of next by the end of the next notebook will be up over 98% accuracy.", "tokens": [12313, 709, 544, 9594, 370, 300, 538, 264, 917, 295, 958, 538, 264, 917, 295, 264, 958, 21060, 486, 312, 493, 670, 20860, 4, 14170, 13], "temperature": 0.0, "avg_logprob": -0.1704650317921358, "compression_ratio": 1.6161616161616161, "no_speech_prob": 5.092892024549656e-06}, {"id": 1787, "seek": 808794, "start": 8112.66, "end": 8113.7, "text": " And that's going to be super cool.", "tokens": [400, 300, 311, 516, 281, 312, 1687, 1627, 13], "temperature": 0.0, "avg_logprob": -0.1704650317921358, "compression_ratio": 1.6161616161616161, "no_speech_prob": 5.092892024549656e-06}, {"id": 1788, "seek": 811370, "start": 8113.7, "end": 8117.86, "text": " And then we're going to do a deep dive into batch norm.", "tokens": [400, 550, 321, 434, 516, 281, 360, 257, 2452, 9192, 666, 15245, 2026, 13], "temperature": 0.0, "avg_logprob": -0.23687865094440738, "compression_ratio": 1.5424528301886793, "no_speech_prob": 1.3842658518115059e-05}, {"id": 1789, "seek": 811370, "start": 8118.98, "end": 8120.26, "text": " Data blocks API.", "tokens": [11888, 8474, 9362, 13], "temperature": 0.0, "avg_logprob": -0.23687865094440738, "compression_ratio": 1.5424528301886793, "no_speech_prob": 1.3842658518115059e-05}, {"id": 1790, "seek": 811370, "start": 8121.3, "end": 8136.099999999999, "text": " Optimizes and transforms and at that point I think we'll have basically all the mechanics we need to go into some more advanced architectures and training methods and see how we did some of the.", "tokens": [35013, 5660, 293, 35592, 293, 412, 300, 935, 286, 519, 321, 603, 362, 1936, 439, 264, 12939, 321, 643, 281, 352, 666, 512, 544, 7339, 6331, 1303, 293, 3097, 7150, 293, 536, 577, 321, 630, 512, 295, 264, 13], "temperature": 0.0, "avg_logprob": -0.23687865094440738, "compression_ratio": 1.5424528301886793, "no_speech_prob": 1.3842658518115059e-05}, {"id": 1791, "seek": 813610, "start": 8136.1, "end": 8142.42, "text": " Call stuff that we did in part one so I'll see you next week.", "tokens": [50364, 7807, 1507, 300, 321, 630, 294, 644, 472, 370, 286, 603, 536, 291, 958, 1243, 13, 50680], "temperature": 0.0, "avg_logprob": -0.2870980563916658, "compression_ratio": 0.9384615384615385, "no_speech_prob": 3.806380118476227e-05}], "language": "en"}