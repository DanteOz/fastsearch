{"text": " Hi and welcome to lesson one of this deep learning MOOC. Thanks for joining us. I'm Jeremy Howard. And I'm Rachel Thomas. We're the people who put this together. You'll be seeing my face in front of the camera most of the time. You'll be hearing Rachel's voice, however. And I'll be asking questions that are coming through a Slack channel asked by students in the in-person version of this course. So we thought before we started we should tell you a little bit about what to expect and maybe you'd get to know us a little bit. My background is really in coding and data. I spent 10 years of management consulting and 10 years running startups. Throughout that time I've been using data and machine learning to try to solve problems. My background is a lot more academic and theoretical. I have a PhD in math and then I worked as a quant, later as a software engineer and data scientist at Uber. One of the most fun and exciting parts of my life was when I spent some time really competing heavily in Kaggle competitions. I was really pleased to win some of those competitions and get to the top of the leaderboard. And I'm hoping to show you guys during this course some of the techniques that I used to do that. I think the techniques that allow you to win Kaggle competitions are the same as the techniques that allow you to create great results on your own models in solving your own problems. We also both love teaching. So I taught calculus one and two when I was in graduate school. And then I later left my job as a software engineer to teach full stack software development to women at Hackbright Academy for a year and a half. I think that's really cool. Rachel was a quant and then she worked as both a data scientist and a full stack engineer at Uber. But she realized that one of the highest leverage things you can do is to teach and it's great fun too. I feel the same way even when I was running startups. I was creating course content online. For example, on the left here is an AngularJS tutorial that I originally created for my colleagues at Kaggle. But I recorded it and put it online and it's had over 200,000 views. It makes me feel really good to know that people are learning from some of the things that I found really helpful myself. So this is a quote from Paul Lockhart who was actually working as a primary school math teacher, got his PhD in math at Columbia and became a math professor at Brown and then left to go back to teaching primary school. He's written a wonderful essay called A Mathematician's Lament on Everything That's Horrible about How Mathematics is Taught in the United States. I think that essay has been really influential to both Rachel and I. Although Rachel stuck with her math education for decades longer than I did, we both definitely felt like modern mathematics education is not done well. Paul Lockhart uses a wonderful analogy about imagine if with music we didn't allow children to sing or play instruments until they had spent years and even decades studying set theory and music notation and could transcribe scales. And only then, once they were in their 20s, we would let them create music. He says that's exactly what we're doing with mathematics, but that we should let people kind of play and create and build patterns with it. And something very similar happens with deep learning and how it's taught. In fact, one of my heroes is a guy called David Perkins who at Harvard has created some really interesting research about effective educational techniques. And he has a very similar analogy to Paul Lockhart, but he talks about baseball. Imagine if the way you learned baseball was that you never saw a game of baseball, but instead you learned about how to stitch a baseball and you learned the physics of a parabola and you learned every aspect of baseball. And then after 20 years of study, you'd be considered good enough to go and actually watch your first game. We tend to think that this is rather the way that most mathematics, perhaps particularly including deep learning, is really taught. So we decided when we set up our research lab, Fast AI, that the first thing we would do would be to try and fill this need. And particularly we decided to focus on deep learning because we both think that deep learning is the most exciting technology that we have ever seen. We think it's going to be more transformative than even the Internet. And so the more people who can participate, the better. Andrew Ng has called it the new electricity, to say that it's going to have the impact on society that electricity has. So some of the kinds of problems we've seen with technical teaching include these. And I just want to say we're introducing this to tell you that this course is taught in a very different style. And so we want to kind of set your expectations ahead of time and motivate why it's so different, how we teach it here. And one is that a lot of existing deep learning materials are very math centric. And even as a mathematician and someone who loves math, I found them to be pretty unhelpful for actually building and creating practical applications. In fact, every time I see somebody ask on a forum or on Hacker News or whatever, what do I need in order to get into deep learning, a whole bunch of people reply by saying, well, first you need five years of real analysis and vector analysis, and then you need to study probability and statistics and blah, blah, blah, blah, blah. And it really comes across to me as something which is all about being exclusive rather than inclusive. So that's why we have this little thing, making neural nets uncool again is kind of our goal. The Fast AI slogan, yeah. The Fast AI slogan. We're all about not being exclusive, but about making things as simple as possible, but never about dumbing it down. Right. Another way that kind of technical education fails is what David Perkins, the Harvard professor Jeremy mentioned a moment ago, calls elementitis. And that's that often math does this so much. It teaches kind of each separate element. And it's only at the end when you've learned all the elements needed that you can put them together and see the whole thing. And that's kind of what was going on in that baseball analogy. And that happens in a lot of deep learning. It's like, you know, we need to teach you probability theory and we need to teach you information theory. And only way later on are we going to let you put it together. You can think of it as being depth first rather than breadth first, if you like. So the traditional depth first approach means that you as a student have to trust that at some point all these things are going to come together and turn into something that's genuinely useful. I think with this breadth first approach, you still have to trust, but it's a different kind of trust, which is that it's OK that when we first show you an end to end process that you don't deeply understand every part, but that you are able to actually do useful things from the very first lesson. And that as the lessons go along, you're going to get more and more in depth understanding of each piece. And two ways that the elementitis or the depth first approach fails are one is motivation. A lot of students kind of give up because they don't have the motivation of seeing how are these going to fit together. And then secondly, it's harder to get the like you don't have the context when you're learning all these discrete elements and you can't learn how they're going to fit into the process until later. Right. And in fact, this goes together with the idea of using a code centric approach instead of a math centric approach with a code centric approach. And looking at the whole game that is an end to end machine learning process from the very start means that you can do experiments. You can actually run experiments and see what goes in and out of each part of the system and build up that intuition. And if this whole game analogy intrigues you, David Perkins has a book called Making Learning Whole, where he goes into a lot more detail about it. Love that book. So then not not only are we going to be showing you end to end processes from this very first lesson, but these processes are going to not going to just end up with good enough results. Nearly all of the deep learning educational materials I've seen so far get you to a point where you can kind of get an OK ish result. Now, the whole point of deep learning is that you can get state of the art results. And so in the very first piece of code we're going to run, we're going to run a piece of code which gives you a state of the art result. We know something is a state of the art result if it is better than other approaches that people have tried. The best way to know that is to try things on a Kaggle competition. Having been the president and chief scientist of Kaggle, I saw again and again that every Kaggle competition beat all previous academic state state of the art results. So very often in this course, we're actually going to use Kaggle benchmarks and see if we can beat them, because we know if we can, then that's truly a world best. So this is from actually a very good book. It's the Ian Goodfellow, Yoshua Bengio, Deep Learning book. But it's a very good math book which teaches you the math of deep learning. And so in this book, when they say here is how we gain some intuition in how to back propagation through time works, this is how they develop intuition. Rachel, as a math PhD, did you find this helped your intuitions? We'll have a very different approach to intuition. So this is a good book if you're interested in math and theorems. In this course, we're really going to be focused on code. In fact, this is what Rachel and I put together when we were trying to explain backprop and specifically stochastic gradient descent and the use of backprop there was we created a spreadsheet. And we found each time that we taught our students in the in-person course through a spreadsheet, they could see every single piece of what was going on, every single intermediate result. And it was very easy for them to experiment with. And so one of the unusual things we do is that you'll see that nearly every major idea is presented at some point using a spreadsheet. We present it in many different ways, but spreadsheets, diagrams and code are three of the key ways that we present these ideas. I believe this is the first deep learning course in the world to implement a convolutional neural network in an Excel spreadsheet. And also, as you see from this page, not just stochastic gradient descent, but AdaGrad, RMSProp, Adam, and even Eve, which just came out a few weeks ago, all modern examples of accelerated SGD approaches. So I think everything you really need to know about the course comes in this very first piece of code that you see. And this very first piece of code that you see, you can see that there's a number of things going on. The first is that this piece of code shows not just how to complete a project, but how to get a state of the art result on a project. This particular piece of code gives you 97 percent accuracy in determining cats versus dogs. As recently as about five years ago, the state of the art for this particular problem was about 80 percent accuracy. It's also an example of showing why working with code is so interesting. Rather than showing math, what we're showing here is some working code. And I'll give you an example of what that means you can do. So the code environment that we're working in is something called Jupyter Notebook. And you'll be using this in every single lesson throughout the course. And in Jupyter Notebook, as you can see, we provide you with prose and information about what's going on. And we draw pictures. And at any point in time, you can take a look at one of these results. And you can take a look at one of these results and you can look to see what's going on behind the scenes. So, for example, in this case, we're running something called VGG.predict. And we're getting back some probabilities. And you might wonder, well, what's VGG.predict actually doing? So at any time, you can take anything and put two question marks on the front and run that piece of code. And it will actually show you the full documentation and source code of what you just ran. Now, in this case, it's actually running a function that we wrote for you. One of the other different things about this course is that we're not just showing you how existing libraries work. But every time we found that using somebody else's library takes more than four or five lines of code, we would make sure we found a way to do it easier. So generally speaking, we show you how to do things in one line of code. And then you can look behind the scenes and see what the lines of code are actually doing. So, for example, in this case, the predict method is running some other predict method called model.predict. And so then what I always encourage people to do is to do some experiments. So what does model.predict actually do? One thing that you can do in Jupyter Notebook at any time is to press Shift Tab a couple of times. And when you press Shift Tab, the first time it pops up, it tells you what parameters you need to pass this method. And it also tells you what the method actually does. If you press it three times, it then gives you additional information about what each of those arguments are and what they're expecting and what it returns. So it's really nice that using this method, you can find out exactly what's going on behind the scenes and do some experiments. And so then, for example, you could find out, OK, well, what is the shape? What is the size and shape of the array that this thing returns? What are the first four elements of the classes that are in this object and so forth? And this is really the way to use this style of teaching effectively. It's to have the code in front of you all the time and in every line, look and see what's being passed in, what's coming out, what else could we do with that? And then even look at the documentation. So VGG.model is apparently a Keras.model.sequential. So if we were to just copy that into Google, then we can click on the first item and find out exactly what's going on. What is being used here? What are the other methods that this could take? And then we can try calling some of these other methods and see what kind of results we get. So really what we're trying to do is in the two hours of each lesson, we're trying to give you enough information to get you started with your own experiments. We're not trying to teach you everything. And we're certainly not assuming that the lesson can stand alone. And we'll talk more about this in a moment, but the videos are just a small part of the course. But the iPython notebooks and the code are a huge resource. And we'll talk about some of the other resources that we have available for you. But the important thing to realize with these six lines of code is that you can run this for anything, not just for dogs versus cats. But these first six lines of code you learn will actually, as it says here, work for any image recognition task with any number of categories. So if you can get this far in today's lesson, then you've learned to do one of the most important types of computer vision, which is image classification for any number of categories for any type of images. As Rachel said, we've actually run this course already. Specifically, what you're going to be seeing are the recorded lessons from an in-person course. And we thought it'd be helpful for you to see what some of our students said about that in-person course, because it might help you to be a more effective learner. And I do want to say, again, because this course is taught in such a different way, it takes some faith that this new technique is worth trying and sticking with. But you can see that almost all the students said that the homework assignments were very helpful or extremely helpful in understanding the material. And the class resources, which includes the Wiki, the scripts that we give you, our forums, our Slack channel, were very helpful or extremely helpful in understanding material. And we wanted to mention that, because Rachel and I have both been kind of Coursera addicts in the past, and Udacity addicts, and generally speaking, we all often watch a video at one and a half speed or two speed and just zip through them. This is not designed to be possible to do that this way. This is designed that you need to use the homework assignments and the class resources. So as you can see from the people who have already been through this class, they're actually finding that these are really important parts of the overall course. Because each video is giving you, you're kind of seeing an end-to-end process of solving a real problem with deep learning. And that means that there's not, though, a separate video on kind of this is everything you need to know about AWS and your environment, and this is everything you need to know about this piece of code. But rather, you're kind of seeing the end-to-end process, but you'll see it again and again throughout the lessons. Now, it's okay if you're coming into this course with either a very large amount or a very small amount of data science background. Everybody in the in-person course simply had to have had at least a year of coding experience. Even with a very wide variety in background, nearly everybody said they found the pacing about right for them. And the reason for that, I think, is that we really give people the ability to pick up as much or as little as they want. So the forums, if you want to dig very, very deep into advanced topics, you can. Or if absolutely everything is new to you, then that's fine, too. There'll be more than enough to do just to get through the basic parts of the assignments. And of course, on the forums, we'd be very happy to help you with all of your questions there. And if you are more advanced, we really appreciate your help in adding new material to the Wiki, answering others' questions on the forums. People started their own threads on the forums around outside-related topics that they were interested in. There are a lot of different ways to be involved. So here's a couple of quotes we got from people after they completed the in-person course. And this is one that we heard again and again. So for example, this person says, I personally fell into the habit of watching the lectures too much and Googling definitions and concepts and so forth too much without running the code. But first, I thought that I should read the code quickly and then spend time researching the theory behind it. In retrospect, I should have spent the majority of my time on the actual code in the notebooks instead, in terms of running it and seeing what goes into it and seeing what comes out of it. And Rachel, I know you've seen similar things in your past teaching experience. Yes, I've seen this in teaching full stack software development to students. And I also know that I've been guilty of it myself sometimes. And that was that students would sometimes kind of rather than start their project, they would keep doing more and more research, reading more and more tutorials, and feeling like there's more and more they need to learn before they can start coding. And two problems with that are one, and I mean, you want to have some background before you begin, but there's a point where you just need to start coding because you can't know exactly what you're going to need until you start coding and building and seeing what errors you get and what things you don't know how to do. And then secondly, the test of whether you understand something is whether you can build with it. And so kind of reading tutorials, it's very possible to think, oh, I understand all this, but it's not till you're writing code yourself, kind of seeing what your what your error rates are and what what's working and what's not that you know whether or not you truly understand something. Yeah. So when I saw students at the study sessions during the week at USF, I would keep telling them the same thing again and again. Just don't stop and wait till you feel ready to code. Start coding now. And it's through that coding experience that you're actually going to figure out what you don't know and what you do know. And you'll be able to develop the intuition by running lots of experiments. This is another interesting quote from somebody talking about this learning style. He said, it's been very interesting learning from somebody who is an entrepreneur. That'd be me. A very no-nonsense approach to getting things done, very hands on, very smart and driven. Your usual career and structure is quite the opposite. So it's been refreshing and even somewhat shocking. This is possibly understating things a bit. It can be. In fact, we heard from quite a few people at the start. It was somewhat shocking to find so many things taught so quickly and it kind of can seem like such a high level. But of course, by the end of the seven weeks, and assuming that each time you're putting 10 hours into those weeks, you've actually got many, many full end-to-end processors under your belt. So by the end of it, you're actually going to develop a very deep and complete understanding. Yeah, I know after the first lesson, I heard a number of students kind of say things like, oh, I didn't really get the details from that lesson and I feel like I need to spend all this time studying the details. And we hadn't taught the details in the first lesson. And the idea is that kind of we went more and more in depth each time. But you're seeing this end-to-end process and then kind of as time goes on, digging into it more. But even after the first lesson, you can apply it. You can actually create world class image recognition models. And so you can go back to your organization and start trying things. This is something else we encourage people to do. Try things with your own data and your own problems from the very first lesson. So it's been an interesting experience in every way. Even the way we built this course was unusual. For example, I actually wrote most of the material while traveling from the northern tip to the southern tip of Japan. I coded and wrote in every possible place you can imagine. And this was really an experiment for me because I studied human learning theory a lot. And I know that in theory, human creativity is meant to be better when you have a wider variety of contexts. And interestingly, I actually found I was more productive in that month than I feel I ever have been before. And you'll see actually in the material that you learn, we show a lot of new techniques or different techniques or different ways of thinking about things. And I think this kind of different way of building the course, perhaps, was really helpful in coming up with this kind of more creative approach. So not everybody in the course, in the in-person course, were able to put in at least eight hours a week, but the vast majority were. And those who didn't still completed the course, they just found they didn't necessarily pick everything up the way that they hoped they would. But of course, the nice thing is you can always come back to it later. So our suggestion would be now that it's a MOOC, now that you don't have to do it every single week, ideally you will put in the 10 hours a week. Did you want to talk about those 10 hours a little bit, Rachel? Yeah, we wanted to give you kind of some suggestions on how to use that time. So the videos are between two and two and a half hours long. And so with those videos, you may find it helpful as you review them to use these notes. So this is coming from our Wiki, wiki.fast.ai. There's a page for each lesson that has notes kind of about the lesson. It also has links to other other resources that may be useful to you. These notes are pretty complete. They're not designed to be read entirely independently from the video lesson, but they are something which you can read on your way to work. Maybe when you don't have, it's not convenient to actually watch the lesson. Sorry, I was going to say we're expecting that you'll watch the lessons more than once. So the first time through, you're kind of watching to get maybe a lot of the high level ideas. Then you'll probably want to read the Wiki, try out the notebooks and then go back and watch the lesson again. Kind of maybe to get more detail. Yeah, I don't think any of our students in the in-person course just watch the lessons once. They saw them live, of course, but then we also they also had the recording for the next day. And I think everybody has spoken to watch them at least twice. And then, of course, the other thing you've got is the notebooks. The notebooks, as you see, have quite a lot of pros in them as well. They've got quite a lot of additional detail that we don't necessarily get into in the video lesson. But most importantly, as we described, they give you an environment in which you can experiment. In fact, not only do we suggest that you experiment, we have a very specific suggestion about how to use these notebooks. Yeah, so we recommend that you read through the notebook and then, and this is after you've watched the video at least once. If everything makes sense, put it aside and try creating a new notebook where you go through that process yourself. And so this is from scratch, right? This is like creating your own notebook to test that you can actually build it yourself. Yeah, we do not want you to just hit Shift Enter, Shift Enter and run through the existing notebook. Because again, the test of whether you know something is can you build and code with it yourself. So if you get stuck, you can always then go back and refer to the class notebook. And then rather than copying and pasting it, make sure you do understand what it's saying. Maybe look up some documentation about that concept and then put that notebook aside and see if you can now do it yourself. So in a sense, you're plagiarizing a lot from the notebook, but you're plagiarizing in a good way. You know, you're plagiarizing not by copying and pasting, but by plagiarizing the concepts and making sure that you can recreate them yourself. And then if you have questions, please ask them. The forums are the first place you should go to first search to see if someone's already asked your question. As we said earlier, there is a separate thread for each lesson that are already have tons of helpful questions and answers from the students that took our in-person course. In fact, there's a great quote which we talk about in one of the lessons from the head of Google Brain, who says that their rule at Google Brain is that if you have a problem, you first of all try to fix it yourself for half an hour. And if after half an hour you can't fix it yourself, you then have to ask somebody. So that ensures that you always give it a go yourself and hopefully learn from the experience, but you never waste too much time on something which somebody else can help you with. Yes, that's great advice. So as Rachel said, the forums are a really helpful resource. And when you go to the forums, you'll find that there's a lot of existing discussions. There's a separate discussion for every lesson, for example, and each and each of those discussions, you'll see that there's a summary of the existing discussion at the start. So you may find that what you need is already in the question and answers there. And if it's not, of course, feel free to add your question and you'll generally find it's responded to within a small number of hours, maybe by Rachel or I or maybe by one of the other students. The other thing you may find helpful is that each lesson has a timeline on the Wiki and those hyperlinks are actually hyperlinks directly to the part of the video which discusses that topic. So if you're trying to remember how momentum works, you can just click on that link and you'll jump straight to me telling you about momentum. As Rachel said, there's also a number of resources available to help you. So this is taken from the front page of our Wiki. There's a whole section of tools with links where you can learn about learn more about each of the pieces that we use in the development environment. And so our goal here is not to be a single source of truth. If somebody else has already done a great job of teaching one of these tools, we'll leave it to them. So we don't attempt to give you a great bash reference or a great umpire reference because people have already done that. So if you want to learn more about one of these things, jump onto the Wiki, click through here and you'll find some curated resources that we think are really helpful. So this lesson, you're going to cover a lot of stuff, but these are the four things to keep in mind. By the end of the lesson, you want to make sure that you can create an AWS instance, that you can connect to it with SSH, that you can run a Jupyter Notebook in it, and that you can run those that state of the art custom model code that we showed you earlier. Those first three things, you're going to be doing every single project in every single lesson. So you're going to want to be really comfortable at doing that. And for those of you who haven't done that before, it might take you a little while to get the hang of it and maybe a few unsuccessful attempts first. So this first lesson is unusual in that it's a lot more about kind of getting your development environment set up and not as much about deep learning. So indeed, if you've got a background in Python and AWS and Linux, you may find this lesson on the easy side, in which case you can zip through it pretty fast. If you don't have a background in these tools, today's class may seem really overwhelming. And we don't want you to be discouraged by that because this is very different from the future lessons, but it's necessary to get your environment set up so that you can be coding throughout the course. Yeah, I mean, the folks who didn't have that background in the in-person course, once they actually got through this, and often it was a lot of work and it was pretty tough, but at the end, they finally got the point and they could say, OK, I've set up a GPU instance in the cloud. I've set up my development environment and I have trained from scratch a model that can recognize dogs from cats. And it was very, very exciting. So if this is hard work for you, just know that when you get through the other end of it, it's going to be really exciting. So I asked that everyone who's trying to decide if this course is for them, try at least the first two lessons, since the first lesson is so much about setup. Yeah, these lessons become obviously we build more and more on the techniques we've learned. And so we're going to be using this infrastructure in every lesson. By the time we get to lesson seven, we're going to be looking at some pretty sophisticated and custom neural network architectures. We're going to cover every different type of SGD optimization. We're going to be covering convolutional neural networks and recurrent neural networks. So there's going to be a lot of exciting stuff. And yeah, we really look forward to seeing you on the forums and good luck with learning about deep learning.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.0, "text": " Hi and welcome to lesson one of this deep learning MOOC. Thanks for joining us. I'm Jeremy Howard.", "tokens": [2421, 293, 2928, 281, 6898, 472, 295, 341, 2452, 2539, 49197, 34, 13, 2561, 337, 5549, 505, 13, 286, 478, 17809, 17626, 13], "temperature": 0.0, "avg_logprob": -0.1298368783821737, "compression_ratio": 1.6246153846153846, "no_speech_prob": 0.01300816424190998}, {"id": 1, "seek": 0, "start": 7.0, "end": 9.0, "text": " And I'm Rachel Thomas.", "tokens": [400, 286, 478, 14246, 8500, 13], "temperature": 0.0, "avg_logprob": -0.1298368783821737, "compression_ratio": 1.6246153846153846, "no_speech_prob": 0.01300816424190998}, {"id": 2, "seek": 0, "start": 9.0, "end": 15.0, "text": " We're the people who put this together. You'll be seeing my face in front of the camera most of the time.", "tokens": [492, 434, 264, 561, 567, 829, 341, 1214, 13, 509, 603, 312, 2577, 452, 1851, 294, 1868, 295, 264, 2799, 881, 295, 264, 565, 13], "temperature": 0.0, "avg_logprob": -0.1298368783821737, "compression_ratio": 1.6246153846153846, "no_speech_prob": 0.01300816424190998}, {"id": 3, "seek": 0, "start": 15.0, "end": 17.0, "text": " You'll be hearing Rachel's voice, however.", "tokens": [509, 603, 312, 4763, 14246, 311, 3177, 11, 4461, 13], "temperature": 0.0, "avg_logprob": -0.1298368783821737, "compression_ratio": 1.6246153846153846, "no_speech_prob": 0.01300816424190998}, {"id": 4, "seek": 0, "start": 17.0, "end": 24.0, "text": " And I'll be asking questions that are coming through a Slack channel asked by students in the in-person version of this course.", "tokens": [400, 286, 603, 312, 3365, 1651, 300, 366, 1348, 807, 257, 37211, 2269, 2351, 538, 1731, 294, 264, 294, 12, 10813, 3037, 295, 341, 1164, 13], "temperature": 0.0, "avg_logprob": -0.1298368783821737, "compression_ratio": 1.6246153846153846, "no_speech_prob": 0.01300816424190998}, {"id": 5, "seek": 0, "start": 24.0, "end": 29.0, "text": " So we thought before we started we should tell you a little bit about what to expect and maybe you'd get to know us a little bit.", "tokens": [407, 321, 1194, 949, 321, 1409, 321, 820, 980, 291, 257, 707, 857, 466, 437, 281, 2066, 293, 1310, 291, 1116, 483, 281, 458, 505, 257, 707, 857, 13], "temperature": 0.0, "avg_logprob": -0.1298368783821737, "compression_ratio": 1.6246153846153846, "no_speech_prob": 0.01300816424190998}, {"id": 6, "seek": 2900, "start": 29.0, "end": 39.0, "text": " My background is really in coding and data. I spent 10 years of management consulting and 10 years running startups.", "tokens": [1222, 3678, 307, 534, 294, 17720, 293, 1412, 13, 286, 4418, 1266, 924, 295, 4592, 23682, 293, 1266, 924, 2614, 28041, 13], "temperature": 0.0, "avg_logprob": -0.09254516043314119, "compression_ratio": 1.5751072961373391, "no_speech_prob": 0.00010379909508628771}, {"id": 7, "seek": 2900, "start": 39.0, "end": 44.0, "text": " Throughout that time I've been using data and machine learning to try to solve problems.", "tokens": [22775, 300, 565, 286, 600, 668, 1228, 1412, 293, 3479, 2539, 281, 853, 281, 5039, 2740, 13], "temperature": 0.0, "avg_logprob": -0.09254516043314119, "compression_ratio": 1.5751072961373391, "no_speech_prob": 0.00010379909508628771}, {"id": 8, "seek": 2900, "start": 44.0, "end": 54.0, "text": " My background is a lot more academic and theoretical. I have a PhD in math and then I worked as a quant, later as a software engineer and data scientist at Uber.", "tokens": [1222, 3678, 307, 257, 688, 544, 7778, 293, 20864, 13, 286, 362, 257, 14476, 294, 5221, 293, 550, 286, 2732, 382, 257, 4426, 11, 1780, 382, 257, 4722, 11403, 293, 1412, 12662, 412, 21839, 13], "temperature": 0.0, "avg_logprob": -0.09254516043314119, "compression_ratio": 1.5751072961373391, "no_speech_prob": 0.00010379909508628771}, {"id": 9, "seek": 5400, "start": 54.0, "end": 62.0, "text": " One of the most fun and exciting parts of my life was when I spent some time really competing heavily in Kaggle competitions.", "tokens": [1485, 295, 264, 881, 1019, 293, 4670, 3166, 295, 452, 993, 390, 562, 286, 4418, 512, 565, 534, 15439, 10950, 294, 48751, 22631, 26185, 13], "temperature": 0.0, "avg_logprob": -0.0751906308260831, "compression_ratio": 1.9525691699604744, "no_speech_prob": 5.3031588322483e-05}, {"id": 10, "seek": 5400, "start": 62.0, "end": 67.0, "text": " I was really pleased to win some of those competitions and get to the top of the leaderboard.", "tokens": [286, 390, 534, 10587, 281, 1942, 512, 295, 729, 26185, 293, 483, 281, 264, 1192, 295, 264, 5263, 3787, 13], "temperature": 0.0, "avg_logprob": -0.0751906308260831, "compression_ratio": 1.9525691699604744, "no_speech_prob": 5.3031588322483e-05}, {"id": 11, "seek": 5400, "start": 67.0, "end": 72.0, "text": " And I'm hoping to show you guys during this course some of the techniques that I used to do that.", "tokens": [400, 286, 478, 7159, 281, 855, 291, 1074, 1830, 341, 1164, 512, 295, 264, 7512, 300, 286, 1143, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.0751906308260831, "compression_ratio": 1.9525691699604744, "no_speech_prob": 5.3031588322483e-05}, {"id": 12, "seek": 7200, "start": 72.0, "end": 84.0, "text": " I think the techniques that allow you to win Kaggle competitions are the same as the techniques that allow you to create great results on your own models in solving your own problems.", "tokens": [286, 519, 264, 7512, 300, 2089, 291, 281, 1942, 48751, 22631, 26185, 366, 264, 912, 382, 264, 7512, 300, 2089, 291, 281, 1884, 869, 3542, 322, 428, 1065, 5245, 294, 12606, 428, 1065, 2740, 13], "temperature": 0.0, "avg_logprob": -0.07153874571605395, "compression_ratio": 1.6947791164658634, "no_speech_prob": 3.4801018045982346e-05}, {"id": 13, "seek": 7200, "start": 84.0, "end": 90.0, "text": " We also both love teaching. So I taught calculus one and two when I was in graduate school.", "tokens": [492, 611, 1293, 959, 4571, 13, 407, 286, 5928, 33400, 472, 293, 732, 562, 286, 390, 294, 8080, 1395, 13], "temperature": 0.0, "avg_logprob": -0.07153874571605395, "compression_ratio": 1.6947791164658634, "no_speech_prob": 3.4801018045982346e-05}, {"id": 14, "seek": 7200, "start": 90.0, "end": 98.0, "text": " And then I later left my job as a software engineer to teach full stack software development to women at Hackbright Academy for a year and a half.", "tokens": [400, 550, 286, 1780, 1411, 452, 1691, 382, 257, 4722, 11403, 281, 2924, 1577, 8630, 4722, 3250, 281, 2266, 412, 35170, 21553, 11735, 337, 257, 1064, 293, 257, 1922, 13], "temperature": 0.0, "avg_logprob": -0.07153874571605395, "compression_ratio": 1.6947791164658634, "no_speech_prob": 3.4801018045982346e-05}, {"id": 15, "seek": 9800, "start": 98.0, "end": 105.0, "text": " I think that's really cool. Rachel was a quant and then she worked as both a data scientist and a full stack engineer at Uber.", "tokens": [286, 519, 300, 311, 534, 1627, 13, 14246, 390, 257, 4426, 293, 550, 750, 2732, 382, 1293, 257, 1412, 12662, 293, 257, 1577, 8630, 11403, 412, 21839, 13], "temperature": 0.0, "avg_logprob": -0.07458510307165292, "compression_ratio": 1.5709090909090908, "no_speech_prob": 8.88508657226339e-05}, {"id": 16, "seek": 9800, "start": 105.0, "end": 111.0, "text": " But she realized that one of the highest leverage things you can do is to teach and it's great fun too.", "tokens": [583, 750, 5334, 300, 472, 295, 264, 6343, 13982, 721, 291, 393, 360, 307, 281, 2924, 293, 309, 311, 869, 1019, 886, 13], "temperature": 0.0, "avg_logprob": -0.07458510307165292, "compression_ratio": 1.5709090909090908, "no_speech_prob": 8.88508657226339e-05}, {"id": 17, "seek": 9800, "start": 111.0, "end": 118.0, "text": " I feel the same way even when I was running startups. I was creating course content online.", "tokens": [286, 841, 264, 912, 636, 754, 562, 286, 390, 2614, 28041, 13, 286, 390, 4084, 1164, 2701, 2950, 13], "temperature": 0.0, "avg_logprob": -0.07458510307165292, "compression_ratio": 1.5709090909090908, "no_speech_prob": 8.88508657226339e-05}, {"id": 18, "seek": 9800, "start": 118.0, "end": 125.0, "text": " For example, on the left here is an AngularJS tutorial that I originally created for my colleagues at Kaggle.", "tokens": [1171, 1365, 11, 322, 264, 1411, 510, 307, 364, 34107, 41, 50, 7073, 300, 286, 7993, 2942, 337, 452, 7734, 412, 48751, 22631, 13], "temperature": 0.0, "avg_logprob": -0.07458510307165292, "compression_ratio": 1.5709090909090908, "no_speech_prob": 8.88508657226339e-05}, {"id": 19, "seek": 12500, "start": 125.0, "end": 129.0, "text": " But I recorded it and put it online and it's had over 200,000 views.", "tokens": [583, 286, 8287, 309, 293, 829, 309, 2950, 293, 309, 311, 632, 670, 2331, 11, 1360, 6809, 13], "temperature": 0.0, "avg_logprob": -0.0695111432026342, "compression_ratio": 1.5670498084291187, "no_speech_prob": 4.98495101055596e-05}, {"id": 20, "seek": 12500, "start": 129.0, "end": 139.0, "text": " It makes me feel really good to know that people are learning from some of the things that I found really helpful myself.", "tokens": [467, 1669, 385, 841, 534, 665, 281, 458, 300, 561, 366, 2539, 490, 512, 295, 264, 721, 300, 286, 1352, 534, 4961, 2059, 13], "temperature": 0.0, "avg_logprob": -0.0695111432026342, "compression_ratio": 1.5670498084291187, "no_speech_prob": 4.98495101055596e-05}, {"id": 21, "seek": 12500, "start": 139.0, "end": 146.0, "text": " So this is a quote from Paul Lockhart who was actually working as a primary school math teacher,", "tokens": [407, 341, 307, 257, 6513, 490, 4552, 16736, 42535, 567, 390, 767, 1364, 382, 257, 6194, 1395, 5221, 5027, 11], "temperature": 0.0, "avg_logprob": -0.0695111432026342, "compression_ratio": 1.5670498084291187, "no_speech_prob": 4.98495101055596e-05}, {"id": 22, "seek": 12500, "start": 146.0, "end": 152.0, "text": " got his PhD in math at Columbia and became a math professor at Brown and then left to go back to teaching primary school.", "tokens": [658, 702, 14476, 294, 5221, 412, 17339, 293, 3062, 257, 5221, 8304, 412, 8030, 293, 550, 1411, 281, 352, 646, 281, 4571, 6194, 1395, 13], "temperature": 0.0, "avg_logprob": -0.0695111432026342, "compression_ratio": 1.5670498084291187, "no_speech_prob": 4.98495101055596e-05}, {"id": 23, "seek": 15200, "start": 152.0, "end": 159.0, "text": " He's written a wonderful essay called A Mathematician's Lament on Everything That's Horrible about How Mathematics is Taught in the United States.", "tokens": [634, 311, 3720, 257, 3715, 16238, 1219, 316, 15776, 14911, 952, 311, 441, 2466, 322, 5471, 663, 311, 10691, 4457, 466, 1012, 15776, 37541, 307, 6551, 1599, 294, 264, 2824, 3040, 13], "temperature": 0.0, "avg_logprob": -0.08491593671132283, "compression_ratio": 1.5333333333333334, "no_speech_prob": 2.014278288697824e-05}, {"id": 24, "seek": 15200, "start": 159.0, "end": 164.0, "text": " I think that essay has been really influential to both Rachel and I.", "tokens": [286, 519, 300, 16238, 575, 668, 534, 22215, 281, 1293, 14246, 293, 286, 13], "temperature": 0.0, "avg_logprob": -0.08491593671132283, "compression_ratio": 1.5333333333333334, "no_speech_prob": 2.014278288697824e-05}, {"id": 25, "seek": 15200, "start": 164.0, "end": 169.0, "text": " Although Rachel stuck with her math education for decades longer than I did,", "tokens": [5780, 14246, 5541, 365, 720, 5221, 3309, 337, 7878, 2854, 813, 286, 630, 11], "temperature": 0.0, "avg_logprob": -0.08491593671132283, "compression_ratio": 1.5333333333333334, "no_speech_prob": 2.014278288697824e-05}, {"id": 26, "seek": 15200, "start": 169.0, "end": 174.0, "text": " we both definitely felt like modern mathematics education is not done well.", "tokens": [321, 1293, 2138, 2762, 411, 4363, 18666, 3309, 307, 406, 1096, 731, 13], "temperature": 0.0, "avg_logprob": -0.08491593671132283, "compression_ratio": 1.5333333333333334, "no_speech_prob": 2.014278288697824e-05}, {"id": 27, "seek": 17400, "start": 174.0, "end": 183.0, "text": " Paul Lockhart uses a wonderful analogy about imagine if with music we didn't allow children to sing or play instruments", "tokens": [4552, 16736, 42535, 4960, 257, 3715, 21663, 466, 3811, 498, 365, 1318, 321, 994, 380, 2089, 2227, 281, 1522, 420, 862, 12190], "temperature": 0.0, "avg_logprob": -0.07830741882324219, "compression_ratio": 1.6313868613138687, "no_speech_prob": 1.952360980794765e-05}, {"id": 28, "seek": 17400, "start": 183.0, "end": 191.0, "text": " until they had spent years and even decades studying set theory and music notation and could transcribe scales.", "tokens": [1826, 436, 632, 4418, 924, 293, 754, 7878, 7601, 992, 5261, 293, 1318, 24657, 293, 727, 1145, 8056, 17408, 13], "temperature": 0.0, "avg_logprob": -0.07830741882324219, "compression_ratio": 1.6313868613138687, "no_speech_prob": 1.952360980794765e-05}, {"id": 29, "seek": 17400, "start": 191.0, "end": 195.0, "text": " And only then, once they were in their 20s, we would let them create music.", "tokens": [400, 787, 550, 11, 1564, 436, 645, 294, 641, 945, 82, 11, 321, 576, 718, 552, 1884, 1318, 13], "temperature": 0.0, "avg_logprob": -0.07830741882324219, "compression_ratio": 1.6313868613138687, "no_speech_prob": 1.952360980794765e-05}, {"id": 30, "seek": 17400, "start": 195.0, "end": 203.0, "text": " He says that's exactly what we're doing with mathematics, but that we should let people kind of play and create and build patterns with it.", "tokens": [634, 1619, 300, 311, 2293, 437, 321, 434, 884, 365, 18666, 11, 457, 300, 321, 820, 718, 561, 733, 295, 862, 293, 1884, 293, 1322, 8294, 365, 309, 13], "temperature": 0.0, "avg_logprob": -0.07830741882324219, "compression_ratio": 1.6313868613138687, "no_speech_prob": 1.952360980794765e-05}, {"id": 31, "seek": 20300, "start": 203.0, "end": 207.0, "text": " And something very similar happens with deep learning and how it's taught.", "tokens": [400, 746, 588, 2531, 2314, 365, 2452, 2539, 293, 577, 309, 311, 5928, 13], "temperature": 0.0, "avg_logprob": -0.08549566378538635, "compression_ratio": 1.576, "no_speech_prob": 6.812139326939359e-05}, {"id": 32, "seek": 20300, "start": 207.0, "end": 218.0, "text": " In fact, one of my heroes is a guy called David Perkins who at Harvard has created some really interesting research about effective educational techniques.", "tokens": [682, 1186, 11, 472, 295, 452, 12332, 307, 257, 2146, 1219, 4389, 3026, 10277, 567, 412, 13378, 575, 2942, 512, 534, 1880, 2132, 466, 4942, 10189, 7512, 13], "temperature": 0.0, "avg_logprob": -0.08549566378538635, "compression_ratio": 1.576, "no_speech_prob": 6.812139326939359e-05}, {"id": 33, "seek": 20300, "start": 218.0, "end": 223.0, "text": " And he has a very similar analogy to Paul Lockhart, but he talks about baseball.", "tokens": [400, 415, 575, 257, 588, 2531, 21663, 281, 4552, 16736, 42535, 11, 457, 415, 6686, 466, 14323, 13], "temperature": 0.0, "avg_logprob": -0.08549566378538635, "compression_ratio": 1.576, "no_speech_prob": 6.812139326939359e-05}, {"id": 34, "seek": 20300, "start": 223.0, "end": 227.0, "text": " Imagine if the way you learned baseball was that you never saw a game of baseball,", "tokens": [11739, 498, 264, 636, 291, 3264, 14323, 390, 300, 291, 1128, 1866, 257, 1216, 295, 14323, 11], "temperature": 0.0, "avg_logprob": -0.08549566378538635, "compression_ratio": 1.576, "no_speech_prob": 6.812139326939359e-05}, {"id": 35, "seek": 22700, "start": 227.0, "end": 234.0, "text": " but instead you learned about how to stitch a baseball and you learned the physics of a parabola and you learned every aspect of baseball.", "tokens": [457, 2602, 291, 3264, 466, 577, 281, 5635, 257, 14323, 293, 291, 3264, 264, 10649, 295, 257, 45729, 4711, 293, 291, 3264, 633, 4171, 295, 14323, 13], "temperature": 0.0, "avg_logprob": -0.0629092810446756, "compression_ratio": 1.7050847457627119, "no_speech_prob": 1.7501057300250977e-05}, {"id": 36, "seek": 22700, "start": 234.0, "end": 240.0, "text": " And then after 20 years of study, you'd be considered good enough to go and actually watch your first game.", "tokens": [400, 550, 934, 945, 924, 295, 2979, 11, 291, 1116, 312, 4888, 665, 1547, 281, 352, 293, 767, 1159, 428, 700, 1216, 13], "temperature": 0.0, "avg_logprob": -0.0629092810446756, "compression_ratio": 1.7050847457627119, "no_speech_prob": 1.7501057300250977e-05}, {"id": 37, "seek": 22700, "start": 240.0, "end": 248.0, "text": " We tend to think that this is rather the way that most mathematics, perhaps particularly including deep learning, is really taught.", "tokens": [492, 3928, 281, 519, 300, 341, 307, 2831, 264, 636, 300, 881, 18666, 11, 4317, 4098, 3009, 2452, 2539, 11, 307, 534, 5928, 13], "temperature": 0.0, "avg_logprob": -0.0629092810446756, "compression_ratio": 1.7050847457627119, "no_speech_prob": 1.7501057300250977e-05}, {"id": 38, "seek": 22700, "start": 248.0, "end": 256.0, "text": " So we decided when we set up our research lab, Fast AI, that the first thing we would do would be to try and fill this need.", "tokens": [407, 321, 3047, 562, 321, 992, 493, 527, 2132, 2715, 11, 15968, 7318, 11, 300, 264, 700, 551, 321, 576, 360, 576, 312, 281, 853, 293, 2836, 341, 643, 13], "temperature": 0.0, "avg_logprob": -0.0629092810446756, "compression_ratio": 1.7050847457627119, "no_speech_prob": 1.7501057300250977e-05}, {"id": 39, "seek": 25600, "start": 256.0, "end": 266.0, "text": " And particularly we decided to focus on deep learning because we both think that deep learning is the most exciting technology that we have ever seen.", "tokens": [400, 4098, 321, 3047, 281, 1879, 322, 2452, 2539, 570, 321, 1293, 519, 300, 2452, 2539, 307, 264, 881, 4670, 2899, 300, 321, 362, 1562, 1612, 13], "temperature": 0.0, "avg_logprob": -0.09526619044217197, "compression_ratio": 1.7217391304347827, "no_speech_prob": 4.46899575763382e-05}, {"id": 40, "seek": 25600, "start": 266.0, "end": 270.0, "text": " We think it's going to be more transformative than even the Internet.", "tokens": [492, 519, 309, 311, 516, 281, 312, 544, 36070, 813, 754, 264, 7703, 13], "temperature": 0.0, "avg_logprob": -0.09526619044217197, "compression_ratio": 1.7217391304347827, "no_speech_prob": 4.46899575763382e-05}, {"id": 41, "seek": 25600, "start": 270.0, "end": 273.0, "text": " And so the more people who can participate, the better.", "tokens": [400, 370, 264, 544, 561, 567, 393, 8197, 11, 264, 1101, 13], "temperature": 0.0, "avg_logprob": -0.09526619044217197, "compression_ratio": 1.7217391304347827, "no_speech_prob": 4.46899575763382e-05}, {"id": 42, "seek": 25600, "start": 273.0, "end": 280.0, "text": " Andrew Ng has called it the new electricity, to say that it's going to have the impact on society that electricity has.", "tokens": [10110, 21198, 575, 1219, 309, 264, 777, 10356, 11, 281, 584, 300, 309, 311, 516, 281, 362, 264, 2712, 322, 4086, 300, 10356, 575, 13], "temperature": 0.0, "avg_logprob": -0.09526619044217197, "compression_ratio": 1.7217391304347827, "no_speech_prob": 4.46899575763382e-05}, {"id": 43, "seek": 28000, "start": 280.0, "end": 286.0, "text": " So some of the kinds of problems we've seen with technical teaching include these.", "tokens": [407, 512, 295, 264, 3685, 295, 2740, 321, 600, 1612, 365, 6191, 4571, 4090, 613, 13], "temperature": 0.0, "avg_logprob": -0.06637421567389305, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.00011230727250222117}, {"id": 44, "seek": 28000, "start": 286.0, "end": 290.0, "text": " And I just want to say we're introducing this to tell you that this course is taught in a very different style.", "tokens": [400, 286, 445, 528, 281, 584, 321, 434, 15424, 341, 281, 980, 291, 300, 341, 1164, 307, 5928, 294, 257, 588, 819, 3758, 13], "temperature": 0.0, "avg_logprob": -0.06637421567389305, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.00011230727250222117}, {"id": 45, "seek": 28000, "start": 290.0, "end": 296.0, "text": " And so we want to kind of set your expectations ahead of time and motivate why it's so different, how we teach it here.", "tokens": [400, 370, 321, 528, 281, 733, 295, 992, 428, 9843, 2286, 295, 565, 293, 28497, 983, 309, 311, 370, 819, 11, 577, 321, 2924, 309, 510, 13], "temperature": 0.0, "avg_logprob": -0.06637421567389305, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.00011230727250222117}, {"id": 46, "seek": 28000, "start": 296.0, "end": 302.0, "text": " And one is that a lot of existing deep learning materials are very math centric.", "tokens": [400, 472, 307, 300, 257, 688, 295, 6741, 2452, 2539, 5319, 366, 588, 5221, 1489, 1341, 13], "temperature": 0.0, "avg_logprob": -0.06637421567389305, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.00011230727250222117}, {"id": 47, "seek": 30200, "start": 302.0, "end": 311.0, "text": " And even as a mathematician and someone who loves math, I found them to be pretty unhelpful for actually building and creating practical applications.", "tokens": [400, 754, 382, 257, 48281, 293, 1580, 567, 6752, 5221, 11, 286, 1352, 552, 281, 312, 1238, 517, 37451, 906, 337, 767, 2390, 293, 4084, 8496, 5821, 13], "temperature": 0.0, "avg_logprob": -0.04870619394082939, "compression_ratio": 1.6996466431095407, "no_speech_prob": 0.00010386583744548261}, {"id": 48, "seek": 30200, "start": 311.0, "end": 319.0, "text": " In fact, every time I see somebody ask on a forum or on Hacker News or whatever, what do I need in order to get into deep learning,", "tokens": [682, 1186, 11, 633, 565, 286, 536, 2618, 1029, 322, 257, 17542, 420, 322, 389, 23599, 7987, 420, 2035, 11, 437, 360, 286, 643, 294, 1668, 281, 483, 666, 2452, 2539, 11], "temperature": 0.0, "avg_logprob": -0.04870619394082939, "compression_ratio": 1.6996466431095407, "no_speech_prob": 0.00010386583744548261}, {"id": 49, "seek": 30200, "start": 319.0, "end": 325.0, "text": " a whole bunch of people reply by saying, well, first you need five years of real analysis and vector analysis,", "tokens": [257, 1379, 3840, 295, 561, 16972, 538, 1566, 11, 731, 11, 700, 291, 643, 1732, 924, 295, 957, 5215, 293, 8062, 5215, 11], "temperature": 0.0, "avg_logprob": -0.04870619394082939, "compression_ratio": 1.6996466431095407, "no_speech_prob": 0.00010386583744548261}, {"id": 50, "seek": 30200, "start": 325.0, "end": 329.0, "text": " and then you need to study probability and statistics and blah, blah, blah, blah, blah.", "tokens": [293, 550, 291, 643, 281, 2979, 8482, 293, 12523, 293, 12288, 11, 12288, 11, 12288, 11, 12288, 11, 12288, 13], "temperature": 0.0, "avg_logprob": -0.04870619394082939, "compression_ratio": 1.6996466431095407, "no_speech_prob": 0.00010386583744548261}, {"id": 51, "seek": 32900, "start": 329.0, "end": 336.0, "text": " And it really comes across to me as something which is all about being exclusive rather than inclusive.", "tokens": [400, 309, 534, 1487, 2108, 281, 385, 382, 746, 597, 307, 439, 466, 885, 13005, 2831, 813, 13429, 13], "temperature": 0.0, "avg_logprob": -0.10167667713571102, "compression_ratio": 1.660633484162896, "no_speech_prob": 5.30690340383444e-05}, {"id": 52, "seek": 32900, "start": 336.0, "end": 343.0, "text": " So that's why we have this little thing, making neural nets uncool again is kind of our goal.", "tokens": [407, 300, 311, 983, 321, 362, 341, 707, 551, 11, 1455, 18161, 36170, 6219, 1092, 797, 307, 733, 295, 527, 3387, 13], "temperature": 0.0, "avg_logprob": -0.10167667713571102, "compression_ratio": 1.660633484162896, "no_speech_prob": 5.30690340383444e-05}, {"id": 53, "seek": 32900, "start": 343.0, "end": 344.0, "text": " The Fast AI slogan, yeah.", "tokens": [440, 15968, 7318, 33052, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.10167667713571102, "compression_ratio": 1.660633484162896, "no_speech_prob": 5.30690340383444e-05}, {"id": 54, "seek": 32900, "start": 344.0, "end": 346.0, "text": " The Fast AI slogan.", "tokens": [440, 15968, 7318, 33052, 13], "temperature": 0.0, "avg_logprob": -0.10167667713571102, "compression_ratio": 1.660633484162896, "no_speech_prob": 5.30690340383444e-05}, {"id": 55, "seek": 32900, "start": 346.0, "end": 354.0, "text": " We're all about not being exclusive, but about making things as simple as possible, but never about dumbing it down.", "tokens": [492, 434, 439, 466, 406, 885, 13005, 11, 457, 466, 1455, 721, 382, 2199, 382, 1944, 11, 457, 1128, 466, 10316, 278, 309, 760, 13], "temperature": 0.0, "avg_logprob": -0.10167667713571102, "compression_ratio": 1.660633484162896, "no_speech_prob": 5.30690340383444e-05}, {"id": 56, "seek": 32900, "start": 354.0, "end": 356.0, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.10167667713571102, "compression_ratio": 1.660633484162896, "no_speech_prob": 5.30690340383444e-05}, {"id": 57, "seek": 35600, "start": 356.0, "end": 367.0, "text": " Another way that kind of technical education fails is what David Perkins, the Harvard professor Jeremy mentioned a moment ago, calls elementitis.", "tokens": [3996, 636, 300, 733, 295, 6191, 3309, 18199, 307, 437, 4389, 3026, 10277, 11, 264, 13378, 8304, 17809, 2835, 257, 1623, 2057, 11, 5498, 4478, 16074, 13], "temperature": 0.0, "avg_logprob": -0.06745979377815316, "compression_ratio": 1.7415730337078652, "no_speech_prob": 1.2410201634338591e-05}, {"id": 58, "seek": 35600, "start": 367.0, "end": 370.0, "text": " And that's that often math does this so much.", "tokens": [400, 300, 311, 300, 2049, 5221, 775, 341, 370, 709, 13], "temperature": 0.0, "avg_logprob": -0.06745979377815316, "compression_ratio": 1.7415730337078652, "no_speech_prob": 1.2410201634338591e-05}, {"id": 59, "seek": 35600, "start": 370.0, "end": 373.0, "text": " It teaches kind of each separate element.", "tokens": [467, 16876, 733, 295, 1184, 4994, 4478, 13], "temperature": 0.0, "avg_logprob": -0.06745979377815316, "compression_ratio": 1.7415730337078652, "no_speech_prob": 1.2410201634338591e-05}, {"id": 60, "seek": 35600, "start": 373.0, "end": 378.0, "text": " And it's only at the end when you've learned all the elements needed that you can put them together and see the whole thing.", "tokens": [400, 309, 311, 787, 412, 264, 917, 562, 291, 600, 3264, 439, 264, 4959, 2978, 300, 291, 393, 829, 552, 1214, 293, 536, 264, 1379, 551, 13], "temperature": 0.0, "avg_logprob": -0.06745979377815316, "compression_ratio": 1.7415730337078652, "no_speech_prob": 1.2410201634338591e-05}, {"id": 61, "seek": 35600, "start": 378.0, "end": 381.0, "text": " And that's kind of what was going on in that baseball analogy.", "tokens": [400, 300, 311, 733, 295, 437, 390, 516, 322, 294, 300, 14323, 21663, 13], "temperature": 0.0, "avg_logprob": -0.06745979377815316, "compression_ratio": 1.7415730337078652, "no_speech_prob": 1.2410201634338591e-05}, {"id": 62, "seek": 35600, "start": 381.0, "end": 382.0, "text": " And that happens in a lot of deep learning.", "tokens": [400, 300, 2314, 294, 257, 688, 295, 2452, 2539, 13], "temperature": 0.0, "avg_logprob": -0.06745979377815316, "compression_ratio": 1.7415730337078652, "no_speech_prob": 1.2410201634338591e-05}, {"id": 63, "seek": 38200, "start": 382.0, "end": 386.0, "text": " It's like, you know, we need to teach you probability theory and we need to teach you information theory.", "tokens": [467, 311, 411, 11, 291, 458, 11, 321, 643, 281, 2924, 291, 8482, 5261, 293, 321, 643, 281, 2924, 291, 1589, 5261, 13], "temperature": 0.0, "avg_logprob": -0.07388096285941906, "compression_ratio": 1.880794701986755, "no_speech_prob": 1.6963433154160157e-05}, {"id": 64, "seek": 38200, "start": 386.0, "end": 389.0, "text": " And only way later on are we going to let you put it together.", "tokens": [400, 787, 636, 1780, 322, 366, 321, 516, 281, 718, 291, 829, 309, 1214, 13], "temperature": 0.0, "avg_logprob": -0.07388096285941906, "compression_ratio": 1.880794701986755, "no_speech_prob": 1.6963433154160157e-05}, {"id": 65, "seek": 38200, "start": 389.0, "end": 393.0, "text": " You can think of it as being depth first rather than breadth first, if you like.", "tokens": [509, 393, 519, 295, 309, 382, 885, 7161, 700, 2831, 813, 35862, 700, 11, 498, 291, 411, 13], "temperature": 0.0, "avg_logprob": -0.07388096285941906, "compression_ratio": 1.880794701986755, "no_speech_prob": 1.6963433154160157e-05}, {"id": 66, "seek": 38200, "start": 393.0, "end": 404.0, "text": " So the traditional depth first approach means that you as a student have to trust that at some point all these things are going to come together and turn into something that's genuinely useful.", "tokens": [407, 264, 5164, 7161, 700, 3109, 1355, 300, 291, 382, 257, 3107, 362, 281, 3361, 300, 412, 512, 935, 439, 613, 721, 366, 516, 281, 808, 1214, 293, 1261, 666, 746, 300, 311, 17839, 4420, 13], "temperature": 0.0, "avg_logprob": -0.07388096285941906, "compression_ratio": 1.880794701986755, "no_speech_prob": 1.6963433154160157e-05}, {"id": 67, "seek": 38200, "start": 404.0, "end": 411.0, "text": " I think with this breadth first approach, you still have to trust, but it's a different kind of trust, which is that it's OK", "tokens": [286, 519, 365, 341, 35862, 700, 3109, 11, 291, 920, 362, 281, 3361, 11, 457, 309, 311, 257, 819, 733, 295, 3361, 11, 597, 307, 300, 309, 311, 2264], "temperature": 0.0, "avg_logprob": -0.07388096285941906, "compression_ratio": 1.880794701986755, "no_speech_prob": 1.6963433154160157e-05}, {"id": 68, "seek": 41100, "start": 411.0, "end": 417.0, "text": " that when we first show you an end to end process that you don't deeply understand every part,", "tokens": [300, 562, 321, 700, 855, 291, 364, 917, 281, 917, 1399, 300, 291, 500, 380, 8760, 1223, 633, 644, 11], "temperature": 0.0, "avg_logprob": -0.05743545816655744, "compression_ratio": 1.8148148148148149, "no_speech_prob": 2.8128858957643388e-06}, {"id": 69, "seek": 41100, "start": 417.0, "end": 422.0, "text": " but that you are able to actually do useful things from the very first lesson.", "tokens": [457, 300, 291, 366, 1075, 281, 767, 360, 4420, 721, 490, 264, 588, 700, 6898, 13], "temperature": 0.0, "avg_logprob": -0.05743545816655744, "compression_ratio": 1.8148148148148149, "no_speech_prob": 2.8128858957643388e-06}, {"id": 70, "seek": 41100, "start": 422.0, "end": 427.0, "text": " And that as the lessons go along, you're going to get more and more in depth understanding of each piece.", "tokens": [400, 300, 382, 264, 8820, 352, 2051, 11, 291, 434, 516, 281, 483, 544, 293, 544, 294, 7161, 3701, 295, 1184, 2522, 13], "temperature": 0.0, "avg_logprob": -0.05743545816655744, "compression_ratio": 1.8148148148148149, "no_speech_prob": 2.8128858957643388e-06}, {"id": 71, "seek": 41100, "start": 427.0, "end": 433.0, "text": " And two ways that the elementitis or the depth first approach fails are one is motivation.", "tokens": [400, 732, 2098, 300, 264, 4478, 16074, 420, 264, 7161, 700, 3109, 18199, 366, 472, 307, 12335, 13], "temperature": 0.0, "avg_logprob": -0.05743545816655744, "compression_ratio": 1.8148148148148149, "no_speech_prob": 2.8128858957643388e-06}, {"id": 72, "seek": 41100, "start": 433.0, "end": 438.0, "text": " A lot of students kind of give up because they don't have the motivation of seeing how are these going to fit together.", "tokens": [316, 688, 295, 1731, 733, 295, 976, 493, 570, 436, 500, 380, 362, 264, 12335, 295, 2577, 577, 366, 613, 516, 281, 3318, 1214, 13], "temperature": 0.0, "avg_logprob": -0.05743545816655744, "compression_ratio": 1.8148148148148149, "no_speech_prob": 2.8128858957643388e-06}, {"id": 73, "seek": 43800, "start": 438.0, "end": 448.0, "text": " And then secondly, it's harder to get the like you don't have the context when you're learning all these discrete elements and you can't learn how they're going to fit into the process until later.", "tokens": [400, 550, 26246, 11, 309, 311, 6081, 281, 483, 264, 411, 291, 500, 380, 362, 264, 4319, 562, 291, 434, 2539, 439, 613, 27706, 4959, 293, 291, 393, 380, 1466, 577, 436, 434, 516, 281, 3318, 666, 264, 1399, 1826, 1780, 13], "temperature": 0.0, "avg_logprob": -0.08698807287653652, "compression_ratio": 1.791044776119403, "no_speech_prob": 6.853992999822367e-06}, {"id": 74, "seek": 43800, "start": 448.0, "end": 456.0, "text": " Right. And in fact, this goes together with the idea of using a code centric approach instead of a math centric approach with a code centric approach.", "tokens": [1779, 13, 400, 294, 1186, 11, 341, 1709, 1214, 365, 264, 1558, 295, 1228, 257, 3089, 1489, 1341, 3109, 2602, 295, 257, 5221, 1489, 1341, 3109, 365, 257, 3089, 1489, 1341, 3109, 13], "temperature": 0.0, "avg_logprob": -0.08698807287653652, "compression_ratio": 1.791044776119403, "no_speech_prob": 6.853992999822367e-06}, {"id": 75, "seek": 43800, "start": 456.0, "end": 464.0, "text": " And looking at the whole game that is an end to end machine learning process from the very start means that you can do experiments.", "tokens": [400, 1237, 412, 264, 1379, 1216, 300, 307, 364, 917, 281, 917, 3479, 2539, 1399, 490, 264, 588, 722, 1355, 300, 291, 393, 360, 12050, 13], "temperature": 0.0, "avg_logprob": -0.08698807287653652, "compression_ratio": 1.791044776119403, "no_speech_prob": 6.853992999822367e-06}, {"id": 76, "seek": 46400, "start": 464.0, "end": 470.0, "text": " You can actually run experiments and see what goes in and out of each part of the system and build up that intuition.", "tokens": [509, 393, 767, 1190, 12050, 293, 536, 437, 1709, 294, 293, 484, 295, 1184, 644, 295, 264, 1185, 293, 1322, 493, 300, 24002, 13], "temperature": 0.0, "avg_logprob": -0.04764713497336851, "compression_ratio": 1.6501766784452296, "no_speech_prob": 9.665734069130849e-06}, {"id": 77, "seek": 46400, "start": 470.0, "end": 477.0, "text": " And if this whole game analogy intrigues you, David Perkins has a book called Making Learning Whole, where he goes into a lot more detail about it.", "tokens": [400, 498, 341, 1379, 1216, 21663, 17934, 1247, 291, 11, 4389, 3026, 10277, 575, 257, 1446, 1219, 14595, 15205, 30336, 11, 689, 415, 1709, 666, 257, 688, 544, 2607, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.04764713497336851, "compression_ratio": 1.6501766784452296, "no_speech_prob": 9.665734069130849e-06}, {"id": 78, "seek": 46400, "start": 477.0, "end": 485.0, "text": " Love that book. So then not not only are we going to be showing you end to end processes from this very first lesson,", "tokens": [5956, 300, 1446, 13, 407, 550, 406, 406, 787, 366, 321, 516, 281, 312, 4099, 291, 917, 281, 917, 7555, 490, 341, 588, 700, 6898, 11], "temperature": 0.0, "avg_logprob": -0.04764713497336851, "compression_ratio": 1.6501766784452296, "no_speech_prob": 9.665734069130849e-06}, {"id": 79, "seek": 46400, "start": 485.0, "end": 489.0, "text": " but these processes are going to not going to just end up with good enough results.", "tokens": [457, 613, 7555, 366, 516, 281, 406, 516, 281, 445, 917, 493, 365, 665, 1547, 3542, 13], "temperature": 0.0, "avg_logprob": -0.04764713497336851, "compression_ratio": 1.6501766784452296, "no_speech_prob": 9.665734069130849e-06}, {"id": 80, "seek": 48900, "start": 489.0, "end": 499.0, "text": " Nearly all of the deep learning educational materials I've seen so far get you to a point where you can kind of get an OK ish result.", "tokens": [38000, 439, 295, 264, 2452, 2539, 10189, 5319, 286, 600, 1612, 370, 1400, 483, 291, 281, 257, 935, 689, 291, 393, 733, 295, 483, 364, 2264, 307, 71, 1874, 13], "temperature": 0.0, "avg_logprob": -0.045111229108727495, "compression_ratio": 1.889795918367347, "no_speech_prob": 8.139400051732082e-06}, {"id": 81, "seek": 48900, "start": 499.0, "end": 503.0, "text": " Now, the whole point of deep learning is that you can get state of the art results.", "tokens": [823, 11, 264, 1379, 935, 295, 2452, 2539, 307, 300, 291, 393, 483, 1785, 295, 264, 1523, 3542, 13], "temperature": 0.0, "avg_logprob": -0.045111229108727495, "compression_ratio": 1.889795918367347, "no_speech_prob": 8.139400051732082e-06}, {"id": 82, "seek": 48900, "start": 503.0, "end": 509.0, "text": " And so in the very first piece of code we're going to run, we're going to run a piece of code which gives you a state of the art result.", "tokens": [400, 370, 294, 264, 588, 700, 2522, 295, 3089, 321, 434, 516, 281, 1190, 11, 321, 434, 516, 281, 1190, 257, 2522, 295, 3089, 597, 2709, 291, 257, 1785, 295, 264, 1523, 1874, 13], "temperature": 0.0, "avg_logprob": -0.045111229108727495, "compression_ratio": 1.889795918367347, "no_speech_prob": 8.139400051732082e-06}, {"id": 83, "seek": 48900, "start": 509.0, "end": 516.0, "text": " We know something is a state of the art result if it is better than other approaches that people have tried.", "tokens": [492, 458, 746, 307, 257, 1785, 295, 264, 1523, 1874, 498, 309, 307, 1101, 813, 661, 11587, 300, 561, 362, 3031, 13], "temperature": 0.0, "avg_logprob": -0.045111229108727495, "compression_ratio": 1.889795918367347, "no_speech_prob": 8.139400051732082e-06}, {"id": 84, "seek": 51600, "start": 516.0, "end": 520.0, "text": " The best way to know that is to try things on a Kaggle competition.", "tokens": [440, 1151, 636, 281, 458, 300, 307, 281, 853, 721, 322, 257, 48751, 22631, 6211, 13], "temperature": 0.0, "avg_logprob": -0.06617643456710012, "compression_ratio": 1.6764705882352942, "no_speech_prob": 1.6186364518944174e-05}, {"id": 85, "seek": 51600, "start": 520.0, "end": 529.0, "text": " Having been the president and chief scientist of Kaggle, I saw again and again that every Kaggle competition beat all previous academic state state of the art results.", "tokens": [10222, 668, 264, 3868, 293, 9588, 12662, 295, 48751, 22631, 11, 286, 1866, 797, 293, 797, 300, 633, 48751, 22631, 6211, 4224, 439, 3894, 7778, 1785, 1785, 295, 264, 1523, 3542, 13], "temperature": 0.0, "avg_logprob": -0.06617643456710012, "compression_ratio": 1.6764705882352942, "no_speech_prob": 1.6186364518944174e-05}, {"id": 86, "seek": 51600, "start": 529.0, "end": 542.0, "text": " So very often in this course, we're actually going to use Kaggle benchmarks and see if we can beat them, because we know if we can, then that's truly a world best.", "tokens": [407, 588, 2049, 294, 341, 1164, 11, 321, 434, 767, 516, 281, 764, 48751, 22631, 43751, 293, 536, 498, 321, 393, 4224, 552, 11, 570, 321, 458, 498, 321, 393, 11, 550, 300, 311, 4908, 257, 1002, 1151, 13], "temperature": 0.0, "avg_logprob": -0.06617643456710012, "compression_ratio": 1.6764705882352942, "no_speech_prob": 1.6186364518944174e-05}, {"id": 87, "seek": 54200, "start": 542.0, "end": 549.0, "text": " So this is from actually a very good book. It's the Ian Goodfellow, Yoshua Bengio, Deep Learning book.", "tokens": [407, 341, 307, 490, 767, 257, 588, 665, 1446, 13, 467, 311, 264, 19595, 2205, 69, 21348, 11, 38949, 4398, 29425, 1004, 11, 14895, 15205, 1446, 13], "temperature": 0.0, "avg_logprob": -0.11252555660173005, "compression_ratio": 1.6229508196721312, "no_speech_prob": 2.5461476980126463e-05}, {"id": 88, "seek": 54200, "start": 549.0, "end": 554.0, "text": " But it's a very good math book which teaches you the math of deep learning.", "tokens": [583, 309, 311, 257, 588, 665, 5221, 1446, 597, 16876, 291, 264, 5221, 295, 2452, 2539, 13], "temperature": 0.0, "avg_logprob": -0.11252555660173005, "compression_ratio": 1.6229508196721312, "no_speech_prob": 2.5461476980126463e-05}, {"id": 89, "seek": 54200, "start": 554.0, "end": 563.0, "text": " And so in this book, when they say here is how we gain some intuition in how to back propagation through time works, this is how they develop intuition.", "tokens": [400, 370, 294, 341, 1446, 11, 562, 436, 584, 510, 307, 577, 321, 6052, 512, 24002, 294, 577, 281, 646, 38377, 807, 565, 1985, 11, 341, 307, 577, 436, 1499, 24002, 13], "temperature": 0.0, "avg_logprob": -0.11252555660173005, "compression_ratio": 1.6229508196721312, "no_speech_prob": 2.5461476980126463e-05}, {"id": 90, "seek": 54200, "start": 563.0, "end": 568.0, "text": " Rachel, as a math PhD, did you find this helped your intuitions?", "tokens": [14246, 11, 382, 257, 5221, 14476, 11, 630, 291, 915, 341, 4254, 428, 16224, 626, 30], "temperature": 0.0, "avg_logprob": -0.11252555660173005, "compression_ratio": 1.6229508196721312, "no_speech_prob": 2.5461476980126463e-05}, {"id": 91, "seek": 56800, "start": 568.0, "end": 573.0, "text": " We'll have a very different approach to intuition. So this is a good book if you're interested in math and theorems.", "tokens": [492, 603, 362, 257, 588, 819, 3109, 281, 24002, 13, 407, 341, 307, 257, 665, 1446, 498, 291, 434, 3102, 294, 5221, 293, 10299, 2592, 13], "temperature": 0.0, "avg_logprob": -0.09004232856664765, "compression_ratio": 1.581896551724138, "no_speech_prob": 7.71540726418607e-05}, {"id": 92, "seek": 56800, "start": 573.0, "end": 577.0, "text": " In this course, we're really going to be focused on code.", "tokens": [682, 341, 1164, 11, 321, 434, 534, 516, 281, 312, 5178, 322, 3089, 13], "temperature": 0.0, "avg_logprob": -0.09004232856664765, "compression_ratio": 1.581896551724138, "no_speech_prob": 7.71540726418607e-05}, {"id": 93, "seek": 56800, "start": 577.0, "end": 588.0, "text": " In fact, this is what Rachel and I put together when we were trying to explain backprop and specifically stochastic gradient descent and the use of backprop there was we created a spreadsheet.", "tokens": [682, 1186, 11, 341, 307, 437, 14246, 293, 286, 829, 1214, 562, 321, 645, 1382, 281, 2903, 646, 79, 1513, 293, 4682, 342, 8997, 2750, 16235, 23475, 293, 264, 764, 295, 646, 79, 1513, 456, 390, 321, 2942, 257, 27733, 13], "temperature": 0.0, "avg_logprob": -0.09004232856664765, "compression_ratio": 1.581896551724138, "no_speech_prob": 7.71540726418607e-05}, {"id": 94, "seek": 58800, "start": 588.0, "end": 599.0, "text": " And we found each time that we taught our students in the in-person course through a spreadsheet, they could see every single piece of what was going on, every single intermediate result.", "tokens": [400, 321, 1352, 1184, 565, 300, 321, 5928, 527, 1731, 294, 264, 294, 12, 10813, 1164, 807, 257, 27733, 11, 436, 727, 536, 633, 2167, 2522, 295, 437, 390, 516, 322, 11, 633, 2167, 19376, 1874, 13], "temperature": 0.0, "avg_logprob": -0.041883590675535654, "compression_ratio": 1.6986301369863013, "no_speech_prob": 2.93073317152448e-05}, {"id": 95, "seek": 58800, "start": 599.0, "end": 601.0, "text": " And it was very easy for them to experiment with.", "tokens": [400, 309, 390, 588, 1858, 337, 552, 281, 5120, 365, 13], "temperature": 0.0, "avg_logprob": -0.041883590675535654, "compression_ratio": 1.6986301369863013, "no_speech_prob": 2.93073317152448e-05}, {"id": 96, "seek": 58800, "start": 601.0, "end": 611.0, "text": " And so one of the unusual things we do is that you'll see that nearly every major idea is presented at some point using a spreadsheet.", "tokens": [400, 370, 472, 295, 264, 10901, 721, 321, 360, 307, 300, 291, 603, 536, 300, 6217, 633, 2563, 1558, 307, 8212, 412, 512, 935, 1228, 257, 27733, 13], "temperature": 0.0, "avg_logprob": -0.041883590675535654, "compression_ratio": 1.6986301369863013, "no_speech_prob": 2.93073317152448e-05}, {"id": 97, "seek": 61100, "start": 611.0, "end": 618.0, "text": " We present it in many different ways, but spreadsheets, diagrams and code are three of the key ways that we present these ideas.", "tokens": [492, 1974, 309, 294, 867, 819, 2098, 11, 457, 23651, 1385, 11, 36709, 293, 3089, 366, 1045, 295, 264, 2141, 2098, 300, 321, 1974, 613, 3487, 13], "temperature": 0.0, "avg_logprob": -0.0944366455078125, "compression_ratio": 1.5643939393939394, "no_speech_prob": 3.119920438621193e-05}, {"id": 98, "seek": 61100, "start": 618.0, "end": 625.0, "text": " I believe this is the first deep learning course in the world to implement a convolutional neural network in an Excel spreadsheet.", "tokens": [286, 1697, 341, 307, 264, 700, 2452, 2539, 1164, 294, 264, 1002, 281, 4445, 257, 45216, 304, 18161, 3209, 294, 364, 19060, 27733, 13], "temperature": 0.0, "avg_logprob": -0.0944366455078125, "compression_ratio": 1.5643939393939394, "no_speech_prob": 3.119920438621193e-05}, {"id": 99, "seek": 61100, "start": 625.0, "end": 634.0, "text": " And also, as you see from this page, not just stochastic gradient descent, but AdaGrad, RMSProp, Adam, and even Eve, which just came out a few weeks ago,", "tokens": [400, 611, 11, 382, 291, 536, 490, 341, 3028, 11, 406, 445, 342, 8997, 2750, 16235, 23475, 11, 457, 32276, 38, 6206, 11, 497, 10288, 47, 1513, 11, 7938, 11, 293, 754, 15544, 11, 597, 445, 1361, 484, 257, 1326, 3259, 2057, 11], "temperature": 0.0, "avg_logprob": -0.0944366455078125, "compression_ratio": 1.5643939393939394, "no_speech_prob": 3.119920438621193e-05}, {"id": 100, "seek": 63400, "start": 634.0, "end": 642.0, "text": " all modern examples of accelerated SGD approaches.", "tokens": [439, 4363, 5110, 295, 29763, 34520, 35, 11587, 13], "temperature": 0.0, "avg_logprob": -0.05329457062941331, "compression_ratio": 1.654320987654321, "no_speech_prob": 9.223133019986562e-06}, {"id": 101, "seek": 63400, "start": 642.0, "end": 648.0, "text": " So I think everything you really need to know about the course comes in this very first piece of code that you see.", "tokens": [407, 286, 519, 1203, 291, 534, 643, 281, 458, 466, 264, 1164, 1487, 294, 341, 588, 700, 2522, 295, 3089, 300, 291, 536, 13], "temperature": 0.0, "avg_logprob": -0.05329457062941331, "compression_ratio": 1.654320987654321, "no_speech_prob": 9.223133019986562e-06}, {"id": 102, "seek": 63400, "start": 648.0, "end": 653.0, "text": " And this very first piece of code that you see, you can see that there's a number of things going on.", "tokens": [400, 341, 588, 700, 2522, 295, 3089, 300, 291, 536, 11, 291, 393, 536, 300, 456, 311, 257, 1230, 295, 721, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.05329457062941331, "compression_ratio": 1.654320987654321, "no_speech_prob": 9.223133019986562e-06}, {"id": 103, "seek": 65300, "start": 653.0, "end": 664.0, "text": " The first is that this piece of code shows not just how to complete a project, but how to get a state of the art result on a project.", "tokens": [440, 700, 307, 300, 341, 2522, 295, 3089, 3110, 406, 445, 577, 281, 3566, 257, 1716, 11, 457, 577, 281, 483, 257, 1785, 295, 264, 1523, 1874, 322, 257, 1716, 13], "temperature": 0.0, "avg_logprob": -0.04087883692521315, "compression_ratio": 1.7236180904522613, "no_speech_prob": 3.0238230465329252e-05}, {"id": 104, "seek": 65300, "start": 664.0, "end": 670.0, "text": " This particular piece of code gives you 97 percent accuracy in determining cats versus dogs.", "tokens": [639, 1729, 2522, 295, 3089, 2709, 291, 23399, 3043, 14170, 294, 23751, 11111, 5717, 7197, 13], "temperature": 0.0, "avg_logprob": -0.04087883692521315, "compression_ratio": 1.7236180904522613, "no_speech_prob": 3.0238230465329252e-05}, {"id": 105, "seek": 65300, "start": 670.0, "end": 679.0, "text": " As recently as about five years ago, the state of the art for this particular problem was about 80 percent accuracy.", "tokens": [1018, 3938, 382, 466, 1732, 924, 2057, 11, 264, 1785, 295, 264, 1523, 337, 341, 1729, 1154, 390, 466, 4688, 3043, 14170, 13], "temperature": 0.0, "avg_logprob": -0.04087883692521315, "compression_ratio": 1.7236180904522613, "no_speech_prob": 3.0238230465329252e-05}, {"id": 106, "seek": 67900, "start": 679.0, "end": 686.0, "text": " It's also an example of showing why working with code is so interesting.", "tokens": [467, 311, 611, 364, 1365, 295, 4099, 983, 1364, 365, 3089, 307, 370, 1880, 13], "temperature": 0.0, "avg_logprob": -0.04253242524822107, "compression_ratio": 1.6933962264150944, "no_speech_prob": 3.822620055871084e-05}, {"id": 107, "seek": 67900, "start": 686.0, "end": 691.0, "text": " Rather than showing math, what we're showing here is some working code.", "tokens": [16571, 813, 4099, 5221, 11, 437, 321, 434, 4099, 510, 307, 512, 1364, 3089, 13], "temperature": 0.0, "avg_logprob": -0.04253242524822107, "compression_ratio": 1.6933962264150944, "no_speech_prob": 3.822620055871084e-05}, {"id": 108, "seek": 67900, "start": 691.0, "end": 694.0, "text": " And I'll give you an example of what that means you can do.", "tokens": [400, 286, 603, 976, 291, 364, 1365, 295, 437, 300, 1355, 291, 393, 360, 13], "temperature": 0.0, "avg_logprob": -0.04253242524822107, "compression_ratio": 1.6933962264150944, "no_speech_prob": 3.822620055871084e-05}, {"id": 109, "seek": 67900, "start": 694.0, "end": 698.0, "text": " So the code environment that we're working in is something called Jupyter Notebook.", "tokens": [407, 264, 3089, 2823, 300, 321, 434, 1364, 294, 307, 746, 1219, 22125, 88, 391, 11633, 2939, 13], "temperature": 0.0, "avg_logprob": -0.04253242524822107, "compression_ratio": 1.6933962264150944, "no_speech_prob": 3.822620055871084e-05}, {"id": 110, "seek": 67900, "start": 698.0, "end": 702.0, "text": " And you'll be using this in every single lesson throughout the course.", "tokens": [400, 291, 603, 312, 1228, 341, 294, 633, 2167, 6898, 3710, 264, 1164, 13], "temperature": 0.0, "avg_logprob": -0.04253242524822107, "compression_ratio": 1.6933962264150944, "no_speech_prob": 3.822620055871084e-05}, {"id": 111, "seek": 70200, "start": 702.0, "end": 710.0, "text": " And in Jupyter Notebook, as you can see, we provide you with prose and information about what's going on.", "tokens": [400, 294, 22125, 88, 391, 11633, 2939, 11, 382, 291, 393, 536, 11, 321, 2893, 291, 365, 12505, 293, 1589, 466, 437, 311, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.0939119446952388, "compression_ratio": 1.735159817351598, "no_speech_prob": 4.785001237905817e-06}, {"id": 112, "seek": 70200, "start": 710.0, "end": 712.0, "text": " And we draw pictures.", "tokens": [400, 321, 2642, 5242, 13], "temperature": 0.0, "avg_logprob": -0.0939119446952388, "compression_ratio": 1.735159817351598, "no_speech_prob": 4.785001237905817e-06}, {"id": 113, "seek": 70200, "start": 712.0, "end": 716.0, "text": " And at any point in time, you can take a look at one of these results.", "tokens": [400, 412, 604, 935, 294, 565, 11, 291, 393, 747, 257, 574, 412, 472, 295, 613, 3542, 13], "temperature": 0.0, "avg_logprob": -0.0939119446952388, "compression_ratio": 1.735159817351598, "no_speech_prob": 4.785001237905817e-06}, {"id": 114, "seek": 70200, "start": 716.0, "end": 725.0, "text": " And you can take a look at one of these results and you can look to see what's going on behind the scenes.", "tokens": [400, 291, 393, 747, 257, 574, 412, 472, 295, 613, 3542, 293, 291, 393, 574, 281, 536, 437, 311, 516, 322, 2261, 264, 8026, 13], "temperature": 0.0, "avg_logprob": -0.0939119446952388, "compression_ratio": 1.735159817351598, "no_speech_prob": 4.785001237905817e-06}, {"id": 115, "seek": 70200, "start": 725.0, "end": 730.0, "text": " So, for example, in this case, we're running something called VGG.predict.", "tokens": [407, 11, 337, 1365, 11, 294, 341, 1389, 11, 321, 434, 2614, 746, 1219, 691, 27561, 13, 79, 24945, 13], "temperature": 0.0, "avg_logprob": -0.0939119446952388, "compression_ratio": 1.735159817351598, "no_speech_prob": 4.785001237905817e-06}, {"id": 116, "seek": 73000, "start": 730.0, "end": 732.0, "text": " And we're getting back some probabilities.", "tokens": [400, 321, 434, 1242, 646, 512, 33783, 13], "temperature": 0.0, "avg_logprob": -0.0291398832620668, "compression_ratio": 1.6711864406779662, "no_speech_prob": 8.139508281601593e-06}, {"id": 117, "seek": 73000, "start": 732.0, "end": 735.0, "text": " And you might wonder, well, what's VGG.predict actually doing?", "tokens": [400, 291, 1062, 2441, 11, 731, 11, 437, 311, 691, 27561, 13, 79, 24945, 767, 884, 30], "temperature": 0.0, "avg_logprob": -0.0291398832620668, "compression_ratio": 1.6711864406779662, "no_speech_prob": 8.139508281601593e-06}, {"id": 118, "seek": 73000, "start": 735.0, "end": 742.0, "text": " So at any time, you can take anything and put two question marks on the front and run that piece of code.", "tokens": [407, 412, 604, 565, 11, 291, 393, 747, 1340, 293, 829, 732, 1168, 10640, 322, 264, 1868, 293, 1190, 300, 2522, 295, 3089, 13], "temperature": 0.0, "avg_logprob": -0.0291398832620668, "compression_ratio": 1.6711864406779662, "no_speech_prob": 8.139508281601593e-06}, {"id": 119, "seek": 73000, "start": 742.0, "end": 748.0, "text": " And it will actually show you the full documentation and source code of what you just ran.", "tokens": [400, 309, 486, 767, 855, 291, 264, 1577, 14333, 293, 4009, 3089, 295, 437, 291, 445, 5872, 13], "temperature": 0.0, "avg_logprob": -0.0291398832620668, "compression_ratio": 1.6711864406779662, "no_speech_prob": 8.139508281601593e-06}, {"id": 120, "seek": 73000, "start": 748.0, "end": 753.0, "text": " Now, in this case, it's actually running a function that we wrote for you.", "tokens": [823, 11, 294, 341, 1389, 11, 309, 311, 767, 2614, 257, 2445, 300, 321, 4114, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.0291398832620668, "compression_ratio": 1.6711864406779662, "no_speech_prob": 8.139508281601593e-06}, {"id": 121, "seek": 73000, "start": 753.0, "end": 759.0, "text": " One of the other different things about this course is that we're not just showing you how existing libraries work.", "tokens": [1485, 295, 264, 661, 819, 721, 466, 341, 1164, 307, 300, 321, 434, 406, 445, 4099, 291, 577, 6741, 15148, 589, 13], "temperature": 0.0, "avg_logprob": -0.0291398832620668, "compression_ratio": 1.6711864406779662, "no_speech_prob": 8.139508281601593e-06}, {"id": 122, "seek": 75900, "start": 759.0, "end": 768.0, "text": " But every time we found that using somebody else's library takes more than four or five lines of code, we would make sure we found a way to do it easier.", "tokens": [583, 633, 565, 321, 1352, 300, 1228, 2618, 1646, 311, 6405, 2516, 544, 813, 1451, 420, 1732, 3876, 295, 3089, 11, 321, 576, 652, 988, 321, 1352, 257, 636, 281, 360, 309, 3571, 13], "temperature": 0.0, "avg_logprob": -0.041427855308239274, "compression_ratio": 1.7317073170731707, "no_speech_prob": 5.682331902789883e-06}, {"id": 123, "seek": 75900, "start": 768.0, "end": 772.0, "text": " So generally speaking, we show you how to do things in one line of code.", "tokens": [407, 5101, 4124, 11, 321, 855, 291, 577, 281, 360, 721, 294, 472, 1622, 295, 3089, 13], "temperature": 0.0, "avg_logprob": -0.041427855308239274, "compression_ratio": 1.7317073170731707, "no_speech_prob": 5.682331902789883e-06}, {"id": 124, "seek": 75900, "start": 772.0, "end": 777.0, "text": " And then you can look behind the scenes and see what the lines of code are actually doing.", "tokens": [400, 550, 291, 393, 574, 2261, 264, 8026, 293, 536, 437, 264, 3876, 295, 3089, 366, 767, 884, 13], "temperature": 0.0, "avg_logprob": -0.041427855308239274, "compression_ratio": 1.7317073170731707, "no_speech_prob": 5.682331902789883e-06}, {"id": 125, "seek": 75900, "start": 777.0, "end": 784.0, "text": " So, for example, in this case, the predict method is running some other predict method called model.predict.", "tokens": [407, 11, 337, 1365, 11, 294, 341, 1389, 11, 264, 6069, 3170, 307, 2614, 512, 661, 6069, 3170, 1219, 2316, 13, 79, 24945, 13], "temperature": 0.0, "avg_logprob": -0.041427855308239274, "compression_ratio": 1.7317073170731707, "no_speech_prob": 5.682331902789883e-06}, {"id": 126, "seek": 78400, "start": 784.0, "end": 789.0, "text": " And so then what I always encourage people to do is to do some experiments.", "tokens": [400, 370, 550, 437, 286, 1009, 5373, 561, 281, 360, 307, 281, 360, 512, 12050, 13], "temperature": 0.0, "avg_logprob": -0.0509895658493042, "compression_ratio": 1.6798245614035088, "no_speech_prob": 2.5612519038986648e-06}, {"id": 127, "seek": 78400, "start": 789.0, "end": 793.0, "text": " So what does model.predict actually do?", "tokens": [407, 437, 775, 2316, 13, 79, 24945, 767, 360, 30], "temperature": 0.0, "avg_logprob": -0.0509895658493042, "compression_ratio": 1.6798245614035088, "no_speech_prob": 2.5612519038986648e-06}, {"id": 128, "seek": 78400, "start": 793.0, "end": 799.0, "text": " One thing that you can do in Jupyter Notebook at any time is to press Shift Tab a couple of times.", "tokens": [1485, 551, 300, 291, 393, 360, 294, 22125, 88, 391, 11633, 2939, 412, 604, 565, 307, 281, 1886, 28304, 14106, 257, 1916, 295, 1413, 13], "temperature": 0.0, "avg_logprob": -0.0509895658493042, "compression_ratio": 1.6798245614035088, "no_speech_prob": 2.5612519038986648e-06}, {"id": 129, "seek": 78400, "start": 799.0, "end": 806.0, "text": " And when you press Shift Tab, the first time it pops up, it tells you what parameters you need to pass this method.", "tokens": [400, 562, 291, 1886, 28304, 14106, 11, 264, 700, 565, 309, 16795, 493, 11, 309, 5112, 291, 437, 9834, 291, 643, 281, 1320, 341, 3170, 13], "temperature": 0.0, "avg_logprob": -0.0509895658493042, "compression_ratio": 1.6798245614035088, "no_speech_prob": 2.5612519038986648e-06}, {"id": 130, "seek": 78400, "start": 806.0, "end": 812.0, "text": " And it also tells you what the method actually does.", "tokens": [400, 309, 611, 5112, 291, 437, 264, 3170, 767, 775, 13], "temperature": 0.0, "avg_logprob": -0.0509895658493042, "compression_ratio": 1.6798245614035088, "no_speech_prob": 2.5612519038986648e-06}, {"id": 131, "seek": 81200, "start": 812.0, "end": 820.0, "text": " If you press it three times, it then gives you additional information about what each of those arguments are and what they're expecting and what it returns.", "tokens": [759, 291, 1886, 309, 1045, 1413, 11, 309, 550, 2709, 291, 4497, 1589, 466, 437, 1184, 295, 729, 12869, 366, 293, 437, 436, 434, 9650, 293, 437, 309, 11247, 13], "temperature": 0.0, "avg_logprob": -0.023957523645139207, "compression_ratio": 1.7028112449799198, "no_speech_prob": 1.6700749256415293e-05}, {"id": 132, "seek": 81200, "start": 820.0, "end": 828.0, "text": " So it's really nice that using this method, you can find out exactly what's going on behind the scenes and do some experiments.", "tokens": [407, 309, 311, 534, 1481, 300, 1228, 341, 3170, 11, 291, 393, 915, 484, 2293, 437, 311, 516, 322, 2261, 264, 8026, 293, 360, 512, 12050, 13], "temperature": 0.0, "avg_logprob": -0.023957523645139207, "compression_ratio": 1.7028112449799198, "no_speech_prob": 1.6700749256415293e-05}, {"id": 133, "seek": 81200, "start": 828.0, "end": 832.0, "text": " And so then, for example, you could find out, OK, well, what is the shape?", "tokens": [400, 370, 550, 11, 337, 1365, 11, 291, 727, 915, 484, 11, 2264, 11, 731, 11, 437, 307, 264, 3909, 30], "temperature": 0.0, "avg_logprob": -0.023957523645139207, "compression_ratio": 1.7028112449799198, "no_speech_prob": 1.6700749256415293e-05}, {"id": 134, "seek": 81200, "start": 832.0, "end": 836.0, "text": " What is the size and shape of the array that this thing returns?", "tokens": [708, 307, 264, 2744, 293, 3909, 295, 264, 10225, 300, 341, 551, 11247, 30], "temperature": 0.0, "avg_logprob": -0.023957523645139207, "compression_ratio": 1.7028112449799198, "no_speech_prob": 1.6700749256415293e-05}, {"id": 135, "seek": 83600, "start": 836.0, "end": 844.0, "text": " What are the first four elements of the classes that are in this object and so forth?", "tokens": [708, 366, 264, 700, 1451, 4959, 295, 264, 5359, 300, 366, 294, 341, 2657, 293, 370, 5220, 30], "temperature": 0.0, "avg_logprob": -0.07544013261795043, "compression_ratio": 1.6020408163265305, "no_speech_prob": 1.5936175259412266e-05}, {"id": 136, "seek": 83600, "start": 844.0, "end": 850.0, "text": " And this is really the way to use this style of teaching effectively.", "tokens": [400, 341, 307, 534, 264, 636, 281, 764, 341, 3758, 295, 4571, 8659, 13], "temperature": 0.0, "avg_logprob": -0.07544013261795043, "compression_ratio": 1.6020408163265305, "no_speech_prob": 1.5936175259412266e-05}, {"id": 137, "seek": 83600, "start": 850.0, "end": 860.0, "text": " It's to have the code in front of you all the time and in every line, look and see what's being passed in, what's coming out, what else could we do with that?", "tokens": [467, 311, 281, 362, 264, 3089, 294, 1868, 295, 291, 439, 264, 565, 293, 294, 633, 1622, 11, 574, 293, 536, 437, 311, 885, 4678, 294, 11, 437, 311, 1348, 484, 11, 437, 1646, 727, 321, 360, 365, 300, 30], "temperature": 0.0, "avg_logprob": -0.07544013261795043, "compression_ratio": 1.6020408163265305, "no_speech_prob": 1.5936175259412266e-05}, {"id": 138, "seek": 86000, "start": 860.0, "end": 867.0, "text": " And then even look at the documentation. So VGG.model is apparently a Keras.model.sequential.", "tokens": [400, 550, 754, 574, 412, 264, 14333, 13, 407, 691, 27561, 13, 8014, 338, 307, 7970, 257, 591, 6985, 13, 8014, 338, 13, 11834, 2549, 13], "temperature": 0.0, "avg_logprob": -0.07193063735961915, "compression_ratio": 1.6127659574468085, "no_speech_prob": 1.1125397577416152e-05}, {"id": 139, "seek": 86000, "start": 867.0, "end": 880.0, "text": " So if we were to just copy that into Google, then we can click on the first item and find out exactly what's going on.", "tokens": [407, 498, 321, 645, 281, 445, 5055, 300, 666, 3329, 11, 550, 321, 393, 2052, 322, 264, 700, 3174, 293, 915, 484, 2293, 437, 311, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.07193063735961915, "compression_ratio": 1.6127659574468085, "no_speech_prob": 1.1125397577416152e-05}, {"id": 140, "seek": 86000, "start": 880.0, "end": 884.0, "text": " What is being used here? What are the other methods that this could take?", "tokens": [708, 307, 885, 1143, 510, 30, 708, 366, 264, 661, 7150, 300, 341, 727, 747, 30], "temperature": 0.0, "avg_logprob": -0.07193063735961915, "compression_ratio": 1.6127659574468085, "no_speech_prob": 1.1125397577416152e-05}, {"id": 141, "seek": 86000, "start": 884.0, "end": 888.0, "text": " And then we can try calling some of these other methods and see what kind of results we get.", "tokens": [400, 550, 321, 393, 853, 5141, 512, 295, 613, 661, 7150, 293, 536, 437, 733, 295, 3542, 321, 483, 13], "temperature": 0.0, "avg_logprob": -0.07193063735961915, "compression_ratio": 1.6127659574468085, "no_speech_prob": 1.1125397577416152e-05}, {"id": 142, "seek": 88800, "start": 888.0, "end": 898.0, "text": " So really what we're trying to do is in the two hours of each lesson, we're trying to give you enough information to get you started with your own experiments.", "tokens": [407, 534, 437, 321, 434, 1382, 281, 360, 307, 294, 264, 732, 2496, 295, 1184, 6898, 11, 321, 434, 1382, 281, 976, 291, 1547, 1589, 281, 483, 291, 1409, 365, 428, 1065, 12050, 13], "temperature": 0.0, "avg_logprob": -0.061428871311125205, "compression_ratio": 1.8, "no_speech_prob": 1.568902916915249e-05}, {"id": 143, "seek": 88800, "start": 898.0, "end": 904.0, "text": " We're not trying to teach you everything. And we're certainly not assuming that the lesson can stand alone.", "tokens": [492, 434, 406, 1382, 281, 2924, 291, 1203, 13, 400, 321, 434, 3297, 406, 11926, 300, 264, 6898, 393, 1463, 3312, 13], "temperature": 0.0, "avg_logprob": -0.061428871311125205, "compression_ratio": 1.8, "no_speech_prob": 1.568902916915249e-05}, {"id": 144, "seek": 88800, "start": 904.0, "end": 908.0, "text": " And we'll talk more about this in a moment, but the videos are just a small part of the course.", "tokens": [400, 321, 603, 751, 544, 466, 341, 294, 257, 1623, 11, 457, 264, 2145, 366, 445, 257, 1359, 644, 295, 264, 1164, 13], "temperature": 0.0, "avg_logprob": -0.061428871311125205, "compression_ratio": 1.8, "no_speech_prob": 1.568902916915249e-05}, {"id": 145, "seek": 88800, "start": 908.0, "end": 911.0, "text": " But the iPython notebooks and the code are a huge resource.", "tokens": [583, 264, 5180, 88, 11943, 43782, 293, 264, 3089, 366, 257, 2603, 7684, 13], "temperature": 0.0, "avg_logprob": -0.061428871311125205, "compression_ratio": 1.8, "no_speech_prob": 1.568902916915249e-05}, {"id": 146, "seek": 88800, "start": 911.0, "end": 915.0, "text": " And we'll talk about some of the other resources that we have available for you.", "tokens": [400, 321, 603, 751, 466, 512, 295, 264, 661, 3593, 300, 321, 362, 2435, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.061428871311125205, "compression_ratio": 1.8, "no_speech_prob": 1.568902916915249e-05}, {"id": 147, "seek": 91500, "start": 915.0, "end": 922.0, "text": " But the important thing to realize with these six lines of code is that you can run this for anything, not just for dogs versus cats.", "tokens": [583, 264, 1021, 551, 281, 4325, 365, 613, 2309, 3876, 295, 3089, 307, 300, 291, 393, 1190, 341, 337, 1340, 11, 406, 445, 337, 7197, 5717, 11111, 13], "temperature": 0.0, "avg_logprob": -0.04584363590587269, "compression_ratio": 1.8226415094339623, "no_speech_prob": 6.813653453718871e-05}, {"id": 148, "seek": 91500, "start": 922.0, "end": 929.0, "text": " But these first six lines of code you learn will actually, as it says here, work for any image recognition task with any number of categories.", "tokens": [583, 613, 700, 2309, 3876, 295, 3089, 291, 1466, 486, 767, 11, 382, 309, 1619, 510, 11, 589, 337, 604, 3256, 11150, 5633, 365, 604, 1230, 295, 10479, 13], "temperature": 0.0, "avg_logprob": -0.04584363590587269, "compression_ratio": 1.8226415094339623, "no_speech_prob": 6.813653453718871e-05}, {"id": 149, "seek": 91500, "start": 929.0, "end": 935.0, "text": " So if you can get this far in today's lesson, then you've learned to do one of the most important types of computer vision,", "tokens": [407, 498, 291, 393, 483, 341, 1400, 294, 965, 311, 6898, 11, 550, 291, 600, 3264, 281, 360, 472, 295, 264, 881, 1021, 3467, 295, 3820, 5201, 11], "temperature": 0.0, "avg_logprob": -0.04584363590587269, "compression_ratio": 1.8226415094339623, "no_speech_prob": 6.813653453718871e-05}, {"id": 150, "seek": 91500, "start": 935.0, "end": 943.0, "text": " which is image classification for any number of categories for any type of images.", "tokens": [597, 307, 3256, 21538, 337, 604, 1230, 295, 10479, 337, 604, 2010, 295, 5267, 13], "temperature": 0.0, "avg_logprob": -0.04584363590587269, "compression_ratio": 1.8226415094339623, "no_speech_prob": 6.813653453718871e-05}, {"id": 151, "seek": 94300, "start": 943.0, "end": 946.0, "text": " As Rachel said, we've actually run this course already.", "tokens": [1018, 14246, 848, 11, 321, 600, 767, 1190, 341, 1164, 1217, 13], "temperature": 0.0, "avg_logprob": -0.041304125785827636, "compression_ratio": 1.6556016597510372, "no_speech_prob": 5.91950083617121e-05}, {"id": 152, "seek": 94300, "start": 946.0, "end": 951.0, "text": " Specifically, what you're going to be seeing are the recorded lessons from an in-person course.", "tokens": [26058, 11, 437, 291, 434, 516, 281, 312, 2577, 366, 264, 8287, 8820, 490, 364, 294, 12, 10813, 1164, 13], "temperature": 0.0, "avg_logprob": -0.041304125785827636, "compression_ratio": 1.6556016597510372, "no_speech_prob": 5.91950083617121e-05}, {"id": 153, "seek": 94300, "start": 951.0, "end": 956.0, "text": " And we thought it'd be helpful for you to see what some of our students said about that in-person course,", "tokens": [400, 321, 1194, 309, 1116, 312, 4961, 337, 291, 281, 536, 437, 512, 295, 527, 1731, 848, 466, 300, 294, 12, 10813, 1164, 11], "temperature": 0.0, "avg_logprob": -0.041304125785827636, "compression_ratio": 1.6556016597510372, "no_speech_prob": 5.91950083617121e-05}, {"id": 154, "seek": 94300, "start": 956.0, "end": 961.0, "text": " because it might help you to be a more effective learner.", "tokens": [570, 309, 1062, 854, 291, 281, 312, 257, 544, 4942, 33347, 13], "temperature": 0.0, "avg_logprob": -0.041304125785827636, "compression_ratio": 1.6556016597510372, "no_speech_prob": 5.91950083617121e-05}, {"id": 155, "seek": 94300, "start": 961.0, "end": 966.0, "text": " And I do want to say, again, because this course is taught in such a different way,", "tokens": [400, 286, 360, 528, 281, 584, 11, 797, 11, 570, 341, 1164, 307, 5928, 294, 1270, 257, 819, 636, 11], "temperature": 0.0, "avg_logprob": -0.041304125785827636, "compression_ratio": 1.6556016597510372, "no_speech_prob": 5.91950083617121e-05}, {"id": 156, "seek": 96600, "start": 966.0, "end": 973.0, "text": " it takes some faith that this new technique is worth trying and sticking with.", "tokens": [309, 2516, 512, 4522, 300, 341, 777, 6532, 307, 3163, 1382, 293, 13465, 365, 13], "temperature": 0.0, "avg_logprob": -0.06908291095011942, "compression_ratio": 1.5694444444444444, "no_speech_prob": 4.3992862629238516e-05}, {"id": 157, "seek": 96600, "start": 973.0, "end": 981.0, "text": " But you can see that almost all the students said that the homework assignments were very helpful", "tokens": [583, 291, 393, 536, 300, 1920, 439, 264, 1731, 848, 300, 264, 14578, 22546, 645, 588, 4961], "temperature": 0.0, "avg_logprob": -0.06908291095011942, "compression_ratio": 1.5694444444444444, "no_speech_prob": 4.3992862629238516e-05}, {"id": 158, "seek": 96600, "start": 981.0, "end": 985.0, "text": " or extremely helpful in understanding the material.", "tokens": [420, 4664, 4961, 294, 3701, 264, 2527, 13], "temperature": 0.0, "avg_logprob": -0.06908291095011942, "compression_ratio": 1.5694444444444444, "no_speech_prob": 4.3992862629238516e-05}, {"id": 159, "seek": 96600, "start": 985.0, "end": 992.0, "text": " And the class resources, which includes the Wiki, the scripts that we give you, our forums, our Slack channel,", "tokens": [400, 264, 1508, 3593, 11, 597, 5974, 264, 35892, 11, 264, 23294, 300, 321, 976, 291, 11, 527, 26998, 11, 527, 37211, 2269, 11], "temperature": 0.0, "avg_logprob": -0.06908291095011942, "compression_ratio": 1.5694444444444444, "no_speech_prob": 4.3992862629238516e-05}, {"id": 160, "seek": 99200, "start": 992.0, "end": 996.0, "text": " were very helpful or extremely helpful in understanding material.", "tokens": [645, 588, 4961, 420, 4664, 4961, 294, 3701, 2527, 13], "temperature": 0.0, "avg_logprob": -0.09637346118688583, "compression_ratio": 1.7335526315789473, "no_speech_prob": 9.607791434973478e-05}, {"id": 161, "seek": 99200, "start": 996.0, "end": 1001.0, "text": " And we wanted to mention that, because Rachel and I have both been kind of Coursera addicts in the past,", "tokens": [400, 321, 1415, 281, 2152, 300, 11, 570, 14246, 293, 286, 362, 1293, 668, 733, 295, 383, 5067, 1663, 22072, 82, 294, 264, 1791, 11], "temperature": 0.0, "avg_logprob": -0.09637346118688583, "compression_ratio": 1.7335526315789473, "no_speech_prob": 9.607791434973478e-05}, {"id": 162, "seek": 99200, "start": 1001.0, "end": 1007.0, "text": " and Udacity addicts, and generally speaking, we all often watch a video at one and a half speed or two speed", "tokens": [293, 624, 67, 19008, 22072, 82, 11, 293, 5101, 4124, 11, 321, 439, 2049, 1159, 257, 960, 412, 472, 293, 257, 1922, 3073, 420, 732, 3073], "temperature": 0.0, "avg_logprob": -0.09637346118688583, "compression_ratio": 1.7335526315789473, "no_speech_prob": 9.607791434973478e-05}, {"id": 163, "seek": 99200, "start": 1007.0, "end": 1009.0, "text": " and just zip through them.", "tokens": [293, 445, 20730, 807, 552, 13], "temperature": 0.0, "avg_logprob": -0.09637346118688583, "compression_ratio": 1.7335526315789473, "no_speech_prob": 9.607791434973478e-05}, {"id": 164, "seek": 99200, "start": 1009.0, "end": 1013.0, "text": " This is not designed to be possible to do that this way.", "tokens": [639, 307, 406, 4761, 281, 312, 1944, 281, 360, 300, 341, 636, 13], "temperature": 0.0, "avg_logprob": -0.09637346118688583, "compression_ratio": 1.7335526315789473, "no_speech_prob": 9.607791434973478e-05}, {"id": 165, "seek": 99200, "start": 1013.0, "end": 1018.0, "text": " This is designed that you need to use the homework assignments and the class resources.", "tokens": [639, 307, 4761, 300, 291, 643, 281, 764, 264, 14578, 22546, 293, 264, 1508, 3593, 13], "temperature": 0.0, "avg_logprob": -0.09637346118688583, "compression_ratio": 1.7335526315789473, "no_speech_prob": 9.607791434973478e-05}, {"id": 166, "seek": 99200, "start": 1018.0, "end": 1021.0, "text": " So as you can see from the people who have already been through this class,", "tokens": [407, 382, 291, 393, 536, 490, 264, 561, 567, 362, 1217, 668, 807, 341, 1508, 11], "temperature": 0.0, "avg_logprob": -0.09637346118688583, "compression_ratio": 1.7335526315789473, "no_speech_prob": 9.607791434973478e-05}, {"id": 167, "seek": 102100, "start": 1021.0, "end": 1024.0, "text": " they're actually finding that these are really important parts of the overall course.", "tokens": [436, 434, 767, 5006, 300, 613, 366, 534, 1021, 3166, 295, 264, 4787, 1164, 13], "temperature": 0.0, "avg_logprob": -0.0801019441513788, "compression_ratio": 1.89247311827957, "no_speech_prob": 0.00018799018289428204}, {"id": 168, "seek": 102100, "start": 1024.0, "end": 1031.0, "text": " Because each video is giving you, you're kind of seeing an end-to-end process of solving a real problem with deep learning.", "tokens": [1436, 1184, 960, 307, 2902, 291, 11, 291, 434, 733, 295, 2577, 364, 917, 12, 1353, 12, 521, 1399, 295, 12606, 257, 957, 1154, 365, 2452, 2539, 13], "temperature": 0.0, "avg_logprob": -0.0801019441513788, "compression_ratio": 1.89247311827957, "no_speech_prob": 0.00018799018289428204}, {"id": 169, "seek": 102100, "start": 1031.0, "end": 1036.0, "text": " And that means that there's not, though, a separate video on kind of this is everything you need to know about AWS", "tokens": [400, 300, 1355, 300, 456, 311, 406, 11, 1673, 11, 257, 4994, 960, 322, 733, 295, 341, 307, 1203, 291, 643, 281, 458, 466, 17650], "temperature": 0.0, "avg_logprob": -0.0801019441513788, "compression_ratio": 1.89247311827957, "no_speech_prob": 0.00018799018289428204}, {"id": 170, "seek": 102100, "start": 1036.0, "end": 1041.0, "text": " and your environment, and this is everything you need to know about this piece of code.", "tokens": [293, 428, 2823, 11, 293, 341, 307, 1203, 291, 643, 281, 458, 466, 341, 2522, 295, 3089, 13], "temperature": 0.0, "avg_logprob": -0.0801019441513788, "compression_ratio": 1.89247311827957, "no_speech_prob": 0.00018799018289428204}, {"id": 171, "seek": 102100, "start": 1041.0, "end": 1047.0, "text": " But rather, you're kind of seeing the end-to-end process, but you'll see it again and again throughout the lessons.", "tokens": [583, 2831, 11, 291, 434, 733, 295, 2577, 264, 917, 12, 1353, 12, 521, 1399, 11, 457, 291, 603, 536, 309, 797, 293, 797, 3710, 264, 8820, 13], "temperature": 0.0, "avg_logprob": -0.0801019441513788, "compression_ratio": 1.89247311827957, "no_speech_prob": 0.00018799018289428204}, {"id": 172, "seek": 104700, "start": 1047.0, "end": 1055.0, "text": " Now, it's okay if you're coming into this course with either a very large amount or a very small amount of data science background.", "tokens": [823, 11, 309, 311, 1392, 498, 291, 434, 1348, 666, 341, 1164, 365, 2139, 257, 588, 2416, 2372, 420, 257, 588, 1359, 2372, 295, 1412, 3497, 3678, 13], "temperature": 0.0, "avg_logprob": -0.061476049599824126, "compression_ratio": 1.712686567164179, "no_speech_prob": 6.107398075982928e-05}, {"id": 173, "seek": 104700, "start": 1055.0, "end": 1061.0, "text": " Everybody in the in-person course simply had to have had at least a year of coding experience.", "tokens": [7646, 294, 264, 294, 12, 10813, 1164, 2935, 632, 281, 362, 632, 412, 1935, 257, 1064, 295, 17720, 1752, 13], "temperature": 0.0, "avg_logprob": -0.061476049599824126, "compression_ratio": 1.712686567164179, "no_speech_prob": 6.107398075982928e-05}, {"id": 174, "seek": 104700, "start": 1061.0, "end": 1068.0, "text": " Even with a very wide variety in background, nearly everybody said they found the pacing about right for them.", "tokens": [2754, 365, 257, 588, 4874, 5673, 294, 3678, 11, 6217, 2201, 848, 436, 1352, 264, 43285, 466, 558, 337, 552, 13], "temperature": 0.0, "avg_logprob": -0.061476049599824126, "compression_ratio": 1.712686567164179, "no_speech_prob": 6.107398075982928e-05}, {"id": 175, "seek": 104700, "start": 1068.0, "end": 1075.0, "text": " And the reason for that, I think, is that we really give people the ability to pick up as much or as little as they want.", "tokens": [400, 264, 1778, 337, 300, 11, 286, 519, 11, 307, 300, 321, 534, 976, 561, 264, 3485, 281, 1888, 493, 382, 709, 420, 382, 707, 382, 436, 528, 13], "temperature": 0.0, "avg_logprob": -0.061476049599824126, "compression_ratio": 1.712686567164179, "no_speech_prob": 6.107398075982928e-05}, {"id": 176, "seek": 107500, "start": 1075.0, "end": 1081.0, "text": " So the forums, if you want to dig very, very deep into advanced topics, you can.", "tokens": [407, 264, 26998, 11, 498, 291, 528, 281, 2528, 588, 11, 588, 2452, 666, 7339, 8378, 11, 291, 393, 13], "temperature": 0.0, "avg_logprob": -0.057512640953063965, "compression_ratio": 1.7352941176470589, "no_speech_prob": 3.218997153453529e-05}, {"id": 177, "seek": 107500, "start": 1081.0, "end": 1085.0, "text": " Or if absolutely everything is new to you, then that's fine, too.", "tokens": [1610, 498, 3122, 1203, 307, 777, 281, 291, 11, 550, 300, 311, 2489, 11, 886, 13], "temperature": 0.0, "avg_logprob": -0.057512640953063965, "compression_ratio": 1.7352941176470589, "no_speech_prob": 3.218997153453529e-05}, {"id": 178, "seek": 107500, "start": 1085.0, "end": 1089.0, "text": " There'll be more than enough to do just to get through the basic parts of the assignments.", "tokens": [821, 603, 312, 544, 813, 1547, 281, 360, 445, 281, 483, 807, 264, 3875, 3166, 295, 264, 22546, 13], "temperature": 0.0, "avg_logprob": -0.057512640953063965, "compression_ratio": 1.7352941176470589, "no_speech_prob": 3.218997153453529e-05}, {"id": 179, "seek": 107500, "start": 1089.0, "end": 1094.0, "text": " And of course, on the forums, we'd be very happy to help you with all of your questions there.", "tokens": [400, 295, 1164, 11, 322, 264, 26998, 11, 321, 1116, 312, 588, 2055, 281, 854, 291, 365, 439, 295, 428, 1651, 456, 13], "temperature": 0.0, "avg_logprob": -0.057512640953063965, "compression_ratio": 1.7352941176470589, "no_speech_prob": 3.218997153453529e-05}, {"id": 180, "seek": 107500, "start": 1094.0, "end": 1099.0, "text": " And if you are more advanced, we really appreciate your help in adding new material to the Wiki,", "tokens": [400, 498, 291, 366, 544, 7339, 11, 321, 534, 4449, 428, 854, 294, 5127, 777, 2527, 281, 264, 35892, 11], "temperature": 0.0, "avg_logprob": -0.057512640953063965, "compression_ratio": 1.7352941176470589, "no_speech_prob": 3.218997153453529e-05}, {"id": 181, "seek": 107500, "start": 1099.0, "end": 1102.0, "text": " answering others' questions on the forums.", "tokens": [13430, 2357, 6, 1651, 322, 264, 26998, 13], "temperature": 0.0, "avg_logprob": -0.057512640953063965, "compression_ratio": 1.7352941176470589, "no_speech_prob": 3.218997153453529e-05}, {"id": 182, "seek": 110200, "start": 1102.0, "end": 1108.0, "text": " People started their own threads on the forums around outside-related topics that they were interested in.", "tokens": [3432, 1409, 641, 1065, 19314, 322, 264, 26998, 926, 2380, 12, 12004, 8378, 300, 436, 645, 3102, 294, 13], "temperature": 0.0, "avg_logprob": -0.08884476326607368, "compression_ratio": 1.7117437722419928, "no_speech_prob": 6.0136189858894795e-05}, {"id": 183, "seek": 110200, "start": 1108.0, "end": 1111.0, "text": " There are a lot of different ways to be involved.", "tokens": [821, 366, 257, 688, 295, 819, 2098, 281, 312, 3288, 13], "temperature": 0.0, "avg_logprob": -0.08884476326607368, "compression_ratio": 1.7117437722419928, "no_speech_prob": 6.0136189858894795e-05}, {"id": 184, "seek": 110200, "start": 1111.0, "end": 1115.0, "text": " So here's a couple of quotes we got from people after they completed the in-person course.", "tokens": [407, 510, 311, 257, 1916, 295, 19963, 321, 658, 490, 561, 934, 436, 7365, 264, 294, 12, 10813, 1164, 13], "temperature": 0.0, "avg_logprob": -0.08884476326607368, "compression_ratio": 1.7117437722419928, "no_speech_prob": 6.0136189858894795e-05}, {"id": 185, "seek": 110200, "start": 1115.0, "end": 1118.0, "text": " And this is one that we heard again and again.", "tokens": [400, 341, 307, 472, 300, 321, 2198, 797, 293, 797, 13], "temperature": 0.0, "avg_logprob": -0.08884476326607368, "compression_ratio": 1.7117437722419928, "no_speech_prob": 6.0136189858894795e-05}, {"id": 186, "seek": 110200, "start": 1118.0, "end": 1120.0, "text": " So for example, this person says,", "tokens": [407, 337, 1365, 11, 341, 954, 1619, 11], "temperature": 0.0, "avg_logprob": -0.08884476326607368, "compression_ratio": 1.7117437722419928, "no_speech_prob": 6.0136189858894795e-05}, {"id": 187, "seek": 110200, "start": 1120.0, "end": 1129.0, "text": " I personally fell into the habit of watching the lectures too much and Googling definitions and concepts and so forth too much without running the code.", "tokens": [286, 5665, 5696, 666, 264, 7164, 295, 1976, 264, 16564, 886, 709, 293, 45005, 1688, 21988, 293, 10392, 293, 370, 5220, 886, 709, 1553, 2614, 264, 3089, 13], "temperature": 0.0, "avg_logprob": -0.08884476326607368, "compression_ratio": 1.7117437722419928, "no_speech_prob": 6.0136189858894795e-05}, {"id": 188, "seek": 112900, "start": 1129.0, "end": 1135.0, "text": " But first, I thought that I should read the code quickly and then spend time researching the theory behind it.", "tokens": [583, 700, 11, 286, 1194, 300, 286, 820, 1401, 264, 3089, 2661, 293, 550, 3496, 565, 24176, 264, 5261, 2261, 309, 13], "temperature": 0.0, "avg_logprob": -0.06527336766897154, "compression_ratio": 1.773972602739726, "no_speech_prob": 5.389395300881006e-05}, {"id": 189, "seek": 112900, "start": 1135.0, "end": 1140.0, "text": " In retrospect, I should have spent the majority of my time on the actual code in the notebooks instead,", "tokens": [682, 34997, 11, 286, 820, 362, 4418, 264, 6286, 295, 452, 565, 322, 264, 3539, 3089, 294, 264, 43782, 2602, 11], "temperature": 0.0, "avg_logprob": -0.06527336766897154, "compression_ratio": 1.773972602739726, "no_speech_prob": 5.389395300881006e-05}, {"id": 190, "seek": 112900, "start": 1140.0, "end": 1145.0, "text": " in terms of running it and seeing what goes into it and seeing what comes out of it.", "tokens": [294, 2115, 295, 2614, 309, 293, 2577, 437, 1709, 666, 309, 293, 2577, 437, 1487, 484, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.06527336766897154, "compression_ratio": 1.773972602739726, "no_speech_prob": 5.389395300881006e-05}, {"id": 191, "seek": 112900, "start": 1145.0, "end": 1148.0, "text": " And Rachel, I know you've seen similar things in your past teaching experience.", "tokens": [400, 14246, 11, 286, 458, 291, 600, 1612, 2531, 721, 294, 428, 1791, 4571, 1752, 13], "temperature": 0.0, "avg_logprob": -0.06527336766897154, "compression_ratio": 1.773972602739726, "no_speech_prob": 5.389395300881006e-05}, {"id": 192, "seek": 112900, "start": 1148.0, "end": 1152.0, "text": " Yes, I've seen this in teaching full stack software development to students.", "tokens": [1079, 11, 286, 600, 1612, 341, 294, 4571, 1577, 8630, 4722, 3250, 281, 1731, 13], "temperature": 0.0, "avg_logprob": -0.06527336766897154, "compression_ratio": 1.773972602739726, "no_speech_prob": 5.389395300881006e-05}, {"id": 193, "seek": 112900, "start": 1152.0, "end": 1156.0, "text": " And I also know that I've been guilty of it myself sometimes.", "tokens": [400, 286, 611, 458, 300, 286, 600, 668, 12341, 295, 309, 2059, 2171, 13], "temperature": 0.0, "avg_logprob": -0.06527336766897154, "compression_ratio": 1.773972602739726, "no_speech_prob": 5.389395300881006e-05}, {"id": 194, "seek": 115600, "start": 1156.0, "end": 1160.0, "text": " And that was that students would sometimes kind of rather than start their project,", "tokens": [400, 300, 390, 300, 1731, 576, 2171, 733, 295, 2831, 813, 722, 641, 1716, 11], "temperature": 0.0, "avg_logprob": -0.06495135699131692, "compression_ratio": 1.9621993127147765, "no_speech_prob": 5.507161404239014e-06}, {"id": 195, "seek": 115600, "start": 1160.0, "end": 1163.0, "text": " they would keep doing more and more research, reading more and more tutorials,", "tokens": [436, 576, 1066, 884, 544, 293, 544, 2132, 11, 3760, 544, 293, 544, 17616, 11], "temperature": 0.0, "avg_logprob": -0.06495135699131692, "compression_ratio": 1.9621993127147765, "no_speech_prob": 5.507161404239014e-06}, {"id": 196, "seek": 115600, "start": 1163.0, "end": 1167.0, "text": " and feeling like there's more and more they need to learn before they can start coding.", "tokens": [293, 2633, 411, 456, 311, 544, 293, 544, 436, 643, 281, 1466, 949, 436, 393, 722, 17720, 13], "temperature": 0.0, "avg_logprob": -0.06495135699131692, "compression_ratio": 1.9621993127147765, "no_speech_prob": 5.507161404239014e-06}, {"id": 197, "seek": 115600, "start": 1167.0, "end": 1172.0, "text": " And two problems with that are one, and I mean, you want to have some background before you begin,", "tokens": [400, 732, 2740, 365, 300, 366, 472, 11, 293, 286, 914, 11, 291, 528, 281, 362, 512, 3678, 949, 291, 1841, 11], "temperature": 0.0, "avg_logprob": -0.06495135699131692, "compression_ratio": 1.9621993127147765, "no_speech_prob": 5.507161404239014e-06}, {"id": 198, "seek": 115600, "start": 1172.0, "end": 1183.0, "text": " but there's a point where you just need to start coding because you can't know exactly what you're going to need until you start coding and building and seeing what errors you get and what things you don't know how to do.", "tokens": [457, 456, 311, 257, 935, 689, 291, 445, 643, 281, 722, 17720, 570, 291, 393, 380, 458, 2293, 437, 291, 434, 516, 281, 643, 1826, 291, 722, 17720, 293, 2390, 293, 2577, 437, 13603, 291, 483, 293, 437, 721, 291, 500, 380, 458, 577, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.06495135699131692, "compression_ratio": 1.9621993127147765, "no_speech_prob": 5.507161404239014e-06}, {"id": 199, "seek": 118300, "start": 1183.0, "end": 1188.0, "text": " And then secondly, the test of whether you understand something is whether you can build with it.", "tokens": [400, 550, 26246, 11, 264, 1500, 295, 1968, 291, 1223, 746, 307, 1968, 291, 393, 1322, 365, 309, 13], "temperature": 0.0, "avg_logprob": -0.06346950846270097, "compression_ratio": 1.8274647887323943, "no_speech_prob": 4.610644100466743e-05}, {"id": 200, "seek": 118300, "start": 1188.0, "end": 1192.0, "text": " And so kind of reading tutorials, it's very possible to think, oh, I understand all this,", "tokens": [400, 370, 733, 295, 3760, 17616, 11, 309, 311, 588, 1944, 281, 519, 11, 1954, 11, 286, 1223, 439, 341, 11], "temperature": 0.0, "avg_logprob": -0.06346950846270097, "compression_ratio": 1.8274647887323943, "no_speech_prob": 4.610644100466743e-05}, {"id": 201, "seek": 118300, "start": 1192.0, "end": 1202.0, "text": " but it's not till you're writing code yourself, kind of seeing what your what your error rates are and what what's working and what's not that you know whether or not you truly understand something.", "tokens": [457, 309, 311, 406, 4288, 291, 434, 3579, 3089, 1803, 11, 733, 295, 2577, 437, 428, 437, 428, 6713, 6846, 366, 293, 437, 437, 311, 1364, 293, 437, 311, 406, 300, 291, 458, 1968, 420, 406, 291, 4908, 1223, 746, 13], "temperature": 0.0, "avg_logprob": -0.06346950846270097, "compression_ratio": 1.8274647887323943, "no_speech_prob": 4.610644100466743e-05}, {"id": 202, "seek": 118300, "start": 1202.0, "end": 1209.0, "text": " Yeah. So when I saw students at the study sessions during the week at USF, I would keep telling them the same thing again and again.", "tokens": [865, 13, 407, 562, 286, 1866, 1731, 412, 264, 2979, 11081, 1830, 264, 1243, 412, 2546, 37, 11, 286, 576, 1066, 3585, 552, 264, 912, 551, 797, 293, 797, 13], "temperature": 0.0, "avg_logprob": -0.06346950846270097, "compression_ratio": 1.8274647887323943, "no_speech_prob": 4.610644100466743e-05}, {"id": 203, "seek": 120900, "start": 1209.0, "end": 1214.0, "text": " Just don't stop and wait till you feel ready to code. Start coding now.", "tokens": [1449, 500, 380, 1590, 293, 1699, 4288, 291, 841, 1919, 281, 3089, 13, 6481, 17720, 586, 13], "temperature": 0.0, "avg_logprob": -0.07535726855499576, "compression_ratio": 1.7233201581027668, "no_speech_prob": 2.3186634280136786e-05}, {"id": 204, "seek": 120900, "start": 1214.0, "end": 1219.0, "text": " And it's through that coding experience that you're actually going to figure out what you don't know and what you do know.", "tokens": [400, 309, 311, 807, 300, 17720, 1752, 300, 291, 434, 767, 516, 281, 2573, 484, 437, 291, 500, 380, 458, 293, 437, 291, 360, 458, 13], "temperature": 0.0, "avg_logprob": -0.07535726855499576, "compression_ratio": 1.7233201581027668, "no_speech_prob": 2.3186634280136786e-05}, {"id": 205, "seek": 120900, "start": 1219.0, "end": 1223.0, "text": " And you'll be able to develop the intuition by running lots of experiments.", "tokens": [400, 291, 603, 312, 1075, 281, 1499, 264, 24002, 538, 2614, 3195, 295, 12050, 13], "temperature": 0.0, "avg_logprob": -0.07535726855499576, "compression_ratio": 1.7233201581027668, "no_speech_prob": 2.3186634280136786e-05}, {"id": 206, "seek": 120900, "start": 1223.0, "end": 1229.0, "text": " This is another interesting quote from somebody talking about this learning style.", "tokens": [639, 307, 1071, 1880, 6513, 490, 2618, 1417, 466, 341, 2539, 3758, 13], "temperature": 0.0, "avg_logprob": -0.07535726855499576, "compression_ratio": 1.7233201581027668, "no_speech_prob": 2.3186634280136786e-05}, {"id": 207, "seek": 120900, "start": 1229.0, "end": 1233.0, "text": " He said, it's been very interesting learning from somebody who is an entrepreneur.", "tokens": [634, 848, 11, 309, 311, 668, 588, 1880, 2539, 490, 2618, 567, 307, 364, 14307, 13], "temperature": 0.0, "avg_logprob": -0.07535726855499576, "compression_ratio": 1.7233201581027668, "no_speech_prob": 2.3186634280136786e-05}, {"id": 208, "seek": 123300, "start": 1233.0, "end": 1240.0, "text": " That'd be me. A very no-nonsense approach to getting things done, very hands on, very smart and driven.", "tokens": [663, 1116, 312, 385, 13, 316, 588, 572, 12, 77, 13039, 3109, 281, 1242, 721, 1096, 11, 588, 2377, 322, 11, 588, 4069, 293, 9555, 13], "temperature": 0.0, "avg_logprob": -0.09794807434082031, "compression_ratio": 1.6415094339622642, "no_speech_prob": 7.527778052462963e-06}, {"id": 209, "seek": 123300, "start": 1240.0, "end": 1243.0, "text": " Your usual career and structure is quite the opposite.", "tokens": [2260, 7713, 3988, 293, 3877, 307, 1596, 264, 6182, 13], "temperature": 0.0, "avg_logprob": -0.09794807434082031, "compression_ratio": 1.6415094339622642, "no_speech_prob": 7.527778052462963e-06}, {"id": 210, "seek": 123300, "start": 1243.0, "end": 1246.0, "text": " So it's been refreshing and even somewhat shocking.", "tokens": [407, 309, 311, 668, 19772, 293, 754, 8344, 18776, 13], "temperature": 0.0, "avg_logprob": -0.09794807434082031, "compression_ratio": 1.6415094339622642, "no_speech_prob": 7.527778052462963e-06}, {"id": 211, "seek": 123300, "start": 1246.0, "end": 1249.0, "text": " This is possibly understating things a bit. It can be.", "tokens": [639, 307, 6264, 833, 372, 990, 721, 257, 857, 13, 467, 393, 312, 13], "temperature": 0.0, "avg_logprob": -0.09794807434082031, "compression_ratio": 1.6415094339622642, "no_speech_prob": 7.527778052462963e-06}, {"id": 212, "seek": 123300, "start": 1249.0, "end": 1251.0, "text": " In fact, we heard from quite a few people at the start.", "tokens": [682, 1186, 11, 321, 2198, 490, 1596, 257, 1326, 561, 412, 264, 722, 13], "temperature": 0.0, "avg_logprob": -0.09794807434082031, "compression_ratio": 1.6415094339622642, "no_speech_prob": 7.527778052462963e-06}, {"id": 213, "seek": 123300, "start": 1251.0, "end": 1262.0, "text": " It was somewhat shocking to find so many things taught so quickly and it kind of can seem like such a high level.", "tokens": [467, 390, 8344, 18776, 281, 915, 370, 867, 721, 5928, 370, 2661, 293, 309, 733, 295, 393, 1643, 411, 1270, 257, 1090, 1496, 13], "temperature": 0.0, "avg_logprob": -0.09794807434082031, "compression_ratio": 1.6415094339622642, "no_speech_prob": 7.527778052462963e-06}, {"id": 214, "seek": 126200, "start": 1262.0, "end": 1268.0, "text": " But of course, by the end of the seven weeks, and assuming that each time you're putting 10 hours into those weeks,", "tokens": [583, 295, 1164, 11, 538, 264, 917, 295, 264, 3407, 3259, 11, 293, 11926, 300, 1184, 565, 291, 434, 3372, 1266, 2496, 666, 729, 3259, 11], "temperature": 0.0, "avg_logprob": -0.0904977398533975, "compression_ratio": 1.7403508771929825, "no_speech_prob": 3.37280762323644e-05}, {"id": 215, "seek": 126200, "start": 1268.0, "end": 1274.0, "text": " you've actually got many, many full end-to-end processors under your belt.", "tokens": [291, 600, 767, 658, 867, 11, 867, 1577, 917, 12, 1353, 12, 521, 27751, 833, 428, 10750, 13], "temperature": 0.0, "avg_logprob": -0.0904977398533975, "compression_ratio": 1.7403508771929825, "no_speech_prob": 3.37280762323644e-05}, {"id": 216, "seek": 126200, "start": 1274.0, "end": 1279.0, "text": " So by the end of it, you're actually going to develop a very deep and complete understanding.", "tokens": [407, 538, 264, 917, 295, 309, 11, 291, 434, 767, 516, 281, 1499, 257, 588, 2452, 293, 3566, 3701, 13], "temperature": 0.0, "avg_logprob": -0.0904977398533975, "compression_ratio": 1.7403508771929825, "no_speech_prob": 3.37280762323644e-05}, {"id": 217, "seek": 126200, "start": 1279.0, "end": 1283.0, "text": " Yeah, I know after the first lesson, I heard a number of students kind of say things like,", "tokens": [865, 11, 286, 458, 934, 264, 700, 6898, 11, 286, 2198, 257, 1230, 295, 1731, 733, 295, 584, 721, 411, 11], "temperature": 0.0, "avg_logprob": -0.0904977398533975, "compression_ratio": 1.7403508771929825, "no_speech_prob": 3.37280762323644e-05}, {"id": 218, "seek": 126200, "start": 1283.0, "end": 1290.0, "text": " oh, I didn't really get the details from that lesson and I feel like I need to spend all this time studying the details.", "tokens": [1954, 11, 286, 994, 380, 534, 483, 264, 4365, 490, 300, 6898, 293, 286, 841, 411, 286, 643, 281, 3496, 439, 341, 565, 7601, 264, 4365, 13], "temperature": 0.0, "avg_logprob": -0.0904977398533975, "compression_ratio": 1.7403508771929825, "no_speech_prob": 3.37280762323644e-05}, {"id": 219, "seek": 129000, "start": 1290.0, "end": 1293.0, "text": " And we hadn't taught the details in the first lesson.", "tokens": [400, 321, 8782, 380, 5928, 264, 4365, 294, 264, 700, 6898, 13], "temperature": 0.0, "avg_logprob": -0.07031244306421991, "compression_ratio": 1.785953177257525, "no_speech_prob": 2.4295733965118416e-05}, {"id": 220, "seek": 129000, "start": 1293.0, "end": 1297.0, "text": " And the idea is that kind of we went more and more in depth each time.", "tokens": [400, 264, 1558, 307, 300, 733, 295, 321, 1437, 544, 293, 544, 294, 7161, 1184, 565, 13], "temperature": 0.0, "avg_logprob": -0.07031244306421991, "compression_ratio": 1.785953177257525, "no_speech_prob": 2.4295733965118416e-05}, {"id": 221, "seek": 129000, "start": 1297.0, "end": 1302.0, "text": " But you're seeing this end-to-end process and then kind of as time goes on, digging into it more.", "tokens": [583, 291, 434, 2577, 341, 917, 12, 1353, 12, 521, 1399, 293, 550, 733, 295, 382, 565, 1709, 322, 11, 17343, 666, 309, 544, 13], "temperature": 0.0, "avg_logprob": -0.07031244306421991, "compression_ratio": 1.785953177257525, "no_speech_prob": 2.4295733965118416e-05}, {"id": 222, "seek": 129000, "start": 1302.0, "end": 1305.0, "text": " But even after the first lesson, you can apply it.", "tokens": [583, 754, 934, 264, 700, 6898, 11, 291, 393, 3079, 309, 13], "temperature": 0.0, "avg_logprob": -0.07031244306421991, "compression_ratio": 1.785953177257525, "no_speech_prob": 2.4295733965118416e-05}, {"id": 223, "seek": 129000, "start": 1305.0, "end": 1308.0, "text": " You can actually create world class image recognition models.", "tokens": [509, 393, 767, 1884, 1002, 1508, 3256, 11150, 5245, 13], "temperature": 0.0, "avg_logprob": -0.07031244306421991, "compression_ratio": 1.785953177257525, "no_speech_prob": 2.4295733965118416e-05}, {"id": 224, "seek": 129000, "start": 1308.0, "end": 1312.0, "text": " And so you can go back to your organization and start trying things.", "tokens": [400, 370, 291, 393, 352, 646, 281, 428, 4475, 293, 722, 1382, 721, 13], "temperature": 0.0, "avg_logprob": -0.07031244306421991, "compression_ratio": 1.785953177257525, "no_speech_prob": 2.4295733965118416e-05}, {"id": 225, "seek": 129000, "start": 1312.0, "end": 1314.0, "text": " This is something else we encourage people to do.", "tokens": [639, 307, 746, 1646, 321, 5373, 561, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.07031244306421991, "compression_ratio": 1.785953177257525, "no_speech_prob": 2.4295733965118416e-05}, {"id": 226, "seek": 129000, "start": 1314.0, "end": 1319.0, "text": " Try things with your own data and your own problems from the very first lesson.", "tokens": [6526, 721, 365, 428, 1065, 1412, 293, 428, 1065, 2740, 490, 264, 588, 700, 6898, 13], "temperature": 0.0, "avg_logprob": -0.07031244306421991, "compression_ratio": 1.785953177257525, "no_speech_prob": 2.4295733965118416e-05}, {"id": 227, "seek": 131900, "start": 1319.0, "end": 1323.0, "text": " So it's been an interesting experience in every way.", "tokens": [407, 309, 311, 668, 364, 1880, 1752, 294, 633, 636, 13], "temperature": 0.0, "avg_logprob": -0.05209836398853975, "compression_ratio": 1.5598290598290598, "no_speech_prob": 3.218676647520624e-05}, {"id": 228, "seek": 131900, "start": 1323.0, "end": 1326.0, "text": " Even the way we built this course was unusual.", "tokens": [2754, 264, 636, 321, 3094, 341, 1164, 390, 10901, 13], "temperature": 0.0, "avg_logprob": -0.05209836398853975, "compression_ratio": 1.5598290598290598, "no_speech_prob": 3.218676647520624e-05}, {"id": 229, "seek": 131900, "start": 1326.0, "end": 1334.0, "text": " For example, I actually wrote most of the material while traveling from the northern tip to the southern tip of Japan.", "tokens": [1171, 1365, 11, 286, 767, 4114, 881, 295, 264, 2527, 1339, 9712, 490, 264, 14197, 4125, 281, 264, 13456, 4125, 295, 3367, 13], "temperature": 0.0, "avg_logprob": -0.05209836398853975, "compression_ratio": 1.5598290598290598, "no_speech_prob": 3.218676647520624e-05}, {"id": 230, "seek": 131900, "start": 1334.0, "end": 1340.0, "text": " I coded and wrote in every possible place you can imagine.", "tokens": [286, 34874, 293, 4114, 294, 633, 1944, 1081, 291, 393, 3811, 13], "temperature": 0.0, "avg_logprob": -0.05209836398853975, "compression_ratio": 1.5598290598290598, "no_speech_prob": 3.218676647520624e-05}, {"id": 231, "seek": 131900, "start": 1340.0, "end": 1345.0, "text": " And this was really an experiment for me because I studied human learning theory a lot.", "tokens": [400, 341, 390, 534, 364, 5120, 337, 385, 570, 286, 9454, 1952, 2539, 5261, 257, 688, 13], "temperature": 0.0, "avg_logprob": -0.05209836398853975, "compression_ratio": 1.5598290598290598, "no_speech_prob": 3.218676647520624e-05}, {"id": 232, "seek": 134500, "start": 1345.0, "end": 1352.0, "text": " And I know that in theory, human creativity is meant to be better when you have a wider variety of contexts.", "tokens": [400, 286, 458, 300, 294, 5261, 11, 1952, 12915, 307, 4140, 281, 312, 1101, 562, 291, 362, 257, 11842, 5673, 295, 30628, 13], "temperature": 0.0, "avg_logprob": -0.06493832117103669, "compression_ratio": 1.6359649122807018, "no_speech_prob": 7.245701999636367e-05}, {"id": 233, "seek": 134500, "start": 1352.0, "end": 1358.0, "text": " And interestingly, I actually found I was more productive in that month than I feel I ever have been before.", "tokens": [400, 25873, 11, 286, 767, 1352, 286, 390, 544, 13304, 294, 300, 1618, 813, 286, 841, 286, 1562, 362, 668, 949, 13], "temperature": 0.0, "avg_logprob": -0.06493832117103669, "compression_ratio": 1.6359649122807018, "no_speech_prob": 7.245701999636367e-05}, {"id": 234, "seek": 134500, "start": 1358.0, "end": 1369.0, "text": " And you'll see actually in the material that you learn, we show a lot of new techniques or different techniques or different ways of thinking about things.", "tokens": [400, 291, 603, 536, 767, 294, 264, 2527, 300, 291, 1466, 11, 321, 855, 257, 688, 295, 777, 7512, 420, 819, 7512, 420, 819, 2098, 295, 1953, 466, 721, 13], "temperature": 0.0, "avg_logprob": -0.06493832117103669, "compression_ratio": 1.6359649122807018, "no_speech_prob": 7.245701999636367e-05}, {"id": 235, "seek": 136900, "start": 1369.0, "end": 1379.0, "text": " And I think this kind of different way of building the course, perhaps, was really helpful in coming up with this kind of more creative approach.", "tokens": [400, 286, 519, 341, 733, 295, 819, 636, 295, 2390, 264, 1164, 11, 4317, 11, 390, 534, 4961, 294, 1348, 493, 365, 341, 733, 295, 544, 5880, 3109, 13], "temperature": 0.0, "avg_logprob": -0.10439742501102277, "compression_ratio": 1.5414364640883977, "no_speech_prob": 6.810119521105662e-05}, {"id": 236, "seek": 136900, "start": 1379.0, "end": 1392.0, "text": " So not everybody in the course, in the in-person course, were able to put in at least eight hours a week, but the vast majority were.", "tokens": [407, 406, 2201, 294, 264, 1164, 11, 294, 264, 294, 12, 10813, 1164, 11, 645, 1075, 281, 829, 294, 412, 1935, 3180, 2496, 257, 1243, 11, 457, 264, 8369, 6286, 645, 13], "temperature": 0.0, "avg_logprob": -0.10439742501102277, "compression_ratio": 1.5414364640883977, "no_speech_prob": 6.810119521105662e-05}, {"id": 237, "seek": 139200, "start": 1392.0, "end": 1401.0, "text": " And those who didn't still completed the course, they just found they didn't necessarily pick everything up the way that they hoped they would.", "tokens": [400, 729, 567, 994, 380, 920, 7365, 264, 1164, 11, 436, 445, 1352, 436, 994, 380, 4725, 1888, 1203, 493, 264, 636, 300, 436, 19737, 436, 576, 13], "temperature": 0.0, "avg_logprob": -0.05781484964325672, "compression_ratio": 1.7074829931972788, "no_speech_prob": 0.00013332349772099406}, {"id": 238, "seek": 139200, "start": 1401.0, "end": 1404.0, "text": " But of course, the nice thing is you can always come back to it later.", "tokens": [583, 295, 1164, 11, 264, 1481, 551, 307, 291, 393, 1009, 808, 646, 281, 309, 1780, 13], "temperature": 0.0, "avg_logprob": -0.05781484964325672, "compression_ratio": 1.7074829931972788, "no_speech_prob": 0.00013332349772099406}, {"id": 239, "seek": 139200, "start": 1404.0, "end": 1414.0, "text": " So our suggestion would be now that it's a MOOC, now that you don't have to do it every single week, ideally you will put in the 10 hours a week.", "tokens": [407, 527, 16541, 576, 312, 586, 300, 309, 311, 257, 49197, 34, 11, 586, 300, 291, 500, 380, 362, 281, 360, 309, 633, 2167, 1243, 11, 22915, 291, 486, 829, 294, 264, 1266, 2496, 257, 1243, 13], "temperature": 0.0, "avg_logprob": -0.05781484964325672, "compression_ratio": 1.7074829931972788, "no_speech_prob": 0.00013332349772099406}, {"id": 240, "seek": 139200, "start": 1414.0, "end": 1416.0, "text": " Did you want to talk about those 10 hours a little bit, Rachel?", "tokens": [2589, 291, 528, 281, 751, 466, 729, 1266, 2496, 257, 707, 857, 11, 14246, 30], "temperature": 0.0, "avg_logprob": -0.05781484964325672, "compression_ratio": 1.7074829931972788, "no_speech_prob": 0.00013332349772099406}, {"id": 241, "seek": 139200, "start": 1416.0, "end": 1421.0, "text": " Yeah, we wanted to give you kind of some suggestions on how to use that time.", "tokens": [865, 11, 321, 1415, 281, 976, 291, 733, 295, 512, 13396, 322, 577, 281, 764, 300, 565, 13], "temperature": 0.0, "avg_logprob": -0.05781484964325672, "compression_ratio": 1.7074829931972788, "no_speech_prob": 0.00013332349772099406}, {"id": 242, "seek": 142100, "start": 1421.0, "end": 1426.0, "text": " So the videos are between two and two and a half hours long.", "tokens": [407, 264, 2145, 366, 1296, 732, 293, 732, 293, 257, 1922, 2496, 938, 13], "temperature": 0.0, "avg_logprob": -0.06793776284093442, "compression_ratio": 1.5576036866359446, "no_speech_prob": 3.535175710567273e-05}, {"id": 243, "seek": 142100, "start": 1426.0, "end": 1434.0, "text": " And so with those videos, you may find it helpful as you review them to use these notes.", "tokens": [400, 370, 365, 729, 2145, 11, 291, 815, 915, 309, 4961, 382, 291, 3131, 552, 281, 764, 613, 5570, 13], "temperature": 0.0, "avg_logprob": -0.06793776284093442, "compression_ratio": 1.5576036866359446, "no_speech_prob": 3.535175710567273e-05}, {"id": 244, "seek": 142100, "start": 1434.0, "end": 1437.0, "text": " So this is coming from our Wiki, wiki.fast.ai.", "tokens": [407, 341, 307, 1348, 490, 527, 35892, 11, 261, 9850, 13, 7011, 13, 1301, 13], "temperature": 0.0, "avg_logprob": -0.06793776284093442, "compression_ratio": 1.5576036866359446, "no_speech_prob": 3.535175710567273e-05}, {"id": 245, "seek": 142100, "start": 1437.0, "end": 1443.0, "text": " There's a page for each lesson that has notes kind of about the lesson.", "tokens": [821, 311, 257, 3028, 337, 1184, 6898, 300, 575, 5570, 733, 295, 466, 264, 6898, 13], "temperature": 0.0, "avg_logprob": -0.06793776284093442, "compression_ratio": 1.5576036866359446, "no_speech_prob": 3.535175710567273e-05}, {"id": 246, "seek": 142100, "start": 1443.0, "end": 1449.0, "text": " It also has links to other other resources that may be useful to you.", "tokens": [467, 611, 575, 6123, 281, 661, 661, 3593, 300, 815, 312, 4420, 281, 291, 13], "temperature": 0.0, "avg_logprob": -0.06793776284093442, "compression_ratio": 1.5576036866359446, "no_speech_prob": 3.535175710567273e-05}, {"id": 247, "seek": 144900, "start": 1449.0, "end": 1451.0, "text": " These notes are pretty complete.", "tokens": [1981, 5570, 366, 1238, 3566, 13], "temperature": 0.0, "avg_logprob": -0.09888846079508463, "compression_ratio": 1.6526717557251909, "no_speech_prob": 9.026475163409486e-05}, {"id": 248, "seek": 144900, "start": 1451.0, "end": 1459.0, "text": " They're not designed to be read entirely independently from the video lesson, but they are something which you can read on your way to work.", "tokens": [814, 434, 406, 4761, 281, 312, 1401, 7696, 21761, 490, 264, 960, 6898, 11, 457, 436, 366, 746, 597, 291, 393, 1401, 322, 428, 636, 281, 589, 13], "temperature": 0.0, "avg_logprob": -0.09888846079508463, "compression_ratio": 1.6526717557251909, "no_speech_prob": 9.026475163409486e-05}, {"id": 249, "seek": 144900, "start": 1459.0, "end": 1464.0, "text": " Maybe when you don't have, it's not convenient to actually watch the lesson.", "tokens": [2704, 562, 291, 500, 380, 362, 11, 309, 311, 406, 10851, 281, 767, 1159, 264, 6898, 13], "temperature": 0.0, "avg_logprob": -0.09888846079508463, "compression_ratio": 1.6526717557251909, "no_speech_prob": 9.026475163409486e-05}, {"id": 250, "seek": 144900, "start": 1464.0, "end": 1469.0, "text": " Sorry, I was going to say we're expecting that you'll watch the lessons more than once.", "tokens": [4919, 11, 286, 390, 516, 281, 584, 321, 434, 9650, 300, 291, 603, 1159, 264, 8820, 544, 813, 1564, 13], "temperature": 0.0, "avg_logprob": -0.09888846079508463, "compression_ratio": 1.6526717557251909, "no_speech_prob": 9.026475163409486e-05}, {"id": 251, "seek": 144900, "start": 1469.0, "end": 1474.0, "text": " So the first time through, you're kind of watching to get maybe a lot of the high level ideas.", "tokens": [407, 264, 700, 565, 807, 11, 291, 434, 733, 295, 1976, 281, 483, 1310, 257, 688, 295, 264, 1090, 1496, 3487, 13], "temperature": 0.0, "avg_logprob": -0.09888846079508463, "compression_ratio": 1.6526717557251909, "no_speech_prob": 9.026475163409486e-05}, {"id": 252, "seek": 147400, "start": 1474.0, "end": 1481.0, "text": " Then you'll probably want to read the Wiki, try out the notebooks and then go back and watch the lesson again.", "tokens": [1396, 291, 603, 1391, 528, 281, 1401, 264, 35892, 11, 853, 484, 264, 43782, 293, 550, 352, 646, 293, 1159, 264, 6898, 797, 13], "temperature": 0.0, "avg_logprob": -0.07855206084765977, "compression_ratio": 1.7633333333333334, "no_speech_prob": 2.468075581418816e-05}, {"id": 253, "seek": 147400, "start": 1481.0, "end": 1483.0, "text": " Kind of maybe to get more detail.", "tokens": [9242, 295, 1310, 281, 483, 544, 2607, 13], "temperature": 0.0, "avg_logprob": -0.07855206084765977, "compression_ratio": 1.7633333333333334, "no_speech_prob": 2.468075581418816e-05}, {"id": 254, "seek": 147400, "start": 1483.0, "end": 1487.0, "text": " Yeah, I don't think any of our students in the in-person course just watch the lessons once.", "tokens": [865, 11, 286, 500, 380, 519, 604, 295, 527, 1731, 294, 264, 294, 12, 10813, 1164, 445, 1159, 264, 8820, 1564, 13], "temperature": 0.0, "avg_logprob": -0.07855206084765977, "compression_ratio": 1.7633333333333334, "no_speech_prob": 2.468075581418816e-05}, {"id": 255, "seek": 147400, "start": 1487.0, "end": 1492.0, "text": " They saw them live, of course, but then we also they also had the recording for the next day.", "tokens": [814, 1866, 552, 1621, 11, 295, 1164, 11, 457, 550, 321, 611, 436, 611, 632, 264, 6613, 337, 264, 958, 786, 13], "temperature": 0.0, "avg_logprob": -0.07855206084765977, "compression_ratio": 1.7633333333333334, "no_speech_prob": 2.468075581418816e-05}, {"id": 256, "seek": 147400, "start": 1492.0, "end": 1496.0, "text": " And I think everybody has spoken to watch them at least twice.", "tokens": [400, 286, 519, 2201, 575, 10759, 281, 1159, 552, 412, 1935, 6091, 13], "temperature": 0.0, "avg_logprob": -0.07855206084765977, "compression_ratio": 1.7633333333333334, "no_speech_prob": 2.468075581418816e-05}, {"id": 257, "seek": 147400, "start": 1496.0, "end": 1499.0, "text": " And then, of course, the other thing you've got is the notebooks.", "tokens": [400, 550, 11, 295, 1164, 11, 264, 661, 551, 291, 600, 658, 307, 264, 43782, 13], "temperature": 0.0, "avg_logprob": -0.07855206084765977, "compression_ratio": 1.7633333333333334, "no_speech_prob": 2.468075581418816e-05}, {"id": 258, "seek": 147400, "start": 1499.0, "end": 1503.0, "text": " The notebooks, as you see, have quite a lot of pros in them as well.", "tokens": [440, 43782, 11, 382, 291, 536, 11, 362, 1596, 257, 688, 295, 6267, 294, 552, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.07855206084765977, "compression_ratio": 1.7633333333333334, "no_speech_prob": 2.468075581418816e-05}, {"id": 259, "seek": 150300, "start": 1503.0, "end": 1509.0, "text": " They've got quite a lot of additional detail that we don't necessarily get into in the video lesson.", "tokens": [814, 600, 658, 1596, 257, 688, 295, 4497, 2607, 300, 321, 500, 380, 4725, 483, 666, 294, 264, 960, 6898, 13], "temperature": 0.0, "avg_logprob": -0.061069797067081225, "compression_ratio": 1.6754716981132076, "no_speech_prob": 4.005967275588773e-05}, {"id": 260, "seek": 150300, "start": 1509.0, "end": 1516.0, "text": " But most importantly, as we described, they give you an environment in which you can experiment.", "tokens": [583, 881, 8906, 11, 382, 321, 7619, 11, 436, 976, 291, 364, 2823, 294, 597, 291, 393, 5120, 13], "temperature": 0.0, "avg_logprob": -0.061069797067081225, "compression_ratio": 1.6754716981132076, "no_speech_prob": 4.005967275588773e-05}, {"id": 261, "seek": 150300, "start": 1516.0, "end": 1523.0, "text": " In fact, not only do we suggest that you experiment, we have a very specific suggestion about how to use these notebooks.", "tokens": [682, 1186, 11, 406, 787, 360, 321, 3402, 300, 291, 5120, 11, 321, 362, 257, 588, 2685, 16541, 466, 577, 281, 764, 613, 43782, 13], "temperature": 0.0, "avg_logprob": -0.061069797067081225, "compression_ratio": 1.6754716981132076, "no_speech_prob": 4.005967275588773e-05}, {"id": 262, "seek": 150300, "start": 1523.0, "end": 1530.0, "text": " Yeah, so we recommend that you read through the notebook and then, and this is after you've watched the video at least once.", "tokens": [865, 11, 370, 321, 2748, 300, 291, 1401, 807, 264, 21060, 293, 550, 11, 293, 341, 307, 934, 291, 600, 6337, 264, 960, 412, 1935, 1564, 13], "temperature": 0.0, "avg_logprob": -0.061069797067081225, "compression_ratio": 1.6754716981132076, "no_speech_prob": 4.005967275588773e-05}, {"id": 263, "seek": 153000, "start": 1530.0, "end": 1537.0, "text": " If everything makes sense, put it aside and try creating a new notebook where you go through that process yourself.", "tokens": [759, 1203, 1669, 2020, 11, 829, 309, 7359, 293, 853, 4084, 257, 777, 21060, 689, 291, 352, 807, 300, 1399, 1803, 13], "temperature": 0.0, "avg_logprob": -0.08992651807583445, "compression_ratio": 1.7842465753424657, "no_speech_prob": 8.748039545025676e-05}, {"id": 264, "seek": 153000, "start": 1537.0, "end": 1539.0, "text": " And so this is from scratch, right?", "tokens": [400, 370, 341, 307, 490, 8459, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.08992651807583445, "compression_ratio": 1.7842465753424657, "no_speech_prob": 8.748039545025676e-05}, {"id": 265, "seek": 153000, "start": 1539.0, "end": 1543.0, "text": " This is like creating your own notebook to test that you can actually build it yourself.", "tokens": [639, 307, 411, 4084, 428, 1065, 21060, 281, 1500, 300, 291, 393, 767, 1322, 309, 1803, 13], "temperature": 0.0, "avg_logprob": -0.08992651807583445, "compression_ratio": 1.7842465753424657, "no_speech_prob": 8.748039545025676e-05}, {"id": 266, "seek": 153000, "start": 1543.0, "end": 1548.0, "text": " Yeah, we do not want you to just hit Shift Enter, Shift Enter and run through the existing notebook.", "tokens": [865, 11, 321, 360, 406, 528, 291, 281, 445, 2045, 28304, 10399, 11, 28304, 10399, 293, 1190, 807, 264, 6741, 21060, 13], "temperature": 0.0, "avg_logprob": -0.08992651807583445, "compression_ratio": 1.7842465753424657, "no_speech_prob": 8.748039545025676e-05}, {"id": 267, "seek": 153000, "start": 1548.0, "end": 1554.0, "text": " Because again, the test of whether you know something is can you build and code with it yourself.", "tokens": [1436, 797, 11, 264, 1500, 295, 1968, 291, 458, 746, 307, 393, 291, 1322, 293, 3089, 365, 309, 1803, 13], "temperature": 0.0, "avg_logprob": -0.08992651807583445, "compression_ratio": 1.7842465753424657, "no_speech_prob": 8.748039545025676e-05}, {"id": 268, "seek": 153000, "start": 1554.0, "end": 1558.0, "text": " So if you get stuck, you can always then go back and refer to the class notebook.", "tokens": [407, 498, 291, 483, 5541, 11, 291, 393, 1009, 550, 352, 646, 293, 2864, 281, 264, 1508, 21060, 13], "temperature": 0.0, "avg_logprob": -0.08992651807583445, "compression_ratio": 1.7842465753424657, "no_speech_prob": 8.748039545025676e-05}, {"id": 269, "seek": 155800, "start": 1558.0, "end": 1563.0, "text": " And then rather than copying and pasting it, make sure you do understand what it's saying.", "tokens": [400, 550, 2831, 813, 27976, 293, 1791, 278, 309, 11, 652, 988, 291, 360, 1223, 437, 309, 311, 1566, 13], "temperature": 0.0, "avg_logprob": -0.03923554547065127, "compression_ratio": 1.9041666666666666, "no_speech_prob": 4.68303041998297e-05}, {"id": 270, "seek": 155800, "start": 1563.0, "end": 1570.0, "text": " Maybe look up some documentation about that concept and then put that notebook aside and see if you can now do it yourself.", "tokens": [2704, 574, 493, 512, 14333, 466, 300, 3410, 293, 550, 829, 300, 21060, 7359, 293, 536, 498, 291, 393, 586, 360, 309, 1803, 13], "temperature": 0.0, "avg_logprob": -0.03923554547065127, "compression_ratio": 1.9041666666666666, "no_speech_prob": 4.68303041998297e-05}, {"id": 271, "seek": 155800, "start": 1570.0, "end": 1575.0, "text": " So in a sense, you're plagiarizing a lot from the notebook, but you're plagiarizing in a good way.", "tokens": [407, 294, 257, 2020, 11, 291, 434, 33756, 9448, 3319, 257, 688, 490, 264, 21060, 11, 457, 291, 434, 33756, 9448, 3319, 294, 257, 665, 636, 13], "temperature": 0.0, "avg_logprob": -0.03923554547065127, "compression_ratio": 1.9041666666666666, "no_speech_prob": 4.68303041998297e-05}, {"id": 272, "seek": 155800, "start": 1575.0, "end": 1584.0, "text": " You know, you're plagiarizing not by copying and pasting, but by plagiarizing the concepts and making sure that you can recreate them yourself.", "tokens": [509, 458, 11, 291, 434, 33756, 9448, 3319, 406, 538, 27976, 293, 1791, 278, 11, 457, 538, 33756, 9448, 3319, 264, 10392, 293, 1455, 988, 300, 291, 393, 25833, 552, 1803, 13], "temperature": 0.0, "avg_logprob": -0.03923554547065127, "compression_ratio": 1.9041666666666666, "no_speech_prob": 4.68303041998297e-05}, {"id": 273, "seek": 158400, "start": 1584.0, "end": 1593.0, "text": " And then if you have questions, please ask them. The forums are the first place you should go to first search to see if someone's already asked your question.", "tokens": [400, 550, 498, 291, 362, 1651, 11, 1767, 1029, 552, 13, 440, 26998, 366, 264, 700, 1081, 291, 820, 352, 281, 700, 3164, 281, 536, 498, 1580, 311, 1217, 2351, 428, 1168, 13], "temperature": 0.0, "avg_logprob": -0.0931829298385466, "compression_ratio": 1.6926070038910506, "no_speech_prob": 7.720915891695768e-05}, {"id": 274, "seek": 158400, "start": 1593.0, "end": 1602.0, "text": " As we said earlier, there is a separate thread for each lesson that are already have tons of helpful questions and answers from the students that took our in-person course.", "tokens": [1018, 321, 848, 3071, 11, 456, 307, 257, 4994, 7207, 337, 1184, 6898, 300, 366, 1217, 362, 9131, 295, 4961, 1651, 293, 6338, 490, 264, 1731, 300, 1890, 527, 294, 12, 10813, 1164, 13], "temperature": 0.0, "avg_logprob": -0.0931829298385466, "compression_ratio": 1.6926070038910506, "no_speech_prob": 7.720915891695768e-05}, {"id": 275, "seek": 158400, "start": 1602.0, "end": 1607.0, "text": " In fact, there's a great quote which we talk about in one of the lessons from the head of Google Brain,", "tokens": [682, 1186, 11, 456, 311, 257, 869, 6513, 597, 321, 751, 466, 294, 472, 295, 264, 8820, 490, 264, 1378, 295, 3329, 29783, 11], "temperature": 0.0, "avg_logprob": -0.0931829298385466, "compression_ratio": 1.6926070038910506, "no_speech_prob": 7.720915891695768e-05}, {"id": 276, "seek": 160700, "start": 1607.0, "end": 1615.0, "text": " who says that their rule at Google Brain is that if you have a problem, you first of all try to fix it yourself for half an hour.", "tokens": [567, 1619, 300, 641, 4978, 412, 3329, 29783, 307, 300, 498, 291, 362, 257, 1154, 11, 291, 700, 295, 439, 853, 281, 3191, 309, 1803, 337, 1922, 364, 1773, 13], "temperature": 0.0, "avg_logprob": -0.058614098108731784, "compression_ratio": 1.6935483870967742, "no_speech_prob": 1.618591159058269e-05}, {"id": 277, "seek": 160700, "start": 1615.0, "end": 1620.0, "text": " And if after half an hour you can't fix it yourself, you then have to ask somebody.", "tokens": [400, 498, 934, 1922, 364, 1773, 291, 393, 380, 3191, 309, 1803, 11, 291, 550, 362, 281, 1029, 2618, 13], "temperature": 0.0, "avg_logprob": -0.058614098108731784, "compression_ratio": 1.6935483870967742, "no_speech_prob": 1.618591159058269e-05}, {"id": 278, "seek": 160700, "start": 1620.0, "end": 1627.0, "text": " So that ensures that you always give it a go yourself and hopefully learn from the experience,", "tokens": [407, 300, 28111, 300, 291, 1009, 976, 309, 257, 352, 1803, 293, 4696, 1466, 490, 264, 1752, 11], "temperature": 0.0, "avg_logprob": -0.058614098108731784, "compression_ratio": 1.6935483870967742, "no_speech_prob": 1.618591159058269e-05}, {"id": 279, "seek": 160700, "start": 1627.0, "end": 1631.0, "text": " but you never waste too much time on something which somebody else can help you with.", "tokens": [457, 291, 1128, 5964, 886, 709, 565, 322, 746, 597, 2618, 1646, 393, 854, 291, 365, 13], "temperature": 0.0, "avg_logprob": -0.058614098108731784, "compression_ratio": 1.6935483870967742, "no_speech_prob": 1.618591159058269e-05}, {"id": 280, "seek": 160700, "start": 1631.0, "end": 1634.0, "text": " Yes, that's great advice.", "tokens": [1079, 11, 300, 311, 869, 5192, 13], "temperature": 0.0, "avg_logprob": -0.058614098108731784, "compression_ratio": 1.6935483870967742, "no_speech_prob": 1.618591159058269e-05}, {"id": 281, "seek": 163400, "start": 1634.0, "end": 1637.0, "text": " So as Rachel said, the forums are a really helpful resource.", "tokens": [407, 382, 14246, 848, 11, 264, 26998, 366, 257, 534, 4961, 7684, 13], "temperature": 0.0, "avg_logprob": -0.0801822046438853, "compression_ratio": 1.8363636363636364, "no_speech_prob": 8.88500944711268e-05}, {"id": 282, "seek": 163400, "start": 1637.0, "end": 1641.0, "text": " And when you go to the forums, you'll find that there's a lot of existing discussions.", "tokens": [400, 562, 291, 352, 281, 264, 26998, 11, 291, 603, 915, 300, 456, 311, 257, 688, 295, 6741, 11088, 13], "temperature": 0.0, "avg_logprob": -0.0801822046438853, "compression_ratio": 1.8363636363636364, "no_speech_prob": 8.88500944711268e-05}, {"id": 283, "seek": 163400, "start": 1641.0, "end": 1650.0, "text": " There's a separate discussion for every lesson, for example, and each and each of those discussions, you'll see that there's a summary of the existing discussion at the start.", "tokens": [821, 311, 257, 4994, 5017, 337, 633, 6898, 11, 337, 1365, 11, 293, 1184, 293, 1184, 295, 729, 11088, 11, 291, 603, 536, 300, 456, 311, 257, 12691, 295, 264, 6741, 5017, 412, 264, 722, 13], "temperature": 0.0, "avg_logprob": -0.0801822046438853, "compression_ratio": 1.8363636363636364, "no_speech_prob": 8.88500944711268e-05}, {"id": 284, "seek": 163400, "start": 1650.0, "end": 1655.0, "text": " So you may find that what you need is already in the question and answers there.", "tokens": [407, 291, 815, 915, 300, 437, 291, 643, 307, 1217, 294, 264, 1168, 293, 6338, 456, 13], "temperature": 0.0, "avg_logprob": -0.0801822046438853, "compression_ratio": 1.8363636363636364, "no_speech_prob": 8.88500944711268e-05}, {"id": 285, "seek": 165500, "start": 1655.0, "end": 1668.0, "text": " And if it's not, of course, feel free to add your question and you'll generally find it's responded to within a small number of hours, maybe by Rachel or I or maybe by one of the other students.", "tokens": [400, 498, 309, 311, 406, 11, 295, 1164, 11, 841, 1737, 281, 909, 428, 1168, 293, 291, 603, 5101, 915, 309, 311, 15806, 281, 1951, 257, 1359, 1230, 295, 2496, 11, 1310, 538, 14246, 420, 286, 420, 1310, 538, 472, 295, 264, 661, 1731, 13], "temperature": 0.0, "avg_logprob": -0.07151771901728032, "compression_ratio": 1.6382978723404256, "no_speech_prob": 9.515785677649546e-06}, {"id": 286, "seek": 165500, "start": 1668.0, "end": 1678.0, "text": " The other thing you may find helpful is that each lesson has a timeline on the Wiki and those hyperlinks are actually hyperlinks directly to the part of the video which discusses that topic.", "tokens": [440, 661, 551, 291, 815, 915, 4961, 307, 300, 1184, 6898, 575, 257, 12933, 322, 264, 35892, 293, 729, 9848, 75, 16431, 366, 767, 9848, 75, 16431, 3838, 281, 264, 644, 295, 264, 960, 597, 2248, 279, 300, 4829, 13], "temperature": 0.0, "avg_logprob": -0.07151771901728032, "compression_ratio": 1.6382978723404256, "no_speech_prob": 9.515785677649546e-06}, {"id": 287, "seek": 167800, "start": 1678.0, "end": 1688.0, "text": " So if you're trying to remember how momentum works, you can just click on that link and you'll jump straight to me telling you about momentum.", "tokens": [407, 498, 291, 434, 1382, 281, 1604, 577, 11244, 1985, 11, 291, 393, 445, 2052, 322, 300, 2113, 293, 291, 603, 3012, 2997, 281, 385, 3585, 291, 466, 11244, 13], "temperature": 0.0, "avg_logprob": -0.04314212700755326, "compression_ratio": 1.6377952755905512, "no_speech_prob": 1.1842231288028415e-05}, {"id": 288, "seek": 167800, "start": 1688.0, "end": 1692.0, "text": " As Rachel said, there's also a number of resources available to help you.", "tokens": [1018, 14246, 848, 11, 456, 311, 611, 257, 1230, 295, 3593, 2435, 281, 854, 291, 13], "temperature": 0.0, "avg_logprob": -0.04314212700755326, "compression_ratio": 1.6377952755905512, "no_speech_prob": 1.1842231288028415e-05}, {"id": 289, "seek": 167800, "start": 1692.0, "end": 1695.0, "text": " So this is taken from the front page of our Wiki.", "tokens": [407, 341, 307, 2726, 490, 264, 1868, 3028, 295, 527, 35892, 13], "temperature": 0.0, "avg_logprob": -0.04314212700755326, "compression_ratio": 1.6377952755905512, "no_speech_prob": 1.1842231288028415e-05}, {"id": 290, "seek": 167800, "start": 1695.0, "end": 1702.0, "text": " There's a whole section of tools with links where you can learn about learn more about each of the pieces that we use in the development environment.", "tokens": [821, 311, 257, 1379, 3541, 295, 3873, 365, 6123, 689, 291, 393, 1466, 466, 1466, 544, 466, 1184, 295, 264, 3755, 300, 321, 764, 294, 264, 3250, 2823, 13], "temperature": 0.0, "avg_logprob": -0.04314212700755326, "compression_ratio": 1.6377952755905512, "no_speech_prob": 1.1842231288028415e-05}, {"id": 291, "seek": 170200, "start": 1702.0, "end": 1710.0, "text": " And so our goal here is not to be a single source of truth. If somebody else has already done a great job of teaching one of these tools, we'll leave it to them.", "tokens": [400, 370, 527, 3387, 510, 307, 406, 281, 312, 257, 2167, 4009, 295, 3494, 13, 759, 2618, 1646, 575, 1217, 1096, 257, 869, 1691, 295, 4571, 472, 295, 613, 3873, 11, 321, 603, 1856, 309, 281, 552, 13], "temperature": 0.0, "avg_logprob": -0.0622631768199885, "compression_ratio": 1.7413127413127414, "no_speech_prob": 1.3419062270259019e-05}, {"id": 292, "seek": 170200, "start": 1710.0, "end": 1717.0, "text": " So we don't attempt to give you a great bash reference or a great umpire reference because people have already done that.", "tokens": [407, 321, 500, 380, 5217, 281, 976, 291, 257, 869, 46183, 6408, 420, 257, 869, 1105, 79, 621, 6408, 570, 561, 362, 1217, 1096, 300, 13], "temperature": 0.0, "avg_logprob": -0.0622631768199885, "compression_ratio": 1.7413127413127414, "no_speech_prob": 1.3419062270259019e-05}, {"id": 293, "seek": 170200, "start": 1717.0, "end": 1727.0, "text": " So if you want to learn more about one of these things, jump onto the Wiki, click through here and you'll find some curated resources that we think are really helpful.", "tokens": [407, 498, 291, 528, 281, 1466, 544, 466, 472, 295, 613, 721, 11, 3012, 3911, 264, 35892, 11, 2052, 807, 510, 293, 291, 603, 915, 512, 47851, 3593, 300, 321, 519, 366, 534, 4961, 13], "temperature": 0.0, "avg_logprob": -0.0622631768199885, "compression_ratio": 1.7413127413127414, "no_speech_prob": 1.3419062270259019e-05}, {"id": 294, "seek": 172700, "start": 1727.0, "end": 1734.0, "text": " So this lesson, you're going to cover a lot of stuff, but these are the four things to keep in mind.", "tokens": [407, 341, 6898, 11, 291, 434, 516, 281, 2060, 257, 688, 295, 1507, 11, 457, 613, 366, 264, 1451, 721, 281, 1066, 294, 1575, 13], "temperature": 0.0, "avg_logprob": -0.08201230693067241, "compression_ratio": 1.8387096774193548, "no_speech_prob": 4.984946281183511e-05}, {"id": 295, "seek": 172700, "start": 1734.0, "end": 1743.0, "text": " By the end of the lesson, you want to make sure that you can create an AWS instance, that you can connect to it with SSH, that you can run a Jupyter Notebook in it,", "tokens": [3146, 264, 917, 295, 264, 6898, 11, 291, 528, 281, 652, 988, 300, 291, 393, 1884, 364, 17650, 5197, 11, 300, 291, 393, 1745, 281, 309, 365, 12238, 39, 11, 300, 291, 393, 1190, 257, 22125, 88, 391, 11633, 2939, 294, 309, 11], "temperature": 0.0, "avg_logprob": -0.08201230693067241, "compression_ratio": 1.8387096774193548, "no_speech_prob": 4.984946281183511e-05}, {"id": 296, "seek": 172700, "start": 1743.0, "end": 1748.0, "text": " and that you can run those that state of the art custom model code that we showed you earlier.", "tokens": [293, 300, 291, 393, 1190, 729, 300, 1785, 295, 264, 1523, 2375, 2316, 3089, 300, 321, 4712, 291, 3071, 13], "temperature": 0.0, "avg_logprob": -0.08201230693067241, "compression_ratio": 1.8387096774193548, "no_speech_prob": 4.984946281183511e-05}, {"id": 297, "seek": 172700, "start": 1748.0, "end": 1754.0, "text": " Those first three things, you're going to be doing every single project in every single lesson.", "tokens": [3950, 700, 1045, 721, 11, 291, 434, 516, 281, 312, 884, 633, 2167, 1716, 294, 633, 2167, 6898, 13], "temperature": 0.0, "avg_logprob": -0.08201230693067241, "compression_ratio": 1.8387096774193548, "no_speech_prob": 4.984946281183511e-05}, {"id": 298, "seek": 175400, "start": 1754.0, "end": 1757.0, "text": " So you're going to want to be really comfortable at doing that.", "tokens": [407, 291, 434, 516, 281, 528, 281, 312, 534, 4619, 412, 884, 300, 13], "temperature": 0.0, "avg_logprob": -0.05291541604434743, "compression_ratio": 1.5938864628820961, "no_speech_prob": 2.9308113880688325e-05}, {"id": 299, "seek": 175400, "start": 1757.0, "end": 1765.0, "text": " And for those of you who haven't done that before, it might take you a little while to get the hang of it and maybe a few unsuccessful attempts first.", "tokens": [400, 337, 729, 295, 291, 567, 2378, 380, 1096, 300, 949, 11, 309, 1062, 747, 291, 257, 707, 1339, 281, 483, 264, 3967, 295, 309, 293, 1310, 257, 1326, 46258, 15257, 700, 13], "temperature": 0.0, "avg_logprob": -0.05291541604434743, "compression_ratio": 1.5938864628820961, "no_speech_prob": 2.9308113880688325e-05}, {"id": 300, "seek": 175400, "start": 1765.0, "end": 1774.0, "text": " So this first lesson is unusual in that it's a lot more about kind of getting your development environment set up and not as much about deep learning.", "tokens": [407, 341, 700, 6898, 307, 10901, 294, 300, 309, 311, 257, 688, 544, 466, 733, 295, 1242, 428, 3250, 2823, 992, 493, 293, 406, 382, 709, 466, 2452, 2539, 13], "temperature": 0.0, "avg_logprob": -0.05291541604434743, "compression_ratio": 1.5938864628820961, "no_speech_prob": 2.9308113880688325e-05}, {"id": 301, "seek": 177400, "start": 1774.0, "end": 1784.0, "text": " So indeed, if you've got a background in Python and AWS and Linux, you may find this lesson on the easy side, in which case you can zip through it pretty fast.", "tokens": [407, 6451, 11, 498, 291, 600, 658, 257, 3678, 294, 15329, 293, 17650, 293, 18734, 11, 291, 815, 915, 341, 6898, 322, 264, 1858, 1252, 11, 294, 597, 1389, 291, 393, 20730, 807, 309, 1238, 2370, 13], "temperature": 0.0, "avg_logprob": -0.05284461351198571, "compression_ratio": 1.629496402877698, "no_speech_prob": 1.7777558241505176e-05}, {"id": 302, "seek": 177400, "start": 1784.0, "end": 1788.0, "text": " If you don't have a background in these tools, today's class may seem really overwhelming.", "tokens": [759, 291, 500, 380, 362, 257, 3678, 294, 613, 3873, 11, 965, 311, 1508, 815, 1643, 534, 13373, 13], "temperature": 0.0, "avg_logprob": -0.05284461351198571, "compression_ratio": 1.629496402877698, "no_speech_prob": 1.7777558241505176e-05}, {"id": 303, "seek": 177400, "start": 1788.0, "end": 1794.0, "text": " And we don't want you to be discouraged by that because this is very different from the future lessons,", "tokens": [400, 321, 500, 380, 528, 291, 281, 312, 35010, 538, 300, 570, 341, 307, 588, 819, 490, 264, 2027, 8820, 11], "temperature": 0.0, "avg_logprob": -0.05284461351198571, "compression_ratio": 1.629496402877698, "no_speech_prob": 1.7777558241505176e-05}, {"id": 304, "seek": 177400, "start": 1794.0, "end": 1798.0, "text": " but it's necessary to get your environment set up so that you can be coding throughout the course.", "tokens": [457, 309, 311, 4818, 281, 483, 428, 2823, 992, 493, 370, 300, 291, 393, 312, 17720, 3710, 264, 1164, 13], "temperature": 0.0, "avg_logprob": -0.05284461351198571, "compression_ratio": 1.629496402877698, "no_speech_prob": 1.7777558241505176e-05}, {"id": 305, "seek": 179800, "start": 1798.0, "end": 1807.0, "text": " Yeah, I mean, the folks who didn't have that background in the in-person course, once they actually got through this, and often it was a lot of work and it was pretty tough,", "tokens": [865, 11, 286, 914, 11, 264, 4024, 567, 994, 380, 362, 300, 3678, 294, 264, 294, 12, 10813, 1164, 11, 1564, 436, 767, 658, 807, 341, 11, 293, 2049, 309, 390, 257, 688, 295, 589, 293, 309, 390, 1238, 4930, 11], "temperature": 0.0, "avg_logprob": -0.05775815544398964, "compression_ratio": 1.6953846153846155, "no_speech_prob": 3.9436083170585334e-05}, {"id": 306, "seek": 179800, "start": 1807.0, "end": 1812.0, "text": " but at the end, they finally got the point and they could say, OK, I've set up a GPU instance in the cloud.", "tokens": [457, 412, 264, 917, 11, 436, 2721, 658, 264, 935, 293, 436, 727, 584, 11, 2264, 11, 286, 600, 992, 493, 257, 18407, 5197, 294, 264, 4588, 13], "temperature": 0.0, "avg_logprob": -0.05775815544398964, "compression_ratio": 1.6953846153846155, "no_speech_prob": 3.9436083170585334e-05}, {"id": 307, "seek": 179800, "start": 1812.0, "end": 1818.0, "text": " I've set up my development environment and I have trained from scratch a model that can recognize dogs from cats.", "tokens": [286, 600, 992, 493, 452, 3250, 2823, 293, 286, 362, 8895, 490, 8459, 257, 2316, 300, 393, 5521, 7197, 490, 11111, 13], "temperature": 0.0, "avg_logprob": -0.05775815544398964, "compression_ratio": 1.6953846153846155, "no_speech_prob": 3.9436083170585334e-05}, {"id": 308, "seek": 179800, "start": 1818.0, "end": 1820.0, "text": " And it was very, very exciting.", "tokens": [400, 309, 390, 588, 11, 588, 4670, 13], "temperature": 0.0, "avg_logprob": -0.05775815544398964, "compression_ratio": 1.6953846153846155, "no_speech_prob": 3.9436083170585334e-05}, {"id": 309, "seek": 179800, "start": 1820.0, "end": 1827.0, "text": " So if this is hard work for you, just know that when you get through the other end of it, it's going to be really exciting.", "tokens": [407, 498, 341, 307, 1152, 589, 337, 291, 11, 445, 458, 300, 562, 291, 483, 807, 264, 661, 917, 295, 309, 11, 309, 311, 516, 281, 312, 534, 4670, 13], "temperature": 0.0, "avg_logprob": -0.05775815544398964, "compression_ratio": 1.6953846153846155, "no_speech_prob": 3.9436083170585334e-05}, {"id": 310, "seek": 182700, "start": 1827.0, "end": 1837.0, "text": " So I asked that everyone who's trying to decide if this course is for them, try at least the first two lessons, since the first lesson is so much about setup.", "tokens": [407, 286, 2351, 300, 1518, 567, 311, 1382, 281, 4536, 498, 341, 1164, 307, 337, 552, 11, 853, 412, 1935, 264, 700, 732, 8820, 11, 1670, 264, 700, 6898, 307, 370, 709, 466, 8657, 13], "temperature": 0.0, "avg_logprob": -0.08133810469247763, "compression_ratio": 1.7083333333333333, "no_speech_prob": 6.501324969576672e-05}, {"id": 311, "seek": 182700, "start": 1837.0, "end": 1842.0, "text": " Yeah, these lessons become obviously we build more and more on the techniques we've learned.", "tokens": [865, 11, 613, 8820, 1813, 2745, 321, 1322, 544, 293, 544, 322, 264, 7512, 321, 600, 3264, 13], "temperature": 0.0, "avg_logprob": -0.08133810469247763, "compression_ratio": 1.7083333333333333, "no_speech_prob": 6.501324969576672e-05}, {"id": 312, "seek": 182700, "start": 1842.0, "end": 1845.0, "text": " And so we're going to be using this infrastructure in every lesson.", "tokens": [400, 370, 321, 434, 516, 281, 312, 1228, 341, 6896, 294, 633, 6898, 13], "temperature": 0.0, "avg_logprob": -0.08133810469247763, "compression_ratio": 1.7083333333333333, "no_speech_prob": 6.501324969576672e-05}, {"id": 313, "seek": 182700, "start": 1845.0, "end": 1853.0, "text": " By the time we get to lesson seven, we're going to be looking at some pretty sophisticated and custom neural network architectures.", "tokens": [3146, 264, 565, 321, 483, 281, 6898, 3407, 11, 321, 434, 516, 281, 312, 1237, 412, 512, 1238, 16950, 293, 2375, 18161, 3209, 6331, 1303, 13], "temperature": 0.0, "avg_logprob": -0.08133810469247763, "compression_ratio": 1.7083333333333333, "no_speech_prob": 6.501324969576672e-05}, {"id": 314, "seek": 185300, "start": 1853.0, "end": 1857.0, "text": " We're going to cover every different type of SGD optimization.", "tokens": [492, 434, 516, 281, 2060, 633, 819, 2010, 295, 34520, 35, 19618, 13], "temperature": 0.0, "avg_logprob": -0.07449804411994086, "compression_ratio": 1.5958549222797926, "no_speech_prob": 2.3503380361944437e-05}, {"id": 315, "seek": 185300, "start": 1857.0, "end": 1861.0, "text": " We're going to be covering convolutional neural networks and recurrent neural networks.", "tokens": [492, 434, 516, 281, 312, 10322, 45216, 304, 18161, 9590, 293, 18680, 1753, 18161, 9590, 13], "temperature": 0.0, "avg_logprob": -0.07449804411994086, "compression_ratio": 1.5958549222797926, "no_speech_prob": 2.3503380361944437e-05}, {"id": 316, "seek": 185300, "start": 1861.0, "end": 1863.0, "text": " So there's going to be a lot of exciting stuff.", "tokens": [407, 456, 311, 516, 281, 312, 257, 688, 295, 4670, 1507, 13], "temperature": 0.0, "avg_logprob": -0.07449804411994086, "compression_ratio": 1.5958549222797926, "no_speech_prob": 2.3503380361944437e-05}, {"id": 317, "seek": 186300, "start": 1863.0, "end": 1884.0, "text": " And yeah, we really look forward to seeing you on the forums and good luck with learning about deep learning.", "tokens": [50364, 400, 1338, 11, 321, 534, 574, 2128, 281, 2577, 291, 322, 264, 26998, 293, 665, 3668, 365, 2539, 466, 2452, 2539, 13, 51414], "temperature": 0.0, "avg_logprob": -0.11383344650268555, "compression_ratio": 1.2247191011235956, "no_speech_prob": 8.638753570267e-06}], "language": "en"}