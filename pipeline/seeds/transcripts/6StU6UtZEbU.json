{"text": " Hi everybody and welcome back. This is lesson 10 of Practical Deep Learning for Coders. It's the second lesson in part two, which is where we're going from deep learning foundations to stable diffusion. So before we dive back into our notebook, I think first of all let's take a look at some of the interesting work that students in the course have done over the last week. I'm just going to show a small sample of what's on the forum, so check out the share your work here thread on the forum for many, many, many more examples. So Puro did something interesting, which is to create a bunch of images of doing a linear interpolation. I mean details actually spherical linear interpolation, but doesn't matter. Doing a linear interpolation between two different latent, you know, noisy, you know, latent noise starting points for an otter picture and then showed all the intermediate results. That came out pretty nice. And then did something similar starting with an old car prompt and going to a modern Ferrari prompt. I can't remember exactly what the prompts were, but you can see how as it kind of goes through that latent space, it actually is changing the image that's coming out. I think that's really cool. And then I love the way Namrata took that and took it to another level in a way, which is starting with a dinosaur and turning into a bird. And this is a very cool intermediate picture of one of the steps along the way, the dino bird. I love it. Dino chick. Fantastic. So much creativity on the forums. I loved this. John Richmond took his daughter's dog and turned it gradually into a unicorn. And I thought this one along the way actually came out very, very nicely. I think this is adorable. And I suspect that John has won the Dad of the Year, or Dad of the Week maybe award this week for this fantastic project. And Maureen did something very interesting, which is she took Jono's parrot image from his lesson and tried bringing it across to various different painter's dials. And so her question was, anyone want to guess the artists in the prompts? So I'm just going to let you pause it before I move on, if you want to try to guess. And there they are. Most of them pretty obvious, I guess. I think it's so funny that Frida Kahlo appears in all of her paintings. So the parrot's actually turned into Frida Kahlo. All right, not all of her paintings, but all of her famous ones. So the very idea of a Frida Kahlo painting without her in it is so unheard of that the parrot's turned into a Frida Kahlo. And I like this Jackson Pollock. It's still got the parrot going on there. So that's a really lovely one, Maureen. Thank you. And this is a good reminder to make sure that you check out the other two lesson videos. So she was working with Jono's Stable Diffusion lesson. So be sure to check that out if you haven't yet. It is available on the course webpage and on the forums and has lots of cool stuff that you can work with, including this parrot. And then the other one to remind you about is the video that Waseem and Tanishk did on the Math of Diffusion. And I do want to read out what Alex said about this, because I'm sure a number of you feel the same way. My first reaction on seeing something with the title Math of Diffusion was to assume that, oh, that's just something for all the smart people who have PhDs in maths on the course, and it'll probably be completely incomprehensible. But of course, it's not that at all. So be sure to check this out even if you don't think of yourself as a math person. I think it's, you know, some nice background that you may find useful. It's certainly not necessary, but you might, yeah, I think it's kind of useful to start to dig in some to some of the math at this point. One particularly interesting project that's been happening during the week is from Jason Antich, who is a bit of a legend around here. Many of you will remember him as being the guy that created Deldify and actually worked closely with us on our research, which together turned into Nogan and Decrapify and other things, created lots of papers. And Jason has kindly joined our middle research team, working on the stuff for these lessons and for developing a kind of fast AI approach to stable diffusion. And he took the idea that I prompted last week, which is maybe we should be using classic optimizers rather than differential equation solvers. And he actually made it work incredibly well already within a week. These faces were generated on a single GPU in a few hours from scratch by using classic deep learning optimizers, which is like an unheard of speed to get this quality of image. And we think that this research direction is looking extremely, extremely promising. So really great news there and thank you, Jason, for this fantastic progress. Yeah, so maybe we'll do a quick reminder of what we looked at last week. So last week I used a bit of a mega one note hand drawn thing. I thought this week I might just turn it into some slides that we can use. So the basic idea, if you remember, is that we started with, if we're doing handwritten digits, for example, we'd start with a number seven. This would be one of the ones with a stroke through it that some countries use. And then we add to it some noise and the seven plus the noise together would equal this noisy seven. And so what we then do is we present this noisy seven as an input to a unit and we have it try to predict which pixels are noise, basically, or predict the noise. And so the unit tries to predict the noise from the number. It then compares its prediction to the actual noise and it's going to then get a loss, which it can use to update the weights in the unit. And that's basically how stable diffusion, the main bit, if you like, the unit is created. To make it easier for the unit, we can also pass in an embedding of the actual digit, the actual number seven. So for example, a one hot encoded vector, which goes through an embedding layer. And the nice thing about that to remind you is that if we do this, then we also have the benefit that then later on we can actually generate specific digits by saying I want a number seven or I want a number five and it knows what they look like. I've skipped over here the VAE latency piece, which we talked about last week. And to remind you, that's just a computational shortcut. It makes it faster. And so we don't need to include that in this picture because it's just a computational shortcut that we can pre-process things into that latent space with the VAE first, if we wish. So that's what the unit does. Now then to remind you, you know, we want to handle things that are more interesting than just the number seven. We want to actually handle things where we can say, for example, a graceful swan or a scene from Hitchcock. And the way we do that is we turn these sentences into embeddings as well. And we turn them into embeddings by trying to create embeddings of these sentences, which are as similar as possible to embeddings of the photos or images that they are connected with. And to remind you, the way we did that, or the way that was done originally as part of this thing called CLIP, was to basically download from the internet lots of examples of, lots of images, find their alt tags, and then for each one we then have their image and its alt tag. So here's the graceful swan and its alt tag. And then we build two models, an image encoder that turns each image into some feature vector. And then we have a text encoder that turns each piece of text into a bunch of features. And then we create a loss function that says that the features for a graceful swan, the text, should be as close as possible to the features for the picture of a graceful swan. And specifically we take the dot product. And then we add up all the green ones, because these are the ones that we want to match, and we subtract all the red ones, because those are ones we don't want to match. Those are where the text doesn't match the image. And so that's the contrastive lost, which gives us the CL in CLIP. So that's a review of some stuff we did last week. And so with this then we can, we now have a text encoder, which we can now say a graceful swan, and it will spit out some embeddings. And those are the embeddings that we can feed into our UNet during training. And so then we don't, haven't been doing any of that training ourselves, except for some fine tuning, because it takes a very long time on a lot of computers. But instead we take pre-trained models and do inference. And the way we do inference is we put in an example of the thing that we want, that we have an embedding for. So let's say we're doing handwritten digits, and we put in some random noise into the UNet, and then it spits out a prediction of which bits of noise you could remove to leave behind a picture of the number three. Initially it's going to do quite a bad job of that, so we subtract just a little bit of that noise from the image to make it a little bit less noisy, and we do it again, and we do it a bunch of times. So here's what that looks like. Creating a, I think somebody here did a smiling picture of Jeremy Howard or something, if I remember correctly. And if we print out the noise at kind of step zero, and at step six, and at step 12, you can see the first signs of a face starting to appear. Definitely a face appearing here, 18, 24. By step 30 it's looking much more like a face. By 42 it's getting there. It's just got a few little blemishes to fix up, and here we are. I think I've slightly messed up my indexes here because it should finish at 60, not 54, but such is life. So rather rosy red lips too, I would have to say. So remember in the early days this took a thousand steps, and now there are some shortcuts to make it take 60 steps, and this is what the process looks like. And the reason this doesn't look like normal noise is because now we are actually doing the VAE latency thing. And so noisy latents don't look like Gaussian noise, they look like, well, they look like this. This is what happens when you decode those noisy latents. Now you might remember last week I complained that things are moving too quickly, and there was a couple of papers that had come out the day before and made everything entirely out of date. So Jono and I and the team have actually now had time to read those papers, and I thought now would be a good time to start going through some papers for the first time. So the, what we're actually going to do is show how these papers have taken the required number of steps to go through this process, down from 60 steps to four steps, which is pretty amazing. So let's talk about that. And the paper specifically is this one progressive distillation for fast sampling of diffusion models. So it's only been a week, so I haven't had much of a chance to try to explain this before, so apologies in advance if this is awkward, but hopefully it's going to make some sense. What we're going to start with is, so we're going to start with this process, which is gradually denoising images, and actually I wonder if we can copy it. Okay, so how are we going to get this down from 60 steps to four steps? The basic idea is that we're going to do a process, we're going to do a process called distillation, which I have no idea how to spell, but hopefully that's close enough that you get the idea. Distillation is a process which is pretty common in deep learning, and the basic idea of distillation is that you take something called a teacher network, which is some neural network that already knows how to do something, but it might be slow and big. And the teacher network is then used by a student network, which tries to learn how to do the same thing, but faster or with less memory. And in this case, we want ours to be faster, we want to do less steps. And the way we can do this conceptually, it's actually, in my opinion, reasonably straightforward. We have, like when I look at this and I think like, wow, you know, neural nets are really amazing. So given neural nets are really amazing, why is it taking like 18 steps to go from there to there? Like that seems like something that you should be able to do in one step. The fact that it's taking 18 steps, and originally, of course, that was hundreds and hundreds of steps, is because it's kind of, that's just a kind of a side effect of the math of how this thing was originally developed, you know, this idea of this diffusion process. But the idea in this paper is something that actually we've, I think I might have even have mentioned in the last lesson, it's something we were thinking of doing ourselves before this paper beat us to it, which is to say, well, what if we train a new model where the model takes as input this image, right, and puts it through some other unit, unit B. Okay. And then that spits out some result. And what we do is we take that result and we compare it to this image, the thing we actually want. Because the nice thing is now, which we've never really had before, is we have for each intermediate output, like the desired goal, where we're trying to get to. And so we could compare those two just using, you know, whatever, mean squared error. Keep on forgetting to change my pen. Mean squared error. And so then if we keep doing this for lots and lots of images and lots and lots of pairs and exactly this way, this unit is going to hopefully learn to take these incomplete images and turn them into complete images. And that is exactly what this paper does. It just says, okay, now that we've got all these examples of showing what step 36 should turn into at step 54, let's just feed those examples into a model. And that works. And you'd kind of expect it to work because you can see that like a human would be able to look at this. And if they were a competent artist, they could turn that into a, you know, a well finished product. So you would expect that a computer could as well. There are some little tweaks around how it makes this work, which I'll briefly describe, because we need to be able to go from kind of step one through to step 10 through to step 20 and so forth. And so the way that it does this, it's actually quite clever. What they do is they initially, so they take their teacher model. So remember the teacher model is one that has already been trained. Okay. So the teacher model already is a complete stable diffusion model. That's finished. We take that as a given and we put in our image. Well, actually it's noise. We put in our noise, right? And we put it through two time steps. Okay. And then we train our unit B or whatever you want to call it to try to go directly from the noise to time step number two. And that's pretty easy for it to do. And so then what they do is they take this, okay. And so this thing here, remember is called the student model. They then say, okay, let's now take that student model and treat that as the new teacher. So they now take their noise and they run it through the student model twice, once and twice, and they get out something at the end. And so then they try to create a new student, which is a copy of the previous student. And it learns to go directly from the noise to two goes of the student model. And you won't be surprised to hear they now take that new student model and use that to go two goes. And then they learn, they use that, then they copy that to become the next student model. And so they're doing it again and again and again. And each time they're basically doubling the amount of work. So it goes one to two, effectively it's then going two to four and then four to eight. And that's basically what they're doing. And they're doing it for multiple different time steps. So this single student model is learning to both do these initial steps, trying to jump multiple steps at a time. And it's also learning to do these later steps, multiple steps at a time. And that's it, believe it or not. So this is this neat paper that came out last week. And that's how it works. Now I mentioned that there was actually two papers. The second one is called On Distillation of Guided Diffusion Models. And the trick now is this second paper, and these came out at basically the same time, if I remember correctly, even though they build on each other from the same teams, is that they say, okay, this is all very well, but we don't just want to create random pictures. We want to be able to do guidance, right? And you might remember, I hope you remember from last week, that we used something called Classifier-Free Guided Diffusion Models, which because I'm lazy, we will just use an acronym, Classifier-Free Guided Diffusion Models. And this one, you may recall, we take, let's say we want a cute puppy. We put in the prompt cute puppy into our clip text encoder, and it spits out an embedding. And we put that, let's ignore the VAE Latents business, we put that into our UNET. But we also put the empty prompt into our clip text encoder. We concatenate these things two together, so that then out the other side, we get back two things. We get back the image of the cute puppy, and we get back the image of some arbitrary thing. Could be anything. And then we effectively do something very much like taking the weighted average of these two things together, combine them, and then we use that for the next stage of our diffusion process. Now, what this paper does is it says, this is all pretty awkward. We end up having to train two images instead of one. And for different types of levels of guided diffusion, we have to like do it multiple different times. That's all pretty annoying. How do we skip it? And based on the description of how we did it before, you may be able to guess. What we do is we do exactly the same student-teacher distillation we did before, but this time we pass in, in addition, the guidance. And so again, we've got the entire stable diffusion model, the teacher model available for us. And we are doing actual CFGD, classifier-free guided diffusion, to create our guided diffusion cute puppy pictures. And we're doing it for a range of different guidance scales. So you might be doing two and 7.5 and 12 and whatever. And those now are becoming inputs to our student model. So the student model now has additional inputs. It's getting the noise, as always. It's getting the caption or the prompt, I guess I should say, as always. But it's now also getting the guidance scale. And so it's learning to find out how all of these things are handled by the teacher model. Like what does it do after a few steps each time? So it's exactly the same thing as before, but now it's learning to use the classifier-free guided diffusion as well. Okay, so that's got quite a lot going on there. And if it's a bit confusing, that's okay. It is a bit confusing. And what I would recommend is you check out the extra information from Gelo, who has a whole video on this. And one of the cool things actually about this video is it's actually a paper walkthrough. And so part of this course is hopefully we're going to start reading papers together. Reading papers is extremely intimidating and overwhelming for all of us all of the time. At least for me, it never gets any better. There's a lot of math. And by watching somebody like Jono, who's an expert at this stuff, read through a paper, you'll kind of get a sense of how he is skipping over lots of the math, right, to focus on, in this case, the really important thing, which is the actual algorithm. And when you actually look at the algorithm, you start to realize it's basically all stuff, nearly all stuff, maybe all stuff that you did in primary school or secondary school. So we've got division. Okay, sampling from a normal distribution, so high score. Subtraction, division, division, multiplication, right. Oh, okay, we've got a log there. But basically, you know, there's not too much going on. And then when you look at the code, you'll find it, you know, once you turn this into code, of course, it becomes even more understandable if you're somebody who's more familiar with code like me. So yeah, definitely check out Jono's video on this. So another paper came out about three hours ago. And I just had to show it to you because I think it's amazing. And so this is definitely the first video about this paper because it only came out a few hours ago. But check this out. This is a paper called iMagic. And with this algorithm, you can pass in an input image. This is just a, you know, a photo you've taken or downloaded off the internet. And then you pass in some text saying, a bird spreading wings. And what it's going to try to do is it's going to try to take this exact bird in this exact pose and leave everything as similar as possible, but adjust it just enough so that the prompt is now matched. So here we take this, this little guy here, and we say, oh, this is actually what we want this to be a person giving the thumbs up. And this is what it produces. And you can see everything else is very, very similar to the previous picture. So this dog is not sitting, but if we put in the prompt, a sitting dog, it turns it into a sitting dog, leaving everything else as similar as possible. So here's an example of a waterfall. And then you say it's a children's drawing of a waterfall and now it's become a children's drawing. So lots of people in the YouTube chat going, oh my God, this is amazing, which it absolutely is. And that's why we're going to show you how it works. And one of the really amazing things is you're going to realize that you understand how it works already. Just to show you some more examples, here's the dog image. Here's the sitting dog, the jumping dog, dog playing with a toy, jumping dog holding a frisbee. Okay. And here's this guy again, giving the thumbs up, crossed arms, in a greeting pose to Namaste hands, holding a cup. So that's pretty amazing. So I had to show you how this works. And I'm not going to go into too much detail, but I think we can get the idea actually pretty well. So what we do is again, we take, we start with a fully pre-trained ready to go generative model like a stable diffusion model. And this is what this is talking about here, pre-trained diffusion model. In the paper, they actually use a model called Imagen, but none of the details as far as I can see in any way depend on what the model is. It should work just fine for stable diffusion. And we take a photo of a bird spreading wings. Okay. So that's our target. And we create an embedding from that using, for example, our clip encoder as usual. And we then pass it through our pre-trained diffusion model. And we then see what it creates. And it doesn't create something that's actually like our bird. So then what they do is they fine tune this embedding. So this is kind of like textual inversion. They fine tune the embedding to try to make the diffusion model output something that's as similar as possible to the, to the input image. And so you can see here, they're saying, oh, we're moving our embedding a little bit. They don't do this for very long. They just want to move it a little bit in the right direction. And then now they lock that in place and they say, okay, now let's fine tune the entire diffusion model end to end, including the VAE. Or actually with Image N they have a super resolution model, but same idea. So we fine tune the entire model end to end. And now the embedding, this optimized embedding we created, we store in place. We don't change that at all. That's now frozen. And we try to make it so that the diffusion model now spits out our bird as close as possible. So you fine tune that for a few epochs. And so you've now got something that takes this, this embedding that we fine tuned, goes through a fine tune model and spits out our bird. And then finally, the original target embedding we actually wanted is a photo of a bird spreading its wings. We ended up with this slightly different embedding and we take the weighted average of the two. That's called the interpolate step, the weighted average of the two. And we pass that through this fine tune diffusion model and we're done. And so that's pretty amazing. This, this would not take, I don't think a particularly long time or require any particular special hardware. It's the kind of thing I expect people will be doing in the coming days and weeks. But it's very interesting because, yeah, I mean the ability to take any photo of a person or whatever and change it, like literally change what the person's doing is, you know, societally very important and really means that anybody, I guess now can generate believable photos that never actually existed. I see Jono in the chat saying that took about eight minutes to do it for Imogen on TPUs. Although Imogen is quite a slow, big model, although the TPUs they used were very, the latest TPUs. So might be, you know, maybe it's an hour or something for stable diffusion on GPUs. All right. So that is a lot of fun. All right. So with that, let's go back to our notebook. Where we left it last time, we had kind of looked at some applications that we can play with in this diffusion NBs repo in the stable diffusion notebook. And what we've got now, and remind you when I say we, it's mainly actually Pedro, Patrick and Suraj, just a little bit of help from me. So hugging face folks. What we slash they have done is they're starting, is they now dig into the pipeline to pull it all apart step by step. So you can see exactly what happens. The first thing I was just going to mention is this is how you can create those gradual denoising pictures. And this is thanks to something called the callback. So you can say here, when you go through the pipeline, every 12 steps call this function. And as you can see, it's going to call it with I and T and the latency. And so then we can just make an image and stick it on the end of a array. And that's all that's happening here. Right. So this is how you can start to interact with a pipeline without rewriting it yourself from scratch. But now what we're going to do is we're actually going to write it, we're going to build it from scratch. So you don't actually have to use a callback because you'll be able to change it yourself. So let's take a look. So looking inside the pipeline, what exactly is going on? So what's going to be going on in the pipeline is seeing all of the steps that we saw in last week's OneNote notes that I drew. And it's going to be all the code. And we're not going to show the code of how each step is implemented. So for example, the clip text model we talked about, the thing that takes as input a prompt and creates an embedding, we just take that as a given. Okay. So we download it. OpenAI has trained one called ClipVIT Large Patch 14. So we just say fromdrychained. So Hugging Face will transform us, we'll download and create that model for us. Ditto for the tokenizer. And so ditto for the autoencoder and ditto for the UNet. So there they all are. We can just grab them. So we just take that all as a given. These are the three models that we've talked about. The text encoder, the clip encoder, the VAE and the UNet. So there they are. So given that we now have those, the next thing we need is that thing that converts time steps into the amount of noise. Remember that graph we drew? And so we can basically again use something that Hugging Face, well actually in this case, Katherine Carlson has already provided which is a scheduler. It's basically something that shows us that connection. So we've got that. So we use that scheduler and we say how much noise we're using. And so we have to make sure that that matches. And so we just use these numbers that we're given. Okay, so now to create our photograph of an astronaut riding a horse again in 70 steps with a 7.5 guidance scale, batch size of one. Step number one is to take our prompt and tokenize it. Okay, so we looked at that in part one of the course. So check that out if you can't remember what tokenizing does, but it's just splitting it in, basically it's splitting it into words or subword units if they're long and unusual words. So here are, so this will be the start of sentence token and this will be our photograph of an astronaut, etc. And then you can see the same token is repeated again and again at the end. That's just the padding to say we're all done. And the reason for that is that GPUs and TPUs really like to do lots of things at once. So we kind of have everything be the same length by padding them. That may sound like a lot of wasted work, which it kind of is, but a GPU would rather do lots of things at the same time on exactly the same sized input. So this is why we have all this padding. So you can see here if we decode that number, it's the end of text marker, just padding really in this case. As well as getting the input IDs, so these are just lookups into a vocabulary. There's also a mask, which is just telling it which one's actual words as opposed to padding, which is not very interesting. So we can now take those input IDs, we can put them on the GPU and we can run them through the Clip Encoder. And so for a batch size of one, so you've got one image, that gives us back a 77 by 768 because we've got 77 here and each one of those creates a 768 long vector. So we've got a 77 by 768 tensor. So these are the embeddings for a photograph of an astronaut riding a horse that come from Clip. So remember everything's pre-trained, so that's all done for us, we're just doing inference. And so remember for the classifier free guidance, we also need the embeddings for the empty string, so we do exactly the same thing. So now we just concatenate those two together because we're just, this is just a trick to get the GPU to do both at the same time because we like the GPU to do as many things at once as possible. And so now we create our noise. And because we're doing it with a VAE, we can call it Latence, but it's just noise really. I wonder if you'd still call it that without the VAE. Maybe you would have to think about that. So that's just random numbers, normally generated, normally distributed random numbers of size one, that's our batch size. And the reason that we've got this divided by eight here is because that's what the VAE does. It allows us to create things that are eight times smaller by height and width, and then it's going to expand it up again for us later. That's why this is so much faster. You'll see a lot of this, after we put it on the GPU, you'll see a lot of this dot half. This is converting things into what's called half precision or FB16. Details don't matter too much. It's just making it half as big in memory by using less precision. Modern GPUs are much, much, much, much faster if we do that. So you'll see that a lot. If you use something like fast AI, you don't have to worry about it. All this stuff is done for you. And we'll see that later as we rebuild this with much, much less code later in the course. So we'll be building our own kind of framework from scratch, which you'll then be able to maintain and work with yourself. Okay. So we have to say we want to do 70 steps. Something that's very important, we won't worry too much about the details right now, but this, what you see here is that we take our round of noise and we scale it. And that's because depending on what stage you're up to, you need to make sure that kind of you have the right amount of variance basically. Otherwise you're going to get activations and gradients that go out of control. This is something we're going to be talking about a huge amount during this course, and we'll show you lots of tricks to handle that kind of thing automatically. Unfortunately, at the moment in the stable diffusion world, this is all done in rather, in my opinion, kind of ways that are too tied to the details of the model. I think we will be able to improve it as the course goes on. But for now we'll stick with how everybody else is doing it. This is how they do it. So we're going to be jumping through. So normally it would take a thousand time steps, but because we're using a fancy scheduler, we get to skip from 999 to 984, 984 to 970 and so forth. So we're going down about 14 time steps. And remember, this is a very, very, very unfortunate word. They're not time steps at all. In fact, they're not even integers. It's just a measure of how much noise are we adding at each time. And you find out how much noise by looking it up on this graph. Okay. That's all time step means. It's not a step of time. And it's a real shame that that word is used because it's incredibly confusing. This is much more helpful. This is the actual amount of noise at each one of those iterations. And so here you can see the amount of noise for each of those time steps. And we're going to be going backwards, as you can see, we start at 999. So we'll start with lots of noise and then we'll be using less and less and less and less noise. So we go through the 70 time steps in a for loop, concatenating our two noise bits together because we've got the classifier free and the prompt versions. Do our scaling, calculate our predictions from the unit. And notice here we're passing in the time step as well as our prompt. That's going to return two things, the unconditional prediction. So that's the one for the empty string. Remember we passed in, one of the two things we passed in was the empty string. So we concatenated them together. And so after they come out of the unit, we can pull them apart again. So dot chunk just means pull them apart into two separate variables. And then we can do the guidance scale that we talked about last week. And so now we can do that update where we take a little bit of the noise and remove it to give us our new latency. So that's the loop. And so at the end of all that, we decode it in the VAE. The paper that created this VAE tells us that we have to divide it by this number to scale it correctly. And once we've done that, that gives us a number which is between negative one and one. Python imaging library expects something between zero and one. So that's what we do here to make it between zero and one, like enforce that to be true. Put that back on the CPU, make sure it's that the order of the dimensions is the same as what Python imaging library expects. And then finally convert it up to between zero and 255 as an int, which is actually what PIO really wants. And there's our picture. So there's all the steps. So what I then did, this is kind of like, so the way I normally build code, I use notebooks for everything, is I kind of do things step by step by step. And then I tend to kind of copy them and I use shift M. I don't know if you've seen that, but what shift M does, it takes two cells and combines them like that. And so I basically combined some of the cells together and I removed a bunch of the pros. So you can see the entire thing on one screen. And what I was trying to do here is I'd like to get to the point where I've got something which I can very quickly do experiments with. So maybe I want to try some different approach to guidance tree classification. Maybe I want to add some callbacks, so on and so forth. So I kind of like to have everything, you know, I like to have all of my important code be able to fit into my screen at once. And so you can see now I do, I've got the whole thing on my screen so I can keep it all in my head. One thing I was playing around with was I was trying to understand the actual guidance tree equation in terms of like, how does it work? Computer scientists tend to write things in software engineers with kind of long words as variable names. Mathematicians tend to use short words, just letters normally. For me, when I want to play around with stuff like that, I turn stuff back into letters. And that's because I actually kind of pulled out one note and I started jutting down this equation and playing around with it to understand how it behaves. So this is just like, it's not better or worse, it's just depending on what you're doing. So actually here I said, okay, G is guidance scale. And then rather than having the unconditional and text embeddings, I just call them U and T. And now I've got this all down into an equation which I can write down in a notebook and play with and understand exactly how it works. So that's something I find really helpful for working with this kind of code is to turn it into a form that I can manipulate algebraically more easily. I also try to make it look as much like the paper that I'm implementing as possible. Anywho, that's that code. So then I copied all this again and I basically, oh, I actually did it for two prompts this time. I thought this was fun. Oil painting of an astronaut riding a horse in the style of Grant Wood. Just to remind you, Grant Wood looks like this. Not obviously astronaut material, which I thought would make it actually kind of particularly interesting. Although he does have horses. I can't see one here. Some of his pictures have horses. So because I did two prompts, I got back two pictures I could do. So here's the Grant Wood one. I don't know what's going on in his back here, but I think it's quite nice. So yeah, I then copied that whole thing again and merged them all together and then just put it into a function. So I took the little bit which creates an image and put that into a function. I took the bit which does the tokenizing and text encoding and put that into a function. And so now all of the code necessary to do the whole thing from top to bottom fits in these two cells, which makes it for me much easier to see exactly what's going on. So you can see I've got the text embeddings. I've got the unconditional embeddings. I've got the embeddings which concatenate the two together. Optional random seed. My latency. And then the loop itself. And you'll also see something I do which is a bit different to a lot of software engineering is I often create things which are kind of like longer lines because I try to have each line be kind of like mathematically one thing that I want to be able to think about as a whole. So yeah, these are some differences between kind of the way I find numerical programming works well compared to the way I would write a more traditional software engineering approach. And again, this is partly a personal preference, but it's something I find works well for me. So we're now at a point where we've got three functions that easily fit on the screen and do everything. So I can now just say make samples and display each image. And so this is something for you to experiment with. And what I specifically suggest as homework is to try picking one of the extra tricks we learned about like image to image or negative prompts. Negative prompts would be a nice easy one. Like see if you can implement negative prompt in your version of this. Or yeah, try doing image to image. That wouldn't be too hard either. Another one you can add is try adding callbacks. And the nice thing is then, you know, you've got code which you fully understand because you know what all the lines do. And you then don't need to wait for the diffusers folks to update it. The library to do, for example, the callbacks are only added like a week ago. So until then you couldn't do callbacks. Well, now you don't have to wait for the diffusers team to add something. The code's all here for you to play with. So that's my recommendation as a bit of homework for this week. Okay, so that brings us to the end of our rapid overview of stable diffusion and some very recent papers that very significantly develop stable diffusion. I hope that's given you a good sense of the kind of very high level slightly hand wavy version of all this and you can actually get started playing with some fun code. What we're going to be doing next is going right back to the start, learning how to multiply two matrices together effectively, and then gradually building from there until we've got to the point that we've rebuilt all this from scratch and we understand why things work the way they do, understand how to debug problems, improve performance, and implement new research papers as well. So that's going to be very exciting and so we're going to have a break and I will see you back here in 10 minutes. Okay, welcome back everybody. I'm really excited about the next part of this. It's going to require some serious tenacity and a certain amount of patience, but I think you're going to learn a lot. A lot of folks I've spoken to have said that previous iterations of this part of the course is like the best course they've ever done and this one's going to be dramatically better than any previous version we've done of this. So hopefully you'll find that the hard work and patience pays off. We're working now through the course 22 P2 repo, so 2022 course part two, and the notebooks are ordered. So we'll start with notebook number one. And okay, so the goal is to get to stable diffusion from the foundations, which means we have to define what are the foundations. So I've decided to define them as follows. We're allowed to use Python. We're allowed to use the Python standard library. So that's all the stuff that comes with Python by default. We're allowed to use Matplotlib because I couldn't be bothered creating my own plotting library. And we're allowed to use Jupyter Notebooks and NBdev, which is something that creates modules from notebooks. So basically what we're going to try to do is to rebuild everything starting from this foundation. Now to be clear, what we are allowed to use are the libraries once we have re-implemented them correctly. And so if we re-implement something from NumPy or from PyTorch or whatever, we're then allowed to use the NumPy or PyTorch or whatever version. Sometimes we'll be creating things that haven't been created before, and that's then going to be becoming our own library. And we're going to be calling that library MiniAI. So we're going to be building our own little framework as we go. So for example, here are some imports. And these imports all come from the Python standard library except for these two. Now to be clear, one challenge we have is that the models we use in stable diffusion were trained on millions of dollars worth of equipment for months, which we don't have the time or money. So another trick we're going to do is we're going to create identical but smaller versions of them. And so once we've got them working, we'll then be allowed to use the big pre-trained versions. So that's the basic idea. So we're going to have to end up with our own VAE, our own UNET, our own Clip Encoder, and so forth. To some degree, I am assuming that you've completed part one of the course. To some degree. I will cover everything at least briefly, but if I cover something about deep learning too fast for you to know what's going on and you get lost, go back and watch part one or go and Google for that term. For stuff that we haven't covered in part one, I will go over it very thoroughly and carefully. All right. So I'm going to assume that you know the basic idea, which is that we're going to need to be doing some matrix multiplication. So we're going to try to take a deep dive into matrix multiplication today. And we're going to need some input data. And I quite like working with MNIST data. MNIST is handwritten digits. It's a classic data set. They're 28 by 28 pixel, gray scale images. And so we can download them from this URL. So we use the path libs path object a lot. It's part of Python and it basically takes a string and turns it into something that you can treat as a path. For example, you can use slash to mean this file inside this subdirectory. So this is how we create a path object. Path objects have, for example, a make directory, work dir method. So I like to get everything set up, but I want to be able to rerun this cell lots of times and not have it like give me errors if I run it more than once. If I run it a second time, it still works. And in that case, that's because I put this exist okay equals true. How did I know that I can say, because otherwise it would try to make the directory, it would already exist and it would give an error. How do I know what parameters I can pass to make dir? I just press shift tab. And so when I hit shift tab, it tells me what options there are. If I press it a few times, it'll actually pop it down to the bottom of the screen to remind me. I can press escape to get rid of it. Or you can just, or else you can just hit tab inside and it'll list all the things you could type here as you can see. All right. So we need to grab this URL. And so Python comes with something for doing that, which is the URL lib library that's part of Python that has something called URL retrieve. And something which I'm always a bit surprised is not widely used as people reading the Python documentation. So you should do that a lot. So if I click on that, here is the documentation for URL retrieve. And so I can find exactly what it can take and I can learn about exactly what it does. And so I, yeah, I read the documentation from the Python docs for every single method I use. And I look at every single option that it takes and then I practice with it. And to practice with it, I practice inside, inside Jupiter. So if I want this import on its own, I can hit control shift hyphen and it's going to split it into two cells and then I'll hit alt enter or option enter. So I can create something underneath and I can type URL retrieve shift tab. And so there is, there it all is. If I'm like way down somewhere in the notebook and I have no idea where URL retrieve comes from, I can just hit shift enter and it actually tells me exactly where it comes from. And if I want to know more about it, I can just hit question mark, shift enter, and it's going to give me the documentation. And most call of all, second question mark and it gives me the full source code. And you can see it's not a lot. You know, reading the source code of Python standard library stuff is often quite revealing and you can see exactly how they do it. That's a great way to learn more about, more about this. So in this case, I'm just going to use a very simple functionality, which is I'm going to say the URL to retrieve and the file name to save it as. And again, I made it so I can run this multiple times. So it's only going to do the URL retrieve if the path doesn't exist. If I've already downloaded it, I don't want to download it again. So I run that cell and notice that I can put exclamation mark followed by a line of bash and it actually runs this using bash. If you're using Windows, this, this won't work. And I would very, very strongly suggest if you're using Windows, use WSL. And if you use WSL, all of these notebooks will work perfectly. So yeah, do that. Or write it on paper space or Lambda labs or something like that, Colab, et cetera. Okay. So this is a gzip file. So thankfully, Python comes with a gzip module. Python comes with quite a lot actually. And so we can open a gzip file using gzip.open and we can pass in the path. And we say we're going to read it as binary as opposed to text. Okay. So this is called a context manager. It's a with clause. And what it's going to do is it's going to open up this gzip file. The gzip object will be called f. And then it runs everything inside the block. And when it's done, it will close the file. So with blocks can do all kinds of different things. But in general, with blocks that involve files, they're going to close the file automatically for you. So we can now do that. And so you can see it's opened up the gzip file and the gzip file contains what's called pickle objects. Pickled objects is basically Python objects that have been saved to disk. It's the main way that people in pure Python save stuff. And it's part of the standard library. So this is how we load in from that file. Now the file contains a tuple of tuples. So when you put a tuple on the left hand side of an equal sign, it's quite neat. It allows us to put the first tuple into two variables called x train and y train and the second into x valid and y valid. This trick here where you put stuff like this on the left is called destructuring. And it's a super handy way to make your code kind of clear and concise. And lots of languages support that, including Python. Okay. So we've now got some data. And so we can have a look at it. Now it's a bit tricky because we're not allowed to use NumPy according to our rules. But unfortunately, this actually comes as NumPy. So I've turned it into a list. All right. So I've taken the first image and I've turned it into a list. And so we can look at a few examples of some values in that list. And here they are. So it looks like they're numbers between zero and one. And this is what I do, you know, when I learn about a new data set. So when I started writing this notebook, what you see here, other than the pros here, is what I actually did when I was working with this data. I wanted to know what it was. So I just grab a little bit of it and look at it. So I kind of got a sense now of what it is. Now, interestingly, it's 784. This image is 784 long list. Oh dear. People freaking out in the comments. No NumPy. Yeah, no NumPy. Do you see NumPy? No NumPy. Why 784? What is that? Well, that's because these are 28 by 28 images. So it's just a flat list here of 784 long. So how do I turn this 784 long thing into 28 by 28? So I want a 28, a list of 28, lists of 28 basically, because we don't have matrices. So how do we do that? And so we're going to be learning a lot of cool stuff in Python here. Sorry, I can't stop laughing at all the stuff in our chat. Oh dear. People are quite reasonably freaking out. That's OK. We'll get there. I promise. I hope. Otherwise I'll embarrass myself. All right. So how do I convert a 784 long list into 28 lists, 28 long list of 28 long lists? I'm going to use something called chunks. And first of all, I'll show you what this thing does and then I'll show you how it works. So vowels is currently a list of 10 things. Now if I take vowels and I pass it to chunks with 5, it creates two lists of 5. Here's list number 1 of 5 elements and here's list number 2 of 5 elements. Hopefully you can see what it's doing. It's chunkifying this list and this is the length of each chunk. Now how did it do that? The way I did it is using a very, very useful thing in Python that far too many people don't know about, which is called yield. And what yield does is you can see here, I've got a loop. It's going to go through from 0 up to the length of my list and it's going to jump by 5 at a time. So it's going to go, in this case, 0, 5. And then it's going to, think of this as being like return for now, it's going to return the list from 0 up to 5. So it returns the first bit of the list. But yield doesn't just return. It kind of like returns a bit and then it continues. And it returns a bit more. And so specifically what yield does is it creates an iterator. An iterator is basically something you can, well actually let's use it, that you can call next on a bunch of times. So let's try it. So we can say iterator equals, okay, oh, gotta run it. So what is iterator? Well iterator, iter is something that I can basically, I can call next on. And next basically says yield the next thing. So this should yield valves 0, 5. There it is. It did, right? There's valves 0, 5. Now if I run that again it's going to give me a different answer because it's now up to the second part of this loop. Now it returns the last 5. Okay. So this is what an iterator does. Now if you pass an iterator to Python's list, it runs through the entire letter iterator until it's finished and creates a list of the results. And what does finished looks like? This is what finished looks like. If you call next and get stop iteration, that means you've run out. And that makes sense, right? Because my loop, there's nothing left in it. So all of that is to say we now have a way of taking a list and chunkifying it. So what if I now take my full image, image number 1, chunkify it into chunks of 28 long and turn that into a list and plot it. Ta-da! We have successfully created an image. So that's good. Now we are done, but there are other ways to create this iterator. And because iterators and generators, which are closely related, are so important, I wanted to show you more about how to do them in Python. It's one of these things that if you understand this, you'll often find that you can throw away huge pieces of enterprise software and basically replace it with an iterator. It lets you stream things one bit at a time. It doesn't store it all in memory. It's this really powerful thing that once I, I often find once I show it to people, they suddenly go like, oh, wow, we've been using all this third-party software and we could have just created a Python iterator. Python comes with a whole standard library module called iter tools just to make it easier to work with iterators. I'll show you one example of something from iter tools, which is iSlic. So let's grab our values again, these 10 values. Okay. So let's take these 10 values and we can take any list and turn it into an iterator by passing it to iter, which I should call it. So I don't override this Python. That's not a keyword, but this thing I don't want to override. So this is now basically something that I can call, actually let's do this. I'll show you that I can call next on it. So if I now go next it, you can see it's giving me each item one at a time. Okay. So that's what converting it into an iterator does. iSlic converts it into a different kind of iterator. Let's call this maybe iSlicer iterator. And so you can see here what it did was it jumped stop. Here we are. So, ah, yes, that's what had been better. So I should query create the iterator and then call next a few times. Sorry. This is what I meant to do. It's now only returning the first five before it calls stop iteration, before it raises stop iteration. So what iSlicer does is it grabs the first n things from an iterable, something that you can iterate. Why is that interesting? Because I can pass it to list, for example. Right. And now if I pass it to list again, this iterator has now grabbed the first five things. So it's now up to thing number six. So if I call it again, it's the next five things. And if I call it again, then there's nothing left. And maybe you can see we've actually now got this defined, but we can do it with iSlicer. And here's how we can do it. It's actually pretty tricky. Iter in Python, you can pass it something like a list to create an iterator, or you can pass it, now this is a really important word, a callable. What's a callable? A callable is generally speaking, it's a function. It's something that you can put parentheses after. Could even be a class, anything you can put parentheses after. You can just think of it for now as a function. So we're going to pass it a function. And in the second form, it's going to be called until the function returns this value here, in this case is empty list. And we just saw that iSlicer will return empty list when it's done. So this here is going to keep calling this function again and again and again. And we've seen exactly what happens because we've called it ourselves before. There it is. Until it gets an empty list. So if we do it with 28, then we're going to get our image again. So we've now got two different ways of creating exactly the same thing. And if you've never used iterators before, now's a good time to pause the video and play with them, right? So for example, you could take this here, right, and if you've not seen lambdas before, they're exactly the same as functions, but you can define them inline. So let's replace that with a function. Okay, so now I've turned it into a function and then you can experiment with it. So let's create our iterator and call f on it. Well, not on it, call f. And you can see there's the first 28. And each time I do it, I'm getting another 28. Now the first two rows are all empty. But finally, look, now I've got some values. Call it again. See how each time I'm getting something else. Just calling it again and again. And that is the values in our iterator. So that gives you a sense of like how you can use Jupyter to experiment. So what you should do is as soon as you hit something in my code that doesn't look familiar to you, I recommend pausing the video and experimenting with that in Jupyter. And for example, iter, most people probably have not used iter at all and certainly very few people have used this to argument form. So hit shift tab a few times. And now you've got at the bottom, there's a description of what it is. Or find that more. Python iter. Here we are. Go to the docs. Well, that's not the right bit of the docs. See API. Wow, crazy. That's terrible. Let's try searching here. There we go. That's more like it. So now you've got links. So if it's like, okay, it returns an iterator object. What's that? Well, click on it. Find out. This is really important to know. And here's that stop exception that we saw. Sorry, stop iteration exception. We saw next already. We can find out what iterable is. And here's an example. And as you can see, it's using exactly the same approach that we did, but here it's being used to read from a file. This is really cool. Here's how to read from a file. 64 bytes at a time until you get nothing. Processing it. Right. So the docs of Python are quite fantastic. As long as you use them. If you don't use them, they're not very useful at all. And I see C for in the comments, our local Haskell programmer appreciating this Haskellness in Python. That's good. It's not quite Haskell, I'm afraid, but it's the closest we're going to come. All right. How are we going for time? Pretty good. Okay. So now that we've got image, which is a list of lists, and each list is 25 long, we can index into it. So we can say image 20. Well, let's do it. Image 20. Okay. Here's a list of 28 numbers. And then we could index into that. Okay. So we can index into it. Now, normally we don't like to do that for matrices. We would normally rather write it like this. Okay. So that means we're going to have to create our own class to make that work. So to create a class in Python, you write class. And then you write the name of it. And then you write some really weird things. The weird things you write have two underscores, a special word, and then two underscores. These things with two underscores on each side are called Dunder methods. And they're all the special magically named methods which have particular meanings to Python. And you just got to learn them, but they're all documented in the Python object model. In it. Object model. Yay, finally. Okay. So what do you eventually find? Oh, it's called data model, not object model. And so this is basically where all the documentation is about absolutely everything. And I can click Dunder in it. And it tells you basically this is the thing that constructs objects. So anytime you want to create a class that you want to construct it, it's going to store some stuff. So in this case, it's going to store our image. You have to define Dunder in it. Python's slightly weird in that every method you have to put self here for reasons we probably don't really need to get into right now. And then any parameters. So we're going to be creating an image passing in the thing to store, the x's. So we're going to be passing in the x's. And so here we're just going to store it inside the self. So once I've got this line of code, I've now got something that knows how to store stuff, the x's inside itself. So now I want to be able to call square bracket 20 comma 15. So how do we do that? Well, basically part of the data model is that there's a special thing called Dunder get item. And when you call square brackets on your object, that's what Python uses. And it's going to pass across the 20 comma 15 here as indices. So we're now basically just going to return this. So the self.x's with the first index and the second index. So let's create that matrix class and run that. And you can now see m20 comma 15 is the same. Oh, quick note on, you know, ways in which my code is different to everybody else's, which it is. It's somewhat unusual to put definitions of methods on the same line as the signature like this. I do it quite a lot for one-liners. As I kind of mentioned before, I find it really helps me to be able to see all the code I'm working with on the screen at once. A lot of the world's best programmers actually have had that approach as well. It seems to work quite well for some people that are extremely productive. It's not common in Python. Some people are quite against it. So if you're at work and your colleagues don't write Python this way, you probably shouldn't either. But if you can get away with it, I think it works quite well. Anywho, OK, so now that we've created something that lets us index into things like this, we're allowed to use PyTorch because we're allowed to use this one feature in PyTorch. OK, so we can now do that. And so now to create a tensor, which is basically a lot like our matrix, we can now pass a list into tensor to get back a tensor version of that list. Or perhaps more interestingly, we could pass in a list of lists. Maybe let's give this a name. Whoopsie-dozy. That needs to be a list of lists, just like we had before for our image. In fact, let's do it for our image. Let's just pass in our image. There we go. And so now we should be able to say tense 20 comma 15. And there we go. OK, so we've successfully reinvented that. All right. So now we can convert all of our lists into tensors. There's a convenient way to do this, which is to use the map function in the Python standard library. So shift, shift tab. Map takes a function and then some iterables, in this case one iterable, and it's going to apply this function to each of these four things and return those four things. And so then I can put four things on the left to receive those four things. So this is going to call tensor X train and put it in X train. Tensor Y train, put it in Y train and so forth. So this is converting all of these lists to tensors and storing them back in the same name. So you can see that X train now is a tensor. So that means it has a shape property. It has 50,000 images in it, which are each 784 long. And you can find out what kind of, what kind of stuff it contains by calling it dot type. So it contains floats. So this is the tensor class. We'll be using a lot of it. So of course you should read its documentation. I don't love the PyTorch documentation. Some of it's good. Some of it's not good. It's a bit all over the place. So here's tensor, but it's well worth scrolling through to get a sense of like, this is actually not bad, right? It tells you how you can construct it. This is how I constructed one before, passing it lists of lists. You can also pass it NumPy arrays. You can change types, so on and so forth. So, you know, it's well worth reading through and like, you're not going to look at every single method it takes, but you're kind of, if you browse through it, you'll get a general sense, right? That tensors do just about everything you couldn't think of for numeric programming. At some point, you will want to know every single one of these, or at least be aware roughly what exists so you know what to search for in the docs. Otherwise you will end up recreating stuff from scratch, which is much, much slower than simply reading the documentation to find out it's there. All right. So instead of calling chunks or iSlices, the thing that is roughly equivalent in a tensor is the reshape method. So reshape, so to reshape our 50,000 by 784 thing, we can simply, we want to turn it into 50,028 by 28 tensors. So I could write here reshape to 50,000 by 28 by 28, but I kind of don't need to because I could just put minus one here and it can figure out that that must be 50,000 because it knows that I have 50,000 by 784 items. So it can figure out, so minus one means just fill this with all the rest. Okay. Now what does the word tensor mean? So there's some very interesting history here, and I'll try not to get too far into it because I'm a bit over enthusiastic about this stuff, I must admit. I'm very, very interested in the history of tensor programming and array programming, and it basically goes back to a language called APL. APL is a, basically originally a mathematical notation that was developed in the mid to late 50s, 1950s, and at first it was used to, as a notation for defining how certain new IBM systems would work. So it was all written out in this notation. It's kind of like a replacement for mathematical notation that was designed to be more consistent and kind of more expressive. In the early 60s, so the guy who wrote and made it was called Ken Iverson. In the early 60s, some implementations that actually allowed this notation to be executed on a computer appeared. Both the notation and the executable implementations, slightly confusingly, are both called APL. APL has been in constant development ever since that time, and today is one of the world's most powerful programming languages, and you can try it by going to try APL. And why am I mentioning it here? Because one of the things Ken Iverson did, well, he studied an area of physics called tensor analysis, and as he developed APL, he basically said, like, oh, what if we took these ideas from tensor analysis and put them into a programming language? So in, yeah, in APL, you can, and you know, have been able to for some time, can basically, you can define a variable, and rather than saying equals, which is a terrible way to define things really mathematically, because that has a very different meaning most of the time in math, instead we use arrow to define things, we can say, okay, that's going to be a tensor, like so. And then we can look at their contents of A, and we can do things like, oh, what if we do A times three, or A minus two, and so forth. And as you can see, what it's doing is it's taking all the contents of this tensor, and it's multiplying them all by three, or subtracting two from all of them. Or perhaps more fun, we could put into B a different tensor, and we can now do things like A divided by B, and you can see it's taking each of A and dividing by each of B. Now, this is very interesting, because now we don't have to write loops anymore. We can just express things directly. We can multiply things by scalars, even if they're, this is called a rank one tensor, that is to say it's basically, in math we'd call it a rank one tensor. We can take two vectors and can divide one by the other, and so forth. It's a really powerful idea. Funnily enough, APL didn't call them tensors, even though Ken Iverson said he got this idea from tensor analysis. APL calls them arrays. NumPy, which was heavily influenced by APL, also calls them arrays. For some reason, PyTorch, which is very heavily influenced by APL, sorry, by NumPy, doesn't call them arrays, it calls them tensors. They're all the same thing. They are rectangular blocks of numbers. They can be one-dimensional, like a vector. They can be two-dimensional, like a matrix. They can be three-dimensional, which is like a bunch of stacked matrices, like a batch of matrices, and so forth. If you are interested in APL, which I hope you are, we have a whole APL and array programming section on our forums, and also we've prepared a whole set of notes on every single glyph in APL, which also covers all kinds of interesting mathematical concepts, like complex direction and magnitude, and all kinds of fun stuff like that. That's all totally optional, but a lot of people who do APL say that they feel like they've become a much better programmer in the process, and also you'll find here at the forums a set of 17 study sessions of an hour or two each covering the entirety of the language, every single glyph. So that's all like where this stuff comes from. So this batch of 50,000 images, 50,028 by 28 images, is what we call a rank 3 tensor in PyTorch. In NumPy, we would call it an array with three dimensions. Those are the same thing. So what is the rank? The rank is just the number of dimensions. It's 50,000 images of 28 high by 28 wide, so there are three dimensions that is the rank of the tensor. So if we then pick out a particular image, right, then we look at its shape, we could call this a matrix. It's a 28 by 28 tensor, or we could call it a rank 2 tensor. A vector is a rank 1 tensor. In APL, a scalar is a rank 0 tensor, and that's the way it should be. A lot of languages and libraries don't unfortunately think of it that way. So what is a scalar is a bit dependent on the language. Okay, so we can index into the zeroth image, 20th row, 15th column to get back this same number. Okay, so we can take xtrain.shape, which is 50,000 by 784, and you can destructure it into n, which is the number of images, and c, which is the number of, the full number of columns, for example. And we can also, well this is actually part of the standard library, so we're allowed to use min, so we can find out in ytrain what's the smallest number, and what's the maximum number, so they go from 0 to 9. So you see here it's not just the number 0, it's the scalar tensor 0. They act almost the same, most of the time. So here's some example of a bit of the ytrain, so you can see these are basically, this is going to be the labels, right, these are our digits, and this is its shape so there's just 50,000 of these labels. Okay, and so since we're allowed to use this in the standard library, well it also exists in PyTorch, so that means we're also allowed to use the.min and.max properties. Alright, so before we wrap up we're going to do one more thing, and I don't know what the, what you would call kind of anti-cheating, but according to our rules we're allowed to use random numbers because there is a random number generator in the Python standard library, but we're going to do random numbers from scratch ourselves, and the reason we're going to do that is even though according to the rules we could be allowed to use the standard library one, it's actually extremely instructive to build our own random number generator from scratch. Well at least I think so. Let's see what you think. So there is no way normally in software to create a random number. Unfortunately, computers you know add, subtract, times, logic gate, stuff like that. So how does one create random numbers? Well you could go to the Australian National University quantum random number generator, and this looks at the quantum fluctuations of the vacuum and provides an API which will actually hook you in and return quantum random fluctuations of the vacuum. So that's about, that's the most random thing I'm aware of. So that would be one way to get random numbers. And there's actually an API for that. So there's a bit of fun. You could do what Cloudflare does. Cloudflare has a huge wall full of lava lamps, and it uses the pixels of a camera looking at those lava lamps to generate random numbers. Intel nowadays actually has something in its chips which you can call rdrand, which will return random numbers on certain Intel chips from 2012. All of these things are kind of slow, they can kind of get you one random number from time to time. We want some way of getting lots and lots of random numbers. And so what we do is we use something called a pseudo random number generator. A pseudo random number generator is a mathematical function that you can call lots of times, and each time you call it, it will give you a number that looks random. To show you what I mean by that, I'm going to run some code. And I've created a function which we'll look at in a moment called rdrand. And if I call rdrand 50 times and plot it, there's no obvious relationship between one call and the next. That's one thing that I would expect to see from my random numbers. I would expect that each time I call rdrand, the numbers would look quite different to each other. The second thing is rdrand is meant to be returning uniformly distributed random numbers. And therefore if I call it lots and lots and lots of times and plot its histogram, I would expect to see exactly this, which is each from 0 to 0.1, there's a few, from 0.1 to 0.2, there's a few, from 0.2 to 0.3, there's a few. It's a fairly evenly spread thing. These are the two key things I would expect to see. An even distribution of random numbers and that there's no correlation or no obvious correlation from one to the other. So we want to try and create a function that has these properties. We're not going to derive it from scratch. I'm just going to tell you that we have a function here called the Wickman-Hill algorithm. This is actually what Python used to use back in before Python 2.3. And the key reason we need to know about this is to understand really well the idea of random state. Random state is a global variable. It's something which is, or at least it can be, most of the time when we use it, we use it as a random variable. And it's just basically one or more numbers. So we're going to start with no random state at all. And we're going to create a function called seed that we're going to pass something to. And I just mashed the keyboard to create this number. Okay, so this is my random number. You could get this from the ANU quantum vacuum generator or from CloudFir's lava lamps or from your Intel chips ID, rand, or, you know, in Python land, we'd pretty much always use the number 42. Any of those are fine. So you pass in some number or you can pass in the current tick count in nanoseconds. There's various ways of getting some random starting point. And if we pass it into seed, it's going to do a bunch of modulo divisions and create a tuple of three things and it's going to store them in this global state. So rand state now contains three numbers. Okay, so why did we do that? The reason we did that is because now this function, which takes our random state, unpacks it into three things and does again a bunch of multiplications and moduloes and then sticks them together with various kind of weights. Modulo one, so this is how you can pull out the decimal part. This returns random numbers. But the key thing I want you to understand is that we pull out the random state at the start. We do some math thingies to it and then we store new random state. And so that means that each time I call this, I'm going to get a different number. Right? So this is a random number generator and this is really important because lots of people in the deep learning world screw this up, including me sometimes, which is to remember that random number generators rely on this state. So let me show you where that will get you if you're not careful. If we use this special thing called fork, that creates a whole separate copy of this Python process. In one copy os.fork returns true. In the other copy it returns false, roughly speaking. So this copy here is this, if I say this version here, the true version, is the original non-copied. It's called the parent. And so in my else here, so this will only be called by the parent. This will only be called by the copy, which is called the child. In each one I'm calling rand. These are two different random numbers, right? Wrong. They're the same number. Now why is that? That's because this process here and this process here are copies of each other and therefore they each contain the same numbers in random state. So this is something that comes up in deep learning all the time because in deep learning we often do parallel processing, for example, to generate lots of augmented images at the same time using multiple processes. Fast AI used to have a bug, in fact, where we failed to correctly initialize the random number generator separately in each process. And in fact, to this day, at least as of October 2022, torch.rand itself by default fails to initialize the random number generator. That's the same number. Okay. So you've got to be careful. Now I have a feeling NumPy gets it right. Let's check. Is that how you do it? I don't quite remember. We'll try. No. Okay. NumPy also doesn't. How interesting. What about Python? Random. Oh, look at that. So Python does actually remember to reinitialize the random stream in each fork. So, you know, this is something that, like, even if you've experimented in Python and you think everything's working well in your data loader or whatever, and then you switch to PyTorch or NumPy and now suddenly everything's broken. So this is why we've spent some time re-implementing the random number generator from scratch, partly because it's fun and interesting and partly because it's important that you now understand that when you're calling rand or any random number generator, kind of the default versions in NumPy and PyTorch, this global state is going to be copied. So you've got to be a bit careful. Now I will mention our random number generator. Okay. So this is called... PercentTimeIt. Percent is a special Jupyter or IPython function. And PercentTimeIt runs a piece of Python code this many times. So to call it 10 times. Well, actually it'll do seven loops and each one will be seven times and it'll take the mean and standard deviation. So here I am going to generate random numbers 7840 times and put them into 10 long chunks. And if I run that, it takes me three milliseconds per loop. If I run it using PyTorch, this is the exact same thing in PyTorch, it's going to take me 73 microseconds per loop. So as you can see, although we could use our version, we're not going to because the PyTorch version is much, much faster. This is how we can create a 784 by 10. And why would we want this? That's because this is our final layer of our neural net. Or if we're doing a linear classifier, our linear weights, we'll need to be 784 because that's 28 by 28 by 10 because that's the number of possible outputs, the number of possible digits. All right. That is it. So quite the intense lesson. I think we can all agree. Should keep you busy for a week. And thanks very much for joining. And see you next time. Bye everybody.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 9.96, "text": " Hi everybody and welcome back. This is lesson 10 of Practical Deep Learning for Coders.", "tokens": [2421, 2201, 293, 2928, 646, 13, 639, 307, 6898, 1266, 295, 19170, 804, 14895, 15205, 337, 383, 378, 433, 13], "temperature": 0.0, "avg_logprob": -0.11144348522564312, "compression_ratio": 1.5719298245614035, "no_speech_prob": 0.008313221856951714}, {"id": 1, "seek": 0, "start": 9.96, "end": 14.92, "text": " It's the second lesson in part two, which is where we're going from deep learning foundations", "tokens": [467, 311, 264, 1150, 6898, 294, 644, 732, 11, 597, 307, 689, 321, 434, 516, 490, 2452, 2539, 22467], "temperature": 0.0, "avg_logprob": -0.11144348522564312, "compression_ratio": 1.5719298245614035, "no_speech_prob": 0.008313221856951714}, {"id": 2, "seek": 0, "start": 14.92, "end": 20.72, "text": " to stable diffusion. So before we dive back into our notebook, I think first of all let's", "tokens": [281, 8351, 25242, 13, 407, 949, 321, 9192, 646, 666, 527, 21060, 11, 286, 519, 700, 295, 439, 718, 311], "temperature": 0.0, "avg_logprob": -0.11144348522564312, "compression_ratio": 1.5719298245614035, "no_speech_prob": 0.008313221856951714}, {"id": 3, "seek": 0, "start": 20.72, "end": 24.68, "text": " take a look at some of the interesting work that students in the course have done over", "tokens": [747, 257, 574, 412, 512, 295, 264, 1880, 589, 300, 1731, 294, 264, 1164, 362, 1096, 670], "temperature": 0.0, "avg_logprob": -0.11144348522564312, "compression_ratio": 1.5719298245614035, "no_speech_prob": 0.008313221856951714}, {"id": 4, "seek": 0, "start": 24.68, "end": 29.76, "text": " the last week. I'm just going to show a small sample of what's on the forum, so check out", "tokens": [264, 1036, 1243, 13, 286, 478, 445, 516, 281, 855, 257, 1359, 6889, 295, 437, 311, 322, 264, 17542, 11, 370, 1520, 484], "temperature": 0.0, "avg_logprob": -0.11144348522564312, "compression_ratio": 1.5719298245614035, "no_speech_prob": 0.008313221856951714}, {"id": 5, "seek": 2976, "start": 29.76, "end": 38.480000000000004, "text": " the share your work here thread on the forum for many, many, many more examples. So Puro", "tokens": [264, 2073, 428, 589, 510, 7207, 322, 264, 17542, 337, 867, 11, 867, 11, 867, 544, 5110, 13, 407, 430, 7052], "temperature": 0.0, "avg_logprob": -0.2013647364473891, "compression_ratio": 1.735159817351598, "no_speech_prob": 3.535436189849861e-05}, {"id": 6, "seek": 2976, "start": 38.480000000000004, "end": 47.28, "text": " did something interesting, which is to create a bunch of images of doing a linear interpolation.", "tokens": [630, 746, 1880, 11, 597, 307, 281, 1884, 257, 3840, 295, 5267, 295, 884, 257, 8213, 44902, 399, 13], "temperature": 0.0, "avg_logprob": -0.2013647364473891, "compression_ratio": 1.735159817351598, "no_speech_prob": 3.535436189849861e-05}, {"id": 7, "seek": 2976, "start": 47.28, "end": 52.0, "text": " I mean details actually spherical linear interpolation, but doesn't matter. Doing a linear interpolation", "tokens": [286, 914, 4365, 767, 37300, 8213, 44902, 399, 11, 457, 1177, 380, 1871, 13, 18496, 257, 8213, 44902, 399], "temperature": 0.0, "avg_logprob": -0.2013647364473891, "compression_ratio": 1.735159817351598, "no_speech_prob": 3.535436189849861e-05}, {"id": 8, "seek": 2976, "start": 52.0, "end": 57.68000000000001, "text": " between two different latent, you know, noisy, you know, latent noise starting points for", "tokens": [1296, 732, 819, 48994, 11, 291, 458, 11, 24518, 11, 291, 458, 11, 48994, 5658, 2891, 2793, 337], "temperature": 0.0, "avg_logprob": -0.2013647364473891, "compression_ratio": 1.735159817351598, "no_speech_prob": 3.535436189849861e-05}, {"id": 9, "seek": 5768, "start": 57.68, "end": 65.28, "text": " an otter picture and then showed all the intermediate results. That came out pretty nice. And then", "tokens": [364, 4337, 391, 3036, 293, 550, 4712, 439, 264, 19376, 3542, 13, 663, 1361, 484, 1238, 1481, 13, 400, 550], "temperature": 0.0, "avg_logprob": -0.128128445476567, "compression_ratio": 1.6521739130434783, "no_speech_prob": 1.8924456526292488e-05}, {"id": 10, "seek": 5768, "start": 65.28, "end": 70.2, "text": " did something similar starting with an old car prompt and going to a modern Ferrari prompt.", "tokens": [630, 746, 2531, 2891, 365, 364, 1331, 1032, 12391, 293, 516, 281, 257, 4363, 29828, 12391, 13], "temperature": 0.0, "avg_logprob": -0.128128445476567, "compression_ratio": 1.6521739130434783, "no_speech_prob": 1.8924456526292488e-05}, {"id": 11, "seek": 5768, "start": 70.2, "end": 75.16, "text": " I can't remember exactly what the prompts were, but you can see how as it kind of goes", "tokens": [286, 393, 380, 1604, 2293, 437, 264, 41095, 645, 11, 457, 291, 393, 536, 577, 382, 309, 733, 295, 1709], "temperature": 0.0, "avg_logprob": -0.128128445476567, "compression_ratio": 1.6521739130434783, "no_speech_prob": 1.8924456526292488e-05}, {"id": 12, "seek": 5768, "start": 75.16, "end": 81.2, "text": " through that latent space, it actually is changing the image that's coming out. I think", "tokens": [807, 300, 48994, 1901, 11, 309, 767, 307, 4473, 264, 3256, 300, 311, 1348, 484, 13, 286, 519], "temperature": 0.0, "avg_logprob": -0.128128445476567, "compression_ratio": 1.6521739130434783, "no_speech_prob": 1.8924456526292488e-05}, {"id": 13, "seek": 5768, "start": 81.2, "end": 87.16, "text": " that's really cool. And then I love the way Namrata took that and took it to another level", "tokens": [300, 311, 534, 1627, 13, 400, 550, 286, 959, 264, 636, 10684, 81, 3274, 1890, 300, 293, 1890, 309, 281, 1071, 1496], "temperature": 0.0, "avg_logprob": -0.128128445476567, "compression_ratio": 1.6521739130434783, "no_speech_prob": 1.8924456526292488e-05}, {"id": 14, "seek": 8716, "start": 87.16, "end": 92.6, "text": " in a way, which is starting with a dinosaur and turning into a bird. And this is a very", "tokens": [294, 257, 636, 11, 597, 307, 2891, 365, 257, 23627, 293, 6246, 666, 257, 5255, 13, 400, 341, 307, 257, 588], "temperature": 0.0, "avg_logprob": -0.14879882167762434, "compression_ratio": 1.4806629834254144, "no_speech_prob": 1.2029505342070479e-05}, {"id": 15, "seek": 8716, "start": 92.6, "end": 104.0, "text": " cool intermediate picture of one of the steps along the way, the dino bird. I love it. Dino", "tokens": [1627, 19376, 3036, 295, 472, 295, 264, 4439, 2051, 264, 636, 11, 264, 274, 2982, 5255, 13, 286, 959, 309, 13, 413, 2982], "temperature": 0.0, "avg_logprob": -0.14879882167762434, "compression_ratio": 1.4806629834254144, "no_speech_prob": 1.2029505342070479e-05}, {"id": 16, "seek": 8716, "start": 104.0, "end": 110.4, "text": " chick. Fantastic. So much creativity on the forums. I loved this. John Richmond took his", "tokens": [14371, 13, 21320, 13, 407, 709, 12915, 322, 264, 26998, 13, 286, 4333, 341, 13, 2619, 39060, 1890, 702], "temperature": 0.0, "avg_logprob": -0.14879882167762434, "compression_ratio": 1.4806629834254144, "no_speech_prob": 1.2029505342070479e-05}, {"id": 17, "seek": 11040, "start": 110.4, "end": 119.88000000000001, "text": " daughter's dog and turned it gradually into a unicorn. And I thought this one along the", "tokens": [4653, 311, 3000, 293, 3574, 309, 13145, 666, 257, 28122, 13, 400, 286, 1194, 341, 472, 2051, 264], "temperature": 0.0, "avg_logprob": -0.12962199702407373, "compression_ratio": 1.5084745762711864, "no_speech_prob": 1.0451340131112374e-05}, {"id": 18, "seek": 11040, "start": 119.88000000000001, "end": 124.16000000000001, "text": " way actually came out very, very nicely. I think this is adorable. And I suspect that", "tokens": [636, 767, 1361, 484, 588, 11, 588, 9594, 13, 286, 519, 341, 307, 18698, 13, 400, 286, 9091, 300], "temperature": 0.0, "avg_logprob": -0.12962199702407373, "compression_ratio": 1.5084745762711864, "no_speech_prob": 1.0451340131112374e-05}, {"id": 19, "seek": 11040, "start": 124.16000000000001, "end": 131.04000000000002, "text": " John has won the Dad of the Year, or Dad of the Week maybe award this week for this fantastic", "tokens": [2619, 575, 1582, 264, 5639, 295, 264, 10289, 11, 420, 5639, 295, 264, 12615, 1310, 7130, 341, 1243, 337, 341, 5456], "temperature": 0.0, "avg_logprob": -0.12962199702407373, "compression_ratio": 1.5084745762711864, "no_speech_prob": 1.0451340131112374e-05}, {"id": 20, "seek": 13104, "start": 131.04, "end": 143.56, "text": " project. And Maureen did something very interesting, which is she took Jono's parrot image from", "tokens": [1716, 13, 400, 4042, 540, 268, 630, 746, 588, 1880, 11, 597, 307, 750, 1890, 7745, 78, 311, 42462, 3256, 490], "temperature": 0.0, "avg_logprob": -0.10642963556142954, "compression_ratio": 1.4324324324324325, "no_speech_prob": 4.029423507745378e-06}, {"id": 21, "seek": 13104, "start": 143.56, "end": 151.04, "text": " his lesson and tried bringing it across to various different painter's dials. And so", "tokens": [702, 6898, 293, 3031, 5062, 309, 2108, 281, 3683, 819, 26619, 311, 5502, 82, 13, 400, 370], "temperature": 0.0, "avg_logprob": -0.10642963556142954, "compression_ratio": 1.4324324324324325, "no_speech_prob": 4.029423507745378e-06}, {"id": 22, "seek": 13104, "start": 151.04, "end": 156.92, "text": " her question was, anyone want to guess the artists in the prompts? So I'm just going", "tokens": [720, 1168, 390, 11, 2878, 528, 281, 2041, 264, 6910, 294, 264, 41095, 30, 407, 286, 478, 445, 516], "temperature": 0.0, "avg_logprob": -0.10642963556142954, "compression_ratio": 1.4324324324324325, "no_speech_prob": 4.029423507745378e-06}, {"id": 23, "seek": 15692, "start": 156.92, "end": 168.32, "text": " to let you pause it before I move on, if you want to try to guess. And there they are.", "tokens": [281, 718, 291, 10465, 309, 949, 286, 1286, 322, 11, 498, 291, 528, 281, 853, 281, 2041, 13, 400, 456, 436, 366, 13], "temperature": 0.0, "avg_logprob": -0.09343853382149128, "compression_ratio": 1.6857142857142857, "no_speech_prob": 1.4510153050650842e-05}, {"id": 24, "seek": 15692, "start": 168.32, "end": 174.0, "text": " Most of them pretty obvious, I guess. I think it's so funny that Frida Kahlo appears in", "tokens": [4534, 295, 552, 1238, 6322, 11, 286, 2041, 13, 286, 519, 309, 311, 370, 4074, 300, 1526, 2887, 39444, 752, 7038, 294], "temperature": 0.0, "avg_logprob": -0.09343853382149128, "compression_ratio": 1.6857142857142857, "no_speech_prob": 1.4510153050650842e-05}, {"id": 25, "seek": 15692, "start": 174.0, "end": 178.67999999999998, "text": " all of her paintings. So the parrot's actually turned into Frida Kahlo. All right, not all", "tokens": [439, 295, 720, 14880, 13, 407, 264, 42462, 311, 767, 3574, 666, 1526, 2887, 39444, 752, 13, 1057, 558, 11, 406, 439], "temperature": 0.0, "avg_logprob": -0.09343853382149128, "compression_ratio": 1.6857142857142857, "no_speech_prob": 1.4510153050650842e-05}, {"id": 26, "seek": 15692, "start": 178.67999999999998, "end": 183.72, "text": " of her paintings, but all of her famous ones. So the very idea of a Frida Kahlo painting", "tokens": [295, 720, 14880, 11, 457, 439, 295, 720, 4618, 2306, 13, 407, 264, 588, 1558, 295, 257, 1526, 2887, 39444, 752, 5370], "temperature": 0.0, "avg_logprob": -0.09343853382149128, "compression_ratio": 1.6857142857142857, "no_speech_prob": 1.4510153050650842e-05}, {"id": 27, "seek": 18372, "start": 183.72, "end": 187.96, "text": " without her in it is so unheard of that the parrot's turned into a Frida Kahlo. And I", "tokens": [1553, 720, 294, 309, 307, 370, 517, 42915, 295, 300, 264, 42462, 311, 3574, 666, 257, 1526, 2887, 39444, 752, 13, 400, 286], "temperature": 0.0, "avg_logprob": -0.1068881774435238, "compression_ratio": 1.5086206896551724, "no_speech_prob": 3.535382711561397e-05}, {"id": 28, "seek": 18372, "start": 187.96, "end": 194.52, "text": " like this Jackson Pollock. It's still got the parrot going on there. So that's a really", "tokens": [411, 341, 10647, 31304, 1560, 13, 467, 311, 920, 658, 264, 42462, 516, 322, 456, 13, 407, 300, 311, 257, 534], "temperature": 0.0, "avg_logprob": -0.1068881774435238, "compression_ratio": 1.5086206896551724, "no_speech_prob": 3.535382711561397e-05}, {"id": 29, "seek": 18372, "start": 194.52, "end": 200.74, "text": " lovely one, Maureen. Thank you. And this is a good reminder to make sure that you check", "tokens": [7496, 472, 11, 4042, 540, 268, 13, 1044, 291, 13, 400, 341, 307, 257, 665, 13548, 281, 652, 988, 300, 291, 1520], "temperature": 0.0, "avg_logprob": -0.1068881774435238, "compression_ratio": 1.5086206896551724, "no_speech_prob": 3.535382711561397e-05}, {"id": 30, "seek": 18372, "start": 200.74, "end": 211.36, "text": " out the other two lesson videos. So she was working with Jono's Stable Diffusion lesson.", "tokens": [484, 264, 661, 732, 6898, 2145, 13, 407, 750, 390, 1364, 365, 7745, 78, 311, 745, 712, 413, 3661, 5704, 6898, 13], "temperature": 0.0, "avg_logprob": -0.1068881774435238, "compression_ratio": 1.5086206896551724, "no_speech_prob": 3.535382711561397e-05}, {"id": 31, "seek": 21136, "start": 211.36, "end": 216.9, "text": " So be sure to check that out if you haven't yet. It is available on the course webpage", "tokens": [407, 312, 988, 281, 1520, 300, 484, 498, 291, 2378, 380, 1939, 13, 467, 307, 2435, 322, 264, 1164, 37852], "temperature": 0.0, "avg_logprob": -0.10881965034886411, "compression_ratio": 1.5550660792951543, "no_speech_prob": 9.665740435593762e-06}, {"id": 32, "seek": 21136, "start": 216.9, "end": 226.8, "text": " and on the forums and has lots of cool stuff that you can work with, including this parrot.", "tokens": [293, 322, 264, 26998, 293, 575, 3195, 295, 1627, 1507, 300, 291, 393, 589, 365, 11, 3009, 341, 42462, 13], "temperature": 0.0, "avg_logprob": -0.10881965034886411, "compression_ratio": 1.5550660792951543, "no_speech_prob": 9.665740435593762e-06}, {"id": 33, "seek": 21136, "start": 226.8, "end": 233.92000000000002, "text": " And then the other one to remind you about is the video that Waseem and Tanishk did on", "tokens": [400, 550, 264, 661, 472, 281, 4160, 291, 466, 307, 264, 960, 300, 343, 651, 443, 293, 314, 7524, 74, 630, 322], "temperature": 0.0, "avg_logprob": -0.10881965034886411, "compression_ratio": 1.5550660792951543, "no_speech_prob": 9.665740435593762e-06}, {"id": 34, "seek": 21136, "start": 233.92000000000002, "end": 239.96, "text": " the Math of Diffusion. And I do want to read out what Alex said about this, because I'm", "tokens": [264, 15776, 295, 413, 3661, 5704, 13, 400, 286, 360, 528, 281, 1401, 484, 437, 5202, 848, 466, 341, 11, 570, 286, 478], "temperature": 0.0, "avg_logprob": -0.10881965034886411, "compression_ratio": 1.5550660792951543, "no_speech_prob": 9.665740435593762e-06}, {"id": 35, "seek": 23996, "start": 239.96, "end": 243.44, "text": " sure a number of you feel the same way. My first reaction on seeing something with the", "tokens": [988, 257, 1230, 295, 291, 841, 264, 912, 636, 13, 1222, 700, 5480, 322, 2577, 746, 365, 264], "temperature": 0.0, "avg_logprob": -0.09082216279119508, "compression_ratio": 1.6231884057971016, "no_speech_prob": 3.426500552450307e-05}, {"id": 36, "seek": 23996, "start": 243.44, "end": 247.32000000000002, "text": " title Math of Diffusion was to assume that, oh, that's just something for all the smart", "tokens": [4876, 15776, 295, 413, 3661, 5704, 390, 281, 6552, 300, 11, 1954, 11, 300, 311, 445, 746, 337, 439, 264, 4069], "temperature": 0.0, "avg_logprob": -0.09082216279119508, "compression_ratio": 1.6231884057971016, "no_speech_prob": 3.426500552450307e-05}, {"id": 37, "seek": 23996, "start": 247.32000000000002, "end": 252.16, "text": " people who have PhDs in maths on the course, and it'll probably be completely incomprehensible.", "tokens": [561, 567, 362, 14476, 82, 294, 36287, 322, 264, 1164, 11, 293, 309, 603, 1391, 312, 2584, 14036, 40128, 30633, 13], "temperature": 0.0, "avg_logprob": -0.09082216279119508, "compression_ratio": 1.6231884057971016, "no_speech_prob": 3.426500552450307e-05}, {"id": 38, "seek": 23996, "start": 252.16, "end": 258.84000000000003, "text": " But of course, it's not that at all. So be sure to check this out even if you don't think", "tokens": [583, 295, 1164, 11, 309, 311, 406, 300, 412, 439, 13, 407, 312, 988, 281, 1520, 341, 484, 754, 498, 291, 500, 380, 519], "temperature": 0.0, "avg_logprob": -0.09082216279119508, "compression_ratio": 1.6231884057971016, "no_speech_prob": 3.426500552450307e-05}, {"id": 39, "seek": 23996, "start": 258.84000000000003, "end": 265.18, "text": " of yourself as a math person. I think it's, you know, some nice background that you may", "tokens": [295, 1803, 382, 257, 5221, 954, 13, 286, 519, 309, 311, 11, 291, 458, 11, 512, 1481, 3678, 300, 291, 815], "temperature": 0.0, "avg_logprob": -0.09082216279119508, "compression_ratio": 1.6231884057971016, "no_speech_prob": 3.426500552450307e-05}, {"id": 40, "seek": 26518, "start": 265.18, "end": 270.8, "text": " find useful. It's certainly not necessary, but you might, yeah, I think it's kind of", "tokens": [915, 4420, 13, 467, 311, 3297, 406, 4818, 11, 457, 291, 1062, 11, 1338, 11, 286, 519, 309, 311, 733, 295], "temperature": 0.0, "avg_logprob": -0.11042770707463644, "compression_ratio": 1.5471698113207548, "no_speech_prob": 1.8630014892551117e-05}, {"id": 41, "seek": 26518, "start": 270.8, "end": 279.34000000000003, "text": " useful to start to dig in some to some of the math at this point.", "tokens": [4420, 281, 722, 281, 2528, 294, 512, 281, 512, 295, 264, 5221, 412, 341, 935, 13], "temperature": 0.0, "avg_logprob": -0.11042770707463644, "compression_ratio": 1.5471698113207548, "no_speech_prob": 1.8630014892551117e-05}, {"id": 42, "seek": 26518, "start": 279.34000000000003, "end": 283.12, "text": " One particularly interesting project that's been happening during the week is from Jason", "tokens": [1485, 4098, 1880, 1716, 300, 311, 668, 2737, 1830, 264, 1243, 307, 490, 11181], "temperature": 0.0, "avg_logprob": -0.11042770707463644, "compression_ratio": 1.5471698113207548, "no_speech_prob": 1.8630014892551117e-05}, {"id": 43, "seek": 26518, "start": 283.12, "end": 289.56, "text": " Antich, who is a bit of a legend around here. Many of you will remember him as being the", "tokens": [5130, 480, 11, 567, 307, 257, 857, 295, 257, 9451, 926, 510, 13, 5126, 295, 291, 486, 1604, 796, 382, 885, 264], "temperature": 0.0, "avg_logprob": -0.11042770707463644, "compression_ratio": 1.5471698113207548, "no_speech_prob": 1.8630014892551117e-05}, {"id": 44, "seek": 28956, "start": 289.56, "end": 297.52, "text": " guy that created Deldify and actually worked closely with us on our research, which together", "tokens": [2146, 300, 2942, 413, 5957, 2505, 293, 767, 2732, 8185, 365, 505, 322, 527, 2132, 11, 597, 1214], "temperature": 0.0, "avg_logprob": -0.2165165729210025, "compression_ratio": 1.502857142857143, "no_speech_prob": 3.844774710159982e-06}, {"id": 45, "seek": 28956, "start": 297.52, "end": 306.68, "text": " turned into Nogan and Decrapify and other things, created lots of papers. And Jason", "tokens": [3574, 666, 426, 21576, 293, 12427, 4007, 2505, 293, 661, 721, 11, 2942, 3195, 295, 10577, 13, 400, 11181], "temperature": 0.0, "avg_logprob": -0.2165165729210025, "compression_ratio": 1.502857142857143, "no_speech_prob": 3.844774710159982e-06}, {"id": 46, "seek": 28956, "start": 306.68, "end": 312.8, "text": " has kindly joined our middle research team, working on the stuff for these lessons and", "tokens": [575, 29736, 6869, 527, 2808, 2132, 1469, 11, 1364, 322, 264, 1507, 337, 613, 8820, 293], "temperature": 0.0, "avg_logprob": -0.2165165729210025, "compression_ratio": 1.502857142857143, "no_speech_prob": 3.844774710159982e-06}, {"id": 47, "seek": 31280, "start": 312.8, "end": 320.12, "text": " for developing a kind of fast AI approach to stable diffusion. And he took the idea", "tokens": [337, 6416, 257, 733, 295, 2370, 7318, 3109, 281, 8351, 25242, 13, 400, 415, 1890, 264, 1558], "temperature": 0.0, "avg_logprob": -0.10957471426431235, "compression_ratio": 1.5, "no_speech_prob": 2.178017166443169e-05}, {"id": 48, "seek": 31280, "start": 320.12, "end": 325.40000000000003, "text": " that I prompted last week, which is maybe we should be using classic optimizers rather", "tokens": [300, 286, 31042, 1036, 1243, 11, 597, 307, 1310, 321, 820, 312, 1228, 7230, 5028, 22525, 2831], "temperature": 0.0, "avg_logprob": -0.10957471426431235, "compression_ratio": 1.5, "no_speech_prob": 2.178017166443169e-05}, {"id": 49, "seek": 31280, "start": 325.40000000000003, "end": 331.04, "text": " than differential equation solvers. And he actually made it work incredibly well already", "tokens": [813, 15756, 5367, 1404, 840, 13, 400, 415, 767, 1027, 309, 589, 6252, 731, 1217], "temperature": 0.0, "avg_logprob": -0.10957471426431235, "compression_ratio": 1.5, "no_speech_prob": 2.178017166443169e-05}, {"id": 50, "seek": 31280, "start": 331.04, "end": 338.28000000000003, "text": " within a week. These faces were generated on a single GPU in a few hours from scratch", "tokens": [1951, 257, 1243, 13, 1981, 8475, 645, 10833, 322, 257, 2167, 18407, 294, 257, 1326, 2496, 490, 8459], "temperature": 0.0, "avg_logprob": -0.10957471426431235, "compression_ratio": 1.5, "no_speech_prob": 2.178017166443169e-05}, {"id": 51, "seek": 33828, "start": 338.28, "end": 347.84, "text": " by using classic deep learning optimizers, which is like an unheard of speed to get this", "tokens": [538, 1228, 7230, 2452, 2539, 5028, 22525, 11, 597, 307, 411, 364, 517, 42915, 295, 3073, 281, 483, 341], "temperature": 0.0, "avg_logprob": -0.1654328987246654, "compression_ratio": 1.469945355191257, "no_speech_prob": 1.0782364370243158e-05}, {"id": 52, "seek": 33828, "start": 347.84, "end": 354.64, "text": " quality of image. And we think that this research direction is looking extremely, extremely", "tokens": [3125, 295, 3256, 13, 400, 321, 519, 300, 341, 2132, 3513, 307, 1237, 4664, 11, 4664], "temperature": 0.0, "avg_logprob": -0.1654328987246654, "compression_ratio": 1.469945355191257, "no_speech_prob": 1.0782364370243158e-05}, {"id": 53, "seek": 33828, "start": 354.64, "end": 363.03999999999996, "text": " promising. So really great news there and thank you, Jason, for this fantastic progress.", "tokens": [20257, 13, 407, 534, 869, 2583, 456, 293, 1309, 291, 11, 11181, 11, 337, 341, 5456, 4205, 13], "temperature": 0.0, "avg_logprob": -0.1654328987246654, "compression_ratio": 1.469945355191257, "no_speech_prob": 1.0782364370243158e-05}, {"id": 54, "seek": 36304, "start": 363.04, "end": 373.12, "text": " Yeah, so maybe we'll do a quick reminder of what we looked at last week. So last week", "tokens": [865, 11, 370, 1310, 321, 603, 360, 257, 1702, 13548, 295, 437, 321, 2956, 412, 1036, 1243, 13, 407, 1036, 1243], "temperature": 0.0, "avg_logprob": -0.10807208882437812, "compression_ratio": 1.5229885057471264, "no_speech_prob": 2.2124555471236818e-05}, {"id": 55, "seek": 36304, "start": 373.12, "end": 379.56, "text": " I used a bit of a mega one note hand drawn thing. I thought this week I might just turn", "tokens": [286, 1143, 257, 857, 295, 257, 17986, 472, 3637, 1011, 10117, 551, 13, 286, 1194, 341, 1243, 286, 1062, 445, 1261], "temperature": 0.0, "avg_logprob": -0.10807208882437812, "compression_ratio": 1.5229885057471264, "no_speech_prob": 2.2124555471236818e-05}, {"id": 56, "seek": 36304, "start": 379.56, "end": 387.32000000000005, "text": " it into some slides that we can use. So the basic idea, if you remember, is that we started", "tokens": [309, 666, 512, 9788, 300, 321, 393, 764, 13, 407, 264, 3875, 1558, 11, 498, 291, 1604, 11, 307, 300, 321, 1409], "temperature": 0.0, "avg_logprob": -0.10807208882437812, "compression_ratio": 1.5229885057471264, "no_speech_prob": 2.2124555471236818e-05}, {"id": 57, "seek": 38732, "start": 387.32, "end": 393.71999999999997, "text": " with, if we're doing handwritten digits, for example, we'd start with a number seven. This", "tokens": [365, 11, 498, 321, 434, 884, 1011, 26859, 27011, 11, 337, 1365, 11, 321, 1116, 722, 365, 257, 1230, 3407, 13, 639], "temperature": 0.0, "avg_logprob": -0.11987685109232808, "compression_ratio": 1.6839622641509433, "no_speech_prob": 6.643354026891757e-06}, {"id": 58, "seek": 38732, "start": 393.71999999999997, "end": 398.84, "text": " would be one of the ones with a stroke through it that some countries use. And then we add", "tokens": [576, 312, 472, 295, 264, 2306, 365, 257, 12403, 807, 309, 300, 512, 3517, 764, 13, 400, 550, 321, 909], "temperature": 0.0, "avg_logprob": -0.11987685109232808, "compression_ratio": 1.6839622641509433, "no_speech_prob": 6.643354026891757e-06}, {"id": 59, "seek": 38732, "start": 398.84, "end": 408.03999999999996, "text": " to it some noise and the seven plus the noise together would equal this noisy seven. And", "tokens": [281, 309, 512, 5658, 293, 264, 3407, 1804, 264, 5658, 1214, 576, 2681, 341, 24518, 3407, 13, 400], "temperature": 0.0, "avg_logprob": -0.11987685109232808, "compression_ratio": 1.6839622641509433, "no_speech_prob": 6.643354026891757e-06}, {"id": 60, "seek": 38732, "start": 408.03999999999996, "end": 416.52, "text": " so what we then do is we present this noisy seven as an input to a unit and we have it", "tokens": [370, 437, 321, 550, 360, 307, 321, 1974, 341, 24518, 3407, 382, 364, 4846, 281, 257, 4985, 293, 321, 362, 309], "temperature": 0.0, "avg_logprob": -0.11987685109232808, "compression_ratio": 1.6839622641509433, "no_speech_prob": 6.643354026891757e-06}, {"id": 61, "seek": 41652, "start": 416.52, "end": 425.68, "text": " try to predict which pixels are noise, basically, or predict the noise. And so the unit tries", "tokens": [853, 281, 6069, 597, 18668, 366, 5658, 11, 1936, 11, 420, 6069, 264, 5658, 13, 400, 370, 264, 4985, 9898], "temperature": 0.0, "avg_logprob": -0.07617360442432004, "compression_ratio": 1.6770186335403727, "no_speech_prob": 1.7778296751203015e-05}, {"id": 62, "seek": 41652, "start": 425.68, "end": 435.03999999999996, "text": " to predict the noise from the number. It then compares its prediction to the actual noise", "tokens": [281, 6069, 264, 5658, 490, 264, 1230, 13, 467, 550, 38334, 1080, 17630, 281, 264, 3539, 5658], "temperature": 0.0, "avg_logprob": -0.07617360442432004, "compression_ratio": 1.6770186335403727, "no_speech_prob": 1.7778296751203015e-05}, {"id": 63, "seek": 41652, "start": 435.03999999999996, "end": 442.08, "text": " and it's going to then get a loss, which it can use to update the weights in the unit.", "tokens": [293, 309, 311, 516, 281, 550, 483, 257, 4470, 11, 597, 309, 393, 764, 281, 5623, 264, 17443, 294, 264, 4985, 13], "temperature": 0.0, "avg_logprob": -0.07617360442432004, "compression_ratio": 1.6770186335403727, "no_speech_prob": 1.7778296751203015e-05}, {"id": 64, "seek": 44208, "start": 442.08, "end": 450.12, "text": " And that's basically how stable diffusion, the main bit, if you like, the unit is created.", "tokens": [400, 300, 311, 1936, 577, 8351, 25242, 11, 264, 2135, 857, 11, 498, 291, 411, 11, 264, 4985, 307, 2942, 13], "temperature": 0.0, "avg_logprob": -0.11073706461035687, "compression_ratio": 1.5936073059360731, "no_speech_prob": 1.0289408237440512e-05}, {"id": 65, "seek": 44208, "start": 450.12, "end": 457.46, "text": " To make it easier for the unit, we can also pass in an embedding of the actual digit,", "tokens": [1407, 652, 309, 3571, 337, 264, 4985, 11, 321, 393, 611, 1320, 294, 364, 12240, 3584, 295, 264, 3539, 14293, 11], "temperature": 0.0, "avg_logprob": -0.11073706461035687, "compression_ratio": 1.5936073059360731, "no_speech_prob": 1.0289408237440512e-05}, {"id": 66, "seek": 44208, "start": 457.46, "end": 463.28, "text": " the actual number seven. So for example, a one hot encoded vector, which goes through", "tokens": [264, 3539, 1230, 3407, 13, 407, 337, 1365, 11, 257, 472, 2368, 2058, 12340, 8062, 11, 597, 1709, 807], "temperature": 0.0, "avg_logprob": -0.11073706461035687, "compression_ratio": 1.5936073059360731, "no_speech_prob": 1.0289408237440512e-05}, {"id": 67, "seek": 44208, "start": 463.28, "end": 468.44, "text": " an embedding layer. And the nice thing about that to remind you is that if we do this,", "tokens": [364, 12240, 3584, 4583, 13, 400, 264, 1481, 551, 466, 300, 281, 4160, 291, 307, 300, 498, 321, 360, 341, 11], "temperature": 0.0, "avg_logprob": -0.11073706461035687, "compression_ratio": 1.5936073059360731, "no_speech_prob": 1.0289408237440512e-05}, {"id": 68, "seek": 46844, "start": 468.44, "end": 473.2, "text": " then we also have the benefit that then later on we can actually generate specific digits", "tokens": [550, 321, 611, 362, 264, 5121, 300, 550, 1780, 322, 321, 393, 767, 8460, 2685, 27011], "temperature": 0.0, "avg_logprob": -0.14697561718168714, "compression_ratio": 1.694980694980695, "no_speech_prob": 1.6701151253073476e-05}, {"id": 69, "seek": 46844, "start": 473.2, "end": 476.32, "text": " by saying I want a number seven or I want a number five and it knows what they look", "tokens": [538, 1566, 286, 528, 257, 1230, 3407, 420, 286, 528, 257, 1230, 1732, 293, 309, 3255, 437, 436, 574], "temperature": 0.0, "avg_logprob": -0.14697561718168714, "compression_ratio": 1.694980694980695, "no_speech_prob": 1.6701151253073476e-05}, {"id": 70, "seek": 46844, "start": 476.32, "end": 483.68, "text": " like. I've skipped over here the VAE latency piece, which we talked about last week. And", "tokens": [411, 13, 286, 600, 30193, 670, 510, 264, 18527, 36, 27043, 2522, 11, 597, 321, 2825, 466, 1036, 1243, 13, 400], "temperature": 0.0, "avg_logprob": -0.14697561718168714, "compression_ratio": 1.694980694980695, "no_speech_prob": 1.6701151253073476e-05}, {"id": 71, "seek": 46844, "start": 483.68, "end": 493.36, "text": " to remind you, that's just a computational shortcut. It makes it faster. And so we don't", "tokens": [281, 4160, 291, 11, 300, 311, 445, 257, 28270, 24822, 13, 467, 1669, 309, 4663, 13, 400, 370, 321, 500, 380], "temperature": 0.0, "avg_logprob": -0.14697561718168714, "compression_ratio": 1.694980694980695, "no_speech_prob": 1.6701151253073476e-05}, {"id": 72, "seek": 46844, "start": 493.36, "end": 497.88, "text": " need to include that in this picture because it's just a computational shortcut that we", "tokens": [643, 281, 4090, 300, 294, 341, 3036, 570, 309, 311, 445, 257, 28270, 24822, 300, 321], "temperature": 0.0, "avg_logprob": -0.14697561718168714, "compression_ratio": 1.694980694980695, "no_speech_prob": 1.6701151253073476e-05}, {"id": 73, "seek": 49788, "start": 497.88, "end": 506.48, "text": " can pre-process things into that latent space with the VAE first, if we wish. So that's", "tokens": [393, 659, 12, 41075, 721, 666, 300, 48994, 1901, 365, 264, 18527, 36, 700, 11, 498, 321, 3172, 13, 407, 300, 311], "temperature": 0.0, "avg_logprob": -0.09981382654068317, "compression_ratio": 1.56, "no_speech_prob": 3.785304443226778e-06}, {"id": 74, "seek": 49788, "start": 506.48, "end": 510.92, "text": " what the unit does. Now then to remind you, you know, we want to handle things that are", "tokens": [437, 264, 4985, 775, 13, 823, 550, 281, 4160, 291, 11, 291, 458, 11, 321, 528, 281, 4813, 721, 300, 366], "temperature": 0.0, "avg_logprob": -0.09981382654068317, "compression_ratio": 1.56, "no_speech_prob": 3.785304443226778e-06}, {"id": 75, "seek": 49788, "start": 510.92, "end": 519.24, "text": " more interesting than just the number seven. We want to actually handle things where we", "tokens": [544, 1880, 813, 445, 264, 1230, 3407, 13, 492, 528, 281, 767, 4813, 721, 689, 321], "temperature": 0.0, "avg_logprob": -0.09981382654068317, "compression_ratio": 1.56, "no_speech_prob": 3.785304443226778e-06}, {"id": 76, "seek": 49788, "start": 519.24, "end": 524.9, "text": " can say, for example, a graceful swan or a scene from Hitchcock. And the way we do that", "tokens": [393, 584, 11, 337, 1365, 11, 257, 10042, 906, 1693, 282, 420, 257, 4145, 490, 389, 1549, 29779, 13, 400, 264, 636, 321, 360, 300], "temperature": 0.0, "avg_logprob": -0.09981382654068317, "compression_ratio": 1.56, "no_speech_prob": 3.785304443226778e-06}, {"id": 77, "seek": 52490, "start": 524.9, "end": 531.72, "text": " is we turn these sentences into embeddings as well. And we turn them into embeddings", "tokens": [307, 321, 1261, 613, 16579, 666, 12240, 29432, 382, 731, 13, 400, 321, 1261, 552, 666, 12240, 29432], "temperature": 0.0, "avg_logprob": -0.10547417944127863, "compression_ratio": 1.8109452736318408, "no_speech_prob": 7.18322644388536e-06}, {"id": 78, "seek": 52490, "start": 531.72, "end": 536.6, "text": " by trying to create embeddings of these sentences, which are as similar as possible to embeddings", "tokens": [538, 1382, 281, 1884, 12240, 29432, 295, 613, 16579, 11, 597, 366, 382, 2531, 382, 1944, 281, 12240, 29432], "temperature": 0.0, "avg_logprob": -0.10547417944127863, "compression_ratio": 1.8109452736318408, "no_speech_prob": 7.18322644388536e-06}, {"id": 79, "seek": 52490, "start": 536.6, "end": 541.6, "text": " of the photos or images that they are connected with. And to remind you, the way we did that,", "tokens": [295, 264, 5787, 420, 5267, 300, 436, 366, 4582, 365, 13, 400, 281, 4160, 291, 11, 264, 636, 321, 630, 300, 11], "temperature": 0.0, "avg_logprob": -0.10547417944127863, "compression_ratio": 1.8109452736318408, "no_speech_prob": 7.18322644388536e-06}, {"id": 80, "seek": 52490, "start": 541.6, "end": 547.56, "text": " or the way that was done originally as part of this thing called CLIP, was to basically", "tokens": [420, 264, 636, 300, 390, 1096, 7993, 382, 644, 295, 341, 551, 1219, 12855, 9139, 11, 390, 281, 1936], "temperature": 0.0, "avg_logprob": -0.10547417944127863, "compression_ratio": 1.8109452736318408, "no_speech_prob": 7.18322644388536e-06}, {"id": 81, "seek": 54756, "start": 547.56, "end": 556.0, "text": " download from the internet lots of examples of, lots of images, find their alt tags, and", "tokens": [5484, 490, 264, 4705, 3195, 295, 5110, 295, 11, 3195, 295, 5267, 11, 915, 641, 4955, 18632, 11, 293], "temperature": 0.0, "avg_logprob": -0.10579538869333792, "compression_ratio": 1.868421052631579, "no_speech_prob": 2.190777649957454e-06}, {"id": 82, "seek": 54756, "start": 556.0, "end": 561.68, "text": " then for each one we then have their image and its alt tag. So here's the graceful swan", "tokens": [550, 337, 1184, 472, 321, 550, 362, 641, 3256, 293, 1080, 4955, 6162, 13, 407, 510, 311, 264, 10042, 906, 1693, 282], "temperature": 0.0, "avg_logprob": -0.10579538869333792, "compression_ratio": 1.868421052631579, "no_speech_prob": 2.190777649957454e-06}, {"id": 83, "seek": 54756, "start": 561.68, "end": 569.0, "text": " and its alt tag. And then we build two models, an image encoder that turns each image into", "tokens": [293, 1080, 4955, 6162, 13, 400, 550, 321, 1322, 732, 5245, 11, 364, 3256, 2058, 19866, 300, 4523, 1184, 3256, 666], "temperature": 0.0, "avg_logprob": -0.10579538869333792, "compression_ratio": 1.868421052631579, "no_speech_prob": 2.190777649957454e-06}, {"id": 84, "seek": 54756, "start": 569.0, "end": 576.64, "text": " some feature vector. And then we have a text encoder that turns each piece of text into", "tokens": [512, 4111, 8062, 13, 400, 550, 321, 362, 257, 2487, 2058, 19866, 300, 4523, 1184, 2522, 295, 2487, 666], "temperature": 0.0, "avg_logprob": -0.10579538869333792, "compression_ratio": 1.868421052631579, "no_speech_prob": 2.190777649957454e-06}, {"id": 85, "seek": 57664, "start": 576.64, "end": 581.48, "text": " a bunch of features. And then we create a loss function that says that the features", "tokens": [257, 3840, 295, 4122, 13, 400, 550, 321, 1884, 257, 4470, 2445, 300, 1619, 300, 264, 4122], "temperature": 0.0, "avg_logprob": -0.08323894535099063, "compression_ratio": 1.954954954954955, "no_speech_prob": 4.565957624436123e-06}, {"id": 86, "seek": 57664, "start": 581.48, "end": 588.48, "text": " for a graceful swan, the text, should be as close as possible to the features for the", "tokens": [337, 257, 10042, 906, 1693, 282, 11, 264, 2487, 11, 820, 312, 382, 1998, 382, 1944, 281, 264, 4122, 337, 264], "temperature": 0.0, "avg_logprob": -0.08323894535099063, "compression_ratio": 1.954954954954955, "no_speech_prob": 4.565957624436123e-06}, {"id": 87, "seek": 57664, "start": 588.48, "end": 593.56, "text": " picture of a graceful swan. And specifically we take the dot product. And then we add up", "tokens": [3036, 295, 257, 10042, 906, 1693, 282, 13, 400, 4682, 321, 747, 264, 5893, 1674, 13, 400, 550, 321, 909, 493], "temperature": 0.0, "avg_logprob": -0.08323894535099063, "compression_ratio": 1.954954954954955, "no_speech_prob": 4.565957624436123e-06}, {"id": 88, "seek": 57664, "start": 593.56, "end": 597.52, "text": " all the green ones, because these are the ones that we want to match, and we subtract", "tokens": [439, 264, 3092, 2306, 11, 570, 613, 366, 264, 2306, 300, 321, 528, 281, 2995, 11, 293, 321, 16390], "temperature": 0.0, "avg_logprob": -0.08323894535099063, "compression_ratio": 1.954954954954955, "no_speech_prob": 4.565957624436123e-06}, {"id": 89, "seek": 57664, "start": 597.52, "end": 601.76, "text": " all the red ones, because those are ones we don't want to match. Those are where the text", "tokens": [439, 264, 2182, 2306, 11, 570, 729, 366, 2306, 321, 500, 380, 528, 281, 2995, 13, 3950, 366, 689, 264, 2487], "temperature": 0.0, "avg_logprob": -0.08323894535099063, "compression_ratio": 1.954954954954955, "no_speech_prob": 4.565957624436123e-06}, {"id": 90, "seek": 60176, "start": 601.76, "end": 608.12, "text": " doesn't match the image. And so that's the contrastive lost, which gives us the CL in", "tokens": [1177, 380, 2995, 264, 3256, 13, 400, 370, 300, 311, 264, 8712, 488, 2731, 11, 597, 2709, 505, 264, 12855, 294], "temperature": 0.0, "avg_logprob": -0.12369897842407226, "compression_ratio": 1.6045454545454545, "no_speech_prob": 1.6028066056605894e-06}, {"id": 91, "seek": 60176, "start": 608.12, "end": 615.54, "text": " CLIP. So that's a review of some stuff we did last week. And so with this then we can,", "tokens": [12855, 9139, 13, 407, 300, 311, 257, 3131, 295, 512, 1507, 321, 630, 1036, 1243, 13, 400, 370, 365, 341, 550, 321, 393, 11], "temperature": 0.0, "avg_logprob": -0.12369897842407226, "compression_ratio": 1.6045454545454545, "no_speech_prob": 1.6028066056605894e-06}, {"id": 92, "seek": 60176, "start": 615.54, "end": 620.8, "text": " we now have a text encoder, which we can now say a graceful swan, and it will spit out", "tokens": [321, 586, 362, 257, 2487, 2058, 19866, 11, 597, 321, 393, 586, 584, 257, 10042, 906, 1693, 282, 11, 293, 309, 486, 22127, 484], "temperature": 0.0, "avg_logprob": -0.12369897842407226, "compression_ratio": 1.6045454545454545, "no_speech_prob": 1.6028066056605894e-06}, {"id": 93, "seek": 60176, "start": 620.8, "end": 630.86, "text": " some embeddings. And those are the embeddings that we can feed into our UNet during training.", "tokens": [512, 12240, 29432, 13, 400, 729, 366, 264, 12240, 29432, 300, 321, 393, 3154, 666, 527, 8229, 302, 1830, 3097, 13], "temperature": 0.0, "avg_logprob": -0.12369897842407226, "compression_ratio": 1.6045454545454545, "no_speech_prob": 1.6028066056605894e-06}, {"id": 94, "seek": 63086, "start": 630.86, "end": 636.0, "text": " And so then we don't, haven't been doing any of that training ourselves, except for some", "tokens": [400, 370, 550, 321, 500, 380, 11, 2378, 380, 668, 884, 604, 295, 300, 3097, 4175, 11, 3993, 337, 512], "temperature": 0.0, "avg_logprob": -0.0924056891737313, "compression_ratio": 1.6730769230769231, "no_speech_prob": 4.860420631302986e-06}, {"id": 95, "seek": 63086, "start": 636.0, "end": 640.72, "text": " fine tuning, because it takes a very long time on a lot of computers. But instead we", "tokens": [2489, 15164, 11, 570, 309, 2516, 257, 588, 938, 565, 322, 257, 688, 295, 10807, 13, 583, 2602, 321], "temperature": 0.0, "avg_logprob": -0.0924056891737313, "compression_ratio": 1.6730769230769231, "no_speech_prob": 4.860420631302986e-06}, {"id": 96, "seek": 63086, "start": 640.72, "end": 646.48, "text": " take pre-trained models and do inference. And the way we do inference is we put in an", "tokens": [747, 659, 12, 17227, 2001, 5245, 293, 360, 38253, 13, 400, 264, 636, 321, 360, 38253, 307, 321, 829, 294, 364], "temperature": 0.0, "avg_logprob": -0.0924056891737313, "compression_ratio": 1.6730769230769231, "no_speech_prob": 4.860420631302986e-06}, {"id": 97, "seek": 63086, "start": 646.48, "end": 650.16, "text": " example of the thing that we want, that we have an embedding for. So let's say we're", "tokens": [1365, 295, 264, 551, 300, 321, 528, 11, 300, 321, 362, 364, 12240, 3584, 337, 13, 407, 718, 311, 584, 321, 434], "temperature": 0.0, "avg_logprob": -0.0924056891737313, "compression_ratio": 1.6730769230769231, "no_speech_prob": 4.860420631302986e-06}, {"id": 98, "seek": 63086, "start": 650.16, "end": 657.24, "text": " doing handwritten digits, and we put in some random noise into the UNet, and then it spits", "tokens": [884, 1011, 26859, 27011, 11, 293, 321, 829, 294, 512, 4974, 5658, 666, 264, 8229, 302, 11, 293, 550, 309, 637, 1208], "temperature": 0.0, "avg_logprob": -0.0924056891737313, "compression_ratio": 1.6730769230769231, "no_speech_prob": 4.860420631302986e-06}, {"id": 99, "seek": 65724, "start": 657.24, "end": 662.5600000000001, "text": " out a prediction of which bits of noise you could remove to leave behind a picture of", "tokens": [484, 257, 17630, 295, 597, 9239, 295, 5658, 291, 727, 4159, 281, 1856, 2261, 257, 3036, 295], "temperature": 0.0, "avg_logprob": -0.09369340150252632, "compression_ratio": 1.6415094339622642, "no_speech_prob": 3.6688466025225352e-06}, {"id": 100, "seek": 65724, "start": 662.5600000000001, "end": 668.4, "text": " the number three. Initially it's going to do quite a bad job of that, so we subtract", "tokens": [264, 1230, 1045, 13, 29446, 309, 311, 516, 281, 360, 1596, 257, 1578, 1691, 295, 300, 11, 370, 321, 16390], "temperature": 0.0, "avg_logprob": -0.09369340150252632, "compression_ratio": 1.6415094339622642, "no_speech_prob": 3.6688466025225352e-06}, {"id": 101, "seek": 65724, "start": 668.4, "end": 673.0, "text": " just a little bit of that noise from the image to make it a little bit less noisy, and we", "tokens": [445, 257, 707, 857, 295, 300, 5658, 490, 264, 3256, 281, 652, 309, 257, 707, 857, 1570, 24518, 11, 293, 321], "temperature": 0.0, "avg_logprob": -0.09369340150252632, "compression_ratio": 1.6415094339622642, "no_speech_prob": 3.6688466025225352e-06}, {"id": 102, "seek": 65724, "start": 673.0, "end": 683.6800000000001, "text": " do it again, and we do it a bunch of times. So here's what that looks like. Creating a,", "tokens": [360, 309, 797, 11, 293, 321, 360, 309, 257, 3840, 295, 1413, 13, 407, 510, 311, 437, 300, 1542, 411, 13, 40002, 257, 11], "temperature": 0.0, "avg_logprob": -0.09369340150252632, "compression_ratio": 1.6415094339622642, "no_speech_prob": 3.6688466025225352e-06}, {"id": 103, "seek": 68368, "start": 683.68, "end": 688.88, "text": " I think somebody here did a smiling picture of Jeremy Howard or something, if I remember", "tokens": [286, 519, 2618, 510, 630, 257, 16005, 3036, 295, 17809, 17626, 420, 746, 11, 498, 286, 1604], "temperature": 0.0, "avg_logprob": -0.15356495069420856, "compression_ratio": 1.5486725663716814, "no_speech_prob": 7.64650758355856e-06}, {"id": 104, "seek": 68368, "start": 688.88, "end": 696.28, "text": " correctly. And if we print out the noise at kind of step zero, and at step six, and at", "tokens": [8944, 13, 400, 498, 321, 4482, 484, 264, 5658, 412, 733, 295, 1823, 4018, 11, 293, 412, 1823, 2309, 11, 293, 412], "temperature": 0.0, "avg_logprob": -0.15356495069420856, "compression_ratio": 1.5486725663716814, "no_speech_prob": 7.64650758355856e-06}, {"id": 105, "seek": 68368, "start": 696.28, "end": 701.7199999999999, "text": " step 12, you can see the first signs of a face starting to appear. Definitely a face", "tokens": [1823, 2272, 11, 291, 393, 536, 264, 700, 7880, 295, 257, 1851, 2891, 281, 4204, 13, 12151, 257, 1851], "temperature": 0.0, "avg_logprob": -0.15356495069420856, "compression_ratio": 1.5486725663716814, "no_speech_prob": 7.64650758355856e-06}, {"id": 106, "seek": 68368, "start": 701.7199999999999, "end": 710.64, "text": " appearing here, 18, 24. By step 30 it's looking much more like a face. By 42 it's getting", "tokens": [19870, 510, 11, 2443, 11, 4022, 13, 3146, 1823, 2217, 309, 311, 1237, 709, 544, 411, 257, 1851, 13, 3146, 14034, 309, 311, 1242], "temperature": 0.0, "avg_logprob": -0.15356495069420856, "compression_ratio": 1.5486725663716814, "no_speech_prob": 7.64650758355856e-06}, {"id": 107, "seek": 71064, "start": 710.64, "end": 716.0, "text": " there. It's just got a few little blemishes to fix up, and here we are. I think I've slightly", "tokens": [456, 13, 467, 311, 445, 658, 257, 1326, 707, 5408, 76, 16423, 281, 3191, 493, 11, 293, 510, 321, 366, 13, 286, 519, 286, 600, 4748], "temperature": 0.0, "avg_logprob": -0.11802222511985085, "compression_ratio": 1.451086956521739, "no_speech_prob": 3.5008454233320663e-06}, {"id": 108, "seek": 71064, "start": 716.0, "end": 723.1999999999999, "text": " messed up my indexes here because it should finish at 60, not 54, but such is life. So", "tokens": [16507, 493, 452, 8186, 279, 510, 570, 309, 820, 2413, 412, 4060, 11, 406, 20793, 11, 457, 1270, 307, 993, 13, 407], "temperature": 0.0, "avg_logprob": -0.11802222511985085, "compression_ratio": 1.451086956521739, "no_speech_prob": 3.5008454233320663e-06}, {"id": 109, "seek": 71064, "start": 723.1999999999999, "end": 732.48, "text": " rather rosy red lips too, I would have to say. So remember in the early days this took", "tokens": [2831, 18953, 88, 2182, 10118, 886, 11, 286, 576, 362, 281, 584, 13, 407, 1604, 294, 264, 2440, 1708, 341, 1890], "temperature": 0.0, "avg_logprob": -0.11802222511985085, "compression_ratio": 1.451086956521739, "no_speech_prob": 3.5008454233320663e-06}, {"id": 110, "seek": 73248, "start": 732.48, "end": 740.76, "text": " a thousand steps, and now there are some shortcuts to make it take 60 steps, and this is what", "tokens": [257, 4714, 4439, 11, 293, 586, 456, 366, 512, 34620, 281, 652, 309, 747, 4060, 4439, 11, 293, 341, 307, 437], "temperature": 0.0, "avg_logprob": -0.09725723047366087, "compression_ratio": 1.7766497461928934, "no_speech_prob": 9.516207683191169e-06}, {"id": 111, "seek": 73248, "start": 740.76, "end": 744.24, "text": " the process looks like. And the reason this doesn't look like normal noise is because", "tokens": [264, 1399, 1542, 411, 13, 400, 264, 1778, 341, 1177, 380, 574, 411, 2710, 5658, 307, 570], "temperature": 0.0, "avg_logprob": -0.09725723047366087, "compression_ratio": 1.7766497461928934, "no_speech_prob": 9.516207683191169e-06}, {"id": 112, "seek": 73248, "start": 744.24, "end": 752.64, "text": " now we are actually doing the VAE latency thing. And so noisy latents don't look like", "tokens": [586, 321, 366, 767, 884, 264, 18527, 36, 27043, 551, 13, 400, 370, 24518, 4465, 791, 500, 380, 574, 411], "temperature": 0.0, "avg_logprob": -0.09725723047366087, "compression_ratio": 1.7766497461928934, "no_speech_prob": 9.516207683191169e-06}, {"id": 113, "seek": 73248, "start": 752.64, "end": 756.5600000000001, "text": " Gaussian noise, they look like, well, they look like this. This is what happens when", "tokens": [39148, 5658, 11, 436, 574, 411, 11, 731, 11, 436, 574, 411, 341, 13, 639, 307, 437, 2314, 562], "temperature": 0.0, "avg_logprob": -0.09725723047366087, "compression_ratio": 1.7766497461928934, "no_speech_prob": 9.516207683191169e-06}, {"id": 114, "seek": 75656, "start": 756.56, "end": 763.76, "text": " you decode those noisy latents. Now you might remember last week I complained", "tokens": [291, 979, 1429, 729, 24518, 4465, 791, 13, 823, 291, 1062, 1604, 1036, 1243, 286, 33951], "temperature": 0.0, "avg_logprob": -0.1366135464158169, "compression_ratio": 1.6244343891402715, "no_speech_prob": 2.902299229390337e-06}, {"id": 115, "seek": 75656, "start": 763.76, "end": 769.16, "text": " that things are moving too quickly, and there was a couple of papers that had come out the", "tokens": [300, 721, 366, 2684, 886, 2661, 11, 293, 456, 390, 257, 1916, 295, 10577, 300, 632, 808, 484, 264], "temperature": 0.0, "avg_logprob": -0.1366135464158169, "compression_ratio": 1.6244343891402715, "no_speech_prob": 2.902299229390337e-06}, {"id": 116, "seek": 75656, "start": 769.16, "end": 777.04, "text": " day before and made everything entirely out of date. So Jono and I and the team have actually", "tokens": [786, 949, 293, 1027, 1203, 7696, 484, 295, 4002, 13, 407, 7745, 78, 293, 286, 293, 264, 1469, 362, 767], "temperature": 0.0, "avg_logprob": -0.1366135464158169, "compression_ratio": 1.6244343891402715, "no_speech_prob": 2.902299229390337e-06}, {"id": 117, "seek": 75656, "start": 777.04, "end": 786.16, "text": " now had time to read those papers, and I thought now would be a good time to start going through", "tokens": [586, 632, 565, 281, 1401, 729, 10577, 11, 293, 286, 1194, 586, 576, 312, 257, 665, 565, 281, 722, 516, 807], "temperature": 0.0, "avg_logprob": -0.1366135464158169, "compression_ratio": 1.6244343891402715, "no_speech_prob": 2.902299229390337e-06}, {"id": 118, "seek": 78616, "start": 786.16, "end": 795.4399999999999, "text": " some papers for the first time. So the, what we're actually going to do is show how these", "tokens": [512, 10577, 337, 264, 700, 565, 13, 407, 264, 11, 437, 321, 434, 767, 516, 281, 360, 307, 855, 577, 613], "temperature": 0.0, "avg_logprob": -0.12161852708503382, "compression_ratio": 1.4838709677419355, "no_speech_prob": 9.08041783986846e-06}, {"id": 119, "seek": 78616, "start": 795.4399999999999, "end": 803.1999999999999, "text": " papers have taken the required number of steps to go through this process, down from 60 steps", "tokens": [10577, 362, 2726, 264, 4739, 1230, 295, 4439, 281, 352, 807, 341, 1399, 11, 760, 490, 4060, 4439], "temperature": 0.0, "avg_logprob": -0.12161852708503382, "compression_ratio": 1.4838709677419355, "no_speech_prob": 9.08041783986846e-06}, {"id": 120, "seek": 78616, "start": 803.1999999999999, "end": 813.48, "text": " to four steps, which is pretty amazing. So let's talk about that. And the paper specifically", "tokens": [281, 1451, 4439, 11, 597, 307, 1238, 2243, 13, 407, 718, 311, 751, 466, 300, 13, 400, 264, 3035, 4682], "temperature": 0.0, "avg_logprob": -0.12161852708503382, "compression_ratio": 1.4838709677419355, "no_speech_prob": 9.08041783986846e-06}, {"id": 121, "seek": 81348, "start": 813.48, "end": 825.76, "text": " is this one progressive distillation for fast sampling of diffusion models. So it's only", "tokens": [307, 341, 472, 16131, 42923, 399, 337, 2370, 21179, 295, 25242, 5245, 13, 407, 309, 311, 787], "temperature": 0.0, "avg_logprob": -0.08738442321321857, "compression_ratio": 1.453551912568306, "no_speech_prob": 1.321177387580974e-05}, {"id": 122, "seek": 81348, "start": 825.76, "end": 829.12, "text": " been a week, so I haven't had much of a chance to try to explain this before, so apologies", "tokens": [668, 257, 1243, 11, 370, 286, 2378, 380, 632, 709, 295, 257, 2931, 281, 853, 281, 2903, 341, 949, 11, 370, 34929], "temperature": 0.0, "avg_logprob": -0.08738442321321857, "compression_ratio": 1.453551912568306, "no_speech_prob": 1.321177387580974e-05}, {"id": 123, "seek": 81348, "start": 829.12, "end": 835.24, "text": " in advance if this is awkward, but hopefully it's going to make some sense. What we're", "tokens": [294, 7295, 498, 341, 307, 11411, 11, 457, 4696, 309, 311, 516, 281, 652, 512, 2020, 13, 708, 321, 434], "temperature": 0.0, "avg_logprob": -0.08738442321321857, "compression_ratio": 1.453551912568306, "no_speech_prob": 1.321177387580974e-05}, {"id": 124, "seek": 83524, "start": 835.24, "end": 847.2, "text": " going to start with is, so we're going to start with this process, which is gradually", "tokens": [516, 281, 722, 365, 307, 11, 370, 321, 434, 516, 281, 722, 365, 341, 1399, 11, 597, 307, 13145], "temperature": 0.0, "avg_logprob": -0.10365716447221472, "compression_ratio": 1.4285714285714286, "no_speech_prob": 1.2606733434950002e-05}, {"id": 125, "seek": 83524, "start": 847.2, "end": 857.02, "text": " denoising images, and actually I wonder if we can copy it. Okay, so how are we going", "tokens": [1441, 78, 3436, 5267, 11, 293, 767, 286, 2441, 498, 321, 393, 5055, 309, 13, 1033, 11, 370, 577, 366, 321, 516], "temperature": 0.0, "avg_logprob": -0.10365716447221472, "compression_ratio": 1.4285714285714286, "no_speech_prob": 1.2606733434950002e-05}, {"id": 126, "seek": 85702, "start": 857.02, "end": 866.1999999999999, "text": " to get this down from 60 steps to four steps? The basic idea is that we're going to do a", "tokens": [281, 483, 341, 760, 490, 4060, 4439, 281, 1451, 4439, 30, 440, 3875, 1558, 307, 300, 321, 434, 516, 281, 360, 257], "temperature": 0.0, "avg_logprob": -0.10161185264587402, "compression_ratio": 1.6, "no_speech_prob": 4.157344392297091e-06}, {"id": 127, "seek": 85702, "start": 866.1999999999999, "end": 877.72, "text": " process, we're going to do a process called distillation, which I have no idea how to", "tokens": [1399, 11, 321, 434, 516, 281, 360, 257, 1399, 1219, 42923, 399, 11, 597, 286, 362, 572, 1558, 577, 281], "temperature": 0.0, "avg_logprob": -0.10161185264587402, "compression_ratio": 1.6, "no_speech_prob": 4.157344392297091e-06}, {"id": 128, "seek": 85702, "start": 877.72, "end": 883.1999999999999, "text": " spell, but hopefully that's close enough that you get the idea. Distillation is a process", "tokens": [9827, 11, 457, 4696, 300, 311, 1998, 1547, 300, 291, 483, 264, 1558, 13, 9840, 373, 399, 307, 257, 1399], "temperature": 0.0, "avg_logprob": -0.10161185264587402, "compression_ratio": 1.6, "no_speech_prob": 4.157344392297091e-06}, {"id": 129, "seek": 88320, "start": 883.2, "end": 888.0400000000001, "text": " which is pretty common in deep learning, and the basic idea of distillation is that you", "tokens": [597, 307, 1238, 2689, 294, 2452, 2539, 11, 293, 264, 3875, 1558, 295, 42923, 399, 307, 300, 291], "temperature": 0.0, "avg_logprob": -0.08311240069837456, "compression_ratio": 1.7777777777777777, "no_speech_prob": 6.962167844903888e-06}, {"id": 130, "seek": 88320, "start": 888.0400000000001, "end": 894.36, "text": " take something called a teacher network, which is some neural network that already knows", "tokens": [747, 746, 1219, 257, 5027, 3209, 11, 597, 307, 512, 18161, 3209, 300, 1217, 3255], "temperature": 0.0, "avg_logprob": -0.08311240069837456, "compression_ratio": 1.7777777777777777, "no_speech_prob": 6.962167844903888e-06}, {"id": 131, "seek": 88320, "start": 894.36, "end": 901.08, "text": " how to do something, but it might be slow and big. And the teacher network is then used", "tokens": [577, 281, 360, 746, 11, 457, 309, 1062, 312, 2964, 293, 955, 13, 400, 264, 5027, 3209, 307, 550, 1143], "temperature": 0.0, "avg_logprob": -0.08311240069837456, "compression_ratio": 1.7777777777777777, "no_speech_prob": 6.962167844903888e-06}, {"id": 132, "seek": 88320, "start": 901.08, "end": 907.44, "text": " by a student network, which tries to learn how to do the same thing, but faster or with", "tokens": [538, 257, 3107, 3209, 11, 597, 9898, 281, 1466, 577, 281, 360, 264, 912, 551, 11, 457, 4663, 420, 365], "temperature": 0.0, "avg_logprob": -0.08311240069837456, "compression_ratio": 1.7777777777777777, "no_speech_prob": 6.962167844903888e-06}, {"id": 133, "seek": 90744, "start": 907.44, "end": 916.0, "text": " less memory. And in this case, we want ours to be faster, we want to do less steps. And", "tokens": [1570, 4675, 13, 400, 294, 341, 1389, 11, 321, 528, 11896, 281, 312, 4663, 11, 321, 528, 281, 360, 1570, 4439, 13, 400], "temperature": 0.0, "avg_logprob": -0.11930334245836413, "compression_ratio": 1.5195530726256983, "no_speech_prob": 3.0415856144827558e-06}, {"id": 134, "seek": 90744, "start": 916.0, "end": 923.08, "text": " the way we can do this conceptually, it's actually, in my opinion, reasonably straightforward.", "tokens": [264, 636, 321, 393, 360, 341, 3410, 671, 11, 309, 311, 767, 11, 294, 452, 4800, 11, 23551, 15325, 13], "temperature": 0.0, "avg_logprob": -0.11930334245836413, "compression_ratio": 1.5195530726256983, "no_speech_prob": 3.0415856144827558e-06}, {"id": 135, "seek": 90744, "start": 923.08, "end": 933.0400000000001, "text": " We have, like when I look at this and I think like, wow, you know, neural nets are really", "tokens": [492, 362, 11, 411, 562, 286, 574, 412, 341, 293, 286, 519, 411, 11, 6076, 11, 291, 458, 11, 18161, 36170, 366, 534], "temperature": 0.0, "avg_logprob": -0.11930334245836413, "compression_ratio": 1.5195530726256983, "no_speech_prob": 3.0415856144827558e-06}, {"id": 136, "seek": 93304, "start": 933.04, "end": 942.4, "text": " amazing. So given neural nets are really amazing, why is it taking like 18 steps to go from", "tokens": [2243, 13, 407, 2212, 18161, 36170, 366, 534, 2243, 11, 983, 307, 309, 1940, 411, 2443, 4439, 281, 352, 490], "temperature": 0.0, "avg_logprob": -0.10879441569833194, "compression_ratio": 1.5536723163841808, "no_speech_prob": 4.495159373618662e-06}, {"id": 137, "seek": 93304, "start": 942.4, "end": 952.88, "text": " there to there? Like that seems like something that you should be able to do in one step.", "tokens": [456, 281, 456, 30, 1743, 300, 2544, 411, 746, 300, 291, 820, 312, 1075, 281, 360, 294, 472, 1823, 13], "temperature": 0.0, "avg_logprob": -0.10879441569833194, "compression_ratio": 1.5536723163841808, "no_speech_prob": 4.495159373618662e-06}, {"id": 138, "seek": 93304, "start": 952.88, "end": 958.52, "text": " The fact that it's taking 18 steps, and originally, of course, that was hundreds and hundreds", "tokens": [440, 1186, 300, 309, 311, 1940, 2443, 4439, 11, 293, 7993, 11, 295, 1164, 11, 300, 390, 6779, 293, 6779], "temperature": 0.0, "avg_logprob": -0.10879441569833194, "compression_ratio": 1.5536723163841808, "no_speech_prob": 4.495159373618662e-06}, {"id": 139, "seek": 95852, "start": 958.52, "end": 967.3199999999999, "text": " of steps, is because it's kind of, that's just a kind of a side effect of the math of", "tokens": [295, 4439, 11, 307, 570, 309, 311, 733, 295, 11, 300, 311, 445, 257, 733, 295, 257, 1252, 1802, 295, 264, 5221, 295], "temperature": 0.0, "avg_logprob": -0.12522589883138968, "compression_ratio": 1.6650943396226414, "no_speech_prob": 5.771864380221814e-06}, {"id": 140, "seek": 95852, "start": 967.3199999999999, "end": 974.46, "text": " how this thing was originally developed, you know, this idea of this diffusion process.", "tokens": [577, 341, 551, 390, 7993, 4743, 11, 291, 458, 11, 341, 1558, 295, 341, 25242, 1399, 13], "temperature": 0.0, "avg_logprob": -0.12522589883138968, "compression_ratio": 1.6650943396226414, "no_speech_prob": 5.771864380221814e-06}, {"id": 141, "seek": 95852, "start": 974.46, "end": 980.28, "text": " But the idea in this paper is something that actually we've, I think I might have even", "tokens": [583, 264, 1558, 294, 341, 3035, 307, 746, 300, 767, 321, 600, 11, 286, 519, 286, 1062, 362, 754], "temperature": 0.0, "avg_logprob": -0.12522589883138968, "compression_ratio": 1.6650943396226414, "no_speech_prob": 5.771864380221814e-06}, {"id": 142, "seek": 95852, "start": 980.28, "end": 983.28, "text": " have mentioned in the last lesson, it's something we were thinking of doing ourselves before", "tokens": [362, 2835, 294, 264, 1036, 6898, 11, 309, 311, 746, 321, 645, 1953, 295, 884, 4175, 949], "temperature": 0.0, "avg_logprob": -0.12522589883138968, "compression_ratio": 1.6650943396226414, "no_speech_prob": 5.771864380221814e-06}, {"id": 143, "seek": 98328, "start": 983.28, "end": 990.72, "text": " this paper beat us to it, which is to say, well, what if we train a new model where the", "tokens": [341, 3035, 4224, 505, 281, 309, 11, 597, 307, 281, 584, 11, 731, 11, 437, 498, 321, 3847, 257, 777, 2316, 689, 264], "temperature": 0.0, "avg_logprob": -0.09963668561449238, "compression_ratio": 1.37984496124031, "no_speech_prob": 1.7880597624753136e-06}, {"id": 144, "seek": 98328, "start": 990.72, "end": 1011.48, "text": " model takes as input this image, right, and puts it through some other unit, unit B. Okay.", "tokens": [2316, 2516, 382, 4846, 341, 3256, 11, 558, 11, 293, 8137, 309, 807, 512, 661, 4985, 11, 4985, 363, 13, 1033, 13], "temperature": 0.0, "avg_logprob": -0.09963668561449238, "compression_ratio": 1.37984496124031, "no_speech_prob": 1.7880597624753136e-06}, {"id": 145, "seek": 101148, "start": 1011.48, "end": 1020.52, "text": " And then that spits out some result. And what we do is we take that result and we compare", "tokens": [400, 550, 300, 637, 1208, 484, 512, 1874, 13, 400, 437, 321, 360, 307, 321, 747, 300, 1874, 293, 321, 6794], "temperature": 0.0, "avg_logprob": -0.09523670647733955, "compression_ratio": 1.672811059907834, "no_speech_prob": 8.059429887907754e-07}, {"id": 146, "seek": 101148, "start": 1020.52, "end": 1028.6, "text": " it to this image, the thing we actually want. Because the nice thing is now, which we've", "tokens": [309, 281, 341, 3256, 11, 264, 551, 321, 767, 528, 13, 1436, 264, 1481, 551, 307, 586, 11, 597, 321, 600], "temperature": 0.0, "avg_logprob": -0.09523670647733955, "compression_ratio": 1.672811059907834, "no_speech_prob": 8.059429887907754e-07}, {"id": 147, "seek": 101148, "start": 1028.6, "end": 1033.64, "text": " never really had before, is we have for each intermediate output, like the desired goal,", "tokens": [1128, 534, 632, 949, 11, 307, 321, 362, 337, 1184, 19376, 5598, 11, 411, 264, 14721, 3387, 11], "temperature": 0.0, "avg_logprob": -0.09523670647733955, "compression_ratio": 1.672811059907834, "no_speech_prob": 8.059429887907754e-07}, {"id": 148, "seek": 101148, "start": 1033.64, "end": 1040.04, "text": " where we're trying to get to. And so we could compare those two just using, you know, whatever,", "tokens": [689, 321, 434, 1382, 281, 483, 281, 13, 400, 370, 321, 727, 6794, 729, 732, 445, 1228, 11, 291, 458, 11, 2035, 11], "temperature": 0.0, "avg_logprob": -0.09523670647733955, "compression_ratio": 1.672811059907834, "no_speech_prob": 8.059429887907754e-07}, {"id": 149, "seek": 104004, "start": 1040.04, "end": 1048.84, "text": " mean squared error. Keep on forgetting to change my pen. Mean squared error. And so", "tokens": [914, 8889, 6713, 13, 5527, 322, 25428, 281, 1319, 452, 3435, 13, 12302, 8889, 6713, 13, 400, 370], "temperature": 0.0, "avg_logprob": -0.13139142185808664, "compression_ratio": 1.7989690721649485, "no_speech_prob": 1.1659480151138268e-05}, {"id": 150, "seek": 104004, "start": 1048.84, "end": 1053.12, "text": " then if we keep doing this for lots and lots of images and lots and lots of pairs and exactly", "tokens": [550, 498, 321, 1066, 884, 341, 337, 3195, 293, 3195, 295, 5267, 293, 3195, 293, 3195, 295, 15494, 293, 2293], "temperature": 0.0, "avg_logprob": -0.13139142185808664, "compression_ratio": 1.7989690721649485, "no_speech_prob": 1.1659480151138268e-05}, {"id": 151, "seek": 104004, "start": 1053.12, "end": 1061.1, "text": " this way, this unit is going to hopefully learn to take these incomplete images and", "tokens": [341, 636, 11, 341, 4985, 307, 516, 281, 4696, 1466, 281, 747, 613, 31709, 5267, 293], "temperature": 0.0, "avg_logprob": -0.13139142185808664, "compression_ratio": 1.7989690721649485, "no_speech_prob": 1.1659480151138268e-05}, {"id": 152, "seek": 104004, "start": 1061.1, "end": 1070.0, "text": " turn them into complete images. And that is exactly what this paper does. It just says,", "tokens": [1261, 552, 666, 3566, 5267, 13, 400, 300, 307, 2293, 437, 341, 3035, 775, 13, 467, 445, 1619, 11], "temperature": 0.0, "avg_logprob": -0.13139142185808664, "compression_ratio": 1.7989690721649485, "no_speech_prob": 1.1659480151138268e-05}, {"id": 153, "seek": 107000, "start": 1070.0, "end": 1076.56, "text": " okay, now that we've got all these examples of showing what step 36 should turn into at", "tokens": [1392, 11, 586, 300, 321, 600, 658, 439, 613, 5110, 295, 4099, 437, 1823, 8652, 820, 1261, 666, 412], "temperature": 0.0, "avg_logprob": -0.11147829760675845, "compression_ratio": 1.6388888888888888, "no_speech_prob": 1.4510373148368672e-05}, {"id": 154, "seek": 107000, "start": 1076.56, "end": 1085.12, "text": " step 54, let's just feed those examples into a model. And that works. And you'd kind of", "tokens": [1823, 20793, 11, 718, 311, 445, 3154, 729, 5110, 666, 257, 2316, 13, 400, 300, 1985, 13, 400, 291, 1116, 733, 295], "temperature": 0.0, "avg_logprob": -0.11147829760675845, "compression_ratio": 1.6388888888888888, "no_speech_prob": 1.4510373148368672e-05}, {"id": 155, "seek": 107000, "start": 1085.12, "end": 1089.2, "text": " expect it to work because you can see that like a human would be able to look at this.", "tokens": [2066, 309, 281, 589, 570, 291, 393, 536, 300, 411, 257, 1952, 576, 312, 1075, 281, 574, 412, 341, 13], "temperature": 0.0, "avg_logprob": -0.11147829760675845, "compression_ratio": 1.6388888888888888, "no_speech_prob": 1.4510373148368672e-05}, {"id": 156, "seek": 107000, "start": 1089.2, "end": 1093.72, "text": " And if they were a competent artist, they could turn that into a, you know, a well finished", "tokens": [400, 498, 436, 645, 257, 29998, 5748, 11, 436, 727, 1261, 300, 666, 257, 11, 291, 458, 11, 257, 731, 4335], "temperature": 0.0, "avg_logprob": -0.11147829760675845, "compression_ratio": 1.6388888888888888, "no_speech_prob": 1.4510373148368672e-05}, {"id": 157, "seek": 109372, "start": 1093.72, "end": 1100.88, "text": " product. So you would expect that a computer could as well. There are some little tweaks", "tokens": [1674, 13, 407, 291, 576, 2066, 300, 257, 3820, 727, 382, 731, 13, 821, 366, 512, 707, 46664], "temperature": 0.0, "avg_logprob": -0.1215700525226015, "compression_ratio": 1.4475138121546962, "no_speech_prob": 2.8130043574492447e-06}, {"id": 158, "seek": 109372, "start": 1100.88, "end": 1106.0, "text": " around how it makes this work, which I'll briefly describe, because we need to be able", "tokens": [926, 577, 309, 1669, 341, 589, 11, 597, 286, 603, 10515, 6786, 11, 570, 321, 643, 281, 312, 1075], "temperature": 0.0, "avg_logprob": -0.1215700525226015, "compression_ratio": 1.4475138121546962, "no_speech_prob": 2.8130043574492447e-06}, {"id": 159, "seek": 109372, "start": 1106.0, "end": 1119.08, "text": " to go from kind of step one through to step 10 through to step 20 and so forth. And so", "tokens": [281, 352, 490, 733, 295, 1823, 472, 807, 281, 1823, 1266, 807, 281, 1823, 945, 293, 370, 5220, 13, 400, 370], "temperature": 0.0, "avg_logprob": -0.1215700525226015, "compression_ratio": 1.4475138121546962, "no_speech_prob": 2.8130043574492447e-06}, {"id": 160, "seek": 111908, "start": 1119.08, "end": 1124.32, "text": " the way that it does this, it's actually quite clever. What they do is they initially, so", "tokens": [264, 636, 300, 309, 775, 341, 11, 309, 311, 767, 1596, 13494, 13, 708, 436, 360, 307, 436, 9105, 11, 370], "temperature": 0.0, "avg_logprob": -0.10387486018491594, "compression_ratio": 1.7, "no_speech_prob": 1.8738717244559666e-06}, {"id": 161, "seek": 111908, "start": 1124.32, "end": 1128.76, "text": " they take their teacher model. So remember the teacher model is one that has already", "tokens": [436, 747, 641, 5027, 2316, 13, 407, 1604, 264, 5027, 2316, 307, 472, 300, 575, 1217], "temperature": 0.0, "avg_logprob": -0.10387486018491594, "compression_ratio": 1.7, "no_speech_prob": 1.8738717244559666e-06}, {"id": 162, "seek": 111908, "start": 1128.76, "end": 1133.96, "text": " been trained. Okay. So the teacher model already is a complete stable diffusion model. That's", "tokens": [668, 8895, 13, 1033, 13, 407, 264, 5027, 2316, 1217, 307, 257, 3566, 8351, 25242, 2316, 13, 663, 311], "temperature": 0.0, "avg_logprob": -0.10387486018491594, "compression_ratio": 1.7, "no_speech_prob": 1.8738717244559666e-06}, {"id": 163, "seek": 111908, "start": 1133.96, "end": 1139.9199999999998, "text": " finished. We take that as a given and we put in our image. Well, actually it's noise. We", "tokens": [4335, 13, 492, 747, 300, 382, 257, 2212, 293, 321, 829, 294, 527, 3256, 13, 1042, 11, 767, 309, 311, 5658, 13, 492], "temperature": 0.0, "avg_logprob": -0.10387486018491594, "compression_ratio": 1.7, "no_speech_prob": 1.8738717244559666e-06}, {"id": 164, "seek": 113992, "start": 1139.92, "end": 1152.72, "text": " put in our noise, right? And we put it through two time steps. Okay. And then we train our", "tokens": [829, 294, 527, 5658, 11, 558, 30, 400, 321, 829, 309, 807, 732, 565, 4439, 13, 1033, 13, 400, 550, 321, 3847, 527], "temperature": 0.0, "avg_logprob": -0.08462367455164592, "compression_ratio": 1.5818181818181818, "no_speech_prob": 7.934489190120075e-07}, {"id": 165, "seek": 113992, "start": 1152.72, "end": 1159.04, "text": " unit B or whatever you want to call it to try to go directly from the noise to time", "tokens": [4985, 363, 420, 2035, 291, 528, 281, 818, 309, 281, 853, 281, 352, 3838, 490, 264, 5658, 281, 565], "temperature": 0.0, "avg_logprob": -0.08462367455164592, "compression_ratio": 1.5818181818181818, "no_speech_prob": 7.934489190120075e-07}, {"id": 166, "seek": 113992, "start": 1159.04, "end": 1165.44, "text": " step number two. And that's pretty easy for it to do. And so then what they do is they", "tokens": [1823, 1230, 732, 13, 400, 300, 311, 1238, 1858, 337, 309, 281, 360, 13, 400, 370, 550, 437, 436, 360, 307, 436], "temperature": 0.0, "avg_logprob": -0.08462367455164592, "compression_ratio": 1.5818181818181818, "no_speech_prob": 7.934489190120075e-07}, {"id": 167, "seek": 116544, "start": 1165.44, "end": 1172.6000000000001, "text": " take this, okay. And so this thing here, remember is called the student model. They then say,", "tokens": [747, 341, 11, 1392, 13, 400, 370, 341, 551, 510, 11, 1604, 307, 1219, 264, 3107, 2316, 13, 814, 550, 584, 11], "temperature": 0.0, "avg_logprob": -0.14199527795763983, "compression_ratio": 1.7151898734177216, "no_speech_prob": 6.276700332819019e-07}, {"id": 168, "seek": 116544, "start": 1172.6000000000001, "end": 1180.4, "text": " okay, let's now take that student model and treat that as the new teacher. So they now", "tokens": [1392, 11, 718, 311, 586, 747, 300, 3107, 2316, 293, 2387, 300, 382, 264, 777, 5027, 13, 407, 436, 586], "temperature": 0.0, "avg_logprob": -0.14199527795763983, "compression_ratio": 1.7151898734177216, "no_speech_prob": 6.276700332819019e-07}, {"id": 169, "seek": 116544, "start": 1180.4, "end": 1189.8, "text": " take their noise and they run it through the student model twice, once and twice, and they", "tokens": [747, 641, 5658, 293, 436, 1190, 309, 807, 264, 3107, 2316, 6091, 11, 1564, 293, 6091, 11, 293, 436], "temperature": 0.0, "avg_logprob": -0.14199527795763983, "compression_ratio": 1.7151898734177216, "no_speech_prob": 6.276700332819019e-07}, {"id": 170, "seek": 118980, "start": 1189.8, "end": 1197.28, "text": " get out something at the end. And so then they try to create a new student, which is", "tokens": [483, 484, 746, 412, 264, 917, 13, 400, 370, 550, 436, 853, 281, 1884, 257, 777, 3107, 11, 597, 307], "temperature": 0.0, "avg_logprob": -0.09735545052422417, "compression_ratio": 1.8473684210526315, "no_speech_prob": 1.1544601647983654e-06}, {"id": 171, "seek": 118980, "start": 1197.28, "end": 1202.8, "text": " a copy of the previous student. And it learns to go directly from the noise to two goes", "tokens": [257, 5055, 295, 264, 3894, 3107, 13, 400, 309, 27152, 281, 352, 3838, 490, 264, 5658, 281, 732, 1709], "temperature": 0.0, "avg_logprob": -0.09735545052422417, "compression_ratio": 1.8473684210526315, "no_speech_prob": 1.1544601647983654e-06}, {"id": 172, "seek": 118980, "start": 1202.8, "end": 1208.08, "text": " of the student model. And you won't be surprised to hear they now take that new student model", "tokens": [295, 264, 3107, 2316, 13, 400, 291, 1582, 380, 312, 6100, 281, 1568, 436, 586, 747, 300, 777, 3107, 2316], "temperature": 0.0, "avg_logprob": -0.09735545052422417, "compression_ratio": 1.8473684210526315, "no_speech_prob": 1.1544601647983654e-06}, {"id": 173, "seek": 118980, "start": 1208.08, "end": 1215.96, "text": " and use that to go two goes. And then they learn, they use that, then they copy that", "tokens": [293, 764, 300, 281, 352, 732, 1709, 13, 400, 550, 436, 1466, 11, 436, 764, 300, 11, 550, 436, 5055, 300], "temperature": 0.0, "avg_logprob": -0.09735545052422417, "compression_ratio": 1.8473684210526315, "no_speech_prob": 1.1544601647983654e-06}, {"id": 174, "seek": 121596, "start": 1215.96, "end": 1221.16, "text": " to become the next student model. And so they're doing it again and again and again. And each", "tokens": [281, 1813, 264, 958, 3107, 2316, 13, 400, 370, 436, 434, 884, 309, 797, 293, 797, 293, 797, 13, 400, 1184], "temperature": 0.0, "avg_logprob": -0.10464213111183861, "compression_ratio": 1.805, "no_speech_prob": 4.637860911316238e-06}, {"id": 175, "seek": 121596, "start": 1221.16, "end": 1225.44, "text": " time they're basically doubling the amount of work. So it goes one to two, effectively", "tokens": [565, 436, 434, 1936, 33651, 264, 2372, 295, 589, 13, 407, 309, 1709, 472, 281, 732, 11, 8659], "temperature": 0.0, "avg_logprob": -0.10464213111183861, "compression_ratio": 1.805, "no_speech_prob": 4.637860911316238e-06}, {"id": 176, "seek": 121596, "start": 1225.44, "end": 1231.68, "text": " it's then going two to four and then four to eight. And that's basically what they're", "tokens": [309, 311, 550, 516, 732, 281, 1451, 293, 550, 1451, 281, 3180, 13, 400, 300, 311, 1936, 437, 436, 434], "temperature": 0.0, "avg_logprob": -0.10464213111183861, "compression_ratio": 1.805, "no_speech_prob": 4.637860911316238e-06}, {"id": 177, "seek": 121596, "start": 1231.68, "end": 1237.4, "text": " doing. And they're doing it for multiple different time steps. So this single student model is", "tokens": [884, 13, 400, 436, 434, 884, 309, 337, 3866, 819, 565, 4439, 13, 407, 341, 2167, 3107, 2316, 307], "temperature": 0.0, "avg_logprob": -0.10464213111183861, "compression_ratio": 1.805, "no_speech_prob": 4.637860911316238e-06}, {"id": 178, "seek": 123740, "start": 1237.4, "end": 1248.48, "text": " learning to both do these initial steps, trying to jump multiple steps at a time. And it's", "tokens": [2539, 281, 1293, 360, 613, 5883, 4439, 11, 1382, 281, 3012, 3866, 4439, 412, 257, 565, 13, 400, 309, 311], "temperature": 0.0, "avg_logprob": -0.10197062624825372, "compression_ratio": 1.7197452229299364, "no_speech_prob": 2.8130084501754027e-06}, {"id": 179, "seek": 123740, "start": 1248.48, "end": 1258.0400000000002, "text": " also learning to do these later steps, multiple steps at a time. And that's it, believe it", "tokens": [611, 2539, 281, 360, 613, 1780, 4439, 11, 3866, 4439, 412, 257, 565, 13, 400, 300, 311, 309, 11, 1697, 309], "temperature": 0.0, "avg_logprob": -0.10197062624825372, "compression_ratio": 1.7197452229299364, "no_speech_prob": 2.8130084501754027e-06}, {"id": 180, "seek": 123740, "start": 1258.0400000000002, "end": 1265.9, "text": " or not. So this is this neat paper that came out last week. And that's how it works. Now", "tokens": [420, 406, 13, 407, 341, 307, 341, 10654, 3035, 300, 1361, 484, 1036, 1243, 13, 400, 300, 311, 577, 309, 1985, 13, 823], "temperature": 0.0, "avg_logprob": -0.10197062624825372, "compression_ratio": 1.7197452229299364, "no_speech_prob": 2.8130084501754027e-06}, {"id": 181, "seek": 126590, "start": 1265.9, "end": 1274.88, "text": " I mentioned that there was actually two papers. The second one is called On Distillation of", "tokens": [286, 2835, 300, 456, 390, 767, 732, 10577, 13, 440, 1150, 472, 307, 1219, 1282, 9840, 373, 399, 295], "temperature": 0.0, "avg_logprob": -0.12351753462606402, "compression_ratio": 1.4594594594594594, "no_speech_prob": 5.338128630683059e-06}, {"id": 182, "seek": 126590, "start": 1274.88, "end": 1286.16, "text": " Guided Diffusion Models. And the trick now is this second paper, and these came out at", "tokens": [2694, 2112, 413, 3661, 5704, 6583, 1625, 13, 400, 264, 4282, 586, 307, 341, 1150, 3035, 11, 293, 613, 1361, 484, 412], "temperature": 0.0, "avg_logprob": -0.12351753462606402, "compression_ratio": 1.4594594594594594, "no_speech_prob": 5.338128630683059e-06}, {"id": 183, "seek": 126590, "start": 1286.16, "end": 1289.92, "text": " basically the same time, if I remember correctly, even though they build on each other from", "tokens": [1936, 264, 912, 565, 11, 498, 286, 1604, 8944, 11, 754, 1673, 436, 1322, 322, 1184, 661, 490], "temperature": 0.0, "avg_logprob": -0.12351753462606402, "compression_ratio": 1.4594594594594594, "no_speech_prob": 5.338128630683059e-06}, {"id": 184, "seek": 128992, "start": 1289.92, "end": 1299.2, "text": " the same teams, is that they say, okay, this is all very well, but we don't just want to", "tokens": [264, 912, 5491, 11, 307, 300, 436, 584, 11, 1392, 11, 341, 307, 439, 588, 731, 11, 457, 321, 500, 380, 445, 528, 281], "temperature": 0.0, "avg_logprob": -0.1448191006978353, "compression_ratio": 1.5324675324675325, "no_speech_prob": 3.90546574635664e-06}, {"id": 185, "seek": 128992, "start": 1299.2, "end": 1307.0, "text": " create random pictures. We want to be able to do guidance, right? And you might remember,", "tokens": [1884, 4974, 5242, 13, 492, 528, 281, 312, 1075, 281, 360, 10056, 11, 558, 30, 400, 291, 1062, 1604, 11], "temperature": 0.0, "avg_logprob": -0.1448191006978353, "compression_ratio": 1.5324675324675325, "no_speech_prob": 3.90546574635664e-06}, {"id": 186, "seek": 128992, "start": 1307.0, "end": 1311.8400000000001, "text": " I hope you remember from last week, that we used something called Classifier-Free Guided", "tokens": [286, 1454, 291, 1604, 490, 1036, 1243, 11, 300, 321, 1143, 746, 1219, 9471, 9902, 12, 45479, 2694, 2112], "temperature": 0.0, "avg_logprob": -0.1448191006978353, "compression_ratio": 1.5324675324675325, "no_speech_prob": 3.90546574635664e-06}, {"id": 187, "seek": 128992, "start": 1311.8400000000001, "end": 1319.74, "text": " Diffusion Models, which because I'm lazy, we will just use an acronym, Classifier-Free", "tokens": [413, 3661, 5704, 6583, 1625, 11, 597, 570, 286, 478, 14847, 11, 321, 486, 445, 764, 364, 39195, 11, 9471, 9902, 12, 45479], "temperature": 0.0, "avg_logprob": -0.1448191006978353, "compression_ratio": 1.5324675324675325, "no_speech_prob": 3.90546574635664e-06}, {"id": 188, "seek": 131974, "start": 1319.74, "end": 1328.16, "text": " Guided Diffusion Models. And this one, you may recall, we take, let's say we want a cute", "tokens": [2694, 2112, 413, 3661, 5704, 6583, 1625, 13, 400, 341, 472, 11, 291, 815, 9901, 11, 321, 747, 11, 718, 311, 584, 321, 528, 257, 4052], "temperature": 0.0, "avg_logprob": -0.18249169102421514, "compression_ratio": 1.3157894736842106, "no_speech_prob": 3.6119649848842528e-06}, {"id": 189, "seek": 131974, "start": 1328.16, "end": 1338.48, "text": " puppy. We put in the prompt cute puppy into our clip text encoder, and it spits out an", "tokens": [18196, 13, 492, 829, 294, 264, 12391, 4052, 18196, 666, 527, 7353, 2487, 2058, 19866, 11, 293, 309, 637, 1208, 484, 364], "temperature": 0.0, "avg_logprob": -0.18249169102421514, "compression_ratio": 1.3157894736842106, "no_speech_prob": 3.6119649848842528e-06}, {"id": 190, "seek": 133848, "start": 1338.48, "end": 1350.24, "text": " embedding. And we put that, let's ignore the VAE Latents business, we put that into our", "tokens": [12240, 3584, 13, 400, 321, 829, 300, 11, 718, 311, 11200, 264, 18527, 36, 7354, 791, 1606, 11, 321, 829, 300, 666, 527], "temperature": 0.0, "avg_logprob": -0.13424342286353017, "compression_ratio": 1.3358778625954197, "no_speech_prob": 1.414472308169934e-06}, {"id": 191, "seek": 133848, "start": 1350.24, "end": 1363.88, "text": " UNET. But we also put the empty prompt into our clip text encoder. We concatenate these", "tokens": [8229, 4850, 13, 583, 321, 611, 829, 264, 6707, 12391, 666, 527, 7353, 2487, 2058, 19866, 13, 492, 1588, 7186, 473, 613], "temperature": 0.0, "avg_logprob": -0.13424342286353017, "compression_ratio": 1.3358778625954197, "no_speech_prob": 1.414472308169934e-06}, {"id": 192, "seek": 136388, "start": 1363.88, "end": 1370.0400000000002, "text": " things two together, so that then out the other side, we get back two things. We get", "tokens": [721, 732, 1214, 11, 370, 300, 550, 484, 264, 661, 1252, 11, 321, 483, 646, 732, 721, 13, 492, 483], "temperature": 0.0, "avg_logprob": -0.0940030434552361, "compression_ratio": 1.8112244897959184, "no_speech_prob": 1.3709537824979634e-06}, {"id": 193, "seek": 136388, "start": 1370.0400000000002, "end": 1378.44, "text": " back the image of the cute puppy, and we get back the image of some arbitrary thing. Could", "tokens": [646, 264, 3256, 295, 264, 4052, 18196, 11, 293, 321, 483, 646, 264, 3256, 295, 512, 23211, 551, 13, 7497], "temperature": 0.0, "avg_logprob": -0.0940030434552361, "compression_ratio": 1.8112244897959184, "no_speech_prob": 1.3709537824979634e-06}, {"id": 194, "seek": 136388, "start": 1378.44, "end": 1384.0, "text": " be anything. And then we effectively do something very much like taking the weighted average", "tokens": [312, 1340, 13, 400, 550, 321, 8659, 360, 746, 588, 709, 411, 1940, 264, 32807, 4274], "temperature": 0.0, "avg_logprob": -0.0940030434552361, "compression_ratio": 1.8112244897959184, "no_speech_prob": 1.3709537824979634e-06}, {"id": 195, "seek": 136388, "start": 1384.0, "end": 1391.44, "text": " of these two things together, combine them, and then we use that for the next stage of", "tokens": [295, 613, 732, 721, 1214, 11, 10432, 552, 11, 293, 550, 321, 764, 300, 337, 264, 958, 3233, 295], "temperature": 0.0, "avg_logprob": -0.0940030434552361, "compression_ratio": 1.8112244897959184, "no_speech_prob": 1.3709537824979634e-06}, {"id": 196, "seek": 139144, "start": 1391.44, "end": 1398.28, "text": " our diffusion process. Now, what this paper does is it says, this is all pretty awkward.", "tokens": [527, 25242, 1399, 13, 823, 11, 437, 341, 3035, 775, 307, 309, 1619, 11, 341, 307, 439, 1238, 11411, 13], "temperature": 0.0, "avg_logprob": -0.10044270000238528, "compression_ratio": 1.5580357142857142, "no_speech_prob": 9.666010555520188e-06}, {"id": 197, "seek": 139144, "start": 1398.28, "end": 1403.72, "text": " We end up having to train two images instead of one. And for different types of levels", "tokens": [492, 917, 493, 1419, 281, 3847, 732, 5267, 2602, 295, 472, 13, 400, 337, 819, 3467, 295, 4358], "temperature": 0.0, "avg_logprob": -0.10044270000238528, "compression_ratio": 1.5580357142857142, "no_speech_prob": 9.666010555520188e-06}, {"id": 198, "seek": 139144, "start": 1403.72, "end": 1407.68, "text": " of guided diffusion, we have to like do it multiple different times. That's all pretty", "tokens": [295, 19663, 25242, 11, 321, 362, 281, 411, 360, 309, 3866, 819, 1413, 13, 663, 311, 439, 1238], "temperature": 0.0, "avg_logprob": -0.10044270000238528, "compression_ratio": 1.5580357142857142, "no_speech_prob": 9.666010555520188e-06}, {"id": 199, "seek": 139144, "start": 1407.68, "end": 1413.68, "text": " annoying. How do we skip it? And based on the description of how we did it before, you", "tokens": [11304, 13, 1012, 360, 321, 10023, 309, 30, 400, 2361, 322, 264, 3855, 295, 577, 321, 630, 309, 949, 11, 291], "temperature": 0.0, "avg_logprob": -0.10044270000238528, "compression_ratio": 1.5580357142857142, "no_speech_prob": 9.666010555520188e-06}, {"id": 200, "seek": 141368, "start": 1413.68, "end": 1422.0800000000002, "text": " may be able to guess. What we do is we do exactly the same student-teacher distillation", "tokens": [815, 312, 1075, 281, 2041, 13, 708, 321, 360, 307, 321, 360, 2293, 264, 912, 3107, 12, 975, 4062, 42923, 399], "temperature": 0.0, "avg_logprob": -0.08902618408203125, "compression_ratio": 1.3358778625954197, "no_speech_prob": 4.936963705404196e-06}, {"id": 201, "seek": 141368, "start": 1422.0800000000002, "end": 1435.1200000000001, "text": " we did before, but this time we pass in, in addition, the guidance. And so again, we've", "tokens": [321, 630, 949, 11, 457, 341, 565, 321, 1320, 294, 11, 294, 4500, 11, 264, 10056, 13, 400, 370, 797, 11, 321, 600], "temperature": 0.0, "avg_logprob": -0.08902618408203125, "compression_ratio": 1.3358778625954197, "no_speech_prob": 4.936963705404196e-06}, {"id": 202, "seek": 143512, "start": 1435.12, "end": 1445.12, "text": " got the entire stable diffusion model, the teacher model available for us. And we are", "tokens": [658, 264, 2302, 8351, 25242, 2316, 11, 264, 5027, 2316, 2435, 337, 505, 13, 400, 321, 366], "temperature": 0.0, "avg_logprob": -0.16231941407726658, "compression_ratio": 1.5636363636363637, "no_speech_prob": 3.6688550153485266e-06}, {"id": 203, "seek": 143512, "start": 1445.12, "end": 1452.3799999999999, "text": " doing actual CFGD, classifier-free guided diffusion, to create our guided diffusion", "tokens": [884, 3539, 21792, 38, 35, 11, 1508, 9902, 12, 10792, 19663, 25242, 11, 281, 1884, 527, 19663, 25242], "temperature": 0.0, "avg_logprob": -0.16231941407726658, "compression_ratio": 1.5636363636363637, "no_speech_prob": 3.6688550153485266e-06}, {"id": 204, "seek": 143512, "start": 1452.3799999999999, "end": 1456.8799999999999, "text": " cute puppy pictures. And we're doing it for a range of different guidance scales. So you", "tokens": [4052, 18196, 5242, 13, 400, 321, 434, 884, 309, 337, 257, 3613, 295, 819, 10056, 17408, 13, 407, 291], "temperature": 0.0, "avg_logprob": -0.16231941407726658, "compression_ratio": 1.5636363636363637, "no_speech_prob": 3.6688550153485266e-06}, {"id": 205, "seek": 145688, "start": 1456.88, "end": 1468.16, "text": " might be doing two and 7.5 and 12 and whatever. And those now are becoming inputs to our student", "tokens": [1062, 312, 884, 732, 293, 1614, 13, 20, 293, 2272, 293, 2035, 13, 400, 729, 586, 366, 5617, 15743, 281, 527, 3107], "temperature": 0.0, "avg_logprob": -0.11332910590701634, "compression_ratio": 1.6094674556213018, "no_speech_prob": 2.4060977921180893e-06}, {"id": 206, "seek": 145688, "start": 1468.16, "end": 1476.8000000000002, "text": " model. So the student model now has additional inputs. It's getting the noise, as always.", "tokens": [2316, 13, 407, 264, 3107, 2316, 586, 575, 4497, 15743, 13, 467, 311, 1242, 264, 5658, 11, 382, 1009, 13], "temperature": 0.0, "avg_logprob": -0.11332910590701634, "compression_ratio": 1.6094674556213018, "no_speech_prob": 2.4060977921180893e-06}, {"id": 207, "seek": 145688, "start": 1476.8000000000002, "end": 1481.92, "text": " It's getting the caption or the prompt, I guess I should say, as always. But it's now", "tokens": [467, 311, 1242, 264, 31974, 420, 264, 12391, 11, 286, 2041, 286, 820, 584, 11, 382, 1009, 13, 583, 309, 311, 586], "temperature": 0.0, "avg_logprob": -0.11332910590701634, "compression_ratio": 1.6094674556213018, "no_speech_prob": 2.4060977921180893e-06}, {"id": 208, "seek": 148192, "start": 1481.92, "end": 1491.2, "text": " also getting the guidance scale. And so it's learning to find out how all of these things", "tokens": [611, 1242, 264, 10056, 4373, 13, 400, 370, 309, 311, 2539, 281, 915, 484, 577, 439, 295, 613, 721], "temperature": 0.0, "avg_logprob": -0.11658760379342471, "compression_ratio": 1.5260115606936415, "no_speech_prob": 1.1726403954526177e-06}, {"id": 209, "seek": 148192, "start": 1491.2, "end": 1499.96, "text": " are handled by the teacher model. Like what does it do after a few steps each time? So", "tokens": [366, 18033, 538, 264, 5027, 2316, 13, 1743, 437, 775, 309, 360, 934, 257, 1326, 4439, 1184, 565, 30, 407], "temperature": 0.0, "avg_logprob": -0.11658760379342471, "compression_ratio": 1.5260115606936415, "no_speech_prob": 1.1726403954526177e-06}, {"id": 210, "seek": 148192, "start": 1499.96, "end": 1505.6000000000001, "text": " it's exactly the same thing as before, but now it's learning to use the classifier-free", "tokens": [309, 311, 2293, 264, 912, 551, 382, 949, 11, 457, 586, 309, 311, 2539, 281, 764, 264, 1508, 9902, 12, 10792], "temperature": 0.0, "avg_logprob": -0.11658760379342471, "compression_ratio": 1.5260115606936415, "no_speech_prob": 1.1726403954526177e-06}, {"id": 211, "seek": 150560, "start": 1505.6, "end": 1515.6, "text": " guided diffusion as well. Okay, so that's got quite a lot going on there. And if it's", "tokens": [19663, 25242, 382, 731, 13, 1033, 11, 370, 300, 311, 658, 1596, 257, 688, 516, 322, 456, 13, 400, 498, 309, 311], "temperature": 0.0, "avg_logprob": -0.1208072238498264, "compression_ratio": 1.631578947368421, "no_speech_prob": 1.9947212877013953e-06}, {"id": 212, "seek": 150560, "start": 1515.6, "end": 1521.28, "text": " a bit confusing, that's okay. It is a bit confusing. And what I would recommend is you", "tokens": [257, 857, 13181, 11, 300, 311, 1392, 13, 467, 307, 257, 857, 13181, 13, 400, 437, 286, 576, 2748, 307, 291], "temperature": 0.0, "avg_logprob": -0.1208072238498264, "compression_ratio": 1.631578947368421, "no_speech_prob": 1.9947212877013953e-06}, {"id": 213, "seek": 150560, "start": 1521.28, "end": 1530.6, "text": " check out the extra information from Gelo, who has a whole video on this. And one of", "tokens": [1520, 484, 264, 2857, 1589, 490, 460, 10590, 11, 567, 575, 257, 1379, 960, 322, 341, 13, 400, 472, 295], "temperature": 0.0, "avg_logprob": -0.1208072238498264, "compression_ratio": 1.631578947368421, "no_speech_prob": 1.9947212877013953e-06}, {"id": 214, "seek": 150560, "start": 1530.6, "end": 1535.04, "text": " the cool things actually about this video is it's actually a paper walkthrough. And", "tokens": [264, 1627, 721, 767, 466, 341, 960, 307, 309, 311, 767, 257, 3035, 1792, 11529, 13, 400], "temperature": 0.0, "avg_logprob": -0.1208072238498264, "compression_ratio": 1.631578947368421, "no_speech_prob": 1.9947212877013953e-06}, {"id": 215, "seek": 153504, "start": 1535.04, "end": 1540.96, "text": " so part of this course is hopefully we're going to start reading papers together. Reading", "tokens": [370, 644, 295, 341, 1164, 307, 4696, 321, 434, 516, 281, 722, 3760, 10577, 1214, 13, 29766], "temperature": 0.0, "avg_logprob": -0.12176990509033203, "compression_ratio": 1.6556776556776556, "no_speech_prob": 2.6687388526625e-05}, {"id": 216, "seek": 153504, "start": 1540.96, "end": 1546.32, "text": " papers is extremely intimidating and overwhelming for all of us all of the time. At least for", "tokens": [10577, 307, 4664, 29714, 293, 13373, 337, 439, 295, 505, 439, 295, 264, 565, 13, 1711, 1935, 337], "temperature": 0.0, "avg_logprob": -0.12176990509033203, "compression_ratio": 1.6556776556776556, "no_speech_prob": 2.6687388526625e-05}, {"id": 217, "seek": 153504, "start": 1546.32, "end": 1553.56, "text": " me, it never gets any better. There's a lot of math. And by watching somebody like Jono,", "tokens": [385, 11, 309, 1128, 2170, 604, 1101, 13, 821, 311, 257, 688, 295, 5221, 13, 400, 538, 1976, 2618, 411, 7745, 78, 11], "temperature": 0.0, "avg_logprob": -0.12176990509033203, "compression_ratio": 1.6556776556776556, "no_speech_prob": 2.6687388526625e-05}, {"id": 218, "seek": 153504, "start": 1553.56, "end": 1557.28, "text": " who's an expert at this stuff, read through a paper, you'll kind of get a sense of how", "tokens": [567, 311, 364, 5844, 412, 341, 1507, 11, 1401, 807, 257, 3035, 11, 291, 603, 733, 295, 483, 257, 2020, 295, 577], "temperature": 0.0, "avg_logprob": -0.12176990509033203, "compression_ratio": 1.6556776556776556, "no_speech_prob": 2.6687388526625e-05}, {"id": 219, "seek": 153504, "start": 1557.28, "end": 1563.3999999999999, "text": " he is skipping over lots of the math, right, to focus on, in this case, the really important", "tokens": [415, 307, 31533, 670, 3195, 295, 264, 5221, 11, 558, 11, 281, 1879, 322, 11, 294, 341, 1389, 11, 264, 534, 1021], "temperature": 0.0, "avg_logprob": -0.12176990509033203, "compression_ratio": 1.6556776556776556, "no_speech_prob": 2.6687388526625e-05}, {"id": 220, "seek": 156340, "start": 1563.4, "end": 1568.3600000000001, "text": " thing, which is the actual algorithm. And when you actually look at the algorithm, you", "tokens": [551, 11, 597, 307, 264, 3539, 9284, 13, 400, 562, 291, 767, 574, 412, 264, 9284, 11, 291], "temperature": 0.0, "avg_logprob": -0.16602679661342076, "compression_ratio": 1.7519685039370079, "no_speech_prob": 1.8631419152370654e-05}, {"id": 221, "seek": 156340, "start": 1568.3600000000001, "end": 1573.3600000000001, "text": " start to realize it's basically all stuff, nearly all stuff, maybe all stuff that you", "tokens": [722, 281, 4325, 309, 311, 1936, 439, 1507, 11, 6217, 439, 1507, 11, 1310, 439, 1507, 300, 291], "temperature": 0.0, "avg_logprob": -0.16602679661342076, "compression_ratio": 1.7519685039370079, "no_speech_prob": 1.8631419152370654e-05}, {"id": 222, "seek": 156340, "start": 1573.3600000000001, "end": 1579.2, "text": " did in primary school or secondary school. So we've got division. Okay, sampling from", "tokens": [630, 294, 6194, 1395, 420, 11396, 1395, 13, 407, 321, 600, 658, 10044, 13, 1033, 11, 21179, 490], "temperature": 0.0, "avg_logprob": -0.16602679661342076, "compression_ratio": 1.7519685039370079, "no_speech_prob": 1.8631419152370654e-05}, {"id": 223, "seek": 156340, "start": 1579.2, "end": 1586.3600000000001, "text": " a normal distribution, so high score. Subtraction, division, division, multiplication, right.", "tokens": [257, 2710, 7316, 11, 370, 1090, 6175, 13, 8511, 83, 26766, 11, 10044, 11, 10044, 11, 27290, 11, 558, 13], "temperature": 0.0, "avg_logprob": -0.16602679661342076, "compression_ratio": 1.7519685039370079, "no_speech_prob": 1.8631419152370654e-05}, {"id": 224, "seek": 156340, "start": 1586.3600000000001, "end": 1592.68, "text": " Oh, okay, we've got a log there. But basically, you know, there's not too much going on. And", "tokens": [876, 11, 1392, 11, 321, 600, 658, 257, 3565, 456, 13, 583, 1936, 11, 291, 458, 11, 456, 311, 406, 886, 709, 516, 322, 13, 400], "temperature": 0.0, "avg_logprob": -0.16602679661342076, "compression_ratio": 1.7519685039370079, "no_speech_prob": 1.8631419152370654e-05}, {"id": 225, "seek": 159268, "start": 1592.68, "end": 1598.16, "text": " then when you look at the code, you'll find it, you know, once you turn this into code,", "tokens": [550, 562, 291, 574, 412, 264, 3089, 11, 291, 603, 915, 309, 11, 291, 458, 11, 1564, 291, 1261, 341, 666, 3089, 11], "temperature": 0.0, "avg_logprob": -0.07988419740096382, "compression_ratio": 1.5113636363636365, "no_speech_prob": 3.611957481552963e-06}, {"id": 226, "seek": 159268, "start": 1598.16, "end": 1601.6000000000001, "text": " of course, it becomes even more understandable if you're somebody who's more familiar with", "tokens": [295, 1164, 11, 309, 3643, 754, 544, 25648, 498, 291, 434, 2618, 567, 311, 544, 4963, 365], "temperature": 0.0, "avg_logprob": -0.07988419740096382, "compression_ratio": 1.5113636363636365, "no_speech_prob": 3.611957481552963e-06}, {"id": 227, "seek": 159268, "start": 1601.6000000000001, "end": 1617.04, "text": " code like me. So yeah, definitely check out Jono's video on this. So another paper came", "tokens": [3089, 411, 385, 13, 407, 1338, 11, 2138, 1520, 484, 7745, 78, 311, 960, 322, 341, 13, 407, 1071, 3035, 1361], "temperature": 0.0, "avg_logprob": -0.07988419740096382, "compression_ratio": 1.5113636363636365, "no_speech_prob": 3.611957481552963e-06}, {"id": 228, "seek": 161704, "start": 1617.04, "end": 1626.3999999999999, "text": " out about three hours ago. And I just had to show it to you because I think it's amazing.", "tokens": [484, 466, 1045, 2496, 2057, 13, 400, 286, 445, 632, 281, 855, 309, 281, 291, 570, 286, 519, 309, 311, 2243, 13], "temperature": 0.0, "avg_logprob": -0.10368857132761101, "compression_ratio": 1.639269406392694, "no_speech_prob": 8.664563210913911e-06}, {"id": 229, "seek": 161704, "start": 1626.3999999999999, "end": 1633.28, "text": " And so this is definitely the first video about this paper because it only came out", "tokens": [400, 370, 341, 307, 2138, 264, 700, 960, 466, 341, 3035, 570, 309, 787, 1361, 484], "temperature": 0.0, "avg_logprob": -0.10368857132761101, "compression_ratio": 1.639269406392694, "no_speech_prob": 8.664563210913911e-06}, {"id": 230, "seek": 161704, "start": 1633.28, "end": 1640.8, "text": " a few hours ago. But check this out. This is a paper called iMagic. And with this algorithm,", "tokens": [257, 1326, 2496, 2057, 13, 583, 1520, 341, 484, 13, 639, 307, 257, 3035, 1219, 741, 37445, 299, 13, 400, 365, 341, 9284, 11], "temperature": 0.0, "avg_logprob": -0.10368857132761101, "compression_ratio": 1.639269406392694, "no_speech_prob": 8.664563210913911e-06}, {"id": 231, "seek": 161704, "start": 1640.8, "end": 1644.82, "text": " you can pass in an input image. This is just a, you know, a photo you've taken or downloaded", "tokens": [291, 393, 1320, 294, 364, 4846, 3256, 13, 639, 307, 445, 257, 11, 291, 458, 11, 257, 5052, 291, 600, 2726, 420, 21748], "temperature": 0.0, "avg_logprob": -0.10368857132761101, "compression_ratio": 1.639269406392694, "no_speech_prob": 8.664563210913911e-06}, {"id": 232, "seek": 164482, "start": 1644.82, "end": 1650.48, "text": " off the internet. And then you pass in some text saying, a bird spreading wings. And what", "tokens": [766, 264, 4705, 13, 400, 550, 291, 1320, 294, 512, 2487, 1566, 11, 257, 5255, 15232, 11405, 13, 400, 437], "temperature": 0.0, "avg_logprob": -0.10263433949700718, "compression_ratio": 1.7607843137254902, "no_speech_prob": 8.013350452529266e-06}, {"id": 233, "seek": 164482, "start": 1650.48, "end": 1653.84, "text": " it's going to try to do is it's going to try to take this exact bird in this exact pose", "tokens": [309, 311, 516, 281, 853, 281, 360, 307, 309, 311, 516, 281, 853, 281, 747, 341, 1900, 5255, 294, 341, 1900, 10774], "temperature": 0.0, "avg_logprob": -0.10263433949700718, "compression_ratio": 1.7607843137254902, "no_speech_prob": 8.013350452529266e-06}, {"id": 234, "seek": 164482, "start": 1653.84, "end": 1659.84, "text": " and leave everything as similar as possible, but adjust it just enough so that the prompt", "tokens": [293, 1856, 1203, 382, 2531, 382, 1944, 11, 457, 4369, 309, 445, 1547, 370, 300, 264, 12391], "temperature": 0.0, "avg_logprob": -0.10263433949700718, "compression_ratio": 1.7607843137254902, "no_speech_prob": 8.013350452529266e-06}, {"id": 235, "seek": 164482, "start": 1659.84, "end": 1666.56, "text": " is now matched. So here we take this, this little guy here, and we say, oh, this is actually", "tokens": [307, 586, 21447, 13, 407, 510, 321, 747, 341, 11, 341, 707, 2146, 510, 11, 293, 321, 584, 11, 1954, 11, 341, 307, 767], "temperature": 0.0, "avg_logprob": -0.10263433949700718, "compression_ratio": 1.7607843137254902, "no_speech_prob": 8.013350452529266e-06}, {"id": 236, "seek": 164482, "start": 1666.56, "end": 1670.56, "text": " what we want this to be a person giving the thumbs up. And this is what it produces. And", "tokens": [437, 321, 528, 341, 281, 312, 257, 954, 2902, 264, 8838, 493, 13, 400, 341, 307, 437, 309, 14725, 13, 400], "temperature": 0.0, "avg_logprob": -0.10263433949700718, "compression_ratio": 1.7607843137254902, "no_speech_prob": 8.013350452529266e-06}, {"id": 237, "seek": 167056, "start": 1670.56, "end": 1676.8, "text": " you can see everything else is very, very similar to the previous picture. So this dog", "tokens": [291, 393, 536, 1203, 1646, 307, 588, 11, 588, 2531, 281, 264, 3894, 3036, 13, 407, 341, 3000], "temperature": 0.0, "avg_logprob": -0.0966703448675375, "compression_ratio": 1.7894736842105263, "no_speech_prob": 4.78506444778759e-06}, {"id": 238, "seek": 167056, "start": 1676.8, "end": 1682.58, "text": " is not sitting, but if we put in the prompt, a sitting dog, it turns it into a sitting", "tokens": [307, 406, 3798, 11, 457, 498, 321, 829, 294, 264, 12391, 11, 257, 3798, 3000, 11, 309, 4523, 309, 666, 257, 3798], "temperature": 0.0, "avg_logprob": -0.0966703448675375, "compression_ratio": 1.7894736842105263, "no_speech_prob": 4.78506444778759e-06}, {"id": 239, "seek": 167056, "start": 1682.58, "end": 1689.8799999999999, "text": " dog, leaving everything else as similar as possible. So here's an example of a waterfall.", "tokens": [3000, 11, 5012, 1203, 1646, 382, 2531, 382, 1944, 13, 407, 510, 311, 364, 1365, 295, 257, 27848, 13], "temperature": 0.0, "avg_logprob": -0.0966703448675375, "compression_ratio": 1.7894736842105263, "no_speech_prob": 4.78506444778759e-06}, {"id": 240, "seek": 167056, "start": 1689.8799999999999, "end": 1693.44, "text": " And then you say it's a children's drawing of a waterfall and now it's become a children's", "tokens": [400, 550, 291, 584, 309, 311, 257, 2227, 311, 6316, 295, 257, 27848, 293, 586, 309, 311, 1813, 257, 2227, 311], "temperature": 0.0, "avg_logprob": -0.0966703448675375, "compression_ratio": 1.7894736842105263, "no_speech_prob": 4.78506444778759e-06}, {"id": 241, "seek": 167056, "start": 1693.44, "end": 1697.6, "text": " drawing. So lots of people in the YouTube chat going, oh my God, this is amazing, which", "tokens": [6316, 13, 407, 3195, 295, 561, 294, 264, 3088, 5081, 516, 11, 1954, 452, 1265, 11, 341, 307, 2243, 11, 597], "temperature": 0.0, "avg_logprob": -0.0966703448675375, "compression_ratio": 1.7894736842105263, "no_speech_prob": 4.78506444778759e-06}, {"id": 242, "seek": 169760, "start": 1697.6, "end": 1701.6399999999999, "text": " it absolutely is. And that's why we're going to show you how it works. And one of the really", "tokens": [309, 3122, 307, 13, 400, 300, 311, 983, 321, 434, 516, 281, 855, 291, 577, 309, 1985, 13, 400, 472, 295, 264, 534], "temperature": 0.0, "avg_logprob": -0.0886051013905515, "compression_ratio": 1.721951219512195, "no_speech_prob": 2.0904510620312067e-06}, {"id": 243, "seek": 169760, "start": 1701.6399999999999, "end": 1707.12, "text": " amazing things is you're going to realize that you understand how it works already.", "tokens": [2243, 721, 307, 291, 434, 516, 281, 4325, 300, 291, 1223, 577, 309, 1985, 1217, 13], "temperature": 0.0, "avg_logprob": -0.0886051013905515, "compression_ratio": 1.721951219512195, "no_speech_prob": 2.0904510620312067e-06}, {"id": 244, "seek": 169760, "start": 1707.12, "end": 1712.3999999999999, "text": " Just to show you some more examples, here's the dog image. Here's the sitting dog, the", "tokens": [1449, 281, 855, 291, 512, 544, 5110, 11, 510, 311, 264, 3000, 3256, 13, 1692, 311, 264, 3798, 3000, 11, 264], "temperature": 0.0, "avg_logprob": -0.0886051013905515, "compression_ratio": 1.721951219512195, "no_speech_prob": 2.0904510620312067e-06}, {"id": 245, "seek": 169760, "start": 1712.3999999999999, "end": 1720.1599999999999, "text": " jumping dog, dog playing with a toy, jumping dog holding a frisbee. Okay. And here's this", "tokens": [11233, 3000, 11, 3000, 2433, 365, 257, 12058, 11, 11233, 3000, 5061, 257, 431, 271, 24872, 13, 1033, 13, 400, 510, 311, 341], "temperature": 0.0, "avg_logprob": -0.0886051013905515, "compression_ratio": 1.721951219512195, "no_speech_prob": 2.0904510620312067e-06}, {"id": 246, "seek": 172016, "start": 1720.16, "end": 1727.92, "text": " guy again, giving the thumbs up, crossed arms, in a greeting pose to Namaste hands, holding", "tokens": [2146, 797, 11, 2902, 264, 8838, 493, 11, 14622, 5812, 11, 294, 257, 28174, 10774, 281, 10684, 9079, 2377, 11, 5061], "temperature": 0.0, "avg_logprob": -0.12463976355159984, "compression_ratio": 1.2971014492753623, "no_speech_prob": 3.3931330563063966e-06}, {"id": 247, "seek": 172016, "start": 1727.92, "end": 1738.48, "text": " a cup. So that's pretty amazing. So I had to show you how this works. And I'm not going", "tokens": [257, 4414, 13, 407, 300, 311, 1238, 2243, 13, 407, 286, 632, 281, 855, 291, 577, 341, 1985, 13, 400, 286, 478, 406, 516], "temperature": 0.0, "avg_logprob": -0.12463976355159984, "compression_ratio": 1.2971014492753623, "no_speech_prob": 3.3931330563063966e-06}, {"id": 248, "seek": 173848, "start": 1738.48, "end": 1754.24, "text": " to go into too much detail, but I think we can get the idea actually pretty well.", "tokens": [281, 352, 666, 886, 709, 2607, 11, 457, 286, 519, 321, 393, 483, 264, 1558, 767, 1238, 731, 13], "temperature": 0.0, "avg_logprob": -0.10125830438401964, "compression_ratio": 1.5142857142857142, "no_speech_prob": 8.93963806447573e-06}, {"id": 249, "seek": 173848, "start": 1754.24, "end": 1760.84, "text": " So what we do is again, we take, we start with a fully pre-trained ready to go generative", "tokens": [407, 437, 321, 360, 307, 797, 11, 321, 747, 11, 321, 722, 365, 257, 4498, 659, 12, 17227, 2001, 1919, 281, 352, 1337, 1166], "temperature": 0.0, "avg_logprob": -0.10125830438401964, "compression_ratio": 1.5142857142857142, "no_speech_prob": 8.93963806447573e-06}, {"id": 250, "seek": 173848, "start": 1760.84, "end": 1767.4, "text": " model like a stable diffusion model. And this is what this is talking about here, pre-trained", "tokens": [2316, 411, 257, 8351, 25242, 2316, 13, 400, 341, 307, 437, 341, 307, 1417, 466, 510, 11, 659, 12, 17227, 2001], "temperature": 0.0, "avg_logprob": -0.10125830438401964, "compression_ratio": 1.5142857142857142, "no_speech_prob": 8.93963806447573e-06}, {"id": 251, "seek": 176740, "start": 1767.4, "end": 1771.52, "text": " diffusion model. In the paper, they actually use a model called Imagen, but none of the", "tokens": [25242, 2316, 13, 682, 264, 3035, 11, 436, 767, 764, 257, 2316, 1219, 4331, 4698, 11, 457, 6022, 295, 264], "temperature": 0.0, "avg_logprob": -0.11693904710852582, "compression_ratio": 1.5530973451327434, "no_speech_prob": 4.637849997379817e-06}, {"id": 252, "seek": 176740, "start": 1771.52, "end": 1775.92, "text": " details as far as I can see in any way depend on what the model is. It should work just", "tokens": [4365, 382, 1400, 382, 286, 393, 536, 294, 604, 636, 5672, 322, 437, 264, 2316, 307, 13, 467, 820, 589, 445], "temperature": 0.0, "avg_logprob": -0.11693904710852582, "compression_ratio": 1.5530973451327434, "no_speech_prob": 4.637849997379817e-06}, {"id": 253, "seek": 176740, "start": 1775.92, "end": 1781.48, "text": " fine for stable diffusion. And we take a photo of a bird spreading wings. Okay. So that's", "tokens": [2489, 337, 8351, 25242, 13, 400, 321, 747, 257, 5052, 295, 257, 5255, 15232, 11405, 13, 1033, 13, 407, 300, 311], "temperature": 0.0, "avg_logprob": -0.11693904710852582, "compression_ratio": 1.5530973451327434, "no_speech_prob": 4.637849997379817e-06}, {"id": 254, "seek": 176740, "start": 1781.48, "end": 1792.24, "text": " our target. And we create an embedding from that using, for example, our clip encoder", "tokens": [527, 3779, 13, 400, 321, 1884, 364, 12240, 3584, 490, 300, 1228, 11, 337, 1365, 11, 527, 7353, 2058, 19866], "temperature": 0.0, "avg_logprob": -0.11693904710852582, "compression_ratio": 1.5530973451327434, "no_speech_prob": 4.637849997379817e-06}, {"id": 255, "seek": 179224, "start": 1792.24, "end": 1805.36, "text": " as usual. And we then pass it through our pre-trained diffusion model. And we then see", "tokens": [382, 7713, 13, 400, 321, 550, 1320, 309, 807, 527, 659, 12, 17227, 2001, 25242, 2316, 13, 400, 321, 550, 536], "temperature": 0.0, "avg_logprob": -0.11329406373044278, "compression_ratio": 1.4094488188976377, "no_speech_prob": 2.3687907741987146e-06}, {"id": 256, "seek": 179224, "start": 1805.36, "end": 1818.08, "text": " what it creates. And it doesn't create something that's actually like our bird. So then what", "tokens": [437, 309, 7829, 13, 400, 309, 1177, 380, 1884, 746, 300, 311, 767, 411, 527, 5255, 13, 407, 550, 437], "temperature": 0.0, "avg_logprob": -0.11329406373044278, "compression_ratio": 1.4094488188976377, "no_speech_prob": 2.3687907741987146e-06}, {"id": 257, "seek": 181808, "start": 1818.08, "end": 1823.8799999999999, "text": " they do is they fine tune this embedding. So this is kind of like textual inversion.", "tokens": [436, 360, 307, 436, 2489, 10864, 341, 12240, 3584, 13, 407, 341, 307, 733, 295, 411, 2487, 901, 43576, 13], "temperature": 0.0, "avg_logprob": -0.09737983502839741, "compression_ratio": 1.7341269841269842, "no_speech_prob": 3.089482561335899e-06}, {"id": 258, "seek": 181808, "start": 1823.8799999999999, "end": 1830.4399999999998, "text": " They fine tune the embedding to try to make the diffusion model output something that's", "tokens": [814, 2489, 10864, 264, 12240, 3584, 281, 853, 281, 652, 264, 25242, 2316, 5598, 746, 300, 311], "temperature": 0.0, "avg_logprob": -0.09737983502839741, "compression_ratio": 1.7341269841269842, "no_speech_prob": 3.089482561335899e-06}, {"id": 259, "seek": 181808, "start": 1830.4399999999998, "end": 1837.24, "text": " as similar as possible to the, to the input image. And so you can see here, they're saying,", "tokens": [382, 2531, 382, 1944, 281, 264, 11, 281, 264, 4846, 3256, 13, 400, 370, 291, 393, 536, 510, 11, 436, 434, 1566, 11], "temperature": 0.0, "avg_logprob": -0.09737983502839741, "compression_ratio": 1.7341269841269842, "no_speech_prob": 3.089482561335899e-06}, {"id": 260, "seek": 181808, "start": 1837.24, "end": 1841.8799999999999, "text": " oh, we're moving our embedding a little bit. They don't do this for very long. They just", "tokens": [1954, 11, 321, 434, 2684, 527, 12240, 3584, 257, 707, 857, 13, 814, 500, 380, 360, 341, 337, 588, 938, 13, 814, 445], "temperature": 0.0, "avg_logprob": -0.09737983502839741, "compression_ratio": 1.7341269841269842, "no_speech_prob": 3.089482561335899e-06}, {"id": 261, "seek": 181808, "start": 1841.8799999999999, "end": 1846.9199999999998, "text": " want to move it a little bit in the right direction. And then now they lock that in", "tokens": [528, 281, 1286, 309, 257, 707, 857, 294, 264, 558, 3513, 13, 400, 550, 586, 436, 4017, 300, 294], "temperature": 0.0, "avg_logprob": -0.09737983502839741, "compression_ratio": 1.7341269841269842, "no_speech_prob": 3.089482561335899e-06}, {"id": 262, "seek": 184692, "start": 1846.92, "end": 1853.2, "text": " place and they say, okay, now let's fine tune the entire diffusion model end to end, including", "tokens": [1081, 293, 436, 584, 11, 1392, 11, 586, 718, 311, 2489, 10864, 264, 2302, 25242, 2316, 917, 281, 917, 11, 3009], "temperature": 0.0, "avg_logprob": -0.11946044365564983, "compression_ratio": 1.6160714285714286, "no_speech_prob": 4.222826191835338e-06}, {"id": 263, "seek": 184692, "start": 1853.2, "end": 1858.96, "text": " the VAE. Or actually with Image N they have a super resolution model, but same idea. So", "tokens": [264, 18527, 36, 13, 1610, 767, 365, 29903, 426, 436, 362, 257, 1687, 8669, 2316, 11, 457, 912, 1558, 13, 407], "temperature": 0.0, "avg_logprob": -0.11946044365564983, "compression_ratio": 1.6160714285714286, "no_speech_prob": 4.222826191835338e-06}, {"id": 264, "seek": 184692, "start": 1858.96, "end": 1864.0800000000002, "text": " we fine tune the entire model end to end. And now the embedding, this optimized embedding", "tokens": [321, 2489, 10864, 264, 2302, 2316, 917, 281, 917, 13, 400, 586, 264, 12240, 3584, 11, 341, 26941, 12240, 3584], "temperature": 0.0, "avg_logprob": -0.11946044365564983, "compression_ratio": 1.6160714285714286, "no_speech_prob": 4.222826191835338e-06}, {"id": 265, "seek": 184692, "start": 1864.0800000000002, "end": 1872.0, "text": " we created, we store in place. We don't change that at all. That's now frozen. And we try", "tokens": [321, 2942, 11, 321, 3531, 294, 1081, 13, 492, 500, 380, 1319, 300, 412, 439, 13, 663, 311, 586, 12496, 13, 400, 321, 853], "temperature": 0.0, "avg_logprob": -0.11946044365564983, "compression_ratio": 1.6160714285714286, "no_speech_prob": 4.222826191835338e-06}, {"id": 266, "seek": 187200, "start": 1872.0, "end": 1882.32, "text": " to make it so that the diffusion model now spits out our bird as close as possible. So", "tokens": [281, 652, 309, 370, 300, 264, 25242, 2316, 586, 637, 1208, 484, 527, 5255, 382, 1998, 382, 1944, 13, 407], "temperature": 0.0, "avg_logprob": -0.08867472344702416, "compression_ratio": 1.7142857142857142, "no_speech_prob": 5.539167204915429e-07}, {"id": 267, "seek": 187200, "start": 1882.32, "end": 1886.32, "text": " you fine tune that for a few epochs. And so you've now got something that takes this,", "tokens": [291, 2489, 10864, 300, 337, 257, 1326, 30992, 28346, 13, 400, 370, 291, 600, 586, 658, 746, 300, 2516, 341, 11], "temperature": 0.0, "avg_logprob": -0.08867472344702416, "compression_ratio": 1.7142857142857142, "no_speech_prob": 5.539167204915429e-07}, {"id": 268, "seek": 187200, "start": 1886.32, "end": 1891.76, "text": " this embedding that we fine tuned, goes through a fine tune model and spits out our bird.", "tokens": [341, 12240, 3584, 300, 321, 2489, 10870, 11, 1709, 807, 257, 2489, 10864, 2316, 293, 637, 1208, 484, 527, 5255, 13], "temperature": 0.0, "avg_logprob": -0.08867472344702416, "compression_ratio": 1.7142857142857142, "no_speech_prob": 5.539167204915429e-07}, {"id": 269, "seek": 187200, "start": 1891.76, "end": 1897.08, "text": " And then finally, the original target embedding we actually wanted is a photo of a bird spreading", "tokens": [400, 550, 2721, 11, 264, 3380, 3779, 12240, 3584, 321, 767, 1415, 307, 257, 5052, 295, 257, 5255, 15232], "temperature": 0.0, "avg_logprob": -0.08867472344702416, "compression_ratio": 1.7142857142857142, "no_speech_prob": 5.539167204915429e-07}, {"id": 270, "seek": 189708, "start": 1897.08, "end": 1902.6399999999999, "text": " its wings. We ended up with this slightly different embedding and we take the weighted", "tokens": [1080, 11405, 13, 492, 4590, 493, 365, 341, 4748, 819, 12240, 3584, 293, 321, 747, 264, 32807], "temperature": 0.0, "avg_logprob": -0.10128060295468284, "compression_ratio": 1.7054263565891472, "no_speech_prob": 3.6688527416117722e-06}, {"id": 271, "seek": 189708, "start": 1902.6399999999999, "end": 1907.8, "text": " average of the two. That's called the interpolate step, the weighted average of the two. And", "tokens": [4274, 295, 264, 732, 13, 663, 311, 1219, 264, 44902, 473, 1823, 11, 264, 32807, 4274, 295, 264, 732, 13, 400], "temperature": 0.0, "avg_logprob": -0.10128060295468284, "compression_ratio": 1.7054263565891472, "no_speech_prob": 3.6688527416117722e-06}, {"id": 272, "seek": 189708, "start": 1907.8, "end": 1916.8, "text": " we pass that through this fine tune diffusion model and we're done. And so that's pretty", "tokens": [321, 1320, 300, 807, 341, 2489, 10864, 25242, 2316, 293, 321, 434, 1096, 13, 400, 370, 300, 311, 1238], "temperature": 0.0, "avg_logprob": -0.10128060295468284, "compression_ratio": 1.7054263565891472, "no_speech_prob": 3.6688527416117722e-06}, {"id": 273, "seek": 189708, "start": 1916.8, "end": 1922.84, "text": " amazing. This, this would not take, I don't think a particularly long time or require", "tokens": [2243, 13, 639, 11, 341, 576, 406, 747, 11, 286, 500, 380, 519, 257, 4098, 938, 565, 420, 3651], "temperature": 0.0, "avg_logprob": -0.10128060295468284, "compression_ratio": 1.7054263565891472, "no_speech_prob": 3.6688527416117722e-06}, {"id": 274, "seek": 189708, "start": 1922.84, "end": 1927.04, "text": " any particular special hardware. It's the kind of thing I expect people will be doing", "tokens": [604, 1729, 2121, 8837, 13, 467, 311, 264, 733, 295, 551, 286, 2066, 561, 486, 312, 884], "temperature": 0.0, "avg_logprob": -0.10128060295468284, "compression_ratio": 1.7054263565891472, "no_speech_prob": 3.6688527416117722e-06}, {"id": 275, "seek": 192704, "start": 1927.04, "end": 1934.8, "text": " in the coming days and weeks. But it's very interesting because, yeah, I mean the ability", "tokens": [294, 264, 1348, 1708, 293, 3259, 13, 583, 309, 311, 588, 1880, 570, 11, 1338, 11, 286, 914, 264, 3485], "temperature": 0.0, "avg_logprob": -0.15234094858169556, "compression_ratio": 1.5290697674418605, "no_speech_prob": 5.014715952711413e-06}, {"id": 276, "seek": 192704, "start": 1934.8, "end": 1945.92, "text": " to take any photo of a person or whatever and change it, like literally change what", "tokens": [281, 747, 604, 5052, 295, 257, 954, 420, 2035, 293, 1319, 309, 11, 411, 3736, 1319, 437], "temperature": 0.0, "avg_logprob": -0.15234094858169556, "compression_ratio": 1.5290697674418605, "no_speech_prob": 5.014715952711413e-06}, {"id": 277, "seek": 192704, "start": 1945.92, "end": 1956.1599999999999, "text": " the person's doing is, you know, societally very important and really means that anybody,", "tokens": [264, 954, 311, 884, 307, 11, 291, 458, 11, 14051, 379, 588, 1021, 293, 534, 1355, 300, 4472, 11], "temperature": 0.0, "avg_logprob": -0.15234094858169556, "compression_ratio": 1.5290697674418605, "no_speech_prob": 5.014715952711413e-06}, {"id": 278, "seek": 195616, "start": 1956.16, "end": 1965.52, "text": " I guess now can generate believable photos that never actually existed. I see Jono in", "tokens": [286, 2041, 586, 393, 8460, 1351, 17915, 5787, 300, 1128, 767, 13135, 13, 286, 536, 7745, 78, 294], "temperature": 0.0, "avg_logprob": -0.17550605052226298, "compression_ratio": 1.454054054054054, "no_speech_prob": 8.801026524452027e-06}, {"id": 279, "seek": 195616, "start": 1965.52, "end": 1974.0, "text": " the chat saying that took about eight minutes to do it for Imogen on TPUs. Although Imogen", "tokens": [264, 5081, 1566, 300, 1890, 466, 3180, 2077, 281, 360, 309, 337, 4331, 8799, 322, 314, 8115, 82, 13, 5780, 4331, 8799], "temperature": 0.0, "avg_logprob": -0.17550605052226298, "compression_ratio": 1.454054054054054, "no_speech_prob": 8.801026524452027e-06}, {"id": 280, "seek": 195616, "start": 1974.0, "end": 1982.76, "text": " is quite a slow, big model, although the TPUs they used were very, the latest TPUs. So might", "tokens": [307, 1596, 257, 2964, 11, 955, 2316, 11, 4878, 264, 314, 8115, 82, 436, 1143, 645, 588, 11, 264, 6792, 314, 8115, 82, 13, 407, 1062], "temperature": 0.0, "avg_logprob": -0.17550605052226298, "compression_ratio": 1.454054054054054, "no_speech_prob": 8.801026524452027e-06}, {"id": 281, "seek": 198276, "start": 1982.76, "end": 1994.12, "text": " be, you know, maybe it's an hour or something for stable diffusion on GPUs. All right. So", "tokens": [312, 11, 291, 458, 11, 1310, 309, 311, 364, 1773, 420, 746, 337, 8351, 25242, 322, 18407, 82, 13, 1057, 558, 13, 407], "temperature": 0.0, "avg_logprob": -0.12705571563155563, "compression_ratio": 1.3507462686567164, "no_speech_prob": 9.874598845271976e-07}, {"id": 282, "seek": 198276, "start": 1994.12, "end": 2009.84, "text": " that is a lot of fun. All right. So with that, let's go back to our notebook. Where we left", "tokens": [300, 307, 257, 688, 295, 1019, 13, 1057, 558, 13, 407, 365, 300, 11, 718, 311, 352, 646, 281, 527, 21060, 13, 2305, 321, 1411], "temperature": 0.0, "avg_logprob": -0.12705571563155563, "compression_ratio": 1.3507462686567164, "no_speech_prob": 9.874598845271976e-07}, {"id": 283, "seek": 200984, "start": 2009.84, "end": 2014.1999999999998, "text": " it last time, we had kind of looked at some applications that we can play with in this", "tokens": [309, 1036, 565, 11, 321, 632, 733, 295, 2956, 412, 512, 5821, 300, 321, 393, 862, 365, 294, 341], "temperature": 0.0, "avg_logprob": -0.1801433563232422, "compression_ratio": 1.5213675213675213, "no_speech_prob": 4.860404715145705e-06}, {"id": 284, "seek": 200984, "start": 2014.1999999999998, "end": 2022.8, "text": " diffusion NBs repo in the stable diffusion notebook. And what we've got now, and remind", "tokens": [25242, 426, 33, 82, 49040, 294, 264, 8351, 25242, 21060, 13, 400, 437, 321, 600, 658, 586, 11, 293, 4160], "temperature": 0.0, "avg_logprob": -0.1801433563232422, "compression_ratio": 1.5213675213675213, "no_speech_prob": 4.860404715145705e-06}, {"id": 285, "seek": 200984, "start": 2022.8, "end": 2027.6799999999998, "text": " you when I say we, it's mainly actually Pedro, Patrick and Suraj, just a little bit of help", "tokens": [291, 562, 286, 584, 321, 11, 309, 311, 8704, 767, 26662, 11, 13980, 293, 6732, 1805, 11, 445, 257, 707, 857, 295, 854], "temperature": 0.0, "avg_logprob": -0.1801433563232422, "compression_ratio": 1.5213675213675213, "no_speech_prob": 4.860404715145705e-06}, {"id": 286, "seek": 200984, "start": 2027.6799999999998, "end": 2034.8, "text": " from me. So hugging face folks. What we slash they have done is they're starting, is they", "tokens": [490, 385, 13, 407, 41706, 1851, 4024, 13, 708, 321, 17330, 436, 362, 1096, 307, 436, 434, 2891, 11, 307, 436], "temperature": 0.0, "avg_logprob": -0.1801433563232422, "compression_ratio": 1.5213675213675213, "no_speech_prob": 4.860404715145705e-06}, {"id": 287, "seek": 203480, "start": 2034.8, "end": 2041.44, "text": " now dig into the pipeline to pull it all apart step by step. So you can see exactly what", "tokens": [586, 2528, 666, 264, 15517, 281, 2235, 309, 439, 4936, 1823, 538, 1823, 13, 407, 291, 393, 536, 2293, 437], "temperature": 0.0, "avg_logprob": -0.1230912750417536, "compression_ratio": 1.6359447004608294, "no_speech_prob": 1.4970839401939884e-05}, {"id": 288, "seek": 203480, "start": 2041.44, "end": 2048.4, "text": " happens. The first thing I was just going to mention is this is how you can create those", "tokens": [2314, 13, 440, 700, 551, 286, 390, 445, 516, 281, 2152, 307, 341, 307, 577, 291, 393, 1884, 729], "temperature": 0.0, "avg_logprob": -0.1230912750417536, "compression_ratio": 1.6359447004608294, "no_speech_prob": 1.4970839401939884e-05}, {"id": 289, "seek": 203480, "start": 2048.4, "end": 2055.7599999999998, "text": " gradual denoising pictures. And this is thanks to something called the callback. So you can", "tokens": [32890, 1441, 78, 3436, 5242, 13, 400, 341, 307, 3231, 281, 746, 1219, 264, 818, 3207, 13, 407, 291, 393], "temperature": 0.0, "avg_logprob": -0.1230912750417536, "compression_ratio": 1.6359447004608294, "no_speech_prob": 1.4970839401939884e-05}, {"id": 290, "seek": 203480, "start": 2055.7599999999998, "end": 2064.08, "text": " say here, when you go through the pipeline, every 12 steps call this function. And as", "tokens": [584, 510, 11, 562, 291, 352, 807, 264, 15517, 11, 633, 2272, 4439, 818, 341, 2445, 13, 400, 382], "temperature": 0.0, "avg_logprob": -0.1230912750417536, "compression_ratio": 1.6359447004608294, "no_speech_prob": 1.4970839401939884e-05}, {"id": 291, "seek": 206408, "start": 2064.08, "end": 2070.44, "text": " you can see, it's going to call it with I and T and the latency. And so then we can", "tokens": [291, 393, 536, 11, 309, 311, 516, 281, 818, 309, 365, 286, 293, 314, 293, 264, 27043, 13, 400, 370, 550, 321, 393], "temperature": 0.0, "avg_logprob": -0.11831391242242628, "compression_ratio": 1.6338028169014085, "no_speech_prob": 8.664537745062262e-06}, {"id": 292, "seek": 206408, "start": 2070.44, "end": 2077.68, "text": " just make an image and stick it on the end of a array. And that's all that's happening", "tokens": [445, 652, 364, 3256, 293, 2897, 309, 322, 264, 917, 295, 257, 10225, 13, 400, 300, 311, 439, 300, 311, 2737], "temperature": 0.0, "avg_logprob": -0.11831391242242628, "compression_ratio": 1.6338028169014085, "no_speech_prob": 8.664537745062262e-06}, {"id": 293, "seek": 206408, "start": 2077.68, "end": 2085.42, "text": " here. Right. So this is how you can start to interact with a pipeline without rewriting", "tokens": [510, 13, 1779, 13, 407, 341, 307, 577, 291, 393, 722, 281, 4648, 365, 257, 15517, 1553, 319, 19868], "temperature": 0.0, "avg_logprob": -0.11831391242242628, "compression_ratio": 1.6338028169014085, "no_speech_prob": 8.664537745062262e-06}, {"id": 294, "seek": 206408, "start": 2085.42, "end": 2088.64, "text": " it yourself from scratch. But now what we're going to do is we're actually going to write", "tokens": [309, 1803, 490, 8459, 13, 583, 586, 437, 321, 434, 516, 281, 360, 307, 321, 434, 767, 516, 281, 2464], "temperature": 0.0, "avg_logprob": -0.11831391242242628, "compression_ratio": 1.6338028169014085, "no_speech_prob": 8.664537745062262e-06}, {"id": 295, "seek": 208864, "start": 2088.64, "end": 2094.48, "text": " it, we're going to build it from scratch. So you don't actually have to use a callback", "tokens": [309, 11, 321, 434, 516, 281, 1322, 309, 490, 8459, 13, 407, 291, 500, 380, 767, 362, 281, 764, 257, 818, 3207], "temperature": 0.0, "avg_logprob": -0.1111254029803806, "compression_ratio": 1.5917159763313609, "no_speech_prob": 6.048881459719269e-06}, {"id": 296, "seek": 208864, "start": 2094.48, "end": 2102.8399999999997, "text": " because you'll be able to change it yourself. So let's take a look. So looking inside the", "tokens": [570, 291, 603, 312, 1075, 281, 1319, 309, 1803, 13, 407, 718, 311, 747, 257, 574, 13, 407, 1237, 1854, 264], "temperature": 0.0, "avg_logprob": -0.1111254029803806, "compression_ratio": 1.5917159763313609, "no_speech_prob": 6.048881459719269e-06}, {"id": 297, "seek": 208864, "start": 2102.8399999999997, "end": 2109.52, "text": " pipeline, what exactly is going on? So what's going to be going on in the pipeline is seeing", "tokens": [15517, 11, 437, 2293, 307, 516, 322, 30, 407, 437, 311, 516, 281, 312, 516, 322, 294, 264, 15517, 307, 2577], "temperature": 0.0, "avg_logprob": -0.1111254029803806, "compression_ratio": 1.5917159763313609, "no_speech_prob": 6.048881459719269e-06}, {"id": 298, "seek": 210952, "start": 2109.52, "end": 2118.8, "text": " all of the steps that we saw in last week's OneNote notes that I drew. And it's going", "tokens": [439, 295, 264, 4439, 300, 321, 1866, 294, 1036, 1243, 311, 1485, 49250, 5570, 300, 286, 12804, 13, 400, 309, 311, 516], "temperature": 0.0, "avg_logprob": -0.12303806580219072, "compression_ratio": 1.6136363636363635, "no_speech_prob": 1.0188082342210691e-06}, {"id": 299, "seek": 210952, "start": 2118.8, "end": 2124.4, "text": " to be all the code. And we're not going to show the code of how each step is implemented.", "tokens": [281, 312, 439, 264, 3089, 13, 400, 321, 434, 406, 516, 281, 855, 264, 3089, 295, 577, 1184, 1823, 307, 12270, 13], "temperature": 0.0, "avg_logprob": -0.12303806580219072, "compression_ratio": 1.6136363636363635, "no_speech_prob": 1.0188082342210691e-06}, {"id": 300, "seek": 210952, "start": 2124.4, "end": 2130.88, "text": " So for example, the clip text model we talked about, the thing that takes as input a prompt", "tokens": [407, 337, 1365, 11, 264, 7353, 2487, 2316, 321, 2825, 466, 11, 264, 551, 300, 2516, 382, 4846, 257, 12391], "temperature": 0.0, "avg_logprob": -0.12303806580219072, "compression_ratio": 1.6136363636363635, "no_speech_prob": 1.0188082342210691e-06}, {"id": 301, "seek": 210952, "start": 2130.88, "end": 2136.36, "text": " and creates an embedding, we just take that as a given. Okay. So we download it. OpenAI", "tokens": [293, 7829, 364, 12240, 3584, 11, 321, 445, 747, 300, 382, 257, 2212, 13, 1033, 13, 407, 321, 5484, 309, 13, 7238, 48698], "temperature": 0.0, "avg_logprob": -0.12303806580219072, "compression_ratio": 1.6136363636363635, "no_speech_prob": 1.0188082342210691e-06}, {"id": 302, "seek": 213636, "start": 2136.36, "end": 2143.36, "text": " has trained one called ClipVIT Large Patch 14. So we just say fromdrychained. So Hugging", "tokens": [575, 8895, 472, 1219, 2033, 647, 53, 3927, 33092, 44359, 3499, 13, 407, 321, 445, 584, 490, 67, 627, 339, 3563, 13, 407, 46892, 3249], "temperature": 0.0, "avg_logprob": -0.19725977457486665, "compression_ratio": 1.592920353982301, "no_speech_prob": 7.2963421189342625e-06}, {"id": 303, "seek": 213636, "start": 2143.36, "end": 2153.36, "text": " Face will transform us, we'll download and create that model for us. Ditto for the tokenizer.", "tokens": [4047, 486, 4088, 505, 11, 321, 603, 5484, 293, 1884, 300, 2316, 337, 505, 13, 413, 34924, 337, 264, 14862, 6545, 13], "temperature": 0.0, "avg_logprob": -0.19725977457486665, "compression_ratio": 1.592920353982301, "no_speech_prob": 7.2963421189342625e-06}, {"id": 304, "seek": 213636, "start": 2153.36, "end": 2158.76, "text": " And so ditto for the autoencoder and ditto for the UNet. So there they all are. We can", "tokens": [400, 370, 274, 34924, 337, 264, 8399, 22660, 19866, 293, 274, 34924, 337, 264, 8229, 302, 13, 407, 456, 436, 439, 366, 13, 492, 393], "temperature": 0.0, "avg_logprob": -0.19725977457486665, "compression_ratio": 1.592920353982301, "no_speech_prob": 7.2963421189342625e-06}, {"id": 305, "seek": 213636, "start": 2158.76, "end": 2164.32, "text": " just grab them. So we just take that all as a given. These are the three models that we've", "tokens": [445, 4444, 552, 13, 407, 321, 445, 747, 300, 439, 382, 257, 2212, 13, 1981, 366, 264, 1045, 5245, 300, 321, 600], "temperature": 0.0, "avg_logprob": -0.19725977457486665, "compression_ratio": 1.592920353982301, "no_speech_prob": 7.2963421189342625e-06}, {"id": 306, "seek": 216432, "start": 2164.32, "end": 2174.0800000000004, "text": " talked about. The text encoder, the clip encoder, the VAE and the UNet. So there they are. So", "tokens": [2825, 466, 13, 440, 2487, 2058, 19866, 11, 264, 7353, 2058, 19866, 11, 264, 18527, 36, 293, 264, 8229, 302, 13, 407, 456, 436, 366, 13, 407], "temperature": 0.0, "avg_logprob": -0.10963403362117402, "compression_ratio": 1.5222222222222221, "no_speech_prob": 5.862791113031562e-06}, {"id": 307, "seek": 216432, "start": 2174.0800000000004, "end": 2180.0800000000004, "text": " given that we now have those, the next thing we need is that thing that converts time steps", "tokens": [2212, 300, 321, 586, 362, 729, 11, 264, 958, 551, 321, 643, 307, 300, 551, 300, 38874, 565, 4439], "temperature": 0.0, "avg_logprob": -0.10963403362117402, "compression_ratio": 1.5222222222222221, "no_speech_prob": 5.862791113031562e-06}, {"id": 308, "seek": 216432, "start": 2180.0800000000004, "end": 2189.44, "text": " into the amount of noise. Remember that graph we drew? And so we can basically again use", "tokens": [666, 264, 2372, 295, 5658, 13, 5459, 300, 4295, 321, 12804, 30, 400, 370, 321, 393, 1936, 797, 764], "temperature": 0.0, "avg_logprob": -0.10963403362117402, "compression_ratio": 1.5222222222222221, "no_speech_prob": 5.862791113031562e-06}, {"id": 309, "seek": 218944, "start": 2189.44, "end": 2194.68, "text": " something that Hugging Face, well actually in this case, Katherine Carlson has already", "tokens": [746, 300, 46892, 3249, 4047, 11, 731, 767, 294, 341, 1389, 11, 33478, 14256, 3015, 575, 1217], "temperature": 0.0, "avg_logprob": -0.1676545946785573, "compression_ratio": 1.6542056074766356, "no_speech_prob": 2.4824732918204973e-06}, {"id": 310, "seek": 218944, "start": 2194.68, "end": 2201.92, "text": " provided which is a scheduler. It's basically something that shows us that connection. So", "tokens": [5649, 597, 307, 257, 12000, 260, 13, 467, 311, 1936, 746, 300, 3110, 505, 300, 4984, 13, 407], "temperature": 0.0, "avg_logprob": -0.1676545946785573, "compression_ratio": 1.6542056074766356, "no_speech_prob": 2.4824732918204973e-06}, {"id": 311, "seek": 218944, "start": 2201.92, "end": 2210.44, "text": " we've got that. So we use that scheduler and we say how much noise we're using. And so", "tokens": [321, 600, 658, 300, 13, 407, 321, 764, 300, 12000, 260, 293, 321, 584, 577, 709, 5658, 321, 434, 1228, 13, 400, 370], "temperature": 0.0, "avg_logprob": -0.1676545946785573, "compression_ratio": 1.6542056074766356, "no_speech_prob": 2.4824732918204973e-06}, {"id": 312, "seek": 218944, "start": 2210.44, "end": 2216.4, "text": " we have to make sure that that matches. And so we just use these numbers that we're given.", "tokens": [321, 362, 281, 652, 988, 300, 300, 10676, 13, 400, 370, 321, 445, 764, 613, 3547, 300, 321, 434, 2212, 13], "temperature": 0.0, "avg_logprob": -0.1676545946785573, "compression_ratio": 1.6542056074766356, "no_speech_prob": 2.4824732918204973e-06}, {"id": 313, "seek": 221640, "start": 2216.4, "end": 2224.52, "text": " Okay, so now to create our photograph of an astronaut riding a horse again in 70 steps", "tokens": [1033, 11, 370, 586, 281, 1884, 527, 8348, 295, 364, 18516, 9546, 257, 6832, 797, 294, 5285, 4439], "temperature": 0.0, "avg_logprob": -0.10900067770352928, "compression_ratio": 1.551111111111111, "no_speech_prob": 1.8342689145356417e-05}, {"id": 314, "seek": 221640, "start": 2224.52, "end": 2231.56, "text": " with a 7.5 guidance scale, batch size of one. Step number one is to take our prompt and", "tokens": [365, 257, 1614, 13, 20, 10056, 4373, 11, 15245, 2744, 295, 472, 13, 5470, 1230, 472, 307, 281, 747, 527, 12391, 293], "temperature": 0.0, "avg_logprob": -0.10900067770352928, "compression_ratio": 1.551111111111111, "no_speech_prob": 1.8342689145356417e-05}, {"id": 315, "seek": 221640, "start": 2231.56, "end": 2238.04, "text": " tokenize it. Okay, so we looked at that in part one of the course. So check that out", "tokens": [14862, 1125, 309, 13, 1033, 11, 370, 321, 2956, 412, 300, 294, 644, 472, 295, 264, 1164, 13, 407, 1520, 300, 484], "temperature": 0.0, "avg_logprob": -0.10900067770352928, "compression_ratio": 1.551111111111111, "no_speech_prob": 1.8342689145356417e-05}, {"id": 316, "seek": 221640, "start": 2238.04, "end": 2241.44, "text": " if you can't remember what tokenizing does, but it's just splitting it in, basically it's", "tokens": [498, 291, 393, 380, 1604, 437, 14862, 3319, 775, 11, 457, 309, 311, 445, 30348, 309, 294, 11, 1936, 309, 311], "temperature": 0.0, "avg_logprob": -0.10900067770352928, "compression_ratio": 1.551111111111111, "no_speech_prob": 1.8342689145356417e-05}, {"id": 317, "seek": 224144, "start": 2241.44, "end": 2248.36, "text": " splitting it into words or subword units if they're long and unusual words. So here are,", "tokens": [30348, 309, 666, 2283, 420, 1422, 7462, 6815, 498, 436, 434, 938, 293, 10901, 2283, 13, 407, 510, 366, 11], "temperature": 0.0, "avg_logprob": -0.12964009402091042, "compression_ratio": 1.7081712062256809, "no_speech_prob": 4.4254702515900135e-06}, {"id": 318, "seek": 224144, "start": 2248.36, "end": 2254.16, "text": " so this will be the start of sentence token and this will be our photograph of an astronaut,", "tokens": [370, 341, 486, 312, 264, 722, 295, 8174, 14862, 293, 341, 486, 312, 527, 8348, 295, 364, 18516, 11], "temperature": 0.0, "avg_logprob": -0.12964009402091042, "compression_ratio": 1.7081712062256809, "no_speech_prob": 4.4254702515900135e-06}, {"id": 319, "seek": 224144, "start": 2254.16, "end": 2258.7200000000003, "text": " etc. And then you can see the same token is repeated again and again at the end. That's", "tokens": [5183, 13, 400, 550, 291, 393, 536, 264, 912, 14862, 307, 10477, 797, 293, 797, 412, 264, 917, 13, 663, 311], "temperature": 0.0, "avg_logprob": -0.12964009402091042, "compression_ratio": 1.7081712062256809, "no_speech_prob": 4.4254702515900135e-06}, {"id": 320, "seek": 224144, "start": 2258.7200000000003, "end": 2265.68, "text": " just the padding to say we're all done. And the reason for that is that GPUs and TPUs", "tokens": [445, 264, 39562, 281, 584, 321, 434, 439, 1096, 13, 400, 264, 1778, 337, 300, 307, 300, 18407, 82, 293, 314, 8115, 82], "temperature": 0.0, "avg_logprob": -0.12964009402091042, "compression_ratio": 1.7081712062256809, "no_speech_prob": 4.4254702515900135e-06}, {"id": 321, "seek": 224144, "start": 2265.68, "end": 2270.88, "text": " really like to do lots of things at once. So we kind of have everything be the same", "tokens": [534, 411, 281, 360, 3195, 295, 721, 412, 1564, 13, 407, 321, 733, 295, 362, 1203, 312, 264, 912], "temperature": 0.0, "avg_logprob": -0.12964009402091042, "compression_ratio": 1.7081712062256809, "no_speech_prob": 4.4254702515900135e-06}, {"id": 322, "seek": 227088, "start": 2270.88, "end": 2277.12, "text": " length by padding them. That may sound like a lot of wasted work, which it kind of is,", "tokens": [4641, 538, 39562, 552, 13, 663, 815, 1626, 411, 257, 688, 295, 19496, 589, 11, 597, 309, 733, 295, 307, 11], "temperature": 0.0, "avg_logprob": -0.09741600610876597, "compression_ratio": 1.5555555555555556, "no_speech_prob": 4.02943987865001e-06}, {"id": 323, "seek": 227088, "start": 2277.12, "end": 2282.84, "text": " but a GPU would rather do lots of things at the same time on exactly the same sized input.", "tokens": [457, 257, 18407, 576, 2831, 360, 3195, 295, 721, 412, 264, 912, 565, 322, 2293, 264, 912, 20004, 4846, 13], "temperature": 0.0, "avg_logprob": -0.09741600610876597, "compression_ratio": 1.5555555555555556, "no_speech_prob": 4.02943987865001e-06}, {"id": 324, "seek": 227088, "start": 2282.84, "end": 2288.92, "text": " So this is why we have all this padding. So you can see here if we decode that number,", "tokens": [407, 341, 307, 983, 321, 362, 439, 341, 39562, 13, 407, 291, 393, 536, 510, 498, 321, 979, 1429, 300, 1230, 11], "temperature": 0.0, "avg_logprob": -0.09741600610876597, "compression_ratio": 1.5555555555555556, "no_speech_prob": 4.02943987865001e-06}, {"id": 325, "seek": 227088, "start": 2288.92, "end": 2296.1600000000003, "text": " it's the end of text marker, just padding really in this case. As well as getting the", "tokens": [309, 311, 264, 917, 295, 2487, 15247, 11, 445, 39562, 534, 294, 341, 1389, 13, 1018, 731, 382, 1242, 264], "temperature": 0.0, "avg_logprob": -0.09741600610876597, "compression_ratio": 1.5555555555555556, "no_speech_prob": 4.02943987865001e-06}, {"id": 326, "seek": 229616, "start": 2296.16, "end": 2302.08, "text": " input IDs, so these are just lookups into a vocabulary. There's also a mask, which is", "tokens": [4846, 48212, 11, 370, 613, 366, 445, 574, 7528, 666, 257, 19864, 13, 821, 311, 611, 257, 6094, 11, 597, 307], "temperature": 0.0, "avg_logprob": -0.11478157043457031, "compression_ratio": 1.5310734463276836, "no_speech_prob": 1.628048380553082e-06}, {"id": 327, "seek": 229616, "start": 2302.08, "end": 2310.3599999999997, "text": " just telling it which one's actual words as opposed to padding, which is not very interesting.", "tokens": [445, 3585, 309, 597, 472, 311, 3539, 2283, 382, 8851, 281, 39562, 11, 597, 307, 406, 588, 1880, 13], "temperature": 0.0, "avg_logprob": -0.11478157043457031, "compression_ratio": 1.5310734463276836, "no_speech_prob": 1.628048380553082e-06}, {"id": 328, "seek": 229616, "start": 2310.3599999999997, "end": 2318.68, "text": " So we can now take those input IDs, we can put them on the GPU and we can run them through", "tokens": [407, 321, 393, 586, 747, 729, 4846, 48212, 11, 321, 393, 829, 552, 322, 264, 18407, 293, 321, 393, 1190, 552, 807], "temperature": 0.0, "avg_logprob": -0.11478157043457031, "compression_ratio": 1.5310734463276836, "no_speech_prob": 1.628048380553082e-06}, {"id": 329, "seek": 231868, "start": 2318.68, "end": 2326.7599999999998, "text": " the Clip Encoder. And so for a batch size of one, so you've got one image, that gives", "tokens": [264, 2033, 647, 29584, 19866, 13, 400, 370, 337, 257, 15245, 2744, 295, 472, 11, 370, 291, 600, 658, 472, 3256, 11, 300, 2709], "temperature": 0.0, "avg_logprob": -0.1270691714709318, "compression_ratio": 1.5172413793103448, "no_speech_prob": 2.7264632080914453e-06}, {"id": 330, "seek": 231868, "start": 2326.7599999999998, "end": 2337.3999999999996, "text": " us back a 77 by 768 because we've got 77 here and each one of those creates a 768 long vector.", "tokens": [505, 646, 257, 25546, 538, 24733, 23, 570, 321, 600, 658, 25546, 510, 293, 1184, 472, 295, 729, 7829, 257, 24733, 23, 938, 8062, 13], "temperature": 0.0, "avg_logprob": -0.1270691714709318, "compression_ratio": 1.5172413793103448, "no_speech_prob": 2.7264632080914453e-06}, {"id": 331, "seek": 231868, "start": 2337.3999999999996, "end": 2344.98, "text": " So we've got a 77 by 768 tensor. So these are the embeddings for a photograph of an", "tokens": [407, 321, 600, 658, 257, 25546, 538, 24733, 23, 40863, 13, 407, 613, 366, 264, 12240, 29432, 337, 257, 8348, 295, 364], "temperature": 0.0, "avg_logprob": -0.1270691714709318, "compression_ratio": 1.5172413793103448, "no_speech_prob": 2.7264632080914453e-06}, {"id": 332, "seek": 234498, "start": 2344.98, "end": 2350.86, "text": " astronaut riding a horse that come from Clip. So remember everything's pre-trained, so that's", "tokens": [18516, 9546, 257, 6832, 300, 808, 490, 2033, 647, 13, 407, 1604, 1203, 311, 659, 12, 17227, 2001, 11, 370, 300, 311], "temperature": 0.0, "avg_logprob": -0.12487342406292351, "compression_ratio": 1.3525179856115108, "no_speech_prob": 3.555970124580199e-06}, {"id": 333, "seek": 234498, "start": 2350.86, "end": 2358.64, "text": " all done for us, we're just doing inference. And so remember for the classifier free guidance,", "tokens": [439, 1096, 337, 505, 11, 321, 434, 445, 884, 38253, 13, 400, 370, 1604, 337, 264, 1508, 9902, 1737, 10056, 11], "temperature": 0.0, "avg_logprob": -0.12487342406292351, "compression_ratio": 1.3525179856115108, "no_speech_prob": 3.555970124580199e-06}, {"id": 334, "seek": 235864, "start": 2358.64, "end": 2376.8799999999997, "text": " we also need the embeddings for the empty string, so we do exactly the same thing. So", "tokens": [321, 611, 643, 264, 12240, 29432, 337, 264, 6707, 6798, 11, 370, 321, 360, 2293, 264, 912, 551, 13, 407], "temperature": 0.0, "avg_logprob": -0.10393928119114466, "compression_ratio": 1.606060606060606, "no_speech_prob": 2.406090061413124e-06}, {"id": 335, "seek": 235864, "start": 2376.8799999999997, "end": 2381.56, "text": " now we just concatenate those two together because we're just, this is just a trick to", "tokens": [586, 321, 445, 1588, 7186, 473, 729, 732, 1214, 570, 321, 434, 445, 11, 341, 307, 445, 257, 4282, 281], "temperature": 0.0, "avg_logprob": -0.10393928119114466, "compression_ratio": 1.606060606060606, "no_speech_prob": 2.406090061413124e-06}, {"id": 336, "seek": 235864, "start": 2381.56, "end": 2385.74, "text": " get the GPU to do both at the same time because we like the GPU to do as many things at once", "tokens": [483, 264, 18407, 281, 360, 1293, 412, 264, 912, 565, 570, 321, 411, 264, 18407, 281, 360, 382, 867, 721, 412, 1564], "temperature": 0.0, "avg_logprob": -0.10393928119114466, "compression_ratio": 1.606060606060606, "no_speech_prob": 2.406090061413124e-06}, {"id": 337, "seek": 238574, "start": 2385.74, "end": 2396.04, "text": " as possible. And so now we create our noise. And because we're doing it with a VAE, we", "tokens": [382, 1944, 13, 400, 370, 586, 321, 1884, 527, 5658, 13, 400, 570, 321, 434, 884, 309, 365, 257, 18527, 36, 11, 321], "temperature": 0.0, "avg_logprob": -0.1640593973795573, "compression_ratio": 1.4972375690607735, "no_speech_prob": 2.260316023239284e-06}, {"id": 338, "seek": 238574, "start": 2396.04, "end": 2402.3199999999997, "text": " can call it Latence, but it's just noise really. I wonder if you'd still call it that without", "tokens": [393, 818, 309, 7354, 655, 11, 457, 309, 311, 445, 5658, 534, 13, 286, 2441, 498, 291, 1116, 920, 818, 309, 300, 1553], "temperature": 0.0, "avg_logprob": -0.1640593973795573, "compression_ratio": 1.4972375690607735, "no_speech_prob": 2.260316023239284e-06}, {"id": 339, "seek": 238574, "start": 2402.3199999999997, "end": 2409.2799999999997, "text": " the VAE. Maybe you would have to think about that. So that's just random numbers, normally", "tokens": [264, 18527, 36, 13, 2704, 291, 576, 362, 281, 519, 466, 300, 13, 407, 300, 311, 445, 4974, 3547, 11, 5646], "temperature": 0.0, "avg_logprob": -0.1640593973795573, "compression_ratio": 1.4972375690607735, "no_speech_prob": 2.260316023239284e-06}, {"id": 340, "seek": 240928, "start": 2409.28, "end": 2416.0, "text": " generated, normally distributed random numbers of size one, that's our batch size. And the", "tokens": [10833, 11, 5646, 12631, 4974, 3547, 295, 2744, 472, 11, 300, 311, 527, 15245, 2744, 13, 400, 264], "temperature": 0.0, "avg_logprob": -0.10638692461211106, "compression_ratio": 1.651685393258427, "no_speech_prob": 7.766898306726944e-06}, {"id": 341, "seek": 240928, "start": 2416.0, "end": 2420.48, "text": " reason that we've got this divided by eight here is because that's what the VAE does.", "tokens": [1778, 300, 321, 600, 658, 341, 6666, 538, 3180, 510, 307, 570, 300, 311, 437, 264, 18527, 36, 775, 13], "temperature": 0.0, "avg_logprob": -0.10638692461211106, "compression_ratio": 1.651685393258427, "no_speech_prob": 7.766898306726944e-06}, {"id": 342, "seek": 240928, "start": 2420.48, "end": 2425.2000000000003, "text": " It allows us to create things that are eight times smaller by height and width, and then", "tokens": [467, 4045, 505, 281, 1884, 721, 300, 366, 3180, 1413, 4356, 538, 6681, 293, 11402, 11, 293, 550], "temperature": 0.0, "avg_logprob": -0.10638692461211106, "compression_ratio": 1.651685393258427, "no_speech_prob": 7.766898306726944e-06}, {"id": 343, "seek": 240928, "start": 2425.2000000000003, "end": 2431.0, "text": " it's going to expand it up again for us later. That's why this is so much faster. You'll", "tokens": [309, 311, 516, 281, 5268, 309, 493, 797, 337, 505, 1780, 13, 663, 311, 983, 341, 307, 370, 709, 4663, 13, 509, 603], "temperature": 0.0, "avg_logprob": -0.10638692461211106, "compression_ratio": 1.651685393258427, "no_speech_prob": 7.766898306726944e-06}, {"id": 344, "seek": 240928, "start": 2431.0, "end": 2435.6800000000003, "text": " see a lot of this, after we put it on the GPU, you'll see a lot of this dot half. This", "tokens": [536, 257, 688, 295, 341, 11, 934, 321, 829, 309, 322, 264, 18407, 11, 291, 603, 536, 257, 688, 295, 341, 5893, 1922, 13, 639], "temperature": 0.0, "avg_logprob": -0.10638692461211106, "compression_ratio": 1.651685393258427, "no_speech_prob": 7.766898306726944e-06}, {"id": 345, "seek": 243568, "start": 2435.68, "end": 2441.56, "text": " is converting things into what's called half precision or FB16. Details don't matter too", "tokens": [307, 29942, 721, 666, 437, 311, 1219, 1922, 18356, 420, 479, 33, 6866, 13, 42811, 500, 380, 1871, 886], "temperature": 0.0, "avg_logprob": -0.13007790143372583, "compression_ratio": 1.6816479400749065, "no_speech_prob": 3.6119627111474983e-06}, {"id": 346, "seek": 243568, "start": 2441.56, "end": 2447.9199999999996, "text": " much. It's just making it half as big in memory by using less precision. Modern GPUs are much,", "tokens": [709, 13, 467, 311, 445, 1455, 309, 1922, 382, 955, 294, 4675, 538, 1228, 1570, 18356, 13, 19814, 18407, 82, 366, 709, 11], "temperature": 0.0, "avg_logprob": -0.13007790143372583, "compression_ratio": 1.6816479400749065, "no_speech_prob": 3.6119627111474983e-06}, {"id": 347, "seek": 243568, "start": 2447.9199999999996, "end": 2453.18, "text": " much, much, much faster if we do that. So you'll see that a lot. If you use something", "tokens": [709, 11, 709, 11, 709, 4663, 498, 321, 360, 300, 13, 407, 291, 603, 536, 300, 257, 688, 13, 759, 291, 764, 746], "temperature": 0.0, "avg_logprob": -0.13007790143372583, "compression_ratio": 1.6816479400749065, "no_speech_prob": 3.6119627111474983e-06}, {"id": 348, "seek": 243568, "start": 2453.18, "end": 2457.6, "text": " like fast AI, you don't have to worry about it. All this stuff is done for you. And we'll", "tokens": [411, 2370, 7318, 11, 291, 500, 380, 362, 281, 3292, 466, 309, 13, 1057, 341, 1507, 307, 1096, 337, 291, 13, 400, 321, 603], "temperature": 0.0, "avg_logprob": -0.13007790143372583, "compression_ratio": 1.6816479400749065, "no_speech_prob": 3.6119627111474983e-06}, {"id": 349, "seek": 243568, "start": 2457.6, "end": 2465.2, "text": " see that later as we rebuild this with much, much less code later in the course. So we'll", "tokens": [536, 300, 1780, 382, 321, 16877, 341, 365, 709, 11, 709, 1570, 3089, 1780, 294, 264, 1164, 13, 407, 321, 603], "temperature": 0.0, "avg_logprob": -0.13007790143372583, "compression_ratio": 1.6816479400749065, "no_speech_prob": 3.6119627111474983e-06}, {"id": 350, "seek": 246520, "start": 2465.2, "end": 2472.04, "text": " be building our own kind of framework from scratch, which you'll then be able to maintain", "tokens": [312, 2390, 527, 1065, 733, 295, 8388, 490, 8459, 11, 597, 291, 603, 550, 312, 1075, 281, 6909], "temperature": 0.0, "avg_logprob": -0.11252803152257745, "compression_ratio": 1.5363636363636364, "no_speech_prob": 4.425471615832066e-06}, {"id": 351, "seek": 246520, "start": 2472.04, "end": 2480.22, "text": " and work with yourself. Okay. So we have to say we want to do 70 steps.", "tokens": [293, 589, 365, 1803, 13, 1033, 13, 407, 321, 362, 281, 584, 321, 528, 281, 360, 5285, 4439, 13], "temperature": 0.0, "avg_logprob": -0.11252803152257745, "compression_ratio": 1.5363636363636364, "no_speech_prob": 4.425471615832066e-06}, {"id": 352, "seek": 246520, "start": 2480.22, "end": 2484.04, "text": " Something that's very important, we won't worry too much about the details right now,", "tokens": [6595, 300, 311, 588, 1021, 11, 321, 1582, 380, 3292, 886, 709, 466, 264, 4365, 558, 586, 11], "temperature": 0.0, "avg_logprob": -0.11252803152257745, "compression_ratio": 1.5363636363636364, "no_speech_prob": 4.425471615832066e-06}, {"id": 353, "seek": 246520, "start": 2484.04, "end": 2489.3999999999996, "text": " but this, what you see here is that we take our round of noise and we scale it. And that's", "tokens": [457, 341, 11, 437, 291, 536, 510, 307, 300, 321, 747, 527, 3098, 295, 5658, 293, 321, 4373, 309, 13, 400, 300, 311], "temperature": 0.0, "avg_logprob": -0.11252803152257745, "compression_ratio": 1.5363636363636364, "no_speech_prob": 4.425471615832066e-06}, {"id": 354, "seek": 248940, "start": 2489.4, "end": 2496.08, "text": " because depending on what stage you're up to, you need to make sure that kind of you", "tokens": [570, 5413, 322, 437, 3233, 291, 434, 493, 281, 11, 291, 643, 281, 652, 988, 300, 733, 295, 291], "temperature": 0.0, "avg_logprob": -0.09607098579406738, "compression_ratio": 1.6968503937007875, "no_speech_prob": 2.857304252756876e-06}, {"id": 355, "seek": 248940, "start": 2496.08, "end": 2501.4, "text": " have the right amount of variance basically. Otherwise you're going to get activations", "tokens": [362, 264, 558, 2372, 295, 21977, 1936, 13, 10328, 291, 434, 516, 281, 483, 2430, 763], "temperature": 0.0, "avg_logprob": -0.09607098579406738, "compression_ratio": 1.6968503937007875, "no_speech_prob": 2.857304252756876e-06}, {"id": 356, "seek": 248940, "start": 2501.4, "end": 2504.52, "text": " and gradients that go out of control. This is something we're going to be talking about", "tokens": [293, 2771, 2448, 300, 352, 484, 295, 1969, 13, 639, 307, 746, 321, 434, 516, 281, 312, 1417, 466], "temperature": 0.0, "avg_logprob": -0.09607098579406738, "compression_ratio": 1.6968503937007875, "no_speech_prob": 2.857304252756876e-06}, {"id": 357, "seek": 248940, "start": 2504.52, "end": 2508.84, "text": " a huge amount during this course, and we'll show you lots of tricks to handle that kind", "tokens": [257, 2603, 2372, 1830, 341, 1164, 11, 293, 321, 603, 855, 291, 3195, 295, 11733, 281, 4813, 300, 733], "temperature": 0.0, "avg_logprob": -0.09607098579406738, "compression_ratio": 1.6968503937007875, "no_speech_prob": 2.857304252756876e-06}, {"id": 358, "seek": 248940, "start": 2508.84, "end": 2515.42, "text": " of thing automatically. Unfortunately, at the moment in the stable diffusion world,", "tokens": [295, 551, 6772, 13, 8590, 11, 412, 264, 1623, 294, 264, 8351, 25242, 1002, 11], "temperature": 0.0, "avg_logprob": -0.09607098579406738, "compression_ratio": 1.6968503937007875, "no_speech_prob": 2.857304252756876e-06}, {"id": 359, "seek": 251542, "start": 2515.42, "end": 2522.8, "text": " this is all done in rather, in my opinion, kind of ways that are too tied to the details", "tokens": [341, 307, 439, 1096, 294, 2831, 11, 294, 452, 4800, 11, 733, 295, 2098, 300, 366, 886, 9601, 281, 264, 4365], "temperature": 0.0, "avg_logprob": -0.08804636383056641, "compression_ratio": 1.5857142857142856, "no_speech_prob": 3.187544734828407e-06}, {"id": 360, "seek": 251542, "start": 2522.8, "end": 2527.7200000000003, "text": " of the model. I think we will be able to improve it as the course goes on. But for now we'll", "tokens": [295, 264, 2316, 13, 286, 519, 321, 486, 312, 1075, 281, 3470, 309, 382, 264, 1164, 1709, 322, 13, 583, 337, 586, 321, 603], "temperature": 0.0, "avg_logprob": -0.08804636383056641, "compression_ratio": 1.5857142857142856, "no_speech_prob": 3.187544734828407e-06}, {"id": 361, "seek": 251542, "start": 2527.7200000000003, "end": 2533.56, "text": " stick with how everybody else is doing it. This is how they do it. So we're going to", "tokens": [2897, 365, 577, 2201, 1646, 307, 884, 309, 13, 639, 307, 577, 436, 360, 309, 13, 407, 321, 434, 516, 281], "temperature": 0.0, "avg_logprob": -0.08804636383056641, "compression_ratio": 1.5857142857142856, "no_speech_prob": 3.187544734828407e-06}, {"id": 362, "seek": 251542, "start": 2533.56, "end": 2538.2400000000002, "text": " be jumping through. So normally it would take a thousand time steps, but because we're using", "tokens": [312, 11233, 807, 13, 407, 5646, 309, 576, 747, 257, 4714, 565, 4439, 11, 457, 570, 321, 434, 1228], "temperature": 0.0, "avg_logprob": -0.08804636383056641, "compression_ratio": 1.5857142857142856, "no_speech_prob": 3.187544734828407e-06}, {"id": 363, "seek": 251542, "start": 2538.2400000000002, "end": 2544.52, "text": " a fancy scheduler, we get to skip from 999 to 984, 984 to 970 and so forth. So we're", "tokens": [257, 10247, 12000, 260, 11, 321, 483, 281, 10023, 490, 1722, 8494, 281, 20860, 19, 11, 20860, 19, 281, 1722, 5867, 293, 370, 5220, 13, 407, 321, 434], "temperature": 0.0, "avg_logprob": -0.08804636383056641, "compression_ratio": 1.5857142857142856, "no_speech_prob": 3.187544734828407e-06}, {"id": 364, "seek": 254452, "start": 2544.52, "end": 2550.68, "text": " going down about 14 time steps. And remember, this is a very, very, very unfortunate word.", "tokens": [516, 760, 466, 3499, 565, 4439, 13, 400, 1604, 11, 341, 307, 257, 588, 11, 588, 11, 588, 17843, 1349, 13], "temperature": 0.0, "avg_logprob": -0.09946460082751363, "compression_ratio": 1.727626459143969, "no_speech_prob": 1.3419870811048895e-05}, {"id": 365, "seek": 254452, "start": 2550.68, "end": 2556.7, "text": " They're not time steps at all. In fact, they're not even integers. It's just a measure of", "tokens": [814, 434, 406, 565, 4439, 412, 439, 13, 682, 1186, 11, 436, 434, 406, 754, 41674, 13, 467, 311, 445, 257, 3481, 295], "temperature": 0.0, "avg_logprob": -0.09946460082751363, "compression_ratio": 1.727626459143969, "no_speech_prob": 1.3419870811048895e-05}, {"id": 366, "seek": 254452, "start": 2556.7, "end": 2561.88, "text": " how much noise are we adding at each time. And you find out how much noise by looking", "tokens": [577, 709, 5658, 366, 321, 5127, 412, 1184, 565, 13, 400, 291, 915, 484, 577, 709, 5658, 538, 1237], "temperature": 0.0, "avg_logprob": -0.09946460082751363, "compression_ratio": 1.727626459143969, "no_speech_prob": 1.3419870811048895e-05}, {"id": 367, "seek": 254452, "start": 2561.88, "end": 2567.78, "text": " it up on this graph. Okay. That's all time step means. It's not a step of time. And it's", "tokens": [309, 493, 322, 341, 4295, 13, 1033, 13, 663, 311, 439, 565, 1823, 1355, 13, 467, 311, 406, 257, 1823, 295, 565, 13, 400, 309, 311], "temperature": 0.0, "avg_logprob": -0.09946460082751363, "compression_ratio": 1.727626459143969, "no_speech_prob": 1.3419870811048895e-05}, {"id": 368, "seek": 254452, "start": 2567.78, "end": 2573.2, "text": " a real shame that that word is used because it's incredibly confusing. This is much more", "tokens": [257, 957, 10069, 300, 300, 1349, 307, 1143, 570, 309, 311, 6252, 13181, 13, 639, 307, 709, 544], "temperature": 0.0, "avg_logprob": -0.09946460082751363, "compression_ratio": 1.727626459143969, "no_speech_prob": 1.3419870811048895e-05}, {"id": 369, "seek": 257320, "start": 2573.2, "end": 2581.4399999999996, "text": " helpful. This is the actual amount of noise at each one of those iterations. And so here", "tokens": [4961, 13, 639, 307, 264, 3539, 2372, 295, 5658, 412, 1184, 472, 295, 729, 36540, 13, 400, 370, 510], "temperature": 0.0, "avg_logprob": -0.15182384974519972, "compression_ratio": 1.6358024691358024, "no_speech_prob": 1.3496974133886397e-06}, {"id": 370, "seek": 257320, "start": 2581.4399999999996, "end": 2589.08, "text": " you can see the amount of noise for each of those time steps. And we're going to be going", "tokens": [291, 393, 536, 264, 2372, 295, 5658, 337, 1184, 295, 729, 565, 4439, 13, 400, 321, 434, 516, 281, 312, 516], "temperature": 0.0, "avg_logprob": -0.15182384974519972, "compression_ratio": 1.6358024691358024, "no_speech_prob": 1.3496974133886397e-06}, {"id": 371, "seek": 257320, "start": 2589.08, "end": 2594.68, "text": " backwards, as you can see, we start at 999. So we'll start with lots of noise and then", "tokens": [12204, 11, 382, 291, 393, 536, 11, 321, 722, 412, 1722, 8494, 13, 407, 321, 603, 722, 365, 3195, 295, 5658, 293, 550], "temperature": 0.0, "avg_logprob": -0.15182384974519972, "compression_ratio": 1.6358024691358024, "no_speech_prob": 1.3496974133886397e-06}, {"id": 372, "seek": 259468, "start": 2594.68, "end": 2604.3999999999996, "text": " we'll be using less and less and less and less noise. So we go through the 70 time steps", "tokens": [321, 603, 312, 1228, 1570, 293, 1570, 293, 1570, 293, 1570, 5658, 13, 407, 321, 352, 807, 264, 5285, 565, 4439], "temperature": 0.0, "avg_logprob": -0.10860501474408961, "compression_ratio": 1.5549132947976878, "no_speech_prob": 1.3081744327791966e-06}, {"id": 373, "seek": 259468, "start": 2604.3999999999996, "end": 2614.3999999999996, "text": " in a for loop, concatenating our two noise bits together because we've got the classifier", "tokens": [294, 257, 337, 6367, 11, 1588, 7186, 990, 527, 732, 5658, 9239, 1214, 570, 321, 600, 658, 264, 1508, 9902], "temperature": 0.0, "avg_logprob": -0.10860501474408961, "compression_ratio": 1.5549132947976878, "no_speech_prob": 1.3081744327791966e-06}, {"id": 374, "seek": 259468, "start": 2614.3999999999996, "end": 2622.3599999999997, "text": " free and the prompt versions. Do our scaling, calculate our predictions from the unit. And", "tokens": [1737, 293, 264, 12391, 9606, 13, 1144, 527, 21589, 11, 8873, 527, 21264, 490, 264, 4985, 13, 400], "temperature": 0.0, "avg_logprob": -0.10860501474408961, "compression_ratio": 1.5549132947976878, "no_speech_prob": 1.3081744327791966e-06}, {"id": 375, "seek": 262236, "start": 2622.36, "end": 2628.7200000000003, "text": " notice here we're passing in the time step as well as our prompt. That's going to return", "tokens": [3449, 510, 321, 434, 8437, 294, 264, 565, 1823, 382, 731, 382, 527, 12391, 13, 663, 311, 516, 281, 2736], "temperature": 0.0, "avg_logprob": -0.09904601357199928, "compression_ratio": 1.782258064516129, "no_speech_prob": 1.873871951829642e-06}, {"id": 376, "seek": 262236, "start": 2628.7200000000003, "end": 2634.52, "text": " two things, the unconditional prediction. So that's the one for the empty string. Remember", "tokens": [732, 721, 11, 264, 47916, 17630, 13, 407, 300, 311, 264, 472, 337, 264, 6707, 6798, 13, 5459], "temperature": 0.0, "avg_logprob": -0.09904601357199928, "compression_ratio": 1.782258064516129, "no_speech_prob": 1.873871951829642e-06}, {"id": 377, "seek": 262236, "start": 2634.52, "end": 2640.6800000000003, "text": " we passed in, one of the two things we passed in was the empty string. So we concatenated", "tokens": [321, 4678, 294, 11, 472, 295, 264, 732, 721, 321, 4678, 294, 390, 264, 6707, 6798, 13, 407, 321, 1588, 7186, 770], "temperature": 0.0, "avg_logprob": -0.09904601357199928, "compression_ratio": 1.782258064516129, "no_speech_prob": 1.873871951829642e-06}, {"id": 378, "seek": 262236, "start": 2640.6800000000003, "end": 2645.92, "text": " them together. And so after they come out of the unit, we can pull them apart again.", "tokens": [552, 1214, 13, 400, 370, 934, 436, 808, 484, 295, 264, 4985, 11, 321, 393, 2235, 552, 4936, 797, 13], "temperature": 0.0, "avg_logprob": -0.09904601357199928, "compression_ratio": 1.782258064516129, "no_speech_prob": 1.873871951829642e-06}, {"id": 379, "seek": 262236, "start": 2645.92, "end": 2651.52, "text": " So dot chunk just means pull them apart into two separate variables. And then we can do", "tokens": [407, 5893, 16635, 445, 1355, 2235, 552, 4936, 666, 732, 4994, 9102, 13, 400, 550, 321, 393, 360], "temperature": 0.0, "avg_logprob": -0.09904601357199928, "compression_ratio": 1.782258064516129, "no_speech_prob": 1.873871951829642e-06}, {"id": 380, "seek": 265152, "start": 2651.52, "end": 2662.2, "text": " the guidance scale that we talked about last week. And so now we can do that update where", "tokens": [264, 10056, 4373, 300, 321, 2825, 466, 1036, 1243, 13, 400, 370, 586, 321, 393, 360, 300, 5623, 689], "temperature": 0.0, "avg_logprob": -0.09740150941384805, "compression_ratio": 1.5284090909090908, "no_speech_prob": 7.453764396814222e-07}, {"id": 381, "seek": 265152, "start": 2662.2, "end": 2670.12, "text": " we take a little bit of the noise and remove it to give us our new latency. So that's the", "tokens": [321, 747, 257, 707, 857, 295, 264, 5658, 293, 4159, 309, 281, 976, 505, 527, 777, 27043, 13, 407, 300, 311, 264], "temperature": 0.0, "avg_logprob": -0.09740150941384805, "compression_ratio": 1.5284090909090908, "no_speech_prob": 7.453764396814222e-07}, {"id": 382, "seek": 265152, "start": 2670.12, "end": 2678.72, "text": " loop. And so at the end of all that, we decode it in the VAE. The paper that created this", "tokens": [6367, 13, 400, 370, 412, 264, 917, 295, 439, 300, 11, 321, 979, 1429, 309, 294, 264, 18527, 36, 13, 440, 3035, 300, 2942, 341], "temperature": 0.0, "avg_logprob": -0.09740150941384805, "compression_ratio": 1.5284090909090908, "no_speech_prob": 7.453764396814222e-07}, {"id": 383, "seek": 267872, "start": 2678.72, "end": 2684.3199999999997, "text": " VAE tells us that we have to divide it by this number to scale it correctly. And once", "tokens": [18527, 36, 5112, 505, 300, 321, 362, 281, 9845, 309, 538, 341, 1230, 281, 4373, 309, 8944, 13, 400, 1564], "temperature": 0.0, "avg_logprob": -0.10720017251003994, "compression_ratio": 1.6525821596244132, "no_speech_prob": 1.3709560562347178e-06}, {"id": 384, "seek": 267872, "start": 2684.3199999999997, "end": 2694.04, "text": " we've done that, that gives us a number which is between negative one and one. Python imaging", "tokens": [321, 600, 1096, 300, 11, 300, 2709, 505, 257, 1230, 597, 307, 1296, 3671, 472, 293, 472, 13, 15329, 25036], "temperature": 0.0, "avg_logprob": -0.10720017251003994, "compression_ratio": 1.6525821596244132, "no_speech_prob": 1.3709560562347178e-06}, {"id": 385, "seek": 267872, "start": 2694.04, "end": 2698.6, "text": " library expects something between zero and one. So that's what we do here to make it", "tokens": [6405, 33280, 746, 1296, 4018, 293, 472, 13, 407, 300, 311, 437, 321, 360, 510, 281, 652, 309], "temperature": 0.0, "avg_logprob": -0.10720017251003994, "compression_ratio": 1.6525821596244132, "no_speech_prob": 1.3709560562347178e-06}, {"id": 386, "seek": 267872, "start": 2698.6, "end": 2705.12, "text": " between zero and one, like enforce that to be true. Put that back on the CPU, make sure", "tokens": [1296, 4018, 293, 472, 11, 411, 24825, 300, 281, 312, 2074, 13, 4935, 300, 646, 322, 264, 13199, 11, 652, 988], "temperature": 0.0, "avg_logprob": -0.10720017251003994, "compression_ratio": 1.6525821596244132, "no_speech_prob": 1.3709560562347178e-06}, {"id": 387, "seek": 270512, "start": 2705.12, "end": 2711.96, "text": " it's that the order of the dimensions is the same as what Python imaging library expects.", "tokens": [309, 311, 300, 264, 1668, 295, 264, 12819, 307, 264, 912, 382, 437, 15329, 25036, 6405, 33280, 13], "temperature": 0.0, "avg_logprob": -0.10042698042733329, "compression_ratio": 1.5084745762711864, "no_speech_prob": 1.529410042167001e-06}, {"id": 388, "seek": 270512, "start": 2711.96, "end": 2716.96, "text": " And then finally convert it up to between zero and 255 as an int, which is actually", "tokens": [400, 550, 2721, 7620, 309, 493, 281, 1296, 4018, 293, 3552, 20, 382, 364, 560, 11, 597, 307, 767], "temperature": 0.0, "avg_logprob": -0.10042698042733329, "compression_ratio": 1.5084745762711864, "no_speech_prob": 1.529410042167001e-06}, {"id": 389, "seek": 270512, "start": 2716.96, "end": 2729.24, "text": " what PIO really wants. And there's our picture. So there's all the steps. So what I then did,", "tokens": [437, 430, 15167, 534, 2738, 13, 400, 456, 311, 527, 3036, 13, 407, 456, 311, 439, 264, 4439, 13, 407, 437, 286, 550, 630, 11], "temperature": 0.0, "avg_logprob": -0.10042698042733329, "compression_ratio": 1.5084745762711864, "no_speech_prob": 1.529410042167001e-06}, {"id": 390, "seek": 272924, "start": 2729.24, "end": 2736.3199999999997, "text": " this is kind of like, so the way I normally build code, I use notebooks for everything,", "tokens": [341, 307, 733, 295, 411, 11, 370, 264, 636, 286, 5646, 1322, 3089, 11, 286, 764, 43782, 337, 1203, 11], "temperature": 0.0, "avg_logprob": -0.12003918697959498, "compression_ratio": 1.6619718309859155, "no_speech_prob": 5.771881660621148e-06}, {"id": 391, "seek": 272924, "start": 2736.3199999999997, "end": 2741.1, "text": " is I kind of do things step by step by step. And then I tend to kind of copy them and I", "tokens": [307, 286, 733, 295, 360, 721, 1823, 538, 1823, 538, 1823, 13, 400, 550, 286, 3928, 281, 733, 295, 5055, 552, 293, 286], "temperature": 0.0, "avg_logprob": -0.12003918697959498, "compression_ratio": 1.6619718309859155, "no_speech_prob": 5.771881660621148e-06}, {"id": 392, "seek": 272924, "start": 2741.1, "end": 2746.4799999999996, "text": " use shift M. I don't know if you've seen that, but what shift M does, it takes two cells", "tokens": [764, 5513, 376, 13, 286, 500, 380, 458, 498, 291, 600, 1612, 300, 11, 457, 437, 5513, 376, 775, 11, 309, 2516, 732, 5438], "temperature": 0.0, "avg_logprob": -0.12003918697959498, "compression_ratio": 1.6619718309859155, "no_speech_prob": 5.771881660621148e-06}, {"id": 393, "seek": 272924, "start": 2746.4799999999996, "end": 2753.4799999999996, "text": " and combines them like that. And so I basically combined some of the cells together and I", "tokens": [293, 29520, 552, 411, 300, 13, 400, 370, 286, 1936, 9354, 512, 295, 264, 5438, 1214, 293, 286], "temperature": 0.0, "avg_logprob": -0.12003918697959498, "compression_ratio": 1.6619718309859155, "no_speech_prob": 5.771881660621148e-06}, {"id": 394, "seek": 275348, "start": 2753.48, "end": 2764.28, "text": " removed a bunch of the pros. So you can see the entire thing on one screen. And what I", "tokens": [7261, 257, 3840, 295, 264, 6267, 13, 407, 291, 393, 536, 264, 2302, 551, 322, 472, 2568, 13, 400, 437, 286], "temperature": 0.0, "avg_logprob": -0.11370214172031569, "compression_ratio": 1.5789473684210527, "no_speech_prob": 3.3931305551959667e-06}, {"id": 395, "seek": 275348, "start": 2764.28, "end": 2767.84, "text": " was trying to do here is I'd like to get to the point where I've got something which I", "tokens": [390, 1382, 281, 360, 510, 307, 286, 1116, 411, 281, 483, 281, 264, 935, 689, 286, 600, 658, 746, 597, 286], "temperature": 0.0, "avg_logprob": -0.11370214172031569, "compression_ratio": 1.5789473684210527, "no_speech_prob": 3.3931305551959667e-06}, {"id": 396, "seek": 275348, "start": 2767.84, "end": 2772.4, "text": " can very quickly do experiments with. So maybe I want to try some different approach to guidance", "tokens": [393, 588, 2661, 360, 12050, 365, 13, 407, 1310, 286, 528, 281, 853, 512, 819, 3109, 281, 10056], "temperature": 0.0, "avg_logprob": -0.11370214172031569, "compression_ratio": 1.5789473684210527, "no_speech_prob": 3.3931305551959667e-06}, {"id": 397, "seek": 275348, "start": 2772.4, "end": 2781.64, "text": " tree classification. Maybe I want to add some callbacks, so on and so forth. So I kind of", "tokens": [4230, 21538, 13, 2704, 286, 528, 281, 909, 512, 818, 17758, 11, 370, 322, 293, 370, 5220, 13, 407, 286, 733, 295], "temperature": 0.0, "avg_logprob": -0.11370214172031569, "compression_ratio": 1.5789473684210527, "no_speech_prob": 3.3931305551959667e-06}, {"id": 398, "seek": 278164, "start": 2781.64, "end": 2786.12, "text": " like to have everything, you know, I like to have all of my important code be able to", "tokens": [411, 281, 362, 1203, 11, 291, 458, 11, 286, 411, 281, 362, 439, 295, 452, 1021, 3089, 312, 1075, 281], "temperature": 0.0, "avg_logprob": -0.12756373856093858, "compression_ratio": 1.6074766355140186, "no_speech_prob": 8.801016520010307e-06}, {"id": 399, "seek": 278164, "start": 2786.12, "end": 2790.68, "text": " fit into my screen at once. And so you can see now I do, I've got the whole thing on", "tokens": [3318, 666, 452, 2568, 412, 1564, 13, 400, 370, 291, 393, 536, 586, 286, 360, 11, 286, 600, 658, 264, 1379, 551, 322], "temperature": 0.0, "avg_logprob": -0.12756373856093858, "compression_ratio": 1.6074766355140186, "no_speech_prob": 8.801016520010307e-06}, {"id": 400, "seek": 278164, "start": 2790.68, "end": 2795.52, "text": " my screen so I can keep it all in my head. One thing I was playing around with was I", "tokens": [452, 2568, 370, 286, 393, 1066, 309, 439, 294, 452, 1378, 13, 1485, 551, 286, 390, 2433, 926, 365, 390, 286], "temperature": 0.0, "avg_logprob": -0.12756373856093858, "compression_ratio": 1.6074766355140186, "no_speech_prob": 8.801016520010307e-06}, {"id": 401, "seek": 278164, "start": 2795.52, "end": 2807.3199999999997, "text": " was trying to understand the actual guidance tree equation in terms of like, how does it", "tokens": [390, 1382, 281, 1223, 264, 3539, 10056, 4230, 5367, 294, 2115, 295, 411, 11, 577, 775, 309], "temperature": 0.0, "avg_logprob": -0.12756373856093858, "compression_ratio": 1.6074766355140186, "no_speech_prob": 8.801016520010307e-06}, {"id": 402, "seek": 280732, "start": 2807.32, "end": 2812.84, "text": " work? Computer scientists tend to write things in software engineers with kind of long words", "tokens": [589, 30, 22289, 7708, 3928, 281, 2464, 721, 294, 4722, 11955, 365, 733, 295, 938, 2283], "temperature": 0.0, "avg_logprob": -0.11515625544956752, "compression_ratio": 1.6218181818181818, "no_speech_prob": 3.219039354007691e-05}, {"id": 403, "seek": 280732, "start": 2812.84, "end": 2819.0800000000004, "text": " as variable names. Mathematicians tend to use short words, just letters normally. For", "tokens": [382, 7006, 5288, 13, 15776, 14911, 2567, 3928, 281, 764, 2099, 2283, 11, 445, 7825, 5646, 13, 1171], "temperature": 0.0, "avg_logprob": -0.11515625544956752, "compression_ratio": 1.6218181818181818, "no_speech_prob": 3.219039354007691e-05}, {"id": 404, "seek": 280732, "start": 2819.0800000000004, "end": 2823.6800000000003, "text": " me, when I want to play around with stuff like that, I turn stuff back into letters.", "tokens": [385, 11, 562, 286, 528, 281, 862, 926, 365, 1507, 411, 300, 11, 286, 1261, 1507, 646, 666, 7825, 13], "temperature": 0.0, "avg_logprob": -0.11515625544956752, "compression_ratio": 1.6218181818181818, "no_speech_prob": 3.219039354007691e-05}, {"id": 405, "seek": 280732, "start": 2823.6800000000003, "end": 2827.0, "text": " And that's because I actually kind of pulled out one note and I started jutting down this", "tokens": [400, 300, 311, 570, 286, 767, 733, 295, 7373, 484, 472, 3637, 293, 286, 1409, 42079, 783, 760, 341], "temperature": 0.0, "avg_logprob": -0.11515625544956752, "compression_ratio": 1.6218181818181818, "no_speech_prob": 3.219039354007691e-05}, {"id": 406, "seek": 280732, "start": 2827.0, "end": 2834.2000000000003, "text": " equation and playing around with it to understand how it behaves. So this is just like, it's", "tokens": [5367, 293, 2433, 926, 365, 309, 281, 1223, 577, 309, 36896, 13, 407, 341, 307, 445, 411, 11, 309, 311], "temperature": 0.0, "avg_logprob": -0.11515625544956752, "compression_ratio": 1.6218181818181818, "no_speech_prob": 3.219039354007691e-05}, {"id": 407, "seek": 283420, "start": 2834.2, "end": 2838.48, "text": " not better or worse, it's just depending on what you're doing. So actually here I said,", "tokens": [406, 1101, 420, 5324, 11, 309, 311, 445, 5413, 322, 437, 291, 434, 884, 13, 407, 767, 510, 286, 848, 11], "temperature": 0.0, "avg_logprob": -0.14883597837675602, "compression_ratio": 1.6199261992619927, "no_speech_prob": 4.936950972478371e-06}, {"id": 408, "seek": 283420, "start": 2838.48, "end": 2845.12, "text": " okay, G is guidance scale. And then rather than having the unconditional and text embeddings,", "tokens": [1392, 11, 460, 307, 10056, 4373, 13, 400, 550, 2831, 813, 1419, 264, 47916, 293, 2487, 12240, 29432, 11], "temperature": 0.0, "avg_logprob": -0.14883597837675602, "compression_ratio": 1.6199261992619927, "no_speech_prob": 4.936950972478371e-06}, {"id": 409, "seek": 283420, "start": 2845.12, "end": 2849.52, "text": " I just call them U and T. And now I've got this all down into an equation which I can", "tokens": [286, 445, 818, 552, 624, 293, 314, 13, 400, 586, 286, 600, 658, 341, 439, 760, 666, 364, 5367, 597, 286, 393], "temperature": 0.0, "avg_logprob": -0.14883597837675602, "compression_ratio": 1.6199261992619927, "no_speech_prob": 4.936950972478371e-06}, {"id": 410, "seek": 283420, "start": 2849.52, "end": 2854.3199999999997, "text": " write down in a notebook and play with and understand exactly how it works. So that's", "tokens": [2464, 760, 294, 257, 21060, 293, 862, 365, 293, 1223, 2293, 577, 309, 1985, 13, 407, 300, 311], "temperature": 0.0, "avg_logprob": -0.14883597837675602, "compression_ratio": 1.6199261992619927, "no_speech_prob": 4.936950972478371e-06}, {"id": 411, "seek": 283420, "start": 2854.3199999999997, "end": 2861.3199999999997, "text": " something I find really helpful for working with this kind of code is to turn it into", "tokens": [746, 286, 915, 534, 4961, 337, 1364, 365, 341, 733, 295, 3089, 307, 281, 1261, 309, 666], "temperature": 0.0, "avg_logprob": -0.14883597837675602, "compression_ratio": 1.6199261992619927, "no_speech_prob": 4.936950972478371e-06}, {"id": 412, "seek": 286132, "start": 2861.32, "end": 2867.0800000000004, "text": " a form that I can manipulate algebraically more easily. I also try to make it look as", "tokens": [257, 1254, 300, 286, 393, 20459, 21989, 984, 544, 3612, 13, 286, 611, 853, 281, 652, 309, 574, 382], "temperature": 0.0, "avg_logprob": -0.1312533508647572, "compression_ratio": 1.554054054054054, "no_speech_prob": 5.338139089872129e-06}, {"id": 413, "seek": 286132, "start": 2867.0800000000004, "end": 2872.7200000000003, "text": " much like the paper that I'm implementing as possible. Anywho, that's that code. So", "tokens": [709, 411, 264, 3035, 300, 286, 478, 18114, 382, 1944, 13, 2639, 13506, 11, 300, 311, 300, 3089, 13, 407], "temperature": 0.0, "avg_logprob": -0.1312533508647572, "compression_ratio": 1.554054054054054, "no_speech_prob": 5.338139089872129e-06}, {"id": 414, "seek": 286132, "start": 2872.7200000000003, "end": 2878.8, "text": " then I copied all this again and I basically, oh, I actually did it for two prompts this", "tokens": [550, 286, 25365, 439, 341, 797, 293, 286, 1936, 11, 1954, 11, 286, 767, 630, 309, 337, 732, 41095, 341], "temperature": 0.0, "avg_logprob": -0.1312533508647572, "compression_ratio": 1.554054054054054, "no_speech_prob": 5.338139089872129e-06}, {"id": 415, "seek": 286132, "start": 2878.8, "end": 2883.28, "text": " time. I thought this was fun. Oil painting of an astronaut riding a horse in the style", "tokens": [565, 13, 286, 1194, 341, 390, 1019, 13, 23545, 5370, 295, 364, 18516, 9546, 257, 6832, 294, 264, 3758], "temperature": 0.0, "avg_logprob": -0.1312533508647572, "compression_ratio": 1.554054054054054, "no_speech_prob": 5.338139089872129e-06}, {"id": 416, "seek": 288328, "start": 2883.28, "end": 2895.7200000000003, "text": " of Grant Wood. Just to remind you, Grant Wood looks like this. Not obviously astronaut material,", "tokens": [295, 17529, 11558, 13, 1449, 281, 4160, 291, 11, 17529, 11558, 1542, 411, 341, 13, 1726, 2745, 18516, 2527, 11], "temperature": 0.0, "avg_logprob": -0.11154013127088547, "compression_ratio": 1.4702702702702704, "no_speech_prob": 1.3497008239937713e-06}, {"id": 417, "seek": 288328, "start": 2895.7200000000003, "end": 2899.28, "text": " which I thought would make it actually kind of particularly interesting. Although he does", "tokens": [597, 286, 1194, 576, 652, 309, 767, 733, 295, 4098, 1880, 13, 5780, 415, 775], "temperature": 0.0, "avg_logprob": -0.11154013127088547, "compression_ratio": 1.4702702702702704, "no_speech_prob": 1.3497008239937713e-06}, {"id": 418, "seek": 288328, "start": 2899.28, "end": 2907.6400000000003, "text": " have horses. I can't see one here. Some of his pictures have horses. So because I did", "tokens": [362, 13112, 13, 286, 393, 380, 536, 472, 510, 13, 2188, 295, 702, 5242, 362, 13112, 13, 407, 570, 286, 630], "temperature": 0.0, "avg_logprob": -0.11154013127088547, "compression_ratio": 1.4702702702702704, "no_speech_prob": 1.3497008239937713e-06}, {"id": 419, "seek": 290764, "start": 2907.64, "end": 2913.2, "text": " two prompts, I got back two pictures I could do. So here's the Grant Wood one. I don't", "tokens": [732, 41095, 11, 286, 658, 646, 732, 5242, 286, 727, 360, 13, 407, 510, 311, 264, 17529, 11558, 472, 13, 286, 500, 380], "temperature": 0.0, "avg_logprob": -0.09510430883853993, "compression_ratio": 1.6296296296296295, "no_speech_prob": 2.8573049348779023e-06}, {"id": 420, "seek": 290764, "start": 2913.2, "end": 2918.8399999999997, "text": " know what's going on in his back here, but I think it's quite nice. So yeah, I then copied", "tokens": [458, 437, 311, 516, 322, 294, 702, 646, 510, 11, 457, 286, 519, 309, 311, 1596, 1481, 13, 407, 1338, 11, 286, 550, 25365], "temperature": 0.0, "avg_logprob": -0.09510430883853993, "compression_ratio": 1.6296296296296295, "no_speech_prob": 2.8573049348779023e-06}, {"id": 421, "seek": 290764, "start": 2918.8399999999997, "end": 2927.14, "text": " that whole thing again and merged them all together and then just put it into a function.", "tokens": [300, 1379, 551, 797, 293, 36427, 552, 439, 1214, 293, 550, 445, 829, 309, 666, 257, 2445, 13], "temperature": 0.0, "avg_logprob": -0.09510430883853993, "compression_ratio": 1.6296296296296295, "no_speech_prob": 2.8573049348779023e-06}, {"id": 422, "seek": 290764, "start": 2927.14, "end": 2933.0, "text": " So I took the little bit which creates an image and put that into a function. I took", "tokens": [407, 286, 1890, 264, 707, 857, 597, 7829, 364, 3256, 293, 829, 300, 666, 257, 2445, 13, 286, 1890], "temperature": 0.0, "avg_logprob": -0.09510430883853993, "compression_ratio": 1.6296296296296295, "no_speech_prob": 2.8573049348779023e-06}, {"id": 423, "seek": 293300, "start": 2933.0, "end": 2938.2, "text": " the bit which does the tokenizing and text encoding and put that into a function. And", "tokens": [264, 857, 597, 775, 264, 14862, 3319, 293, 2487, 43430, 293, 829, 300, 666, 257, 2445, 13, 400], "temperature": 0.0, "avg_logprob": -0.08927421891287471, "compression_ratio": 1.6844660194174756, "no_speech_prob": 2.3320671971305273e-06}, {"id": 424, "seek": 293300, "start": 2938.2, "end": 2945.18, "text": " so now all of the code necessary to do the whole thing from top to bottom fits in these", "tokens": [370, 586, 439, 295, 264, 3089, 4818, 281, 360, 264, 1379, 551, 490, 1192, 281, 2767, 9001, 294, 613], "temperature": 0.0, "avg_logprob": -0.08927421891287471, "compression_ratio": 1.6844660194174756, "no_speech_prob": 2.3320671971305273e-06}, {"id": 425, "seek": 293300, "start": 2945.18, "end": 2953.44, "text": " two cells, which makes it for me much easier to see exactly what's going on. So you can", "tokens": [732, 5438, 11, 597, 1669, 309, 337, 385, 709, 3571, 281, 536, 2293, 437, 311, 516, 322, 13, 407, 291, 393], "temperature": 0.0, "avg_logprob": -0.08927421891287471, "compression_ratio": 1.6844660194174756, "no_speech_prob": 2.3320671971305273e-06}, {"id": 426, "seek": 293300, "start": 2953.44, "end": 2958.58, "text": " see I've got the text embeddings. I've got the unconditional embeddings. I've got the", "tokens": [536, 286, 600, 658, 264, 2487, 12240, 29432, 13, 286, 600, 658, 264, 47916, 12240, 29432, 13, 286, 600, 658, 264], "temperature": 0.0, "avg_logprob": -0.08927421891287471, "compression_ratio": 1.6844660194174756, "no_speech_prob": 2.3320671971305273e-06}, {"id": 427, "seek": 295858, "start": 2958.58, "end": 2967.68, "text": " embeddings which concatenate the two together. Optional random seed. My latency. And then", "tokens": [12240, 29432, 597, 1588, 7186, 473, 264, 732, 1214, 13, 29284, 304, 4974, 8871, 13, 1222, 27043, 13, 400, 550], "temperature": 0.0, "avg_logprob": -0.1240087487231726, "compression_ratio": 1.6515837104072397, "no_speech_prob": 1.5534945987383253e-06}, {"id": 428, "seek": 295858, "start": 2967.68, "end": 2974.12, "text": " the loop itself. And you'll also see something I do which is a bit different to a lot of", "tokens": [264, 6367, 2564, 13, 400, 291, 603, 611, 536, 746, 286, 360, 597, 307, 257, 857, 819, 281, 257, 688, 295], "temperature": 0.0, "avg_logprob": -0.1240087487231726, "compression_ratio": 1.6515837104072397, "no_speech_prob": 1.5534945987383253e-06}, {"id": 429, "seek": 295858, "start": 2974.12, "end": 2980.12, "text": " software engineering is I often create things which are kind of like longer lines because", "tokens": [4722, 7043, 307, 286, 2049, 1884, 721, 597, 366, 733, 295, 411, 2854, 3876, 570], "temperature": 0.0, "avg_logprob": -0.1240087487231726, "compression_ratio": 1.6515837104072397, "no_speech_prob": 1.5534945987383253e-06}, {"id": 430, "seek": 295858, "start": 2980.12, "end": 2985.68, "text": " I try to have each line be kind of like mathematically one thing that I want to be able to think", "tokens": [286, 853, 281, 362, 1184, 1622, 312, 733, 295, 411, 44003, 472, 551, 300, 286, 528, 281, 312, 1075, 281, 519], "temperature": 0.0, "avg_logprob": -0.1240087487231726, "compression_ratio": 1.6515837104072397, "no_speech_prob": 1.5534945987383253e-06}, {"id": 431, "seek": 298568, "start": 2985.68, "end": 2993.12, "text": " about as a whole. So yeah, these are some differences between kind of the way I find", "tokens": [466, 382, 257, 1379, 13, 407, 1338, 11, 613, 366, 512, 7300, 1296, 733, 295, 264, 636, 286, 915], "temperature": 0.0, "avg_logprob": -0.11454953193664551, "compression_ratio": 1.6360153256704981, "no_speech_prob": 4.637851816369221e-06}, {"id": 432, "seek": 298568, "start": 2993.12, "end": 2996.68, "text": " numerical programming works well compared to the way I would write a more traditional", "tokens": [29054, 9410, 1985, 731, 5347, 281, 264, 636, 286, 576, 2464, 257, 544, 5164], "temperature": 0.0, "avg_logprob": -0.11454953193664551, "compression_ratio": 1.6360153256704981, "no_speech_prob": 4.637851816369221e-06}, {"id": 433, "seek": 298568, "start": 2996.68, "end": 3002.2, "text": " software engineering approach. And again, this is partly a personal preference, but", "tokens": [4722, 7043, 3109, 13, 400, 797, 11, 341, 307, 17031, 257, 2973, 17502, 11, 457], "temperature": 0.0, "avg_logprob": -0.11454953193664551, "compression_ratio": 1.6360153256704981, "no_speech_prob": 4.637851816369221e-06}, {"id": 434, "seek": 298568, "start": 3002.2, "end": 3007.3199999999997, "text": " it's something I find works well for me. So we're now at a point where we've got three", "tokens": [309, 311, 746, 286, 915, 1985, 731, 337, 385, 13, 407, 321, 434, 586, 412, 257, 935, 689, 321, 600, 658, 1045], "temperature": 0.0, "avg_logprob": -0.11454953193664551, "compression_ratio": 1.6360153256704981, "no_speech_prob": 4.637851816369221e-06}, {"id": 435, "seek": 298568, "start": 3007.3199999999997, "end": 3011.7799999999997, "text": " functions that easily fit on the screen and do everything. So I can now just say make", "tokens": [6828, 300, 3612, 3318, 322, 264, 2568, 293, 360, 1203, 13, 407, 286, 393, 586, 445, 584, 652], "temperature": 0.0, "avg_logprob": -0.11454953193664551, "compression_ratio": 1.6360153256704981, "no_speech_prob": 4.637851816369221e-06}, {"id": 436, "seek": 301178, "start": 3011.78, "end": 3023.1600000000003, "text": " samples and display each image. And so this is something for you to experiment with. And", "tokens": [10938, 293, 4674, 1184, 3256, 13, 400, 370, 341, 307, 746, 337, 291, 281, 5120, 365, 13, 400], "temperature": 0.0, "avg_logprob": -0.09131138078097639, "compression_ratio": 1.5, "no_speech_prob": 1.9033806211155024e-06}, {"id": 437, "seek": 301178, "start": 3023.1600000000003, "end": 3034.2000000000003, "text": " what I specifically suggest as homework is to try picking one of the extra tricks we", "tokens": [437, 286, 4682, 3402, 382, 14578, 307, 281, 853, 8867, 472, 295, 264, 2857, 11733, 321], "temperature": 0.0, "avg_logprob": -0.09131138078097639, "compression_ratio": 1.5, "no_speech_prob": 1.9033806211155024e-06}, {"id": 438, "seek": 301178, "start": 3034.2000000000003, "end": 3039.88, "text": " learned about like image to image or negative prompts. Negative prompts would be a nice", "tokens": [3264, 466, 411, 3256, 281, 3256, 420, 3671, 41095, 13, 43230, 41095, 576, 312, 257, 1481], "temperature": 0.0, "avg_logprob": -0.09131138078097639, "compression_ratio": 1.5, "no_speech_prob": 1.9033806211155024e-06}, {"id": 439, "seek": 303988, "start": 3039.88, "end": 3055.1600000000003, "text": " easy one. Like see if you can implement negative prompt in your version of this. Or yeah, try", "tokens": [1858, 472, 13, 1743, 536, 498, 291, 393, 4445, 3671, 12391, 294, 428, 3037, 295, 341, 13, 1610, 1338, 11, 853], "temperature": 0.0, "avg_logprob": -0.08722079140799387, "compression_ratio": 1.4725274725274726, "no_speech_prob": 3.138137117275619e-06}, {"id": 440, "seek": 303988, "start": 3055.1600000000003, "end": 3060.48, "text": " doing image to image. That wouldn't be too hard either. Another one you can add is try", "tokens": [884, 3256, 281, 3256, 13, 663, 2759, 380, 312, 886, 1152, 2139, 13, 3996, 472, 291, 393, 909, 307, 853], "temperature": 0.0, "avg_logprob": -0.08722079140799387, "compression_ratio": 1.4725274725274726, "no_speech_prob": 3.138137117275619e-06}, {"id": 441, "seek": 303988, "start": 3060.48, "end": 3067.28, "text": " adding callbacks. And the nice thing is then, you know, you've got code which you fully", "tokens": [5127, 818, 17758, 13, 400, 264, 1481, 551, 307, 550, 11, 291, 458, 11, 291, 600, 658, 3089, 597, 291, 4498], "temperature": 0.0, "avg_logprob": -0.08722079140799387, "compression_ratio": 1.4725274725274726, "no_speech_prob": 3.138137117275619e-06}, {"id": 442, "seek": 306728, "start": 3067.28, "end": 3075.48, "text": " understand because you know what all the lines do. And you then don't need to wait for the", "tokens": [1223, 570, 291, 458, 437, 439, 264, 3876, 360, 13, 400, 291, 550, 500, 380, 643, 281, 1699, 337, 264], "temperature": 0.0, "avg_logprob": -0.0933860011936463, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.601602773211198e-06}, {"id": 443, "seek": 306728, "start": 3075.48, "end": 3080.96, "text": " diffusers folks to update it. The library to do, for example, the callbacks are only", "tokens": [7593, 301, 433, 4024, 281, 5623, 309, 13, 440, 6405, 281, 360, 11, 337, 1365, 11, 264, 818, 17758, 366, 787], "temperature": 0.0, "avg_logprob": -0.0933860011936463, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.601602773211198e-06}, {"id": 444, "seek": 306728, "start": 3080.96, "end": 3085.0, "text": " added like a week ago. So until then you couldn't do callbacks. Well, now you don't have to", "tokens": [3869, 411, 257, 1243, 2057, 13, 407, 1826, 550, 291, 2809, 380, 360, 818, 17758, 13, 1042, 11, 586, 291, 500, 380, 362, 281], "temperature": 0.0, "avg_logprob": -0.0933860011936463, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.601602773211198e-06}, {"id": 445, "seek": 306728, "start": 3085.0, "end": 3090.88, "text": " wait for the diffusers team to add something. The code's all here for you to play with.", "tokens": [1699, 337, 264, 7593, 301, 433, 1469, 281, 909, 746, 13, 440, 3089, 311, 439, 510, 337, 291, 281, 862, 365, 13], "temperature": 0.0, "avg_logprob": -0.0933860011936463, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.601602773211198e-06}, {"id": 446, "seek": 309088, "start": 3090.88, "end": 3102.0, "text": " So that's my recommendation as a bit of homework for this week. Okay, so that brings us to", "tokens": [407, 300, 311, 452, 11879, 382, 257, 857, 295, 14578, 337, 341, 1243, 13, 1033, 11, 370, 300, 5607, 505, 281], "temperature": 0.0, "avg_logprob": -0.08498679674588717, "compression_ratio": 1.5555555555555556, "no_speech_prob": 3.1875335935183102e-06}, {"id": 447, "seek": 309088, "start": 3102.0, "end": 3111.44, "text": " the end of our rapid overview of stable diffusion and some very recent papers that very significantly", "tokens": [264, 917, 295, 527, 7558, 12492, 295, 8351, 25242, 293, 512, 588, 5162, 10577, 300, 588, 10591], "temperature": 0.0, "avg_logprob": -0.08498679674588717, "compression_ratio": 1.5555555555555556, "no_speech_prob": 3.1875335935183102e-06}, {"id": 448, "seek": 309088, "start": 3111.44, "end": 3118.56, "text": " develop stable diffusion. I hope that's given you a good sense of the kind of very high", "tokens": [1499, 8351, 25242, 13, 286, 1454, 300, 311, 2212, 291, 257, 665, 2020, 295, 264, 733, 295, 588, 1090], "temperature": 0.0, "avg_logprob": -0.08498679674588717, "compression_ratio": 1.5555555555555556, "no_speech_prob": 3.1875335935183102e-06}, {"id": 449, "seek": 311856, "start": 3118.56, "end": 3123.48, "text": " level slightly hand wavy version of all this and you can actually get started playing with", "tokens": [1496, 4748, 1011, 261, 15498, 3037, 295, 439, 341, 293, 291, 393, 767, 483, 1409, 2433, 365], "temperature": 0.0, "avg_logprob": -0.08972770785108025, "compression_ratio": 1.6621004566210045, "no_speech_prob": 2.8408421712811105e-05}, {"id": 450, "seek": 311856, "start": 3123.48, "end": 3132.2799999999997, "text": " some fun code. What we're going to be doing next is going right back to the start, learning", "tokens": [512, 1019, 3089, 13, 708, 321, 434, 516, 281, 312, 884, 958, 307, 516, 558, 646, 281, 264, 722, 11, 2539], "temperature": 0.0, "avg_logprob": -0.08972770785108025, "compression_ratio": 1.6621004566210045, "no_speech_prob": 2.8408421712811105e-05}, {"id": 451, "seek": 311856, "start": 3132.2799999999997, "end": 3137.72, "text": " how to multiply two matrices together effectively, and then gradually building from there until", "tokens": [577, 281, 12972, 732, 32284, 1214, 8659, 11, 293, 550, 13145, 2390, 490, 456, 1826], "temperature": 0.0, "avg_logprob": -0.08972770785108025, "compression_ratio": 1.6621004566210045, "no_speech_prob": 2.8408421712811105e-05}, {"id": 452, "seek": 311856, "start": 3137.72, "end": 3140.92, "text": " we've got to the point that we've rebuilt all this from scratch and we understand why", "tokens": [321, 600, 658, 281, 264, 935, 300, 321, 600, 38532, 439, 341, 490, 8459, 293, 321, 1223, 983], "temperature": 0.0, "avg_logprob": -0.08972770785108025, "compression_ratio": 1.6621004566210045, "no_speech_prob": 2.8408421712811105e-05}, {"id": 453, "seek": 314092, "start": 3140.92, "end": 3149.16, "text": " things work the way they do, understand how to debug problems, improve performance, and", "tokens": [721, 589, 264, 636, 436, 360, 11, 1223, 577, 281, 24083, 2740, 11, 3470, 3389, 11, 293], "temperature": 0.0, "avg_logprob": -0.13351320497917407, "compression_ratio": 1.456989247311828, "no_speech_prob": 2.2958975023357198e-06}, {"id": 454, "seek": 314092, "start": 3149.16, "end": 3154.6, "text": " implement new research papers as well. So that's going to be very exciting and so we're", "tokens": [4445, 777, 2132, 10577, 382, 731, 13, 407, 300, 311, 516, 281, 312, 588, 4670, 293, 370, 321, 434], "temperature": 0.0, "avg_logprob": -0.13351320497917407, "compression_ratio": 1.456989247311828, "no_speech_prob": 2.2958975023357198e-06}, {"id": 455, "seek": 314092, "start": 3154.6, "end": 3168.36, "text": " going to have a break and I will see you back here in 10 minutes. Okay, welcome back everybody.", "tokens": [516, 281, 362, 257, 1821, 293, 286, 486, 536, 291, 646, 510, 294, 1266, 2077, 13, 1033, 11, 2928, 646, 2201, 13], "temperature": 0.0, "avg_logprob": -0.13351320497917407, "compression_ratio": 1.456989247311828, "no_speech_prob": 2.2958975023357198e-06}, {"id": 456, "seek": 316836, "start": 3168.36, "end": 3178.08, "text": " I'm really excited about the next part of this. It's going to require some serious tenacity", "tokens": [286, 478, 534, 2919, 466, 264, 958, 644, 295, 341, 13, 467, 311, 516, 281, 3651, 512, 3156, 2064, 19008], "temperature": 0.0, "avg_logprob": -0.07685930078679865, "compression_ratio": 1.6743119266055047, "no_speech_prob": 5.224836786510423e-05}, {"id": 457, "seek": 316836, "start": 3178.08, "end": 3185.28, "text": " and a certain amount of patience, but I think you're going to learn a lot. A lot of folks", "tokens": [293, 257, 1629, 2372, 295, 14826, 11, 457, 286, 519, 291, 434, 516, 281, 1466, 257, 688, 13, 316, 688, 295, 4024], "temperature": 0.0, "avg_logprob": -0.07685930078679865, "compression_ratio": 1.6743119266055047, "no_speech_prob": 5.224836786510423e-05}, {"id": 458, "seek": 316836, "start": 3185.28, "end": 3191.28, "text": " I've spoken to have said that previous iterations of this part of the course is like the best", "tokens": [286, 600, 10759, 281, 362, 848, 300, 3894, 36540, 295, 341, 644, 295, 264, 1164, 307, 411, 264, 1151], "temperature": 0.0, "avg_logprob": -0.07685930078679865, "compression_ratio": 1.6743119266055047, "no_speech_prob": 5.224836786510423e-05}, {"id": 459, "seek": 316836, "start": 3191.28, "end": 3195.2400000000002, "text": " course they've ever done and this one's going to be dramatically better than any previous", "tokens": [1164, 436, 600, 1562, 1096, 293, 341, 472, 311, 516, 281, 312, 17548, 1101, 813, 604, 3894], "temperature": 0.0, "avg_logprob": -0.07685930078679865, "compression_ratio": 1.6743119266055047, "no_speech_prob": 5.224836786510423e-05}, {"id": 460, "seek": 319524, "start": 3195.24, "end": 3203.0, "text": " version we've done of this. So hopefully you'll find that the hard work and patience pays", "tokens": [3037, 321, 600, 1096, 295, 341, 13, 407, 4696, 291, 603, 915, 300, 264, 1152, 589, 293, 14826, 10604], "temperature": 0.0, "avg_logprob": -0.21550669568650266, "compression_ratio": 1.3134328358208955, "no_speech_prob": 1.0289235433447175e-05}, {"id": 461, "seek": 319524, "start": 3203.0, "end": 3216.56, "text": " off. We're working now through the course 22 P2 repo, so 2022 course part two, and the", "tokens": [766, 13, 492, 434, 1364, 586, 807, 264, 1164, 5853, 430, 17, 49040, 11, 370, 20229, 1164, 644, 732, 11, 293, 264], "temperature": 0.0, "avg_logprob": -0.21550669568650266, "compression_ratio": 1.3134328358208955, "no_speech_prob": 1.0289235433447175e-05}, {"id": 462, "seek": 321656, "start": 3216.56, "end": 3225.72, "text": " notebooks are ordered. So we'll start with notebook number one. And okay, so the goal is to get", "tokens": [43782, 366, 8866, 13, 407, 321, 603, 722, 365, 21060, 1230, 472, 13, 400, 1392, 11, 370, 264, 3387, 307, 281, 483], "temperature": 0.0, "avg_logprob": -0.13937351621430497, "compression_ratio": 1.7075471698113207, "no_speech_prob": 3.373519939486869e-05}, {"id": 463, "seek": 321656, "start": 3225.72, "end": 3230.12, "text": " to stable diffusion from the foundations, which means we have to define what are the", "tokens": [281, 8351, 25242, 490, 264, 22467, 11, 597, 1355, 321, 362, 281, 6964, 437, 366, 264], "temperature": 0.0, "avg_logprob": -0.13937351621430497, "compression_ratio": 1.7075471698113207, "no_speech_prob": 3.373519939486869e-05}, {"id": 464, "seek": 321656, "start": 3230.12, "end": 3238.56, "text": " foundations. So I've decided to define them as follows. We're allowed to use Python. We're", "tokens": [22467, 13, 407, 286, 600, 3047, 281, 6964, 552, 382, 10002, 13, 492, 434, 4350, 281, 764, 15329, 13, 492, 434], "temperature": 0.0, "avg_logprob": -0.13937351621430497, "compression_ratio": 1.7075471698113207, "no_speech_prob": 3.373519939486869e-05}, {"id": 465, "seek": 321656, "start": 3238.56, "end": 3243.6, "text": " allowed to use the Python standard library. So that's all the stuff that comes with Python", "tokens": [4350, 281, 764, 264, 15329, 3832, 6405, 13, 407, 300, 311, 439, 264, 1507, 300, 1487, 365, 15329], "temperature": 0.0, "avg_logprob": -0.13937351621430497, "compression_ratio": 1.7075471698113207, "no_speech_prob": 3.373519939486869e-05}, {"id": 466, "seek": 324360, "start": 3243.6, "end": 3247.6, "text": " by default. We're allowed to use Matplotlib because I couldn't be bothered creating my", "tokens": [538, 7576, 13, 492, 434, 4350, 281, 764, 6789, 564, 310, 38270, 570, 286, 2809, 380, 312, 22996, 4084, 452], "temperature": 0.0, "avg_logprob": -0.1451297957321693, "compression_ratio": 1.6090909090909091, "no_speech_prob": 1.2805260666937102e-05}, {"id": 467, "seek": 324360, "start": 3247.6, "end": 3253.7599999999998, "text": " own plotting library. And we're allowed to use Jupyter Notebooks and NBdev, which is", "tokens": [1065, 41178, 6405, 13, 400, 321, 434, 4350, 281, 764, 22125, 88, 391, 11633, 15170, 293, 426, 33, 40343, 11, 597, 307], "temperature": 0.0, "avg_logprob": -0.1451297957321693, "compression_ratio": 1.6090909090909091, "no_speech_prob": 1.2805260666937102e-05}, {"id": 468, "seek": 324360, "start": 3253.7599999999998, "end": 3261.0, "text": " something that creates modules from notebooks. So basically what we're going to try to do", "tokens": [746, 300, 7829, 16679, 490, 43782, 13, 407, 1936, 437, 321, 434, 516, 281, 853, 281, 360], "temperature": 0.0, "avg_logprob": -0.1451297957321693, "compression_ratio": 1.6090909090909091, "no_speech_prob": 1.2805260666937102e-05}, {"id": 469, "seek": 324360, "start": 3261.0, "end": 3270.88, "text": " is to rebuild everything starting from this foundation. Now to be clear, what we are allowed", "tokens": [307, 281, 16877, 1203, 2891, 490, 341, 7030, 13, 823, 281, 312, 1850, 11, 437, 321, 366, 4350], "temperature": 0.0, "avg_logprob": -0.1451297957321693, "compression_ratio": 1.6090909090909091, "no_speech_prob": 1.2805260666937102e-05}, {"id": 470, "seek": 327088, "start": 3270.88, "end": 3278.4, "text": " to use are the libraries once we have re-implemented them correctly. And so if we re-implement", "tokens": [281, 764, 366, 264, 15148, 1564, 321, 362, 319, 12, 332, 781, 14684, 552, 8944, 13, 400, 370, 498, 321, 319, 12, 332, 43704], "temperature": 0.0, "avg_logprob": -0.09186062954439975, "compression_ratio": 1.766355140186916, "no_speech_prob": 2.443982566546765e-06}, {"id": 471, "seek": 327088, "start": 3278.4, "end": 3285.2000000000003, "text": " something from NumPy or from PyTorch or whatever, we're then allowed to use the NumPy or PyTorch", "tokens": [746, 490, 22592, 47, 88, 420, 490, 9953, 51, 284, 339, 420, 2035, 11, 321, 434, 550, 4350, 281, 764, 264, 22592, 47, 88, 420, 9953, 51, 284, 339], "temperature": 0.0, "avg_logprob": -0.09186062954439975, "compression_ratio": 1.766355140186916, "no_speech_prob": 2.443982566546765e-06}, {"id": 472, "seek": 327088, "start": 3285.2000000000003, "end": 3292.52, "text": " or whatever version. Sometimes we'll be creating things that haven't been created before, and", "tokens": [420, 2035, 3037, 13, 4803, 321, 603, 312, 4084, 721, 300, 2378, 380, 668, 2942, 949, 11, 293], "temperature": 0.0, "avg_logprob": -0.09186062954439975, "compression_ratio": 1.766355140186916, "no_speech_prob": 2.443982566546765e-06}, {"id": 473, "seek": 327088, "start": 3292.52, "end": 3298.0, "text": " that's then going to be becoming our own library. And we're going to be calling that library", "tokens": [300, 311, 550, 516, 281, 312, 5617, 527, 1065, 6405, 13, 400, 321, 434, 516, 281, 312, 5141, 300, 6405], "temperature": 0.0, "avg_logprob": -0.09186062954439975, "compression_ratio": 1.766355140186916, "no_speech_prob": 2.443982566546765e-06}, {"id": 474, "seek": 329800, "start": 3298.0, "end": 3304.8, "text": " MiniAI. So we're going to be building our own little framework as we go. So for example,", "tokens": [18239, 48698, 13, 407, 321, 434, 516, 281, 312, 2390, 527, 1065, 707, 8388, 382, 321, 352, 13, 407, 337, 1365, 11], "temperature": 0.0, "avg_logprob": -0.1547625669792517, "compression_ratio": 1.4565217391304348, "no_speech_prob": 6.276698059082264e-07}, {"id": 475, "seek": 329800, "start": 3304.8, "end": 3311.84, "text": " here are some imports. And these imports all come from the Python standard library except", "tokens": [510, 366, 512, 41596, 13, 400, 613, 41596, 439, 808, 490, 264, 15329, 3832, 6405, 3993], "temperature": 0.0, "avg_logprob": -0.1547625669792517, "compression_ratio": 1.4565217391304348, "no_speech_prob": 6.276698059082264e-07}, {"id": 476, "seek": 329800, "start": 3311.84, "end": 3323.24, "text": " for these two. Now to be clear, one challenge we have is that the models we use in stable", "tokens": [337, 613, 732, 13, 823, 281, 312, 1850, 11, 472, 3430, 321, 362, 307, 300, 264, 5245, 321, 764, 294, 8351], "temperature": 0.0, "avg_logprob": -0.1547625669792517, "compression_ratio": 1.4565217391304348, "no_speech_prob": 6.276698059082264e-07}, {"id": 477, "seek": 332324, "start": 3323.24, "end": 3330.16, "text": " diffusion were trained on millions of dollars worth of equipment for months, which we don't", "tokens": [25242, 645, 8895, 322, 6803, 295, 3808, 3163, 295, 5927, 337, 2493, 11, 597, 321, 500, 380], "temperature": 0.0, "avg_logprob": -0.06580185890197754, "compression_ratio": 1.691588785046729, "no_speech_prob": 1.3287739193401649e-06}, {"id": 478, "seek": 332324, "start": 3330.16, "end": 3337.9599999999996, "text": " have the time or money. So another trick we're going to do is we're going to create identical", "tokens": [362, 264, 565, 420, 1460, 13, 407, 1071, 4282, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 1884, 14800], "temperature": 0.0, "avg_logprob": -0.06580185890197754, "compression_ratio": 1.691588785046729, "no_speech_prob": 1.3287739193401649e-06}, {"id": 479, "seek": 332324, "start": 3337.9599999999996, "end": 3343.4399999999996, "text": " but smaller versions of them. And so once we've got them working, we'll then be allowed", "tokens": [457, 4356, 9606, 295, 552, 13, 400, 370, 1564, 321, 600, 658, 552, 1364, 11, 321, 603, 550, 312, 4350], "temperature": 0.0, "avg_logprob": -0.06580185890197754, "compression_ratio": 1.691588785046729, "no_speech_prob": 1.3287739193401649e-06}, {"id": 480, "seek": 332324, "start": 3343.4399999999996, "end": 3348.8799999999997, "text": " to use the big pre-trained versions. So that's the basic idea. So we're going to have to", "tokens": [281, 764, 264, 955, 659, 12, 17227, 2001, 9606, 13, 407, 300, 311, 264, 3875, 1558, 13, 407, 321, 434, 516, 281, 362, 281], "temperature": 0.0, "avg_logprob": -0.06580185890197754, "compression_ratio": 1.691588785046729, "no_speech_prob": 1.3287739193401649e-06}, {"id": 481, "seek": 334888, "start": 3348.88, "end": 3359.32, "text": " end up with our own VAE, our own UNET, our own Clip Encoder, and so forth.", "tokens": [917, 493, 365, 527, 1065, 18527, 36, 11, 527, 1065, 8229, 4850, 11, 527, 1065, 2033, 647, 29584, 19866, 11, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.1537939292797144, "compression_ratio": 1.4795321637426901, "no_speech_prob": 1.1015922609658446e-06}, {"id": 482, "seek": 334888, "start": 3359.32, "end": 3364.88, "text": " To some degree, I am assuming that you've completed part one of the course. To some", "tokens": [1407, 512, 4314, 11, 286, 669, 11926, 300, 291, 600, 7365, 644, 472, 295, 264, 1164, 13, 1407, 512], "temperature": 0.0, "avg_logprob": -0.1537939292797144, "compression_ratio": 1.4795321637426901, "no_speech_prob": 1.1015922609658446e-06}, {"id": 483, "seek": 334888, "start": 3364.88, "end": 3372.36, "text": " degree. I will cover everything at least briefly, but if I cover something about deep learning", "tokens": [4314, 13, 286, 486, 2060, 1203, 412, 1935, 10515, 11, 457, 498, 286, 2060, 746, 466, 2452, 2539], "temperature": 0.0, "avg_logprob": -0.1537939292797144, "compression_ratio": 1.4795321637426901, "no_speech_prob": 1.1015922609658446e-06}, {"id": 484, "seek": 337236, "start": 3372.36, "end": 3380.4, "text": " too fast for you to know what's going on and you get lost, go back and watch part one or", "tokens": [886, 2370, 337, 291, 281, 458, 437, 311, 516, 322, 293, 291, 483, 2731, 11, 352, 646, 293, 1159, 644, 472, 420], "temperature": 0.0, "avg_logprob": -0.10364644606034834, "compression_ratio": 1.588235294117647, "no_speech_prob": 6.962119641684694e-06}, {"id": 485, "seek": 337236, "start": 3380.4, "end": 3386.1600000000003, "text": " go and Google for that term. For stuff that we haven't covered in part one, I will go", "tokens": [352, 293, 3329, 337, 300, 1433, 13, 1171, 1507, 300, 321, 2378, 380, 5343, 294, 644, 472, 11, 286, 486, 352], "temperature": 0.0, "avg_logprob": -0.10364644606034834, "compression_ratio": 1.588235294117647, "no_speech_prob": 6.962119641684694e-06}, {"id": 486, "seek": 337236, "start": 3386.1600000000003, "end": 3395.5, "text": " over it very thoroughly and carefully. All right. So I'm going to assume that you know", "tokens": [670, 309, 588, 17987, 293, 7500, 13, 1057, 558, 13, 407, 286, 478, 516, 281, 6552, 300, 291, 458], "temperature": 0.0, "avg_logprob": -0.10364644606034834, "compression_ratio": 1.588235294117647, "no_speech_prob": 6.962119641684694e-06}, {"id": 487, "seek": 337236, "start": 3395.5, "end": 3401.88, "text": " the basic idea, which is that we're going to need to be doing some matrix multiplication.", "tokens": [264, 3875, 1558, 11, 597, 307, 300, 321, 434, 516, 281, 643, 281, 312, 884, 512, 8141, 27290, 13], "temperature": 0.0, "avg_logprob": -0.10364644606034834, "compression_ratio": 1.588235294117647, "no_speech_prob": 6.962119641684694e-06}, {"id": 488, "seek": 340188, "start": 3401.88, "end": 3406.2000000000003, "text": " So we're going to try to take a deep dive into matrix multiplication today. And we're", "tokens": [407, 321, 434, 516, 281, 853, 281, 747, 257, 2452, 9192, 666, 8141, 27290, 965, 13, 400, 321, 434], "temperature": 0.0, "avg_logprob": -0.15604840098200617, "compression_ratio": 1.456043956043956, "no_speech_prob": 5.8626787904358935e-06}, {"id": 489, "seek": 340188, "start": 3406.2000000000003, "end": 3415.4, "text": " going to need some input data. And I quite like working with MNIST data. MNIST is handwritten", "tokens": [516, 281, 643, 512, 4846, 1412, 13, 400, 286, 1596, 411, 1364, 365, 376, 45, 19756, 1412, 13, 376, 45, 19756, 307, 1011, 26859], "temperature": 0.0, "avg_logprob": -0.15604840098200617, "compression_ratio": 1.456043956043956, "no_speech_prob": 5.8626787904358935e-06}, {"id": 490, "seek": 340188, "start": 3415.4, "end": 3431.7200000000003, "text": " digits. It's a classic data set. They're 28 by 28 pixel, gray scale images. And so we", "tokens": [27011, 13, 467, 311, 257, 7230, 1412, 992, 13, 814, 434, 7562, 538, 7562, 19261, 11, 10855, 4373, 5267, 13, 400, 370, 321], "temperature": 0.0, "avg_logprob": -0.15604840098200617, "compression_ratio": 1.456043956043956, "no_speech_prob": 5.8626787904358935e-06}, {"id": 491, "seek": 343172, "start": 3431.72, "end": 3440.3199999999997, "text": " can download them from this URL. So we use the path libs path object a lot. It's part", "tokens": [393, 5484, 552, 490, 341, 12905, 13, 407, 321, 764, 264, 3100, 375, 929, 3100, 2657, 257, 688, 13, 467, 311, 644], "temperature": 0.0, "avg_logprob": -0.1160078363104181, "compression_ratio": 1.627906976744186, "no_speech_prob": 1.6442087144241668e-05}, {"id": 492, "seek": 343172, "start": 3440.3199999999997, "end": 3444.9199999999996, "text": " of Python and it basically takes a string and turns it into something that you can treat", "tokens": [295, 15329, 293, 309, 1936, 2516, 257, 6798, 293, 4523, 309, 666, 746, 300, 291, 393, 2387], "temperature": 0.0, "avg_logprob": -0.1160078363104181, "compression_ratio": 1.627906976744186, "no_speech_prob": 1.6442087144241668e-05}, {"id": 493, "seek": 343172, "start": 3444.9199999999996, "end": 3452.4399999999996, "text": " as a path. For example, you can use slash to mean this file inside this subdirectory.", "tokens": [382, 257, 3100, 13, 1171, 1365, 11, 291, 393, 764, 17330, 281, 914, 341, 3991, 1854, 341, 31662, 11890, 827, 13], "temperature": 0.0, "avg_logprob": -0.1160078363104181, "compression_ratio": 1.627906976744186, "no_speech_prob": 1.6442087144241668e-05}, {"id": 494, "seek": 343172, "start": 3452.4399999999996, "end": 3458.6, "text": " So this is how we create a path object. Path objects have, for example, a make directory,", "tokens": [407, 341, 307, 577, 321, 1884, 257, 3100, 2657, 13, 21914, 6565, 362, 11, 337, 1365, 11, 257, 652, 21120, 11], "temperature": 0.0, "avg_logprob": -0.1160078363104181, "compression_ratio": 1.627906976744186, "no_speech_prob": 1.6442087144241668e-05}, {"id": 495, "seek": 345860, "start": 3458.6, "end": 3467.36, "text": " work dir method. So I like to get everything set up, but I want to be able to rerun this", "tokens": [589, 4746, 3170, 13, 407, 286, 411, 281, 483, 1203, 992, 493, 11, 457, 286, 528, 281, 312, 1075, 281, 43819, 409, 341], "temperature": 0.0, "avg_logprob": -0.15960116305593716, "compression_ratio": 1.6872586872586872, "no_speech_prob": 2.2473837816505693e-05}, {"id": 496, "seek": 345860, "start": 3467.36, "end": 3472.56, "text": " cell lots of times and not have it like give me errors if I run it more than once. If I", "tokens": [2815, 3195, 295, 1413, 293, 406, 362, 309, 411, 976, 385, 13603, 498, 286, 1190, 309, 544, 813, 1564, 13, 759, 286], "temperature": 0.0, "avg_logprob": -0.15960116305593716, "compression_ratio": 1.6872586872586872, "no_speech_prob": 2.2473837816505693e-05}, {"id": 497, "seek": 345860, "start": 3472.56, "end": 3477.48, "text": " run it a second time, it still works. And in that case, that's because I put this exist", "tokens": [1190, 309, 257, 1150, 565, 11, 309, 920, 1985, 13, 400, 294, 300, 1389, 11, 300, 311, 570, 286, 829, 341, 2514], "temperature": 0.0, "avg_logprob": -0.15960116305593716, "compression_ratio": 1.6872586872586872, "no_speech_prob": 2.2473837816505693e-05}, {"id": 498, "seek": 345860, "start": 3477.48, "end": 3482.0, "text": " okay equals true. How did I know that I can say, because otherwise it would try to make", "tokens": [1392, 6915, 2074, 13, 1012, 630, 286, 458, 300, 286, 393, 584, 11, 570, 5911, 309, 576, 853, 281, 652], "temperature": 0.0, "avg_logprob": -0.15960116305593716, "compression_ratio": 1.6872586872586872, "no_speech_prob": 2.2473837816505693e-05}, {"id": 499, "seek": 345860, "start": 3482.0, "end": 3485.8399999999997, "text": " the directory, it would already exist and it would give an error. How do I know what", "tokens": [264, 21120, 11, 309, 576, 1217, 2514, 293, 309, 576, 976, 364, 6713, 13, 1012, 360, 286, 458, 437], "temperature": 0.0, "avg_logprob": -0.15960116305593716, "compression_ratio": 1.6872586872586872, "no_speech_prob": 2.2473837816505693e-05}, {"id": 500, "seek": 348584, "start": 3485.84, "end": 3492.04, "text": " parameters I can pass to make dir? I just press shift tab. And so when I hit shift tab,", "tokens": [9834, 286, 393, 1320, 281, 652, 4746, 30, 286, 445, 1886, 5513, 4421, 13, 400, 370, 562, 286, 2045, 5513, 4421, 11], "temperature": 0.0, "avg_logprob": -0.10835670780491184, "compression_ratio": 1.4858757062146892, "no_speech_prob": 3.4465695080143632e-06}, {"id": 501, "seek": 348584, "start": 3492.04, "end": 3499.04, "text": " it tells me what options there are. If I press it a few times, it'll actually pop it down", "tokens": [309, 5112, 385, 437, 3956, 456, 366, 13, 759, 286, 1886, 309, 257, 1326, 1413, 11, 309, 603, 767, 1665, 309, 760], "temperature": 0.0, "avg_logprob": -0.10835670780491184, "compression_ratio": 1.4858757062146892, "no_speech_prob": 3.4465695080143632e-06}, {"id": 502, "seek": 348584, "start": 3499.04, "end": 3509.44, "text": " to the bottom of the screen to remind me. I can press escape to get rid of it. Or you", "tokens": [281, 264, 2767, 295, 264, 2568, 281, 4160, 385, 13, 286, 393, 1886, 7615, 281, 483, 3973, 295, 309, 13, 1610, 291], "temperature": 0.0, "avg_logprob": -0.10835670780491184, "compression_ratio": 1.4858757062146892, "no_speech_prob": 3.4465695080143632e-06}, {"id": 503, "seek": 350944, "start": 3509.44, "end": 3518.2400000000002, "text": " can just, or else you can just hit tab inside and it'll list all the things you could type", "tokens": [393, 445, 11, 420, 1646, 291, 393, 445, 2045, 4421, 1854, 293, 309, 603, 1329, 439, 264, 721, 291, 727, 2010], "temperature": 0.0, "avg_logprob": -0.17374412380919166, "compression_ratio": 1.3333333333333333, "no_speech_prob": 2.7264618438493926e-06}, {"id": 504, "seek": 350944, "start": 3518.2400000000002, "end": 3532.92, "text": " here as you can see. All right. So we need to grab this URL. And so Python comes with", "tokens": [510, 382, 291, 393, 536, 13, 1057, 558, 13, 407, 321, 643, 281, 4444, 341, 12905, 13, 400, 370, 15329, 1487, 365], "temperature": 0.0, "avg_logprob": -0.17374412380919166, "compression_ratio": 1.3333333333333333, "no_speech_prob": 2.7264618438493926e-06}, {"id": 505, "seek": 353292, "start": 3532.92, "end": 3539.8, "text": " something for doing that, which is the URL lib library that's part of Python that has", "tokens": [746, 337, 884, 300, 11, 597, 307, 264, 12905, 22854, 6405, 300, 311, 644, 295, 15329, 300, 575], "temperature": 0.0, "avg_logprob": -0.13212052602616567, "compression_ratio": 1.4944444444444445, "no_speech_prob": 3.0415837954933522e-06}, {"id": 506, "seek": 353292, "start": 3539.8, "end": 3546.6800000000003, "text": " something called URL retrieve. And something which I'm always a bit surprised is not widely", "tokens": [746, 1219, 12905, 30254, 13, 400, 746, 597, 286, 478, 1009, 257, 857, 6100, 307, 406, 13371], "temperature": 0.0, "avg_logprob": -0.13212052602616567, "compression_ratio": 1.4944444444444445, "no_speech_prob": 3.0415837954933522e-06}, {"id": 507, "seek": 353292, "start": 3546.6800000000003, "end": 3557.44, "text": " used as people reading the Python documentation. So you should do that a lot. So if I click", "tokens": [1143, 382, 561, 3760, 264, 15329, 14333, 13, 407, 291, 820, 360, 300, 257, 688, 13, 407, 498, 286, 2052], "temperature": 0.0, "avg_logprob": -0.13212052602616567, "compression_ratio": 1.4944444444444445, "no_speech_prob": 3.0415837954933522e-06}, {"id": 508, "seek": 355744, "start": 3557.44, "end": 3571.52, "text": " on that, here is the documentation for URL retrieve. And so I can find exactly what it", "tokens": [322, 300, 11, 510, 307, 264, 14333, 337, 12905, 30254, 13, 400, 370, 286, 393, 915, 2293, 437, 309], "temperature": 0.0, "avg_logprob": -0.09984048883965675, "compression_ratio": 1.5, "no_speech_prob": 1.5534964177277288e-06}, {"id": 509, "seek": 355744, "start": 3571.52, "end": 3583.88, "text": " can take and I can learn about exactly what it does. And so I, yeah, I read the documentation", "tokens": [393, 747, 293, 286, 393, 1466, 466, 2293, 437, 309, 775, 13, 400, 370, 286, 11, 1338, 11, 286, 1401, 264, 14333], "temperature": 0.0, "avg_logprob": -0.09984048883965675, "compression_ratio": 1.5, "no_speech_prob": 1.5534964177277288e-06}, {"id": 510, "seek": 358388, "start": 3583.88, "end": 3591.1600000000003, "text": " from the Python docs for every single method I use. And I look at every single option that", "tokens": [490, 264, 15329, 45623, 337, 633, 2167, 3170, 286, 764, 13, 400, 286, 574, 412, 633, 2167, 3614, 300], "temperature": 0.0, "avg_logprob": -0.131536442892892, "compression_ratio": 1.5755813953488371, "no_speech_prob": 3.726626573552494e-06}, {"id": 511, "seek": 358388, "start": 3591.1600000000003, "end": 3601.0, "text": " it takes and then I practice with it. And to practice with it, I practice inside, inside", "tokens": [309, 2516, 293, 550, 286, 3124, 365, 309, 13, 400, 281, 3124, 365, 309, 11, 286, 3124, 1854, 11, 1854], "temperature": 0.0, "avg_logprob": -0.131536442892892, "compression_ratio": 1.5755813953488371, "no_speech_prob": 3.726626573552494e-06}, {"id": 512, "seek": 358388, "start": 3601.0, "end": 3610.08, "text": " Jupiter. So if I want this import on its own, I can hit control shift hyphen and it's going", "tokens": [24567, 13, 407, 498, 286, 528, 341, 974, 322, 1080, 1065, 11, 286, 393, 2045, 1969, 5513, 2477, 47059, 293, 309, 311, 516], "temperature": 0.0, "avg_logprob": -0.131536442892892, "compression_ratio": 1.5755813953488371, "no_speech_prob": 3.726626573552494e-06}, {"id": 513, "seek": 361008, "start": 3610.08, "end": 3619.84, "text": " to split it into two cells and then I'll hit alt enter or option enter. So I can create", "tokens": [281, 7472, 309, 666, 732, 5438, 293, 550, 286, 603, 2045, 4955, 3242, 420, 3614, 3242, 13, 407, 286, 393, 1884], "temperature": 0.0, "avg_logprob": -0.11909354942432349, "compression_ratio": 1.5454545454545454, "no_speech_prob": 6.339178980852012e-06}, {"id": 514, "seek": 361008, "start": 3619.84, "end": 3627.88, "text": " something underneath and I can type URL retrieve shift tab. And so there is, there it all is.", "tokens": [746, 7223, 293, 286, 393, 2010, 12905, 30254, 5513, 4421, 13, 400, 370, 456, 307, 11, 456, 309, 439, 307, 13], "temperature": 0.0, "avg_logprob": -0.11909354942432349, "compression_ratio": 1.5454545454545454, "no_speech_prob": 6.339178980852012e-06}, {"id": 515, "seek": 361008, "start": 3627.88, "end": 3634.0, "text": " If I'm like way down somewhere in the notebook and I have no idea where URL retrieve comes", "tokens": [759, 286, 478, 411, 636, 760, 4079, 294, 264, 21060, 293, 286, 362, 572, 1558, 689, 12905, 30254, 1487], "temperature": 0.0, "avg_logprob": -0.11909354942432349, "compression_ratio": 1.5454545454545454, "no_speech_prob": 6.339178980852012e-06}, {"id": 516, "seek": 363400, "start": 3634.0, "end": 3640.36, "text": " from, I can just hit shift enter and it actually tells me exactly where it comes from. And", "tokens": [490, 11, 286, 393, 445, 2045, 5513, 3242, 293, 309, 767, 5112, 385, 2293, 689, 309, 1487, 490, 13, 400], "temperature": 0.0, "avg_logprob": -0.07867264240346056, "compression_ratio": 1.7211538461538463, "no_speech_prob": 2.058045083686011e-06}, {"id": 517, "seek": 363400, "start": 3640.36, "end": 3645.4, "text": " if I want to know more about it, I can just hit question mark, shift enter, and it's going", "tokens": [498, 286, 528, 281, 458, 544, 466, 309, 11, 286, 393, 445, 2045, 1168, 1491, 11, 5513, 3242, 11, 293, 309, 311, 516], "temperature": 0.0, "avg_logprob": -0.07867264240346056, "compression_ratio": 1.7211538461538463, "no_speech_prob": 2.058045083686011e-06}, {"id": 518, "seek": 363400, "start": 3645.4, "end": 3653.84, "text": " to give me the documentation. And most call of all, second question mark and it gives", "tokens": [281, 976, 385, 264, 14333, 13, 400, 881, 818, 295, 439, 11, 1150, 1168, 1491, 293, 309, 2709], "temperature": 0.0, "avg_logprob": -0.07867264240346056, "compression_ratio": 1.7211538461538463, "no_speech_prob": 2.058045083686011e-06}, {"id": 519, "seek": 363400, "start": 3653.84, "end": 3659.88, "text": " me the full source code. And you can see it's not a lot. You know, reading the source code", "tokens": [385, 264, 1577, 4009, 3089, 13, 400, 291, 393, 536, 309, 311, 406, 257, 688, 13, 509, 458, 11, 3760, 264, 4009, 3089], "temperature": 0.0, "avg_logprob": -0.07867264240346056, "compression_ratio": 1.7211538461538463, "no_speech_prob": 2.058045083686011e-06}, {"id": 520, "seek": 365988, "start": 3659.88, "end": 3665.7200000000003, "text": " of Python standard library stuff is often quite revealing and you can see exactly how", "tokens": [295, 15329, 3832, 6405, 1507, 307, 2049, 1596, 23983, 293, 291, 393, 536, 2293, 577], "temperature": 0.0, "avg_logprob": -0.11185423065634335, "compression_ratio": 1.4408602150537635, "no_speech_prob": 1.3709545783058275e-06}, {"id": 521, "seek": 365988, "start": 3665.7200000000003, "end": 3682.32, "text": " they do it. That's a great way to learn more about, more about this. So in this case, I'm", "tokens": [436, 360, 309, 13, 663, 311, 257, 869, 636, 281, 1466, 544, 466, 11, 544, 466, 341, 13, 407, 294, 341, 1389, 11, 286, 478], "temperature": 0.0, "avg_logprob": -0.11185423065634335, "compression_ratio": 1.4408602150537635, "no_speech_prob": 1.3709545783058275e-06}, {"id": 522, "seek": 365988, "start": 3682.32, "end": 3686.92, "text": " just going to use a very simple functionality, which is I'm going to say the URL to retrieve", "tokens": [445, 516, 281, 764, 257, 588, 2199, 14980, 11, 597, 307, 286, 478, 516, 281, 584, 264, 12905, 281, 30254], "temperature": 0.0, "avg_logprob": -0.11185423065634335, "compression_ratio": 1.4408602150537635, "no_speech_prob": 1.3709545783058275e-06}, {"id": 523, "seek": 368692, "start": 3686.92, "end": 3694.36, "text": " and the file name to save it as. And again, I made it so I can run this multiple times.", "tokens": [293, 264, 3991, 1315, 281, 3155, 309, 382, 13, 400, 797, 11, 286, 1027, 309, 370, 286, 393, 1190, 341, 3866, 1413, 13], "temperature": 0.0, "avg_logprob": -0.10530769064071331, "compression_ratio": 1.5863636363636364, "no_speech_prob": 5.507569312612759e-06}, {"id": 524, "seek": 368692, "start": 3694.36, "end": 3699.52, "text": " So it's only going to do the URL retrieve if the path doesn't exist. If I've already", "tokens": [407, 309, 311, 787, 516, 281, 360, 264, 12905, 30254, 498, 264, 3100, 1177, 380, 2514, 13, 759, 286, 600, 1217], "temperature": 0.0, "avg_logprob": -0.10530769064071331, "compression_ratio": 1.5863636363636364, "no_speech_prob": 5.507569312612759e-06}, {"id": 525, "seek": 368692, "start": 3699.52, "end": 3704.08, "text": " downloaded it, I don't want to download it again. So I run that cell and notice that", "tokens": [21748, 309, 11, 286, 500, 380, 528, 281, 5484, 309, 797, 13, 407, 286, 1190, 300, 2815, 293, 3449, 300], "temperature": 0.0, "avg_logprob": -0.10530769064071331, "compression_ratio": 1.5863636363636364, "no_speech_prob": 5.507569312612759e-06}, {"id": 526, "seek": 368692, "start": 3704.08, "end": 3715.48, "text": " I can put exclamation mark followed by a line of bash and it actually runs this using bash.", "tokens": [286, 393, 829, 1624, 43233, 1491, 6263, 538, 257, 1622, 295, 46183, 293, 309, 767, 6676, 341, 1228, 46183, 13], "temperature": 0.0, "avg_logprob": -0.10530769064071331, "compression_ratio": 1.5863636363636364, "no_speech_prob": 5.507569312612759e-06}, {"id": 527, "seek": 371548, "start": 3715.48, "end": 3726.4, "text": " If you're using Windows, this, this won't work. And I would very, very strongly suggest", "tokens": [759, 291, 434, 1228, 8591, 11, 341, 11, 341, 1582, 380, 589, 13, 400, 286, 576, 588, 11, 588, 10613, 3402], "temperature": 0.0, "avg_logprob": -0.16380439864264595, "compression_ratio": 1.5202312138728324, "no_speech_prob": 3.7266238450683886e-06}, {"id": 528, "seek": 371548, "start": 3726.4, "end": 3731.28, "text": " if you're using Windows, use WSL. And if you use WSL, all of these notebooks will work", "tokens": [498, 291, 434, 1228, 8591, 11, 764, 343, 47012, 13, 400, 498, 291, 764, 343, 47012, 11, 439, 295, 613, 43782, 486, 589], "temperature": 0.0, "avg_logprob": -0.16380439864264595, "compression_ratio": 1.5202312138728324, "no_speech_prob": 3.7266238450683886e-06}, {"id": 529, "seek": 371548, "start": 3731.28, "end": 3738.84, "text": " perfectly. So yeah, do that. Or write it on paper space or Lambda labs or something like", "tokens": [6239, 13, 407, 1338, 11, 360, 300, 13, 1610, 2464, 309, 322, 3035, 1901, 420, 45691, 20339, 420, 746, 411], "temperature": 0.0, "avg_logprob": -0.16380439864264595, "compression_ratio": 1.5202312138728324, "no_speech_prob": 3.7266238450683886e-06}, {"id": 530, "seek": 373884, "start": 3738.84, "end": 3749.84, "text": " that, Colab, et cetera. Okay. So this is a gzip file. So thankfully, Python comes with", "tokens": [300, 11, 4004, 455, 11, 1030, 11458, 13, 1033, 13, 407, 341, 307, 257, 290, 27268, 3991, 13, 407, 27352, 11, 15329, 1487, 365], "temperature": 0.0, "avg_logprob": -0.17043000233324268, "compression_ratio": 1.5529411764705883, "no_speech_prob": 1.2679261089942884e-06}, {"id": 531, "seek": 373884, "start": 3749.84, "end": 3755.56, "text": " a gzip module. Python comes with quite a lot actually. And so we can open a gzip file using", "tokens": [257, 290, 27268, 10088, 13, 15329, 1487, 365, 1596, 257, 688, 767, 13, 400, 370, 321, 393, 1269, 257, 290, 27268, 3991, 1228], "temperature": 0.0, "avg_logprob": -0.17043000233324268, "compression_ratio": 1.5529411764705883, "no_speech_prob": 1.2679261089942884e-06}, {"id": 532, "seek": 373884, "start": 3755.56, "end": 3762.92, "text": " gzip.open and we can pass in the path. And we say we're going to read it as binary as", "tokens": [290, 27268, 13, 15752, 293, 321, 393, 1320, 294, 264, 3100, 13, 400, 321, 584, 321, 434, 516, 281, 1401, 309, 382, 17434, 382], "temperature": 0.0, "avg_logprob": -0.17043000233324268, "compression_ratio": 1.5529411764705883, "no_speech_prob": 1.2679261089942884e-06}, {"id": 533, "seek": 376292, "start": 3762.92, "end": 3770.64, "text": " opposed to text. Okay. So this is called a context manager. It's a with clause. And what", "tokens": [8851, 281, 2487, 13, 1033, 13, 407, 341, 307, 1219, 257, 4319, 6598, 13, 467, 311, 257, 365, 25925, 13, 400, 437], "temperature": 0.0, "avg_logprob": -0.13082708631243026, "compression_ratio": 1.6396396396396395, "no_speech_prob": 2.9944310426799348e-06}, {"id": 534, "seek": 376292, "start": 3770.64, "end": 3776.52, "text": " it's going to do is it's going to open up this gzip file. The gzip object will be called", "tokens": [309, 311, 516, 281, 360, 307, 309, 311, 516, 281, 1269, 493, 341, 290, 27268, 3991, 13, 440, 290, 27268, 2657, 486, 312, 1219], "temperature": 0.0, "avg_logprob": -0.13082708631243026, "compression_ratio": 1.6396396396396395, "no_speech_prob": 2.9944310426799348e-06}, {"id": 535, "seek": 376292, "start": 3776.52, "end": 3783.04, "text": " f. And then it runs everything inside the block. And when it's done, it will close the", "tokens": [283, 13, 400, 550, 309, 6676, 1203, 1854, 264, 3461, 13, 400, 562, 309, 311, 1096, 11, 309, 486, 1998, 264], "temperature": 0.0, "avg_logprob": -0.13082708631243026, "compression_ratio": 1.6396396396396395, "no_speech_prob": 2.9944310426799348e-06}, {"id": 536, "seek": 376292, "start": 3783.04, "end": 3790.2000000000003, "text": " file. So with blocks can do all kinds of different things. But in general, with blocks that involve", "tokens": [3991, 13, 407, 365, 8474, 393, 360, 439, 3685, 295, 819, 721, 13, 583, 294, 2674, 11, 365, 8474, 300, 9494], "temperature": 0.0, "avg_logprob": -0.13082708631243026, "compression_ratio": 1.6396396396396395, "no_speech_prob": 2.9944310426799348e-06}, {"id": 537, "seek": 379020, "start": 3790.2, "end": 3797.2799999999997, "text": " files, they're going to close the file automatically for you. So we can now do that. And so you", "tokens": [7098, 11, 436, 434, 516, 281, 1998, 264, 3991, 6772, 337, 291, 13, 407, 321, 393, 586, 360, 300, 13, 400, 370, 291], "temperature": 0.0, "avg_logprob": -0.09366231379301651, "compression_ratio": 1.6380090497737556, "no_speech_prob": 7.811470936758269e-07}, {"id": 538, "seek": 379020, "start": 3797.2799999999997, "end": 3803.96, "text": " can see it's opened up the gzip file and the gzip file contains what's called pickle objects.", "tokens": [393, 536, 309, 311, 5625, 493, 264, 290, 27268, 3991, 293, 264, 290, 27268, 3991, 8306, 437, 311, 1219, 31433, 6565, 13], "temperature": 0.0, "avg_logprob": -0.09366231379301651, "compression_ratio": 1.6380090497737556, "no_speech_prob": 7.811470936758269e-07}, {"id": 539, "seek": 379020, "start": 3803.96, "end": 3808.8399999999997, "text": " Pickled objects is basically Python objects that have been saved to disk. It's the main", "tokens": [14129, 1493, 6565, 307, 1936, 15329, 6565, 300, 362, 668, 6624, 281, 12355, 13, 467, 311, 264, 2135], "temperature": 0.0, "avg_logprob": -0.09366231379301651, "compression_ratio": 1.6380090497737556, "no_speech_prob": 7.811470936758269e-07}, {"id": 540, "seek": 379020, "start": 3808.8399999999997, "end": 3815.48, "text": " way that people in pure Python save stuff. And it's part of the standard library. So", "tokens": [636, 300, 561, 294, 6075, 15329, 3155, 1507, 13, 400, 309, 311, 644, 295, 264, 3832, 6405, 13, 407], "temperature": 0.0, "avg_logprob": -0.09366231379301651, "compression_ratio": 1.6380090497737556, "no_speech_prob": 7.811470936758269e-07}, {"id": 541, "seek": 381548, "start": 3815.48, "end": 3824.04, "text": " this is how we load in from that file. Now the file contains a tuple of tuples. So when", "tokens": [341, 307, 577, 321, 3677, 294, 490, 300, 3991, 13, 823, 264, 3991, 8306, 257, 2604, 781, 295, 2604, 2622, 13, 407, 562], "temperature": 0.0, "avg_logprob": -0.10077334988501764, "compression_ratio": 1.6634146341463414, "no_speech_prob": 1.5294115200958913e-06}, {"id": 542, "seek": 381548, "start": 3824.04, "end": 3828.86, "text": " you put a tuple on the left hand side of an equal sign, it's quite neat. It allows us", "tokens": [291, 829, 257, 2604, 781, 322, 264, 1411, 1011, 1252, 295, 364, 2681, 1465, 11, 309, 311, 1596, 10654, 13, 467, 4045, 505], "temperature": 0.0, "avg_logprob": -0.10077334988501764, "compression_ratio": 1.6634146341463414, "no_speech_prob": 1.5294115200958913e-06}, {"id": 543, "seek": 381548, "start": 3828.86, "end": 3834.08, "text": " to put the first tuple into two variables called x train and y train and the second", "tokens": [281, 829, 264, 700, 2604, 781, 666, 732, 9102, 1219, 2031, 3847, 293, 288, 3847, 293, 264, 1150], "temperature": 0.0, "avg_logprob": -0.10077334988501764, "compression_ratio": 1.6634146341463414, "no_speech_prob": 1.5294115200958913e-06}, {"id": 544, "seek": 381548, "start": 3834.08, "end": 3840.8, "text": " into x valid and y valid. This trick here where you put stuff like this on the left", "tokens": [666, 2031, 7363, 293, 288, 7363, 13, 639, 4282, 510, 689, 291, 829, 1507, 411, 341, 322, 264, 1411], "temperature": 0.0, "avg_logprob": -0.10077334988501764, "compression_ratio": 1.6634146341463414, "no_speech_prob": 1.5294115200958913e-06}, {"id": 545, "seek": 384080, "start": 3840.8, "end": 3848.76, "text": " is called destructuring. And it's a super handy way to make your code kind of clear", "tokens": [307, 1219, 2677, 1757, 1345, 13, 400, 309, 311, 257, 1687, 13239, 636, 281, 652, 428, 3089, 733, 295, 1850], "temperature": 0.0, "avg_logprob": -0.12951493525243069, "compression_ratio": 1.502183406113537, "no_speech_prob": 8.013415026653092e-06}, {"id": 546, "seek": 384080, "start": 3848.76, "end": 3856.5600000000004, "text": " and concise. And lots of languages support that, including Python. Okay. So we've now", "tokens": [293, 44882, 13, 400, 3195, 295, 8650, 1406, 300, 11, 3009, 15329, 13, 1033, 13, 407, 321, 600, 586], "temperature": 0.0, "avg_logprob": -0.12951493525243069, "compression_ratio": 1.502183406113537, "no_speech_prob": 8.013415026653092e-06}, {"id": 547, "seek": 384080, "start": 3856.5600000000004, "end": 3862.92, "text": " got some data. And so we can have a look at it. Now it's a bit tricky because we're not", "tokens": [658, 512, 1412, 13, 400, 370, 321, 393, 362, 257, 574, 412, 309, 13, 823, 309, 311, 257, 857, 12414, 570, 321, 434, 406], "temperature": 0.0, "avg_logprob": -0.12951493525243069, "compression_ratio": 1.502183406113537, "no_speech_prob": 8.013415026653092e-06}, {"id": 548, "seek": 384080, "start": 3862.92, "end": 3867.04, "text": " allowed to use NumPy according to our rules. But unfortunately, this actually comes as", "tokens": [4350, 281, 764, 22592, 47, 88, 4650, 281, 527, 4474, 13, 583, 7015, 11, 341, 767, 1487, 382], "temperature": 0.0, "avg_logprob": -0.12951493525243069, "compression_ratio": 1.502183406113537, "no_speech_prob": 8.013415026653092e-06}, {"id": 549, "seek": 386704, "start": 3867.04, "end": 3876.7599999999998, "text": " NumPy. So I've turned it into a list. All right. So I've taken the first image and I've", "tokens": [22592, 47, 88, 13, 407, 286, 600, 3574, 309, 666, 257, 1329, 13, 1057, 558, 13, 407, 286, 600, 2726, 264, 700, 3256, 293, 286, 600], "temperature": 0.0, "avg_logprob": -0.08490054130554199, "compression_ratio": 1.661904761904762, "no_speech_prob": 6.144131020846544e-06}, {"id": 550, "seek": 386704, "start": 3876.7599999999998, "end": 3884.7599999999998, "text": " turned it into a list. And so we can look at a few examples of some values in that list.", "tokens": [3574, 309, 666, 257, 1329, 13, 400, 370, 321, 393, 574, 412, 257, 1326, 5110, 295, 512, 4190, 294, 300, 1329, 13], "temperature": 0.0, "avg_logprob": -0.08490054130554199, "compression_ratio": 1.661904761904762, "no_speech_prob": 6.144131020846544e-06}, {"id": 551, "seek": 386704, "start": 3884.7599999999998, "end": 3888.72, "text": " And here they are. So it looks like they're numbers between zero and one. And this is", "tokens": [400, 510, 436, 366, 13, 407, 309, 1542, 411, 436, 434, 3547, 1296, 4018, 293, 472, 13, 400, 341, 307], "temperature": 0.0, "avg_logprob": -0.08490054130554199, "compression_ratio": 1.661904761904762, "no_speech_prob": 6.144131020846544e-06}, {"id": 552, "seek": 386704, "start": 3888.72, "end": 3895.84, "text": " what I do, you know, when I learn about a new data set. So when I started writing this", "tokens": [437, 286, 360, 11, 291, 458, 11, 562, 286, 1466, 466, 257, 777, 1412, 992, 13, 407, 562, 286, 1409, 3579, 341], "temperature": 0.0, "avg_logprob": -0.08490054130554199, "compression_ratio": 1.661904761904762, "no_speech_prob": 6.144131020846544e-06}, {"id": 553, "seek": 389584, "start": 3895.84, "end": 3904.1600000000003, "text": " notebook, what you see here, other than the pros here, is what I actually did when I was", "tokens": [21060, 11, 437, 291, 536, 510, 11, 661, 813, 264, 6267, 510, 11, 307, 437, 286, 767, 630, 562, 286, 390], "temperature": 0.0, "avg_logprob": -0.12405253429802096, "compression_ratio": 1.3968253968253967, "no_speech_prob": 1.7502676200820133e-05}, {"id": 554, "seek": 389584, "start": 3904.1600000000003, "end": 3911.6800000000003, "text": " working with this data. I wanted to know what it was. So I just grab a little bit of it", "tokens": [1364, 365, 341, 1412, 13, 286, 1415, 281, 458, 437, 309, 390, 13, 407, 286, 445, 4444, 257, 707, 857, 295, 309], "temperature": 0.0, "avg_logprob": -0.12405253429802096, "compression_ratio": 1.3968253968253967, "no_speech_prob": 1.7502676200820133e-05}, {"id": 555, "seek": 391168, "start": 3911.68, "end": 3930.9199999999996, "text": " and look at it. So I kind of got a sense now of what it is. Now, interestingly, it's 784.", "tokens": [293, 574, 412, 309, 13, 407, 286, 733, 295, 658, 257, 2020, 586, 295, 437, 309, 307, 13, 823, 11, 25873, 11, 309, 311, 1614, 25494, 13], "temperature": 0.0, "avg_logprob": -0.15907300751784753, "compression_ratio": 1.267605633802817, "no_speech_prob": 5.507566129381303e-06}, {"id": 556, "seek": 391168, "start": 3930.9199999999996, "end": 3940.48, "text": " This image is 784 long list. Oh dear. People freaking out in the comments. No NumPy. Yeah,", "tokens": [639, 3256, 307, 1614, 25494, 938, 1329, 13, 876, 6875, 13, 3432, 14612, 484, 294, 264, 3053, 13, 883, 22592, 47, 88, 13, 865, 11], "temperature": 0.0, "avg_logprob": -0.15907300751784753, "compression_ratio": 1.267605633802817, "no_speech_prob": 5.507566129381303e-06}, {"id": 557, "seek": 394048, "start": 3940.48, "end": 3947.44, "text": " no NumPy. Do you see NumPy? No NumPy. Why 784? What is that? Well, that's because these", "tokens": [572, 22592, 47, 88, 13, 1144, 291, 536, 22592, 47, 88, 30, 883, 22592, 47, 88, 13, 1545, 1614, 25494, 30, 708, 307, 300, 30, 1042, 11, 300, 311, 570, 613], "temperature": 0.0, "avg_logprob": -0.1292618389787345, "compression_ratio": 1.6342592592592593, "no_speech_prob": 1.5446079487446696e-05}, {"id": 558, "seek": 394048, "start": 3947.44, "end": 3956.96, "text": " are 28 by 28 images. So it's just a flat list here of 784 long. So how do I turn this 784", "tokens": [366, 7562, 538, 7562, 5267, 13, 407, 309, 311, 445, 257, 4962, 1329, 510, 295, 1614, 25494, 938, 13, 407, 577, 360, 286, 1261, 341, 1614, 25494], "temperature": 0.0, "avg_logprob": -0.1292618389787345, "compression_ratio": 1.6342592592592593, "no_speech_prob": 1.5446079487446696e-05}, {"id": 559, "seek": 394048, "start": 3956.96, "end": 3963.64, "text": " long thing into 28 by 28? So I want a 28, a list of 28, lists of 28 basically, because", "tokens": [938, 551, 666, 7562, 538, 7562, 30, 407, 286, 528, 257, 7562, 11, 257, 1329, 295, 7562, 11, 14511, 295, 7562, 1936, 11, 570], "temperature": 0.0, "avg_logprob": -0.1292618389787345, "compression_ratio": 1.6342592592592593, "no_speech_prob": 1.5446079487446696e-05}, {"id": 560, "seek": 394048, "start": 3963.64, "end": 3969.6, "text": " we don't have matrices. So how do we do that? And so we're going to be learning a lot of", "tokens": [321, 500, 380, 362, 32284, 13, 407, 577, 360, 321, 360, 300, 30, 400, 370, 321, 434, 516, 281, 312, 2539, 257, 688, 295], "temperature": 0.0, "avg_logprob": -0.1292618389787345, "compression_ratio": 1.6342592592592593, "no_speech_prob": 1.5446079487446696e-05}, {"id": 561, "seek": 396960, "start": 3969.6, "end": 3978.16, "text": " cool stuff in Python here. Sorry, I can't stop laughing at all the stuff in our chat.", "tokens": [1627, 1507, 294, 15329, 510, 13, 4919, 11, 286, 393, 380, 1590, 5059, 412, 439, 264, 1507, 294, 527, 5081, 13], "temperature": 0.0, "avg_logprob": -0.1757954375384605, "compression_ratio": 1.3769633507853403, "no_speech_prob": 2.4682445655344054e-05}, {"id": 562, "seek": 396960, "start": 3978.16, "end": 3986.7999999999997, "text": " Oh dear. People are quite reasonably freaking out. That's OK. We'll get there. I promise.", "tokens": [876, 6875, 13, 3432, 366, 1596, 23551, 14612, 484, 13, 663, 311, 2264, 13, 492, 603, 483, 456, 13, 286, 6228, 13], "temperature": 0.0, "avg_logprob": -0.1757954375384605, "compression_ratio": 1.3769633507853403, "no_speech_prob": 2.4682445655344054e-05}, {"id": 563, "seek": 396960, "start": 3986.7999999999997, "end": 3994.24, "text": " I hope. Otherwise I'll embarrass myself. All right. So how do I convert a 784 long list", "tokens": [286, 1454, 13, 10328, 286, 603, 9187, 2059, 13, 1057, 558, 13, 407, 577, 360, 286, 7620, 257, 1614, 25494, 938, 1329], "temperature": 0.0, "avg_logprob": -0.1757954375384605, "compression_ratio": 1.3769633507853403, "no_speech_prob": 2.4682445655344054e-05}, {"id": 564, "seek": 399424, "start": 3994.24, "end": 4004.7999999999997, "text": " into 28 lists, 28 long list of 28 long lists? I'm going to use something called chunks.", "tokens": [666, 7562, 14511, 11, 7562, 938, 1329, 295, 7562, 938, 14511, 30, 286, 478, 516, 281, 764, 746, 1219, 24004, 13], "temperature": 0.0, "avg_logprob": -0.13565100836999638, "compression_ratio": 1.6603773584905661, "no_speech_prob": 4.22281163992011e-06}, {"id": 565, "seek": 399424, "start": 4004.7999999999997, "end": 4008.7599999999998, "text": " And first of all, I'll show you what this thing does and then I'll show you how it works.", "tokens": [400, 700, 295, 439, 11, 286, 603, 855, 291, 437, 341, 551, 775, 293, 550, 286, 603, 855, 291, 577, 309, 1985, 13], "temperature": 0.0, "avg_logprob": -0.13565100836999638, "compression_ratio": 1.6603773584905661, "no_speech_prob": 4.22281163992011e-06}, {"id": 566, "seek": 399424, "start": 4008.7599999999998, "end": 4016.7999999999997, "text": " So vowels is currently a list of 10 things. Now if I take vowels and I pass it to chunks", "tokens": [407, 44972, 307, 4362, 257, 1329, 295, 1266, 721, 13, 823, 498, 286, 747, 44972, 293, 286, 1320, 309, 281, 24004], "temperature": 0.0, "avg_logprob": -0.13565100836999638, "compression_ratio": 1.6603773584905661, "no_speech_prob": 4.22281163992011e-06}, {"id": 567, "seek": 399424, "start": 4016.7999999999997, "end": 4023.8399999999997, "text": " with 5, it creates two lists of 5. Here's list number 1 of 5 elements and here's list", "tokens": [365, 1025, 11, 309, 7829, 732, 14511, 295, 1025, 13, 1692, 311, 1329, 1230, 502, 295, 1025, 4959, 293, 510, 311, 1329], "temperature": 0.0, "avg_logprob": -0.13565100836999638, "compression_ratio": 1.6603773584905661, "no_speech_prob": 4.22281163992011e-06}, {"id": 568, "seek": 402384, "start": 4023.84, "end": 4032.08, "text": " number 2 of 5 elements. Hopefully you can see what it's doing. It's chunkifying this", "tokens": [1230, 568, 295, 1025, 4959, 13, 10429, 291, 393, 536, 437, 309, 311, 884, 13, 467, 311, 16635, 5489, 341], "temperature": 0.0, "avg_logprob": -0.12603242773758738, "compression_ratio": 1.5132743362831858, "no_speech_prob": 7.690368306612072e-07}, {"id": 569, "seek": 402384, "start": 4032.08, "end": 4039.6800000000003, "text": " list and this is the length of each chunk. Now how did it do that? The way I did it is", "tokens": [1329, 293, 341, 307, 264, 4641, 295, 1184, 16635, 13, 823, 577, 630, 309, 360, 300, 30, 440, 636, 286, 630, 309, 307], "temperature": 0.0, "avg_logprob": -0.12603242773758738, "compression_ratio": 1.5132743362831858, "no_speech_prob": 7.690368306612072e-07}, {"id": 570, "seek": 402384, "start": 4039.6800000000003, "end": 4044.4, "text": " using a very, very useful thing in Python that far too many people don't know about,", "tokens": [1228, 257, 588, 11, 588, 4420, 551, 294, 15329, 300, 1400, 886, 867, 561, 500, 380, 458, 466, 11], "temperature": 0.0, "avg_logprob": -0.12603242773758738, "compression_ratio": 1.5132743362831858, "no_speech_prob": 7.690368306612072e-07}, {"id": 571, "seek": 402384, "start": 4044.4, "end": 4050.6000000000004, "text": " which is called yield. And what yield does is you can see here, I've got a loop. It's", "tokens": [597, 307, 1219, 11257, 13, 400, 437, 11257, 775, 307, 291, 393, 536, 510, 11, 286, 600, 658, 257, 6367, 13, 467, 311], "temperature": 0.0, "avg_logprob": -0.12603242773758738, "compression_ratio": 1.5132743362831858, "no_speech_prob": 7.690368306612072e-07}, {"id": 572, "seek": 405060, "start": 4050.6, "end": 4058.68, "text": " going to go through from 0 up to the length of my list and it's going to jump by 5 at", "tokens": [516, 281, 352, 807, 490, 1958, 493, 281, 264, 4641, 295, 452, 1329, 293, 309, 311, 516, 281, 3012, 538, 1025, 412], "temperature": 0.0, "avg_logprob": -0.1274372383400246, "compression_ratio": 1.7124183006535947, "no_speech_prob": 3.966976692026947e-06}, {"id": 573, "seek": 405060, "start": 4058.68, "end": 4065.92, "text": " a time. So it's going to go, in this case, 0, 5. And then it's going to, think of this", "tokens": [257, 565, 13, 407, 309, 311, 516, 281, 352, 11, 294, 341, 1389, 11, 1958, 11, 1025, 13, 400, 550, 309, 311, 516, 281, 11, 519, 295, 341], "temperature": 0.0, "avg_logprob": -0.1274372383400246, "compression_ratio": 1.7124183006535947, "no_speech_prob": 3.966976692026947e-06}, {"id": 574, "seek": 405060, "start": 4065.92, "end": 4073.3399999999997, "text": " as being like return for now, it's going to return the list from 0 up to 5. So it returns", "tokens": [382, 885, 411, 2736, 337, 586, 11, 309, 311, 516, 281, 2736, 264, 1329, 490, 1958, 493, 281, 1025, 13, 407, 309, 11247], "temperature": 0.0, "avg_logprob": -0.1274372383400246, "compression_ratio": 1.7124183006535947, "no_speech_prob": 3.966976692026947e-06}, {"id": 575, "seek": 407334, "start": 4073.34, "end": 4081.92, "text": " the first bit of the list. But yield doesn't just return. It kind of like returns a bit", "tokens": [264, 700, 857, 295, 264, 1329, 13, 583, 11257, 1177, 380, 445, 2736, 13, 467, 733, 295, 411, 11247, 257, 857], "temperature": 0.0, "avg_logprob": -0.1375733889066256, "compression_ratio": 1.6226415094339623, "no_speech_prob": 1.5779588693476398e-06}, {"id": 576, "seek": 407334, "start": 4081.92, "end": 4091.2400000000002, "text": " and then it continues. And it returns a bit more. And so specifically what yield does", "tokens": [293, 550, 309, 6515, 13, 400, 309, 11247, 257, 857, 544, 13, 400, 370, 4682, 437, 11257, 775], "temperature": 0.0, "avg_logprob": -0.1375733889066256, "compression_ratio": 1.6226415094339623, "no_speech_prob": 1.5779588693476398e-06}, {"id": 577, "seek": 407334, "start": 4091.2400000000002, "end": 4098.360000000001, "text": " is it creates an iterator. An iterator is basically something you can, well actually", "tokens": [307, 309, 7829, 364, 17138, 1639, 13, 1107, 17138, 1639, 307, 1936, 746, 291, 393, 11, 731, 767], "temperature": 0.0, "avg_logprob": -0.1375733889066256, "compression_ratio": 1.6226415094339623, "no_speech_prob": 1.5779588693476398e-06}, {"id": 578, "seek": 409836, "start": 4098.36, "end": 4104.839999999999, "text": " let's use it, that you can call next on a bunch of times. So let's try it. So we can", "tokens": [718, 311, 764, 309, 11, 300, 291, 393, 818, 958, 322, 257, 3840, 295, 1413, 13, 407, 718, 311, 853, 309, 13, 407, 321, 393], "temperature": 0.0, "avg_logprob": -0.16883151154769094, "compression_ratio": 1.628930817610063, "no_speech_prob": 3.2887435281736543e-06}, {"id": 579, "seek": 409836, "start": 4104.839999999999, "end": 4118.04, "text": " say iterator equals, okay, oh, gotta run it. So what is iterator? Well iterator, iter is", "tokens": [584, 17138, 1639, 6915, 11, 1392, 11, 1954, 11, 3428, 1190, 309, 13, 407, 437, 307, 17138, 1639, 30, 1042, 17138, 1639, 11, 17138, 307], "temperature": 0.0, "avg_logprob": -0.16883151154769094, "compression_ratio": 1.628930817610063, "no_speech_prob": 3.2887435281736543e-06}, {"id": 580, "seek": 409836, "start": 4118.04, "end": 4122.799999999999, "text": " something that I can basically, I can call next on. And next basically says yield the", "tokens": [746, 300, 286, 393, 1936, 11, 286, 393, 818, 958, 322, 13, 400, 958, 1936, 1619, 11257, 264], "temperature": 0.0, "avg_logprob": -0.16883151154769094, "compression_ratio": 1.628930817610063, "no_speech_prob": 3.2887435281736543e-06}, {"id": 581, "seek": 412280, "start": 4122.8, "end": 4134.96, "text": " next thing. So this should yield valves 0, 5. There it is. It did, right? There's valves", "tokens": [958, 551, 13, 407, 341, 820, 11257, 34950, 1958, 11, 1025, 13, 821, 309, 307, 13, 467, 630, 11, 558, 30, 821, 311, 34950], "temperature": 0.0, "avg_logprob": -0.1609516143798828, "compression_ratio": 1.3134328358208955, "no_speech_prob": 1.7061818198271794e-06}, {"id": 582, "seek": 412280, "start": 4134.96, "end": 4141.12, "text": " 0, 5. Now if I run that again it's going to give me a different answer because it's now", "tokens": [1958, 11, 1025, 13, 823, 498, 286, 1190, 300, 797, 309, 311, 516, 281, 976, 385, 257, 819, 1867, 570, 309, 311, 586], "temperature": 0.0, "avg_logprob": -0.1609516143798828, "compression_ratio": 1.3134328358208955, "no_speech_prob": 1.7061818198271794e-06}, {"id": 583, "seek": 414112, "start": 4141.12, "end": 4156.24, "text": " up to the second part of this loop. Now it returns the last 5. Okay. So this is what", "tokens": [493, 281, 264, 1150, 644, 295, 341, 6367, 13, 823, 309, 11247, 264, 1036, 1025, 13, 1033, 13, 407, 341, 307, 437], "temperature": 0.0, "avg_logprob": -0.13686583075724856, "compression_ratio": 1.5823529411764705, "no_speech_prob": 1.4823567653365899e-06}, {"id": 584, "seek": 414112, "start": 4156.24, "end": 4163.32, "text": " an iterator does. Now if you pass an iterator to Python's list, it runs through the entire", "tokens": [364, 17138, 1639, 775, 13, 823, 498, 291, 1320, 364, 17138, 1639, 281, 15329, 311, 1329, 11, 309, 6676, 807, 264, 2302], "temperature": 0.0, "avg_logprob": -0.13686583075724856, "compression_ratio": 1.5823529411764705, "no_speech_prob": 1.4823567653365899e-06}, {"id": 585, "seek": 414112, "start": 4163.32, "end": 4167.96, "text": " letter iterator until it's finished and creates a list of the results. And what does finished", "tokens": [5063, 17138, 1639, 1826, 309, 311, 4335, 293, 7829, 257, 1329, 295, 264, 3542, 13, 400, 437, 775, 4335], "temperature": 0.0, "avg_logprob": -0.13686583075724856, "compression_ratio": 1.5823529411764705, "no_speech_prob": 1.4823567653365899e-06}, {"id": 586, "seek": 416796, "start": 4167.96, "end": 4174.08, "text": " looks like? This is what finished looks like. If you call next and get stop iteration, that", "tokens": [1542, 411, 30, 639, 307, 437, 4335, 1542, 411, 13, 759, 291, 818, 958, 293, 483, 1590, 24784, 11, 300], "temperature": 0.0, "avg_logprob": -0.09718058207263686, "compression_ratio": 1.459016393442623, "no_speech_prob": 2.5612760055082617e-06}, {"id": 587, "seek": 416796, "start": 4174.08, "end": 4179.24, "text": " means you've run out. And that makes sense, right? Because my loop, there's nothing left", "tokens": [1355, 291, 600, 1190, 484, 13, 400, 300, 1669, 2020, 11, 558, 30, 1436, 452, 6367, 11, 456, 311, 1825, 1411], "temperature": 0.0, "avg_logprob": -0.09718058207263686, "compression_ratio": 1.459016393442623, "no_speech_prob": 2.5612760055082617e-06}, {"id": 588, "seek": 416796, "start": 4179.24, "end": 4188.96, "text": " in it. So all of that is to say we now have a way of taking a list and chunkifying it.", "tokens": [294, 309, 13, 407, 439, 295, 300, 307, 281, 584, 321, 586, 362, 257, 636, 295, 1940, 257, 1329, 293, 16635, 5489, 309, 13], "temperature": 0.0, "avg_logprob": -0.09718058207263686, "compression_ratio": 1.459016393442623, "no_speech_prob": 2.5612760055082617e-06}, {"id": 589, "seek": 418896, "start": 4188.96, "end": 4199.12, "text": " So what if I now take my full image, image number 1, chunkify it into chunks of 28 long", "tokens": [407, 437, 498, 286, 586, 747, 452, 1577, 3256, 11, 3256, 1230, 502, 11, 16635, 2505, 309, 666, 24004, 295, 7562, 938], "temperature": 0.0, "avg_logprob": -0.1258816623687744, "compression_ratio": 1.3257575757575757, "no_speech_prob": 5.285505721985828e-07}, {"id": 590, "seek": 418896, "start": 4199.12, "end": 4209.28, "text": " and turn that into a list and plot it. Ta-da! We have successfully created an image. So", "tokens": [293, 1261, 300, 666, 257, 1329, 293, 7542, 309, 13, 6551, 12, 2675, 0, 492, 362, 10727, 2942, 364, 3256, 13, 407], "temperature": 0.0, "avg_logprob": -0.1258816623687744, "compression_ratio": 1.3257575757575757, "no_speech_prob": 5.285505721985828e-07}, {"id": 591, "seek": 420928, "start": 4209.28, "end": 4228.16, "text": " that's good. Now we are done, but there are other ways to create this iterator. And because", "tokens": [300, 311, 665, 13, 823, 321, 366, 1096, 11, 457, 456, 366, 661, 2098, 281, 1884, 341, 17138, 1639, 13, 400, 570], "temperature": 0.0, "avg_logprob": -0.12118440089018448, "compression_ratio": 1.3875968992248062, "no_speech_prob": 1.2878934967375244e-06}, {"id": 592, "seek": 420928, "start": 4228.16, "end": 4235.44, "text": " iterators and generators, which are closely related, are so important, I wanted to show", "tokens": [17138, 3391, 293, 38662, 11, 597, 366, 8185, 4077, 11, 366, 370, 1021, 11, 286, 1415, 281, 855], "temperature": 0.0, "avg_logprob": -0.12118440089018448, "compression_ratio": 1.3875968992248062, "no_speech_prob": 1.2878934967375244e-06}, {"id": 593, "seek": 423544, "start": 4235.44, "end": 4242.48, "text": " you more about how to do them in Python. It's one of these things that if you understand", "tokens": [291, 544, 466, 577, 281, 360, 552, 294, 15329, 13, 467, 311, 472, 295, 613, 721, 300, 498, 291, 1223], "temperature": 0.0, "avg_logprob": -0.07518748031265435, "compression_ratio": 1.613953488372093, "no_speech_prob": 1.7778065739548765e-05}, {"id": 594, "seek": 423544, "start": 4242.48, "end": 4251.2, "text": " this, you'll often find that you can throw away huge pieces of enterprise software and", "tokens": [341, 11, 291, 603, 2049, 915, 300, 291, 393, 3507, 1314, 2603, 3755, 295, 14132, 4722, 293], "temperature": 0.0, "avg_logprob": -0.07518748031265435, "compression_ratio": 1.613953488372093, "no_speech_prob": 1.7778065739548765e-05}, {"id": 595, "seek": 423544, "start": 4251.2, "end": 4256.32, "text": " basically replace it with an iterator. It lets you stream things one bit at a time.", "tokens": [1936, 7406, 309, 365, 364, 17138, 1639, 13, 467, 6653, 291, 4309, 721, 472, 857, 412, 257, 565, 13], "temperature": 0.0, "avg_logprob": -0.07518748031265435, "compression_ratio": 1.613953488372093, "no_speech_prob": 1.7778065739548765e-05}, {"id": 596, "seek": 423544, "start": 4256.32, "end": 4262.4, "text": " It doesn't store it all in memory. It's this really powerful thing that once I, I often", "tokens": [467, 1177, 380, 3531, 309, 439, 294, 4675, 13, 467, 311, 341, 534, 4005, 551, 300, 1564, 286, 11, 286, 2049], "temperature": 0.0, "avg_logprob": -0.07518748031265435, "compression_ratio": 1.613953488372093, "no_speech_prob": 1.7778065739548765e-05}, {"id": 597, "seek": 426240, "start": 4262.4, "end": 4269.759999999999, "text": " find once I show it to people, they suddenly go like, oh, wow, we've been using all this", "tokens": [915, 1564, 286, 855, 309, 281, 561, 11, 436, 5800, 352, 411, 11, 1954, 11, 6076, 11, 321, 600, 668, 1228, 439, 341], "temperature": 0.0, "avg_logprob": -0.21234302730350704, "compression_ratio": 1.5367965367965368, "no_speech_prob": 5.862767920916667e-06}, {"id": 598, "seek": 426240, "start": 4269.759999999999, "end": 4274.639999999999, "text": " third-party software and we could have just created a Python iterator. Python comes with", "tokens": [2636, 12, 23409, 4722, 293, 321, 727, 362, 445, 2942, 257, 15329, 17138, 1639, 13, 15329, 1487, 365], "temperature": 0.0, "avg_logprob": -0.21234302730350704, "compression_ratio": 1.5367965367965368, "no_speech_prob": 5.862767920916667e-06}, {"id": 599, "seek": 426240, "start": 4274.639999999999, "end": 4280.2, "text": " a whole standard library module called iter tools just to make it easier to work with", "tokens": [257, 1379, 3832, 6405, 10088, 1219, 17138, 3873, 445, 281, 652, 309, 3571, 281, 589, 365], "temperature": 0.0, "avg_logprob": -0.21234302730350704, "compression_ratio": 1.5367965367965368, "no_speech_prob": 5.862767920916667e-06}, {"id": 600, "seek": 426240, "start": 4280.2, "end": 4287.2, "text": " iterators. I'll show you one example of something from iter tools, which is iSlic. So let's", "tokens": [17138, 3391, 13, 286, 603, 855, 291, 472, 1365, 295, 746, 490, 17138, 3873, 11, 597, 307, 741, 50, 1050, 13, 407, 718, 311], "temperature": 0.0, "avg_logprob": -0.21234302730350704, "compression_ratio": 1.5367965367965368, "no_speech_prob": 5.862767920916667e-06}, {"id": 601, "seek": 428720, "start": 4287.2, "end": 4302.96, "text": " grab our values again, these 10 values. Okay. So let's take these 10 values and we can take", "tokens": [4444, 527, 4190, 797, 11, 613, 1266, 4190, 13, 1033, 13, 407, 718, 311, 747, 613, 1266, 4190, 293, 321, 393, 747], "temperature": 0.0, "avg_logprob": -0.150932141070096, "compression_ratio": 1.3955223880597014, "no_speech_prob": 6.540391041198745e-06}, {"id": 602, "seek": 428720, "start": 4302.96, "end": 4310.639999999999, "text": " any list and turn it into an iterator by passing it to iter, which I should call it. So I don't", "tokens": [604, 1329, 293, 1261, 309, 666, 364, 17138, 1639, 538, 8437, 309, 281, 17138, 11, 597, 286, 820, 818, 309, 13, 407, 286, 500, 380], "temperature": 0.0, "avg_logprob": -0.150932141070096, "compression_ratio": 1.3955223880597014, "no_speech_prob": 6.540391041198745e-06}, {"id": 603, "seek": 431064, "start": 4310.64, "end": 4320.08, "text": " override this Python. That's not a keyword, but this thing I don't want to override. So", "tokens": [42321, 341, 15329, 13, 663, 311, 406, 257, 20428, 11, 457, 341, 551, 286, 500, 380, 528, 281, 42321, 13, 407], "temperature": 0.0, "avg_logprob": -0.16265925845584353, "compression_ratio": 1.5748502994011977, "no_speech_prob": 5.1738902584475e-06}, {"id": 604, "seek": 431064, "start": 4320.08, "end": 4323.72, "text": " this is now basically something that I can call, actually let's do this. I'll show you", "tokens": [341, 307, 586, 1936, 746, 300, 286, 393, 818, 11, 767, 718, 311, 360, 341, 13, 286, 603, 855, 291], "temperature": 0.0, "avg_logprob": -0.16265925845584353, "compression_ratio": 1.5748502994011977, "no_speech_prob": 5.1738902584475e-06}, {"id": 605, "seek": 431064, "start": 4323.72, "end": 4337.8, "text": " that I can call next on it. So if I now go next it, you can see it's giving me each item", "tokens": [300, 286, 393, 818, 958, 322, 309, 13, 407, 498, 286, 586, 352, 958, 309, 11, 291, 393, 536, 309, 311, 2902, 385, 1184, 3174], "temperature": 0.0, "avg_logprob": -0.16265925845584353, "compression_ratio": 1.5748502994011977, "no_speech_prob": 5.1738902584475e-06}, {"id": 606, "seek": 433780, "start": 4337.8, "end": 4348.72, "text": " one at a time. Okay. So that's what converting it into an iterator does. iSlic converts it", "tokens": [472, 412, 257, 565, 13, 1033, 13, 407, 300, 311, 437, 29942, 309, 666, 364, 17138, 1639, 775, 13, 741, 50, 1050, 38874, 309], "temperature": 0.0, "avg_logprob": -0.11819965498788017, "compression_ratio": 1.1111111111111112, "no_speech_prob": 1.4510376786347479e-05}, {"id": 607, "seek": 434872, "start": 4348.72, "end": 4371.12, "text": " into a different kind of iterator. Let's call this maybe iSlicer iterator. And so you can", "tokens": [666, 257, 819, 733, 295, 17138, 1639, 13, 961, 311, 818, 341, 1310, 741, 50, 1050, 260, 17138, 1639, 13, 400, 370, 291, 393], "temperature": 0.0, "avg_logprob": -0.169175318309239, "compression_ratio": 1.1125, "no_speech_prob": 1.6280491763609461e-06}, {"id": 608, "seek": 437112, "start": 4371.12, "end": 4382.12, "text": " see here what it did was it jumped stop. Here we are. So, ah, yes, that's what had been", "tokens": [536, 510, 437, 309, 630, 390, 309, 13864, 1590, 13, 1692, 321, 366, 13, 407, 11, 3716, 11, 2086, 11, 300, 311, 437, 632, 668], "temperature": 0.0, "avg_logprob": -0.20249558766682943, "compression_ratio": 1.4972375690607735, "no_speech_prob": 1.7603398418941651e-06}, {"id": 609, "seek": 437112, "start": 4382.12, "end": 4386.5599999999995, "text": " better. So I should query create the iterator and then call next a few times. Sorry. This", "tokens": [1101, 13, 407, 286, 820, 14581, 1884, 264, 17138, 1639, 293, 550, 818, 958, 257, 1326, 1413, 13, 4919, 13, 639], "temperature": 0.0, "avg_logprob": -0.20249558766682943, "compression_ratio": 1.4972375690607735, "no_speech_prob": 1.7603398418941651e-06}, {"id": 610, "seek": 437112, "start": 4386.5599999999995, "end": 4396.28, "text": " is what I meant to do. It's now only returning the first five before it calls stop iteration,", "tokens": [307, 437, 286, 4140, 281, 360, 13, 467, 311, 586, 787, 12678, 264, 700, 1732, 949, 309, 5498, 1590, 24784, 11], "temperature": 0.0, "avg_logprob": -0.20249558766682943, "compression_ratio": 1.4972375690607735, "no_speech_prob": 1.7603398418941651e-06}, {"id": 611, "seek": 439628, "start": 4396.28, "end": 4403.96, "text": " before it raises stop iteration. So what iSlicer does is it grabs the first n things from an", "tokens": [949, 309, 19658, 1590, 24784, 13, 407, 437, 741, 50, 1050, 260, 775, 307, 309, 30028, 264, 700, 297, 721, 490, 364], "temperature": 0.0, "avg_logprob": -0.11381748360647283, "compression_ratio": 1.5348837209302326, "no_speech_prob": 1.7603398418941651e-06}, {"id": 612, "seek": 439628, "start": 4403.96, "end": 4415.88, "text": " iterable, something that you can iterate. Why is that interesting? Because I can pass", "tokens": [17138, 712, 11, 746, 300, 291, 393, 44497, 13, 1545, 307, 300, 1880, 30, 1436, 286, 393, 1320], "temperature": 0.0, "avg_logprob": -0.11381748360647283, "compression_ratio": 1.5348837209302326, "no_speech_prob": 1.7603398418941651e-06}, {"id": 613, "seek": 439628, "start": 4415.88, "end": 4424.36, "text": " it to list, for example. Right. And now if I pass it to list again, this iterator has", "tokens": [309, 281, 1329, 11, 337, 1365, 13, 1779, 13, 400, 586, 498, 286, 1320, 309, 281, 1329, 797, 11, 341, 17138, 1639, 575], "temperature": 0.0, "avg_logprob": -0.11381748360647283, "compression_ratio": 1.5348837209302326, "no_speech_prob": 1.7603398418941651e-06}, {"id": 614, "seek": 442436, "start": 4424.36, "end": 4429.32, "text": " now grabbed the first five things. So it's now up to thing number six. So if I call it", "tokens": [586, 18607, 264, 700, 1732, 721, 13, 407, 309, 311, 586, 493, 281, 551, 1230, 2309, 13, 407, 498, 286, 818, 309], "temperature": 0.0, "avg_logprob": -0.10858789243196186, "compression_ratio": 1.63125, "no_speech_prob": 3.3405249268980697e-06}, {"id": 615, "seek": 442436, "start": 4429.32, "end": 4439.599999999999, "text": " again, it's the next five things. And if I call it again, then there's nothing left.", "tokens": [797, 11, 309, 311, 264, 958, 1732, 721, 13, 400, 498, 286, 818, 309, 797, 11, 550, 456, 311, 1825, 1411, 13], "temperature": 0.0, "avg_logprob": -0.10858789243196186, "compression_ratio": 1.63125, "no_speech_prob": 3.3405249268980697e-06}, {"id": 616, "seek": 442436, "start": 4439.599999999999, "end": 4447.16, "text": " And maybe you can see we've actually now got this defined, but we can do it with iSlicer.", "tokens": [400, 1310, 291, 393, 536, 321, 600, 767, 586, 658, 341, 7642, 11, 457, 321, 393, 360, 309, 365, 741, 50, 1050, 260, 13], "temperature": 0.0, "avg_logprob": -0.10858789243196186, "compression_ratio": 1.63125, "no_speech_prob": 3.3405249268980697e-06}, {"id": 617, "seek": 444716, "start": 4447.16, "end": 4456.28, "text": " And here's how we can do it. It's actually pretty tricky. Iter in Python, you can pass", "tokens": [400, 510, 311, 577, 321, 393, 360, 309, 13, 467, 311, 767, 1238, 12414, 13, 286, 391, 294, 15329, 11, 291, 393, 1320], "temperature": 0.0, "avg_logprob": -0.11503313779830933, "compression_ratio": 1.553072625698324, "no_speech_prob": 6.854294497316005e-06}, {"id": 618, "seek": 444716, "start": 4456.28, "end": 4461.76, "text": " it something like a list to create an iterator, or you can pass it, now this is a really important", "tokens": [309, 746, 411, 257, 1329, 281, 1884, 364, 17138, 1639, 11, 420, 291, 393, 1320, 309, 11, 586, 341, 307, 257, 534, 1021], "temperature": 0.0, "avg_logprob": -0.11503313779830933, "compression_ratio": 1.553072625698324, "no_speech_prob": 6.854294497316005e-06}, {"id": 619, "seek": 444716, "start": 4461.76, "end": 4468.84, "text": " word, a callable. What's a callable? A callable is generally speaking, it's a function. It's", "tokens": [1349, 11, 257, 818, 712, 13, 708, 311, 257, 818, 712, 30, 316, 818, 712, 307, 5101, 4124, 11, 309, 311, 257, 2445, 13, 467, 311], "temperature": 0.0, "avg_logprob": -0.11503313779830933, "compression_ratio": 1.553072625698324, "no_speech_prob": 6.854294497316005e-06}, {"id": 620, "seek": 446884, "start": 4468.84, "end": 4477.92, "text": " something that you can put parentheses after. Could even be a class, anything you can put", "tokens": [746, 300, 291, 393, 829, 34153, 934, 13, 7497, 754, 312, 257, 1508, 11, 1340, 291, 393, 829], "temperature": 0.0, "avg_logprob": -0.09756486312202785, "compression_ratio": 1.7524271844660195, "no_speech_prob": 5.014721864426974e-06}, {"id": 621, "seek": 446884, "start": 4477.92, "end": 4481.72, "text": " parentheses after. You can just think of it for now as a function. So we're going to pass", "tokens": [34153, 934, 13, 509, 393, 445, 519, 295, 309, 337, 586, 382, 257, 2445, 13, 407, 321, 434, 516, 281, 1320], "temperature": 0.0, "avg_logprob": -0.09756486312202785, "compression_ratio": 1.7524271844660195, "no_speech_prob": 5.014721864426974e-06}, {"id": 622, "seek": 446884, "start": 4481.72, "end": 4490.72, "text": " it a function. And in the second form, it's going to be called until the function returns", "tokens": [309, 257, 2445, 13, 400, 294, 264, 1150, 1254, 11, 309, 311, 516, 281, 312, 1219, 1826, 264, 2445, 11247], "temperature": 0.0, "avg_logprob": -0.09756486312202785, "compression_ratio": 1.7524271844660195, "no_speech_prob": 5.014721864426974e-06}, {"id": 623, "seek": 446884, "start": 4490.72, "end": 4495.56, "text": " this value here, in this case is empty list. And we just saw that iSlicer will return empty", "tokens": [341, 2158, 510, 11, 294, 341, 1389, 307, 6707, 1329, 13, 400, 321, 445, 1866, 300, 741, 50, 1050, 260, 486, 2736, 6707], "temperature": 0.0, "avg_logprob": -0.09756486312202785, "compression_ratio": 1.7524271844660195, "no_speech_prob": 5.014721864426974e-06}, {"id": 624, "seek": 449556, "start": 4495.56, "end": 4506.88, "text": " list when it's done. So this here is going to keep calling this function again and again", "tokens": [1329, 562, 309, 311, 1096, 13, 407, 341, 510, 307, 516, 281, 1066, 5141, 341, 2445, 797, 293, 797], "temperature": 0.0, "avg_logprob": -0.09608403496120287, "compression_ratio": 1.502857142857143, "no_speech_prob": 3.3405287922505522e-06}, {"id": 625, "seek": 449556, "start": 4506.88, "end": 4512.6, "text": " and again. And we've seen exactly what happens because we've called it ourselves before.", "tokens": [293, 797, 13, 400, 321, 600, 1612, 2293, 437, 2314, 570, 321, 600, 1219, 309, 4175, 949, 13], "temperature": 0.0, "avg_logprob": -0.09608403496120287, "compression_ratio": 1.502857142857143, "no_speech_prob": 3.3405287922505522e-06}, {"id": 626, "seek": 449556, "start": 4512.6, "end": 4522.72, "text": " There it is. Until it gets an empty list. So if we do it with 28, then we're going to", "tokens": [821, 309, 307, 13, 9088, 309, 2170, 364, 6707, 1329, 13, 407, 498, 321, 360, 309, 365, 7562, 11, 550, 321, 434, 516, 281], "temperature": 0.0, "avg_logprob": -0.09608403496120287, "compression_ratio": 1.502857142857143, "no_speech_prob": 3.3405287922505522e-06}, {"id": 627, "seek": 452272, "start": 4522.72, "end": 4533.360000000001, "text": " get our image again. So we've now got two different ways of creating exactly the same", "tokens": [483, 527, 3256, 797, 13, 407, 321, 600, 586, 658, 732, 819, 2098, 295, 4084, 2293, 264, 912], "temperature": 0.0, "avg_logprob": -0.09675084627591647, "compression_ratio": 1.6355140186915889, "no_speech_prob": 4.785075361724012e-06}, {"id": 628, "seek": 452272, "start": 4533.360000000001, "end": 4542.26, "text": " thing. And if you've never used iterators before, now's a good time to pause the video", "tokens": [551, 13, 400, 498, 291, 600, 1128, 1143, 17138, 3391, 949, 11, 586, 311, 257, 665, 565, 281, 10465, 264, 960], "temperature": 0.0, "avg_logprob": -0.09675084627591647, "compression_ratio": 1.6355140186915889, "no_speech_prob": 4.785075361724012e-06}, {"id": 629, "seek": 452272, "start": 4542.26, "end": 4547.68, "text": " and play with them, right? So for example, you could take this here, right, and if you've", "tokens": [293, 862, 365, 552, 11, 558, 30, 407, 337, 1365, 11, 291, 727, 747, 341, 510, 11, 558, 11, 293, 498, 291, 600], "temperature": 0.0, "avg_logprob": -0.09675084627591647, "compression_ratio": 1.6355140186915889, "no_speech_prob": 4.785075361724012e-06}, {"id": 630, "seek": 452272, "start": 4547.68, "end": 4552.0, "text": " not seen lambdas before, they're exactly the same as functions, but you can define them", "tokens": [406, 1612, 10097, 27476, 949, 11, 436, 434, 2293, 264, 912, 382, 6828, 11, 457, 291, 393, 6964, 552], "temperature": 0.0, "avg_logprob": -0.09675084627591647, "compression_ratio": 1.6355140186915889, "no_speech_prob": 4.785075361724012e-06}, {"id": 631, "seek": 455200, "start": 4552.0, "end": 4561.72, "text": " inline. So let's replace that with a function. Okay, so now I've turned it into a function", "tokens": [294, 1889, 13, 407, 718, 311, 7406, 300, 365, 257, 2445, 13, 1033, 11, 370, 586, 286, 600, 3574, 309, 666, 257, 2445], "temperature": 0.0, "avg_logprob": -0.1694220762986403, "compression_ratio": 1.3953488372093024, "no_speech_prob": 6.0488896451715846e-06}, {"id": 632, "seek": 455200, "start": 4561.72, "end": 4576.04, "text": " and then you can experiment with it. So let's create our iterator and call f on it. Well,", "tokens": [293, 550, 291, 393, 5120, 365, 309, 13, 407, 718, 311, 1884, 527, 17138, 1639, 293, 818, 283, 322, 309, 13, 1042, 11], "temperature": 0.0, "avg_logprob": -0.1694220762986403, "compression_ratio": 1.3953488372093024, "no_speech_prob": 6.0488896451715846e-06}, {"id": 633, "seek": 457604, "start": 4576.04, "end": 4584.6, "text": " not on it, call f. And you can see there's the first 28. And each time I do it, I'm getting", "tokens": [406, 322, 309, 11, 818, 283, 13, 400, 291, 393, 536, 456, 311, 264, 700, 7562, 13, 400, 1184, 565, 286, 360, 309, 11, 286, 478, 1242], "temperature": 0.0, "avg_logprob": -0.1267854559655283, "compression_ratio": 1.6529680365296804, "no_speech_prob": 1.7330482933175517e-06}, {"id": 634, "seek": 457604, "start": 4584.6, "end": 4588.32, "text": " another 28. Now the first two rows are all empty. But finally, look, now I've got some", "tokens": [1071, 7562, 13, 823, 264, 700, 732, 13241, 366, 439, 6707, 13, 583, 2721, 11, 574, 11, 586, 286, 600, 658, 512], "temperature": 0.0, "avg_logprob": -0.1267854559655283, "compression_ratio": 1.6529680365296804, "no_speech_prob": 1.7330482933175517e-06}, {"id": 635, "seek": 457604, "start": 4588.32, "end": 4593.32, "text": " values. Call it again. See how each time I'm getting something else. Just calling it again", "tokens": [4190, 13, 7807, 309, 797, 13, 3008, 577, 1184, 565, 286, 478, 1242, 746, 1646, 13, 1449, 5141, 309, 797], "temperature": 0.0, "avg_logprob": -0.1267854559655283, "compression_ratio": 1.6529680365296804, "no_speech_prob": 1.7330482933175517e-06}, {"id": 636, "seek": 457604, "start": 4593.32, "end": 4600.24, "text": " and again. And that is the values in our iterator. So that gives you a sense of like how you", "tokens": [293, 797, 13, 400, 300, 307, 264, 4190, 294, 527, 17138, 1639, 13, 407, 300, 2709, 291, 257, 2020, 295, 411, 577, 291], "temperature": 0.0, "avg_logprob": -0.1267854559655283, "compression_ratio": 1.6529680365296804, "no_speech_prob": 1.7330482933175517e-06}, {"id": 637, "seek": 460024, "start": 4600.24, "end": 4608.679999999999, "text": " can use Jupyter to experiment. So what you should do is as soon as you hit something", "tokens": [393, 764, 22125, 88, 391, 281, 5120, 13, 407, 437, 291, 820, 360, 307, 382, 2321, 382, 291, 2045, 746], "temperature": 0.0, "avg_logprob": -0.09709118713032115, "compression_ratio": 1.614678899082569, "no_speech_prob": 7.07185654391651e-06}, {"id": 638, "seek": 460024, "start": 4608.679999999999, "end": 4617.16, "text": " in my code that doesn't look familiar to you, I recommend pausing the video and experimenting", "tokens": [294, 452, 3089, 300, 1177, 380, 574, 4963, 281, 291, 11, 286, 2748, 2502, 7981, 264, 960, 293, 29070], "temperature": 0.0, "avg_logprob": -0.09709118713032115, "compression_ratio": 1.614678899082569, "no_speech_prob": 7.07185654391651e-06}, {"id": 639, "seek": 460024, "start": 4617.16, "end": 4625.24, "text": " with that in Jupyter. And for example, iter, most people probably have not used iter at", "tokens": [365, 300, 294, 22125, 88, 391, 13, 400, 337, 1365, 11, 17138, 11, 881, 561, 1391, 362, 406, 1143, 17138, 412], "temperature": 0.0, "avg_logprob": -0.09709118713032115, "compression_ratio": 1.614678899082569, "no_speech_prob": 7.07185654391651e-06}, {"id": 640, "seek": 460024, "start": 4625.24, "end": 4629.48, "text": " all and certainly very few people have used this to argument form. So hit shift tab a", "tokens": [439, 293, 3297, 588, 1326, 561, 362, 1143, 341, 281, 6770, 1254, 13, 407, 2045, 5513, 4421, 257], "temperature": 0.0, "avg_logprob": -0.09709118713032115, "compression_ratio": 1.614678899082569, "no_speech_prob": 7.07185654391651e-06}, {"id": 641, "seek": 462948, "start": 4629.48, "end": 4637.48, "text": " few times. And now you've got at the bottom, there's a description of what it is. Or find", "tokens": [1326, 1413, 13, 400, 586, 291, 600, 658, 412, 264, 2767, 11, 456, 311, 257, 3855, 295, 437, 309, 307, 13, 1610, 915], "temperature": 0.0, "avg_logprob": -0.21995589418231315, "compression_ratio": 1.3037037037037038, "no_speech_prob": 3.2191976060857996e-05}, {"id": 642, "seek": 462948, "start": 4637.48, "end": 4649.16, "text": " that more. Python iter. Here we are. Go to the docs. Well, that's not the right bit of", "tokens": [300, 544, 13, 15329, 17138, 13, 1692, 321, 366, 13, 1037, 281, 264, 45623, 13, 1042, 11, 300, 311, 406, 264, 558, 857, 295], "temperature": 0.0, "avg_logprob": -0.21995589418231315, "compression_ratio": 1.3037037037037038, "no_speech_prob": 3.2191976060857996e-05}, {"id": 643, "seek": 464916, "start": 4649.16, "end": 4671.68, "text": " the docs. See API. Wow, crazy. That's terrible. Let's try searching here. There we go. That's", "tokens": [264, 45623, 13, 3008, 9362, 13, 3153, 11, 3219, 13, 663, 311, 6237, 13, 961, 311, 853, 10808, 510, 13, 821, 321, 352, 13, 663, 311], "temperature": 0.0, "avg_logprob": -0.18271945204053605, "compression_ratio": 1.2992700729927007, "no_speech_prob": 4.029444880870869e-06}, {"id": 644, "seek": 464916, "start": 4671.68, "end": 4675.48, "text": " more like it. So now you've got links. So if it's like, okay, it returns an iterator", "tokens": [544, 411, 309, 13, 407, 586, 291, 600, 658, 6123, 13, 407, 498, 309, 311, 411, 11, 1392, 11, 309, 11247, 364, 17138, 1639], "temperature": 0.0, "avg_logprob": -0.18271945204053605, "compression_ratio": 1.2992700729927007, "no_speech_prob": 4.029444880870869e-06}, {"id": 645, "seek": 467548, "start": 4675.48, "end": 4680.12, "text": " object. What's that? Well, click on it. Find out. This is really important to know. And", "tokens": [2657, 13, 708, 311, 300, 30, 1042, 11, 2052, 322, 309, 13, 11809, 484, 13, 639, 307, 534, 1021, 281, 458, 13, 400], "temperature": 0.0, "avg_logprob": -0.1431469645926623, "compression_ratio": 1.75390625, "no_speech_prob": 4.222821644361829e-06}, {"id": 646, "seek": 467548, "start": 4680.12, "end": 4686.24, "text": " here's that stop exception that we saw. Sorry, stop iteration exception. We saw next already.", "tokens": [510, 311, 300, 1590, 11183, 300, 321, 1866, 13, 4919, 11, 1590, 24784, 11183, 13, 492, 1866, 958, 1217, 13], "temperature": 0.0, "avg_logprob": -0.1431469645926623, "compression_ratio": 1.75390625, "no_speech_prob": 4.222821644361829e-06}, {"id": 647, "seek": 467548, "start": 4686.24, "end": 4693.679999999999, "text": " We can find out what iterable is. And here's an example. And as you can see, it's using", "tokens": [492, 393, 915, 484, 437, 17138, 712, 307, 13, 400, 510, 311, 364, 1365, 13, 400, 382, 291, 393, 536, 11, 309, 311, 1228], "temperature": 0.0, "avg_logprob": -0.1431469645926623, "compression_ratio": 1.75390625, "no_speech_prob": 4.222821644361829e-06}, {"id": 648, "seek": 467548, "start": 4693.679999999999, "end": 4698.959999999999, "text": " exactly the same approach that we did, but here it's being used to read from a file.", "tokens": [2293, 264, 912, 3109, 300, 321, 630, 11, 457, 510, 309, 311, 885, 1143, 281, 1401, 490, 257, 3991, 13], "temperature": 0.0, "avg_logprob": -0.1431469645926623, "compression_ratio": 1.75390625, "no_speech_prob": 4.222821644361829e-06}, {"id": 649, "seek": 467548, "start": 4698.959999999999, "end": 4705.4, "text": " This is really cool. Here's how to read from a file. 64 bytes at a time until you get nothing.", "tokens": [639, 307, 534, 1627, 13, 1692, 311, 577, 281, 1401, 490, 257, 3991, 13, 12145, 36088, 412, 257, 565, 1826, 291, 483, 1825, 13], "temperature": 0.0, "avg_logprob": -0.1431469645926623, "compression_ratio": 1.75390625, "no_speech_prob": 4.222821644361829e-06}, {"id": 650, "seek": 470540, "start": 4705.4, "end": 4715.08, "text": " Processing it. Right. So the docs of Python are quite fantastic. As long as you use them.", "tokens": [31093, 278, 309, 13, 1779, 13, 407, 264, 45623, 295, 15329, 366, 1596, 5456, 13, 1018, 938, 382, 291, 764, 552, 13], "temperature": 0.0, "avg_logprob": -0.17149491628011068, "compression_ratio": 1.4385026737967914, "no_speech_prob": 9.516204954707064e-06}, {"id": 651, "seek": 470540, "start": 4715.08, "end": 4725.44, "text": " If you don't use them, they're not very useful at all. And I see C for in the comments, our", "tokens": [759, 291, 500, 380, 764, 552, 11, 436, 434, 406, 588, 4420, 412, 439, 13, 400, 286, 536, 383, 337, 294, 264, 3053, 11, 527], "temperature": 0.0, "avg_logprob": -0.17149491628011068, "compression_ratio": 1.4385026737967914, "no_speech_prob": 9.516204954707064e-06}, {"id": 652, "seek": 470540, "start": 4725.44, "end": 4732.48, "text": " local Haskell programmer appreciating this Haskellness in Python. That's good. It's not", "tokens": [2654, 8646, 43723, 32116, 3616, 990, 341, 8646, 43723, 1287, 294, 15329, 13, 663, 311, 665, 13, 467, 311, 406], "temperature": 0.0, "avg_logprob": -0.17149491628011068, "compression_ratio": 1.4385026737967914, "no_speech_prob": 9.516204954707064e-06}, {"id": 653, "seek": 473248, "start": 4732.48, "end": 4740.959999999999, "text": " quite Haskell, I'm afraid, but it's the closest we're going to come. All right. How are we", "tokens": [1596, 8646, 43723, 11, 286, 478, 4638, 11, 457, 309, 311, 264, 13699, 321, 434, 516, 281, 808, 13, 1057, 558, 13, 1012, 366, 321], "temperature": 0.0, "avg_logprob": -0.18929652469914135, "compression_ratio": 1.4126984126984128, "no_speech_prob": 5.594311915047001e-06}, {"id": 654, "seek": 473248, "start": 4740.959999999999, "end": 4753.5599999999995, "text": " going for time? Pretty good. Okay. So now that we've got image, which is a list of lists,", "tokens": [516, 337, 565, 30, 10693, 665, 13, 1033, 13, 407, 586, 300, 321, 600, 658, 3256, 11, 597, 307, 257, 1329, 295, 14511, 11], "temperature": 0.0, "avg_logprob": -0.18929652469914135, "compression_ratio": 1.4126984126984128, "no_speech_prob": 5.594311915047001e-06}, {"id": 655, "seek": 473248, "start": 4753.5599999999995, "end": 4759.679999999999, "text": " and each list is 25 long, we can index into it. So we can say image 20. Well, let's do", "tokens": [293, 1184, 1329, 307, 3552, 938, 11, 321, 393, 8186, 666, 309, 13, 407, 321, 393, 584, 3256, 945, 13, 1042, 11, 718, 311, 360], "temperature": 0.0, "avg_logprob": -0.18929652469914135, "compression_ratio": 1.4126984126984128, "no_speech_prob": 5.594311915047001e-06}, {"id": 656, "seek": 475968, "start": 4759.68, "end": 4776.8, "text": " it. Image 20. Okay. Here's a list of 28 numbers. And then we could index into that. Okay. So", "tokens": [309, 13, 29903, 945, 13, 1033, 13, 1692, 311, 257, 1329, 295, 7562, 3547, 13, 400, 550, 321, 727, 8186, 666, 300, 13, 1033, 13, 407], "temperature": 0.0, "avg_logprob": -0.10308848487006293, "compression_ratio": 1.3503649635036497, "no_speech_prob": 5.043472128818394e-07}, {"id": 657, "seek": 475968, "start": 4776.8, "end": 4783.72, "text": " we can index into it. Now, normally we don't like to do that for matrices. We would normally", "tokens": [321, 393, 8186, 666, 309, 13, 823, 11, 5646, 321, 500, 380, 411, 281, 360, 300, 337, 32284, 13, 492, 576, 5646], "temperature": 0.0, "avg_logprob": -0.10308848487006293, "compression_ratio": 1.3503649635036497, "no_speech_prob": 5.043472128818394e-07}, {"id": 658, "seek": 478372, "start": 4783.72, "end": 4791.280000000001, "text": " rather write it like this. Okay. So that means we're going to have to create our own class", "tokens": [2831, 2464, 309, 411, 341, 13, 1033, 13, 407, 300, 1355, 321, 434, 516, 281, 362, 281, 1884, 527, 1065, 1508], "temperature": 0.0, "avg_logprob": -0.08815039584511204, "compression_ratio": 1.9005235602094241, "no_speech_prob": 4.710884695668938e-06}, {"id": 659, "seek": 478372, "start": 4791.280000000001, "end": 4799.52, "text": " to make that work. So to create a class in Python, you write class. And then you write", "tokens": [281, 652, 300, 589, 13, 407, 281, 1884, 257, 1508, 294, 15329, 11, 291, 2464, 1508, 13, 400, 550, 291, 2464], "temperature": 0.0, "avg_logprob": -0.08815039584511204, "compression_ratio": 1.9005235602094241, "no_speech_prob": 4.710884695668938e-06}, {"id": 660, "seek": 478372, "start": 4799.52, "end": 4805.92, "text": " the name of it. And then you write some really weird things. The weird things you write have", "tokens": [264, 1315, 295, 309, 13, 400, 550, 291, 2464, 512, 534, 3657, 721, 13, 440, 3657, 721, 291, 2464, 362], "temperature": 0.0, "avg_logprob": -0.08815039584511204, "compression_ratio": 1.9005235602094241, "no_speech_prob": 4.710884695668938e-06}, {"id": 661, "seek": 478372, "start": 4805.92, "end": 4813.2, "text": " two underscores, a special word, and then two underscores. These things with two underscores", "tokens": [732, 16692, 66, 2706, 11, 257, 2121, 1349, 11, 293, 550, 732, 16692, 66, 2706, 13, 1981, 721, 365, 732, 16692, 66, 2706], "temperature": 0.0, "avg_logprob": -0.08815039584511204, "compression_ratio": 1.9005235602094241, "no_speech_prob": 4.710884695668938e-06}, {"id": 662, "seek": 481320, "start": 4813.2, "end": 4819.88, "text": " on each side are called Dunder methods. And they're all the special magically named methods", "tokens": [322, 1184, 1252, 366, 1219, 413, 6617, 7150, 13, 400, 436, 434, 439, 264, 2121, 39763, 4926, 7150], "temperature": 0.0, "avg_logprob": -0.21154440366304839, "compression_ratio": 1.4915254237288136, "no_speech_prob": 4.785081728186924e-06}, {"id": 663, "seek": 481320, "start": 4819.88, "end": 4825.2, "text": " which have particular meanings to Python. And you just got to learn them, but they're", "tokens": [597, 362, 1729, 28138, 281, 15329, 13, 400, 291, 445, 658, 281, 1466, 552, 11, 457, 436, 434], "temperature": 0.0, "avg_logprob": -0.21154440366304839, "compression_ratio": 1.4915254237288136, "no_speech_prob": 4.785081728186924e-06}, {"id": 664, "seek": 481320, "start": 4825.2, "end": 4839.76, "text": " all documented in the Python object model. In it. Object model. Yay, finally. Okay. So", "tokens": [439, 23007, 294, 264, 15329, 2657, 2316, 13, 682, 309, 13, 24753, 2316, 13, 13268, 11, 2721, 13, 1033, 13, 407], "temperature": 0.0, "avg_logprob": -0.21154440366304839, "compression_ratio": 1.4915254237288136, "no_speech_prob": 4.785081728186924e-06}, {"id": 665, "seek": 483976, "start": 4839.76, "end": 4845.8, "text": " what do you eventually find? Oh, it's called data model, not object model. And so this", "tokens": [437, 360, 291, 4728, 915, 30, 876, 11, 309, 311, 1219, 1412, 2316, 11, 406, 2657, 2316, 13, 400, 370, 341], "temperature": 0.0, "avg_logprob": -0.14387381076812744, "compression_ratio": 1.6363636363636365, "no_speech_prob": 9.818291800911538e-06}, {"id": 666, "seek": 483976, "start": 4845.8, "end": 4849.320000000001, "text": " is basically where all the documentation is about absolutely everything. And I can click", "tokens": [307, 1936, 689, 439, 264, 14333, 307, 466, 3122, 1203, 13, 400, 286, 393, 2052], "temperature": 0.0, "avg_logprob": -0.14387381076812744, "compression_ratio": 1.6363636363636365, "no_speech_prob": 9.818291800911538e-06}, {"id": 667, "seek": 483976, "start": 4849.320000000001, "end": 4856.0, "text": " Dunder in it. And it tells you basically this is the thing that constructs objects. So anytime", "tokens": [413, 6617, 294, 309, 13, 400, 309, 5112, 291, 1936, 341, 307, 264, 551, 300, 7690, 82, 6565, 13, 407, 13038], "temperature": 0.0, "avg_logprob": -0.14387381076812744, "compression_ratio": 1.6363636363636365, "no_speech_prob": 9.818291800911538e-06}, {"id": 668, "seek": 483976, "start": 4856.0, "end": 4863.04, "text": " you want to create a class that you want to construct it, it's going to store some stuff.", "tokens": [291, 528, 281, 1884, 257, 1508, 300, 291, 528, 281, 7690, 309, 11, 309, 311, 516, 281, 3531, 512, 1507, 13], "temperature": 0.0, "avg_logprob": -0.14387381076812744, "compression_ratio": 1.6363636363636365, "no_speech_prob": 9.818291800911538e-06}, {"id": 669, "seek": 486304, "start": 4863.04, "end": 4870.28, "text": " So in this case, it's going to store our image. You have to define Dunder in it. Python's", "tokens": [407, 294, 341, 1389, 11, 309, 311, 516, 281, 3531, 527, 3256, 13, 509, 362, 281, 6964, 413, 6617, 294, 309, 13, 15329, 311], "temperature": 0.0, "avg_logprob": -0.11284021003959105, "compression_ratio": 1.7028301886792452, "no_speech_prob": 1.130075179389678e-05}, {"id": 670, "seek": 486304, "start": 4870.28, "end": 4877.92, "text": " slightly weird in that every method you have to put self here for reasons we probably don't", "tokens": [4748, 3657, 294, 300, 633, 3170, 291, 362, 281, 829, 2698, 510, 337, 4112, 321, 1391, 500, 380], "temperature": 0.0, "avg_logprob": -0.11284021003959105, "compression_ratio": 1.7028301886792452, "no_speech_prob": 1.130075179389678e-05}, {"id": 671, "seek": 486304, "start": 4877.92, "end": 4882.76, "text": " really need to get into right now. And then any parameters. So we're going to be creating", "tokens": [534, 643, 281, 483, 666, 558, 586, 13, 400, 550, 604, 9834, 13, 407, 321, 434, 516, 281, 312, 4084], "temperature": 0.0, "avg_logprob": -0.11284021003959105, "compression_ratio": 1.7028301886792452, "no_speech_prob": 1.130075179389678e-05}, {"id": 672, "seek": 486304, "start": 4882.76, "end": 4889.14, "text": " an image passing in the thing to store, the x's. So we're going to be passing in the x's.", "tokens": [364, 3256, 8437, 294, 264, 551, 281, 3531, 11, 264, 2031, 311, 13, 407, 321, 434, 516, 281, 312, 8437, 294, 264, 2031, 311, 13], "temperature": 0.0, "avg_logprob": -0.11284021003959105, "compression_ratio": 1.7028301886792452, "no_speech_prob": 1.130075179389678e-05}, {"id": 673, "seek": 488914, "start": 4889.14, "end": 4894.8, "text": " And so here we're just going to store it inside the self. So once I've got this line of code,", "tokens": [400, 370, 510, 321, 434, 445, 516, 281, 3531, 309, 1854, 264, 2698, 13, 407, 1564, 286, 600, 658, 341, 1622, 295, 3089, 11], "temperature": 0.0, "avg_logprob": -0.10442954666760502, "compression_ratio": 1.5560344827586208, "no_speech_prob": 3.340531293360982e-06}, {"id": 674, "seek": 488914, "start": 4894.8, "end": 4901.400000000001, "text": " I've now got something that knows how to store stuff, the x's inside itself. So now I want", "tokens": [286, 600, 586, 658, 746, 300, 3255, 577, 281, 3531, 1507, 11, 264, 2031, 311, 1854, 2564, 13, 407, 586, 286, 528], "temperature": 0.0, "avg_logprob": -0.10442954666760502, "compression_ratio": 1.5560344827586208, "no_speech_prob": 3.340531293360982e-06}, {"id": 675, "seek": 488914, "start": 4901.400000000001, "end": 4911.56, "text": " to be able to call square bracket 20 comma 15. So how do we do that? Well, basically", "tokens": [281, 312, 1075, 281, 818, 3732, 16904, 945, 22117, 2119, 13, 407, 577, 360, 321, 360, 300, 30, 1042, 11, 1936], "temperature": 0.0, "avg_logprob": -0.10442954666760502, "compression_ratio": 1.5560344827586208, "no_speech_prob": 3.340531293360982e-06}, {"id": 676, "seek": 488914, "start": 4911.56, "end": 4917.04, "text": " part of the data model is that there's a special thing called Dunder get item. And when you", "tokens": [644, 295, 264, 1412, 2316, 307, 300, 456, 311, 257, 2121, 551, 1219, 413, 6617, 483, 3174, 13, 400, 562, 291], "temperature": 0.0, "avg_logprob": -0.10442954666760502, "compression_ratio": 1.5560344827586208, "no_speech_prob": 3.340531293360982e-06}, {"id": 677, "seek": 491704, "start": 4917.04, "end": 4923.28, "text": " call square brackets on your object, that's what Python uses. And it's going to pass across", "tokens": [818, 3732, 26179, 322, 428, 2657, 11, 300, 311, 437, 15329, 4960, 13, 400, 309, 311, 516, 281, 1320, 2108], "temperature": 0.0, "avg_logprob": -0.09611798735225902, "compression_ratio": 1.434065934065934, "no_speech_prob": 1.9947253804275533e-06}, {"id": 678, "seek": 491704, "start": 4923.28, "end": 4932.92, "text": " the 20 comma 15 here as indices. So we're now basically just going to return this. So", "tokens": [264, 945, 22117, 2119, 510, 382, 43840, 13, 407, 321, 434, 586, 1936, 445, 516, 281, 2736, 341, 13, 407], "temperature": 0.0, "avg_logprob": -0.09611798735225902, "compression_ratio": 1.434065934065934, "no_speech_prob": 1.9947253804275533e-06}, {"id": 679, "seek": 491704, "start": 4932.92, "end": 4941.44, "text": " the self.x's with the first index and the second index. So let's create that matrix", "tokens": [264, 2698, 13, 87, 311, 365, 264, 700, 8186, 293, 264, 1150, 8186, 13, 407, 718, 311, 1884, 300, 8141], "temperature": 0.0, "avg_logprob": -0.09611798735225902, "compression_ratio": 1.434065934065934, "no_speech_prob": 1.9947253804275533e-06}, {"id": 680, "seek": 494144, "start": 4941.44, "end": 4950.0, "text": " class and run that. And you can now see m20 comma 15 is the same. Oh, quick note on, you", "tokens": [1508, 293, 1190, 300, 13, 400, 291, 393, 586, 536, 275, 2009, 22117, 2119, 307, 264, 912, 13, 876, 11, 1702, 3637, 322, 11, 291], "temperature": 0.0, "avg_logprob": -0.14845000372992623, "compression_ratio": 1.440217391304348, "no_speech_prob": 4.289322077966062e-06}, {"id": 681, "seek": 494144, "start": 4950.0, "end": 4955.919999999999, "text": " know, ways in which my code is different to everybody else's, which it is. It's somewhat", "tokens": [458, 11, 2098, 294, 597, 452, 3089, 307, 819, 281, 2201, 1646, 311, 11, 597, 309, 307, 13, 467, 311, 8344], "temperature": 0.0, "avg_logprob": -0.14845000372992623, "compression_ratio": 1.440217391304348, "no_speech_prob": 4.289322077966062e-06}, {"id": 682, "seek": 494144, "start": 4955.919999999999, "end": 4966.719999999999, "text": " unusual to put definitions of methods on the same line as the signature like this. I do", "tokens": [10901, 281, 829, 21988, 295, 7150, 322, 264, 912, 1622, 382, 264, 13397, 411, 341, 13, 286, 360], "temperature": 0.0, "avg_logprob": -0.14845000372992623, "compression_ratio": 1.440217391304348, "no_speech_prob": 4.289322077966062e-06}, {"id": 683, "seek": 496672, "start": 4966.72, "end": 4972.56, "text": " it quite a lot for one-liners. As I kind of mentioned before, I find it really helps me", "tokens": [309, 1596, 257, 688, 337, 472, 12, 5045, 433, 13, 1018, 286, 733, 295, 2835, 949, 11, 286, 915, 309, 534, 3665, 385], "temperature": 0.0, "avg_logprob": -0.05785524217705978, "compression_ratio": 1.6494464944649447, "no_speech_prob": 4.222784809826408e-06}, {"id": 684, "seek": 496672, "start": 4972.56, "end": 4978.72, "text": " to be able to see all the code I'm working with on the screen at once. A lot of the world's", "tokens": [281, 312, 1075, 281, 536, 439, 264, 3089, 286, 478, 1364, 365, 322, 264, 2568, 412, 1564, 13, 316, 688, 295, 264, 1002, 311], "temperature": 0.0, "avg_logprob": -0.05785524217705978, "compression_ratio": 1.6494464944649447, "no_speech_prob": 4.222784809826408e-06}, {"id": 685, "seek": 496672, "start": 4978.72, "end": 4982.84, "text": " best programmers actually have had that approach as well. It seems to work quite well for some", "tokens": [1151, 41504, 767, 362, 632, 300, 3109, 382, 731, 13, 467, 2544, 281, 589, 1596, 731, 337, 512], "temperature": 0.0, "avg_logprob": -0.05785524217705978, "compression_ratio": 1.6494464944649447, "no_speech_prob": 4.222784809826408e-06}, {"id": 686, "seek": 496672, "start": 4982.84, "end": 4988.04, "text": " people that are extremely productive. It's not common in Python. Some people are quite", "tokens": [561, 300, 366, 4664, 13304, 13, 467, 311, 406, 2689, 294, 15329, 13, 2188, 561, 366, 1596], "temperature": 0.0, "avg_logprob": -0.05785524217705978, "compression_ratio": 1.6494464944649447, "no_speech_prob": 4.222784809826408e-06}, {"id": 687, "seek": 496672, "start": 4988.04, "end": 4995.320000000001, "text": " against it. So if you're at work and your colleagues don't write Python this way, you", "tokens": [1970, 309, 13, 407, 498, 291, 434, 412, 589, 293, 428, 7734, 500, 380, 2464, 15329, 341, 636, 11, 291], "temperature": 0.0, "avg_logprob": -0.05785524217705978, "compression_ratio": 1.6494464944649447, "no_speech_prob": 4.222784809826408e-06}, {"id": 688, "seek": 499532, "start": 4995.32, "end": 5000.4, "text": " probably shouldn't either. But if you can get away with it, I think it works quite well.", "tokens": [1391, 4659, 380, 2139, 13, 583, 498, 291, 393, 483, 1314, 365, 309, 11, 286, 519, 309, 1985, 1596, 731, 13], "temperature": 0.0, "avg_logprob": -0.17287271499633788, "compression_ratio": 1.6227272727272728, "no_speech_prob": 2.0462670363485813e-05}, {"id": 689, "seek": 499532, "start": 5000.4, "end": 5005.24, "text": " Anywho, OK, so now that we've created something that lets us index into things like this,", "tokens": [2639, 13506, 11, 2264, 11, 370, 586, 300, 321, 600, 2942, 746, 300, 6653, 505, 8186, 666, 721, 411, 341, 11], "temperature": 0.0, "avg_logprob": -0.17287271499633788, "compression_ratio": 1.6227272727272728, "no_speech_prob": 2.0462670363485813e-05}, {"id": 690, "seek": 499532, "start": 5005.24, "end": 5011.32, "text": " we're allowed to use PyTorch because we're allowed to use this one feature in PyTorch.", "tokens": [321, 434, 4350, 281, 764, 9953, 51, 284, 339, 570, 321, 434, 4350, 281, 764, 341, 472, 4111, 294, 9953, 51, 284, 339, 13], "temperature": 0.0, "avg_logprob": -0.17287271499633788, "compression_ratio": 1.6227272727272728, "no_speech_prob": 2.0462670363485813e-05}, {"id": 691, "seek": 499532, "start": 5011.32, "end": 5023.0, "text": " OK, so we can now do that. And so now to create a tensor, which is basically a lot like our", "tokens": [2264, 11, 370, 321, 393, 586, 360, 300, 13, 400, 370, 586, 281, 1884, 257, 40863, 11, 597, 307, 1936, 257, 688, 411, 527], "temperature": 0.0, "avg_logprob": -0.17287271499633788, "compression_ratio": 1.6227272727272728, "no_speech_prob": 2.0462670363485813e-05}, {"id": 692, "seek": 502300, "start": 5023.0, "end": 5032.92, "text": " matrix, we can now pass a list into tensor to get back a tensor version of that list.", "tokens": [8141, 11, 321, 393, 586, 1320, 257, 1329, 666, 40863, 281, 483, 646, 257, 40863, 3037, 295, 300, 1329, 13], "temperature": 0.0, "avg_logprob": -0.07415689362419976, "compression_ratio": 1.3870967741935485, "no_speech_prob": 8.013369551918004e-06}, {"id": 693, "seek": 502300, "start": 5032.92, "end": 5044.16, "text": " Or perhaps more interestingly, we could pass in a list of lists. Maybe let's give this", "tokens": [1610, 4317, 544, 25873, 11, 321, 727, 1320, 294, 257, 1329, 295, 14511, 13, 2704, 718, 311, 976, 341], "temperature": 0.0, "avg_logprob": -0.07415689362419976, "compression_ratio": 1.3870967741935485, "no_speech_prob": 8.013369551918004e-06}, {"id": 694, "seek": 504416, "start": 5044.16, "end": 5056.12, "text": " a name. Whoopsie-dozy. That needs to be a list of lists, just like we had before for", "tokens": [257, 1315, 13, 45263, 414, 12, 2595, 1229, 13, 663, 2203, 281, 312, 257, 1329, 295, 14511, 11, 445, 411, 321, 632, 949, 337], "temperature": 0.0, "avg_logprob": -0.11881902388163976, "compression_ratio": 1.403225806451613, "no_speech_prob": 4.785033070220379e-06}, {"id": 695, "seek": 504416, "start": 5056.12, "end": 5067.7, "text": " our image. In fact, let's do it for our image. Let's just pass in our image. There we go.", "tokens": [527, 3256, 13, 682, 1186, 11, 718, 311, 360, 309, 337, 527, 3256, 13, 961, 311, 445, 1320, 294, 527, 3256, 13, 821, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.11881902388163976, "compression_ratio": 1.403225806451613, "no_speech_prob": 4.785033070220379e-06}, {"id": 696, "seek": 506770, "start": 5067.7, "end": 5077.04, "text": " And so now we should be able to say tense 20 comma 15. And there we go. OK, so we've", "tokens": [400, 370, 586, 321, 820, 312, 1075, 281, 584, 18760, 945, 22117, 2119, 13, 400, 456, 321, 352, 13, 2264, 11, 370, 321, 600], "temperature": 0.0, "avg_logprob": -0.14462847510973612, "compression_ratio": 1.2611940298507462, "no_speech_prob": 9.132521654464654e-07}, {"id": 697, "seek": 506770, "start": 5077.04, "end": 5092.679999999999, "text": " successfully reinvented that. All right. So now we can convert all of our lists into", "tokens": [10727, 33477, 292, 300, 13, 1057, 558, 13, 407, 586, 321, 393, 7620, 439, 295, 527, 14511, 666], "temperature": 0.0, "avg_logprob": -0.14462847510973612, "compression_ratio": 1.2611940298507462, "no_speech_prob": 9.132521654464654e-07}, {"id": 698, "seek": 509268, "start": 5092.68, "end": 5100.4800000000005, "text": " tensors. There's a convenient way to do this, which is to use the map function in the Python", "tokens": [10688, 830, 13, 821, 311, 257, 10851, 636, 281, 360, 341, 11, 597, 307, 281, 764, 264, 4471, 2445, 294, 264, 15329], "temperature": 0.0, "avg_logprob": -0.14509623845418293, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.393131692064344e-06}, {"id": 699, "seek": 509268, "start": 5100.4800000000005, "end": 5109.64, "text": " standard library. So shift, shift tab. Map takes a function and then some iterables,", "tokens": [3832, 6405, 13, 407, 5513, 11, 5513, 4421, 13, 22053, 2516, 257, 2445, 293, 550, 512, 17138, 2965, 11], "temperature": 0.0, "avg_logprob": -0.14509623845418293, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.393131692064344e-06}, {"id": 700, "seek": 509268, "start": 5109.64, "end": 5116.280000000001, "text": " in this case one iterable, and it's going to apply this function to each of these four", "tokens": [294, 341, 1389, 472, 17138, 712, 11, 293, 309, 311, 516, 281, 3079, 341, 2445, 281, 1184, 295, 613, 1451], "temperature": 0.0, "avg_logprob": -0.14509623845418293, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.393131692064344e-06}, {"id": 701, "seek": 509268, "start": 5116.280000000001, "end": 5121.88, "text": " things and return those four things. And so then I can put four things on the left to", "tokens": [721, 293, 2736, 729, 1451, 721, 13, 400, 370, 550, 286, 393, 829, 1451, 721, 322, 264, 1411, 281], "temperature": 0.0, "avg_logprob": -0.14509623845418293, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.393131692064344e-06}, {"id": 702, "seek": 512188, "start": 5121.88, "end": 5127.4800000000005, "text": " receive those four things. So this is going to call tensor X train and put it in X train.", "tokens": [4774, 729, 1451, 721, 13, 407, 341, 307, 516, 281, 818, 40863, 1783, 3847, 293, 829, 309, 294, 1783, 3847, 13], "temperature": 0.0, "avg_logprob": -0.16422650248733991, "compression_ratio": 1.7115384615384615, "no_speech_prob": 4.78507445222931e-06}, {"id": 703, "seek": 512188, "start": 5127.4800000000005, "end": 5131.78, "text": " Tensor Y train, put it in Y train and so forth. So this is converting all of these lists to", "tokens": [34306, 398, 3847, 11, 829, 309, 294, 398, 3847, 293, 370, 5220, 13, 407, 341, 307, 29942, 439, 295, 613, 14511, 281], "temperature": 0.0, "avg_logprob": -0.16422650248733991, "compression_ratio": 1.7115384615384615, "no_speech_prob": 4.78507445222931e-06}, {"id": 704, "seek": 512188, "start": 5131.78, "end": 5141.32, "text": " tensors and storing them back in the same name. So you can see that X train now is a", "tokens": [10688, 830, 293, 26085, 552, 646, 294, 264, 912, 1315, 13, 407, 291, 393, 536, 300, 1783, 3847, 586, 307, 257], "temperature": 0.0, "avg_logprob": -0.16422650248733991, "compression_ratio": 1.7115384615384615, "no_speech_prob": 4.78507445222931e-06}, {"id": 705, "seek": 512188, "start": 5141.32, "end": 5147.68, "text": " tensor. So that means it has a shape property. It has 50,000 images in it, which are each", "tokens": [40863, 13, 407, 300, 1355, 309, 575, 257, 3909, 4707, 13, 467, 575, 2625, 11, 1360, 5267, 294, 309, 11, 597, 366, 1184], "temperature": 0.0, "avg_logprob": -0.16422650248733991, "compression_ratio": 1.7115384615384615, "no_speech_prob": 4.78507445222931e-06}, {"id": 706, "seek": 514768, "start": 5147.68, "end": 5155.84, "text": " 784 long. And you can find out what kind of, what kind of stuff it contains by calling", "tokens": [1614, 25494, 938, 13, 400, 291, 393, 915, 484, 437, 733, 295, 11, 437, 733, 295, 1507, 309, 8306, 538, 5141], "temperature": 0.0, "avg_logprob": -0.11965140555668803, "compression_ratio": 1.6807511737089202, "no_speech_prob": 6.540410140587483e-06}, {"id": 707, "seek": 514768, "start": 5155.84, "end": 5161.92, "text": " it dot type. So it contains floats. So this is the tensor class. We'll be using a lot", "tokens": [309, 5893, 2010, 13, 407, 309, 8306, 37878, 13, 407, 341, 307, 264, 40863, 1508, 13, 492, 603, 312, 1228, 257, 688], "temperature": 0.0, "avg_logprob": -0.11965140555668803, "compression_ratio": 1.6807511737089202, "no_speech_prob": 6.540410140587483e-06}, {"id": 708, "seek": 514768, "start": 5161.92, "end": 5170.5, "text": " of it. So of course you should read its documentation. I don't love the PyTorch documentation. Some", "tokens": [295, 309, 13, 407, 295, 1164, 291, 820, 1401, 1080, 14333, 13, 286, 500, 380, 959, 264, 9953, 51, 284, 339, 14333, 13, 2188], "temperature": 0.0, "avg_logprob": -0.11965140555668803, "compression_ratio": 1.6807511737089202, "no_speech_prob": 6.540410140587483e-06}, {"id": 709, "seek": 514768, "start": 5170.5, "end": 5176.6, "text": " of it's good. Some of it's not good. It's a bit all over the place. So here's tensor,", "tokens": [295, 309, 311, 665, 13, 2188, 295, 309, 311, 406, 665, 13, 467, 311, 257, 857, 439, 670, 264, 1081, 13, 407, 510, 311, 40863, 11], "temperature": 0.0, "avg_logprob": -0.11965140555668803, "compression_ratio": 1.6807511737089202, "no_speech_prob": 6.540410140587483e-06}, {"id": 710, "seek": 517660, "start": 5176.6, "end": 5179.8, "text": " but it's well worth scrolling through to get a sense of like, this is actually not bad,", "tokens": [457, 309, 311, 731, 3163, 29053, 807, 281, 483, 257, 2020, 295, 411, 11, 341, 307, 767, 406, 1578, 11], "temperature": 0.0, "avg_logprob": -0.13960517166007277, "compression_ratio": 1.7058823529411764, "no_speech_prob": 3.169298724969849e-05}, {"id": 711, "seek": 517660, "start": 5179.8, "end": 5183.280000000001, "text": " right? It tells you how you can construct it. This is how I constructed one before,", "tokens": [558, 30, 467, 5112, 291, 577, 291, 393, 7690, 309, 13, 639, 307, 577, 286, 17083, 472, 949, 11], "temperature": 0.0, "avg_logprob": -0.13960517166007277, "compression_ratio": 1.7058823529411764, "no_speech_prob": 3.169298724969849e-05}, {"id": 712, "seek": 517660, "start": 5183.280000000001, "end": 5196.360000000001, "text": " passing it lists of lists. You can also pass it NumPy arrays. You can change types, so", "tokens": [8437, 309, 14511, 295, 14511, 13, 509, 393, 611, 1320, 309, 22592, 47, 88, 41011, 13, 509, 393, 1319, 3467, 11, 370], "temperature": 0.0, "avg_logprob": -0.13960517166007277, "compression_ratio": 1.7058823529411764, "no_speech_prob": 3.169298724969849e-05}, {"id": 713, "seek": 517660, "start": 5196.360000000001, "end": 5200.52, "text": " on and so forth. So, you know, it's well worth reading through and like, you're not going", "tokens": [322, 293, 370, 5220, 13, 407, 11, 291, 458, 11, 309, 311, 731, 3163, 3760, 807, 293, 411, 11, 291, 434, 406, 516], "temperature": 0.0, "avg_logprob": -0.13960517166007277, "compression_ratio": 1.7058823529411764, "no_speech_prob": 3.169298724969849e-05}, {"id": 714, "seek": 517660, "start": 5200.52, "end": 5204.400000000001, "text": " to look at every single method it takes, but you're kind of, if you browse through it,", "tokens": [281, 574, 412, 633, 2167, 3170, 309, 2516, 11, 457, 291, 434, 733, 295, 11, 498, 291, 31442, 807, 309, 11], "temperature": 0.0, "avg_logprob": -0.13960517166007277, "compression_ratio": 1.7058823529411764, "no_speech_prob": 3.169298724969849e-05}, {"id": 715, "seek": 520440, "start": 5204.4, "end": 5210.24, "text": " you'll get a general sense, right? That tensors do just about everything you couldn't think", "tokens": [291, 603, 483, 257, 2674, 2020, 11, 558, 30, 663, 10688, 830, 360, 445, 466, 1203, 291, 2809, 380, 519], "temperature": 0.0, "avg_logprob": -0.11583511395887895, "compression_ratio": 1.5521739130434782, "no_speech_prob": 4.637823167286115e-06}, {"id": 716, "seek": 520440, "start": 5210.24, "end": 5217.719999999999, "text": " of for numeric programming. At some point, you will want to know every single one of", "tokens": [295, 337, 7866, 299, 9410, 13, 1711, 512, 935, 11, 291, 486, 528, 281, 458, 633, 2167, 472, 295], "temperature": 0.0, "avg_logprob": -0.11583511395887895, "compression_ratio": 1.5521739130434782, "no_speech_prob": 4.637823167286115e-06}, {"id": 717, "seek": 520440, "start": 5217.719999999999, "end": 5224.24, "text": " these, or at least be aware roughly what exists so you know what to search for in the docs.", "tokens": [613, 11, 420, 412, 1935, 312, 3650, 9810, 437, 8198, 370, 291, 458, 437, 281, 3164, 337, 294, 264, 45623, 13], "temperature": 0.0, "avg_logprob": -0.11583511395887895, "compression_ratio": 1.5521739130434782, "no_speech_prob": 4.637823167286115e-06}, {"id": 718, "seek": 520440, "start": 5224.24, "end": 5230.08, "text": " Otherwise you will end up recreating stuff from scratch, which is much, much slower than", "tokens": [10328, 291, 486, 917, 493, 850, 44613, 1507, 490, 8459, 11, 597, 307, 709, 11, 709, 14009, 813], "temperature": 0.0, "avg_logprob": -0.11583511395887895, "compression_ratio": 1.5521739130434782, "no_speech_prob": 4.637823167286115e-06}, {"id": 719, "seek": 523008, "start": 5230.08, "end": 5237.64, "text": " simply reading the documentation to find out it's there. All right. So instead of calling", "tokens": [2935, 3760, 264, 14333, 281, 915, 484, 309, 311, 456, 13, 1057, 558, 13, 407, 2602, 295, 5141], "temperature": 0.0, "avg_logprob": -0.2053958002726237, "compression_ratio": 1.4673913043478262, "no_speech_prob": 4.2228134589095134e-06}, {"id": 720, "seek": 523008, "start": 5237.64, "end": 5246.12, "text": " chunks or iSlices, the thing that is roughly equivalent in a tensor is the reshape method.", "tokens": [24004, 420, 741, 50, 1050, 279, 11, 264, 551, 300, 307, 9810, 10344, 294, 257, 40863, 307, 264, 725, 42406, 3170, 13], "temperature": 0.0, "avg_logprob": -0.2053958002726237, "compression_ratio": 1.4673913043478262, "no_speech_prob": 4.2228134589095134e-06}, {"id": 721, "seek": 523008, "start": 5246.12, "end": 5252.96, "text": " So reshape, so to reshape our 50,000 by 784 thing, we can simply, we want to turn it into", "tokens": [407, 725, 42406, 11, 370, 281, 725, 42406, 527, 2625, 11, 1360, 538, 1614, 25494, 551, 11, 321, 393, 2935, 11, 321, 528, 281, 1261, 309, 666], "temperature": 0.0, "avg_logprob": -0.2053958002726237, "compression_ratio": 1.4673913043478262, "no_speech_prob": 4.2228134589095134e-06}, {"id": 722, "seek": 525296, "start": 5252.96, "end": 5266.96, "text": " 50,028 by 28 tensors. So I could write here reshape to 50,000 by 28 by 28, but I kind", "tokens": [2625, 11, 15, 11205, 538, 7562, 10688, 830, 13, 407, 286, 727, 2464, 510, 725, 42406, 281, 2625, 11, 1360, 538, 7562, 538, 7562, 11, 457, 286, 733], "temperature": 0.0, "avg_logprob": -0.10373962981791436, "compression_ratio": 1.5802469135802468, "no_speech_prob": 1.8448214404997998e-06}, {"id": 723, "seek": 525296, "start": 5266.96, "end": 5274.2, "text": " of don't need to because I could just put minus one here and it can figure out that", "tokens": [295, 500, 380, 643, 281, 570, 286, 727, 445, 829, 3175, 472, 510, 293, 309, 393, 2573, 484, 300], "temperature": 0.0, "avg_logprob": -0.10373962981791436, "compression_ratio": 1.5802469135802468, "no_speech_prob": 1.8448214404997998e-06}, {"id": 724, "seek": 525296, "start": 5274.2, "end": 5281.64, "text": " that must be 50,000 because it knows that I have 50,000 by 784 items. So it can figure", "tokens": [300, 1633, 312, 2625, 11, 1360, 570, 309, 3255, 300, 286, 362, 2625, 11, 1360, 538, 1614, 25494, 4754, 13, 407, 309, 393, 2573], "temperature": 0.0, "avg_logprob": -0.10373962981791436, "compression_ratio": 1.5802469135802468, "no_speech_prob": 1.8448214404997998e-06}, {"id": 725, "seek": 528164, "start": 5281.64, "end": 5295.240000000001, "text": " out, so minus one means just fill this with all the rest. Okay. Now what does the word", "tokens": [484, 11, 370, 3175, 472, 1355, 445, 2836, 341, 365, 439, 264, 1472, 13, 1033, 13, 823, 437, 775, 264, 1349], "temperature": 0.0, "avg_logprob": -0.1391425737192933, "compression_ratio": 1.4748603351955307, "no_speech_prob": 3.726622026078985e-06}, {"id": 726, "seek": 528164, "start": 5295.240000000001, "end": 5304.72, "text": " tensor mean? So there's some very interesting history here, and I'll try not to get too", "tokens": [40863, 914, 30, 407, 456, 311, 512, 588, 1880, 2503, 510, 11, 293, 286, 603, 853, 406, 281, 483, 886], "temperature": 0.0, "avg_logprob": -0.1391425737192933, "compression_ratio": 1.4748603351955307, "no_speech_prob": 3.726622026078985e-06}, {"id": 727, "seek": 528164, "start": 5304.72, "end": 5309.4400000000005, "text": " far into it because I'm a bit over enthusiastic about this stuff, I must admit. I'm very,", "tokens": [1400, 666, 309, 570, 286, 478, 257, 857, 670, 28574, 466, 341, 1507, 11, 286, 1633, 9796, 13, 286, 478, 588, 11], "temperature": 0.0, "avg_logprob": -0.1391425737192933, "compression_ratio": 1.4748603351955307, "no_speech_prob": 3.726622026078985e-06}, {"id": 728, "seek": 530944, "start": 5309.44, "end": 5315.08, "text": " very interested in the history of tensor programming and array programming, and it basically goes", "tokens": [588, 3102, 294, 264, 2503, 295, 40863, 9410, 293, 10225, 9410, 11, 293, 309, 1936, 1709], "temperature": 0.0, "avg_logprob": -0.1069866138346055, "compression_ratio": 1.6184971098265897, "no_speech_prob": 4.637860911316238e-06}, {"id": 729, "seek": 530944, "start": 5315.08, "end": 5325.139999999999, "text": " back to a language called APL. APL is a, basically originally a mathematical notation that was", "tokens": [646, 281, 257, 2856, 1219, 5372, 43, 13, 5372, 43, 307, 257, 11, 1936, 7993, 257, 18894, 24657, 300, 390], "temperature": 0.0, "avg_logprob": -0.1069866138346055, "compression_ratio": 1.6184971098265897, "no_speech_prob": 4.637860911316238e-06}, {"id": 730, "seek": 530944, "start": 5325.139999999999, "end": 5333.639999999999, "text": " developed in the mid to late 50s, 1950s, and at first it was used to, as a notation for", "tokens": [4743, 294, 264, 2062, 281, 3469, 2625, 82, 11, 18141, 82, 11, 293, 412, 700, 309, 390, 1143, 281, 11, 382, 257, 24657, 337], "temperature": 0.0, "avg_logprob": -0.1069866138346055, "compression_ratio": 1.6184971098265897, "no_speech_prob": 4.637860911316238e-06}, {"id": 731, "seek": 533364, "start": 5333.64, "end": 5342.4400000000005, "text": " defining how certain new IBM systems would work. So it was all written out in this notation.", "tokens": [17827, 577, 1629, 777, 23487, 3652, 576, 589, 13, 407, 309, 390, 439, 3720, 484, 294, 341, 24657, 13], "temperature": 0.0, "avg_logprob": -0.11509325320904072, "compression_ratio": 1.46524064171123, "no_speech_prob": 7.296328021766385e-06}, {"id": 732, "seek": 533364, "start": 5342.4400000000005, "end": 5350.4800000000005, "text": " It's kind of like a replacement for mathematical notation that was designed to be more consistent", "tokens": [467, 311, 733, 295, 411, 257, 14419, 337, 18894, 24657, 300, 390, 4761, 281, 312, 544, 8398], "temperature": 0.0, "avg_logprob": -0.11509325320904072, "compression_ratio": 1.46524064171123, "no_speech_prob": 7.296328021766385e-06}, {"id": 733, "seek": 533364, "start": 5350.4800000000005, "end": 5357.4800000000005, "text": " and kind of more expressive. In the early 60s, so the guy who wrote and made it was", "tokens": [293, 733, 295, 544, 40189, 13, 682, 264, 2440, 4060, 82, 11, 370, 264, 2146, 567, 4114, 293, 1027, 309, 390], "temperature": 0.0, "avg_logprob": -0.11509325320904072, "compression_ratio": 1.46524064171123, "no_speech_prob": 7.296328021766385e-06}, {"id": 734, "seek": 535748, "start": 5357.48, "end": 5364.16, "text": " called Ken Iverson. In the early 60s, some implementations that actually allowed this", "tokens": [1219, 8273, 286, 840, 266, 13, 682, 264, 2440, 4060, 82, 11, 512, 4445, 763, 300, 767, 4350, 341], "temperature": 0.0, "avg_logprob": -0.14113302791819854, "compression_ratio": 1.5844155844155845, "no_speech_prob": 4.936936420563143e-06}, {"id": 735, "seek": 535748, "start": 5364.16, "end": 5371.28, "text": " notation to be executed on a computer appeared. Both the notation and the executable implementations,", "tokens": [24657, 281, 312, 17577, 322, 257, 3820, 8516, 13, 6767, 264, 24657, 293, 264, 7568, 712, 4445, 763, 11], "temperature": 0.0, "avg_logprob": -0.14113302791819854, "compression_ratio": 1.5844155844155845, "no_speech_prob": 4.936936420563143e-06}, {"id": 736, "seek": 535748, "start": 5371.28, "end": 5377.0, "text": " slightly confusingly, are both called APL. APL has been in constant development ever", "tokens": [4748, 13181, 356, 11, 366, 1293, 1219, 5372, 43, 13, 5372, 43, 575, 668, 294, 5754, 3250, 1562], "temperature": 0.0, "avg_logprob": -0.14113302791819854, "compression_ratio": 1.5844155844155845, "no_speech_prob": 4.936936420563143e-06}, {"id": 737, "seek": 535748, "start": 5377.0, "end": 5382.24, "text": " since that time, and today is one of the world's most powerful programming languages, and you", "tokens": [1670, 300, 565, 11, 293, 965, 307, 472, 295, 264, 1002, 311, 881, 4005, 9410, 8650, 11, 293, 291], "temperature": 0.0, "avg_logprob": -0.14113302791819854, "compression_ratio": 1.5844155844155845, "no_speech_prob": 4.936936420563143e-06}, {"id": 738, "seek": 538224, "start": 5382.24, "end": 5390.4, "text": " can try it by going to try APL. And why am I mentioning it here? Because one of the things", "tokens": [393, 853, 309, 538, 516, 281, 853, 5372, 43, 13, 400, 983, 669, 286, 18315, 309, 510, 30, 1436, 472, 295, 264, 721], "temperature": 0.0, "avg_logprob": -0.11958112620344066, "compression_ratio": 1.5478260869565217, "no_speech_prob": 2.295902277182904e-06}, {"id": 739, "seek": 538224, "start": 5390.4, "end": 5397.599999999999, "text": " Ken Iverson did, well, he studied an area of physics called tensor analysis, and as", "tokens": [8273, 286, 840, 266, 630, 11, 731, 11, 415, 9454, 364, 1859, 295, 10649, 1219, 40863, 5215, 11, 293, 382], "temperature": 0.0, "avg_logprob": -0.11958112620344066, "compression_ratio": 1.5478260869565217, "no_speech_prob": 2.295902277182904e-06}, {"id": 740, "seek": 538224, "start": 5397.599999999999, "end": 5403.66, "text": " he developed APL, he basically said, like, oh, what if we took these ideas from tensor", "tokens": [415, 4743, 5372, 43, 11, 415, 1936, 848, 11, 411, 11, 1954, 11, 437, 498, 321, 1890, 613, 3487, 490, 40863], "temperature": 0.0, "avg_logprob": -0.11958112620344066, "compression_ratio": 1.5478260869565217, "no_speech_prob": 2.295902277182904e-06}, {"id": 741, "seek": 538224, "start": 5403.66, "end": 5412.08, "text": " analysis and put them into a programming language? So in, yeah, in APL, you can, and you know,", "tokens": [5215, 293, 829, 552, 666, 257, 9410, 2856, 30, 407, 294, 11, 1338, 11, 294, 5372, 43, 11, 291, 393, 11, 293, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.11958112620344066, "compression_ratio": 1.5478260869565217, "no_speech_prob": 2.295902277182904e-06}, {"id": 742, "seek": 541208, "start": 5412.08, "end": 5419.84, "text": " have been able to for some time, can basically, you can define a variable, and rather than", "tokens": [362, 668, 1075, 281, 337, 512, 565, 11, 393, 1936, 11, 291, 393, 6964, 257, 7006, 11, 293, 2831, 813], "temperature": 0.0, "avg_logprob": -0.13747622750022195, "compression_ratio": 1.6729857819905214, "no_speech_prob": 1.2805370715796016e-05}, {"id": 743, "seek": 541208, "start": 5419.84, "end": 5426.08, "text": " saying equals, which is a terrible way to define things really mathematically, because", "tokens": [1566, 6915, 11, 597, 307, 257, 6237, 636, 281, 6964, 721, 534, 44003, 11, 570], "temperature": 0.0, "avg_logprob": -0.13747622750022195, "compression_ratio": 1.6729857819905214, "no_speech_prob": 1.2805370715796016e-05}, {"id": 744, "seek": 541208, "start": 5426.08, "end": 5430.04, "text": " that has a very different meaning most of the time in math, instead we use arrow to", "tokens": [300, 575, 257, 588, 819, 3620, 881, 295, 264, 565, 294, 5221, 11, 2602, 321, 764, 11610, 281], "temperature": 0.0, "avg_logprob": -0.13747622750022195, "compression_ratio": 1.6729857819905214, "no_speech_prob": 1.2805370715796016e-05}, {"id": 745, "seek": 541208, "start": 5430.04, "end": 5439.76, "text": " define things, we can say, okay, that's going to be a tensor, like so. And then we can look", "tokens": [6964, 721, 11, 321, 393, 584, 11, 1392, 11, 300, 311, 516, 281, 312, 257, 40863, 11, 411, 370, 13, 400, 550, 321, 393, 574], "temperature": 0.0, "avg_logprob": -0.13747622750022195, "compression_ratio": 1.6729857819905214, "no_speech_prob": 1.2805370715796016e-05}, {"id": 746, "seek": 543976, "start": 5439.76, "end": 5448.92, "text": " at their contents of A, and we can do things like, oh, what if we do A times three, or", "tokens": [412, 641, 15768, 295, 316, 11, 293, 321, 393, 360, 721, 411, 11, 1954, 11, 437, 498, 321, 360, 316, 1413, 1045, 11, 420], "temperature": 0.0, "avg_logprob": -0.10855130513509115, "compression_ratio": 1.63125, "no_speech_prob": 4.6378604565688875e-06}, {"id": 747, "seek": 543976, "start": 5448.92, "end": 5459.24, "text": " A minus two, and so forth. And as you can see, what it's doing is it's taking all the", "tokens": [316, 3175, 732, 11, 293, 370, 5220, 13, 400, 382, 291, 393, 536, 11, 437, 309, 311, 884, 307, 309, 311, 1940, 439, 264], "temperature": 0.0, "avg_logprob": -0.10855130513509115, "compression_ratio": 1.63125, "no_speech_prob": 4.6378604565688875e-06}, {"id": 748, "seek": 543976, "start": 5459.24, "end": 5464.84, "text": " contents of this tensor, and it's multiplying them all by three, or subtracting two from", "tokens": [15768, 295, 341, 40863, 11, 293, 309, 311, 30955, 552, 439, 538, 1045, 11, 420, 16390, 278, 732, 490], "temperature": 0.0, "avg_logprob": -0.10855130513509115, "compression_ratio": 1.63125, "no_speech_prob": 4.6378604565688875e-06}, {"id": 749, "seek": 546484, "start": 5464.84, "end": 5474.96, "text": " all of them. Or perhaps more fun, we could put into B a different tensor, and we can", "tokens": [439, 295, 552, 13, 1610, 4317, 544, 1019, 11, 321, 727, 829, 666, 363, 257, 819, 40863, 11, 293, 321, 393], "temperature": 0.0, "avg_logprob": -0.09399105111757915, "compression_ratio": 1.328125, "no_speech_prob": 1.1726406228262931e-06}, {"id": 750, "seek": 546484, "start": 5474.96, "end": 5484.24, "text": " now do things like A divided by B, and you can see it's taking each of A and dividing", "tokens": [586, 360, 721, 411, 316, 6666, 538, 363, 11, 293, 291, 393, 536, 309, 311, 1940, 1184, 295, 316, 293, 26764], "temperature": 0.0, "avg_logprob": -0.09399105111757915, "compression_ratio": 1.328125, "no_speech_prob": 1.1726406228262931e-06}, {"id": 751, "seek": 548424, "start": 5484.24, "end": 5497.48, "text": " by each of B. Now, this is very interesting, because now we don't have to write loops anymore.", "tokens": [538, 1184, 295, 363, 13, 823, 11, 341, 307, 588, 1880, 11, 570, 586, 321, 500, 380, 362, 281, 2464, 16121, 3602, 13], "temperature": 0.0, "avg_logprob": -0.1626285918771404, "compression_ratio": 1.469945355191257, "no_speech_prob": 3.1875551940174773e-06}, {"id": 752, "seek": 548424, "start": 5497.48, "end": 5503.2, "text": " We can just express things directly. We can multiply things by scalars, even if they're,", "tokens": [492, 393, 445, 5109, 721, 3838, 13, 492, 393, 12972, 721, 538, 15664, 685, 11, 754, 498, 436, 434, 11], "temperature": 0.0, "avg_logprob": -0.1626285918771404, "compression_ratio": 1.469945355191257, "no_speech_prob": 3.1875551940174773e-06}, {"id": 753, "seek": 548424, "start": 5503.2, "end": 5509.32, "text": " this is called a rank one tensor, that is to say it's basically, in math we'd call it", "tokens": [341, 307, 1219, 257, 6181, 472, 40863, 11, 300, 307, 281, 584, 309, 311, 1936, 11, 294, 5221, 321, 1116, 818, 309], "temperature": 0.0, "avg_logprob": -0.1626285918771404, "compression_ratio": 1.469945355191257, "no_speech_prob": 3.1875551940174773e-06}, {"id": 754, "seek": 550932, "start": 5509.32, "end": 5514.2, "text": " a rank one tensor. We can take two vectors and can divide one by the other, and so forth.", "tokens": [257, 6181, 472, 40863, 13, 492, 393, 747, 732, 18875, 293, 393, 9845, 472, 538, 264, 661, 11, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.122787079008499, "compression_ratio": 1.5570175438596492, "no_speech_prob": 8.013411388674285e-06}, {"id": 755, "seek": 550932, "start": 5514.2, "end": 5521.12, "text": " It's a really powerful idea. Funnily enough, APL didn't call them tensors, even though", "tokens": [467, 311, 257, 534, 4005, 1558, 13, 11166, 77, 953, 1547, 11, 5372, 43, 994, 380, 818, 552, 10688, 830, 11, 754, 1673], "temperature": 0.0, "avg_logprob": -0.122787079008499, "compression_ratio": 1.5570175438596492, "no_speech_prob": 8.013411388674285e-06}, {"id": 756, "seek": 550932, "start": 5521.12, "end": 5530.799999999999, "text": " Ken Iverson said he got this idea from tensor analysis. APL calls them arrays. NumPy, which", "tokens": [8273, 286, 840, 266, 848, 415, 658, 341, 1558, 490, 40863, 5215, 13, 5372, 43, 5498, 552, 41011, 13, 22592, 47, 88, 11, 597], "temperature": 0.0, "avg_logprob": -0.122787079008499, "compression_ratio": 1.5570175438596492, "no_speech_prob": 8.013411388674285e-06}, {"id": 757, "seek": 550932, "start": 5530.799999999999, "end": 5538.5199999999995, "text": " was heavily influenced by APL, also calls them arrays. For some reason, PyTorch, which", "tokens": [390, 10950, 15269, 538, 5372, 43, 11, 611, 5498, 552, 41011, 13, 1171, 512, 1778, 11, 9953, 51, 284, 339, 11, 597], "temperature": 0.0, "avg_logprob": -0.122787079008499, "compression_ratio": 1.5570175438596492, "no_speech_prob": 8.013411388674285e-06}, {"id": 758, "seek": 553852, "start": 5538.52, "end": 5544.320000000001, "text": " is very heavily influenced by APL, sorry, by NumPy, doesn't call them arrays, it calls", "tokens": [307, 588, 10950, 15269, 538, 5372, 43, 11, 2597, 11, 538, 22592, 47, 88, 11, 1177, 380, 818, 552, 41011, 11, 309, 5498], "temperature": 0.0, "avg_logprob": -0.1214713520473904, "compression_ratio": 1.562874251497006, "no_speech_prob": 3.7853139929211466e-06}, {"id": 759, "seek": 553852, "start": 5544.320000000001, "end": 5557.4800000000005, "text": " them tensors. They're all the same thing. They are rectangular blocks of numbers. They", "tokens": [552, 10688, 830, 13, 814, 434, 439, 264, 912, 551, 13, 814, 366, 31167, 8474, 295, 3547, 13, 814], "temperature": 0.0, "avg_logprob": -0.1214713520473904, "compression_ratio": 1.562874251497006, "no_speech_prob": 3.7853139929211466e-06}, {"id": 760, "seek": 553852, "start": 5557.4800000000005, "end": 5562.72, "text": " can be one-dimensional, like a vector. They can be two-dimensional, like a matrix. They", "tokens": [393, 312, 472, 12, 18759, 11, 411, 257, 8062, 13, 814, 393, 312, 732, 12, 18759, 11, 411, 257, 8141, 13, 814], "temperature": 0.0, "avg_logprob": -0.1214713520473904, "compression_ratio": 1.562874251497006, "no_speech_prob": 3.7853139929211466e-06}, {"id": 761, "seek": 556272, "start": 5562.72, "end": 5568.8, "text": " can be three-dimensional, which is like a bunch of stacked matrices, like a batch of", "tokens": [393, 312, 1045, 12, 18759, 11, 597, 307, 411, 257, 3840, 295, 28867, 32284, 11, 411, 257, 15245, 295], "temperature": 0.0, "avg_logprob": -0.1369158686423788, "compression_ratio": 1.408, "no_speech_prob": 4.8603960749460384e-06}, {"id": 762, "seek": 556272, "start": 5568.8, "end": 5582.68, "text": " matrices, and so forth. If you are interested in APL, which I hope you are, we have a whole", "tokens": [32284, 11, 293, 370, 5220, 13, 759, 291, 366, 3102, 294, 5372, 43, 11, 597, 286, 1454, 291, 366, 11, 321, 362, 257, 1379], "temperature": 0.0, "avg_logprob": -0.1369158686423788, "compression_ratio": 1.408, "no_speech_prob": 4.8603960749460384e-06}, {"id": 763, "seek": 558268, "start": 5582.68, "end": 5595.8, "text": " APL and array programming section on our forums, and also we've prepared a whole set of notes", "tokens": [5372, 43, 293, 10225, 9410, 3541, 322, 527, 26998, 11, 293, 611, 321, 600, 4927, 257, 1379, 992, 295, 5570], "temperature": 0.0, "avg_logprob": -0.08958930969238281, "compression_ratio": 1.3597122302158273, "no_speech_prob": 3.3931257803487824e-06}, {"id": 764, "seek": 558268, "start": 5595.8, "end": 5607.6, "text": " on every single glyph in APL, which also covers all kinds of interesting mathematical concepts,", "tokens": [322, 633, 2167, 22633, 950, 294, 5372, 43, 11, 597, 611, 10538, 439, 3685, 295, 1880, 18894, 10392, 11], "temperature": 0.0, "avg_logprob": -0.08958930969238281, "compression_ratio": 1.3597122302158273, "no_speech_prob": 3.3931257803487824e-06}, {"id": 765, "seek": 560760, "start": 5607.6, "end": 5618.160000000001, "text": " like complex direction and magnitude, and all kinds of fun stuff like that. That's all", "tokens": [411, 3997, 3513, 293, 15668, 11, 293, 439, 3685, 295, 1019, 1507, 411, 300, 13, 663, 311, 439], "temperature": 0.0, "avg_logprob": -0.09344749450683594, "compression_ratio": 1.5418502202643172, "no_speech_prob": 1.644161056901794e-05}, {"id": 766, "seek": 560760, "start": 5618.160000000001, "end": 5622.68, "text": " totally optional, but a lot of people who do APL say that they feel like they've become", "tokens": [3879, 17312, 11, 457, 257, 688, 295, 561, 567, 360, 5372, 43, 584, 300, 436, 841, 411, 436, 600, 1813], "temperature": 0.0, "avg_logprob": -0.09344749450683594, "compression_ratio": 1.5418502202643172, "no_speech_prob": 1.644161056901794e-05}, {"id": 767, "seek": 560760, "start": 5622.68, "end": 5630.120000000001, "text": " a much better programmer in the process, and also you'll find here at the forums a set", "tokens": [257, 709, 1101, 32116, 294, 264, 1399, 11, 293, 611, 291, 603, 915, 510, 412, 264, 26998, 257, 992], "temperature": 0.0, "avg_logprob": -0.09344749450683594, "compression_ratio": 1.5418502202643172, "no_speech_prob": 1.644161056901794e-05}, {"id": 768, "seek": 560760, "start": 5630.120000000001, "end": 5636.52, "text": " of 17 study sessions of an hour or two each covering the entirety of the language, every", "tokens": [295, 3282, 2979, 11081, 295, 364, 1773, 420, 732, 1184, 10322, 264, 31557, 295, 264, 2856, 11, 633], "temperature": 0.0, "avg_logprob": -0.09344749450683594, "compression_ratio": 1.5418502202643172, "no_speech_prob": 1.644161056901794e-05}, {"id": 769, "seek": 563652, "start": 5636.52, "end": 5646.68, "text": " single glyph. So that's all like where this stuff comes from. So this batch of 50,000", "tokens": [2167, 22633, 950, 13, 407, 300, 311, 439, 411, 689, 341, 1507, 1487, 490, 13, 407, 341, 15245, 295, 2625, 11, 1360], "temperature": 0.0, "avg_logprob": -0.1882603250700852, "compression_ratio": 1.2481751824817517, "no_speech_prob": 6.643243978032842e-06}, {"id": 770, "seek": 563652, "start": 5646.68, "end": 5658.84, "text": " images, 50,028 by 28 images, is what we call a rank 3 tensor in PyTorch. In NumPy, we", "tokens": [5267, 11, 2625, 11, 15, 11205, 538, 7562, 5267, 11, 307, 437, 321, 818, 257, 6181, 805, 40863, 294, 9953, 51, 284, 339, 13, 682, 22592, 47, 88, 11, 321], "temperature": 0.0, "avg_logprob": -0.1882603250700852, "compression_ratio": 1.2481751824817517, "no_speech_prob": 6.643243978032842e-06}, {"id": 771, "seek": 565884, "start": 5658.84, "end": 5668.28, "text": " would call it an array with three dimensions. Those are the same thing. So what is the rank?", "tokens": [576, 818, 309, 364, 10225, 365, 1045, 12819, 13, 3950, 366, 264, 912, 551, 13, 407, 437, 307, 264, 6181, 30], "temperature": 0.0, "avg_logprob": -0.06130470548357282, "compression_ratio": 1.587878787878788, "no_speech_prob": 3.1381177905132063e-06}, {"id": 772, "seek": 565884, "start": 5668.28, "end": 5675.56, "text": " The rank is just the number of dimensions. It's 50,000 images of 28 high by 28 wide,", "tokens": [440, 6181, 307, 445, 264, 1230, 295, 12819, 13, 467, 311, 2625, 11, 1360, 5267, 295, 7562, 1090, 538, 7562, 4874, 11], "temperature": 0.0, "avg_logprob": -0.06130470548357282, "compression_ratio": 1.587878787878788, "no_speech_prob": 3.1381177905132063e-06}, {"id": 773, "seek": 565884, "start": 5675.56, "end": 5682.0, "text": " so there are three dimensions that is the rank of the tensor. So if we then pick out", "tokens": [370, 456, 366, 1045, 12819, 300, 307, 264, 6181, 295, 264, 40863, 13, 407, 498, 321, 550, 1888, 484], "temperature": 0.0, "avg_logprob": -0.06130470548357282, "compression_ratio": 1.587878787878788, "no_speech_prob": 3.1381177905132063e-06}, {"id": 774, "seek": 568200, "start": 5682.0, "end": 5693.36, "text": " a particular image, right, then we look at its shape, we could call this a matrix. It's", "tokens": [257, 1729, 3256, 11, 558, 11, 550, 321, 574, 412, 1080, 3909, 11, 321, 727, 818, 341, 257, 8141, 13, 467, 311], "temperature": 0.0, "avg_logprob": -0.10097908973693848, "compression_ratio": 1.5317919075144508, "no_speech_prob": 1.9333558611833723e-06}, {"id": 775, "seek": 568200, "start": 5693.36, "end": 5701.0, "text": " a 28 by 28 tensor, or we could call it a rank 2 tensor. A vector is a rank 1 tensor. In", "tokens": [257, 7562, 538, 7562, 40863, 11, 420, 321, 727, 818, 309, 257, 6181, 568, 40863, 13, 316, 8062, 307, 257, 6181, 502, 40863, 13, 682], "temperature": 0.0, "avg_logprob": -0.10097908973693848, "compression_ratio": 1.5317919075144508, "no_speech_prob": 1.9333558611833723e-06}, {"id": 776, "seek": 568200, "start": 5701.0, "end": 5708.2, "text": " APL, a scalar is a rank 0 tensor, and that's the way it should be. A lot of languages and", "tokens": [5372, 43, 11, 257, 39684, 307, 257, 6181, 1958, 40863, 11, 293, 300, 311, 264, 636, 309, 820, 312, 13, 316, 688, 295, 8650, 293], "temperature": 0.0, "avg_logprob": -0.10097908973693848, "compression_ratio": 1.5317919075144508, "no_speech_prob": 1.9333558611833723e-06}, {"id": 777, "seek": 570820, "start": 5708.2, "end": 5713.679999999999, "text": " libraries don't unfortunately think of it that way. So what is a scalar is a bit dependent", "tokens": [15148, 500, 380, 7015, 519, 295, 309, 300, 636, 13, 407, 437, 307, 257, 39684, 307, 257, 857, 12334], "temperature": 0.0, "avg_logprob": -0.11848245348249163, "compression_ratio": 1.2919708029197081, "no_speech_prob": 9.721522928884951e-07}, {"id": 778, "seek": 570820, "start": 5713.679999999999, "end": 5724.44, "text": " on the language. Okay, so we can index into the zeroth image, 20th row, 15th column to", "tokens": [322, 264, 2856, 13, 1033, 11, 370, 321, 393, 8186, 666, 264, 44746, 900, 3256, 11, 945, 392, 5386, 11, 2119, 392, 7738, 281], "temperature": 0.0, "avg_logprob": -0.11848245348249163, "compression_ratio": 1.2919708029197081, "no_speech_prob": 9.721522928884951e-07}, {"id": 779, "seek": 572444, "start": 5724.44, "end": 5747.599999999999, "text": " get back this same number. Okay, so we can take xtrain.shape, which is 50,000 by 784,", "tokens": [483, 646, 341, 912, 1230, 13, 1033, 11, 370, 321, 393, 747, 220, 734, 7146, 13, 82, 42406, 11, 597, 307, 2625, 11, 1360, 538, 1614, 25494, 11], "temperature": 0.0, "avg_logprob": -0.1076147130557469, "compression_ratio": 1.3902439024390243, "no_speech_prob": 2.1233695406408515e-06}, {"id": 780, "seek": 572444, "start": 5747.599999999999, "end": 5753.599999999999, "text": " and you can destructure it into n, which is the number of images, and c, which is the", "tokens": [293, 291, 393, 2677, 2885, 309, 666, 297, 11, 597, 307, 264, 1230, 295, 5267, 11, 293, 269, 11, 597, 307, 264], "temperature": 0.0, "avg_logprob": -0.1076147130557469, "compression_ratio": 1.3902439024390243, "no_speech_prob": 2.1233695406408515e-06}, {"id": 781, "seek": 575360, "start": 5753.6, "end": 5763.400000000001, "text": " number of, the full number of columns, for example. And we can also, well this is actually", "tokens": [1230, 295, 11, 264, 1577, 1230, 295, 13766, 11, 337, 1365, 13, 400, 321, 393, 611, 11, 731, 341, 307, 767], "temperature": 0.0, "avg_logprob": -0.14245670318603515, "compression_ratio": 1.6153846153846154, "no_speech_prob": 4.637857728084782e-06}, {"id": 782, "seek": 575360, "start": 5763.400000000001, "end": 5768.320000000001, "text": " part of the standard library, so we're allowed to use min, so we can find out in ytrain what's", "tokens": [644, 295, 264, 3832, 6405, 11, 370, 321, 434, 4350, 281, 764, 923, 11, 370, 321, 393, 915, 484, 294, 288, 83, 7146, 437, 311], "temperature": 0.0, "avg_logprob": -0.14245670318603515, "compression_ratio": 1.6153846153846154, "no_speech_prob": 4.637857728084782e-06}, {"id": 783, "seek": 575360, "start": 5768.320000000001, "end": 5775.68, "text": " the smallest number, and what's the maximum number, so they go from 0 to 9. So you see", "tokens": [264, 16998, 1230, 11, 293, 437, 311, 264, 6674, 1230, 11, 370, 436, 352, 490, 1958, 281, 1722, 13, 407, 291, 536], "temperature": 0.0, "avg_logprob": -0.14245670318603515, "compression_ratio": 1.6153846153846154, "no_speech_prob": 4.637857728084782e-06}, {"id": 784, "seek": 575360, "start": 5775.68, "end": 5783.52, "text": " here it's not just the number 0, it's the scalar tensor 0. They act almost the same,", "tokens": [510, 309, 311, 406, 445, 264, 1230, 1958, 11, 309, 311, 264, 39684, 40863, 1958, 13, 814, 605, 1920, 264, 912, 11], "temperature": 0.0, "avg_logprob": -0.14245670318603515, "compression_ratio": 1.6153846153846154, "no_speech_prob": 4.637857728084782e-06}, {"id": 785, "seek": 578352, "start": 5783.52, "end": 5791.68, "text": " most of the time. So here's some example of a bit of the ytrain, so you can see these", "tokens": [881, 295, 264, 565, 13, 407, 510, 311, 512, 1365, 295, 257, 857, 295, 264, 288, 83, 7146, 11, 370, 291, 393, 536, 613], "temperature": 0.0, "avg_logprob": -0.15030037273060193, "compression_ratio": 1.558139534883721, "no_speech_prob": 5.0147245929110795e-06}, {"id": 786, "seek": 578352, "start": 5791.68, "end": 5797.0, "text": " are basically, this is going to be the labels, right, these are our digits, and this is its", "tokens": [366, 1936, 11, 341, 307, 516, 281, 312, 264, 16949, 11, 558, 11, 613, 366, 527, 27011, 11, 293, 341, 307, 1080], "temperature": 0.0, "avg_logprob": -0.15030037273060193, "compression_ratio": 1.558139534883721, "no_speech_prob": 5.0147245929110795e-06}, {"id": 787, "seek": 578352, "start": 5797.0, "end": 5810.84, "text": " shape so there's just 50,000 of these labels. Okay, and so since we're allowed to use this", "tokens": [3909, 370, 456, 311, 445, 2625, 11, 1360, 295, 613, 16949, 13, 1033, 11, 293, 370, 1670, 321, 434, 4350, 281, 764, 341], "temperature": 0.0, "avg_logprob": -0.15030037273060193, "compression_ratio": 1.558139534883721, "no_speech_prob": 5.0147245929110795e-06}, {"id": 788, "seek": 581084, "start": 5810.84, "end": 5814.76, "text": " in the standard library, well it also exists in PyTorch, so that means we're also allowed", "tokens": [294, 264, 3832, 6405, 11, 731, 309, 611, 8198, 294, 9953, 51, 284, 339, 11, 370, 300, 1355, 321, 434, 611, 4350], "temperature": 0.0, "avg_logprob": -0.1125986099243164, "compression_ratio": 1.6164383561643836, "no_speech_prob": 2.601608457553084e-06}, {"id": 789, "seek": 581084, "start": 5814.76, "end": 5823.2, "text": " to use the.min and.max properties. Alright, so before we wrap up we're going to do one", "tokens": [281, 764, 264, 2411, 2367, 293, 2411, 41167, 7221, 13, 2798, 11, 370, 949, 321, 7019, 493, 321, 434, 516, 281, 360, 472], "temperature": 0.0, "avg_logprob": -0.1125986099243164, "compression_ratio": 1.6164383561643836, "no_speech_prob": 2.601608457553084e-06}, {"id": 790, "seek": 581084, "start": 5823.2, "end": 5829.4400000000005, "text": " more thing, and I don't know what the, what you would call kind of anti-cheating, but", "tokens": [544, 551, 11, 293, 286, 500, 380, 458, 437, 264, 11, 437, 291, 576, 818, 733, 295, 6061, 12, 1876, 990, 11, 457], "temperature": 0.0, "avg_logprob": -0.1125986099243164, "compression_ratio": 1.6164383561643836, "no_speech_prob": 2.601608457553084e-06}, {"id": 791, "seek": 581084, "start": 5829.4400000000005, "end": 5835.96, "text": " according to our rules we're allowed to use random numbers because there is a random number", "tokens": [4650, 281, 527, 4474, 321, 434, 4350, 281, 764, 4974, 3547, 570, 456, 307, 257, 4974, 1230], "temperature": 0.0, "avg_logprob": -0.1125986099243164, "compression_ratio": 1.6164383561643836, "no_speech_prob": 2.601608457553084e-06}, {"id": 792, "seek": 583596, "start": 5835.96, "end": 5841.64, "text": " generator in the Python standard library, but we're going to do random numbers from", "tokens": [19265, 294, 264, 15329, 3832, 6405, 11, 457, 321, 434, 516, 281, 360, 4974, 3547, 490], "temperature": 0.0, "avg_logprob": -0.09943040987340415, "compression_ratio": 1.7047619047619047, "no_speech_prob": 5.682393748429604e-06}, {"id": 793, "seek": 583596, "start": 5841.64, "end": 5847.28, "text": " scratch ourselves, and the reason we're going to do that is even though according to the", "tokens": [8459, 4175, 11, 293, 264, 1778, 321, 434, 516, 281, 360, 300, 307, 754, 1673, 4650, 281, 264], "temperature": 0.0, "avg_logprob": -0.09943040987340415, "compression_ratio": 1.7047619047619047, "no_speech_prob": 5.682393748429604e-06}, {"id": 794, "seek": 583596, "start": 5847.28, "end": 5852.64, "text": " rules we could be allowed to use the standard library one, it's actually extremely instructive", "tokens": [4474, 321, 727, 312, 4350, 281, 764, 264, 3832, 6405, 472, 11, 309, 311, 767, 4664, 7232, 488], "temperature": 0.0, "avg_logprob": -0.09943040987340415, "compression_ratio": 1.7047619047619047, "no_speech_prob": 5.682393748429604e-06}, {"id": 795, "seek": 583596, "start": 5852.64, "end": 5858.4800000000005, "text": " to build our own random number generator from scratch. Well at least I think so. Let's see", "tokens": [281, 1322, 527, 1065, 4974, 1230, 19265, 490, 8459, 13, 1042, 412, 1935, 286, 519, 370, 13, 961, 311, 536], "temperature": 0.0, "avg_logprob": -0.09943040987340415, "compression_ratio": 1.7047619047619047, "no_speech_prob": 5.682393748429604e-06}, {"id": 796, "seek": 585848, "start": 5858.48, "end": 5878.08, "text": " what you think. So there is no way normally in software to create a random number. Unfortunately,", "tokens": [437, 291, 519, 13, 407, 456, 307, 572, 636, 5646, 294, 4722, 281, 1884, 257, 4974, 1230, 13, 8590, 11], "temperature": 0.0, "avg_logprob": -0.19198360848934093, "compression_ratio": 1.3669064748201438, "no_speech_prob": 1.2878922461823095e-06}, {"id": 797, "seek": 585848, "start": 5878.08, "end": 5886.24, "text": " computers you know add, subtract, times, logic gate, stuff like that. So how does one create", "tokens": [10807, 291, 458, 909, 11, 16390, 11, 1413, 11, 9952, 8539, 11, 1507, 411, 300, 13, 407, 577, 775, 472, 1884], "temperature": 0.0, "avg_logprob": -0.19198360848934093, "compression_ratio": 1.3669064748201438, "no_speech_prob": 1.2878922461823095e-06}, {"id": 798, "seek": 588624, "start": 5886.24, "end": 5891.24, "text": " random numbers? Well you could go to the Australian National University quantum random number", "tokens": [4974, 3547, 30, 1042, 291, 727, 352, 281, 264, 13337, 4862, 3535, 13018, 4974, 1230], "temperature": 0.0, "avg_logprob": -0.11689145881009388, "compression_ratio": 1.7361111111111112, "no_speech_prob": 4.4951707423024345e-06}, {"id": 799, "seek": 588624, "start": 5891.24, "end": 5900.639999999999, "text": " generator, and this looks at the quantum fluctuations of the vacuum and provides an API which will", "tokens": [19265, 11, 293, 341, 1542, 412, 264, 13018, 45276, 295, 264, 14224, 293, 6417, 364, 9362, 597, 486], "temperature": 0.0, "avg_logprob": -0.11689145881009388, "compression_ratio": 1.7361111111111112, "no_speech_prob": 4.4951707423024345e-06}, {"id": 800, "seek": 588624, "start": 5900.639999999999, "end": 5911.0, "text": " actually hook you in and return quantum random fluctuations of the vacuum. So that's about,", "tokens": [767, 6328, 291, 294, 293, 2736, 13018, 4974, 45276, 295, 264, 14224, 13, 407, 300, 311, 466, 11], "temperature": 0.0, "avg_logprob": -0.11689145881009388, "compression_ratio": 1.7361111111111112, "no_speech_prob": 4.4951707423024345e-06}, {"id": 801, "seek": 588624, "start": 5911.0, "end": 5916.12, "text": " that's the most random thing I'm aware of. So that would be one way to get random numbers.", "tokens": [300, 311, 264, 881, 4974, 551, 286, 478, 3650, 295, 13, 407, 300, 576, 312, 472, 636, 281, 483, 4974, 3547, 13], "temperature": 0.0, "avg_logprob": -0.11689145881009388, "compression_ratio": 1.7361111111111112, "no_speech_prob": 4.4951707423024345e-06}, {"id": 802, "seek": 591612, "start": 5916.12, "end": 5925.32, "text": " And there's actually an API for that. So there's a bit of fun. You could do what Cloudflare", "tokens": [400, 456, 311, 767, 364, 9362, 337, 300, 13, 407, 456, 311, 257, 857, 295, 1019, 13, 509, 727, 360, 437, 8061, 3423, 543], "temperature": 0.0, "avg_logprob": -0.12528157234191895, "compression_ratio": 1.3458646616541354, "no_speech_prob": 4.936959612678038e-06}, {"id": 803, "seek": 591612, "start": 5925.32, "end": 5944.32, "text": " does. Cloudflare has a huge wall full of lava lamps, and it uses the pixels of a camera", "tokens": [775, 13, 8061, 3423, 543, 575, 257, 2603, 2929, 1577, 295, 22097, 34887, 11, 293, 309, 4960, 264, 18668, 295, 257, 2799], "temperature": 0.0, "avg_logprob": -0.12528157234191895, "compression_ratio": 1.3458646616541354, "no_speech_prob": 4.936959612678038e-06}, {"id": 804, "seek": 594432, "start": 5944.32, "end": 5953.08, "text": " looking at those lava lamps to generate random numbers. Intel nowadays actually has something", "tokens": [1237, 412, 729, 22097, 34887, 281, 8460, 4974, 3547, 13, 19762, 13434, 767, 575, 746], "temperature": 0.0, "avg_logprob": -0.12817513942718506, "compression_ratio": 1.4296875, "no_speech_prob": 4.860422905039741e-06}, {"id": 805, "seek": 594432, "start": 5953.08, "end": 5967.5199999999995, "text": " in its chips which you can call rdrand, which will return random numbers on certain Intel", "tokens": [294, 1080, 11583, 597, 291, 393, 818, 367, 67, 3699, 11, 597, 486, 2736, 4974, 3547, 322, 1629, 19762], "temperature": 0.0, "avg_logprob": -0.12817513942718506, "compression_ratio": 1.4296875, "no_speech_prob": 4.860422905039741e-06}, {"id": 806, "seek": 596752, "start": 5967.52, "end": 5975.040000000001, "text": " chips from 2012. All of these things are kind of slow, they can kind of get you one random", "tokens": [11583, 490, 9125, 13, 1057, 295, 613, 721, 366, 733, 295, 2964, 11, 436, 393, 733, 295, 483, 291, 472, 4974], "temperature": 0.0, "avg_logprob": -0.07618827130421098, "compression_ratio": 1.7897435897435898, "no_speech_prob": 8.139602869050577e-06}, {"id": 807, "seek": 596752, "start": 5975.040000000001, "end": 5980.8, "text": " number from time to time. We want some way of getting lots and lots of random numbers.", "tokens": [1230, 490, 565, 281, 565, 13, 492, 528, 512, 636, 295, 1242, 3195, 293, 3195, 295, 4974, 3547, 13], "temperature": 0.0, "avg_logprob": -0.07618827130421098, "compression_ratio": 1.7897435897435898, "no_speech_prob": 8.139602869050577e-06}, {"id": 808, "seek": 596752, "start": 5980.8, "end": 5988.8, "text": " And so what we do is we use something called a pseudo random number generator. A pseudo", "tokens": [400, 370, 437, 321, 360, 307, 321, 764, 746, 1219, 257, 35899, 4974, 1230, 19265, 13, 316, 35899], "temperature": 0.0, "avg_logprob": -0.07618827130421098, "compression_ratio": 1.7897435897435898, "no_speech_prob": 8.139602869050577e-06}, {"id": 809, "seek": 596752, "start": 5988.8, "end": 5995.4400000000005, "text": " random number generator is a mathematical function that you can call lots of times,", "tokens": [4974, 1230, 19265, 307, 257, 18894, 2445, 300, 291, 393, 818, 3195, 295, 1413, 11], "temperature": 0.0, "avg_logprob": -0.07618827130421098, "compression_ratio": 1.7897435897435898, "no_speech_prob": 8.139602869050577e-06}, {"id": 810, "seek": 599544, "start": 5995.44, "end": 6005.32, "text": " and each time you call it, it will give you a number that looks random. To show you what", "tokens": [293, 1184, 565, 291, 818, 309, 11, 309, 486, 976, 291, 257, 1230, 300, 1542, 4974, 13, 1407, 855, 291, 437], "temperature": 0.0, "avg_logprob": -0.08809257776309283, "compression_ratio": 1.469945355191257, "no_speech_prob": 6.4390201259811874e-06}, {"id": 811, "seek": 599544, "start": 6005.32, "end": 6012.719999999999, "text": " I mean by that, I'm going to run some code. And I've created a function which we'll look", "tokens": [286, 914, 538, 300, 11, 286, 478, 516, 281, 1190, 512, 3089, 13, 400, 286, 600, 2942, 257, 2445, 597, 321, 603, 574], "temperature": 0.0, "avg_logprob": -0.08809257776309283, "compression_ratio": 1.469945355191257, "no_speech_prob": 6.4390201259811874e-06}, {"id": 812, "seek": 599544, "start": 6012.719999999999, "end": 6021.28, "text": " at in a moment called rdrand. And if I call rdrand 50 times and plot it, there's no obvious", "tokens": [412, 294, 257, 1623, 1219, 367, 67, 3699, 13, 400, 498, 286, 818, 367, 67, 3699, 2625, 1413, 293, 7542, 309, 11, 456, 311, 572, 6322], "temperature": 0.0, "avg_logprob": -0.08809257776309283, "compression_ratio": 1.469945355191257, "no_speech_prob": 6.4390201259811874e-06}, {"id": 813, "seek": 602128, "start": 6021.28, "end": 6026.96, "text": " relationship between one call and the next. That's one thing that I would expect to see", "tokens": [2480, 1296, 472, 818, 293, 264, 958, 13, 663, 311, 472, 551, 300, 286, 576, 2066, 281, 536], "temperature": 0.0, "avg_logprob": -0.08348172692691579, "compression_ratio": 1.7684729064039408, "no_speech_prob": 2.1444689991767518e-05}, {"id": 814, "seek": 602128, "start": 6026.96, "end": 6032.28, "text": " from my random numbers. I would expect that each time I call rdrand, the numbers would", "tokens": [490, 452, 4974, 3547, 13, 286, 576, 2066, 300, 1184, 565, 286, 818, 367, 67, 3699, 11, 264, 3547, 576], "temperature": 0.0, "avg_logprob": -0.08348172692691579, "compression_ratio": 1.7684729064039408, "no_speech_prob": 2.1444689991767518e-05}, {"id": 815, "seek": 602128, "start": 6032.28, "end": 6039.36, "text": " look quite different to each other. The second thing is rdrand is meant to be returning uniformly", "tokens": [574, 1596, 819, 281, 1184, 661, 13, 440, 1150, 551, 307, 367, 67, 3699, 307, 4140, 281, 312, 12678, 48806], "temperature": 0.0, "avg_logprob": -0.08348172692691579, "compression_ratio": 1.7684729064039408, "no_speech_prob": 2.1444689991767518e-05}, {"id": 816, "seek": 602128, "start": 6039.36, "end": 6044.92, "text": " distributed random numbers. And therefore if I call it lots and lots and lots of times", "tokens": [12631, 4974, 3547, 13, 400, 4412, 498, 286, 818, 309, 3195, 293, 3195, 293, 3195, 295, 1413], "temperature": 0.0, "avg_logprob": -0.08348172692691579, "compression_ratio": 1.7684729064039408, "no_speech_prob": 2.1444689991767518e-05}, {"id": 817, "seek": 604492, "start": 6044.92, "end": 6051.4, "text": " and plot its histogram, I would expect to see exactly this, which is each from 0 to", "tokens": [293, 7542, 1080, 49816, 11, 286, 576, 2066, 281, 536, 2293, 341, 11, 597, 307, 1184, 490, 1958, 281], "temperature": 0.0, "avg_logprob": -0.11346254070985665, "compression_ratio": 1.771144278606965, "no_speech_prob": 3.5008433769689873e-06}, {"id": 818, "seek": 604492, "start": 6051.4, "end": 6056.8, "text": " 0.1, there's a few, from 0.1 to 0.2, there's a few, from 0.2 to 0.3, there's a few. It's", "tokens": [1958, 13, 16, 11, 456, 311, 257, 1326, 11, 490, 1958, 13, 16, 281, 1958, 13, 17, 11, 456, 311, 257, 1326, 11, 490, 1958, 13, 17, 281, 1958, 13, 18, 11, 456, 311, 257, 1326, 13, 467, 311], "temperature": 0.0, "avg_logprob": -0.11346254070985665, "compression_ratio": 1.771144278606965, "no_speech_prob": 3.5008433769689873e-06}, {"id": 819, "seek": 604492, "start": 6056.8, "end": 6062.76, "text": " a fairly evenly spread thing. These are the two key things I would expect to see. An even", "tokens": [257, 6457, 17658, 3974, 551, 13, 1981, 366, 264, 732, 2141, 721, 286, 576, 2066, 281, 536, 13, 1107, 754], "temperature": 0.0, "avg_logprob": -0.11346254070985665, "compression_ratio": 1.771144278606965, "no_speech_prob": 3.5008433769689873e-06}, {"id": 820, "seek": 604492, "start": 6062.76, "end": 6068.12, "text": " distribution of random numbers and that there's no correlation or no obvious correlation from", "tokens": [7316, 295, 4974, 3547, 293, 300, 456, 311, 572, 20009, 420, 572, 6322, 20009, 490], "temperature": 0.0, "avg_logprob": -0.11346254070985665, "compression_ratio": 1.771144278606965, "no_speech_prob": 3.5008433769689873e-06}, {"id": 821, "seek": 606812, "start": 6068.12, "end": 6075.04, "text": " one to the other. So we want to try and create a function that has these properties. We're", "tokens": [472, 281, 264, 661, 13, 407, 321, 528, 281, 853, 293, 1884, 257, 2445, 300, 575, 613, 7221, 13, 492, 434], "temperature": 0.0, "avg_logprob": -0.08728965553077492, "compression_ratio": 1.6346863468634687, "no_speech_prob": 1.3007037523493636e-05}, {"id": 822, "seek": 606812, "start": 6075.04, "end": 6078.0, "text": " not going to derive it from scratch. I'm just going to tell you that we have a function", "tokens": [406, 516, 281, 28446, 309, 490, 8459, 13, 286, 478, 445, 516, 281, 980, 291, 300, 321, 362, 257, 2445], "temperature": 0.0, "avg_logprob": -0.08728965553077492, "compression_ratio": 1.6346863468634687, "no_speech_prob": 1.3007037523493636e-05}, {"id": 823, "seek": 606812, "start": 6078.0, "end": 6082.96, "text": " here called the Wickman-Hill algorithm. This is actually what Python used to use back in", "tokens": [510, 1219, 264, 47702, 1601, 12, 39, 373, 9284, 13, 639, 307, 767, 437, 15329, 1143, 281, 764, 646, 294], "temperature": 0.0, "avg_logprob": -0.08728965553077492, "compression_ratio": 1.6346863468634687, "no_speech_prob": 1.3007037523493636e-05}, {"id": 824, "seek": 606812, "start": 6082.96, "end": 6089.32, "text": " before Python 2.3. And the key reason we need to know about this is to understand really", "tokens": [949, 15329, 568, 13, 18, 13, 400, 264, 2141, 1778, 321, 643, 281, 458, 466, 341, 307, 281, 1223, 534], "temperature": 0.0, "avg_logprob": -0.08728965553077492, "compression_ratio": 1.6346863468634687, "no_speech_prob": 1.3007037523493636e-05}, {"id": 825, "seek": 606812, "start": 6089.32, "end": 6095.94, "text": " well the idea of random state. Random state is a global variable. It's something which", "tokens": [731, 264, 1558, 295, 4974, 1785, 13, 37603, 1785, 307, 257, 4338, 7006, 13, 467, 311, 746, 597], "temperature": 0.0, "avg_logprob": -0.08728965553077492, "compression_ratio": 1.6346863468634687, "no_speech_prob": 1.3007037523493636e-05}, {"id": 826, "seek": 609594, "start": 6095.94, "end": 6102.16, "text": " is, or at least it can be, most of the time when we use it, we use it as a random variable.", "tokens": [307, 11, 420, 412, 1935, 309, 393, 312, 11, 881, 295, 264, 565, 562, 321, 764, 309, 11, 321, 764, 309, 382, 257, 4974, 7006, 13], "temperature": 0.0, "avg_logprob": -0.1415801207224528, "compression_ratio": 1.7272727272727273, "no_speech_prob": 9.223390406987164e-06}, {"id": 827, "seek": 609594, "start": 6102.16, "end": 6106.2, "text": " And it's just basically one or more numbers. So we're going to start with no random state", "tokens": [400, 309, 311, 445, 1936, 472, 420, 544, 3547, 13, 407, 321, 434, 516, 281, 722, 365, 572, 4974, 1785], "temperature": 0.0, "avg_logprob": -0.1415801207224528, "compression_ratio": 1.7272727272727273, "no_speech_prob": 9.223390406987164e-06}, {"id": 828, "seek": 609594, "start": 6106.2, "end": 6111.36, "text": " at all. And we're going to create a function called seed that we're going to pass something", "tokens": [412, 439, 13, 400, 321, 434, 516, 281, 1884, 257, 2445, 1219, 8871, 300, 321, 434, 516, 281, 1320, 746], "temperature": 0.0, "avg_logprob": -0.1415801207224528, "compression_ratio": 1.7272727272727273, "no_speech_prob": 9.223390406987164e-06}, {"id": 829, "seek": 609594, "start": 6111.36, "end": 6117.66, "text": " to. And I just mashed the keyboard to create this number. Okay, so this is my random number.", "tokens": [281, 13, 400, 286, 445, 38964, 264, 10186, 281, 1884, 341, 1230, 13, 1033, 11, 370, 341, 307, 452, 4974, 1230, 13], "temperature": 0.0, "avg_logprob": -0.1415801207224528, "compression_ratio": 1.7272727272727273, "no_speech_prob": 9.223390406987164e-06}, {"id": 830, "seek": 609594, "start": 6117.66, "end": 6123.48, "text": " You could get this from the ANU quantum vacuum generator or from CloudFir's lava lamps or", "tokens": [509, 727, 483, 341, 490, 264, 5252, 52, 13018, 14224, 19265, 420, 490, 8061, 37, 347, 311, 22097, 34887, 420], "temperature": 0.0, "avg_logprob": -0.1415801207224528, "compression_ratio": 1.7272727272727273, "no_speech_prob": 9.223390406987164e-06}, {"id": 831, "seek": 612348, "start": 6123.48, "end": 6128.48, "text": " from your Intel chips ID, rand, or, you know, in Python land, we'd pretty much always use", "tokens": [490, 428, 19762, 11583, 7348, 11, 367, 474, 11, 420, 11, 291, 458, 11, 294, 15329, 2117, 11, 321, 1116, 1238, 709, 1009, 764], "temperature": 0.0, "avg_logprob": -0.1592934528986613, "compression_ratio": 1.5454545454545454, "no_speech_prob": 1.5056978099892149e-06}, {"id": 832, "seek": 612348, "start": 6128.48, "end": 6133.24, "text": " the number 42. Any of those are fine. So you pass in some number or you can pass in the", "tokens": [264, 1230, 14034, 13, 2639, 295, 729, 366, 2489, 13, 407, 291, 1320, 294, 512, 1230, 420, 291, 393, 1320, 294, 264], "temperature": 0.0, "avg_logprob": -0.1592934528986613, "compression_ratio": 1.5454545454545454, "no_speech_prob": 1.5056978099892149e-06}, {"id": 833, "seek": 612348, "start": 6133.24, "end": 6138.48, "text": " current tick count in nanoseconds. There's various ways of getting some random starting", "tokens": [2190, 5204, 1207, 294, 14067, 541, 28750, 13, 821, 311, 3683, 2098, 295, 1242, 512, 4974, 2891], "temperature": 0.0, "avg_logprob": -0.1592934528986613, "compression_ratio": 1.5454545454545454, "no_speech_prob": 1.5056978099892149e-06}, {"id": 834, "seek": 612348, "start": 6138.48, "end": 6149.28, "text": " point. And if we pass it into seed, it's going to do a bunch of modulo divisions and create", "tokens": [935, 13, 400, 498, 321, 1320, 309, 666, 8871, 11, 309, 311, 516, 281, 360, 257, 3840, 295, 1072, 13455, 24328, 293, 1884], "temperature": 0.0, "avg_logprob": -0.1592934528986613, "compression_ratio": 1.5454545454545454, "no_speech_prob": 1.5056978099892149e-06}, {"id": 835, "seek": 614928, "start": 6149.28, "end": 6155.96, "text": " a tuple of three things and it's going to store them in this global state. So rand state", "tokens": [257, 2604, 781, 295, 1045, 721, 293, 309, 311, 516, 281, 3531, 552, 294, 341, 4338, 1785, 13, 407, 367, 474, 1785], "temperature": 0.0, "avg_logprob": -0.08513182488040648, "compression_ratio": 1.5555555555555556, "no_speech_prob": 1.2289171991142211e-06}, {"id": 836, "seek": 614928, "start": 6155.96, "end": 6166.639999999999, "text": " now contains three numbers. Okay, so why did we do that? The reason we did that is because", "tokens": [586, 8306, 1045, 3547, 13, 1033, 11, 370, 983, 630, 321, 360, 300, 30, 440, 1778, 321, 630, 300, 307, 570], "temperature": 0.0, "avg_logprob": -0.08513182488040648, "compression_ratio": 1.5555555555555556, "no_speech_prob": 1.2289171991142211e-06}, {"id": 837, "seek": 614928, "start": 6166.639999999999, "end": 6173.88, "text": " now this function, which takes our random state, unpacks it into three things and does", "tokens": [586, 341, 2445, 11, 597, 2516, 527, 4974, 1785, 11, 20994, 7424, 309, 666, 1045, 721, 293, 775], "temperature": 0.0, "avg_logprob": -0.08513182488040648, "compression_ratio": 1.5555555555555556, "no_speech_prob": 1.2289171991142211e-06}, {"id": 838, "seek": 617388, "start": 6173.88, "end": 6179.400000000001, "text": " again a bunch of multiplications and moduloes and then sticks them together with various", "tokens": [797, 257, 3840, 295, 17596, 763, 293, 1072, 13455, 279, 293, 550, 12518, 552, 1214, 365, 3683], "temperature": 0.0, "avg_logprob": -0.09964057836639748, "compression_ratio": 1.6527777777777777, "no_speech_prob": 1.9033826674785814e-06}, {"id": 839, "seek": 617388, "start": 6179.400000000001, "end": 6187.0, "text": " kind of weights. Modulo one, so this is how you can pull out the decimal part. This returns", "tokens": [733, 295, 17443, 13, 6583, 13455, 472, 11, 370, 341, 307, 577, 291, 393, 2235, 484, 264, 26601, 644, 13, 639, 11247], "temperature": 0.0, "avg_logprob": -0.09964057836639748, "compression_ratio": 1.6527777777777777, "no_speech_prob": 1.9033826674785814e-06}, {"id": 840, "seek": 617388, "start": 6187.0, "end": 6194.32, "text": " random numbers. But the key thing I want you to understand is that we pull out the random", "tokens": [4974, 3547, 13, 583, 264, 2141, 551, 286, 528, 291, 281, 1223, 307, 300, 321, 2235, 484, 264, 4974], "temperature": 0.0, "avg_logprob": -0.09964057836639748, "compression_ratio": 1.6527777777777777, "no_speech_prob": 1.9033826674785814e-06}, {"id": 841, "seek": 617388, "start": 6194.32, "end": 6203.28, "text": " state at the start. We do some math thingies to it and then we store new random state.", "tokens": [1785, 412, 264, 722, 13, 492, 360, 512, 5221, 551, 530, 281, 309, 293, 550, 321, 3531, 777, 4974, 1785, 13], "temperature": 0.0, "avg_logprob": -0.09964057836639748, "compression_ratio": 1.6527777777777777, "no_speech_prob": 1.9033826674785814e-06}, {"id": 842, "seek": 620328, "start": 6203.28, "end": 6211.84, "text": " And so that means that each time I call this, I'm going to get a different number. Right?", "tokens": [400, 370, 300, 1355, 300, 1184, 565, 286, 818, 341, 11, 286, 478, 516, 281, 483, 257, 819, 1230, 13, 1779, 30], "temperature": 0.0, "avg_logprob": -0.10642134271016936, "compression_ratio": 1.627906976744186, "no_speech_prob": 3.3931387406482827e-06}, {"id": 843, "seek": 620328, "start": 6211.84, "end": 6216.12, "text": " So this is a random number generator and this is really important because lots of people", "tokens": [407, 341, 307, 257, 4974, 1230, 19265, 293, 341, 307, 534, 1021, 570, 3195, 295, 561], "temperature": 0.0, "avg_logprob": -0.10642134271016936, "compression_ratio": 1.627906976744186, "no_speech_prob": 3.3931387406482827e-06}, {"id": 844, "seek": 620328, "start": 6216.12, "end": 6224.639999999999, "text": " in the deep learning world screw this up, including me sometimes, which is to remember", "tokens": [294, 264, 2452, 2539, 1002, 5630, 341, 493, 11, 3009, 385, 2171, 11, 597, 307, 281, 1604], "temperature": 0.0, "avg_logprob": -0.10642134271016936, "compression_ratio": 1.627906976744186, "no_speech_prob": 3.3931387406482827e-06}, {"id": 845, "seek": 620328, "start": 6224.639999999999, "end": 6232.4, "text": " that random number generators rely on this state. So let me show you where that will", "tokens": [300, 4974, 1230, 38662, 10687, 322, 341, 1785, 13, 407, 718, 385, 855, 291, 689, 300, 486], "temperature": 0.0, "avg_logprob": -0.10642134271016936, "compression_ratio": 1.627906976744186, "no_speech_prob": 3.3931387406482827e-06}, {"id": 846, "seek": 623240, "start": 6232.4, "end": 6239.92, "text": " get you if you're not careful. If we use this special thing called fork, that creates a", "tokens": [483, 291, 498, 291, 434, 406, 5026, 13, 759, 321, 764, 341, 2121, 551, 1219, 17716, 11, 300, 7829, 257], "temperature": 0.0, "avg_logprob": -0.14275423685709634, "compression_ratio": 1.5402298850574712, "no_speech_prob": 1.750283445289824e-05}, {"id": 847, "seek": 623240, "start": 6239.92, "end": 6250.04, "text": " whole separate copy of this Python process. In one copy os.fork returns true. In the other", "tokens": [1379, 4994, 5055, 295, 341, 15329, 1399, 13, 682, 472, 5055, 3003, 13, 69, 1284, 11247, 2074, 13, 682, 264, 661], "temperature": 0.0, "avg_logprob": -0.14275423685709634, "compression_ratio": 1.5402298850574712, "no_speech_prob": 1.750283445289824e-05}, {"id": 848, "seek": 623240, "start": 6250.04, "end": 6258.36, "text": " copy it returns false, roughly speaking. So this copy here is this, if I say this version", "tokens": [5055, 309, 11247, 7908, 11, 9810, 4124, 13, 407, 341, 5055, 510, 307, 341, 11, 498, 286, 584, 341, 3037], "temperature": 0.0, "avg_logprob": -0.14275423685709634, "compression_ratio": 1.5402298850574712, "no_speech_prob": 1.750283445289824e-05}, {"id": 849, "seek": 625836, "start": 6258.36, "end": 6264.2, "text": " here, the true version, is the original non-copied. It's called the parent. And so in my else", "tokens": [510, 11, 264, 2074, 3037, 11, 307, 264, 3380, 2107, 12, 13084, 1091, 13, 467, 311, 1219, 264, 2596, 13, 400, 370, 294, 452, 1646], "temperature": 0.0, "avg_logprob": -0.16568573075111467, "compression_ratio": 1.7023255813953488, "no_speech_prob": 1.7330501123069553e-06}, {"id": 850, "seek": 625836, "start": 6264.2, "end": 6268.799999999999, "text": " here, so this will only be called by the parent. This will only be called by the copy, which", "tokens": [510, 11, 370, 341, 486, 787, 312, 1219, 538, 264, 2596, 13, 639, 486, 787, 312, 1219, 538, 264, 5055, 11, 597], "temperature": 0.0, "avg_logprob": -0.16568573075111467, "compression_ratio": 1.7023255813953488, "no_speech_prob": 1.7330501123069553e-06}, {"id": 851, "seek": 625836, "start": 6268.799999999999, "end": 6275.5599999999995, "text": " is called the child. In each one I'm calling rand. These are two different random numbers,", "tokens": [307, 1219, 264, 1440, 13, 682, 1184, 472, 286, 478, 5141, 367, 474, 13, 1981, 366, 732, 819, 4974, 3547, 11], "temperature": 0.0, "avg_logprob": -0.16568573075111467, "compression_ratio": 1.7023255813953488, "no_speech_prob": 1.7330501123069553e-06}, {"id": 852, "seek": 625836, "start": 6275.5599999999995, "end": 6285.639999999999, "text": " right? Wrong. They're the same number. Now why is that? That's because this process here", "tokens": [558, 30, 28150, 13, 814, 434, 264, 912, 1230, 13, 823, 983, 307, 300, 30, 663, 311, 570, 341, 1399, 510], "temperature": 0.0, "avg_logprob": -0.16568573075111467, "compression_ratio": 1.7023255813953488, "no_speech_prob": 1.7330501123069553e-06}, {"id": 853, "seek": 628564, "start": 6285.64, "end": 6293.320000000001, "text": " and this process here are copies of each other and therefore they each contain the same numbers", "tokens": [293, 341, 1399, 510, 366, 14341, 295, 1184, 661, 293, 4412, 436, 1184, 5304, 264, 912, 3547], "temperature": 0.0, "avg_logprob": -0.07754973570505778, "compression_ratio": 1.6352941176470588, "no_speech_prob": 6.276698627516453e-07}, {"id": 854, "seek": 628564, "start": 6293.320000000001, "end": 6301.860000000001, "text": " in random state. So this is something that comes up in deep learning all the time because", "tokens": [294, 4974, 1785, 13, 407, 341, 307, 746, 300, 1487, 493, 294, 2452, 2539, 439, 264, 565, 570], "temperature": 0.0, "avg_logprob": -0.07754973570505778, "compression_ratio": 1.6352941176470588, "no_speech_prob": 6.276698627516453e-07}, {"id": 855, "seek": 628564, "start": 6301.860000000001, "end": 6311.4400000000005, "text": " in deep learning we often do parallel processing, for example, to generate lots of augmented", "tokens": [294, 2452, 2539, 321, 2049, 360, 8952, 9007, 11, 337, 1365, 11, 281, 8460, 3195, 295, 36155], "temperature": 0.0, "avg_logprob": -0.07754973570505778, "compression_ratio": 1.6352941176470588, "no_speech_prob": 6.276698627516453e-07}, {"id": 856, "seek": 631144, "start": 6311.44, "end": 6317.96, "text": " images at the same time using multiple processes. Fast AI used to have a bug, in fact, where", "tokens": [5267, 412, 264, 912, 565, 1228, 3866, 7555, 13, 15968, 7318, 1143, 281, 362, 257, 7426, 11, 294, 1186, 11, 689], "temperature": 0.0, "avg_logprob": -0.11712725482769866, "compression_ratio": 1.4627659574468086, "no_speech_prob": 3.4465706448827405e-06}, {"id": 857, "seek": 631144, "start": 6317.96, "end": 6325.639999999999, "text": " we failed to correctly initialize the random number generator separately in each process.", "tokens": [321, 7612, 281, 8944, 5883, 1125, 264, 4974, 1230, 19265, 14759, 294, 1184, 1399, 13], "temperature": 0.0, "avg_logprob": -0.11712725482769866, "compression_ratio": 1.4627659574468086, "no_speech_prob": 3.4465706448827405e-06}, {"id": 858, "seek": 631144, "start": 6325.639999999999, "end": 6336.44, "text": " And in fact, to this day, at least as of October 2022, torch.rand itself by default fails to", "tokens": [400, 294, 1186, 11, 281, 341, 786, 11, 412, 1935, 382, 295, 7617, 20229, 11, 27822, 13, 3699, 2564, 538, 7576, 18199, 281], "temperature": 0.0, "avg_logprob": -0.11712725482769866, "compression_ratio": 1.4627659574468086, "no_speech_prob": 3.4465706448827405e-06}, {"id": 859, "seek": 633644, "start": 6336.44, "end": 6342.679999999999, "text": " initialize the random number generator. That's the same number. Okay. So you've got to be", "tokens": [5883, 1125, 264, 4974, 1230, 19265, 13, 663, 311, 264, 912, 1230, 13, 1033, 13, 407, 291, 600, 658, 281, 312], "temperature": 0.0, "avg_logprob": -0.22974731371952936, "compression_ratio": 1.2753623188405796, "no_speech_prob": 3.844918410322862e-06}, {"id": 860, "seek": 633644, "start": 6342.679999999999, "end": 6361.04, "text": " careful. Now I have a feeling NumPy gets it right. Let's check. Is that how you do it?", "tokens": [5026, 13, 823, 286, 362, 257, 2633, 22592, 47, 88, 2170, 309, 558, 13, 961, 311, 1520, 13, 1119, 300, 577, 291, 360, 309, 30], "temperature": 0.0, "avg_logprob": -0.22974731371952936, "compression_ratio": 1.2753623188405796, "no_speech_prob": 3.844918410322862e-06}, {"id": 861, "seek": 636104, "start": 6361.04, "end": 6372.96, "text": " I don't quite remember. We'll try. No. Okay. NumPy also doesn't. How interesting. What", "tokens": [286, 500, 380, 1596, 1604, 13, 492, 603, 853, 13, 883, 13, 1033, 13, 22592, 47, 88, 611, 1177, 380, 13, 1012, 1880, 13, 708], "temperature": 0.0, "avg_logprob": -0.18401617839418608, "compression_ratio": 1.0238095238095237, "no_speech_prob": 2.392269016127102e-05}, {"id": 862, "seek": 637296, "start": 6372.96, "end": 6396.92, "text": " about Python? Random. Oh, look at that. So Python does actually remember to reinitialize", "tokens": [466, 15329, 30, 37603, 13, 876, 11, 574, 412, 300, 13, 407, 15329, 775, 767, 1604, 281, 6561, 270, 831, 1125], "temperature": 0.0, "avg_logprob": -0.10415016174316406, "compression_ratio": 1.0476190476190477, "no_speech_prob": 8.315259378832707e-07}, {"id": 863, "seek": 639692, "start": 6396.92, "end": 6404.24, "text": " the random stream in each fork. So, you know, this is something that, like, even if you've", "tokens": [264, 4974, 4309, 294, 1184, 17716, 13, 407, 11, 291, 458, 11, 341, 307, 746, 300, 11, 411, 11, 754, 498, 291, 600], "temperature": 0.0, "avg_logprob": -0.1699866904868736, "compression_ratio": 1.722007722007722, "no_speech_prob": 3.668821364044561e-06}, {"id": 864, "seek": 639692, "start": 6404.24, "end": 6408.56, "text": " experimented in Python and you think everything's working well in your data loader or whatever,", "tokens": [5120, 292, 294, 15329, 293, 291, 519, 1203, 311, 1364, 731, 294, 428, 1412, 3677, 260, 420, 2035, 11], "temperature": 0.0, "avg_logprob": -0.1699866904868736, "compression_ratio": 1.722007722007722, "no_speech_prob": 3.668821364044561e-06}, {"id": 865, "seek": 639692, "start": 6408.56, "end": 6414.4400000000005, "text": " and then you switch to PyTorch or NumPy and now suddenly everything's broken. So this", "tokens": [293, 550, 291, 3679, 281, 9953, 51, 284, 339, 420, 22592, 47, 88, 293, 586, 5800, 1203, 311, 5463, 13, 407, 341], "temperature": 0.0, "avg_logprob": -0.1699866904868736, "compression_ratio": 1.722007722007722, "no_speech_prob": 3.668821364044561e-06}, {"id": 866, "seek": 639692, "start": 6414.4400000000005, "end": 6421.28, "text": " is why we've spent some time re-implementing the random number generator from scratch,", "tokens": [307, 983, 321, 600, 4418, 512, 565, 319, 12, 332, 43704, 278, 264, 4974, 1230, 19265, 490, 8459, 11], "temperature": 0.0, "avg_logprob": -0.1699866904868736, "compression_ratio": 1.722007722007722, "no_speech_prob": 3.668821364044561e-06}, {"id": 867, "seek": 639692, "start": 6421.28, "end": 6424.4400000000005, "text": " partly because it's fun and interesting and partly because it's important that you now", "tokens": [17031, 570, 309, 311, 1019, 293, 1880, 293, 17031, 570, 309, 311, 1021, 300, 291, 586], "temperature": 0.0, "avg_logprob": -0.1699866904868736, "compression_ratio": 1.722007722007722, "no_speech_prob": 3.668821364044561e-06}, {"id": 868, "seek": 642444, "start": 6424.44, "end": 6430.28, "text": " understand that when you're calling rand or any random number generator, kind of the default", "tokens": [1223, 300, 562, 291, 434, 5141, 367, 474, 420, 604, 4974, 1230, 19265, 11, 733, 295, 264, 7576], "temperature": 0.0, "avg_logprob": -0.10505754607064384, "compression_ratio": 1.5260115606936415, "no_speech_prob": 3.1875572403805563e-06}, {"id": 869, "seek": 642444, "start": 6430.28, "end": 6437.5199999999995, "text": " versions in NumPy and PyTorch, this global state is going to be copied. So you've got", "tokens": [9606, 294, 22592, 47, 88, 293, 9953, 51, 284, 339, 11, 341, 4338, 1785, 307, 516, 281, 312, 25365, 13, 407, 291, 600, 658], "temperature": 0.0, "avg_logprob": -0.10505754607064384, "compression_ratio": 1.5260115606936415, "no_speech_prob": 3.1875572403805563e-06}, {"id": 870, "seek": 642444, "start": 6437.5199999999995, "end": 6446.799999999999, "text": " to be a bit careful. Now I will mention our random number generator. Okay. So this is", "tokens": [281, 312, 257, 857, 5026, 13, 823, 286, 486, 2152, 527, 4974, 1230, 19265, 13, 1033, 13, 407, 341, 307], "temperature": 0.0, "avg_logprob": -0.10505754607064384, "compression_ratio": 1.5260115606936415, "no_speech_prob": 3.1875572403805563e-06}, {"id": 871, "seek": 644680, "start": 6446.8, "end": 6456.28, "text": " called... PercentTimeIt. Percent is a special Jupyter or IPython function. And PercentTimeIt", "tokens": [1219, 485, 3026, 2207, 22233, 3522, 13, 3026, 2207, 307, 257, 2121, 22125, 88, 391, 420, 8671, 88, 11943, 2445, 13, 400, 3026, 2207, 22233, 3522], "temperature": 0.0, "avg_logprob": -0.2438825916599583, "compression_ratio": 1.5084745762711864, "no_speech_prob": 8.530288141628262e-06}, {"id": 872, "seek": 644680, "start": 6456.28, "end": 6463.08, "text": " runs a piece of Python code this many times. So to call it 10 times. Well, actually it'll", "tokens": [6676, 257, 2522, 295, 15329, 3089, 341, 867, 1413, 13, 407, 281, 818, 309, 1266, 1413, 13, 1042, 11, 767, 309, 603], "temperature": 0.0, "avg_logprob": -0.2438825916599583, "compression_ratio": 1.5084745762711864, "no_speech_prob": 8.530288141628262e-06}, {"id": 873, "seek": 644680, "start": 6463.08, "end": 6466.4800000000005, "text": " do seven loops and each one will be seven times and it'll take the mean and standard", "tokens": [360, 3407, 16121, 293, 1184, 472, 486, 312, 3407, 1413, 293, 309, 603, 747, 264, 914, 293, 3832], "temperature": 0.0, "avg_logprob": -0.2438825916599583, "compression_ratio": 1.5084745762711864, "no_speech_prob": 8.530288141628262e-06}, {"id": 874, "seek": 646648, "start": 6466.48, "end": 6479.04, "text": " deviation. So here I am going to generate random numbers 7840 times and put them into", "tokens": [25163, 13, 407, 510, 286, 669, 516, 281, 8460, 4974, 3547, 26369, 5254, 1413, 293, 829, 552, 666], "temperature": 0.0, "avg_logprob": -0.1272926848867665, "compression_ratio": 1.300751879699248, "no_speech_prob": 3.1875511012913194e-06}, {"id": 875, "seek": 646648, "start": 6479.04, "end": 6493.32, "text": " 10 long chunks. And if I run that, it takes me three milliseconds per loop. If I run it", "tokens": [1266, 938, 24004, 13, 400, 498, 286, 1190, 300, 11, 309, 2516, 385, 1045, 34184, 680, 6367, 13, 759, 286, 1190, 309], "temperature": 0.0, "avg_logprob": -0.1272926848867665, "compression_ratio": 1.300751879699248, "no_speech_prob": 3.1875511012913194e-06}, {"id": 876, "seek": 649332, "start": 6493.32, "end": 6500.96, "text": " using PyTorch, this is the exact same thing in PyTorch, it's going to take me 73 microseconds", "tokens": [1228, 9953, 51, 284, 339, 11, 341, 307, 264, 1900, 912, 551, 294, 9953, 51, 284, 339, 11, 309, 311, 516, 281, 747, 385, 28387, 3123, 37841, 28750], "temperature": 0.0, "avg_logprob": -0.11660168305882868, "compression_ratio": 1.5726872246696035, "no_speech_prob": 2.3687928205617936e-06}, {"id": 877, "seek": 649332, "start": 6500.96, "end": 6507.16, "text": " per loop. So as you can see, although we could use our version, we're not going to because", "tokens": [680, 6367, 13, 407, 382, 291, 393, 536, 11, 4878, 321, 727, 764, 527, 3037, 11, 321, 434, 406, 516, 281, 570], "temperature": 0.0, "avg_logprob": -0.11660168305882868, "compression_ratio": 1.5726872246696035, "no_speech_prob": 2.3687928205617936e-06}, {"id": 878, "seek": 649332, "start": 6507.16, "end": 6512.88, "text": " the PyTorch version is much, much faster. This is how we can create a 784 by 10. And", "tokens": [264, 9953, 51, 284, 339, 3037, 307, 709, 11, 709, 4663, 13, 639, 307, 577, 321, 393, 1884, 257, 1614, 25494, 538, 1266, 13, 400], "temperature": 0.0, "avg_logprob": -0.11660168305882868, "compression_ratio": 1.5726872246696035, "no_speech_prob": 2.3687928205617936e-06}, {"id": 879, "seek": 649332, "start": 6512.88, "end": 6518.08, "text": " why would we want this? That's because this is our final layer of our neural net. Or if", "tokens": [983, 576, 321, 528, 341, 30, 663, 311, 570, 341, 307, 527, 2572, 4583, 295, 527, 18161, 2533, 13, 1610, 498], "temperature": 0.0, "avg_logprob": -0.11660168305882868, "compression_ratio": 1.5726872246696035, "no_speech_prob": 2.3687928205617936e-06}, {"id": 880, "seek": 651808, "start": 6518.08, "end": 6523.5599999999995, "text": " we're doing a linear classifier, our linear weights, we'll need to be 784 because that's", "tokens": [321, 434, 884, 257, 8213, 1508, 9902, 11, 527, 8213, 17443, 11, 321, 603, 643, 281, 312, 1614, 25494, 570, 300, 311], "temperature": 0.0, "avg_logprob": -0.18006910218132866, "compression_ratio": 1.5139664804469273, "no_speech_prob": 1.3081727274766308e-06}, {"id": 881, "seek": 651808, "start": 6523.5599999999995, "end": 6532.68, "text": " 28 by 28 by 10 because that's the number of possible outputs, the number of possible digits.", "tokens": [7562, 538, 7562, 538, 1266, 570, 300, 311, 264, 1230, 295, 1944, 23930, 11, 264, 1230, 295, 1944, 27011, 13], "temperature": 0.0, "avg_logprob": -0.18006910218132866, "compression_ratio": 1.5139664804469273, "no_speech_prob": 1.3081727274766308e-06}, {"id": 882, "seek": 651808, "start": 6532.68, "end": 6542.24, "text": " All right. That is it. So quite the intense lesson. I think we can all agree. Should keep", "tokens": [1057, 558, 13, 663, 307, 309, 13, 407, 1596, 264, 9447, 6898, 13, 286, 519, 321, 393, 439, 3986, 13, 6454, 1066], "temperature": 0.0, "avg_logprob": -0.18006910218132866, "compression_ratio": 1.5139664804469273, "no_speech_prob": 1.3081727274766308e-06}, {"id": 883, "seek": 654224, "start": 6542.24, "end": 6551.48, "text": " you busy for a week. And thanks very much for joining. And see you next time. Bye everybody.", "tokens": [50364, 291, 5856, 337, 257, 1243, 13, 400, 3231, 588, 709, 337, 5549, 13, 400, 536, 291, 958, 565, 13, 4621, 2201, 13, 50826], "temperature": 0.0, "avg_logprob": -0.14428696632385254, "compression_ratio": 1.15, "no_speech_prob": 1.4281426047091372e-05}], "language": "en"}