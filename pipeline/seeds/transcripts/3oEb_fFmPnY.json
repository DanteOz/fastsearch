{"text": " Get back to it Before ritual tells you about GRU's I've just got a couple of follow-ups from previous lessons the first is I Lied to you. I told you our Turkish classification model was 89% accurate actually. It's 91% accurate It turns out there was a bug in the fast AI library with parallel processing that was making it slightly wrong So if you rerun it with the latest fast AI you'll actually get 91 Which is even better and of course we didn't do the backward model for that so you could then ensemble that with a backward model To get better still One of the things that came out of the Translation was that the the translated sentences Even in the version with attention were Were not great in that they had like kind of repetition a lot of repetition and Part of the reason for that is Obviously we just we weren't using very much data And we weren't using very much compute and we were using quite a small model like Translations difficult the Google translation model has eight layers of LSTM's I think it's eight and They train on a huge corpus and they use a large portion of a data center to you know Do that for a long period of time? We intentionally picked a small subset of a fairly small corpus But part of the reason for the problem is The the way that we did generation So the the get predictions Lope if we go back and have a look at it. I moved on from Turkish to the follow up from the attention translation and And the thing that so Rachel had this get predictions Method and It just goes through one batch at a time and for each batch it goes through one sentence at a time and the key thing it does is for this is with The result of the model and the thing that came out of the model The last thing is the actual predictions and then we did an arg max to say which word has the highest probability right and That so this this is the kind of the the inference time Decoder that we use so this is what we would call a greedy search And a greedy search is actually not a good way to generate Text so we're not it in the bit of the lesson. I'm doing we're not going to retrain a model at all We're just going to say if you have a trained model How do you generate the best text you can and so this approach that we showed you here of using a greedy? Word by word approach is the the worst possible thing you can do and so to give you a sense of why? Think about this Here are the let's imagine that you've got a translation model, and it feeds you back the top three Choices are all pretty similar probabilities, and it could be there or at or some and then the second word It gives you back three possibilities, and they are you know high probability all that equal could be fluffy or big or hot and Then the third possibility all about equal could be dog or cat or pig right so in the greedy approach You would just pick the highest probability for each right so let's say the probabilities for each were like 0.4 point 2.2. They're all pretty similar. You know and maybe this was like point four point two point two and This was like point four point two point two right so the greedy version would say oh the answer which the our inference would be Some because that's the highest Fluffy because that's the highest pig right and then the rest of sentence But that doesn't make a Hello a lot of sense because You don't normally describe pigs as fluffy right so What you would Prefer to do is to kind of like figure out a path Through this if this is kind of like a graph right we can pick at each point Some node and then we can create an edge to the next part right so there's like Huge number of possible ways we could go through this graph right if there's a lot if there's a long sentence So there's you know three possibilities here and three possibilities here and three possibilities here And so forth rest of the sentence and of course in real life we have thousands of possibilities for each possible word So to solve Our way to solve this problem is to use something called beam search and beam search is how We normally do both people do generation for something like translation now what we do in beam search is we say okay? We're going to go through each stage, but we're not going to just pick one Possibility we're going to pick K possibilities. We're going to pick some number and that number K is called the beam width So if we picked like K of 2 right actually let's make these Not all equal So we can pick a K of 2 Then We would say okay after word at point time point one it could be the Or it could be some so they are two highest possibilities And then at the second point so then for each of the and some we would rerun our decoder and get a new probability of each of these things given that we start with the and each of these things given we start with some because remember that the RNM at each time step has this input both the Hidden state but also like what was the previous word you predicted so at this point You can't just say it's point four point three point two, but it actually depends on your previous word So based on those two words, maybe the two highest possibilities now at this point Some fluffy and the big Okay, and so what we do is we again we pick only the top K So K is two for us beam width two word strings that because we could it for each of the and some there are three possibilities So there'd be six possible next steps. We just keep the top two So we just say okay the you know at this point with either the big or some fluffy And then again we do the same thing for step three right so for each of these two Possibilities there's three words we can add on and so for each of these possibilities We again the decoder has Different probabilities for each of those based on each of those possible inputs And so we end up and then we just select out of those two times three is six again the top two three word strings by probability so in this case the top might be The big dog Versus oops Some fluffy Cat and So forth right so this works out nicely because if the next kind of three words Turned out to be with high probability bit the postman Right then this approach allows us to identify that like oh, it's probably the big dog Bit the postman, so it's it's only like partially greedy now. Okay, so it's a it's a greedy approach, but we we're keeping a beam width number of options all the way through Coding this is a bit of a pain in the ass, so I'm not going to bother besides which fast AI already has it but if you're interested you can go to fast AI and Search for beam say and Find beam search, and then you can click source code And There it is So it's kind of let's see One and a half screens, so it's not lots And but it's just basically that doing that loop and selecting the top K at each sides Time step and here's that beam width sorry beam size you can pass in Okay, so that's the kind of normal approach for Decoding the translation model, and it's a good approach for kind of decoding any kind of text where you have a Something kind of pointing you as to which way to go so with translation at every step There's like something saying like this word is probably that or that this word is probably that or that Which is very different to the GPT-2 Unicorn example we saw remember the GPT-2 unicorn example was like Here's a paragraph of English text now What might the rest of it be and like that's got no guidance? It's not translating something and that kind of text generation is very different There was a paper Let's find it, so I'm now looking at the 7b sector sec nucleus Notebook Notebook which has a link to this paper called the curious case of neural text generation which is less than two months old, so this is quite a recent paper this came from the Allen AI Institute also known as AI 2 and They were they pointed out something really fascinating and this is like a pretty easy Read so for people who are just starting to get into reading academic papers. This is a good good one to check out They did something interesting which is they said what happens if we take the GPT-2 model and we do the unicorns prompt exactly the same as in their blog post and we use beam search and the answer is they get the continuation Professor of the Department of your sangelis the most important place the world to be recognition of the world to be of the world to be Of the world to be of the world Not great and so it's interesting they actually point out that beam search seems to have problems so for GPT-2 What they did was they did what's cat called top case sampling and so top case sampling? Is this I will show you So if you look at 7b sector sec nucleus what I've done Oh By the way if you ever see things like this torch dot kuda dot set device in our notebooks or anywhere That's just means that I was running something on a machine with more than one GPU And I wanted to pick which GPU I wanted if you only have one GPU that will give an error So you should just delete it So what I did was I didn't want to have all that Cells and cells of code to rerun every time and copy it from other notebooks So I just pasted in pasted it into a sector sec dot PI Module just like we talked about the other day just makes life easier So all the stuff from Rachel and Sylvain's notebook you can see there So the one thing I did copy from the notebook was the sector sec RNN with attention So that's exactly the same like I say, we're not training a model any differently And what I did was I trained a model for only five epochs to make it like particularly crappy Because I wanted to show you the differences When you're kind of working with a crappy model And so I took The get predictions Method function that we just looked at With that, you know arg max thing, but I also added in the actual raw Activations the raw probabilities so rather than giving us back three things that reconstructed X Y Z It also gives us the pure probabilities and so that lets us now write our own decoders our own inference decoders so then I wrote a Top K decoder so it's only three lines of code and so we just Grab the output activations We throw them through a soft max. I'm to turn them into probabilities and then we simply call the pie torch top K method which will just give me the top K highest probabilities giving me both the probabilities and their indexes and then I So top case sampling just randomly picks one of those top K with Like literally just a random integer So that's all top case sampling is so if you want to use top case sampling you need to then put it into a Decoder so what I did then was I literally just copied and pasted the decode method and put it into a new function and Then I replaced the line which says In the decoder I replaced the line that says Dot max and I replaced it with a line that says Select top K Okay, so that would give us top case sampling which is this which is the top case sampling this would be the same as GPT-2 and So which is not bad so this paper asked the question though like Well, we thought beam search was meant to be the best way to do inference. Why is beam search so terrible? Why is it creating so much repetition as we saw in the examples Rachel showed last week and what they found was interesting They looked at a few examples of sentences Such as this one which is actual human generated text, right? Are you cold? He asked his voice full of concern. I just shrugged and squeezed my eyes shut I saw Kojas blowing eyes and sword and what they did is for each time step for each word They said in a language model What's the probability of the word the human picked and what they found is sometimes as you expect? The probability is right up close to one. The blue is the human text But very often the the human is picking extremely unlikely words for example Kojas, I've never even heard that name before Where else beam search by definition? This is the probability of beam search beam search picks a very very likely Thing at every point. So here's the beam search generation who looks at the clouds who looks at the clouds He looks at the clouds. He looks at the cloud. Okay, so Actually beam search it turns out generates Very likely per time step speech but very unlikely distributions So this paper Makes a suggestion as to how we avoid that and it's basically to use So they have a number of sections which describe like why does beam search Have this problem, but what they then do which is the main bit I'm interested in is how do we fix it? And They propose something they call nuclear sampling and nuclear sampling is Basically top-k sampling but they point out something problematic about top-k sampling which is if you're doing top-k sampling with K of 5 say and your language model or translation model says I am 99% sure The next token in this sentence is a full stop Then you shouldn't do top-k sampling at that point, right? You shouldn't pick options 2 3 4 or 5 because option 1 is almost certain All right, so top-k sampling Has its own problems So they suggest something which is basically a top-k sampling with a dynamic K so changing the value of K each time and specifically what they do is they say let's do top-k sampling But we will pick K such that the total probability of those K words is Something and in that case they pick point nine So they basically so they call it top-p sampling So they say let's pick a K so that the words we choose from have a probability of point nine So in the case that there was one word that already has a 0.99 probability Then K is 1 So we just pick that one every time in the case where our model has no idea what the next word is and it's got like 20 things that all have like a point oh two probability and they're they're the top then it might be a top You know K of 20 So here is So now we've taken select top K and we turn it into select nucleus That's what they call this algorithm and so it's interesting as it's often the case when you implement a paper In code the thing you implement is often just three or four lines of code so in this case we have seven lines of code and the step one again is do a softmax and then step two is get the indexes which sort the the probabilities because we want to kind of do a cumulative sum to find how many K's we need and Then just go through each one calculating the cumulative sum of the probabilities and when your cumulative sum is above P Then pick a random choice of the ones you've seen so far And that's it that is nuclear sampling and so If we pick the greedy sampling that we used in the previous Neural translation lesson with our you know, particularly crappy five epoch model and we try to translate something which is meant to be What gaps remain in our knowledge of unknown on which future research should focus the translation is? for greedy so kind of top K where K is 1 What gaps are needed in our work? And what is the research of the work at what research will be in place to future? You can see like the crappy translation model has this You know this repetition problem If instead we switch out the greedy and we're put in nucleus and then call Predict with decode which as you can see just posted into eval no grad calls that decode And then returns the reconstructed output we get what gaps are needed in our understanding of work and security and how? Research will need to be put in place. So like it's a much less repetition better sounding sentence So I kind of just wanted to mention this I'm not sure like partly because after last class I Asked the students if anyone is interested in pair programming an algorithm paper, and so we spent the afternoon a couple of hours I guess it was implementing this so wanted to kind of show you What that looks like give you an example of a paper that's worth looking at but also just have a kind of discussion about generation in the translation Notebook so far. We've only talked about how do you train the model? We didn't talk about actually how do you generate? text and so these techniques of beam search and top K and nucleus top P sampling are the three that you probably need to know about So that is that so you may have already said this is there a way to I guess generate text or? If you're generating text without those and just taking the top probability and Do you end up with the repetition Yes, we saw that last way. Yeah. Yeah, I mean some of that I Guess how much does that relate to I know the temperature of like? Kind of the how much randomness you injected each step how much can that help with it? Yeah, so I guess that's a Another way you can do it is doing multinomial sampling where you select From the full set with a probability based on the p values of the softmax I I Think again you kind of end up with the problem that the nucleus paper pointed out around like you're going to end up Picking overly high probability things too often But you know I guess as you can see this is only a two months old paper, so I would describe this as a open area of research and These are some current best practices Now I would say that the translation Probably beam search is generally still going to be best because it's this like you know it's not at all like open-ended generation And it's only in this case the only reason it's we kind of needed it to look any good Was because we have such a crappy translation model because we trained it so quickly on so little data I'm not sure anybody's tried nucleus sampling on high quality Translation models it would be interesting to find out but The other place beam search is used all the time is like speech recognition It's a similar thing right speech recognition is basically translating audio into words You use the same kinds of approaches the same kind of language you model But you're trying to find something that the words match the the audio, but also makes sense as a kind of complete end-to-end sentence Oh, thank you", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 2.0, "text": " Get back to it", "tokens": [3240, 646, 281, 309], "temperature": 0.0, "avg_logprob": -0.3625785034972352, "compression_ratio": 1.39, "no_speech_prob": 0.08137144148349762}, {"id": 1, "seek": 0, "start": 4.5, "end": 6.74, "text": " Before ritual tells you about", "tokens": [4546, 13792, 5112, 291, 466], "temperature": 0.0, "avg_logprob": -0.3625785034972352, "compression_ratio": 1.39, "no_speech_prob": 0.08137144148349762}, {"id": 2, "seek": 0, "start": 8.36, "end": 13.92, "text": " GRU's I've just got a couple of follow-ups from previous lessons the first is", "tokens": [10903, 52, 311, 286, 600, 445, 658, 257, 1916, 295, 1524, 12, 7528, 490, 3894, 8820, 264, 700, 307], "temperature": 0.0, "avg_logprob": -0.3625785034972352, "compression_ratio": 1.39, "no_speech_prob": 0.08137144148349762}, {"id": 3, "seek": 0, "start": 16.080000000000002, "end": 18.080000000000002, "text": " I", "tokens": [286], "temperature": 0.0, "avg_logprob": -0.3625785034972352, "compression_ratio": 1.39, "no_speech_prob": 0.08137144148349762}, {"id": 4, "seek": 0, "start": 18.16, "end": 25.740000000000002, "text": " Lied to you. I told you our Turkish classification model was 89% accurate actually. It's 91% accurate", "tokens": [441, 1091, 281, 291, 13, 286, 1907, 291, 527, 18565, 21538, 2316, 390, 31877, 4, 8559, 767, 13, 467, 311, 31064, 4, 8559], "temperature": 0.0, "avg_logprob": -0.3625785034972352, "compression_ratio": 1.39, "no_speech_prob": 0.08137144148349762}, {"id": 5, "seek": 2574, "start": 25.74, "end": 32.42, "text": " It turns out there was a bug in the fast AI library with parallel processing that was making it slightly wrong", "tokens": [467, 4523, 484, 456, 390, 257, 7426, 294, 264, 2370, 7318, 6405, 365, 8952, 9007, 300, 390, 1455, 309, 4748, 2085], "temperature": 0.0, "avg_logprob": -0.18931394815444946, "compression_ratio": 1.6666666666666667, "no_speech_prob": 4.8318102926714346e-05}, {"id": 6, "seek": 2574, "start": 32.739999999999995, "end": 36.22, "text": " So if you rerun it with the latest fast AI you'll actually get", "tokens": [407, 498, 291, 43819, 409, 309, 365, 264, 6792, 2370, 7318, 291, 603, 767, 483], "temperature": 0.0, "avg_logprob": -0.18931394815444946, "compression_ratio": 1.6666666666666667, "no_speech_prob": 4.8318102926714346e-05}, {"id": 7, "seek": 2574, "start": 36.86, "end": 38.339999999999996, "text": " 91", "tokens": [31064], "temperature": 0.0, "avg_logprob": -0.18931394815444946, "compression_ratio": 1.6666666666666667, "no_speech_prob": 4.8318102926714346e-05}, {"id": 8, "seek": 2574, "start": 38.339999999999996, "end": 44.76, "text": " Which is even better and of course we didn't do the backward model for that so you could then ensemble that with a backward model", "tokens": [3013, 307, 754, 1101, 293, 295, 1164, 321, 994, 380, 360, 264, 23897, 2316, 337, 300, 370, 291, 727, 550, 19492, 300, 365, 257, 23897, 2316], "temperature": 0.0, "avg_logprob": -0.18931394815444946, "compression_ratio": 1.6666666666666667, "no_speech_prob": 4.8318102926714346e-05}, {"id": 9, "seek": 2574, "start": 44.76, "end": 46.76, "text": " To get better still", "tokens": [1407, 483, 1101, 920], "temperature": 0.0, "avg_logprob": -0.18931394815444946, "compression_ratio": 1.6666666666666667, "no_speech_prob": 4.8318102926714346e-05}, {"id": 10, "seek": 2574, "start": 48.18, "end": 51.3, "text": " One of the things that came out of the", "tokens": [1485, 295, 264, 721, 300, 1361, 484, 295, 264], "temperature": 0.0, "avg_logprob": -0.18931394815444946, "compression_ratio": 1.6666666666666667, "no_speech_prob": 4.8318102926714346e-05}, {"id": 11, "seek": 5130, "start": 51.3, "end": 58.28, "text": " Translation was that the the translated sentences", "tokens": [6531, 24278, 390, 300, 264, 264, 16805, 16579], "temperature": 0.0, "avg_logprob": -0.2682090103626251, "compression_ratio": 1.6025641025641026, "no_speech_prob": 2.2826012354926206e-05}, {"id": 12, "seek": 5130, "start": 60.379999999999995, "end": 62.379999999999995, "text": " Even in the version with attention", "tokens": [2754, 294, 264, 3037, 365, 3202], "temperature": 0.0, "avg_logprob": -0.2682090103626251, "compression_ratio": 1.6025641025641026, "no_speech_prob": 2.2826012354926206e-05}, {"id": 13, "seek": 5130, "start": 63.22, "end": 64.5, "text": " were", "tokens": [645], "temperature": 0.0, "avg_logprob": -0.2682090103626251, "compression_ratio": 1.6025641025641026, "no_speech_prob": 2.2826012354926206e-05}, {"id": 14, "seek": 5130, "start": 64.5, "end": 66.5, "text": " Were not great in that they", "tokens": [12448, 406, 869, 294, 300, 436], "temperature": 0.0, "avg_logprob": -0.2682090103626251, "compression_ratio": 1.6025641025641026, "no_speech_prob": 2.2826012354926206e-05}, {"id": 15, "seek": 5130, "start": 67.25999999999999, "end": 70.82, "text": " had like kind of repetition a lot of repetition and", "tokens": [632, 411, 733, 295, 30432, 257, 688, 295, 30432, 293], "temperature": 0.0, "avg_logprob": -0.2682090103626251, "compression_ratio": 1.6025641025641026, "no_speech_prob": 2.2826012354926206e-05}, {"id": 16, "seek": 5130, "start": 73.5, "end": 75.5, "text": " Part of the reason for that is", "tokens": [4100, 295, 264, 1778, 337, 300, 307], "temperature": 0.0, "avg_logprob": -0.2682090103626251, "compression_ratio": 1.6025641025641026, "no_speech_prob": 2.2826012354926206e-05}, {"id": 17, "seek": 5130, "start": 75.9, "end": 77.94, "text": " Obviously we just we weren't using very much data", "tokens": [7580, 321, 445, 321, 4999, 380, 1228, 588, 709, 1412], "temperature": 0.0, "avg_logprob": -0.2682090103626251, "compression_ratio": 1.6025641025641026, "no_speech_prob": 2.2826012354926206e-05}, {"id": 18, "seek": 7794, "start": 77.94, "end": 81.66, "text": " And we weren't using very much compute and we were using quite a small model like", "tokens": [400, 321, 4999, 380, 1228, 588, 709, 14722, 293, 321, 645, 1228, 1596, 257, 1359, 2316, 411], "temperature": 0.0, "avg_logprob": -0.19483371575673422, "compression_ratio": 1.6069868995633187, "no_speech_prob": 1.7777932953322306e-05}, {"id": 19, "seek": 7794, "start": 82.3, "end": 87.48, "text": " Translations difficult the Google translation model has eight layers of", "tokens": [6531, 75, 763, 2252, 264, 3329, 12853, 2316, 575, 3180, 7914, 295], "temperature": 0.0, "avg_logprob": -0.19483371575673422, "compression_ratio": 1.6069868995633187, "no_speech_prob": 1.7777932953322306e-05}, {"id": 20, "seek": 7794, "start": 88.38, "end": 90.38, "text": " LSTM's I think it's eight", "tokens": [441, 6840, 44, 311, 286, 519, 309, 311, 3180], "temperature": 0.0, "avg_logprob": -0.19483371575673422, "compression_ratio": 1.6069868995633187, "no_speech_prob": 1.7777932953322306e-05}, {"id": 21, "seek": 7794, "start": 90.7, "end": 92.22, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.19483371575673422, "compression_ratio": 1.6069868995633187, "no_speech_prob": 1.7777932953322306e-05}, {"id": 22, "seek": 7794, "start": 92.22, "end": 98.1, "text": " They train on a huge corpus and they use a large portion of a data center to you know", "tokens": [814, 3847, 322, 257, 2603, 1181, 31624, 293, 436, 764, 257, 2416, 8044, 295, 257, 1412, 3056, 281, 291, 458], "temperature": 0.0, "avg_logprob": -0.19483371575673422, "compression_ratio": 1.6069868995633187, "no_speech_prob": 1.7777932953322306e-05}, {"id": 23, "seek": 7794, "start": 98.58, "end": 100.58, "text": " Do that for a long period of time?", "tokens": [1144, 300, 337, 257, 938, 2896, 295, 565, 30], "temperature": 0.0, "avg_logprob": -0.19483371575673422, "compression_ratio": 1.6069868995633187, "no_speech_prob": 1.7777932953322306e-05}, {"id": 24, "seek": 7794, "start": 101.06, "end": 105.68, "text": " We intentionally picked a small subset of a fairly small corpus", "tokens": [492, 22062, 6183, 257, 1359, 25993, 295, 257, 6457, 1359, 1181, 31624], "temperature": 0.0, "avg_logprob": -0.19483371575673422, "compression_ratio": 1.6069868995633187, "no_speech_prob": 1.7777932953322306e-05}, {"id": 25, "seek": 10568, "start": 105.68, "end": 108.2, "text": " But part of the reason for the problem is", "tokens": [583, 644, 295, 264, 1778, 337, 264, 1154, 307], "temperature": 0.0, "avg_logprob": -0.29291613442557196, "compression_ratio": 1.5755813953488371, "no_speech_prob": 6.643180768151069e-06}, {"id": 26, "seek": 10568, "start": 109.12, "end": 111.46000000000001, "text": " The the way that we did generation", "tokens": [440, 264, 636, 300, 321, 630, 5125], "temperature": 0.0, "avg_logprob": -0.29291613442557196, "compression_ratio": 1.5755813953488371, "no_speech_prob": 6.643180768151069e-06}, {"id": 27, "seek": 10568, "start": 112.0, "end": 114.4, "text": " So the the get predictions", "tokens": [407, 264, 264, 483, 21264], "temperature": 0.0, "avg_logprob": -0.29291613442557196, "compression_ratio": 1.5755813953488371, "no_speech_prob": 6.643180768151069e-06}, {"id": 28, "seek": 10568, "start": 115.92, "end": 121.32000000000001, "text": " Lope if we go back and have a look at it. I moved on from Turkish to the", "tokens": [441, 1114, 498, 321, 352, 646, 293, 362, 257, 574, 412, 309, 13, 286, 4259, 322, 490, 18565, 281, 264], "temperature": 0.0, "avg_logprob": -0.29291613442557196, "compression_ratio": 1.5755813953488371, "no_speech_prob": 6.643180768151069e-06}, {"id": 29, "seek": 10568, "start": 122.24000000000001, "end": 125.2, "text": " follow up from the attention translation and", "tokens": [1524, 493, 490, 264, 3202, 12853, 293], "temperature": 0.0, "avg_logprob": -0.29291613442557196, "compression_ratio": 1.5755813953488371, "no_speech_prob": 6.643180768151069e-06}, {"id": 30, "seek": 12520, "start": 125.2, "end": 133.64000000000001, "text": " And the thing that so Rachel had this get predictions", "tokens": [400, 264, 551, 300, 370, 14246, 632, 341, 483, 21264], "temperature": 0.0, "avg_logprob": -0.1940626090681049, "compression_ratio": 1.767741935483871, "no_speech_prob": 2.5463776182732545e-05}, {"id": 31, "seek": 12520, "start": 135.72, "end": 137.44, "text": " Method and", "tokens": [25285, 293], "temperature": 0.0, "avg_logprob": -0.1940626090681049, "compression_ratio": 1.767741935483871, "no_speech_prob": 2.5463776182732545e-05}, {"id": 32, "seek": 12520, "start": 137.44, "end": 144.1, "text": " It just goes through one batch at a time and for each batch it goes through one sentence at a time and the key thing", "tokens": [467, 445, 1709, 807, 472, 15245, 412, 257, 565, 293, 337, 1184, 15245, 309, 1709, 807, 472, 8174, 412, 257, 565, 293, 264, 2141, 551], "temperature": 0.0, "avg_logprob": -0.1940626090681049, "compression_ratio": 1.767741935483871, "no_speech_prob": 2.5463776182732545e-05}, {"id": 33, "seek": 12520, "start": 144.1, "end": 146.1, "text": " it does is", "tokens": [309, 775, 307], "temperature": 0.0, "avg_logprob": -0.1940626090681049, "compression_ratio": 1.767741935483871, "no_speech_prob": 2.5463776182732545e-05}, {"id": 34, "seek": 12520, "start": 146.32, "end": 148.32, "text": " for this is with", "tokens": [337, 341, 307, 365], "temperature": 0.0, "avg_logprob": -0.1940626090681049, "compression_ratio": 1.767741935483871, "no_speech_prob": 2.5463776182732545e-05}, {"id": 35, "seek": 14832, "start": 148.32, "end": 154.35999999999999, "text": " The result of the model and the thing that came out of the model", "tokens": [440, 1874, 295, 264, 2316, 293, 264, 551, 300, 1361, 484, 295, 264, 2316], "temperature": 0.0, "avg_logprob": -0.21802166530064174, "compression_ratio": 1.7032967032967032, "no_speech_prob": 9.818056241783779e-06}, {"id": 36, "seek": 14832, "start": 156.23999999999998, "end": 163.16, "text": " The last thing is the actual predictions and then we did an arg max to say which word has the highest probability", "tokens": [440, 1036, 551, 307, 264, 3539, 21264, 293, 550, 321, 630, 364, 3882, 11469, 281, 584, 597, 1349, 575, 264, 6343, 8482], "temperature": 0.0, "avg_logprob": -0.21802166530064174, "compression_ratio": 1.7032967032967032, "no_speech_prob": 9.818056241783779e-06}, {"id": 37, "seek": 14832, "start": 164.0, "end": 166.0, "text": " right and", "tokens": [558, 293], "temperature": 0.0, "avg_logprob": -0.21802166530064174, "compression_ratio": 1.7032967032967032, "no_speech_prob": 9.818056241783779e-06}, {"id": 38, "seek": 14832, "start": 167.88, "end": 172.72, "text": " That so this this is the kind of the the inference time", "tokens": [663, 370, 341, 341, 307, 264, 733, 295, 264, 264, 38253, 565], "temperature": 0.0, "avg_logprob": -0.21802166530064174, "compression_ratio": 1.7032967032967032, "no_speech_prob": 9.818056241783779e-06}, {"id": 39, "seek": 14832, "start": 173.56, "end": 177.28, "text": " Decoder that we use so this is what we would call a greedy search", "tokens": [12427, 19866, 300, 321, 764, 370, 341, 307, 437, 321, 576, 818, 257, 28228, 3164], "temperature": 0.0, "avg_logprob": -0.21802166530064174, "compression_ratio": 1.7032967032967032, "no_speech_prob": 9.818056241783779e-06}, {"id": 40, "seek": 17728, "start": 177.28, "end": 183.16, "text": " And a greedy search is actually not a good way to generate", "tokens": [400, 257, 28228, 3164, 307, 767, 406, 257, 665, 636, 281, 8460], "temperature": 0.0, "avg_logprob": -0.1375450954258999, "compression_ratio": 1.7542372881355932, "no_speech_prob": 8.013251317606773e-06}, {"id": 41, "seek": 17728, "start": 183.88, "end": 190.44, "text": " Text so we're not it in the bit of the lesson. I'm doing we're not going to retrain a model at all", "tokens": [18643, 370, 321, 434, 406, 309, 294, 264, 857, 295, 264, 6898, 13, 286, 478, 884, 321, 434, 406, 516, 281, 1533, 7146, 257, 2316, 412, 439], "temperature": 0.0, "avg_logprob": -0.1375450954258999, "compression_ratio": 1.7542372881355932, "no_speech_prob": 8.013251317606773e-06}, {"id": 42, "seek": 17728, "start": 190.56, "end": 192.84, "text": " We're just going to say if you have a trained model", "tokens": [492, 434, 445, 516, 281, 584, 498, 291, 362, 257, 8895, 2316], "temperature": 0.0, "avg_logprob": -0.1375450954258999, "compression_ratio": 1.7542372881355932, "no_speech_prob": 8.013251317606773e-06}, {"id": 43, "seek": 17728, "start": 192.88, "end": 199.68, "text": " How do you generate the best text you can and so this approach that we showed you here of using a greedy?", "tokens": [1012, 360, 291, 8460, 264, 1151, 2487, 291, 393, 293, 370, 341, 3109, 300, 321, 4712, 291, 510, 295, 1228, 257, 28228, 30], "temperature": 0.0, "avg_logprob": -0.1375450954258999, "compression_ratio": 1.7542372881355932, "no_speech_prob": 8.013251317606773e-06}, {"id": 44, "seek": 19968, "start": 199.68, "end": 207.12, "text": " Word by word approach is the the worst possible thing you can do and so to give you a sense of why?", "tokens": [8725, 538, 1349, 3109, 307, 264, 264, 5855, 1944, 551, 291, 393, 360, 293, 370, 281, 976, 291, 257, 2020, 295, 983, 30], "temperature": 0.0, "avg_logprob": -0.1821058481046469, "compression_ratio": 1.7489878542510122, "no_speech_prob": 3.2377010938944295e-06}, {"id": 45, "seek": 19968, "start": 208.64000000000001, "end": 210.64000000000001, "text": " Think about this", "tokens": [6557, 466, 341], "temperature": 0.0, "avg_logprob": -0.1821058481046469, "compression_ratio": 1.7489878542510122, "no_speech_prob": 3.2377010938944295e-06}, {"id": 46, "seek": 19968, "start": 211.16, "end": 217.60000000000002, "text": " Here are the let's imagine that you've got a translation model, and it feeds you back the top three", "tokens": [1692, 366, 264, 718, 311, 3811, 300, 291, 600, 658, 257, 12853, 2316, 11, 293, 309, 23712, 291, 646, 264, 1192, 1045], "temperature": 0.0, "avg_logprob": -0.1821058481046469, "compression_ratio": 1.7489878542510122, "no_speech_prob": 3.2377010938944295e-06}, {"id": 47, "seek": 19968, "start": 218.20000000000002, "end": 223.64000000000001, "text": " Choices are all pretty similar probabilities, and it could be there or at or some and then the second word", "tokens": [12366, 1473, 366, 439, 1238, 2531, 33783, 11, 293, 309, 727, 312, 456, 420, 412, 420, 512, 293, 550, 264, 1150, 1349], "temperature": 0.0, "avg_logprob": -0.1821058481046469, "compression_ratio": 1.7489878542510122, "no_speech_prob": 3.2377010938944295e-06}, {"id": 48, "seek": 22364, "start": 223.64, "end": 230.83999999999997, "text": " It gives you back three possibilities, and they are you know high probability all that equal could be fluffy or big or hot and", "tokens": [467, 2709, 291, 646, 1045, 12178, 11, 293, 436, 366, 291, 458, 1090, 8482, 439, 300, 2681, 727, 312, 22778, 420, 955, 420, 2368, 293], "temperature": 0.0, "avg_logprob": -0.1994194079049026, "compression_ratio": 1.6926829268292682, "no_speech_prob": 3.340476268931525e-06}, {"id": 49, "seek": 22364, "start": 231.2, "end": 237.72, "text": " Then the third possibility all about equal could be dog or cat or pig right so in the greedy approach", "tokens": [1396, 264, 2636, 7959, 439, 466, 2681, 727, 312, 3000, 420, 3857, 420, 8120, 558, 370, 294, 264, 28228, 3109], "temperature": 0.0, "avg_logprob": -0.1994194079049026, "compression_ratio": 1.6926829268292682, "no_speech_prob": 3.340476268931525e-06}, {"id": 50, "seek": 22364, "start": 237.72, "end": 244.32, "text": " You would just pick the highest probability for each right so let's say the probabilities for each were like", "tokens": [509, 576, 445, 1888, 264, 6343, 8482, 337, 1184, 558, 370, 718, 311, 584, 264, 33783, 337, 1184, 645, 411], "temperature": 0.0, "avg_logprob": -0.1994194079049026, "compression_ratio": 1.6926829268292682, "no_speech_prob": 3.340476268931525e-06}, {"id": 51, "seek": 22364, "start": 246.2, "end": 248.2, "text": " 0.4 point", "tokens": [1958, 13, 19, 935], "temperature": 0.0, "avg_logprob": -0.1994194079049026, "compression_ratio": 1.6926829268292682, "no_speech_prob": 3.340476268931525e-06}, {"id": 52, "seek": 24820, "start": 248.2, "end": 255.6, "text": " 2.2. They're all pretty similar. You know and maybe this was like point four", "tokens": [568, 13, 17, 13, 814, 434, 439, 1238, 2531, 13, 509, 458, 293, 1310, 341, 390, 411, 935, 1451], "temperature": 0.0, "avg_logprob": -0.3214503330970878, "compression_ratio": 1.7124183006535947, "no_speech_prob": 3.611945203374489e-06}, {"id": 53, "seek": 24820, "start": 256.52, "end": 258.71999999999997, "text": " point two point two and", "tokens": [935, 732, 935, 732, 293], "temperature": 0.0, "avg_logprob": -0.3214503330970878, "compression_ratio": 1.7124183006535947, "no_speech_prob": 3.611945203374489e-06}, {"id": 54, "seek": 24820, "start": 259.44, "end": 261.32, "text": " This was like", "tokens": [639, 390, 411], "temperature": 0.0, "avg_logprob": -0.3214503330970878, "compression_ratio": 1.7124183006535947, "no_speech_prob": 3.611945203374489e-06}, {"id": 55, "seek": 24820, "start": 261.32, "end": 270.2, "text": " point four point two point two right so the greedy version would say oh the answer which the our inference would be", "tokens": [935, 1451, 935, 732, 935, 732, 558, 370, 264, 28228, 3037, 576, 584, 1954, 264, 1867, 597, 264, 527, 38253, 576, 312], "temperature": 0.0, "avg_logprob": -0.3214503330970878, "compression_ratio": 1.7124183006535947, "no_speech_prob": 3.611945203374489e-06}, {"id": 56, "seek": 24820, "start": 270.71999999999997, "end": 272.71999999999997, "text": " Some because that's the highest", "tokens": [2188, 570, 300, 311, 264, 6343], "temperature": 0.0, "avg_logprob": -0.3214503330970878, "compression_ratio": 1.7124183006535947, "no_speech_prob": 3.611945203374489e-06}, {"id": 57, "seek": 27272, "start": 272.72, "end": 278.24, "text": " Fluffy because that's the highest pig right and then the rest of sentence", "tokens": [3235, 14297, 570, 300, 311, 264, 6343, 8120, 558, 293, 550, 264, 1472, 295, 8174], "temperature": 0.0, "avg_logprob": -0.25206596263940784, "compression_ratio": 1.5, "no_speech_prob": 1.3419620700005908e-05}, {"id": 58, "seek": 27272, "start": 279.68, "end": 281.68, "text": " But that doesn't make a", "tokens": [583, 300, 1177, 380, 652, 257], "temperature": 0.0, "avg_logprob": -0.25206596263940784, "compression_ratio": 1.5, "no_speech_prob": 1.3419620700005908e-05}, {"id": 59, "seek": 27272, "start": 282.16, "end": 284.16, "text": " Hello a lot of sense", "tokens": [2425, 257, 688, 295, 2020], "temperature": 0.0, "avg_logprob": -0.25206596263940784, "compression_ratio": 1.5, "no_speech_prob": 1.3419620700005908e-05}, {"id": 60, "seek": 27272, "start": 284.40000000000003, "end": 286.40000000000003, "text": " because", "tokens": [570], "temperature": 0.0, "avg_logprob": -0.25206596263940784, "compression_ratio": 1.5, "no_speech_prob": 1.3419620700005908e-05}, {"id": 61, "seek": 27272, "start": 286.40000000000003, "end": 288.40000000000003, "text": " You don't normally describe", "tokens": [509, 500, 380, 5646, 6786], "temperature": 0.0, "avg_logprob": -0.25206596263940784, "compression_ratio": 1.5, "no_speech_prob": 1.3419620700005908e-05}, {"id": 62, "seek": 27272, "start": 288.56, "end": 290.8, "text": " pigs as fluffy right so", "tokens": [24380, 382, 22778, 558, 370], "temperature": 0.0, "avg_logprob": -0.25206596263940784, "compression_ratio": 1.5, "no_speech_prob": 1.3419620700005908e-05}, {"id": 63, "seek": 27272, "start": 294.0, "end": 296.0, "text": " What you would", "tokens": [708, 291, 576], "temperature": 0.0, "avg_logprob": -0.25206596263940784, "compression_ratio": 1.5, "no_speech_prob": 1.3419620700005908e-05}, {"id": 64, "seek": 29600, "start": 296.0, "end": 301.88, "text": " Prefer to do is to kind of like figure out a path", "tokens": [48401, 281, 360, 307, 281, 733, 295, 411, 2573, 484, 257, 3100], "temperature": 0.0, "avg_logprob": -0.17702713791204958, "compression_ratio": 2.033980582524272, "no_speech_prob": 2.2252536382438848e-06}, {"id": 65, "seek": 29600, "start": 302.76, "end": 306.44, "text": " Through this if this is kind of like a graph right we can pick at each point", "tokens": [8927, 341, 498, 341, 307, 733, 295, 411, 257, 4295, 558, 321, 393, 1888, 412, 1184, 935], "temperature": 0.0, "avg_logprob": -0.17702713791204958, "compression_ratio": 2.033980582524272, "no_speech_prob": 2.2252536382438848e-06}, {"id": 66, "seek": 29600, "start": 307.0, "end": 310.82, "text": " Some node and then we can create an edge to the next part right so there's like", "tokens": [2188, 9984, 293, 550, 321, 393, 1884, 364, 4691, 281, 264, 958, 644, 558, 370, 456, 311, 411], "temperature": 0.0, "avg_logprob": -0.17702713791204958, "compression_ratio": 2.033980582524272, "no_speech_prob": 2.2252536382438848e-06}, {"id": 67, "seek": 29600, "start": 315.12, "end": 321.36, "text": " Huge number of possible ways we could go through this graph right if there's a lot if there's a long sentence", "tokens": [37043, 1230, 295, 1944, 2098, 321, 727, 352, 807, 341, 4295, 558, 498, 456, 311, 257, 688, 498, 456, 311, 257, 938, 8174], "temperature": 0.0, "avg_logprob": -0.17702713791204958, "compression_ratio": 2.033980582524272, "no_speech_prob": 2.2252536382438848e-06}, {"id": 68, "seek": 29600, "start": 321.36, "end": 325.68, "text": " So there's you know three possibilities here and three possibilities here and three possibilities here", "tokens": [407, 456, 311, 291, 458, 1045, 12178, 510, 293, 1045, 12178, 510, 293, 1045, 12178, 510], "temperature": 0.0, "avg_logprob": -0.17702713791204958, "compression_ratio": 2.033980582524272, "no_speech_prob": 2.2252536382438848e-06}, {"id": 69, "seek": 32568, "start": 325.68, "end": 330.32, "text": " And so forth rest of the sentence and of course in real life we have", "tokens": [400, 370, 5220, 1472, 295, 264, 8174, 293, 295, 1164, 294, 957, 993, 321, 362], "temperature": 0.0, "avg_logprob": -0.1904606819152832, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.3405005979147973e-06}, {"id": 70, "seek": 32568, "start": 331.2, "end": 332.6, "text": " thousands of", "tokens": [5383, 295], "temperature": 0.0, "avg_logprob": -0.1904606819152832, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.3405005979147973e-06}, {"id": 71, "seek": 32568, "start": 332.6, "end": 334.6, "text": " possibilities for each possible word", "tokens": [12178, 337, 1184, 1944, 1349], "temperature": 0.0, "avg_logprob": -0.1904606819152832, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.3405005979147973e-06}, {"id": 72, "seek": 32568, "start": 338.08, "end": 340.08, "text": " So to solve", "tokens": [407, 281, 5039], "temperature": 0.0, "avg_logprob": -0.1904606819152832, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.3405005979147973e-06}, {"id": 73, "seek": 32568, "start": 340.08, "end": 345.04, "text": " Our way to solve this problem is to use something called beam search and beam search is how", "tokens": [2621, 636, 281, 5039, 341, 1154, 307, 281, 764, 746, 1219, 14269, 3164, 293, 14269, 3164, 307, 577], "temperature": 0.0, "avg_logprob": -0.1904606819152832, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.3405005979147973e-06}, {"id": 74, "seek": 32568, "start": 345.8, "end": 353.48, "text": " We normally do both people do generation for something like translation now what we do in beam search is we say okay?", "tokens": [492, 5646, 360, 1293, 561, 360, 5125, 337, 746, 411, 12853, 586, 437, 321, 360, 294, 14269, 3164, 307, 321, 584, 1392, 30], "temperature": 0.0, "avg_logprob": -0.1904606819152832, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.3405005979147973e-06}, {"id": 75, "seek": 35348, "start": 353.48, "end": 358.44, "text": " We're going to go through each stage, but we're not going to just pick one", "tokens": [492, 434, 516, 281, 352, 807, 1184, 3233, 11, 457, 321, 434, 406, 516, 281, 445, 1888, 472], "temperature": 0.0, "avg_logprob": -0.17699032065309125, "compression_ratio": 1.6174863387978142, "no_speech_prob": 9.515822057437617e-06}, {"id": 76, "seek": 35348, "start": 359.40000000000003, "end": 366.58000000000004, "text": " Possibility we're going to pick K possibilities. We're going to pick some number and that number K is called the beam width", "tokens": [33112, 2841, 321, 434, 516, 281, 1888, 591, 12178, 13, 492, 434, 516, 281, 1888, 512, 1230, 293, 300, 1230, 591, 307, 1219, 264, 14269, 11402], "temperature": 0.0, "avg_logprob": -0.17699032065309125, "compression_ratio": 1.6174863387978142, "no_speech_prob": 9.515822057437617e-06}, {"id": 77, "seek": 35348, "start": 367.28000000000003, "end": 371.52000000000004, "text": " So if we picked like K of 2 right actually let's make these", "tokens": [407, 498, 321, 6183, 411, 591, 295, 568, 558, 767, 718, 311, 652, 613], "temperature": 0.0, "avg_logprob": -0.17699032065309125, "compression_ratio": 1.6174863387978142, "no_speech_prob": 9.515822057437617e-06}, {"id": 78, "seek": 35348, "start": 372.24, "end": 374.24, "text": " Not all equal", "tokens": [1726, 439, 2681], "temperature": 0.0, "avg_logprob": -0.17699032065309125, "compression_ratio": 1.6174863387978142, "no_speech_prob": 9.515822057437617e-06}, {"id": 79, "seek": 35348, "start": 374.6, "end": 376.6, "text": " So we can pick a K of 2", "tokens": [407, 321, 393, 1888, 257, 591, 295, 568], "temperature": 0.0, "avg_logprob": -0.17699032065309125, "compression_ratio": 1.6174863387978142, "no_speech_prob": 9.515822057437617e-06}, {"id": 80, "seek": 37660, "start": 376.6, "end": 378.6, "text": " Then", "tokens": [1396], "temperature": 0.0, "avg_logprob": -0.27939201123786694, "compression_ratio": 1.585987261146497, "no_speech_prob": 6.437787760660285e-06}, {"id": 81, "seek": 37660, "start": 382.8, "end": 388.24, "text": " We would say okay after word at point time point one it could be the", "tokens": [492, 576, 584, 1392, 934, 1349, 412, 935, 565, 935, 472, 309, 727, 312, 264], "temperature": 0.0, "avg_logprob": -0.27939201123786694, "compression_ratio": 1.585987261146497, "no_speech_prob": 6.437787760660285e-06}, {"id": 82, "seek": 37660, "start": 389.56, "end": 394.44, "text": " Or it could be some so they are two highest possibilities", "tokens": [1610, 309, 727, 312, 512, 370, 436, 366, 732, 6343, 12178], "temperature": 0.0, "avg_logprob": -0.27939201123786694, "compression_ratio": 1.585987261146497, "no_speech_prob": 6.437787760660285e-06}, {"id": 83, "seek": 37660, "start": 394.44, "end": 401.82000000000005, "text": " And then at the second point so then for each of the and some we would rerun our decoder", "tokens": [400, 550, 412, 264, 1150, 935, 370, 550, 337, 1184, 295, 264, 293, 512, 321, 576, 43819, 409, 527, 979, 19866], "temperature": 0.0, "avg_logprob": -0.27939201123786694, "compression_ratio": 1.585987261146497, "no_speech_prob": 6.437787760660285e-06}, {"id": 84, "seek": 37660, "start": 402.16, "end": 404.16, "text": " and get a new probability of", "tokens": [293, 483, 257, 777, 8482, 295], "temperature": 0.0, "avg_logprob": -0.27939201123786694, "compression_ratio": 1.585987261146497, "no_speech_prob": 6.437787760660285e-06}, {"id": 85, "seek": 40416, "start": 404.16, "end": 407.0, "text": " each of these things", "tokens": [1184, 295, 613, 721], "temperature": 0.0, "avg_logprob": -0.19004972121294808, "compression_ratio": 1.7733990147783252, "no_speech_prob": 6.961854978726478e-06}, {"id": 86, "seek": 40416, "start": 407.6, "end": 414.68, "text": " given that we start with the and each of these things given we start with some because remember that the", "tokens": [2212, 300, 321, 722, 365, 264, 293, 1184, 295, 613, 721, 2212, 321, 722, 365, 512, 570, 1604, 300, 264], "temperature": 0.0, "avg_logprob": -0.19004972121294808, "compression_ratio": 1.7733990147783252, "no_speech_prob": 6.961854978726478e-06}, {"id": 87, "seek": 40416, "start": 415.32000000000005, "end": 419.68, "text": " RNM at each time step has this input both the", "tokens": [45702, 44, 412, 1184, 565, 1823, 575, 341, 4846, 1293, 264], "temperature": 0.0, "avg_logprob": -0.19004972121294808, "compression_ratio": 1.7733990147783252, "no_speech_prob": 6.961854978726478e-06}, {"id": 88, "seek": 40416, "start": 420.36, "end": 425.6, "text": " Hidden state but also like what was the previous word you predicted so at this point", "tokens": [41156, 1785, 457, 611, 411, 437, 390, 264, 3894, 1349, 291, 19147, 370, 412, 341, 935], "temperature": 0.0, "avg_logprob": -0.19004972121294808, "compression_ratio": 1.7733990147783252, "no_speech_prob": 6.961854978726478e-06}, {"id": 89, "seek": 40416, "start": 425.6, "end": 430.16, "text": " You can't just say it's point four point three point two, but it actually depends on your previous word", "tokens": [509, 393, 380, 445, 584, 309, 311, 935, 1451, 935, 1045, 935, 732, 11, 457, 309, 767, 5946, 322, 428, 3894, 1349], "temperature": 0.0, "avg_logprob": -0.19004972121294808, "compression_ratio": 1.7733990147783252, "no_speech_prob": 6.961854978726478e-06}, {"id": 90, "seek": 43016, "start": 430.16, "end": 436.96000000000004, "text": " So based on those two words, maybe the two highest possibilities now at this point", "tokens": [407, 2361, 322, 729, 732, 2283, 11, 1310, 264, 732, 6343, 12178, 586, 412, 341, 935], "temperature": 0.0, "avg_logprob": -0.2861525275490501, "compression_ratio": 1.4028776978417266, "no_speech_prob": 1.3709370705328183e-06}, {"id": 91, "seek": 43016, "start": 439.48, "end": 443.5, "text": " Some fluffy and the big", "tokens": [2188, 22778, 293, 264, 955], "temperature": 0.0, "avg_logprob": -0.2861525275490501, "compression_ratio": 1.4028776978417266, "no_speech_prob": 1.3709370705328183e-06}, {"id": 92, "seek": 43016, "start": 444.36, "end": 451.40000000000003, "text": " Okay, and so what we do is we again we pick only the top K", "tokens": [1033, 11, 293, 370, 437, 321, 360, 307, 321, 797, 321, 1888, 787, 264, 1192, 591], "temperature": 0.0, "avg_logprob": -0.2861525275490501, "compression_ratio": 1.4028776978417266, "no_speech_prob": 1.3709370705328183e-06}, {"id": 93, "seek": 43016, "start": 451.64000000000004, "end": 453.64000000000004, "text": " So K is two for us beam width", "tokens": [407, 591, 307, 732, 337, 505, 14269, 11402], "temperature": 0.0, "avg_logprob": -0.2861525275490501, "compression_ratio": 1.4028776978417266, "no_speech_prob": 1.3709370705328183e-06}, {"id": 94, "seek": 45364, "start": 453.64, "end": 461.64, "text": " two word strings that because we could it for each of the and some there are three possibilities", "tokens": [732, 1349, 13985, 300, 570, 321, 727, 309, 337, 1184, 295, 264, 293, 512, 456, 366, 1045, 12178], "temperature": 0.0, "avg_logprob": -0.5041676240808823, "compression_ratio": 1.6417910447761195, "no_speech_prob": 3.844817001663614e-06}, {"id": 95, "seek": 45364, "start": 461.64, "end": 465.15999999999997, "text": " So there'd be six possible next steps. We just keep the top two", "tokens": [407, 456, 1116, 312, 2309, 1944, 958, 4439, 13, 492, 445, 1066, 264, 1192, 732], "temperature": 0.0, "avg_logprob": -0.5041676240808823, "compression_ratio": 1.6417910447761195, "no_speech_prob": 3.844817001663614e-06}, {"id": 96, "seek": 45364, "start": 465.64, "end": 469.64, "text": " So we just say okay the you know at this point with either the", "tokens": [407, 321, 445, 584, 1392, 264, 291, 458, 412, 341, 935, 365, 2139, 264], "temperature": 0.0, "avg_logprob": -0.5041676240808823, "compression_ratio": 1.6417910447761195, "no_speech_prob": 3.844817001663614e-06}, {"id": 97, "seek": 45364, "start": 470.36, "end": 472.36, "text": " big or", "tokens": [955, 420], "temperature": 0.0, "avg_logprob": -0.5041676240808823, "compression_ratio": 1.6417910447761195, "no_speech_prob": 3.844817001663614e-06}, {"id": 98, "seek": 45364, "start": 473.68, "end": 475.68, "text": " some", "tokens": [512], "temperature": 0.0, "avg_logprob": -0.5041676240808823, "compression_ratio": 1.6417910447761195, "no_speech_prob": 3.844817001663614e-06}, {"id": 99, "seek": 45364, "start": 475.68, "end": 477.68, "text": " fluffy", "tokens": [22778], "temperature": 0.0, "avg_logprob": -0.5041676240808823, "compression_ratio": 1.6417910447761195, "no_speech_prob": 3.844817001663614e-06}, {"id": 100, "seek": 47768, "start": 477.68, "end": 484.0, "text": " And then again we do the same thing for step three right so for each of these two", "tokens": [400, 550, 797, 321, 360, 264, 912, 551, 337, 1823, 1045, 558, 370, 337, 1184, 295, 613, 732], "temperature": 0.0, "avg_logprob": -0.1485860456119884, "compression_ratio": 1.8826530612244898, "no_speech_prob": 2.09044242183154e-06}, {"id": 101, "seek": 47768, "start": 484.8, "end": 489.7, "text": " Possibilities there's three words we can add on and so for each of these possibilities", "tokens": [33112, 8261, 456, 311, 1045, 2283, 321, 393, 909, 322, 293, 370, 337, 1184, 295, 613, 12178], "temperature": 0.0, "avg_logprob": -0.1485860456119884, "compression_ratio": 1.8826530612244898, "no_speech_prob": 2.09044242183154e-06}, {"id": 102, "seek": 47768, "start": 490.76, "end": 492.76, "text": " We again the decoder has", "tokens": [492, 797, 264, 979, 19866, 575], "temperature": 0.0, "avg_logprob": -0.1485860456119884, "compression_ratio": 1.8826530612244898, "no_speech_prob": 2.09044242183154e-06}, {"id": 103, "seek": 47768, "start": 494.36, "end": 498.8, "text": " Different probabilities for each of those based on each of those possible inputs", "tokens": [20825, 33783, 337, 1184, 295, 729, 2361, 322, 1184, 295, 729, 1944, 15743], "temperature": 0.0, "avg_logprob": -0.1485860456119884, "compression_ratio": 1.8826530612244898, "no_speech_prob": 2.09044242183154e-06}, {"id": 104, "seek": 47768, "start": 498.8, "end": 505.88, "text": " And so we end up and then we just select out of those two times three is six again the top two", "tokens": [400, 370, 321, 917, 493, 293, 550, 321, 445, 3048, 484, 295, 729, 732, 1413, 1045, 307, 2309, 797, 264, 1192, 732], "temperature": 0.0, "avg_logprob": -0.1485860456119884, "compression_ratio": 1.8826530612244898, "no_speech_prob": 2.09044242183154e-06}, {"id": 105, "seek": 50588, "start": 505.88, "end": 510.15999999999997, "text": " three word strings by probability so in this case the top might be", "tokens": [1045, 1349, 13985, 538, 8482, 370, 294, 341, 1389, 264, 1192, 1062, 312], "temperature": 0.0, "avg_logprob": -0.31436087160694354, "compression_ratio": 1.3385826771653544, "no_speech_prob": 3.611956344684586e-06}, {"id": 106, "seek": 50588, "start": 512.12, "end": 515.78, "text": " The big dog", "tokens": [440, 955, 3000], "temperature": 0.0, "avg_logprob": -0.31436087160694354, "compression_ratio": 1.3385826771653544, "no_speech_prob": 3.611956344684586e-06}, {"id": 107, "seek": 50588, "start": 518.0, "end": 520.0, "text": " Versus oops", "tokens": [12226, 301, 34166], "temperature": 0.0, "avg_logprob": -0.31436087160694354, "compression_ratio": 1.3385826771653544, "no_speech_prob": 3.611956344684586e-06}, {"id": 108, "seek": 50588, "start": 521.88, "end": 523.88, "text": " Some fluffy", "tokens": [2188, 22778], "temperature": 0.0, "avg_logprob": -0.31436087160694354, "compression_ratio": 1.3385826771653544, "no_speech_prob": 3.611956344684586e-06}, {"id": 109, "seek": 50588, "start": 526.56, "end": 528.56, "text": " Cat and", "tokens": [9565, 293], "temperature": 0.0, "avg_logprob": -0.31436087160694354, "compression_ratio": 1.3385826771653544, "no_speech_prob": 3.611956344684586e-06}, {"id": 110, "seek": 50588, "start": 529.12, "end": 534.0, "text": " So forth right so this works out nicely because if the next", "tokens": [407, 5220, 558, 370, 341, 1985, 484, 9594, 570, 498, 264, 958], "temperature": 0.0, "avg_logprob": -0.31436087160694354, "compression_ratio": 1.3385826771653544, "no_speech_prob": 3.611956344684586e-06}, {"id": 111, "seek": 53400, "start": 534.0, "end": 536.0, "text": " kind of three words", "tokens": [733, 295, 1045, 2283], "temperature": 0.0, "avg_logprob": -0.4114875378816024, "compression_ratio": 1.3305785123966942, "no_speech_prob": 3.1071843409336e-07}, {"id": 112, "seek": 53400, "start": 537.0, "end": 539.0, "text": " Turned out to be with high probability", "tokens": [7956, 292, 484, 281, 312, 365, 1090, 8482], "temperature": 0.0, "avg_logprob": -0.4114875378816024, "compression_ratio": 1.3305785123966942, "no_speech_prob": 3.1071843409336e-07}, {"id": 113, "seek": 53400, "start": 540.2, "end": 542.2, "text": " bit", "tokens": [857], "temperature": 0.0, "avg_logprob": -0.4114875378816024, "compression_ratio": 1.3305785123966942, "no_speech_prob": 3.1071843409336e-07}, {"id": 114, "seek": 53400, "start": 542.2, "end": 544.2, "text": " the postman", "tokens": [264, 2183, 1601], "temperature": 0.0, "avg_logprob": -0.4114875378816024, "compression_ratio": 1.3305785123966942, "no_speech_prob": 3.1071843409336e-07}, {"id": 115, "seek": 53400, "start": 546.88, "end": 555.24, "text": " Right then this approach allows us to identify that like oh, it's probably the big dog", "tokens": [1779, 550, 341, 3109, 4045, 505, 281, 5876, 300, 411, 1954, 11, 309, 311, 1391, 264, 955, 3000], "temperature": 0.0, "avg_logprob": -0.4114875378816024, "compression_ratio": 1.3305785123966942, "no_speech_prob": 3.1071843409336e-07}, {"id": 116, "seek": 55524, "start": 555.24, "end": 565.04, "text": " Bit the postman, so it's it's only like partially greedy now. Okay, so it's a it's a greedy approach, but we we're keeping a", "tokens": [9101, 264, 2183, 1601, 11, 370, 309, 311, 309, 311, 787, 411, 18886, 28228, 586, 13, 1033, 11, 370, 309, 311, 257, 309, 311, 257, 28228, 3109, 11, 457, 321, 321, 434, 5145, 257], "temperature": 0.0, "avg_logprob": -0.16414252660607778, "compression_ratio": 1.5687203791469195, "no_speech_prob": 2.6841707949643023e-06}, {"id": 117, "seek": 55524, "start": 565.96, "end": 569.08, "text": " beam width number of options all the way through", "tokens": [14269, 11402, 1230, 295, 3956, 439, 264, 636, 807], "temperature": 0.0, "avg_logprob": -0.16414252660607778, "compression_ratio": 1.5687203791469195, "no_speech_prob": 2.6841707949643023e-06}, {"id": 118, "seek": 55524, "start": 571.64, "end": 577.0, "text": " Coding this is a bit of a pain in the ass, so I'm not going to bother besides which fast AI already has it", "tokens": [383, 8616, 341, 307, 257, 857, 295, 257, 1822, 294, 264, 1256, 11, 370, 286, 478, 406, 516, 281, 8677, 11868, 597, 2370, 7318, 1217, 575, 309], "temperature": 0.0, "avg_logprob": -0.16414252660607778, "compression_ratio": 1.5687203791469195, "no_speech_prob": 2.6841707949643023e-06}, {"id": 119, "seek": 55524, "start": 577.8, "end": 579.8, "text": " but if you're interested", "tokens": [457, 498, 291, 434, 3102], "temperature": 0.0, "avg_logprob": -0.16414252660607778, "compression_ratio": 1.5687203791469195, "no_speech_prob": 2.6841707949643023e-06}, {"id": 120, "seek": 57980, "start": 579.8, "end": 584.8, "text": " you can go to fast AI and", "tokens": [291, 393, 352, 281, 2370, 7318, 293], "temperature": 0.0, "avg_logprob": -0.4094916363151706, "compression_ratio": 1.2857142857142858, "no_speech_prob": 2.6686908313422464e-05}, {"id": 121, "seek": 57980, "start": 586.28, "end": 590.12, "text": " Search for beam say and", "tokens": [17180, 337, 14269, 584, 293], "temperature": 0.0, "avg_logprob": -0.4094916363151706, "compression_ratio": 1.2857142857142858, "no_speech_prob": 2.6686908313422464e-05}, {"id": 122, "seek": 57980, "start": 594.4, "end": 598.12, "text": " Find beam search, and then you can click source code", "tokens": [11809, 14269, 3164, 11, 293, 550, 291, 393, 2052, 4009, 3089], "temperature": 0.0, "avg_logprob": -0.4094916363151706, "compression_ratio": 1.2857142857142858, "no_speech_prob": 2.6686908313422464e-05}, {"id": 123, "seek": 57980, "start": 600.0799999999999, "end": 601.56, "text": " And", "tokens": [400], "temperature": 0.0, "avg_logprob": -0.4094916363151706, "compression_ratio": 1.2857142857142858, "no_speech_prob": 2.6686908313422464e-05}, {"id": 124, "seek": 57980, "start": 601.56, "end": 603.0, "text": " There it is", "tokens": [821, 309, 307], "temperature": 0.0, "avg_logprob": -0.4094916363151706, "compression_ratio": 1.2857142857142858, "no_speech_prob": 2.6686908313422464e-05}, {"id": 125, "seek": 57980, "start": 603.0, "end": 605.42, "text": " So it's kind of let's see", "tokens": [407, 309, 311, 733, 295, 718, 311, 536], "temperature": 0.0, "avg_logprob": -0.4094916363151706, "compression_ratio": 1.2857142857142858, "no_speech_prob": 2.6686908313422464e-05}, {"id": 126, "seek": 60542, "start": 605.42, "end": 608.62, "text": " One and a half screens, so it's not lots", "tokens": [1485, 293, 257, 1922, 11171, 11, 370, 309, 311, 406, 3195], "temperature": 0.0, "avg_logprob": -0.47786159791808197, "compression_ratio": 1.4969325153374233, "no_speech_prob": 4.710854682343779e-06}, {"id": 127, "seek": 60542, "start": 609.14, "end": 613.02, "text": " And but it's just basically that doing that loop and selecting the top", "tokens": [400, 457, 309, 311, 445, 1936, 300, 884, 300, 6367, 293, 18182, 264, 1192], "temperature": 0.0, "avg_logprob": -0.47786159791808197, "compression_ratio": 1.4969325153374233, "no_speech_prob": 4.710854682343779e-06}, {"id": 128, "seek": 60542, "start": 614.06, "end": 616.06, "text": " K at each sides", "tokens": [591, 412, 1184, 4881], "temperature": 0.0, "avg_logprob": -0.47786159791808197, "compression_ratio": 1.4969325153374233, "no_speech_prob": 4.710854682343779e-06}, {"id": 129, "seek": 60542, "start": 616.06, "end": 620.4599999999999, "text": " Time step and here's that beam width sorry beam size you can pass in", "tokens": [6161, 1823, 293, 510, 311, 300, 14269, 11402, 2597, 14269, 2744, 291, 393, 1320, 294], "temperature": 0.0, "avg_logprob": -0.47786159791808197, "compression_ratio": 1.4969325153374233, "no_speech_prob": 4.710854682343779e-06}, {"id": 130, "seek": 60542, "start": 622.78, "end": 624.78, "text": " Okay, so that's", "tokens": [1033, 11, 370, 300, 311], "temperature": 0.0, "avg_logprob": -0.47786159791808197, "compression_ratio": 1.4969325153374233, "no_speech_prob": 4.710854682343779e-06}, {"id": 131, "seek": 60542, "start": 625.8199999999999, "end": 629.02, "text": " the kind of normal approach for", "tokens": [264, 733, 295, 2710, 3109, 337], "temperature": 0.0, "avg_logprob": -0.47786159791808197, "compression_ratio": 1.4969325153374233, "no_speech_prob": 4.710854682343779e-06}, {"id": 132, "seek": 62902, "start": 629.02, "end": 636.66, "text": " Decoding the translation model, and it's a good approach for kind of decoding any kind of text where you have a", "tokens": [12427, 8616, 264, 12853, 2316, 11, 293, 309, 311, 257, 665, 3109, 337, 733, 295, 979, 8616, 604, 733, 295, 2487, 689, 291, 362, 257], "temperature": 0.0, "avg_logprob": -0.29848539949667574, "compression_ratio": 1.8008849557522124, "no_speech_prob": 8.664563210913911e-06}, {"id": 133, "seek": 62902, "start": 638.38, "end": 642.54, "text": " Something kind of pointing you as to which way to go so with translation at every step", "tokens": [6595, 733, 295, 12166, 291, 382, 281, 597, 636, 281, 352, 370, 365, 12853, 412, 633, 1823], "temperature": 0.0, "avg_logprob": -0.29848539949667574, "compression_ratio": 1.8008849557522124, "no_speech_prob": 8.664563210913911e-06}, {"id": 134, "seek": 62902, "start": 642.54, "end": 648.06, "text": " There's like something saying like this word is probably that or that this word is probably that or that", "tokens": [821, 311, 411, 746, 1566, 411, 341, 1349, 307, 1391, 300, 420, 300, 341, 1349, 307, 1391, 300, 420, 300], "temperature": 0.0, "avg_logprob": -0.29848539949667574, "compression_ratio": 1.8008849557522124, "no_speech_prob": 8.664563210913911e-06}, {"id": 135, "seek": 62902, "start": 649.1, "end": 652.06, "text": " Which is very different to the GPT-2", "tokens": [3013, 307, 588, 819, 281, 264, 26039, 51, 12, 17], "temperature": 0.0, "avg_logprob": -0.29848539949667574, "compression_ratio": 1.8008849557522124, "no_speech_prob": 8.664563210913911e-06}, {"id": 136, "seek": 62902, "start": 653.1, "end": 656.9399999999999, "text": " Unicorn example we saw remember the GPT-2 unicorn example was like", "tokens": [1156, 23115, 1365, 321, 1866, 1604, 264, 26039, 51, 12, 17, 28122, 1365, 390, 411], "temperature": 0.0, "avg_logprob": -0.29848539949667574, "compression_ratio": 1.8008849557522124, "no_speech_prob": 8.664563210913911e-06}, {"id": 137, "seek": 65694, "start": 656.94, "end": 659.1800000000001, "text": " Here's a paragraph of English text now", "tokens": [1692, 311, 257, 18865, 295, 3669, 2487, 586], "temperature": 0.0, "avg_logprob": -0.33130908656764674, "compression_ratio": 1.4623655913978495, "no_speech_prob": 3.5559457955969265e-06}, {"id": 138, "seek": 65694, "start": 659.1800000000001, "end": 662.7, "text": " What might the rest of it be and like that's got no guidance?", "tokens": [708, 1062, 264, 1472, 295, 309, 312, 293, 411, 300, 311, 658, 572, 10056, 30], "temperature": 0.0, "avg_logprob": -0.33130908656764674, "compression_ratio": 1.4623655913978495, "no_speech_prob": 3.5559457955969265e-06}, {"id": 139, "seek": 65694, "start": 662.7, "end": 667.1, "text": " It's not translating something and that kind of text generation is very different", "tokens": [467, 311, 406, 35030, 746, 293, 300, 733, 295, 2487, 5125, 307, 588, 819], "temperature": 0.0, "avg_logprob": -0.33130908656764674, "compression_ratio": 1.4623655913978495, "no_speech_prob": 3.5559457955969265e-06}, {"id": 140, "seek": 65694, "start": 667.98, "end": 669.98, "text": " There was a paper", "tokens": [821, 390, 257, 3035], "temperature": 0.0, "avg_logprob": -0.33130908656764674, "compression_ratio": 1.4623655913978495, "no_speech_prob": 3.5559457955969265e-06}, {"id": 141, "seek": 65694, "start": 672.1400000000001, "end": 676.7, "text": " Let's find it, so I'm now looking at the 7b sector sec nucleus", "tokens": [961, 311, 915, 309, 11, 370, 286, 478, 586, 1237, 412, 264, 1614, 65, 6977, 907, 28055], "temperature": 0.0, "avg_logprob": -0.33130908656764674, "compression_ratio": 1.4623655913978495, "no_speech_prob": 3.5559457955969265e-06}, {"id": 142, "seek": 65694, "start": 683.82, "end": 685.82, "text": " Notebook", "tokens": [11633, 2939], "temperature": 0.0, "avg_logprob": -0.33130908656764674, "compression_ratio": 1.4623655913978495, "no_speech_prob": 3.5559457955969265e-06}, {"id": 143, "seek": 68582, "start": 685.82, "end": 692.3000000000001, "text": " Notebook which has a link to this paper called the curious case of neural text generation which is", "tokens": [11633, 2939, 597, 575, 257, 2113, 281, 341, 3035, 1219, 264, 6369, 1389, 295, 18161, 2487, 5125, 597, 307], "temperature": 0.0, "avg_logprob": -0.20391150580512152, "compression_ratio": 1.5943775100401607, "no_speech_prob": 1.0952825505228247e-05}, {"id": 144, "seek": 68582, "start": 693.0200000000001, "end": 697.9000000000001, "text": " less than two months old, so this is quite a recent paper this came from the", "tokens": [1570, 813, 732, 2493, 1331, 11, 370, 341, 307, 1596, 257, 5162, 3035, 341, 1361, 490, 264], "temperature": 0.0, "avg_logprob": -0.20391150580512152, "compression_ratio": 1.5943775100401607, "no_speech_prob": 1.0952825505228247e-05}, {"id": 145, "seek": 68582, "start": 699.2600000000001, "end": 702.94, "text": " Allen AI Institute also known as AI 2 and", "tokens": [17160, 7318, 9446, 611, 2570, 382, 7318, 568, 293], "temperature": 0.0, "avg_logprob": -0.20391150580512152, "compression_ratio": 1.5943775100401607, "no_speech_prob": 1.0952825505228247e-05}, {"id": 146, "seek": 68582, "start": 703.74, "end": 710.1, "text": " They were they pointed out something really fascinating and this is like a pretty easy", "tokens": [814, 645, 436, 10932, 484, 746, 534, 10343, 293, 341, 307, 411, 257, 1238, 1858], "temperature": 0.0, "avg_logprob": -0.20391150580512152, "compression_ratio": 1.5943775100401607, "no_speech_prob": 1.0952825505228247e-05}, {"id": 147, "seek": 71010, "start": 710.1, "end": 717.02, "text": " Read so for people who are just starting to get into reading academic papers. This is a good good one to check out", "tokens": [17604, 370, 337, 561, 567, 366, 445, 2891, 281, 483, 666, 3760, 7778, 10577, 13, 639, 307, 257, 665, 665, 472, 281, 1520, 484], "temperature": 0.0, "avg_logprob": -0.18262504709178004, "compression_ratio": 1.566820276497696, "no_speech_prob": 5.862679245183244e-06}, {"id": 148, "seek": 71010, "start": 717.14, "end": 721.7, "text": " They did something interesting which is they said what happens if we take the GPT-2", "tokens": [814, 630, 746, 1880, 597, 307, 436, 848, 437, 2314, 498, 321, 747, 264, 26039, 51, 12, 17], "temperature": 0.0, "avg_logprob": -0.18262504709178004, "compression_ratio": 1.566820276497696, "no_speech_prob": 5.862679245183244e-06}, {"id": 149, "seek": 71010, "start": 722.86, "end": 724.5, "text": " model and", "tokens": [2316, 293], "temperature": 0.0, "avg_logprob": -0.18262504709178004, "compression_ratio": 1.566820276497696, "no_speech_prob": 5.862679245183244e-06}, {"id": 150, "seek": 71010, "start": 724.5, "end": 726.3000000000001, "text": " we do the", "tokens": [321, 360, 264], "temperature": 0.0, "avg_logprob": -0.18262504709178004, "compression_ratio": 1.566820276497696, "no_speech_prob": 5.862679245183244e-06}, {"id": 151, "seek": 71010, "start": 726.3000000000001, "end": 727.5400000000001, "text": " unicorns", "tokens": [28122, 82], "temperature": 0.0, "avg_logprob": -0.18262504709178004, "compression_ratio": 1.566820276497696, "no_speech_prob": 5.862679245183244e-06}, {"id": 152, "seek": 71010, "start": 727.5400000000001, "end": 734.1, "text": " prompt exactly the same as in their blog post and we use beam search and the answer is they get the", "tokens": [12391, 2293, 264, 912, 382, 294, 641, 6968, 2183, 293, 321, 764, 14269, 3164, 293, 264, 1867, 307, 436, 483, 264], "temperature": 0.0, "avg_logprob": -0.18262504709178004, "compression_ratio": 1.566820276497696, "no_speech_prob": 5.862679245183244e-06}, {"id": 153, "seek": 71010, "start": 735.82, "end": 737.82, "text": " continuation", "tokens": [29357], "temperature": 0.0, "avg_logprob": -0.18262504709178004, "compression_ratio": 1.566820276497696, "no_speech_prob": 5.862679245183244e-06}, {"id": 154, "seek": 73782, "start": 737.82, "end": 743.86, "text": " Professor of the Department of your sangelis the most important place the world to be recognition of the world to be of the world to be", "tokens": [8419, 295, 264, 5982, 295, 428, 262, 14282, 271, 264, 881, 1021, 1081, 264, 1002, 281, 312, 11150, 295, 264, 1002, 281, 312, 295, 264, 1002, 281, 312], "temperature": 0.0, "avg_logprob": -0.2706582475416731, "compression_ratio": 1.7737556561085972, "no_speech_prob": 5.093558684166055e-06}, {"id": 155, "seek": 73782, "start": 743.86, "end": 745.86, "text": " Of the world to be of the world", "tokens": [2720, 264, 1002, 281, 312, 295, 264, 1002], "temperature": 0.0, "avg_logprob": -0.2706582475416731, "compression_ratio": 1.7737556561085972, "no_speech_prob": 5.093558684166055e-06}, {"id": 156, "seek": 73782, "start": 746.62, "end": 750.2600000000001, "text": " Not great and so it's interesting they actually point out that", "tokens": [1726, 869, 293, 370, 309, 311, 1880, 436, 767, 935, 484, 300], "temperature": 0.0, "avg_logprob": -0.2706582475416731, "compression_ratio": 1.7737556561085972, "no_speech_prob": 5.093558684166055e-06}, {"id": 157, "seek": 73782, "start": 751.1, "end": 753.1, "text": " beam search seems to have", "tokens": [14269, 3164, 2544, 281, 362], "temperature": 0.0, "avg_logprob": -0.2706582475416731, "compression_ratio": 1.7737556561085972, "no_speech_prob": 5.093558684166055e-06}, {"id": 158, "seek": 73782, "start": 753.34, "end": 755.46, "text": " problems so for GPT-2", "tokens": [2740, 370, 337, 26039, 51, 12, 17], "temperature": 0.0, "avg_logprob": -0.2706582475416731, "compression_ratio": 1.7737556561085972, "no_speech_prob": 5.093558684166055e-06}, {"id": 159, "seek": 73782, "start": 756.38, "end": 764.3000000000001, "text": " What they did was they did what's cat called top case sampling and so top case sampling?", "tokens": [708, 436, 630, 390, 436, 630, 437, 311, 3857, 1219, 1192, 1389, 21179, 293, 370, 1192, 1389, 21179, 30], "temperature": 0.0, "avg_logprob": -0.2706582475416731, "compression_ratio": 1.7737556561085972, "no_speech_prob": 5.093558684166055e-06}, {"id": 160, "seek": 76430, "start": 764.3, "end": 767.14, "text": " Is this I will show you", "tokens": [1119, 341, 286, 486, 855, 291], "temperature": 0.0, "avg_logprob": -0.22019481658935547, "compression_ratio": 1.5892857142857142, "no_speech_prob": 1.4284686585597228e-05}, {"id": 161, "seek": 76430, "start": 769.54, "end": 774.74, "text": " So if you look at 7b sector sec nucleus what I've done", "tokens": [407, 498, 291, 574, 412, 1614, 65, 6977, 907, 28055, 437, 286, 600, 1096], "temperature": 0.0, "avg_logprob": -0.22019481658935547, "compression_ratio": 1.5892857142857142, "no_speech_prob": 1.4284686585597228e-05}, {"id": 162, "seek": 76430, "start": 777.0999999999999, "end": 779.0999999999999, "text": " Oh", "tokens": [876], "temperature": 0.0, "avg_logprob": -0.22019481658935547, "compression_ratio": 1.5892857142857142, "no_speech_prob": 1.4284686585597228e-05}, {"id": 163, "seek": 76430, "start": 779.26, "end": 784.5, "text": " By the way if you ever see things like this torch dot kuda dot set device in our notebooks or anywhere", "tokens": [3146, 264, 636, 498, 291, 1562, 536, 721, 411, 341, 27822, 5893, 350, 11152, 5893, 992, 4302, 294, 527, 43782, 420, 4992], "temperature": 0.0, "avg_logprob": -0.22019481658935547, "compression_ratio": 1.5892857142857142, "no_speech_prob": 1.4284686585597228e-05}, {"id": 164, "seek": 76430, "start": 784.5, "end": 789.2199999999999, "text": " That's just means that I was running something on a machine with more than one GPU", "tokens": [663, 311, 445, 1355, 300, 286, 390, 2614, 746, 322, 257, 3479, 365, 544, 813, 472, 18407], "temperature": 0.0, "avg_logprob": -0.22019481658935547, "compression_ratio": 1.5892857142857142, "no_speech_prob": 1.4284686585597228e-05}, {"id": 165, "seek": 76430, "start": 789.38, "end": 793.62, "text": " And I wanted to pick which GPU I wanted if you only have one GPU that will give an error", "tokens": [400, 286, 1415, 281, 1888, 597, 18407, 286, 1415, 498, 291, 787, 362, 472, 18407, 300, 486, 976, 364, 6713], "temperature": 0.0, "avg_logprob": -0.22019481658935547, "compression_ratio": 1.5892857142857142, "no_speech_prob": 1.4284686585597228e-05}, {"id": 166, "seek": 79362, "start": 793.62, "end": 795.62, "text": " So you should just delete it", "tokens": [407, 291, 820, 445, 12097, 309], "temperature": 0.0, "avg_logprob": -0.17131131155449048, "compression_ratio": 1.7530364372469636, "no_speech_prob": 1.1842537787742913e-05}, {"id": 167, "seek": 79362, "start": 797.14, "end": 799.54, "text": " So what I did was I didn't want to have all that", "tokens": [407, 437, 286, 630, 390, 286, 994, 380, 528, 281, 362, 439, 300], "temperature": 0.0, "avg_logprob": -0.17131131155449048, "compression_ratio": 1.7530364372469636, "no_speech_prob": 1.1842537787742913e-05}, {"id": 168, "seek": 79362, "start": 799.9, "end": 803.7, "text": " Cells and cells of code to rerun every time and copy it from other notebooks", "tokens": [383, 13677, 293, 5438, 295, 3089, 281, 43819, 409, 633, 565, 293, 5055, 309, 490, 661, 43782], "temperature": 0.0, "avg_logprob": -0.17131131155449048, "compression_ratio": 1.7530364372469636, "no_speech_prob": 1.1842537787742913e-05}, {"id": 169, "seek": 79362, "start": 803.7, "end": 807.18, "text": " So I just pasted in pasted it into a sector sec dot PI", "tokens": [407, 286, 445, 1791, 292, 294, 1791, 292, 309, 666, 257, 6977, 907, 5893, 27176], "temperature": 0.0, "avg_logprob": -0.17131131155449048, "compression_ratio": 1.7530364372469636, "no_speech_prob": 1.1842537787742913e-05}, {"id": 170, "seek": 79362, "start": 807.58, "end": 810.78, "text": " Module just like we talked about the other day just makes life easier", "tokens": [48251, 445, 411, 321, 2825, 466, 264, 661, 786, 445, 1669, 993, 3571], "temperature": 0.0, "avg_logprob": -0.17131131155449048, "compression_ratio": 1.7530364372469636, "no_speech_prob": 1.1842537787742913e-05}, {"id": 171, "seek": 79362, "start": 811.22, "end": 816.18, "text": " So all the stuff from Rachel and Sylvain's notebook you can see there", "tokens": [407, 439, 264, 1507, 490, 14246, 293, 3902, 14574, 491, 311, 21060, 291, 393, 536, 456], "temperature": 0.0, "avg_logprob": -0.17131131155449048, "compression_ratio": 1.7530364372469636, "no_speech_prob": 1.1842537787742913e-05}, {"id": 172, "seek": 79362, "start": 817.0600000000001, "end": 821.26, "text": " So the one thing I did copy from the notebook was the sector sec RNN with attention", "tokens": [407, 264, 472, 551, 286, 630, 5055, 490, 264, 21060, 390, 264, 6977, 907, 45702, 45, 365, 3202], "temperature": 0.0, "avg_logprob": -0.17131131155449048, "compression_ratio": 1.7530364372469636, "no_speech_prob": 1.1842537787742913e-05}, {"id": 173, "seek": 82126, "start": 821.26, "end": 825.34, "text": " So that's exactly the same like I say, we're not training a model any differently", "tokens": [407, 300, 311, 2293, 264, 912, 411, 286, 584, 11, 321, 434, 406, 3097, 257, 2316, 604, 7614], "temperature": 0.0, "avg_logprob": -0.22399214811103288, "compression_ratio": 1.5642201834862386, "no_speech_prob": 2.3687839529884513e-06}, {"id": 174, "seek": 82126, "start": 826.1, "end": 831.5, "text": " And what I did was I trained a model for only five epochs to make it like particularly crappy", "tokens": [400, 437, 286, 630, 390, 286, 8895, 257, 2316, 337, 787, 1732, 30992, 28346, 281, 652, 309, 411, 4098, 36531], "temperature": 0.0, "avg_logprob": -0.22399214811103288, "compression_ratio": 1.5642201834862386, "no_speech_prob": 2.3687839529884513e-06}, {"id": 175, "seek": 82126, "start": 832.3, "end": 834.34, "text": " Because I wanted to show you the differences", "tokens": [1436, 286, 1415, 281, 855, 291, 264, 7300], "temperature": 0.0, "avg_logprob": -0.22399214811103288, "compression_ratio": 1.5642201834862386, "no_speech_prob": 2.3687839529884513e-06}, {"id": 176, "seek": 82126, "start": 835.54, "end": 837.54, "text": " When you're kind of working with a crappy model", "tokens": [1133, 291, 434, 733, 295, 1364, 365, 257, 36531, 2316], "temperature": 0.0, "avg_logprob": -0.22399214811103288, "compression_ratio": 1.5642201834862386, "no_speech_prob": 2.3687839529884513e-06}, {"id": 177, "seek": 82126, "start": 839.38, "end": 841.38, "text": " And so I took", "tokens": [400, 370, 286, 1890], "temperature": 0.0, "avg_logprob": -0.22399214811103288, "compression_ratio": 1.5642201834862386, "no_speech_prob": 2.3687839529884513e-06}, {"id": 178, "seek": 82126, "start": 842.78, "end": 844.78, "text": " The get predictions", "tokens": [440, 483, 21264], "temperature": 0.0, "avg_logprob": -0.22399214811103288, "compression_ratio": 1.5642201834862386, "no_speech_prob": 2.3687839529884513e-06}, {"id": 179, "seek": 82126, "start": 845.42, "end": 847.42, "text": " Method function that we just looked at", "tokens": [25285, 2445, 300, 321, 445, 2956, 412], "temperature": 0.0, "avg_logprob": -0.22399214811103288, "compression_ratio": 1.5642201834862386, "no_speech_prob": 2.3687839529884513e-06}, {"id": 180, "seek": 84742, "start": 847.42, "end": 853.86, "text": " With that, you know arg max thing, but I also added in the actual raw", "tokens": [2022, 300, 11, 291, 458, 3882, 11469, 551, 11, 457, 286, 611, 3869, 294, 264, 3539, 8936], "temperature": 0.0, "avg_logprob": -0.23264048961882897, "compression_ratio": 1.650943396226415, "no_speech_prob": 6.240832135517849e-06}, {"id": 181, "seek": 84742, "start": 854.8199999999999, "end": 860.86, "text": " Activations the raw probabilities so rather than giving us back three things that reconstructed X Y Z", "tokens": [28550, 763, 264, 8936, 33783, 370, 2831, 813, 2902, 505, 646, 1045, 721, 300, 31499, 292, 1783, 398, 1176], "temperature": 0.0, "avg_logprob": -0.23264048961882897, "compression_ratio": 1.650943396226415, "no_speech_prob": 6.240832135517849e-06}, {"id": 182, "seek": 84742, "start": 861.14, "end": 865.66, "text": " It also gives us the pure probabilities and so that lets us now", "tokens": [467, 611, 2709, 505, 264, 6075, 33783, 293, 370, 300, 6653, 505, 586], "temperature": 0.0, "avg_logprob": -0.23264048961882897, "compression_ratio": 1.650943396226415, "no_speech_prob": 6.240832135517849e-06}, {"id": 183, "seek": 84742, "start": 866.3, "end": 868.18, "text": " write our own", "tokens": [2464, 527, 1065], "temperature": 0.0, "avg_logprob": -0.23264048961882897, "compression_ratio": 1.650943396226415, "no_speech_prob": 6.240832135517849e-06}, {"id": 184, "seek": 84742, "start": 868.18, "end": 870.18, "text": " decoders our own inference decoders", "tokens": [979, 378, 433, 527, 1065, 38253, 979, 378, 433], "temperature": 0.0, "avg_logprob": -0.23264048961882897, "compression_ratio": 1.650943396226415, "no_speech_prob": 6.240832135517849e-06}, {"id": 185, "seek": 84742, "start": 870.74, "end": 872.74, "text": " so then I wrote a", "tokens": [370, 550, 286, 4114, 257], "temperature": 0.0, "avg_logprob": -0.23264048961882897, "compression_ratio": 1.650943396226415, "no_speech_prob": 6.240832135517849e-06}, {"id": 186, "seek": 84742, "start": 873.54, "end": 876.62, "text": " Top K decoder so it's only three lines of code", "tokens": [8840, 591, 979, 19866, 370, 309, 311, 787, 1045, 3876, 295, 3089], "temperature": 0.0, "avg_logprob": -0.23264048961882897, "compression_ratio": 1.650943396226415, "no_speech_prob": 6.240832135517849e-06}, {"id": 187, "seek": 87662, "start": 876.62, "end": 878.62, "text": " and so we just", "tokens": [293, 370, 321, 445], "temperature": 0.0, "avg_logprob": -0.24489447649787455, "compression_ratio": 1.6715686274509804, "no_speech_prob": 4.860380613536108e-06}, {"id": 188, "seek": 87662, "start": 879.0600000000001, "end": 881.0600000000001, "text": " Grab the output activations", "tokens": [20357, 264, 5598, 2430, 763], "temperature": 0.0, "avg_logprob": -0.24489447649787455, "compression_ratio": 1.6715686274509804, "no_speech_prob": 4.860380613536108e-06}, {"id": 189, "seek": 87662, "start": 881.46, "end": 888.58, "text": " We throw them through a soft max. I'm to turn them into probabilities and then we simply call the pie torch top K", "tokens": [492, 3507, 552, 807, 257, 2787, 11469, 13, 286, 478, 281, 1261, 552, 666, 33783, 293, 550, 321, 2935, 818, 264, 1730, 27822, 1192, 591], "temperature": 0.0, "avg_logprob": -0.24489447649787455, "compression_ratio": 1.6715686274509804, "no_speech_prob": 4.860380613536108e-06}, {"id": 190, "seek": 87662, "start": 889.62, "end": 892.1, "text": " method which will just give me the", "tokens": [3170, 597, 486, 445, 976, 385, 264], "temperature": 0.0, "avg_logprob": -0.24489447649787455, "compression_ratio": 1.6715686274509804, "no_speech_prob": 4.860380613536108e-06}, {"id": 191, "seek": 87662, "start": 892.74, "end": 895.54, "text": " top K highest probabilities", "tokens": [1192, 591, 6343, 33783], "temperature": 0.0, "avg_logprob": -0.24489447649787455, "compression_ratio": 1.6715686274509804, "no_speech_prob": 4.860380613536108e-06}, {"id": 192, "seek": 87662, "start": 896.5, "end": 900.82, "text": " giving me both the probabilities and their indexes and then I", "tokens": [2902, 385, 1293, 264, 33783, 293, 641, 8186, 279, 293, 550, 286], "temperature": 0.0, "avg_logprob": -0.24489447649787455, "compression_ratio": 1.6715686274509804, "no_speech_prob": 4.860380613536108e-06}, {"id": 193, "seek": 87662, "start": 901.1800000000001, "end": 905.3, "text": " So top case sampling just randomly picks one of those top K", "tokens": [407, 1192, 1389, 21179, 445, 16979, 16137, 472, 295, 729, 1192, 591], "temperature": 0.0, "avg_logprob": -0.24489447649787455, "compression_ratio": 1.6715686274509804, "no_speech_prob": 4.860380613536108e-06}, {"id": 194, "seek": 90530, "start": 905.3, "end": 906.6999999999999, "text": " with", "tokens": [365], "temperature": 0.0, "avg_logprob": -0.22784494218372164, "compression_ratio": 1.696629213483146, "no_speech_prob": 5.539154130929091e-07}, {"id": 195, "seek": 90530, "start": 906.6999999999999, "end": 909.4599999999999, "text": " Like literally just a random integer", "tokens": [1743, 3736, 445, 257, 4974, 24922], "temperature": 0.0, "avg_logprob": -0.22784494218372164, "compression_ratio": 1.696629213483146, "no_speech_prob": 5.539154130929091e-07}, {"id": 196, "seek": 90530, "start": 910.2199999999999, "end": 912.2199999999999, "text": " So that's all top case sampling is", "tokens": [407, 300, 311, 439, 1192, 1389, 21179, 307], "temperature": 0.0, "avg_logprob": -0.22784494218372164, "compression_ratio": 1.696629213483146, "no_speech_prob": 5.539154130929091e-07}, {"id": 197, "seek": 90530, "start": 913.38, "end": 917.54, "text": " so if you want to use top case sampling you need to then put it into a", "tokens": [370, 498, 291, 528, 281, 764, 1192, 1389, 21179, 291, 643, 281, 550, 829, 309, 666, 257], "temperature": 0.0, "avg_logprob": -0.22784494218372164, "compression_ratio": 1.696629213483146, "no_speech_prob": 5.539154130929091e-07}, {"id": 198, "seek": 90530, "start": 918.66, "end": 921.6999999999999, "text": " Decoder so what I did then was I literally just", "tokens": [12427, 19866, 370, 437, 286, 630, 550, 390, 286, 3736, 445], "temperature": 0.0, "avg_logprob": -0.22784494218372164, "compression_ratio": 1.696629213483146, "no_speech_prob": 5.539154130929091e-07}, {"id": 199, "seek": 90530, "start": 922.54, "end": 927.06, "text": " copied and pasted the decode method and put it into a new function and", "tokens": [25365, 293, 1791, 292, 264, 979, 1429, 3170, 293, 829, 309, 666, 257, 777, 2445, 293], "temperature": 0.0, "avg_logprob": -0.22784494218372164, "compression_ratio": 1.696629213483146, "no_speech_prob": 5.539154130929091e-07}, {"id": 200, "seek": 90530, "start": 927.8199999999999, "end": 929.5799999999999, "text": " Then I replaced", "tokens": [1396, 286, 10772], "temperature": 0.0, "avg_logprob": -0.22784494218372164, "compression_ratio": 1.696629213483146, "no_speech_prob": 5.539154130929091e-07}, {"id": 201, "seek": 90530, "start": 929.5799999999999, "end": 931.5799999999999, "text": " the line which says", "tokens": [264, 1622, 597, 1619], "temperature": 0.0, "avg_logprob": -0.22784494218372164, "compression_ratio": 1.696629213483146, "no_speech_prob": 5.539154130929091e-07}, {"id": 202, "seek": 93158, "start": 931.58, "end": 939.3000000000001, "text": " In the decoder I replaced the line that says", "tokens": [682, 264, 979, 19866, 286, 10772, 264, 1622, 300, 1619], "temperature": 0.0, "avg_logprob": -0.6429657846126916, "compression_ratio": 1.5806451612903225, "no_speech_prob": 1.6280353065667441e-06}, {"id": 203, "seek": 93158, "start": 942.3000000000001, "end": 949.0600000000001, "text": " Dot max and I replaced it with a line that says", "tokens": [38753, 11469, 293, 286, 10772, 309, 365, 257, 1622, 300, 1619], "temperature": 0.0, "avg_logprob": -0.6429657846126916, "compression_ratio": 1.5806451612903225, "no_speech_prob": 1.6280353065667441e-06}, {"id": 204, "seek": 93158, "start": 953.34, "end": 955.34, "text": " Select top K", "tokens": [13638, 1192, 591], "temperature": 0.0, "avg_logprob": -0.6429657846126916, "compression_ratio": 1.5806451612903225, "no_speech_prob": 1.6280353065667441e-06}, {"id": 205, "seek": 93158, "start": 955.34, "end": 960.94, "text": " Okay, so that would give us top case sampling which is this which is the top case sampling", "tokens": [1033, 11, 370, 300, 576, 976, 505, 1192, 1389, 21179, 597, 307, 341, 597, 307, 264, 1192, 1389, 21179], "temperature": 0.0, "avg_logprob": -0.6429657846126916, "compression_ratio": 1.5806451612903225, "no_speech_prob": 1.6280353065667441e-06}, {"id": 206, "seek": 96094, "start": 960.94, "end": 962.94, "text": " this would be the same as", "tokens": [341, 576, 312, 264, 912, 382], "temperature": 0.0, "avg_logprob": -0.1798124005717616, "compression_ratio": 1.619047619047619, "no_speech_prob": 5.014703219785588e-06}, {"id": 207, "seek": 96094, "start": 963.7, "end": 965.7, "text": " GPT-2 and", "tokens": [26039, 51, 12, 17, 293], "temperature": 0.0, "avg_logprob": -0.1798124005717616, "compression_ratio": 1.619047619047619, "no_speech_prob": 5.014703219785588e-06}, {"id": 208, "seek": 96094, "start": 967.2600000000001, "end": 971.2, "text": " So which is not bad so this paper asked the question though like", "tokens": [407, 597, 307, 406, 1578, 370, 341, 3035, 2351, 264, 1168, 1673, 411], "temperature": 0.0, "avg_logprob": -0.1798124005717616, "compression_ratio": 1.619047619047619, "no_speech_prob": 5.014703219785588e-06}, {"id": 209, "seek": 96094, "start": 971.6600000000001, "end": 978.46, "text": " Well, we thought beam search was meant to be the best way to do inference. Why is beam search so terrible?", "tokens": [1042, 11, 321, 1194, 14269, 3164, 390, 4140, 281, 312, 264, 1151, 636, 281, 360, 38253, 13, 1545, 307, 14269, 3164, 370, 6237, 30], "temperature": 0.0, "avg_logprob": -0.1798124005717616, "compression_ratio": 1.619047619047619, "no_speech_prob": 5.014703219785588e-06}, {"id": 210, "seek": 96094, "start": 978.46, "end": 985.62, "text": " Why is it creating so much repetition as we saw in the examples Rachel showed last week and what they found was interesting", "tokens": [1545, 307, 309, 4084, 370, 709, 30432, 382, 321, 1866, 294, 264, 5110, 14246, 4712, 1036, 1243, 293, 437, 436, 1352, 390, 1880], "temperature": 0.0, "avg_logprob": -0.1798124005717616, "compression_ratio": 1.619047619047619, "no_speech_prob": 5.014703219785588e-06}, {"id": 211, "seek": 96094, "start": 986.5, "end": 988.7, "text": " They looked at a few examples of sentences", "tokens": [814, 2956, 412, 257, 1326, 5110, 295, 16579], "temperature": 0.0, "avg_logprob": -0.1798124005717616, "compression_ratio": 1.619047619047619, "no_speech_prob": 5.014703219785588e-06}, {"id": 212, "seek": 98870, "start": 988.7, "end": 993.7800000000001, "text": " Such as this one which is actual human generated text, right?", "tokens": [9653, 382, 341, 472, 597, 307, 3539, 1952, 10833, 2487, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.20854136727072975, "compression_ratio": 1.6641509433962265, "no_speech_prob": 5.09363826495246e-06}, {"id": 213, "seek": 98870, "start": 994.3000000000001, "end": 999.1800000000001, "text": " Are you cold? He asked his voice full of concern. I just shrugged and squeezed my eyes shut", "tokens": [2014, 291, 3554, 30, 634, 2351, 702, 3177, 1577, 295, 3136, 13, 286, 445, 9884, 697, 3004, 293, 39470, 452, 2575, 5309], "temperature": 0.0, "avg_logprob": -0.20854136727072975, "compression_ratio": 1.6641509433962265, "no_speech_prob": 5.09363826495246e-06}, {"id": 214, "seek": 98870, "start": 999.1800000000001, "end": 1005.34, "text": " I saw Kojas blowing eyes and sword and what they did is for each time step for each word", "tokens": [286, 1866, 10509, 19221, 15068, 2575, 293, 10576, 293, 437, 436, 630, 307, 337, 1184, 565, 1823, 337, 1184, 1349], "temperature": 0.0, "avg_logprob": -0.20854136727072975, "compression_ratio": 1.6641509433962265, "no_speech_prob": 5.09363826495246e-06}, {"id": 215, "seek": 98870, "start": 1005.74, "end": 1007.74, "text": " They said in a language model", "tokens": [814, 848, 294, 257, 2856, 2316], "temperature": 0.0, "avg_logprob": -0.20854136727072975, "compression_ratio": 1.6641509433962265, "no_speech_prob": 5.09363826495246e-06}, {"id": 216, "seek": 98870, "start": 1008.1, "end": 1013.6600000000001, "text": " What's the probability of the word the human picked and what they found is sometimes as you expect?", "tokens": [708, 311, 264, 8482, 295, 264, 1349, 264, 1952, 6183, 293, 437, 436, 1352, 307, 2171, 382, 291, 2066, 30], "temperature": 0.0, "avg_logprob": -0.20854136727072975, "compression_ratio": 1.6641509433962265, "no_speech_prob": 5.09363826495246e-06}, {"id": 217, "seek": 98870, "start": 1013.7800000000001, "end": 1018.1400000000001, "text": " The probability is right up close to one. The blue is the human text", "tokens": [440, 8482, 307, 558, 493, 1998, 281, 472, 13, 440, 3344, 307, 264, 1952, 2487], "temperature": 0.0, "avg_logprob": -0.20854136727072975, "compression_ratio": 1.6641509433962265, "no_speech_prob": 5.09363826495246e-06}, {"id": 218, "seek": 101814, "start": 1018.14, "end": 1023.26, "text": " But very often the the human is picking extremely unlikely words", "tokens": [583, 588, 2049, 264, 264, 1952, 307, 8867, 4664, 17518, 2283], "temperature": 0.0, "avg_logprob": -0.2583058980795053, "compression_ratio": 1.8504273504273505, "no_speech_prob": 1.3496896826836746e-06}, {"id": 219, "seek": 101814, "start": 1024.02, "end": 1025.62, "text": " for example", "tokens": [337, 1365], "temperature": 0.0, "avg_logprob": -0.2583058980795053, "compression_ratio": 1.8504273504273505, "no_speech_prob": 1.3496896826836746e-06}, {"id": 220, "seek": 101814, "start": 1025.62, "end": 1028.3, "text": " Kojas, I've never even heard that name before", "tokens": [10509, 19221, 11, 286, 600, 1128, 754, 2198, 300, 1315, 949], "temperature": 0.0, "avg_logprob": -0.2583058980795053, "compression_ratio": 1.8504273504273505, "no_speech_prob": 1.3496896826836746e-06}, {"id": 221, "seek": 101814, "start": 1029.1, "end": 1035.3, "text": " Where else beam search by definition? This is the probability of beam search beam search picks a very very likely", "tokens": [2305, 1646, 14269, 3164, 538, 7123, 30, 639, 307, 264, 8482, 295, 14269, 3164, 14269, 3164, 16137, 257, 588, 588, 3700], "temperature": 0.0, "avg_logprob": -0.2583058980795053, "compression_ratio": 1.8504273504273505, "no_speech_prob": 1.3496896826836746e-06}, {"id": 222, "seek": 101814, "start": 1036.3, "end": 1040.82, "text": " Thing at every point. So here's the beam search generation who looks at the clouds who looks at the clouds", "tokens": [30902, 412, 633, 935, 13, 407, 510, 311, 264, 14269, 3164, 5125, 567, 1542, 412, 264, 12193, 567, 1542, 412, 264, 12193], "temperature": 0.0, "avg_logprob": -0.2583058980795053, "compression_ratio": 1.8504273504273505, "no_speech_prob": 1.3496896826836746e-06}, {"id": 223, "seek": 101814, "start": 1040.82, "end": 1042.98, "text": " He looks at the clouds. He looks at the cloud. Okay, so", "tokens": [634, 1542, 412, 264, 12193, 13, 634, 1542, 412, 264, 4588, 13, 1033, 11, 370], "temperature": 0.0, "avg_logprob": -0.2583058980795053, "compression_ratio": 1.8504273504273505, "no_speech_prob": 1.3496896826836746e-06}, {"id": 224, "seek": 101814, "start": 1043.82, "end": 1045.82, "text": " Actually beam search it turns out", "tokens": [5135, 14269, 3164, 309, 4523, 484], "temperature": 0.0, "avg_logprob": -0.2583058980795053, "compression_ratio": 1.8504273504273505, "no_speech_prob": 1.3496896826836746e-06}, {"id": 225, "seek": 104582, "start": 1045.82, "end": 1047.82, "text": " generates", "tokens": [23815], "temperature": 0.0, "avg_logprob": -0.2906706203113903, "compression_ratio": 1.4777070063694266, "no_speech_prob": 3.340452849442954e-06}, {"id": 226, "seek": 104582, "start": 1048.34, "end": 1055.6599999999999, "text": " Very likely per time step speech but very unlikely distributions", "tokens": [4372, 3700, 680, 565, 1823, 6218, 457, 588, 17518, 37870], "temperature": 0.0, "avg_logprob": -0.2906706203113903, "compression_ratio": 1.4777070063694266, "no_speech_prob": 3.340452849442954e-06}, {"id": 227, "seek": 104582, "start": 1057.8999999999999, "end": 1059.8999999999999, "text": " So this paper", "tokens": [407, 341, 3035], "temperature": 0.0, "avg_logprob": -0.2906706203113903, "compression_ratio": 1.4777070063694266, "no_speech_prob": 3.340452849442954e-06}, {"id": 228, "seek": 104582, "start": 1061.3, "end": 1067.62, "text": " Makes a suggestion as to how we avoid that and it's basically to use", "tokens": [25245, 257, 16541, 382, 281, 577, 321, 5042, 300, 293, 309, 311, 1936, 281, 764], "temperature": 0.0, "avg_logprob": -0.2906706203113903, "compression_ratio": 1.4777070063694266, "no_speech_prob": 3.340452849442954e-06}, {"id": 229, "seek": 104582, "start": 1068.98, "end": 1072.82, "text": " So they have a number of sections which describe like why does beam search", "tokens": [407, 436, 362, 257, 1230, 295, 10863, 597, 6786, 411, 983, 775, 14269, 3164], "temperature": 0.0, "avg_logprob": -0.2906706203113903, "compression_ratio": 1.4777070063694266, "no_speech_prob": 3.340452849442954e-06}, {"id": 230, "seek": 107282, "start": 1072.82, "end": 1079.7, "text": " Have this problem, but what they then do which is the main bit I'm interested in is how do we fix it?", "tokens": [3560, 341, 1154, 11, 457, 437, 436, 550, 360, 597, 307, 264, 2135, 857, 286, 478, 3102, 294, 307, 577, 360, 321, 3191, 309, 30], "temperature": 0.0, "avg_logprob": -0.24473850727081298, "compression_ratio": 1.7173913043478262, "no_speech_prob": 2.3320462787523866e-06}, {"id": 231, "seek": 107282, "start": 1082.9399999999998, "end": 1084.9399999999998, "text": " And", "tokens": [400], "temperature": 0.0, "avg_logprob": -0.24473850727081298, "compression_ratio": 1.7173913043478262, "no_speech_prob": 2.3320462787523866e-06}, {"id": 232, "seek": 107282, "start": 1085.62, "end": 1088.78, "text": " They propose something they call nuclear sampling and", "tokens": [814, 17421, 746, 436, 818, 8179, 21179, 293], "temperature": 0.0, "avg_logprob": -0.24473850727081298, "compression_ratio": 1.7173913043478262, "no_speech_prob": 2.3320462787523866e-06}, {"id": 233, "seek": 107282, "start": 1089.6599999999999, "end": 1091.6599999999999, "text": " nuclear sampling is", "tokens": [8179, 21179, 307], "temperature": 0.0, "avg_logprob": -0.24473850727081298, "compression_ratio": 1.7173913043478262, "no_speech_prob": 2.3320462787523866e-06}, {"id": 234, "seek": 107282, "start": 1092.4199999999998, "end": 1095.58, "text": " Basically top-k sampling but they point out something", "tokens": [8537, 1192, 12, 74, 21179, 457, 436, 935, 484, 746], "temperature": 0.0, "avg_logprob": -0.24473850727081298, "compression_ratio": 1.7173913043478262, "no_speech_prob": 2.3320462787523866e-06}, {"id": 235, "seek": 107282, "start": 1096.26, "end": 1100.74, "text": " problematic about top-k sampling which is if you're doing top-k sampling with K of", "tokens": [19011, 466, 1192, 12, 74, 21179, 597, 307, 498, 291, 434, 884, 1192, 12, 74, 21179, 365, 591, 295], "temperature": 0.0, "avg_logprob": -0.24473850727081298, "compression_ratio": 1.7173913043478262, "no_speech_prob": 2.3320462787523866e-06}, {"id": 236, "seek": 110074, "start": 1100.74, "end": 1107.98, "text": " 5 say and your language model or translation model says I am 99% sure", "tokens": [1025, 584, 293, 428, 2856, 2316, 420, 12853, 2316, 1619, 286, 669, 11803, 4, 988], "temperature": 0.0, "avg_logprob": -0.28848154951886434, "compression_ratio": 1.4974874371859297, "no_speech_prob": 3.3405026442778762e-06}, {"id": 237, "seek": 110074, "start": 1108.66, "end": 1112.26, "text": " The next token in this sentence is a full stop", "tokens": [440, 958, 14862, 294, 341, 8174, 307, 257, 1577, 1590], "temperature": 0.0, "avg_logprob": -0.28848154951886434, "compression_ratio": 1.4974874371859297, "no_speech_prob": 3.3405026442778762e-06}, {"id": 238, "seek": 110074, "start": 1112.98, "end": 1116.3, "text": " Then you shouldn't do top-k sampling at that point, right?", "tokens": [1396, 291, 4659, 380, 360, 1192, 12, 74, 21179, 412, 300, 935, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.28848154951886434, "compression_ratio": 1.4974874371859297, "no_speech_prob": 3.3405026442778762e-06}, {"id": 239, "seek": 110074, "start": 1116.3, "end": 1121.9, "text": " You shouldn't pick options 2 3 4 or 5 because option 1 is almost certain", "tokens": [509, 4659, 380, 1888, 3956, 568, 805, 1017, 420, 1025, 570, 3614, 502, 307, 1920, 1629], "temperature": 0.0, "avg_logprob": -0.28848154951886434, "compression_ratio": 1.4974874371859297, "no_speech_prob": 3.3405026442778762e-06}, {"id": 240, "seek": 110074, "start": 1122.66, "end": 1125.02, "text": " All right, so top-k sampling", "tokens": [1057, 558, 11, 370, 1192, 12, 74, 21179], "temperature": 0.0, "avg_logprob": -0.28848154951886434, "compression_ratio": 1.4974874371859297, "no_speech_prob": 3.3405026442778762e-06}, {"id": 241, "seek": 110074, "start": 1126.5, "end": 1128.34, "text": " Has its own problems", "tokens": [8646, 1080, 1065, 2740], "temperature": 0.0, "avg_logprob": -0.28848154951886434, "compression_ratio": 1.4974874371859297, "no_speech_prob": 3.3405026442778762e-06}, {"id": 242, "seek": 112834, "start": 1128.34, "end": 1134.1799999999998, "text": " So they suggest something which is basically a top-k sampling with a dynamic", "tokens": [407, 436, 3402, 746, 597, 307, 1936, 257, 1192, 12, 74, 21179, 365, 257, 8546], "temperature": 0.0, "avg_logprob": -0.19062042236328125, "compression_ratio": 1.7192118226600985, "no_speech_prob": 9.874565876089036e-07}, {"id": 243, "seek": 112834, "start": 1134.5, "end": 1140.86, "text": " K so changing the value of K each time and specifically what they do is they say let's do top-k sampling", "tokens": [591, 370, 4473, 264, 2158, 295, 591, 1184, 565, 293, 4682, 437, 436, 360, 307, 436, 584, 718, 311, 360, 1192, 12, 74, 21179], "temperature": 0.0, "avg_logprob": -0.19062042236328125, "compression_ratio": 1.7192118226600985, "no_speech_prob": 9.874565876089036e-07}, {"id": 244, "seek": 112834, "start": 1140.86, "end": 1142.86, "text": " But we will pick K", "tokens": [583, 321, 486, 1888, 591], "temperature": 0.0, "avg_logprob": -0.19062042236328125, "compression_ratio": 1.7192118226600985, "no_speech_prob": 9.874565876089036e-07}, {"id": 245, "seek": 112834, "start": 1143.6599999999999, "end": 1147.78, "text": " such that the total probability of those K words is", "tokens": [1270, 300, 264, 3217, 8482, 295, 729, 591, 2283, 307], "temperature": 0.0, "avg_logprob": -0.19062042236328125, "compression_ratio": 1.7192118226600985, "no_speech_prob": 9.874565876089036e-07}, {"id": 246, "seek": 112834, "start": 1149.4599999999998, "end": 1151.62, "text": " Something and in that case they pick point nine", "tokens": [6595, 293, 294, 300, 1389, 436, 1888, 935, 4949], "temperature": 0.0, "avg_logprob": -0.19062042236328125, "compression_ratio": 1.7192118226600985, "no_speech_prob": 9.874565876089036e-07}, {"id": 247, "seek": 112834, "start": 1152.5, "end": 1155.6399999999999, "text": " So they basically so they call it top-p sampling", "tokens": [407, 436, 1936, 370, 436, 818, 309, 1192, 12, 79, 21179], "temperature": 0.0, "avg_logprob": -0.19062042236328125, "compression_ratio": 1.7192118226600985, "no_speech_prob": 9.874565876089036e-07}, {"id": 248, "seek": 115564, "start": 1155.64, "end": 1162.5200000000002, "text": " So they say let's pick a K so that the words we choose from have a probability of", "tokens": [407, 436, 584, 718, 311, 1888, 257, 591, 370, 300, 264, 2283, 321, 2826, 490, 362, 257, 8482, 295], "temperature": 0.0, "avg_logprob": -0.21574971614739832, "compression_ratio": 1.6198830409356726, "no_speech_prob": 2.561244400567375e-06}, {"id": 249, "seek": 115564, "start": 1163.88, "end": 1165.88, "text": " point nine", "tokens": [935, 4949], "temperature": 0.0, "avg_logprob": -0.21574971614739832, "compression_ratio": 1.6198830409356726, "no_speech_prob": 2.561244400567375e-06}, {"id": 250, "seek": 115564, "start": 1166.76, "end": 1173.0, "text": " So in the case that there was one word that already has a", "tokens": [407, 294, 264, 1389, 300, 456, 390, 472, 1349, 300, 1217, 575, 257], "temperature": 0.0, "avg_logprob": -0.21574971614739832, "compression_ratio": 1.6198830409356726, "no_speech_prob": 2.561244400567375e-06}, {"id": 251, "seek": 115564, "start": 1174.3200000000002, "end": 1175.92, "text": " 0.99 probability", "tokens": [1958, 13, 8494, 8482], "temperature": 0.0, "avg_logprob": -0.21574971614739832, "compression_ratio": 1.6198830409356726, "no_speech_prob": 2.561244400567375e-06}, {"id": 252, "seek": 115564, "start": 1175.92, "end": 1177.92, "text": " Then K is 1", "tokens": [1396, 591, 307, 502], "temperature": 0.0, "avg_logprob": -0.21574971614739832, "compression_ratio": 1.6198830409356726, "no_speech_prob": 2.561244400567375e-06}, {"id": 253, "seek": 115564, "start": 1177.92, "end": 1183.68, "text": " So we just pick that one every time in the case where our model has no idea what the next word is", "tokens": [407, 321, 445, 1888, 300, 472, 633, 565, 294, 264, 1389, 689, 527, 2316, 575, 572, 1558, 437, 264, 958, 1349, 307], "temperature": 0.0, "avg_logprob": -0.21574971614739832, "compression_ratio": 1.6198830409356726, "no_speech_prob": 2.561244400567375e-06}, {"id": 254, "seek": 118368, "start": 1183.68, "end": 1185.68, "text": " and it's got like", "tokens": [293, 309, 311, 658, 411], "temperature": 0.0, "avg_logprob": -0.2508297562599182, "compression_ratio": 1.4797297297297298, "no_speech_prob": 3.7853012599953217e-06}, {"id": 255, "seek": 118368, "start": 1185.8400000000001, "end": 1191.76, "text": " 20 things that all have like a point oh two probability and they're they're the top then it might be a top", "tokens": [945, 721, 300, 439, 362, 411, 257, 935, 1954, 732, 8482, 293, 436, 434, 436, 434, 264, 1192, 550, 309, 1062, 312, 257, 1192], "temperature": 0.0, "avg_logprob": -0.2508297562599182, "compression_ratio": 1.4797297297297298, "no_speech_prob": 3.7853012599953217e-06}, {"id": 256, "seek": 118368, "start": 1192.1200000000001, "end": 1194.1200000000001, "text": " You know K of 20", "tokens": [509, 458, 591, 295, 945], "temperature": 0.0, "avg_logprob": -0.2508297562599182, "compression_ratio": 1.4797297297297298, "no_speech_prob": 3.7853012599953217e-06}, {"id": 257, "seek": 118368, "start": 1195.0800000000002, "end": 1197.8600000000001, "text": " So here is", "tokens": [407, 510, 307], "temperature": 0.0, "avg_logprob": -0.2508297562599182, "compression_ratio": 1.4797297297297298, "no_speech_prob": 3.7853012599953217e-06}, {"id": 258, "seek": 118368, "start": 1206.48, "end": 1210.68, "text": " So now we've taken select top K and we turn it into select nucleus", "tokens": [407, 586, 321, 600, 2726, 3048, 1192, 591, 293, 321, 1261, 309, 666, 3048, 28055], "temperature": 0.0, "avg_logprob": -0.2508297562599182, "compression_ratio": 1.4797297297297298, "no_speech_prob": 3.7853012599953217e-06}, {"id": 259, "seek": 121068, "start": 1210.68, "end": 1218.44, "text": " That's what they call this algorithm and so it's interesting as it's often the case when you implement a paper", "tokens": [663, 311, 437, 436, 818, 341, 9284, 293, 370, 309, 311, 1880, 382, 309, 311, 2049, 264, 1389, 562, 291, 4445, 257, 3035], "temperature": 0.0, "avg_logprob": -0.20105432328723727, "compression_ratio": 1.7431693989071038, "no_speech_prob": 5.173865702090552e-06}, {"id": 260, "seek": 121068, "start": 1218.5600000000002, "end": 1223.1200000000001, "text": " In code the thing you implement is often just three or four lines of code", "tokens": [682, 3089, 264, 551, 291, 4445, 307, 2049, 445, 1045, 420, 1451, 3876, 295, 3089], "temperature": 0.0, "avg_logprob": -0.20105432328723727, "compression_ratio": 1.7431693989071038, "no_speech_prob": 5.173865702090552e-06}, {"id": 261, "seek": 121068, "start": 1223.1200000000001, "end": 1227.92, "text": " so in this case we have seven lines of code and the step one again is", "tokens": [370, 294, 341, 1389, 321, 362, 3407, 3876, 295, 3089, 293, 264, 1823, 472, 797, 307], "temperature": 0.0, "avg_logprob": -0.20105432328723727, "compression_ratio": 1.7431693989071038, "no_speech_prob": 5.173865702090552e-06}, {"id": 262, "seek": 121068, "start": 1228.64, "end": 1230.64, "text": " do a softmax and", "tokens": [360, 257, 2787, 41167, 293], "temperature": 0.0, "avg_logprob": -0.20105432328723727, "compression_ratio": 1.7431693989071038, "no_speech_prob": 5.173865702090552e-06}, {"id": 263, "seek": 121068, "start": 1230.64, "end": 1232.64, "text": " then step two is", "tokens": [550, 1823, 732, 307], "temperature": 0.0, "avg_logprob": -0.20105432328723727, "compression_ratio": 1.7431693989071038, "no_speech_prob": 5.173865702090552e-06}, {"id": 264, "seek": 121068, "start": 1232.72, "end": 1235.76, "text": " get the indexes which sort the", "tokens": [483, 264, 8186, 279, 597, 1333, 264], "temperature": 0.0, "avg_logprob": -0.20105432328723727, "compression_ratio": 1.7431693989071038, "no_speech_prob": 5.173865702090552e-06}, {"id": 265, "seek": 123576, "start": 1235.76, "end": 1242.6, "text": " the probabilities because we want to kind of do a cumulative sum to find how many K's we need and", "tokens": [264, 33783, 570, 321, 528, 281, 733, 295, 360, 257, 38379, 2408, 281, 915, 577, 867, 591, 311, 321, 643, 293], "temperature": 0.0, "avg_logprob": -0.24874038200873833, "compression_ratio": 1.6858638743455496, "no_speech_prob": 3.138125293844496e-06}, {"id": 266, "seek": 123576, "start": 1242.84, "end": 1244.84, "text": " Then just go through each one", "tokens": [1396, 445, 352, 807, 1184, 472], "temperature": 0.0, "avg_logprob": -0.24874038200873833, "compression_ratio": 1.6858638743455496, "no_speech_prob": 3.138125293844496e-06}, {"id": 267, "seek": 123576, "start": 1245.32, "end": 1250.84, "text": " calculating the cumulative sum of the probabilities and when your cumulative sum is above P", "tokens": [28258, 264, 38379, 2408, 295, 264, 33783, 293, 562, 428, 38379, 2408, 307, 3673, 430], "temperature": 0.0, "avg_logprob": -0.24874038200873833, "compression_ratio": 1.6858638743455496, "no_speech_prob": 3.138125293844496e-06}, {"id": 268, "seek": 123576, "start": 1251.8, "end": 1256.3799999999999, "text": " Then pick a random choice of the ones you've seen so far", "tokens": [1396, 1888, 257, 4974, 3922, 295, 264, 2306, 291, 600, 1612, 370, 1400], "temperature": 0.0, "avg_logprob": -0.24874038200873833, "compression_ratio": 1.6858638743455496, "no_speech_prob": 3.138125293844496e-06}, {"id": 269, "seek": 123576, "start": 1257.28, "end": 1262.08, "text": " And that's it that is nuclear sampling and so", "tokens": [400, 300, 311, 309, 300, 307, 8179, 21179, 293, 370], "temperature": 0.0, "avg_logprob": -0.24874038200873833, "compression_ratio": 1.6858638743455496, "no_speech_prob": 3.138125293844496e-06}, {"id": 270, "seek": 126208, "start": 1262.08, "end": 1267.98, "text": " If we pick the greedy sampling that we used in the previous", "tokens": [759, 321, 1888, 264, 28228, 21179, 300, 321, 1143, 294, 264, 3894], "temperature": 0.0, "avg_logprob": -0.26460126080090485, "compression_ratio": 1.5596330275229358, "no_speech_prob": 2.0580366708600195e-06}, {"id": 271, "seek": 126208, "start": 1268.9199999999998, "end": 1276.9199999999998, "text": " Neural translation lesson with our you know, particularly crappy five epoch model and we try to translate", "tokens": [1734, 1807, 12853, 6898, 365, 527, 291, 458, 11, 4098, 36531, 1732, 30992, 339, 2316, 293, 321, 853, 281, 13799], "temperature": 0.0, "avg_logprob": -0.26460126080090485, "compression_ratio": 1.5596330275229358, "no_speech_prob": 2.0580366708600195e-06}, {"id": 272, "seek": 126208, "start": 1277.6799999999998, "end": 1279.48, "text": " something which is meant to be", "tokens": [746, 597, 307, 4140, 281, 312], "temperature": 0.0, "avg_logprob": -0.26460126080090485, "compression_ratio": 1.5596330275229358, "no_speech_prob": 2.0580366708600195e-06}, {"id": 273, "seek": 126208, "start": 1279.48, "end": 1286.36, "text": " What gaps remain in our knowledge of unknown on which future research should focus the translation is?", "tokens": [708, 15031, 6222, 294, 527, 3601, 295, 9841, 322, 597, 2027, 2132, 820, 1879, 264, 12853, 307, 30], "temperature": 0.0, "avg_logprob": -0.26460126080090485, "compression_ratio": 1.5596330275229358, "no_speech_prob": 2.0580366708600195e-06}, {"id": 274, "seek": 126208, "start": 1287.28, "end": 1290.84, "text": " for greedy so kind of top K where K is 1", "tokens": [337, 28228, 370, 733, 295, 1192, 591, 689, 591, 307, 502], "temperature": 0.0, "avg_logprob": -0.26460126080090485, "compression_ratio": 1.5596330275229358, "no_speech_prob": 2.0580366708600195e-06}, {"id": 275, "seek": 129084, "start": 1290.84, "end": 1292.84, "text": " What gaps are needed in our work?", "tokens": [708, 15031, 366, 2978, 294, 527, 589, 30], "temperature": 0.0, "avg_logprob": -0.5418995666503906, "compression_ratio": 1.5359116022099448, "no_speech_prob": 7.646345693501644e-06}, {"id": 276, "seek": 129084, "start": 1293.24, "end": 1298.08, "text": " And what is the research of the work at what research will be in place to future?", "tokens": [400, 437, 307, 264, 2132, 295, 264, 589, 412, 437, 2132, 486, 312, 294, 1081, 281, 2027, 30], "temperature": 0.0, "avg_logprob": -0.5418995666503906, "compression_ratio": 1.5359116022099448, "no_speech_prob": 7.646345693501644e-06}, {"id": 277, "seek": 129084, "start": 1298.08, "end": 1300.8799999999999, "text": " You can see like the crappy translation model has this", "tokens": [509, 393, 536, 411, 264, 36531, 12853, 2316, 575, 341], "temperature": 0.0, "avg_logprob": -0.5418995666503906, "compression_ratio": 1.5359116022099448, "no_speech_prob": 7.646345693501644e-06}, {"id": 278, "seek": 129084, "start": 1301.76, "end": 1304.36, "text": " You know this repetition problem", "tokens": [509, 458, 341, 30432, 1154], "temperature": 0.0, "avg_logprob": -0.5418995666503906, "compression_ratio": 1.5359116022099448, "no_speech_prob": 7.646345693501644e-06}, {"id": 279, "seek": 129084, "start": 1305.8799999999999, "end": 1308.1999999999998, "text": " If instead we switch out", "tokens": [759, 2602, 321, 3679, 484], "temperature": 0.0, "avg_logprob": -0.5418995666503906, "compression_ratio": 1.5359116022099448, "no_speech_prob": 7.646345693501644e-06}, {"id": 280, "seek": 129084, "start": 1309.3999999999999, "end": 1312.36, "text": " the greedy and we're put in nucleus and", "tokens": [264, 28228, 293, 321, 434, 829, 294, 28055, 293], "temperature": 0.0, "avg_logprob": -0.5418995666503906, "compression_ratio": 1.5359116022099448, "no_speech_prob": 7.646345693501644e-06}, {"id": 281, "seek": 129084, "start": 1313.0, "end": 1314.52, "text": " then", "tokens": [550], "temperature": 0.0, "avg_logprob": -0.5418995666503906, "compression_ratio": 1.5359116022099448, "no_speech_prob": 7.646345693501644e-06}, {"id": 282, "seek": 129084, "start": 1314.52, "end": 1315.8, "text": " call", "tokens": [818], "temperature": 0.0, "avg_logprob": -0.5418995666503906, "compression_ratio": 1.5359116022099448, "no_speech_prob": 7.646345693501644e-06}, {"id": 283, "seek": 131580, "start": 1315.8, "end": 1321.1599999999999, "text": " Predict with decode which as you can see just posted into eval no grad calls that decode", "tokens": [430, 24945, 365, 979, 1429, 597, 382, 291, 393, 536, 445, 9437, 666, 1073, 304, 572, 2771, 5498, 300, 979, 1429], "temperature": 0.0, "avg_logprob": -0.4024407397741559, "compression_ratio": 1.5625, "no_speech_prob": 6.747989118593978e-06}, {"id": 284, "seek": 131580, "start": 1323.8799999999999, "end": 1331.36, "text": " And then returns the reconstructed output we get what gaps are needed in our understanding of work and security and how?", "tokens": [400, 550, 11247, 264, 31499, 292, 5598, 321, 483, 437, 15031, 366, 2978, 294, 527, 3701, 295, 589, 293, 3825, 293, 577, 30], "temperature": 0.0, "avg_logprob": -0.4024407397741559, "compression_ratio": 1.5625, "no_speech_prob": 6.747989118593978e-06}, {"id": 285, "seek": 131580, "start": 1331.76, "end": 1335.08, "text": " Research will need to be put in place. So like it's a much less", "tokens": [10303, 486, 643, 281, 312, 829, 294, 1081, 13, 407, 411, 309, 311, 257, 709, 1570], "temperature": 0.0, "avg_logprob": -0.4024407397741559, "compression_ratio": 1.5625, "no_speech_prob": 6.747989118593978e-06}, {"id": 286, "seek": 131580, "start": 1336.76, "end": 1338.36, "text": " repetition", "tokens": [30432], "temperature": 0.0, "avg_logprob": -0.4024407397741559, "compression_ratio": 1.5625, "no_speech_prob": 6.747989118593978e-06}, {"id": 287, "seek": 131580, "start": 1338.36, "end": 1340.36, "text": " better sounding sentence", "tokens": [1101, 24931, 8174], "temperature": 0.0, "avg_logprob": -0.4024407397741559, "compression_ratio": 1.5625, "no_speech_prob": 6.747989118593978e-06}, {"id": 288, "seek": 131580, "start": 1340.76, "end": 1343.24, "text": " So I kind of just wanted to mention this", "tokens": [407, 286, 733, 295, 445, 1415, 281, 2152, 341], "temperature": 0.0, "avg_logprob": -0.4024407397741559, "compression_ratio": 1.5625, "no_speech_prob": 6.747989118593978e-06}, {"id": 289, "seek": 134324, "start": 1343.24, "end": 1348.32, "text": " I'm not sure like partly because after last class I", "tokens": [286, 478, 406, 988, 411, 17031, 570, 934, 1036, 1508, 286], "temperature": 0.0, "avg_logprob": -0.19362468719482423, "compression_ratio": 1.6270491803278688, "no_speech_prob": 2.4298858988913707e-05}, {"id": 290, "seek": 134324, "start": 1349.72, "end": 1356.0, "text": " Asked the students if anyone is interested in pair programming an algorithm paper, and so we spent the afternoon a couple of hours", "tokens": [12320, 292, 264, 1731, 498, 2878, 307, 3102, 294, 6119, 9410, 364, 9284, 3035, 11, 293, 370, 321, 4418, 264, 6499, 257, 1916, 295, 2496], "temperature": 0.0, "avg_logprob": -0.19362468719482423, "compression_ratio": 1.6270491803278688, "no_speech_prob": 2.4298858988913707e-05}, {"id": 291, "seek": 134324, "start": 1356.0, "end": 1359.48, "text": " I guess it was implementing this so wanted to kind of show you", "tokens": [286, 2041, 309, 390, 18114, 341, 370, 1415, 281, 733, 295, 855, 291], "temperature": 0.0, "avg_logprob": -0.19362468719482423, "compression_ratio": 1.6270491803278688, "no_speech_prob": 2.4298858988913707e-05}, {"id": 292, "seek": 134324, "start": 1360.16, "end": 1366.4, "text": " What that looks like give you an example of a paper that's worth looking at but also just have a kind of discussion about", "tokens": [708, 300, 1542, 411, 976, 291, 364, 1365, 295, 257, 3035, 300, 311, 3163, 1237, 412, 457, 611, 445, 362, 257, 733, 295, 5017, 466], "temperature": 0.0, "avg_logprob": -0.19362468719482423, "compression_ratio": 1.6270491803278688, "no_speech_prob": 2.4298858988913707e-05}, {"id": 293, "seek": 134324, "start": 1367.44, "end": 1369.44, "text": " generation in the translation", "tokens": [5125, 294, 264, 12853], "temperature": 0.0, "avg_logprob": -0.19362468719482423, "compression_ratio": 1.6270491803278688, "no_speech_prob": 2.4298858988913707e-05}, {"id": 294, "seek": 136944, "start": 1369.44, "end": 1377.16, "text": " Notebook so far. We've only talked about how do you train the model? We didn't talk about actually how do you generate?", "tokens": [11633, 2939, 370, 1400, 13, 492, 600, 787, 2825, 466, 577, 360, 291, 3847, 264, 2316, 30, 492, 994, 380, 751, 466, 767, 577, 360, 291, 8460, 30], "temperature": 0.0, "avg_logprob": -0.2534847045212649, "compression_ratio": 1.5813953488372092, "no_speech_prob": 2.225264097432955e-06}, {"id": 295, "seek": 136944, "start": 1378.04, "end": 1380.92, "text": " text and so these techniques of", "tokens": [2487, 293, 370, 613, 7512, 295], "temperature": 0.0, "avg_logprob": -0.2534847045212649, "compression_ratio": 1.5813953488372092, "no_speech_prob": 2.225264097432955e-06}, {"id": 296, "seek": 136944, "start": 1381.76, "end": 1384.68, "text": " beam search and top K and", "tokens": [14269, 3164, 293, 1192, 591, 293], "temperature": 0.0, "avg_logprob": -0.2534847045212649, "compression_ratio": 1.5813953488372092, "no_speech_prob": 2.225264097432955e-06}, {"id": 297, "seek": 136944, "start": 1385.3200000000002, "end": 1388.56, "text": " nucleus top P sampling are the three that you", "tokens": [28055, 1192, 430, 21179, 366, 264, 1045, 300, 291], "temperature": 0.0, "avg_logprob": -0.2534847045212649, "compression_ratio": 1.5813953488372092, "no_speech_prob": 2.225264097432955e-06}, {"id": 298, "seek": 136944, "start": 1389.28, "end": 1390.8400000000001, "text": " probably", "tokens": [1391], "temperature": 0.0, "avg_logprob": -0.2534847045212649, "compression_ratio": 1.5813953488372092, "no_speech_prob": 2.225264097432955e-06}, {"id": 299, "seek": 136944, "start": 1390.8400000000001, "end": 1392.56, "text": " need to know about", "tokens": [643, 281, 458, 466], "temperature": 0.0, "avg_logprob": -0.2534847045212649, "compression_ratio": 1.5813953488372092, "no_speech_prob": 2.225264097432955e-06}, {"id": 300, "seek": 139256, "start": 1392.56, "end": 1399.72, "text": " So that is that so you may have already said this is there a way to I guess generate text or?", "tokens": [407, 300, 307, 300, 370, 291, 815, 362, 1217, 848, 341, 307, 456, 257, 636, 281, 286, 2041, 8460, 2487, 420, 30], "temperature": 0.0, "avg_logprob": -0.26209430036873654, "compression_ratio": 1.6028708133971292, "no_speech_prob": 2.6273510229657404e-05}, {"id": 301, "seek": 139256, "start": 1400.3999999999999, "end": 1404.6399999999999, "text": " If you're generating text without those and just taking the top probability and", "tokens": [759, 291, 434, 17746, 2487, 1553, 729, 293, 445, 1940, 264, 1192, 8482, 293], "temperature": 0.0, "avg_logprob": -0.26209430036873654, "compression_ratio": 1.6028708133971292, "no_speech_prob": 2.6273510229657404e-05}, {"id": 302, "seek": 139256, "start": 1406.6399999999999, "end": 1408.6399999999999, "text": " Do you end up with the repetition", "tokens": [1144, 291, 917, 493, 365, 264, 30432], "temperature": 0.0, "avg_logprob": -0.26209430036873654, "compression_ratio": 1.6028708133971292, "no_speech_prob": 2.6273510229657404e-05}, {"id": 303, "seek": 139256, "start": 1409.2, "end": 1415.6399999999999, "text": " Yes, we saw that last way. Yeah. Yeah, I mean some of that I", "tokens": [1079, 11, 321, 1866, 300, 1036, 636, 13, 865, 13, 865, 11, 286, 914, 512, 295, 300, 286], "temperature": 0.0, "avg_logprob": -0.26209430036873654, "compression_ratio": 1.6028708133971292, "no_speech_prob": 2.6273510229657404e-05}, {"id": 304, "seek": 139256, "start": 1417.1599999999999, "end": 1420.6, "text": " Guess how much does that relate to I know the temperature of like?", "tokens": [17795, 577, 709, 775, 300, 10961, 281, 286, 458, 264, 4292, 295, 411, 30], "temperature": 0.0, "avg_logprob": -0.26209430036873654, "compression_ratio": 1.6028708133971292, "no_speech_prob": 2.6273510229657404e-05}, {"id": 305, "seek": 142060, "start": 1420.6, "end": 1425.4399999999998, "text": " Kind of the how much randomness you injected each step how much can that help with it?", "tokens": [9242, 295, 264, 577, 709, 4974, 1287, 291, 36967, 1184, 1823, 577, 709, 393, 300, 854, 365, 309, 30], "temperature": 0.0, "avg_logprob": -0.24331073760986327, "compression_ratio": 1.458100558659218, "no_speech_prob": 5.422091362561332e-06}, {"id": 306, "seek": 142060, "start": 1427.84, "end": 1429.84, "text": " Yeah, so I guess that's a", "tokens": [865, 11, 370, 286, 2041, 300, 311, 257], "temperature": 0.0, "avg_logprob": -0.24331073760986327, "compression_ratio": 1.458100558659218, "no_speech_prob": 5.422091362561332e-06}, {"id": 307, "seek": 142060, "start": 1430.4399999999998, "end": 1434.8, "text": " Another way you can do it is doing multinomial sampling where you select", "tokens": [3996, 636, 291, 393, 360, 309, 307, 884, 45872, 47429, 21179, 689, 291, 3048], "temperature": 0.0, "avg_logprob": -0.24331073760986327, "compression_ratio": 1.458100558659218, "no_speech_prob": 5.422091362561332e-06}, {"id": 308, "seek": 142060, "start": 1435.24, "end": 1442.08, "text": " From the full set with a probability based on the p values of the softmax", "tokens": [3358, 264, 1577, 992, 365, 257, 8482, 2361, 322, 264, 280, 4190, 295, 264, 2787, 41167], "temperature": 0.0, "avg_logprob": -0.24331073760986327, "compression_ratio": 1.458100558659218, "no_speech_prob": 5.422091362561332e-06}, {"id": 309, "seek": 142060, "start": 1443.56, "end": 1445.56, "text": " I", "tokens": [286], "temperature": 0.0, "avg_logprob": -0.24331073760986327, "compression_ratio": 1.458100558659218, "no_speech_prob": 5.422091362561332e-06}, {"id": 310, "seek": 144556, "start": 1445.56, "end": 1448.1599999999999, "text": " I", "tokens": [286], "temperature": 0.0, "avg_logprob": -0.18897132873535155, "compression_ratio": 1.5509259259259258, "no_speech_prob": 5.682276423613075e-06}, {"id": 311, "seek": 144556, "start": 1448.1599999999999, "end": 1454.6399999999999, "text": " Think again you kind of end up with the problem that the nucleus paper pointed out around like you're going to end up", "tokens": [6557, 797, 291, 733, 295, 917, 493, 365, 264, 1154, 300, 264, 28055, 3035, 10932, 484, 926, 411, 291, 434, 516, 281, 917, 493], "temperature": 0.0, "avg_logprob": -0.18897132873535155, "compression_ratio": 1.5509259259259258, "no_speech_prob": 5.682276423613075e-06}, {"id": 312, "seek": 144556, "start": 1454.84, "end": 1458.32, "text": " Picking overly high probability things too often", "tokens": [430, 10401, 24324, 1090, 8482, 721, 886, 2049], "temperature": 0.0, "avg_logprob": -0.18897132873535155, "compression_ratio": 1.5509259259259258, "no_speech_prob": 5.682276423613075e-06}, {"id": 313, "seek": 144556, "start": 1461.32, "end": 1465.6799999999998, "text": " But you know I guess as you can see this is only a two months old paper, so I would describe this as a", "tokens": [583, 291, 458, 286, 2041, 382, 291, 393, 536, 341, 307, 787, 257, 732, 2493, 1331, 3035, 11, 370, 286, 576, 6786, 341, 382, 257], "temperature": 0.0, "avg_logprob": -0.18897132873535155, "compression_ratio": 1.5509259259259258, "no_speech_prob": 5.682276423613075e-06}, {"id": 314, "seek": 144556, "start": 1466.28, "end": 1468.2, "text": " open area of", "tokens": [1269, 1859, 295], "temperature": 0.0, "avg_logprob": -0.18897132873535155, "compression_ratio": 1.5509259259259258, "no_speech_prob": 5.682276423613075e-06}, {"id": 315, "seek": 144556, "start": 1468.2, "end": 1469.72, "text": " research and", "tokens": [2132, 293], "temperature": 0.0, "avg_logprob": -0.18897132873535155, "compression_ratio": 1.5509259259259258, "no_speech_prob": 5.682276423613075e-06}, {"id": 316, "seek": 144556, "start": 1469.72, "end": 1471.72, "text": " These are some current best practices", "tokens": [1981, 366, 512, 2190, 1151, 7525], "temperature": 0.0, "avg_logprob": -0.18897132873535155, "compression_ratio": 1.5509259259259258, "no_speech_prob": 5.682276423613075e-06}, {"id": 317, "seek": 147172, "start": 1471.72, "end": 1475.1200000000001, "text": " Now I would say that the translation", "tokens": [823, 286, 576, 584, 300, 264, 12853], "temperature": 0.0, "avg_logprob": -0.1806878158726643, "compression_ratio": 1.6639344262295082, "no_speech_prob": 3.5559564821596723e-06}, {"id": 318, "seek": 147172, "start": 1478.04, "end": 1483.68, "text": " Probably beam search is generally still going to be best because it's this like you know it's not at all like", "tokens": [9210, 14269, 3164, 307, 5101, 920, 516, 281, 312, 1151, 570, 309, 311, 341, 411, 291, 458, 309, 311, 406, 412, 439, 411], "temperature": 0.0, "avg_logprob": -0.1806878158726643, "compression_ratio": 1.6639344262295082, "no_speech_prob": 3.5559564821596723e-06}, {"id": 319, "seek": 147172, "start": 1484.76, "end": 1486.52, "text": " open-ended generation", "tokens": [1269, 12, 3502, 5125], "temperature": 0.0, "avg_logprob": -0.1806878158726643, "compression_ratio": 1.6639344262295082, "no_speech_prob": 3.5559564821596723e-06}, {"id": 320, "seek": 147172, "start": 1486.52, "end": 1490.92, "text": " And it's only in this case the only reason it's we kind of needed it to look any good", "tokens": [400, 309, 311, 787, 294, 341, 1389, 264, 787, 1778, 309, 311, 321, 733, 295, 2978, 309, 281, 574, 604, 665], "temperature": 0.0, "avg_logprob": -0.1806878158726643, "compression_ratio": 1.6639344262295082, "no_speech_prob": 3.5559564821596723e-06}, {"id": 321, "seek": 147172, "start": 1491.16, "end": 1495.64, "text": " Was because we have such a crappy translation model because we trained it so quickly on so little data", "tokens": [3027, 570, 321, 362, 1270, 257, 36531, 12853, 2316, 570, 321, 8895, 309, 370, 2661, 322, 370, 707, 1412], "temperature": 0.0, "avg_logprob": -0.1806878158726643, "compression_ratio": 1.6639344262295082, "no_speech_prob": 3.5559564821596723e-06}, {"id": 322, "seek": 149564, "start": 1495.64, "end": 1502.6000000000001, "text": " I'm not sure anybody's tried nucleus sampling on high quality", "tokens": [286, 478, 406, 988, 4472, 311, 3031, 28055, 21179, 322, 1090, 3125], "temperature": 0.0, "avg_logprob": -0.23074207058200588, "compression_ratio": 1.6451612903225807, "no_speech_prob": 6.540132744703442e-06}, {"id": 323, "seek": 149564, "start": 1505.0400000000002, "end": 1509.64, "text": " Translation models it would be interesting to find out but", "tokens": [6531, 24278, 5245, 309, 576, 312, 1880, 281, 915, 484, 457], "temperature": 0.0, "avg_logprob": -0.23074207058200588, "compression_ratio": 1.6451612903225807, "no_speech_prob": 6.540132744703442e-06}, {"id": 324, "seek": 149564, "start": 1511.16, "end": 1514.68, "text": " The other place beam search is used all the time is like speech recognition", "tokens": [440, 661, 1081, 14269, 3164, 307, 1143, 439, 264, 565, 307, 411, 6218, 11150], "temperature": 0.0, "avg_logprob": -0.23074207058200588, "compression_ratio": 1.6451612903225807, "no_speech_prob": 6.540132744703442e-06}, {"id": 325, "seek": 149564, "start": 1515.16, "end": 1520.46, "text": " It's a similar thing right speech recognition is basically translating audio into words", "tokens": [467, 311, 257, 2531, 551, 558, 6218, 11150, 307, 1936, 35030, 6278, 666, 2283], "temperature": 0.0, "avg_logprob": -0.23074207058200588, "compression_ratio": 1.6451612903225807, "no_speech_prob": 6.540132744703442e-06}, {"id": 326, "seek": 149564, "start": 1520.96, "end": 1524.72, "text": " You use the same kinds of approaches the same kind of language you model", "tokens": [509, 764, 264, 912, 3685, 295, 11587, 264, 912, 733, 295, 2856, 291, 2316], "temperature": 0.0, "avg_logprob": -0.23074207058200588, "compression_ratio": 1.6451612903225807, "no_speech_prob": 6.540132744703442e-06}, {"id": 327, "seek": 152472, "start": 1524.72, "end": 1532.44, "text": " But you're trying to find something that the words match the the audio, but also makes sense as a kind of", "tokens": [583, 291, 434, 1382, 281, 915, 746, 300, 264, 2283, 2995, 264, 264, 6278, 11, 457, 611, 1669, 2020, 382, 257, 733, 295], "temperature": 0.0, "avg_logprob": -0.1780674763214894, "compression_ratio": 1.2743362831858407, "no_speech_prob": 2.2819818696007133e-05}, {"id": 328, "seek": 152472, "start": 1533.04, "end": 1535.04, "text": " complete end-to-end sentence", "tokens": [3566, 917, 12, 1353, 12, 521, 8174], "temperature": 0.0, "avg_logprob": -0.1780674763214894, "compression_ratio": 1.2743362831858407, "no_speech_prob": 2.2819818696007133e-05}, {"id": 329, "seek": 153504, "start": 1535.04, "end": 1557.04, "text": " Oh, thank you", "tokens": [50364, 876, 11, 1309, 291, 51464], "temperature": 0.0, "avg_logprob": -0.7299340793064663, "compression_ratio": 0.6190476190476191, "no_speech_prob": 0.0001575312198838219}], "language": "en"}