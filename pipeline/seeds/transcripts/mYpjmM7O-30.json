{"text": " Hello, everyone. My name is Wasim. I am an entrepreneur in residence at Fast AI. I'm currently at the Fast AI headquarters at the moment in Australia, although I'm originally from South Africa, from Cape Town. I'm joined here today by Tanishq. Tanishq works at Stability AI. We've been working together with a couple other people on diffusion models, generative modeling, and that's been super fun. Tanishq, do you want to maybe introduce yourself as well? Yeah. My name is Tanishq. I am a PhD student at UC Davis, but I also work at Stability AI. I've been exploring and playing around with diffusion models for the past several months. It's been great to also explore that with the Fast AI community as well in these last few weeks as well. Awesome. Cool. This talk is for me trying to understand the math behind diffusion. If you've done the Fast AI courses before, you know that you don't need to understand the math to be effective with any of these models. In fact, you don't even need the math to do research, to do novel research and contribute to these. But for me, it came out of interest. I thought it's beautiful what diffusion models were discovered. I think a large part of that was thanks to some really clever math. So I wanted to understand that I don't have a math background and so I want to help describe how I think about it and how you can interpret all of these notations and things. Cool. Yeah. So I can just dive into it, I think. The first bit of math that we see in this paper is Q of x superscript 0. They call this the data distribution. Do you want to mention exactly which paper this is? Right. Good question. This paper is the 2015 paper. Do you remember the authors of that paper, Tanishk? I think it's Jasa Sol Dikstein, who now works at Google, I think. It's from Surya Ganguly's lab. So this was the paper, as far as I understand, that introduced this idea of diffusion. 2015 by those authors. They start out by defining this data distribution and they use this notation. Already, like a lot of people, myself included, find this quite confusing. But let's go through what's described here. So they have an x and in math, x is often used as the input variable, much like y, which is then used often as the output variable. Yeah. The fact that it has a superscript also implies something. So the fact that we have x superscript 0 implies that there might be a sequence of x's and I think it's useful to get comfortable with this idea of simple compact notations implying a lot more than might be obvious at first glance. So x implies that it means something about this quantity, it's an input variable and the 0 implies that there might be other things. So you might have an x1, an x2 and so on, but we might see that. Then the third part is you have q. Q is what we call a probability density function. So the first part here is probability. The question is, what does q have to do with probabilities? Well, it's because usually we use the letter p to describe probability density functions of interest, and then because q is right after that, it's another common one. So it's like how you use x and y, we use p and q. The fact that we use q here instead of p is because it suggests that there might be a p that we'll introduce, and maybe p is the thing that we're modeling, and q is supplementary to that. Does that sound right, Tanishq? Yeah. I think it's also helpful to maybe think about x0 in a more practical, concrete way. Of course, if we work with images, then x0 would be that's what's representing the images. So it's also useful to think about it from that concrete practical approach as well. So x0 might be an MNIST digit. Then we got q. So q, I'll just use this to mean q is some function. So we look at it as a box. It takes in x0, and it gives us the probability that this x0, which is an image, looks like an MNIST digit. So in this case, this would be 0.9 or maybe even, yeah, it's still 0.98. So this is quite high probability that this is an MNIST digit. Hi, this is Jeremy. Can I jump in for a moment? Please do. Oh, thank you. I just wanted to double-check. This looks a lot like the magic API that we had at the start of lesson 1, that you feed in a digit and it gives you back a probability. Is that basically what Q is doing here? Absolutely. It's a magic API. It's a good way to think of it. We couldn't write down what Q is, but we imagine that somebody has it somewhere. So this is a concrete example. If you had to do something to this image, you might get a smaller number here. So another thing worth mentioning here is probability density functions. So these are these magic APIs that give us a number, tells us how likely the thing is. You don't often see them, they don't often make it all the way to your code. In fact, they very rarely will appear in your code. But it turns out that there are very useful ways or tools to work with random quantities. Because they allow you to represent random quantities as functions, just ordinary functions. And because they're functions, you have a whole centuries worth of math to analyze and understand it. So you'll often find probability density functions in papers, and eventually they work out to really simple equations or formulas that end up in your code. Do you want to add anything, Tanish? I think that sounds all correct. Of course, I think you probably will go over some examples of probability density functions, especially relevant to this one. But yeah, it's useful to think about the also the sorts of functions you may have in a simplified case. And that's what we probably are going to talk about next, right? Yeah, yeah, that's exactly what we talk. So we have this Qx0, and then we introduce another one. And like you said, this is going to turn out to have a really nice, simple form. But before that, the next thing we define is QxT, QnxT minus one. So we'll say what we define this to be. But to begin with, this is another probability density function. And this bar over here means it's a conditional probability density function, which you can think of as you are given the thing on the right to calculate probabilities over the thing on the left. In this case, you can think of it as something that takes images. So maybe another magic API and produces other images. But we don't know what these look like yet because we haven't defined over here. And this we would call kind of, you know, Xt minus one, which could be X0. And this would be Xt, which in the X0 case would be X1. Something worth noting here is this notation can be a little bit confusing because we've said Q is one thing earlier. Now we're saying Q is another thing. So this, yeah, I'm going to need your help on this one, Tanisha. I think people would usually, if in the strictest sense, define the first one, you know, like this, maybe. And the second one was a subscript. And this notation that we see here on the left is just a shortcut where they wanted to save the space of writing that and kind of included that, implied it by what was in the practice. Is that true? Yeah, I mean, I think here they use the variables Q and then, of course, later on we see P to kind of describe, as we'll see different aspects of the diffusion model, the sort of different processes of the diffusion model, which we'll see. So I think that's what, you know, they use the same variables to kind of demonstrate this is corresponding to this process. The other variable corresponds to the other process of the diffusion model. So we'll obviously go over that. So I think that's where those variables or those letters are being used in that matter. But if you do want to make it more specific, more clear, yeah, I think that notation is fine as well. Right. OK. Yeah, that makes sense. OK, so let's describe what this Q does to the image on the left to produce the one on the right. So I'll start over here so we have more space. I'll write it out first and then we can go into the details. OK, so kind of like the bar, you can think of this semicolon as, you know, grouping things together. And so you have the things on the left and things on the right. My understanding is these two things on the right are the parameters of the model, sorry, of the probability. And the thing on the left is actually, Dhanesh, could you help me understand what the thing on the left is? Do you know? Right. Well, so this is again like a probability distribution and the thing on the left is a probability distribution for this particular variable. So that's just representing what it is a probability distribution for. And then the stuff on the right are the parameters for this probability distribution. So that's kind of what's going on here. So, like, yeah, any time you have a normal distribution and it's describing some variable, you'll have that sort of notation. It's the normal distribution of some variable. And then these are the parameters that describe that normal distribution. Right. So just to clarify, the bit after the semicolon is the bit that we're kind of used to seeing to describe a normal distribution, which is the mean and variance of the normal distribution. So we're going to be sampling random numbers from that normal distribution according to that mean and that variance. Is that right? Yes, that's correct. Yeah. Mm hmm. Yeah. So we need to describe a bit more there about normal distribution. We kind of, you know, skip past that. So we have this fancy N and fancy letters in math for distributions usually refer to well-known distributions. And the N stands for normal, which is also known as the Gaussian distribution. And it's probably the most well-known probability distribution that you can you can find. And when I say well-known, I mean that these things pop up everywhere. You know, you can do in all sorts of fields measuring all sorts of things. Turns out that they follow roughly something that looks like this distribution. And because they pop up so much, you know, people studied them, studied all of their properties, and we understand them really well now. The reason that they used often in cases like this is because they turns out they have really useful properties and they're easy to work with. Some reasons are they are described by just two parameters. So the mean called the mean and the covariance. Another property is that they have kind of, you know, what people would call thin tails. Which kind of means that they only you only need to describe their behavior in a small region of space. You can kind of just ignore the rest. Do you mind drawing a quick example of a normal distribution? That's a good point. So we have, let's say, our random variable is just one kind of dimensional. So just a single number of floats. This is sort of what the normal distribution would look like. And in this case, that would be our mean. And the variance would sort of describe the width over here. Which in this case, you'd use a small sigma because you're doing a single variable. In our case, we use a capital sigma, which is the symbol for multiple variables or multiple dimensions. And yeah, I also didn't say that this is the Greek letter mu. So capital sigma mu and lowercase sigma. I just wanted to note that typically the lowercase sigma represents the standard deviation, which is the square root of the variance. So for example, sometimes you may see in papers sigma squared and that's just the variance. But they will write it sometimes as sigma squared instead. So it depends on the notation. So sigma is the standard deviation often and sigma squared would then be the variance. Cool. Yeah, we can also show with our example what this would look like. So we start out with an MNIST digit. Put it through this magic API. And what would we get out? Okay, so something we didn't describe is, you know, what does this I mean? Did you want me to talk about that, Wasim? Ah, yes, please. Okay, sure. So because I think this is something which actually can I borrow your pen? It actually came up in the lesson we were doing kind of in an interesting way. So in that lesson. Do you want to get in the video? No, they know what I look like. Oh, well, okay. I'm in the video now. Yeah, in the video. Hi, Tanishk. Nice to see you. Yeah, so in the lesson, like, we did this thing for clip. I don't know if you remember, Wasim, where we had the, you know, the various pictures down here. I'm so embarrassed. You're better at the graphics tablet than I am. And it's my graphics tablet. And we had the various sentences along here. Right. And we said, oh, you know, it'd be kind of cool to like take the dot product of their embeddings. Because like if their dot products are high, that means they're high. The high that means they're similar to each other. And, you know, if we subtracted the means from those first, right, then you've got the dot product. And instead of having images down here, right. What if we had the exact same? The exact same vectors on each side. Then what you've got down here is basically X minus, you know, the average. If it's to check that first, squared. And that is the variance. Right. So that's like the variance for each one of these vectors. But what's interesting, as you pointed out, is that like normally, you know, at high school, when we look at a normal distribution, it looks like this. Right. But you're not just doing one normal distribution. You've got a whole bunch of kind of normal distributions, right, for all of your different pixels. They're the pixels, right, Tanishq? Normal distribution of every pixel. So there's a whole bunch of them. And so one of them might have a normal distribution that's there, and another one might have a normal distribution that's here, and another one might have a normal distribution that's like here. And it's more than that, though, because like it's possible that, you know, two different you know, one pixel tends to be higher when another pixel tends to be higher, or one pixel tends to be higher when another pixel is lower. So it actually can kind of create this like surface, you know, in n-dimensional space, where n is the number of pixels. So if you now like look at like, okay, well, what happens if we multiply this by this, just like we did in Clip, right, then if this number is high, then it's saying that when this variable is high, where this pixel is high, this pixel tends to be high, and vice versa. Or if it's low, it's saying when this pixel tends to be high, this one tends to be low. Or, interesting to us, what happens, oopsie-daisy, sorry about that, what happens if this is zero? That says that if this is high, then this could be anything. Or this is high, this could be anything. There's no relationship between them. So statistically, we would say that these two pixels are independent. And so now, that basically means we could do that for all of these. We could say, oh, you know, these are all zeros. And what that says is that, oh, every pixel is independent of every other pixel. Now, of course, in real pictures, that's not how real pixels work, but that's the assumption we're making. Because if we start with a very special matrix called I, which is one, one, one, one, zero, zero, zero. If we take this very special matrix, it's very special because I can multiply it by something, say beta. And if I multiply it by a matrix, I get back the original matrix. If I multiply it by a scalar, I'm going to get beta, beta, beta, and lots of zeros. And so if I multiply something by this matrix, then I'm just multiplying it by beta. But what's interesting about this is that this is what Waseem wrote. Waseem wrote I times beta, I times beta t. So what he's saying is, oh, we've now got a covariance matrix where for each individual pixel, it's like pixel number one, beta one, pixel number two, beta two. This is the variances of each one. And the covariance is, you know, the relationship between the pixels, zero, they're expected to be independent. So that's where we're kind of going from like statistics you do in high school to statistics you do at university. It's like suddenly covariance is now a matrices, not individual numbers. Does that sound about right to you, Tanishq? Yeah, that's a great explanation of it. Yes. Awesome. Cool. So now let's try to describe, you know, what this would do to aim this digit. So, you know, we let's put back our mean equation. And our covariance, whoops, our covariance. So mean and our covariance. And let's look at how this behaves, you know, at the edges, sort of. So it's really hard to, you know, understand this. I don't think anybody can kind of just look at this and know what it means. What we typically do is we try to describe it kind of at the edges. And so we'll start with like what happens if that's zero. And we'll work with X zero as well instead of, you know, Xt minus one, which would mean like an MNIST digit. So if beta is zero and we get our X zero, you know, square root one minus zero, which is one. And square root of one is one. So that kind of falls away. So we just have a mean of our previous image. And this is just variance of zero. So we have a normal distribution with a mean of our previous image, a variance of zero, which means we have the same image. Yeah, just to clarify, when you have a variance of zero, that means that there's really no noise or anything. It's just at that mean and, you know, your distribution is just saying that's the only point that you can get from it. So yeah, that's what it just becomes the same image because, yeah, there's no noise or variance because the variance is zero. Yeah, exactly. And then when our beta is one, we still have this. And then we have, you know, square root one minus one. And that becomes zero. So this whole thing becomes zero. And this thing becomes i times theta t, which is, you know, i. And if it's just i, then as Jeremy described, it would imply a variance of one. And so our image through this function would just be pure noise. So, you know, mean of zero, standard deviation of one, and it would just be a bunch of noise. And kind of somewhere in between that, we have to say over here, you know, what would it produce? It would be some mixture. So, you know, like maybe a light, the lighter pixels of eight, and some noise, maybe a bit darker. And we can kind of draw this and you would have seen this in the previous lecture. So, you know, if you look at the graph, you can draw the sequence of things that become progressively more noisy in very small steps. All the way until it becomes pure noise. This is what we call the forward diffusion process. So, you know, we can draw this sequence. So, this is the forward diffusion process. And we can now describe some of these things. So, this would be a sample from our data distribution, Qx0. This would be the function for the conditional probability density function that takes, so of x1, given x0, and so on. And the way that the terminology that we would use, or that mathematicians use to describe this, is they would call it a Markov process with Gaussian transitions. And, you know, this can sound quite scary, but we've just described exactly what this is. So, when we say process, it usually means, you know, something where there's a sequence involved. When we say Markov, it means that the thing at time t depends only on the thing at t minus one. The transition is this function. How do you actually go from t minus one to t? And Gaussian is the fact that that transition is the normal distribution. Does that sound right? Yes. Just to also clarify a couple of things. When we say that, you know, we're sampling from the data distribution, what that is referring to is trying to find some random, you know, sample or some random data point that maximizes that likelihood, or that has a high likelihood. So, when we say that, you know, we're looking at that API, that magic API we were talking about, and we're trying to get some, you know, some data points that have a high value from that API. And, you know, for some distributions, it's very simple and we know how it works, like a Gaussian distribution, and we know the parameters of that Gaussian distribution, it's very easy to be able to do that sampling. And then, of course, in other cases, it's not very easy. It's quite difficult to do that sampling. So, then we have to figure out alternative ways of doing that sampling. But that's why in this case, with the forward distribution, we just have these simple Gaussian transitions, and we already know the parameters of those Gaussian transitions, so we can easily do that sampling. And going back also to that, I think it's worthwhile to also kind of show and think about maybe how this is again done practically. Because one of the nice properties of Gaussian distributions as a whole is that you can, you know, simply take some normal noise with a mean of zero and variance of one. So that's, I think they usually typically call that a unit distribution. It's just like, yeah, normal of zero, one. And then if you want to get to some other point with a mean of whatever value you specify and a variance of whatever value you specify, you can simply take that normal distribution, scale it by the, you multiply it by the variance, and then you add your mean. So then there's a simple equation that you can take to get any particular mean and variance. So that's how you would get the samples for these other distributions that we have defined throughout the forward distribution. So, for example, when you're coding this up, of course, a lot of these softwares, they will have a way of getting a sample from this normal distribution of zero, one. And then you just use that equation then to get it at the desired mean and variance. And so that's how it kind of happens under the hood when you're kind of describe this with code. That's really helpful. Yeah, and this idea of we can't really sample from this thing. That's exactly the problem that generative kind of modeling is trying to solve. Like, how do you represent this in such a way that you can easily sample from it? And so it turns out that if you have one of these processes, you know, where you have many, many steps, so let's say a thousand steps, the thousand of these steps going to the right. And they're all very small steps that eventually go to noise. Somebody, you know, maybe in the 1950s, I think, discovered that you can represent the process of going backwards in exactly the same functional form with just different parameters. So what that means is if we say P is the thing that goes backwards, so you know, the previous one, given the current one, this P has the same functional form. So it's also the transitions are also normal, but the mean is, you know, some unknown. So we'll use a square and the variance is some unknown. We use a triangle. Is that correct? Yeah, that's correct. And just going back to our previous point about P versus Q, here we can see that the the Q was describing the sort of forward process going, you know, yeah, this sort of steps that we're doing. And then the P is describing what we're going in the reverse way. So that's why, you know, the P is going backwards. So that's why, you know, these papers are using, you know, Q for one process and then P for another. That's what they're kind of indicating, at least in the diffusion model literature. And P is kind of like X, you know, it's the one we want to figure out. So like Q is kind of like Y and P is kind of like X. That's how I like to think of that. And so, you know, we have this functional form and the next question is, how can we use this? Or, you know, we just don't know what these parameters are. How can we figure out what those are? And this is goes back, you know, to early kind of statistics literature where you can fit this model using by maximizing what's called the likelihood function. So we can try different parameters until we have one that maximizes the likelihood. It turns out that we can't quite do this exactly. Because you would need to calculate some integral. And that integral is over very high dimensional values, continuous values. So you can't actually calculate this. I think you can think of it because, you know, we're having these thousands of steps that we're trying to go in this reverse process. And so, you know, you have these thousands of steps that there are going to be many possible values for each step. So it's kind of hard to evaluate it over all these thousands of steps and all the possible values for all these differences. So I think that's kind of where the challenges arise. And that's what it makes it difficult because you have to find, you have to evaluate it over these multiple steps and try to find these functions for all these different steps. So that's kind of where the challenge is. And so you might see people talk not about the likelihood function, but about the log likelihood. And correct me if I'm wrong here, Tanish, but I think the log here is a bit of a, you know, computational trick almost. So I think it has a few properties. The first is that it's always increasing and, you know, people would call this, I think, monotonic. You know, it looks always kind of increasing. And because it's always increasing, it's always increasing. So you can see that it's always increasing. And because it's always increasing, it's the same, you get the same parameters if you optimize the log likelihood versus you optimize the likelihood. It also takes the product to sums. Because, and that's helpful because we have joint distributions, you know, which turn out to be products. So it turns out we have a lot of products there and they become sums, which is easy to work with. And the last thing is that, you know, this normal distribution has exponentials or exponential functions. And those disappear with the log. So this is a much friendlier thing to optimize. Yep, that's correct. Cool. And then there's one more step. You know, we still can't optimize the log likelihood of the thing that this eventually describes. But again, and this is kind of the beauty of math, is that somebody figured out a long time ago that there's a way to optimize some other quantity called the ELBO for short. Which stands for evidence lower bound. And the evidence is just another name for the likelihood. And the lower bound means, sorry, you know, the lower bound of the evidence. And if you optimize that, it's almost as good as optimizing the thing that we really want to. But this one we can calculate very, very easily. And so you can use this as a loss function to train two neural networks. That predict our square from earlier, which was our mean. And our triangle, which is our variance of this reverse process. And once you have that, you go all the way back here. So then you have these values. You can start with pure noise and keep calling these neural networks. Sampling from those normal distributions. Kind of applying that iteratively over many steps. And you recover the data distribution. One thing that's important to clarify here is that you can recover the whole distribution, but you can't just do it in a single step. You can recover the whole distribution, but you can't necessarily take a single image, convert it to pure noise, and then convert it back. So this operates sort of at the distribution level. So you can take this kind of magic API. You can reconstruct that whole API. And if you can do that, then you can generate images, MNIST digits, or cats, or dogs, or whatever you want to. I want to just clarify one thing about this process of the loss function. So this sort of evidence lower bound loss function, the kind of approach that it's taking is that we have this forward process. We can go from the original images and figure out these sorts of intermediate distributions going all the way finally to noise. With this sort of evidence lower bound loss function, what we're really kind of doing is trying to match our distribution that we're trying to optimize to those distributions that we saw in the forward process. So that's what we're trying to do. We're trying to match those distributions. And there's a specific type of function that is able to do that. It's called a KL divergence. That's the sort of function that can compare probability distributions. And again, because we're dealing with Gaussians, you can calculate that analytically and a lot of the math becomes very simple. So that's again, with the whole Gaussians, we know them quite well. The math is very simple. So that allows us to do this sort of comparison between these distributions very easily and optimize that. And so we want to kind of minimize the difference between the distributions we see in the forward process and the distributions we're finally determined for the reverse process. Perfect. Then there's one more thing, I think, one more kind of major step to get closer to the form that you would have seen in Jeremy's lesson. So there was the 2020 paper. The initials of that model is DDPM. Tanish, do you know what this stands for? Yeah, it stands for denoising diffusion probabilistic model. OK, cool. And what they did was they said, let's assume that this variance is DDPM. Let's assume that this variance is just a constant, so we don't learn it. And we assume also that the step size from earlier, the variance of the noise that we added each step, is also a constant. We don't learn that. We're just predicting the mean, and these are set to some really convenient values. Then the last turns out to be that you predict the noise. So you can restructure this whole thing as you take in, you need to train a network that takes in images. So here's your network, and it tells you what of this image is noise. Thanks to these simplifying assumptions. And even though they're assumptions, turns out you can train much more models that produce much better images. Now, I think this relates to something from the lesson that Jeremy gave. Tanish, do you remember there was something about the gradient or something like that? Yes, yes. So this idea of adding noise and learning to remove noise, the idea is that kind of by, you know, again, you have this sort of this image that you have noise, right? And by, sorry, let me think about the best way to say this. Oh, yeah, sorry. Let me start over. So I'll just start. Yeah. So like Jeremy will say in the lesson, what we want to do is we want to figure out the gradient of this likelihood function. So this is just kind of a different way about thinking about this. If we had some information about this gradient, then we could, for example, you know, use that information to produce, like we talked about, kind of this optimization, kind of produce images with high likelihood. So the idea is that we can add noise to the images that we have. So those are samples that we have. And that kind of takes us away from, you know, the regular images that we have. And, you know, that kind of decreases the likelihood, right? So we have those images and we're adding noise that decreases the likelihood. And we want to kind of learn how to get back to high likelihood images and kind of use that to provide some sort of estimate of our gradient. So this sort of denoising process actually allows us to do that. So there are actually theorems also, I think, from the 1950s that demonstrate that, especially in the case of this sort of Gaussian noise that we're working with, this denoising process is equivalent to learning what is known as the score function. And the score function is the gradient of the log of the likelihood. So, again, they have this log here, which, again, makes the math nicer and easier to work with. But the general idea is the same, because as we talked about, log is a monotonic function. So, again, the general ideas are the same, but the score function specifically refers to the gradient of the log likelihood. So this sort of denoising process allows us to learn the score function. So that's what we're doing, this noise predicting that we had this whole probabilistic framework using that sort of likelihood framework. And it came back down to just predicting the noise. And that's what the DPM paper showed in 2020. But it turns out that is equivalent to calculating out this sort of score function and using that information to be able to sample from our distribution. So that's kind of how these two approaches connect. So there's a lot of literature talking about maybe that sort of probabilistic likelihood perspective of diffusion models. And there's also a lot of literature talking about this score-based perspective. But this hopefully allows you to think about the similarities and how these two approaches connect with each other. Awesome. Yeah. And that's kind of the beauty, I think, of the math side of things here is that you find all of these relationships between different fields and also between different centuries, basically. That allows you to do really kind of powerful and unexpected things. OK. So you can just do a quick recap of where we got to. So we started out with our data distribution, which we want to model. We said we'll define this forward diffusion process, which is a way of kind of adding noise to this model. And because we added in this specific way, thanks to some discovery in the 1950s, the reverse process has the same form. And then we already know how to train a neural network for this using the ALBO. And then a couple of years later came the discovery, simplifying assumptions that in the end, all we do is predict the noise. And I just remembered we take actually the MSE of this noise prediction, the mean squared error, which is a nice, very simple framing of the model. And then Taneesh spoke about another way to derive all of this, which is the score function approach, the gradient of the log likelihood. OK, cool. Yeah, I highly recommend checking out the course lesson as well if you haven't. If you don't understand this, there's no need to be intimidated. You can still be very effective without ever using math. You can be very effective at deep learning, as fast AI has shown us. And you can do novel research as well. For me, this is interesting. And it's even beautiful in a way. So I recommend checking it out, but don't feel intimidated. You can find the course lesson links in the past AI forum. We'll add those links as well in the description of this video. We'll also have a topic in the forum for this lesson. You can have discussions there, post any comments, add any relevant links to the math. And then we have another lesson video by Jono, which I really recommend checking out. He's a great teacher, and I think he was the first person to do a full course on stable diffusion. Yeah, Jono's video is kind of a deep dive into some of the code a little bit more and into some of the concepts a little bit more. So I feel like between these three videos, it's a good overview. I think, I mean, just to clarify, you don't need to understand all the math that Wastim described in this video. That's not to say you won't need to understand math. We'll be covering lots of math in these lessons. But we'll be covering just the math you need to understand and build on the code. And we'll be covering it over many, many more hours than this rather rapid overview. Perfect. Cool. And yeah, thank you so much, Denise. I had a lot of fun. And thank you so much, Wastim. That was awesome. Cool. Bye bye.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 4.36, "text": " Hello, everyone. My name is Wasim.", "tokens": [2425, 11, 1518, 13, 1222, 1315, 307, 3027, 332, 13], "temperature": 0.0, "avg_logprob": -0.2797662517692469, "compression_ratio": 1.4270833333333333, "no_speech_prob": 0.07670570909976959}, {"id": 1, "seek": 0, "start": 4.36, "end": 9.120000000000001, "text": " I am an entrepreneur in residence at Fast AI.", "tokens": [286, 669, 364, 14307, 294, 19607, 412, 15968, 7318, 13], "temperature": 0.0, "avg_logprob": -0.2797662517692469, "compression_ratio": 1.4270833333333333, "no_speech_prob": 0.07670570909976959}, {"id": 2, "seek": 0, "start": 9.52, "end": 15.76, "text": " I'm currently at the Fast AI headquarters at the moment in Australia,", "tokens": [286, 478, 4362, 412, 264, 15968, 7318, 21052, 412, 264, 1623, 294, 7060, 11], "temperature": 0.0, "avg_logprob": -0.2797662517692469, "compression_ratio": 1.4270833333333333, "no_speech_prob": 0.07670570909976959}, {"id": 3, "seek": 0, "start": 15.76, "end": 20.36, "text": " although I'm originally from South Africa, from Cape Town.", "tokens": [4878, 286, 478, 7993, 490, 4242, 7349, 11, 490, 27517, 15954, 13], "temperature": 0.0, "avg_logprob": -0.2797662517692469, "compression_ratio": 1.4270833333333333, "no_speech_prob": 0.07670570909976959}, {"id": 4, "seek": 0, "start": 20.96, "end": 24.16, "text": " I'm joined here today by Tanishq.", "tokens": [286, 478, 6869, 510, 965, 538, 314, 7524, 80, 13], "temperature": 0.0, "avg_logprob": -0.2797662517692469, "compression_ratio": 1.4270833333333333, "no_speech_prob": 0.07670570909976959}, {"id": 5, "seek": 0, "start": 24.16, "end": 27.2, "text": " Tanishq works at Stability AI.", "tokens": [314, 7524, 80, 1985, 412, 745, 2310, 7318, 13], "temperature": 0.0, "avg_logprob": -0.2797662517692469, "compression_ratio": 1.4270833333333333, "no_speech_prob": 0.07670570909976959}, {"id": 6, "seek": 2720, "start": 27.2, "end": 31.68, "text": " We've been working together with a couple other people on", "tokens": [492, 600, 668, 1364, 1214, 365, 257, 1916, 661, 561, 322], "temperature": 0.0, "avg_logprob": -0.18778377532958984, "compression_ratio": 1.5146443514644352, "no_speech_prob": 0.00022536514734383672}, {"id": 7, "seek": 2720, "start": 31.68, "end": 37.72, "text": " diffusion models, generative modeling, and that's been super fun.", "tokens": [25242, 5245, 11, 1337, 1166, 15983, 11, 293, 300, 311, 668, 1687, 1019, 13], "temperature": 0.0, "avg_logprob": -0.18778377532958984, "compression_ratio": 1.5146443514644352, "no_speech_prob": 0.00022536514734383672}, {"id": 8, "seek": 2720, "start": 37.72, "end": 42.04, "text": " Tanishq, do you want to maybe introduce yourself as well?", "tokens": [314, 7524, 80, 11, 360, 291, 528, 281, 1310, 5366, 1803, 382, 731, 30], "temperature": 0.0, "avg_logprob": -0.18778377532958984, "compression_ratio": 1.5146443514644352, "no_speech_prob": 0.00022536514734383672}, {"id": 9, "seek": 2720, "start": 42.04, "end": 44.44, "text": " Yeah. My name is Tanishq.", "tokens": [865, 13, 1222, 1315, 307, 314, 7524, 80, 13], "temperature": 0.0, "avg_logprob": -0.18778377532958984, "compression_ratio": 1.5146443514644352, "no_speech_prob": 0.00022536514734383672}, {"id": 10, "seek": 2720, "start": 44.44, "end": 47.56, "text": " I am a PhD student at UC Davis,", "tokens": [286, 669, 257, 14476, 3107, 412, 14079, 15658, 11], "temperature": 0.0, "avg_logprob": -0.18778377532958984, "compression_ratio": 1.5146443514644352, "no_speech_prob": 0.00022536514734383672}, {"id": 11, "seek": 2720, "start": 47.56, "end": 50.0, "text": " but I also work at Stability AI.", "tokens": [457, 286, 611, 589, 412, 745, 2310, 7318, 13], "temperature": 0.0, "avg_logprob": -0.18778377532958984, "compression_ratio": 1.5146443514644352, "no_speech_prob": 0.00022536514734383672}, {"id": 12, "seek": 2720, "start": 50.0, "end": 52.760000000000005, "text": " I've been exploring and playing around with", "tokens": [286, 600, 668, 12736, 293, 2433, 926, 365], "temperature": 0.0, "avg_logprob": -0.18778377532958984, "compression_ratio": 1.5146443514644352, "no_speech_prob": 0.00022536514734383672}, {"id": 13, "seek": 2720, "start": 52.760000000000005, "end": 55.84, "text": " diffusion models for the past several months.", "tokens": [25242, 5245, 337, 264, 1791, 2940, 2493, 13], "temperature": 0.0, "avg_logprob": -0.18778377532958984, "compression_ratio": 1.5146443514644352, "no_speech_prob": 0.00022536514734383672}, {"id": 14, "seek": 5584, "start": 55.84, "end": 61.56, "text": " It's been great to also explore that with", "tokens": [467, 311, 668, 869, 281, 611, 6839, 300, 365], "temperature": 0.0, "avg_logprob": -0.1943713343420694, "compression_ratio": 1.5960591133004927, "no_speech_prob": 3.8678812416037545e-05}, {"id": 15, "seek": 5584, "start": 61.56, "end": 65.72, "text": " the Fast AI community as well in these last few weeks as well.", "tokens": [264, 15968, 7318, 1768, 382, 731, 294, 613, 1036, 1326, 3259, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.1943713343420694, "compression_ratio": 1.5960591133004927, "no_speech_prob": 3.8678812416037545e-05}, {"id": 16, "seek": 5584, "start": 65.72, "end": 68.4, "text": " Awesome. Cool.", "tokens": [10391, 13, 8561, 13], "temperature": 0.0, "avg_logprob": -0.1943713343420694, "compression_ratio": 1.5960591133004927, "no_speech_prob": 3.8678812416037545e-05}, {"id": 17, "seek": 5584, "start": 68.4, "end": 77.28, "text": " This talk is for me trying to understand the math behind diffusion.", "tokens": [639, 751, 307, 337, 385, 1382, 281, 1223, 264, 5221, 2261, 25242, 13], "temperature": 0.0, "avg_logprob": -0.1943713343420694, "compression_ratio": 1.5960591133004927, "no_speech_prob": 3.8678812416037545e-05}, {"id": 18, "seek": 5584, "start": 77.28, "end": 80.52000000000001, "text": " If you've done the Fast AI courses before,", "tokens": [759, 291, 600, 1096, 264, 15968, 7318, 7712, 949, 11], "temperature": 0.0, "avg_logprob": -0.1943713343420694, "compression_ratio": 1.5960591133004927, "no_speech_prob": 3.8678812416037545e-05}, {"id": 19, "seek": 5584, "start": 80.52000000000001, "end": 82.68, "text": " you know that you don't need to understand", "tokens": [291, 458, 300, 291, 500, 380, 643, 281, 1223], "temperature": 0.0, "avg_logprob": -0.1943713343420694, "compression_ratio": 1.5960591133004927, "no_speech_prob": 3.8678812416037545e-05}, {"id": 20, "seek": 5584, "start": 82.68, "end": 85.64, "text": " the math to be effective with any of these models.", "tokens": [264, 5221, 281, 312, 4942, 365, 604, 295, 613, 5245, 13], "temperature": 0.0, "avg_logprob": -0.1943713343420694, "compression_ratio": 1.5960591133004927, "no_speech_prob": 3.8678812416037545e-05}, {"id": 21, "seek": 8564, "start": 85.64, "end": 88.88, "text": " In fact, you don't even need the math to do research,", "tokens": [682, 1186, 11, 291, 500, 380, 754, 643, 264, 5221, 281, 360, 2132, 11], "temperature": 0.0, "avg_logprob": -0.1668695356787705, "compression_ratio": 1.527363184079602, "no_speech_prob": 1.4958052815927658e-05}, {"id": 22, "seek": 8564, "start": 88.88, "end": 91.6, "text": " to do novel research and contribute to these.", "tokens": [281, 360, 7613, 2132, 293, 10586, 281, 613, 13], "temperature": 0.0, "avg_logprob": -0.1668695356787705, "compression_ratio": 1.527363184079602, "no_speech_prob": 1.4958052815927658e-05}, {"id": 23, "seek": 8564, "start": 91.6, "end": 96.52, "text": " But for me, it came out of interest.", "tokens": [583, 337, 385, 11, 309, 1361, 484, 295, 1179, 13], "temperature": 0.0, "avg_logprob": -0.1668695356787705, "compression_ratio": 1.527363184079602, "no_speech_prob": 1.4958052815927658e-05}, {"id": 24, "seek": 8564, "start": 96.52, "end": 105.48, "text": " I thought it's beautiful what diffusion models were discovered.", "tokens": [286, 1194, 309, 311, 2238, 437, 25242, 5245, 645, 6941, 13], "temperature": 0.0, "avg_logprob": -0.1668695356787705, "compression_ratio": 1.527363184079602, "no_speech_prob": 1.4958052815927658e-05}, {"id": 25, "seek": 8564, "start": 105.48, "end": 110.6, "text": " I think a large part of that was thanks to some really clever math.", "tokens": [286, 519, 257, 2416, 644, 295, 300, 390, 3231, 281, 512, 534, 13494, 5221, 13], "temperature": 0.0, "avg_logprob": -0.1668695356787705, "compression_ratio": 1.527363184079602, "no_speech_prob": 1.4958052815927658e-05}, {"id": 26, "seek": 8564, "start": 110.6, "end": 115.2, "text": " So I wanted to understand that I don't", "tokens": [407, 286, 1415, 281, 1223, 300, 286, 500, 380], "temperature": 0.0, "avg_logprob": -0.1668695356787705, "compression_ratio": 1.527363184079602, "no_speech_prob": 1.4958052815927658e-05}, {"id": 27, "seek": 11520, "start": 115.2, "end": 123.04, "text": " have a math background and so I want to help", "tokens": [362, 257, 5221, 3678, 293, 370, 286, 528, 281, 854], "temperature": 0.0, "avg_logprob": -0.23219289352644734, "compression_ratio": 1.4294478527607362, "no_speech_prob": 8.064444409683347e-05}, {"id": 28, "seek": 11520, "start": 123.04, "end": 128.28, "text": " describe how I think about it and how you", "tokens": [6786, 577, 286, 519, 466, 309, 293, 577, 291], "temperature": 0.0, "avg_logprob": -0.23219289352644734, "compression_ratio": 1.4294478527607362, "no_speech_prob": 8.064444409683347e-05}, {"id": 29, "seek": 11520, "start": 128.28, "end": 133.32, "text": " can interpret all of these notations and things.", "tokens": [393, 7302, 439, 295, 613, 406, 763, 293, 721, 13], "temperature": 0.0, "avg_logprob": -0.23219289352644734, "compression_ratio": 1.4294478527607362, "no_speech_prob": 8.064444409683347e-05}, {"id": 30, "seek": 11520, "start": 133.68, "end": 139.56, "text": " Cool. Yeah. So I can just dive into it, I think.", "tokens": [8561, 13, 865, 13, 407, 286, 393, 445, 9192, 666, 309, 11, 286, 519, 13], "temperature": 0.0, "avg_logprob": -0.23219289352644734, "compression_ratio": 1.4294478527607362, "no_speech_prob": 8.064444409683347e-05}, {"id": 31, "seek": 13956, "start": 139.56, "end": 148.68, "text": " The first bit of math that we see in this paper is Q of x superscript 0.", "tokens": [440, 700, 857, 295, 5221, 300, 321, 536, 294, 341, 3035, 307, 1249, 295, 2031, 37906, 5944, 1958, 13], "temperature": 0.0, "avg_logprob": -0.28203344345092773, "compression_ratio": 1.4943820224719102, "no_speech_prob": 0.0001694982056505978}, {"id": 32, "seek": 13956, "start": 148.68, "end": 152.6, "text": " They call this the data distribution.", "tokens": [814, 818, 341, 264, 1412, 7316, 13], "temperature": 0.0, "avg_logprob": -0.28203344345092773, "compression_ratio": 1.4943820224719102, "no_speech_prob": 0.0001694982056505978}, {"id": 33, "seek": 13956, "start": 154.12, "end": 158.64000000000001, "text": " Do you want to mention exactly which paper this is?", "tokens": [1144, 291, 528, 281, 2152, 2293, 597, 3035, 341, 307, 30], "temperature": 0.0, "avg_logprob": -0.28203344345092773, "compression_ratio": 1.4943820224719102, "no_speech_prob": 0.0001694982056505978}, {"id": 34, "seek": 13956, "start": 158.64000000000001, "end": 161.36, "text": " Right. Good question.", "tokens": [1779, 13, 2205, 1168, 13], "temperature": 0.0, "avg_logprob": -0.28203344345092773, "compression_ratio": 1.4943820224719102, "no_speech_prob": 0.0001694982056505978}, {"id": 35, "seek": 13956, "start": 161.36, "end": 164.16, "text": " This paper is the 2015 paper.", "tokens": [639, 3035, 307, 264, 7546, 3035, 13], "temperature": 0.0, "avg_logprob": -0.28203344345092773, "compression_ratio": 1.4943820224719102, "no_speech_prob": 0.0001694982056505978}, {"id": 36, "seek": 13956, "start": 164.16, "end": 167.12, "text": " Do you remember the authors of that paper, Tanishk?", "tokens": [1144, 291, 1604, 264, 16552, 295, 300, 3035, 11, 314, 7524, 74, 30], "temperature": 0.0, "avg_logprob": -0.28203344345092773, "compression_ratio": 1.4943820224719102, "no_speech_prob": 0.0001694982056505978}, {"id": 37, "seek": 16712, "start": 167.12, "end": 171.16, "text": " I think it's Jasa Sol Dikstein,", "tokens": [286, 519, 309, 311, 508, 9994, 7026, 413, 1035, 9089, 11], "temperature": 0.0, "avg_logprob": -0.301168836396316, "compression_ratio": 1.4343434343434343, "no_speech_prob": 5.554279050556943e-05}, {"id": 38, "seek": 16712, "start": 171.16, "end": 173.6, "text": " who now works at Google, I think.", "tokens": [567, 586, 1985, 412, 3329, 11, 286, 519, 13], "temperature": 0.0, "avg_logprob": -0.301168836396316, "compression_ratio": 1.4343434343434343, "no_speech_prob": 5.554279050556943e-05}, {"id": 39, "seek": 16712, "start": 173.6, "end": 177.04, "text": " It's from Surya Ganguly's lab.", "tokens": [467, 311, 490, 318, 2598, 64, 17984, 3540, 311, 2715, 13], "temperature": 0.0, "avg_logprob": -0.301168836396316, "compression_ratio": 1.4343434343434343, "no_speech_prob": 5.554279050556943e-05}, {"id": 40, "seek": 16712, "start": 177.04, "end": 179.92000000000002, "text": " So this was the paper,", "tokens": [407, 341, 390, 264, 3035, 11], "temperature": 0.0, "avg_logprob": -0.301168836396316, "compression_ratio": 1.4343434343434343, "no_speech_prob": 5.554279050556943e-05}, {"id": 41, "seek": 16712, "start": 179.92000000000002, "end": 181.12, "text": " as far as I understand,", "tokens": [382, 1400, 382, 286, 1223, 11], "temperature": 0.0, "avg_logprob": -0.301168836396316, "compression_ratio": 1.4343434343434343, "no_speech_prob": 5.554279050556943e-05}, {"id": 42, "seek": 16712, "start": 181.12, "end": 184.44, "text": " that introduced this idea of diffusion.", "tokens": [300, 7268, 341, 1558, 295, 25242, 13], "temperature": 0.0, "avg_logprob": -0.301168836396316, "compression_ratio": 1.4343434343434343, "no_speech_prob": 5.554279050556943e-05}, {"id": 43, "seek": 16712, "start": 184.64000000000001, "end": 188.6, "text": " 2015 by those authors.", "tokens": [7546, 538, 729, 16552, 13], "temperature": 0.0, "avg_logprob": -0.301168836396316, "compression_ratio": 1.4343434343434343, "no_speech_prob": 5.554279050556943e-05}, {"id": 44, "seek": 16712, "start": 188.6, "end": 194.68, "text": " They start out by defining this data distribution and they use this notation.", "tokens": [814, 722, 484, 538, 17827, 341, 1412, 7316, 293, 436, 764, 341, 24657, 13], "temperature": 0.0, "avg_logprob": -0.301168836396316, "compression_ratio": 1.4343434343434343, "no_speech_prob": 5.554279050556943e-05}, {"id": 45, "seek": 19468, "start": 194.68, "end": 198.08, "text": " Already, like a lot of people, myself included,", "tokens": [23741, 11, 411, 257, 688, 295, 561, 11, 2059, 5556, 11], "temperature": 0.0, "avg_logprob": -0.20227639104278994, "compression_ratio": 1.4705882352941178, "no_speech_prob": 2.4953053070930764e-05}, {"id": 46, "seek": 19468, "start": 198.08, "end": 200.24, "text": " find this quite confusing.", "tokens": [915, 341, 1596, 13181, 13], "temperature": 0.0, "avg_logprob": -0.20227639104278994, "compression_ratio": 1.4705882352941178, "no_speech_prob": 2.4953053070930764e-05}, {"id": 47, "seek": 19468, "start": 200.24, "end": 203.28, "text": " But let's go through what's described here.", "tokens": [583, 718, 311, 352, 807, 437, 311, 7619, 510, 13], "temperature": 0.0, "avg_logprob": -0.20227639104278994, "compression_ratio": 1.4705882352941178, "no_speech_prob": 2.4953053070930764e-05}, {"id": 48, "seek": 19468, "start": 203.28, "end": 208.08, "text": " So they have an x and in math,", "tokens": [407, 436, 362, 364, 2031, 293, 294, 5221, 11], "temperature": 0.0, "avg_logprob": -0.20227639104278994, "compression_ratio": 1.4705882352941178, "no_speech_prob": 2.4953053070930764e-05}, {"id": 49, "seek": 19468, "start": 208.08, "end": 212.20000000000002, "text": " x is often used as the input variable,", "tokens": [2031, 307, 2049, 1143, 382, 264, 4846, 7006, 11], "temperature": 0.0, "avg_logprob": -0.20227639104278994, "compression_ratio": 1.4705882352941178, "no_speech_prob": 2.4953053070930764e-05}, {"id": 50, "seek": 21220, "start": 212.2, "end": 223.2, "text": " much like y, which is then used often as the output variable.", "tokens": [709, 411, 288, 11, 597, 307, 550, 1143, 2049, 382, 264, 5598, 7006, 13], "temperature": 0.0, "avg_logprob": -0.20868669771680645, "compression_ratio": 1.4661654135338347, "no_speech_prob": 3.695923805935308e-05}, {"id": 51, "seek": 21220, "start": 227.39999999999998, "end": 233.07999999999998, "text": " Yeah. The fact that it has a superscript also implies something.", "tokens": [865, 13, 440, 1186, 300, 309, 575, 257, 37906, 5944, 611, 18779, 746, 13], "temperature": 0.0, "avg_logprob": -0.20868669771680645, "compression_ratio": 1.4661654135338347, "no_speech_prob": 3.695923805935308e-05}, {"id": 52, "seek": 21220, "start": 233.07999999999998, "end": 241.12, "text": " So the fact that we have x superscript 0 implies that there might be", "tokens": [407, 264, 1186, 300, 321, 362, 2031, 37906, 5944, 1958, 18779, 300, 456, 1062, 312], "temperature": 0.0, "avg_logprob": -0.20868669771680645, "compression_ratio": 1.4661654135338347, "no_speech_prob": 3.695923805935308e-05}, {"id": 53, "seek": 24112, "start": 241.12, "end": 248.4, "text": " a sequence of x's and I think it's useful to get comfortable with this idea", "tokens": [257, 8310, 295, 2031, 311, 293, 286, 519, 309, 311, 4420, 281, 483, 4619, 365, 341, 1558], "temperature": 0.0, "avg_logprob": -0.1605518186414564, "compression_ratio": 1.5526315789473684, "no_speech_prob": 0.00020134984515607357}, {"id": 54, "seek": 24112, "start": 248.4, "end": 253.8, "text": " of simple compact notations", "tokens": [295, 2199, 14679, 406, 763], "temperature": 0.0, "avg_logprob": -0.1605518186414564, "compression_ratio": 1.5526315789473684, "no_speech_prob": 0.00020134984515607357}, {"id": 55, "seek": 24112, "start": 253.8, "end": 258.56, "text": " implying a lot more than might be obvious at first glance.", "tokens": [704, 7310, 257, 688, 544, 813, 1062, 312, 6322, 412, 700, 21094, 13], "temperature": 0.0, "avg_logprob": -0.1605518186414564, "compression_ratio": 1.5526315789473684, "no_speech_prob": 0.00020134984515607357}, {"id": 56, "seek": 24112, "start": 258.56, "end": 262.64, "text": " So x implies that it means something about this quantity,", "tokens": [407, 2031, 18779, 300, 309, 1355, 746, 466, 341, 11275, 11], "temperature": 0.0, "avg_logprob": -0.1605518186414564, "compression_ratio": 1.5526315789473684, "no_speech_prob": 0.00020134984515607357}, {"id": 57, "seek": 24112, "start": 262.64, "end": 266.88, "text": " it's an input variable and the 0 implies that there might be other things.", "tokens": [309, 311, 364, 4846, 7006, 293, 264, 1958, 18779, 300, 456, 1062, 312, 661, 721, 13], "temperature": 0.0, "avg_logprob": -0.1605518186414564, "compression_ratio": 1.5526315789473684, "no_speech_prob": 0.00020134984515607357}, {"id": 58, "seek": 26688, "start": 266.88, "end": 271.4, "text": " So you might have an x1,", "tokens": [407, 291, 1062, 362, 364, 2031, 16, 11], "temperature": 0.0, "avg_logprob": -0.2666310787200928, "compression_ratio": 1.4242424242424243, "no_speech_prob": 3.643933450803161e-05}, {"id": 59, "seek": 26688, "start": 271.4, "end": 275.4, "text": " an x2 and so on, but we might see that.", "tokens": [364, 2031, 17, 293, 370, 322, 11, 457, 321, 1062, 536, 300, 13], "temperature": 0.0, "avg_logprob": -0.2666310787200928, "compression_ratio": 1.4242424242424243, "no_speech_prob": 3.643933450803161e-05}, {"id": 60, "seek": 26688, "start": 275.4, "end": 279.84, "text": " Then the third part is you have q.", "tokens": [1396, 264, 2636, 644, 307, 291, 362, 9505, 13], "temperature": 0.0, "avg_logprob": -0.2666310787200928, "compression_ratio": 1.4242424242424243, "no_speech_prob": 3.643933450803161e-05}, {"id": 61, "seek": 26688, "start": 280.32, "end": 287.36, "text": " Q is what we call a probability density function.", "tokens": [1249, 307, 437, 321, 818, 257, 8482, 10305, 2445, 13], "temperature": 0.0, "avg_logprob": -0.2666310787200928, "compression_ratio": 1.4242424242424243, "no_speech_prob": 3.643933450803161e-05}, {"id": 62, "seek": 26688, "start": 288.71999999999997, "end": 292.76, "text": " So the first part here is probability.", "tokens": [407, 264, 700, 644, 510, 307, 8482, 13], "temperature": 0.0, "avg_logprob": -0.2666310787200928, "compression_ratio": 1.4242424242424243, "no_speech_prob": 3.643933450803161e-05}, {"id": 63, "seek": 29276, "start": 292.76, "end": 297.0, "text": " The question is, what does q have to do with probabilities?", "tokens": [440, 1168, 307, 11, 437, 775, 9505, 362, 281, 360, 365, 33783, 30], "temperature": 0.0, "avg_logprob": -0.12602278391520183, "compression_ratio": 1.670995670995671, "no_speech_prob": 5.142603549757041e-05}, {"id": 64, "seek": 29276, "start": 297.0, "end": 300.32, "text": " Well, it's because usually we use", "tokens": [1042, 11, 309, 311, 570, 2673, 321, 764], "temperature": 0.0, "avg_logprob": -0.12602278391520183, "compression_ratio": 1.670995670995671, "no_speech_prob": 5.142603549757041e-05}, {"id": 65, "seek": 29276, "start": 300.32, "end": 304.68, "text": " the letter p to describe probability density functions of interest,", "tokens": [264, 5063, 280, 281, 6786, 8482, 10305, 6828, 295, 1179, 11], "temperature": 0.0, "avg_logprob": -0.12602278391520183, "compression_ratio": 1.670995670995671, "no_speech_prob": 5.142603549757041e-05}, {"id": 66, "seek": 29276, "start": 304.68, "end": 308.56, "text": " and then because q is right after that, it's another common one.", "tokens": [293, 550, 570, 9505, 307, 558, 934, 300, 11, 309, 311, 1071, 2689, 472, 13], "temperature": 0.0, "avg_logprob": -0.12602278391520183, "compression_ratio": 1.670995670995671, "no_speech_prob": 5.142603549757041e-05}, {"id": 67, "seek": 29276, "start": 308.56, "end": 313.12, "text": " So it's like how you use x and y, we use p and q.", "tokens": [407, 309, 311, 411, 577, 291, 764, 2031, 293, 288, 11, 321, 764, 280, 293, 9505, 13], "temperature": 0.0, "avg_logprob": -0.12602278391520183, "compression_ratio": 1.670995670995671, "no_speech_prob": 5.142603549757041e-05}, {"id": 68, "seek": 29276, "start": 313.12, "end": 316.0, "text": " The fact that we use q here instead of p is", "tokens": [440, 1186, 300, 321, 764, 9505, 510, 2602, 295, 280, 307], "temperature": 0.0, "avg_logprob": -0.12602278391520183, "compression_ratio": 1.670995670995671, "no_speech_prob": 5.142603549757041e-05}, {"id": 69, "seek": 29276, "start": 316.0, "end": 321.08, "text": " because it suggests that there might be a p that we'll introduce,", "tokens": [570, 309, 13409, 300, 456, 1062, 312, 257, 280, 300, 321, 603, 5366, 11], "temperature": 0.0, "avg_logprob": -0.12602278391520183, "compression_ratio": 1.670995670995671, "no_speech_prob": 5.142603549757041e-05}, {"id": 70, "seek": 32108, "start": 321.08, "end": 323.64, "text": " and maybe p is the thing that we're modeling,", "tokens": [293, 1310, 280, 307, 264, 551, 300, 321, 434, 15983, 11], "temperature": 0.0, "avg_logprob": -0.18406853720406505, "compression_ratio": 1.6277056277056277, "no_speech_prob": 3.372142600710504e-05}, {"id": 71, "seek": 32108, "start": 323.64, "end": 326.91999999999996, "text": " and q is supplementary to that.", "tokens": [293, 9505, 307, 15436, 822, 281, 300, 13], "temperature": 0.0, "avg_logprob": -0.18406853720406505, "compression_ratio": 1.6277056277056277, "no_speech_prob": 3.372142600710504e-05}, {"id": 72, "seek": 32108, "start": 326.91999999999996, "end": 329.8, "text": " Does that sound right, Tanishq?", "tokens": [4402, 300, 1626, 558, 11, 314, 7524, 80, 30], "temperature": 0.0, "avg_logprob": -0.18406853720406505, "compression_ratio": 1.6277056277056277, "no_speech_prob": 3.372142600710504e-05}, {"id": 73, "seek": 32108, "start": 329.8, "end": 333.84, "text": " Yeah. I think it's also helpful to maybe think", "tokens": [865, 13, 286, 519, 309, 311, 611, 4961, 281, 1310, 519], "temperature": 0.0, "avg_logprob": -0.18406853720406505, "compression_ratio": 1.6277056277056277, "no_speech_prob": 3.372142600710504e-05}, {"id": 74, "seek": 32108, "start": 333.84, "end": 338.15999999999997, "text": " about x0 in a more practical, concrete way.", "tokens": [466, 2031, 15, 294, 257, 544, 8496, 11, 9859, 636, 13], "temperature": 0.0, "avg_logprob": -0.18406853720406505, "compression_ratio": 1.6277056277056277, "no_speech_prob": 3.372142600710504e-05}, {"id": 75, "seek": 32108, "start": 338.15999999999997, "end": 340.47999999999996, "text": " Of course, if we work with images,", "tokens": [2720, 1164, 11, 498, 321, 589, 365, 5267, 11], "temperature": 0.0, "avg_logprob": -0.18406853720406505, "compression_ratio": 1.6277056277056277, "no_speech_prob": 3.372142600710504e-05}, {"id": 76, "seek": 32108, "start": 340.47999999999996, "end": 344.59999999999997, "text": " then x0 would be that's what's representing the images.", "tokens": [550, 2031, 15, 576, 312, 300, 311, 437, 311, 13460, 264, 5267, 13], "temperature": 0.0, "avg_logprob": -0.18406853720406505, "compression_ratio": 1.6277056277056277, "no_speech_prob": 3.372142600710504e-05}, {"id": 77, "seek": 32108, "start": 344.59999999999997, "end": 350.0, "text": " So it's also useful to think about it from that concrete practical approach as well.", "tokens": [407, 309, 311, 611, 4420, 281, 519, 466, 309, 490, 300, 9859, 8496, 3109, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.18406853720406505, "compression_ratio": 1.6277056277056277, "no_speech_prob": 3.372142600710504e-05}, {"id": 78, "seek": 35000, "start": 350.0, "end": 357.72, "text": " So x0 might be an MNIST digit.", "tokens": [407, 2031, 15, 1062, 312, 364, 376, 45, 19756, 14293, 13], "temperature": 0.0, "avg_logprob": -0.25675627280925883, "compression_ratio": 1.205128205128205, "no_speech_prob": 3.760256731766276e-05}, {"id": 79, "seek": 35000, "start": 357.72, "end": 361.48, "text": " Then we got q. So q,", "tokens": [1396, 321, 658, 9505, 13, 407, 9505, 11], "temperature": 0.0, "avg_logprob": -0.25675627280925883, "compression_ratio": 1.205128205128205, "no_speech_prob": 3.760256731766276e-05}, {"id": 80, "seek": 35000, "start": 361.48, "end": 367.04, "text": " I'll just use this to mean q is some function.", "tokens": [286, 603, 445, 764, 341, 281, 914, 9505, 307, 512, 2445, 13], "temperature": 0.0, "avg_logprob": -0.25675627280925883, "compression_ratio": 1.205128205128205, "no_speech_prob": 3.760256731766276e-05}, {"id": 81, "seek": 35000, "start": 367.04, "end": 369.64, "text": " So we look at it as a box.", "tokens": [407, 321, 574, 412, 309, 382, 257, 2424, 13], "temperature": 0.0, "avg_logprob": -0.25675627280925883, "compression_ratio": 1.205128205128205, "no_speech_prob": 3.760256731766276e-05}, {"id": 82, "seek": 35000, "start": 374.56, "end": 378.56, "text": " It takes in x0,", "tokens": [467, 2516, 294, 2031, 15, 11], "temperature": 0.0, "avg_logprob": -0.25675627280925883, "compression_ratio": 1.205128205128205, "no_speech_prob": 3.760256731766276e-05}, {"id": 83, "seek": 37856, "start": 378.56, "end": 384.08, "text": " and it gives us the probability that this x0,", "tokens": [293, 309, 2709, 505, 264, 8482, 300, 341, 2031, 15, 11], "temperature": 0.0, "avg_logprob": -0.14134178505287515, "compression_ratio": 1.5159817351598173, "no_speech_prob": 0.00017585940076969564}, {"id": 84, "seek": 37856, "start": 384.08, "end": 387.92, "text": " which is an image, looks like an MNIST digit.", "tokens": [597, 307, 364, 3256, 11, 1542, 411, 364, 376, 45, 19756, 14293, 13], "temperature": 0.0, "avg_logprob": -0.14134178505287515, "compression_ratio": 1.5159817351598173, "no_speech_prob": 0.00017585940076969564}, {"id": 85, "seek": 37856, "start": 387.92, "end": 393.28000000000003, "text": " So in this case, this would be 0.9 or maybe even,", "tokens": [407, 294, 341, 1389, 11, 341, 576, 312, 1958, 13, 24, 420, 1310, 754, 11], "temperature": 0.0, "avg_logprob": -0.14134178505287515, "compression_ratio": 1.5159817351598173, "no_speech_prob": 0.00017585940076969564}, {"id": 86, "seek": 37856, "start": 393.28000000000003, "end": 394.96, "text": " yeah, it's still 0.98.", "tokens": [1338, 11, 309, 311, 920, 1958, 13, 22516, 13], "temperature": 0.0, "avg_logprob": -0.14134178505287515, "compression_ratio": 1.5159817351598173, "no_speech_prob": 0.00017585940076969564}, {"id": 87, "seek": 37856, "start": 394.96, "end": 398.52, "text": " So this is quite high probability that this is an MNIST digit.", "tokens": [407, 341, 307, 1596, 1090, 8482, 300, 341, 307, 364, 376, 45, 19756, 14293, 13], "temperature": 0.0, "avg_logprob": -0.14134178505287515, "compression_ratio": 1.5159817351598173, "no_speech_prob": 0.00017585940076969564}, {"id": 88, "seek": 37856, "start": 398.52, "end": 401.32, "text": " Hi, this is Jeremy. Can I jump in for a moment?", "tokens": [2421, 11, 341, 307, 17809, 13, 1664, 286, 3012, 294, 337, 257, 1623, 30], "temperature": 0.0, "avg_logprob": -0.14134178505287515, "compression_ratio": 1.5159817351598173, "no_speech_prob": 0.00017585940076969564}, {"id": 89, "seek": 37856, "start": 401.32, "end": 402.4, "text": " Please do.", "tokens": [2555, 360, 13], "temperature": 0.0, "avg_logprob": -0.14134178505287515, "compression_ratio": 1.5159817351598173, "no_speech_prob": 0.00017585940076969564}, {"id": 90, "seek": 37856, "start": 402.4, "end": 405.32, "text": " Oh, thank you. I just wanted to double-check.", "tokens": [876, 11, 1309, 291, 13, 286, 445, 1415, 281, 3834, 12, 15723, 13], "temperature": 0.0, "avg_logprob": -0.14134178505287515, "compression_ratio": 1.5159817351598173, "no_speech_prob": 0.00017585940076969564}, {"id": 91, "seek": 40532, "start": 405.32, "end": 410.59999999999997, "text": " This looks a lot like the magic API that we had at the start of lesson 1,", "tokens": [639, 1542, 257, 688, 411, 264, 5585, 9362, 300, 321, 632, 412, 264, 722, 295, 6898, 502, 11], "temperature": 0.0, "avg_logprob": -0.2102969487508138, "compression_ratio": 1.5442477876106195, "no_speech_prob": 3.1197687349049374e-05}, {"id": 92, "seek": 40532, "start": 410.59999999999997, "end": 413.0, "text": " that you feed in a digit and it gives you back a probability.", "tokens": [300, 291, 3154, 294, 257, 14293, 293, 309, 2709, 291, 646, 257, 8482, 13], "temperature": 0.0, "avg_logprob": -0.2102969487508138, "compression_ratio": 1.5442477876106195, "no_speech_prob": 3.1197687349049374e-05}, {"id": 93, "seek": 40532, "start": 413.0, "end": 415.71999999999997, "text": " Is that basically what Q is doing here?", "tokens": [1119, 300, 1936, 437, 1249, 307, 884, 510, 30], "temperature": 0.0, "avg_logprob": -0.2102969487508138, "compression_ratio": 1.5442477876106195, "no_speech_prob": 3.1197687349049374e-05}, {"id": 94, "seek": 40532, "start": 415.71999999999997, "end": 419.12, "text": " Absolutely. It's a magic API.", "tokens": [7021, 13, 467, 311, 257, 5585, 9362, 13], "temperature": 0.0, "avg_logprob": -0.2102969487508138, "compression_ratio": 1.5442477876106195, "no_speech_prob": 3.1197687349049374e-05}, {"id": 95, "seek": 40532, "start": 419.12, "end": 420.36, "text": " It's a good way to think of it.", "tokens": [467, 311, 257, 665, 636, 281, 519, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.2102969487508138, "compression_ratio": 1.5442477876106195, "no_speech_prob": 3.1197687349049374e-05}, {"id": 96, "seek": 40532, "start": 420.36, "end": 424.15999999999997, "text": " We couldn't write down what Q is,", "tokens": [492, 2809, 380, 2464, 760, 437, 1249, 307, 11], "temperature": 0.0, "avg_logprob": -0.2102969487508138, "compression_ratio": 1.5442477876106195, "no_speech_prob": 3.1197687349049374e-05}, {"id": 97, "seek": 40532, "start": 424.15999999999997, "end": 428.24, "text": " but we imagine that somebody has it somewhere.", "tokens": [457, 321, 3811, 300, 2618, 575, 309, 4079, 13], "temperature": 0.0, "avg_logprob": -0.2102969487508138, "compression_ratio": 1.5442477876106195, "no_speech_prob": 3.1197687349049374e-05}, {"id": 98, "seek": 40532, "start": 429.03999999999996, "end": 431.64, "text": " So this is a concrete example.", "tokens": [407, 341, 307, 257, 9859, 1365, 13], "temperature": 0.0, "avg_logprob": -0.2102969487508138, "compression_ratio": 1.5442477876106195, "no_speech_prob": 3.1197687349049374e-05}, {"id": 99, "seek": 43164, "start": 431.64, "end": 436.0, "text": " If you had to do something to this image,", "tokens": [759, 291, 632, 281, 360, 746, 281, 341, 3256, 11], "temperature": 0.0, "avg_logprob": -0.2252778445973116, "compression_ratio": 1.5517241379310345, "no_speech_prob": 4.820510002900846e-05}, {"id": 100, "seek": 43164, "start": 436.0, "end": 439.47999999999996, "text": " you might get a smaller number here.", "tokens": [291, 1062, 483, 257, 4356, 1230, 510, 13], "temperature": 0.0, "avg_logprob": -0.2252778445973116, "compression_ratio": 1.5517241379310345, "no_speech_prob": 4.820510002900846e-05}, {"id": 101, "seek": 43164, "start": 440.68, "end": 446.91999999999996, "text": " So another thing worth mentioning here is probability density functions.", "tokens": [407, 1071, 551, 3163, 18315, 510, 307, 8482, 10305, 6828, 13], "temperature": 0.0, "avg_logprob": -0.2252778445973116, "compression_ratio": 1.5517241379310345, "no_speech_prob": 4.820510002900846e-05}, {"id": 102, "seek": 43164, "start": 446.91999999999996, "end": 450.84, "text": " So these are these magic APIs that give us a number,", "tokens": [407, 613, 366, 613, 5585, 21445, 300, 976, 505, 257, 1230, 11], "temperature": 0.0, "avg_logprob": -0.2252778445973116, "compression_ratio": 1.5517241379310345, "no_speech_prob": 4.820510002900846e-05}, {"id": 103, "seek": 43164, "start": 450.84, "end": 453.71999999999997, "text": " tells us how likely the thing is.", "tokens": [5112, 505, 577, 3700, 264, 551, 307, 13], "temperature": 0.0, "avg_logprob": -0.2252778445973116, "compression_ratio": 1.5517241379310345, "no_speech_prob": 4.820510002900846e-05}, {"id": 104, "seek": 43164, "start": 454.76, "end": 457.15999999999997, "text": " You don't often see them,", "tokens": [509, 500, 380, 2049, 536, 552, 11], "temperature": 0.0, "avg_logprob": -0.2252778445973116, "compression_ratio": 1.5517241379310345, "no_speech_prob": 4.820510002900846e-05}, {"id": 105, "seek": 43164, "start": 457.15999999999997, "end": 459.71999999999997, "text": " they don't often make it all the way to your code.", "tokens": [436, 500, 380, 2049, 652, 309, 439, 264, 636, 281, 428, 3089, 13], "temperature": 0.0, "avg_logprob": -0.2252778445973116, "compression_ratio": 1.5517241379310345, "no_speech_prob": 4.820510002900846e-05}, {"id": 106, "seek": 45972, "start": 459.72, "end": 463.64000000000004, "text": " In fact, they very rarely will appear in your code.", "tokens": [682, 1186, 11, 436, 588, 13752, 486, 4204, 294, 428, 3089, 13], "temperature": 0.0, "avg_logprob": -0.2008382338511793, "compression_ratio": 1.6310679611650485, "no_speech_prob": 8.797160990070552e-05}, {"id": 107, "seek": 45972, "start": 463.64000000000004, "end": 470.6, "text": " But it turns out that there are very useful ways or tools to work with random quantities.", "tokens": [583, 309, 4523, 484, 300, 456, 366, 588, 4420, 2098, 420, 3873, 281, 589, 365, 4974, 22927, 13], "temperature": 0.0, "avg_logprob": -0.2008382338511793, "compression_ratio": 1.6310679611650485, "no_speech_prob": 8.797160990070552e-05}, {"id": 108, "seek": 45972, "start": 470.6, "end": 476.44000000000005, "text": " Because they allow you to represent random quantities as functions,", "tokens": [1436, 436, 2089, 291, 281, 2906, 4974, 22927, 382, 6828, 11], "temperature": 0.0, "avg_logprob": -0.2008382338511793, "compression_ratio": 1.6310679611650485, "no_speech_prob": 8.797160990070552e-05}, {"id": 109, "seek": 45972, "start": 476.44000000000005, "end": 478.56, "text": " just ordinary functions.", "tokens": [445, 10547, 6828, 13], "temperature": 0.0, "avg_logprob": -0.2008382338511793, "compression_ratio": 1.6310679611650485, "no_speech_prob": 8.797160990070552e-05}, {"id": 110, "seek": 45972, "start": 478.56, "end": 480.24, "text": " And because they're functions,", "tokens": [400, 570, 436, 434, 6828, 11], "temperature": 0.0, "avg_logprob": -0.2008382338511793, "compression_ratio": 1.6310679611650485, "no_speech_prob": 8.797160990070552e-05}, {"id": 111, "seek": 45972, "start": 480.24, "end": 487.40000000000003, "text": " you have a whole centuries worth of math to analyze and understand it.", "tokens": [291, 362, 257, 1379, 13926, 3163, 295, 5221, 281, 12477, 293, 1223, 309, 13], "temperature": 0.0, "avg_logprob": -0.2008382338511793, "compression_ratio": 1.6310679611650485, "no_speech_prob": 8.797160990070552e-05}, {"id": 112, "seek": 48740, "start": 487.4, "end": 491.79999999999995, "text": " So you'll often find probability density functions in papers,", "tokens": [407, 291, 603, 2049, 915, 8482, 10305, 6828, 294, 10577, 11], "temperature": 0.0, "avg_logprob": -0.17251251404543957, "compression_ratio": 1.610091743119266, "no_speech_prob": 0.0001365589560009539}, {"id": 113, "seek": 48740, "start": 491.79999999999995, "end": 498.79999999999995, "text": " and eventually they work out to really simple equations or formulas that end up in your code.", "tokens": [293, 4728, 436, 589, 484, 281, 534, 2199, 11787, 420, 30546, 300, 917, 493, 294, 428, 3089, 13], "temperature": 0.0, "avg_logprob": -0.17251251404543957, "compression_ratio": 1.610091743119266, "no_speech_prob": 0.0001365589560009539}, {"id": 114, "seek": 48740, "start": 501.0, "end": 504.23999999999995, "text": " Do you want to add anything, Tanish?", "tokens": [1144, 291, 528, 281, 909, 1340, 11, 314, 7524, 30], "temperature": 0.0, "avg_logprob": -0.17251251404543957, "compression_ratio": 1.610091743119266, "no_speech_prob": 0.0001365589560009539}, {"id": 115, "seek": 48740, "start": 504.23999999999995, "end": 506.35999999999996, "text": " I think that sounds all correct.", "tokens": [286, 519, 300, 3263, 439, 3006, 13], "temperature": 0.0, "avg_logprob": -0.17251251404543957, "compression_ratio": 1.610091743119266, "no_speech_prob": 0.0001365589560009539}, {"id": 116, "seek": 48740, "start": 506.35999999999996, "end": 513.1999999999999, "text": " Of course, I think you probably will go over some examples of probability density functions,", "tokens": [2720, 1164, 11, 286, 519, 291, 1391, 486, 352, 670, 512, 5110, 295, 8482, 10305, 6828, 11], "temperature": 0.0, "avg_logprob": -0.17251251404543957, "compression_ratio": 1.610091743119266, "no_speech_prob": 0.0001365589560009539}, {"id": 117, "seek": 48740, "start": 513.1999999999999, "end": 514.84, "text": " especially relevant to this one.", "tokens": [2318, 7340, 281, 341, 472, 13], "temperature": 0.0, "avg_logprob": -0.17251251404543957, "compression_ratio": 1.610091743119266, "no_speech_prob": 0.0001365589560009539}, {"id": 118, "seek": 51484, "start": 514.84, "end": 521.72, "text": " But yeah, it's useful to think about the also the sorts of functions you may have in a simplified case.", "tokens": [583, 1338, 11, 309, 311, 4420, 281, 519, 466, 264, 611, 264, 7527, 295, 6828, 291, 815, 362, 294, 257, 26335, 1389, 13], "temperature": 0.0, "avg_logprob": -0.19548159837722778, "compression_ratio": 1.5871559633027523, "no_speech_prob": 9.021397272590548e-05}, {"id": 119, "seek": 51484, "start": 521.72, "end": 525.4, "text": " And that's what we probably are going to talk about next, right?", "tokens": [400, 300, 311, 437, 321, 1391, 366, 516, 281, 751, 466, 958, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.19548159837722778, "compression_ratio": 1.5871559633027523, "no_speech_prob": 9.021397272590548e-05}, {"id": 120, "seek": 51484, "start": 525.4, "end": 527.5600000000001, "text": " Yeah, yeah, that's exactly what we talk.", "tokens": [865, 11, 1338, 11, 300, 311, 2293, 437, 321, 751, 13], "temperature": 0.0, "avg_logprob": -0.19548159837722778, "compression_ratio": 1.5871559633027523, "no_speech_prob": 9.021397272590548e-05}, {"id": 121, "seek": 51484, "start": 527.5600000000001, "end": 532.1600000000001, "text": " So we have this Qx0, and then we introduce another one.", "tokens": [407, 321, 362, 341, 1249, 87, 15, 11, 293, 550, 321, 5366, 1071, 472, 13], "temperature": 0.0, "avg_logprob": -0.19548159837722778, "compression_ratio": 1.5871559633027523, "no_speech_prob": 9.021397272590548e-05}, {"id": 122, "seek": 51484, "start": 532.1600000000001, "end": 539.6800000000001, "text": " And like you said, this is going to turn out to have a really nice, simple form.", "tokens": [400, 411, 291, 848, 11, 341, 307, 516, 281, 1261, 484, 281, 362, 257, 534, 1481, 11, 2199, 1254, 13], "temperature": 0.0, "avg_logprob": -0.19548159837722778, "compression_ratio": 1.5871559633027523, "no_speech_prob": 9.021397272590548e-05}, {"id": 123, "seek": 53968, "start": 539.68, "end": 549.12, "text": " But before that, the next thing we define is QxT, QnxT minus one.", "tokens": [583, 949, 300, 11, 264, 958, 551, 321, 6964, 307, 1249, 87, 51, 11, 1249, 77, 87, 51, 3175, 472, 13], "temperature": 0.0, "avg_logprob": -0.14793115503647747, "compression_ratio": 1.5696202531645569, "no_speech_prob": 3.587257015169598e-05}, {"id": 124, "seek": 53968, "start": 549.12, "end": 551.8399999999999, "text": " So we'll say what we define this to be.", "tokens": [407, 321, 603, 584, 437, 321, 6964, 341, 281, 312, 13], "temperature": 0.0, "avg_logprob": -0.14793115503647747, "compression_ratio": 1.5696202531645569, "no_speech_prob": 3.587257015169598e-05}, {"id": 125, "seek": 53968, "start": 551.8399999999999, "end": 557.3599999999999, "text": " But to begin with, this is another probability density function.", "tokens": [583, 281, 1841, 365, 11, 341, 307, 1071, 8482, 10305, 2445, 13], "temperature": 0.0, "avg_logprob": -0.14793115503647747, "compression_ratio": 1.5696202531645569, "no_speech_prob": 3.587257015169598e-05}, {"id": 126, "seek": 53968, "start": 557.3599999999999, "end": 563.52, "text": " And this bar over here means it's a conditional probability density function,", "tokens": [400, 341, 2159, 670, 510, 1355, 309, 311, 257, 27708, 8482, 10305, 2445, 11], "temperature": 0.0, "avg_logprob": -0.14793115503647747, "compression_ratio": 1.5696202531645569, "no_speech_prob": 3.587257015169598e-05}, {"id": 127, "seek": 56352, "start": 563.52, "end": 573.6, "text": " which you can think of as you are given the thing on the right to calculate probabilities over the thing on the left.", "tokens": [597, 291, 393, 519, 295, 382, 291, 366, 2212, 264, 551, 322, 264, 558, 281, 8873, 33783, 670, 264, 551, 322, 264, 1411, 13], "temperature": 0.0, "avg_logprob": -0.10780235926310221, "compression_ratio": 1.5775862068965518, "no_speech_prob": 2.1079153157188557e-05}, {"id": 128, "seek": 56352, "start": 573.6, "end": 583.4, "text": " In this case, you can think of it as something that takes images.", "tokens": [682, 341, 1389, 11, 291, 393, 519, 295, 309, 382, 746, 300, 2516, 5267, 13], "temperature": 0.0, "avg_logprob": -0.10780235926310221, "compression_ratio": 1.5775862068965518, "no_speech_prob": 2.1079153157188557e-05}, {"id": 129, "seek": 58340, "start": 583.4, "end": 595.04, "text": " So maybe another magic API and produces other images.", "tokens": [407, 1310, 1071, 5585, 9362, 293, 14725, 661, 5267, 13], "temperature": 0.0, "avg_logprob": -0.17076942838471512, "compression_ratio": 1.3571428571428572, "no_speech_prob": 6.852530077594565e-06}, {"id": 130, "seek": 58340, "start": 595.04, "end": 598.3199999999999, "text": " But we don't know what these look like yet because we haven't defined over here.", "tokens": [583, 321, 500, 380, 458, 437, 613, 574, 411, 1939, 570, 321, 2378, 380, 7642, 670, 510, 13], "temperature": 0.0, "avg_logprob": -0.17076942838471512, "compression_ratio": 1.3571428571428572, "no_speech_prob": 6.852530077594565e-06}, {"id": 131, "seek": 58340, "start": 598.3199999999999, "end": 605.84, "text": " And this we would call kind of, you know, Xt minus one, which could be X0.", "tokens": [400, 341, 321, 576, 818, 733, 295, 11, 291, 458, 11, 1783, 83, 3175, 472, 11, 597, 727, 312, 1783, 15, 13], "temperature": 0.0, "avg_logprob": -0.17076942838471512, "compression_ratio": 1.3571428571428572, "no_speech_prob": 6.852530077594565e-06}, {"id": 132, "seek": 60584, "start": 605.84, "end": 614.0400000000001, "text": " And this would be Xt, which in the X0 case would be X1.", "tokens": [400, 341, 576, 312, 1783, 83, 11, 597, 294, 264, 1783, 15, 1389, 576, 312, 1783, 16, 13], "temperature": 0.0, "avg_logprob": -0.13975991010665895, "compression_ratio": 1.5027322404371584, "no_speech_prob": 1.804849125619512e-05}, {"id": 133, "seek": 60584, "start": 614.0400000000001, "end": 623.32, "text": " Something worth noting here is this notation can be a little bit confusing because we've said Q is one thing earlier.", "tokens": [6595, 3163, 26801, 510, 307, 341, 24657, 393, 312, 257, 707, 857, 13181, 570, 321, 600, 848, 1249, 307, 472, 551, 3071, 13], "temperature": 0.0, "avg_logprob": -0.13975991010665895, "compression_ratio": 1.5027322404371584, "no_speech_prob": 1.804849125619512e-05}, {"id": 134, "seek": 60584, "start": 623.32, "end": 625.96, "text": " Now we're saying Q is another thing.", "tokens": [823, 321, 434, 1566, 1249, 307, 1071, 551, 13], "temperature": 0.0, "avg_logprob": -0.13975991010665895, "compression_ratio": 1.5027322404371584, "no_speech_prob": 1.804849125619512e-05}, {"id": 135, "seek": 60584, "start": 625.96, "end": 632.48, "text": " So this, yeah, I'm going to need your help on this one, Tanisha.", "tokens": [407, 341, 11, 1338, 11, 286, 478, 516, 281, 643, 428, 854, 322, 341, 472, 11, 314, 7524, 64, 13], "temperature": 0.0, "avg_logprob": -0.13975991010665895, "compression_ratio": 1.5027322404371584, "no_speech_prob": 1.804849125619512e-05}, {"id": 136, "seek": 63248, "start": 632.48, "end": 646.2, "text": " I think people would usually, if in the strictest sense, define the first one, you know, like this, maybe.", "tokens": [286, 519, 561, 576, 2673, 11, 498, 294, 264, 10910, 377, 2020, 11, 6964, 264, 700, 472, 11, 291, 458, 11, 411, 341, 11, 1310, 13], "temperature": 0.0, "avg_logprob": -0.1577745763266959, "compression_ratio": 1.2678571428571428, "no_speech_prob": 0.00010082429798785597}, {"id": 137, "seek": 63248, "start": 646.2, "end": 654.4, "text": " And the second one was a subscript.", "tokens": [400, 264, 1150, 472, 390, 257, 2325, 662, 13], "temperature": 0.0, "avg_logprob": -0.1577745763266959, "compression_ratio": 1.2678571428571428, "no_speech_prob": 0.00010082429798785597}, {"id": 138, "seek": 65440, "start": 654.4, "end": 674.6, "text": " And this notation that we see here on the left is just a shortcut where they wanted to save the space of writing that and kind of included that, implied it by what was in the practice. Is that true?", "tokens": [400, 341, 24657, 300, 321, 536, 510, 322, 264, 1411, 307, 445, 257, 24822, 689, 436, 1415, 281, 3155, 264, 1901, 295, 3579, 300, 293, 733, 295, 5556, 300, 11, 32614, 309, 538, 437, 390, 294, 264, 3124, 13, 1119, 300, 2074, 30], "temperature": 0.0, "avg_logprob": -0.1923512923411834, "compression_ratio": 1.5656565656565657, "no_speech_prob": 3.582015779102221e-05}, {"id": 139, "seek": 65440, "start": 674.6, "end": 683.28, "text": " Yeah, I mean, I think here they use the variables Q and then, of course, later on we see P to kind of describe,", "tokens": [865, 11, 286, 914, 11, 286, 519, 510, 436, 764, 264, 9102, 1249, 293, 550, 11, 295, 1164, 11, 1780, 322, 321, 536, 430, 281, 733, 295, 6786, 11], "temperature": 0.0, "avg_logprob": -0.1923512923411834, "compression_ratio": 1.5656565656565657, "no_speech_prob": 3.582015779102221e-05}, {"id": 140, "seek": 68328, "start": 683.28, "end": 692.4399999999999, "text": " as we'll see different aspects of the diffusion model, the sort of different processes of the diffusion model, which we'll see.", "tokens": [382, 321, 603, 536, 819, 7270, 295, 264, 25242, 2316, 11, 264, 1333, 295, 819, 7555, 295, 264, 25242, 2316, 11, 597, 321, 603, 536, 13], "temperature": 0.0, "avg_logprob": -0.1222743710267891, "compression_ratio": 2.0271493212669682, "no_speech_prob": 0.00017937005031853914}, {"id": 141, "seek": 68328, "start": 692.4399999999999, "end": 699.68, "text": " So I think that's what, you know, they use the same variables to kind of demonstrate this is corresponding to this process.", "tokens": [407, 286, 519, 300, 311, 437, 11, 291, 458, 11, 436, 764, 264, 912, 9102, 281, 733, 295, 11698, 341, 307, 11760, 281, 341, 1399, 13], "temperature": 0.0, "avg_logprob": -0.1222743710267891, "compression_ratio": 2.0271493212669682, "no_speech_prob": 0.00017937005031853914}, {"id": 142, "seek": 68328, "start": 699.68, "end": 703.4399999999999, "text": " The other variable corresponds to the other process of the diffusion model.", "tokens": [440, 661, 7006, 23249, 281, 264, 661, 1399, 295, 264, 25242, 2316, 13], "temperature": 0.0, "avg_logprob": -0.1222743710267891, "compression_ratio": 2.0271493212669682, "no_speech_prob": 0.00017937005031853914}, {"id": 143, "seek": 68328, "start": 703.4399999999999, "end": 705.52, "text": " So we'll obviously go over that.", "tokens": [407, 321, 603, 2745, 352, 670, 300, 13], "temperature": 0.0, "avg_logprob": -0.1222743710267891, "compression_ratio": 2.0271493212669682, "no_speech_prob": 0.00017937005031853914}, {"id": 144, "seek": 68328, "start": 705.52, "end": 713.12, "text": " So I think that's where those variables or those letters are being used in that matter.", "tokens": [407, 286, 519, 300, 311, 689, 729, 9102, 420, 729, 7825, 366, 885, 1143, 294, 300, 1871, 13], "temperature": 0.0, "avg_logprob": -0.1222743710267891, "compression_ratio": 2.0271493212669682, "no_speech_prob": 0.00017937005031853914}, {"id": 145, "seek": 71312, "start": 713.12, "end": 720.28, "text": " But if you do want to make it more specific, more clear, yeah, I think that notation is fine as well.", "tokens": [583, 498, 291, 360, 528, 281, 652, 309, 544, 2685, 11, 544, 1850, 11, 1338, 11, 286, 519, 300, 24657, 307, 2489, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.12267770884949485, "compression_ratio": 1.4736842105263157, "no_speech_prob": 9.877006959868595e-05}, {"id": 146, "seek": 71312, "start": 720.28, "end": 725.32, "text": " Right. OK. Yeah, that makes sense.", "tokens": [1779, 13, 2264, 13, 865, 11, 300, 1669, 2020, 13], "temperature": 0.0, "avg_logprob": -0.12267770884949485, "compression_ratio": 1.4736842105263157, "no_speech_prob": 9.877006959868595e-05}, {"id": 147, "seek": 71312, "start": 725.32, "end": 732.04, "text": " OK, so let's describe what this Q does to the image on the left to produce the one on the right.", "tokens": [2264, 11, 370, 718, 311, 6786, 437, 341, 1249, 775, 281, 264, 3256, 322, 264, 1411, 281, 5258, 264, 472, 322, 264, 558, 13], "temperature": 0.0, "avg_logprob": -0.12267770884949485, "compression_ratio": 1.4736842105263157, "no_speech_prob": 9.877006959868595e-05}, {"id": 148, "seek": 71312, "start": 732.04, "end": 741.6800000000001, "text": " So I'll start over here so we have more space.", "tokens": [407, 286, 603, 722, 670, 510, 370, 321, 362, 544, 1901, 13], "temperature": 0.0, "avg_logprob": -0.12267770884949485, "compression_ratio": 1.4736842105263157, "no_speech_prob": 9.877006959868595e-05}, {"id": 149, "seek": 74168, "start": 741.68, "end": 760.64, "text": " I'll write it out first and then we can go into the details.", "tokens": [286, 603, 2464, 309, 484, 700, 293, 550, 321, 393, 352, 666, 264, 4365, 13], "temperature": 0.0, "avg_logprob": -0.11713752539261528, "compression_ratio": 1.3089430894308942, "no_speech_prob": 7.219369581434876e-05}, {"id": 150, "seek": 74168, "start": 760.64, "end": 771.5999999999999, "text": " OK, so kind of like the bar, you can think of this semicolon as, you know, grouping things together.", "tokens": [2264, 11, 370, 733, 295, 411, 264, 2159, 11, 291, 393, 519, 295, 341, 27515, 38780, 382, 11, 291, 458, 11, 40149, 721, 1214, 13], "temperature": 0.0, "avg_logprob": -0.11713752539261528, "compression_ratio": 1.3089430894308942, "no_speech_prob": 7.219369581434876e-05}, {"id": 151, "seek": 77160, "start": 771.6, "end": 776.52, "text": " And so you have the things on the left and things on the right.", "tokens": [400, 370, 291, 362, 264, 721, 322, 264, 1411, 293, 721, 322, 264, 558, 13], "temperature": 0.0, "avg_logprob": -0.1590521772142867, "compression_ratio": 1.7898089171974523, "no_speech_prob": 3.4773813240462914e-05}, {"id": 152, "seek": 77160, "start": 776.52, "end": 788.88, "text": " My understanding is these two things on the right are the parameters of the model, sorry, of the probability.", "tokens": [1222, 3701, 307, 613, 732, 721, 322, 264, 558, 366, 264, 9834, 295, 264, 2316, 11, 2597, 11, 295, 264, 8482, 13], "temperature": 0.0, "avg_logprob": -0.1590521772142867, "compression_ratio": 1.7898089171974523, "no_speech_prob": 3.4773813240462914e-05}, {"id": 153, "seek": 77160, "start": 788.88, "end": 795.8000000000001, "text": " And the thing on the left is actually, Dhanesh, could you help me understand what the thing on the left is?", "tokens": [400, 264, 551, 322, 264, 1411, 307, 767, 11, 413, 3451, 14935, 11, 727, 291, 854, 385, 1223, 437, 264, 551, 322, 264, 1411, 307, 30], "temperature": 0.0, "avg_logprob": -0.1590521772142867, "compression_ratio": 1.7898089171974523, "no_speech_prob": 3.4773813240462914e-05}, {"id": 154, "seek": 79580, "start": 795.8, "end": 806.5999999999999, "text": " Do you know? Right. Well, so this is again like a probability distribution and the thing on the left is a probability distribution for this particular variable.", "tokens": [1144, 291, 458, 30, 1779, 13, 1042, 11, 370, 341, 307, 797, 411, 257, 8482, 7316, 293, 264, 551, 322, 264, 1411, 307, 257, 8482, 7316, 337, 341, 1729, 7006, 13], "temperature": 0.0, "avg_logprob": -0.12466181516647339, "compression_ratio": 1.8888888888888888, "no_speech_prob": 5.5601150961592793e-05}, {"id": 155, "seek": 79580, "start": 806.5999999999999, "end": 810.28, "text": " So that's just representing what it is a probability distribution for.", "tokens": [407, 300, 311, 445, 13460, 437, 309, 307, 257, 8482, 7316, 337, 13], "temperature": 0.0, "avg_logprob": -0.12466181516647339, "compression_ratio": 1.8888888888888888, "no_speech_prob": 5.5601150961592793e-05}, {"id": 156, "seek": 79580, "start": 810.28, "end": 816.76, "text": " And then the stuff on the right are the parameters for this probability distribution.", "tokens": [400, 550, 264, 1507, 322, 264, 558, 366, 264, 9834, 337, 341, 8482, 7316, 13], "temperature": 0.0, "avg_logprob": -0.12466181516647339, "compression_ratio": 1.8888888888888888, "no_speech_prob": 5.5601150961592793e-05}, {"id": 157, "seek": 79580, "start": 816.76, "end": 819.4799999999999, "text": " So that's kind of what's going on here.", "tokens": [407, 300, 311, 733, 295, 437, 311, 516, 322, 510, 13], "temperature": 0.0, "avg_logprob": -0.12466181516647339, "compression_ratio": 1.8888888888888888, "no_speech_prob": 5.5601150961592793e-05}, {"id": 158, "seek": 81948, "start": 819.48, "end": 826.2, "text": " So, like, yeah, any time you have a normal distribution and it's describing some variable, you'll have that sort of notation.", "tokens": [407, 11, 411, 11, 1338, 11, 604, 565, 291, 362, 257, 2710, 7316, 293, 309, 311, 16141, 512, 7006, 11, 291, 603, 362, 300, 1333, 295, 24657, 13], "temperature": 0.0, "avg_logprob": -0.16235460860005926, "compression_ratio": 1.8317307692307692, "no_speech_prob": 1.3626451618620194e-05}, {"id": 159, "seek": 81948, "start": 826.2, "end": 829.76, "text": " It's the normal distribution of some variable.", "tokens": [467, 311, 264, 2710, 7316, 295, 512, 7006, 13], "temperature": 0.0, "avg_logprob": -0.16235460860005926, "compression_ratio": 1.8317307692307692, "no_speech_prob": 1.3626451618620194e-05}, {"id": 160, "seek": 81948, "start": 829.76, "end": 835.0, "text": " And then these are the parameters that describe that normal distribution.", "tokens": [400, 550, 613, 366, 264, 9834, 300, 6786, 300, 2710, 7316, 13], "temperature": 0.0, "avg_logprob": -0.16235460860005926, "compression_ratio": 1.8317307692307692, "no_speech_prob": 1.3626451618620194e-05}, {"id": 161, "seek": 81948, "start": 835.0, "end": 847.12, "text": " Right. So just to clarify, the bit after the semicolon is the bit that we're kind of used to seeing to describe a normal distribution,", "tokens": [1779, 13, 407, 445, 281, 17594, 11, 264, 857, 934, 264, 27515, 38780, 307, 264, 857, 300, 321, 434, 733, 295, 1143, 281, 2577, 281, 6786, 257, 2710, 7316, 11], "temperature": 0.0, "avg_logprob": -0.16235460860005926, "compression_ratio": 1.8317307692307692, "no_speech_prob": 1.3626451618620194e-05}, {"id": 162, "seek": 84712, "start": 847.12, "end": 852.84, "text": " which is the mean and variance of the normal distribution.", "tokens": [597, 307, 264, 914, 293, 21977, 295, 264, 2710, 7316, 13], "temperature": 0.0, "avg_logprob": -0.1477429469426473, "compression_ratio": 1.7126436781609196, "no_speech_prob": 6.912994285812601e-05}, {"id": 163, "seek": 84712, "start": 852.84, "end": 862.48, "text": " So we're going to be sampling random numbers from that normal distribution according to that mean and that variance.", "tokens": [407, 321, 434, 516, 281, 312, 21179, 4974, 3547, 490, 300, 2710, 7316, 4650, 281, 300, 914, 293, 300, 21977, 13], "temperature": 0.0, "avg_logprob": -0.1477429469426473, "compression_ratio": 1.7126436781609196, "no_speech_prob": 6.912994285812601e-05}, {"id": 164, "seek": 84712, "start": 862.48, "end": 866.64, "text": " Is that right? Yes, that's correct. Yeah.", "tokens": [1119, 300, 558, 30, 1079, 11, 300, 311, 3006, 13, 865, 13], "temperature": 0.0, "avg_logprob": -0.1477429469426473, "compression_ratio": 1.7126436781609196, "no_speech_prob": 6.912994285812601e-05}, {"id": 165, "seek": 84712, "start": 866.64, "end": 875.04, "text": " Mm hmm. Yeah. So we need to describe a bit more there about normal distribution.", "tokens": [8266, 16478, 13, 865, 13, 407, 321, 643, 281, 6786, 257, 857, 544, 456, 466, 2710, 7316, 13], "temperature": 0.0, "avg_logprob": -0.1477429469426473, "compression_ratio": 1.7126436781609196, "no_speech_prob": 6.912994285812601e-05}, {"id": 166, "seek": 87504, "start": 875.04, "end": 888.7199999999999, "text": " We kind of, you know, skip past that. So we have this fancy N and fancy letters in math for distributions usually refer to well-known distributions.", "tokens": [492, 733, 295, 11, 291, 458, 11, 10023, 1791, 300, 13, 407, 321, 362, 341, 10247, 426, 293, 10247, 7825, 294, 5221, 337, 37870, 2673, 2864, 281, 731, 12, 6861, 37870, 13], "temperature": 0.0, "avg_logprob": -0.15252837619265994, "compression_ratio": 1.6596858638743455, "no_speech_prob": 6.203176599228755e-05}, {"id": 167, "seek": 87504, "start": 888.7199999999999, "end": 898.3199999999999, "text": " And the N stands for normal, which is also known as the Gaussian distribution.", "tokens": [400, 264, 426, 7382, 337, 2710, 11, 597, 307, 611, 2570, 382, 264, 39148, 7316, 13], "temperature": 0.0, "avg_logprob": -0.15252837619265994, "compression_ratio": 1.6596858638743455, "no_speech_prob": 6.203176599228755e-05}, {"id": 168, "seek": 87504, "start": 898.3199999999999, "end": 904.4, "text": " And it's probably the most well-known probability distribution that you can you can find.", "tokens": [400, 309, 311, 1391, 264, 881, 731, 12, 6861, 8482, 7316, 300, 291, 393, 291, 393, 915, 13], "temperature": 0.0, "avg_logprob": -0.15252837619265994, "compression_ratio": 1.6596858638743455, "no_speech_prob": 6.203176599228755e-05}, {"id": 169, "seek": 90440, "start": 904.4, "end": 911.84, "text": " And when I say well-known, I mean that these things pop up everywhere.", "tokens": [400, 562, 286, 584, 731, 12, 6861, 11, 286, 914, 300, 613, 721, 1665, 493, 5315, 13], "temperature": 0.0, "avg_logprob": -0.11164574290430823, "compression_ratio": 1.6454545454545455, "no_speech_prob": 1.401166355208261e-05}, {"id": 170, "seek": 90440, "start": 911.84, "end": 916.0799999999999, "text": " You know, you can do in all sorts of fields measuring all sorts of things.", "tokens": [509, 458, 11, 291, 393, 360, 294, 439, 7527, 295, 7909, 13389, 439, 7527, 295, 721, 13], "temperature": 0.0, "avg_logprob": -0.11164574290430823, "compression_ratio": 1.6454545454545455, "no_speech_prob": 1.401166355208261e-05}, {"id": 171, "seek": 90440, "start": 916.0799999999999, "end": 922.8, "text": " Turns out that they follow roughly something that looks like this distribution.", "tokens": [29524, 484, 300, 436, 1524, 9810, 746, 300, 1542, 411, 341, 7316, 13], "temperature": 0.0, "avg_logprob": -0.11164574290430823, "compression_ratio": 1.6454545454545455, "no_speech_prob": 1.401166355208261e-05}, {"id": 172, "seek": 90440, "start": 922.8, "end": 934.36, "text": " And because they pop up so much, you know, people studied them, studied all of their properties, and we understand them really well now.", "tokens": [400, 570, 436, 1665, 493, 370, 709, 11, 291, 458, 11, 561, 9454, 552, 11, 9454, 439, 295, 641, 7221, 11, 293, 321, 1223, 552, 534, 731, 586, 13], "temperature": 0.0, "avg_logprob": -0.11164574290430823, "compression_ratio": 1.6454545454545455, "no_speech_prob": 1.401166355208261e-05}, {"id": 173, "seek": 93436, "start": 934.36, "end": 943.64, "text": " The reason that they used often in cases like this is because they turns out they have really useful properties and they're easy to work with.", "tokens": [440, 1778, 300, 436, 1143, 2049, 294, 3331, 411, 341, 307, 570, 436, 4523, 484, 436, 362, 534, 4420, 7221, 293, 436, 434, 1858, 281, 589, 365, 13], "temperature": 0.0, "avg_logprob": -0.19477463975737366, "compression_ratio": 1.6536585365853658, "no_speech_prob": 4.533595347311348e-05}, {"id": 174, "seek": 93436, "start": 943.64, "end": 949.88, "text": " Some reasons are they are described by just two parameters.", "tokens": [2188, 4112, 366, 436, 366, 7619, 538, 445, 732, 9834, 13], "temperature": 0.0, "avg_logprob": -0.19477463975737366, "compression_ratio": 1.6536585365853658, "no_speech_prob": 4.533595347311348e-05}, {"id": 175, "seek": 93436, "start": 949.88, "end": 954.28, "text": " So the mean called the mean and the covariance.", "tokens": [407, 264, 914, 1219, 264, 914, 293, 264, 49851, 719, 13], "temperature": 0.0, "avg_logprob": -0.19477463975737366, "compression_ratio": 1.6536585365853658, "no_speech_prob": 4.533595347311348e-05}, {"id": 176, "seek": 93436, "start": 954.28, "end": 961.64, "text": " Another property is that they have kind of, you know, what people would call thin tails.", "tokens": [3996, 4707, 307, 300, 436, 362, 733, 295, 11, 291, 458, 11, 437, 561, 576, 818, 5862, 28537, 13], "temperature": 0.0, "avg_logprob": -0.19477463975737366, "compression_ratio": 1.6536585365853658, "no_speech_prob": 4.533595347311348e-05}, {"id": 177, "seek": 96164, "start": 961.64, "end": 968.76, "text": " Which kind of means that they only you only need to describe their behavior in a small region of space.", "tokens": [3013, 733, 295, 1355, 300, 436, 787, 291, 787, 643, 281, 6786, 641, 5223, 294, 257, 1359, 4458, 295, 1901, 13], "temperature": 0.0, "avg_logprob": -0.2267666296525435, "compression_ratio": 1.5296803652968036, "no_speech_prob": 0.00015074590919539332}, {"id": 178, "seek": 96164, "start": 968.76, "end": 975.56, "text": " You can kind of just ignore the rest.", "tokens": [509, 393, 733, 295, 445, 11200, 264, 1472, 13], "temperature": 0.0, "avg_logprob": -0.2267666296525435, "compression_ratio": 1.5296803652968036, "no_speech_prob": 0.00015074590919539332}, {"id": 179, "seek": 96164, "start": 975.56, "end": 979.56, "text": " Do you mind drawing a quick example of a normal distribution?", "tokens": [1144, 291, 1575, 6316, 257, 1702, 1365, 295, 257, 2710, 7316, 30], "temperature": 0.0, "avg_logprob": -0.2267666296525435, "compression_ratio": 1.5296803652968036, "no_speech_prob": 0.00015074590919539332}, {"id": 180, "seek": 96164, "start": 979.56, "end": 981.88, "text": " That's a good point.", "tokens": [663, 311, 257, 665, 935, 13], "temperature": 0.0, "avg_logprob": -0.2267666296525435, "compression_ratio": 1.5296803652968036, "no_speech_prob": 0.00015074590919539332}, {"id": 181, "seek": 96164, "start": 981.88, "end": 987.08, "text": " So we have, let's say, our random variable is just one kind of dimensional.", "tokens": [407, 321, 362, 11, 718, 311, 584, 11, 527, 4974, 7006, 307, 445, 472, 733, 295, 18795, 13], "temperature": 0.0, "avg_logprob": -0.2267666296525435, "compression_ratio": 1.5296803652968036, "no_speech_prob": 0.00015074590919539332}, {"id": 182, "seek": 96164, "start": 987.08, "end": 990.68, "text": " So just a single number of floats.", "tokens": [407, 445, 257, 2167, 1230, 295, 37878, 13], "temperature": 0.0, "avg_logprob": -0.2267666296525435, "compression_ratio": 1.5296803652968036, "no_speech_prob": 0.00015074590919539332}, {"id": 183, "seek": 99068, "start": 990.68, "end": 995.4, "text": " This is sort of what the normal distribution would look like.", "tokens": [639, 307, 1333, 295, 437, 264, 2710, 7316, 576, 574, 411, 13], "temperature": 0.0, "avg_logprob": -0.1665215790271759, "compression_ratio": 1.5403726708074534, "no_speech_prob": 0.000112286361400038}, {"id": 184, "seek": 99068, "start": 995.4, "end": 1000.12, "text": " And in this case, that would be our mean.", "tokens": [400, 294, 341, 1389, 11, 300, 576, 312, 527, 914, 13], "temperature": 0.0, "avg_logprob": -0.1665215790271759, "compression_ratio": 1.5403726708074534, "no_speech_prob": 0.000112286361400038}, {"id": 185, "seek": 99068, "start": 1000.12, "end": 1009.2399999999999, "text": " And the variance would sort of describe the width over here.", "tokens": [400, 264, 21977, 576, 1333, 295, 6786, 264, 11402, 670, 510, 13], "temperature": 0.0, "avg_logprob": -0.1665215790271759, "compression_ratio": 1.5403726708074534, "no_speech_prob": 0.000112286361400038}, {"id": 186, "seek": 99068, "start": 1009.2399999999999, "end": 1016.28, "text": " Which in this case, you'd use a small sigma because you're doing a single variable.", "tokens": [3013, 294, 341, 1389, 11, 291, 1116, 764, 257, 1359, 12771, 570, 291, 434, 884, 257, 2167, 7006, 13], "temperature": 0.0, "avg_logprob": -0.1665215790271759, "compression_ratio": 1.5403726708074534, "no_speech_prob": 0.000112286361400038}, {"id": 187, "seek": 101628, "start": 1016.28, "end": 1026.92, "text": " In our case, we use a capital sigma, which is the symbol for multiple variables or multiple dimensions.", "tokens": [682, 527, 1389, 11, 321, 764, 257, 4238, 12771, 11, 597, 307, 264, 5986, 337, 3866, 9102, 420, 3866, 12819, 13], "temperature": 0.0, "avg_logprob": -0.18327256043752035, "compression_ratio": 1.6021505376344085, "no_speech_prob": 9.601338388165459e-05}, {"id": 188, "seek": 101628, "start": 1026.92, "end": 1031.0, "text": " And yeah, I also didn't say that this is the Greek letter mu.", "tokens": [400, 1338, 11, 286, 611, 994, 380, 584, 300, 341, 307, 264, 10281, 5063, 2992, 13], "temperature": 0.0, "avg_logprob": -0.18327256043752035, "compression_ratio": 1.6021505376344085, "no_speech_prob": 9.601338388165459e-05}, {"id": 189, "seek": 101628, "start": 1031.0, "end": 1037.24, "text": " So capital sigma mu and lowercase sigma.", "tokens": [407, 4238, 12771, 2992, 293, 3126, 9765, 12771, 13], "temperature": 0.0, "avg_logprob": -0.18327256043752035, "compression_ratio": 1.6021505376344085, "no_speech_prob": 9.601338388165459e-05}, {"id": 190, "seek": 101628, "start": 1037.24, "end": 1044.36, "text": " I just wanted to note that typically the lowercase sigma represents the standard deviation,", "tokens": [286, 445, 1415, 281, 3637, 300, 5850, 264, 3126, 9765, 12771, 8855, 264, 3832, 25163, 11], "temperature": 0.0, "avg_logprob": -0.18327256043752035, "compression_ratio": 1.6021505376344085, "no_speech_prob": 9.601338388165459e-05}, {"id": 191, "seek": 104436, "start": 1044.36, "end": 1049.0, "text": " which is the square root of the variance.", "tokens": [597, 307, 264, 3732, 5593, 295, 264, 21977, 13], "temperature": 0.0, "avg_logprob": -0.14606270918975006, "compression_ratio": 1.802325581395349, "no_speech_prob": 4.066176188644022e-05}, {"id": 192, "seek": 104436, "start": 1049.0, "end": 1057.9599999999998, "text": " So for example, sometimes you may see in papers sigma squared and that's just the variance.", "tokens": [407, 337, 1365, 11, 2171, 291, 815, 536, 294, 10577, 12771, 8889, 293, 300, 311, 445, 264, 21977, 13], "temperature": 0.0, "avg_logprob": -0.14606270918975006, "compression_ratio": 1.802325581395349, "no_speech_prob": 4.066176188644022e-05}, {"id": 193, "seek": 104436, "start": 1057.9599999999998, "end": 1061.6399999999999, "text": " But they will write it sometimes as sigma squared instead.", "tokens": [583, 436, 486, 2464, 309, 2171, 382, 12771, 8889, 2602, 13], "temperature": 0.0, "avg_logprob": -0.14606270918975006, "compression_ratio": 1.802325581395349, "no_speech_prob": 4.066176188644022e-05}, {"id": 194, "seek": 104436, "start": 1061.6399999999999, "end": 1065.32, "text": " So it depends on the notation.", "tokens": [407, 309, 5946, 322, 264, 24657, 13], "temperature": 0.0, "avg_logprob": -0.14606270918975006, "compression_ratio": 1.802325581395349, "no_speech_prob": 4.066176188644022e-05}, {"id": 195, "seek": 104436, "start": 1065.32, "end": 1073.7199999999998, "text": " So sigma is the standard deviation often and sigma squared would then be the variance.", "tokens": [407, 12771, 307, 264, 3832, 25163, 2049, 293, 12771, 8889, 576, 550, 312, 264, 21977, 13], "temperature": 0.0, "avg_logprob": -0.14606270918975006, "compression_ratio": 1.802325581395349, "no_speech_prob": 4.066176188644022e-05}, {"id": 196, "seek": 107372, "start": 1073.72, "end": 1078.2, "text": " Cool. Yeah, we can also show with our example what this would look like.", "tokens": [8561, 13, 865, 11, 321, 393, 611, 855, 365, 527, 1365, 437, 341, 576, 574, 411, 13], "temperature": 0.0, "avg_logprob": -0.17537073893089816, "compression_ratio": 1.4069767441860466, "no_speech_prob": 0.00029516860377043486}, {"id": 197, "seek": 107372, "start": 1078.2, "end": 1086.76, "text": " So we start out with an MNIST digit.", "tokens": [407, 321, 722, 484, 365, 364, 376, 45, 19756, 14293, 13], "temperature": 0.0, "avg_logprob": -0.17537073893089816, "compression_ratio": 1.4069767441860466, "no_speech_prob": 0.00029516860377043486}, {"id": 198, "seek": 107372, "start": 1086.76, "end": 1090.1200000000001, "text": " Put it through this magic API.", "tokens": [4935, 309, 807, 341, 5585, 9362, 13], "temperature": 0.0, "avg_logprob": -0.17537073893089816, "compression_ratio": 1.4069767441860466, "no_speech_prob": 0.00029516860377043486}, {"id": 199, "seek": 107372, "start": 1090.1200000000001, "end": 1093.8, "text": " And what would we get out?", "tokens": [400, 437, 576, 321, 483, 484, 30], "temperature": 0.0, "avg_logprob": -0.17537073893089816, "compression_ratio": 1.4069767441860466, "no_speech_prob": 0.00029516860377043486}, {"id": 200, "seek": 107372, "start": 1093.8, "end": 1101.56, "text": " Okay, so something we didn't describe is, you know, what does this I mean?", "tokens": [1033, 11, 370, 746, 321, 994, 380, 6786, 307, 11, 291, 458, 11, 437, 775, 341, 286, 914, 30], "temperature": 0.0, "avg_logprob": -0.17537073893089816, "compression_ratio": 1.4069767441860466, "no_speech_prob": 0.00029516860377043486}, {"id": 201, "seek": 110156, "start": 1101.56, "end": 1103.96, "text": " Did you want me to talk about that, Wasim?", "tokens": [2589, 291, 528, 385, 281, 751, 466, 300, 11, 3027, 332, 30], "temperature": 0.0, "avg_logprob": -0.21136309305826823, "compression_ratio": 1.6072874493927125, "no_speech_prob": 0.000415858521591872}, {"id": 202, "seek": 110156, "start": 1103.96, "end": 1105.24, "text": " Ah, yes, please.", "tokens": [2438, 11, 2086, 11, 1767, 13], "temperature": 0.0, "avg_logprob": -0.21136309305826823, "compression_ratio": 1.6072874493927125, "no_speech_prob": 0.000415858521591872}, {"id": 203, "seek": 110156, "start": 1105.24, "end": 1105.96, "text": " Okay, sure.", "tokens": [1033, 11, 988, 13], "temperature": 0.0, "avg_logprob": -0.21136309305826823, "compression_ratio": 1.6072874493927125, "no_speech_prob": 0.000415858521591872}, {"id": 204, "seek": 110156, "start": 1105.96, "end": 1109.3999999999999, "text": " So because I think this is something which actually can I borrow your pen?", "tokens": [407, 570, 286, 519, 341, 307, 746, 597, 767, 393, 286, 11172, 428, 3435, 30], "temperature": 0.0, "avg_logprob": -0.21136309305826823, "compression_ratio": 1.6072874493927125, "no_speech_prob": 0.000415858521591872}, {"id": 205, "seek": 110156, "start": 1109.3999999999999, "end": 1115.6399999999999, "text": " It actually came up in the lesson we were doing kind of in an interesting way.", "tokens": [467, 767, 1361, 493, 294, 264, 6898, 321, 645, 884, 733, 295, 294, 364, 1880, 636, 13], "temperature": 0.0, "avg_logprob": -0.21136309305826823, "compression_ratio": 1.6072874493927125, "no_speech_prob": 0.000415858521591872}, {"id": 206, "seek": 110156, "start": 1115.6399999999999, "end": 1118.44, "text": " So in that lesson.", "tokens": [407, 294, 300, 6898, 13], "temperature": 0.0, "avg_logprob": -0.21136309305826823, "compression_ratio": 1.6072874493927125, "no_speech_prob": 0.000415858521591872}, {"id": 207, "seek": 110156, "start": 1118.44, "end": 1119.48, "text": " Do you want to get in the video?", "tokens": [1144, 291, 528, 281, 483, 294, 264, 960, 30], "temperature": 0.0, "avg_logprob": -0.21136309305826823, "compression_ratio": 1.6072874493927125, "no_speech_prob": 0.000415858521591872}, {"id": 208, "seek": 110156, "start": 1121.0, "end": 1122.6, "text": " No, they know what I look like.", "tokens": [883, 11, 436, 458, 437, 286, 574, 411, 13], "temperature": 0.0, "avg_logprob": -0.21136309305826823, "compression_ratio": 1.6072874493927125, "no_speech_prob": 0.000415858521591872}, {"id": 209, "seek": 110156, "start": 1122.6, "end": 1123.48, "text": " Oh, well, okay.", "tokens": [876, 11, 731, 11, 1392, 13], "temperature": 0.0, "avg_logprob": -0.21136309305826823, "compression_ratio": 1.6072874493927125, "no_speech_prob": 0.000415858521591872}, {"id": 210, "seek": 110156, "start": 1123.48, "end": 1124.44, "text": " I'm in the video now.", "tokens": [286, 478, 294, 264, 960, 586, 13], "temperature": 0.0, "avg_logprob": -0.21136309305826823, "compression_ratio": 1.6072874493927125, "no_speech_prob": 0.000415858521591872}, {"id": 211, "seek": 110156, "start": 1125.8799999999999, "end": 1126.76, "text": " Yeah, in the video.", "tokens": [865, 11, 294, 264, 960, 13], "temperature": 0.0, "avg_logprob": -0.21136309305826823, "compression_ratio": 1.6072874493927125, "no_speech_prob": 0.000415858521591872}, {"id": 212, "seek": 110156, "start": 1127.72, "end": 1128.44, "text": " Hi, Tanishk.", "tokens": [2421, 11, 314, 7524, 74, 13], "temperature": 0.0, "avg_logprob": -0.21136309305826823, "compression_ratio": 1.6072874493927125, "no_speech_prob": 0.000415858521591872}, {"id": 213, "seek": 110156, "start": 1128.44, "end": 1129.1599999999999, "text": " Nice to see you.", "tokens": [5490, 281, 536, 291, 13], "temperature": 0.0, "avg_logprob": -0.21136309305826823, "compression_ratio": 1.6072874493927125, "no_speech_prob": 0.000415858521591872}, {"id": 214, "seek": 112916, "start": 1129.16, "end": 1132.6000000000001, "text": " Yeah, so in the lesson, like, we did this thing for clip.", "tokens": [865, 11, 370, 294, 264, 6898, 11, 411, 11, 321, 630, 341, 551, 337, 7353, 13], "temperature": 0.0, "avg_logprob": -0.28633776833029356, "compression_ratio": 1.7380073800738007, "no_speech_prob": 5.728985706809908e-05}, {"id": 215, "seek": 112916, "start": 1132.6000000000001, "end": 1140.76, "text": " I don't know if you remember, Wasim, where we had the, you know, the various pictures down here.", "tokens": [286, 500, 380, 458, 498, 291, 1604, 11, 3027, 332, 11, 689, 321, 632, 264, 11, 291, 458, 11, 264, 3683, 5242, 760, 510, 13], "temperature": 0.0, "avg_logprob": -0.28633776833029356, "compression_ratio": 1.7380073800738007, "no_speech_prob": 5.728985706809908e-05}, {"id": 216, "seek": 112916, "start": 1141.5600000000002, "end": 1142.28, "text": " I'm so embarrassed.", "tokens": [286, 478, 370, 16843, 13], "temperature": 0.0, "avg_logprob": -0.28633776833029356, "compression_ratio": 1.7380073800738007, "no_speech_prob": 5.728985706809908e-05}, {"id": 217, "seek": 112916, "start": 1142.28, "end": 1143.88, "text": " You're better at the graphics tablet than I am.", "tokens": [509, 434, 1101, 412, 264, 11837, 14136, 813, 286, 669, 13], "temperature": 0.0, "avg_logprob": -0.28633776833029356, "compression_ratio": 1.7380073800738007, "no_speech_prob": 5.728985706809908e-05}, {"id": 218, "seek": 112916, "start": 1143.88, "end": 1145.3200000000002, "text": " And it's my graphics tablet.", "tokens": [400, 309, 311, 452, 11837, 14136, 13], "temperature": 0.0, "avg_logprob": -0.28633776833029356, "compression_ratio": 1.7380073800738007, "no_speech_prob": 5.728985706809908e-05}, {"id": 219, "seek": 112916, "start": 1145.3200000000002, "end": 1147.48, "text": " And we had the various sentences along here.", "tokens": [400, 321, 632, 264, 3683, 16579, 2051, 510, 13], "temperature": 0.0, "avg_logprob": -0.28633776833029356, "compression_ratio": 1.7380073800738007, "no_speech_prob": 5.728985706809908e-05}, {"id": 220, "seek": 112916, "start": 1148.1200000000001, "end": 1148.68, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.28633776833029356, "compression_ratio": 1.7380073800738007, "no_speech_prob": 5.728985706809908e-05}, {"id": 221, "seek": 112916, "start": 1148.68, "end": 1153.3200000000002, "text": " And we said, oh, you know, it'd be kind of cool to like take the dot product of their embeddings.", "tokens": [400, 321, 848, 11, 1954, 11, 291, 458, 11, 309, 1116, 312, 733, 295, 1627, 281, 411, 747, 264, 5893, 1674, 295, 641, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.28633776833029356, "compression_ratio": 1.7380073800738007, "no_speech_prob": 5.728985706809908e-05}, {"id": 222, "seek": 112916, "start": 1154.44, "end": 1157.96, "text": " Because like if their dot products are high, that means they're high.", "tokens": [1436, 411, 498, 641, 5893, 3383, 366, 1090, 11, 300, 1355, 436, 434, 1090, 13], "temperature": 0.0, "avg_logprob": -0.28633776833029356, "compression_ratio": 1.7380073800738007, "no_speech_prob": 5.728985706809908e-05}, {"id": 223, "seek": 115796, "start": 1157.96, "end": 1160.68, "text": " The high that means they're similar to each other.", "tokens": [440, 1090, 300, 1355, 436, 434, 2531, 281, 1184, 661, 13], "temperature": 0.0, "avg_logprob": -0.18719905131572, "compression_ratio": 1.5056818181818181, "no_speech_prob": 2.621474050101824e-05}, {"id": 224, "seek": 115796, "start": 1161.96, "end": 1170.2, "text": " And, you know, if we subtracted the means from those first, right, then you've got the dot product.", "tokens": [400, 11, 291, 458, 11, 498, 321, 16390, 292, 264, 1355, 490, 729, 700, 11, 558, 11, 550, 291, 600, 658, 264, 5893, 1674, 13], "temperature": 0.0, "avg_logprob": -0.18719905131572, "compression_ratio": 1.5056818181818181, "no_speech_prob": 2.621474050101824e-05}, {"id": 225, "seek": 115796, "start": 1170.2, "end": 1174.3600000000001, "text": " And instead of having images down here, right.", "tokens": [400, 2602, 295, 1419, 5267, 760, 510, 11, 558, 13], "temperature": 0.0, "avg_logprob": -0.18719905131572, "compression_ratio": 1.5056818181818181, "no_speech_prob": 2.621474050101824e-05}, {"id": 226, "seek": 115796, "start": 1175.0, "end": 1178.1200000000001, "text": " What if we had the exact same?", "tokens": [708, 498, 321, 632, 264, 1900, 912, 30], "temperature": 0.0, "avg_logprob": -0.18719905131572, "compression_ratio": 1.5056818181818181, "no_speech_prob": 2.621474050101824e-05}, {"id": 227, "seek": 115796, "start": 1182.1200000000001, "end": 1185.72, "text": " The exact same vectors on each side.", "tokens": [440, 1900, 912, 18875, 322, 1184, 1252, 13], "temperature": 0.0, "avg_logprob": -0.18719905131572, "compression_ratio": 1.5056818181818181, "no_speech_prob": 2.621474050101824e-05}, {"id": 228, "seek": 118572, "start": 1185.72, "end": 1193.24, "text": " Then what you've got down here is basically X minus, you know, the average.", "tokens": [1396, 437, 291, 600, 658, 760, 510, 307, 1936, 1783, 3175, 11, 291, 458, 11, 264, 4274, 13], "temperature": 0.0, "avg_logprob": -0.3184651594895583, "compression_ratio": 1.625, "no_speech_prob": 9.813600627239794e-06}, {"id": 229, "seek": 118572, "start": 1194.44, "end": 1196.28, "text": " If it's to check that first, squared.", "tokens": [759, 309, 311, 281, 1520, 300, 700, 11, 8889, 13], "temperature": 0.0, "avg_logprob": -0.3184651594895583, "compression_ratio": 1.625, "no_speech_prob": 9.813600627239794e-06}, {"id": 230, "seek": 118572, "start": 1197.4, "end": 1198.76, "text": " And that is the variance.", "tokens": [400, 300, 307, 264, 21977, 13], "temperature": 0.0, "avg_logprob": -0.3184651594895583, "compression_ratio": 1.625, "no_speech_prob": 9.813600627239794e-06}, {"id": 231, "seek": 118572, "start": 1199.24, "end": 1199.64, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.3184651594895583, "compression_ratio": 1.625, "no_speech_prob": 9.813600627239794e-06}, {"id": 232, "seek": 118572, "start": 1199.64, "end": 1205.24, "text": " So that's like the variance for each one of these vectors.", "tokens": [407, 300, 311, 411, 264, 21977, 337, 1184, 472, 295, 613, 18875, 13], "temperature": 0.0, "avg_logprob": -0.3184651594895583, "compression_ratio": 1.625, "no_speech_prob": 9.813600627239794e-06}, {"id": 233, "seek": 118572, "start": 1206.2, "end": 1211.32, "text": " But what's interesting, as you pointed out, is that like normally, you know, at high school,", "tokens": [583, 437, 311, 1880, 11, 382, 291, 10932, 484, 11, 307, 300, 411, 5646, 11, 291, 458, 11, 412, 1090, 1395, 11], "temperature": 0.0, "avg_logprob": -0.3184651594895583, "compression_ratio": 1.625, "no_speech_prob": 9.813600627239794e-06}, {"id": 234, "seek": 118572, "start": 1211.32, "end": 1213.64, "text": " when we look at a normal distribution, it looks like this.", "tokens": [562, 321, 574, 412, 257, 2710, 7316, 11, 309, 1542, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.3184651594895583, "compression_ratio": 1.625, "no_speech_prob": 9.813600627239794e-06}, {"id": 235, "seek": 118572, "start": 1213.64, "end": 1214.2, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.3184651594895583, "compression_ratio": 1.625, "no_speech_prob": 9.813600627239794e-06}, {"id": 236, "seek": 121420, "start": 1214.2, "end": 1216.6000000000001, "text": " But you're not just doing one normal distribution.", "tokens": [583, 291, 434, 406, 445, 884, 472, 2710, 7316, 13], "temperature": 0.0, "avg_logprob": -0.25302302042643227, "compression_ratio": 2.204081632653061, "no_speech_prob": 0.00012141649494878948}, {"id": 237, "seek": 121420, "start": 1216.6000000000001, "end": 1222.3600000000001, "text": " You've got a whole bunch of kind of normal distributions, right, for all of your different pixels.", "tokens": [509, 600, 658, 257, 1379, 3840, 295, 733, 295, 2710, 37870, 11, 558, 11, 337, 439, 295, 428, 819, 18668, 13], "temperature": 0.0, "avg_logprob": -0.25302302042643227, "compression_ratio": 2.204081632653061, "no_speech_prob": 0.00012141649494878948}, {"id": 238, "seek": 121420, "start": 1222.3600000000001, "end": 1223.72, "text": " They're the pixels, right, Tanishq?", "tokens": [814, 434, 264, 18668, 11, 558, 11, 314, 7524, 80, 30], "temperature": 0.0, "avg_logprob": -0.25302302042643227, "compression_ratio": 2.204081632653061, "no_speech_prob": 0.00012141649494878948}, {"id": 239, "seek": 121420, "start": 1223.72, "end": 1226.1200000000001, "text": " Normal distribution of every pixel.", "tokens": [21277, 7316, 295, 633, 19261, 13], "temperature": 0.0, "avg_logprob": -0.25302302042643227, "compression_ratio": 2.204081632653061, "no_speech_prob": 0.00012141649494878948}, {"id": 240, "seek": 121420, "start": 1226.1200000000001, "end": 1227.32, "text": " So there's a whole bunch of them.", "tokens": [407, 456, 311, 257, 1379, 3840, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.25302302042643227, "compression_ratio": 2.204081632653061, "no_speech_prob": 0.00012141649494878948}, {"id": 241, "seek": 121420, "start": 1228.1200000000001, "end": 1232.1200000000001, "text": " And so one of them might have a normal distribution that's there, and another one might have a normal", "tokens": [400, 370, 472, 295, 552, 1062, 362, 257, 2710, 7316, 300, 311, 456, 11, 293, 1071, 472, 1062, 362, 257, 2710], "temperature": 0.0, "avg_logprob": -0.25302302042643227, "compression_ratio": 2.204081632653061, "no_speech_prob": 0.00012141649494878948}, {"id": 242, "seek": 121420, "start": 1232.1200000000001, "end": 1235.96, "text": " distribution that's here, and another one might have a normal distribution that's like here.", "tokens": [7316, 300, 311, 510, 11, 293, 1071, 472, 1062, 362, 257, 2710, 7316, 300, 311, 411, 510, 13], "temperature": 0.0, "avg_logprob": -0.25302302042643227, "compression_ratio": 2.204081632653061, "no_speech_prob": 0.00012141649494878948}, {"id": 243, "seek": 121420, "start": 1237.0, "end": 1243.4, "text": " And it's more than that, though, because like it's possible that, you know, two different", "tokens": [400, 309, 311, 544, 813, 300, 11, 1673, 11, 570, 411, 309, 311, 1944, 300, 11, 291, 458, 11, 732, 819], "temperature": 0.0, "avg_logprob": -0.25302302042643227, "compression_ratio": 2.204081632653061, "no_speech_prob": 0.00012141649494878948}, {"id": 244, "seek": 124340, "start": 1243.4, "end": 1249.0, "text": " you know, one pixel tends to be higher when another pixel tends to be higher, or one pixel", "tokens": [291, 458, 11, 472, 19261, 12258, 281, 312, 2946, 562, 1071, 19261, 12258, 281, 312, 2946, 11, 420, 472, 19261], "temperature": 0.0, "avg_logprob": -0.14292922262418067, "compression_ratio": 1.9254385964912282, "no_speech_prob": 4.755424743052572e-05}, {"id": 245, "seek": 124340, "start": 1249.0, "end": 1251.0800000000002, "text": " tends to be higher when another pixel is lower.", "tokens": [12258, 281, 312, 2946, 562, 1071, 19261, 307, 3126, 13], "temperature": 0.0, "avg_logprob": -0.14292922262418067, "compression_ratio": 1.9254385964912282, "no_speech_prob": 4.755424743052572e-05}, {"id": 246, "seek": 124340, "start": 1251.0800000000002, "end": 1258.0400000000002, "text": " So it actually can kind of create this like surface, you know, in n-dimensional space,", "tokens": [407, 309, 767, 393, 733, 295, 1884, 341, 411, 3753, 11, 291, 458, 11, 294, 297, 12, 18759, 1901, 11], "temperature": 0.0, "avg_logprob": -0.14292922262418067, "compression_ratio": 1.9254385964912282, "no_speech_prob": 4.755424743052572e-05}, {"id": 247, "seek": 124340, "start": 1258.0400000000002, "end": 1259.3200000000002, "text": " where n is the number of pixels.", "tokens": [689, 297, 307, 264, 1230, 295, 18668, 13], "temperature": 0.0, "avg_logprob": -0.14292922262418067, "compression_ratio": 1.9254385964912282, "no_speech_prob": 4.755424743052572e-05}, {"id": 248, "seek": 124340, "start": 1260.52, "end": 1265.8000000000002, "text": " So if you now like look at like, okay, well, what happens if we multiply this by this,", "tokens": [407, 498, 291, 586, 411, 574, 412, 411, 11, 1392, 11, 731, 11, 437, 2314, 498, 321, 12972, 341, 538, 341, 11], "temperature": 0.0, "avg_logprob": -0.14292922262418067, "compression_ratio": 1.9254385964912282, "no_speech_prob": 4.755424743052572e-05}, {"id": 249, "seek": 124340, "start": 1265.8000000000002, "end": 1273.0, "text": " just like we did in Clip, right, then if this number is high, then it's saying that when this", "tokens": [445, 411, 321, 630, 294, 2033, 647, 11, 558, 11, 550, 498, 341, 1230, 307, 1090, 11, 550, 309, 311, 1566, 300, 562, 341], "temperature": 0.0, "avg_logprob": -0.14292922262418067, "compression_ratio": 1.9254385964912282, "no_speech_prob": 4.755424743052572e-05}, {"id": 250, "seek": 127300, "start": 1273.0, "end": 1277.08, "text": " variable is high, where this pixel is high, this pixel tends to be high, and vice versa.", "tokens": [7006, 307, 1090, 11, 689, 341, 19261, 307, 1090, 11, 341, 19261, 12258, 281, 312, 1090, 11, 293, 11964, 25650, 13], "temperature": 0.0, "avg_logprob": -0.12719554575080547, "compression_ratio": 1.985645933014354, "no_speech_prob": 7.367027865257114e-05}, {"id": 251, "seek": 127300, "start": 1277.08, "end": 1280.92, "text": " Or if it's low, it's saying when this pixel tends to be high, this one tends to be low.", "tokens": [1610, 498, 309, 311, 2295, 11, 309, 311, 1566, 562, 341, 19261, 12258, 281, 312, 1090, 11, 341, 472, 12258, 281, 312, 2295, 13], "temperature": 0.0, "avg_logprob": -0.12719554575080547, "compression_ratio": 1.985645933014354, "no_speech_prob": 7.367027865257114e-05}, {"id": 252, "seek": 127300, "start": 1281.48, "end": 1292.28, "text": " Or, interesting to us, what happens, oopsie-daisy, sorry about that, what happens if this is zero?", "tokens": [1610, 11, 1880, 281, 505, 11, 437, 2314, 11, 34166, 414, 12, 67, 1527, 88, 11, 2597, 466, 300, 11, 437, 2314, 498, 341, 307, 4018, 30], "temperature": 0.0, "avg_logprob": -0.12719554575080547, "compression_ratio": 1.985645933014354, "no_speech_prob": 7.367027865257114e-05}, {"id": 253, "seek": 127300, "start": 1293.8, "end": 1297.72, "text": " That says that if this is high, then this could be anything.", "tokens": [663, 1619, 300, 498, 341, 307, 1090, 11, 550, 341, 727, 312, 1340, 13], "temperature": 0.0, "avg_logprob": -0.12719554575080547, "compression_ratio": 1.985645933014354, "no_speech_prob": 7.367027865257114e-05}, {"id": 254, "seek": 127300, "start": 1297.72, "end": 1299.0, "text": " Or this is high, this could be anything.", "tokens": [1610, 341, 307, 1090, 11, 341, 727, 312, 1340, 13], "temperature": 0.0, "avg_logprob": -0.12719554575080547, "compression_ratio": 1.985645933014354, "no_speech_prob": 7.367027865257114e-05}, {"id": 255, "seek": 127300, "start": 1299.0, "end": 1300.92, "text": " There's no relationship between them.", "tokens": [821, 311, 572, 2480, 1296, 552, 13], "temperature": 0.0, "avg_logprob": -0.12719554575080547, "compression_ratio": 1.985645933014354, "no_speech_prob": 7.367027865257114e-05}, {"id": 256, "seek": 130092, "start": 1300.92, "end": 1305.72, "text": " So statistically, we would say that these two pixels are independent.", "tokens": [407, 36478, 11, 321, 576, 584, 300, 613, 732, 18668, 366, 6695, 13], "temperature": 0.0, "avg_logprob": -0.05743050339198349, "compression_ratio": 1.7395348837209301, "no_speech_prob": 5.387984128901735e-05}, {"id": 257, "seek": 130092, "start": 1306.6000000000001, "end": 1311.0800000000002, "text": " And so now, that basically means we could do that for all of these.", "tokens": [400, 370, 586, 11, 300, 1936, 1355, 321, 727, 360, 300, 337, 439, 295, 613, 13], "temperature": 0.0, "avg_logprob": -0.05743050339198349, "compression_ratio": 1.7395348837209301, "no_speech_prob": 5.387984128901735e-05}, {"id": 258, "seek": 130092, "start": 1311.0800000000002, "end": 1314.52, "text": " We could say, oh, you know, these are all zeros.", "tokens": [492, 727, 584, 11, 1954, 11, 291, 458, 11, 613, 366, 439, 35193, 13], "temperature": 0.0, "avg_logprob": -0.05743050339198349, "compression_ratio": 1.7395348837209301, "no_speech_prob": 5.387984128901735e-05}, {"id": 259, "seek": 130092, "start": 1316.6000000000001, "end": 1321.48, "text": " And what that says is that, oh, every pixel is independent of every other pixel.", "tokens": [400, 437, 300, 1619, 307, 300, 11, 1954, 11, 633, 19261, 307, 6695, 295, 633, 661, 19261, 13], "temperature": 0.0, "avg_logprob": -0.05743050339198349, "compression_ratio": 1.7395348837209301, "no_speech_prob": 5.387984128901735e-05}, {"id": 260, "seek": 130092, "start": 1321.48, "end": 1325.64, "text": " Now, of course, in real pictures, that's not how real pixels work,", "tokens": [823, 11, 295, 1164, 11, 294, 957, 5242, 11, 300, 311, 406, 577, 957, 18668, 589, 11], "temperature": 0.0, "avg_logprob": -0.05743050339198349, "compression_ratio": 1.7395348837209301, "no_speech_prob": 5.387984128901735e-05}, {"id": 261, "seek": 130092, "start": 1325.64, "end": 1327.3200000000002, "text": " but that's the assumption we're making.", "tokens": [457, 300, 311, 264, 15302, 321, 434, 1455, 13], "temperature": 0.0, "avg_logprob": -0.05743050339198349, "compression_ratio": 1.7395348837209301, "no_speech_prob": 5.387984128901735e-05}, {"id": 262, "seek": 132732, "start": 1327.32, "end": 1335.96, "text": " Because if we start with a very special matrix called I, which is one, one, one, one, zero, zero, zero.", "tokens": [1436, 498, 321, 722, 365, 257, 588, 2121, 8141, 1219, 286, 11, 597, 307, 472, 11, 472, 11, 472, 11, 472, 11, 4018, 11, 4018, 11, 4018, 13], "temperature": 0.0, "avg_logprob": -0.15148032795299182, "compression_ratio": 1.767741935483871, "no_speech_prob": 8.611496014054865e-05}, {"id": 263, "seek": 132732, "start": 1341.72, "end": 1346.4399999999998, "text": " If we take this very special matrix, it's very special because I can multiply it by something,", "tokens": [759, 321, 747, 341, 588, 2121, 8141, 11, 309, 311, 588, 2121, 570, 286, 393, 12972, 309, 538, 746, 11], "temperature": 0.0, "avg_logprob": -0.15148032795299182, "compression_ratio": 1.767741935483871, "no_speech_prob": 8.611496014054865e-05}, {"id": 264, "seek": 132732, "start": 1346.4399999999998, "end": 1347.24, "text": " say beta.", "tokens": [584, 9861, 13], "temperature": 0.0, "avg_logprob": -0.15148032795299182, "compression_ratio": 1.767741935483871, "no_speech_prob": 8.611496014054865e-05}, {"id": 265, "seek": 132732, "start": 1348.4399999999998, "end": 1353.08, "text": " And if I multiply it by a matrix, I get back the original matrix.", "tokens": [400, 498, 286, 12972, 309, 538, 257, 8141, 11, 286, 483, 646, 264, 3380, 8141, 13], "temperature": 0.0, "avg_logprob": -0.15148032795299182, "compression_ratio": 1.767741935483871, "no_speech_prob": 8.611496014054865e-05}, {"id": 266, "seek": 135308, "start": 1353.08, "end": 1359.96, "text": " If I multiply it by a scalar, I'm going to get beta, beta, beta, and lots of zeros.", "tokens": [759, 286, 12972, 309, 538, 257, 39684, 11, 286, 478, 516, 281, 483, 9861, 11, 9861, 11, 9861, 11, 293, 3195, 295, 35193, 13], "temperature": 0.0, "avg_logprob": -0.11785449051275486, "compression_ratio": 1.7073170731707317, "no_speech_prob": 6.498386937892064e-05}, {"id": 267, "seek": 135308, "start": 1361.1599999999999, "end": 1367.8, "text": " And so if I multiply something by this matrix, then I'm just multiplying it by beta.", "tokens": [400, 370, 498, 286, 12972, 746, 538, 341, 8141, 11, 550, 286, 478, 445, 30955, 309, 538, 9861, 13], "temperature": 0.0, "avg_logprob": -0.11785449051275486, "compression_ratio": 1.7073170731707317, "no_speech_prob": 6.498386937892064e-05}, {"id": 268, "seek": 135308, "start": 1368.52, "end": 1373.1599999999999, "text": " But what's interesting about this is that this is what Waseem wrote.", "tokens": [583, 437, 311, 1880, 466, 341, 307, 300, 341, 307, 437, 343, 651, 443, 4114, 13], "temperature": 0.0, "avg_logprob": -0.11785449051275486, "compression_ratio": 1.7073170731707317, "no_speech_prob": 6.498386937892064e-05}, {"id": 269, "seek": 135308, "start": 1374.28, "end": 1376.9199999999998, "text": " Waseem wrote I times beta, I times beta t.", "tokens": [343, 651, 443, 4114, 286, 1413, 9861, 11, 286, 1413, 9861, 256, 13], "temperature": 0.0, "avg_logprob": -0.11785449051275486, "compression_ratio": 1.7073170731707317, "no_speech_prob": 6.498386937892064e-05}, {"id": 270, "seek": 137692, "start": 1376.92, "end": 1385.5600000000002, "text": " So what he's saying is, oh, we've now got a covariance matrix where for each individual pixel,", "tokens": [407, 437, 415, 311, 1566, 307, 11, 1954, 11, 321, 600, 586, 658, 257, 49851, 719, 8141, 689, 337, 1184, 2609, 19261, 11], "temperature": 0.0, "avg_logprob": -0.23502419545100287, "compression_ratio": 1.670995670995671, "no_speech_prob": 2.391529778833501e-05}, {"id": 271, "seek": 137692, "start": 1386.52, "end": 1390.04, "text": " it's like pixel number one, beta one, pixel number two, beta two.", "tokens": [309, 311, 411, 19261, 1230, 472, 11, 9861, 472, 11, 19261, 1230, 732, 11, 9861, 732, 13], "temperature": 0.0, "avg_logprob": -0.23502419545100287, "compression_ratio": 1.670995670995671, "no_speech_prob": 2.391529778833501e-05}, {"id": 272, "seek": 137692, "start": 1390.04, "end": 1392.6000000000001, "text": " This is the variances of each one.", "tokens": [639, 307, 264, 1374, 21518, 295, 1184, 472, 13], "temperature": 0.0, "avg_logprob": -0.23502419545100287, "compression_ratio": 1.670995670995671, "no_speech_prob": 2.391529778833501e-05}, {"id": 273, "seek": 137692, "start": 1393.5600000000002, "end": 1398.52, "text": " And the covariance is, you know, the relationship between the pixels, zero,", "tokens": [400, 264, 49851, 719, 307, 11, 291, 458, 11, 264, 2480, 1296, 264, 18668, 11, 4018, 11], "temperature": 0.0, "avg_logprob": -0.23502419545100287, "compression_ratio": 1.670995670995671, "no_speech_prob": 2.391529778833501e-05}, {"id": 274, "seek": 137692, "start": 1399.0800000000002, "end": 1401.4, "text": " they're expected to be independent.", "tokens": [436, 434, 5176, 281, 312, 6695, 13], "temperature": 0.0, "avg_logprob": -0.23502419545100287, "compression_ratio": 1.670995670995671, "no_speech_prob": 2.391529778833501e-05}, {"id": 275, "seek": 137692, "start": 1401.88, "end": 1406.28, "text": " So that's where we're kind of going from like statistics you do in high school", "tokens": [407, 300, 311, 689, 321, 434, 733, 295, 516, 490, 411, 12523, 291, 360, 294, 1090, 1395], "temperature": 0.0, "avg_logprob": -0.23502419545100287, "compression_ratio": 1.670995670995671, "no_speech_prob": 2.391529778833501e-05}, {"id": 276, "seek": 140628, "start": 1406.28, "end": 1409.16, "text": " to statistics you do at university.", "tokens": [281, 12523, 291, 360, 412, 5454, 13], "temperature": 0.0, "avg_logprob": -0.15066236799413507, "compression_ratio": 1.4264705882352942, "no_speech_prob": 0.0001820046454668045}, {"id": 277, "seek": 140628, "start": 1409.16, "end": 1414.36, "text": " It's like suddenly covariance is now a matrices, not individual numbers.", "tokens": [467, 311, 411, 5800, 49851, 719, 307, 586, 257, 32284, 11, 406, 2609, 3547, 13], "temperature": 0.0, "avg_logprob": -0.15066236799413507, "compression_ratio": 1.4264705882352942, "no_speech_prob": 0.0001820046454668045}, {"id": 278, "seek": 140628, "start": 1414.36, "end": 1416.52, "text": " Does that sound about right to you, Tanishq?", "tokens": [4402, 300, 1626, 466, 558, 281, 291, 11, 314, 7524, 80, 30], "temperature": 0.0, "avg_logprob": -0.15066236799413507, "compression_ratio": 1.4264705882352942, "no_speech_prob": 0.0001820046454668045}, {"id": 279, "seek": 140628, "start": 1416.52, "end": 1418.68, "text": " Yeah, that's a great explanation of it.", "tokens": [865, 11, 300, 311, 257, 869, 10835, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.15066236799413507, "compression_ratio": 1.4264705882352942, "no_speech_prob": 0.0001820046454668045}, {"id": 280, "seek": 140628, "start": 1418.68, "end": 1419.18, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.15066236799413507, "compression_ratio": 1.4264705882352942, "no_speech_prob": 0.0001820046454668045}, {"id": 281, "seek": 140628, "start": 1421.6399999999999, "end": 1422.1399999999999, "text": " Awesome.", "tokens": [10391, 13], "temperature": 0.0, "avg_logprob": -0.15066236799413507, "compression_ratio": 1.4264705882352942, "no_speech_prob": 0.0001820046454668045}, {"id": 282, "seek": 140628, "start": 1423.8, "end": 1424.12, "text": " Cool.", "tokens": [8561, 13], "temperature": 0.0, "avg_logprob": -0.15066236799413507, "compression_ratio": 1.4264705882352942, "no_speech_prob": 0.0001820046454668045}, {"id": 283, "seek": 140628, "start": 1424.12, "end": 1429.0, "text": " So now let's try to describe, you know, what this would do to aim this digit.", "tokens": [407, 586, 718, 311, 853, 281, 6786, 11, 291, 458, 11, 437, 341, 576, 360, 281, 5939, 341, 14293, 13], "temperature": 0.0, "avg_logprob": -0.15066236799413507, "compression_ratio": 1.4264705882352942, "no_speech_prob": 0.0001820046454668045}, {"id": 284, "seek": 142900, "start": 1429.0, "end": 1436.28, "text": " So, you know, we let's put back our mean equation.", "tokens": [407, 11, 291, 458, 11, 321, 718, 311, 829, 646, 527, 914, 5367, 13], "temperature": 0.0, "avg_logprob": -0.18100312468293425, "compression_ratio": 1.6575342465753424, "no_speech_prob": 0.0002303311339346692}, {"id": 285, "seek": 142900, "start": 1440.92, "end": 1446.44, "text": " And our covariance, whoops, our covariance.", "tokens": [400, 527, 49851, 719, 11, 567, 3370, 11, 527, 49851, 719, 13], "temperature": 0.0, "avg_logprob": -0.18100312468293425, "compression_ratio": 1.6575342465753424, "no_speech_prob": 0.0002303311339346692}, {"id": 286, "seek": 142900, "start": 1448.52, "end": 1450.6, "text": " So mean and our covariance.", "tokens": [407, 914, 293, 527, 49851, 719, 13], "temperature": 0.0, "avg_logprob": -0.18100312468293425, "compression_ratio": 1.6575342465753424, "no_speech_prob": 0.0002303311339346692}, {"id": 287, "seek": 142900, "start": 1450.6, "end": 1454.68, "text": " And let's look at how this behaves, you know, at the edges, sort of.", "tokens": [400, 718, 311, 574, 412, 577, 341, 36896, 11, 291, 458, 11, 412, 264, 8819, 11, 1333, 295, 13], "temperature": 0.0, "avg_logprob": -0.18100312468293425, "compression_ratio": 1.6575342465753424, "no_speech_prob": 0.0002303311339346692}, {"id": 288, "seek": 142900, "start": 1454.68, "end": 1458.44, "text": " So it's really hard to, you know, understand this.", "tokens": [407, 309, 311, 534, 1152, 281, 11, 291, 458, 11, 1223, 341, 13], "temperature": 0.0, "avg_logprob": -0.18100312468293425, "compression_ratio": 1.6575342465753424, "no_speech_prob": 0.0002303311339346692}, {"id": 289, "seek": 145844, "start": 1458.44, "end": 1462.3600000000001, "text": " I don't think anybody can kind of just look at this and know what it means.", "tokens": [286, 500, 380, 519, 4472, 393, 733, 295, 445, 574, 412, 341, 293, 458, 437, 309, 1355, 13], "temperature": 0.0, "avg_logprob": -0.14016894112646053, "compression_ratio": 1.6291666666666667, "no_speech_prob": 7.357761933235452e-05}, {"id": 290, "seek": 145844, "start": 1463.56, "end": 1468.8400000000001, "text": " What we typically do is we try to describe it kind of at the edges.", "tokens": [708, 321, 5850, 360, 307, 321, 853, 281, 6786, 309, 733, 295, 412, 264, 8819, 13], "temperature": 0.0, "avg_logprob": -0.14016894112646053, "compression_ratio": 1.6291666666666667, "no_speech_prob": 7.357761933235452e-05}, {"id": 291, "seek": 145844, "start": 1468.8400000000001, "end": 1471.88, "text": " And so we'll start with like what happens if that's zero.", "tokens": [400, 370, 321, 603, 722, 365, 411, 437, 2314, 498, 300, 311, 4018, 13], "temperature": 0.0, "avg_logprob": -0.14016894112646053, "compression_ratio": 1.6291666666666667, "no_speech_prob": 7.357761933235452e-05}, {"id": 292, "seek": 145844, "start": 1472.76, "end": 1476.44, "text": " And we'll work with X zero as well instead of, you know, Xt minus one,", "tokens": [400, 321, 603, 589, 365, 1783, 4018, 382, 731, 2602, 295, 11, 291, 458, 11, 1783, 83, 3175, 472, 11], "temperature": 0.0, "avg_logprob": -0.14016894112646053, "compression_ratio": 1.6291666666666667, "no_speech_prob": 7.357761933235452e-05}, {"id": 293, "seek": 145844, "start": 1477.4, "end": 1479.0, "text": " which would mean like an MNIST digit.", "tokens": [597, 576, 914, 411, 364, 376, 45, 19756, 14293, 13], "temperature": 0.0, "avg_logprob": -0.14016894112646053, "compression_ratio": 1.6291666666666667, "no_speech_prob": 7.357761933235452e-05}, {"id": 294, "seek": 147900, "start": 1479.0, "end": 1489.88, "text": " So if beta is zero and we get our X zero, you know, square root one minus zero, which is one.", "tokens": [407, 498, 9861, 307, 4018, 293, 321, 483, 527, 1783, 4018, 11, 291, 458, 11, 3732, 5593, 472, 3175, 4018, 11, 597, 307, 472, 13], "temperature": 0.0, "avg_logprob": -0.09639589079133756, "compression_ratio": 1.803370786516854, "no_speech_prob": 2.500020491424948e-05}, {"id": 295, "seek": 147900, "start": 1489.88, "end": 1491.64, "text": " And square root of one is one.", "tokens": [400, 3732, 5593, 295, 472, 307, 472, 13], "temperature": 0.0, "avg_logprob": -0.09639589079133756, "compression_ratio": 1.803370786516854, "no_speech_prob": 2.500020491424948e-05}, {"id": 296, "seek": 147900, "start": 1491.64, "end": 1493.16, "text": " So that kind of falls away.", "tokens": [407, 300, 733, 295, 8804, 1314, 13], "temperature": 0.0, "avg_logprob": -0.09639589079133756, "compression_ratio": 1.803370786516854, "no_speech_prob": 2.500020491424948e-05}, {"id": 297, "seek": 147900, "start": 1493.16, "end": 1497.4, "text": " So we just have a mean of our previous image.", "tokens": [407, 321, 445, 362, 257, 914, 295, 527, 3894, 3256, 13], "temperature": 0.0, "avg_logprob": -0.09639589079133756, "compression_ratio": 1.803370786516854, "no_speech_prob": 2.500020491424948e-05}, {"id": 298, "seek": 147900, "start": 1498.28, "end": 1501.0, "text": " And this is just variance of zero.", "tokens": [400, 341, 307, 445, 21977, 295, 4018, 13], "temperature": 0.0, "avg_logprob": -0.09639589079133756, "compression_ratio": 1.803370786516854, "no_speech_prob": 2.500020491424948e-05}, {"id": 299, "seek": 150100, "start": 1501.0, "end": 1507.4, "text": " So we have a normal distribution with a mean of our previous image, a variance of zero,", "tokens": [407, 321, 362, 257, 2710, 7316, 365, 257, 914, 295, 527, 3894, 3256, 11, 257, 21977, 295, 4018, 11], "temperature": 0.0, "avg_logprob": -0.2589438883463542, "compression_ratio": 1.5847953216374269, "no_speech_prob": 2.045930887106806e-05}, {"id": 300, "seek": 150100, "start": 1509.48, "end": 1515.4, "text": " which means we have the same image.", "tokens": [597, 1355, 321, 362, 264, 912, 3256, 13], "temperature": 0.0, "avg_logprob": -0.2589438883463542, "compression_ratio": 1.5847953216374269, "no_speech_prob": 2.045930887106806e-05}, {"id": 301, "seek": 150100, "start": 1521.16, "end": 1523.4, "text": " Yeah, just to clarify, when you have a variance of zero,", "tokens": [865, 11, 445, 281, 17594, 11, 562, 291, 362, 257, 21977, 295, 4018, 11], "temperature": 0.0, "avg_logprob": -0.2589438883463542, "compression_ratio": 1.5847953216374269, "no_speech_prob": 2.045930887106806e-05}, {"id": 302, "seek": 150100, "start": 1523.4, "end": 1525.4, "text": " that means that there's really no noise or anything.", "tokens": [300, 1355, 300, 456, 311, 534, 572, 5658, 420, 1340, 13], "temperature": 0.0, "avg_logprob": -0.2589438883463542, "compression_ratio": 1.5847953216374269, "no_speech_prob": 2.045930887106806e-05}, {"id": 303, "seek": 150100, "start": 1525.4, "end": 1527.8, "text": " It's just at that mean and, you know,", "tokens": [467, 311, 445, 412, 300, 914, 293, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.2589438883463542, "compression_ratio": 1.5847953216374269, "no_speech_prob": 2.045930887106806e-05}, {"id": 304, "seek": 152780, "start": 1527.8, "end": 1531.24, "text": " your distribution is just saying that's the only point that you can get from it.", "tokens": [428, 7316, 307, 445, 1566, 300, 311, 264, 787, 935, 300, 291, 393, 483, 490, 309, 13], "temperature": 0.0, "avg_logprob": -0.2657167886950306, "compression_ratio": 1.6990291262135921, "no_speech_prob": 1.6441432308056392e-05}, {"id": 305, "seek": 152780, "start": 1531.24, "end": 1534.2, "text": " So yeah, that's what it just becomes the same image because,", "tokens": [407, 1338, 11, 300, 311, 437, 309, 445, 3643, 264, 912, 3256, 570, 11], "temperature": 0.0, "avg_logprob": -0.2657167886950306, "compression_ratio": 1.6990291262135921, "no_speech_prob": 1.6441432308056392e-05}, {"id": 306, "seek": 152780, "start": 1535.08, "end": 1538.84, "text": " yeah, there's no noise or variance because the variance is zero.", "tokens": [1338, 11, 456, 311, 572, 5658, 420, 21977, 570, 264, 21977, 307, 4018, 13], "temperature": 0.0, "avg_logprob": -0.2657167886950306, "compression_ratio": 1.6990291262135921, "no_speech_prob": 1.6441432308056392e-05}, {"id": 307, "seek": 152780, "start": 1539.72, "end": 1541.32, "text": " Yeah, exactly.", "tokens": [865, 11, 2293, 13], "temperature": 0.0, "avg_logprob": -0.2657167886950306, "compression_ratio": 1.6990291262135921, "no_speech_prob": 1.6441432308056392e-05}, {"id": 308, "seek": 152780, "start": 1542.04, "end": 1548.76, "text": " And then when our beta is one, we still have this.", "tokens": [400, 550, 562, 527, 9861, 307, 472, 11, 321, 920, 362, 341, 13], "temperature": 0.0, "avg_logprob": -0.2657167886950306, "compression_ratio": 1.6990291262135921, "no_speech_prob": 1.6441432308056392e-05}, {"id": 309, "seek": 152780, "start": 1548.76, "end": 1552.12, "text": " And then we have, you know, square root one minus one.", "tokens": [400, 550, 321, 362, 11, 291, 458, 11, 3732, 5593, 472, 3175, 472, 13], "temperature": 0.0, "avg_logprob": -0.2657167886950306, "compression_ratio": 1.6990291262135921, "no_speech_prob": 1.6441432308056392e-05}, {"id": 310, "seek": 152780, "start": 1553.3999999999999, "end": 1554.9199999999998, "text": " And that becomes zero.", "tokens": [400, 300, 3643, 4018, 13], "temperature": 0.0, "avg_logprob": -0.2657167886950306, "compression_ratio": 1.6990291262135921, "no_speech_prob": 1.6441432308056392e-05}, {"id": 311, "seek": 155492, "start": 1554.92, "end": 1557.96, "text": " So this whole thing becomes zero.", "tokens": [407, 341, 1379, 551, 3643, 4018, 13], "temperature": 0.0, "avg_logprob": -0.33556981946601244, "compression_ratio": 1.4429530201342282, "no_speech_prob": 3.5170534829376265e-05}, {"id": 312, "seek": 155492, "start": 1559.72, "end": 1565.64, "text": " And this thing becomes i times theta t, which is, you know, i.", "tokens": [400, 341, 551, 3643, 741, 1413, 9725, 256, 11, 597, 307, 11, 291, 458, 11, 741, 13], "temperature": 0.0, "avg_logprob": -0.33556981946601244, "compression_ratio": 1.4429530201342282, "no_speech_prob": 3.5170534829376265e-05}, {"id": 313, "seek": 155492, "start": 1566.52, "end": 1572.2, "text": " And if it's just i, then as Jeremy described, it would imply a variance of one.", "tokens": [400, 498, 309, 311, 445, 741, 11, 550, 382, 17809, 7619, 11, 309, 576, 33616, 257, 21977, 295, 472, 13], "temperature": 0.0, "avg_logprob": -0.33556981946601244, "compression_ratio": 1.4429530201342282, "no_speech_prob": 3.5170534829376265e-05}, {"id": 314, "seek": 155492, "start": 1573.72, "end": 1578.28, "text": " And so our image through this function", "tokens": [400, 370, 527, 3256, 807, 341, 2445], "temperature": 0.0, "avg_logprob": -0.33556981946601244, "compression_ratio": 1.4429530201342282, "no_speech_prob": 3.5170534829376265e-05}, {"id": 315, "seek": 157828, "start": 1578.28, "end": 1580.52, "text": " would just be pure noise.", "tokens": [576, 445, 312, 6075, 5658, 13], "temperature": 0.0, "avg_logprob": -0.37187555753267726, "compression_ratio": 1.4863013698630136, "no_speech_prob": 3.268272121204063e-05}, {"id": 316, "seek": 157828, "start": 1580.52, "end": 1585.56, "text": " So, you know, mean of zero, standard deviation of one,", "tokens": [407, 11, 291, 458, 11, 914, 295, 4018, 11, 3832, 25163, 295, 472, 11], "temperature": 0.0, "avg_logprob": -0.37187555753267726, "compression_ratio": 1.4863013698630136, "no_speech_prob": 3.268272121204063e-05}, {"id": 317, "seek": 157828, "start": 1586.84, "end": 1588.92, "text": " and it would just be a bunch of noise.", "tokens": [293, 309, 576, 445, 312, 257, 3840, 295, 5658, 13], "temperature": 0.0, "avg_logprob": -0.37187555753267726, "compression_ratio": 1.4863013698630136, "no_speech_prob": 3.268272121204063e-05}, {"id": 318, "seek": 157828, "start": 1593.16, "end": 1595.32, "text": " And kind of somewhere in between that,", "tokens": [400, 733, 295, 4079, 294, 1296, 300, 11], "temperature": 0.0, "avg_logprob": -0.37187555753267726, "compression_ratio": 1.4863013698630136, "no_speech_prob": 3.268272121204063e-05}, {"id": 319, "seek": 157828, "start": 1596.04, "end": 1600.04, "text": " we have to say over here, you know, what would it produce?", "tokens": [321, 362, 281, 584, 670, 510, 11, 291, 458, 11, 437, 576, 309, 5258, 30], "temperature": 0.0, "avg_logprob": -0.37187555753267726, "compression_ratio": 1.4863013698630136, "no_speech_prob": 3.268272121204063e-05}, {"id": 320, "seek": 160004, "start": 1600.04, "end": 1606.6, "text": " It would be some mixture. So, you know, like maybe a light, the lighter pixels of eight,", "tokens": [467, 576, 312, 512, 9925, 13, 407, 11, 291, 458, 11, 411, 1310, 257, 1442, 11, 264, 11546, 18668, 295, 3180, 11], "temperature": 0.0, "avg_logprob": -0.5507269308600627, "compression_ratio": 1.603896103896104, "no_speech_prob": 0.0001508283312432468}, {"id": 321, "seek": 160004, "start": 1607.1599999999999, "end": 1610.6, "text": " and some noise, maybe a bit darker.", "tokens": [293, 512, 5658, 11, 1310, 257, 857, 12741, 13], "temperature": 0.0, "avg_logprob": -0.5507269308600627, "compression_ratio": 1.603896103896104, "no_speech_prob": 0.0001508283312432468}, {"id": 322, "seek": 160004, "start": 1616.84, "end": 1621.96, "text": " And we can kind of draw this and you would have seen this in the previous lecture.", "tokens": [400, 321, 393, 733, 295, 2642, 341, 293, 291, 576, 362, 1612, 341, 294, 264, 3894, 7991, 13], "temperature": 0.0, "avg_logprob": -0.5507269308600627, "compression_ratio": 1.603896103896104, "no_speech_prob": 0.0001508283312432468}, {"id": 323, "seek": 160004, "start": 1621.96, "end": 1625.1599999999999, "text": " So, you know, if you look at the graph,", "tokens": [407, 11, 291, 458, 11, 498, 291, 574, 412, 264, 4295, 11], "temperature": 0.0, "avg_logprob": -0.5507269308600627, "compression_ratio": 1.603896103896104, "no_speech_prob": 0.0001508283312432468}, {"id": 324, "seek": 162516, "start": 1625.16, "end": 1634.28, "text": " you can draw the sequence of things that become progressively more noisy in very small steps.", "tokens": [291, 393, 2642, 264, 8310, 295, 721, 300, 1813, 46667, 544, 24518, 294, 588, 1359, 4439, 13], "temperature": 0.0, "avg_logprob": -0.6735185991253769, "compression_ratio": 1.474025974025974, "no_speech_prob": 0.0011735649313777685}, {"id": 325, "seek": 162516, "start": 1635.24, "end": 1637.5600000000002, "text": " All the way until it becomes pure noise.", "tokens": [1057, 264, 636, 1826, 309, 3643, 6075, 5658, 13], "temperature": 0.0, "avg_logprob": -0.6735185991253769, "compression_ratio": 1.474025974025974, "no_speech_prob": 0.0011735649313777685}, {"id": 326, "seek": 162516, "start": 1638.28, "end": 1641.96, "text": " This is what we call the forward diffusion process.", "tokens": [639, 307, 437, 321, 818, 264, 2128, 25242, 1399, 13], "temperature": 0.0, "avg_logprob": -0.6735185991253769, "compression_ratio": 1.474025974025974, "no_speech_prob": 0.0011735649313777685}, {"id": 327, "seek": 162516, "start": 1646.0400000000002, "end": 1649.0, "text": " So, you know, we can draw this sequence.", "tokens": [407, 11, 291, 458, 11, 321, 393, 2642, 341, 8310, 13], "temperature": 0.0, "avg_logprob": -0.6735185991253769, "compression_ratio": 1.474025974025974, "no_speech_prob": 0.0011735649313777685}, {"id": 328, "seek": 164900, "start": 1649.0, "end": 1655.56, "text": " So, this is the forward diffusion process.", "tokens": [407, 11, 341, 307, 264, 2128, 25242, 1399, 13], "temperature": 0.0, "avg_logprob": -0.44374758698219474, "compression_ratio": 1.2542372881355932, "no_speech_prob": 0.00015473745588678867}, {"id": 329, "seek": 164900, "start": 1660.6, "end": 1663.8, "text": " And we can now describe some of these things.", "tokens": [400, 321, 393, 586, 6786, 512, 295, 613, 721, 13], "temperature": 0.0, "avg_logprob": -0.44374758698219474, "compression_ratio": 1.2542372881355932, "no_speech_prob": 0.00015473745588678867}, {"id": 330, "seek": 164900, "start": 1663.8, "end": 1672.36, "text": " So, this would be a sample from our data distribution, Qx0.", "tokens": [407, 11, 341, 576, 312, 257, 6889, 490, 527, 1412, 7316, 11, 1249, 87, 15, 13], "temperature": 0.0, "avg_logprob": -0.44374758698219474, "compression_ratio": 1.2542372881355932, "no_speech_prob": 0.00015473745588678867}, {"id": 331, "seek": 167236, "start": 1672.36, "end": 1681.08, "text": " This would be the function for the conditional probability density function that takes,", "tokens": [639, 576, 312, 264, 2445, 337, 264, 27708, 8482, 10305, 2445, 300, 2516, 11], "temperature": 0.0, "avg_logprob": -0.23765979494367326, "compression_ratio": 1.4625850340136055, "no_speech_prob": 6.47738270345144e-05}, {"id": 332, "seek": 167236, "start": 1682.12, "end": 1688.76, "text": " so of x1, given x0, and so on.", "tokens": [370, 295, 2031, 16, 11, 2212, 2031, 15, 11, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.23765979494367326, "compression_ratio": 1.4625850340136055, "no_speech_prob": 6.47738270345144e-05}, {"id": 333, "seek": 167236, "start": 1693.24, "end": 1698.76, "text": " And the way that the terminology that we would use,", "tokens": [400, 264, 636, 300, 264, 27575, 300, 321, 576, 764, 11], "temperature": 0.0, "avg_logprob": -0.23765979494367326, "compression_ratio": 1.4625850340136055, "no_speech_prob": 6.47738270345144e-05}, {"id": 334, "seek": 169876, "start": 1698.76, "end": 1707.56, "text": " or that mathematicians use to describe this, is they would call it a Markov process", "tokens": [420, 300, 32811, 2567, 764, 281, 6786, 341, 11, 307, 436, 576, 818, 309, 257, 3934, 5179, 1399], "temperature": 0.0, "avg_logprob": -0.11597817551855948, "compression_ratio": 1.36986301369863, "no_speech_prob": 0.00027961618616245687}, {"id": 335, "seek": 169876, "start": 1710.76, "end": 1713.8, "text": " with Gaussian transitions.", "tokens": [365, 39148, 23767, 13], "temperature": 0.0, "avg_logprob": -0.11597817551855948, "compression_ratio": 1.36986301369863, "no_speech_prob": 0.00027961618616245687}, {"id": 336, "seek": 169876, "start": 1719.64, "end": 1724.68, "text": " And, you know, this can sound quite scary, but we've just described exactly what this is.", "tokens": [400, 11, 291, 458, 11, 341, 393, 1626, 1596, 6958, 11, 457, 321, 600, 445, 7619, 2293, 437, 341, 307, 13], "temperature": 0.0, "avg_logprob": -0.11597817551855948, "compression_ratio": 1.36986301369863, "no_speech_prob": 0.00027961618616245687}, {"id": 337, "seek": 172468, "start": 1724.68, "end": 1730.44, "text": " So, when we say process, it usually means, you know, something where there's a sequence involved.", "tokens": [407, 11, 562, 321, 584, 1399, 11, 309, 2673, 1355, 11, 291, 458, 11, 746, 689, 456, 311, 257, 8310, 3288, 13], "temperature": 0.0, "avg_logprob": -0.20959497142482447, "compression_ratio": 1.5813953488372092, "no_speech_prob": 7.189335155999288e-05}, {"id": 338, "seek": 172468, "start": 1732.2, "end": 1742.44, "text": " When we say Markov, it means that the thing at time t depends only on the thing at t minus one.", "tokens": [1133, 321, 584, 3934, 5179, 11, 309, 1355, 300, 264, 551, 412, 565, 256, 5946, 787, 322, 264, 551, 412, 256, 3175, 472, 13], "temperature": 0.0, "avg_logprob": -0.20959497142482447, "compression_ratio": 1.5813953488372092, "no_speech_prob": 7.189335155999288e-05}, {"id": 339, "seek": 172468, "start": 1744.68, "end": 1747.24, "text": " The transition is this function.", "tokens": [440, 6034, 307, 341, 2445, 13], "temperature": 0.0, "avg_logprob": -0.20959497142482447, "compression_ratio": 1.5813953488372092, "no_speech_prob": 7.189335155999288e-05}, {"id": 340, "seek": 172468, "start": 1748.44, "end": 1752.28, "text": " How do you actually go from t minus one to t?", "tokens": [1012, 360, 291, 767, 352, 490, 256, 3175, 472, 281, 256, 30], "temperature": 0.0, "avg_logprob": -0.20959497142482447, "compression_ratio": 1.5813953488372092, "no_speech_prob": 7.189335155999288e-05}, {"id": 341, "seek": 175228, "start": 1752.28, "end": 1758.44, "text": " And Gaussian is the fact that that transition is the normal distribution.", "tokens": [400, 39148, 307, 264, 1186, 300, 300, 6034, 307, 264, 2710, 7316, 13], "temperature": 0.0, "avg_logprob": -0.10143711016728328, "compression_ratio": 1.6302083333333333, "no_speech_prob": 2.9022050966887036e-06}, {"id": 342, "seek": 175228, "start": 1760.44, "end": 1761.48, "text": " Does that sound right?", "tokens": [4402, 300, 1626, 558, 30], "temperature": 0.0, "avg_logprob": -0.10143711016728328, "compression_ratio": 1.6302083333333333, "no_speech_prob": 2.9022050966887036e-06}, {"id": 343, "seek": 175228, "start": 1762.6, "end": 1766.12, "text": " Yes. Just to also clarify a couple of things.", "tokens": [1079, 13, 1449, 281, 611, 17594, 257, 1916, 295, 721, 13], "temperature": 0.0, "avg_logprob": -0.10143711016728328, "compression_ratio": 1.6302083333333333, "no_speech_prob": 2.9022050966887036e-06}, {"id": 344, "seek": 175228, "start": 1766.12, "end": 1770.6, "text": " When we say that, you know, we're sampling from the data distribution,", "tokens": [1133, 321, 584, 300, 11, 291, 458, 11, 321, 434, 21179, 490, 264, 1412, 7316, 11], "temperature": 0.0, "avg_logprob": -0.10143711016728328, "compression_ratio": 1.6302083333333333, "no_speech_prob": 2.9022050966887036e-06}, {"id": 345, "seek": 175228, "start": 1770.6, "end": 1779.56, "text": " what that is referring to is trying to find some random, you know, sample or some random data point", "tokens": [437, 300, 307, 13761, 281, 307, 1382, 281, 915, 512, 4974, 11, 291, 458, 11, 6889, 420, 512, 4974, 1412, 935], "temperature": 0.0, "avg_logprob": -0.10143711016728328, "compression_ratio": 1.6302083333333333, "no_speech_prob": 2.9022050966887036e-06}, {"id": 346, "seek": 177956, "start": 1779.56, "end": 1784.84, "text": " that maximizes that likelihood, or that has a high likelihood.", "tokens": [300, 5138, 5660, 300, 22119, 11, 420, 300, 575, 257, 1090, 22119, 13], "temperature": 0.0, "avg_logprob": -0.11781512366400824, "compression_ratio": 1.8354978354978355, "no_speech_prob": 7.2953048402268905e-06}, {"id": 347, "seek": 177956, "start": 1784.84, "end": 1791.0, "text": " So, when we say that, you know, we're looking at that API, that magic API we were talking about,", "tokens": [407, 11, 562, 321, 584, 300, 11, 291, 458, 11, 321, 434, 1237, 412, 300, 9362, 11, 300, 5585, 9362, 321, 645, 1417, 466, 11], "temperature": 0.0, "avg_logprob": -0.11781512366400824, "compression_ratio": 1.8354978354978355, "no_speech_prob": 7.2953048402268905e-06}, {"id": 348, "seek": 177956, "start": 1791.0, "end": 1797.72, "text": " and we're trying to get some, you know, some data points that have a high value from that API.", "tokens": [293, 321, 434, 1382, 281, 483, 512, 11, 291, 458, 11, 512, 1412, 2793, 300, 362, 257, 1090, 2158, 490, 300, 9362, 13], "temperature": 0.0, "avg_logprob": -0.11781512366400824, "compression_ratio": 1.8354978354978355, "no_speech_prob": 7.2953048402268905e-06}, {"id": 349, "seek": 177956, "start": 1797.72, "end": 1803.6399999999999, "text": " And, you know, for some distributions, it's very simple and we know how it works,", "tokens": [400, 11, 291, 458, 11, 337, 512, 37870, 11, 309, 311, 588, 2199, 293, 321, 458, 577, 309, 1985, 11], "temperature": 0.0, "avg_logprob": -0.11781512366400824, "compression_ratio": 1.8354978354978355, "no_speech_prob": 7.2953048402268905e-06}, {"id": 350, "seek": 177956, "start": 1803.6399999999999, "end": 1807.8799999999999, "text": " like a Gaussian distribution, and we know the parameters of that Gaussian distribution,", "tokens": [411, 257, 39148, 7316, 11, 293, 321, 458, 264, 9834, 295, 300, 39148, 7316, 11], "temperature": 0.0, "avg_logprob": -0.11781512366400824, "compression_ratio": 1.8354978354978355, "no_speech_prob": 7.2953048402268905e-06}, {"id": 351, "seek": 180788, "start": 1807.88, "end": 1810.8400000000001, "text": " it's very easy to be able to do that sampling.", "tokens": [309, 311, 588, 1858, 281, 312, 1075, 281, 360, 300, 21179, 13], "temperature": 0.0, "avg_logprob": -0.10132129842584783, "compression_ratio": 1.7974683544303798, "no_speech_prob": 2.0142420908086933e-05}, {"id": 352, "seek": 180788, "start": 1810.8400000000001, "end": 1813.5600000000002, "text": " And then, of course, in other cases, it's not very easy.", "tokens": [400, 550, 11, 295, 1164, 11, 294, 661, 3331, 11, 309, 311, 406, 588, 1858, 13], "temperature": 0.0, "avg_logprob": -0.10132129842584783, "compression_ratio": 1.7974683544303798, "no_speech_prob": 2.0142420908086933e-05}, {"id": 353, "seek": 180788, "start": 1814.3600000000001, "end": 1816.1200000000001, "text": " It's quite difficult to do that sampling.", "tokens": [467, 311, 1596, 2252, 281, 360, 300, 21179, 13], "temperature": 0.0, "avg_logprob": -0.10132129842584783, "compression_ratio": 1.7974683544303798, "no_speech_prob": 2.0142420908086933e-05}, {"id": 354, "seek": 180788, "start": 1816.1200000000001, "end": 1820.2800000000002, "text": " So, then we have to figure out alternative ways of doing that sampling.", "tokens": [407, 11, 550, 321, 362, 281, 2573, 484, 8535, 2098, 295, 884, 300, 21179, 13], "temperature": 0.0, "avg_logprob": -0.10132129842584783, "compression_ratio": 1.7974683544303798, "no_speech_prob": 2.0142420908086933e-05}, {"id": 355, "seek": 180788, "start": 1820.2800000000002, "end": 1823.0800000000002, "text": " But that's why in this case, with the forward distribution,", "tokens": [583, 300, 311, 983, 294, 341, 1389, 11, 365, 264, 2128, 7316, 11], "temperature": 0.0, "avg_logprob": -0.10132129842584783, "compression_ratio": 1.7974683544303798, "no_speech_prob": 2.0142420908086933e-05}, {"id": 356, "seek": 180788, "start": 1823.0800000000002, "end": 1825.96, "text": " we just have these simple Gaussian transitions,", "tokens": [321, 445, 362, 613, 2199, 39148, 23767, 11], "temperature": 0.0, "avg_logprob": -0.10132129842584783, "compression_ratio": 1.7974683544303798, "no_speech_prob": 2.0142420908086933e-05}, {"id": 357, "seek": 180788, "start": 1825.96, "end": 1830.68, "text": " and we already know the parameters of those Gaussian transitions,", "tokens": [293, 321, 1217, 458, 264, 9834, 295, 729, 39148, 23767, 11], "temperature": 0.0, "avg_logprob": -0.10132129842584783, "compression_ratio": 1.7974683544303798, "no_speech_prob": 2.0142420908086933e-05}, {"id": 358, "seek": 180788, "start": 1830.68, "end": 1833.0800000000002, "text": " so we can easily do that sampling.", "tokens": [370, 321, 393, 3612, 360, 300, 21179, 13], "temperature": 0.0, "avg_logprob": -0.10132129842584783, "compression_ratio": 1.7974683544303798, "no_speech_prob": 2.0142420908086933e-05}, {"id": 359, "seek": 183308, "start": 1833.08, "end": 1838.52, "text": " And going back also to that, I think it's worthwhile to also kind of show", "tokens": [400, 516, 646, 611, 281, 300, 11, 286, 519, 309, 311, 28159, 281, 611, 733, 295, 855], "temperature": 0.0, "avg_logprob": -0.1766564581129286, "compression_ratio": 1.6120689655172413, "no_speech_prob": 2.9766402803943492e-05}, {"id": 360, "seek": 183308, "start": 1838.52, "end": 1842.1999999999998, "text": " and think about maybe how this is again done practically.", "tokens": [293, 519, 466, 1310, 577, 341, 307, 797, 1096, 15667, 13], "temperature": 0.0, "avg_logprob": -0.1766564581129286, "compression_ratio": 1.6120689655172413, "no_speech_prob": 2.9766402803943492e-05}, {"id": 361, "seek": 183308, "start": 1842.1999999999998, "end": 1846.76, "text": " Because one of the nice properties of Gaussian distributions as a whole", "tokens": [1436, 472, 295, 264, 1481, 7221, 295, 39148, 37870, 382, 257, 1379], "temperature": 0.0, "avg_logprob": -0.1766564581129286, "compression_ratio": 1.6120689655172413, "no_speech_prob": 2.9766402803943492e-05}, {"id": 362, "seek": 183308, "start": 1846.76, "end": 1855.72, "text": " is that you can, you know, simply take some normal noise with a mean of zero and variance of one.", "tokens": [307, 300, 291, 393, 11, 291, 458, 11, 2935, 747, 512, 2710, 5658, 365, 257, 914, 295, 4018, 293, 21977, 295, 472, 13], "temperature": 0.0, "avg_logprob": -0.1766564581129286, "compression_ratio": 1.6120689655172413, "no_speech_prob": 2.9766402803943492e-05}, {"id": 363, "seek": 183308, "start": 1855.72, "end": 1861.96, "text": " So that's, I think they usually typically call that a unit distribution.", "tokens": [407, 300, 311, 11, 286, 519, 436, 2673, 5850, 818, 300, 257, 4985, 7316, 13], "temperature": 0.0, "avg_logprob": -0.1766564581129286, "compression_ratio": 1.6120689655172413, "no_speech_prob": 2.9766402803943492e-05}, {"id": 364, "seek": 186196, "start": 1861.96, "end": 1865.0, "text": " It's just like, yeah, normal of zero, one.", "tokens": [467, 311, 445, 411, 11, 1338, 11, 2710, 295, 4018, 11, 472, 13], "temperature": 0.0, "avg_logprob": -0.18897663354873656, "compression_ratio": 1.7062146892655368, "no_speech_prob": 2.9306918804650195e-05}, {"id": 365, "seek": 186196, "start": 1865.0, "end": 1870.92, "text": " And then if you want to get to some other point with a mean of whatever value you specify", "tokens": [400, 550, 498, 291, 528, 281, 483, 281, 512, 661, 935, 365, 257, 914, 295, 2035, 2158, 291, 16500], "temperature": 0.0, "avg_logprob": -0.18897663354873656, "compression_ratio": 1.7062146892655368, "no_speech_prob": 2.9306918804650195e-05}, {"id": 366, "seek": 186196, "start": 1870.92, "end": 1873.8, "text": " and a variance of whatever value you specify,", "tokens": [293, 257, 21977, 295, 2035, 2158, 291, 16500, 11], "temperature": 0.0, "avg_logprob": -0.18897663354873656, "compression_ratio": 1.7062146892655368, "no_speech_prob": 2.9306918804650195e-05}, {"id": 367, "seek": 186196, "start": 1873.8, "end": 1880.44, "text": " you can simply take that normal distribution, scale it by the,", "tokens": [291, 393, 2935, 747, 300, 2710, 7316, 11, 4373, 309, 538, 264, 11], "temperature": 0.0, "avg_logprob": -0.18897663354873656, "compression_ratio": 1.7062146892655368, "no_speech_prob": 2.9306918804650195e-05}, {"id": 368, "seek": 186196, "start": 1882.2, "end": 1886.44, "text": " you multiply it by the variance, and then you add your mean.", "tokens": [291, 12972, 309, 538, 264, 21977, 11, 293, 550, 291, 909, 428, 914, 13], "temperature": 0.0, "avg_logprob": -0.18897663354873656, "compression_ratio": 1.7062146892655368, "no_speech_prob": 2.9306918804650195e-05}, {"id": 369, "seek": 188644, "start": 1886.44, "end": 1897.24, "text": " So then there's a simple equation that you can take to get any particular mean and variance.", "tokens": [407, 550, 456, 311, 257, 2199, 5367, 300, 291, 393, 747, 281, 483, 604, 1729, 914, 293, 21977, 13], "temperature": 0.0, "avg_logprob": -0.11228682543780352, "compression_ratio": 1.6063829787234043, "no_speech_prob": 8.138508746924344e-06}, {"id": 370, "seek": 188644, "start": 1897.24, "end": 1904.04, "text": " So that's how you would get the samples for these other distributions", "tokens": [407, 300, 311, 577, 291, 576, 483, 264, 10938, 337, 613, 661, 37870], "temperature": 0.0, "avg_logprob": -0.11228682543780352, "compression_ratio": 1.6063829787234043, "no_speech_prob": 8.138508746924344e-06}, {"id": 371, "seek": 188644, "start": 1904.04, "end": 1907.88, "text": " that we have defined throughout the forward distribution.", "tokens": [300, 321, 362, 7642, 3710, 264, 2128, 7316, 13], "temperature": 0.0, "avg_logprob": -0.11228682543780352, "compression_ratio": 1.6063829787234043, "no_speech_prob": 8.138508746924344e-06}, {"id": 372, "seek": 188644, "start": 1907.88, "end": 1913.96, "text": " So, for example, when you're coding this up, of course, a lot of these softwares,", "tokens": [407, 11, 337, 1365, 11, 562, 291, 434, 17720, 341, 493, 11, 295, 1164, 11, 257, 688, 295, 613, 2787, 4151, 495, 11], "temperature": 0.0, "avg_logprob": -0.11228682543780352, "compression_ratio": 1.6063829787234043, "no_speech_prob": 8.138508746924344e-06}, {"id": 373, "seek": 191396, "start": 1913.96, "end": 1921.16, "text": " they will have a way of getting a sample from this normal distribution of zero, one.", "tokens": [436, 486, 362, 257, 636, 295, 1242, 257, 6889, 490, 341, 2710, 7316, 295, 4018, 11, 472, 13], "temperature": 0.0, "avg_logprob": -0.1275183465745714, "compression_ratio": 1.6296296296296295, "no_speech_prob": 5.2245370170567185e-05}, {"id": 374, "seek": 191396, "start": 1922.04, "end": 1926.92, "text": " And then you just use that equation then to get it at the desired mean and variance.", "tokens": [400, 550, 291, 445, 764, 300, 5367, 550, 281, 483, 309, 412, 264, 14721, 914, 293, 21977, 13], "temperature": 0.0, "avg_logprob": -0.1275183465745714, "compression_ratio": 1.6296296296296295, "no_speech_prob": 5.2245370170567185e-05}, {"id": 375, "seek": 191396, "start": 1926.92, "end": 1933.24, "text": " And so that's how it kind of happens under the hood when you're kind of describe this with code.", "tokens": [400, 370, 300, 311, 577, 309, 733, 295, 2314, 833, 264, 13376, 562, 291, 434, 733, 295, 6786, 341, 365, 3089, 13], "temperature": 0.0, "avg_logprob": -0.1275183465745714, "compression_ratio": 1.6296296296296295, "no_speech_prob": 5.2245370170567185e-05}, {"id": 376, "seek": 191396, "start": 1935.08, "end": 1936.1200000000001, "text": " That's really helpful.", "tokens": [663, 311, 534, 4961, 13], "temperature": 0.0, "avg_logprob": -0.1275183465745714, "compression_ratio": 1.6296296296296295, "no_speech_prob": 5.2245370170567185e-05}, {"id": 377, "seek": 191396, "start": 1937.4, "end": 1941.88, "text": " Yeah, and this idea of we can't really sample from this thing.", "tokens": [865, 11, 293, 341, 1558, 295, 321, 393, 380, 534, 6889, 490, 341, 551, 13], "temperature": 0.0, "avg_logprob": -0.1275183465745714, "compression_ratio": 1.6296296296296295, "no_speech_prob": 5.2245370170567185e-05}, {"id": 378, "seek": 194188, "start": 1941.88, "end": 1947.3200000000002, "text": " That's exactly the problem that generative kind of modeling is trying to solve.", "tokens": [663, 311, 2293, 264, 1154, 300, 1337, 1166, 733, 295, 15983, 307, 1382, 281, 5039, 13], "temperature": 0.0, "avg_logprob": -0.24889446440197172, "compression_ratio": 1.6707818930041152, "no_speech_prob": 2.2782516680308618e-05}, {"id": 379, "seek": 194188, "start": 1947.3200000000002, "end": 1952.1200000000001, "text": " Like, how do you represent this in such a way that you can easily sample from it?", "tokens": [1743, 11, 577, 360, 291, 2906, 341, 294, 1270, 257, 636, 300, 291, 393, 3612, 6889, 490, 309, 30], "temperature": 0.0, "avg_logprob": -0.24889446440197172, "compression_ratio": 1.6707818930041152, "no_speech_prob": 2.2782516680308618e-05}, {"id": 380, "seek": 194188, "start": 1953.0800000000002, "end": 1957.88, "text": " And so it turns out that if you have one of these processes,", "tokens": [400, 370, 309, 4523, 484, 300, 498, 291, 362, 472, 295, 613, 7555, 11], "temperature": 0.0, "avg_logprob": -0.24889446440197172, "compression_ratio": 1.6707818930041152, "no_speech_prob": 2.2782516680308618e-05}, {"id": 381, "seek": 194188, "start": 1958.92, "end": 1962.5200000000002, "text": " you know, where you have many, many steps, so let's say a thousand steps,", "tokens": [291, 458, 11, 689, 291, 362, 867, 11, 867, 4439, 11, 370, 718, 311, 584, 257, 4714, 4439, 11], "temperature": 0.0, "avg_logprob": -0.24889446440197172, "compression_ratio": 1.6707818930041152, "no_speech_prob": 2.2782516680308618e-05}, {"id": 382, "seek": 194188, "start": 1962.5200000000002, "end": 1965.0800000000002, "text": " the thousand of these steps going to the right.", "tokens": [264, 4714, 295, 613, 4439, 516, 281, 264, 558, 13], "temperature": 0.0, "avg_logprob": -0.24889446440197172, "compression_ratio": 1.6707818930041152, "no_speech_prob": 2.2782516680308618e-05}, {"id": 383, "seek": 194188, "start": 1966.5200000000002, "end": 1970.2, "text": " And they're all very small steps that eventually go to noise.", "tokens": [400, 436, 434, 439, 588, 1359, 4439, 300, 4728, 352, 281, 5658, 13], "temperature": 0.0, "avg_logprob": -0.24889446440197172, "compression_ratio": 1.6707818930041152, "no_speech_prob": 2.2782516680308618e-05}, {"id": 384, "seek": 197020, "start": 1970.2, "end": 1980.6000000000001, "text": " Somebody, you know, maybe in the 1950s, I think, discovered that you can represent", "tokens": [13463, 11, 291, 458, 11, 1310, 294, 264, 18141, 82, 11, 286, 519, 11, 6941, 300, 291, 393, 2906], "temperature": 0.0, "avg_logprob": -0.2698006312052409, "compression_ratio": 1.4078212290502794, "no_speech_prob": 0.00020762240455951542}, {"id": 385, "seek": 197020, "start": 1981.4, "end": 1991.96, "text": " the process of going backwards in exactly the same functional form with just different parameters.", "tokens": [264, 1399, 295, 516, 12204, 294, 2293, 264, 912, 11745, 1254, 365, 445, 819, 9834, 13], "temperature": 0.0, "avg_logprob": -0.2698006312052409, "compression_ratio": 1.4078212290502794, "no_speech_prob": 0.00020762240455951542}, {"id": 386, "seek": 197020, "start": 1991.96, "end": 1997.48, "text": " So what that means is if we say P is the thing that goes backwards, so", "tokens": [407, 437, 300, 1355, 307, 498, 321, 584, 430, 307, 264, 551, 300, 1709, 12204, 11, 370], "temperature": 0.0, "avg_logprob": -0.2698006312052409, "compression_ratio": 1.4078212290502794, "no_speech_prob": 0.00020762240455951542}, {"id": 387, "seek": 199748, "start": 1997.48, "end": 2009.08, "text": " you know, the previous one, given the current one, this P has the same functional form.", "tokens": [291, 458, 11, 264, 3894, 472, 11, 2212, 264, 2190, 472, 11, 341, 430, 575, 264, 912, 11745, 1254, 13], "temperature": 0.0, "avg_logprob": -0.303275730298913, "compression_ratio": 1.585987261146497, "no_speech_prob": 2.077458702842705e-05}, {"id": 388, "seek": 199748, "start": 2009.08, "end": 2017.32, "text": " So it's also the transitions are also normal, but the mean is, you know, some unknown.", "tokens": [407, 309, 311, 611, 264, 23767, 366, 611, 2710, 11, 457, 264, 914, 307, 11, 291, 458, 11, 512, 9841, 13], "temperature": 0.0, "avg_logprob": -0.303275730298913, "compression_ratio": 1.585987261146497, "no_speech_prob": 2.077458702842705e-05}, {"id": 389, "seek": 199748, "start": 2017.32, "end": 2020.76, "text": " So we'll use a square and the variance is some unknown.", "tokens": [407, 321, 603, 764, 257, 3732, 293, 264, 21977, 307, 512, 9841, 13], "temperature": 0.0, "avg_logprob": -0.303275730298913, "compression_ratio": 1.585987261146497, "no_speech_prob": 2.077458702842705e-05}, {"id": 390, "seek": 199748, "start": 2020.76, "end": 2021.72, "text": " We use a triangle.", "tokens": [492, 764, 257, 13369, 13], "temperature": 0.0, "avg_logprob": -0.303275730298913, "compression_ratio": 1.585987261146497, "no_speech_prob": 2.077458702842705e-05}, {"id": 391, "seek": 202172, "start": 2021.72, "end": 2025.72, "text": " Is that correct?", "tokens": [1119, 300, 3006, 30], "temperature": 0.0, "avg_logprob": -0.33715080492424243, "compression_ratio": 1.7277227722772277, "no_speech_prob": 5.5597320169908926e-05}, {"id": 392, "seek": 202172, "start": 2025.72, "end": 2026.44, "text": " Yeah, that's correct.", "tokens": [865, 11, 300, 311, 3006, 13], "temperature": 0.0, "avg_logprob": -0.33715080492424243, "compression_ratio": 1.7277227722772277, "no_speech_prob": 5.5597320169908926e-05}, {"id": 393, "seek": 202172, "start": 2026.44, "end": 2031.88, "text": " And just going back to our previous point about P versus Q, here we can see that the", "tokens": [400, 445, 516, 646, 281, 527, 3894, 935, 466, 430, 5717, 1249, 11, 510, 321, 393, 536, 300, 264], "temperature": 0.0, "avg_logprob": -0.33715080492424243, "compression_ratio": 1.7277227722772277, "no_speech_prob": 5.5597320169908926e-05}, {"id": 394, "seek": 202172, "start": 2031.88, "end": 2037.16, "text": " the Q was describing the sort of forward process going, you know, yeah, this sort of steps", "tokens": [264, 1249, 390, 16141, 264, 1333, 295, 2128, 1399, 516, 11, 291, 458, 11, 1338, 11, 341, 1333, 295, 4439], "temperature": 0.0, "avg_logprob": -0.33715080492424243, "compression_ratio": 1.7277227722772277, "no_speech_prob": 5.5597320169908926e-05}, {"id": 395, "seek": 202172, "start": 2037.16, "end": 2037.88, "text": " that we're doing.", "tokens": [300, 321, 434, 884, 13], "temperature": 0.0, "avg_logprob": -0.33715080492424243, "compression_ratio": 1.7277227722772277, "no_speech_prob": 5.5597320169908926e-05}, {"id": 396, "seek": 202172, "start": 2037.88, "end": 2044.2, "text": " And then the P is describing what we're going in the reverse way.", "tokens": [400, 550, 264, 430, 307, 16141, 437, 321, 434, 516, 294, 264, 9943, 636, 13], "temperature": 0.0, "avg_logprob": -0.33715080492424243, "compression_ratio": 1.7277227722772277, "no_speech_prob": 5.5597320169908926e-05}, {"id": 397, "seek": 202172, "start": 2044.2, "end": 2047.64, "text": " So that's why, you know, the P is going backwards.", "tokens": [407, 300, 311, 983, 11, 291, 458, 11, 264, 430, 307, 516, 12204, 13], "temperature": 0.0, "avg_logprob": -0.33715080492424243, "compression_ratio": 1.7277227722772277, "no_speech_prob": 5.5597320169908926e-05}, {"id": 398, "seek": 204764, "start": 2047.64, "end": 2056.92, "text": " So that's why, you know, these papers are using, you know, Q for one process and then P for another.", "tokens": [407, 300, 311, 983, 11, 291, 458, 11, 613, 10577, 366, 1228, 11, 291, 458, 11, 1249, 337, 472, 1399, 293, 550, 430, 337, 1071, 13], "temperature": 0.0, "avg_logprob": -0.15410836537679037, "compression_ratio": 1.7461928934010151, "no_speech_prob": 0.00016329440404661}, {"id": 399, "seek": 204764, "start": 2056.92, "end": 2060.12, "text": " That's what they're kind of indicating, at least in the diffusion model literature.", "tokens": [663, 311, 437, 436, 434, 733, 295, 25604, 11, 412, 1935, 294, 264, 25242, 2316, 10394, 13], "temperature": 0.0, "avg_logprob": -0.15410836537679037, "compression_ratio": 1.7461928934010151, "no_speech_prob": 0.00016329440404661}, {"id": 400, "seek": 204764, "start": 2062.44, "end": 2067.2400000000002, "text": " And P is kind of like X, you know, it's the one we want to figure out.", "tokens": [400, 430, 307, 733, 295, 411, 1783, 11, 291, 458, 11, 309, 311, 264, 472, 321, 528, 281, 2573, 484, 13], "temperature": 0.0, "avg_logprob": -0.15410836537679037, "compression_ratio": 1.7461928934010151, "no_speech_prob": 0.00016329440404661}, {"id": 401, "seek": 204764, "start": 2067.2400000000002, "end": 2070.76, "text": " So like Q is kind of like Y and P is kind of like X.", "tokens": [407, 411, 1249, 307, 733, 295, 411, 398, 293, 430, 307, 733, 295, 411, 1783, 13], "temperature": 0.0, "avg_logprob": -0.15410836537679037, "compression_ratio": 1.7461928934010151, "no_speech_prob": 0.00016329440404661}, {"id": 402, "seek": 204764, "start": 2070.76, "end": 2072.84, "text": " That's how I like to think of that.", "tokens": [663, 311, 577, 286, 411, 281, 519, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.15410836537679037, "compression_ratio": 1.7461928934010151, "no_speech_prob": 0.00016329440404661}, {"id": 403, "seek": 207284, "start": 2072.84, "end": 2080.44, "text": " And so, you know, we have this functional form and the next question is, how can we use this?", "tokens": [400, 370, 11, 291, 458, 11, 321, 362, 341, 11745, 1254, 293, 264, 958, 1168, 307, 11, 577, 393, 321, 764, 341, 30], "temperature": 0.0, "avg_logprob": -0.21224603017171223, "compression_ratio": 1.5738636363636365, "no_speech_prob": 2.0794459487660788e-05}, {"id": 404, "seek": 207284, "start": 2080.44, "end": 2083.56, "text": " Or, you know, we just don't know what these parameters are.", "tokens": [1610, 11, 291, 458, 11, 321, 445, 500, 380, 458, 437, 613, 9834, 366, 13], "temperature": 0.0, "avg_logprob": -0.21224603017171223, "compression_ratio": 1.5738636363636365, "no_speech_prob": 2.0794459487660788e-05}, {"id": 405, "seek": 207284, "start": 2083.56, "end": 2085.7200000000003, "text": " How can we figure out what those are?", "tokens": [1012, 393, 321, 2573, 484, 437, 729, 366, 30], "temperature": 0.0, "avg_logprob": -0.21224603017171223, "compression_ratio": 1.5738636363636365, "no_speech_prob": 2.0794459487660788e-05}, {"id": 406, "seek": 207284, "start": 2086.92, "end": 2095.08, "text": " And this is goes back, you know, to early kind of statistics literature where you can", "tokens": [400, 341, 307, 1709, 646, 11, 291, 458, 11, 281, 2440, 733, 295, 12523, 10394, 689, 291, 393], "temperature": 0.0, "avg_logprob": -0.21224603017171223, "compression_ratio": 1.5738636363636365, "no_speech_prob": 2.0794459487660788e-05}, {"id": 407, "seek": 209508, "start": 2095.08, "end": 2103.24, "text": " fit this model using by maximizing what's called the likelihood function.", "tokens": [3318, 341, 2316, 1228, 538, 5138, 3319, 437, 311, 1219, 264, 22119, 2445, 13], "temperature": 0.0, "avg_logprob": -0.16475974536332927, "compression_ratio": 1.4913294797687862, "no_speech_prob": 8.528636499249842e-06}, {"id": 408, "seek": 209508, "start": 2103.24, "end": 2110.04, "text": " So we can try different parameters until we have one that maximizes the likelihood.", "tokens": [407, 321, 393, 853, 819, 9834, 1826, 321, 362, 472, 300, 5138, 5660, 264, 22119, 13], "temperature": 0.0, "avg_logprob": -0.16475974536332927, "compression_ratio": 1.4913294797687862, "no_speech_prob": 8.528636499249842e-06}, {"id": 409, "seek": 209508, "start": 2112.7599999999998, "end": 2116.52, "text": " It turns out that we can't quite do this exactly.", "tokens": [467, 4523, 484, 300, 321, 393, 380, 1596, 360, 341, 2293, 13], "temperature": 0.0, "avg_logprob": -0.16475974536332927, "compression_ratio": 1.4913294797687862, "no_speech_prob": 8.528636499249842e-06}, {"id": 410, "seek": 209508, "start": 2119.72, "end": 2122.12, "text": " Because you would need to calculate some integral.", "tokens": [1436, 291, 576, 643, 281, 8873, 512, 11573, 13], "temperature": 0.0, "avg_logprob": -0.16475974536332927, "compression_ratio": 1.4913294797687862, "no_speech_prob": 8.528636499249842e-06}, {"id": 411, "seek": 212212, "start": 2122.12, "end": 2126.68, "text": " And that integral is over very high dimensional values, continuous values.", "tokens": [400, 300, 11573, 307, 670, 588, 1090, 18795, 4190, 11, 10957, 4190, 13], "temperature": 0.0, "avg_logprob": -0.22023194577513622, "compression_ratio": 1.944, "no_speech_prob": 7.130459562176839e-05}, {"id": 412, "seek": 212212, "start": 2126.68, "end": 2128.2799999999997, "text": " So you can't actually calculate this.", "tokens": [407, 291, 393, 380, 767, 8873, 341, 13], "temperature": 0.0, "avg_logprob": -0.22023194577513622, "compression_ratio": 1.944, "no_speech_prob": 7.130459562176839e-05}, {"id": 413, "seek": 212212, "start": 2131.08, "end": 2136.7599999999998, "text": " I think you can think of it because, you know, we're having these thousands of steps that", "tokens": [286, 519, 291, 393, 519, 295, 309, 570, 11, 291, 458, 11, 321, 434, 1419, 613, 5383, 295, 4439, 300], "temperature": 0.0, "avg_logprob": -0.22023194577513622, "compression_ratio": 1.944, "no_speech_prob": 7.130459562176839e-05}, {"id": 414, "seek": 212212, "start": 2136.7599999999998, "end": 2139.08, "text": " we're trying to go in this reverse process.", "tokens": [321, 434, 1382, 281, 352, 294, 341, 9943, 1399, 13], "temperature": 0.0, "avg_logprob": -0.22023194577513622, "compression_ratio": 1.944, "no_speech_prob": 7.130459562176839e-05}, {"id": 415, "seek": 212212, "start": 2139.08, "end": 2142.8399999999997, "text": " And so, you know, you have these thousands of steps that there are going to be many", "tokens": [400, 370, 11, 291, 458, 11, 291, 362, 613, 5383, 295, 4439, 300, 456, 366, 516, 281, 312, 867], "temperature": 0.0, "avg_logprob": -0.22023194577513622, "compression_ratio": 1.944, "no_speech_prob": 7.130459562176839e-05}, {"id": 416, "seek": 212212, "start": 2142.8399999999997, "end": 2144.7599999999998, "text": " possible values for each step.", "tokens": [1944, 4190, 337, 1184, 1823, 13], "temperature": 0.0, "avg_logprob": -0.22023194577513622, "compression_ratio": 1.944, "no_speech_prob": 7.130459562176839e-05}, {"id": 417, "seek": 212212, "start": 2144.7599999999998, "end": 2149.0, "text": " So it's kind of hard to evaluate it over all these thousands of steps and all the", "tokens": [407, 309, 311, 733, 295, 1152, 281, 13059, 309, 670, 439, 613, 5383, 295, 4439, 293, 439, 264], "temperature": 0.0, "avg_logprob": -0.22023194577513622, "compression_ratio": 1.944, "no_speech_prob": 7.130459562176839e-05}, {"id": 418, "seek": 212212, "start": 2149.0, "end": 2151.08, "text": " possible values for all these differences.", "tokens": [1944, 4190, 337, 439, 613, 7300, 13], "temperature": 0.0, "avg_logprob": -0.22023194577513622, "compression_ratio": 1.944, "no_speech_prob": 7.130459562176839e-05}, {"id": 419, "seek": 215108, "start": 2151.08, "end": 2154.52, "text": " So I think that's kind of where the challenges arise.", "tokens": [407, 286, 519, 300, 311, 733, 295, 689, 264, 4759, 20288, 13], "temperature": 0.0, "avg_logprob": -0.2653959062364366, "compression_ratio": 1.8137254901960784, "no_speech_prob": 0.0001849371037678793}, {"id": 420, "seek": 215108, "start": 2154.52, "end": 2159.0, "text": " And that's what it makes it difficult because you have to find, you have to evaluate it", "tokens": [400, 300, 311, 437, 309, 1669, 309, 2252, 570, 291, 362, 281, 915, 11, 291, 362, 281, 13059, 309], "temperature": 0.0, "avg_logprob": -0.2653959062364366, "compression_ratio": 1.8137254901960784, "no_speech_prob": 0.0001849371037678793}, {"id": 421, "seek": 215108, "start": 2159.0, "end": 2164.2, "text": " over these multiple steps and try to find these functions for all these different steps.", "tokens": [670, 613, 3866, 4439, 293, 853, 281, 915, 613, 6828, 337, 439, 613, 819, 4439, 13], "temperature": 0.0, "avg_logprob": -0.2653959062364366, "compression_ratio": 1.8137254901960784, "no_speech_prob": 0.0001849371037678793}, {"id": 422, "seek": 215108, "start": 2164.2, "end": 2166.52, "text": " So that's kind of where the challenge is.", "tokens": [407, 300, 311, 733, 295, 689, 264, 3430, 307, 13], "temperature": 0.0, "avg_logprob": -0.2653959062364366, "compression_ratio": 1.8137254901960784, "no_speech_prob": 0.0001849371037678793}, {"id": 423, "seek": 215108, "start": 2171.16, "end": 2176.36, "text": " And so you might see people talk not about the likelihood function, but about the log", "tokens": [400, 370, 291, 1062, 536, 561, 751, 406, 466, 264, 22119, 2445, 11, 457, 466, 264, 3565], "temperature": 0.0, "avg_logprob": -0.2653959062364366, "compression_ratio": 1.8137254901960784, "no_speech_prob": 0.0001849371037678793}, {"id": 424, "seek": 215108, "start": 2176.36, "end": 2177.16, "text": " likelihood.", "tokens": [22119, 13], "temperature": 0.0, "avg_logprob": -0.2653959062364366, "compression_ratio": 1.8137254901960784, "no_speech_prob": 0.0001849371037678793}, {"id": 425, "seek": 217716, "start": 2177.16, "end": 2183.3199999999997, "text": " And correct me if I'm wrong here, Tanish, but I think the log here is a bit of a, you", "tokens": [400, 3006, 385, 498, 286, 478, 2085, 510, 11, 314, 7524, 11, 457, 286, 519, 264, 3565, 510, 307, 257, 857, 295, 257, 11, 291], "temperature": 0.0, "avg_logprob": -0.43414243615191916, "compression_ratio": 1.8493150684931507, "no_speech_prob": 0.00018702249508351088}, {"id": 426, "seek": 217716, "start": 2183.3199999999997, "end": 2185.3999999999996, "text": " know, computational trick almost.", "tokens": [458, 11, 28270, 4282, 1920, 13], "temperature": 0.0, "avg_logprob": -0.43414243615191916, "compression_ratio": 1.8493150684931507, "no_speech_prob": 0.00018702249508351088}, {"id": 427, "seek": 217716, "start": 2186.12, "end": 2188.7599999999998, "text": " So I think it has a few properties.", "tokens": [407, 286, 519, 309, 575, 257, 1326, 7221, 13], "temperature": 0.0, "avg_logprob": -0.43414243615191916, "compression_ratio": 1.8493150684931507, "no_speech_prob": 0.00018702249508351088}, {"id": 428, "seek": 217716, "start": 2188.7599999999998, "end": 2194.2799999999997, "text": " The first is that it's always increasing and, you know, people would call this, I think,", "tokens": [440, 700, 307, 300, 309, 311, 1009, 5662, 293, 11, 291, 458, 11, 561, 576, 818, 341, 11, 286, 519, 11], "temperature": 0.0, "avg_logprob": -0.43414243615191916, "compression_ratio": 1.8493150684931507, "no_speech_prob": 0.00018702249508351088}, {"id": 429, "seek": 217716, "start": 2194.2799999999997, "end": 2195.08, "text": " monotonic.", "tokens": [1108, 310, 11630, 13], "temperature": 0.0, "avg_logprob": -0.43414243615191916, "compression_ratio": 1.8493150684931507, "no_speech_prob": 0.00018702249508351088}, {"id": 430, "seek": 217716, "start": 2196.6, "end": 2199.72, "text": " You know, it looks always kind of increasing.", "tokens": [509, 458, 11, 309, 1542, 1009, 733, 295, 5662, 13], "temperature": 0.0, "avg_logprob": -0.43414243615191916, "compression_ratio": 1.8493150684931507, "no_speech_prob": 0.00018702249508351088}, {"id": 431, "seek": 217716, "start": 2200.2799999999997, "end": 2202.7599999999998, "text": " And because it's always increasing, it's always increasing.", "tokens": [400, 570, 309, 311, 1009, 5662, 11, 309, 311, 1009, 5662, 13], "temperature": 0.0, "avg_logprob": -0.43414243615191916, "compression_ratio": 1.8493150684931507, "no_speech_prob": 0.00018702249508351088}, {"id": 432, "seek": 217716, "start": 2202.7599999999998, "end": 2205.3199999999997, "text": " So you can see that it's always increasing.", "tokens": [407, 291, 393, 536, 300, 309, 311, 1009, 5662, 13], "temperature": 0.0, "avg_logprob": -0.43414243615191916, "compression_ratio": 1.8493150684931507, "no_speech_prob": 0.00018702249508351088}, {"id": 433, "seek": 220532, "start": 2205.32, "end": 2211.4, "text": " And because it's always increasing, it's the same, you get the same parameters if you", "tokens": [400, 570, 309, 311, 1009, 5662, 11, 309, 311, 264, 912, 11, 291, 483, 264, 912, 9834, 498, 291], "temperature": 0.0, "avg_logprob": -0.16266397554047254, "compression_ratio": 1.7477477477477477, "no_speech_prob": 0.00010318480053683743}, {"id": 434, "seek": 220532, "start": 2211.4, "end": 2215.56, "text": " optimize the log likelihood versus you optimize the likelihood.", "tokens": [19719, 264, 3565, 22119, 5717, 291, 19719, 264, 22119, 13], "temperature": 0.0, "avg_logprob": -0.16266397554047254, "compression_ratio": 1.7477477477477477, "no_speech_prob": 0.00010318480053683743}, {"id": 435, "seek": 220532, "start": 2216.76, "end": 2220.6800000000003, "text": " It also takes the product to sums.", "tokens": [467, 611, 2516, 264, 1674, 281, 34499, 13], "temperature": 0.0, "avg_logprob": -0.16266397554047254, "compression_ratio": 1.7477477477477477, "no_speech_prob": 0.00010318480053683743}, {"id": 436, "seek": 220532, "start": 2223.0800000000004, "end": 2228.6800000000003, "text": " Because, and that's helpful because we have joint distributions, you know, which turn", "tokens": [1436, 11, 293, 300, 311, 4961, 570, 321, 362, 7225, 37870, 11, 291, 458, 11, 597, 1261], "temperature": 0.0, "avg_logprob": -0.16266397554047254, "compression_ratio": 1.7477477477477477, "no_speech_prob": 0.00010318480053683743}, {"id": 437, "seek": 220532, "start": 2228.6800000000003, "end": 2229.32, "text": " out to be products.", "tokens": [484, 281, 312, 3383, 13], "temperature": 0.0, "avg_logprob": -0.16266397554047254, "compression_ratio": 1.7477477477477477, "no_speech_prob": 0.00010318480053683743}, {"id": 438, "seek": 220532, "start": 2229.32, "end": 2233.32, "text": " So it turns out we have a lot of products there and they become sums, which is easy", "tokens": [407, 309, 4523, 484, 321, 362, 257, 688, 295, 3383, 456, 293, 436, 1813, 34499, 11, 597, 307, 1858], "temperature": 0.0, "avg_logprob": -0.16266397554047254, "compression_ratio": 1.7477477477477477, "no_speech_prob": 0.00010318480053683743}, {"id": 439, "seek": 220532, "start": 2233.32, "end": 2233.96, "text": " to work with.", "tokens": [281, 589, 365, 13], "temperature": 0.0, "avg_logprob": -0.16266397554047254, "compression_ratio": 1.7477477477477477, "no_speech_prob": 0.00010318480053683743}, {"id": 440, "seek": 223396, "start": 2233.96, "end": 2240.84, "text": " And the last thing is that, you know, this normal distribution has exponentials or", "tokens": [400, 264, 1036, 551, 307, 300, 11, 291, 458, 11, 341, 2710, 7316, 575, 21510, 82, 420], "temperature": 0.0, "avg_logprob": -0.31163804190499445, "compression_ratio": 1.4819277108433735, "no_speech_prob": 5.089262776891701e-05}, {"id": 441, "seek": 223396, "start": 2240.84, "end": 2242.68, "text": " exponential functions.", "tokens": [21510, 6828, 13], "temperature": 0.0, "avg_logprob": -0.31163804190499445, "compression_ratio": 1.4819277108433735, "no_speech_prob": 5.089262776891701e-05}, {"id": 442, "seek": 223396, "start": 2244.36, "end": 2248.04, "text": " And those disappear with the log.", "tokens": [400, 729, 11596, 365, 264, 3565, 13], "temperature": 0.0, "avg_logprob": -0.31163804190499445, "compression_ratio": 1.4819277108433735, "no_speech_prob": 5.089262776891701e-05}, {"id": 443, "seek": 223396, "start": 2250.04, "end": 2252.84, "text": " So this is a much friendlier thing to optimize.", "tokens": [407, 341, 307, 257, 709, 1277, 2753, 551, 281, 19719, 13], "temperature": 0.0, "avg_logprob": -0.31163804190499445, "compression_ratio": 1.4819277108433735, "no_speech_prob": 5.089262776891701e-05}, {"id": 444, "seek": 223396, "start": 2254.6, "end": 2255.4, "text": " Yep, that's correct.", "tokens": [7010, 11, 300, 311, 3006, 13], "temperature": 0.0, "avg_logprob": -0.31163804190499445, "compression_ratio": 1.4819277108433735, "no_speech_prob": 5.089262776891701e-05}, {"id": 445, "seek": 223396, "start": 2256.76, "end": 2257.2400000000002, "text": " Cool.", "tokens": [8561, 13], "temperature": 0.0, "avg_logprob": -0.31163804190499445, "compression_ratio": 1.4819277108433735, "no_speech_prob": 5.089262776891701e-05}, {"id": 446, "seek": 223396, "start": 2258.76, "end": 2260.36, "text": " And then there's one more step.", "tokens": [400, 550, 456, 311, 472, 544, 1823, 13], "temperature": 0.0, "avg_logprob": -0.31163804190499445, "compression_ratio": 1.4819277108433735, "no_speech_prob": 5.089262776891701e-05}, {"id": 447, "seek": 226036, "start": 2260.36, "end": 2266.6800000000003, "text": " You know, we still can't optimize the log likelihood of the thing that this eventually", "tokens": [509, 458, 11, 321, 920, 393, 380, 19719, 264, 3565, 22119, 295, 264, 551, 300, 341, 4728], "temperature": 0.0, "avg_logprob": -0.10564502547768985, "compression_ratio": 1.489010989010989, "no_speech_prob": 0.0001183528293040581}, {"id": 448, "seek": 226036, "start": 2266.6800000000003, "end": 2267.4, "text": " describes.", "tokens": [15626, 13], "temperature": 0.0, "avg_logprob": -0.10564502547768985, "compression_ratio": 1.489010989010989, "no_speech_prob": 0.0001183528293040581}, {"id": 449, "seek": 226036, "start": 2269.32, "end": 2274.04, "text": " But again, and this is kind of the beauty of math, is that somebody figured out a long", "tokens": [583, 797, 11, 293, 341, 307, 733, 295, 264, 6643, 295, 5221, 11, 307, 300, 2618, 8932, 484, 257, 938], "temperature": 0.0, "avg_logprob": -0.10564502547768985, "compression_ratio": 1.489010989010989, "no_speech_prob": 0.0001183528293040581}, {"id": 450, "seek": 226036, "start": 2274.04, "end": 2283.56, "text": " time ago that there's a way to optimize some other quantity called the ELBO for short.", "tokens": [565, 2057, 300, 456, 311, 257, 636, 281, 19719, 512, 661, 11275, 1219, 264, 14426, 15893, 337, 2099, 13], "temperature": 0.0, "avg_logprob": -0.10564502547768985, "compression_ratio": 1.489010989010989, "no_speech_prob": 0.0001183528293040581}, {"id": 451, "seek": 228356, "start": 2283.56, "end": 2286.92, "text": " Which stands for evidence lower bound.", "tokens": [3013, 7382, 337, 4467, 3126, 5472, 13], "temperature": 0.0, "avg_logprob": -0.4828018275174228, "compression_ratio": 1.5727272727272728, "no_speech_prob": 0.00019035772129427642}, {"id": 452, "seek": 228356, "start": 2300.04, "end": 2302.92, "text": " And the evidence is just another name for the likelihood.", "tokens": [400, 264, 4467, 307, 445, 1071, 1315, 337, 264, 22119, 13], "temperature": 0.0, "avg_logprob": -0.4828018275174228, "compression_ratio": 1.5727272727272728, "no_speech_prob": 0.00019035772129427642}, {"id": 453, "seek": 228356, "start": 2305.0, "end": 2310.84, "text": " And the lower bound means, sorry, you know, the lower bound of the evidence.", "tokens": [400, 264, 3126, 5472, 1355, 11, 2597, 11, 291, 458, 11, 264, 3126, 5472, 295, 264, 4467, 13], "temperature": 0.0, "avg_logprob": -0.4828018275174228, "compression_ratio": 1.5727272727272728, "no_speech_prob": 0.00019035772129427642}, {"id": 454, "seek": 231084, "start": 2310.84, "end": 2315.88, "text": " And if you optimize that, it's almost as good as optimizing the thing that we really want", "tokens": [400, 498, 291, 19719, 300, 11, 309, 311, 1920, 382, 665, 382, 40425, 264, 551, 300, 321, 534, 528], "temperature": 0.0, "avg_logprob": -0.3172589006095097, "compression_ratio": 1.4726027397260273, "no_speech_prob": 6.959516031201929e-05}, {"id": 455, "seek": 231084, "start": 2315.88, "end": 2316.28, "text": " to.", "tokens": [281, 13], "temperature": 0.0, "avg_logprob": -0.3172589006095097, "compression_ratio": 1.4726027397260273, "no_speech_prob": 6.959516031201929e-05}, {"id": 456, "seek": 231084, "start": 2316.92, "end": 2320.2000000000003, "text": " But this one we can calculate very, very easily.", "tokens": [583, 341, 472, 321, 393, 8873, 588, 11, 588, 3612, 13], "temperature": 0.0, "avg_logprob": -0.3172589006095097, "compression_ratio": 1.4726027397260273, "no_speech_prob": 6.959516031201929e-05}, {"id": 457, "seek": 231084, "start": 2323.7200000000003, "end": 2338.76, "text": " And so you can use this as a loss function to train two neural networks.", "tokens": [400, 370, 291, 393, 764, 341, 382, 257, 4470, 2445, 281, 3847, 732, 18161, 9590, 13], "temperature": 0.0, "avg_logprob": -0.3172589006095097, "compression_ratio": 1.4726027397260273, "no_speech_prob": 6.959516031201929e-05}, {"id": 458, "seek": 233876, "start": 2338.76, "end": 2341.88, "text": " That predict our square from earlier, which was our mean.", "tokens": [663, 6069, 527, 3732, 490, 3071, 11, 597, 390, 527, 914, 13], "temperature": 0.0, "avg_logprob": -0.3584942616207499, "compression_ratio": 1.538888888888889, "no_speech_prob": 0.00019990443252027035}, {"id": 459, "seek": 233876, "start": 2344.0400000000004, "end": 2348.84, "text": " And our triangle, which is our variance of this reverse process.", "tokens": [400, 527, 13369, 11, 597, 307, 527, 21977, 295, 341, 9943, 1399, 13], "temperature": 0.0, "avg_logprob": -0.3584942616207499, "compression_ratio": 1.538888888888889, "no_speech_prob": 0.00019990443252027035}, {"id": 460, "seek": 233876, "start": 2350.6800000000003, "end": 2354.6800000000003, "text": " And once you have that, you go all the way back here.", "tokens": [400, 1564, 291, 362, 300, 11, 291, 352, 439, 264, 636, 646, 510, 13], "temperature": 0.0, "avg_logprob": -0.3584942616207499, "compression_ratio": 1.538888888888889, "no_speech_prob": 0.00019990443252027035}, {"id": 461, "seek": 233876, "start": 2354.6800000000003, "end": 2356.6800000000003, "text": " So then you have these values.", "tokens": [407, 550, 291, 362, 613, 4190, 13], "temperature": 0.0, "avg_logprob": -0.3584942616207499, "compression_ratio": 1.538888888888889, "no_speech_prob": 0.00019990443252027035}, {"id": 462, "seek": 233876, "start": 2357.88, "end": 2363.7200000000003, "text": " You can start with pure noise and keep calling these neural networks.", "tokens": [509, 393, 722, 365, 6075, 5658, 293, 1066, 5141, 613, 18161, 9590, 13], "temperature": 0.0, "avg_logprob": -0.3584942616207499, "compression_ratio": 1.538888888888889, "no_speech_prob": 0.00019990443252027035}, {"id": 463, "seek": 236372, "start": 2363.72, "end": 2369.64, "text": " Sampling from those normal distributions.", "tokens": [4832, 11970, 490, 729, 2710, 37870, 13], "temperature": 0.0, "avg_logprob": -0.5502578735351562, "compression_ratio": 1.5868263473053892, "no_speech_prob": 5.4648477089358494e-05}, {"id": 464, "seek": 236372, "start": 2369.64, "end": 2372.8399999999997, "text": " Kind of applying that iteratively over many steps.", "tokens": [9242, 295, 9275, 300, 17138, 19020, 670, 867, 4439, 13], "temperature": 0.0, "avg_logprob": -0.5502578735351562, "compression_ratio": 1.5868263473053892, "no_speech_prob": 5.4648477089358494e-05}, {"id": 465, "seek": 236372, "start": 2373.56, "end": 2376.12, "text": " And you recover the data distribution.", "tokens": [400, 291, 8114, 264, 1412, 7316, 13], "temperature": 0.0, "avg_logprob": -0.5502578735351562, "compression_ratio": 1.5868263473053892, "no_speech_prob": 5.4648477089358494e-05}, {"id": 466, "seek": 236372, "start": 2379.3999999999996, "end": 2386.68, "text": " One thing that's important to clarify here is that you can recover the whole distribution,", "tokens": [1485, 551, 300, 311, 1021, 281, 17594, 510, 307, 300, 291, 393, 8114, 264, 1379, 7316, 11], "temperature": 0.0, "avg_logprob": -0.5502578735351562, "compression_ratio": 1.5868263473053892, "no_speech_prob": 5.4648477089358494e-05}, {"id": 467, "seek": 236372, "start": 2386.68, "end": 2389.64, "text": " but you can't just do it in a single step.", "tokens": [457, 291, 393, 380, 445, 360, 309, 294, 257, 2167, 1823, 13], "temperature": 0.0, "avg_logprob": -0.5502578735351562, "compression_ratio": 1.5868263473053892, "no_speech_prob": 5.4648477089358494e-05}, {"id": 468, "seek": 238964, "start": 2389.64, "end": 2395.7999999999997, "text": " You can recover the whole distribution, but you can't necessarily take a single image,", "tokens": [509, 393, 8114, 264, 1379, 7316, 11, 457, 291, 393, 380, 4725, 747, 257, 2167, 3256, 11], "temperature": 0.0, "avg_logprob": -0.14805067502535307, "compression_ratio": 1.6763285024154588, "no_speech_prob": 0.00022752818767912686}, {"id": 469, "seek": 238964, "start": 2396.7599999999998, "end": 2399.48, "text": " convert it to pure noise, and then convert it back.", "tokens": [7620, 309, 281, 6075, 5658, 11, 293, 550, 7620, 309, 646, 13], "temperature": 0.0, "avg_logprob": -0.14805067502535307, "compression_ratio": 1.6763285024154588, "no_speech_prob": 0.00022752818767912686}, {"id": 470, "seek": 238964, "start": 2400.92, "end": 2403.64, "text": " So this operates sort of at the distribution level.", "tokens": [407, 341, 22577, 1333, 295, 412, 264, 7316, 1496, 13], "temperature": 0.0, "avg_logprob": -0.14805067502535307, "compression_ratio": 1.6763285024154588, "no_speech_prob": 0.00022752818767912686}, {"id": 471, "seek": 238964, "start": 2403.64, "end": 2409.0, "text": " So you can take this kind of magic API.", "tokens": [407, 291, 393, 747, 341, 733, 295, 5585, 9362, 13], "temperature": 0.0, "avg_logprob": -0.14805067502535307, "compression_ratio": 1.6763285024154588, "no_speech_prob": 0.00022752818767912686}, {"id": 472, "seek": 238964, "start": 2409.0, "end": 2410.92, "text": " You can reconstruct that whole API.", "tokens": [509, 393, 31499, 300, 1379, 9362, 13], "temperature": 0.0, "avg_logprob": -0.14805067502535307, "compression_ratio": 1.6763285024154588, "no_speech_prob": 0.00022752818767912686}, {"id": 473, "seek": 241092, "start": 2410.92, "end": 2423.2400000000002, "text": " And if you can do that, then you can generate images, MNIST digits, or cats, or dogs, or whatever you want to.", "tokens": [400, 498, 291, 393, 360, 300, 11, 550, 291, 393, 8460, 5267, 11, 376, 45, 19756, 27011, 11, 420, 11111, 11, 420, 7197, 11, 420, 2035, 291, 528, 281, 13], "temperature": 0.0, "avg_logprob": -0.22727375938778832, "compression_ratio": 1.4936708860759493, "no_speech_prob": 1.69621216628002e-05}, {"id": 474, "seek": 241092, "start": 2427.2400000000002, "end": 2433.0, "text": " I want to just clarify one thing about this process of the loss function.", "tokens": [286, 528, 281, 445, 17594, 472, 551, 466, 341, 1399, 295, 264, 4470, 2445, 13], "temperature": 0.0, "avg_logprob": -0.22727375938778832, "compression_ratio": 1.4936708860759493, "no_speech_prob": 1.69621216628002e-05}, {"id": 475, "seek": 241092, "start": 2433.0, "end": 2435.7200000000003, "text": " So this sort of evidence lower bound loss function,", "tokens": [407, 341, 1333, 295, 4467, 3126, 5472, 4470, 2445, 11], "temperature": 0.0, "avg_logprob": -0.22727375938778832, "compression_ratio": 1.4936708860759493, "no_speech_prob": 1.69621216628002e-05}, {"id": 476, "seek": 243572, "start": 2435.72, "end": 2440.4399999999996, "text": " the kind of approach that it's taking is that we have this forward process.", "tokens": [264, 733, 295, 3109, 300, 309, 311, 1940, 307, 300, 321, 362, 341, 2128, 1399, 13], "temperature": 0.0, "avg_logprob": -0.18315899049913562, "compression_ratio": 1.6169154228855722, "no_speech_prob": 1.0614329767122399e-05}, {"id": 477, "seek": 243572, "start": 2440.4399999999996, "end": 2448.9199999999996, "text": " We can go from the original images and figure out these sorts of intermediate distributions", "tokens": [492, 393, 352, 490, 264, 3380, 5267, 293, 2573, 484, 613, 7527, 295, 19376, 37870], "temperature": 0.0, "avg_logprob": -0.18315899049913562, "compression_ratio": 1.6169154228855722, "no_speech_prob": 1.0614329767122399e-05}, {"id": 478, "seek": 243572, "start": 2448.9199999999996, "end": 2452.2, "text": " going all the way finally to noise.", "tokens": [516, 439, 264, 636, 2721, 281, 5658, 13], "temperature": 0.0, "avg_logprob": -0.18315899049913562, "compression_ratio": 1.6169154228855722, "no_speech_prob": 1.0614329767122399e-05}, {"id": 479, "seek": 243572, "start": 2452.2, "end": 2456.52, "text": " With this sort of evidence lower bound loss function,", "tokens": [2022, 341, 1333, 295, 4467, 3126, 5472, 4470, 2445, 11], "temperature": 0.0, "avg_logprob": -0.18315899049913562, "compression_ratio": 1.6169154228855722, "no_speech_prob": 1.0614329767122399e-05}, {"id": 480, "seek": 243572, "start": 2456.52, "end": 2462.04, "text": " what we're really kind of doing is trying to match our distribution", "tokens": [437, 321, 434, 534, 733, 295, 884, 307, 1382, 281, 2995, 527, 7316], "temperature": 0.0, "avg_logprob": -0.18315899049913562, "compression_ratio": 1.6169154228855722, "no_speech_prob": 1.0614329767122399e-05}, {"id": 481, "seek": 246204, "start": 2462.04, "end": 2467.32, "text": " that we're trying to optimize to those distributions that we saw in the forward process.", "tokens": [300, 321, 434, 1382, 281, 19719, 281, 729, 37870, 300, 321, 1866, 294, 264, 2128, 1399, 13], "temperature": 0.0, "avg_logprob": -0.16780135557823575, "compression_ratio": 1.7887323943661972, "no_speech_prob": 3.762981577892788e-05}, {"id": 482, "seek": 246204, "start": 2467.32, "end": 2468.6, "text": " So that's what we're trying to do.", "tokens": [407, 300, 311, 437, 321, 434, 1382, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.16780135557823575, "compression_ratio": 1.7887323943661972, "no_speech_prob": 3.762981577892788e-05}, {"id": 483, "seek": 246204, "start": 2468.6, "end": 2473.16, "text": " We're trying to match those distributions.", "tokens": [492, 434, 1382, 281, 2995, 729, 37870, 13], "temperature": 0.0, "avg_logprob": -0.16780135557823575, "compression_ratio": 1.7887323943661972, "no_speech_prob": 3.762981577892788e-05}, {"id": 484, "seek": 246204, "start": 2473.16, "end": 2478.36, "text": " And there's a specific type of function that is able to do that.", "tokens": [400, 456, 311, 257, 2685, 2010, 295, 2445, 300, 307, 1075, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.16780135557823575, "compression_ratio": 1.7887323943661972, "no_speech_prob": 3.762981577892788e-05}, {"id": 485, "seek": 246204, "start": 2478.36, "end": 2480.36, "text": " It's called a KL divergence.", "tokens": [467, 311, 1219, 257, 47991, 47387, 13], "temperature": 0.0, "avg_logprob": -0.16780135557823575, "compression_ratio": 1.7887323943661972, "no_speech_prob": 3.762981577892788e-05}, {"id": 486, "seek": 246204, "start": 2480.36, "end": 2485.16, "text": " That's the sort of function that can compare probability distributions.", "tokens": [663, 311, 264, 1333, 295, 2445, 300, 393, 6794, 8482, 37870, 13], "temperature": 0.0, "avg_logprob": -0.16780135557823575, "compression_ratio": 1.7887323943661972, "no_speech_prob": 3.762981577892788e-05}, {"id": 487, "seek": 246204, "start": 2485.16, "end": 2487.64, "text": " And again, because we're dealing with Gaussians,", "tokens": [400, 797, 11, 570, 321, 434, 6260, 365, 10384, 2023, 2567, 11], "temperature": 0.0, "avg_logprob": -0.16780135557823575, "compression_ratio": 1.7887323943661972, "no_speech_prob": 3.762981577892788e-05}, {"id": 488, "seek": 248764, "start": 2487.64, "end": 2493.3199999999997, "text": " you can calculate that analytically and a lot of the math becomes very simple.", "tokens": [291, 393, 8873, 300, 10783, 984, 293, 257, 688, 295, 264, 5221, 3643, 588, 2199, 13], "temperature": 0.0, "avg_logprob": -0.2060789025348166, "compression_ratio": 1.7142857142857142, "no_speech_prob": 2.0458748622331768e-05}, {"id": 489, "seek": 248764, "start": 2493.3199999999997, "end": 2499.0, "text": " So that's again, with the whole Gaussians, we know them quite well.", "tokens": [407, 300, 311, 797, 11, 365, 264, 1379, 10384, 2023, 2567, 11, 321, 458, 552, 1596, 731, 13], "temperature": 0.0, "avg_logprob": -0.2060789025348166, "compression_ratio": 1.7142857142857142, "no_speech_prob": 2.0458748622331768e-05}, {"id": 490, "seek": 248764, "start": 2499.0, "end": 2500.44, "text": " The math is very simple.", "tokens": [440, 5221, 307, 588, 2199, 13], "temperature": 0.0, "avg_logprob": -0.2060789025348166, "compression_ratio": 1.7142857142857142, "no_speech_prob": 2.0458748622331768e-05}, {"id": 491, "seek": 248764, "start": 2500.44, "end": 2505.56, "text": " So that allows us to do this sort of comparison between these distributions very easily", "tokens": [407, 300, 4045, 505, 281, 360, 341, 1333, 295, 9660, 1296, 613, 37870, 588, 3612], "temperature": 0.0, "avg_logprob": -0.2060789025348166, "compression_ratio": 1.7142857142857142, "no_speech_prob": 2.0458748622331768e-05}, {"id": 492, "seek": 248764, "start": 2505.56, "end": 2507.16, "text": " and optimize that.", "tokens": [293, 19719, 300, 13], "temperature": 0.0, "avg_logprob": -0.2060789025348166, "compression_ratio": 1.7142857142857142, "no_speech_prob": 2.0458748622331768e-05}, {"id": 493, "seek": 248764, "start": 2507.16, "end": 2512.8399999999997, "text": " And so we want to kind of minimize the difference between the distributions we see in the forward process", "tokens": [400, 370, 321, 528, 281, 733, 295, 17522, 264, 2649, 1296, 264, 37870, 321, 536, 294, 264, 2128, 1399], "temperature": 0.0, "avg_logprob": -0.2060789025348166, "compression_ratio": 1.7142857142857142, "no_speech_prob": 2.0458748622331768e-05}, {"id": 494, "seek": 251284, "start": 2512.84, "end": 2518.36, "text": " and the distributions we're finally determined for the reverse process.", "tokens": [293, 264, 37870, 321, 434, 2721, 9540, 337, 264, 9943, 1399, 13], "temperature": 0.0, "avg_logprob": -0.3571519118088942, "compression_ratio": 1.3977272727272727, "no_speech_prob": 8.07929682196118e-05}, {"id": 495, "seek": 251284, "start": 2518.36, "end": 2522.36, "text": " Perfect.", "tokens": [10246, 13], "temperature": 0.0, "avg_logprob": -0.3571519118088942, "compression_ratio": 1.3977272727272727, "no_speech_prob": 8.07929682196118e-05}, {"id": 496, "seek": 251284, "start": 2522.36, "end": 2530.36, "text": " Then there's one more thing, I think, one more kind of major step to get closer to the form", "tokens": [1396, 456, 311, 472, 544, 551, 11, 286, 519, 11, 472, 544, 733, 295, 2563, 1823, 281, 483, 4966, 281, 264, 1254], "temperature": 0.0, "avg_logprob": -0.3571519118088942, "compression_ratio": 1.3977272727272727, "no_speech_prob": 8.07929682196118e-05}, {"id": 497, "seek": 251284, "start": 2530.36, "end": 2532.36, "text": " that you would have seen in Jeremy's lesson.", "tokens": [300, 291, 576, 362, 1612, 294, 17809, 311, 6898, 13], "temperature": 0.0, "avg_logprob": -0.3571519118088942, "compression_ratio": 1.3977272727272727, "no_speech_prob": 8.07929682196118e-05}, {"id": 498, "seek": 251284, "start": 2534.36, "end": 2536.36, "text": " So there was the 2020 paper.", "tokens": [407, 456, 390, 264, 4808, 3035, 13], "temperature": 0.0, "avg_logprob": -0.3571519118088942, "compression_ratio": 1.3977272727272727, "no_speech_prob": 8.07929682196118e-05}, {"id": 499, "seek": 253636, "start": 2536.36, "end": 2542.36, "text": " The initials of that model is DDPM.", "tokens": [440, 5883, 82, 295, 300, 2316, 307, 413, 11373, 44, 13], "temperature": 0.0, "avg_logprob": -0.24959002903529576, "compression_ratio": 1.414012738853503, "no_speech_prob": 0.00016242907440755516}, {"id": 500, "seek": 253636, "start": 2542.36, "end": 2544.36, "text": " Tanish, do you know what this stands for?", "tokens": [17046, 742, 11, 360, 291, 458, 437, 341, 7382, 337, 30], "temperature": 0.0, "avg_logprob": -0.24959002903529576, "compression_ratio": 1.414012738853503, "no_speech_prob": 0.00016242907440755516}, {"id": 501, "seek": 253636, "start": 2544.36, "end": 2550.36, "text": " Yeah, it stands for denoising diffusion probabilistic model.", "tokens": [865, 11, 309, 7382, 337, 1441, 78, 3436, 25242, 31959, 3142, 2316, 13], "temperature": 0.0, "avg_logprob": -0.24959002903529576, "compression_ratio": 1.414012738853503, "no_speech_prob": 0.00016242907440755516}, {"id": 502, "seek": 253636, "start": 2552.36, "end": 2556.36, "text": " OK, cool.", "tokens": [2264, 11, 1627, 13], "temperature": 0.0, "avg_logprob": -0.24959002903529576, "compression_ratio": 1.414012738853503, "no_speech_prob": 0.00016242907440755516}, {"id": 503, "seek": 253636, "start": 2558.36, "end": 2564.36, "text": " And what they did was they said, let's assume that this variance is DDPM.", "tokens": [400, 437, 436, 630, 390, 436, 848, 11, 718, 311, 6552, 300, 341, 21977, 307, 413, 11373, 44, 13], "temperature": 0.0, "avg_logprob": -0.24959002903529576, "compression_ratio": 1.414012738853503, "no_speech_prob": 0.00016242907440755516}, {"id": 504, "seek": 256436, "start": 2564.36, "end": 2572.36, "text": " Let's assume that this variance is just a constant, so we don't learn it.", "tokens": [961, 311, 6552, 300, 341, 21977, 307, 445, 257, 5754, 11, 370, 321, 500, 380, 1466, 309, 13], "temperature": 0.0, "avg_logprob": -0.10653294324874878, "compression_ratio": 1.6358695652173914, "no_speech_prob": 0.00011475431529106572}, {"id": 505, "seek": 256436, "start": 2572.36, "end": 2578.36, "text": " And we assume also that the step size from earlier, the variance of the noise that we", "tokens": [400, 321, 6552, 611, 300, 264, 1823, 2744, 490, 3071, 11, 264, 21977, 295, 264, 5658, 300, 321], "temperature": 0.0, "avg_logprob": -0.10653294324874878, "compression_ratio": 1.6358695652173914, "no_speech_prob": 0.00011475431529106572}, {"id": 506, "seek": 256436, "start": 2578.36, "end": 2581.36, "text": " added each step, is also a constant.", "tokens": [3869, 1184, 1823, 11, 307, 611, 257, 5754, 13], "temperature": 0.0, "avg_logprob": -0.10653294324874878, "compression_ratio": 1.6358695652173914, "no_speech_prob": 0.00011475431529106572}, {"id": 507, "seek": 256436, "start": 2581.36, "end": 2584.36, "text": " We don't learn that.", "tokens": [492, 500, 380, 1466, 300, 13], "temperature": 0.0, "avg_logprob": -0.10653294324874878, "compression_ratio": 1.6358695652173914, "no_speech_prob": 0.00011475431529106572}, {"id": 508, "seek": 256436, "start": 2584.36, "end": 2590.36, "text": " We're just predicting the mean, and these are set to some really convenient values.", "tokens": [492, 434, 445, 32884, 264, 914, 11, 293, 613, 366, 992, 281, 512, 534, 10851, 4190, 13], "temperature": 0.0, "avg_logprob": -0.10653294324874878, "compression_ratio": 1.6358695652173914, "no_speech_prob": 0.00011475431529106572}, {"id": 509, "seek": 259036, "start": 2590.36, "end": 2600.36, "text": " Then the last turns out to be that you predict the noise.", "tokens": [1396, 264, 1036, 4523, 484, 281, 312, 300, 291, 6069, 264, 5658, 13], "temperature": 0.0, "avg_logprob": -0.11957824495103624, "compression_ratio": 1.4051724137931034, "no_speech_prob": 0.0001467175461584702}, {"id": 510, "seek": 259036, "start": 2602.36, "end": 2610.36, "text": " So you can restructure this whole thing as you take in, you need to train a network that", "tokens": [407, 291, 393, 1472, 2885, 341, 1379, 551, 382, 291, 747, 294, 11, 291, 643, 281, 3847, 257, 3209, 300], "temperature": 0.0, "avg_logprob": -0.11957824495103624, "compression_ratio": 1.4051724137931034, "no_speech_prob": 0.0001467175461584702}, {"id": 511, "seek": 259036, "start": 2610.36, "end": 2612.36, "text": " takes in images.", "tokens": [2516, 294, 5267, 13], "temperature": 0.0, "avg_logprob": -0.11957824495103624, "compression_ratio": 1.4051724137931034, "no_speech_prob": 0.0001467175461584702}, {"id": 512, "seek": 261236, "start": 2612.36, "end": 2622.36, "text": " So here's your network, and it tells you what of this image is noise.", "tokens": [407, 510, 311, 428, 3209, 11, 293, 309, 5112, 291, 437, 295, 341, 3256, 307, 5658, 13], "temperature": 0.0, "avg_logprob": -0.11727551546963778, "compression_ratio": 1.4350649350649352, "no_speech_prob": 5.909285755478777e-05}, {"id": 513, "seek": 261236, "start": 2626.36, "end": 2630.36, "text": " Thanks to these simplifying assumptions.", "tokens": [2561, 281, 613, 6883, 5489, 17695, 13], "temperature": 0.0, "avg_logprob": -0.11727551546963778, "compression_ratio": 1.4350649350649352, "no_speech_prob": 5.909285755478777e-05}, {"id": 514, "seek": 261236, "start": 2630.36, "end": 2638.36, "text": " And even though they're assumptions, turns out you can train much more models that produce", "tokens": [400, 754, 1673, 436, 434, 17695, 11, 4523, 484, 291, 393, 3847, 709, 544, 5245, 300, 5258], "temperature": 0.0, "avg_logprob": -0.11727551546963778, "compression_ratio": 1.4350649350649352, "no_speech_prob": 5.909285755478777e-05}, {"id": 515, "seek": 261236, "start": 2638.36, "end": 2640.36, "text": " much better images.", "tokens": [709, 1101, 5267, 13], "temperature": 0.0, "avg_logprob": -0.11727551546963778, "compression_ratio": 1.4350649350649352, "no_speech_prob": 5.909285755478777e-05}, {"id": 516, "seek": 264036, "start": 2640.36, "end": 2648.36, "text": " Now, I think this relates to something from the lesson that Jeremy gave.", "tokens": [823, 11, 286, 519, 341, 16155, 281, 746, 490, 264, 6898, 300, 17809, 2729, 13], "temperature": 0.0, "avg_logprob": -0.12750084698200226, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.00042376367491669953}, {"id": 517, "seek": 264036, "start": 2648.36, "end": 2654.36, "text": " Tanish, do you remember there was something about the gradient or something like that?", "tokens": [314, 7524, 11, 360, 291, 1604, 456, 390, 746, 466, 264, 16235, 420, 746, 411, 300, 30], "temperature": 0.0, "avg_logprob": -0.12750084698200226, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.00042376367491669953}, {"id": 518, "seek": 264036, "start": 2654.36, "end": 2656.36, "text": " Yes, yes.", "tokens": [1079, 11, 2086, 13], "temperature": 0.0, "avg_logprob": -0.12750084698200226, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.00042376367491669953}, {"id": 519, "seek": 264036, "start": 2656.36, "end": 2666.36, "text": " So this idea of adding noise and learning to remove noise, the idea is that kind of", "tokens": [407, 341, 1558, 295, 5127, 5658, 293, 2539, 281, 4159, 5658, 11, 264, 1558, 307, 300, 733, 295], "temperature": 0.0, "avg_logprob": -0.12750084698200226, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.00042376367491669953}, {"id": 520, "seek": 266636, "start": 2666.36, "end": 2674.36, "text": " by, you know, again, you have this sort of this image that you have noise, right?", "tokens": [538, 11, 291, 458, 11, 797, 11, 291, 362, 341, 1333, 295, 341, 3256, 300, 291, 362, 5658, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.266314206804548, "compression_ratio": 1.4, "no_speech_prob": 4.9066879000747576e-05}, {"id": 521, "seek": 266636, "start": 2674.36, "end": 2682.36, "text": " And by, sorry, let me think about the best way to say this.", "tokens": [400, 538, 11, 2597, 11, 718, 385, 519, 466, 264, 1151, 636, 281, 584, 341, 13], "temperature": 0.0, "avg_logprob": -0.266314206804548, "compression_ratio": 1.4, "no_speech_prob": 4.9066879000747576e-05}, {"id": 522, "seek": 266636, "start": 2682.36, "end": 2684.36, "text": " Oh, yeah, sorry.", "tokens": [876, 11, 1338, 11, 2597, 13], "temperature": 0.0, "avg_logprob": -0.266314206804548, "compression_ratio": 1.4, "no_speech_prob": 4.9066879000747576e-05}, {"id": 523, "seek": 266636, "start": 2684.36, "end": 2686.36, "text": " Let me start over.", "tokens": [961, 385, 722, 670, 13], "temperature": 0.0, "avg_logprob": -0.266314206804548, "compression_ratio": 1.4, "no_speech_prob": 4.9066879000747576e-05}, {"id": 524, "seek": 266636, "start": 2686.36, "end": 2688.36, "text": " So I'll just start.", "tokens": [407, 286, 603, 445, 722, 13], "temperature": 0.0, "avg_logprob": -0.266314206804548, "compression_ratio": 1.4, "no_speech_prob": 4.9066879000747576e-05}, {"id": 525, "seek": 266636, "start": 2688.36, "end": 2690.36, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.266314206804548, "compression_ratio": 1.4, "no_speech_prob": 4.9066879000747576e-05}, {"id": 526, "seek": 269036, "start": 2690.36, "end": 2698.36, "text": " So like Jeremy will say in the lesson, what we want to do is we want to figure out the", "tokens": [407, 411, 17809, 486, 584, 294, 264, 6898, 11, 437, 321, 528, 281, 360, 307, 321, 528, 281, 2573, 484, 264], "temperature": 0.0, "avg_logprob": -0.11707479688856337, "compression_ratio": 1.7311320754716981, "no_speech_prob": 4.288554919185117e-06}, {"id": 527, "seek": 269036, "start": 2698.36, "end": 2702.36, "text": " gradient of this likelihood function.", "tokens": [16235, 295, 341, 22119, 2445, 13], "temperature": 0.0, "avg_logprob": -0.11707479688856337, "compression_ratio": 1.7311320754716981, "no_speech_prob": 4.288554919185117e-06}, {"id": 528, "seek": 269036, "start": 2702.36, "end": 2706.36, "text": " So this is just kind of a different way about thinking about this.", "tokens": [407, 341, 307, 445, 733, 295, 257, 819, 636, 466, 1953, 466, 341, 13], "temperature": 0.0, "avg_logprob": -0.11707479688856337, "compression_ratio": 1.7311320754716981, "no_speech_prob": 4.288554919185117e-06}, {"id": 529, "seek": 269036, "start": 2706.36, "end": 2712.36, "text": " If we had some information about this gradient, then we could, for example, you know, use", "tokens": [759, 321, 632, 512, 1589, 466, 341, 16235, 11, 550, 321, 727, 11, 337, 1365, 11, 291, 458, 11, 764], "temperature": 0.0, "avg_logprob": -0.11707479688856337, "compression_ratio": 1.7311320754716981, "no_speech_prob": 4.288554919185117e-06}, {"id": 530, "seek": 269036, "start": 2712.36, "end": 2718.36, "text": " that information to produce, like we talked about, kind of this optimization, kind of", "tokens": [300, 1589, 281, 5258, 11, 411, 321, 2825, 466, 11, 733, 295, 341, 19618, 11, 733, 295], "temperature": 0.0, "avg_logprob": -0.11707479688856337, "compression_ratio": 1.7311320754716981, "no_speech_prob": 4.288554919185117e-06}, {"id": 531, "seek": 271836, "start": 2718.36, "end": 2721.36, "text": " produce images with high likelihood.", "tokens": [5258, 5267, 365, 1090, 22119, 13], "temperature": 0.0, "avg_logprob": -0.09024783839350162, "compression_ratio": 1.9832402234636872, "no_speech_prob": 5.223137850407511e-05}, {"id": 532, "seek": 271836, "start": 2721.36, "end": 2727.36, "text": " So the idea is that we can add noise to the images that we have.", "tokens": [407, 264, 1558, 307, 300, 321, 393, 909, 5658, 281, 264, 5267, 300, 321, 362, 13], "temperature": 0.0, "avg_logprob": -0.09024783839350162, "compression_ratio": 1.9832402234636872, "no_speech_prob": 5.223137850407511e-05}, {"id": 533, "seek": 271836, "start": 2727.36, "end": 2730.36, "text": " So those are samples that we have.", "tokens": [407, 729, 366, 10938, 300, 321, 362, 13], "temperature": 0.0, "avg_logprob": -0.09024783839350162, "compression_ratio": 1.9832402234636872, "no_speech_prob": 5.223137850407511e-05}, {"id": 534, "seek": 271836, "start": 2730.36, "end": 2736.36, "text": " And that kind of takes us away from, you know, the regular images that we have.", "tokens": [400, 300, 733, 295, 2516, 505, 1314, 490, 11, 291, 458, 11, 264, 3890, 5267, 300, 321, 362, 13], "temperature": 0.0, "avg_logprob": -0.09024783839350162, "compression_ratio": 1.9832402234636872, "no_speech_prob": 5.223137850407511e-05}, {"id": 535, "seek": 271836, "start": 2736.36, "end": 2739.36, "text": " And, you know, that kind of decreases the likelihood, right?", "tokens": [400, 11, 291, 458, 11, 300, 733, 295, 24108, 264, 22119, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.09024783839350162, "compression_ratio": 1.9832402234636872, "no_speech_prob": 5.223137850407511e-05}, {"id": 536, "seek": 271836, "start": 2739.36, "end": 2743.36, "text": " So we have those images and we're adding noise that decreases the likelihood.", "tokens": [407, 321, 362, 729, 5267, 293, 321, 434, 5127, 5658, 300, 24108, 264, 22119, 13], "temperature": 0.0, "avg_logprob": -0.09024783839350162, "compression_ratio": 1.9832402234636872, "no_speech_prob": 5.223137850407511e-05}, {"id": 537, "seek": 274336, "start": 2743.36, "end": 2750.36, "text": " And we want to kind of learn how to get back to high likelihood images and kind of use", "tokens": [400, 321, 528, 281, 733, 295, 1466, 577, 281, 483, 646, 281, 1090, 22119, 5267, 293, 733, 295, 764], "temperature": 0.0, "avg_logprob": -0.05057642857233683, "compression_ratio": 1.6842105263157894, "no_speech_prob": 2.9758615710306913e-05}, {"id": 538, "seek": 274336, "start": 2750.36, "end": 2754.36, "text": " that to provide some sort of estimate of our gradient.", "tokens": [300, 281, 2893, 512, 1333, 295, 12539, 295, 527, 16235, 13], "temperature": 0.0, "avg_logprob": -0.05057642857233683, "compression_ratio": 1.6842105263157894, "no_speech_prob": 2.9758615710306913e-05}, {"id": 539, "seek": 274336, "start": 2754.36, "end": 2758.36, "text": " So this sort of denoising process actually allows us to do that.", "tokens": [407, 341, 1333, 295, 1441, 78, 3436, 1399, 767, 4045, 505, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.05057642857233683, "compression_ratio": 1.6842105263157894, "no_speech_prob": 2.9758615710306913e-05}, {"id": 540, "seek": 274336, "start": 2758.36, "end": 2763.36, "text": " So there are actually theorems also, I think, from the 1950s that demonstrate that,", "tokens": [407, 456, 366, 767, 10299, 2592, 611, 11, 286, 519, 11, 490, 264, 18141, 82, 300, 11698, 300, 11], "temperature": 0.0, "avg_logprob": -0.05057642857233683, "compression_ratio": 1.6842105263157894, "no_speech_prob": 2.9758615710306913e-05}, {"id": 541, "seek": 274336, "start": 2763.36, "end": 2769.36, "text": " especially in the case of this sort of Gaussian noise that we're working with, this denoising", "tokens": [2318, 294, 264, 1389, 295, 341, 1333, 295, 39148, 5658, 300, 321, 434, 1364, 365, 11, 341, 1441, 78, 3436], "temperature": 0.0, "avg_logprob": -0.05057642857233683, "compression_ratio": 1.6842105263157894, "no_speech_prob": 2.9758615710306913e-05}, {"id": 542, "seek": 276936, "start": 2769.36, "end": 2775.36, "text": " process is equivalent to learning what is known as the score function.", "tokens": [1399, 307, 10344, 281, 2539, 437, 307, 2570, 382, 264, 6175, 2445, 13], "temperature": 0.0, "avg_logprob": -0.0731016206741333, "compression_ratio": 1.84037558685446, "no_speech_prob": 2.2121306756162085e-05}, {"id": 543, "seek": 276936, "start": 2775.36, "end": 2782.36, "text": " And the score function is the gradient of the log of the likelihood.", "tokens": [400, 264, 6175, 2445, 307, 264, 16235, 295, 264, 3565, 295, 264, 22119, 13], "temperature": 0.0, "avg_logprob": -0.0731016206741333, "compression_ratio": 1.84037558685446, "no_speech_prob": 2.2121306756162085e-05}, {"id": 544, "seek": 276936, "start": 2782.36, "end": 2787.36, "text": " So, again, they have this log here, which, again, makes the math nicer and easier to", "tokens": [407, 11, 797, 11, 436, 362, 341, 3565, 510, 11, 597, 11, 797, 11, 1669, 264, 5221, 22842, 293, 3571, 281], "temperature": 0.0, "avg_logprob": -0.0731016206741333, "compression_ratio": 1.84037558685446, "no_speech_prob": 2.2121306756162085e-05}, {"id": 545, "seek": 276936, "start": 2787.36, "end": 2792.36, "text": " work with. But the general idea is the same, because as we talked about, log is a", "tokens": [589, 365, 13, 583, 264, 2674, 1558, 307, 264, 912, 11, 570, 382, 321, 2825, 466, 11, 3565, 307, 257], "temperature": 0.0, "avg_logprob": -0.0731016206741333, "compression_ratio": 1.84037558685446, "no_speech_prob": 2.2121306756162085e-05}, {"id": 546, "seek": 276936, "start": 2792.36, "end": 2798.36, "text": " monotonic function. So, again, the general ideas are the same, but the score function", "tokens": [1108, 310, 11630, 2445, 13, 407, 11, 797, 11, 264, 2674, 3487, 366, 264, 912, 11, 457, 264, 6175, 2445], "temperature": 0.0, "avg_logprob": -0.0731016206741333, "compression_ratio": 1.84037558685446, "no_speech_prob": 2.2121306756162085e-05}, {"id": 547, "seek": 279836, "start": 2798.36, "end": 2802.36, "text": " specifically refers to the gradient of the log likelihood.", "tokens": [4682, 14942, 281, 264, 16235, 295, 264, 3565, 22119, 13], "temperature": 0.0, "avg_logprob": -0.08727218327897318, "compression_ratio": 1.6912442396313363, "no_speech_prob": 1.749811417539604e-05}, {"id": 548, "seek": 279836, "start": 2802.36, "end": 2810.36, "text": " So this sort of denoising process allows us to learn the score function.", "tokens": [407, 341, 1333, 295, 1441, 78, 3436, 1399, 4045, 505, 281, 1466, 264, 6175, 2445, 13], "temperature": 0.0, "avg_logprob": -0.08727218327897318, "compression_ratio": 1.6912442396313363, "no_speech_prob": 1.749811417539604e-05}, {"id": 549, "seek": 279836, "start": 2810.36, "end": 2816.36, "text": " So that's what we're doing, this noise predicting that we had this whole probabilistic", "tokens": [407, 300, 311, 437, 321, 434, 884, 11, 341, 5658, 32884, 300, 321, 632, 341, 1379, 31959, 3142], "temperature": 0.0, "avg_logprob": -0.08727218327897318, "compression_ratio": 1.6912442396313363, "no_speech_prob": 1.749811417539604e-05}, {"id": 550, "seek": 279836, "start": 2816.36, "end": 2819.36, "text": " framework using that sort of likelihood framework.", "tokens": [8388, 1228, 300, 1333, 295, 22119, 8388, 13], "temperature": 0.0, "avg_logprob": -0.08727218327897318, "compression_ratio": 1.6912442396313363, "no_speech_prob": 1.749811417539604e-05}, {"id": 551, "seek": 279836, "start": 2819.36, "end": 2822.36, "text": " And it came back down to just predicting the noise.", "tokens": [400, 309, 1361, 646, 760, 281, 445, 32884, 264, 5658, 13], "temperature": 0.0, "avg_logprob": -0.08727218327897318, "compression_ratio": 1.6912442396313363, "no_speech_prob": 1.749811417539604e-05}, {"id": 552, "seek": 279836, "start": 2822.36, "end": 2825.36, "text": " And that's what the DPM paper showed in 2020.", "tokens": [400, 300, 311, 437, 264, 413, 18819, 3035, 4712, 294, 4808, 13], "temperature": 0.0, "avg_logprob": -0.08727218327897318, "compression_ratio": 1.6912442396313363, "no_speech_prob": 1.749811417539604e-05}, {"id": 553, "seek": 282536, "start": 2825.36, "end": 2832.36, "text": " But it turns out that is equivalent to calculating out this sort of score function", "tokens": [583, 309, 4523, 484, 300, 307, 10344, 281, 28258, 484, 341, 1333, 295, 6175, 2445], "temperature": 0.0, "avg_logprob": -0.050513474146525066, "compression_ratio": 1.7816593886462881, "no_speech_prob": 4.681845894083381e-05}, {"id": 554, "seek": 282536, "start": 2832.36, "end": 2838.36, "text": " and using that information to be able to sample from our distribution.", "tokens": [293, 1228, 300, 1589, 281, 312, 1075, 281, 6889, 490, 527, 7316, 13], "temperature": 0.0, "avg_logprob": -0.050513474146525066, "compression_ratio": 1.7816593886462881, "no_speech_prob": 4.681845894083381e-05}, {"id": 555, "seek": 282536, "start": 2838.36, "end": 2841.36, "text": " So that's kind of how these two approaches connect.", "tokens": [407, 300, 311, 733, 295, 577, 613, 732, 11587, 1745, 13], "temperature": 0.0, "avg_logprob": -0.050513474146525066, "compression_ratio": 1.7816593886462881, "no_speech_prob": 4.681845894083381e-05}, {"id": 556, "seek": 282536, "start": 2841.36, "end": 2846.36, "text": " So there's a lot of literature talking about maybe that sort of probabilistic likelihood", "tokens": [407, 456, 311, 257, 688, 295, 10394, 1417, 466, 1310, 300, 1333, 295, 31959, 3142, 22119], "temperature": 0.0, "avg_logprob": -0.050513474146525066, "compression_ratio": 1.7816593886462881, "no_speech_prob": 4.681845894083381e-05}, {"id": 557, "seek": 282536, "start": 2846.36, "end": 2848.36, "text": " perspective of diffusion models.", "tokens": [4585, 295, 25242, 5245, 13], "temperature": 0.0, "avg_logprob": -0.050513474146525066, "compression_ratio": 1.7816593886462881, "no_speech_prob": 4.681845894083381e-05}, {"id": 558, "seek": 282536, "start": 2848.36, "end": 2853.36, "text": " And there's also a lot of literature talking about this score-based perspective.", "tokens": [400, 456, 311, 611, 257, 688, 295, 10394, 1417, 466, 341, 6175, 12, 6032, 4585, 13], "temperature": 0.0, "avg_logprob": -0.050513474146525066, "compression_ratio": 1.7816593886462881, "no_speech_prob": 4.681845894083381e-05}, {"id": 559, "seek": 285336, "start": 2853.36, "end": 2858.36, "text": " But this hopefully allows you to think about the similarities and how these two", "tokens": [583, 341, 4696, 4045, 291, 281, 519, 466, 264, 24197, 293, 577, 613, 732], "temperature": 0.0, "avg_logprob": -0.1393991978963216, "compression_ratio": 1.6, "no_speech_prob": 2.8779140848200768e-05}, {"id": 560, "seek": 285336, "start": 2858.36, "end": 2861.36, "text": " approaches connect with each other.", "tokens": [11587, 1745, 365, 1184, 661, 13], "temperature": 0.0, "avg_logprob": -0.1393991978963216, "compression_ratio": 1.6, "no_speech_prob": 2.8779140848200768e-05}, {"id": 561, "seek": 285336, "start": 2861.36, "end": 2863.36, "text": " Awesome. Yeah.", "tokens": [10391, 13, 865, 13], "temperature": 0.0, "avg_logprob": -0.1393991978963216, "compression_ratio": 1.6, "no_speech_prob": 2.8779140848200768e-05}, {"id": 562, "seek": 285336, "start": 2863.36, "end": 2868.36, "text": " And that's kind of the beauty, I think, of the math side of things here is that you", "tokens": [400, 300, 311, 733, 295, 264, 6643, 11, 286, 519, 11, 295, 264, 5221, 1252, 295, 721, 510, 307, 300, 291], "temperature": 0.0, "avg_logprob": -0.1393991978963216, "compression_ratio": 1.6, "no_speech_prob": 2.8779140848200768e-05}, {"id": 563, "seek": 285336, "start": 2868.36, "end": 2875.36, "text": " find all of these relationships between different fields and also between different", "tokens": [915, 439, 295, 613, 6159, 1296, 819, 7909, 293, 611, 1296, 819], "temperature": 0.0, "avg_logprob": -0.1393991978963216, "compression_ratio": 1.6, "no_speech_prob": 2.8779140848200768e-05}, {"id": 564, "seek": 285336, "start": 2875.36, "end": 2877.36, "text": " centuries, basically.", "tokens": [13926, 11, 1936, 13], "temperature": 0.0, "avg_logprob": -0.1393991978963216, "compression_ratio": 1.6, "no_speech_prob": 2.8779140848200768e-05}, {"id": 565, "seek": 287736, "start": 2877.36, "end": 2883.36, "text": " That allows you to do really kind of powerful and unexpected things.", "tokens": [663, 4045, 291, 281, 360, 534, 733, 295, 4005, 293, 13106, 721, 13], "temperature": 0.0, "avg_logprob": -0.0841822741944113, "compression_ratio": 1.5252525252525253, "no_speech_prob": 0.00014651985839009285}, {"id": 566, "seek": 287736, "start": 2883.36, "end": 2884.36, "text": " OK.", "tokens": [2264, 13], "temperature": 0.0, "avg_logprob": -0.0841822741944113, "compression_ratio": 1.5252525252525253, "no_speech_prob": 0.00014651985839009285}, {"id": 567, "seek": 287736, "start": 2884.36, "end": 2890.36, "text": " So you can just do a quick recap of where we got to.", "tokens": [407, 291, 393, 445, 360, 257, 1702, 20928, 295, 689, 321, 658, 281, 13], "temperature": 0.0, "avg_logprob": -0.0841822741944113, "compression_ratio": 1.5252525252525253, "no_speech_prob": 0.00014651985839009285}, {"id": 568, "seek": 287736, "start": 2890.36, "end": 2895.36, "text": " So we started out with our data distribution, which we want to model.", "tokens": [407, 321, 1409, 484, 365, 527, 1412, 7316, 11, 597, 321, 528, 281, 2316, 13], "temperature": 0.0, "avg_logprob": -0.0841822741944113, "compression_ratio": 1.5252525252525253, "no_speech_prob": 0.00014651985839009285}, {"id": 569, "seek": 287736, "start": 2895.36, "end": 2902.36, "text": " We said we'll define this forward diffusion process, which is a way of kind of adding", "tokens": [492, 848, 321, 603, 6964, 341, 2128, 25242, 1399, 11, 597, 307, 257, 636, 295, 733, 295, 5127], "temperature": 0.0, "avg_logprob": -0.0841822741944113, "compression_ratio": 1.5252525252525253, "no_speech_prob": 0.00014651985839009285}, {"id": 570, "seek": 287736, "start": 2902.36, "end": 2904.36, "text": " noise to this model.", "tokens": [5658, 281, 341, 2316, 13], "temperature": 0.0, "avg_logprob": -0.0841822741944113, "compression_ratio": 1.5252525252525253, "no_speech_prob": 0.00014651985839009285}, {"id": 571, "seek": 290436, "start": 2904.36, "end": 2915.36, "text": " And because we added in this specific way, thanks to some discovery in the 1950s, the", "tokens": [400, 570, 321, 3869, 294, 341, 2685, 636, 11, 3231, 281, 512, 12114, 294, 264, 18141, 82, 11, 264], "temperature": 0.0, "avg_logprob": -0.10589720652653621, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.00019682545098476112}, {"id": 572, "seek": 290436, "start": 2915.36, "end": 2920.36, "text": " reverse process has the same form.", "tokens": [9943, 1399, 575, 264, 912, 1254, 13], "temperature": 0.0, "avg_logprob": -0.10589720652653621, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.00019682545098476112}, {"id": 573, "seek": 290436, "start": 2920.36, "end": 2932.36, "text": " And then we already know how to train a neural network for this using the ALBO.", "tokens": [400, 550, 321, 1217, 458, 577, 281, 3847, 257, 18161, 3209, 337, 341, 1228, 264, 7056, 15893, 13], "temperature": 0.0, "avg_logprob": -0.10589720652653621, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.00019682545098476112}, {"id": 574, "seek": 293236, "start": 2932.36, "end": 2939.36, "text": " And then a couple of years later came the discovery, simplifying assumptions that in", "tokens": [400, 550, 257, 1916, 295, 924, 1780, 1361, 264, 12114, 11, 6883, 5489, 17695, 300, 294], "temperature": 0.0, "avg_logprob": -0.11421161693531079, "compression_ratio": 1.6150442477876106, "no_speech_prob": 0.0001673741644481197}, {"id": 575, "seek": 293236, "start": 2939.36, "end": 2942.36, "text": " the end, all we do is predict the noise.", "tokens": [264, 917, 11, 439, 321, 360, 307, 6069, 264, 5658, 13], "temperature": 0.0, "avg_logprob": -0.11421161693531079, "compression_ratio": 1.6150442477876106, "no_speech_prob": 0.0001673741644481197}, {"id": 576, "seek": 293236, "start": 2942.36, "end": 2948.36, "text": " And I just remembered we take actually the MSE of this noise prediction, the mean squared", "tokens": [400, 286, 445, 13745, 321, 747, 767, 264, 376, 5879, 295, 341, 5658, 17630, 11, 264, 914, 8889], "temperature": 0.0, "avg_logprob": -0.11421161693531079, "compression_ratio": 1.6150442477876106, "no_speech_prob": 0.0001673741644481197}, {"id": 577, "seek": 293236, "start": 2948.36, "end": 2952.36, "text": " error, which is a nice, very simple framing of the model.", "tokens": [6713, 11, 597, 307, 257, 1481, 11, 588, 2199, 28971, 295, 264, 2316, 13], "temperature": 0.0, "avg_logprob": -0.11421161693531079, "compression_ratio": 1.6150442477876106, "no_speech_prob": 0.0001673741644481197}, {"id": 578, "seek": 293236, "start": 2952.36, "end": 2958.36, "text": " And then Taneesh spoke about another way to derive all of this, which is the score function", "tokens": [400, 550, 314, 1929, 14935, 7179, 466, 1071, 636, 281, 28446, 439, 295, 341, 11, 597, 307, 264, 6175, 2445], "temperature": 0.0, "avg_logprob": -0.11421161693531079, "compression_ratio": 1.6150442477876106, "no_speech_prob": 0.0001673741644481197}, {"id": 579, "seek": 295836, "start": 2958.36, "end": 2963.36, "text": " approach, the gradient of the log likelihood.", "tokens": [3109, 11, 264, 16235, 295, 264, 3565, 22119, 13], "temperature": 0.0, "avg_logprob": -0.09838208486867506, "compression_ratio": 1.5450236966824644, "no_speech_prob": 0.00010522327647777274}, {"id": 580, "seek": 295836, "start": 2963.36, "end": 2966.36, "text": " OK, cool.", "tokens": [2264, 11, 1627, 13], "temperature": 0.0, "avg_logprob": -0.09838208486867506, "compression_ratio": 1.5450236966824644, "no_speech_prob": 0.00010522327647777274}, {"id": 581, "seek": 295836, "start": 2966.36, "end": 2973.36, "text": " Yeah, I highly recommend checking out the course lesson as well if you haven't.", "tokens": [865, 11, 286, 5405, 2748, 8568, 484, 264, 1164, 6898, 382, 731, 498, 291, 2378, 380, 13], "temperature": 0.0, "avg_logprob": -0.09838208486867506, "compression_ratio": 1.5450236966824644, "no_speech_prob": 0.00010522327647777274}, {"id": 582, "seek": 295836, "start": 2973.36, "end": 2978.36, "text": " If you don't understand this, there's no need to be intimidated.", "tokens": [759, 291, 500, 380, 1223, 341, 11, 456, 311, 572, 643, 281, 312, 40234, 13], "temperature": 0.0, "avg_logprob": -0.09838208486867506, "compression_ratio": 1.5450236966824644, "no_speech_prob": 0.00010522327647777274}, {"id": 583, "seek": 295836, "start": 2978.36, "end": 2983.36, "text": " You can still be very effective without ever using math.", "tokens": [509, 393, 920, 312, 588, 4942, 1553, 1562, 1228, 5221, 13], "temperature": 0.0, "avg_logprob": -0.09838208486867506, "compression_ratio": 1.5450236966824644, "no_speech_prob": 0.00010522327647777274}, {"id": 584, "seek": 295836, "start": 2983.36, "end": 2987.36, "text": " You can be very effective at deep learning, as fast AI has shown us.", "tokens": [509, 393, 312, 588, 4942, 412, 2452, 2539, 11, 382, 2370, 7318, 575, 4898, 505, 13], "temperature": 0.0, "avg_logprob": -0.09838208486867506, "compression_ratio": 1.5450236966824644, "no_speech_prob": 0.00010522327647777274}, {"id": 585, "seek": 298736, "start": 2987.36, "end": 2991.36, "text": " And you can do novel research as well.", "tokens": [400, 291, 393, 360, 7613, 2132, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.07570537100446985, "compression_ratio": 1.572093023255814, "no_speech_prob": 0.00021988866501487792}, {"id": 586, "seek": 298736, "start": 2991.36, "end": 2993.36, "text": " For me, this is interesting.", "tokens": [1171, 385, 11, 341, 307, 1880, 13], "temperature": 0.0, "avg_logprob": -0.07570537100446985, "compression_ratio": 1.572093023255814, "no_speech_prob": 0.00021988866501487792}, {"id": 587, "seek": 298736, "start": 2993.36, "end": 2997.36, "text": " And it's even beautiful in a way.", "tokens": [400, 309, 311, 754, 2238, 294, 257, 636, 13], "temperature": 0.0, "avg_logprob": -0.07570537100446985, "compression_ratio": 1.572093023255814, "no_speech_prob": 0.00021988866501487792}, {"id": 588, "seek": 298736, "start": 2997.36, "end": 3002.36, "text": " So I recommend checking it out, but don't feel intimidated.", "tokens": [407, 286, 2748, 8568, 309, 484, 11, 457, 500, 380, 841, 40234, 13], "temperature": 0.0, "avg_logprob": -0.07570537100446985, "compression_ratio": 1.572093023255814, "no_speech_prob": 0.00021988866501487792}, {"id": 589, "seek": 298736, "start": 3002.36, "end": 3007.36, "text": " You can find the course lesson links in the past AI forum.", "tokens": [509, 393, 915, 264, 1164, 6898, 6123, 294, 264, 1791, 7318, 17542, 13], "temperature": 0.0, "avg_logprob": -0.07570537100446985, "compression_ratio": 1.572093023255814, "no_speech_prob": 0.00021988866501487792}, {"id": 590, "seek": 298736, "start": 3007.36, "end": 3011.36, "text": " We'll add those links as well in the description of this video.", "tokens": [492, 603, 909, 729, 6123, 382, 731, 294, 264, 3855, 295, 341, 960, 13], "temperature": 0.0, "avg_logprob": -0.07570537100446985, "compression_ratio": 1.572093023255814, "no_speech_prob": 0.00021988866501487792}, {"id": 591, "seek": 298736, "start": 3011.36, "end": 3015.36, "text": " We'll also have a topic in the forum for this lesson.", "tokens": [492, 603, 611, 362, 257, 4829, 294, 264, 17542, 337, 341, 6898, 13], "temperature": 0.0, "avg_logprob": -0.07570537100446985, "compression_ratio": 1.572093023255814, "no_speech_prob": 0.00021988866501487792}, {"id": 592, "seek": 301536, "start": 3015.36, "end": 3022.36, "text": " You can have discussions there, post any comments, add any relevant links to the math.", "tokens": [509, 393, 362, 11088, 456, 11, 2183, 604, 3053, 11, 909, 604, 7340, 6123, 281, 264, 5221, 13], "temperature": 0.0, "avg_logprob": -0.06203137092219973, "compression_ratio": 1.6680497925311204, "no_speech_prob": 0.00024356508220080286}, {"id": 593, "seek": 301536, "start": 3022.36, "end": 3030.36, "text": " And then we have another lesson video by Jono, which I really recommend checking out.", "tokens": [400, 550, 321, 362, 1071, 6898, 960, 538, 7745, 78, 11, 597, 286, 534, 2748, 8568, 484, 13], "temperature": 0.0, "avg_logprob": -0.06203137092219973, "compression_ratio": 1.6680497925311204, "no_speech_prob": 0.00024356508220080286}, {"id": 594, "seek": 301536, "start": 3030.36, "end": 3037.36, "text": " He's a great teacher, and I think he was the first person to do a full course on stable diffusion.", "tokens": [634, 311, 257, 869, 5027, 11, 293, 286, 519, 415, 390, 264, 700, 954, 281, 360, 257, 1577, 1164, 322, 8351, 25242, 13], "temperature": 0.0, "avg_logprob": -0.06203137092219973, "compression_ratio": 1.6680497925311204, "no_speech_prob": 0.00024356508220080286}, {"id": 595, "seek": 301536, "start": 3037.36, "end": 3042.36, "text": " Yeah, Jono's video is kind of a deep dive into some of the code a little bit more and", "tokens": [865, 11, 7745, 78, 311, 960, 307, 733, 295, 257, 2452, 9192, 666, 512, 295, 264, 3089, 257, 707, 857, 544, 293], "temperature": 0.0, "avg_logprob": -0.06203137092219973, "compression_ratio": 1.6680497925311204, "no_speech_prob": 0.00024356508220080286}, {"id": 596, "seek": 301536, "start": 3042.36, "end": 3044.36, "text": " into some of the concepts a little bit more.", "tokens": [666, 512, 295, 264, 10392, 257, 707, 857, 544, 13], "temperature": 0.0, "avg_logprob": -0.06203137092219973, "compression_ratio": 1.6680497925311204, "no_speech_prob": 0.00024356508220080286}, {"id": 597, "seek": 304436, "start": 3044.36, "end": 3049.36, "text": " So I feel like between these three videos, it's a good overview.", "tokens": [407, 286, 841, 411, 1296, 613, 1045, 2145, 11, 309, 311, 257, 665, 12492, 13], "temperature": 0.0, "avg_logprob": -0.09652560519188949, "compression_ratio": 1.7355769230769231, "no_speech_prob": 0.00015233563317451626}, {"id": 598, "seek": 304436, "start": 3049.36, "end": 3058.36, "text": " I think, I mean, just to clarify, you don't need to understand all the math that Wastim described in this video.", "tokens": [286, 519, 11, 286, 914, 11, 445, 281, 17594, 11, 291, 500, 380, 643, 281, 1223, 439, 264, 5221, 300, 343, 525, 332, 7619, 294, 341, 960, 13], "temperature": 0.0, "avg_logprob": -0.09652560519188949, "compression_ratio": 1.7355769230769231, "no_speech_prob": 0.00015233563317451626}, {"id": 599, "seek": 304436, "start": 3058.36, "end": 3060.36, "text": " That's not to say you won't need to understand math.", "tokens": [663, 311, 406, 281, 584, 291, 1582, 380, 643, 281, 1223, 5221, 13], "temperature": 0.0, "avg_logprob": -0.09652560519188949, "compression_ratio": 1.7355769230769231, "no_speech_prob": 0.00015233563317451626}, {"id": 600, "seek": 304436, "start": 3060.36, "end": 3065.36, "text": " We'll be covering lots of math in these lessons.", "tokens": [492, 603, 312, 10322, 3195, 295, 5221, 294, 613, 8820, 13], "temperature": 0.0, "avg_logprob": -0.09652560519188949, "compression_ratio": 1.7355769230769231, "no_speech_prob": 0.00015233563317451626}, {"id": 601, "seek": 304436, "start": 3065.36, "end": 3072.36, "text": " But we'll be covering just the math you need to understand and build on the code.", "tokens": [583, 321, 603, 312, 10322, 445, 264, 5221, 291, 643, 281, 1223, 293, 1322, 322, 264, 3089, 13], "temperature": 0.0, "avg_logprob": -0.09652560519188949, "compression_ratio": 1.7355769230769231, "no_speech_prob": 0.00015233563317451626}, {"id": 602, "seek": 307236, "start": 3072.36, "end": 3079.36, "text": " And we'll be covering it over many, many more hours than this rather rapid overview.", "tokens": [400, 321, 603, 312, 10322, 309, 670, 867, 11, 867, 544, 2496, 813, 341, 2831, 7558, 12492, 13], "temperature": 0.0, "avg_logprob": -0.1347688486878301, "compression_ratio": 1.4493670886075949, "no_speech_prob": 0.00010664674482541159}, {"id": 603, "seek": 307236, "start": 3079.36, "end": 3081.36, "text": " Perfect. Cool.", "tokens": [10246, 13, 8561, 13], "temperature": 0.0, "avg_logprob": -0.1347688486878301, "compression_ratio": 1.4493670886075949, "no_speech_prob": 0.00010664674482541159}, {"id": 604, "seek": 307236, "start": 3081.36, "end": 3085.36, "text": " And yeah, thank you so much, Denise. I had a lot of fun.", "tokens": [400, 1338, 11, 1309, 291, 370, 709, 11, 38133, 13, 286, 632, 257, 688, 295, 1019, 13], "temperature": 0.0, "avg_logprob": -0.1347688486878301, "compression_ratio": 1.4493670886075949, "no_speech_prob": 0.00010664674482541159}, {"id": 605, "seek": 307236, "start": 3085.36, "end": 3089.36, "text": " And thank you so much, Wastim. That was awesome.", "tokens": [400, 1309, 291, 370, 709, 11, 343, 525, 332, 13, 663, 390, 3476, 13], "temperature": 0.0, "avg_logprob": -0.1347688486878301, "compression_ratio": 1.4493670886075949, "no_speech_prob": 0.00010664674482541159}, {"id": 606, "seek": 308936, "start": 3089.36, "end": 3103.36, "text": " Cool. Bye bye.", "tokens": [50364, 8561, 13, 4621, 6543, 13, 51064], "temperature": 0.0, "avg_logprob": -0.28735604882240295, "compression_ratio": 0.6363636363636364, "no_speech_prob": 0.0003363100113347173}], "language": "en"}