{"text": " Okay, so welcome back, so we're going to start by doing some review, and we're going to talk about test sets training sets validation sets and OOB Something we haven't covered yet, but we will cover in more detail later is also cross validation But I'm going to talk about that as well right so We have a data set with a bunch of rows in it, and we've got some dependent variable and So what's the difference between like machine learning and Kind of pretty much any other kind of work that the difference is that in machine learning the thing we care about is the generalization Accuracy or the generalization error where else in like pretty much everything else all we care about is is how well we could have mapped to the observations will stop and so this this thing about generalization is the key unique piece of machine learning and So if we want to know whether we're good doing a good job of machine learning we need to do know whether we're doing a Good job of generalizing if we don't know that we know nothing right By generalizing to mean like scaling being able to scale larger No, I don't mean scaling at all so scaling is an important thing in many many areas. It's like okay We've got something that works on my computer with 10,000 Items I now need to work make it work on 10,000 items per second or something so scaling is important But not just for machine learning for just about everything we put in production Generalization is where I say okay here is a model that can predict Cats from dogs I've looked at five pictures of cats five pictures of dogs, and I've built a model that is perfect and Then I look at a different set of five cats and dogs and it gets them all wrong So in that case what it learned was not the district a cat and a dog But it learned what those five exact cats look like and those five exact dogs look like or I've got a model of predicting grocery sales for a particular Product so for toilet rolls in New Jersey last month And then I go and put it into production and it scales great in other words it has a great latency I don't have a high CPU load, but it fails to predict anything well other than Toilet rolls in New Jersey, and it also turns out I don't you did it well for last month not the next month, so these are all generalization areas so The most common way that people check for the ability to generalize is To create a random sample, so they'll grab a few rows at random and pull it out Into a test set and Then they'll build all of their models on the rest of the rows and then when they're finished They'll check the the accuracy they got on there, so the rest of the rows are called the training set everything else everything Else We could call the training set and So at the end of their modeling process on the training set they got an accuracy of 99% you're predicting cats from dogs at the very end they check it against a test set to make sure that the model really does generalize now the problem is What if it doesn't? Right so okay well I could go back and change some hyper parameters do some data Augmentation and whatever else try to create a more generalizable model, and then I'll go back again After doing all that and check and it's still no good But and I'll keep doing this again and again until eventually after 50 attempts it does generalize But does it really generalize because maybe all I've done is accidentally found this one which happens to work just for that test set because I've tried 50 different things Right and so if I've got something which is like right coincidentally 0.05 5% of the time then a very likely to accidentally get a good result So what we generally do is we put aside a second data set They've got a couple more of these and put these aside into a validation set validation set right and then everything that's not in the Validation of test is now training and so what we do is we train a model check it against the validation to see if it generalizes Do that a few times and then when we finally got something we were like okay? We think this generalizes successfully based on the validation set and then at the end of the project we check it against the test set Yeah So basically by making this two layer test that validation set if it gets one right the other one wrong you kind of double checking your Errors kind of like it's checking that we haven't over fit to the validation set so if we're using the validation set again and again Then we could end up not coming up with a generalizable set of hyper parameters But a set of hyper parameters that just so happened to work on the training set and the validation set so So if we try 50 different models against The validation set and then at the end of all that we then check that against the test set and it still generalizes Well, then we're kind of going to say okay. That's good We've actually come up with generalizable model if it doesn't then that's going to say okay We've actually now over fit to the validation set at which point you're kind of in trouble right because You don't you know you don't have anything left Behind right so the idea is to use effective Techniques during the modeling so that so that doesn't happen right, but but if it's going to happen you want to find out about it like you need that test set to be there because otherwise when you put it in production and Then it turns out that it doesn't generalize that would be a really bad outcome right you end up with less people clicking on your ads or selling less of your products or Providing car insurance to very risky vehicles or whatever Just to make sure do you need to ever check if the validation set and the test set is Coherent or you just keep test set So if you've done what I've just done here, which is to randomly sample There's no particular reason to check as long as they're as long as they're big enough, right? But we're going to come back to your question in a different context in just a moment Now Another trick we've learned for random forests is a way of Not needing a validation set and the way that we learned was to use instead use the OOB Error or the OOB score and so this idea was to say well every time we train a tree in a random forest There's a bunch of observations that are held out anyway because that's how we get some of the randomness and so let's Calculate our score for each tree based on those held out samples and therefore the forest by averaging the trees that each Row was not part of training Okay and So the OOB score gives us something which is pretty similar to the Validation score But on average it's a little less good Can anybody either remember or figure out why on average it's a little less good? Quite a subtle one. Thank you. It's a change I'm not sure but is it because you are treating like you are doing every kind of probe Pre-processing on your test and so the OOB score is reflecting the performance on testing set No, so the OOB score is not using the test set at all The OOB score is using the held out rows in the training set of each tree So I mean the you are basically testing each tree on some data from the training set. Yes So you are you have the potential for overfitting? I Shouldn't cause overfitting because each one is looking at a held out Sample, so it's not an overfitting issue. It's quite a subtle issue Ernest. We'll never try Aren't the samples from OOB bootstrap samples they are so then you're never gonna on average They only grab 63% of right chance. I'm average the OOB is 1 minus 63% exactly Yeah, what's the issue? So then if you're not why would the score be lower than the validation score? And that implies that you're leaving sort of like a black hole in the data that there's like data points You're never going to sample and they're not gonna be represented by the model No, that's not true though, because each tree is looking at a different set, right? So the OOB so like we've got like I don't know dozens of models right and in each one. There's a different set of rows Which which happened to be held out? right And so when we calculate the OOB score for like let's say row three We say okay row three is in this tree this tree And that's it and so we calculate the prediction on that tree and for that tree and we'd average those two predictions And so with enough trees You know each one has a 30 or so percent chance sorry 40 or so percent chance that the row is in that tree So if you have 50 trees It's almost certain that every row is going to be mentioned somewhere Did you have an idea? With validation set we can use the whole forest to make the predictions, but here we cannot use the whole forest So we cannot exactly see exactly so every row is Going to be using a subset of the trees to make its prediction and with less trees We know we get a less accurate prediction, so that's like that's a subtle one Right and if you didn't get it have a think during the week until you understand Why this is because it's a really interesting test if you're understanding of random forests of like why is OOB score on average less good than your validation score. They're both using random subs random held out subsets Anyway, it's generally close enough right so Why have a validation set at all when you're using random forest? If it's a randomly chosen validation set it's not strictly speaking necessary But you know you've got like four levels of things to test right so you could like test on the OOB When that's working well you can test on the validation set You know when hopefully by the time you check against the test set there's going to be no surprises, so that'd be one good reason Then what Kaggle do the way they do this is kind of clever what Kaggle do is they split the test set into two pieces a public and a private and they don't tell you which is rich, so you submit your predictions to Kaggle and then a Random 30% of those are used to tell you the leaderboard score But then at the end of the competition that gets thrown away, and they use the other 70% to calculate your real score so What that's doing is that you're making sure that you're not like? Continually using that feedback from the leaderboard to figure out some set of hyper parameters that happens to do well on the public But actually doesn't generalize okay, so it's a great test But this is one of the reasons why it's good practice to use Kaggle Because at the end of a competition at some point this will happen to you And you'll drop a hundred places on the leaderboard the last day of the competition when they use the private test set and say oh That's what it feels like to overfit And it's much better to practice and get that sense there than it is to do it in a company where there's hundreds of millions of dollars on the line Okay, so this is like the easiest possible situation where you're able to use a random sample for your validation set Why might I not be able to use a random sample from my validation set? In the case of something where we're forecasting we can't randomly sample because we need to maintain the temporal ordering Go on why is that? Because it doesn't it doesn't make sense so in the case of like an ARMA model I can't use like I can't pull out random rows because there's I'm thinking that there's like a certain dependency or I'm I'm trying to model a certain dependency that relies on like a specific lag term Now if I randomly sample those things then that lag term isn't there for me to okay, so it could be like a a technical modeling issue that like I'm using a model that relies on like Yesterday the day before and the day before that and if I've randomly removed some things I don't have yesterday and my model might just fail okay. That's true, but there's a more fundamental issue You want to pass it to Tyler? It's a really good point Although you know in general we're going to try to build models that are not that are more resilient than that particularly with Yeah Temporal order we expect things that are close by in time to be related to things close to them so we so if we destroy the order like if if we destroy the order we Really aren't going to be able to use that this time is close to this other time Um I don't think that's true because can pull out a random sample for a validation set and still keep everything nicely ordered well We would like to predict things in the future Which we would require as much data Close to the end of our okay. That's true I mean we could be like limiting the amount of data that we have by taking some of it out But my claim is stronger my claim is that by using a random validation set We could get totally the wrong idea about our model Karen. Do you want to have a try? So if our data is imbalanced for example We can if you're randomly sampling it we can only have one class in our validation set so our fitted model maybe That's true as well, so maybe you're trying to predict in a medical situation Who's going to die of lung cancer and that's only one out of a hundred people and we pick out a validation set that we Accidentally have nobody that died of lung cancer That's also true. These are all good niche examples But none of them quite say like why could the validation set just be plain? wrong like give you a totally Inaccurate idea of whether this is going to generalize and so let's talk about And the closest is is what Tyler was saying about time closeness in time The important thing to remember is when you build a model You're always you always have a systematic error Which is that you're going to use the model at a later time than the time that you built it right like You're going to put it into production by which time the world is different To the world that you're in now and even when you're building the model you're using data Which is older than today anyway right so there's some lag between the data that you're building it on and the data that it's going to actually be used on your life and A lot of the time if not most of the time that matters right so if we're doing stuff in like predicting who's going to buy toilet paper in New Jersey and it takes us two weeks to put it in production and We did it using data from the last couple of years then by that time you know things may look very different right and Particularly our validation set if we randomly sampled it right and it was like from a four-year period Then the vast majority of that data is going to be over a year old right and it may be that the toilet buying habits of folks in New Jersey may have Dramatically shifted maybe they've got a terrible recession there now, and they can't afford high quality toilet paper anymore Or maybe they know their paper making industry has gone through the roof and suddenly you know they could they're buying lots more toilet paper Because it's so cheap or whatever right so the world changes and therefore if you use a random Sample for your validation set then you're actually checking how good are you at predicting things that are totally obsolete now? But how good are you at predicting things that happened four years ago? That's not interesting Okay, so what we want to do in practice Anytime there's some temporal piece Is to instead say assuming that we've ordered it by time All right, so this is old and this is new That's our validation set Okay, or if we you know I suppose actually do it properly that's our validation set that's our test set Make sense right so here's our training set and we use that and we try and build a model that still works on stuff That's later in time than anything the model was built on and so we're not just testing generalization in some kind of abstract sense, but in a very Specific time sense which is it generalizes to the future could you pass it to siraj, please? So when we are as you said As you said there is some temporal ordering in the data So in that case is it wise to take the entire old data for training or only a few recent data set? So validation test or training training Yeah, that's a whole nother question right so how do you how do you get the validation set to be good? So I build them a random forest on all the training data. It looks good on the training data It looks good on the OOB But and this is actually a really good reason to have OOB if it looks good on the OOB But it means you're not overfitting in a statistical sense Right like it's it's it's working well on a random sample But then it looks bad on the validation set so what happened well what happened was that you you somehow failed to? predict the future you only predicted the past and So sir I've had an idea about how we could fix that would be okay Well, maybe we should just train so like maybe we shouldn't use the whole training set We should try a recent period only and now you know on the downside We're not using less data so we can create less rich models on the upside It's it's more up-to-date data, and this is something you have to play around with most Machine learning functions have the ability to provide a weight that is given to each row So for example with a random forest rather than bootstrapping at random You could have a weight on every row and randomly pick that row with some probability right and we could like say Here's our like probability we could like pick a Curve that looks like that So that the most recent rows have a higher probability of being selected that can work really well Yeah, it's something that you have to try and and if you don't have a validation set that represents the future Compared to what you're training on you have no way to know which of your techniques are working How do you make the compromise between amount of data versus recency of data? So what I tend to do is is when I have this kind of temporal issue, which is probably most of the time Once I have something that's working well on the validation set I wouldn't then go and just use that model on the test set because the thing that I've trained on is now like Much you know the test set is much more in the future compared to the training set so I would then Replicate building that model again, but this time I would combine the training and validation sets together Okay, and and retrain the model and at that point you've got no way to test Against a validation set so you have to make sure you have a reproducible Script or notebook that does exactly the same steps in exactly the same ways Because if you get something wrong, then you're going to find on the test set that you've you've got a problem So So what what I do in practice is I need to know is my validation set a truly representative of the test set so what I do is I build five models on the training set I Build five models on the training set and I try to have them kind of vary in how good I think they are Right and then and then I score them my five models on the validation set Right and then I also score them on the test set right so I'm not cheating So I'm not using any feedback from the test set to change my hyper parameters I'm only using it for this one thing which is to check my validation set so I get my five scores from the test set and Then I check That they fall in a line Okay And if they don't then you're not going to get good enough feedback from the validation set so keep doing that process Until you're getting a line and that can be quite tricky right sometimes the the test set You know trying to create something that's as similar to the real world outcome as possible It's difficult right and when you're kind of in the real world the same is true of creating the test set like the test set Has to be a close to production as possible So like what's the actual mix of customers that are going to be using this? How much time is there actually going to be between when you build the model and when you put it in production? How often are you going to be able to refresh the model these are all the things to think about when you build that test set? Okay So you want to say that first make five models on the training data Yeah, and then till you get a straight line relationship Change your validation and test set you can't really change the test set generally So this is assuming that the test sets given the change to change the validation set So if you start with a random sample validation set and then it's all over the place and you realize Oh, I should have picked the last two months And then you pick the last two months and still go over the place and you realize Oh, I should have picked it so that's also from the first of the month to the 15th of the month and They'll keep going until changing your validation set until you found a validation set which is indicative of your test set results So the five models like you would start maybe like just the random data and then average and they just make it better Yeah, yeah, yeah, yeah, maybe a Exactly, maybe kind of five like not terrible ones, but you want some variety and you also particularly want some variety in like How well they might generalize through time? So one that was trained on the whole training set one that was trained on the last two weeks One that was trained on the last six weeks One which used you know lots and lots of columns and might overfit a bit more Yeah, so you kind of want to get a sense of like oh if my validation set fails to Generalize temporarily I'd want to see that if it fails to generalize statistically. I want to see that Sorry, can you explain a bit more detail what you mean by change your validation set so it indicates the test set like what? Does that look like? So possibly so let's take the groceries competition where we're trying to predict The next two weeks of grocery sales so possible validation sets that Terrence and I played with was a random sample the Last month of data the last two weeks of data And the other one we tried was same day range One month earlier so that the test set in this competition was the first to the 15th of August Sorry this 15th that maybe the 15th the 30th of August So we tried like a random sample is four years we tried the 15th of July to the 15th of August we tried the 1st of August to the 15th of August and we tried the 15th of July to the 30th of July and So there were four different validation sets we tried and so with random you know our kind of results were all over the place With last month you know they were like not bad, but not great the last two weeks There was a couple that didn't look good, but on the whole they were good and same day range of months earlier They've got a basically perfect line. That's the part. I'm talking right there What exactly are you comparing it to from the test set? I just might confuse what you're creating that graph off of So for each of those so for each of my so I build five models right so there might be like Just predict the average do some kind of simple group mean of the whole data set do some group mean of the last month of The data set build a random forest of the whole thing build a random forest in the last two weeks on each of those I calculate the validation score and then I retrain the model on the whole training set and calculate the same thing on the test set and So each of these points now tells me how about it to go in the validation set how well did it go in the test set? And so if the validation set is useful we would say every time the validation set improves The test set should also score should also improve Yeah, so you just said retreat dreaming retreating the model on training and validation Yeah, that was a step. I was going back here So once I've got the validation score based on just the training set and then retrain it on the train and validation And check against it Somebody else so just to clarify By test set you mean Submitting it to Kaggle and then checking the score If it's Kaggle, then your test set is Kaggle's leaderboard In the real world the test set is this third data set that you put aside and it's that third data set that Having it reflect real-world production differences is the most important step in a machine learning project Why is it the most important step because if you screw up everything else that you don't screw up that You'll know you screwed up Right like if you've got a good test set Then you'll know you screwed up because you screwed up something else and you tested it and it didn't work out And it's like okay, you're not going to destroy the company, right if you screwed up creating the test set That would be awful right because then you don't know if you've made a mistake Right you try to build a model you test it on the test set it looks good But the test set was not indicative of real-world Environment So you don't actually know if you're going to destroy the company Now hopefully you've got ways to put things into production gradually so you won't actually destroy the company But you'll at least destroy your reputation at work, right? It's like oh Jeremy tried to put this thing into production and In the first week the cohort we tried it on their sales halved and we're never going to give Jeremy a machine learning job again All right, but if Jeremy had used a proper test set then like he would have known oh This is like half as good as my validation set said it would be I'll keep trying And now I'm not going to get in any trouble. I was actually like oh Jeremy's awesome He is identifies ahead of time when there's going to be a generalization problem Okay, so this is like This is something that kind of everybody talks about a little bit in machine learning classes But often it kind of stops at the point where you learn that there's a thing in SK learn Called make test train split and it returns these things and off you go right, but the fact that like Or here's the cross validation function, right? So The fact that these things always give you random samples tells you that like Much if not most of the time you shouldn't be using them The fact that random forest gives you an OOB for free It's useful, but it only tells you that this generalizes in a statistical sense not in a practical sense, right? so then finally there's cross validation right which Outside of class you guys have been talking about a lot which makes me feel somebody's been overemphasizing the value of this technique So I'll explain what cross validation is and then I'll explain why you probably shouldn't be using it most of the time So cross validation says let's not just pull out one validation set, but let's pull out five Say so let's assume that we're going to randomly shuffle the data first, right? This is critical, right? We first randomly shuffle the data and then we're going to split it into Five groups and then for model number one, we'll call this the validation set and We'll call this the training set Okay, and we'll train and we'll check against the validation and we'll get some RMS a R squared whatever and then we'll throw that away And we'll call this the validation set and we'll call this The training set and we'll get another score we'll do that five times And then we'll take the average Average okay, so that's a cross validation average Accuracy, so who can tell me like a benefit of using cross validation over a The kind of standard validation set I talked about before Could you pass it to fun? If you have a small data set is in a cross validation will make use over the data you have Yeah, you can use all of the data You don't have to put anything aside and you kind of get a little benefit as well in that like You've now got five models that you could ensemble together each one of used which used 80% of the data So, you know sometimes that on some link can be helpful I'm Fun could you tell me like what what could be some reasons that you wouldn't use cross validation? We have enough data. So we don't not want the validation set to be included in the model trainings process to like to pollute like Okay, yeah I'm not sure the cross validation is necessarily polluting the model. What would be a key like downside of cross validation? but like for deep learning if you have learned the pictures and Then you're gonna work will know the pictures and it's more likely to predict this as a right So sure, but if we if we put aside some data each time in the cross validation, can you pass it to Suraj? I'm I'm not so worried about Like I don't think there's like one of these validation sets is More statistically accurate. Yes, sir. I Think that's what fun was worried about I don't see why that would happen like each time we're fitting a model just behind you Each time we're fitting a model. We are absolutely holding out 20% of the sample All right. So yes, the five models between them have seen all of the data But but it's kind of like a random forest and that is a lot like a random first Each model has only been trained on the subset of the data Yes, Nisha see if it is like a large data set like it will take a lot of time. Oh, yes, exactly Right, so we have to fit five models rather than one. So here's a key downside number one Is time and so if we're? Doing deep learning and it takes a day to run suddenly it now takes five days or we need five GPUs Okay, what about my earlier issues about validation sets? Do you want to pass it over there? What's your name Jose? So if you had like temporal data wouldn't you be like by shuffling when you be breaking that relation Well, we could unshuffle it afterwards We could reorder it like we could shuffle get the training set out and then sort it by time Like I'd like this presumably there's a date column there. So I Don't think I don't think it's going to stop us from building a model. Did you have? With cross validation you're building five even validation sets And if there is some sort of structure that you're trying to capture in your validation set to mirror your test set you're essentially Just throwing that a chance to construct that yourself Right. I think you're going to say that I think you said the same thing as I'm going to say which is which is that Our earlier concerns about why random validation sets are a problem are entirely relevant here. All these validation sets are random So if a random validation set is not appropriate for your problem most likely because for example of temporal issues Then none of these four validation set five validation sets are any good. They're all random right and so if you have Temporal data like we did here There's no way to do cross validation really or like probably no good way to do cross validation. I mean You want to have Your validation set be as close to the test set as possible. And so you can't do that by randomly sampling different things so So as fun said you may well not need To do cross validation because most of the time in the real world We don't really have that little data right unless your data is based on some very very expensive labeling process or some Experiments that take a little cost a lot to run or whatever but nowadays that's Data scientists are not very often doing that kind of work summer in which cases is an issue, but most of us aren't so we probably don't need to as Nishan said if we do do it, it's going to take a whole lot of time Right and then as Ernest said even if we did do it and we took up all that time It might give us totally the wrong answer because random validation sets are inappropriate for our problem Okay, so I'm not going to be spending much time on cross validation because I just I think it's an interesting tool to Have it's easy to use SK learn has a cross validation thing you can go ahead and use but It's it's it's not that often that it's going to be an important part of your toolbox in my opinion It'll come up sometimes Okay, so that is validation sets so then the other thing we Started talking about last week And got a little bit stuck on because I screwed it up was tree interpretation So I'm actually going to cover that again without the error And dig into it in a bit more detail So can anybody tell me? What tree interpreter does and how it does it? What do you remember it's a difficult one to explain I don't think I did a good job of explaining it So don't worry if you don't do a great job, but does anybody want to have a go at explaining it? Okay, that's fine, so Let's start with the output of tree interpreter so if we look at a single model a single tree in other words Here is a single tree Okay, and So to remind us the top of a tree is before there's been any split at all so ten point one eight nine Is the average log price of all of the options in our training set? So I'm going to go ahead and draw Right here ten point one eight nine is the average of all Okay, and then if I go a couple of system less than or equal to point five Then I get ten point three four five okay, so for this subset of 16,800 Cuppler is less than or equal to point five the average is ten point three four five and Then off the people with a couple of system less than or equal to point five We then take the subset where enclosure is less than or equal to two and the average there of log sale price is nine point nine Five five so here's nine point nine five five and then final step in our tree is Is Model ID just for this group with no coupler system with enclosure less than or equal to two then let's just take model ID less than or equal to 45 73 and That gives us ten point two two six Okay, so then we can say all right starting with Ten point one oh nine one eight nine average for everybody in our training set for this particular tree sub sample of 20,000 Adding in the coupler decision or coupler less than or equal to point five Increased our prediction by point one five six so if we predict it with a naive model of just the mean it would have been ten point one oh nine Adding in just the coupler decision would have changed it to ten point three four five so this Variable is responsible for a point one five six increase in our prediction From that the enclosure decision was responsible for a minus point three nine five decrease The model ID was responsible for a point two seven six increase until eventually that was our final decision That was our prediction for this auction of this particular sale price So we can draw that as what's called a Waterfall plot right and waterfall plots are one of the most useful plots I know about and weirdly enough there's nothing in Python to do them and this is one of these things where there's this disconnect between like the world of like management consulting and business where everybody uses waterfall plots all the time and like academia Who have no idea what these things are but like every time like you're looking at say Here is Last year's sales for Apple and then there was a change in the iPhones increased by this amount Macs decreased by that amount and iPads increased by that amount every time you have a starting point In a number of changes and a finishing point waterfall charts are pretty much always the best way to show it so here Our prediction for price based on everything ten point one eight nine There was an increase blue means increase of point one five six the coupler decrease of point three nine five for enclosure Increase model ID of point two seven six so decrease Sorry increase decrease increase to get to our final ten point two six six so you see how waterfall chart works So with Excel 2016 you it's built-in you just click insert waterfall chart and there it is if you want to be a hero Create a waterfall chart Package for a map plot let put it on pip and everybody will love you for it There are some like really crappy Gists and manual Notebooks and stuff around these are actually super easy to build like you basically do a stacked column plot where this the bottom Of this is like all white Right like you can kind of do it But if you can wrap that up all and put the data the points in the right spots and color them nicely That would be totally awesome. I think you've all got the skills to do it and would make you know be a terrific thing for your portfolio So there's an idea Could make an interesting cattle kernel even like here's how to build a waterfall plot from scratch and by the way I've been put this up on tip you can all use it So in general therefore obviously going from the all and then going through each change Then the sum of all of those is going to be equal to the final prediction So that's how we could say if we were just doing a decision tree Then you know you're coming along and saying like how come this particular option was this particular price And it's like well your prediction for it and like oh, it's because of these three things had these three impacts, right? So for a random forest We could do that across all of the trees that so every time we see coupler We add up that change every time we see enclosure we add up that change every time we see model We add up that change okay, and so then we combine them all together We get what? Tree interpreter does but so you could go into the source code for tree interpreter, right? It's not at all complex logic, or you could build it yourself Right and you can see How it does exactly this so when you go tree interpreter dot predict with a random forest model for some specific? Auction so I've got a specific row here. This is my zero index row It tells you okay. This is the prediction the same as the random forest prediction Bias this is going to be always the same. It's the average sale price for for everybody for each of the random samples in the tree and then contributions is The average of also the total of all the contributions for each time we see that Specific column appear in a tree So last time I made the mistake of not sorting this correctly so this time and P dot arc sort is a super handy Function it sorts it doesn't actually sort Contribution zero it just tells you where each item would move to if it were sorted so now by passing ID access to each one of The column the level Contribution I can then print out all those in the right order so I can see here. Here's my column here's the level And the contribution so the fact that it's a small version of this piece of industrial equipment meant that it was less expensive Right but the fact it was made pretty recently meant that was more expensive The fact that it's pretty old however made that it was less expensive right so this is not going to Really help you much at all with like a Kaggle style situation where you just need predictions That's going to help you a lot in a production environment or even pre-production right so like something which Any good manager should you should do if you say here's a machine learning model? I think we should use is they should go away and grab a few examples of actual customers or or actual Auctions or whatever and check whether your model looks intuitive right and if it says like my prediction is that You know Lots and lots of people are going to really enjoy This crappy movie you know and it's like well That was a really crappy movie then they're going to come back to you and say like explain why your models telling me That I'm going to like this movie because I hate that movie and then you can go back and you say well It's because you like this movie and because you're this age range and you're this gender on average actually people like you Did like that movie? Yeah What's the second element of each table? This is saying for this particular row It was a mini and it was 11 years old and it was a hydraulic excavator track three to four metric tons So I was just feeding back and telling you it's it because this is actually what it was It was these numbers, so I just went back to the original data to actually pull out the descriptive versions of each one Okay, so if we sum up all the contributions together and Then add them to the bias Then that would be the same as adding up those three things Adding it to this and as we know from our waterfall chart that gives us our final prediction this is a Almost totally unknown technique and this particular Library is almost totally unknown as well so like it's a great opportunity to You know show something that a lot of people like it's totally critical in my opinion But but rarely known so that's That's kind of the end of the random forest interpretation piece and hopefully you've now seen enough that when somebody says We can't use modern machine learning techniques because they're black boxes that are interpretable you have enough information to say you're full of shit All right, like they're extremely interpretable and the stuff that we've just done You know try to do that with a linear model Good luck to you You know even where you can do something similar to linear model trying to do it so that's not giving you totally the wrong answer And you had no idea it's a wrong answer. It's going to be a real challenge So the last step we're going to do before we try and build our own random forest is deal with this tricky issue of extrapolation so in this case If we look at our tree Let's look at the accuracy of our most recent trees We still have You know a big difference between our validation score and our training score the Actually this case it's not too bad that The difference between the OOB and the validation is actually pretty close So if there was a big difference between validation and OOB like I'd be very worried about that We've dealt with the temporal side of things correctly Let's just have a look at think I'm most recent model. I hear it was Yeah, so there's a tiny difference right and so on Kaggle at least you kind of need that last decimal place in the real world. I probably stop here But quite often you'll see there's a big difference between your validation score and your OOB score And I want to show you how you would deal with that Particularly because actually we know that the OOB should be a little worse Because it's using this less tree So it gives me a sense that we should be able to do a little bit better And so the reason the way we should be able to do a little bit better is by handling the time component a little bit better so Here's the problem with random forests when it comes to extrapolation when you When you've got a data set That's like you know for got four years of sales data in it and you create your tree Right and it says like oh if these if it's in some particular store And it's some particular item and it is on special You know here's the average price right it actually tells us the average price you know Over the whole training set which could be pretty old right and so when you then want To step forward to like well. What's going to be the price next month? It's never seen next month and and where else with a kind of a linear model It can find a relationship between time and price where even though we only had this much data When you then go and predict something in the future it can extrapolate that but a random forest can't do that There's no way if you think about it for a tree to be able to say well next month. It would be higher still so There's a few ways to deal with this and we'll talk about it over the next couple of lessons but one simple way is just to try to Avoid using time Variables as predictors if there's something else we could use that's going to give us a better You know something of a kind of a stronger relationship. That's actually going to work in the future so in this case What I wanted to do Was to first of all figure out? What's the difference between our validation set and our training set like if I understand the difference between our validation set and our Training set then that tells me What are the predictors? which which have a strong temporal component and therefore they may be Irrelevant by the time I get to the future time period so I do something really interesting Which is I create a random forest Where my dependent variable is is it in the validation set? Right so I've gone back, and I've got my whole data frame with the training and validation all together and I've created a new column called is valid Which I've set to one and then for all of the stuff in the training set I set it to zero That's I've got a new column Which is just is this in the validation set or not and then I'm going to use that as my dependent variable and build a random forest So this is a random forest not to predict price the predict is this in the validation set or not and so if your Variables were not time dependent, then it shouldn't be possible to figure out if something's in the validation set or not This is a great trick in Kaggle right because in Kaggle They often won't tell you whether the test set is a random sample or not So you could put the test set and the training set together Create a new column called is test and see if you can predict it if you can you don't have a random sample Which means you have to come and figure out how to create a validation set from it right and so in this case I can see I don't have a random sample because my validation set can be predicted with a point nine nine nine nine r-squared and So then if I look at feature importance the top thing is sales ID and so this is really interesting It tells us very clearly sales ID is not a random identifier, but probably it's something that's just set consecutively as time goes on we just increase the sales ID So I'll elapsed that was the number of days since the first date in our data set so not surprisingly that also is a good predictor interestingly machine ID Clearly each machine is being labeled with some consecutive identifier as well And then there's a big don't just look at the order look at the value so point seven point one point oh seven point oh oh two Okay, stop right these top three hundreds of times more important than the rest right so let's next grab those top three Right and we can then have a look at their values both in the training set and In the validation set and so we can see for example sales ID on average is a divided by a thousand on average is 1.8 million in the training set and 5.8 million in the validation set right so you like you can see Just confirm like okay. They're very different So let's drop them Okay, so after I drop them. Let's now see if I can predict whether something's in the validation set I still can with point nine eight password So once you remove some things then other things can like come to the front and it now turns out okay That's not surprisingly age You know things that are old You know more likely I guess to be in the validation set because if you know earlier on in the training set They can't be old yet You made same reason so then we can Try removing those as well and So once we let's see where do we go here? Yeah, so what we can try doing is we can then say all right. Let's take the sales ID So that's machine ID from the first one The age year made sale day of year from the second one and say okay. These are all time dependent features So I still want them in my random forest if they're important Right, but if they're not important then taking them out There are some other non time dependent variables that that work just as well that would be better Right because now I'm going to have a model that generalizes over time better So here I'm just going to go ahead and go through each one of those features and drop each one one at a time Okay, retrain a new random forest and print out the score Okay, so before we do any of that our score was 0.88 for our validation versus 0.89 oob and You can see here when I remove sales ID my score goes up and This this is like what we're hoping for we've removed a time dependent variable There were other variables that could find similar relationships without the time dependency so removing it Caused our validation to go up now. Oh, oh B didn't go up Right because this is genuinely Statistically a useful predictor right, but it's a time dependent one and we have a time dependent validation set So this is like really subtle, but it can be really important right it's trying to find the things that gives you a a generalizable time across time prediction, and here's how you can see it so by so it's like okay We should remove sales ID for sure right but sale elapsed didn't get better Okay, so we don't want that machine ID Did get better went from 888 to 893 so it's actually quite a bit better Age Got a bit better Year made got worse sale day of year got a bit better okay, so now we can say alright Let's get rid of the three Where we know that getting rid of it actually made it better Okay, and as a result look at this we're now up to 915 Okay, so we've got rid of three time dependent things and now as expected our validation is better than our OB Okay, so that was a super successful approach there right and so now we can check the feature importance And let's go ahead and say alright that was pretty damn good. Let's now Leave it for a while, so give it 160 trees. Let it chill on it and see how that goes Okay, and so as you can see like we did all of our interpretation all of our fine-tuning Basically with smaller models subsets and at the end we run the whole thing it actually still only took 16 seconds And so we've now got an RMSE of point two one okay, so now we can check that against Kaggle again, we can't we unfortunately this Older competition we're not allowed to enter any more to see how we would have gone so the best we can do is check Whether it looks like we could have done well based on our validation set So it should be in the right area and yeah based on that we would have come first Okay, so You know I think this is an interesting series of steps right so you can go through the same series of steps in your Kaggle projects and more importantly your real-world projects so one of the challenges is once you leave this learning environment Suddenly you're surrounded by people who they never have enough time. They always want you to be in a hurry They're always telling you you know do this and then do that you need to find the time to step away Right and go back because this is a genuine real-world modeling process you can use And it gives well I said it gives world-class results I I mean it right like this guy who won this list of costs sadly he's passed away But he is the top Kaggle Competitor of all time like he he won I believe like dozens of competition So if we can get a score even within kui of him, then we are doing really really well Okay, so let's take a five-minute break, and we're going to come back and build our own random forest I just wanted to clarify something quickly a very good point during the break was Going back to the change in r-squared between here and Here it's not just due to the fact that we removed these three predictors We also went reset RF samples, but so to actually see the impact of just removing we need to compare it to The final step earlier, so it's actually compared to 907 so removing those three things took us from 907 to 915 Okay, so I mean and you know in the end of course what matters is our final model that yeah, just to clarify Okay so Some of you have asked me about writing your own random forests from scratch I don't know if any of you have given it a try yet my original plan here was to Do it in real time and then as I started to do it I realized that that would have kind of been boring because for you because I Screwed things up all the time so instead we might do more of like a walk through the code together Just as an aside This reminds me talking about the exam actually somebody asked on the forum about like what what can you expect from the exam? the basic plan is to make it a The exam be very similar to these notebooks, so it'll probably be a notebook that you have to you know Get a data set create a model trainer feature importance whatever right and the plan is that it'll be Open book open internet you can use whatever resources you like so basically if you're entering cat competitions the exam should be very straightforward I Also expect that there will be some pieces about like Here's a partially completed random forest or something you know finish Finish writing this step here, or here's a random forest Implement feature importance or you know implement one of the things we've talked about so it'll be you know The exam will be much like what we do in class and what you're expected to be doing during the week There won't be any Define this or tell me the difference between this word and that word or whatever there's not going to be any rote learning It'll be entirely like are you an effective machine learning practitioner ie can you use the algorithms? Do you know can you create an effective validation set and can you can you create parts of the algorithm? Implement them from scratch, so it'll be all about writing code basically, so if you're not comfortable writing code to practice machine learning then You should be practicing that all the time if you are comfortable you should be practicing that all the time also Whatever you're doing write code to implement random to do machine learning Okay So I I kind of have a particular way of Writing code And I'm not going to claim It's the only way of writing code, but it might be a little bit different to what you're used to and hopefully you'll find it At least interesting creating implementing random forest algorithms Is actually quite tricky not because the codes tricky like generally speaking Most random forest algorithms are pretty conceptually easy You know that generally speaking Academic papers and books have a knack of making them look difficult, but they're not difficult conceptually what's difficult is getting all the details right and Knowing and knowing when you're right and so in other words we need a good way of doing testing So if we're going to re-implement something that already exists so like say we wanted to create a random forest in some different Framework different language different operating system. You know I would always start with something that does exist right so in this case We're just going to do as a learning exercise writing a random forest in Python so for testing I'm going to compare it to an existing random forest implementation Okay, so that's like critical anytime. You're doing anything Involving like non trivial amounts of code in machine learning Knowing whether you've got it right or wrong is kind of the hardest bit I always assume that I've screwed everything up at every step and so I'm thinking like okay assuming that I screwed it up How do I figure out that I screwed it up right and then much to my surprise from time to time? I actually get something right and then I can move on but most of the time I get it wrong so Unfortunately with machine learning there's a lot of ways you can get things wrong that don't give you an error They just make your result like slightly less good And so that's that's what you want to pick up So given that I want to kind of compare it to an existing implementation I'm going to use our existing data set our existing validation set and then to simplify things I'm just going to use two columns to start with So let's go ahead and start writing a random forest so my way of writing Nearly all code is top-down just like my teaching and so by top-down I Start by assuming that everything I want already exists Right so in other words the first thing I want to do I'm going to call this a tree ensemble Right so to create a random forest the first question I have is What do I need to pass in? Right what do I need to initialize my random first so I'm going to need some independent variables some dependent variable Pick how many trees I want? I'm going to use the sample size parameter from the start here So how big you want each sample to be and then maybe some optional parameter of what's the smallest leaf size? Okay For testing it's nice to use a constant random seed so we'll get the same result each time So this is just how you set a random seed, okay? Maybe it's worth mentioning this for those who aren't familiar with it Random number generators on computers aren't random at all. They're actually called pseudo random number generators And what they do is given some initial starting point in this case 42 a pseudo random number generator is a mathematical function that generates a deterministic always the same sequence of numbers such that those numbers are designed to be as Uncorrelated with the previous number as possible Okay, and as unpredictable as possible and As uncorrelated as possible with something with a different random seed so the second number in in the sequence starting with 42 should be very different to the second number starting with 41 and Generally they involve kind of like taking you know You know using big prime numbers and taking mods and stuff like that. It's kind of an interesting area of math If you want real random numbers the only way to do that is again You can actually buy Hardware called a hardware random number generator that will have inside them like a little bit of some radioactive Reactive substance and and like something that detects how many things it's spitting out or you know, there'll be some hardware thing Getting current System time is is it a valid? Random like random number generation process so that would be for maybe for a random seed, right? So this thing is like what do we start the function with? so one of the really interesting areas is like in your computer if you don't set the random seed what is it set to and Yeah, quite often people use the current time for security like obviously We use a lot of random number stuff for security stuff Like if you're generating an SSH key, you need some it needs to be random It turns out like, you know people can figure out roughly when you created a key Like they could look at like Oh ID RSA has a timestamp and they could try, you know All the different nanoseconds starting points for a random number generator around that time step and figure out your key So in practice a lot of like really random High randomness requiring applications actually have a step that say please move your mouse and type random stuff at the keyboard for a while And so it like gets you to be a sort of it's called entropy to be a source of entropy other approaches is they'll look at like you know the hash of some of your log files or You know stuff like that It's a really really fun area So in our case our purpose actually is to remove randomness So we're saying okay generate a series of pseudo random numbers starting with 42. So it always should be the same So if you haven't done much stuff in Python, oh, oh, this is a basically standard idiom at least I mean I write it this way most people don't but if you pass in like One two three four five things that you're going to want to keep inside this object Then you basically have to say self dot X equals X self dot Y equals Y self dot sample equals sample Right and so we can assign to a tuple from a tuple, so You know again, this is like my way of coding most people think this is horrible But I prefer to be able to see everything at once and so I know in my code anytime I see something looks like this. It's always all of the Stuff in the method being set if I did it a different way then half the codes now come off the bottom of the Page and you can't see it. So alright, so So that was the first thing I thought about was like okay to create a random forest What information do you need then I'm going to need to store that information inside my object and so then I? Need to create some trees right a random forest is something that creates is something that has some trees So I basically figured okay List comprehension to create a list of trees. How many trees do we have or we've got n trees trees? That's what we asked for so range n trees gives me the numbers from 0 up to n trees minus 1 Okay, so if I create a list comprehension that loops through that range Calling create tree each time I now have n trees trees And now so I had to write that I didn't have to think at all like that's all like Obvious and so I've kind of delayed the thinking to the point where it's like well wait We don't have something to create a tree Okay, no worries, but let's pretend we did if we did we've now created a random forest Okay, we still need to like do a few things on top of that for example once we have it We would need a predict function so okay. Well. Let's write a predict function. How do you predict in a random forest? Can somebody tell me Either based on their own understanding or based on this line of code What would be like your one or two sentence answer? How do you make a prediction in a random forest? Spencer You would want to over every tree for your like the row that you're trying to predict on Average the values that your that each tree would produce for that But it's not like good and so you know that's a summary of what this says right so for a particular row I don't maybe this is a number of rows go through each tree Calculate is prediction so here is a list comprehension that is calculating the prediction for every tree for X I don't know if X is one row or multiple rows doesn't matter right As long as as long as tree dot predict works on it And then once you've got a list of things a cool trick to know is you can pass numpy dot mean a regular non numpy list Okay, and it'll take the mean you just need to tell it Axis equals zero means average it across the lists okay, so this is going to return the average of Dot predict for each tree and so I find list comprehensions Allow me to write the code in the way that my brain works like you could take the words Spencer said and like translate them into this code or you could take this code and translate them into words like the one Spencer Said right and so when I write code I want it to be as much like that as possible I want it to be readable and so hopefully you'll find like when you look at the fast AI code You're trying to understand how to Jeremy do X I try to write things in a way that you can read it and like it kind of turned into English in your head So if I say correctly that predict method is recursive It's no it's calling tree dot predict and we haven't written a tree yet So self dot trees is going to contain a tree object so this is tree ensemble dot predict and Inside the trees is a tree not a tree ensemble. So this is calling tree dot predict not tree ensemble dot predict Good question Okay, so we've nearly finished writing our random forest haven't we all we need to do now is write create tree right, so based on this code here or On your own understanding of how we create trees in a random forest. Can somebody tell me? Let's take a few seconds Have a read and have a think and then I'm going to try and come up with a way of saying How do you create a tree in a random forest? Okay, who wants to tell me Yes, okay. Let's time. It's got closer You take your You're essentially taking a random sample or of the original data and then you're just Get just constructing a tree. However, that happens So construct a decision tree like a non random tree from a random sample of the data Okay. So again like we've delayed any actual thought process here. We basically said okay We could pick some random IDs. This is a good trick to know if you call and pay dot random dot permutation passing in an int it'll give you back a Randomly shuffled sequence from zero to that it right and so then if you grab the first colon and Items of that that's now a random sub sample. So this is not doing bootstrapping. We're not doing sampling replacement here Which I think is fine, you know for my random forest I'm deciding that it's going to be something where we do the sub sampling not bootstrapping. Okay So here's a good line of code to know how to write Because it comes up all the time like I find in machine learning most algorithms I use are Somewhat random and so often I need some kind of random sample. Can you pass that tighter or change? Won't they give you one one extra because the you said it'll go from zero to length No, so this will give you if linself.y is Size n this will give you n a sequence of length n so 0 to n minus 1 Okay, and then from that I'm picking out Colon self dot sample size. So the first sample size ideas. I Have a comment on bootstrapping I think this method is better because we have chance of giving more weights to each Observation or am I thinking wrong? No, I mean I think you for bootstrapping. We could also give weights. I mean weighing single observations more than they are like without wanting that weight because when bootstrapping with replacement we can have Single observation and duplicates of it. Yeah the same tree. Yeah, it does feel weird, but I think I'm not sure that the actual Theory or empirical results backs up higher intuition that it's worse It would be interesting to look look back at that actually Personally I prefer this because I feel like most of the time we have more data than we Want to put a tree at once I feel like back when Breiman created random forests. It was 1999 It was kind of a very different world, you know where we pretty much always wanted to use all the data we had But nowadays I would say that's Generally not what we want We normally have too much data and so what people tend to do is they're like fire up a spark cluster and they'll run it on hundreds of machines when It makes no sense because if they had just used a sub sample each time they could have done it on one machine and like the the overhead of like Spark is a huge amount of IO overhead like I know you guys are doing distributed computing now If you if you've looked at some of the benchmarks Yeah, yeah exactly So if you do something on a single machine, it can often be hundreds of times faster Because you don't have all this this IO overhead and also tends to be easier to write the algorithms like you can use like SK learn easier to visualize cheaper so forth so like I Almost always avoid distributed computing and I have my whole life like even 25 years ago when I was starting in machine learning I you know still didn't use These clusters because I so I always feel like whatever I could do with a cluster now I could do with a single machine in five years time So why don't us focus on always being as good as possible with the single machine? You know and that's going to be more interactive and more iterative and work for me Okay, so so again we've like delayed thinking To the point where we have to write decision tree And so hopefully you get an idea that this top-down approach the goal is going to be that we're going to keep delaying thinking So long that that we delay it forever like like eventually we've somehow written the whole thing without actually having to think Right and that's that's kind of what I need because I'm kind of slow right so this is why I write Code this way and notice like you never have to design anything You know you just say hey, what if somebody already gave me the exact API I needed how would I use it? Okay, and then and then okay to implement that next stage What would be the exact API I would need to implement that you keep going down until eventually you're like oh that already exists Okay, so This assumes we've got a class for decision tree, so we're going to have to create that So a decision tree Is something so we already know what we're going to have to pass it because we just passed it right so we're passing in a random sample of X's a random sample of Y's Indexes is actually so we know that down the track so I've got to plan a tiny bit We know that a decision tree is going to contain decision trees which themselves contain decision trees And so as we go down the decision tree There's going to be some subset of the original data that we've kind of got and so I'm going to pass in the indexes of The data that we're actually going to use here, okay, so initially it's the entire Random sample right so I've got the whole I've got the whole range And I turn that into an array so that's zero the index is from zero to the size of the sample and Then we're just passed down the min leaf size so everything that we got for Constructing the random forest we're going to pass down the decision tree except of course num trees which is irrelevant for the decision tree So again now that we know that's the information we need we can go ahead and store it inside this object So I'm pretty likely to need to know How many rows we have in this tree which I generally call n? How many columns do I have which I generally call C? So the number of rows is just equal to the number of indexes We are given and the number of columns is just like however many columns there are in our independent variables So then we're going to need This value here We need to know for this tree What's its prediction right so The prediction for this tree is the mean of Our dependent variable Or those indexes which are inside this part of the tree right so at the very top Of the tree it contains all the indexes Right I'm assuming that by the time we've got to this point remember. We've already done the Random sampling Right so when we're talking about indexes we're not talking about the random sampling to create the tree We're assuming this tree now has some random sample inside decision tree This is this is the one of the nice things right inside decision tree whole random sampling things gone All right that was done by the random first right so at this point. We're building something That's just a plain old decision tree. It's not in any way a random sampling anything It's just a plain old decision tree right so the indexes is literally like Which subset of the data have we got to so far in this tree and so at the top of the decision tree It's all the data right so it's all of the indexes Okay So all of the indexes so this is therefore all of the dependent variable that are in this part of the tree And so this is the value mean of that So make sense anybody could be any questions about about that So yes, please ask to change she Actually just to let you know that's a large portion of us don't have a OOP I Mean all P experiments, okay sure so So quick so quick OOP primer would be helpful Great yeah, okay Who has done object-oriented programming in some programming language, okay? So you've all used actually lots of object-oriented programming in terms of using existing classes Right so every time we've created a random forest We've called the random forests constructor, and it's returned an object and then we've called methods and attributes on that object so fit is a method you can tell because it's got parentheses after it right where else Yeah, I will be score is a Property or an attribute doesn't have parentheses after it okay, so inside an object There are kind of two kinds of things there the functions that you can call So you have object dot function Parenthesis arguments or there are the properties or attributes you can grab which is Object dot and then just the attribute name with no parentheses So when and then the other thing that we do with objects is we create them Okay, we pass in the name of the class and it returns us the object and you have to tell it all of the parameters necessary to Get constructed So let's just copy this code And See how we're going to go ahead and build this So the first step is we're not going to go and it was random forest regressor We're going to go M equals tree ensemble. We're creating a class called tree ensemble, and we're going to pass in various Bits of information okay So maybe we'll have ten trees sample size of a thousand Maybe a min leaf of three right and you can always like choose to name your arguments or not So when you've got quite a few it's kind of nice to name them so that just so we can see what each one means It's always optional right So we're going to try and create a class that we can use like this and then I'm not sure we're going to bother with dot fit Because we've passed in the X and the Y right like in scikit-learn They use an approach where first of all you construct something without telling it what data to use and then you pass in the day We're doing these two steps at once. We're actually passing in the data right and so then after that we're going to be going and Dot so we're going to go preds equals M dot predict Passing in maybe some validation set Okay, so we're good. That's that's the API. We're kind of creating here So this thing here is called a constructor something that creates an object is called a constructor and Python There's a lot of ugly hideous things about Python one of which is they it uses these special magic method names Underscore underscore in it underscore underscore is a special magic method That's caught what's called when you try to construct a class so when I call tree ensemble Parenthesis it actually calls tree ensemble dot They see people say Dunder in it. I kind of hate it, but anyway Dunder in it double underscore in it double underscore Dunder in it So that's why we've got this method called Dunder in it. Okay, so when I call tree ensemble is going to call this method another hideously ugly thing about Python's oh, oh is that there's this special thing where if you have a class and to create a class you just write class in The name of class all of its methods Automatically get sent one extra parameter one extra argument Which is the first argument and you can call it anything you like if you call it anything other than self Everybody will hate you and you're a bad person Okay, so call it anything you like as long as itself Okay, so So that's why you always see this and in fact I can immediately see here I have a bug Anybody see the bug in my predict function. I should have self right I I Like they always do it right so anytime you try and call a method on your own class And you get something saying you passed in two parameters, and it was only expecting one you forgot self Okay, so like this is a really dumb way to add OOP to a programming language But the older languages like Python often did this because they kind of needed to they started out not being oh Oh, and then they kind of added. Oh, oh in a way that was hideously ugly so Perl Which predates Python by a little bit kind of I think really came up with this approach and unfortunately Other languages of that era stuck with it so You have to add in this magic self so the magic self now when you're inside this class You can now pretend as if any property name you like exists, so I can now pretend There's something called self dot X. I can read from it I can write to it right, but if I read from it, and I haven't yet written to it. I'll get an error So the stuff that's passed to the constructor Get thrown away by default like there's nothing that like says you need to this class needs to remember what these things are But anything that we stick inside self it's remembered for all time You know as long as this object exists you can access it. It's remembered so now that I've gone In fact, let's do this right, so let's let's create the tree ensemble class and Let's now Instantiate it okay Of course we haven't got X. We need to call X train Y train Okay decision tree is not defined so let's Create a really minimal decision tree There we go okay, so here is enough to actually Instantiate our tree ensemble okay, so we have to find the in it for it We have to find the in it for decision tree We need decision trees in it to be defined because inside our ensemble in it it called self dot create tree and Then self dot create tree called the decision tree constructor and then decision tree constructor Basically does nothing at all other than save some information right so at this point we can now go m dot Okay, and if I press tab at this point can anybody tell me what I would expect to see We would see like a we would see a drop-down of all available methods for that class okay In this case so if m is a tree ensemble we would have create tree and predict okay anything else um Wait what oh yeah, and as well as Ernest whispered the variables as well. Yeah, so the Variable could mean a lot of things well say the attributes so the things that we put inside self so if I hit tab Right there. They are right as Taylor said there's create tree there's predict, and then there's everything else be put inside self right so if I look at m dot Min leaf if I hit shift enter what will I say? Yeah, the number that I just put there I put min leaf is three so that went up here to min leaf this here is a default argument So it says if I don't pass anything it'll be five, but I did pass something right so three self dot min leaf Here is there going to be equal to min leaf here so something which Like because of this rather annoying way of doing o o It does mean that it's very easy to accidentally forget To do that right so if I don't assign it to self dot min leaf Right then I get an error and so here tree ensemble doesn't happen min leaf All right, so how do I create that attribute? I just put something in it Okay, so if you want to like if you don't know what a value of it should be yet But you kind of need to be able to refer to it. You can always go like self dot min leaf equals none Right so at least it's something you can read check for noneness and not have an error Great now Interestingly I was able to instantiate she ensemble even though predict refers to a method of decision tree That doesn't exist and this is actually something very nice about the dynamic nature of Python is that Because it's not like compiling it. It's not checking anything unless you're using it right so we can go ahead and create decision D dot predict later and Then our our instantiate an object will magically start working right it doesn't actually look up That functions that methods details until you use it and so it really helps with top-down programming Okay, so when you're Inside a class definition in other words you're at that indentation level you know indented one in so these are all class definitions Any function that you create unless you do some special things that we're not going to talk about yet is Automatically a method of that class and so every method of that class magically gets a self pass to it So we could call Since we've got a tree ensemble We could call M dot create tree and we don't put anything inside those parentheses Because the magic self will be passed and the magic self will be whatever M is Okay, so M dot create tree returns a decision tree just like we asked it to right so M dot create tree dot ID access Will give us the self dot ID access inside the decision tree Okay, which is set to And P dot arrange range self dot sample size Why is data scientists do we care about object-oriented programming? Because a lot of the stuff you use is going to require you to implement stuff with OOP for example Every single pie torch model of any kind is created with OOP. It's the only way to create by torch models um good news is What you see here is the entirety of what you need to know so you this is all you need to know you need to Know to create something called in it to assign the things that are passed in it to something called self and Then to stick the word self after each of your methods Okay, and so the nice thing is like now to think as an OOP programmer is to realize you don't now have to pass around X Y sample size and minleaf to every function that uses them by assigning them to Attributes of self they're now available like magic All right, so this is why OOP is super handy if you're particularly I started trying to create a decision tree initially without using OOP and try to like keep track of Like what that decision tree was meant to know about was very difficult You know where else with OOP you can just say it inside the decision tree, you know self dot indexes equals this and Everything just works. Okay. Okay. That's great. So we're out of time. I think that's That's great timing because There's an introduction to OOP, but this week You know next class I'm going to assume that you can use it right? So you should create some classes instantiate some classes look at their methods and properties Have them call each other and so forth until you feel Comfortable with them and maybe for those of you that haven't done OOP before you and find some other useful resources You could pop them onto the wiki thread so that other people know what you find Useful great. Thanks everybody", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 5.54, "text": " Okay, so welcome back, so we're going to start by doing some review, and we're going to talk about", "tokens": [1033, 11, 370, 2928, 646, 11, 370, 321, 434, 516, 281, 722, 538, 884, 512, 3131, 11, 293, 321, 434, 516, 281, 751, 466], "temperature": 0.0, "avg_logprob": -0.2128047499545785, "compression_ratio": 1.691891891891892, "no_speech_prob": 0.002844359027221799}, {"id": 1, "seek": 0, "start": 6.8, "end": 8.56, "text": " test sets", "tokens": [1500, 6352], "temperature": 0.0, "avg_logprob": -0.2128047499545785, "compression_ratio": 1.691891891891892, "no_speech_prob": 0.002844359027221799}, {"id": 2, "seek": 0, "start": 8.56, "end": 10.32, "text": " training sets", "tokens": [3097, 6352], "temperature": 0.0, "avg_logprob": -0.2128047499545785, "compression_ratio": 1.691891891891892, "no_speech_prob": 0.002844359027221799}, {"id": 3, "seek": 0, "start": 10.32, "end": 12.32, "text": " validation sets and OOB", "tokens": [24071, 6352, 293, 422, 46, 33], "temperature": 0.0, "avg_logprob": -0.2128047499545785, "compression_ratio": 1.691891891891892, "no_speech_prob": 0.002844359027221799}, {"id": 4, "seek": 0, "start": 14.1, "end": 19.62, "text": " Something we haven't covered yet, but we will cover in more detail later is also cross validation", "tokens": [6595, 321, 2378, 380, 5343, 1939, 11, 457, 321, 486, 2060, 294, 544, 2607, 1780, 307, 611, 3278, 24071], "temperature": 0.0, "avg_logprob": -0.2128047499545785, "compression_ratio": 1.691891891891892, "no_speech_prob": 0.002844359027221799}, {"id": 5, "seek": 0, "start": 19.62, "end": 22.240000000000002, "text": " But I'm going to talk about that as well right so", "tokens": [583, 286, 478, 516, 281, 751, 466, 300, 382, 731, 558, 370], "temperature": 0.0, "avg_logprob": -0.2128047499545785, "compression_ratio": 1.691891891891892, "no_speech_prob": 0.002844359027221799}, {"id": 6, "seek": 0, "start": 22.92, "end": 24.92, "text": " We have a data set", "tokens": [492, 362, 257, 1412, 992], "temperature": 0.0, "avg_logprob": -0.2128047499545785, "compression_ratio": 1.691891891891892, "no_speech_prob": 0.002844359027221799}, {"id": 7, "seek": 2492, "start": 24.92, "end": 31.040000000000003, "text": " with a bunch of rows in it, and we've got some dependent variable and", "tokens": [365, 257, 3840, 295, 13241, 294, 309, 11, 293, 321, 600, 658, 512, 12334, 7006, 293], "temperature": 0.0, "avg_logprob": -0.24095992743968964, "compression_ratio": 1.606060606060606, "no_speech_prob": 1.6963669622782618e-05}, {"id": 8, "seek": 2492, "start": 33.6, "end": 35.6, "text": " So what's the difference between", "tokens": [407, 437, 311, 264, 2649, 1296], "temperature": 0.0, "avg_logprob": -0.24095992743968964, "compression_ratio": 1.606060606060606, "no_speech_prob": 1.6963669622782618e-05}, {"id": 9, "seek": 2492, "start": 36.68, "end": 38.68, "text": " like machine learning and", "tokens": [411, 3479, 2539, 293], "temperature": 0.0, "avg_logprob": -0.24095992743968964, "compression_ratio": 1.606060606060606, "no_speech_prob": 1.6963669622782618e-05}, {"id": 10, "seek": 2492, "start": 40.52, "end": 47.6, "text": " Kind of pretty much any other kind of work that the difference is that in machine learning the thing we care about is", "tokens": [9242, 295, 1238, 709, 604, 661, 733, 295, 589, 300, 264, 2649, 307, 300, 294, 3479, 2539, 264, 551, 321, 1127, 466, 307], "temperature": 0.0, "avg_logprob": -0.24095992743968964, "compression_ratio": 1.606060606060606, "no_speech_prob": 1.6963669622782618e-05}, {"id": 11, "seek": 2492, "start": 47.88, "end": 49.480000000000004, "text": " the generalization", "tokens": [264, 2674, 2144], "temperature": 0.0, "avg_logprob": -0.24095992743968964, "compression_ratio": 1.606060606060606, "no_speech_prob": 1.6963669622782618e-05}, {"id": 12, "seek": 4948, "start": 49.48, "end": 56.31999999999999, "text": " Accuracy or the generalization error where else in like pretty much everything else all we care about is is", "tokens": [5725, 374, 2551, 420, 264, 2674, 2144, 6713, 689, 1646, 294, 411, 1238, 709, 1203, 1646, 439, 321, 1127, 466, 307, 307], "temperature": 0.0, "avg_logprob": -0.20615460612986347, "compression_ratio": 1.8722466960352422, "no_speech_prob": 6.540353297168622e-06}, {"id": 13, "seek": 4948, "start": 56.72, "end": 61.04, "text": " how well we could have mapped to the observations will stop and", "tokens": [577, 731, 321, 727, 362, 33318, 281, 264, 18163, 486, 1590, 293], "temperature": 0.0, "avg_logprob": -0.20615460612986347, "compression_ratio": 1.8722466960352422, "no_speech_prob": 6.540353297168622e-06}, {"id": 14, "seek": 4948, "start": 61.599999999999994, "end": 66.96, "text": " so this this thing about generalization is the key unique piece of", "tokens": [370, 341, 341, 551, 466, 2674, 2144, 307, 264, 2141, 3845, 2522, 295], "temperature": 0.0, "avg_logprob": -0.20615460612986347, "compression_ratio": 1.8722466960352422, "no_speech_prob": 6.540353297168622e-06}, {"id": 15, "seek": 4948, "start": 67.6, "end": 69.32, "text": " machine learning and", "tokens": [3479, 2539, 293], "temperature": 0.0, "avg_logprob": -0.20615460612986347, "compression_ratio": 1.8722466960352422, "no_speech_prob": 6.540353297168622e-06}, {"id": 16, "seek": 4948, "start": 69.32, "end": 75.44, "text": " So if we want to know whether we're good doing a good job of machine learning we need to do know whether we're doing a", "tokens": [407, 498, 321, 528, 281, 458, 1968, 321, 434, 665, 884, 257, 665, 1691, 295, 3479, 2539, 321, 643, 281, 360, 458, 1968, 321, 434, 884, 257], "temperature": 0.0, "avg_logprob": -0.20615460612986347, "compression_ratio": 1.8722466960352422, "no_speech_prob": 6.540353297168622e-06}, {"id": 17, "seek": 7544, "start": 75.44, "end": 80.08, "text": " Good job of generalizing if we don't know that we know nothing", "tokens": [2205, 1691, 295, 2674, 3319, 498, 321, 500, 380, 458, 300, 321, 458, 1825], "temperature": 0.0, "avg_logprob": -0.2306739091873169, "compression_ratio": 1.5508021390374331, "no_speech_prob": 1.844817688834155e-06}, {"id": 18, "seek": 7544, "start": 80.64, "end": 82.64, "text": " right", "tokens": [558], "temperature": 0.0, "avg_logprob": -0.2306739091873169, "compression_ratio": 1.5508021390374331, "no_speech_prob": 1.844817688834155e-06}, {"id": 19, "seek": 7544, "start": 86.36, "end": 89.52, "text": " By generalizing to mean like scaling being able to scale larger", "tokens": [3146, 2674, 3319, 281, 914, 411, 21589, 885, 1075, 281, 4373, 4833], "temperature": 0.0, "avg_logprob": -0.2306739091873169, "compression_ratio": 1.5508021390374331, "no_speech_prob": 1.844817688834155e-06}, {"id": 20, "seek": 7544, "start": 90.24, "end": 97.3, "text": " No, I don't mean scaling at all so scaling is an important thing in many many areas. It's like okay", "tokens": [883, 11, 286, 500, 380, 914, 21589, 412, 439, 370, 21589, 307, 364, 1021, 551, 294, 867, 867, 3179, 13, 467, 311, 411, 1392], "temperature": 0.0, "avg_logprob": -0.2306739091873169, "compression_ratio": 1.5508021390374331, "no_speech_prob": 1.844817688834155e-06}, {"id": 21, "seek": 7544, "start": 97.3, "end": 99.3, "text": " We've got something that works", "tokens": [492, 600, 658, 746, 300, 1985], "temperature": 0.0, "avg_logprob": -0.2306739091873169, "compression_ratio": 1.5508021390374331, "no_speech_prob": 1.844817688834155e-06}, {"id": 22, "seek": 7544, "start": 99.32, "end": 101.32, "text": " on my computer with", "tokens": [322, 452, 3820, 365], "temperature": 0.0, "avg_logprob": -0.2306739091873169, "compression_ratio": 1.5508021390374331, "no_speech_prob": 1.844817688834155e-06}, {"id": 23, "seek": 7544, "start": 101.56, "end": 103.03999999999999, "text": " 10,000", "tokens": [1266, 11, 1360], "temperature": 0.0, "avg_logprob": -0.2306739091873169, "compression_ratio": 1.5508021390374331, "no_speech_prob": 1.844817688834155e-06}, {"id": 24, "seek": 10304, "start": 103.04, "end": 108.34, "text": " Items I now need to work make it work on 10,000 items per second or something so scaling is important", "tokens": [467, 9097, 286, 586, 643, 281, 589, 652, 309, 589, 322, 1266, 11, 1360, 4754, 680, 1150, 420, 746, 370, 21589, 307, 1021], "temperature": 0.0, "avg_logprob": -0.17960885306385077, "compression_ratio": 1.6842105263157894, "no_speech_prob": 4.09288668379304e-06}, {"id": 25, "seek": 10304, "start": 109.04, "end": 112.28, "text": " But not just for machine learning for just about everything we put in production", "tokens": [583, 406, 445, 337, 3479, 2539, 337, 445, 466, 1203, 321, 829, 294, 4265], "temperature": 0.0, "avg_logprob": -0.17960885306385077, "compression_ratio": 1.6842105263157894, "no_speech_prob": 4.09288668379304e-06}, {"id": 26, "seek": 10304, "start": 113.80000000000001, "end": 119.76, "text": " Generalization is where I say okay here is a model that can predict", "tokens": [6996, 2144, 307, 689, 286, 584, 1392, 510, 307, 257, 2316, 300, 393, 6069], "temperature": 0.0, "avg_logprob": -0.17960885306385077, "compression_ratio": 1.6842105263157894, "no_speech_prob": 4.09288668379304e-06}, {"id": 27, "seek": 10304, "start": 120.36000000000001, "end": 127.2, "text": " Cats from dogs I've looked at five pictures of cats five pictures of dogs, and I've built a model that is perfect and", "tokens": [40902, 490, 7197, 286, 600, 2956, 412, 1732, 5242, 295, 11111, 1732, 5242, 295, 7197, 11, 293, 286, 600, 3094, 257, 2316, 300, 307, 2176, 293], "temperature": 0.0, "avg_logprob": -0.17960885306385077, "compression_ratio": 1.6842105263157894, "no_speech_prob": 4.09288668379304e-06}, {"id": 28, "seek": 10304, "start": 128.04000000000002, "end": 131.86, "text": " Then I look at a different set of five cats and dogs and it gets them all wrong", "tokens": [1396, 286, 574, 412, 257, 819, 992, 295, 1732, 11111, 293, 7197, 293, 309, 2170, 552, 439, 2085], "temperature": 0.0, "avg_logprob": -0.17960885306385077, "compression_ratio": 1.6842105263157894, "no_speech_prob": 4.09288668379304e-06}, {"id": 29, "seek": 13186, "start": 131.86, "end": 135.54000000000002, "text": " So in that case what it learned was not the district a cat and a dog", "tokens": [407, 294, 300, 1389, 437, 309, 3264, 390, 406, 264, 6566, 257, 3857, 293, 257, 3000], "temperature": 0.0, "avg_logprob": -0.2800655565763775, "compression_ratio": 1.7110091743119267, "no_speech_prob": 6.339170795399696e-06}, {"id": 30, "seek": 13186, "start": 135.66000000000003, "end": 142.3, "text": " But it learned what those five exact cats look like and those five exact dogs look like or I've got a model of", "tokens": [583, 309, 3264, 437, 729, 1732, 1900, 11111, 574, 411, 293, 729, 1732, 1900, 7197, 574, 411, 420, 286, 600, 658, 257, 2316, 295], "temperature": 0.0, "avg_logprob": -0.2800655565763775, "compression_ratio": 1.7110091743119267, "no_speech_prob": 6.339170795399696e-06}, {"id": 31, "seek": 13186, "start": 143.10000000000002, "end": 144.98000000000002, "text": " predicting", "tokens": [32884], "temperature": 0.0, "avg_logprob": -0.2800655565763775, "compression_ratio": 1.7110091743119267, "no_speech_prob": 6.339170795399696e-06}, {"id": 32, "seek": 13186, "start": 144.98000000000002, "end": 146.66000000000003, "text": " grocery sales", "tokens": [14410, 5763], "temperature": 0.0, "avg_logprob": -0.2800655565763775, "compression_ratio": 1.7110091743119267, "no_speech_prob": 6.339170795399696e-06}, {"id": 33, "seek": 13186, "start": 146.66000000000003, "end": 148.66000000000003, "text": " for a particular", "tokens": [337, 257, 1729], "temperature": 0.0, "avg_logprob": -0.2800655565763775, "compression_ratio": 1.7110091743119267, "no_speech_prob": 6.339170795399696e-06}, {"id": 34, "seek": 13186, "start": 149.02, "end": 151.02, "text": " Product so for toilet rolls", "tokens": [22005, 370, 337, 11137, 15767], "temperature": 0.0, "avg_logprob": -0.2800655565763775, "compression_ratio": 1.7110091743119267, "no_speech_prob": 6.339170795399696e-06}, {"id": 35, "seek": 13186, "start": 151.66000000000003, "end": 154.02, "text": " in New Jersey last month", "tokens": [294, 1873, 16601, 1036, 1618], "temperature": 0.0, "avg_logprob": -0.2800655565763775, "compression_ratio": 1.7110091743119267, "no_speech_prob": 6.339170795399696e-06}, {"id": 36, "seek": 13186, "start": 155.06, "end": 161.34, "text": " And then I go and put it into production and it scales great in other words it has a great latency", "tokens": [400, 550, 286, 352, 293, 829, 309, 666, 4265, 293, 309, 17408, 869, 294, 661, 2283, 309, 575, 257, 869, 27043], "temperature": 0.0, "avg_logprob": -0.2800655565763775, "compression_ratio": 1.7110091743119267, "no_speech_prob": 6.339170795399696e-06}, {"id": 37, "seek": 16134, "start": 161.34, "end": 167.0, "text": " I don't have a high CPU load, but it fails to predict anything well other than", "tokens": [286, 500, 380, 362, 257, 1090, 13199, 3677, 11, 457, 309, 18199, 281, 6069, 1340, 731, 661, 813], "temperature": 0.0, "avg_logprob": -0.2206025557084517, "compression_ratio": 1.5646551724137931, "no_speech_prob": 3.5008274608117063e-06}, {"id": 38, "seek": 16134, "start": 167.94, "end": 170.46, "text": " Toilet rolls in New Jersey, and it also turns out", "tokens": [1407, 388, 302, 15767, 294, 1873, 16601, 11, 293, 309, 611, 4523, 484], "temperature": 0.0, "avg_logprob": -0.2206025557084517, "compression_ratio": 1.5646551724137931, "no_speech_prob": 3.5008274608117063e-06}, {"id": 39, "seek": 16134, "start": 170.46, "end": 174.18, "text": " I don't you did it well for last month not the next month, so these are all generalization", "tokens": [286, 500, 380, 291, 630, 309, 731, 337, 1036, 1618, 406, 264, 958, 1618, 11, 370, 613, 366, 439, 2674, 2144], "temperature": 0.0, "avg_logprob": -0.2206025557084517, "compression_ratio": 1.5646551724137931, "no_speech_prob": 3.5008274608117063e-06}, {"id": 40, "seek": 16134, "start": 175.14000000000001, "end": 177.1, "text": " areas", "tokens": [3179], "temperature": 0.0, "avg_logprob": -0.2206025557084517, "compression_ratio": 1.5646551724137931, "no_speech_prob": 3.5008274608117063e-06}, {"id": 41, "seek": 16134, "start": 177.1, "end": 178.54, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.2206025557084517, "compression_ratio": 1.5646551724137931, "no_speech_prob": 3.5008274608117063e-06}, {"id": 42, "seek": 16134, "start": 178.54, "end": 184.54, "text": " The most common way that people check for the ability to generalize is", "tokens": [440, 881, 2689, 636, 300, 561, 1520, 337, 264, 3485, 281, 2674, 1125, 307], "temperature": 0.0, "avg_logprob": -0.2206025557084517, "compression_ratio": 1.5646551724137931, "no_speech_prob": 3.5008274608117063e-06}, {"id": 43, "seek": 16134, "start": 185.22, "end": 190.34, "text": " To create a random sample, so they'll grab a few rows at random", "tokens": [1407, 1884, 257, 4974, 6889, 11, 370, 436, 603, 4444, 257, 1326, 13241, 412, 4974], "temperature": 0.0, "avg_logprob": -0.2206025557084517, "compression_ratio": 1.5646551724137931, "no_speech_prob": 3.5008274608117063e-06}, {"id": 44, "seek": 19034, "start": 190.34, "end": 191.70000000000002, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.26190699063814604, "compression_ratio": 1.7818181818181817, "no_speech_prob": 4.092883045814233e-06}, {"id": 45, "seek": 19034, "start": 191.70000000000002, "end": 193.70000000000002, "text": " pull it out", "tokens": [2235, 309, 484], "temperature": 0.0, "avg_logprob": -0.26190699063814604, "compression_ratio": 1.7818181818181817, "no_speech_prob": 4.092883045814233e-06}, {"id": 46, "seek": 19034, "start": 194.5, "end": 196.66, "text": " Into a test set and", "tokens": [23373, 257, 1500, 992, 293], "temperature": 0.0, "avg_logprob": -0.26190699063814604, "compression_ratio": 1.7818181818181817, "no_speech_prob": 4.092883045814233e-06}, {"id": 47, "seek": 19034, "start": 198.06, "end": 204.34, "text": " Then they'll build all of their models on the rest of the rows and then when they're finished", "tokens": [1396, 436, 603, 1322, 439, 295, 641, 5245, 322, 264, 1472, 295, 264, 13241, 293, 550, 562, 436, 434, 4335], "temperature": 0.0, "avg_logprob": -0.26190699063814604, "compression_ratio": 1.7818181818181817, "no_speech_prob": 4.092883045814233e-06}, {"id": 48, "seek": 19034, "start": 204.74, "end": 211.3, "text": " They'll check the the accuracy they got on there, so the rest of the rows are called the training set everything else", "tokens": [814, 603, 1520, 264, 264, 14170, 436, 658, 322, 456, 11, 370, 264, 1472, 295, 264, 13241, 366, 1219, 264, 3097, 992, 1203, 1646], "temperature": 0.0, "avg_logprob": -0.26190699063814604, "compression_ratio": 1.7818181818181817, "no_speech_prob": 4.092883045814233e-06}, {"id": 49, "seek": 19034, "start": 212.5, "end": 214.5, "text": " everything", "tokens": [1203], "temperature": 0.0, "avg_logprob": -0.26190699063814604, "compression_ratio": 1.7818181818181817, "no_speech_prob": 4.092883045814233e-06}, {"id": 50, "seek": 19034, "start": 214.5, "end": 216.5, "text": " Else", "tokens": [45472], "temperature": 0.0, "avg_logprob": -0.26190699063814604, "compression_ratio": 1.7818181818181817, "no_speech_prob": 4.092883045814233e-06}, {"id": 51, "seek": 21650, "start": 216.5, "end": 220.5, "text": " We could call the training set and", "tokens": [492, 727, 818, 264, 3097, 992, 293], "temperature": 0.0, "avg_logprob": -0.19760178013851767, "compression_ratio": 1.6375545851528384, "no_speech_prob": 5.594308731815545e-06}, {"id": 52, "seek": 21650, "start": 221.74, "end": 226.18, "text": " So at the end of their modeling process on the training set they got an accuracy of", "tokens": [407, 412, 264, 917, 295, 641, 15983, 1399, 322, 264, 3097, 992, 436, 658, 364, 14170, 295], "temperature": 0.0, "avg_logprob": -0.19760178013851767, "compression_ratio": 1.6375545851528384, "no_speech_prob": 5.594308731815545e-06}, {"id": 53, "seek": 21650, "start": 226.74, "end": 233.06, "text": " 99% you're predicting cats from dogs at the very end they check it against a test set to make sure that the model really does", "tokens": [11803, 4, 291, 434, 32884, 11111, 490, 7197, 412, 264, 588, 917, 436, 1520, 309, 1970, 257, 1500, 992, 281, 652, 988, 300, 264, 2316, 534, 775], "temperature": 0.0, "avg_logprob": -0.19760178013851767, "compression_ratio": 1.6375545851528384, "no_speech_prob": 5.594308731815545e-06}, {"id": 54, "seek": 21650, "start": 233.06, "end": 234.18, "text": " generalize", "tokens": [2674, 1125], "temperature": 0.0, "avg_logprob": -0.19760178013851767, "compression_ratio": 1.6375545851528384, "no_speech_prob": 5.594308731815545e-06}, {"id": 55, "seek": 21650, "start": 234.18, "end": 236.18, "text": " now the problem is", "tokens": [586, 264, 1154, 307], "temperature": 0.0, "avg_logprob": -0.19760178013851767, "compression_ratio": 1.6375545851528384, "no_speech_prob": 5.594308731815545e-06}, {"id": 56, "seek": 21650, "start": 236.26, "end": 238.22, "text": " What if it doesn't?", "tokens": [708, 498, 309, 1177, 380, 30], "temperature": 0.0, "avg_logprob": -0.19760178013851767, "compression_ratio": 1.6375545851528384, "no_speech_prob": 5.594308731815545e-06}, {"id": 57, "seek": 21650, "start": 238.22, "end": 243.34, "text": " Right so okay well I could go back and change some hyper parameters do some data", "tokens": [1779, 370, 1392, 731, 286, 727, 352, 646, 293, 1319, 512, 9848, 9834, 360, 512, 1412], "temperature": 0.0, "avg_logprob": -0.19760178013851767, "compression_ratio": 1.6375545851528384, "no_speech_prob": 5.594308731815545e-06}, {"id": 58, "seek": 24334, "start": 243.34, "end": 248.18, "text": " Augmentation and whatever else try to create a more generalizable model, and then I'll go back again", "tokens": [6088, 19631, 293, 2035, 1646, 853, 281, 1884, 257, 544, 2674, 22395, 2316, 11, 293, 550, 286, 603, 352, 646, 797], "temperature": 0.0, "avg_logprob": -0.20234975126600757, "compression_ratio": 1.704, "no_speech_prob": 2.26031670536031e-06}, {"id": 59, "seek": 24334, "start": 248.86, "end": 251.94, "text": " After doing all that and check and it's still no good", "tokens": [2381, 884, 439, 300, 293, 1520, 293, 309, 311, 920, 572, 665], "temperature": 0.0, "avg_logprob": -0.20234975126600757, "compression_ratio": 1.704, "no_speech_prob": 2.26031670536031e-06}, {"id": 60, "seek": 24334, "start": 252.42000000000002, "end": 259.46, "text": " But and I'll keep doing this again and again until eventually after 50 attempts it does generalize", "tokens": [583, 293, 286, 603, 1066, 884, 341, 797, 293, 797, 1826, 4728, 934, 2625, 15257, 309, 775, 2674, 1125], "temperature": 0.0, "avg_logprob": -0.20234975126600757, "compression_ratio": 1.704, "no_speech_prob": 2.26031670536031e-06}, {"id": 61, "seek": 24334, "start": 260.34000000000003, "end": 267.24, "text": " But does it really generalize because maybe all I've done is accidentally found this one which happens to work just for that test set because", "tokens": [583, 775, 309, 534, 2674, 1125, 570, 1310, 439, 286, 600, 1096, 307, 15715, 1352, 341, 472, 597, 2314, 281, 589, 445, 337, 300, 1500, 992, 570], "temperature": 0.0, "avg_logprob": -0.20234975126600757, "compression_ratio": 1.704, "no_speech_prob": 2.26031670536031e-06}, {"id": 62, "seek": 24334, "start": 267.24, "end": 268.94, "text": " I've tried 50 different things", "tokens": [286, 600, 3031, 2625, 819, 721], "temperature": 0.0, "avg_logprob": -0.20234975126600757, "compression_ratio": 1.704, "no_speech_prob": 2.26031670536031e-06}, {"id": 63, "seek": 26894, "start": 268.94, "end": 274.18, "text": " Right and so if I've got something which is like right coincidentally", "tokens": [1779, 293, 370, 498, 286, 600, 658, 746, 597, 307, 411, 558, 13001, 36578], "temperature": 0.0, "avg_logprob": -0.2093320497324769, "compression_ratio": 1.547486033519553, "no_speech_prob": 1.6797265516288462e-06}, {"id": 64, "seek": 26894, "start": 275.3, "end": 280.3, "text": " 0.05 5% of the time then a very likely to accidentally get a good result", "tokens": [1958, 13, 13328, 1025, 4, 295, 264, 565, 550, 257, 588, 3700, 281, 15715, 483, 257, 665, 1874], "temperature": 0.0, "avg_logprob": -0.2093320497324769, "compression_ratio": 1.547486033519553, "no_speech_prob": 1.6797265516288462e-06}, {"id": 65, "seek": 26894, "start": 281.34, "end": 286.18, "text": " So what we generally do is we put aside a second data set", "tokens": [407, 437, 321, 5101, 360, 307, 321, 829, 7359, 257, 1150, 1412, 992], "temperature": 0.0, "avg_logprob": -0.2093320497324769, "compression_ratio": 1.547486033519553, "no_speech_prob": 1.6797265516288462e-06}, {"id": 66, "seek": 26894, "start": 288.78, "end": 294.5, "text": " They've got a couple more of these and put these aside into a validation set", "tokens": [814, 600, 658, 257, 1916, 544, 295, 613, 293, 829, 613, 7359, 666, 257, 24071, 992], "temperature": 0.0, "avg_logprob": -0.2093320497324769, "compression_ratio": 1.547486033519553, "no_speech_prob": 1.6797265516288462e-06}, {"id": 67, "seek": 29450, "start": 294.5, "end": 299.54, "text": " validation set right and then everything that's not in the", "tokens": [24071, 992, 558, 293, 550, 1203, 300, 311, 406, 294, 264], "temperature": 0.0, "avg_logprob": -0.1889310081799825, "compression_ratio": 1.8288288288288288, "no_speech_prob": 1.3081698853056878e-06}, {"id": 68, "seek": 29450, "start": 299.58, "end": 306.24, "text": " Validation of test is now training and so what we do is we train a model check it against the validation to see if it generalizes", "tokens": [7188, 327, 399, 295, 1500, 307, 586, 3097, 293, 370, 437, 321, 360, 307, 321, 3847, 257, 2316, 1520, 309, 1970, 264, 24071, 281, 536, 498, 309, 2674, 5660], "temperature": 0.0, "avg_logprob": -0.1889310081799825, "compression_ratio": 1.8288288288288288, "no_speech_prob": 1.3081698853056878e-06}, {"id": 69, "seek": 29450, "start": 306.98, "end": 311.18, "text": " Do that a few times and then when we finally got something we were like okay?", "tokens": [1144, 300, 257, 1326, 1413, 293, 550, 562, 321, 2721, 658, 746, 321, 645, 411, 1392, 30], "temperature": 0.0, "avg_logprob": -0.1889310081799825, "compression_ratio": 1.8288288288288288, "no_speech_prob": 1.3081698853056878e-06}, {"id": 70, "seek": 29450, "start": 311.18, "end": 316.98, "text": " We think this generalizes successfully based on the validation set and then at the end of the project we check it against the test set", "tokens": [492, 519, 341, 2674, 5660, 10727, 2361, 322, 264, 24071, 992, 293, 550, 412, 264, 917, 295, 264, 1716, 321, 1520, 309, 1970, 264, 1500, 992], "temperature": 0.0, "avg_logprob": -0.1889310081799825, "compression_ratio": 1.8288288288288288, "no_speech_prob": 1.3081698853056878e-06}, {"id": 71, "seek": 29450, "start": 317.98, "end": 319.62, "text": " Yeah", "tokens": [865], "temperature": 0.0, "avg_logprob": -0.1889310081799825, "compression_ratio": 1.8288288288288288, "no_speech_prob": 1.3081698853056878e-06}, {"id": 72, "seek": 31962, "start": 319.62, "end": 326.74, "text": " So basically by making this two layer test that validation set if it gets one right the other one wrong you kind of double checking your", "tokens": [407, 1936, 538, 1455, 341, 732, 4583, 1500, 300, 24071, 992, 498, 309, 2170, 472, 558, 264, 661, 472, 2085, 291, 733, 295, 3834, 8568, 428], "temperature": 0.0, "avg_logprob": -0.17540686471121653, "compression_ratio": 1.8383458646616542, "no_speech_prob": 4.936951881973073e-06}, {"id": 73, "seek": 31962, "start": 326.74, "end": 333.86, "text": " Errors kind of like it's checking that we haven't over fit to the validation set so if we're using the validation set again and again", "tokens": [3300, 9734, 733, 295, 411, 309, 311, 8568, 300, 321, 2378, 380, 670, 3318, 281, 264, 24071, 992, 370, 498, 321, 434, 1228, 264, 24071, 992, 797, 293, 797], "temperature": 0.0, "avg_logprob": -0.17540686471121653, "compression_ratio": 1.8383458646616542, "no_speech_prob": 4.936951881973073e-06}, {"id": 74, "seek": 31962, "start": 334.86, "end": 338.66, "text": " Then we could end up not coming up with a generalizable set of hyper parameters", "tokens": [1396, 321, 727, 917, 493, 406, 1348, 493, 365, 257, 2674, 22395, 992, 295, 9848, 9834], "temperature": 0.0, "avg_logprob": -0.17540686471121653, "compression_ratio": 1.8383458646616542, "no_speech_prob": 4.936951881973073e-06}, {"id": 75, "seek": 31962, "start": 338.66, "end": 343.2, "text": " But a set of hyper parameters that just so happened to work on the training set and the validation set", "tokens": [583, 257, 992, 295, 9848, 9834, 300, 445, 370, 2011, 281, 589, 322, 264, 3097, 992, 293, 264, 24071, 992], "temperature": 0.0, "avg_logprob": -0.17540686471121653, "compression_ratio": 1.8383458646616542, "no_speech_prob": 4.936951881973073e-06}, {"id": 76, "seek": 31962, "start": 344.06, "end": 345.5, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.17540686471121653, "compression_ratio": 1.8383458646616542, "no_speech_prob": 4.936951881973073e-06}, {"id": 77, "seek": 34550, "start": 345.5, "end": 349.74, "text": " So if we try 50 different models against", "tokens": [407, 498, 321, 853, 2625, 819, 5245, 1970], "temperature": 0.0, "avg_logprob": -0.16565295869270258, "compression_ratio": 1.8207171314741035, "no_speech_prob": 1.1365599448254216e-06}, {"id": 78, "seek": 34550, "start": 351.66, "end": 357.98, "text": " The validation set and then at the end of all that we then check that against the test set and it still generalizes", "tokens": [440, 24071, 992, 293, 550, 412, 264, 917, 295, 439, 300, 321, 550, 1520, 300, 1970, 264, 1500, 992, 293, 309, 920, 2674, 5660], "temperature": 0.0, "avg_logprob": -0.16565295869270258, "compression_ratio": 1.8207171314741035, "no_speech_prob": 1.1365599448254216e-06}, {"id": 79, "seek": 34550, "start": 357.98, "end": 360.26, "text": " Well, then we're kind of going to say okay. That's good", "tokens": [1042, 11, 550, 321, 434, 733, 295, 516, 281, 584, 1392, 13, 663, 311, 665], "temperature": 0.0, "avg_logprob": -0.16565295869270258, "compression_ratio": 1.8207171314741035, "no_speech_prob": 1.1365599448254216e-06}, {"id": 80, "seek": 34550, "start": 360.26, "end": 364.44, "text": " We've actually come up with generalizable model if it doesn't then that's going to say okay", "tokens": [492, 600, 767, 808, 493, 365, 2674, 22395, 2316, 498, 309, 1177, 380, 550, 300, 311, 516, 281, 584, 1392], "temperature": 0.0, "avg_logprob": -0.16565295869270258, "compression_ratio": 1.8207171314741035, "no_speech_prob": 1.1365599448254216e-06}, {"id": 81, "seek": 34550, "start": 364.44, "end": 370.0, "text": " We've actually now over fit to the validation set at which point you're kind of in trouble right because", "tokens": [492, 600, 767, 586, 670, 3318, 281, 264, 24071, 992, 412, 597, 935, 291, 434, 733, 295, 294, 5253, 558, 570], "temperature": 0.0, "avg_logprob": -0.16565295869270258, "compression_ratio": 1.8207171314741035, "no_speech_prob": 1.1365599448254216e-06}, {"id": 82, "seek": 34550, "start": 370.7, "end": 373.14, "text": " You don't you know you don't have anything left", "tokens": [509, 500, 380, 291, 458, 291, 500, 380, 362, 1340, 1411], "temperature": 0.0, "avg_logprob": -0.16565295869270258, "compression_ratio": 1.8207171314741035, "no_speech_prob": 1.1365599448254216e-06}, {"id": 83, "seek": 37314, "start": 373.14, "end": 377.94, "text": " Behind right so the idea is to use effective", "tokens": [20475, 558, 370, 264, 1558, 307, 281, 764, 4942], "temperature": 0.0, "avg_logprob": -0.1601334956654331, "compression_ratio": 1.7813620071684588, "no_speech_prob": 2.1907687823841115e-06}, {"id": 84, "seek": 37314, "start": 378.62, "end": 384.02, "text": " Techniques during the modeling so that so that doesn't happen right, but but if it's going to happen you want to find out", "tokens": [8337, 4911, 1830, 264, 15983, 370, 300, 370, 300, 1177, 380, 1051, 558, 11, 457, 457, 498, 309, 311, 516, 281, 1051, 291, 528, 281, 915, 484], "temperature": 0.0, "avg_logprob": -0.1601334956654331, "compression_ratio": 1.7813620071684588, "no_speech_prob": 2.1907687823841115e-06}, {"id": 85, "seek": 37314, "start": 384.02, "end": 388.78, "text": " about it like you need that test set to be there because otherwise when you put it in production and", "tokens": [466, 309, 411, 291, 643, 300, 1500, 992, 281, 312, 456, 570, 5911, 562, 291, 829, 309, 294, 4265, 293], "temperature": 0.0, "avg_logprob": -0.1601334956654331, "compression_ratio": 1.7813620071684588, "no_speech_prob": 2.1907687823841115e-06}, {"id": 86, "seek": 37314, "start": 389.78, "end": 394.41999999999996, "text": " Then it turns out that it doesn't generalize that would be a really bad outcome right you end up with", "tokens": [1396, 309, 4523, 484, 300, 309, 1177, 380, 2674, 1125, 300, 576, 312, 257, 534, 1578, 9700, 558, 291, 917, 493, 365], "temperature": 0.0, "avg_logprob": -0.1601334956654331, "compression_ratio": 1.7813620071684588, "no_speech_prob": 2.1907687823841115e-06}, {"id": 87, "seek": 37314, "start": 394.94, "end": 398.38, "text": " less people clicking on your ads or selling less of your products or", "tokens": [1570, 561, 9697, 322, 428, 10342, 420, 6511, 1570, 295, 428, 3383, 420], "temperature": 0.0, "avg_logprob": -0.1601334956654331, "compression_ratio": 1.7813620071684588, "no_speech_prob": 2.1907687823841115e-06}, {"id": 88, "seek": 39838, "start": 398.38, "end": 402.5, "text": " Providing car insurance to very risky vehicles or whatever", "tokens": [15685, 2819, 1032, 7214, 281, 588, 21137, 8948, 420, 2035], "temperature": 0.0, "avg_logprob": -0.17419053873884568, "compression_ratio": 1.6615384615384616, "no_speech_prob": 8.93959713721415e-06}, {"id": 89, "seek": 39838, "start": 404.02, "end": 408.98, "text": " Just to make sure do you need to ever check if the validation set and the test set is", "tokens": [1449, 281, 652, 988, 360, 291, 643, 281, 1562, 1520, 498, 264, 24071, 992, 293, 264, 1500, 992, 307], "temperature": 0.0, "avg_logprob": -0.17419053873884568, "compression_ratio": 1.6615384615384616, "no_speech_prob": 8.93959713721415e-06}, {"id": 90, "seek": 39838, "start": 409.65999999999997, "end": 412.14, "text": " Coherent or you just keep test set", "tokens": [3066, 511, 317, 420, 291, 445, 1066, 1500, 992], "temperature": 0.0, "avg_logprob": -0.17419053873884568, "compression_ratio": 1.6615384615384616, "no_speech_prob": 8.93959713721415e-06}, {"id": 91, "seek": 39838, "start": 412.65999999999997, "end": 416.46, "text": " So if you've done what I've just done here, which is to randomly sample", "tokens": [407, 498, 291, 600, 1096, 437, 286, 600, 445, 1096, 510, 11, 597, 307, 281, 16979, 6889], "temperature": 0.0, "avg_logprob": -0.17419053873884568, "compression_ratio": 1.6615384615384616, "no_speech_prob": 8.93959713721415e-06}, {"id": 92, "seek": 39838, "start": 416.46, "end": 420.58, "text": " There's no particular reason to check as long as they're as long as they're big enough, right?", "tokens": [821, 311, 572, 1729, 1778, 281, 1520, 382, 938, 382, 436, 434, 382, 938, 382, 436, 434, 955, 1547, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.17419053873884568, "compression_ratio": 1.6615384615384616, "no_speech_prob": 8.93959713721415e-06}, {"id": 93, "seek": 39838, "start": 420.58, "end": 424.65999999999997, "text": " But we're going to come back to your question in a different context in just a moment", "tokens": [583, 321, 434, 516, 281, 808, 646, 281, 428, 1168, 294, 257, 819, 4319, 294, 445, 257, 1623], "temperature": 0.0, "avg_logprob": -0.17419053873884568, "compression_ratio": 1.6615384615384616, "no_speech_prob": 8.93959713721415e-06}, {"id": 94, "seek": 42466, "start": 424.66, "end": 426.66, "text": " Now", "tokens": [823], "temperature": 0.0, "avg_logprob": -0.20468280933521413, "compression_ratio": 1.6010362694300517, "no_speech_prob": 2.4824646516208304e-06}, {"id": 95, "seek": 42466, "start": 430.86, "end": 433.98, "text": " Another trick we've learned for random forests is a way of", "tokens": [3996, 4282, 321, 600, 3264, 337, 4974, 21700, 307, 257, 636, 295], "temperature": 0.0, "avg_logprob": -0.20468280933521413, "compression_ratio": 1.6010362694300517, "no_speech_prob": 2.4824646516208304e-06}, {"id": 96, "seek": 42466, "start": 435.02000000000004, "end": 442.06, "text": " Not needing a validation set and the way that we learned was to use instead use the OOB", "tokens": [1726, 18006, 257, 24071, 992, 293, 264, 636, 300, 321, 3264, 390, 281, 764, 2602, 764, 264, 422, 46, 33], "temperature": 0.0, "avg_logprob": -0.20468280933521413, "compression_ratio": 1.6010362694300517, "no_speech_prob": 2.4824646516208304e-06}, {"id": 97, "seek": 42466, "start": 442.74, "end": 450.94000000000005, "text": " Error or the OOB score and so this idea was to say well every time we train a tree in a random forest", "tokens": [3300, 2874, 420, 264, 422, 46, 33, 6175, 293, 370, 341, 1558, 390, 281, 584, 731, 633, 565, 321, 3847, 257, 4230, 294, 257, 4974, 6719], "temperature": 0.0, "avg_logprob": -0.20468280933521413, "compression_ratio": 1.6010362694300517, "no_speech_prob": 2.4824646516208304e-06}, {"id": 98, "seek": 45094, "start": 450.94, "end": 457.42, "text": " There's a bunch of observations that are held out anyway because that's how we get some of the randomness and so let's", "tokens": [821, 311, 257, 3840, 295, 18163, 300, 366, 5167, 484, 4033, 570, 300, 311, 577, 321, 483, 512, 295, 264, 4974, 1287, 293, 370, 718, 311], "temperature": 0.0, "avg_logprob": -0.20739691598074778, "compression_ratio": 1.6214953271028036, "no_speech_prob": 2.4824657884892076e-06}, {"id": 99, "seek": 45094, "start": 458.21999999999997, "end": 465.42, "text": " Calculate our score for each tree based on those held out samples and therefore the forest by averaging the trees that", "tokens": [3511, 2444, 473, 527, 6175, 337, 1184, 4230, 2361, 322, 729, 5167, 484, 10938, 293, 4412, 264, 6719, 538, 47308, 264, 5852, 300], "temperature": 0.0, "avg_logprob": -0.20739691598074778, "compression_ratio": 1.6214953271028036, "no_speech_prob": 2.4824657884892076e-06}, {"id": 100, "seek": 45094, "start": 466.1, "end": 467.26, "text": " each", "tokens": [1184], "temperature": 0.0, "avg_logprob": -0.20739691598074778, "compression_ratio": 1.6214953271028036, "no_speech_prob": 2.4824657884892076e-06}, {"id": 101, "seek": 45094, "start": 467.26, "end": 469.26, "text": " Row was not part of training", "tokens": [20309, 390, 406, 644, 295, 3097], "temperature": 0.0, "avg_logprob": -0.20739691598074778, "compression_ratio": 1.6214953271028036, "no_speech_prob": 2.4824657884892076e-06}, {"id": 102, "seek": 45094, "start": 469.94, "end": 471.14, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.20739691598074778, "compression_ratio": 1.6214953271028036, "no_speech_prob": 2.4824657884892076e-06}, {"id": 103, "seek": 45094, "start": 471.14, "end": 472.22, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.20739691598074778, "compression_ratio": 1.6214953271028036, "no_speech_prob": 2.4824657884892076e-06}, {"id": 104, "seek": 45094, "start": 472.22, "end": 476.42, "text": " So the OOB score gives us something which is pretty similar", "tokens": [407, 264, 422, 46, 33, 6175, 2709, 505, 746, 597, 307, 1238, 2531], "temperature": 0.0, "avg_logprob": -0.20739691598074778, "compression_ratio": 1.6214953271028036, "no_speech_prob": 2.4824657884892076e-06}, {"id": 105, "seek": 45094, "start": 477.3, "end": 479.3, "text": " to the", "tokens": [281, 264], "temperature": 0.0, "avg_logprob": -0.20739691598074778, "compression_ratio": 1.6214953271028036, "no_speech_prob": 2.4824657884892076e-06}, {"id": 106, "seek": 47930, "start": 479.3, "end": 480.90000000000003, "text": " Validation score", "tokens": [7188, 327, 399, 6175], "temperature": 0.0, "avg_logprob": -0.2818157770862318, "compression_ratio": 1.5730994152046784, "no_speech_prob": 1.8631148122949526e-05}, {"id": 107, "seek": 47930, "start": 480.90000000000003, "end": 484.94, "text": " But on average it's a little less good", "tokens": [583, 322, 4274, 309, 311, 257, 707, 1570, 665], "temperature": 0.0, "avg_logprob": -0.2818157770862318, "compression_ratio": 1.5730994152046784, "no_speech_prob": 1.8631148122949526e-05}, {"id": 108, "seek": 47930, "start": 485.46000000000004, "end": 489.58, "text": " Can anybody either remember or figure out why on average it's a little less good?", "tokens": [1664, 4472, 2139, 1604, 420, 2573, 484, 983, 322, 4274, 309, 311, 257, 707, 1570, 665, 30], "temperature": 0.0, "avg_logprob": -0.2818157770862318, "compression_ratio": 1.5730994152046784, "no_speech_prob": 1.8631148122949526e-05}, {"id": 109, "seek": 47930, "start": 491.82, "end": 493.46000000000004, "text": " Quite a subtle one. Thank you. It's a change", "tokens": [20464, 257, 13743, 472, 13, 1044, 291, 13, 467, 311, 257, 1319], "temperature": 0.0, "avg_logprob": -0.2818157770862318, "compression_ratio": 1.5730994152046784, "no_speech_prob": 1.8631148122949526e-05}, {"id": 110, "seek": 47930, "start": 495.82, "end": 502.7, "text": " I'm not sure but is it because you are treating like you are doing every kind of probe", "tokens": [286, 478, 406, 988, 457, 307, 309, 570, 291, 366, 15083, 411, 291, 366, 884, 633, 733, 295, 22715], "temperature": 0.0, "avg_logprob": -0.2818157770862318, "compression_ratio": 1.5730994152046784, "no_speech_prob": 1.8631148122949526e-05}, {"id": 111, "seek": 50270, "start": 502.7, "end": 510.97999999999996, "text": " Pre-processing on your test and so the OOB score is reflecting the performance on testing set", "tokens": [6001, 12, 41075, 278, 322, 428, 1500, 293, 370, 264, 422, 46, 33, 6175, 307, 23543, 264, 3389, 322, 4997, 992], "temperature": 0.0, "avg_logprob": -0.24008114483891702, "compression_ratio": 1.800995024875622, "no_speech_prob": 8.939370673033409e-06}, {"id": 112, "seek": 50270, "start": 511.42, "end": 514.34, "text": " No, so the OOB score is not using the test set at all", "tokens": [883, 11, 370, 264, 422, 46, 33, 6175, 307, 406, 1228, 264, 1500, 992, 412, 439], "temperature": 0.0, "avg_logprob": -0.24008114483891702, "compression_ratio": 1.800995024875622, "no_speech_prob": 8.939370673033409e-06}, {"id": 113, "seek": 50270, "start": 514.34, "end": 519.34, "text": " The OOB score is using the held out rows in the training set of each tree", "tokens": [440, 422, 46, 33, 6175, 307, 1228, 264, 5167, 484, 13241, 294, 264, 3097, 992, 295, 1184, 4230], "temperature": 0.0, "avg_logprob": -0.24008114483891702, "compression_ratio": 1.800995024875622, "no_speech_prob": 8.939370673033409e-06}, {"id": 114, "seek": 50270, "start": 519.42, "end": 525.56, "text": " So I mean the you are basically testing each tree on some data from the training set. Yes", "tokens": [407, 286, 914, 264, 291, 366, 1936, 4997, 1184, 4230, 322, 512, 1412, 490, 264, 3097, 992, 13, 1079], "temperature": 0.0, "avg_logprob": -0.24008114483891702, "compression_ratio": 1.800995024875622, "no_speech_prob": 8.939370673033409e-06}, {"id": 115, "seek": 50270, "start": 525.56, "end": 528.62, "text": " So you are you have the potential for overfitting?", "tokens": [407, 291, 366, 291, 362, 264, 3995, 337, 670, 69, 2414, 30], "temperature": 0.0, "avg_logprob": -0.24008114483891702, "compression_ratio": 1.800995024875622, "no_speech_prob": 8.939370673033409e-06}, {"id": 116, "seek": 52862, "start": 528.62, "end": 530.62, "text": " I", "tokens": [286], "temperature": 0.0, "avg_logprob": -0.31663380836953925, "compression_ratio": 1.5158371040723981, "no_speech_prob": 7.071794243529439e-06}, {"id": 117, "seek": 52862, "start": 531.94, "end": 535.86, "text": " Shouldn't cause overfitting because each one is looking at a held out", "tokens": [34170, 380, 3082, 670, 69, 2414, 570, 1184, 472, 307, 1237, 412, 257, 5167, 484], "temperature": 0.0, "avg_logprob": -0.31663380836953925, "compression_ratio": 1.5158371040723981, "no_speech_prob": 7.071794243529439e-06}, {"id": 118, "seek": 52862, "start": 536.58, "end": 541.26, "text": " Sample, so it's not an overfitting issue. It's quite a subtle issue Ernest. We'll never try", "tokens": [4832, 781, 11, 370, 309, 311, 406, 364, 670, 69, 2414, 2734, 13, 467, 311, 1596, 257, 13743, 2734, 24147, 377, 13, 492, 603, 1128, 853], "temperature": 0.0, "avg_logprob": -0.31663380836953925, "compression_ratio": 1.5158371040723981, "no_speech_prob": 7.071794243529439e-06}, {"id": 119, "seek": 52862, "start": 544.5, "end": 551.02, "text": " Aren't the samples from OOB bootstrap samples they are so then you're never gonna on average", "tokens": [15464, 380, 264, 10938, 490, 422, 46, 33, 11450, 372, 4007, 10938, 436, 366, 370, 550, 291, 434, 1128, 799, 322, 4274], "temperature": 0.0, "avg_logprob": -0.31663380836953925, "compression_ratio": 1.5158371040723981, "no_speech_prob": 7.071794243529439e-06}, {"id": 120, "seek": 52862, "start": 551.02, "end": 557.22, "text": " They only grab 63% of right chance. I'm average the OOB is 1 minus 63% exactly", "tokens": [814, 787, 4444, 25082, 4, 295, 558, 2931, 13, 286, 478, 4274, 264, 422, 46, 33, 307, 502, 3175, 25082, 4, 2293], "temperature": 0.0, "avg_logprob": -0.31663380836953925, "compression_ratio": 1.5158371040723981, "no_speech_prob": 7.071794243529439e-06}, {"id": 121, "seek": 55722, "start": 557.22, "end": 558.6600000000001, "text": " Yeah, what's the issue?", "tokens": [865, 11, 437, 311, 264, 2734, 30], "temperature": 0.0, "avg_logprob": -0.18757503152751237, "compression_ratio": 1.7156862745098038, "no_speech_prob": 2.561274186518858e-06}, {"id": 122, "seek": 55722, "start": 558.6600000000001, "end": 563.02, "text": " So then if you're not why would the score be lower than the validation score?", "tokens": [407, 550, 498, 291, 434, 406, 983, 576, 264, 6175, 312, 3126, 813, 264, 24071, 6175, 30], "temperature": 0.0, "avg_logprob": -0.18757503152751237, "compression_ratio": 1.7156862745098038, "no_speech_prob": 2.561274186518858e-06}, {"id": 123, "seek": 55722, "start": 563.02, "end": 566.98, "text": " And that implies that you're leaving sort of like a black hole in the data that there's like data points", "tokens": [400, 300, 18779, 300, 291, 434, 5012, 1333, 295, 411, 257, 2211, 5458, 294, 264, 1412, 300, 456, 311, 411, 1412, 2793], "temperature": 0.0, "avg_logprob": -0.18757503152751237, "compression_ratio": 1.7156862745098038, "no_speech_prob": 2.561274186518858e-06}, {"id": 124, "seek": 55722, "start": 566.98, "end": 569.22, "text": " You're never going to sample and they're not gonna be represented by the model", "tokens": [509, 434, 1128, 516, 281, 6889, 293, 436, 434, 406, 799, 312, 10379, 538, 264, 2316], "temperature": 0.0, "avg_logprob": -0.18757503152751237, "compression_ratio": 1.7156862745098038, "no_speech_prob": 2.561274186518858e-06}, {"id": 125, "seek": 55722, "start": 569.22, "end": 573.0400000000001, "text": " No, that's not true though, because each tree is looking at a different set, right?", "tokens": [883, 11, 300, 311, 406, 2074, 1673, 11, 570, 1184, 4230, 307, 1237, 412, 257, 819, 992, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.18757503152751237, "compression_ratio": 1.7156862745098038, "no_speech_prob": 2.561274186518858e-06}, {"id": 126, "seek": 55722, "start": 573.0400000000001, "end": 580.7, "text": " So the OOB so like we've got like I don't know dozens of models right and in each one. There's a different set of", "tokens": [407, 264, 422, 46, 33, 370, 411, 321, 600, 658, 411, 286, 500, 380, 458, 18431, 295, 5245, 558, 293, 294, 1184, 472, 13, 821, 311, 257, 819, 992, 295], "temperature": 0.0, "avg_logprob": -0.18757503152751237, "compression_ratio": 1.7156862745098038, "no_speech_prob": 2.561274186518858e-06}, {"id": 127, "seek": 55722, "start": 581.4200000000001, "end": 583.14, "text": " rows", "tokens": [13241], "temperature": 0.0, "avg_logprob": -0.18757503152751237, "compression_ratio": 1.7156862745098038, "no_speech_prob": 2.561274186518858e-06}, {"id": 128, "seek": 55722, "start": 583.14, "end": 585.14, "text": " Which which happened to be held out?", "tokens": [3013, 597, 2011, 281, 312, 5167, 484, 30], "temperature": 0.0, "avg_logprob": -0.18757503152751237, "compression_ratio": 1.7156862745098038, "no_speech_prob": 2.561274186518858e-06}, {"id": 129, "seek": 58514, "start": 585.14, "end": 587.14, "text": " right", "tokens": [558], "temperature": 0.0, "avg_logprob": -0.1896993319193522, "compression_ratio": 1.8507462686567164, "no_speech_prob": 6.540381491504377e-06}, {"id": 130, "seek": 58514, "start": 587.66, "end": 593.22, "text": " And so when we calculate the OOB score for like let's say row three", "tokens": [400, 370, 562, 321, 8873, 264, 422, 46, 33, 6175, 337, 411, 718, 311, 584, 5386, 1045], "temperature": 0.0, "avg_logprob": -0.1896993319193522, "compression_ratio": 1.8507462686567164, "no_speech_prob": 6.540381491504377e-06}, {"id": 131, "seek": 58514, "start": 593.54, "end": 596.78, "text": " We say okay row three is in this tree this tree", "tokens": [492, 584, 1392, 5386, 1045, 307, 294, 341, 4230, 341, 4230], "temperature": 0.0, "avg_logprob": -0.1896993319193522, "compression_ratio": 1.8507462686567164, "no_speech_prob": 6.540381491504377e-06}, {"id": 132, "seek": 58514, "start": 596.78, "end": 604.8199999999999, "text": " And that's it and so we calculate the prediction on that tree and for that tree and we'd average those two predictions", "tokens": [400, 300, 311, 309, 293, 370, 321, 8873, 264, 17630, 322, 300, 4230, 293, 337, 300, 4230, 293, 321, 1116, 4274, 729, 732, 21264], "temperature": 0.0, "avg_logprob": -0.1896993319193522, "compression_ratio": 1.8507462686567164, "no_speech_prob": 6.540381491504377e-06}, {"id": 133, "seek": 58514, "start": 604.8199999999999, "end": 607.02, "text": " And so with enough trees", "tokens": [400, 370, 365, 1547, 5852], "temperature": 0.0, "avg_logprob": -0.1896993319193522, "compression_ratio": 1.8507462686567164, "no_speech_prob": 6.540381491504377e-06}, {"id": 134, "seek": 58514, "start": 607.6999999999999, "end": 613.9, "text": " You know each one has a 30 or so percent chance sorry 40 or so percent chance that the row is in that tree", "tokens": [509, 458, 1184, 472, 575, 257, 2217, 420, 370, 3043, 2931, 2597, 3356, 420, 370, 3043, 2931, 300, 264, 5386, 307, 294, 300, 4230], "temperature": 0.0, "avg_logprob": -0.1896993319193522, "compression_ratio": 1.8507462686567164, "no_speech_prob": 6.540381491504377e-06}, {"id": 135, "seek": 61390, "start": 613.9, "end": 615.5799999999999, "text": " So if you have 50 trees", "tokens": [407, 498, 291, 362, 2625, 5852], "temperature": 0.0, "avg_logprob": -0.16244883970780807, "compression_ratio": 1.791044776119403, "no_speech_prob": 2.0462506654439494e-05}, {"id": 136, "seek": 61390, "start": 615.5799999999999, "end": 619.3, "text": " It's almost certain that every row is going to be mentioned somewhere", "tokens": [467, 311, 1920, 1629, 300, 633, 5386, 307, 516, 281, 312, 2835, 4079], "temperature": 0.0, "avg_logprob": -0.16244883970780807, "compression_ratio": 1.791044776119403, "no_speech_prob": 2.0462506654439494e-05}, {"id": 137, "seek": 61390, "start": 620.02, "end": 622.02, "text": " Did you have an idea?", "tokens": [2589, 291, 362, 364, 1558, 30], "temperature": 0.0, "avg_logprob": -0.16244883970780807, "compression_ratio": 1.791044776119403, "no_speech_prob": 2.0462506654439494e-05}, {"id": 138, "seek": 61390, "start": 623.62, "end": 629.6999999999999, "text": " With validation set we can use the whole forest to make the predictions, but here we cannot use the whole forest", "tokens": [2022, 24071, 992, 321, 393, 764, 264, 1379, 6719, 281, 652, 264, 21264, 11, 457, 510, 321, 2644, 764, 264, 1379, 6719], "temperature": 0.0, "avg_logprob": -0.16244883970780807, "compression_ratio": 1.791044776119403, "no_speech_prob": 2.0462506654439494e-05}, {"id": 139, "seek": 61390, "start": 629.6999999999999, "end": 633.78, "text": " So we cannot exactly see exactly so every row is", "tokens": [407, 321, 2644, 2293, 536, 2293, 370, 633, 5386, 307], "temperature": 0.0, "avg_logprob": -0.16244883970780807, "compression_ratio": 1.791044776119403, "no_speech_prob": 2.0462506654439494e-05}, {"id": 140, "seek": 61390, "start": 634.5, "end": 639.64, "text": " Going to be using a subset of the trees to make its prediction and with less trees", "tokens": [10963, 281, 312, 1228, 257, 25993, 295, 264, 5852, 281, 652, 1080, 17630, 293, 365, 1570, 5852], "temperature": 0.0, "avg_logprob": -0.16244883970780807, "compression_ratio": 1.791044776119403, "no_speech_prob": 2.0462506654439494e-05}, {"id": 141, "seek": 63964, "start": 639.64, "end": 643.98, "text": " We know we get a less accurate prediction, so that's like that's a subtle one", "tokens": [492, 458, 321, 483, 257, 1570, 8559, 17630, 11, 370, 300, 311, 411, 300, 311, 257, 13743, 472], "temperature": 0.0, "avg_logprob": -0.18956760738206946, "compression_ratio": 1.6309012875536482, "no_speech_prob": 2.406092562523554e-06}, {"id": 142, "seek": 63964, "start": 644.5, "end": 647.6999999999999, "text": " Right and if you didn't get it have a think during the week", "tokens": [1779, 293, 498, 291, 994, 380, 483, 309, 362, 257, 519, 1830, 264, 1243], "temperature": 0.0, "avg_logprob": -0.18956760738206946, "compression_ratio": 1.6309012875536482, "no_speech_prob": 2.406092562523554e-06}, {"id": 143, "seek": 63964, "start": 648.6999999999999, "end": 650.6999999999999, "text": " until you understand", "tokens": [1826, 291, 1223], "temperature": 0.0, "avg_logprob": -0.18956760738206946, "compression_ratio": 1.6309012875536482, "no_speech_prob": 2.406092562523554e-06}, {"id": 144, "seek": 63964, "start": 650.98, "end": 657.22, "text": " Why this is because it's a really interesting test if you're understanding of random forests of like why is", "tokens": [1545, 341, 307, 570, 309, 311, 257, 534, 1880, 1500, 498, 291, 434, 3701, 295, 4974, 21700, 295, 411, 983, 307], "temperature": 0.0, "avg_logprob": -0.18956760738206946, "compression_ratio": 1.6309012875536482, "no_speech_prob": 2.406092562523554e-06}, {"id": 145, "seek": 63964, "start": 657.66, "end": 665.58, "text": " OOB score on average less good than your validation score. They're both using random subs random held out subsets", "tokens": [422, 46, 33, 6175, 322, 4274, 1570, 665, 813, 428, 24071, 6175, 13, 814, 434, 1293, 1228, 4974, 2090, 4974, 5167, 484, 2090, 1385], "temperature": 0.0, "avg_logprob": -0.18956760738206946, "compression_ratio": 1.6309012875536482, "no_speech_prob": 2.406092562523554e-06}, {"id": 146, "seek": 66558, "start": 665.58, "end": 669.6800000000001, "text": " Anyway, it's generally close enough right so", "tokens": [5684, 11, 309, 311, 5101, 1998, 1547, 558, 370], "temperature": 0.0, "avg_logprob": -0.20621512917911305, "compression_ratio": 1.6844660194174756, "no_speech_prob": 1.816216240513313e-06}, {"id": 147, "seek": 66558, "start": 671.0, "end": 676.48, "text": " Why have a validation set at all when you're using random forest?", "tokens": [1545, 362, 257, 24071, 992, 412, 439, 562, 291, 434, 1228, 4974, 6719, 30], "temperature": 0.0, "avg_logprob": -0.20621512917911305, "compression_ratio": 1.6844660194174756, "no_speech_prob": 1.816216240513313e-06}, {"id": 148, "seek": 66558, "start": 679.08, "end": 684.44, "text": " If it's a randomly chosen validation set it's not strictly speaking necessary", "tokens": [759, 309, 311, 257, 16979, 8614, 24071, 992, 309, 311, 406, 20792, 4124, 4818], "temperature": 0.0, "avg_logprob": -0.20621512917911305, "compression_ratio": 1.6844660194174756, "no_speech_prob": 1.816216240513313e-06}, {"id": 149, "seek": 66558, "start": 684.44, "end": 689.6, "text": " But you know you've got like four levels of things to test right so you could like test on the OOB", "tokens": [583, 291, 458, 291, 600, 658, 411, 1451, 4358, 295, 721, 281, 1500, 558, 370, 291, 727, 411, 1500, 322, 264, 422, 46, 33], "temperature": 0.0, "avg_logprob": -0.20621512917911305, "compression_ratio": 1.6844660194174756, "no_speech_prob": 1.816216240513313e-06}, {"id": 150, "seek": 66558, "start": 689.76, "end": 692.6600000000001, "text": " When that's working well you can test on the validation set", "tokens": [1133, 300, 311, 1364, 731, 291, 393, 1500, 322, 264, 24071, 992], "temperature": 0.0, "avg_logprob": -0.20621512917911305, "compression_ratio": 1.6844660194174756, "no_speech_prob": 1.816216240513313e-06}, {"id": 151, "seek": 69266, "start": 692.66, "end": 699.1, "text": " You know when hopefully by the time you check against the test set there's going to be no surprises, so that'd be one good reason", "tokens": [509, 458, 562, 4696, 538, 264, 565, 291, 1520, 1970, 264, 1500, 992, 456, 311, 516, 281, 312, 572, 22655, 11, 370, 300, 1116, 312, 472, 665, 1778], "temperature": 0.0, "avg_logprob": -0.19462997682632938, "compression_ratio": 1.7061611374407584, "no_speech_prob": 4.029435785923852e-06}, {"id": 152, "seek": 69266, "start": 700.14, "end": 707.3, "text": " Then what Kaggle do the way they do this is kind of clever what Kaggle do is they split the test set into two pieces", "tokens": [1396, 437, 48751, 22631, 360, 264, 636, 436, 360, 341, 307, 733, 295, 13494, 437, 48751, 22631, 360, 307, 436, 7472, 264, 1500, 992, 666, 732, 3755], "temperature": 0.0, "avg_logprob": -0.19462997682632938, "compression_ratio": 1.7061611374407584, "no_speech_prob": 4.029435785923852e-06}, {"id": 153, "seek": 69266, "start": 707.3, "end": 709.3, "text": " a public and", "tokens": [257, 1908, 293], "temperature": 0.0, "avg_logprob": -0.19462997682632938, "compression_ratio": 1.7061611374407584, "no_speech_prob": 4.029435785923852e-06}, {"id": 154, "seek": 69266, "start": 709.9, "end": 711.9, "text": " a private and", "tokens": [257, 4551, 293], "temperature": 0.0, "avg_logprob": -0.19462997682632938, "compression_ratio": 1.7061611374407584, "no_speech_prob": 4.029435785923852e-06}, {"id": 155, "seek": 69266, "start": 712.2199999999999, "end": 718.26, "text": " they don't tell you which is rich, so you submit your predictions to Kaggle and then a", "tokens": [436, 500, 380, 980, 291, 597, 307, 4593, 11, 370, 291, 10315, 428, 21264, 281, 48751, 22631, 293, 550, 257], "temperature": 0.0, "avg_logprob": -0.19462997682632938, "compression_ratio": 1.7061611374407584, "no_speech_prob": 4.029435785923852e-06}, {"id": 156, "seek": 71826, "start": 718.26, "end": 723.18, "text": " Random 30% of those are used to tell you the leaderboard score", "tokens": [37603, 2217, 4, 295, 729, 366, 1143, 281, 980, 291, 264, 5263, 3787, 6175], "temperature": 0.0, "avg_logprob": -0.14579106252127833, "compression_ratio": 1.6716981132075472, "no_speech_prob": 5.285504016683262e-07}, {"id": 157, "seek": 71826, "start": 724.62, "end": 731.26, "text": " But then at the end of the competition that gets thrown away, and they use the other 70% to calculate your real score", "tokens": [583, 550, 412, 264, 917, 295, 264, 6211, 300, 2170, 11732, 1314, 11, 293, 436, 764, 264, 661, 5285, 4, 281, 8873, 428, 957, 6175], "temperature": 0.0, "avg_logprob": -0.14579106252127833, "compression_ratio": 1.6716981132075472, "no_speech_prob": 5.285504016683262e-07}, {"id": 158, "seek": 71826, "start": 732.7, "end": 733.98, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.14579106252127833, "compression_ratio": 1.6716981132075472, "no_speech_prob": 5.285504016683262e-07}, {"id": 159, "seek": 71826, "start": 733.98, "end": 736.68, "text": " What that's doing is that you're making sure that you're not like?", "tokens": [708, 300, 311, 884, 307, 300, 291, 434, 1455, 988, 300, 291, 434, 406, 411, 30], "temperature": 0.0, "avg_logprob": -0.14579106252127833, "compression_ratio": 1.6716981132075472, "no_speech_prob": 5.285504016683262e-07}, {"id": 160, "seek": 71826, "start": 736.98, "end": 743.78, "text": " Continually using that feedback from the leaderboard to figure out some set of hyper parameters that happens to do well on the public", "tokens": [14674, 671, 1228, 300, 5824, 490, 264, 5263, 3787, 281, 2573, 484, 512, 992, 295, 9848, 9834, 300, 2314, 281, 360, 731, 322, 264, 1908], "temperature": 0.0, "avg_logprob": -0.14579106252127833, "compression_ratio": 1.6716981132075472, "no_speech_prob": 5.285504016683262e-07}, {"id": 161, "seek": 71826, "start": 744.02, "end": 748.02, "text": " But actually doesn't generalize okay, so it's a great test", "tokens": [583, 767, 1177, 380, 2674, 1125, 1392, 11, 370, 309, 311, 257, 869, 1500], "temperature": 0.0, "avg_logprob": -0.14579106252127833, "compression_ratio": 1.6716981132075472, "no_speech_prob": 5.285504016683262e-07}, {"id": 162, "seek": 74802, "start": 748.02, "end": 751.5, "text": " But this is one of the reasons why it's good practice to use Kaggle", "tokens": [583, 341, 307, 472, 295, 264, 4112, 983, 309, 311, 665, 3124, 281, 764, 48751, 22631], "temperature": 0.0, "avg_logprob": -0.13467524536943012, "compression_ratio": 1.7782101167315174, "no_speech_prob": 7.527946763730142e-06}, {"id": 163, "seek": 74802, "start": 752.06, "end": 755.66, "text": " Because at the end of a competition at some point this will happen to you", "tokens": [1436, 412, 264, 917, 295, 257, 6211, 412, 512, 935, 341, 486, 1051, 281, 291], "temperature": 0.0, "avg_logprob": -0.13467524536943012, "compression_ratio": 1.7782101167315174, "no_speech_prob": 7.527946763730142e-06}, {"id": 164, "seek": 74802, "start": 755.66, "end": 762.02, "text": " And you'll drop a hundred places on the leaderboard the last day of the competition when they use the private test set and say oh", "tokens": [400, 291, 603, 3270, 257, 3262, 3190, 322, 264, 5263, 3787, 264, 1036, 786, 295, 264, 6211, 562, 436, 764, 264, 4551, 1500, 992, 293, 584, 1954], "temperature": 0.0, "avg_logprob": -0.13467524536943012, "compression_ratio": 1.7782101167315174, "no_speech_prob": 7.527946763730142e-06}, {"id": 165, "seek": 74802, "start": 762.9, "end": 765.26, "text": " That's what it feels like to overfit", "tokens": [663, 311, 437, 309, 3417, 411, 281, 670, 6845], "temperature": 0.0, "avg_logprob": -0.13467524536943012, "compression_ratio": 1.7782101167315174, "no_speech_prob": 7.527946763730142e-06}, {"id": 166, "seek": 74802, "start": 765.26, "end": 771.6999999999999, "text": " And it's much better to practice and get that sense there than it is to do it in a company where there's hundreds of millions", "tokens": [400, 309, 311, 709, 1101, 281, 3124, 293, 483, 300, 2020, 456, 813, 309, 307, 281, 360, 309, 294, 257, 2237, 689, 456, 311, 6779, 295, 6803], "temperature": 0.0, "avg_logprob": -0.13467524536943012, "compression_ratio": 1.7782101167315174, "no_speech_prob": 7.527946763730142e-06}, {"id": 167, "seek": 74802, "start": 771.6999999999999, "end": 773.6999999999999, "text": " of dollars on the line", "tokens": [295, 3808, 322, 264, 1622], "temperature": 0.0, "avg_logprob": -0.13467524536943012, "compression_ratio": 1.7782101167315174, "no_speech_prob": 7.527946763730142e-06}, {"id": 168, "seek": 77370, "start": 773.7, "end": 781.22, "text": " Okay, so this is like the easiest possible situation where you're able to use a", "tokens": [1033, 11, 370, 341, 307, 411, 264, 12889, 1944, 2590, 689, 291, 434, 1075, 281, 764, 257], "temperature": 0.0, "avg_logprob": -0.1886487767316293, "compression_ratio": 1.7093023255813953, "no_speech_prob": 3.1875470085651614e-06}, {"id": 169, "seek": 77370, "start": 782.1, "end": 785.3000000000001, "text": " random sample for your validation set", "tokens": [4974, 6889, 337, 428, 24071, 992], "temperature": 0.0, "avg_logprob": -0.1886487767316293, "compression_ratio": 1.7093023255813953, "no_speech_prob": 3.1875470085651614e-06}, {"id": 170, "seek": 77370, "start": 786.94, "end": 791.6400000000001, "text": " Why might I not be able to use a random sample from my validation set?", "tokens": [1545, 1062, 286, 406, 312, 1075, 281, 764, 257, 4974, 6889, 490, 452, 24071, 992, 30], "temperature": 0.0, "avg_logprob": -0.1886487767316293, "compression_ratio": 1.7093023255813953, "no_speech_prob": 3.1875470085651614e-06}, {"id": 171, "seek": 77370, "start": 795.86, "end": 801.26, "text": " In the case of something where we're forecasting we can't randomly sample because we need to maintain the", "tokens": [682, 264, 1389, 295, 746, 689, 321, 434, 44331, 321, 393, 380, 16979, 6889, 570, 321, 643, 281, 6909, 264], "temperature": 0.0, "avg_logprob": -0.1886487767316293, "compression_ratio": 1.7093023255813953, "no_speech_prob": 3.1875470085651614e-06}, {"id": 172, "seek": 80126, "start": 801.26, "end": 803.26, "text": " temporal ordering", "tokens": [30881, 21739], "temperature": 0.0, "avg_logprob": -0.17684967177254812, "compression_ratio": 1.7773109243697478, "no_speech_prob": 1.0451443813508376e-05}, {"id": 173, "seek": 80126, "start": 803.54, "end": 805.38, "text": " Go on why is that?", "tokens": [1037, 322, 983, 307, 300, 30], "temperature": 0.0, "avg_logprob": -0.17684967177254812, "compression_ratio": 1.7773109243697478, "no_speech_prob": 1.0451443813508376e-05}, {"id": 174, "seek": 80126, "start": 805.38, "end": 809.3, "text": " Because it doesn't it doesn't make sense so in the case of like an ARMA model", "tokens": [1436, 309, 1177, 380, 309, 1177, 380, 652, 2020, 370, 294, 264, 1389, 295, 411, 364, 8943, 9998, 2316], "temperature": 0.0, "avg_logprob": -0.17684967177254812, "compression_ratio": 1.7773109243697478, "no_speech_prob": 1.0451443813508376e-05}, {"id": 175, "seek": 80126, "start": 809.3, "end": 814.1, "text": " I can't use like I can't pull out random rows because there's", "tokens": [286, 393, 380, 764, 411, 286, 393, 380, 2235, 484, 4974, 13241, 570, 456, 311], "temperature": 0.0, "avg_logprob": -0.17684967177254812, "compression_ratio": 1.7773109243697478, "no_speech_prob": 1.0451443813508376e-05}, {"id": 176, "seek": 80126, "start": 815.18, "end": 817.98, "text": " I'm thinking that there's like a certain dependency or I'm", "tokens": [286, 478, 1953, 300, 456, 311, 411, 257, 1629, 33621, 420, 286, 478], "temperature": 0.0, "avg_logprob": -0.17684967177254812, "compression_ratio": 1.7773109243697478, "no_speech_prob": 1.0451443813508376e-05}, {"id": 177, "seek": 80126, "start": 818.18, "end": 822.64, "text": " I'm trying to model a certain dependency that relies on like a specific lag term", "tokens": [286, 478, 1382, 281, 2316, 257, 1629, 33621, 300, 30910, 322, 411, 257, 2685, 8953, 1433], "temperature": 0.0, "avg_logprob": -0.17684967177254812, "compression_ratio": 1.7773109243697478, "no_speech_prob": 1.0451443813508376e-05}, {"id": 178, "seek": 80126, "start": 822.8199999999999, "end": 828.9399999999999, "text": " Now if I randomly sample those things then that lag term isn't there for me to okay, so it could be like a", "tokens": [823, 498, 286, 16979, 6889, 729, 721, 550, 300, 8953, 1433, 1943, 380, 456, 337, 385, 281, 1392, 11, 370, 309, 727, 312, 411, 257], "temperature": 0.0, "avg_logprob": -0.17684967177254812, "compression_ratio": 1.7773109243697478, "no_speech_prob": 1.0451443813508376e-05}, {"id": 179, "seek": 82894, "start": 828.94, "end": 830.4200000000001, "text": " a", "tokens": [257], "temperature": 0.0, "avg_logprob": -0.17802158154939351, "compression_ratio": 1.7014925373134329, "no_speech_prob": 2.813005949064973e-06}, {"id": 180, "seek": 82894, "start": 830.4200000000001, "end": 831.46, "text": " technical", "tokens": [6191], "temperature": 0.0, "avg_logprob": -0.17802158154939351, "compression_ratio": 1.7014925373134329, "no_speech_prob": 2.813005949064973e-06}, {"id": 181, "seek": 82894, "start": 831.46, "end": 835.1800000000001, "text": " modeling issue that like I'm using a model that relies on like", "tokens": [15983, 2734, 300, 411, 286, 478, 1228, 257, 2316, 300, 30910, 322, 411], "temperature": 0.0, "avg_logprob": -0.17802158154939351, "compression_ratio": 1.7014925373134329, "no_speech_prob": 2.813005949064973e-06}, {"id": 182, "seek": 82894, "start": 835.9000000000001, "end": 839.7, "text": " Yesterday the day before and the day before that and if I've randomly removed some things", "tokens": [19765, 264, 786, 949, 293, 264, 786, 949, 300, 293, 498, 286, 600, 16979, 7261, 512, 721], "temperature": 0.0, "avg_logprob": -0.17802158154939351, "compression_ratio": 1.7014925373134329, "no_speech_prob": 2.813005949064973e-06}, {"id": 183, "seek": 82894, "start": 839.7, "end": 846.58, "text": " I don't have yesterday and my model might just fail okay. That's true, but there's a more fundamental issue", "tokens": [286, 500, 380, 362, 5186, 293, 452, 2316, 1062, 445, 3061, 1392, 13, 663, 311, 2074, 11, 457, 456, 311, 257, 544, 8088, 2734], "temperature": 0.0, "avg_logprob": -0.17802158154939351, "compression_ratio": 1.7014925373134329, "no_speech_prob": 2.813005949064973e-06}, {"id": 184, "seek": 82894, "start": 846.58, "end": 848.58, "text": " You want to pass it to Tyler?", "tokens": [509, 528, 281, 1320, 309, 281, 16869, 30], "temperature": 0.0, "avg_logprob": -0.17802158154939351, "compression_ratio": 1.7014925373134329, "no_speech_prob": 2.813005949064973e-06}, {"id": 185, "seek": 82894, "start": 849.1800000000001, "end": 851.0200000000001, "text": " It's a really good point", "tokens": [467, 311, 257, 534, 665, 935], "temperature": 0.0, "avg_logprob": -0.17802158154939351, "compression_ratio": 1.7014925373134329, "no_speech_prob": 2.813005949064973e-06}, {"id": 186, "seek": 82894, "start": 851.0200000000001, "end": 856.0200000000001, "text": " Although you know in general we're going to try to build models that are not that are more resilient than that", "tokens": [5780, 291, 458, 294, 2674, 321, 434, 516, 281, 853, 281, 1322, 5245, 300, 366, 406, 300, 366, 544, 23699, 813, 300], "temperature": 0.0, "avg_logprob": -0.17802158154939351, "compression_ratio": 1.7014925373134329, "no_speech_prob": 2.813005949064973e-06}, {"id": 187, "seek": 82894, "start": 857.0200000000001, "end": 858.86, "text": " particularly with", "tokens": [4098, 365], "temperature": 0.0, "avg_logprob": -0.17802158154939351, "compression_ratio": 1.7014925373134329, "no_speech_prob": 2.813005949064973e-06}, {"id": 188, "seek": 85886, "start": 858.86, "end": 859.82, "text": " Yeah", "tokens": [865], "temperature": 0.0, "avg_logprob": -0.20114232113486843, "compression_ratio": 1.7546296296296295, "no_speech_prob": 1.9637843706732383e-06}, {"id": 189, "seek": 85886, "start": 859.82, "end": 866.7, "text": " Temporal order we expect things that are close by in time to be related to things close to them", "tokens": [8095, 2816, 304, 1668, 321, 2066, 721, 300, 366, 1998, 538, 294, 565, 281, 312, 4077, 281, 721, 1998, 281, 552], "temperature": 0.0, "avg_logprob": -0.20114232113486843, "compression_ratio": 1.7546296296296295, "no_speech_prob": 1.9637843706732383e-06}, {"id": 190, "seek": 85886, "start": 866.78, "end": 868.78, "text": " so we so", "tokens": [370, 321, 370], "temperature": 0.0, "avg_logprob": -0.20114232113486843, "compression_ratio": 1.7546296296296295, "no_speech_prob": 1.9637843706732383e-06}, {"id": 191, "seek": 85886, "start": 869.66, "end": 875.3000000000001, "text": " if we destroy the order like if if we destroy the order we", "tokens": [498, 321, 5293, 264, 1668, 411, 498, 498, 321, 5293, 264, 1668, 321], "temperature": 0.0, "avg_logprob": -0.20114232113486843, "compression_ratio": 1.7546296296296295, "no_speech_prob": 1.9637843706732383e-06}, {"id": 192, "seek": 85886, "start": 875.9, "end": 880.22, "text": " Really aren't going to be able to use that this time is close to this other time", "tokens": [4083, 3212, 380, 516, 281, 312, 1075, 281, 764, 300, 341, 565, 307, 1998, 281, 341, 661, 565], "temperature": 0.0, "avg_logprob": -0.20114232113486843, "compression_ratio": 1.7546296296296295, "no_speech_prob": 1.9637843706732383e-06}, {"id": 193, "seek": 88022, "start": 880.22, "end": 889.02, "text": " Um I don't think that's true because can pull out a random sample for a validation set and still keep everything nicely ordered well", "tokens": [3301, 286, 500, 380, 519, 300, 311, 2074, 570, 393, 2235, 484, 257, 4974, 6889, 337, 257, 24071, 992, 293, 920, 1066, 1203, 9594, 8866, 731], "temperature": 0.0, "avg_logprob": -0.2150183272087711, "compression_ratio": 1.5585585585585586, "no_speech_prob": 1.7061760217984556e-06}, {"id": 194, "seek": 88022, "start": 889.02, "end": 891.46, "text": " We would like to predict things in the future", "tokens": [492, 576, 411, 281, 6069, 721, 294, 264, 2027], "temperature": 0.0, "avg_logprob": -0.2150183272087711, "compression_ratio": 1.5585585585585586, "no_speech_prob": 1.7061760217984556e-06}, {"id": 195, "seek": 88022, "start": 892.4200000000001, "end": 894.4200000000001, "text": " Which we would require", "tokens": [3013, 321, 576, 3651], "temperature": 0.0, "avg_logprob": -0.2150183272087711, "compression_ratio": 1.5585585585585586, "no_speech_prob": 1.7061760217984556e-06}, {"id": 196, "seek": 88022, "start": 895.22, "end": 896.86, "text": " as much data", "tokens": [382, 709, 1412], "temperature": 0.0, "avg_logprob": -0.2150183272087711, "compression_ratio": 1.5585585585585586, "no_speech_prob": 1.7061760217984556e-06}, {"id": 197, "seek": 88022, "start": 896.86, "end": 900.0600000000001, "text": " Close to the end of our okay. That's true", "tokens": [16346, 281, 264, 917, 295, 527, 1392, 13, 663, 311, 2074], "temperature": 0.0, "avg_logprob": -0.2150183272087711, "compression_ratio": 1.5585585585585586, "no_speech_prob": 1.7061760217984556e-06}, {"id": 198, "seek": 88022, "start": 900.0600000000001, "end": 905.58, "text": " I mean we could be like limiting the amount of data that we have by taking some of it out", "tokens": [286, 914, 321, 727, 312, 411, 22083, 264, 2372, 295, 1412, 300, 321, 362, 538, 1940, 512, 295, 309, 484], "temperature": 0.0, "avg_logprob": -0.2150183272087711, "compression_ratio": 1.5585585585585586, "no_speech_prob": 1.7061760217984556e-06}, {"id": 199, "seek": 90558, "start": 905.58, "end": 911.58, "text": " But my claim is stronger my claim is that by using a random validation set", "tokens": [583, 452, 3932, 307, 7249, 452, 3932, 307, 300, 538, 1228, 257, 4974, 24071, 992], "temperature": 0.0, "avg_logprob": -0.18331168533919692, "compression_ratio": 1.6185567010309279, "no_speech_prob": 7.646453013876453e-06}, {"id": 200, "seek": 90558, "start": 912.3000000000001, "end": 917.3000000000001, "text": " We could get totally the wrong idea about our model Karen. Do you want to have a try?", "tokens": [492, 727, 483, 3879, 264, 2085, 1558, 466, 527, 2316, 14834, 13, 1144, 291, 528, 281, 362, 257, 853, 30], "temperature": 0.0, "avg_logprob": -0.18331168533919692, "compression_ratio": 1.6185567010309279, "no_speech_prob": 7.646453013876453e-06}, {"id": 201, "seek": 90558, "start": 920.7, "end": 924.22, "text": " So if our data is imbalanced for example", "tokens": [407, 498, 527, 1412, 307, 566, 40251, 337, 1365], "temperature": 0.0, "avg_logprob": -0.18331168533919692, "compression_ratio": 1.6185567010309279, "no_speech_prob": 7.646453013876453e-06}, {"id": 202, "seek": 90558, "start": 924.22, "end": 931.7, "text": " We can if you're randomly sampling it we can only have one class in our validation set so our fitted model maybe", "tokens": [492, 393, 498, 291, 434, 16979, 21179, 309, 321, 393, 787, 362, 472, 1508, 294, 527, 24071, 992, 370, 527, 26321, 2316, 1310], "temperature": 0.0, "avg_logprob": -0.18331168533919692, "compression_ratio": 1.6185567010309279, "no_speech_prob": 7.646453013876453e-06}, {"id": 203, "seek": 93170, "start": 931.7, "end": 935.5, "text": " That's true as well, so maybe you're trying to predict in a medical situation", "tokens": [663, 311, 2074, 382, 731, 11, 370, 1310, 291, 434, 1382, 281, 6069, 294, 257, 4625, 2590], "temperature": 0.0, "avg_logprob": -0.17469697952270508, "compression_ratio": 1.662551440329218, "no_speech_prob": 3.8449111343652476e-06}, {"id": 204, "seek": 93170, "start": 935.86, "end": 942.34, "text": " Who's going to die of lung cancer and that's only one out of a hundred people and we pick out a validation set that we", "tokens": [2102, 311, 516, 281, 978, 295, 16730, 5592, 293, 300, 311, 787, 472, 484, 295, 257, 3262, 561, 293, 321, 1888, 484, 257, 24071, 992, 300, 321], "temperature": 0.0, "avg_logprob": -0.17469697952270508, "compression_ratio": 1.662551440329218, "no_speech_prob": 3.8449111343652476e-06}, {"id": 205, "seek": 93170, "start": 942.4200000000001, "end": 944.4200000000001, "text": " Accidentally have nobody that died of lung cancer", "tokens": [5725, 36578, 362, 5079, 300, 4539, 295, 16730, 5592], "temperature": 0.0, "avg_logprob": -0.17469697952270508, "compression_ratio": 1.662551440329218, "no_speech_prob": 3.8449111343652476e-06}, {"id": 206, "seek": 93170, "start": 945.26, "end": 950.46, "text": " That's also true. These are all good niche examples", "tokens": [663, 311, 611, 2074, 13, 1981, 366, 439, 665, 19956, 5110], "temperature": 0.0, "avg_logprob": -0.17469697952270508, "compression_ratio": 1.662551440329218, "no_speech_prob": 3.8449111343652476e-06}, {"id": 207, "seek": 93170, "start": 951.1, "end": 955.5400000000001, "text": " But none of them quite say like why could the validation set just be plain?", "tokens": [583, 6022, 295, 552, 1596, 584, 411, 983, 727, 264, 24071, 992, 445, 312, 11121, 30], "temperature": 0.0, "avg_logprob": -0.17469697952270508, "compression_ratio": 1.662551440329218, "no_speech_prob": 3.8449111343652476e-06}, {"id": 208, "seek": 93170, "start": 957.0200000000001, "end": 959.0200000000001, "text": " wrong like give you a totally", "tokens": [2085, 411, 976, 291, 257, 3879], "temperature": 0.0, "avg_logprob": -0.17469697952270508, "compression_ratio": 1.662551440329218, "no_speech_prob": 3.8449111343652476e-06}, {"id": 209, "seek": 95902, "start": 959.02, "end": 964.02, "text": " Inaccurate idea of whether this is going to generalize and so let's talk about", "tokens": [682, 8476, 33144, 1558, 295, 1968, 341, 307, 516, 281, 2674, 1125, 293, 370, 718, 311, 751, 466], "temperature": 0.0, "avg_logprob": -0.1935182359483507, "compression_ratio": 1.6712962962962963, "no_speech_prob": 6.962207407923415e-06}, {"id": 210, "seek": 95902, "start": 964.66, "end": 968.34, "text": " And the closest is is what Tyler was saying about time", "tokens": [400, 264, 13699, 307, 307, 437, 16869, 390, 1566, 466, 565], "temperature": 0.0, "avg_logprob": -0.1935182359483507, "compression_ratio": 1.6712962962962963, "no_speech_prob": 6.962207407923415e-06}, {"id": 211, "seek": 95902, "start": 969.34, "end": 971.34, "text": " closeness in time", "tokens": [2611, 15264, 294, 565], "temperature": 0.0, "avg_logprob": -0.1935182359483507, "compression_ratio": 1.6712962962962963, "no_speech_prob": 6.962207407923415e-06}, {"id": 212, "seek": 95902, "start": 971.34, "end": 973.86, "text": " The important thing to remember is when you build a model", "tokens": [440, 1021, 551, 281, 1604, 307, 562, 291, 1322, 257, 2316], "temperature": 0.0, "avg_logprob": -0.1935182359483507, "compression_ratio": 1.6712962962962963, "no_speech_prob": 6.962207407923415e-06}, {"id": 213, "seek": 95902, "start": 974.62, "end": 978.06, "text": " You're always you always have a systematic error", "tokens": [509, 434, 1009, 291, 1009, 362, 257, 27249, 6713], "temperature": 0.0, "avg_logprob": -0.1935182359483507, "compression_ratio": 1.6712962962962963, "no_speech_prob": 6.962207407923415e-06}, {"id": 214, "seek": 95902, "start": 978.54, "end": 984.92, "text": " Which is that you're going to use the model at a later time than the time that you built it right like", "tokens": [3013, 307, 300, 291, 434, 516, 281, 764, 264, 2316, 412, 257, 1780, 565, 813, 264, 565, 300, 291, 3094, 309, 558, 411], "temperature": 0.0, "avg_logprob": -0.1935182359483507, "compression_ratio": 1.6712962962962963, "no_speech_prob": 6.962207407923415e-06}, {"id": 215, "seek": 98492, "start": 984.92, "end": 989.4799999999999, "text": " You're going to put it into production by which time the world is different", "tokens": [509, 434, 516, 281, 829, 309, 666, 4265, 538, 597, 565, 264, 1002, 307, 819], "temperature": 0.0, "avg_logprob": -0.16251137143089658, "compression_ratio": 1.8405172413793103, "no_speech_prob": 2.6425734631629894e-06}, {"id": 216, "seek": 98492, "start": 989.88, "end": 994.56, "text": " To the world that you're in now and even when you're building the model you're using data", "tokens": [1407, 264, 1002, 300, 291, 434, 294, 586, 293, 754, 562, 291, 434, 2390, 264, 2316, 291, 434, 1228, 1412], "temperature": 0.0, "avg_logprob": -0.16251137143089658, "compression_ratio": 1.8405172413793103, "no_speech_prob": 2.6425734631629894e-06}, {"id": 217, "seek": 98492, "start": 994.56, "end": 998.24, "text": " Which is older than today anyway right so there's some lag", "tokens": [3013, 307, 4906, 813, 965, 4033, 558, 370, 456, 311, 512, 8953], "temperature": 0.0, "avg_logprob": -0.16251137143089658, "compression_ratio": 1.8405172413793103, "no_speech_prob": 2.6425734631629894e-06}, {"id": 218, "seek": 98492, "start": 998.76, "end": 1003.8399999999999, "text": " between the data that you're building it on and the data that it's going to actually be used on your life and", "tokens": [1296, 264, 1412, 300, 291, 434, 2390, 309, 322, 293, 264, 1412, 300, 309, 311, 516, 281, 767, 312, 1143, 322, 428, 993, 293], "temperature": 0.0, "avg_logprob": -0.16251137143089658, "compression_ratio": 1.8405172413793103, "no_speech_prob": 2.6425734631629894e-06}, {"id": 219, "seek": 98492, "start": 1004.4, "end": 1010.5999999999999, "text": " A lot of the time if not most of the time that matters right so if we're doing stuff in like", "tokens": [316, 688, 295, 264, 565, 498, 406, 881, 295, 264, 565, 300, 7001, 558, 370, 498, 321, 434, 884, 1507, 294, 411], "temperature": 0.0, "avg_logprob": -0.16251137143089658, "compression_ratio": 1.8405172413793103, "no_speech_prob": 2.6425734631629894e-06}, {"id": 220, "seek": 101060, "start": 1010.6, "end": 1015.12, "text": " predicting who's going to buy toilet paper in New Jersey and", "tokens": [32884, 567, 311, 516, 281, 2256, 11137, 3035, 294, 1873, 16601, 293], "temperature": 0.0, "avg_logprob": -0.19182090388918385, "compression_ratio": 1.67578125, "no_speech_prob": 9.080376003112178e-06}, {"id": 221, "seek": 101060, "start": 1015.84, "end": 1018.2, "text": " it takes us two weeks to put it in production and", "tokens": [309, 2516, 505, 732, 3259, 281, 829, 309, 294, 4265, 293], "temperature": 0.0, "avg_logprob": -0.19182090388918385, "compression_ratio": 1.67578125, "no_speech_prob": 9.080376003112178e-06}, {"id": 222, "seek": 101060, "start": 1018.96, "end": 1024.96, "text": " We did it using data from the last couple of years then by that time you know things may look", "tokens": [492, 630, 309, 1228, 1412, 490, 264, 1036, 1916, 295, 924, 550, 538, 300, 565, 291, 458, 721, 815, 574], "temperature": 0.0, "avg_logprob": -0.19182090388918385, "compression_ratio": 1.67578125, "no_speech_prob": 9.080376003112178e-06}, {"id": 223, "seek": 101060, "start": 1025.56, "end": 1027.56, "text": " very different right and", "tokens": [588, 819, 558, 293], "temperature": 0.0, "avg_logprob": -0.19182090388918385, "compression_ratio": 1.67578125, "no_speech_prob": 9.080376003112178e-06}, {"id": 224, "seek": 101060, "start": 1028.16, "end": 1034.24, "text": " Particularly our validation set if we randomly sampled it right and it was like from a four-year period", "tokens": [32281, 527, 24071, 992, 498, 321, 16979, 3247, 15551, 309, 558, 293, 309, 390, 411, 490, 257, 1451, 12, 5294, 2896], "temperature": 0.0, "avg_logprob": -0.19182090388918385, "compression_ratio": 1.67578125, "no_speech_prob": 9.080376003112178e-06}, {"id": 225, "seek": 103424, "start": 1034.24, "end": 1040.48, "text": " Then the vast majority of that data is going to be over a year old right and it may be that the", "tokens": [1396, 264, 8369, 6286, 295, 300, 1412, 307, 516, 281, 312, 670, 257, 1064, 1331, 558, 293, 309, 815, 312, 300, 264], "temperature": 0.0, "avg_logprob": -0.20569241300542304, "compression_ratio": 1.689516129032258, "no_speech_prob": 3.1875367767497664e-06}, {"id": 226, "seek": 103424, "start": 1041.28, "end": 1044.68, "text": " toilet buying habits of folks in New Jersey may have", "tokens": [11137, 6382, 14100, 295, 4024, 294, 1873, 16601, 815, 362], "temperature": 0.0, "avg_logprob": -0.20569241300542304, "compression_ratio": 1.689516129032258, "no_speech_prob": 3.1875367767497664e-06}, {"id": 227, "seek": 103424, "start": 1045.32, "end": 1049.92, "text": " Dramatically shifted maybe they've got a terrible recession there now, and they can't afford", "tokens": [413, 2356, 5030, 18892, 1310, 436, 600, 658, 257, 6237, 24828, 456, 586, 11, 293, 436, 393, 380, 6157], "temperature": 0.0, "avg_logprob": -0.20569241300542304, "compression_ratio": 1.689516129032258, "no_speech_prob": 3.1875367767497664e-06}, {"id": 228, "seek": 103424, "start": 1050.68, "end": 1052.68, "text": " high quality toilet paper anymore", "tokens": [1090, 3125, 11137, 3035, 3602], "temperature": 0.0, "avg_logprob": -0.20569241300542304, "compression_ratio": 1.689516129032258, "no_speech_prob": 3.1875367767497664e-06}, {"id": 229, "seek": 103424, "start": 1053.44, "end": 1060.76, "text": " Or maybe they know their paper making industry has gone through the roof and suddenly you know they could they're buying lots more toilet paper", "tokens": [1610, 1310, 436, 458, 641, 3035, 1455, 3518, 575, 2780, 807, 264, 8418, 293, 5800, 291, 458, 436, 727, 436, 434, 6382, 3195, 544, 11137, 3035], "temperature": 0.0, "avg_logprob": -0.20569241300542304, "compression_ratio": 1.689516129032258, "no_speech_prob": 3.1875367767497664e-06}, {"id": 230, "seek": 106076, "start": 1060.76, "end": 1068.28, "text": " Because it's so cheap or whatever right so the world changes and therefore if you use a random", "tokens": [1436, 309, 311, 370, 7084, 420, 2035, 558, 370, 264, 1002, 2962, 293, 4412, 498, 291, 764, 257, 4974], "temperature": 0.0, "avg_logprob": -0.20036935270502326, "compression_ratio": 1.6952789699570816, "no_speech_prob": 4.092879407835426e-06}, {"id": 231, "seek": 106076, "start": 1068.52, "end": 1075.58, "text": " Sample for your validation set then you're actually checking how good are you at predicting things that are totally obsolete now?", "tokens": [4832, 781, 337, 428, 24071, 992, 550, 291, 434, 767, 8568, 577, 665, 366, 291, 412, 32884, 721, 300, 366, 3879, 46333, 586, 30], "temperature": 0.0, "avg_logprob": -0.20036935270502326, "compression_ratio": 1.6952789699570816, "no_speech_prob": 4.092879407835426e-06}, {"id": 232, "seek": 106076, "start": 1076.04, "end": 1081.24, "text": " But how good are you at predicting things that happened four years ago? That's not interesting", "tokens": [583, 577, 665, 366, 291, 412, 32884, 721, 300, 2011, 1451, 924, 2057, 30, 663, 311, 406, 1880], "temperature": 0.0, "avg_logprob": -0.20036935270502326, "compression_ratio": 1.6952789699570816, "no_speech_prob": 4.092879407835426e-06}, {"id": 233, "seek": 106076, "start": 1082.0, "end": 1084.76, "text": " Okay, so what we want to do in practice", "tokens": [1033, 11, 370, 437, 321, 528, 281, 360, 294, 3124], "temperature": 0.0, "avg_logprob": -0.20036935270502326, "compression_ratio": 1.6952789699570816, "no_speech_prob": 4.092879407835426e-06}, {"id": 234, "seek": 106076, "start": 1086.24, "end": 1088.32, "text": " Anytime there's some temporal piece", "tokens": [39401, 456, 311, 512, 30881, 2522], "temperature": 0.0, "avg_logprob": -0.20036935270502326, "compression_ratio": 1.6952789699570816, "no_speech_prob": 4.092879407835426e-06}, {"id": 235, "seek": 108832, "start": 1088.32, "end": 1095.06, "text": " Is to instead say assuming that we've ordered it by time", "tokens": [1119, 281, 2602, 584, 11926, 300, 321, 600, 8866, 309, 538, 565], "temperature": 0.0, "avg_logprob": -0.1829710642496745, "compression_ratio": 1.5302013422818792, "no_speech_prob": 5.173862518859096e-06}, {"id": 236, "seek": 108832, "start": 1098.6799999999998, "end": 1102.04, "text": " All right, so this is old and this is new", "tokens": [1057, 558, 11, 370, 341, 307, 1331, 293, 341, 307, 777], "temperature": 0.0, "avg_logprob": -0.1829710642496745, "compression_ratio": 1.5302013422818792, "no_speech_prob": 5.173862518859096e-06}, {"id": 237, "seek": 108832, "start": 1105.4399999999998, "end": 1107.4399999999998, "text": " That's our validation set", "tokens": [663, 311, 527, 24071, 992], "temperature": 0.0, "avg_logprob": -0.1829710642496745, "compression_ratio": 1.5302013422818792, "no_speech_prob": 5.173862518859096e-06}, {"id": 238, "seek": 110744, "start": 1107.44, "end": 1117.3200000000002, "text": " Okay, or if we you know I suppose actually do it properly that's our validation set that's our test set", "tokens": [1033, 11, 420, 498, 321, 291, 458, 286, 7297, 767, 360, 309, 6108, 300, 311, 527, 24071, 992, 300, 311, 527, 1500, 992], "temperature": 0.0, "avg_logprob": -0.15557535807291667, "compression_ratio": 1.59375, "no_speech_prob": 6.540380127262324e-06}, {"id": 239, "seek": 110744, "start": 1119.88, "end": 1128.1200000000001, "text": " Make sense right so here's our training set and we use that and we try and build a model that still works on stuff", "tokens": [4387, 2020, 558, 370, 510, 311, 527, 3097, 992, 293, 321, 764, 300, 293, 321, 853, 293, 1322, 257, 2316, 300, 920, 1985, 322, 1507], "temperature": 0.0, "avg_logprob": -0.15557535807291667, "compression_ratio": 1.59375, "no_speech_prob": 6.540380127262324e-06}, {"id": 240, "seek": 110744, "start": 1128.56, "end": 1134.04, "text": " That's later in time than anything the model was built on and so we're not just testing", "tokens": [663, 311, 1780, 294, 565, 813, 1340, 264, 2316, 390, 3094, 322, 293, 370, 321, 434, 406, 445, 4997], "temperature": 0.0, "avg_logprob": -0.15557535807291667, "compression_ratio": 1.59375, "no_speech_prob": 6.540380127262324e-06}, {"id": 241, "seek": 113404, "start": 1134.04, "end": 1137.72, "text": " generalization in some kind of abstract sense, but in a very", "tokens": [2674, 2144, 294, 512, 733, 295, 12649, 2020, 11, 457, 294, 257, 588], "temperature": 0.0, "avg_logprob": -0.28324378620494495, "compression_ratio": 1.6231884057971016, "no_speech_prob": 1.863104807853233e-05}, {"id": 242, "seek": 113404, "start": 1138.56, "end": 1143.24, "text": " Specific time sense which is it generalizes to the future could you pass it to siraj, please?", "tokens": [20484, 1089, 565, 2020, 597, 307, 309, 2674, 5660, 281, 264, 2027, 727, 291, 1320, 309, 281, 4735, 1805, 11, 1767, 30], "temperature": 0.0, "avg_logprob": -0.28324378620494495, "compression_ratio": 1.6231884057971016, "no_speech_prob": 1.863104807853233e-05}, {"id": 243, "seek": 113404, "start": 1147.68, "end": 1150.04, "text": " So when we are as you said", "tokens": [407, 562, 321, 366, 382, 291, 848], "temperature": 0.0, "avg_logprob": -0.28324378620494495, "compression_ratio": 1.6231884057971016, "no_speech_prob": 1.863104807853233e-05}, {"id": 244, "seek": 113404, "start": 1150.76, "end": 1153.48, "text": " As you said there is some temporal ordering in the data", "tokens": [1018, 291, 848, 456, 307, 512, 30881, 21739, 294, 264, 1412], "temperature": 0.0, "avg_logprob": -0.28324378620494495, "compression_ratio": 1.6231884057971016, "no_speech_prob": 1.863104807853233e-05}, {"id": 245, "seek": 113404, "start": 1153.72, "end": 1160.92, "text": " So in that case is it wise to take the entire old data for training or only a few recent data set?", "tokens": [407, 294, 300, 1389, 307, 309, 10829, 281, 747, 264, 2302, 1331, 1412, 337, 3097, 420, 787, 257, 1326, 5162, 1412, 992, 30], "temperature": 0.0, "avg_logprob": -0.28324378620494495, "compression_ratio": 1.6231884057971016, "no_speech_prob": 1.863104807853233e-05}, {"id": 246, "seek": 116092, "start": 1160.92, "end": 1164.5600000000002, "text": " So validation test or training training", "tokens": [407, 24071, 1500, 420, 3097, 3097], "temperature": 0.0, "avg_logprob": -0.24906709653521897, "compression_ratio": 1.8577981651376148, "no_speech_prob": 5.0936255320266355e-06}, {"id": 247, "seek": 116092, "start": 1165.68, "end": 1172.22, "text": " Yeah, that's a whole nother question right so how do you how do you get the validation set to be good?", "tokens": [865, 11, 300, 311, 257, 1379, 406, 511, 1168, 558, 370, 577, 360, 291, 577, 360, 291, 483, 264, 24071, 992, 281, 312, 665, 30], "temperature": 0.0, "avg_logprob": -0.24906709653521897, "compression_ratio": 1.8577981651376148, "no_speech_prob": 5.0936255320266355e-06}, {"id": 248, "seek": 116092, "start": 1172.22, "end": 1177.74, "text": " So I build them a random forest on all the training data. It looks good on the training data", "tokens": [407, 286, 1322, 552, 257, 4974, 6719, 322, 439, 264, 3097, 1412, 13, 467, 1542, 665, 322, 264, 3097, 1412], "temperature": 0.0, "avg_logprob": -0.24906709653521897, "compression_ratio": 1.8577981651376148, "no_speech_prob": 5.0936255320266355e-06}, {"id": 249, "seek": 116092, "start": 1178.24, "end": 1180.24, "text": " It looks good on the OOB", "tokens": [467, 1542, 665, 322, 264, 422, 46, 33], "temperature": 0.0, "avg_logprob": -0.24906709653521897, "compression_ratio": 1.8577981651376148, "no_speech_prob": 5.0936255320266355e-06}, {"id": 250, "seek": 116092, "start": 1180.72, "end": 1184.96, "text": " But and this is actually a really good reason to have OOB if it looks good on the OOB", "tokens": [583, 293, 341, 307, 767, 257, 534, 665, 1778, 281, 362, 422, 46, 33, 498, 309, 1542, 665, 322, 264, 422, 46, 33], "temperature": 0.0, "avg_logprob": -0.24906709653521897, "compression_ratio": 1.8577981651376148, "no_speech_prob": 5.0936255320266355e-06}, {"id": 251, "seek": 116092, "start": 1184.96, "end": 1188.48, "text": " But it means you're not overfitting in a statistical sense", "tokens": [583, 309, 1355, 291, 434, 406, 670, 69, 2414, 294, 257, 22820, 2020], "temperature": 0.0, "avg_logprob": -0.24906709653521897, "compression_ratio": 1.8577981651376148, "no_speech_prob": 5.0936255320266355e-06}, {"id": 252, "seek": 118848, "start": 1188.48, "end": 1191.84, "text": " Right like it's it's it's working well on a random sample", "tokens": [1779, 411, 309, 311, 309, 311, 309, 311, 1364, 731, 322, 257, 4974, 6889], "temperature": 0.0, "avg_logprob": -0.20612859725952148, "compression_ratio": 1.73828125, "no_speech_prob": 4.2893052523140796e-06}, {"id": 253, "seek": 118848, "start": 1192.96, "end": 1200.68, "text": " But then it looks bad on the validation set so what happened well what happened was that you you somehow failed to?", "tokens": [583, 550, 309, 1542, 1578, 322, 264, 24071, 992, 370, 437, 2011, 731, 437, 2011, 390, 300, 291, 291, 6063, 7612, 281, 30], "temperature": 0.0, "avg_logprob": -0.20612859725952148, "compression_ratio": 1.73828125, "no_speech_prob": 4.2893052523140796e-06}, {"id": 254, "seek": 118848, "start": 1200.8, "end": 1203.84, "text": " predict the future you only predicted the past and", "tokens": [6069, 264, 2027, 291, 787, 19147, 264, 1791, 293], "temperature": 0.0, "avg_logprob": -0.20612859725952148, "compression_ratio": 1.73828125, "no_speech_prob": 4.2893052523140796e-06}, {"id": 255, "seek": 118848, "start": 1204.1200000000001, "end": 1207.28, "text": " So sir I've had an idea about how we could fix that would be okay", "tokens": [407, 4735, 286, 600, 632, 364, 1558, 466, 577, 321, 727, 3191, 300, 576, 312, 1392], "temperature": 0.0, "avg_logprob": -0.20612859725952148, "compression_ratio": 1.73828125, "no_speech_prob": 4.2893052523140796e-06}, {"id": 256, "seek": 118848, "start": 1207.28, "end": 1211.2, "text": " Well, maybe we should just train so like maybe we shouldn't use the whole training set", "tokens": [1042, 11, 1310, 321, 820, 445, 3847, 370, 411, 1310, 321, 4659, 380, 764, 264, 1379, 3097, 992], "temperature": 0.0, "avg_logprob": -0.20612859725952148, "compression_ratio": 1.73828125, "no_speech_prob": 4.2893052523140796e-06}, {"id": 257, "seek": 118848, "start": 1211.2, "end": 1215.48, "text": " We should try a recent period only and now you know on the downside", "tokens": [492, 820, 853, 257, 5162, 2896, 787, 293, 586, 291, 458, 322, 264, 25060], "temperature": 0.0, "avg_logprob": -0.20612859725952148, "compression_ratio": 1.73828125, "no_speech_prob": 4.2893052523140796e-06}, {"id": 258, "seek": 121548, "start": 1215.48, "end": 1220.3600000000001, "text": " We're not using less data so we can create less rich models on the upside", "tokens": [492, 434, 406, 1228, 1570, 1412, 370, 321, 393, 1884, 1570, 4593, 5245, 322, 264, 14119], "temperature": 0.0, "avg_logprob": -0.12383455496567947, "compression_ratio": 1.693798449612403, "no_speech_prob": 5.255330961517757e-06}, {"id": 259, "seek": 121548, "start": 1220.3600000000001, "end": 1225.6, "text": " It's it's more up-to-date data, and this is something you have to play around with", "tokens": [467, 311, 309, 311, 544, 493, 12, 1353, 12, 17393, 1412, 11, 293, 341, 307, 746, 291, 362, 281, 862, 926, 365], "temperature": 0.0, "avg_logprob": -0.12383455496567947, "compression_ratio": 1.693798449612403, "no_speech_prob": 5.255330961517757e-06}, {"id": 260, "seek": 121548, "start": 1226.48, "end": 1227.76, "text": " most", "tokens": [881], "temperature": 0.0, "avg_logprob": -0.12383455496567947, "compression_ratio": 1.693798449612403, "no_speech_prob": 5.255330961517757e-06}, {"id": 261, "seek": 121548, "start": 1227.76, "end": 1233.28, "text": " Machine learning functions have the ability to provide a weight that is given to each row", "tokens": [22155, 2539, 6828, 362, 264, 3485, 281, 2893, 257, 3364, 300, 307, 2212, 281, 1184, 5386], "temperature": 0.0, "avg_logprob": -0.12383455496567947, "compression_ratio": 1.693798449612403, "no_speech_prob": 5.255330961517757e-06}, {"id": 262, "seek": 121548, "start": 1233.96, "end": 1238.0, "text": " So for example with a random forest rather than bootstrapping at random", "tokens": [407, 337, 1365, 365, 257, 4974, 6719, 2831, 813, 11450, 19639, 3759, 412, 4974], "temperature": 0.0, "avg_logprob": -0.12383455496567947, "compression_ratio": 1.693798449612403, "no_speech_prob": 5.255330961517757e-06}, {"id": 263, "seek": 123800, "start": 1238.0, "end": 1244.88, "text": " You could have a weight on every row and randomly pick that row with some probability right and we could like say", "tokens": [509, 727, 362, 257, 3364, 322, 633, 5386, 293, 16979, 1888, 300, 5386, 365, 512, 8482, 558, 293, 321, 727, 411, 584], "temperature": 0.0, "avg_logprob": -0.1262927004086074, "compression_ratio": 1.7543859649122806, "no_speech_prob": 1.788057147678046e-06}, {"id": 264, "seek": 123800, "start": 1245.6, "end": 1247.6, "text": " Here's our like probability", "tokens": [1692, 311, 527, 411, 8482], "temperature": 0.0, "avg_logprob": -0.1262927004086074, "compression_ratio": 1.7543859649122806, "no_speech_prob": 1.788057147678046e-06}, {"id": 265, "seek": 123800, "start": 1248.32, "end": 1250.32, "text": " we could like pick a", "tokens": [321, 727, 411, 1888, 257], "temperature": 0.0, "avg_logprob": -0.1262927004086074, "compression_ratio": 1.7543859649122806, "no_speech_prob": 1.788057147678046e-06}, {"id": 266, "seek": 123800, "start": 1250.88, "end": 1252.88, "text": " Curve that looks like that", "tokens": [7907, 303, 300, 1542, 411, 300], "temperature": 0.0, "avg_logprob": -0.1262927004086074, "compression_ratio": 1.7543859649122806, "no_speech_prob": 1.788057147678046e-06}, {"id": 267, "seek": 123800, "start": 1253.12, "end": 1259.62, "text": " So that the most recent rows have a higher probability of being selected that can work really well", "tokens": [407, 300, 264, 881, 5162, 13241, 362, 257, 2946, 8482, 295, 885, 8209, 300, 393, 589, 534, 731], "temperature": 0.0, "avg_logprob": -0.1262927004086074, "compression_ratio": 1.7543859649122806, "no_speech_prob": 1.788057147678046e-06}, {"id": 268, "seek": 125962, "start": 1259.62, "end": 1267.5, "text": " Yeah, it's something that you have to try and and if you don't have a validation set that represents the future", "tokens": [865, 11, 309, 311, 746, 300, 291, 362, 281, 853, 293, 293, 498, 291, 500, 380, 362, 257, 24071, 992, 300, 8855, 264, 2027], "temperature": 0.0, "avg_logprob": -0.11265742394231981, "compression_ratio": 1.6483050847457628, "no_speech_prob": 5.771848009317182e-06}, {"id": 269, "seek": 125962, "start": 1267.8999999999999, "end": 1272.7399999999998, "text": " Compared to what you're training on you have no way to know which of your techniques are working", "tokens": [30539, 281, 437, 291, 434, 3097, 322, 291, 362, 572, 636, 281, 458, 597, 295, 428, 7512, 366, 1364], "temperature": 0.0, "avg_logprob": -0.11265742394231981, "compression_ratio": 1.6483050847457628, "no_speech_prob": 5.771848009317182e-06}, {"id": 270, "seek": 125962, "start": 1272.7399999999998, "end": 1277.2199999999998, "text": " How do you make the compromise between amount of data versus recency of data?", "tokens": [1012, 360, 291, 652, 264, 18577, 1296, 2372, 295, 1412, 5717, 850, 3020, 295, 1412, 30], "temperature": 0.0, "avg_logprob": -0.11265742394231981, "compression_ratio": 1.6483050847457628, "no_speech_prob": 5.771848009317182e-06}, {"id": 271, "seek": 125962, "start": 1279.62, "end": 1286.58, "text": " So what I tend to do is is when I have this kind of temporal issue, which is probably most of the time", "tokens": [407, 437, 286, 3928, 281, 360, 307, 307, 562, 286, 362, 341, 733, 295, 30881, 2734, 11, 597, 307, 1391, 881, 295, 264, 565], "temperature": 0.0, "avg_logprob": -0.11265742394231981, "compression_ratio": 1.6483050847457628, "no_speech_prob": 5.771848009317182e-06}, {"id": 272, "seek": 128658, "start": 1286.58, "end": 1290.8999999999999, "text": " Once I have something that's working well on the validation set", "tokens": [3443, 286, 362, 746, 300, 311, 1364, 731, 322, 264, 24071, 992], "temperature": 0.0, "avg_logprob": -0.16074648963080512, "compression_ratio": 1.7971698113207548, "no_speech_prob": 2.2603121578868013e-06}, {"id": 273, "seek": 128658, "start": 1290.8999999999999, "end": 1296.82, "text": " I wouldn't then go and just use that model on the test set because the thing that I've trained on is now like", "tokens": [286, 2759, 380, 550, 352, 293, 445, 764, 300, 2316, 322, 264, 1500, 992, 570, 264, 551, 300, 286, 600, 8895, 322, 307, 586, 411], "temperature": 0.0, "avg_logprob": -0.16074648963080512, "compression_ratio": 1.7971698113207548, "no_speech_prob": 2.2603121578868013e-06}, {"id": 274, "seek": 128658, "start": 1297.74, "end": 1302.6599999999999, "text": " Much you know the test set is much more in the future compared to the training set so I would then", "tokens": [12313, 291, 458, 264, 1500, 992, 307, 709, 544, 294, 264, 2027, 5347, 281, 264, 3097, 992, 370, 286, 576, 550], "temperature": 0.0, "avg_logprob": -0.16074648963080512, "compression_ratio": 1.7971698113207548, "no_speech_prob": 2.2603121578868013e-06}, {"id": 275, "seek": 128658, "start": 1303.1399999999999, "end": 1307.1399999999999, "text": " Replicate building that model again, but this time I would combine", "tokens": [1300, 4770, 473, 2390, 300, 2316, 797, 11, 457, 341, 565, 286, 576, 10432], "temperature": 0.0, "avg_logprob": -0.16074648963080512, "compression_ratio": 1.7971698113207548, "no_speech_prob": 2.2603121578868013e-06}, {"id": 276, "seek": 128658, "start": 1307.8999999999999, "end": 1309.98, "text": " the training and validation sets together", "tokens": [264, 3097, 293, 24071, 6352, 1214], "temperature": 0.0, "avg_logprob": -0.16074648963080512, "compression_ratio": 1.7971698113207548, "no_speech_prob": 2.2603121578868013e-06}, {"id": 277, "seek": 130998, "start": 1309.98, "end": 1316.22, "text": " Okay, and and retrain the model and at that point you've got no way to test", "tokens": [1033, 11, 293, 293, 1533, 7146, 264, 2316, 293, 412, 300, 935, 291, 600, 658, 572, 636, 281, 1500], "temperature": 0.0, "avg_logprob": -0.1700687186662541, "compression_ratio": 1.6504854368932038, "no_speech_prob": 1.3287728961586254e-06}, {"id": 278, "seek": 130998, "start": 1317.1, "end": 1321.14, "text": " Against a validation set so you have to make sure you have a reproducible", "tokens": [29995, 257, 24071, 992, 370, 291, 362, 281, 652, 988, 291, 362, 257, 11408, 32128], "temperature": 0.0, "avg_logprob": -0.1700687186662541, "compression_ratio": 1.6504854368932038, "no_speech_prob": 1.3287728961586254e-06}, {"id": 279, "seek": 130998, "start": 1322.18, "end": 1325.98, "text": " Script or notebook that does exactly the same steps in exactly the same ways", "tokens": [15675, 420, 21060, 300, 775, 2293, 264, 912, 4439, 294, 2293, 264, 912, 2098], "temperature": 0.0, "avg_logprob": -0.1700687186662541, "compression_ratio": 1.6504854368932038, "no_speech_prob": 1.3287728961586254e-06}, {"id": 280, "seek": 130998, "start": 1327.02, "end": 1331.94, "text": " Because if you get something wrong, then you're going to find on the test set that you've you've got a problem", "tokens": [1436, 498, 291, 483, 746, 2085, 11, 550, 291, 434, 516, 281, 915, 322, 264, 1500, 992, 300, 291, 600, 291, 600, 658, 257, 1154], "temperature": 0.0, "avg_logprob": -0.1700687186662541, "compression_ratio": 1.6504854368932038, "no_speech_prob": 1.3287728961586254e-06}, {"id": 281, "seek": 130998, "start": 1333.78, "end": 1335.26, "text": " So", "tokens": [407], "temperature": 0.0, "avg_logprob": -0.1700687186662541, "compression_ratio": 1.6504854368932038, "no_speech_prob": 1.3287728961586254e-06}, {"id": 282, "seek": 133526, "start": 1335.26, "end": 1340.42, "text": " So what what I do in practice is I need to know is my validation set", "tokens": [407, 437, 437, 286, 360, 294, 3124, 307, 286, 643, 281, 458, 307, 452, 24071, 992], "temperature": 0.0, "avg_logprob": -0.20681101935250418, "compression_ratio": 1.7179487179487178, "no_speech_prob": 1.6028025129344314e-06}, {"id": 283, "seek": 133526, "start": 1341.58, "end": 1348.82, "text": " a truly representative of the test set so what I do is I build five models on the", "tokens": [257, 4908, 12424, 295, 264, 1500, 992, 370, 437, 286, 360, 307, 286, 1322, 1732, 5245, 322, 264], "temperature": 0.0, "avg_logprob": -0.20681101935250418, "compression_ratio": 1.7179487179487178, "no_speech_prob": 1.6028025129344314e-06}, {"id": 284, "seek": 133526, "start": 1349.66, "end": 1351.66, "text": " training set I", "tokens": [3097, 992, 286], "temperature": 0.0, "avg_logprob": -0.20681101935250418, "compression_ratio": 1.7179487179487178, "no_speech_prob": 1.6028025129344314e-06}, {"id": 285, "seek": 133526, "start": 1356.02, "end": 1361.18, "text": " Build five models on the training set and I try to have them kind of vary in how good I think they are", "tokens": [11875, 1732, 5245, 322, 264, 3097, 992, 293, 286, 853, 281, 362, 552, 733, 295, 10559, 294, 577, 665, 286, 519, 436, 366], "temperature": 0.0, "avg_logprob": -0.20681101935250418, "compression_ratio": 1.7179487179487178, "no_speech_prob": 1.6028025129344314e-06}, {"id": 286, "seek": 136118, "start": 1361.18, "end": 1367.8600000000001, "text": " Right and then and then I score them my five models on the validation set", "tokens": [1779, 293, 550, 293, 550, 286, 6175, 552, 452, 1732, 5245, 322, 264, 24071, 992], "temperature": 0.0, "avg_logprob": -0.13085501090339993, "compression_ratio": 1.9351351351351351, "no_speech_prob": 2.2959081888984656e-06}, {"id": 287, "seek": 136118, "start": 1369.0600000000002, "end": 1374.78, "text": " Right and then I also score them on the test set right so I'm not cheating", "tokens": [1779, 293, 550, 286, 611, 6175, 552, 322, 264, 1500, 992, 558, 370, 286, 478, 406, 18309], "temperature": 0.0, "avg_logprob": -0.13085501090339993, "compression_ratio": 1.9351351351351351, "no_speech_prob": 2.2959081888984656e-06}, {"id": 288, "seek": 136118, "start": 1374.78, "end": 1378.54, "text": " So I'm not using any feedback from the test set to change my hyper parameters", "tokens": [407, 286, 478, 406, 1228, 604, 5824, 490, 264, 1500, 992, 281, 1319, 452, 9848, 9834], "temperature": 0.0, "avg_logprob": -0.13085501090339993, "compression_ratio": 1.9351351351351351, "no_speech_prob": 2.2959081888984656e-06}, {"id": 289, "seek": 136118, "start": 1378.54, "end": 1383.74, "text": " I'm only using it for this one thing which is to check my validation set so I get my five scores", "tokens": [286, 478, 787, 1228, 309, 337, 341, 472, 551, 597, 307, 281, 1520, 452, 24071, 992, 370, 286, 483, 452, 1732, 13444], "temperature": 0.0, "avg_logprob": -0.13085501090339993, "compression_ratio": 1.9351351351351351, "no_speech_prob": 2.2959081888984656e-06}, {"id": 290, "seek": 136118, "start": 1384.94, "end": 1386.94, "text": " from the test set and", "tokens": [490, 264, 1500, 992, 293], "temperature": 0.0, "avg_logprob": -0.13085501090339993, "compression_ratio": 1.9351351351351351, "no_speech_prob": 2.2959081888984656e-06}, {"id": 291, "seek": 136118, "start": 1388.1000000000001, "end": 1390.1000000000001, "text": " Then I check", "tokens": [1396, 286, 1520], "temperature": 0.0, "avg_logprob": -0.13085501090339993, "compression_ratio": 1.9351351351351351, "no_speech_prob": 2.2959081888984656e-06}, {"id": 292, "seek": 139010, "start": 1390.1, "end": 1392.1, "text": " That they fall in a line", "tokens": [663, 436, 2100, 294, 257, 1622], "temperature": 0.0, "avg_logprob": -0.16414504784804124, "compression_ratio": 1.7611336032388665, "no_speech_prob": 2.3320553736994043e-06}, {"id": 293, "seek": 139010, "start": 1392.5, "end": 1393.62, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.16414504784804124, "compression_ratio": 1.7611336032388665, "no_speech_prob": 2.3320553736994043e-06}, {"id": 294, "seek": 139010, "start": 1393.62, "end": 1400.02, "text": " And if they don't then you're not going to get good enough feedback from the validation set so keep doing that process", "tokens": [400, 498, 436, 500, 380, 550, 291, 434, 406, 516, 281, 483, 665, 1547, 5824, 490, 264, 24071, 992, 370, 1066, 884, 300, 1399], "temperature": 0.0, "avg_logprob": -0.16414504784804124, "compression_ratio": 1.7611336032388665, "no_speech_prob": 2.3320553736994043e-06}, {"id": 295, "seek": 139010, "start": 1400.6999999999998, "end": 1404.82, "text": " Until you're getting a line and that can be quite tricky right sometimes", "tokens": [9088, 291, 434, 1242, 257, 1622, 293, 300, 393, 312, 1596, 12414, 558, 2171], "temperature": 0.0, "avg_logprob": -0.16414504784804124, "compression_ratio": 1.7611336032388665, "no_speech_prob": 2.3320553736994043e-06}, {"id": 296, "seek": 139010, "start": 1405.82, "end": 1407.82, "text": " the the test set", "tokens": [264, 264, 1500, 992], "temperature": 0.0, "avg_logprob": -0.16414504784804124, "compression_ratio": 1.7611336032388665, "no_speech_prob": 2.3320553736994043e-06}, {"id": 297, "seek": 139010, "start": 1409.1399999999999, "end": 1413.86, "text": " You know trying to create something that's as similar to the real world outcome as possible", "tokens": [509, 458, 1382, 281, 1884, 746, 300, 311, 382, 2531, 281, 264, 957, 1002, 9700, 382, 1944], "temperature": 0.0, "avg_logprob": -0.16414504784804124, "compression_ratio": 1.7611336032388665, "no_speech_prob": 2.3320553736994043e-06}, {"id": 298, "seek": 141386, "start": 1413.86, "end": 1420.78, "text": " It's difficult right and when you're kind of in the real world the same is true of creating the test set like the test set", "tokens": [467, 311, 2252, 558, 293, 562, 291, 434, 733, 295, 294, 264, 957, 1002, 264, 912, 307, 2074, 295, 4084, 264, 1500, 992, 411, 264, 1500, 992], "temperature": 0.0, "avg_logprob": -0.15901072550628145, "compression_ratio": 1.9246031746031746, "no_speech_prob": 7.52789446778479e-06}, {"id": 299, "seek": 141386, "start": 1420.9799999999998, "end": 1422.9799999999998, "text": " Has to be a close to production as possible", "tokens": [8646, 281, 312, 257, 1998, 281, 4265, 382, 1944], "temperature": 0.0, "avg_logprob": -0.15901072550628145, "compression_ratio": 1.9246031746031746, "no_speech_prob": 7.52789446778479e-06}, {"id": 300, "seek": 141386, "start": 1423.62, "end": 1427.84, "text": " So like what's the actual mix of customers that are going to be using this?", "tokens": [407, 411, 437, 311, 264, 3539, 2890, 295, 4581, 300, 366, 516, 281, 312, 1228, 341, 30], "temperature": 0.0, "avg_logprob": -0.15901072550628145, "compression_ratio": 1.9246031746031746, "no_speech_prob": 7.52789446778479e-06}, {"id": 301, "seek": 141386, "start": 1427.8999999999999, "end": 1431.9799999999998, "text": " How much time is there actually going to be between when you build the model and when you put it in production?", "tokens": [1012, 709, 565, 307, 456, 767, 516, 281, 312, 1296, 562, 291, 1322, 264, 2316, 293, 562, 291, 829, 309, 294, 4265, 30], "temperature": 0.0, "avg_logprob": -0.15901072550628145, "compression_ratio": 1.9246031746031746, "no_speech_prob": 7.52789446778479e-06}, {"id": 302, "seek": 141386, "start": 1432.26, "end": 1437.1599999999999, "text": " How often are you going to be able to refresh the model these are all the things to think about when you build that test set?", "tokens": [1012, 2049, 366, 291, 516, 281, 312, 1075, 281, 15134, 264, 2316, 613, 366, 439, 264, 721, 281, 519, 466, 562, 291, 1322, 300, 1500, 992, 30], "temperature": 0.0, "avg_logprob": -0.15901072550628145, "compression_ratio": 1.9246031746031746, "no_speech_prob": 7.52789446778479e-06}, {"id": 303, "seek": 141386, "start": 1438.02, "end": 1440.02, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.15901072550628145, "compression_ratio": 1.9246031746031746, "no_speech_prob": 7.52789446778479e-06}, {"id": 304, "seek": 144002, "start": 1440.02, "end": 1445.54, "text": " So you want to say that first make five models on the training data", "tokens": [407, 291, 528, 281, 584, 300, 700, 652, 1732, 5245, 322, 264, 3097, 1412], "temperature": 0.0, "avg_logprob": -0.20062443415323894, "compression_ratio": 1.9622641509433962, "no_speech_prob": 5.77181617700262e-06}, {"id": 305, "seek": 144002, "start": 1445.54, "end": 1449.2, "text": " Yeah, and then till you get a straight line relationship", "tokens": [865, 11, 293, 550, 4288, 291, 483, 257, 2997, 1622, 2480], "temperature": 0.0, "avg_logprob": -0.20062443415323894, "compression_ratio": 1.9622641509433962, "no_speech_prob": 5.77181617700262e-06}, {"id": 306, "seek": 144002, "start": 1449.66, "end": 1453.5, "text": " Change your validation and test set you can't really change the test set generally", "tokens": [15060, 428, 24071, 293, 1500, 992, 291, 393, 380, 534, 1319, 264, 1500, 992, 5101], "temperature": 0.0, "avg_logprob": -0.20062443415323894, "compression_ratio": 1.9622641509433962, "no_speech_prob": 5.77181617700262e-06}, {"id": 307, "seek": 144002, "start": 1453.5, "end": 1457.42, "text": " So this is assuming that the test sets given the change to change the validation set", "tokens": [407, 341, 307, 11926, 300, 264, 1500, 6352, 2212, 264, 1319, 281, 1319, 264, 24071, 992], "temperature": 0.0, "avg_logprob": -0.20062443415323894, "compression_ratio": 1.9622641509433962, "no_speech_prob": 5.77181617700262e-06}, {"id": 308, "seek": 144002, "start": 1457.46, "end": 1463.2, "text": " So if you start with a random sample validation set and then it's all over the place and you realize", "tokens": [407, 498, 291, 722, 365, 257, 4974, 6889, 24071, 992, 293, 550, 309, 311, 439, 670, 264, 1081, 293, 291, 4325], "temperature": 0.0, "avg_logprob": -0.20062443415323894, "compression_ratio": 1.9622641509433962, "no_speech_prob": 5.77181617700262e-06}, {"id": 309, "seek": 144002, "start": 1463.2, "end": 1465.2, "text": " Oh, I should have picked the last two months", "tokens": [876, 11, 286, 820, 362, 6183, 264, 1036, 732, 2493], "temperature": 0.0, "avg_logprob": -0.20062443415323894, "compression_ratio": 1.9622641509433962, "no_speech_prob": 5.77181617700262e-06}, {"id": 310, "seek": 144002, "start": 1465.54, "end": 1468.26, "text": " And then you pick the last two months and still go over the place and you realize", "tokens": [400, 550, 291, 1888, 264, 1036, 732, 2493, 293, 920, 352, 670, 264, 1081, 293, 291, 4325], "temperature": 0.0, "avg_logprob": -0.20062443415323894, "compression_ratio": 1.9622641509433962, "no_speech_prob": 5.77181617700262e-06}, {"id": 311, "seek": 146826, "start": 1468.26, "end": 1472.98, "text": " Oh, I should have picked it so that's also from the first of the month to the 15th of the month and", "tokens": [876, 11, 286, 820, 362, 6183, 309, 370, 300, 311, 611, 490, 264, 700, 295, 264, 1618, 281, 264, 2119, 392, 295, 264, 1618, 293], "temperature": 0.0, "avg_logprob": -0.25238758129077954, "compression_ratio": 1.7008928571428572, "no_speech_prob": 7.183160505519481e-06}, {"id": 312, "seek": 146826, "start": 1473.58, "end": 1481.14, "text": " They'll keep going until changing your validation set until you found a validation set which is indicative of your test set results", "tokens": [814, 603, 1066, 516, 1826, 4473, 428, 24071, 992, 1826, 291, 1352, 257, 24071, 992, 597, 307, 47513, 295, 428, 1500, 992, 3542], "temperature": 0.0, "avg_logprob": -0.25238758129077954, "compression_ratio": 1.7008928571428572, "no_speech_prob": 7.183160505519481e-06}, {"id": 313, "seek": 146826, "start": 1485.74, "end": 1491.34, "text": " So the five models like you would start maybe like just the random data and then average and they just make it better", "tokens": [407, 264, 1732, 5245, 411, 291, 576, 722, 1310, 411, 445, 264, 4974, 1412, 293, 550, 4274, 293, 436, 445, 652, 309, 1101], "temperature": 0.0, "avg_logprob": -0.25238758129077954, "compression_ratio": 1.7008928571428572, "no_speech_prob": 7.183160505519481e-06}, {"id": 314, "seek": 146826, "start": 1491.34, "end": 1493.86, "text": " Yeah, yeah, yeah, yeah, maybe a", "tokens": [865, 11, 1338, 11, 1338, 11, 1338, 11, 1310, 257], "temperature": 0.0, "avg_logprob": -0.25238758129077954, "compression_ratio": 1.7008928571428572, "no_speech_prob": 7.183160505519481e-06}, {"id": 315, "seek": 149386, "start": 1493.86, "end": 1501.34, "text": " Exactly, maybe kind of five like not terrible ones, but you want some variety and you also particularly want some variety in like", "tokens": [7587, 11, 1310, 733, 295, 1732, 411, 406, 6237, 2306, 11, 457, 291, 528, 512, 5673, 293, 291, 611, 4098, 528, 512, 5673, 294, 411], "temperature": 0.0, "avg_logprob": -0.14580573022893045, "compression_ratio": 1.8359375, "no_speech_prob": 2.994428541569505e-06}, {"id": 316, "seek": 149386, "start": 1502.5, "end": 1504.86, "text": " How well they might generalize through time?", "tokens": [1012, 731, 436, 1062, 2674, 1125, 807, 565, 30], "temperature": 0.0, "avg_logprob": -0.14580573022893045, "compression_ratio": 1.8359375, "no_speech_prob": 2.994428541569505e-06}, {"id": 317, "seek": 149386, "start": 1504.86, "end": 1508.82, "text": " So one that was trained on the whole training set one that was trained on the last two weeks", "tokens": [407, 472, 300, 390, 8895, 322, 264, 1379, 3097, 992, 472, 300, 390, 8895, 322, 264, 1036, 732, 3259], "temperature": 0.0, "avg_logprob": -0.14580573022893045, "compression_ratio": 1.8359375, "no_speech_prob": 2.994428541569505e-06}, {"id": 318, "seek": 149386, "start": 1509.2199999999998, "end": 1511.58, "text": " One that was trained on the last six weeks", "tokens": [1485, 300, 390, 8895, 322, 264, 1036, 2309, 3259], "temperature": 0.0, "avg_logprob": -0.14580573022893045, "compression_ratio": 1.8359375, "no_speech_prob": 2.994428541569505e-06}, {"id": 319, "seek": 149386, "start": 1512.3, "end": 1516.4599999999998, "text": " One which used you know lots and lots of columns and might overfit a bit more", "tokens": [1485, 597, 1143, 291, 458, 3195, 293, 3195, 295, 13766, 293, 1062, 670, 6845, 257, 857, 544], "temperature": 0.0, "avg_logprob": -0.14580573022893045, "compression_ratio": 1.8359375, "no_speech_prob": 2.994428541569505e-06}, {"id": 320, "seek": 149386, "start": 1516.82, "end": 1521.86, "text": " Yeah, so you kind of want to get a sense of like oh if my validation set fails to", "tokens": [865, 11, 370, 291, 733, 295, 528, 281, 483, 257, 2020, 295, 411, 1954, 498, 452, 24071, 992, 18199, 281], "temperature": 0.0, "avg_logprob": -0.14580573022893045, "compression_ratio": 1.8359375, "no_speech_prob": 2.994428541569505e-06}, {"id": 321, "seek": 152186, "start": 1521.86, "end": 1526.84, "text": " Generalize temporarily I'd want to see that if it fails to generalize statistically. I want to see that", "tokens": [6996, 1125, 23750, 286, 1116, 528, 281, 536, 300, 498, 309, 18199, 281, 2674, 1125, 36478, 13, 286, 528, 281, 536, 300], "temperature": 0.0, "avg_logprob": -0.2429351432650697, "compression_ratio": 1.6679104477611941, "no_speech_prob": 1.7061743164958898e-06}, {"id": 322, "seek": 152186, "start": 1528.54, "end": 1533.3799999999999, "text": " Sorry, can you explain a bit more detail what you mean by change your validation set so it indicates the test set like what?", "tokens": [4919, 11, 393, 291, 2903, 257, 857, 544, 2607, 437, 291, 914, 538, 1319, 428, 24071, 992, 370, 309, 16203, 264, 1500, 992, 411, 437, 30], "temperature": 0.0, "avg_logprob": -0.2429351432650697, "compression_ratio": 1.6679104477611941, "no_speech_prob": 1.7061743164958898e-06}, {"id": 323, "seek": 152186, "start": 1533.3799999999999, "end": 1535.3799999999999, "text": " Does that look like?", "tokens": [4402, 300, 574, 411, 30], "temperature": 0.0, "avg_logprob": -0.2429351432650697, "compression_ratio": 1.6679104477611941, "no_speech_prob": 1.7061743164958898e-06}, {"id": 324, "seek": 152186, "start": 1535.54, "end": 1539.9399999999998, "text": " So possibly so let's take the groceries competition where we're trying to predict", "tokens": [407, 6264, 370, 718, 311, 747, 264, 31391, 6211, 689, 321, 434, 1382, 281, 6069], "temperature": 0.0, "avg_logprob": -0.2429351432650697, "compression_ratio": 1.6679104477611941, "no_speech_prob": 1.7061743164958898e-06}, {"id": 325, "seek": 152186, "start": 1540.4199999999998, "end": 1548.62, "text": " The next two weeks of grocery sales so possible validation sets that Terrence and I played with was a random sample", "tokens": [440, 958, 732, 3259, 295, 14410, 5763, 370, 1944, 24071, 6352, 300, 6564, 10760, 293, 286, 3737, 365, 390, 257, 4974, 6889], "temperature": 0.0, "avg_logprob": -0.2429351432650697, "compression_ratio": 1.6679104477611941, "no_speech_prob": 1.7061743164958898e-06}, {"id": 326, "seek": 154862, "start": 1548.62, "end": 1550.62, "text": " the", "tokens": [264], "temperature": 0.0, "avg_logprob": -0.24337800943626547, "compression_ratio": 1.4615384615384615, "no_speech_prob": 6.0488418966997415e-06}, {"id": 327, "seek": 154862, "start": 1551.7399999999998, "end": 1556.78, "text": " Last month of data the last two weeks of data", "tokens": [5264, 1618, 295, 1412, 264, 1036, 732, 3259, 295, 1412], "temperature": 0.0, "avg_logprob": -0.24337800943626547, "compression_ratio": 1.4615384615384615, "no_speech_prob": 6.0488418966997415e-06}, {"id": 328, "seek": 154862, "start": 1559.9799999999998, "end": 1565.02, "text": " And the other one we tried was same day range", "tokens": [400, 264, 661, 472, 321, 3031, 390, 912, 786, 3613], "temperature": 0.0, "avg_logprob": -0.24337800943626547, "compression_ratio": 1.4615384615384615, "no_speech_prob": 6.0488418966997415e-06}, {"id": 329, "seek": 154862, "start": 1568.26, "end": 1574.6999999999998, "text": " One month earlier so that the test set in this competition was the first to the 15th of", "tokens": [1485, 1618, 3071, 370, 300, 264, 1500, 992, 294, 341, 6211, 390, 264, 700, 281, 264, 2119, 392, 295], "temperature": 0.0, "avg_logprob": -0.24337800943626547, "compression_ratio": 1.4615384615384615, "no_speech_prob": 6.0488418966997415e-06}, {"id": 330, "seek": 154862, "start": 1575.5, "end": 1576.8999999999999, "text": " August", "tokens": [6897], "temperature": 0.0, "avg_logprob": -0.24337800943626547, "compression_ratio": 1.4615384615384615, "no_speech_prob": 6.0488418966997415e-06}, {"id": 331, "seek": 157690, "start": 1576.9, "end": 1580.6200000000001, "text": " Sorry this 15th that maybe the 15th the 30th of August", "tokens": [4919, 341, 2119, 392, 300, 1310, 264, 2119, 392, 264, 2217, 392, 295, 6897], "temperature": 0.0, "avg_logprob": -0.20877158287728187, "compression_ratio": 2.032258064516129, "no_speech_prob": 9.368579412694089e-06}, {"id": 332, "seek": 157690, "start": 1581.42, "end": 1585.2800000000002, "text": " So we tried like a random sample is four years we tried", "tokens": [407, 321, 3031, 411, 257, 4974, 6889, 307, 1451, 924, 321, 3031], "temperature": 0.0, "avg_logprob": -0.20877158287728187, "compression_ratio": 2.032258064516129, "no_speech_prob": 9.368579412694089e-06}, {"id": 333, "seek": 157690, "start": 1586.46, "end": 1594.14, "text": " the 15th of July to the 15th of August we tried the 1st of August to the 15th of August and we tried the", "tokens": [264, 2119, 392, 295, 7370, 281, 264, 2119, 392, 295, 6897, 321, 3031, 264, 502, 372, 295, 6897, 281, 264, 2119, 392, 295, 6897, 293, 321, 3031, 264], "temperature": 0.0, "avg_logprob": -0.20877158287728187, "compression_ratio": 2.032258064516129, "no_speech_prob": 9.368579412694089e-06}, {"id": 334, "seek": 157690, "start": 1594.7800000000002, "end": 1596.94, "text": " 15th of July to the 30th of July and", "tokens": [2119, 392, 295, 7370, 281, 264, 2217, 392, 295, 7370, 293], "temperature": 0.0, "avg_logprob": -0.20877158287728187, "compression_ratio": 2.032258064516129, "no_speech_prob": 9.368579412694089e-06}, {"id": 335, "seek": 157690, "start": 1597.3400000000001, "end": 1604.6200000000001, "text": " So there were four different validation sets we tried and so with random you know our kind of results were all over the place", "tokens": [407, 456, 645, 1451, 819, 24071, 6352, 321, 3031, 293, 370, 365, 4974, 291, 458, 527, 733, 295, 3542, 645, 439, 670, 264, 1081], "temperature": 0.0, "avg_logprob": -0.20877158287728187, "compression_ratio": 2.032258064516129, "no_speech_prob": 9.368579412694089e-06}, {"id": 336, "seek": 160462, "start": 1604.62, "end": 1610.6, "text": " With last month you know they were like not bad, but not great the last two weeks", "tokens": [2022, 1036, 1618, 291, 458, 436, 645, 411, 406, 1578, 11, 457, 406, 869, 264, 1036, 732, 3259], "temperature": 0.0, "avg_logprob": -0.2125572302402594, "compression_ratio": 1.6950354609929077, "no_speech_prob": 7.296297098946525e-06}, {"id": 337, "seek": 160462, "start": 1610.6, "end": 1614.9599999999998, "text": " There was a couple that didn't look good, but on the whole they were good and same day range of months earlier", "tokens": [821, 390, 257, 1916, 300, 994, 380, 574, 665, 11, 457, 322, 264, 1379, 436, 645, 665, 293, 912, 786, 3613, 295, 2493, 3071], "temperature": 0.0, "avg_logprob": -0.2125572302402594, "compression_ratio": 1.6950354609929077, "no_speech_prob": 7.296297098946525e-06}, {"id": 338, "seek": 160462, "start": 1614.9599999999998, "end": 1618.3, "text": " They've got a basically perfect line. That's the part. I'm talking right there", "tokens": [814, 600, 658, 257, 1936, 2176, 1622, 13, 663, 311, 264, 644, 13, 286, 478, 1417, 558, 456], "temperature": 0.0, "avg_logprob": -0.2125572302402594, "compression_ratio": 1.6950354609929077, "no_speech_prob": 7.296297098946525e-06}, {"id": 339, "seek": 160462, "start": 1618.3, "end": 1623.9799999999998, "text": " What exactly are you comparing it to from the test set? I just might confuse what you're creating that graph off of", "tokens": [708, 2293, 366, 291, 15763, 309, 281, 490, 264, 1500, 992, 30, 286, 445, 1062, 28584, 437, 291, 434, 4084, 300, 4295, 766, 295], "temperature": 0.0, "avg_logprob": -0.2125572302402594, "compression_ratio": 1.6950354609929077, "no_speech_prob": 7.296297098946525e-06}, {"id": 340, "seek": 160462, "start": 1624.54, "end": 1631.1999999999998, "text": " So for each of those so for each of my so I build five models right so there might be like", "tokens": [407, 337, 1184, 295, 729, 370, 337, 1184, 295, 452, 370, 286, 1322, 1732, 5245, 558, 370, 456, 1062, 312, 411], "temperature": 0.0, "avg_logprob": -0.2125572302402594, "compression_ratio": 1.6950354609929077, "no_speech_prob": 7.296297098946525e-06}, {"id": 341, "seek": 163120, "start": 1631.2, "end": 1638.04, "text": " Just predict the average do some kind of simple group mean of the whole data set do some group mean of the last month of", "tokens": [1449, 6069, 264, 4274, 360, 512, 733, 295, 2199, 1594, 914, 295, 264, 1379, 1412, 992, 360, 512, 1594, 914, 295, 264, 1036, 1618, 295], "temperature": 0.0, "avg_logprob": -0.15745914491832766, "compression_ratio": 2.103896103896104, "no_speech_prob": 4.092870312888408e-06}, {"id": 342, "seek": 163120, "start": 1638.04, "end": 1642.1200000000001, "text": " The data set build a random forest of the whole thing build a random forest in the last two weeks", "tokens": [440, 1412, 992, 1322, 257, 4974, 6719, 295, 264, 1379, 551, 1322, 257, 4974, 6719, 294, 264, 1036, 732, 3259], "temperature": 0.0, "avg_logprob": -0.15745914491832766, "compression_ratio": 2.103896103896104, "no_speech_prob": 4.092870312888408e-06}, {"id": 343, "seek": 163120, "start": 1642.72, "end": 1644.72, "text": " on each of those I calculate the", "tokens": [322, 1184, 295, 729, 286, 8873, 264], "temperature": 0.0, "avg_logprob": -0.15745914491832766, "compression_ratio": 2.103896103896104, "no_speech_prob": 4.092870312888408e-06}, {"id": 344, "seek": 163120, "start": 1645.24, "end": 1647.24, "text": " validation score and", "tokens": [24071, 6175, 293], "temperature": 0.0, "avg_logprob": -0.15745914491832766, "compression_ratio": 2.103896103896104, "no_speech_prob": 4.092870312888408e-06}, {"id": 345, "seek": 163120, "start": 1647.24, "end": 1652.76, "text": " then I retrain the model on the whole training set and calculate the same thing on the test set and", "tokens": [550, 286, 1533, 7146, 264, 2316, 322, 264, 1379, 3097, 992, 293, 8873, 264, 912, 551, 322, 264, 1500, 992, 293], "temperature": 0.0, "avg_logprob": -0.15745914491832766, "compression_ratio": 2.103896103896104, "no_speech_prob": 4.092870312888408e-06}, {"id": 346, "seek": 163120, "start": 1653.32, "end": 1659.2, "text": " So each of these points now tells me how about it to go in the validation set how well did it go in the test set?", "tokens": [407, 1184, 295, 613, 2793, 586, 5112, 385, 577, 466, 309, 281, 352, 294, 264, 24071, 992, 577, 731, 630, 309, 352, 294, 264, 1500, 992, 30], "temperature": 0.0, "avg_logprob": -0.15745914491832766, "compression_ratio": 2.103896103896104, "no_speech_prob": 4.092870312888408e-06}, {"id": 347, "seek": 165920, "start": 1659.2, "end": 1665.2, "text": " And so if the validation set is useful we would say every time the validation set improves", "tokens": [400, 370, 498, 264, 24071, 992, 307, 4420, 321, 576, 584, 633, 565, 264, 24071, 992, 24771], "temperature": 0.0, "avg_logprob": -0.2437921389184817, "compression_ratio": 1.8602620087336244, "no_speech_prob": 4.495115263125626e-06}, {"id": 348, "seek": 165920, "start": 1665.44, "end": 1668.28, "text": " The test set should also score should also improve", "tokens": [440, 1500, 992, 820, 611, 6175, 820, 611, 3470], "temperature": 0.0, "avg_logprob": -0.2437921389184817, "compression_ratio": 1.8602620087336244, "no_speech_prob": 4.495115263125626e-06}, {"id": 349, "seek": 165920, "start": 1670.56, "end": 1675.76, "text": " Yeah, so you just said retreat dreaming retreating the model on training and validation", "tokens": [865, 11, 370, 291, 445, 848, 15505, 21475, 15505, 278, 264, 2316, 322, 3097, 293, 24071], "temperature": 0.0, "avg_logprob": -0.2437921389184817, "compression_ratio": 1.8602620087336244, "no_speech_prob": 4.495115263125626e-06}, {"id": 350, "seek": 165920, "start": 1675.76, "end": 1677.48, "text": " Yeah, that was a step. I was going back here", "tokens": [865, 11, 300, 390, 257, 1823, 13, 286, 390, 516, 646, 510], "temperature": 0.0, "avg_logprob": -0.2437921389184817, "compression_ratio": 1.8602620087336244, "no_speech_prob": 4.495115263125626e-06}, {"id": 351, "seek": 165920, "start": 1677.48, "end": 1682.96, "text": " So once I've got the validation score based on just the training set and then retrain it on the train and validation", "tokens": [407, 1564, 286, 600, 658, 264, 24071, 6175, 2361, 322, 445, 264, 3097, 992, 293, 550, 1533, 7146, 309, 322, 264, 3847, 293, 24071], "temperature": 0.0, "avg_logprob": -0.2437921389184817, "compression_ratio": 1.8602620087336244, "no_speech_prob": 4.495115263125626e-06}, {"id": 352, "seek": 165920, "start": 1683.24, "end": 1685.24, "text": " And check against it", "tokens": [400, 1520, 1970, 309], "temperature": 0.0, "avg_logprob": -0.2437921389184817, "compression_ratio": 1.8602620087336244, "no_speech_prob": 4.495115263125626e-06}, {"id": 353, "seek": 168524, "start": 1685.24, "end": 1691.44, "text": " Somebody else so just to clarify", "tokens": [13463, 1646, 370, 445, 281, 17594], "temperature": 0.0, "avg_logprob": -0.26823766072591143, "compression_ratio": 1.69375, "no_speech_prob": 9.368580322188791e-06}, {"id": 354, "seek": 168524, "start": 1693.84, "end": 1695.84, "text": " By test set you mean", "tokens": [3146, 1500, 992, 291, 914], "temperature": 0.0, "avg_logprob": -0.26823766072591143, "compression_ratio": 1.69375, "no_speech_prob": 9.368580322188791e-06}, {"id": 355, "seek": 168524, "start": 1697.44, "end": 1699.72, "text": " Submitting it to Kaggle and then checking the score", "tokens": [8511, 76, 2414, 309, 281, 48751, 22631, 293, 550, 8568, 264, 6175], "temperature": 0.0, "avg_logprob": -0.26823766072591143, "compression_ratio": 1.69375, "no_speech_prob": 9.368580322188791e-06}, {"id": 356, "seek": 168524, "start": 1700.4, "end": 1704.36, "text": " If it's Kaggle, then your test set is Kaggle's leaderboard", "tokens": [759, 309, 311, 48751, 22631, 11, 550, 428, 1500, 992, 307, 48751, 22631, 311, 5263, 3787], "temperature": 0.0, "avg_logprob": -0.26823766072591143, "compression_ratio": 1.69375, "no_speech_prob": 9.368580322188791e-06}, {"id": 357, "seek": 168524, "start": 1704.92, "end": 1711.8, "text": " In the real world the test set is this third data set that you put aside and it's that third data set that", "tokens": [682, 264, 957, 1002, 264, 1500, 992, 307, 341, 2636, 1412, 992, 300, 291, 829, 7359, 293, 309, 311, 300, 2636, 1412, 992, 300], "temperature": 0.0, "avg_logprob": -0.26823766072591143, "compression_ratio": 1.69375, "no_speech_prob": 9.368580322188791e-06}, {"id": 358, "seek": 171180, "start": 1711.8, "end": 1719.68, "text": " Having it reflect real-world production differences is the most important step in a machine learning project", "tokens": [10222, 309, 5031, 957, 12, 13217, 4265, 7300, 307, 264, 881, 1021, 1823, 294, 257, 3479, 2539, 1716], "temperature": 0.0, "avg_logprob": -0.16629003948635526, "compression_ratio": 1.8309859154929577, "no_speech_prob": 1.482352672610432e-06}, {"id": 359, "seek": 171180, "start": 1721.28, "end": 1728.24, "text": " Why is it the most important step because if you screw up everything else that you don't screw up that", "tokens": [1545, 307, 309, 264, 881, 1021, 1823, 570, 498, 291, 5630, 493, 1203, 1646, 300, 291, 500, 380, 5630, 493, 300], "temperature": 0.0, "avg_logprob": -0.16629003948635526, "compression_ratio": 1.8309859154929577, "no_speech_prob": 1.482352672610432e-06}, {"id": 360, "seek": 171180, "start": 1728.52, "end": 1730.52, "text": " You'll know you screwed up", "tokens": [509, 603, 458, 291, 20331, 493], "temperature": 0.0, "avg_logprob": -0.16629003948635526, "compression_ratio": 1.8309859154929577, "no_speech_prob": 1.482352672610432e-06}, {"id": 361, "seek": 171180, "start": 1730.68, "end": 1732.9199999999998, "text": " Right like if you've got a good test set", "tokens": [1779, 411, 498, 291, 600, 658, 257, 665, 1500, 992], "temperature": 0.0, "avg_logprob": -0.16629003948635526, "compression_ratio": 1.8309859154929577, "no_speech_prob": 1.482352672610432e-06}, {"id": 362, "seek": 171180, "start": 1733.3999999999999, "end": 1737.96, "text": " Then you'll know you screwed up because you screwed up something else and you tested it and it didn't work out", "tokens": [1396, 291, 603, 458, 291, 20331, 493, 570, 291, 20331, 493, 746, 1646, 293, 291, 8246, 309, 293, 309, 994, 380, 589, 484], "temperature": 0.0, "avg_logprob": -0.16629003948635526, "compression_ratio": 1.8309859154929577, "no_speech_prob": 1.482352672610432e-06}, {"id": 363, "seek": 173796, "start": 1737.96, "end": 1743.1200000000001, "text": " And it's like okay, you're not going to destroy the company, right if you screwed up creating the test set", "tokens": [400, 309, 311, 411, 1392, 11, 291, 434, 406, 516, 281, 5293, 264, 2237, 11, 558, 498, 291, 20331, 493, 4084, 264, 1500, 992], "temperature": 0.0, "avg_logprob": -0.14319467544555664, "compression_ratio": 1.8935361216730038, "no_speech_prob": 1.653683739277767e-06}, {"id": 364, "seek": 173796, "start": 1743.64, "end": 1747.8, "text": " That would be awful right because then you don't know if you've made a mistake", "tokens": [663, 576, 312, 11232, 558, 570, 550, 291, 500, 380, 458, 498, 291, 600, 1027, 257, 6146], "temperature": 0.0, "avg_logprob": -0.14319467544555664, "compression_ratio": 1.8935361216730038, "no_speech_prob": 1.653683739277767e-06}, {"id": 365, "seek": 173796, "start": 1748.4, "end": 1752.68, "text": " Right you try to build a model you test it on the test set it looks good", "tokens": [1779, 291, 853, 281, 1322, 257, 2316, 291, 1500, 309, 322, 264, 1500, 992, 309, 1542, 665], "temperature": 0.0, "avg_logprob": -0.14319467544555664, "compression_ratio": 1.8935361216730038, "no_speech_prob": 1.653683739277767e-06}, {"id": 366, "seek": 173796, "start": 1752.68, "end": 1755.3600000000001, "text": " But the test set was not indicative of real-world", "tokens": [583, 264, 1500, 992, 390, 406, 47513, 295, 957, 12, 13217], "temperature": 0.0, "avg_logprob": -0.14319467544555664, "compression_ratio": 1.8935361216730038, "no_speech_prob": 1.653683739277767e-06}, {"id": 367, "seek": 173796, "start": 1757.44, "end": 1758.8, "text": " Environment", "tokens": [35354], "temperature": 0.0, "avg_logprob": -0.14319467544555664, "compression_ratio": 1.8935361216730038, "no_speech_prob": 1.653683739277767e-06}, {"id": 368, "seek": 173796, "start": 1758.8, "end": 1761.52, "text": " So you don't actually know if you're going to destroy the company", "tokens": [407, 291, 500, 380, 767, 458, 498, 291, 434, 516, 281, 5293, 264, 2237], "temperature": 0.0, "avg_logprob": -0.14319467544555664, "compression_ratio": 1.8935361216730038, "no_speech_prob": 1.653683739277767e-06}, {"id": 369, "seek": 173796, "start": 1761.92, "end": 1766.72, "text": " Now hopefully you've got ways to put things into production gradually so you won't actually destroy the company", "tokens": [823, 4696, 291, 600, 658, 2098, 281, 829, 721, 666, 4265, 13145, 370, 291, 1582, 380, 767, 5293, 264, 2237], "temperature": 0.0, "avg_logprob": -0.14319467544555664, "compression_ratio": 1.8935361216730038, "no_speech_prob": 1.653683739277767e-06}, {"id": 370, "seek": 176672, "start": 1766.72, "end": 1772.84, "text": " But you'll at least destroy your reputation at work, right? It's like oh Jeremy tried to put this thing into production and", "tokens": [583, 291, 603, 412, 1935, 5293, 428, 13061, 412, 589, 11, 558, 30, 467, 311, 411, 1954, 17809, 3031, 281, 829, 341, 551, 666, 4265, 293], "temperature": 0.0, "avg_logprob": -0.1903132766973777, "compression_ratio": 1.7108843537414966, "no_speech_prob": 6.240878519747639e-06}, {"id": 371, "seek": 176672, "start": 1773.56, "end": 1780.0, "text": " In the first week the cohort we tried it on their sales halved and we're never going to give Jeremy a machine learning job again", "tokens": [682, 264, 700, 1243, 264, 28902, 321, 3031, 309, 322, 641, 5763, 7523, 937, 293, 321, 434, 1128, 516, 281, 976, 17809, 257, 3479, 2539, 1691, 797], "temperature": 0.0, "avg_logprob": -0.1903132766973777, "compression_ratio": 1.7108843537414966, "no_speech_prob": 6.240878519747639e-06}, {"id": 372, "seek": 176672, "start": 1780.2, "end": 1785.2, "text": " All right, but if Jeremy had used a proper test set then like he would have known oh", "tokens": [1057, 558, 11, 457, 498, 17809, 632, 1143, 257, 2296, 1500, 992, 550, 411, 415, 576, 362, 2570, 1954], "temperature": 0.0, "avg_logprob": -0.1903132766973777, "compression_ratio": 1.7108843537414966, "no_speech_prob": 6.240878519747639e-06}, {"id": 373, "seek": 176672, "start": 1785.64, "end": 1790.32, "text": " This is like half as good as my validation set said it would be I'll keep trying", "tokens": [639, 307, 411, 1922, 382, 665, 382, 452, 24071, 992, 848, 309, 576, 312, 286, 603, 1066, 1382], "temperature": 0.0, "avg_logprob": -0.1903132766973777, "compression_ratio": 1.7108843537414966, "no_speech_prob": 6.240878519747639e-06}, {"id": 374, "seek": 176672, "start": 1790.84, "end": 1794.56, "text": " And now I'm not going to get in any trouble. I was actually like oh Jeremy's awesome", "tokens": [400, 586, 286, 478, 406, 516, 281, 483, 294, 604, 5253, 13, 286, 390, 767, 411, 1954, 17809, 311, 3476], "temperature": 0.0, "avg_logprob": -0.1903132766973777, "compression_ratio": 1.7108843537414966, "no_speech_prob": 6.240878519747639e-06}, {"id": 375, "seek": 179456, "start": 1794.56, "end": 1799.2, "text": " He is identifies ahead of time when there's going to be a generalization problem", "tokens": [634, 307, 34597, 2286, 295, 565, 562, 456, 311, 516, 281, 312, 257, 2674, 2144, 1154], "temperature": 0.0, "avg_logprob": -0.14614259495454676, "compression_ratio": 1.532258064516129, "no_speech_prob": 1.903376301015669e-06}, {"id": 376, "seek": 179456, "start": 1805.36, "end": 1809.0, "text": " Okay, so this is like", "tokens": [1033, 11, 370, 341, 307, 411], "temperature": 0.0, "avg_logprob": -0.14614259495454676, "compression_ratio": 1.532258064516129, "no_speech_prob": 1.903376301015669e-06}, {"id": 377, "seek": 179456, "start": 1811.28, "end": 1816.1599999999999, "text": " This is something that kind of everybody talks about a little bit in machine learning classes", "tokens": [639, 307, 746, 300, 733, 295, 2201, 6686, 466, 257, 707, 857, 294, 3479, 2539, 5359], "temperature": 0.0, "avg_logprob": -0.14614259495454676, "compression_ratio": 1.532258064516129, "no_speech_prob": 1.903376301015669e-06}, {"id": 378, "seek": 179456, "start": 1816.1599999999999, "end": 1820.86, "text": " But often it kind of stops at the point where you learn that there's a thing in SK learn", "tokens": [583, 2049, 309, 733, 295, 10094, 412, 264, 935, 689, 291, 1466, 300, 456, 311, 257, 551, 294, 21483, 1466], "temperature": 0.0, "avg_logprob": -0.14614259495454676, "compression_ratio": 1.532258064516129, "no_speech_prob": 1.903376301015669e-06}, {"id": 379, "seek": 182086, "start": 1820.86, "end": 1827.5, "text": " Called make test train split and it returns these things and off you go right, but the fact that like", "tokens": [45001, 652, 1500, 3847, 7472, 293, 309, 11247, 613, 721, 293, 766, 291, 352, 558, 11, 457, 264, 1186, 300, 411], "temperature": 0.0, "avg_logprob": -0.18321772984095983, "compression_ratio": 1.6715686274509804, "no_speech_prob": 3.668834779091412e-06}, {"id": 380, "seek": 182086, "start": 1828.06, "end": 1831.26, "text": " Or here's the cross validation function, right? So", "tokens": [1610, 510, 311, 264, 3278, 24071, 2445, 11, 558, 30, 407], "temperature": 0.0, "avg_logprob": -0.18321772984095983, "compression_ratio": 1.6715686274509804, "no_speech_prob": 3.668834779091412e-06}, {"id": 381, "seek": 182086, "start": 1832.74, "end": 1839.1399999999999, "text": " The fact that these things always give you random samples tells you that like", "tokens": [440, 1186, 300, 613, 721, 1009, 976, 291, 4974, 10938, 5112, 291, 300, 411], "temperature": 0.0, "avg_logprob": -0.18321772984095983, "compression_ratio": 1.6715686274509804, "no_speech_prob": 3.668834779091412e-06}, {"id": 382, "seek": 182086, "start": 1839.74, "end": 1842.86, "text": " Much if not most of the time you shouldn't be using them", "tokens": [12313, 498, 406, 881, 295, 264, 565, 291, 4659, 380, 312, 1228, 552], "temperature": 0.0, "avg_logprob": -0.18321772984095983, "compression_ratio": 1.6715686274509804, "no_speech_prob": 3.668834779091412e-06}, {"id": 383, "seek": 182086, "start": 1844.74, "end": 1847.86, "text": " The fact that random forest gives you an OOB for free", "tokens": [440, 1186, 300, 4974, 6719, 2709, 291, 364, 422, 46, 33, 337, 1737], "temperature": 0.0, "avg_logprob": -0.18321772984095983, "compression_ratio": 1.6715686274509804, "no_speech_prob": 3.668834779091412e-06}, {"id": 384, "seek": 184786, "start": 1847.86, "end": 1854.8799999999999, "text": " It's useful, but it only tells you that this generalizes in a statistical sense not in a practical sense, right?", "tokens": [467, 311, 4420, 11, 457, 309, 787, 5112, 291, 300, 341, 2674, 5660, 294, 257, 22820, 2020, 406, 294, 257, 8496, 2020, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1529821862979811, "compression_ratio": 1.6600790513833992, "no_speech_prob": 3.668848421511939e-06}, {"id": 385, "seek": 184786, "start": 1854.8999999999999, "end": 1859.1599999999999, "text": " so then finally there's cross validation right which", "tokens": [370, 550, 2721, 456, 311, 3278, 24071, 558, 597], "temperature": 0.0, "avg_logprob": -0.1529821862979811, "compression_ratio": 1.6600790513833992, "no_speech_prob": 3.668848421511939e-06}, {"id": 386, "seek": 184786, "start": 1861.26, "end": 1865.74, "text": " Outside of class you guys have been talking about a lot which makes me feel somebody's been", "tokens": [28218, 295, 1508, 291, 1074, 362, 668, 1417, 466, 257, 688, 597, 1669, 385, 841, 2618, 311, 668], "temperature": 0.0, "avg_logprob": -0.1529821862979811, "compression_ratio": 1.6600790513833992, "no_speech_prob": 3.668848421511939e-06}, {"id": 387, "seek": 184786, "start": 1866.86, "end": 1868.86, "text": " overemphasizing the value of this technique", "tokens": [38657, 76, 7485, 3319, 264, 2158, 295, 341, 6532], "temperature": 0.0, "avg_logprob": -0.1529821862979811, "compression_ratio": 1.6600790513833992, "no_speech_prob": 3.668848421511939e-06}, {"id": 388, "seek": 184786, "start": 1869.34, "end": 1875.7199999999998, "text": " So I'll explain what cross validation is and then I'll explain why you probably shouldn't be using it most of the time", "tokens": [407, 286, 603, 2903, 437, 3278, 24071, 307, 293, 550, 286, 603, 2903, 983, 291, 1391, 4659, 380, 312, 1228, 309, 881, 295, 264, 565], "temperature": 0.0, "avg_logprob": -0.1529821862979811, "compression_ratio": 1.6600790513833992, "no_speech_prob": 3.668848421511939e-06}, {"id": 389, "seek": 187572, "start": 1875.72, "end": 1882.52, "text": " So cross validation says let's not just pull out one validation set, but let's pull out five", "tokens": [407, 3278, 24071, 1619, 718, 311, 406, 445, 2235, 484, 472, 24071, 992, 11, 457, 718, 311, 2235, 484, 1732], "temperature": 0.0, "avg_logprob": -0.1815373102823893, "compression_ratio": 1.8031088082901554, "no_speech_prob": 1.9637984678411158e-06}, {"id": 390, "seek": 187572, "start": 1882.72, "end": 1890.16, "text": " Say so let's assume that we're going to randomly shuffle the data first, right? This is critical, right?", "tokens": [6463, 370, 718, 311, 6552, 300, 321, 434, 516, 281, 16979, 39426, 264, 1412, 700, 11, 558, 30, 639, 307, 4924, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1815373102823893, "compression_ratio": 1.8031088082901554, "no_speech_prob": 1.9637984678411158e-06}, {"id": 391, "seek": 187572, "start": 1890.16, "end": 1893.84, "text": " We first randomly shuffle the data and then we're going to split it into", "tokens": [492, 700, 16979, 39426, 264, 1412, 293, 550, 321, 434, 516, 281, 7472, 309, 666], "temperature": 0.0, "avg_logprob": -0.1815373102823893, "compression_ratio": 1.8031088082901554, "no_speech_prob": 1.9637984678411158e-06}, {"id": 392, "seek": 189384, "start": 1893.84, "end": 1904.08, "text": " Five groups and then for model number one, we'll call this the validation set and", "tokens": [9436, 3935, 293, 550, 337, 2316, 1230, 472, 11, 321, 603, 818, 341, 264, 24071, 992, 293], "temperature": 0.0, "avg_logprob": -0.2569855616642879, "compression_ratio": 1.6339869281045751, "no_speech_prob": 1.8738631979431375e-06}, {"id": 393, "seek": 189384, "start": 1906.12, "end": 1908.56, "text": " We'll call this the training set", "tokens": [492, 603, 818, 341, 264, 3097, 992], "temperature": 0.0, "avg_logprob": -0.2569855616642879, "compression_ratio": 1.6339869281045751, "no_speech_prob": 1.8738631979431375e-06}, {"id": 394, "seek": 189384, "start": 1910.6799999999998, "end": 1914.9599999999998, "text": " Okay, and we'll train and we'll check against the validation and we'll get some", "tokens": [1033, 11, 293, 321, 603, 3847, 293, 321, 603, 1520, 1970, 264, 24071, 293, 321, 603, 483, 512], "temperature": 0.0, "avg_logprob": -0.2569855616642879, "compression_ratio": 1.6339869281045751, "no_speech_prob": 1.8738631979431375e-06}, {"id": 395, "seek": 189384, "start": 1915.6, "end": 1919.48, "text": " RMS a R squared whatever and then we'll throw that away", "tokens": [497, 10288, 257, 497, 8889, 2035, 293, 550, 321, 603, 3507, 300, 1314], "temperature": 0.0, "avg_logprob": -0.2569855616642879, "compression_ratio": 1.6339869281045751, "no_speech_prob": 1.8738631979431375e-06}, {"id": 396, "seek": 191948, "start": 1919.48, "end": 1927.52, "text": " And we'll call this the validation set and we'll call this", "tokens": [400, 321, 603, 818, 341, 264, 24071, 992, 293, 321, 603, 818, 341], "temperature": 0.0, "avg_logprob": -0.23413551554960363, "compression_ratio": 1.5901639344262295, "no_speech_prob": 1.8162087371820235e-06}, {"id": 397, "seek": 191948, "start": 1930.76, "end": 1936.88, "text": " The training set and we'll get another score we'll do that five times", "tokens": [440, 3097, 992, 293, 321, 603, 483, 1071, 6175, 321, 603, 360, 300, 1732, 1413], "temperature": 0.0, "avg_logprob": -0.23413551554960363, "compression_ratio": 1.5901639344262295, "no_speech_prob": 1.8162087371820235e-06}, {"id": 398, "seek": 191948, "start": 1939.32, "end": 1941.32, "text": " And then we'll take the average", "tokens": [400, 550, 321, 603, 747, 264, 4274], "temperature": 0.0, "avg_logprob": -0.23413551554960363, "compression_ratio": 1.5901639344262295, "no_speech_prob": 1.8162087371820235e-06}, {"id": 399, "seek": 194132, "start": 1941.32, "end": 1948.28, "text": " Average okay, so that's a cross validation", "tokens": [316, 3623, 1392, 11, 370, 300, 311, 257, 3278, 24071], "temperature": 0.0, "avg_logprob": -0.27299532397040005, "compression_ratio": 1.4755244755244756, "no_speech_prob": 1.3006650988245383e-05}, {"id": 400, "seek": 194132, "start": 1949.52, "end": 1950.48, "text": " average", "tokens": [4274], "temperature": 0.0, "avg_logprob": -0.27299532397040005, "compression_ratio": 1.4755244755244756, "no_speech_prob": 1.3006650988245383e-05}, {"id": 401, "seek": 194132, "start": 1950.48, "end": 1956.32, "text": " Accuracy, so who can tell me like a benefit of using cross validation over a", "tokens": [5725, 374, 2551, 11, 370, 567, 393, 980, 385, 411, 257, 5121, 295, 1228, 3278, 24071, 670, 257], "temperature": 0.0, "avg_logprob": -0.27299532397040005, "compression_ratio": 1.4755244755244756, "no_speech_prob": 1.3006650988245383e-05}, {"id": 402, "seek": 194132, "start": 1957.1599999999999, "end": 1960.12, "text": " The kind of standard validation set I talked about before", "tokens": [440, 733, 295, 3832, 24071, 992, 286, 2825, 466, 949], "temperature": 0.0, "avg_logprob": -0.27299532397040005, "compression_ratio": 1.4755244755244756, "no_speech_prob": 1.3006650988245383e-05}, {"id": 403, "seek": 194132, "start": 1961.1599999999999, "end": 1963.1599999999999, "text": " Could you pass it to fun?", "tokens": [7497, 291, 1320, 309, 281, 1019, 30], "temperature": 0.0, "avg_logprob": -0.27299532397040005, "compression_ratio": 1.4755244755244756, "no_speech_prob": 1.3006650988245383e-05}, {"id": 404, "seek": 196316, "start": 1963.16, "end": 1974.44, "text": " If you have a small data set is in a cross validation will make use over the data you have", "tokens": [759, 291, 362, 257, 1359, 1412, 992, 307, 294, 257, 3278, 24071, 486, 652, 764, 670, 264, 1412, 291, 362], "temperature": 0.0, "avg_logprob": -0.28598811229070026, "compression_ratio": 1.6550218340611353, "no_speech_prob": 1.922254523378797e-05}, {"id": 405, "seek": 196316, "start": 1974.44, "end": 1976.0400000000002, "text": " Yeah, you can use all of the data", "tokens": [865, 11, 291, 393, 764, 439, 295, 264, 1412], "temperature": 0.0, "avg_logprob": -0.28598811229070026, "compression_ratio": 1.6550218340611353, "no_speech_prob": 1.922254523378797e-05}, {"id": 406, "seek": 196316, "start": 1976.0400000000002, "end": 1980.7, "text": " You don't have to put anything aside and you kind of get a little benefit as well in that like", "tokens": [509, 500, 380, 362, 281, 829, 1340, 7359, 293, 291, 733, 295, 483, 257, 707, 5121, 382, 731, 294, 300, 411], "temperature": 0.0, "avg_logprob": -0.28598811229070026, "compression_ratio": 1.6550218340611353, "no_speech_prob": 1.922254523378797e-05}, {"id": 407, "seek": 196316, "start": 1980.88, "end": 1986.38, "text": " You've now got five models that you could ensemble together each one of used which used 80% of the data", "tokens": [509, 600, 586, 658, 1732, 5245, 300, 291, 727, 19492, 1214, 1184, 472, 295, 1143, 597, 1143, 4688, 4, 295, 264, 1412], "temperature": 0.0, "avg_logprob": -0.28598811229070026, "compression_ratio": 1.6550218340611353, "no_speech_prob": 1.922254523378797e-05}, {"id": 408, "seek": 196316, "start": 1986.38, "end": 1988.68, "text": " So, you know sometimes that on some link can be helpful", "tokens": [407, 11, 291, 458, 2171, 300, 322, 512, 2113, 393, 312, 4961], "temperature": 0.0, "avg_logprob": -0.28598811229070026, "compression_ratio": 1.6550218340611353, "no_speech_prob": 1.922254523378797e-05}, {"id": 409, "seek": 198868, "start": 1988.68, "end": 1990.68, "text": " I'm", "tokens": [286, 478], "temperature": 0.0, "avg_logprob": -0.26412005483368295, "compression_ratio": 1.643979057591623, "no_speech_prob": 1.7778149413061328e-05}, {"id": 410, "seek": 198868, "start": 1992.0800000000002, "end": 1996.8400000000001, "text": " Fun could you tell me like what what could be some reasons that you wouldn't use cross validation?", "tokens": [11166, 727, 291, 980, 385, 411, 437, 437, 727, 312, 512, 4112, 300, 291, 2759, 380, 764, 3278, 24071, 30], "temperature": 0.0, "avg_logprob": -0.26412005483368295, "compression_ratio": 1.643979057591623, "no_speech_prob": 1.7778149413061328e-05}, {"id": 411, "seek": 198868, "start": 1997.96, "end": 2005.3, "text": " We have enough data. So we don't not want the validation set to be included in the model trainings", "tokens": [492, 362, 1547, 1412, 13, 407, 321, 500, 380, 406, 528, 264, 24071, 992, 281, 312, 5556, 294, 264, 2316, 33856], "temperature": 0.0, "avg_logprob": -0.26412005483368295, "compression_ratio": 1.643979057591623, "no_speech_prob": 1.7778149413061328e-05}, {"id": 412, "seek": 198868, "start": 2006.28, "end": 2007.44, "text": " process", "tokens": [1399], "temperature": 0.0, "avg_logprob": -0.26412005483368295, "compression_ratio": 1.643979057591623, "no_speech_prob": 1.7778149413061328e-05}, {"id": 413, "seek": 198868, "start": 2007.44, "end": 2010.2, "text": " to like to pollute like", "tokens": [281, 411, 281, 6418, 1169, 411], "temperature": 0.0, "avg_logprob": -0.26412005483368295, "compression_ratio": 1.643979057591623, "no_speech_prob": 1.7778149413061328e-05}, {"id": 414, "seek": 198868, "start": 2012.4, "end": 2014.4, "text": " Okay, yeah", "tokens": [1033, 11, 1338], "temperature": 0.0, "avg_logprob": -0.26412005483368295, "compression_ratio": 1.643979057591623, "no_speech_prob": 1.7778149413061328e-05}, {"id": 415, "seek": 201440, "start": 2014.4, "end": 2021.76, "text": " I'm not sure the cross validation is necessarily polluting the model. What would be a key like downside of cross validation?", "tokens": [286, 478, 406, 988, 264, 3278, 24071, 307, 4725, 6418, 10861, 264, 2316, 13, 708, 576, 312, 257, 2141, 411, 25060, 295, 3278, 24071, 30], "temperature": 0.0, "avg_logprob": -0.3290706047644982, "compression_ratio": 1.709016393442623, "no_speech_prob": 8.26770337880589e-06}, {"id": 416, "seek": 201440, "start": 2021.92, "end": 2026.2800000000002, "text": " but like for deep learning if you have learned the pictures and", "tokens": [457, 411, 337, 2452, 2539, 498, 291, 362, 3264, 264, 5242, 293], "temperature": 0.0, "avg_logprob": -0.3290706047644982, "compression_ratio": 1.709016393442623, "no_speech_prob": 8.26770337880589e-06}, {"id": 417, "seek": 201440, "start": 2026.96, "end": 2032.48, "text": " Then you're gonna work will know the pictures and it's more likely to predict this as a right", "tokens": [1396, 291, 434, 799, 589, 486, 458, 264, 5242, 293, 309, 311, 544, 3700, 281, 6069, 341, 382, 257, 558], "temperature": 0.0, "avg_logprob": -0.3290706047644982, "compression_ratio": 1.709016393442623, "no_speech_prob": 8.26770337880589e-06}, {"id": 418, "seek": 201440, "start": 2032.48, "end": 2039.16, "text": " So sure, but if we if we put aside some data each time in the cross validation, can you pass it to Suraj?", "tokens": [407, 988, 11, 457, 498, 321, 498, 321, 829, 7359, 512, 1412, 1184, 565, 294, 264, 3278, 24071, 11, 393, 291, 1320, 309, 281, 6732, 1805, 30], "temperature": 0.0, "avg_logprob": -0.3290706047644982, "compression_ratio": 1.709016393442623, "no_speech_prob": 8.26770337880589e-06}, {"id": 419, "seek": 201440, "start": 2039.16, "end": 2042.0400000000002, "text": " I'm I'm not so worried about", "tokens": [286, 478, 286, 478, 406, 370, 5804, 466], "temperature": 0.0, "avg_logprob": -0.3290706047644982, "compression_ratio": 1.709016393442623, "no_speech_prob": 8.26770337880589e-06}, {"id": 420, "seek": 204204, "start": 2042.04, "end": 2046.6399999999999, "text": " Like I don't think there's like one of these validation sets is", "tokens": [1743, 286, 500, 380, 519, 456, 311, 411, 472, 295, 613, 24071, 6352, 307], "temperature": 0.0, "avg_logprob": -0.21924459017240083, "compression_ratio": 1.5445544554455446, "no_speech_prob": 1.3419649803836364e-05}, {"id": 421, "seek": 204204, "start": 2048.24, "end": 2051.4, "text": " More statistically accurate. Yes, sir. I", "tokens": [5048, 36478, 8559, 13, 1079, 11, 4735, 13, 286], "temperature": 0.0, "avg_logprob": -0.21924459017240083, "compression_ratio": 1.5445544554455446, "no_speech_prob": 1.3419649803836364e-05}, {"id": 422, "seek": 204204, "start": 2057.2, "end": 2062.68, "text": " Think that's what fun was worried about I don't see why that would happen like each time we're fitting a model just behind you", "tokens": [6557, 300, 311, 437, 1019, 390, 5804, 466, 286, 500, 380, 536, 983, 300, 576, 1051, 411, 1184, 565, 321, 434, 15669, 257, 2316, 445, 2261, 291], "temperature": 0.0, "avg_logprob": -0.21924459017240083, "compression_ratio": 1.5445544554455446, "no_speech_prob": 1.3419649803836364e-05}, {"id": 423, "seek": 204204, "start": 2063.48, "end": 2068.7, "text": " Each time we're fitting a model. We are absolutely holding out 20% of the sample", "tokens": [6947, 565, 321, 434, 15669, 257, 2316, 13, 492, 366, 3122, 5061, 484, 945, 4, 295, 264, 6889], "temperature": 0.0, "avg_logprob": -0.21924459017240083, "compression_ratio": 1.5445544554455446, "no_speech_prob": 1.3419649803836364e-05}, {"id": 424, "seek": 206870, "start": 2068.7, "end": 2072.58, "text": " All right. So yes, the five models between them have seen all of the data", "tokens": [1057, 558, 13, 407, 2086, 11, 264, 1732, 5245, 1296, 552, 362, 1612, 439, 295, 264, 1412], "temperature": 0.0, "avg_logprob": -0.20511919765149134, "compression_ratio": 1.6987951807228916, "no_speech_prob": 3.6688281852548243e-06}, {"id": 425, "seek": 206870, "start": 2072.58, "end": 2076.62, "text": " But but it's kind of like a random forest and that is a lot like a random first", "tokens": [583, 457, 309, 311, 733, 295, 411, 257, 4974, 6719, 293, 300, 307, 257, 688, 411, 257, 4974, 700], "temperature": 0.0, "avg_logprob": -0.20511919765149134, "compression_ratio": 1.6987951807228916, "no_speech_prob": 3.6688281852548243e-06}, {"id": 426, "seek": 206870, "start": 2076.62, "end": 2079.7, "text": " Each model has only been trained on the subset of the data", "tokens": [6947, 2316, 575, 787, 668, 8895, 322, 264, 25993, 295, 264, 1412], "temperature": 0.0, "avg_logprob": -0.20511919765149134, "compression_ratio": 1.6987951807228916, "no_speech_prob": 3.6688281852548243e-06}, {"id": 427, "seek": 206870, "start": 2080.46, "end": 2085.66, "text": " Yes, Nisha see if it is like a large data set like it will take a lot of time. Oh, yes, exactly", "tokens": [1079, 11, 426, 16546, 536, 498, 309, 307, 411, 257, 2416, 1412, 992, 411, 309, 486, 747, 257, 688, 295, 565, 13, 876, 11, 2086, 11, 2293], "temperature": 0.0, "avg_logprob": -0.20511919765149134, "compression_ratio": 1.6987951807228916, "no_speech_prob": 3.6688281852548243e-06}, {"id": 428, "seek": 206870, "start": 2085.9399999999996, "end": 2091.46, "text": " Right, so we have to fit five models rather than one. So here's a key downside number one", "tokens": [1779, 11, 370, 321, 362, 281, 3318, 1732, 5245, 2831, 813, 472, 13, 407, 510, 311, 257, 2141, 25060, 1230, 472], "temperature": 0.0, "avg_logprob": -0.20511919765149134, "compression_ratio": 1.6987951807228916, "no_speech_prob": 3.6688281852548243e-06}, {"id": 429, "seek": 206870, "start": 2092.3799999999997, "end": 2095.02, "text": " Is time and so if we're?", "tokens": [1119, 565, 293, 370, 498, 321, 434, 30], "temperature": 0.0, "avg_logprob": -0.20511919765149134, "compression_ratio": 1.6987951807228916, "no_speech_prob": 3.6688281852548243e-06}, {"id": 430, "seek": 209502, "start": 2095.02, "end": 2101.2599999999998, "text": " Doing deep learning and it takes a day to run suddenly it now takes five days or we need five GPUs", "tokens": [18496, 2452, 2539, 293, 309, 2516, 257, 786, 281, 1190, 5800, 309, 586, 2516, 1732, 1708, 420, 321, 643, 1732, 18407, 82], "temperature": 0.0, "avg_logprob": -0.21351499871893242, "compression_ratio": 1.5502183406113537, "no_speech_prob": 9.080296877073124e-06}, {"id": 431, "seek": 209502, "start": 2102.58, "end": 2106.74, "text": " Okay, what about my earlier issues about validation sets? Do you want to pass it over there?", "tokens": [1033, 11, 437, 466, 452, 3071, 2663, 466, 24071, 6352, 30, 1144, 291, 528, 281, 1320, 309, 670, 456, 30], "temperature": 0.0, "avg_logprob": -0.21351499871893242, "compression_ratio": 1.5502183406113537, "no_speech_prob": 9.080296877073124e-06}, {"id": 432, "seek": 209502, "start": 2107.74, "end": 2109.74, "text": " What's your name Jose?", "tokens": [708, 311, 428, 1315, 8635, 30], "temperature": 0.0, "avg_logprob": -0.21351499871893242, "compression_ratio": 1.5502183406113537, "no_speech_prob": 9.080296877073124e-06}, {"id": 433, "seek": 209502, "start": 2111.5, "end": 2119.34, "text": " So if you had like temporal data wouldn't you be like by shuffling when you be breaking that relation", "tokens": [407, 498, 291, 632, 411, 30881, 1412, 2759, 380, 291, 312, 411, 538, 402, 1245, 1688, 562, 291, 312, 7697, 300, 9721], "temperature": 0.0, "avg_logprob": -0.21351499871893242, "compression_ratio": 1.5502183406113537, "no_speech_prob": 9.080296877073124e-06}, {"id": 434, "seek": 209502, "start": 2119.94, "end": 2121.98, "text": " Well, we could unshuffle it afterwards", "tokens": [1042, 11, 321, 727, 2693, 71, 21665, 309, 10543], "temperature": 0.0, "avg_logprob": -0.21351499871893242, "compression_ratio": 1.5502183406113537, "no_speech_prob": 9.080296877073124e-06}, {"id": 435, "seek": 212198, "start": 2121.98, "end": 2127.22, "text": " We could reorder it like we could shuffle get the training set out and then sort it by time", "tokens": [492, 727, 319, 4687, 309, 411, 321, 727, 39426, 483, 264, 3097, 992, 484, 293, 550, 1333, 309, 538, 565], "temperature": 0.0, "avg_logprob": -0.158441694656221, "compression_ratio": 1.7355371900826446, "no_speech_prob": 6.4389878389192745e-06}, {"id": 436, "seek": 212198, "start": 2128.82, "end": 2132.3, "text": " Like I'd like this presumably there's a date column there. So I", "tokens": [1743, 286, 1116, 411, 341, 26742, 456, 311, 257, 4002, 7738, 456, 13, 407, 286], "temperature": 0.0, "avg_logprob": -0.158441694656221, "compression_ratio": 1.7355371900826446, "no_speech_prob": 6.4389878389192745e-06}, {"id": 437, "seek": 212198, "start": 2133.1, "end": 2136.7400000000002, "text": " Don't think I don't think it's going to stop us from building a model. Did you have?", "tokens": [1468, 380, 519, 286, 500, 380, 519, 309, 311, 516, 281, 1590, 505, 490, 2390, 257, 2316, 13, 2589, 291, 362, 30], "temperature": 0.0, "avg_logprob": -0.158441694656221, "compression_ratio": 1.7355371900826446, "no_speech_prob": 6.4389878389192745e-06}, {"id": 438, "seek": 212198, "start": 2143.7, "end": 2147.38, "text": " With cross validation you're building five even validation sets", "tokens": [2022, 3278, 24071, 291, 434, 2390, 1732, 754, 24071, 6352], "temperature": 0.0, "avg_logprob": -0.158441694656221, "compression_ratio": 1.7355371900826446, "no_speech_prob": 6.4389878389192745e-06}, {"id": 439, "seek": 214738, "start": 2147.38, "end": 2153.02, "text": " And if there is some sort of structure that you're trying to capture in your validation set to mirror your test set you're essentially", "tokens": [400, 498, 456, 307, 512, 1333, 295, 3877, 300, 291, 434, 1382, 281, 7983, 294, 428, 24071, 992, 281, 8013, 428, 1500, 992, 291, 434, 4476], "temperature": 0.0, "avg_logprob": -0.15818656027854025, "compression_ratio": 1.942084942084942, "no_speech_prob": 7.889149856055155e-06}, {"id": 440, "seek": 214738, "start": 2153.02, "end": 2155.1800000000003, "text": " Just throwing that a chance to construct that", "tokens": [1449, 10238, 300, 257, 2931, 281, 7690, 300], "temperature": 0.0, "avg_logprob": -0.15818656027854025, "compression_ratio": 1.942084942084942, "no_speech_prob": 7.889149856055155e-06}, {"id": 441, "seek": 214738, "start": 2155.9, "end": 2157.42, "text": " yourself", "tokens": [1803], "temperature": 0.0, "avg_logprob": -0.15818656027854025, "compression_ratio": 1.942084942084942, "no_speech_prob": 7.889149856055155e-06}, {"id": 442, "seek": 214738, "start": 2157.42, "end": 2162.2200000000003, "text": " Right. I think you're going to say that I think you said the same thing as I'm going to say which is which is that", "tokens": [1779, 13, 286, 519, 291, 434, 516, 281, 584, 300, 286, 519, 291, 848, 264, 912, 551, 382, 286, 478, 516, 281, 584, 597, 307, 597, 307, 300], "temperature": 0.0, "avg_logprob": -0.15818656027854025, "compression_ratio": 1.942084942084942, "no_speech_prob": 7.889149856055155e-06}, {"id": 443, "seek": 214738, "start": 2162.2200000000003, "end": 2170.26, "text": " Our earlier concerns about why random validation sets are a problem are entirely relevant here. All these validation sets are random", "tokens": [2621, 3071, 7389, 466, 983, 4974, 24071, 6352, 366, 257, 1154, 366, 7696, 7340, 510, 13, 1057, 613, 24071, 6352, 366, 4974], "temperature": 0.0, "avg_logprob": -0.15818656027854025, "compression_ratio": 1.942084942084942, "no_speech_prob": 7.889149856055155e-06}, {"id": 444, "seek": 214738, "start": 2171.3, "end": 2175.1, "text": " So if a random validation set is not appropriate for your problem", "tokens": [407, 498, 257, 4974, 24071, 992, 307, 406, 6854, 337, 428, 1154], "temperature": 0.0, "avg_logprob": -0.15818656027854025, "compression_ratio": 1.942084942084942, "no_speech_prob": 7.889149856055155e-06}, {"id": 445, "seek": 217510, "start": 2175.1, "end": 2176.62, "text": " most", "tokens": [881], "temperature": 0.0, "avg_logprob": -0.21508279660852944, "compression_ratio": 1.6894736842105262, "no_speech_prob": 6.048876002751058e-06}, {"id": 446, "seek": 217510, "start": 2176.62, "end": 2179.5, "text": " likely because for example of temporal issues", "tokens": [3700, 570, 337, 1365, 295, 30881, 2663], "temperature": 0.0, "avg_logprob": -0.21508279660852944, "compression_ratio": 1.6894736842105262, "no_speech_prob": 6.048876002751058e-06}, {"id": 447, "seek": 217510, "start": 2179.9, "end": 2184.66, "text": " Then none of these four validation set five validation sets are any good. They're all random", "tokens": [1396, 6022, 295, 613, 1451, 24071, 992, 1732, 24071, 6352, 366, 604, 665, 13, 814, 434, 439, 4974], "temperature": 0.0, "avg_logprob": -0.21508279660852944, "compression_ratio": 1.6894736842105262, "no_speech_prob": 6.048876002751058e-06}, {"id": 448, "seek": 217510, "start": 2184.94, "end": 2187.5, "text": " right and so if you have", "tokens": [558, 293, 370, 498, 291, 362], "temperature": 0.0, "avg_logprob": -0.21508279660852944, "compression_ratio": 1.6894736842105262, "no_speech_prob": 6.048876002751058e-06}, {"id": 449, "seek": 217510, "start": 2188.66, "end": 2191.18, "text": " Temporal data like we did here", "tokens": [8095, 2816, 304, 1412, 411, 321, 630, 510], "temperature": 0.0, "avg_logprob": -0.21508279660852944, "compression_ratio": 1.6894736842105262, "no_speech_prob": 6.048876002751058e-06}, {"id": 450, "seek": 217510, "start": 2191.54, "end": 2197.1, "text": " There's no way to do cross validation really or like probably no good way to do cross validation. I mean", "tokens": [821, 311, 572, 636, 281, 360, 3278, 24071, 534, 420, 411, 1391, 572, 665, 636, 281, 360, 3278, 24071, 13, 286, 914], "temperature": 0.0, "avg_logprob": -0.21508279660852944, "compression_ratio": 1.6894736842105262, "no_speech_prob": 6.048876002751058e-06}, {"id": 451, "seek": 217510, "start": 2197.94, "end": 2199.74, "text": " You want to have", "tokens": [509, 528, 281, 362], "temperature": 0.0, "avg_logprob": -0.21508279660852944, "compression_ratio": 1.6894736842105262, "no_speech_prob": 6.048876002751058e-06}, {"id": 452, "seek": 219974, "start": 2199.74, "end": 2206.22, "text": " Your validation set be as close to the test set as possible. And so you can't do that by randomly sampling different things", "tokens": [2260, 24071, 992, 312, 382, 1998, 281, 264, 1500, 992, 382, 1944, 13, 400, 370, 291, 393, 380, 360, 300, 538, 16979, 21179, 819, 721], "temperature": 0.0, "avg_logprob": -0.18688166375253715, "compression_ratio": 1.6463878326996197, "no_speech_prob": 2.9944237667223206e-06}, {"id": 453, "seek": 219974, "start": 2207.4199999999996, "end": 2209.4199999999996, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.18688166375253715, "compression_ratio": 1.6463878326996197, "no_speech_prob": 2.9944237667223206e-06}, {"id": 454, "seek": 219974, "start": 2209.4599999999996, "end": 2212.7799999999997, "text": " So as fun said you may well not need", "tokens": [407, 382, 1019, 848, 291, 815, 731, 406, 643], "temperature": 0.0, "avg_logprob": -0.18688166375253715, "compression_ratio": 1.6463878326996197, "no_speech_prob": 2.9944237667223206e-06}, {"id": 455, "seek": 219974, "start": 2213.2999999999997, "end": 2216.2599999999998, "text": " To do cross validation because most of the time in the real world", "tokens": [1407, 360, 3278, 24071, 570, 881, 295, 264, 565, 294, 264, 957, 1002], "temperature": 0.0, "avg_logprob": -0.18688166375253715, "compression_ratio": 1.6463878326996197, "no_speech_prob": 2.9944237667223206e-06}, {"id": 456, "seek": 219974, "start": 2216.2599999999998, "end": 2223.8599999999997, "text": " We don't really have that little data right unless your data is based on some very very expensive labeling process or some", "tokens": [492, 500, 380, 534, 362, 300, 707, 1412, 558, 5969, 428, 1412, 307, 2361, 322, 512, 588, 588, 5124, 40244, 1399, 420, 512], "temperature": 0.0, "avg_logprob": -0.18688166375253715, "compression_ratio": 1.6463878326996197, "no_speech_prob": 2.9944237667223206e-06}, {"id": 457, "seek": 219974, "start": 2224.1, "end": 2227.2999999999997, "text": " Experiments that take a little cost a lot to run or whatever but nowadays that's", "tokens": [12522, 8321, 300, 747, 257, 707, 2063, 257, 688, 281, 1190, 420, 2035, 457, 13434, 300, 311], "temperature": 0.0, "avg_logprob": -0.18688166375253715, "compression_ratio": 1.6463878326996197, "no_speech_prob": 2.9944237667223206e-06}, {"id": 458, "seek": 222730, "start": 2227.3, "end": 2234.1000000000004, "text": " Data scientists are not very often doing that kind of work summer in which cases is an issue, but most of us aren't", "tokens": [11888, 7708, 366, 406, 588, 2049, 884, 300, 733, 295, 589, 4266, 294, 597, 3331, 307, 364, 2734, 11, 457, 881, 295, 505, 3212, 380], "temperature": 0.0, "avg_logprob": -0.18416957855224608, "compression_ratio": 1.6993464052287581, "no_speech_prob": 3.39311691277544e-06}, {"id": 459, "seek": 222730, "start": 2234.82, "end": 2236.82, "text": " so we probably don't need to as", "tokens": [370, 321, 1391, 500, 380, 643, 281, 382], "temperature": 0.0, "avg_logprob": -0.18416957855224608, "compression_ratio": 1.6993464052287581, "no_speech_prob": 3.39311691277544e-06}, {"id": 460, "seek": 222730, "start": 2237.3, "end": 2240.54, "text": " Nishan said if we do do it, it's going to take a whole lot of time", "tokens": [426, 742, 282, 848, 498, 321, 360, 360, 309, 11, 309, 311, 516, 281, 747, 257, 1379, 688, 295, 565], "temperature": 0.0, "avg_logprob": -0.18416957855224608, "compression_ratio": 1.6993464052287581, "no_speech_prob": 3.39311691277544e-06}, {"id": 461, "seek": 222730, "start": 2240.98, "end": 2246.3, "text": " Right and then as Ernest said even if we did do it and we took up all that time", "tokens": [1779, 293, 550, 382, 24147, 377, 848, 754, 498, 321, 630, 360, 309, 293, 321, 1890, 493, 439, 300, 565], "temperature": 0.0, "avg_logprob": -0.18416957855224608, "compression_ratio": 1.6993464052287581, "no_speech_prob": 3.39311691277544e-06}, {"id": 462, "seek": 222730, "start": 2246.3, "end": 2250.6800000000003, "text": " It might give us totally the wrong answer because random validation sets are inappropriate for our problem", "tokens": [467, 1062, 976, 505, 3879, 264, 2085, 1867, 570, 4974, 24071, 6352, 366, 26723, 337, 527, 1154], "temperature": 0.0, "avg_logprob": -0.18416957855224608, "compression_ratio": 1.6993464052287581, "no_speech_prob": 3.39311691277544e-06}, {"id": 463, "seek": 222730, "start": 2251.1000000000004, "end": 2257.1800000000003, "text": " Okay, so I'm not going to be spending much time on cross validation because I just I think it's an interesting tool to", "tokens": [1033, 11, 370, 286, 478, 406, 516, 281, 312, 6434, 709, 565, 322, 3278, 24071, 570, 286, 445, 286, 519, 309, 311, 364, 1880, 2290, 281], "temperature": 0.0, "avg_logprob": -0.18416957855224608, "compression_ratio": 1.6993464052287581, "no_speech_prob": 3.39311691277544e-06}, {"id": 464, "seek": 225718, "start": 2257.18, "end": 2262.06, "text": " Have it's easy to use SK learn has a cross validation thing you can go ahead and use", "tokens": [3560, 309, 311, 1858, 281, 764, 21483, 1466, 575, 257, 3278, 24071, 551, 291, 393, 352, 2286, 293, 764], "temperature": 0.0, "avg_logprob": -0.1806922711824116, "compression_ratio": 1.590643274853801, "no_speech_prob": 1.3006811968807597e-05}, {"id": 465, "seek": 225718, "start": 2262.94, "end": 2264.58, "text": " but", "tokens": [457], "temperature": 0.0, "avg_logprob": -0.1806922711824116, "compression_ratio": 1.590643274853801, "no_speech_prob": 1.3006811968807597e-05}, {"id": 466, "seek": 225718, "start": 2264.58, "end": 2270.7, "text": " It's it's it's not that often that it's going to be an important part of your toolbox in my opinion", "tokens": [467, 311, 309, 311, 309, 311, 406, 300, 2049, 300, 309, 311, 516, 281, 312, 364, 1021, 644, 295, 428, 44593, 294, 452, 4800], "temperature": 0.0, "avg_logprob": -0.1806922711824116, "compression_ratio": 1.590643274853801, "no_speech_prob": 1.3006811968807597e-05}, {"id": 467, "seek": 225718, "start": 2270.7, "end": 2272.7, "text": " It'll come up sometimes", "tokens": [467, 603, 808, 493, 2171], "temperature": 0.0, "avg_logprob": -0.1806922711824116, "compression_ratio": 1.590643274853801, "no_speech_prob": 1.3006811968807597e-05}, {"id": 468, "seek": 225718, "start": 2278.3399999999997, "end": 2280.3399999999997, "text": " Okay, so that is", "tokens": [1033, 11, 370, 300, 307], "temperature": 0.0, "avg_logprob": -0.1806922711824116, "compression_ratio": 1.590643274853801, "no_speech_prob": 1.3006811968807597e-05}, {"id": 469, "seek": 225718, "start": 2281.1, "end": 2283.7799999999997, "text": " validation sets so then the other thing we", "tokens": [24071, 6352, 370, 550, 264, 661, 551, 321], "temperature": 0.0, "avg_logprob": -0.1806922711824116, "compression_ratio": 1.590643274853801, "no_speech_prob": 1.3006811968807597e-05}, {"id": 470, "seek": 228378, "start": 2283.78, "end": 2286.82, "text": " Started talking about last week", "tokens": [39715, 1417, 466, 1036, 1243], "temperature": 0.0, "avg_logprob": -0.20607394642300075, "compression_ratio": 1.5303867403314917, "no_speech_prob": 2.090439465973759e-06}, {"id": 471, "seek": 228378, "start": 2289.5, "end": 2295.8, "text": " And got a little bit stuck on because I screwed it up was tree interpretation", "tokens": [400, 658, 257, 707, 857, 5541, 322, 570, 286, 20331, 309, 493, 390, 4230, 14174], "temperature": 0.0, "avg_logprob": -0.20607394642300075, "compression_ratio": 1.5303867403314917, "no_speech_prob": 2.090439465973759e-06}, {"id": 472, "seek": 228378, "start": 2296.5800000000004, "end": 2298.5800000000004, "text": " So I'm actually going to cover that again", "tokens": [407, 286, 478, 767, 516, 281, 2060, 300, 797], "temperature": 0.0, "avg_logprob": -0.20607394642300075, "compression_ratio": 1.5303867403314917, "no_speech_prob": 2.090439465973759e-06}, {"id": 473, "seek": 228378, "start": 2299.6200000000003, "end": 2301.6200000000003, "text": " without the error", "tokens": [1553, 264, 6713], "temperature": 0.0, "avg_logprob": -0.20607394642300075, "compression_ratio": 1.5303867403314917, "no_speech_prob": 2.090439465973759e-06}, {"id": 474, "seek": 228378, "start": 2301.6200000000003, "end": 2303.6200000000003, "text": " And dig into it in a bit more detail", "tokens": [400, 2528, 666, 309, 294, 257, 857, 544, 2607], "temperature": 0.0, "avg_logprob": -0.20607394642300075, "compression_ratio": 1.5303867403314917, "no_speech_prob": 2.090439465973759e-06}, {"id": 475, "seek": 228378, "start": 2304.98, "end": 2307.36, "text": " So can anybody tell me?", "tokens": [407, 393, 4472, 980, 385, 30], "temperature": 0.0, "avg_logprob": -0.20607394642300075, "compression_ratio": 1.5303867403314917, "no_speech_prob": 2.090439465973759e-06}, {"id": 476, "seek": 230736, "start": 2307.36, "end": 2313.7200000000003, "text": " What tree interpreter does and how it does it?", "tokens": [708, 4230, 34132, 775, 293, 577, 309, 775, 309, 30], "temperature": 0.0, "avg_logprob": -0.17601908760509272, "compression_ratio": 1.6256410256410256, "no_speech_prob": 5.122882953401131e-07}, {"id": 477, "seek": 230736, "start": 2317.0, "end": 2321.4, "text": " What do you remember it's a difficult one to explain I don't think I did a good job of explaining it", "tokens": [708, 360, 291, 1604, 309, 311, 257, 2252, 472, 281, 2903, 286, 500, 380, 519, 286, 630, 257, 665, 1691, 295, 13468, 309], "temperature": 0.0, "avg_logprob": -0.17601908760509272, "compression_ratio": 1.6256410256410256, "no_speech_prob": 5.122882953401131e-07}, {"id": 478, "seek": 230736, "start": 2321.4, "end": 2325.42, "text": " So don't worry if you don't do a great job, but does anybody want to have a go at explaining it?", "tokens": [407, 500, 380, 3292, 498, 291, 500, 380, 360, 257, 869, 1691, 11, 457, 775, 4472, 528, 281, 362, 257, 352, 412, 13468, 309, 30], "temperature": 0.0, "avg_logprob": -0.17601908760509272, "compression_ratio": 1.6256410256410256, "no_speech_prob": 5.122882953401131e-07}, {"id": 479, "seek": 230736, "start": 2327.6400000000003, "end": 2329.6400000000003, "text": " Okay, that's fine, so", "tokens": [1033, 11, 300, 311, 2489, 11, 370], "temperature": 0.0, "avg_logprob": -0.17601908760509272, "compression_ratio": 1.6256410256410256, "no_speech_prob": 5.122882953401131e-07}, {"id": 480, "seek": 230736, "start": 2331.4, "end": 2334.8, "text": " Let's start with the output of tree interpreter so", "tokens": [961, 311, 722, 365, 264, 5598, 295, 4230, 34132, 370], "temperature": 0.0, "avg_logprob": -0.17601908760509272, "compression_ratio": 1.6256410256410256, "no_speech_prob": 5.122882953401131e-07}, {"id": 481, "seek": 233480, "start": 2334.8, "end": 2340.36, "text": " if we look at a single model a single tree in other words", "tokens": [498, 321, 574, 412, 257, 2167, 2316, 257, 2167, 4230, 294, 661, 2283], "temperature": 0.0, "avg_logprob": -0.1560414980535638, "compression_ratio": 1.5662650602409638, "no_speech_prob": 1.101590441976441e-06}, {"id": 482, "seek": 233480, "start": 2342.4, "end": 2344.4, "text": " Here is a single tree", "tokens": [1692, 307, 257, 2167, 4230], "temperature": 0.0, "avg_logprob": -0.1560414980535638, "compression_ratio": 1.5662650602409638, "no_speech_prob": 1.101590441976441e-06}, {"id": 483, "seek": 233480, "start": 2346.0, "end": 2347.6000000000004, "text": " Okay, and", "tokens": [1033, 11, 293], "temperature": 0.0, "avg_logprob": -0.1560414980535638, "compression_ratio": 1.5662650602409638, "no_speech_prob": 1.101590441976441e-06}, {"id": 484, "seek": 233480, "start": 2347.6000000000004, "end": 2354.26, "text": " So to remind us the top of a tree is before there's been any split at all", "tokens": [407, 281, 4160, 505, 264, 1192, 295, 257, 4230, 307, 949, 456, 311, 668, 604, 7472, 412, 439], "temperature": 0.0, "avg_logprob": -0.1560414980535638, "compression_ratio": 1.5662650602409638, "no_speech_prob": 1.101590441976441e-06}, {"id": 485, "seek": 233480, "start": 2355.2400000000002, "end": 2357.2400000000002, "text": " so ten point one eight nine", "tokens": [370, 2064, 935, 472, 3180, 4949], "temperature": 0.0, "avg_logprob": -0.1560414980535638, "compression_ratio": 1.5662650602409638, "no_speech_prob": 1.101590441976441e-06}, {"id": 486, "seek": 235724, "start": 2357.24, "end": 2363.7999999999997, "text": " Is the average log price of all of the options in our training set?", "tokens": [1119, 264, 4274, 3565, 3218, 295, 439, 295, 264, 3956, 294, 527, 3097, 992, 30], "temperature": 0.0, "avg_logprob": -0.2749142352445626, "compression_ratio": 1.5989304812834224, "no_speech_prob": 2.6425705073052086e-06}, {"id": 487, "seek": 235724, "start": 2364.8399999999997, "end": 2367.64, "text": " So I'm going to go ahead and draw", "tokens": [407, 286, 478, 516, 281, 352, 2286, 293, 2642], "temperature": 0.0, "avg_logprob": -0.2749142352445626, "compression_ratio": 1.5989304812834224, "no_speech_prob": 2.6425705073052086e-06}, {"id": 488, "seek": 235724, "start": 2369.04, "end": 2373.9199999999996, "text": " Right here ten point one eight nine is the average of all", "tokens": [1779, 510, 2064, 935, 472, 3180, 4949, 307, 264, 4274, 295, 439], "temperature": 0.0, "avg_logprob": -0.2749142352445626, "compression_ratio": 1.5989304812834224, "no_speech_prob": 2.6425705073052086e-06}, {"id": 489, "seek": 235724, "start": 2374.64, "end": 2379.12, "text": " Okay, and then if I go a couple of system less than or equal to point five", "tokens": [1033, 11, 293, 550, 498, 286, 352, 257, 1916, 295, 1185, 1570, 813, 420, 2681, 281, 935, 1732], "temperature": 0.0, "avg_logprob": -0.2749142352445626, "compression_ratio": 1.5989304812834224, "no_speech_prob": 2.6425705073052086e-06}, {"id": 490, "seek": 235724, "start": 2380.12, "end": 2385.08, "text": " Then I get ten point three four five okay, so for this subset of", "tokens": [1396, 286, 483, 2064, 935, 1045, 1451, 1732, 1392, 11, 370, 337, 341, 25993, 295], "temperature": 0.0, "avg_logprob": -0.2749142352445626, "compression_ratio": 1.5989304812834224, "no_speech_prob": 2.6425705073052086e-06}, {"id": 491, "seek": 238508, "start": 2385.08, "end": 2387.08, "text": " 16,800", "tokens": [3165, 11, 14423], "temperature": 0.0, "avg_logprob": -0.2829541858873869, "compression_ratio": 1.8955223880597014, "no_speech_prob": 3.785304897974129e-06}, {"id": 492, "seek": 238508, "start": 2388.68, "end": 2392.44, "text": " Cuppler is less than or equal to point five the average is ten point three four five and", "tokens": [383, 10504, 1918, 307, 1570, 813, 420, 2681, 281, 935, 1732, 264, 4274, 307, 2064, 935, 1045, 1451, 1732, 293], "temperature": 0.0, "avg_logprob": -0.2829541858873869, "compression_ratio": 1.8955223880597014, "no_speech_prob": 3.785304897974129e-06}, {"id": 493, "seek": 238508, "start": 2394.08, "end": 2398.24, "text": " Then off the people with a couple of system less than or equal to point five", "tokens": [1396, 766, 264, 561, 365, 257, 1916, 295, 1185, 1570, 813, 420, 2681, 281, 935, 1732], "temperature": 0.0, "avg_logprob": -0.2829541858873869, "compression_ratio": 1.8955223880597014, "no_speech_prob": 3.785304897974129e-06}, {"id": 494, "seek": 238508, "start": 2398.4, "end": 2405.7999999999997, "text": " We then take the subset where enclosure is less than or equal to two and the average there of log sale price is nine point nine", "tokens": [492, 550, 747, 264, 25993, 689, 34093, 307, 1570, 813, 420, 2681, 281, 732, 293, 264, 4274, 456, 295, 3565, 8680, 3218, 307, 4949, 935, 4949], "temperature": 0.0, "avg_logprob": -0.2829541858873869, "compression_ratio": 1.8955223880597014, "no_speech_prob": 3.785304897974129e-06}, {"id": 495, "seek": 238508, "start": 2405.7999999999997, "end": 2411.4, "text": " Five five so here's nine point nine five five and then final step in our tree is", "tokens": [9436, 1732, 370, 510, 311, 4949, 935, 4949, 1732, 1732, 293, 550, 2572, 1823, 294, 527, 4230, 307], "temperature": 0.0, "avg_logprob": -0.2829541858873869, "compression_ratio": 1.8955223880597014, "no_speech_prob": 3.785304897974129e-06}, {"id": 496, "seek": 241140, "start": 2411.4, "end": 2413.4, "text": " Is", "tokens": [1119], "temperature": 0.0, "avg_logprob": -0.2530533926827567, "compression_ratio": 1.5178571428571428, "no_speech_prob": 4.15734393754974e-06}, {"id": 497, "seek": 241140, "start": 2414.2400000000002, "end": 2421.84, "text": " Model ID just for this group with no coupler system with enclosure less than or equal to two then let's just take model ID less than or", "tokens": [17105, 7348, 445, 337, 341, 1594, 365, 572, 1384, 22732, 1185, 365, 34093, 1570, 813, 420, 2681, 281, 732, 550, 718, 311, 445, 747, 2316, 7348, 1570, 813, 420], "temperature": 0.0, "avg_logprob": -0.2530533926827567, "compression_ratio": 1.5178571428571428, "no_speech_prob": 4.15734393754974e-06}, {"id": 498, "seek": 241140, "start": 2421.84, "end": 2424.0, "text": " equal to 45 73 and", "tokens": [2681, 281, 6905, 28387, 293], "temperature": 0.0, "avg_logprob": -0.2530533926827567, "compression_ratio": 1.5178571428571428, "no_speech_prob": 4.15734393754974e-06}, {"id": 499, "seek": 241140, "start": 2425.04, "end": 2428.28, "text": " That gives us ten point two two six", "tokens": [663, 2709, 505, 2064, 935, 732, 732, 2309], "temperature": 0.0, "avg_logprob": -0.2530533926827567, "compression_ratio": 1.5178571428571428, "no_speech_prob": 4.15734393754974e-06}, {"id": 500, "seek": 241140, "start": 2430.1600000000003, "end": 2432.88, "text": " Okay, so then we can say all right starting with", "tokens": [1033, 11, 370, 550, 321, 393, 584, 439, 558, 2891, 365], "temperature": 0.0, "avg_logprob": -0.2530533926827567, "compression_ratio": 1.5178571428571428, "no_speech_prob": 4.15734393754974e-06}, {"id": 501, "seek": 243288, "start": 2432.88, "end": 2440.32, "text": " Ten point one oh nine one eight nine average for everybody in our training set for this particular tree sub sample of 20,000", "tokens": [9380, 935, 472, 1954, 4949, 472, 3180, 4949, 4274, 337, 2201, 294, 527, 3097, 992, 337, 341, 1729, 4230, 1422, 6889, 295, 945, 11, 1360], "temperature": 0.0, "avg_logprob": -0.1938094002859933, "compression_ratio": 1.8481012658227849, "no_speech_prob": 3.1875506465439685e-06}, {"id": 502, "seek": 243288, "start": 2442.4, "end": 2445.04, "text": " Adding in the coupler decision or", "tokens": [31204, 294, 264, 1384, 22732, 3537, 420], "temperature": 0.0, "avg_logprob": -0.1938094002859933, "compression_ratio": 1.8481012658227849, "no_speech_prob": 3.1875506465439685e-06}, {"id": 503, "seek": 243288, "start": 2445.48, "end": 2447.48, "text": " coupler less than or equal to point five", "tokens": [1384, 22732, 1570, 813, 420, 2681, 281, 935, 1732], "temperature": 0.0, "avg_logprob": -0.1938094002859933, "compression_ratio": 1.8481012658227849, "no_speech_prob": 3.1875506465439685e-06}, {"id": 504, "seek": 243288, "start": 2447.6400000000003, "end": 2454.6800000000003, "text": " Increased our prediction by point one five six so if we predict it with a naive model of just the mean it would have been ten point", "tokens": [30367, 1937, 527, 17630, 538, 935, 472, 1732, 2309, 370, 498, 321, 6069, 309, 365, 257, 29052, 2316, 295, 445, 264, 914, 309, 576, 362, 668, 2064, 935], "temperature": 0.0, "avg_logprob": -0.1938094002859933, "compression_ratio": 1.8481012658227849, "no_speech_prob": 3.1875506465439685e-06}, {"id": 505, "seek": 243288, "start": 2454.6800000000003, "end": 2456.12, "text": " one oh nine", "tokens": [472, 1954, 4949], "temperature": 0.0, "avg_logprob": -0.1938094002859933, "compression_ratio": 1.8481012658227849, "no_speech_prob": 3.1875506465439685e-06}, {"id": 506, "seek": 243288, "start": 2456.12, "end": 2461.48, "text": " Adding in just the coupler decision would have changed it to ten point three four five so this", "tokens": [31204, 294, 445, 264, 1384, 22732, 3537, 576, 362, 3105, 309, 281, 2064, 935, 1045, 1451, 1732, 370, 341], "temperature": 0.0, "avg_logprob": -0.1938094002859933, "compression_ratio": 1.8481012658227849, "no_speech_prob": 3.1875506465439685e-06}, {"id": 507, "seek": 246148, "start": 2461.48, "end": 2466.28, "text": " Variable is responsible for a point one five six increase in our prediction", "tokens": [32511, 712, 307, 6250, 337, 257, 935, 472, 1732, 2309, 3488, 294, 527, 17630], "temperature": 0.0, "avg_logprob": -0.16762255459296993, "compression_ratio": 1.8737864077669903, "no_speech_prob": 2.406093472018256e-06}, {"id": 508, "seek": 246148, "start": 2466.88, "end": 2472.4, "text": " From that the enclosure decision was responsible for a minus point three nine five decrease", "tokens": [3358, 300, 264, 34093, 3537, 390, 6250, 337, 257, 3175, 935, 1045, 4949, 1732, 11514], "temperature": 0.0, "avg_logprob": -0.16762255459296993, "compression_ratio": 1.8737864077669903, "no_speech_prob": 2.406093472018256e-06}, {"id": 509, "seek": 246148, "start": 2473.12, "end": 2480.28, "text": " The model ID was responsible for a point two seven six increase until eventually that was our final decision", "tokens": [440, 2316, 7348, 390, 6250, 337, 257, 935, 732, 3407, 2309, 3488, 1826, 4728, 300, 390, 527, 2572, 3537], "temperature": 0.0, "avg_logprob": -0.16762255459296993, "compression_ratio": 1.8737864077669903, "no_speech_prob": 2.406093472018256e-06}, {"id": 510, "seek": 246148, "start": 2480.44, "end": 2485.6, "text": " That was our prediction for this auction of this particular sale price", "tokens": [663, 390, 527, 17630, 337, 341, 24139, 295, 341, 1729, 8680, 3218], "temperature": 0.0, "avg_logprob": -0.16762255459296993, "compression_ratio": 1.8737864077669903, "no_speech_prob": 2.406093472018256e-06}, {"id": 511, "seek": 246148, "start": 2486.32, "end": 2488.44, "text": " So we can draw that as what's called a", "tokens": [407, 321, 393, 2642, 300, 382, 437, 311, 1219, 257], "temperature": 0.0, "avg_logprob": -0.16762255459296993, "compression_ratio": 1.8737864077669903, "no_speech_prob": 2.406093472018256e-06}, {"id": 512, "seek": 248844, "start": 2488.44, "end": 2493.44, "text": " Waterfall plot right and waterfall plots are one of the most useful plots", "tokens": [8772, 6691, 7542, 558, 293, 27848, 28609, 366, 472, 295, 264, 881, 4420, 28609], "temperature": 0.0, "avg_logprob": -0.13923269448821077, "compression_ratio": 1.8382978723404255, "no_speech_prob": 4.09288668379304e-06}, {"id": 513, "seek": 248844, "start": 2493.44, "end": 2498.32, "text": " I know about and weirdly enough there's nothing in Python to do them", "tokens": [286, 458, 466, 293, 48931, 1547, 456, 311, 1825, 294, 15329, 281, 360, 552], "temperature": 0.0, "avg_logprob": -0.13923269448821077, "compression_ratio": 1.8382978723404255, "no_speech_prob": 4.09288668379304e-06}, {"id": 514, "seek": 248844, "start": 2498.32, "end": 2505.44, "text": " and this is one of these things where there's this disconnect between like the world of like management consulting and business where everybody uses waterfall plots all", "tokens": [293, 341, 307, 472, 295, 613, 721, 689, 456, 311, 341, 14299, 1296, 411, 264, 1002, 295, 411, 4592, 23682, 293, 1606, 689, 2201, 4960, 27848, 28609, 439], "temperature": 0.0, "avg_logprob": -0.13923269448821077, "compression_ratio": 1.8382978723404255, "no_speech_prob": 4.09288668379304e-06}, {"id": 515, "seek": 248844, "start": 2505.44, "end": 2506.76, "text": " the time and", "tokens": [264, 565, 293], "temperature": 0.0, "avg_logprob": -0.13923269448821077, "compression_ratio": 1.8382978723404255, "no_speech_prob": 4.09288668379304e-06}, {"id": 516, "seek": 248844, "start": 2506.76, "end": 2508.4, "text": " like academia", "tokens": [411, 28937], "temperature": 0.0, "avg_logprob": -0.13923269448821077, "compression_ratio": 1.8382978723404255, "no_speech_prob": 4.09288668379304e-06}, {"id": 517, "seek": 248844, "start": 2508.4, "end": 2513.36, "text": " Who have no idea what these things are but like every time like you're looking at say", "tokens": [2102, 362, 572, 1558, 437, 613, 721, 366, 457, 411, 633, 565, 411, 291, 434, 1237, 412, 584], "temperature": 0.0, "avg_logprob": -0.13923269448821077, "compression_ratio": 1.8382978723404255, "no_speech_prob": 4.09288668379304e-06}, {"id": 518, "seek": 248844, "start": 2514.7200000000003, "end": 2516.2000000000003, "text": " Here is", "tokens": [1692, 307], "temperature": 0.0, "avg_logprob": -0.13923269448821077, "compression_ratio": 1.8382978723404255, "no_speech_prob": 4.09288668379304e-06}, {"id": 519, "seek": 251620, "start": 2516.2, "end": 2522.04, "text": " Last year's sales for Apple and then there was a change in the iPhones increased by this amount", "tokens": [5264, 1064, 311, 5763, 337, 6373, 293, 550, 456, 390, 257, 1319, 294, 264, 43793, 6505, 538, 341, 2372], "temperature": 0.0, "avg_logprob": -0.18866299270489895, "compression_ratio": 1.8676470588235294, "no_speech_prob": 1.3211728401074652e-05}, {"id": 520, "seek": 251620, "start": 2522.3199999999997, "end": 2527.8399999999997, "text": " Macs decreased by that amount and iPads increased by that amount every time you have a starting point", "tokens": [5707, 82, 24436, 538, 300, 2372, 293, 5180, 5834, 6505, 538, 300, 2372, 633, 565, 291, 362, 257, 2891, 935], "temperature": 0.0, "avg_logprob": -0.18866299270489895, "compression_ratio": 1.8676470588235294, "no_speech_prob": 1.3211728401074652e-05}, {"id": 521, "seek": 251620, "start": 2527.96, "end": 2534.12, "text": " In a number of changes and a finishing point waterfall charts are pretty much always the best way to show it so here", "tokens": [682, 257, 1230, 295, 2962, 293, 257, 12693, 935, 27848, 17767, 366, 1238, 709, 1009, 264, 1151, 636, 281, 855, 309, 370, 510], "temperature": 0.0, "avg_logprob": -0.18866299270489895, "compression_ratio": 1.8676470588235294, "no_speech_prob": 1.3211728401074652e-05}, {"id": 522, "seek": 251620, "start": 2534.3999999999996, "end": 2537.64, "text": " Our prediction for price based on everything ten point one eight nine", "tokens": [2621, 17630, 337, 3218, 2361, 322, 1203, 2064, 935, 472, 3180, 4949], "temperature": 0.0, "avg_logprob": -0.18866299270489895, "compression_ratio": 1.8676470588235294, "no_speech_prob": 1.3211728401074652e-05}, {"id": 523, "seek": 251620, "start": 2538.3199999999997, "end": 2545.2, "text": " There was an increase blue means increase of point one five six the coupler decrease of point three nine five for enclosure", "tokens": [821, 390, 364, 3488, 3344, 1355, 3488, 295, 935, 472, 1732, 2309, 264, 1384, 22732, 11514, 295, 935, 1045, 4949, 1732, 337, 34093], "temperature": 0.0, "avg_logprob": -0.18866299270489895, "compression_ratio": 1.8676470588235294, "no_speech_prob": 1.3211728401074652e-05}, {"id": 524, "seek": 254520, "start": 2545.2, "end": 2550.24, "text": " Increase model ID of point two seven six so decrease", "tokens": [30367, 651, 2316, 7348, 295, 935, 732, 3407, 2309, 370, 11514], "temperature": 0.0, "avg_logprob": -0.20980738568049606, "compression_ratio": 1.6637554585152838, "no_speech_prob": 1.1843000720546115e-05}, {"id": 525, "seek": 254520, "start": 2550.7999999999997, "end": 2558.0, "text": " Sorry increase decrease increase to get to our final ten point two six six so you see how waterfall chart works", "tokens": [4919, 3488, 11514, 3488, 281, 483, 281, 527, 2572, 2064, 935, 732, 2309, 2309, 370, 291, 536, 577, 27848, 6927, 1985], "temperature": 0.0, "avg_logprob": -0.20980738568049606, "compression_ratio": 1.6637554585152838, "no_speech_prob": 1.1843000720546115e-05}, {"id": 526, "seek": 254520, "start": 2558.52, "end": 2565.8799999999997, "text": " So with Excel 2016 you it's built-in you just click insert waterfall chart and there it is if you want to be a hero", "tokens": [407, 365, 19060, 6549, 291, 309, 311, 3094, 12, 259, 291, 445, 2052, 8969, 27848, 6927, 293, 456, 309, 307, 498, 291, 528, 281, 312, 257, 5316], "temperature": 0.0, "avg_logprob": -0.20980738568049606, "compression_ratio": 1.6637554585152838, "no_speech_prob": 1.1843000720546115e-05}, {"id": 527, "seek": 254520, "start": 2566.9199999999996, "end": 2568.9199999999996, "text": " Create a waterfall chart", "tokens": [20248, 257, 27848, 6927], "temperature": 0.0, "avg_logprob": -0.20980738568049606, "compression_ratio": 1.6637554585152838, "no_speech_prob": 1.1843000720546115e-05}, {"id": 528, "seek": 254520, "start": 2569.56, "end": 2573.8399999999997, "text": " Package for a map plot let put it on pip and everybody will love you for it", "tokens": [18466, 609, 337, 257, 4471, 7542, 718, 829, 309, 322, 8489, 293, 2201, 486, 959, 291, 337, 309], "temperature": 0.0, "avg_logprob": -0.20980738568049606, "compression_ratio": 1.6637554585152838, "no_speech_prob": 1.1843000720546115e-05}, {"id": 529, "seek": 257384, "start": 2573.84, "end": 2575.96, "text": " There are some like really crappy", "tokens": [821, 366, 512, 411, 534, 36531], "temperature": 0.0, "avg_logprob": -0.17560047015809177, "compression_ratio": 1.723021582733813, "no_speech_prob": 5.862768830411369e-06}, {"id": 530, "seek": 257384, "start": 2577.1200000000003, "end": 2579.1200000000003, "text": " Gists and manual", "tokens": [460, 1751, 293, 9688], "temperature": 0.0, "avg_logprob": -0.17560047015809177, "compression_ratio": 1.723021582733813, "no_speech_prob": 5.862768830411369e-06}, {"id": 531, "seek": 257384, "start": 2579.36, "end": 2587.56, "text": " Notebooks and stuff around these are actually super easy to build like you basically do a stacked column plot where this the bottom", "tokens": [11633, 15170, 293, 1507, 926, 613, 366, 767, 1687, 1858, 281, 1322, 411, 291, 1936, 360, 257, 28867, 7738, 7542, 689, 341, 264, 2767], "temperature": 0.0, "avg_logprob": -0.17560047015809177, "compression_ratio": 1.723021582733813, "no_speech_prob": 5.862768830411369e-06}, {"id": 532, "seek": 257384, "start": 2587.56, "end": 2589.36, "text": " Of this is like all white", "tokens": [2720, 341, 307, 411, 439, 2418], "temperature": 0.0, "avg_logprob": -0.17560047015809177, "compression_ratio": 1.723021582733813, "no_speech_prob": 5.862768830411369e-06}, {"id": 533, "seek": 257384, "start": 2589.36, "end": 2590.96, "text": " Right like you can kind of do it", "tokens": [1779, 411, 291, 393, 733, 295, 360, 309], "temperature": 0.0, "avg_logprob": -0.17560047015809177, "compression_ratio": 1.723021582733813, "no_speech_prob": 5.862768830411369e-06}, {"id": 534, "seek": 257384, "start": 2590.96, "end": 2596.56, "text": " But if you can wrap that up all and put the data the points in the right spots and color them nicely", "tokens": [583, 498, 291, 393, 7019, 300, 493, 439, 293, 829, 264, 1412, 264, 2793, 294, 264, 558, 10681, 293, 2017, 552, 9594], "temperature": 0.0, "avg_logprob": -0.17560047015809177, "compression_ratio": 1.723021582733813, "no_speech_prob": 5.862768830411369e-06}, {"id": 535, "seek": 259656, "start": 2596.56, "end": 2603.36, "text": " That would be totally awesome. I think you've all got the skills to do it and would make you know be a terrific thing for your portfolio", "tokens": [663, 576, 312, 3879, 3476, 13, 286, 519, 291, 600, 439, 658, 264, 3942, 281, 360, 309, 293, 576, 652, 291, 458, 312, 257, 20899, 551, 337, 428, 12583], "temperature": 0.0, "avg_logprob": -0.1499071717262268, "compression_ratio": 1.6504065040650406, "no_speech_prob": 3.3931273719645105e-06}, {"id": 536, "seek": 259656, "start": 2604.68, "end": 2606.68, "text": " So there's an idea", "tokens": [407, 456, 311, 364, 1558], "temperature": 0.0, "avg_logprob": -0.1499071717262268, "compression_ratio": 1.6504065040650406, "no_speech_prob": 3.3931273719645105e-06}, {"id": 537, "seek": 259656, "start": 2607.84, "end": 2613.18, "text": " Could make an interesting cattle kernel even like here's how to build a waterfall plot from scratch and by the way", "tokens": [7497, 652, 364, 1880, 19992, 28256, 754, 411, 510, 311, 577, 281, 1322, 257, 27848, 7542, 490, 8459, 293, 538, 264, 636], "temperature": 0.0, "avg_logprob": -0.1499071717262268, "compression_ratio": 1.6504065040650406, "no_speech_prob": 3.3931273719645105e-06}, {"id": 538, "seek": 259656, "start": 2613.18, "end": 2615.36, "text": " I've been put this up on tip you can all use it", "tokens": [286, 600, 668, 829, 341, 493, 322, 4125, 291, 393, 439, 764, 309], "temperature": 0.0, "avg_logprob": -0.1499071717262268, "compression_ratio": 1.6504065040650406, "no_speech_prob": 3.3931273719645105e-06}, {"id": 539, "seek": 259656, "start": 2618.04, "end": 2623.16, "text": " So in general therefore obviously going from the all and then going through each change", "tokens": [407, 294, 2674, 4412, 2745, 516, 490, 264, 439, 293, 550, 516, 807, 1184, 1319], "temperature": 0.0, "avg_logprob": -0.1499071717262268, "compression_ratio": 1.6504065040650406, "no_speech_prob": 3.3931273719645105e-06}, {"id": 540, "seek": 262316, "start": 2623.16, "end": 2628.94, "text": " Then the sum of all of those is going to be equal to the final prediction", "tokens": [1396, 264, 2408, 295, 439, 295, 729, 307, 516, 281, 312, 2681, 281, 264, 2572, 17630], "temperature": 0.0, "avg_logprob": -0.14141784830296295, "compression_ratio": 1.7149122807017543, "no_speech_prob": 6.144131020846544e-06}, {"id": 541, "seek": 262316, "start": 2630.3999999999996, "end": 2633.48, "text": " So that's how we could say if we were just doing a decision tree", "tokens": [407, 300, 311, 577, 321, 727, 584, 498, 321, 645, 445, 884, 257, 3537, 4230], "temperature": 0.0, "avg_logprob": -0.14141784830296295, "compression_ratio": 1.7149122807017543, "no_speech_prob": 6.144131020846544e-06}, {"id": 542, "seek": 262316, "start": 2633.7599999999998, "end": 2639.56, "text": " Then you know you're coming along and saying like how come this particular option was this particular price", "tokens": [1396, 291, 458, 291, 434, 1348, 2051, 293, 1566, 411, 577, 808, 341, 1729, 3614, 390, 341, 1729, 3218], "temperature": 0.0, "avg_logprob": -0.14141784830296295, "compression_ratio": 1.7149122807017543, "no_speech_prob": 6.144131020846544e-06}, {"id": 543, "seek": 262316, "start": 2639.56, "end": 2646.56, "text": " And it's like well your prediction for it and like oh, it's because of these three things had these three impacts, right?", "tokens": [400, 309, 311, 411, 731, 428, 17630, 337, 309, 293, 411, 1954, 11, 309, 311, 570, 295, 613, 1045, 721, 632, 613, 1045, 11606, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.14141784830296295, "compression_ratio": 1.7149122807017543, "no_speech_prob": 6.144131020846544e-06}, {"id": 544, "seek": 262316, "start": 2647.24, "end": 2649.24, "text": " So for a random forest", "tokens": [407, 337, 257, 4974, 6719], "temperature": 0.0, "avg_logprob": -0.14141784830296295, "compression_ratio": 1.7149122807017543, "no_speech_prob": 6.144131020846544e-06}, {"id": 545, "seek": 264924, "start": 2649.24, "end": 2654.12, "text": " We could do that across all of the trees that so every time we see coupler", "tokens": [492, 727, 360, 300, 2108, 439, 295, 264, 5852, 300, 370, 633, 565, 321, 536, 1384, 22732], "temperature": 0.0, "avg_logprob": -0.17655971072135715, "compression_ratio": 1.9107142857142858, "no_speech_prob": 2.0580419004545547e-06}, {"id": 546, "seek": 264924, "start": 2654.24, "end": 2659.2799999999997, "text": " We add up that change every time we see enclosure we add up that change every time we see model", "tokens": [492, 909, 493, 300, 1319, 633, 565, 321, 536, 34093, 321, 909, 493, 300, 1319, 633, 565, 321, 536, 2316], "temperature": 0.0, "avg_logprob": -0.17655971072135715, "compression_ratio": 1.9107142857142858, "no_speech_prob": 2.0580419004545547e-06}, {"id": 547, "seek": 264924, "start": 2659.2799999999997, "end": 2663.2799999999997, "text": " We add up that change okay, and so then we combine them all together", "tokens": [492, 909, 493, 300, 1319, 1392, 11, 293, 370, 550, 321, 10432, 552, 439, 1214], "temperature": 0.0, "avg_logprob": -0.17655971072135715, "compression_ratio": 1.9107142857142858, "no_speech_prob": 2.0580419004545547e-06}, {"id": 548, "seek": 264924, "start": 2664.8399999999997, "end": 2666.3599999999997, "text": " We get what?", "tokens": [492, 483, 437, 30], "temperature": 0.0, "avg_logprob": -0.17655971072135715, "compression_ratio": 1.9107142857142858, "no_speech_prob": 2.0580419004545547e-06}, {"id": 549, "seek": 264924, "start": 2666.3599999999997, "end": 2671.08, "text": " Tree interpreter does but so you could go into the source code for tree interpreter, right?", "tokens": [22291, 34132, 775, 457, 370, 291, 727, 352, 666, 264, 4009, 3089, 337, 4230, 34132, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.17655971072135715, "compression_ratio": 1.9107142857142858, "no_speech_prob": 2.0580419004545547e-06}, {"id": 550, "seek": 264924, "start": 2671.2799999999997, "end": 2673.72, "text": " It's not at all complex logic, or you could build it yourself", "tokens": [467, 311, 406, 412, 439, 3997, 9952, 11, 420, 291, 727, 1322, 309, 1803], "temperature": 0.0, "avg_logprob": -0.17655971072135715, "compression_ratio": 1.9107142857142858, "no_speech_prob": 2.0580419004545547e-06}, {"id": 551, "seek": 264924, "start": 2674.3599999999997, "end": 2676.3599999999997, "text": " Right and you can see", "tokens": [1779, 293, 291, 393, 536], "temperature": 0.0, "avg_logprob": -0.17655971072135715, "compression_ratio": 1.9107142857142858, "no_speech_prob": 2.0580419004545547e-06}, {"id": 552, "seek": 267636, "start": 2676.36, "end": 2683.28, "text": " How it does exactly this so when you go tree interpreter dot predict with a random forest model for some specific?", "tokens": [1012, 309, 775, 2293, 341, 370, 562, 291, 352, 4230, 34132, 5893, 6069, 365, 257, 4974, 6719, 2316, 337, 512, 2685, 30], "temperature": 0.0, "avg_logprob": -0.19615495819406412, "compression_ratio": 1.7577092511013215, "no_speech_prob": 1.6028047866711859e-06}, {"id": 553, "seek": 267636, "start": 2684.28, "end": 2688.8, "text": " Auction so I've got a specific row here. This is my zero index row", "tokens": [12160, 882, 370, 286, 600, 658, 257, 2685, 5386, 510, 13, 639, 307, 452, 4018, 8186, 5386], "temperature": 0.0, "avg_logprob": -0.19615495819406412, "compression_ratio": 1.7577092511013215, "no_speech_prob": 1.6028047866711859e-06}, {"id": 554, "seek": 267636, "start": 2689.88, "end": 2694.8, "text": " It tells you okay. This is the prediction the same as the random forest prediction", "tokens": [467, 5112, 291, 1392, 13, 639, 307, 264, 17630, 264, 912, 382, 264, 4974, 6719, 17630], "temperature": 0.0, "avg_logprob": -0.19615495819406412, "compression_ratio": 1.7577092511013215, "no_speech_prob": 1.6028047866711859e-06}, {"id": 555, "seek": 267636, "start": 2695.7200000000003, "end": 2698.84, "text": " Bias this is going to be always the same. It's the average", "tokens": [363, 4609, 341, 307, 516, 281, 312, 1009, 264, 912, 13, 467, 311, 264, 4274], "temperature": 0.0, "avg_logprob": -0.19615495819406412, "compression_ratio": 1.7577092511013215, "no_speech_prob": 1.6028047866711859e-06}, {"id": 556, "seek": 267636, "start": 2699.36, "end": 2704.76, "text": " sale price for for everybody for each of the random samples in the tree and", "tokens": [8680, 3218, 337, 337, 2201, 337, 1184, 295, 264, 4974, 10938, 294, 264, 4230, 293], "temperature": 0.0, "avg_logprob": -0.19615495819406412, "compression_ratio": 1.7577092511013215, "no_speech_prob": 1.6028047866711859e-06}, {"id": 557, "seek": 270476, "start": 2704.76, "end": 2706.76, "text": " then", "tokens": [550], "temperature": 0.0, "avg_logprob": -0.33516309032701463, "compression_ratio": 1.5879120879120878, "no_speech_prob": 1.788059989848989e-06}, {"id": 558, "seek": 270476, "start": 2706.84, "end": 2708.84, "text": " contributions is", "tokens": [15725, 307], "temperature": 0.0, "avg_logprob": -0.33516309032701463, "compression_ratio": 1.5879120879120878, "no_speech_prob": 1.788059989848989e-06}, {"id": 559, "seek": 270476, "start": 2709.6400000000003, "end": 2715.1200000000003, "text": " The average of also the total of all the contributions for each time we see that", "tokens": [440, 4274, 295, 611, 264, 3217, 295, 439, 264, 15725, 337, 1184, 565, 321, 536, 300], "temperature": 0.0, "avg_logprob": -0.33516309032701463, "compression_ratio": 1.5879120879120878, "no_speech_prob": 1.788059989848989e-06}, {"id": 560, "seek": 270476, "start": 2716.28, "end": 2718.86, "text": " Specific column appear in a tree", "tokens": [20484, 1089, 7738, 4204, 294, 257, 4230], "temperature": 0.0, "avg_logprob": -0.33516309032701463, "compression_ratio": 1.5879120879120878, "no_speech_prob": 1.788059989848989e-06}, {"id": 561, "seek": 270476, "start": 2719.8, "end": 2726.1400000000003, "text": " So last time I made the mistake of not sorting this correctly so this time and P dot arc sort is a super handy", "tokens": [407, 1036, 565, 286, 1027, 264, 6146, 295, 406, 32411, 341, 8944, 370, 341, 565, 293, 430, 5893, 10346, 1333, 307, 257, 1687, 13239], "temperature": 0.0, "avg_logprob": -0.33516309032701463, "compression_ratio": 1.5879120879120878, "no_speech_prob": 1.788059989848989e-06}, {"id": 562, "seek": 270476, "start": 2726.78, "end": 2729.8, "text": " Function it sorts it doesn't actually sort", "tokens": [11166, 882, 309, 7527, 309, 1177, 380, 767, 1333], "temperature": 0.0, "avg_logprob": -0.33516309032701463, "compression_ratio": 1.5879120879120878, "no_speech_prob": 1.788059989848989e-06}, {"id": 563, "seek": 272980, "start": 2729.8, "end": 2736.82, "text": " Contribution zero it just tells you where each item would move to if it were sorted", "tokens": [4839, 30783, 4018, 309, 445, 5112, 291, 689, 1184, 3174, 576, 1286, 281, 498, 309, 645, 25462], "temperature": 0.0, "avg_logprob": -0.2294036212720369, "compression_ratio": 1.5562130177514792, "no_speech_prob": 3.6119622564001475e-06}, {"id": 564, "seek": 272980, "start": 2736.86, "end": 2740.1000000000004, "text": " so now by passing ID access to each one of", "tokens": [370, 586, 538, 8437, 7348, 2105, 281, 1184, 472, 295], "temperature": 0.0, "avg_logprob": -0.2294036212720369, "compression_ratio": 1.5562130177514792, "no_speech_prob": 3.6119622564001475e-06}, {"id": 565, "seek": 272980, "start": 2741.86, "end": 2743.86, "text": " The column", "tokens": [440, 7738], "temperature": 0.0, "avg_logprob": -0.2294036212720369, "compression_ratio": 1.5562130177514792, "no_speech_prob": 3.6119622564001475e-06}, {"id": 566, "seek": 272980, "start": 2744.26, "end": 2745.7400000000002, "text": " the", "tokens": [264], "temperature": 0.0, "avg_logprob": -0.2294036212720369, "compression_ratio": 1.5562130177514792, "no_speech_prob": 3.6119622564001475e-06}, {"id": 567, "seek": 272980, "start": 2745.7400000000002, "end": 2747.7400000000002, "text": " level", "tokens": [1496], "temperature": 0.0, "avg_logprob": -0.2294036212720369, "compression_ratio": 1.5562130177514792, "no_speech_prob": 3.6119622564001475e-06}, {"id": 568, "seek": 272980, "start": 2747.86, "end": 2753.8, "text": " Contribution I can then print out all those in the right order so I can see here. Here's my column", "tokens": [4839, 30783, 286, 393, 550, 4482, 484, 439, 729, 294, 264, 558, 1668, 370, 286, 393, 536, 510, 13, 1692, 311, 452, 7738], "temperature": 0.0, "avg_logprob": -0.2294036212720369, "compression_ratio": 1.5562130177514792, "no_speech_prob": 3.6119622564001475e-06}, {"id": 569, "seek": 272980, "start": 2754.42, "end": 2756.26, "text": " here's the", "tokens": [510, 311, 264], "temperature": 0.0, "avg_logprob": -0.2294036212720369, "compression_ratio": 1.5562130177514792, "no_speech_prob": 3.6119622564001475e-06}, {"id": 570, "seek": 272980, "start": 2756.26, "end": 2757.3, "text": " level", "tokens": [1496], "temperature": 0.0, "avg_logprob": -0.2294036212720369, "compression_ratio": 1.5562130177514792, "no_speech_prob": 3.6119622564001475e-06}, {"id": 571, "seek": 275730, "start": 2757.3, "end": 2765.1800000000003, "text": " And the contribution so the fact that it's a small version of this piece of industrial equipment meant that it was less expensive", "tokens": [400, 264, 13150, 370, 264, 1186, 300, 309, 311, 257, 1359, 3037, 295, 341, 2522, 295, 9987, 5927, 4140, 300, 309, 390, 1570, 5124], "temperature": 0.0, "avg_logprob": -0.2191601018796022, "compression_ratio": 1.798206278026906, "no_speech_prob": 4.0928916860139e-06}, {"id": 572, "seek": 275730, "start": 2765.82, "end": 2770.0600000000004, "text": " Right but the fact it was made pretty recently meant that was more expensive", "tokens": [1779, 457, 264, 1186, 309, 390, 1027, 1238, 3938, 4140, 300, 390, 544, 5124], "temperature": 0.0, "avg_logprob": -0.2191601018796022, "compression_ratio": 1.798206278026906, "no_speech_prob": 4.0928916860139e-06}, {"id": 573, "seek": 275730, "start": 2770.6600000000003, "end": 2776.42, "text": " The fact that it's pretty old however made that it was less expensive right so this is not going to", "tokens": [440, 1186, 300, 309, 311, 1238, 1331, 4461, 1027, 300, 309, 390, 1570, 5124, 558, 370, 341, 307, 406, 516, 281], "temperature": 0.0, "avg_logprob": -0.2191601018796022, "compression_ratio": 1.798206278026906, "no_speech_prob": 4.0928916860139e-06}, {"id": 574, "seek": 275730, "start": 2777.6200000000003, "end": 2782.44, "text": " Really help you much at all with like a Kaggle style situation where you just need predictions", "tokens": [4083, 854, 291, 709, 412, 439, 365, 411, 257, 48751, 22631, 3758, 2590, 689, 291, 445, 643, 21264], "temperature": 0.0, "avg_logprob": -0.2191601018796022, "compression_ratio": 1.798206278026906, "no_speech_prob": 4.0928916860139e-06}, {"id": 575, "seek": 278244, "start": 2782.44, "end": 2788.46, "text": " That's going to help you a lot in a production environment or even pre-production right so like something which", "tokens": [663, 311, 516, 281, 854, 291, 257, 688, 294, 257, 4265, 2823, 420, 754, 659, 12, 40827, 558, 370, 411, 746, 597], "temperature": 0.0, "avg_logprob": -0.17373781706157485, "compression_ratio": 1.6587301587301588, "no_speech_prob": 4.860410172113916e-06}, {"id": 576, "seek": 278244, "start": 2788.92, "end": 2792.94, "text": " Any good manager should you should do if you say here's a machine learning model?", "tokens": [2639, 665, 6598, 820, 291, 820, 360, 498, 291, 584, 510, 311, 257, 3479, 2539, 2316, 30], "temperature": 0.0, "avg_logprob": -0.17373781706157485, "compression_ratio": 1.6587301587301588, "no_speech_prob": 4.860410172113916e-06}, {"id": 577, "seek": 278244, "start": 2792.94, "end": 2800.0, "text": " I think we should use is they should go away and grab a few examples of actual customers or or actual", "tokens": [286, 519, 321, 820, 764, 307, 436, 820, 352, 1314, 293, 4444, 257, 1326, 5110, 295, 3539, 4581, 420, 420, 3539], "temperature": 0.0, "avg_logprob": -0.17373781706157485, "compression_ratio": 1.6587301587301588, "no_speech_prob": 4.860410172113916e-06}, {"id": 578, "seek": 278244, "start": 2800.16, "end": 2808.32, "text": " Auctions or whatever and check whether your model looks intuitive right and if it says like my prediction is that", "tokens": [12160, 3916, 420, 2035, 293, 1520, 1968, 428, 2316, 1542, 21769, 558, 293, 498, 309, 1619, 411, 452, 17630, 307, 300], "temperature": 0.0, "avg_logprob": -0.17373781706157485, "compression_ratio": 1.6587301587301588, "no_speech_prob": 4.860410172113916e-06}, {"id": 579, "seek": 278244, "start": 2809.8, "end": 2811.8, "text": " You know", "tokens": [509, 458], "temperature": 0.0, "avg_logprob": -0.17373781706157485, "compression_ratio": 1.6587301587301588, "no_speech_prob": 4.860410172113916e-06}, {"id": 580, "seek": 281180, "start": 2811.8, "end": 2814.6400000000003, "text": " Lots and lots of people are going to really enjoy", "tokens": [15908, 293, 3195, 295, 561, 366, 516, 281, 534, 2103], "temperature": 0.0, "avg_logprob": -0.1774497074363506, "compression_ratio": 1.9458333333333333, "no_speech_prob": 1.1189395081601106e-06}, {"id": 581, "seek": 281180, "start": 2815.7200000000003, "end": 2818.1200000000003, "text": " This crappy movie you know and it's like well", "tokens": [639, 36531, 3169, 291, 458, 293, 309, 311, 411, 731], "temperature": 0.0, "avg_logprob": -0.1774497074363506, "compression_ratio": 1.9458333333333333, "no_speech_prob": 1.1189395081601106e-06}, {"id": 582, "seek": 281180, "start": 2818.1200000000003, "end": 2823.76, "text": " That was a really crappy movie then they're going to come back to you and say like explain why your models telling me", "tokens": [663, 390, 257, 534, 36531, 3169, 550, 436, 434, 516, 281, 808, 646, 281, 291, 293, 584, 411, 2903, 983, 428, 5245, 3585, 385], "temperature": 0.0, "avg_logprob": -0.1774497074363506, "compression_ratio": 1.9458333333333333, "no_speech_prob": 1.1189395081601106e-06}, {"id": 583, "seek": 281180, "start": 2825.7200000000003, "end": 2830.0, "text": " That I'm going to like this movie because I hate that movie and then you can go back and you say well", "tokens": [663, 286, 478, 516, 281, 411, 341, 3169, 570, 286, 4700, 300, 3169, 293, 550, 291, 393, 352, 646, 293, 291, 584, 731], "temperature": 0.0, "avg_logprob": -0.1774497074363506, "compression_ratio": 1.9458333333333333, "no_speech_prob": 1.1189395081601106e-06}, {"id": 584, "seek": 281180, "start": 2830.0, "end": 2836.8, "text": " It's because you like this movie and because you're this age range and you're this gender on average actually people like you", "tokens": [467, 311, 570, 291, 411, 341, 3169, 293, 570, 291, 434, 341, 3205, 3613, 293, 291, 434, 341, 7898, 322, 4274, 767, 561, 411, 291], "temperature": 0.0, "avg_logprob": -0.1774497074363506, "compression_ratio": 1.9458333333333333, "no_speech_prob": 1.1189395081601106e-06}, {"id": 585, "seek": 281180, "start": 2837.32, "end": 2839.32, "text": " Did like that movie?", "tokens": [2589, 411, 300, 3169, 30], "temperature": 0.0, "avg_logprob": -0.1774497074363506, "compression_ratio": 1.9458333333333333, "no_speech_prob": 1.1189395081601106e-06}, {"id": 586, "seek": 283932, "start": 2839.32, "end": 2842.32, "text": " Yeah", "tokens": [865], "temperature": 0.0, "avg_logprob": -0.23457234342333297, "compression_ratio": 1.4864864864864864, "no_speech_prob": 1.3006754670641385e-05}, {"id": 587, "seek": 283932, "start": 2845.48, "end": 2848.2000000000003, "text": " What's the second element of each table?", "tokens": [708, 311, 264, 1150, 4478, 295, 1184, 3199, 30], "temperature": 0.0, "avg_logprob": -0.23457234342333297, "compression_ratio": 1.4864864864864864, "no_speech_prob": 1.3006754670641385e-05}, {"id": 588, "seek": 283932, "start": 2849.36, "end": 2851.44, "text": " This is saying for this particular row", "tokens": [639, 307, 1566, 337, 341, 1729, 5386], "temperature": 0.0, "avg_logprob": -0.23457234342333297, "compression_ratio": 1.4864864864864864, "no_speech_prob": 1.3006754670641385e-05}, {"id": 589, "seek": 283932, "start": 2852.1200000000003, "end": 2859.36, "text": " It was a mini and it was 11 years old and it was a hydraulic excavator track three to four metric tons", "tokens": [467, 390, 257, 8382, 293, 309, 390, 2975, 924, 1331, 293, 309, 390, 257, 32134, 34351, 1639, 2837, 1045, 281, 1451, 20678, 9131], "temperature": 0.0, "avg_logprob": -0.23457234342333297, "compression_ratio": 1.4864864864864864, "no_speech_prob": 1.3006754670641385e-05}, {"id": 590, "seek": 283932, "start": 2861.6400000000003, "end": 2866.2000000000003, "text": " So I was just feeding back and telling you it's it because this is actually what it was", "tokens": [407, 286, 390, 445, 12919, 646, 293, 3585, 291, 309, 311, 309, 570, 341, 307, 767, 437, 309, 390], "temperature": 0.0, "avg_logprob": -0.23457234342333297, "compression_ratio": 1.4864864864864864, "no_speech_prob": 1.3006754670641385e-05}, {"id": 591, "seek": 286620, "start": 2866.2, "end": 2870.08, "text": " It was these numbers, so I just went back to", "tokens": [467, 390, 613, 3547, 11, 370, 286, 445, 1437, 646, 281], "temperature": 0.0, "avg_logprob": -0.2248955325803895, "compression_ratio": 1.5114942528735633, "no_speech_prob": 3.340519015182508e-06}, {"id": 592, "seek": 286620, "start": 2871.4399999999996, "end": 2873.16, "text": " the original", "tokens": [264, 3380], "temperature": 0.0, "avg_logprob": -0.2248955325803895, "compression_ratio": 1.5114942528735633, "no_speech_prob": 3.340519015182508e-06}, {"id": 593, "seek": 286620, "start": 2873.16, "end": 2875.16, "text": " data to actually pull out the", "tokens": [1412, 281, 767, 2235, 484, 264], "temperature": 0.0, "avg_logprob": -0.2248955325803895, "compression_ratio": 1.5114942528735633, "no_speech_prob": 3.340519015182508e-06}, {"id": 594, "seek": 286620, "start": 2876.2799999999997, "end": 2878.2799999999997, "text": " descriptive versions of each one", "tokens": [42585, 9606, 295, 1184, 472], "temperature": 0.0, "avg_logprob": -0.2248955325803895, "compression_ratio": 1.5114942528735633, "no_speech_prob": 3.340519015182508e-06}, {"id": 595, "seek": 286620, "start": 2881.68, "end": 2886.3999999999996, "text": " Okay, so if we sum up all the contributions together and", "tokens": [1033, 11, 370, 498, 321, 2408, 493, 439, 264, 15725, 1214, 293], "temperature": 0.0, "avg_logprob": -0.2248955325803895, "compression_ratio": 1.5114942528735633, "no_speech_prob": 3.340519015182508e-06}, {"id": 596, "seek": 286620, "start": 2887.7999999999997, "end": 2891.08, "text": " Then add them to the bias", "tokens": [1396, 909, 552, 281, 264, 12577], "temperature": 0.0, "avg_logprob": -0.2248955325803895, "compression_ratio": 1.5114942528735633, "no_speech_prob": 3.340519015182508e-06}, {"id": 597, "seek": 289108, "start": 2891.08, "end": 2895.56, "text": " Then that would be the same as adding up those three things", "tokens": [1396, 300, 576, 312, 264, 912, 382, 5127, 493, 729, 1045, 721], "temperature": 0.0, "avg_logprob": -0.20979189536940884, "compression_ratio": 1.60989010989011, "no_speech_prob": 4.710876510216622e-06}, {"id": 598, "seek": 289108, "start": 2896.6, "end": 2901.84, "text": " Adding it to this and as we know from our waterfall chart that gives us our final", "tokens": [31204, 309, 281, 341, 293, 382, 321, 458, 490, 527, 27848, 6927, 300, 2709, 505, 527, 2572], "temperature": 0.0, "avg_logprob": -0.20979189536940884, "compression_ratio": 1.60989010989011, "no_speech_prob": 4.710876510216622e-06}, {"id": 599, "seek": 289108, "start": 2902.88, "end": 2904.7999999999997, "text": " prediction", "tokens": [17630], "temperature": 0.0, "avg_logprob": -0.20979189536940884, "compression_ratio": 1.60989010989011, "no_speech_prob": 4.710876510216622e-06}, {"id": 600, "seek": 289108, "start": 2904.7999999999997, "end": 2906.7999999999997, "text": " this is a", "tokens": [341, 307, 257], "temperature": 0.0, "avg_logprob": -0.20979189536940884, "compression_ratio": 1.60989010989011, "no_speech_prob": 4.710876510216622e-06}, {"id": 601, "seek": 289108, "start": 2907.72, "end": 2911.2799999999997, "text": " Almost totally unknown technique and this particular", "tokens": [12627, 3879, 9841, 6532, 293, 341, 1729], "temperature": 0.0, "avg_logprob": -0.20979189536940884, "compression_ratio": 1.60989010989011, "no_speech_prob": 4.710876510216622e-06}, {"id": 602, "seek": 289108, "start": 2913.56, "end": 2915.56, "text": " Library is almost totally unknown as well", "tokens": [12806, 307, 1920, 3879, 9841, 382, 731], "temperature": 0.0, "avg_logprob": -0.20979189536940884, "compression_ratio": 1.60989010989011, "no_speech_prob": 4.710876510216622e-06}, {"id": 603, "seek": 289108, "start": 2916.4, "end": 2918.4, "text": " so like it's a great opportunity to", "tokens": [370, 411, 309, 311, 257, 869, 2650, 281], "temperature": 0.0, "avg_logprob": -0.20979189536940884, "compression_ratio": 1.60989010989011, "no_speech_prob": 4.710876510216622e-06}, {"id": 604, "seek": 291840, "start": 2918.4, "end": 2923.76, "text": " You know show something that a lot of people like it's totally critical in my opinion", "tokens": [509, 458, 855, 746, 300, 257, 688, 295, 561, 411, 309, 311, 3879, 4924, 294, 452, 4800], "temperature": 0.0, "avg_logprob": -0.14720160540412455, "compression_ratio": 1.65, "no_speech_prob": 3.2887242014112417e-06}, {"id": 605, "seek": 291840, "start": 2924.7200000000003, "end": 2927.6, "text": " But but rarely known so that's", "tokens": [583, 457, 13752, 2570, 370, 300, 311], "temperature": 0.0, "avg_logprob": -0.14720160540412455, "compression_ratio": 1.65, "no_speech_prob": 3.2887242014112417e-06}, {"id": 606, "seek": 291840, "start": 2931.2000000000003, "end": 2937.64, "text": " That's kind of the end of the random forest interpretation piece and hopefully you've now seen enough that when somebody says", "tokens": [663, 311, 733, 295, 264, 917, 295, 264, 4974, 6719, 14174, 2522, 293, 4696, 291, 600, 586, 1612, 1547, 300, 562, 2618, 1619], "temperature": 0.0, "avg_logprob": -0.14720160540412455, "compression_ratio": 1.65, "no_speech_prob": 3.2887242014112417e-06}, {"id": 607, "seek": 291840, "start": 2938.12, "end": 2945.28, "text": " We can't use modern machine learning techniques because they're black boxes that are interpretable you have enough information to say you're full of shit", "tokens": [492, 393, 380, 764, 4363, 3479, 2539, 7512, 570, 436, 434, 2211, 9002, 300, 366, 7302, 712, 291, 362, 1547, 1589, 281, 584, 291, 434, 1577, 295, 4611], "temperature": 0.0, "avg_logprob": -0.14720160540412455, "compression_ratio": 1.65, "no_speech_prob": 3.2887242014112417e-06}, {"id": 608, "seek": 294528, "start": 2945.28, "end": 2950.44, "text": " All right, like they're extremely interpretable and the stuff that we've just done", "tokens": [1057, 558, 11, 411, 436, 434, 4664, 7302, 712, 293, 264, 1507, 300, 321, 600, 445, 1096], "temperature": 0.0, "avg_logprob": -0.19444186998450239, "compression_ratio": 1.7058823529411764, "no_speech_prob": 1.414466169080697e-06}, {"id": 609, "seek": 294528, "start": 2950.44, "end": 2952.44, "text": " You know try to do that with a linear model", "tokens": [509, 458, 853, 281, 360, 300, 365, 257, 8213, 2316], "temperature": 0.0, "avg_logprob": -0.19444186998450239, "compression_ratio": 1.7058823529411764, "no_speech_prob": 1.414466169080697e-06}, {"id": 610, "seek": 294528, "start": 2952.8, "end": 2953.76, "text": " Good luck to you", "tokens": [2205, 3668, 281, 291], "temperature": 0.0, "avg_logprob": -0.19444186998450239, "compression_ratio": 1.7058823529411764, "no_speech_prob": 1.414466169080697e-06}, {"id": 611, "seek": 294528, "start": 2953.76, "end": 2959.36, "text": " You know even where you can do something similar to linear model trying to do it so that's not giving you totally the wrong answer", "tokens": [509, 458, 754, 689, 291, 393, 360, 746, 2531, 281, 8213, 2316, 1382, 281, 360, 309, 370, 300, 311, 406, 2902, 291, 3879, 264, 2085, 1867], "temperature": 0.0, "avg_logprob": -0.19444186998450239, "compression_ratio": 1.7058823529411764, "no_speech_prob": 1.414466169080697e-06}, {"id": 612, "seek": 294528, "start": 2959.36, "end": 2962.48, "text": " And you had no idea it's a wrong answer. It's going to be a real challenge", "tokens": [400, 291, 632, 572, 1558, 309, 311, 257, 2085, 1867, 13, 467, 311, 516, 281, 312, 257, 957, 3430], "temperature": 0.0, "avg_logprob": -0.19444186998450239, "compression_ratio": 1.7058823529411764, "no_speech_prob": 1.414466169080697e-06}, {"id": 613, "seek": 294528, "start": 2963.84, "end": 2971.6800000000003, "text": " So the last step we're going to do before we try and build our own random forest is deal with this tricky issue of", "tokens": [407, 264, 1036, 1823, 321, 434, 516, 281, 360, 949, 321, 853, 293, 1322, 527, 1065, 4974, 6719, 307, 2028, 365, 341, 12414, 2734, 295], "temperature": 0.0, "avg_logprob": -0.19444186998450239, "compression_ratio": 1.7058823529411764, "no_speech_prob": 1.414466169080697e-06}, {"id": 614, "seek": 297168, "start": 2971.68, "end": 2974.7999999999997, "text": " extrapolation so in this case", "tokens": [48224, 399, 370, 294, 341, 1389], "temperature": 0.0, "avg_logprob": -0.24031271757902922, "compression_ratio": 1.451851851851852, "no_speech_prob": 5.014678208681289e-06}, {"id": 615, "seek": 297168, "start": 2975.3999999999996, "end": 2978.24, "text": " If we look at our tree", "tokens": [759, 321, 574, 412, 527, 4230], "temperature": 0.0, "avg_logprob": -0.24031271757902922, "compression_ratio": 1.451851851851852, "no_speech_prob": 5.014678208681289e-06}, {"id": 616, "seek": 297168, "start": 2981.12, "end": 2983.7999999999997, "text": " Let's look at the accuracy of our most recent trees", "tokens": [961, 311, 574, 412, 264, 14170, 295, 527, 881, 5162, 5852], "temperature": 0.0, "avg_logprob": -0.24031271757902922, "compression_ratio": 1.451851851851852, "no_speech_prob": 5.014678208681289e-06}, {"id": 617, "seek": 297168, "start": 2989.6, "end": 2991.6, "text": " We still have", "tokens": [492, 920, 362], "temperature": 0.0, "avg_logprob": -0.24031271757902922, "compression_ratio": 1.451851851851852, "no_speech_prob": 5.014678208681289e-06}, {"id": 618, "seek": 297168, "start": 2992.04, "end": 2994.44, "text": " You know a big difference between our validation", "tokens": [509, 458, 257, 955, 2649, 1296, 527, 24071], "temperature": 0.0, "avg_logprob": -0.24031271757902922, "compression_ratio": 1.451851851851852, "no_speech_prob": 5.014678208681289e-06}, {"id": 619, "seek": 297168, "start": 2995.48, "end": 2997.48, "text": " score and our", "tokens": [6175, 293, 527], "temperature": 0.0, "avg_logprob": -0.24031271757902922, "compression_ratio": 1.451851851851852, "no_speech_prob": 5.014678208681289e-06}, {"id": 620, "seek": 299748, "start": 2997.48, "end": 3001.96, "text": " training score the", "tokens": [3097, 6175, 264], "temperature": 0.0, "avg_logprob": -0.20913227511123872, "compression_ratio": 1.548913043478261, "no_speech_prob": 3.844889306492405e-06}, {"id": 621, "seek": 299748, "start": 3003.16, "end": 3005.72, "text": " Actually this case it's not too bad that", "tokens": [5135, 341, 1389, 309, 311, 406, 886, 1578, 300], "temperature": 0.0, "avg_logprob": -0.20913227511123872, "compression_ratio": 1.548913043478261, "no_speech_prob": 3.844889306492405e-06}, {"id": 622, "seek": 299748, "start": 3006.76, "end": 3011.68, "text": " The difference between the OOB and the validation is actually pretty close", "tokens": [440, 2649, 1296, 264, 422, 46, 33, 293, 264, 24071, 307, 767, 1238, 1998], "temperature": 0.0, "avg_logprob": -0.20913227511123872, "compression_ratio": 1.548913043478261, "no_speech_prob": 3.844889306492405e-06}, {"id": 623, "seek": 299748, "start": 3012.2, "end": 3017.52, "text": " So if there was a big difference between validation and OOB like I'd be very worried about that", "tokens": [407, 498, 456, 390, 257, 955, 2649, 1296, 24071, 293, 422, 46, 33, 411, 286, 1116, 312, 588, 5804, 466, 300], "temperature": 0.0, "avg_logprob": -0.20913227511123872, "compression_ratio": 1.548913043478261, "no_speech_prob": 3.844889306492405e-06}, {"id": 624, "seek": 299748, "start": 3017.52, "end": 3019.52, "text": " We've dealt with the temporal side of things", "tokens": [492, 600, 15991, 365, 264, 30881, 1252, 295, 721], "temperature": 0.0, "avg_logprob": -0.20913227511123872, "compression_ratio": 1.548913043478261, "no_speech_prob": 3.844889306492405e-06}, {"id": 625, "seek": 299748, "start": 3020.44, "end": 3022.44, "text": " correctly", "tokens": [8944], "temperature": 0.0, "avg_logprob": -0.20913227511123872, "compression_ratio": 1.548913043478261, "no_speech_prob": 3.844889306492405e-06}, {"id": 626, "seek": 302244, "start": 3022.44, "end": 3027.48, "text": " Let's just have a look at think I'm most recent model. I hear it was", "tokens": [961, 311, 445, 362, 257, 574, 412, 519, 286, 478, 881, 5162, 2316, 13, 286, 1568, 309, 390], "temperature": 0.0, "avg_logprob": -0.19346562951011995, "compression_ratio": 1.6240875912408759, "no_speech_prob": 2.60159617937461e-06}, {"id": 627, "seek": 302244, "start": 3028.44, "end": 3031.2000000000003, "text": " Yeah, so there's a tiny difference right and so on", "tokens": [865, 11, 370, 456, 311, 257, 5870, 2649, 558, 293, 370, 322], "temperature": 0.0, "avg_logprob": -0.19346562951011995, "compression_ratio": 1.6240875912408759, "no_speech_prob": 2.60159617937461e-06}, {"id": 628, "seek": 302244, "start": 3032.6, "end": 3038.44, "text": " Kaggle at least you kind of need that last decimal place in the real world. I probably stop here", "tokens": [48751, 22631, 412, 1935, 291, 733, 295, 643, 300, 1036, 26601, 1081, 294, 264, 957, 1002, 13, 286, 1391, 1590, 510], "temperature": 0.0, "avg_logprob": -0.19346562951011995, "compression_ratio": 1.6240875912408759, "no_speech_prob": 2.60159617937461e-06}, {"id": 629, "seek": 302244, "start": 3038.84, "end": 3043.28, "text": " But quite often you'll see there's a big difference between your validation score and your OOB score", "tokens": [583, 1596, 2049, 291, 603, 536, 456, 311, 257, 955, 2649, 1296, 428, 24071, 6175, 293, 428, 422, 46, 33, 6175], "temperature": 0.0, "avg_logprob": -0.19346562951011995, "compression_ratio": 1.6240875912408759, "no_speech_prob": 2.60159617937461e-06}, {"id": 630, "seek": 302244, "start": 3043.28, "end": 3045.28, "text": " And I want to show you how you would deal with that", "tokens": [400, 286, 528, 281, 855, 291, 577, 291, 576, 2028, 365, 300], "temperature": 0.0, "avg_logprob": -0.19346562951011995, "compression_ratio": 1.6240875912408759, "no_speech_prob": 2.60159617937461e-06}, {"id": 631, "seek": 302244, "start": 3047.8, "end": 3051.2400000000002, "text": " Particularly because actually we know that the OOB should be a little worse", "tokens": [32281, 570, 767, 321, 458, 300, 264, 422, 46, 33, 820, 312, 257, 707, 5324], "temperature": 0.0, "avg_logprob": -0.19346562951011995, "compression_ratio": 1.6240875912408759, "no_speech_prob": 2.60159617937461e-06}, {"id": 632, "seek": 305124, "start": 3051.24, "end": 3053.24, "text": " Because it's using this less tree", "tokens": [1436, 309, 311, 1228, 341, 1570, 4230], "temperature": 0.0, "avg_logprob": -0.18520123771067415, "compression_ratio": 1.7883597883597884, "no_speech_prob": 7.766870112391189e-06}, {"id": 633, "seek": 305124, "start": 3053.24, "end": 3055.6, "text": " So it gives me a sense that we should be able to do a little bit better", "tokens": [407, 309, 2709, 385, 257, 2020, 300, 321, 820, 312, 1075, 281, 360, 257, 707, 857, 1101], "temperature": 0.0, "avg_logprob": -0.18520123771067415, "compression_ratio": 1.7883597883597884, "no_speech_prob": 7.766870112391189e-06}, {"id": 634, "seek": 305124, "start": 3055.6, "end": 3062.52, "text": " And so the reason the way we should be able to do a little bit better is by handling the time component a little bit better", "tokens": [400, 370, 264, 1778, 264, 636, 321, 820, 312, 1075, 281, 360, 257, 707, 857, 1101, 307, 538, 13175, 264, 565, 6542, 257, 707, 857, 1101], "temperature": 0.0, "avg_logprob": -0.18520123771067415, "compression_ratio": 1.7883597883597884, "no_speech_prob": 7.766870112391189e-06}, {"id": 635, "seek": 305124, "start": 3063.72, "end": 3065.8399999999997, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.18520123771067415, "compression_ratio": 1.7883597883597884, "no_speech_prob": 7.766870112391189e-06}, {"id": 636, "seek": 305124, "start": 3065.8399999999997, "end": 3070.04, "text": " Here's the problem with random forests when it comes to extrapolation", "tokens": [1692, 311, 264, 1154, 365, 4974, 21700, 562, 309, 1487, 281, 48224, 399], "temperature": 0.0, "avg_logprob": -0.18520123771067415, "compression_ratio": 1.7883597883597884, "no_speech_prob": 7.766870112391189e-06}, {"id": 637, "seek": 305124, "start": 3071.2799999999997, "end": 3073.2799999999997, "text": " when you", "tokens": [562, 291], "temperature": 0.0, "avg_logprob": -0.18520123771067415, "compression_ratio": 1.7883597883597884, "no_speech_prob": 7.766870112391189e-06}, {"id": 638, "seek": 305124, "start": 3074.7599999999998, "end": 3076.7599999999998, "text": " When you've got a data set", "tokens": [1133, 291, 600, 658, 257, 1412, 992], "temperature": 0.0, "avg_logprob": -0.18520123771067415, "compression_ratio": 1.7883597883597884, "no_speech_prob": 7.766870112391189e-06}, {"id": 639, "seek": 307676, "start": 3076.76, "end": 3082.1000000000004, "text": " That's like you know for got four years of sales data in it and you create your tree", "tokens": [663, 311, 411, 291, 458, 337, 658, 1451, 924, 295, 5763, 1412, 294, 309, 293, 291, 1884, 428, 4230], "temperature": 0.0, "avg_logprob": -0.17905521392822266, "compression_ratio": 1.7547169811320755, "no_speech_prob": 4.785062174050836e-06}, {"id": 640, "seek": 307676, "start": 3082.48, "end": 3087.94, "text": " Right and it says like oh if these if it's in some particular store", "tokens": [1779, 293, 309, 1619, 411, 1954, 498, 613, 498, 309, 311, 294, 512, 1729, 3531], "temperature": 0.0, "avg_logprob": -0.17905521392822266, "compression_ratio": 1.7547169811320755, "no_speech_prob": 4.785062174050836e-06}, {"id": 641, "seek": 307676, "start": 3088.2400000000002, "end": 3092.32, "text": " And it's some particular item and it is on special", "tokens": [400, 309, 311, 512, 1729, 3174, 293, 309, 307, 322, 2121], "temperature": 0.0, "avg_logprob": -0.17905521392822266, "compression_ratio": 1.7547169811320755, "no_speech_prob": 4.785062174050836e-06}, {"id": 642, "seek": 307676, "start": 3092.96, "end": 3098.86, "text": " You know here's the average price right it actually tells us the average price you know", "tokens": [509, 458, 510, 311, 264, 4274, 3218, 558, 309, 767, 5112, 505, 264, 4274, 3218, 291, 458], "temperature": 0.0, "avg_logprob": -0.17905521392822266, "compression_ratio": 1.7547169811320755, "no_speech_prob": 4.785062174050836e-06}, {"id": 643, "seek": 307676, "start": 3099.28, "end": 3104.84, "text": " Over the whole training set which could be pretty old right and so when you then", "tokens": [4886, 264, 1379, 3097, 992, 597, 727, 312, 1238, 1331, 558, 293, 370, 562, 291, 550], "temperature": 0.0, "avg_logprob": -0.17905521392822266, "compression_ratio": 1.7547169811320755, "no_speech_prob": 4.785062174050836e-06}, {"id": 644, "seek": 310484, "start": 3104.84, "end": 3106.2000000000003, "text": " want", "tokens": [528], "temperature": 0.0, "avg_logprob": -0.1713889056238635, "compression_ratio": 1.7032967032967032, "no_speech_prob": 4.356841145636281e-06}, {"id": 645, "seek": 310484, "start": 3106.2000000000003, "end": 3109.6400000000003, "text": " To step forward to like well. What's going to be the price next month?", "tokens": [1407, 1823, 2128, 281, 411, 731, 13, 708, 311, 516, 281, 312, 264, 3218, 958, 1618, 30], "temperature": 0.0, "avg_logprob": -0.1713889056238635, "compression_ratio": 1.7032967032967032, "no_speech_prob": 4.356841145636281e-06}, {"id": 646, "seek": 310484, "start": 3110.44, "end": 3115.04, "text": " It's never seen next month and and where else with a kind of a linear model", "tokens": [467, 311, 1128, 1612, 958, 1618, 293, 293, 689, 1646, 365, 257, 733, 295, 257, 8213, 2316], "temperature": 0.0, "avg_logprob": -0.1713889056238635, "compression_ratio": 1.7032967032967032, "no_speech_prob": 4.356841145636281e-06}, {"id": 647, "seek": 310484, "start": 3115.04, "end": 3121.0, "text": " It can find a relationship between time and price where even though we only had this much data", "tokens": [467, 393, 915, 257, 2480, 1296, 565, 293, 3218, 689, 754, 1673, 321, 787, 632, 341, 709, 1412], "temperature": 0.0, "avg_logprob": -0.1713889056238635, "compression_ratio": 1.7032967032967032, "no_speech_prob": 4.356841145636281e-06}, {"id": 648, "seek": 310484, "start": 3121.28, "end": 3127.44, "text": " When you then go and predict something in the future it can extrapolate that but a random forest can't do that", "tokens": [1133, 291, 550, 352, 293, 6069, 746, 294, 264, 2027, 309, 393, 48224, 473, 300, 457, 257, 4974, 6719, 393, 380, 360, 300], "temperature": 0.0, "avg_logprob": -0.1713889056238635, "compression_ratio": 1.7032967032967032, "no_speech_prob": 4.356841145636281e-06}, {"id": 649, "seek": 310484, "start": 3127.44, "end": 3132.84, "text": " There's no way if you think about it for a tree to be able to say well next month. It would be higher still", "tokens": [821, 311, 572, 636, 498, 291, 519, 466, 309, 337, 257, 4230, 281, 312, 1075, 281, 584, 731, 958, 1618, 13, 467, 576, 312, 2946, 920], "temperature": 0.0, "avg_logprob": -0.1713889056238635, "compression_ratio": 1.7032967032967032, "no_speech_prob": 4.356841145636281e-06}, {"id": 650, "seek": 313284, "start": 3132.84, "end": 3134.8, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.17100904502120673, "compression_ratio": 1.5916666666666666, "no_speech_prob": 5.014695034333272e-06}, {"id": 651, "seek": 313284, "start": 3134.8, "end": 3138.6400000000003, "text": " There's a few ways to deal with this and we'll talk about it over the next couple of lessons", "tokens": [821, 311, 257, 1326, 2098, 281, 2028, 365, 341, 293, 321, 603, 751, 466, 309, 670, 264, 958, 1916, 295, 8820], "temperature": 0.0, "avg_logprob": -0.17100904502120673, "compression_ratio": 1.5916666666666666, "no_speech_prob": 5.014695034333272e-06}, {"id": 652, "seek": 313284, "start": 3138.6400000000003, "end": 3140.88, "text": " but one simple way is just to try to", "tokens": [457, 472, 2199, 636, 307, 445, 281, 853, 281], "temperature": 0.0, "avg_logprob": -0.17100904502120673, "compression_ratio": 1.5916666666666666, "no_speech_prob": 5.014695034333272e-06}, {"id": 653, "seek": 313284, "start": 3142.6800000000003, "end": 3144.6800000000003, "text": " Avoid using", "tokens": [41061, 1228], "temperature": 0.0, "avg_logprob": -0.17100904502120673, "compression_ratio": 1.5916666666666666, "no_speech_prob": 5.014695034333272e-06}, {"id": 654, "seek": 313284, "start": 3145.1200000000003, "end": 3146.56, "text": " time", "tokens": [565], "temperature": 0.0, "avg_logprob": -0.17100904502120673, "compression_ratio": 1.5916666666666666, "no_speech_prob": 5.014695034333272e-06}, {"id": 655, "seek": 313284, "start": 3146.56, "end": 3151.3, "text": " Variables as predictors if there's something else we could use that's going to give us a better", "tokens": [32511, 2965, 382, 6069, 830, 498, 456, 311, 746, 1646, 321, 727, 764, 300, 311, 516, 281, 976, 505, 257, 1101], "temperature": 0.0, "avg_logprob": -0.17100904502120673, "compression_ratio": 1.5916666666666666, "no_speech_prob": 5.014695034333272e-06}, {"id": 656, "seek": 313284, "start": 3151.48, "end": 3158.1600000000003, "text": " You know something of a kind of a stronger relationship. That's actually going to work in the future so in this case", "tokens": [509, 458, 746, 295, 257, 733, 295, 257, 7249, 2480, 13, 663, 311, 767, 516, 281, 589, 294, 264, 2027, 370, 294, 341, 1389], "temperature": 0.0, "avg_logprob": -0.17100904502120673, "compression_ratio": 1.5916666666666666, "no_speech_prob": 5.014695034333272e-06}, {"id": 657, "seek": 313284, "start": 3158.92, "end": 3160.6000000000004, "text": " What I wanted to do", "tokens": [708, 286, 1415, 281, 360], "temperature": 0.0, "avg_logprob": -0.17100904502120673, "compression_ratio": 1.5916666666666666, "no_speech_prob": 5.014695034333272e-06}, {"id": 658, "seek": 316060, "start": 3160.6, "end": 3162.72, "text": " Was to first of all figure out?", "tokens": [3027, 281, 700, 295, 439, 2573, 484, 30], "temperature": 0.0, "avg_logprob": -0.19847393035888672, "compression_ratio": 1.8148148148148149, "no_speech_prob": 5.594275080511579e-06}, {"id": 659, "seek": 316060, "start": 3167.2799999999997, "end": 3175.16, "text": " What's the difference between our validation set and our training set like if I understand the difference between our validation set and our", "tokens": [708, 311, 264, 2649, 1296, 527, 24071, 992, 293, 527, 3097, 992, 411, 498, 286, 1223, 264, 2649, 1296, 527, 24071, 992, 293, 527], "temperature": 0.0, "avg_logprob": -0.19847393035888672, "compression_ratio": 1.8148148148148149, "no_speech_prob": 5.594275080511579e-06}, {"id": 660, "seek": 316060, "start": 3175.16, "end": 3177.16, "text": " Training set then that tells me", "tokens": [20620, 992, 550, 300, 5112, 385], "temperature": 0.0, "avg_logprob": -0.19847393035888672, "compression_ratio": 1.8148148148148149, "no_speech_prob": 5.594275080511579e-06}, {"id": 661, "seek": 316060, "start": 3177.88, "end": 3179.7999999999997, "text": " What are the predictors?", "tokens": [708, 366, 264, 6069, 830, 30], "temperature": 0.0, "avg_logprob": -0.19847393035888672, "compression_ratio": 1.8148148148148149, "no_speech_prob": 5.594275080511579e-06}, {"id": 662, "seek": 316060, "start": 3179.7999999999997, "end": 3184.12, "text": " which which have a strong temporal component and therefore they may be", "tokens": [597, 597, 362, 257, 2068, 30881, 6542, 293, 4412, 436, 815, 312], "temperature": 0.0, "avg_logprob": -0.19847393035888672, "compression_ratio": 1.8148148148148149, "no_speech_prob": 5.594275080511579e-06}, {"id": 663, "seek": 318412, "start": 3184.12, "end": 3190.0, "text": " Irrelevant by the time I get to the future time period so I do something really interesting", "tokens": [9151, 265, 25638, 538, 264, 565, 286, 483, 281, 264, 2027, 565, 2896, 370, 286, 360, 746, 534, 1880], "temperature": 0.0, "avg_logprob": -0.17132478807030654, "compression_ratio": 1.6105769230769231, "no_speech_prob": 9.132522791333031e-07}, {"id": 664, "seek": 318412, "start": 3190.52, "end": 3193.08, "text": " Which is I create a random forest", "tokens": [3013, 307, 286, 1884, 257, 4974, 6719], "temperature": 0.0, "avg_logprob": -0.17132478807030654, "compression_ratio": 1.6105769230769231, "no_speech_prob": 9.132522791333031e-07}, {"id": 665, "seek": 318412, "start": 3194.12, "end": 3199.0, "text": " Where my dependent variable is is it in the validation set?", "tokens": [2305, 452, 12334, 7006, 307, 307, 309, 294, 264, 24071, 992, 30], "temperature": 0.0, "avg_logprob": -0.17132478807030654, "compression_ratio": 1.6105769230769231, "no_speech_prob": 9.132522791333031e-07}, {"id": 666, "seek": 318412, "start": 3200.6, "end": 3205.56, "text": " Right so I've gone back, and I've got my whole data frame with the training and validation all together and", "tokens": [1779, 370, 286, 600, 2780, 646, 11, 293, 286, 600, 658, 452, 1379, 1412, 3920, 365, 264, 3097, 293, 24071, 439, 1214, 293], "temperature": 0.0, "avg_logprob": -0.17132478807030654, "compression_ratio": 1.6105769230769231, "no_speech_prob": 9.132522791333031e-07}, {"id": 667, "seek": 318412, "start": 3206.24, "end": 3208.92, "text": " I've created a new column called is valid", "tokens": [286, 600, 2942, 257, 777, 7738, 1219, 307, 7363], "temperature": 0.0, "avg_logprob": -0.17132478807030654, "compression_ratio": 1.6105769230769231, "no_speech_prob": 9.132522791333031e-07}, {"id": 668, "seek": 320892, "start": 3208.92, "end": 3215.44, "text": " Which I've set to one and then for all of the stuff in the training set I set it to zero", "tokens": [3013, 286, 600, 992, 281, 472, 293, 550, 337, 439, 295, 264, 1507, 294, 264, 3097, 992, 286, 992, 309, 281, 4018], "temperature": 0.0, "avg_logprob": -0.20339747269948324, "compression_ratio": 1.865979381443299, "no_speech_prob": 3.555959892764804e-06}, {"id": 669, "seek": 320892, "start": 3215.6800000000003, "end": 3217.04, "text": " That's I've got a new column", "tokens": [663, 311, 286, 600, 658, 257, 777, 7738], "temperature": 0.0, "avg_logprob": -0.20339747269948324, "compression_ratio": 1.865979381443299, "no_speech_prob": 3.555959892764804e-06}, {"id": 670, "seek": 320892, "start": 3217.04, "end": 3223.48, "text": " Which is just is this in the validation set or not and then I'm going to use that as my dependent variable and", "tokens": [3013, 307, 445, 307, 341, 294, 264, 24071, 992, 420, 406, 293, 550, 286, 478, 516, 281, 764, 300, 382, 452, 12334, 7006, 293], "temperature": 0.0, "avg_logprob": -0.20339747269948324, "compression_ratio": 1.865979381443299, "no_speech_prob": 3.555959892764804e-06}, {"id": 671, "seek": 320892, "start": 3223.76, "end": 3225.56, "text": " build a random forest", "tokens": [1322, 257, 4974, 6719], "temperature": 0.0, "avg_logprob": -0.20339747269948324, "compression_ratio": 1.865979381443299, "no_speech_prob": 3.555959892764804e-06}, {"id": 672, "seek": 320892, "start": 3225.56, "end": 3228.2000000000003, "text": " So this is a random forest not to predict price", "tokens": [407, 341, 307, 257, 4974, 6719, 406, 281, 6069, 3218], "temperature": 0.0, "avg_logprob": -0.20339747269948324, "compression_ratio": 1.865979381443299, "no_speech_prob": 3.555959892764804e-06}, {"id": 673, "seek": 320892, "start": 3228.8, "end": 3233.2400000000002, "text": " the predict is this in the validation set or not and so if your", "tokens": [264, 6069, 307, 341, 294, 264, 24071, 992, 420, 406, 293, 370, 498, 428], "temperature": 0.0, "avg_logprob": -0.20339747269948324, "compression_ratio": 1.865979381443299, "no_speech_prob": 3.555959892764804e-06}, {"id": 674, "seek": 323324, "start": 3233.24, "end": 3240.3799999999997, "text": " Variables were not time dependent, then it shouldn't be possible to figure out if something's in the validation set or not", "tokens": [32511, 2965, 645, 406, 565, 12334, 11, 550, 309, 4659, 380, 312, 1944, 281, 2573, 484, 498, 746, 311, 294, 264, 24071, 992, 420, 406], "temperature": 0.0, "avg_logprob": -0.15451344321755803, "compression_ratio": 1.7489539748953975, "no_speech_prob": 6.048875093256356e-06}, {"id": 675, "seek": 323324, "start": 3240.56, "end": 3243.3199999999997, "text": " This is a great trick in Kaggle right because in Kaggle", "tokens": [639, 307, 257, 869, 4282, 294, 48751, 22631, 558, 570, 294, 48751, 22631], "temperature": 0.0, "avg_logprob": -0.15451344321755803, "compression_ratio": 1.7489539748953975, "no_speech_prob": 6.048875093256356e-06}, {"id": 676, "seek": 323324, "start": 3244.24, "end": 3249.3599999999997, "text": " They often won't tell you whether the test set is a random sample or not", "tokens": [814, 2049, 1582, 380, 980, 291, 1968, 264, 1500, 992, 307, 257, 4974, 6889, 420, 406], "temperature": 0.0, "avg_logprob": -0.15451344321755803, "compression_ratio": 1.7489539748953975, "no_speech_prob": 6.048875093256356e-06}, {"id": 677, "seek": 323324, "start": 3250.2799999999997, "end": 3252.8999999999996, "text": " So you could put the test set and the training set together", "tokens": [407, 291, 727, 829, 264, 1500, 992, 293, 264, 3097, 992, 1214], "temperature": 0.0, "avg_logprob": -0.15451344321755803, "compression_ratio": 1.7489539748953975, "no_speech_prob": 6.048875093256356e-06}, {"id": 678, "seek": 323324, "start": 3253.4799999999996, "end": 3259.7799999999997, "text": " Create a new column called is test and see if you can predict it if you can you don't have a random sample", "tokens": [20248, 257, 777, 7738, 1219, 307, 1500, 293, 536, 498, 291, 393, 6069, 309, 498, 291, 393, 291, 500, 380, 362, 257, 4974, 6889], "temperature": 0.0, "avg_logprob": -0.15451344321755803, "compression_ratio": 1.7489539748953975, "no_speech_prob": 6.048875093256356e-06}, {"id": 679, "seek": 325978, "start": 3259.78, "end": 3266.46, "text": " Which means you have to come and figure out how to create a validation set from it right and so in this case", "tokens": [3013, 1355, 291, 362, 281, 808, 293, 2573, 484, 577, 281, 1884, 257, 24071, 992, 490, 309, 558, 293, 370, 294, 341, 1389], "temperature": 0.0, "avg_logprob": -0.17316497946685216, "compression_ratio": 1.6973180076628354, "no_speech_prob": 8.059424203565868e-07}, {"id": 680, "seek": 325978, "start": 3266.46, "end": 3272.42, "text": " I can see I don't have a random sample because my validation set can be predicted with a point nine nine nine nine", "tokens": [286, 393, 536, 286, 500, 380, 362, 257, 4974, 6889, 570, 452, 24071, 992, 393, 312, 19147, 365, 257, 935, 4949, 4949, 4949, 4949], "temperature": 0.0, "avg_logprob": -0.17316497946685216, "compression_ratio": 1.6973180076628354, "no_speech_prob": 8.059424203565868e-07}, {"id": 681, "seek": 325978, "start": 3273.82, "end": 3275.82, "text": " r-squared and", "tokens": [367, 12, 33292, 1642, 293], "temperature": 0.0, "avg_logprob": -0.17316497946685216, "compression_ratio": 1.6973180076628354, "no_speech_prob": 8.059424203565868e-07}, {"id": 682, "seek": 325978, "start": 3276.1800000000003, "end": 3281.94, "text": " So then if I look at feature importance the top thing is sales ID and so this is really interesting", "tokens": [407, 550, 498, 286, 574, 412, 4111, 7379, 264, 1192, 551, 307, 5763, 7348, 293, 370, 341, 307, 534, 1880], "temperature": 0.0, "avg_logprob": -0.17316497946685216, "compression_ratio": 1.6973180076628354, "no_speech_prob": 8.059424203565868e-07}, {"id": 683, "seek": 325978, "start": 3281.94, "end": 3288.5800000000004, "text": " It tells us very clearly sales ID is not a random identifier, but probably it's something that's just set", "tokens": [467, 5112, 505, 588, 4448, 5763, 7348, 307, 406, 257, 4974, 45690, 11, 457, 1391, 309, 311, 746, 300, 311, 445, 992], "temperature": 0.0, "avg_logprob": -0.17316497946685216, "compression_ratio": 1.6973180076628354, "no_speech_prob": 8.059424203565868e-07}, {"id": 684, "seek": 328858, "start": 3288.58, "end": 3292.7, "text": " consecutively as time goes on we just increase the sales ID", "tokens": [27154, 3413, 382, 565, 1709, 322, 321, 445, 3488, 264, 5763, 7348], "temperature": 0.0, "avg_logprob": -0.24082318130804567, "compression_ratio": 1.7004048582995952, "no_speech_prob": 7.183214165706886e-06}, {"id": 685, "seek": 328858, "start": 3294.1, "end": 3301.62, "text": " So I'll elapsed that was the number of days since the first date in our data set so not surprisingly that also is a good predictor", "tokens": [407, 286, 603, 806, 2382, 292, 300, 390, 264, 1230, 295, 1708, 1670, 264, 700, 4002, 294, 527, 1412, 992, 370, 406, 17600, 300, 611, 307, 257, 665, 6069, 284], "temperature": 0.0, "avg_logprob": -0.24082318130804567, "compression_ratio": 1.7004048582995952, "no_speech_prob": 7.183214165706886e-06}, {"id": 686, "seek": 328858, "start": 3302.5, "end": 3304.5, "text": " interestingly machine ID", "tokens": [25873, 3479, 7348], "temperature": 0.0, "avg_logprob": -0.24082318130804567, "compression_ratio": 1.7004048582995952, "no_speech_prob": 7.183214165706886e-06}, {"id": 687, "seek": 328858, "start": 3305.7799999999997, "end": 3309.7799999999997, "text": " Clearly each machine is being labeled with some consecutive identifier as well", "tokens": [24120, 1184, 3479, 307, 885, 21335, 365, 512, 30497, 45690, 382, 731], "temperature": 0.0, "avg_logprob": -0.24082318130804567, "compression_ratio": 1.7004048582995952, "no_speech_prob": 7.183214165706886e-06}, {"id": 688, "seek": 328858, "start": 3310.66, "end": 3317.54, "text": " And then there's a big don't just look at the order look at the value so point seven point one point oh seven point oh oh two", "tokens": [400, 550, 456, 311, 257, 955, 500, 380, 445, 574, 412, 264, 1668, 574, 412, 264, 2158, 370, 935, 3407, 935, 472, 935, 1954, 3407, 935, 1954, 1954, 732], "temperature": 0.0, "avg_logprob": -0.24082318130804567, "compression_ratio": 1.7004048582995952, "no_speech_prob": 7.183214165706886e-06}, {"id": 689, "seek": 331754, "start": 3317.54, "end": 3325.38, "text": " Okay, stop right these top three hundreds of times more important than the rest right so let's next grab those top three", "tokens": [1033, 11, 1590, 558, 613, 1192, 1045, 6779, 295, 1413, 544, 1021, 813, 264, 1472, 558, 370, 718, 311, 958, 4444, 729, 1192, 1045], "temperature": 0.0, "avg_logprob": -0.1635542024265636, "compression_ratio": 1.7058823529411764, "no_speech_prob": 2.521564965718426e-06}, {"id": 690, "seek": 331754, "start": 3326.22, "end": 3331.14, "text": " Right and we can then have a look at their values", "tokens": [1779, 293, 321, 393, 550, 362, 257, 574, 412, 641, 4190], "temperature": 0.0, "avg_logprob": -0.1635542024265636, "compression_ratio": 1.7058823529411764, "no_speech_prob": 2.521564965718426e-06}, {"id": 691, "seek": 331754, "start": 3332.1, "end": 3334.1, "text": " both in the training set and", "tokens": [1293, 294, 264, 3097, 992, 293], "temperature": 0.0, "avg_logprob": -0.1635542024265636, "compression_ratio": 1.7058823529411764, "no_speech_prob": 2.521564965718426e-06}, {"id": 692, "seek": 331754, "start": 3334.46, "end": 3342.5, "text": " In the validation set and so we can see for example sales ID on average is a divided by a thousand on average is 1.8", "tokens": [682, 264, 24071, 992, 293, 370, 321, 393, 536, 337, 1365, 5763, 7348, 322, 4274, 307, 257, 6666, 538, 257, 4714, 322, 4274, 307, 502, 13, 23], "temperature": 0.0, "avg_logprob": -0.1635542024265636, "compression_ratio": 1.7058823529411764, "no_speech_prob": 2.521564965718426e-06}, {"id": 693, "seek": 331754, "start": 3342.5, "end": 3343.46, "text": " million", "tokens": [2459], "temperature": 0.0, "avg_logprob": -0.1635542024265636, "compression_ratio": 1.7058823529411764, "no_speech_prob": 2.521564965718426e-06}, {"id": 694, "seek": 331754, "start": 3343.46, "end": 3345.46, "text": " in the training set and", "tokens": [294, 264, 3097, 992, 293], "temperature": 0.0, "avg_logprob": -0.1635542024265636, "compression_ratio": 1.7058823529411764, "no_speech_prob": 2.521564965718426e-06}, {"id": 695, "seek": 334546, "start": 3345.46, "end": 3349.86, "text": " 5.8 million in the validation set right so you like you can see", "tokens": [1025, 13, 23, 2459, 294, 264, 24071, 992, 558, 370, 291, 411, 291, 393, 536], "temperature": 0.0, "avg_logprob": -0.19345620962289664, "compression_ratio": 1.6932773109243697, "no_speech_prob": 7.766880116832908e-06}, {"id": 696, "seek": 334546, "start": 3350.62, "end": 3352.7, "text": " Just confirm like okay. They're very different", "tokens": [1449, 9064, 411, 1392, 13, 814, 434, 588, 819], "temperature": 0.0, "avg_logprob": -0.19345620962289664, "compression_ratio": 1.6932773109243697, "no_speech_prob": 7.766880116832908e-06}, {"id": 697, "seek": 334546, "start": 3353.38, "end": 3355.38, "text": " So let's drop them", "tokens": [407, 718, 311, 3270, 552], "temperature": 0.0, "avg_logprob": -0.19345620962289664, "compression_ratio": 1.6932773109243697, "no_speech_prob": 7.766880116832908e-06}, {"id": 698, "seek": 334546, "start": 3356.14, "end": 3361.06, "text": " Okay, so after I drop them. Let's now see if I can predict whether something's in the validation set", "tokens": [1033, 11, 370, 934, 286, 3270, 552, 13, 961, 311, 586, 536, 498, 286, 393, 6069, 1968, 746, 311, 294, 264, 24071, 992], "temperature": 0.0, "avg_logprob": -0.19345620962289664, "compression_ratio": 1.6932773109243697, "no_speech_prob": 7.766880116832908e-06}, {"id": 699, "seek": 334546, "start": 3361.06, "end": 3363.2400000000002, "text": " I still can with point nine eight", "tokens": [286, 920, 393, 365, 935, 4949, 3180], "temperature": 0.0, "avg_logprob": -0.19345620962289664, "compression_ratio": 1.6932773109243697, "no_speech_prob": 7.766880116832908e-06}, {"id": 700, "seek": 334546, "start": 3364.2200000000003, "end": 3366.2200000000003, "text": " password", "tokens": [11524], "temperature": 0.0, "avg_logprob": -0.19345620962289664, "compression_ratio": 1.6932773109243697, "no_speech_prob": 7.766880116832908e-06}, {"id": 701, "seek": 334546, "start": 3366.46, "end": 3371.86, "text": " So once you remove some things then other things can like come to the front and it now turns out okay", "tokens": [407, 1564, 291, 4159, 512, 721, 550, 661, 721, 393, 411, 808, 281, 264, 1868, 293, 309, 586, 4523, 484, 1392], "temperature": 0.0, "avg_logprob": -0.19345620962289664, "compression_ratio": 1.6932773109243697, "no_speech_prob": 7.766880116832908e-06}, {"id": 702, "seek": 334546, "start": 3371.86, "end": 3373.86, "text": " That's not surprisingly age", "tokens": [663, 311, 406, 17600, 3205], "temperature": 0.0, "avg_logprob": -0.19345620962289664, "compression_ratio": 1.6932773109243697, "no_speech_prob": 7.766880116832908e-06}, {"id": 703, "seek": 337386, "start": 3373.86, "end": 3376.26, "text": " You know things that are old", "tokens": [509, 458, 721, 300, 366, 1331], "temperature": 0.0, "avg_logprob": -0.2625373899936676, "compression_ratio": 1.5202702702702702, "no_speech_prob": 3.7852914829272777e-06}, {"id": 704, "seek": 337386, "start": 3378.1800000000003, "end": 3384.5, "text": " You know more likely I guess to be in the validation set because if you know earlier on in the training set", "tokens": [509, 458, 544, 3700, 286, 2041, 281, 312, 294, 264, 24071, 992, 570, 498, 291, 458, 3071, 322, 294, 264, 3097, 992], "temperature": 0.0, "avg_logprob": -0.2625373899936676, "compression_ratio": 1.5202702702702702, "no_speech_prob": 3.7852914829272777e-06}, {"id": 705, "seek": 337386, "start": 3384.5, "end": 3386.5, "text": " They can't be old yet", "tokens": [814, 393, 380, 312, 1331, 1939], "temperature": 0.0, "avg_logprob": -0.2625373899936676, "compression_ratio": 1.5202702702702702, "no_speech_prob": 3.7852914829272777e-06}, {"id": 706, "seek": 337386, "start": 3386.54, "end": 3388.54, "text": " You made same reason", "tokens": [509, 1027, 912, 1778], "temperature": 0.0, "avg_logprob": -0.2625373899936676, "compression_ratio": 1.5202702702702702, "no_speech_prob": 3.7852914829272777e-06}, {"id": 707, "seek": 337386, "start": 3389.34, "end": 3391.34, "text": " so then we can", "tokens": [370, 550, 321, 393], "temperature": 0.0, "avg_logprob": -0.2625373899936676, "compression_ratio": 1.5202702702702702, "no_speech_prob": 3.7852914829272777e-06}, {"id": 708, "seek": 337386, "start": 3395.82, "end": 3397.82, "text": " Try removing those as well", "tokens": [6526, 12720, 729, 382, 731], "temperature": 0.0, "avg_logprob": -0.2625373899936676, "compression_ratio": 1.5202702702702702, "no_speech_prob": 3.7852914829272777e-06}, {"id": 709, "seek": 337386, "start": 3398.9, "end": 3400.82, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.2625373899936676, "compression_ratio": 1.5202702702702702, "no_speech_prob": 3.7852914829272777e-06}, {"id": 710, "seek": 340082, "start": 3400.82, "end": 3403.9, "text": " So once we let's see where do we go here?", "tokens": [407, 1564, 321, 718, 311, 536, 689, 360, 321, 352, 510, 30], "temperature": 0.0, "avg_logprob": -0.18201147991677988, "compression_ratio": 1.5876777251184835, "no_speech_prob": 6.339150331768906e-06}, {"id": 711, "seek": 340082, "start": 3404.42, "end": 3408.5800000000004, "text": " Yeah, so what we can try doing is we can then say all right. Let's take the sales ID", "tokens": [865, 11, 370, 437, 321, 393, 853, 884, 307, 321, 393, 550, 584, 439, 558, 13, 961, 311, 747, 264, 5763, 7348], "temperature": 0.0, "avg_logprob": -0.18201147991677988, "compression_ratio": 1.5876777251184835, "no_speech_prob": 6.339150331768906e-06}, {"id": 712, "seek": 340082, "start": 3408.5800000000004, "end": 3410.9, "text": " So that's machine ID from the first one", "tokens": [407, 300, 311, 3479, 7348, 490, 264, 700, 472], "temperature": 0.0, "avg_logprob": -0.18201147991677988, "compression_ratio": 1.5876777251184835, "no_speech_prob": 6.339150331768906e-06}, {"id": 713, "seek": 340082, "start": 3411.5, "end": 3416.6200000000003, "text": " The age year made sale day of year from the second one and say okay. These are all", "tokens": [440, 3205, 1064, 1027, 8680, 786, 295, 1064, 490, 264, 1150, 472, 293, 584, 1392, 13, 1981, 366, 439], "temperature": 0.0, "avg_logprob": -0.18201147991677988, "compression_ratio": 1.5876777251184835, "no_speech_prob": 6.339150331768906e-06}, {"id": 714, "seek": 340082, "start": 3417.86, "end": 3419.86, "text": " time dependent features", "tokens": [565, 12334, 4122], "temperature": 0.0, "avg_logprob": -0.18201147991677988, "compression_ratio": 1.5876777251184835, "no_speech_prob": 6.339150331768906e-06}, {"id": 715, "seek": 340082, "start": 3421.86, "end": 3425.6400000000003, "text": " So I still want them in my random forest if they're important", "tokens": [407, 286, 920, 528, 552, 294, 452, 4974, 6719, 498, 436, 434, 1021], "temperature": 0.0, "avg_logprob": -0.18201147991677988, "compression_ratio": 1.5876777251184835, "no_speech_prob": 6.339150331768906e-06}, {"id": 716, "seek": 342564, "start": 3425.64, "end": 3430.24, "text": " Right, but if they're not important then taking them out", "tokens": [1779, 11, 457, 498, 436, 434, 406, 1021, 550, 1940, 552, 484], "temperature": 0.0, "avg_logprob": -0.13504455286428468, "compression_ratio": 1.7164750957854407, "no_speech_prob": 7.112411140042241e-07}, {"id": 717, "seek": 342564, "start": 3430.24, "end": 3434.96, "text": " There are some other non time dependent variables that that work just as well that would be better", "tokens": [821, 366, 512, 661, 2107, 565, 12334, 9102, 300, 300, 589, 445, 382, 731, 300, 576, 312, 1101], "temperature": 0.0, "avg_logprob": -0.13504455286428468, "compression_ratio": 1.7164750957854407, "no_speech_prob": 7.112411140042241e-07}, {"id": 718, "seek": 342564, "start": 3435.6, "end": 3438.6, "text": " Right because now I'm going to have a model that generalizes over time better", "tokens": [1779, 570, 586, 286, 478, 516, 281, 362, 257, 2316, 300, 2674, 5660, 670, 565, 1101], "temperature": 0.0, "avg_logprob": -0.13504455286428468, "compression_ratio": 1.7164750957854407, "no_speech_prob": 7.112411140042241e-07}, {"id": 719, "seek": 342564, "start": 3438.72, "end": 3444.1, "text": " So here I'm just going to go ahead and go through each one of those features and drop each one one at a time", "tokens": [407, 510, 286, 478, 445, 516, 281, 352, 2286, 293, 352, 807, 1184, 472, 295, 729, 4122, 293, 3270, 1184, 472, 472, 412, 257, 565], "temperature": 0.0, "avg_logprob": -0.13504455286428468, "compression_ratio": 1.7164750957854407, "no_speech_prob": 7.112411140042241e-07}, {"id": 720, "seek": 342564, "start": 3445.24, "end": 3448.94, "text": " Okay, retrain a new random forest and print out the score", "tokens": [1033, 11, 1533, 7146, 257, 777, 4974, 6719, 293, 4482, 484, 264, 6175], "temperature": 0.0, "avg_logprob": -0.13504455286428468, "compression_ratio": 1.7164750957854407, "no_speech_prob": 7.112411140042241e-07}, {"id": 721, "seek": 344894, "start": 3448.94, "end": 3456.42, "text": " Okay, so before we do any of that our score was 0.88", "tokens": [1033, 11, 370, 949, 321, 360, 604, 295, 300, 527, 6175, 390, 1958, 13, 16919], "temperature": 0.0, "avg_logprob": -0.2847867347824741, "compression_ratio": 1.406060606060606, "no_speech_prob": 1.6280473573715426e-06}, {"id": 722, "seek": 344894, "start": 3457.58, "end": 3459.38, "text": " for our", "tokens": [337, 527], "temperature": 0.0, "avg_logprob": -0.2847867347824741, "compression_ratio": 1.406060606060606, "no_speech_prob": 1.6280473573715426e-06}, {"id": 723, "seek": 344894, "start": 3459.38, "end": 3460.62, "text": " validation", "tokens": [24071], "temperature": 0.0, "avg_logprob": -0.2847867347824741, "compression_ratio": 1.406060606060606, "no_speech_prob": 1.6280473573715426e-06}, {"id": 724, "seek": 344894, "start": 3460.62, "end": 3462.62, "text": " versus 0.89 oob and", "tokens": [5717, 1958, 13, 21115, 277, 996, 293], "temperature": 0.0, "avg_logprob": -0.2847867347824741, "compression_ratio": 1.406060606060606, "no_speech_prob": 1.6280473573715426e-06}, {"id": 725, "seek": 344894, "start": 3463.42, "end": 3465.42, "text": " You can see here", "tokens": [509, 393, 536, 510], "temperature": 0.0, "avg_logprob": -0.2847867347824741, "compression_ratio": 1.406060606060606, "no_speech_prob": 1.6280473573715426e-06}, {"id": 726, "seek": 344894, "start": 3465.54, "end": 3469.14, "text": " when I remove sales ID my score goes up and", "tokens": [562, 286, 4159, 5763, 7348, 452, 6175, 1709, 493, 293], "temperature": 0.0, "avg_logprob": -0.2847867347824741, "compression_ratio": 1.406060606060606, "no_speech_prob": 1.6280473573715426e-06}, {"id": 727, "seek": 344894, "start": 3469.94, "end": 3474.26, "text": " This this is like what we're hoping for we've removed a time dependent variable", "tokens": [639, 341, 307, 411, 437, 321, 434, 7159, 337, 321, 600, 7261, 257, 565, 12334, 7006], "temperature": 0.0, "avg_logprob": -0.2847867347824741, "compression_ratio": 1.406060606060606, "no_speech_prob": 1.6280473573715426e-06}, {"id": 728, "seek": 347426, "start": 3474.26, "end": 3479.98, "text": " There were other variables that could find similar relationships without the time dependency so removing it", "tokens": [821, 645, 661, 9102, 300, 727, 915, 2531, 6159, 1553, 264, 565, 33621, 370, 12720, 309], "temperature": 0.0, "avg_logprob": -0.21224105601408044, "compression_ratio": 1.6944444444444444, "no_speech_prob": 5.368739834921143e-07}, {"id": 729, "seek": 347426, "start": 3480.3, "end": 3484.1000000000004, "text": " Caused our validation to go up now. Oh, oh B didn't go up", "tokens": [7544, 4717, 527, 24071, 281, 352, 493, 586, 13, 876, 11, 1954, 363, 994, 380, 352, 493], "temperature": 0.0, "avg_logprob": -0.21224105601408044, "compression_ratio": 1.6944444444444444, "no_speech_prob": 5.368739834921143e-07}, {"id": 730, "seek": 347426, "start": 3484.6600000000003, "end": 3486.6600000000003, "text": " Right because this is genuinely", "tokens": [1779, 570, 341, 307, 17839], "temperature": 0.0, "avg_logprob": -0.21224105601408044, "compression_ratio": 1.6944444444444444, "no_speech_prob": 5.368739834921143e-07}, {"id": 731, "seek": 347426, "start": 3486.7000000000003, "end": 3492.5400000000004, "text": " Statistically a useful predictor right, but it's a time dependent one and we have a time dependent validation set", "tokens": [16249, 20458, 257, 4420, 6069, 284, 558, 11, 457, 309, 311, 257, 565, 12334, 472, 293, 321, 362, 257, 565, 12334, 24071, 992], "temperature": 0.0, "avg_logprob": -0.21224105601408044, "compression_ratio": 1.6944444444444444, "no_speech_prob": 5.368739834921143e-07}, {"id": 732, "seek": 347426, "start": 3492.5400000000004, "end": 3498.82, "text": " So this is like really subtle, but it can be really important right it's trying to find the things that gives you a", "tokens": [407, 341, 307, 411, 534, 13743, 11, 457, 309, 393, 312, 534, 1021, 558, 309, 311, 1382, 281, 915, 264, 721, 300, 2709, 291, 257], "temperature": 0.0, "avg_logprob": -0.21224105601408044, "compression_ratio": 1.6944444444444444, "no_speech_prob": 5.368739834921143e-07}, {"id": 733, "seek": 349882, "start": 3498.82, "end": 3504.5, "text": " a generalizable time across time prediction, and here's how you can see it so by so it's like okay", "tokens": [257, 2674, 22395, 565, 2108, 565, 17630, 11, 293, 510, 311, 577, 291, 393, 536, 309, 370, 538, 370, 309, 311, 411, 1392], "temperature": 0.0, "avg_logprob": -0.21169172286987303, "compression_ratio": 1.592920353982301, "no_speech_prob": 3.844914772344055e-06}, {"id": 734, "seek": 349882, "start": 3504.54, "end": 3510.6600000000003, "text": " We should remove sales ID for sure right but sale elapsed didn't get better", "tokens": [492, 820, 4159, 5763, 7348, 337, 988, 558, 457, 8680, 806, 2382, 292, 994, 380, 483, 1101], "temperature": 0.0, "avg_logprob": -0.21169172286987303, "compression_ratio": 1.592920353982301, "no_speech_prob": 3.844914772344055e-06}, {"id": 735, "seek": 349882, "start": 3511.5, "end": 3513.5, "text": " Okay, so we don't want that machine ID", "tokens": [1033, 11, 370, 321, 500, 380, 528, 300, 3479, 7348], "temperature": 0.0, "avg_logprob": -0.21169172286987303, "compression_ratio": 1.592920353982301, "no_speech_prob": 3.844914772344055e-06}, {"id": 736, "seek": 349882, "start": 3514.1400000000003, "end": 3518.6600000000003, "text": " Did get better went from 888 to 893 so it's actually quite a bit better", "tokens": [2589, 483, 1101, 1437, 490, 1649, 16919, 281, 1649, 26372, 370, 309, 311, 767, 1596, 257, 857, 1101], "temperature": 0.0, "avg_logprob": -0.21169172286987303, "compression_ratio": 1.592920353982301, "no_speech_prob": 3.844914772344055e-06}, {"id": 737, "seek": 349882, "start": 3521.46, "end": 3522.82, "text": " Age", "tokens": [16280], "temperature": 0.0, "avg_logprob": -0.21169172286987303, "compression_ratio": 1.592920353982301, "no_speech_prob": 3.844914772344055e-06}, {"id": 738, "seek": 349882, "start": 3522.82, "end": 3524.46, "text": " Got a bit better", "tokens": [5803, 257, 857, 1101], "temperature": 0.0, "avg_logprob": -0.21169172286987303, "compression_ratio": 1.592920353982301, "no_speech_prob": 3.844914772344055e-06}, {"id": 739, "seek": 352446, "start": 3524.46, "end": 3531.86, "text": " Year made got worse sale day of year got a bit better okay, so now we can say alright", "tokens": [10289, 1027, 658, 5324, 8680, 786, 295, 1064, 658, 257, 857, 1101, 1392, 11, 370, 586, 321, 393, 584, 5845], "temperature": 0.0, "avg_logprob": -0.18210995581842238, "compression_ratio": 1.596244131455399, "no_speech_prob": 9.132483569374017e-07}, {"id": 740, "seek": 352446, "start": 3532.06, "end": 3534.06, "text": " Let's get rid of", "tokens": [961, 311, 483, 3973, 295], "temperature": 0.0, "avg_logprob": -0.18210995581842238, "compression_ratio": 1.596244131455399, "no_speech_prob": 9.132483569374017e-07}, {"id": 741, "seek": 352446, "start": 3534.2200000000003, "end": 3536.1, "text": " the three", "tokens": [264, 1045], "temperature": 0.0, "avg_logprob": -0.18210995581842238, "compression_ratio": 1.596244131455399, "no_speech_prob": 9.132483569374017e-07}, {"id": 742, "seek": 352446, "start": 3536.1, "end": 3538.34, "text": " Where we know that getting rid of it actually made it better", "tokens": [2305, 321, 458, 300, 1242, 3973, 295, 309, 767, 1027, 309, 1101], "temperature": 0.0, "avg_logprob": -0.18210995581842238, "compression_ratio": 1.596244131455399, "no_speech_prob": 9.132483569374017e-07}, {"id": 743, "seek": 352446, "start": 3539.5, "end": 3543.38, "text": " Okay, and as a result look at this we're now up to 915", "tokens": [1033, 11, 293, 382, 257, 1874, 574, 412, 341, 321, 434, 586, 493, 281, 1722, 5211], "temperature": 0.0, "avg_logprob": -0.18210995581842238, "compression_ratio": 1.596244131455399, "no_speech_prob": 9.132483569374017e-07}, {"id": 744, "seek": 354338, "start": 3543.38, "end": 3553.08, "text": " Okay, so we've got rid of three time dependent things and now as expected our validation is better than our OB", "tokens": [1033, 11, 370, 321, 600, 658, 3973, 295, 1045, 565, 12334, 721, 293, 586, 382, 5176, 527, 24071, 307, 1101, 813, 527, 35538], "temperature": 0.0, "avg_logprob": -0.2131150843022944, "compression_ratio": 1.5965665236051503, "no_speech_prob": 8.801006515568588e-06}, {"id": 745, "seek": 354338, "start": 3554.82, "end": 3559.58, "text": " Okay, so that was a super successful approach there right and so now we can check the feature importance", "tokens": [1033, 11, 370, 300, 390, 257, 1687, 4406, 3109, 456, 558, 293, 370, 586, 321, 393, 1520, 264, 4111, 7379], "temperature": 0.0, "avg_logprob": -0.2131150843022944, "compression_ratio": 1.5965665236051503, "no_speech_prob": 8.801006515568588e-06}, {"id": 746, "seek": 354338, "start": 3561.02, "end": 3566.92, "text": " And let's go ahead and say alright that was pretty damn good. Let's now", "tokens": [400, 718, 311, 352, 2286, 293, 584, 5845, 300, 390, 1238, 8151, 665, 13, 961, 311, 586], "temperature": 0.0, "avg_logprob": -0.2131150843022944, "compression_ratio": 1.5965665236051503, "no_speech_prob": 8.801006515568588e-06}, {"id": 747, "seek": 356692, "start": 3566.92, "end": 3573.12, "text": " Leave it for a while, so give it 160 trees. Let it chill on it and see how that goes", "tokens": [9825, 309, 337, 257, 1339, 11, 370, 976, 309, 21243, 5852, 13, 961, 309, 11355, 322, 309, 293, 536, 577, 300, 1709], "temperature": 0.0, "avg_logprob": -0.22317216873168946, "compression_ratio": 1.5833333333333333, "no_speech_prob": 2.406083694950212e-06}, {"id": 748, "seek": 356692, "start": 3573.88, "end": 3578.88, "text": " Okay, and so as you can see like we did all of our interpretation all of our fine-tuning", "tokens": [1033, 11, 293, 370, 382, 291, 393, 536, 411, 321, 630, 439, 295, 527, 14174, 439, 295, 527, 2489, 12, 83, 37726], "temperature": 0.0, "avg_logprob": -0.22317216873168946, "compression_ratio": 1.5833333333333333, "no_speech_prob": 2.406083694950212e-06}, {"id": 749, "seek": 356692, "start": 3579.96, "end": 3586.32, "text": " Basically with smaller models subsets and at the end we run the whole thing it actually still only took 16 seconds", "tokens": [8537, 365, 4356, 5245, 2090, 1385, 293, 412, 264, 917, 321, 1190, 264, 1379, 551, 309, 767, 920, 787, 1890, 3165, 3949], "temperature": 0.0, "avg_logprob": -0.22317216873168946, "compression_ratio": 1.5833333333333333, "no_speech_prob": 2.406083694950212e-06}, {"id": 750, "seek": 356692, "start": 3587.8, "end": 3594.7200000000003, "text": " And so we've now got an RMSE of point two one okay, so now we can check that against Kaggle", "tokens": [400, 370, 321, 600, 586, 658, 364, 23790, 5879, 295, 935, 732, 472, 1392, 11, 370, 586, 321, 393, 1520, 300, 1970, 48751, 22631], "temperature": 0.0, "avg_logprob": -0.22317216873168946, "compression_ratio": 1.5833333333333333, "no_speech_prob": 2.406083694950212e-06}, {"id": 751, "seek": 359472, "start": 3594.72, "end": 3598.72, "text": " again, we can't we", "tokens": [797, 11, 321, 393, 380, 321], "temperature": 0.0, "avg_logprob": -0.1968816121419271, "compression_ratio": 1.600896860986547, "no_speech_prob": 2.295903186677606e-06}, {"id": 752, "seek": 359472, "start": 3599.8799999999997, "end": 3601.8799999999997, "text": " unfortunately this", "tokens": [7015, 341], "temperature": 0.0, "avg_logprob": -0.1968816121419271, "compression_ratio": 1.600896860986547, "no_speech_prob": 2.295903186677606e-06}, {"id": 753, "seek": 359472, "start": 3601.8799999999997, "end": 3606.58, "text": " Older competition we're not allowed to enter any more to see how we would have gone so the best we can do is check", "tokens": [8633, 260, 6211, 321, 434, 406, 4350, 281, 3242, 604, 544, 281, 536, 577, 321, 576, 362, 2780, 370, 264, 1151, 321, 393, 360, 307, 1520], "temperature": 0.0, "avg_logprob": -0.1968816121419271, "compression_ratio": 1.600896860986547, "no_speech_prob": 2.295903186677606e-06}, {"id": 754, "seek": 359472, "start": 3607.52, "end": 3610.62, "text": " Whether it looks like we could have done well based on our validation set", "tokens": [8503, 309, 1542, 411, 321, 727, 362, 1096, 731, 2361, 322, 527, 24071, 992], "temperature": 0.0, "avg_logprob": -0.1968816121419271, "compression_ratio": 1.600896860986547, "no_speech_prob": 2.295903186677606e-06}, {"id": 755, "seek": 359472, "start": 3611.12, "end": 3615.2, "text": " So it should be in the right area and yeah based on that we would have come first", "tokens": [407, 309, 820, 312, 294, 264, 558, 1859, 293, 1338, 2361, 322, 300, 321, 576, 362, 808, 700], "temperature": 0.0, "avg_logprob": -0.1968816121419271, "compression_ratio": 1.600896860986547, "no_speech_prob": 2.295903186677606e-06}, {"id": 756, "seek": 359472, "start": 3616.9199999999996, "end": 3618.9199999999996, "text": " Okay, so", "tokens": [1033, 11, 370], "temperature": 0.0, "avg_logprob": -0.1968816121419271, "compression_ratio": 1.600896860986547, "no_speech_prob": 2.295903186677606e-06}, {"id": 757, "seek": 359472, "start": 3619.3199999999997, "end": 3621.3199999999997, "text": " You know I think this is an interesting", "tokens": [509, 458, 286, 519, 341, 307, 364, 1880], "temperature": 0.0, "avg_logprob": -0.1968816121419271, "compression_ratio": 1.600896860986547, "no_speech_prob": 2.295903186677606e-06}, {"id": 758, "seek": 362132, "start": 3621.32, "end": 3626.88, "text": " series of steps right so you can go through the same series of steps in your", "tokens": [2638, 295, 4439, 558, 370, 291, 393, 352, 807, 264, 912, 2638, 295, 4439, 294, 428], "temperature": 0.0, "avg_logprob": -0.19778501859275244, "compression_ratio": 1.7219917012448134, "no_speech_prob": 3.1381136977870483e-06}, {"id": 759, "seek": 362132, "start": 3627.6400000000003, "end": 3635.2000000000003, "text": " Kaggle projects and more importantly your real-world projects so one of the challenges is once you leave this learning environment", "tokens": [48751, 22631, 4455, 293, 544, 8906, 428, 957, 12, 13217, 4455, 370, 472, 295, 264, 4759, 307, 1564, 291, 1856, 341, 2539, 2823], "temperature": 0.0, "avg_logprob": -0.19778501859275244, "compression_ratio": 1.7219917012448134, "no_speech_prob": 3.1381136977870483e-06}, {"id": 760, "seek": 362132, "start": 3635.76, "end": 3640.8, "text": " Suddenly you're surrounded by people who they never have enough time. They always want you to be in a hurry", "tokens": [21194, 291, 434, 13221, 538, 561, 567, 436, 1128, 362, 1547, 565, 13, 814, 1009, 528, 291, 281, 312, 294, 257, 11025], "temperature": 0.0, "avg_logprob": -0.19778501859275244, "compression_ratio": 1.7219917012448134, "no_speech_prob": 3.1381136977870483e-06}, {"id": 761, "seek": 362132, "start": 3641.1200000000003, "end": 3645.76, "text": " They're always telling you you know do this and then do that you need to find the time to step away", "tokens": [814, 434, 1009, 3585, 291, 291, 458, 360, 341, 293, 550, 360, 300, 291, 643, 281, 915, 264, 565, 281, 1823, 1314], "temperature": 0.0, "avg_logprob": -0.19778501859275244, "compression_ratio": 1.7219917012448134, "no_speech_prob": 3.1381136977870483e-06}, {"id": 762, "seek": 364576, "start": 3645.76, "end": 3651.0800000000004, "text": " Right and go back because this is a genuine real-world modeling process you can use", "tokens": [1779, 293, 352, 646, 570, 341, 307, 257, 16699, 957, 12, 13217, 15983, 1399, 291, 393, 764], "temperature": 0.0, "avg_logprob": -0.30313403265816824, "compression_ratio": 1.541062801932367, "no_speech_prob": 9.516122190689202e-06}, {"id": 763, "seek": 364576, "start": 3651.7200000000003, "end": 3655.0400000000004, "text": " And it gives well I said it gives world-class results", "tokens": [400, 309, 2709, 731, 286, 848, 309, 2709, 1002, 12, 11665, 3542], "temperature": 0.0, "avg_logprob": -0.30313403265816824, "compression_ratio": 1.541062801932367, "no_speech_prob": 9.516122190689202e-06}, {"id": 764, "seek": 364576, "start": 3655.0400000000004, "end": 3660.5600000000004, "text": " I I mean it right like this guy who won this list of costs sadly he's passed away", "tokens": [286, 286, 914, 309, 558, 411, 341, 2146, 567, 1582, 341, 1329, 295, 5497, 22023, 415, 311, 4678, 1314], "temperature": 0.0, "avg_logprob": -0.30313403265816824, "compression_ratio": 1.541062801932367, "no_speech_prob": 9.516122190689202e-06}, {"id": 765, "seek": 364576, "start": 3661.0400000000004, "end": 3663.0400000000004, "text": " But he is the top", "tokens": [583, 415, 307, 264, 1192], "temperature": 0.0, "avg_logprob": -0.30313403265816824, "compression_ratio": 1.541062801932367, "no_speech_prob": 9.516122190689202e-06}, {"id": 766, "seek": 364576, "start": 3663.88, "end": 3665.88, "text": " Kaggle", "tokens": [48751, 22631], "temperature": 0.0, "avg_logprob": -0.30313403265816824, "compression_ratio": 1.541062801932367, "no_speech_prob": 9.516122190689202e-06}, {"id": 767, "seek": 364576, "start": 3666.0400000000004, "end": 3671.6200000000003, "text": " Competitor of all time like he he won I believe like dozens of competition", "tokens": [32216, 3029, 295, 439, 565, 411, 415, 415, 1582, 286, 1697, 411, 18431, 295, 6211], "temperature": 0.0, "avg_logprob": -0.30313403265816824, "compression_ratio": 1.541062801932367, "no_speech_prob": 9.516122190689202e-06}, {"id": 768, "seek": 367162, "start": 3671.62, "end": 3677.16, "text": " So if we can get a score even within kui of him, then we are doing really really well", "tokens": [407, 498, 321, 393, 483, 257, 6175, 754, 1951, 350, 3077, 295, 796, 11, 550, 321, 366, 884, 534, 534, 731], "temperature": 0.0, "avg_logprob": -0.22653673676883473, "compression_ratio": 1.4861878453038675, "no_speech_prob": 3.6688247746496927e-06}, {"id": 769, "seek": 367162, "start": 3679.48, "end": 3684.2799999999997, "text": " Okay, so let's take a five-minute break, and we're going to come back and build our own random forest", "tokens": [1033, 11, 370, 718, 311, 747, 257, 1732, 12, 18256, 1821, 11, 293, 321, 434, 516, 281, 808, 646, 293, 1322, 527, 1065, 4974, 6719], "temperature": 0.0, "avg_logprob": -0.22653673676883473, "compression_ratio": 1.4861878453038675, "no_speech_prob": 3.6688247746496927e-06}, {"id": 770, "seek": 368428, "start": 3684.28, "end": 3698.3, "text": " I just wanted to clarify something quickly a very good point during the break was", "tokens": [286, 445, 1415, 281, 17594, 746, 2661, 257, 588, 665, 935, 1830, 264, 1821, 390], "temperature": 0.0, "avg_logprob": -0.279563401874743, "compression_ratio": 1.2477064220183487, "no_speech_prob": 4.2892215788015164e-06}, {"id": 771, "seek": 368428, "start": 3702.36, "end": 3704.36, "text": " Going back to the", "tokens": [10963, 646, 281, 264], "temperature": 0.0, "avg_logprob": -0.279563401874743, "compression_ratio": 1.2477064220183487, "no_speech_prob": 4.2892215788015164e-06}, {"id": 772, "seek": 368428, "start": 3705.6000000000004, "end": 3707.6000000000004, "text": " change in r-squared", "tokens": [1319, 294, 367, 12, 33292, 1642], "temperature": 0.0, "avg_logprob": -0.279563401874743, "compression_ratio": 1.2477064220183487, "no_speech_prob": 4.2892215788015164e-06}, {"id": 773, "seek": 368428, "start": 3707.92, "end": 3709.92, "text": " between here and", "tokens": [1296, 510, 293], "temperature": 0.0, "avg_logprob": -0.279563401874743, "compression_ratio": 1.2477064220183487, "no_speech_prob": 4.2892215788015164e-06}, {"id": 774, "seek": 370992, "start": 3709.92, "end": 3714.32, "text": " Here it's not just due to the fact that we removed", "tokens": [1692, 309, 311, 406, 445, 3462, 281, 264, 1186, 300, 321, 7261], "temperature": 0.0, "avg_logprob": -0.20457122132584854, "compression_ratio": 1.5737704918032787, "no_speech_prob": 1.0129802831215784e-05}, {"id": 775, "seek": 370992, "start": 3715.7200000000003, "end": 3717.7200000000003, "text": " these three predictors", "tokens": [613, 1045, 6069, 830], "temperature": 0.0, "avg_logprob": -0.20457122132584854, "compression_ratio": 1.5737704918032787, "no_speech_prob": 1.0129802831215784e-05}, {"id": 776, "seek": 370992, "start": 3718.2400000000002, "end": 3724.64, "text": " We also went reset RF samples, but so to actually see the impact of just removing we need to compare it to", "tokens": [492, 611, 1437, 14322, 26204, 10938, 11, 457, 370, 281, 767, 536, 264, 2712, 295, 445, 12720, 321, 643, 281, 6794, 309, 281], "temperature": 0.0, "avg_logprob": -0.20457122132584854, "compression_ratio": 1.5737704918032787, "no_speech_prob": 1.0129802831215784e-05}, {"id": 777, "seek": 370992, "start": 3727.2400000000002, "end": 3732.44, "text": " The final step earlier, so it's actually compared to 907 so removing those three things took us from", "tokens": [440, 2572, 1823, 3071, 11, 370, 309, 311, 767, 5347, 281, 4289, 22, 370, 12720, 729, 1045, 721, 1890, 505, 490], "temperature": 0.0, "avg_logprob": -0.20457122132584854, "compression_ratio": 1.5737704918032787, "no_speech_prob": 1.0129802831215784e-05}, {"id": 778, "seek": 373244, "start": 3732.44, "end": 3741.8, "text": " 907 to 915", "tokens": [4289, 22, 281, 1722, 5211], "temperature": 0.0, "avg_logprob": -0.2849673567147091, "compression_ratio": 1.3333333333333333, "no_speech_prob": 1.130057262344053e-05}, {"id": 779, "seek": 373244, "start": 3742.04, "end": 3747.36, "text": " Okay, so I mean and you know in the end of course what matters is our final model that yeah, just to clarify", "tokens": [1033, 11, 370, 286, 914, 293, 291, 458, 294, 264, 917, 295, 1164, 437, 7001, 307, 527, 2572, 2316, 300, 1338, 11, 445, 281, 17594], "temperature": 0.0, "avg_logprob": -0.2849673567147091, "compression_ratio": 1.3333333333333333, "no_speech_prob": 1.130057262344053e-05}, {"id": 780, "seek": 373244, "start": 3749.36, "end": 3751.36, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.2849673567147091, "compression_ratio": 1.3333333333333333, "no_speech_prob": 1.130057262344053e-05}, {"id": 781, "seek": 373244, "start": 3751.52, "end": 3753.52, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.2849673567147091, "compression_ratio": 1.3333333333333333, "no_speech_prob": 1.130057262344053e-05}, {"id": 782, "seek": 373244, "start": 3754.76, "end": 3757.96, "text": " Some of you have asked me about writing your own random forests from scratch", "tokens": [2188, 295, 291, 362, 2351, 385, 466, 3579, 428, 1065, 4974, 21700, 490, 8459], "temperature": 0.0, "avg_logprob": -0.2849673567147091, "compression_ratio": 1.3333333333333333, "no_speech_prob": 1.130057262344053e-05}, {"id": 783, "seek": 375796, "start": 3757.96, "end": 3763.56, "text": " I don't know if any of you have given it a try yet my original plan here was to", "tokens": [286, 500, 380, 458, 498, 604, 295, 291, 362, 2212, 309, 257, 853, 1939, 452, 3380, 1393, 510, 390, 281], "temperature": 0.0, "avg_logprob": -0.1530747500332919, "compression_ratio": 1.6642066420664208, "no_speech_prob": 1.834259273891803e-05}, {"id": 784, "seek": 375796, "start": 3764.2, "end": 3767.16, "text": " Do it in real time and then as I started to do it", "tokens": [1144, 309, 294, 957, 565, 293, 550, 382, 286, 1409, 281, 360, 309], "temperature": 0.0, "avg_logprob": -0.1530747500332919, "compression_ratio": 1.6642066420664208, "no_speech_prob": 1.834259273891803e-05}, {"id": 785, "seek": 375796, "start": 3767.16, "end": 3770.52, "text": " I realized that that would have kind of been boring because for you because I", "tokens": [286, 5334, 300, 300, 576, 362, 733, 295, 668, 9989, 570, 337, 291, 570, 286], "temperature": 0.0, "avg_logprob": -0.1530747500332919, "compression_ratio": 1.6642066420664208, "no_speech_prob": 1.834259273891803e-05}, {"id": 786, "seek": 375796, "start": 3770.88, "end": 3775.92, "text": " Screwed things up all the time so instead we might do more of like a walk through the code together", "tokens": [42630, 292, 721, 493, 439, 264, 565, 370, 2602, 321, 1062, 360, 544, 295, 411, 257, 1792, 807, 264, 3089, 1214], "temperature": 0.0, "avg_logprob": -0.1530747500332919, "compression_ratio": 1.6642066420664208, "no_speech_prob": 1.834259273891803e-05}, {"id": 787, "seek": 375796, "start": 3779.56, "end": 3781.56, "text": " Just as an aside", "tokens": [1449, 382, 364, 7359], "temperature": 0.0, "avg_logprob": -0.1530747500332919, "compression_ratio": 1.6642066420664208, "no_speech_prob": 1.834259273891803e-05}, {"id": 788, "seek": 378156, "start": 3781.56, "end": 3787.96, "text": " This reminds me talking about the exam actually somebody asked on the forum about like what what can you expect from the exam?", "tokens": [639, 12025, 385, 1417, 466, 264, 1139, 767, 2618, 2351, 322, 264, 17542, 466, 411, 437, 437, 393, 291, 2066, 490, 264, 1139, 30], "temperature": 0.0, "avg_logprob": -0.1583866889660175, "compression_ratio": 1.7335766423357664, "no_speech_prob": 1.4510246728605125e-05}, {"id": 789, "seek": 378156, "start": 3787.96, "end": 3791.24, "text": " the basic plan is to make it a", "tokens": [264, 3875, 1393, 307, 281, 652, 309, 257], "temperature": 0.0, "avg_logprob": -0.1583866889660175, "compression_ratio": 1.7335766423357664, "no_speech_prob": 1.4510246728605125e-05}, {"id": 790, "seek": 378156, "start": 3791.7599999999998, "end": 3797.56, "text": " The exam be very similar to these notebooks, so it'll probably be a notebook that you have to you know", "tokens": [440, 1139, 312, 588, 2531, 281, 613, 43782, 11, 370, 309, 603, 1391, 312, 257, 21060, 300, 291, 362, 281, 291, 458], "temperature": 0.0, "avg_logprob": -0.1583866889660175, "compression_ratio": 1.7335766423357664, "no_speech_prob": 1.4510246728605125e-05}, {"id": 791, "seek": 378156, "start": 3798.32, "end": 3805.2, "text": " Get a data set create a model trainer feature importance whatever right and the plan is that it'll be", "tokens": [3240, 257, 1412, 992, 1884, 257, 2316, 21110, 4111, 7379, 2035, 558, 293, 264, 1393, 307, 300, 309, 603, 312], "temperature": 0.0, "avg_logprob": -0.1583866889660175, "compression_ratio": 1.7335766423357664, "no_speech_prob": 1.4510246728605125e-05}, {"id": 792, "seek": 380520, "start": 3805.2, "end": 3813.6, "text": " Open book open internet you can use whatever resources you like so basically if you're entering cat competitions the exam should be very straightforward I", "tokens": [7238, 1446, 1269, 4705, 291, 393, 764, 2035, 3593, 291, 411, 370, 1936, 498, 291, 434, 11104, 3857, 26185, 264, 1139, 820, 312, 588, 15325, 286], "temperature": 0.0, "avg_logprob": -0.23853640860699593, "compression_ratio": 1.6958174904942966, "no_speech_prob": 1.5534845942966058e-06}, {"id": 793, "seek": 380520, "start": 3814.8399999999997, "end": 3818.52, "text": " Also expect that there will be some pieces about like", "tokens": [2743, 2066, 300, 456, 486, 312, 512, 3755, 466, 411], "temperature": 0.0, "avg_logprob": -0.23853640860699593, "compression_ratio": 1.6958174904942966, "no_speech_prob": 1.5534845942966058e-06}, {"id": 794, "seek": 380520, "start": 3819.12, "end": 3822.64, "text": " Here's a partially completed random forest or something you know finish", "tokens": [1692, 311, 257, 18886, 7365, 4974, 6719, 420, 746, 291, 458, 2413], "temperature": 0.0, "avg_logprob": -0.23853640860699593, "compression_ratio": 1.6958174904942966, "no_speech_prob": 1.5534845942966058e-06}, {"id": 795, "seek": 380520, "start": 3823.3999999999996, "end": 3826.08, "text": " Finish writing this step here, or here's a random forest", "tokens": [31583, 3579, 341, 1823, 510, 11, 420, 510, 311, 257, 4974, 6719], "temperature": 0.0, "avg_logprob": -0.23853640860699593, "compression_ratio": 1.6958174904942966, "no_speech_prob": 1.5534845942966058e-06}, {"id": 796, "seek": 380520, "start": 3826.7999999999997, "end": 3834.08, "text": " Implement feature importance or you know implement one of the things we've talked about so it'll be you know", "tokens": [4331, 43704, 4111, 7379, 420, 291, 458, 4445, 472, 295, 264, 721, 321, 600, 2825, 466, 370, 309, 603, 312, 291, 458], "temperature": 0.0, "avg_logprob": -0.23853640860699593, "compression_ratio": 1.6958174904942966, "no_speech_prob": 1.5534845942966058e-06}, {"id": 797, "seek": 383408, "start": 3834.08, "end": 3839.34, "text": " The exam will be much like what we do in class and what you're expected to be doing during the week", "tokens": [440, 1139, 486, 312, 709, 411, 437, 321, 360, 294, 1508, 293, 437, 291, 434, 5176, 281, 312, 884, 1830, 264, 1243], "temperature": 0.0, "avg_logprob": -0.138136625289917, "compression_ratio": 1.8076923076923077, "no_speech_prob": 8.664494089316577e-06}, {"id": 798, "seek": 383408, "start": 3839.7999999999997, "end": 3841.7999999999997, "text": " There won't be any", "tokens": [821, 1582, 380, 312, 604], "temperature": 0.0, "avg_logprob": -0.138136625289917, "compression_ratio": 1.8076923076923077, "no_speech_prob": 8.664494089316577e-06}, {"id": 799, "seek": 383408, "start": 3842.0, "end": 3847.36, "text": " Define this or tell me the difference between this word and that word or whatever there's not going to be any rote learning", "tokens": [9548, 533, 341, 420, 980, 385, 264, 2649, 1296, 341, 1349, 293, 300, 1349, 420, 2035, 456, 311, 406, 516, 281, 312, 604, 367, 1370, 2539], "temperature": 0.0, "avg_logprob": -0.138136625289917, "compression_ratio": 1.8076923076923077, "no_speech_prob": 8.664494089316577e-06}, {"id": 800, "seek": 383408, "start": 3847.36, "end": 3852.58, "text": " It'll be entirely like are you an effective machine learning practitioner ie can you use the algorithms?", "tokens": [467, 603, 312, 7696, 411, 366, 291, 364, 4942, 3479, 2539, 32125, 43203, 393, 291, 764, 264, 14642, 30], "temperature": 0.0, "avg_logprob": -0.138136625289917, "compression_ratio": 1.8076923076923077, "no_speech_prob": 8.664494089316577e-06}, {"id": 801, "seek": 383408, "start": 3853.6, "end": 3859.6, "text": " Do you know can you create an effective validation set and can you can you create parts of the algorithm?", "tokens": [1144, 291, 458, 393, 291, 1884, 364, 4942, 24071, 992, 293, 393, 291, 393, 291, 1884, 3166, 295, 264, 9284, 30], "temperature": 0.0, "avg_logprob": -0.138136625289917, "compression_ratio": 1.8076923076923077, "no_speech_prob": 8.664494089316577e-06}, {"id": 802, "seek": 385960, "start": 3859.6, "end": 3863.4, "text": " Implement them from scratch, so it'll be all about writing code", "tokens": [4331, 43704, 552, 490, 8459, 11, 370, 309, 603, 312, 439, 466, 3579, 3089], "temperature": 0.0, "avg_logprob": -0.18103504180908203, "compression_ratio": 1.9060773480662982, "no_speech_prob": 1.0129790098289959e-05}, {"id": 803, "seek": 385960, "start": 3864.36, "end": 3866.04, "text": " basically, so", "tokens": [1936, 11, 370], "temperature": 0.0, "avg_logprob": -0.18103504180908203, "compression_ratio": 1.9060773480662982, "no_speech_prob": 1.0129790098289959e-05}, {"id": 804, "seek": 385960, "start": 3866.04, "end": 3870.52, "text": " if you're not comfortable writing code to practice machine learning then", "tokens": [498, 291, 434, 406, 4619, 3579, 3089, 281, 3124, 3479, 2539, 550], "temperature": 0.0, "avg_logprob": -0.18103504180908203, "compression_ratio": 1.9060773480662982, "no_speech_prob": 1.0129790098289959e-05}, {"id": 805, "seek": 385960, "start": 3871.36, "end": 3876.16, "text": " You should be practicing that all the time if you are comfortable you should be practicing that all the time also", "tokens": [509, 820, 312, 11350, 300, 439, 264, 565, 498, 291, 366, 4619, 291, 820, 312, 11350, 300, 439, 264, 565, 611], "temperature": 0.0, "avg_logprob": -0.18103504180908203, "compression_ratio": 1.9060773480662982, "no_speech_prob": 1.0129790098289959e-05}, {"id": 806, "seek": 385960, "start": 3876.52, "end": 3880.96, "text": " Whatever you're doing write code to implement random to do machine learning", "tokens": [8541, 291, 434, 884, 2464, 3089, 281, 4445, 4974, 281, 360, 3479, 2539], "temperature": 0.0, "avg_logprob": -0.18103504180908203, "compression_ratio": 1.9060773480662982, "no_speech_prob": 1.0129790098289959e-05}, {"id": 807, "seek": 385960, "start": 3884.56, "end": 3886.56, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.18103504180908203, "compression_ratio": 1.9060773480662982, "no_speech_prob": 1.0129790098289959e-05}, {"id": 808, "seek": 388656, "start": 3886.56, "end": 3890.72, "text": " So I I kind of have a particular way of", "tokens": [407, 286, 286, 733, 295, 362, 257, 1729, 636, 295], "temperature": 0.0, "avg_logprob": -0.21659337859792807, "compression_ratio": 1.680327868852459, "no_speech_prob": 3.2887194265640574e-06}, {"id": 809, "seek": 388656, "start": 3892.24, "end": 3893.68, "text": " Writing code", "tokens": [32774, 3089], "temperature": 0.0, "avg_logprob": -0.21659337859792807, "compression_ratio": 1.680327868852459, "no_speech_prob": 3.2887194265640574e-06}, {"id": 810, "seek": 388656, "start": 3893.68, "end": 3894.92, "text": " And I'm not going to claim", "tokens": [400, 286, 478, 406, 516, 281, 3932], "temperature": 0.0, "avg_logprob": -0.21659337859792807, "compression_ratio": 1.680327868852459, "no_speech_prob": 3.2887194265640574e-06}, {"id": 811, "seek": 388656, "start": 3894.92, "end": 3900.2, "text": " It's the only way of writing code, but it might be a little bit different to what you're used to and hopefully you'll find it", "tokens": [467, 311, 264, 787, 636, 295, 3579, 3089, 11, 457, 309, 1062, 312, 257, 707, 857, 819, 281, 437, 291, 434, 1143, 281, 293, 4696, 291, 603, 915, 309], "temperature": 0.0, "avg_logprob": -0.21659337859792807, "compression_ratio": 1.680327868852459, "no_speech_prob": 3.2887194265640574e-06}, {"id": 812, "seek": 388656, "start": 3900.2, "end": 3902.2, "text": " At least interesting", "tokens": [1711, 1935, 1880], "temperature": 0.0, "avg_logprob": -0.21659337859792807, "compression_ratio": 1.680327868852459, "no_speech_prob": 3.2887194265640574e-06}, {"id": 813, "seek": 388656, "start": 3902.32, "end": 3903.52, "text": " creating", "tokens": [4084], "temperature": 0.0, "avg_logprob": -0.21659337859792807, "compression_ratio": 1.680327868852459, "no_speech_prob": 3.2887194265640574e-06}, {"id": 814, "seek": 388656, "start": 3903.52, "end": 3905.52, "text": " implementing random forest algorithms", "tokens": [18114, 4974, 6719, 14642], "temperature": 0.0, "avg_logprob": -0.21659337859792807, "compression_ratio": 1.680327868852459, "no_speech_prob": 3.2887194265640574e-06}, {"id": 815, "seek": 388656, "start": 3905.96, "end": 3910.6, "text": " Is actually quite tricky not because the codes tricky like generally speaking", "tokens": [1119, 767, 1596, 12414, 406, 570, 264, 14211, 12414, 411, 5101, 4124], "temperature": 0.0, "avg_logprob": -0.21659337859792807, "compression_ratio": 1.680327868852459, "no_speech_prob": 3.2887194265640574e-06}, {"id": 816, "seek": 388656, "start": 3912.08, "end": 3915.56, "text": " Most random forest algorithms are pretty conceptually easy", "tokens": [4534, 4974, 6719, 14642, 366, 1238, 3410, 671, 1858], "temperature": 0.0, "avg_logprob": -0.21659337859792807, "compression_ratio": 1.680327868852459, "no_speech_prob": 3.2887194265640574e-06}, {"id": 817, "seek": 391556, "start": 3915.56, "end": 3918.24, "text": " You know that generally speaking", "tokens": [509, 458, 300, 5101, 4124], "temperature": 0.0, "avg_logprob": -0.18620683791789602, "compression_ratio": 1.657258064516129, "no_speech_prob": 1.5446094039361924e-05}, {"id": 818, "seek": 391556, "start": 3919.6, "end": 3926.72, "text": " Academic papers and books have a knack of making them look difficult, but they're not difficult conceptually", "tokens": [36139, 10577, 293, 3642, 362, 257, 444, 501, 295, 1455, 552, 574, 2252, 11, 457, 436, 434, 406, 2252, 3410, 671], "temperature": 0.0, "avg_logprob": -0.18620683791789602, "compression_ratio": 1.657258064516129, "no_speech_prob": 1.5446094039361924e-05}, {"id": 819, "seek": 391556, "start": 3926.72, "end": 3929.56, "text": " what's difficult is getting all the details right and", "tokens": [437, 311, 2252, 307, 1242, 439, 264, 4365, 558, 293], "temperature": 0.0, "avg_logprob": -0.18620683791789602, "compression_ratio": 1.657258064516129, "no_speech_prob": 1.5446094039361924e-05}, {"id": 820, "seek": 391556, "start": 3930.12, "end": 3935.68, "text": " Knowing and knowing when you're right and so in other words we need a good way of doing testing", "tokens": [25499, 293, 5276, 562, 291, 434, 558, 293, 370, 294, 661, 2283, 321, 643, 257, 665, 636, 295, 884, 4997], "temperature": 0.0, "avg_logprob": -0.18620683791789602, "compression_ratio": 1.657258064516129, "no_speech_prob": 1.5446094039361924e-05}, {"id": 821, "seek": 391556, "start": 3936.7999999999997, "end": 3943.34, "text": " So if we're going to re-implement something that already exists so like say we wanted to create a random forest in some", "tokens": [407, 498, 321, 434, 516, 281, 319, 12, 332, 43704, 746, 300, 1217, 8198, 370, 411, 584, 321, 1415, 281, 1884, 257, 4974, 6719, 294, 512], "temperature": 0.0, "avg_logprob": -0.18620683791789602, "compression_ratio": 1.657258064516129, "no_speech_prob": 1.5446094039361924e-05}, {"id": 822, "seek": 394334, "start": 3943.34, "end": 3945.26, "text": " different", "tokens": [819], "temperature": 0.0, "avg_logprob": -0.18525440818385075, "compression_ratio": 1.6833976833976834, "no_speech_prob": 2.123366812156746e-06}, {"id": 823, "seek": 394334, "start": 3945.26, "end": 3951.76, "text": " Framework different language different operating system. You know I would always start with something that does exist right so in this case", "tokens": [31628, 1902, 819, 2856, 819, 7447, 1185, 13, 509, 458, 286, 576, 1009, 722, 365, 746, 300, 775, 2514, 558, 370, 294, 341, 1389], "temperature": 0.0, "avg_logprob": -0.18525440818385075, "compression_ratio": 1.6833976833976834, "no_speech_prob": 2.123366812156746e-06}, {"id": 824, "seek": 394334, "start": 3951.76, "end": 3956.6000000000004, "text": " We're just going to do as a learning exercise writing a random forest in Python so for testing", "tokens": [492, 434, 445, 516, 281, 360, 382, 257, 2539, 5380, 3579, 257, 4974, 6719, 294, 15329, 370, 337, 4997], "temperature": 0.0, "avg_logprob": -0.18525440818385075, "compression_ratio": 1.6833976833976834, "no_speech_prob": 2.123366812156746e-06}, {"id": 825, "seek": 394334, "start": 3956.6000000000004, "end": 3960.1600000000003, "text": " I'm going to compare it to an existing random forest implementation", "tokens": [286, 478, 516, 281, 6794, 309, 281, 364, 6741, 4974, 6719, 11420], "temperature": 0.0, "avg_logprob": -0.18525440818385075, "compression_ratio": 1.6833976833976834, "no_speech_prob": 2.123366812156746e-06}, {"id": 826, "seek": 394334, "start": 3960.7400000000002, "end": 3964.42, "text": " Okay, so that's like critical anytime. You're doing anything", "tokens": [1033, 11, 370, 300, 311, 411, 4924, 13038, 13, 509, 434, 884, 1340], "temperature": 0.0, "avg_logprob": -0.18525440818385075, "compression_ratio": 1.6833976833976834, "no_speech_prob": 2.123366812156746e-06}, {"id": 827, "seek": 394334, "start": 3965.7400000000002, "end": 3969.1000000000004, "text": " Involving like non trivial amounts of code in machine learning", "tokens": [682, 9646, 798, 411, 2107, 26703, 11663, 295, 3089, 294, 3479, 2539], "temperature": 0.0, "avg_logprob": -0.18525440818385075, "compression_ratio": 1.6833976833976834, "no_speech_prob": 2.123366812156746e-06}, {"id": 828, "seek": 396910, "start": 3969.1, "end": 3973.02, "text": " Knowing whether you've got it right or wrong is kind of the hardest bit", "tokens": [25499, 1968, 291, 600, 658, 309, 558, 420, 2085, 307, 733, 295, 264, 13158, 857], "temperature": 0.0, "avg_logprob": -0.13741095727231323, "compression_ratio": 1.8058608058608059, "no_speech_prob": 7.183198249549605e-06}, {"id": 829, "seek": 396910, "start": 3973.02, "end": 3979.74, "text": " I always assume that I've screwed everything up at every step and so I'm thinking like okay assuming that I screwed it up", "tokens": [286, 1009, 6552, 300, 286, 600, 20331, 1203, 493, 412, 633, 1823, 293, 370, 286, 478, 1953, 411, 1392, 11926, 300, 286, 20331, 309, 493], "temperature": 0.0, "avg_logprob": -0.13741095727231323, "compression_ratio": 1.8058608058608059, "no_speech_prob": 7.183198249549605e-06}, {"id": 830, "seek": 396910, "start": 3980.06, "end": 3984.66, "text": " How do I figure out that I screwed it up right and then much to my surprise from time to time?", "tokens": [1012, 360, 286, 2573, 484, 300, 286, 20331, 309, 493, 558, 293, 550, 709, 281, 452, 6365, 490, 565, 281, 565, 30], "temperature": 0.0, "avg_logprob": -0.13741095727231323, "compression_ratio": 1.8058608058608059, "no_speech_prob": 7.183198249549605e-06}, {"id": 831, "seek": 396910, "start": 3984.66, "end": 3990.12, "text": " I actually get something right and then I can move on but most of the time I get it wrong", "tokens": [286, 767, 483, 746, 558, 293, 550, 286, 393, 1286, 322, 457, 881, 295, 264, 565, 286, 483, 309, 2085], "temperature": 0.0, "avg_logprob": -0.13741095727231323, "compression_ratio": 1.8058608058608059, "no_speech_prob": 7.183198249549605e-06}, {"id": 832, "seek": 396910, "start": 3990.7, "end": 3992.46, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.13741095727231323, "compression_ratio": 1.8058608058608059, "no_speech_prob": 7.183198249549605e-06}, {"id": 833, "seek": 396910, "start": 3992.46, "end": 3996.9, "text": " Unfortunately with machine learning there's a lot of ways you can get things wrong that don't give you an error", "tokens": [8590, 365, 3479, 2539, 456, 311, 257, 688, 295, 2098, 291, 393, 483, 721, 2085, 300, 500, 380, 976, 291, 364, 6713], "temperature": 0.0, "avg_logprob": -0.13741095727231323, "compression_ratio": 1.8058608058608059, "no_speech_prob": 7.183198249549605e-06}, {"id": 834, "seek": 399690, "start": 3996.9, "end": 4000.3, "text": " They just make your result like slightly less good", "tokens": [814, 445, 652, 428, 1874, 411, 4748, 1570, 665], "temperature": 0.0, "avg_logprob": -0.1424740730447972, "compression_ratio": 1.75, "no_speech_prob": 3.393131692064344e-06}, {"id": 835, "seek": 399690, "start": 4001.3, "end": 4003.58, "text": " And so that's that's what you want to pick up", "tokens": [400, 370, 300, 311, 300, 311, 437, 291, 528, 281, 1888, 493], "temperature": 0.0, "avg_logprob": -0.1424740730447972, "compression_ratio": 1.75, "no_speech_prob": 3.393131692064344e-06}, {"id": 836, "seek": 399690, "start": 4004.62, "end": 4008.78, "text": " So given that I want to kind of compare it to an existing implementation", "tokens": [407, 2212, 300, 286, 528, 281, 733, 295, 6794, 309, 281, 364, 6741, 11420], "temperature": 0.0, "avg_logprob": -0.1424740730447972, "compression_ratio": 1.75, "no_speech_prob": 3.393131692064344e-06}, {"id": 837, "seek": 399690, "start": 4008.78, "end": 4014.02, "text": " I'm going to use our existing data set our existing validation set and then to simplify things", "tokens": [286, 478, 516, 281, 764, 527, 6741, 1412, 992, 527, 6741, 24071, 992, 293, 550, 281, 20460, 721], "temperature": 0.0, "avg_logprob": -0.1424740730447972, "compression_ratio": 1.75, "no_speech_prob": 3.393131692064344e-06}, {"id": 838, "seek": 399690, "start": 4014.02, "end": 4017.12, "text": " I'm just going to use two columns to start with", "tokens": [286, 478, 445, 516, 281, 764, 732, 13766, 281, 722, 365], "temperature": 0.0, "avg_logprob": -0.1424740730447972, "compression_ratio": 1.75, "no_speech_prob": 3.393131692064344e-06}, {"id": 839, "seek": 399690, "start": 4019.14, "end": 4023.98, "text": " So let's go ahead and start writing a random forest so my way of writing", "tokens": [407, 718, 311, 352, 2286, 293, 722, 3579, 257, 4974, 6719, 370, 452, 636, 295, 3579], "temperature": 0.0, "avg_logprob": -0.1424740730447972, "compression_ratio": 1.75, "no_speech_prob": 3.393131692064344e-06}, {"id": 840, "seek": 402398, "start": 4023.98, "end": 4030.38, "text": " Nearly all code is top-down just like my teaching and so by top-down I", "tokens": [38000, 439, 3089, 307, 1192, 12, 5093, 445, 411, 452, 4571, 293, 370, 538, 1192, 12, 5093, 286], "temperature": 0.0, "avg_logprob": -0.21471117749626253, "compression_ratio": 1.5765306122448979, "no_speech_prob": 4.495156645134557e-06}, {"id": 841, "seek": 402398, "start": 4030.78, "end": 4035.66, "text": " Start by assuming that everything I want already exists", "tokens": [6481, 538, 11926, 300, 1203, 286, 528, 1217, 8198], "temperature": 0.0, "avg_logprob": -0.21471117749626253, "compression_ratio": 1.5765306122448979, "no_speech_prob": 4.495156645134557e-06}, {"id": 842, "seek": 402398, "start": 4036.38, "end": 4040.7400000000002, "text": " Right so in other words the first thing I want to do I'm going to call this a tree ensemble", "tokens": [1779, 370, 294, 661, 2283, 264, 700, 551, 286, 528, 281, 360, 286, 478, 516, 281, 818, 341, 257, 4230, 19492], "temperature": 0.0, "avg_logprob": -0.21471117749626253, "compression_ratio": 1.5765306122448979, "no_speech_prob": 4.495156645134557e-06}, {"id": 843, "seek": 402398, "start": 4041.3, "end": 4045.78, "text": " Right so to create a random forest the first question I have is", "tokens": [1779, 370, 281, 1884, 257, 4974, 6719, 264, 700, 1168, 286, 362, 307], "temperature": 0.0, "avg_logprob": -0.21471117749626253, "compression_ratio": 1.5765306122448979, "no_speech_prob": 4.495156645134557e-06}, {"id": 844, "seek": 402398, "start": 4047.18, "end": 4049.18, "text": " What do I need to pass in?", "tokens": [708, 360, 286, 643, 281, 1320, 294, 30], "temperature": 0.0, "avg_logprob": -0.21471117749626253, "compression_ratio": 1.5765306122448979, "no_speech_prob": 4.495156645134557e-06}, {"id": 845, "seek": 404918, "start": 4049.18, "end": 4055.5, "text": " Right what do I need to initialize my random first so I'm going to need some independent variables", "tokens": [1779, 437, 360, 286, 643, 281, 5883, 1125, 452, 4974, 700, 370, 286, 478, 516, 281, 643, 512, 6695, 9102], "temperature": 0.0, "avg_logprob": -0.1333590848946277, "compression_ratio": 1.6616161616161615, "no_speech_prob": 1.4823535821051337e-06}, {"id": 846, "seek": 404918, "start": 4056.2999999999997, "end": 4058.2999999999997, "text": " some dependent variable", "tokens": [512, 12334, 7006], "temperature": 0.0, "avg_logprob": -0.1333590848946277, "compression_ratio": 1.6616161616161615, "no_speech_prob": 1.4823535821051337e-06}, {"id": 847, "seek": 404918, "start": 4058.54, "end": 4060.54, "text": " Pick how many trees I want?", "tokens": [14129, 577, 867, 5852, 286, 528, 30], "temperature": 0.0, "avg_logprob": -0.1333590848946277, "compression_ratio": 1.6616161616161615, "no_speech_prob": 1.4823535821051337e-06}, {"id": 848, "seek": 404918, "start": 4060.8999999999996, "end": 4063.8599999999997, "text": " I'm going to use the sample size parameter from the start here", "tokens": [286, 478, 516, 281, 764, 264, 6889, 2744, 13075, 490, 264, 722, 510], "temperature": 0.0, "avg_logprob": -0.1333590848946277, "compression_ratio": 1.6616161616161615, "no_speech_prob": 1.4823535821051337e-06}, {"id": 849, "seek": 404918, "start": 4063.8599999999997, "end": 4070.46, "text": " So how big you want each sample to be and then maybe some optional parameter of what's the smallest leaf size?", "tokens": [407, 577, 955, 291, 528, 1184, 6889, 281, 312, 293, 550, 1310, 512, 17312, 13075, 295, 437, 311, 264, 16998, 10871, 2744, 30], "temperature": 0.0, "avg_logprob": -0.1333590848946277, "compression_ratio": 1.6616161616161615, "no_speech_prob": 1.4823535821051337e-06}, {"id": 850, "seek": 404918, "start": 4071.2599999999998, "end": 4073.2599999999998, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.1333590848946277, "compression_ratio": 1.6616161616161615, "no_speech_prob": 1.4823535821051337e-06}, {"id": 851, "seek": 407326, "start": 4073.26, "end": 4079.3, "text": " For testing it's nice to use a constant random seed so we'll get the same result each time", "tokens": [1171, 4997, 309, 311, 1481, 281, 764, 257, 5754, 4974, 8871, 370, 321, 603, 483, 264, 912, 1874, 1184, 565], "temperature": 0.0, "avg_logprob": -0.15867154316235615, "compression_ratio": 1.6048387096774193, "no_speech_prob": 4.710884695668938e-06}, {"id": 852, "seek": 407326, "start": 4079.3, "end": 4082.38, "text": " So this is just how you set a random seed, okay?", "tokens": [407, 341, 307, 445, 577, 291, 992, 257, 4974, 8871, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.15867154316235615, "compression_ratio": 1.6048387096774193, "no_speech_prob": 4.710884695668938e-06}, {"id": 853, "seek": 407326, "start": 4083.3, "end": 4085.94, "text": " Maybe it's worth mentioning this for those who aren't familiar with it", "tokens": [2704, 309, 311, 3163, 18315, 341, 337, 729, 567, 3212, 380, 4963, 365, 309], "temperature": 0.0, "avg_logprob": -0.15867154316235615, "compression_ratio": 1.6048387096774193, "no_speech_prob": 4.710884695668938e-06}, {"id": 854, "seek": 407326, "start": 4086.7400000000002, "end": 4092.7400000000002, "text": " Random number generators on computers aren't random at all. They're actually called pseudo random number generators", "tokens": [37603, 1230, 38662, 322, 10807, 3212, 380, 4974, 412, 439, 13, 814, 434, 767, 1219, 35899, 4974, 1230, 38662], "temperature": 0.0, "avg_logprob": -0.15867154316235615, "compression_ratio": 1.6048387096774193, "no_speech_prob": 4.710884695668938e-06}, {"id": 855, "seek": 407326, "start": 4093.26, "end": 4098.780000000001, "text": " And what they do is given some initial starting point in this case 42 a", "tokens": [400, 437, 436, 360, 307, 2212, 512, 5883, 2891, 935, 294, 341, 1389, 14034, 257], "temperature": 0.0, "avg_logprob": -0.15867154316235615, "compression_ratio": 1.6048387096774193, "no_speech_prob": 4.710884695668938e-06}, {"id": 856, "seek": 409878, "start": 4098.78, "end": 4103.62, "text": " pseudo random number generator is a mathematical function that generates a", "tokens": [35899, 4974, 1230, 19265, 307, 257, 18894, 2445, 300, 23815, 257], "temperature": 0.0, "avg_logprob": -0.19309345292456356, "compression_ratio": 1.7899543378995433, "no_speech_prob": 5.368739266486955e-07}, {"id": 857, "seek": 409878, "start": 4104.179999999999, "end": 4110.0599999999995, "text": " deterministic always the same sequence of numbers such that those numbers are designed to be as", "tokens": [15957, 3142, 1009, 264, 912, 8310, 295, 3547, 1270, 300, 729, 3547, 366, 4761, 281, 312, 382], "temperature": 0.0, "avg_logprob": -0.19309345292456356, "compression_ratio": 1.7899543378995433, "no_speech_prob": 5.368739266486955e-07}, {"id": 858, "seek": 409878, "start": 4110.62, "end": 4112.78, "text": " Uncorrelated with the previous number as possible", "tokens": [1156, 19558, 12004, 365, 264, 3894, 1230, 382, 1944], "temperature": 0.0, "avg_logprob": -0.19309345292456356, "compression_ratio": 1.7899543378995433, "no_speech_prob": 5.368739266486955e-07}, {"id": 859, "seek": 409878, "start": 4113.3, "end": 4117.38, "text": " Okay, and as unpredictable as possible and", "tokens": [1033, 11, 293, 382, 31160, 382, 1944, 293], "temperature": 0.0, "avg_logprob": -0.19309345292456356, "compression_ratio": 1.7899543378995433, "no_speech_prob": 5.368739266486955e-07}, {"id": 860, "seek": 409878, "start": 4118.42, "end": 4126.12, "text": " As uncorrelated as possible with something with a different random seed so the second number in in the sequence starting with 42", "tokens": [1018, 6219, 284, 12004, 382, 1944, 365, 746, 365, 257, 819, 4974, 8871, 370, 264, 1150, 1230, 294, 294, 264, 8310, 2891, 365, 14034], "temperature": 0.0, "avg_logprob": -0.19309345292456356, "compression_ratio": 1.7899543378995433, "no_speech_prob": 5.368739266486955e-07}, {"id": 861, "seek": 412612, "start": 4126.12, "end": 4129.16, "text": " should be very different to the second number starting with 41 and", "tokens": [820, 312, 588, 819, 281, 264, 1150, 1230, 2891, 365, 18173, 293], "temperature": 0.0, "avg_logprob": -0.1821020245552063, "compression_ratio": 1.7171314741035857, "no_speech_prob": 1.733044541651907e-06}, {"id": 862, "seek": 412612, "start": 4129.76, "end": 4132.32, "text": " Generally they involve kind of like taking you know", "tokens": [21082, 436, 9494, 733, 295, 411, 1940, 291, 458], "temperature": 0.0, "avg_logprob": -0.1821020245552063, "compression_ratio": 1.7171314741035857, "no_speech_prob": 1.733044541651907e-06}, {"id": 863, "seek": 412612, "start": 4134.8, "end": 4141.64, "text": " You know using big prime numbers and taking mods and stuff like that. It's kind of an interesting area of math", "tokens": [509, 458, 1228, 955, 5835, 3547, 293, 1940, 30899, 293, 1507, 411, 300, 13, 467, 311, 733, 295, 364, 1880, 1859, 295, 5221], "temperature": 0.0, "avg_logprob": -0.1821020245552063, "compression_ratio": 1.7171314741035857, "no_speech_prob": 1.733044541651907e-06}, {"id": 864, "seek": 412612, "start": 4143.44, "end": 4146.92, "text": " If you want real random numbers the only way to do that is again", "tokens": [759, 291, 528, 957, 4974, 3547, 264, 787, 636, 281, 360, 300, 307, 797], "temperature": 0.0, "avg_logprob": -0.1821020245552063, "compression_ratio": 1.7171314741035857, "no_speech_prob": 1.733044541651907e-06}, {"id": 865, "seek": 412612, "start": 4146.92, "end": 4148.5199999999995, "text": " You can actually buy", "tokens": [509, 393, 767, 2256], "temperature": 0.0, "avg_logprob": -0.1821020245552063, "compression_ratio": 1.7171314741035857, "no_speech_prob": 1.733044541651907e-06}, {"id": 866, "seek": 412612, "start": 4148.5199999999995, "end": 4153.92, "text": " Hardware called a hardware random number generator that will have inside them like a little bit of some radioactive", "tokens": [11817, 3039, 1219, 257, 8837, 4974, 1230, 19265, 300, 486, 362, 1854, 552, 411, 257, 707, 857, 295, 512, 35844], "temperature": 0.0, "avg_logprob": -0.1821020245552063, "compression_ratio": 1.7171314741035857, "no_speech_prob": 1.733044541651907e-06}, {"id": 867, "seek": 415392, "start": 4153.92, "end": 4161.24, "text": " Reactive substance and and like something that detects how many things it's spitting out or you know, there'll be some hardware thing", "tokens": [1300, 12596, 12961, 293, 293, 411, 746, 300, 5531, 82, 577, 867, 721, 309, 311, 637, 2414, 484, 420, 291, 458, 11, 456, 603, 312, 512, 8837, 551], "temperature": 0.0, "avg_logprob": -0.23464370355373476, "compression_ratio": 1.6028708133971292, "no_speech_prob": 3.28872079080611e-06}, {"id": 868, "seek": 415392, "start": 4165.64, "end": 4167.4, "text": " Getting current", "tokens": [13674, 2190], "temperature": 0.0, "avg_logprob": -0.23464370355373476, "compression_ratio": 1.6028708133971292, "no_speech_prob": 3.28872079080611e-06}, {"id": 869, "seek": 415392, "start": 4167.4, "end": 4170.32, "text": " System time is is it a valid?", "tokens": [8910, 565, 307, 307, 309, 257, 7363, 30], "temperature": 0.0, "avg_logprob": -0.23464370355373476, "compression_ratio": 1.6028708133971292, "no_speech_prob": 3.28872079080611e-06}, {"id": 870, "seek": 415392, "start": 4171.0, "end": 4177.08, "text": " Random like random number generation process so that would be for maybe for a random seed, right?", "tokens": [37603, 411, 4974, 1230, 5125, 1399, 370, 300, 576, 312, 337, 1310, 337, 257, 4974, 8871, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.23464370355373476, "compression_ratio": 1.6028708133971292, "no_speech_prob": 3.28872079080611e-06}, {"id": 871, "seek": 415392, "start": 4177.08, "end": 4179.36, "text": " So this thing is like what do we start the function with?", "tokens": [407, 341, 551, 307, 411, 437, 360, 321, 722, 264, 2445, 365, 30], "temperature": 0.0, "avg_logprob": -0.23464370355373476, "compression_ratio": 1.6028708133971292, "no_speech_prob": 3.28872079080611e-06}, {"id": 872, "seek": 417936, "start": 4179.36, "end": 4186.339999999999, "text": " so one of the really interesting areas is like in your computer if you don't set the random seed what is it set to and", "tokens": [370, 472, 295, 264, 534, 1880, 3179, 307, 411, 294, 428, 3820, 498, 291, 500, 380, 992, 264, 4974, 8871, 437, 307, 309, 992, 281, 293], "temperature": 0.0, "avg_logprob": -0.16208067628526196, "compression_ratio": 1.6962025316455696, "no_speech_prob": 4.356848421593895e-06}, {"id": 873, "seek": 417936, "start": 4188.2, "end": 4194.32, "text": " Yeah, quite often people use the current time for security like obviously", "tokens": [865, 11, 1596, 2049, 561, 764, 264, 2190, 565, 337, 3825, 411, 2745], "temperature": 0.0, "avg_logprob": -0.16208067628526196, "compression_ratio": 1.6962025316455696, "no_speech_prob": 4.356848421593895e-06}, {"id": 874, "seek": 417936, "start": 4194.32, "end": 4196.5199999999995, "text": " We use a lot of random number stuff for security stuff", "tokens": [492, 764, 257, 688, 295, 4974, 1230, 1507, 337, 3825, 1507], "temperature": 0.0, "avg_logprob": -0.16208067628526196, "compression_ratio": 1.6962025316455696, "no_speech_prob": 4.356848421593895e-06}, {"id": 875, "seek": 417936, "start": 4196.5199999999995, "end": 4200.599999999999, "text": " Like if you're generating an SSH key, you need some it needs to be random", "tokens": [1743, 498, 291, 434, 17746, 364, 12238, 39, 2141, 11, 291, 643, 512, 309, 2203, 281, 312, 4974], "temperature": 0.0, "avg_logprob": -0.16208067628526196, "compression_ratio": 1.6962025316455696, "no_speech_prob": 4.356848421593895e-06}, {"id": 876, "seek": 417936, "start": 4201.719999999999, "end": 4206.7, "text": " It turns out like, you know people can figure out roughly when you created a key", "tokens": [467, 4523, 484, 411, 11, 291, 458, 561, 393, 2573, 484, 9810, 562, 291, 2942, 257, 2141], "temperature": 0.0, "avg_logprob": -0.16208067628526196, "compression_ratio": 1.6962025316455696, "no_speech_prob": 4.356848421593895e-06}, {"id": 877, "seek": 420670, "start": 4206.7, "end": 4211.82, "text": " Like they could look at like Oh ID RSA has a timestamp and they could try, you know", "tokens": [1743, 436, 727, 574, 412, 411, 876, 7348, 497, 8886, 575, 257, 49108, 1215, 293, 436, 727, 853, 11, 291, 458], "temperature": 0.0, "avg_logprob": -0.19734644456343217, "compression_ratio": 1.7, "no_speech_prob": 1.5779548903083196e-06}, {"id": 878, "seek": 420670, "start": 4211.82, "end": 4217.32, "text": " All the different nanoseconds starting points for a random number generator around that time step and figure out your key", "tokens": [1057, 264, 819, 14067, 541, 28750, 2891, 2793, 337, 257, 4974, 1230, 19265, 926, 300, 565, 1823, 293, 2573, 484, 428, 2141], "temperature": 0.0, "avg_logprob": -0.19734644456343217, "compression_ratio": 1.7, "no_speech_prob": 1.5779548903083196e-06}, {"id": 879, "seek": 420670, "start": 4217.62, "end": 4221.74, "text": " So in practice a lot of like really random", "tokens": [407, 294, 3124, 257, 688, 295, 411, 534, 4974], "temperature": 0.0, "avg_logprob": -0.19734644456343217, "compression_ratio": 1.7, "no_speech_prob": 1.5779548903083196e-06}, {"id": 880, "seek": 420670, "start": 4224.34, "end": 4231.08, "text": " High randomness requiring applications actually have a step that say please move your mouse and type random stuff at the keyboard for a while", "tokens": [5229, 4974, 1287, 24165, 5821, 767, 362, 257, 1823, 300, 584, 1767, 1286, 428, 9719, 293, 2010, 4974, 1507, 412, 264, 10186, 337, 257, 1339], "temperature": 0.0, "avg_logprob": -0.19734644456343217, "compression_ratio": 1.7, "no_speech_prob": 1.5779548903083196e-06}, {"id": 881, "seek": 420670, "start": 4231.08, "end": 4235.34, "text": " And so it like gets you to be a sort of it's called entropy to be a source of entropy", "tokens": [400, 370, 309, 411, 2170, 291, 281, 312, 257, 1333, 295, 309, 311, 1219, 30867, 281, 312, 257, 4009, 295, 30867], "temperature": 0.0, "avg_logprob": -0.19734644456343217, "compression_ratio": 1.7, "no_speech_prob": 1.5779548903083196e-06}, {"id": 882, "seek": 423534, "start": 4235.34, "end": 4238.38, "text": " other approaches is they'll look at like", "tokens": [661, 11587, 307, 436, 603, 574, 412, 411], "temperature": 0.0, "avg_logprob": -0.20187857491629466, "compression_ratio": 1.619607843137255, "no_speech_prob": 6.144136932562105e-06}, {"id": 883, "seek": 423534, "start": 4238.860000000001, "end": 4242.54, "text": " you know the hash of some of your log files or", "tokens": [291, 458, 264, 22019, 295, 512, 295, 428, 3565, 7098, 420], "temperature": 0.0, "avg_logprob": -0.20187857491629466, "compression_ratio": 1.619607843137255, "no_speech_prob": 6.144136932562105e-06}, {"id": 884, "seek": 423534, "start": 4243.14, "end": 4245.14, "text": " You know stuff like that", "tokens": [509, 458, 1507, 411, 300], "temperature": 0.0, "avg_logprob": -0.20187857491629466, "compression_ratio": 1.619607843137255, "no_speech_prob": 6.144136932562105e-06}, {"id": 885, "seek": 423534, "start": 4245.54, "end": 4247.54, "text": " It's a really really fun area", "tokens": [467, 311, 257, 534, 534, 1019, 1859], "temperature": 0.0, "avg_logprob": -0.20187857491629466, "compression_ratio": 1.619607843137255, "no_speech_prob": 6.144136932562105e-06}, {"id": 886, "seek": 423534, "start": 4248.58, "end": 4251.58, "text": " So in our case our purpose actually is to remove randomness", "tokens": [407, 294, 527, 1389, 527, 4334, 767, 307, 281, 4159, 4974, 1287], "temperature": 0.0, "avg_logprob": -0.20187857491629466, "compression_ratio": 1.619607843137255, "no_speech_prob": 6.144136932562105e-06}, {"id": 887, "seek": 423534, "start": 4251.66, "end": 4257.22, "text": " So we're saying okay generate a series of pseudo random numbers starting with 42. So it always should be the same", "tokens": [407, 321, 434, 1566, 1392, 8460, 257, 2638, 295, 35899, 4974, 3547, 2891, 365, 14034, 13, 407, 309, 1009, 820, 312, 264, 912], "temperature": 0.0, "avg_logprob": -0.20187857491629466, "compression_ratio": 1.619607843137255, "no_speech_prob": 6.144136932562105e-06}, {"id": 888, "seek": 425722, "start": 4257.22, "end": 4265.18, "text": " So if you haven't done much stuff in Python, oh, oh, this is a basically standard idiom at least", "tokens": [407, 498, 291, 2378, 380, 1096, 709, 1507, 294, 15329, 11, 1954, 11, 1954, 11, 341, 307, 257, 1936, 3832, 18014, 298, 412, 1935], "temperature": 0.0, "avg_logprob": -0.21666490047349843, "compression_ratio": 1.6735537190082646, "no_speech_prob": 6.854224466223968e-06}, {"id": 889, "seek": 425722, "start": 4265.18, "end": 4269.34, "text": " I mean I write it this way most people don't but if you pass in like", "tokens": [286, 914, 286, 2464, 309, 341, 636, 881, 561, 500, 380, 457, 498, 291, 1320, 294, 411], "temperature": 0.0, "avg_logprob": -0.21666490047349843, "compression_ratio": 1.6735537190082646, "no_speech_prob": 6.854224466223968e-06}, {"id": 890, "seek": 425722, "start": 4269.62, "end": 4273.62, "text": " One two three four five things that you're going to want to keep inside this object", "tokens": [1485, 732, 1045, 1451, 1732, 721, 300, 291, 434, 516, 281, 528, 281, 1066, 1854, 341, 2657], "temperature": 0.0, "avg_logprob": -0.21666490047349843, "compression_ratio": 1.6735537190082646, "no_speech_prob": 6.854224466223968e-06}, {"id": 891, "seek": 425722, "start": 4274.06, "end": 4279.84, "text": " Then you basically have to say self dot X equals X self dot Y equals Y self dot sample equals sample", "tokens": [1396, 291, 1936, 362, 281, 584, 2698, 5893, 1783, 6915, 1783, 2698, 5893, 398, 6915, 398, 2698, 5893, 6889, 6915, 6889], "temperature": 0.0, "avg_logprob": -0.21666490047349843, "compression_ratio": 1.6735537190082646, "no_speech_prob": 6.854224466223968e-06}, {"id": 892, "seek": 425722, "start": 4280.54, "end": 4283.58, "text": " Right and so we can assign to a tuple", "tokens": [1779, 293, 370, 321, 393, 6269, 281, 257, 2604, 781], "temperature": 0.0, "avg_logprob": -0.21666490047349843, "compression_ratio": 1.6735537190082646, "no_speech_prob": 6.854224466223968e-06}, {"id": 893, "seek": 425722, "start": 4284.22, "end": 4286.22, "text": " from a tuple, so", "tokens": [490, 257, 2604, 781, 11, 370], "temperature": 0.0, "avg_logprob": -0.21666490047349843, "compression_ratio": 1.6735537190082646, "no_speech_prob": 6.854224466223968e-06}, {"id": 894, "seek": 428622, "start": 4286.22, "end": 4289.780000000001, "text": " You know again, this is like my way of coding most people think this is horrible", "tokens": [509, 458, 797, 11, 341, 307, 411, 452, 636, 295, 17720, 881, 561, 519, 341, 307, 9263], "temperature": 0.0, "avg_logprob": -0.16977567258088486, "compression_ratio": 1.746031746031746, "no_speech_prob": 1.5056938309498946e-06}, {"id": 895, "seek": 428622, "start": 4289.780000000001, "end": 4294.360000000001, "text": " But I prefer to be able to see everything at once and so I know in my code anytime", "tokens": [583, 286, 4382, 281, 312, 1075, 281, 536, 1203, 412, 1564, 293, 370, 286, 458, 294, 452, 3089, 13038], "temperature": 0.0, "avg_logprob": -0.16977567258088486, "compression_ratio": 1.746031746031746, "no_speech_prob": 1.5056938309498946e-06}, {"id": 896, "seek": 428622, "start": 4294.360000000001, "end": 4296.9800000000005, "text": " I see something looks like this. It's always all of the", "tokens": [286, 536, 746, 1542, 411, 341, 13, 467, 311, 1009, 439, 295, 264], "temperature": 0.0, "avg_logprob": -0.16977567258088486, "compression_ratio": 1.746031746031746, "no_speech_prob": 1.5056938309498946e-06}, {"id": 897, "seek": 428622, "start": 4297.38, "end": 4302.66, "text": " Stuff in the method being set if I did it a different way then half the codes now come off the bottom of the", "tokens": [31347, 294, 264, 3170, 885, 992, 498, 286, 630, 309, 257, 819, 636, 550, 1922, 264, 14211, 586, 808, 766, 264, 2767, 295, 264], "temperature": 0.0, "avg_logprob": -0.16977567258088486, "compression_ratio": 1.746031746031746, "no_speech_prob": 1.5056938309498946e-06}, {"id": 898, "seek": 428622, "start": 4302.900000000001, "end": 4304.900000000001, "text": " Page and you can't see it. So", "tokens": [21217, 293, 291, 393, 380, 536, 309, 13, 407], "temperature": 0.0, "avg_logprob": -0.16977567258088486, "compression_ratio": 1.746031746031746, "no_speech_prob": 1.5056938309498946e-06}, {"id": 899, "seek": 428622, "start": 4305.42, "end": 4307.42, "text": " alright, so", "tokens": [5845, 11, 370], "temperature": 0.0, "avg_logprob": -0.16977567258088486, "compression_ratio": 1.746031746031746, "no_speech_prob": 1.5056938309498946e-06}, {"id": 900, "seek": 428622, "start": 4308.06, "end": 4311.76, "text": " So that was the first thing I thought about was like okay to create a random forest", "tokens": [407, 300, 390, 264, 700, 551, 286, 1194, 466, 390, 411, 1392, 281, 1884, 257, 4974, 6719], "temperature": 0.0, "avg_logprob": -0.16977567258088486, "compression_ratio": 1.746031746031746, "no_speech_prob": 1.5056938309498946e-06}, {"id": 901, "seek": 431176, "start": 4311.76, "end": 4317.66, "text": " What information do you need then I'm going to need to store that information inside my object and so then I?", "tokens": [708, 1589, 360, 291, 643, 550, 286, 478, 516, 281, 643, 281, 3531, 300, 1589, 1854, 452, 2657, 293, 370, 550, 286, 30], "temperature": 0.0, "avg_logprob": -0.19509247633127066, "compression_ratio": 1.7651821862348178, "no_speech_prob": 3.089478468609741e-06}, {"id": 902, "seek": 431176, "start": 4318.26, "end": 4323.38, "text": " Need to create some trees right a random forest is something that creates is something that has some trees", "tokens": [16984, 281, 1884, 512, 5852, 558, 257, 4974, 6719, 307, 746, 300, 7829, 307, 746, 300, 575, 512, 5852], "temperature": 0.0, "avg_logprob": -0.19509247633127066, "compression_ratio": 1.7651821862348178, "no_speech_prob": 3.089478468609741e-06}, {"id": 903, "seek": 431176, "start": 4323.38, "end": 4325.38, "text": " So I basically figured okay", "tokens": [407, 286, 1936, 8932, 1392], "temperature": 0.0, "avg_logprob": -0.19509247633127066, "compression_ratio": 1.7651821862348178, "no_speech_prob": 3.089478468609741e-06}, {"id": 904, "seek": 431176, "start": 4325.66, "end": 4331.42, "text": " List comprehension to create a list of trees. How many trees do we have or we've got n trees trees?", "tokens": [17668, 44991, 281, 1884, 257, 1329, 295, 5852, 13, 1012, 867, 5852, 360, 321, 362, 420, 321, 600, 658, 297, 5852, 5852, 30], "temperature": 0.0, "avg_logprob": -0.19509247633127066, "compression_ratio": 1.7651821862348178, "no_speech_prob": 3.089478468609741e-06}, {"id": 905, "seek": 431176, "start": 4332.54, "end": 4339.400000000001, "text": " That's what we asked for so range n trees gives me the numbers from 0 up to n trees minus 1", "tokens": [663, 311, 437, 321, 2351, 337, 370, 3613, 297, 5852, 2709, 385, 264, 3547, 490, 1958, 493, 281, 297, 5852, 3175, 502], "temperature": 0.0, "avg_logprob": -0.19509247633127066, "compression_ratio": 1.7651821862348178, "no_speech_prob": 3.089478468609741e-06}, {"id": 906, "seek": 433940, "start": 4339.4, "end": 4343.719999999999, "text": " Okay, so if I create a list comprehension that loops through that range", "tokens": [1033, 11, 370, 498, 286, 1884, 257, 1329, 44991, 300, 16121, 807, 300, 3613], "temperature": 0.0, "avg_logprob": -0.14625478064876862, "compression_ratio": 1.6600985221674878, "no_speech_prob": 2.2603164779866347e-06}, {"id": 907, "seek": 433940, "start": 4344.5599999999995, "end": 4349.5599999999995, "text": " Calling create tree each time I now have n trees trees", "tokens": [44150, 1884, 4230, 1184, 565, 286, 586, 362, 297, 5852, 5852], "temperature": 0.0, "avg_logprob": -0.14625478064876862, "compression_ratio": 1.6600985221674878, "no_speech_prob": 2.2603164779866347e-06}, {"id": 908, "seek": 433940, "start": 4350.599999999999, "end": 4356.12, "text": " And now so I had to write that I didn't have to think at all like that's all like", "tokens": [400, 586, 370, 286, 632, 281, 2464, 300, 286, 994, 380, 362, 281, 519, 412, 439, 411, 300, 311, 439, 411], "temperature": 0.0, "avg_logprob": -0.14625478064876862, "compression_ratio": 1.6600985221674878, "no_speech_prob": 2.2603164779866347e-06}, {"id": 909, "seek": 433940, "start": 4357.24, "end": 4362.28, "text": " Obvious and so I've kind of delayed the thinking to the point where it's like well wait", "tokens": [4075, 1502, 293, 370, 286, 600, 733, 295, 20268, 264, 1953, 281, 264, 935, 689, 309, 311, 411, 731, 1699], "temperature": 0.0, "avg_logprob": -0.14625478064876862, "compression_ratio": 1.6600985221674878, "no_speech_prob": 2.2603164779866347e-06}, {"id": 910, "seek": 433940, "start": 4362.759999999999, "end": 4364.759999999999, "text": " We don't have something to create a tree", "tokens": [492, 500, 380, 362, 746, 281, 1884, 257, 4230], "temperature": 0.0, "avg_logprob": -0.14625478064876862, "compression_ratio": 1.6600985221674878, "no_speech_prob": 2.2603164779866347e-06}, {"id": 911, "seek": 436476, "start": 4364.76, "end": 4371.04, "text": " Okay, no worries, but let's pretend we did if we did we've now created a random forest", "tokens": [1033, 11, 572, 16340, 11, 457, 718, 311, 11865, 321, 630, 498, 321, 630, 321, 600, 586, 2942, 257, 4974, 6719], "temperature": 0.0, "avg_logprob": -0.1766426705906534, "compression_ratio": 1.7350746268656716, "no_speech_prob": 3.446565870035556e-06}, {"id": 912, "seek": 436476, "start": 4371.24, "end": 4376.3, "text": " Okay, we still need to like do a few things on top of that for example once we have it", "tokens": [1033, 11, 321, 920, 643, 281, 411, 360, 257, 1326, 721, 322, 1192, 295, 300, 337, 1365, 1564, 321, 362, 309], "temperature": 0.0, "avg_logprob": -0.1766426705906534, "compression_ratio": 1.7350746268656716, "no_speech_prob": 3.446565870035556e-06}, {"id": 913, "seek": 436476, "start": 4376.3, "end": 4383.320000000001, "text": " We would need a predict function so okay. Well. Let's write a predict function. How do you predict in a random forest?", "tokens": [492, 576, 643, 257, 6069, 2445, 370, 1392, 13, 1042, 13, 961, 311, 2464, 257, 6069, 2445, 13, 1012, 360, 291, 6069, 294, 257, 4974, 6719, 30], "temperature": 0.0, "avg_logprob": -0.1766426705906534, "compression_ratio": 1.7350746268656716, "no_speech_prob": 3.446565870035556e-06}, {"id": 914, "seek": 436476, "start": 4385.04, "end": 4387.04, "text": " Can somebody tell me", "tokens": [1664, 2618, 980, 385], "temperature": 0.0, "avg_logprob": -0.1766426705906534, "compression_ratio": 1.7350746268656716, "no_speech_prob": 3.446565870035556e-06}, {"id": 915, "seek": 436476, "start": 4387.16, "end": 4390.34, "text": " Either based on their own understanding or based on this line of code", "tokens": [13746, 2361, 322, 641, 1065, 3701, 420, 2361, 322, 341, 1622, 295, 3089], "temperature": 0.0, "avg_logprob": -0.1766426705906534, "compression_ratio": 1.7350746268656716, "no_speech_prob": 3.446565870035556e-06}, {"id": 916, "seek": 439034, "start": 4390.34, "end": 4395.9800000000005, "text": " What would be like your one or two sentence answer? How do you make a prediction in a random forest?", "tokens": [708, 576, 312, 411, 428, 472, 420, 732, 8174, 1867, 30, 1012, 360, 291, 652, 257, 17630, 294, 257, 4974, 6719, 30], "temperature": 0.0, "avg_logprob": -0.20694687787224264, "compression_ratio": 1.6748971193415638, "no_speech_prob": 1.5056959909998113e-06}, {"id": 917, "seek": 439034, "start": 4397.14, "end": 4399.14, "text": " Spencer", "tokens": [31996], "temperature": 0.0, "avg_logprob": -0.20694687787224264, "compression_ratio": 1.6748971193415638, "no_speech_prob": 1.5056959909998113e-06}, {"id": 918, "seek": 439034, "start": 4400.42, "end": 4406.02, "text": " You would want to over every tree for your like the row that you're trying to predict on", "tokens": [509, 576, 528, 281, 670, 633, 4230, 337, 428, 411, 264, 5386, 300, 291, 434, 1382, 281, 6069, 322], "temperature": 0.0, "avg_logprob": -0.20694687787224264, "compression_ratio": 1.6748971193415638, "no_speech_prob": 1.5056959909998113e-06}, {"id": 919, "seek": 439034, "start": 4406.5, "end": 4410.46, "text": " Average the values that your that each tree would produce for that", "tokens": [316, 3623, 264, 4190, 300, 428, 300, 1184, 4230, 576, 5258, 337, 300], "temperature": 0.0, "avg_logprob": -0.20694687787224264, "compression_ratio": 1.6748971193415638, "no_speech_prob": 1.5056959909998113e-06}, {"id": 920, "seek": 439034, "start": 4410.46, "end": 4416.22, "text": " But it's not like good and so you know that's a summary of what this says right so for a particular row", "tokens": [583, 309, 311, 406, 411, 665, 293, 370, 291, 458, 300, 311, 257, 12691, 295, 437, 341, 1619, 558, 370, 337, 257, 1729, 5386], "temperature": 0.0, "avg_logprob": -0.20694687787224264, "compression_ratio": 1.6748971193415638, "no_speech_prob": 1.5056959909998113e-06}, {"id": 921, "seek": 441622, "start": 4416.22, "end": 4421.38, "text": " I don't maybe this is a number of rows go through each tree", "tokens": [286, 500, 380, 1310, 341, 307, 257, 1230, 295, 13241, 352, 807, 1184, 4230], "temperature": 0.0, "avg_logprob": -0.14644620836395578, "compression_ratio": 1.7022222222222223, "no_speech_prob": 3.041586751351133e-06}, {"id": 922, "seek": 441622, "start": 4423.66, "end": 4430.820000000001, "text": " Calculate is prediction so here is a list comprehension that is calculating the prediction for every tree for X", "tokens": [3511, 2444, 473, 307, 17630, 370, 510, 307, 257, 1329, 44991, 300, 307, 28258, 264, 17630, 337, 633, 4230, 337, 1783], "temperature": 0.0, "avg_logprob": -0.14644620836395578, "compression_ratio": 1.7022222222222223, "no_speech_prob": 3.041586751351133e-06}, {"id": 923, "seek": 441622, "start": 4430.820000000001, "end": 4434.5, "text": " I don't know if X is one row or multiple rows doesn't matter right", "tokens": [286, 500, 380, 458, 498, 1783, 307, 472, 5386, 420, 3866, 13241, 1177, 380, 1871, 558], "temperature": 0.0, "avg_logprob": -0.14644620836395578, "compression_ratio": 1.7022222222222223, "no_speech_prob": 3.041586751351133e-06}, {"id": 924, "seek": 441622, "start": 4435.54, "end": 4439.280000000001, "text": " As long as as long as tree dot predict works on it", "tokens": [1018, 938, 382, 382, 938, 382, 4230, 5893, 6069, 1985, 322, 309], "temperature": 0.0, "avg_logprob": -0.14644620836395578, "compression_ratio": 1.7022222222222223, "no_speech_prob": 3.041586751351133e-06}, {"id": 925, "seek": 443928, "start": 4439.28, "end": 4446.0599999999995, "text": " And then once you've got a list of things a cool trick to know is you can pass numpy dot mean a", "tokens": [400, 550, 1564, 291, 600, 658, 257, 1329, 295, 721, 257, 1627, 4282, 281, 458, 307, 291, 393, 1320, 1031, 8200, 5893, 914, 257], "temperature": 0.0, "avg_logprob": -0.18962630358609286, "compression_ratio": 1.5586854460093897, "no_speech_prob": 1.8738701328402385e-06}, {"id": 926, "seek": 443928, "start": 4447.099999999999, "end": 4449.099999999999, "text": " regular non numpy list", "tokens": [3890, 2107, 1031, 8200, 1329], "temperature": 0.0, "avg_logprob": -0.18962630358609286, "compression_ratio": 1.5586854460093897, "no_speech_prob": 1.8738701328402385e-06}, {"id": 927, "seek": 443928, "start": 4449.3, "end": 4452.74, "text": " Okay, and it'll take the mean you just need to tell it", "tokens": [1033, 11, 293, 309, 603, 747, 264, 914, 291, 445, 643, 281, 980, 309], "temperature": 0.0, "avg_logprob": -0.18962630358609286, "compression_ratio": 1.5586854460093897, "no_speech_prob": 1.8738701328402385e-06}, {"id": 928, "seek": 443928, "start": 4453.38, "end": 4460.54, "text": " Axis equals zero means average it across the lists okay, so this is going to return the average of", "tokens": [20118, 271, 6915, 4018, 1355, 4274, 309, 2108, 264, 14511, 1392, 11, 370, 341, 307, 516, 281, 2736, 264, 4274, 295], "temperature": 0.0, "avg_logprob": -0.18962630358609286, "compression_ratio": 1.5586854460093897, "no_speech_prob": 1.8738701328402385e-06}, {"id": 929, "seek": 443928, "start": 4461.98, "end": 4467.58, "text": " Dot predict for each tree and so I find list comprehensions", "tokens": [38753, 6069, 337, 1184, 4230, 293, 370, 286, 915, 1329, 10753, 8302], "temperature": 0.0, "avg_logprob": -0.18962630358609286, "compression_ratio": 1.5586854460093897, "no_speech_prob": 1.8738701328402385e-06}, {"id": 930, "seek": 446758, "start": 4467.58, "end": 4474.14, "text": " Allow me to write the code in the way that my brain works like you could take the words", "tokens": [32225, 385, 281, 2464, 264, 3089, 294, 264, 636, 300, 452, 3567, 1985, 411, 291, 727, 747, 264, 2283], "temperature": 0.0, "avg_logprob": -0.22498470488048736, "compression_ratio": 1.888412017167382, "no_speech_prob": 3.5559730804379797e-06}, {"id": 931, "seek": 446758, "start": 4474.38, "end": 4480.68, "text": " Spencer said and like translate them into this code or you could take this code and translate them into words like the one Spencer", "tokens": [31996, 848, 293, 411, 13799, 552, 666, 341, 3089, 420, 291, 727, 747, 341, 3089, 293, 13799, 552, 666, 2283, 411, 264, 472, 31996], "temperature": 0.0, "avg_logprob": -0.22498470488048736, "compression_ratio": 1.888412017167382, "no_speech_prob": 3.5559730804379797e-06}, {"id": 932, "seek": 446758, "start": 4480.68, "end": 4485.18, "text": " Said right and so when I write code I want it to be as much like that as possible", "tokens": [26490, 558, 293, 370, 562, 286, 2464, 3089, 286, 528, 309, 281, 312, 382, 709, 411, 300, 382, 1944], "temperature": 0.0, "avg_logprob": -0.22498470488048736, "compression_ratio": 1.888412017167382, "no_speech_prob": 3.5559730804379797e-06}, {"id": 933, "seek": 446758, "start": 4485.62, "end": 4490.94, "text": " I want it to be readable and so hopefully you'll find like when you look at the fast AI code", "tokens": [286, 528, 309, 281, 312, 49857, 293, 370, 4696, 291, 603, 915, 411, 562, 291, 574, 412, 264, 2370, 7318, 3089], "temperature": 0.0, "avg_logprob": -0.22498470488048736, "compression_ratio": 1.888412017167382, "no_speech_prob": 3.5559730804379797e-06}, {"id": 934, "seek": 446758, "start": 4490.94, "end": 4492.9, "text": " You're trying to understand how to Jeremy do X", "tokens": [509, 434, 1382, 281, 1223, 577, 281, 17809, 360, 1783], "temperature": 0.0, "avg_logprob": -0.22498470488048736, "compression_ratio": 1.888412017167382, "no_speech_prob": 3.5559730804379797e-06}, {"id": 935, "seek": 449290, "start": 4492.9, "end": 4498.0199999999995, "text": " I try to write things in a way that you can read it and like it kind of turned into English in your head", "tokens": [286, 853, 281, 2464, 721, 294, 257, 636, 300, 291, 393, 1401, 309, 293, 411, 309, 733, 295, 3574, 666, 3669, 294, 428, 1378], "temperature": 0.0, "avg_logprob": -0.1898987092167498, "compression_ratio": 1.6615384615384616, "no_speech_prob": 3.6688488762592897e-06}, {"id": 936, "seek": 449290, "start": 4501.86, "end": 4506.139999999999, "text": " So if I say correctly that predict method is recursive", "tokens": [407, 498, 286, 584, 8944, 300, 6069, 3170, 307, 20560, 488], "temperature": 0.0, "avg_logprob": -0.1898987092167498, "compression_ratio": 1.6615384615384616, "no_speech_prob": 3.6688488762592897e-06}, {"id": 937, "seek": 449290, "start": 4506.299999999999, "end": 4511.219999999999, "text": " It's no it's calling tree dot predict and we haven't written a tree yet", "tokens": [467, 311, 572, 309, 311, 5141, 4230, 5893, 6069, 293, 321, 2378, 380, 3720, 257, 4230, 1939], "temperature": 0.0, "avg_logprob": -0.1898987092167498, "compression_ratio": 1.6615384615384616, "no_speech_prob": 3.6688488762592897e-06}, {"id": 938, "seek": 449290, "start": 4511.46, "end": 4516.0199999999995, "text": " So self dot trees is going to contain a tree object", "tokens": [407, 2698, 5893, 5852, 307, 516, 281, 5304, 257, 4230, 2657], "temperature": 0.0, "avg_logprob": -0.1898987092167498, "compression_ratio": 1.6615384615384616, "no_speech_prob": 3.6688488762592897e-06}, {"id": 939, "seek": 449290, "start": 4516.5, "end": 4519.299999999999, "text": " so this is tree ensemble dot predict and", "tokens": [370, 341, 307, 4230, 19492, 5893, 6069, 293], "temperature": 0.0, "avg_logprob": -0.1898987092167498, "compression_ratio": 1.6615384615384616, "no_speech_prob": 3.6688488762592897e-06}, {"id": 940, "seek": 451930, "start": 4519.3, "end": 4526.1, "text": " Inside the trees is a tree not a tree ensemble. So this is calling tree dot predict not tree ensemble dot predict", "tokens": [15123, 264, 5852, 307, 257, 4230, 406, 257, 4230, 19492, 13, 407, 341, 307, 5141, 4230, 5893, 6069, 406, 4230, 19492, 5893, 6069], "temperature": 0.0, "avg_logprob": -0.18291362126668295, "compression_ratio": 1.5625, "no_speech_prob": 2.2252734197536483e-06}, {"id": 941, "seek": 451930, "start": 4527.58, "end": 4529.58, "text": " Good question", "tokens": [2205, 1168], "temperature": 0.0, "avg_logprob": -0.18291362126668295, "compression_ratio": 1.5625, "no_speech_prob": 2.2252734197536483e-06}, {"id": 942, "seek": 451930, "start": 4529.78, "end": 4535.9800000000005, "text": " Okay, so we've nearly finished writing our random forest haven't we all we need to do now is write create tree", "tokens": [1033, 11, 370, 321, 600, 6217, 4335, 3579, 527, 4974, 6719, 2378, 380, 321, 439, 321, 643, 281, 360, 586, 307, 2464, 1884, 4230], "temperature": 0.0, "avg_logprob": -0.18291362126668295, "compression_ratio": 1.5625, "no_speech_prob": 2.2252734197536483e-06}, {"id": 943, "seek": 451930, "start": 4536.54, "end": 4538.54, "text": " right, so", "tokens": [558, 11, 370], "temperature": 0.0, "avg_logprob": -0.18291362126668295, "compression_ratio": 1.5625, "no_speech_prob": 2.2252734197536483e-06}, {"id": 944, "seek": 451930, "start": 4539.06, "end": 4540.5, "text": " based on", "tokens": [2361, 322], "temperature": 0.0, "avg_logprob": -0.18291362126668295, "compression_ratio": 1.5625, "no_speech_prob": 2.2252734197536483e-06}, {"id": 945, "seek": 451930, "start": 4540.5, "end": 4542.5, "text": " this code here or", "tokens": [341, 3089, 510, 420], "temperature": 0.0, "avg_logprob": -0.18291362126668295, "compression_ratio": 1.5625, "no_speech_prob": 2.2252734197536483e-06}, {"id": 946, "seek": 454250, "start": 4542.5, "end": 4549.12, "text": " On your own understanding of how we create trees in a random forest. Can somebody tell me?", "tokens": [1282, 428, 1065, 3701, 295, 577, 321, 1884, 5852, 294, 257, 4974, 6719, 13, 1664, 2618, 980, 385, 30], "temperature": 0.0, "avg_logprob": -0.2442656065288343, "compression_ratio": 1.569377990430622, "no_speech_prob": 1.184293159894878e-05}, {"id": 947, "seek": 454250, "start": 4550.38, "end": 4551.98, "text": " Let's take a few seconds", "tokens": [961, 311, 747, 257, 1326, 3949], "temperature": 0.0, "avg_logprob": -0.2442656065288343, "compression_ratio": 1.569377990430622, "no_speech_prob": 1.184293159894878e-05}, {"id": 948, "seek": 454250, "start": 4551.98, "end": 4555.06, "text": " Have a read and have a think and then I'm going to try and come up with a way of saying", "tokens": [3560, 257, 1401, 293, 362, 257, 519, 293, 550, 286, 478, 516, 281, 853, 293, 808, 493, 365, 257, 636, 295, 1566], "temperature": 0.0, "avg_logprob": -0.2442656065288343, "compression_ratio": 1.569377990430622, "no_speech_prob": 1.184293159894878e-05}, {"id": 949, "seek": 454250, "start": 4555.42, "end": 4559.38, "text": " How do you create a tree in a random forest?", "tokens": [1012, 360, 291, 1884, 257, 4230, 294, 257, 4974, 6719, 30], "temperature": 0.0, "avg_logprob": -0.2442656065288343, "compression_ratio": 1.569377990430622, "no_speech_prob": 1.184293159894878e-05}, {"id": 950, "seek": 454250, "start": 4562.34, "end": 4564.34, "text": " Okay, who wants to tell me", "tokens": [1033, 11, 567, 2738, 281, 980, 385], "temperature": 0.0, "avg_logprob": -0.2442656065288343, "compression_ratio": 1.569377990430622, "no_speech_prob": 1.184293159894878e-05}, {"id": 951, "seek": 454250, "start": 4564.58, "end": 4567.34, "text": " Yes, okay. Let's time. It's got closer", "tokens": [1079, 11, 1392, 13, 961, 311, 565, 13, 467, 311, 658, 4966], "temperature": 0.0, "avg_logprob": -0.2442656065288343, "compression_ratio": 1.569377990430622, "no_speech_prob": 1.184293159894878e-05}, {"id": 952, "seek": 456734, "start": 4567.34, "end": 4570.74, "text": " You take your", "tokens": [509, 747, 428], "temperature": 0.0, "avg_logprob": -0.21440164144937093, "compression_ratio": 1.587378640776699, "no_speech_prob": 2.7693929496308556e-06}, {"id": 953, "seek": 456734, "start": 4572.5, "end": 4579.3, "text": " You're essentially taking a random sample or of the original data and then you're just", "tokens": [509, 434, 4476, 1940, 257, 4974, 6889, 420, 295, 264, 3380, 1412, 293, 550, 291, 434, 445], "temperature": 0.0, "avg_logprob": -0.21440164144937093, "compression_ratio": 1.587378640776699, "no_speech_prob": 2.7693929496308556e-06}, {"id": 954, "seek": 456734, "start": 4579.900000000001, "end": 4583.14, "text": " Get just constructing a tree. However, that happens", "tokens": [3240, 445, 39969, 257, 4230, 13, 2908, 11, 300, 2314], "temperature": 0.0, "avg_logprob": -0.21440164144937093, "compression_ratio": 1.587378640776699, "no_speech_prob": 2.7693929496308556e-06}, {"id": 955, "seek": 456734, "start": 4583.3, "end": 4589.54, "text": " So construct a decision tree like a non random tree from a random sample of the data", "tokens": [407, 7690, 257, 3537, 4230, 411, 257, 2107, 4974, 4230, 490, 257, 4974, 6889, 295, 264, 1412], "temperature": 0.0, "avg_logprob": -0.21440164144937093, "compression_ratio": 1.587378640776699, "no_speech_prob": 2.7693929496308556e-06}, {"id": 956, "seek": 456734, "start": 4590.38, "end": 4596.62, "text": " Okay. So again like we've delayed any actual thought process here. We basically said okay", "tokens": [1033, 13, 407, 797, 411, 321, 600, 20268, 604, 3539, 1194, 1399, 510, 13, 492, 1936, 848, 1392], "temperature": 0.0, "avg_logprob": -0.21440164144937093, "compression_ratio": 1.587378640776699, "no_speech_prob": 2.7693929496308556e-06}, {"id": 957, "seek": 459662, "start": 4596.62, "end": 4603.38, "text": " We could pick some random IDs. This is a good trick to know if you call and pay dot random dot permutation", "tokens": [492, 727, 1888, 512, 4974, 48212, 13, 639, 307, 257, 665, 4282, 281, 458, 498, 291, 818, 293, 1689, 5893, 4974, 5893, 4784, 11380], "temperature": 0.0, "avg_logprob": -0.23934161537571957, "compression_ratio": 1.6008583690987124, "no_speech_prob": 1.3709537824979634e-06}, {"id": 958, "seek": 459662, "start": 4604.3, "end": 4607.68, "text": " passing in an int it'll give you back a", "tokens": [8437, 294, 364, 560, 309, 603, 976, 291, 646, 257], "temperature": 0.0, "avg_logprob": -0.23934161537571957, "compression_ratio": 1.6008583690987124, "no_speech_prob": 1.3709537824979634e-06}, {"id": 959, "seek": 459662, "start": 4608.38, "end": 4614.18, "text": " Randomly shuffled sequence from zero to that it right and so then if you grab the first", "tokens": [37603, 356, 402, 33974, 8310, 490, 4018, 281, 300, 309, 558, 293, 370, 550, 498, 291, 4444, 264, 700], "temperature": 0.0, "avg_logprob": -0.23934161537571957, "compression_ratio": 1.6008583690987124, "no_speech_prob": 1.3709537824979634e-06}, {"id": 960, "seek": 459662, "start": 4615.42, "end": 4617.099999999999, "text": " colon and", "tokens": [8255, 293], "temperature": 0.0, "avg_logprob": -0.23934161537571957, "compression_ratio": 1.6008583690987124, "no_speech_prob": 1.3709537824979634e-06}, {"id": 961, "seek": 461710, "start": 4617.1, "end": 4626.780000000001, "text": " Items of that that's now a random sub sample. So this is not doing bootstrapping. We're not doing sampling replacement here", "tokens": [467, 9097, 295, 300, 300, 311, 586, 257, 4974, 1422, 6889, 13, 407, 341, 307, 406, 884, 11450, 19639, 3759, 13, 492, 434, 406, 884, 21179, 14419, 510], "temperature": 0.0, "avg_logprob": -0.16790693051347108, "compression_ratio": 1.6774193548387097, "no_speech_prob": 2.6425802843732527e-06}, {"id": 962, "seek": 461710, "start": 4626.900000000001, "end": 4630.42, "text": " Which I think is fine, you know for my random forest", "tokens": [3013, 286, 519, 307, 2489, 11, 291, 458, 337, 452, 4974, 6719], "temperature": 0.0, "avg_logprob": -0.16790693051347108, "compression_ratio": 1.6774193548387097, "no_speech_prob": 2.6425802843732527e-06}, {"id": 963, "seek": 461710, "start": 4630.42, "end": 4634.700000000001, "text": " I'm deciding that it's going to be something where we do the sub sampling not bootstrapping. Okay", "tokens": [286, 478, 17990, 300, 309, 311, 516, 281, 312, 746, 689, 321, 360, 264, 1422, 21179, 406, 11450, 19639, 3759, 13, 1033], "temperature": 0.0, "avg_logprob": -0.16790693051347108, "compression_ratio": 1.6774193548387097, "no_speech_prob": 2.6425802843732527e-06}, {"id": 964, "seek": 461710, "start": 4634.700000000001, "end": 4638.26, "text": " So here's a good line of code to know how to write", "tokens": [407, 510, 311, 257, 665, 1622, 295, 3089, 281, 458, 577, 281, 2464], "temperature": 0.0, "avg_logprob": -0.16790693051347108, "compression_ratio": 1.6774193548387097, "no_speech_prob": 2.6425802843732527e-06}, {"id": 965, "seek": 461710, "start": 4639.360000000001, "end": 4642.740000000001, "text": " Because it comes up all the time like I find in machine learning", "tokens": [1436, 309, 1487, 493, 439, 264, 565, 411, 286, 915, 294, 3479, 2539], "temperature": 0.0, "avg_logprob": -0.16790693051347108, "compression_ratio": 1.6774193548387097, "no_speech_prob": 2.6425802843732527e-06}, {"id": 966, "seek": 461710, "start": 4643.5, "end": 4645.5, "text": " most algorithms I use are", "tokens": [881, 14642, 286, 764, 366], "temperature": 0.0, "avg_logprob": -0.16790693051347108, "compression_ratio": 1.6774193548387097, "no_speech_prob": 2.6425802843732527e-06}, {"id": 967, "seek": 464550, "start": 4645.5, "end": 4650.74, "text": " Somewhat random and so often I need some kind of random sample. Can you pass that tighter or change?", "tokens": [2188, 5479, 4974, 293, 370, 2049, 286, 643, 512, 733, 295, 4974, 6889, 13, 1664, 291, 1320, 300, 30443, 420, 1319, 30], "temperature": 0.0, "avg_logprob": -0.31617720730333443, "compression_ratio": 1.5364583333333333, "no_speech_prob": 5.014702765038237e-06}, {"id": 968, "seek": 464550, "start": 4655.86, "end": 4661.14, "text": " Won't they give you one one extra because the you said it'll go from zero to length", "tokens": [14710, 380, 436, 976, 291, 472, 472, 2857, 570, 264, 291, 848, 309, 603, 352, 490, 4018, 281, 4641], "temperature": 0.0, "avg_logprob": -0.31617720730333443, "compression_ratio": 1.5364583333333333, "no_speech_prob": 5.014702765038237e-06}, {"id": 969, "seek": 464550, "start": 4663.46, "end": 4667.18, "text": " No, so this will give you if linself.y is", "tokens": [883, 11, 370, 341, 486, 976, 291, 498, 287, 1292, 1967, 13, 88, 307], "temperature": 0.0, "avg_logprob": -0.31617720730333443, "compression_ratio": 1.5364583333333333, "no_speech_prob": 5.014702765038237e-06}, {"id": 970, "seek": 464550, "start": 4668.1, "end": 4674.42, "text": " Size n this will give you n a sequence of length n so 0 to n minus 1", "tokens": [35818, 297, 341, 486, 976, 291, 297, 257, 8310, 295, 4641, 297, 370, 1958, 281, 297, 3175, 502], "temperature": 0.0, "avg_logprob": -0.31617720730333443, "compression_ratio": 1.5364583333333333, "no_speech_prob": 5.014702765038237e-06}, {"id": 971, "seek": 467442, "start": 4674.42, "end": 4677.38, "text": " Okay, and then from that I'm picking out", "tokens": [1033, 11, 293, 550, 490, 300, 286, 478, 8867, 484], "temperature": 0.0, "avg_logprob": -0.25446113673123444, "compression_ratio": 1.588785046728972, "no_speech_prob": 1.5445941244252026e-05}, {"id": 972, "seek": 467442, "start": 4678.3, "end": 4682.66, "text": " Colon self dot sample size. So the first sample size ideas. I", "tokens": [21408, 2698, 5893, 6889, 2744, 13, 407, 264, 700, 6889, 2744, 3487, 13, 286], "temperature": 0.0, "avg_logprob": -0.25446113673123444, "compression_ratio": 1.588785046728972, "no_speech_prob": 1.5445941244252026e-05}, {"id": 973, "seek": 467442, "start": 4686.58, "end": 4694.14, "text": " Have a comment on bootstrapping I think this method is better because we have chance of giving more weights to each", "tokens": [3560, 257, 2871, 322, 11450, 19639, 3759, 286, 519, 341, 3170, 307, 1101, 570, 321, 362, 2931, 295, 2902, 544, 17443, 281, 1184], "temperature": 0.0, "avg_logprob": -0.25446113673123444, "compression_ratio": 1.588785046728972, "no_speech_prob": 1.5445941244252026e-05}, {"id": 974, "seek": 467442, "start": 4695.1, "end": 4701.46, "text": " Observation or am I thinking wrong? No, I mean I think you for bootstrapping. We could also give weights. I mean", "tokens": [20707, 6864, 420, 669, 286, 1953, 2085, 30, 883, 11, 286, 914, 286, 519, 291, 337, 11450, 19639, 3759, 13, 492, 727, 611, 976, 17443, 13, 286, 914], "temperature": 0.0, "avg_logprob": -0.25446113673123444, "compression_ratio": 1.588785046728972, "no_speech_prob": 1.5445941244252026e-05}, {"id": 975, "seek": 467442, "start": 4702.06, "end": 4703.38, "text": " weighing", "tokens": [31986], "temperature": 0.0, "avg_logprob": -0.25446113673123444, "compression_ratio": 1.588785046728972, "no_speech_prob": 1.5445941244252026e-05}, {"id": 976, "seek": 470338, "start": 4703.38, "end": 4705.74, "text": " single observations more than they are like", "tokens": [2167, 18163, 544, 813, 436, 366, 411], "temperature": 0.0, "avg_logprob": -0.2861896164115818, "compression_ratio": 1.648068669527897, "no_speech_prob": 2.7108289941679686e-05}, {"id": 977, "seek": 470338, "start": 4706.74, "end": 4711.62, "text": " without wanting that weight because when bootstrapping with replacement we can have", "tokens": [1553, 7935, 300, 3364, 570, 562, 11450, 19639, 3759, 365, 14419, 321, 393, 362], "temperature": 0.0, "avg_logprob": -0.2861896164115818, "compression_ratio": 1.648068669527897, "no_speech_prob": 2.7108289941679686e-05}, {"id": 978, "seek": 470338, "start": 4712.82, "end": 4719.900000000001, "text": " Single observation and duplicates of it. Yeah the same tree. Yeah, it does feel weird, but I think", "tokens": [31248, 14816, 293, 17154, 1024, 295, 309, 13, 865, 264, 912, 4230, 13, 865, 11, 309, 775, 841, 3657, 11, 457, 286, 519], "temperature": 0.0, "avg_logprob": -0.2861896164115818, "compression_ratio": 1.648068669527897, "no_speech_prob": 2.7108289941679686e-05}, {"id": 979, "seek": 470338, "start": 4722.22, "end": 4724.22, "text": " I'm not sure that the actual", "tokens": [286, 478, 406, 988, 300, 264, 3539], "temperature": 0.0, "avg_logprob": -0.2861896164115818, "compression_ratio": 1.648068669527897, "no_speech_prob": 2.7108289941679686e-05}, {"id": 980, "seek": 470338, "start": 4724.54, "end": 4727.96, "text": " Theory or empirical results backs up higher intuition that it's worse", "tokens": [29009, 420, 31886, 3542, 19513, 493, 2946, 24002, 300, 309, 311, 5324], "temperature": 0.0, "avg_logprob": -0.2861896164115818, "compression_ratio": 1.648068669527897, "no_speech_prob": 2.7108289941679686e-05}, {"id": 981, "seek": 472796, "start": 4727.96, "end": 4732.96, "text": " It would be interesting to look look back at that actually", "tokens": [467, 576, 312, 1880, 281, 574, 574, 646, 412, 300, 767], "temperature": 0.0, "avg_logprob": -0.20344655990600585, "compression_ratio": 1.6126482213438735, "no_speech_prob": 1.1478663509478793e-05}, {"id": 982, "seek": 472796, "start": 4734.92, "end": 4739.64, "text": " Personally I prefer this because I feel like most of the time we have more data than we", "tokens": [21079, 286, 4382, 341, 570, 286, 841, 411, 881, 295, 264, 565, 321, 362, 544, 1412, 813, 321], "temperature": 0.0, "avg_logprob": -0.20344655990600585, "compression_ratio": 1.6126482213438735, "no_speech_prob": 1.1478663509478793e-05}, {"id": 983, "seek": 472796, "start": 4739.96, "end": 4745.18, "text": " Want to put a tree at once I feel like back when Breiman created random forests. It was 1999", "tokens": [11773, 281, 829, 257, 4230, 412, 1564, 286, 841, 411, 646, 562, 7090, 25504, 2942, 4974, 21700, 13, 467, 390, 19952], "temperature": 0.0, "avg_logprob": -0.20344655990600585, "compression_ratio": 1.6126482213438735, "no_speech_prob": 1.1478663509478793e-05}, {"id": 984, "seek": 472796, "start": 4745.18, "end": 4749.82, "text": " It was kind of a very different world, you know where we pretty much always wanted to use all the data we had", "tokens": [467, 390, 733, 295, 257, 588, 819, 1002, 11, 291, 458, 689, 321, 1238, 709, 1009, 1415, 281, 764, 439, 264, 1412, 321, 632], "temperature": 0.0, "avg_logprob": -0.20344655990600585, "compression_ratio": 1.6126482213438735, "no_speech_prob": 1.1478663509478793e-05}, {"id": 985, "seek": 472796, "start": 4750.2, "end": 4752.2, "text": " But nowadays I would say that's", "tokens": [583, 13434, 286, 576, 584, 300, 311], "temperature": 0.0, "avg_logprob": -0.20344655990600585, "compression_ratio": 1.6126482213438735, "no_speech_prob": 1.1478663509478793e-05}, {"id": 986, "seek": 472796, "start": 4752.88, "end": 4754.4800000000005, "text": " Generally not what we want", "tokens": [21082, 406, 437, 321, 528], "temperature": 0.0, "avg_logprob": -0.20344655990600585, "compression_ratio": 1.6126482213438735, "no_speech_prob": 1.1478663509478793e-05}, {"id": 987, "seek": 475448, "start": 4754.48, "end": 4760.599999999999, "text": " We normally have too much data and so what people tend to do is they're like fire up a spark cluster and they'll run it", "tokens": [492, 5646, 362, 886, 709, 1412, 293, 370, 437, 561, 3928, 281, 360, 307, 436, 434, 411, 2610, 493, 257, 9908, 13630, 293, 436, 603, 1190, 309], "temperature": 0.0, "avg_logprob": -0.18040277261649612, "compression_ratio": 1.6703296703296704, "no_speech_prob": 6.854242201370653e-06}, {"id": 988, "seek": 475448, "start": 4760.599999999999, "end": 4762.04, "text": " on hundreds of machines", "tokens": [322, 6779, 295, 8379], "temperature": 0.0, "avg_logprob": -0.18040277261649612, "compression_ratio": 1.6703296703296704, "no_speech_prob": 6.854242201370653e-06}, {"id": 989, "seek": 475448, "start": 4762.04, "end": 4763.32, "text": " when", "tokens": [562], "temperature": 0.0, "avg_logprob": -0.18040277261649612, "compression_ratio": 1.6703296703296704, "no_speech_prob": 6.854242201370653e-06}, {"id": 990, "seek": 475448, "start": 4763.32, "end": 4768.44, "text": " It makes no sense because if they had just used a sub sample each time they could have done it on one machine and like", "tokens": [467, 1669, 572, 2020, 570, 498, 436, 632, 445, 1143, 257, 1422, 6889, 1184, 565, 436, 727, 362, 1096, 309, 322, 472, 3479, 293, 411], "temperature": 0.0, "avg_logprob": -0.18040277261649612, "compression_ratio": 1.6703296703296704, "no_speech_prob": 6.854242201370653e-06}, {"id": 991, "seek": 475448, "start": 4768.5199999999995, "end": 4770.5199999999995, "text": " the the overhead of like", "tokens": [264, 264, 19922, 295, 411], "temperature": 0.0, "avg_logprob": -0.18040277261649612, "compression_ratio": 1.6703296703296704, "no_speech_prob": 6.854242201370653e-06}, {"id": 992, "seek": 475448, "start": 4771.04, "end": 4776.0, "text": " Spark is a huge amount of IO overhead like I know you guys are doing distributed computing now", "tokens": [23424, 307, 257, 2603, 2372, 295, 39839, 19922, 411, 286, 458, 291, 1074, 366, 884, 12631, 15866, 586], "temperature": 0.0, "avg_logprob": -0.18040277261649612, "compression_ratio": 1.6703296703296704, "no_speech_prob": 6.854242201370653e-06}, {"id": 993, "seek": 475448, "start": 4776.0, "end": 4778.48, "text": " If you if you've looked at some of the benchmarks", "tokens": [759, 291, 498, 291, 600, 2956, 412, 512, 295, 264, 43751], "temperature": 0.0, "avg_logprob": -0.18040277261649612, "compression_ratio": 1.6703296703296704, "no_speech_prob": 6.854242201370653e-06}, {"id": 994, "seek": 475448, "start": 4779.679999999999, "end": 4781.4, "text": " Yeah, yeah exactly", "tokens": [865, 11, 1338, 2293], "temperature": 0.0, "avg_logprob": -0.18040277261649612, "compression_ratio": 1.6703296703296704, "no_speech_prob": 6.854242201370653e-06}, {"id": 995, "seek": 478140, "start": 4781.4, "end": 4786.04, "text": " So if you do something on a single machine, it can often be hundreds of times faster", "tokens": [407, 498, 291, 360, 746, 322, 257, 2167, 3479, 11, 309, 393, 2049, 312, 6779, 295, 1413, 4663], "temperature": 0.0, "avg_logprob": -0.21953045592016104, "compression_ratio": 1.6130268199233717, "no_speech_prob": 5.4221654863795266e-06}, {"id": 996, "seek": 478140, "start": 4786.799999999999, "end": 4793.36, "text": " Because you don't have all this this IO overhead and also tends to be easier to write the algorithms like you can use like SK learn", "tokens": [1436, 291, 500, 380, 362, 439, 341, 341, 39839, 19922, 293, 611, 12258, 281, 312, 3571, 281, 2464, 264, 14642, 411, 291, 393, 764, 411, 21483, 1466], "temperature": 0.0, "avg_logprob": -0.21953045592016104, "compression_ratio": 1.6130268199233717, "no_speech_prob": 5.4221654863795266e-06}, {"id": 997, "seek": 478140, "start": 4794.44, "end": 4796.44, "text": " easier to visualize", "tokens": [3571, 281, 23273], "temperature": 0.0, "avg_logprob": -0.21953045592016104, "compression_ratio": 1.6130268199233717, "no_speech_prob": 5.4221654863795266e-06}, {"id": 998, "seek": 478140, "start": 4796.44, "end": 4798.44, "text": " cheaper so forth so like I", "tokens": [12284, 370, 5220, 370, 411, 286], "temperature": 0.0, "avg_logprob": -0.21953045592016104, "compression_ratio": 1.6130268199233717, "no_speech_prob": 5.4221654863795266e-06}, {"id": 999, "seek": 478140, "start": 4799.48, "end": 4806.32, "text": " Almost always avoid distributed computing and I have my whole life like even 25 years ago when I was starting in machine learning", "tokens": [12627, 1009, 5042, 12631, 15866, 293, 286, 362, 452, 1379, 993, 411, 754, 3552, 924, 2057, 562, 286, 390, 2891, 294, 3479, 2539], "temperature": 0.0, "avg_logprob": -0.21953045592016104, "compression_ratio": 1.6130268199233717, "no_speech_prob": 5.4221654863795266e-06}, {"id": 1000, "seek": 478140, "start": 4806.32, "end": 4808.5599999999995, "text": " I you know still didn't use", "tokens": [286, 291, 458, 920, 994, 380, 764], "temperature": 0.0, "avg_logprob": -0.21953045592016104, "compression_ratio": 1.6130268199233717, "no_speech_prob": 5.4221654863795266e-06}, {"id": 1001, "seek": 480856, "start": 4808.56, "end": 4813.64, "text": " These clusters because I so I always feel like whatever I could do with a cluster now", "tokens": [1981, 23313, 570, 286, 370, 286, 1009, 841, 411, 2035, 286, 727, 360, 365, 257, 13630, 586], "temperature": 0.0, "avg_logprob": -0.19691412789481028, "compression_ratio": 1.6736401673640167, "no_speech_prob": 3.844906132144388e-06}, {"id": 1002, "seek": 480856, "start": 4813.64, "end": 4816.0, "text": " I could do with a single machine in five years time", "tokens": [286, 727, 360, 365, 257, 2167, 3479, 294, 1732, 924, 565], "temperature": 0.0, "avg_logprob": -0.19691412789481028, "compression_ratio": 1.6736401673640167, "no_speech_prob": 3.844906132144388e-06}, {"id": 1003, "seek": 480856, "start": 4816.4800000000005, "end": 4820.160000000001, "text": " So why don't us focus on always being as good as possible with the single machine?", "tokens": [407, 983, 500, 380, 505, 1879, 322, 1009, 885, 382, 665, 382, 1944, 365, 264, 2167, 3479, 30], "temperature": 0.0, "avg_logprob": -0.19691412789481028, "compression_ratio": 1.6736401673640167, "no_speech_prob": 3.844906132144388e-06}, {"id": 1004, "seek": 480856, "start": 4820.160000000001, "end": 4824.5, "text": " You know and that's going to be more interactive and more iterative and work for me", "tokens": [509, 458, 293, 300, 311, 516, 281, 312, 544, 15141, 293, 544, 17138, 1166, 293, 589, 337, 385], "temperature": 0.0, "avg_logprob": -0.19691412789481028, "compression_ratio": 1.6736401673640167, "no_speech_prob": 3.844906132144388e-06}, {"id": 1005, "seek": 480856, "start": 4826.72, "end": 4830.360000000001, "text": " Okay, so so again we've like delayed thinking", "tokens": [1033, 11, 370, 370, 797, 321, 600, 411, 20268, 1953], "temperature": 0.0, "avg_logprob": -0.19691412789481028, "compression_ratio": 1.6736401673640167, "no_speech_prob": 3.844906132144388e-06}, {"id": 1006, "seek": 480856, "start": 4831.64, "end": 4833.900000000001, "text": " To the point where we have to write decision tree", "tokens": [1407, 264, 935, 689, 321, 362, 281, 2464, 3537, 4230], "temperature": 0.0, "avg_logprob": -0.19691412789481028, "compression_ratio": 1.6736401673640167, "no_speech_prob": 3.844906132144388e-06}, {"id": 1007, "seek": 483390, "start": 4833.9, "end": 4839.759999999999, "text": " And so hopefully you get an idea that this top-down approach the goal is going to be that we're going to keep delaying thinking", "tokens": [400, 370, 4696, 291, 483, 364, 1558, 300, 341, 1192, 12, 5093, 3109, 264, 3387, 307, 516, 281, 312, 300, 321, 434, 516, 281, 1066, 8577, 278, 1953], "temperature": 0.0, "avg_logprob": -0.13604526984982374, "compression_ratio": 1.7575757575757576, "no_speech_prob": 1.1726368711606483e-06}, {"id": 1008, "seek": 483390, "start": 4839.759999999999, "end": 4846.679999999999, "text": " So long that that we delay it forever like like eventually we've somehow written the whole thing without actually having to think", "tokens": [407, 938, 300, 300, 321, 8577, 309, 5680, 411, 411, 4728, 321, 600, 6063, 3720, 264, 1379, 551, 1553, 767, 1419, 281, 519], "temperature": 0.0, "avg_logprob": -0.13604526984982374, "compression_ratio": 1.7575757575757576, "no_speech_prob": 1.1726368711606483e-06}, {"id": 1009, "seek": 483390, "start": 4846.879999999999, "end": 4851.5199999999995, "text": " Right and that's that's kind of what I need because I'm kind of slow right so this is why I write", "tokens": [1779, 293, 300, 311, 300, 311, 733, 295, 437, 286, 643, 570, 286, 478, 733, 295, 2964, 558, 370, 341, 307, 983, 286, 2464], "temperature": 0.0, "avg_logprob": -0.13604526984982374, "compression_ratio": 1.7575757575757576, "no_speech_prob": 1.1726368711606483e-06}, {"id": 1010, "seek": 483390, "start": 4851.639999999999, "end": 4855.28, "text": " Code this way and notice like you never have to design anything", "tokens": [15549, 341, 636, 293, 3449, 411, 291, 1128, 362, 281, 1715, 1340], "temperature": 0.0, "avg_logprob": -0.13604526984982374, "compression_ratio": 1.7575757575757576, "no_speech_prob": 1.1726368711606483e-06}, {"id": 1011, "seek": 483390, "start": 4856.0, "end": 4860.719999999999, "text": " You know you just say hey, what if somebody already gave me the exact API I needed how would I use it?", "tokens": [509, 458, 291, 445, 584, 4177, 11, 437, 498, 2618, 1217, 2729, 385, 264, 1900, 9362, 286, 2978, 577, 576, 286, 764, 309, 30], "temperature": 0.0, "avg_logprob": -0.13604526984982374, "compression_ratio": 1.7575757575757576, "no_speech_prob": 1.1726368711606483e-06}, {"id": 1012, "seek": 486072, "start": 4860.72, "end": 4864.72, "text": " Okay, and then and then okay to implement that next stage", "tokens": [1033, 11, 293, 550, 293, 550, 1392, 281, 4445, 300, 958, 3233], "temperature": 0.0, "avg_logprob": -0.14310926657456619, "compression_ratio": 1.786610878661088, "no_speech_prob": 2.3320594664255623e-06}, {"id": 1013, "seek": 486072, "start": 4865.400000000001, "end": 4871.88, "text": " What would be the exact API I would need to implement that you keep going down until eventually you're like oh that already exists", "tokens": [708, 576, 312, 264, 1900, 9362, 286, 576, 643, 281, 4445, 300, 291, 1066, 516, 760, 1826, 4728, 291, 434, 411, 1954, 300, 1217, 8198], "temperature": 0.0, "avg_logprob": -0.14310926657456619, "compression_ratio": 1.786610878661088, "no_speech_prob": 2.3320594664255623e-06}, {"id": 1014, "seek": 486072, "start": 4872.400000000001, "end": 4874.12, "text": " Okay, so", "tokens": [1033, 11, 370], "temperature": 0.0, "avg_logprob": -0.14310926657456619, "compression_ratio": 1.786610878661088, "no_speech_prob": 2.3320594664255623e-06}, {"id": 1015, "seek": 486072, "start": 4874.12, "end": 4878.4400000000005, "text": " This assumes we've got a class for decision tree, so we're going to have to create that", "tokens": [639, 37808, 321, 600, 658, 257, 1508, 337, 3537, 4230, 11, 370, 321, 434, 516, 281, 362, 281, 1884, 300], "temperature": 0.0, "avg_logprob": -0.14310926657456619, "compression_ratio": 1.786610878661088, "no_speech_prob": 2.3320594664255623e-06}, {"id": 1016, "seek": 486072, "start": 4880.72, "end": 4882.72, "text": " So a decision tree", "tokens": [407, 257, 3537, 4230], "temperature": 0.0, "avg_logprob": -0.14310926657456619, "compression_ratio": 1.786610878661088, "no_speech_prob": 2.3320594664255623e-06}, {"id": 1017, "seek": 488272, "start": 4882.72, "end": 4889.88, "text": " Is something so we already know what we're going to have to pass it because we just passed it right so we're passing in a", "tokens": [1119, 746, 370, 321, 1217, 458, 437, 321, 434, 516, 281, 362, 281, 1320, 309, 570, 321, 445, 4678, 309, 558, 370, 321, 434, 8437, 294, 257], "temperature": 0.0, "avg_logprob": -0.17121803897550736, "compression_ratio": 1.809278350515464, "no_speech_prob": 9.570808288117405e-07}, {"id": 1018, "seek": 488272, "start": 4890.96, "end": 4892.96, "text": " random sample of X's a", "tokens": [4974, 6889, 295, 1783, 311, 257], "temperature": 0.0, "avg_logprob": -0.17121803897550736, "compression_ratio": 1.809278350515464, "no_speech_prob": 9.570808288117405e-07}, {"id": 1019, "seek": 488272, "start": 4893.360000000001, "end": 4895.360000000001, "text": " random sample of Y's", "tokens": [4974, 6889, 295, 398, 311], "temperature": 0.0, "avg_logprob": -0.17121803897550736, "compression_ratio": 1.809278350515464, "no_speech_prob": 9.570808288117405e-07}, {"id": 1020, "seek": 488272, "start": 4901.2, "end": 4906.76, "text": " Indexes is actually so we know that down the track so I've got to plan a tiny bit", "tokens": [33552, 279, 307, 767, 370, 321, 458, 300, 760, 264, 2837, 370, 286, 600, 658, 281, 1393, 257, 5870, 857], "temperature": 0.0, "avg_logprob": -0.17121803897550736, "compression_ratio": 1.809278350515464, "no_speech_prob": 9.570808288117405e-07}, {"id": 1021, "seek": 488272, "start": 4906.76, "end": 4912.64, "text": " We know that a decision tree is going to contain decision trees which themselves contain decision trees", "tokens": [492, 458, 300, 257, 3537, 4230, 307, 516, 281, 5304, 3537, 5852, 597, 2969, 5304, 3537, 5852], "temperature": 0.0, "avg_logprob": -0.17121803897550736, "compression_ratio": 1.809278350515464, "no_speech_prob": 9.570808288117405e-07}, {"id": 1022, "seek": 491264, "start": 4912.64, "end": 4914.64, "text": " And so as we go down the decision tree", "tokens": [400, 370, 382, 321, 352, 760, 264, 3537, 4230], "temperature": 0.0, "avg_logprob": -0.17883535262641556, "compression_ratio": 1.7841409691629957, "no_speech_prob": 1.228915607498493e-06}, {"id": 1023, "seek": 491264, "start": 4914.64, "end": 4921.12, "text": " There's going to be some subset of the original data that we've kind of got and so I'm going to pass in the indexes of", "tokens": [821, 311, 516, 281, 312, 512, 25993, 295, 264, 3380, 1412, 300, 321, 600, 733, 295, 658, 293, 370, 286, 478, 516, 281, 1320, 294, 264, 8186, 279, 295], "temperature": 0.0, "avg_logprob": -0.17883535262641556, "compression_ratio": 1.7841409691629957, "no_speech_prob": 1.228915607498493e-06}, {"id": 1024, "seek": 491264, "start": 4921.52, "end": 4926.280000000001, "text": " The data that we're actually going to use here, okay, so initially it's the entire", "tokens": [440, 1412, 300, 321, 434, 767, 516, 281, 764, 510, 11, 1392, 11, 370, 9105, 309, 311, 264, 2302], "temperature": 0.0, "avg_logprob": -0.17883535262641556, "compression_ratio": 1.7841409691629957, "no_speech_prob": 1.228915607498493e-06}, {"id": 1025, "seek": 491264, "start": 4926.92, "end": 4929.84, "text": " Random sample right so I've got the whole", "tokens": [37603, 6889, 558, 370, 286, 600, 658, 264, 1379], "temperature": 0.0, "avg_logprob": -0.17883535262641556, "compression_ratio": 1.7841409691629957, "no_speech_prob": 1.228915607498493e-06}, {"id": 1026, "seek": 491264, "start": 4932.0, "end": 4934.0, "text": " I've got the whole range", "tokens": [286, 600, 658, 264, 1379, 3613], "temperature": 0.0, "avg_logprob": -0.17883535262641556, "compression_ratio": 1.7841409691629957, "no_speech_prob": 1.228915607498493e-06}, {"id": 1027, "seek": 491264, "start": 4934.72, "end": 4940.08, "text": " And I turn that into an array so that's zero the index is from zero to the size of the sample and", "tokens": [400, 286, 1261, 300, 666, 364, 10225, 370, 300, 311, 4018, 264, 8186, 307, 490, 4018, 281, 264, 2744, 295, 264, 6889, 293], "temperature": 0.0, "avg_logprob": -0.17883535262641556, "compression_ratio": 1.7841409691629957, "no_speech_prob": 1.228915607498493e-06}, {"id": 1028, "seek": 494008, "start": 4940.08, "end": 4944.32, "text": " Then we're just passed down the min leaf size so everything that we got for", "tokens": [1396, 321, 434, 445, 4678, 760, 264, 923, 10871, 2744, 370, 1203, 300, 321, 658, 337], "temperature": 0.0, "avg_logprob": -0.1890868377685547, "compression_ratio": 1.7113821138211383, "no_speech_prob": 7.224429623420292e-07}, {"id": 1029, "seek": 494008, "start": 4945.16, "end": 4951.8, "text": " Constructing the random forest we're going to pass down the decision tree except of course num trees which is irrelevant for the decision tree", "tokens": [8574, 1757, 278, 264, 4974, 6719, 321, 434, 516, 281, 1320, 760, 264, 3537, 4230, 3993, 295, 1164, 1031, 5852, 597, 307, 28682, 337, 264, 3537, 4230], "temperature": 0.0, "avg_logprob": -0.1890868377685547, "compression_ratio": 1.7113821138211383, "no_speech_prob": 7.224429623420292e-07}, {"id": 1030, "seek": 494008, "start": 4952.8, "end": 4958.6, "text": " So again now that we know that's the information we need we can go ahead and store it inside this object", "tokens": [407, 797, 586, 300, 321, 458, 300, 311, 264, 1589, 321, 643, 321, 393, 352, 2286, 293, 3531, 309, 1854, 341, 2657], "temperature": 0.0, "avg_logprob": -0.1890868377685547, "compression_ratio": 1.7113821138211383, "no_speech_prob": 7.224429623420292e-07}, {"id": 1031, "seek": 494008, "start": 4960.6, "end": 4962.6, "text": " So I'm pretty likely to need to know", "tokens": [407, 286, 478, 1238, 3700, 281, 643, 281, 458], "temperature": 0.0, "avg_logprob": -0.1890868377685547, "compression_ratio": 1.7113821138211383, "no_speech_prob": 7.224429623420292e-07}, {"id": 1032, "seek": 494008, "start": 4963.5599999999995, "end": 4968.08, "text": " How many rows we have in this tree which I generally call n?", "tokens": [1012, 867, 13241, 321, 362, 294, 341, 4230, 597, 286, 5101, 818, 297, 30], "temperature": 0.0, "avg_logprob": -0.1890868377685547, "compression_ratio": 1.7113821138211383, "no_speech_prob": 7.224429623420292e-07}, {"id": 1033, "seek": 496808, "start": 4968.08, "end": 4971.64, "text": " How many columns do I have which I generally call C?", "tokens": [1012, 867, 13766, 360, 286, 362, 597, 286, 5101, 818, 383, 30], "temperature": 0.0, "avg_logprob": -0.1412950243268694, "compression_ratio": 1.6417910447761195, "no_speech_prob": 1.3709550330531783e-06}, {"id": 1034, "seek": 496808, "start": 4971.92, "end": 4975.2, "text": " So the number of rows is just equal to the number of indexes", "tokens": [407, 264, 1230, 295, 13241, 307, 445, 2681, 281, 264, 1230, 295, 8186, 279], "temperature": 0.0, "avg_logprob": -0.1412950243268694, "compression_ratio": 1.6417910447761195, "no_speech_prob": 1.3709550330531783e-06}, {"id": 1035, "seek": 496808, "start": 4975.2, "end": 4981.4, "text": " We are given and the number of columns is just like however many columns there are in our independent variables", "tokens": [492, 366, 2212, 293, 264, 1230, 295, 13766, 307, 445, 411, 4461, 867, 13766, 456, 366, 294, 527, 6695, 9102], "temperature": 0.0, "avg_logprob": -0.1412950243268694, "compression_ratio": 1.6417910447761195, "no_speech_prob": 1.3709550330531783e-06}, {"id": 1036, "seek": 496808, "start": 4984.12, "end": 4986.12, "text": " So then we're going to need", "tokens": [407, 550, 321, 434, 516, 281, 643], "temperature": 0.0, "avg_logprob": -0.1412950243268694, "compression_ratio": 1.6417910447761195, "no_speech_prob": 1.3709550330531783e-06}, {"id": 1037, "seek": 496808, "start": 4987.88, "end": 4989.88, "text": " This value here", "tokens": [639, 2158, 510], "temperature": 0.0, "avg_logprob": -0.1412950243268694, "compression_ratio": 1.6417910447761195, "no_speech_prob": 1.3709550330531783e-06}, {"id": 1038, "seek": 496808, "start": 4991.16, "end": 4993.16, "text": " We need to know for this tree", "tokens": [492, 643, 281, 458, 337, 341, 4230], "temperature": 0.0, "avg_logprob": -0.1412950243268694, "compression_ratio": 1.6417910447761195, "no_speech_prob": 1.3709550330531783e-06}, {"id": 1039, "seek": 499316, "start": 4993.16, "end": 4997.38, "text": " What's its prediction right so", "tokens": [708, 311, 1080, 17630, 558, 370], "temperature": 0.0, "avg_logprob": -0.31038274263080795, "compression_ratio": 1.5912408759124088, "no_speech_prob": 1.7061757944247802e-06}, {"id": 1040, "seek": 499316, "start": 4999.72, "end": 5002.84, "text": " The prediction for this tree is the mean of", "tokens": [440, 17630, 337, 341, 4230, 307, 264, 914, 295], "temperature": 0.0, "avg_logprob": -0.31038274263080795, "compression_ratio": 1.5912408759124088, "no_speech_prob": 1.7061757944247802e-06}, {"id": 1041, "seek": 499316, "start": 5005.0, "end": 5007.0, "text": " Our dependent variable", "tokens": [2621, 12334, 7006], "temperature": 0.0, "avg_logprob": -0.31038274263080795, "compression_ratio": 1.5912408759124088, "no_speech_prob": 1.7061757944247802e-06}, {"id": 1042, "seek": 499316, "start": 5007.04, "end": 5013.26, "text": " Or those indexes which are inside this part of the tree right so at the very top", "tokens": [1610, 729, 8186, 279, 597, 366, 1854, 341, 644, 295, 264, 4230, 558, 370, 412, 264, 588, 1192], "temperature": 0.0, "avg_logprob": -0.31038274263080795, "compression_ratio": 1.5912408759124088, "no_speech_prob": 1.7061757944247802e-06}, {"id": 1043, "seek": 499316, "start": 5013.8, "end": 5016.5199999999995, "text": " Of the tree it contains all the indexes", "tokens": [2720, 264, 4230, 309, 8306, 439, 264, 8186, 279], "temperature": 0.0, "avg_logprob": -0.31038274263080795, "compression_ratio": 1.5912408759124088, "no_speech_prob": 1.7061757944247802e-06}, {"id": 1044, "seek": 501652, "start": 5016.52, "end": 5022.68, "text": " Right I'm assuming that by the time we've got to this point remember. We've already done the", "tokens": [1779, 286, 478, 11926, 300, 538, 264, 565, 321, 600, 658, 281, 341, 935, 1604, 13, 492, 600, 1217, 1096, 264], "temperature": 0.0, "avg_logprob": -0.16753870790654962, "compression_ratio": 1.9211822660098523, "no_speech_prob": 1.3925427992944606e-06}, {"id": 1045, "seek": 501652, "start": 5025.040000000001, "end": 5026.68, "text": " Random sampling", "tokens": [37603, 21179], "temperature": 0.0, "avg_logprob": -0.16753870790654962, "compression_ratio": 1.9211822660098523, "no_speech_prob": 1.3925427992944606e-06}, {"id": 1046, "seek": 501652, "start": 5026.68, "end": 5031.84, "text": " Right so when we're talking about indexes we're not talking about the random sampling to create the tree", "tokens": [1779, 370, 562, 321, 434, 1417, 466, 8186, 279, 321, 434, 406, 1417, 466, 264, 4974, 21179, 281, 1884, 264, 4230], "temperature": 0.0, "avg_logprob": -0.16753870790654962, "compression_ratio": 1.9211822660098523, "no_speech_prob": 1.3925427992944606e-06}, {"id": 1047, "seek": 501652, "start": 5031.92, "end": 5037.0, "text": " We're assuming this tree now has some random sample inside decision tree", "tokens": [492, 434, 11926, 341, 4230, 586, 575, 512, 4974, 6889, 1854, 3537, 4230], "temperature": 0.0, "avg_logprob": -0.16753870790654962, "compression_ratio": 1.9211822660098523, "no_speech_prob": 1.3925427992944606e-06}, {"id": 1048, "seek": 501652, "start": 5037.080000000001, "end": 5042.14, "text": " This is this is the one of the nice things right inside decision tree whole random sampling things gone", "tokens": [639, 307, 341, 307, 264, 472, 295, 264, 1481, 721, 558, 1854, 3537, 4230, 1379, 4974, 21179, 721, 2780], "temperature": 0.0, "avg_logprob": -0.16753870790654962, "compression_ratio": 1.9211822660098523, "no_speech_prob": 1.3925427992944606e-06}, {"id": 1049, "seek": 504214, "start": 5042.14, "end": 5046.4800000000005, "text": " All right that was done by the random first right so at this point. We're building something", "tokens": [1057, 558, 300, 390, 1096, 538, 264, 4974, 700, 558, 370, 412, 341, 935, 13, 492, 434, 2390, 746], "temperature": 0.0, "avg_logprob": -0.15238817329080695, "compression_ratio": 1.9356223175965666, "no_speech_prob": 2.9023040042375214e-06}, {"id": 1050, "seek": 504214, "start": 5046.4800000000005, "end": 5050.780000000001, "text": " That's just a plain old decision tree. It's not in any way a random sampling anything", "tokens": [663, 311, 445, 257, 11121, 1331, 3537, 4230, 13, 467, 311, 406, 294, 604, 636, 257, 4974, 21179, 1340], "temperature": 0.0, "avg_logprob": -0.15238817329080695, "compression_ratio": 1.9356223175965666, "no_speech_prob": 2.9023040042375214e-06}, {"id": 1051, "seek": 504214, "start": 5050.780000000001, "end": 5055.42, "text": " It's just a plain old decision tree right so the indexes is literally like", "tokens": [467, 311, 445, 257, 11121, 1331, 3537, 4230, 558, 370, 264, 8186, 279, 307, 3736, 411], "temperature": 0.0, "avg_logprob": -0.15238817329080695, "compression_ratio": 1.9356223175965666, "no_speech_prob": 2.9023040042375214e-06}, {"id": 1052, "seek": 504214, "start": 5055.9800000000005, "end": 5062.1, "text": " Which subset of the data have we got to so far in this tree and so at the top of the decision tree", "tokens": [3013, 25993, 295, 264, 1412, 362, 321, 658, 281, 370, 1400, 294, 341, 4230, 293, 370, 412, 264, 1192, 295, 264, 3537, 4230], "temperature": 0.0, "avg_logprob": -0.15238817329080695, "compression_ratio": 1.9356223175965666, "no_speech_prob": 2.9023040042375214e-06}, {"id": 1053, "seek": 504214, "start": 5062.1, "end": 5065.4400000000005, "text": " It's all the data right so it's all of the indexes", "tokens": [467, 311, 439, 264, 1412, 558, 370, 309, 311, 439, 295, 264, 8186, 279], "temperature": 0.0, "avg_logprob": -0.15238817329080695, "compression_ratio": 1.9356223175965666, "no_speech_prob": 2.9023040042375214e-06}, {"id": 1054, "seek": 504214, "start": 5066.1, "end": 5068.02, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.15238817329080695, "compression_ratio": 1.9356223175965666, "no_speech_prob": 2.9023040042375214e-06}, {"id": 1055, "seek": 506802, "start": 5068.02, "end": 5075.88, "text": " So all of the indexes so this is therefore all of the dependent variable that are in this part of the tree", "tokens": [407, 439, 295, 264, 8186, 279, 370, 341, 307, 4412, 439, 295, 264, 12334, 7006, 300, 366, 294, 341, 644, 295, 264, 4230], "temperature": 0.0, "avg_logprob": -0.3019257970603116, "compression_ratio": 1.592964824120603, "no_speech_prob": 1.2606664313352667e-05}, {"id": 1056, "seek": 506802, "start": 5075.88, "end": 5078.88, "text": " And so this is the value mean of that", "tokens": [400, 370, 341, 307, 264, 2158, 914, 295, 300], "temperature": 0.0, "avg_logprob": -0.3019257970603116, "compression_ratio": 1.592964824120603, "no_speech_prob": 1.2606664313352667e-05}, {"id": 1057, "seek": 506802, "start": 5080.26, "end": 5083.92, "text": " So make sense anybody could be any questions about about that", "tokens": [407, 652, 2020, 4472, 727, 312, 604, 1651, 466, 466, 300], "temperature": 0.0, "avg_logprob": -0.3019257970603116, "compression_ratio": 1.592964824120603, "no_speech_prob": 1.2606664313352667e-05}, {"id": 1058, "seek": 506802, "start": 5085.46, "end": 5087.72, "text": " So yes, please ask to change she", "tokens": [407, 2086, 11, 1767, 1029, 281, 1319, 750], "temperature": 0.0, "avg_logprob": -0.3019257970603116, "compression_ratio": 1.592964824120603, "no_speech_prob": 1.2606664313352667e-05}, {"id": 1059, "seek": 506802, "start": 5090.860000000001, "end": 5095.9800000000005, "text": " Actually just to let you know that's a large portion of us don't have a OOP I", "tokens": [5135, 445, 281, 718, 291, 458, 300, 311, 257, 2416, 8044, 295, 505, 500, 380, 362, 257, 422, 12059, 286], "temperature": 0.0, "avg_logprob": -0.3019257970603116, "compression_ratio": 1.592964824120603, "no_speech_prob": 1.2606664313352667e-05}, {"id": 1060, "seek": 509598, "start": 5095.98, "end": 5100.459999999999, "text": " Mean all P experiments, okay sure so", "tokens": [12302, 439, 430, 12050, 11, 1392, 988, 370], "temperature": 0.0, "avg_logprob": -0.2695340376633864, "compression_ratio": 1.5804597701149425, "no_speech_prob": 1.2029350727971178e-05}, {"id": 1061, "seek": 509598, "start": 5101.419999999999, "end": 5104.099999999999, "text": " So quick so quick OOP primer would be helpful", "tokens": [407, 1702, 370, 1702, 422, 12059, 12595, 576, 312, 4961], "temperature": 0.0, "avg_logprob": -0.2695340376633864, "compression_ratio": 1.5804597701149425, "no_speech_prob": 1.2029350727971178e-05}, {"id": 1062, "seek": 509598, "start": 5105.179999999999, "end": 5107.179999999999, "text": " Great yeah, okay", "tokens": [3769, 1338, 11, 1392], "temperature": 0.0, "avg_logprob": -0.2695340376633864, "compression_ratio": 1.5804597701149425, "no_speech_prob": 1.2029350727971178e-05}, {"id": 1063, "seek": 509598, "start": 5109.5, "end": 5114.339999999999, "text": " Who has done object-oriented programming in some programming language, okay?", "tokens": [2102, 575, 1096, 2657, 12, 27414, 9410, 294, 512, 9410, 2856, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.2695340376633864, "compression_ratio": 1.5804597701149425, "no_speech_prob": 1.2029350727971178e-05}, {"id": 1064, "seek": 511434, "start": 5114.34, "end": 5125.16, "text": " So you've all used actually lots of object-oriented programming in terms of using existing classes", "tokens": [407, 291, 600, 439, 1143, 767, 3195, 295, 2657, 12, 27414, 9410, 294, 2115, 295, 1228, 6741, 5359], "temperature": 0.0, "avg_logprob": -0.20185853784734553, "compression_ratio": 1.5477707006369428, "no_speech_prob": 3.5008274608117063e-06}, {"id": 1065, "seek": 511434, "start": 5125.54, "end": 5128.900000000001, "text": " Right so every time we've created a random forest", "tokens": [1779, 370, 633, 565, 321, 600, 2942, 257, 4974, 6719], "temperature": 0.0, "avg_logprob": -0.20185853784734553, "compression_ratio": 1.5477707006369428, "no_speech_prob": 3.5008274608117063e-06}, {"id": 1066, "seek": 511434, "start": 5131.3, "end": 5138.78, "text": " We've called the random forests constructor, and it's returned an object and then we've called", "tokens": [492, 600, 1219, 264, 4974, 21700, 47479, 11, 293, 309, 311, 8752, 364, 2657, 293, 550, 321, 600, 1219], "temperature": 0.0, "avg_logprob": -0.20185853784734553, "compression_ratio": 1.5477707006369428, "no_speech_prob": 3.5008274608117063e-06}, {"id": 1067, "seek": 513878, "start": 5138.78, "end": 5148.98, "text": " methods and attributes on that object so fit is a method you can tell because it's got parentheses after it right where else", "tokens": [7150, 293, 17212, 322, 300, 2657, 370, 3318, 307, 257, 3170, 291, 393, 980, 570, 309, 311, 658, 34153, 934, 309, 558, 689, 1646], "temperature": 0.0, "avg_logprob": -0.2245283842086792, "compression_ratio": 1.6834170854271358, "no_speech_prob": 5.255355517874705e-06}, {"id": 1068, "seek": 513878, "start": 5151.66, "end": 5154.34, "text": " Yeah, I will be score is a", "tokens": [865, 11, 286, 486, 312, 6175, 307, 257], "temperature": 0.0, "avg_logprob": -0.2245283842086792, "compression_ratio": 1.6834170854271358, "no_speech_prob": 5.255355517874705e-06}, {"id": 1069, "seek": 513878, "start": 5155.099999999999, "end": 5161.139999999999, "text": " Property or an attribute doesn't have parentheses after it okay, so inside an object", "tokens": [48966, 420, 364, 19667, 1177, 380, 362, 34153, 934, 309, 1392, 11, 370, 1854, 364, 2657], "temperature": 0.0, "avg_logprob": -0.2245283842086792, "compression_ratio": 1.6834170854271358, "no_speech_prob": 5.255355517874705e-06}, {"id": 1070, "seek": 513878, "start": 5161.139999999999, "end": 5164.0, "text": " There are kind of two kinds of things there the functions that you can call", "tokens": [821, 366, 733, 295, 732, 3685, 295, 721, 456, 264, 6828, 300, 291, 393, 818], "temperature": 0.0, "avg_logprob": -0.2245283842086792, "compression_ratio": 1.6834170854271358, "no_speech_prob": 5.255355517874705e-06}, {"id": 1071, "seek": 513878, "start": 5164.86, "end": 5167.5, "text": " So you have object dot", "tokens": [407, 291, 362, 2657, 5893], "temperature": 0.0, "avg_logprob": -0.2245283842086792, "compression_ratio": 1.6834170854271358, "no_speech_prob": 5.255355517874705e-06}, {"id": 1072, "seek": 516750, "start": 5167.5, "end": 5168.86, "text": " function", "tokens": [2445], "temperature": 0.0, "avg_logprob": -0.22650839487711588, "compression_ratio": 1.724770642201835, "no_speech_prob": 8.801029252936132e-06}, {"id": 1073, "seek": 516750, "start": 5168.86, "end": 5173.82, "text": " Parenthesis arguments or there are the properties or attributes you can grab which is", "tokens": [430, 20616, 9374, 12869, 420, 456, 366, 264, 7221, 420, 17212, 291, 393, 4444, 597, 307], "temperature": 0.0, "avg_logprob": -0.22650839487711588, "compression_ratio": 1.724770642201835, "no_speech_prob": 8.801029252936132e-06}, {"id": 1074, "seek": 516750, "start": 5174.58, "end": 5178.32, "text": " Object dot and then just the attribute name with no parentheses", "tokens": [24753, 5893, 293, 550, 445, 264, 19667, 1315, 365, 572, 34153], "temperature": 0.0, "avg_logprob": -0.22650839487711588, "compression_ratio": 1.724770642201835, "no_speech_prob": 8.801029252936132e-06}, {"id": 1075, "seek": 516750, "start": 5178.74, "end": 5184.52, "text": " So when and then the other thing that we do with objects is we create them", "tokens": [407, 562, 293, 550, 264, 661, 551, 300, 321, 360, 365, 6565, 307, 321, 1884, 552], "temperature": 0.0, "avg_logprob": -0.22650839487711588, "compression_ratio": 1.724770642201835, "no_speech_prob": 8.801029252936132e-06}, {"id": 1076, "seek": 516750, "start": 5185.34, "end": 5192.1, "text": " Okay, we pass in the name of the class and it returns us the object and you have to tell it all of the parameters", "tokens": [1033, 11, 321, 1320, 294, 264, 1315, 295, 264, 1508, 293, 309, 11247, 505, 264, 2657, 293, 291, 362, 281, 980, 309, 439, 295, 264, 9834], "temperature": 0.0, "avg_logprob": -0.22650839487711588, "compression_ratio": 1.724770642201835, "no_speech_prob": 8.801029252936132e-06}, {"id": 1077, "seek": 516750, "start": 5192.58, "end": 5193.98, "text": " necessary to", "tokens": [4818, 281], "temperature": 0.0, "avg_logprob": -0.22650839487711588, "compression_ratio": 1.724770642201835, "no_speech_prob": 8.801029252936132e-06}, {"id": 1078, "seek": 516750, "start": 5193.98, "end": 5195.62, "text": " Get constructed", "tokens": [3240, 17083], "temperature": 0.0, "avg_logprob": -0.22650839487711588, "compression_ratio": 1.724770642201835, "no_speech_prob": 8.801029252936132e-06}, {"id": 1079, "seek": 519562, "start": 5195.62, "end": 5198.18, "text": " So let's just copy this code", "tokens": [407, 718, 311, 445, 5055, 341, 3089], "temperature": 0.0, "avg_logprob": -0.2141756258512798, "compression_ratio": 1.6832298136645962, "no_speech_prob": 1.8738663811745937e-06}, {"id": 1080, "seek": 519562, "start": 5203.58, "end": 5205.38, "text": " And", "tokens": [400], "temperature": 0.0, "avg_logprob": -0.2141756258512798, "compression_ratio": 1.6832298136645962, "no_speech_prob": 1.8738663811745937e-06}, {"id": 1081, "seek": 519562, "start": 5205.38, "end": 5207.5, "text": " See how we're going to go ahead and build this", "tokens": [3008, 577, 321, 434, 516, 281, 352, 2286, 293, 1322, 341], "temperature": 0.0, "avg_logprob": -0.2141756258512798, "compression_ratio": 1.6832298136645962, "no_speech_prob": 1.8738663811745937e-06}, {"id": 1082, "seek": 519562, "start": 5208.22, "end": 5212.86, "text": " So the first step is we're not going to go and it was random forest regressor", "tokens": [407, 264, 700, 1823, 307, 321, 434, 406, 516, 281, 352, 293, 309, 390, 4974, 6719, 1121, 735, 284], "temperature": 0.0, "avg_logprob": -0.2141756258512798, "compression_ratio": 1.6832298136645962, "no_speech_prob": 1.8738663811745937e-06}, {"id": 1083, "seek": 519562, "start": 5212.86, "end": 5219.58, "text": " We're going to go M equals tree ensemble. We're creating a class called tree ensemble, and we're going to pass in", "tokens": [492, 434, 516, 281, 352, 376, 6915, 4230, 19492, 13, 492, 434, 4084, 257, 1508, 1219, 4230, 19492, 11, 293, 321, 434, 516, 281, 1320, 294], "temperature": 0.0, "avg_logprob": -0.2141756258512798, "compression_ratio": 1.6832298136645962, "no_speech_prob": 1.8738663811745937e-06}, {"id": 1084, "seek": 521958, "start": 5219.58, "end": 5221.58, "text": " various", "tokens": [3683], "temperature": 0.0, "avg_logprob": -0.16049046114266638, "compression_ratio": 1.5320197044334976, "no_speech_prob": 6.962188308534678e-06}, {"id": 1085, "seek": 521958, "start": 5225.0199999999995, "end": 5227.0199999999995, "text": " Bits of information okay", "tokens": [363, 1208, 295, 1589, 1392], "temperature": 0.0, "avg_logprob": -0.16049046114266638, "compression_ratio": 1.5320197044334976, "no_speech_prob": 6.962188308534678e-06}, {"id": 1086, "seek": 521958, "start": 5228.66, "end": 5230.66, "text": " So maybe we'll have ten trees", "tokens": [407, 1310, 321, 603, 362, 2064, 5852], "temperature": 0.0, "avg_logprob": -0.16049046114266638, "compression_ratio": 1.5320197044334976, "no_speech_prob": 6.962188308534678e-06}, {"id": 1087, "seek": 521958, "start": 5231.82, "end": 5233.82, "text": " sample size of a thousand", "tokens": [6889, 2744, 295, 257, 4714], "temperature": 0.0, "avg_logprob": -0.16049046114266638, "compression_ratio": 1.5320197044334976, "no_speech_prob": 6.962188308534678e-06}, {"id": 1088, "seek": 521958, "start": 5233.94, "end": 5239.0199999999995, "text": " Maybe a min leaf of three right and you can always like choose to name your arguments or not", "tokens": [2704, 257, 923, 10871, 295, 1045, 558, 293, 291, 393, 1009, 411, 2826, 281, 1315, 428, 12869, 420, 406], "temperature": 0.0, "avg_logprob": -0.16049046114266638, "compression_ratio": 1.5320197044334976, "no_speech_prob": 6.962188308534678e-06}, {"id": 1089, "seek": 521958, "start": 5239.0199999999995, "end": 5244.94, "text": " So when you've got quite a few it's kind of nice to name them so that just so we can see what each one means", "tokens": [407, 562, 291, 600, 658, 1596, 257, 1326, 309, 311, 733, 295, 1481, 281, 1315, 552, 370, 300, 445, 370, 321, 393, 536, 437, 1184, 472, 1355], "temperature": 0.0, "avg_logprob": -0.16049046114266638, "compression_ratio": 1.5320197044334976, "no_speech_prob": 6.962188308534678e-06}, {"id": 1090, "seek": 524494, "start": 5244.94, "end": 5248.78, "text": " It's always optional right", "tokens": [467, 311, 1009, 17312, 558], "temperature": 0.0, "avg_logprob": -0.19855132632785374, "compression_ratio": 1.6320754716981132, "no_speech_prob": 3.3931262350961333e-06}, {"id": 1091, "seek": 524494, "start": 5251.099999999999, "end": 5256.78, "text": " So we're going to try and create a class that we can use like this and then", "tokens": [407, 321, 434, 516, 281, 853, 293, 1884, 257, 1508, 300, 321, 393, 764, 411, 341, 293, 550], "temperature": 0.0, "avg_logprob": -0.19855132632785374, "compression_ratio": 1.6320754716981132, "no_speech_prob": 3.3931262350961333e-06}, {"id": 1092, "seek": 524494, "start": 5258.419999999999, "end": 5260.54, "text": " I'm not sure we're going to bother with dot fit", "tokens": [286, 478, 406, 988, 321, 434, 516, 281, 8677, 365, 5893, 3318], "temperature": 0.0, "avg_logprob": -0.19855132632785374, "compression_ratio": 1.6320754716981132, "no_speech_prob": 3.3931262350961333e-06}, {"id": 1093, "seek": 524494, "start": 5261.0199999999995, "end": 5265.139999999999, "text": " Because we've passed in the X and the Y right like in scikit-learn", "tokens": [1436, 321, 600, 4678, 294, 264, 1783, 293, 264, 398, 558, 411, 294, 2180, 22681, 12, 306, 1083], "temperature": 0.0, "avg_logprob": -0.19855132632785374, "compression_ratio": 1.6320754716981132, "no_speech_prob": 3.3931262350961333e-06}, {"id": 1094, "seek": 524494, "start": 5265.46, "end": 5271.339999999999, "text": " They use an approach where first of all you construct something without telling it what data to use and then you pass in the day", "tokens": [814, 764, 364, 3109, 689, 700, 295, 439, 291, 7690, 746, 1553, 3585, 309, 437, 1412, 281, 764, 293, 550, 291, 1320, 294, 264, 786], "temperature": 0.0, "avg_logprob": -0.19855132632785374, "compression_ratio": 1.6320754716981132, "no_speech_prob": 3.3931262350961333e-06}, {"id": 1095, "seek": 527134, "start": 5271.34, "end": 5278.26, "text": " We're doing these two steps at once. We're actually passing in the data right and so then after that we're going to be going and", "tokens": [492, 434, 884, 613, 732, 4439, 412, 1564, 13, 492, 434, 767, 8437, 294, 264, 1412, 558, 293, 370, 550, 934, 300, 321, 434, 516, 281, 312, 516, 293], "temperature": 0.0, "avg_logprob": -0.24149517021556893, "compression_ratio": 1.7543859649122806, "no_speech_prob": 4.4254584281588905e-06}, {"id": 1096, "seek": 527134, "start": 5279.66, "end": 5283.9800000000005, "text": " Dot so we're going to go preds equals M dot predict", "tokens": [38753, 370, 321, 434, 516, 281, 352, 3852, 82, 6915, 376, 5893, 6069], "temperature": 0.0, "avg_logprob": -0.24149517021556893, "compression_ratio": 1.7543859649122806, "no_speech_prob": 4.4254584281588905e-06}, {"id": 1097, "seek": 527134, "start": 5284.82, "end": 5287.02, "text": " Passing in maybe some validation set", "tokens": [10319, 278, 294, 1310, 512, 24071, 992], "temperature": 0.0, "avg_logprob": -0.24149517021556893, "compression_ratio": 1.7543859649122806, "no_speech_prob": 4.4254584281588905e-06}, {"id": 1098, "seek": 527134, "start": 5287.62, "end": 5292.26, "text": " Okay, so we're good. That's that's the API. We're kind of creating here", "tokens": [1033, 11, 370, 321, 434, 665, 13, 663, 311, 300, 311, 264, 9362, 13, 492, 434, 733, 295, 4084, 510], "temperature": 0.0, "avg_logprob": -0.24149517021556893, "compression_ratio": 1.7543859649122806, "no_speech_prob": 4.4254584281588905e-06}, {"id": 1099, "seek": 527134, "start": 5293.22, "end": 5298.26, "text": " So this thing here is called a constructor something that creates an object is called a constructor", "tokens": [407, 341, 551, 510, 307, 1219, 257, 47479, 746, 300, 7829, 364, 2657, 307, 1219, 257, 47479], "temperature": 0.0, "avg_logprob": -0.24149517021556893, "compression_ratio": 1.7543859649122806, "no_speech_prob": 4.4254584281588905e-06}, {"id": 1100, "seek": 529826, "start": 5298.26, "end": 5300.74, "text": " and Python", "tokens": [293, 15329], "temperature": 0.0, "avg_logprob": -0.2651015281677246, "compression_ratio": 1.8021390374331552, "no_speech_prob": 2.3687921384407673e-06}, {"id": 1101, "seek": 529826, "start": 5302.5, "end": 5308.74, "text": " There's a lot of ugly hideous things about Python one of which is they it uses these special magic", "tokens": [821, 311, 257, 688, 295, 12246, 6479, 563, 721, 466, 15329, 472, 295, 597, 307, 436, 309, 4960, 613, 2121, 5585], "temperature": 0.0, "avg_logprob": -0.2651015281677246, "compression_ratio": 1.8021390374331552, "no_speech_prob": 2.3687921384407673e-06}, {"id": 1102, "seek": 529826, "start": 5309.46, "end": 5311.22, "text": " method names", "tokens": [3170, 5288], "temperature": 0.0, "avg_logprob": -0.2651015281677246, "compression_ratio": 1.8021390374331552, "no_speech_prob": 2.3687921384407673e-06}, {"id": 1103, "seek": 529826, "start": 5311.22, "end": 5315.7, "text": " Underscore underscore in it underscore underscore is a special magic method", "tokens": [2719, 433, 12352, 37556, 294, 309, 37556, 37556, 307, 257, 2121, 5585, 3170], "temperature": 0.0, "avg_logprob": -0.2651015281677246, "compression_ratio": 1.8021390374331552, "no_speech_prob": 2.3687921384407673e-06}, {"id": 1104, "seek": 529826, "start": 5315.7, "end": 5321.18, "text": " That's caught what's called when you try to construct a class so when I call tree ensemble", "tokens": [663, 311, 5415, 437, 311, 1219, 562, 291, 853, 281, 7690, 257, 1508, 370, 562, 286, 818, 4230, 19492], "temperature": 0.0, "avg_logprob": -0.2651015281677246, "compression_ratio": 1.8021390374331552, "no_speech_prob": 2.3687921384407673e-06}, {"id": 1105, "seek": 529826, "start": 5321.820000000001, "end": 5325.06, "text": " Parenthesis it actually calls tree ensemble dot", "tokens": [430, 20616, 9374, 309, 767, 5498, 4230, 19492, 5893], "temperature": 0.0, "avg_logprob": -0.2651015281677246, "compression_ratio": 1.8021390374331552, "no_speech_prob": 2.3687921384407673e-06}, {"id": 1106, "seek": 532506, "start": 5325.06, "end": 5332.700000000001, "text": " They see people say Dunder in it. I kind of hate it, but anyway Dunder in it double underscore in it double underscore", "tokens": [814, 536, 561, 584, 413, 6617, 294, 309, 13, 286, 733, 295, 4700, 309, 11, 457, 4033, 413, 6617, 294, 309, 3834, 37556, 294, 309, 3834, 37556], "temperature": 0.0, "avg_logprob": -0.25637467702229816, "compression_ratio": 1.7903930131004366, "no_speech_prob": 2.156805521735805e-06}, {"id": 1107, "seek": 532506, "start": 5333.06, "end": 5335.06, "text": " Dunder in it", "tokens": [413, 6617, 294, 309], "temperature": 0.0, "avg_logprob": -0.25637467702229816, "compression_ratio": 1.7903930131004366, "no_speech_prob": 2.156805521735805e-06}, {"id": 1108, "seek": 532506, "start": 5335.14, "end": 5341.620000000001, "text": " So that's why we've got this method called Dunder in it. Okay, so when I call tree ensemble is going to call this method", "tokens": [407, 300, 311, 983, 321, 600, 658, 341, 3170, 1219, 413, 6617, 294, 309, 13, 1033, 11, 370, 562, 286, 818, 4230, 19492, 307, 516, 281, 818, 341, 3170], "temperature": 0.0, "avg_logprob": -0.25637467702229816, "compression_ratio": 1.7903930131004366, "no_speech_prob": 2.156805521735805e-06}, {"id": 1109, "seek": 532506, "start": 5343.14, "end": 5344.780000000001, "text": " another", "tokens": [1071], "temperature": 0.0, "avg_logprob": -0.25637467702229816, "compression_ratio": 1.7903930131004366, "no_speech_prob": 2.156805521735805e-06}, {"id": 1110, "seek": 532506, "start": 5344.780000000001, "end": 5346.780000000001, "text": " hideously ugly thing about", "tokens": [6479, 5098, 12246, 551, 466], "temperature": 0.0, "avg_logprob": -0.25637467702229816, "compression_ratio": 1.7903930131004366, "no_speech_prob": 2.156805521735805e-06}, {"id": 1111, "seek": 532506, "start": 5347.1, "end": 5354.54, "text": " Python's oh, oh is that there's this special thing where if you have a class and to create a class you just write class in", "tokens": [15329, 311, 1954, 11, 1954, 307, 300, 456, 311, 341, 2121, 551, 689, 498, 291, 362, 257, 1508, 293, 281, 1884, 257, 1508, 291, 445, 2464, 1508, 294], "temperature": 0.0, "avg_logprob": -0.25637467702229816, "compression_ratio": 1.7903930131004366, "no_speech_prob": 2.156805521735805e-06}, {"id": 1112, "seek": 535454, "start": 5354.54, "end": 5356.98, "text": " The name of class all of its methods", "tokens": [440, 1315, 295, 1508, 439, 295, 1080, 7150], "temperature": 0.0, "avg_logprob": -0.1793840445724188, "compression_ratio": 1.7210300429184548, "no_speech_prob": 3.844907951133791e-06}, {"id": 1113, "seek": 535454, "start": 5358.1, "end": 5360.22, "text": " Automatically get sent one extra", "tokens": [24619, 5030, 483, 2279, 472, 2857], "temperature": 0.0, "avg_logprob": -0.1793840445724188, "compression_ratio": 1.7210300429184548, "no_speech_prob": 3.844907951133791e-06}, {"id": 1114, "seek": 535454, "start": 5360.94, "end": 5362.94, "text": " parameter one extra argument", "tokens": [13075, 472, 2857, 6770], "temperature": 0.0, "avg_logprob": -0.1793840445724188, "compression_ratio": 1.7210300429184548, "no_speech_prob": 3.844907951133791e-06}, {"id": 1115, "seek": 535454, "start": 5362.94, "end": 5368.98, "text": " Which is the first argument and you can call it anything you like if you call it anything other than self", "tokens": [3013, 307, 264, 700, 6770, 293, 291, 393, 818, 309, 1340, 291, 411, 498, 291, 818, 309, 1340, 661, 813, 2698], "temperature": 0.0, "avg_logprob": -0.1793840445724188, "compression_ratio": 1.7210300429184548, "no_speech_prob": 3.844907951133791e-06}, {"id": 1116, "seek": 535454, "start": 5369.34, "end": 5371.34, "text": " Everybody will hate you and you're a bad person", "tokens": [7646, 486, 4700, 291, 293, 291, 434, 257, 1578, 954], "temperature": 0.0, "avg_logprob": -0.1793840445724188, "compression_ratio": 1.7210300429184548, "no_speech_prob": 3.844907951133791e-06}, {"id": 1117, "seek": 535454, "start": 5371.98, "end": 5375.18, "text": " Okay, so call it anything you like as long as itself", "tokens": [1033, 11, 370, 818, 309, 1340, 291, 411, 382, 938, 382, 2564], "temperature": 0.0, "avg_logprob": -0.1793840445724188, "compression_ratio": 1.7210300429184548, "no_speech_prob": 3.844907951133791e-06}, {"id": 1118, "seek": 535454, "start": 5376.14, "end": 5378.14, "text": " Okay, so", "tokens": [1033, 11, 370], "temperature": 0.0, "avg_logprob": -0.1793840445724188, "compression_ratio": 1.7210300429184548, "no_speech_prob": 3.844907951133791e-06}, {"id": 1119, "seek": 537814, "start": 5378.14, "end": 5383.58, "text": " So that's why you always see this and in fact I can immediately see here I have a bug", "tokens": [407, 300, 311, 983, 291, 1009, 536, 341, 293, 294, 1186, 286, 393, 4258, 536, 510, 286, 362, 257, 7426], "temperature": 0.0, "avg_logprob": -0.18032565483680138, "compression_ratio": 1.6245210727969348, "no_speech_prob": 6.748006853740662e-06}, {"id": 1120, "seek": 537814, "start": 5384.660000000001, "end": 5388.9400000000005, "text": " Anybody see the bug in my predict function. I should have self right I I", "tokens": [19082, 536, 264, 7426, 294, 452, 6069, 2445, 13, 286, 820, 362, 2698, 558, 286, 286], "temperature": 0.0, "avg_logprob": -0.18032565483680138, "compression_ratio": 1.6245210727969348, "no_speech_prob": 6.748006853740662e-06}, {"id": 1121, "seek": 537814, "start": 5390.660000000001, "end": 5395.3, "text": " Like they always do it right so anytime you try and call a method on your own class", "tokens": [1743, 436, 1009, 360, 309, 558, 370, 13038, 291, 853, 293, 818, 257, 3170, 322, 428, 1065, 1508], "temperature": 0.0, "avg_logprob": -0.18032565483680138, "compression_ratio": 1.6245210727969348, "no_speech_prob": 6.748006853740662e-06}, {"id": 1122, "seek": 537814, "start": 5395.3, "end": 5400.860000000001, "text": " And you get something saying you passed in two parameters, and it was only expecting one you forgot self", "tokens": [400, 291, 483, 746, 1566, 291, 4678, 294, 732, 9834, 11, 293, 309, 390, 787, 9650, 472, 291, 5298, 2698], "temperature": 0.0, "avg_logprob": -0.18032565483680138, "compression_ratio": 1.6245210727969348, "no_speech_prob": 6.748006853740662e-06}, {"id": 1123, "seek": 537814, "start": 5401.34, "end": 5406.5, "text": " Okay, so like this is a really dumb way to add OOP to a programming language", "tokens": [1033, 11, 370, 411, 341, 307, 257, 534, 10316, 636, 281, 909, 422, 12059, 281, 257, 9410, 2856], "temperature": 0.0, "avg_logprob": -0.18032565483680138, "compression_ratio": 1.6245210727969348, "no_speech_prob": 6.748006853740662e-06}, {"id": 1124, "seek": 540650, "start": 5406.5, "end": 5412.5, "text": " But the older languages like Python often did this because they kind of needed to they started out not being oh", "tokens": [583, 264, 4906, 8650, 411, 15329, 2049, 630, 341, 570, 436, 733, 295, 2978, 281, 436, 1409, 484, 406, 885, 1954], "temperature": 0.0, "avg_logprob": -0.2063978141713365, "compression_ratio": 1.6731517509727627, "no_speech_prob": 6.748021405655891e-06}, {"id": 1125, "seek": 540650, "start": 5412.5, "end": 5417.62, "text": " Oh, and then they kind of added. Oh, oh in a way that was hideously ugly so Perl", "tokens": [876, 11, 293, 550, 436, 733, 295, 3869, 13, 876, 11, 1954, 294, 257, 636, 300, 390, 6479, 5098, 12246, 370, 3026, 75], "temperature": 0.0, "avg_logprob": -0.2063978141713365, "compression_ratio": 1.6731517509727627, "no_speech_prob": 6.748021405655891e-06}, {"id": 1126, "seek": 540650, "start": 5418.02, "end": 5423.86, "text": " Which predates Python by a little bit kind of I think really came up with this approach and unfortunately", "tokens": [3013, 3852, 1024, 15329, 538, 257, 707, 857, 733, 295, 286, 519, 534, 1361, 493, 365, 341, 3109, 293, 7015], "temperature": 0.0, "avg_logprob": -0.2063978141713365, "compression_ratio": 1.6731517509727627, "no_speech_prob": 6.748021405655891e-06}, {"id": 1127, "seek": 540650, "start": 5424.38, "end": 5426.6, "text": " Other languages of that era stuck with it", "tokens": [5358, 8650, 295, 300, 4249, 5541, 365, 309], "temperature": 0.0, "avg_logprob": -0.2063978141713365, "compression_ratio": 1.6731517509727627, "no_speech_prob": 6.748021405655891e-06}, {"id": 1128, "seek": 540650, "start": 5427.42, "end": 5428.58, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.2063978141713365, "compression_ratio": 1.6731517509727627, "no_speech_prob": 6.748021405655891e-06}, {"id": 1129, "seek": 540650, "start": 5428.58, "end": 5432.56, "text": " You have to add in this magic self so the magic self now", "tokens": [509, 362, 281, 909, 294, 341, 5585, 2698, 370, 264, 5585, 2698, 586], "temperature": 0.0, "avg_logprob": -0.2063978141713365, "compression_ratio": 1.6731517509727627, "no_speech_prob": 6.748021405655891e-06}, {"id": 1130, "seek": 543256, "start": 5432.56, "end": 5435.64, "text": " when you're inside this class", "tokens": [562, 291, 434, 1854, 341, 1508], "temperature": 0.0, "avg_logprob": -0.17134973135861484, "compression_ratio": 1.583756345177665, "no_speech_prob": 5.955106644250918e-06}, {"id": 1131, "seek": 543256, "start": 5436.280000000001, "end": 5442.8, "text": " You can now pretend as if any property name you like exists, so I can now pretend", "tokens": [509, 393, 586, 11865, 382, 498, 604, 4707, 1315, 291, 411, 8198, 11, 370, 286, 393, 586, 11865], "temperature": 0.0, "avg_logprob": -0.17134973135861484, "compression_ratio": 1.583756345177665, "no_speech_prob": 5.955106644250918e-06}, {"id": 1132, "seek": 543256, "start": 5442.8, "end": 5446.02, "text": " There's something called self dot X. I can read from it", "tokens": [821, 311, 746, 1219, 2698, 5893, 1783, 13, 286, 393, 1401, 490, 309], "temperature": 0.0, "avg_logprob": -0.17134973135861484, "compression_ratio": 1.583756345177665, "no_speech_prob": 5.955106644250918e-06}, {"id": 1133, "seek": 543256, "start": 5446.02, "end": 5451.22, "text": " I can write to it right, but if I read from it, and I haven't yet written to it. I'll get an error", "tokens": [286, 393, 2464, 281, 309, 558, 11, 457, 498, 286, 1401, 490, 309, 11, 293, 286, 2378, 380, 1939, 3720, 281, 309, 13, 286, 603, 483, 364, 6713], "temperature": 0.0, "avg_logprob": -0.17134973135861484, "compression_ratio": 1.583756345177665, "no_speech_prob": 5.955106644250918e-06}, {"id": 1134, "seek": 543256, "start": 5452.56, "end": 5454.56, "text": " So the stuff that's passed", "tokens": [407, 264, 1507, 300, 311, 4678], "temperature": 0.0, "avg_logprob": -0.17134973135861484, "compression_ratio": 1.583756345177665, "no_speech_prob": 5.955106644250918e-06}, {"id": 1135, "seek": 543256, "start": 5454.88, "end": 5456.76, "text": " to the constructor", "tokens": [281, 264, 47479], "temperature": 0.0, "avg_logprob": -0.17134973135861484, "compression_ratio": 1.583756345177665, "no_speech_prob": 5.955106644250918e-06}, {"id": 1136, "seek": 545676, "start": 5456.76, "end": 5463.320000000001, "text": " Get thrown away by default like there's nothing that like says you need to this class needs to remember what these things are", "tokens": [3240, 11732, 1314, 538, 7576, 411, 456, 311, 1825, 300, 411, 1619, 291, 643, 281, 341, 1508, 2203, 281, 1604, 437, 613, 721, 366], "temperature": 0.0, "avg_logprob": -0.218340827423392, "compression_ratio": 1.6835443037974684, "no_speech_prob": 9.666007827036083e-06}, {"id": 1137, "seek": 545676, "start": 5463.68, "end": 5468.68, "text": " But anything that we stick inside self it's remembered for all time", "tokens": [583, 1340, 300, 321, 2897, 1854, 2698, 309, 311, 13745, 337, 439, 565], "temperature": 0.0, "avg_logprob": -0.218340827423392, "compression_ratio": 1.6835443037974684, "no_speech_prob": 9.666007827036083e-06}, {"id": 1138, "seek": 545676, "start": 5468.68, "end": 5475.04, "text": " You know as long as this object exists you can access it. It's remembered so now that I've gone", "tokens": [509, 458, 382, 938, 382, 341, 2657, 8198, 291, 393, 2105, 309, 13, 467, 311, 13745, 370, 586, 300, 286, 600, 2780], "temperature": 0.0, "avg_logprob": -0.218340827423392, "compression_ratio": 1.6835443037974684, "no_speech_prob": 9.666007827036083e-06}, {"id": 1139, "seek": 545676, "start": 5475.360000000001, "end": 5479.52, "text": " In fact, let's do this right, so let's let's create the tree ensemble class", "tokens": [682, 1186, 11, 718, 311, 360, 341, 558, 11, 370, 718, 311, 718, 311, 1884, 264, 4230, 19492, 1508], "temperature": 0.0, "avg_logprob": -0.218340827423392, "compression_ratio": 1.6835443037974684, "no_speech_prob": 9.666007827036083e-06}, {"id": 1140, "seek": 545676, "start": 5480.76, "end": 5482.400000000001, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.218340827423392, "compression_ratio": 1.6835443037974684, "no_speech_prob": 9.666007827036083e-06}, {"id": 1141, "seek": 545676, "start": 5482.400000000001, "end": 5483.84, "text": " Let's now", "tokens": [961, 311, 586], "temperature": 0.0, "avg_logprob": -0.218340827423392, "compression_ratio": 1.6835443037974684, "no_speech_prob": 9.666007827036083e-06}, {"id": 1142, "seek": 545676, "start": 5483.84, "end": 5485.84, "text": " Instantiate it okay", "tokens": [2730, 11520, 473, 309, 1392], "temperature": 0.0, "avg_logprob": -0.218340827423392, "compression_ratio": 1.6835443037974684, "no_speech_prob": 9.666007827036083e-06}, {"id": 1143, "seek": 548584, "start": 5485.84, "end": 5488.54, "text": " Of course we haven't got X. We need to call", "tokens": [2720, 1164, 321, 2378, 380, 658, 1783, 13, 492, 643, 281, 818], "temperature": 0.0, "avg_logprob": -0.25787384565486465, "compression_ratio": 1.2844036697247707, "no_speech_prob": 5.01471959069022e-06}, {"id": 1144, "seek": 548584, "start": 5489.84, "end": 5491.84, "text": " X train", "tokens": [1783, 3847], "temperature": 0.0, "avg_logprob": -0.25787384565486465, "compression_ratio": 1.2844036697247707, "no_speech_prob": 5.01471959069022e-06}, {"id": 1145, "seek": 548584, "start": 5492.16, "end": 5494.16, "text": " Y train", "tokens": [398, 3847], "temperature": 0.0, "avg_logprob": -0.25787384565486465, "compression_ratio": 1.2844036697247707, "no_speech_prob": 5.01471959069022e-06}, {"id": 1146, "seek": 548584, "start": 5497.88, "end": 5500.96, "text": " Okay decision tree is not defined so let's", "tokens": [1033, 3537, 4230, 307, 406, 7642, 370, 718, 311], "temperature": 0.0, "avg_logprob": -0.25787384565486465, "compression_ratio": 1.2844036697247707, "no_speech_prob": 5.01471959069022e-06}, {"id": 1147, "seek": 548584, "start": 5503.68, "end": 5505.68, "text": " Create a really minimal decision tree", "tokens": [20248, 257, 534, 13206, 3537, 4230], "temperature": 0.0, "avg_logprob": -0.25787384565486465, "compression_ratio": 1.2844036697247707, "no_speech_prob": 5.01471959069022e-06}, {"id": 1148, "seek": 550568, "start": 5505.68, "end": 5515.04, "text": " There we go okay, so here is enough to actually", "tokens": [821, 321, 352, 1392, 11, 370, 510, 307, 1547, 281, 767], "temperature": 0.0, "avg_logprob": -0.22526837491441046, "compression_ratio": 2.083798882681564, "no_speech_prob": 4.7378929934893677e-07}, {"id": 1149, "seek": 550568, "start": 5515.56, "end": 5519.84, "text": " Instantiate our tree ensemble okay, so we have to find the in it for it", "tokens": [2730, 11520, 473, 527, 4230, 19492, 1392, 11, 370, 321, 362, 281, 915, 264, 294, 309, 337, 309], "temperature": 0.0, "avg_logprob": -0.22526837491441046, "compression_ratio": 2.083798882681564, "no_speech_prob": 4.7378929934893677e-07}, {"id": 1150, "seek": 550568, "start": 5519.92, "end": 5522.0, "text": " We have to find the in it for decision tree", "tokens": [492, 362, 281, 915, 264, 294, 309, 337, 3537, 4230], "temperature": 0.0, "avg_logprob": -0.22526837491441046, "compression_ratio": 2.083798882681564, "no_speech_prob": 4.7378929934893677e-07}, {"id": 1151, "seek": 550568, "start": 5522.0, "end": 5528.6, "text": " We need decision trees in it to be defined because inside our ensemble in it it called self dot create tree and", "tokens": [492, 643, 3537, 5852, 294, 309, 281, 312, 7642, 570, 1854, 527, 19492, 294, 309, 309, 1219, 2698, 5893, 1884, 4230, 293], "temperature": 0.0, "avg_logprob": -0.22526837491441046, "compression_ratio": 2.083798882681564, "no_speech_prob": 4.7378929934893677e-07}, {"id": 1152, "seek": 552860, "start": 5528.6, "end": 5535.0, "text": " Then self dot create tree called the decision tree constructor and then decision tree constructor", "tokens": [1396, 2698, 5893, 1884, 4230, 1219, 264, 3537, 4230, 47479, 293, 550, 3537, 4230, 47479], "temperature": 0.0, "avg_logprob": -0.18848638685922775, "compression_ratio": 1.6235955056179776, "no_speech_prob": 3.0894693736627232e-06}, {"id": 1153, "seek": 552860, "start": 5536.0, "end": 5542.52, "text": " Basically does nothing at all other than save some information right so at this point we can now go m dot", "tokens": [8537, 775, 1825, 412, 439, 661, 813, 3155, 512, 1589, 558, 370, 412, 341, 935, 321, 393, 586, 352, 275, 5893], "temperature": 0.0, "avg_logprob": -0.18848638685922775, "compression_ratio": 1.6235955056179776, "no_speech_prob": 3.0894693736627232e-06}, {"id": 1154, "seek": 554252, "start": 5542.52, "end": 5556.360000000001, "text": " Okay, and if I press tab at this point can anybody tell me what I would expect to see", "tokens": [1033, 11, 293, 498, 286, 1886, 4421, 412, 341, 935, 393, 4472, 980, 385, 437, 286, 576, 2066, 281, 536], "temperature": 0.0, "avg_logprob": -0.2011248644660501, "compression_ratio": 1.5542857142857143, "no_speech_prob": 8.013427759578917e-06}, {"id": 1155, "seek": 554252, "start": 5558.8, "end": 5564.320000000001, "text": " We would see like a we would see a drop-down of all available methods for that class okay", "tokens": [492, 576, 536, 411, 257, 321, 576, 536, 257, 3270, 12, 5093, 295, 439, 2435, 7150, 337, 300, 1508, 1392], "temperature": 0.0, "avg_logprob": -0.2011248644660501, "compression_ratio": 1.5542857142857143, "no_speech_prob": 8.013427759578917e-06}, {"id": 1156, "seek": 554252, "start": 5565.4400000000005, "end": 5570.72, "text": " In this case so if m is a tree ensemble we would have create tree and predict okay anything else", "tokens": [682, 341, 1389, 370, 498, 275, 307, 257, 4230, 19492, 321, 576, 362, 1884, 4230, 293, 6069, 1392, 1340, 1646], "temperature": 0.0, "avg_logprob": -0.2011248644660501, "compression_ratio": 1.5542857142857143, "no_speech_prob": 8.013427759578917e-06}, {"id": 1157, "seek": 557072, "start": 5570.72, "end": 5571.88, "text": " um", "tokens": [1105], "temperature": 0.0, "avg_logprob": -0.3056376100790621, "compression_ratio": 1.704225352112676, "no_speech_prob": 5.771875294158235e-06}, {"id": 1158, "seek": 557072, "start": 5571.88, "end": 5578.240000000001, "text": " Wait what oh yeah, and as well as Ernest whispered the variables as well. Yeah, so the", "tokens": [3802, 437, 1954, 1338, 11, 293, 382, 731, 382, 24147, 377, 26018, 292, 264, 9102, 382, 731, 13, 865, 11, 370, 264], "temperature": 0.0, "avg_logprob": -0.3056376100790621, "compression_ratio": 1.704225352112676, "no_speech_prob": 5.771875294158235e-06}, {"id": 1159, "seek": 557072, "start": 5579.68, "end": 5584.92, "text": " Variable could mean a lot of things well say the attributes so the things that we put inside self so if I hit tab", "tokens": [32511, 712, 727, 914, 257, 688, 295, 721, 731, 584, 264, 17212, 370, 264, 721, 300, 321, 829, 1854, 2698, 370, 498, 286, 2045, 4421], "temperature": 0.0, "avg_logprob": -0.3056376100790621, "compression_ratio": 1.704225352112676, "no_speech_prob": 5.771875294158235e-06}, {"id": 1160, "seek": 557072, "start": 5585.4800000000005, "end": 5591.8, "text": " Right there. They are right as Taylor said there's create tree there's predict, and then there's everything else be put inside self", "tokens": [1779, 456, 13, 814, 366, 558, 382, 12060, 848, 456, 311, 1884, 4230, 456, 311, 6069, 11, 293, 550, 456, 311, 1203, 1646, 312, 829, 1854, 2698], "temperature": 0.0, "avg_logprob": -0.3056376100790621, "compression_ratio": 1.704225352112676, "no_speech_prob": 5.771875294158235e-06}, {"id": 1161, "seek": 557072, "start": 5592.04, "end": 5594.04, "text": " right so if I", "tokens": [558, 370, 498, 286], "temperature": 0.0, "avg_logprob": -0.3056376100790621, "compression_ratio": 1.704225352112676, "no_speech_prob": 5.771875294158235e-06}, {"id": 1162, "seek": 557072, "start": 5594.360000000001, "end": 5596.360000000001, "text": " look at", "tokens": [574, 412], "temperature": 0.0, "avg_logprob": -0.3056376100790621, "compression_ratio": 1.704225352112676, "no_speech_prob": 5.771875294158235e-06}, {"id": 1163, "seek": 557072, "start": 5596.360000000001, "end": 5598.360000000001, "text": " m dot", "tokens": [275, 5893], "temperature": 0.0, "avg_logprob": -0.3056376100790621, "compression_ratio": 1.704225352112676, "no_speech_prob": 5.771875294158235e-06}, {"id": 1164, "seek": 559836, "start": 5598.36, "end": 5602.24, "text": " Min leaf if I hit shift enter what will I say?", "tokens": [2829, 10871, 498, 286, 2045, 5513, 3242, 437, 486, 286, 584, 30], "temperature": 0.0, "avg_logprob": -0.26323997096011514, "compression_ratio": 1.729064039408867, "no_speech_prob": 4.2893052523140796e-06}, {"id": 1165, "seek": 559836, "start": 5604.24, "end": 5606.32, "text": " Yeah, the number that I just put there", "tokens": [865, 11, 264, 1230, 300, 286, 445, 829, 456], "temperature": 0.0, "avg_logprob": -0.26323997096011514, "compression_ratio": 1.729064039408867, "no_speech_prob": 4.2893052523140796e-06}, {"id": 1166, "seek": 559836, "start": 5606.32, "end": 5612.299999999999, "text": " I put min leaf is three so that went up here to min leaf this here is a default argument", "tokens": [286, 829, 923, 10871, 307, 1045, 370, 300, 1437, 493, 510, 281, 923, 10871, 341, 510, 307, 257, 7576, 6770], "temperature": 0.0, "avg_logprob": -0.26323997096011514, "compression_ratio": 1.729064039408867, "no_speech_prob": 4.2893052523140796e-06}, {"id": 1167, "seek": 559836, "start": 5612.299999999999, "end": 5617.799999999999, "text": " So it says if I don't pass anything it'll be five, but I did pass something right so three self dot min leaf", "tokens": [407, 309, 1619, 498, 286, 500, 380, 1320, 1340, 309, 603, 312, 1732, 11, 457, 286, 630, 1320, 746, 558, 370, 1045, 2698, 5893, 923, 10871], "temperature": 0.0, "avg_logprob": -0.26323997096011514, "compression_ratio": 1.729064039408867, "no_speech_prob": 4.2893052523140796e-06}, {"id": 1168, "seek": 559836, "start": 5618.4, "end": 5621.98, "text": " Here is there going to be equal to min leaf here", "tokens": [1692, 307, 456, 516, 281, 312, 2681, 281, 923, 10871, 510], "temperature": 0.0, "avg_logprob": -0.26323997096011514, "compression_ratio": 1.729064039408867, "no_speech_prob": 4.2893052523140796e-06}, {"id": 1169, "seek": 559836, "start": 5622.719999999999, "end": 5624.719999999999, "text": " so something which", "tokens": [370, 746, 597], "temperature": 0.0, "avg_logprob": -0.26323997096011514, "compression_ratio": 1.729064039408867, "no_speech_prob": 4.2893052523140796e-06}, {"id": 1170, "seek": 562472, "start": 5624.72, "end": 5628.56, "text": " Like because of this rather annoying way of doing o o", "tokens": [1743, 570, 295, 341, 2831, 11304, 636, 295, 884, 277, 277], "temperature": 0.0, "avg_logprob": -0.22300547864063677, "compression_ratio": 1.5533980582524272, "no_speech_prob": 1.3496998008122318e-06}, {"id": 1171, "seek": 562472, "start": 5629.64, "end": 5632.6, "text": " It does mean that it's very easy to accidentally forget", "tokens": [467, 775, 914, 300, 309, 311, 588, 1858, 281, 15715, 2870], "temperature": 0.0, "avg_logprob": -0.22300547864063677, "compression_ratio": 1.5533980582524272, "no_speech_prob": 1.3496998008122318e-06}, {"id": 1172, "seek": 562472, "start": 5633.88, "end": 5638.16, "text": " To do that right so if I don't assign it to self dot min leaf", "tokens": [1407, 360, 300, 558, 370, 498, 286, 500, 380, 6269, 309, 281, 2698, 5893, 923, 10871], "temperature": 0.0, "avg_logprob": -0.22300547864063677, "compression_ratio": 1.5533980582524272, "no_speech_prob": 1.3496998008122318e-06}, {"id": 1173, "seek": 562472, "start": 5639.64, "end": 5644.76, "text": " Right then I get an error and so here tree ensemble doesn't happen min leaf", "tokens": [1779, 550, 286, 483, 364, 6713, 293, 370, 510, 4230, 19492, 1177, 380, 1051, 923, 10871], "temperature": 0.0, "avg_logprob": -0.22300547864063677, "compression_ratio": 1.5533980582524272, "no_speech_prob": 1.3496998008122318e-06}, {"id": 1174, "seek": 562472, "start": 5645.08, "end": 5649.8, "text": " All right, so how do I create that attribute? I just put something in it", "tokens": [1057, 558, 11, 370, 577, 360, 286, 1884, 300, 19667, 30, 286, 445, 829, 746, 294, 309], "temperature": 0.0, "avg_logprob": -0.22300547864063677, "compression_ratio": 1.5533980582524272, "no_speech_prob": 1.3496998008122318e-06}, {"id": 1175, "seek": 564980, "start": 5649.8, "end": 5656.04, "text": " Okay, so if you want to like if you don't know what a value of it should be yet", "tokens": [1033, 11, 370, 498, 291, 528, 281, 411, 498, 291, 500, 380, 458, 437, 257, 2158, 295, 309, 820, 312, 1939], "temperature": 0.0, "avg_logprob": -0.18841506242752076, "compression_ratio": 1.489247311827957, "no_speech_prob": 1.6797267790025217e-06}, {"id": 1176, "seek": 564980, "start": 5656.04, "end": 5660.66, "text": " But you kind of need to be able to refer to it. You can always go like self dot min leaf", "tokens": [583, 291, 733, 295, 643, 281, 312, 1075, 281, 2864, 281, 309, 13, 509, 393, 1009, 352, 411, 2698, 5893, 923, 10871], "temperature": 0.0, "avg_logprob": -0.18841506242752076, "compression_ratio": 1.489247311827957, "no_speech_prob": 1.6797267790025217e-06}, {"id": 1177, "seek": 564980, "start": 5661.92, "end": 5663.28, "text": " equals none", "tokens": [6915, 6022], "temperature": 0.0, "avg_logprob": -0.18841506242752076, "compression_ratio": 1.489247311827957, "no_speech_prob": 1.6797267790025217e-06}, {"id": 1178, "seek": 564980, "start": 5663.28, "end": 5667.12, "text": " Right so at least it's something you can read check for noneness and not have an error", "tokens": [1779, 370, 412, 1935, 309, 311, 746, 291, 393, 1401, 1520, 337, 2107, 15264, 293, 406, 362, 364, 6713], "temperature": 0.0, "avg_logprob": -0.18841506242752076, "compression_ratio": 1.489247311827957, "no_speech_prob": 1.6797267790025217e-06}, {"id": 1179, "seek": 564980, "start": 5671.16, "end": 5673.0, "text": " Great", "tokens": [3769], "temperature": 0.0, "avg_logprob": -0.18841506242752076, "compression_ratio": 1.489247311827957, "no_speech_prob": 1.6797267790025217e-06}, {"id": 1180, "seek": 564980, "start": 5673.0, "end": 5674.8, "text": " now", "tokens": [586], "temperature": 0.0, "avg_logprob": -0.18841506242752076, "compression_ratio": 1.489247311827957, "no_speech_prob": 1.6797267790025217e-06}, {"id": 1181, "seek": 567480, "start": 5674.8, "end": 5682.56, "text": " Interestingly I was able to instantiate she ensemble even though predict refers to a method of decision tree", "tokens": [30564, 286, 390, 1075, 281, 9836, 13024, 750, 19492, 754, 1673, 6069, 14942, 281, 257, 3170, 295, 3537, 4230], "temperature": 0.0, "avg_logprob": -0.22829036948121625, "compression_ratio": 1.5844155844155845, "no_speech_prob": 1.3081740917186835e-06}, {"id": 1182, "seek": 567480, "start": 5682.56, "end": 5690.8, "text": " That doesn't exist and this is actually something very nice about the dynamic nature of Python is that", "tokens": [663, 1177, 380, 2514, 293, 341, 307, 767, 746, 588, 1481, 466, 264, 8546, 3687, 295, 15329, 307, 300], "temperature": 0.0, "avg_logprob": -0.22829036948121625, "compression_ratio": 1.5844155844155845, "no_speech_prob": 1.3081740917186835e-06}, {"id": 1183, "seek": 567480, "start": 5692.24, "end": 5696.900000000001, "text": " Because it's not like compiling it. It's not checking anything unless you're using it", "tokens": [1436, 309, 311, 406, 411, 715, 4883, 309, 13, 467, 311, 406, 8568, 1340, 5969, 291, 434, 1228, 309], "temperature": 0.0, "avg_logprob": -0.22829036948121625, "compression_ratio": 1.5844155844155845, "no_speech_prob": 1.3081740917186835e-06}, {"id": 1184, "seek": 567480, "start": 5697.24, "end": 5702.4800000000005, "text": " right so we can go ahead and create decision D dot predict later and", "tokens": [558, 370, 321, 393, 352, 2286, 293, 1884, 3537, 413, 5893, 6069, 1780, 293], "temperature": 0.0, "avg_logprob": -0.22829036948121625, "compression_ratio": 1.5844155844155845, "no_speech_prob": 1.3081740917186835e-06}, {"id": 1185, "seek": 570248, "start": 5702.48, "end": 5709.0, "text": " Then our our instantiate an object will magically start working right it doesn't actually look up", "tokens": [1396, 527, 527, 9836, 13024, 364, 2657, 486, 39763, 722, 1364, 558, 309, 1177, 380, 767, 574, 493], "temperature": 0.0, "avg_logprob": -0.17785042656792535, "compression_ratio": 1.620253164556962, "no_speech_prob": 2.2603117031394504e-06}, {"id": 1186, "seek": 570248, "start": 5709.32, "end": 5714.5199999999995, "text": " That functions that methods details until you use it and so it really helps with top-down", "tokens": [663, 6828, 300, 7150, 4365, 1826, 291, 764, 309, 293, 370, 309, 534, 3665, 365, 1192, 12, 5093], "temperature": 0.0, "avg_logprob": -0.17785042656792535, "compression_ratio": 1.620253164556962, "no_speech_prob": 2.2603117031394504e-06}, {"id": 1187, "seek": 570248, "start": 5715.32, "end": 5717.32, "text": " programming", "tokens": [9410], "temperature": 0.0, "avg_logprob": -0.17785042656792535, "compression_ratio": 1.620253164556962, "no_speech_prob": 2.2603117031394504e-06}, {"id": 1188, "seek": 570248, "start": 5718.799999999999, "end": 5720.32, "text": " Okay, so", "tokens": [1033, 11, 370], "temperature": 0.0, "avg_logprob": -0.17785042656792535, "compression_ratio": 1.620253164556962, "no_speech_prob": 2.2603117031394504e-06}, {"id": 1189, "seek": 570248, "start": 5720.32, "end": 5721.879999999999, "text": " when you're", "tokens": [562, 291, 434], "temperature": 0.0, "avg_logprob": -0.17785042656792535, "compression_ratio": 1.620253164556962, "no_speech_prob": 2.2603117031394504e-06}, {"id": 1190, "seek": 570248, "start": 5721.879999999999, "end": 5729.48, "text": " Inside a class definition in other words you're at that indentation level you know indented one in so these are all class definitions", "tokens": [15123, 257, 1508, 7123, 294, 661, 2283, 291, 434, 412, 300, 44494, 399, 1496, 291, 458, 1016, 6003, 472, 294, 370, 613, 366, 439, 1508, 21988], "temperature": 0.0, "avg_logprob": -0.17785042656792535, "compression_ratio": 1.620253164556962, "no_speech_prob": 2.2603117031394504e-06}, {"id": 1191, "seek": 572948, "start": 5729.48, "end": 5736.28, "text": " Any function that you create unless you do some special things that we're not going to talk about yet is", "tokens": [2639, 2445, 300, 291, 1884, 5969, 291, 360, 512, 2121, 721, 300, 321, 434, 406, 516, 281, 751, 466, 1939, 307], "temperature": 0.0, "avg_logprob": -0.2563963497386259, "compression_ratio": 1.6748768472906403, "no_speech_prob": 2.123370904882904e-06}, {"id": 1192, "seek": 572948, "start": 5736.839999999999, "end": 5741.0199999999995, "text": " Automatically a method of that class and so every method of that class", "tokens": [24619, 5030, 257, 3170, 295, 300, 1508, 293, 370, 633, 3170, 295, 300, 1508], "temperature": 0.0, "avg_logprob": -0.2563963497386259, "compression_ratio": 1.6748768472906403, "no_speech_prob": 2.123370904882904e-06}, {"id": 1193, "seek": 572948, "start": 5741.719999999999, "end": 5743.32, "text": " magically gets a", "tokens": [39763, 2170, 257], "temperature": 0.0, "avg_logprob": -0.2563963497386259, "compression_ratio": 1.6748768472906403, "no_speech_prob": 2.123370904882904e-06}, {"id": 1194, "seek": 572948, "start": 5743.32, "end": 5745.32, "text": " self pass to it", "tokens": [2698, 1320, 281, 309], "temperature": 0.0, "avg_logprob": -0.2563963497386259, "compression_ratio": 1.6748768472906403, "no_speech_prob": 2.123370904882904e-06}, {"id": 1195, "seek": 572948, "start": 5746.04, "end": 5748.04, "text": " So we could call", "tokens": [407, 321, 727, 818], "temperature": 0.0, "avg_logprob": -0.2563963497386259, "compression_ratio": 1.6748768472906403, "no_speech_prob": 2.123370904882904e-06}, {"id": 1196, "seek": 572948, "start": 5748.839999999999, "end": 5750.44, "text": " Since we've got a tree ensemble", "tokens": [4162, 321, 600, 658, 257, 4230, 19492], "temperature": 0.0, "avg_logprob": -0.2563963497386259, "compression_ratio": 1.6748768472906403, "no_speech_prob": 2.123370904882904e-06}, {"id": 1197, "seek": 572948, "start": 5750.44, "end": 5754.959999999999, "text": " We could call M dot create tree and we don't put anything inside those parentheses", "tokens": [492, 727, 818, 376, 5893, 1884, 4230, 293, 321, 500, 380, 829, 1340, 1854, 729, 34153], "temperature": 0.0, "avg_logprob": -0.2563963497386259, "compression_ratio": 1.6748768472906403, "no_speech_prob": 2.123370904882904e-06}, {"id": 1198, "seek": 575496, "start": 5754.96, "end": 5760.24, "text": " Because the magic self will be passed and the magic self will be whatever M is", "tokens": [1436, 264, 5585, 2698, 486, 312, 4678, 293, 264, 5585, 2698, 486, 312, 2035, 376, 307], "temperature": 0.0, "avg_logprob": -0.23509461170918233, "compression_ratio": 1.7707006369426752, "no_speech_prob": 1.3287733509059763e-06}, {"id": 1199, "seek": 575496, "start": 5761.32, "end": 5768.8, "text": " Okay, so M dot create tree returns a decision tree just like we asked it to right so", "tokens": [1033, 11, 370, 376, 5893, 1884, 4230, 11247, 257, 3537, 4230, 445, 411, 321, 2351, 309, 281, 558, 370], "temperature": 0.0, "avg_logprob": -0.23509461170918233, "compression_ratio": 1.7707006369426752, "no_speech_prob": 1.3287733509059763e-06}, {"id": 1200, "seek": 575496, "start": 5769.32, "end": 5771.32, "text": " M dot create tree", "tokens": [376, 5893, 1884, 4230], "temperature": 0.0, "avg_logprob": -0.23509461170918233, "compression_ratio": 1.7707006369426752, "no_speech_prob": 1.3287733509059763e-06}, {"id": 1201, "seek": 575496, "start": 5771.92, "end": 5773.92, "text": " dot ID access", "tokens": [5893, 7348, 2105], "temperature": 0.0, "avg_logprob": -0.23509461170918233, "compression_ratio": 1.7707006369426752, "no_speech_prob": 1.3287733509059763e-06}, {"id": 1202, "seek": 575496, "start": 5774.2, "end": 5777.64, "text": " Will give us the self dot ID access inside the decision tree", "tokens": [3099, 976, 505, 264, 2698, 5893, 7348, 2105, 1854, 264, 3537, 4230], "temperature": 0.0, "avg_logprob": -0.23509461170918233, "compression_ratio": 1.7707006369426752, "no_speech_prob": 1.3287733509059763e-06}, {"id": 1203, "seek": 575496, "start": 5779.62, "end": 5781.62, "text": " Okay, which is set to", "tokens": [1033, 11, 597, 307, 992, 281], "temperature": 0.0, "avg_logprob": -0.23509461170918233, "compression_ratio": 1.7707006369426752, "no_speech_prob": 1.3287733509059763e-06}, {"id": 1204, "seek": 578162, "start": 5781.62, "end": 5784.7, "text": " And P dot arrange range self dot sample size", "tokens": [400, 430, 5893, 9424, 3613, 2698, 5893, 6889, 2744], "temperature": 0.0, "avg_logprob": -0.22507164695046164, "compression_ratio": 1.5142857142857142, "no_speech_prob": 1.8738708149612648e-06}, {"id": 1205, "seek": 578162, "start": 5788.38, "end": 5792.18, "text": " Why is data scientists do we care about object-oriented programming?", "tokens": [1545, 307, 1412, 7708, 360, 321, 1127, 466, 2657, 12, 27414, 9410, 30], "temperature": 0.0, "avg_logprob": -0.22507164695046164, "compression_ratio": 1.5142857142857142, "no_speech_prob": 1.8738708149612648e-06}, {"id": 1206, "seek": 578162, "start": 5793.5599999999995, "end": 5801.62, "text": " Because a lot of the stuff you use is going to require you to implement stuff with OOP for example", "tokens": [1436, 257, 688, 295, 264, 1507, 291, 764, 307, 516, 281, 3651, 291, 281, 4445, 1507, 365, 422, 12059, 337, 1365], "temperature": 0.0, "avg_logprob": -0.22507164695046164, "compression_ratio": 1.5142857142857142, "no_speech_prob": 1.8738708149612648e-06}, {"id": 1207, "seek": 578162, "start": 5802.36, "end": 5809.9, "text": " Every single pie torch model of any kind is created with OOP. It's the only way to create by torch models", "tokens": [2048, 2167, 1730, 27822, 2316, 295, 604, 733, 307, 2942, 365, 422, 12059, 13, 467, 311, 264, 787, 636, 281, 1884, 538, 27822, 5245], "temperature": 0.0, "avg_logprob": -0.22507164695046164, "compression_ratio": 1.5142857142857142, "no_speech_prob": 1.8738708149612648e-06}, {"id": 1208, "seek": 580990, "start": 5809.9, "end": 5811.9, "text": " um good news is", "tokens": [1105, 665, 2583, 307], "temperature": 0.0, "avg_logprob": -0.18952830786843902, "compression_ratio": 1.8243243243243243, "no_speech_prob": 2.6016036827058997e-06}, {"id": 1209, "seek": 580990, "start": 5813.379999999999, "end": 5819.7, "text": " What you see here is the entirety of what you need to know so you this is all you need to know you need to", "tokens": [708, 291, 536, 510, 307, 264, 31557, 295, 437, 291, 643, 281, 458, 370, 291, 341, 307, 439, 291, 643, 281, 458, 291, 643, 281], "temperature": 0.0, "avg_logprob": -0.18952830786843902, "compression_ratio": 1.8243243243243243, "no_speech_prob": 2.6016036827058997e-06}, {"id": 1210, "seek": 580990, "start": 5819.7, "end": 5821.7, "text": " Know to create something called in it", "tokens": [10265, 281, 1884, 746, 1219, 294, 309], "temperature": 0.0, "avg_logprob": -0.18952830786843902, "compression_ratio": 1.8243243243243243, "no_speech_prob": 2.6016036827058997e-06}, {"id": 1211, "seek": 580990, "start": 5822.259999999999, "end": 5825.74, "text": " to assign the things that are passed in it to something called self and", "tokens": [281, 6269, 264, 721, 300, 366, 4678, 294, 309, 281, 746, 1219, 2698, 293], "temperature": 0.0, "avg_logprob": -0.18952830786843902, "compression_ratio": 1.8243243243243243, "no_speech_prob": 2.6016036827058997e-06}, {"id": 1212, "seek": 580990, "start": 5826.66, "end": 5829.42, "text": " Then to stick the word self after each of your methods", "tokens": [1396, 281, 2897, 264, 1349, 2698, 934, 1184, 295, 428, 7150], "temperature": 0.0, "avg_logprob": -0.18952830786843902, "compression_ratio": 1.8243243243243243, "no_speech_prob": 2.6016036827058997e-06}, {"id": 1213, "seek": 580990, "start": 5829.9, "end": 5837.66, "text": " Okay, and so the nice thing is like now to think as an OOP programmer is to realize you don't now have to pass around", "tokens": [1033, 11, 293, 370, 264, 1481, 551, 307, 411, 586, 281, 519, 382, 364, 422, 12059, 32116, 307, 281, 4325, 291, 500, 380, 586, 362, 281, 1320, 926], "temperature": 0.0, "avg_logprob": -0.18952830786843902, "compression_ratio": 1.8243243243243243, "no_speech_prob": 2.6016036827058997e-06}, {"id": 1214, "seek": 583766, "start": 5837.66, "end": 5843.78, "text": " X Y sample size and minleaf to every function that uses them by assigning them to", "tokens": [1783, 398, 6889, 2744, 293, 923, 306, 2792, 281, 633, 2445, 300, 4960, 552, 538, 49602, 552, 281], "temperature": 0.0, "avg_logprob": -0.20180007086859808, "compression_ratio": 1.5847457627118644, "no_speech_prob": 1.2482679494496551e-06}, {"id": 1215, "seek": 583766, "start": 5844.58, "end": 5848.22, "text": " Attributes of self they're now available like magic", "tokens": [7298, 2024, 1819, 295, 2698, 436, 434, 586, 2435, 411, 5585], "temperature": 0.0, "avg_logprob": -0.20180007086859808, "compression_ratio": 1.5847457627118644, "no_speech_prob": 1.2482679494496551e-06}, {"id": 1216, "seek": 583766, "start": 5848.78, "end": 5852.98, "text": " All right, so this is why OOP is super handy if you're particularly", "tokens": [1057, 558, 11, 370, 341, 307, 983, 422, 12059, 307, 1687, 13239, 498, 291, 434, 4098], "temperature": 0.0, "avg_logprob": -0.20180007086859808, "compression_ratio": 1.5847457627118644, "no_speech_prob": 1.2482679494496551e-06}, {"id": 1217, "seek": 583766, "start": 5852.98, "end": 5858.94, "text": " I started trying to create a decision tree initially without using OOP and try to like keep track of", "tokens": [286, 1409, 1382, 281, 1884, 257, 3537, 4230, 9105, 1553, 1228, 422, 12059, 293, 853, 281, 411, 1066, 2837, 295], "temperature": 0.0, "avg_logprob": -0.20180007086859808, "compression_ratio": 1.5847457627118644, "no_speech_prob": 1.2482679494496551e-06}, {"id": 1218, "seek": 583766, "start": 5859.42, "end": 5863.72, "text": " Like what that decision tree was meant to know about was very difficult", "tokens": [1743, 437, 300, 3537, 4230, 390, 4140, 281, 458, 466, 390, 588, 2252], "temperature": 0.0, "avg_logprob": -0.20180007086859808, "compression_ratio": 1.5847457627118644, "no_speech_prob": 1.2482679494496551e-06}, {"id": 1219, "seek": 586372, "start": 5863.72, "end": 5870.42, "text": " You know where else with OOP you can just say it inside the decision tree, you know self dot indexes equals this and", "tokens": [509, 458, 689, 1646, 365, 422, 12059, 291, 393, 445, 584, 309, 1854, 264, 3537, 4230, 11, 291, 458, 2698, 5893, 8186, 279, 6915, 341, 293], "temperature": 0.0, "avg_logprob": -0.21661163879944398, "compression_ratio": 1.6566037735849057, "no_speech_prob": 5.255362339084968e-06}, {"id": 1220, "seek": 586372, "start": 5871.26, "end": 5875.900000000001, "text": " Everything just works. Okay. Okay. That's great. So we're out of time. I think that's", "tokens": [5471, 445, 1985, 13, 1033, 13, 1033, 13, 663, 311, 869, 13, 407, 321, 434, 484, 295, 565, 13, 286, 519, 300, 311], "temperature": 0.0, "avg_logprob": -0.21661163879944398, "compression_ratio": 1.6566037735849057, "no_speech_prob": 5.255362339084968e-06}, {"id": 1221, "seek": 586372, "start": 5876.9400000000005, "end": 5878.9400000000005, "text": " That's great timing because", "tokens": [663, 311, 869, 10822, 570], "temperature": 0.0, "avg_logprob": -0.21661163879944398, "compression_ratio": 1.6566037735849057, "no_speech_prob": 5.255362339084968e-06}, {"id": 1222, "seek": 586372, "start": 5879.820000000001, "end": 5882.54, "text": " There's an introduction to OOP, but this week", "tokens": [821, 311, 364, 9339, 281, 422, 12059, 11, 457, 341, 1243], "temperature": 0.0, "avg_logprob": -0.21661163879944398, "compression_ratio": 1.6566037735849057, "no_speech_prob": 5.255362339084968e-06}, {"id": 1223, "seek": 586372, "start": 5883.34, "end": 5888.820000000001, "text": " You know next class I'm going to assume that you can use it right? So you should create some classes", "tokens": [509, 458, 958, 1508, 286, 478, 516, 281, 6552, 300, 291, 393, 764, 309, 558, 30, 407, 291, 820, 1884, 512, 5359], "temperature": 0.0, "avg_logprob": -0.21661163879944398, "compression_ratio": 1.6566037735849057, "no_speech_prob": 5.255362339084968e-06}, {"id": 1224, "seek": 586372, "start": 5889.5, "end": 5892.84, "text": " instantiate some classes look at their methods and properties", "tokens": [9836, 13024, 512, 5359, 574, 412, 641, 7150, 293, 7221], "temperature": 0.0, "avg_logprob": -0.21661163879944398, "compression_ratio": 1.6566037735849057, "no_speech_prob": 5.255362339084968e-06}, {"id": 1225, "seek": 589284, "start": 5892.84, "end": 5896.56, "text": " Have them call each other and so forth until you feel", "tokens": [3560, 552, 818, 1184, 661, 293, 370, 5220, 1826, 291, 841], "temperature": 0.0, "avg_logprob": -0.22107157987706802, "compression_ratio": 1.5268817204301075, "no_speech_prob": 1.406351657351479e-05}, {"id": 1226, "seek": 589284, "start": 5897.28, "end": 5904.2, "text": " Comfortable with them and maybe for those of you that haven't done OOP before you and find some other useful resources", "tokens": [2432, 10124, 365, 552, 293, 1310, 337, 729, 295, 291, 300, 2378, 380, 1096, 422, 12059, 949, 291, 293, 915, 512, 661, 4420, 3593], "temperature": 0.0, "avg_logprob": -0.22107157987706802, "compression_ratio": 1.5268817204301075, "no_speech_prob": 1.406351657351479e-05}, {"id": 1227, "seek": 589284, "start": 5904.2, "end": 5907.24, "text": " You could pop them onto the wiki thread so that other people know what you find", "tokens": [509, 727, 1665, 552, 3911, 264, 261, 9850, 7207, 370, 300, 661, 561, 458, 437, 291, 915], "temperature": 0.0, "avg_logprob": -0.22107157987706802, "compression_ratio": 1.5268817204301075, "no_speech_prob": 1.406351657351479e-05}, {"id": 1228, "seek": 590724, "start": 5907.24, "end": 5922.679999999999, "text": " Useful great. Thanks everybody", "tokens": [50364, 8278, 906, 869, 13, 2561, 2201, 51136], "temperature": 0.0, "avg_logprob": -0.6334172354804145, "compression_ratio": 0.7894736842105263, "no_speech_prob": 1.0779073818412144e-05}], "language": "en"}