{"text": " So we talked about pseudo-labeling a couple of weeks ago, and this is this way of dealing with semi-supervised learning. So remember how in the state farm competition we had far more unlabeled images in the test set than we had in the training set? And so the question was how do we take advantage of knowing something about the structure even though we don't have labels? We learned this crazy technique called pseudo-labeling, or a combination of pseudo-labeling and knowledge distillation, which is where you predict the outputs of the test set and then you act as if those outputs were true labels and you kind of add them in to your training. And the reason I wasn't able to actually implement that and see how it works was because we needed a way of combining two different sets of batches. And in particular, I think the advice I saw from Jeff Hinton when he wrote about pseudo-labeling is that you want something like 1 in 3 or 1 in 4 of your training data to come from the pseudo-labeled data and the rest to come from your real data. So the good news is I built that thing and it was ridiculously easy. This is the entire code. I call it the mix iterator and it will be in our utils class from tomorrow. And all it does is it's something where you create whatever generators of batches you like and then you pass an array of those iterators to this constructor and then every time the Keras system calls next on it, it grabs the next batch from all of those sets of batches and concatenates them all together. And so what that means in practice is that I tried doing pseudo-labeling for example on MNIST. Remember on MNIST we already had that pretty close to state of the art result, which was 99.69. So I thought, can we improve it anymore if we use pseudo-labeling on the test set? And so to do so, you just do this. You grab your training batches as usual, using data orientation if you want, whatever else. So here's my training batches. And then you create your pseudo-batches by saying, okay, my data is my test set and my labels are my predictions, and these are the predictions that I calculated back up here. So now this is the second set of batches, which is my pseudo-batches. And so then passing an array of those two things to the mix iterator now creates a new batch generator, which is going to give us a few images from here and a few images from here. How many? Well, however many you asked for. So in this case I was getting 64 from my training set and 64 divided by 4 from my test set. Come to think of it, that's probably less than Hinton recommends. That probably should have been divided by 3. And now I can use that just like any other generator. So then I just call model.fit generator and pass in that thing that I just created. And so what it's going to do is it's going to create a bunch of batches which will be 64 items from my regular training set and a quarter of that number of items from my pseudo-labeled set. And lo and behold, it gave me a slightly better score. There's only so much better we can do at this point, but that took us up to 99.72. It's worth mentioning that every.01% at this point is just one image, so we're really on the edges at this point. This is getting even closer to the state of the art, despite the fact that we're not doing any handwriting-specific techniques. I also tried it on the FISH dataset. I realized at that point that this allows us to do something else which is pretty neat, which is normally when we train on the training set and set aside a validation set, if we then want to submit to Kaggle, we've only trained on a subset of the data that they gave us. We didn't train on the validation set as well, which is not great. So what you can actually do is you can send 3 sets of batches to the MixedEterator. You can have your regular training batches, you can have your pseudo-labeled test batches, and if you think about it, you can also add in some validation batches using the true labels from the validation set. So this is something you do right at the end. When you say, this is a model I'm happy with, you can fine-tune it a bit using some of the real validation data. You can see here I've got out of my batch size of 64, I'm putting 44 from the training set, 4 from the validation set, and 16 from the pseudo-labeled test set. This worked pretty well. It got me from about 110th to about 60th on the leaderboard. Question from the audience. So if we go to Keras documentation, there is something called sample weight. I wonder if you can just set the sample weight to be lower for... Answer the question. I will mention that I found the way I'm doing it seems a little slow. There are some obvious ways I can speed it up. I'm not quite sure why it is, but it might be because this concatenation each time is kind of having to create new memory and that takes a long time. There are some obvious things I could do to try and speed it up. It's good enough and simple to do the job. I'm pleased that we now have a way to do convenient pseudo-label in Keras, and it seems to do a pretty good job. Question from the audience. So the other thing I wanted to talk about before we move on to the new material today is embeddings. I've had lots of questions about embeddings, and I think it's pretty clear that at least for some of you, some additional explanations would be helpful. So I wanted to start out by reminding you that when I introduced embeddings to you, the data that we had, we looked at this crosstab form of data. And when it's in this crosstab form, it's very easy to visualize what embeddings look like, which is for movie number 27 and user ID number 14, here is that movie ID's embedding right here, and here is that user ID's embedding right here, and so here is the dot product of the two right here. So that was all pretty straightforward. And so then all we had to do to optimize our embeddings was use the gradient descent solver that is built into Microsoft Excel, which is called solver, and we just told it what our objective is, which is this cell, and we set to minimize it by changing these sets of cells. Now the data that we are given in the MovieLens dataset, however, requires some manipulation to get into a crosstab form. We're actually given it in this form. And we wouldn't want to create a crosstab with all of this data because it would be way too big. It would be every single user times every single movie, and it would also be very inconvenient. So that's not how Keras works. Keras uses this data in exactly this format. And so let me show you how that works and what an embedding is really doing. So here is the exact same thing, but I'm going to show you this using the data in the format that Keras uses. So this is our input data. Every rating is a row and has a user ID, a movie ID, and a rating. And this is what an embedding matrix looks like for 15 users. So these are the user IDs, and for each user ID, here's user ID 14's embedding, and this is 29's embedding, and this is 72's embedding. At this stage, they're just random. They're just initializing random numbers. So this thing here is called an embedding matrix. And here is the movie embedding matrix. So the embedding matrix for Movie27 are these 5 numbers. So what happens when we look at user ID 14, movie ID 417, rating number 2? The first thing that happens is that we have to find user ID 14. And here it is. User ID 14 is the first thing in this array. So the index of user ID 14 is 1. So then here is the first row from the user embedding matrix. Similarly, movie ID 417, here is movie ID 417, and it is the 14th row of this table. So we want to return the 14th row. So you can see here it has looked up and found that it's the 14th row, and then indexed into the table and grabbed the 14th row. And so then to calculate the dot product, we simply take the dot product of the user embedding with the movie embedding. And then to calculate the loss, we simply take the rating and subtract the prediction and square it. And then to get the total loss function, we just add that all up and take the square root. So the orange background cells are the cells which we want our SGD solver to change in order to minimize this cell here, and then all of the orange bold cells are the calculated cells. So when I was saying last week that an embedding is simply looking up an array by an index, you can see why I was saying that. It's literally taking an index and it looks it up in an array and returns that row. That's literally all it's doing. You might want to convince yourself during the week if you haven't looked at this yet that this is identical to taking a one-pot encoded matrix and multiplying it by an embedding matrix. That's identical to doing this kind of lookup. So we can do exactly the same thing in this way, we can say data solver, we want to set this cell to a minimum by changing these cells. And if I say solve, then Excel will go away and try to improve our objective. You can see it's decreasing, it's down to about 2.5. And so what it's doing here is it's using gradient descent to try to find ways to increase or decrease all of these numbers such that that IRMSC becomes as low as possible. So that's literally all that is going on in our Keras example here, this dot product. So this thing here where we said create an embedding for a user, that's just saying create something where I can look up the user ID and find their row. This is doing the same for a movie, look up the movie ID and find its row. This here says take the dot product once you've found the two. And then this here says train a model where you take in that user ID and movie ID and try to predict the rating and use SGD to make it better and better. So you can see here that it's got the root mean squared error down to 0.4. So for example, the first one predicted 3, it's actually 2, predicted 4.5, it's actually 4, predicted 4.6, it's actually 5 and so forth. So you kind of get the idea of how it works. Word embeddings work exactly the same way. So inspired by one of the students who talked about this during the week, I grabbed the text of Green Eggs and Ham. So here is the text of Green Eggs and Ham. I am Daniel, I am Sam, Sam I am, that's Sam I am, etc. And I've turned this poem into a matrix. The way I did that was to take every unique word in that poem. Here is the ID of each of those words, just indexed from one. And so then I just randomly generated an embedding matrix. I equally well could have used the downloaded GloVe embeddings instead. And so then just for each word, I just look up in the list to find that word and find out what number it is. So I is number 8, and so here is the 8th row of the embedding matrix. So you can see here that we've started with a poem and we've turned it into a matrix of floats. So the reason we do this is because our machine learning tools want a matrix of floats, not a poem. So all of the questions about does it matter what the word IDs are, you can see it doesn't matter at all. All we're doing is we're looking them up in this matrix and returning the floats. And once we've done that, we never use them again. We just use this matrix of floats. So that's what embeddings are. So I hope that's helpful. Feel free to ask if you have any questions, either now or any other time, because we're going to be using embeddings throughout this class. So hopefully that helped a few people clarify what's going on. Okay, so let's get back to recurrent neural networks. So to remind you, we talked about the purpose of recurrent neural networks as being really all about memory. So it's really all about this idea of memory. If we're going to handle something like recognizing a comment start and a comment end and being able to keep track of the fact that we're in a comment for all of this time so that we can do modeling on this kind of structured language data, we're really going to need memory. That allows us to handle long-term dependencies and provides this stateful representation. So in general, the stuff we're talking about, we're going to be looking at things that kind of particularly need these three things. And it's also somewhat helpful just for when you have a variable length sequence. Question about embeddings. One is how does the size of my embedding depend on the number of unique words? So mapping green eggs and ham to 5 real numbers seems sufficient, but wouldn't be for all of JRR token. So your choice of how big to make your embedding matrix, as in how many latent factors to create, is one of these architectural decisions which we don't really have an answer to. My best suggestion, there's a few. One is to read the Word2Vec paper, which kind of introduced a lot of this, or at least took a look at the difference between a 50-dimensional, 100-dimensional, 200-dimensional, 300-dimensional, 600-dimensional, and see what are the different levels of accuracy that those different size embedding matrices created when the authors of that paper provided this information. So that's a quick shortcut because other people have already experimented and provided those results for you. The other is to do your own experiments. Try a few different sizes. It's not really about the length of the word list, it's really about the complexity of the language or other problem that you're trying to solve. And that's really problem-dependent and will require both your intuition developed from reading and experimenting and also your own experiments. And what would be the range of root mean square error value to say that a model is good? To say that a model is good is another model-specific issue. So a root mean square error is very interpretable. It's basically how far out is it on average. So if we were finding that we were getting ratings within about.4, if we're getting within.4 on average, that sounds like it's probably good enough to be useful for helping people find movies that they might like. But there's really no one solution. I actually wrote a whole paper about this. If you look up designing great data products, this is based on really mainly 10 years of work I did at a company I created called Optimal Decisions Group. And Optimal Decisions Group was all about how to use predictive modeling not just to make predictions but to optimize actions. And this whole paper is about that. In the end, it's really about coming up with a way to measure the benefit to your organization or to your project of getting that extra 0.1% accuracy. There are some suggestions on how to do that in this paper. So we looked at a visual vocabulary that we developed for writing down neural nets where any colored box represents a matrix of activations. That's a really important point to remember. A colored box represents a matrix of activations. So it could either be the input matrix, it could be the output matrix, or it could be the matrix that comes from taking an input and putting it through like a matrix product. The rectangle boxes represent inputs. The circular ones represent hidden, so intermediate, activations. And the triangles represent outputs. Arrows, very importantly, represent what we'll call layer operations. And a layer operation is anything that you do to one colored box to create another colored box. And in general, it's almost always going to involve some kind of linear function like a matrix product or a convolution. And it will probably also include some kind of activation function like ReLU or softness. Because the activation functions are pretty unimportant in terms of detail, I started removing those from the pictures as we started to look at more complex models. And then in fact, because the layer operations are pretty consistent, we probably know what they are, I started removing those as well, just to keep these simple. And so we're simplifying these diagrams to try and just keep the main pieces. And as we did so, we could start to create more complex diagrams. And so we talked about a kind of language model where we would take inputs of a character, character number 1 and character number 2, and we would try and predict character number 3. And so we thought one way to do that would be to create a deep neural network with 2 layers. The character1 input would go through a layer operation to create our first fully connected layer. That would go through another layer operation to create a second fully connected layer. And we would also add our second character input going through its own fully connected layer at this point. And to recall, the last important thing we have to learn is that 2 arrows going into a single shape means that we are adding the results of those 2 layer operations together. So 2 arrows going into a shape represents summing up element-wise the results of these 2 layer operations. So this was the little visual vocabulary that we set up last week. And I've kept track of it down here as to what the things are in case you forget. So now I wanted to point out something really interesting, which is that there's 3 kinds of layer operations going on. Here I'm expanding this now. We've got predicting a 4th character of a sequence using characters 1, 2 and 3 using exactly the same method as on the previous slide. There are layer operations that turn a character input into a hidden activation matrix. There are layer operations that turn one hidden layer activation into a new hidden layer activation. And then there's an operation that takes hidden activations and turns it into output activations. And so you can see here I've colored them in. Here I've got a little legend of these different colors. Green is the input to hidden, blue is the hidden to output, and orange is the hidden to hidden. So my claim is that the dimensions of the weight matrices for each of these different colored arrows, all of the green ones have the same dimensions because they're taking an input of vocab size and turning it into an output hidden activation of size number of activations. So all of these arrows represent weight matrices of the same dimensionality. Ditto, the orange arrows represent weight matrices of the same dimensionality. I would go further than that though and say the green arrows represent semantically the same thing. They're all saying how do you take a character and convert it into a hidden state. And the orange arrows are all saying how do you take a hidden state from a previous character and turn it into a hidden state for a new character. And then the blue one is saying how do you take a hidden state and turn it into outputs. When you look at it that way, all of these circles are basically the same thing. They're just representing this hidden state at a different point in time. And I'm going to use this word time in a fairly general way. I'm not really talking about time, I'm just talking about the sequence in which we're presenting additional pieces of information to this model. We first of all present the first character, then the second character, and then the third character. So we could redraw this whole thing in a simpler way and a more general way. Before we do, I'm actually going to show you in Keras how to build this model. And in doing so, we're going to learn a bit more about the functional API, which hopefully you'll find pretty interesting and useful. To do that, we are going to use this corpus of all of the collected works of Nietzsche. So we load in those works, we find all of the unique characters, of which there are 86. Then we create a mapping from the character to the index at which it appears in this list, and a mapping from the index to the character. So this is basically creating the equivalent of these tables, or more specifically, I guess, this table. Rather than using words, we're looking at characters. So that allows us to take the text of Nietzsche and convert it into a list of numbers where the numbers represent the number at which the character appears in this list. So here are the first 10. So at any point we can turn this, that's called idx, so we've converted our whole text into the equivalent indices. At any point we can turn it back into text by simply taking those indexes and looking them up in our index to character mapping. So here you can see we've turned it back into the start of the text again. So that's the data we're working with. The data we're working with is a list of character ids at this point, where those character ids represent the collected works of Nietzsche. So we're going to build a model which attempts to predict the fourth character from the previous character. So to do that, we're going to go through our whole list of indexes from 0 up to the end minus 3, and we're going to create a whole list of the 0th, 4th, 8th, 12th, etc characters, and a list of the 1st, 5th, 9th, etc, and the 2nd, 6th, 10th, etc. So this is going to represent the first character of each sequence, the second character of each sequence, the third character of each sequence, and this is the one we want to predict, the fourth character of each sequence. So we can now turn these into NumPy arrays just by stacking them up together. And so now we've got our input for our first characters, second characters, and third characters of every 4 character piece of this collected works. And then our y's, our labels, will simply be the fourth characters of each sequence. So here you can see them. So for example, if we took x1, x2, and x3 and took the first element of each, this is the first character of the text, the second character of the text, the third character of the text, and the fourth character of the text. So we'll be trying to predict this based on these 3. And then we'll try to predict this based on these 3. So that's our data format. So you can see we've got about 200,000 of these inputs for each of x1, x3, and y. And so as per usual, we're going to first of all turn them into embeddings by creating an embedding matrix. I will mention this is not normal. I haven't actually seen anybody else do this. Most people just treat them as one-hot encodings. So for example, the most widely used blog post about car RNNs, which really made them popular was Andre Kapathy's. You can see that in his version, he shows them as being one-hot encoded. We're not going to do that. We're going to turn them into embeddings. I think it makes a lot of sense. But like capital A and lowercase a have some similarities that an embedding can understand. Different types of things that have to be opened and closed, like different types of parentheses and quotes, have certain characteristics that can be constructed in embedding. There's all kinds of things that we would expect an embedding to capture. So my hypothesis was that an embedding is going to do a better job than just a one-hot encoding. In my experiments over the last couple of weeks, that generally seems to be true. So we're going to take each character, 1 through 3, and turn them into embeddings by first creating an input layer for them and then creating an embedding layer for that input. And then we return the input layer and the flattened version of the embedding layer. So this is the input to and output of our 3 embedding layers for our 3 input characters. So that's basically our inputs. So we now have to decide how many activations do we want. And so that's something we can just pick. So I've decided to go with 256. That's something that seemed reasonable, seems to have worked okay. So we now have to somehow construct something where each of our green arrows ends up with the same weight matrix. It turns out Keras makes this really easy with the Keras Functional API. When you call dense like this, what it's actually doing is it's creating a layer with a specific weight matrix. Notice that I haven't passed in anything here to say what it's connected to, so it's not part of a model yet. This is just saying I'm going to have something which is a dense layer which creates 256 activations. So it doesn't actually do anything until I then do this, so I connect it to something. So here I'm going to say character1's hidden state comes from taking character1, which was the output of our first embedding, and putting it through this denseIn layer. So this is the thing which creates our first circle. So the embedding is the thing which creates the output of our first rectangle. This creates our first circle. And so denseIn is the green arrow. So what that means is that in order to create the next set of activations, we need to create the orange arrow. So since the orange arrow is different weight matrix to the green arrow, we have to create a new dense layer. So here it is. I've got a new dense layer, and again with n hidden outputs. So by creating a new dense layer, this is a whole separate weight matrix. So now that I've done that, I can create my character2 hidden state, which is here, and I'm going to have to sum up two separate things. I'm going to take my character2 embedding, put it through my green arrow, denseIn, that's going to be there. I'm going to take the output of my character1's hidden state and run it through my orange arrow, which we call denseHidden, and then we're going to merge the two together. And merge by default does a sum. So this is adding together these two outputs. In other words, it's adding together these two layer operation outputs. And that gives us this circle. So the third character output is done in exactly the same way. We take the third character's embedding, run it through our green arrow, take the result of our previous hidden activations and run it through our orange arrow, and then merge the two together. Question from the audience. Is the first output the size of the latent fields in the embedding? The size of the latent embeddings we defined when we created the embeddings up here, and we defined them as having nfat size, and nfat we defined as 42. So C1, C2 and C3 represent the result of putting each character through this embedding and getting out 42 latent factors. Those are then the things we put into our green arrow. So after doing this 3 times, we now have C3 hidden, which is 1, 2, 3 here. So we now need a new set of weights. We need another dense layer, the blue arrow, so we'll call that dense out. And this needs to create an output of size 86, vocab size. We need to create something which can match to the one-part encoded list of possible characters, which is 86 long. So now that we've got this orange arrow, we can apply that to our final hidden state to get our output. So in Keras, all we need to do now is call model passing in the 3 inputs, and so the 3 inputs were returned to us way back here each time we created an embedding, we returned the input layer. So passing in the 3 inputs and passing in our output. So that's our model. So we can now compile it, set a learning rate, fit it, and as you can see, its loss is gradually decreasing. And we can then test that out very easily by creating a little function that we're going to pass 3 letters. We're going to take those 3 letters and turn them into character indices, so look them up to find the indexes, turn each of those into a NumPy array, call model.predict on those 3 arrays. That gives us 86 outputs, which we then do aardmax to find which index into those 86 is the highest. And that's the character number that we want to return. So if we pass in PHI, it thinks that L is most likely next. Space TH, it thinks E is most likely next. Space AN, it thinks that D is most likely next. So you can see that it seems to be doing a pretty reasonable job of taking 3 characters and returning a 4th character that seems pretty sensible. Not the world's most powerful model, but a good example of how we can construct pretty arbitrary architectures using Keras and then letting SGD do the work. Question on this model, how would it consider the context in which we're trying to predict the next? There's nothing about the context. All it has at any point in time is the previous 3 characters. So it's not a great model. We've got to improve it though. We've got to start somewhere. In order to answer your question, let's build this up a little further. Rather than trying to predict character 4 from the previous 3 characters, let's try and predict character N from the previous N-1 characters. And since all of these circles basically mean the same thing, which is the hidden state at this point, and since all of these orange arrows are literally the same thing, it's a dense layer with exactly the same weight matrix, let's stick all of the circles on top of each other, which means that these orange arrows can just become one arrow pointing into itself. And this is the definition of a recurrent neural network. When we see it in this form, we say that we're looking at it in its recurrent form. When we see it in this form, we can say that we're looking at it in its unrolled form or unfolded form. They're both very common. This is obviously neater, and so for quickly sketching out an RNN architecture, this is much more convenient. But actually, this unrolled form is really important. For example, when Keras uses TensorFlow as a backend, it actually always unrolls it in this way in order to compute it. That obviously takes up a lot more memory, and so it's quite nice being able to use the Theano backend with Keras, which can actually directly implement it as this kind of loop. And that's what we'll be doing today. In general, we've got the same idea. We're going to have character1 input come in, go through the first green arrow, go through the first orange arrow, and then from then on we can just say take the second character, repeat the third character, repeat. And at each time period, we're getting a new character going through a layer operation, as well as taking the previous hidden state and putting it through its layer operation. And then at the very end, we will put it through a different layer operation, the blue arrow, to get an output. So I'm going to show you this in Keras now. Question from the audience. Does every fully connected layer have to have the same activation function? In general, no. In all of the models we've seen so far, we have constructed them in a way where you can write anything you like as the activation function. In general, though, I haven't seen any examples of successful architectures which mix activation functions, other than of course that the output layer would pretty much always be a softmax for classification. I'm not sure it's not something that might become a good idea. It's just not something that anybody's done anything very successfully with so far. I will mention something important about activation functions though, which is that you can use pretty much any non-linear function as an activation function and get pretty reasonable results. There are actually some papers, some pretty cool papers people have written where they've tried all kinds of weird activation functions and they pretty much all work. So it's not something to get hung up about. Certain activation functions will train more quickly and more resiliently. In particular, ReLU and ReLU variations tend to work particularly well. So let's implement this. So we're going to use a very similar approach to what we used before. We're going to create our first RNN. We're going to create it from scratch using nothing but standard Keras dense layers. In this case, the inputs will not be, we can't create C1, C2 and C3. We're going to have to create an array of our inputs. We're going to have to decide what N we're going to use. So for this one, I've decided to use 8. So CS is characters. So I'm going to use 8 characters to predict the 9th character. So I'm going to create an array with 8 elements in it. And each element will contain a list of the 0, 8, 16, 24th character, the 1, 9, 17, etc. character, the 2, 10, 18, etc. character, just like before. So we're going to have a sequence of inputs where each one is offset by one from the previous one. And then our output will be exactly the same thing except we're going to look at the indexed across by CS, so 8. So this will be the 8th thing in each sequence. I'm going to predict it with the previous ones. So now we can go through every one of those input data items, lists, and turn them into a NumPy array. And so here you can see that we have 8 inputs and each one is of length 75,000 or so. Do the same thing for our Y, create a NumPy array out of it. And here we can visualize it. So here are the first 8 elements of X. In looking at the first 8 elements of X, let's look at the very first element of each one, 40, 42, 29. So this column is the first 8 characters of our text. And here is the 9th character. So the first thing that the model will try to do is to look at these 8 to predict this. And then look at these 8 to predict this. And indeed you can see that this list here is exactly the same as this list here. The final character of each sequence is the same as the first character of the next sequence. So it's almost exactly the same as our previous data, we've just done it in a more flexible way. We'll create 43 latent factors as before. We'll use exactly the same embedding input function as before. And again, we're just going to have to use lists to store everything. So in this case, all of our embeddings are going to be in the list. So we'll go through each of our characters and create an embedding input and output for each one. We'll store it here. And here we have, we're going to define them all at once, our green arrow, orange arrow and blue arrow. So here we're basically saying we've got 3 different weight matrices that we want Keras to keep track of for us. So the very first hidden state here is going to take the list of all of our inputs, we're going to take the first one of those, and then that's a tuple of 2 things. The first is the input to it and the second is the output of the embedding. So we're going to take the output of the embedding for the very first character, pass that into our green arrow, and that's going to give us our initial hidden state. And then this looks exactly the same as we saw before, but rather than doing it listing separately, we're just going to loop through all of our remaining 1 through 8 characters and go ahead and create the green arrow, orange arrow and add the 2 together. So finally we can take that final hidden state, put it through our blue arrow to create our final output. So we can then tell Keras that our model is all of the embedding inputs for that list to be created together, that's our inputs, and then our output that we just created is the output. And we can go ahead and fit that model. So we would expect this to be more accurate because it's now got 8 pieces of context in order to predict. So previously we were getting this time we get down to 1.8. So it's still not great, but it's an improvement. We can create exactly the same kind of test as before, so now we can pass in 8 characters and get a prediction of the 9th. And these all look pretty reasonable. So that is our first RNN that we've now built from scratch. This kind of RNN where we're taking a list and predicting a single thing is most likely to be useful for things like sentiment analysis. Remember our sentiment analysis example using IMDV? So in this case we were taking a sequence, a list of words in a sentence, and predicting whether or not something is positive sentiment or negative sentiment. So that would seem like an appropriate kind of use case for this style of RNN. So at that moment my computer crashed and we lost a little bit of the class's video. So I'm just going to fill in the bit that we missed here. Sorry for the slight discontinuity. So I wanted to show you something kind of interesting which you may have noticed which is when we created our hidden dense layer, that is our orange arrow, I did not initialize it in the default way, which is the Gloro initialization, but instead I said init equals identity. You may also have noticed that the equivalent thing was shown in our Keras RNN. This here where it says inner init equals identity is referring to the same thing. It's referring to what is the initialization that is used for this orange arrow. How are those weights originally initialized? So rather than initializing them randomly we're going to initialize them with an identity matrix. An identity matrix you may recall from your linear algebra at school is a matrix which is all zeros except it is just ones down the diagonal. So if you multiply any matrix by the identity matrix it doesn't change the original matrix at all. You get back exactly what you started with. So in other words we're going to start off by initializing our orange arrow not with a random matrix but with a matrix that causes the hidden state to not change at all. That makes some intuitive sense. It seems reasonable to say well in the absence of other knowledge to the contrary why don't we start off by having the hidden state stay the same until the SGD has a chance to update that. But it actually turns out that it also makes sense based on an empirical analysis. So since we always only do things that Jeffrey Hinton tells us to do that's good news because this is a paper by Jeff Hinton in which he points out this rather neat trick which is if you initialize an RNN with the hidden weight matrix initialized to an identity matrix and use rectified linear units as we are here you actually get an architecture which can get fantastic results on some reasonably significant problems including speech recognition and language modeling. I don't see this paper referred to or discussed very often even though it is well over a year old now so I'm not sure if people forgot about it or haven't noticed it or what but this is actually a good trick to remember is that you can often get quite a long way doing nothing but an identity matrix initialization and rectified linear units in just as we have done here to set up our architecture. Ok so that's a nice little trick to remember and so the next thing we're going to do is to make a couple of minor changes to this diagram. So the first change we're going to make is we're going to take this rectangle here so this rectangle is referring to what is it that we repeat and so since in this case we're predicting character n from characters 1 through n-1 then this whole area here we're looping from 2 to n-1 before we generate our output once again. So what we're going to do is we're going to take this triangle and we're going to put it inside the loop put it inside the rectangle and so what that means is that every time we loop through this we're going to generate another output so rather than generating one output at the end this is going to predict characters 2 through n using characters n-1 through n-1 so it's going to predict character 2 using character 1 and character 3 using characters 1 and 2 and character 4 using characters 1, 2 and 3 and so forth. And so that's what this model would do it's nearly exactly the same as the previous model except after every single step after creating the hidden state on every step we're going to create an output every time. So this is not going to create a single output like this does which predicted a single character the last character or in fact the next after the last character of the sequence character n using characters 1 through n-1 this is going to predict a whole sequence of characters 2 through n using characters 1 through n-1. Okay so that was all the stuff that we lost when we had our computer crash so let's now go back to the lesson. Let's now talk about how we would implement this sequence where we're going to predict characters 2 through n using characters 1 through n-1. Now why would this be a good idea? There's a few reasons but one obvious reason why this would be a good idea is that if we're only predicting one output for every n inputs then the number of times that our model has the opportunity to propagate those in great gradients and improve those weights is just once for each sequence of characters. If we predict characters 2 through n using characters 1 through n-1, we're actually getting a whole lot of feedback about how our model is going. So we can back propagate n-1 times every time we do another sequence. So there's a lot more learning going on for nearly the same amount of computation. The other reason this is handy is that as you'll see in a moment, it's very helpful for creating RNNs which can do truly long-term dependencies or context as one of the people asking a question earlier described it. So we're going to start here before we look at how to do context. And so really any time you're doing a sequence-to-sequence exercise, you probably want to construct something of this format where your triangle is inside the square rather than outside the square. It's going to look very, very similar. And so I'm calling this returning sequences. Rather than returning a single character, we're going to return a sequence. And really most things are the same. Our character in data is identical to before, so I've just commented it out. And now our character out, our output, isn't just a single character, but it's actually a list of 8 sequences again. In fact, it's exactly the same as the input except that I have removed the minus 1. So it's just shifted over by 1. So in each sequence, the first character will be used to predict the second, the first and second will predict the third, the first, second and third will predict the fourth, and so forth. So we've got a lot more predictions going on and therefore a lot more opportunity for the model to learn. So then we will create our y's just as before with our x's. And so now our y dataset looks exactly like our x dataset did, but everything is just shifted across by 1 character. And the model's going to look almost identical as well. We've got our 3 dense layers as before. But we're going to do one other thing different to before. Rather than treating the first character as special, I won't treat it as special. I'm going to move the character into here. So rather than repeating from 2 to n-1, I'm going to repeat from 1 to n-1. So I've moved my first character into here. So the only thing I have to be careful of is that we have to somehow initialize our hidden state to something. So we're going to initialize our hidden state to a vector of 0's. So here we do that. We say we're going to have to have something to initialize our hidden state, which we're going to feed it with a vector of 0's shortly. So our initial hidden state is just going to be the result of that. And then our loop is identical to before, but at the end of every loop, we're going to append this output. So we're now going to have 8 outputs for every sequence rather than 1. And so now our model has 2 changes. The first is it's got an array of outputs, and the second is that we have to add the thing that we're going to use to store our vector of 0's somewhere. So we're going to put this into our input as well. Question from the audience. The box refers to the area that we're looping. So initially we repeated the character n input coming into here and then the hidden state going back to itself from 2 to n-1. So the box is the thing which I'm looping through all those times. This time I'm looping through this whole thing. So character input coming in, generating the hidden state, and creating an output, repeating that whole thing every time. And so now you can see creating the output is inside the loop rather than outside the loop. So therefore we end up with an array of outputs. Our model is nearly exactly the same as before, it's just got these 2 changes. So now when we fit our model, we're going to add an array of 0's to the start of our inputs. Our outputs are going to be those lists of 8 that have been offset by 1. And we can go ahead and train this. And you can see that as we train it, we don't just have 1 loss, we have 8 losses. And that's because every one of those 8 outputs has its own loss. How are we going at predicting character 1 in each sequence? As you would expect, our ability to predict the first character using nothing but a vector of 0's is pretty limited. So that very quickly flattens out. Where else is our ability to predict the 8th character? It has a lot more context. It has 7 characters of context. And so you can see that the 8th character's loss keeps on improving. And indeed by a few epochs, we have a significantly better loss than we did before. So this is what a sequence model looks like. And so you can see a sequence model, when we test it, we pass in a sequence like this, space this is, and after every character, it returns its guess. So after seeing a space, it guesses the next will be a T. After seeing a space T, it guesses the next will be an H. After seeing space TH, it guesses the next will be an E. And so forth. And so you can see that it's predicting some pretty reasonable things here. And indeed quite often there, what actually happened. So it sees after seeing space PART, it expects that will be the end of the word. And indeed it was. And after seeing part, it's guessing that the next word is going to be of. So it's able to use sequences of 8 to create context, which isn't brilliant, but it's an improvement. So how do we do that same thing with Keras? With Keras, it's identical to our previous model, except that we have to use the different input and output arrays, just like I just showed you, so the whole sequence of labels and the whole sequence of inputs. And then the second thing we have to do is add one parameter, which is return sequences equals true. Return sequences equals true simply says, rather than putting the triangle outside the loop, put the triangle inside the loop. And so return an output from every time you go to another time step, rather than just returning a single output at the end. So it's that easy in Keras. I add this return sequences equals true. I don't have to change my data at all other than some very minor dimensionality changes. Then I can just go ahead and fit it. As you can see, I get a pretty similar loss function to what I did before. And I can build something that looks very much like we had before and generate some pretty similar results. So that's how we create a sequence model with Keras. So then the question of how do you create more state? How do you generate a model which is able to handle long-term dependencies? To generate a model that understands long-term dependencies, we can't anymore present our pieces of data at random. So far we've always been using the default when we do fit model, which is shuffle equals true. So it's passing across these sequences of 8 in a random order. If we're going to do something which understands long-term dependencies, the first thing we have to do is we're going to have to use shuffle equals false. The second thing we're going to have to do is we're going to have to stop passing in an array of zeros as my starting point every time around. So effectively, what I want to do is I want to pass in my array of zeros right at the very start when I first start training, but then at the end of my sequence of 8, rather than going back to initialize to zeros, I actually want to keep this hidden state. So then I'd start my next sequence of 8 with this hidden state exactly where it was before. And that's going to allow it to basically build up arbitrarily long dependencies. So in Keras, that's actually as simple as adding one additional parameter. And the additional parameter is called stateful. And so when you say stateful equals true, what that tells Keras is that at the end of each sequence, don't reset the hidden activation to zero, but leave them as they are. That means we have to make sure we pass shuffle equals false when we train it. So it's not going to pass the first 8 characters of the book, then the second 8 characters of the book, then the third 8 characters of the book, leaving the hidden state untouched between each one and therefore it's allowing it to continue to build up as much state as it wants to. Training these stateful models is a lot harder than training the models we've seen so far. And the reason is this. In these stateful models, this orange arrow, this single weight matrix, it's being applied to this hidden matrix not 8 times, but 100,000 times or more, depending on how big your text is. And just imagine if this weight matrix was even slightly poorly scaled, so if there was like one number in it which was just a bit too high, then effectively that number is going to be to the power of 100,000. It's being multiplied again and again and again. So what can happen is you get this problem they call exploding gradients, or really in some ways it's better described as exploding activations. Because we're multiplying this by almost the same weight matrix each time, if that weight matrix is anything less than perfectly scaled, then it's going to make our hidden matrix disappear off into infinity. And so we have to be very careful of how to train these. And indeed, these kinds of long-term dependency models were thought of as impossible to train for a while, until some folks in the mid-90s came up with a model called the LSTM, or long short-term memory. In the long short-term memory, and we'll learn more about it next week, we're actually going to implement it ourselves from scratch, we replace this loop here with a loop where there is actually a neural network inside the loop that decides how much of this state matrix to keep and how much to use at each activation. And so by having a neural network which actually controls how much state is kept and how much is used, it can actually learn how to avoid those gradient explosions. It can actually learn how to create an effective sequence. So we're going to look at that a lot more next week. But for now, I will tell you that when I tried to run this using a simple RNN, even with an identity matrix, initialization and relu's, I had no luck at all. So I had to replace it with an LSTM. Even that wasn't enough. I had to have well-scaled inputs, so I added a batch normalization layer after my embeddings. And after I did those things, then I could fit it. It still ran pretty slowly, so before I was getting 4 seconds per epoch, now it's 13 seconds per epoch. And the reason here is it's much harder to parallelize this. It has to do each sequence in order, so it's going to be slower. But over time, it does eventually get substantially better loss than I had before, and that's because it's able to keep track of and use this state. Question. Doesn't it make sense to use batch norm in the loop as well? That's a good question. Definitely maybe. There's been a lot of discussion and papers about this recently. There's something called layer normalization, which is a method which is explicitly designed to work well with RNNs. Standard batch norm doesn't. It turns out it's actually very easy to do layer normalization with Keras using a couple of simple parameters you can provide to the normal batch norm constructor. In my experiments, that hasn't worked so well, and I will show you a lot more about that in just a few minutes. Stakeful models are great. We're going to look at some very successful stateful models in just a moment, but just be aware that they are more challenging to train. You'll see another thing I had to do here is I had to reduce the learning rate in the middle, again, because you just have to be so careful of these exploding gradient problems. Let me show you what I did with this. Which is I tried to create a stateful model which worked as well as I could. So I took the same Nietzsche data as before, and I tried splitting it into chunks of 40 rather than 8, so each one could do more work. So here are some examples of those chunks of 40. I built a model that was slightly more sophisticated than the previous one in 2 ways. The first is it has an RNN feeding into an RNN. That's kind of a crazy idea, so I've drawn a picture. So an RNN feeding into an RNN means that the output is no longer going to an output. It's actually the output of the first RNN is becoming the input to the second RNN. So the character input goes into our first RNN and has the state updates as per usual, and then each time we go through the sequence, it feeds the result to the state of the second RNN. Why is this useful? Well, because it means that this output is now coming from not just a single dense matrix with a single dense matrix here, it's actually going through 1, 2, 3 dense matrices and activation functions. So I now have a deep neural network, assuming that 2 layers get to count as deep, between my first character and my first output. And then indeed between every hidden state and every output, I now have multiple hidden layers. So effectively what this is allowing us to do is to create a little deep neural net for all of our activations. And that turns out to work really well because the structure of language is pretty complex and so it's nice to be able to give it a more flexible function that it can learn. So that's the first thing I do. And it's this easy to create that. You just copy and paste whatever your RNN line is twice. You can see I've now added dropout inside my RNN. And as I talked about before, adding dropout inside your RNN turns out to be a really good idea and there's a really great paper about that quite recently showing that this is a great way to regularize an RNN. And then the second change I made is rather than going straight from the RNN to our output, I went through a dense layer. Now there's something that you might have noticed here is that our dense layers have this extra word at the front. Why do they have this extra word at the front? Time distributed. It might be easier to understand why by looking at this earlier sequence model with Keras. And note that the output of our RNN is not just a vector of length 256, but 8 vectors of length 256 because it's actually predicting 8 outputs. So we can't just have a normal dense layer because a normal dense layer needs a single dimension that it can squish down. So in this case, what we actually want to do is we want to create 8 separate dense layers at the output, one for every one of the outputs. And so what time distributed does is it says whatever the layer is in the middle, I want you to create 8 copies of them, or however long this dimension is. And every one of those copies is going to share the same weight matrix, which is exactly what we want. So the short version here is in Keras, any time you say return sequences equals true, any dense layers you have after that will always have to have time distributed wrapped around them because we want to create not just one dense layer, but 8 dense layers. So in this case, since we're saying return sequences equals true, we then have a time distributed dense layer, some dropout, and another time distributed dense layer. Question- Does the first RNN complete before it passes to the second, or is it layer by layer? Answer- No, it's operating exactly like this. So my initialization starts, my first character comes in, and at the output of that comes two things, the hidden state for my next hidden state and the output that goes into my second LSTN. So everything is just pushed through. The best way to think of this is to actually draw it in the unrolled form, and then you'll realize there's nothing magical about this at all. In an unrolled form, it just looks like a pretty standard deep neural net. Question- What's dropout underscore u and dropout underscore w? Answer- We'll talk about that more next week. In an LSTN, I mentioned that there's little neural nets that control how the state updates work, and so this is talking about how the dropout works inside these little neural nets. Question- When stateful is false, can you explain again what is reset after each training example? Answer- Sure. The best way to describe that is to show us doing it. So remember that the RNNs that we built are identical to what Keras does, or close enough to identical. So let's go and have a look at our version of return sequences. So you can see that what we did was we created a matrix of zeros that we stuck onto the front of our inputs. So every set of 8 characters now starts with a vector of zeros. So in other words, this initialized to zeros happens every time we finish a sequence. So in other words, this hidden state gets initialized to zero at the end of every sequence. And it's this hidden state which is where all of these dependencies and state is kept. So doing that is resetting the state every time we look at a new sequence. So when we say stateful equals false, it only does this initialized to zero step once at the very start, or when we explicitly ask it to. And so when I actually run this model, the way I do it is I wrote a little thing called runepox that goes model.resetStates and then does a fit on one epoch, which is what you really want at the end of your entire works of Nietzsche. You want to reset the state because you're about to go back to the very start and start again. So with this multilayer LSTM going into a multilayer neural net, I then tried seeing how that goes. And remember that with our simpler versions, we were getting 1.6 loss was the best we could do. After one epoch, it's awful. And now rather than just printing out one letter, I'm starting with a whole sequence of letters, which is that, and asking it to generate a sequence. You can see it starts out by generating a pretty rubbishy sequence. Question from the audience. In the double LSTM layer model, what is the input to the second LSTM in addition to the output of the first LSTM? In addition to the output of the first LSTM is the previous output of its own hidden state. So after a few more epochs, it's starting to create some actual proper English words, although the English words aren't necessarily making a lot of sense. So keep running epochs. At this point it's learned how to start chapters. This is actually how in this book the chapters always start with a number and then an equals sign. It hasn't learned how to close quotes, apparently. It's not really saying anything useful. So anyway, I kind of ran this overnight, and I then seeded it with a larger amount of data. I started getting some pretty reasonable results. Shreds into one's own suffering sounds exactly like the kind of thing you might see. Religions have acts done by man. It's not all perfect, but it's not bad. Interestingly, this sequence here, when I looked it up, it actually appears in his book. This makes sense, right? It's a kind of overfitting in a sense. He loves talking in all caps, but he only does it from time to time. So once it so happened to start writing something in all caps that looked like this phrase, that only appeared once and is very unique, there was no other way that it could have finished it. So sometimes you get these little rare phrases that basically it's plagiarized directly from nature. Now, I didn't stop there because I thought how can we improve this. It was at this point that I started thinking about batch normalization. I started fiddling around with a lot of different types of batch normalization and layer normalization and discovered this interesting insight, which is that, at least in this case, the very best approach was when I simply applied batch normalization to the embedding layer. So I want to show you what happened. When I applied batch normalization to the embedding layer, this is the training curve that I got. With the epochs, this is my loss. With no batch normalization on the embedding layer, this was my loss. And so you can see this was actually starting to flatten out. This one really wasn't. And this one was training a lot quicker. So then I tried training it with batch norm on the embedding layer overnight, and I was pretty stunned by the results. This was my seeding text, and after a thousand epochs, this is what it came up with. And it's got all kinds of actually pretty interesting little things. Perhaps some morality equals self-glorification. This is really cool. For there are holy eyes to Schopenhauer's blind, you can see that it's learned to close quotes even when those quotes were opened a long time ago. So if we weren't using stateful, it would never have learned how to do this. I've looked up these words in the original text, and pretty much none of these phrases appear. This is actually a genuine, novel, produced piece of text. It's not perfect by any means, but considering that this is only doing it character by character, using nothing but a 42 long embedding matrix for each character, and nothing but there's no pre-trained vectors or anything, there's just a pretty short 600,000 character epoch, I think it's done a pretty amazing job of creating a pretty good model. So there's all kinds of things you could do with a model like this. The most obvious one would be if you were producing a software keyboard for a mobile phone, for example, you could use this to have a pretty accurate guess as to what they were going to type next and correct it for them. You could do something similar on a word basis. But more generally, you could do something like anomaly detection with this. You could generate a sequence that is predicting what the rest of the sequence is going to look like for the next hour and then recognize if something falls outside of what your prediction was and then you know that there's been some kind of anomaly. There's all kinds of things you can do with these kinds of models. I think that's pretty fun. I want to show you something else which is pretty fun, which is to build an RNN from scratch in Theano. What we're going to do is we're going to try and work up to next week where we're going to build an RNN from scratch in NumPy. And we're also going to build an LSTM from scratch in Theano. And the reason we're doing this is because next week's our last class in this part of the course. I want us to leave with feeling like we really understand the details of what's going on behind the scenes. The main thing I wanted to teach in this class is the applied stuff, these practical tips about how you build a sequence model. Use return equals true, put batch normal in the embedding layer, add time distributed to the dense layer. But I also know that to really debug your models and to build your architectures and stuff, it really helps to understand what's going on. Particularly in the current situation where the tools and libraries available are not that mature, they still require a whole lot of manual stuff. So I do want to try and explain a bit more about what's going on behind the scenes. In order to build an RNN in Theano, I'm going to first of all make a small change to our Keras model, which is that I'm going to use one-hot encoding. So I don't know if you noticed this, but we did something pretty cool in all of our models so far, which is that we never actually one-hot encoded our output. Question from the audience. So if you don't add time distributed dense to a model where return sequences equals true, it literally won't work, it won't compile. Because you're trying to predict 8 things and the dense layer is going to stick that all into one thing, so it's going to say there's a mismatch in your dimensions. But no, it doesn't really add much time because that's something that can be very easily parallelized. And since a lot of things in RNNs can't be easily parallelized, there generally is plenty of room in your GPU to do more work. So that should be fine. The short answer is you have to use it, otherwise it won't work. I wanted to point out something, which is that in all of our models so far, we did not one-hot encode our outputs. So our outputs, remember, looked like this. They were sequences of numbers. And so always before, we've had to one-hot encode our outputs to use them. It turns out that Keras has a very cool loss function called sparse categorical cross-entropy. And this is identical to categorical cross-entropy, but rather than taking a one-hot encoded target, it takes an integer target and basically it acts as if you had one-hot encoded it. So it basically does the indexing into it directly. So this is a really helpful thing to know about because when you have a lot of output categories, like for example, if you're doing a word model, you could have 100,000 output categories. There's no way you want to create a matrix that is 100,000 long. Only all zeros for every single word in your output. So by using sparse categorical cross-entropy, you can just forget the whole one-hot encoding. You don't have to do it. Keras implicitly does it for you, but without ever actually explicitly doing it, it just does a direct lookup into the matrix. However, because I want to make things simpler for us to understand, I'm going to go ahead and recreate our Keras model using one-hot encoding. And so I'm going to take exactly the same model that we had before with return sequences equals true, but this time I'm going to use normal categorical cross-entropy. The other thing I'm doing is I don't have an embedding layer. So since I don't have an embedding layer, I also have to one-hot encode my inputs. So you can see I'm calling 2 categorical on all of my inputs and 2 categorical on all of my outputs. So now the shape is 75,000 by 8, as before, by 86. So this is the one-hot encoding dimension. There are 85 zeros and 1 one. So we fit this in exactly the same way, we get exactly the same answer. So the only reason I was doing that was because I want to use one-hot encoding for the version that we're going to create ourselves from scratch. So we haven't really looked at Theano before. But particularly if you come back next year, as we start to try to add more and more stuff on top of Keras or into Keras, increasingly you'll find yourself wanting to use Theano. Because Theano is the language that Keras is using behind the scenes and therefore it's kind of the language which you can use to extend it. Of course you can use TensorFlow as well, but we're using Theano in this course because I think it's much easier for this kind of application. So let's learn to use Theano. In the process of doing it in Theano, we're going to have to force ourselves to think through a lot more of the details than we have before. Because Theano doesn't have any of the conveniences that Keras has. There's no such thing as a layer. We have to think about all of the weight matrices and activation functions and everything ourselves. So let me show you how it works. In Theano, there's this concept of a variable. And a variable is something which we basically define like so. We can say there is a variable which is a matrix which I will call tInput. And there is a variable which is a matrix that we'll call tOutput. And there is a variable that is a vector that we will call h0. Now what these are all saying is that these are things that we will give values to later. Programming in Theano is very different to programming in normal Python. The reason for this is Theano's job in life is to provide a way for you to describe a computation that you want to do, and then it's going to compile it for the GPU. And then it's going to run it on the GPU. So it's going to be a little more complex to work in Theano. Because Theano isn't going to be something where we immediately say do this and then do this and then do this. Instead we're going to build up what's called a computation graph. It's going to be a series of steps. We're going to say in the future, I'm going to give you some data, and when I do, I want you to do these steps. So rather than actually starting off by giving it data, we start off by just describing the types of data that when we do give it data, we're going to give it. So eventually we're going to give it some input data, we're going to give it some output data, and we're going to give it some way of initializing the first hidden state. Oh, and also we'll give it a learning rate because we might want to change it later. So that's all these things do. They create Theano variables. So then we can create a list of those, and this is all of the arguments that we're going to have to provide to Theano later on. So there's no data here, nothing's being computed, we're just telling Theano that these things are going to be used in the future. The next thing that we need to do is we're going to try to build this, is we're going to have to build all of the pieces in all of these layer operations. So specifically we're going to have to create the weight vector and bias matrix for the orange arrow, the weight matrix and the bias vector for the green arrow, the weight matrix and the bias vector for the green arrow, and the weight matrix and the bias vector for the blue arrow. That's what these layer operations are. They're a matrix multiply followed by a nonlinear activation function. So I've created some functions to do that. So WH is what I'm going to call the weights and bias to my hidden layer. WX will be my weights and bias to my input, and WY will be my weights and bias to my output. And so to create them, I've created this little function called weights and bias, in which I tell it the size of the matrix that I want to create. So the matrix that goes from input to hidden therefore has n input rows and n hidden columns. So weights and bias is here, and it's going to return a tuple, it's going to return our weights, and it's going to return our bias. So how do we create the weights? To create the weights, we first of all calculate the magic Loro number, the square root of 2 over fan in, so that's the scale of the random numbers that we're going to use. We then create those random numbers using the NumPyNormalRandomNumber function. And then we use a special Theano keyword called shared. And what shared does is it says to Theano, this data is something that I'm going to want you to pass off to the GPU later and keep track of. So as soon as you wrap something in shared, it kind of belongs to Theano now. So here is a weight matrix that belongs to Theano. Here is a vector of zeros that belongs to Theano, and that's our initial bias. So we've initialized our weights and our bias, so we can do that for our inputs and we can do that for our outputs. And then for our hidden, which is the orange arrow, we're going to do something slightly different, which is we will initialize it using an identity matrix. And rather amusingly in NumPy, it is I for identity. So this is an identity matrix, believe it or not, of size n by n. And so that's our initial weights, and our initial bias is exactly as plot. It's a vector of zeros. So you can see we've had to manually construct each of these 3 weight matrices and bias vectors. It's nice to now stick them all into a single list, and Python has this thing called chain from iterable, which basically takes all of these tuples and dumps them all together into a single list. So this now has all 6 weight matrices and bias vectors in a single list. Now we have defined the initial contents of each of these arrows, and we've also defined kind of symbolically the concept that we're going to have something to initialize it with here, something to initialize it with here, and some target to initialize it with here. So the next thing we have to do is to tell Theano what happens each time we take a single step of this RNN. On the GPU, you can't use a for loop. The reason you can't use a for loop is because the GPU wants to be able to parallelize things and wants to do things at the same time. And a for loop by definition can't do the second part of the loop until it's done the first part of the loop. I don't know if we'll get time to do it in this course or not, but there's a very neat result which shows that there's something very similar to a for loop that you can parallelize, and it's called a scan operation. A scan operation is something that's defined in a very particular way. A scan operation is something where you call some function for every element of some sequence. And at every point, the function returns some output, and the next time through that function is called, it's going to get the output of the previous time you called it along with the next element of the sequence. So in fact, I've got an example of it. I actually wrote a very simple example of it in Python. Here is the definition of scan. Here is an example of scan. Let's start with the example. I want to do a scan and the function I'm going to use is to add two things together. And I'm going to start off with the number 0, and then I'm going to pass in a range of numbers from 0 to 4. So what scan does is it starts out by taking the first time through, it's going to call this function with that argument and the first element of this. So it's going to be 0 plus 0 equals 0. The second time, it's going to call this function with the second element of this, along with the result of the previous call. So it will be 0 plus 1 equals 1. The next time through, it's going to call this function with the result of the previous call plus the next element of this range. So it will be 1 plus 2 equals 3. So you can see here this scan operation defines a cumulative sum. So you can see the definition of scan here. We're going to be returning an array of results. Initially we take our starting point, 0, and that's our initial value for the previous answer from scan. And then we're going to go through everything in the sequence, 0 through 4. We're going to apply this function, which in this case was addThingsUp, and we're going to apply it to the previous result along with the next element of the sequence. Stick the result at the end of our list, set the previous result to whatever we just got, and then go back to the next element of the sequence. So it may be very surprising, hopefully it is very surprising because it's an extraordinary work, but it is possible to write a parallel version of this. So if you can turn your algorithm into a scan, you can run it quickly on GPU. So what we're going to do is our job is to turn this RNN into something that we can put into this kind of format, into a scan. So let's do that. So the function that we're going to call on each step through is the function called step. And the function called step is going to be something which hopefully will not be very surprising to you. It's going to be something which takes our input, X, it does a dot product by that weight matrix we created earlier, WX, and adds on that bias vector we created earlier. And then we do the same thing, taking our previous hidden state, multiplying it by the weight matrix for the hidden state, and adding the biases for the hidden state. And then puts the whole thing through an activation function, relu. So in other words, that was calculating these. So we had one bit which was calculating our previous hidden state and putting it through the hidden state weight matrix, which is an orange arrow. It was taking our next input and putting it through the input one and then adding the two together. So that's what we have here. The X by WX and the H by WH, and then adding the two together along with the biases, and then put that through an activation function. So once we've done that, we now want to create an output every single time. And so our output is going to be exactly the same thing. It's going to take the result of that, which we call H, our hidden state, multiply it by the output's weight vector, adding on the bias, and this time we're going to use soft maths. So you can see that this sequence here is describing how to do one of these things. And so this therefore defines what we want to do each step through. And at the end of that, we're going to return the hidden state we have so far and our output. So that's what's going to happen each step. So the sequence that we're going to pass into it, we're not going to give it any data yet. Remember, all we're doing is describing a computation. So for now, we're just telling it that it will be a matrix. So we're saying it will be a matrix. We're going to pass you a matrix. It also needs a starting point. And so the starting point is, again, we are going to provide to you an initial value for our hidden state, but we haven't done it yet. And then finally, in Theano, you have to tell it what are all of the other things that are passed to the function, and we're going to pass it that whole list of weights. So that's why we have here the X, the hidden, and then all of the weights and biases. So that's now described how to execute a whole sequence of steps for an RNN. So we've now described how to do this to Theano. We haven't given it any data to do it. We've just set up the computation. And so when that computation is run, it's going to return two things, because step returned two things. It's going to return the hidden state, and it's going to return our output activations. So now we need to calculate our error. So our error will be the categorical cross-entropy. So these things are all part of Theano. You can see I'm using some Theano functions here. And so we're going to compare the output that came out of our scan, and we're going to compare it to what we don't know yet, but it will be a matrix. And then once you do that, add it all together. Now here's the amazing thing. Every step, we're going to want to apply SGD, which means every step, we're going to want to take the derivative of this whole thing with respect to all of the weights and use that along with the learning rate to update all of the weights. In Theano, that's how you do it. You just say, please tell me the gradient of this function with respect to these inputs. And Theano will symbolically automatically calculate all of the derivatives for you. So that's very nearly magic. But we don't have to worry about derivatives because it's going to calculate them all for us. So at this point, I now have a function that calculates our loss, and I have a function that calculates all of the gradients that we need with respect to all of the different weights and parameters that we have. So we're now ready to build our final function. And so our final function as input takes all of our arguments, that is these 4 things, which are the things we told it we're going to need later. The thing that it's going to create as an output is the error, which was this output. And at each step, it's going to do some updates. And so it's going to update, what are the updates it's going to do? The updates it's going to do is the result of this little function, and this little function is something that creates a dictionary that is going to map every one of our weights to that weight minus each one of our gradients times the learning rate. So it's going to update every weight to itself minus its gradient times the learning rate. So basically what Theano does is it says, it's got this little thing called updates, it says every time you calculate the next step, I want you to change your shared variables as bollards. So there's our list of changes to make. And so that's it. So we use our one hot encoded x's and our one hot encoded y's, and we have to now manually create our own loop. Theano doesn't have any built-in stuff for us, so we're going to go through every element of our input and we're going to say, let's call that function, so that function is the function that we just created, and now we have to pass in all of these inputs. So we have to finally pass in a value for the initial hidden state, the input, the target, and the learning rate. So this is where we get to do it, when we finally call it here. So here's our initial hidden state, just a bunch of zeros, our input, our output, and our learning rate, which we set to.01. And then I've just set it to something here that says, okay, every 1000 times, print out the error. And so as you can see, over time, it learns. And so at the end of learning, I create a new Theano function which takes some piece of input along with some initial hidden state, and it produces not the loss, but the output. Are we using gradient descent and not stochastic gradient descent here? We're using stochastic gradient descent with a mini-batch size of 1. So gradient descent without stochastic actually means you're using a mini-batch size of the whole data set. So this is kind of the opposite of that. I think this is called online gradient descent. So remember earlier on, we had this thing to calculate the vector of outputs. So now to do our testing, we're going to create a new function which goes from our input to our vector of outputs. And so our predictions will be to take that function, pass it in our initial hidden state, and some input, and that's going to give us some predictions. So if we call it, we can now see, let's now grab some sequence of text, pass it to our function to get some predictions, and let's see what it does. So after t, it expected h, after t, h, it expected e, after t, h, e, it expected space, it actually really wanted space after t, h, e, n, it expected space, after t, h, e, n, it expected space, after t, h, e, n, it expected space, and so forth. So you can see here that we have successfully built an RNN from scratch using Theano. That's been a very, very quick run-through. My goal really tonight is to get to a point where you can start to look at this during the week and see all the pieces. Because next week, we're going to try and build an LSTM in Theano, which is going to mean that I want you by next week to start to feel like you've got a good understanding of what's going on. So of course, please ask lots of questions on the forum, look at the documentation, and so forth. The next thing we're going to do after that is we're going to build an RNN without using Theano. We're going to use pure NumPy, and that means that we're not going to be able to use t.grad. We're going to have to calculate the gradients by hand. So hopefully that will be a useful exercise in really understanding what's going on in that propagation. So I want to make sure you feel like you've got enough information to get started with looking at Theano this week. So did anybody want to ask any questions about this piece so far? Question- So this is maybe a bit too far away from what we did today, but how would you apply an RNN to images, something else than text? Is that something that's worth doing, and if so, what changes about it? Answer- So the main way in which an RNN is applied to images is what we looked at last week, which is these things called attentional models, which is where you basically say, given which part of the image you're currently looking at, which part would make sense to look at next. This is most useful on really big images where you can't really look at the whole thing at once because it would just eat up all your GPU's RAM, so you can only look at it a little bit at a time. Another way that RNNs are very useful for images is for captioning images. And so we'll talk a lot more about this in the next year's course, but have a think about this in the meantime. If we've got an image, then a CNN can turn that into a vector representation of that image. For example, we could chuck it through VGG and take the penultimate layer's activations. There's all kinds of things we could do, but in some way we can turn an image and turn it into some vector representation of that. We could do the same thing to a sentence. We can take a sentence consisting of a number of words and we can stick that through a RNN and at the end of it, we will get some state. And that state is also just a vector. What we could then do is learn a neural network which maps the picture to the text, assuming that this sentence was actually originally a caption that had been created for this image. And so in that way, if we can learn a mapping from some representation of the image that came out of a CNN to some representation of a sentence which came out of an RNN, then we could basically reverse that in order to generate captions for an image. So basically what we could then do is we could take some new image that we've never seen before, chuck it through the CNN to get our state out, and then we could figure out what RNN state we would expect would be attached to that based on this neural net that we had learned, and then we can basically do a sequence generation just like we have been today and generate a sequence of words. And this is roughly how these image captioning systems work. So finally, the only other way in which I've seen RNNs applied to images is for really big 3D images, for example, like in medical imaging. So if you've got some MRI that's basically a series of labels, it's too big to look at the whole thing. Instead you can use an RNN to start in the top corner and then look one pixel to the left, then one pixel across, then one pixel back, and then it can go down into the next layer. It can gradually look one pixel at a time. And it can do that and gradually cover the whole thing. And in that way, it's gradually able to generate state about what is contained in this 3D volume. And so this is not something which is very widely used, at least at this point, but I think it's worth thinking about. Because again, you could combine this with a CNN. Maybe you could have a CNN that looks at large chunks of this MRI at a time and generate state for each of these chunks, and then maybe you could use an RNN to go through the chunks. There's all kinds of ways basically that you can combine CNNs and RNNs together. Question- Can you build a custom layer in the Anno and then mix it with Keras? Answer- Oh for sure. In fact, it's incredibly easy. So if you go Keras, custom layer, there's lots of examples of them that you'll generally find. They're generally in the GitHub issues for Keras, where people will show, I was trying to build this layer and I had this problem, but it's kind of a good way to see how to build them. The other thing I find really useful to do is to actually look at the definition of the layers in Keras. So one of the things I actually did was I created this little thing called PyPath which allows me to put in any Python module and it returns the directory that that module is defined in. And then so I can go, let's have a look at how any particular layer is defined. So let's say I want to look at pooling. So here is a maxPooling1D layer and you can see it's defined in 9 lines of code. And so generally speaking, you can kind of see that layers don't take very much code at all. Question from the audience. Question from the audience. Could we, given a caption, create an image? You can absolutely create an image from a caption. There's a lot of image generation stuff going on at the moment. It's not at a point that it's probably useful for anything in practice. It's more like an interesting research journey I guess. So generally speaking, this is in the area called generative models. And we'll be looking at generative models next year because they're very important for unsupervised and semi-supervised learning. And what could get the best performance on a document classification task? CNN and RNN or both? That's a great question. So let's go back to sentiment analysis. And to remind ourselves, when we looked at sentiment analysis for IMDB, the best result we got came from a multi-size convolutional neural network where we basically took a bunch of convolutional neural networks of varying sizes. A simple convolutional neural network was nearly as good. I actually tried an LSTM for this, and I found the accuracy that I got was less good than the accuracy of the CNN. And I think the reason for this is that when you have a whole movie review, which is a few paragraphs, the information you can get just by looking at a few words at a time is enough to tell you whether this is a positive review or a negative review. If you see a sequence of 5 words like, this is totally shit, you can probably learn that's not a good thing, or else if this is totally awesome, you can probably learn that is a good thing. The amount of nuance built into reading sentence word by word an entire review, it just doesn't seem like there's any need for that in practice. So in general, once you get to a certain sized piece of text, like a paragraph or two, there doesn't seem to be any sign that RNNs are helpful, at least at this stage. So before I close off, I wanted to show you two little tricks, because I don't spend enough time showing you cool little tricks. So when I was working with Brad today, there were two little tricks that we realized that other people might like to learn about. The first trick I wanted to point out to you is, if you want to learn about how a function works, what would be a quick way to find out. And if you've got a function there on your screen and you hit Shift-Tab, all of the parameters to it will pop up. If you hit Shift-Tab twice, the documentation will pop up. So that was one little tip that I wanted you guys to know about, because I think it's pretty handy. The second little tip that you may not be aware of is that you can actually run the Python debugger inside Jupyter Notebook. And so today we were trying to do that when we were trying to debug our pure Python RNN. So we can see an example of that. So let's say we were having some problem inside our loop here. You can go import pdb, that's the Python debugger, and then you can set a breakpoint anywhere. You can go pdb.setTrace, that's the breakpoint. And so now if I run that, as soon as it gets to here, it pops up a little dialog box. And at this point I can look at anything. So for example, I can say, what's the value of err at this point? And I can say, what are the lines I'm about to execute? And I can say, okay, execute the next one line. If you want to learn about the Python debugger, just Google for a Python debugger. But learning to use the debugger is one of the most helpful things because it lets you step through each step of what's going on and see the values of all of your variables and do all kinds of cool stuff like that. So those were two little tips I thought I would leave you with. So we can finish on a high note. And that's 9 o'clock. Thanks very much, everybody.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.12, "text": " So we talked about pseudo-labeling a couple of weeks ago, and this is this way of dealing", "tokens": [407, 321, 2825, 466, 35899, 12, 44990, 11031, 257, 1916, 295, 3259, 2057, 11, 293, 341, 307, 341, 636, 295, 6260], "temperature": 0.0, "avg_logprob": -0.22091299422243807, "compression_ratio": 1.6244541484716157, "no_speech_prob": 0.0074572074227035046}, {"id": 1, "seek": 0, "start": 7.12, "end": 9.08, "text": " with semi-supervised learning.", "tokens": [365, 12909, 12, 48172, 24420, 2539, 13], "temperature": 0.0, "avg_logprob": -0.22091299422243807, "compression_ratio": 1.6244541484716157, "no_speech_prob": 0.0074572074227035046}, {"id": 2, "seek": 0, "start": 9.08, "end": 16.240000000000002, "text": " So remember how in the state farm competition we had far more unlabeled images in the test", "tokens": [407, 1604, 577, 294, 264, 1785, 5421, 6211, 321, 632, 1400, 544, 32118, 18657, 292, 5267, 294, 264, 1500], "temperature": 0.0, "avg_logprob": -0.22091299422243807, "compression_ratio": 1.6244541484716157, "no_speech_prob": 0.0074572074227035046}, {"id": 3, "seek": 0, "start": 16.240000000000002, "end": 19.12, "text": " set than we had in the training set?", "tokens": [992, 813, 321, 632, 294, 264, 3097, 992, 30], "temperature": 0.0, "avg_logprob": -0.22091299422243807, "compression_ratio": 1.6244541484716157, "no_speech_prob": 0.0074572074227035046}, {"id": 4, "seek": 0, "start": 19.12, "end": 23.240000000000002, "text": " And so the question was how do we take advantage of knowing something about the structure even", "tokens": [400, 370, 264, 1168, 390, 577, 360, 321, 747, 5002, 295, 5276, 746, 466, 264, 3877, 754], "temperature": 0.0, "avg_logprob": -0.22091299422243807, "compression_ratio": 1.6244541484716157, "no_speech_prob": 0.0074572074227035046}, {"id": 5, "seek": 0, "start": 23.240000000000002, "end": 26.080000000000002, "text": " though we don't have labels?", "tokens": [1673, 321, 500, 380, 362, 16949, 30], "temperature": 0.0, "avg_logprob": -0.22091299422243807, "compression_ratio": 1.6244541484716157, "no_speech_prob": 0.0074572074227035046}, {"id": 6, "seek": 2608, "start": 26.08, "end": 30.459999999999997, "text": " We learned this crazy technique called pseudo-labeling, or a combination of pseudo-labeling and knowledge", "tokens": [492, 3264, 341, 3219, 6532, 1219, 35899, 12, 44990, 11031, 11, 420, 257, 6562, 295, 35899, 12, 44990, 11031, 293, 3601], "temperature": 0.0, "avg_logprob": -0.12326026916503906, "compression_ratio": 1.6653543307086613, "no_speech_prob": 2.178168506361544e-05}, {"id": 7, "seek": 2608, "start": 30.459999999999997, "end": 39.64, "text": " distillation, which is where you predict the outputs of the test set and then you act as", "tokens": [42923, 399, 11, 597, 307, 689, 291, 6069, 264, 23930, 295, 264, 1500, 992, 293, 550, 291, 605, 382], "temperature": 0.0, "avg_logprob": -0.12326026916503906, "compression_ratio": 1.6653543307086613, "no_speech_prob": 2.178168506361544e-05}, {"id": 8, "seek": 2608, "start": 39.64, "end": 45.4, "text": " if those outputs were true labels and you kind of add them in to your training.", "tokens": [498, 729, 23930, 645, 2074, 16949, 293, 291, 733, 295, 909, 552, 294, 281, 428, 3097, 13], "temperature": 0.0, "avg_logprob": -0.12326026916503906, "compression_ratio": 1.6653543307086613, "no_speech_prob": 2.178168506361544e-05}, {"id": 9, "seek": 2608, "start": 45.4, "end": 50.099999999999994, "text": " And the reason I wasn't able to actually implement that and see how it works was because we needed", "tokens": [400, 264, 1778, 286, 2067, 380, 1075, 281, 767, 4445, 300, 293, 536, 577, 309, 1985, 390, 570, 321, 2978], "temperature": 0.0, "avg_logprob": -0.12326026916503906, "compression_ratio": 1.6653543307086613, "no_speech_prob": 2.178168506361544e-05}, {"id": 10, "seek": 2608, "start": 50.099999999999994, "end": 54.36, "text": " a way of combining two different sets of batches.", "tokens": [257, 636, 295, 21928, 732, 819, 6352, 295, 15245, 279, 13], "temperature": 0.0, "avg_logprob": -0.12326026916503906, "compression_ratio": 1.6653543307086613, "no_speech_prob": 2.178168506361544e-05}, {"id": 11, "seek": 5436, "start": 54.36, "end": 61.88, "text": " And in particular, I think the advice I saw from Jeff Hinton when he wrote about pseudo-labeling", "tokens": [400, 294, 1729, 11, 286, 519, 264, 5192, 286, 1866, 490, 7506, 389, 12442, 562, 415, 4114, 466, 35899, 12, 44990, 11031], "temperature": 0.0, "avg_logprob": -0.12183809280395508, "compression_ratio": 1.6037735849056605, "no_speech_prob": 4.399931276566349e-05}, {"id": 12, "seek": 5436, "start": 61.88, "end": 68.76, "text": " is that you want something like 1 in 3 or 1 in 4 of your training data to come from", "tokens": [307, 300, 291, 528, 746, 411, 502, 294, 805, 420, 502, 294, 1017, 295, 428, 3097, 1412, 281, 808, 490], "temperature": 0.0, "avg_logprob": -0.12183809280395508, "compression_ratio": 1.6037735849056605, "no_speech_prob": 4.399931276566349e-05}, {"id": 13, "seek": 5436, "start": 68.76, "end": 73.78, "text": " the pseudo-labeled data and the rest to come from your real data.", "tokens": [264, 35899, 12, 75, 18657, 292, 1412, 293, 264, 1472, 281, 808, 490, 428, 957, 1412, 13], "temperature": 0.0, "avg_logprob": -0.12183809280395508, "compression_ratio": 1.6037735849056605, "no_speech_prob": 4.399931276566349e-05}, {"id": 14, "seek": 5436, "start": 73.78, "end": 81.32, "text": " So the good news is I built that thing and it was ridiculously easy.", "tokens": [407, 264, 665, 2583, 307, 286, 3094, 300, 551, 293, 309, 390, 41358, 1858, 13], "temperature": 0.0, "avg_logprob": -0.12183809280395508, "compression_ratio": 1.6037735849056605, "no_speech_prob": 4.399931276566349e-05}, {"id": 15, "seek": 5436, "start": 81.32, "end": 83.52, "text": " This is the entire code.", "tokens": [639, 307, 264, 2302, 3089, 13], "temperature": 0.0, "avg_logprob": -0.12183809280395508, "compression_ratio": 1.6037735849056605, "no_speech_prob": 4.399931276566349e-05}, {"id": 16, "seek": 8352, "start": 83.52, "end": 91.03999999999999, "text": " I call it the mix iterator and it will be in our utils class from tomorrow.", "tokens": [286, 818, 309, 264, 2890, 17138, 1639, 293, 309, 486, 312, 294, 527, 2839, 4174, 1508, 490, 4153, 13], "temperature": 0.0, "avg_logprob": -0.14675831240276957, "compression_ratio": 1.674757281553398, "no_speech_prob": 3.785294211411383e-06}, {"id": 17, "seek": 8352, "start": 91.03999999999999, "end": 96.75999999999999, "text": " And all it does is it's something where you create whatever generators of batches you", "tokens": [400, 439, 309, 775, 307, 309, 311, 746, 689, 291, 1884, 2035, 38662, 295, 15245, 279, 291], "temperature": 0.0, "avg_logprob": -0.14675831240276957, "compression_ratio": 1.674757281553398, "no_speech_prob": 3.785294211411383e-06}, {"id": 18, "seek": 8352, "start": 96.75999999999999, "end": 106.32, "text": " like and then you pass an array of those iterators to this constructor and then every time the", "tokens": [411, 293, 550, 291, 1320, 364, 10225, 295, 729, 17138, 3391, 281, 341, 47479, 293, 550, 633, 565, 264], "temperature": 0.0, "avg_logprob": -0.14675831240276957, "compression_ratio": 1.674757281553398, "no_speech_prob": 3.785294211411383e-06}, {"id": 19, "seek": 8352, "start": 106.32, "end": 112.82, "text": " Keras system calls next on it, it grabs the next batch from all of those sets of batches", "tokens": [591, 6985, 1185, 5498, 958, 322, 309, 11, 309, 30028, 264, 958, 15245, 490, 439, 295, 729, 6352, 295, 15245, 279], "temperature": 0.0, "avg_logprob": -0.14675831240276957, "compression_ratio": 1.674757281553398, "no_speech_prob": 3.785294211411383e-06}, {"id": 20, "seek": 11282, "start": 112.82, "end": 115.24, "text": " and concatenates them all together.", "tokens": [293, 1588, 7186, 1024, 552, 439, 1214, 13], "temperature": 0.0, "avg_logprob": -0.19496212005615235, "compression_ratio": 1.4714285714285715, "no_speech_prob": 8.801021067483816e-06}, {"id": 21, "seek": 11282, "start": 115.24, "end": 120.0, "text": " And so what that means in practice is that I tried doing pseudo-labeling for example", "tokens": [400, 370, 437, 300, 1355, 294, 3124, 307, 300, 286, 3031, 884, 35899, 12, 44990, 11031, 337, 1365], "temperature": 0.0, "avg_logprob": -0.19496212005615235, "compression_ratio": 1.4714285714285715, "no_speech_prob": 8.801021067483816e-06}, {"id": 22, "seek": 11282, "start": 120.0, "end": 121.0, "text": " on MNIST.", "tokens": [322, 376, 45, 19756, 13], "temperature": 0.0, "avg_logprob": -0.19496212005615235, "compression_ratio": 1.4714285714285715, "no_speech_prob": 8.801021067483816e-06}, {"id": 23, "seek": 11282, "start": 121.0, "end": 127.8, "text": " Remember on MNIST we already had that pretty close to state of the art result, which was", "tokens": [5459, 322, 376, 45, 19756, 321, 1217, 632, 300, 1238, 1998, 281, 1785, 295, 264, 1523, 1874, 11, 597, 390], "temperature": 0.0, "avg_logprob": -0.19496212005615235, "compression_ratio": 1.4714285714285715, "no_speech_prob": 8.801021067483816e-06}, {"id": 24, "seek": 11282, "start": 127.8, "end": 135.16, "text": " 99.69.", "tokens": [11803, 13, 30908, 13], "temperature": 0.0, "avg_logprob": -0.19496212005615235, "compression_ratio": 1.4714285714285715, "no_speech_prob": 8.801021067483816e-06}, {"id": 25, "seek": 11282, "start": 135.16, "end": 140.28, "text": " So I thought, can we improve it anymore if we use pseudo-labeling on the test set?", "tokens": [407, 286, 1194, 11, 393, 321, 3470, 309, 3602, 498, 321, 764, 35899, 12, 44990, 11031, 322, 264, 1500, 992, 30], "temperature": 0.0, "avg_logprob": -0.19496212005615235, "compression_ratio": 1.4714285714285715, "no_speech_prob": 8.801021067483816e-06}, {"id": 26, "seek": 14028, "start": 140.28, "end": 144.08, "text": " And so to do so, you just do this.", "tokens": [400, 370, 281, 360, 370, 11, 291, 445, 360, 341, 13], "temperature": 0.0, "avg_logprob": -0.1861441840588207, "compression_ratio": 1.515527950310559, "no_speech_prob": 1.7778307665139437e-05}, {"id": 27, "seek": 14028, "start": 144.08, "end": 151.48, "text": " You grab your training batches as usual, using data orientation if you want, whatever else.", "tokens": [509, 4444, 428, 3097, 15245, 279, 382, 7713, 11, 1228, 1412, 14764, 498, 291, 528, 11, 2035, 1646, 13], "temperature": 0.0, "avg_logprob": -0.1861441840588207, "compression_ratio": 1.515527950310559, "no_speech_prob": 1.7778307665139437e-05}, {"id": 28, "seek": 14028, "start": 151.48, "end": 153.8, "text": " So here's my training batches.", "tokens": [407, 510, 311, 452, 3097, 15245, 279, 13], "temperature": 0.0, "avg_logprob": -0.1861441840588207, "compression_ratio": 1.515527950310559, "no_speech_prob": 1.7778307665139437e-05}, {"id": 29, "seek": 14028, "start": 153.8, "end": 165.36, "text": " And then you create your pseudo-batches by saying, okay, my data is my test set and my", "tokens": [400, 550, 291, 1884, 428, 35899, 12, 65, 852, 279, 538, 1566, 11, 1392, 11, 452, 1412, 307, 452, 1500, 992, 293, 452], "temperature": 0.0, "avg_logprob": -0.1861441840588207, "compression_ratio": 1.515527950310559, "no_speech_prob": 1.7778307665139437e-05}, {"id": 30, "seek": 16536, "start": 165.36, "end": 172.04000000000002, "text": " labels are my predictions, and these are the predictions that I calculated back up here.", "tokens": [16949, 366, 452, 21264, 11, 293, 613, 366, 264, 21264, 300, 286, 15598, 646, 493, 510, 13], "temperature": 0.0, "avg_logprob": -0.11789012872255765, "compression_ratio": 1.6946902654867257, "no_speech_prob": 2.058040536212502e-06}, {"id": 31, "seek": 16536, "start": 172.04000000000002, "end": 177.8, "text": " So now this is the second set of batches, which is my pseudo-batches.", "tokens": [407, 586, 341, 307, 264, 1150, 992, 295, 15245, 279, 11, 597, 307, 452, 35899, 12, 65, 852, 279, 13], "temperature": 0.0, "avg_logprob": -0.11789012872255765, "compression_ratio": 1.6946902654867257, "no_speech_prob": 2.058040536212502e-06}, {"id": 32, "seek": 16536, "start": 177.8, "end": 182.52, "text": " And so then passing an array of those two things to the mix iterator now creates a new", "tokens": [400, 370, 550, 8437, 364, 10225, 295, 729, 732, 721, 281, 264, 2890, 17138, 1639, 586, 7829, 257, 777], "temperature": 0.0, "avg_logprob": -0.11789012872255765, "compression_ratio": 1.6946902654867257, "no_speech_prob": 2.058040536212502e-06}, {"id": 33, "seek": 16536, "start": 182.52, "end": 188.72000000000003, "text": " batch generator, which is going to give us a few images from here and a few images from", "tokens": [15245, 19265, 11, 597, 307, 516, 281, 976, 505, 257, 1326, 5267, 490, 510, 293, 257, 1326, 5267, 490], "temperature": 0.0, "avg_logprob": -0.11789012872255765, "compression_ratio": 1.6946902654867257, "no_speech_prob": 2.058040536212502e-06}, {"id": 34, "seek": 16536, "start": 188.72000000000003, "end": 189.72000000000003, "text": " here.", "tokens": [510, 13], "temperature": 0.0, "avg_logprob": -0.11789012872255765, "compression_ratio": 1.6946902654867257, "no_speech_prob": 2.058040536212502e-06}, {"id": 35, "seek": 16536, "start": 189.72000000000003, "end": 190.72000000000003, "text": " How many?", "tokens": [1012, 867, 30], "temperature": 0.0, "avg_logprob": -0.11789012872255765, "compression_ratio": 1.6946902654867257, "no_speech_prob": 2.058040536212502e-06}, {"id": 36, "seek": 16536, "start": 190.72000000000003, "end": 192.58, "text": " Well, however many you asked for.", "tokens": [1042, 11, 4461, 867, 291, 2351, 337, 13], "temperature": 0.0, "avg_logprob": -0.11789012872255765, "compression_ratio": 1.6946902654867257, "no_speech_prob": 2.058040536212502e-06}, {"id": 37, "seek": 19258, "start": 192.58, "end": 203.96, "text": " So in this case I was getting 64 from my training set and 64 divided by 4 from my test set.", "tokens": [407, 294, 341, 1389, 286, 390, 1242, 12145, 490, 452, 3097, 992, 293, 12145, 6666, 538, 1017, 490, 452, 1500, 992, 13], "temperature": 0.0, "avg_logprob": -0.18017194512185086, "compression_ratio": 1.5813953488372092, "no_speech_prob": 3.6688466025225352e-06}, {"id": 38, "seek": 19258, "start": 203.96, "end": 207.32000000000002, "text": " Come to think of it, that's probably less than Hinton recommends.", "tokens": [2492, 281, 519, 295, 309, 11, 300, 311, 1391, 1570, 813, 389, 12442, 34556, 13], "temperature": 0.0, "avg_logprob": -0.18017194512185086, "compression_ratio": 1.5813953488372092, "no_speech_prob": 3.6688466025225352e-06}, {"id": 39, "seek": 19258, "start": 207.32000000000002, "end": 210.16000000000003, "text": " That probably should have been divided by 3.", "tokens": [663, 1391, 820, 362, 668, 6666, 538, 805, 13], "temperature": 0.0, "avg_logprob": -0.18017194512185086, "compression_ratio": 1.5813953488372092, "no_speech_prob": 3.6688466025225352e-06}, {"id": 40, "seek": 19258, "start": 210.16000000000003, "end": 214.12, "text": " And now I can use that just like any other generator.", "tokens": [400, 586, 286, 393, 764, 300, 445, 411, 604, 661, 19265, 13], "temperature": 0.0, "avg_logprob": -0.18017194512185086, "compression_ratio": 1.5813953488372092, "no_speech_prob": 3.6688466025225352e-06}, {"id": 41, "seek": 19258, "start": 214.12, "end": 219.32000000000002, "text": " So then I just call model.fit generator and pass in that thing that I just created.", "tokens": [407, 550, 286, 445, 818, 2316, 13, 6845, 19265, 293, 1320, 294, 300, 551, 300, 286, 445, 2942, 13], "temperature": 0.0, "avg_logprob": -0.18017194512185086, "compression_ratio": 1.5813953488372092, "no_speech_prob": 3.6688466025225352e-06}, {"id": 42, "seek": 21932, "start": 219.32, "end": 223.07999999999998, "text": " And so what it's going to do is it's going to create a bunch of batches which will be", "tokens": [400, 370, 437, 309, 311, 516, 281, 360, 307, 309, 311, 516, 281, 1884, 257, 3840, 295, 15245, 279, 597, 486, 312], "temperature": 0.0, "avg_logprob": -0.12050286759721472, "compression_ratio": 1.5424528301886793, "no_speech_prob": 7.88921897765249e-06}, {"id": 43, "seek": 21932, "start": 223.07999999999998, "end": 233.6, "text": " 64 items from my regular training set and a quarter of that number of items from my", "tokens": [12145, 4754, 490, 452, 3890, 3097, 992, 293, 257, 6555, 295, 300, 1230, 295, 4754, 490, 452], "temperature": 0.0, "avg_logprob": -0.12050286759721472, "compression_ratio": 1.5424528301886793, "no_speech_prob": 7.88921897765249e-06}, {"id": 44, "seek": 21932, "start": 233.6, "end": 236.07999999999998, "text": " pseudo-labeled set.", "tokens": [35899, 12, 75, 18657, 292, 992, 13], "temperature": 0.0, "avg_logprob": -0.12050286759721472, "compression_ratio": 1.5424528301886793, "no_speech_prob": 7.88921897765249e-06}, {"id": 45, "seek": 21932, "start": 236.07999999999998, "end": 240.56, "text": " And lo and behold, it gave me a slightly better score.", "tokens": [400, 450, 293, 27234, 11, 309, 2729, 385, 257, 4748, 1101, 6175, 13], "temperature": 0.0, "avg_logprob": -0.12050286759721472, "compression_ratio": 1.5424528301886793, "no_speech_prob": 7.88921897765249e-06}, {"id": 46, "seek": 21932, "start": 240.56, "end": 245.56, "text": " There's only so much better we can do at this point, but that took us up to 99.72.", "tokens": [821, 311, 787, 370, 709, 1101, 321, 393, 360, 412, 341, 935, 11, 457, 300, 1890, 505, 493, 281, 11803, 13, 28890, 13], "temperature": 0.0, "avg_logprob": -0.12050286759721472, "compression_ratio": 1.5424528301886793, "no_speech_prob": 7.88921897765249e-06}, {"id": 47, "seek": 24556, "start": 245.56, "end": 252.12, "text": " It's worth mentioning that every.01% at this point is just one image, so we're really on", "tokens": [467, 311, 3163, 18315, 300, 633, 2411, 10607, 4, 412, 341, 935, 307, 445, 472, 3256, 11, 370, 321, 434, 534, 322], "temperature": 0.0, "avg_logprob": -0.22237805525461832, "compression_ratio": 1.5913043478260869, "no_speech_prob": 6.643362212344073e-06}, {"id": 48, "seek": 24556, "start": 252.12, "end": 254.24, "text": " the edges at this point.", "tokens": [264, 8819, 412, 341, 935, 13], "temperature": 0.0, "avg_logprob": -0.22237805525461832, "compression_ratio": 1.5913043478260869, "no_speech_prob": 6.643362212344073e-06}, {"id": 49, "seek": 24556, "start": 254.24, "end": 258.48, "text": " This is getting even closer to the state of the art, despite the fact that we're not doing", "tokens": [639, 307, 1242, 754, 4966, 281, 264, 1785, 295, 264, 1523, 11, 7228, 264, 1186, 300, 321, 434, 406, 884], "temperature": 0.0, "avg_logprob": -0.22237805525461832, "compression_ratio": 1.5913043478260869, "no_speech_prob": 6.643362212344073e-06}, {"id": 50, "seek": 24556, "start": 258.48, "end": 262.76, "text": " any handwriting-specific techniques.", "tokens": [604, 39179, 12, 29258, 7512, 13], "temperature": 0.0, "avg_logprob": -0.22237805525461832, "compression_ratio": 1.5913043478260869, "no_speech_prob": 6.643362212344073e-06}, {"id": 51, "seek": 24556, "start": 262.76, "end": 266.12, "text": " I also tried it on the FISH dataset.", "tokens": [286, 611, 3031, 309, 322, 264, 479, 18842, 28872, 13], "temperature": 0.0, "avg_logprob": -0.22237805525461832, "compression_ratio": 1.5913043478260869, "no_speech_prob": 6.643362212344073e-06}, {"id": 52, "seek": 24556, "start": 266.12, "end": 271.12, "text": " I realized at that point that this allows us to do something else which is pretty neat,", "tokens": [286, 5334, 412, 300, 935, 300, 341, 4045, 505, 281, 360, 746, 1646, 597, 307, 1238, 10654, 11], "temperature": 0.0, "avg_logprob": -0.22237805525461832, "compression_ratio": 1.5913043478260869, "no_speech_prob": 6.643362212344073e-06}, {"id": 53, "seek": 27112, "start": 271.12, "end": 277.0, "text": " which is normally when we train on the training set and set aside a validation set, if we", "tokens": [597, 307, 5646, 562, 321, 3847, 322, 264, 3097, 992, 293, 992, 7359, 257, 24071, 992, 11, 498, 321], "temperature": 0.0, "avg_logprob": -0.1332923122960278, "compression_ratio": 1.781512605042017, "no_speech_prob": 1.0451493835716974e-05}, {"id": 54, "seek": 27112, "start": 277.0, "end": 281.96, "text": " then want to submit to Kaggle, we've only trained on a subset of the data that they", "tokens": [550, 528, 281, 10315, 281, 48751, 22631, 11, 321, 600, 787, 8895, 322, 257, 25993, 295, 264, 1412, 300, 436], "temperature": 0.0, "avg_logprob": -0.1332923122960278, "compression_ratio": 1.781512605042017, "no_speech_prob": 1.0451493835716974e-05}, {"id": 55, "seek": 27112, "start": 281.96, "end": 282.96, "text": " gave us.", "tokens": [2729, 505, 13], "temperature": 0.0, "avg_logprob": -0.1332923122960278, "compression_ratio": 1.781512605042017, "no_speech_prob": 1.0451493835716974e-05}, {"id": 56, "seek": 27112, "start": 282.96, "end": 287.16, "text": " We didn't train on the validation set as well, which is not great.", "tokens": [492, 994, 380, 3847, 322, 264, 24071, 992, 382, 731, 11, 597, 307, 406, 869, 13], "temperature": 0.0, "avg_logprob": -0.1332923122960278, "compression_ratio": 1.781512605042017, "no_speech_prob": 1.0451493835716974e-05}, {"id": 57, "seek": 27112, "start": 287.16, "end": 292.84000000000003, "text": " So what you can actually do is you can send 3 sets of batches to the MixedEterator.", "tokens": [407, 437, 291, 393, 767, 360, 307, 291, 393, 2845, 805, 6352, 295, 15245, 279, 281, 264, 12769, 292, 36, 391, 1639, 13], "temperature": 0.0, "avg_logprob": -0.1332923122960278, "compression_ratio": 1.781512605042017, "no_speech_prob": 1.0451493835716974e-05}, {"id": 58, "seek": 27112, "start": 292.84000000000003, "end": 299.84000000000003, "text": " You can have your regular training batches, you can have your pseudo-labeled test batches,", "tokens": [509, 393, 362, 428, 3890, 3097, 15245, 279, 11, 291, 393, 362, 428, 35899, 12, 75, 18657, 292, 1500, 15245, 279, 11], "temperature": 0.0, "avg_logprob": -0.1332923122960278, "compression_ratio": 1.781512605042017, "no_speech_prob": 1.0451493835716974e-05}, {"id": 59, "seek": 29984, "start": 299.84, "end": 304.2, "text": " and if you think about it, you can also add in some validation batches using the true", "tokens": [293, 498, 291, 519, 466, 309, 11, 291, 393, 611, 909, 294, 512, 24071, 15245, 279, 1228, 264, 2074], "temperature": 0.0, "avg_logprob": -0.2051074341053271, "compression_ratio": 1.773076923076923, "no_speech_prob": 2.0145262169535272e-05}, {"id": 60, "seek": 29984, "start": 304.2, "end": 306.03999999999996, "text": " labels from the validation set.", "tokens": [16949, 490, 264, 24071, 992, 13], "temperature": 0.0, "avg_logprob": -0.2051074341053271, "compression_ratio": 1.773076923076923, "no_speech_prob": 2.0145262169535272e-05}, {"id": 61, "seek": 29984, "start": 306.03999999999996, "end": 309.28, "text": " So this is something you do right at the end.", "tokens": [407, 341, 307, 746, 291, 360, 558, 412, 264, 917, 13], "temperature": 0.0, "avg_logprob": -0.2051074341053271, "compression_ratio": 1.773076923076923, "no_speech_prob": 2.0145262169535272e-05}, {"id": 62, "seek": 29984, "start": 309.28, "end": 313.59999999999997, "text": " When you say, this is a model I'm happy with, you can fine-tune it a bit using some of the", "tokens": [1133, 291, 584, 11, 341, 307, 257, 2316, 286, 478, 2055, 365, 11, 291, 393, 2489, 12, 83, 2613, 309, 257, 857, 1228, 512, 295, 264], "temperature": 0.0, "avg_logprob": -0.2051074341053271, "compression_ratio": 1.773076923076923, "no_speech_prob": 2.0145262169535272e-05}, {"id": 63, "seek": 29984, "start": 313.59999999999997, "end": 315.96, "text": " real validation data.", "tokens": [957, 24071, 1412, 13], "temperature": 0.0, "avg_logprob": -0.2051074341053271, "compression_ratio": 1.773076923076923, "no_speech_prob": 2.0145262169535272e-05}, {"id": 64, "seek": 29984, "start": 315.96, "end": 320.32, "text": " You can see here I've got out of my batch size of 64, I'm putting 44 from the training", "tokens": [509, 393, 536, 510, 286, 600, 658, 484, 295, 452, 15245, 2744, 295, 12145, 11, 286, 478, 3372, 16408, 490, 264, 3097], "temperature": 0.0, "avg_logprob": -0.2051074341053271, "compression_ratio": 1.773076923076923, "no_speech_prob": 2.0145262169535272e-05}, {"id": 65, "seek": 29984, "start": 320.32, "end": 327.64, "text": " set, 4 from the validation set, and 16 from the pseudo-labeled test set.", "tokens": [992, 11, 1017, 490, 264, 24071, 992, 11, 293, 3165, 490, 264, 35899, 12, 75, 18657, 292, 1500, 992, 13], "temperature": 0.0, "avg_logprob": -0.2051074341053271, "compression_ratio": 1.773076923076923, "no_speech_prob": 2.0145262169535272e-05}, {"id": 66, "seek": 29984, "start": 327.64, "end": 328.64, "text": " This worked pretty well.", "tokens": [639, 2732, 1238, 731, 13], "temperature": 0.0, "avg_logprob": -0.2051074341053271, "compression_ratio": 1.773076923076923, "no_speech_prob": 2.0145262169535272e-05}, {"id": 67, "seek": 32864, "start": 328.64, "end": 336.15999999999997, "text": " It got me from about 110th to about 60th on the leaderboard.", "tokens": [467, 658, 385, 490, 466, 20154, 392, 281, 466, 4060, 392, 322, 264, 5263, 3787, 13], "temperature": 0.0, "avg_logprob": -0.44576737188523813, "compression_ratio": 1.44375, "no_speech_prob": 0.00011234978592256084}, {"id": 68, "seek": 32864, "start": 336.15999999999997, "end": 342.84, "text": " Question from the audience.", "tokens": [14464, 490, 264, 4034, 13], "temperature": 0.0, "avg_logprob": -0.44576737188523813, "compression_ratio": 1.44375, "no_speech_prob": 0.00011234978592256084}, {"id": 69, "seek": 32864, "start": 342.84, "end": 347.8, "text": " So if we go to Keras documentation, there is something called sample weight.", "tokens": [407, 498, 321, 352, 281, 591, 6985, 14333, 11, 456, 307, 746, 1219, 6889, 3364, 13], "temperature": 0.0, "avg_logprob": -0.44576737188523813, "compression_ratio": 1.44375, "no_speech_prob": 0.00011234978592256084}, {"id": 70, "seek": 32864, "start": 347.8, "end": 352.08, "text": " I wonder if you can just set the sample weight to be lower for...", "tokens": [286, 2441, 498, 291, 393, 445, 992, 264, 6889, 3364, 281, 312, 3126, 337, 1097], "temperature": 0.0, "avg_logprob": -0.44576737188523813, "compression_ratio": 1.44375, "no_speech_prob": 0.00011234978592256084}, {"id": 71, "seek": 35208, "start": 352.08, "end": 374.84, "text": " Answer the question.", "tokens": [24545, 264, 1168, 13], "temperature": 0.0, "avg_logprob": -0.3353724656281648, "compression_ratio": 1.1111111111111112, "no_speech_prob": 7.5278535405232105e-06}, {"id": 72, "seek": 35208, "start": 374.84, "end": 379.68, "text": " I will mention that I found the way I'm doing it seems a little slow.", "tokens": [286, 486, 2152, 300, 286, 1352, 264, 636, 286, 478, 884, 309, 2544, 257, 707, 2964, 13], "temperature": 0.0, "avg_logprob": -0.3353724656281648, "compression_ratio": 1.1111111111111112, "no_speech_prob": 7.5278535405232105e-06}, {"id": 73, "seek": 37968, "start": 379.68, "end": 382.6, "text": " There are some obvious ways I can speed it up.", "tokens": [821, 366, 512, 6322, 2098, 286, 393, 3073, 309, 493, 13], "temperature": 0.0, "avg_logprob": -0.22494505219540353, "compression_ratio": 1.709016393442623, "no_speech_prob": 1.1842489584523719e-05}, {"id": 74, "seek": 37968, "start": 382.6, "end": 389.48, "text": " I'm not quite sure why it is, but it might be because this concatenation each time is", "tokens": [286, 478, 406, 1596, 988, 983, 309, 307, 11, 457, 309, 1062, 312, 570, 341, 1588, 7186, 399, 1184, 565, 307], "temperature": 0.0, "avg_logprob": -0.22494505219540353, "compression_ratio": 1.709016393442623, "no_speech_prob": 1.1842489584523719e-05}, {"id": 75, "seek": 37968, "start": 389.48, "end": 393.40000000000003, "text": " kind of having to create new memory and that takes a long time.", "tokens": [733, 295, 1419, 281, 1884, 777, 4675, 293, 300, 2516, 257, 938, 565, 13], "temperature": 0.0, "avg_logprob": -0.22494505219540353, "compression_ratio": 1.709016393442623, "no_speech_prob": 1.1842489584523719e-05}, {"id": 76, "seek": 37968, "start": 393.40000000000003, "end": 397.56, "text": " There are some obvious things I could do to try and speed it up.", "tokens": [821, 366, 512, 6322, 721, 286, 727, 360, 281, 853, 293, 3073, 309, 493, 13], "temperature": 0.0, "avg_logprob": -0.22494505219540353, "compression_ratio": 1.709016393442623, "no_speech_prob": 1.1842489584523719e-05}, {"id": 77, "seek": 37968, "start": 397.56, "end": 399.6, "text": " It's good enough and simple to do the job.", "tokens": [467, 311, 665, 1547, 293, 2199, 281, 360, 264, 1691, 13], "temperature": 0.0, "avg_logprob": -0.22494505219540353, "compression_ratio": 1.709016393442623, "no_speech_prob": 1.1842489584523719e-05}, {"id": 78, "seek": 37968, "start": 399.6, "end": 406.36, "text": " I'm pleased that we now have a way to do convenient pseudo-label in Keras, and it seems to do", "tokens": [286, 478, 10587, 300, 321, 586, 362, 257, 636, 281, 360, 10851, 35899, 12, 75, 18657, 294, 591, 6985, 11, 293, 309, 2544, 281, 360], "temperature": 0.0, "avg_logprob": -0.22494505219540353, "compression_ratio": 1.709016393442623, "no_speech_prob": 1.1842489584523719e-05}, {"id": 79, "seek": 37968, "start": 406.36, "end": 407.36, "text": " a pretty good job.", "tokens": [257, 1238, 665, 1691, 13], "temperature": 0.0, "avg_logprob": -0.22494505219540353, "compression_ratio": 1.709016393442623, "no_speech_prob": 1.1842489584523719e-05}, {"id": 80, "seek": 40736, "start": 407.36, "end": 411.88, "text": " Question from the audience.", "tokens": [14464, 490, 264, 4034, 13], "temperature": 0.0, "avg_logprob": -0.17385689242855534, "compression_ratio": 1.6590909090909092, "no_speech_prob": 1.6186897482839413e-05}, {"id": 81, "seek": 40736, "start": 411.88, "end": 416.84000000000003, "text": " So the other thing I wanted to talk about before we move on to the new material today", "tokens": [407, 264, 661, 551, 286, 1415, 281, 751, 466, 949, 321, 1286, 322, 281, 264, 777, 2527, 965], "temperature": 0.0, "avg_logprob": -0.17385689242855534, "compression_ratio": 1.6590909090909092, "no_speech_prob": 1.6186897482839413e-05}, {"id": 82, "seek": 40736, "start": 416.84000000000003, "end": 417.84000000000003, "text": " is embeddings.", "tokens": [307, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.17385689242855534, "compression_ratio": 1.6590909090909092, "no_speech_prob": 1.6186897482839413e-05}, {"id": 83, "seek": 40736, "start": 417.84000000000003, "end": 425.6, "text": " I've had lots of questions about embeddings, and I think it's pretty clear that at least", "tokens": [286, 600, 632, 3195, 295, 1651, 466, 12240, 29432, 11, 293, 286, 519, 309, 311, 1238, 1850, 300, 412, 1935], "temperature": 0.0, "avg_logprob": -0.17385689242855534, "compression_ratio": 1.6590909090909092, "no_speech_prob": 1.6186897482839413e-05}, {"id": 84, "seek": 40736, "start": 425.6, "end": 429.2, "text": " for some of you, some additional explanations would be helpful.", "tokens": [337, 512, 295, 291, 11, 512, 4497, 28708, 576, 312, 4961, 13], "temperature": 0.0, "avg_logprob": -0.17385689242855534, "compression_ratio": 1.6590909090909092, "no_speech_prob": 1.6186897482839413e-05}, {"id": 85, "seek": 40736, "start": 429.2, "end": 436.72, "text": " So I wanted to start out by reminding you that when I introduced embeddings to you,", "tokens": [407, 286, 1415, 281, 722, 484, 538, 27639, 291, 300, 562, 286, 7268, 12240, 29432, 281, 291, 11], "temperature": 0.0, "avg_logprob": -0.17385689242855534, "compression_ratio": 1.6590909090909092, "no_speech_prob": 1.6186897482839413e-05}, {"id": 86, "seek": 43672, "start": 436.72, "end": 442.64000000000004, "text": " the data that we had, we looked at this crosstab form of data.", "tokens": [264, 1412, 300, 321, 632, 11, 321, 2956, 412, 341, 28108, 372, 455, 1254, 295, 1412, 13], "temperature": 0.0, "avg_logprob": -0.19611820605916713, "compression_ratio": 1.7954545454545454, "no_speech_prob": 4.198429815005511e-05}, {"id": 87, "seek": 43672, "start": 442.64000000000004, "end": 446.52000000000004, "text": " And when it's in this crosstab form, it's very easy to visualize what embeddings look", "tokens": [400, 562, 309, 311, 294, 341, 28108, 372, 455, 1254, 11, 309, 311, 588, 1858, 281, 23273, 437, 12240, 29432, 574], "temperature": 0.0, "avg_logprob": -0.19611820605916713, "compression_ratio": 1.7954545454545454, "no_speech_prob": 4.198429815005511e-05}, {"id": 88, "seek": 43672, "start": 446.52000000000004, "end": 453.06, "text": " like, which is for movie number 27 and user ID number 14, here is that movie ID's embedding", "tokens": [411, 11, 597, 307, 337, 3169, 1230, 7634, 293, 4195, 7348, 1230, 3499, 11, 510, 307, 300, 3169, 7348, 311, 12240, 3584], "temperature": 0.0, "avg_logprob": -0.19611820605916713, "compression_ratio": 1.7954545454545454, "no_speech_prob": 4.198429815005511e-05}, {"id": 89, "seek": 43672, "start": 453.06, "end": 458.28000000000003, "text": " right here, and here is that user ID's embedding right here, and so here is the dot product", "tokens": [558, 510, 11, 293, 510, 307, 300, 4195, 7348, 311, 12240, 3584, 558, 510, 11, 293, 370, 510, 307, 264, 5893, 1674], "temperature": 0.0, "avg_logprob": -0.19611820605916713, "compression_ratio": 1.7954545454545454, "no_speech_prob": 4.198429815005511e-05}, {"id": 90, "seek": 43672, "start": 458.28000000000003, "end": 461.84000000000003, "text": " of the two right here.", "tokens": [295, 264, 732, 558, 510, 13], "temperature": 0.0, "avg_logprob": -0.19611820605916713, "compression_ratio": 1.7954545454545454, "no_speech_prob": 4.198429815005511e-05}, {"id": 91, "seek": 43672, "start": 461.84000000000003, "end": 465.0, "text": " So that was all pretty straightforward.", "tokens": [407, 300, 390, 439, 1238, 15325, 13], "temperature": 0.0, "avg_logprob": -0.19611820605916713, "compression_ratio": 1.7954545454545454, "no_speech_prob": 4.198429815005511e-05}, {"id": 92, "seek": 46500, "start": 465.0, "end": 471.04, "text": " And so then all we had to do to optimize our embeddings was use the gradient descent solver", "tokens": [400, 370, 550, 439, 321, 632, 281, 360, 281, 19719, 527, 12240, 29432, 390, 764, 264, 16235, 23475, 1404, 331], "temperature": 0.0, "avg_logprob": -0.18588106397172094, "compression_ratio": 1.5337078651685394, "no_speech_prob": 4.4254020394873805e-06}, {"id": 93, "seek": 46500, "start": 471.04, "end": 478.4, "text": " that is built into Microsoft Excel, which is called solver, and we just told it what", "tokens": [300, 307, 3094, 666, 8116, 19060, 11, 597, 307, 1219, 1404, 331, 11, 293, 321, 445, 1907, 309, 437], "temperature": 0.0, "avg_logprob": -0.18588106397172094, "compression_ratio": 1.5337078651685394, "no_speech_prob": 4.4254020394873805e-06}, {"id": 94, "seek": 46500, "start": 478.4, "end": 485.32, "text": " our objective is, which is this cell, and we set to minimize it by changing these sets", "tokens": [527, 10024, 307, 11, 597, 307, 341, 2815, 11, 293, 321, 992, 281, 17522, 309, 538, 4473, 613, 6352], "temperature": 0.0, "avg_logprob": -0.18588106397172094, "compression_ratio": 1.5337078651685394, "no_speech_prob": 4.4254020394873805e-06}, {"id": 95, "seek": 46500, "start": 485.32, "end": 488.64, "text": " of cells.", "tokens": [295, 5438, 13], "temperature": 0.0, "avg_logprob": -0.18588106397172094, "compression_ratio": 1.5337078651685394, "no_speech_prob": 4.4254020394873805e-06}, {"id": 96, "seek": 48864, "start": 488.64, "end": 497.0, "text": " Now the data that we are given in the MovieLens dataset, however, requires some manipulation", "tokens": [823, 264, 1412, 300, 321, 366, 2212, 294, 264, 28766, 43, 694, 28872, 11, 4461, 11, 7029, 512, 26475], "temperature": 0.0, "avg_logprob": -0.14077990279238448, "compression_ratio": 1.7258064516129032, "no_speech_prob": 3.187539050486521e-06}, {"id": 97, "seek": 48864, "start": 497.0, "end": 498.36, "text": " to get into a crosstab form.", "tokens": [281, 483, 666, 257, 28108, 372, 455, 1254, 13], "temperature": 0.0, "avg_logprob": -0.14077990279238448, "compression_ratio": 1.7258064516129032, "no_speech_prob": 3.187539050486521e-06}, {"id": 98, "seek": 48864, "start": 498.36, "end": 500.4, "text": " We're actually given it in this form.", "tokens": [492, 434, 767, 2212, 309, 294, 341, 1254, 13], "temperature": 0.0, "avg_logprob": -0.14077990279238448, "compression_ratio": 1.7258064516129032, "no_speech_prob": 3.187539050486521e-06}, {"id": 99, "seek": 48864, "start": 500.4, "end": 504.03999999999996, "text": " And we wouldn't want to create a crosstab with all of this data because it would be", "tokens": [400, 321, 2759, 380, 528, 281, 1884, 257, 28108, 372, 455, 365, 439, 295, 341, 1412, 570, 309, 576, 312], "temperature": 0.0, "avg_logprob": -0.14077990279238448, "compression_ratio": 1.7258064516129032, "no_speech_prob": 3.187539050486521e-06}, {"id": 100, "seek": 48864, "start": 504.03999999999996, "end": 505.03999999999996, "text": " way too big.", "tokens": [636, 886, 955, 13], "temperature": 0.0, "avg_logprob": -0.14077990279238448, "compression_ratio": 1.7258064516129032, "no_speech_prob": 3.187539050486521e-06}, {"id": 101, "seek": 48864, "start": 505.03999999999996, "end": 509.52, "text": " It would be every single user times every single movie, and it would also be very inconvenient.", "tokens": [467, 576, 312, 633, 2167, 4195, 1413, 633, 2167, 3169, 11, 293, 309, 576, 611, 312, 588, 46196, 13], "temperature": 0.0, "avg_logprob": -0.14077990279238448, "compression_ratio": 1.7258064516129032, "no_speech_prob": 3.187539050486521e-06}, {"id": 102, "seek": 48864, "start": 509.52, "end": 511.44, "text": " So that's not how Keras works.", "tokens": [407, 300, 311, 406, 577, 591, 6985, 1985, 13], "temperature": 0.0, "avg_logprob": -0.14077990279238448, "compression_ratio": 1.7258064516129032, "no_speech_prob": 3.187539050486521e-06}, {"id": 103, "seek": 48864, "start": 511.44, "end": 515.6, "text": " Keras uses this data in exactly this format.", "tokens": [591, 6985, 4960, 341, 1412, 294, 2293, 341, 7877, 13], "temperature": 0.0, "avg_logprob": -0.14077990279238448, "compression_ratio": 1.7258064516129032, "no_speech_prob": 3.187539050486521e-06}, {"id": 104, "seek": 51560, "start": 515.6, "end": 521.24, "text": " And so let me show you how that works and what an embedding is really doing.", "tokens": [400, 370, 718, 385, 855, 291, 577, 300, 1985, 293, 437, 364, 12240, 3584, 307, 534, 884, 13], "temperature": 0.0, "avg_logprob": -0.10163170400291982, "compression_ratio": 1.6285714285714286, "no_speech_prob": 1.4285382349044085e-05}, {"id": 105, "seek": 51560, "start": 521.24, "end": 526.88, "text": " So here is the exact same thing, but I'm going to show you this using the data in the format", "tokens": [407, 510, 307, 264, 1900, 912, 551, 11, 457, 286, 478, 516, 281, 855, 291, 341, 1228, 264, 1412, 294, 264, 7877], "temperature": 0.0, "avg_logprob": -0.10163170400291982, "compression_ratio": 1.6285714285714286, "no_speech_prob": 1.4285382349044085e-05}, {"id": 106, "seek": 51560, "start": 526.88, "end": 528.4, "text": " that Keras uses.", "tokens": [300, 591, 6985, 4960, 13], "temperature": 0.0, "avg_logprob": -0.10163170400291982, "compression_ratio": 1.6285714285714286, "no_speech_prob": 1.4285382349044085e-05}, {"id": 107, "seek": 51560, "start": 528.4, "end": 531.5600000000001, "text": " So this is our input data.", "tokens": [407, 341, 307, 527, 4846, 1412, 13], "temperature": 0.0, "avg_logprob": -0.10163170400291982, "compression_ratio": 1.6285714285714286, "no_speech_prob": 1.4285382349044085e-05}, {"id": 108, "seek": 51560, "start": 531.5600000000001, "end": 537.46, "text": " Every rating is a row and has a user ID, a movie ID, and a rating.", "tokens": [2048, 10990, 307, 257, 5386, 293, 575, 257, 4195, 7348, 11, 257, 3169, 7348, 11, 293, 257, 10990, 13], "temperature": 0.0, "avg_logprob": -0.10163170400291982, "compression_ratio": 1.6285714285714286, "no_speech_prob": 1.4285382349044085e-05}, {"id": 109, "seek": 51560, "start": 537.46, "end": 542.64, "text": " And this is what an embedding matrix looks like for 15 users.", "tokens": [400, 341, 307, 437, 364, 12240, 3584, 8141, 1542, 411, 337, 2119, 5022, 13], "temperature": 0.0, "avg_logprob": -0.10163170400291982, "compression_ratio": 1.6285714285714286, "no_speech_prob": 1.4285382349044085e-05}, {"id": 110, "seek": 54264, "start": 542.64, "end": 550.24, "text": " So these are the user IDs, and for each user ID, here's user ID 14's embedding, and this", "tokens": [407, 613, 366, 264, 4195, 48212, 11, 293, 337, 1184, 4195, 7348, 11, 510, 311, 4195, 7348, 3499, 311, 12240, 3584, 11, 293, 341], "temperature": 0.0, "avg_logprob": -0.2309128504533034, "compression_ratio": 1.894736842105263, "no_speech_prob": 8.397923011216335e-06}, {"id": 111, "seek": 54264, "start": 550.24, "end": 553.1999999999999, "text": " is 29's embedding, and this is 72's embedding.", "tokens": [307, 9413, 311, 12240, 3584, 11, 293, 341, 307, 18731, 311, 12240, 3584, 13], "temperature": 0.0, "avg_logprob": -0.2309128504533034, "compression_ratio": 1.894736842105263, "no_speech_prob": 8.397923011216335e-06}, {"id": 112, "seek": 54264, "start": 553.1999999999999, "end": 554.1999999999999, "text": " At this stage, they're just random.", "tokens": [1711, 341, 3233, 11, 436, 434, 445, 4974, 13], "temperature": 0.0, "avg_logprob": -0.2309128504533034, "compression_ratio": 1.894736842105263, "no_speech_prob": 8.397923011216335e-06}, {"id": 113, "seek": 54264, "start": 554.1999999999999, "end": 556.92, "text": " They're just initializing random numbers.", "tokens": [814, 434, 445, 5883, 3319, 4974, 3547, 13], "temperature": 0.0, "avg_logprob": -0.2309128504533034, "compression_ratio": 1.894736842105263, "no_speech_prob": 8.397923011216335e-06}, {"id": 114, "seek": 54264, "start": 556.92, "end": 560.92, "text": " So this thing here is called an embedding matrix.", "tokens": [407, 341, 551, 510, 307, 1219, 364, 12240, 3584, 8141, 13], "temperature": 0.0, "avg_logprob": -0.2309128504533034, "compression_ratio": 1.894736842105263, "no_speech_prob": 8.397923011216335e-06}, {"id": 115, "seek": 54264, "start": 560.92, "end": 563.74, "text": " And here is the movie embedding matrix.", "tokens": [400, 510, 307, 264, 3169, 12240, 3584, 8141, 13], "temperature": 0.0, "avg_logprob": -0.2309128504533034, "compression_ratio": 1.894736842105263, "no_speech_prob": 8.397923011216335e-06}, {"id": 116, "seek": 54264, "start": 563.74, "end": 569.4, "text": " So the embedding matrix for Movie27 are these 5 numbers.", "tokens": [407, 264, 12240, 3584, 8141, 337, 28766, 10076, 366, 613, 1025, 3547, 13], "temperature": 0.0, "avg_logprob": -0.2309128504533034, "compression_ratio": 1.894736842105263, "no_speech_prob": 8.397923011216335e-06}, {"id": 117, "seek": 56940, "start": 569.4, "end": 577.4, "text": " So what happens when we look at user ID 14, movie ID 417, rating number 2?", "tokens": [407, 437, 2314, 562, 321, 574, 412, 4195, 7348, 3499, 11, 3169, 7348, 1017, 7773, 11, 10990, 1230, 568, 30], "temperature": 0.0, "avg_logprob": -0.15931445605134312, "compression_ratio": 1.5743243243243243, "no_speech_prob": 2.2252725102589466e-06}, {"id": 118, "seek": 56940, "start": 577.4, "end": 581.88, "text": " The first thing that happens is that we have to find user ID 14.", "tokens": [440, 700, 551, 300, 2314, 307, 300, 321, 362, 281, 915, 4195, 7348, 3499, 13], "temperature": 0.0, "avg_logprob": -0.15931445605134312, "compression_ratio": 1.5743243243243243, "no_speech_prob": 2.2252725102589466e-06}, {"id": 119, "seek": 56940, "start": 581.88, "end": 582.88, "text": " And here it is.", "tokens": [400, 510, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.15931445605134312, "compression_ratio": 1.5743243243243243, "no_speech_prob": 2.2252725102589466e-06}, {"id": 120, "seek": 56940, "start": 582.88, "end": 586.86, "text": " User ID 14 is the first thing in this array.", "tokens": [32127, 7348, 3499, 307, 264, 700, 551, 294, 341, 10225, 13], "temperature": 0.0, "avg_logprob": -0.15931445605134312, "compression_ratio": 1.5743243243243243, "no_speech_prob": 2.2252725102589466e-06}, {"id": 121, "seek": 56940, "start": 586.86, "end": 590.8, "text": " So the index of user ID 14 is 1.", "tokens": [407, 264, 8186, 295, 4195, 7348, 3499, 307, 502, 13], "temperature": 0.0, "avg_logprob": -0.15931445605134312, "compression_ratio": 1.5743243243243243, "no_speech_prob": 2.2252725102589466e-06}, {"id": 122, "seek": 59080, "start": 590.8, "end": 600.04, "text": " So then here is the first row from the user embedding matrix.", "tokens": [407, 550, 510, 307, 264, 700, 5386, 490, 264, 4195, 12240, 3584, 8141, 13], "temperature": 0.0, "avg_logprob": -0.2576485516732199, "compression_ratio": 1.448, "no_speech_prob": 3.187543825333705e-06}, {"id": 123, "seek": 59080, "start": 600.04, "end": 615.68, "text": " Similarly, movie ID 417, here is movie ID 417, and it is the 14th row of this table.", "tokens": [13157, 11, 3169, 7348, 1017, 7773, 11, 510, 307, 3169, 7348, 1017, 7773, 11, 293, 309, 307, 264, 3499, 392, 5386, 295, 341, 3199, 13], "temperature": 0.0, "avg_logprob": -0.2576485516732199, "compression_ratio": 1.448, "no_speech_prob": 3.187543825333705e-06}, {"id": 124, "seek": 59080, "start": 615.68, "end": 618.8, "text": " So we want to return the 14th row.", "tokens": [407, 321, 528, 281, 2736, 264, 3499, 392, 5386, 13], "temperature": 0.0, "avg_logprob": -0.2576485516732199, "compression_ratio": 1.448, "no_speech_prob": 3.187543825333705e-06}, {"id": 125, "seek": 61880, "start": 618.8, "end": 623.64, "text": " So you can see here it has looked up and found that it's the 14th row, and then indexed into", "tokens": [407, 291, 393, 536, 510, 309, 575, 2956, 493, 293, 1352, 300, 309, 311, 264, 3499, 392, 5386, 11, 293, 550, 8186, 292, 666], "temperature": 0.0, "avg_logprob": -0.13556865988106565, "compression_ratio": 1.9427312775330396, "no_speech_prob": 4.356855697551509e-06}, {"id": 126, "seek": 61880, "start": 623.64, "end": 626.68, "text": " the table and grabbed the 14th row.", "tokens": [264, 3199, 293, 18607, 264, 3499, 392, 5386, 13], "temperature": 0.0, "avg_logprob": -0.13556865988106565, "compression_ratio": 1.9427312775330396, "no_speech_prob": 4.356855697551509e-06}, {"id": 127, "seek": 61880, "start": 626.68, "end": 632.16, "text": " And so then to calculate the dot product, we simply take the dot product of the user", "tokens": [400, 370, 550, 281, 8873, 264, 5893, 1674, 11, 321, 2935, 747, 264, 5893, 1674, 295, 264, 4195], "temperature": 0.0, "avg_logprob": -0.13556865988106565, "compression_ratio": 1.9427312775330396, "no_speech_prob": 4.356855697551509e-06}, {"id": 128, "seek": 61880, "start": 632.16, "end": 635.16, "text": " embedding with the movie embedding.", "tokens": [12240, 3584, 365, 264, 3169, 12240, 3584, 13], "temperature": 0.0, "avg_logprob": -0.13556865988106565, "compression_ratio": 1.9427312775330396, "no_speech_prob": 4.356855697551509e-06}, {"id": 129, "seek": 61880, "start": 635.16, "end": 640.6999999999999, "text": " And then to calculate the loss, we simply take the rating and subtract the prediction", "tokens": [400, 550, 281, 8873, 264, 4470, 11, 321, 2935, 747, 264, 10990, 293, 16390, 264, 17630], "temperature": 0.0, "avg_logprob": -0.13556865988106565, "compression_ratio": 1.9427312775330396, "no_speech_prob": 4.356855697551509e-06}, {"id": 130, "seek": 61880, "start": 640.6999999999999, "end": 642.0799999999999, "text": " and square it.", "tokens": [293, 3732, 309, 13], "temperature": 0.0, "avg_logprob": -0.13556865988106565, "compression_ratio": 1.9427312775330396, "no_speech_prob": 4.356855697551509e-06}, {"id": 131, "seek": 61880, "start": 642.0799999999999, "end": 648.28, "text": " And then to get the total loss function, we just add that all up and take the square root.", "tokens": [400, 550, 281, 483, 264, 3217, 4470, 2445, 11, 321, 445, 909, 300, 439, 493, 293, 747, 264, 3732, 5593, 13], "temperature": 0.0, "avg_logprob": -0.13556865988106565, "compression_ratio": 1.9427312775330396, "no_speech_prob": 4.356855697551509e-06}, {"id": 132, "seek": 64828, "start": 648.28, "end": 658.64, "text": " So the orange background cells are the cells which we want our SGD solver to change in", "tokens": [407, 264, 7671, 3678, 5438, 366, 264, 5438, 597, 321, 528, 527, 34520, 35, 1404, 331, 281, 1319, 294], "temperature": 0.0, "avg_logprob": -0.12660070146833147, "compression_ratio": 1.5593220338983051, "no_speech_prob": 9.818239050218835e-06}, {"id": 133, "seek": 64828, "start": 658.64, "end": 667.36, "text": " order to minimize this cell here, and then all of the orange bold cells are the calculated", "tokens": [1668, 281, 17522, 341, 2815, 510, 11, 293, 550, 439, 295, 264, 7671, 11928, 5438, 366, 264, 15598], "temperature": 0.0, "avg_logprob": -0.12660070146833147, "compression_ratio": 1.5593220338983051, "no_speech_prob": 9.818239050218835e-06}, {"id": 134, "seek": 64828, "start": 667.36, "end": 669.18, "text": " cells.", "tokens": [5438, 13], "temperature": 0.0, "avg_logprob": -0.12660070146833147, "compression_ratio": 1.5593220338983051, "no_speech_prob": 9.818239050218835e-06}, {"id": 135, "seek": 64828, "start": 669.18, "end": 677.4, "text": " So when I was saying last week that an embedding is simply looking up an array by an index,", "tokens": [407, 562, 286, 390, 1566, 1036, 1243, 300, 364, 12240, 3584, 307, 2935, 1237, 493, 364, 10225, 538, 364, 8186, 11], "temperature": 0.0, "avg_logprob": -0.12660070146833147, "compression_ratio": 1.5593220338983051, "no_speech_prob": 9.818239050218835e-06}, {"id": 136, "seek": 67740, "start": 677.4, "end": 678.84, "text": " you can see why I was saying that.", "tokens": [291, 393, 536, 983, 286, 390, 1566, 300, 13], "temperature": 0.0, "avg_logprob": -0.14452462102852615, "compression_ratio": 1.7035398230088497, "no_speech_prob": 1.5689332940382883e-05}, {"id": 137, "seek": 67740, "start": 678.84, "end": 684.64, "text": " It's literally taking an index and it looks it up in an array and returns that row.", "tokens": [467, 311, 3736, 1940, 364, 8186, 293, 309, 1542, 309, 493, 294, 364, 10225, 293, 11247, 300, 5386, 13], "temperature": 0.0, "avg_logprob": -0.14452462102852615, "compression_ratio": 1.7035398230088497, "no_speech_prob": 1.5689332940382883e-05}, {"id": 138, "seek": 67740, "start": 684.64, "end": 687.64, "text": " That's literally all it's doing.", "tokens": [663, 311, 3736, 439, 309, 311, 884, 13], "temperature": 0.0, "avg_logprob": -0.14452462102852615, "compression_ratio": 1.7035398230088497, "no_speech_prob": 1.5689332940382883e-05}, {"id": 139, "seek": 67740, "start": 687.64, "end": 690.4, "text": " You might want to convince yourself during the week if you haven't looked at this yet", "tokens": [509, 1062, 528, 281, 13447, 1803, 1830, 264, 1243, 498, 291, 2378, 380, 2956, 412, 341, 1939], "temperature": 0.0, "avg_logprob": -0.14452462102852615, "compression_ratio": 1.7035398230088497, "no_speech_prob": 1.5689332940382883e-05}, {"id": 140, "seek": 67740, "start": 690.4, "end": 696.64, "text": " that this is identical to taking a one-pot encoded matrix and multiplying it by an embedding", "tokens": [300, 341, 307, 14800, 281, 1940, 257, 472, 12, 17698, 2058, 12340, 8141, 293, 30955, 309, 538, 364, 12240, 3584], "temperature": 0.0, "avg_logprob": -0.14452462102852615, "compression_ratio": 1.7035398230088497, "no_speech_prob": 1.5689332940382883e-05}, {"id": 141, "seek": 67740, "start": 696.64, "end": 697.76, "text": " matrix.", "tokens": [8141, 13], "temperature": 0.0, "avg_logprob": -0.14452462102852615, "compression_ratio": 1.7035398230088497, "no_speech_prob": 1.5689332940382883e-05}, {"id": 142, "seek": 67740, "start": 697.76, "end": 701.48, "text": " That's identical to doing this kind of lookup.", "tokens": [663, 311, 14800, 281, 884, 341, 733, 295, 574, 1010, 13], "temperature": 0.0, "avg_logprob": -0.14452462102852615, "compression_ratio": 1.7035398230088497, "no_speech_prob": 1.5689332940382883e-05}, {"id": 143, "seek": 70148, "start": 701.48, "end": 711.24, "text": " So we can do exactly the same thing in this way, we can say data solver, we want to set", "tokens": [407, 321, 393, 360, 2293, 264, 912, 551, 294, 341, 636, 11, 321, 393, 584, 1412, 1404, 331, 11, 321, 528, 281, 992], "temperature": 0.0, "avg_logprob": -0.23772472530216365, "compression_ratio": 1.4615384615384615, "no_speech_prob": 7.5279376687831245e-06}, {"id": 144, "seek": 70148, "start": 711.24, "end": 715.96, "text": " this cell to a minimum by changing these cells.", "tokens": [341, 2815, 281, 257, 7285, 538, 4473, 613, 5438, 13], "temperature": 0.0, "avg_logprob": -0.23772472530216365, "compression_ratio": 1.4615384615384615, "no_speech_prob": 7.5279376687831245e-06}, {"id": 145, "seek": 70148, "start": 715.96, "end": 721.96, "text": " And if I say solve, then Excel will go away and try to improve our objective.", "tokens": [400, 498, 286, 584, 5039, 11, 550, 19060, 486, 352, 1314, 293, 853, 281, 3470, 527, 10024, 13], "temperature": 0.0, "avg_logprob": -0.23772472530216365, "compression_ratio": 1.4615384615384615, "no_speech_prob": 7.5279376687831245e-06}, {"id": 146, "seek": 70148, "start": 721.96, "end": 727.2, "text": " You can see it's decreasing, it's down to about 2.5.", "tokens": [509, 393, 536, 309, 311, 23223, 11, 309, 311, 760, 281, 466, 568, 13, 20, 13], "temperature": 0.0, "avg_logprob": -0.23772472530216365, "compression_ratio": 1.4615384615384615, "no_speech_prob": 7.5279376687831245e-06}, {"id": 147, "seek": 72720, "start": 727.2, "end": 732.58, "text": " And so what it's doing here is it's using gradient descent to try to find ways to increase", "tokens": [400, 370, 437, 309, 311, 884, 510, 307, 309, 311, 1228, 16235, 23475, 281, 853, 281, 915, 2098, 281, 3488], "temperature": 0.0, "avg_logprob": -0.14799487952030066, "compression_ratio": 1.4659090909090908, "no_speech_prob": 7.183193702076096e-06}, {"id": 148, "seek": 72720, "start": 732.58, "end": 740.44, "text": " or decrease all of these numbers such that that IRMSC becomes as low as possible.", "tokens": [420, 11514, 439, 295, 613, 3547, 1270, 300, 300, 16486, 10288, 34, 3643, 382, 2295, 382, 1944, 13], "temperature": 0.0, "avg_logprob": -0.14799487952030066, "compression_ratio": 1.4659090909090908, "no_speech_prob": 7.183193702076096e-06}, {"id": 149, "seek": 72720, "start": 740.44, "end": 753.36, "text": " So that's literally all that is going on in our Keras example here, this dot product.", "tokens": [407, 300, 311, 3736, 439, 300, 307, 516, 322, 294, 527, 591, 6985, 1365, 510, 11, 341, 5893, 1674, 13], "temperature": 0.0, "avg_logprob": -0.14799487952030066, "compression_ratio": 1.4659090909090908, "no_speech_prob": 7.183193702076096e-06}, {"id": 150, "seek": 75336, "start": 753.36, "end": 757.32, "text": " So this thing here where we said create an embedding for a user, that's just saying create", "tokens": [407, 341, 551, 510, 689, 321, 848, 1884, 364, 12240, 3584, 337, 257, 4195, 11, 300, 311, 445, 1566, 1884], "temperature": 0.0, "avg_logprob": -0.185551380289012, "compression_ratio": 1.8974358974358974, "no_speech_prob": 2.0784640582860447e-05}, {"id": 151, "seek": 75336, "start": 757.32, "end": 761.6, "text": " something where I can look up the user ID and find their row.", "tokens": [746, 689, 286, 393, 574, 493, 264, 4195, 7348, 293, 915, 641, 5386, 13], "temperature": 0.0, "avg_logprob": -0.185551380289012, "compression_ratio": 1.8974358974358974, "no_speech_prob": 2.0784640582860447e-05}, {"id": 152, "seek": 75336, "start": 761.6, "end": 765.88, "text": " This is doing the same for a movie, look up the movie ID and find its row.", "tokens": [639, 307, 884, 264, 912, 337, 257, 3169, 11, 574, 493, 264, 3169, 7348, 293, 915, 1080, 5386, 13], "temperature": 0.0, "avg_logprob": -0.185551380289012, "compression_ratio": 1.8974358974358974, "no_speech_prob": 2.0784640582860447e-05}, {"id": 153, "seek": 75336, "start": 765.88, "end": 770.2, "text": " This here says take the dot product once you've found the two.", "tokens": [639, 510, 1619, 747, 264, 5893, 1674, 1564, 291, 600, 1352, 264, 732, 13], "temperature": 0.0, "avg_logprob": -0.185551380289012, "compression_ratio": 1.8974358974358974, "no_speech_prob": 2.0784640582860447e-05}, {"id": 154, "seek": 75336, "start": 770.2, "end": 774.44, "text": " And then this here says train a model where you take in that user ID and movie ID and", "tokens": [400, 550, 341, 510, 1619, 3847, 257, 2316, 689, 291, 747, 294, 300, 4195, 7348, 293, 3169, 7348, 293], "temperature": 0.0, "avg_logprob": -0.185551380289012, "compression_ratio": 1.8974358974358974, "no_speech_prob": 2.0784640582860447e-05}, {"id": 155, "seek": 75336, "start": 774.44, "end": 783.12, "text": " try to predict the rating and use SGD to make it better and better.", "tokens": [853, 281, 6069, 264, 10990, 293, 764, 34520, 35, 281, 652, 309, 1101, 293, 1101, 13], "temperature": 0.0, "avg_logprob": -0.185551380289012, "compression_ratio": 1.8974358974358974, "no_speech_prob": 2.0784640582860447e-05}, {"id": 156, "seek": 78312, "start": 783.12, "end": 792.2, "text": " So you can see here that it's got the root mean squared error down to 0.4.", "tokens": [407, 291, 393, 536, 510, 300, 309, 311, 658, 264, 5593, 914, 8889, 6713, 760, 281, 1958, 13, 19, 13], "temperature": 0.0, "avg_logprob": -0.24682818402300824, "compression_ratio": 1.6428571428571428, "no_speech_prob": 2.247367774543818e-05}, {"id": 157, "seek": 78312, "start": 792.2, "end": 797.6, "text": " So for example, the first one predicted 3, it's actually 2, predicted 4.5, it's actually", "tokens": [407, 337, 1365, 11, 264, 700, 472, 19147, 805, 11, 309, 311, 767, 568, 11, 19147, 1017, 13, 20, 11, 309, 311, 767], "temperature": 0.0, "avg_logprob": -0.24682818402300824, "compression_ratio": 1.6428571428571428, "no_speech_prob": 2.247367774543818e-05}, {"id": 158, "seek": 78312, "start": 797.6, "end": 800.6, "text": " 4, predicted 4.6, it's actually 5 and so forth.", "tokens": [1017, 11, 19147, 1017, 13, 21, 11, 309, 311, 767, 1025, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.24682818402300824, "compression_ratio": 1.6428571428571428, "no_speech_prob": 2.247367774543818e-05}, {"id": 159, "seek": 78312, "start": 800.6, "end": 805.08, "text": " So you kind of get the idea of how it works.", "tokens": [407, 291, 733, 295, 483, 264, 1558, 295, 577, 309, 1985, 13], "temperature": 0.0, "avg_logprob": -0.24682818402300824, "compression_ratio": 1.6428571428571428, "no_speech_prob": 2.247367774543818e-05}, {"id": 160, "seek": 78312, "start": 805.08, "end": 808.44, "text": " Word embeddings work exactly the same way.", "tokens": [8725, 12240, 29432, 589, 2293, 264, 912, 636, 13], "temperature": 0.0, "avg_logprob": -0.24682818402300824, "compression_ratio": 1.6428571428571428, "no_speech_prob": 2.247367774543818e-05}, {"id": 161, "seek": 80844, "start": 808.44, "end": 815.96, "text": " So inspired by one of the students who talked about this during the week, I grabbed the", "tokens": [407, 7547, 538, 472, 295, 264, 1731, 567, 2825, 466, 341, 1830, 264, 1243, 11, 286, 18607, 264], "temperature": 0.0, "avg_logprob": -0.17775167563022712, "compression_ratio": 1.5393939393939393, "no_speech_prob": 2.144461359421257e-05}, {"id": 162, "seek": 80844, "start": 815.96, "end": 819.8800000000001, "text": " text of Green Eggs and Ham.", "tokens": [2487, 295, 6969, 42486, 293, 8234, 13], "temperature": 0.0, "avg_logprob": -0.17775167563022712, "compression_ratio": 1.5393939393939393, "no_speech_prob": 2.144461359421257e-05}, {"id": 163, "seek": 80844, "start": 819.8800000000001, "end": 821.72, "text": " So here is the text of Green Eggs and Ham.", "tokens": [407, 510, 307, 264, 2487, 295, 6969, 42486, 293, 8234, 13], "temperature": 0.0, "avg_logprob": -0.17775167563022712, "compression_ratio": 1.5393939393939393, "no_speech_prob": 2.144461359421257e-05}, {"id": 164, "seek": 80844, "start": 821.72, "end": 828.0, "text": " I am Daniel, I am Sam, Sam I am, that's Sam I am, etc.", "tokens": [286, 669, 8033, 11, 286, 669, 4832, 11, 4832, 286, 669, 11, 300, 311, 4832, 286, 669, 11, 5183, 13], "temperature": 0.0, "avg_logprob": -0.17775167563022712, "compression_ratio": 1.5393939393939393, "no_speech_prob": 2.144461359421257e-05}, {"id": 165, "seek": 80844, "start": 828.0, "end": 835.36, "text": " And I've turned this poem into a matrix.", "tokens": [400, 286, 600, 3574, 341, 13065, 666, 257, 8141, 13], "temperature": 0.0, "avg_logprob": -0.17775167563022712, "compression_ratio": 1.5393939393939393, "no_speech_prob": 2.144461359421257e-05}, {"id": 166, "seek": 83536, "start": 835.36, "end": 843.12, "text": " The way I did that was to take every unique word in that poem.", "tokens": [440, 636, 286, 630, 300, 390, 281, 747, 633, 3845, 1349, 294, 300, 13065, 13], "temperature": 0.0, "avg_logprob": -0.17798809090045967, "compression_ratio": 1.6106194690265487, "no_speech_prob": 8.530193554179277e-06}, {"id": 167, "seek": 83536, "start": 843.12, "end": 847.5600000000001, "text": " Here is the ID of each of those words, just indexed from one.", "tokens": [1692, 307, 264, 7348, 295, 1184, 295, 729, 2283, 11, 445, 8186, 292, 490, 472, 13], "temperature": 0.0, "avg_logprob": -0.17798809090045967, "compression_ratio": 1.6106194690265487, "no_speech_prob": 8.530193554179277e-06}, {"id": 168, "seek": 83536, "start": 847.5600000000001, "end": 851.6800000000001, "text": " And so then I just randomly generated an embedding matrix.", "tokens": [400, 370, 550, 286, 445, 16979, 10833, 364, 12240, 3584, 8141, 13], "temperature": 0.0, "avg_logprob": -0.17798809090045967, "compression_ratio": 1.6106194690265487, "no_speech_prob": 8.530193554179277e-06}, {"id": 169, "seek": 83536, "start": 851.6800000000001, "end": 858.92, "text": " I equally well could have used the downloaded GloVe embeddings instead.", "tokens": [286, 12309, 731, 727, 362, 1143, 264, 21748, 10786, 53, 68, 12240, 29432, 2602, 13], "temperature": 0.0, "avg_logprob": -0.17798809090045967, "compression_ratio": 1.6106194690265487, "no_speech_prob": 8.530193554179277e-06}, {"id": 170, "seek": 83536, "start": 858.92, "end": 863.4, "text": " And so then just for each word, I just look up in the list to find that word and find", "tokens": [400, 370, 550, 445, 337, 1184, 1349, 11, 286, 445, 574, 493, 294, 264, 1329, 281, 915, 300, 1349, 293, 915], "temperature": 0.0, "avg_logprob": -0.17798809090045967, "compression_ratio": 1.6106194690265487, "no_speech_prob": 8.530193554179277e-06}, {"id": 171, "seek": 83536, "start": 863.4, "end": 864.4, "text": " out what number it is.", "tokens": [484, 437, 1230, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.17798809090045967, "compression_ratio": 1.6106194690265487, "no_speech_prob": 8.530193554179277e-06}, {"id": 172, "seek": 86440, "start": 864.4, "end": 871.0799999999999, "text": " So I is number 8, and so here is the 8th row of the embedding matrix.", "tokens": [407, 286, 307, 1230, 1649, 11, 293, 370, 510, 307, 264, 1649, 392, 5386, 295, 264, 12240, 3584, 8141, 13], "temperature": 0.0, "avg_logprob": -0.17032812787340834, "compression_ratio": 1.5773809523809523, "no_speech_prob": 4.289302069082623e-06}, {"id": 173, "seek": 86440, "start": 871.0799999999999, "end": 878.0, "text": " So you can see here that we've started with a poem and we've turned it into a matrix of", "tokens": [407, 291, 393, 536, 510, 300, 321, 600, 1409, 365, 257, 13065, 293, 321, 600, 3574, 309, 666, 257, 8141, 295], "temperature": 0.0, "avg_logprob": -0.17032812787340834, "compression_ratio": 1.5773809523809523, "no_speech_prob": 4.289302069082623e-06}, {"id": 174, "seek": 86440, "start": 878.0, "end": 879.0, "text": " floats.", "tokens": [37878, 13], "temperature": 0.0, "avg_logprob": -0.17032812787340834, "compression_ratio": 1.5773809523809523, "no_speech_prob": 4.289302069082623e-06}, {"id": 175, "seek": 86440, "start": 879.0, "end": 885.3199999999999, "text": " So the reason we do this is because our machine learning tools want a matrix of floats, not", "tokens": [407, 264, 1778, 321, 360, 341, 307, 570, 527, 3479, 2539, 3873, 528, 257, 8141, 295, 37878, 11, 406], "temperature": 0.0, "avg_logprob": -0.17032812787340834, "compression_ratio": 1.5773809523809523, "no_speech_prob": 4.289302069082623e-06}, {"id": 176, "seek": 86440, "start": 885.3199999999999, "end": 887.72, "text": " a poem.", "tokens": [257, 13065, 13], "temperature": 0.0, "avg_logprob": -0.17032812787340834, "compression_ratio": 1.5773809523809523, "no_speech_prob": 4.289302069082623e-06}, {"id": 177, "seek": 88772, "start": 887.72, "end": 894.96, "text": " So all of the questions about does it matter what the word IDs are, you can see it doesn't", "tokens": [407, 439, 295, 264, 1651, 466, 775, 309, 1871, 437, 264, 1349, 48212, 366, 11, 291, 393, 536, 309, 1177, 380], "temperature": 0.0, "avg_logprob": -0.1424831189607319, "compression_ratio": 1.6336633663366336, "no_speech_prob": 1.012984193948796e-05}, {"id": 178, "seek": 88772, "start": 894.96, "end": 896.08, "text": " matter at all.", "tokens": [1871, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.1424831189607319, "compression_ratio": 1.6336633663366336, "no_speech_prob": 1.012984193948796e-05}, {"id": 179, "seek": 88772, "start": 896.08, "end": 901.9200000000001, "text": " All we're doing is we're looking them up in this matrix and returning the floats.", "tokens": [1057, 321, 434, 884, 307, 321, 434, 1237, 552, 493, 294, 341, 8141, 293, 12678, 264, 37878, 13], "temperature": 0.0, "avg_logprob": -0.1424831189607319, "compression_ratio": 1.6336633663366336, "no_speech_prob": 1.012984193948796e-05}, {"id": 180, "seek": 88772, "start": 901.9200000000001, "end": 904.32, "text": " And once we've done that, we never use them again.", "tokens": [400, 1564, 321, 600, 1096, 300, 11, 321, 1128, 764, 552, 797, 13], "temperature": 0.0, "avg_logprob": -0.1424831189607319, "compression_ratio": 1.6336633663366336, "no_speech_prob": 1.012984193948796e-05}, {"id": 181, "seek": 88772, "start": 904.32, "end": 907.5600000000001, "text": " We just use this matrix of floats.", "tokens": [492, 445, 764, 341, 8141, 295, 37878, 13], "temperature": 0.0, "avg_logprob": -0.1424831189607319, "compression_ratio": 1.6336633663366336, "no_speech_prob": 1.012984193948796e-05}, {"id": 182, "seek": 88772, "start": 907.5600000000001, "end": 912.88, "text": " So that's what embeddings are.", "tokens": [407, 300, 311, 437, 12240, 29432, 366, 13], "temperature": 0.0, "avg_logprob": -0.1424831189607319, "compression_ratio": 1.6336633663366336, "no_speech_prob": 1.012984193948796e-05}, {"id": 183, "seek": 88772, "start": 912.88, "end": 913.88, "text": " So I hope that's helpful.", "tokens": [407, 286, 1454, 300, 311, 4961, 13], "temperature": 0.0, "avg_logprob": -0.1424831189607319, "compression_ratio": 1.6336633663366336, "no_speech_prob": 1.012984193948796e-05}, {"id": 184, "seek": 91388, "start": 913.88, "end": 918.84, "text": " Feel free to ask if you have any questions, either now or any other time, because we're", "tokens": [14113, 1737, 281, 1029, 498, 291, 362, 604, 1651, 11, 2139, 586, 420, 604, 661, 565, 11, 570, 321, 434], "temperature": 0.0, "avg_logprob": -0.20667524048776337, "compression_ratio": 1.4463276836158192, "no_speech_prob": 2.28277021960821e-05}, {"id": 185, "seek": 91388, "start": 918.84, "end": 924.0, "text": " going to be using embeddings throughout this class.", "tokens": [516, 281, 312, 1228, 12240, 29432, 3710, 341, 1508, 13], "temperature": 0.0, "avg_logprob": -0.20667524048776337, "compression_ratio": 1.4463276836158192, "no_speech_prob": 2.28277021960821e-05}, {"id": 186, "seek": 91388, "start": 924.0, "end": 930.72, "text": " So hopefully that helped a few people clarify what's going on.", "tokens": [407, 4696, 300, 4254, 257, 1326, 561, 17594, 437, 311, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.20667524048776337, "compression_ratio": 1.4463276836158192, "no_speech_prob": 2.28277021960821e-05}, {"id": 187, "seek": 91388, "start": 930.72, "end": 939.28, "text": " Okay, so let's get back to recurrent neural networks.", "tokens": [1033, 11, 370, 718, 311, 483, 646, 281, 18680, 1753, 18161, 9590, 13], "temperature": 0.0, "avg_logprob": -0.20667524048776337, "compression_ratio": 1.4463276836158192, "no_speech_prob": 2.28277021960821e-05}, {"id": 188, "seek": 93928, "start": 939.28, "end": 950.9599999999999, "text": " So to remind you, we talked about the purpose of recurrent neural networks as being really", "tokens": [407, 281, 4160, 291, 11, 321, 2825, 466, 264, 4334, 295, 18680, 1753, 18161, 9590, 382, 885, 534], "temperature": 0.0, "avg_logprob": -0.13586922389705006, "compression_ratio": 1.375, "no_speech_prob": 8.013425031094812e-06}, {"id": 189, "seek": 93928, "start": 950.9599999999999, "end": 958.4, "text": " all about memory.", "tokens": [439, 466, 4675, 13], "temperature": 0.0, "avg_logprob": -0.13586922389705006, "compression_ratio": 1.375, "no_speech_prob": 8.013425031094812e-06}, {"id": 190, "seek": 93928, "start": 958.4, "end": 962.92, "text": " So it's really all about this idea of memory.", "tokens": [407, 309, 311, 534, 439, 466, 341, 1558, 295, 4675, 13], "temperature": 0.0, "avg_logprob": -0.13586922389705006, "compression_ratio": 1.375, "no_speech_prob": 8.013425031094812e-06}, {"id": 191, "seek": 96292, "start": 962.92, "end": 969.8, "text": " If we're going to handle something like recognizing a comment start and a comment end and being", "tokens": [759, 321, 434, 516, 281, 4813, 746, 411, 18538, 257, 2871, 722, 293, 257, 2871, 917, 293, 885], "temperature": 0.0, "avg_logprob": -0.11078091717641288, "compression_ratio": 1.7821011673151752, "no_speech_prob": 1.892467844299972e-05}, {"id": 192, "seek": 96292, "start": 969.8, "end": 974.1999999999999, "text": " able to keep track of the fact that we're in a comment for all of this time so that", "tokens": [1075, 281, 1066, 2837, 295, 264, 1186, 300, 321, 434, 294, 257, 2871, 337, 439, 295, 341, 565, 370, 300], "temperature": 0.0, "avg_logprob": -0.11078091717641288, "compression_ratio": 1.7821011673151752, "no_speech_prob": 1.892467844299972e-05}, {"id": 193, "seek": 96292, "start": 974.1999999999999, "end": 978.5999999999999, "text": " we can do modeling on this kind of structured language data, we're really going to need", "tokens": [321, 393, 360, 15983, 322, 341, 733, 295, 18519, 2856, 1412, 11, 321, 434, 534, 516, 281, 643], "temperature": 0.0, "avg_logprob": -0.11078091717641288, "compression_ratio": 1.7821011673151752, "no_speech_prob": 1.892467844299972e-05}, {"id": 194, "seek": 96292, "start": 978.5999999999999, "end": 980.9599999999999, "text": " memory.", "tokens": [4675, 13], "temperature": 0.0, "avg_logprob": -0.11078091717641288, "compression_ratio": 1.7821011673151752, "no_speech_prob": 1.892467844299972e-05}, {"id": 195, "seek": 96292, "start": 980.9599999999999, "end": 987.26, "text": " That allows us to handle long-term dependencies and provides this stateful representation.", "tokens": [663, 4045, 505, 281, 4813, 938, 12, 7039, 36606, 293, 6417, 341, 1785, 906, 10290, 13], "temperature": 0.0, "avg_logprob": -0.11078091717641288, "compression_ratio": 1.7821011673151752, "no_speech_prob": 1.892467844299972e-05}, {"id": 196, "seek": 96292, "start": 987.26, "end": 990.4, "text": " So in general, the stuff we're talking about, we're going to be looking at things that kind", "tokens": [407, 294, 2674, 11, 264, 1507, 321, 434, 1417, 466, 11, 321, 434, 516, 281, 312, 1237, 412, 721, 300, 733], "temperature": 0.0, "avg_logprob": -0.11078091717641288, "compression_ratio": 1.7821011673151752, "no_speech_prob": 1.892467844299972e-05}, {"id": 197, "seek": 99040, "start": 990.4, "end": 993.48, "text": " of particularly need these three things.", "tokens": [295, 4098, 643, 613, 1045, 721, 13], "temperature": 0.0, "avg_logprob": -0.222190607161749, "compression_ratio": 1.4755555555555555, "no_speech_prob": 1.2606574273377191e-05}, {"id": 198, "seek": 99040, "start": 993.48, "end": 998.0, "text": " And it's also somewhat helpful just for when you have a variable length sequence.", "tokens": [400, 309, 311, 611, 8344, 4961, 445, 337, 562, 291, 362, 257, 7006, 4641, 8310, 13], "temperature": 0.0, "avg_logprob": -0.222190607161749, "compression_ratio": 1.4755555555555555, "no_speech_prob": 1.2606574273377191e-05}, {"id": 199, "seek": 99040, "start": 998.0, "end": 1001.88, "text": " Question about embeddings.", "tokens": [14464, 466, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.222190607161749, "compression_ratio": 1.4755555555555555, "no_speech_prob": 1.2606574273377191e-05}, {"id": 200, "seek": 99040, "start": 1001.88, "end": 1006.3199999999999, "text": " One is how does the size of my embedding depend on the number of unique words?", "tokens": [1485, 307, 577, 775, 264, 2744, 295, 452, 12240, 3584, 5672, 322, 264, 1230, 295, 3845, 2283, 30], "temperature": 0.0, "avg_logprob": -0.222190607161749, "compression_ratio": 1.4755555555555555, "no_speech_prob": 1.2606574273377191e-05}, {"id": 201, "seek": 99040, "start": 1006.3199999999999, "end": 1011.48, "text": " So mapping green eggs and ham to 5 real numbers seems sufficient, but wouldn't be for all", "tokens": [407, 18350, 3092, 6466, 293, 7852, 281, 1025, 957, 3547, 2544, 11563, 11, 457, 2759, 380, 312, 337, 439], "temperature": 0.0, "avg_logprob": -0.222190607161749, "compression_ratio": 1.4755555555555555, "no_speech_prob": 1.2606574273377191e-05}, {"id": 202, "seek": 99040, "start": 1011.48, "end": 1014.04, "text": " of JRR token.", "tokens": [295, 32849, 49, 14862, 13], "temperature": 0.0, "avg_logprob": -0.222190607161749, "compression_ratio": 1.4755555555555555, "no_speech_prob": 1.2606574273377191e-05}, {"id": 203, "seek": 101404, "start": 1014.04, "end": 1021.7199999999999, "text": " So your choice of how big to make your embedding matrix, as in how many latent factors to create,", "tokens": [407, 428, 3922, 295, 577, 955, 281, 652, 428, 12240, 3584, 8141, 11, 382, 294, 577, 867, 48994, 6771, 281, 1884, 11], "temperature": 0.0, "avg_logprob": -0.15521588737581982, "compression_ratio": 1.4734299516908214, "no_speech_prob": 3.785286480706418e-06}, {"id": 204, "seek": 101404, "start": 1021.7199999999999, "end": 1028.92, "text": " is one of these architectural decisions which we don't really have an answer to.", "tokens": [307, 472, 295, 613, 26621, 5327, 597, 321, 500, 380, 534, 362, 364, 1867, 281, 13], "temperature": 0.0, "avg_logprob": -0.15521588737581982, "compression_ratio": 1.4734299516908214, "no_speech_prob": 3.785286480706418e-06}, {"id": 205, "seek": 101404, "start": 1028.92, "end": 1031.04, "text": " My best suggestion, there's a few.", "tokens": [1222, 1151, 16541, 11, 456, 311, 257, 1326, 13], "temperature": 0.0, "avg_logprob": -0.15521588737581982, "compression_ratio": 1.4734299516908214, "no_speech_prob": 3.785286480706418e-06}, {"id": 206, "seek": 101404, "start": 1031.04, "end": 1039.04, "text": " One is to read the Word2Vec paper, which kind of introduced a lot of this, or at least took", "tokens": [1485, 307, 281, 1401, 264, 8725, 17, 53, 3045, 3035, 11, 597, 733, 295, 7268, 257, 688, 295, 341, 11, 420, 412, 1935, 1890], "temperature": 0.0, "avg_logprob": -0.15521588737581982, "compression_ratio": 1.4734299516908214, "no_speech_prob": 3.785286480706418e-06}, {"id": 207, "seek": 103904, "start": 1039.04, "end": 1047.3999999999999, "text": " a look at the difference between a 50-dimensional, 100-dimensional, 200-dimensional, 300-dimensional,", "tokens": [257, 574, 412, 264, 2649, 1296, 257, 2625, 12, 18759, 11, 2319, 12, 18759, 11, 2331, 12, 18759, 11, 6641, 12, 18759, 11], "temperature": 0.0, "avg_logprob": -0.17898474193754652, "compression_ratio": 1.8729508196721312, "no_speech_prob": 7.183147317846306e-06}, {"id": 208, "seek": 103904, "start": 1047.3999999999999, "end": 1052.32, "text": " 600-dimensional, and see what are the different levels of accuracy that those different size", "tokens": [11849, 12, 18759, 11, 293, 536, 437, 366, 264, 819, 4358, 295, 14170, 300, 729, 819, 2744], "temperature": 0.0, "avg_logprob": -0.17898474193754652, "compression_ratio": 1.8729508196721312, "no_speech_prob": 7.183147317846306e-06}, {"id": 209, "seek": 103904, "start": 1052.32, "end": 1058.8799999999999, "text": " embedding matrices created when the authors of that paper provided this information.", "tokens": [12240, 3584, 32284, 2942, 562, 264, 16552, 295, 300, 3035, 5649, 341, 1589, 13], "temperature": 0.0, "avg_logprob": -0.17898474193754652, "compression_ratio": 1.8729508196721312, "no_speech_prob": 7.183147317846306e-06}, {"id": 210, "seek": 103904, "start": 1058.8799999999999, "end": 1063.0, "text": " So that's a quick shortcut because other people have already experimented and provided those", "tokens": [407, 300, 311, 257, 1702, 24822, 570, 661, 561, 362, 1217, 5120, 292, 293, 5649, 729], "temperature": 0.0, "avg_logprob": -0.17898474193754652, "compression_ratio": 1.8729508196721312, "no_speech_prob": 7.183147317846306e-06}, {"id": 211, "seek": 103904, "start": 1063.0, "end": 1064.8799999999999, "text": " results for you.", "tokens": [3542, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.17898474193754652, "compression_ratio": 1.8729508196721312, "no_speech_prob": 7.183147317846306e-06}, {"id": 212, "seek": 103904, "start": 1064.8799999999999, "end": 1066.72, "text": " The other is to do your own experiments.", "tokens": [440, 661, 307, 281, 360, 428, 1065, 12050, 13], "temperature": 0.0, "avg_logprob": -0.17898474193754652, "compression_ratio": 1.8729508196721312, "no_speech_prob": 7.183147317846306e-06}, {"id": 213, "seek": 103904, "start": 1066.72, "end": 1068.72, "text": " Try a few different sizes.", "tokens": [6526, 257, 1326, 819, 11602, 13], "temperature": 0.0, "avg_logprob": -0.17898474193754652, "compression_ratio": 1.8729508196721312, "no_speech_prob": 7.183147317846306e-06}, {"id": 214, "seek": 106872, "start": 1068.72, "end": 1074.68, "text": " It's not really about the length of the word list, it's really about the complexity of", "tokens": [467, 311, 406, 534, 466, 264, 4641, 295, 264, 1349, 1329, 11, 309, 311, 534, 466, 264, 14024, 295], "temperature": 0.0, "avg_logprob": -0.16641700267791748, "compression_ratio": 1.6844444444444444, "no_speech_prob": 2.753524313447997e-05}, {"id": 215, "seek": 106872, "start": 1074.68, "end": 1078.72, "text": " the language or other problem that you're trying to solve.", "tokens": [264, 2856, 420, 661, 1154, 300, 291, 434, 1382, 281, 5039, 13], "temperature": 0.0, "avg_logprob": -0.16641700267791748, "compression_ratio": 1.6844444444444444, "no_speech_prob": 2.753524313447997e-05}, {"id": 216, "seek": 106872, "start": 1078.72, "end": 1085.48, "text": " And that's really problem-dependent and will require both your intuition developed from", "tokens": [400, 300, 311, 534, 1154, 12, 36763, 317, 293, 486, 3651, 1293, 428, 24002, 4743, 490], "temperature": 0.0, "avg_logprob": -0.16641700267791748, "compression_ratio": 1.6844444444444444, "no_speech_prob": 2.753524313447997e-05}, {"id": 217, "seek": 106872, "start": 1085.48, "end": 1089.56, "text": " reading and experimenting and also your own experiments.", "tokens": [3760, 293, 29070, 293, 611, 428, 1065, 12050, 13], "temperature": 0.0, "avg_logprob": -0.16641700267791748, "compression_ratio": 1.6844444444444444, "no_speech_prob": 2.753524313447997e-05}, {"id": 218, "seek": 106872, "start": 1089.56, "end": 1096.8, "text": " And what would be the range of root mean square error value to say that a model is good?", "tokens": [400, 437, 576, 312, 264, 3613, 295, 5593, 914, 3732, 6713, 2158, 281, 584, 300, 257, 2316, 307, 665, 30], "temperature": 0.0, "avg_logprob": -0.16641700267791748, "compression_ratio": 1.6844444444444444, "no_speech_prob": 2.753524313447997e-05}, {"id": 219, "seek": 109680, "start": 1096.8, "end": 1102.0, "text": " To say that a model is good is another model-specific issue.", "tokens": [1407, 584, 300, 257, 2316, 307, 665, 307, 1071, 2316, 12, 29258, 2734, 13], "temperature": 0.0, "avg_logprob": -0.1842253171480619, "compression_ratio": 1.4906832298136645, "no_speech_prob": 5.064348079031333e-05}, {"id": 220, "seek": 109680, "start": 1102.0, "end": 1105.8, "text": " So a root mean square error is very interpretable.", "tokens": [407, 257, 5593, 914, 3732, 6713, 307, 588, 7302, 712, 13], "temperature": 0.0, "avg_logprob": -0.1842253171480619, "compression_ratio": 1.4906832298136645, "no_speech_prob": 5.064348079031333e-05}, {"id": 221, "seek": 109680, "start": 1105.8, "end": 1109.84, "text": " It's basically how far out is it on average.", "tokens": [467, 311, 1936, 577, 1400, 484, 307, 309, 322, 4274, 13], "temperature": 0.0, "avg_logprob": -0.1842253171480619, "compression_ratio": 1.4906832298136645, "no_speech_prob": 5.064348079031333e-05}, {"id": 222, "seek": 109680, "start": 1109.84, "end": 1123.52, "text": " So if we were finding that we were getting ratings within about.4, if we're getting", "tokens": [407, 498, 321, 645, 5006, 300, 321, 645, 1242, 24603, 1951, 466, 2411, 19, 11, 498, 321, 434, 1242], "temperature": 0.0, "avg_logprob": -0.1842253171480619, "compression_ratio": 1.4906832298136645, "no_speech_prob": 5.064348079031333e-05}, {"id": 223, "seek": 112352, "start": 1123.52, "end": 1130.36, "text": " within.4 on average, that sounds like it's probably good enough to be useful for helping", "tokens": [1951, 2411, 19, 322, 4274, 11, 300, 3263, 411, 309, 311, 1391, 665, 1547, 281, 312, 4420, 337, 4315], "temperature": 0.0, "avg_logprob": -0.22664149602254233, "compression_ratio": 1.3774834437086092, "no_speech_prob": 1.5206639545795042e-05}, {"id": 224, "seek": 112352, "start": 1130.36, "end": 1133.92, "text": " people find movies that they might like.", "tokens": [561, 915, 6233, 300, 436, 1062, 411, 13], "temperature": 0.0, "avg_logprob": -0.22664149602254233, "compression_ratio": 1.3774834437086092, "no_speech_prob": 1.5206639545795042e-05}, {"id": 225, "seek": 112352, "start": 1133.92, "end": 1136.4, "text": " But there's really no one solution.", "tokens": [583, 456, 311, 534, 572, 472, 3827, 13], "temperature": 0.0, "avg_logprob": -0.22664149602254233, "compression_ratio": 1.3774834437086092, "no_speech_prob": 1.5206639545795042e-05}, {"id": 226, "seek": 112352, "start": 1136.4, "end": 1149.68, "text": " I actually wrote a whole paper about this.", "tokens": [286, 767, 4114, 257, 1379, 3035, 466, 341, 13], "temperature": 0.0, "avg_logprob": -0.22664149602254233, "compression_ratio": 1.3774834437086092, "no_speech_prob": 1.5206639545795042e-05}, {"id": 227, "seek": 114968, "start": 1149.68, "end": 1157.64, "text": " If you look up designing great data products, this is based on really mainly 10 years of", "tokens": [759, 291, 574, 493, 14685, 869, 1412, 3383, 11, 341, 307, 2361, 322, 534, 8704, 1266, 924, 295], "temperature": 0.0, "avg_logprob": -0.16643498160622336, "compression_ratio": 1.5771144278606966, "no_speech_prob": 1.3419787137536332e-05}, {"id": 228, "seek": 114968, "start": 1157.64, "end": 1162.28, "text": " work I did at a company I created called Optimal Decisions Group.", "tokens": [589, 286, 630, 412, 257, 2237, 286, 2942, 1219, 21455, 10650, 12427, 4252, 10500, 13], "temperature": 0.0, "avg_logprob": -0.16643498160622336, "compression_ratio": 1.5771144278606966, "no_speech_prob": 1.3419787137536332e-05}, {"id": 229, "seek": 114968, "start": 1162.28, "end": 1169.96, "text": " And Optimal Decisions Group was all about how to use predictive modeling not just to", "tokens": [400, 21455, 10650, 12427, 4252, 10500, 390, 439, 466, 577, 281, 764, 35521, 15983, 406, 445, 281], "temperature": 0.0, "avg_logprob": -0.16643498160622336, "compression_ratio": 1.5771144278606966, "no_speech_prob": 1.3419787137536332e-05}, {"id": 230, "seek": 114968, "start": 1169.96, "end": 1172.8400000000001, "text": " make predictions but to optimize actions.", "tokens": [652, 21264, 457, 281, 19719, 5909, 13], "temperature": 0.0, "avg_logprob": -0.16643498160622336, "compression_ratio": 1.5771144278606966, "no_speech_prob": 1.3419787137536332e-05}, {"id": 231, "seek": 114968, "start": 1172.8400000000001, "end": 1175.48, "text": " And this whole paper is about that.", "tokens": [400, 341, 1379, 3035, 307, 466, 300, 13], "temperature": 0.0, "avg_logprob": -0.16643498160622336, "compression_ratio": 1.5771144278606966, "no_speech_prob": 1.3419787137536332e-05}, {"id": 232, "seek": 117548, "start": 1175.48, "end": 1182.08, "text": " In the end, it's really about coming up with a way to measure the benefit to your organization", "tokens": [682, 264, 917, 11, 309, 311, 534, 466, 1348, 493, 365, 257, 636, 281, 3481, 264, 5121, 281, 428, 4475], "temperature": 0.0, "avg_logprob": -0.18663425445556642, "compression_ratio": 1.3636363636363635, "no_speech_prob": 6.438965556299081e-06}, {"id": 233, "seek": 117548, "start": 1182.08, "end": 1187.32, "text": " or to your project of getting that extra 0.1% accuracy.", "tokens": [420, 281, 428, 1716, 295, 1242, 300, 2857, 1958, 13, 16, 4, 14170, 13], "temperature": 0.0, "avg_logprob": -0.18663425445556642, "compression_ratio": 1.3636363636363635, "no_speech_prob": 6.438965556299081e-06}, {"id": 234, "seek": 117548, "start": 1187.32, "end": 1197.84, "text": " There are some suggestions on how to do that in this paper.", "tokens": [821, 366, 512, 13396, 322, 577, 281, 360, 300, 294, 341, 3035, 13], "temperature": 0.0, "avg_logprob": -0.18663425445556642, "compression_ratio": 1.3636363636363635, "no_speech_prob": 6.438965556299081e-06}, {"id": 235, "seek": 119784, "start": 1197.84, "end": 1211.3999999999999, "text": " So we looked at a visual vocabulary that we developed for writing down neural nets where", "tokens": [407, 321, 2956, 412, 257, 5056, 19864, 300, 321, 4743, 337, 3579, 760, 18161, 36170, 689], "temperature": 0.0, "avg_logprob": -0.2391521453857422, "compression_ratio": 1.6319444444444444, "no_speech_prob": 8.530224476999138e-06}, {"id": 236, "seek": 119784, "start": 1211.3999999999999, "end": 1215.84, "text": " any colored box represents a matrix of activations.", "tokens": [604, 14332, 2424, 8855, 257, 8141, 295, 2430, 763, 13], "temperature": 0.0, "avg_logprob": -0.2391521453857422, "compression_ratio": 1.6319444444444444, "no_speech_prob": 8.530224476999138e-06}, {"id": 237, "seek": 119784, "start": 1215.84, "end": 1218.4399999999998, "text": " That's a really important point to remember.", "tokens": [663, 311, 257, 534, 1021, 935, 281, 1604, 13], "temperature": 0.0, "avg_logprob": -0.2391521453857422, "compression_ratio": 1.6319444444444444, "no_speech_prob": 8.530224476999138e-06}, {"id": 238, "seek": 119784, "start": 1218.4399999999998, "end": 1222.6, "text": " A colored box represents a matrix of activations.", "tokens": [316, 14332, 2424, 8855, 257, 8141, 295, 2430, 763, 13], "temperature": 0.0, "avg_logprob": -0.2391521453857422, "compression_ratio": 1.6319444444444444, "no_speech_prob": 8.530224476999138e-06}, {"id": 239, "seek": 122260, "start": 1222.6, "end": 1228.12, "text": " So it could either be the input matrix, it could be the output matrix, or it could be", "tokens": [407, 309, 727, 2139, 312, 264, 4846, 8141, 11, 309, 727, 312, 264, 5598, 8141, 11, 420, 309, 727, 312], "temperature": 0.0, "avg_logprob": -0.17421581320566673, "compression_ratio": 1.7796610169491525, "no_speech_prob": 4.637843630916905e-06}, {"id": 240, "seek": 122260, "start": 1228.12, "end": 1237.3799999999999, "text": " the matrix that comes from taking an input and putting it through like a matrix product.", "tokens": [264, 8141, 300, 1487, 490, 1940, 364, 4846, 293, 3372, 309, 807, 411, 257, 8141, 1674, 13], "temperature": 0.0, "avg_logprob": -0.17421581320566673, "compression_ratio": 1.7796610169491525, "no_speech_prob": 4.637843630916905e-06}, {"id": 241, "seek": 122260, "start": 1237.3799999999999, "end": 1240.48, "text": " The rectangle boxes represent inputs.", "tokens": [440, 21930, 9002, 2906, 15743, 13], "temperature": 0.0, "avg_logprob": -0.17421581320566673, "compression_ratio": 1.7796610169491525, "no_speech_prob": 4.637843630916905e-06}, {"id": 242, "seek": 122260, "start": 1240.48, "end": 1246.52, "text": " The circular ones represent hidden, so intermediate, activations.", "tokens": [440, 16476, 2306, 2906, 7633, 11, 370, 19376, 11, 2430, 763, 13], "temperature": 0.0, "avg_logprob": -0.17421581320566673, "compression_ratio": 1.7796610169491525, "no_speech_prob": 4.637843630916905e-06}, {"id": 243, "seek": 122260, "start": 1246.52, "end": 1249.12, "text": " And the triangles represent outputs.", "tokens": [400, 264, 29896, 2906, 23930, 13], "temperature": 0.0, "avg_logprob": -0.17421581320566673, "compression_ratio": 1.7796610169491525, "no_speech_prob": 4.637843630916905e-06}, {"id": 244, "seek": 124912, "start": 1249.12, "end": 1254.84, "text": " Arrows, very importantly, represent what we'll call layer operations.", "tokens": [1587, 28251, 11, 588, 8906, 11, 2906, 437, 321, 603, 818, 4583, 7705, 13], "temperature": 0.0, "avg_logprob": -0.1762255350748698, "compression_ratio": 1.6711111111111112, "no_speech_prob": 8.664568667882122e-06}, {"id": 245, "seek": 124912, "start": 1254.84, "end": 1258.6, "text": " And a layer operation is anything that you do to one colored box to create another colored", "tokens": [400, 257, 4583, 6916, 307, 1340, 300, 291, 360, 281, 472, 14332, 2424, 281, 1884, 1071, 14332], "temperature": 0.0, "avg_logprob": -0.1762255350748698, "compression_ratio": 1.6711111111111112, "no_speech_prob": 8.664568667882122e-06}, {"id": 246, "seek": 124912, "start": 1258.6, "end": 1259.6, "text": " box.", "tokens": [2424, 13], "temperature": 0.0, "avg_logprob": -0.1762255350748698, "compression_ratio": 1.6711111111111112, "no_speech_prob": 8.664568667882122e-06}, {"id": 247, "seek": 124912, "start": 1259.6, "end": 1263.56, "text": " And in general, it's almost always going to involve some kind of linear function like", "tokens": [400, 294, 2674, 11, 309, 311, 1920, 1009, 516, 281, 9494, 512, 733, 295, 8213, 2445, 411], "temperature": 0.0, "avg_logprob": -0.1762255350748698, "compression_ratio": 1.6711111111111112, "no_speech_prob": 8.664568667882122e-06}, {"id": 248, "seek": 124912, "start": 1263.56, "end": 1266.32, "text": " a matrix product or a convolution.", "tokens": [257, 8141, 1674, 420, 257, 45216, 13], "temperature": 0.0, "avg_logprob": -0.1762255350748698, "compression_ratio": 1.6711111111111112, "no_speech_prob": 8.664568667882122e-06}, {"id": 249, "seek": 124912, "start": 1266.32, "end": 1274.04, "text": " And it will probably also include some kind of activation function like ReLU or softness.", "tokens": [400, 309, 486, 1391, 611, 4090, 512, 733, 295, 24433, 2445, 411, 1300, 43, 52, 420, 2787, 1287, 13], "temperature": 0.0, "avg_logprob": -0.1762255350748698, "compression_ratio": 1.6711111111111112, "no_speech_prob": 8.664568667882122e-06}, {"id": 250, "seek": 127404, "start": 1274.04, "end": 1279.6399999999999, "text": " Because the activation functions are pretty unimportant in terms of detail, I started", "tokens": [1436, 264, 24433, 6828, 366, 1238, 517, 41654, 294, 2115, 295, 2607, 11, 286, 1409], "temperature": 0.0, "avg_logprob": -0.16472529490059667, "compression_ratio": 1.8470588235294119, "no_speech_prob": 1.6187364963116124e-05}, {"id": 251, "seek": 127404, "start": 1279.6399999999999, "end": 1285.6399999999999, "text": " removing those from the pictures as we started to look at more complex models.", "tokens": [12720, 729, 490, 264, 5242, 382, 321, 1409, 281, 574, 412, 544, 3997, 5245, 13], "temperature": 0.0, "avg_logprob": -0.16472529490059667, "compression_ratio": 1.8470588235294119, "no_speech_prob": 1.6187364963116124e-05}, {"id": 252, "seek": 127404, "start": 1285.6399999999999, "end": 1289.68, "text": " And then in fact, because the layer operations are pretty consistent, we probably know what", "tokens": [400, 550, 294, 1186, 11, 570, 264, 4583, 7705, 366, 1238, 8398, 11, 321, 1391, 458, 437], "temperature": 0.0, "avg_logprob": -0.16472529490059667, "compression_ratio": 1.8470588235294119, "no_speech_prob": 1.6187364963116124e-05}, {"id": 253, "seek": 127404, "start": 1289.68, "end": 1295.62, "text": " they are, I started removing those as well, just to keep these simple.", "tokens": [436, 366, 11, 286, 1409, 12720, 729, 382, 731, 11, 445, 281, 1066, 613, 2199, 13], "temperature": 0.0, "avg_logprob": -0.16472529490059667, "compression_ratio": 1.8470588235294119, "no_speech_prob": 1.6187364963116124e-05}, {"id": 254, "seek": 127404, "start": 1295.62, "end": 1300.26, "text": " And so we're simplifying these diagrams to try and just keep the main pieces.", "tokens": [400, 370, 321, 434, 6883, 5489, 613, 36709, 281, 853, 293, 445, 1066, 264, 2135, 3755, 13], "temperature": 0.0, "avg_logprob": -0.16472529490059667, "compression_ratio": 1.8470588235294119, "no_speech_prob": 1.6187364963116124e-05}, {"id": 255, "seek": 127404, "start": 1300.26, "end": 1303.58, "text": " And as we did so, we could start to create more complex diagrams.", "tokens": [400, 382, 321, 630, 370, 11, 321, 727, 722, 281, 1884, 544, 3997, 36709, 13], "temperature": 0.0, "avg_logprob": -0.16472529490059667, "compression_ratio": 1.8470588235294119, "no_speech_prob": 1.6187364963116124e-05}, {"id": 256, "seek": 130358, "start": 1303.58, "end": 1310.84, "text": " And so we talked about a kind of language model where we would take inputs of a character,", "tokens": [400, 370, 321, 2825, 466, 257, 733, 295, 2856, 2316, 689, 321, 576, 747, 15743, 295, 257, 2517, 11], "temperature": 0.0, "avg_logprob": -0.18935825513756793, "compression_ratio": 1.7980769230769231, "no_speech_prob": 5.507567948370706e-06}, {"id": 257, "seek": 130358, "start": 1310.84, "end": 1315.36, "text": " character number 1 and character number 2, and we would try and predict character number", "tokens": [2517, 1230, 502, 293, 2517, 1230, 568, 11, 293, 321, 576, 853, 293, 6069, 2517, 1230], "temperature": 0.0, "avg_logprob": -0.18935825513756793, "compression_ratio": 1.7980769230769231, "no_speech_prob": 5.507567948370706e-06}, {"id": 258, "seek": 130358, "start": 1315.36, "end": 1317.4399999999998, "text": " 3.", "tokens": [805, 13], "temperature": 0.0, "avg_logprob": -0.18935825513756793, "compression_ratio": 1.7980769230769231, "no_speech_prob": 5.507567948370706e-06}, {"id": 259, "seek": 130358, "start": 1317.4399999999998, "end": 1323.6399999999999, "text": " And so we thought one way to do that would be to create a deep neural network with 2", "tokens": [400, 370, 321, 1194, 472, 636, 281, 360, 300, 576, 312, 281, 1884, 257, 2452, 18161, 3209, 365, 568], "temperature": 0.0, "avg_logprob": -0.18935825513756793, "compression_ratio": 1.7980769230769231, "no_speech_prob": 5.507567948370706e-06}, {"id": 260, "seek": 130358, "start": 1323.6399999999999, "end": 1325.12, "text": " layers.", "tokens": [7914, 13], "temperature": 0.0, "avg_logprob": -0.18935825513756793, "compression_ratio": 1.7980769230769231, "no_speech_prob": 5.507567948370706e-06}, {"id": 261, "seek": 130358, "start": 1325.12, "end": 1330.32, "text": " The character1 input would go through a layer operation to create our first fully connected", "tokens": [440, 2517, 16, 4846, 576, 352, 807, 257, 4583, 6916, 281, 1884, 527, 700, 4498, 4582], "temperature": 0.0, "avg_logprob": -0.18935825513756793, "compression_ratio": 1.7980769230769231, "no_speech_prob": 5.507567948370706e-06}, {"id": 262, "seek": 130358, "start": 1330.32, "end": 1332.0, "text": " layer.", "tokens": [4583, 13], "temperature": 0.0, "avg_logprob": -0.18935825513756793, "compression_ratio": 1.7980769230769231, "no_speech_prob": 5.507567948370706e-06}, {"id": 263, "seek": 133200, "start": 1332.0, "end": 1335.88, "text": " That would go through another layer operation to create a second fully connected layer.", "tokens": [663, 576, 352, 807, 1071, 4583, 6916, 281, 1884, 257, 1150, 4498, 4582, 4583, 13], "temperature": 0.0, "avg_logprob": -0.11975737315852468, "compression_ratio": 1.7452830188679245, "no_speech_prob": 3.3931212328752736e-06}, {"id": 264, "seek": 133200, "start": 1335.88, "end": 1341.64, "text": " And we would also add our second character input going through its own fully connected", "tokens": [400, 321, 576, 611, 909, 527, 1150, 2517, 4846, 516, 807, 1080, 1065, 4498, 4582], "temperature": 0.0, "avg_logprob": -0.11975737315852468, "compression_ratio": 1.7452830188679245, "no_speech_prob": 3.3931212328752736e-06}, {"id": 265, "seek": 133200, "start": 1341.64, "end": 1343.48, "text": " layer at this point.", "tokens": [4583, 412, 341, 935, 13], "temperature": 0.0, "avg_logprob": -0.11975737315852468, "compression_ratio": 1.7452830188679245, "no_speech_prob": 3.3931212328752736e-06}, {"id": 266, "seek": 133200, "start": 1343.48, "end": 1348.2, "text": " And to recall, the last important thing we have to learn is that 2 arrows going into", "tokens": [400, 281, 9901, 11, 264, 1036, 1021, 551, 321, 362, 281, 1466, 307, 300, 568, 19669, 516, 666], "temperature": 0.0, "avg_logprob": -0.11975737315852468, "compression_ratio": 1.7452830188679245, "no_speech_prob": 3.3931212328752736e-06}, {"id": 267, "seek": 133200, "start": 1348.2, "end": 1354.0, "text": " a single shape means that we are adding the results of those 2 layer operations together.", "tokens": [257, 2167, 3909, 1355, 300, 321, 366, 5127, 264, 3542, 295, 729, 568, 4583, 7705, 1214, 13], "temperature": 0.0, "avg_logprob": -0.11975737315852468, "compression_ratio": 1.7452830188679245, "no_speech_prob": 3.3931212328752736e-06}, {"id": 268, "seek": 135400, "start": 1354.0, "end": 1362.12, "text": " So 2 arrows going into a shape represents summing up element-wise the results of these", "tokens": [407, 568, 19669, 516, 666, 257, 3909, 8855, 2408, 2810, 493, 4478, 12, 3711, 264, 3542, 295, 613], "temperature": 0.0, "avg_logprob": -0.16606733410857444, "compression_ratio": 1.528888888888889, "no_speech_prob": 4.289309799787588e-06}, {"id": 269, "seek": 135400, "start": 1362.12, "end": 1365.48, "text": " 2 layer operations.", "tokens": [568, 4583, 7705, 13], "temperature": 0.0, "avg_logprob": -0.16606733410857444, "compression_ratio": 1.528888888888889, "no_speech_prob": 4.289309799787588e-06}, {"id": 270, "seek": 135400, "start": 1365.48, "end": 1371.08, "text": " So this was the little visual vocabulary that we set up last week.", "tokens": [407, 341, 390, 264, 707, 5056, 19864, 300, 321, 992, 493, 1036, 1243, 13], "temperature": 0.0, "avg_logprob": -0.16606733410857444, "compression_ratio": 1.528888888888889, "no_speech_prob": 4.289309799787588e-06}, {"id": 271, "seek": 135400, "start": 1371.08, "end": 1376.78, "text": " And I've kept track of it down here as to what the things are in case you forget.", "tokens": [400, 286, 600, 4305, 2837, 295, 309, 760, 510, 382, 281, 437, 264, 721, 366, 294, 1389, 291, 2870, 13], "temperature": 0.0, "avg_logprob": -0.16606733410857444, "compression_ratio": 1.528888888888889, "no_speech_prob": 4.289309799787588e-06}, {"id": 272, "seek": 135400, "start": 1376.78, "end": 1383.78, "text": " So now I wanted to point out something really interesting, which is that there's 3 kinds", "tokens": [407, 586, 286, 1415, 281, 935, 484, 746, 534, 1880, 11, 597, 307, 300, 456, 311, 805, 3685], "temperature": 0.0, "avg_logprob": -0.16606733410857444, "compression_ratio": 1.528888888888889, "no_speech_prob": 4.289309799787588e-06}, {"id": 273, "seek": 138378, "start": 1383.78, "end": 1387.44, "text": " of layer operations going on.", "tokens": [295, 4583, 7705, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.193056369173354, "compression_ratio": 1.518918918918919, "no_speech_prob": 1.4970893062127288e-05}, {"id": 274, "seek": 138378, "start": 1387.44, "end": 1388.44, "text": " Here I'm expanding this now.", "tokens": [1692, 286, 478, 14702, 341, 586, 13], "temperature": 0.0, "avg_logprob": -0.193056369173354, "compression_ratio": 1.518918918918919, "no_speech_prob": 1.4970893062127288e-05}, {"id": 275, "seek": 138378, "start": 1388.44, "end": 1393.8799999999999, "text": " We've got predicting a 4th character of a sequence using characters 1, 2 and 3 using", "tokens": [492, 600, 658, 32884, 257, 1017, 392, 2517, 295, 257, 8310, 1228, 4342, 502, 11, 568, 293, 805, 1228], "temperature": 0.0, "avg_logprob": -0.193056369173354, "compression_ratio": 1.518918918918919, "no_speech_prob": 1.4970893062127288e-05}, {"id": 276, "seek": 138378, "start": 1393.8799999999999, "end": 1398.6399999999999, "text": " exactly the same method as on the previous slide.", "tokens": [2293, 264, 912, 3170, 382, 322, 264, 3894, 4137, 13], "temperature": 0.0, "avg_logprob": -0.193056369173354, "compression_ratio": 1.518918918918919, "no_speech_prob": 1.4970893062127288e-05}, {"id": 277, "seek": 138378, "start": 1398.6399999999999, "end": 1409.52, "text": " There are layer operations that turn a character input into a hidden activation matrix.", "tokens": [821, 366, 4583, 7705, 300, 1261, 257, 2517, 4846, 666, 257, 7633, 24433, 8141, 13], "temperature": 0.0, "avg_logprob": -0.193056369173354, "compression_ratio": 1.518918918918919, "no_speech_prob": 1.4970893062127288e-05}, {"id": 278, "seek": 140952, "start": 1409.52, "end": 1416.96, "text": " There are layer operations that turn one hidden layer activation into a new hidden layer activation.", "tokens": [821, 366, 4583, 7705, 300, 1261, 472, 7633, 4583, 24433, 666, 257, 777, 7633, 4583, 24433, 13], "temperature": 0.0, "avg_logprob": -0.17724684874216715, "compression_ratio": 1.9178743961352658, "no_speech_prob": 1.8738661538009183e-06}, {"id": 279, "seek": 140952, "start": 1416.96, "end": 1423.4, "text": " And then there's an operation that takes hidden activations and turns it into output activations.", "tokens": [400, 550, 456, 311, 364, 6916, 300, 2516, 7633, 2430, 763, 293, 4523, 309, 666, 5598, 2430, 763, 13], "temperature": 0.0, "avg_logprob": -0.17724684874216715, "compression_ratio": 1.9178743961352658, "no_speech_prob": 1.8738661538009183e-06}, {"id": 280, "seek": 140952, "start": 1423.4, "end": 1425.12, "text": " And so you can see here I've colored them in.", "tokens": [400, 370, 291, 393, 536, 510, 286, 600, 14332, 552, 294, 13], "temperature": 0.0, "avg_logprob": -0.17724684874216715, "compression_ratio": 1.9178743961352658, "no_speech_prob": 1.8738661538009183e-06}, {"id": 281, "seek": 140952, "start": 1425.12, "end": 1429.28, "text": " Here I've got a little legend of these different colors.", "tokens": [1692, 286, 600, 658, 257, 707, 9451, 295, 613, 819, 4577, 13], "temperature": 0.0, "avg_logprob": -0.17724684874216715, "compression_ratio": 1.9178743961352658, "no_speech_prob": 1.8738661538009183e-06}, {"id": 282, "seek": 140952, "start": 1429.28, "end": 1434.6399999999999, "text": " Green is the input to hidden, blue is the hidden to output, and orange is the hidden", "tokens": [6969, 307, 264, 4846, 281, 7633, 11, 3344, 307, 264, 7633, 281, 5598, 11, 293, 7671, 307, 264, 7633], "temperature": 0.0, "avg_logprob": -0.17724684874216715, "compression_ratio": 1.9178743961352658, "no_speech_prob": 1.8738661538009183e-06}, {"id": 283, "seek": 140952, "start": 1434.6399999999999, "end": 1436.6, "text": " to hidden.", "tokens": [281, 7633, 13], "temperature": 0.0, "avg_logprob": -0.17724684874216715, "compression_ratio": 1.9178743961352658, "no_speech_prob": 1.8738661538009183e-06}, {"id": 284, "seek": 143660, "start": 1436.6, "end": 1443.3999999999999, "text": " So my claim is that the dimensions of the weight matrices for each of these different", "tokens": [407, 452, 3932, 307, 300, 264, 12819, 295, 264, 3364, 32284, 337, 1184, 295, 613, 819], "temperature": 0.0, "avg_logprob": -0.18894701366183125, "compression_ratio": 1.7676767676767677, "no_speech_prob": 7.411176738969516e-06}, {"id": 285, "seek": 143660, "start": 1443.3999999999999, "end": 1447.54, "text": " colored arrows, all of the green ones have the same dimensions because they're taking", "tokens": [14332, 19669, 11, 439, 295, 264, 3092, 2306, 362, 264, 912, 12819, 570, 436, 434, 1940], "temperature": 0.0, "avg_logprob": -0.18894701366183125, "compression_ratio": 1.7676767676767677, "no_speech_prob": 7.411176738969516e-06}, {"id": 286, "seek": 143660, "start": 1447.54, "end": 1455.8799999999999, "text": " an input of vocab size and turning it into an output hidden activation of size number", "tokens": [364, 4846, 295, 2329, 455, 2744, 293, 6246, 309, 666, 364, 5598, 7633, 24433, 295, 2744, 1230], "temperature": 0.0, "avg_logprob": -0.18894701366183125, "compression_ratio": 1.7676767676767677, "no_speech_prob": 7.411176738969516e-06}, {"id": 287, "seek": 143660, "start": 1455.8799999999999, "end": 1457.4199999999998, "text": " of activations.", "tokens": [295, 2430, 763, 13], "temperature": 0.0, "avg_logprob": -0.18894701366183125, "compression_ratio": 1.7676767676767677, "no_speech_prob": 7.411176738969516e-06}, {"id": 288, "seek": 143660, "start": 1457.4199999999998, "end": 1462.6399999999999, "text": " So all of these arrows represent weight matrices of the same dimensionality.", "tokens": [407, 439, 295, 613, 19669, 2906, 3364, 32284, 295, 264, 912, 10139, 1860, 13], "temperature": 0.0, "avg_logprob": -0.18894701366183125, "compression_ratio": 1.7676767676767677, "no_speech_prob": 7.411176738969516e-06}, {"id": 289, "seek": 146264, "start": 1462.64, "end": 1468.8400000000001, "text": " Ditto, the orange arrows represent weight matrices of the same dimensionality.", "tokens": [413, 34924, 11, 264, 7671, 19669, 2906, 3364, 32284, 295, 264, 912, 10139, 1860, 13], "temperature": 0.0, "avg_logprob": -0.1357352336247762, "compression_ratio": 1.933649289099526, "no_speech_prob": 4.2893257159448694e-06}, {"id": 290, "seek": 146264, "start": 1468.8400000000001, "end": 1473.96, "text": " I would go further than that though and say the green arrows represent semantically the", "tokens": [286, 576, 352, 3052, 813, 300, 1673, 293, 584, 264, 3092, 19669, 2906, 4361, 49505, 264], "temperature": 0.0, "avg_logprob": -0.1357352336247762, "compression_ratio": 1.933649289099526, "no_speech_prob": 4.2893257159448694e-06}, {"id": 291, "seek": 146264, "start": 1473.96, "end": 1474.96, "text": " same thing.", "tokens": [912, 551, 13], "temperature": 0.0, "avg_logprob": -0.1357352336247762, "compression_ratio": 1.933649289099526, "no_speech_prob": 4.2893257159448694e-06}, {"id": 292, "seek": 146264, "start": 1474.96, "end": 1481.24, "text": " They're all saying how do you take a character and convert it into a hidden state.", "tokens": [814, 434, 439, 1566, 577, 360, 291, 747, 257, 2517, 293, 7620, 309, 666, 257, 7633, 1785, 13], "temperature": 0.0, "avg_logprob": -0.1357352336247762, "compression_ratio": 1.933649289099526, "no_speech_prob": 4.2893257159448694e-06}, {"id": 293, "seek": 146264, "start": 1481.24, "end": 1486.2800000000002, "text": " And the orange arrows are all saying how do you take a hidden state from a previous character", "tokens": [400, 264, 7671, 19669, 366, 439, 1566, 577, 360, 291, 747, 257, 7633, 1785, 490, 257, 3894, 2517], "temperature": 0.0, "avg_logprob": -0.1357352336247762, "compression_ratio": 1.933649289099526, "no_speech_prob": 4.2893257159448694e-06}, {"id": 294, "seek": 146264, "start": 1486.2800000000002, "end": 1489.72, "text": " and turn it into a hidden state for a new character.", "tokens": [293, 1261, 309, 666, 257, 7633, 1785, 337, 257, 777, 2517, 13], "temperature": 0.0, "avg_logprob": -0.1357352336247762, "compression_ratio": 1.933649289099526, "no_speech_prob": 4.2893257159448694e-06}, {"id": 295, "seek": 148972, "start": 1489.72, "end": 1496.0, "text": " And then the blue one is saying how do you take a hidden state and turn it into outputs.", "tokens": [400, 550, 264, 3344, 472, 307, 1566, 577, 360, 291, 747, 257, 7633, 1785, 293, 1261, 309, 666, 23930, 13], "temperature": 0.0, "avg_logprob": -0.13606139716752078, "compression_ratio": 1.7203065134099618, "no_speech_prob": 3.3931326015590457e-06}, {"id": 296, "seek": 148972, "start": 1496.0, "end": 1500.32, "text": " When you look at it that way, all of these circles are basically the same thing.", "tokens": [1133, 291, 574, 412, 309, 300, 636, 11, 439, 295, 613, 13040, 366, 1936, 264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.13606139716752078, "compression_ratio": 1.7203065134099618, "no_speech_prob": 3.3931326015590457e-06}, {"id": 297, "seek": 148972, "start": 1500.32, "end": 1505.24, "text": " They're just representing this hidden state at a different point in time.", "tokens": [814, 434, 445, 13460, 341, 7633, 1785, 412, 257, 819, 935, 294, 565, 13], "temperature": 0.0, "avg_logprob": -0.13606139716752078, "compression_ratio": 1.7203065134099618, "no_speech_prob": 3.3931326015590457e-06}, {"id": 298, "seek": 148972, "start": 1505.24, "end": 1508.48, "text": " And I'm going to use this word time in a fairly general way.", "tokens": [400, 286, 478, 516, 281, 764, 341, 1349, 565, 294, 257, 6457, 2674, 636, 13], "temperature": 0.0, "avg_logprob": -0.13606139716752078, "compression_ratio": 1.7203065134099618, "no_speech_prob": 3.3931326015590457e-06}, {"id": 299, "seek": 148972, "start": 1508.48, "end": 1512.3600000000001, "text": " I'm not really talking about time, I'm just talking about the sequence in which we're", "tokens": [286, 478, 406, 534, 1417, 466, 565, 11, 286, 478, 445, 1417, 466, 264, 8310, 294, 597, 321, 434], "temperature": 0.0, "avg_logprob": -0.13606139716752078, "compression_ratio": 1.7203065134099618, "no_speech_prob": 3.3931326015590457e-06}, {"id": 300, "seek": 148972, "start": 1512.3600000000001, "end": 1515.3600000000001, "text": " presenting additional pieces of information to this model.", "tokens": [15578, 4497, 3755, 295, 1589, 281, 341, 2316, 13], "temperature": 0.0, "avg_logprob": -0.13606139716752078, "compression_ratio": 1.7203065134099618, "no_speech_prob": 3.3931326015590457e-06}, {"id": 301, "seek": 151536, "start": 1515.36, "end": 1521.08, "text": " We first of all present the first character, then the second character, and then the third", "tokens": [492, 700, 295, 439, 1974, 264, 700, 2517, 11, 550, 264, 1150, 2517, 11, 293, 550, 264, 2636], "temperature": 0.0, "avg_logprob": -0.1827662785847982, "compression_ratio": 1.6338028169014085, "no_speech_prob": 1.165941466751974e-05}, {"id": 302, "seek": 151536, "start": 1521.08, "end": 1522.08, "text": " character.", "tokens": [2517, 13], "temperature": 0.0, "avg_logprob": -0.1827662785847982, "compression_ratio": 1.6338028169014085, "no_speech_prob": 1.165941466751974e-05}, {"id": 303, "seek": 151536, "start": 1522.08, "end": 1528.4399999999998, "text": " So we could redraw this whole thing in a simpler way and a more general way.", "tokens": [407, 321, 727, 2182, 5131, 341, 1379, 551, 294, 257, 18587, 636, 293, 257, 544, 2674, 636, 13], "temperature": 0.0, "avg_logprob": -0.1827662785847982, "compression_ratio": 1.6338028169014085, "no_speech_prob": 1.165941466751974e-05}, {"id": 304, "seek": 151536, "start": 1528.4399999999998, "end": 1535.6799999999998, "text": " Before we do, I'm actually going to show you in Keras how to build this model.", "tokens": [4546, 321, 360, 11, 286, 478, 767, 516, 281, 855, 291, 294, 591, 6985, 577, 281, 1322, 341, 2316, 13], "temperature": 0.0, "avg_logprob": -0.1827662785847982, "compression_ratio": 1.6338028169014085, "no_speech_prob": 1.165941466751974e-05}, {"id": 305, "seek": 151536, "start": 1535.6799999999998, "end": 1539.52, "text": " And in doing so, we're going to learn a bit more about the functional API, which hopefully", "tokens": [400, 294, 884, 370, 11, 321, 434, 516, 281, 1466, 257, 857, 544, 466, 264, 11745, 9362, 11, 597, 4696], "temperature": 0.0, "avg_logprob": -0.1827662785847982, "compression_ratio": 1.6338028169014085, "no_speech_prob": 1.165941466751974e-05}, {"id": 306, "seek": 153952, "start": 1539.52, "end": 1547.28, "text": " you'll find pretty interesting and useful.", "tokens": [291, 603, 915, 1238, 1880, 293, 4420, 13], "temperature": 0.0, "avg_logprob": -0.18424401744719474, "compression_ratio": 1.4342105263157894, "no_speech_prob": 7.646475751243997e-06}, {"id": 307, "seek": 153952, "start": 1547.28, "end": 1554.08, "text": " To do that, we are going to use this corpus of all of the collected works of Nietzsche.", "tokens": [1407, 360, 300, 11, 321, 366, 516, 281, 764, 341, 1181, 31624, 295, 439, 295, 264, 11087, 1985, 295, 36583, 89, 12287, 13], "temperature": 0.0, "avg_logprob": -0.18424401744719474, "compression_ratio": 1.4342105263157894, "no_speech_prob": 7.646475751243997e-06}, {"id": 308, "seek": 153952, "start": 1554.08, "end": 1562.72, "text": " So we load in those works, we find all of the unique characters, of which there are", "tokens": [407, 321, 3677, 294, 729, 1985, 11, 321, 915, 439, 295, 264, 3845, 4342, 11, 295, 597, 456, 366], "temperature": 0.0, "avg_logprob": -0.18424401744719474, "compression_ratio": 1.4342105263157894, "no_speech_prob": 7.646475751243997e-06}, {"id": 309, "seek": 153952, "start": 1562.72, "end": 1563.72, "text": " 86.", "tokens": [26687, 13], "temperature": 0.0, "avg_logprob": -0.18424401744719474, "compression_ratio": 1.4342105263157894, "no_speech_prob": 7.646475751243997e-06}, {"id": 310, "seek": 156372, "start": 1563.72, "end": 1572.68, "text": " Then we create a mapping from the character to the index at which it appears in this list,", "tokens": [1396, 321, 1884, 257, 18350, 490, 264, 2517, 281, 264, 8186, 412, 597, 309, 7038, 294, 341, 1329, 11], "temperature": 0.0, "avg_logprob": -0.15676008511896003, "compression_ratio": 1.6444444444444444, "no_speech_prob": 1.7502481568953954e-05}, {"id": 311, "seek": 156372, "start": 1572.68, "end": 1575.76, "text": " and a mapping from the index to the character.", "tokens": [293, 257, 18350, 490, 264, 8186, 281, 264, 2517, 13], "temperature": 0.0, "avg_logprob": -0.15676008511896003, "compression_ratio": 1.6444444444444444, "no_speech_prob": 1.7502481568953954e-05}, {"id": 312, "seek": 156372, "start": 1575.76, "end": 1584.3600000000001, "text": " So this is basically creating the equivalent of these tables, or more specifically, I guess,", "tokens": [407, 341, 307, 1936, 4084, 264, 10344, 295, 613, 8020, 11, 420, 544, 4682, 11, 286, 2041, 11], "temperature": 0.0, "avg_logprob": -0.15676008511896003, "compression_ratio": 1.6444444444444444, "no_speech_prob": 1.7502481568953954e-05}, {"id": 313, "seek": 156372, "start": 1584.3600000000001, "end": 1585.3600000000001, "text": " this table.", "tokens": [341, 3199, 13], "temperature": 0.0, "avg_logprob": -0.15676008511896003, "compression_ratio": 1.6444444444444444, "no_speech_prob": 1.7502481568953954e-05}, {"id": 314, "seek": 156372, "start": 1585.3600000000001, "end": 1589.44, "text": " Rather than using words, we're looking at characters.", "tokens": [16571, 813, 1228, 2283, 11, 321, 434, 1237, 412, 4342, 13], "temperature": 0.0, "avg_logprob": -0.15676008511896003, "compression_ratio": 1.6444444444444444, "no_speech_prob": 1.7502481568953954e-05}, {"id": 315, "seek": 158944, "start": 1589.44, "end": 1597.3600000000001, "text": " So that allows us to take the text of Nietzsche and convert it into a list of numbers where", "tokens": [407, 300, 4045, 505, 281, 747, 264, 2487, 295, 36583, 89, 12287, 293, 7620, 309, 666, 257, 1329, 295, 3547, 689], "temperature": 0.0, "avg_logprob": -0.1485475974507851, "compression_ratio": 1.7186147186147187, "no_speech_prob": 8.939533472585026e-06}, {"id": 316, "seek": 158944, "start": 1597.3600000000001, "end": 1602.24, "text": " the numbers represent the number at which the character appears in this list.", "tokens": [264, 3547, 2906, 264, 1230, 412, 597, 264, 2517, 7038, 294, 341, 1329, 13], "temperature": 0.0, "avg_logprob": -0.1485475974507851, "compression_ratio": 1.7186147186147187, "no_speech_prob": 8.939533472585026e-06}, {"id": 317, "seek": 158944, "start": 1602.24, "end": 1605.3600000000001, "text": " So here are the first 10.", "tokens": [407, 510, 366, 264, 700, 1266, 13], "temperature": 0.0, "avg_logprob": -0.1485475974507851, "compression_ratio": 1.7186147186147187, "no_speech_prob": 8.939533472585026e-06}, {"id": 318, "seek": 158944, "start": 1605.3600000000001, "end": 1611.0800000000002, "text": " So at any point we can turn this, that's called idx, so we've converted our whole text into", "tokens": [407, 412, 604, 935, 321, 393, 1261, 341, 11, 300, 311, 1219, 4496, 87, 11, 370, 321, 600, 16424, 527, 1379, 2487, 666], "temperature": 0.0, "avg_logprob": -0.1485475974507851, "compression_ratio": 1.7186147186147187, "no_speech_prob": 8.939533472585026e-06}, {"id": 319, "seek": 158944, "start": 1611.0800000000002, "end": 1614.0800000000002, "text": " the equivalent indices.", "tokens": [264, 10344, 43840, 13], "temperature": 0.0, "avg_logprob": -0.1485475974507851, "compression_ratio": 1.7186147186147187, "no_speech_prob": 8.939533472585026e-06}, {"id": 320, "seek": 158944, "start": 1614.0800000000002, "end": 1618.72, "text": " At any point we can turn it back into text by simply taking those indexes and looking", "tokens": [1711, 604, 935, 321, 393, 1261, 309, 646, 666, 2487, 538, 2935, 1940, 729, 8186, 279, 293, 1237], "temperature": 0.0, "avg_logprob": -0.1485475974507851, "compression_ratio": 1.7186147186147187, "no_speech_prob": 8.939533472585026e-06}, {"id": 321, "seek": 161872, "start": 1618.72, "end": 1621.24, "text": " them up in our index to character mapping.", "tokens": [552, 493, 294, 527, 8186, 281, 2517, 18350, 13], "temperature": 0.0, "avg_logprob": -0.18406747817993163, "compression_ratio": 1.7466666666666666, "no_speech_prob": 8.800982868706342e-06}, {"id": 322, "seek": 161872, "start": 1621.24, "end": 1627.1200000000001, "text": " So here you can see we've turned it back into the start of the text again.", "tokens": [407, 510, 291, 393, 536, 321, 600, 3574, 309, 646, 666, 264, 722, 295, 264, 2487, 797, 13], "temperature": 0.0, "avg_logprob": -0.18406747817993163, "compression_ratio": 1.7466666666666666, "no_speech_prob": 8.800982868706342e-06}, {"id": 323, "seek": 161872, "start": 1627.1200000000001, "end": 1628.32, "text": " So that's the data we're working with.", "tokens": [407, 300, 311, 264, 1412, 321, 434, 1364, 365, 13], "temperature": 0.0, "avg_logprob": -0.18406747817993163, "compression_ratio": 1.7466666666666666, "no_speech_prob": 8.800982868706342e-06}, {"id": 324, "seek": 161872, "start": 1628.32, "end": 1633.64, "text": " The data we're working with is a list of character ids at this point, where those character ids", "tokens": [440, 1412, 321, 434, 1364, 365, 307, 257, 1329, 295, 2517, 220, 3742, 412, 341, 935, 11, 689, 729, 2517, 220, 3742], "temperature": 0.0, "avg_logprob": -0.18406747817993163, "compression_ratio": 1.7466666666666666, "no_speech_prob": 8.800982868706342e-06}, {"id": 325, "seek": 161872, "start": 1633.64, "end": 1638.08, "text": " represent the collected works of Nietzsche.", "tokens": [2906, 264, 11087, 1985, 295, 36583, 89, 12287, 13], "temperature": 0.0, "avg_logprob": -0.18406747817993163, "compression_ratio": 1.7466666666666666, "no_speech_prob": 8.800982868706342e-06}, {"id": 326, "seek": 161872, "start": 1638.08, "end": 1648.4, "text": " So we're going to build a model which attempts to predict the fourth character from the previous", "tokens": [407, 321, 434, 516, 281, 1322, 257, 2316, 597, 15257, 281, 6069, 264, 6409, 2517, 490, 264, 3894], "temperature": 0.0, "avg_logprob": -0.18406747817993163, "compression_ratio": 1.7466666666666666, "no_speech_prob": 8.800982868706342e-06}, {"id": 327, "seek": 164840, "start": 1648.4, "end": 1650.0800000000002, "text": " character.", "tokens": [2517, 13], "temperature": 0.0, "avg_logprob": -0.16813402791177073, "compression_ratio": 1.484375, "no_speech_prob": 1.2411397619871423e-05}, {"id": 328, "seek": 164840, "start": 1650.0800000000002, "end": 1659.6000000000001, "text": " So to do that, we're going to go through our whole list of indexes from 0 up to the end", "tokens": [407, 281, 360, 300, 11, 321, 434, 516, 281, 352, 807, 527, 1379, 1329, 295, 8186, 279, 490, 1958, 493, 281, 264, 917], "temperature": 0.0, "avg_logprob": -0.16813402791177073, "compression_ratio": 1.484375, "no_speech_prob": 1.2411397619871423e-05}, {"id": 329, "seek": 164840, "start": 1659.6000000000001, "end": 1672.2800000000002, "text": " minus 3, and we're going to create a whole list of the 0th, 4th, 8th, 12th, etc characters,", "tokens": [3175, 805, 11, 293, 321, 434, 516, 281, 1884, 257, 1379, 1329, 295, 264, 1958, 392, 11, 1017, 392, 11, 1649, 392, 11, 2272, 392, 11, 5183, 4342, 11], "temperature": 0.0, "avg_logprob": -0.16813402791177073, "compression_ratio": 1.484375, "no_speech_prob": 1.2411397619871423e-05}, {"id": 330, "seek": 167228, "start": 1672.28, "end": 1680.36, "text": " and a list of the 1st, 5th, 9th, etc, and the 2nd, 6th, 10th, etc.", "tokens": [293, 257, 1329, 295, 264, 502, 372, 11, 1025, 392, 11, 1722, 392, 11, 5183, 11, 293, 264, 568, 273, 11, 1386, 392, 11, 1266, 392, 11, 5183, 13], "temperature": 0.0, "avg_logprob": -0.13784819264565745, "compression_ratio": 1.9787234042553192, "no_speech_prob": 2.9944237667223206e-06}, {"id": 331, "seek": 167228, "start": 1680.36, "end": 1683.96, "text": " So this is going to represent the first character of each sequence, the second character of", "tokens": [407, 341, 307, 516, 281, 2906, 264, 700, 2517, 295, 1184, 8310, 11, 264, 1150, 2517, 295], "temperature": 0.0, "avg_logprob": -0.13784819264565745, "compression_ratio": 1.9787234042553192, "no_speech_prob": 2.9944237667223206e-06}, {"id": 332, "seek": 167228, "start": 1683.96, "end": 1687.48, "text": " each sequence, the third character of each sequence, and this is the one we want to predict,", "tokens": [1184, 8310, 11, 264, 2636, 2517, 295, 1184, 8310, 11, 293, 341, 307, 264, 472, 321, 528, 281, 6069, 11], "temperature": 0.0, "avg_logprob": -0.13784819264565745, "compression_ratio": 1.9787234042553192, "no_speech_prob": 2.9944237667223206e-06}, {"id": 333, "seek": 167228, "start": 1687.48, "end": 1691.12, "text": " the fourth character of each sequence.", "tokens": [264, 6409, 2517, 295, 1184, 8310, 13], "temperature": 0.0, "avg_logprob": -0.13784819264565745, "compression_ratio": 1.9787234042553192, "no_speech_prob": 2.9944237667223206e-06}, {"id": 334, "seek": 167228, "start": 1691.12, "end": 1696.6, "text": " So we can now turn these into NumPy arrays just by stacking them up together.", "tokens": [407, 321, 393, 586, 1261, 613, 666, 22592, 47, 88, 41011, 445, 538, 41376, 552, 493, 1214, 13], "temperature": 0.0, "avg_logprob": -0.13784819264565745, "compression_ratio": 1.9787234042553192, "no_speech_prob": 2.9944237667223206e-06}, {"id": 335, "seek": 167228, "start": 1696.6, "end": 1702.2, "text": " And so now we've got our input for our first characters, second characters, and third characters", "tokens": [400, 370, 586, 321, 600, 658, 527, 4846, 337, 527, 700, 4342, 11, 1150, 4342, 11, 293, 2636, 4342], "temperature": 0.0, "avg_logprob": -0.13784819264565745, "compression_ratio": 1.9787234042553192, "no_speech_prob": 2.9944237667223206e-06}, {"id": 336, "seek": 170220, "start": 1702.2, "end": 1707.52, "text": " of every 4 character piece of this collected works.", "tokens": [295, 633, 1017, 2517, 2522, 295, 341, 11087, 1985, 13], "temperature": 0.0, "avg_logprob": -0.1531179973057338, "compression_ratio": 1.7447916666666667, "no_speech_prob": 4.006281596957706e-05}, {"id": 337, "seek": 170220, "start": 1707.52, "end": 1713.72, "text": " And then our y's, our labels, will simply be the fourth characters of each sequence.", "tokens": [400, 550, 527, 288, 311, 11, 527, 16949, 11, 486, 2935, 312, 264, 6409, 4342, 295, 1184, 8310, 13], "temperature": 0.0, "avg_logprob": -0.1531179973057338, "compression_ratio": 1.7447916666666667, "no_speech_prob": 4.006281596957706e-05}, {"id": 338, "seek": 170220, "start": 1713.72, "end": 1715.8600000000001, "text": " So here you can see them.", "tokens": [407, 510, 291, 393, 536, 552, 13], "temperature": 0.0, "avg_logprob": -0.1531179973057338, "compression_ratio": 1.7447916666666667, "no_speech_prob": 4.006281596957706e-05}, {"id": 339, "seek": 170220, "start": 1715.8600000000001, "end": 1723.6000000000001, "text": " So for example, if we took x1, x2, and x3 and took the first element of each, this is", "tokens": [407, 337, 1365, 11, 498, 321, 1890, 2031, 16, 11, 2031, 17, 11, 293, 2031, 18, 293, 1890, 264, 700, 4478, 295, 1184, 11, 341, 307], "temperature": 0.0, "avg_logprob": -0.1531179973057338, "compression_ratio": 1.7447916666666667, "no_speech_prob": 4.006281596957706e-05}, {"id": 340, "seek": 170220, "start": 1723.6000000000001, "end": 1729.48, "text": " the first character of the text, the second character of the text, the third character", "tokens": [264, 700, 2517, 295, 264, 2487, 11, 264, 1150, 2517, 295, 264, 2487, 11, 264, 2636, 2517], "temperature": 0.0, "avg_logprob": -0.1531179973057338, "compression_ratio": 1.7447916666666667, "no_speech_prob": 4.006281596957706e-05}, {"id": 341, "seek": 172948, "start": 1729.48, "end": 1732.28, "text": " of the text, and the fourth character of the text.", "tokens": [295, 264, 2487, 11, 293, 264, 6409, 2517, 295, 264, 2487, 13], "temperature": 0.0, "avg_logprob": -0.12341522308717291, "compression_ratio": 1.6358024691358024, "no_speech_prob": 2.9480033845175058e-06}, {"id": 342, "seek": 172948, "start": 1732.28, "end": 1737.0, "text": " So we'll be trying to predict this based on these 3.", "tokens": [407, 321, 603, 312, 1382, 281, 6069, 341, 2361, 322, 613, 805, 13], "temperature": 0.0, "avg_logprob": -0.12341522308717291, "compression_ratio": 1.6358024691358024, "no_speech_prob": 2.9480033845175058e-06}, {"id": 343, "seek": 172948, "start": 1737.0, "end": 1741.52, "text": " And then we'll try to predict this based on these 3.", "tokens": [400, 550, 321, 603, 853, 281, 6069, 341, 2361, 322, 613, 805, 13], "temperature": 0.0, "avg_logprob": -0.12341522308717291, "compression_ratio": 1.6358024691358024, "no_speech_prob": 2.9480033845175058e-06}, {"id": 344, "seek": 172948, "start": 1741.52, "end": 1746.84, "text": " So that's our data format.", "tokens": [407, 300, 311, 527, 1412, 7877, 13], "temperature": 0.0, "avg_logprob": -0.12341522308717291, "compression_ratio": 1.6358024691358024, "no_speech_prob": 2.9480033845175058e-06}, {"id": 345, "seek": 172948, "start": 1746.84, "end": 1758.64, "text": " So you can see we've got about 200,000 of these inputs for each of x1, x3, and y.", "tokens": [407, 291, 393, 536, 321, 600, 658, 466, 2331, 11, 1360, 295, 613, 15743, 337, 1184, 295, 2031, 16, 11, 2031, 18, 11, 293, 288, 13], "temperature": 0.0, "avg_logprob": -0.12341522308717291, "compression_ratio": 1.6358024691358024, "no_speech_prob": 2.9480033845175058e-06}, {"id": 346, "seek": 175864, "start": 1758.64, "end": 1763.3600000000001, "text": " And so as per usual, we're going to first of all turn them into embeddings by creating", "tokens": [400, 370, 382, 680, 7713, 11, 321, 434, 516, 281, 700, 295, 439, 1261, 552, 666, 12240, 29432, 538, 4084], "temperature": 0.0, "avg_logprob": -0.19022002749972874, "compression_ratio": 1.4930875576036866, "no_speech_prob": 1.1843006177514326e-05}, {"id": 347, "seek": 175864, "start": 1763.3600000000001, "end": 1765.16, "text": " an embedding matrix.", "tokens": [364, 12240, 3584, 8141, 13], "temperature": 0.0, "avg_logprob": -0.19022002749972874, "compression_ratio": 1.4930875576036866, "no_speech_prob": 1.1843006177514326e-05}, {"id": 348, "seek": 175864, "start": 1765.16, "end": 1769.92, "text": " I will mention this is not normal.", "tokens": [286, 486, 2152, 341, 307, 406, 2710, 13], "temperature": 0.0, "avg_logprob": -0.19022002749972874, "compression_ratio": 1.4930875576036866, "no_speech_prob": 1.1843006177514326e-05}, {"id": 349, "seek": 175864, "start": 1769.92, "end": 1773.44, "text": " I haven't actually seen anybody else do this.", "tokens": [286, 2378, 380, 767, 1612, 4472, 1646, 360, 341, 13], "temperature": 0.0, "avg_logprob": -0.19022002749972874, "compression_ratio": 1.4930875576036866, "no_speech_prob": 1.1843006177514326e-05}, {"id": 350, "seek": 175864, "start": 1773.44, "end": 1777.5200000000002, "text": " Most people just treat them as one-hot encodings.", "tokens": [4534, 561, 445, 2387, 552, 382, 472, 12, 12194, 2058, 378, 1109, 13], "temperature": 0.0, "avg_logprob": -0.19022002749972874, "compression_ratio": 1.4930875576036866, "no_speech_prob": 1.1843006177514326e-05}, {"id": 351, "seek": 175864, "start": 1777.5200000000002, "end": 1786.96, "text": " So for example, the most widely used blog post about car RNNs, which really made them", "tokens": [407, 337, 1365, 11, 264, 881, 13371, 1143, 6968, 2183, 466, 1032, 45702, 45, 82, 11, 597, 534, 1027, 552], "temperature": 0.0, "avg_logprob": -0.19022002749972874, "compression_ratio": 1.4930875576036866, "no_speech_prob": 1.1843006177514326e-05}, {"id": 352, "seek": 178696, "start": 1786.96, "end": 1791.4, "text": " popular was Andre Kapathy's.", "tokens": [3743, 390, 20667, 21216, 9527, 311, 13], "temperature": 0.0, "avg_logprob": -0.27049088111290565, "compression_ratio": 1.3972602739726028, "no_speech_prob": 1.9222603441448882e-05}, {"id": 353, "seek": 178696, "start": 1791.4, "end": 1805.72, "text": " You can see that in his version, he shows them as being one-hot encoded.", "tokens": [509, 393, 536, 300, 294, 702, 3037, 11, 415, 3110, 552, 382, 885, 472, 12, 12194, 2058, 12340, 13], "temperature": 0.0, "avg_logprob": -0.27049088111290565, "compression_ratio": 1.3972602739726028, "no_speech_prob": 1.9222603441448882e-05}, {"id": 354, "seek": 178696, "start": 1805.72, "end": 1806.72, "text": " We're not going to do that.", "tokens": [492, 434, 406, 516, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.27049088111290565, "compression_ratio": 1.3972602739726028, "no_speech_prob": 1.9222603441448882e-05}, {"id": 355, "seek": 178696, "start": 1806.72, "end": 1810.1200000000001, "text": " We're going to turn them into embeddings.", "tokens": [492, 434, 516, 281, 1261, 552, 666, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.27049088111290565, "compression_ratio": 1.3972602739726028, "no_speech_prob": 1.9222603441448882e-05}, {"id": 356, "seek": 178696, "start": 1810.1200000000001, "end": 1811.76, "text": " I think it makes a lot of sense.", "tokens": [286, 519, 309, 1669, 257, 688, 295, 2020, 13], "temperature": 0.0, "avg_logprob": -0.27049088111290565, "compression_ratio": 1.3972602739726028, "no_speech_prob": 1.9222603441448882e-05}, {"id": 357, "seek": 181176, "start": 1811.76, "end": 1817.92, "text": " But like capital A and lowercase a have some similarities that an embedding can understand.", "tokens": [583, 411, 4238, 316, 293, 3126, 9765, 257, 362, 512, 24197, 300, 364, 12240, 3584, 393, 1223, 13], "temperature": 0.0, "avg_logprob": -0.18096177419026693, "compression_ratio": 1.6798029556650247, "no_speech_prob": 8.664438610139769e-06}, {"id": 358, "seek": 181176, "start": 1817.92, "end": 1821.68, "text": " Different types of things that have to be opened and closed, like different types of", "tokens": [20825, 3467, 295, 721, 300, 362, 281, 312, 5625, 293, 5395, 11, 411, 819, 3467, 295], "temperature": 0.0, "avg_logprob": -0.18096177419026693, "compression_ratio": 1.6798029556650247, "no_speech_prob": 8.664438610139769e-06}, {"id": 359, "seek": 181176, "start": 1821.68, "end": 1827.56, "text": " parentheses and quotes, have certain characteristics that can be constructed in embedding.", "tokens": [34153, 293, 19963, 11, 362, 1629, 10891, 300, 393, 312, 17083, 294, 12240, 3584, 13], "temperature": 0.0, "avg_logprob": -0.18096177419026693, "compression_ratio": 1.6798029556650247, "no_speech_prob": 8.664438610139769e-06}, {"id": 360, "seek": 181176, "start": 1827.56, "end": 1831.6, "text": " There's all kinds of things that we would expect an embedding to capture.", "tokens": [821, 311, 439, 3685, 295, 721, 300, 321, 576, 2066, 364, 12240, 3584, 281, 7983, 13], "temperature": 0.0, "avg_logprob": -0.18096177419026693, "compression_ratio": 1.6798029556650247, "no_speech_prob": 8.664438610139769e-06}, {"id": 361, "seek": 183160, "start": 1831.6, "end": 1847.1599999999999, "text": " So my hypothesis was that an embedding is going to do a better job than just a one-hot", "tokens": [407, 452, 17291, 390, 300, 364, 12240, 3584, 307, 516, 281, 360, 257, 1101, 1691, 813, 445, 257, 472, 12, 12194], "temperature": 0.0, "avg_logprob": -0.1309066911538442, "compression_ratio": 1.3484848484848484, "no_speech_prob": 4.565937160805333e-06}, {"id": 362, "seek": 183160, "start": 1847.1599999999999, "end": 1848.9199999999998, "text": " encoding.", "tokens": [43430, 13], "temperature": 0.0, "avg_logprob": -0.1309066911538442, "compression_ratio": 1.3484848484848484, "no_speech_prob": 4.565937160805333e-06}, {"id": 363, "seek": 183160, "start": 1848.9199999999998, "end": 1855.0, "text": " In my experiments over the last couple of weeks, that generally seems to be true.", "tokens": [682, 452, 12050, 670, 264, 1036, 1916, 295, 3259, 11, 300, 5101, 2544, 281, 312, 2074, 13], "temperature": 0.0, "avg_logprob": -0.1309066911538442, "compression_ratio": 1.3484848484848484, "no_speech_prob": 4.565937160805333e-06}, {"id": 364, "seek": 185500, "start": 1855.0, "end": 1861.96, "text": " So we're going to take each character, 1 through 3, and turn them into embeddings by first", "tokens": [407, 321, 434, 516, 281, 747, 1184, 2517, 11, 502, 807, 805, 11, 293, 1261, 552, 666, 12240, 29432, 538, 700], "temperature": 0.0, "avg_logprob": -0.1878808709078057, "compression_ratio": 1.9021739130434783, "no_speech_prob": 9.080334166355897e-06}, {"id": 365, "seek": 185500, "start": 1861.96, "end": 1867.24, "text": " creating an input layer for them and then creating an embedding layer for that input.", "tokens": [4084, 364, 4846, 4583, 337, 552, 293, 550, 4084, 364, 12240, 3584, 4583, 337, 300, 4846, 13], "temperature": 0.0, "avg_logprob": -0.1878808709078057, "compression_ratio": 1.9021739130434783, "no_speech_prob": 9.080334166355897e-06}, {"id": 366, "seek": 185500, "start": 1867.24, "end": 1872.96, "text": " And then we return the input layer and the flattened version of the embedding layer.", "tokens": [400, 550, 321, 2736, 264, 4846, 4583, 293, 264, 24183, 292, 3037, 295, 264, 12240, 3584, 4583, 13], "temperature": 0.0, "avg_logprob": -0.1878808709078057, "compression_ratio": 1.9021739130434783, "no_speech_prob": 9.080334166355897e-06}, {"id": 367, "seek": 185500, "start": 1872.96, "end": 1882.36, "text": " So this is the input to and output of our 3 embedding layers for our 3 input characters.", "tokens": [407, 341, 307, 264, 4846, 281, 293, 5598, 295, 527, 805, 12240, 3584, 7914, 337, 527, 805, 4846, 4342, 13], "temperature": 0.0, "avg_logprob": -0.1878808709078057, "compression_ratio": 1.9021739130434783, "no_speech_prob": 9.080334166355897e-06}, {"id": 368, "seek": 188236, "start": 1882.36, "end": 1886.36, "text": " So that's basically our inputs.", "tokens": [407, 300, 311, 1936, 527, 15743, 13], "temperature": 0.0, "avg_logprob": -0.22838181715745193, "compression_ratio": 1.4528301886792452, "no_speech_prob": 1.1125456694571767e-05}, {"id": 369, "seek": 188236, "start": 1886.36, "end": 1898.3999999999999, "text": " So we now have to decide how many activations do we want.", "tokens": [407, 321, 586, 362, 281, 4536, 577, 867, 2430, 763, 360, 321, 528, 13], "temperature": 0.0, "avg_logprob": -0.22838181715745193, "compression_ratio": 1.4528301886792452, "no_speech_prob": 1.1125456694571767e-05}, {"id": 370, "seek": 188236, "start": 1898.3999999999999, "end": 1900.74, "text": " And so that's something we can just pick.", "tokens": [400, 370, 300, 311, 746, 321, 393, 445, 1888, 13], "temperature": 0.0, "avg_logprob": -0.22838181715745193, "compression_ratio": 1.4528301886792452, "no_speech_prob": 1.1125456694571767e-05}, {"id": 371, "seek": 188236, "start": 1900.74, "end": 1903.0, "text": " So I've decided to go with 256.", "tokens": [407, 286, 600, 3047, 281, 352, 365, 38882, 13], "temperature": 0.0, "avg_logprob": -0.22838181715745193, "compression_ratio": 1.4528301886792452, "no_speech_prob": 1.1125456694571767e-05}, {"id": 372, "seek": 188236, "start": 1903.0, "end": 1907.9199999999998, "text": " That's something that seemed reasonable, seems to have worked okay.", "tokens": [663, 311, 746, 300, 6576, 10585, 11, 2544, 281, 362, 2732, 1392, 13], "temperature": 0.0, "avg_logprob": -0.22838181715745193, "compression_ratio": 1.4528301886792452, "no_speech_prob": 1.1125456694571767e-05}, {"id": 373, "seek": 190792, "start": 1907.92, "end": 1917.8000000000002, "text": " So we now have to somehow construct something where each of our green arrows ends up with", "tokens": [407, 321, 586, 362, 281, 6063, 7690, 746, 689, 1184, 295, 527, 3092, 19669, 5314, 493, 365], "temperature": 0.0, "avg_logprob": -0.14712650299072266, "compression_ratio": 1.5204081632653061, "no_speech_prob": 7.646490303159226e-06}, {"id": 374, "seek": 190792, "start": 1917.8000000000002, "end": 1919.8000000000002, "text": " the same weight matrix.", "tokens": [264, 912, 3364, 8141, 13], "temperature": 0.0, "avg_logprob": -0.14712650299072266, "compression_ratio": 1.5204081632653061, "no_speech_prob": 7.646490303159226e-06}, {"id": 375, "seek": 190792, "start": 1919.8000000000002, "end": 1925.28, "text": " It turns out Keras makes this really easy with the Keras Functional API.", "tokens": [467, 4523, 484, 591, 6985, 1669, 341, 534, 1858, 365, 264, 591, 6985, 11166, 41048, 9362, 13], "temperature": 0.0, "avg_logprob": -0.14712650299072266, "compression_ratio": 1.5204081632653061, "no_speech_prob": 7.646490303159226e-06}, {"id": 376, "seek": 190792, "start": 1925.28, "end": 1935.04, "text": " When you call dense like this, what it's actually doing is it's creating a layer with a specific", "tokens": [1133, 291, 818, 18011, 411, 341, 11, 437, 309, 311, 767, 884, 307, 309, 311, 4084, 257, 4583, 365, 257, 2685], "temperature": 0.0, "avg_logprob": -0.14712650299072266, "compression_ratio": 1.5204081632653061, "no_speech_prob": 7.646490303159226e-06}, {"id": 377, "seek": 190792, "start": 1935.04, "end": 1936.04, "text": " weight matrix.", "tokens": [3364, 8141, 13], "temperature": 0.0, "avg_logprob": -0.14712650299072266, "compression_ratio": 1.5204081632653061, "no_speech_prob": 7.646490303159226e-06}, {"id": 378, "seek": 193604, "start": 1936.04, "end": 1940.72, "text": " Notice that I haven't passed in anything here to say what it's connected to, so it's not", "tokens": [13428, 300, 286, 2378, 380, 4678, 294, 1340, 510, 281, 584, 437, 309, 311, 4582, 281, 11, 370, 309, 311, 406], "temperature": 0.0, "avg_logprob": -0.15706803248478815, "compression_ratio": 1.544041450777202, "no_speech_prob": 9.666020559961908e-06}, {"id": 379, "seek": 193604, "start": 1940.72, "end": 1942.44, "text": " part of a model yet.", "tokens": [644, 295, 257, 2316, 1939, 13], "temperature": 0.0, "avg_logprob": -0.15706803248478815, "compression_ratio": 1.544041450777202, "no_speech_prob": 9.666020559961908e-06}, {"id": 380, "seek": 193604, "start": 1942.44, "end": 1956.32, "text": " This is just saying I'm going to have something which is a dense layer which creates 256 activations.", "tokens": [639, 307, 445, 1566, 286, 478, 516, 281, 362, 746, 597, 307, 257, 18011, 4583, 597, 7829, 38882, 2430, 763, 13], "temperature": 0.0, "avg_logprob": -0.15706803248478815, "compression_ratio": 1.544041450777202, "no_speech_prob": 9.666020559961908e-06}, {"id": 381, "seek": 193604, "start": 1956.32, "end": 1961.2, "text": " So it doesn't actually do anything until I then do this, so I connect it to something.", "tokens": [407, 309, 1177, 380, 767, 360, 1340, 1826, 286, 550, 360, 341, 11, 370, 286, 1745, 309, 281, 746, 13], "temperature": 0.0, "avg_logprob": -0.15706803248478815, "compression_ratio": 1.544041450777202, "no_speech_prob": 9.666020559961908e-06}, {"id": 382, "seek": 196120, "start": 1961.2, "end": 1967.04, "text": " So here I'm going to say character1's hidden state comes from taking character1, which", "tokens": [407, 510, 286, 478, 516, 281, 584, 2517, 16, 311, 7633, 1785, 1487, 490, 1940, 2517, 16, 11, 597], "temperature": 0.0, "avg_logprob": -0.13225626176403416, "compression_ratio": 1.946808510638298, "no_speech_prob": 7.646490303159226e-06}, {"id": 383, "seek": 196120, "start": 1967.04, "end": 1973.68, "text": " was the output of our first embedding, and putting it through this denseIn layer.", "tokens": [390, 264, 5598, 295, 527, 700, 12240, 3584, 11, 293, 3372, 309, 807, 341, 18011, 4575, 4583, 13], "temperature": 0.0, "avg_logprob": -0.13225626176403416, "compression_ratio": 1.946808510638298, "no_speech_prob": 7.646490303159226e-06}, {"id": 384, "seek": 196120, "start": 1973.68, "end": 1978.8, "text": " So this is the thing which creates our first circle.", "tokens": [407, 341, 307, 264, 551, 597, 7829, 527, 700, 6329, 13], "temperature": 0.0, "avg_logprob": -0.13225626176403416, "compression_ratio": 1.946808510638298, "no_speech_prob": 7.646490303159226e-06}, {"id": 385, "seek": 196120, "start": 1978.8, "end": 1982.1200000000001, "text": " So the embedding is the thing which creates the output of our first rectangle.", "tokens": [407, 264, 12240, 3584, 307, 264, 551, 597, 7829, 264, 5598, 295, 527, 700, 21930, 13], "temperature": 0.0, "avg_logprob": -0.13225626176403416, "compression_ratio": 1.946808510638298, "no_speech_prob": 7.646490303159226e-06}, {"id": 386, "seek": 196120, "start": 1982.1200000000001, "end": 1984.1200000000001, "text": " This creates our first circle.", "tokens": [639, 7829, 527, 700, 6329, 13], "temperature": 0.0, "avg_logprob": -0.13225626176403416, "compression_ratio": 1.946808510638298, "no_speech_prob": 7.646490303159226e-06}, {"id": 387, "seek": 196120, "start": 1984.1200000000001, "end": 1989.0, "text": " And so denseIn is the green arrow.", "tokens": [400, 370, 18011, 4575, 307, 264, 3092, 11610, 13], "temperature": 0.0, "avg_logprob": -0.13225626176403416, "compression_ratio": 1.946808510638298, "no_speech_prob": 7.646490303159226e-06}, {"id": 388, "seek": 198900, "start": 1989.0, "end": 1994.92, "text": " So what that means is that in order to create the next set of activations, we need to create", "tokens": [407, 437, 300, 1355, 307, 300, 294, 1668, 281, 1884, 264, 958, 992, 295, 2430, 763, 11, 321, 643, 281, 1884], "temperature": 0.0, "avg_logprob": -0.17865625381469727, "compression_ratio": 1.8592964824120604, "no_speech_prob": 3.0894802875991445e-06}, {"id": 389, "seek": 198900, "start": 1994.92, "end": 1996.6, "text": " the orange arrow.", "tokens": [264, 7671, 11610, 13], "temperature": 0.0, "avg_logprob": -0.17865625381469727, "compression_ratio": 1.8592964824120604, "no_speech_prob": 3.0894802875991445e-06}, {"id": 390, "seek": 198900, "start": 1996.6, "end": 2000.96, "text": " So since the orange arrow is different weight matrix to the green arrow, we have to create", "tokens": [407, 1670, 264, 7671, 11610, 307, 819, 3364, 8141, 281, 264, 3092, 11610, 11, 321, 362, 281, 1884], "temperature": 0.0, "avg_logprob": -0.17865625381469727, "compression_ratio": 1.8592964824120604, "no_speech_prob": 3.0894802875991445e-06}, {"id": 391, "seek": 198900, "start": 2000.96, "end": 2001.96, "text": " a new dense layer.", "tokens": [257, 777, 18011, 4583, 13], "temperature": 0.0, "avg_logprob": -0.17865625381469727, "compression_ratio": 1.8592964824120604, "no_speech_prob": 3.0894802875991445e-06}, {"id": 392, "seek": 198900, "start": 2001.96, "end": 2002.96, "text": " So here it is.", "tokens": [407, 510, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.17865625381469727, "compression_ratio": 1.8592964824120604, "no_speech_prob": 3.0894802875991445e-06}, {"id": 393, "seek": 198900, "start": 2002.96, "end": 2008.52, "text": " I've got a new dense layer, and again with n hidden outputs.", "tokens": [286, 600, 658, 257, 777, 18011, 4583, 11, 293, 797, 365, 297, 7633, 23930, 13], "temperature": 0.0, "avg_logprob": -0.17865625381469727, "compression_ratio": 1.8592964824120604, "no_speech_prob": 3.0894802875991445e-06}, {"id": 394, "seek": 198900, "start": 2008.52, "end": 2014.84, "text": " So by creating a new dense layer, this is a whole separate weight matrix.", "tokens": [407, 538, 4084, 257, 777, 18011, 4583, 11, 341, 307, 257, 1379, 4994, 3364, 8141, 13], "temperature": 0.0, "avg_logprob": -0.17865625381469727, "compression_ratio": 1.8592964824120604, "no_speech_prob": 3.0894802875991445e-06}, {"id": 395, "seek": 201484, "start": 2014.84, "end": 2023.12, "text": " So now that I've done that, I can create my character2 hidden state, which is here, and", "tokens": [407, 586, 300, 286, 600, 1096, 300, 11, 286, 393, 1884, 452, 2517, 17, 7633, 1785, 11, 597, 307, 510, 11, 293], "temperature": 0.0, "avg_logprob": -0.14504487463768492, "compression_ratio": 1.8010752688172043, "no_speech_prob": 8.714330874681764e-07}, {"id": 396, "seek": 201484, "start": 2023.12, "end": 2026.04, "text": " I'm going to have to sum up two separate things.", "tokens": [286, 478, 516, 281, 362, 281, 2408, 493, 732, 4994, 721, 13], "temperature": 0.0, "avg_logprob": -0.14504487463768492, "compression_ratio": 1.8010752688172043, "no_speech_prob": 8.714330874681764e-07}, {"id": 397, "seek": 201484, "start": 2026.04, "end": 2033.1999999999998, "text": " I'm going to take my character2 embedding, put it through my green arrow, denseIn, that's", "tokens": [286, 478, 516, 281, 747, 452, 2517, 17, 12240, 3584, 11, 829, 309, 807, 452, 3092, 11610, 11, 18011, 4575, 11, 300, 311], "temperature": 0.0, "avg_logprob": -0.14504487463768492, "compression_ratio": 1.8010752688172043, "no_speech_prob": 8.714330874681764e-07}, {"id": 398, "seek": 201484, "start": 2033.1999999999998, "end": 2034.1999999999998, "text": " going to be there.", "tokens": [516, 281, 312, 456, 13], "temperature": 0.0, "avg_logprob": -0.14504487463768492, "compression_ratio": 1.8010752688172043, "no_speech_prob": 8.714330874681764e-07}, {"id": 399, "seek": 201484, "start": 2034.1999999999998, "end": 2041.8799999999999, "text": " I'm going to take the output of my character1's hidden state and run it through my orange", "tokens": [286, 478, 516, 281, 747, 264, 5598, 295, 452, 2517, 16, 311, 7633, 1785, 293, 1190, 309, 807, 452, 7671], "temperature": 0.0, "avg_logprob": -0.14504487463768492, "compression_ratio": 1.8010752688172043, "no_speech_prob": 8.714330874681764e-07}, {"id": 400, "seek": 204188, "start": 2041.88, "end": 2048.32, "text": " arrow, which we call denseHidden, and then we're going to merge the two together.", "tokens": [11610, 11, 597, 321, 818, 18011, 39, 6171, 11, 293, 550, 321, 434, 516, 281, 22183, 264, 732, 1214, 13], "temperature": 0.0, "avg_logprob": -0.18908920288085937, "compression_ratio": 1.663265306122449, "no_speech_prob": 2.2252775124798063e-06}, {"id": 401, "seek": 204188, "start": 2048.32, "end": 2052.48, "text": " And merge by default does a sum.", "tokens": [400, 22183, 538, 7576, 775, 257, 2408, 13], "temperature": 0.0, "avg_logprob": -0.18908920288085937, "compression_ratio": 1.663265306122449, "no_speech_prob": 2.2252775124798063e-06}, {"id": 402, "seek": 204188, "start": 2052.48, "end": 2056.92, "text": " So this is adding together these two outputs.", "tokens": [407, 341, 307, 5127, 1214, 613, 732, 23930, 13], "temperature": 0.0, "avg_logprob": -0.18908920288085937, "compression_ratio": 1.663265306122449, "no_speech_prob": 2.2252775124798063e-06}, {"id": 403, "seek": 204188, "start": 2056.92, "end": 2061.12, "text": " In other words, it's adding together these two layer operation outputs.", "tokens": [682, 661, 2283, 11, 309, 311, 5127, 1214, 613, 732, 4583, 6916, 23930, 13], "temperature": 0.0, "avg_logprob": -0.18908920288085937, "compression_ratio": 1.663265306122449, "no_speech_prob": 2.2252775124798063e-06}, {"id": 404, "seek": 204188, "start": 2061.12, "end": 2064.6400000000003, "text": " And that gives us this circle.", "tokens": [400, 300, 2709, 505, 341, 6329, 13], "temperature": 0.0, "avg_logprob": -0.18908920288085937, "compression_ratio": 1.663265306122449, "no_speech_prob": 2.2252775124798063e-06}, {"id": 405, "seek": 204188, "start": 2064.6400000000003, "end": 2068.2000000000003, "text": " So the third character output is done in exactly the same way.", "tokens": [407, 264, 2636, 2517, 5598, 307, 1096, 294, 2293, 264, 912, 636, 13], "temperature": 0.0, "avg_logprob": -0.18908920288085937, "compression_ratio": 1.663265306122449, "no_speech_prob": 2.2252775124798063e-06}, {"id": 406, "seek": 206820, "start": 2068.2, "end": 2072.9199999999996, "text": " We take the third character's embedding, run it through our green arrow, take the result", "tokens": [492, 747, 264, 2636, 2517, 311, 12240, 3584, 11, 1190, 309, 807, 527, 3092, 11610, 11, 747, 264, 1874], "temperature": 0.0, "avg_logprob": -0.21358928885511172, "compression_ratio": 1.8086124401913874, "no_speech_prob": 1.8342667317483574e-05}, {"id": 407, "seek": 206820, "start": 2072.9199999999996, "end": 2076.96, "text": " of our previous hidden activations and run it through our orange arrow, and then merge", "tokens": [295, 527, 3894, 7633, 2430, 763, 293, 1190, 309, 807, 527, 7671, 11610, 11, 293, 550, 22183], "temperature": 0.0, "avg_logprob": -0.21358928885511172, "compression_ratio": 1.8086124401913874, "no_speech_prob": 1.8342667317483574e-05}, {"id": 408, "seek": 206820, "start": 2076.96, "end": 2077.96, "text": " the two together.", "tokens": [264, 732, 1214, 13], "temperature": 0.0, "avg_logprob": -0.21358928885511172, "compression_ratio": 1.8086124401913874, "no_speech_prob": 1.8342667317483574e-05}, {"id": 409, "seek": 206820, "start": 2077.96, "end": 2078.96, "text": " Question from the audience.", "tokens": [14464, 490, 264, 4034, 13], "temperature": 0.0, "avg_logprob": -0.21358928885511172, "compression_ratio": 1.8086124401913874, "no_speech_prob": 1.8342667317483574e-05}, {"id": 410, "seek": 206820, "start": 2078.96, "end": 2091.0, "text": " Is the first output the size of the latent fields in the embedding?", "tokens": [1119, 264, 700, 5598, 264, 2744, 295, 264, 48994, 7909, 294, 264, 12240, 3584, 30], "temperature": 0.0, "avg_logprob": -0.21358928885511172, "compression_ratio": 1.8086124401913874, "no_speech_prob": 1.8342667317483574e-05}, {"id": 411, "seek": 206820, "start": 2091.0, "end": 2097.96, "text": " The size of the latent embeddings we defined when we created the embeddings up here, and", "tokens": [440, 2744, 295, 264, 48994, 12240, 29432, 321, 7642, 562, 321, 2942, 264, 12240, 29432, 493, 510, 11, 293], "temperature": 0.0, "avg_logprob": -0.21358928885511172, "compression_ratio": 1.8086124401913874, "no_speech_prob": 1.8342667317483574e-05}, {"id": 412, "seek": 209796, "start": 2097.96, "end": 2107.2400000000002, "text": " we defined them as having nfat size, and nfat we defined as 42.", "tokens": [321, 7642, 552, 382, 1419, 297, 35293, 2744, 11, 293, 297, 35293, 321, 7642, 382, 14034, 13], "temperature": 0.0, "avg_logprob": -0.15254592895507812, "compression_ratio": 1.4545454545454546, "no_speech_prob": 1.130073178501334e-05}, {"id": 413, "seek": 209796, "start": 2107.2400000000002, "end": 2113.32, "text": " So C1, C2 and C3 represent the result of putting each character through this embedding and", "tokens": [407, 383, 16, 11, 383, 17, 293, 383, 18, 2906, 264, 1874, 295, 3372, 1184, 2517, 807, 341, 12240, 3584, 293], "temperature": 0.0, "avg_logprob": -0.15254592895507812, "compression_ratio": 1.4545454545454546, "no_speech_prob": 1.130073178501334e-05}, {"id": 414, "seek": 209796, "start": 2113.32, "end": 2118.84, "text": " getting out 42 latent factors.", "tokens": [1242, 484, 14034, 48994, 6771, 13], "temperature": 0.0, "avg_logprob": -0.15254592895507812, "compression_ratio": 1.4545454545454546, "no_speech_prob": 1.130073178501334e-05}, {"id": 415, "seek": 209796, "start": 2118.84, "end": 2125.28, "text": " Those are then the things we put into our green arrow.", "tokens": [3950, 366, 550, 264, 721, 321, 829, 666, 527, 3092, 11610, 13], "temperature": 0.0, "avg_logprob": -0.15254592895507812, "compression_ratio": 1.4545454545454546, "no_speech_prob": 1.130073178501334e-05}, {"id": 416, "seek": 212528, "start": 2125.28, "end": 2133.2000000000003, "text": " So after doing this 3 times, we now have C3 hidden, which is 1, 2, 3 here.", "tokens": [407, 934, 884, 341, 805, 1413, 11, 321, 586, 362, 383, 18, 7633, 11, 597, 307, 502, 11, 568, 11, 805, 510, 13], "temperature": 0.0, "avg_logprob": -0.18285142807733445, "compression_ratio": 1.5859030837004404, "no_speech_prob": 4.637843176169554e-06}, {"id": 417, "seek": 212528, "start": 2133.2000000000003, "end": 2135.2000000000003, "text": " So we now need a new set of weights.", "tokens": [407, 321, 586, 643, 257, 777, 992, 295, 17443, 13], "temperature": 0.0, "avg_logprob": -0.18285142807733445, "compression_ratio": 1.5859030837004404, "no_speech_prob": 4.637843176169554e-06}, {"id": 418, "seek": 212528, "start": 2135.2000000000003, "end": 2142.28, "text": " We need another dense layer, the blue arrow, so we'll call that dense out.", "tokens": [492, 643, 1071, 18011, 4583, 11, 264, 3344, 11610, 11, 370, 321, 603, 818, 300, 18011, 484, 13], "temperature": 0.0, "avg_logprob": -0.18285142807733445, "compression_ratio": 1.5859030837004404, "no_speech_prob": 4.637843176169554e-06}, {"id": 419, "seek": 212528, "start": 2142.28, "end": 2146.92, "text": " And this needs to create an output of size 86, vocab size.", "tokens": [400, 341, 2203, 281, 1884, 364, 5598, 295, 2744, 26687, 11, 2329, 455, 2744, 13], "temperature": 0.0, "avg_logprob": -0.18285142807733445, "compression_ratio": 1.5859030837004404, "no_speech_prob": 4.637843176169554e-06}, {"id": 420, "seek": 212528, "start": 2146.92, "end": 2152.28, "text": " We need to create something which can match to the one-part encoded list of possible characters,", "tokens": [492, 643, 281, 1884, 746, 597, 393, 2995, 281, 264, 472, 12, 6971, 2058, 12340, 1329, 295, 1944, 4342, 11], "temperature": 0.0, "avg_logprob": -0.18285142807733445, "compression_ratio": 1.5859030837004404, "no_speech_prob": 4.637843176169554e-06}, {"id": 421, "seek": 212528, "start": 2152.28, "end": 2154.5600000000004, "text": " which is 86 long.", "tokens": [597, 307, 26687, 938, 13], "temperature": 0.0, "avg_logprob": -0.18285142807733445, "compression_ratio": 1.5859030837004404, "no_speech_prob": 4.637843176169554e-06}, {"id": 422, "seek": 215456, "start": 2154.56, "end": 2159.84, "text": " So now that we've got this orange arrow, we can apply that to our final hidden state to", "tokens": [407, 586, 300, 321, 600, 658, 341, 7671, 11610, 11, 321, 393, 3079, 300, 281, 527, 2572, 7633, 1785, 281], "temperature": 0.0, "avg_logprob": -0.1832223868951565, "compression_ratio": 1.5945945945945945, "no_speech_prob": 3.13811960950261e-06}, {"id": 423, "seek": 215456, "start": 2159.84, "end": 2162.44, "text": " get our output.", "tokens": [483, 527, 5598, 13], "temperature": 0.0, "avg_logprob": -0.1832223868951565, "compression_ratio": 1.5945945945945945, "no_speech_prob": 3.13811960950261e-06}, {"id": 424, "seek": 215456, "start": 2162.44, "end": 2169.64, "text": " So in Keras, all we need to do now is call model passing in the 3 inputs, and so the", "tokens": [407, 294, 591, 6985, 11, 439, 321, 643, 281, 360, 586, 307, 818, 2316, 8437, 294, 264, 805, 15743, 11, 293, 370, 264], "temperature": 0.0, "avg_logprob": -0.1832223868951565, "compression_ratio": 1.5945945945945945, "no_speech_prob": 3.13811960950261e-06}, {"id": 425, "seek": 215456, "start": 2169.64, "end": 2176.68, "text": " 3 inputs were returned to us way back here each time we created an embedding, we returned", "tokens": [805, 15743, 645, 8752, 281, 505, 636, 646, 510, 1184, 565, 321, 2942, 364, 12240, 3584, 11, 321, 8752], "temperature": 0.0, "avg_logprob": -0.1832223868951565, "compression_ratio": 1.5945945945945945, "no_speech_prob": 3.13811960950261e-06}, {"id": 426, "seek": 215456, "start": 2176.68, "end": 2178.32, "text": " the input layer.", "tokens": [264, 4846, 4583, 13], "temperature": 0.0, "avg_logprob": -0.1832223868951565, "compression_ratio": 1.5945945945945945, "no_speech_prob": 3.13811960950261e-06}, {"id": 427, "seek": 217832, "start": 2178.32, "end": 2188.28, "text": " So passing in the 3 inputs and passing in our output.", "tokens": [407, 8437, 294, 264, 805, 15743, 293, 8437, 294, 527, 5598, 13], "temperature": 0.0, "avg_logprob": -0.19037026893801806, "compression_ratio": 1.564516129032258, "no_speech_prob": 4.4254620661376975e-06}, {"id": 428, "seek": 217832, "start": 2188.28, "end": 2189.76, "text": " So that's our model.", "tokens": [407, 300, 311, 527, 2316, 13], "temperature": 0.0, "avg_logprob": -0.19037026893801806, "compression_ratio": 1.564516129032258, "no_speech_prob": 4.4254620661376975e-06}, {"id": 429, "seek": 217832, "start": 2189.76, "end": 2197.0, "text": " So we can now compile it, set a learning rate, fit it, and as you can see, its loss is gradually", "tokens": [407, 321, 393, 586, 31413, 309, 11, 992, 257, 2539, 3314, 11, 3318, 309, 11, 293, 382, 291, 393, 536, 11, 1080, 4470, 307, 13145], "temperature": 0.0, "avg_logprob": -0.19037026893801806, "compression_ratio": 1.564516129032258, "no_speech_prob": 4.4254620661376975e-06}, {"id": 430, "seek": 217832, "start": 2197.0, "end": 2199.1600000000003, "text": " decreasing.", "tokens": [23223, 13], "temperature": 0.0, "avg_logprob": -0.19037026893801806, "compression_ratio": 1.564516129032258, "no_speech_prob": 4.4254620661376975e-06}, {"id": 431, "seek": 217832, "start": 2199.1600000000003, "end": 2203.76, "text": " And we can then test that out very easily by creating a little function that we're going", "tokens": [400, 321, 393, 550, 1500, 300, 484, 588, 3612, 538, 4084, 257, 707, 2445, 300, 321, 434, 516], "temperature": 0.0, "avg_logprob": -0.19037026893801806, "compression_ratio": 1.564516129032258, "no_speech_prob": 4.4254620661376975e-06}, {"id": 432, "seek": 217832, "start": 2203.76, "end": 2206.6000000000004, "text": " to pass 3 letters.", "tokens": [281, 1320, 805, 7825, 13], "temperature": 0.0, "avg_logprob": -0.19037026893801806, "compression_ratio": 1.564516129032258, "no_speech_prob": 4.4254620661376975e-06}, {"id": 433, "seek": 220660, "start": 2206.6, "end": 2212.24, "text": " We're going to take those 3 letters and turn them into character indices, so look them", "tokens": [492, 434, 516, 281, 747, 729, 805, 7825, 293, 1261, 552, 666, 2517, 43840, 11, 370, 574, 552], "temperature": 0.0, "avg_logprob": -0.19306999299584365, "compression_ratio": 1.5319148936170213, "no_speech_prob": 1.2411283023539e-05}, {"id": 434, "seek": 220660, "start": 2212.24, "end": 2219.24, "text": " up to find the indexes, turn each of those into a NumPy array, call model.predict on", "tokens": [493, 281, 915, 264, 8186, 279, 11, 1261, 1184, 295, 729, 666, 257, 22592, 47, 88, 10225, 11, 818, 2316, 13, 79, 24945, 322], "temperature": 0.0, "avg_logprob": -0.19306999299584365, "compression_ratio": 1.5319148936170213, "no_speech_prob": 1.2411283023539e-05}, {"id": 435, "seek": 220660, "start": 2219.24, "end": 2223.04, "text": " those 3 arrays.", "tokens": [729, 805, 41011, 13], "temperature": 0.0, "avg_logprob": -0.19306999299584365, "compression_ratio": 1.5319148936170213, "no_speech_prob": 1.2411283023539e-05}, {"id": 436, "seek": 220660, "start": 2223.04, "end": 2234.12, "text": " That gives us 86 outputs, which we then do aardmax to find which index into those 86", "tokens": [663, 2709, 505, 26687, 23930, 11, 597, 321, 550, 360, 257, 515, 41167, 281, 915, 597, 8186, 666, 729, 26687], "temperature": 0.0, "avg_logprob": -0.19306999299584365, "compression_ratio": 1.5319148936170213, "no_speech_prob": 1.2411283023539e-05}, {"id": 437, "seek": 220660, "start": 2234.12, "end": 2235.68, "text": " is the highest.", "tokens": [307, 264, 6343, 13], "temperature": 0.0, "avg_logprob": -0.19306999299584365, "compression_ratio": 1.5319148936170213, "no_speech_prob": 1.2411283023539e-05}, {"id": 438, "seek": 223568, "start": 2235.68, "end": 2238.44, "text": " And that's the character number that we want to return.", "tokens": [400, 300, 311, 264, 2517, 1230, 300, 321, 528, 281, 2736, 13], "temperature": 0.0, "avg_logprob": -0.1876721231560958, "compression_ratio": 1.7868020304568528, "no_speech_prob": 6.962155111978063e-06}, {"id": 439, "seek": 223568, "start": 2238.44, "end": 2244.3199999999997, "text": " So if we pass in PHI, it thinks that L is most likely next.", "tokens": [407, 498, 321, 1320, 294, 16530, 40, 11, 309, 7309, 300, 441, 307, 881, 3700, 958, 13], "temperature": 0.0, "avg_logprob": -0.1876721231560958, "compression_ratio": 1.7868020304568528, "no_speech_prob": 6.962155111978063e-06}, {"id": 440, "seek": 223568, "start": 2244.3199999999997, "end": 2247.68, "text": " Space TH, it thinks E is most likely next.", "tokens": [8705, 3578, 11, 309, 7309, 462, 307, 881, 3700, 958, 13], "temperature": 0.0, "avg_logprob": -0.1876721231560958, "compression_ratio": 1.7868020304568528, "no_speech_prob": 6.962155111978063e-06}, {"id": 441, "seek": 223568, "start": 2247.68, "end": 2251.54, "text": " Space AN, it thinks that D is most likely next.", "tokens": [8705, 5252, 11, 309, 7309, 300, 413, 307, 881, 3700, 958, 13], "temperature": 0.0, "avg_logprob": -0.1876721231560958, "compression_ratio": 1.7868020304568528, "no_speech_prob": 6.962155111978063e-06}, {"id": 442, "seek": 223568, "start": 2251.54, "end": 2256.52, "text": " So you can see that it seems to be doing a pretty reasonable job of taking 3 characters", "tokens": [407, 291, 393, 536, 300, 309, 2544, 281, 312, 884, 257, 1238, 10585, 1691, 295, 1940, 805, 4342], "temperature": 0.0, "avg_logprob": -0.1876721231560958, "compression_ratio": 1.7868020304568528, "no_speech_prob": 6.962155111978063e-06}, {"id": 443, "seek": 223568, "start": 2256.52, "end": 2262.0, "text": " and returning a 4th character that seems pretty sensible.", "tokens": [293, 12678, 257, 1017, 392, 2517, 300, 2544, 1238, 25380, 13], "temperature": 0.0, "avg_logprob": -0.1876721231560958, "compression_ratio": 1.7868020304568528, "no_speech_prob": 6.962155111978063e-06}, {"id": 444, "seek": 226200, "start": 2262.0, "end": 2270.16, "text": " Not the world's most powerful model, but a good example of how we can construct pretty", "tokens": [1726, 264, 1002, 311, 881, 4005, 2316, 11, 457, 257, 665, 1365, 295, 577, 321, 393, 7690, 1238], "temperature": 0.0, "avg_logprob": -0.2577804931222576, "compression_ratio": 1.482233502538071, "no_speech_prob": 3.2698717404855415e-05}, {"id": 445, "seek": 226200, "start": 2270.16, "end": 2277.4, "text": " arbitrary architectures using Keras and then letting SGD do the work.", "tokens": [23211, 6331, 1303, 1228, 591, 6985, 293, 550, 8295, 34520, 35, 360, 264, 589, 13], "temperature": 0.0, "avg_logprob": -0.2577804931222576, "compression_ratio": 1.482233502538071, "no_speech_prob": 3.2698717404855415e-05}, {"id": 446, "seek": 226200, "start": 2277.4, "end": 2285.92, "text": " Question on this model, how would it consider the context in which we're trying to predict", "tokens": [14464, 322, 341, 2316, 11, 577, 576, 309, 1949, 264, 4319, 294, 597, 321, 434, 1382, 281, 6069], "temperature": 0.0, "avg_logprob": -0.2577804931222576, "compression_ratio": 1.482233502538071, "no_speech_prob": 3.2698717404855415e-05}, {"id": 447, "seek": 226200, "start": 2285.92, "end": 2286.92, "text": " the next?", "tokens": [264, 958, 30], "temperature": 0.0, "avg_logprob": -0.2577804931222576, "compression_ratio": 1.482233502538071, "no_speech_prob": 3.2698717404855415e-05}, {"id": 448, "seek": 226200, "start": 2286.92, "end": 2289.08, "text": " There's nothing about the context.", "tokens": [821, 311, 1825, 466, 264, 4319, 13], "temperature": 0.0, "avg_logprob": -0.2577804931222576, "compression_ratio": 1.482233502538071, "no_speech_prob": 3.2698717404855415e-05}, {"id": 449, "seek": 228908, "start": 2289.08, "end": 2293.24, "text": " All it has at any point in time is the previous 3 characters.", "tokens": [1057, 309, 575, 412, 604, 935, 294, 565, 307, 264, 3894, 805, 4342, 13], "temperature": 0.0, "avg_logprob": -0.3498013423039363, "compression_ratio": 1.4050632911392404, "no_speech_prob": 2.627434514579363e-05}, {"id": 450, "seek": 228908, "start": 2293.24, "end": 2294.24, "text": " So it's not a great model.", "tokens": [407, 309, 311, 406, 257, 869, 2316, 13], "temperature": 0.0, "avg_logprob": -0.3498013423039363, "compression_ratio": 1.4050632911392404, "no_speech_prob": 2.627434514579363e-05}, {"id": 451, "seek": 228908, "start": 2294.24, "end": 2297.24, "text": " We've got to improve it though.", "tokens": [492, 600, 658, 281, 3470, 309, 1673, 13], "temperature": 0.0, "avg_logprob": -0.3498013423039363, "compression_ratio": 1.4050632911392404, "no_speech_prob": 2.627434514579363e-05}, {"id": 452, "seek": 228908, "start": 2297.24, "end": 2300.24, "text": " We've got to start somewhere.", "tokens": [492, 600, 658, 281, 722, 4079, 13], "temperature": 0.0, "avg_logprob": -0.3498013423039363, "compression_ratio": 1.4050632911392404, "no_speech_prob": 2.627434514579363e-05}, {"id": 453, "seek": 228908, "start": 2300.24, "end": 2318.4, "text": " In order to answer your question, let's build this up a little further.", "tokens": [682, 1668, 281, 1867, 428, 1168, 11, 718, 311, 1322, 341, 493, 257, 707, 3052, 13], "temperature": 0.0, "avg_logprob": -0.3498013423039363, "compression_ratio": 1.4050632911392404, "no_speech_prob": 2.627434514579363e-05}, {"id": 454, "seek": 231840, "start": 2318.4, "end": 2322.36, "text": " Rather than trying to predict character 4 from the previous 3 characters, let's try", "tokens": [16571, 813, 1382, 281, 6069, 2517, 1017, 490, 264, 3894, 805, 4342, 11, 718, 311, 853], "temperature": 0.0, "avg_logprob": -0.13430281008704234, "compression_ratio": 1.9573643410852712, "no_speech_prob": 1.2606738891918212e-05}, {"id": 455, "seek": 231840, "start": 2322.36, "end": 2326.96, "text": " and predict character N from the previous N-1 characters.", "tokens": [293, 6069, 2517, 426, 490, 264, 3894, 426, 12, 16, 4342, 13], "temperature": 0.0, "avg_logprob": -0.13430281008704234, "compression_ratio": 1.9573643410852712, "no_speech_prob": 1.2606738891918212e-05}, {"id": 456, "seek": 231840, "start": 2326.96, "end": 2331.08, "text": " And since all of these circles basically mean the same thing, which is the hidden state", "tokens": [400, 1670, 439, 295, 613, 13040, 1936, 914, 264, 912, 551, 11, 597, 307, 264, 7633, 1785], "temperature": 0.0, "avg_logprob": -0.13430281008704234, "compression_ratio": 1.9573643410852712, "no_speech_prob": 1.2606738891918212e-05}, {"id": 457, "seek": 231840, "start": 2331.08, "end": 2335.52, "text": " at this point, and since all of these orange arrows are literally the same thing, it's", "tokens": [412, 341, 935, 11, 293, 1670, 439, 295, 613, 7671, 19669, 366, 3736, 264, 912, 551, 11, 309, 311], "temperature": 0.0, "avg_logprob": -0.13430281008704234, "compression_ratio": 1.9573643410852712, "no_speech_prob": 1.2606738891918212e-05}, {"id": 458, "seek": 231840, "start": 2335.52, "end": 2339.5, "text": " a dense layer with exactly the same weight matrix, let's stick all of the circles on", "tokens": [257, 18011, 4583, 365, 2293, 264, 912, 3364, 8141, 11, 718, 311, 2897, 439, 295, 264, 13040, 322], "temperature": 0.0, "avg_logprob": -0.13430281008704234, "compression_ratio": 1.9573643410852712, "no_speech_prob": 1.2606738891918212e-05}, {"id": 459, "seek": 231840, "start": 2339.5, "end": 2344.32, "text": " top of each other, which means that these orange arrows can just become one arrow pointing", "tokens": [1192, 295, 1184, 661, 11, 597, 1355, 300, 613, 7671, 19669, 393, 445, 1813, 472, 11610, 12166], "temperature": 0.0, "avg_logprob": -0.13430281008704234, "compression_ratio": 1.9573643410852712, "no_speech_prob": 1.2606738891918212e-05}, {"id": 460, "seek": 231840, "start": 2344.32, "end": 2346.08, "text": " into itself.", "tokens": [666, 2564, 13], "temperature": 0.0, "avg_logprob": -0.13430281008704234, "compression_ratio": 1.9573643410852712, "no_speech_prob": 1.2606738891918212e-05}, {"id": 461, "seek": 234608, "start": 2346.08, "end": 2350.44, "text": " And this is the definition of a recurrent neural network.", "tokens": [400, 341, 307, 264, 7123, 295, 257, 18680, 1753, 18161, 3209, 13], "temperature": 0.0, "avg_logprob": -0.13803647834563923, "compression_ratio": 1.8142857142857143, "no_speech_prob": 4.860421086050337e-06}, {"id": 462, "seek": 234608, "start": 2350.44, "end": 2355.3199999999997, "text": " When we see it in this form, we say that we're looking at it in its recurrent form.", "tokens": [1133, 321, 536, 309, 294, 341, 1254, 11, 321, 584, 300, 321, 434, 1237, 412, 309, 294, 1080, 18680, 1753, 1254, 13], "temperature": 0.0, "avg_logprob": -0.13803647834563923, "compression_ratio": 1.8142857142857143, "no_speech_prob": 4.860421086050337e-06}, {"id": 463, "seek": 234608, "start": 2355.3199999999997, "end": 2360.56, "text": " When we see it in this form, we can say that we're looking at it in its unrolled form or", "tokens": [1133, 321, 536, 309, 294, 341, 1254, 11, 321, 393, 584, 300, 321, 434, 1237, 412, 309, 294, 1080, 517, 28850, 1254, 420], "temperature": 0.0, "avg_logprob": -0.13803647834563923, "compression_ratio": 1.8142857142857143, "no_speech_prob": 4.860421086050337e-06}, {"id": 464, "seek": 234608, "start": 2360.56, "end": 2362.36, "text": " unfolded form.", "tokens": [17980, 292, 1254, 13], "temperature": 0.0, "avg_logprob": -0.13803647834563923, "compression_ratio": 1.8142857142857143, "no_speech_prob": 4.860421086050337e-06}, {"id": 465, "seek": 234608, "start": 2362.36, "end": 2364.04, "text": " They're both very common.", "tokens": [814, 434, 1293, 588, 2689, 13], "temperature": 0.0, "avg_logprob": -0.13803647834563923, "compression_ratio": 1.8142857142857143, "no_speech_prob": 4.860421086050337e-06}, {"id": 466, "seek": 234608, "start": 2364.04, "end": 2371.6, "text": " This is obviously neater, and so for quickly sketching out an RNN architecture, this is", "tokens": [639, 307, 2745, 408, 771, 11, 293, 370, 337, 2661, 12325, 278, 484, 364, 45702, 45, 9482, 11, 341, 307], "temperature": 0.0, "avg_logprob": -0.13803647834563923, "compression_ratio": 1.8142857142857143, "no_speech_prob": 4.860421086050337e-06}, {"id": 467, "seek": 234608, "start": 2371.6, "end": 2372.6, "text": " much more convenient.", "tokens": [709, 544, 10851, 13], "temperature": 0.0, "avg_logprob": -0.13803647834563923, "compression_ratio": 1.8142857142857143, "no_speech_prob": 4.860421086050337e-06}, {"id": 468, "seek": 237260, "start": 2372.6, "end": 2376.2799999999997, "text": " But actually, this unrolled form is really important.", "tokens": [583, 767, 11, 341, 517, 28850, 1254, 307, 534, 1021, 13], "temperature": 0.0, "avg_logprob": -0.18125815921359592, "compression_ratio": 1.5739910313901346, "no_speech_prob": 7.527946763730142e-06}, {"id": 469, "seek": 237260, "start": 2376.2799999999997, "end": 2382.64, "text": " For example, when Keras uses TensorFlow as a backend, it actually always unrolls it in", "tokens": [1171, 1365, 11, 562, 591, 6985, 4960, 37624, 382, 257, 38087, 11, 309, 767, 1009, 517, 3970, 82, 309, 294], "temperature": 0.0, "avg_logprob": -0.18125815921359592, "compression_ratio": 1.5739910313901346, "no_speech_prob": 7.527946763730142e-06}, {"id": 470, "seek": 237260, "start": 2382.64, "end": 2387.24, "text": " this way in order to compute it.", "tokens": [341, 636, 294, 1668, 281, 14722, 309, 13], "temperature": 0.0, "avg_logprob": -0.18125815921359592, "compression_ratio": 1.5739910313901346, "no_speech_prob": 7.527946763730142e-06}, {"id": 471, "seek": 237260, "start": 2387.24, "end": 2392.88, "text": " That obviously takes up a lot more memory, and so it's quite nice being able to use the", "tokens": [663, 2745, 2516, 493, 257, 688, 544, 4675, 11, 293, 370, 309, 311, 1596, 1481, 885, 1075, 281, 764, 264], "temperature": 0.0, "avg_logprob": -0.18125815921359592, "compression_ratio": 1.5739910313901346, "no_speech_prob": 7.527946763730142e-06}, {"id": 472, "seek": 237260, "start": 2392.88, "end": 2398.96, "text": " Theano backend with Keras, which can actually directly implement it as this kind of loop.", "tokens": [440, 3730, 38087, 365, 591, 6985, 11, 597, 393, 767, 3838, 4445, 309, 382, 341, 733, 295, 6367, 13], "temperature": 0.0, "avg_logprob": -0.18125815921359592, "compression_ratio": 1.5739910313901346, "no_speech_prob": 7.527946763730142e-06}, {"id": 473, "seek": 239896, "start": 2398.96, "end": 2403.28, "text": " And that's what we'll be doing today.", "tokens": [400, 300, 311, 437, 321, 603, 312, 884, 965, 13], "temperature": 0.0, "avg_logprob": -0.24529517426782724, "compression_ratio": 1.7746478873239437, "no_speech_prob": 4.2646406654966995e-05}, {"id": 474, "seek": 239896, "start": 2403.28, "end": 2404.28, "text": " In general, we've got the same idea.", "tokens": [682, 2674, 11, 321, 600, 658, 264, 912, 1558, 13], "temperature": 0.0, "avg_logprob": -0.24529517426782724, "compression_ratio": 1.7746478873239437, "no_speech_prob": 4.2646406654966995e-05}, {"id": 475, "seek": 239896, "start": 2404.28, "end": 2411.68, "text": " We're going to have character1 input come in, go through the first green arrow, go through", "tokens": [492, 434, 516, 281, 362, 2517, 16, 4846, 808, 294, 11, 352, 807, 264, 700, 3092, 11610, 11, 352, 807], "temperature": 0.0, "avg_logprob": -0.24529517426782724, "compression_ratio": 1.7746478873239437, "no_speech_prob": 4.2646406654966995e-05}, {"id": 476, "seek": 239896, "start": 2411.68, "end": 2417.44, "text": " the first orange arrow, and then from then on we can just say take the second character,", "tokens": [264, 700, 7671, 11610, 11, 293, 550, 490, 550, 322, 321, 393, 445, 584, 747, 264, 1150, 2517, 11], "temperature": 0.0, "avg_logprob": -0.24529517426782724, "compression_ratio": 1.7746478873239437, "no_speech_prob": 4.2646406654966995e-05}, {"id": 477, "seek": 239896, "start": 2417.44, "end": 2419.64, "text": " repeat the third character, repeat.", "tokens": [7149, 264, 2636, 2517, 11, 7149, 13], "temperature": 0.0, "avg_logprob": -0.24529517426782724, "compression_ratio": 1.7746478873239437, "no_speech_prob": 4.2646406654966995e-05}, {"id": 478, "seek": 239896, "start": 2419.64, "end": 2425.64, "text": " And at each time period, we're getting a new character going through a layer operation,", "tokens": [400, 412, 1184, 565, 2896, 11, 321, 434, 1242, 257, 777, 2517, 516, 807, 257, 4583, 6916, 11], "temperature": 0.0, "avg_logprob": -0.24529517426782724, "compression_ratio": 1.7746478873239437, "no_speech_prob": 4.2646406654966995e-05}, {"id": 479, "seek": 242564, "start": 2425.64, "end": 2431.48, "text": " as well as taking the previous hidden state and putting it through its layer operation.", "tokens": [382, 731, 382, 1940, 264, 3894, 7633, 1785, 293, 3372, 309, 807, 1080, 4583, 6916, 13], "temperature": 0.0, "avg_logprob": -0.2249639226102281, "compression_ratio": 1.5844748858447488, "no_speech_prob": 8.013415936147794e-06}, {"id": 480, "seek": 242564, "start": 2431.48, "end": 2435.8799999999997, "text": " And then at the very end, we will put it through a different layer operation, the blue arrow,", "tokens": [400, 550, 412, 264, 588, 917, 11, 321, 486, 829, 309, 807, 257, 819, 4583, 6916, 11, 264, 3344, 11610, 11], "temperature": 0.0, "avg_logprob": -0.2249639226102281, "compression_ratio": 1.5844748858447488, "no_speech_prob": 8.013415936147794e-06}, {"id": 481, "seek": 242564, "start": 2435.8799999999997, "end": 2436.8799999999997, "text": " to get an output.", "tokens": [281, 483, 364, 5598, 13], "temperature": 0.0, "avg_logprob": -0.2249639226102281, "compression_ratio": 1.5844748858447488, "no_speech_prob": 8.013415936147794e-06}, {"id": 482, "seek": 242564, "start": 2436.8799999999997, "end": 2440.04, "text": " So I'm going to show you this in Keras now.", "tokens": [407, 286, 478, 516, 281, 855, 291, 341, 294, 591, 6985, 586, 13], "temperature": 0.0, "avg_logprob": -0.2249639226102281, "compression_ratio": 1.5844748858447488, "no_speech_prob": 8.013415936147794e-06}, {"id": 483, "seek": 242564, "start": 2440.04, "end": 2441.04, "text": " Question from the audience.", "tokens": [14464, 490, 264, 4034, 13], "temperature": 0.0, "avg_logprob": -0.2249639226102281, "compression_ratio": 1.5844748858447488, "no_speech_prob": 8.013415936147794e-06}, {"id": 484, "seek": 242564, "start": 2441.04, "end": 2450.56, "text": " Does every fully connected layer have to have the same activation function?", "tokens": [4402, 633, 4498, 4582, 4583, 362, 281, 362, 264, 912, 24433, 2445, 30], "temperature": 0.0, "avg_logprob": -0.2249639226102281, "compression_ratio": 1.5844748858447488, "no_speech_prob": 8.013415936147794e-06}, {"id": 485, "seek": 245056, "start": 2450.56, "end": 2454.84, "text": " In general, no.", "tokens": [682, 2674, 11, 572, 13], "temperature": 0.0, "avg_logprob": -0.1411363073142178, "compression_ratio": 1.6, "no_speech_prob": 1.892475484055467e-05}, {"id": 486, "seek": 245056, "start": 2454.84, "end": 2461.88, "text": " In all of the models we've seen so far, we have constructed them in a way where you can", "tokens": [682, 439, 295, 264, 5245, 321, 600, 1612, 370, 1400, 11, 321, 362, 17083, 552, 294, 257, 636, 689, 291, 393], "temperature": 0.0, "avg_logprob": -0.1411363073142178, "compression_ratio": 1.6, "no_speech_prob": 1.892475484055467e-05}, {"id": 487, "seek": 245056, "start": 2461.88, "end": 2467.44, "text": " write anything you like as the activation function.", "tokens": [2464, 1340, 291, 411, 382, 264, 24433, 2445, 13], "temperature": 0.0, "avg_logprob": -0.1411363073142178, "compression_ratio": 1.6, "no_speech_prob": 1.892475484055467e-05}, {"id": 488, "seek": 245056, "start": 2467.44, "end": 2474.6, "text": " In general, though, I haven't seen any examples of successful architectures which mix activation", "tokens": [682, 2674, 11, 1673, 11, 286, 2378, 380, 1612, 604, 5110, 295, 4406, 6331, 1303, 597, 2890, 24433], "temperature": 0.0, "avg_logprob": -0.1411363073142178, "compression_ratio": 1.6, "no_speech_prob": 1.892475484055467e-05}, {"id": 489, "seek": 245056, "start": 2474.6, "end": 2479.44, "text": " functions, other than of course that the output layer would pretty much always be a softmax", "tokens": [6828, 11, 661, 813, 295, 1164, 300, 264, 5598, 4583, 576, 1238, 709, 1009, 312, 257, 2787, 41167], "temperature": 0.0, "avg_logprob": -0.1411363073142178, "compression_ratio": 1.6, "no_speech_prob": 1.892475484055467e-05}, {"id": 490, "seek": 247944, "start": 2479.44, "end": 2482.12, "text": " for classification.", "tokens": [337, 21538, 13], "temperature": 0.0, "avg_logprob": -0.17181325199628117, "compression_ratio": 1.7677165354330708, "no_speech_prob": 1.3006969311391003e-05}, {"id": 491, "seek": 247944, "start": 2482.12, "end": 2486.92, "text": " I'm not sure it's not something that might become a good idea.", "tokens": [286, 478, 406, 988, 309, 311, 406, 746, 300, 1062, 1813, 257, 665, 1558, 13], "temperature": 0.0, "avg_logprob": -0.17181325199628117, "compression_ratio": 1.7677165354330708, "no_speech_prob": 1.3006969311391003e-05}, {"id": 492, "seek": 247944, "start": 2486.92, "end": 2492.6, "text": " It's just not something that anybody's done anything very successfully with so far.", "tokens": [467, 311, 445, 406, 746, 300, 4472, 311, 1096, 1340, 588, 10727, 365, 370, 1400, 13], "temperature": 0.0, "avg_logprob": -0.17181325199628117, "compression_ratio": 1.7677165354330708, "no_speech_prob": 1.3006969311391003e-05}, {"id": 493, "seek": 247944, "start": 2492.6, "end": 2499.36, "text": " I will mention something important about activation functions though, which is that you can use", "tokens": [286, 486, 2152, 746, 1021, 466, 24433, 6828, 1673, 11, 597, 307, 300, 291, 393, 764], "temperature": 0.0, "avg_logprob": -0.17181325199628117, "compression_ratio": 1.7677165354330708, "no_speech_prob": 1.3006969311391003e-05}, {"id": 494, "seek": 247944, "start": 2499.36, "end": 2504.36, "text": " pretty much any non-linear function as an activation function and get pretty reasonable", "tokens": [1238, 709, 604, 2107, 12, 28263, 2445, 382, 364, 24433, 2445, 293, 483, 1238, 10585], "temperature": 0.0, "avg_logprob": -0.17181325199628117, "compression_ratio": 1.7677165354330708, "no_speech_prob": 1.3006969311391003e-05}, {"id": 495, "seek": 247944, "start": 2504.36, "end": 2505.36, "text": " results.", "tokens": [3542, 13], "temperature": 0.0, "avg_logprob": -0.17181325199628117, "compression_ratio": 1.7677165354330708, "no_speech_prob": 1.3006969311391003e-05}, {"id": 496, "seek": 247944, "start": 2505.36, "end": 2508.28, "text": " There are actually some papers, some pretty cool papers people have written where they've", "tokens": [821, 366, 767, 512, 10577, 11, 512, 1238, 1627, 10577, 561, 362, 3720, 689, 436, 600], "temperature": 0.0, "avg_logprob": -0.17181325199628117, "compression_ratio": 1.7677165354330708, "no_speech_prob": 1.3006969311391003e-05}, {"id": 497, "seek": 250828, "start": 2508.28, "end": 2512.8, "text": " tried all kinds of weird activation functions and they pretty much all work.", "tokens": [3031, 439, 3685, 295, 3657, 24433, 6828, 293, 436, 1238, 709, 439, 589, 13], "temperature": 0.0, "avg_logprob": -0.17230710718366835, "compression_ratio": 1.553191489361702, "no_speech_prob": 7.411168098769849e-06}, {"id": 498, "seek": 250828, "start": 2512.8, "end": 2516.32, "text": " So it's not something to get hung up about.", "tokens": [407, 309, 311, 406, 746, 281, 483, 5753, 493, 466, 13], "temperature": 0.0, "avg_logprob": -0.17230710718366835, "compression_ratio": 1.553191489361702, "no_speech_prob": 7.411168098769849e-06}, {"id": 499, "seek": 250828, "start": 2516.32, "end": 2522.4, "text": " Certain activation functions will train more quickly and more resiliently.", "tokens": [13407, 24433, 6828, 486, 3847, 544, 2661, 293, 544, 23699, 356, 13], "temperature": 0.0, "avg_logprob": -0.17230710718366835, "compression_ratio": 1.553191489361702, "no_speech_prob": 7.411168098769849e-06}, {"id": 500, "seek": 250828, "start": 2522.4, "end": 2530.5600000000004, "text": " In particular, ReLU and ReLU variations tend to work particularly well.", "tokens": [682, 1729, 11, 1300, 43, 52, 293, 1300, 43, 52, 17840, 3928, 281, 589, 4098, 731, 13], "temperature": 0.0, "avg_logprob": -0.17230710718366835, "compression_ratio": 1.553191489361702, "no_speech_prob": 7.411168098769849e-06}, {"id": 501, "seek": 250828, "start": 2530.5600000000004, "end": 2535.32, "text": " So let's implement this.", "tokens": [407, 718, 311, 4445, 341, 13], "temperature": 0.0, "avg_logprob": -0.17230710718366835, "compression_ratio": 1.553191489361702, "no_speech_prob": 7.411168098769849e-06}, {"id": 502, "seek": 253532, "start": 2535.32, "end": 2540.88, "text": " So we're going to use a very similar approach to what we used before.", "tokens": [407, 321, 434, 516, 281, 764, 257, 588, 2531, 3109, 281, 437, 321, 1143, 949, 13], "temperature": 0.0, "avg_logprob": -0.18676683637830946, "compression_ratio": 1.8009708737864079, "no_speech_prob": 3.0717390473000705e-05}, {"id": 503, "seek": 253532, "start": 2540.88, "end": 2542.7200000000003, "text": " We're going to create our first RNN.", "tokens": [492, 434, 516, 281, 1884, 527, 700, 45702, 45, 13], "temperature": 0.0, "avg_logprob": -0.18676683637830946, "compression_ratio": 1.8009708737864079, "no_speech_prob": 3.0717390473000705e-05}, {"id": 504, "seek": 253532, "start": 2542.7200000000003, "end": 2549.92, "text": " We're going to create it from scratch using nothing but standard Keras dense layers.", "tokens": [492, 434, 516, 281, 1884, 309, 490, 8459, 1228, 1825, 457, 3832, 591, 6985, 18011, 7914, 13], "temperature": 0.0, "avg_logprob": -0.18676683637830946, "compression_ratio": 1.8009708737864079, "no_speech_prob": 3.0717390473000705e-05}, {"id": 505, "seek": 253532, "start": 2549.92, "end": 2557.04, "text": " In this case, the inputs will not be, we can't create C1, C2 and C3.", "tokens": [682, 341, 1389, 11, 264, 15743, 486, 406, 312, 11, 321, 393, 380, 1884, 383, 16, 11, 383, 17, 293, 383, 18, 13], "temperature": 0.0, "avg_logprob": -0.18676683637830946, "compression_ratio": 1.8009708737864079, "no_speech_prob": 3.0717390473000705e-05}, {"id": 506, "seek": 253532, "start": 2557.04, "end": 2560.0, "text": " We're going to have to create an array of our inputs.", "tokens": [492, 434, 516, 281, 362, 281, 1884, 364, 10225, 295, 527, 15743, 13], "temperature": 0.0, "avg_logprob": -0.18676683637830946, "compression_ratio": 1.8009708737864079, "no_speech_prob": 3.0717390473000705e-05}, {"id": 507, "seek": 253532, "start": 2560.0, "end": 2563.32, "text": " We're going to have to decide what N we're going to use.", "tokens": [492, 434, 516, 281, 362, 281, 4536, 437, 426, 321, 434, 516, 281, 764, 13], "temperature": 0.0, "avg_logprob": -0.18676683637830946, "compression_ratio": 1.8009708737864079, "no_speech_prob": 3.0717390473000705e-05}, {"id": 508, "seek": 256332, "start": 2563.32, "end": 2566.1600000000003, "text": " So for this one, I've decided to use 8.", "tokens": [407, 337, 341, 472, 11, 286, 600, 3047, 281, 764, 1649, 13], "temperature": 0.0, "avg_logprob": -0.17211863199869792, "compression_ratio": 1.7301587301587302, "no_speech_prob": 1.2029508070554584e-05}, {"id": 509, "seek": 256332, "start": 2566.1600000000003, "end": 2567.1600000000003, "text": " So CS is characters.", "tokens": [407, 9460, 307, 4342, 13], "temperature": 0.0, "avg_logprob": -0.17211863199869792, "compression_ratio": 1.7301587301587302, "no_speech_prob": 1.2029508070554584e-05}, {"id": 510, "seek": 256332, "start": 2567.1600000000003, "end": 2572.86, "text": " So I'm going to use 8 characters to predict the 9th character.", "tokens": [407, 286, 478, 516, 281, 764, 1649, 4342, 281, 6069, 264, 1722, 392, 2517, 13], "temperature": 0.0, "avg_logprob": -0.17211863199869792, "compression_ratio": 1.7301587301587302, "no_speech_prob": 1.2029508070554584e-05}, {"id": 511, "seek": 256332, "start": 2572.86, "end": 2578.0800000000004, "text": " So I'm going to create an array with 8 elements in it.", "tokens": [407, 286, 478, 516, 281, 1884, 364, 10225, 365, 1649, 4959, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.17211863199869792, "compression_ratio": 1.7301587301587302, "no_speech_prob": 1.2029508070554584e-05}, {"id": 512, "seek": 256332, "start": 2578.0800000000004, "end": 2587.2000000000003, "text": " And each element will contain a list of the 0, 8, 16, 24th character, the 1, 9, 17, etc.", "tokens": [400, 1184, 4478, 486, 5304, 257, 1329, 295, 264, 1958, 11, 1649, 11, 3165, 11, 4022, 392, 2517, 11, 264, 502, 11, 1722, 11, 3282, 11, 5183, 13], "temperature": 0.0, "avg_logprob": -0.17211863199869792, "compression_ratio": 1.7301587301587302, "no_speech_prob": 1.2029508070554584e-05}, {"id": 513, "seek": 256332, "start": 2587.2000000000003, "end": 2592.6400000000003, "text": " character, the 2, 10, 18, etc. character, just like before.", "tokens": [2517, 11, 264, 568, 11, 1266, 11, 2443, 11, 5183, 13, 2517, 11, 445, 411, 949, 13], "temperature": 0.0, "avg_logprob": -0.17211863199869792, "compression_ratio": 1.7301587301587302, "no_speech_prob": 1.2029508070554584e-05}, {"id": 514, "seek": 259264, "start": 2592.64, "end": 2600.48, "text": " So we're going to have a sequence of inputs where each one is offset by one from the previous", "tokens": [407, 321, 434, 516, 281, 362, 257, 8310, 295, 15743, 689, 1184, 472, 307, 18687, 538, 472, 490, 264, 3894], "temperature": 0.0, "avg_logprob": -0.15001859221347544, "compression_ratio": 1.6157894736842104, "no_speech_prob": 1.6701105778338388e-05}, {"id": 515, "seek": 259264, "start": 2600.48, "end": 2601.92, "text": " one.", "tokens": [472, 13], "temperature": 0.0, "avg_logprob": -0.15001859221347544, "compression_ratio": 1.6157894736842104, "no_speech_prob": 1.6701105778338388e-05}, {"id": 516, "seek": 259264, "start": 2601.92, "end": 2609.52, "text": " And then our output will be exactly the same thing except we're going to look at the indexed", "tokens": [400, 550, 527, 5598, 486, 312, 2293, 264, 912, 551, 3993, 321, 434, 516, 281, 574, 412, 264, 8186, 292], "temperature": 0.0, "avg_logprob": -0.15001859221347544, "compression_ratio": 1.6157894736842104, "no_speech_prob": 1.6701105778338388e-05}, {"id": 517, "seek": 259264, "start": 2609.52, "end": 2611.8399999999997, "text": " across by CS, so 8.", "tokens": [2108, 538, 9460, 11, 370, 1649, 13], "temperature": 0.0, "avg_logprob": -0.15001859221347544, "compression_ratio": 1.6157894736842104, "no_speech_prob": 1.6701105778338388e-05}, {"id": 518, "seek": 259264, "start": 2611.8399999999997, "end": 2614.64, "text": " So this will be the 8th thing in each sequence.", "tokens": [407, 341, 486, 312, 264, 1649, 392, 551, 294, 1184, 8310, 13], "temperature": 0.0, "avg_logprob": -0.15001859221347544, "compression_ratio": 1.6157894736842104, "no_speech_prob": 1.6701105778338388e-05}, {"id": 519, "seek": 259264, "start": 2614.64, "end": 2619.7, "text": " I'm going to predict it with the previous ones.", "tokens": [286, 478, 516, 281, 6069, 309, 365, 264, 3894, 2306, 13], "temperature": 0.0, "avg_logprob": -0.15001859221347544, "compression_ratio": 1.6157894736842104, "no_speech_prob": 1.6701105778338388e-05}, {"id": 520, "seek": 261970, "start": 2619.7, "end": 2625.04, "text": " So now we can go through every one of those input data items, lists, and turn them into", "tokens": [407, 586, 321, 393, 352, 807, 633, 472, 295, 729, 4846, 1412, 4754, 11, 14511, 11, 293, 1261, 552, 666], "temperature": 0.0, "avg_logprob": -0.186273175616597, "compression_ratio": 1.4761904761904763, "no_speech_prob": 1.1843025276903063e-05}, {"id": 521, "seek": 261970, "start": 2625.04, "end": 2627.22, "text": " a NumPy array.", "tokens": [257, 22592, 47, 88, 10225, 13], "temperature": 0.0, "avg_logprob": -0.186273175616597, "compression_ratio": 1.4761904761904763, "no_speech_prob": 1.1843025276903063e-05}, {"id": 522, "seek": 261970, "start": 2627.22, "end": 2639.04, "text": " And so here you can see that we have 8 inputs and each one is of length 75,000 or so.", "tokens": [400, 370, 510, 291, 393, 536, 300, 321, 362, 1649, 15743, 293, 1184, 472, 307, 295, 4641, 9562, 11, 1360, 420, 370, 13], "temperature": 0.0, "avg_logprob": -0.186273175616597, "compression_ratio": 1.4761904761904763, "no_speech_prob": 1.1843025276903063e-05}, {"id": 523, "seek": 261970, "start": 2639.04, "end": 2644.5, "text": " Do the same thing for our Y, create a NumPy array out of it.", "tokens": [1144, 264, 912, 551, 337, 527, 398, 11, 1884, 257, 22592, 47, 88, 10225, 484, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.186273175616597, "compression_ratio": 1.4761904761904763, "no_speech_prob": 1.1843025276903063e-05}, {"id": 524, "seek": 261970, "start": 2644.5, "end": 2649.2599999999998, "text": " And here we can visualize it.", "tokens": [400, 510, 321, 393, 23273, 309, 13], "temperature": 0.0, "avg_logprob": -0.186273175616597, "compression_ratio": 1.4761904761904763, "no_speech_prob": 1.1843025276903063e-05}, {"id": 525, "seek": 264926, "start": 2649.26, "end": 2657.28, "text": " So here are the first 8 elements of X.", "tokens": [407, 510, 366, 264, 700, 1649, 4959, 295, 1783, 13], "temperature": 0.0, "avg_logprob": -0.15084210262503675, "compression_ratio": 1.7740112994350283, "no_speech_prob": 8.664590495754965e-06}, {"id": 526, "seek": 264926, "start": 2657.28, "end": 2661.88, "text": " In looking at the first 8 elements of X, let's look at the very first element of each one,", "tokens": [682, 1237, 412, 264, 700, 1649, 4959, 295, 1783, 11, 718, 311, 574, 412, 264, 588, 700, 4478, 295, 1184, 472, 11], "temperature": 0.0, "avg_logprob": -0.15084210262503675, "compression_ratio": 1.7740112994350283, "no_speech_prob": 8.664590495754965e-06}, {"id": 527, "seek": 264926, "start": 2661.88, "end": 2663.5200000000004, "text": " 40, 42, 29.", "tokens": [3356, 11, 14034, 11, 9413, 13], "temperature": 0.0, "avg_logprob": -0.15084210262503675, "compression_ratio": 1.7740112994350283, "no_speech_prob": 8.664590495754965e-06}, {"id": 528, "seek": 264926, "start": 2663.5200000000004, "end": 2668.92, "text": " So this column is the first 8 characters of our text.", "tokens": [407, 341, 7738, 307, 264, 700, 1649, 4342, 295, 527, 2487, 13], "temperature": 0.0, "avg_logprob": -0.15084210262503675, "compression_ratio": 1.7740112994350283, "no_speech_prob": 8.664590495754965e-06}, {"id": 529, "seek": 264926, "start": 2668.92, "end": 2671.6000000000004, "text": " And here is the 9th character.", "tokens": [400, 510, 307, 264, 1722, 392, 2517, 13], "temperature": 0.0, "avg_logprob": -0.15084210262503675, "compression_ratio": 1.7740112994350283, "no_speech_prob": 8.664590495754965e-06}, {"id": 530, "seek": 264926, "start": 2671.6000000000004, "end": 2676.5600000000004, "text": " So the first thing that the model will try to do is to look at these 8 to predict this.", "tokens": [407, 264, 700, 551, 300, 264, 2316, 486, 853, 281, 360, 307, 281, 574, 412, 613, 1649, 281, 6069, 341, 13], "temperature": 0.0, "avg_logprob": -0.15084210262503675, "compression_ratio": 1.7740112994350283, "no_speech_prob": 8.664590495754965e-06}, {"id": 531, "seek": 267656, "start": 2676.56, "end": 2681.44, "text": " And then look at these 8 to predict this.", "tokens": [400, 550, 574, 412, 613, 1649, 281, 6069, 341, 13], "temperature": 0.0, "avg_logprob": -0.20648265921551248, "compression_ratio": 1.679245283018868, "no_speech_prob": 4.936923687637318e-06}, {"id": 532, "seek": 267656, "start": 2681.44, "end": 2688.4, "text": " And indeed you can see that this list here is exactly the same as this list here.", "tokens": [400, 6451, 291, 393, 536, 300, 341, 1329, 510, 307, 2293, 264, 912, 382, 341, 1329, 510, 13], "temperature": 0.0, "avg_logprob": -0.20648265921551248, "compression_ratio": 1.679245283018868, "no_speech_prob": 4.936923687637318e-06}, {"id": 533, "seek": 267656, "start": 2688.4, "end": 2693.08, "text": " The final character of each sequence is the same as the first character of the next sequence.", "tokens": [440, 2572, 2517, 295, 1184, 8310, 307, 264, 912, 382, 264, 700, 2517, 295, 264, 958, 8310, 13], "temperature": 0.0, "avg_logprob": -0.20648265921551248, "compression_ratio": 1.679245283018868, "no_speech_prob": 4.936923687637318e-06}, {"id": 534, "seek": 267656, "start": 2693.08, "end": 2697.12, "text": " So it's almost exactly the same as our previous data, we've just done it in a more flexible", "tokens": [407, 309, 311, 1920, 2293, 264, 912, 382, 527, 3894, 1412, 11, 321, 600, 445, 1096, 309, 294, 257, 544, 11358], "temperature": 0.0, "avg_logprob": -0.20648265921551248, "compression_ratio": 1.679245283018868, "no_speech_prob": 4.936923687637318e-06}, {"id": 535, "seek": 267656, "start": 2697.12, "end": 2698.12, "text": " way.", "tokens": [636, 13], "temperature": 0.0, "avg_logprob": -0.20648265921551248, "compression_ratio": 1.679245283018868, "no_speech_prob": 4.936923687637318e-06}, {"id": 536, "seek": 267656, "start": 2698.12, "end": 2703.48, "text": " We'll create 43 latent factors as before.", "tokens": [492, 603, 1884, 17914, 48994, 6771, 382, 949, 13], "temperature": 0.0, "avg_logprob": -0.20648265921551248, "compression_ratio": 1.679245283018868, "no_speech_prob": 4.936923687637318e-06}, {"id": 537, "seek": 270348, "start": 2703.48, "end": 2707.56, "text": " We'll use exactly the same embedding input function as before.", "tokens": [492, 603, 764, 2293, 264, 912, 12240, 3584, 4846, 2445, 382, 949, 13], "temperature": 0.0, "avg_logprob": -0.15119019532815003, "compression_ratio": 1.7848101265822784, "no_speech_prob": 1.2411366697051562e-05}, {"id": 538, "seek": 270348, "start": 2707.56, "end": 2712.0, "text": " And again, we're just going to have to use lists to store everything.", "tokens": [400, 797, 11, 321, 434, 445, 516, 281, 362, 281, 764, 14511, 281, 3531, 1203, 13], "temperature": 0.0, "avg_logprob": -0.15119019532815003, "compression_ratio": 1.7848101265822784, "no_speech_prob": 1.2411366697051562e-05}, {"id": 539, "seek": 270348, "start": 2712.0, "end": 2715.0, "text": " So in this case, all of our embeddings are going to be in the list.", "tokens": [407, 294, 341, 1389, 11, 439, 295, 527, 12240, 29432, 366, 516, 281, 312, 294, 264, 1329, 13], "temperature": 0.0, "avg_logprob": -0.15119019532815003, "compression_ratio": 1.7848101265822784, "no_speech_prob": 1.2411366697051562e-05}, {"id": 540, "seek": 270348, "start": 2715.0, "end": 2720.96, "text": " So we'll go through each of our characters and create an embedding input and output for", "tokens": [407, 321, 603, 352, 807, 1184, 295, 527, 4342, 293, 1884, 364, 12240, 3584, 4846, 293, 5598, 337], "temperature": 0.0, "avg_logprob": -0.15119019532815003, "compression_ratio": 1.7848101265822784, "no_speech_prob": 1.2411366697051562e-05}, {"id": 541, "seek": 270348, "start": 2720.96, "end": 2721.96, "text": " each one.", "tokens": [1184, 472, 13], "temperature": 0.0, "avg_logprob": -0.15119019532815003, "compression_ratio": 1.7848101265822784, "no_speech_prob": 1.2411366697051562e-05}, {"id": 542, "seek": 270348, "start": 2721.96, "end": 2725.44, "text": " We'll store it here.", "tokens": [492, 603, 3531, 309, 510, 13], "temperature": 0.0, "avg_logprob": -0.15119019532815003, "compression_ratio": 1.7848101265822784, "no_speech_prob": 1.2411366697051562e-05}, {"id": 543, "seek": 270348, "start": 2725.44, "end": 2729.96, "text": " And here we have, we're going to define them all at once, our green arrow, orange arrow", "tokens": [400, 510, 321, 362, 11, 321, 434, 516, 281, 6964, 552, 439, 412, 1564, 11, 527, 3092, 11610, 11, 7671, 11610], "temperature": 0.0, "avg_logprob": -0.15119019532815003, "compression_ratio": 1.7848101265822784, "no_speech_prob": 1.2411366697051562e-05}, {"id": 544, "seek": 270348, "start": 2729.96, "end": 2730.96, "text": " and blue arrow.", "tokens": [293, 3344, 11610, 13], "temperature": 0.0, "avg_logprob": -0.15119019532815003, "compression_ratio": 1.7848101265822784, "no_speech_prob": 1.2411366697051562e-05}, {"id": 545, "seek": 273096, "start": 2730.96, "end": 2735.8, "text": " So here we're basically saying we've got 3 different weight matrices that we want Keras", "tokens": [407, 510, 321, 434, 1936, 1566, 321, 600, 658, 805, 819, 3364, 32284, 300, 321, 528, 591, 6985], "temperature": 0.0, "avg_logprob": -0.19690362612406412, "compression_ratio": 1.680952380952381, "no_speech_prob": 6.854257208033232e-06}, {"id": 546, "seek": 273096, "start": 2735.8, "end": 2740.0, "text": " to keep track of for us.", "tokens": [281, 1066, 2837, 295, 337, 505, 13], "temperature": 0.0, "avg_logprob": -0.19690362612406412, "compression_ratio": 1.680952380952381, "no_speech_prob": 6.854257208033232e-06}, {"id": 547, "seek": 273096, "start": 2740.0, "end": 2748.76, "text": " So the very first hidden state here is going to take the list of all of our inputs, we're", "tokens": [407, 264, 588, 700, 7633, 1785, 510, 307, 516, 281, 747, 264, 1329, 295, 439, 295, 527, 15743, 11, 321, 434], "temperature": 0.0, "avg_logprob": -0.19690362612406412, "compression_ratio": 1.680952380952381, "no_speech_prob": 6.854257208033232e-06}, {"id": 548, "seek": 273096, "start": 2748.76, "end": 2752.96, "text": " going to take the first one of those, and then that's a tuple of 2 things.", "tokens": [516, 281, 747, 264, 700, 472, 295, 729, 11, 293, 550, 300, 311, 257, 2604, 781, 295, 568, 721, 13], "temperature": 0.0, "avg_logprob": -0.19690362612406412, "compression_ratio": 1.680952380952381, "no_speech_prob": 6.854257208033232e-06}, {"id": 549, "seek": 273096, "start": 2752.96, "end": 2756.2400000000002, "text": " The first is the input to it and the second is the output of the embedding.", "tokens": [440, 700, 307, 264, 4846, 281, 309, 293, 264, 1150, 307, 264, 5598, 295, 264, 12240, 3584, 13], "temperature": 0.0, "avg_logprob": -0.19690362612406412, "compression_ratio": 1.680952380952381, "no_speech_prob": 6.854257208033232e-06}, {"id": 550, "seek": 275624, "start": 2756.24, "end": 2761.8799999999997, "text": " So we're going to take the output of the embedding for the very first character, pass that into", "tokens": [407, 321, 434, 516, 281, 747, 264, 5598, 295, 264, 12240, 3584, 337, 264, 588, 700, 2517, 11, 1320, 300, 666], "temperature": 0.0, "avg_logprob": -0.12435943324391435, "compression_ratio": 1.6103286384976525, "no_speech_prob": 4.222810730425408e-06}, {"id": 551, "seek": 275624, "start": 2761.8799999999997, "end": 2768.24, "text": " our green arrow, and that's going to give us our initial hidden state.", "tokens": [527, 3092, 11610, 11, 293, 300, 311, 516, 281, 976, 505, 527, 5883, 7633, 1785, 13], "temperature": 0.0, "avg_logprob": -0.12435943324391435, "compression_ratio": 1.6103286384976525, "no_speech_prob": 4.222810730425408e-06}, {"id": 552, "seek": 275624, "start": 2768.24, "end": 2774.08, "text": " And then this looks exactly the same as we saw before, but rather than doing it listing", "tokens": [400, 550, 341, 1542, 2293, 264, 912, 382, 321, 1866, 949, 11, 457, 2831, 813, 884, 309, 22161], "temperature": 0.0, "avg_logprob": -0.12435943324391435, "compression_ratio": 1.6103286384976525, "no_speech_prob": 4.222810730425408e-06}, {"id": 553, "seek": 275624, "start": 2774.08, "end": 2779.4399999999996, "text": " separately, we're just going to loop through all of our remaining 1 through 8 characters", "tokens": [14759, 11, 321, 434, 445, 516, 281, 6367, 807, 439, 295, 527, 8877, 502, 807, 1649, 4342], "temperature": 0.0, "avg_logprob": -0.12435943324391435, "compression_ratio": 1.6103286384976525, "no_speech_prob": 4.222810730425408e-06}, {"id": 554, "seek": 277944, "start": 2779.44, "end": 2787.94, "text": " and go ahead and create the green arrow, orange arrow and add the 2 together.", "tokens": [293, 352, 2286, 293, 1884, 264, 3092, 11610, 11, 7671, 11610, 293, 909, 264, 568, 1214, 13], "temperature": 0.0, "avg_logprob": -0.16222285220497532, "compression_ratio": 1.7951219512195122, "no_speech_prob": 2.5612716854084283e-06}, {"id": 555, "seek": 277944, "start": 2787.94, "end": 2793.48, "text": " So finally we can take that final hidden state, put it through our blue arrow to create our", "tokens": [407, 2721, 321, 393, 747, 300, 2572, 7633, 1785, 11, 829, 309, 807, 527, 3344, 11610, 281, 1884, 527], "temperature": 0.0, "avg_logprob": -0.16222285220497532, "compression_ratio": 1.7951219512195122, "no_speech_prob": 2.5612716854084283e-06}, {"id": 556, "seek": 277944, "start": 2793.48, "end": 2795.32, "text": " final output.", "tokens": [2572, 5598, 13], "temperature": 0.0, "avg_logprob": -0.16222285220497532, "compression_ratio": 1.7951219512195122, "no_speech_prob": 2.5612716854084283e-06}, {"id": 557, "seek": 277944, "start": 2795.32, "end": 2802.04, "text": " So we can then tell Keras that our model is all of the embedding inputs for that list", "tokens": [407, 321, 393, 550, 980, 591, 6985, 300, 527, 2316, 307, 439, 295, 264, 12240, 3584, 15743, 337, 300, 1329], "temperature": 0.0, "avg_logprob": -0.16222285220497532, "compression_ratio": 1.7951219512195122, "no_speech_prob": 2.5612716854084283e-06}, {"id": 558, "seek": 277944, "start": 2802.04, "end": 2806.8, "text": " to be created together, that's our inputs, and then our output that we just created is", "tokens": [281, 312, 2942, 1214, 11, 300, 311, 527, 15743, 11, 293, 550, 527, 5598, 300, 321, 445, 2942, 307], "temperature": 0.0, "avg_logprob": -0.16222285220497532, "compression_ratio": 1.7951219512195122, "no_speech_prob": 2.5612716854084283e-06}, {"id": 559, "seek": 277944, "start": 2806.8, "end": 2808.3, "text": " the output.", "tokens": [264, 5598, 13], "temperature": 0.0, "avg_logprob": -0.16222285220497532, "compression_ratio": 1.7951219512195122, "no_speech_prob": 2.5612716854084283e-06}, {"id": 560, "seek": 280830, "start": 2808.3, "end": 2811.04, "text": " And we can go ahead and fit that model.", "tokens": [400, 321, 393, 352, 2286, 293, 3318, 300, 2316, 13], "temperature": 0.0, "avg_logprob": -0.15596428962603007, "compression_ratio": 1.4739884393063585, "no_speech_prob": 5.338133178156568e-06}, {"id": 561, "seek": 280830, "start": 2811.04, "end": 2817.76, "text": " So we would expect this to be more accurate because it's now got 8 pieces of context in", "tokens": [407, 321, 576, 2066, 341, 281, 312, 544, 8559, 570, 309, 311, 586, 658, 1649, 3755, 295, 4319, 294], "temperature": 0.0, "avg_logprob": -0.15596428962603007, "compression_ratio": 1.4739884393063585, "no_speech_prob": 5.338133178156568e-06}, {"id": 562, "seek": 280830, "start": 2817.76, "end": 2819.1200000000003, "text": " order to predict.", "tokens": [1668, 281, 6069, 13], "temperature": 0.0, "avg_logprob": -0.15596428962603007, "compression_ratio": 1.4739884393063585, "no_speech_prob": 5.338133178156568e-06}, {"id": 563, "seek": 280830, "start": 2819.1200000000003, "end": 2831.2400000000002, "text": " So previously we were getting this time we get down to 1.8.", "tokens": [407, 8046, 321, 645, 1242, 341, 565, 321, 483, 760, 281, 502, 13, 23, 13], "temperature": 0.0, "avg_logprob": -0.15596428962603007, "compression_ratio": 1.4739884393063585, "no_speech_prob": 5.338133178156568e-06}, {"id": 564, "seek": 280830, "start": 2831.2400000000002, "end": 2833.6800000000003, "text": " So it's still not great, but it's an improvement.", "tokens": [407, 309, 311, 920, 406, 869, 11, 457, 309, 311, 364, 10444, 13], "temperature": 0.0, "avg_logprob": -0.15596428962603007, "compression_ratio": 1.4739884393063585, "no_speech_prob": 5.338133178156568e-06}, {"id": 565, "seek": 283368, "start": 2833.68, "end": 2838.44, "text": " We can create exactly the same kind of test as before, so now we can pass in 8 characters", "tokens": [492, 393, 1884, 2293, 264, 912, 733, 295, 1500, 382, 949, 11, 370, 586, 321, 393, 1320, 294, 1649, 4342], "temperature": 0.0, "avg_logprob": -0.1283686917002608, "compression_ratio": 1.5098039215686274, "no_speech_prob": 1.5294074273697333e-06}, {"id": 566, "seek": 283368, "start": 2838.44, "end": 2840.68, "text": " and get a prediction of the 9th.", "tokens": [293, 483, 257, 17630, 295, 264, 1722, 392, 13], "temperature": 0.0, "avg_logprob": -0.1283686917002608, "compression_ratio": 1.5098039215686274, "no_speech_prob": 1.5294074273697333e-06}, {"id": 567, "seek": 283368, "start": 2840.68, "end": 2844.2, "text": " And these all look pretty reasonable.", "tokens": [400, 613, 439, 574, 1238, 10585, 13], "temperature": 0.0, "avg_logprob": -0.1283686917002608, "compression_ratio": 1.5098039215686274, "no_speech_prob": 1.5294074273697333e-06}, {"id": 568, "seek": 283368, "start": 2844.2, "end": 2850.48, "text": " So that is our first RNN that we've now built from scratch.", "tokens": [407, 300, 307, 527, 700, 45702, 45, 300, 321, 600, 586, 3094, 490, 8459, 13], "temperature": 0.0, "avg_logprob": -0.1283686917002608, "compression_ratio": 1.5098039215686274, "no_speech_prob": 1.5294074273697333e-06}, {"id": 569, "seek": 283368, "start": 2850.48, "end": 2858.22, "text": " This kind of RNN where we're taking a list and predicting a single thing is most likely", "tokens": [639, 733, 295, 45702, 45, 689, 321, 434, 1940, 257, 1329, 293, 32884, 257, 2167, 551, 307, 881, 3700], "temperature": 0.0, "avg_logprob": -0.1283686917002608, "compression_ratio": 1.5098039215686274, "no_speech_prob": 1.5294074273697333e-06}, {"id": 570, "seek": 285822, "start": 2858.22, "end": 2864.0, "text": " to be useful for things like sentiment analysis.", "tokens": [281, 312, 4420, 337, 721, 411, 16149, 5215, 13], "temperature": 0.0, "avg_logprob": -0.15316118001937867, "compression_ratio": 1.6095238095238096, "no_speech_prob": 3.187538140991819e-06}, {"id": 571, "seek": 285822, "start": 2864.0, "end": 2867.9399999999996, "text": " Remember our sentiment analysis example using IMDV?", "tokens": [5459, 527, 16149, 5215, 1365, 1228, 21463, 35, 53, 30], "temperature": 0.0, "avg_logprob": -0.15316118001937867, "compression_ratio": 1.6095238095238096, "no_speech_prob": 3.187538140991819e-06}, {"id": 572, "seek": 285822, "start": 2867.9399999999996, "end": 2872.8799999999997, "text": " So in this case we were taking a sequence, a list of words in a sentence, and predicting", "tokens": [407, 294, 341, 1389, 321, 645, 1940, 257, 8310, 11, 257, 1329, 295, 2283, 294, 257, 8174, 11, 293, 32884], "temperature": 0.0, "avg_logprob": -0.15316118001937867, "compression_ratio": 1.6095238095238096, "no_speech_prob": 3.187538140991819e-06}, {"id": 573, "seek": 285822, "start": 2872.8799999999997, "end": 2876.4399999999996, "text": " whether or not something is positive sentiment or negative sentiment.", "tokens": [1968, 420, 406, 746, 307, 3353, 16149, 420, 3671, 16149, 13], "temperature": 0.0, "avg_logprob": -0.15316118001937867, "compression_ratio": 1.6095238095238096, "no_speech_prob": 3.187538140991819e-06}, {"id": 574, "seek": 285822, "start": 2876.4399999999996, "end": 2886.3399999999997, "text": " So that would seem like an appropriate kind of use case for this style of RNN.", "tokens": [407, 300, 576, 1643, 411, 364, 6854, 733, 295, 764, 1389, 337, 341, 3758, 295, 45702, 45, 13], "temperature": 0.0, "avg_logprob": -0.15316118001937867, "compression_ratio": 1.6095238095238096, "no_speech_prob": 3.187538140991819e-06}, {"id": 575, "seek": 288634, "start": 2886.34, "end": 2893.04, "text": " So at that moment my computer crashed and we lost a little bit of the class's video.", "tokens": [407, 412, 300, 1623, 452, 3820, 24190, 293, 321, 2731, 257, 707, 857, 295, 264, 1508, 311, 960, 13], "temperature": 0.0, "avg_logprob": -0.14724485198063636, "compression_ratio": 1.5142857142857142, "no_speech_prob": 1.4285044017015025e-05}, {"id": 576, "seek": 288634, "start": 2893.04, "end": 2897.76, "text": " So I'm just going to fill in the bit that we missed here.", "tokens": [407, 286, 478, 445, 516, 281, 2836, 294, 264, 857, 300, 321, 6721, 510, 13], "temperature": 0.0, "avg_logprob": -0.14724485198063636, "compression_ratio": 1.5142857142857142, "no_speech_prob": 1.4285044017015025e-05}, {"id": 577, "seek": 288634, "start": 2897.76, "end": 2903.48, "text": " Sorry for the slight discontinuity.", "tokens": [4919, 337, 264, 4036, 31420, 21757, 13], "temperature": 0.0, "avg_logprob": -0.14724485198063636, "compression_ratio": 1.5142857142857142, "no_speech_prob": 1.4285044017015025e-05}, {"id": 578, "seek": 288634, "start": 2903.48, "end": 2907.6800000000003, "text": " So I wanted to show you something kind of interesting which you may have noticed which", "tokens": [407, 286, 1415, 281, 855, 291, 746, 733, 295, 1880, 597, 291, 815, 362, 5694, 597], "temperature": 0.0, "avg_logprob": -0.14724485198063636, "compression_ratio": 1.5142857142857142, "no_speech_prob": 1.4285044017015025e-05}, {"id": 579, "seek": 290768, "start": 2907.68, "end": 2920.2, "text": " is when we created our hidden dense layer, that is our orange arrow, I did not initialize", "tokens": [307, 562, 321, 2942, 527, 7633, 18011, 4583, 11, 300, 307, 527, 7671, 11610, 11, 286, 630, 406, 5883, 1125], "temperature": 0.0, "avg_logprob": -0.14371866498674665, "compression_ratio": 1.4806629834254144, "no_speech_prob": 1.1842721505672671e-05}, {"id": 580, "seek": 290768, "start": 2920.2, "end": 2925.8799999999997, "text": " it in the default way, which is the Gloro initialization, but instead I said init equals", "tokens": [309, 294, 264, 7576, 636, 11, 597, 307, 264, 5209, 10780, 5883, 2144, 11, 457, 2602, 286, 848, 3157, 6915], "temperature": 0.0, "avg_logprob": -0.14371866498674665, "compression_ratio": 1.4806629834254144, "no_speech_prob": 1.1842721505672671e-05}, {"id": 581, "seek": 290768, "start": 2925.8799999999997, "end": 2928.64, "text": " identity.", "tokens": [6575, 13], "temperature": 0.0, "avg_logprob": -0.14371866498674665, "compression_ratio": 1.4806629834254144, "no_speech_prob": 1.1842721505672671e-05}, {"id": 582, "seek": 290768, "start": 2928.64, "end": 2937.3599999999997, "text": " You may also have noticed that the equivalent thing was shown in our Keras RNN.", "tokens": [509, 815, 611, 362, 5694, 300, 264, 10344, 551, 390, 4898, 294, 527, 591, 6985, 45702, 45, 13], "temperature": 0.0, "avg_logprob": -0.14371866498674665, "compression_ratio": 1.4806629834254144, "no_speech_prob": 1.1842721505672671e-05}, {"id": 583, "seek": 293736, "start": 2937.36, "end": 2942.4, "text": " This here where it says inner init equals identity is referring to the same thing.", "tokens": [639, 510, 689, 309, 1619, 7284, 3157, 6915, 6575, 307, 13761, 281, 264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.11106557846069336, "compression_ratio": 1.7747747747747749, "no_speech_prob": 7.296260264411103e-06}, {"id": 584, "seek": 293736, "start": 2942.4, "end": 2948.96, "text": " It's referring to what is the initialization that is used for this orange arrow.", "tokens": [467, 311, 13761, 281, 437, 307, 264, 5883, 2144, 300, 307, 1143, 337, 341, 7671, 11610, 13], "temperature": 0.0, "avg_logprob": -0.11106557846069336, "compression_ratio": 1.7747747747747749, "no_speech_prob": 7.296260264411103e-06}, {"id": 585, "seek": 293736, "start": 2948.96, "end": 2952.1400000000003, "text": " How are those weights originally initialized?", "tokens": [1012, 366, 729, 17443, 7993, 5883, 1602, 30], "temperature": 0.0, "avg_logprob": -0.11106557846069336, "compression_ratio": 1.7747747747747749, "no_speech_prob": 7.296260264411103e-06}, {"id": 586, "seek": 293736, "start": 2952.1400000000003, "end": 2958.08, "text": " So rather than initializing them randomly we're going to initialize them with an identity", "tokens": [407, 2831, 813, 5883, 3319, 552, 16979, 321, 434, 516, 281, 5883, 1125, 552, 365, 364, 6575], "temperature": 0.0, "avg_logprob": -0.11106557846069336, "compression_ratio": 1.7747747747747749, "no_speech_prob": 7.296260264411103e-06}, {"id": 587, "seek": 293736, "start": 2958.08, "end": 2959.08, "text": " matrix.", "tokens": [8141, 13], "temperature": 0.0, "avg_logprob": -0.11106557846069336, "compression_ratio": 1.7747747747747749, "no_speech_prob": 7.296260264411103e-06}, {"id": 588, "seek": 293736, "start": 2959.08, "end": 2964.76, "text": " An identity matrix you may recall from your linear algebra at school is a matrix which", "tokens": [1107, 6575, 8141, 291, 815, 9901, 490, 428, 8213, 21989, 412, 1395, 307, 257, 8141, 597], "temperature": 0.0, "avg_logprob": -0.11106557846069336, "compression_ratio": 1.7747747747747749, "no_speech_prob": 7.296260264411103e-06}, {"id": 589, "seek": 296476, "start": 2964.76, "end": 2970.36, "text": " is all zeros except it is just ones down the diagonal.", "tokens": [307, 439, 35193, 3993, 309, 307, 445, 2306, 760, 264, 21539, 13], "temperature": 0.0, "avg_logprob": -0.07774972915649414, "compression_ratio": 1.5494505494505495, "no_speech_prob": 2.332065378141124e-06}, {"id": 590, "seek": 296476, "start": 2970.36, "end": 2976.84, "text": " So if you multiply any matrix by the identity matrix it doesn't change the original matrix", "tokens": [407, 498, 291, 12972, 604, 8141, 538, 264, 6575, 8141, 309, 1177, 380, 1319, 264, 3380, 8141], "temperature": 0.0, "avg_logprob": -0.07774972915649414, "compression_ratio": 1.5494505494505495, "no_speech_prob": 2.332065378141124e-06}, {"id": 591, "seek": 296476, "start": 2976.84, "end": 2977.84, "text": " at all.", "tokens": [412, 439, 13], "temperature": 0.0, "avg_logprob": -0.07774972915649414, "compression_ratio": 1.5494505494505495, "no_speech_prob": 2.332065378141124e-06}, {"id": 592, "seek": 296476, "start": 2977.84, "end": 2980.46, "text": " You get back exactly what you started with.", "tokens": [509, 483, 646, 2293, 437, 291, 1409, 365, 13], "temperature": 0.0, "avg_logprob": -0.07774972915649414, "compression_ratio": 1.5494505494505495, "no_speech_prob": 2.332065378141124e-06}, {"id": 593, "seek": 296476, "start": 2980.46, "end": 2987.6800000000003, "text": " So in other words we're going to start off by initializing our orange arrow not with", "tokens": [407, 294, 661, 2283, 321, 434, 516, 281, 722, 766, 538, 5883, 3319, 527, 7671, 11610, 406, 365], "temperature": 0.0, "avg_logprob": -0.07774972915649414, "compression_ratio": 1.5494505494505495, "no_speech_prob": 2.332065378141124e-06}, {"id": 594, "seek": 298768, "start": 2987.68, "end": 2996.9199999999996, "text": " a random matrix but with a matrix that causes the hidden state to not change at all.", "tokens": [257, 4974, 8141, 457, 365, 257, 8141, 300, 7700, 264, 7633, 1785, 281, 406, 1319, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.08595238233867444, "compression_ratio": 1.5612244897959184, "no_speech_prob": 1.6028066056605894e-06}, {"id": 595, "seek": 298768, "start": 2996.9199999999996, "end": 3000.08, "text": " That makes some intuitive sense.", "tokens": [663, 1669, 512, 21769, 2020, 13], "temperature": 0.0, "avg_logprob": -0.08595238233867444, "compression_ratio": 1.5612244897959184, "no_speech_prob": 1.6028066056605894e-06}, {"id": 596, "seek": 298768, "start": 3000.08, "end": 3004.56, "text": " It seems reasonable to say well in the absence of other knowledge to the contrary why don't", "tokens": [467, 2544, 10585, 281, 584, 731, 294, 264, 17145, 295, 661, 3601, 281, 264, 19506, 983, 500, 380], "temperature": 0.0, "avg_logprob": -0.08595238233867444, "compression_ratio": 1.5612244897959184, "no_speech_prob": 1.6028066056605894e-06}, {"id": 597, "seek": 298768, "start": 3004.56, "end": 3011.3599999999997, "text": " we start off by having the hidden state stay the same until the SGD has a chance to update", "tokens": [321, 722, 766, 538, 1419, 264, 7633, 1785, 1754, 264, 912, 1826, 264, 34520, 35, 575, 257, 2931, 281, 5623], "temperature": 0.0, "avg_logprob": -0.08595238233867444, "compression_ratio": 1.5612244897959184, "no_speech_prob": 1.6028066056605894e-06}, {"id": 598, "seek": 298768, "start": 3011.3599999999997, "end": 3012.3599999999997, "text": " that.", "tokens": [300, 13], "temperature": 0.0, "avg_logprob": -0.08595238233867444, "compression_ratio": 1.5612244897959184, "no_speech_prob": 1.6028066056605894e-06}, {"id": 599, "seek": 301236, "start": 3012.36, "end": 3018.4, "text": " But it actually turns out that it also makes sense based on an empirical analysis.", "tokens": [583, 309, 767, 4523, 484, 300, 309, 611, 1669, 2020, 2361, 322, 364, 31886, 5215, 13], "temperature": 0.0, "avg_logprob": -0.0840875988914853, "compression_ratio": 1.5990990990990992, "no_speech_prob": 4.6378322622331325e-06}, {"id": 600, "seek": 301236, "start": 3018.4, "end": 3023.7200000000003, "text": " So since we always only do things that Jeffrey Hinton tells us to do that's good news because", "tokens": [407, 1670, 321, 1009, 787, 360, 721, 300, 28721, 389, 12442, 5112, 505, 281, 360, 300, 311, 665, 2583, 570], "temperature": 0.0, "avg_logprob": -0.0840875988914853, "compression_ratio": 1.5990990990990992, "no_speech_prob": 4.6378322622331325e-06}, {"id": 601, "seek": 301236, "start": 3023.7200000000003, "end": 3030.0, "text": " this is a paper by Jeff Hinton in which he points out this rather neat trick which is", "tokens": [341, 307, 257, 3035, 538, 7506, 389, 12442, 294, 597, 415, 2793, 484, 341, 2831, 10654, 4282, 597, 307], "temperature": 0.0, "avg_logprob": -0.0840875988914853, "compression_ratio": 1.5990990990990992, "no_speech_prob": 4.6378322622331325e-06}, {"id": 602, "seek": 301236, "start": 3030.0, "end": 3040.0, "text": " if you initialize an RNN with the hidden weight matrix initialized to an identity matrix and", "tokens": [498, 291, 5883, 1125, 364, 45702, 45, 365, 264, 7633, 3364, 8141, 5883, 1602, 281, 364, 6575, 8141, 293], "temperature": 0.0, "avg_logprob": -0.0840875988914853, "compression_ratio": 1.5990990990990992, "no_speech_prob": 4.6378322622331325e-06}, {"id": 603, "seek": 304000, "start": 3040.0, "end": 3051.28, "text": " use rectified linear units as we are here you actually get an architecture which can", "tokens": [764, 11048, 2587, 8213, 6815, 382, 321, 366, 510, 291, 767, 483, 364, 9482, 597, 393], "temperature": 0.0, "avg_logprob": -0.11138332085531266, "compression_ratio": 1.5051546391752577, "no_speech_prob": 6.6433231040718965e-06}, {"id": 604, "seek": 304000, "start": 3051.28, "end": 3059.88, "text": " get fantastic results on some reasonably significant problems including speech recognition and", "tokens": [483, 5456, 3542, 322, 512, 23551, 4776, 2740, 3009, 6218, 11150, 293], "temperature": 0.0, "avg_logprob": -0.11138332085531266, "compression_ratio": 1.5051546391752577, "no_speech_prob": 6.6433231040718965e-06}, {"id": 605, "seek": 304000, "start": 3059.88, "end": 3061.16, "text": " language modeling.", "tokens": [2856, 15983, 13], "temperature": 0.0, "avg_logprob": -0.11138332085531266, "compression_ratio": 1.5051546391752577, "no_speech_prob": 6.6433231040718965e-06}, {"id": 606, "seek": 304000, "start": 3061.16, "end": 3067.84, "text": " I don't see this paper referred to or discussed very often even though it is well over a year", "tokens": [286, 500, 380, 536, 341, 3035, 10839, 281, 420, 7152, 588, 2049, 754, 1673, 309, 307, 731, 670, 257, 1064], "temperature": 0.0, "avg_logprob": -0.11138332085531266, "compression_ratio": 1.5051546391752577, "no_speech_prob": 6.6433231040718965e-06}, {"id": 607, "seek": 306784, "start": 3067.84, "end": 3073.1200000000003, "text": " old now so I'm not sure if people forgot about it or haven't noticed it or what but this", "tokens": [1331, 586, 370, 286, 478, 406, 988, 498, 561, 5298, 466, 309, 420, 2378, 380, 5694, 309, 420, 437, 457, 341], "temperature": 0.0, "avg_logprob": -0.14081549000095678, "compression_ratio": 1.5124378109452736, "no_speech_prob": 1.644195981498342e-05}, {"id": 608, "seek": 306784, "start": 3073.1200000000003, "end": 3079.56, "text": " is actually a good trick to remember is that you can often get quite a long way doing nothing", "tokens": [307, 767, 257, 665, 4282, 281, 1604, 307, 300, 291, 393, 2049, 483, 1596, 257, 938, 636, 884, 1825], "temperature": 0.0, "avg_logprob": -0.14081549000095678, "compression_ratio": 1.5124378109452736, "no_speech_prob": 1.644195981498342e-05}, {"id": 609, "seek": 306784, "start": 3079.56, "end": 3085.7200000000003, "text": " but an identity matrix initialization and rectified linear units in just as we have", "tokens": [457, 364, 6575, 8141, 5883, 2144, 293, 11048, 2587, 8213, 6815, 294, 445, 382, 321, 362], "temperature": 0.0, "avg_logprob": -0.14081549000095678, "compression_ratio": 1.5124378109452736, "no_speech_prob": 1.644195981498342e-05}, {"id": 610, "seek": 306784, "start": 3085.7200000000003, "end": 3091.04, "text": " done here to set up our architecture.", "tokens": [1096, 510, 281, 992, 493, 527, 9482, 13], "temperature": 0.0, "avg_logprob": -0.14081549000095678, "compression_ratio": 1.5124378109452736, "no_speech_prob": 1.644195981498342e-05}, {"id": 611, "seek": 309104, "start": 3091.04, "end": 3102.2, "text": " Ok so that's a nice little trick to remember and so the next thing we're going to do is", "tokens": [3477, 370, 300, 311, 257, 1481, 707, 4282, 281, 1604, 293, 370, 264, 958, 551, 321, 434, 516, 281, 360, 307], "temperature": 0.0, "avg_logprob": -0.11704213619232177, "compression_ratio": 1.8411764705882352, "no_speech_prob": 2.1568077954725595e-06}, {"id": 612, "seek": 309104, "start": 3102.2, "end": 3107.2799999999997, "text": " to make a couple of minor changes to this diagram.", "tokens": [281, 652, 257, 1916, 295, 6696, 2962, 281, 341, 10686, 13], "temperature": 0.0, "avg_logprob": -0.11704213619232177, "compression_ratio": 1.8411764705882352, "no_speech_prob": 2.1568077954725595e-06}, {"id": 613, "seek": 309104, "start": 3107.2799999999997, "end": 3111.12, "text": " So the first change we're going to make is we're going to take this rectangle here so", "tokens": [407, 264, 700, 1319, 321, 434, 516, 281, 652, 307, 321, 434, 516, 281, 747, 341, 21930, 510, 370], "temperature": 0.0, "avg_logprob": -0.11704213619232177, "compression_ratio": 1.8411764705882352, "no_speech_prob": 2.1568077954725595e-06}, {"id": 614, "seek": 309104, "start": 3111.12, "end": 3117.4, "text": " this rectangle is referring to what is it that we repeat and so since in this case we're", "tokens": [341, 21930, 307, 13761, 281, 437, 307, 309, 300, 321, 7149, 293, 370, 1670, 294, 341, 1389, 321, 434], "temperature": 0.0, "avg_logprob": -0.11704213619232177, "compression_ratio": 1.8411764705882352, "no_speech_prob": 2.1568077954725595e-06}, {"id": 615, "seek": 311740, "start": 3117.4, "end": 3127.96, "text": " predicting character n from characters 1 through n-1 then this whole area here we're looping", "tokens": [32884, 2517, 297, 490, 4342, 502, 807, 297, 12, 16, 550, 341, 1379, 1859, 510, 321, 434, 6367, 278], "temperature": 0.0, "avg_logprob": -0.11878925416527725, "compression_ratio": 1.8146067415730338, "no_speech_prob": 4.860416538576828e-06}, {"id": 616, "seek": 311740, "start": 3127.96, "end": 3133.56, "text": " from 2 to n-1 before we generate our output once again.", "tokens": [490, 568, 281, 297, 12, 16, 949, 321, 8460, 527, 5598, 1564, 797, 13], "temperature": 0.0, "avg_logprob": -0.11878925416527725, "compression_ratio": 1.8146067415730338, "no_speech_prob": 4.860416538576828e-06}, {"id": 617, "seek": 311740, "start": 3133.56, "end": 3137.6, "text": " So what we're going to do is we're going to take this triangle and we're going to put", "tokens": [407, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 747, 341, 13369, 293, 321, 434, 516, 281, 829], "temperature": 0.0, "avg_logprob": -0.11878925416527725, "compression_ratio": 1.8146067415730338, "no_speech_prob": 4.860416538576828e-06}, {"id": 618, "seek": 311740, "start": 3137.6, "end": 3143.6600000000003, "text": " it inside the loop put it inside the rectangle and so what that means is that every time", "tokens": [309, 1854, 264, 6367, 829, 309, 1854, 264, 21930, 293, 370, 437, 300, 1355, 307, 300, 633, 565], "temperature": 0.0, "avg_logprob": -0.11878925416527725, "compression_ratio": 1.8146067415730338, "no_speech_prob": 4.860416538576828e-06}, {"id": 619, "seek": 314366, "start": 3143.66, "end": 3148.56, "text": " we loop through this we're going to generate another output so rather than generating one", "tokens": [321, 6367, 807, 341, 321, 434, 516, 281, 8460, 1071, 5598, 370, 2831, 813, 17746, 472], "temperature": 0.0, "avg_logprob": -0.12829483879937065, "compression_ratio": 2.1645569620253164, "no_speech_prob": 3.90546574635664e-06}, {"id": 620, "seek": 314366, "start": 3148.56, "end": 3156.0, "text": " output at the end this is going to predict characters 2 through n using characters n-1", "tokens": [5598, 412, 264, 917, 341, 307, 516, 281, 6069, 4342, 568, 807, 297, 1228, 4342, 297, 12, 16], "temperature": 0.0, "avg_logprob": -0.12829483879937065, "compression_ratio": 2.1645569620253164, "no_speech_prob": 3.90546574635664e-06}, {"id": 621, "seek": 314366, "start": 3156.0, "end": 3163.64, "text": " through n-1 so it's going to predict character 2 using character 1 and character 3 using", "tokens": [807, 297, 12, 16, 370, 309, 311, 516, 281, 6069, 2517, 568, 1228, 2517, 502, 293, 2517, 805, 1228], "temperature": 0.0, "avg_logprob": -0.12829483879937065, "compression_ratio": 2.1645569620253164, "no_speech_prob": 3.90546574635664e-06}, {"id": 622, "seek": 314366, "start": 3163.64, "end": 3170.52, "text": " characters 1 and 2 and character 4 using characters 1, 2 and 3 and so forth.", "tokens": [4342, 502, 293, 568, 293, 2517, 1017, 1228, 4342, 502, 11, 568, 293, 805, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.12829483879937065, "compression_ratio": 2.1645569620253164, "no_speech_prob": 3.90546574635664e-06}, {"id": 623, "seek": 317052, "start": 3170.52, "end": 3174.28, "text": " And so that's what this model would do it's nearly exactly the same as the previous model", "tokens": [400, 370, 300, 311, 437, 341, 2316, 576, 360, 309, 311, 6217, 2293, 264, 912, 382, 264, 3894, 2316], "temperature": 0.0, "avg_logprob": -0.09051238948648627, "compression_ratio": 1.9086538461538463, "no_speech_prob": 1.933349039973109e-06}, {"id": 624, "seek": 317052, "start": 3174.28, "end": 3180.8, "text": " except after every single step after creating the hidden state on every step we're going", "tokens": [3993, 934, 633, 2167, 1823, 934, 4084, 264, 7633, 1785, 322, 633, 1823, 321, 434, 516], "temperature": 0.0, "avg_logprob": -0.09051238948648627, "compression_ratio": 1.9086538461538463, "no_speech_prob": 1.933349039973109e-06}, {"id": 625, "seek": 317052, "start": 3180.8, "end": 3183.12, "text": " to create an output every time.", "tokens": [281, 1884, 364, 5598, 633, 565, 13], "temperature": 0.0, "avg_logprob": -0.09051238948648627, "compression_ratio": 1.9086538461538463, "no_speech_prob": 1.933349039973109e-06}, {"id": 626, "seek": 317052, "start": 3183.12, "end": 3189.04, "text": " So this is not going to create a single output like this does which predicted a single character", "tokens": [407, 341, 307, 406, 516, 281, 1884, 257, 2167, 5598, 411, 341, 775, 597, 19147, 257, 2167, 2517], "temperature": 0.0, "avg_logprob": -0.09051238948648627, "compression_ratio": 1.9086538461538463, "no_speech_prob": 1.933349039973109e-06}, {"id": 627, "seek": 317052, "start": 3189.04, "end": 3194.52, "text": " the last character or in fact the next after the last character of the sequence character", "tokens": [264, 1036, 2517, 420, 294, 1186, 264, 958, 934, 264, 1036, 2517, 295, 264, 8310, 2517], "temperature": 0.0, "avg_logprob": -0.09051238948648627, "compression_ratio": 1.9086538461538463, "no_speech_prob": 1.933349039973109e-06}, {"id": 628, "seek": 319452, "start": 3194.52, "end": 3200.68, "text": " n using characters 1 through n-1 this is going to predict a whole sequence of characters", "tokens": [297, 1228, 4342, 502, 807, 297, 12, 16, 341, 307, 516, 281, 6069, 257, 1379, 8310, 295, 4342], "temperature": 0.0, "avg_logprob": -0.16212254478817895, "compression_ratio": 1.7553191489361701, "no_speech_prob": 1.22189630928915e-05}, {"id": 629, "seek": 319452, "start": 3200.68, "end": 3206.7599999999998, "text": " 2 through n using characters 1 through n-1.", "tokens": [568, 807, 297, 1228, 4342, 502, 807, 297, 12, 16, 13], "temperature": 0.0, "avg_logprob": -0.16212254478817895, "compression_ratio": 1.7553191489361701, "no_speech_prob": 1.22189630928915e-05}, {"id": 630, "seek": 319452, "start": 3206.7599999999998, "end": 3211.56, "text": " Okay so that was all the stuff that we lost when we had our computer crash so let's now", "tokens": [1033, 370, 300, 390, 439, 264, 1507, 300, 321, 2731, 562, 321, 632, 527, 3820, 8252, 370, 718, 311, 586], "temperature": 0.0, "avg_logprob": -0.16212254478817895, "compression_ratio": 1.7553191489361701, "no_speech_prob": 1.22189630928915e-05}, {"id": 631, "seek": 319452, "start": 3211.56, "end": 3214.48, "text": " go back to the lesson.", "tokens": [352, 646, 281, 264, 6898, 13], "temperature": 0.0, "avg_logprob": -0.16212254478817895, "compression_ratio": 1.7553191489361701, "no_speech_prob": 1.22189630928915e-05}, {"id": 632, "seek": 319452, "start": 3214.48, "end": 3218.44, "text": " Let's now talk about how we would implement this sequence where we're going to predict", "tokens": [961, 311, 586, 751, 466, 577, 321, 576, 4445, 341, 8310, 689, 321, 434, 516, 281, 6069], "temperature": 0.0, "avg_logprob": -0.16212254478817895, "compression_ratio": 1.7553191489361701, "no_speech_prob": 1.22189630928915e-05}, {"id": 633, "seek": 321844, "start": 3218.44, "end": 3224.56, "text": " characters 2 through n using characters 1 through n-1.", "tokens": [4342, 568, 807, 297, 1228, 4342, 502, 807, 297, 12, 16, 13], "temperature": 0.0, "avg_logprob": -0.1534576748692712, "compression_ratio": 1.6318181818181818, "no_speech_prob": 6.439003300329205e-06}, {"id": 634, "seek": 321844, "start": 3224.56, "end": 3227.12, "text": " Now why would this be a good idea?", "tokens": [823, 983, 576, 341, 312, 257, 665, 1558, 30], "temperature": 0.0, "avg_logprob": -0.1534576748692712, "compression_ratio": 1.6318181818181818, "no_speech_prob": 6.439003300329205e-06}, {"id": 635, "seek": 321844, "start": 3227.12, "end": 3231.8, "text": " There's a few reasons but one obvious reason why this would be a good idea is that if we're", "tokens": [821, 311, 257, 1326, 4112, 457, 472, 6322, 1778, 983, 341, 576, 312, 257, 665, 1558, 307, 300, 498, 321, 434], "temperature": 0.0, "avg_logprob": -0.1534576748692712, "compression_ratio": 1.6318181818181818, "no_speech_prob": 6.439003300329205e-06}, {"id": 636, "seek": 321844, "start": 3231.8, "end": 3239.54, "text": " only predicting one output for every n inputs then the number of times that our model has", "tokens": [787, 32884, 472, 5598, 337, 633, 297, 15743, 550, 264, 1230, 295, 1413, 300, 527, 2316, 575], "temperature": 0.0, "avg_logprob": -0.1534576748692712, "compression_ratio": 1.6318181818181818, "no_speech_prob": 6.439003300329205e-06}, {"id": 637, "seek": 321844, "start": 3239.54, "end": 3245.44, "text": " the opportunity to propagate those in great gradients and improve those weights is just", "tokens": [264, 2650, 281, 48256, 729, 294, 869, 2771, 2448, 293, 3470, 729, 17443, 307, 445], "temperature": 0.0, "avg_logprob": -0.1534576748692712, "compression_ratio": 1.6318181818181818, "no_speech_prob": 6.439003300329205e-06}, {"id": 638, "seek": 324544, "start": 3245.44, "end": 3249.28, "text": " once for each sequence of characters.", "tokens": [1564, 337, 1184, 8310, 295, 4342, 13], "temperature": 0.0, "avg_logprob": -0.10529392885874553, "compression_ratio": 1.6231884057971016, "no_speech_prob": 3.7266242998157395e-06}, {"id": 639, "seek": 324544, "start": 3249.28, "end": 3255.92, "text": " If we predict characters 2 through n using characters 1 through n-1, we're actually getting", "tokens": [759, 321, 6069, 4342, 568, 807, 297, 1228, 4342, 502, 807, 297, 12, 16, 11, 321, 434, 767, 1242], "temperature": 0.0, "avg_logprob": -0.10529392885874553, "compression_ratio": 1.6231884057971016, "no_speech_prob": 3.7266242998157395e-06}, {"id": 640, "seek": 324544, "start": 3255.92, "end": 3258.88, "text": " a whole lot of feedback about how our model is going.", "tokens": [257, 1379, 688, 295, 5824, 466, 577, 527, 2316, 307, 516, 13], "temperature": 0.0, "avg_logprob": -0.10529392885874553, "compression_ratio": 1.6231884057971016, "no_speech_prob": 3.7266242998157395e-06}, {"id": 641, "seek": 324544, "start": 3258.88, "end": 3267.64, "text": " So we can back propagate n-1 times every time we do another sequence.", "tokens": [407, 321, 393, 646, 48256, 297, 12, 16, 1413, 633, 565, 321, 360, 1071, 8310, 13], "temperature": 0.0, "avg_logprob": -0.10529392885874553, "compression_ratio": 1.6231884057971016, "no_speech_prob": 3.7266242998157395e-06}, {"id": 642, "seek": 324544, "start": 3267.64, "end": 3275.28, "text": " So there's a lot more learning going on for nearly the same amount of computation.", "tokens": [407, 456, 311, 257, 688, 544, 2539, 516, 322, 337, 6217, 264, 912, 2372, 295, 24903, 13], "temperature": 0.0, "avg_logprob": -0.10529392885874553, "compression_ratio": 1.6231884057971016, "no_speech_prob": 3.7266242998157395e-06}, {"id": 643, "seek": 327528, "start": 3275.28, "end": 3279.0, "text": " The other reason this is handy is that as you'll see in a moment, it's very helpful", "tokens": [440, 661, 1778, 341, 307, 13239, 307, 300, 382, 291, 603, 536, 294, 257, 1623, 11, 309, 311, 588, 4961], "temperature": 0.0, "avg_logprob": -0.12667973187504983, "compression_ratio": 1.5393700787401574, "no_speech_prob": 1.5936404452077113e-05}, {"id": 644, "seek": 327528, "start": 3279.0, "end": 3287.52, "text": " for creating RNNs which can do truly long-term dependencies or context as one of the people", "tokens": [337, 4084, 45702, 45, 82, 597, 393, 360, 4908, 938, 12, 7039, 36606, 420, 4319, 382, 472, 295, 264, 561], "temperature": 0.0, "avg_logprob": -0.12667973187504983, "compression_ratio": 1.5393700787401574, "no_speech_prob": 1.5936404452077113e-05}, {"id": 645, "seek": 327528, "start": 3287.52, "end": 3289.96, "text": " asking a question earlier described it.", "tokens": [3365, 257, 1168, 3071, 7619, 309, 13], "temperature": 0.0, "avg_logprob": -0.12667973187504983, "compression_ratio": 1.5393700787401574, "no_speech_prob": 1.5936404452077113e-05}, {"id": 646, "seek": 327528, "start": 3289.96, "end": 3293.88, "text": " So we're going to start here before we look at how to do context.", "tokens": [407, 321, 434, 516, 281, 722, 510, 949, 321, 574, 412, 577, 281, 360, 4319, 13], "temperature": 0.0, "avg_logprob": -0.12667973187504983, "compression_ratio": 1.5393700787401574, "no_speech_prob": 1.5936404452077113e-05}, {"id": 647, "seek": 327528, "start": 3293.88, "end": 3301.5800000000004, "text": " And so really any time you're doing a sequence-to-sequence exercise, you probably want to construct something", "tokens": [400, 370, 534, 604, 565, 291, 434, 884, 257, 8310, 12, 1353, 12, 11834, 655, 5380, 11, 291, 1391, 528, 281, 7690, 746], "temperature": 0.0, "avg_logprob": -0.12667973187504983, "compression_ratio": 1.5393700787401574, "no_speech_prob": 1.5936404452077113e-05}, {"id": 648, "seek": 330158, "start": 3301.58, "end": 3310.3199999999997, "text": " of this format where your triangle is inside the square rather than outside the square.", "tokens": [295, 341, 7877, 689, 428, 13369, 307, 1854, 264, 3732, 2831, 813, 2380, 264, 3732, 13], "temperature": 0.0, "avg_logprob": -0.2183003586329771, "compression_ratio": 1.6712962962962963, "no_speech_prob": 7.646489393664524e-06}, {"id": 649, "seek": 330158, "start": 3310.3199999999997, "end": 3313.4, "text": " It's going to look very, very similar.", "tokens": [467, 311, 516, 281, 574, 588, 11, 588, 2531, 13], "temperature": 0.0, "avg_logprob": -0.2183003586329771, "compression_ratio": 1.6712962962962963, "no_speech_prob": 7.646489393664524e-06}, {"id": 650, "seek": 330158, "start": 3313.4, "end": 3315.0, "text": " And so I'm calling this returning sequences.", "tokens": [400, 370, 286, 478, 5141, 341, 12678, 22978, 13], "temperature": 0.0, "avg_logprob": -0.2183003586329771, "compression_ratio": 1.6712962962962963, "no_speech_prob": 7.646489393664524e-06}, {"id": 651, "seek": 330158, "start": 3315.0, "end": 3319.2799999999997, "text": " Rather than returning a single character, we're going to return a sequence.", "tokens": [16571, 813, 12678, 257, 2167, 2517, 11, 321, 434, 516, 281, 2736, 257, 8310, 13], "temperature": 0.0, "avg_logprob": -0.2183003586329771, "compression_ratio": 1.6712962962962963, "no_speech_prob": 7.646489393664524e-06}, {"id": 652, "seek": 330158, "start": 3319.2799999999997, "end": 3322.48, "text": " And really most things are the same.", "tokens": [400, 534, 881, 721, 366, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.2183003586329771, "compression_ratio": 1.6712962962962963, "no_speech_prob": 7.646489393664524e-06}, {"id": 653, "seek": 330158, "start": 3322.48, "end": 3328.66, "text": " Our character in data is identical to before, so I've just commented it out.", "tokens": [2621, 2517, 294, 1412, 307, 14800, 281, 949, 11, 370, 286, 600, 445, 26940, 309, 484, 13], "temperature": 0.0, "avg_logprob": -0.2183003586329771, "compression_ratio": 1.6712962962962963, "no_speech_prob": 7.646489393664524e-06}, {"id": 654, "seek": 332866, "start": 3328.66, "end": 3334.0, "text": " And now our character out, our output, isn't just a single character, but it's actually", "tokens": [400, 586, 527, 2517, 484, 11, 527, 5598, 11, 1943, 380, 445, 257, 2167, 2517, 11, 457, 309, 311, 767], "temperature": 0.0, "avg_logprob": -0.1596831934792655, "compression_ratio": 1.8464912280701755, "no_speech_prob": 3.237746568629518e-06}, {"id": 655, "seek": 332866, "start": 3334.0, "end": 3336.72, "text": " a list of 8 sequences again.", "tokens": [257, 1329, 295, 1649, 22978, 797, 13], "temperature": 0.0, "avg_logprob": -0.1596831934792655, "compression_ratio": 1.8464912280701755, "no_speech_prob": 3.237746568629518e-06}, {"id": 656, "seek": 332866, "start": 3336.72, "end": 3343.2799999999997, "text": " In fact, it's exactly the same as the input except that I have removed the minus 1.", "tokens": [682, 1186, 11, 309, 311, 2293, 264, 912, 382, 264, 4846, 3993, 300, 286, 362, 7261, 264, 3175, 502, 13], "temperature": 0.0, "avg_logprob": -0.1596831934792655, "compression_ratio": 1.8464912280701755, "no_speech_prob": 3.237746568629518e-06}, {"id": 657, "seek": 332866, "start": 3343.2799999999997, "end": 3346.16, "text": " So it's just shifted over by 1.", "tokens": [407, 309, 311, 445, 18892, 670, 538, 502, 13], "temperature": 0.0, "avg_logprob": -0.1596831934792655, "compression_ratio": 1.8464912280701755, "no_speech_prob": 3.237746568629518e-06}, {"id": 658, "seek": 332866, "start": 3346.16, "end": 3351.7599999999998, "text": " So in each sequence, the first character will be used to predict the second, the first and", "tokens": [407, 294, 1184, 8310, 11, 264, 700, 2517, 486, 312, 1143, 281, 6069, 264, 1150, 11, 264, 700, 293], "temperature": 0.0, "avg_logprob": -0.1596831934792655, "compression_ratio": 1.8464912280701755, "no_speech_prob": 3.237746568629518e-06}, {"id": 659, "seek": 332866, "start": 3351.7599999999998, "end": 3355.6, "text": " second will predict the third, the first, second and third will predict the fourth,", "tokens": [1150, 486, 6069, 264, 2636, 11, 264, 700, 11, 1150, 293, 2636, 486, 6069, 264, 6409, 11], "temperature": 0.0, "avg_logprob": -0.1596831934792655, "compression_ratio": 1.8464912280701755, "no_speech_prob": 3.237746568629518e-06}, {"id": 660, "seek": 332866, "start": 3355.6, "end": 3358.06, "text": " and so forth.", "tokens": [293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.1596831934792655, "compression_ratio": 1.8464912280701755, "no_speech_prob": 3.237746568629518e-06}, {"id": 661, "seek": 335806, "start": 3358.06, "end": 3361.64, "text": " So we've got a lot more predictions going on and therefore a lot more opportunity for", "tokens": [407, 321, 600, 658, 257, 688, 544, 21264, 516, 322, 293, 4412, 257, 688, 544, 2650, 337], "temperature": 0.0, "avg_logprob": -0.15026677738536487, "compression_ratio": 1.5943396226415094, "no_speech_prob": 1.1478658962005284e-05}, {"id": 662, "seek": 335806, "start": 3361.64, "end": 3364.44, "text": " the model to learn.", "tokens": [264, 2316, 281, 1466, 13], "temperature": 0.0, "avg_logprob": -0.15026677738536487, "compression_ratio": 1.5943396226415094, "no_speech_prob": 1.1478658962005284e-05}, {"id": 663, "seek": 335806, "start": 3364.44, "end": 3372.44, "text": " So then we will create our y's just as before with our x's.", "tokens": [407, 550, 321, 486, 1884, 527, 288, 311, 445, 382, 949, 365, 527, 2031, 311, 13], "temperature": 0.0, "avg_logprob": -0.15026677738536487, "compression_ratio": 1.5943396226415094, "no_speech_prob": 1.1478658962005284e-05}, {"id": 664, "seek": 335806, "start": 3372.44, "end": 3378.22, "text": " And so now our y dataset looks exactly like our x dataset did, but everything is just", "tokens": [400, 370, 586, 527, 288, 28872, 1542, 2293, 411, 527, 2031, 28872, 630, 11, 457, 1203, 307, 445], "temperature": 0.0, "avg_logprob": -0.15026677738536487, "compression_ratio": 1.5943396226415094, "no_speech_prob": 1.1478658962005284e-05}, {"id": 665, "seek": 335806, "start": 3378.22, "end": 3381.4, "text": " shifted across by 1 character.", "tokens": [18892, 2108, 538, 502, 2517, 13], "temperature": 0.0, "avg_logprob": -0.15026677738536487, "compression_ratio": 1.5943396226415094, "no_speech_prob": 1.1478658962005284e-05}, {"id": 666, "seek": 335806, "start": 3381.4, "end": 3384.2, "text": " And the model's going to look almost identical as well.", "tokens": [400, 264, 2316, 311, 516, 281, 574, 1920, 14800, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.15026677738536487, "compression_ratio": 1.5943396226415094, "no_speech_prob": 1.1478658962005284e-05}, {"id": 667, "seek": 338420, "start": 3384.2, "end": 3389.8799999999997, "text": " We've got our 3 dense layers as before.", "tokens": [492, 600, 658, 527, 805, 18011, 7914, 382, 949, 13], "temperature": 0.0, "avg_logprob": -0.13351383972167968, "compression_ratio": 1.879668049792531, "no_speech_prob": 2.0580391719704494e-06}, {"id": 668, "seek": 338420, "start": 3389.8799999999997, "end": 3392.72, "text": " But we're going to do one other thing different to before.", "tokens": [583, 321, 434, 516, 281, 360, 472, 661, 551, 819, 281, 949, 13], "temperature": 0.0, "avg_logprob": -0.13351383972167968, "compression_ratio": 1.879668049792531, "no_speech_prob": 2.0580391719704494e-06}, {"id": 669, "seek": 338420, "start": 3392.72, "end": 3397.72, "text": " Rather than treating the first character as special, I won't treat it as special.", "tokens": [16571, 813, 15083, 264, 700, 2517, 382, 2121, 11, 286, 1582, 380, 2387, 309, 382, 2121, 13], "temperature": 0.0, "avg_logprob": -0.13351383972167968, "compression_ratio": 1.879668049792531, "no_speech_prob": 2.0580391719704494e-06}, {"id": 670, "seek": 338420, "start": 3397.72, "end": 3400.08, "text": " I'm going to move the character into here.", "tokens": [286, 478, 516, 281, 1286, 264, 2517, 666, 510, 13], "temperature": 0.0, "avg_logprob": -0.13351383972167968, "compression_ratio": 1.879668049792531, "no_speech_prob": 2.0580391719704494e-06}, {"id": 671, "seek": 338420, "start": 3400.08, "end": 3405.08, "text": " So rather than repeating from 2 to n-1, I'm going to repeat from 1 to n-1.", "tokens": [407, 2831, 813, 18617, 490, 568, 281, 297, 12, 16, 11, 286, 478, 516, 281, 7149, 490, 502, 281, 297, 12, 16, 13], "temperature": 0.0, "avg_logprob": -0.13351383972167968, "compression_ratio": 1.879668049792531, "no_speech_prob": 2.0580391719704494e-06}, {"id": 672, "seek": 338420, "start": 3405.08, "end": 3407.56, "text": " So I've moved my first character into here.", "tokens": [407, 286, 600, 4259, 452, 700, 2517, 666, 510, 13], "temperature": 0.0, "avg_logprob": -0.13351383972167968, "compression_ratio": 1.879668049792531, "no_speech_prob": 2.0580391719704494e-06}, {"id": 673, "seek": 338420, "start": 3407.56, "end": 3411.64, "text": " So the only thing I have to be careful of is that we have to somehow initialize our", "tokens": [407, 264, 787, 551, 286, 362, 281, 312, 5026, 295, 307, 300, 321, 362, 281, 6063, 5883, 1125, 527], "temperature": 0.0, "avg_logprob": -0.13351383972167968, "compression_ratio": 1.879668049792531, "no_speech_prob": 2.0580391719704494e-06}, {"id": 674, "seek": 338420, "start": 3411.64, "end": 3413.48, "text": " hidden state to something.", "tokens": [7633, 1785, 281, 746, 13], "temperature": 0.0, "avg_logprob": -0.13351383972167968, "compression_ratio": 1.879668049792531, "no_speech_prob": 2.0580391719704494e-06}, {"id": 675, "seek": 341348, "start": 3413.48, "end": 3419.2400000000002, "text": " So we're going to initialize our hidden state to a vector of 0's.", "tokens": [407, 321, 434, 516, 281, 5883, 1125, 527, 7633, 1785, 281, 257, 8062, 295, 1958, 311, 13], "temperature": 0.0, "avg_logprob": -0.11030443724211272, "compression_ratio": 1.98, "no_speech_prob": 9.223413144354708e-06}, {"id": 676, "seek": 341348, "start": 3419.2400000000002, "end": 3420.56, "text": " So here we do that.", "tokens": [407, 510, 321, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.11030443724211272, "compression_ratio": 1.98, "no_speech_prob": 9.223413144354708e-06}, {"id": 677, "seek": 341348, "start": 3420.56, "end": 3424.32, "text": " We say we're going to have to have something to initialize our hidden state, which we're", "tokens": [492, 584, 321, 434, 516, 281, 362, 281, 362, 746, 281, 5883, 1125, 527, 7633, 1785, 11, 597, 321, 434], "temperature": 0.0, "avg_logprob": -0.11030443724211272, "compression_ratio": 1.98, "no_speech_prob": 9.223413144354708e-06}, {"id": 678, "seek": 341348, "start": 3424.32, "end": 3426.72, "text": " going to feed it with a vector of 0's shortly.", "tokens": [516, 281, 3154, 309, 365, 257, 8062, 295, 1958, 311, 13392, 13], "temperature": 0.0, "avg_logprob": -0.11030443724211272, "compression_ratio": 1.98, "no_speech_prob": 9.223413144354708e-06}, {"id": 679, "seek": 341348, "start": 3426.72, "end": 3431.2400000000002, "text": " So our initial hidden state is just going to be the result of that.", "tokens": [407, 527, 5883, 7633, 1785, 307, 445, 516, 281, 312, 264, 1874, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.11030443724211272, "compression_ratio": 1.98, "no_speech_prob": 9.223413144354708e-06}, {"id": 680, "seek": 341348, "start": 3431.2400000000002, "end": 3436.72, "text": " And then our loop is identical to before, but at the end of every loop, we're going", "tokens": [400, 550, 527, 6367, 307, 14800, 281, 949, 11, 457, 412, 264, 917, 295, 633, 6367, 11, 321, 434, 516], "temperature": 0.0, "avg_logprob": -0.11030443724211272, "compression_ratio": 1.98, "no_speech_prob": 9.223413144354708e-06}, {"id": 681, "seek": 341348, "start": 3436.72, "end": 3440.12, "text": " to append this output.", "tokens": [281, 34116, 341, 5598, 13], "temperature": 0.0, "avg_logprob": -0.11030443724211272, "compression_ratio": 1.98, "no_speech_prob": 9.223413144354708e-06}, {"id": 682, "seek": 344012, "start": 3440.12, "end": 3445.0, "text": " So we're now going to have 8 outputs for every sequence rather than 1.", "tokens": [407, 321, 434, 586, 516, 281, 362, 1649, 23930, 337, 633, 8310, 2831, 813, 502, 13], "temperature": 0.0, "avg_logprob": -0.22217817509427984, "compression_ratio": 1.5943396226415094, "no_speech_prob": 3.393128054085537e-06}, {"id": 683, "seek": 344012, "start": 3445.0, "end": 3447.44, "text": " And so now our model has 2 changes.", "tokens": [400, 370, 586, 527, 2316, 575, 568, 2962, 13], "temperature": 0.0, "avg_logprob": -0.22217817509427984, "compression_ratio": 1.5943396226415094, "no_speech_prob": 3.393128054085537e-06}, {"id": 684, "seek": 344012, "start": 3447.44, "end": 3453.52, "text": " The first is it's got an array of outputs, and the second is that we have to add the", "tokens": [440, 700, 307, 309, 311, 658, 364, 10225, 295, 23930, 11, 293, 264, 1150, 307, 300, 321, 362, 281, 909, 264], "temperature": 0.0, "avg_logprob": -0.22217817509427984, "compression_ratio": 1.5943396226415094, "no_speech_prob": 3.393128054085537e-06}, {"id": 685, "seek": 344012, "start": 3453.52, "end": 3458.3599999999997, "text": " thing that we're going to use to store our vector of 0's somewhere.", "tokens": [551, 300, 321, 434, 516, 281, 764, 281, 3531, 527, 8062, 295, 1958, 311, 4079, 13], "temperature": 0.0, "avg_logprob": -0.22217817509427984, "compression_ratio": 1.5943396226415094, "no_speech_prob": 3.393128054085537e-06}, {"id": 686, "seek": 344012, "start": 3458.3599999999997, "end": 3461.16, "text": " So we're going to put this into our input as well.", "tokens": [407, 321, 434, 516, 281, 829, 341, 666, 527, 4846, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.22217817509427984, "compression_ratio": 1.5943396226415094, "no_speech_prob": 3.393128054085537e-06}, {"id": 687, "seek": 344012, "start": 3461.16, "end": 3466.64, "text": " Question from the audience.", "tokens": [14464, 490, 264, 4034, 13], "temperature": 0.0, "avg_logprob": -0.22217817509427984, "compression_ratio": 1.5943396226415094, "no_speech_prob": 3.393128054085537e-06}, {"id": 688, "seek": 346664, "start": 3466.64, "end": 3471.92, "text": " The box refers to the area that we're looping.", "tokens": [440, 2424, 14942, 281, 264, 1859, 300, 321, 434, 6367, 278, 13], "temperature": 0.0, "avg_logprob": -0.15338365810433613, "compression_ratio": 1.7924528301886793, "no_speech_prob": 4.9369696171197575e-06}, {"id": 689, "seek": 346664, "start": 3471.92, "end": 3479.72, "text": " So initially we repeated the character n input coming into here and then the hidden state", "tokens": [407, 9105, 321, 10477, 264, 2517, 297, 4846, 1348, 666, 510, 293, 550, 264, 7633, 1785], "temperature": 0.0, "avg_logprob": -0.15338365810433613, "compression_ratio": 1.7924528301886793, "no_speech_prob": 4.9369696171197575e-06}, {"id": 690, "seek": 346664, "start": 3479.72, "end": 3482.08, "text": " going back to itself from 2 to n-1.", "tokens": [516, 646, 281, 2564, 490, 568, 281, 297, 12, 16, 13], "temperature": 0.0, "avg_logprob": -0.15338365810433613, "compression_ratio": 1.7924528301886793, "no_speech_prob": 4.9369696171197575e-06}, {"id": 691, "seek": 346664, "start": 3482.08, "end": 3486.96, "text": " So the box is the thing which I'm looping through all those times.", "tokens": [407, 264, 2424, 307, 264, 551, 597, 286, 478, 6367, 278, 807, 439, 729, 1413, 13], "temperature": 0.0, "avg_logprob": -0.15338365810433613, "compression_ratio": 1.7924528301886793, "no_speech_prob": 4.9369696171197575e-06}, {"id": 692, "seek": 346664, "start": 3486.96, "end": 3489.4, "text": " This time I'm looping through this whole thing.", "tokens": [639, 565, 286, 478, 6367, 278, 807, 341, 1379, 551, 13], "temperature": 0.0, "avg_logprob": -0.15338365810433613, "compression_ratio": 1.7924528301886793, "no_speech_prob": 4.9369696171197575e-06}, {"id": 693, "seek": 346664, "start": 3489.4, "end": 3494.7999999999997, "text": " So character input coming in, generating the hidden state, and creating an output, repeating", "tokens": [407, 2517, 4846, 1348, 294, 11, 17746, 264, 7633, 1785, 11, 293, 4084, 364, 5598, 11, 18617], "temperature": 0.0, "avg_logprob": -0.15338365810433613, "compression_ratio": 1.7924528301886793, "no_speech_prob": 4.9369696171197575e-06}, {"id": 694, "seek": 349480, "start": 3494.8, "end": 3498.52, "text": " that whole thing every time.", "tokens": [300, 1379, 551, 633, 565, 13], "temperature": 0.0, "avg_logprob": -0.17524482264663233, "compression_ratio": 1.467455621301775, "no_speech_prob": 4.029443971376168e-06}, {"id": 695, "seek": 349480, "start": 3498.52, "end": 3505.0, "text": " And so now you can see creating the output is inside the loop rather than outside the", "tokens": [400, 370, 586, 291, 393, 536, 4084, 264, 5598, 307, 1854, 264, 6367, 2831, 813, 2380, 264], "temperature": 0.0, "avg_logprob": -0.17524482264663233, "compression_ratio": 1.467455621301775, "no_speech_prob": 4.029443971376168e-06}, {"id": 696, "seek": 349480, "start": 3505.0, "end": 3506.0, "text": " loop.", "tokens": [6367, 13], "temperature": 0.0, "avg_logprob": -0.17524482264663233, "compression_ratio": 1.467455621301775, "no_speech_prob": 4.029443971376168e-06}, {"id": 697, "seek": 349480, "start": 3506.0, "end": 3511.04, "text": " So therefore we end up with an array of outputs.", "tokens": [407, 4412, 321, 917, 493, 365, 364, 10225, 295, 23930, 13], "temperature": 0.0, "avg_logprob": -0.17524482264663233, "compression_ratio": 1.467455621301775, "no_speech_prob": 4.029443971376168e-06}, {"id": 698, "seek": 349480, "start": 3511.04, "end": 3514.7200000000003, "text": " Our model is nearly exactly the same as before, it's just got these 2 changes.", "tokens": [2621, 2316, 307, 6217, 2293, 264, 912, 382, 949, 11, 309, 311, 445, 658, 613, 568, 2962, 13], "temperature": 0.0, "avg_logprob": -0.17524482264663233, "compression_ratio": 1.467455621301775, "no_speech_prob": 4.029443971376168e-06}, {"id": 699, "seek": 351472, "start": 3514.72, "end": 3525.64, "text": " So now when we fit our model, we're going to add an array of 0's to the start of our", "tokens": [407, 586, 562, 321, 3318, 527, 2316, 11, 321, 434, 516, 281, 909, 364, 10225, 295, 1958, 311, 281, 264, 722, 295, 527], "temperature": 0.0, "avg_logprob": -0.14024716754292332, "compression_ratio": 1.554945054945055, "no_speech_prob": 2.295916146977106e-06}, {"id": 700, "seek": 351472, "start": 3525.64, "end": 3526.64, "text": " inputs.", "tokens": [15743, 13], "temperature": 0.0, "avg_logprob": -0.14024716754292332, "compression_ratio": 1.554945054945055, "no_speech_prob": 2.295916146977106e-06}, {"id": 701, "seek": 351472, "start": 3526.64, "end": 3532.52, "text": " Our outputs are going to be those lists of 8 that have been offset by 1.", "tokens": [2621, 23930, 366, 516, 281, 312, 729, 14511, 295, 1649, 300, 362, 668, 18687, 538, 502, 13], "temperature": 0.0, "avg_logprob": -0.14024716754292332, "compression_ratio": 1.554945054945055, "no_speech_prob": 2.295916146977106e-06}, {"id": 702, "seek": 351472, "start": 3532.52, "end": 3535.2, "text": " And we can go ahead and train this.", "tokens": [400, 321, 393, 352, 2286, 293, 3847, 341, 13], "temperature": 0.0, "avg_logprob": -0.14024716754292332, "compression_ratio": 1.554945054945055, "no_speech_prob": 2.295916146977106e-06}, {"id": 703, "seek": 351472, "start": 3535.2, "end": 3544.4399999999996, "text": " And you can see that as we train it, we don't just have 1 loss, we have 8 losses.", "tokens": [400, 291, 393, 536, 300, 382, 321, 3847, 309, 11, 321, 500, 380, 445, 362, 502, 4470, 11, 321, 362, 1649, 15352, 13], "temperature": 0.0, "avg_logprob": -0.14024716754292332, "compression_ratio": 1.554945054945055, "no_speech_prob": 2.295916146977106e-06}, {"id": 704, "seek": 354444, "start": 3544.44, "end": 3548.08, "text": " And that's because every one of those 8 outputs has its own loss.", "tokens": [400, 300, 311, 570, 633, 472, 295, 729, 1649, 23930, 575, 1080, 1065, 4470, 13], "temperature": 0.0, "avg_logprob": -0.196044738476093, "compression_ratio": 1.6652542372881356, "no_speech_prob": 1.1843009815493133e-05}, {"id": 705, "seek": 354444, "start": 3548.08, "end": 3552.04, "text": " How are we going at predicting character 1 in each sequence?", "tokens": [1012, 366, 321, 516, 412, 32884, 2517, 502, 294, 1184, 8310, 30], "temperature": 0.0, "avg_logprob": -0.196044738476093, "compression_ratio": 1.6652542372881356, "no_speech_prob": 1.1843009815493133e-05}, {"id": 706, "seek": 354444, "start": 3552.04, "end": 3558.56, "text": " As you would expect, our ability to predict the first character using nothing but a vector", "tokens": [1018, 291, 576, 2066, 11, 527, 3485, 281, 6069, 264, 700, 2517, 1228, 1825, 457, 257, 8062], "temperature": 0.0, "avg_logprob": -0.196044738476093, "compression_ratio": 1.6652542372881356, "no_speech_prob": 1.1843009815493133e-05}, {"id": 707, "seek": 354444, "start": 3558.56, "end": 3560.7000000000003, "text": " of 0's is pretty limited.", "tokens": [295, 1958, 311, 307, 1238, 5567, 13], "temperature": 0.0, "avg_logprob": -0.196044738476093, "compression_ratio": 1.6652542372881356, "no_speech_prob": 1.1843009815493133e-05}, {"id": 708, "seek": 354444, "start": 3560.7000000000003, "end": 3563.92, "text": " So that very quickly flattens out.", "tokens": [407, 300, 588, 2661, 932, 1591, 694, 484, 13], "temperature": 0.0, "avg_logprob": -0.196044738476093, "compression_ratio": 1.6652542372881356, "no_speech_prob": 1.1843009815493133e-05}, {"id": 709, "seek": 354444, "start": 3563.92, "end": 3566.6, "text": " Where else is our ability to predict the 8th character?", "tokens": [2305, 1646, 307, 527, 3485, 281, 6069, 264, 1649, 392, 2517, 30], "temperature": 0.0, "avg_logprob": -0.196044738476093, "compression_ratio": 1.6652542372881356, "no_speech_prob": 1.1843009815493133e-05}, {"id": 710, "seek": 354444, "start": 3566.6, "end": 3568.84, "text": " It has a lot more context.", "tokens": [467, 575, 257, 688, 544, 4319, 13], "temperature": 0.0, "avg_logprob": -0.196044738476093, "compression_ratio": 1.6652542372881356, "no_speech_prob": 1.1843009815493133e-05}, {"id": 711, "seek": 354444, "start": 3568.84, "end": 3571.88, "text": " It has 7 characters of context.", "tokens": [467, 575, 1614, 4342, 295, 4319, 13], "temperature": 0.0, "avg_logprob": -0.196044738476093, "compression_ratio": 1.6652542372881356, "no_speech_prob": 1.1843009815493133e-05}, {"id": 712, "seek": 357188, "start": 3571.88, "end": 3577.2400000000002, "text": " And so you can see that the 8th character's loss keeps on improving.", "tokens": [400, 370, 291, 393, 536, 300, 264, 1649, 392, 2517, 311, 4470, 5965, 322, 11470, 13], "temperature": 0.0, "avg_logprob": -0.17524780765656503, "compression_ratio": 1.6650717703349283, "no_speech_prob": 3.3405212889192626e-06}, {"id": 713, "seek": 357188, "start": 3577.2400000000002, "end": 3584.2000000000003, "text": " And indeed by a few epochs, we have a significantly better loss than we did before.", "tokens": [400, 6451, 538, 257, 1326, 30992, 28346, 11, 321, 362, 257, 10591, 1101, 4470, 813, 321, 630, 949, 13], "temperature": 0.0, "avg_logprob": -0.17524780765656503, "compression_ratio": 1.6650717703349283, "no_speech_prob": 3.3405212889192626e-06}, {"id": 714, "seek": 357188, "start": 3584.2000000000003, "end": 3586.84, "text": " So this is what a sequence model looks like.", "tokens": [407, 341, 307, 437, 257, 8310, 2316, 1542, 411, 13], "temperature": 0.0, "avg_logprob": -0.17524780765656503, "compression_ratio": 1.6650717703349283, "no_speech_prob": 3.3405212889192626e-06}, {"id": 715, "seek": 357188, "start": 3586.84, "end": 3593.88, "text": " And so you can see a sequence model, when we test it, we pass in a sequence like this,", "tokens": [400, 370, 291, 393, 536, 257, 8310, 2316, 11, 562, 321, 1500, 309, 11, 321, 1320, 294, 257, 8310, 411, 341, 11], "temperature": 0.0, "avg_logprob": -0.17524780765656503, "compression_ratio": 1.6650717703349283, "no_speech_prob": 3.3405212889192626e-06}, {"id": 716, "seek": 357188, "start": 3593.88, "end": 3599.12, "text": " space this is, and after every character, it returns its guess.", "tokens": [1901, 341, 307, 11, 293, 934, 633, 2517, 11, 309, 11247, 1080, 2041, 13], "temperature": 0.0, "avg_logprob": -0.17524780765656503, "compression_ratio": 1.6650717703349283, "no_speech_prob": 3.3405212889192626e-06}, {"id": 717, "seek": 359912, "start": 3599.12, "end": 3604.4, "text": " So after seeing a space, it guesses the next will be a T. After seeing a space T, it guesses", "tokens": [407, 934, 2577, 257, 1901, 11, 309, 42703, 264, 958, 486, 312, 257, 314, 13, 2381, 2577, 257, 1901, 314, 11, 309, 42703], "temperature": 0.0, "avg_logprob": -0.2653949971784625, "compression_ratio": 1.980952380952381, "no_speech_prob": 3.2698419090593234e-05}, {"id": 718, "seek": 359912, "start": 3604.4, "end": 3609.52, "text": " the next will be an H. After seeing space TH, it guesses the next will be an E. And", "tokens": [264, 958, 486, 312, 364, 389, 13, 2381, 2577, 1901, 3578, 11, 309, 42703, 264, 958, 486, 312, 364, 462, 13, 400], "temperature": 0.0, "avg_logprob": -0.2653949971784625, "compression_ratio": 1.980952380952381, "no_speech_prob": 3.2698419090593234e-05}, {"id": 719, "seek": 359912, "start": 3609.52, "end": 3612.12, "text": " so forth.", "tokens": [370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.2653949971784625, "compression_ratio": 1.980952380952381, "no_speech_prob": 3.2698419090593234e-05}, {"id": 720, "seek": 359912, "start": 3612.12, "end": 3617.88, "text": " And so you can see that it's predicting some pretty reasonable things here.", "tokens": [400, 370, 291, 393, 536, 300, 309, 311, 32884, 512, 1238, 10585, 721, 510, 13], "temperature": 0.0, "avg_logprob": -0.2653949971784625, "compression_ratio": 1.980952380952381, "no_speech_prob": 3.2698419090593234e-05}, {"id": 721, "seek": 359912, "start": 3617.88, "end": 3621.12, "text": " And indeed quite often there, what actually happened.", "tokens": [400, 6451, 1596, 2049, 456, 11, 437, 767, 2011, 13], "temperature": 0.0, "avg_logprob": -0.2653949971784625, "compression_ratio": 1.980952380952381, "no_speech_prob": 3.2698419090593234e-05}, {"id": 722, "seek": 359912, "start": 3621.12, "end": 3625.56, "text": " So it sees after seeing space PART, it expects that will be the end of the word.", "tokens": [407, 309, 8194, 934, 2577, 1901, 21720, 51, 11, 309, 33280, 300, 486, 312, 264, 917, 295, 264, 1349, 13], "temperature": 0.0, "avg_logprob": -0.2653949971784625, "compression_ratio": 1.980952380952381, "no_speech_prob": 3.2698419090593234e-05}, {"id": 723, "seek": 359912, "start": 3625.56, "end": 3626.9, "text": " And indeed it was.", "tokens": [400, 6451, 309, 390, 13], "temperature": 0.0, "avg_logprob": -0.2653949971784625, "compression_ratio": 1.980952380952381, "no_speech_prob": 3.2698419090593234e-05}, {"id": 724, "seek": 362690, "start": 3626.9, "end": 3632.6, "text": " And after seeing part, it's guessing that the next word is going to be of.", "tokens": [400, 934, 2577, 644, 11, 309, 311, 17939, 300, 264, 958, 1349, 307, 516, 281, 312, 295, 13], "temperature": 0.0, "avg_logprob": -0.15351472898971202, "compression_ratio": 1.5396039603960396, "no_speech_prob": 6.7479904828360304e-06}, {"id": 725, "seek": 362690, "start": 3632.6, "end": 3642.6, "text": " So it's able to use sequences of 8 to create context, which isn't brilliant, but it's an", "tokens": [407, 309, 311, 1075, 281, 764, 22978, 295, 1649, 281, 1884, 4319, 11, 597, 1943, 380, 10248, 11, 457, 309, 311, 364], "temperature": 0.0, "avg_logprob": -0.15351472898971202, "compression_ratio": 1.5396039603960396, "no_speech_prob": 6.7479904828360304e-06}, {"id": 726, "seek": 362690, "start": 3642.6, "end": 3644.4, "text": " improvement.", "tokens": [10444, 13], "temperature": 0.0, "avg_logprob": -0.15351472898971202, "compression_ratio": 1.5396039603960396, "no_speech_prob": 6.7479904828360304e-06}, {"id": 727, "seek": 362690, "start": 3644.4, "end": 3647.64, "text": " So how do we do that same thing with Keras?", "tokens": [407, 577, 360, 321, 360, 300, 912, 551, 365, 591, 6985, 30], "temperature": 0.0, "avg_logprob": -0.15351472898971202, "compression_ratio": 1.5396039603960396, "no_speech_prob": 6.7479904828360304e-06}, {"id": 728, "seek": 362690, "start": 3647.64, "end": 3655.6, "text": " With Keras, it's identical to our previous model, except that we have to use the different", "tokens": [2022, 591, 6985, 11, 309, 311, 14800, 281, 527, 3894, 2316, 11, 3993, 300, 321, 362, 281, 764, 264, 819], "temperature": 0.0, "avg_logprob": -0.15351472898971202, "compression_ratio": 1.5396039603960396, "no_speech_prob": 6.7479904828360304e-06}, {"id": 729, "seek": 365560, "start": 3655.6, "end": 3661.4, "text": " input and output arrays, just like I just showed you, so the whole sequence of labels", "tokens": [4846, 293, 5598, 41011, 11, 445, 411, 286, 445, 4712, 291, 11, 370, 264, 1379, 8310, 295, 16949], "temperature": 0.0, "avg_logprob": -0.13962261422166547, "compression_ratio": 1.915929203539823, "no_speech_prob": 1.0129879228770733e-05}, {"id": 730, "seek": 365560, "start": 3661.4, "end": 3665.7599999999998, "text": " and the whole sequence of inputs.", "tokens": [293, 264, 1379, 8310, 295, 15743, 13], "temperature": 0.0, "avg_logprob": -0.13962261422166547, "compression_ratio": 1.915929203539823, "no_speech_prob": 1.0129879228770733e-05}, {"id": 731, "seek": 365560, "start": 3665.7599999999998, "end": 3670.12, "text": " And then the second thing we have to do is add one parameter, which is return sequences", "tokens": [400, 550, 264, 1150, 551, 321, 362, 281, 360, 307, 909, 472, 13075, 11, 597, 307, 2736, 22978], "temperature": 0.0, "avg_logprob": -0.13962261422166547, "compression_ratio": 1.915929203539823, "no_speech_prob": 1.0129879228770733e-05}, {"id": 732, "seek": 365560, "start": 3670.12, "end": 3671.96, "text": " equals true.", "tokens": [6915, 2074, 13], "temperature": 0.0, "avg_logprob": -0.13962261422166547, "compression_ratio": 1.915929203539823, "no_speech_prob": 1.0129879228770733e-05}, {"id": 733, "seek": 365560, "start": 3671.96, "end": 3676.92, "text": " Return sequences equals true simply says, rather than putting the triangle outside the", "tokens": [24350, 22978, 6915, 2074, 2935, 1619, 11, 2831, 813, 3372, 264, 13369, 2380, 264], "temperature": 0.0, "avg_logprob": -0.13962261422166547, "compression_ratio": 1.915929203539823, "no_speech_prob": 1.0129879228770733e-05}, {"id": 734, "seek": 365560, "start": 3676.92, "end": 3679.98, "text": " loop, put the triangle inside the loop.", "tokens": [6367, 11, 829, 264, 13369, 1854, 264, 6367, 13], "temperature": 0.0, "avg_logprob": -0.13962261422166547, "compression_ratio": 1.915929203539823, "no_speech_prob": 1.0129879228770733e-05}, {"id": 735, "seek": 365560, "start": 3679.98, "end": 3685.56, "text": " And so return an output from every time you go to another time step, rather than just", "tokens": [400, 370, 2736, 364, 5598, 490, 633, 565, 291, 352, 281, 1071, 565, 1823, 11, 2831, 813, 445], "temperature": 0.0, "avg_logprob": -0.13962261422166547, "compression_ratio": 1.915929203539823, "no_speech_prob": 1.0129879228770733e-05}, {"id": 736, "seek": 368556, "start": 3685.56, "end": 3688.7999999999997, "text": " returning a single output at the end.", "tokens": [12678, 257, 2167, 5598, 412, 264, 917, 13], "temperature": 0.0, "avg_logprob": -0.17472548817479333, "compression_ratio": 1.4902912621359223, "no_speech_prob": 9.516178579360712e-06}, {"id": 737, "seek": 368556, "start": 3688.7999999999997, "end": 3690.84, "text": " So it's that easy in Keras.", "tokens": [407, 309, 311, 300, 1858, 294, 591, 6985, 13], "temperature": 0.0, "avg_logprob": -0.17472548817479333, "compression_ratio": 1.4902912621359223, "no_speech_prob": 9.516178579360712e-06}, {"id": 738, "seek": 368556, "start": 3690.84, "end": 3694.16, "text": " I add this return sequences equals true.", "tokens": [286, 909, 341, 2736, 22978, 6915, 2074, 13], "temperature": 0.0, "avg_logprob": -0.17472548817479333, "compression_ratio": 1.4902912621359223, "no_speech_prob": 9.516178579360712e-06}, {"id": 739, "seek": 368556, "start": 3694.16, "end": 3702.7999999999997, "text": " I don't have to change my data at all other than some very minor dimensionality changes.", "tokens": [286, 500, 380, 362, 281, 1319, 452, 1412, 412, 439, 661, 813, 512, 588, 6696, 10139, 1860, 2962, 13], "temperature": 0.0, "avg_logprob": -0.17472548817479333, "compression_ratio": 1.4902912621359223, "no_speech_prob": 9.516178579360712e-06}, {"id": 740, "seek": 368556, "start": 3702.7999999999997, "end": 3705.7999999999997, "text": " Then I can just go ahead and fit it.", "tokens": [1396, 286, 393, 445, 352, 2286, 293, 3318, 309, 13], "temperature": 0.0, "avg_logprob": -0.17472548817479333, "compression_ratio": 1.4902912621359223, "no_speech_prob": 9.516178579360712e-06}, {"id": 741, "seek": 368556, "start": 3705.7999999999997, "end": 3711.52, "text": " As you can see, I get a pretty similar loss function to what I did before.", "tokens": [1018, 291, 393, 536, 11, 286, 483, 257, 1238, 2531, 4470, 2445, 281, 437, 286, 630, 949, 13], "temperature": 0.0, "avg_logprob": -0.17472548817479333, "compression_ratio": 1.4902912621359223, "no_speech_prob": 9.516178579360712e-06}, {"id": 742, "seek": 371152, "start": 3711.52, "end": 3716.92, "text": " And I can build something that looks very much like we had before and generate some", "tokens": [400, 286, 393, 1322, 746, 300, 1542, 588, 709, 411, 321, 632, 949, 293, 8460, 512], "temperature": 0.0, "avg_logprob": -0.11952989395350626, "compression_ratio": 1.5343915343915344, "no_speech_prob": 5.5075870477594435e-06}, {"id": 743, "seek": 371152, "start": 3716.92, "end": 3718.96, "text": " pretty similar results.", "tokens": [1238, 2531, 3542, 13], "temperature": 0.0, "avg_logprob": -0.11952989395350626, "compression_ratio": 1.5343915343915344, "no_speech_prob": 5.5075870477594435e-06}, {"id": 744, "seek": 371152, "start": 3718.96, "end": 3727.68, "text": " So that's how we create a sequence model with Keras.", "tokens": [407, 300, 311, 577, 321, 1884, 257, 8310, 2316, 365, 591, 6985, 13], "temperature": 0.0, "avg_logprob": -0.11952989395350626, "compression_ratio": 1.5343915343915344, "no_speech_prob": 5.5075870477594435e-06}, {"id": 745, "seek": 371152, "start": 3727.68, "end": 3732.72, "text": " So then the question of how do you create more state?", "tokens": [407, 550, 264, 1168, 295, 577, 360, 291, 1884, 544, 1785, 30], "temperature": 0.0, "avg_logprob": -0.11952989395350626, "compression_ratio": 1.5343915343915344, "no_speech_prob": 5.5075870477594435e-06}, {"id": 746, "seek": 371152, "start": 3732.72, "end": 3739.68, "text": " How do you generate a model which is able to handle long-term dependencies?", "tokens": [1012, 360, 291, 8460, 257, 2316, 597, 307, 1075, 281, 4813, 938, 12, 7039, 36606, 30], "temperature": 0.0, "avg_logprob": -0.11952989395350626, "compression_ratio": 1.5343915343915344, "no_speech_prob": 5.5075870477594435e-06}, {"id": 747, "seek": 373968, "start": 3739.68, "end": 3746.96, "text": " To generate a model that understands long-term dependencies, we can't anymore present our", "tokens": [1407, 8460, 257, 2316, 300, 15146, 938, 12, 7039, 36606, 11, 321, 393, 380, 3602, 1974, 527], "temperature": 0.0, "avg_logprob": -0.1557814915974935, "compression_ratio": 1.6898148148148149, "no_speech_prob": 3.966968506574631e-06}, {"id": 748, "seek": 373968, "start": 3746.96, "end": 3748.8399999999997, "text": " pieces of data at random.", "tokens": [3755, 295, 1412, 412, 4974, 13], "temperature": 0.0, "avg_logprob": -0.1557814915974935, "compression_ratio": 1.6898148148148149, "no_speech_prob": 3.966968506574631e-06}, {"id": 749, "seek": 373968, "start": 3748.8399999999997, "end": 3755.96, "text": " So far we've always been using the default when we do fit model, which is shuffle equals", "tokens": [407, 1400, 321, 600, 1009, 668, 1228, 264, 7576, 562, 321, 360, 3318, 2316, 11, 597, 307, 39426, 6915], "temperature": 0.0, "avg_logprob": -0.1557814915974935, "compression_ratio": 1.6898148148148149, "no_speech_prob": 3.966968506574631e-06}, {"id": 750, "seek": 373968, "start": 3755.96, "end": 3757.02, "text": " true.", "tokens": [2074, 13], "temperature": 0.0, "avg_logprob": -0.1557814915974935, "compression_ratio": 1.6898148148148149, "no_speech_prob": 3.966968506574631e-06}, {"id": 751, "seek": 373968, "start": 3757.02, "end": 3762.8799999999997, "text": " So it's passing across these sequences of 8 in a random order.", "tokens": [407, 309, 311, 8437, 2108, 613, 22978, 295, 1649, 294, 257, 4974, 1668, 13], "temperature": 0.0, "avg_logprob": -0.1557814915974935, "compression_ratio": 1.6898148148148149, "no_speech_prob": 3.966968506574631e-06}, {"id": 752, "seek": 373968, "start": 3762.8799999999997, "end": 3766.24, "text": " If we're going to do something which understands long-term dependencies, the first thing we", "tokens": [759, 321, 434, 516, 281, 360, 746, 597, 15146, 938, 12, 7039, 36606, 11, 264, 700, 551, 321], "temperature": 0.0, "avg_logprob": -0.1557814915974935, "compression_ratio": 1.6898148148148149, "no_speech_prob": 3.966968506574631e-06}, {"id": 753, "seek": 376624, "start": 3766.24, "end": 3770.9599999999996, "text": " have to do is we're going to have to use shuffle equals false.", "tokens": [362, 281, 360, 307, 321, 434, 516, 281, 362, 281, 764, 39426, 6915, 7908, 13], "temperature": 0.0, "avg_logprob": -0.11346758328951322, "compression_ratio": 1.81875, "no_speech_prob": 2.8573003874043934e-06}, {"id": 754, "seek": 376624, "start": 3770.9599999999996, "end": 3776.04, "text": " The second thing we're going to have to do is we're going to have to stop passing in", "tokens": [440, 1150, 551, 321, 434, 516, 281, 362, 281, 360, 307, 321, 434, 516, 281, 362, 281, 1590, 8437, 294], "temperature": 0.0, "avg_logprob": -0.11346758328951322, "compression_ratio": 1.81875, "no_speech_prob": 2.8573003874043934e-06}, {"id": 755, "seek": 376624, "start": 3776.04, "end": 3781.8799999999997, "text": " an array of zeros as my starting point every time around.", "tokens": [364, 10225, 295, 35193, 382, 452, 2891, 935, 633, 565, 926, 13], "temperature": 0.0, "avg_logprob": -0.11346758328951322, "compression_ratio": 1.81875, "no_speech_prob": 2.8573003874043934e-06}, {"id": 756, "seek": 376624, "start": 3781.8799999999997, "end": 3792.04, "text": " So effectively, what I want to do is I want to pass in my array of zeros right at the", "tokens": [407, 8659, 11, 437, 286, 528, 281, 360, 307, 286, 528, 281, 1320, 294, 452, 10225, 295, 35193, 558, 412, 264], "temperature": 0.0, "avg_logprob": -0.11346758328951322, "compression_ratio": 1.81875, "no_speech_prob": 2.8573003874043934e-06}, {"id": 757, "seek": 379204, "start": 3792.04, "end": 3798.08, "text": " very start when I first start training, but then at the end of my sequence of 8, rather", "tokens": [588, 722, 562, 286, 700, 722, 3097, 11, 457, 550, 412, 264, 917, 295, 452, 8310, 295, 1649, 11, 2831], "temperature": 0.0, "avg_logprob": -0.1224478426433745, "compression_ratio": 1.6226415094339623, "no_speech_prob": 2.7264582058705855e-06}, {"id": 758, "seek": 379204, "start": 3798.08, "end": 3804.72, "text": " than going back to initialize to zeros, I actually want to keep this hidden state.", "tokens": [813, 516, 646, 281, 5883, 1125, 281, 35193, 11, 286, 767, 528, 281, 1066, 341, 7633, 1785, 13], "temperature": 0.0, "avg_logprob": -0.1224478426433745, "compression_ratio": 1.6226415094339623, "no_speech_prob": 2.7264582058705855e-06}, {"id": 759, "seek": 379204, "start": 3804.72, "end": 3810.0, "text": " So then I'd start my next sequence of 8 with this hidden state exactly where it was before.", "tokens": [407, 550, 286, 1116, 722, 452, 958, 8310, 295, 1649, 365, 341, 7633, 1785, 2293, 689, 309, 390, 949, 13], "temperature": 0.0, "avg_logprob": -0.1224478426433745, "compression_ratio": 1.6226415094339623, "no_speech_prob": 2.7264582058705855e-06}, {"id": 760, "seek": 379204, "start": 3810.0, "end": 3817.4, "text": " And that's going to allow it to basically build up arbitrarily long dependencies.", "tokens": [400, 300, 311, 516, 281, 2089, 309, 281, 1936, 1322, 493, 19071, 3289, 938, 36606, 13], "temperature": 0.0, "avg_logprob": -0.1224478426433745, "compression_ratio": 1.6226415094339623, "no_speech_prob": 2.7264582058705855e-06}, {"id": 761, "seek": 381740, "start": 3817.4, "end": 3825.6800000000003, "text": " So in Keras, that's actually as simple as adding one additional parameter.", "tokens": [407, 294, 591, 6985, 11, 300, 311, 767, 382, 2199, 382, 5127, 472, 4497, 13075, 13], "temperature": 0.0, "avg_logprob": -0.11012175348069933, "compression_ratio": 1.536764705882353, "no_speech_prob": 1.0677022146410309e-06}, {"id": 762, "seek": 381740, "start": 3825.6800000000003, "end": 3833.4, "text": " And the additional parameter is called stateful.", "tokens": [400, 264, 4497, 13075, 307, 1219, 1785, 906, 13], "temperature": 0.0, "avg_logprob": -0.11012175348069933, "compression_ratio": 1.536764705882353, "no_speech_prob": 1.0677022146410309e-06}, {"id": 763, "seek": 381740, "start": 3833.4, "end": 3839.08, "text": " And so when you say stateful equals true, what that tells Keras is that at the end of", "tokens": [400, 370, 562, 291, 584, 1785, 906, 6915, 2074, 11, 437, 300, 5112, 591, 6985, 307, 300, 412, 264, 917, 295], "temperature": 0.0, "avg_logprob": -0.11012175348069933, "compression_ratio": 1.536764705882353, "no_speech_prob": 1.0677022146410309e-06}, {"id": 764, "seek": 383908, "start": 3839.08, "end": 3847.7999999999997, "text": " each sequence, don't reset the hidden activation to zero, but leave them as they are.", "tokens": [1184, 8310, 11, 500, 380, 14322, 264, 7633, 24433, 281, 4018, 11, 457, 1856, 552, 382, 436, 366, 13], "temperature": 0.0, "avg_logprob": -0.16514100574311755, "compression_ratio": 1.8270042194092826, "no_speech_prob": 3.1875515560386702e-06}, {"id": 765, "seek": 383908, "start": 3847.7999999999997, "end": 3852.92, "text": " That means we have to make sure we pass shuffle equals false when we train it.", "tokens": [663, 1355, 321, 362, 281, 652, 988, 321, 1320, 39426, 6915, 7908, 562, 321, 3847, 309, 13], "temperature": 0.0, "avg_logprob": -0.16514100574311755, "compression_ratio": 1.8270042194092826, "no_speech_prob": 3.1875515560386702e-06}, {"id": 766, "seek": 383908, "start": 3852.92, "end": 3856.64, "text": " So it's not going to pass the first 8 characters of the book, then the second 8 characters", "tokens": [407, 309, 311, 406, 516, 281, 1320, 264, 700, 1649, 4342, 295, 264, 1446, 11, 550, 264, 1150, 1649, 4342], "temperature": 0.0, "avg_logprob": -0.16514100574311755, "compression_ratio": 1.8270042194092826, "no_speech_prob": 3.1875515560386702e-06}, {"id": 767, "seek": 383908, "start": 3856.64, "end": 3862.72, "text": " of the book, then the third 8 characters of the book, leaving the hidden state untouched", "tokens": [295, 264, 1446, 11, 550, 264, 2636, 1649, 4342, 295, 264, 1446, 11, 5012, 264, 7633, 1785, 1701, 36740], "temperature": 0.0, "avg_logprob": -0.16514100574311755, "compression_ratio": 1.8270042194092826, "no_speech_prob": 3.1875515560386702e-06}, {"id": 768, "seek": 383908, "start": 3862.72, "end": 3867.12, "text": " between each one and therefore it's allowing it to continue to build up as much state as", "tokens": [1296, 1184, 472, 293, 4412, 309, 311, 8293, 309, 281, 2354, 281, 1322, 493, 382, 709, 1785, 382], "temperature": 0.0, "avg_logprob": -0.16514100574311755, "compression_ratio": 1.8270042194092826, "no_speech_prob": 3.1875515560386702e-06}, {"id": 769, "seek": 386712, "start": 3867.12, "end": 3870.7599999999998, "text": " it wants to.", "tokens": [309, 2738, 281, 13], "temperature": 0.0, "avg_logprob": -0.14059886699769555, "compression_ratio": 1.6030927835051547, "no_speech_prob": 1.2878929283033358e-06}, {"id": 770, "seek": 386712, "start": 3870.7599999999998, "end": 3878.2799999999997, "text": " Training these stateful models is a lot harder than training the models we've seen so far.", "tokens": [20620, 613, 1785, 906, 5245, 307, 257, 688, 6081, 813, 3097, 264, 5245, 321, 600, 1612, 370, 1400, 13], "temperature": 0.0, "avg_logprob": -0.14059886699769555, "compression_ratio": 1.6030927835051547, "no_speech_prob": 1.2878929283033358e-06}, {"id": 771, "seek": 386712, "start": 3878.2799999999997, "end": 3879.96, "text": " And the reason is this.", "tokens": [400, 264, 1778, 307, 341, 13], "temperature": 0.0, "avg_logprob": -0.14059886699769555, "compression_ratio": 1.6030927835051547, "no_speech_prob": 1.2878929283033358e-06}, {"id": 772, "seek": 386712, "start": 3879.96, "end": 3887.96, "text": " In these stateful models, this orange arrow, this single weight matrix, it's being applied", "tokens": [682, 613, 1785, 906, 5245, 11, 341, 7671, 11610, 11, 341, 2167, 3364, 8141, 11, 309, 311, 885, 6456], "temperature": 0.0, "avg_logprob": -0.14059886699769555, "compression_ratio": 1.6030927835051547, "no_speech_prob": 1.2878929283033358e-06}, {"id": 773, "seek": 386712, "start": 3887.96, "end": 3896.3599999999997, "text": " to this hidden matrix not 8 times, but 100,000 times or more, depending on how big your text", "tokens": [281, 341, 7633, 8141, 406, 1649, 1413, 11, 457, 2319, 11, 1360, 1413, 420, 544, 11, 5413, 322, 577, 955, 428, 2487], "temperature": 0.0, "avg_logprob": -0.14059886699769555, "compression_ratio": 1.6030927835051547, "no_speech_prob": 1.2878929283033358e-06}, {"id": 774, "seek": 389636, "start": 3896.36, "end": 3897.36, "text": " is.", "tokens": [307, 13], "temperature": 0.0, "avg_logprob": -0.1714290507788797, "compression_ratio": 1.608695652173913, "no_speech_prob": 6.4389801082143094e-06}, {"id": 775, "seek": 389636, "start": 3897.36, "end": 3903.2000000000003, "text": " And just imagine if this weight matrix was even slightly poorly scaled, so if there was", "tokens": [400, 445, 3811, 498, 341, 3364, 8141, 390, 754, 4748, 22271, 36039, 11, 370, 498, 456, 390], "temperature": 0.0, "avg_logprob": -0.1714290507788797, "compression_ratio": 1.608695652173913, "no_speech_prob": 6.4389801082143094e-06}, {"id": 776, "seek": 389636, "start": 3903.2000000000003, "end": 3908.96, "text": " like one number in it which was just a bit too high, then effectively that number is", "tokens": [411, 472, 1230, 294, 309, 597, 390, 445, 257, 857, 886, 1090, 11, 550, 8659, 300, 1230, 307], "temperature": 0.0, "avg_logprob": -0.1714290507788797, "compression_ratio": 1.608695652173913, "no_speech_prob": 6.4389801082143094e-06}, {"id": 777, "seek": 389636, "start": 3908.96, "end": 3912.4, "text": " going to be to the power of 100,000.", "tokens": [516, 281, 312, 281, 264, 1347, 295, 2319, 11, 1360, 13], "temperature": 0.0, "avg_logprob": -0.1714290507788797, "compression_ratio": 1.608695652173913, "no_speech_prob": 6.4389801082143094e-06}, {"id": 778, "seek": 389636, "start": 3912.4, "end": 3915.6, "text": " It's being multiplied again and again and again.", "tokens": [467, 311, 885, 17207, 797, 293, 797, 293, 797, 13], "temperature": 0.0, "avg_logprob": -0.1714290507788797, "compression_ratio": 1.608695652173913, "no_speech_prob": 6.4389801082143094e-06}, {"id": 779, "seek": 389636, "start": 3915.6, "end": 3920.92, "text": " So what can happen is you get this problem they call exploding gradients, or really in", "tokens": [407, 437, 393, 1051, 307, 291, 483, 341, 1154, 436, 818, 35175, 2771, 2448, 11, 420, 534, 294], "temperature": 0.0, "avg_logprob": -0.1714290507788797, "compression_ratio": 1.608695652173913, "no_speech_prob": 6.4389801082143094e-06}, {"id": 780, "seek": 389636, "start": 3920.92, "end": 3924.6, "text": " some ways it's better described as exploding activations.", "tokens": [512, 2098, 309, 311, 1101, 7619, 382, 35175, 2430, 763, 13], "temperature": 0.0, "avg_logprob": -0.1714290507788797, "compression_ratio": 1.608695652173913, "no_speech_prob": 6.4389801082143094e-06}, {"id": 781, "seek": 392460, "start": 3924.6, "end": 3930.7799999999997, "text": " Because we're multiplying this by almost the same weight matrix each time, if that weight", "tokens": [1436, 321, 434, 30955, 341, 538, 1920, 264, 912, 3364, 8141, 1184, 565, 11, 498, 300, 3364], "temperature": 0.0, "avg_logprob": -0.12771716749811746, "compression_ratio": 1.6126126126126126, "no_speech_prob": 1.3081734095976572e-06}, {"id": 782, "seek": 392460, "start": 3930.7799999999997, "end": 3937.54, "text": " matrix is anything less than perfectly scaled, then it's going to make our hidden matrix", "tokens": [8141, 307, 1340, 1570, 813, 6239, 36039, 11, 550, 309, 311, 516, 281, 652, 527, 7633, 8141], "temperature": 0.0, "avg_logprob": -0.12771716749811746, "compression_ratio": 1.6126126126126126, "no_speech_prob": 1.3081734095976572e-06}, {"id": 783, "seek": 392460, "start": 3937.54, "end": 3940.4, "text": " disappear off into infinity.", "tokens": [11596, 766, 666, 13202, 13], "temperature": 0.0, "avg_logprob": -0.12771716749811746, "compression_ratio": 1.6126126126126126, "no_speech_prob": 1.3081734095976572e-06}, {"id": 784, "seek": 392460, "start": 3940.4, "end": 3943.88, "text": " And so we have to be very careful of how to train these.", "tokens": [400, 370, 321, 362, 281, 312, 588, 5026, 295, 577, 281, 3847, 613, 13], "temperature": 0.0, "avg_logprob": -0.12771716749811746, "compression_ratio": 1.6126126126126126, "no_speech_prob": 1.3081734095976572e-06}, {"id": 785, "seek": 392460, "start": 3943.88, "end": 3950.24, "text": " And indeed, these kinds of long-term dependency models were thought of as impossible to train", "tokens": [400, 6451, 11, 613, 3685, 295, 938, 12, 7039, 33621, 5245, 645, 1194, 295, 382, 6243, 281, 3847], "temperature": 0.0, "avg_logprob": -0.12771716749811746, "compression_ratio": 1.6126126126126126, "no_speech_prob": 1.3081734095976572e-06}, {"id": 786, "seek": 395024, "start": 3950.24, "end": 3961.68, "text": " for a while, until some folks in the mid-90s came up with a model called the LSTM, or long", "tokens": [337, 257, 1339, 11, 1826, 512, 4024, 294, 264, 2062, 12, 7771, 82, 1361, 493, 365, 257, 2316, 1219, 264, 441, 6840, 44, 11, 420, 938], "temperature": 0.0, "avg_logprob": -0.1869272470474243, "compression_ratio": 1.5449735449735449, "no_speech_prob": 5.255379619484302e-06}, {"id": 787, "seek": 395024, "start": 3961.68, "end": 3963.68, "text": " short-term memory.", "tokens": [2099, 12, 7039, 4675, 13], "temperature": 0.0, "avg_logprob": -0.1869272470474243, "compression_ratio": 1.5449735449735449, "no_speech_prob": 5.255379619484302e-06}, {"id": 788, "seek": 395024, "start": 3963.68, "end": 3967.12, "text": " In the long short-term memory, and we'll learn more about it next week, we're actually going", "tokens": [682, 264, 938, 2099, 12, 7039, 4675, 11, 293, 321, 603, 1466, 544, 466, 309, 958, 1243, 11, 321, 434, 767, 516], "temperature": 0.0, "avg_logprob": -0.1869272470474243, "compression_ratio": 1.5449735449735449, "no_speech_prob": 5.255379619484302e-06}, {"id": 789, "seek": 395024, "start": 3967.12, "end": 3973.2, "text": " to implement it ourselves from scratch, we replace this loop here with a loop where there", "tokens": [281, 4445, 309, 4175, 490, 8459, 11, 321, 7406, 341, 6367, 510, 365, 257, 6367, 689, 456], "temperature": 0.0, "avg_logprob": -0.1869272470474243, "compression_ratio": 1.5449735449735449, "no_speech_prob": 5.255379619484302e-06}, {"id": 790, "seek": 397320, "start": 3973.2, "end": 3980.7599999999998, "text": " is actually a neural network inside the loop that decides how much of this state matrix", "tokens": [307, 767, 257, 18161, 3209, 1854, 264, 6367, 300, 14898, 577, 709, 295, 341, 1785, 8141], "temperature": 0.0, "avg_logprob": -0.09341594447260318, "compression_ratio": 1.7241379310344827, "no_speech_prob": 9.516168574918993e-06}, {"id": 791, "seek": 397320, "start": 3980.7599999999998, "end": 3985.52, "text": " to keep and how much to use at each activation.", "tokens": [281, 1066, 293, 577, 709, 281, 764, 412, 1184, 24433, 13], "temperature": 0.0, "avg_logprob": -0.09341594447260318, "compression_ratio": 1.7241379310344827, "no_speech_prob": 9.516168574918993e-06}, {"id": 792, "seek": 397320, "start": 3985.52, "end": 3991.3599999999997, "text": " And so by having a neural network which actually controls how much state is kept and how much", "tokens": [400, 370, 538, 1419, 257, 18161, 3209, 597, 767, 9003, 577, 709, 1785, 307, 4305, 293, 577, 709], "temperature": 0.0, "avg_logprob": -0.09341594447260318, "compression_ratio": 1.7241379310344827, "no_speech_prob": 9.516168574918993e-06}, {"id": 793, "seek": 397320, "start": 3991.3599999999997, "end": 3999.2, "text": " is used, it can actually learn how to avoid those gradient explosions.", "tokens": [307, 1143, 11, 309, 393, 767, 1466, 577, 281, 5042, 729, 16235, 36872, 13], "temperature": 0.0, "avg_logprob": -0.09341594447260318, "compression_ratio": 1.7241379310344827, "no_speech_prob": 9.516168574918993e-06}, {"id": 794, "seek": 399920, "start": 3999.2, "end": 4004.6, "text": " It can actually learn how to create an effective sequence.", "tokens": [467, 393, 767, 1466, 577, 281, 1884, 364, 4942, 8310, 13], "temperature": 0.0, "avg_logprob": -0.15095937495328943, "compression_ratio": 1.4776785714285714, "no_speech_prob": 1.3419889910437632e-05}, {"id": 795, "seek": 399920, "start": 4004.6, "end": 4007.24, "text": " So we're going to look at that a lot more next week.", "tokens": [407, 321, 434, 516, 281, 574, 412, 300, 257, 688, 544, 958, 1243, 13], "temperature": 0.0, "avg_logprob": -0.15095937495328943, "compression_ratio": 1.4776785714285714, "no_speech_prob": 1.3419889910437632e-05}, {"id": 796, "seek": 399920, "start": 4007.24, "end": 4013.68, "text": " But for now, I will tell you that when I tried to run this using a simple RNN, even with", "tokens": [583, 337, 586, 11, 286, 486, 980, 291, 300, 562, 286, 3031, 281, 1190, 341, 1228, 257, 2199, 45702, 45, 11, 754, 365], "temperature": 0.0, "avg_logprob": -0.15095937495328943, "compression_ratio": 1.4776785714285714, "no_speech_prob": 1.3419889910437632e-05}, {"id": 797, "seek": 399920, "start": 4013.68, "end": 4021.04, "text": " an identity matrix, initialization and relu's, I had no luck at all.", "tokens": [364, 6575, 8141, 11, 5883, 2144, 293, 1039, 84, 311, 11, 286, 632, 572, 3668, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.15095937495328943, "compression_ratio": 1.4776785714285714, "no_speech_prob": 1.3419889910437632e-05}, {"id": 798, "seek": 399920, "start": 4021.04, "end": 4023.4399999999996, "text": " So I had to replace it with an LSTM.", "tokens": [407, 286, 632, 281, 7406, 309, 365, 364, 441, 6840, 44, 13], "temperature": 0.0, "avg_logprob": -0.15095937495328943, "compression_ratio": 1.4776785714285714, "no_speech_prob": 1.3419889910437632e-05}, {"id": 799, "seek": 399920, "start": 4023.4399999999996, "end": 4024.96, "text": " Even that wasn't enough.", "tokens": [2754, 300, 2067, 380, 1547, 13], "temperature": 0.0, "avg_logprob": -0.15095937495328943, "compression_ratio": 1.4776785714285714, "no_speech_prob": 1.3419889910437632e-05}, {"id": 800, "seek": 402496, "start": 4024.96, "end": 4031.36, "text": " I had to have well-scaled inputs, so I added a batch normalization layer after my embeddings.", "tokens": [286, 632, 281, 362, 731, 12, 4417, 5573, 15743, 11, 370, 286, 3869, 257, 15245, 2710, 2144, 4583, 934, 452, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.11787779977388471, "compression_ratio": 1.5982905982905984, "no_speech_prob": 2.014539495576173e-05}, {"id": 801, "seek": 402496, "start": 4031.36, "end": 4036.92, "text": " And after I did those things, then I could fit it.", "tokens": [400, 934, 286, 630, 729, 721, 11, 550, 286, 727, 3318, 309, 13], "temperature": 0.0, "avg_logprob": -0.11787779977388471, "compression_ratio": 1.5982905982905984, "no_speech_prob": 2.014539495576173e-05}, {"id": 802, "seek": 402496, "start": 4036.92, "end": 4043.56, "text": " It still ran pretty slowly, so before I was getting 4 seconds per epoch, now it's 13 seconds", "tokens": [467, 920, 5872, 1238, 5692, 11, 370, 949, 286, 390, 1242, 1017, 3949, 680, 30992, 339, 11, 586, 309, 311, 3705, 3949], "temperature": 0.0, "avg_logprob": -0.11787779977388471, "compression_ratio": 1.5982905982905984, "no_speech_prob": 2.014539495576173e-05}, {"id": 803, "seek": 402496, "start": 4043.56, "end": 4044.76, "text": " per epoch.", "tokens": [680, 30992, 339, 13], "temperature": 0.0, "avg_logprob": -0.11787779977388471, "compression_ratio": 1.5982905982905984, "no_speech_prob": 2.014539495576173e-05}, {"id": 804, "seek": 402496, "start": 4044.76, "end": 4047.36, "text": " And the reason here is it's much harder to parallelize this.", "tokens": [400, 264, 1778, 510, 307, 309, 311, 709, 6081, 281, 8952, 1125, 341, 13], "temperature": 0.0, "avg_logprob": -0.11787779977388471, "compression_ratio": 1.5982905982905984, "no_speech_prob": 2.014539495576173e-05}, {"id": 805, "seek": 402496, "start": 4047.36, "end": 4053.4, "text": " It has to do each sequence in order, so it's going to be slower.", "tokens": [467, 575, 281, 360, 1184, 8310, 294, 1668, 11, 370, 309, 311, 516, 281, 312, 14009, 13], "temperature": 0.0, "avg_logprob": -0.11787779977388471, "compression_ratio": 1.5982905982905984, "no_speech_prob": 2.014539495576173e-05}, {"id": 806, "seek": 405340, "start": 4053.4, "end": 4061.52, "text": " But over time, it does eventually get substantially better loss than I had before, and that's", "tokens": [583, 670, 565, 11, 309, 775, 4728, 483, 30797, 1101, 4470, 813, 286, 632, 949, 11, 293, 300, 311], "temperature": 0.0, "avg_logprob": -0.22140672471788195, "compression_ratio": 1.434065934065934, "no_speech_prob": 8.530235390935559e-06}, {"id": 807, "seek": 405340, "start": 4061.52, "end": 4064.76, "text": " because it's able to keep track of and use this state.", "tokens": [570, 309, 311, 1075, 281, 1066, 2837, 295, 293, 764, 341, 1785, 13], "temperature": 0.0, "avg_logprob": -0.22140672471788195, "compression_ratio": 1.434065934065934, "no_speech_prob": 8.530235390935559e-06}, {"id": 808, "seek": 405340, "start": 4064.76, "end": 4065.76, "text": " Question.", "tokens": [14464, 13], "temperature": 0.0, "avg_logprob": -0.22140672471788195, "compression_ratio": 1.434065934065934, "no_speech_prob": 8.530235390935559e-06}, {"id": 809, "seek": 405340, "start": 4065.76, "end": 4071.56, "text": " Doesn't it make sense to use batch norm in the loop as well?", "tokens": [12955, 380, 309, 652, 2020, 281, 764, 15245, 2026, 294, 264, 6367, 382, 731, 30], "temperature": 0.0, "avg_logprob": -0.22140672471788195, "compression_ratio": 1.434065934065934, "no_speech_prob": 8.530235390935559e-06}, {"id": 810, "seek": 405340, "start": 4071.56, "end": 4076.44, "text": " That's a good question.", "tokens": [663, 311, 257, 665, 1168, 13], "temperature": 0.0, "avg_logprob": -0.22140672471788195, "compression_ratio": 1.434065934065934, "no_speech_prob": 8.530235390935559e-06}, {"id": 811, "seek": 405340, "start": 4076.44, "end": 4078.76, "text": " Definitely maybe.", "tokens": [12151, 1310, 13], "temperature": 0.0, "avg_logprob": -0.22140672471788195, "compression_ratio": 1.434065934065934, "no_speech_prob": 8.530235390935559e-06}, {"id": 812, "seek": 407876, "start": 4078.76, "end": 4083.6000000000004, "text": " There's been a lot of discussion and papers about this recently.", "tokens": [821, 311, 668, 257, 688, 295, 5017, 293, 10577, 466, 341, 3938, 13], "temperature": 0.0, "avg_logprob": -0.14348670873749123, "compression_ratio": 1.6120689655172413, "no_speech_prob": 1.3419846254691947e-05}, {"id": 813, "seek": 407876, "start": 4083.6000000000004, "end": 4088.44, "text": " There's something called layer normalization, which is a method which is explicitly designed", "tokens": [821, 311, 746, 1219, 4583, 2710, 2144, 11, 597, 307, 257, 3170, 597, 307, 20803, 4761], "temperature": 0.0, "avg_logprob": -0.14348670873749123, "compression_ratio": 1.6120689655172413, "no_speech_prob": 1.3419846254691947e-05}, {"id": 814, "seek": 407876, "start": 4088.44, "end": 4092.7200000000003, "text": " to work well with RNNs.", "tokens": [281, 589, 731, 365, 45702, 45, 82, 13], "temperature": 0.0, "avg_logprob": -0.14348670873749123, "compression_ratio": 1.6120689655172413, "no_speech_prob": 1.3419846254691947e-05}, {"id": 815, "seek": 407876, "start": 4092.7200000000003, "end": 4095.6800000000003, "text": " Standard batch norm doesn't.", "tokens": [21298, 15245, 2026, 1177, 380, 13], "temperature": 0.0, "avg_logprob": -0.14348670873749123, "compression_ratio": 1.6120689655172413, "no_speech_prob": 1.3419846254691947e-05}, {"id": 816, "seek": 407876, "start": 4095.6800000000003, "end": 4100.92, "text": " It turns out it's actually very easy to do layer normalization with Keras using a couple", "tokens": [467, 4523, 484, 309, 311, 767, 588, 1858, 281, 360, 4583, 2710, 2144, 365, 591, 6985, 1228, 257, 1916], "temperature": 0.0, "avg_logprob": -0.14348670873749123, "compression_ratio": 1.6120689655172413, "no_speech_prob": 1.3419846254691947e-05}, {"id": 817, "seek": 407876, "start": 4100.92, "end": 4105.88, "text": " of simple parameters you can provide to the normal batch norm constructor.", "tokens": [295, 2199, 9834, 291, 393, 2893, 281, 264, 2710, 15245, 2026, 47479, 13], "temperature": 0.0, "avg_logprob": -0.14348670873749123, "compression_ratio": 1.6120689655172413, "no_speech_prob": 1.3419846254691947e-05}, {"id": 818, "seek": 410588, "start": 4105.88, "end": 4112.68, "text": " In my experiments, that hasn't worked so well, and I will show you a lot more about that", "tokens": [682, 452, 12050, 11, 300, 6132, 380, 2732, 370, 731, 11, 293, 286, 486, 855, 291, 257, 688, 544, 466, 300], "temperature": 0.0, "avg_logprob": -0.18352052354321038, "compression_ratio": 1.6188340807174888, "no_speech_prob": 8.800951945886482e-06}, {"id": 819, "seek": 410588, "start": 4112.68, "end": 4118.2, "text": " in just a few minutes.", "tokens": [294, 445, 257, 1326, 2077, 13], "temperature": 0.0, "avg_logprob": -0.18352052354321038, "compression_ratio": 1.6188340807174888, "no_speech_prob": 8.800951945886482e-06}, {"id": 820, "seek": 410588, "start": 4118.2, "end": 4119.84, "text": " Stakeful models are great.", "tokens": [745, 619, 906, 5245, 366, 869, 13], "temperature": 0.0, "avg_logprob": -0.18352052354321038, "compression_ratio": 1.6188340807174888, "no_speech_prob": 8.800951945886482e-06}, {"id": 821, "seek": 410588, "start": 4119.84, "end": 4124.64, "text": " We're going to look at some very successful stateful models in just a moment, but just", "tokens": [492, 434, 516, 281, 574, 412, 512, 588, 4406, 1785, 906, 5245, 294, 445, 257, 1623, 11, 457, 445], "temperature": 0.0, "avg_logprob": -0.18352052354321038, "compression_ratio": 1.6188340807174888, "no_speech_prob": 8.800951945886482e-06}, {"id": 822, "seek": 410588, "start": 4124.64, "end": 4128.04, "text": " be aware that they are more challenging to train.", "tokens": [312, 3650, 300, 436, 366, 544, 7595, 281, 3847, 13], "temperature": 0.0, "avg_logprob": -0.18352052354321038, "compression_ratio": 1.6188340807174888, "no_speech_prob": 8.800951945886482e-06}, {"id": 823, "seek": 410588, "start": 4128.04, "end": 4131.36, "text": " You'll see another thing I had to do here is I had to reduce the learning rate in the", "tokens": [509, 603, 536, 1071, 551, 286, 632, 281, 360, 510, 307, 286, 632, 281, 5407, 264, 2539, 3314, 294, 264], "temperature": 0.0, "avg_logprob": -0.18352052354321038, "compression_ratio": 1.6188340807174888, "no_speech_prob": 8.800951945886482e-06}, {"id": 824, "seek": 413136, "start": 4131.36, "end": 4144.839999999999, "text": " middle, again, because you just have to be so careful of these exploding gradient problems.", "tokens": [2808, 11, 797, 11, 570, 291, 445, 362, 281, 312, 370, 5026, 295, 613, 35175, 16235, 2740, 13], "temperature": 0.0, "avg_logprob": -0.19858183684172453, "compression_ratio": 1.3733333333333333, "no_speech_prob": 2.994429678437882e-06}, {"id": 825, "seek": 413136, "start": 4144.839999999999, "end": 4148.639999999999, "text": " Let me show you what I did with this.", "tokens": [961, 385, 855, 291, 437, 286, 630, 365, 341, 13], "temperature": 0.0, "avg_logprob": -0.19858183684172453, "compression_ratio": 1.3733333333333333, "no_speech_prob": 2.994429678437882e-06}, {"id": 826, "seek": 413136, "start": 4148.639999999999, "end": 4154.839999999999, "text": " Which is I tried to create a stateful model which worked as well as I could.", "tokens": [3013, 307, 286, 3031, 281, 1884, 257, 1785, 906, 2316, 597, 2732, 382, 731, 382, 286, 727, 13], "temperature": 0.0, "avg_logprob": -0.19858183684172453, "compression_ratio": 1.3733333333333333, "no_speech_prob": 2.994429678437882e-06}, {"id": 827, "seek": 415484, "start": 4154.84, "end": 4161.4400000000005, "text": " So I took the same Nietzsche data as before, and I tried splitting it into chunks of 40", "tokens": [407, 286, 1890, 264, 912, 36583, 89, 12287, 1412, 382, 949, 11, 293, 286, 3031, 30348, 309, 666, 24004, 295, 3356], "temperature": 0.0, "avg_logprob": -0.16256491343180338, "compression_ratio": 1.488262910798122, "no_speech_prob": 3.2377481602452463e-06}, {"id": 828, "seek": 415484, "start": 4161.4400000000005, "end": 4167.84, "text": " rather than 8, so each one could do more work.", "tokens": [2831, 813, 1649, 11, 370, 1184, 472, 727, 360, 544, 589, 13], "temperature": 0.0, "avg_logprob": -0.16256491343180338, "compression_ratio": 1.488262910798122, "no_speech_prob": 3.2377481602452463e-06}, {"id": 829, "seek": 415484, "start": 4167.84, "end": 4173.12, "text": " So here are some examples of those chunks of 40.", "tokens": [407, 510, 366, 512, 5110, 295, 729, 24004, 295, 3356, 13], "temperature": 0.0, "avg_logprob": -0.16256491343180338, "compression_ratio": 1.488262910798122, "no_speech_prob": 3.2377481602452463e-06}, {"id": 830, "seek": 415484, "start": 4173.12, "end": 4177.56, "text": " I built a model that was slightly more sophisticated than the previous one in 2 ways.", "tokens": [286, 3094, 257, 2316, 300, 390, 4748, 544, 16950, 813, 264, 3894, 472, 294, 568, 2098, 13], "temperature": 0.0, "avg_logprob": -0.16256491343180338, "compression_ratio": 1.488262910798122, "no_speech_prob": 3.2377481602452463e-06}, {"id": 831, "seek": 415484, "start": 4177.56, "end": 4184.24, "text": " The first is it has an RNN feeding into an RNN.", "tokens": [440, 700, 307, 309, 575, 364, 45702, 45, 12919, 666, 364, 45702, 45, 13], "temperature": 0.0, "avg_logprob": -0.16256491343180338, "compression_ratio": 1.488262910798122, "no_speech_prob": 3.2377481602452463e-06}, {"id": 832, "seek": 418424, "start": 4184.24, "end": 4188.84, "text": " That's kind of a crazy idea, so I've drawn a picture.", "tokens": [663, 311, 733, 295, 257, 3219, 1558, 11, 370, 286, 600, 10117, 257, 3036, 13], "temperature": 0.0, "avg_logprob": -0.12868735369514017, "compression_ratio": 1.6041666666666667, "no_speech_prob": 8.939616236602888e-06}, {"id": 833, "seek": 418424, "start": 4188.84, "end": 4197.12, "text": " So an RNN feeding into an RNN means that the output is no longer going to an output.", "tokens": [407, 364, 45702, 45, 12919, 666, 364, 45702, 45, 1355, 300, 264, 5598, 307, 572, 2854, 516, 281, 364, 5598, 13], "temperature": 0.0, "avg_logprob": -0.12868735369514017, "compression_ratio": 1.6041666666666667, "no_speech_prob": 8.939616236602888e-06}, {"id": 834, "seek": 418424, "start": 4197.12, "end": 4204.099999999999, "text": " It's actually the output of the first RNN is becoming the input to the second RNN.", "tokens": [467, 311, 767, 264, 5598, 295, 264, 700, 45702, 45, 307, 5617, 264, 4846, 281, 264, 1150, 45702, 45, 13], "temperature": 0.0, "avg_logprob": -0.12868735369514017, "compression_ratio": 1.6041666666666667, "no_speech_prob": 8.939616236602888e-06}, {"id": 835, "seek": 418424, "start": 4204.099999999999, "end": 4210.5199999999995, "text": " So the character input goes into our first RNN and has the state updates as per usual,", "tokens": [407, 264, 2517, 4846, 1709, 666, 527, 700, 45702, 45, 293, 575, 264, 1785, 9205, 382, 680, 7713, 11], "temperature": 0.0, "avg_logprob": -0.12868735369514017, "compression_ratio": 1.6041666666666667, "no_speech_prob": 8.939616236602888e-06}, {"id": 836, "seek": 421052, "start": 4210.52, "end": 4217.280000000001, "text": " and then each time we go through the sequence, it feeds the result to the state of the second", "tokens": [293, 550, 1184, 565, 321, 352, 807, 264, 8310, 11, 309, 23712, 264, 1874, 281, 264, 1785, 295, 264, 1150], "temperature": 0.0, "avg_logprob": -0.17140725942758414, "compression_ratio": 1.5828877005347595, "no_speech_prob": 3.6119695323577616e-06}, {"id": 837, "seek": 421052, "start": 4217.280000000001, "end": 4219.080000000001, "text": " RNN.", "tokens": [45702, 45, 13], "temperature": 0.0, "avg_logprob": -0.17140725942758414, "compression_ratio": 1.5828877005347595, "no_speech_prob": 3.6119695323577616e-06}, {"id": 838, "seek": 421052, "start": 4219.080000000001, "end": 4220.080000000001, "text": " Why is this useful?", "tokens": [1545, 307, 341, 4420, 30], "temperature": 0.0, "avg_logprob": -0.17140725942758414, "compression_ratio": 1.5828877005347595, "no_speech_prob": 3.6119695323577616e-06}, {"id": 839, "seek": 421052, "start": 4220.080000000001, "end": 4230.4800000000005, "text": " Well, because it means that this output is now coming from not just a single dense matrix", "tokens": [1042, 11, 570, 309, 1355, 300, 341, 5598, 307, 586, 1348, 490, 406, 445, 257, 2167, 18011, 8141], "temperature": 0.0, "avg_logprob": -0.17140725942758414, "compression_ratio": 1.5828877005347595, "no_speech_prob": 3.6119695323577616e-06}, {"id": 840, "seek": 421052, "start": 4230.4800000000005, "end": 4240.4800000000005, "text": " with a single dense matrix here, it's actually going through 1, 2, 3 dense matrices and", "tokens": [365, 257, 2167, 18011, 8141, 510, 11, 309, 311, 767, 516, 807, 502, 11, 568, 11, 805, 18011, 32284, 293], "temperature": 0.0, "avg_logprob": -0.17140725942758414, "compression_ratio": 1.5828877005347595, "no_speech_prob": 3.6119695323577616e-06}, {"id": 841, "seek": 424048, "start": 4240.48, "end": 4241.799999999999, "text": " activation functions.", "tokens": [24433, 6828, 13], "temperature": 0.0, "avg_logprob": -0.22580628984429862, "compression_ratio": 1.7014218009478672, "no_speech_prob": 4.936957793688634e-06}, {"id": 842, "seek": 424048, "start": 4241.799999999999, "end": 4248.2, "text": " So I now have a deep neural network, assuming that 2 layers get to count as deep, between", "tokens": [407, 286, 586, 362, 257, 2452, 18161, 3209, 11, 11926, 300, 568, 7914, 483, 281, 1207, 382, 2452, 11, 1296], "temperature": 0.0, "avg_logprob": -0.22580628984429862, "compression_ratio": 1.7014218009478672, "no_speech_prob": 4.936957793688634e-06}, {"id": 843, "seek": 424048, "start": 4248.2, "end": 4251.919999999999, "text": " my first character and my first output.", "tokens": [452, 700, 2517, 293, 452, 700, 5598, 13], "temperature": 0.0, "avg_logprob": -0.22580628984429862, "compression_ratio": 1.7014218009478672, "no_speech_prob": 4.936957793688634e-06}, {"id": 844, "seek": 424048, "start": 4251.919999999999, "end": 4257.12, "text": " And then indeed between every hidden state and every output, I now have multiple hidden", "tokens": [400, 550, 6451, 1296, 633, 7633, 1785, 293, 633, 5598, 11, 286, 586, 362, 3866, 7633], "temperature": 0.0, "avg_logprob": -0.22580628984429862, "compression_ratio": 1.7014218009478672, "no_speech_prob": 4.936957793688634e-06}, {"id": 845, "seek": 424048, "start": 4257.12, "end": 4258.28, "text": " layers.", "tokens": [7914, 13], "temperature": 0.0, "avg_logprob": -0.22580628984429862, "compression_ratio": 1.7014218009478672, "no_speech_prob": 4.936957793688634e-06}, {"id": 846, "seek": 424048, "start": 4258.28, "end": 4265.16, "text": " So effectively what this is allowing us to do is to create a little deep neural net for", "tokens": [407, 8659, 437, 341, 307, 8293, 505, 281, 360, 307, 281, 1884, 257, 707, 2452, 18161, 2533, 337], "temperature": 0.0, "avg_logprob": -0.22580628984429862, "compression_ratio": 1.7014218009478672, "no_speech_prob": 4.936957793688634e-06}, {"id": 847, "seek": 424048, "start": 4265.16, "end": 4267.879999999999, "text": " all of our activations.", "tokens": [439, 295, 527, 2430, 763, 13], "temperature": 0.0, "avg_logprob": -0.22580628984429862, "compression_ratio": 1.7014218009478672, "no_speech_prob": 4.936957793688634e-06}, {"id": 848, "seek": 426788, "start": 4267.88, "end": 4274.84, "text": " And that turns out to work really well because the structure of language is pretty complex", "tokens": [400, 300, 4523, 484, 281, 589, 534, 731, 570, 264, 3877, 295, 2856, 307, 1238, 3997], "temperature": 0.0, "avg_logprob": -0.16917129709750792, "compression_ratio": 1.4924623115577889, "no_speech_prob": 3.3405265185137978e-06}, {"id": 849, "seek": 426788, "start": 4274.84, "end": 4282.12, "text": " and so it's nice to be able to give it a more flexible function that it can learn.", "tokens": [293, 370, 309, 311, 1481, 281, 312, 1075, 281, 976, 309, 257, 544, 11358, 2445, 300, 309, 393, 1466, 13], "temperature": 0.0, "avg_logprob": -0.16917129709750792, "compression_ratio": 1.4924623115577889, "no_speech_prob": 3.3405265185137978e-06}, {"id": 850, "seek": 426788, "start": 4282.12, "end": 4283.400000000001, "text": " So that's the first thing I do.", "tokens": [407, 300, 311, 264, 700, 551, 286, 360, 13], "temperature": 0.0, "avg_logprob": -0.16917129709750792, "compression_ratio": 1.4924623115577889, "no_speech_prob": 3.3405265185137978e-06}, {"id": 851, "seek": 426788, "start": 4283.400000000001, "end": 4285.96, "text": " And it's this easy to create that.", "tokens": [400, 309, 311, 341, 1858, 281, 1884, 300, 13], "temperature": 0.0, "avg_logprob": -0.16917129709750792, "compression_ratio": 1.4924623115577889, "no_speech_prob": 3.3405265185137978e-06}, {"id": 852, "seek": 426788, "start": 4285.96, "end": 4293.0, "text": " You just copy and paste whatever your RNN line is twice.", "tokens": [509, 445, 5055, 293, 9163, 2035, 428, 45702, 45, 1622, 307, 6091, 13], "temperature": 0.0, "avg_logprob": -0.16917129709750792, "compression_ratio": 1.4924623115577889, "no_speech_prob": 3.3405265185137978e-06}, {"id": 853, "seek": 429300, "start": 4293.0, "end": 4297.76, "text": " You can see I've now added dropout inside my RNN.", "tokens": [509, 393, 536, 286, 600, 586, 3869, 3270, 346, 1854, 452, 45702, 45, 13], "temperature": 0.0, "avg_logprob": -0.16783800432758947, "compression_ratio": 1.625, "no_speech_prob": 6.24087397227413e-06}, {"id": 854, "seek": 429300, "start": 4297.76, "end": 4303.72, "text": " And as I talked about before, adding dropout inside your RNN turns out to be a really good", "tokens": [400, 382, 286, 2825, 466, 949, 11, 5127, 3270, 346, 1854, 428, 45702, 45, 4523, 484, 281, 312, 257, 534, 665], "temperature": 0.0, "avg_logprob": -0.16783800432758947, "compression_ratio": 1.625, "no_speech_prob": 6.24087397227413e-06}, {"id": 855, "seek": 429300, "start": 4303.72, "end": 4309.28, "text": " idea and there's a really great paper about that quite recently showing that this is a", "tokens": [1558, 293, 456, 311, 257, 534, 869, 3035, 466, 300, 1596, 3938, 4099, 300, 341, 307, 257], "temperature": 0.0, "avg_logprob": -0.16783800432758947, "compression_ratio": 1.625, "no_speech_prob": 6.24087397227413e-06}, {"id": 856, "seek": 429300, "start": 4309.28, "end": 4312.88, "text": " great way to regularize an RNN.", "tokens": [869, 636, 281, 3890, 1125, 364, 45702, 45, 13], "temperature": 0.0, "avg_logprob": -0.16783800432758947, "compression_ratio": 1.625, "no_speech_prob": 6.24087397227413e-06}, {"id": 857, "seek": 429300, "start": 4312.88, "end": 4319.6, "text": " And then the second change I made is rather than going straight from the RNN to our output,", "tokens": [400, 550, 264, 1150, 1319, 286, 1027, 307, 2831, 813, 516, 2997, 490, 264, 45702, 45, 281, 527, 5598, 11], "temperature": 0.0, "avg_logprob": -0.16783800432758947, "compression_ratio": 1.625, "no_speech_prob": 6.24087397227413e-06}, {"id": 858, "seek": 431960, "start": 4319.6, "end": 4323.240000000001, "text": " I went through a dense layer.", "tokens": [286, 1437, 807, 257, 18011, 4583, 13], "temperature": 0.0, "avg_logprob": -0.15160299602307772, "compression_ratio": 1.639344262295082, "no_speech_prob": 5.093660092825303e-06}, {"id": 859, "seek": 431960, "start": 4323.240000000001, "end": 4328.200000000001, "text": " Now there's something that you might have noticed here is that our dense layers have", "tokens": [823, 456, 311, 746, 300, 291, 1062, 362, 5694, 510, 307, 300, 527, 18011, 7914, 362], "temperature": 0.0, "avg_logprob": -0.15160299602307772, "compression_ratio": 1.639344262295082, "no_speech_prob": 5.093660092825303e-06}, {"id": 860, "seek": 431960, "start": 4328.200000000001, "end": 4331.04, "text": " this extra word at the front.", "tokens": [341, 2857, 1349, 412, 264, 1868, 13], "temperature": 0.0, "avg_logprob": -0.15160299602307772, "compression_ratio": 1.639344262295082, "no_speech_prob": 5.093660092825303e-06}, {"id": 861, "seek": 431960, "start": 4331.04, "end": 4332.88, "text": " Why do they have this extra word at the front?", "tokens": [1545, 360, 436, 362, 341, 2857, 1349, 412, 264, 1868, 30], "temperature": 0.0, "avg_logprob": -0.15160299602307772, "compression_ratio": 1.639344262295082, "no_speech_prob": 5.093660092825303e-06}, {"id": 862, "seek": 431960, "start": 4332.88, "end": 4333.88, "text": " Time distributed.", "tokens": [6161, 12631, 13], "temperature": 0.0, "avg_logprob": -0.15160299602307772, "compression_ratio": 1.639344262295082, "no_speech_prob": 5.093660092825303e-06}, {"id": 863, "seek": 431960, "start": 4333.88, "end": 4343.200000000001, "text": " It might be easier to understand why by looking at this earlier sequence model with Keras.", "tokens": [467, 1062, 312, 3571, 281, 1223, 983, 538, 1237, 412, 341, 3071, 8310, 2316, 365, 591, 6985, 13], "temperature": 0.0, "avg_logprob": -0.15160299602307772, "compression_ratio": 1.639344262295082, "no_speech_prob": 5.093660092825303e-06}, {"id": 864, "seek": 434320, "start": 4343.2, "end": 4352.24, "text": " And note that the output of our RNN is not just a vector of length 256, but 8 vectors", "tokens": [400, 3637, 300, 264, 5598, 295, 527, 45702, 45, 307, 406, 445, 257, 8062, 295, 4641, 38882, 11, 457, 1649, 18875], "temperature": 0.0, "avg_logprob": -0.12158023105578476, "compression_ratio": 1.6824644549763033, "no_speech_prob": 2.190772192989243e-06}, {"id": 865, "seek": 434320, "start": 4352.24, "end": 4356.72, "text": " of length 256 because it's actually predicting 8 outputs.", "tokens": [295, 4641, 38882, 570, 309, 311, 767, 32884, 1649, 23930, 13], "temperature": 0.0, "avg_logprob": -0.12158023105578476, "compression_ratio": 1.6824644549763033, "no_speech_prob": 2.190772192989243e-06}, {"id": 866, "seek": 434320, "start": 4356.72, "end": 4365.04, "text": " So we can't just have a normal dense layer because a normal dense layer needs a single", "tokens": [407, 321, 393, 380, 445, 362, 257, 2710, 18011, 4583, 570, 257, 2710, 18011, 4583, 2203, 257, 2167], "temperature": 0.0, "avg_logprob": -0.12158023105578476, "compression_ratio": 1.6824644549763033, "no_speech_prob": 2.190772192989243e-06}, {"id": 867, "seek": 434320, "start": 4365.04, "end": 4368.08, "text": " dimension that it can squish down.", "tokens": [10139, 300, 309, 393, 31379, 760, 13], "temperature": 0.0, "avg_logprob": -0.12158023105578476, "compression_ratio": 1.6824644549763033, "no_speech_prob": 2.190772192989243e-06}, {"id": 868, "seek": 434320, "start": 4368.08, "end": 4373.12, "text": " So in this case, what we actually want to do is we want to create 8 separate dense layers", "tokens": [407, 294, 341, 1389, 11, 437, 321, 767, 528, 281, 360, 307, 321, 528, 281, 1884, 1649, 4994, 18011, 7914], "temperature": 0.0, "avg_logprob": -0.12158023105578476, "compression_ratio": 1.6824644549763033, "no_speech_prob": 2.190772192989243e-06}, {"id": 869, "seek": 437312, "start": 4373.12, "end": 4376.599999999999, "text": " at the output, one for every one of the outputs.", "tokens": [412, 264, 5598, 11, 472, 337, 633, 472, 295, 264, 23930, 13], "temperature": 0.0, "avg_logprob": -0.1322350738072159, "compression_ratio": 1.6794871794871795, "no_speech_prob": 3.4465649605408544e-06}, {"id": 870, "seek": 437312, "start": 4376.599999999999, "end": 4382.4, "text": " And so what time distributed does is it says whatever the layer is in the middle, I want", "tokens": [400, 370, 437, 565, 12631, 775, 307, 309, 1619, 2035, 264, 4583, 307, 294, 264, 2808, 11, 286, 528], "temperature": 0.0, "avg_logprob": -0.1322350738072159, "compression_ratio": 1.6794871794871795, "no_speech_prob": 3.4465649605408544e-06}, {"id": 871, "seek": 437312, "start": 4382.4, "end": 4388.599999999999, "text": " you to create 8 copies of them, or however long this dimension is.", "tokens": [291, 281, 1884, 1649, 14341, 295, 552, 11, 420, 4461, 938, 341, 10139, 307, 13], "temperature": 0.0, "avg_logprob": -0.1322350738072159, "compression_ratio": 1.6794871794871795, "no_speech_prob": 3.4465649605408544e-06}, {"id": 872, "seek": 437312, "start": 4388.599999999999, "end": 4392.44, "text": " And every one of those copies is going to share the same weight matrix, which is exactly", "tokens": [400, 633, 472, 295, 729, 14341, 307, 516, 281, 2073, 264, 912, 3364, 8141, 11, 597, 307, 2293], "temperature": 0.0, "avg_logprob": -0.1322350738072159, "compression_ratio": 1.6794871794871795, "no_speech_prob": 3.4465649605408544e-06}, {"id": 873, "seek": 437312, "start": 4392.44, "end": 4393.98, "text": " what we want.", "tokens": [437, 321, 528, 13], "temperature": 0.0, "avg_logprob": -0.1322350738072159, "compression_ratio": 1.6794871794871795, "no_speech_prob": 3.4465649605408544e-06}, {"id": 874, "seek": 437312, "start": 4393.98, "end": 4401.16, "text": " So the short version here is in Keras, any time you say return sequences equals true,", "tokens": [407, 264, 2099, 3037, 510, 307, 294, 591, 6985, 11, 604, 565, 291, 584, 2736, 22978, 6915, 2074, 11], "temperature": 0.0, "avg_logprob": -0.1322350738072159, "compression_ratio": 1.6794871794871795, "no_speech_prob": 3.4465649605408544e-06}, {"id": 875, "seek": 440116, "start": 4401.16, "end": 4406.0199999999995, "text": " any dense layers you have after that will always have to have time distributed wrapped", "tokens": [604, 18011, 7914, 291, 362, 934, 300, 486, 1009, 362, 281, 362, 565, 12631, 14226], "temperature": 0.0, "avg_logprob": -0.1418135135204761, "compression_ratio": 1.7925531914893618, "no_speech_prob": 2.1233736333670095e-06}, {"id": 876, "seek": 440116, "start": 4406.0199999999995, "end": 4413.78, "text": " around them because we want to create not just one dense layer, but 8 dense layers.", "tokens": [926, 552, 570, 321, 528, 281, 1884, 406, 445, 472, 18011, 4583, 11, 457, 1649, 18011, 7914, 13], "temperature": 0.0, "avg_logprob": -0.1418135135204761, "compression_ratio": 1.7925531914893618, "no_speech_prob": 2.1233736333670095e-06}, {"id": 877, "seek": 440116, "start": 4413.78, "end": 4418.84, "text": " So in this case, since we're saying return sequences equals true, we then have a time", "tokens": [407, 294, 341, 1389, 11, 1670, 321, 434, 1566, 2736, 22978, 6915, 2074, 11, 321, 550, 362, 257, 565], "temperature": 0.0, "avg_logprob": -0.1418135135204761, "compression_ratio": 1.7925531914893618, "no_speech_prob": 2.1233736333670095e-06}, {"id": 878, "seek": 440116, "start": 4418.84, "end": 4424.599999999999, "text": " distributed dense layer, some dropout, and another time distributed dense layer.", "tokens": [12631, 18011, 4583, 11, 512, 3270, 346, 11, 293, 1071, 565, 12631, 18011, 4583, 13], "temperature": 0.0, "avg_logprob": -0.1418135135204761, "compression_ratio": 1.7925531914893618, "no_speech_prob": 2.1233736333670095e-06}, {"id": 879, "seek": 442460, "start": 4424.6, "end": 4432.84, "text": " Question- Does the first RNN complete before it passes to the second, or is it layer by", "tokens": [14464, 12, 4402, 264, 700, 45702, 45, 3566, 949, 309, 11335, 281, 264, 1150, 11, 420, 307, 309, 4583, 538], "temperature": 0.0, "avg_logprob": -0.19988960094666214, "compression_ratio": 1.5789473684210527, "no_speech_prob": 2.014525671256706e-05}, {"id": 880, "seek": 442460, "start": 4432.84, "end": 4433.84, "text": " layer?", "tokens": [4583, 30], "temperature": 0.0, "avg_logprob": -0.19988960094666214, "compression_ratio": 1.5789473684210527, "no_speech_prob": 2.014525671256706e-05}, {"id": 881, "seek": 442460, "start": 4433.84, "end": 4438.52, "text": " Answer- No, it's operating exactly like this.", "tokens": [24545, 12, 883, 11, 309, 311, 7447, 2293, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.19988960094666214, "compression_ratio": 1.5789473684210527, "no_speech_prob": 2.014525671256706e-05}, {"id": 882, "seek": 442460, "start": 4438.52, "end": 4446.120000000001, "text": " So my initialization starts, my first character comes in, and at the output of that comes", "tokens": [407, 452, 5883, 2144, 3719, 11, 452, 700, 2517, 1487, 294, 11, 293, 412, 264, 5598, 295, 300, 1487], "temperature": 0.0, "avg_logprob": -0.19988960094666214, "compression_ratio": 1.5789473684210527, "no_speech_prob": 2.014525671256706e-05}, {"id": 883, "seek": 442460, "start": 4446.120000000001, "end": 4453.0, "text": " two things, the hidden state for my next hidden state and the output that goes into my second", "tokens": [732, 721, 11, 264, 7633, 1785, 337, 452, 958, 7633, 1785, 293, 264, 5598, 300, 1709, 666, 452, 1150], "temperature": 0.0, "avg_logprob": -0.19988960094666214, "compression_ratio": 1.5789473684210527, "no_speech_prob": 2.014525671256706e-05}, {"id": 884, "seek": 442460, "start": 4453.0, "end": 4454.400000000001, "text": " LSTN.", "tokens": [441, 6840, 45, 13], "temperature": 0.0, "avg_logprob": -0.19988960094666214, "compression_ratio": 1.5789473684210527, "no_speech_prob": 2.014525671256706e-05}, {"id": 885, "seek": 445440, "start": 4454.4, "end": 4458.719999999999, "text": " So everything is just pushed through.", "tokens": [407, 1203, 307, 445, 9152, 807, 13], "temperature": 0.0, "avg_logprob": -0.22478438929507608, "compression_ratio": 1.5895196506550218, "no_speech_prob": 2.5071307391044684e-05}, {"id": 886, "seek": 445440, "start": 4458.719999999999, "end": 4465.639999999999, "text": " The best way to think of this is to actually draw it in the unrolled form, and then you'll", "tokens": [440, 1151, 636, 281, 519, 295, 341, 307, 281, 767, 2642, 309, 294, 264, 517, 28850, 1254, 11, 293, 550, 291, 603], "temperature": 0.0, "avg_logprob": -0.22478438929507608, "compression_ratio": 1.5895196506550218, "no_speech_prob": 2.5071307391044684e-05}, {"id": 887, "seek": 445440, "start": 4465.639999999999, "end": 4468.98, "text": " realize there's nothing magical about this at all.", "tokens": [4325, 456, 311, 1825, 12066, 466, 341, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.22478438929507608, "compression_ratio": 1.5895196506550218, "no_speech_prob": 2.5071307391044684e-05}, {"id": 888, "seek": 445440, "start": 4468.98, "end": 4473.2, "text": " In an unrolled form, it just looks like a pretty standard deep neural net.", "tokens": [682, 364, 517, 28850, 1254, 11, 309, 445, 1542, 411, 257, 1238, 3832, 2452, 18161, 2533, 13], "temperature": 0.0, "avg_logprob": -0.22478438929507608, "compression_ratio": 1.5895196506550218, "no_speech_prob": 2.5071307391044684e-05}, {"id": 889, "seek": 445440, "start": 4473.2, "end": 4477.5599999999995, "text": " Question- What's dropout underscore u and dropout underscore w?", "tokens": [14464, 12, 708, 311, 3270, 346, 37556, 344, 293, 3270, 346, 37556, 261, 30], "temperature": 0.0, "avg_logprob": -0.22478438929507608, "compression_ratio": 1.5895196506550218, "no_speech_prob": 2.5071307391044684e-05}, {"id": 890, "seek": 445440, "start": 4477.5599999999995, "end": 4480.879999999999, "text": " Answer- We'll talk about that more next week.", "tokens": [24545, 12, 492, 603, 751, 466, 300, 544, 958, 1243, 13], "temperature": 0.0, "avg_logprob": -0.22478438929507608, "compression_ratio": 1.5895196506550218, "no_speech_prob": 2.5071307391044684e-05}, {"id": 891, "seek": 448088, "start": 4480.88, "end": 4487.2, "text": " In an LSTN, I mentioned that there's little neural nets that control how the state updates", "tokens": [682, 364, 441, 6840, 45, 11, 286, 2835, 300, 456, 311, 707, 18161, 36170, 300, 1969, 577, 264, 1785, 9205], "temperature": 0.0, "avg_logprob": -0.1723438580830892, "compression_ratio": 1.5560538116591929, "no_speech_prob": 1.4738857316842768e-05}, {"id": 892, "seek": 448088, "start": 4487.2, "end": 4492.4800000000005, "text": " work, and so this is talking about how the dropout works inside these little neural nets.", "tokens": [589, 11, 293, 370, 341, 307, 1417, 466, 577, 264, 3270, 346, 1985, 1854, 613, 707, 18161, 36170, 13], "temperature": 0.0, "avg_logprob": -0.1723438580830892, "compression_ratio": 1.5560538116591929, "no_speech_prob": 1.4738857316842768e-05}, {"id": 893, "seek": 448088, "start": 4492.4800000000005, "end": 4498.04, "text": " Question- When stateful is false, can you explain again what is reset after each training", "tokens": [14464, 12, 1133, 1785, 906, 307, 7908, 11, 393, 291, 2903, 797, 437, 307, 14322, 934, 1184, 3097], "temperature": 0.0, "avg_logprob": -0.1723438580830892, "compression_ratio": 1.5560538116591929, "no_speech_prob": 1.4738857316842768e-05}, {"id": 894, "seek": 448088, "start": 4498.04, "end": 4499.04, "text": " example?", "tokens": [1365, 30], "temperature": 0.0, "avg_logprob": -0.1723438580830892, "compression_ratio": 1.5560538116591929, "no_speech_prob": 1.4738857316842768e-05}, {"id": 895, "seek": 448088, "start": 4499.04, "end": 4501.88, "text": " Answer- Sure.", "tokens": [24545, 12, 4894, 13], "temperature": 0.0, "avg_logprob": -0.1723438580830892, "compression_ratio": 1.5560538116591929, "no_speech_prob": 1.4738857316842768e-05}, {"id": 896, "seek": 448088, "start": 4501.88, "end": 4507.36, "text": " The best way to describe that is to show us doing it.", "tokens": [440, 1151, 636, 281, 6786, 300, 307, 281, 855, 505, 884, 309, 13], "temperature": 0.0, "avg_logprob": -0.1723438580830892, "compression_ratio": 1.5560538116591929, "no_speech_prob": 1.4738857316842768e-05}, {"id": 897, "seek": 450736, "start": 4507.36, "end": 4516.679999999999, "text": " So remember that the RNNs that we built are identical to what Keras does, or close enough", "tokens": [407, 1604, 300, 264, 45702, 45, 82, 300, 321, 3094, 366, 14800, 281, 437, 591, 6985, 775, 11, 420, 1998, 1547], "temperature": 0.0, "avg_logprob": -0.11936604349236739, "compression_ratio": 1.5164835164835164, "no_speech_prob": 5.682410574081587e-06}, {"id": 898, "seek": 450736, "start": 4516.679999999999, "end": 4518.0, "text": " to identical.", "tokens": [281, 14800, 13], "temperature": 0.0, "avg_logprob": -0.11936604349236739, "compression_ratio": 1.5164835164835164, "no_speech_prob": 5.682410574081587e-06}, {"id": 899, "seek": 450736, "start": 4518.0, "end": 4524.4, "text": " So let's go and have a look at our version of return sequences.", "tokens": [407, 718, 311, 352, 293, 362, 257, 574, 412, 527, 3037, 295, 2736, 22978, 13], "temperature": 0.0, "avg_logprob": -0.11936604349236739, "compression_ratio": 1.5164835164835164, "no_speech_prob": 5.682410574081587e-06}, {"id": 900, "seek": 450736, "start": 4524.4, "end": 4533.4, "text": " So you can see that what we did was we created a matrix of zeros that we stuck onto the front", "tokens": [407, 291, 393, 536, 300, 437, 321, 630, 390, 321, 2942, 257, 8141, 295, 35193, 300, 321, 5541, 3911, 264, 1868], "temperature": 0.0, "avg_logprob": -0.11936604349236739, "compression_ratio": 1.5164835164835164, "no_speech_prob": 5.682410574081587e-06}, {"id": 901, "seek": 450736, "start": 4533.4, "end": 4535.16, "text": " of our inputs.", "tokens": [295, 527, 15743, 13], "temperature": 0.0, "avg_logprob": -0.11936604349236739, "compression_ratio": 1.5164835164835164, "no_speech_prob": 5.682410574081587e-06}, {"id": 902, "seek": 453516, "start": 4535.16, "end": 4541.26, "text": " So every set of 8 characters now starts with a vector of zeros.", "tokens": [407, 633, 992, 295, 1649, 4342, 586, 3719, 365, 257, 8062, 295, 35193, 13], "temperature": 0.0, "avg_logprob": -0.12035373449325562, "compression_ratio": 1.8324022346368716, "no_speech_prob": 2.6841873932426097e-06}, {"id": 903, "seek": 453516, "start": 4541.26, "end": 4550.4, "text": " So in other words, this initialized to zeros happens every time we finish a sequence.", "tokens": [407, 294, 661, 2283, 11, 341, 5883, 1602, 281, 35193, 2314, 633, 565, 321, 2413, 257, 8310, 13], "temperature": 0.0, "avg_logprob": -0.12035373449325562, "compression_ratio": 1.8324022346368716, "no_speech_prob": 2.6841873932426097e-06}, {"id": 904, "seek": 453516, "start": 4550.4, "end": 4556.5599999999995, "text": " So in other words, this hidden state gets initialized to zero at the end of every sequence.", "tokens": [407, 294, 661, 2283, 11, 341, 7633, 1785, 2170, 5883, 1602, 281, 4018, 412, 264, 917, 295, 633, 8310, 13], "temperature": 0.0, "avg_logprob": -0.12035373449325562, "compression_ratio": 1.8324022346368716, "no_speech_prob": 2.6841873932426097e-06}, {"id": 905, "seek": 453516, "start": 4556.5599999999995, "end": 4564.82, "text": " And it's this hidden state which is where all of these dependencies and state is kept.", "tokens": [400, 309, 311, 341, 7633, 1785, 597, 307, 689, 439, 295, 613, 36606, 293, 1785, 307, 4305, 13], "temperature": 0.0, "avg_logprob": -0.12035373449325562, "compression_ratio": 1.8324022346368716, "no_speech_prob": 2.6841873932426097e-06}, {"id": 906, "seek": 456482, "start": 4564.82, "end": 4569.759999999999, "text": " So doing that is resetting the state every time we look at a new sequence.", "tokens": [407, 884, 300, 307, 14322, 783, 264, 1785, 633, 565, 321, 574, 412, 257, 777, 8310, 13], "temperature": 0.0, "avg_logprob": -0.09172212481498718, "compression_ratio": 1.5463917525773196, "no_speech_prob": 4.56595580544672e-06}, {"id": 907, "seek": 456482, "start": 4569.759999999999, "end": 4577.799999999999, "text": " So when we say stateful equals false, it only does this initialized to zero step once at", "tokens": [407, 562, 321, 584, 1785, 906, 6915, 7908, 11, 309, 787, 775, 341, 5883, 1602, 281, 4018, 1823, 1564, 412], "temperature": 0.0, "avg_logprob": -0.09172212481498718, "compression_ratio": 1.5463917525773196, "no_speech_prob": 4.56595580544672e-06}, {"id": 908, "seek": 456482, "start": 4577.799999999999, "end": 4583.0, "text": " the very start, or when we explicitly ask it to.", "tokens": [264, 588, 722, 11, 420, 562, 321, 20803, 1029, 309, 281, 13], "temperature": 0.0, "avg_logprob": -0.09172212481498718, "compression_ratio": 1.5463917525773196, "no_speech_prob": 4.56595580544672e-06}, {"id": 909, "seek": 456482, "start": 4583.0, "end": 4589.32, "text": " And so when I actually run this model, the way I do it is I wrote a little thing called", "tokens": [400, 370, 562, 286, 767, 1190, 341, 2316, 11, 264, 636, 286, 360, 309, 307, 286, 4114, 257, 707, 551, 1219], "temperature": 0.0, "avg_logprob": -0.09172212481498718, "compression_ratio": 1.5463917525773196, "no_speech_prob": 4.56595580544672e-06}, {"id": 910, "seek": 458932, "start": 4589.32, "end": 4596.679999999999, "text": " runepox that goes model.resetStates and then does a fit on one epoch, which is what you", "tokens": [1190, 595, 5230, 300, 1709, 2316, 13, 495, 302, 4520, 1024, 293, 550, 775, 257, 3318, 322, 472, 30992, 339, 11, 597, 307, 437, 291], "temperature": 0.0, "avg_logprob": -0.16589528492518835, "compression_ratio": 1.4906832298136645, "no_speech_prob": 2.8130016289651394e-06}, {"id": 911, "seek": 458932, "start": 4596.679999999999, "end": 4601.34, "text": " really want at the end of your entire works of Nietzsche.", "tokens": [534, 528, 412, 264, 917, 295, 428, 2302, 1985, 295, 36583, 89, 12287, 13], "temperature": 0.0, "avg_logprob": -0.16589528492518835, "compression_ratio": 1.4906832298136645, "no_speech_prob": 2.8130016289651394e-06}, {"id": 912, "seek": 458932, "start": 4601.34, "end": 4605.44, "text": " You want to reset the state because you're about to go back to the very start and start", "tokens": [509, 528, 281, 14322, 264, 1785, 570, 291, 434, 466, 281, 352, 646, 281, 264, 588, 722, 293, 722], "temperature": 0.0, "avg_logprob": -0.16589528492518835, "compression_ratio": 1.4906832298136645, "no_speech_prob": 2.8130016289651394e-06}, {"id": 913, "seek": 458932, "start": 4605.44, "end": 4615.799999999999, "text": " again.", "tokens": [797, 13], "temperature": 0.0, "avg_logprob": -0.16589528492518835, "compression_ratio": 1.4906832298136645, "no_speech_prob": 2.8130016289651394e-06}, {"id": 914, "seek": 461580, "start": 4615.8, "end": 4623.68, "text": " So with this multilayer LSTM going into a multilayer neural net, I then tried seeing", "tokens": [407, 365, 341, 2120, 388, 11167, 441, 6840, 44, 516, 666, 257, 2120, 388, 11167, 18161, 2533, 11, 286, 550, 3031, 2577], "temperature": 0.0, "avg_logprob": -0.14989886500618674, "compression_ratio": 1.4927536231884058, "no_speech_prob": 2.190774694099673e-06}, {"id": 915, "seek": 461580, "start": 4623.68, "end": 4625.4800000000005, "text": " how that goes.", "tokens": [577, 300, 1709, 13], "temperature": 0.0, "avg_logprob": -0.14989886500618674, "compression_ratio": 1.4927536231884058, "no_speech_prob": 2.190774694099673e-06}, {"id": 916, "seek": 461580, "start": 4625.4800000000005, "end": 4631.4400000000005, "text": " And remember that with our simpler versions, we were getting 1.6 loss was the best we could", "tokens": [400, 1604, 300, 365, 527, 18587, 9606, 11, 321, 645, 1242, 502, 13, 21, 4470, 390, 264, 1151, 321, 727], "temperature": 0.0, "avg_logprob": -0.14989886500618674, "compression_ratio": 1.4927536231884058, "no_speech_prob": 2.190774694099673e-06}, {"id": 917, "seek": 461580, "start": 4631.4400000000005, "end": 4634.76, "text": " do.", "tokens": [360, 13], "temperature": 0.0, "avg_logprob": -0.14989886500618674, "compression_ratio": 1.4927536231884058, "no_speech_prob": 2.190774694099673e-06}, {"id": 918, "seek": 461580, "start": 4634.76, "end": 4637.76, "text": " After one epoch, it's awful.", "tokens": [2381, 472, 30992, 339, 11, 309, 311, 11232, 13], "temperature": 0.0, "avg_logprob": -0.14989886500618674, "compression_ratio": 1.4927536231884058, "no_speech_prob": 2.190774694099673e-06}, {"id": 919, "seek": 461580, "start": 4637.76, "end": 4644.0, "text": " And now rather than just printing out one letter, I'm starting with a whole sequence", "tokens": [400, 586, 2831, 813, 445, 14699, 484, 472, 5063, 11, 286, 478, 2891, 365, 257, 1379, 8310], "temperature": 0.0, "avg_logprob": -0.14989886500618674, "compression_ratio": 1.4927536231884058, "no_speech_prob": 2.190774694099673e-06}, {"id": 920, "seek": 464400, "start": 4644.0, "end": 4647.92, "text": " of letters, which is that, and asking it to generate a sequence.", "tokens": [295, 7825, 11, 597, 307, 300, 11, 293, 3365, 309, 281, 8460, 257, 8310, 13], "temperature": 0.0, "avg_logprob": -0.24127858644956118, "compression_ratio": 1.5222222222222221, "no_speech_prob": 1.3211797522671986e-05}, {"id": 921, "seek": 464400, "start": 4647.92, "end": 4651.76, "text": " You can see it starts out by generating a pretty rubbishy sequence.", "tokens": [509, 393, 536, 309, 3719, 484, 538, 17746, 257, 1238, 29978, 88, 8310, 13], "temperature": 0.0, "avg_logprob": -0.24127858644956118, "compression_ratio": 1.5222222222222221, "no_speech_prob": 1.3211797522671986e-05}, {"id": 922, "seek": 464400, "start": 4651.76, "end": 4654.76, "text": " Question from the audience.", "tokens": [14464, 490, 264, 4034, 13], "temperature": 0.0, "avg_logprob": -0.24127858644956118, "compression_ratio": 1.5222222222222221, "no_speech_prob": 1.3211797522671986e-05}, {"id": 923, "seek": 464400, "start": 4654.76, "end": 4660.72, "text": " In the double LSTM layer model, what is the input to the second LSTM in addition to the", "tokens": [682, 264, 3834, 441, 6840, 44, 4583, 2316, 11, 437, 307, 264, 4846, 281, 264, 1150, 441, 6840, 44, 294, 4500, 281, 264], "temperature": 0.0, "avg_logprob": -0.24127858644956118, "compression_ratio": 1.5222222222222221, "no_speech_prob": 1.3211797522671986e-05}, {"id": 924, "seek": 464400, "start": 4660.72, "end": 4666.0, "text": " output of the first LSTM?", "tokens": [5598, 295, 264, 700, 441, 6840, 44, 30], "temperature": 0.0, "avg_logprob": -0.24127858644956118, "compression_ratio": 1.5222222222222221, "no_speech_prob": 1.3211797522671986e-05}, {"id": 925, "seek": 466600, "start": 4666.0, "end": 4681.04, "text": " In addition to the output of the first LSTM is the previous output of its own hidden state.", "tokens": [682, 4500, 281, 264, 5598, 295, 264, 700, 441, 6840, 44, 307, 264, 3894, 5598, 295, 1080, 1065, 7633, 1785, 13], "temperature": 0.0, "avg_logprob": -0.1333517168388992, "compression_ratio": 1.473053892215569, "no_speech_prob": 8.939608051150572e-06}, {"id": 926, "seek": 466600, "start": 4681.04, "end": 4688.52, "text": " So after a few more epochs, it's starting to create some actual proper English words,", "tokens": [407, 934, 257, 1326, 544, 30992, 28346, 11, 309, 311, 2891, 281, 1884, 512, 3539, 2296, 3669, 2283, 11], "temperature": 0.0, "avg_logprob": -0.1333517168388992, "compression_ratio": 1.473053892215569, "no_speech_prob": 8.939608051150572e-06}, {"id": 927, "seek": 466600, "start": 4688.52, "end": 4693.86, "text": " although the English words aren't necessarily making a lot of sense.", "tokens": [4878, 264, 3669, 2283, 3212, 380, 4725, 1455, 257, 688, 295, 2020, 13], "temperature": 0.0, "avg_logprob": -0.1333517168388992, "compression_ratio": 1.473053892215569, "no_speech_prob": 8.939608051150572e-06}, {"id": 928, "seek": 469386, "start": 4693.86, "end": 4696.0199999999995, "text": " So keep running epochs.", "tokens": [407, 1066, 2614, 30992, 28346, 13], "temperature": 0.0, "avg_logprob": -0.23040428161621093, "compression_ratio": 1.6063348416289593, "no_speech_prob": 4.5396711357170716e-05}, {"id": 929, "seek": 469386, "start": 4696.0199999999995, "end": 4698.12, "text": " At this point it's learned how to start chapters.", "tokens": [1711, 341, 935, 309, 311, 3264, 577, 281, 722, 20013, 13], "temperature": 0.0, "avg_logprob": -0.23040428161621093, "compression_ratio": 1.6063348416289593, "no_speech_prob": 4.5396711357170716e-05}, {"id": 930, "seek": 469386, "start": 4698.12, "end": 4703.4, "text": " This is actually how in this book the chapters always start with a number and then an equals", "tokens": [639, 307, 767, 577, 294, 341, 1446, 264, 20013, 1009, 722, 365, 257, 1230, 293, 550, 364, 6915], "temperature": 0.0, "avg_logprob": -0.23040428161621093, "compression_ratio": 1.6063348416289593, "no_speech_prob": 4.5396711357170716e-05}, {"id": 931, "seek": 469386, "start": 4703.4, "end": 4704.4, "text": " sign.", "tokens": [1465, 13], "temperature": 0.0, "avg_logprob": -0.23040428161621093, "compression_ratio": 1.6063348416289593, "no_speech_prob": 4.5396711357170716e-05}, {"id": 932, "seek": 469386, "start": 4704.4, "end": 4707.32, "text": " It hasn't learned how to close quotes, apparently.", "tokens": [467, 6132, 380, 3264, 577, 281, 1998, 19963, 11, 7970, 13], "temperature": 0.0, "avg_logprob": -0.23040428161621093, "compression_ratio": 1.6063348416289593, "no_speech_prob": 4.5396711357170716e-05}, {"id": 933, "seek": 469386, "start": 4707.32, "end": 4709.98, "text": " It's not really saying anything useful.", "tokens": [467, 311, 406, 534, 1566, 1340, 4420, 13], "temperature": 0.0, "avg_logprob": -0.23040428161621093, "compression_ratio": 1.6063348416289593, "no_speech_prob": 4.5396711357170716e-05}, {"id": 934, "seek": 469386, "start": 4709.98, "end": 4719.639999999999, "text": " So anyway, I kind of ran this overnight, and I then seeded it with a larger amount of data.", "tokens": [407, 4033, 11, 286, 733, 295, 5872, 341, 13935, 11, 293, 286, 550, 8871, 292, 309, 365, 257, 4833, 2372, 295, 1412, 13], "temperature": 0.0, "avg_logprob": -0.23040428161621093, "compression_ratio": 1.6063348416289593, "no_speech_prob": 4.5396711357170716e-05}, {"id": 935, "seek": 471964, "start": 4719.64, "end": 4725.360000000001, "text": " I started getting some pretty reasonable results.", "tokens": [286, 1409, 1242, 512, 1238, 10585, 3542, 13], "temperature": 0.0, "avg_logprob": -0.2850794459498206, "compression_ratio": 1.5070422535211268, "no_speech_prob": 2.046241752395872e-05}, {"id": 936, "seek": 471964, "start": 4725.360000000001, "end": 4731.6, "text": " Shreds into one's own suffering sounds exactly like the kind of thing you might see.", "tokens": [1160, 986, 82, 666, 472, 311, 1065, 7755, 3263, 2293, 411, 264, 733, 295, 551, 291, 1062, 536, 13], "temperature": 0.0, "avg_logprob": -0.2850794459498206, "compression_ratio": 1.5070422535211268, "no_speech_prob": 2.046241752395872e-05}, {"id": 937, "seek": 471964, "start": 4731.6, "end": 4734.4800000000005, "text": " Religions have acts done by man.", "tokens": [33436, 626, 362, 10672, 1096, 538, 587, 13], "temperature": 0.0, "avg_logprob": -0.2850794459498206, "compression_ratio": 1.5070422535211268, "no_speech_prob": 2.046241752395872e-05}, {"id": 938, "seek": 471964, "start": 4734.4800000000005, "end": 4737.56, "text": " It's not all perfect, but it's not bad.", "tokens": [467, 311, 406, 439, 2176, 11, 457, 309, 311, 406, 1578, 13], "temperature": 0.0, "avg_logprob": -0.2850794459498206, "compression_ratio": 1.5070422535211268, "no_speech_prob": 2.046241752395872e-05}, {"id": 939, "seek": 471964, "start": 4737.56, "end": 4745.12, "text": " Interestingly, this sequence here, when I looked it up, it actually appears in his book.", "tokens": [30564, 11, 341, 8310, 510, 11, 562, 286, 2956, 309, 493, 11, 309, 767, 7038, 294, 702, 1446, 13], "temperature": 0.0, "avg_logprob": -0.2850794459498206, "compression_ratio": 1.5070422535211268, "no_speech_prob": 2.046241752395872e-05}, {"id": 940, "seek": 471964, "start": 4745.12, "end": 4746.88, "text": " This makes sense, right?", "tokens": [639, 1669, 2020, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2850794459498206, "compression_ratio": 1.5070422535211268, "no_speech_prob": 2.046241752395872e-05}, {"id": 941, "seek": 474688, "start": 4746.88, "end": 4752.76, "text": " It's a kind of overfitting in a sense.", "tokens": [467, 311, 257, 733, 295, 670, 69, 2414, 294, 257, 2020, 13], "temperature": 0.0, "avg_logprob": -0.23029108047485353, "compression_ratio": 1.5767195767195767, "no_speech_prob": 2.668757406354416e-05}, {"id": 942, "seek": 474688, "start": 4752.76, "end": 4756.92, "text": " He loves talking in all caps, but he only does it from time to time.", "tokens": [634, 6752, 1417, 294, 439, 13855, 11, 457, 415, 787, 775, 309, 490, 565, 281, 565, 13], "temperature": 0.0, "avg_logprob": -0.23029108047485353, "compression_ratio": 1.5767195767195767, "no_speech_prob": 2.668757406354416e-05}, {"id": 943, "seek": 474688, "start": 4756.92, "end": 4764.28, "text": " So once it so happened to start writing something in all caps that looked like this phrase,", "tokens": [407, 1564, 309, 370, 2011, 281, 722, 3579, 746, 294, 439, 13855, 300, 2956, 411, 341, 9535, 11], "temperature": 0.0, "avg_logprob": -0.23029108047485353, "compression_ratio": 1.5767195767195767, "no_speech_prob": 2.668757406354416e-05}, {"id": 944, "seek": 474688, "start": 4764.28, "end": 4768.76, "text": " that only appeared once and is very unique, there was no other way that it could have", "tokens": [300, 787, 8516, 1564, 293, 307, 588, 3845, 11, 456, 390, 572, 661, 636, 300, 309, 727, 362], "temperature": 0.0, "avg_logprob": -0.23029108047485353, "compression_ratio": 1.5767195767195767, "no_speech_prob": 2.668757406354416e-05}, {"id": 945, "seek": 474688, "start": 4768.76, "end": 4770.04, "text": " finished it.", "tokens": [4335, 309, 13], "temperature": 0.0, "avg_logprob": -0.23029108047485353, "compression_ratio": 1.5767195767195767, "no_speech_prob": 2.668757406354416e-05}, {"id": 946, "seek": 477004, "start": 4770.04, "end": 4776.96, "text": " So sometimes you get these little rare phrases that basically it's plagiarized directly from", "tokens": [407, 2171, 291, 483, 613, 707, 5892, 20312, 300, 1936, 309, 311, 33756, 9448, 1602, 3838, 490], "temperature": 0.0, "avg_logprob": -0.23454115125868055, "compression_ratio": 1.6103286384976525, "no_speech_prob": 1.3211679288360756e-05}, {"id": 947, "seek": 477004, "start": 4776.96, "end": 4777.96, "text": " nature.", "tokens": [3687, 13], "temperature": 0.0, "avg_logprob": -0.23454115125868055, "compression_ratio": 1.6103286384976525, "no_speech_prob": 1.3211679288360756e-05}, {"id": 948, "seek": 477004, "start": 4777.96, "end": 4783.16, "text": " Now, I didn't stop there because I thought how can we improve this.", "tokens": [823, 11, 286, 994, 380, 1590, 456, 570, 286, 1194, 577, 393, 321, 3470, 341, 13], "temperature": 0.0, "avg_logprob": -0.23454115125868055, "compression_ratio": 1.6103286384976525, "no_speech_prob": 1.3211679288360756e-05}, {"id": 949, "seek": 477004, "start": 4783.16, "end": 4786.6, "text": " It was at this point that I started thinking about batch normalization.", "tokens": [467, 390, 412, 341, 935, 300, 286, 1409, 1953, 466, 15245, 2710, 2144, 13], "temperature": 0.0, "avg_logprob": -0.23454115125868055, "compression_ratio": 1.6103286384976525, "no_speech_prob": 1.3211679288360756e-05}, {"id": 950, "seek": 477004, "start": 4786.6, "end": 4791.84, "text": " I started fiddling around with a lot of different types of batch normalization and layer normalization", "tokens": [286, 1409, 283, 14273, 1688, 926, 365, 257, 688, 295, 819, 3467, 295, 15245, 2710, 2144, 293, 4583, 2710, 2144], "temperature": 0.0, "avg_logprob": -0.23454115125868055, "compression_ratio": 1.6103286384976525, "no_speech_prob": 1.3211679288360756e-05}, {"id": 951, "seek": 479184, "start": 4791.84, "end": 4802.04, "text": " and discovered this interesting insight, which is that, at least in this case, the very best", "tokens": [293, 6941, 341, 1880, 11269, 11, 597, 307, 300, 11, 412, 1935, 294, 341, 1389, 11, 264, 588, 1151], "temperature": 0.0, "avg_logprob": -0.13001626416256554, "compression_ratio": 1.7586206896551724, "no_speech_prob": 6.339109404507326e-06}, {"id": 952, "seek": 479184, "start": 4802.04, "end": 4809.64, "text": " approach was when I simply applied batch normalization to the embedding layer.", "tokens": [3109, 390, 562, 286, 2935, 6456, 15245, 2710, 2144, 281, 264, 12240, 3584, 4583, 13], "temperature": 0.0, "avg_logprob": -0.13001626416256554, "compression_ratio": 1.7586206896551724, "no_speech_prob": 6.339109404507326e-06}, {"id": 953, "seek": 479184, "start": 4809.64, "end": 4814.88, "text": " So I want to show you what happened.", "tokens": [407, 286, 528, 281, 855, 291, 437, 2011, 13], "temperature": 0.0, "avg_logprob": -0.13001626416256554, "compression_ratio": 1.7586206896551724, "no_speech_prob": 6.339109404507326e-06}, {"id": 954, "seek": 479184, "start": 4814.88, "end": 4819.84, "text": " When I applied batch normalization to the embedding layer, this is the training curve", "tokens": [1133, 286, 6456, 15245, 2710, 2144, 281, 264, 12240, 3584, 4583, 11, 341, 307, 264, 3097, 7605], "temperature": 0.0, "avg_logprob": -0.13001626416256554, "compression_ratio": 1.7586206896551724, "no_speech_prob": 6.339109404507326e-06}, {"id": 955, "seek": 479184, "start": 4819.84, "end": 4820.84, "text": " that I got.", "tokens": [300, 286, 658, 13], "temperature": 0.0, "avg_logprob": -0.13001626416256554, "compression_ratio": 1.7586206896551724, "no_speech_prob": 6.339109404507326e-06}, {"id": 956, "seek": 482084, "start": 4820.84, "end": 4824.4400000000005, "text": " With the epochs, this is my loss.", "tokens": [2022, 264, 30992, 28346, 11, 341, 307, 452, 4470, 13], "temperature": 0.0, "avg_logprob": -0.16601503271805612, "compression_ratio": 1.7107843137254901, "no_speech_prob": 7.646465746802278e-06}, {"id": 957, "seek": 482084, "start": 4824.4400000000005, "end": 4828.72, "text": " With no batch normalization on the embedding layer, this was my loss.", "tokens": [2022, 572, 15245, 2710, 2144, 322, 264, 12240, 3584, 4583, 11, 341, 390, 452, 4470, 13], "temperature": 0.0, "avg_logprob": -0.16601503271805612, "compression_ratio": 1.7107843137254901, "no_speech_prob": 7.646465746802278e-06}, {"id": 958, "seek": 482084, "start": 4828.72, "end": 4831.88, "text": " And so you can see this was actually starting to flatten out.", "tokens": [400, 370, 291, 393, 536, 341, 390, 767, 2891, 281, 24183, 484, 13], "temperature": 0.0, "avg_logprob": -0.16601503271805612, "compression_ratio": 1.7107843137254901, "no_speech_prob": 7.646465746802278e-06}, {"id": 959, "seek": 482084, "start": 4831.88, "end": 4832.88, "text": " This one really wasn't.", "tokens": [639, 472, 534, 2067, 380, 13], "temperature": 0.0, "avg_logprob": -0.16601503271805612, "compression_ratio": 1.7107843137254901, "no_speech_prob": 7.646465746802278e-06}, {"id": 960, "seek": 482084, "start": 4832.88, "end": 4835.6, "text": " And this one was training a lot quicker.", "tokens": [400, 341, 472, 390, 3097, 257, 688, 16255, 13], "temperature": 0.0, "avg_logprob": -0.16601503271805612, "compression_ratio": 1.7107843137254901, "no_speech_prob": 7.646465746802278e-06}, {"id": 961, "seek": 482084, "start": 4835.6, "end": 4841.04, "text": " So then I tried training it with batch norm on the embedding layer overnight, and I was", "tokens": [407, 550, 286, 3031, 3097, 309, 365, 15245, 2026, 322, 264, 12240, 3584, 4583, 13935, 11, 293, 286, 390], "temperature": 0.0, "avg_logprob": -0.16601503271805612, "compression_ratio": 1.7107843137254901, "no_speech_prob": 7.646465746802278e-06}, {"id": 962, "seek": 482084, "start": 4841.04, "end": 4845.6, "text": " pretty stunned by the results.", "tokens": [1238, 35394, 538, 264, 3542, 13], "temperature": 0.0, "avg_logprob": -0.16601503271805612, "compression_ratio": 1.7107843137254901, "no_speech_prob": 7.646465746802278e-06}, {"id": 963, "seek": 484560, "start": 4845.6, "end": 4858.96, "text": " This was my seeding text, and after a thousand epochs, this is what it came up with.", "tokens": [639, 390, 452, 8871, 278, 2487, 11, 293, 934, 257, 4714, 30992, 28346, 11, 341, 307, 437, 309, 1361, 493, 365, 13], "temperature": 0.0, "avg_logprob": -0.246294371287028, "compression_ratio": 1.39375, "no_speech_prob": 2.7108510039397515e-05}, {"id": 964, "seek": 484560, "start": 4858.96, "end": 4862.4800000000005, "text": " And it's got all kinds of actually pretty interesting little things.", "tokens": [400, 309, 311, 658, 439, 3685, 295, 767, 1238, 1880, 707, 721, 13], "temperature": 0.0, "avg_logprob": -0.246294371287028, "compression_ratio": 1.39375, "no_speech_prob": 2.7108510039397515e-05}, {"id": 965, "seek": 484560, "start": 4862.4800000000005, "end": 4866.740000000001, "text": " Perhaps some morality equals self-glorification.", "tokens": [10517, 512, 29106, 6915, 2698, 12, 70, 6746, 3774, 13], "temperature": 0.0, "avg_logprob": -0.246294371287028, "compression_ratio": 1.39375, "no_speech_prob": 2.7108510039397515e-05}, {"id": 966, "seek": 484560, "start": 4866.740000000001, "end": 4870.240000000001, "text": " This is really cool.", "tokens": [639, 307, 534, 1627, 13], "temperature": 0.0, "avg_logprob": -0.246294371287028, "compression_ratio": 1.39375, "no_speech_prob": 2.7108510039397515e-05}, {"id": 967, "seek": 487024, "start": 4870.24, "end": 4878.8, "text": " For there are holy eyes to Schopenhauer's blind, you can see that it's learned to close", "tokens": [1171, 456, 366, 10622, 2575, 281, 2065, 15752, 71, 18120, 311, 6865, 11, 291, 393, 536, 300, 309, 311, 3264, 281, 1998], "temperature": 0.0, "avg_logprob": -0.17754926120533662, "compression_ratio": 1.5142857142857142, "no_speech_prob": 1.1300687219772954e-05}, {"id": 968, "seek": 487024, "start": 4878.8, "end": 4882.28, "text": " quotes even when those quotes were opened a long time ago.", "tokens": [19963, 754, 562, 729, 19963, 645, 5625, 257, 938, 565, 2057, 13], "temperature": 0.0, "avg_logprob": -0.17754926120533662, "compression_ratio": 1.5142857142857142, "no_speech_prob": 1.1300687219772954e-05}, {"id": 969, "seek": 487024, "start": 4882.28, "end": 4888.48, "text": " So if we weren't using stateful, it would never have learned how to do this.", "tokens": [407, 498, 321, 4999, 380, 1228, 1785, 906, 11, 309, 576, 1128, 362, 3264, 577, 281, 360, 341, 13], "temperature": 0.0, "avg_logprob": -0.17754926120533662, "compression_ratio": 1.5142857142857142, "no_speech_prob": 1.1300687219772954e-05}, {"id": 970, "seek": 487024, "start": 4888.48, "end": 4893.24, "text": " I've looked up these words in the original text, and pretty much none of these phrases", "tokens": [286, 600, 2956, 493, 613, 2283, 294, 264, 3380, 2487, 11, 293, 1238, 709, 6022, 295, 613, 20312], "temperature": 0.0, "avg_logprob": -0.17754926120533662, "compression_ratio": 1.5142857142857142, "no_speech_prob": 1.1300687219772954e-05}, {"id": 971, "seek": 487024, "start": 4893.24, "end": 4894.24, "text": " appear.", "tokens": [4204, 13], "temperature": 0.0, "avg_logprob": -0.17754926120533662, "compression_ratio": 1.5142857142857142, "no_speech_prob": 1.1300687219772954e-05}, {"id": 972, "seek": 489424, "start": 4894.24, "end": 4903.16, "text": " This is actually a genuine, novel, produced piece of text.", "tokens": [639, 307, 767, 257, 16699, 11, 7613, 11, 7126, 2522, 295, 2487, 13], "temperature": 0.0, "avg_logprob": -0.2199058992316924, "compression_ratio": 1.6057692307692308, "no_speech_prob": 1.9222941773477942e-05}, {"id": 973, "seek": 489424, "start": 4903.16, "end": 4909.84, "text": " It's not perfect by any means, but considering that this is only doing it character by character,", "tokens": [467, 311, 406, 2176, 538, 604, 1355, 11, 457, 8079, 300, 341, 307, 787, 884, 309, 2517, 538, 2517, 11], "temperature": 0.0, "avg_logprob": -0.2199058992316924, "compression_ratio": 1.6057692307692308, "no_speech_prob": 1.9222941773477942e-05}, {"id": 974, "seek": 489424, "start": 4909.84, "end": 4916.92, "text": " using nothing but a 42 long embedding matrix for each character, and nothing but there's", "tokens": [1228, 1825, 457, 257, 14034, 938, 12240, 3584, 8141, 337, 1184, 2517, 11, 293, 1825, 457, 456, 311], "temperature": 0.0, "avg_logprob": -0.2199058992316924, "compression_ratio": 1.6057692307692308, "no_speech_prob": 1.9222941773477942e-05}, {"id": 975, "seek": 489424, "start": 4916.92, "end": 4923.8, "text": " no pre-trained vectors or anything, there's just a pretty short 600,000 character epoch,", "tokens": [572, 659, 12, 17227, 2001, 18875, 420, 1340, 11, 456, 311, 445, 257, 1238, 2099, 11849, 11, 1360, 2517, 30992, 339, 11], "temperature": 0.0, "avg_logprob": -0.2199058992316924, "compression_ratio": 1.6057692307692308, "no_speech_prob": 1.9222941773477942e-05}, {"id": 976, "seek": 492380, "start": 4923.8, "end": 4929.72, "text": " I think it's done a pretty amazing job of creating a pretty good model.", "tokens": [286, 519, 309, 311, 1096, 257, 1238, 2243, 1691, 295, 4084, 257, 1238, 665, 2316, 13], "temperature": 0.0, "avg_logprob": -0.17997352749693626, "compression_ratio": 1.6790123456790123, "no_speech_prob": 7.25411664461717e-05}, {"id": 977, "seek": 492380, "start": 4929.72, "end": 4932.72, "text": " So there's all kinds of things you could do with a model like this.", "tokens": [407, 456, 311, 439, 3685, 295, 721, 291, 727, 360, 365, 257, 2316, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.17997352749693626, "compression_ratio": 1.6790123456790123, "no_speech_prob": 7.25411664461717e-05}, {"id": 978, "seek": 492380, "start": 4932.72, "end": 4939.28, "text": " The most obvious one would be if you were producing a software keyboard for a mobile", "tokens": [440, 881, 6322, 472, 576, 312, 498, 291, 645, 10501, 257, 4722, 10186, 337, 257, 6013], "temperature": 0.0, "avg_logprob": -0.17997352749693626, "compression_ratio": 1.6790123456790123, "no_speech_prob": 7.25411664461717e-05}, {"id": 979, "seek": 492380, "start": 4939.28, "end": 4943.4400000000005, "text": " phone, for example, you could use this to have a pretty accurate guess as to what they", "tokens": [2593, 11, 337, 1365, 11, 291, 727, 764, 341, 281, 362, 257, 1238, 8559, 2041, 382, 281, 437, 436], "temperature": 0.0, "avg_logprob": -0.17997352749693626, "compression_ratio": 1.6790123456790123, "no_speech_prob": 7.25411664461717e-05}, {"id": 980, "seek": 492380, "start": 4943.4400000000005, "end": 4946.76, "text": " were going to type next and correct it for them.", "tokens": [645, 516, 281, 2010, 958, 293, 3006, 309, 337, 552, 13], "temperature": 0.0, "avg_logprob": -0.17997352749693626, "compression_ratio": 1.6790123456790123, "no_speech_prob": 7.25411664461717e-05}, {"id": 981, "seek": 492380, "start": 4946.76, "end": 4949.92, "text": " You could do something similar on a word basis.", "tokens": [509, 727, 360, 746, 2531, 322, 257, 1349, 5143, 13], "temperature": 0.0, "avg_logprob": -0.17997352749693626, "compression_ratio": 1.6790123456790123, "no_speech_prob": 7.25411664461717e-05}, {"id": 982, "seek": 494992, "start": 4949.92, "end": 4955.36, "text": " But more generally, you could do something like anomaly detection with this.", "tokens": [583, 544, 5101, 11, 291, 727, 360, 746, 411, 42737, 17784, 365, 341, 13], "temperature": 0.0, "avg_logprob": -0.1554035167304837, "compression_ratio": 1.7763713080168777, "no_speech_prob": 2.6841912585950922e-06}, {"id": 983, "seek": 494992, "start": 4955.36, "end": 4960.72, "text": " You could generate a sequence that is predicting what the rest of the sequence is going to", "tokens": [509, 727, 8460, 257, 8310, 300, 307, 32884, 437, 264, 1472, 295, 264, 8310, 307, 516, 281], "temperature": 0.0, "avg_logprob": -0.1554035167304837, "compression_ratio": 1.7763713080168777, "no_speech_prob": 2.6841912585950922e-06}, {"id": 984, "seek": 494992, "start": 4960.72, "end": 4966.8, "text": " look like for the next hour and then recognize if something falls outside of what your prediction", "tokens": [574, 411, 337, 264, 958, 1773, 293, 550, 5521, 498, 746, 8804, 2380, 295, 437, 428, 17630], "temperature": 0.0, "avg_logprob": -0.1554035167304837, "compression_ratio": 1.7763713080168777, "no_speech_prob": 2.6841912585950922e-06}, {"id": 985, "seek": 494992, "start": 4966.8, "end": 4969.92, "text": " was and then you know that there's been some kind of anomaly.", "tokens": [390, 293, 550, 291, 458, 300, 456, 311, 668, 512, 733, 295, 42737, 13], "temperature": 0.0, "avg_logprob": -0.1554035167304837, "compression_ratio": 1.7763713080168777, "no_speech_prob": 2.6841912585950922e-06}, {"id": 986, "seek": 494992, "start": 4969.92, "end": 4974.68, "text": " There's all kinds of things you can do with these kinds of models.", "tokens": [821, 311, 439, 3685, 295, 721, 291, 393, 360, 365, 613, 3685, 295, 5245, 13], "temperature": 0.0, "avg_logprob": -0.1554035167304837, "compression_ratio": 1.7763713080168777, "no_speech_prob": 2.6841912585950922e-06}, {"id": 987, "seek": 494992, "start": 4974.68, "end": 4977.8, "text": " I think that's pretty fun.", "tokens": [286, 519, 300, 311, 1238, 1019, 13], "temperature": 0.0, "avg_logprob": -0.1554035167304837, "compression_ratio": 1.7763713080168777, "no_speech_prob": 2.6841912585950922e-06}, {"id": 988, "seek": 497780, "start": 4977.8, "end": 4986.52, "text": " I want to show you something else which is pretty fun, which is to build an RNN from", "tokens": [286, 528, 281, 855, 291, 746, 1646, 597, 307, 1238, 1019, 11, 597, 307, 281, 1322, 364, 45702, 45, 490], "temperature": 0.0, "avg_logprob": -0.14138202233748, "compression_ratio": 1.8481012658227849, "no_speech_prob": 2.5465749786235392e-05}, {"id": 989, "seek": 497780, "start": 4986.52, "end": 4989.84, "text": " scratch in Theano.", "tokens": [8459, 294, 440, 3730, 13], "temperature": 0.0, "avg_logprob": -0.14138202233748, "compression_ratio": 1.8481012658227849, "no_speech_prob": 2.5465749786235392e-05}, {"id": 990, "seek": 497780, "start": 4989.84, "end": 4993.88, "text": " What we're going to do is we're going to try and work up to next week where we're going", "tokens": [708, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 853, 293, 589, 493, 281, 958, 1243, 689, 321, 434, 516], "temperature": 0.0, "avg_logprob": -0.14138202233748, "compression_ratio": 1.8481012658227849, "no_speech_prob": 2.5465749786235392e-05}, {"id": 991, "seek": 497780, "start": 4993.88, "end": 4998.96, "text": " to build an RNN from scratch in NumPy.", "tokens": [281, 1322, 364, 45702, 45, 490, 8459, 294, 22592, 47, 88, 13], "temperature": 0.0, "avg_logprob": -0.14138202233748, "compression_ratio": 1.8481012658227849, "no_speech_prob": 2.5465749786235392e-05}, {"id": 992, "seek": 497780, "start": 4998.96, "end": 5006.28, "text": " And we're also going to build an LSTM from scratch in Theano.", "tokens": [400, 321, 434, 611, 516, 281, 1322, 364, 441, 6840, 44, 490, 8459, 294, 440, 3730, 13], "temperature": 0.0, "avg_logprob": -0.14138202233748, "compression_ratio": 1.8481012658227849, "no_speech_prob": 2.5465749786235392e-05}, {"id": 993, "seek": 500628, "start": 5006.28, "end": 5010.84, "text": " And the reason we're doing this is because next week's our last class in this part of", "tokens": [400, 264, 1778, 321, 434, 884, 341, 307, 570, 958, 1243, 311, 527, 1036, 1508, 294, 341, 644, 295], "temperature": 0.0, "avg_logprob": -0.17540992810888198, "compression_ratio": 1.624031007751938, "no_speech_prob": 2.4824605588946724e-06}, {"id": 994, "seek": 500628, "start": 5010.84, "end": 5011.84, "text": " the course.", "tokens": [264, 1164, 13], "temperature": 0.0, "avg_logprob": -0.17540992810888198, "compression_ratio": 1.624031007751938, "no_speech_prob": 2.4824605588946724e-06}, {"id": 995, "seek": 500628, "start": 5011.84, "end": 5019.2, "text": " I want us to leave with feeling like we really understand the details of what's going on", "tokens": [286, 528, 505, 281, 1856, 365, 2633, 411, 321, 534, 1223, 264, 4365, 295, 437, 311, 516, 322], "temperature": 0.0, "avg_logprob": -0.17540992810888198, "compression_ratio": 1.624031007751938, "no_speech_prob": 2.4824605588946724e-06}, {"id": 996, "seek": 500628, "start": 5019.2, "end": 5020.2, "text": " behind the scenes.", "tokens": [2261, 264, 8026, 13], "temperature": 0.0, "avg_logprob": -0.17540992810888198, "compression_ratio": 1.624031007751938, "no_speech_prob": 2.4824605588946724e-06}, {"id": 997, "seek": 500628, "start": 5020.2, "end": 5026.639999999999, "text": " The main thing I wanted to teach in this class is the applied stuff, these practical tips", "tokens": [440, 2135, 551, 286, 1415, 281, 2924, 294, 341, 1508, 307, 264, 6456, 1507, 11, 613, 8496, 6082], "temperature": 0.0, "avg_logprob": -0.17540992810888198, "compression_ratio": 1.624031007751938, "no_speech_prob": 2.4824605588946724e-06}, {"id": 998, "seek": 500628, "start": 5026.639999999999, "end": 5029.48, "text": " about how you build a sequence model.", "tokens": [466, 577, 291, 1322, 257, 8310, 2316, 13], "temperature": 0.0, "avg_logprob": -0.17540992810888198, "compression_ratio": 1.624031007751938, "no_speech_prob": 2.4824605588946724e-06}, {"id": 999, "seek": 500628, "start": 5029.48, "end": 5034.5199999999995, "text": " Use return equals true, put batch normal in the embedding layer, add time distributed", "tokens": [8278, 2736, 6915, 2074, 11, 829, 15245, 2710, 294, 264, 12240, 3584, 4583, 11, 909, 565, 12631], "temperature": 0.0, "avg_logprob": -0.17540992810888198, "compression_ratio": 1.624031007751938, "no_speech_prob": 2.4824605588946724e-06}, {"id": 1000, "seek": 503452, "start": 5034.52, "end": 5036.52, "text": " to the dense layer.", "tokens": [281, 264, 18011, 4583, 13], "temperature": 0.0, "avg_logprob": -0.19896643691592747, "compression_ratio": 1.5721649484536082, "no_speech_prob": 5.338080427463865e-06}, {"id": 1001, "seek": 503452, "start": 5036.52, "end": 5044.200000000001, "text": " But I also know that to really debug your models and to build your architectures and", "tokens": [583, 286, 611, 458, 300, 281, 534, 24083, 428, 5245, 293, 281, 1322, 428, 6331, 1303, 293], "temperature": 0.0, "avg_logprob": -0.19896643691592747, "compression_ratio": 1.5721649484536082, "no_speech_prob": 5.338080427463865e-06}, {"id": 1002, "seek": 503452, "start": 5044.200000000001, "end": 5049.120000000001, "text": " stuff, it really helps to understand what's going on.", "tokens": [1507, 11, 309, 534, 3665, 281, 1223, 437, 311, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.19896643691592747, "compression_ratio": 1.5721649484536082, "no_speech_prob": 5.338080427463865e-06}, {"id": 1003, "seek": 503452, "start": 5049.120000000001, "end": 5053.4800000000005, "text": " Particularly in the current situation where the tools and libraries available are not", "tokens": [32281, 294, 264, 2190, 2590, 689, 264, 3873, 293, 15148, 2435, 366, 406], "temperature": 0.0, "avg_logprob": -0.19896643691592747, "compression_ratio": 1.5721649484536082, "no_speech_prob": 5.338080427463865e-06}, {"id": 1004, "seek": 503452, "start": 5053.4800000000005, "end": 5059.72, "text": " that mature, they still require a whole lot of manual stuff.", "tokens": [300, 14442, 11, 436, 920, 3651, 257, 1379, 688, 295, 9688, 1507, 13], "temperature": 0.0, "avg_logprob": -0.19896643691592747, "compression_ratio": 1.5721649484536082, "no_speech_prob": 5.338080427463865e-06}, {"id": 1005, "seek": 505972, "start": 5059.72, "end": 5067.84, "text": " So I do want to try and explain a bit more about what's going on behind the scenes.", "tokens": [407, 286, 360, 528, 281, 853, 293, 2903, 257, 857, 544, 466, 437, 311, 516, 322, 2261, 264, 8026, 13], "temperature": 0.0, "avg_logprob": -0.11810590909874957, "compression_ratio": 1.5116279069767442, "no_speech_prob": 4.860406079387758e-06}, {"id": 1006, "seek": 505972, "start": 5067.84, "end": 5074.4400000000005, "text": " In order to build an RNN in Theano, I'm going to first of all make a small change to our", "tokens": [682, 1668, 281, 1322, 364, 45702, 45, 294, 440, 3730, 11, 286, 478, 516, 281, 700, 295, 439, 652, 257, 1359, 1319, 281, 527], "temperature": 0.0, "avg_logprob": -0.11810590909874957, "compression_ratio": 1.5116279069767442, "no_speech_prob": 4.860406079387758e-06}, {"id": 1007, "seek": 505972, "start": 5074.4400000000005, "end": 5079.7, "text": " Keras model, which is that I'm going to use one-hot encoding.", "tokens": [591, 6985, 2316, 11, 597, 307, 300, 286, 478, 516, 281, 764, 472, 12, 12194, 43430, 13], "temperature": 0.0, "avg_logprob": -0.11810590909874957, "compression_ratio": 1.5116279069767442, "no_speech_prob": 4.860406079387758e-06}, {"id": 1008, "seek": 505972, "start": 5079.7, "end": 5084.72, "text": " So I don't know if you noticed this, but we did something pretty cool in all of our models", "tokens": [407, 286, 500, 380, 458, 498, 291, 5694, 341, 11, 457, 321, 630, 746, 1238, 1627, 294, 439, 295, 527, 5245], "temperature": 0.0, "avg_logprob": -0.11810590909874957, "compression_ratio": 1.5116279069767442, "no_speech_prob": 4.860406079387758e-06}, {"id": 1009, "seek": 508472, "start": 5084.72, "end": 5091.76, "text": " so far, which is that we never actually one-hot encoded our output.", "tokens": [370, 1400, 11, 597, 307, 300, 321, 1128, 767, 472, 12, 12194, 2058, 12340, 527, 5598, 13], "temperature": 0.0, "avg_logprob": -0.3004357072173572, "compression_ratio": 1.4339622641509433, "no_speech_prob": 1.221887578140013e-05}, {"id": 1010, "seek": 508472, "start": 5091.76, "end": 5101.240000000001, "text": " Question from the audience.", "tokens": [14464, 490, 264, 4034, 13], "temperature": 0.0, "avg_logprob": -0.3004357072173572, "compression_ratio": 1.4339622641509433, "no_speech_prob": 1.221887578140013e-05}, {"id": 1011, "seek": 508472, "start": 5101.240000000001, "end": 5106.88, "text": " So if you don't add time distributed dense to a model where return sequences equals true,", "tokens": [407, 498, 291, 500, 380, 909, 565, 12631, 18011, 281, 257, 2316, 689, 2736, 22978, 6915, 2074, 11], "temperature": 0.0, "avg_logprob": -0.3004357072173572, "compression_ratio": 1.4339622641509433, "no_speech_prob": 1.221887578140013e-05}, {"id": 1012, "seek": 508472, "start": 5106.88, "end": 5109.68, "text": " it literally won't work, it won't compile.", "tokens": [309, 3736, 1582, 380, 589, 11, 309, 1582, 380, 31413, 13], "temperature": 0.0, "avg_logprob": -0.3004357072173572, "compression_ratio": 1.4339622641509433, "no_speech_prob": 1.221887578140013e-05}, {"id": 1013, "seek": 510968, "start": 5109.68, "end": 5114.6, "text": " Because you're trying to predict 8 things and the dense layer is going to stick that", "tokens": [1436, 291, 434, 1382, 281, 6069, 1649, 721, 293, 264, 18011, 4583, 307, 516, 281, 2897, 300], "temperature": 0.0, "avg_logprob": -0.17504402235442518, "compression_ratio": 1.6416666666666666, "no_speech_prob": 1.9525083189364523e-05}, {"id": 1014, "seek": 510968, "start": 5114.6, "end": 5121.6, "text": " all into one thing, so it's going to say there's a mismatch in your dimensions.", "tokens": [439, 666, 472, 551, 11, 370, 309, 311, 516, 281, 584, 456, 311, 257, 23220, 852, 294, 428, 12819, 13], "temperature": 0.0, "avg_logprob": -0.17504402235442518, "compression_ratio": 1.6416666666666666, "no_speech_prob": 1.9525083189364523e-05}, {"id": 1015, "seek": 510968, "start": 5121.6, "end": 5129.84, "text": " But no, it doesn't really add much time because that's something that can be very easily parallelized.", "tokens": [583, 572, 11, 309, 1177, 380, 534, 909, 709, 565, 570, 300, 311, 746, 300, 393, 312, 588, 3612, 8952, 1602, 13], "temperature": 0.0, "avg_logprob": -0.17504402235442518, "compression_ratio": 1.6416666666666666, "no_speech_prob": 1.9525083189364523e-05}, {"id": 1016, "seek": 510968, "start": 5129.84, "end": 5134.4400000000005, "text": " And since a lot of things in RNNs can't be easily parallelized, there generally is plenty", "tokens": [400, 1670, 257, 688, 295, 721, 294, 45702, 45, 82, 393, 380, 312, 3612, 8952, 1602, 11, 456, 5101, 307, 7140], "temperature": 0.0, "avg_logprob": -0.17504402235442518, "compression_ratio": 1.6416666666666666, "no_speech_prob": 1.9525083189364523e-05}, {"id": 1017, "seek": 510968, "start": 5134.4400000000005, "end": 5137.12, "text": " of room in your GPU to do more work.", "tokens": [295, 1808, 294, 428, 18407, 281, 360, 544, 589, 13], "temperature": 0.0, "avg_logprob": -0.17504402235442518, "compression_ratio": 1.6416666666666666, "no_speech_prob": 1.9525083189364523e-05}, {"id": 1018, "seek": 513712, "start": 5137.12, "end": 5141.8, "text": " So that should be fine.", "tokens": [407, 300, 820, 312, 2489, 13], "temperature": 0.0, "avg_logprob": -0.17548379506150338, "compression_ratio": 1.4790419161676647, "no_speech_prob": 6.144101007521385e-06}, {"id": 1019, "seek": 513712, "start": 5141.8, "end": 5146.04, "text": " The short answer is you have to use it, otherwise it won't work.", "tokens": [440, 2099, 1867, 307, 291, 362, 281, 764, 309, 11, 5911, 309, 1582, 380, 589, 13], "temperature": 0.0, "avg_logprob": -0.17548379506150338, "compression_ratio": 1.4790419161676647, "no_speech_prob": 6.144101007521385e-06}, {"id": 1020, "seek": 513712, "start": 5146.04, "end": 5152.5199999999995, "text": " I wanted to point out something, which is that in all of our models so far, we did not", "tokens": [286, 1415, 281, 935, 484, 746, 11, 597, 307, 300, 294, 439, 295, 527, 5245, 370, 1400, 11, 321, 630, 406], "temperature": 0.0, "avg_logprob": -0.17548379506150338, "compression_ratio": 1.4790419161676647, "no_speech_prob": 6.144101007521385e-06}, {"id": 1021, "seek": 513712, "start": 5152.5199999999995, "end": 5155.96, "text": " one-hot encode our outputs.", "tokens": [472, 12, 12194, 2058, 1429, 527, 23930, 13], "temperature": 0.0, "avg_logprob": -0.17548379506150338, "compression_ratio": 1.4790419161676647, "no_speech_prob": 6.144101007521385e-06}, {"id": 1022, "seek": 513712, "start": 5155.96, "end": 5166.24, "text": " So our outputs, remember, looked like this.", "tokens": [407, 527, 23930, 11, 1604, 11, 2956, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.17548379506150338, "compression_ratio": 1.4790419161676647, "no_speech_prob": 6.144101007521385e-06}, {"id": 1023, "seek": 516624, "start": 5166.24, "end": 5168.44, "text": " They were sequences of numbers.", "tokens": [814, 645, 22978, 295, 3547, 13], "temperature": 0.0, "avg_logprob": -0.13375598628346513, "compression_ratio": 1.581151832460733, "no_speech_prob": 3.966965778090525e-06}, {"id": 1024, "seek": 516624, "start": 5168.44, "end": 5175.599999999999, "text": " And so always before, we've had to one-hot encode our outputs to use them.", "tokens": [400, 370, 1009, 949, 11, 321, 600, 632, 281, 472, 12, 12194, 2058, 1429, 527, 23930, 281, 764, 552, 13], "temperature": 0.0, "avg_logprob": -0.13375598628346513, "compression_ratio": 1.581151832460733, "no_speech_prob": 3.966965778090525e-06}, {"id": 1025, "seek": 516624, "start": 5175.599999999999, "end": 5184.96, "text": " It turns out that Keras has a very cool loss function called sparse categorical cross-entropy.", "tokens": [467, 4523, 484, 300, 591, 6985, 575, 257, 588, 1627, 4470, 2445, 1219, 637, 11668, 19250, 804, 3278, 12, 317, 27514, 13], "temperature": 0.0, "avg_logprob": -0.13375598628346513, "compression_ratio": 1.581151832460733, "no_speech_prob": 3.966965778090525e-06}, {"id": 1026, "seek": 516624, "start": 5184.96, "end": 5193.92, "text": " And this is identical to categorical cross-entropy, but rather than taking a one-hot encoded target,", "tokens": [400, 341, 307, 14800, 281, 19250, 804, 3278, 12, 317, 27514, 11, 457, 2831, 813, 1940, 257, 472, 12, 12194, 2058, 12340, 3779, 11], "temperature": 0.0, "avg_logprob": -0.13375598628346513, "compression_ratio": 1.581151832460733, "no_speech_prob": 3.966965778090525e-06}, {"id": 1027, "seek": 519392, "start": 5193.92, "end": 5199.76, "text": " it takes an integer target and basically it acts as if you had one-hot encoded it.", "tokens": [309, 2516, 364, 24922, 3779, 293, 1936, 309, 10672, 382, 498, 291, 632, 472, 12, 12194, 2058, 12340, 309, 13], "temperature": 0.0, "avg_logprob": -0.14116927952442354, "compression_ratio": 1.646808510638298, "no_speech_prob": 3.500837692627101e-06}, {"id": 1028, "seek": 519392, "start": 5199.76, "end": 5204.28, "text": " So it basically does the indexing into it directly.", "tokens": [407, 309, 1936, 775, 264, 8186, 278, 666, 309, 3838, 13], "temperature": 0.0, "avg_logprob": -0.14116927952442354, "compression_ratio": 1.646808510638298, "no_speech_prob": 3.500837692627101e-06}, {"id": 1029, "seek": 519392, "start": 5204.28, "end": 5210.6, "text": " So this is a really helpful thing to know about because when you have a lot of output", "tokens": [407, 341, 307, 257, 534, 4961, 551, 281, 458, 466, 570, 562, 291, 362, 257, 688, 295, 5598], "temperature": 0.0, "avg_logprob": -0.14116927952442354, "compression_ratio": 1.646808510638298, "no_speech_prob": 3.500837692627101e-06}, {"id": 1030, "seek": 519392, "start": 5210.6, "end": 5217.16, "text": " categories, like for example, if you're doing a word model, you could have 100,000 output", "tokens": [10479, 11, 411, 337, 1365, 11, 498, 291, 434, 884, 257, 1349, 2316, 11, 291, 727, 362, 2319, 11, 1360, 5598], "temperature": 0.0, "avg_logprob": -0.14116927952442354, "compression_ratio": 1.646808510638298, "no_speech_prob": 3.500837692627101e-06}, {"id": 1031, "seek": 519392, "start": 5217.16, "end": 5218.16, "text": " categories.", "tokens": [10479, 13], "temperature": 0.0, "avg_logprob": -0.14116927952442354, "compression_ratio": 1.646808510638298, "no_speech_prob": 3.500837692627101e-06}, {"id": 1032, "seek": 519392, "start": 5218.16, "end": 5222.84, "text": " There's no way you want to create a matrix that is 100,000 long.", "tokens": [821, 311, 572, 636, 291, 528, 281, 1884, 257, 8141, 300, 307, 2319, 11, 1360, 938, 13], "temperature": 0.0, "avg_logprob": -0.14116927952442354, "compression_ratio": 1.646808510638298, "no_speech_prob": 3.500837692627101e-06}, {"id": 1033, "seek": 522284, "start": 5222.84, "end": 5226.68, "text": " Only all zeros for every single word in your output.", "tokens": [5686, 439, 35193, 337, 633, 2167, 1349, 294, 428, 5598, 13], "temperature": 0.0, "avg_logprob": -0.1663983785189115, "compression_ratio": 1.549800796812749, "no_speech_prob": 7.527898105763597e-06}, {"id": 1034, "seek": 522284, "start": 5226.68, "end": 5232.52, "text": " So by using sparse categorical cross-entropy, you can just forget the whole one-hot encoding.", "tokens": [407, 538, 1228, 637, 11668, 19250, 804, 3278, 12, 317, 27514, 11, 291, 393, 445, 2870, 264, 1379, 472, 12, 12194, 43430, 13], "temperature": 0.0, "avg_logprob": -0.1663983785189115, "compression_ratio": 1.549800796812749, "no_speech_prob": 7.527898105763597e-06}, {"id": 1035, "seek": 522284, "start": 5232.52, "end": 5234.24, "text": " You don't have to do it.", "tokens": [509, 500, 380, 362, 281, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.1663983785189115, "compression_ratio": 1.549800796812749, "no_speech_prob": 7.527898105763597e-06}, {"id": 1036, "seek": 522284, "start": 5234.24, "end": 5239.56, "text": " Keras implicitly does it for you, but without ever actually explicitly doing it, it just", "tokens": [591, 6985, 26947, 356, 775, 309, 337, 291, 11, 457, 1553, 1562, 767, 20803, 884, 309, 11, 309, 445], "temperature": 0.0, "avg_logprob": -0.1663983785189115, "compression_ratio": 1.549800796812749, "no_speech_prob": 7.527898105763597e-06}, {"id": 1037, "seek": 522284, "start": 5239.56, "end": 5243.8, "text": " does a direct lookup into the matrix.", "tokens": [775, 257, 2047, 574, 1010, 666, 264, 8141, 13], "temperature": 0.0, "avg_logprob": -0.1663983785189115, "compression_ratio": 1.549800796812749, "no_speech_prob": 7.527898105763597e-06}, {"id": 1038, "seek": 522284, "start": 5243.8, "end": 5250.24, "text": " However, because I want to make things simpler for us to understand, I'm going to go ahead", "tokens": [2908, 11, 570, 286, 528, 281, 652, 721, 18587, 337, 505, 281, 1223, 11, 286, 478, 516, 281, 352, 2286], "temperature": 0.0, "avg_logprob": -0.1663983785189115, "compression_ratio": 1.549800796812749, "no_speech_prob": 7.527898105763597e-06}, {"id": 1039, "seek": 525024, "start": 5250.24, "end": 5256.679999999999, "text": " and recreate our Keras model using one-hot encoding.", "tokens": [293, 25833, 527, 591, 6985, 2316, 1228, 472, 12, 12194, 43430, 13], "temperature": 0.0, "avg_logprob": -0.14764745712280272, "compression_ratio": 1.6774193548387097, "no_speech_prob": 2.482474201315199e-06}, {"id": 1040, "seek": 525024, "start": 5256.679999999999, "end": 5262.08, "text": " And so I'm going to take exactly the same model that we had before with return sequences", "tokens": [400, 370, 286, 478, 516, 281, 747, 2293, 264, 912, 2316, 300, 321, 632, 949, 365, 2736, 22978], "temperature": 0.0, "avg_logprob": -0.14764745712280272, "compression_ratio": 1.6774193548387097, "no_speech_prob": 2.482474201315199e-06}, {"id": 1041, "seek": 525024, "start": 5262.08, "end": 5270.5599999999995, "text": " equals true, but this time I'm going to use normal categorical cross-entropy.", "tokens": [6915, 2074, 11, 457, 341, 565, 286, 478, 516, 281, 764, 2710, 19250, 804, 3278, 12, 317, 27514, 13], "temperature": 0.0, "avg_logprob": -0.14764745712280272, "compression_ratio": 1.6774193548387097, "no_speech_prob": 2.482474201315199e-06}, {"id": 1042, "seek": 525024, "start": 5270.5599999999995, "end": 5273.2, "text": " The other thing I'm doing is I don't have an embedding layer.", "tokens": [440, 661, 551, 286, 478, 884, 307, 286, 500, 380, 362, 364, 12240, 3584, 4583, 13], "temperature": 0.0, "avg_logprob": -0.14764745712280272, "compression_ratio": 1.6774193548387097, "no_speech_prob": 2.482474201315199e-06}, {"id": 1043, "seek": 525024, "start": 5273.2, "end": 5278.28, "text": " So since I don't have an embedding layer, I also have to one-hot encode my inputs.", "tokens": [407, 1670, 286, 500, 380, 362, 364, 12240, 3584, 4583, 11, 286, 611, 362, 281, 472, 12, 12194, 2058, 1429, 452, 15743, 13], "temperature": 0.0, "avg_logprob": -0.14764745712280272, "compression_ratio": 1.6774193548387097, "no_speech_prob": 2.482474201315199e-06}, {"id": 1044, "seek": 527828, "start": 5278.28, "end": 5283.36, "text": " So you can see I'm calling 2 categorical on all of my inputs and 2 categorical on all", "tokens": [407, 291, 393, 536, 286, 478, 5141, 568, 19250, 804, 322, 439, 295, 452, 15743, 293, 568, 19250, 804, 322, 439], "temperature": 0.0, "avg_logprob": -0.20022295857523822, "compression_ratio": 1.5913978494623655, "no_speech_prob": 1.8448187120156945e-06}, {"id": 1045, "seek": 527828, "start": 5283.36, "end": 5286.84, "text": " of my outputs.", "tokens": [295, 452, 23930, 13], "temperature": 0.0, "avg_logprob": -0.20022295857523822, "compression_ratio": 1.5913978494623655, "no_speech_prob": 1.8448187120156945e-06}, {"id": 1046, "seek": 527828, "start": 5286.84, "end": 5294.679999999999, "text": " So now the shape is 75,000 by 8, as before, by 86.", "tokens": [407, 586, 264, 3909, 307, 9562, 11, 1360, 538, 1649, 11, 382, 949, 11, 538, 26687, 13], "temperature": 0.0, "avg_logprob": -0.20022295857523822, "compression_ratio": 1.5913978494623655, "no_speech_prob": 1.8448187120156945e-06}, {"id": 1047, "seek": 527828, "start": 5294.679999999999, "end": 5296.719999999999, "text": " So this is the one-hot encoding dimension.", "tokens": [407, 341, 307, 264, 472, 12, 12194, 43430, 10139, 13], "temperature": 0.0, "avg_logprob": -0.20022295857523822, "compression_ratio": 1.5913978494623655, "no_speech_prob": 1.8448187120156945e-06}, {"id": 1048, "seek": 527828, "start": 5296.719999999999, "end": 5301.5199999999995, "text": " There are 85 zeros and 1 one.", "tokens": [821, 366, 14695, 35193, 293, 502, 472, 13], "temperature": 0.0, "avg_logprob": -0.20022295857523822, "compression_ratio": 1.5913978494623655, "no_speech_prob": 1.8448187120156945e-06}, {"id": 1049, "seek": 527828, "start": 5301.5199999999995, "end": 5307.4, "text": " So we fit this in exactly the same way, we get exactly the same answer.", "tokens": [407, 321, 3318, 341, 294, 2293, 264, 912, 636, 11, 321, 483, 2293, 264, 912, 1867, 13], "temperature": 0.0, "avg_logprob": -0.20022295857523822, "compression_ratio": 1.5913978494623655, "no_speech_prob": 1.8448187120156945e-06}, {"id": 1050, "seek": 530740, "start": 5307.4, "end": 5313.759999999999, "text": " So the only reason I was doing that was because I want to use one-hot encoding for the version", "tokens": [407, 264, 787, 1778, 286, 390, 884, 300, 390, 570, 286, 528, 281, 764, 472, 12, 12194, 43430, 337, 264, 3037], "temperature": 0.0, "avg_logprob": -0.11077788141038683, "compression_ratio": 1.492063492063492, "no_speech_prob": 4.356854788056808e-06}, {"id": 1051, "seek": 530740, "start": 5313.759999999999, "end": 5320.04, "text": " that we're going to create ourselves from scratch.", "tokens": [300, 321, 434, 516, 281, 1884, 4175, 490, 8459, 13], "temperature": 0.0, "avg_logprob": -0.11077788141038683, "compression_ratio": 1.492063492063492, "no_speech_prob": 4.356854788056808e-06}, {"id": 1052, "seek": 530740, "start": 5320.04, "end": 5323.5199999999995, "text": " So we haven't really looked at Theano before.", "tokens": [407, 321, 2378, 380, 534, 2956, 412, 440, 3730, 949, 13], "temperature": 0.0, "avg_logprob": -0.11077788141038683, "compression_ratio": 1.492063492063492, "no_speech_prob": 4.356854788056808e-06}, {"id": 1053, "seek": 530740, "start": 5323.5199999999995, "end": 5332.759999999999, "text": " But particularly if you come back next year, as we start to try to add more and more stuff", "tokens": [583, 4098, 498, 291, 808, 646, 958, 1064, 11, 382, 321, 722, 281, 853, 281, 909, 544, 293, 544, 1507], "temperature": 0.0, "avg_logprob": -0.11077788141038683, "compression_ratio": 1.492063492063492, "no_speech_prob": 4.356854788056808e-06}, {"id": 1054, "seek": 533276, "start": 5332.76, "end": 5340.04, "text": " on top of Keras or into Keras, increasingly you'll find yourself wanting to use Theano.", "tokens": [322, 1192, 295, 591, 6985, 420, 666, 591, 6985, 11, 12980, 291, 603, 915, 1803, 7935, 281, 764, 440, 3730, 13], "temperature": 0.0, "avg_logprob": -0.1490408420562744, "compression_ratio": 1.6544502617801047, "no_speech_prob": 2.1111956812092103e-05}, {"id": 1055, "seek": 533276, "start": 5340.04, "end": 5347.52, "text": " Because Theano is the language that Keras is using behind the scenes and therefore it's", "tokens": [1436, 440, 3730, 307, 264, 2856, 300, 591, 6985, 307, 1228, 2261, 264, 8026, 293, 4412, 309, 311], "temperature": 0.0, "avg_logprob": -0.1490408420562744, "compression_ratio": 1.6544502617801047, "no_speech_prob": 2.1111956812092103e-05}, {"id": 1056, "seek": 533276, "start": 5347.52, "end": 5350.4800000000005, "text": " kind of the language which you can use to extend it.", "tokens": [733, 295, 264, 2856, 597, 291, 393, 764, 281, 10101, 309, 13], "temperature": 0.0, "avg_logprob": -0.1490408420562744, "compression_ratio": 1.6544502617801047, "no_speech_prob": 2.1111956812092103e-05}, {"id": 1057, "seek": 533276, "start": 5350.4800000000005, "end": 5356.4400000000005, "text": " Of course you can use TensorFlow as well, but we're using Theano in this course because", "tokens": [2720, 1164, 291, 393, 764, 37624, 382, 731, 11, 457, 321, 434, 1228, 440, 3730, 294, 341, 1164, 570], "temperature": 0.0, "avg_logprob": -0.1490408420562744, "compression_ratio": 1.6544502617801047, "no_speech_prob": 2.1111956812092103e-05}, {"id": 1058, "seek": 535644, "start": 5356.44, "end": 5363.5599999999995, "text": " I think it's much easier for this kind of application.", "tokens": [286, 519, 309, 311, 709, 3571, 337, 341, 733, 295, 3861, 13], "temperature": 0.0, "avg_logprob": -0.1380091957423998, "compression_ratio": 1.5721153846153846, "no_speech_prob": 7.411006208712934e-06}, {"id": 1059, "seek": 535644, "start": 5363.5599999999995, "end": 5366.759999999999, "text": " So let's learn to use Theano.", "tokens": [407, 718, 311, 1466, 281, 764, 440, 3730, 13], "temperature": 0.0, "avg_logprob": -0.1380091957423998, "compression_ratio": 1.5721153846153846, "no_speech_prob": 7.411006208712934e-06}, {"id": 1060, "seek": 535644, "start": 5366.759999999999, "end": 5373.44, "text": " In the process of doing it in Theano, we're going to have to force ourselves to think", "tokens": [682, 264, 1399, 295, 884, 309, 294, 440, 3730, 11, 321, 434, 516, 281, 362, 281, 3464, 4175, 281, 519], "temperature": 0.0, "avg_logprob": -0.1380091957423998, "compression_ratio": 1.5721153846153846, "no_speech_prob": 7.411006208712934e-06}, {"id": 1061, "seek": 535644, "start": 5373.44, "end": 5377.759999999999, "text": " through a lot more of the details than we have before.", "tokens": [807, 257, 688, 544, 295, 264, 4365, 813, 321, 362, 949, 13], "temperature": 0.0, "avg_logprob": -0.1380091957423998, "compression_ratio": 1.5721153846153846, "no_speech_prob": 7.411006208712934e-06}, {"id": 1062, "seek": 535644, "start": 5377.759999999999, "end": 5381.2, "text": " Because Theano doesn't have any of the conveniences that Keras has.", "tokens": [1436, 440, 3730, 1177, 380, 362, 604, 295, 264, 7158, 14004, 300, 591, 6985, 575, 13], "temperature": 0.0, "avg_logprob": -0.1380091957423998, "compression_ratio": 1.5721153846153846, "no_speech_prob": 7.411006208712934e-06}, {"id": 1063, "seek": 535644, "start": 5381.2, "end": 5384.28, "text": " There's no such thing as a layer.", "tokens": [821, 311, 572, 1270, 551, 382, 257, 4583, 13], "temperature": 0.0, "avg_logprob": -0.1380091957423998, "compression_ratio": 1.5721153846153846, "no_speech_prob": 7.411006208712934e-06}, {"id": 1064, "seek": 538428, "start": 5384.28, "end": 5389.4, "text": " We have to think about all of the weight matrices and activation functions and everything ourselves.", "tokens": [492, 362, 281, 519, 466, 439, 295, 264, 3364, 32284, 293, 24433, 6828, 293, 1203, 4175, 13], "temperature": 0.0, "avg_logprob": -0.1928412437438965, "compression_ratio": 1.75, "no_speech_prob": 3.023951649083756e-05}, {"id": 1065, "seek": 538428, "start": 5389.4, "end": 5392.92, "text": " So let me show you how it works.", "tokens": [407, 718, 385, 855, 291, 577, 309, 1985, 13], "temperature": 0.0, "avg_logprob": -0.1928412437438965, "compression_ratio": 1.75, "no_speech_prob": 3.023951649083756e-05}, {"id": 1066, "seek": 538428, "start": 5392.92, "end": 5398.679999999999, "text": " In Theano, there's this concept of a variable.", "tokens": [682, 440, 3730, 11, 456, 311, 341, 3410, 295, 257, 7006, 13], "temperature": 0.0, "avg_logprob": -0.1928412437438965, "compression_ratio": 1.75, "no_speech_prob": 3.023951649083756e-05}, {"id": 1067, "seek": 538428, "start": 5398.679999999999, "end": 5402.5599999999995, "text": " And a variable is something which we basically define like so.", "tokens": [400, 257, 7006, 307, 746, 597, 321, 1936, 6964, 411, 370, 13], "temperature": 0.0, "avg_logprob": -0.1928412437438965, "compression_ratio": 1.75, "no_speech_prob": 3.023951649083756e-05}, {"id": 1068, "seek": 538428, "start": 5402.5599999999995, "end": 5408.599999999999, "text": " We can say there is a variable which is a matrix which I will call tInput.", "tokens": [492, 393, 584, 456, 307, 257, 7006, 597, 307, 257, 8141, 597, 286, 486, 818, 256, 4575, 2582, 13], "temperature": 0.0, "avg_logprob": -0.1928412437438965, "compression_ratio": 1.75, "no_speech_prob": 3.023951649083756e-05}, {"id": 1069, "seek": 538428, "start": 5408.599999999999, "end": 5411.679999999999, "text": " And there is a variable which is a matrix that we'll call tOutput.", "tokens": [400, 456, 307, 257, 7006, 597, 307, 257, 8141, 300, 321, 603, 818, 256, 28353, 2582, 13], "temperature": 0.0, "avg_logprob": -0.1928412437438965, "compression_ratio": 1.75, "no_speech_prob": 3.023951649083756e-05}, {"id": 1070, "seek": 541168, "start": 5411.68, "end": 5416.240000000001, "text": " And there is a variable that is a vector that we will call h0.", "tokens": [400, 456, 307, 257, 7006, 300, 307, 257, 8062, 300, 321, 486, 818, 276, 15, 13], "temperature": 0.0, "avg_logprob": -0.15097458362579347, "compression_ratio": 1.6631016042780749, "no_speech_prob": 3.611957481552963e-06}, {"id": 1071, "seek": 541168, "start": 5416.240000000001, "end": 5425.320000000001, "text": " Now what these are all saying is that these are things that we will give values to later.", "tokens": [823, 437, 613, 366, 439, 1566, 307, 300, 613, 366, 721, 300, 321, 486, 976, 4190, 281, 1780, 13], "temperature": 0.0, "avg_logprob": -0.15097458362579347, "compression_ratio": 1.6631016042780749, "no_speech_prob": 3.611957481552963e-06}, {"id": 1072, "seek": 541168, "start": 5425.320000000001, "end": 5429.4800000000005, "text": " Programming in Theano is very different to programming in normal Python.", "tokens": [8338, 2810, 294, 440, 3730, 307, 588, 819, 281, 9410, 294, 2710, 15329, 13], "temperature": 0.0, "avg_logprob": -0.15097458362579347, "compression_ratio": 1.6631016042780749, "no_speech_prob": 3.611957481552963e-06}, {"id": 1073, "seek": 541168, "start": 5429.4800000000005, "end": 5436.68, "text": " The reason for this is Theano's job in life is to provide a way for you to describe a", "tokens": [440, 1778, 337, 341, 307, 440, 3730, 311, 1691, 294, 993, 307, 281, 2893, 257, 636, 337, 291, 281, 6786, 257], "temperature": 0.0, "avg_logprob": -0.15097458362579347, "compression_ratio": 1.6631016042780749, "no_speech_prob": 3.611957481552963e-06}, {"id": 1074, "seek": 543668, "start": 5436.68, "end": 5443.12, "text": " computation that you want to do, and then it's going to compile it for the GPU.", "tokens": [24903, 300, 291, 528, 281, 360, 11, 293, 550, 309, 311, 516, 281, 31413, 309, 337, 264, 18407, 13], "temperature": 0.0, "avg_logprob": -0.14727855074232904, "compression_ratio": 1.94, "no_speech_prob": 7.071849267958896e-06}, {"id": 1075, "seek": 543668, "start": 5443.12, "end": 5445.92, "text": " And then it's going to run it on the GPU.", "tokens": [400, 550, 309, 311, 516, 281, 1190, 309, 322, 264, 18407, 13], "temperature": 0.0, "avg_logprob": -0.14727855074232904, "compression_ratio": 1.94, "no_speech_prob": 7.071849267958896e-06}, {"id": 1076, "seek": 543668, "start": 5445.92, "end": 5449.0, "text": " So it's going to be a little more complex to work in Theano.", "tokens": [407, 309, 311, 516, 281, 312, 257, 707, 544, 3997, 281, 589, 294, 440, 3730, 13], "temperature": 0.0, "avg_logprob": -0.14727855074232904, "compression_ratio": 1.94, "no_speech_prob": 7.071849267958896e-06}, {"id": 1077, "seek": 543668, "start": 5449.0, "end": 5453.52, "text": " Because Theano isn't going to be something where we immediately say do this and then", "tokens": [1436, 440, 3730, 1943, 380, 516, 281, 312, 746, 689, 321, 4258, 584, 360, 341, 293, 550], "temperature": 0.0, "avg_logprob": -0.14727855074232904, "compression_ratio": 1.94, "no_speech_prob": 7.071849267958896e-06}, {"id": 1078, "seek": 543668, "start": 5453.52, "end": 5455.4400000000005, "text": " do this and then do this.", "tokens": [360, 341, 293, 550, 360, 341, 13], "temperature": 0.0, "avg_logprob": -0.14727855074232904, "compression_ratio": 1.94, "no_speech_prob": 7.071849267958896e-06}, {"id": 1079, "seek": 543668, "start": 5455.4400000000005, "end": 5458.200000000001, "text": " Instead we're going to build up what's called a computation graph.", "tokens": [7156, 321, 434, 516, 281, 1322, 493, 437, 311, 1219, 257, 24903, 4295, 13], "temperature": 0.0, "avg_logprob": -0.14727855074232904, "compression_ratio": 1.94, "no_speech_prob": 7.071849267958896e-06}, {"id": 1080, "seek": 543668, "start": 5458.200000000001, "end": 5460.04, "text": " It's going to be a series of steps.", "tokens": [467, 311, 516, 281, 312, 257, 2638, 295, 4439, 13], "temperature": 0.0, "avg_logprob": -0.14727855074232904, "compression_ratio": 1.94, "no_speech_prob": 7.071849267958896e-06}, {"id": 1081, "seek": 543668, "start": 5460.04, "end": 5465.860000000001, "text": " We're going to say in the future, I'm going to give you some data, and when I do, I want", "tokens": [492, 434, 516, 281, 584, 294, 264, 2027, 11, 286, 478, 516, 281, 976, 291, 512, 1412, 11, 293, 562, 286, 360, 11, 286, 528], "temperature": 0.0, "avg_logprob": -0.14727855074232904, "compression_ratio": 1.94, "no_speech_prob": 7.071849267958896e-06}, {"id": 1082, "seek": 546586, "start": 5465.86, "end": 5468.24, "text": " you to do these steps.", "tokens": [291, 281, 360, 613, 4439, 13], "temperature": 0.0, "avg_logprob": -0.11868819361147673, "compression_ratio": 2.0, "no_speech_prob": 8.80099196365336e-06}, {"id": 1083, "seek": 546586, "start": 5468.24, "end": 5473.679999999999, "text": " So rather than actually starting off by giving it data, we start off by just describing the", "tokens": [407, 2831, 813, 767, 2891, 766, 538, 2902, 309, 1412, 11, 321, 722, 766, 538, 445, 16141, 264], "temperature": 0.0, "avg_logprob": -0.11868819361147673, "compression_ratio": 2.0, "no_speech_prob": 8.80099196365336e-06}, {"id": 1084, "seek": 546586, "start": 5473.679999999999, "end": 5477.5599999999995, "text": " types of data that when we do give it data, we're going to give it.", "tokens": [3467, 295, 1412, 300, 562, 321, 360, 976, 309, 1412, 11, 321, 434, 516, 281, 976, 309, 13], "temperature": 0.0, "avg_logprob": -0.11868819361147673, "compression_ratio": 2.0, "no_speech_prob": 8.80099196365336e-06}, {"id": 1085, "seek": 546586, "start": 5477.5599999999995, "end": 5483.4, "text": " So eventually we're going to give it some input data, we're going to give it some output", "tokens": [407, 4728, 321, 434, 516, 281, 976, 309, 512, 4846, 1412, 11, 321, 434, 516, 281, 976, 309, 512, 5598], "temperature": 0.0, "avg_logprob": -0.11868819361147673, "compression_ratio": 2.0, "no_speech_prob": 8.80099196365336e-06}, {"id": 1086, "seek": 546586, "start": 5483.4, "end": 5489.5599999999995, "text": " data, and we're going to give it some way of initializing the first hidden state.", "tokens": [1412, 11, 293, 321, 434, 516, 281, 976, 309, 512, 636, 295, 5883, 3319, 264, 700, 7633, 1785, 13], "temperature": 0.0, "avg_logprob": -0.11868819361147673, "compression_ratio": 2.0, "no_speech_prob": 8.80099196365336e-06}, {"id": 1087, "seek": 546586, "start": 5489.5599999999995, "end": 5495.04, "text": " Oh, and also we'll give it a learning rate because we might want to change it later.", "tokens": [876, 11, 293, 611, 321, 603, 976, 309, 257, 2539, 3314, 570, 321, 1062, 528, 281, 1319, 309, 1780, 13], "temperature": 0.0, "avg_logprob": -0.11868819361147673, "compression_ratio": 2.0, "no_speech_prob": 8.80099196365336e-06}, {"id": 1088, "seek": 549504, "start": 5495.04, "end": 5496.04, "text": " So that's all these things do.", "tokens": [407, 300, 311, 439, 613, 721, 360, 13], "temperature": 0.0, "avg_logprob": -0.15998075704658982, "compression_ratio": 1.8837209302325582, "no_speech_prob": 9.81821085588308e-06}, {"id": 1089, "seek": 549504, "start": 5496.04, "end": 5499.44, "text": " They create Theano variables.", "tokens": [814, 1884, 440, 3730, 9102, 13], "temperature": 0.0, "avg_logprob": -0.15998075704658982, "compression_ratio": 1.8837209302325582, "no_speech_prob": 9.81821085588308e-06}, {"id": 1090, "seek": 549504, "start": 5499.44, "end": 5503.6, "text": " So then we can create a list of those, and this is all of the arguments that we're going", "tokens": [407, 550, 321, 393, 1884, 257, 1329, 295, 729, 11, 293, 341, 307, 439, 295, 264, 12869, 300, 321, 434, 516], "temperature": 0.0, "avg_logprob": -0.15998075704658982, "compression_ratio": 1.8837209302325582, "no_speech_prob": 9.81821085588308e-06}, {"id": 1091, "seek": 549504, "start": 5503.6, "end": 5506.0, "text": " to have to provide to Theano later on.", "tokens": [281, 362, 281, 2893, 281, 440, 3730, 1780, 322, 13], "temperature": 0.0, "avg_logprob": -0.15998075704658982, "compression_ratio": 1.8837209302325582, "no_speech_prob": 9.81821085588308e-06}, {"id": 1092, "seek": 549504, "start": 5506.0, "end": 5510.96, "text": " So there's no data here, nothing's being computed, we're just telling Theano that these things", "tokens": [407, 456, 311, 572, 1412, 510, 11, 1825, 311, 885, 40610, 11, 321, 434, 445, 3585, 440, 3730, 300, 613, 721], "temperature": 0.0, "avg_logprob": -0.15998075704658982, "compression_ratio": 1.8837209302325582, "no_speech_prob": 9.81821085588308e-06}, {"id": 1093, "seek": 549504, "start": 5510.96, "end": 5514.72, "text": " are going to be used in the future.", "tokens": [366, 516, 281, 312, 1143, 294, 264, 2027, 13], "temperature": 0.0, "avg_logprob": -0.15998075704658982, "compression_ratio": 1.8837209302325582, "no_speech_prob": 9.81821085588308e-06}, {"id": 1094, "seek": 549504, "start": 5514.72, "end": 5522.62, "text": " The next thing that we need to do is we're going to try to build this, is we're going", "tokens": [440, 958, 551, 300, 321, 643, 281, 360, 307, 321, 434, 516, 281, 853, 281, 1322, 341, 11, 307, 321, 434, 516], "temperature": 0.0, "avg_logprob": -0.15998075704658982, "compression_ratio": 1.8837209302325582, "no_speech_prob": 9.81821085588308e-06}, {"id": 1095, "seek": 552262, "start": 5522.62, "end": 5527.099999999999, "text": " to have to build all of the pieces in all of these layer operations.", "tokens": [281, 362, 281, 1322, 439, 295, 264, 3755, 294, 439, 295, 613, 4583, 7705, 13], "temperature": 0.0, "avg_logprob": -0.18061974708070147, "compression_ratio": 2.3353293413173652, "no_speech_prob": 1.2218876690894831e-05}, {"id": 1096, "seek": 552262, "start": 5527.099999999999, "end": 5531.76, "text": " So specifically we're going to have to create the weight vector and bias matrix for the", "tokens": [407, 4682, 321, 434, 516, 281, 362, 281, 1884, 264, 3364, 8062, 293, 12577, 8141, 337, 264], "temperature": 0.0, "avg_logprob": -0.18061974708070147, "compression_ratio": 2.3353293413173652, "no_speech_prob": 1.2218876690894831e-05}, {"id": 1097, "seek": 552262, "start": 5531.76, "end": 5539.96, "text": " orange arrow, the weight matrix and the bias vector for the green arrow, the weight matrix", "tokens": [7671, 11610, 11, 264, 3364, 8141, 293, 264, 12577, 8062, 337, 264, 3092, 11610, 11, 264, 3364, 8141], "temperature": 0.0, "avg_logprob": -0.18061974708070147, "compression_ratio": 2.3353293413173652, "no_speech_prob": 1.2218876690894831e-05}, {"id": 1098, "seek": 552262, "start": 5539.96, "end": 5543.599999999999, "text": " and the bias vector for the green arrow, and the weight matrix and the bias vector for", "tokens": [293, 264, 12577, 8062, 337, 264, 3092, 11610, 11, 293, 264, 3364, 8141, 293, 264, 12577, 8062, 337], "temperature": 0.0, "avg_logprob": -0.18061974708070147, "compression_ratio": 2.3353293413173652, "no_speech_prob": 1.2218876690894831e-05}, {"id": 1099, "seek": 552262, "start": 5543.599999999999, "end": 5544.599999999999, "text": " the blue arrow.", "tokens": [264, 3344, 11610, 13], "temperature": 0.0, "avg_logprob": -0.18061974708070147, "compression_ratio": 2.3353293413173652, "no_speech_prob": 1.2218876690894831e-05}, {"id": 1100, "seek": 552262, "start": 5544.599999999999, "end": 5546.8, "text": " That's what these layer operations are.", "tokens": [663, 311, 437, 613, 4583, 7705, 366, 13], "temperature": 0.0, "avg_logprob": -0.18061974708070147, "compression_ratio": 2.3353293413173652, "no_speech_prob": 1.2218876690894831e-05}, {"id": 1101, "seek": 554680, "start": 5546.8, "end": 5553.2, "text": " They're a matrix multiply followed by a nonlinear activation function.", "tokens": [814, 434, 257, 8141, 12972, 6263, 538, 257, 2107, 28263, 24433, 2445, 13], "temperature": 0.0, "avg_logprob": -0.19999802112579346, "compression_ratio": 1.8137254901960784, "no_speech_prob": 1.0348509249524795e-06}, {"id": 1102, "seek": 554680, "start": 5553.2, "end": 5556.12, "text": " So I've created some functions to do that.", "tokens": [407, 286, 600, 2942, 512, 6828, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.19999802112579346, "compression_ratio": 1.8137254901960784, "no_speech_prob": 1.0348509249524795e-06}, {"id": 1103, "seek": 554680, "start": 5556.12, "end": 5562.92, "text": " So WH is what I'm going to call the weights and bias to my hidden layer.", "tokens": [407, 8183, 307, 437, 286, 478, 516, 281, 818, 264, 17443, 293, 12577, 281, 452, 7633, 4583, 13], "temperature": 0.0, "avg_logprob": -0.19999802112579346, "compression_ratio": 1.8137254901960784, "no_speech_prob": 1.0348509249524795e-06}, {"id": 1104, "seek": 554680, "start": 5562.92, "end": 5569.28, "text": " WX will be my weights and bias to my input, and WY will be my weights and bias to my output.", "tokens": [343, 55, 486, 312, 452, 17443, 293, 12577, 281, 452, 4846, 11, 293, 46410, 486, 312, 452, 17443, 293, 12577, 281, 452, 5598, 13], "temperature": 0.0, "avg_logprob": -0.19999802112579346, "compression_ratio": 1.8137254901960784, "no_speech_prob": 1.0348509249524795e-06}, {"id": 1105, "seek": 554680, "start": 5569.28, "end": 5574.320000000001, "text": " And so to create them, I've created this little function called weights and bias, in which", "tokens": [400, 370, 281, 1884, 552, 11, 286, 600, 2942, 341, 707, 2445, 1219, 17443, 293, 12577, 11, 294, 597], "temperature": 0.0, "avg_logprob": -0.19999802112579346, "compression_ratio": 1.8137254901960784, "no_speech_prob": 1.0348509249524795e-06}, {"id": 1106, "seek": 557432, "start": 5574.32, "end": 5579.32, "text": " I tell it the size of the matrix that I want to create.", "tokens": [286, 980, 309, 264, 2744, 295, 264, 8141, 300, 286, 528, 281, 1884, 13], "temperature": 0.0, "avg_logprob": -0.13428306579589844, "compression_ratio": 1.891566265060241, "no_speech_prob": 1.3081752285870607e-06}, {"id": 1107, "seek": 557432, "start": 5579.32, "end": 5589.9, "text": " So the matrix that goes from input to hidden therefore has n input rows and n hidden columns.", "tokens": [407, 264, 8141, 300, 1709, 490, 4846, 281, 7633, 4412, 575, 297, 4846, 13241, 293, 297, 7633, 13766, 13], "temperature": 0.0, "avg_logprob": -0.13428306579589844, "compression_ratio": 1.891566265060241, "no_speech_prob": 1.3081752285870607e-06}, {"id": 1108, "seek": 557432, "start": 5589.9, "end": 5596.88, "text": " So weights and bias is here, and it's going to return a tuple, it's going to return our", "tokens": [407, 17443, 293, 12577, 307, 510, 11, 293, 309, 311, 516, 281, 2736, 257, 2604, 781, 11, 309, 311, 516, 281, 2736, 527], "temperature": 0.0, "avg_logprob": -0.13428306579589844, "compression_ratio": 1.891566265060241, "no_speech_prob": 1.3081752285870607e-06}, {"id": 1109, "seek": 557432, "start": 5596.88, "end": 5601.44, "text": " weights, and it's going to return our bias.", "tokens": [17443, 11, 293, 309, 311, 516, 281, 2736, 527, 12577, 13], "temperature": 0.0, "avg_logprob": -0.13428306579589844, "compression_ratio": 1.891566265060241, "no_speech_prob": 1.3081752285870607e-06}, {"id": 1110, "seek": 557432, "start": 5601.44, "end": 5603.679999999999, "text": " So how do we create the weights?", "tokens": [407, 577, 360, 321, 1884, 264, 17443, 30], "temperature": 0.0, "avg_logprob": -0.13428306579589844, "compression_ratio": 1.891566265060241, "no_speech_prob": 1.3081752285870607e-06}, {"id": 1111, "seek": 560368, "start": 5603.68, "end": 5609.6, "text": " To create the weights, we first of all calculate the magic Loro number, the square root of", "tokens": [1407, 1884, 264, 17443, 11, 321, 700, 295, 439, 8873, 264, 5585, 441, 10780, 1230, 11, 264, 3732, 5593, 295], "temperature": 0.0, "avg_logprob": -0.17755492140607135, "compression_ratio": 1.5634517766497462, "no_speech_prob": 5.4222055041464046e-06}, {"id": 1112, "seek": 560368, "start": 5609.6, "end": 5616.08, "text": " 2 over fan in, so that's the scale of the random numbers that we're going to use.", "tokens": [568, 670, 3429, 294, 11, 370, 300, 311, 264, 4373, 295, 264, 4974, 3547, 300, 321, 434, 516, 281, 764, 13], "temperature": 0.0, "avg_logprob": -0.17755492140607135, "compression_ratio": 1.5634517766497462, "no_speech_prob": 5.4222055041464046e-06}, {"id": 1113, "seek": 560368, "start": 5616.08, "end": 5624.400000000001, "text": " We then create those random numbers using the NumPyNormalRandomNumber function.", "tokens": [492, 550, 1884, 729, 4974, 3547, 1228, 264, 22592, 47, 88, 45, 24440, 49, 4606, 45, 4182, 2445, 13], "temperature": 0.0, "avg_logprob": -0.17755492140607135, "compression_ratio": 1.5634517766497462, "no_speech_prob": 5.4222055041464046e-06}, {"id": 1114, "seek": 560368, "start": 5624.400000000001, "end": 5629.04, "text": " And then we use a special Theano keyword called shared.", "tokens": [400, 550, 321, 764, 257, 2121, 440, 3730, 20428, 1219, 5507, 13], "temperature": 0.0, "avg_logprob": -0.17755492140607135, "compression_ratio": 1.5634517766497462, "no_speech_prob": 5.4222055041464046e-06}, {"id": 1115, "seek": 562904, "start": 5629.04, "end": 5636.6, "text": " And what shared does is it says to Theano, this data is something that I'm going to want", "tokens": [400, 437, 5507, 775, 307, 309, 1619, 281, 440, 3730, 11, 341, 1412, 307, 746, 300, 286, 478, 516, 281, 528], "temperature": 0.0, "avg_logprob": -0.09230123956998189, "compression_ratio": 1.681159420289855, "no_speech_prob": 8.530285413144156e-06}, {"id": 1116, "seek": 562904, "start": 5636.6, "end": 5640.88, "text": " you to pass off to the GPU later and keep track of.", "tokens": [291, 281, 1320, 766, 281, 264, 18407, 1780, 293, 1066, 2837, 295, 13], "temperature": 0.0, "avg_logprob": -0.09230123956998189, "compression_ratio": 1.681159420289855, "no_speech_prob": 8.530285413144156e-06}, {"id": 1117, "seek": 562904, "start": 5640.88, "end": 5646.04, "text": " So as soon as you wrap something in shared, it kind of belongs to Theano now.", "tokens": [407, 382, 2321, 382, 291, 7019, 746, 294, 5507, 11, 309, 733, 295, 12953, 281, 440, 3730, 586, 13], "temperature": 0.0, "avg_logprob": -0.09230123956998189, "compression_ratio": 1.681159420289855, "no_speech_prob": 8.530285413144156e-06}, {"id": 1118, "seek": 562904, "start": 5646.04, "end": 5650.32, "text": " So here is a weight matrix that belongs to Theano.", "tokens": [407, 510, 307, 257, 3364, 8141, 300, 12953, 281, 440, 3730, 13], "temperature": 0.0, "avg_logprob": -0.09230123956998189, "compression_ratio": 1.681159420289855, "no_speech_prob": 8.530285413144156e-06}, {"id": 1119, "seek": 562904, "start": 5650.32, "end": 5656.86, "text": " Here is a vector of zeros that belongs to Theano, and that's our initial bias.", "tokens": [1692, 307, 257, 8062, 295, 35193, 300, 12953, 281, 440, 3730, 11, 293, 300, 311, 527, 5883, 12577, 13], "temperature": 0.0, "avg_logprob": -0.09230123956998189, "compression_ratio": 1.681159420289855, "no_speech_prob": 8.530285413144156e-06}, {"id": 1120, "seek": 565686, "start": 5656.86, "end": 5663.599999999999, "text": " So we've initialized our weights and our bias, so we can do that for our inputs and we can", "tokens": [407, 321, 600, 5883, 1602, 527, 17443, 293, 527, 12577, 11, 370, 321, 393, 360, 300, 337, 527, 15743, 293, 321, 393], "temperature": 0.0, "avg_logprob": -0.15158565477891403, "compression_ratio": 1.6839378238341969, "no_speech_prob": 6.748020496161189e-06}, {"id": 1121, "seek": 565686, "start": 5663.599999999999, "end": 5666.12, "text": " do that for our outputs.", "tokens": [360, 300, 337, 527, 23930, 13], "temperature": 0.0, "avg_logprob": -0.15158565477891403, "compression_ratio": 1.6839378238341969, "no_speech_prob": 6.748020496161189e-06}, {"id": 1122, "seek": 565686, "start": 5666.12, "end": 5672.5199999999995, "text": " And then for our hidden, which is the orange arrow, we're going to do something slightly", "tokens": [400, 550, 337, 527, 7633, 11, 597, 307, 264, 7671, 11610, 11, 321, 434, 516, 281, 360, 746, 4748], "temperature": 0.0, "avg_logprob": -0.15158565477891403, "compression_ratio": 1.6839378238341969, "no_speech_prob": 6.748020496161189e-06}, {"id": 1123, "seek": 565686, "start": 5672.5199999999995, "end": 5677.4, "text": " different, which is we will initialize it using an identity matrix.", "tokens": [819, 11, 597, 307, 321, 486, 5883, 1125, 309, 1228, 364, 6575, 8141, 13], "temperature": 0.0, "avg_logprob": -0.15158565477891403, "compression_ratio": 1.6839378238341969, "no_speech_prob": 6.748020496161189e-06}, {"id": 1124, "seek": 565686, "start": 5677.4, "end": 5683.12, "text": " And rather amusingly in NumPy, it is I for identity.", "tokens": [400, 2831, 47809, 356, 294, 22592, 47, 88, 11, 309, 307, 286, 337, 6575, 13], "temperature": 0.0, "avg_logprob": -0.15158565477891403, "compression_ratio": 1.6839378238341969, "no_speech_prob": 6.748020496161189e-06}, {"id": 1125, "seek": 568312, "start": 5683.12, "end": 5689.599999999999, "text": " So this is an identity matrix, believe it or not, of size n by n.", "tokens": [407, 341, 307, 364, 6575, 8141, 11, 1697, 309, 420, 406, 11, 295, 2744, 297, 538, 297, 13], "temperature": 0.0, "avg_logprob": -0.20314258999294704, "compression_ratio": 1.4802259887005649, "no_speech_prob": 1.7061786365957232e-06}, {"id": 1126, "seek": 568312, "start": 5689.599999999999, "end": 5695.04, "text": " And so that's our initial weights, and our initial bias is exactly as plot.", "tokens": [400, 370, 300, 311, 527, 5883, 17443, 11, 293, 527, 5883, 12577, 307, 2293, 382, 7542, 13], "temperature": 0.0, "avg_logprob": -0.20314258999294704, "compression_ratio": 1.4802259887005649, "no_speech_prob": 1.7061786365957232e-06}, {"id": 1127, "seek": 568312, "start": 5695.04, "end": 5698.36, "text": " It's a vector of zeros.", "tokens": [467, 311, 257, 8062, 295, 35193, 13], "temperature": 0.0, "avg_logprob": -0.20314258999294704, "compression_ratio": 1.4802259887005649, "no_speech_prob": 1.7061786365957232e-06}, {"id": 1128, "seek": 568312, "start": 5698.36, "end": 5708.24, "text": " So you can see we've had to manually construct each of these 3 weight matrices and bias vectors.", "tokens": [407, 291, 393, 536, 321, 600, 632, 281, 16945, 7690, 1184, 295, 613, 805, 3364, 32284, 293, 12577, 18875, 13], "temperature": 0.0, "avg_logprob": -0.20314258999294704, "compression_ratio": 1.4802259887005649, "no_speech_prob": 1.7061786365957232e-06}, {"id": 1129, "seek": 570824, "start": 5708.24, "end": 5713.16, "text": " It's nice to now stick them all into a single list, and Python has this thing called chain", "tokens": [467, 311, 1481, 281, 586, 2897, 552, 439, 666, 257, 2167, 1329, 11, 293, 15329, 575, 341, 551, 1219, 5021], "temperature": 0.0, "avg_logprob": -0.1517132641224379, "compression_ratio": 1.6604651162790698, "no_speech_prob": 4.092893959750654e-06}, {"id": 1130, "seek": 570824, "start": 5713.16, "end": 5717.96, "text": " from iterable, which basically takes all of these tuples and dumps them all together into", "tokens": [490, 17138, 712, 11, 597, 1936, 2516, 439, 295, 613, 2604, 2622, 293, 11430, 82, 552, 439, 1214, 666], "temperature": 0.0, "avg_logprob": -0.1517132641224379, "compression_ratio": 1.6604651162790698, "no_speech_prob": 4.092893959750654e-06}, {"id": 1131, "seek": 570824, "start": 5717.96, "end": 5719.44, "text": " a single list.", "tokens": [257, 2167, 1329, 13], "temperature": 0.0, "avg_logprob": -0.1517132641224379, "compression_ratio": 1.6604651162790698, "no_speech_prob": 4.092893959750654e-06}, {"id": 1132, "seek": 570824, "start": 5719.44, "end": 5728.5199999999995, "text": " So this now has all 6 weight matrices and bias vectors in a single list.", "tokens": [407, 341, 586, 575, 439, 1386, 3364, 32284, 293, 12577, 18875, 294, 257, 2167, 1329, 13], "temperature": 0.0, "avg_logprob": -0.1517132641224379, "compression_ratio": 1.6604651162790698, "no_speech_prob": 4.092893959750654e-06}, {"id": 1133, "seek": 570824, "start": 5728.5199999999995, "end": 5737.08, "text": " Now we have defined the initial contents of each of these arrows, and we've also defined", "tokens": [823, 321, 362, 7642, 264, 5883, 15768, 295, 1184, 295, 613, 19669, 11, 293, 321, 600, 611, 7642], "temperature": 0.0, "avg_logprob": -0.1517132641224379, "compression_ratio": 1.6604651162790698, "no_speech_prob": 4.092893959750654e-06}, {"id": 1134, "seek": 573708, "start": 5737.08, "end": 5742.5199999999995, "text": " kind of symbolically the concept that we're going to have something to initialize it with", "tokens": [733, 295, 5986, 984, 264, 3410, 300, 321, 434, 516, 281, 362, 746, 281, 5883, 1125, 309, 365], "temperature": 0.0, "avg_logprob": -0.08898945604817252, "compression_ratio": 1.7272727272727273, "no_speech_prob": 4.860422905039741e-06}, {"id": 1135, "seek": 573708, "start": 5742.5199999999995, "end": 5748.12, "text": " here, something to initialize it with here, and some target to initialize it with here.", "tokens": [510, 11, 746, 281, 5883, 1125, 309, 365, 510, 11, 293, 512, 3779, 281, 5883, 1125, 309, 365, 510, 13], "temperature": 0.0, "avg_logprob": -0.08898945604817252, "compression_ratio": 1.7272727272727273, "no_speech_prob": 4.860422905039741e-06}, {"id": 1136, "seek": 573708, "start": 5748.12, "end": 5755.2, "text": " So the next thing we have to do is to tell Theano what happens each time we take a single", "tokens": [407, 264, 958, 551, 321, 362, 281, 360, 307, 281, 980, 440, 3730, 437, 2314, 1184, 565, 321, 747, 257, 2167], "temperature": 0.0, "avg_logprob": -0.08898945604817252, "compression_ratio": 1.7272727272727273, "no_speech_prob": 4.860422905039741e-06}, {"id": 1137, "seek": 573708, "start": 5755.2, "end": 5760.72, "text": " step of this RNN.", "tokens": [1823, 295, 341, 45702, 45, 13], "temperature": 0.0, "avg_logprob": -0.08898945604817252, "compression_ratio": 1.7272727272727273, "no_speech_prob": 4.860422905039741e-06}, {"id": 1138, "seek": 573708, "start": 5760.72, "end": 5764.8, "text": " On the GPU, you can't use a for loop.", "tokens": [1282, 264, 18407, 11, 291, 393, 380, 764, 257, 337, 6367, 13], "temperature": 0.0, "avg_logprob": -0.08898945604817252, "compression_ratio": 1.7272727272727273, "no_speech_prob": 4.860422905039741e-06}, {"id": 1139, "seek": 576480, "start": 5764.8, "end": 5769.4400000000005, "text": " The reason you can't use a for loop is because the GPU wants to be able to parallelize things", "tokens": [440, 1778, 291, 393, 380, 764, 257, 337, 6367, 307, 570, 264, 18407, 2738, 281, 312, 1075, 281, 8952, 1125, 721], "temperature": 0.0, "avg_logprob": -0.15712768800797, "compression_ratio": 1.803921568627451, "no_speech_prob": 5.77189848627313e-06}, {"id": 1140, "seek": 576480, "start": 5769.4400000000005, "end": 5771.84, "text": " and wants to do things at the same time.", "tokens": [293, 2738, 281, 360, 721, 412, 264, 912, 565, 13], "temperature": 0.0, "avg_logprob": -0.15712768800797, "compression_ratio": 1.803921568627451, "no_speech_prob": 5.77189848627313e-06}, {"id": 1141, "seek": 576480, "start": 5771.84, "end": 5775.76, "text": " And a for loop by definition can't do the second part of the loop until it's done the", "tokens": [400, 257, 337, 6367, 538, 7123, 393, 380, 360, 264, 1150, 644, 295, 264, 6367, 1826, 309, 311, 1096, 264], "temperature": 0.0, "avg_logprob": -0.15712768800797, "compression_ratio": 1.803921568627451, "no_speech_prob": 5.77189848627313e-06}, {"id": 1142, "seek": 576480, "start": 5775.76, "end": 5777.8, "text": " first part of the loop.", "tokens": [700, 644, 295, 264, 6367, 13], "temperature": 0.0, "avg_logprob": -0.15712768800797, "compression_ratio": 1.803921568627451, "no_speech_prob": 5.77189848627313e-06}, {"id": 1143, "seek": 576480, "start": 5777.8, "end": 5782.68, "text": " I don't know if we'll get time to do it in this course or not, but there's a very neat", "tokens": [286, 500, 380, 458, 498, 321, 603, 483, 565, 281, 360, 309, 294, 341, 1164, 420, 406, 11, 457, 456, 311, 257, 588, 10654], "temperature": 0.0, "avg_logprob": -0.15712768800797, "compression_ratio": 1.803921568627451, "no_speech_prob": 5.77189848627313e-06}, {"id": 1144, "seek": 576480, "start": 5782.68, "end": 5788.360000000001, "text": " result which shows that there's something very similar to a for loop that you can parallelize,", "tokens": [1874, 597, 3110, 300, 456, 311, 746, 588, 2531, 281, 257, 337, 6367, 300, 291, 393, 8952, 1125, 11], "temperature": 0.0, "avg_logprob": -0.15712768800797, "compression_ratio": 1.803921568627451, "no_speech_prob": 5.77189848627313e-06}, {"id": 1145, "seek": 576480, "start": 5788.360000000001, "end": 5791.72, "text": " and it's called a scan operation.", "tokens": [293, 309, 311, 1219, 257, 11049, 6916, 13], "temperature": 0.0, "avg_logprob": -0.15712768800797, "compression_ratio": 1.803921568627451, "no_speech_prob": 5.77189848627313e-06}, {"id": 1146, "seek": 579172, "start": 5791.72, "end": 5797.400000000001, "text": " A scan operation is something that's defined in a very particular way.", "tokens": [316, 11049, 6916, 307, 746, 300, 311, 7642, 294, 257, 588, 1729, 636, 13], "temperature": 0.0, "avg_logprob": -0.11149112994854267, "compression_ratio": 1.7875647668393781, "no_speech_prob": 1.2679251994995866e-06}, {"id": 1147, "seek": 579172, "start": 5797.400000000001, "end": 5808.08, "text": " A scan operation is something where you call some function for every element of some sequence.", "tokens": [316, 11049, 6916, 307, 746, 689, 291, 818, 512, 2445, 337, 633, 4478, 295, 512, 8310, 13], "temperature": 0.0, "avg_logprob": -0.11149112994854267, "compression_ratio": 1.7875647668393781, "no_speech_prob": 1.2679251994995866e-06}, {"id": 1148, "seek": 579172, "start": 5808.08, "end": 5815.360000000001, "text": " And at every point, the function returns some output, and the next time through that function", "tokens": [400, 412, 633, 935, 11, 264, 2445, 11247, 512, 5598, 11, 293, 264, 958, 565, 807, 300, 2445], "temperature": 0.0, "avg_logprob": -0.11149112994854267, "compression_ratio": 1.7875647668393781, "no_speech_prob": 1.2679251994995866e-06}, {"id": 1149, "seek": 579172, "start": 5815.360000000001, "end": 5820.88, "text": " is called, it's going to get the output of the previous time you called it along with", "tokens": [307, 1219, 11, 309, 311, 516, 281, 483, 264, 5598, 295, 264, 3894, 565, 291, 1219, 309, 2051, 365], "temperature": 0.0, "avg_logprob": -0.11149112994854267, "compression_ratio": 1.7875647668393781, "no_speech_prob": 1.2679251994995866e-06}, {"id": 1150, "seek": 582088, "start": 5820.88, "end": 5823.64, "text": " the next element of the sequence.", "tokens": [264, 958, 4478, 295, 264, 8310, 13], "temperature": 0.0, "avg_logprob": -0.2249665115818833, "compression_ratio": 1.5571428571428572, "no_speech_prob": 7.183228717622114e-06}, {"id": 1151, "seek": 582088, "start": 5823.64, "end": 5830.24, "text": " So in fact, I've got an example of it.", "tokens": [407, 294, 1186, 11, 286, 600, 658, 364, 1365, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.2249665115818833, "compression_ratio": 1.5571428571428572, "no_speech_prob": 7.183228717622114e-06}, {"id": 1152, "seek": 582088, "start": 5830.24, "end": 5837.96, "text": " I actually wrote a very simple example of it in Python.", "tokens": [286, 767, 4114, 257, 588, 2199, 1365, 295, 309, 294, 15329, 13], "temperature": 0.0, "avg_logprob": -0.2249665115818833, "compression_ratio": 1.5571428571428572, "no_speech_prob": 7.183228717622114e-06}, {"id": 1153, "seek": 582088, "start": 5837.96, "end": 5840.4400000000005, "text": " Here is the definition of scan.", "tokens": [1692, 307, 264, 7123, 295, 11049, 13], "temperature": 0.0, "avg_logprob": -0.2249665115818833, "compression_ratio": 1.5571428571428572, "no_speech_prob": 7.183228717622114e-06}, {"id": 1154, "seek": 582088, "start": 5840.4400000000005, "end": 5841.84, "text": " Here is an example of scan.", "tokens": [1692, 307, 364, 1365, 295, 11049, 13], "temperature": 0.0, "avg_logprob": -0.2249665115818833, "compression_ratio": 1.5571428571428572, "no_speech_prob": 7.183228717622114e-06}, {"id": 1155, "seek": 582088, "start": 5841.84, "end": 5844.04, "text": " Let's start with the example.", "tokens": [961, 311, 722, 365, 264, 1365, 13], "temperature": 0.0, "avg_logprob": -0.2249665115818833, "compression_ratio": 1.5571428571428572, "no_speech_prob": 7.183228717622114e-06}, {"id": 1156, "seek": 584404, "start": 5844.04, "end": 5851.86, "text": " I want to do a scan and the function I'm going to use is to add two things together.", "tokens": [286, 528, 281, 360, 257, 11049, 293, 264, 2445, 286, 478, 516, 281, 764, 307, 281, 909, 732, 721, 1214, 13], "temperature": 0.0, "avg_logprob": -0.14088743704336662, "compression_ratio": 1.7731481481481481, "no_speech_prob": 4.8604360927129164e-06}, {"id": 1157, "seek": 584404, "start": 5851.86, "end": 5856.96, "text": " And I'm going to start off with the number 0, and then I'm going to pass in a range of", "tokens": [400, 286, 478, 516, 281, 722, 766, 365, 264, 1230, 1958, 11, 293, 550, 286, 478, 516, 281, 1320, 294, 257, 3613, 295], "temperature": 0.0, "avg_logprob": -0.14088743704336662, "compression_ratio": 1.7731481481481481, "no_speech_prob": 4.8604360927129164e-06}, {"id": 1158, "seek": 584404, "start": 5856.96, "end": 5859.64, "text": " numbers from 0 to 4.", "tokens": [3547, 490, 1958, 281, 1017, 13], "temperature": 0.0, "avg_logprob": -0.14088743704336662, "compression_ratio": 1.7731481481481481, "no_speech_prob": 4.8604360927129164e-06}, {"id": 1159, "seek": 584404, "start": 5859.64, "end": 5865.04, "text": " So what scan does is it starts out by taking the first time through, it's going to call", "tokens": [407, 437, 11049, 775, 307, 309, 3719, 484, 538, 1940, 264, 700, 565, 807, 11, 309, 311, 516, 281, 818], "temperature": 0.0, "avg_logprob": -0.14088743704336662, "compression_ratio": 1.7731481481481481, "no_speech_prob": 4.8604360927129164e-06}, {"id": 1160, "seek": 584404, "start": 5865.04, "end": 5870.04, "text": " this function with that argument and the first element of this.", "tokens": [341, 2445, 365, 300, 6770, 293, 264, 700, 4478, 295, 341, 13], "temperature": 0.0, "avg_logprob": -0.14088743704336662, "compression_ratio": 1.7731481481481481, "no_speech_prob": 4.8604360927129164e-06}, {"id": 1161, "seek": 584404, "start": 5870.04, "end": 5873.56, "text": " So it's going to be 0 plus 0 equals 0.", "tokens": [407, 309, 311, 516, 281, 312, 1958, 1804, 1958, 6915, 1958, 13], "temperature": 0.0, "avg_logprob": -0.14088743704336662, "compression_ratio": 1.7731481481481481, "no_speech_prob": 4.8604360927129164e-06}, {"id": 1162, "seek": 587356, "start": 5873.56, "end": 5880.400000000001, "text": " The second time, it's going to call this function with the second element of this, along with", "tokens": [440, 1150, 565, 11, 309, 311, 516, 281, 818, 341, 2445, 365, 264, 1150, 4478, 295, 341, 11, 2051, 365], "temperature": 0.0, "avg_logprob": -0.12763403762470593, "compression_ratio": 2.037974683544304, "no_speech_prob": 3.2887426186789526e-06}, {"id": 1163, "seek": 587356, "start": 5880.400000000001, "end": 5882.88, "text": " the result of the previous call.", "tokens": [264, 1874, 295, 264, 3894, 818, 13], "temperature": 0.0, "avg_logprob": -0.12763403762470593, "compression_ratio": 2.037974683544304, "no_speech_prob": 3.2887426186789526e-06}, {"id": 1164, "seek": 587356, "start": 5882.88, "end": 5886.64, "text": " So it will be 0 plus 1 equals 1.", "tokens": [407, 309, 486, 312, 1958, 1804, 502, 6915, 502, 13], "temperature": 0.0, "avg_logprob": -0.12763403762470593, "compression_ratio": 2.037974683544304, "no_speech_prob": 3.2887426186789526e-06}, {"id": 1165, "seek": 587356, "start": 5886.64, "end": 5891.780000000001, "text": " The next time through, it's going to call this function with the result of the previous", "tokens": [440, 958, 565, 807, 11, 309, 311, 516, 281, 818, 341, 2445, 365, 264, 1874, 295, 264, 3894], "temperature": 0.0, "avg_logprob": -0.12763403762470593, "compression_ratio": 2.037974683544304, "no_speech_prob": 3.2887426186789526e-06}, {"id": 1166, "seek": 587356, "start": 5891.780000000001, "end": 5894.700000000001, "text": " call plus the next element of this range.", "tokens": [818, 1804, 264, 958, 4478, 295, 341, 3613, 13], "temperature": 0.0, "avg_logprob": -0.12763403762470593, "compression_ratio": 2.037974683544304, "no_speech_prob": 3.2887426186789526e-06}, {"id": 1167, "seek": 587356, "start": 5894.700000000001, "end": 5898.68, "text": " So it will be 1 plus 2 equals 3.", "tokens": [407, 309, 486, 312, 502, 1804, 568, 6915, 805, 13], "temperature": 0.0, "avg_logprob": -0.12763403762470593, "compression_ratio": 2.037974683544304, "no_speech_prob": 3.2887426186789526e-06}, {"id": 1168, "seek": 589868, "start": 5898.68, "end": 5905.4400000000005, "text": " So you can see here this scan operation defines a cumulative sum.", "tokens": [407, 291, 393, 536, 510, 341, 11049, 6916, 23122, 257, 38379, 2408, 13], "temperature": 0.0, "avg_logprob": -0.2041271911270317, "compression_ratio": 1.6142857142857143, "no_speech_prob": 2.058044174191309e-06}, {"id": 1169, "seek": 589868, "start": 5905.4400000000005, "end": 5908.240000000001, "text": " So you can see the definition of scan here.", "tokens": [407, 291, 393, 536, 264, 7123, 295, 11049, 510, 13], "temperature": 0.0, "avg_logprob": -0.2041271911270317, "compression_ratio": 1.6142857142857143, "no_speech_prob": 2.058044174191309e-06}, {"id": 1170, "seek": 589868, "start": 5908.240000000001, "end": 5911.76, "text": " We're going to be returning an array of results.", "tokens": [492, 434, 516, 281, 312, 12678, 364, 10225, 295, 3542, 13], "temperature": 0.0, "avg_logprob": -0.2041271911270317, "compression_ratio": 1.6142857142857143, "no_speech_prob": 2.058044174191309e-06}, {"id": 1171, "seek": 589868, "start": 5911.76, "end": 5918.12, "text": " Initially we take our starting point, 0, and that's our initial value for the previous", "tokens": [29446, 321, 747, 527, 2891, 935, 11, 1958, 11, 293, 300, 311, 527, 5883, 2158, 337, 264, 3894], "temperature": 0.0, "avg_logprob": -0.2041271911270317, "compression_ratio": 1.6142857142857143, "no_speech_prob": 2.058044174191309e-06}, {"id": 1172, "seek": 589868, "start": 5918.12, "end": 5920.240000000001, "text": " answer from scan.", "tokens": [1867, 490, 11049, 13], "temperature": 0.0, "avg_logprob": -0.2041271911270317, "compression_ratio": 1.6142857142857143, "no_speech_prob": 2.058044174191309e-06}, {"id": 1173, "seek": 589868, "start": 5920.240000000001, "end": 5924.8, "text": " And then we're going to go through everything in the sequence, 0 through 4.", "tokens": [400, 550, 321, 434, 516, 281, 352, 807, 1203, 294, 264, 8310, 11, 1958, 807, 1017, 13], "temperature": 0.0, "avg_logprob": -0.2041271911270317, "compression_ratio": 1.6142857142857143, "no_speech_prob": 2.058044174191309e-06}, {"id": 1174, "seek": 592480, "start": 5924.8, "end": 5929.12, "text": " We're going to apply this function, which in this case was addThingsUp, and we're going", "tokens": [492, 434, 516, 281, 3079, 341, 2445, 11, 597, 294, 341, 1389, 390, 909, 51, 571, 82, 22164, 11, 293, 321, 434, 516], "temperature": 0.0, "avg_logprob": -0.14867215156555175, "compression_ratio": 1.8571428571428572, "no_speech_prob": 1.1911065485037398e-06}, {"id": 1175, "seek": 592480, "start": 5929.12, "end": 5935.52, "text": " to apply it to the previous result along with the next element of the sequence.", "tokens": [281, 3079, 309, 281, 264, 3894, 1874, 2051, 365, 264, 958, 4478, 295, 264, 8310, 13], "temperature": 0.0, "avg_logprob": -0.14867215156555175, "compression_ratio": 1.8571428571428572, "no_speech_prob": 1.1911065485037398e-06}, {"id": 1176, "seek": 592480, "start": 5935.52, "end": 5941.6, "text": " Stick the result at the end of our list, set the previous result to whatever we just got,", "tokens": [22744, 264, 1874, 412, 264, 917, 295, 527, 1329, 11, 992, 264, 3894, 1874, 281, 2035, 321, 445, 658, 11], "temperature": 0.0, "avg_logprob": -0.14867215156555175, "compression_ratio": 1.8571428571428572, "no_speech_prob": 1.1911065485037398e-06}, {"id": 1177, "seek": 592480, "start": 5941.6, "end": 5944.88, "text": " and then go back to the next element of the sequence.", "tokens": [293, 550, 352, 646, 281, 264, 958, 4478, 295, 264, 8310, 13], "temperature": 0.0, "avg_logprob": -0.14867215156555175, "compression_ratio": 1.8571428571428572, "no_speech_prob": 1.1911065485037398e-06}, {"id": 1178, "seek": 592480, "start": 5944.88, "end": 5950.4400000000005, "text": " So it may be very surprising, hopefully it is very surprising because it's an extraordinary", "tokens": [407, 309, 815, 312, 588, 8830, 11, 4696, 309, 307, 588, 8830, 570, 309, 311, 364, 10581], "temperature": 0.0, "avg_logprob": -0.14867215156555175, "compression_ratio": 1.8571428571428572, "no_speech_prob": 1.1911065485037398e-06}, {"id": 1179, "seek": 595044, "start": 5950.44, "end": 5956.719999999999, "text": " work, but it is possible to write a parallel version of this.", "tokens": [589, 11, 457, 309, 307, 1944, 281, 2464, 257, 8952, 3037, 295, 341, 13], "temperature": 0.0, "avg_logprob": -0.12735242207845052, "compression_ratio": 1.5113636363636365, "no_speech_prob": 5.093683284940198e-06}, {"id": 1180, "seek": 595044, "start": 5956.719999999999, "end": 5962.759999999999, "text": " So if you can turn your algorithm into a scan, you can run it quickly on GPU.", "tokens": [407, 498, 291, 393, 1261, 428, 9284, 666, 257, 11049, 11, 291, 393, 1190, 309, 2661, 322, 18407, 13], "temperature": 0.0, "avg_logprob": -0.12735242207845052, "compression_ratio": 1.5113636363636365, "no_speech_prob": 5.093683284940198e-06}, {"id": 1181, "seek": 595044, "start": 5962.759999999999, "end": 5971.5199999999995, "text": " So what we're going to do is our job is to turn this RNN into something that we can put", "tokens": [407, 437, 321, 434, 516, 281, 360, 307, 527, 1691, 307, 281, 1261, 341, 45702, 45, 666, 746, 300, 321, 393, 829], "temperature": 0.0, "avg_logprob": -0.12735242207845052, "compression_ratio": 1.5113636363636365, "no_speech_prob": 5.093683284940198e-06}, {"id": 1182, "seek": 595044, "start": 5971.5199999999995, "end": 5975.5199999999995, "text": " into this kind of format, into a scan.", "tokens": [666, 341, 733, 295, 7877, 11, 666, 257, 11049, 13], "temperature": 0.0, "avg_logprob": -0.12735242207845052, "compression_ratio": 1.5113636363636365, "no_speech_prob": 5.093683284940198e-06}, {"id": 1183, "seek": 597552, "start": 5975.52, "end": 5985.68, "text": " So let's do that.", "tokens": [407, 718, 311, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.15145430198082557, "compression_ratio": 1.7674418604651163, "no_speech_prob": 4.222818006383022e-06}, {"id": 1184, "seek": 597552, "start": 5985.68, "end": 5992.56, "text": " So the function that we're going to call on each step through is the function called step.", "tokens": [407, 264, 2445, 300, 321, 434, 516, 281, 818, 322, 1184, 1823, 807, 307, 264, 2445, 1219, 1823, 13], "temperature": 0.0, "avg_logprob": -0.15145430198082557, "compression_ratio": 1.7674418604651163, "no_speech_prob": 4.222818006383022e-06}, {"id": 1185, "seek": 597552, "start": 5992.56, "end": 5996.64, "text": " And the function called step is going to be something which hopefully will not be very", "tokens": [400, 264, 2445, 1219, 1823, 307, 516, 281, 312, 746, 597, 4696, 486, 406, 312, 588], "temperature": 0.0, "avg_logprob": -0.15145430198082557, "compression_ratio": 1.7674418604651163, "no_speech_prob": 4.222818006383022e-06}, {"id": 1186, "seek": 597552, "start": 5996.64, "end": 5997.64, "text": " surprising to you.", "tokens": [8830, 281, 291, 13], "temperature": 0.0, "avg_logprob": -0.15145430198082557, "compression_ratio": 1.7674418604651163, "no_speech_prob": 4.222818006383022e-06}, {"id": 1187, "seek": 597552, "start": 5997.64, "end": 6003.160000000001, "text": " It's going to be something which takes our input, X, it does a dot product by that weight", "tokens": [467, 311, 516, 281, 312, 746, 597, 2516, 527, 4846, 11, 1783, 11, 309, 775, 257, 5893, 1674, 538, 300, 3364], "temperature": 0.0, "avg_logprob": -0.15145430198082557, "compression_ratio": 1.7674418604651163, "no_speech_prob": 4.222818006383022e-06}, {"id": 1188, "seek": 600316, "start": 6003.16, "end": 6010.44, "text": " matrix we created earlier, WX, and adds on that bias vector we created earlier.", "tokens": [8141, 321, 2942, 3071, 11, 343, 55, 11, 293, 10860, 322, 300, 12577, 8062, 321, 2942, 3071, 13], "temperature": 0.0, "avg_logprob": -0.1990900473161177, "compression_ratio": 1.7524271844660195, "no_speech_prob": 9.516163117950782e-06}, {"id": 1189, "seek": 600316, "start": 6010.44, "end": 6015.28, "text": " And then we do the same thing, taking our previous hidden state, multiplying it by the", "tokens": [400, 550, 321, 360, 264, 912, 551, 11, 1940, 527, 3894, 7633, 1785, 11, 30955, 309, 538, 264], "temperature": 0.0, "avg_logprob": -0.1990900473161177, "compression_ratio": 1.7524271844660195, "no_speech_prob": 9.516163117950782e-06}, {"id": 1190, "seek": 600316, "start": 6015.28, "end": 6019.72, "text": " weight matrix for the hidden state, and adding the biases for the hidden state.", "tokens": [3364, 8141, 337, 264, 7633, 1785, 11, 293, 5127, 264, 32152, 337, 264, 7633, 1785, 13], "temperature": 0.0, "avg_logprob": -0.1990900473161177, "compression_ratio": 1.7524271844660195, "no_speech_prob": 9.516163117950782e-06}, {"id": 1191, "seek": 600316, "start": 6019.72, "end": 6024.04, "text": " And then puts the whole thing through an activation function, relu.", "tokens": [400, 550, 8137, 264, 1379, 551, 807, 364, 24433, 2445, 11, 1039, 84, 13], "temperature": 0.0, "avg_logprob": -0.1990900473161177, "compression_ratio": 1.7524271844660195, "no_speech_prob": 9.516163117950782e-06}, {"id": 1192, "seek": 600316, "start": 6024.04, "end": 6030.36, "text": " So in other words, that was calculating these.", "tokens": [407, 294, 661, 2283, 11, 300, 390, 28258, 613, 13], "temperature": 0.0, "avg_logprob": -0.1990900473161177, "compression_ratio": 1.7524271844660195, "no_speech_prob": 9.516163117950782e-06}, {"id": 1193, "seek": 603036, "start": 6030.36, "end": 6034.36, "text": " So we had one bit which was calculating our previous hidden state and putting it through", "tokens": [407, 321, 632, 472, 857, 597, 390, 28258, 527, 3894, 7633, 1785, 293, 3372, 309, 807], "temperature": 0.0, "avg_logprob": -0.18134720811566102, "compression_ratio": 1.8807339449541285, "no_speech_prob": 3.726626118805143e-06}, {"id": 1194, "seek": 603036, "start": 6034.36, "end": 6039.4, "text": " the hidden state weight matrix, which is an orange arrow.", "tokens": [264, 7633, 1785, 3364, 8141, 11, 597, 307, 364, 7671, 11610, 13], "temperature": 0.0, "avg_logprob": -0.18134720811566102, "compression_ratio": 1.8807339449541285, "no_speech_prob": 3.726626118805143e-06}, {"id": 1195, "seek": 603036, "start": 6039.4, "end": 6044.86, "text": " It was taking our next input and putting it through the input one and then adding the", "tokens": [467, 390, 1940, 527, 958, 4846, 293, 3372, 309, 807, 264, 4846, 472, 293, 550, 5127, 264], "temperature": 0.0, "avg_logprob": -0.18134720811566102, "compression_ratio": 1.8807339449541285, "no_speech_prob": 3.726626118805143e-06}, {"id": 1196, "seek": 603036, "start": 6044.86, "end": 6047.92, "text": " two together.", "tokens": [732, 1214, 13], "temperature": 0.0, "avg_logprob": -0.18134720811566102, "compression_ratio": 1.8807339449541285, "no_speech_prob": 3.726626118805143e-06}, {"id": 1197, "seek": 603036, "start": 6047.92, "end": 6049.32, "text": " So that's what we have here.", "tokens": [407, 300, 311, 437, 321, 362, 510, 13], "temperature": 0.0, "avg_logprob": -0.18134720811566102, "compression_ratio": 1.8807339449541285, "no_speech_prob": 3.726626118805143e-06}, {"id": 1198, "seek": 603036, "start": 6049.32, "end": 6056.44, "text": " The X by WX and the H by WH, and then adding the two together along with the biases, and", "tokens": [440, 1783, 538, 343, 55, 293, 264, 389, 538, 8183, 11, 293, 550, 5127, 264, 732, 1214, 2051, 365, 264, 32152, 11, 293], "temperature": 0.0, "avg_logprob": -0.18134720811566102, "compression_ratio": 1.8807339449541285, "no_speech_prob": 3.726626118805143e-06}, {"id": 1199, "seek": 603036, "start": 6056.44, "end": 6060.0, "text": " then put that through an activation function.", "tokens": [550, 829, 300, 807, 364, 24433, 2445, 13], "temperature": 0.0, "avg_logprob": -0.18134720811566102, "compression_ratio": 1.8807339449541285, "no_speech_prob": 3.726626118805143e-06}, {"id": 1200, "seek": 606000, "start": 6060.0, "end": 6066.72, "text": " So once we've done that, we now want to create an output every single time.", "tokens": [407, 1564, 321, 600, 1096, 300, 11, 321, 586, 528, 281, 1884, 364, 5598, 633, 2167, 565, 13], "temperature": 0.0, "avg_logprob": -0.16687574057743468, "compression_ratio": 1.5778894472361809, "no_speech_prob": 2.332069016119931e-06}, {"id": 1201, "seek": 606000, "start": 6066.72, "end": 6070.04, "text": " And so our output is going to be exactly the same thing.", "tokens": [400, 370, 527, 5598, 307, 516, 281, 312, 2293, 264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.16687574057743468, "compression_ratio": 1.5778894472361809, "no_speech_prob": 2.332069016119931e-06}, {"id": 1202, "seek": 606000, "start": 6070.04, "end": 6074.98, "text": " It's going to take the result of that, which we call H, our hidden state, multiply it by", "tokens": [467, 311, 516, 281, 747, 264, 1874, 295, 300, 11, 597, 321, 818, 389, 11, 527, 7633, 1785, 11, 12972, 309, 538], "temperature": 0.0, "avg_logprob": -0.16687574057743468, "compression_ratio": 1.5778894472361809, "no_speech_prob": 2.332069016119931e-06}, {"id": 1203, "seek": 606000, "start": 6074.98, "end": 6079.96, "text": " the output's weight vector, adding on the bias, and this time we're going to use soft", "tokens": [264, 5598, 311, 3364, 8062, 11, 5127, 322, 264, 12577, 11, 293, 341, 565, 321, 434, 516, 281, 764, 2787], "temperature": 0.0, "avg_logprob": -0.16687574057743468, "compression_ratio": 1.5778894472361809, "no_speech_prob": 2.332069016119931e-06}, {"id": 1204, "seek": 606000, "start": 6079.96, "end": 6081.98, "text": " maths.", "tokens": [36287, 13], "temperature": 0.0, "avg_logprob": -0.16687574057743468, "compression_ratio": 1.5778894472361809, "no_speech_prob": 2.332069016119931e-06}, {"id": 1205, "seek": 608198, "start": 6081.98, "end": 6091.5199999999995, "text": " So you can see that this sequence here is describing how to do one of these things.", "tokens": [407, 291, 393, 536, 300, 341, 8310, 510, 307, 16141, 577, 281, 360, 472, 295, 613, 721, 13], "temperature": 0.0, "avg_logprob": -0.09554730591021086, "compression_ratio": 1.6327683615819208, "no_speech_prob": 1.1365610816937988e-06}, {"id": 1206, "seek": 608198, "start": 6091.5199999999995, "end": 6096.98, "text": " And so this therefore defines what we want to do each step through.", "tokens": [400, 370, 341, 4412, 23122, 437, 321, 528, 281, 360, 1184, 1823, 807, 13], "temperature": 0.0, "avg_logprob": -0.09554730591021086, "compression_ratio": 1.6327683615819208, "no_speech_prob": 1.1365610816937988e-06}, {"id": 1207, "seek": 608198, "start": 6096.98, "end": 6106.16, "text": " And at the end of that, we're going to return the hidden state we have so far and our output.", "tokens": [400, 412, 264, 917, 295, 300, 11, 321, 434, 516, 281, 2736, 264, 7633, 1785, 321, 362, 370, 1400, 293, 527, 5598, 13], "temperature": 0.0, "avg_logprob": -0.09554730591021086, "compression_ratio": 1.6327683615819208, "no_speech_prob": 1.1365610816937988e-06}, {"id": 1208, "seek": 608198, "start": 6106.16, "end": 6108.599999999999, "text": " So that's what's going to happen each step.", "tokens": [407, 300, 311, 437, 311, 516, 281, 1051, 1184, 1823, 13], "temperature": 0.0, "avg_logprob": -0.09554730591021086, "compression_ratio": 1.6327683615819208, "no_speech_prob": 1.1365610816937988e-06}, {"id": 1209, "seek": 610860, "start": 6108.6, "end": 6115.96, "text": " So the sequence that we're going to pass into it, we're not going to give it any data yet.", "tokens": [407, 264, 8310, 300, 321, 434, 516, 281, 1320, 666, 309, 11, 321, 434, 406, 516, 281, 976, 309, 604, 1412, 1939, 13], "temperature": 0.0, "avg_logprob": -0.2200564702351888, "compression_ratio": 1.75, "no_speech_prob": 5.682401024387218e-06}, {"id": 1210, "seek": 610860, "start": 6115.96, "end": 6119.04, "text": " Remember, all we're doing is describing a computation.", "tokens": [5459, 11, 439, 321, 434, 884, 307, 16141, 257, 24903, 13], "temperature": 0.0, "avg_logprob": -0.2200564702351888, "compression_ratio": 1.75, "no_speech_prob": 5.682401024387218e-06}, {"id": 1211, "seek": 610860, "start": 6119.04, "end": 6126.240000000001, "text": " So for now, we're just telling it that it will be a matrix.", "tokens": [407, 337, 586, 11, 321, 434, 445, 3585, 309, 300, 309, 486, 312, 257, 8141, 13], "temperature": 0.0, "avg_logprob": -0.2200564702351888, "compression_ratio": 1.75, "no_speech_prob": 5.682401024387218e-06}, {"id": 1212, "seek": 610860, "start": 6126.240000000001, "end": 6128.360000000001, "text": " So we're saying it will be a matrix.", "tokens": [407, 321, 434, 1566, 309, 486, 312, 257, 8141, 13], "temperature": 0.0, "avg_logprob": -0.2200564702351888, "compression_ratio": 1.75, "no_speech_prob": 5.682401024387218e-06}, {"id": 1213, "seek": 610860, "start": 6128.360000000001, "end": 6131.96, "text": " We're going to pass you a matrix.", "tokens": [492, 434, 516, 281, 1320, 291, 257, 8141, 13], "temperature": 0.0, "avg_logprob": -0.2200564702351888, "compression_ratio": 1.75, "no_speech_prob": 5.682401024387218e-06}, {"id": 1214, "seek": 610860, "start": 6131.96, "end": 6134.120000000001, "text": " It also needs a starting point.", "tokens": [467, 611, 2203, 257, 2891, 935, 13], "temperature": 0.0, "avg_logprob": -0.2200564702351888, "compression_ratio": 1.75, "no_speech_prob": 5.682401024387218e-06}, {"id": 1215, "seek": 613412, "start": 6134.12, "end": 6141.16, "text": " And so the starting point is, again, we are going to provide to you an initial value for", "tokens": [400, 370, 264, 2891, 935, 307, 11, 797, 11, 321, 366, 516, 281, 2893, 281, 291, 364, 5883, 2158, 337], "temperature": 0.0, "avg_logprob": -0.18528625441760552, "compression_ratio": 1.5916230366492146, "no_speech_prob": 4.936955974699231e-06}, {"id": 1216, "seek": 613412, "start": 6141.16, "end": 6146.12, "text": " our hidden state, but we haven't done it yet.", "tokens": [527, 7633, 1785, 11, 457, 321, 2378, 380, 1096, 309, 1939, 13], "temperature": 0.0, "avg_logprob": -0.18528625441760552, "compression_ratio": 1.5916230366492146, "no_speech_prob": 4.936955974699231e-06}, {"id": 1217, "seek": 613412, "start": 6146.12, "end": 6149.5599999999995, "text": " And then finally, in Theano, you have to tell it what are all of the other things that are", "tokens": [400, 550, 2721, 11, 294, 440, 3730, 11, 291, 362, 281, 980, 309, 437, 366, 439, 295, 264, 661, 721, 300, 366], "temperature": 0.0, "avg_logprob": -0.18528625441760552, "compression_ratio": 1.5916230366492146, "no_speech_prob": 4.936955974699231e-06}, {"id": 1218, "seek": 613412, "start": 6149.5599999999995, "end": 6153.96, "text": " passed to the function, and we're going to pass it that whole list of weights.", "tokens": [4678, 281, 264, 2445, 11, 293, 321, 434, 516, 281, 1320, 309, 300, 1379, 1329, 295, 17443, 13], "temperature": 0.0, "avg_logprob": -0.18528625441760552, "compression_ratio": 1.5916230366492146, "no_speech_prob": 4.936955974699231e-06}, {"id": 1219, "seek": 615396, "start": 6153.96, "end": 6165.4, "text": " So that's why we have here the X, the hidden, and then all of the weights and biases.", "tokens": [407, 300, 311, 983, 321, 362, 510, 264, 1783, 11, 264, 7633, 11, 293, 550, 439, 295, 264, 17443, 293, 32152, 13], "temperature": 0.0, "avg_logprob": -0.12613288776294604, "compression_ratio": 1.5432098765432098, "no_speech_prob": 1.7880539644465898e-06}, {"id": 1220, "seek": 615396, "start": 6165.4, "end": 6175.14, "text": " So that's now described how to execute a whole sequence of steps for an RNN.", "tokens": [407, 300, 311, 586, 7619, 577, 281, 14483, 257, 1379, 8310, 295, 4439, 337, 364, 45702, 45, 13], "temperature": 0.0, "avg_logprob": -0.12613288776294604, "compression_ratio": 1.5432098765432098, "no_speech_prob": 1.7880539644465898e-06}, {"id": 1221, "seek": 615396, "start": 6175.14, "end": 6179.5, "text": " So we've now described how to do this to Theano.", "tokens": [407, 321, 600, 586, 7619, 577, 281, 360, 341, 281, 440, 3730, 13], "temperature": 0.0, "avg_logprob": -0.12613288776294604, "compression_ratio": 1.5432098765432098, "no_speech_prob": 1.7880539644465898e-06}, {"id": 1222, "seek": 615396, "start": 6179.5, "end": 6181.56, "text": " We haven't given it any data to do it.", "tokens": [492, 2378, 380, 2212, 309, 604, 1412, 281, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.12613288776294604, "compression_ratio": 1.5432098765432098, "no_speech_prob": 1.7880539644465898e-06}, {"id": 1223, "seek": 618156, "start": 6181.56, "end": 6185.72, "text": " We've just set up the computation.", "tokens": [492, 600, 445, 992, 493, 264, 24903, 13], "temperature": 0.0, "avg_logprob": -0.15368081842150008, "compression_ratio": 1.7695652173913043, "no_speech_prob": 5.507581590791233e-06}, {"id": 1224, "seek": 618156, "start": 6185.72, "end": 6190.4400000000005, "text": " And so when that computation is run, it's going to return two things, because step returned", "tokens": [400, 370, 562, 300, 24903, 307, 1190, 11, 309, 311, 516, 281, 2736, 732, 721, 11, 570, 1823, 8752], "temperature": 0.0, "avg_logprob": -0.15368081842150008, "compression_ratio": 1.7695652173913043, "no_speech_prob": 5.507581590791233e-06}, {"id": 1225, "seek": 618156, "start": 6190.4400000000005, "end": 6191.4400000000005, "text": " two things.", "tokens": [732, 721, 13], "temperature": 0.0, "avg_logprob": -0.15368081842150008, "compression_ratio": 1.7695652173913043, "no_speech_prob": 5.507581590791233e-06}, {"id": 1226, "seek": 618156, "start": 6191.4400000000005, "end": 6199.080000000001, "text": " It's going to return the hidden state, and it's going to return our output activations.", "tokens": [467, 311, 516, 281, 2736, 264, 7633, 1785, 11, 293, 309, 311, 516, 281, 2736, 527, 5598, 2430, 763, 13], "temperature": 0.0, "avg_logprob": -0.15368081842150008, "compression_ratio": 1.7695652173913043, "no_speech_prob": 5.507581590791233e-06}, {"id": 1227, "seek": 618156, "start": 6199.080000000001, "end": 6201.68, "text": " So now we need to calculate our error.", "tokens": [407, 586, 321, 643, 281, 8873, 527, 6713, 13], "temperature": 0.0, "avg_logprob": -0.15368081842150008, "compression_ratio": 1.7695652173913043, "no_speech_prob": 5.507581590791233e-06}, {"id": 1228, "seek": 618156, "start": 6201.68, "end": 6205.240000000001, "text": " So our error will be the categorical cross-entropy.", "tokens": [407, 527, 6713, 486, 312, 264, 19250, 804, 3278, 12, 317, 27514, 13], "temperature": 0.0, "avg_logprob": -0.15368081842150008, "compression_ratio": 1.7695652173913043, "no_speech_prob": 5.507581590791233e-06}, {"id": 1229, "seek": 618156, "start": 6205.240000000001, "end": 6207.0, "text": " So these things are all part of Theano.", "tokens": [407, 613, 721, 366, 439, 644, 295, 440, 3730, 13], "temperature": 0.0, "avg_logprob": -0.15368081842150008, "compression_ratio": 1.7695652173913043, "no_speech_prob": 5.507581590791233e-06}, {"id": 1230, "seek": 618156, "start": 6207.0, "end": 6210.120000000001, "text": " You can see I'm using some Theano functions here.", "tokens": [509, 393, 536, 286, 478, 1228, 512, 440, 3730, 6828, 510, 13], "temperature": 0.0, "avg_logprob": -0.15368081842150008, "compression_ratio": 1.7695652173913043, "no_speech_prob": 5.507581590791233e-06}, {"id": 1231, "seek": 621012, "start": 6210.12, "end": 6216.0, "text": " And so we're going to compare the output that came out of our scan, and we're going to compare", "tokens": [400, 370, 321, 434, 516, 281, 6794, 264, 5598, 300, 1361, 484, 295, 527, 11049, 11, 293, 321, 434, 516, 281, 6794], "temperature": 0.0, "avg_logprob": -0.128695881885031, "compression_ratio": 1.7043010752688172, "no_speech_prob": 1.6701294953236356e-05}, {"id": 1232, "seek": 621012, "start": 6216.0, "end": 6222.62, "text": " it to what we don't know yet, but it will be a matrix.", "tokens": [309, 281, 437, 321, 500, 380, 458, 1939, 11, 457, 309, 486, 312, 257, 8141, 13], "temperature": 0.0, "avg_logprob": -0.128695881885031, "compression_ratio": 1.7043010752688172, "no_speech_prob": 1.6701294953236356e-05}, {"id": 1233, "seek": 621012, "start": 6222.62, "end": 6225.92, "text": " And then once you do that, add it all together.", "tokens": [400, 550, 1564, 291, 360, 300, 11, 909, 309, 439, 1214, 13], "temperature": 0.0, "avg_logprob": -0.128695881885031, "compression_ratio": 1.7043010752688172, "no_speech_prob": 1.6701294953236356e-05}, {"id": 1234, "seek": 621012, "start": 6225.92, "end": 6228.2, "text": " Now here's the amazing thing.", "tokens": [823, 510, 311, 264, 2243, 551, 13], "temperature": 0.0, "avg_logprob": -0.128695881885031, "compression_ratio": 1.7043010752688172, "no_speech_prob": 1.6701294953236356e-05}, {"id": 1235, "seek": 621012, "start": 6228.2, "end": 6233.28, "text": " Every step, we're going to want to apply SGD, which means every step, we're going to want", "tokens": [2048, 1823, 11, 321, 434, 516, 281, 528, 281, 3079, 34520, 35, 11, 597, 1355, 633, 1823, 11, 321, 434, 516, 281, 528], "temperature": 0.0, "avg_logprob": -0.128695881885031, "compression_ratio": 1.7043010752688172, "no_speech_prob": 1.6701294953236356e-05}, {"id": 1236, "seek": 623328, "start": 6233.28, "end": 6241.2, "text": " to take the derivative of this whole thing with respect to all of the weights and use", "tokens": [281, 747, 264, 13760, 295, 341, 1379, 551, 365, 3104, 281, 439, 295, 264, 17443, 293, 764], "temperature": 0.0, "avg_logprob": -0.12388983438181322, "compression_ratio": 1.7450980392156863, "no_speech_prob": 3.611972715589218e-06}, {"id": 1237, "seek": 623328, "start": 6241.2, "end": 6245.08, "text": " that along with the learning rate to update all of the weights.", "tokens": [300, 2051, 365, 264, 2539, 3314, 281, 5623, 439, 295, 264, 17443, 13], "temperature": 0.0, "avg_logprob": -0.12388983438181322, "compression_ratio": 1.7450980392156863, "no_speech_prob": 3.611972715589218e-06}, {"id": 1238, "seek": 623328, "start": 6245.08, "end": 6248.639999999999, "text": " In Theano, that's how you do it.", "tokens": [682, 440, 3730, 11, 300, 311, 577, 291, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.12388983438181322, "compression_ratio": 1.7450980392156863, "no_speech_prob": 3.611972715589218e-06}, {"id": 1239, "seek": 623328, "start": 6248.639999999999, "end": 6256.84, "text": " You just say, please tell me the gradient of this function with respect to these inputs.", "tokens": [509, 445, 584, 11, 1767, 980, 385, 264, 16235, 295, 341, 2445, 365, 3104, 281, 613, 15743, 13], "temperature": 0.0, "avg_logprob": -0.12388983438181322, "compression_ratio": 1.7450980392156863, "no_speech_prob": 3.611972715589218e-06}, {"id": 1240, "seek": 623328, "start": 6256.84, "end": 6262.66, "text": " And Theano will symbolically automatically calculate all of the derivatives for you.", "tokens": [400, 440, 3730, 486, 5986, 984, 6772, 8873, 439, 295, 264, 33733, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.12388983438181322, "compression_ratio": 1.7450980392156863, "no_speech_prob": 3.611972715589218e-06}, {"id": 1241, "seek": 626266, "start": 6262.66, "end": 6265.48, "text": " So that's very nearly magic.", "tokens": [407, 300, 311, 588, 6217, 5585, 13], "temperature": 0.0, "avg_logprob": -0.16482532501220704, "compression_ratio": 1.7522935779816513, "no_speech_prob": 6.144143753772369e-06}, {"id": 1242, "seek": 626266, "start": 6265.48, "end": 6269.5199999999995, "text": " But we don't have to worry about derivatives because it's going to calculate them all for", "tokens": [583, 321, 500, 380, 362, 281, 3292, 466, 33733, 570, 309, 311, 516, 281, 8873, 552, 439, 337], "temperature": 0.0, "avg_logprob": -0.16482532501220704, "compression_ratio": 1.7522935779816513, "no_speech_prob": 6.144143753772369e-06}, {"id": 1243, "seek": 626266, "start": 6269.5199999999995, "end": 6271.24, "text": " us.", "tokens": [505, 13], "temperature": 0.0, "avg_logprob": -0.16482532501220704, "compression_ratio": 1.7522935779816513, "no_speech_prob": 6.144143753772369e-06}, {"id": 1244, "seek": 626266, "start": 6271.24, "end": 6276.3, "text": " So at this point, I now have a function that calculates our loss, and I have a function", "tokens": [407, 412, 341, 935, 11, 286, 586, 362, 257, 2445, 300, 4322, 1024, 527, 4470, 11, 293, 286, 362, 257, 2445], "temperature": 0.0, "avg_logprob": -0.16482532501220704, "compression_ratio": 1.7522935779816513, "no_speech_prob": 6.144143753772369e-06}, {"id": 1245, "seek": 626266, "start": 6276.3, "end": 6280.04, "text": " that calculates all of the gradients that we need with respect to all of the different", "tokens": [300, 4322, 1024, 439, 295, 264, 2771, 2448, 300, 321, 643, 365, 3104, 281, 439, 295, 264, 819], "temperature": 0.0, "avg_logprob": -0.16482532501220704, "compression_ratio": 1.7522935779816513, "no_speech_prob": 6.144143753772369e-06}, {"id": 1246, "seek": 626266, "start": 6280.04, "end": 6283.76, "text": " weights and parameters that we have.", "tokens": [17443, 293, 9834, 300, 321, 362, 13], "temperature": 0.0, "avg_logprob": -0.16482532501220704, "compression_ratio": 1.7522935779816513, "no_speech_prob": 6.144143753772369e-06}, {"id": 1247, "seek": 626266, "start": 6283.76, "end": 6288.18, "text": " So we're now ready to build our final function.", "tokens": [407, 321, 434, 586, 1919, 281, 1322, 527, 2572, 2445, 13], "temperature": 0.0, "avg_logprob": -0.16482532501220704, "compression_ratio": 1.7522935779816513, "no_speech_prob": 6.144143753772369e-06}, {"id": 1248, "seek": 628818, "start": 6288.18, "end": 6295.4400000000005, "text": " And so our final function as input takes all of our arguments, that is these 4 things,", "tokens": [400, 370, 527, 2572, 2445, 382, 4846, 2516, 439, 295, 527, 12869, 11, 300, 307, 613, 1017, 721, 11], "temperature": 0.0, "avg_logprob": -0.19193354223528478, "compression_ratio": 2.060747663551402, "no_speech_prob": 2.0580419004545547e-06}, {"id": 1249, "seek": 628818, "start": 6295.4400000000005, "end": 6300.4400000000005, "text": " which are the things we told it we're going to need later.", "tokens": [597, 366, 264, 721, 321, 1907, 309, 321, 434, 516, 281, 643, 1780, 13], "temperature": 0.0, "avg_logprob": -0.19193354223528478, "compression_ratio": 2.060747663551402, "no_speech_prob": 2.0580419004545547e-06}, {"id": 1250, "seek": 628818, "start": 6300.4400000000005, "end": 6307.4400000000005, "text": " The thing that it's going to create as an output is the error, which was this output.", "tokens": [440, 551, 300, 309, 311, 516, 281, 1884, 382, 364, 5598, 307, 264, 6713, 11, 597, 390, 341, 5598, 13], "temperature": 0.0, "avg_logprob": -0.19193354223528478, "compression_ratio": 2.060747663551402, "no_speech_prob": 2.0580419004545547e-06}, {"id": 1251, "seek": 628818, "start": 6307.4400000000005, "end": 6310.52, "text": " And at each step, it's going to do some updates.", "tokens": [400, 412, 1184, 1823, 11, 309, 311, 516, 281, 360, 512, 9205, 13], "temperature": 0.0, "avg_logprob": -0.19193354223528478, "compression_ratio": 2.060747663551402, "no_speech_prob": 2.0580419004545547e-06}, {"id": 1252, "seek": 628818, "start": 6310.52, "end": 6312.72, "text": " And so it's going to update, what are the updates it's going to do?", "tokens": [400, 370, 309, 311, 516, 281, 5623, 11, 437, 366, 264, 9205, 309, 311, 516, 281, 360, 30], "temperature": 0.0, "avg_logprob": -0.19193354223528478, "compression_ratio": 2.060747663551402, "no_speech_prob": 2.0580419004545547e-06}, {"id": 1253, "seek": 628818, "start": 6312.72, "end": 6317.320000000001, "text": " The updates it's going to do is the result of this little function, and this little function", "tokens": [440, 9205, 309, 311, 516, 281, 360, 307, 264, 1874, 295, 341, 707, 2445, 11, 293, 341, 707, 2445], "temperature": 0.0, "avg_logprob": -0.19193354223528478, "compression_ratio": 2.060747663551402, "no_speech_prob": 2.0580419004545547e-06}, {"id": 1254, "seek": 631732, "start": 6317.32, "end": 6324.92, "text": " is something that creates a dictionary that is going to map every one of our weights to", "tokens": [307, 746, 300, 7829, 257, 25890, 300, 307, 516, 281, 4471, 633, 472, 295, 527, 17443, 281], "temperature": 0.0, "avg_logprob": -0.157808352739383, "compression_ratio": 1.8651685393258426, "no_speech_prob": 2.9944299058115575e-06}, {"id": 1255, "seek": 631732, "start": 6324.92, "end": 6333.34, "text": " that weight minus each one of our gradients times the learning rate.", "tokens": [300, 3364, 3175, 1184, 472, 295, 527, 2771, 2448, 1413, 264, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.157808352739383, "compression_ratio": 1.8651685393258426, "no_speech_prob": 2.9944299058115575e-06}, {"id": 1256, "seek": 631732, "start": 6333.34, "end": 6341.5599999999995, "text": " So it's going to update every weight to itself minus its gradient times the learning rate.", "tokens": [407, 309, 311, 516, 281, 5623, 633, 3364, 281, 2564, 3175, 1080, 16235, 1413, 264, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.157808352739383, "compression_ratio": 1.8651685393258426, "no_speech_prob": 2.9944299058115575e-06}, {"id": 1257, "seek": 631732, "start": 6341.5599999999995, "end": 6346.84, "text": " So basically what Theano does is it says, it's got this little thing called updates,", "tokens": [407, 1936, 437, 440, 3730, 775, 307, 309, 1619, 11, 309, 311, 658, 341, 707, 551, 1219, 9205, 11], "temperature": 0.0, "avg_logprob": -0.157808352739383, "compression_ratio": 1.8651685393258426, "no_speech_prob": 2.9944299058115575e-06}, {"id": 1258, "seek": 634684, "start": 6346.84, "end": 6353.8, "text": " it says every time you calculate the next step, I want you to change your shared variables", "tokens": [309, 1619, 633, 565, 291, 8873, 264, 958, 1823, 11, 286, 528, 291, 281, 1319, 428, 5507, 9102], "temperature": 0.0, "avg_logprob": -0.14889226004342052, "compression_ratio": 1.6278026905829597, "no_speech_prob": 4.092893505003303e-06}, {"id": 1259, "seek": 634684, "start": 6353.8, "end": 6354.8, "text": " as bollards.", "tokens": [382, 748, 285, 2287, 13], "temperature": 0.0, "avg_logprob": -0.14889226004342052, "compression_ratio": 1.6278026905829597, "no_speech_prob": 4.092893505003303e-06}, {"id": 1260, "seek": 634684, "start": 6354.8, "end": 6359.04, "text": " So there's our list of changes to make.", "tokens": [407, 456, 311, 527, 1329, 295, 2962, 281, 652, 13], "temperature": 0.0, "avg_logprob": -0.14889226004342052, "compression_ratio": 1.6278026905829597, "no_speech_prob": 4.092893505003303e-06}, {"id": 1261, "seek": 634684, "start": 6359.04, "end": 6360.64, "text": " And so that's it.", "tokens": [400, 370, 300, 311, 309, 13], "temperature": 0.0, "avg_logprob": -0.14889226004342052, "compression_ratio": 1.6278026905829597, "no_speech_prob": 4.092893505003303e-06}, {"id": 1262, "seek": 634684, "start": 6360.64, "end": 6366.4400000000005, "text": " So we use our one hot encoded x's and our one hot encoded y's, and we have to now manually", "tokens": [407, 321, 764, 527, 472, 2368, 2058, 12340, 2031, 311, 293, 527, 472, 2368, 2058, 12340, 288, 311, 11, 293, 321, 362, 281, 586, 16945], "temperature": 0.0, "avg_logprob": -0.14889226004342052, "compression_ratio": 1.6278026905829597, "no_speech_prob": 4.092893505003303e-06}, {"id": 1263, "seek": 634684, "start": 6366.4400000000005, "end": 6368.12, "text": " create our own loop.", "tokens": [1884, 527, 1065, 6367, 13], "temperature": 0.0, "avg_logprob": -0.14889226004342052, "compression_ratio": 1.6278026905829597, "no_speech_prob": 4.092893505003303e-06}, {"id": 1264, "seek": 634684, "start": 6368.12, "end": 6373.360000000001, "text": " Theano doesn't have any built-in stuff for us, so we're going to go through every element", "tokens": [440, 3730, 1177, 380, 362, 604, 3094, 12, 259, 1507, 337, 505, 11, 370, 321, 434, 516, 281, 352, 807, 633, 4478], "temperature": 0.0, "avg_logprob": -0.14889226004342052, "compression_ratio": 1.6278026905829597, "no_speech_prob": 4.092893505003303e-06}, {"id": 1265, "seek": 637336, "start": 6373.36, "end": 6381.679999999999, "text": " of our input and we're going to say, let's call that function, so that function is the", "tokens": [295, 527, 4846, 293, 321, 434, 516, 281, 584, 11, 718, 311, 818, 300, 2445, 11, 370, 300, 2445, 307, 264], "temperature": 0.0, "avg_logprob": -0.16758932965867063, "compression_ratio": 1.758974358974359, "no_speech_prob": 5.338131813914515e-06}, {"id": 1266, "seek": 637336, "start": 6381.679999999999, "end": 6386.96, "text": " function that we just created, and now we have to pass in all of these inputs.", "tokens": [2445, 300, 321, 445, 2942, 11, 293, 586, 321, 362, 281, 1320, 294, 439, 295, 613, 15743, 13], "temperature": 0.0, "avg_logprob": -0.16758932965867063, "compression_ratio": 1.758974358974359, "no_speech_prob": 5.338131813914515e-06}, {"id": 1267, "seek": 637336, "start": 6386.96, "end": 6394.88, "text": " So we have to finally pass in a value for the initial hidden state, the input, the target,", "tokens": [407, 321, 362, 281, 2721, 1320, 294, 257, 2158, 337, 264, 5883, 7633, 1785, 11, 264, 4846, 11, 264, 3779, 11], "temperature": 0.0, "avg_logprob": -0.16758932965867063, "compression_ratio": 1.758974358974359, "no_speech_prob": 5.338131813914515e-06}, {"id": 1268, "seek": 637336, "start": 6394.88, "end": 6395.88, "text": " and the learning rate.", "tokens": [293, 264, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.16758932965867063, "compression_ratio": 1.758974358974359, "no_speech_prob": 5.338131813914515e-06}, {"id": 1269, "seek": 637336, "start": 6395.88, "end": 6399.16, "text": " So this is where we get to do it, when we finally call it here.", "tokens": [407, 341, 307, 689, 321, 483, 281, 360, 309, 11, 562, 321, 2721, 818, 309, 510, 13], "temperature": 0.0, "avg_logprob": -0.16758932965867063, "compression_ratio": 1.758974358974359, "no_speech_prob": 5.338131813914515e-06}, {"id": 1270, "seek": 639916, "start": 6399.16, "end": 6407.4, "text": " So here's our initial hidden state, just a bunch of zeros, our input, our output, and", "tokens": [407, 510, 311, 527, 5883, 7633, 1785, 11, 445, 257, 3840, 295, 35193, 11, 527, 4846, 11, 527, 5598, 11, 293], "temperature": 0.0, "avg_logprob": -0.224052028891481, "compression_ratio": 1.4944444444444445, "no_speech_prob": 8.139660167216789e-06}, {"id": 1271, "seek": 639916, "start": 6407.4, "end": 6410.92, "text": " our learning rate, which we set to.01.", "tokens": [527, 2539, 3314, 11, 597, 321, 992, 281, 2411, 10607, 13], "temperature": 0.0, "avg_logprob": -0.224052028891481, "compression_ratio": 1.4944444444444445, "no_speech_prob": 8.139660167216789e-06}, {"id": 1272, "seek": 639916, "start": 6410.92, "end": 6414.72, "text": " And then I've just set it to something here that says, okay, every 1000 times, print out", "tokens": [400, 550, 286, 600, 445, 992, 309, 281, 746, 510, 300, 1619, 11, 1392, 11, 633, 9714, 1413, 11, 4482, 484], "temperature": 0.0, "avg_logprob": -0.224052028891481, "compression_ratio": 1.4944444444444445, "no_speech_prob": 8.139660167216789e-06}, {"id": 1273, "seek": 639916, "start": 6414.72, "end": 6416.08, "text": " the error.", "tokens": [264, 6713, 13], "temperature": 0.0, "avg_logprob": -0.224052028891481, "compression_ratio": 1.4944444444444445, "no_speech_prob": 8.139660167216789e-06}, {"id": 1274, "seek": 639916, "start": 6416.08, "end": 6421.48, "text": " And so as you can see, over time, it learns.", "tokens": [400, 370, 382, 291, 393, 536, 11, 670, 565, 11, 309, 27152, 13], "temperature": 0.0, "avg_logprob": -0.224052028891481, "compression_ratio": 1.4944444444444445, "no_speech_prob": 8.139660167216789e-06}, {"id": 1275, "seek": 642148, "start": 6421.48, "end": 6430.12, "text": " And so at the end of learning, I create a new Theano function which takes some piece", "tokens": [400, 370, 412, 264, 917, 295, 2539, 11, 286, 1884, 257, 777, 440, 3730, 2445, 597, 2516, 512, 2522], "temperature": 0.0, "avg_logprob": -0.20231157834412622, "compression_ratio": 1.5276073619631902, "no_speech_prob": 3.340519015182508e-06}, {"id": 1276, "seek": 642148, "start": 6430.12, "end": 6440.719999999999, "text": " of input along with some initial hidden state, and it produces not the loss, but the output.", "tokens": [295, 4846, 2051, 365, 512, 5883, 7633, 1785, 11, 293, 309, 14725, 406, 264, 4470, 11, 457, 264, 5598, 13], "temperature": 0.0, "avg_logprob": -0.20231157834412622, "compression_ratio": 1.5276073619631902, "no_speech_prob": 3.340519015182508e-06}, {"id": 1277, "seek": 642148, "start": 6440.719999999999, "end": 6447.28, "text": " Are we using gradient descent and not stochastic gradient descent here?", "tokens": [2014, 321, 1228, 16235, 23475, 293, 406, 342, 8997, 2750, 16235, 23475, 510, 30], "temperature": 0.0, "avg_logprob": -0.20231157834412622, "compression_ratio": 1.5276073619631902, "no_speech_prob": 3.340519015182508e-06}, {"id": 1278, "seek": 644728, "start": 6447.28, "end": 6451.8, "text": " We're using stochastic gradient descent with a mini-batch size of 1.", "tokens": [492, 434, 1228, 342, 8997, 2750, 16235, 23475, 365, 257, 8382, 12, 65, 852, 2744, 295, 502, 13], "temperature": 0.0, "avg_logprob": -0.14480846530788546, "compression_ratio": 1.8148148148148149, "no_speech_prob": 7.411237675114535e-06}, {"id": 1279, "seek": 644728, "start": 6451.8, "end": 6456.32, "text": " So gradient descent without stochastic actually means you're using a mini-batch size of the", "tokens": [407, 16235, 23475, 1553, 342, 8997, 2750, 767, 1355, 291, 434, 1228, 257, 8382, 12, 65, 852, 2744, 295, 264], "temperature": 0.0, "avg_logprob": -0.14480846530788546, "compression_ratio": 1.8148148148148149, "no_speech_prob": 7.411237675114535e-06}, {"id": 1280, "seek": 644728, "start": 6456.32, "end": 6457.32, "text": " whole data set.", "tokens": [1379, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.14480846530788546, "compression_ratio": 1.8148148148148149, "no_speech_prob": 7.411237675114535e-06}, {"id": 1281, "seek": 644728, "start": 6457.32, "end": 6458.88, "text": " So this is kind of the opposite of that.", "tokens": [407, 341, 307, 733, 295, 264, 6182, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.14480846530788546, "compression_ratio": 1.8148148148148149, "no_speech_prob": 7.411237675114535e-06}, {"id": 1282, "seek": 644728, "start": 6458.88, "end": 6465.36, "text": " I think this is called online gradient descent.", "tokens": [286, 519, 341, 307, 1219, 2950, 16235, 23475, 13], "temperature": 0.0, "avg_logprob": -0.14480846530788546, "compression_ratio": 1.8148148148148149, "no_speech_prob": 7.411237675114535e-06}, {"id": 1283, "seek": 644728, "start": 6465.36, "end": 6473.96, "text": " So remember earlier on, we had this thing to calculate the vector of outputs.", "tokens": [407, 1604, 3071, 322, 11, 321, 632, 341, 551, 281, 8873, 264, 8062, 295, 23930, 13], "temperature": 0.0, "avg_logprob": -0.14480846530788546, "compression_ratio": 1.8148148148148149, "no_speech_prob": 7.411237675114535e-06}, {"id": 1284, "seek": 647396, "start": 6473.96, "end": 6478.8, "text": " So now to do our testing, we're going to create a new function which goes from our input to", "tokens": [407, 586, 281, 360, 527, 4997, 11, 321, 434, 516, 281, 1884, 257, 777, 2445, 597, 1709, 490, 527, 4846, 281], "temperature": 0.0, "avg_logprob": -0.1487027458522631, "compression_ratio": 1.6181818181818182, "no_speech_prob": 3.3931335110537475e-06}, {"id": 1285, "seek": 647396, "start": 6478.8, "end": 6480.68, "text": " our vector of outputs.", "tokens": [527, 8062, 295, 23930, 13], "temperature": 0.0, "avg_logprob": -0.1487027458522631, "compression_ratio": 1.6181818181818182, "no_speech_prob": 3.3931335110537475e-06}, {"id": 1286, "seek": 647396, "start": 6480.68, "end": 6488.2, "text": " And so our predictions will be to take that function, pass it in our initial hidden state,", "tokens": [400, 370, 527, 21264, 486, 312, 281, 747, 300, 2445, 11, 1320, 309, 294, 527, 5883, 7633, 1785, 11], "temperature": 0.0, "avg_logprob": -0.1487027458522631, "compression_ratio": 1.6181818181818182, "no_speech_prob": 3.3931335110537475e-06}, {"id": 1287, "seek": 647396, "start": 6488.2, "end": 6493.66, "text": " and some input, and that's going to give us some predictions.", "tokens": [293, 512, 4846, 11, 293, 300, 311, 516, 281, 976, 505, 512, 21264, 13], "temperature": 0.0, "avg_logprob": -0.1487027458522631, "compression_ratio": 1.6181818181818182, "no_speech_prob": 3.3931335110537475e-06}, {"id": 1288, "seek": 649366, "start": 6493.66, "end": 6507.24, "text": " So if we call it, we can now see, let's now grab some sequence of text, pass it to our", "tokens": [407, 498, 321, 818, 309, 11, 321, 393, 586, 536, 11, 718, 311, 586, 4444, 512, 8310, 295, 2487, 11, 1320, 309, 281, 527], "temperature": 0.0, "avg_logprob": -0.2690692076811919, "compression_ratio": 1.710144927536232, "no_speech_prob": 2.0145458620390855e-05}, {"id": 1289, "seek": 649366, "start": 6507.24, "end": 6509.84, "text": " function to get some predictions, and let's see what it does.", "tokens": [2445, 281, 483, 512, 21264, 11, 293, 718, 311, 536, 437, 309, 775, 13], "temperature": 0.0, "avg_logprob": -0.2690692076811919, "compression_ratio": 1.710144927536232, "no_speech_prob": 2.0145458620390855e-05}, {"id": 1290, "seek": 649366, "start": 6509.84, "end": 6516.72, "text": " So after t, it expected h, after t, h, it expected e, after t, h, e, it expected space,", "tokens": [407, 934, 256, 11, 309, 5176, 276, 11, 934, 256, 11, 276, 11, 309, 5176, 308, 11, 934, 256, 11, 276, 11, 308, 11, 309, 5176, 1901, 11], "temperature": 0.0, "avg_logprob": -0.2690692076811919, "compression_ratio": 1.710144927536232, "no_speech_prob": 2.0145458620390855e-05}, {"id": 1291, "seek": 651672, "start": 6516.72, "end": 6527.400000000001, "text": " it actually really wanted space after t, h, e, n, it expected space, after t, h, e, n,", "tokens": [309, 767, 534, 1415, 1901, 934, 256, 11, 276, 11, 308, 11, 297, 11, 309, 5176, 1901, 11, 934, 256, 11, 276, 11, 308, 11, 297, 11], "temperature": 0.6, "avg_logprob": -0.2856070486347327, "compression_ratio": 1.79874213836478, "no_speech_prob": 6.1441082834789995e-06}, {"id": 1292, "seek": 651672, "start": 6527.400000000001, "end": 6530.42, "text": " it expected space, after t, h, e, n, it expected space, and so forth.", "tokens": [309, 5176, 1901, 11, 934, 256, 11, 276, 11, 308, 11, 297, 11, 309, 5176, 1901, 11, 293, 370, 5220, 13], "temperature": 0.6, "avg_logprob": -0.2856070486347327, "compression_ratio": 1.79874213836478, "no_speech_prob": 6.1441082834789995e-06}, {"id": 1293, "seek": 651672, "start": 6530.42, "end": 6542.08, "text": " So you can see here that we have successfully built an RNN from scratch using Theano.", "tokens": [407, 291, 393, 536, 510, 300, 321, 362, 10727, 3094, 364, 45702, 45, 490, 8459, 1228, 440, 3730, 13], "temperature": 0.6, "avg_logprob": -0.2856070486347327, "compression_ratio": 1.79874213836478, "no_speech_prob": 6.1441082834789995e-06}, {"id": 1294, "seek": 651672, "start": 6542.08, "end": 6546.56, "text": " That's been a very, very quick run-through.", "tokens": [663, 311, 668, 257, 588, 11, 588, 1702, 1190, 12, 11529, 13], "temperature": 0.6, "avg_logprob": -0.2856070486347327, "compression_ratio": 1.79874213836478, "no_speech_prob": 6.1441082834789995e-06}, {"id": 1295, "seek": 654656, "start": 6546.56, "end": 6553.4400000000005, "text": " My goal really tonight is to get to a point where you can start to look at this during", "tokens": [1222, 3387, 534, 4440, 307, 281, 483, 281, 257, 935, 689, 291, 393, 722, 281, 574, 412, 341, 1830], "temperature": 0.0, "avg_logprob": -0.21946889504619027, "compression_ratio": 1.583756345177665, "no_speech_prob": 0.0029796427115797997}, {"id": 1296, "seek": 654656, "start": 6553.4400000000005, "end": 6556.200000000001, "text": " the week and see all the pieces.", "tokens": [264, 1243, 293, 536, 439, 264, 3755, 13], "temperature": 0.0, "avg_logprob": -0.21946889504619027, "compression_ratio": 1.583756345177665, "no_speech_prob": 0.0029796427115797997}, {"id": 1297, "seek": 654656, "start": 6556.200000000001, "end": 6562.320000000001, "text": " Because next week, we're going to try and build an LSTM in Theano, which is going to", "tokens": [1436, 958, 1243, 11, 321, 434, 516, 281, 853, 293, 1322, 364, 441, 6840, 44, 294, 440, 3730, 11, 597, 307, 516, 281], "temperature": 0.0, "avg_logprob": -0.21946889504619027, "compression_ratio": 1.583756345177665, "no_speech_prob": 0.0029796427115797997}, {"id": 1298, "seek": 654656, "start": 6562.320000000001, "end": 6567.6, "text": " mean that I want you by next week to start to feel like you've got a good understanding", "tokens": [914, 300, 286, 528, 291, 538, 958, 1243, 281, 722, 281, 841, 411, 291, 600, 658, 257, 665, 3701], "temperature": 0.0, "avg_logprob": -0.21946889504619027, "compression_ratio": 1.583756345177665, "no_speech_prob": 0.0029796427115797997}, {"id": 1299, "seek": 654656, "start": 6567.6, "end": 6568.6, "text": " of what's going on.", "tokens": [295, 437, 311, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.21946889504619027, "compression_ratio": 1.583756345177665, "no_speech_prob": 0.0029796427115797997}, {"id": 1300, "seek": 656860, "start": 6568.6, "end": 6576.72, "text": " So of course, please ask lots of questions on the forum, look at the documentation, and", "tokens": [407, 295, 1164, 11, 1767, 1029, 3195, 295, 1651, 322, 264, 17542, 11, 574, 412, 264, 14333, 11, 293], "temperature": 0.0, "avg_logprob": -0.14657160758972168, "compression_ratio": 1.7064676616915422, "no_speech_prob": 1.280505966860801e-05}, {"id": 1301, "seek": 656860, "start": 6576.72, "end": 6577.72, "text": " so forth.", "tokens": [370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.14657160758972168, "compression_ratio": 1.7064676616915422, "no_speech_prob": 1.280505966860801e-05}, {"id": 1302, "seek": 656860, "start": 6577.72, "end": 6583.4800000000005, "text": " The next thing we're going to do after that is we're going to build an RNN without using", "tokens": [440, 958, 551, 321, 434, 516, 281, 360, 934, 300, 307, 321, 434, 516, 281, 1322, 364, 45702, 45, 1553, 1228], "temperature": 0.0, "avg_logprob": -0.14657160758972168, "compression_ratio": 1.7064676616915422, "no_speech_prob": 1.280505966860801e-05}, {"id": 1303, "seek": 656860, "start": 6583.4800000000005, "end": 6584.4800000000005, "text": " Theano.", "tokens": [440, 3730, 13], "temperature": 0.0, "avg_logprob": -0.14657160758972168, "compression_ratio": 1.7064676616915422, "no_speech_prob": 1.280505966860801e-05}, {"id": 1304, "seek": 656860, "start": 6584.4800000000005, "end": 6591.4400000000005, "text": " We're going to use pure NumPy, and that means that we're not going to be able to use t.grad.", "tokens": [492, 434, 516, 281, 764, 6075, 22592, 47, 88, 11, 293, 300, 1355, 300, 321, 434, 406, 516, 281, 312, 1075, 281, 764, 256, 13, 7165, 13], "temperature": 0.0, "avg_logprob": -0.14657160758972168, "compression_ratio": 1.7064676616915422, "no_speech_prob": 1.280505966860801e-05}, {"id": 1305, "seek": 656860, "start": 6591.4400000000005, "end": 6595.360000000001, "text": " We're going to have to calculate the gradients by hand.", "tokens": [492, 434, 516, 281, 362, 281, 8873, 264, 2771, 2448, 538, 1011, 13], "temperature": 0.0, "avg_logprob": -0.14657160758972168, "compression_ratio": 1.7064676616915422, "no_speech_prob": 1.280505966860801e-05}, {"id": 1306, "seek": 659536, "start": 6595.36, "end": 6605.28, "text": " So hopefully that will be a useful exercise in really understanding what's going on in", "tokens": [407, 4696, 300, 486, 312, 257, 4420, 5380, 294, 534, 3701, 437, 311, 516, 322, 294], "temperature": 0.0, "avg_logprob": -0.20255039356372975, "compression_ratio": 1.4285714285714286, "no_speech_prob": 4.757338319905102e-05}, {"id": 1307, "seek": 659536, "start": 6605.28, "end": 6609.16, "text": " that propagation.", "tokens": [300, 38377, 13], "temperature": 0.0, "avg_logprob": -0.20255039356372975, "compression_ratio": 1.4285714285714286, "no_speech_prob": 4.757338319905102e-05}, {"id": 1308, "seek": 659536, "start": 6609.16, "end": 6613.759999999999, "text": " So I want to make sure you feel like you've got enough information to get started with", "tokens": [407, 286, 528, 281, 652, 988, 291, 841, 411, 291, 600, 658, 1547, 1589, 281, 483, 1409, 365], "temperature": 0.0, "avg_logprob": -0.20255039356372975, "compression_ratio": 1.4285714285714286, "no_speech_prob": 4.757338319905102e-05}, {"id": 1309, "seek": 659536, "start": 6613.759999999999, "end": 6615.679999999999, "text": " looking at Theano this week.", "tokens": [1237, 412, 440, 3730, 341, 1243, 13], "temperature": 0.0, "avg_logprob": -0.20255039356372975, "compression_ratio": 1.4285714285714286, "no_speech_prob": 4.757338319905102e-05}, {"id": 1310, "seek": 661568, "start": 6615.68, "end": 6626.320000000001, "text": " So did anybody want to ask any questions about this piece so far?", "tokens": [407, 630, 4472, 528, 281, 1029, 604, 1651, 466, 341, 2522, 370, 1400, 30], "temperature": 0.0, "avg_logprob": -0.3249138497017525, "compression_ratio": 1.5027322404371584, "no_speech_prob": 0.00011587848712224513}, {"id": 1311, "seek": 661568, "start": 6626.320000000001, "end": 6637.0, "text": " Question- So this is maybe a bit too far away from what we did today, but how would you", "tokens": [14464, 12, 407, 341, 307, 1310, 257, 857, 886, 1400, 1314, 490, 437, 321, 630, 965, 11, 457, 577, 576, 291], "temperature": 0.0, "avg_logprob": -0.3249138497017525, "compression_ratio": 1.5027322404371584, "no_speech_prob": 0.00011587848712224513}, {"id": 1312, "seek": 661568, "start": 6637.0, "end": 6640.320000000001, "text": " apply an RNN to images, something else than text?", "tokens": [3079, 364, 45702, 45, 281, 5267, 11, 746, 1646, 813, 2487, 30], "temperature": 0.0, "avg_logprob": -0.3249138497017525, "compression_ratio": 1.5027322404371584, "no_speech_prob": 0.00011587848712224513}, {"id": 1313, "seek": 661568, "start": 6640.320000000001, "end": 6644.0, "text": " Is that something that's worth doing, and if so, what changes about it?", "tokens": [1119, 300, 746, 300, 311, 3163, 884, 11, 293, 498, 370, 11, 437, 2962, 466, 309, 30], "temperature": 0.0, "avg_logprob": -0.3249138497017525, "compression_ratio": 1.5027322404371584, "no_speech_prob": 0.00011587848712224513}, {"id": 1314, "seek": 664400, "start": 6644.0, "end": 6651.36, "text": " Answer- So the main way in which an RNN is applied to images is what we looked at last", "tokens": [24545, 12, 407, 264, 2135, 636, 294, 597, 364, 45702, 45, 307, 6456, 281, 5267, 307, 437, 321, 2956, 412, 1036], "temperature": 0.0, "avg_logprob": -0.18094888875182247, "compression_ratio": 1.558659217877095, "no_speech_prob": 4.356803401606157e-06}, {"id": 1315, "seek": 664400, "start": 6651.36, "end": 6657.64, "text": " week, which is these things called attentional models, which is where you basically say,", "tokens": [1243, 11, 597, 307, 613, 721, 1219, 3202, 304, 5245, 11, 597, 307, 689, 291, 1936, 584, 11], "temperature": 0.0, "avg_logprob": -0.18094888875182247, "compression_ratio": 1.558659217877095, "no_speech_prob": 4.356803401606157e-06}, {"id": 1316, "seek": 664400, "start": 6657.64, "end": 6664.0, "text": " given which part of the image you're currently looking at, which part would make sense to", "tokens": [2212, 597, 644, 295, 264, 3256, 291, 434, 4362, 1237, 412, 11, 597, 644, 576, 652, 2020, 281], "temperature": 0.0, "avg_logprob": -0.18094888875182247, "compression_ratio": 1.558659217877095, "no_speech_prob": 4.356803401606157e-06}, {"id": 1317, "seek": 664400, "start": 6664.0, "end": 6666.92, "text": " look at next.", "tokens": [574, 412, 958, 13], "temperature": 0.0, "avg_logprob": -0.18094888875182247, "compression_ratio": 1.558659217877095, "no_speech_prob": 4.356803401606157e-06}, {"id": 1318, "seek": 666692, "start": 6666.92, "end": 6683.84, "text": " This is most useful on really big images where you can't really look at the whole thing at", "tokens": [639, 307, 881, 4420, 322, 534, 955, 5267, 689, 291, 393, 380, 534, 574, 412, 264, 1379, 551, 412], "temperature": 0.0, "avg_logprob": -0.19170503263120298, "compression_ratio": 1.3732394366197183, "no_speech_prob": 2.1233540792309213e-06}, {"id": 1319, "seek": 666692, "start": 6683.84, "end": 6689.12, "text": " once because it would just eat up all your GPU's RAM, so you can only look at it a little", "tokens": [1564, 570, 309, 576, 445, 1862, 493, 439, 428, 18407, 311, 14561, 11, 370, 291, 393, 787, 574, 412, 309, 257, 707], "temperature": 0.0, "avg_logprob": -0.19170503263120298, "compression_ratio": 1.3732394366197183, "no_speech_prob": 2.1233540792309213e-06}, {"id": 1320, "seek": 666692, "start": 6689.12, "end": 6692.24, "text": " bit at a time.", "tokens": [857, 412, 257, 565, 13], "temperature": 0.0, "avg_logprob": -0.19170503263120298, "compression_ratio": 1.3732394366197183, "no_speech_prob": 2.1233540792309213e-06}, {"id": 1321, "seek": 669224, "start": 6692.24, "end": 6706.36, "text": " Another way that RNNs are very useful for images is for captioning images.", "tokens": [3996, 636, 300, 45702, 45, 82, 366, 588, 4420, 337, 5267, 307, 337, 31974, 278, 5267, 13], "temperature": 0.0, "avg_logprob": -0.13867215926830584, "compression_ratio": 1.3777777777777778, "no_speech_prob": 3.6119511150900507e-06}, {"id": 1322, "seek": 669224, "start": 6706.36, "end": 6714.36, "text": " And so we'll talk a lot more about this in the next year's course, but have a think about", "tokens": [400, 370, 321, 603, 751, 257, 688, 544, 466, 341, 294, 264, 958, 1064, 311, 1164, 11, 457, 362, 257, 519, 466], "temperature": 0.0, "avg_logprob": -0.13867215926830584, "compression_ratio": 1.3777777777777778, "no_speech_prob": 3.6119511150900507e-06}, {"id": 1323, "seek": 669224, "start": 6714.36, "end": 6716.48, "text": " this in the meantime.", "tokens": [341, 294, 264, 14991, 13], "temperature": 0.0, "avg_logprob": -0.13867215926830584, "compression_ratio": 1.3777777777777778, "no_speech_prob": 3.6119511150900507e-06}, {"id": 1324, "seek": 671648, "start": 6716.48, "end": 6726.679999999999, "text": " If we've got an image, then a CNN can turn that into a vector representation of that", "tokens": [759, 321, 600, 658, 364, 3256, 11, 550, 257, 24859, 393, 1261, 300, 666, 257, 8062, 10290, 295, 300], "temperature": 0.0, "avg_logprob": -0.16468374413180065, "compression_ratio": 1.6368421052631579, "no_speech_prob": 6.339135779853677e-06}, {"id": 1325, "seek": 671648, "start": 6726.679999999999, "end": 6728.139999999999, "text": " image.", "tokens": [3256, 13], "temperature": 0.0, "avg_logprob": -0.16468374413180065, "compression_ratio": 1.6368421052631579, "no_speech_prob": 6.339135779853677e-06}, {"id": 1326, "seek": 671648, "start": 6728.139999999999, "end": 6734.719999999999, "text": " For example, we could chuck it through VGG and take the penultimate layer's activations.", "tokens": [1171, 1365, 11, 321, 727, 20870, 309, 807, 691, 27561, 293, 747, 264, 3435, 723, 2905, 4583, 311, 2430, 763, 13], "temperature": 0.0, "avg_logprob": -0.16468374413180065, "compression_ratio": 1.6368421052631579, "no_speech_prob": 6.339135779853677e-06}, {"id": 1327, "seek": 671648, "start": 6734.719999999999, "end": 6739.959999999999, "text": " There's all kinds of things we could do, but in some way we can turn an image and turn", "tokens": [821, 311, 439, 3685, 295, 721, 321, 727, 360, 11, 457, 294, 512, 636, 321, 393, 1261, 364, 3256, 293, 1261], "temperature": 0.0, "avg_logprob": -0.16468374413180065, "compression_ratio": 1.6368421052631579, "no_speech_prob": 6.339135779853677e-06}, {"id": 1328, "seek": 671648, "start": 6739.959999999999, "end": 6746.28, "text": " it into some vector representation of that.", "tokens": [309, 666, 512, 8062, 10290, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.16468374413180065, "compression_ratio": 1.6368421052631579, "no_speech_prob": 6.339135779853677e-06}, {"id": 1329, "seek": 674628, "start": 6746.28, "end": 6748.78, "text": " We could do the same thing to a sentence.", "tokens": [492, 727, 360, 264, 912, 551, 281, 257, 8174, 13], "temperature": 0.0, "avg_logprob": -0.14495209724672378, "compression_ratio": 1.469387755102041, "no_speech_prob": 3.2887394354474964e-06}, {"id": 1330, "seek": 674628, "start": 6748.78, "end": 6756.08, "text": " We can take a sentence consisting of a number of words and we can stick that through a RNN", "tokens": [492, 393, 747, 257, 8174, 33921, 295, 257, 1230, 295, 2283, 293, 321, 393, 2897, 300, 807, 257, 45702, 45], "temperature": 0.0, "avg_logprob": -0.14495209724672378, "compression_ratio": 1.469387755102041, "no_speech_prob": 3.2887394354474964e-06}, {"id": 1331, "seek": 674628, "start": 6756.08, "end": 6761.719999999999, "text": " and at the end of it, we will get some state.", "tokens": [293, 412, 264, 917, 295, 309, 11, 321, 486, 483, 512, 1785, 13], "temperature": 0.0, "avg_logprob": -0.14495209724672378, "compression_ratio": 1.469387755102041, "no_speech_prob": 3.2887394354474964e-06}, {"id": 1332, "seek": 674628, "start": 6761.719999999999, "end": 6767.48, "text": " And that state is also just a vector.", "tokens": [400, 300, 1785, 307, 611, 445, 257, 8062, 13], "temperature": 0.0, "avg_logprob": -0.14495209724672378, "compression_ratio": 1.469387755102041, "no_speech_prob": 3.2887394354474964e-06}, {"id": 1333, "seek": 676748, "start": 6767.48, "end": 6779.04, "text": " What we could then do is learn a neural network which maps the picture to the text, assuming", "tokens": [708, 321, 727, 550, 360, 307, 1466, 257, 18161, 3209, 597, 11317, 264, 3036, 281, 264, 2487, 11, 11926], "temperature": 0.0, "avg_logprob": -0.11536417468901604, "compression_ratio": 1.5542857142857143, "no_speech_prob": 2.7264638902124716e-06}, {"id": 1334, "seek": 676748, "start": 6779.04, "end": 6787.12, "text": " that this sentence was actually originally a caption that had been created for this image.", "tokens": [300, 341, 8174, 390, 767, 7993, 257, 31974, 300, 632, 668, 2942, 337, 341, 3256, 13], "temperature": 0.0, "avg_logprob": -0.11536417468901604, "compression_ratio": 1.5542857142857143, "no_speech_prob": 2.7264638902124716e-06}, {"id": 1335, "seek": 676748, "start": 6787.12, "end": 6795.5199999999995, "text": " And so in that way, if we can learn a mapping from some representation of the image that", "tokens": [400, 370, 294, 300, 636, 11, 498, 321, 393, 1466, 257, 18350, 490, 512, 10290, 295, 264, 3256, 300], "temperature": 0.0, "avg_logprob": -0.11536417468901604, "compression_ratio": 1.5542857142857143, "no_speech_prob": 2.7264638902124716e-06}, {"id": 1336, "seek": 679552, "start": 6795.52, "end": 6803.360000000001, "text": " came out of a CNN to some representation of a sentence which came out of an RNN, then", "tokens": [1361, 484, 295, 257, 24859, 281, 512, 10290, 295, 257, 8174, 597, 1361, 484, 295, 364, 45702, 45, 11, 550], "temperature": 0.0, "avg_logprob": -0.1353306827774967, "compression_ratio": 1.7035175879396984, "no_speech_prob": 9.516139471088536e-06}, {"id": 1337, "seek": 679552, "start": 6803.360000000001, "end": 6809.360000000001, "text": " we could basically reverse that in order to generate captions for an image.", "tokens": [321, 727, 1936, 9943, 300, 294, 1668, 281, 8460, 44832, 337, 364, 3256, 13], "temperature": 0.0, "avg_logprob": -0.1353306827774967, "compression_ratio": 1.7035175879396984, "no_speech_prob": 9.516139471088536e-06}, {"id": 1338, "seek": 679552, "start": 6809.360000000001, "end": 6813.360000000001, "text": " So basically what we could then do is we could take some new image that we've never seen", "tokens": [407, 1936, 437, 321, 727, 550, 360, 307, 321, 727, 747, 512, 777, 3256, 300, 321, 600, 1128, 1612], "temperature": 0.0, "avg_logprob": -0.1353306827774967, "compression_ratio": 1.7035175879396984, "no_speech_prob": 9.516139471088536e-06}, {"id": 1339, "seek": 679552, "start": 6813.360000000001, "end": 6820.84, "text": " before, chuck it through the CNN to get our state out, and then we could figure out what", "tokens": [949, 11, 20870, 309, 807, 264, 24859, 281, 483, 527, 1785, 484, 11, 293, 550, 321, 727, 2573, 484, 437], "temperature": 0.0, "avg_logprob": -0.1353306827774967, "compression_ratio": 1.7035175879396984, "no_speech_prob": 9.516139471088536e-06}, {"id": 1340, "seek": 682084, "start": 6820.84, "end": 6829.360000000001, "text": " RNN state we would expect would be attached to that based on this neural net that we had", "tokens": [45702, 45, 1785, 321, 576, 2066, 576, 312, 8570, 281, 300, 2361, 322, 341, 18161, 2533, 300, 321, 632], "temperature": 0.0, "avg_logprob": -0.2129429743840144, "compression_ratio": 1.5280898876404494, "no_speech_prob": 9.080405106942635e-06}, {"id": 1341, "seek": 682084, "start": 6829.360000000001, "end": 6836.58, "text": " learned, and then we can basically do a sequence generation just like we have been today and", "tokens": [3264, 11, 293, 550, 321, 393, 1936, 360, 257, 8310, 5125, 445, 411, 321, 362, 668, 965, 293], "temperature": 0.0, "avg_logprob": -0.2129429743840144, "compression_ratio": 1.5280898876404494, "no_speech_prob": 9.080405106942635e-06}, {"id": 1342, "seek": 682084, "start": 6836.58, "end": 6839.6, "text": " generate a sequence of words.", "tokens": [8460, 257, 8310, 295, 2283, 13], "temperature": 0.0, "avg_logprob": -0.2129429743840144, "compression_ratio": 1.5280898876404494, "no_speech_prob": 9.080405106942635e-06}, {"id": 1343, "seek": 682084, "start": 6839.6, "end": 6850.72, "text": " And this is roughly how these image captioning systems work.", "tokens": [400, 341, 307, 9810, 577, 613, 3256, 31974, 278, 3652, 589, 13], "temperature": 0.0, "avg_logprob": -0.2129429743840144, "compression_ratio": 1.5280898876404494, "no_speech_prob": 9.080405106942635e-06}, {"id": 1344, "seek": 685072, "start": 6850.72, "end": 6860.280000000001, "text": " So finally, the only other way in which I've seen RNNs applied to images is for really", "tokens": [407, 2721, 11, 264, 787, 661, 636, 294, 597, 286, 600, 1612, 45702, 45, 82, 6456, 281, 5267, 307, 337, 534], "temperature": 0.0, "avg_logprob": -0.18286551066807338, "compression_ratio": 1.4046242774566473, "no_speech_prob": 4.356850240583299e-06}, {"id": 1345, "seek": 685072, "start": 6860.280000000001, "end": 6866.64, "text": " big 3D images, for example, like in medical imaging.", "tokens": [955, 805, 35, 5267, 11, 337, 1365, 11, 411, 294, 4625, 25036, 13], "temperature": 0.0, "avg_logprob": -0.18286551066807338, "compression_ratio": 1.4046242774566473, "no_speech_prob": 4.356850240583299e-06}, {"id": 1346, "seek": 685072, "start": 6866.64, "end": 6872.52, "text": " So if you've got some MRI that's basically a series of labels, it's too big to look at", "tokens": [407, 498, 291, 600, 658, 512, 32812, 300, 311, 1936, 257, 2638, 295, 16949, 11, 309, 311, 886, 955, 281, 574, 412], "temperature": 0.0, "avg_logprob": -0.18286551066807338, "compression_ratio": 1.4046242774566473, "no_speech_prob": 4.356850240583299e-06}, {"id": 1347, "seek": 685072, "start": 6872.52, "end": 6874.12, "text": " the whole thing.", "tokens": [264, 1379, 551, 13], "temperature": 0.0, "avg_logprob": -0.18286551066807338, "compression_ratio": 1.4046242774566473, "no_speech_prob": 4.356850240583299e-06}, {"id": 1348, "seek": 687412, "start": 6874.12, "end": 6882.72, "text": " Instead you can use an RNN to start in the top corner and then look one pixel to the", "tokens": [7156, 291, 393, 764, 364, 45702, 45, 281, 722, 294, 264, 1192, 4538, 293, 550, 574, 472, 19261, 281, 264], "temperature": 0.0, "avg_logprob": -0.17909976689502446, "compression_ratio": 1.7692307692307692, "no_speech_prob": 8.801023795967922e-06}, {"id": 1349, "seek": 687412, "start": 6882.72, "end": 6887.64, "text": " left, then one pixel across, then one pixel back, and then it can go down into the next", "tokens": [1411, 11, 550, 472, 19261, 2108, 11, 550, 472, 19261, 646, 11, 293, 550, 309, 393, 352, 760, 666, 264, 958], "temperature": 0.0, "avg_logprob": -0.17909976689502446, "compression_ratio": 1.7692307692307692, "no_speech_prob": 8.801023795967922e-06}, {"id": 1350, "seek": 687412, "start": 6887.64, "end": 6888.64, "text": " layer.", "tokens": [4583, 13], "temperature": 0.0, "avg_logprob": -0.17909976689502446, "compression_ratio": 1.7692307692307692, "no_speech_prob": 8.801023795967922e-06}, {"id": 1351, "seek": 687412, "start": 6888.64, "end": 6891.84, "text": " It can gradually look one pixel at a time.", "tokens": [467, 393, 13145, 574, 472, 19261, 412, 257, 565, 13], "temperature": 0.0, "avg_logprob": -0.17909976689502446, "compression_ratio": 1.7692307692307692, "no_speech_prob": 8.801023795967922e-06}, {"id": 1352, "seek": 687412, "start": 6891.84, "end": 6895.32, "text": " And it can do that and gradually cover the whole thing.", "tokens": [400, 309, 393, 360, 300, 293, 13145, 2060, 264, 1379, 551, 13], "temperature": 0.0, "avg_logprob": -0.17909976689502446, "compression_ratio": 1.7692307692307692, "no_speech_prob": 8.801023795967922e-06}, {"id": 1353, "seek": 687412, "start": 6895.32, "end": 6903.84, "text": " And in that way, it's gradually able to generate state about what is contained in this 3D", "tokens": [400, 294, 300, 636, 11, 309, 311, 13145, 1075, 281, 8460, 1785, 466, 437, 307, 16212, 294, 341, 805, 35], "temperature": 0.0, "avg_logprob": -0.17909976689502446, "compression_ratio": 1.7692307692307692, "no_speech_prob": 8.801023795967922e-06}, {"id": 1354, "seek": 690384, "start": 6903.84, "end": 6906.08, "text": " volume.", "tokens": [5523, 13], "temperature": 0.0, "avg_logprob": -0.15311020299008019, "compression_ratio": 1.6318181818181818, "no_speech_prob": 4.029435331176501e-06}, {"id": 1355, "seek": 690384, "start": 6906.08, "end": 6913.72, "text": " And so this is not something which is very widely used, at least at this point, but I", "tokens": [400, 370, 341, 307, 406, 746, 597, 307, 588, 13371, 1143, 11, 412, 1935, 412, 341, 935, 11, 457, 286], "temperature": 0.0, "avg_logprob": -0.15311020299008019, "compression_ratio": 1.6318181818181818, "no_speech_prob": 4.029435331176501e-06}, {"id": 1356, "seek": 690384, "start": 6913.72, "end": 6916.32, "text": " think it's worth thinking about.", "tokens": [519, 309, 311, 3163, 1953, 466, 13], "temperature": 0.0, "avg_logprob": -0.15311020299008019, "compression_ratio": 1.6318181818181818, "no_speech_prob": 4.029435331176501e-06}, {"id": 1357, "seek": 690384, "start": 6916.32, "end": 6919.360000000001, "text": " Because again, you could combine this with a CNN.", "tokens": [1436, 797, 11, 291, 727, 10432, 341, 365, 257, 24859, 13], "temperature": 0.0, "avg_logprob": -0.15311020299008019, "compression_ratio": 1.6318181818181818, "no_speech_prob": 4.029435331176501e-06}, {"id": 1358, "seek": 690384, "start": 6919.360000000001, "end": 6927.82, "text": " Maybe you could have a CNN that looks at large chunks of this MRI at a time and generate", "tokens": [2704, 291, 727, 362, 257, 24859, 300, 1542, 412, 2416, 24004, 295, 341, 32812, 412, 257, 565, 293, 8460], "temperature": 0.0, "avg_logprob": -0.15311020299008019, "compression_ratio": 1.6318181818181818, "no_speech_prob": 4.029435331176501e-06}, {"id": 1359, "seek": 690384, "start": 6927.82, "end": 6933.64, "text": " state for each of these chunks, and then maybe you could use an RNN to go through the chunks.", "tokens": [1785, 337, 1184, 295, 613, 24004, 11, 293, 550, 1310, 291, 727, 764, 364, 45702, 45, 281, 352, 807, 264, 24004, 13], "temperature": 0.0, "avg_logprob": -0.15311020299008019, "compression_ratio": 1.6318181818181818, "no_speech_prob": 4.029435331176501e-06}, {"id": 1360, "seek": 693364, "start": 6933.64, "end": 6940.64, "text": " There's all kinds of ways basically that you can combine CNNs and RNNs together.", "tokens": [821, 311, 439, 3685, 295, 2098, 1936, 300, 291, 393, 10432, 24859, 82, 293, 45702, 45, 82, 1214, 13], "temperature": 0.0, "avg_logprob": -0.3314927721780444, "compression_ratio": 1.3024691358024691, "no_speech_prob": 3.535528594511561e-05}, {"id": 1361, "seek": 693364, "start": 6940.64, "end": 6953.280000000001, "text": " Question- Can you build a custom layer in the Anno and then mix it with Keras?", "tokens": [14464, 12, 1664, 291, 1322, 257, 2375, 4583, 294, 264, 1107, 1771, 293, 550, 2890, 309, 365, 591, 6985, 30], "temperature": 0.0, "avg_logprob": -0.3314927721780444, "compression_ratio": 1.3024691358024691, "no_speech_prob": 3.535528594511561e-05}, {"id": 1362, "seek": 693364, "start": 6953.280000000001, "end": 6955.52, "text": " Answer- Oh for sure.", "tokens": [24545, 12, 876, 337, 988, 13], "temperature": 0.0, "avg_logprob": -0.3314927721780444, "compression_ratio": 1.3024691358024691, "no_speech_prob": 3.535528594511561e-05}, {"id": 1363, "seek": 693364, "start": 6955.52, "end": 6959.04, "text": " In fact, it's incredibly easy.", "tokens": [682, 1186, 11, 309, 311, 6252, 1858, 13], "temperature": 0.0, "avg_logprob": -0.3314927721780444, "compression_ratio": 1.3024691358024691, "no_speech_prob": 3.535528594511561e-05}, {"id": 1364, "seek": 695904, "start": 6959.04, "end": 6978.16, "text": " So if you go Keras, custom layer, there's lots of examples of them that you'll generally", "tokens": [407, 498, 291, 352, 591, 6985, 11, 2375, 4583, 11, 456, 311, 3195, 295, 5110, 295, 552, 300, 291, 603, 5101], "temperature": 0.0, "avg_logprob": -0.24960681915283203, "compression_ratio": 1.3308823529411764, "no_speech_prob": 1.1659356459858827e-05}, {"id": 1365, "seek": 695904, "start": 6978.16, "end": 6979.16, "text": " find.", "tokens": [915, 13], "temperature": 0.0, "avg_logprob": -0.24960681915283203, "compression_ratio": 1.3308823529411764, "no_speech_prob": 1.1659356459858827e-05}, {"id": 1366, "seek": 695904, "start": 6979.16, "end": 6985.6, "text": " They're generally in the GitHub issues for Keras, where people will show, I was trying", "tokens": [814, 434, 5101, 294, 264, 23331, 2663, 337, 591, 6985, 11, 689, 561, 486, 855, 11, 286, 390, 1382], "temperature": 0.0, "avg_logprob": -0.24960681915283203, "compression_ratio": 1.3308823529411764, "no_speech_prob": 1.1659356459858827e-05}, {"id": 1367, "seek": 698560, "start": 6985.6, "end": 6992.360000000001, "text": " to build this layer and I had this problem, but it's kind of a good way to see how to", "tokens": [281, 1322, 341, 4583, 293, 286, 632, 341, 1154, 11, 457, 309, 311, 733, 295, 257, 665, 636, 281, 536, 577, 281], "temperature": 0.0, "avg_logprob": -0.17356680179464407, "compression_ratio": 1.4326241134751774, "no_speech_prob": 1.6187232176889665e-05}, {"id": 1368, "seek": 698560, "start": 6992.360000000001, "end": 6993.6, "text": " build them.", "tokens": [1322, 552, 13], "temperature": 0.0, "avg_logprob": -0.17356680179464407, "compression_ratio": 1.4326241134751774, "no_speech_prob": 1.6187232176889665e-05}, {"id": 1369, "seek": 698560, "start": 6993.6, "end": 7009.72, "text": " The other thing I find really useful to do is to actually look at the definition of the", "tokens": [440, 661, 551, 286, 915, 534, 4420, 281, 360, 307, 281, 767, 574, 412, 264, 7123, 295, 264], "temperature": 0.0, "avg_logprob": -0.17356680179464407, "compression_ratio": 1.4326241134751774, "no_speech_prob": 1.6187232176889665e-05}, {"id": 1370, "seek": 698560, "start": 7009.72, "end": 7012.92, "text": " layers in Keras.", "tokens": [7914, 294, 591, 6985, 13], "temperature": 0.0, "avg_logprob": -0.17356680179464407, "compression_ratio": 1.4326241134751774, "no_speech_prob": 1.6187232176889665e-05}, {"id": 1371, "seek": 701292, "start": 7012.92, "end": 7025.32, "text": " So one of the things I actually did was I created this little thing called PyPath which", "tokens": [407, 472, 295, 264, 721, 286, 767, 630, 390, 286, 2942, 341, 707, 551, 1219, 9953, 47, 998, 597], "temperature": 0.0, "avg_logprob": -0.20404062879846452, "compression_ratio": 1.4090909090909092, "no_speech_prob": 1.777826400939375e-05}, {"id": 1372, "seek": 701292, "start": 7025.32, "end": 7037.4400000000005, "text": " allows me to put in any Python module and it returns the directory that that module", "tokens": [4045, 385, 281, 829, 294, 604, 15329, 10088, 293, 309, 11247, 264, 21120, 300, 300, 10088], "temperature": 0.0, "avg_logprob": -0.20404062879846452, "compression_ratio": 1.4090909090909092, "no_speech_prob": 1.777826400939375e-05}, {"id": 1373, "seek": 701292, "start": 7037.4400000000005, "end": 7040.8, "text": " is defined in.", "tokens": [307, 7642, 294, 13], "temperature": 0.0, "avg_logprob": -0.20404062879846452, "compression_ratio": 1.4090909090909092, "no_speech_prob": 1.777826400939375e-05}, {"id": 1374, "seek": 704080, "start": 7040.8, "end": 7049.6, "text": " And then so I can go, let's have a look at how any particular layer is defined.", "tokens": [400, 550, 370, 286, 393, 352, 11, 718, 311, 362, 257, 574, 412, 577, 604, 1729, 4583, 307, 7642, 13], "temperature": 0.0, "avg_logprob": -0.23925432562828064, "compression_ratio": 1.408450704225352, "no_speech_prob": 3.0894771043676883e-06}, {"id": 1375, "seek": 704080, "start": 7049.6, "end": 7054.0, "text": " So let's say I want to look at pooling.", "tokens": [407, 718, 311, 584, 286, 528, 281, 574, 412, 7005, 278, 13], "temperature": 0.0, "avg_logprob": -0.23925432562828064, "compression_ratio": 1.408450704225352, "no_speech_prob": 3.0894771043676883e-06}, {"id": 1376, "seek": 704080, "start": 7054.0, "end": 7062.8, "text": " So here is a maxPooling1D layer and you can see it's defined in 9 lines of code.", "tokens": [407, 510, 307, 257, 11469, 47, 1092, 278, 16, 35, 4583, 293, 291, 393, 536, 309, 311, 7642, 294, 1722, 3876, 295, 3089, 13], "temperature": 0.0, "avg_logprob": -0.23925432562828064, "compression_ratio": 1.408450704225352, "no_speech_prob": 3.0894771043676883e-06}, {"id": 1377, "seek": 706280, "start": 7062.8, "end": 7071.04, "text": " And so generally speaking, you can kind of see that layers don't take very much code", "tokens": [400, 370, 5101, 4124, 11, 291, 393, 733, 295, 536, 300, 7914, 500, 380, 747, 588, 709, 3089], "temperature": 0.0, "avg_logprob": -0.29255018001649435, "compression_ratio": 1.6157894736842104, "no_speech_prob": 1.5294042441382771e-06}, {"id": 1378, "seek": 706280, "start": 7071.04, "end": 7072.04, "text": " at all.", "tokens": [412, 439, 13], "temperature": 0.0, "avg_logprob": -0.29255018001649435, "compression_ratio": 1.6157894736842104, "no_speech_prob": 1.5294042441382771e-06}, {"id": 1379, "seek": 706280, "start": 7072.04, "end": 7073.04, "text": " Question from the audience.", "tokens": [14464, 490, 264, 4034, 13], "temperature": 0.0, "avg_logprob": -0.29255018001649435, "compression_ratio": 1.6157894736842104, "no_speech_prob": 1.5294042441382771e-06}, {"id": 1380, "seek": 706280, "start": 7073.04, "end": 7074.04, "text": " Question from the audience.", "tokens": [14464, 490, 264, 4034, 13], "temperature": 0.0, "avg_logprob": -0.29255018001649435, "compression_ratio": 1.6157894736842104, "no_speech_prob": 1.5294042441382771e-06}, {"id": 1381, "seek": 706280, "start": 7074.04, "end": 7082.04, "text": " Could we, given a caption, create an image?", "tokens": [7497, 321, 11, 2212, 257, 31974, 11, 1884, 364, 3256, 30], "temperature": 0.0, "avg_logprob": -0.29255018001649435, "compression_ratio": 1.6157894736842104, "no_speech_prob": 1.5294042441382771e-06}, {"id": 1382, "seek": 706280, "start": 7082.04, "end": 7084.4800000000005, "text": " You can absolutely create an image from a caption.", "tokens": [509, 393, 3122, 1884, 364, 3256, 490, 257, 31974, 13], "temperature": 0.0, "avg_logprob": -0.29255018001649435, "compression_ratio": 1.6157894736842104, "no_speech_prob": 1.5294042441382771e-06}, {"id": 1383, "seek": 706280, "start": 7084.4800000000005, "end": 7090.12, "text": " There's a lot of image generation stuff going on at the moment.", "tokens": [821, 311, 257, 688, 295, 3256, 5125, 1507, 516, 322, 412, 264, 1623, 13], "temperature": 0.0, "avg_logprob": -0.29255018001649435, "compression_ratio": 1.6157894736842104, "no_speech_prob": 1.5294042441382771e-06}, {"id": 1384, "seek": 709012, "start": 7090.12, "end": 7096.0, "text": " It's not at a point that it's probably useful for anything in practice.", "tokens": [467, 311, 406, 412, 257, 935, 300, 309, 311, 1391, 4420, 337, 1340, 294, 3124, 13], "temperature": 0.0, "avg_logprob": -0.19504484879343134, "compression_ratio": 1.604, "no_speech_prob": 5.955022970738355e-06}, {"id": 1385, "seek": 709012, "start": 7096.0, "end": 7102.5599999999995, "text": " It's more like an interesting research journey I guess.", "tokens": [467, 311, 544, 411, 364, 1880, 2132, 4671, 286, 2041, 13], "temperature": 0.0, "avg_logprob": -0.19504484879343134, "compression_ratio": 1.604, "no_speech_prob": 5.955022970738355e-06}, {"id": 1386, "seek": 709012, "start": 7102.5599999999995, "end": 7106.76, "text": " So generally speaking, this is in the area called generative models.", "tokens": [407, 5101, 4124, 11, 341, 307, 294, 264, 1859, 1219, 1337, 1166, 5245, 13], "temperature": 0.0, "avg_logprob": -0.19504484879343134, "compression_ratio": 1.604, "no_speech_prob": 5.955022970738355e-06}, {"id": 1387, "seek": 709012, "start": 7106.76, "end": 7110.08, "text": " And we'll be looking at generative models next year because they're very important for", "tokens": [400, 321, 603, 312, 1237, 412, 1337, 1166, 5245, 958, 1064, 570, 436, 434, 588, 1021, 337], "temperature": 0.0, "avg_logprob": -0.19504484879343134, "compression_ratio": 1.604, "no_speech_prob": 5.955022970738355e-06}, {"id": 1388, "seek": 709012, "start": 7110.08, "end": 7113.96, "text": " unsupervised and semi-supervised learning.", "tokens": [2693, 12879, 24420, 293, 12909, 12, 48172, 24420, 2539, 13], "temperature": 0.0, "avg_logprob": -0.19504484879343134, "compression_ratio": 1.604, "no_speech_prob": 5.955022970738355e-06}, {"id": 1389, "seek": 709012, "start": 7113.96, "end": 7118.28, "text": " And what could get the best performance on a document classification task?", "tokens": [400, 437, 727, 483, 264, 1151, 3389, 322, 257, 4166, 21538, 5633, 30], "temperature": 0.0, "avg_logprob": -0.19504484879343134, "compression_ratio": 1.604, "no_speech_prob": 5.955022970738355e-06}, {"id": 1390, "seek": 711828, "start": 7118.28, "end": 7121.08, "text": " CNN and RNN or both?", "tokens": [24859, 293, 45702, 45, 420, 1293, 30], "temperature": 0.0, "avg_logprob": -0.20217667096926842, "compression_ratio": 1.5517241379310345, "no_speech_prob": 5.594273261522176e-06}, {"id": 1391, "seek": 711828, "start": 7121.08, "end": 7123.08, "text": " That's a great question.", "tokens": [663, 311, 257, 869, 1168, 13], "temperature": 0.0, "avg_logprob": -0.20217667096926842, "compression_ratio": 1.5517241379310345, "no_speech_prob": 5.594273261522176e-06}, {"id": 1392, "seek": 711828, "start": 7123.08, "end": 7128.04, "text": " So let's go back to sentiment analysis.", "tokens": [407, 718, 311, 352, 646, 281, 16149, 5215, 13], "temperature": 0.0, "avg_logprob": -0.20217667096926842, "compression_ratio": 1.5517241379310345, "no_speech_prob": 5.594273261522176e-06}, {"id": 1393, "seek": 711828, "start": 7128.04, "end": 7133.5599999999995, "text": " And to remind ourselves, when we looked at sentiment analysis for IMDB, the best result", "tokens": [400, 281, 4160, 4175, 11, 562, 321, 2956, 412, 16149, 5215, 337, 21463, 27735, 11, 264, 1151, 1874], "temperature": 0.0, "avg_logprob": -0.20217667096926842, "compression_ratio": 1.5517241379310345, "no_speech_prob": 5.594273261522176e-06}, {"id": 1394, "seek": 711828, "start": 7133.5599999999995, "end": 7139.8, "text": " we got came from a multi-size convolutional neural network where we basically took a bunch", "tokens": [321, 658, 1361, 490, 257, 4825, 12, 27553, 45216, 304, 18161, 3209, 689, 321, 1936, 1890, 257, 3840], "temperature": 0.0, "avg_logprob": -0.20217667096926842, "compression_ratio": 1.5517241379310345, "no_speech_prob": 5.594273261522176e-06}, {"id": 1395, "seek": 711828, "start": 7139.8, "end": 7143.04, "text": " of convolutional neural networks of varying sizes.", "tokens": [295, 45216, 304, 18161, 9590, 295, 22984, 11602, 13], "temperature": 0.0, "avg_logprob": -0.20217667096926842, "compression_ratio": 1.5517241379310345, "no_speech_prob": 5.594273261522176e-06}, {"id": 1396, "seek": 714304, "start": 7143.04, "end": 7153.2, "text": " A simple convolutional neural network was nearly as good.", "tokens": [316, 2199, 45216, 304, 18161, 3209, 390, 6217, 382, 665, 13], "temperature": 0.0, "avg_logprob": -0.14231010193520405, "compression_ratio": 1.3650793650793651, "no_speech_prob": 6.643383585469564e-06}, {"id": 1397, "seek": 714304, "start": 7153.2, "end": 7166.28, "text": " I actually tried an LSTM for this, and I found the accuracy that I got was less good than", "tokens": [286, 767, 3031, 364, 441, 6840, 44, 337, 341, 11, 293, 286, 1352, 264, 14170, 300, 286, 658, 390, 1570, 665, 813], "temperature": 0.0, "avg_logprob": -0.14231010193520405, "compression_ratio": 1.3650793650793651, "no_speech_prob": 6.643383585469564e-06}, {"id": 1398, "seek": 714304, "start": 7166.28, "end": 7168.68, "text": " the accuracy of the CNN.", "tokens": [264, 14170, 295, 264, 24859, 13], "temperature": 0.0, "avg_logprob": -0.14231010193520405, "compression_ratio": 1.3650793650793651, "no_speech_prob": 6.643383585469564e-06}, {"id": 1399, "seek": 716868, "start": 7168.68, "end": 7176.320000000001, "text": " And I think the reason for this is that when you have a whole movie review, which is a", "tokens": [400, 286, 519, 264, 1778, 337, 341, 307, 300, 562, 291, 362, 257, 1379, 3169, 3131, 11, 597, 307, 257], "temperature": 0.0, "avg_logprob": -0.15649789922377644, "compression_ratio": 1.642512077294686, "no_speech_prob": 2.796837179630529e-05}, {"id": 1400, "seek": 716868, "start": 7176.320000000001, "end": 7184.18, "text": " few paragraphs, the information you can get just by looking at a few words at a time is", "tokens": [1326, 48910, 11, 264, 1589, 291, 393, 483, 445, 538, 1237, 412, 257, 1326, 2283, 412, 257, 565, 307], "temperature": 0.0, "avg_logprob": -0.15649789922377644, "compression_ratio": 1.642512077294686, "no_speech_prob": 2.796837179630529e-05}, {"id": 1401, "seek": 716868, "start": 7184.18, "end": 7190.02, "text": " enough to tell you whether this is a positive review or a negative review.", "tokens": [1547, 281, 980, 291, 1968, 341, 307, 257, 3353, 3131, 420, 257, 3671, 3131, 13], "temperature": 0.0, "avg_logprob": -0.15649789922377644, "compression_ratio": 1.642512077294686, "no_speech_prob": 2.796837179630529e-05}, {"id": 1402, "seek": 716868, "start": 7190.02, "end": 7195.12, "text": " If you see a sequence of 5 words like, this is totally shit, you can probably learn that's", "tokens": [759, 291, 536, 257, 8310, 295, 1025, 2283, 411, 11, 341, 307, 3879, 4611, 11, 291, 393, 1391, 1466, 300, 311], "temperature": 0.0, "avg_logprob": -0.15649789922377644, "compression_ratio": 1.642512077294686, "no_speech_prob": 2.796837179630529e-05}, {"id": 1403, "seek": 719512, "start": 7195.12, "end": 7200.4, "text": " not a good thing, or else if this is totally awesome, you can probably learn that is a", "tokens": [406, 257, 665, 551, 11, 420, 1646, 498, 341, 307, 3879, 3476, 11, 291, 393, 1391, 1466, 300, 307, 257], "temperature": 0.0, "avg_logprob": -0.20312123071579707, "compression_ratio": 1.49079754601227, "no_speech_prob": 1.2029515346512198e-05}, {"id": 1404, "seek": 719512, "start": 7200.4, "end": 7201.88, "text": " good thing.", "tokens": [665, 551, 13], "temperature": 0.0, "avg_logprob": -0.20312123071579707, "compression_ratio": 1.49079754601227, "no_speech_prob": 1.2029515346512198e-05}, {"id": 1405, "seek": 719512, "start": 7201.88, "end": 7209.92, "text": " The amount of nuance built into reading sentence word by word an entire review, it just doesn't", "tokens": [440, 2372, 295, 42625, 3094, 666, 3760, 8174, 1349, 538, 1349, 364, 2302, 3131, 11, 309, 445, 1177, 380], "temperature": 0.0, "avg_logprob": -0.20312123071579707, "compression_ratio": 1.49079754601227, "no_speech_prob": 1.2029515346512198e-05}, {"id": 1406, "seek": 719512, "start": 7209.92, "end": 7213.36, "text": " seem like there's any need for that in practice.", "tokens": [1643, 411, 456, 311, 604, 643, 337, 300, 294, 3124, 13], "temperature": 0.0, "avg_logprob": -0.20312123071579707, "compression_ratio": 1.49079754601227, "no_speech_prob": 1.2029515346512198e-05}, {"id": 1407, "seek": 721336, "start": 7213.36, "end": 7225.32, "text": " So in general, once you get to a certain sized piece of text, like a paragraph or two, there", "tokens": [407, 294, 2674, 11, 1564, 291, 483, 281, 257, 1629, 20004, 2522, 295, 2487, 11, 411, 257, 18865, 420, 732, 11, 456], "temperature": 0.0, "avg_logprob": -0.19377658367156983, "compression_ratio": 1.5179487179487179, "no_speech_prob": 4.7108933358686045e-06}, {"id": 1408, "seek": 721336, "start": 7225.32, "end": 7235.08, "text": " doesn't seem to be any sign that RNNs are helpful, at least at this stage.", "tokens": [1177, 380, 1643, 281, 312, 604, 1465, 300, 45702, 45, 82, 366, 4961, 11, 412, 1935, 412, 341, 3233, 13], "temperature": 0.0, "avg_logprob": -0.19377658367156983, "compression_ratio": 1.5179487179487179, "no_speech_prob": 4.7108933358686045e-06}, {"id": 1409, "seek": 721336, "start": 7235.08, "end": 7239.839999999999, "text": " So before I close off, I wanted to show you two little tricks, because I don't spend enough", "tokens": [407, 949, 286, 1998, 766, 11, 286, 1415, 281, 855, 291, 732, 707, 11733, 11, 570, 286, 500, 380, 3496, 1547], "temperature": 0.0, "avg_logprob": -0.19377658367156983, "compression_ratio": 1.5179487179487179, "no_speech_prob": 4.7108933358686045e-06}, {"id": 1410, "seek": 721336, "start": 7239.839999999999, "end": 7241.88, "text": " time showing you cool little tricks.", "tokens": [565, 4099, 291, 1627, 707, 11733, 13], "temperature": 0.0, "avg_logprob": -0.19377658367156983, "compression_ratio": 1.5179487179487179, "no_speech_prob": 4.7108933358686045e-06}, {"id": 1411, "seek": 724188, "start": 7241.88, "end": 7247.24, "text": " So when I was working with Brad today, there were two little tricks that we realized that", "tokens": [407, 562, 286, 390, 1364, 365, 11895, 965, 11, 456, 645, 732, 707, 11733, 300, 321, 5334, 300], "temperature": 0.0, "avg_logprob": -0.17832976254549893, "compression_ratio": 1.4666666666666666, "no_speech_prob": 2.5071092750295065e-05}, {"id": 1412, "seek": 724188, "start": 7247.24, "end": 7250.400000000001, "text": " other people might like to learn about.", "tokens": [661, 561, 1062, 411, 281, 1466, 466, 13], "temperature": 0.0, "avg_logprob": -0.17832976254549893, "compression_ratio": 1.4666666666666666, "no_speech_prob": 2.5071092750295065e-05}, {"id": 1413, "seek": 724188, "start": 7250.400000000001, "end": 7269.08, "text": " The first trick I wanted to point out to you is, if you want to learn about how a function", "tokens": [440, 700, 4282, 286, 1415, 281, 935, 484, 281, 291, 307, 11, 498, 291, 528, 281, 1466, 466, 577, 257, 2445], "temperature": 0.0, "avg_logprob": -0.17832976254549893, "compression_ratio": 1.4666666666666666, "no_speech_prob": 2.5071092750295065e-05}, {"id": 1414, "seek": 726908, "start": 7269.08, "end": 7273.96, "text": " works, what would be a quick way to find out.", "tokens": [1985, 11, 437, 576, 312, 257, 1702, 636, 281, 915, 484, 13], "temperature": 0.0, "avg_logprob": -0.1848095924623551, "compression_ratio": 1.551219512195122, "no_speech_prob": 3.705178096424788e-05}, {"id": 1415, "seek": 726908, "start": 7273.96, "end": 7280.12, "text": " And if you've got a function there on your screen and you hit Shift-Tab, all of the parameters", "tokens": [400, 498, 291, 600, 658, 257, 2445, 456, 322, 428, 2568, 293, 291, 2045, 28304, 12, 51, 455, 11, 439, 295, 264, 9834], "temperature": 0.0, "avg_logprob": -0.1848095924623551, "compression_ratio": 1.551219512195122, "no_speech_prob": 3.705178096424788e-05}, {"id": 1416, "seek": 726908, "start": 7280.12, "end": 7283.0, "text": " to it will pop up.", "tokens": [281, 309, 486, 1665, 493, 13], "temperature": 0.0, "avg_logprob": -0.1848095924623551, "compression_ratio": 1.551219512195122, "no_speech_prob": 3.705178096424788e-05}, {"id": 1417, "seek": 726908, "start": 7283.0, "end": 7290.0, "text": " If you hit Shift-Tab twice, the documentation will pop up.", "tokens": [759, 291, 2045, 28304, 12, 51, 455, 6091, 11, 264, 14333, 486, 1665, 493, 13], "temperature": 0.0, "avg_logprob": -0.1848095924623551, "compression_ratio": 1.551219512195122, "no_speech_prob": 3.705178096424788e-05}, {"id": 1418, "seek": 726908, "start": 7290.0, "end": 7294.4, "text": " So that was one little tip that I wanted you guys to know about, because I think it's pretty", "tokens": [407, 300, 390, 472, 707, 4125, 300, 286, 1415, 291, 1074, 281, 458, 466, 11, 570, 286, 519, 309, 311, 1238], "temperature": 0.0, "avg_logprob": -0.1848095924623551, "compression_ratio": 1.551219512195122, "no_speech_prob": 3.705178096424788e-05}, {"id": 1419, "seek": 726908, "start": 7294.4, "end": 7295.4, "text": " handy.", "tokens": [13239, 13], "temperature": 0.0, "avg_logprob": -0.1848095924623551, "compression_ratio": 1.551219512195122, "no_speech_prob": 3.705178096424788e-05}, {"id": 1420, "seek": 729540, "start": 7295.4, "end": 7299.759999999999, "text": " The second little tip that you may not be aware of is that you can actually run the", "tokens": [440, 1150, 707, 4125, 300, 291, 815, 406, 312, 3650, 295, 307, 300, 291, 393, 767, 1190, 264], "temperature": 0.0, "avg_logprob": -0.20007614862351192, "compression_ratio": 1.6145833333333333, "no_speech_prob": 1.7502656191936694e-05}, {"id": 1421, "seek": 729540, "start": 7299.759999999999, "end": 7303.48, "text": " Python debugger inside Jupyter Notebook.", "tokens": [15329, 24083, 1321, 1854, 22125, 88, 391, 11633, 2939, 13], "temperature": 0.0, "avg_logprob": -0.20007614862351192, "compression_ratio": 1.6145833333333333, "no_speech_prob": 1.7502656191936694e-05}, {"id": 1422, "seek": 729540, "start": 7303.48, "end": 7310.04, "text": " And so today we were trying to do that when we were trying to debug our pure Python RNN.", "tokens": [400, 370, 965, 321, 645, 1382, 281, 360, 300, 562, 321, 645, 1382, 281, 24083, 527, 6075, 15329, 45702, 45, 13], "temperature": 0.0, "avg_logprob": -0.20007614862351192, "compression_ratio": 1.6145833333333333, "no_speech_prob": 1.7502656191936694e-05}, {"id": 1423, "seek": 729540, "start": 7310.04, "end": 7318.28, "text": " So we can see an example of that.", "tokens": [407, 321, 393, 536, 364, 1365, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.20007614862351192, "compression_ratio": 1.6145833333333333, "no_speech_prob": 1.7502656191936694e-05}, {"id": 1424, "seek": 729540, "start": 7318.28, "end": 7323.28, "text": " So let's say we were having some problem inside our loop here.", "tokens": [407, 718, 311, 584, 321, 645, 1419, 512, 1154, 1854, 527, 6367, 510, 13], "temperature": 0.0, "avg_logprob": -0.20007614862351192, "compression_ratio": 1.6145833333333333, "no_speech_prob": 1.7502656191936694e-05}, {"id": 1425, "seek": 732328, "start": 7323.28, "end": 7329.96, "text": " You can go import pdb, that's the Python debugger, and then you can set a breakpoint anywhere.", "tokens": [509, 393, 352, 974, 280, 67, 65, 11, 300, 311, 264, 15329, 24083, 1321, 11, 293, 550, 291, 393, 992, 257, 1821, 6053, 4992, 13], "temperature": 0.0, "avg_logprob": -0.2292171571312881, "compression_ratio": 1.7117903930131004, "no_speech_prob": 5.01469412483857e-06}, {"id": 1426, "seek": 732328, "start": 7329.96, "end": 7334.2, "text": " You can go pdb.setTrace, that's the breakpoint.", "tokens": [509, 393, 352, 280, 67, 65, 13, 3854, 14252, 617, 11, 300, 311, 264, 1821, 6053, 13], "temperature": 0.0, "avg_logprob": -0.2292171571312881, "compression_ratio": 1.7117903930131004, "no_speech_prob": 5.01469412483857e-06}, {"id": 1427, "seek": 732328, "start": 7334.2, "end": 7340.4, "text": " And so now if I run that, as soon as it gets to here, it pops up a little dialog box.", "tokens": [400, 370, 586, 498, 286, 1190, 300, 11, 382, 2321, 382, 309, 2170, 281, 510, 11, 309, 16795, 493, 257, 707, 19308, 2424, 13], "temperature": 0.0, "avg_logprob": -0.2292171571312881, "compression_ratio": 1.7117903930131004, "no_speech_prob": 5.01469412483857e-06}, {"id": 1428, "seek": 732328, "start": 7340.4, "end": 7342.8, "text": " And at this point I can look at anything.", "tokens": [400, 412, 341, 935, 286, 393, 574, 412, 1340, 13], "temperature": 0.0, "avg_logprob": -0.2292171571312881, "compression_ratio": 1.7117903930131004, "no_speech_prob": 5.01469412483857e-06}, {"id": 1429, "seek": 732328, "start": 7342.8, "end": 7346.719999999999, "text": " So for example, I can say, what's the value of err at this point?", "tokens": [407, 337, 1365, 11, 286, 393, 584, 11, 437, 311, 264, 2158, 295, 45267, 412, 341, 935, 30], "temperature": 0.0, "avg_logprob": -0.2292171571312881, "compression_ratio": 1.7117903930131004, "no_speech_prob": 5.01469412483857e-06}, {"id": 1430, "seek": 732328, "start": 7346.719999999999, "end": 7350.04, "text": " And I can say, what are the lines I'm about to execute?", "tokens": [400, 286, 393, 584, 11, 437, 366, 264, 3876, 286, 478, 466, 281, 14483, 30], "temperature": 0.0, "avg_logprob": -0.2292171571312881, "compression_ratio": 1.7117903930131004, "no_speech_prob": 5.01469412483857e-06}, {"id": 1431, "seek": 735004, "start": 7350.04, "end": 7355.5199999999995, "text": " And I can say, okay, execute the next one line.", "tokens": [400, 286, 393, 584, 11, 1392, 11, 14483, 264, 958, 472, 1622, 13], "temperature": 0.0, "avg_logprob": -0.22754962627704328, "compression_ratio": 1.655430711610487, "no_speech_prob": 8.664576853334438e-06}, {"id": 1432, "seek": 735004, "start": 7355.5199999999995, "end": 7360.92, "text": " If you want to learn about the Python debugger, just Google for a Python debugger.", "tokens": [759, 291, 528, 281, 1466, 466, 264, 15329, 24083, 1321, 11, 445, 3329, 337, 257, 15329, 24083, 1321, 13], "temperature": 0.0, "avg_logprob": -0.22754962627704328, "compression_ratio": 1.655430711610487, "no_speech_prob": 8.664576853334438e-06}, {"id": 1433, "seek": 735004, "start": 7360.92, "end": 7364.8, "text": " But learning to use the debugger is one of the most helpful things because it lets you", "tokens": [583, 2539, 281, 764, 264, 24083, 1321, 307, 472, 295, 264, 881, 4961, 721, 570, 309, 6653, 291], "temperature": 0.0, "avg_logprob": -0.22754962627704328, "compression_ratio": 1.655430711610487, "no_speech_prob": 8.664576853334438e-06}, {"id": 1434, "seek": 735004, "start": 7364.8, "end": 7370.84, "text": " step through each step of what's going on and see the values of all of your variables", "tokens": [1823, 807, 1184, 1823, 295, 437, 311, 516, 322, 293, 536, 264, 4190, 295, 439, 295, 428, 9102], "temperature": 0.0, "avg_logprob": -0.22754962627704328, "compression_ratio": 1.655430711610487, "no_speech_prob": 8.664576853334438e-06}, {"id": 1435, "seek": 735004, "start": 7370.84, "end": 7374.84, "text": " and do all kinds of cool stuff like that.", "tokens": [293, 360, 439, 3685, 295, 1627, 1507, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.22754962627704328, "compression_ratio": 1.655430711610487, "no_speech_prob": 8.664576853334438e-06}, {"id": 1436, "seek": 735004, "start": 7374.84, "end": 7377.64, "text": " So those were two little tips I thought I would leave you with.", "tokens": [407, 729, 645, 732, 707, 6082, 286, 1194, 286, 576, 1856, 291, 365, 13], "temperature": 0.0, "avg_logprob": -0.22754962627704328, "compression_ratio": 1.655430711610487, "no_speech_prob": 8.664576853334438e-06}, {"id": 1437, "seek": 735004, "start": 7377.64, "end": 7378.64, "text": " So we can finish on a high note.", "tokens": [407, 321, 393, 2413, 322, 257, 1090, 3637, 13], "temperature": 0.0, "avg_logprob": -0.22754962627704328, "compression_ratio": 1.655430711610487, "no_speech_prob": 8.664576853334438e-06}, {"id": 1438, "seek": 737864, "start": 7378.64, "end": 7380.240000000001, "text": " And that's 9 o'clock.", "tokens": [400, 300, 311, 1722, 277, 6, 9023, 13], "temperature": 0.0, "avg_logprob": -0.35647319492540863, "compression_ratio": 0.9090909090909091, "no_speech_prob": 3.6464967706706375e-05}, {"id": 1439, "seek": 738024, "start": 7380.24, "end": 7408.8, "text": " Thanks very much, everybody.", "tokens": [50364, 2561, 588, 709, 11, 2201, 13, 51792], "temperature": 0.0, "avg_logprob": -0.5107313791910807, "compression_ratio": 0.8484848484848485, "no_speech_prob": 6.056770507711917e-05}], "language": "en"}