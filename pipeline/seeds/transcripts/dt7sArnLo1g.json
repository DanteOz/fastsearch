{"text": " Alright, so let's keep going with Notebook 3. So you'll remember last time we were looking at the problem of wanting to classify the sentiment of movie reviews. And we're going to be doing it in this notebook with logistic regression, naive Bayes, and then in Notebook 4, we'll get to see how to do this with neural networks. So it's going to be two different, very different approaches for the same problem. And last time we were really talking, and it's really important to kind of understand how your data is set up and what we have kind of these different methods and dictionaries for using it. And so I just wanted to look at that again briefly. So here, this has been processed into tokens, and we have a number of kind of these special tokens, such as XXUNC for an unknown word, beginning of string or beginning of the text, repetition, that the next word begins with a capital letter, and so on. And then can anyone tell me what I2S does or is? Louder? Into string. Yeah, so that's going from an integer to a string. And why do we need to go between the two? Yeah, and I'll repeat that for the microphone. So for the computer program, you're often using integers. That's how we kind of are indexing our tokens. Anything human oriented, you want to see the string so that you both can understand it as the programmer. Also, the original was in a string, and so we're kind of converting it to integers to make it easier to work with. But sometimes we'll want to switch back. And so we had I2S string and also string2I. And what kind of data structures were I2S string and string2I? List and dictionary, right? So I2S string is a list because you don't actually have to store the integer. It's kind of implicit in the list of the order it's in, whereas you need a dictionary for string2I to be able to look up a string as a key and get back an integer as your value. And so here we saw, yeah, saw part of the kind of dictionary string2I. We created the term document matrix ourselves using a sparse matrix in SciPy. And we talked through what the different ways to store sparse matrices are. Do you guys want to review that today or how do you feel about it? Sorry, his thumbs up for review or thumbs up as you feel good? Feeling good? Does anyone want to review it? Okay, yeah, I see some hands for reviewing it. So we'll just quickly take a look at it again. So the first one we saw was coordinate-wise storage, which I think is the most intuitive, where you've got a value, its row number, and its column number. And so you're basically storing these three arrays that has all your information. Actually, I guess first I should ask, what is a sparse matrix? Maria? Yeah, sparse matrix has lots of zeros and so not as many non-zero values. And I think sometimes more precisely, although this is a little bit flexible, people will say the number of non-zero values is scaling with n and not n squared, if you had like an n by n matrix. So coordinate-wise, I think is the most straightforward, and that's how we'll typically enter, kind of as a human enter our values for sparse matrix. Then one that was a little bit weirder when you first see it is compressed sparse row data structure. And here you're still storing the column number and the value, but you've got this row pointer. And the row pointer is just keeping track of when you've gone on to the next row. And so since we have 0, 1, 2, 3, we've gone to the next row, 4, 5, 6, we've gone to the next row, 7, 8, 9, that's where this 0, 3, 6, 9 comes from. And the thing to note here, is there a relationship between 9, 22, and 1? Shake your head no or nod your head yes. No, yes, I see a few shaking heads no. So this is a little bit confusing how they've written this here. But the 22 and 1 is referring to this entry here. And that has, yeah, not related to this 9. The only thing is if you look at index 3, this is telling us that the 9th entry we've gone on to the third row in a zero based counting system. Or, you know, starting from zero, we've gone row 0, row 1, row 2, row 3 for the 9th entry. But that has nothing to do with this value of 22. So it's a little confusing how they've written it here. We saw another form, which I think is confusing in a different way of writing it. Here where they spaced out the row start index, you know, to show you, but you're not actually storing those spaces. Any questions about the compressed sparse row format? So a benefit of it is it takes even less storage space and also makes accessing your data by rows very quick and easy. And you can convert between them. If you were primarily going to be accessing by columns, you would want to use compressed column sparse storage. And so some of this is kind of keeping in mind how you'll be using your data. And it's relatively efficient to change between these different formats. Yeah, so we saw this last time and we use this to make our term document matrix down here. And actually, I guess we were we were feeding it into a CSR matrix. OK, we saw how to display these as dense. And then I guess I left off with the sorry with an exercise. Was it this one of since the word. OK, got it. Hey, does anyone want to share what they did? Maybe the catch box or catch. OK, so what I did was since that review came from the validation set, I took the valid turned up document matrix. And well, before that, I first looked at I had called string to end for late. Yeah, I got an index of four fifty one. So then I know that, OK, I got to look at the 450 first column in this validation document to a matrix. And I knew that the index of this review was one. So if I look at row one column for fifty one in the validation term document matrix, I get to exactly. Great. Thanks. Yeah. So look up the the integer for late at four fifty one and then you look it up in the validation term document matrix. I notice looking in here, this review has one hundred and forty four tokens total and eighty one distinct tokens. And I can get that from looking at this row in the matrix. It says this is one by six thousand ten with eighty one stored elements. So next exercise or first, I just ask questions about that. How could you convert review data back to text? And here, since we're using a text list, we have these nice kind of like helper attributes of review data and review text. But without just looking up review text, how could you convert review data to text? So take a moment to write the code for that. I use the movie. That's a list. Sorry. To get convert all the integers to strings. Exactly. Yes. Yeah. So use that use the list and you can do this inside of list comprehension if you want to get the movie review.vocab. I to say for a interview that data. And then I think we have one one more exercise confirmed that the review has eighty one distinct tokens. You can get the distinct numbers through a set and then find the exactly. Yeah. So get the length of the set. I'll tell you how many distinct ones. Well, thank you. Hey, it's I just oh, I guess I have a little bit more. Oh, here I show that and I think we talked about this last time that string to end is longer than into string. Since many words are not mapping to unknown and I was kind of just curious about what words were getting mapped to unknown. And so I. But a little loop to go through the items and see if they map to unknown, put them in this list. Just to see what they were. And here, remember, we've capped it at only having six thousand words in our vocabulary. So a lot of these words are. I know words that probably do show up in many movie reviews and later on will switch to right now. We're just working on a sample of the data set. Later on, we'll switch to a larger data set and use a larger vocabulary. But I like doing I like doing some of these exercises of just making sure that you really understand kind of what the data means and how it's formatted and how you can ask little questions about it. I think it's nice before you dive into the actual kind of computation that you're doing. Yeah, so to to use naive Bayes and I should check. Have you covered naive Bayes this year and other classes? OK, I see lots of nodding. So hopefully you're familiar with this. We don't have to spend too long on it. We'll define the log count ratio for each word. And that's the ratio of the feature and positive documents over the ratio of the feature and negative documents. And so. So we define our X and Y, our validation Y. And then here to kind of to calculate these log count ratios, we'll want to get the. Is there a question? Want to get the. The words that show up in positive reviews and the words that show up in negative reviews and kind of the number of times on average. So you can think like how often on average does the word loved show up in a positive review versus a negative review. And so that's what we're doing here with taking the negative reviews and then X is giving us word counts from our term document metrics. What is that? What is NP dot squeeze do? If you have a nested array here, it's taking off. Kind of one of those nestings and I saw some people doing the motion and kind of giving you giving you a single single array. So here we had this was and really it's taking off a dimension. But in this case, you're going from a. Array inside an array just to an array. So summing up the positives and negatives. So now I want you to compare how often does love appear in positive reviews versus negative reviews. So take a moment to. To write the code for that. So for for loved, I looked up the index of that with moody reusable cabs index. And then I just indexed P zero and P one by that since it's just the length of the vocab for all positive or negative documents. And that gave me the number of times the terms is done in the same for hated exactly great. Thank you. Yeah, so it's just looking up the strength index for these words and then looking them up and P not and P one. And so this is kind of a sanity check on your data. I would expect love to show up more in positive reviews. And let's confirm. Where did I. Find P not P one is positive P not is negative. And so yes, love is showing up more often in positive reviews. Hated is showing up more often in negative reviews. There are questions about that. So here we've really just taken the counts of kind of how often these these words show up in positive versus negative reviews. And then I was I was curious to look at an example of a positive review with the word hated in it. So. Just getting the integer for that token is nineteen seventy seven. I used and P dot are where to find which reviews have a non zero value for that. Then saw. See. Which of those were then I got all the positive reviews and I took a set intersection of those two things. So got all the reviews that have the word hated. Saw which ones of those were positive. It was these three. Rose for the movie reviews and looking at one of them. It was interesting to see that it. Yeah, this episode is extremely underrated. I love the unknown parts and the big twist at the end. I absolutely love that scene. For some reason, people have always hated the unknown episodes, yet I've always liked them. And so it's just kind of neat to confirm like, yes, this is clearly a positive review, but it does have hated because it's saying how other people hate this. And then I did the same thing for finding a negative review with the word loved. I got all the word other views that have loved all the reviews that are negative took their set intersection and. Looked at this one. I love the first is zooming movie. I have to say that this movie was much weaker than I expected. And you can see how this is a negative, even though they're talking about having loved a previous movie. Yes, Rebecca. Oh, can I throw you the catch box? Oh, wait, let me sorry. Where do we get the 200 that we're indexing? The 200. Oh, OK, so I am I might have I think I did this on the subsample set and then later had done it on the full set. So that may be an error. Well. Yeah, I'm going to guess that that's from a previous a previous time through. We could just plug in one of those numbers. Yeah, sorry about that. I have not been consistent. I think when running the notebook, sometimes I am running the cells. Sometimes I'm not. But use use one of the numbers that shows up here in the set intersection. Which maybe it's risky to try to see. OK, let's let's go on to applying naive Bayes. But that's just I thought a neat way to kind of also see like what is this P zero and P one mean in terms of having these counts of words and positive reviews and words and negative reviews. I've just rewritten them here so you could see kind of all at once what they are. What we'll be doing is adding one to each and this is just averaging over the number of positive reviews averaging over the number of negative reviews. And we've added a one in the numerator and in the denominator to help with numerical stability. For these. And then we take the log of their ratio. And I'll talk about I've got a notebook 3B. Well, we'll talk about we can do this briefly since you've seen naive Bayes before derivation of naive Bayes. But here I just wanted to use it and kind of see does this does this make sense. I looked up the vocabulary most likely associated with positive and negative reviews. And I used NP-ARC partition, which does not perfectly sort things, but it'll give you the number you request. It'll create a partition. So I wanted to get the the top 10. The 10 biggest things and the 10 smallest things here. And so, again, this is from the log ratios are that we've got here. And so these are the words that most kind of most indicate that a review is positive or that a review is negative. And so I looked up I looked up by BICO. As being a positive positive word. Let me run this. And the kind of issue I ran into. I think with these, because we're still on the set of just the smaller sample that some of these words only show up in one movie review. And so they show up a lot of times. So that kind of colors it. But here I found a review of this is a review of Cry Freedom in which BICO is a historical figure, one of the main characters, very positive review for negative. I was curious about Soderbergh is a director who's directed both good and bad movies. And so I looked here at the smaller subsample. Oh, actually, I guess no. Sorry. Let me see. Yeah. So I looked here and Soderbergh is only showing up in one of the reviews. So I think that's a little bit more informative to do on our full data set. But I did find this person really did not like this particular Soderbergh movie. Although I think they even say in the review that he's done others that are better. So just a little bit more data exploration of kind of what is our our log count ratios giving us. Oh, OK. So if you don't have this in your notebook, that means that these are changes I did not copy. As I have two different copies of the repo, one that has the answers and more material than you have. I'll put the updated version in after this. Sorry about that. Make a note. Yes. Always let me know. I try to I try to copy over the updated versions for you. But if I forget, let me know. Although I think I didn't make too many changes. I think from here on, it's it's it's pretty good. So then getting back to naive, naive bays, you do want to normalize by or not normalize, but define B to be the the log ratio of the average number of positive reviews to negative reviews. Kind of what percentage of the reviews are positive versus negative. And then we get our predictions by taking the validation term document times are plus B and saying I think when those are greater than zero, it's we're predicting positive, less than zero, predicting negative. We can check we've got 64 percent accuracy, which I think is reasonably good given that this was on a sample of our data using a pretty simple technique. Questions about that before we apply it to our full data set. OK, so going to the full data set and need to download that data and process it. And this is always I would say always start on just a sample of your data set when you're just figuring out what code you want to write, if your calculations work, because if things are slower on a large data set, you don't want to be kind of wasting that time while you're still still experimenting. And so now we'll have a set of I think twenty five thousand reviews in our training set and twenty five thousand in our validation set. And then here we're doing some processing that takes a little bit longer. So it's four or five seconds to get our term document matrices. It's often a good idea to save these so we can reload them later for quicker. Great. Now you've based on the full data set. So now we have thirty eight thousand tokens as opposed to just six thousand. This will give us a much, much larger vocabulary. Go through the same process. So here I have ended up kind of formalizing what we have done before in looking at when words appear in negative reviews versus positive reviews and just wrote a little helper method that gives you the ratio of for a word. So we convert it string to end. And then what's the ratio P not to P one. We're here P not is the negative reviews are P not the count of the word appearing in negative reviews. P one is the count of how often it appears in positive reviews. And you can get a sense hated is much more likely to be a negative two times as likely to be a negative reviews. Liked anything less than one is more likely to be in positive reviews. Loved is even more disproportionately likely to be in positive reviews than negative and so on. I just thought that was interesting to look at. Worst is almost ten times as likely to be in a negative review as a positive. So you can do the same same calculation before of adding one to P one and dividing by the total number of positive reviews plus one. Taking the log. Here I was just looking at these are so this is kind of similar to this idea of before taking a ratio. We've just added one and taken a log to help it be more numerically stable. Oops. I've got an accuracy of 80% for the full data set. So this is that looking at a review based on the words predicting if it's positive or negative. Questions. Yes. And we'll talk a little bit. I'll talk about numerical stability in a moment. But yeah to avoid zeros. Well yeah I guess it is just avoiding zeros in that case. Yeah. And then the log is helping with with numerical stability. So another variation we can take on naive Bayes is binarized naive Bayes which is maybe it only matters whether a word is in the review or not not the frequency of the word. So previously we were looking at the counts of how often a word appeared in the review. Maybe that's not as important. So instead of keeping track of love showed up 10 times in this review. Now it's just going to be a yes or no was loved in the review at all. To do that we can just convert our term document matrix using dot sign. And we get 82.9% accuracy. So this is even more accurate. Let me show you actually I should show you just a little bit of So this is just a matrix of ones and zeros telling you whether or not that word was present at all in the in the review. Questions about binarized naive Bayes. And this before running it. I was not sure which would be more accurate. Like it's not obvious to me that having a word that the word frequencies don't matter. But it's definitely something worth trying and that yeah in this case was was more successful or led to a higher accuracy. So now we're going to Use logistic regression and here the features are going to be Rebecca. C2I converts it converts a class to an integer. So in this case the classes are nag and pause and this is letting us know if that's one or zero. That's a good question. So the question was whether when to use binarized versus regular naive Bayes. I would say often I feel like it's probably pretty quick to once you've done dot done one to be able to try the other. But yeah, I don't have a roll of thumb on that. Right. So for logistic regression. So What's a I should ask him what's a what's a unigram A word. Yeah, so it's just a word because it's a well we'll talk about this more in a moment. An N gram. We're going to be using N grams to refer to a sequence of N words. Here's N is one. So it's a single word. There are people that also N gram can also refer to characters like at a character level in which case it would be a single character. But for our purpose will be will be looking at words. So here we can just fit a logistic regression To To our term document matrix. We do have to convert the y values to be integers, I guess here since their their classes Come up with a prediction and we get we get 88% on this this first try. So that's a even better than naive Bayes and the binarized version. So we went from 88.3% to 88.5% by switching to doing the binarized version. And so now we're going to Move on and do a or Kind of build on this to do something that will hopefully perform even better, which is to do a version of logistic regression with with naive Bayes features. So for every document we compute the binarized features as described above. And this time we're also going to use by grams and try grams. So here we're referring to sequences of words. So by grams would be the dog said that can't you. So any sequence of two words and kind of by making this into a feature. Like an obvious benefit is having things like not in there. You know, It said I did not love this movie having not love is, you know, going to be something very different than love when you're only looking at the individual words. So looking at the same same movie review text that again we're going to go back to the sample since we're doing something, something new. Actually, I should run these. And so next we want to do is we want to iterate the through the sequences of words to create our end grams. So we're going to kind of have to go through each review to get what are all the sequences of two words that show up. And we'll do this with with nested loops. And this is can be helpful for the homework problem. This kind of has more going on than what you'll have to do in the homework. And we want to Kind of at the end, we're going to create a counter of what the end grams were, how often they appear, what And kind of it and in each in each review. And something to note about this is that This is going to end up being much larger than when we were just looking at individual words, since it can Scale in a bigger way. So it's good. It's good. We went back down to the sample sample data set. Actually, maybe this is a good time to take our break. So let's take a five minute break and be back at 1205 and hopefully our Matrix will be done being computed by then. All right, let's let's start back up. So we just we just finished computing our our train end gram Our end gram document matrix. So before we were saying term document matrix when our terms were just unigrams. Now we've got end grams. And so it's our end gram document matrix for the training set. And here note this has got 260,000 tokens. So a lot of a lot of tokens because this can grow conceivably in like n factorial, you know, the fact that you can take all these different combinations. So let's take a take a look at what this what this looks like. I think I just randomly looked up 2000 2005. Where did that come from? Well, let me look at. OK, so for I to end gram, I was just curious about what's what's element 100,000. And it says the end gram looks at looks like this array of two things. So this must be a by gram with two words in it. 189 and 1301. And if I look up what words those correspond to, I get nothing changes. So it's conceivable that these two words were were next to each other. So this is one of our by grams. And here this is what what corresponds to element 100,000. I look up another one, 100,000 10. This is 6348, which is has and so that's another another by gram. Here I find a trigram with index 6116 and it's doesn't even is the is the trigram. There are questions about this is just to give you a sense of kind of like what is this matrix that we've found. So this matrix has go back up 800 reviews in it. And then it's got 260,000 and grams and it's a combination of some of these are unigrams, some are by grams and some are trigrams. And they're recorded as these arrays of one, two or three things, but they correspond to sets of two pulls of words with one, two or three words in them. They can be, you know, by gram like nothing changes or try trigram like doesn't even. Another trigram I found was look her usual business and political. So just to kind of get a sense of what what words appear together. There are questions about this. Rebecca. So here for tokenization, yes, I'll say most tokenizers will split up contractions into separate separate words. The other way they sometimes get split is and we'll see this later. Sometimes you just take the T off and have that as like apostrophe T as its own its own word. But yeah, that's a standard way to tokenize and. Oh, here I guess we're using the fast AI tokenizer. But in the in the red Jax lesson, we'll look at how we could write our own our own tokenizer. So if you have any questions. OK, so I did that with the training set. Now this is the same code as above to make a validation matrix. To also be a little bit slow. Really, I probably should have opened my my saved versions of these. But since these are slower to calculate, you definitely would want to save them and then just load them in the future. So wait. That validation set has 200 reviews in it. Same 260,000 token or 260,000 n grams that we're using. And now we can do naive based on this in a similar way as to before. Here I'm just showing that the predictions look like this array of trues and falses. And now our validation accuracy is 76 percent. So this was a little bit or not just a little bit. This was worse than what we had gotten with logistic regression. And ultimately what we're going to end up doing is using trigram features in a logistic regression to improve the performance. But this is kind of a way to get acquainted with with having trigrams and bigrams. Do the binarized version of this. And again, the binarized version is a little bit. No, in this case, the binarized version is worse than the regular version. Questions on using naive base with trigrams. All right. So now we're going to do logistic regression where the features are the trigrams. So, again, this will be having these sequences of words. Here we're going to use scikit learns count vectorizer to to create our trigrams and we give it or create our n grams. We give it a range of this. This is similar to what's asked for in the homework of saying, OK, I want you to give me the unigrams, bigrams and trigrams back all together. In this range, notice it has you set a number of max features, which makes sense because these can get very large, particularly if you had a even wider range of what n grams you were looking at. Yes. I believe this is inclusive. Yeah. And we could check the. The documentation. Yeah, you'd have to look at the scikit learn docs, but I believe this is inclusive. And the one in the homework asked for. Oh, here it is. Yes. It's inclusive. All values of n such that min n less than or equal to n less than or equal to max n will be used. And this is how you can just hit shift tab or shift tab tab to get documentation in a Jupyter Notebook. Oh, and here I think I was just looking at the vocabulary of the vectorizer. You can see here are a bunch of unigrams are still in there. Now we're in a section of bigrams. You can also this can see how they add them typically kind of by going through a sentence and it's also here, you know, like rolling a window over looking at each combination of two words, lovable self, self in in this, this comma, which normally normally makes makes me. And the reason you know you're kind of taking what's the bigram, which normally normally is in kind of like the second spot. So then you slide over. You've got normally in the first spot normally make. So this sentence would read lovable self in this, which normally makes me forgive her shallow acting would be the sentence that this must have come from. We go further. Let me see if we get to. Yeah. And then here we get to some trigrams in here too. Hard to believe she was the producer on this dog. So you can kind of see how the trigrams were constructed from the sentence questions about that. Right. Binarized naive Bayes using n grams from count vectorizer. To run this. So there I was getting 83% with the Binarized version 78% without. Let me. Down to this part. Questions. Something. Oh, go ahead. Yeah, I mean, I, I would guess you probably rarely want. Yeah. Something I mentioned in the intro that I haven't had a chance to look at yet is sentence piece, which is this library that does sub word units, which is kind of the similar idea. I think of looking at like character and grams, but it's kind of pieces of words. I know people that have used that kind of in conjunction with deep learning and gotten good, good outcomes. But yeah, I don't, I don't know of cases where people have taken like six grams or yeah, because the other thing is as it gets longer. I think they're going to get more and more distinct, you know, they're probably like a lot of seven grams that only show up once or something in the, in the corpus. Welcome.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 13.0, "text": " Alright, so let's keep going with Notebook 3. So you'll remember last time we were looking at the problem of wanting to classify the sentiment of movie reviews.", "tokens": [2798, 11, 370, 718, 311, 1066, 516, 365, 11633, 2939, 805, 13, 407, 291, 603, 1604, 1036, 565, 321, 645, 1237, 412, 264, 1154, 295, 7935, 281, 33872, 264, 16149, 295, 3169, 10229, 13], "temperature": 0.0, "avg_logprob": -0.21138920783996581, "compression_ratio": 1.5330188679245282, "no_speech_prob": 0.014491690322756767}, {"id": 1, "seek": 0, "start": 13.0, "end": 24.0, "text": " And we're going to be doing it in this notebook with logistic regression, naive Bayes, and then in Notebook 4, we'll get to see how to do this with neural networks.", "tokens": [400, 321, 434, 516, 281, 312, 884, 309, 294, 341, 21060, 365, 3565, 3142, 24590, 11, 29052, 7840, 279, 11, 293, 550, 294, 11633, 2939, 1017, 11, 321, 603, 483, 281, 536, 577, 281, 360, 341, 365, 18161, 9590, 13], "temperature": 0.0, "avg_logprob": -0.21138920783996581, "compression_ratio": 1.5330188679245282, "no_speech_prob": 0.014491690322756767}, {"id": 2, "seek": 2400, "start": 24.0, "end": 31.0, "text": " So it's going to be two different, very different approaches for the same problem.", "tokens": [407, 309, 311, 516, 281, 312, 732, 819, 11, 588, 819, 11587, 337, 264, 912, 1154, 13], "temperature": 0.0, "avg_logprob": -0.08929396303076494, "compression_ratio": 1.588235294117647, "no_speech_prob": 4.0061306208372116e-05}, {"id": 3, "seek": 2400, "start": 31.0, "end": 43.0, "text": " And last time we were really talking, and it's really important to kind of understand how your data is set up and what we have kind of these different methods and dictionaries for using it.", "tokens": [400, 1036, 565, 321, 645, 534, 1417, 11, 293, 309, 311, 534, 1021, 281, 733, 295, 1223, 577, 428, 1412, 307, 992, 493, 293, 437, 321, 362, 733, 295, 613, 819, 7150, 293, 22352, 4889, 337, 1228, 309, 13], "temperature": 0.0, "avg_logprob": -0.08929396303076494, "compression_ratio": 1.588235294117647, "no_speech_prob": 4.0061306208372116e-05}, {"id": 4, "seek": 2400, "start": 43.0, "end": 49.0, "text": " And so I just wanted to look at that again briefly.", "tokens": [400, 370, 286, 445, 1415, 281, 574, 412, 300, 797, 10515, 13], "temperature": 0.0, "avg_logprob": -0.08929396303076494, "compression_ratio": 1.588235294117647, "no_speech_prob": 4.0061306208372116e-05}, {"id": 5, "seek": 4900, "start": 49.0, "end": 67.0, "text": " So here, this has been processed into tokens, and we have a number of kind of these special tokens, such as XXUNC for an unknown word, beginning of string or beginning of the text,", "tokens": [407, 510, 11, 341, 575, 668, 18846, 666, 22667, 11, 293, 321, 362, 257, 1230, 295, 733, 295, 613, 2121, 22667, 11, 1270, 382, 27050, 3979, 34, 337, 364, 9841, 1349, 11, 2863, 295, 6798, 420, 2863, 295, 264, 2487, 11], "temperature": 0.0, "avg_logprob": -0.16229185982355995, "compression_ratio": 1.5089820359281436, "no_speech_prob": 6.013130769133568e-05}, {"id": 6, "seek": 4900, "start": 67.0, "end": 78.0, "text": " repetition, that the next word begins with a capital letter, and so on.", "tokens": [30432, 11, 300, 264, 958, 1349, 7338, 365, 257, 4238, 5063, 11, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.16229185982355995, "compression_ratio": 1.5089820359281436, "no_speech_prob": 6.013130769133568e-05}, {"id": 7, "seek": 7800, "start": 78.0, "end": 87.0, "text": " And then can anyone tell me what I2S does or is?", "tokens": [400, 550, 393, 2878, 980, 385, 437, 286, 17, 50, 775, 420, 307, 30], "temperature": 0.0, "avg_logprob": -0.17273724384796926, "compression_ratio": 1.1428571428571428, "no_speech_prob": 0.0001851628621807322}, {"id": 8, "seek": 7800, "start": 87.0, "end": 93.0, "text": " Louder? Into string. Yeah, so that's going from an integer to a string.", "tokens": [7272, 1068, 30, 23373, 6798, 13, 865, 11, 370, 300, 311, 516, 490, 364, 24922, 281, 257, 6798, 13], "temperature": 0.0, "avg_logprob": -0.17273724384796926, "compression_ratio": 1.1428571428571428, "no_speech_prob": 0.0001851628621807322}, {"id": 9, "seek": 9300, "start": 93.0, "end": 117.0, "text": " And why do we need to go between the two?", "tokens": [400, 983, 360, 321, 643, 281, 352, 1296, 264, 732, 30], "temperature": 0.0, "avg_logprob": -0.11367624600728353, "compression_ratio": 0.8541666666666666, "no_speech_prob": 2.1104515326442197e-05}, {"id": 10, "seek": 11700, "start": 117.0, "end": 123.0, "text": " Yeah, and I'll repeat that for the microphone. So for the computer program, you're often using integers.", "tokens": [865, 11, 293, 286, 603, 7149, 300, 337, 264, 10952, 13, 407, 337, 264, 3820, 1461, 11, 291, 434, 2049, 1228, 41674, 13], "temperature": 0.0, "avg_logprob": -0.11144128896422305, "compression_ratio": 1.6863468634686347, "no_speech_prob": 1.80564838956343e-05}, {"id": 11, "seek": 11700, "start": 123.0, "end": 127.0, "text": " That's how we kind of are indexing our tokens.", "tokens": [663, 311, 577, 321, 733, 295, 366, 8186, 278, 527, 22667, 13], "temperature": 0.0, "avg_logprob": -0.11144128896422305, "compression_ratio": 1.6863468634686347, "no_speech_prob": 1.80564838956343e-05}, {"id": 12, "seek": 11700, "start": 127.0, "end": 134.0, "text": " Anything human oriented, you want to see the string so that you both can understand it as the programmer.", "tokens": [11998, 1952, 21841, 11, 291, 528, 281, 536, 264, 6798, 370, 300, 291, 1293, 393, 1223, 309, 382, 264, 32116, 13], "temperature": 0.0, "avg_logprob": -0.11144128896422305, "compression_ratio": 1.6863468634686347, "no_speech_prob": 1.80564838956343e-05}, {"id": 13, "seek": 11700, "start": 134.0, "end": 141.0, "text": " Also, the original was in a string, and so we're kind of converting it to integers to make it easier to work with.", "tokens": [2743, 11, 264, 3380, 390, 294, 257, 6798, 11, 293, 370, 321, 434, 733, 295, 29942, 309, 281, 41674, 281, 652, 309, 3571, 281, 589, 365, 13], "temperature": 0.0, "avg_logprob": -0.11144128896422305, "compression_ratio": 1.6863468634686347, "no_speech_prob": 1.80564838956343e-05}, {"id": 14, "seek": 11700, "start": 141.0, "end": 146.0, "text": " But sometimes we'll want to switch back. And so we had I2S string and also string2I.", "tokens": [583, 2171, 321, 603, 528, 281, 3679, 646, 13, 400, 370, 321, 632, 286, 17, 50, 6798, 293, 611, 6798, 17, 40, 13], "temperature": 0.0, "avg_logprob": -0.11144128896422305, "compression_ratio": 1.6863468634686347, "no_speech_prob": 1.80564838956343e-05}, {"id": 15, "seek": 14600, "start": 146.0, "end": 152.0, "text": " And what kind of data structures were I2S string and string2I?", "tokens": [400, 437, 733, 295, 1412, 9227, 645, 286, 17, 50, 6798, 293, 6798, 17, 40, 30], "temperature": 0.0, "avg_logprob": -0.09892370367562899, "compression_ratio": 1.6398104265402844, "no_speech_prob": 4.9856000259751454e-05}, {"id": 16, "seek": 14600, "start": 152.0, "end": 159.0, "text": " List and dictionary, right? So I2S string is a list because you don't actually have to store the integer.", "tokens": [17668, 293, 25890, 11, 558, 30, 407, 286, 17, 50, 6798, 307, 257, 1329, 570, 291, 500, 380, 767, 362, 281, 3531, 264, 24922, 13], "temperature": 0.0, "avg_logprob": -0.09892370367562899, "compression_ratio": 1.6398104265402844, "no_speech_prob": 4.9856000259751454e-05}, {"id": 17, "seek": 14600, "start": 159.0, "end": 175.0, "text": " It's kind of implicit in the list of the order it's in, whereas you need a dictionary for string2I to be able to look up a string as a key and get back an integer as your value.", "tokens": [467, 311, 733, 295, 26947, 294, 264, 1329, 295, 264, 1668, 309, 311, 294, 11, 9735, 291, 643, 257, 25890, 337, 6798, 17, 40, 281, 312, 1075, 281, 574, 493, 257, 6798, 382, 257, 2141, 293, 483, 646, 364, 24922, 382, 428, 2158, 13], "temperature": 0.0, "avg_logprob": -0.09892370367562899, "compression_ratio": 1.6398104265402844, "no_speech_prob": 4.9856000259751454e-05}, {"id": 18, "seek": 17500, "start": 175.0, "end": 184.0, "text": " And so here we saw, yeah, saw part of the kind of dictionary string2I.", "tokens": [400, 370, 510, 321, 1866, 11, 1338, 11, 1866, 644, 295, 264, 733, 295, 25890, 6798, 17, 40, 13], "temperature": 0.0, "avg_logprob": -0.13780037831451933, "compression_ratio": 1.4088050314465408, "no_speech_prob": 1.777782199496869e-05}, {"id": 19, "seek": 17500, "start": 184.0, "end": 192.0, "text": " We created the term document matrix ourselves using a sparse matrix in SciPy.", "tokens": [492, 2942, 264, 1433, 4166, 8141, 4175, 1228, 257, 637, 11668, 8141, 294, 16942, 47, 88, 13], "temperature": 0.0, "avg_logprob": -0.13780037831451933, "compression_ratio": 1.4088050314465408, "no_speech_prob": 1.777782199496869e-05}, {"id": 20, "seek": 17500, "start": 192.0, "end": 198.0, "text": " And we talked through what the different ways to store sparse matrices are.", "tokens": [400, 321, 2825, 807, 437, 264, 819, 2098, 281, 3531, 637, 11668, 32284, 366, 13], "temperature": 0.0, "avg_logprob": -0.13780037831451933, "compression_ratio": 1.4088050314465408, "no_speech_prob": 1.777782199496869e-05}, {"id": 21, "seek": 19800, "start": 198.0, "end": 206.0, "text": " Do you guys want to review that today or how do you feel about it?", "tokens": [1144, 291, 1074, 528, 281, 3131, 300, 965, 420, 577, 360, 291, 841, 466, 309, 30], "temperature": 0.0, "avg_logprob": -0.11941797892252604, "compression_ratio": 1.567251461988304, "no_speech_prob": 5.422079539130209e-06}, {"id": 22, "seek": 19800, "start": 206.0, "end": 211.0, "text": " Sorry, his thumbs up for review or thumbs up as you feel good?", "tokens": [4919, 11, 702, 8838, 493, 337, 3131, 420, 8838, 493, 382, 291, 841, 665, 30], "temperature": 0.0, "avg_logprob": -0.11941797892252604, "compression_ratio": 1.567251461988304, "no_speech_prob": 5.422079539130209e-06}, {"id": 23, "seek": 19800, "start": 211.0, "end": 214.0, "text": " Feeling good? Does anyone want to review it?", "tokens": [29945, 665, 30, 4402, 2878, 528, 281, 3131, 309, 30], "temperature": 0.0, "avg_logprob": -0.11941797892252604, "compression_ratio": 1.567251461988304, "no_speech_prob": 5.422079539130209e-06}, {"id": 24, "seek": 19800, "start": 214.0, "end": 221.0, "text": " Okay, yeah, I see some hands for reviewing it. So we'll just quickly take a look at it again.", "tokens": [1033, 11, 1338, 11, 286, 536, 512, 2377, 337, 19576, 309, 13, 407, 321, 603, 445, 2661, 747, 257, 574, 412, 309, 797, 13], "temperature": 0.0, "avg_logprob": -0.11941797892252604, "compression_ratio": 1.567251461988304, "no_speech_prob": 5.422079539130209e-06}, {"id": 25, "seek": 22100, "start": 221.0, "end": 232.0, "text": " So the first one we saw was coordinate-wise storage, which I think is the most intuitive, where you've got a value, its row number, and its column number.", "tokens": [407, 264, 700, 472, 321, 1866, 390, 15670, 12, 3711, 6725, 11, 597, 286, 519, 307, 264, 881, 21769, 11, 689, 291, 600, 658, 257, 2158, 11, 1080, 5386, 1230, 11, 293, 1080, 7738, 1230, 13], "temperature": 0.0, "avg_logprob": -0.0909139053731025, "compression_ratio": 1.5, "no_speech_prob": 2.1780993847642094e-05}, {"id": 26, "seek": 22100, "start": 232.0, "end": 237.0, "text": " And so you're basically storing these three arrays that has all your information.", "tokens": [400, 370, 291, 434, 1936, 26085, 613, 1045, 41011, 300, 575, 439, 428, 1589, 13], "temperature": 0.0, "avg_logprob": -0.0909139053731025, "compression_ratio": 1.5, "no_speech_prob": 2.1780993847642094e-05}, {"id": 27, "seek": 22100, "start": 237.0, "end": 243.0, "text": " Actually, I guess first I should ask, what is a sparse matrix?", "tokens": [5135, 11, 286, 2041, 700, 286, 820, 1029, 11, 437, 307, 257, 637, 11668, 8141, 30], "temperature": 0.0, "avg_logprob": -0.0909139053731025, "compression_ratio": 1.5, "no_speech_prob": 2.1780993847642094e-05}, {"id": 28, "seek": 22100, "start": 243.0, "end": 245.0, "text": " Maria?", "tokens": [12734, 30], "temperature": 0.0, "avg_logprob": -0.0909139053731025, "compression_ratio": 1.5, "no_speech_prob": 2.1780993847642094e-05}, {"id": 29, "seek": 24500, "start": 245.0, "end": 253.0, "text": " Yeah, sparse matrix has lots of zeros and so not as many non-zero values.", "tokens": [865, 11, 637, 11668, 8141, 575, 3195, 295, 35193, 293, 370, 406, 382, 867, 2107, 12, 32226, 4190, 13], "temperature": 0.0, "avg_logprob": -0.10600003643312315, "compression_ratio": 1.4725274725274726, "no_speech_prob": 1.593582237546798e-05}, {"id": 30, "seek": 24500, "start": 253.0, "end": 267.0, "text": " And I think sometimes more precisely, although this is a little bit flexible, people will say the number of non-zero values is scaling with n and not n squared, if you had like an n by n matrix.", "tokens": [400, 286, 519, 2171, 544, 13402, 11, 4878, 341, 307, 257, 707, 857, 11358, 11, 561, 486, 584, 264, 1230, 295, 2107, 12, 32226, 4190, 307, 21589, 365, 297, 293, 406, 297, 8889, 11, 498, 291, 632, 411, 364, 297, 538, 297, 8141, 13], "temperature": 0.0, "avg_logprob": -0.10600003643312315, "compression_ratio": 1.4725274725274726, "no_speech_prob": 1.593582237546798e-05}, {"id": 31, "seek": 26700, "start": 267.0, "end": 277.0, "text": " So coordinate-wise, I think is the most straightforward, and that's how we'll typically enter, kind of as a human enter our values for sparse matrix.", "tokens": [407, 15670, 12, 3711, 11, 286, 519, 307, 264, 881, 15325, 11, 293, 300, 311, 577, 321, 603, 5850, 3242, 11, 733, 295, 382, 257, 1952, 3242, 527, 4190, 337, 637, 11668, 8141, 13], "temperature": 0.0, "avg_logprob": -0.13675044831775485, "compression_ratio": 1.4342857142857144, "no_speech_prob": 1.130054170062067e-05}, {"id": 32, "seek": 26700, "start": 277.0, "end": 287.0, "text": " Then one that was a little bit weirder when you first see it is compressed sparse row data structure.", "tokens": [1396, 472, 300, 390, 257, 707, 857, 321, 347, 1068, 562, 291, 700, 536, 309, 307, 30353, 637, 11668, 5386, 1412, 3877, 13], "temperature": 0.0, "avg_logprob": -0.13675044831775485, "compression_ratio": 1.4342857142857144, "no_speech_prob": 1.130054170062067e-05}, {"id": 33, "seek": 28700, "start": 287.0, "end": 298.0, "text": " And here you're still storing the column number and the value, but you've got this row pointer.", "tokens": [400, 510, 291, 434, 920, 26085, 264, 7738, 1230, 293, 264, 2158, 11, 457, 291, 600, 658, 341, 5386, 23918, 13], "temperature": 0.0, "avg_logprob": -0.05643819207730501, "compression_ratio": 1.4274193548387097, "no_speech_prob": 3.883010504068807e-05}, {"id": 34, "seek": 28700, "start": 298.0, "end": 305.0, "text": " And the row pointer is just keeping track of when you've gone on to the next row.", "tokens": [400, 264, 5386, 23918, 307, 445, 5145, 2837, 295, 562, 291, 600, 2780, 322, 281, 264, 958, 5386, 13], "temperature": 0.0, "avg_logprob": -0.05643819207730501, "compression_ratio": 1.4274193548387097, "no_speech_prob": 3.883010504068807e-05}, {"id": 35, "seek": 30500, "start": 305.0, "end": 318.0, "text": " And so since we have 0, 1, 2, 3, we've gone to the next row, 4, 5, 6, we've gone to the next row, 7, 8, 9, that's where this 0, 3, 6, 9 comes from.", "tokens": [400, 370, 1670, 321, 362, 1958, 11, 502, 11, 568, 11, 805, 11, 321, 600, 2780, 281, 264, 958, 5386, 11, 1017, 11, 1025, 11, 1386, 11, 321, 600, 2780, 281, 264, 958, 5386, 11, 1614, 11, 1649, 11, 1722, 11, 300, 311, 689, 341, 1958, 11, 805, 11, 1386, 11, 1722, 1487, 490, 13], "temperature": 0.0, "avg_logprob": -0.07598035231880519, "compression_ratio": 1.5232558139534884, "no_speech_prob": 2.8856276912847534e-05}, {"id": 36, "seek": 30500, "start": 318.0, "end": 328.0, "text": " And the thing to note here, is there a relationship between 9, 22, and 1?", "tokens": [400, 264, 551, 281, 3637, 510, 11, 307, 456, 257, 2480, 1296, 1722, 11, 5853, 11, 293, 502, 30], "temperature": 0.0, "avg_logprob": -0.07598035231880519, "compression_ratio": 1.5232558139534884, "no_speech_prob": 2.8856276912847534e-05}, {"id": 37, "seek": 30500, "start": 328.0, "end": 333.0, "text": " Shake your head no or nod your head yes.", "tokens": [27809, 428, 1378, 572, 420, 15224, 428, 1378, 2086, 13], "temperature": 0.0, "avg_logprob": -0.07598035231880519, "compression_ratio": 1.5232558139534884, "no_speech_prob": 2.8856276912847534e-05}, {"id": 38, "seek": 33300, "start": 333.0, "end": 340.0, "text": " No, yes, I see a few shaking heads no. So this is a little bit confusing how they've written this here.", "tokens": [883, 11, 2086, 11, 286, 536, 257, 1326, 15415, 8050, 572, 13, 407, 341, 307, 257, 707, 857, 13181, 577, 436, 600, 3720, 341, 510, 13], "temperature": 0.0, "avg_logprob": -0.12125606813292572, "compression_ratio": 1.4337349397590362, "no_speech_prob": 1.2805194273823872e-05}, {"id": 39, "seek": 33300, "start": 340.0, "end": 348.0, "text": " But the 22 and 1 is referring to this entry here.", "tokens": [583, 264, 5853, 293, 502, 307, 13761, 281, 341, 8729, 510, 13], "temperature": 0.0, "avg_logprob": -0.12125606813292572, "compression_ratio": 1.4337349397590362, "no_speech_prob": 1.2805194273823872e-05}, {"id": 40, "seek": 33300, "start": 348.0, "end": 356.0, "text": " And that has, yeah, not related to this 9. The only thing is if you look at index 3,", "tokens": [400, 300, 575, 11, 1338, 11, 406, 4077, 281, 341, 1722, 13, 440, 787, 551, 307, 498, 291, 574, 412, 8186, 805, 11], "temperature": 0.0, "avg_logprob": -0.12125606813292572, "compression_ratio": 1.4337349397590362, "no_speech_prob": 1.2805194273823872e-05}, {"id": 41, "seek": 35600, "start": 356.0, "end": 366.0, "text": " this is telling us that the 9th entry we've gone on to the third row in a zero based counting system.", "tokens": [341, 307, 3585, 505, 300, 264, 1722, 392, 8729, 321, 600, 2780, 322, 281, 264, 2636, 5386, 294, 257, 4018, 2361, 13251, 1185, 13], "temperature": 0.0, "avg_logprob": -0.10386643298836641, "compression_ratio": 1.5360824742268042, "no_speech_prob": 5.422159574663965e-06}, {"id": 42, "seek": 35600, "start": 366.0, "end": 373.0, "text": " Or, you know, starting from zero, we've gone row 0, row 1, row 2, row 3 for the 9th entry.", "tokens": [1610, 11, 291, 458, 11, 2891, 490, 4018, 11, 321, 600, 2780, 5386, 1958, 11, 5386, 502, 11, 5386, 568, 11, 5386, 805, 337, 264, 1722, 392, 8729, 13], "temperature": 0.0, "avg_logprob": -0.10386643298836641, "compression_ratio": 1.5360824742268042, "no_speech_prob": 5.422159574663965e-06}, {"id": 43, "seek": 35600, "start": 373.0, "end": 379.0, "text": " But that has nothing to do with this value of 22. So it's a little confusing how they've written it here.", "tokens": [583, 300, 575, 1825, 281, 360, 365, 341, 2158, 295, 5853, 13, 407, 309, 311, 257, 707, 13181, 577, 436, 600, 3720, 309, 510, 13], "temperature": 0.0, "avg_logprob": -0.10386643298836641, "compression_ratio": 1.5360824742268042, "no_speech_prob": 5.422159574663965e-06}, {"id": 44, "seek": 37900, "start": 379.0, "end": 389.0, "text": " We saw another form, which I think is confusing in a different way of writing it.", "tokens": [492, 1866, 1071, 1254, 11, 597, 286, 519, 307, 13181, 294, 257, 819, 636, 295, 3579, 309, 13], "temperature": 0.0, "avg_logprob": -0.10084285736083984, "compression_ratio": 1.375, "no_speech_prob": 6.240845777938375e-06}, {"id": 45, "seek": 37900, "start": 389.0, "end": 398.0, "text": " Here where they spaced out the row start index, you know, to show you, but you're not actually storing those spaces.", "tokens": [1692, 689, 436, 43766, 484, 264, 5386, 722, 8186, 11, 291, 458, 11, 281, 855, 291, 11, 457, 291, 434, 406, 767, 26085, 729, 7673, 13], "temperature": 0.0, "avg_logprob": -0.10084285736083984, "compression_ratio": 1.375, "no_speech_prob": 6.240845777938375e-06}, {"id": 46, "seek": 39800, "start": 398.0, "end": 409.0, "text": " Any questions about the compressed sparse row format?", "tokens": [2639, 1651, 466, 264, 30353, 637, 11668, 5386, 7877, 30], "temperature": 0.0, "avg_logprob": -0.09219817998932629, "compression_ratio": 1.34375, "no_speech_prob": 3.9669016587140504e-06}, {"id": 47, "seek": 39800, "start": 409.0, "end": 421.0, "text": " So a benefit of it is it takes even less storage space and also makes accessing your data by rows very quick and easy.", "tokens": [407, 257, 5121, 295, 309, 307, 309, 2516, 754, 1570, 6725, 1901, 293, 611, 1669, 26440, 428, 1412, 538, 13241, 588, 1702, 293, 1858, 13], "temperature": 0.0, "avg_logprob": -0.09219817998932629, "compression_ratio": 1.34375, "no_speech_prob": 3.9669016587140504e-06}, {"id": 48, "seek": 42100, "start": 421.0, "end": 432.0, "text": " And you can convert between them. If you were primarily going to be accessing by columns, you would want to use compressed column sparse storage.", "tokens": [400, 291, 393, 7620, 1296, 552, 13, 759, 291, 645, 10029, 516, 281, 312, 26440, 538, 13766, 11, 291, 576, 528, 281, 764, 30353, 7738, 637, 11668, 6725, 13], "temperature": 0.0, "avg_logprob": -0.09647241760702695, "compression_ratio": 1.549738219895288, "no_speech_prob": 1.0783055586216506e-05}, {"id": 49, "seek": 42100, "start": 432.0, "end": 438.0, "text": " And so some of this is kind of keeping in mind how you'll be using your data.", "tokens": [400, 370, 512, 295, 341, 307, 733, 295, 5145, 294, 1575, 577, 291, 603, 312, 1228, 428, 1412, 13], "temperature": 0.0, "avg_logprob": -0.09647241760702695, "compression_ratio": 1.549738219895288, "no_speech_prob": 1.0783055586216506e-05}, {"id": 50, "seek": 42100, "start": 438.0, "end": 446.0, "text": " And it's relatively efficient to change between these different formats.", "tokens": [400, 309, 311, 7226, 7148, 281, 1319, 1296, 613, 819, 25879, 13], "temperature": 0.0, "avg_logprob": -0.09647241760702695, "compression_ratio": 1.549738219895288, "no_speech_prob": 1.0783055586216506e-05}, {"id": 51, "seek": 44600, "start": 446.0, "end": 458.0, "text": " Yeah, so we saw this last time and we use this to make our term document matrix down here.", "tokens": [865, 11, 370, 321, 1866, 341, 1036, 565, 293, 321, 764, 341, 281, 652, 527, 1433, 4166, 8141, 760, 510, 13], "temperature": 0.0, "avg_logprob": -0.12619383768601852, "compression_ratio": 1.3057851239669422, "no_speech_prob": 2.5866580472211353e-05}, {"id": 52, "seek": 44600, "start": 458.0, "end": 475.0, "text": " And actually, I guess we were we were feeding it into a CSR matrix.", "tokens": [400, 767, 11, 286, 2041, 321, 645, 321, 645, 12919, 309, 666, 257, 9460, 49, 8141, 13], "temperature": 0.0, "avg_logprob": -0.12619383768601852, "compression_ratio": 1.3057851239669422, "no_speech_prob": 2.5866580472211353e-05}, {"id": 53, "seek": 47500, "start": 475.0, "end": 483.0, "text": " OK, we saw how to display these as dense. And then I guess I left off with the sorry with an exercise.", "tokens": [2264, 11, 321, 1866, 577, 281, 4674, 613, 382, 18011, 13, 400, 550, 286, 2041, 286, 1411, 766, 365, 264, 2597, 365, 364, 5380, 13], "temperature": 0.0, "avg_logprob": -0.1874954382578532, "compression_ratio": 1.3541666666666667, "no_speech_prob": 1.7777705579646863e-05}, {"id": 54, "seek": 47500, "start": 483.0, "end": 492.0, "text": " Was it this one of since the word.", "tokens": [3027, 309, 341, 472, 295, 1670, 264, 1349, 13], "temperature": 0.0, "avg_logprob": -0.1874954382578532, "compression_ratio": 1.3541666666666667, "no_speech_prob": 1.7777705579646863e-05}, {"id": 55, "seek": 47500, "start": 492.0, "end": 498.0, "text": " OK, got it.", "tokens": [2264, 11, 658, 309, 13], "temperature": 0.0, "avg_logprob": -0.1874954382578532, "compression_ratio": 1.3541666666666667, "no_speech_prob": 1.7777705579646863e-05}, {"id": 56, "seek": 47500, "start": 498.0, "end": 503.0, "text": " Hey, does anyone want to share what they did?", "tokens": [1911, 11, 775, 2878, 528, 281, 2073, 437, 436, 630, 30], "temperature": 0.0, "avg_logprob": -0.1874954382578532, "compression_ratio": 1.3541666666666667, "no_speech_prob": 1.7777705579646863e-05}, {"id": 57, "seek": 50300, "start": 503.0, "end": 509.0, "text": " Maybe the catch box or catch.", "tokens": [2704, 264, 3745, 2424, 420, 3745, 13], "temperature": 0.0, "avg_logprob": -0.2211108773441638, "compression_ratio": 1.4258064516129032, "no_speech_prob": 4.263735900167376e-05}, {"id": 58, "seek": 50300, "start": 509.0, "end": 521.0, "text": " OK, so what I did was since that review came from the validation set, I took the valid turned up document matrix.", "tokens": [2264, 11, 370, 437, 286, 630, 390, 1670, 300, 3131, 1361, 490, 264, 24071, 992, 11, 286, 1890, 264, 7363, 3574, 493, 4166, 8141, 13], "temperature": 0.0, "avg_logprob": -0.2211108773441638, "compression_ratio": 1.4258064516129032, "no_speech_prob": 4.263735900167376e-05}, {"id": 59, "seek": 50300, "start": 521.0, "end": 527.0, "text": " And well, before that, I first looked at I had called string to end for late.", "tokens": [400, 731, 11, 949, 300, 11, 286, 700, 2956, 412, 286, 632, 1219, 6798, 281, 917, 337, 3469, 13], "temperature": 0.0, "avg_logprob": -0.2211108773441638, "compression_ratio": 1.4258064516129032, "no_speech_prob": 4.263735900167376e-05}, {"id": 60, "seek": 52700, "start": 527.0, "end": 540.0, "text": " Yeah, I got an index of four fifty one. So then I know that, OK, I got to look at the 450 first column in this validation document to a matrix.", "tokens": [865, 11, 286, 658, 364, 8186, 295, 1451, 13442, 472, 13, 407, 550, 286, 458, 300, 11, 2264, 11, 286, 658, 281, 574, 412, 264, 26034, 700, 7738, 294, 341, 24071, 4166, 281, 257, 8141, 13], "temperature": 0.0, "avg_logprob": -0.19809410240076766, "compression_ratio": 1.6818181818181819, "no_speech_prob": 7.2954012466652784e-06}, {"id": 61, "seek": 52700, "start": 540.0, "end": 545.0, "text": " And I knew that the index of this review was one.", "tokens": [400, 286, 2586, 300, 264, 8186, 295, 341, 3131, 390, 472, 13], "temperature": 0.0, "avg_logprob": -0.19809410240076766, "compression_ratio": 1.6818181818181819, "no_speech_prob": 7.2954012466652784e-06}, {"id": 62, "seek": 52700, "start": 545.0, "end": 552.0, "text": " So if I look at row one column for fifty one in the validation term document matrix, I get to exactly.", "tokens": [407, 498, 286, 574, 412, 5386, 472, 7738, 337, 13442, 472, 294, 264, 24071, 1433, 4166, 8141, 11, 286, 483, 281, 2293, 13], "temperature": 0.0, "avg_logprob": -0.19809410240076766, "compression_ratio": 1.6818181818181819, "no_speech_prob": 7.2954012466652784e-06}, {"id": 63, "seek": 55200, "start": 552.0, "end": 557.0, "text": " Great. Thanks.", "tokens": [3769, 13, 2561, 13], "temperature": 0.0, "avg_logprob": -0.19493298917203336, "compression_ratio": 1.2432432432432432, "no_speech_prob": 8.990837727651524e-07}, {"id": 64, "seek": 55200, "start": 557.0, "end": 572.0, "text": " Yeah. So look up the the integer for late at four fifty one and then you look it up in the validation term document matrix.", "tokens": [865, 13, 407, 574, 493, 264, 264, 24922, 337, 3469, 412, 1451, 13442, 472, 293, 550, 291, 574, 309, 493, 294, 264, 24071, 1433, 4166, 8141, 13], "temperature": 0.0, "avg_logprob": -0.19493298917203336, "compression_ratio": 1.2432432432432432, "no_speech_prob": 8.990837727651524e-07}, {"id": 65, "seek": 57200, "start": 572.0, "end": 582.0, "text": " I notice looking in here, this review has one hundred and forty four tokens total and eighty one distinct tokens.", "tokens": [286, 3449, 1237, 294, 510, 11, 341, 3131, 575, 472, 3262, 293, 15815, 1451, 22667, 3217, 293, 26348, 472, 10644, 22667, 13], "temperature": 0.0, "avg_logprob": -0.11036189127776583, "compression_ratio": 1.5605095541401275, "no_speech_prob": 2.225219986939919e-06}, {"id": 66, "seek": 57200, "start": 582.0, "end": 586.0, "text": " And I can get that from looking at this row in the matrix.", "tokens": [400, 286, 393, 483, 300, 490, 1237, 412, 341, 5386, 294, 264, 8141, 13], "temperature": 0.0, "avg_logprob": -0.11036189127776583, "compression_ratio": 1.5605095541401275, "no_speech_prob": 2.225219986939919e-06}, {"id": 67, "seek": 57200, "start": 586.0, "end": 594.0, "text": " It says this is one by six thousand ten with eighty one stored elements.", "tokens": [467, 1619, 341, 307, 472, 538, 2309, 4714, 2064, 365, 26348, 472, 12187, 4959, 13], "temperature": 0.0, "avg_logprob": -0.11036189127776583, "compression_ratio": 1.5605095541401275, "no_speech_prob": 2.225219986939919e-06}, {"id": 68, "seek": 59400, "start": 594.0, "end": 602.0, "text": " So next exercise or first, I just ask questions about that.", "tokens": [407, 958, 5380, 420, 700, 11, 286, 445, 1029, 1651, 466, 300, 13], "temperature": 0.0, "avg_logprob": -0.16319489169430423, "compression_ratio": 1.6613756613756614, "no_speech_prob": 7.182933586591389e-06}, {"id": 69, "seek": 59400, "start": 602.0, "end": 606.0, "text": " How could you convert review data back to text?", "tokens": [1012, 727, 291, 7620, 3131, 1412, 646, 281, 2487, 30], "temperature": 0.0, "avg_logprob": -0.16319489169430423, "compression_ratio": 1.6613756613756614, "no_speech_prob": 7.182933586591389e-06}, {"id": 70, "seek": 59400, "start": 606.0, "end": 616.0, "text": " And here, since we're using a text list, we have these nice kind of like helper attributes of review data and review text.", "tokens": [400, 510, 11, 1670, 321, 434, 1228, 257, 2487, 1329, 11, 321, 362, 613, 1481, 733, 295, 411, 36133, 17212, 295, 3131, 1412, 293, 3131, 2487, 13], "temperature": 0.0, "avg_logprob": -0.16319489169430423, "compression_ratio": 1.6613756613756614, "no_speech_prob": 7.182933586591389e-06}, {"id": 71, "seek": 59400, "start": 616.0, "end": 622.0, "text": " But without just looking up review text, how could you convert review data to text?", "tokens": [583, 1553, 445, 1237, 493, 3131, 2487, 11, 577, 727, 291, 7620, 3131, 1412, 281, 2487, 30], "temperature": 0.0, "avg_logprob": -0.16319489169430423, "compression_ratio": 1.6613756613756614, "no_speech_prob": 7.182933586591389e-06}, {"id": 72, "seek": 62200, "start": 622.0, "end": 630.0, "text": " So take a moment to write the code for that.", "tokens": [407, 747, 257, 1623, 281, 2464, 264, 3089, 337, 300, 13], "temperature": 0.0, "avg_logprob": -0.36887905120849607, "compression_ratio": 1.2564102564102564, "no_speech_prob": 4.539479050436057e-05}, {"id": 73, "seek": 62200, "start": 630.0, "end": 634.0, "text": " I use the movie.", "tokens": [286, 764, 264, 3169, 13], "temperature": 0.0, "avg_logprob": -0.36887905120849607, "compression_ratio": 1.2564102564102564, "no_speech_prob": 4.539479050436057e-05}, {"id": 74, "seek": 62200, "start": 634.0, "end": 636.0, "text": " That's a list. Sorry.", "tokens": [663, 311, 257, 1329, 13, 4919, 13], "temperature": 0.0, "avg_logprob": -0.36887905120849607, "compression_ratio": 1.2564102564102564, "no_speech_prob": 4.539479050436057e-05}, {"id": 75, "seek": 62200, "start": 636.0, "end": 642.0, "text": " To get convert all the integers to strings.", "tokens": [1407, 483, 7620, 439, 264, 41674, 281, 13985, 13], "temperature": 0.0, "avg_logprob": -0.36887905120849607, "compression_ratio": 1.2564102564102564, "no_speech_prob": 4.539479050436057e-05}, {"id": 76, "seek": 62200, "start": 642.0, "end": 644.0, "text": " Exactly. Yes. Yeah.", "tokens": [7587, 13, 1079, 13, 865, 13], "temperature": 0.0, "avg_logprob": -0.36887905120849607, "compression_ratio": 1.2564102564102564, "no_speech_prob": 4.539479050436057e-05}, {"id": 77, "seek": 64400, "start": 644.0, "end": 655.0, "text": " So use that use the list and you can do this inside of list comprehension if you want to get the movie review.vocab.", "tokens": [407, 764, 300, 764, 264, 1329, 293, 291, 393, 360, 341, 1854, 295, 1329, 44991, 498, 291, 528, 281, 483, 264, 3169, 3131, 13, 20836, 455, 13], "temperature": 0.0, "avg_logprob": -0.26410943269729614, "compression_ratio": 1.5481927710843373, "no_speech_prob": 1.1300351616228e-05}, {"id": 78, "seek": 64400, "start": 655.0, "end": 660.0, "text": " I to say for a interview that data.", "tokens": [286, 281, 584, 337, 257, 4049, 300, 1412, 13], "temperature": 0.0, "avg_logprob": -0.26410943269729614, "compression_ratio": 1.5481927710843373, "no_speech_prob": 1.1300351616228e-05}, {"id": 79, "seek": 64400, "start": 660.0, "end": 667.0, "text": " And then I think we have one one more exercise confirmed that the review has eighty one distinct tokens.", "tokens": [400, 550, 286, 519, 321, 362, 472, 472, 544, 5380, 11341, 300, 264, 3131, 575, 26348, 472, 10644, 22667, 13], "temperature": 0.0, "avg_logprob": -0.26410943269729614, "compression_ratio": 1.5481927710843373, "no_speech_prob": 1.1300351616228e-05}, {"id": 80, "seek": 66700, "start": 667.0, "end": 674.0, "text": " You can get the distinct numbers through a set and then find the exactly. Yeah.", "tokens": [509, 393, 483, 264, 10644, 3547, 807, 257, 992, 293, 550, 915, 264, 2293, 13, 865, 13], "temperature": 0.0, "avg_logprob": -0.1984722739771793, "compression_ratio": 1.5566037735849056, "no_speech_prob": 1.9525074094417505e-05}, {"id": 81, "seek": 66700, "start": 674.0, "end": 675.0, "text": " So get the length of the set.", "tokens": [407, 483, 264, 4641, 295, 264, 992, 13], "temperature": 0.0, "avg_logprob": -0.1984722739771793, "compression_ratio": 1.5566037735849056, "no_speech_prob": 1.9525074094417505e-05}, {"id": 82, "seek": 66700, "start": 675.0, "end": 681.0, "text": " I'll tell you how many distinct ones.", "tokens": [286, 603, 980, 291, 577, 867, 10644, 2306, 13], "temperature": 0.0, "avg_logprob": -0.1984722739771793, "compression_ratio": 1.5566037735849056, "no_speech_prob": 1.9525074094417505e-05}, {"id": 83, "seek": 66700, "start": 681.0, "end": 683.0, "text": " Well, thank you.", "tokens": [1042, 11, 1309, 291, 13], "temperature": 0.0, "avg_logprob": -0.1984722739771793, "compression_ratio": 1.5566037735849056, "no_speech_prob": 1.9525074094417505e-05}, {"id": 84, "seek": 66700, "start": 683.0, "end": 688.0, "text": " Hey, it's I just oh, I guess I have a little bit more.", "tokens": [1911, 11, 309, 311, 286, 445, 1954, 11, 286, 2041, 286, 362, 257, 707, 857, 544, 13], "temperature": 0.0, "avg_logprob": -0.1984722739771793, "compression_ratio": 1.5566037735849056, "no_speech_prob": 1.9525074094417505e-05}, {"id": 85, "seek": 66700, "start": 688.0, "end": 696.0, "text": " Oh, here I show that and I think we talked about this last time that string to end is longer than into string.", "tokens": [876, 11, 510, 286, 855, 300, 293, 286, 519, 321, 2825, 466, 341, 1036, 565, 300, 6798, 281, 917, 307, 2854, 813, 666, 6798, 13], "temperature": 0.0, "avg_logprob": -0.1984722739771793, "compression_ratio": 1.5566037735849056, "no_speech_prob": 1.9525074094417505e-05}, {"id": 86, "seek": 69600, "start": 696.0, "end": 705.0, "text": " Since many words are not mapping to unknown and I was kind of just curious about what words were getting mapped to unknown.", "tokens": [4162, 867, 2283, 366, 406, 18350, 281, 9841, 293, 286, 390, 733, 295, 445, 6369, 466, 437, 2283, 645, 1242, 33318, 281, 9841, 13], "temperature": 0.0, "avg_logprob": -0.14326723946465386, "compression_ratio": 1.599078341013825, "no_speech_prob": 3.822330108960159e-05}, {"id": 87, "seek": 69600, "start": 705.0, "end": 708.0, "text": " And so I.", "tokens": [400, 370, 286, 13], "temperature": 0.0, "avg_logprob": -0.14326723946465386, "compression_ratio": 1.599078341013825, "no_speech_prob": 3.822330108960159e-05}, {"id": 88, "seek": 69600, "start": 708.0, "end": 716.0, "text": " But a little loop to go through the items and see if they map to unknown, put them in this list.", "tokens": [583, 257, 707, 6367, 281, 352, 807, 264, 4754, 293, 536, 498, 436, 4471, 281, 9841, 11, 829, 552, 294, 341, 1329, 13], "temperature": 0.0, "avg_logprob": -0.14326723946465386, "compression_ratio": 1.599078341013825, "no_speech_prob": 3.822330108960159e-05}, {"id": 89, "seek": 69600, "start": 716.0, "end": 718.0, "text": " Just to see what they were.", "tokens": [1449, 281, 536, 437, 436, 645, 13], "temperature": 0.0, "avg_logprob": -0.14326723946465386, "compression_ratio": 1.599078341013825, "no_speech_prob": 3.822330108960159e-05}, {"id": 90, "seek": 69600, "start": 718.0, "end": 723.0, "text": " And here, remember, we've capped it at only having six thousand words in our vocabulary.", "tokens": [400, 510, 11, 1604, 11, 321, 600, 1335, 3320, 309, 412, 787, 1419, 2309, 4714, 2283, 294, 527, 19864, 13], "temperature": 0.0, "avg_logprob": -0.14326723946465386, "compression_ratio": 1.599078341013825, "no_speech_prob": 3.822330108960159e-05}, {"id": 91, "seek": 72300, "start": 723.0, "end": 726.0, "text": " So a lot of these words are.", "tokens": [407, 257, 688, 295, 613, 2283, 366, 13], "temperature": 0.0, "avg_logprob": -0.10440274831411, "compression_ratio": 1.725868725868726, "no_speech_prob": 2.7531366868061014e-05}, {"id": 92, "seek": 72300, "start": 726.0, "end": 731.0, "text": " I know words that probably do show up in many movie reviews and later on will switch to right now.", "tokens": [286, 458, 2283, 300, 1391, 360, 855, 493, 294, 867, 3169, 10229, 293, 1780, 322, 486, 3679, 281, 558, 586, 13], "temperature": 0.0, "avg_logprob": -0.10440274831411, "compression_ratio": 1.725868725868726, "no_speech_prob": 2.7531366868061014e-05}, {"id": 93, "seek": 72300, "start": 731.0, "end": 733.0, "text": " We're just working on a sample of the data set.", "tokens": [492, 434, 445, 1364, 322, 257, 6889, 295, 264, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.10440274831411, "compression_ratio": 1.725868725868726, "no_speech_prob": 2.7531366868061014e-05}, {"id": 94, "seek": 72300, "start": 733.0, "end": 737.0, "text": " Later on, we'll switch to a larger data set and use a larger vocabulary.", "tokens": [11965, 322, 11, 321, 603, 3679, 281, 257, 4833, 1412, 992, 293, 764, 257, 4833, 19864, 13], "temperature": 0.0, "avg_logprob": -0.10440274831411, "compression_ratio": 1.725868725868726, "no_speech_prob": 2.7531366868061014e-05}, {"id": 95, "seek": 72300, "start": 737.0, "end": 748.0, "text": " But I like doing I like doing some of these exercises of just making sure that you really understand kind of what the data means and how it's formatted and how you can ask little questions about it.", "tokens": [583, 286, 411, 884, 286, 411, 884, 512, 295, 613, 11900, 295, 445, 1455, 988, 300, 291, 534, 1223, 733, 295, 437, 264, 1412, 1355, 293, 577, 309, 311, 1254, 32509, 293, 577, 291, 393, 1029, 707, 1651, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.10440274831411, "compression_ratio": 1.725868725868726, "no_speech_prob": 2.7531366868061014e-05}, {"id": 96, "seek": 74800, "start": 748.0, "end": 755.0, "text": " I think it's nice before you dive into the actual kind of computation that you're doing.", "tokens": [286, 519, 309, 311, 1481, 949, 291, 9192, 666, 264, 3539, 733, 295, 24903, 300, 291, 434, 884, 13], "temperature": 0.0, "avg_logprob": -0.146922550201416, "compression_ratio": 1.5129310344827587, "no_speech_prob": 2.2124080715002492e-05}, {"id": 97, "seek": 74800, "start": 755.0, "end": 759.0, "text": " Yeah, so to to use naive Bayes and I should check.", "tokens": [865, 11, 370, 281, 281, 764, 29052, 7840, 279, 293, 286, 820, 1520, 13], "temperature": 0.0, "avg_logprob": -0.146922550201416, "compression_ratio": 1.5129310344827587, "no_speech_prob": 2.2124080715002492e-05}, {"id": 98, "seek": 74800, "start": 759.0, "end": 762.0, "text": " Have you covered naive Bayes this year and other classes?", "tokens": [3560, 291, 5343, 29052, 7840, 279, 341, 1064, 293, 661, 5359, 30], "temperature": 0.0, "avg_logprob": -0.146922550201416, "compression_ratio": 1.5129310344827587, "no_speech_prob": 2.2124080715002492e-05}, {"id": 99, "seek": 74800, "start": 762.0, "end": 764.0, "text": " OK, I see lots of nodding.", "tokens": [2264, 11, 286, 536, 3195, 295, 15224, 3584, 13], "temperature": 0.0, "avg_logprob": -0.146922550201416, "compression_ratio": 1.5129310344827587, "no_speech_prob": 2.2124080715002492e-05}, {"id": 100, "seek": 74800, "start": 764.0, "end": 766.0, "text": " So hopefully you're familiar with this.", "tokens": [407, 4696, 291, 434, 4963, 365, 341, 13], "temperature": 0.0, "avg_logprob": -0.146922550201416, "compression_ratio": 1.5129310344827587, "no_speech_prob": 2.2124080715002492e-05}, {"id": 101, "seek": 74800, "start": 766.0, "end": 768.0, "text": " We don't have to spend too long on it.", "tokens": [492, 500, 380, 362, 281, 3496, 886, 938, 322, 309, 13], "temperature": 0.0, "avg_logprob": -0.146922550201416, "compression_ratio": 1.5129310344827587, "no_speech_prob": 2.2124080715002492e-05}, {"id": 102, "seek": 74800, "start": 768.0, "end": 772.0, "text": " We'll define the log count ratio for each word.", "tokens": [492, 603, 6964, 264, 3565, 1207, 8509, 337, 1184, 1349, 13], "temperature": 0.0, "avg_logprob": -0.146922550201416, "compression_ratio": 1.5129310344827587, "no_speech_prob": 2.2124080715002492e-05}, {"id": 103, "seek": 77200, "start": 772.0, "end": 782.0, "text": " And that's the ratio of the feature and positive documents over the ratio of the feature and negative documents.", "tokens": [400, 300, 311, 264, 8509, 295, 264, 4111, 293, 3353, 8512, 670, 264, 8509, 295, 264, 4111, 293, 3671, 8512, 13], "temperature": 0.0, "avg_logprob": -0.1561370546167547, "compression_ratio": 1.5327102803738317, "no_speech_prob": 3.6119135984336026e-06}, {"id": 104, "seek": 77200, "start": 782.0, "end": 786.0, "text": " And so.", "tokens": [400, 370, 13], "temperature": 0.0, "avg_logprob": -0.1561370546167547, "compression_ratio": 1.5327102803738317, "no_speech_prob": 3.6119135984336026e-06}, {"id": 105, "seek": 77200, "start": 786.0, "end": 794.0, "text": " So we define our X and Y, our validation Y.", "tokens": [407, 321, 6964, 527, 1783, 293, 398, 11, 527, 24071, 398, 13], "temperature": 0.0, "avg_logprob": -0.1561370546167547, "compression_ratio": 1.5327102803738317, "no_speech_prob": 3.6119135984336026e-06}, {"id": 106, "seek": 79400, "start": 794.0, "end": 804.0, "text": " And then here to kind of to calculate these log count ratios, we'll want to get the.", "tokens": [400, 550, 510, 281, 733, 295, 281, 8873, 613, 3565, 1207, 32435, 11, 321, 603, 528, 281, 483, 264, 13], "temperature": 0.0, "avg_logprob": -0.12638115882873535, "compression_ratio": 1.2708333333333333, "no_speech_prob": 3.844816546916263e-06}, {"id": 107, "seek": 79400, "start": 804.0, "end": 811.0, "text": " Is there a question?", "tokens": [1119, 456, 257, 1168, 30], "temperature": 0.0, "avg_logprob": -0.12638115882873535, "compression_ratio": 1.2708333333333333, "no_speech_prob": 3.844816546916263e-06}, {"id": 108, "seek": 79400, "start": 811.0, "end": 815.0, "text": " Want to get the.", "tokens": [11773, 281, 483, 264, 13], "temperature": 0.0, "avg_logprob": -0.12638115882873535, "compression_ratio": 1.2708333333333333, "no_speech_prob": 3.844816546916263e-06}, {"id": 109, "seek": 81500, "start": 815.0, "end": 824.0, "text": " The words that show up in positive reviews and the words that show up in negative reviews and kind of the number of times on average.", "tokens": [440, 2283, 300, 855, 493, 294, 3353, 10229, 293, 264, 2283, 300, 855, 493, 294, 3671, 10229, 293, 733, 295, 264, 1230, 295, 1413, 322, 4274, 13], "temperature": 0.0, "avg_logprob": -0.09185428180913816, "compression_ratio": 1.9207920792079207, "no_speech_prob": 9.817855243454687e-06}, {"id": 110, "seek": 81500, "start": 824.0, "end": 831.0, "text": " So you can think like how often on average does the word loved show up in a positive review versus a negative review.", "tokens": [407, 291, 393, 519, 411, 577, 2049, 322, 4274, 775, 264, 1349, 4333, 855, 493, 294, 257, 3353, 3131, 5717, 257, 3671, 3131, 13], "temperature": 0.0, "avg_logprob": -0.09185428180913816, "compression_ratio": 1.9207920792079207, "no_speech_prob": 9.817855243454687e-06}, {"id": 111, "seek": 81500, "start": 831.0, "end": 843.0, "text": " And so that's what we're doing here with taking the negative reviews and then X is giving us word counts from our term document metrics.", "tokens": [400, 370, 300, 311, 437, 321, 434, 884, 510, 365, 1940, 264, 3671, 10229, 293, 550, 1783, 307, 2902, 505, 1349, 14893, 490, 527, 1433, 4166, 16367, 13], "temperature": 0.0, "avg_logprob": -0.09185428180913816, "compression_ratio": 1.9207920792079207, "no_speech_prob": 9.817855243454687e-06}, {"id": 112, "seek": 84300, "start": 843.0, "end": 850.0, "text": " What is that? What is NP dot squeeze do?", "tokens": [708, 307, 300, 30, 708, 307, 38611, 5893, 13578, 360, 30], "temperature": 0.0, "avg_logprob": -0.1866214554031174, "compression_ratio": 1.5909090909090908, "no_speech_prob": 2.0460907762753777e-05}, {"id": 113, "seek": 84300, "start": 850.0, "end": 855.0, "text": " If you have a nested array here, it's taking off.", "tokens": [759, 291, 362, 257, 15646, 292, 10225, 510, 11, 309, 311, 1940, 766, 13], "temperature": 0.0, "avg_logprob": -0.1866214554031174, "compression_ratio": 1.5909090909090908, "no_speech_prob": 2.0460907762753777e-05}, {"id": 114, "seek": 84300, "start": 855.0, "end": 862.0, "text": " Kind of one of those nestings and I saw some people doing the motion and kind of giving you giving you a single single array.", "tokens": [9242, 295, 472, 295, 729, 15646, 1109, 293, 286, 1866, 512, 561, 884, 264, 5394, 293, 733, 295, 2902, 291, 2902, 291, 257, 2167, 2167, 10225, 13], "temperature": 0.0, "avg_logprob": -0.1866214554031174, "compression_ratio": 1.5909090909090908, "no_speech_prob": 2.0460907762753777e-05}, {"id": 115, "seek": 84300, "start": 862.0, "end": 865.0, "text": " So here we had this was and really it's taking off a dimension.", "tokens": [407, 510, 321, 632, 341, 390, 293, 534, 309, 311, 1940, 766, 257, 10139, 13], "temperature": 0.0, "avg_logprob": -0.1866214554031174, "compression_ratio": 1.5909090909090908, "no_speech_prob": 2.0460907762753777e-05}, {"id": 116, "seek": 86500, "start": 865.0, "end": 873.0, "text": " But in this case, you're going from a.", "tokens": [583, 294, 341, 1389, 11, 291, 434, 516, 490, 257, 13], "temperature": 0.0, "avg_logprob": -0.16018258279828884, "compression_ratio": 1.528301886792453, "no_speech_prob": 9.665682227932848e-06}, {"id": 117, "seek": 86500, "start": 873.0, "end": 876.0, "text": " Array inside an array just to an array.", "tokens": [1587, 3458, 1854, 364, 10225, 445, 281, 364, 10225, 13], "temperature": 0.0, "avg_logprob": -0.16018258279828884, "compression_ratio": 1.528301886792453, "no_speech_prob": 9.665682227932848e-06}, {"id": 118, "seek": 86500, "start": 876.0, "end": 879.0, "text": " So summing up the positives and negatives.", "tokens": [407, 2408, 2810, 493, 264, 35127, 293, 40019, 13], "temperature": 0.0, "avg_logprob": -0.16018258279828884, "compression_ratio": 1.528301886792453, "no_speech_prob": 9.665682227932848e-06}, {"id": 119, "seek": 86500, "start": 879.0, "end": 887.0, "text": " So now I want you to compare how often does love appear in positive reviews versus negative reviews.", "tokens": [407, 586, 286, 528, 291, 281, 6794, 577, 2049, 775, 959, 4204, 294, 3353, 10229, 5717, 3671, 10229, 13], "temperature": 0.0, "avg_logprob": -0.16018258279828884, "compression_ratio": 1.528301886792453, "no_speech_prob": 9.665682227932848e-06}, {"id": 120, "seek": 86500, "start": 887.0, "end": 890.0, "text": " So take a moment to.", "tokens": [407, 747, 257, 1623, 281, 13], "temperature": 0.0, "avg_logprob": -0.16018258279828884, "compression_ratio": 1.528301886792453, "no_speech_prob": 9.665682227932848e-06}, {"id": 121, "seek": 89000, "start": 890.0, "end": 901.0, "text": " To write the code for that. So for for loved, I looked up the index of that with moody reusable cabs index.", "tokens": [1407, 2464, 264, 3089, 337, 300, 13, 407, 337, 337, 4333, 11, 286, 2956, 493, 264, 8186, 295, 300, 365, 705, 843, 41807, 5487, 82, 8186, 13], "temperature": 0.0, "avg_logprob": -0.23041543399586398, "compression_ratio": 1.5845410628019323, "no_speech_prob": 6.438601758418372e-06}, {"id": 122, "seek": 89000, "start": 901.0, "end": 910.0, "text": " And then I just indexed P zero and P one by that since it's just the length of the vocab for all positive or negative documents.", "tokens": [400, 550, 286, 445, 8186, 292, 430, 4018, 293, 430, 472, 538, 300, 1670, 309, 311, 445, 264, 4641, 295, 264, 2329, 455, 337, 439, 3353, 420, 3671, 8512, 13], "temperature": 0.0, "avg_logprob": -0.23041543399586398, "compression_ratio": 1.5845410628019323, "no_speech_prob": 6.438601758418372e-06}, {"id": 123, "seek": 89000, "start": 910.0, "end": 915.0, "text": " And that gave me the number of times the terms is done in the same for hated exactly great.", "tokens": [400, 300, 2729, 385, 264, 1230, 295, 1413, 264, 2115, 307, 1096, 294, 264, 912, 337, 17398, 2293, 869, 13], "temperature": 0.0, "avg_logprob": -0.23041543399586398, "compression_ratio": 1.5845410628019323, "no_speech_prob": 6.438601758418372e-06}, {"id": 124, "seek": 91500, "start": 915.0, "end": 923.0, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.12671473622322083, "compression_ratio": 1.4197530864197532, "no_speech_prob": 2.2602462195209228e-06}, {"id": 125, "seek": 91500, "start": 923.0, "end": 932.0, "text": " Yeah, so it's just looking up the strength index for these words and then looking them up and P not and P one.", "tokens": [865, 11, 370, 309, 311, 445, 1237, 493, 264, 3800, 8186, 337, 613, 2283, 293, 550, 1237, 552, 493, 293, 430, 406, 293, 430, 472, 13], "temperature": 0.0, "avg_logprob": -0.12671473622322083, "compression_ratio": 1.4197530864197532, "no_speech_prob": 2.2602462195209228e-06}, {"id": 126, "seek": 91500, "start": 932.0, "end": 936.0, "text": " And so this is kind of a sanity check on your data.", "tokens": [400, 370, 341, 307, 733, 295, 257, 47892, 1520, 322, 428, 1412, 13], "temperature": 0.0, "avg_logprob": -0.12671473622322083, "compression_ratio": 1.4197530864197532, "no_speech_prob": 2.2602462195209228e-06}, {"id": 127, "seek": 91500, "start": 936.0, "end": 942.0, "text": " I would expect love to show up more in positive reviews.", "tokens": [286, 576, 2066, 959, 281, 855, 493, 544, 294, 3353, 10229, 13], "temperature": 0.0, "avg_logprob": -0.12671473622322083, "compression_ratio": 1.4197530864197532, "no_speech_prob": 2.2602462195209228e-06}, {"id": 128, "seek": 94200, "start": 942.0, "end": 946.0, "text": " And let's confirm.", "tokens": [400, 718, 311, 9064, 13], "temperature": 0.0, "avg_logprob": -0.15755604252670752, "compression_ratio": 1.5694444444444444, "no_speech_prob": 1.1659038136713207e-05}, {"id": 129, "seek": 94200, "start": 946.0, "end": 948.0, "text": " Where did I.", "tokens": [2305, 630, 286, 13], "temperature": 0.0, "avg_logprob": -0.15755604252670752, "compression_ratio": 1.5694444444444444, "no_speech_prob": 1.1659038136713207e-05}, {"id": 130, "seek": 94200, "start": 948.0, "end": 953.0, "text": " Find P not P one is positive P not is negative.", "tokens": [11809, 430, 406, 430, 472, 307, 3353, 430, 406, 307, 3671, 13], "temperature": 0.0, "avg_logprob": -0.15755604252670752, "compression_ratio": 1.5694444444444444, "no_speech_prob": 1.1659038136713207e-05}, {"id": 131, "seek": 94200, "start": 953.0, "end": 957.0, "text": " And so yes, love is showing up more often in positive reviews.", "tokens": [400, 370, 2086, 11, 959, 307, 4099, 493, 544, 2049, 294, 3353, 10229, 13], "temperature": 0.0, "avg_logprob": -0.15755604252670752, "compression_ratio": 1.5694444444444444, "no_speech_prob": 1.1659038136713207e-05}, {"id": 132, "seek": 94200, "start": 957.0, "end": 961.0, "text": " Hated is showing up more often in negative reviews.", "tokens": [389, 770, 307, 4099, 493, 544, 2049, 294, 3671, 10229, 13], "temperature": 0.0, "avg_logprob": -0.15755604252670752, "compression_ratio": 1.5694444444444444, "no_speech_prob": 1.1659038136713207e-05}, {"id": 133, "seek": 94200, "start": 961.0, "end": 963.0, "text": " There are questions about that.", "tokens": [821, 366, 1651, 466, 300, 13], "temperature": 0.0, "avg_logprob": -0.15755604252670752, "compression_ratio": 1.5694444444444444, "no_speech_prob": 1.1659038136713207e-05}, {"id": 134, "seek": 96300, "start": 963.0, "end": 975.0, "text": " So here we've really just taken the counts of kind of how often these these words show up in positive versus negative reviews.", "tokens": [407, 510, 321, 600, 534, 445, 2726, 264, 14893, 295, 733, 295, 577, 2049, 613, 613, 2283, 855, 493, 294, 3353, 5717, 3671, 10229, 13], "temperature": 0.0, "avg_logprob": -0.09344180425008138, "compression_ratio": 1.5828877005347595, "no_speech_prob": 3.844855655188439e-06}, {"id": 135, "seek": 96300, "start": 975.0, "end": 981.0, "text": " And then I was I was curious to look at an example of a positive review with the word hated in it.", "tokens": [400, 550, 286, 390, 286, 390, 6369, 281, 574, 412, 364, 1365, 295, 257, 3353, 3131, 365, 264, 1349, 17398, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.09344180425008138, "compression_ratio": 1.5828877005347595, "no_speech_prob": 3.844855655188439e-06}, {"id": 136, "seek": 96300, "start": 981.0, "end": 983.0, "text": " So.", "tokens": [407, 13], "temperature": 0.0, "avg_logprob": -0.09344180425008138, "compression_ratio": 1.5828877005347595, "no_speech_prob": 3.844855655188439e-06}, {"id": 137, "seek": 96300, "start": 983.0, "end": 991.0, "text": " Just getting the integer for that token is nineteen seventy seven.", "tokens": [1449, 1242, 264, 24922, 337, 300, 14862, 307, 31555, 25662, 3407, 13], "temperature": 0.0, "avg_logprob": -0.09344180425008138, "compression_ratio": 1.5828877005347595, "no_speech_prob": 3.844855655188439e-06}, {"id": 138, "seek": 99100, "start": 991.0, "end": 1002.0, "text": " I used and P dot are where to find which reviews have a non zero value for that.", "tokens": [286, 1143, 293, 430, 5893, 366, 689, 281, 915, 597, 10229, 362, 257, 2107, 4018, 2158, 337, 300, 13], "temperature": 0.0, "avg_logprob": -0.18088715318320453, "compression_ratio": 1.56875, "no_speech_prob": 1.5445764802279882e-05}, {"id": 139, "seek": 99100, "start": 1002.0, "end": 1005.0, "text": " Then saw.", "tokens": [1396, 1866, 13], "temperature": 0.0, "avg_logprob": -0.18088715318320453, "compression_ratio": 1.56875, "no_speech_prob": 1.5445764802279882e-05}, {"id": 140, "seek": 99100, "start": 1005.0, "end": 1009.0, "text": " See.", "tokens": [3008, 13], "temperature": 0.0, "avg_logprob": -0.18088715318320453, "compression_ratio": 1.56875, "no_speech_prob": 1.5445764802279882e-05}, {"id": 141, "seek": 99100, "start": 1009.0, "end": 1015.0, "text": " Which of those were then I got all the positive reviews and I took a set intersection of those two things.", "tokens": [3013, 295, 729, 645, 550, 286, 658, 439, 264, 3353, 10229, 293, 286, 1890, 257, 992, 15236, 295, 729, 732, 721, 13], "temperature": 0.0, "avg_logprob": -0.18088715318320453, "compression_ratio": 1.56875, "no_speech_prob": 1.5445764802279882e-05}, {"id": 142, "seek": 99100, "start": 1015.0, "end": 1018.0, "text": " So got all the reviews that have the word hated.", "tokens": [407, 658, 439, 264, 10229, 300, 362, 264, 1349, 17398, 13], "temperature": 0.0, "avg_logprob": -0.18088715318320453, "compression_ratio": 1.56875, "no_speech_prob": 1.5445764802279882e-05}, {"id": 143, "seek": 101800, "start": 1018.0, "end": 1021.0, "text": " Saw which ones of those were positive.", "tokens": [27307, 597, 2306, 295, 729, 645, 3353, 13], "temperature": 0.0, "avg_logprob": -0.13290589815610415, "compression_ratio": 1.547486033519553, "no_speech_prob": 1.0289021702192258e-05}, {"id": 144, "seek": 101800, "start": 1021.0, "end": 1024.0, "text": " It was these three.", "tokens": [467, 390, 613, 1045, 13], "temperature": 0.0, "avg_logprob": -0.13290589815610415, "compression_ratio": 1.547486033519553, "no_speech_prob": 1.0289021702192258e-05}, {"id": 145, "seek": 101800, "start": 1024.0, "end": 1029.0, "text": " Rose for the movie reviews and looking at one of them.", "tokens": [12765, 337, 264, 3169, 10229, 293, 1237, 412, 472, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.13290589815610415, "compression_ratio": 1.547486033519553, "no_speech_prob": 1.0289021702192258e-05}, {"id": 146, "seek": 101800, "start": 1029.0, "end": 1031.0, "text": " It was interesting to see that it.", "tokens": [467, 390, 1880, 281, 536, 300, 309, 13], "temperature": 0.0, "avg_logprob": -0.13290589815610415, "compression_ratio": 1.547486033519553, "no_speech_prob": 1.0289021702192258e-05}, {"id": 147, "seek": 101800, "start": 1031.0, "end": 1038.0, "text": " Yeah, this episode is extremely underrated.", "tokens": [865, 11, 341, 3500, 307, 4664, 833, 5468, 13], "temperature": 0.0, "avg_logprob": -0.13290589815610415, "compression_ratio": 1.547486033519553, "no_speech_prob": 1.0289021702192258e-05}, {"id": 148, "seek": 101800, "start": 1038.0, "end": 1041.0, "text": " I love the unknown parts and the big twist at the end.", "tokens": [286, 959, 264, 9841, 3166, 293, 264, 955, 8203, 412, 264, 917, 13], "temperature": 0.0, "avg_logprob": -0.13290589815610415, "compression_ratio": 1.547486033519553, "no_speech_prob": 1.0289021702192258e-05}, {"id": 149, "seek": 101800, "start": 1041.0, "end": 1044.0, "text": " I absolutely love that scene.", "tokens": [286, 3122, 959, 300, 4145, 13], "temperature": 0.0, "avg_logprob": -0.13290589815610415, "compression_ratio": 1.547486033519553, "no_speech_prob": 1.0289021702192258e-05}, {"id": 150, "seek": 104400, "start": 1044.0, "end": 1050.0, "text": " For some reason, people have always hated the unknown episodes, yet I've always liked them.", "tokens": [1171, 512, 1778, 11, 561, 362, 1009, 17398, 264, 9841, 9313, 11, 1939, 286, 600, 1009, 4501, 552, 13], "temperature": 0.0, "avg_logprob": -0.08501676321029664, "compression_ratio": 1.6097560975609757, "no_speech_prob": 2.6015593448391883e-06}, {"id": 151, "seek": 104400, "start": 1050.0, "end": 1061.0, "text": " And so it's just kind of neat to confirm like, yes, this is clearly a positive review, but it does have hated because it's saying how other people hate this.", "tokens": [400, 370, 309, 311, 445, 733, 295, 10654, 281, 9064, 411, 11, 2086, 11, 341, 307, 4448, 257, 3353, 3131, 11, 457, 309, 775, 362, 17398, 570, 309, 311, 1566, 577, 661, 561, 4700, 341, 13], "temperature": 0.0, "avg_logprob": -0.08501676321029664, "compression_ratio": 1.6097560975609757, "no_speech_prob": 2.6015593448391883e-06}, {"id": 152, "seek": 104400, "start": 1061.0, "end": 1066.0, "text": " And then I did the same thing for finding a negative review with the word loved.", "tokens": [400, 550, 286, 630, 264, 912, 551, 337, 5006, 257, 3671, 3131, 365, 264, 1349, 4333, 13], "temperature": 0.0, "avg_logprob": -0.08501676321029664, "compression_ratio": 1.6097560975609757, "no_speech_prob": 2.6015593448391883e-06}, {"id": 153, "seek": 106600, "start": 1066.0, "end": 1077.0, "text": " I got all the word other views that have loved all the reviews that are negative took their set intersection and.", "tokens": [286, 658, 439, 264, 1349, 661, 6809, 300, 362, 4333, 439, 264, 10229, 300, 366, 3671, 1890, 641, 992, 15236, 293, 13], "temperature": 0.0, "avg_logprob": -0.16067105929056805, "compression_ratio": 1.5503355704697988, "no_speech_prob": 5.862672878720332e-06}, {"id": 154, "seek": 106600, "start": 1077.0, "end": 1079.0, "text": " Looked at this one.", "tokens": [2053, 292, 412, 341, 472, 13], "temperature": 0.0, "avg_logprob": -0.16067105929056805, "compression_ratio": 1.5503355704697988, "no_speech_prob": 5.862672878720332e-06}, {"id": 155, "seek": 106600, "start": 1079.0, "end": 1082.0, "text": " I love the first is zooming movie.", "tokens": [286, 959, 264, 700, 307, 48226, 3169, 13], "temperature": 0.0, "avg_logprob": -0.16067105929056805, "compression_ratio": 1.5503355704697988, "no_speech_prob": 5.862672878720332e-06}, {"id": 156, "seek": 106600, "start": 1082.0, "end": 1089.0, "text": " I have to say that this movie was much weaker than I expected.", "tokens": [286, 362, 281, 584, 300, 341, 3169, 390, 709, 24286, 813, 286, 5176, 13], "temperature": 0.0, "avg_logprob": -0.16067105929056805, "compression_ratio": 1.5503355704697988, "no_speech_prob": 5.862672878720332e-06}, {"id": 157, "seek": 108900, "start": 1089.0, "end": 1100.0, "text": " And you can see how this is a negative, even though they're talking about having loved a previous movie.", "tokens": [400, 291, 393, 536, 577, 341, 307, 257, 3671, 11, 754, 1673, 436, 434, 1417, 466, 1419, 4333, 257, 3894, 3169, 13], "temperature": 0.0, "avg_logprob": -0.24848438316667584, "compression_ratio": 1.3430232558139534, "no_speech_prob": 2.4680659407749772e-05}, {"id": 158, "seek": 108900, "start": 1100.0, "end": 1106.0, "text": " Yes, Rebecca. Oh, can I throw you the catch box?", "tokens": [1079, 11, 19381, 13, 876, 11, 393, 286, 3507, 291, 264, 3745, 2424, 30], "temperature": 0.0, "avg_logprob": -0.24848438316667584, "compression_ratio": 1.3430232558139534, "no_speech_prob": 2.4680659407749772e-05}, {"id": 159, "seek": 108900, "start": 1106.0, "end": 1108.0, "text": " Oh, wait, let me sorry.", "tokens": [876, 11, 1699, 11, 718, 385, 2597, 13], "temperature": 0.0, "avg_logprob": -0.24848438316667584, "compression_ratio": 1.3430232558139534, "no_speech_prob": 2.4680659407749772e-05}, {"id": 160, "seek": 108900, "start": 1108.0, "end": 1112.0, "text": " Where do we get the 200 that we're indexing?", "tokens": [2305, 360, 321, 483, 264, 2331, 300, 321, 434, 8186, 278, 30], "temperature": 0.0, "avg_logprob": -0.24848438316667584, "compression_ratio": 1.3430232558139534, "no_speech_prob": 2.4680659407749772e-05}, {"id": 161, "seek": 108900, "start": 1112.0, "end": 1116.0, "text": " The 200.", "tokens": [440, 2331, 13], "temperature": 0.0, "avg_logprob": -0.24848438316667584, "compression_ratio": 1.3430232558139534, "no_speech_prob": 2.4680659407749772e-05}, {"id": 162, "seek": 111600, "start": 1116.0, "end": 1125.0, "text": " Oh, OK, so I am I might have I think I did this on the subsample set and then later had done it on the full set.", "tokens": [876, 11, 2264, 11, 370, 286, 669, 286, 1062, 362, 286, 519, 286, 630, 341, 322, 264, 2090, 335, 781, 992, 293, 550, 1780, 632, 1096, 309, 322, 264, 1577, 992, 13], "temperature": 0.0, "avg_logprob": -0.1516400796395761, "compression_ratio": 1.4722222222222223, "no_speech_prob": 2.111126195813995e-05}, {"id": 163, "seek": 111600, "start": 1125.0, "end": 1128.0, "text": " So that may be an error.", "tokens": [407, 300, 815, 312, 364, 6713, 13], "temperature": 0.0, "avg_logprob": -0.1516400796395761, "compression_ratio": 1.4722222222222223, "no_speech_prob": 2.111126195813995e-05}, {"id": 164, "seek": 111600, "start": 1128.0, "end": 1134.0, "text": " Well.", "tokens": [1042, 13], "temperature": 0.0, "avg_logprob": -0.1516400796395761, "compression_ratio": 1.4722222222222223, "no_speech_prob": 2.111126195813995e-05}, {"id": 165, "seek": 111600, "start": 1134.0, "end": 1138.0, "text": " Yeah, I'm going to guess that that's from a previous a previous time through.", "tokens": [865, 11, 286, 478, 516, 281, 2041, 300, 300, 311, 490, 257, 3894, 257, 3894, 565, 807, 13], "temperature": 0.0, "avg_logprob": -0.1516400796395761, "compression_ratio": 1.4722222222222223, "no_speech_prob": 2.111126195813995e-05}, {"id": 166, "seek": 111600, "start": 1138.0, "end": 1143.0, "text": " We could just plug in one of those numbers.", "tokens": [492, 727, 445, 5452, 294, 472, 295, 729, 3547, 13], "temperature": 0.0, "avg_logprob": -0.1516400796395761, "compression_ratio": 1.4722222222222223, "no_speech_prob": 2.111126195813995e-05}, {"id": 167, "seek": 114300, "start": 1143.0, "end": 1146.0, "text": " Yeah, sorry about that. I have not been consistent.", "tokens": [865, 11, 2597, 466, 300, 13, 286, 362, 406, 668, 8398, 13], "temperature": 0.0, "avg_logprob": -0.12384221428319027, "compression_ratio": 1.4861111111111112, "no_speech_prob": 5.47530289622955e-05}, {"id": 168, "seek": 114300, "start": 1146.0, "end": 1150.0, "text": " I think when running the notebook, sometimes I am running the cells.", "tokens": [286, 519, 562, 2614, 264, 21060, 11, 2171, 286, 669, 2614, 264, 5438, 13], "temperature": 0.0, "avg_logprob": -0.12384221428319027, "compression_ratio": 1.4861111111111112, "no_speech_prob": 5.47530289622955e-05}, {"id": 169, "seek": 114300, "start": 1150.0, "end": 1151.0, "text": " Sometimes I'm not.", "tokens": [4803, 286, 478, 406, 13], "temperature": 0.0, "avg_logprob": -0.12384221428319027, "compression_ratio": 1.4861111111111112, "no_speech_prob": 5.47530289622955e-05}, {"id": 170, "seek": 114300, "start": 1151.0, "end": 1158.0, "text": " But use use one of the numbers that shows up here in the set intersection.", "tokens": [583, 764, 764, 472, 295, 264, 3547, 300, 3110, 493, 510, 294, 264, 992, 15236, 13], "temperature": 0.0, "avg_logprob": -0.12384221428319027, "compression_ratio": 1.4861111111111112, "no_speech_prob": 5.47530289622955e-05}, {"id": 171, "seek": 115800, "start": 1158.0, "end": 1184.0, "text": " Which maybe it's risky to try to see.", "tokens": [3013, 1310, 309, 311, 21137, 281, 853, 281, 536, 13], "temperature": 0.0, "avg_logprob": -0.12813590254102433, "compression_ratio": 0.9024390243902439, "no_speech_prob": 6.048735031072283e-06}, {"id": 172, "seek": 118400, "start": 1184.0, "end": 1189.0, "text": " OK, let's let's go on to applying naive Bayes.", "tokens": [2264, 11, 718, 311, 718, 311, 352, 322, 281, 9275, 29052, 7840, 279, 13], "temperature": 0.0, "avg_logprob": -0.12450892666736281, "compression_ratio": 1.595, "no_speech_prob": 1.2028865967295133e-05}, {"id": 173, "seek": 118400, "start": 1189.0, "end": 1202.0, "text": " But that's just I thought a neat way to kind of also see like what is this P zero and P one mean in terms of having these counts of words and positive reviews and words and negative reviews.", "tokens": [583, 300, 311, 445, 286, 1194, 257, 10654, 636, 281, 733, 295, 611, 536, 411, 437, 307, 341, 430, 4018, 293, 430, 472, 914, 294, 2115, 295, 1419, 613, 14893, 295, 2283, 293, 3353, 10229, 293, 2283, 293, 3671, 10229, 13], "temperature": 0.0, "avg_logprob": -0.12450892666736281, "compression_ratio": 1.595, "no_speech_prob": 1.2028865967295133e-05}, {"id": 174, "seek": 118400, "start": 1202.0, "end": 1207.0, "text": " I've just rewritten them here so you could see kind of all at once what they are.", "tokens": [286, 600, 445, 319, 26859, 552, 510, 370, 291, 727, 536, 733, 295, 439, 412, 1564, 437, 436, 366, 13], "temperature": 0.0, "avg_logprob": -0.12450892666736281, "compression_ratio": 1.595, "no_speech_prob": 1.2028865967295133e-05}, {"id": 175, "seek": 120700, "start": 1207.0, "end": 1219.0, "text": " What we'll be doing is adding one to each and this is just averaging over the number of positive reviews averaging over the number of negative reviews.", "tokens": [708, 321, 603, 312, 884, 307, 5127, 472, 281, 1184, 293, 341, 307, 445, 47308, 670, 264, 1230, 295, 3353, 10229, 47308, 670, 264, 1230, 295, 3671, 10229, 13], "temperature": 0.0, "avg_logprob": -0.09389646959976411, "compression_ratio": 1.7283236994219653, "no_speech_prob": 7.766461749270093e-06}, {"id": 176, "seek": 120700, "start": 1219.0, "end": 1227.0, "text": " And we've added a one in the numerator and in the denominator to help with numerical stability.", "tokens": [400, 321, 600, 3869, 257, 472, 294, 264, 30380, 293, 294, 264, 20687, 281, 854, 365, 29054, 11826, 13], "temperature": 0.0, "avg_logprob": -0.09389646959976411, "compression_ratio": 1.7283236994219653, "no_speech_prob": 7.766461749270093e-06}, {"id": 177, "seek": 120700, "start": 1227.0, "end": 1229.0, "text": " For these.", "tokens": [1171, 613, 13], "temperature": 0.0, "avg_logprob": -0.09389646959976411, "compression_ratio": 1.7283236994219653, "no_speech_prob": 7.766461749270093e-06}, {"id": 178, "seek": 120700, "start": 1229.0, "end": 1233.0, "text": " And then we take the log of their ratio.", "tokens": [400, 550, 321, 747, 264, 3565, 295, 641, 8509, 13], "temperature": 0.0, "avg_logprob": -0.09389646959976411, "compression_ratio": 1.7283236994219653, "no_speech_prob": 7.766461749270093e-06}, {"id": 179, "seek": 123300, "start": 1233.0, "end": 1242.0, "text": " And I'll talk about I've got a notebook 3B. Well, we'll talk about we can do this briefly since you've seen naive Bayes before derivation of naive Bayes.", "tokens": [400, 286, 603, 751, 466, 286, 600, 658, 257, 21060, 805, 33, 13, 1042, 11, 321, 603, 751, 466, 321, 393, 360, 341, 10515, 1670, 291, 600, 1612, 29052, 7840, 279, 949, 10151, 399, 295, 29052, 7840, 279, 13], "temperature": 0.0, "avg_logprob": -0.14189790487289428, "compression_ratio": 1.5533980582524272, "no_speech_prob": 8.529904334864113e-06}, {"id": 180, "seek": 123300, "start": 1242.0, "end": 1248.0, "text": " But here I just wanted to use it and kind of see does this does this make sense.", "tokens": [583, 510, 286, 445, 1415, 281, 764, 309, 293, 733, 295, 536, 775, 341, 775, 341, 652, 2020, 13], "temperature": 0.0, "avg_logprob": -0.14189790487289428, "compression_ratio": 1.5533980582524272, "no_speech_prob": 8.529904334864113e-06}, {"id": 181, "seek": 123300, "start": 1248.0, "end": 1255.0, "text": " I looked up the vocabulary most likely associated with positive and negative reviews.", "tokens": [286, 2956, 493, 264, 19864, 881, 3700, 6615, 365, 3353, 293, 3671, 10229, 13], "temperature": 0.0, "avg_logprob": -0.14189790487289428, "compression_ratio": 1.5533980582524272, "no_speech_prob": 8.529904334864113e-06}, {"id": 182, "seek": 125500, "start": 1255.0, "end": 1265.0, "text": " And I used NP-ARC partition, which does not perfectly sort things, but it'll give you the number you request.", "tokens": [400, 286, 1143, 38611, 12, 1899, 34, 24808, 11, 597, 775, 406, 6239, 1333, 721, 11, 457, 309, 603, 976, 291, 264, 1230, 291, 5308, 13], "temperature": 0.0, "avg_logprob": -0.20947837829589844, "compression_ratio": 1.3888888888888888, "no_speech_prob": 6.643188044108683e-06}, {"id": 183, "seek": 125500, "start": 1265.0, "end": 1270.0, "text": " It'll create a partition. So I wanted to get the the top 10.", "tokens": [467, 603, 1884, 257, 24808, 13, 407, 286, 1415, 281, 483, 264, 264, 1192, 1266, 13], "temperature": 0.0, "avg_logprob": -0.20947837829589844, "compression_ratio": 1.3888888888888888, "no_speech_prob": 6.643188044108683e-06}, {"id": 184, "seek": 125500, "start": 1270.0, "end": 1275.0, "text": " The 10 biggest things and the 10 smallest things here.", "tokens": [440, 1266, 3880, 721, 293, 264, 1266, 16998, 721, 510, 13], "temperature": 0.0, "avg_logprob": -0.20947837829589844, "compression_ratio": 1.3888888888888888, "no_speech_prob": 6.643188044108683e-06}, {"id": 185, "seek": 127500, "start": 1275.0, "end": 1291.0, "text": " And so, again, this is from the log ratios are that we've got here. And so these are the words that most kind of most indicate that a review is positive or that a review is negative.", "tokens": [400, 370, 11, 797, 11, 341, 307, 490, 264, 3565, 32435, 366, 300, 321, 600, 658, 510, 13, 400, 370, 613, 366, 264, 2283, 300, 881, 733, 295, 881, 13330, 300, 257, 3131, 307, 3353, 420, 300, 257, 3131, 307, 3671, 13], "temperature": 0.0, "avg_logprob": -0.19354578653971355, "compression_ratio": 1.5633802816901408, "no_speech_prob": 7.571041464871087e-07}, {"id": 186, "seek": 127500, "start": 1291.0, "end": 1297.0, "text": " And so I looked up I looked up by BICO.", "tokens": [400, 370, 286, 2956, 493, 286, 2956, 493, 538, 363, 36568, 13], "temperature": 0.0, "avg_logprob": -0.19354578653971355, "compression_ratio": 1.5633802816901408, "no_speech_prob": 7.571041464871087e-07}, {"id": 187, "seek": 129700, "start": 1297.0, "end": 1305.0, "text": " As being a positive positive word. Let me run this.", "tokens": [1018, 885, 257, 3353, 3353, 1349, 13, 961, 385, 1190, 341, 13], "temperature": 0.0, "avg_logprob": -0.13051399714510206, "compression_ratio": 1.5542168674698795, "no_speech_prob": 1.5205902855086606e-05}, {"id": 188, "seek": 129700, "start": 1305.0, "end": 1308.0, "text": " And the kind of issue I ran into.", "tokens": [400, 264, 733, 295, 2734, 286, 5872, 666, 13], "temperature": 0.0, "avg_logprob": -0.13051399714510206, "compression_ratio": 1.5542168674698795, "no_speech_prob": 1.5205902855086606e-05}, {"id": 189, "seek": 129700, "start": 1308.0, "end": 1317.0, "text": " I think with these, because we're still on the set of just the smaller sample that some of these words only show up in one movie review.", "tokens": [286, 519, 365, 613, 11, 570, 321, 434, 920, 322, 264, 992, 295, 445, 264, 4356, 6889, 300, 512, 295, 613, 2283, 787, 855, 493, 294, 472, 3169, 3131, 13], "temperature": 0.0, "avg_logprob": -0.13051399714510206, "compression_ratio": 1.5542168674698795, "no_speech_prob": 1.5205902855086606e-05}, {"id": 190, "seek": 129700, "start": 1317.0, "end": 1320.0, "text": " And so they show up a lot of times.", "tokens": [400, 370, 436, 855, 493, 257, 688, 295, 1413, 13], "temperature": 0.0, "avg_logprob": -0.13051399714510206, "compression_ratio": 1.5542168674698795, "no_speech_prob": 1.5205902855086606e-05}, {"id": 191, "seek": 132000, "start": 1320.0, "end": 1337.0, "text": " So that kind of colors it. But here I found a review of this is a review of Cry Freedom in which BICO is a historical figure, one of the main characters, very positive review for negative.", "tokens": [407, 300, 733, 295, 4577, 309, 13, 583, 510, 286, 1352, 257, 3131, 295, 341, 307, 257, 3131, 295, 12267, 22208, 294, 597, 363, 36568, 307, 257, 8584, 2573, 11, 472, 295, 264, 2135, 4342, 11, 588, 3353, 3131, 337, 3671, 13], "temperature": 0.0, "avg_logprob": -0.17368358724257527, "compression_ratio": 1.46524064171123, "no_speech_prob": 4.029295723739779e-06}, {"id": 192, "seek": 132000, "start": 1337.0, "end": 1343.0, "text": " I was curious about Soderbergh is a director who's directed both good and bad movies.", "tokens": [286, 390, 6369, 466, 318, 19866, 6873, 71, 307, 257, 5391, 567, 311, 12898, 1293, 665, 293, 1578, 6233, 13], "temperature": 0.0, "avg_logprob": -0.17368358724257527, "compression_ratio": 1.46524064171123, "no_speech_prob": 4.029295723739779e-06}, {"id": 193, "seek": 134300, "start": 1343.0, "end": 1351.0, "text": " And so I looked here at the smaller subsample.", "tokens": [400, 370, 286, 2956, 510, 412, 264, 4356, 2090, 335, 781, 13], "temperature": 0.0, "avg_logprob": -0.35178416859019884, "compression_ratio": 1.3053435114503817, "no_speech_prob": 5.771694304712582e-06}, {"id": 194, "seek": 134300, "start": 1351.0, "end": 1355.0, "text": " Oh, actually, I guess no. Sorry.", "tokens": [876, 11, 767, 11, 286, 2041, 572, 13, 4919, 13], "temperature": 0.0, "avg_logprob": -0.35178416859019884, "compression_ratio": 1.3053435114503817, "no_speech_prob": 5.771694304712582e-06}, {"id": 195, "seek": 134300, "start": 1355.0, "end": 1362.0, "text": " Let me see. Yeah. So I looked here and Soderbergh is only showing up in one of the reviews.", "tokens": [961, 385, 536, 13, 865, 13, 407, 286, 2956, 510, 293, 318, 19866, 6873, 71, 307, 787, 4099, 493, 294, 472, 295, 264, 10229, 13], "temperature": 0.0, "avg_logprob": -0.35178416859019884, "compression_ratio": 1.3053435114503817, "no_speech_prob": 5.771694304712582e-06}, {"id": 196, "seek": 136200, "start": 1362.0, "end": 1374.0, "text": " So I think that's a little bit more informative to do on our full data set. But I did find this person really did not like this particular Soderbergh movie.", "tokens": [407, 286, 519, 300, 311, 257, 707, 857, 544, 27759, 281, 360, 322, 527, 1577, 1412, 992, 13, 583, 286, 630, 915, 341, 954, 534, 630, 406, 411, 341, 1729, 318, 19866, 6873, 71, 3169, 13], "temperature": 0.0, "avg_logprob": -0.19310183697436228, "compression_ratio": 1.6018957345971565, "no_speech_prob": 1.4144245596980909e-06}, {"id": 197, "seek": 136200, "start": 1374.0, "end": 1379.0, "text": " Although I think they even say in the review that he's done others that are better.", "tokens": [5780, 286, 519, 436, 754, 584, 294, 264, 3131, 300, 415, 311, 1096, 2357, 300, 366, 1101, 13], "temperature": 0.0, "avg_logprob": -0.19310183697436228, "compression_ratio": 1.6018957345971565, "no_speech_prob": 1.4144245596980909e-06}, {"id": 198, "seek": 136200, "start": 1379.0, "end": 1386.0, "text": " So just a little bit more data exploration of kind of what is our our log count ratios giving us.", "tokens": [407, 445, 257, 707, 857, 544, 1412, 16197, 295, 733, 295, 437, 307, 527, 527, 3565, 1207, 32435, 2902, 505, 13], "temperature": 0.0, "avg_logprob": -0.19310183697436228, "compression_ratio": 1.6018957345971565, "no_speech_prob": 1.4144245596980909e-06}, {"id": 199, "seek": 138600, "start": 1386.0, "end": 1394.0, "text": " Oh, OK. So if you don't have this in your notebook, that means that these are changes I did not copy.", "tokens": [876, 11, 2264, 13, 407, 498, 291, 500, 380, 362, 341, 294, 428, 21060, 11, 300, 1355, 300, 613, 366, 2962, 286, 630, 406, 5055, 13], "temperature": 0.0, "avg_logprob": -0.10568533624921526, "compression_ratio": 1.5142857142857142, "no_speech_prob": 1.045035332936095e-05}, {"id": 200, "seek": 138600, "start": 1394.0, "end": 1400.0, "text": " As I have two different copies of the repo, one that has the answers and more material than you have.", "tokens": [1018, 286, 362, 732, 819, 14341, 295, 264, 49040, 11, 472, 300, 575, 264, 6338, 293, 544, 2527, 813, 291, 362, 13], "temperature": 0.0, "avg_logprob": -0.10568533624921526, "compression_ratio": 1.5142857142857142, "no_speech_prob": 1.045035332936095e-05}, {"id": 201, "seek": 138600, "start": 1400.0, "end": 1404.0, "text": " I'll put the updated version in after this. Sorry about that.", "tokens": [286, 603, 829, 264, 10588, 3037, 294, 934, 341, 13, 4919, 466, 300, 13], "temperature": 0.0, "avg_logprob": -0.10568533624921526, "compression_ratio": 1.5142857142857142, "no_speech_prob": 1.045035332936095e-05}, {"id": 202, "seek": 140400, "start": 1404.0, "end": 1426.0, "text": " Make a note. Yes. Always let me know. I try to I try to copy over the updated versions for you. But if I forget, let me know.", "tokens": [4387, 257, 3637, 13, 1079, 13, 11270, 718, 385, 458, 13, 286, 853, 281, 286, 853, 281, 5055, 670, 264, 10588, 9606, 337, 291, 13, 583, 498, 286, 2870, 11, 718, 385, 458, 13], "temperature": 0.0, "avg_logprob": -0.09418007821747751, "compression_ratio": 1.4834437086092715, "no_speech_prob": 6.642916105192853e-06}, {"id": 203, "seek": 140400, "start": 1426.0, "end": 1433.0, "text": " Although I think I didn't make too many changes. I think from here on, it's it's it's pretty good.", "tokens": [5780, 286, 519, 286, 994, 380, 652, 886, 867, 2962, 13, 286, 519, 490, 510, 322, 11, 309, 311, 309, 311, 309, 311, 1238, 665, 13], "temperature": 0.0, "avg_logprob": -0.09418007821747751, "compression_ratio": 1.4834437086092715, "no_speech_prob": 6.642916105192853e-06}, {"id": 204, "seek": 143300, "start": 1433.0, "end": 1451.0, "text": " So then getting back to naive, naive bays, you do want to normalize by or not normalize, but define B to be the the log ratio of the average number of positive reviews to negative reviews.", "tokens": [407, 550, 1242, 646, 281, 29052, 11, 29052, 272, 3772, 11, 291, 360, 528, 281, 2710, 1125, 538, 420, 406, 2710, 1125, 11, 457, 6964, 363, 281, 312, 264, 264, 3565, 8509, 295, 264, 4274, 1230, 295, 3353, 10229, 281, 3671, 10229, 13], "temperature": 0.0, "avg_logprob": -0.1571464382234167, "compression_ratio": 1.6369426751592357, "no_speech_prob": 1.3630171451950446e-05}, {"id": 205, "seek": 143300, "start": 1451.0, "end": 1456.0, "text": " Kind of what percentage of the reviews are positive versus negative.", "tokens": [9242, 295, 437, 9668, 295, 264, 10229, 366, 3353, 5717, 3671, 13], "temperature": 0.0, "avg_logprob": -0.1571464382234167, "compression_ratio": 1.6369426751592357, "no_speech_prob": 1.3630171451950446e-05}, {"id": 206, "seek": 145600, "start": 1456.0, "end": 1472.0, "text": " And then we get our predictions by taking the validation term document times are plus B and saying I think when those are greater than zero, it's we're predicting positive, less than zero, predicting negative.", "tokens": [400, 550, 321, 483, 527, 21264, 538, 1940, 264, 24071, 1433, 4166, 1413, 366, 1804, 363, 293, 1566, 286, 519, 562, 729, 366, 5044, 813, 4018, 11, 309, 311, 321, 434, 32884, 3353, 11, 1570, 813, 4018, 11, 32884, 3671, 13], "temperature": 0.0, "avg_logprob": -0.1404644118414985, "compression_ratio": 1.471830985915493, "no_speech_prob": 5.5074469855753705e-06}, {"id": 207, "seek": 147200, "start": 1472.0, "end": 1486.0, "text": " We can check we've got 64 percent accuracy, which I think is reasonably good given that this was on a sample of our data using a pretty simple technique.", "tokens": [492, 393, 1520, 321, 600, 658, 12145, 3043, 14170, 11, 597, 286, 519, 307, 23551, 665, 2212, 300, 341, 390, 322, 257, 6889, 295, 527, 1412, 1228, 257, 1238, 2199, 6532, 13], "temperature": 0.0, "avg_logprob": -0.09800938998951632, "compression_ratio": 1.360759493670886, "no_speech_prob": 1.2411081115715206e-05}, {"id": 208, "seek": 147200, "start": 1486.0, "end": 1494.0, "text": " Questions about that before we apply it to our full data set.", "tokens": [27738, 466, 300, 949, 321, 3079, 309, 281, 527, 1577, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.09800938998951632, "compression_ratio": 1.360759493670886, "no_speech_prob": 1.2411081115715206e-05}, {"id": 209, "seek": 149400, "start": 1494.0, "end": 1503.0, "text": " OK, so going to the full data set and need to download that data and process it.", "tokens": [2264, 11, 370, 516, 281, 264, 1577, 1412, 992, 293, 643, 281, 5484, 300, 1412, 293, 1399, 309, 13], "temperature": 0.0, "avg_logprob": -0.06604658902346433, "compression_ratio": 1.6565217391304348, "no_speech_prob": 7.071642357914243e-06}, {"id": 210, "seek": 149400, "start": 1503.0, "end": 1512.0, "text": " And this is always I would say always start on just a sample of your data set when you're just figuring out what code you want to write, if your calculations work,", "tokens": [400, 341, 307, 1009, 286, 576, 584, 1009, 722, 322, 445, 257, 6889, 295, 428, 1412, 992, 562, 291, 434, 445, 15213, 484, 437, 3089, 291, 528, 281, 2464, 11, 498, 428, 20448, 589, 11], "temperature": 0.0, "avg_logprob": -0.06604658902346433, "compression_ratio": 1.6565217391304348, "no_speech_prob": 7.071642357914243e-06}, {"id": 211, "seek": 149400, "start": 1512.0, "end": 1523.0, "text": " because if things are slower on a large data set, you don't want to be kind of wasting that time while you're still still experimenting.", "tokens": [570, 498, 721, 366, 14009, 322, 257, 2416, 1412, 992, 11, 291, 500, 380, 528, 281, 312, 733, 295, 20457, 300, 565, 1339, 291, 434, 920, 920, 29070, 13], "temperature": 0.0, "avg_logprob": -0.06604658902346433, "compression_ratio": 1.6565217391304348, "no_speech_prob": 7.071642357914243e-06}, {"id": 212, "seek": 152300, "start": 1523.0, "end": 1542.0, "text": " And so now we'll have a set of I think twenty five thousand reviews in our training set and twenty five thousand in our validation set.", "tokens": [400, 370, 586, 321, 603, 362, 257, 992, 295, 286, 519, 7699, 1732, 4714, 10229, 294, 527, 3097, 992, 293, 7699, 1732, 4714, 294, 527, 24071, 992, 13], "temperature": 0.0, "avg_logprob": -0.14014673233032227, "compression_ratio": 1.40625, "no_speech_prob": 4.539047949947417e-05}, {"id": 213, "seek": 154200, "start": 1542.0, "end": 1561.0, "text": " And then here we're doing some processing that takes a little bit longer.", "tokens": [400, 550, 510, 321, 434, 884, 512, 9007, 300, 2516, 257, 707, 857, 2854, 13], "temperature": 0.0, "avg_logprob": -0.16461156543932462, "compression_ratio": 1.028169014084507, "no_speech_prob": 3.726407612703042e-06}, {"id": 214, "seek": 156100, "start": 1561.0, "end": 1579.0, "text": " So it's four or five seconds to get our term document matrices. It's often a good idea to save these so we can reload them later for quicker.", "tokens": [407, 309, 311, 1451, 420, 1732, 3949, 281, 483, 527, 1433, 4166, 32284, 13, 467, 311, 2049, 257, 665, 1558, 281, 3155, 613, 370, 321, 393, 25628, 552, 1780, 337, 16255, 13], "temperature": 0.0, "avg_logprob": -0.07770121097564697, "compression_ratio": 1.2818181818181817, "no_speech_prob": 2.726320417423267e-06}, {"id": 215, "seek": 157900, "start": 1579.0, "end": 1594.0, "text": " Great. Now you've based on the full data set. So now we have thirty eight thousand tokens as opposed to just six thousand. This will give us a much, much larger vocabulary.", "tokens": [3769, 13, 823, 291, 600, 2361, 322, 264, 1577, 1412, 992, 13, 407, 586, 321, 362, 11790, 3180, 4714, 22667, 382, 8851, 281, 445, 2309, 4714, 13, 639, 486, 976, 505, 257, 709, 11, 709, 4833, 19864, 13], "temperature": 0.0, "avg_logprob": -0.1626497459411621, "compression_ratio": 1.348993288590604, "no_speech_prob": 7.41089161238051e-06}, {"id": 216, "seek": 157900, "start": 1594.0, "end": 1603.0, "text": " Go through the same process.", "tokens": [1037, 807, 264, 912, 1399, 13], "temperature": 0.0, "avg_logprob": -0.1626497459411621, "compression_ratio": 1.348993288590604, "no_speech_prob": 7.41089161238051e-06}, {"id": 217, "seek": 160300, "start": 1603.0, "end": 1622.0, "text": " So here I have ended up kind of formalizing what we have done before in looking at when words appear in negative reviews versus positive reviews and just wrote a little helper method that gives you the ratio of for a word.", "tokens": [407, 510, 286, 362, 4590, 493, 733, 295, 9860, 3319, 437, 321, 362, 1096, 949, 294, 1237, 412, 562, 2283, 4204, 294, 3671, 10229, 5717, 3353, 10229, 293, 445, 4114, 257, 707, 36133, 3170, 300, 2709, 291, 264, 8509, 295, 337, 257, 1349, 13], "temperature": 0.0, "avg_logprob": -0.20106029510498047, "compression_ratio": 1.4899328859060403, "no_speech_prob": 5.5940377023944166e-06}, {"id": 218, "seek": 162200, "start": 1622.0, "end": 1634.0, "text": " So we convert it string to end. And then what's the ratio P not to P one. We're here P not is the negative reviews are P not the count of the word appearing in negative reviews.", "tokens": [407, 321, 7620, 309, 6798, 281, 917, 13, 400, 550, 437, 311, 264, 8509, 430, 406, 281, 430, 472, 13, 492, 434, 510, 430, 406, 307, 264, 3671, 10229, 366, 430, 406, 264, 1207, 295, 264, 1349, 19870, 294, 3671, 10229, 13], "temperature": 0.0, "avg_logprob": -0.2348302205403646, "compression_ratio": 1.84375, "no_speech_prob": 6.747810402885079e-06}, {"id": 219, "seek": 162200, "start": 1634.0, "end": 1648.0, "text": " P one is the count of how often it appears in positive reviews. And you can get a sense hated is much more likely to be a negative two times as likely to be a negative reviews.", "tokens": [430, 472, 307, 264, 1207, 295, 577, 2049, 309, 7038, 294, 3353, 10229, 13, 400, 291, 393, 483, 257, 2020, 17398, 307, 709, 544, 3700, 281, 312, 257, 3671, 732, 1413, 382, 3700, 281, 312, 257, 3671, 10229, 13], "temperature": 0.0, "avg_logprob": -0.2348302205403646, "compression_ratio": 1.84375, "no_speech_prob": 6.747810402885079e-06}, {"id": 220, "seek": 164800, "start": 1648.0, "end": 1654.0, "text": " Liked anything less than one is more likely to be in positive reviews.", "tokens": [441, 44070, 1340, 1570, 813, 472, 307, 544, 3700, 281, 312, 294, 3353, 10229, 13], "temperature": 0.0, "avg_logprob": -0.07826760886372, "compression_ratio": 1.7804878048780488, "no_speech_prob": 5.862710168003105e-06}, {"id": 221, "seek": 164800, "start": 1654.0, "end": 1662.0, "text": " Loved is even more disproportionately likely to be in positive reviews than negative and so on. I just thought that was interesting to look at.", "tokens": [6130, 937, 307, 754, 544, 43397, 3700, 281, 312, 294, 3353, 10229, 813, 3671, 293, 370, 322, 13, 286, 445, 1194, 300, 390, 1880, 281, 574, 412, 13], "temperature": 0.0, "avg_logprob": -0.07826760886372, "compression_ratio": 1.7804878048780488, "no_speech_prob": 5.862710168003105e-06}, {"id": 222, "seek": 164800, "start": 1662.0, "end": 1670.0, "text": " Worst is almost ten times as likely to be in a negative review as a positive.", "tokens": [26363, 372, 307, 1920, 2064, 1413, 382, 3700, 281, 312, 294, 257, 3671, 3131, 382, 257, 3353, 13], "temperature": 0.0, "avg_logprob": -0.07826760886372, "compression_ratio": 1.7804878048780488, "no_speech_prob": 5.862710168003105e-06}, {"id": 223, "seek": 167000, "start": 1670.0, "end": 1683.0, "text": " So you can do the same same calculation before of adding one to P one and dividing by the total number of positive reviews plus one.", "tokens": [407, 291, 393, 360, 264, 912, 912, 17108, 949, 295, 5127, 472, 281, 430, 472, 293, 26764, 538, 264, 3217, 1230, 295, 3353, 10229, 1804, 472, 13], "temperature": 0.0, "avg_logprob": -0.19274256680462812, "compression_ratio": 1.3097345132743363, "no_speech_prob": 2.046211557171773e-05}, {"id": 224, "seek": 167000, "start": 1683.0, "end": 1689.0, "text": " Taking the log.", "tokens": [17837, 264, 3565, 13], "temperature": 0.0, "avg_logprob": -0.19274256680462812, "compression_ratio": 1.3097345132743363, "no_speech_prob": 2.046211557171773e-05}, {"id": 225, "seek": 168900, "start": 1689.0, "end": 1710.0, "text": " Here I was just looking at these are so this is kind of similar to this idea of before taking a ratio. We've just added one and taken a log to help it be more numerically stable.", "tokens": [1692, 286, 390, 445, 1237, 412, 613, 366, 370, 341, 307, 733, 295, 2531, 281, 341, 1558, 295, 949, 1940, 257, 8509, 13, 492, 600, 445, 3869, 472, 293, 2726, 257, 3565, 281, 854, 309, 312, 544, 7866, 984, 8351, 13], "temperature": 0.0, "avg_logprob": -0.13752237955729166, "compression_ratio": 1.390625, "no_speech_prob": 7.766771886963397e-06}, {"id": 226, "seek": 171000, "start": 1710.0, "end": 1727.0, "text": " Oops.", "tokens": [21726, 13], "temperature": 0.0, "avg_logprob": -0.610471248626709, "compression_ratio": 0.38461538461538464, "no_speech_prob": 4.710740086011356e-06}, {"id": 227, "seek": 172700, "start": 1727.0, "end": 1741.0, "text": " I've got an accuracy of 80% for the full data set. So this is that looking at a review based on the words predicting if it's positive or negative.", "tokens": [286, 600, 658, 364, 14170, 295, 4688, 4, 337, 264, 1577, 1412, 992, 13, 407, 341, 307, 300, 1237, 412, 257, 3131, 2361, 322, 264, 2283, 32884, 498, 309, 311, 3353, 420, 3671, 13], "temperature": 0.0, "avg_logprob": -0.20972376284391983, "compression_ratio": 1.2366412213740459, "no_speech_prob": 3.0714050808455795e-05}, {"id": 228, "seek": 172700, "start": 1741.0, "end": 1743.0, "text": " Questions.", "tokens": [27738, 13], "temperature": 0.0, "avg_logprob": -0.20972376284391983, "compression_ratio": 1.2366412213740459, "no_speech_prob": 3.0714050808455795e-05}, {"id": 229, "seek": 172700, "start": 1743.0, "end": 1749.0, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.20972376284391983, "compression_ratio": 1.2366412213740459, "no_speech_prob": 3.0714050808455795e-05}, {"id": 230, "seek": 174900, "start": 1749.0, "end": 1758.0, "text": " And we'll talk a little bit. I'll talk about numerical stability in a moment. But yeah to avoid zeros. Well yeah I guess it is just avoiding zeros in that case. Yeah.", "tokens": [400, 321, 603, 751, 257, 707, 857, 13, 286, 603, 751, 466, 29054, 11826, 294, 257, 1623, 13, 583, 1338, 281, 5042, 35193, 13, 1042, 1338, 286, 2041, 309, 307, 445, 20220, 35193, 294, 300, 1389, 13, 865, 13], "temperature": 0.0, "avg_logprob": -0.14987335886274064, "compression_ratio": 1.5100671140939597, "no_speech_prob": 1.2606164091266692e-05}, {"id": 231, "seek": 174900, "start": 1758.0, "end": 1765.0, "text": " And then the log is helping with with numerical stability.", "tokens": [400, 550, 264, 3565, 307, 4315, 365, 365, 29054, 11826, 13], "temperature": 0.0, "avg_logprob": -0.14987335886274064, "compression_ratio": 1.5100671140939597, "no_speech_prob": 1.2606164091266692e-05}, {"id": 232, "seek": 176500, "start": 1765.0, "end": 1783.0, "text": " So another variation we can take on naive Bayes is binarized naive Bayes which is maybe it only matters whether a word is in the review or not not the frequency of the word. So previously we were looking at the counts of how often a word appeared in the review.", "tokens": [407, 1071, 12990, 321, 393, 747, 322, 29052, 7840, 279, 307, 5171, 289, 1602, 29052, 7840, 279, 597, 307, 1310, 309, 787, 7001, 1968, 257, 1349, 307, 294, 264, 3131, 420, 406, 406, 264, 7893, 295, 264, 1349, 13, 407, 8046, 321, 645, 1237, 412, 264, 14893, 295, 577, 2049, 257, 1349, 8516, 294, 264, 3131, 13], "temperature": 0.0, "avg_logprob": -0.12157306514802527, "compression_ratio": 1.6012269938650308, "no_speech_prob": 4.222692041366827e-06}, {"id": 233, "seek": 178300, "start": 1783.0, "end": 1795.0, "text": " Maybe that's not as important. So instead of keeping track of love showed up 10 times in this review. Now it's just going to be a yes or no was loved in the review at all.", "tokens": [2704, 300, 311, 406, 382, 1021, 13, 407, 2602, 295, 5145, 2837, 295, 959, 4712, 493, 1266, 1413, 294, 341, 3131, 13, 823, 309, 311, 445, 516, 281, 312, 257, 2086, 420, 572, 390, 4333, 294, 264, 3131, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.05953696038987902, "compression_ratio": 1.3255813953488371, "no_speech_prob": 2.812926595652243e-06}, {"id": 234, "seek": 179500, "start": 1795.0, "end": 1814.0, "text": " To do that we can just convert our term document matrix using dot sign.", "tokens": [1407, 360, 300, 321, 393, 445, 7620, 527, 1433, 4166, 8141, 1228, 5893, 1465, 13], "temperature": 0.0, "avg_logprob": -0.07268206696761281, "compression_ratio": 1.0142857142857142, "no_speech_prob": 7.646396625204943e-06}, {"id": 235, "seek": 181400, "start": 1814.0, "end": 1836.0, "text": " And we get 82.9% accuracy. So this is even more accurate. Let me show you actually I should show you just a little bit of", "tokens": [400, 321, 483, 29097, 13, 24, 4, 14170, 13, 407, 341, 307, 754, 544, 8559, 13, 961, 385, 855, 291, 767, 286, 820, 855, 291, 445, 257, 707, 857, 295], "temperature": 0.0, "avg_logprob": -0.09373837358811322, "compression_ratio": 1.1415094339622642, "no_speech_prob": 2.684135324670933e-06}, {"id": 236, "seek": 183600, "start": 1836.0, "end": 1850.0, "text": " So this is just a matrix of ones and zeros telling you whether or not that word was present at all in the in the review.", "tokens": [407, 341, 307, 445, 257, 8141, 295, 2306, 293, 35193, 3585, 291, 1968, 420, 406, 300, 1349, 390, 1974, 412, 439, 294, 264, 294, 264, 3131, 13], "temperature": 0.0, "avg_logprob": -0.09292043390728179, "compression_ratio": 1.3032786885245902, "no_speech_prob": 3.1874039905233076e-06}, {"id": 237, "seek": 183600, "start": 1850.0, "end": 1860.0, "text": " Questions about binarized naive Bayes.", "tokens": [27738, 466, 5171, 289, 1602, 29052, 7840, 279, 13], "temperature": 0.0, "avg_logprob": -0.09292043390728179, "compression_ratio": 1.3032786885245902, "no_speech_prob": 3.1874039905233076e-06}, {"id": 238, "seek": 186000, "start": 1860.0, "end": 1870.0, "text": " And this before running it. I was not sure which would be more accurate. Like it's not obvious to me that having a word that the word frequencies don't matter.", "tokens": [400, 341, 949, 2614, 309, 13, 286, 390, 406, 988, 597, 576, 312, 544, 8559, 13, 1743, 309, 311, 406, 6322, 281, 385, 300, 1419, 257, 1349, 300, 264, 1349, 20250, 500, 380, 1871, 13], "temperature": 0.0, "avg_logprob": -0.14344277015099158, "compression_ratio": 1.5326086956521738, "no_speech_prob": 5.422007234301418e-06}, {"id": 239, "seek": 186000, "start": 1870.0, "end": 1888.0, "text": " But it's definitely something worth trying and that yeah in this case was was more successful or led to a higher accuracy.", "tokens": [583, 309, 311, 2138, 746, 3163, 1382, 293, 300, 1338, 294, 341, 1389, 390, 390, 544, 4406, 420, 4684, 281, 257, 2946, 14170, 13], "temperature": 0.0, "avg_logprob": -0.14344277015099158, "compression_ratio": 1.5326086956521738, "no_speech_prob": 5.422007234301418e-06}, {"id": 240, "seek": 188800, "start": 1888.0, "end": 1893.0, "text": " So now we're going to", "tokens": [407, 586, 321, 434, 516, 281], "temperature": 0.0, "avg_logprob": -0.17754272314218375, "compression_ratio": 1.15, "no_speech_prob": 4.133230686420575e-05}, {"id": 241, "seek": 188800, "start": 1893.0, "end": 1904.0, "text": " Use logistic regression and here the features are going to be Rebecca.", "tokens": [8278, 3565, 3142, 24590, 293, 510, 264, 4122, 366, 516, 281, 312, 19381, 13], "temperature": 0.0, "avg_logprob": -0.17754272314218375, "compression_ratio": 1.15, "no_speech_prob": 4.133230686420575e-05}, {"id": 242, "seek": 190400, "start": 1904.0, "end": 1931.0, "text": " C2I converts it converts a class to an integer. So in this case the classes are nag and pause and this is letting us know if that's one or zero.", "tokens": [383, 17, 40, 38874, 309, 38874, 257, 1508, 281, 364, 24922, 13, 407, 294, 341, 1389, 264, 5359, 366, 17096, 293, 10465, 293, 341, 307, 8295, 505, 458, 498, 300, 311, 472, 420, 4018, 13], "temperature": 0.0, "avg_logprob": -0.2078541731223082, "compression_ratio": 1.3211009174311927, "no_speech_prob": 6.10836868872866e-05}, {"id": 243, "seek": 193100, "start": 1931.0, "end": 1942.0, "text": " That's a good question. So the question was whether when to use binarized versus regular naive Bayes.", "tokens": [663, 311, 257, 665, 1168, 13, 407, 264, 1168, 390, 1968, 562, 281, 764, 5171, 289, 1602, 5717, 3890, 29052, 7840, 279, 13], "temperature": 0.0, "avg_logprob": -0.1248795146673498, "compression_ratio": 1.4725274725274726, "no_speech_prob": 4.710594566859072e-06}, {"id": 244, "seek": 193100, "start": 1942.0, "end": 1960.0, "text": " I would say often I feel like it's probably pretty quick to once you've done dot done one to be able to try the other. But yeah, I don't have a roll of thumb on that.", "tokens": [286, 576, 584, 2049, 286, 841, 411, 309, 311, 1391, 1238, 1702, 281, 1564, 291, 600, 1096, 5893, 1096, 472, 281, 312, 1075, 281, 853, 264, 661, 13, 583, 1338, 11, 286, 500, 380, 362, 257, 3373, 295, 9298, 322, 300, 13], "temperature": 0.0, "avg_logprob": -0.1248795146673498, "compression_ratio": 1.4725274725274726, "no_speech_prob": 4.710594566859072e-06}, {"id": 245, "seek": 196000, "start": 1960.0, "end": 1964.0, "text": " Right. So for logistic regression. So", "tokens": [1779, 13, 407, 337, 3565, 3142, 24590, 13, 407], "temperature": 0.0, "avg_logprob": -0.22267544523198554, "compression_ratio": 1.5357142857142858, "no_speech_prob": 5.3068462875671685e-05}, {"id": 246, "seek": 196000, "start": 1964.0, "end": 1970.0, "text": " What's a I should ask him what's a what's a unigram", "tokens": [708, 311, 257, 286, 820, 1029, 796, 437, 311, 257, 437, 311, 257, 517, 33737], "temperature": 0.0, "avg_logprob": -0.22267544523198554, "compression_ratio": 1.5357142857142858, "no_speech_prob": 5.3068462875671685e-05}, {"id": 247, "seek": 196000, "start": 1970.0, "end": 1979.0, "text": " A word. Yeah, so it's just a word because it's a well we'll talk about this more in a moment. An N gram.", "tokens": [316, 1349, 13, 865, 11, 370, 309, 311, 445, 257, 1349, 570, 309, 311, 257, 731, 321, 603, 751, 466, 341, 544, 294, 257, 1623, 13, 1107, 426, 21353, 13], "temperature": 0.0, "avg_logprob": -0.22267544523198554, "compression_ratio": 1.5357142857142858, "no_speech_prob": 5.3068462875671685e-05}, {"id": 248, "seek": 196000, "start": 1979.0, "end": 1986.0, "text": " We're going to be using N grams to refer to a sequence of N words. Here's N is one. So it's a single word.", "tokens": [492, 434, 516, 281, 312, 1228, 426, 11899, 281, 2864, 281, 257, 8310, 295, 426, 2283, 13, 1692, 311, 426, 307, 472, 13, 407, 309, 311, 257, 2167, 1349, 13], "temperature": 0.0, "avg_logprob": -0.22267544523198554, "compression_ratio": 1.5357142857142858, "no_speech_prob": 5.3068462875671685e-05}, {"id": 249, "seek": 198600, "start": 1986.0, "end": 1996.0, "text": " There are people that also N gram can also refer to characters like at a character level in which case it would be a single character. But for our purpose will be will be looking at words.", "tokens": [821, 366, 561, 300, 611, 426, 21353, 393, 611, 2864, 281, 4342, 411, 412, 257, 2517, 1496, 294, 597, 1389, 309, 576, 312, 257, 2167, 2517, 13, 583, 337, 527, 4334, 486, 312, 486, 312, 1237, 412, 2283, 13], "temperature": 0.0, "avg_logprob": -0.14717878144362878, "compression_ratio": 1.5095541401273886, "no_speech_prob": 1.0129742804565467e-05}, {"id": 250, "seek": 198600, "start": 1996.0, "end": 2000.0, "text": " So here we can just fit a logistic regression", "tokens": [407, 510, 321, 393, 445, 3318, 257, 3565, 3142, 24590], "temperature": 0.0, "avg_logprob": -0.14717878144362878, "compression_ratio": 1.5095541401273886, "no_speech_prob": 1.0129742804565467e-05}, {"id": 251, "seek": 198600, "start": 2000.0, "end": 2003.0, "text": " To", "tokens": [1407], "temperature": 0.0, "avg_logprob": -0.14717878144362878, "compression_ratio": 1.5095541401273886, "no_speech_prob": 1.0129742804565467e-05}, {"id": 252, "seek": 200300, "start": 2003.0, "end": 2016.0, "text": " To our term document matrix. We do have to convert the y values to be integers, I guess here since their their classes", "tokens": [1407, 527, 1433, 4166, 8141, 13, 492, 360, 362, 281, 7620, 264, 288, 4190, 281, 312, 41674, 11, 286, 2041, 510, 1670, 641, 641, 5359], "temperature": 0.0, "avg_logprob": -0.17956569277007003, "compression_ratio": 1.2164948453608246, "no_speech_prob": 9.08027413970558e-06}, {"id": 253, "seek": 201600, "start": 2016.0, "end": 2043.0, "text": " Come up with a prediction and we get we get 88% on this this first try. So that's a even better than naive Bayes and the binarized version. So we went from 88.3% to 88.5% by switching to doing the binarized version.", "tokens": [2492, 493, 365, 257, 17630, 293, 321, 483, 321, 483, 24587, 4, 322, 341, 341, 700, 853, 13, 407, 300, 311, 257, 754, 1101, 813, 29052, 7840, 279, 293, 264, 5171, 289, 1602, 3037, 13, 407, 321, 1437, 490, 24587, 13, 18, 4, 281, 24587, 13, 20, 4, 538, 16493, 281, 884, 264, 5171, 289, 1602, 3037, 13], "temperature": 0.0, "avg_logprob": -0.1135988389292071, "compression_ratio": 1.4625850340136055, "no_speech_prob": 6.854118055343861e-06}, {"id": 254, "seek": 204300, "start": 2043.0, "end": 2046.0, "text": " And so now we're going to", "tokens": [400, 370, 586, 321, 434, 516, 281], "temperature": 0.0, "avg_logprob": -0.07779846551283351, "compression_ratio": 1.3783783783783783, "no_speech_prob": 3.6118822208663914e-06}, {"id": 255, "seek": 204300, "start": 2046.0, "end": 2050.0, "text": " Move on and do a or", "tokens": [10475, 322, 293, 360, 257, 420], "temperature": 0.0, "avg_logprob": -0.07779846551283351, "compression_ratio": 1.3783783783783783, "no_speech_prob": 3.6118822208663914e-06}, {"id": 256, "seek": 204300, "start": 2050.0, "end": 2063.0, "text": " Kind of build on this to do something that will hopefully perform even better, which is to do a version of logistic regression with with naive Bayes features.", "tokens": [9242, 295, 1322, 322, 341, 281, 360, 746, 300, 486, 4696, 2042, 754, 1101, 11, 597, 307, 281, 360, 257, 3037, 295, 3565, 3142, 24590, 365, 365, 29052, 7840, 279, 4122, 13], "temperature": 0.0, "avg_logprob": -0.07779846551283351, "compression_ratio": 1.3783783783783783, "no_speech_prob": 3.6118822208663914e-06}, {"id": 257, "seek": 206300, "start": 2063.0, "end": 2074.0, "text": " So for every document we compute the binarized features as described above. And this time we're also going to use by grams and try grams.", "tokens": [407, 337, 633, 4166, 321, 14722, 264, 5171, 289, 1602, 4122, 382, 7619, 3673, 13, 400, 341, 565, 321, 434, 611, 516, 281, 764, 538, 11899, 293, 853, 11899, 13], "temperature": 0.0, "avg_logprob": -0.11106810251871745, "compression_ratio": 1.627659574468085, "no_speech_prob": 2.6015650291810744e-06}, {"id": 258, "seek": 206300, "start": 2074.0, "end": 2089.0, "text": " So here we're referring to sequences of words. So by grams would be the dog said that can't you. So any sequence of two words and kind of by making this into a feature.", "tokens": [407, 510, 321, 434, 13761, 281, 22978, 295, 2283, 13, 407, 538, 11899, 576, 312, 264, 3000, 848, 300, 393, 380, 291, 13, 407, 604, 8310, 295, 732, 2283, 293, 733, 295, 538, 1455, 341, 666, 257, 4111, 13], "temperature": 0.0, "avg_logprob": -0.11106810251871745, "compression_ratio": 1.627659574468085, "no_speech_prob": 2.6015650291810744e-06}, {"id": 259, "seek": 208900, "start": 2089.0, "end": 2094.0, "text": " Like an obvious benefit is having things like not in there.", "tokens": [1743, 364, 6322, 5121, 307, 1419, 721, 411, 406, 294, 456, 13], "temperature": 0.0, "avg_logprob": -0.13201170617883856, "compression_ratio": 1.7777777777777777, "no_speech_prob": 6.339057563309325e-06}, {"id": 260, "seek": 208900, "start": 2094.0, "end": 2096.0, "text": " You know,", "tokens": [509, 458, 11], "temperature": 0.0, "avg_logprob": -0.13201170617883856, "compression_ratio": 1.7777777777777777, "no_speech_prob": 6.339057563309325e-06}, {"id": 261, "seek": 208900, "start": 2096.0, "end": 2107.0, "text": " It said I did not love this movie having not love is, you know, going to be something very different than love when you're only looking at the individual words.", "tokens": [467, 848, 286, 630, 406, 959, 341, 3169, 1419, 406, 959, 307, 11, 291, 458, 11, 516, 281, 312, 746, 588, 819, 813, 959, 562, 291, 434, 787, 1237, 412, 264, 2609, 2283, 13], "temperature": 0.0, "avg_logprob": -0.13201170617883856, "compression_ratio": 1.7777777777777777, "no_speech_prob": 6.339057563309325e-06}, {"id": 262, "seek": 208900, "start": 2107.0, "end": 2118.0, "text": " So looking at the same same movie review text that again we're going to go back to the sample since we're doing something, something new.", "tokens": [407, 1237, 412, 264, 912, 912, 3169, 3131, 2487, 300, 797, 321, 434, 516, 281, 352, 646, 281, 264, 6889, 1670, 321, 434, 884, 746, 11, 746, 777, 13], "temperature": 0.0, "avg_logprob": -0.13201170617883856, "compression_ratio": 1.7777777777777777, "no_speech_prob": 6.339057563309325e-06}, {"id": 263, "seek": 211800, "start": 2118.0, "end": 2135.0, "text": " Actually, I should run these.", "tokens": [5135, 11, 286, 820, 1190, 613, 13], "temperature": 0.0, "avg_logprob": -0.17450894536198797, "compression_ratio": 1.280373831775701, "no_speech_prob": 2.561190513006295e-06}, {"id": 264, "seek": 211800, "start": 2135.0, "end": 2143.0, "text": " And so next we want to do is we want to iterate the through the sequences of words to create our end grams.", "tokens": [400, 370, 958, 321, 528, 281, 360, 307, 321, 528, 281, 44497, 264, 807, 264, 22978, 295, 2283, 281, 1884, 527, 917, 11899, 13], "temperature": 0.0, "avg_logprob": -0.17450894536198797, "compression_ratio": 1.280373831775701, "no_speech_prob": 2.561190513006295e-06}, {"id": 265, "seek": 214300, "start": 2143.0, "end": 2151.0, "text": " So we're going to kind of have to go through each review to get what are all the sequences of two words that show up.", "tokens": [407, 321, 434, 516, 281, 733, 295, 362, 281, 352, 807, 1184, 3131, 281, 483, 437, 366, 439, 264, 22978, 295, 732, 2283, 300, 855, 493, 13], "temperature": 0.0, "avg_logprob": -0.09112727200543438, "compression_ratio": 1.6467391304347827, "no_speech_prob": 2.482408035575645e-06}, {"id": 266, "seek": 214300, "start": 2151.0, "end": 2161.0, "text": " And we'll do this with with nested loops. And this is can be helpful for the homework problem.", "tokens": [400, 321, 603, 360, 341, 365, 365, 15646, 292, 16121, 13, 400, 341, 307, 393, 312, 4961, 337, 264, 14578, 1154, 13], "temperature": 0.0, "avg_logprob": -0.09112727200543438, "compression_ratio": 1.6467391304347827, "no_speech_prob": 2.482408035575645e-06}, {"id": 267, "seek": 214300, "start": 2161.0, "end": 2168.0, "text": " This kind of has more going on than what you'll have to do in the homework.", "tokens": [639, 733, 295, 575, 544, 516, 322, 813, 437, 291, 603, 362, 281, 360, 294, 264, 14578, 13], "temperature": 0.0, "avg_logprob": -0.09112727200543438, "compression_ratio": 1.6467391304347827, "no_speech_prob": 2.482408035575645e-06}, {"id": 268, "seek": 214300, "start": 2168.0, "end": 2171.0, "text": " And we want to", "tokens": [400, 321, 528, 281], "temperature": 0.0, "avg_logprob": -0.09112727200543438, "compression_ratio": 1.6467391304347827, "no_speech_prob": 2.482408035575645e-06}, {"id": 269, "seek": 217100, "start": 2171.0, "end": 2184.0, "text": " Kind of at the end, we're going to create a counter of what the end grams were, how often they appear, what", "tokens": [9242, 295, 412, 264, 917, 11, 321, 434, 516, 281, 1884, 257, 5682, 295, 437, 264, 917, 11899, 645, 11, 577, 2049, 436, 4204, 11, 437], "temperature": 0.0, "avg_logprob": -0.14093631108601887, "compression_ratio": 1.244186046511628, "no_speech_prob": 2.976946962007787e-05}, {"id": 270, "seek": 218400, "start": 2184.0, "end": 2204.0, "text": " And kind of it and in each in each review.", "tokens": [400, 733, 295, 309, 293, 294, 1184, 294, 1184, 3131, 13], "temperature": 0.0, "avg_logprob": -0.13176374435424804, "compression_ratio": 1.1369863013698631, "no_speech_prob": 3.28862847709388e-06}, {"id": 271, "seek": 218400, "start": 2204.0, "end": 2209.0, "text": " And something to note about this is that", "tokens": [400, 746, 281, 3637, 466, 341, 307, 300], "temperature": 0.0, "avg_logprob": -0.13176374435424804, "compression_ratio": 1.1369863013698631, "no_speech_prob": 3.28862847709388e-06}, {"id": 272, "seek": 220900, "start": 2209.0, "end": 2221.0, "text": " This is going to end up being much larger than when we were just looking at individual words, since it can", "tokens": [639, 307, 516, 281, 917, 493, 885, 709, 4833, 813, 562, 321, 645, 445, 1237, 412, 2609, 2283, 11, 1670, 309, 393], "temperature": 0.0, "avg_logprob": -0.08967870932358962, "compression_ratio": 1.2045454545454546, "no_speech_prob": 1.3418971320788842e-05}, {"id": 273, "seek": 222100, "start": 2221.0, "end": 2247.0, "text": " Scale in a bigger way. So it's good. It's good. We went back down to the sample sample data set.", "tokens": [42999, 294, 257, 3801, 636, 13, 407, 309, 311, 665, 13, 467, 311, 665, 13, 492, 1437, 646, 760, 281, 264, 6889, 6889, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.07321059703826904, "compression_ratio": 1.2, "no_speech_prob": 1.221818911290029e-05}, {"id": 274, "seek": 224700, "start": 2247.0, "end": 2255.0, "text": " Actually, maybe this is a good time to take our break. So let's take a five minute break and be back at 1205 and hopefully our", "tokens": [5135, 11, 1310, 341, 307, 257, 665, 565, 281, 747, 527, 1821, 13, 407, 718, 311, 747, 257, 1732, 3456, 1821, 293, 312, 646, 412, 2272, 13328, 293, 4696, 527], "temperature": 0.0, "avg_logprob": -0.20926321877373588, "compression_ratio": 1.4861878453038675, "no_speech_prob": 1.300540770898806e-05}, {"id": 275, "seek": 224700, "start": 2255.0, "end": 2259.0, "text": " Matrix will be done being computed by then.", "tokens": [36274, 486, 312, 1096, 885, 40610, 538, 550, 13], "temperature": 0.0, "avg_logprob": -0.20926321877373588, "compression_ratio": 1.4861878453038675, "no_speech_prob": 1.300540770898806e-05}, {"id": 276, "seek": 224700, "start": 2259.0, "end": 2264.0, "text": " All right, let's let's start back up.", "tokens": [1057, 558, 11, 718, 311, 718, 311, 722, 646, 493, 13], "temperature": 0.0, "avg_logprob": -0.20926321877373588, "compression_ratio": 1.4861878453038675, "no_speech_prob": 1.300540770898806e-05}, {"id": 277, "seek": 224700, "start": 2264.0, "end": 2273.0, "text": " So we just we just finished computing our our train end gram", "tokens": [407, 321, 445, 321, 445, 4335, 15866, 527, 527, 3847, 917, 21353], "temperature": 0.0, "avg_logprob": -0.20926321877373588, "compression_ratio": 1.4861878453038675, "no_speech_prob": 1.300540770898806e-05}, {"id": 278, "seek": 227300, "start": 2273.0, "end": 2282.0, "text": " Our end gram document matrix. So before we were saying term document matrix when our terms were just unigrams.", "tokens": [2621, 917, 21353, 4166, 8141, 13, 407, 949, 321, 645, 1566, 1433, 4166, 8141, 562, 527, 2115, 645, 445, 517, 33737, 82, 13], "temperature": 0.0, "avg_logprob": -0.17856690240284753, "compression_ratio": 1.5855263157894737, "no_speech_prob": 1.5205606359813828e-05}, {"id": 279, "seek": 227300, "start": 2282.0, "end": 2289.0, "text": " Now we've got end grams. And so it's our end gram document matrix for the training set.", "tokens": [823, 321, 600, 658, 917, 11899, 13, 400, 370, 309, 311, 527, 917, 21353, 4166, 8141, 337, 264, 3097, 992, 13], "temperature": 0.0, "avg_logprob": -0.17856690240284753, "compression_ratio": 1.5855263157894737, "no_speech_prob": 1.5205606359813828e-05}, {"id": 280, "seek": 227300, "start": 2289.0, "end": 2294.0, "text": " And here note this has got 260,000 tokens.", "tokens": [400, 510, 3637, 341, 575, 658, 44624, 11, 1360, 22667, 13], "temperature": 0.0, "avg_logprob": -0.17856690240284753, "compression_ratio": 1.5855263157894737, "no_speech_prob": 1.5205606359813828e-05}, {"id": 281, "seek": 229400, "start": 2294.0, "end": 2308.0, "text": " So a lot of a lot of tokens because this can grow conceivably in like n factorial, you know, the fact that you can take all these different combinations.", "tokens": [407, 257, 688, 295, 257, 688, 295, 22667, 570, 341, 393, 1852, 10413, 592, 1188, 294, 411, 297, 36916, 11, 291, 458, 11, 264, 1186, 300, 291, 393, 747, 439, 613, 819, 21267, 13], "temperature": 0.0, "avg_logprob": -0.15153217315673828, "compression_ratio": 1.5454545454545454, "no_speech_prob": 1.1189312090209569e-06}, {"id": 282, "seek": 229400, "start": 2308.0, "end": 2321.0, "text": " So let's take a take a look at what this what this looks like. I think I just randomly looked up 2000", "tokens": [407, 718, 311, 747, 257, 747, 257, 574, 412, 437, 341, 437, 341, 1542, 411, 13, 286, 519, 286, 445, 16979, 2956, 493, 8132], "temperature": 0.0, "avg_logprob": -0.15153217315673828, "compression_ratio": 1.5454545454545454, "no_speech_prob": 1.1189312090209569e-06}, {"id": 283, "seek": 232100, "start": 2321.0, "end": 2326.0, "text": " 2005. Where did that come from?", "tokens": [14394, 13, 2305, 630, 300, 808, 490, 30], "temperature": 0.0, "avg_logprob": -0.17486910386518997, "compression_ratio": 1.4269662921348314, "no_speech_prob": 1.0129709153261501e-05}, {"id": 284, "seek": 232100, "start": 2326.0, "end": 2334.0, "text": " Well, let me look at. OK, so for I to end gram, I was just curious about what's what's element 100,000.", "tokens": [1042, 11, 718, 385, 574, 412, 13, 2264, 11, 370, 337, 286, 281, 917, 21353, 11, 286, 390, 445, 6369, 466, 437, 311, 437, 311, 4478, 2319, 11, 1360, 13], "temperature": 0.0, "avg_logprob": -0.17486910386518997, "compression_ratio": 1.4269662921348314, "no_speech_prob": 1.0129709153261501e-05}, {"id": 285, "seek": 232100, "start": 2334.0, "end": 2340.0, "text": " And it says the end gram looks at looks like this array of two things.", "tokens": [400, 309, 1619, 264, 917, 21353, 1542, 412, 1542, 411, 341, 10225, 295, 732, 721, 13], "temperature": 0.0, "avg_logprob": -0.17486910386518997, "compression_ratio": 1.4269662921348314, "no_speech_prob": 1.0129709153261501e-05}, {"id": 286, "seek": 232100, "start": 2340.0, "end": 2344.0, "text": " So this must be a by gram with two words in it.", "tokens": [407, 341, 1633, 312, 257, 538, 21353, 365, 732, 2283, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.17486910386518997, "compression_ratio": 1.4269662921348314, "no_speech_prob": 1.0129709153261501e-05}, {"id": 287, "seek": 234400, "start": 2344.0, "end": 2353.0, "text": " 189 and 1301. And if I look up what words those correspond to, I get nothing changes.", "tokens": [2443, 24, 293, 19966, 16, 13, 400, 498, 286, 574, 493, 437, 2283, 729, 6805, 281, 11, 286, 483, 1825, 2962, 13], "temperature": 0.0, "avg_logprob": -0.0939028122845818, "compression_ratio": 1.4529411764705882, "no_speech_prob": 3.2377104162151227e-06}, {"id": 288, "seek": 234400, "start": 2353.0, "end": 2358.0, "text": " So it's conceivable that these two words were were next to each other.", "tokens": [407, 309, 311, 10413, 34376, 300, 613, 732, 2283, 645, 645, 958, 281, 1184, 661, 13], "temperature": 0.0, "avg_logprob": -0.0939028122845818, "compression_ratio": 1.4529411764705882, "no_speech_prob": 3.2377104162151227e-06}, {"id": 289, "seek": 234400, "start": 2358.0, "end": 2367.0, "text": " So this is one of our by grams. And here this is what what corresponds to element 100,000.", "tokens": [407, 341, 307, 472, 295, 527, 538, 11899, 13, 400, 510, 341, 307, 437, 437, 23249, 281, 4478, 2319, 11, 1360, 13], "temperature": 0.0, "avg_logprob": -0.0939028122845818, "compression_ratio": 1.4529411764705882, "no_speech_prob": 3.2377104162151227e-06}, {"id": 290, "seek": 236700, "start": 2367.0, "end": 2381.0, "text": " I look up another one, 100,000 10. This is 6348, which is has and so that's another another by gram.", "tokens": [286, 574, 493, 1071, 472, 11, 2319, 11, 1360, 1266, 13, 639, 307, 1386, 12249, 23, 11, 597, 307, 575, 293, 370, 300, 311, 1071, 1071, 538, 21353, 13], "temperature": 0.0, "avg_logprob": -0.18593375965700312, "compression_ratio": 1.3555555555555556, "no_speech_prob": 1.9033552689506905e-06}, {"id": 291, "seek": 236700, "start": 2381.0, "end": 2394.0, "text": " Here I find a trigram with index 6116 and it's doesn't even is the is the trigram.", "tokens": [1692, 286, 915, 257, 504, 33737, 365, 8186, 1386, 5348, 21, 293, 309, 311, 1177, 380, 754, 307, 264, 307, 264, 504, 33737, 13], "temperature": 0.0, "avg_logprob": -0.18593375965700312, "compression_ratio": 1.3555555555555556, "no_speech_prob": 1.9033552689506905e-06}, {"id": 292, "seek": 239400, "start": 2394.0, "end": 2400.0, "text": " There are questions about this is just to give you a sense of kind of like what is this matrix that we've found.", "tokens": [821, 366, 1651, 466, 341, 307, 445, 281, 976, 291, 257, 2020, 295, 733, 295, 411, 437, 307, 341, 8141, 300, 321, 600, 1352, 13], "temperature": 0.0, "avg_logprob": -0.14754797220230104, "compression_ratio": 1.5783783783783785, "no_speech_prob": 1.012972370517673e-05}, {"id": 293, "seek": 239400, "start": 2400.0, "end": 2405.0, "text": " So this matrix has go back up 800 reviews in it.", "tokens": [407, 341, 8141, 575, 352, 646, 493, 13083, 10229, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.14754797220230104, "compression_ratio": 1.5783783783783785, "no_speech_prob": 1.012972370517673e-05}, {"id": 294, "seek": 239400, "start": 2405.0, "end": 2415.0, "text": " And then it's got 260,000 and grams and it's a combination of some of these are unigrams, some are by grams and some are trigrams.", "tokens": [400, 550, 309, 311, 658, 44624, 11, 1360, 293, 11899, 293, 309, 311, 257, 6562, 295, 512, 295, 613, 366, 517, 33737, 82, 11, 512, 366, 538, 11899, 293, 512, 366, 504, 33737, 82, 13], "temperature": 0.0, "avg_logprob": -0.14754797220230104, "compression_ratio": 1.5783783783783785, "no_speech_prob": 1.012972370517673e-05}, {"id": 295, "seek": 241500, "start": 2415.0, "end": 2427.0, "text": " And they're recorded as these arrays of one, two or three things, but they correspond to sets of two pulls of words with one, two or three words in them.", "tokens": [400, 436, 434, 8287, 382, 613, 41011, 295, 472, 11, 732, 420, 1045, 721, 11, 457, 436, 6805, 281, 6352, 295, 732, 16982, 295, 2283, 365, 472, 11, 732, 420, 1045, 2283, 294, 552, 13], "temperature": 0.0, "avg_logprob": -0.16334476778584142, "compression_ratio": 1.5620915032679739, "no_speech_prob": 8.714288242117618e-07}, {"id": 296, "seek": 241500, "start": 2427.0, "end": 2435.0, "text": " They can be, you know, by gram like nothing changes or try trigram like doesn't even.", "tokens": [814, 393, 312, 11, 291, 458, 11, 538, 21353, 411, 1825, 2962, 420, 853, 504, 33737, 411, 1177, 380, 754, 13], "temperature": 0.0, "avg_logprob": -0.16334476778584142, "compression_ratio": 1.5620915032679739, "no_speech_prob": 8.714288242117618e-07}, {"id": 297, "seek": 243500, "start": 2435.0, "end": 2448.0, "text": " Another trigram I found was look her usual business and political. So just to kind of get a sense of what what words appear together.", "tokens": [3996, 504, 33737, 286, 1352, 390, 574, 720, 7713, 1606, 293, 3905, 13, 407, 445, 281, 733, 295, 483, 257, 2020, 295, 437, 437, 2283, 4204, 1214, 13], "temperature": 0.0, "avg_logprob": -0.1424671695345924, "compression_ratio": 1.3282442748091603, "no_speech_prob": 1.2804848665837198e-05}, {"id": 298, "seek": 243500, "start": 2448.0, "end": 2460.0, "text": " There are questions about this. Rebecca.", "tokens": [821, 366, 1651, 466, 341, 13, 19381, 13], "temperature": 0.0, "avg_logprob": -0.1424671695345924, "compression_ratio": 1.3282442748091603, "no_speech_prob": 1.2804848665837198e-05}, {"id": 299, "seek": 246000, "start": 2460.0, "end": 2469.0, "text": " So here for tokenization, yes, I'll say most tokenizers will split up contractions into separate separate words.", "tokens": [407, 510, 337, 14862, 2144, 11, 2086, 11, 286, 603, 584, 881, 14862, 22525, 486, 7472, 493, 4364, 626, 666, 4994, 4994, 2283, 13], "temperature": 0.0, "avg_logprob": -0.16501513771388843, "compression_ratio": 1.6023391812865497, "no_speech_prob": 4.2643881897674873e-05}, {"id": 300, "seek": 246000, "start": 2469.0, "end": 2472.0, "text": " The other way they sometimes get split is and we'll see this later.", "tokens": [440, 661, 636, 436, 2171, 483, 7472, 307, 293, 321, 603, 536, 341, 1780, 13], "temperature": 0.0, "avg_logprob": -0.16501513771388843, "compression_ratio": 1.6023391812865497, "no_speech_prob": 4.2643881897674873e-05}, {"id": 301, "seek": 246000, "start": 2472.0, "end": 2480.0, "text": " Sometimes you just take the T off and have that as like apostrophe T as its own its own word.", "tokens": [4803, 291, 445, 747, 264, 314, 766, 293, 362, 300, 382, 411, 19484, 27194, 314, 382, 1080, 1065, 1080, 1065, 1349, 13], "temperature": 0.0, "avg_logprob": -0.16501513771388843, "compression_ratio": 1.6023391812865497, "no_speech_prob": 4.2643881897674873e-05}, {"id": 302, "seek": 248000, "start": 2480.0, "end": 2490.0, "text": " But yeah, that's a standard way to tokenize and.", "tokens": [583, 1338, 11, 300, 311, 257, 3832, 636, 281, 14862, 1125, 293, 13], "temperature": 0.0, "avg_logprob": -0.22551744778951008, "compression_ratio": 1.416058394160584, "no_speech_prob": 1.2410967428877484e-05}, {"id": 303, "seek": 248000, "start": 2490.0, "end": 2492.0, "text": " Oh, here I guess we're using the fast AI tokenizer.", "tokens": [876, 11, 510, 286, 2041, 321, 434, 1228, 264, 2370, 7318, 14862, 6545, 13], "temperature": 0.0, "avg_logprob": -0.22551744778951008, "compression_ratio": 1.416058394160584, "no_speech_prob": 1.2410967428877484e-05}, {"id": 304, "seek": 248000, "start": 2492.0, "end": 2501.0, "text": " But in the in the red Jax lesson, we'll look at how we could write our own our own tokenizer.", "tokens": [583, 294, 264, 294, 264, 2182, 508, 2797, 6898, 11, 321, 603, 574, 412, 577, 321, 727, 2464, 527, 1065, 527, 1065, 14862, 6545, 13], "temperature": 0.0, "avg_logprob": -0.22551744778951008, "compression_ratio": 1.416058394160584, "no_speech_prob": 1.2410967428877484e-05}, {"id": 305, "seek": 250100, "start": 2501.0, "end": 2511.0, "text": " So if you have any questions.", "tokens": [407, 498, 291, 362, 604, 1651, 13], "temperature": 0.0, "avg_logprob": -0.27544829845428465, "compression_ratio": 1.1964285714285714, "no_speech_prob": 3.0894520932633895e-06}, {"id": 306, "seek": 250100, "start": 2511.0, "end": 2516.0, "text": " OK, so I did that with the training set.", "tokens": [2264, 11, 370, 286, 630, 300, 365, 264, 3097, 992, 13], "temperature": 0.0, "avg_logprob": -0.27544829845428465, "compression_ratio": 1.1964285714285714, "no_speech_prob": 3.0894520932633895e-06}, {"id": 307, "seek": 250100, "start": 2516.0, "end": 2523.0, "text": " Now this is the same code as above to make a validation matrix.", "tokens": [823, 341, 307, 264, 912, 3089, 382, 3673, 281, 652, 257, 24071, 8141, 13], "temperature": 0.0, "avg_logprob": -0.27544829845428465, "compression_ratio": 1.1964285714285714, "no_speech_prob": 3.0894520932633895e-06}, {"id": 308, "seek": 252300, "start": 2523.0, "end": 2531.0, "text": " To also be a little bit slow. Really, I probably should have opened my my saved versions of these.", "tokens": [1407, 611, 312, 257, 707, 857, 2964, 13, 4083, 11, 286, 1391, 820, 362, 5625, 452, 452, 6624, 9606, 295, 613, 13], "temperature": 0.0, "avg_logprob": -0.10908370751600999, "compression_ratio": 1.1136363636363635, "no_speech_prob": 8.61316584632732e-05}, {"id": 309, "seek": 253100, "start": 2531.0, "end": 2557.0, "text": " But since these are slower to calculate, you definitely would want to save them and then just load them in the future.", "tokens": [583, 1670, 613, 366, 14009, 281, 8873, 11, 291, 2138, 576, 528, 281, 3155, 552, 293, 550, 445, 3677, 552, 294, 264, 2027, 13], "temperature": 0.0, "avg_logprob": -0.09615898983819145, "compression_ratio": 1.2291666666666667, "no_speech_prob": 1.4063263733987696e-05}, {"id": 310, "seek": 255700, "start": 2557.0, "end": 2563.0, "text": " So wait. That validation set has 200 reviews in it.", "tokens": [407, 1699, 13, 663, 24071, 992, 575, 2331, 10229, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.23825426662669463, "compression_ratio": 1.1505376344086022, "no_speech_prob": 2.5865123461699113e-05}, {"id": 311, "seek": 255700, "start": 2563.0, "end": 2571.0, "text": " Same 260,000 token or 260,000 n grams that we're using.", "tokens": [10635, 44624, 11, 1360, 14862, 420, 44624, 11, 1360, 297, 11899, 300, 321, 434, 1228, 13], "temperature": 0.0, "avg_logprob": -0.23825426662669463, "compression_ratio": 1.1505376344086022, "no_speech_prob": 2.5865123461699113e-05}, {"id": 312, "seek": 257100, "start": 2571.0, "end": 2598.0, "text": " And now we can do naive based on this in a similar way as to before.", "tokens": [400, 586, 321, 393, 360, 29052, 2361, 322, 341, 294, 257, 2531, 636, 382, 281, 949, 13], "temperature": 0.0, "avg_logprob": -0.10468889418102446, "compression_ratio": 1.0303030303030303, "no_speech_prob": 1.4969767107686494e-05}, {"id": 313, "seek": 259800, "start": 2598.0, "end": 2606.0, "text": " Here I'm just showing that the predictions look like this array of trues and falses.", "tokens": [1692, 286, 478, 445, 4099, 300, 264, 21264, 574, 411, 341, 10225, 295, 504, 1247, 293, 16720, 279, 13], "temperature": 0.0, "avg_logprob": -0.07267837226390839, "compression_ratio": 1.4702380952380953, "no_speech_prob": 1.7230904632015154e-05}, {"id": 314, "seek": 259800, "start": 2606.0, "end": 2611.0, "text": " And now our validation accuracy is 76 percent.", "tokens": [400, 586, 527, 24071, 14170, 307, 24733, 3043, 13], "temperature": 0.0, "avg_logprob": -0.07267837226390839, "compression_ratio": 1.4702380952380953, "no_speech_prob": 1.7230904632015154e-05}, {"id": 315, "seek": 259800, "start": 2611.0, "end": 2614.0, "text": " So this was a little bit or not just a little bit.", "tokens": [407, 341, 390, 257, 707, 857, 420, 406, 445, 257, 707, 857, 13], "temperature": 0.0, "avg_logprob": -0.07267837226390839, "compression_ratio": 1.4702380952380953, "no_speech_prob": 1.7230904632015154e-05}, {"id": 316, "seek": 259800, "start": 2614.0, "end": 2622.0, "text": " This was worse than what we had gotten with logistic regression.", "tokens": [639, 390, 5324, 813, 437, 321, 632, 5768, 365, 3565, 3142, 24590, 13], "temperature": 0.0, "avg_logprob": -0.07267837226390839, "compression_ratio": 1.4702380952380953, "no_speech_prob": 1.7230904632015154e-05}, {"id": 317, "seek": 262200, "start": 2622.0, "end": 2630.0, "text": " And ultimately what we're going to end up doing is using trigram features in a logistic regression to improve the performance.", "tokens": [400, 6284, 437, 321, 434, 516, 281, 917, 493, 884, 307, 1228, 504, 33737, 4122, 294, 257, 3565, 3142, 24590, 281, 3470, 264, 3389, 13], "temperature": 0.0, "avg_logprob": -0.07314335841398972, "compression_ratio": 1.4413793103448276, "no_speech_prob": 1.341933966614306e-05}, {"id": 318, "seek": 262200, "start": 2630.0, "end": 2645.0, "text": " But this is kind of a way to get acquainted with with having trigrams and bigrams.", "tokens": [583, 341, 307, 733, 295, 257, 636, 281, 483, 50224, 365, 365, 1419, 504, 33737, 82, 293, 955, 2356, 82, 13], "temperature": 0.0, "avg_logprob": -0.07314335841398972, "compression_ratio": 1.4413793103448276, "no_speech_prob": 1.341933966614306e-05}, {"id": 319, "seek": 264500, "start": 2645.0, "end": 2657.0, "text": " Do the binarized version of this.", "tokens": [1144, 264, 5171, 289, 1602, 3037, 295, 341, 13], "temperature": 0.0, "avg_logprob": -0.17589207795950082, "compression_ratio": 0.825, "no_speech_prob": 2.3550877813249826e-05}, {"id": 320, "seek": 265700, "start": 2657.0, "end": 2676.0, "text": " And again, the binarized version is a little bit. No, in this case, the binarized version is worse than the regular version.", "tokens": [400, 797, 11, 264, 5171, 289, 1602, 3037, 307, 257, 707, 857, 13, 883, 11, 294, 341, 1389, 11, 264, 5171, 289, 1602, 3037, 307, 5324, 813, 264, 3890, 3037, 13], "temperature": 0.0, "avg_logprob": -0.06679445675441197, "compression_ratio": 1.4090909090909092, "no_speech_prob": 7.888853360782377e-06}, {"id": 321, "seek": 267600, "start": 2676.0, "end": 2690.0, "text": " Questions on using naive base with trigrams.", "tokens": [27738, 322, 1228, 29052, 3096, 365, 504, 33737, 82, 13], "temperature": 0.0, "avg_logprob": -0.12800390315505694, "compression_ratio": 1.3857142857142857, "no_speech_prob": 3.785209628404118e-06}, {"id": 322, "seek": 267600, "start": 2690.0, "end": 2697.0, "text": " All right. So now we're going to do logistic regression where the features are the trigrams.", "tokens": [1057, 558, 13, 407, 586, 321, 434, 516, 281, 360, 3565, 3142, 24590, 689, 264, 4122, 366, 264, 504, 33737, 82, 13], "temperature": 0.0, "avg_logprob": -0.12800390315505694, "compression_ratio": 1.3857142857142857, "no_speech_prob": 3.785209628404118e-06}, {"id": 323, "seek": 267600, "start": 2697.0, "end": 2705.0, "text": " So, again, this will be having these sequences of words.", "tokens": [407, 11, 797, 11, 341, 486, 312, 1419, 613, 22978, 295, 2283, 13], "temperature": 0.0, "avg_logprob": -0.12800390315505694, "compression_ratio": 1.3857142857142857, "no_speech_prob": 3.785209628404118e-06}, {"id": 324, "seek": 270500, "start": 2705.0, "end": 2715.0, "text": " Here we're going to use scikit learns count vectorizer to to create our trigrams and we give it or create our n grams.", "tokens": [1692, 321, 434, 516, 281, 764, 2180, 22681, 27152, 1207, 8062, 6545, 281, 281, 1884, 527, 504, 33737, 82, 293, 321, 976, 309, 420, 1884, 527, 297, 11899, 13], "temperature": 0.0, "avg_logprob": -0.1397937636777579, "compression_ratio": 1.5792349726775956, "no_speech_prob": 1.952515412995126e-05}, {"id": 325, "seek": 270500, "start": 2715.0, "end": 2717.0, "text": " We give it a range of this.", "tokens": [492, 976, 309, 257, 3613, 295, 341, 13], "temperature": 0.0, "avg_logprob": -0.1397937636777579, "compression_ratio": 1.5792349726775956, "no_speech_prob": 1.952515412995126e-05}, {"id": 326, "seek": 270500, "start": 2717.0, "end": 2728.0, "text": " This is similar to what's asked for in the homework of saying, OK, I want you to give me the unigrams, bigrams and trigrams back all together.", "tokens": [639, 307, 2531, 281, 437, 311, 2351, 337, 294, 264, 14578, 295, 1566, 11, 2264, 11, 286, 528, 291, 281, 976, 385, 264, 517, 33737, 82, 11, 955, 2356, 82, 293, 504, 33737, 82, 646, 439, 1214, 13], "temperature": 0.0, "avg_logprob": -0.1397937636777579, "compression_ratio": 1.5792349726775956, "no_speech_prob": 1.952515412995126e-05}, {"id": 327, "seek": 272800, "start": 2728.0, "end": 2747.0, "text": " In this range, notice it has you set a number of max features, which makes sense because these can get very large, particularly if you had a even wider range of what n grams you were looking at.", "tokens": [682, 341, 3613, 11, 3449, 309, 575, 291, 992, 257, 1230, 295, 11469, 4122, 11, 597, 1669, 2020, 570, 613, 393, 483, 588, 2416, 11, 4098, 498, 291, 632, 257, 754, 11842, 3613, 295, 437, 297, 11899, 291, 645, 1237, 412, 13], "temperature": 0.0, "avg_logprob": -0.14037539482116698, "compression_ratio": 1.3724137931034484, "no_speech_prob": 1.0129741895070765e-05}, {"id": 328, "seek": 272800, "start": 2747.0, "end": 2750.0, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.14037539482116698, "compression_ratio": 1.3724137931034484, "no_speech_prob": 1.0129741895070765e-05}, {"id": 329, "seek": 275000, "start": 2750.0, "end": 2764.0, "text": " I believe this is inclusive. Yeah. And we could check the.", "tokens": [286, 1697, 341, 307, 13429, 13, 865, 13, 400, 321, 727, 1520, 264, 13], "temperature": 0.0, "avg_logprob": -0.17108665528844613, "compression_ratio": 1.4794520547945205, "no_speech_prob": 1.5689083738834597e-05}, {"id": 330, "seek": 275000, "start": 2764.0, "end": 2770.0, "text": " The documentation. Yeah, you'd have to look at the scikit learn docs, but I believe this is inclusive.", "tokens": [440, 14333, 13, 865, 11, 291, 1116, 362, 281, 574, 412, 264, 2180, 22681, 1466, 45623, 11, 457, 286, 1697, 341, 307, 13429, 13], "temperature": 0.0, "avg_logprob": -0.17108665528844613, "compression_ratio": 1.4794520547945205, "no_speech_prob": 1.5689083738834597e-05}, {"id": 331, "seek": 275000, "start": 2770.0, "end": 2773.0, "text": " And the one in the homework asked for. Oh, here it is.", "tokens": [400, 264, 472, 294, 264, 14578, 2351, 337, 13, 876, 11, 510, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.17108665528844613, "compression_ratio": 1.4794520547945205, "no_speech_prob": 1.5689083738834597e-05}, {"id": 332, "seek": 277300, "start": 2773.0, "end": 2783.0, "text": " Yes. It's inclusive. All values of n such that min n less than or equal to n less than or equal to max n will be used.", "tokens": [1079, 13, 467, 311, 13429, 13, 1057, 4190, 295, 297, 1270, 300, 923, 297, 1570, 813, 420, 2681, 281, 297, 1570, 813, 420, 2681, 281, 11469, 297, 486, 312, 1143, 13], "temperature": 0.0, "avg_logprob": -0.1440514292035784, "compression_ratio": 1.3111111111111111, "no_speech_prob": 1.7230377125088125e-05}, {"id": 333, "seek": 278300, "start": 2783.0, "end": 2807.0, "text": " And this is how you can just hit shift tab or shift tab tab to get documentation in a Jupyter Notebook.", "tokens": [400, 341, 307, 577, 291, 393, 445, 2045, 5513, 4421, 420, 5513, 4421, 4421, 281, 483, 14333, 294, 257, 22125, 88, 391, 11633, 2939, 13], "temperature": 0.0, "avg_logprob": -0.17525593987826643, "compression_ratio": 1.1839080459770115, "no_speech_prob": 1.6441732441307977e-05}, {"id": 334, "seek": 280700, "start": 2807.0, "end": 2819.0, "text": " Oh, and here I think I was just looking at the vocabulary of the vectorizer.", "tokens": [876, 11, 293, 510, 286, 519, 286, 390, 445, 1237, 412, 264, 19864, 295, 264, 8062, 6545, 13], "temperature": 0.0, "avg_logprob": -0.12144622615739412, "compression_ratio": 1.3333333333333333, "no_speech_prob": 1.6187124856514856e-05}, {"id": 335, "seek": 280700, "start": 2819.0, "end": 2826.0, "text": " You can see here are a bunch of unigrams are still in there. Now we're in a section of bigrams.", "tokens": [509, 393, 536, 510, 366, 257, 3840, 295, 517, 33737, 82, 366, 920, 294, 456, 13, 823, 321, 434, 294, 257, 3541, 295, 955, 2356, 82, 13], "temperature": 0.0, "avg_logprob": -0.12144622615739412, "compression_ratio": 1.3333333333333333, "no_speech_prob": 1.6187124856514856e-05}, {"id": 336, "seek": 282600, "start": 2826.0, "end": 2845.0, "text": " You can also this can see how they add them typically kind of by going through a sentence and it's also here, you know, like rolling a window over looking at each combination of two words, lovable self, self in in this, this comma, which normally normally makes makes me.", "tokens": [509, 393, 611, 341, 393, 536, 577, 436, 909, 552, 5850, 733, 295, 538, 516, 807, 257, 8174, 293, 309, 311, 611, 510, 11, 291, 458, 11, 411, 9439, 257, 4910, 670, 1237, 412, 1184, 6562, 295, 732, 2283, 11, 450, 17915, 2698, 11, 2698, 294, 294, 341, 11, 341, 22117, 11, 597, 5646, 5646, 1669, 1669, 385, 13], "temperature": 0.0, "avg_logprob": -0.16052112882099454, "compression_ratio": 1.5485714285714285, "no_speech_prob": 1.4509826542052906e-05}, {"id": 337, "seek": 284500, "start": 2845.0, "end": 2858.0, "text": " And the reason you know you're kind of taking what's the bigram, which normally normally is in kind of like the second spot. So then you slide over. You've got normally in the first spot normally make.", "tokens": [400, 264, 1778, 291, 458, 291, 434, 733, 295, 1940, 437, 311, 264, 955, 2356, 11, 597, 5646, 5646, 307, 294, 733, 295, 411, 264, 1150, 4008, 13, 407, 550, 291, 4137, 670, 13, 509, 600, 658, 5646, 294, 264, 700, 4008, 5646, 652, 13], "temperature": 0.0, "avg_logprob": -0.10918320549858941, "compression_ratio": 1.7401960784313726, "no_speech_prob": 2.2958738554734737e-06}, {"id": 338, "seek": 284500, "start": 2858.0, "end": 2870.0, "text": " So this sentence would read lovable self in this, which normally makes me forgive her shallow acting would be the sentence that this must have come from.", "tokens": [407, 341, 8174, 576, 1401, 450, 17915, 2698, 294, 341, 11, 597, 5646, 1669, 385, 10718, 720, 20488, 6577, 576, 312, 264, 8174, 300, 341, 1633, 362, 808, 490, 13], "temperature": 0.0, "avg_logprob": -0.10918320549858941, "compression_ratio": 1.7401960784313726, "no_speech_prob": 2.2958738554734737e-06}, {"id": 339, "seek": 287000, "start": 2870.0, "end": 2875.0, "text": " We go further. Let me see if we get to. Yeah. And then here we get to some trigrams in here too.", "tokens": [492, 352, 3052, 13, 961, 385, 536, 498, 321, 483, 281, 13, 865, 13, 400, 550, 510, 321, 483, 281, 512, 504, 33737, 82, 294, 510, 886, 13], "temperature": 0.0, "avg_logprob": -0.2284921556711197, "compression_ratio": 1.1851851851851851, "no_speech_prob": 4.831777550862171e-05}, {"id": 340, "seek": 287500, "start": 2875.0, "end": 2901.0, "text": " Hard to believe she was the producer on this dog. So you can kind of see how the trigrams were constructed from the sentence questions about that.", "tokens": [11817, 281, 1697, 750, 390, 264, 12314, 322, 341, 3000, 13, 407, 291, 393, 733, 295, 536, 577, 264, 504, 33737, 82, 645, 17083, 490, 264, 8174, 1651, 466, 300, 13], "temperature": 0.0, "avg_logprob": -0.08413630894252232, "compression_ratio": 1.2920353982300885, "no_speech_prob": 2.1110743546159938e-05}, {"id": 341, "seek": 290100, "start": 2901.0, "end": 2924.0, "text": " Right. Binarized naive Bayes using n grams from count vectorizer.", "tokens": [1779, 13, 363, 6470, 1602, 29052, 7840, 279, 1228, 297, 11899, 490, 1207, 8062, 6545, 13], "temperature": 0.0, "avg_logprob": -0.4278865814208984, "compression_ratio": 0.9285714285714286, "no_speech_prob": 6.401300197467208e-05}, {"id": 342, "seek": 292400, "start": 2924.0, "end": 2939.0, "text": " To run this.", "tokens": [1407, 1190, 341, 13], "temperature": 0.0, "avg_logprob": -0.17730584740638733, "compression_ratio": 0.6, "no_speech_prob": 1.834157228586264e-05}, {"id": 343, "seek": 293900, "start": 2939.0, "end": 2960.0, "text": " So there I was getting 83% with the Binarized version 78% without.", "tokens": [407, 456, 286, 390, 1242, 30997, 4, 365, 264, 363, 6470, 1602, 3037, 26369, 4, 1553, 13], "temperature": 0.0, "avg_logprob": -0.23986507597423734, "compression_ratio": 0.9850746268656716, "no_speech_prob": 1.1658929906843696e-05}, {"id": 344, "seek": 296000, "start": 2960.0, "end": 2964.0, "text": " Let me.", "tokens": [961, 385, 13], "temperature": 0.0, "avg_logprob": -0.5039449419294085, "compression_ratio": 0.4666666666666667, "no_speech_prob": 1.593584966030903e-05}, {"id": 345, "seek": 296400, "start": 2964.0, "end": 2991.0, "text": " Down to this part.", "tokens": [9506, 281, 341, 644, 13], "temperature": 0.0, "avg_logprob": -0.2088538408279419, "compression_ratio": 0.6923076923076923, "no_speech_prob": 1.5445781173184514e-05}, {"id": 346, "seek": 299100, "start": 2991.0, "end": 3001.0, "text": " Questions.", "tokens": [27738, 13], "temperature": 0.0, "avg_logprob": -0.43562257289886475, "compression_ratio": 0.5555555555555556, "no_speech_prob": 6.10643983236514e-05}, {"id": 347, "seek": 300100, "start": 3001.0, "end": 3021.0, "text": " Something. Oh, go ahead.", "tokens": [6595, 13, 876, 11, 352, 2286, 13], "temperature": 0.0, "avg_logprob": -0.2593291456049139, "compression_ratio": 0.75, "no_speech_prob": 4.565658855426591e-06}, {"id": 348, "seek": 302100, "start": 3021.0, "end": 3031.0, "text": " Yeah, I mean, I, I would guess you probably rarely want. Yeah.", "tokens": [865, 11, 286, 914, 11, 286, 11, 286, 576, 2041, 291, 1391, 13752, 528, 13, 865, 13], "temperature": 0.0, "avg_logprob": -0.2952788897923061, "compression_ratio": 0.96875, "no_speech_prob": 7.1828394538897555e-06}, {"id": 349, "seek": 303100, "start": 3031.0, "end": 3051.0, "text": " Something I mentioned in the intro that I haven't had a chance to look at yet is sentence piece, which is this library that does sub word units, which is kind of the similar idea. I think of looking at like character and grams, but it's kind of pieces of words.", "tokens": [6595, 286, 2835, 294, 264, 12897, 300, 286, 2378, 380, 632, 257, 2931, 281, 574, 412, 1939, 307, 8174, 2522, 11, 597, 307, 341, 6405, 300, 775, 1422, 1349, 6815, 11, 597, 307, 733, 295, 264, 2531, 1558, 13, 286, 519, 295, 1237, 412, 411, 2517, 293, 11899, 11, 457, 309, 311, 733, 295, 3755, 295, 2283, 13], "temperature": 0.0, "avg_logprob": -0.10461084702435662, "compression_ratio": 1.64, "no_speech_prob": 1.593554588907864e-05}, {"id": 350, "seek": 303100, "start": 3051.0, "end": 3059.0, "text": " I know people that have used that kind of in conjunction with deep learning and gotten good, good outcomes.", "tokens": [286, 458, 561, 300, 362, 1143, 300, 733, 295, 294, 27482, 365, 2452, 2539, 293, 5768, 665, 11, 665, 10070, 13], "temperature": 0.0, "avg_logprob": -0.10461084702435662, "compression_ratio": 1.64, "no_speech_prob": 1.593554588907864e-05}, {"id": 351, "seek": 305900, "start": 3059.0, "end": 3068.0, "text": " But yeah, I don't, I don't know of cases where people have taken like six grams or yeah, because the other thing is as it gets longer.", "tokens": [583, 1338, 11, 286, 500, 380, 11, 286, 500, 380, 458, 295, 3331, 689, 561, 362, 2726, 411, 2309, 11899, 420, 1338, 11, 570, 264, 661, 551, 307, 382, 309, 2170, 2854, 13], "temperature": 0.0, "avg_logprob": -0.09437778668525891, "compression_ratio": 1.5904255319148937, "no_speech_prob": 1.3844492968928535e-05}, {"id": 352, "seek": 305900, "start": 3068.0, "end": 3081.0, "text": " I think they're going to get more and more distinct, you know, they're probably like a lot of seven grams that only show up once or something in the, in the corpus.", "tokens": [286, 519, 436, 434, 516, 281, 483, 544, 293, 544, 10644, 11, 291, 458, 11, 436, 434, 1391, 411, 257, 688, 295, 3407, 11899, 300, 787, 855, 493, 1564, 420, 746, 294, 264, 11, 294, 264, 1181, 31624, 13], "temperature": 0.0, "avg_logprob": -0.09437778668525891, "compression_ratio": 1.5904255319148937, "no_speech_prob": 1.3844492968928535e-05}, {"id": 353, "seek": 308100, "start": 3081.0, "end": 3090.0, "text": " Welcome.", "tokens": [50364, 4027, 13, 50814], "temperature": 0.0, "avg_logprob": -0.6158297538757325, "compression_ratio": 0.5, "no_speech_prob": 0.0005946545861661434}], "language": "en"}