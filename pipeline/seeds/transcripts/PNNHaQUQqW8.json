{"text": " So we're going to be starting, and I, I renumbered the notebooks yesterday. This is now notebook five, but revisiting, going to be revisiting this task of sentiment classification of movie reviews only using neural networks now. And we'll be using transfer learning, so we'll be taking what I think initially seems to be like a more roundabout approach to kind of begin by creating a language model and then using that for classification. And this is a, we'll kind of start with WikiText 103, which is a different, different data set. But later we'll fine tune it for our movie review data set. This is a really exciting area to be looking at. Transfer learning has been widely used with great success in the area of computer vision for several years. But it's much, much more recently that it's being successfully applied to NLP. And this kind of began with ULMFIT last year, and was also then built upon by BERT and GPT-2. There is an article I really liked by Sebastian Ruter in The Gradient last, last summer saying that NLP's ImageNet moment has arrived. And this kind of has two meanings. I mean, one, it's the successful application of transfer learning to NLP. Also, ImageNet just led to such huge advances in the field of computer vision and that that's kind of now happening in the area of NLP. And actually to check, have you covered transfer learning in your other classes? Yeah? Okay. And so we'll talk more about this, but yeah, that it's kind of letting you train a model on a different and often larger data set and then, oh, some people are shaking their heads no. So, and I'll have a, we'll have a few slides on this next time, but it's the general idea is that you are training a model on a large data set that's probably different from your data set, which is often smaller, but you're able to kind of take advantage of having this model that was already trained on this big data set. And you can fine tune it for your smaller data set and that's, and get state of the art results, which is really, really exciting. So we'll start with building a language model. And remember, we'll later come back to this goal of deciding if a movie review is positive or negative. I wanted to highlight Janelle Shane, who blogs and tweets about, she does a lot of very creative work with AI. But she, and she often uses text. She's done posts trying to train a neural network to tell jokes. So this came up with, what do you call a cat? Does it take to screw in a light bulb? They could worry the banana. What did the new aunt say after a dog? It was a pirate. These don't actually make sense, but they're funny. And I like a lot of her, a lot of her work is very humorous in a kind of absurd way. Since she was having trouble with her neural net learning jokes, she tried retraining it with what do you get when you cross a blank with a blank? What do you get when you cross a pirate in a little butter, a bathroom? What do you get when you cross a dinosaur? They get a lawyer. So I see that many of you like these jokes from the neural net. But I think this is an example of kind of some of the whimsical and creative things you can do with language. Why was six afraid of seven? Because he doesn't have a birthday. Yeah, and I mean it's neat because the setups do, they do kind of fit with what we would expect from a joke. Although it's still, and this is even something where you can see how they're on the right track that eventually maybe these will be more human-like jokes. Yeah, so it didn't do that well with the most common jokes. Which one was that? So I like that one. She's also done using a neural network to create recipes. This is another language task, so small sandwiches using a dish, chili, lemons, salads, and seafood with shortening, snow peas, peach halves. I don't know what this remaining posting. Two large bones of sliced chicken salmon. It's also something that if you don't read it too closely, it seems like it could be a recipe, but then if you read it more closely and think about it, some of these don't make sense. But it's got three tablespoons coarsely chopped green onions. But I encourage you to check out her blog and her Twitter because she posts a lot of really fun, fun applications. She's also done D&D character bios and names of heavy metal bands and new tomato breeds. So there's a lot on her website. And this can also give you ideas if there are things you want to try with a language model. So you will need to use the Fast AI library for this lesson. Hopefully most of you got it installed when we did Notebook 3. But if not, definitely please let me know. And in Lesson 3, I post, or I guess Notebook 3 be the link to troubleshooting your Fast AI installation. We will need a GPO to train your neural net on this part. And for many of you that are using Macs, you'll need to use a GPU in the cloud since you don't have an NVIDIA GPU, which is what the major deep learning libraries need. And let me just check, have you guys used GPUs in previous classes? Okay, so I see a mix of yeses and maybes. This, let me show you, so we on the Fast AI set up, list a number of options. Here are some ready to run one-click sites. CoLab is nice in that it's free as a product from Google. A number of these give you, you know, a certain amount of free credit when you first start. Definitely, yeah, let me know if you get stuck on this. The important thing to know with any option that involves payment, which is pretty much, I think, everything other than Google CoLab, although Google CoLab has kind of time limits of when it'll shut you down, is to always shut down your instance when you're done. Otherwise, you could end up with an expensive bill. And so you definitely don't want to start something, and then two months from now you've forgotten all about the class and get a bill that it's been running the whole time. Jeremy? Google Cloud has $300 of free credit. Oh, okay, Google Cloud has $300 of free credit. So what do you have to do to get those? Oh, okay, and you don't have to do anything to get those. So yeah, definitely, I guess, be on the lookout, because I think many service providers are kind of offering these discounts to get started. But this is, yes, something that's necessary to have a GPU in order to really kind of be taking advantage of the speed you'll need to train some of these, and this will be even more important in the next two notebooks as well. And actually, just to check, have you all used PyTorch previously? Noting, yes, okay, cool. So let me... So language models can use a lot of GPU, so you may need to decrease the batch size here. I noticed the significance of GPUs is, so these are graphics processing units. They're what's used in video games, and the kind of video and computer game industry really pushed the technology on them over the past few decades, and it's the same type of calculation. So they are really great at kind of performing these matrix calculations, which is exactly what you need to do when you're training a neural net. And so it's really great that we can kind of take advantage of this technology that's made neural nets so much faster to train. And this is part of what's made neural networks feasible now in a way that they weren't, say, in the 90s when people were looking at them, is being able to use GPUs. So this is the same data set as before of IMDB movie samples. And once again, we're going to start on a sample of our data set to try to get things set up. And so we've seen this before, but what's going on behind the scenes is tokenization, so the text are having to be processed to split the raw sentences into words or more exactly tokens. Numericalization, which is this process of mapping the tokens to IDs and keeping track of that. Here we're going to have a maximum vocabulary size of 60,000. By default, we're using these same special tokens that we used previously, xxunk is unknown, xxbeginning of string, end of string, xxmage for capitalization, and so on. And we can see same type of reviews we've been looking at before. If you haven't seen Zombie Bloodbath, you haven't a contest like Make Your Own Horror Movie in One Day. And again, what the model is going to be working with is these list of numbers that represent the reviews. And then this is just showing, and this is actually what we used previously, an alternate way to load our data set with a data block API, which here is kind of very human readable. We're taking something from a CSV, we're going to split the data frame, this is the label we're going to use, and so on. So what we're going to do, so we're not going to start with trying to train from scratch to classify reviews. We're going to use a model that was pre-trained. And so here we're going to start with a model that was pre-trained on Wikitext 103, which is a clean subset of Wikipedia. So it is not all of Wikipedia, this is something that Stephen Marity put together, he was previously at Metamind, which was acquired by Salesforce. And so the Wikitext language modeling data set is a collection of over 100 million tokens extracted from the set of verified, good, and featured articles on Wikipedia. Compared to Penn Treebank, which is another famous data set in NLP, it's over two times larger, oh sorry, Wikitext 2 is over two times larger, Wikitext 103 is over 110 times larger. Also, Penn Treebank had removed punctuation and numbers, which Wikitext keeps. And this is in keeping with kind of some of the trends we talked about back in lesson one of how we're wanting to throw away less and less information in pre-processing with neural nets, because we have a complex model that can handle the complexity of our data. We don't need to do as much cleaning as was done kind of in previous decades with NLP. And so we're going to, this is just saying it's shuffling the order of the reviews kind of each time as we put them into the model so that it's not just, we don't want the model kind of learning the pattern of how the reviews are, since that's not significant. Here we can look at what is, so a batch is kind of the group of reviews that you're going to evaluate at a time. And typically this is on a GPU, a neural network is taking in a batch of inputs, so that's multiple reviews at one time. Here we can look at, see what a batch looks like. In this case we've got five reviews. We can see their index and the tokenized version of them. Bizarre condition that characters had to deal like rats in a labyrinth. And see that this is kind of what we expect of, given that we know we're going to be looking at these movie reviews. So that was kind of this, we're processing the data, our movie reviews, kind of, that's one thing we're doing. The other thing we're going to do is getting this pre-trained model on WikiText 103 data. And so we'll create this language model learner. And a question that always comes up is what if the vocabulary is different? And in fact it will be, right? There are probably words in the Wikipedia data set that are not in our movie reviews and vice versa. There might be new words in our movie reviews that weren't in this Wikipedia data set. And so I just wanted to take a look at that. So we've got our Wikipedia index to string mapping, which is going to be separate from our, for our movie reviews, which is the data LM vocab is separate. And so I'm going to compare these. Yeah, so here I see that WikiI2String has 60,000 words in it. Here the vocab.I2String, that's the vocabulary for our movie reviews, only has 49,000. And I can go through these and get the set of Wiki words, the set of IMDB words, should be running these. And then I can take the set difference to find the words that are on Wikipedia, in the Wikipedia data set and not IMDB and the IMDB data set, not Wikipedia. And I'm doing this to kind of just as an exploratory analysis, just to see like what, what are these things that we're looking at and working with? And so then I wanted to get a list of some of these words. And really, I think what we're more interested in is what's, what's an IMDB, but not on our Wikipedia data set. And I found here, I'm just looking at a subset, a few of them. For instance, well, modernization is in Wiki words, but not, not IMDB. 30-something is in IMDB and not Wikipedia. And just a key thing to note is how does the model deal with this, that we have started with our model that was pre-trained on Wikipedia and now we're applying it to IMDB, which has some new words. And we're just going to initialize the words that weren't, weren't present when we, when we pre-trained with Wikipedia, initialize them all to the same random thing, and then we'll be kind of updating all of these, these word embeddings to, as we train it on our IMDB data set. And so here, here, so the, the encoder, that's kind of just the first layer. And did you, you covered word embeddings twice in the program? Okay, so here we've got these word embeddings and we can see what they look like, and I'm just showing that 30-something and link later both were not in the Wikipedia data set, and so they've been initialized to the same, the same random thing. So what, what we'll do first is generating some fake, fake movie reviews using our WikiText 103 model. And I should, well no, I'll talk about that later. So here, what you can do to generate text with a language model is give it some text you want to start with, and then it'll keep predicting what's next to generate some sentences. And so here I was, actually I was trying to answer the question of what color is the sky, and so I tried the color of the sky is the blue mark, Arabic white, the color is used by the Greek Orthodox Church, the Roman Catholic Church, and Daily Express. And so you can see it is talking about colors, it's not, say if you were using this as a chat bot, it's not going to be very great. We also, we haven't trained very long yet, and we'll try this again as we train the model longer. Here I started kind of trying to make up a movie review. I hated this movie and the film The Royal Stranger. The film is set in the 1950s when World War II broke. I hated this movie, but after it was a critical success, he made a few unsuccessful attempts to re-film the film. He died in 1985, aged 94. Something that's interesting is that I feel like you can definitely hear the influence of Wikipedia in this movie review that I've made up. I feel like parts of that to me kind of sound like the language you would get in Wikipedia, and we'll see as we train it longer, it'll become more movie review like. Something to note, actually I should ask, is there a question about what I'm doing right here? We'll go back more into the details, kind of keeping with my teaching philosophy. We're starting with kind of how do we use this, and then we'll get more into kind of like what's actually happening underneath. Here the idea is kind of how do you use a language model? I want to highlight there is this parameter temperature that lets you determine how much randomness you want to inject, and so you can lower the temperature to make the text less randomized. Here I have the temperature higher, so they're a little bit more random. This one, I hated this movie, but the film was released in October 2005. The film was released on DVD and Blu-ray on November 1st, 2006. I hated this movie, but the British Film Institute has called it the most important film in the history of British cinema. I think these are kind of fun to see what you can generate. This is a more predictable one, and you'll notice here I run it again, and this is pretty similar to the previous one, so you're going to get less variation in your output with a lower temperature. Yes? Can I throw you the box? I have a couple of questions. You mentioned that the words are not in Wikipedia, but in IMDb are going to be mapped to random vectors, and they are going to be different for each word, can you imagine? I think initially they're all initialized with the same random vector, but then they get updated as you train the model. So it's in one word? I mean, initially they're all the same, but you have a separate one for each word that gets updated to something different. The second question is how temperature works. I understood that it's something that works with the sub-magnet, that the T that you divide by makes the weights, I mean the logits more similar, but how does that relate, or yeah, how does it work? So we can look at the docs for predicting on the language model. Let me see if this gives more information. So that's kind of it. Yeah, I mean I think some of this is kind of adjusting the probabilities of what comes next. So kind of if you, you can end up creating a language model that's very deterministic, where given something it's always going to predict the same thing after it, which you don't want. You kind of want to have different words you're choosing from with different probabilities, and this is I think kind of giving more probability towards other options. Yeah, I was just wondering this, that's for prediction, even if they are more squeezed together like the maximum one, what are you going to end up predicting? Like, should be the same, but I'm wrong, but I don't know like where, or is it, but it's fine, I'll just like go. Okay, and I'll think about, I'll think about another way to explain this in more detail next time. And then do you want to pass it to Omar? So you have mentioned that dropping the temperature makes the answers less variant, so if, let's say there's a review that starts with I hate this movie in the EMDB dataset. If we make the temperature really small, would this command return the exact review on the EMDB dataset? That's, I mean, so something like I hate this movie, there's probably many movies that say this, but let me see, let's try, oh, you can't do zero. So I don't know if this would happen with this model, and this is something you can play around with, they're definitely like Markov models I've seen that end up just returning the exact same text again and again and again. I'm not sure if you would get that here, but you, like you will see, like with this one having a low temperature, this is very similar, like what we're getting. It's saying I hated this movie, but the film was a commercial success, it was released on DVD. There's kind of, yeah, very similar reviews, let me run this again and see if we get another, again, I hated this movie, but the film was released in October 2006. I think you could end up, oh, this is a little bit different. Yeah, I don't know that you can completely get it to be deterministic, but it's getting kind of closer to, like they're not that many options. All right, and so again, this was, we haven't even really trained the model here. We basically just, let's go back up here. Yeah, we basically just created it. This was in AWD LSTM, which I'll talk about more later, but let's try training it and see how that impacts what we get. But I kind of just wanted you to have a comparison to see where it was starting from, and then we'll see where we end up. So kind of key parameter to choose when training a neural net is the learning rate, and that's kind of the multiplier when you're making updates. You can think of that as the step size of how much you want to update, and with learning rate, you don't want it to be too big because you can kind of totally overshoot where you're going. Also, you don't want it to be too small because your model may take forever to train. And so kind of one of the tricky things is choosing your learning rate, and FastAI has a learning rate finder, which basically just tests out a variety of learning rates, and you want to choose, and then it plots the loss. You want to choose something where the loss is still decreasing, so you kind of want the loss to be low but still decreasing at the point you choose it. Here, choose 1e-2. Maybe even do like 3e-1. Let's go with 1e-2. So choosing this point where it's low and decreasing, what would you choose here, Jeremy? Thumbs up for 1e-2. So this first step was just even deciding how do we want to set our learning rate so that we can start training our model. And again, the reviews we were getting above, this was from this pre-trained model on Wikipedia, and I was kind of looking at, okay, this was trained on Wikipedia, what kind of movie reviews will it produce? Oh, okay, this is going to be slow. Okay, let me load the one I've saved. Thank you, Jeremy. And interrupt the kernel. Okay, so I stopped the kernel because that was going to be slow to load. Oh, no, okay. Should I skip ahead and load, I guess my fine-tuned? What? Okay, so I should not have run this cell. The idea is this is going to be slow to train. As you train it, you're going to periodically want to stop and save your weights so you don't have to start over each time. And I was planning to load in my saved weights, but I just overrode them accidentally for the first round, but fortunately I have some later ones. And kind of a key thing with transfer learning is that typically when you first start fine-tuning it on your data set, you just want to train the last few layers and keep the earlier layers frozen. And that means you're not updating the weights for the early layers. And then you will unfreeze those and update all the weights. And this is kind of something that helps it learn faster. So here I'm going to load in this already. Oh, no. Okay, I have somehow changed the size of my model. I will fix this before next time. Let me for here, I'll just show you kind of what I got last time I ran this, but I will run this on my own time since it's going to be slow, slow to run. So here I checked. So before I was looking at the encoder weights for the embeddings of 30-something and link later, which you'll remember both show up in the IMDB vocab, but not in the Wikitext 103 vocab. And initially they were the same because they had been initialized to the same random thing. Now that I have trained the model some, they're no longer the same. And so that's why this evaluates to false. This was just a way to illustrate that. There are also, I had stored the random thing that every, all the new vocab was initialized to in new word vector. And 30-something is no longer close to that because we've updated it as we've learned more about how this word is used in our vocabulary. So now actually, even though it's slow, I will start this running just in case, but I'll show you the movie reviews. So now I'm looking at the language model again and I'm starting it with the sentence, I liked this movie because, again, I liked this movie because I didn't know how to match the filmic quality of the movie. This movie is just like the original, quite as bad as Channel 4, but it reminds me of a famous, I liked this movie because it was a horror movie I had already seen. Here's another one, this movie was is how I started. And you can see here that's the text being fed in, this movie was a great one because it was a very Swedish one, cute. The storyline is beautiful, the characters are colorful, and the film is great. So I think on the whole, like these are still, they're not great English. I think they do sound probably more like movie reviews and less like Wikipedia articles than before, you know, before the structure I think was particularly Wikipedia-like. We're still going to train this for longer. So now I've decreased the learning rate even further, and that's something you typically will do, kind of as you're training, often you'll want to decrease the learning rate once you're kind of, now you've gotten more in the vicinity of good answers. Oh, these I've, let me try loading fine-tuned. Okay, let me try loading that one and see if it's the same size as my current size. No, it's a different size. Okay. Well, we'll come back to this next time and I'll get these retrained, and you can also run these on your own, and again, allow time since some of these can take, you know, for instance, like the initial training I'm trying to do is taking 10 minutes, and so I didn't want to wait 10 minutes to see it here, but you can do this. I do want to note some of the risk of language models while we're talking about language models, and we'll come back to this more later on in the course when we get to ethics and bias, but recently, so who here heard about OpenAIs, GPT-2? I see a few hands, although not everybody, so this was a language model that produces much, much better text than what we've seen with these generated movie reviews, and maybe I'll even share a clip next time, and so the researchers did not release all the details of the model because they felt like this is so compelling. It could be a risk. In an article about it, Jeremy was quoted, this was the Verge's coverage, yeah, OpenAIs, and this was, I should say, a controversial decision about should they have released it or not, but Jeremy said, I've been trying to warn people about this for a while. We have the technology to totally fill Twitter, email, and the web up with reasonable sounding context-appropriate prose, which would drown out all other speech and be impossible to filter, and for just a small example, someone created a website, I think it's Robot Flow, and it's like Stack Overflow, except the answers are all created by a language model, and this is a fun site that's not malicious, but then somebody else started cross-posting some of those answers to Stack Overflow, and these are answers that sound reasonable, but they're incorrect, they don't actually make sense, and so then there was a discussion on Stack Overflow of how should they deal with this, because this is something that could potentially be very confusing for people to have reasonable sounding, wrong, computer-generated answers, and Hamil Hussein, who is a BASTI student and a machine learning engineer at GitHub, highlighted this. This is an example of how easy it is to overwhelm public discourse with generative models and highlights the importance of thinking about defenses, solutions. The language model was trained by one person using Fast.ai, but not for this purpose. This was, and actually I can go to... Yeah, Fast.ai student project, and I can pull up ask Roboflow, the AI that answers programming questions, which I think as an application is fine, but yeah, then you've got the what does this mean when somebody starts posting these to Stack Overflow and they're wrong and misleading. Did you have more you want to say about this, Jeremy? Hamil actually used this approach to something really handy, which is that GitHub, he created something which uses the same technology to find useful snippets of GitHub code based on your own natural language queries, which is pretty cool. Yeah, and this was GitHub's semantic code search. And so that's something to keep in mind with language models, that they have this kind of dual, they can be fun and even useful, but also have risk. Actually, I think I will stop here just a little bit early today since I am not able to load my saved models, but we'll come back and we'll cover this kind of in more detail next time and to move on to the classifier.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 5.36, "text": " So we're going to be starting, and I, I renumbered the notebooks yesterday.", "tokens": [407, 321, 434, 516, 281, 312, 2891, 11, 293, 286, 11, 286, 8124, 4182, 292, 264, 43782, 5186, 13], "temperature": 0.0, "avg_logprob": -0.2771368327893709, "compression_ratio": 1.6311475409836065, "no_speech_prob": 0.0009397433022968471}, {"id": 1, "seek": 0, "start": 5.36, "end": 11.84, "text": " This is now notebook five, but revisiting, going to be revisiting this task", "tokens": [639, 307, 586, 21060, 1732, 11, 457, 20767, 1748, 11, 516, 281, 312, 20767, 1748, 341, 5633], "temperature": 0.0, "avg_logprob": -0.2771368327893709, "compression_ratio": 1.6311475409836065, "no_speech_prob": 0.0009397433022968471}, {"id": 2, "seek": 0, "start": 11.84, "end": 16.44, "text": " of sentiment classification of movie reviews only using neural networks now.", "tokens": [295, 16149, 21538, 295, 3169, 10229, 787, 1228, 18161, 9590, 586, 13], "temperature": 0.0, "avg_logprob": -0.2771368327893709, "compression_ratio": 1.6311475409836065, "no_speech_prob": 0.0009397433022968471}, {"id": 3, "seek": 0, "start": 16.44, "end": 21.92, "text": " And we'll be using transfer learning, so we'll be taking what I think initially seems", "tokens": [400, 321, 603, 312, 1228, 5003, 2539, 11, 370, 321, 603, 312, 1940, 437, 286, 519, 9105, 2544], "temperature": 0.0, "avg_logprob": -0.2771368327893709, "compression_ratio": 1.6311475409836065, "no_speech_prob": 0.0009397433022968471}, {"id": 4, "seek": 0, "start": 21.92, "end": 29.52, "text": " to be like a more roundabout approach to kind of begin by creating a language model", "tokens": [281, 312, 411, 257, 544, 3098, 21970, 3109, 281, 733, 295, 1841, 538, 4084, 257, 2856, 2316], "temperature": 0.0, "avg_logprob": -0.2771368327893709, "compression_ratio": 1.6311475409836065, "no_speech_prob": 0.0009397433022968471}, {"id": 5, "seek": 2952, "start": 29.52, "end": 31.84, "text": " and then using that for classification.", "tokens": [293, 550, 1228, 300, 337, 21538, 13], "temperature": 0.0, "avg_logprob": -0.19536930805928, "compression_ratio": 1.6325757575757576, "no_speech_prob": 5.5609969422221184e-05}, {"id": 6, "seek": 2952, "start": 31.84, "end": 35.24, "text": " And this is a, we'll kind of start with WikiText 103,", "tokens": [400, 341, 307, 257, 11, 321, 603, 733, 295, 722, 365, 35892, 50198, 48784, 11], "temperature": 0.0, "avg_logprob": -0.19536930805928, "compression_ratio": 1.6325757575757576, "no_speech_prob": 5.5609969422221184e-05}, {"id": 7, "seek": 2952, "start": 35.24, "end": 38.0, "text": " which is a different, different data set.", "tokens": [597, 307, 257, 819, 11, 819, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.19536930805928, "compression_ratio": 1.6325757575757576, "no_speech_prob": 5.5609969422221184e-05}, {"id": 8, "seek": 2952, "start": 38.0, "end": 41.96, "text": " But later we'll fine tune it for our movie review data set.", "tokens": [583, 1780, 321, 603, 2489, 10864, 309, 337, 527, 3169, 3131, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.19536930805928, "compression_ratio": 1.6325757575757576, "no_speech_prob": 5.5609969422221184e-05}, {"id": 9, "seek": 2952, "start": 41.96, "end": 46.56, "text": " This is a really exciting area to be looking at.", "tokens": [639, 307, 257, 534, 4670, 1859, 281, 312, 1237, 412, 13], "temperature": 0.0, "avg_logprob": -0.19536930805928, "compression_ratio": 1.6325757575757576, "no_speech_prob": 5.5609969422221184e-05}, {"id": 10, "seek": 2952, "start": 46.56, "end": 51.04, "text": " Transfer learning has been widely used with great success in the area", "tokens": [35025, 2539, 575, 668, 13371, 1143, 365, 869, 2245, 294, 264, 1859], "temperature": 0.0, "avg_logprob": -0.19536930805928, "compression_ratio": 1.6325757575757576, "no_speech_prob": 5.5609969422221184e-05}, {"id": 11, "seek": 2952, "start": 51.04, "end": 53.4, "text": " of computer vision for several years.", "tokens": [295, 3820, 5201, 337, 2940, 924, 13], "temperature": 0.0, "avg_logprob": -0.19536930805928, "compression_ratio": 1.6325757575757576, "no_speech_prob": 5.5609969422221184e-05}, {"id": 12, "seek": 2952, "start": 53.4, "end": 59.239999999999995, "text": " But it's much, much more recently that it's being successfully applied to NLP.", "tokens": [583, 309, 311, 709, 11, 709, 544, 3938, 300, 309, 311, 885, 10727, 6456, 281, 426, 45196, 13], "temperature": 0.0, "avg_logprob": -0.19536930805928, "compression_ratio": 1.6325757575757576, "no_speech_prob": 5.5609969422221184e-05}, {"id": 13, "seek": 5924, "start": 59.24, "end": 63.64, "text": " And this kind of began with ULMFIT last year,", "tokens": [400, 341, 733, 295, 4283, 365, 624, 43, 44, 37, 3927, 1036, 1064, 11], "temperature": 0.0, "avg_logprob": -0.2249883729584363, "compression_ratio": 1.4401709401709402, "no_speech_prob": 3.119285247521475e-05}, {"id": 14, "seek": 5924, "start": 63.64, "end": 68.68, "text": " and was also then built upon by BERT and GPT-2.", "tokens": [293, 390, 611, 550, 3094, 3564, 538, 363, 31479, 293, 26039, 51, 12, 17, 13], "temperature": 0.0, "avg_logprob": -0.2249883729584363, "compression_ratio": 1.4401709401709402, "no_speech_prob": 3.119285247521475e-05}, {"id": 15, "seek": 5924, "start": 68.68, "end": 74.92, "text": " There is an article I really liked by Sebastian Ruter in The Gradient last,", "tokens": [821, 307, 364, 7222, 286, 534, 4501, 538, 31102, 497, 20314, 294, 440, 16710, 1196, 1036, 11], "temperature": 0.0, "avg_logprob": -0.2249883729584363, "compression_ratio": 1.4401709401709402, "no_speech_prob": 3.119285247521475e-05}, {"id": 16, "seek": 5924, "start": 74.92, "end": 79.96000000000001, "text": " last summer saying that NLP's ImageNet moment has arrived.", "tokens": [1036, 4266, 1566, 300, 426, 45196, 311, 29903, 31890, 1623, 575, 6678, 13], "temperature": 0.0, "avg_logprob": -0.2249883729584363, "compression_ratio": 1.4401709401709402, "no_speech_prob": 3.119285247521475e-05}, {"id": 17, "seek": 5924, "start": 79.96000000000001, "end": 82.6, "text": " And this kind of has two meanings.", "tokens": [400, 341, 733, 295, 575, 732, 28138, 13], "temperature": 0.0, "avg_logprob": -0.2249883729584363, "compression_ratio": 1.4401709401709402, "no_speech_prob": 3.119285247521475e-05}, {"id": 18, "seek": 5924, "start": 82.6, "end": 87.80000000000001, "text": " I mean, one, it's the successful application of transfer learning to NLP.", "tokens": [286, 914, 11, 472, 11, 309, 311, 264, 4406, 3861, 295, 5003, 2539, 281, 426, 45196, 13], "temperature": 0.0, "avg_logprob": -0.2249883729584363, "compression_ratio": 1.4401709401709402, "no_speech_prob": 3.119285247521475e-05}, {"id": 19, "seek": 8780, "start": 87.8, "end": 94.12, "text": " Also, ImageNet just led to such huge advances in the field of computer vision", "tokens": [2743, 11, 29903, 31890, 445, 4684, 281, 1270, 2603, 25297, 294, 264, 2519, 295, 3820, 5201], "temperature": 0.0, "avg_logprob": -0.18138272421700613, "compression_ratio": 1.5023696682464456, "no_speech_prob": 8.529521437594667e-06}, {"id": 20, "seek": 8780, "start": 94.12, "end": 97.52, "text": " and that that's kind of now happening in the area of NLP.", "tokens": [293, 300, 300, 311, 733, 295, 586, 2737, 294, 264, 1859, 295, 426, 45196, 13], "temperature": 0.0, "avg_logprob": -0.18138272421700613, "compression_ratio": 1.5023696682464456, "no_speech_prob": 8.529521437594667e-06}, {"id": 21, "seek": 8780, "start": 97.52, "end": 105.16, "text": " And actually to check, have you covered transfer learning in your other classes?", "tokens": [400, 767, 281, 1520, 11, 362, 291, 5343, 5003, 2539, 294, 428, 661, 5359, 30], "temperature": 0.0, "avg_logprob": -0.18138272421700613, "compression_ratio": 1.5023696682464456, "no_speech_prob": 8.529521437594667e-06}, {"id": 22, "seek": 8780, "start": 105.16, "end": 106.8, "text": " Yeah? Okay.", "tokens": [865, 30, 1033, 13], "temperature": 0.0, "avg_logprob": -0.18138272421700613, "compression_ratio": 1.5023696682464456, "no_speech_prob": 8.529521437594667e-06}, {"id": 23, "seek": 8780, "start": 106.8, "end": 113.92, "text": " And so we'll talk more about this, but yeah, that it's kind of letting you train a model", "tokens": [400, 370, 321, 603, 751, 544, 466, 341, 11, 457, 1338, 11, 300, 309, 311, 733, 295, 8295, 291, 3847, 257, 2316], "temperature": 0.0, "avg_logprob": -0.18138272421700613, "compression_ratio": 1.5023696682464456, "no_speech_prob": 8.529521437594667e-06}, {"id": 24, "seek": 11392, "start": 113.92, "end": 120.04, "text": " on a different and often larger data set and then, oh,", "tokens": [322, 257, 819, 293, 2049, 4833, 1412, 992, 293, 550, 11, 1954, 11], "temperature": 0.0, "avg_logprob": -0.1569153671964593, "compression_ratio": 1.7820512820512822, "no_speech_prob": 3.269180524512194e-05}, {"id": 25, "seek": 11392, "start": 120.04, "end": 124.4, "text": " some people are shaking their heads no.", "tokens": [512, 561, 366, 15415, 641, 8050, 572, 13], "temperature": 0.0, "avg_logprob": -0.1569153671964593, "compression_ratio": 1.7820512820512822, "no_speech_prob": 3.269180524512194e-05}, {"id": 26, "seek": 11392, "start": 124.4, "end": 128.6, "text": " So, and I'll have a, we'll have a few slides on this next time,", "tokens": [407, 11, 293, 286, 603, 362, 257, 11, 321, 603, 362, 257, 1326, 9788, 322, 341, 958, 565, 11], "temperature": 0.0, "avg_logprob": -0.1569153671964593, "compression_ratio": 1.7820512820512822, "no_speech_prob": 3.269180524512194e-05}, {"id": 27, "seek": 11392, "start": 128.6, "end": 134.0, "text": " but it's the general idea is that you are training a model on a large data set", "tokens": [457, 309, 311, 264, 2674, 1558, 307, 300, 291, 366, 3097, 257, 2316, 322, 257, 2416, 1412, 992], "temperature": 0.0, "avg_logprob": -0.1569153671964593, "compression_ratio": 1.7820512820512822, "no_speech_prob": 3.269180524512194e-05}, {"id": 28, "seek": 11392, "start": 134.0, "end": 138.76, "text": " that's probably different from your data set, which is often smaller, but you're able", "tokens": [300, 311, 1391, 819, 490, 428, 1412, 992, 11, 597, 307, 2049, 4356, 11, 457, 291, 434, 1075], "temperature": 0.0, "avg_logprob": -0.1569153671964593, "compression_ratio": 1.7820512820512822, "no_speech_prob": 3.269180524512194e-05}, {"id": 29, "seek": 11392, "start": 138.76, "end": 143.44, "text": " to kind of take advantage of having this model that was already trained on this big data set.", "tokens": [281, 733, 295, 747, 5002, 295, 1419, 341, 2316, 300, 390, 1217, 8895, 322, 341, 955, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.1569153671964593, "compression_ratio": 1.7820512820512822, "no_speech_prob": 3.269180524512194e-05}, {"id": 30, "seek": 14344, "start": 143.44, "end": 149.04, "text": " And you can fine tune it for your smaller data set and that's,", "tokens": [400, 291, 393, 2489, 10864, 309, 337, 428, 4356, 1412, 992, 293, 300, 311, 11], "temperature": 0.0, "avg_logprob": -0.16807500598500075, "compression_ratio": 1.5742971887550201, "no_speech_prob": 1.1658353287202772e-05}, {"id": 31, "seek": 14344, "start": 149.04, "end": 154.28, "text": " and get state of the art results, which is really, really exciting.", "tokens": [293, 483, 1785, 295, 264, 1523, 3542, 11, 597, 307, 534, 11, 534, 4670, 13], "temperature": 0.0, "avg_logprob": -0.16807500598500075, "compression_ratio": 1.5742971887550201, "no_speech_prob": 1.1658353287202772e-05}, {"id": 32, "seek": 14344, "start": 154.28, "end": 157.4, "text": " So we'll start with building a language model.", "tokens": [407, 321, 603, 722, 365, 2390, 257, 2856, 2316, 13], "temperature": 0.0, "avg_logprob": -0.16807500598500075, "compression_ratio": 1.5742971887550201, "no_speech_prob": 1.1658353287202772e-05}, {"id": 33, "seek": 14344, "start": 157.4, "end": 160.28, "text": " And remember, we'll later come back to this goal", "tokens": [400, 1604, 11, 321, 603, 1780, 808, 646, 281, 341, 3387], "temperature": 0.0, "avg_logprob": -0.16807500598500075, "compression_ratio": 1.5742971887550201, "no_speech_prob": 1.1658353287202772e-05}, {"id": 34, "seek": 14344, "start": 160.28, "end": 163.64, "text": " of deciding if a movie review is positive or negative.", "tokens": [295, 17990, 498, 257, 3169, 3131, 307, 3353, 420, 3671, 13], "temperature": 0.0, "avg_logprob": -0.16807500598500075, "compression_ratio": 1.5742971887550201, "no_speech_prob": 1.1658353287202772e-05}, {"id": 35, "seek": 14344, "start": 163.64, "end": 169.72, "text": " I wanted to highlight Janelle Shane, who blogs and tweets about,", "tokens": [286, 1415, 281, 5078, 4956, 4434, 25865, 11, 567, 31038, 293, 25671, 466, 11], "temperature": 0.0, "avg_logprob": -0.16807500598500075, "compression_ratio": 1.5742971887550201, "no_speech_prob": 1.1658353287202772e-05}, {"id": 36, "seek": 14344, "start": 169.72, "end": 172.84, "text": " she does a lot of very creative work with AI.", "tokens": [750, 775, 257, 688, 295, 588, 5880, 589, 365, 7318, 13], "temperature": 0.0, "avg_logprob": -0.16807500598500075, "compression_ratio": 1.5742971887550201, "no_speech_prob": 1.1658353287202772e-05}, {"id": 37, "seek": 17284, "start": 172.84, "end": 176.0, "text": " But she, and she often uses text.", "tokens": [583, 750, 11, 293, 750, 2049, 4960, 2487, 13], "temperature": 0.0, "avg_logprob": -0.16643753970961972, "compression_ratio": 1.417989417989418, "no_speech_prob": 3.089316805926501e-06}, {"id": 38, "seek": 17284, "start": 176.0, "end": 185.6, "text": " She's done posts trying to train a neural network to tell jokes.", "tokens": [1240, 311, 1096, 12300, 1382, 281, 3847, 257, 18161, 3209, 281, 980, 14439, 13], "temperature": 0.0, "avg_logprob": -0.16643753970961972, "compression_ratio": 1.417989417989418, "no_speech_prob": 3.089316805926501e-06}, {"id": 39, "seek": 17284, "start": 185.6, "end": 190.08, "text": " So this came up with, what do you call a cat?", "tokens": [407, 341, 1361, 493, 365, 11, 437, 360, 291, 818, 257, 3857, 30], "temperature": 0.0, "avg_logprob": -0.16643753970961972, "compression_ratio": 1.417989417989418, "no_speech_prob": 3.089316805926501e-06}, {"id": 40, "seek": 17284, "start": 190.08, "end": 192.8, "text": " Does it take to screw in a light bulb?", "tokens": [4402, 309, 747, 281, 5630, 294, 257, 1442, 21122, 30], "temperature": 0.0, "avg_logprob": -0.16643753970961972, "compression_ratio": 1.417989417989418, "no_speech_prob": 3.089316805926501e-06}, {"id": 41, "seek": 17284, "start": 192.8, "end": 194.88, "text": " They could worry the banana.", "tokens": [814, 727, 3292, 264, 14194, 13], "temperature": 0.0, "avg_logprob": -0.16643753970961972, "compression_ratio": 1.417989417989418, "no_speech_prob": 3.089316805926501e-06}, {"id": 42, "seek": 17284, "start": 194.88, "end": 198.96, "text": " What did the new aunt say after a dog?", "tokens": [708, 630, 264, 777, 15654, 584, 934, 257, 3000, 30], "temperature": 0.0, "avg_logprob": -0.16643753970961972, "compression_ratio": 1.417989417989418, "no_speech_prob": 3.089316805926501e-06}, {"id": 43, "seek": 17284, "start": 198.96, "end": 201.04, "text": " It was a pirate.", "tokens": [467, 390, 257, 27424, 13], "temperature": 0.0, "avg_logprob": -0.16643753970961972, "compression_ratio": 1.417989417989418, "no_speech_prob": 3.089316805926501e-06}, {"id": 44, "seek": 20104, "start": 201.04, "end": 207.56, "text": " These don't actually make sense, but they're funny.", "tokens": [1981, 500, 380, 767, 652, 2020, 11, 457, 436, 434, 4074, 13], "temperature": 0.0, "avg_logprob": -0.17806780183470095, "compression_ratio": 1.5053191489361701, "no_speech_prob": 5.954831067356281e-06}, {"id": 45, "seek": 20104, "start": 207.56, "end": 215.6, "text": " And I like a lot of her, a lot of her work is very humorous in a kind of absurd way.", "tokens": [400, 286, 411, 257, 688, 295, 720, 11, 257, 688, 295, 720, 589, 307, 588, 14318, 563, 294, 257, 733, 295, 19774, 636, 13], "temperature": 0.0, "avg_logprob": -0.17806780183470095, "compression_ratio": 1.5053191489361701, "no_speech_prob": 5.954831067356281e-06}, {"id": 46, "seek": 20104, "start": 215.6, "end": 220.2, "text": " Since she was having trouble with her neural net learning jokes,", "tokens": [4162, 750, 390, 1419, 5253, 365, 720, 18161, 2533, 2539, 14439, 11], "temperature": 0.0, "avg_logprob": -0.17806780183470095, "compression_ratio": 1.5053191489361701, "no_speech_prob": 5.954831067356281e-06}, {"id": 47, "seek": 20104, "start": 220.2, "end": 228.39999999999998, "text": " she tried retraining it with what do you get when you cross a blank with a blank?", "tokens": [750, 3031, 49356, 1760, 309, 365, 437, 360, 291, 483, 562, 291, 3278, 257, 8247, 365, 257, 8247, 30], "temperature": 0.0, "avg_logprob": -0.17806780183470095, "compression_ratio": 1.5053191489361701, "no_speech_prob": 5.954831067356281e-06}, {"id": 48, "seek": 22840, "start": 228.4, "end": 234.08, "text": " What do you get when you cross a pirate in a little butter, a bathroom?", "tokens": [708, 360, 291, 483, 562, 291, 3278, 257, 27424, 294, 257, 707, 5517, 11, 257, 8687, 30], "temperature": 0.0, "avg_logprob": -0.2041179656982422, "compression_ratio": 1.5824742268041236, "no_speech_prob": 4.3566151362028904e-06}, {"id": 49, "seek": 22840, "start": 234.08, "end": 237.24, "text": " What do you get when you cross a dinosaur?", "tokens": [708, 360, 291, 483, 562, 291, 3278, 257, 23627, 30], "temperature": 0.0, "avg_logprob": -0.2041179656982422, "compression_ratio": 1.5824742268041236, "no_speech_prob": 4.3566151362028904e-06}, {"id": 50, "seek": 22840, "start": 237.24, "end": 240.84, "text": " They get a lawyer.", "tokens": [814, 483, 257, 11613, 13], "temperature": 0.0, "avg_logprob": -0.2041179656982422, "compression_ratio": 1.5824742268041236, "no_speech_prob": 4.3566151362028904e-06}, {"id": 51, "seek": 22840, "start": 240.84, "end": 245.36, "text": " So I see that many of you like these jokes from the neural net.", "tokens": [407, 286, 536, 300, 867, 295, 291, 411, 613, 14439, 490, 264, 18161, 2533, 13], "temperature": 0.0, "avg_logprob": -0.2041179656982422, "compression_ratio": 1.5824742268041236, "no_speech_prob": 4.3566151362028904e-06}, {"id": 52, "seek": 22840, "start": 245.36, "end": 250.0, "text": " But I think this is an example of kind of some of the whimsical", "tokens": [583, 286, 519, 341, 307, 364, 1365, 295, 733, 295, 512, 295, 264, 315, 49945], "temperature": 0.0, "avg_logprob": -0.2041179656982422, "compression_ratio": 1.5824742268041236, "no_speech_prob": 4.3566151362028904e-06}, {"id": 53, "seek": 22840, "start": 250.0, "end": 253.68, "text": " and creative things you can do with language.", "tokens": [293, 5880, 721, 291, 393, 360, 365, 2856, 13], "temperature": 0.0, "avg_logprob": -0.2041179656982422, "compression_ratio": 1.5824742268041236, "no_speech_prob": 4.3566151362028904e-06}, {"id": 54, "seek": 25368, "start": 253.68, "end": 258.36, "text": " Why was six afraid of seven?", "tokens": [1545, 390, 2309, 4638, 295, 3407, 30], "temperature": 0.0, "avg_logprob": -0.23169060827980578, "compression_ratio": 1.3867403314917126, "no_speech_prob": 2.8853319236077368e-05}, {"id": 55, "seek": 25368, "start": 258.36, "end": 260.36, "text": " Because he doesn't have a birthday.", "tokens": [1436, 415, 1177, 380, 362, 257, 6154, 13], "temperature": 0.0, "avg_logprob": -0.23169060827980578, "compression_ratio": 1.3867403314917126, "no_speech_prob": 2.8853319236077368e-05}, {"id": 56, "seek": 25368, "start": 260.36, "end": 271.16, "text": " Yeah, and I mean it's neat because the setups do,", "tokens": [865, 11, 293, 286, 914, 309, 311, 10654, 570, 264, 46832, 360, 11], "temperature": 0.0, "avg_logprob": -0.23169060827980578, "compression_ratio": 1.3867403314917126, "no_speech_prob": 2.8853319236077368e-05}, {"id": 57, "seek": 25368, "start": 271.16, "end": 275.08, "text": " they do kind of fit with what we would expect from a joke.", "tokens": [436, 360, 733, 295, 3318, 365, 437, 321, 576, 2066, 490, 257, 7647, 13], "temperature": 0.0, "avg_logprob": -0.23169060827980578, "compression_ratio": 1.3867403314917126, "no_speech_prob": 2.8853319236077368e-05}, {"id": 58, "seek": 25368, "start": 275.08, "end": 278.56, "text": " Although it's still, and this is even something where you can see how they're", "tokens": [5780, 309, 311, 920, 11, 293, 341, 307, 754, 746, 689, 291, 393, 536, 577, 436, 434], "temperature": 0.0, "avg_logprob": -0.23169060827980578, "compression_ratio": 1.3867403314917126, "no_speech_prob": 2.8853319236077368e-05}, {"id": 59, "seek": 27856, "start": 278.56, "end": 285.36, "text": " on the right track that eventually maybe these will be more human-like jokes.", "tokens": [322, 264, 558, 2837, 300, 4728, 1310, 613, 486, 312, 544, 1952, 12, 4092, 14439, 13], "temperature": 0.0, "avg_logprob": -0.31775632015494415, "compression_ratio": 1.2975206611570247, "no_speech_prob": 1.7498248780611902e-05}, {"id": 60, "seek": 27856, "start": 285.36, "end": 295.52, "text": " Yeah, so it didn't do that well with the most common jokes.", "tokens": [865, 11, 370, 309, 994, 380, 360, 300, 731, 365, 264, 881, 2689, 14439, 13], "temperature": 0.0, "avg_logprob": -0.31775632015494415, "compression_ratio": 1.2975206611570247, "no_speech_prob": 1.7498248780611902e-05}, {"id": 61, "seek": 29552, "start": 295.52, "end": 310.59999999999997, "text": " Which one was that?", "tokens": [3013, 472, 390, 300, 30], "temperature": 0.0, "avg_logprob": -0.19234926469864383, "compression_ratio": 1.1149425287356323, "no_speech_prob": 1.202800831379136e-05}, {"id": 62, "seek": 29552, "start": 310.59999999999997, "end": 312.71999999999997, "text": " So I like that one.", "tokens": [407, 286, 411, 300, 472, 13], "temperature": 0.0, "avg_logprob": -0.19234926469864383, "compression_ratio": 1.1149425287356323, "no_speech_prob": 1.202800831379136e-05}, {"id": 63, "seek": 29552, "start": 312.71999999999997, "end": 320.35999999999996, "text": " She's also done using a neural network to create recipes.", "tokens": [1240, 311, 611, 1096, 1228, 257, 18161, 3209, 281, 1884, 13035, 13], "temperature": 0.0, "avg_logprob": -0.19234926469864383, "compression_ratio": 1.1149425287356323, "no_speech_prob": 1.202800831379136e-05}, {"id": 64, "seek": 32036, "start": 320.36, "end": 327.04, "text": " This is another language task, so small sandwiches using a dish,", "tokens": [639, 307, 1071, 2856, 5633, 11, 370, 1359, 29022, 1228, 257, 5025, 11], "temperature": 0.0, "avg_logprob": -0.22583621123741413, "compression_ratio": 1.4331210191082802, "no_speech_prob": 2.2470905605587177e-05}, {"id": 65, "seek": 32036, "start": 327.04, "end": 339.64, "text": " chili, lemons, salads, and seafood with shortening, snow peas, peach halves.", "tokens": [15575, 11, 47098, 11, 48025, 11, 293, 23206, 365, 2099, 4559, 11, 5756, 24494, 11, 25917, 38490, 13], "temperature": 0.0, "avg_logprob": -0.22583621123741413, "compression_ratio": 1.4331210191082802, "no_speech_prob": 2.2470905605587177e-05}, {"id": 66, "seek": 32036, "start": 339.64, "end": 343.64, "text": " I don't know what this remaining posting.", "tokens": [286, 500, 380, 458, 437, 341, 8877, 15978, 13], "temperature": 0.0, "avg_logprob": -0.22583621123741413, "compression_ratio": 1.4331210191082802, "no_speech_prob": 2.2470905605587177e-05}, {"id": 67, "seek": 32036, "start": 343.64, "end": 347.12, "text": " Two large bones of sliced chicken salmon.", "tokens": [4453, 2416, 10491, 295, 27098, 4662, 18518, 13], "temperature": 0.0, "avg_logprob": -0.22583621123741413, "compression_ratio": 1.4331210191082802, "no_speech_prob": 2.2470905605587177e-05}, {"id": 68, "seek": 34712, "start": 347.12, "end": 351.56, "text": " It's also something that if you don't read it too closely,", "tokens": [467, 311, 611, 746, 300, 498, 291, 500, 380, 1401, 309, 886, 8185, 11], "temperature": 0.0, "avg_logprob": -0.15782940513209293, "compression_ratio": 1.6216216216216217, "no_speech_prob": 0.00016593383043073118}, {"id": 69, "seek": 34712, "start": 351.56, "end": 357.32, "text": " it seems like it could be a recipe, but then if you read it more closely", "tokens": [309, 2544, 411, 309, 727, 312, 257, 6782, 11, 457, 550, 498, 291, 1401, 309, 544, 8185], "temperature": 0.0, "avg_logprob": -0.15782940513209293, "compression_ratio": 1.6216216216216217, "no_speech_prob": 0.00016593383043073118}, {"id": 70, "seek": 34712, "start": 357.32, "end": 360.48, "text": " and think about it, some of these don't make sense.", "tokens": [293, 519, 466, 309, 11, 512, 295, 613, 500, 380, 652, 2020, 13], "temperature": 0.0, "avg_logprob": -0.15782940513209293, "compression_ratio": 1.6216216216216217, "no_speech_prob": 0.00016593383043073118}, {"id": 71, "seek": 34712, "start": 360.48, "end": 364.24, "text": " But it's got three tablespoons coarsely chopped green onions.", "tokens": [583, 309, 311, 658, 1045, 21615, 598, 685, 736, 16497, 3092, 13146, 13], "temperature": 0.0, "avg_logprob": -0.15782940513209293, "compression_ratio": 1.6216216216216217, "no_speech_prob": 0.00016593383043073118}, {"id": 72, "seek": 34712, "start": 364.24, "end": 368.08, "text": " But I encourage you to check out her blog and her Twitter", "tokens": [583, 286, 5373, 291, 281, 1520, 484, 720, 6968, 293, 720, 5794], "temperature": 0.0, "avg_logprob": -0.15782940513209293, "compression_ratio": 1.6216216216216217, "no_speech_prob": 0.00016593383043073118}, {"id": 73, "seek": 34712, "start": 368.08, "end": 373.0, "text": " because she posts a lot of really fun, fun applications.", "tokens": [570, 750, 12300, 257, 688, 295, 534, 1019, 11, 1019, 5821, 13], "temperature": 0.0, "avg_logprob": -0.15782940513209293, "compression_ratio": 1.6216216216216217, "no_speech_prob": 0.00016593383043073118}, {"id": 74, "seek": 37300, "start": 373.0, "end": 387.92, "text": " She's also done D&D character bios and names of heavy metal bands", "tokens": [1240, 311, 611, 1096, 413, 5, 35, 2517, 36997, 293, 5288, 295, 4676, 5760, 13543], "temperature": 0.0, "avg_logprob": -0.22512320221447554, "compression_ratio": 1.4105960264900663, "no_speech_prob": 1.2801627235603519e-05}, {"id": 75, "seek": 37300, "start": 387.92, "end": 389.52, "text": " and new tomato breeds.", "tokens": [293, 777, 9288, 41609, 13], "temperature": 0.0, "avg_logprob": -0.22512320221447554, "compression_ratio": 1.4105960264900663, "no_speech_prob": 1.2801627235603519e-05}, {"id": 76, "seek": 37300, "start": 389.52, "end": 391.8, "text": " So there's a lot on her website.", "tokens": [407, 456, 311, 257, 688, 322, 720, 3144, 13], "temperature": 0.0, "avg_logprob": -0.22512320221447554, "compression_ratio": 1.4105960264900663, "no_speech_prob": 1.2801627235603519e-05}, {"id": 77, "seek": 37300, "start": 391.8, "end": 394.48, "text": " And this can also give you ideas if there are things you want", "tokens": [400, 341, 393, 611, 976, 291, 3487, 498, 456, 366, 721, 291, 528], "temperature": 0.0, "avg_logprob": -0.22512320221447554, "compression_ratio": 1.4105960264900663, "no_speech_prob": 1.2801627235603519e-05}, {"id": 78, "seek": 37300, "start": 394.48, "end": 399.4, "text": " to try with a language model.", "tokens": [281, 853, 365, 257, 2856, 2316, 13], "temperature": 0.0, "avg_logprob": -0.22512320221447554, "compression_ratio": 1.4105960264900663, "no_speech_prob": 1.2801627235603519e-05}, {"id": 79, "seek": 39940, "start": 399.4, "end": 405.03999999999996, "text": " So you will need to use the Fast AI library for this lesson.", "tokens": [407, 291, 486, 643, 281, 764, 264, 15968, 7318, 6405, 337, 341, 6898, 13], "temperature": 0.0, "avg_logprob": -0.19434188514627437, "compression_ratio": 1.5231481481481481, "no_speech_prob": 0.00015339479432441294}, {"id": 80, "seek": 39940, "start": 405.03999999999996, "end": 409.03999999999996, "text": " Hopefully most of you got it installed when we did Notebook 3.", "tokens": [10429, 881, 295, 291, 658, 309, 8899, 562, 321, 630, 11633, 2939, 805, 13], "temperature": 0.0, "avg_logprob": -0.19434188514627437, "compression_ratio": 1.5231481481481481, "no_speech_prob": 0.00015339479432441294}, {"id": 81, "seek": 39940, "start": 409.03999999999996, "end": 412.03999999999996, "text": " But if not, definitely please let me know.", "tokens": [583, 498, 406, 11, 2138, 1767, 718, 385, 458, 13], "temperature": 0.0, "avg_logprob": -0.19434188514627437, "compression_ratio": 1.5231481481481481, "no_speech_prob": 0.00015339479432441294}, {"id": 82, "seek": 39940, "start": 412.03999999999996, "end": 417.71999999999997, "text": " And in Lesson 3, I post, or I guess Notebook 3 be the link", "tokens": [400, 294, 18649, 266, 805, 11, 286, 2183, 11, 420, 286, 2041, 11633, 2939, 805, 312, 264, 2113], "temperature": 0.0, "avg_logprob": -0.19434188514627437, "compression_ratio": 1.5231481481481481, "no_speech_prob": 0.00015339479432441294}, {"id": 83, "seek": 39940, "start": 417.71999999999997, "end": 420.52, "text": " to troubleshooting your Fast AI installation.", "tokens": [281, 15379, 47011, 428, 15968, 7318, 13260, 13], "temperature": 0.0, "avg_logprob": -0.19434188514627437, "compression_ratio": 1.5231481481481481, "no_speech_prob": 0.00015339479432441294}, {"id": 84, "seek": 39940, "start": 420.52, "end": 429.12, "text": " We will need a GPO to train your neural net on this part.", "tokens": [492, 486, 643, 257, 26039, 46, 281, 3847, 428, 18161, 2533, 322, 341, 644, 13], "temperature": 0.0, "avg_logprob": -0.19434188514627437, "compression_ratio": 1.5231481481481481, "no_speech_prob": 0.00015339479432441294}, {"id": 85, "seek": 42912, "start": 429.12, "end": 436.4, "text": " And for many of you that are using Macs, you'll need to use a GPU", "tokens": [400, 337, 867, 295, 291, 300, 366, 1228, 5707, 82, 11, 291, 603, 643, 281, 764, 257, 18407], "temperature": 0.0, "avg_logprob": -0.15679231144133068, "compression_ratio": 1.404040404040404, "no_speech_prob": 8.347086259163916e-05}, {"id": 86, "seek": 42912, "start": 436.4, "end": 438.8, "text": " in the cloud since you don't have an NVIDIA GPU,", "tokens": [294, 264, 4588, 1670, 291, 500, 380, 362, 364, 426, 3958, 6914, 18407, 11], "temperature": 0.0, "avg_logprob": -0.15679231144133068, "compression_ratio": 1.404040404040404, "no_speech_prob": 8.347086259163916e-05}, {"id": 87, "seek": 42912, "start": 438.8, "end": 442.16, "text": " which is what the major deep learning libraries need.", "tokens": [597, 307, 437, 264, 2563, 2452, 2539, 15148, 643, 13], "temperature": 0.0, "avg_logprob": -0.15679231144133068, "compression_ratio": 1.404040404040404, "no_speech_prob": 8.347086259163916e-05}, {"id": 88, "seek": 42912, "start": 442.16, "end": 447.32, "text": " And let me just check, have you guys used GPUs in previous classes?", "tokens": [400, 718, 385, 445, 1520, 11, 362, 291, 1074, 1143, 18407, 82, 294, 3894, 5359, 30], "temperature": 0.0, "avg_logprob": -0.15679231144133068, "compression_ratio": 1.404040404040404, "no_speech_prob": 8.347086259163916e-05}, {"id": 89, "seek": 42912, "start": 447.32, "end": 453.16, "text": " Okay, so I see a mix of yeses and maybes.", "tokens": [1033, 11, 370, 286, 536, 257, 2890, 295, 2086, 279, 293, 815, 6446, 13], "temperature": 0.0, "avg_logprob": -0.15679231144133068, "compression_ratio": 1.404040404040404, "no_speech_prob": 8.347086259163916e-05}, {"id": 90, "seek": 45316, "start": 453.16, "end": 461.40000000000003, "text": " This, let me show you, so we on the Fast AI set up,", "tokens": [639, 11, 718, 385, 855, 291, 11, 370, 321, 322, 264, 15968, 7318, 992, 493, 11], "temperature": 0.0, "avg_logprob": -0.18328685953159524, "compression_ratio": 1.4931506849315068, "no_speech_prob": 2.2123769667814486e-05}, {"id": 91, "seek": 45316, "start": 461.40000000000003, "end": 464.32000000000005, "text": " list a number of options.", "tokens": [1329, 257, 1230, 295, 3956, 13], "temperature": 0.0, "avg_logprob": -0.18328685953159524, "compression_ratio": 1.4931506849315068, "no_speech_prob": 2.2123769667814486e-05}, {"id": 92, "seek": 45316, "start": 464.32000000000005, "end": 467.64000000000004, "text": " Here are some ready to run one-click sites.", "tokens": [1692, 366, 512, 1919, 281, 1190, 472, 12, 18548, 7533, 13], "temperature": 0.0, "avg_logprob": -0.18328685953159524, "compression_ratio": 1.4931506849315068, "no_speech_prob": 2.2123769667814486e-05}, {"id": 93, "seek": 45316, "start": 467.64000000000004, "end": 472.44000000000005, "text": " CoLab is nice in that it's free as a product from Google.", "tokens": [3066, 37880, 307, 1481, 294, 300, 309, 311, 1737, 382, 257, 1674, 490, 3329, 13], "temperature": 0.0, "avg_logprob": -0.18328685953159524, "compression_ratio": 1.4931506849315068, "no_speech_prob": 2.2123769667814486e-05}, {"id": 94, "seek": 45316, "start": 472.44000000000005, "end": 476.28000000000003, "text": " A number of these give you, you know, a certain amount", "tokens": [316, 1230, 295, 613, 976, 291, 11, 291, 458, 11, 257, 1629, 2372], "temperature": 0.0, "avg_logprob": -0.18328685953159524, "compression_ratio": 1.4931506849315068, "no_speech_prob": 2.2123769667814486e-05}, {"id": 95, "seek": 45316, "start": 476.28000000000003, "end": 480.24, "text": " of free credit when you first start.", "tokens": [295, 1737, 5397, 562, 291, 700, 722, 13], "temperature": 0.0, "avg_logprob": -0.18328685953159524, "compression_ratio": 1.4931506849315068, "no_speech_prob": 2.2123769667814486e-05}, {"id": 96, "seek": 45316, "start": 480.24, "end": 483.12, "text": " Definitely, yeah, let me know if you get stuck on this.", "tokens": [12151, 11, 1338, 11, 718, 385, 458, 498, 291, 483, 5541, 322, 341, 13], "temperature": 0.0, "avg_logprob": -0.18328685953159524, "compression_ratio": 1.4931506849315068, "no_speech_prob": 2.2123769667814486e-05}, {"id": 97, "seek": 48312, "start": 483.12, "end": 487.32, "text": " The important thing to know with any option that involves payment,", "tokens": [440, 1021, 551, 281, 458, 365, 604, 3614, 300, 11626, 10224, 11], "temperature": 0.0, "avg_logprob": -0.14499066367981925, "compression_ratio": 1.6262295081967213, "no_speech_prob": 1.6441563275293447e-05}, {"id": 98, "seek": 48312, "start": 487.32, "end": 491.12, "text": " which is pretty much, I think, everything other than Google CoLab,", "tokens": [597, 307, 1238, 709, 11, 286, 519, 11, 1203, 661, 813, 3329, 3066, 37880, 11], "temperature": 0.0, "avg_logprob": -0.14499066367981925, "compression_ratio": 1.6262295081967213, "no_speech_prob": 1.6441563275293447e-05}, {"id": 99, "seek": 48312, "start": 491.12, "end": 496.44, "text": " although Google CoLab has kind of time limits of when it'll shut you down,", "tokens": [4878, 3329, 3066, 37880, 575, 733, 295, 565, 10406, 295, 562, 309, 603, 5309, 291, 760, 11], "temperature": 0.0, "avg_logprob": -0.14499066367981925, "compression_ratio": 1.6262295081967213, "no_speech_prob": 1.6441563275293447e-05}, {"id": 100, "seek": 48312, "start": 496.44, "end": 499.48, "text": " is to always shut down your instance when you're done.", "tokens": [307, 281, 1009, 5309, 760, 428, 5197, 562, 291, 434, 1096, 13], "temperature": 0.0, "avg_logprob": -0.14499066367981925, "compression_ratio": 1.6262295081967213, "no_speech_prob": 1.6441563275293447e-05}, {"id": 101, "seek": 48312, "start": 499.48, "end": 501.8, "text": " Otherwise, you could end up with an expensive bill.", "tokens": [10328, 11, 291, 727, 917, 493, 365, 364, 5124, 2961, 13], "temperature": 0.0, "avg_logprob": -0.14499066367981925, "compression_ratio": 1.6262295081967213, "no_speech_prob": 1.6441563275293447e-05}, {"id": 102, "seek": 48312, "start": 501.8, "end": 505.04, "text": " And so you definitely don't want to start something, and then two months", "tokens": [400, 370, 291, 2138, 500, 380, 528, 281, 722, 746, 11, 293, 550, 732, 2493], "temperature": 0.0, "avg_logprob": -0.14499066367981925, "compression_ratio": 1.6262295081967213, "no_speech_prob": 1.6441563275293447e-05}, {"id": 103, "seek": 48312, "start": 505.04, "end": 508.36, "text": " from now you've forgotten all about the class and get a bill", "tokens": [490, 586, 291, 600, 11832, 439, 466, 264, 1508, 293, 483, 257, 2961], "temperature": 0.0, "avg_logprob": -0.14499066367981925, "compression_ratio": 1.6262295081967213, "no_speech_prob": 1.6441563275293447e-05}, {"id": 104, "seek": 48312, "start": 508.36, "end": 510.76, "text": " that it's been running the whole time.", "tokens": [300, 309, 311, 668, 2614, 264, 1379, 565, 13], "temperature": 0.0, "avg_logprob": -0.14499066367981925, "compression_ratio": 1.6262295081967213, "no_speech_prob": 1.6441563275293447e-05}, {"id": 105, "seek": 48312, "start": 510.76, "end": 512.12, "text": " Jeremy?", "tokens": [17809, 30], "temperature": 0.0, "avg_logprob": -0.14499066367981925, "compression_ratio": 1.6262295081967213, "no_speech_prob": 1.6441563275293447e-05}, {"id": 106, "seek": 51212, "start": 512.12, "end": 514.96, "text": " Google Cloud has $300 of free credit.", "tokens": [3329, 8061, 575, 1848, 12566, 295, 1737, 5397, 13], "temperature": 0.0, "avg_logprob": -0.18994969981057302, "compression_ratio": 1.6824034334763949, "no_speech_prob": 6.012609082972631e-05}, {"id": 107, "seek": 51212, "start": 514.96, "end": 518.48, "text": " Oh, okay, Google Cloud has $300 of free credit.", "tokens": [876, 11, 1392, 11, 3329, 8061, 575, 1848, 12566, 295, 1737, 5397, 13], "temperature": 0.0, "avg_logprob": -0.18994969981057302, "compression_ratio": 1.6824034334763949, "no_speech_prob": 6.012609082972631e-05}, {"id": 108, "seek": 51212, "start": 518.48, "end": 521.44, "text": " So what do you have to do to get those?", "tokens": [407, 437, 360, 291, 362, 281, 360, 281, 483, 729, 30], "temperature": 0.0, "avg_logprob": -0.18994969981057302, "compression_ratio": 1.6824034334763949, "no_speech_prob": 6.012609082972631e-05}, {"id": 109, "seek": 51212, "start": 521.44, "end": 525.0, "text": " Oh, okay, and you don't have to do anything to get those.", "tokens": [876, 11, 1392, 11, 293, 291, 500, 380, 362, 281, 360, 1340, 281, 483, 729, 13], "temperature": 0.0, "avg_logprob": -0.18994969981057302, "compression_ratio": 1.6824034334763949, "no_speech_prob": 6.012609082972631e-05}, {"id": 110, "seek": 51212, "start": 525.0, "end": 526.6, "text": " So yeah, definitely, I guess, be on the lookout,", "tokens": [407, 1338, 11, 2138, 11, 286, 2041, 11, 312, 322, 264, 41025, 11], "temperature": 0.0, "avg_logprob": -0.18994969981057302, "compression_ratio": 1.6824034334763949, "no_speech_prob": 6.012609082972631e-05}, {"id": 111, "seek": 51212, "start": 526.6, "end": 529.0, "text": " because I think many service providers are kind", "tokens": [570, 286, 519, 867, 2643, 11330, 366, 733], "temperature": 0.0, "avg_logprob": -0.18994969981057302, "compression_ratio": 1.6824034334763949, "no_speech_prob": 6.012609082972631e-05}, {"id": 112, "seek": 51212, "start": 529.0, "end": 532.16, "text": " of offering these discounts to get started.", "tokens": [295, 8745, 613, 37930, 281, 483, 1409, 13], "temperature": 0.0, "avg_logprob": -0.18994969981057302, "compression_ratio": 1.6824034334763949, "no_speech_prob": 6.012609082972631e-05}, {"id": 113, "seek": 51212, "start": 532.16, "end": 538.6800000000001, "text": " But this is, yes, something that's necessary to have a GPU in order", "tokens": [583, 341, 307, 11, 2086, 11, 746, 300, 311, 4818, 281, 362, 257, 18407, 294, 1668], "temperature": 0.0, "avg_logprob": -0.18994969981057302, "compression_ratio": 1.6824034334763949, "no_speech_prob": 6.012609082972631e-05}, {"id": 114, "seek": 53868, "start": 538.68, "end": 543.16, "text": " to really kind of be taking advantage of the speed you'll need to train some", "tokens": [281, 534, 733, 295, 312, 1940, 5002, 295, 264, 3073, 291, 603, 643, 281, 3847, 512], "temperature": 0.0, "avg_logprob": -0.27919779068384415, "compression_ratio": 1.4064171122994653, "no_speech_prob": 7.527218258474022e-06}, {"id": 115, "seek": 53868, "start": 543.16, "end": 546.04, "text": " of these, and this will be even more important", "tokens": [295, 613, 11, 293, 341, 486, 312, 754, 544, 1021], "temperature": 0.0, "avg_logprob": -0.27919779068384415, "compression_ratio": 1.4064171122994653, "no_speech_prob": 7.527218258474022e-06}, {"id": 116, "seek": 53868, "start": 546.04, "end": 549.88, "text": " in the next two notebooks as well.", "tokens": [294, 264, 958, 732, 43782, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.27919779068384415, "compression_ratio": 1.4064171122994653, "no_speech_prob": 7.527218258474022e-06}, {"id": 117, "seek": 53868, "start": 549.88, "end": 555.8399999999999, "text": " And actually, just to check, have you all used PyTorch previously?", "tokens": [400, 767, 11, 445, 281, 1520, 11, 362, 291, 439, 1143, 9953, 51, 284, 339, 8046, 30], "temperature": 0.0, "avg_logprob": -0.27919779068384415, "compression_ratio": 1.4064171122994653, "no_speech_prob": 7.527218258474022e-06}, {"id": 118, "seek": 53868, "start": 555.8399999999999, "end": 558.8, "text": " Noting, yes, okay, cool.", "tokens": [1726, 278, 11, 2086, 11, 1392, 11, 1627, 13], "temperature": 0.0, "avg_logprob": -0.27919779068384415, "compression_ratio": 1.4064171122994653, "no_speech_prob": 7.527218258474022e-06}, {"id": 119, "seek": 53868, "start": 558.8, "end": 561.8, "text": " So let me...", "tokens": [407, 718, 385, 485], "temperature": 0.0, "avg_logprob": -0.27919779068384415, "compression_ratio": 1.4064171122994653, "no_speech_prob": 7.527218258474022e-06}, {"id": 120, "seek": 56180, "start": 561.8, "end": 571.8, "text": " So language models can use a lot of GPU, so you may need", "tokens": [407, 2856, 5245, 393, 764, 257, 688, 295, 18407, 11, 370, 291, 815, 643], "temperature": 0.0, "avg_logprob": -0.16600369753902905, "compression_ratio": 1.4948453608247423, "no_speech_prob": 3.8821839552838355e-05}, {"id": 121, "seek": 56180, "start": 571.8, "end": 574.9599999999999, "text": " to decrease the batch size here.", "tokens": [281, 11514, 264, 15245, 2744, 510, 13], "temperature": 0.0, "avg_logprob": -0.16600369753902905, "compression_ratio": 1.4948453608247423, "no_speech_prob": 3.8821839552838355e-05}, {"id": 122, "seek": 56180, "start": 574.9599999999999, "end": 580.4399999999999, "text": " I noticed the significance of GPUs is, so these are graphics processing units.", "tokens": [286, 5694, 264, 17687, 295, 18407, 82, 307, 11, 370, 613, 366, 11837, 9007, 6815, 13], "temperature": 0.0, "avg_logprob": -0.16600369753902905, "compression_ratio": 1.4948453608247423, "no_speech_prob": 3.8821839552838355e-05}, {"id": 123, "seek": 56180, "start": 580.4399999999999, "end": 584.4799999999999, "text": " They're what's used in video games, and the kind of video", "tokens": [814, 434, 437, 311, 1143, 294, 960, 2813, 11, 293, 264, 733, 295, 960], "temperature": 0.0, "avg_logprob": -0.16600369753902905, "compression_ratio": 1.4948453608247423, "no_speech_prob": 3.8821839552838355e-05}, {"id": 124, "seek": 56180, "start": 584.4799999999999, "end": 588.0, "text": " and computer game industry really pushed the technology on them", "tokens": [293, 3820, 1216, 3518, 534, 9152, 264, 2899, 322, 552], "temperature": 0.0, "avg_logprob": -0.16600369753902905, "compression_ratio": 1.4948453608247423, "no_speech_prob": 3.8821839552838355e-05}, {"id": 125, "seek": 58800, "start": 588.0, "end": 592.8, "text": " over the past few decades, and it's the same type of calculation.", "tokens": [670, 264, 1791, 1326, 7878, 11, 293, 309, 311, 264, 912, 2010, 295, 17108, 13], "temperature": 0.0, "avg_logprob": -0.10464897155761718, "compression_ratio": 1.7638376383763839, "no_speech_prob": 2.0460369341890328e-05}, {"id": 126, "seek": 58800, "start": 592.8, "end": 597.56, "text": " So they are really great at kind of performing these matrix calculations,", "tokens": [407, 436, 366, 534, 869, 412, 733, 295, 10205, 613, 8141, 20448, 11], "temperature": 0.0, "avg_logprob": -0.10464897155761718, "compression_ratio": 1.7638376383763839, "no_speech_prob": 2.0460369341890328e-05}, {"id": 127, "seek": 58800, "start": 597.56, "end": 602.32, "text": " which is exactly what you need to do when you're training a neural net.", "tokens": [597, 307, 2293, 437, 291, 643, 281, 360, 562, 291, 434, 3097, 257, 18161, 2533, 13], "temperature": 0.0, "avg_logprob": -0.10464897155761718, "compression_ratio": 1.7638376383763839, "no_speech_prob": 2.0460369341890328e-05}, {"id": 128, "seek": 58800, "start": 602.32, "end": 605.24, "text": " And so it's really great that we can kind of take advantage", "tokens": [400, 370, 309, 311, 534, 869, 300, 321, 393, 733, 295, 747, 5002], "temperature": 0.0, "avg_logprob": -0.10464897155761718, "compression_ratio": 1.7638376383763839, "no_speech_prob": 2.0460369341890328e-05}, {"id": 129, "seek": 58800, "start": 605.24, "end": 609.64, "text": " of this technology that's made neural nets so much faster to train.", "tokens": [295, 341, 2899, 300, 311, 1027, 18161, 36170, 370, 709, 4663, 281, 3847, 13], "temperature": 0.0, "avg_logprob": -0.10464897155761718, "compression_ratio": 1.7638376383763839, "no_speech_prob": 2.0460369341890328e-05}, {"id": 130, "seek": 58800, "start": 609.64, "end": 612.76, "text": " And this is part of what's made neural networks feasible now in a way", "tokens": [400, 341, 307, 644, 295, 437, 311, 1027, 18161, 9590, 26648, 586, 294, 257, 636], "temperature": 0.0, "avg_logprob": -0.10464897155761718, "compression_ratio": 1.7638376383763839, "no_speech_prob": 2.0460369341890328e-05}, {"id": 131, "seek": 58800, "start": 612.76, "end": 616.96, "text": " that they weren't, say, in the 90s when people were looking at them,", "tokens": [300, 436, 4999, 380, 11, 584, 11, 294, 264, 4289, 82, 562, 561, 645, 1237, 412, 552, 11], "temperature": 0.0, "avg_logprob": -0.10464897155761718, "compression_ratio": 1.7638376383763839, "no_speech_prob": 2.0460369341890328e-05}, {"id": 132, "seek": 61696, "start": 616.96, "end": 620.24, "text": " is being able to use GPUs.", "tokens": [307, 885, 1075, 281, 764, 18407, 82, 13], "temperature": 0.0, "avg_logprob": -0.22494520459856307, "compression_ratio": 1.338235294117647, "no_speech_prob": 1.8627624740474857e-05}, {"id": 133, "seek": 61696, "start": 622.0, "end": 628.8000000000001, "text": " So this is the same data set as before of IMDB movie samples.", "tokens": [407, 341, 307, 264, 912, 1412, 992, 382, 949, 295, 21463, 27735, 3169, 10938, 13], "temperature": 0.0, "avg_logprob": -0.22494520459856307, "compression_ratio": 1.338235294117647, "no_speech_prob": 1.8627624740474857e-05}, {"id": 134, "seek": 61696, "start": 628.8000000000001, "end": 632.36, "text": " And once again, we're going to start on a sample of our data set", "tokens": [400, 1564, 797, 11, 321, 434, 516, 281, 722, 322, 257, 6889, 295, 527, 1412, 992], "temperature": 0.0, "avg_logprob": -0.22494520459856307, "compression_ratio": 1.338235294117647, "no_speech_prob": 1.8627624740474857e-05}, {"id": 135, "seek": 63236, "start": 632.36, "end": 648.04, "text": " to try to get things set up.", "tokens": [281, 853, 281, 483, 721, 992, 493, 13], "temperature": 0.0, "avg_logprob": -0.15029247240586716, "compression_ratio": 1.3076923076923077, "no_speech_prob": 1.3210117685957812e-05}, {"id": 136, "seek": 63236, "start": 648.04, "end": 652.0, "text": " And so we've seen this before, but what's going on behind the scenes", "tokens": [400, 370, 321, 600, 1612, 341, 949, 11, 457, 437, 311, 516, 322, 2261, 264, 8026], "temperature": 0.0, "avg_logprob": -0.15029247240586716, "compression_ratio": 1.3076923076923077, "no_speech_prob": 1.3210117685957812e-05}, {"id": 137, "seek": 63236, "start": 652.0, "end": 656.6800000000001, "text": " is tokenization, so the text are having to be processed", "tokens": [307, 14862, 2144, 11, 370, 264, 2487, 366, 1419, 281, 312, 18846], "temperature": 0.0, "avg_logprob": -0.15029247240586716, "compression_ratio": 1.3076923076923077, "no_speech_prob": 1.3210117685957812e-05}, {"id": 138, "seek": 65668, "start": 656.68, "end": 664.8399999999999, "text": " to split the raw sentences into words or more exactly tokens.", "tokens": [281, 7472, 264, 8936, 16579, 666, 2283, 420, 544, 2293, 22667, 13], "temperature": 0.0, "avg_logprob": -0.1512876101902553, "compression_ratio": 1.4308510638297873, "no_speech_prob": 2.976717041747179e-05}, {"id": 139, "seek": 65668, "start": 664.8399999999999, "end": 670.92, "text": " Numericalization, which is this process of mapping the tokens to IDs", "tokens": [426, 15583, 804, 2144, 11, 597, 307, 341, 1399, 295, 18350, 264, 22667, 281, 48212], "temperature": 0.0, "avg_logprob": -0.1512876101902553, "compression_ratio": 1.4308510638297873, "no_speech_prob": 2.976717041747179e-05}, {"id": 140, "seek": 65668, "start": 670.92, "end": 673.64, "text": " and keeping track of that.", "tokens": [293, 5145, 2837, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.1512876101902553, "compression_ratio": 1.4308510638297873, "no_speech_prob": 2.976717041747179e-05}, {"id": 141, "seek": 65668, "start": 673.64, "end": 679.16, "text": " Here we're going to have a maximum vocabulary size of 60,000.", "tokens": [1692, 321, 434, 516, 281, 362, 257, 6674, 19864, 2744, 295, 4060, 11, 1360, 13], "temperature": 0.0, "avg_logprob": -0.1512876101902553, "compression_ratio": 1.4308510638297873, "no_speech_prob": 2.976717041747179e-05}, {"id": 142, "seek": 65668, "start": 679.16, "end": 684.52, "text": " By default, we're using these same special tokens", "tokens": [3146, 7576, 11, 321, 434, 1228, 613, 912, 2121, 22667], "temperature": 0.0, "avg_logprob": -0.1512876101902553, "compression_ratio": 1.4308510638297873, "no_speech_prob": 2.976717041747179e-05}, {"id": 143, "seek": 68452, "start": 684.52, "end": 689.92, "text": " that we used previously, xxunk is unknown, xxbeginning of string,", "tokens": [300, 321, 1143, 8046, 11, 2031, 87, 3197, 307, 9841, 11, 2031, 87, 650, 1494, 773, 295, 6798, 11], "temperature": 0.0, "avg_logprob": -0.2208536946496298, "compression_ratio": 1.5078534031413613, "no_speech_prob": 8.39750009618001e-06}, {"id": 144, "seek": 68452, "start": 689.92, "end": 696.92, "text": " end of string, xxmage for capitalization, and so on.", "tokens": [917, 295, 6798, 11, 2031, 87, 76, 609, 337, 4238, 2144, 11, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.2208536946496298, "compression_ratio": 1.5078534031413613, "no_speech_prob": 8.39750009618001e-06}, {"id": 145, "seek": 68452, "start": 696.92, "end": 704.68, "text": " And we can see same type of reviews we've been looking at before.", "tokens": [400, 321, 393, 536, 912, 2010, 295, 10229, 321, 600, 668, 1237, 412, 949, 13], "temperature": 0.0, "avg_logprob": -0.2208536946496298, "compression_ratio": 1.5078534031413613, "no_speech_prob": 8.39750009618001e-06}, {"id": 146, "seek": 68452, "start": 704.68, "end": 707.88, "text": " If you haven't seen Zombie Bloodbath, you haven't a contest", "tokens": [759, 291, 2378, 380, 1612, 48952, 17428, 65, 998, 11, 291, 2378, 380, 257, 10287], "temperature": 0.0, "avg_logprob": -0.2208536946496298, "compression_ratio": 1.5078534031413613, "no_speech_prob": 8.39750009618001e-06}, {"id": 147, "seek": 70788, "start": 707.88, "end": 714.88, "text": " like Make Your Own Horror Movie in One Day.", "tokens": [411, 4387, 2260, 25964, 42993, 28766, 294, 1485, 5226, 13], "temperature": 0.0, "avg_logprob": -0.1767774694106158, "compression_ratio": 1.5324074074074074, "no_speech_prob": 2.3549609977635555e-05}, {"id": 148, "seek": 70788, "start": 714.88, "end": 719.4399999999999, "text": " And again, what the model is going to be working with is these list of numbers", "tokens": [400, 797, 11, 437, 264, 2316, 307, 516, 281, 312, 1364, 365, 307, 613, 1329, 295, 3547], "temperature": 0.0, "avg_logprob": -0.1767774694106158, "compression_ratio": 1.5324074074074074, "no_speech_prob": 2.3549609977635555e-05}, {"id": 149, "seek": 70788, "start": 719.4399999999999, "end": 723.52, "text": " that represent the reviews.", "tokens": [300, 2906, 264, 10229, 13], "temperature": 0.0, "avg_logprob": -0.1767774694106158, "compression_ratio": 1.5324074074074074, "no_speech_prob": 2.3549609977635555e-05}, {"id": 150, "seek": 70788, "start": 723.52, "end": 726.36, "text": " And then this is just showing, and this is actually what we used previously,", "tokens": [400, 550, 341, 307, 445, 4099, 11, 293, 341, 307, 767, 437, 321, 1143, 8046, 11], "temperature": 0.0, "avg_logprob": -0.1767774694106158, "compression_ratio": 1.5324074074074074, "no_speech_prob": 2.3549609977635555e-05}, {"id": 151, "seek": 70788, "start": 726.36, "end": 731.56, "text": " an alternate way to load our data set with a data block API,", "tokens": [364, 18873, 636, 281, 3677, 527, 1412, 992, 365, 257, 1412, 3461, 9362, 11], "temperature": 0.0, "avg_logprob": -0.1767774694106158, "compression_ratio": 1.5324074074074074, "no_speech_prob": 2.3549609977635555e-05}, {"id": 152, "seek": 70788, "start": 731.56, "end": 735.96, "text": " which here is kind of very human readable.", "tokens": [597, 510, 307, 733, 295, 588, 1952, 49857, 13], "temperature": 0.0, "avg_logprob": -0.1767774694106158, "compression_ratio": 1.5324074074074074, "no_speech_prob": 2.3549609977635555e-05}, {"id": 153, "seek": 73596, "start": 735.96, "end": 742.6800000000001, "text": " We're taking something from a CSV, we're going to split the data frame,", "tokens": [492, 434, 1940, 746, 490, 257, 48814, 11, 321, 434, 516, 281, 7472, 264, 1412, 3920, 11], "temperature": 0.0, "avg_logprob": -0.16132198680530896, "compression_ratio": 1.5972222222222223, "no_speech_prob": 1.8923268726211973e-05}, {"id": 154, "seek": 73596, "start": 742.6800000000001, "end": 757.52, "text": " this is the label we're going to use, and so on.", "tokens": [341, 307, 264, 7645, 321, 434, 516, 281, 764, 11, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.16132198680530896, "compression_ratio": 1.5972222222222223, "no_speech_prob": 1.8923268726211973e-05}, {"id": 155, "seek": 73596, "start": 757.52, "end": 762.44, "text": " So what we're going to do, so we're not going to start with trying to train", "tokens": [407, 437, 321, 434, 516, 281, 360, 11, 370, 321, 434, 406, 516, 281, 722, 365, 1382, 281, 3847], "temperature": 0.0, "avg_logprob": -0.16132198680530896, "compression_ratio": 1.5972222222222223, "no_speech_prob": 1.8923268726211973e-05}, {"id": 156, "seek": 73596, "start": 762.44, "end": 765.12, "text": " from scratch to classify reviews.", "tokens": [490, 8459, 281, 33872, 10229, 13], "temperature": 0.0, "avg_logprob": -0.16132198680530896, "compression_ratio": 1.5972222222222223, "no_speech_prob": 1.8923268726211973e-05}, {"id": 157, "seek": 76512, "start": 765.12, "end": 768.2, "text": " We're going to use a model that was pre-trained.", "tokens": [492, 434, 516, 281, 764, 257, 2316, 300, 390, 659, 12, 17227, 2001, 13], "temperature": 0.0, "avg_logprob": -0.1997233187214712, "compression_ratio": 1.614213197969543, "no_speech_prob": 8.091810013866052e-05}, {"id": 158, "seek": 76512, "start": 768.2, "end": 773.96, "text": " And so here we're going to start with a model that was pre-trained on", "tokens": [400, 370, 510, 321, 434, 516, 281, 722, 365, 257, 2316, 300, 390, 659, 12, 17227, 2001, 322], "temperature": 0.0, "avg_logprob": -0.1997233187214712, "compression_ratio": 1.614213197969543, "no_speech_prob": 8.091810013866052e-05}, {"id": 159, "seek": 76512, "start": 773.96, "end": 778.4, "text": " Wikitext 103, which is a clean subset of Wikipedia.", "tokens": [23377, 642, 734, 48784, 11, 597, 307, 257, 2541, 25993, 295, 28999, 13], "temperature": 0.0, "avg_logprob": -0.1997233187214712, "compression_ratio": 1.614213197969543, "no_speech_prob": 8.091810013866052e-05}, {"id": 160, "seek": 76512, "start": 778.4, "end": 781.8, "text": " So it is not all of Wikipedia, this is something that Stephen Marity", "tokens": [407, 309, 307, 406, 439, 295, 28999, 11, 341, 307, 746, 300, 13391, 2039, 507], "temperature": 0.0, "avg_logprob": -0.1997233187214712, "compression_ratio": 1.614213197969543, "no_speech_prob": 8.091810013866052e-05}, {"id": 161, "seek": 76512, "start": 781.8, "end": 790.4, "text": " put together, he was previously at Metamind, which was acquired by Salesforce.", "tokens": [829, 1214, 11, 415, 390, 8046, 412, 6377, 335, 471, 11, 597, 390, 17554, 538, 40398, 13], "temperature": 0.0, "avg_logprob": -0.1997233187214712, "compression_ratio": 1.614213197969543, "no_speech_prob": 8.091810013866052e-05}, {"id": 162, "seek": 79040, "start": 790.4, "end": 795.84, "text": " And so the Wikitext language modeling data set is a collection of over 100", "tokens": [400, 370, 264, 23377, 642, 734, 2856, 15983, 1412, 992, 307, 257, 5765, 295, 670, 2319], "temperature": 0.0, "avg_logprob": -0.20443586123886928, "compression_ratio": 1.6009174311926606, "no_speech_prob": 7.64584638091037e-06}, {"id": 163, "seek": 79040, "start": 795.84, "end": 801.04, "text": " million tokens extracted from the set of verified, good, and featured articles", "tokens": [2459, 22667, 34086, 490, 264, 992, 295, 31197, 11, 665, 11, 293, 13822, 11290], "temperature": 0.0, "avg_logprob": -0.20443586123886928, "compression_ratio": 1.6009174311926606, "no_speech_prob": 7.64584638091037e-06}, {"id": 164, "seek": 79040, "start": 801.04, "end": 803.72, "text": " on Wikipedia.", "tokens": [322, 28999, 13], "temperature": 0.0, "avg_logprob": -0.20443586123886928, "compression_ratio": 1.6009174311926606, "no_speech_prob": 7.64584638091037e-06}, {"id": 165, "seek": 79040, "start": 803.72, "end": 808.48, "text": " Compared to Penn Treebank, which is another famous data set in NLP,", "tokens": [30539, 281, 12667, 22291, 25423, 11, 597, 307, 1071, 4618, 1412, 992, 294, 426, 45196, 11], "temperature": 0.0, "avg_logprob": -0.20443586123886928, "compression_ratio": 1.6009174311926606, "no_speech_prob": 7.64584638091037e-06}, {"id": 166, "seek": 79040, "start": 808.48, "end": 813.64, "text": " it's over two times larger, oh sorry, Wikitext 2 is over two times larger,", "tokens": [309, 311, 670, 732, 1413, 4833, 11, 1954, 2597, 11, 23377, 642, 734, 568, 307, 670, 732, 1413, 4833, 11], "temperature": 0.0, "avg_logprob": -0.20443586123886928, "compression_ratio": 1.6009174311926606, "no_speech_prob": 7.64584638091037e-06}, {"id": 167, "seek": 79040, "start": 813.64, "end": 818.24, "text": " Wikitext 103 is over 110 times larger.", "tokens": [23377, 642, 734, 48784, 307, 670, 20154, 1413, 4833, 13], "temperature": 0.0, "avg_logprob": -0.20443586123886928, "compression_ratio": 1.6009174311926606, "no_speech_prob": 7.64584638091037e-06}, {"id": 168, "seek": 81824, "start": 818.24, "end": 823.44, "text": " Also, Penn Treebank had removed punctuation and numbers,", "tokens": [2743, 11, 12667, 22291, 25423, 632, 7261, 27006, 16073, 293, 3547, 11], "temperature": 0.0, "avg_logprob": -0.12548749851730634, "compression_ratio": 1.6015325670498084, "no_speech_prob": 4.936547156830784e-06}, {"id": 169, "seek": 81824, "start": 823.44, "end": 825.44, "text": " which Wikitext keeps.", "tokens": [597, 23377, 642, 734, 5965, 13], "temperature": 0.0, "avg_logprob": -0.12548749851730634, "compression_ratio": 1.6015325670498084, "no_speech_prob": 4.936547156830784e-06}, {"id": 170, "seek": 81824, "start": 825.44, "end": 828.52, "text": " And this is in keeping with kind of some of the trends we talked about back in", "tokens": [400, 341, 307, 294, 5145, 365, 733, 295, 512, 295, 264, 13892, 321, 2825, 466, 646, 294], "temperature": 0.0, "avg_logprob": -0.12548749851730634, "compression_ratio": 1.6015325670498084, "no_speech_prob": 4.936547156830784e-06}, {"id": 171, "seek": 81824, "start": 828.52, "end": 833.4, "text": " lesson one of how we're wanting to throw away less and less information in", "tokens": [6898, 472, 295, 577, 321, 434, 7935, 281, 3507, 1314, 1570, 293, 1570, 1589, 294], "temperature": 0.0, "avg_logprob": -0.12548749851730634, "compression_ratio": 1.6015325670498084, "no_speech_prob": 4.936547156830784e-06}, {"id": 172, "seek": 81824, "start": 833.4, "end": 839.44, "text": " pre-processing with neural nets, because we have a complex model that can", "tokens": [659, 12, 41075, 278, 365, 18161, 36170, 11, 570, 321, 362, 257, 3997, 2316, 300, 393], "temperature": 0.0, "avg_logprob": -0.12548749851730634, "compression_ratio": 1.6015325670498084, "no_speech_prob": 4.936547156830784e-06}, {"id": 173, "seek": 81824, "start": 839.44, "end": 842.28, "text": " handle the complexity of our data.", "tokens": [4813, 264, 14024, 295, 527, 1412, 13], "temperature": 0.0, "avg_logprob": -0.12548749851730634, "compression_ratio": 1.6015325670498084, "no_speech_prob": 4.936547156830784e-06}, {"id": 174, "seek": 81824, "start": 842.28, "end": 846.12, "text": " We don't need to do as much cleaning as was done kind of in previous decades", "tokens": [492, 500, 380, 643, 281, 360, 382, 709, 8924, 382, 390, 1096, 733, 295, 294, 3894, 7878], "temperature": 0.0, "avg_logprob": -0.12548749851730634, "compression_ratio": 1.6015325670498084, "no_speech_prob": 4.936547156830784e-06}, {"id": 175, "seek": 84612, "start": 846.12, "end": 869.4, "text": " with NLP.", "tokens": [365, 426, 45196, 13], "temperature": 0.0, "avg_logprob": -0.1711305300394694, "compression_ratio": 1.037037037037037, "no_speech_prob": 3.48036119248718e-05}, {"id": 176, "seek": 84612, "start": 869.4, "end": 873.0, "text": " And so we're going to, this is just saying it's shuffling the order of the", "tokens": [400, 370, 321, 434, 516, 281, 11, 341, 307, 445, 1566, 309, 311, 402, 1245, 1688, 264, 1668, 295, 264], "temperature": 0.0, "avg_logprob": -0.1711305300394694, "compression_ratio": 1.037037037037037, "no_speech_prob": 3.48036119248718e-05}, {"id": 177, "seek": 87300, "start": 873.0, "end": 880.64, "text": " reviews kind of each time as we put them into the model so that it's not just,", "tokens": [10229, 733, 295, 1184, 565, 382, 321, 829, 552, 666, 264, 2316, 370, 300, 309, 311, 406, 445, 11], "temperature": 0.0, "avg_logprob": -0.13193497148532313, "compression_ratio": 1.662280701754386, "no_speech_prob": 2.796704757201951e-05}, {"id": 178, "seek": 87300, "start": 880.64, "end": 884.48, "text": " we don't want the model kind of learning the pattern of how the reviews are,", "tokens": [321, 500, 380, 528, 264, 2316, 733, 295, 2539, 264, 5102, 295, 577, 264, 10229, 366, 11], "temperature": 0.0, "avg_logprob": -0.13193497148532313, "compression_ratio": 1.662280701754386, "no_speech_prob": 2.796704757201951e-05}, {"id": 179, "seek": 87300, "start": 884.48, "end": 887.88, "text": " since that's not significant.", "tokens": [1670, 300, 311, 406, 4776, 13], "temperature": 0.0, "avg_logprob": -0.13193497148532313, "compression_ratio": 1.662280701754386, "no_speech_prob": 2.796704757201951e-05}, {"id": 180, "seek": 87300, "start": 887.88, "end": 892.92, "text": " Here we can look at what is, so a batch is kind of the group of reviews that", "tokens": [1692, 321, 393, 574, 412, 437, 307, 11, 370, 257, 15245, 307, 733, 295, 264, 1594, 295, 10229, 300], "temperature": 0.0, "avg_logprob": -0.13193497148532313, "compression_ratio": 1.662280701754386, "no_speech_prob": 2.796704757201951e-05}, {"id": 181, "seek": 87300, "start": 892.92, "end": 894.92, "text": " you're going to evaluate at a time.", "tokens": [291, 434, 516, 281, 13059, 412, 257, 565, 13], "temperature": 0.0, "avg_logprob": -0.13193497148532313, "compression_ratio": 1.662280701754386, "no_speech_prob": 2.796704757201951e-05}, {"id": 182, "seek": 87300, "start": 894.92, "end": 901.88, "text": " And typically this is on a GPU, a neural network is taking in a batch of inputs,", "tokens": [400, 5850, 341, 307, 322, 257, 18407, 11, 257, 18161, 3209, 307, 1940, 294, 257, 15245, 295, 15743, 11], "temperature": 0.0, "avg_logprob": -0.13193497148532313, "compression_ratio": 1.662280701754386, "no_speech_prob": 2.796704757201951e-05}, {"id": 183, "seek": 90188, "start": 901.88, "end": 905.92, "text": " so that's multiple reviews at one time.", "tokens": [370, 300, 311, 3866, 10229, 412, 472, 565, 13], "temperature": 0.0, "avg_logprob": -0.17386128040070228, "compression_ratio": 1.543778801843318, "no_speech_prob": 4.425370661920169e-06}, {"id": 184, "seek": 90188, "start": 905.92, "end": 909.08, "text": " Here we can look at, see what a batch looks like.", "tokens": [1692, 321, 393, 574, 412, 11, 536, 437, 257, 15245, 1542, 411, 13], "temperature": 0.0, "avg_logprob": -0.17386128040070228, "compression_ratio": 1.543778801843318, "no_speech_prob": 4.425370661920169e-06}, {"id": 185, "seek": 90188, "start": 909.08, "end": 912.68, "text": " In this case we've got five reviews.", "tokens": [682, 341, 1389, 321, 600, 658, 1732, 10229, 13], "temperature": 0.0, "avg_logprob": -0.17386128040070228, "compression_ratio": 1.543778801843318, "no_speech_prob": 4.425370661920169e-06}, {"id": 186, "seek": 90188, "start": 912.68, "end": 917.56, "text": " We can see their index and the tokenized version of them.", "tokens": [492, 393, 536, 641, 8186, 293, 264, 14862, 1602, 3037, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.17386128040070228, "compression_ratio": 1.543778801843318, "no_speech_prob": 4.425370661920169e-06}, {"id": 187, "seek": 90188, "start": 917.56, "end": 923.52, "text": " Bizarre condition that characters had to deal like rats in a labyrinth.", "tokens": [363, 9736, 265, 4188, 300, 4342, 632, 281, 2028, 411, 25691, 294, 257, 287, 46800, 392, 13], "temperature": 0.0, "avg_logprob": -0.17386128040070228, "compression_ratio": 1.543778801843318, "no_speech_prob": 4.425370661920169e-06}, {"id": 188, "seek": 90188, "start": 923.52, "end": 930.56, "text": " And see that this is kind of what we expect of, given that we know we're going", "tokens": [400, 536, 300, 341, 307, 733, 295, 437, 321, 2066, 295, 11, 2212, 300, 321, 458, 321, 434, 516], "temperature": 0.0, "avg_logprob": -0.17386128040070228, "compression_ratio": 1.543778801843318, "no_speech_prob": 4.425370661920169e-06}, {"id": 189, "seek": 93056, "start": 930.56, "end": 933.2399999999999, "text": " to be looking at these movie reviews.", "tokens": [281, 312, 1237, 412, 613, 3169, 10229, 13], "temperature": 0.0, "avg_logprob": -0.2199051089403106, "compression_ratio": 1.5856353591160222, "no_speech_prob": 8.664276720082853e-06}, {"id": 190, "seek": 93056, "start": 940.4399999999999, "end": 946.7199999999999, "text": " So that was kind of this, we're processing the data, our movie reviews,", "tokens": [407, 300, 390, 733, 295, 341, 11, 321, 434, 9007, 264, 1412, 11, 527, 3169, 10229, 11], "temperature": 0.0, "avg_logprob": -0.2199051089403106, "compression_ratio": 1.5856353591160222, "no_speech_prob": 8.664276720082853e-06}, {"id": 191, "seek": 93056, "start": 946.7199999999999, "end": 948.4399999999999, "text": " kind of, that's one thing we're doing.", "tokens": [733, 295, 11, 300, 311, 472, 551, 321, 434, 884, 13], "temperature": 0.0, "avg_logprob": -0.2199051089403106, "compression_ratio": 1.5856353591160222, "no_speech_prob": 8.664276720082853e-06}, {"id": 192, "seek": 93056, "start": 948.4399999999999, "end": 951.9599999999999, "text": " The other thing we're going to do is getting this pre-trained model on", "tokens": [440, 661, 551, 321, 434, 516, 281, 360, 307, 1242, 341, 659, 12, 17227, 2001, 2316, 322], "temperature": 0.0, "avg_logprob": -0.2199051089403106, "compression_ratio": 1.5856353591160222, "no_speech_prob": 8.664276720082853e-06}, {"id": 193, "seek": 93056, "start": 951.9599999999999, "end": 955.52, "text": " WikiText 103 data.", "tokens": [35892, 50198, 48784, 1412, 13], "temperature": 0.0, "avg_logprob": -0.2199051089403106, "compression_ratio": 1.5856353591160222, "no_speech_prob": 8.664276720082853e-06}, {"id": 194, "seek": 93056, "start": 955.52, "end": 959.64, "text": " And so we'll create this language model learner.", "tokens": [400, 370, 321, 603, 1884, 341, 2856, 2316, 33347, 13], "temperature": 0.0, "avg_logprob": -0.2199051089403106, "compression_ratio": 1.5856353591160222, "no_speech_prob": 8.664276720082853e-06}, {"id": 195, "seek": 95964, "start": 959.64, "end": 964.68, "text": " And a question that always comes up is what if the vocabulary is different?", "tokens": [400, 257, 1168, 300, 1009, 1487, 493, 307, 437, 498, 264, 19864, 307, 819, 30], "temperature": 0.0, "avg_logprob": -0.16808534551549842, "compression_ratio": 1.6954732510288066, "no_speech_prob": 6.540128651977284e-06}, {"id": 196, "seek": 95964, "start": 964.68, "end": 966.08, "text": " And in fact it will be, right?", "tokens": [400, 294, 1186, 309, 486, 312, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.16808534551549842, "compression_ratio": 1.6954732510288066, "no_speech_prob": 6.540128651977284e-06}, {"id": 197, "seek": 95964, "start": 966.08, "end": 971.04, "text": " There are probably words in the Wikipedia data set that are not in our movie", "tokens": [821, 366, 1391, 2283, 294, 264, 28999, 1412, 992, 300, 366, 406, 294, 527, 3169], "temperature": 0.0, "avg_logprob": -0.16808534551549842, "compression_ratio": 1.6954732510288066, "no_speech_prob": 6.540128651977284e-06}, {"id": 198, "seek": 95964, "start": 971.04, "end": 972.48, "text": " reviews and vice versa.", "tokens": [10229, 293, 11964, 25650, 13], "temperature": 0.0, "avg_logprob": -0.16808534551549842, "compression_ratio": 1.6954732510288066, "no_speech_prob": 6.540128651977284e-06}, {"id": 199, "seek": 95964, "start": 972.48, "end": 975.68, "text": " There might be new words in our movie reviews that weren't in this", "tokens": [821, 1062, 312, 777, 2283, 294, 527, 3169, 10229, 300, 4999, 380, 294, 341], "temperature": 0.0, "avg_logprob": -0.16808534551549842, "compression_ratio": 1.6954732510288066, "no_speech_prob": 6.540128651977284e-06}, {"id": 200, "seek": 95964, "start": 975.68, "end": 977.76, "text": " Wikipedia data set.", "tokens": [28999, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.16808534551549842, "compression_ratio": 1.6954732510288066, "no_speech_prob": 6.540128651977284e-06}, {"id": 201, "seek": 95964, "start": 977.76, "end": 983.16, "text": " And so I just wanted to take a look at that.", "tokens": [400, 370, 286, 445, 1415, 281, 747, 257, 574, 412, 300, 13], "temperature": 0.0, "avg_logprob": -0.16808534551549842, "compression_ratio": 1.6954732510288066, "no_speech_prob": 6.540128651977284e-06}, {"id": 202, "seek": 95964, "start": 983.16, "end": 989.6, "text": " So we've got our Wikipedia index to string mapping, which is going to be", "tokens": [407, 321, 600, 658, 527, 28999, 8186, 281, 6798, 18350, 11, 597, 307, 516, 281, 312], "temperature": 0.0, "avg_logprob": -0.16808534551549842, "compression_ratio": 1.6954732510288066, "no_speech_prob": 6.540128651977284e-06}, {"id": 203, "seek": 98960, "start": 989.6, "end": 1001.64, "text": " separate from our, for our movie reviews, which is the data LM vocab is separate.", "tokens": [4994, 490, 527, 11, 337, 527, 3169, 10229, 11, 597, 307, 264, 1412, 46529, 2329, 455, 307, 4994, 13], "temperature": 0.0, "avg_logprob": -0.32161793989293713, "compression_ratio": 1.183673469387755, "no_speech_prob": 3.071642277063802e-05}, {"id": 204, "seek": 98960, "start": 1001.64, "end": 1005.8000000000001, "text": " And so I'm going to compare these.", "tokens": [400, 370, 286, 478, 516, 281, 6794, 613, 13], "temperature": 0.0, "avg_logprob": -0.32161793989293713, "compression_ratio": 1.183673469387755, "no_speech_prob": 3.071642277063802e-05}, {"id": 205, "seek": 100580, "start": 1005.8, "end": 1020.76, "text": " Yeah, so here I see that WikiI2String has 60,000 words in it.", "tokens": [865, 11, 370, 510, 286, 536, 300, 35892, 40, 17, 4520, 2937, 575, 4060, 11, 1360, 2283, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.1947543427750871, "compression_ratio": 1.3788819875776397, "no_speech_prob": 1.0952331649605185e-05}, {"id": 206, "seek": 100580, "start": 1020.76, "end": 1026.24, "text": " Here the vocab.I2String, that's the vocabulary for our movie reviews,", "tokens": [1692, 264, 2329, 455, 13, 40, 17, 4520, 2937, 11, 300, 311, 264, 19864, 337, 527, 3169, 10229, 11], "temperature": 0.0, "avg_logprob": -0.1947543427750871, "compression_ratio": 1.3788819875776397, "no_speech_prob": 1.0952331649605185e-05}, {"id": 207, "seek": 100580, "start": 1026.24, "end": 1028.84, "text": " only has 49,000.", "tokens": [787, 575, 16513, 11, 1360, 13], "temperature": 0.0, "avg_logprob": -0.1947543427750871, "compression_ratio": 1.3788819875776397, "no_speech_prob": 1.0952331649605185e-05}, {"id": 208, "seek": 100580, "start": 1028.84, "end": 1034.52, "text": " And I can go through these and get the set of Wiki words, the set of IMDB", "tokens": [400, 286, 393, 352, 807, 613, 293, 483, 264, 992, 295, 35892, 2283, 11, 264, 992, 295, 21463, 27735], "temperature": 0.0, "avg_logprob": -0.1947543427750871, "compression_ratio": 1.3788819875776397, "no_speech_prob": 1.0952331649605185e-05}, {"id": 209, "seek": 103452, "start": 1034.52, "end": 1036.76, "text": " words, should be running these.", "tokens": [2283, 11, 820, 312, 2614, 613, 13], "temperature": 0.0, "avg_logprob": -0.18885464614696718, "compression_ratio": 1.6551724137931034, "no_speech_prob": 1.7501855836599134e-05}, {"id": 210, "seek": 103452, "start": 1042.6, "end": 1047.36, "text": " And then I can take the set difference to find the words that are on Wikipedia,", "tokens": [400, 550, 286, 393, 747, 264, 992, 2649, 281, 915, 264, 2283, 300, 366, 322, 28999, 11], "temperature": 0.0, "avg_logprob": -0.18885464614696718, "compression_ratio": 1.6551724137931034, "no_speech_prob": 1.7501855836599134e-05}, {"id": 211, "seek": 103452, "start": 1047.36, "end": 1052.84, "text": " in the Wikipedia data set and not IMDB and the IMDB data set, not Wikipedia.", "tokens": [294, 264, 28999, 1412, 992, 293, 406, 21463, 27735, 293, 264, 21463, 27735, 1412, 992, 11, 406, 28999, 13], "temperature": 0.0, "avg_logprob": -0.18885464614696718, "compression_ratio": 1.6551724137931034, "no_speech_prob": 1.7501855836599134e-05}, {"id": 212, "seek": 103452, "start": 1052.84, "end": 1057.68, "text": " And I'm doing this to kind of just as an exploratory analysis, just to see like", "tokens": [400, 286, 478, 884, 341, 281, 733, 295, 445, 382, 364, 24765, 4745, 5215, 11, 445, 281, 536, 411], "temperature": 0.0, "avg_logprob": -0.18885464614696718, "compression_ratio": 1.6551724137931034, "no_speech_prob": 1.7501855836599134e-05}, {"id": 213, "seek": 103452, "start": 1057.68, "end": 1063.48, "text": " what, what are these things that we're looking at and working with?", "tokens": [437, 11, 437, 366, 613, 721, 300, 321, 434, 1237, 412, 293, 1364, 365, 30], "temperature": 0.0, "avg_logprob": -0.18885464614696718, "compression_ratio": 1.6551724137931034, "no_speech_prob": 1.7501855836599134e-05}, {"id": 214, "seek": 106348, "start": 1063.48, "end": 1068.84, "text": " And so then I wanted to get a list of some of these words.", "tokens": [400, 370, 550, 286, 1415, 281, 483, 257, 1329, 295, 512, 295, 613, 2283, 13], "temperature": 0.0, "avg_logprob": -0.13602654651928975, "compression_ratio": 1.504950495049505, "no_speech_prob": 3.3733787859091535e-05}, {"id": 215, "seek": 106348, "start": 1068.84, "end": 1073.76, "text": " And really, I think what we're more interested in is what's, what's an IMDB,", "tokens": [400, 534, 11, 286, 519, 437, 321, 434, 544, 3102, 294, 307, 437, 311, 11, 437, 311, 364, 21463, 27735, 11], "temperature": 0.0, "avg_logprob": -0.13602654651928975, "compression_ratio": 1.504950495049505, "no_speech_prob": 3.3733787859091535e-05}, {"id": 216, "seek": 106348, "start": 1073.76, "end": 1077.52, "text": " but not on our Wikipedia data set.", "tokens": [457, 406, 322, 527, 28999, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.13602654651928975, "compression_ratio": 1.504950495049505, "no_speech_prob": 3.3733787859091535e-05}, {"id": 217, "seek": 106348, "start": 1077.52, "end": 1082.52, "text": " And I found here, I'm just looking at a subset, a few of them.", "tokens": [400, 286, 1352, 510, 11, 286, 478, 445, 1237, 412, 257, 25993, 11, 257, 1326, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.13602654651928975, "compression_ratio": 1.504950495049505, "no_speech_prob": 3.3733787859091535e-05}, {"id": 218, "seek": 106348, "start": 1082.52, "end": 1092.8, "text": " For instance, well, modernization is in Wiki words, but not, not IMDB.", "tokens": [1171, 5197, 11, 731, 11, 4363, 2144, 307, 294, 35892, 2283, 11, 457, 406, 11, 406, 21463, 27735, 13], "temperature": 0.0, "avg_logprob": -0.13602654651928975, "compression_ratio": 1.504950495049505, "no_speech_prob": 3.3733787859091535e-05}, {"id": 219, "seek": 109280, "start": 1092.8, "end": 1096.56, "text": " 30-something is in IMDB and not Wikipedia.", "tokens": [2217, 12, 31681, 307, 294, 21463, 27735, 293, 406, 28999, 13], "temperature": 0.0, "avg_logprob": -0.1330781551676059, "compression_ratio": 1.7733333333333334, "no_speech_prob": 1.4968762116041034e-05}, {"id": 220, "seek": 109280, "start": 1096.56, "end": 1102.44, "text": " And just a key thing to note is how does the model deal with this, that we have", "tokens": [400, 445, 257, 2141, 551, 281, 3637, 307, 577, 775, 264, 2316, 2028, 365, 341, 11, 300, 321, 362], "temperature": 0.0, "avg_logprob": -0.1330781551676059, "compression_ratio": 1.7733333333333334, "no_speech_prob": 1.4968762116041034e-05}, {"id": 221, "seek": 109280, "start": 1102.44, "end": 1107.28, "text": " started with our model that was pre-trained on Wikipedia and now we're applying it to", "tokens": [1409, 365, 527, 2316, 300, 390, 659, 12, 17227, 2001, 322, 28999, 293, 586, 321, 434, 9275, 309, 281], "temperature": 0.0, "avg_logprob": -0.1330781551676059, "compression_ratio": 1.7733333333333334, "no_speech_prob": 1.4968762116041034e-05}, {"id": 222, "seek": 109280, "start": 1107.28, "end": 1109.9199999999998, "text": " IMDB, which has some new words.", "tokens": [21463, 27735, 11, 597, 575, 512, 777, 2283, 13], "temperature": 0.0, "avg_logprob": -0.1330781551676059, "compression_ratio": 1.7733333333333334, "no_speech_prob": 1.4968762116041034e-05}, {"id": 223, "seek": 109280, "start": 1109.9199999999998, "end": 1115.84, "text": " And we're just going to initialize the words that weren't, weren't present when", "tokens": [400, 321, 434, 445, 516, 281, 5883, 1125, 264, 2283, 300, 4999, 380, 11, 4999, 380, 1974, 562], "temperature": 0.0, "avg_logprob": -0.1330781551676059, "compression_ratio": 1.7733333333333334, "no_speech_prob": 1.4968762116041034e-05}, {"id": 224, "seek": 109280, "start": 1115.84, "end": 1120.32, "text": " we, when we pre-trained with Wikipedia, initialize them all to the same random", "tokens": [321, 11, 562, 321, 659, 12, 17227, 2001, 365, 28999, 11, 5883, 1125, 552, 439, 281, 264, 912, 4974], "temperature": 0.0, "avg_logprob": -0.1330781551676059, "compression_ratio": 1.7733333333333334, "no_speech_prob": 1.4968762116041034e-05}, {"id": 225, "seek": 112032, "start": 1120.32, "end": 1128.32, "text": " thing, and then we'll be kind of updating all of these, these word embeddings to,", "tokens": [551, 11, 293, 550, 321, 603, 312, 733, 295, 25113, 439, 295, 613, 11, 613, 1349, 12240, 29432, 281, 11], "temperature": 0.0, "avg_logprob": -0.13946166595855317, "compression_ratio": 1.5449101796407185, "no_speech_prob": 9.368211976834573e-06}, {"id": 226, "seek": 112032, "start": 1128.32, "end": 1136.84, "text": " as we train it on our IMDB data set.", "tokens": [382, 321, 3847, 309, 322, 527, 21463, 27735, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.13946166595855317, "compression_ratio": 1.5449101796407185, "no_speech_prob": 9.368211976834573e-06}, {"id": 227, "seek": 112032, "start": 1136.84, "end": 1144.56, "text": " And so here, here, so the, the encoder, that's kind of just the first layer.", "tokens": [400, 370, 510, 11, 510, 11, 370, 264, 11, 264, 2058, 19866, 11, 300, 311, 733, 295, 445, 264, 700, 4583, 13], "temperature": 0.0, "avg_logprob": -0.13946166595855317, "compression_ratio": 1.5449101796407185, "no_speech_prob": 9.368211976834573e-06}, {"id": 228, "seek": 112032, "start": 1144.56, "end": 1149.2, "text": " And did you, you covered word embeddings twice in the program?", "tokens": [400, 630, 291, 11, 291, 5343, 1349, 12240, 29432, 6091, 294, 264, 1461, 30], "temperature": 0.0, "avg_logprob": -0.13946166595855317, "compression_ratio": 1.5449101796407185, "no_speech_prob": 9.368211976834573e-06}, {"id": 229, "seek": 114920, "start": 1149.2, "end": 1154.64, "text": " Okay, so here we've got these word embeddings and we can see what they look", "tokens": [1033, 11, 370, 510, 321, 600, 658, 613, 1349, 12240, 29432, 293, 321, 393, 536, 437, 436, 574], "temperature": 0.0, "avg_logprob": -0.1629571487654501, "compression_ratio": 1.4698795180722892, "no_speech_prob": 3.555825969669968e-06}, {"id": 230, "seek": 114920, "start": 1154.64, "end": 1159.2, "text": " like, and I'm just showing that 30-something and link later both were not", "tokens": [411, 11, 293, 286, 478, 445, 4099, 300, 2217, 12, 31681, 293, 2113, 1780, 1293, 645, 406], "temperature": 0.0, "avg_logprob": -0.1629571487654501, "compression_ratio": 1.4698795180722892, "no_speech_prob": 3.555825969669968e-06}, {"id": 231, "seek": 114920, "start": 1159.2, "end": 1163.4, "text": " in the Wikipedia data set, and so they've been initialized to the same, the same", "tokens": [294, 264, 28999, 1412, 992, 11, 293, 370, 436, 600, 668, 5883, 1602, 281, 264, 912, 11, 264, 912], "temperature": 0.0, "avg_logprob": -0.1629571487654501, "compression_ratio": 1.4698795180722892, "no_speech_prob": 3.555825969669968e-06}, {"id": 232, "seek": 114920, "start": 1163.4, "end": 1175.0, "text": " random thing.", "tokens": [4974, 551, 13], "temperature": 0.0, "avg_logprob": -0.1629571487654501, "compression_ratio": 1.4698795180722892, "no_speech_prob": 3.555825969669968e-06}, {"id": 233, "seek": 117500, "start": 1175.0, "end": 1182.04, "text": " So what, what we'll do first is generating some fake, fake movie reviews", "tokens": [407, 437, 11, 437, 321, 603, 360, 700, 307, 17746, 512, 7592, 11, 7592, 3169, 10229], "temperature": 0.0, "avg_logprob": -0.16065559387207032, "compression_ratio": 1.5857142857142856, "no_speech_prob": 2.0461100575630553e-05}, {"id": 234, "seek": 117500, "start": 1182.04, "end": 1185.96, "text": " using our WikiText 103 model.", "tokens": [1228, 527, 35892, 50198, 48784, 2316, 13], "temperature": 0.0, "avg_logprob": -0.16065559387207032, "compression_ratio": 1.5857142857142856, "no_speech_prob": 2.0461100575630553e-05}, {"id": 235, "seek": 117500, "start": 1185.96, "end": 1190.12, "text": " And I should, well no, I'll talk about that later.", "tokens": [400, 286, 820, 11, 731, 572, 11, 286, 603, 751, 466, 300, 1780, 13], "temperature": 0.0, "avg_logprob": -0.16065559387207032, "compression_ratio": 1.5857142857142856, "no_speech_prob": 2.0461100575630553e-05}, {"id": 236, "seek": 117500, "start": 1190.12, "end": 1195.72, "text": " So here, what you can do to generate text with a language model is give it some", "tokens": [407, 510, 11, 437, 291, 393, 360, 281, 8460, 2487, 365, 257, 2856, 2316, 307, 976, 309, 512], "temperature": 0.0, "avg_logprob": -0.16065559387207032, "compression_ratio": 1.5857142857142856, "no_speech_prob": 2.0461100575630553e-05}, {"id": 237, "seek": 117500, "start": 1195.72, "end": 1201.04, "text": " text you want to start with, and then it'll keep predicting what's next to", "tokens": [2487, 291, 528, 281, 722, 365, 11, 293, 550, 309, 603, 1066, 32884, 437, 311, 958, 281], "temperature": 0.0, "avg_logprob": -0.16065559387207032, "compression_ratio": 1.5857142857142856, "no_speech_prob": 2.0461100575630553e-05}, {"id": 238, "seek": 117500, "start": 1201.04, "end": 1203.36, "text": " generate some sentences.", "tokens": [8460, 512, 16579, 13], "temperature": 0.0, "avg_logprob": -0.16065559387207032, "compression_ratio": 1.5857142857142856, "no_speech_prob": 2.0461100575630553e-05}, {"id": 239, "seek": 120336, "start": 1203.36, "end": 1207.28, "text": " And so here I was, actually I was trying to answer the question of what color is", "tokens": [400, 370, 510, 286, 390, 11, 767, 286, 390, 1382, 281, 1867, 264, 1168, 295, 437, 2017, 307], "temperature": 0.0, "avg_logprob": -0.13423800468444824, "compression_ratio": 1.6897810218978102, "no_speech_prob": 1.544494807603769e-05}, {"id": 240, "seek": 120336, "start": 1207.28, "end": 1212.7199999999998, "text": " the sky, and so I tried the color of the sky is the blue mark, Arabic white, the", "tokens": [264, 5443, 11, 293, 370, 286, 3031, 264, 2017, 295, 264, 5443, 307, 264, 3344, 1491, 11, 19938, 2418, 11, 264], "temperature": 0.0, "avg_logprob": -0.13423800468444824, "compression_ratio": 1.6897810218978102, "no_speech_prob": 1.544494807603769e-05}, {"id": 241, "seek": 120336, "start": 1212.7199999999998, "end": 1216.76, "text": " color is used by the Greek Orthodox Church, the Roman Catholic Church, and", "tokens": [2017, 307, 1143, 538, 264, 10281, 32833, 7882, 11, 264, 8566, 11981, 7882, 11, 293], "temperature": 0.0, "avg_logprob": -0.13423800468444824, "compression_ratio": 1.6897810218978102, "no_speech_prob": 1.544494807603769e-05}, {"id": 242, "seek": 120336, "start": 1216.76, "end": 1218.6, "text": " Daily Express.", "tokens": [19685, 20212, 13], "temperature": 0.0, "avg_logprob": -0.13423800468444824, "compression_ratio": 1.6897810218978102, "no_speech_prob": 1.544494807603769e-05}, {"id": 243, "seek": 120336, "start": 1218.6, "end": 1224.08, "text": " And so you can see it is talking about colors, it's not, say if you were using", "tokens": [400, 370, 291, 393, 536, 309, 307, 1417, 466, 4577, 11, 309, 311, 406, 11, 584, 498, 291, 645, 1228], "temperature": 0.0, "avg_logprob": -0.13423800468444824, "compression_ratio": 1.6897810218978102, "no_speech_prob": 1.544494807603769e-05}, {"id": 244, "seek": 120336, "start": 1224.08, "end": 1227.3999999999999, "text": " this as a chat bot, it's not going to be very great.", "tokens": [341, 382, 257, 5081, 10592, 11, 309, 311, 406, 516, 281, 312, 588, 869, 13], "temperature": 0.0, "avg_logprob": -0.13423800468444824, "compression_ratio": 1.6897810218978102, "no_speech_prob": 1.544494807603769e-05}, {"id": 245, "seek": 120336, "start": 1227.3999999999999, "end": 1230.8, "text": " We also, we haven't trained very long yet, and we'll try this again as we train", "tokens": [492, 611, 11, 321, 2378, 380, 8895, 588, 938, 1939, 11, 293, 321, 603, 853, 341, 797, 382, 321, 3847], "temperature": 0.0, "avg_logprob": -0.13423800468444824, "compression_ratio": 1.6897810218978102, "no_speech_prob": 1.544494807603769e-05}, {"id": 246, "seek": 123080, "start": 1230.8, "end": 1235.08, "text": " the model longer.", "tokens": [264, 2316, 2854, 13], "temperature": 0.0, "avg_logprob": -0.16618956119642345, "compression_ratio": 1.5691699604743083, "no_speech_prob": 1.568787956784945e-05}, {"id": 247, "seek": 123080, "start": 1235.08, "end": 1239.36, "text": " Here I started kind of trying to make up a movie review.", "tokens": [1692, 286, 1409, 733, 295, 1382, 281, 652, 493, 257, 3169, 3131, 13], "temperature": 0.0, "avg_logprob": -0.16618956119642345, "compression_ratio": 1.5691699604743083, "no_speech_prob": 1.568787956784945e-05}, {"id": 248, "seek": 123080, "start": 1239.36, "end": 1242.72, "text": " I hated this movie and the film The Royal Stranger.", "tokens": [286, 17398, 341, 3169, 293, 264, 2007, 440, 12717, 8251, 3176, 13], "temperature": 0.0, "avg_logprob": -0.16618956119642345, "compression_ratio": 1.5691699604743083, "no_speech_prob": 1.568787956784945e-05}, {"id": 249, "seek": 123080, "start": 1242.72, "end": 1246.6, "text": " The film is set in the 1950s when World War II broke.", "tokens": [440, 2007, 307, 992, 294, 264, 18141, 82, 562, 3937, 3630, 6351, 6902, 13], "temperature": 0.0, "avg_logprob": -0.16618956119642345, "compression_ratio": 1.5691699604743083, "no_speech_prob": 1.568787956784945e-05}, {"id": 250, "seek": 123080, "start": 1246.6, "end": 1249.84, "text": " I hated this movie, but after it was a critical success, he made a few", "tokens": [286, 17398, 341, 3169, 11, 457, 934, 309, 390, 257, 4924, 2245, 11, 415, 1027, 257, 1326], "temperature": 0.0, "avg_logprob": -0.16618956119642345, "compression_ratio": 1.5691699604743083, "no_speech_prob": 1.568787956784945e-05}, {"id": 251, "seek": 123080, "start": 1249.84, "end": 1252.84, "text": " unsuccessful attempts to re-film the film.", "tokens": [46258, 15257, 281, 319, 12, 37023, 264, 2007, 13], "temperature": 0.0, "avg_logprob": -0.16618956119642345, "compression_ratio": 1.5691699604743083, "no_speech_prob": 1.568787956784945e-05}, {"id": 252, "seek": 123080, "start": 1252.84, "end": 1255.2, "text": " He died in 1985, aged 94.", "tokens": [634, 4539, 294, 28962, 11, 21213, 30849, 13], "temperature": 0.0, "avg_logprob": -0.16618956119642345, "compression_ratio": 1.5691699604743083, "no_speech_prob": 1.568787956784945e-05}, {"id": 253, "seek": 123080, "start": 1255.2, "end": 1259.56, "text": " Something that's interesting is that I feel like you can definitely hear the", "tokens": [6595, 300, 311, 1880, 307, 300, 286, 841, 411, 291, 393, 2138, 1568, 264], "temperature": 0.0, "avg_logprob": -0.16618956119642345, "compression_ratio": 1.5691699604743083, "no_speech_prob": 1.568787956784945e-05}, {"id": 254, "seek": 125956, "start": 1259.56, "end": 1265.24, "text": " influence of Wikipedia in this movie review that I've made up.", "tokens": [6503, 295, 28999, 294, 341, 3169, 3131, 300, 286, 600, 1027, 493, 13], "temperature": 0.0, "avg_logprob": -0.15667154622632404, "compression_ratio": 1.5454545454545454, "no_speech_prob": 1.7775249943952076e-05}, {"id": 255, "seek": 125956, "start": 1265.24, "end": 1269.08, "text": " I feel like parts of that to me kind of sound like the language you would get in", "tokens": [286, 841, 411, 3166, 295, 300, 281, 385, 733, 295, 1626, 411, 264, 2856, 291, 576, 483, 294], "temperature": 0.0, "avg_logprob": -0.15667154622632404, "compression_ratio": 1.5454545454545454, "no_speech_prob": 1.7775249943952076e-05}, {"id": 256, "seek": 125956, "start": 1269.08, "end": 1274.28, "text": " Wikipedia, and we'll see as we train it longer, it'll become more movie review", "tokens": [28999, 11, 293, 321, 603, 536, 382, 321, 3847, 309, 2854, 11, 309, 603, 1813, 544, 3169, 3131], "temperature": 0.0, "avg_logprob": -0.15667154622632404, "compression_ratio": 1.5454545454545454, "no_speech_prob": 1.7775249943952076e-05}, {"id": 257, "seek": 125956, "start": 1274.28, "end": 1278.48, "text": " like.", "tokens": [411, 13], "temperature": 0.0, "avg_logprob": -0.15667154622632404, "compression_ratio": 1.5454545454545454, "no_speech_prob": 1.7775249943952076e-05}, {"id": 258, "seek": 125956, "start": 1278.48, "end": 1282.96, "text": " Something to note, actually I should ask, is there a question about what I'm doing", "tokens": [6595, 281, 3637, 11, 767, 286, 820, 1029, 11, 307, 456, 257, 1168, 466, 437, 286, 478, 884], "temperature": 0.0, "avg_logprob": -0.15667154622632404, "compression_ratio": 1.5454545454545454, "no_speech_prob": 1.7775249943952076e-05}, {"id": 259, "seek": 125956, "start": 1282.96, "end": 1288.76, "text": " right here?", "tokens": [558, 510, 30], "temperature": 0.0, "avg_logprob": -0.15667154622632404, "compression_ratio": 1.5454545454545454, "no_speech_prob": 1.7775249943952076e-05}, {"id": 260, "seek": 128876, "start": 1288.76, "end": 1293.08, "text": " We'll go back more into the details, kind of keeping with my teaching philosophy.", "tokens": [492, 603, 352, 646, 544, 666, 264, 4365, 11, 733, 295, 5145, 365, 452, 4571, 10675, 13], "temperature": 0.0, "avg_logprob": -0.19559938361845822, "compression_ratio": 1.6359223300970873, "no_speech_prob": 3.82218167942483e-05}, {"id": 261, "seek": 128876, "start": 1293.08, "end": 1297.04, "text": " We're starting with kind of how do we use this, and then we'll get more into", "tokens": [492, 434, 2891, 365, 733, 295, 577, 360, 321, 764, 341, 11, 293, 550, 321, 603, 483, 544, 666], "temperature": 0.0, "avg_logprob": -0.19559938361845822, "compression_ratio": 1.6359223300970873, "no_speech_prob": 3.82218167942483e-05}, {"id": 262, "seek": 128876, "start": 1297.04, "end": 1301.2, "text": " kind of like what's actually happening underneath.", "tokens": [733, 295, 411, 437, 311, 767, 2737, 7223, 13], "temperature": 0.0, "avg_logprob": -0.19559938361845822, "compression_ratio": 1.6359223300970873, "no_speech_prob": 3.82218167942483e-05}, {"id": 263, "seek": 128876, "start": 1301.2, "end": 1306.32, "text": " Here the idea is kind of how do you use a language model?", "tokens": [1692, 264, 1558, 307, 733, 295, 577, 360, 291, 764, 257, 2856, 2316, 30], "temperature": 0.0, "avg_logprob": -0.19559938361845822, "compression_ratio": 1.6359223300970873, "no_speech_prob": 3.82218167942483e-05}, {"id": 264, "seek": 128876, "start": 1306.32, "end": 1314.28, "text": " I want to highlight there is this parameter temperature that lets you", "tokens": [286, 528, 281, 5078, 456, 307, 341, 13075, 4292, 300, 6653, 291], "temperature": 0.0, "avg_logprob": -0.19559938361845822, "compression_ratio": 1.6359223300970873, "no_speech_prob": 3.82218167942483e-05}, {"id": 265, "seek": 131428, "start": 1314.28, "end": 1322.68, "text": " determine how much randomness you want to inject, and so you can lower the", "tokens": [6997, 577, 709, 4974, 1287, 291, 528, 281, 10711, 11, 293, 370, 291, 393, 3126, 264], "temperature": 0.0, "avg_logprob": -0.14398982607085128, "compression_ratio": 1.5639810426540284, "no_speech_prob": 8.13937822385924e-06}, {"id": 266, "seek": 131428, "start": 1322.68, "end": 1325.16, "text": " temperature to make the text less randomized.", "tokens": [4292, 281, 652, 264, 2487, 1570, 38513, 13], "temperature": 0.0, "avg_logprob": -0.14398982607085128, "compression_ratio": 1.5639810426540284, "no_speech_prob": 8.13937822385924e-06}, {"id": 267, "seek": 131428, "start": 1325.16, "end": 1335.16, "text": " Here I have the temperature higher, so they're a little bit more random.", "tokens": [1692, 286, 362, 264, 4292, 2946, 11, 370, 436, 434, 257, 707, 857, 544, 4974, 13], "temperature": 0.0, "avg_logprob": -0.14398982607085128, "compression_ratio": 1.5639810426540284, "no_speech_prob": 8.13937822385924e-06}, {"id": 268, "seek": 131428, "start": 1335.16, "end": 1339.24, "text": " This one, I hated this movie, but the film was released in October 2005.", "tokens": [639, 472, 11, 286, 17398, 341, 3169, 11, 457, 264, 2007, 390, 4736, 294, 7617, 14394, 13], "temperature": 0.0, "avg_logprob": -0.14398982607085128, "compression_ratio": 1.5639810426540284, "no_speech_prob": 8.13937822385924e-06}, {"id": 269, "seek": 131428, "start": 1339.24, "end": 1343.56, "text": " The film was released on DVD and Blu-ray on November 1st, 2006.", "tokens": [440, 2007, 390, 4736, 322, 21187, 293, 2177, 84, 12, 3458, 322, 7674, 502, 372, 11, 14062, 13], "temperature": 0.0, "avg_logprob": -0.14398982607085128, "compression_ratio": 1.5639810426540284, "no_speech_prob": 8.13937822385924e-06}, {"id": 270, "seek": 134356, "start": 1343.56, "end": 1346.52, "text": " I hated this movie, but the British Film Institute has called it the most", "tokens": [286, 17398, 341, 3169, 11, 457, 264, 6221, 13801, 9446, 575, 1219, 309, 264, 881], "temperature": 0.0, "avg_logprob": -0.18486454612330386, "compression_ratio": 1.5899581589958158, "no_speech_prob": 6.643122105742805e-06}, {"id": 271, "seek": 134356, "start": 1346.52, "end": 1349.04, "text": " important film in the history of British cinema.", "tokens": [1021, 2007, 294, 264, 2503, 295, 6221, 17178, 13], "temperature": 0.0, "avg_logprob": -0.18486454612330386, "compression_ratio": 1.5899581589958158, "no_speech_prob": 6.643122105742805e-06}, {"id": 272, "seek": 134356, "start": 1349.04, "end": 1353.76, "text": " I think these are kind of fun to see what you can generate.", "tokens": [286, 519, 613, 366, 733, 295, 1019, 281, 536, 437, 291, 393, 8460, 13], "temperature": 0.0, "avg_logprob": -0.18486454612330386, "compression_ratio": 1.5899581589958158, "no_speech_prob": 6.643122105742805e-06}, {"id": 273, "seek": 134356, "start": 1353.76, "end": 1363.96, "text": " This is a more predictable one, and you'll notice here I run it again, and this is", "tokens": [639, 307, 257, 544, 27737, 472, 11, 293, 291, 603, 3449, 510, 286, 1190, 309, 797, 11, 293, 341, 307], "temperature": 0.0, "avg_logprob": -0.18486454612330386, "compression_ratio": 1.5899581589958158, "no_speech_prob": 6.643122105742805e-06}, {"id": 274, "seek": 134356, "start": 1363.96, "end": 1367.8799999999999, "text": " pretty similar to the previous one, so you're going to get less variation in", "tokens": [1238, 2531, 281, 264, 3894, 472, 11, 370, 291, 434, 516, 281, 483, 1570, 12990, 294], "temperature": 0.0, "avg_logprob": -0.18486454612330386, "compression_ratio": 1.5899581589958158, "no_speech_prob": 6.643122105742805e-06}, {"id": 275, "seek": 134356, "start": 1367.8799999999999, "end": 1370.76, "text": " your output with a lower temperature.", "tokens": [428, 5598, 365, 257, 3126, 4292, 13], "temperature": 0.0, "avg_logprob": -0.18486454612330386, "compression_ratio": 1.5899581589958158, "no_speech_prob": 6.643122105742805e-06}, {"id": 276, "seek": 137076, "start": 1370.76, "end": 1375.96, "text": " Yes?", "tokens": [1079, 30], "temperature": 0.0, "avg_logprob": -0.4066347394670759, "compression_ratio": 1.3986928104575163, "no_speech_prob": 0.000551978824660182}, {"id": 277, "seek": 137076, "start": 1375.96, "end": 1383.96, "text": " Can I throw you the box?", "tokens": [1664, 286, 3507, 291, 264, 2424, 30], "temperature": 0.0, "avg_logprob": -0.4066347394670759, "compression_ratio": 1.3986928104575163, "no_speech_prob": 0.000551978824660182}, {"id": 278, "seek": 137076, "start": 1383.96, "end": 1385.96, "text": " I have a couple of questions.", "tokens": [286, 362, 257, 1916, 295, 1651, 13], "temperature": 0.0, "avg_logprob": -0.4066347394670759, "compression_ratio": 1.3986928104575163, "no_speech_prob": 0.000551978824660182}, {"id": 279, "seek": 137076, "start": 1385.96, "end": 1393.96, "text": " You mentioned that the words are not in Wikipedia, but in IMDb are going to be", "tokens": [509, 2835, 300, 264, 2283, 366, 406, 294, 28999, 11, 457, 294, 21463, 35, 65, 366, 516, 281, 312], "temperature": 0.0, "avg_logprob": -0.4066347394670759, "compression_ratio": 1.3986928104575163, "no_speech_prob": 0.000551978824660182}, {"id": 280, "seek": 137076, "start": 1393.96, "end": 1398.96, "text": " mapped to random vectors, and they are going to be different for each word,", "tokens": [33318, 281, 4974, 18875, 11, 293, 436, 366, 516, 281, 312, 819, 337, 1184, 1349, 11], "temperature": 0.0, "avg_logprob": -0.4066347394670759, "compression_ratio": 1.3986928104575163, "no_speech_prob": 0.000551978824660182}, {"id": 281, "seek": 139896, "start": 1398.96, "end": 1400.96, "text": " can you imagine?", "tokens": [393, 291, 3811, 30], "temperature": 0.0, "avg_logprob": -0.2179433911345726, "compression_ratio": 1.6482412060301508, "no_speech_prob": 0.0005610131192952394}, {"id": 282, "seek": 139896, "start": 1400.96, "end": 1404.96, "text": " I think initially they're all initialized with the same random vector, but then", "tokens": [286, 519, 9105, 436, 434, 439, 5883, 1602, 365, 264, 912, 4974, 8062, 11, 457, 550], "temperature": 0.0, "avg_logprob": -0.2179433911345726, "compression_ratio": 1.6482412060301508, "no_speech_prob": 0.0005610131192952394}, {"id": 283, "seek": 139896, "start": 1404.96, "end": 1407.96, "text": " they get updated as you train the model.", "tokens": [436, 483, 10588, 382, 291, 3847, 264, 2316, 13], "temperature": 0.0, "avg_logprob": -0.2179433911345726, "compression_ratio": 1.6482412060301508, "no_speech_prob": 0.0005610131192952394}, {"id": 284, "seek": 139896, "start": 1407.96, "end": 1409.96, "text": " So it's in one word?", "tokens": [407, 309, 311, 294, 472, 1349, 30], "temperature": 0.0, "avg_logprob": -0.2179433911345726, "compression_ratio": 1.6482412060301508, "no_speech_prob": 0.0005610131192952394}, {"id": 285, "seek": 139896, "start": 1409.96, "end": 1416.96, "text": " I mean, initially they're all the same, but you have a separate one for each", "tokens": [286, 914, 11, 9105, 436, 434, 439, 264, 912, 11, 457, 291, 362, 257, 4994, 472, 337, 1184], "temperature": 0.0, "avg_logprob": -0.2179433911345726, "compression_ratio": 1.6482412060301508, "no_speech_prob": 0.0005610131192952394}, {"id": 286, "seek": 139896, "start": 1416.96, "end": 1419.96, "text": " word that gets updated to something different.", "tokens": [1349, 300, 2170, 10588, 281, 746, 819, 13], "temperature": 0.0, "avg_logprob": -0.2179433911345726, "compression_ratio": 1.6482412060301508, "no_speech_prob": 0.0005610131192952394}, {"id": 287, "seek": 139896, "start": 1419.96, "end": 1422.96, "text": " The second question is how temperature works.", "tokens": [440, 1150, 1168, 307, 577, 4292, 1985, 13], "temperature": 0.0, "avg_logprob": -0.2179433911345726, "compression_ratio": 1.6482412060301508, "no_speech_prob": 0.0005610131192952394}, {"id": 288, "seek": 142296, "start": 1422.96, "end": 1428.96, "text": " I understood that it's something that works with the sub-magnet, that the", "tokens": [286, 7320, 300, 309, 311, 746, 300, 1985, 365, 264, 1422, 12, 76, 4535, 302, 11, 300, 264], "temperature": 0.0, "avg_logprob": -0.2660829226175944, "compression_ratio": 1.5454545454545454, "no_speech_prob": 6.604611553484574e-05}, {"id": 289, "seek": 142296, "start": 1428.96, "end": 1435.96, "text": " T that you divide by makes the weights, I mean the logits more similar, but how", "tokens": [314, 300, 291, 9845, 538, 1669, 264, 17443, 11, 286, 914, 264, 3565, 1208, 544, 2531, 11, 457, 577], "temperature": 0.0, "avg_logprob": -0.2660829226175944, "compression_ratio": 1.5454545454545454, "no_speech_prob": 6.604611553484574e-05}, {"id": 290, "seek": 142296, "start": 1435.96, "end": 1441.96, "text": " does that relate, or yeah, how does it work?", "tokens": [775, 300, 10961, 11, 420, 1338, 11, 577, 775, 309, 589, 30], "temperature": 0.0, "avg_logprob": -0.2660829226175944, "compression_ratio": 1.5454545454545454, "no_speech_prob": 6.604611553484574e-05}, {"id": 291, "seek": 142296, "start": 1441.96, "end": 1448.96, "text": " So we can look at the docs for predicting on the language model.", "tokens": [407, 321, 393, 574, 412, 264, 45623, 337, 32884, 322, 264, 2856, 2316, 13], "temperature": 0.0, "avg_logprob": -0.2660829226175944, "compression_ratio": 1.5454545454545454, "no_speech_prob": 6.604611553484574e-05}, {"id": 292, "seek": 142296, "start": 1448.96, "end": 1451.96, "text": " Let me see if this gives more information.", "tokens": [961, 385, 536, 498, 341, 2709, 544, 1589, 13], "temperature": 0.0, "avg_logprob": -0.2660829226175944, "compression_ratio": 1.5454545454545454, "no_speech_prob": 6.604611553484574e-05}, {"id": 293, "seek": 145196, "start": 1451.96, "end": 1453.96, "text": " So that's kind of it.", "tokens": [50364, 407, 300, 311, 733, 295, 309, 13, 50464], "temperature": 0.0, "avg_logprob": -0.42623438835144045, "compression_ratio": 0.7241379310344828, "no_speech_prob": 0.003854942275211215}, {"id": 294, "seek": 148196, "start": 1481.96, "end": 1489.96, "text": " Yeah, I mean I think some of this is kind of adjusting the probabilities of", "tokens": [865, 11, 286, 914, 286, 519, 512, 295, 341, 307, 733, 295, 23559, 264, 33783, 295], "temperature": 0.0, "avg_logprob": -0.13701244954312786, "compression_ratio": 1.5945945945945945, "no_speech_prob": 0.015901435166597366}, {"id": 295, "seek": 148196, "start": 1489.96, "end": 1491.96, "text": " what comes next.", "tokens": [437, 1487, 958, 13], "temperature": 0.0, "avg_logprob": -0.13701244954312786, "compression_ratio": 1.5945945945945945, "no_speech_prob": 0.015901435166597366}, {"id": 296, "seek": 148196, "start": 1491.96, "end": 1496.96, "text": " So kind of if you, you can end up creating a language model that's very", "tokens": [407, 733, 295, 498, 291, 11, 291, 393, 917, 493, 4084, 257, 2856, 2316, 300, 311, 588], "temperature": 0.0, "avg_logprob": -0.13701244954312786, "compression_ratio": 1.5945945945945945, "no_speech_prob": 0.015901435166597366}, {"id": 297, "seek": 148196, "start": 1496.96, "end": 1501.96, "text": " deterministic, where given something it's always going to predict the same", "tokens": [15957, 3142, 11, 689, 2212, 746, 309, 311, 1009, 516, 281, 6069, 264, 912], "temperature": 0.0, "avg_logprob": -0.13701244954312786, "compression_ratio": 1.5945945945945945, "no_speech_prob": 0.015901435166597366}, {"id": 298, "seek": 148196, "start": 1501.96, "end": 1503.96, "text": " thing after it, which you don't want.", "tokens": [551, 934, 309, 11, 597, 291, 500, 380, 528, 13], "temperature": 0.0, "avg_logprob": -0.13701244954312786, "compression_ratio": 1.5945945945945945, "no_speech_prob": 0.015901435166597366}, {"id": 299, "seek": 148196, "start": 1503.96, "end": 1508.96, "text": " You kind of want to have different words you're choosing from with different", "tokens": [509, 733, 295, 528, 281, 362, 819, 2283, 291, 434, 10875, 490, 365, 819], "temperature": 0.0, "avg_logprob": -0.13701244954312786, "compression_ratio": 1.5945945945945945, "no_speech_prob": 0.015901435166597366}, {"id": 300, "seek": 150896, "start": 1508.96, "end": 1515.96, "text": " probabilities, and this is I think kind of giving more probability towards other", "tokens": [33783, 11, 293, 341, 307, 286, 519, 733, 295, 2902, 544, 8482, 3030, 661], "temperature": 0.0, "avg_logprob": -0.18058350880940754, "compression_ratio": 1.4502923976608186, "no_speech_prob": 3.4806824260158464e-05}, {"id": 301, "seek": 150896, "start": 1515.96, "end": 1517.96, "text": " options.", "tokens": [3956, 13], "temperature": 0.0, "avg_logprob": -0.18058350880940754, "compression_ratio": 1.4502923976608186, "no_speech_prob": 3.4806824260158464e-05}, {"id": 302, "seek": 150896, "start": 1517.96, "end": 1525.96, "text": " Yeah, I was just wondering this, that's for prediction, even if they are more", "tokens": [865, 11, 286, 390, 445, 6359, 341, 11, 300, 311, 337, 17630, 11, 754, 498, 436, 366, 544], "temperature": 0.0, "avg_logprob": -0.18058350880940754, "compression_ratio": 1.4502923976608186, "no_speech_prob": 3.4806824260158464e-05}, {"id": 303, "seek": 150896, "start": 1525.96, "end": 1531.96, "text": " squeezed together like the maximum one, what are you going to end up predicting?", "tokens": [39470, 1214, 411, 264, 6674, 472, 11, 437, 366, 291, 516, 281, 917, 493, 32884, 30], "temperature": 0.0, "avg_logprob": -0.18058350880940754, "compression_ratio": 1.4502923976608186, "no_speech_prob": 3.4806824260158464e-05}, {"id": 304, "seek": 153196, "start": 1531.96, "end": 1538.96, "text": " Like, should be the same, but I'm wrong, but I don't know like where, or is it,", "tokens": [1743, 11, 820, 312, 264, 912, 11, 457, 286, 478, 2085, 11, 457, 286, 500, 380, 458, 411, 689, 11, 420, 307, 309, 11], "temperature": 0.0, "avg_logprob": -0.21854856390702096, "compression_ratio": 1.539906103286385, "no_speech_prob": 0.00017398636555299163}, {"id": 305, "seek": 153196, "start": 1538.96, "end": 1540.96, "text": " but it's fine, I'll just like go.", "tokens": [457, 309, 311, 2489, 11, 286, 603, 445, 411, 352, 13], "temperature": 0.0, "avg_logprob": -0.21854856390702096, "compression_ratio": 1.539906103286385, "no_speech_prob": 0.00017398636555299163}, {"id": 306, "seek": 153196, "start": 1540.96, "end": 1543.96, "text": " Okay, and I'll think about, I'll think about another way to explain this in more", "tokens": [1033, 11, 293, 286, 603, 519, 466, 11, 286, 603, 519, 466, 1071, 636, 281, 2903, 341, 294, 544], "temperature": 0.0, "avg_logprob": -0.21854856390702096, "compression_ratio": 1.539906103286385, "no_speech_prob": 0.00017398636555299163}, {"id": 307, "seek": 153196, "start": 1543.96, "end": 1545.96, "text": " detail next time.", "tokens": [2607, 958, 565, 13], "temperature": 0.0, "avg_logprob": -0.21854856390702096, "compression_ratio": 1.539906103286385, "no_speech_prob": 0.00017398636555299163}, {"id": 308, "seek": 153196, "start": 1545.96, "end": 1550.96, "text": " And then do you want to pass it to Omar?", "tokens": [400, 550, 360, 291, 528, 281, 1320, 309, 281, 33784, 30], "temperature": 0.0, "avg_logprob": -0.21854856390702096, "compression_ratio": 1.539906103286385, "no_speech_prob": 0.00017398636555299163}, {"id": 309, "seek": 153196, "start": 1550.96, "end": 1555.96, "text": " So you have mentioned that dropping the temperature makes the answers less", "tokens": [407, 291, 362, 2835, 300, 13601, 264, 4292, 1669, 264, 6338, 1570], "temperature": 0.0, "avg_logprob": -0.21854856390702096, "compression_ratio": 1.539906103286385, "no_speech_prob": 0.00017398636555299163}, {"id": 310, "seek": 155596, "start": 1555.96, "end": 1561.96, "text": " variant, so if, let's say there's a review that starts with I hate this movie in", "tokens": [17501, 11, 370, 498, 11, 718, 311, 584, 456, 311, 257, 3131, 300, 3719, 365, 286, 4700, 341, 3169, 294], "temperature": 0.0, "avg_logprob": -0.15008394341719777, "compression_ratio": 1.5706214689265536, "no_speech_prob": 9.609077096683905e-05}, {"id": 311, "seek": 155596, "start": 1561.96, "end": 1565.96, "text": " the EMDB dataset.", "tokens": [264, 16237, 27735, 28872, 13], "temperature": 0.0, "avg_logprob": -0.15008394341719777, "compression_ratio": 1.5706214689265536, "no_speech_prob": 9.609077096683905e-05}, {"id": 312, "seek": 155596, "start": 1565.96, "end": 1572.96, "text": " If we make the temperature really small, would this command return the exact", "tokens": [759, 321, 652, 264, 4292, 534, 1359, 11, 576, 341, 5622, 2736, 264, 1900], "temperature": 0.0, "avg_logprob": -0.15008394341719777, "compression_ratio": 1.5706214689265536, "no_speech_prob": 9.609077096683905e-05}, {"id": 313, "seek": 155596, "start": 1572.96, "end": 1575.96, "text": " review on the EMDB dataset?", "tokens": [3131, 322, 264, 16237, 27735, 28872, 30], "temperature": 0.0, "avg_logprob": -0.15008394341719777, "compression_ratio": 1.5706214689265536, "no_speech_prob": 9.609077096683905e-05}, {"id": 314, "seek": 155596, "start": 1575.96, "end": 1584.96, "text": " That's, I mean, so something like I hate this movie, there's probably many", "tokens": [663, 311, 11, 286, 914, 11, 370, 746, 411, 286, 4700, 341, 3169, 11, 456, 311, 1391, 867], "temperature": 0.0, "avg_logprob": -0.15008394341719777, "compression_ratio": 1.5706214689265536, "no_speech_prob": 9.609077096683905e-05}, {"id": 315, "seek": 158496, "start": 1584.96, "end": 1603.96, "text": " movies that say this, but let me see, let's try, oh, you can't do zero.", "tokens": [6233, 300, 584, 341, 11, 457, 718, 385, 536, 11, 718, 311, 853, 11, 1954, 11, 291, 393, 380, 360, 4018, 13], "temperature": 0.0, "avg_logprob": -0.08025962891785995, "compression_ratio": 1.2905982905982907, "no_speech_prob": 3.7050001992611215e-05}, {"id": 316, "seek": 158496, "start": 1603.96, "end": 1612.96, "text": " So I don't know if this would happen with this model, and this is something you", "tokens": [407, 286, 500, 380, 458, 498, 341, 576, 1051, 365, 341, 2316, 11, 293, 341, 307, 746, 291], "temperature": 0.0, "avg_logprob": -0.08025962891785995, "compression_ratio": 1.2905982905982907, "no_speech_prob": 3.7050001992611215e-05}, {"id": 317, "seek": 161296, "start": 1612.96, "end": 1618.96, "text": " can play around with, they're definitely like Markov models I've seen that end", "tokens": [393, 862, 926, 365, 11, 436, 434, 2138, 411, 3934, 5179, 5245, 286, 600, 1612, 300, 917], "temperature": 0.0, "avg_logprob": -0.10497203100295294, "compression_ratio": 1.6153846153846154, "no_speech_prob": 1.6186355423997156e-05}, {"id": 318, "seek": 161296, "start": 1618.96, "end": 1622.96, "text": " up just returning the exact same text again and again and again.", "tokens": [493, 445, 12678, 264, 1900, 912, 2487, 797, 293, 797, 293, 797, 13], "temperature": 0.0, "avg_logprob": -0.10497203100295294, "compression_ratio": 1.6153846153846154, "no_speech_prob": 1.6186355423997156e-05}, {"id": 319, "seek": 161296, "start": 1622.96, "end": 1625.96, "text": " I'm not sure if you would get that here, but you, like you will see, like with", "tokens": [286, 478, 406, 988, 498, 291, 576, 483, 300, 510, 11, 457, 291, 11, 411, 291, 486, 536, 11, 411, 365], "temperature": 0.0, "avg_logprob": -0.10497203100295294, "compression_ratio": 1.6153846153846154, "no_speech_prob": 1.6186355423997156e-05}, {"id": 320, "seek": 161296, "start": 1625.96, "end": 1632.96, "text": " this one having a low temperature, this is very similar, like what we're getting.", "tokens": [341, 472, 1419, 257, 2295, 4292, 11, 341, 307, 588, 2531, 11, 411, 437, 321, 434, 1242, 13], "temperature": 0.0, "avg_logprob": -0.10497203100295294, "compression_ratio": 1.6153846153846154, "no_speech_prob": 1.6186355423997156e-05}, {"id": 321, "seek": 161296, "start": 1632.96, "end": 1637.96, "text": " It's saying I hated this movie, but the film was a commercial success, it was", "tokens": [467, 311, 1566, 286, 17398, 341, 3169, 11, 457, 264, 2007, 390, 257, 6841, 2245, 11, 309, 390], "temperature": 0.0, "avg_logprob": -0.10497203100295294, "compression_ratio": 1.6153846153846154, "no_speech_prob": 1.6186355423997156e-05}, {"id": 322, "seek": 161296, "start": 1637.96, "end": 1639.96, "text": " released on DVD.", "tokens": [4736, 322, 21187, 13], "temperature": 0.0, "avg_logprob": -0.10497203100295294, "compression_ratio": 1.6153846153846154, "no_speech_prob": 1.6186355423997156e-05}, {"id": 323, "seek": 163996, "start": 1639.96, "end": 1642.96, "text": " There's kind of, yeah, very similar reviews, let me run this again and see if", "tokens": [821, 311, 733, 295, 11, 1338, 11, 588, 2531, 10229, 11, 718, 385, 1190, 341, 797, 293, 536, 498], "temperature": 0.0, "avg_logprob": -0.09384974872364718, "compression_ratio": 1.4634146341463414, "no_speech_prob": 3.0239461921155453e-05}, {"id": 324, "seek": 163996, "start": 1642.96, "end": 1647.96, "text": " we get another, again, I hated this movie, but the film was released in October", "tokens": [321, 483, 1071, 11, 797, 11, 286, 17398, 341, 3169, 11, 457, 264, 2007, 390, 4736, 294, 7617], "temperature": 0.0, "avg_logprob": -0.09384974872364718, "compression_ratio": 1.4634146341463414, "no_speech_prob": 3.0239461921155453e-05}, {"id": 325, "seek": 163996, "start": 1647.96, "end": 1652.96, "text": " 2006.", "tokens": [14062, 13], "temperature": 0.0, "avg_logprob": -0.09384974872364718, "compression_ratio": 1.4634146341463414, "no_speech_prob": 3.0239461921155453e-05}, {"id": 326, "seek": 163996, "start": 1652.96, "end": 1655.96, "text": " I think you could end up, oh, this is a little bit different.", "tokens": [286, 519, 291, 727, 917, 493, 11, 1954, 11, 341, 307, 257, 707, 857, 819, 13], "temperature": 0.0, "avg_logprob": -0.09384974872364718, "compression_ratio": 1.4634146341463414, "no_speech_prob": 3.0239461921155453e-05}, {"id": 327, "seek": 163996, "start": 1655.96, "end": 1659.96, "text": " Yeah, I don't know that you can completely get it to be deterministic, but", "tokens": [865, 11, 286, 500, 380, 458, 300, 291, 393, 2584, 483, 309, 281, 312, 15957, 3142, 11, 457], "temperature": 0.0, "avg_logprob": -0.09384974872364718, "compression_ratio": 1.4634146341463414, "no_speech_prob": 3.0239461921155453e-05}, {"id": 328, "seek": 165996, "start": 1659.96, "end": 1676.96, "text": " it's getting kind of closer to, like they're not that many options.", "tokens": [309, 311, 1242, 733, 295, 4966, 281, 11, 411, 436, 434, 406, 300, 867, 3956, 13], "temperature": 0.0, "avg_logprob": -0.13361053033308548, "compression_ratio": 1.2735042735042734, "no_speech_prob": 1.0289173587807454e-05}, {"id": 329, "seek": 165996, "start": 1676.96, "end": 1680.96, "text": " All right, and so again, this was, we haven't even really trained the model", "tokens": [1057, 558, 11, 293, 370, 797, 11, 341, 390, 11, 321, 2378, 380, 754, 534, 8895, 264, 2316], "temperature": 0.0, "avg_logprob": -0.13361053033308548, "compression_ratio": 1.2735042735042734, "no_speech_prob": 1.0289173587807454e-05}, {"id": 330, "seek": 165996, "start": 1680.96, "end": 1681.96, "text": " here.", "tokens": [510, 13], "temperature": 0.0, "avg_logprob": -0.13361053033308548, "compression_ratio": 1.2735042735042734, "no_speech_prob": 1.0289173587807454e-05}, {"id": 331, "seek": 168196, "start": 1681.96, "end": 1692.96, "text": " We basically just, let's go back up here.", "tokens": [492, 1936, 445, 11, 718, 311, 352, 646, 493, 510, 13], "temperature": 0.0, "avg_logprob": -0.10453545460935498, "compression_ratio": 1.3401360544217686, "no_speech_prob": 9.080208656087052e-06}, {"id": 332, "seek": 168196, "start": 1692.96, "end": 1694.96, "text": " Yeah, we basically just created it.", "tokens": [865, 11, 321, 1936, 445, 2942, 309, 13], "temperature": 0.0, "avg_logprob": -0.10453545460935498, "compression_ratio": 1.3401360544217686, "no_speech_prob": 9.080208656087052e-06}, {"id": 333, "seek": 168196, "start": 1694.96, "end": 1701.96, "text": " This was in AWD LSTM, which I'll talk about more later, but let's try training", "tokens": [639, 390, 294, 25815, 35, 441, 6840, 44, 11, 597, 286, 603, 751, 466, 544, 1780, 11, 457, 718, 311, 853, 3097], "temperature": 0.0, "avg_logprob": -0.10453545460935498, "compression_ratio": 1.3401360544217686, "no_speech_prob": 9.080208656087052e-06}, {"id": 334, "seek": 168196, "start": 1701.96, "end": 1706.96, "text": " it and see how that impacts what we get.", "tokens": [309, 293, 536, 577, 300, 11606, 437, 321, 483, 13], "temperature": 0.0, "avg_logprob": -0.10453545460935498, "compression_ratio": 1.3401360544217686, "no_speech_prob": 9.080208656087052e-06}, {"id": 335, "seek": 170696, "start": 1706.96, "end": 1711.96, "text": " But I kind of just wanted you to have a comparison to see where it was starting", "tokens": [583, 286, 733, 295, 445, 1415, 291, 281, 362, 257, 9660, 281, 536, 689, 309, 390, 2891], "temperature": 0.0, "avg_logprob": -0.06020550105882728, "compression_ratio": 1.552325581395349, "no_speech_prob": 1.9832426914945245e-05}, {"id": 336, "seek": 170696, "start": 1711.96, "end": 1718.96, "text": " from, and then we'll see where we end up.", "tokens": [490, 11, 293, 550, 321, 603, 536, 689, 321, 917, 493, 13], "temperature": 0.0, "avg_logprob": -0.06020550105882728, "compression_ratio": 1.552325581395349, "no_speech_prob": 1.9832426914945245e-05}, {"id": 337, "seek": 170696, "start": 1718.96, "end": 1724.96, "text": " So kind of key parameter to choose when training a neural net is the learning", "tokens": [407, 733, 295, 2141, 13075, 281, 2826, 562, 3097, 257, 18161, 2533, 307, 264, 2539], "temperature": 0.0, "avg_logprob": -0.06020550105882728, "compression_ratio": 1.552325581395349, "no_speech_prob": 1.9832426914945245e-05}, {"id": 338, "seek": 170696, "start": 1724.96, "end": 1730.96, "text": " rate, and that's kind of the multiplier when you're making updates.", "tokens": [3314, 11, 293, 300, 311, 733, 295, 264, 44106, 562, 291, 434, 1455, 9205, 13], "temperature": 0.0, "avg_logprob": -0.06020550105882728, "compression_ratio": 1.552325581395349, "no_speech_prob": 1.9832426914945245e-05}, {"id": 339, "seek": 173096, "start": 1730.96, "end": 1738.96, "text": " You can think of that as the step size of how much you want to update, and with", "tokens": [509, 393, 519, 295, 300, 382, 264, 1823, 2744, 295, 577, 709, 291, 528, 281, 5623, 11, 293, 365], "temperature": 0.0, "avg_logprob": -0.07394196124787027, "compression_ratio": 1.7587939698492463, "no_speech_prob": 4.710785560746444e-06}, {"id": 340, "seek": 173096, "start": 1738.96, "end": 1742.96, "text": " learning rate, you don't want it to be too big because you can kind of totally", "tokens": [2539, 3314, 11, 291, 500, 380, 528, 309, 281, 312, 886, 955, 570, 291, 393, 733, 295, 3879], "temperature": 0.0, "avg_logprob": -0.07394196124787027, "compression_ratio": 1.7587939698492463, "no_speech_prob": 4.710785560746444e-06}, {"id": 341, "seek": 173096, "start": 1742.96, "end": 1744.96, "text": " overshoot where you're going.", "tokens": [15488, 24467, 689, 291, 434, 516, 13], "temperature": 0.0, "avg_logprob": -0.07394196124787027, "compression_ratio": 1.7587939698492463, "no_speech_prob": 4.710785560746444e-06}, {"id": 342, "seek": 173096, "start": 1744.96, "end": 1747.96, "text": " Also, you don't want it to be too small because your model may take forever to", "tokens": [2743, 11, 291, 500, 380, 528, 309, 281, 312, 886, 1359, 570, 428, 2316, 815, 747, 5680, 281], "temperature": 0.0, "avg_logprob": -0.07394196124787027, "compression_ratio": 1.7587939698492463, "no_speech_prob": 4.710785560746444e-06}, {"id": 343, "seek": 173096, "start": 1747.96, "end": 1748.96, "text": " train.", "tokens": [3847, 13], "temperature": 0.0, "avg_logprob": -0.07394196124787027, "compression_ratio": 1.7587939698492463, "no_speech_prob": 4.710785560746444e-06}, {"id": 344, "seek": 173096, "start": 1748.96, "end": 1753.96, "text": " And so kind of one of the tricky things is choosing your learning rate, and", "tokens": [400, 370, 733, 295, 472, 295, 264, 12414, 721, 307, 10875, 428, 2539, 3314, 11, 293], "temperature": 0.0, "avg_logprob": -0.07394196124787027, "compression_ratio": 1.7587939698492463, "no_speech_prob": 4.710785560746444e-06}, {"id": 345, "seek": 175396, "start": 1753.96, "end": 1760.96, "text": " FastAI has a learning rate finder, which basically just tests out a variety of", "tokens": [15968, 48698, 575, 257, 2539, 3314, 915, 260, 11, 597, 1936, 445, 6921, 484, 257, 5673, 295], "temperature": 0.0, "avg_logprob": -0.08603601455688477, "compression_ratio": 1.7183908045977012, "no_speech_prob": 9.818194484978449e-06}, {"id": 346, "seek": 175396, "start": 1760.96, "end": 1764.96, "text": " learning rates, and you want to choose, and then it plots the loss.", "tokens": [2539, 6846, 11, 293, 291, 528, 281, 2826, 11, 293, 550, 309, 28609, 264, 4470, 13], "temperature": 0.0, "avg_logprob": -0.08603601455688477, "compression_ratio": 1.7183908045977012, "no_speech_prob": 9.818194484978449e-06}, {"id": 347, "seek": 175396, "start": 1764.96, "end": 1771.96, "text": " You want to choose something where the loss is still decreasing, so you kind of", "tokens": [509, 528, 281, 2826, 746, 689, 264, 4470, 307, 920, 23223, 11, 370, 291, 733, 295], "temperature": 0.0, "avg_logprob": -0.08603601455688477, "compression_ratio": 1.7183908045977012, "no_speech_prob": 9.818194484978449e-06}, {"id": 348, "seek": 175396, "start": 1771.96, "end": 1775.96, "text": " want the loss to be low but still decreasing at the point you choose it.", "tokens": [528, 264, 4470, 281, 312, 2295, 457, 920, 23223, 412, 264, 935, 291, 2826, 309, 13], "temperature": 0.0, "avg_logprob": -0.08603601455688477, "compression_ratio": 1.7183908045977012, "no_speech_prob": 9.818194484978449e-06}, {"id": 349, "seek": 177596, "start": 1775.96, "end": 1784.96, "text": " Here, choose 1e-2. Maybe even do like 3e-1.", "tokens": [1692, 11, 2826, 502, 68, 12, 17, 13, 2704, 754, 360, 411, 805, 68, 12, 16, 13], "temperature": 0.0, "avg_logprob": -0.12009621858596801, "compression_ratio": 1.4367816091954022, "no_speech_prob": 5.862757006980246e-06}, {"id": 350, "seek": 177596, "start": 1784.96, "end": 1790.96, "text": " Let's go with 1e-2. So choosing this point where it's low and decreasing, what", "tokens": [961, 311, 352, 365, 502, 68, 12, 17, 13, 407, 10875, 341, 935, 689, 309, 311, 2295, 293, 23223, 11, 437], "temperature": 0.0, "avg_logprob": -0.12009621858596801, "compression_ratio": 1.4367816091954022, "no_speech_prob": 5.862757006980246e-06}, {"id": 351, "seek": 177596, "start": 1790.96, "end": 1798.96, "text": " would you choose here, Jeremy? Thumbs up for 1e-2.", "tokens": [576, 291, 2826, 510, 11, 17809, 30, 334, 7808, 493, 337, 502, 68, 12, 17, 13], "temperature": 0.0, "avg_logprob": -0.12009621858596801, "compression_ratio": 1.4367816091954022, "no_speech_prob": 5.862757006980246e-06}, {"id": 352, "seek": 177596, "start": 1798.96, "end": 1801.96, "text": " So this first step was just even deciding how do we want to set our learning", "tokens": [407, 341, 700, 1823, 390, 445, 754, 17990, 577, 360, 321, 528, 281, 992, 527, 2539], "temperature": 0.0, "avg_logprob": -0.12009621858596801, "compression_ratio": 1.4367816091954022, "no_speech_prob": 5.862757006980246e-06}, {"id": 353, "seek": 180196, "start": 1801.96, "end": 1805.96, "text": " rate so that we can start training our model.", "tokens": [3314, 370, 300, 321, 393, 722, 3097, 527, 2316, 13], "temperature": 0.0, "avg_logprob": -0.0831403305281454, "compression_ratio": 1.6114649681528663, "no_speech_prob": 3.763495260500349e-05}, {"id": 354, "seek": 180196, "start": 1805.96, "end": 1809.96, "text": " And again, the reviews we were getting above, this was from this pre-trained", "tokens": [400, 797, 11, 264, 10229, 321, 645, 1242, 3673, 11, 341, 390, 490, 341, 659, 12, 17227, 2001], "temperature": 0.0, "avg_logprob": -0.0831403305281454, "compression_ratio": 1.6114649681528663, "no_speech_prob": 3.763495260500349e-05}, {"id": 355, "seek": 180196, "start": 1809.96, "end": 1814.96, "text": " model on Wikipedia, and I was kind of looking at, okay, this was trained on", "tokens": [2316, 322, 28999, 11, 293, 286, 390, 733, 295, 1237, 412, 11, 1392, 11, 341, 390, 8895, 322], "temperature": 0.0, "avg_logprob": -0.0831403305281454, "compression_ratio": 1.6114649681528663, "no_speech_prob": 3.763495260500349e-05}, {"id": 356, "seek": 180196, "start": 1814.96, "end": 1830.96, "text": " Wikipedia, what kind of movie reviews will it produce?", "tokens": [28999, 11, 437, 733, 295, 3169, 10229, 486, 309, 5258, 30], "temperature": 0.0, "avg_logprob": -0.0831403305281454, "compression_ratio": 1.6114649681528663, "no_speech_prob": 3.763495260500349e-05}, {"id": 357, "seek": 183096, "start": 1830.96, "end": 1837.96, "text": " Oh, okay, this is going to be slow.", "tokens": [876, 11, 1392, 11, 341, 307, 516, 281, 312, 2964, 13], "temperature": 0.0, "avg_logprob": -0.13210793697472775, "compression_ratio": 1.0574712643678161, "no_speech_prob": 5.82761422265321e-05}, {"id": 358, "seek": 183096, "start": 1837.96, "end": 1846.96, "text": " Okay, let me load the one I've saved. Thank you, Jeremy.", "tokens": [1033, 11, 718, 385, 3677, 264, 472, 286, 600, 6624, 13, 1044, 291, 11, 17809, 13], "temperature": 0.0, "avg_logprob": -0.13210793697472775, "compression_ratio": 1.0574712643678161, "no_speech_prob": 5.82761422265321e-05}, {"id": 359, "seek": 184696, "start": 1846.96, "end": 1871.96, "text": " And interrupt the kernel.", "tokens": [400, 12729, 264, 28256, 13], "temperature": 0.0, "avg_logprob": -0.30716609954833984, "compression_ratio": 0.7575757575757576, "no_speech_prob": 1.3006802873860579e-05}, {"id": 360, "seek": 187196, "start": 1871.96, "end": 1877.96, "text": " Okay, so I stopped the kernel because that was going to be slow to load.", "tokens": [1033, 11, 370, 286, 5936, 264, 28256, 570, 300, 390, 516, 281, 312, 2964, 281, 3677, 13], "temperature": 0.0, "avg_logprob": -0.09850039533389512, "compression_ratio": 1.566326530612245, "no_speech_prob": 1.4970607480790932e-05}, {"id": 361, "seek": 187196, "start": 1877.96, "end": 1885.96, "text": " Oh, no, okay.", "tokens": [876, 11, 572, 11, 1392, 13], "temperature": 0.0, "avg_logprob": -0.09850039533389512, "compression_ratio": 1.566326530612245, "no_speech_prob": 1.4970607480790932e-05}, {"id": 362, "seek": 187196, "start": 1885.96, "end": 1889.96, "text": " Should I skip ahead and load, I guess my fine-tuned? What?", "tokens": [6454, 286, 10023, 2286, 293, 3677, 11, 286, 2041, 452, 2489, 12, 83, 43703, 30, 708, 30], "temperature": 0.0, "avg_logprob": -0.09850039533389512, "compression_ratio": 1.566326530612245, "no_speech_prob": 1.4970607480790932e-05}, {"id": 363, "seek": 187196, "start": 1889.96, "end": 1893.96, "text": " Okay, so I should not have run this cell.", "tokens": [1033, 11, 370, 286, 820, 406, 362, 1190, 341, 2815, 13], "temperature": 0.0, "avg_logprob": -0.09850039533389512, "compression_ratio": 1.566326530612245, "no_speech_prob": 1.4970607480790932e-05}, {"id": 364, "seek": 187196, "start": 1893.96, "end": 1895.96, "text": " The idea is this is going to be slow to train.", "tokens": [440, 1558, 307, 341, 307, 516, 281, 312, 2964, 281, 3847, 13], "temperature": 0.0, "avg_logprob": -0.09850039533389512, "compression_ratio": 1.566326530612245, "no_speech_prob": 1.4970607480790932e-05}, {"id": 365, "seek": 187196, "start": 1895.96, "end": 1898.96, "text": " As you train it, you're going to periodically want to stop and save your", "tokens": [1018, 291, 3847, 309, 11, 291, 434, 516, 281, 38916, 528, 281, 1590, 293, 3155, 428], "temperature": 0.0, "avg_logprob": -0.09850039533389512, "compression_ratio": 1.566326530612245, "no_speech_prob": 1.4970607480790932e-05}, {"id": 366, "seek": 189896, "start": 1898.96, "end": 1901.96, "text": " weights so you don't have to start over each time.", "tokens": [17443, 370, 291, 500, 380, 362, 281, 722, 670, 1184, 565, 13], "temperature": 0.0, "avg_logprob": -0.053342674555403466, "compression_ratio": 1.6415094339622642, "no_speech_prob": 3.763333006645553e-05}, {"id": 367, "seek": 189896, "start": 1901.96, "end": 1905.96, "text": " And I was planning to load in my saved weights, but I just overrode them", "tokens": [400, 286, 390, 5038, 281, 3677, 294, 452, 6624, 17443, 11, 457, 286, 445, 670, 340, 1479, 552], "temperature": 0.0, "avg_logprob": -0.053342674555403466, "compression_ratio": 1.6415094339622642, "no_speech_prob": 3.763333006645553e-05}, {"id": 368, "seek": 189896, "start": 1905.96, "end": 1911.96, "text": " accidentally for the first round, but fortunately I have some later ones.", "tokens": [15715, 337, 264, 700, 3098, 11, 457, 25511, 286, 362, 512, 1780, 2306, 13], "temperature": 0.0, "avg_logprob": -0.053342674555403466, "compression_ratio": 1.6415094339622642, "no_speech_prob": 3.763333006645553e-05}, {"id": 369, "seek": 189896, "start": 1911.96, "end": 1918.96, "text": " And kind of a key thing with transfer learning is that typically when you", "tokens": [400, 733, 295, 257, 2141, 551, 365, 5003, 2539, 307, 300, 5850, 562, 291], "temperature": 0.0, "avg_logprob": -0.053342674555403466, "compression_ratio": 1.6415094339622642, "no_speech_prob": 3.763333006645553e-05}, {"id": 370, "seek": 189896, "start": 1918.96, "end": 1923.96, "text": " first start fine-tuning it on your data set, you just want to train the last", "tokens": [700, 722, 2489, 12, 83, 37726, 309, 322, 428, 1412, 992, 11, 291, 445, 528, 281, 3847, 264, 1036], "temperature": 0.0, "avg_logprob": -0.053342674555403466, "compression_ratio": 1.6415094339622642, "no_speech_prob": 3.763333006645553e-05}, {"id": 371, "seek": 192396, "start": 1923.96, "end": 1928.96, "text": " few layers and keep the earlier layers frozen.", "tokens": [1326, 7914, 293, 1066, 264, 3071, 7914, 12496, 13], "temperature": 0.0, "avg_logprob": -0.06250154972076416, "compression_ratio": 1.6294117647058823, "no_speech_prob": 1.3211683835834265e-05}, {"id": 372, "seek": 192396, "start": 1928.96, "end": 1932.96, "text": " And that means you're not updating the weights for the early layers.", "tokens": [400, 300, 1355, 291, 434, 406, 25113, 264, 17443, 337, 264, 2440, 7914, 13], "temperature": 0.0, "avg_logprob": -0.06250154972076416, "compression_ratio": 1.6294117647058823, "no_speech_prob": 1.3211683835834265e-05}, {"id": 373, "seek": 192396, "start": 1932.96, "end": 1936.96, "text": " And then you will unfreeze those and update all the weights.", "tokens": [400, 550, 291, 486, 3971, 701, 1381, 729, 293, 5623, 439, 264, 17443, 13], "temperature": 0.0, "avg_logprob": -0.06250154972076416, "compression_ratio": 1.6294117647058823, "no_speech_prob": 1.3211683835834265e-05}, {"id": 374, "seek": 192396, "start": 1936.96, "end": 1941.96, "text": " And this is kind of something that helps it learn faster.", "tokens": [400, 341, 307, 733, 295, 746, 300, 3665, 309, 1466, 4663, 13], "temperature": 0.0, "avg_logprob": -0.06250154972076416, "compression_ratio": 1.6294117647058823, "no_speech_prob": 1.3211683835834265e-05}, {"id": 375, "seek": 192396, "start": 1941.96, "end": 1949.96, "text": " So here I'm going to load in this already.", "tokens": [407, 510, 286, 478, 516, 281, 3677, 294, 341, 1217, 13], "temperature": 0.0, "avg_logprob": -0.06250154972076416, "compression_ratio": 1.6294117647058823, "no_speech_prob": 1.3211683835834265e-05}, {"id": 376, "seek": 194996, "start": 1949.96, "end": 1961.96, "text": " Oh, no.", "tokens": [876, 11, 572, 13], "temperature": 0.0, "avg_logprob": -0.0864697282964533, "compression_ratio": 1.2900763358778626, "no_speech_prob": 1.2029544450342655e-05}, {"id": 377, "seek": 194996, "start": 1961.96, "end": 1966.96, "text": " Okay, I have somehow changed the size of my model.", "tokens": [1033, 11, 286, 362, 6063, 3105, 264, 2744, 295, 452, 2316, 13], "temperature": 0.0, "avg_logprob": -0.0864697282964533, "compression_ratio": 1.2900763358778626, "no_speech_prob": 1.2029544450342655e-05}, {"id": 378, "seek": 194996, "start": 1966.96, "end": 1971.96, "text": " I will fix this before next time.", "tokens": [286, 486, 3191, 341, 949, 958, 565, 13], "temperature": 0.0, "avg_logprob": -0.0864697282964533, "compression_ratio": 1.2900763358778626, "no_speech_prob": 1.2029544450342655e-05}, {"id": 379, "seek": 194996, "start": 1971.96, "end": 1976.96, "text": " Let me for here, I'll just show you kind of what I got last time I ran this,", "tokens": [961, 385, 337, 510, 11, 286, 603, 445, 855, 291, 733, 295, 437, 286, 658, 1036, 565, 286, 5872, 341, 11], "temperature": 0.0, "avg_logprob": -0.0864697282964533, "compression_ratio": 1.2900763358778626, "no_speech_prob": 1.2029544450342655e-05}, {"id": 380, "seek": 197696, "start": 1976.96, "end": 1985.96, "text": " but I will run this on my own time since it's going to be slow, slow to run.", "tokens": [457, 286, 486, 1190, 341, 322, 452, 1065, 565, 1670, 309, 311, 516, 281, 312, 2964, 11, 2964, 281, 1190, 13], "temperature": 0.0, "avg_logprob": -0.11492142396814684, "compression_ratio": 1.4242424242424243, "no_speech_prob": 2.355201831960585e-05}, {"id": 381, "seek": 197696, "start": 1985.96, "end": 1987.96, "text": " So here I checked.", "tokens": [407, 510, 286, 10033, 13], "temperature": 0.0, "avg_logprob": -0.11492142396814684, "compression_ratio": 1.4242424242424243, "no_speech_prob": 2.355201831960585e-05}, {"id": 382, "seek": 197696, "start": 1987.96, "end": 1992.96, "text": " So before I was looking at the encoder weights for the embeddings of 30-something", "tokens": [407, 949, 286, 390, 1237, 412, 264, 2058, 19866, 17443, 337, 264, 12240, 29432, 295, 2217, 12, 31681], "temperature": 0.0, "avg_logprob": -0.11492142396814684, "compression_ratio": 1.4242424242424243, "no_speech_prob": 2.355201831960585e-05}, {"id": 383, "seek": 197696, "start": 1992.96, "end": 1999.96, "text": " and link later, which you'll remember both show up in the IMDB vocab, but not", "tokens": [293, 2113, 1780, 11, 597, 291, 603, 1604, 1293, 855, 493, 294, 264, 21463, 27735, 2329, 455, 11, 457, 406], "temperature": 0.0, "avg_logprob": -0.11492142396814684, "compression_ratio": 1.4242424242424243, "no_speech_prob": 2.355201831960585e-05}, {"id": 384, "seek": 197696, "start": 1999.96, "end": 2002.96, "text": " in the Wikitext 103 vocab.", "tokens": [294, 264, 23377, 642, 734, 48784, 2329, 455, 13], "temperature": 0.0, "avg_logprob": -0.11492142396814684, "compression_ratio": 1.4242424242424243, "no_speech_prob": 2.355201831960585e-05}, {"id": 385, "seek": 200296, "start": 2002.96, "end": 2006.96, "text": " And initially they were the same because they had been initialized to the same", "tokens": [400, 9105, 436, 645, 264, 912, 570, 436, 632, 668, 5883, 1602, 281, 264, 912], "temperature": 0.0, "avg_logprob": -0.12670837480997302, "compression_ratio": 1.7047619047619047, "no_speech_prob": 0.0001088891876861453}, {"id": 386, "seek": 200296, "start": 2006.96, "end": 2007.96, "text": " random thing.", "tokens": [4974, 551, 13], "temperature": 0.0, "avg_logprob": -0.12670837480997302, "compression_ratio": 1.7047619047619047, "no_speech_prob": 0.0001088891876861453}, {"id": 387, "seek": 200296, "start": 2007.96, "end": 2011.96, "text": " Now that I have trained the model some, they're no longer the same.", "tokens": [823, 300, 286, 362, 8895, 264, 2316, 512, 11, 436, 434, 572, 2854, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.12670837480997302, "compression_ratio": 1.7047619047619047, "no_speech_prob": 0.0001088891876861453}, {"id": 388, "seek": 200296, "start": 2011.96, "end": 2014.96, "text": " And so that's why this evaluates to false.", "tokens": [400, 370, 300, 311, 983, 341, 6133, 1024, 281, 7908, 13], "temperature": 0.0, "avg_logprob": -0.12670837480997302, "compression_ratio": 1.7047619047619047, "no_speech_prob": 0.0001088891876861453}, {"id": 389, "seek": 200296, "start": 2014.96, "end": 2017.96, "text": " This was just a way to illustrate that.", "tokens": [639, 390, 445, 257, 636, 281, 23221, 300, 13], "temperature": 0.0, "avg_logprob": -0.12670837480997302, "compression_ratio": 1.7047619047619047, "no_speech_prob": 0.0001088891876861453}, {"id": 390, "seek": 200296, "start": 2017.96, "end": 2025.96, "text": " There are also, I had stored the random thing that every, all the new vocab was", "tokens": [821, 366, 611, 11, 286, 632, 12187, 264, 4974, 551, 300, 633, 11, 439, 264, 777, 2329, 455, 390], "temperature": 0.0, "avg_logprob": -0.12670837480997302, "compression_ratio": 1.7047619047619047, "no_speech_prob": 0.0001088891876861453}, {"id": 391, "seek": 200296, "start": 2025.96, "end": 2028.96, "text": " initialized to in new word vector.", "tokens": [5883, 1602, 281, 294, 777, 1349, 8062, 13], "temperature": 0.0, "avg_logprob": -0.12670837480997302, "compression_ratio": 1.7047619047619047, "no_speech_prob": 0.0001088891876861453}, {"id": 392, "seek": 202896, "start": 2028.96, "end": 2034.96, "text": " And 30-something is no longer close to that because we've updated it as we've", "tokens": [400, 2217, 12, 31681, 307, 572, 2854, 1998, 281, 300, 570, 321, 600, 10588, 309, 382, 321, 600], "temperature": 0.0, "avg_logprob": -0.08565768024377655, "compression_ratio": 1.3478260869565217, "no_speech_prob": 1.2218862138979603e-05}, {"id": 393, "seek": 202896, "start": 2034.96, "end": 2040.96, "text": " learned more about how this word is used in our vocabulary.", "tokens": [3264, 544, 466, 577, 341, 1349, 307, 1143, 294, 527, 19864, 13], "temperature": 0.0, "avg_logprob": -0.08565768024377655, "compression_ratio": 1.3478260869565217, "no_speech_prob": 1.2218862138979603e-05}, {"id": 394, "seek": 202896, "start": 2040.96, "end": 2050.96, "text": " So now actually, even though it's slow, I will start this running just in case,", "tokens": [407, 586, 767, 11, 754, 1673, 309, 311, 2964, 11, 286, 486, 722, 341, 2614, 445, 294, 1389, 11], "temperature": 0.0, "avg_logprob": -0.08565768024377655, "compression_ratio": 1.3478260869565217, "no_speech_prob": 1.2218862138979603e-05}, {"id": 395, "seek": 205096, "start": 2050.96, "end": 2058.96, "text": " but I'll show you the movie reviews.", "tokens": [457, 286, 603, 855, 291, 264, 3169, 10229, 13], "temperature": 0.0, "avg_logprob": -0.07878692205562147, "compression_ratio": 1.6363636363636365, "no_speech_prob": 1.2606662494363263e-05}, {"id": 396, "seek": 205096, "start": 2058.96, "end": 2064.96, "text": " So now I'm looking at the language model again and I'm starting it with the", "tokens": [407, 586, 286, 478, 1237, 412, 264, 2856, 2316, 797, 293, 286, 478, 2891, 309, 365, 264], "temperature": 0.0, "avg_logprob": -0.07878692205562147, "compression_ratio": 1.6363636363636365, "no_speech_prob": 1.2606662494363263e-05}, {"id": 397, "seek": 205096, "start": 2064.96, "end": 2069.96, "text": " sentence, I liked this movie because, again, I liked this movie because I didn't", "tokens": [8174, 11, 286, 4501, 341, 3169, 570, 11, 797, 11, 286, 4501, 341, 3169, 570, 286, 994, 380], "temperature": 0.0, "avg_logprob": -0.07878692205562147, "compression_ratio": 1.6363636363636365, "no_speech_prob": 1.2606662494363263e-05}, {"id": 398, "seek": 205096, "start": 2069.96, "end": 2072.96, "text": " know how to match the filmic quality of the movie.", "tokens": [458, 577, 281, 2995, 264, 2007, 299, 3125, 295, 264, 3169, 13], "temperature": 0.0, "avg_logprob": -0.07878692205562147, "compression_ratio": 1.6363636363636365, "no_speech_prob": 1.2606662494363263e-05}, {"id": 399, "seek": 205096, "start": 2072.96, "end": 2077.96, "text": " This movie is just like the original, quite as bad as Channel 4, but it reminds", "tokens": [639, 3169, 307, 445, 411, 264, 3380, 11, 1596, 382, 1578, 382, 13553, 1017, 11, 457, 309, 12025], "temperature": 0.0, "avg_logprob": -0.07878692205562147, "compression_ratio": 1.6363636363636365, "no_speech_prob": 1.2606662494363263e-05}, {"id": 400, "seek": 207796, "start": 2077.96, "end": 2082.96, "text": " me of a famous, I liked this movie because it was a horror movie I had already", "tokens": [385, 295, 257, 4618, 11, 286, 4501, 341, 3169, 570, 309, 390, 257, 11501, 3169, 286, 632, 1217], "temperature": 0.0, "avg_logprob": -0.09814064126265676, "compression_ratio": 1.6942148760330578, "no_speech_prob": 3.119904795312323e-05}, {"id": 401, "seek": 207796, "start": 2082.96, "end": 2084.96, "text": " seen.", "tokens": [1612, 13], "temperature": 0.0, "avg_logprob": -0.09814064126265676, "compression_ratio": 1.6942148760330578, "no_speech_prob": 3.119904795312323e-05}, {"id": 402, "seek": 207796, "start": 2084.96, "end": 2088.96, "text": " Here's another one, this movie was is how I started.", "tokens": [1692, 311, 1071, 472, 11, 341, 3169, 390, 307, 577, 286, 1409, 13], "temperature": 0.0, "avg_logprob": -0.09814064126265676, "compression_ratio": 1.6942148760330578, "no_speech_prob": 3.119904795312323e-05}, {"id": 403, "seek": 207796, "start": 2088.96, "end": 2092.96, "text": " And you can see here that's the text being fed in, this movie was a great one", "tokens": [400, 291, 393, 536, 510, 300, 311, 264, 2487, 885, 4636, 294, 11, 341, 3169, 390, 257, 869, 472], "temperature": 0.0, "avg_logprob": -0.09814064126265676, "compression_ratio": 1.6942148760330578, "no_speech_prob": 3.119904795312323e-05}, {"id": 404, "seek": 207796, "start": 2092.96, "end": 2094.96, "text": " because it was a very Swedish one, cute.", "tokens": [570, 309, 390, 257, 588, 23523, 472, 11, 4052, 13], "temperature": 0.0, "avg_logprob": -0.09814064126265676, "compression_ratio": 1.6942148760330578, "no_speech_prob": 3.119904795312323e-05}, {"id": 405, "seek": 207796, "start": 2094.96, "end": 2097.96, "text": " The storyline is beautiful, the characters are colorful, and the film is", "tokens": [440, 30828, 307, 2238, 11, 264, 4342, 366, 18506, 11, 293, 264, 2007, 307], "temperature": 0.0, "avg_logprob": -0.09814064126265676, "compression_ratio": 1.6942148760330578, "no_speech_prob": 3.119904795312323e-05}, {"id": 406, "seek": 207796, "start": 2097.96, "end": 2100.96, "text": " great.", "tokens": [869, 13], "temperature": 0.0, "avg_logprob": -0.09814064126265676, "compression_ratio": 1.6942148760330578, "no_speech_prob": 3.119904795312323e-05}, {"id": 407, "seek": 207796, "start": 2100.96, "end": 2106.96, "text": " So I think on the whole, like these are still, they're not great English.", "tokens": [407, 286, 519, 322, 264, 1379, 11, 411, 613, 366, 920, 11, 436, 434, 406, 869, 3669, 13], "temperature": 0.0, "avg_logprob": -0.09814064126265676, "compression_ratio": 1.6942148760330578, "no_speech_prob": 3.119904795312323e-05}, {"id": 408, "seek": 210696, "start": 2106.96, "end": 2111.96, "text": " I think they do sound probably more like movie reviews and less like Wikipedia", "tokens": [286, 519, 436, 360, 1626, 1391, 544, 411, 3169, 10229, 293, 1570, 411, 28999], "temperature": 0.0, "avg_logprob": -0.09279169355119977, "compression_ratio": 1.5128205128205128, "no_speech_prob": 1.952405546035152e-05}, {"id": 409, "seek": 210696, "start": 2111.96, "end": 2115.96, "text": " articles than before, you know, before the structure I think was particularly", "tokens": [11290, 813, 949, 11, 291, 458, 11, 949, 264, 3877, 286, 519, 390, 4098], "temperature": 0.0, "avg_logprob": -0.09279169355119977, "compression_ratio": 1.5128205128205128, "no_speech_prob": 1.952405546035152e-05}, {"id": 410, "seek": 210696, "start": 2115.96, "end": 2119.96, "text": " Wikipedia-like.", "tokens": [28999, 12, 4092, 13], "temperature": 0.0, "avg_logprob": -0.09279169355119977, "compression_ratio": 1.5128205128205128, "no_speech_prob": 1.952405546035152e-05}, {"id": 411, "seek": 210696, "start": 2119.96, "end": 2128.96, "text": " We're still going to train this for longer.", "tokens": [492, 434, 920, 516, 281, 3847, 341, 337, 2854, 13], "temperature": 0.0, "avg_logprob": -0.09279169355119977, "compression_ratio": 1.5128205128205128, "no_speech_prob": 1.952405546035152e-05}, {"id": 412, "seek": 210696, "start": 2128.96, "end": 2133.96, "text": " So now I've decreased the learning rate even further, and that's something you", "tokens": [407, 586, 286, 600, 24436, 264, 2539, 3314, 754, 3052, 11, 293, 300, 311, 746, 291], "temperature": 0.0, "avg_logprob": -0.09279169355119977, "compression_ratio": 1.5128205128205128, "no_speech_prob": 1.952405546035152e-05}, {"id": 413, "seek": 213396, "start": 2133.96, "end": 2138.96, "text": " typically will do, kind of as you're training, often you'll want to decrease", "tokens": [5850, 486, 360, 11, 733, 295, 382, 291, 434, 3097, 11, 2049, 291, 603, 528, 281, 11514], "temperature": 0.0, "avg_logprob": -0.13356547770292862, "compression_ratio": 1.425, "no_speech_prob": 4.068746056873351e-05}, {"id": 414, "seek": 213396, "start": 2138.96, "end": 2142.96, "text": " the learning rate once you're kind of, now you've gotten more in the vicinity", "tokens": [264, 2539, 3314, 1564, 291, 434, 733, 295, 11, 586, 291, 600, 5768, 544, 294, 264, 42387], "temperature": 0.0, "avg_logprob": -0.13356547770292862, "compression_ratio": 1.425, "no_speech_prob": 4.068746056873351e-05}, {"id": 415, "seek": 213396, "start": 2142.96, "end": 2148.96, "text": " of good answers.", "tokens": [295, 665, 6338, 13], "temperature": 0.0, "avg_logprob": -0.13356547770292862, "compression_ratio": 1.425, "no_speech_prob": 4.068746056873351e-05}, {"id": 416, "seek": 214896, "start": 2148.96, "end": 2165.96, "text": " Oh, these I've, let me try loading fine-tuned.", "tokens": [876, 11, 613, 286, 600, 11, 718, 385, 853, 15114, 2489, 12, 83, 43703, 13], "temperature": 0.0, "avg_logprob": -0.22722670906468442, "compression_ratio": 0.8518518518518519, "no_speech_prob": 1.669931225478649e-05}, {"id": 417, "seek": 216596, "start": 2165.96, "end": 2179.96, "text": " Okay, let me try loading that one and see if it's the same size as my current", "tokens": [1033, 11, 718, 385, 853, 15114, 300, 472, 293, 536, 498, 309, 311, 264, 912, 2744, 382, 452, 2190], "temperature": 0.0, "avg_logprob": -0.12645383503126062, "compression_ratio": 1.0405405405405406, "no_speech_prob": 6.7043503804598e-05}, {"id": 418, "seek": 217996, "start": 2179.96, "end": 2196.96, "text": " size.", "tokens": [2744, 13], "temperature": 0.0, "avg_logprob": -0.1752959546588716, "compression_ratio": 1.1485148514851484, "no_speech_prob": 2.0142571884207428e-05}, {"id": 419, "seek": 217996, "start": 2196.96, "end": 2202.96, "text": " No, it's a different size.", "tokens": [883, 11, 309, 311, 257, 819, 2744, 13], "temperature": 0.0, "avg_logprob": -0.1752959546588716, "compression_ratio": 1.1485148514851484, "no_speech_prob": 2.0142571884207428e-05}, {"id": 420, "seek": 217996, "start": 2202.96, "end": 2203.96, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.1752959546588716, "compression_ratio": 1.1485148514851484, "no_speech_prob": 2.0142571884207428e-05}, {"id": 421, "seek": 217996, "start": 2203.96, "end": 2206.96, "text": " Well, we'll come back to this next time and I'll get these retrained, and you", "tokens": [1042, 11, 321, 603, 808, 646, 281, 341, 958, 565, 293, 286, 603, 483, 613, 1533, 31774, 11, 293, 291], "temperature": 0.0, "avg_logprob": -0.1752959546588716, "compression_ratio": 1.1485148514851484, "no_speech_prob": 2.0142571884207428e-05}, {"id": 422, "seek": 220696, "start": 2206.96, "end": 2210.96, "text": " can also run these on your own, and again, allow time since some of these can", "tokens": [393, 611, 1190, 613, 322, 428, 1065, 11, 293, 797, 11, 2089, 565, 1670, 512, 295, 613, 393], "temperature": 0.0, "avg_logprob": -0.08182886794761375, "compression_ratio": 1.7478260869565216, "no_speech_prob": 7.367330545093864e-05}, {"id": 423, "seek": 220696, "start": 2210.96, "end": 2214.96, "text": " take, you know, for instance, like the initial training I'm trying to do is", "tokens": [747, 11, 291, 458, 11, 337, 5197, 11, 411, 264, 5883, 3097, 286, 478, 1382, 281, 360, 307], "temperature": 0.0, "avg_logprob": -0.08182886794761375, "compression_ratio": 1.7478260869565216, "no_speech_prob": 7.367330545093864e-05}, {"id": 424, "seek": 220696, "start": 2214.96, "end": 2218.96, "text": " taking 10 minutes, and so I didn't want to wait 10 minutes to see it here, but", "tokens": [1940, 1266, 2077, 11, 293, 370, 286, 994, 380, 528, 281, 1699, 1266, 2077, 281, 536, 309, 510, 11, 457], "temperature": 0.0, "avg_logprob": -0.08182886794761375, "compression_ratio": 1.7478260869565216, "no_speech_prob": 7.367330545093864e-05}, {"id": 425, "seek": 220696, "start": 2218.96, "end": 2221.96, "text": " you can do this.", "tokens": [291, 393, 360, 341, 13], "temperature": 0.0, "avg_logprob": -0.08182886794761375, "compression_ratio": 1.7478260869565216, "no_speech_prob": 7.367330545093864e-05}, {"id": 426, "seek": 220696, "start": 2221.96, "end": 2225.96, "text": " I do want to note some of the risk of language models while we're talking", "tokens": [286, 360, 528, 281, 3637, 512, 295, 264, 3148, 295, 2856, 5245, 1339, 321, 434, 1417], "temperature": 0.0, "avg_logprob": -0.08182886794761375, "compression_ratio": 1.7478260869565216, "no_speech_prob": 7.367330545093864e-05}, {"id": 427, "seek": 220696, "start": 2225.96, "end": 2229.96, "text": " about language models, and we'll come back to this more later on in the course", "tokens": [466, 2856, 5245, 11, 293, 321, 603, 808, 646, 281, 341, 544, 1780, 322, 294, 264, 1164], "temperature": 0.0, "avg_logprob": -0.08182886794761375, "compression_ratio": 1.7478260869565216, "no_speech_prob": 7.367330545093864e-05}, {"id": 428, "seek": 222996, "start": 2229.96, "end": 2237.96, "text": " when we get to ethics and bias, but recently, so who here heard about OpenAIs,", "tokens": [562, 321, 483, 281, 19769, 293, 12577, 11, 457, 3938, 11, 370, 567, 510, 2198, 466, 7238, 32, 6802, 11], "temperature": 0.0, "avg_logprob": -0.08559884148082514, "compression_ratio": 1.4907407407407407, "no_speech_prob": 1.4284565622801892e-05}, {"id": 429, "seek": 222996, "start": 2237.96, "end": 2240.96, "text": " GPT-2?", "tokens": [26039, 51, 12, 17, 30], "temperature": 0.0, "avg_logprob": -0.08559884148082514, "compression_ratio": 1.4907407407407407, "no_speech_prob": 1.4284565622801892e-05}, {"id": 430, "seek": 222996, "start": 2240.96, "end": 2245.96, "text": " I see a few hands, although not everybody, so this was a language model that", "tokens": [286, 536, 257, 1326, 2377, 11, 4878, 406, 2201, 11, 370, 341, 390, 257, 2856, 2316, 300], "temperature": 0.0, "avg_logprob": -0.08559884148082514, "compression_ratio": 1.4907407407407407, "no_speech_prob": 1.4284565622801892e-05}, {"id": 431, "seek": 222996, "start": 2245.96, "end": 2251.96, "text": " produces much, much better text than what we've seen with these generated movie", "tokens": [14725, 709, 11, 709, 1101, 2487, 813, 437, 321, 600, 1612, 365, 613, 10833, 3169], "temperature": 0.0, "avg_logprob": -0.08559884148082514, "compression_ratio": 1.4907407407407407, "no_speech_prob": 1.4284565622801892e-05}, {"id": 432, "seek": 222996, "start": 2251.96, "end": 2257.96, "text": " reviews, and maybe I'll even share a clip next time, and so the researchers did", "tokens": [10229, 11, 293, 1310, 286, 603, 754, 2073, 257, 7353, 958, 565, 11, 293, 370, 264, 10309, 630], "temperature": 0.0, "avg_logprob": -0.08559884148082514, "compression_ratio": 1.4907407407407407, "no_speech_prob": 1.4284565622801892e-05}, {"id": 433, "seek": 225796, "start": 2257.96, "end": 2261.96, "text": " not release all the details of the model because they felt like this is so", "tokens": [406, 4374, 439, 264, 4365, 295, 264, 2316, 570, 436, 2762, 411, 341, 307, 370], "temperature": 0.0, "avg_logprob": -0.0913751220703125, "compression_ratio": 1.6177777777777778, "no_speech_prob": 2.9770217224722728e-05}, {"id": 434, "seek": 225796, "start": 2261.96, "end": 2262.96, "text": " compelling.", "tokens": [20050, 13], "temperature": 0.0, "avg_logprob": -0.0913751220703125, "compression_ratio": 1.6177777777777778, "no_speech_prob": 2.9770217224722728e-05}, {"id": 435, "seek": 225796, "start": 2262.96, "end": 2265.96, "text": " It could be a risk.", "tokens": [467, 727, 312, 257, 3148, 13], "temperature": 0.0, "avg_logprob": -0.0913751220703125, "compression_ratio": 1.6177777777777778, "no_speech_prob": 2.9770217224722728e-05}, {"id": 436, "seek": 225796, "start": 2265.96, "end": 2271.96, "text": " In an article about it, Jeremy was quoted, this was the Verge's coverage,", "tokens": [682, 364, 7222, 466, 309, 11, 17809, 390, 30047, 11, 341, 390, 264, 4281, 432, 311, 9645, 11], "temperature": 0.0, "avg_logprob": -0.0913751220703125, "compression_ratio": 1.6177777777777778, "no_speech_prob": 2.9770217224722728e-05}, {"id": 437, "seek": 225796, "start": 2271.96, "end": 2277.96, "text": " yeah, OpenAIs, and this was, I should say, a controversial decision about", "tokens": [1338, 11, 7238, 32, 6802, 11, 293, 341, 390, 11, 286, 820, 584, 11, 257, 17323, 3537, 466], "temperature": 0.0, "avg_logprob": -0.0913751220703125, "compression_ratio": 1.6177777777777778, "no_speech_prob": 2.9770217224722728e-05}, {"id": 438, "seek": 225796, "start": 2277.96, "end": 2282.96, "text": " should they have released it or not, but Jeremy said, I've been trying to warn", "tokens": [820, 436, 362, 4736, 309, 420, 406, 11, 457, 17809, 848, 11, 286, 600, 668, 1382, 281, 12286], "temperature": 0.0, "avg_logprob": -0.0913751220703125, "compression_ratio": 1.6177777777777778, "no_speech_prob": 2.9770217224722728e-05}, {"id": 439, "seek": 225796, "start": 2282.96, "end": 2284.96, "text": " people about this for a while.", "tokens": [561, 466, 341, 337, 257, 1339, 13], "temperature": 0.0, "avg_logprob": -0.0913751220703125, "compression_ratio": 1.6177777777777778, "no_speech_prob": 2.9770217224722728e-05}, {"id": 440, "seek": 228496, "start": 2284.96, "end": 2288.96, "text": " We have the technology to totally fill Twitter, email, and the web up with", "tokens": [492, 362, 264, 2899, 281, 3879, 2836, 5794, 11, 3796, 11, 293, 264, 3670, 493, 365], "temperature": 0.0, "avg_logprob": -0.0982081632356386, "compression_ratio": 1.4541062801932367, "no_speech_prob": 1.0450962690811139e-05}, {"id": 441, "seek": 228496, "start": 2288.96, "end": 2293.96, "text": " reasonable sounding context-appropriate prose, which would drown out all other", "tokens": [10585, 24931, 4319, 12, 21771, 473, 12505, 11, 597, 576, 20337, 484, 439, 661], "temperature": 0.0, "avg_logprob": -0.0982081632356386, "compression_ratio": 1.4541062801932367, "no_speech_prob": 1.0450962690811139e-05}, {"id": 442, "seek": 228496, "start": 2293.96, "end": 2300.96, "text": " speech and be impossible to filter, and for just a small example, someone", "tokens": [6218, 293, 312, 6243, 281, 6608, 11, 293, 337, 445, 257, 1359, 1365, 11, 1580], "temperature": 0.0, "avg_logprob": -0.0982081632356386, "compression_ratio": 1.4541062801932367, "no_speech_prob": 1.0450962690811139e-05}, {"id": 443, "seek": 228496, "start": 2300.96, "end": 2307.96, "text": " created a website, I think it's Robot Flow, and it's like Stack Overflow,", "tokens": [2942, 257, 3144, 11, 286, 519, 309, 311, 29601, 32792, 11, 293, 309, 311, 411, 37649, 4886, 10565, 11], "temperature": 0.0, "avg_logprob": -0.0982081632356386, "compression_ratio": 1.4541062801932367, "no_speech_prob": 1.0450962690811139e-05}, {"id": 444, "seek": 230796, "start": 2307.96, "end": 2315.96, "text": " except the answers are all created by a language model, and this is a fun site", "tokens": [3993, 264, 6338, 366, 439, 2942, 538, 257, 2856, 2316, 11, 293, 341, 307, 257, 1019, 3621], "temperature": 0.0, "avg_logprob": -0.0470349609210927, "compression_ratio": 1.6899563318777293, "no_speech_prob": 9.223012057191227e-06}, {"id": 445, "seek": 230796, "start": 2315.96, "end": 2320.96, "text": " that's not malicious, but then somebody else started cross-posting some of those", "tokens": [300, 311, 406, 33496, 11, 457, 550, 2618, 1646, 1409, 3278, 12, 23744, 278, 512, 295, 729], "temperature": 0.0, "avg_logprob": -0.0470349609210927, "compression_ratio": 1.6899563318777293, "no_speech_prob": 9.223012057191227e-06}, {"id": 446, "seek": 230796, "start": 2320.96, "end": 2325.96, "text": " answers to Stack Overflow, and these are answers that sound reasonable, but", "tokens": [6338, 281, 37649, 4886, 10565, 11, 293, 613, 366, 6338, 300, 1626, 10585, 11, 457], "temperature": 0.0, "avg_logprob": -0.0470349609210927, "compression_ratio": 1.6899563318777293, "no_speech_prob": 9.223012057191227e-06}, {"id": 447, "seek": 230796, "start": 2325.96, "end": 2329.96, "text": " they're incorrect, they don't actually make sense, and so then there was a", "tokens": [436, 434, 18424, 11, 436, 500, 380, 767, 652, 2020, 11, 293, 370, 550, 456, 390, 257], "temperature": 0.0, "avg_logprob": -0.0470349609210927, "compression_ratio": 1.6899563318777293, "no_speech_prob": 9.223012057191227e-06}, {"id": 448, "seek": 230796, "start": 2329.96, "end": 2335.96, "text": " discussion on Stack Overflow of how should they deal with this, because this", "tokens": [5017, 322, 37649, 4886, 10565, 295, 577, 820, 436, 2028, 365, 341, 11, 570, 341], "temperature": 0.0, "avg_logprob": -0.0470349609210927, "compression_ratio": 1.6899563318777293, "no_speech_prob": 9.223012057191227e-06}, {"id": 449, "seek": 233596, "start": 2335.96, "end": 2338.96, "text": " is something that could potentially be very confusing for people to have", "tokens": [307, 746, 300, 727, 7263, 312, 588, 13181, 337, 561, 281, 362], "temperature": 0.0, "avg_logprob": -0.15740857641380954, "compression_ratio": 1.5188284518828452, "no_speech_prob": 1.8629491023602895e-05}, {"id": 450, "seek": 233596, "start": 2338.96, "end": 2346.96, "text": " reasonable sounding, wrong, computer-generated answers, and Hamil", "tokens": [10585, 24931, 11, 2085, 11, 3820, 12, 21848, 770, 6338, 11, 293, 8234, 388], "temperature": 0.0, "avg_logprob": -0.15740857641380954, "compression_ratio": 1.5188284518828452, "no_speech_prob": 1.8629491023602895e-05}, {"id": 451, "seek": 233596, "start": 2346.96, "end": 2350.96, "text": " Hussein, who is a BASTI student and a machine learning engineer at GitHub,", "tokens": [21282, 33042, 11, 567, 307, 257, 363, 3160, 5422, 3107, 293, 257, 3479, 2539, 11403, 412, 23331, 11], "temperature": 0.0, "avg_logprob": -0.15740857641380954, "compression_ratio": 1.5188284518828452, "no_speech_prob": 1.8629491023602895e-05}, {"id": 452, "seek": 233596, "start": 2350.96, "end": 2355.96, "text": " highlighted this. This is an example of how easy it is to overwhelm public", "tokens": [17173, 341, 13, 639, 307, 364, 1365, 295, 577, 1858, 309, 307, 281, 9103, 76, 1908], "temperature": 0.0, "avg_logprob": -0.15740857641380954, "compression_ratio": 1.5188284518828452, "no_speech_prob": 1.8629491023602895e-05}, {"id": 453, "seek": 233596, "start": 2355.96, "end": 2360.96, "text": " discourse with generative models and highlights the importance of thinking", "tokens": [23938, 365, 1337, 1166, 5245, 293, 14254, 264, 7379, 295, 1953], "temperature": 0.0, "avg_logprob": -0.15740857641380954, "compression_ratio": 1.5188284518828452, "no_speech_prob": 1.8629491023602895e-05}, {"id": 454, "seek": 236096, "start": 2360.96, "end": 2365.96, "text": " about defenses, solutions. The language model was trained by one person using", "tokens": [466, 35989, 11, 6547, 13, 440, 2856, 2316, 390, 8895, 538, 472, 954, 1228], "temperature": 0.0, "avg_logprob": -0.24675823393322172, "compression_ratio": 1.3719512195121952, "no_speech_prob": 5.306871025823057e-05}, {"id": 455, "seek": 236096, "start": 2365.96, "end": 2370.96, "text": " Fast.ai, but not for this purpose. This was, and actually I can go to...", "tokens": [15968, 13, 1301, 11, 457, 406, 337, 341, 4334, 13, 639, 390, 11, 293, 767, 286, 393, 352, 281, 485], "temperature": 0.0, "avg_logprob": -0.24675823393322172, "compression_ratio": 1.3719512195121952, "no_speech_prob": 5.306871025823057e-05}, {"id": 456, "seek": 236096, "start": 2370.96, "end": 2382.96, "text": " Yeah, Fast.ai student project, and I can pull up ask Roboflow, the AI that", "tokens": [865, 11, 15968, 13, 1301, 3107, 1716, 11, 293, 286, 393, 2235, 493, 1029, 5424, 2670, 14107, 11, 264, 7318, 300], "temperature": 0.0, "avg_logprob": -0.24675823393322172, "compression_ratio": 1.3719512195121952, "no_speech_prob": 5.306871025823057e-05}, {"id": 457, "seek": 238296, "start": 2382.96, "end": 2390.96, "text": " answers programming questions, which I think as an application is fine, but", "tokens": [6338, 9410, 1651, 11, 597, 286, 519, 382, 364, 3861, 307, 2489, 11, 457], "temperature": 0.0, "avg_logprob": -0.08792076706886291, "compression_ratio": 1.4977777777777779, "no_speech_prob": 3.0239545594668016e-05}, {"id": 458, "seek": 238296, "start": 2390.96, "end": 2395.96, "text": " yeah, then you've got the what does this mean when somebody starts posting these", "tokens": [1338, 11, 550, 291, 600, 658, 264, 437, 775, 341, 914, 562, 2618, 3719, 15978, 613], "temperature": 0.0, "avg_logprob": -0.08792076706886291, "compression_ratio": 1.4977777777777779, "no_speech_prob": 3.0239545594668016e-05}, {"id": 459, "seek": 238296, "start": 2395.96, "end": 2401.96, "text": " to Stack Overflow and they're wrong and misleading. Did you have more you want", "tokens": [281, 37649, 4886, 10565, 293, 436, 434, 2085, 293, 36429, 13, 2589, 291, 362, 544, 291, 528], "temperature": 0.0, "avg_logprob": -0.08792076706886291, "compression_ratio": 1.4977777777777779, "no_speech_prob": 3.0239545594668016e-05}, {"id": 460, "seek": 238296, "start": 2401.96, "end": 2404.96, "text": " to say about this, Jeremy?", "tokens": [281, 584, 466, 341, 11, 17809, 30], "temperature": 0.0, "avg_logprob": -0.08792076706886291, "compression_ratio": 1.4977777777777779, "no_speech_prob": 3.0239545594668016e-05}, {"id": 461, "seek": 238296, "start": 2404.96, "end": 2408.96, "text": " Hamil actually used this approach to something really handy, which is that", "tokens": [8234, 388, 767, 1143, 341, 3109, 281, 746, 534, 13239, 11, 597, 307, 300], "temperature": 0.0, "avg_logprob": -0.08792076706886291, "compression_ratio": 1.4977777777777779, "no_speech_prob": 3.0239545594668016e-05}, {"id": 462, "seek": 240896, "start": 2408.96, "end": 2413.96, "text": " GitHub, he created something which uses the same technology to find useful", "tokens": [23331, 11, 415, 2942, 746, 597, 4960, 264, 912, 2899, 281, 915, 4420], "temperature": 0.0, "avg_logprob": -0.119996217580942, "compression_ratio": 1.5631067961165048, "no_speech_prob": 1.8922624803963117e-05}, {"id": 463, "seek": 240896, "start": 2413.96, "end": 2419.96, "text": " snippets of GitHub code based on your own natural language queries, which is", "tokens": [35623, 1385, 295, 23331, 3089, 2361, 322, 428, 1065, 3303, 2856, 24109, 11, 597, 307], "temperature": 0.0, "avg_logprob": -0.119996217580942, "compression_ratio": 1.5631067961165048, "no_speech_prob": 1.8922624803963117e-05}, {"id": 464, "seek": 240896, "start": 2419.96, "end": 2420.96, "text": " pretty cool.", "tokens": [1238, 1627, 13], "temperature": 0.0, "avg_logprob": -0.119996217580942, "compression_ratio": 1.5631067961165048, "no_speech_prob": 1.8922624803963117e-05}, {"id": 465, "seek": 240896, "start": 2420.96, "end": 2424.96, "text": " Yeah, and this was GitHub's semantic code search. And so that's something to", "tokens": [865, 11, 293, 341, 390, 23331, 311, 47982, 3089, 3164, 13, 400, 370, 300, 311, 746, 281], "temperature": 0.0, "avg_logprob": -0.119996217580942, "compression_ratio": 1.5631067961165048, "no_speech_prob": 1.8922624803963117e-05}, {"id": 466, "seek": 240896, "start": 2424.96, "end": 2429.96, "text": " keep in mind with language models, that they have this kind of dual, they can be", "tokens": [1066, 294, 1575, 365, 2856, 5245, 11, 300, 436, 362, 341, 733, 295, 11848, 11, 436, 393, 312], "temperature": 0.0, "avg_logprob": -0.119996217580942, "compression_ratio": 1.5631067961165048, "no_speech_prob": 1.8922624803963117e-05}, {"id": 467, "seek": 242996, "start": 2429.96, "end": 2441.96, "text": " fun and even useful, but also have risk.", "tokens": [1019, 293, 754, 4420, 11, 457, 611, 362, 3148, 13], "temperature": 0.0, "avg_logprob": -0.08919747092507102, "compression_ratio": 1.3724137931034484, "no_speech_prob": 9.665689503890462e-06}, {"id": 468, "seek": 242996, "start": 2441.96, "end": 2447.96, "text": " Actually, I think I will stop here just a little bit early today since I am not", "tokens": [5135, 11, 286, 519, 286, 486, 1590, 510, 445, 257, 707, 857, 2440, 965, 1670, 286, 669, 406], "temperature": 0.0, "avg_logprob": -0.08919747092507102, "compression_ratio": 1.3724137931034484, "no_speech_prob": 9.665689503890462e-06}, {"id": 469, "seek": 242996, "start": 2447.96, "end": 2451.96, "text": " able to load my saved models, but we'll come back and we'll cover this kind of", "tokens": [1075, 281, 3677, 452, 6624, 5245, 11, 457, 321, 603, 808, 646, 293, 321, 603, 2060, 341, 733, 295], "temperature": 0.0, "avg_logprob": -0.08919747092507102, "compression_ratio": 1.3724137931034484, "no_speech_prob": 9.665689503890462e-06}, {"id": 470, "seek": 245196, "start": 2451.96, "end": 2459.96, "text": " in more detail next time and to move on to the classifier.", "tokens": [50364, 294, 544, 2607, 958, 565, 293, 281, 1286, 322, 281, 264, 1508, 9902, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1654952694387997, "compression_ratio": 0.9666666666666667, "no_speech_prob": 3.926342469640076e-05}], "language": "en"}