{"text": " Okay. All right, so this is the repo, fastai-paperspace-setup. I've started a machine. I'll cd to my home directory. I'll get clone the repo. I'll cd into the thing I get cloned. I'll run.slash setup.sh. Okay, and it says install complete. Please start a new instance. So then I'll stop the machine. And then I'll start a machine. And that's going to install a prerun.sh script, which is going to set up all these things and all these things. And it's going to install a.bash.local script, which will set up our path. And it's going to also set up things for installing software pipi for pip install and mamba i for mamba install. So we now have a machine running. And so we should now create a terminal. So just press press terminal. Nope, something's happening. Great. Okay, try creating a terminal here then. Okay, much better. All right. So in theory, if we look at our home directory, oh, look at that. All this stuff is now symlinked to slash storage. So I should be able to pipi fastai and get the latest version. I wonder if I can add a minus u to say upgrade. Yes, I can. So that's how I get the latest version. And so that should have installed it locally. There it is. And okay, so now if I create a notebook, import fastai.fastai.version. Okay, I'm ready to go! Wow, so it's going around. Import fastai.version. Ah look, that's a good start. Okay, next question. Can we install Binary's? For example Universal Ctex. Member i for Member Install Universal Ctex. There we go. So you see the nice thing about this is even all this persistent stuff we're installing into a, you know, all works on the free paper space as well. So we should now be able to check c tags. Ta-da! It works! And which one is it? And that is actually in our storage. Oh, so I think we've done it. What do you guys think? Is that simple enough? That's good. All right, that's good. Okay, so next step is I thought we might try to fix a, I don't know if you call it fixing a bug or maybe it's probably, we could generously call it adding an enhancement to fastai, which is to add normalization to TEM models. So all right, so let's grab fastai. Now this is where, so when I git clone this, so let's go to notebooks. So slash notebooks is persistent on a particular machine. And I think this will not work because I'm using ssh. Oh, it's already there. That's interesting. Oh, you know, so there's a bug in our script, which is I didn't pop d. So let's fix that. Prerun.sh. I did a push d at the start, but no pop d at the end. Okay, fixed. All right, no worries. No worries. That means, okay, yes, we're actually in here. No worries. All right, so let's restart this. And then I'll tell you about the bug we're fixing while we wait for it. Okay, so, so normalization is where we, so normalization is where we subtract the means and divide by the standard deviation of each channel for vision. And that goes, that's a transform called normalize. And we need to use the same standard deviation and mean that was used in the, when the model was pre-trained. Because, you know, there's, you know, so some people, you know, will normalize. So it's, everything's between zero and one, someone normalize, so it's got a mean of zero and a standard deviation of one. So we need to make sure we use the same, you know, divide by the same thing to track the same thing. If you look at vision learner, vision learner has a normalized parameter. And if it's true, then it will attempt to add the correct normalization here. So if it's not a pre-trained model, it doesn't do anything because it doesn't know what to normalize by. Otherwise, it's going to try and get the correct statistics from the model's metadata. So the model's metadata is here, model underscore meta. And it's just a list of models with with metadata. And the metadata here, stats, image net stats. So the image net stats is the mean and standard deviation of image net, which I can't quite remember where that comes from, but that's something we import from somewhere. So none of these are TEM models. And so that means currently TEM models aren't normalized. Now, now, TEM has its own stats. Not this, not this. There's a lot of stuff in TEM I still haven't looked into. I actually haven't used this transforms factory. Maybe in fast AI 3, we should consider using more of this functionality from TEM. There's like a configuration for them. I guess we can just try and find it. Oh, actually, we forgot to edit this. Oops. My bad. It's not letting me start the machine. Here we go. Okay. So we can just do this locally now. All right. So this happens in Vision Learner. And TEM is optional. You don't have to use it. But if you do, then we have a create TEM model, which you don't normally call yourself. Normally, you just call Vision Learner and you pass in an architecture as a string. And if it's a string, it will create a TEM model for you. So there's best models, for example. Let's say conv next or something like that. I don't know what conv it is. Never tried that one. Let's do a tiny. So we can create a model using like create model. We pass in a string. And I have a feeling that's got a config. Here we are. Yeah. See? And it's got a mean and a standard deviation. So models equals TEM.listModels. Maybe we'll just do pre-trained ones. So I wonder if they all have this for M in models. So let's create a model. And have a look at M.defaultConfig.me. And standard deviation. Yeah. So you can see a lot of them use.5. And then some of them use image stats. And I'm guessing they're the only two options. So, okay. So hopefully you get the idea. I mean, not necessarily. Sometimes people make the minimum zero and the maximum one. But what we need to do is use the same stats that it was pre-trained with. Because we want our range to be the same as the range it was pre-trained with. Otherwise our data has a different meaning. So okay. So let's go to add norm. Okay. So here's add norm. And it's being passed to meta. Stats. So this only works for non-TIM. So how about we put this here and we'll create an else. Or I guess really an LF. And then here we'll have for TIM, if normalize, we could have a TIM normalize. We can refactor out some duplicate code later. But basically for TIM, we're going to be passing in the architecture. Oh, we don't need to pass in the architecture. We can just pass in the model. And to protect against future like ability to pass in other types that are strings that aren't TIM. Do you think there's any benefit in having like default normalization function that if you pass through, you can actually do your own normalization? No, because my answer to all of those questions is always you ain't going to need it. So I very intentionally don't do like dealing with things that may or may not happen in the future. It'd be simpler just to create your own vision liner because that looks like there's not much going on there that you could duplicate if you wanted to have support for different model. Yeah, exactly. I mean, this is just a small little wrapper really. You can call create TIM model or create vision model. You can call learner. You can call create head. Okay, so we'll call that M. So the normalize takes a mean and a standard deviation. So it should be just those two things, I guess. Like so. All right. Okay, TIM normalize using the model and pretread. Okay. I see I already had an else there. So do that. There we go. And okay. So let's test this out. So what happens when you add a transform? It adds a transform to each data loader in it. Okay. So what does that do? Oh, what did I do wrong? Just the return---- Oh, it's part of, I see. It's part of... Okay, that's a bit confusing. Right. Okay, so let's find, sometimes it's just easiest to look at the code. I see. So it's just calling add. I see for this particular event. And we're adding it. I see we're adding it to the after batch event. So we should find there's a after batch event. Here we are. I see. And there's our transforms. So if we call vision learner, that should change our data loader. Yep. And it's now got normalized using the image net stats. And if we now try it for a string version. Now that's interesting. Okay. What happened differently? Oh, I see. We need to recreate the data loaders for this test. So that doesn't have normalized anymore. And that gives us, okay, that gives us an error. And that's because it says we're passing a sequential object. Okay, that makes sense. Because create Tim model actually. Yeah, modifies things. That's why. And it creates a sequential model because it's got the head and the body in it. So we need to change how we do this. All right. This is Tim body. Here is the model. And oh, look, here we use default config to get stuff here. Interesting. So Tim body is called from here. I guess like it would be nice to know how Tim does this exactly. Where does that default config come from? So when we call Tim.createModel, set layer config. I wonder if we should take a look. Default config. Here's data config.py. So where does it get set? Maybe bottles help us. Build model with config. Well, seems like this button needs restructuring. Create vision model. Create body and create body. Here, this is where it creates the model. So let's do some, I guess, redesign maybe. But it assumes it's already instantiated. So we would remove that. So that's now not going to work, of course. So then we're creating body with model. And so then we have to instantiate that. So we may as well just do that directly, right? Maybe make this a function. So it gets new on each time. Okay. So in this refactoring, create head won't change. Or the model meta stuff doesn't change. Okay. So this changes. So now we say model equals arch pretrained. Pass in model. Okay. Looks hopeful. So we're going to do the same thing for Tim. We're going to pass in a model. Okay. So it's going to be the same here. Let's see if vision learner still works. Okay. It does. So maybe we should move, keep moving this back further and further. So to make Tim work, do that. Tim. Okay. Problem with that is the keyword arguments. So there's a lot of, this is, this gets a bit crazy. There's a lot of keyword arguments when you create a model and. And then you can actually do it. To Tim. So I think actually what we'll do. Okay. And so Tim body. Doesn't need quags anymore. What we might do. Let's say this is the result. And we'll return. Those things. Or even return. So now we've got the config. And so we can pass the config. So let's see how much we. We can pass it. Okay. So create Tim model. Yes, we do pass it in architecture after all. We just changed that back. So we should find that if we create a. The IT. And check its default config. Yep. That looks good. Okay. Now conflicts tiny on the other hand, uses image net stats. Excellent. That looks very hopeful. So I think that's a very interesting and. Valuable problem to solve. Making create unit model work with Tim would be super helpful. All right. Now create unit model. Needs to do the same thing. So let's see if we can. Instantiate the model. Is anybody potentially interested in having a go at doing unit models with Tim? If so, did you want to talk about it? I'd be interested. Okay. So. All right. Let's just get this working first. All right. So Tim, do you have any experience with using units? In general. Dynamic unit. A little bit. I'm training one at the moment. That's my maximum experience. I've been through some notebooks to walk through. And I've been able to do some. The interesting, okay. So, you know, the basic idea of a unit is. That it has. Not just the usual kind of. Downward sampling path where the image is getting kind of effectively through the input from the previous layer. And then we also take the input from the previous layer. And then we also take the input from the previous layer. And then rather than averaging those to get a vector and using those as our features for our head. Instead, we go through reverse convolutions, which are things which make it bigger and bigger. And as we do that, we also. Don't just take the input from the previous layer of the up sampling, but also the input from the equivalently sized down sample, the size down sampling. And then we also take the input from the previous layer. And then we also take the input from the previous layer. And so that's how we do it. So we had to be. Only handled a fixed size. But Keram did was he created this thing called the dynamic unit, which would look to see how big. Each size was on the downward path and automatically create. An appropriate size thing. On the upward path. And that's what the dynamic unit does. And so it's not something that's necessarily. Aggressive. In like using pre-trained models everywhere. So something we added to this idea is this idea that. The downward sampling path. Can be. Can have a pre-trained model. Which is. Not rocket science. Obviously it's like this, this one line of code. And then we also have the. So to understand like at the moment I'm using say like a ResNet 34. Does that mean the down path? Is it ResNet 34 backbone? And then there's a reverse ResNet 34 being automatically generated. And then there's a reverse ResNet 34. It's not a reverse ResNet 34. It's it is a ResNet 34 backbone. So here's our dynamic unit. The upward sample. The up sampling path. Is. Has a fixed architecture. Which is. A reverse VIT. So it's not a reverse VIT. But they're not like. If you use as a downward sampling path. You know, Downward sampling of VIT the upward sampling is not going to be a reverse VIT. It's not a mirror. No, exactly. Would there be an advantage in doing that? Or is it just not really helpful? I don't see why there would be. I'd also don't see why there wouldn't be. Nobody's tried it as far as I know. I don't even know if there's such a thing as an up sampling. Transformer block. There may well be. Without digressing. There's no need to worry about that. The key thing is that. In the downward sampling path. What we do is we. We have the downward sampling bit. We call the encoder. Okay. And what we do is we do a. A dummy of vowel. Now a dummy of vowel is basically to take a, I can't remember. Like I either a zero length batch or a one length batch, like a very small batch. And pass it through. At some image size. And. We use, I believe we use hooks. If I remember correctly. I think I remember that. What's happening to my screen. My screen's gone crazy. Okay. Yeah, so we've got these hawks. With a pie torch hawks. Yes. Okay. So we use fast AI's hawk outputs function. And we use the same. And so. What is SCCGCHG indexes? So this is, yeah. Okay. So that's a great question. So this is the. Indices of this is the key thing. This is the indices of the layers. Where the size changes. Right. Either just before that or just after that, you know. So get, get, get the indices with the size changes. So the sizes. Here. Model sizes. So we hook outputs. We do a dummy of L. And we find the shape. Of each thing. And yeah. So here you can see dummy of L is using just a single. Image. And so, yeah, this just returns the shape. Of the output of every layer. That's going to be in sizes. And so then this is just a very simple function. Which just goes through and finds. Where the size changes. Okay. And so this is the indices of those. Things. So now that we know where the size changes, we know where we want our cross connections to be. Now for each of the cross connections, we need to. Store the output of the model at that point, because that's that's going to be an input in the up sampling block. So these SFs. And so we can see that the unit block we create. So for each change in the index, for each up sampling block, you have to pass in. That. Those. Outputs. Sampling. Side. So this is the index where it happened. And so this will be the actual. So if we go to the unit block. And then we can see that the blocks get created on the other side. So it's going to be passed the hook, right? Which is. And so that that's just the hook that was used. That's the hook that was used. On the down sampling side. And from that, we can get the stored. Activations. So this is the. Shape of those stored activations. And this is a minor tweak. So let's just ignore this if block for a moment. Basically, all we then do is we take those activations, stick them through a batch norm. Concatenate them with the previous layers up sampling. And chuck that through a value. And then we do. Some cons. And the com cons aren't just coms their fast AI coms, which. Can include all kinds of things like. Activation, whatever. So it's it's a. Some combination of batch norm, you know, activation convolution. You can also do up sampling. So it's transpose. That's not can go first or last, whatever. So that's quite as you know, a very rich convolutional layer. Okay, so then this if part here is that it's possible that. Things didn't quite round off nicely so that the cross connection doesn't quite have the right size. And if that happens, then we'll interpolate. The cross connection to be the same shape as the up sampling. Connection. And again, I don't know if anybody else does this, but this is to try to make it so that. The dynamic unit always just works. That's the basic idea. Yeah, so to make this work for Tim. You know, this encoder. You know, you need to know about the spots right. But yeah, it would. You know, to figure out what doesn't work, you know, you would need to change this line to say, oh, if it's a string, create trim model otherwise do this, you know, and then you'd like create body would need to be create Tim body if it's a string so like at minimum do the same stuff that create vision model does. And then, yeah, and then see if this works. Right. And it might well. Now, I will say, if you do get it working. Tim does have an API to actually tell you where the feature sizes change. So like you could actually optimize out that dummy of L stuff but I don't even know if I'd bother because it makes the code more complex for no particular benefit. So, look, I think if you know this you commit this as a PR I'll definitely be looking at it. I was actually going to try confidence in my unit so I had no idea it wouldn't work actually so that would have been, I would have noticed that already but I just haven't had time. So I'd love to because I, you know, tried and resident 32 I've got particular results and I'd like to see we can push it with a different model. Yeah, no I mean I think there'd be a lot of benefit to that. So, all right. So now we should run the tests. Just, just to know with that all likely be in the same notebook that you're editing the vision letter is that when most of the source code is unit. Let us, or is it a different. I don't know I was just using this right, I'll find it jump jump jump to whatever automatically in Vim so I was using Vim see tags to jump around, so I don't, I have no idea where I was. Actually, so yeah so there's a models unit, where the dynamic unit lives. Okay. Is there anything unique about the fact that the team model doesn't that sort of an option there to cut the tail and head off. Does that need to be done with the unit architecture. Oh, got an error here. Yeah, so yeah, you absolutely have to cut the head off. That's because it comes with a default classifier head. So you will need, you know, so you know you once you get it working, you'll probably find you can factor out some duplicate code between the unit and the vision letter. But yeah, you basically have to cut off the classifier head in the same way that create Tim body does. And I don't think you'll need to change any input processing as far as I know. So the vision, create vision model. You know, handles, like, you know, if you've only got one or two or four channel inputs and the models are three channel input it handles that automatically, but Tim actually, I think, Ross and I independently invented this as far as I know we both kind of automatically handle like copying weights if necessary or deleting weights if necessary or whatever but yeah so the same stuff from vision when I should should work there as well. So interestingly layers, the layers notebook doesn't work because it's actually creating a model, which is curious. It's easily fixed. Yeah, that's interesting. Okay. Okay. So, the big question then is, can we still predict race disease. Okay. So, I don't know if it's going to make much difference or not. You know, because we're pretty careful about fine tuning the batch norm layers. It would actually be interesting to see whether normalization matters as much as it used to. It used to be absolutely critical. So, it's like a layer that learns the normalization sort of thing. Yeah, I mean that's basically what batch norm does, you know, to understand it's a those weights in the batch norm layer basically learning the aggregate of that batch that optimally give the best activations for the next layer. Exactly. Yeah, yeah, it's just, it's just, you know, multiply by something and add something. So it's finding what's the best thing to multiply by and add by. So, let's take a look. So I mean, alright, so this got 47% error. Yeah. So I mean, it's a bit disappointing after all that work it doesn't actually, I mean this is fascinating, like, yeah, when you find tune the way we do. Basically doesn't really matter, you know. And let's just double check it actually is. It actually is working. Would it be fair to say that the one advantage would be if you wanted to use pre trained models without fine tuning you definitely want the statistics in there right. Yes, absolutely. I mean, I don't know if that's an actual thing that people do. But yes, if you did. Alright, so we did deals train after batch. Yep. There it is. Groovy. Yeah, it's funny these things that you know we've been doing for years and I guess never question. I have a question relating to that because one of the things I wanted to do is get this unit into a mobile app so I use the latest torch script, and it works with the demo app to fill around the locks is broken from pytorch. But of course in there you need to provide the averaging statistics for the app. So it's like inference mode. So I wonder, I know that at the moment, the fast AI is kind of idea is that you dump everything is like a pickle, but conceivably it would be helpful if you could maybe extract those new fine tuned statistics or something for your deployment in particular environments, because that, how would I go about doing that. I mean, they're just parameters and batch normally is, you know, they're just parameters. So there'll be in the parameters attribute of the model. But like they're not, they're not really parameters that makes sense independently of all the other parameters at all. So I don't think you would treat them any differently. If you use, say, image nets statistics when you're fine tuning and that's the result of your model right you're going to use that down the track as well. Well, yes and no, like that's what you normalize with but, but you've got batch normal layers which then obviously dividing and subtracting themselves. So yeah, I mean, you're those normalization stats are going to change but there isn't really any reason to, you know, it would only be if you trained a new model from scratch. I just want to have a look at this next one. So this is 27 to 1824. Yeah, this is actually kind of what I thought might happen is on a slightly better model, you know, we may be getting slightly better errors, initially, then as it trains a bit. Makes no difference. Cool. Alright, so, yeah, I'd love people to try out faster AI from master because many of your models look substantially better or even more important substantially worse or to normalize Tim models. Okay. This is 3716. Does anybody have any questions before we wrap it up. Just with normalize. It's just the initial error it will be a bit more less than earlier approach right. Yeah, so like that that that, you know, well, at first you have a random head. It doesn't actually matter right it randoms random whether you normalize or not. So, maybe you know the fastest 10 batches. It's better or something. But, yeah, I don't know, like, I mean, be interesting to see if anybody notices a difference. It's just this used to matter a lot right for a couple of reasons. One is that most people didn't find chain models most people train most models and scratch until until fast AI came along, pretty much. And then secondly, what we didn't have batch norm. Right, so it was totally critical. And then even when batch norm came along we didn't know how to find your models with batch norm. So we just fine tuned the head. At that point, we didn't realize that you had to find tune the batch norm layers as well. So, I remember emailing Francois the creator of care us and I was saying to him like I'm trying to find tune your care us model and it's like bizarrely bad like why why is that. I'm probably doing the wrong thing here's documentation whatever like no I'm pretty sure I'm doing the right thing and I spent like three months, trying to answer this question. Eventually I realized it's like, holy shit it's the best normalize I sent him an email and said oh we can't find tune care us models like this, which you have to find in batch normalize, which I don't think they changed for years. Anyway, so those there so those changes is why I guess this whole normalization layer thing is much less interesting than I guess we thought, which is why we hadn't really noticed it wasn't working before. Because I'm also training fine. Anybody else have any questions before we wrap up. Okay. Thank you. Let's see you all. Good luck with unit. Bye.", "segments": [{"id": 0, "seek": 0, "start": 0.48, "end": 2.98, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.24595776875813802, "compression_ratio": 1.4817518248175183, "no_speech_prob": 0.02158377319574356}, {"id": 1, "seek": 0, "start": 6.0, "end": 11.6, "text": " All right, so this is the repo, fastai-paperspace-setup. I've started a machine.", "tokens": [1057, 558, 11, 370, 341, 307, 264, 49040, 11, 2370, 1301, 12, 79, 14441, 17940, 12, 3854, 1010, 13, 286, 600, 1409, 257, 3479, 13], "temperature": 0.0, "avg_logprob": -0.24595776875813802, "compression_ratio": 1.4817518248175183, "no_speech_prob": 0.02158377319574356}, {"id": 2, "seek": 0, "start": 13.200000000000001, "end": 16.48, "text": " I'll cd to my home directory. I'll get clone the repo.", "tokens": [286, 603, 269, 67, 281, 452, 1280, 21120, 13, 286, 603, 483, 26506, 264, 49040, 13], "temperature": 0.0, "avg_logprob": -0.24595776875813802, "compression_ratio": 1.4817518248175183, "no_speech_prob": 0.02158377319574356}, {"id": 3, "seek": 0, "start": 19.28, "end": 24.32, "text": " I'll cd into the thing I get cloned. I'll run.slash setup.sh.", "tokens": [286, 603, 269, 67, 666, 264, 551, 286, 483, 596, 19009, 13, 286, 603, 1190, 2411, 10418, 1299, 8657, 13, 2716, 13], "temperature": 0.0, "avg_logprob": -0.24595776875813802, "compression_ratio": 1.4817518248175183, "no_speech_prob": 0.02158377319574356}, {"id": 4, "seek": 2432, "start": 24.32, "end": 30.16, "text": " Okay, and it says install complete. Please start a new instance. So then I'll stop the machine.", "tokens": [1033, 11, 293, 309, 1619, 3625, 3566, 13, 2555, 722, 257, 777, 5197, 13, 407, 550, 286, 603, 1590, 264, 3479, 13], "temperature": 0.0, "avg_logprob": -0.27535216013590497, "compression_ratio": 1.2989690721649485, "no_speech_prob": 3.479518272797577e-05}, {"id": 5, "seek": 2432, "start": 35.2, "end": 43.6, "text": " And then I'll start a machine.", "tokens": [400, 550, 286, 603, 722, 257, 3479, 13], "temperature": 0.0, "avg_logprob": -0.27535216013590497, "compression_ratio": 1.2989690721649485, "no_speech_prob": 3.479518272797577e-05}, {"id": 6, "seek": 4360, "start": 43.6, "end": 59.2, "text": " And that's going to install a prerun.sh script, which is going to set up all these things and all these things.", "tokens": [400, 300, 311, 516, 281, 3625, 257, 582, 260, 409, 13, 2716, 5755, 11, 597, 307, 516, 281, 992, 493, 439, 613, 721, 293, 439, 613, 721, 13], "temperature": 0.0, "avg_logprob": -0.2360180412850729, "compression_ratio": 2.0067567567567566, "no_speech_prob": 1.5205680028884672e-05}, {"id": 7, "seek": 4360, "start": 60.32, "end": 67.68, "text": " And it's going to install a.bash.local script, which will set up our path. And it's going to", "tokens": [400, 309, 311, 516, 281, 3625, 257, 2411, 65, 1299, 13, 5842, 304, 5755, 11, 597, 486, 992, 493, 527, 3100, 13, 400, 309, 311, 516, 281], "temperature": 0.0, "avg_logprob": -0.2360180412850729, "compression_ratio": 2.0067567567567566, "no_speech_prob": 1.5205680028884672e-05}, {"id": 8, "seek": 6768, "start": 67.68, "end": 74.80000000000001, "text": " also set up things for installing software pipi for pip install and mamba i for mamba install.", "tokens": [611, 992, 493, 721, 337, 20762, 4722, 8489, 72, 337, 8489, 3625, 293, 275, 23337, 741, 337, 275, 23337, 3625, 13], "temperature": 0.0, "avg_logprob": -0.2502729191499598, "compression_ratio": 1.3617021276595744, "no_speech_prob": 1.450641684641596e-05}, {"id": 9, "seek": 7480, "start": 74.8, "end": 99.84, "text": " So we now have a machine running. And so we should now create a terminal.", "tokens": [50364, 407, 321, 586, 362, 257, 3479, 2614, 13, 400, 370, 321, 820, 586, 1884, 257, 14709, 13, 51616], "temperature": 0.0, "avg_logprob": -0.253031849861145, "compression_ratio": 1.028169014084507, "no_speech_prob": 6.70564259053208e-05}, {"id": 10, "seek": 10480, "start": 105.52, "end": 113.84, "text": " So just press press terminal.", "tokens": [407, 445, 1886, 1886, 14709, 13], "temperature": 0.0, "avg_logprob": -0.7115417003631592, "compression_ratio": 0.9354838709677419, "no_speech_prob": 0.014055026695132256}, {"id": 11, "seek": 11384, "start": 113.84, "end": 138.8, "text": " Nope, something's happening. Great. Okay, try creating a terminal here then. Okay, much better.", "tokens": [12172, 11, 746, 311, 2737, 13, 3769, 13, 1033, 11, 853, 4084, 257, 14709, 510, 550, 13, 1033, 11, 709, 1101, 13], "temperature": 0.0, "avg_logprob": -0.27471054517305815, "compression_ratio": 1.1176470588235294, "no_speech_prob": 4.465243182494305e-05}, {"id": 12, "seek": 13880, "start": 138.8, "end": 143.36, "text": " All right. So in theory, if we look at our home directory,", "tokens": [1057, 558, 13, 407, 294, 5261, 11, 498, 321, 574, 412, 527, 1280, 21120, 11], "temperature": 0.0, "avg_logprob": -0.24064013832493833, "compression_ratio": 1.3597122302158273, "no_speech_prob": 8.799592251307331e-06}, {"id": 13, "seek": 13880, "start": 143.84, "end": 148.48000000000002, "text": " oh, look at that. All this stuff is now symlinked to slash storage.", "tokens": [1954, 11, 574, 412, 300, 13, 1057, 341, 1507, 307, 586, 6697, 22473, 292, 281, 17330, 6725, 13], "temperature": 0.0, "avg_logprob": -0.24064013832493833, "compression_ratio": 1.3597122302158273, "no_speech_prob": 8.799592251307331e-06}, {"id": 14, "seek": 13880, "start": 150.56, "end": 154.48000000000002, "text": " So I should be able to pipi fastai and get the latest version.", "tokens": [407, 286, 820, 312, 1075, 281, 8489, 72, 2370, 1301, 293, 483, 264, 6792, 3037, 13], "temperature": 0.0, "avg_logprob": -0.24064013832493833, "compression_ratio": 1.3597122302158273, "no_speech_prob": 8.799592251307331e-06}, {"id": 15, "seek": 15448, "start": 154.48, "end": 162.72, "text": " I wonder if I can add a minus u to say upgrade.", "tokens": [286, 2441, 498, 286, 393, 909, 257, 3175, 344, 281, 584, 11484, 13], "temperature": 0.0, "avg_logprob": -0.35806839965110604, "compression_ratio": 1.25, "no_speech_prob": 4.092675226274878e-06}, {"id": 16, "seek": 15448, "start": 164.88, "end": 172.0, "text": " Yes, I can. So that's how I get the latest version. And so that should have installed it locally.", "tokens": [1079, 11, 286, 393, 13, 407, 300, 311, 577, 286, 483, 264, 6792, 3037, 13, 400, 370, 300, 820, 362, 8899, 309, 16143, 13], "temperature": 0.0, "avg_logprob": -0.35806839965110604, "compression_ratio": 1.25, "no_speech_prob": 4.092675226274878e-06}, {"id": 17, "seek": 17200, "start": 172.0, "end": 188.48, "text": " There it is. And okay, so now if I create a notebook,", "tokens": [821, 309, 307, 13, 400, 1392, 11, 370, 586, 498, 286, 1884, 257, 21060, 11], "temperature": 0.0, "avg_logprob": -0.5276470184326172, "compression_ratio": 0.8833333333333333, "no_speech_prob": 1.679682100075297e-06}, {"id": 18, "seek": 18848, "start": 188.48, "end": 204.95999999999998, "text": " import fastai.fastai.version. Okay, I'm ready to go! Wow, so it's going around.", "tokens": [974, 2370, 1301, 13, 69, 525, 1301, 13, 29153, 13, 1033, 11, 286, 478, 1919, 281, 352, 0, 3153, 11, 370, 309, 311, 516, 926, 13], "temperature": 1.0, "avg_logprob": -2.028972371419271, "compression_ratio": 1.0, "no_speech_prob": 1.2410193448886275e-05}, {"id": 19, "seek": 20496, "start": 204.96, "end": 217.60000000000002, "text": " Import fastai.version.", "tokens": [26391, 2370, 1301, 13, 29153, 13], "temperature": 0.0, "avg_logprob": -0.44490926019076643, "compression_ratio": 1.0232558139534884, "no_speech_prob": 0.03303436189889908}, {"id": 20, "seek": 20496, "start": 220.0, "end": 225.20000000000002, "text": " Ah look, that's a good start. Okay, next question. Can we install", "tokens": [2438, 574, 11, 300, 311, 257, 665, 722, 13, 1033, 11, 958, 1168, 13, 1664, 321, 3625], "temperature": 0.0, "avg_logprob": -0.44490926019076643, "compression_ratio": 1.0232558139534884, "no_speech_prob": 0.03303436189889908}, {"id": 21, "seek": 22520, "start": 225.2, "end": 245.83999999999997, "text": " Binary's? For example Universal Ctex. Member i for Member Install Universal Ctex.", "tokens": [50364, 363, 4066, 311, 30, 1171, 1365, 22617, 383, 975, 87, 13, 16037, 741, 337, 16037, 31982, 22617, 383, 975, 87, 13, 51396], "temperature": 0.0, "avg_logprob": -0.8424418767293295, "compression_ratio": 1.173913043478261, "no_speech_prob": 0.0006208730046637356}, {"id": 22, "seek": 25520, "start": 256.0, "end": 260.64, "text": " There we go. So you see the nice thing about this is even all this persistent", "tokens": [821, 321, 352, 13, 407, 291, 536, 264, 1481, 551, 466, 341, 307, 754, 439, 341, 24315], "temperature": 0.0, "avg_logprob": -0.2066770204356019, "compression_ratio": 1.4260355029585798, "no_speech_prob": 0.0018094771075993776}, {"id": 23, "seek": 25520, "start": 261.76, "end": 266.8, "text": " stuff we're installing into a, you know, all works on the free paper space as well.", "tokens": [1507, 321, 434, 20762, 666, 257, 11, 291, 458, 11, 439, 1985, 322, 264, 1737, 3035, 1901, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.2066770204356019, "compression_ratio": 1.4260355029585798, "no_speech_prob": 0.0018094771075993776}, {"id": 24, "seek": 25520, "start": 270.4, "end": 272.64, "text": " So we should now be able to check c tags.", "tokens": [407, 321, 820, 586, 312, 1075, 281, 1520, 269, 18632, 13], "temperature": 0.0, "avg_logprob": -0.2066770204356019, "compression_ratio": 1.4260355029585798, "no_speech_prob": 0.0018094771075993776}, {"id": 25, "seek": 25520, "start": 275.68, "end": 279.28, "text": " Ta-da! It works! And which one is it?", "tokens": [6551, 12, 2675, 0, 467, 1985, 0, 400, 597, 472, 307, 309, 30], "temperature": 0.0, "avg_logprob": -0.2066770204356019, "compression_ratio": 1.4260355029585798, "no_speech_prob": 0.0018094771075993776}, {"id": 26, "seek": 27928, "start": 279.28, "end": 288.15999999999997, "text": " And that is actually in our storage. Oh, so I think we've done it.", "tokens": [400, 300, 307, 767, 294, 527, 6725, 13, 876, 11, 370, 286, 519, 321, 600, 1096, 309, 13], "temperature": 0.0, "avg_logprob": -0.18991632814760562, "compression_ratio": 1.2932330827067668, "no_speech_prob": 2.0141986169619486e-05}, {"id": 27, "seek": 27928, "start": 290.15999999999997, "end": 293.52, "text": " What do you guys think? Is that simple enough?", "tokens": [708, 360, 291, 1074, 519, 30, 1119, 300, 2199, 1547, 30], "temperature": 0.0, "avg_logprob": -0.18991632814760562, "compression_ratio": 1.2932330827067668, "no_speech_prob": 2.0141986169619486e-05}, {"id": 28, "seek": 27928, "start": 296.79999999999995, "end": 305.59999999999997, "text": " That's good. All right, that's good. Okay, so next step is", "tokens": [663, 311, 665, 13, 1057, 558, 11, 300, 311, 665, 13, 1033, 11, 370, 958, 1823, 307], "temperature": 0.0, "avg_logprob": -0.18991632814760562, "compression_ratio": 1.2932330827067668, "no_speech_prob": 2.0141986169619486e-05}, {"id": 29, "seek": 30560, "start": 305.6, "end": 312.64000000000004, "text": " I thought we might try to fix a, I don't know if you call it fixing a bug or maybe it's probably,", "tokens": [286, 1194, 321, 1062, 853, 281, 3191, 257, 11, 286, 500, 380, 458, 498, 291, 818, 309, 19442, 257, 7426, 420, 1310, 309, 311, 1391, 11], "temperature": 0.0, "avg_logprob": -0.31641491976651276, "compression_ratio": 1.3533333333333333, "no_speech_prob": 2.752761974988971e-05}, {"id": 30, "seek": 30560, "start": 312.64000000000004, "end": 322.88, "text": " we could generously call it adding an enhancement to fastai, which is to add normalization to TEM models.", "tokens": [321, 727, 48983, 818, 309, 5127, 364, 40776, 281, 2370, 1301, 11, 597, 307, 281, 909, 2710, 2144, 281, 314, 6683, 5245, 13], "temperature": 0.0, "avg_logprob": -0.31641491976651276, "compression_ratio": 1.3533333333333333, "no_speech_prob": 2.752761974988971e-05}, {"id": 31, "seek": 32288, "start": 322.88, "end": 335.44, "text": " So all right, so let's grab fastai.", "tokens": [407, 439, 558, 11, 370, 718, 311, 4444, 2370, 1301, 13], "temperature": 0.0, "avg_logprob": -0.49413121979812097, "compression_ratio": 1.0256410256410255, "no_speech_prob": 5.0932731028296985e-06}, {"id": 32, "seek": 32288, "start": 338.15999999999997, "end": 344.88, "text": " Now this is where, so when I git clone this,", "tokens": [823, 341, 307, 689, 11, 370, 562, 286, 18331, 26506, 341, 11], "temperature": 0.0, "avg_logprob": -0.49413121979812097, "compression_ratio": 1.0256410256410255, "no_speech_prob": 5.0932731028296985e-06}, {"id": 33, "seek": 34488, "start": 344.88, "end": 350.71999999999997, "text": " so let's go to notebooks. So slash notebooks is persistent on a particular machine.", "tokens": [370, 718, 311, 352, 281, 43782, 13, 407, 17330, 43782, 307, 24315, 322, 257, 1729, 3479, 13], "temperature": 0.0, "avg_logprob": -0.39305904388427737, "compression_ratio": 1.3211678832116789, "no_speech_prob": 9.97158986137947e-06}, {"id": 34, "seek": 34488, "start": 352.24, "end": 357.36, "text": " And I think this will not work because I'm using ssh. Oh, it's already there.", "tokens": [400, 286, 519, 341, 486, 406, 589, 570, 286, 478, 1228, 262, 2716, 13, 876, 11, 309, 311, 1217, 456, 13], "temperature": 0.0, "avg_logprob": -0.39305904388427737, "compression_ratio": 1.3211678832116789, "no_speech_prob": 9.97158986137947e-06}, {"id": 35, "seek": 34488, "start": 362.0, "end": 365.52, "text": " That's interesting.", "tokens": [663, 311, 1880, 13], "temperature": 0.0, "avg_logprob": -0.39305904388427737, "compression_ratio": 1.3211678832116789, "no_speech_prob": 9.97158986137947e-06}, {"id": 36, "seek": 36552, "start": 365.52, "end": 376.88, "text": " Oh, you know, so there's a bug in our script, which is I didn't pop d.", "tokens": [876, 11, 291, 458, 11, 370, 456, 311, 257, 7426, 294, 527, 5755, 11, 597, 307, 286, 994, 380, 1665, 274, 13], "temperature": 0.0, "avg_logprob": -0.16538884346945243, "compression_ratio": 1.2727272727272727, "no_speech_prob": 3.0893108942109393e-06}, {"id": 37, "seek": 36552, "start": 381.03999999999996, "end": 388.32, "text": " So let's fix that. Prerun.sh. I did a push d at the start, but no pop d at the end.", "tokens": [407, 718, 311, 3191, 300, 13, 2114, 260, 409, 13, 2716, 13, 286, 630, 257, 2944, 274, 412, 264, 722, 11, 457, 572, 1665, 274, 412, 264, 917, 13], "temperature": 0.0, "avg_logprob": -0.16538884346945243, "compression_ratio": 1.2727272727272727, "no_speech_prob": 3.0893108942109393e-06}, {"id": 38, "seek": 38832, "start": 388.32, "end": 398.64, "text": " Okay, fixed. All right, no worries.", "tokens": [1033, 11, 6806, 13, 1057, 558, 11, 572, 16340, 13], "temperature": 0.0, "avg_logprob": -0.6754204205104283, "compression_ratio": 0.813953488372093, "no_speech_prob": 2.429549567750655e-05}, {"id": 39, "seek": 39864, "start": 398.64, "end": 402.0, "text": " No worries.", "tokens": [883, 16340, 13], "temperature": 0.0, "avg_logprob": -0.3064439495404561, "compression_ratio": 1.0294117647058822, "no_speech_prob": 1.892034379125107e-05}, {"id": 40, "seek": 40200, "start": 402.0, "end": 413.52, "text": " That means, okay, yes, we're actually in here. No worries.", "tokens": [663, 1355, 11, 1392, 11, 2086, 11, 321, 434, 767, 294, 510, 13, 883, 16340, 13], "temperature": 0.0, "avg_logprob": -0.3357306798299154, "compression_ratio": 1.069767441860465, "no_speech_prob": 4.221737526677316e-06}, {"id": 41, "seek": 41352, "start": 413.52, "end": 430.79999999999995, "text": " All right, so let's restart this. And then I'll tell you about the bug we're fixing while we wait for it.", "tokens": [1057, 558, 11, 370, 718, 311, 21022, 341, 13, 400, 550, 286, 603, 980, 291, 466, 264, 7426, 321, 434, 19442, 1339, 321, 1699, 337, 309, 13], "temperature": 0.0, "avg_logprob": -0.5383246006109775, "compression_ratio": 1.1904761904761905, "no_speech_prob": 1.0615373867040034e-05}, {"id": 42, "seek": 43080, "start": 430.8, "end": 452.16, "text": " Okay, so, so normalization is where we,", "tokens": [1033, 11, 370, 11, 370, 2710, 2144, 307, 689, 321, 11], "temperature": 0.0, "avg_logprob": -0.7485713958740234, "compression_ratio": 0.8863636363636364, "no_speech_prob": 2.110409695887938e-05}, {"id": 43, "seek": 45216, "start": 452.16, "end": 460.8, "text": " so normalization is where we subtract the means and divide by the standard deviation of each channel", "tokens": [370, 2710, 2144, 307, 689, 321, 16390, 264, 1355, 293, 9845, 538, 264, 3832, 25163, 295, 1184, 2269], "temperature": 0.0, "avg_logprob": -0.28574443900066876, "compression_ratio": 1.4328358208955223, "no_speech_prob": 7.181580713222502e-06}, {"id": 44, "seek": 45216, "start": 461.44, "end": 478.48, "text": " for vision. And that goes, that's a transform called normalize. And we need to use the same", "tokens": [337, 5201, 13, 400, 300, 1709, 11, 300, 311, 257, 4088, 1219, 2710, 1125, 13, 400, 321, 643, 281, 764, 264, 912], "temperature": 0.0, "avg_logprob": -0.28574443900066876, "compression_ratio": 1.4328358208955223, "no_speech_prob": 7.181580713222502e-06}, {"id": 45, "seek": 47848, "start": 478.48, "end": 484.88, "text": " standard deviation and mean that was used in the, when the model was pre-trained.", "tokens": [3832, 25163, 293, 914, 300, 390, 1143, 294, 264, 11, 562, 264, 2316, 390, 659, 12, 17227, 2001, 13], "temperature": 0.0, "avg_logprob": -0.22957859617291074, "compression_ratio": 1.778894472361809, "no_speech_prob": 1.921916009450797e-05}, {"id": 46, "seek": 47848, "start": 486.72, "end": 491.44, "text": " Because, you know, there's, you know, so some people, you know, will normalize. So it's,", "tokens": [1436, 11, 291, 458, 11, 456, 311, 11, 291, 458, 11, 370, 512, 561, 11, 291, 458, 11, 486, 2710, 1125, 13, 407, 309, 311, 11], "temperature": 0.0, "avg_logprob": -0.22957859617291074, "compression_ratio": 1.778894472361809, "no_speech_prob": 1.921916009450797e-05}, {"id": 47, "seek": 47848, "start": 491.44, "end": 495.76, "text": " everything's between zero and one, someone normalize, so it's got a mean of zero and a", "tokens": [1203, 311, 1296, 4018, 293, 472, 11, 1580, 2710, 1125, 11, 370, 309, 311, 658, 257, 914, 295, 4018, 293, 257], "temperature": 0.0, "avg_logprob": -0.22957859617291074, "compression_ratio": 1.778894472361809, "no_speech_prob": 1.921916009450797e-05}, {"id": 48, "seek": 47848, "start": 495.76, "end": 503.6, "text": " standard deviation of one. So we need to make sure we use the same, you know, divide by the same", "tokens": [3832, 25163, 295, 472, 13, 407, 321, 643, 281, 652, 988, 321, 764, 264, 912, 11, 291, 458, 11, 9845, 538, 264, 912], "temperature": 0.0, "avg_logprob": -0.22957859617291074, "compression_ratio": 1.778894472361809, "no_speech_prob": 1.921916009450797e-05}, {"id": 49, "seek": 50360, "start": 503.6, "end": 512.0, "text": " thing to track the same thing. If you look at vision learner,", "tokens": [551, 281, 2837, 264, 912, 551, 13, 759, 291, 574, 412, 5201, 33347, 11], "temperature": 0.0, "avg_logprob": -0.1840089910170611, "compression_ratio": 1.5568862275449102, "no_speech_prob": 6.048739578545792e-06}, {"id": 50, "seek": 50360, "start": 516.32, "end": 522.8000000000001, "text": " vision learner has a normalized parameter. And if it's true, then it will attempt to add the correct", "tokens": [5201, 33347, 575, 257, 48704, 13075, 13, 400, 498, 309, 311, 2074, 11, 550, 309, 486, 5217, 281, 909, 264, 3006], "temperature": 0.0, "avg_logprob": -0.1840089910170611, "compression_ratio": 1.5568862275449102, "no_speech_prob": 6.048739578545792e-06}, {"id": 51, "seek": 50360, "start": 522.8000000000001, "end": 533.52, "text": " normalization here. So if it's not a pre-trained model, it doesn't do anything because it doesn't", "tokens": [2710, 2144, 510, 13, 407, 498, 309, 311, 406, 257, 659, 12, 17227, 2001, 2316, 11, 309, 1177, 380, 360, 1340, 570, 309, 1177, 380], "temperature": 0.0, "avg_logprob": -0.1840089910170611, "compression_ratio": 1.5568862275449102, "no_speech_prob": 6.048739578545792e-06}, {"id": 52, "seek": 53352, "start": 533.52, "end": 538.0, "text": " know what to normalize by. Otherwise, it's going to try and get the correct statistics", "tokens": [458, 437, 281, 2710, 1125, 538, 13, 10328, 11, 309, 311, 516, 281, 853, 293, 483, 264, 3006, 12523], "temperature": 0.0, "avg_logprob": -0.1016007176151982, "compression_ratio": 1.474820143884892, "no_speech_prob": 1.1478124179120641e-05}, {"id": 53, "seek": 53352, "start": 538.8, "end": 544.0, "text": " from the model's metadata. So the model's metadata is here, model underscore meta.", "tokens": [490, 264, 2316, 311, 26603, 13, 407, 264, 2316, 311, 26603, 307, 510, 11, 2316, 37556, 19616, 13], "temperature": 0.0, "avg_logprob": -0.1016007176151982, "compression_ratio": 1.474820143884892, "no_speech_prob": 1.1478124179120641e-05}, {"id": 54, "seek": 53352, "start": 548.3199999999999, "end": 551.92, "text": " And it's just a list of models with", "tokens": [400, 309, 311, 445, 257, 1329, 295, 5245, 365], "temperature": 0.0, "avg_logprob": -0.1016007176151982, "compression_ratio": 1.474820143884892, "no_speech_prob": 1.1478124179120641e-05}, {"id": 55, "seek": 55192, "start": 551.92, "end": 566.0799999999999, "text": " with metadata. And the metadata here, stats, image net stats. So the image net stats is the mean and", "tokens": [365, 26603, 13, 400, 264, 26603, 510, 11, 18152, 11, 3256, 2533, 18152, 13, 407, 264, 3256, 2533, 18152, 307, 264, 914, 293], "temperature": 0.0, "avg_logprob": -0.15333421465376734, "compression_ratio": 1.6187845303867403, "no_speech_prob": 5.4220054153120145e-06}, {"id": 56, "seek": 55192, "start": 566.0799999999999, "end": 571.76, "text": " standard deviation of image net, which I can't quite remember where that comes from, but that's", "tokens": [3832, 25163, 295, 3256, 2533, 11, 597, 286, 393, 380, 1596, 1604, 689, 300, 1487, 490, 11, 457, 300, 311], "temperature": 0.0, "avg_logprob": -0.15333421465376734, "compression_ratio": 1.6187845303867403, "no_speech_prob": 5.4220054153120145e-06}, {"id": 57, "seek": 55192, "start": 573.1999999999999, "end": 580.0799999999999, "text": " something we import from somewhere. So none of these are TEM models. And so that means currently", "tokens": [746, 321, 974, 490, 4079, 13, 407, 6022, 295, 613, 366, 314, 6683, 5245, 13, 400, 370, 300, 1355, 4362], "temperature": 0.0, "avg_logprob": -0.15333421465376734, "compression_ratio": 1.6187845303867403, "no_speech_prob": 5.4220054153120145e-06}, {"id": 58, "seek": 58008, "start": 580.08, "end": 587.36, "text": " TEM models aren't normalized. Now,", "tokens": [314, 6683, 5245, 3212, 380, 48704, 13, 823, 11], "temperature": 0.0, "avg_logprob": -0.6040967794565054, "compression_ratio": 0.8095238095238095, "no_speech_prob": 2.796524131554179e-05}, {"id": 59, "seek": 58736, "start": 587.36, "end": 603.36, "text": " now, TEM has its own stats.", "tokens": [586, 11, 314, 6683, 575, 1080, 1065, 18152, 13], "temperature": 0.0, "avg_logprob": -0.5198020568260779, "compression_ratio": 0.7714285714285715, "no_speech_prob": 2.8850850867456757e-05}, {"id": 60, "seek": 60336, "start": 603.36, "end": 613.84, "text": " Not this, not this.", "tokens": [50364, 1726, 341, 11, 406, 341, 13, 50888], "temperature": 0.0, "avg_logprob": -0.8245290650261773, "compression_ratio": 0.9047619047619048, "no_speech_prob": 0.00017864069377537817}, {"id": 61, "seek": 63336, "start": 634.08, "end": 638.88, "text": " There's a lot of stuff in TEM I still haven't looked into. I actually haven't used this", "tokens": [821, 311, 257, 688, 295, 1507, 294, 314, 6683, 286, 920, 2378, 380, 2956, 666, 13, 286, 767, 2378, 380, 1143, 341], "temperature": 0.0, "avg_logprob": -0.22098146166120256, "compression_ratio": 1.2945205479452055, "no_speech_prob": 0.0017001861706376076}, {"id": 62, "seek": 63336, "start": 638.88, "end": 654.08, "text": " transforms factory. Maybe in fast AI 3, we should consider using more of this functionality from TEM.", "tokens": [35592, 9265, 13, 2704, 294, 2370, 7318, 805, 11, 321, 820, 1949, 1228, 544, 295, 341, 14980, 490, 314, 6683, 13], "temperature": 0.0, "avg_logprob": -0.22098146166120256, "compression_ratio": 1.2945205479452055, "no_speech_prob": 0.0017001861706376076}, {"id": 63, "seek": 65408, "start": 654.08, "end": 663.5200000000001, "text": " There's like a", "tokens": [821, 311, 411, 257], "temperature": 0.0, "avg_logprob": -0.2810821533203125, "compression_ratio": 1.0, "no_speech_prob": 9.079254596144892e-06}, {"id": 64, "seek": 65408, "start": 665.84, "end": 672.5600000000001, "text": " configuration for them. I guess we can just try and find it.", "tokens": [11694, 337, 552, 13, 286, 2041, 321, 393, 445, 853, 293, 915, 309, 13], "temperature": 0.0, "avg_logprob": -0.2810821533203125, "compression_ratio": 1.0, "no_speech_prob": 9.079254596144892e-06}, {"id": 65, "seek": 67256, "start": 672.56, "end": 684.4799999999999, "text": " Oh, actually, we forgot to edit this.", "tokens": [876, 11, 767, 11, 321, 5298, 281, 8129, 341, 13], "temperature": 0.0, "avg_logprob": -0.4397016631232368, "compression_ratio": 0.8775510204081632, "no_speech_prob": 3.2171650673262775e-05}, {"id": 66, "seek": 67256, "start": 689.04, "end": 691.04, "text": " Oops.", "tokens": [21726, 13], "temperature": 0.0, "avg_logprob": -0.4397016631232368, "compression_ratio": 0.8775510204081632, "no_speech_prob": 3.2171650673262775e-05}, {"id": 67, "seek": 69104, "start": 691.04, "end": 698.64, "text": " My bad.", "tokens": [50364, 1222, 1578, 13, 50744], "temperature": 0.0, "avg_logprob": -0.8114741643269857, "compression_ratio": 0.4666666666666667, "no_speech_prob": 0.00584068801254034}, {"id": 68, "seek": 72104, "start": 721.1999999999999, "end": 729.28, "text": " It's not letting me start the machine. Here we go. Okay. So we can just do this locally now.", "tokens": [467, 311, 406, 8295, 385, 722, 264, 3479, 13, 1692, 321, 352, 13, 1033, 13, 407, 321, 393, 445, 360, 341, 16143, 586, 13], "temperature": 0.0, "avg_logprob": -0.2763966118417135, "compression_ratio": 1.1896551724137931, "no_speech_prob": 0.010324754752218723}, {"id": 69, "seek": 72104, "start": 737.28, "end": 742.64, "text": " All right. So this happens in Vision Learner.", "tokens": [1057, 558, 13, 407, 341, 2314, 294, 25170, 17216, 260, 13], "temperature": 0.0, "avg_logprob": -0.2763966118417135, "compression_ratio": 1.1896551724137931, "no_speech_prob": 0.010324754752218723}, {"id": 70, "seek": 74264, "start": 742.64, "end": 751.36, "text": " And TEM is optional. You don't have to use it.", "tokens": [400, 314, 6683, 307, 17312, 13, 509, 500, 380, 362, 281, 764, 309, 13], "temperature": 0.0, "avg_logprob": -0.10147472740947336, "compression_ratio": 1.4457831325301205, "no_speech_prob": 5.593913101620274e-06}, {"id": 71, "seek": 74264, "start": 755.36, "end": 763.1999999999999, "text": " But if you do, then we have a create TEM model, which you don't normally call yourself. Normally,", "tokens": [583, 498, 291, 360, 11, 550, 321, 362, 257, 1884, 314, 6683, 2316, 11, 597, 291, 500, 380, 5646, 818, 1803, 13, 17424, 11], "temperature": 0.0, "avg_logprob": -0.10147472740947336, "compression_ratio": 1.4457831325301205, "no_speech_prob": 5.593913101620274e-06}, {"id": 72, "seek": 74264, "start": 763.1999999999999, "end": 768.16, "text": " you just call Vision Learner and you pass in an architecture as a string. And if it's a string,", "tokens": [291, 445, 818, 25170, 17216, 260, 293, 291, 1320, 294, 364, 9482, 382, 257, 6798, 13, 400, 498, 309, 311, 257, 6798, 11], "temperature": 0.0, "avg_logprob": -0.10147472740947336, "compression_ratio": 1.4457831325301205, "no_speech_prob": 5.593913101620274e-06}, {"id": 73, "seek": 76816, "start": 768.16, "end": 780.16, "text": " it will create a TEM model for you. So there's best models, for example.", "tokens": [309, 486, 1884, 257, 314, 6683, 2316, 337, 291, 13, 407, 456, 311, 1151, 5245, 11, 337, 1365, 13], "temperature": 0.0, "avg_logprob": -0.1645897456577846, "compression_ratio": 1.2923076923076924, "no_speech_prob": 1.0288161320204381e-05}, {"id": 74, "seek": 76816, "start": 781.92, "end": 788.16, "text": " Let's say conv next or something like that. I don't know what conv it is. Never tried that one.", "tokens": [961, 311, 584, 3754, 958, 420, 746, 411, 300, 13, 286, 500, 380, 458, 437, 3754, 309, 307, 13, 7344, 3031, 300, 472, 13], "temperature": 0.0, "avg_logprob": -0.1645897456577846, "compression_ratio": 1.2923076923076924, "no_speech_prob": 1.0288161320204381e-05}, {"id": 75, "seek": 78816, "start": 788.16, "end": 799.4399999999999, "text": " Let's do a tiny. So we can create a model using like create model. We pass in a string.", "tokens": [961, 311, 360, 257, 5870, 13, 407, 321, 393, 1884, 257, 2316, 1228, 411, 1884, 2316, 13, 492, 1320, 294, 257, 6798, 13], "temperature": 0.0, "avg_logprob": -0.16125221252441407, "compression_ratio": 1.3916083916083917, "no_speech_prob": 2.561170958870207e-06}, {"id": 76, "seek": 78816, "start": 802.48, "end": 808.7199999999999, "text": " And I have a feeling that's got a config. Here we are.", "tokens": [400, 286, 362, 257, 2633, 300, 311, 658, 257, 6662, 13, 1692, 321, 366, 13], "temperature": 0.0, "avg_logprob": -0.16125221252441407, "compression_ratio": 1.3916083916083917, "no_speech_prob": 2.561170958870207e-06}, {"id": 77, "seek": 80872, "start": 808.72, "end": 816.72, "text": " Yeah. See? And it's got a mean and a standard deviation.", "tokens": [865, 13, 3008, 30, 400, 309, 311, 658, 257, 914, 293, 257, 3832, 25163, 13], "temperature": 0.0, "avg_logprob": -0.3000892816587936, "compression_ratio": 1.1441441441441442, "no_speech_prob": 1.6536520206500427e-06}, {"id": 78, "seek": 80872, "start": 820.08, "end": 826.72, "text": " So models equals TEM.listModels. Maybe we'll just do pre-trained ones.", "tokens": [407, 5245, 6915, 314, 6683, 13, 8264, 44, 378, 1625, 13, 2704, 321, 603, 445, 360, 659, 12, 17227, 2001, 2306, 13], "temperature": 0.0, "avg_logprob": -0.3000892816587936, "compression_ratio": 1.1441441441441442, "no_speech_prob": 1.6536520206500427e-06}, {"id": 79, "seek": 82672, "start": 826.72, "end": 836.08, "text": " So I wonder if they all have this for M in", "tokens": [407, 286, 2441, 498, 436, 439, 362, 341, 337, 376, 294], "temperature": 0.0, "avg_logprob": -0.25123633836445053, "compression_ratio": 1.1428571428571428, "no_speech_prob": 6.540238700836198e-06}, {"id": 80, "seek": 82672, "start": 839.52, "end": 843.6800000000001, "text": " models. So let's create a model.", "tokens": [5245, 13, 407, 718, 311, 1884, 257, 2316, 13], "temperature": 0.0, "avg_logprob": -0.25123633836445053, "compression_ratio": 1.1428571428571428, "no_speech_prob": 6.540238700836198e-06}, {"id": 81, "seek": 84368, "start": 843.68, "end": 853.3599999999999, "text": " And have a look at M.defaultConfig.me.", "tokens": [400, 362, 257, 574, 412, 376, 13, 20595, 5107, 9838, 20646, 13, 1398, 13], "temperature": 0.0, "avg_logprob": -0.5199658687298114, "compression_ratio": 0.9428571428571428, "no_speech_prob": 2.3174819943960756e-05}, {"id": 82, "seek": 85336, "start": 853.36, "end": 868.32, "text": " And standard deviation.", "tokens": [400, 3832, 25163, 13], "temperature": 0.0, "avg_logprob": -0.8417959809303284, "compression_ratio": 0.7419354838709677, "no_speech_prob": 0.0001250275527127087}, {"id": 83, "seek": 86832, "start": 868.32, "end": 886.48, "text": " Yeah. So you can see a lot of them use.5.", "tokens": [865, 13, 407, 291, 393, 536, 257, 688, 295, 552, 764, 2411, 20, 13], "temperature": 0.0, "avg_logprob": -0.2671266396840413, "compression_ratio": 0.8367346938775511, "no_speech_prob": 1.694961974862963e-05}, {"id": 84, "seek": 88648, "start": 886.48, "end": 902.8000000000001, "text": " And then some of them use image stats. And I'm guessing they're the only two options.", "tokens": [400, 550, 512, 295, 552, 764, 3256, 18152, 13, 400, 286, 478, 17939, 436, 434, 264, 787, 732, 3956, 13], "temperature": 0.0, "avg_logprob": -0.2176478249686105, "compression_ratio": 1.2254901960784315, "no_speech_prob": 7.644905963388737e-06}, {"id": 85, "seek": 90280, "start": 902.8, "end": 912.7199999999999, "text": " So, okay. So hopefully you get the idea.", "tokens": [407, 11, 1392, 13, 407, 4696, 291, 483, 264, 1558, 13], "temperature": 0.0, "avg_logprob": -0.4300268809000651, "compression_ratio": 0.8333333333333334, "no_speech_prob": 1.0127057976205833e-05}, {"id": 86, "seek": 91272, "start": 912.72, "end": 934.24, "text": " I mean, not necessarily. Sometimes people make the minimum zero and the maximum one.", "tokens": [286, 914, 11, 406, 4725, 13, 4803, 561, 652, 264, 7285, 4018, 293, 264, 6674, 472, 13], "temperature": 0.0, "avg_logprob": -0.3063910121009463, "compression_ratio": 1.0769230769230769, "no_speech_prob": 9.816313649935182e-06}, {"id": 87, "seek": 93424, "start": 934.24, "end": 938.48, "text": " But what we need to do is use the same stats that it was pre-trained with.", "tokens": [583, 437, 321, 643, 281, 360, 307, 764, 264, 912, 18152, 300, 309, 390, 659, 12, 17227, 2001, 365, 13], "temperature": 0.0, "avg_logprob": -0.3588294313665022, "compression_ratio": 1.5625, "no_speech_prob": 8.799824172456283e-06}, {"id": 88, "seek": 93424, "start": 939.2, "end": 944.4, "text": " Because we want our range to be the same as the range it was pre-trained with. Otherwise our", "tokens": [1436, 321, 528, 527, 3613, 281, 312, 264, 912, 382, 264, 3613, 309, 390, 659, 12, 17227, 2001, 365, 13, 10328, 527], "temperature": 0.0, "avg_logprob": -0.3588294313665022, "compression_ratio": 1.5625, "no_speech_prob": 8.799824172456283e-06}, {"id": 89, "seek": 93424, "start": 945.28, "end": 955.44, "text": " data has a different meaning. So", "tokens": [1412, 575, 257, 819, 3620, 13, 407], "temperature": 0.0, "avg_logprob": -0.3588294313665022, "compression_ratio": 1.5625, "no_speech_prob": 8.799824172456283e-06}, {"id": 90, "seek": 95544, "start": 955.44, "end": 971.2, "text": " okay. So let's go to add norm. Okay. So here's add norm. And it's being passed to meta.", "tokens": [1392, 13, 407, 718, 311, 352, 281, 909, 2026, 13, 1033, 13, 407, 510, 311, 909, 2026, 13, 400, 309, 311, 885, 4678, 281, 19616, 13], "temperature": 0.0, "avg_logprob": -0.30081588200160436, "compression_ratio": 1.175, "no_speech_prob": 3.7039368180558085e-05}, {"id": 91, "seek": 95544, "start": 973.9200000000001, "end": 974.48, "text": " Stats.", "tokens": [745, 1720, 13], "temperature": 0.0, "avg_logprob": -0.30081588200160436, "compression_ratio": 1.175, "no_speech_prob": 3.7039368180558085e-05}, {"id": 92, "seek": 97448, "start": 974.48, "end": 981.76, "text": " So", "tokens": [407], "temperature": 0.0, "avg_logprob": -0.27636122703552246, "compression_ratio": 1.108910891089109, "no_speech_prob": 2.4679864509380423e-05}, {"id": 93, "seek": 97448, "start": 989.76, "end": 995.2, "text": " this only works for non-TIM. So how about we put this here and we'll create an else.", "tokens": [341, 787, 1985, 337, 2107, 12, 5422, 44, 13, 407, 577, 466, 321, 829, 341, 510, 293, 321, 603, 1884, 364, 1646, 13], "temperature": 0.0, "avg_logprob": -0.27636122703552246, "compression_ratio": 1.108910891089109, "no_speech_prob": 2.4679864509380423e-05}, {"id": 94, "seek": 97448, "start": 996.4, "end": 997.6800000000001, "text": " Or I guess really an LF.", "tokens": [1610, 286, 2041, 534, 364, 441, 37, 13], "temperature": 0.0, "avg_logprob": -0.27636122703552246, "compression_ratio": 1.108910891089109, "no_speech_prob": 2.4679864509380423e-05}, {"id": 95, "seek": 99768, "start": 997.68, "end": 1014.9599999999999, "text": " And then here we'll have for TIM, if normalize, we could have a TIM normalize.", "tokens": [400, 550, 510, 321, 603, 362, 337, 20187, 11, 498, 2710, 1125, 11, 321, 727, 362, 257, 20187, 2710, 1125, 13], "temperature": 0.0, "avg_logprob": -0.33469161987304685, "compression_ratio": 1.0985915492957747, "no_speech_prob": 4.3993950384901837e-05}, {"id": 96, "seek": 101496, "start": 1014.96, "end": 1040.72, "text": " We can refactor out some duplicate code later. But basically for TIM, we're going to be passing in", "tokens": [492, 393, 1895, 15104, 484, 512, 23976, 3089, 1780, 13, 583, 1936, 337, 20187, 11, 321, 434, 516, 281, 312, 8437, 294], "temperature": 0.0, "avg_logprob": -0.24597587952247033, "compression_ratio": 1.053763440860215, "no_speech_prob": 2.8846665372839198e-05}, {"id": 97, "seek": 104072, "start": 1040.72, "end": 1044.0, "text": " the architecture.", "tokens": [264, 9482, 13], "temperature": 0.0, "avg_logprob": -0.293181127972073, "compression_ratio": 1.475, "no_speech_prob": 6.107439548941329e-05}, {"id": 98, "seek": 104072, "start": 1060.4, "end": 1062.96, "text": " Oh, we don't need to pass in the architecture. We can just pass in the model.", "tokens": [876, 11, 321, 500, 380, 643, 281, 1320, 294, 264, 9482, 13, 492, 393, 445, 1320, 294, 264, 2316, 13], "temperature": 0.0, "avg_logprob": -0.293181127972073, "compression_ratio": 1.475, "no_speech_prob": 6.107439548941329e-05}, {"id": 99, "seek": 106296, "start": 1062.96, "end": 1077.2, "text": " And to protect against future like ability to pass in other types that are strings that aren't TIM.", "tokens": [400, 281, 2371, 1970, 2027, 411, 3485, 281, 1320, 294, 661, 3467, 300, 366, 13985, 300, 3212, 380, 20187, 13], "temperature": 0.0, "avg_logprob": -0.17403529672061696, "compression_ratio": 1.5101010101010102, "no_speech_prob": 4.02928617404541e-06}, {"id": 100, "seek": 106296, "start": 1077.2, "end": 1081.76, "text": " Do you think there's any benefit in having like default normalization function that if you pass", "tokens": [1144, 291, 519, 456, 311, 604, 5121, 294, 1419, 411, 7576, 2710, 2144, 2445, 300, 498, 291, 1320], "temperature": 0.0, "avg_logprob": -0.17403529672061696, "compression_ratio": 1.5101010101010102, "no_speech_prob": 4.02928617404541e-06}, {"id": 101, "seek": 106296, "start": 1081.76, "end": 1090.96, "text": " through, you can actually do your own normalization? No, because my answer to all of those questions is", "tokens": [807, 11, 291, 393, 767, 360, 428, 1065, 2710, 2144, 30, 883, 11, 570, 452, 1867, 281, 439, 295, 729, 1651, 307], "temperature": 0.0, "avg_logprob": -0.17403529672061696, "compression_ratio": 1.5101010101010102, "no_speech_prob": 4.02928617404541e-06}, {"id": 102, "seek": 109096, "start": 1090.96, "end": 1098.64, "text": " always you ain't going to need it. So I very intentionally don't do like", "tokens": [1009, 291, 7862, 380, 516, 281, 643, 309, 13, 407, 286, 588, 22062, 500, 380, 360, 411], "temperature": 0.0, "avg_logprob": -0.14545621370014392, "compression_ratio": 1.5560975609756098, "no_speech_prob": 4.710758730652742e-06}, {"id": 103, "seek": 109096, "start": 1102.56, "end": 1104.8, "text": " dealing with things that may or may not happen in the future.", "tokens": [6260, 365, 721, 300, 815, 420, 815, 406, 1051, 294, 264, 2027, 13], "temperature": 0.0, "avg_logprob": -0.14545621370014392, "compression_ratio": 1.5560975609756098, "no_speech_prob": 4.710758730652742e-06}, {"id": 104, "seek": 109096, "start": 1106.16, "end": 1110.0, "text": " It'd be simpler just to create your own vision liner because that looks like there's not much", "tokens": [467, 1116, 312, 18587, 445, 281, 1884, 428, 1065, 5201, 24468, 570, 300, 1542, 411, 456, 311, 406, 709], "temperature": 0.0, "avg_logprob": -0.14545621370014392, "compression_ratio": 1.5560975609756098, "no_speech_prob": 4.710758730652742e-06}, {"id": 105, "seek": 109096, "start": 1110.0, "end": 1113.3600000000001, "text": " going on there that you could duplicate if you wanted to have support for different model.", "tokens": [516, 322, 456, 300, 291, 727, 23976, 498, 291, 1415, 281, 362, 1406, 337, 819, 2316, 13], "temperature": 0.0, "avg_logprob": -0.14545621370014392, "compression_ratio": 1.5560975609756098, "no_speech_prob": 4.710758730652742e-06}, {"id": 106, "seek": 111336, "start": 1113.36, "end": 1124.1599999999999, "text": " Yeah, exactly. I mean, this is just a small little wrapper really. You can call", "tokens": [865, 11, 2293, 13, 286, 914, 11, 341, 307, 445, 257, 1359, 707, 46906, 534, 13, 509, 393, 818], "temperature": 0.0, "avg_logprob": -0.30439931696111505, "compression_ratio": 1.4961832061068703, "no_speech_prob": 1.1654908121272456e-05}, {"id": 107, "seek": 111336, "start": 1125.28, "end": 1130.32, "text": " create TIM model or create vision model. You can call learner. You can call create head.", "tokens": [1884, 20187, 2316, 420, 1884, 5201, 2316, 13, 509, 393, 818, 33347, 13, 509, 393, 818, 1884, 1378, 13], "temperature": 0.0, "avg_logprob": -0.30439931696111505, "compression_ratio": 1.4961832061068703, "no_speech_prob": 1.1654908121272456e-05}, {"id": 108, "seek": 111336, "start": 1134.1599999999999, "end": 1137.12, "text": " Okay, so we'll call that M.", "tokens": [1033, 11, 370, 321, 603, 818, 300, 376, 13], "temperature": 0.0, "avg_logprob": -0.30439931696111505, "compression_ratio": 1.4961832061068703, "no_speech_prob": 1.1654908121272456e-05}, {"id": 109, "seek": 113712, "start": 1137.12, "end": 1141.12, "text": " So the normalize", "tokens": [407, 264, 2710, 1125], "temperature": 0.0, "avg_logprob": -0.2477074940999349, "compression_ratio": 1.1318681318681318, "no_speech_prob": 3.319649476907216e-05}, {"id": 110, "seek": 113712, "start": 1151.28, "end": 1161.1999999999998, "text": " takes a mean and a standard deviation. So it should be just those two things, I guess.", "tokens": [2516, 257, 914, 293, 257, 3832, 25163, 13, 407, 309, 820, 312, 445, 729, 732, 721, 11, 286, 2041, 13], "temperature": 0.0, "avg_logprob": -0.2477074940999349, "compression_ratio": 1.1318681318681318, "no_speech_prob": 3.319649476907216e-05}, {"id": 111, "seek": 116120, "start": 1161.2, "end": 1165.44, "text": " Like so.", "tokens": [1743, 370, 13], "temperature": 0.0, "avg_logprob": -0.8575196266174316, "compression_ratio": 0.7037037037037037, "no_speech_prob": 3.07119516946841e-05}, {"id": 112, "seek": 116120, "start": 1179.52, "end": 1180.0, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.8575196266174316, "compression_ratio": 0.7037037037037037, "no_speech_prob": 3.07119516946841e-05}, {"id": 113, "seek": 118000, "start": 1180.0, "end": 1188.64, "text": " Okay, TIM normalize", "tokens": [1033, 11, 20187, 2710, 1125], "temperature": 0.0, "avg_logprob": -0.4738284663150185, "compression_ratio": 0.8870967741935484, "no_speech_prob": 1.5440791685250588e-05}, {"id": 114, "seek": 118864, "start": 1188.64, "end": 1209.5200000000002, "text": " using the model and pretread. Okay. I see I already had an else there.", "tokens": [1228, 264, 2316, 293, 1162, 2538, 13, 1033, 13, 286, 536, 286, 1217, 632, 364, 1646, 456, 13], "temperature": 0.0, "avg_logprob": -0.5204345096241344, "compression_ratio": 0.9859154929577465, "no_speech_prob": 1.4282879419624805e-05}, {"id": 115, "seek": 120952, "start": 1209.52, "end": 1217.52, "text": " So do that.", "tokens": [407, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.8902030357947717, "compression_ratio": 0.8923076923076924, "no_speech_prob": 1.3004624634049833e-05}, {"id": 116, "seek": 120952, "start": 1218.48, "end": 1220.0, "text": " There we go.", "tokens": [821, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.8902030357947717, "compression_ratio": 0.8923076923076924, "no_speech_prob": 1.3004624634049833e-05}, {"id": 117, "seek": 120952, "start": 1221.84, "end": 1228.0, "text": " And okay. So let's test this out.", "tokens": [400, 1392, 13, 407, 718, 311, 1500, 341, 484, 13], "temperature": 0.0, "avg_logprob": -0.8902030357947717, "compression_ratio": 0.8923076923076924, "no_speech_prob": 1.3004624634049833e-05}, {"id": 118, "seek": 122800, "start": 1228.0, "end": 1235.92, "text": " So what happens when you add a transform?", "tokens": [407, 437, 2314, 562, 291, 909, 257, 4088, 30], "temperature": 0.0, "avg_logprob": -0.5727230707804362, "compression_ratio": 1.1547619047619047, "no_speech_prob": 3.0894291285221698e-06}, {"id": 119, "seek": 122800, "start": 1238.4, "end": 1245.28, "text": " It adds a transform to each data loader in it. Okay. So", "tokens": [467, 10860, 257, 4088, 281, 1184, 1412, 3677, 260, 294, 309, 13, 1033, 13, 407], "temperature": 0.0, "avg_logprob": -0.5727230707804362, "compression_ratio": 1.1547619047619047, "no_speech_prob": 3.0894291285221698e-06}, {"id": 120, "seek": 124528, "start": 1245.28, "end": 1256.24, "text": " what does that do?", "tokens": [437, 775, 300, 360, 30], "temperature": 0.0, "avg_logprob": -0.7931576277080336, "compression_ratio": 1.0238095238095237, "no_speech_prob": 1.2098280421923846e-06}, {"id": 121, "seek": 124528, "start": 1256.24, "end": 1260.24, "text": " Oh, what did I do wrong?", "tokens": [876, 11, 437, 630, 286, 360, 2085, 30], "temperature": 0.0, "avg_logprob": -0.7931576277080336, "compression_ratio": 1.0238095238095237, "no_speech_prob": 1.2098280421923846e-06}, {"id": 122, "seek": 126024, "start": 1260.24, "end": 1285.52, "text": " Just the return----", "tokens": [1449, 264, 2736, 10922], "temperature": 1.0, "avg_logprob": -5.2300004959106445, "compression_ratio": 0.76, "no_speech_prob": 6.01161336817313e-05}, {"id": 123, "seek": 128552, "start": 1285.52, "end": 1291.12, "text": " Oh, it's part of, I see.", "tokens": [876, 11, 309, 311, 644, 295, 11, 286, 536, 13], "temperature": 0.0, "avg_logprob": -0.5118642622424711, "compression_ratio": 1.044776119402985, "no_speech_prob": 0.032041825354099274}, {"id": 124, "seek": 128552, "start": 1291.12, "end": 1294.12, "text": " It's part of...", "tokens": [467, 311, 644, 295, 485], "temperature": 0.0, "avg_logprob": -0.5118642622424711, "compression_ratio": 1.044776119402985, "no_speech_prob": 0.032041825354099274}, {"id": 125, "seek": 128552, "start": 1294.12, "end": 1311.12, "text": " Okay, that's a bit confusing.", "tokens": [1033, 11, 300, 311, 257, 857, 13181, 13], "temperature": 0.0, "avg_logprob": -0.5118642622424711, "compression_ratio": 1.044776119402985, "no_speech_prob": 0.032041825354099274}, {"id": 126, "seek": 131112, "start": 1311.12, "end": 1316.12, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.5302187846257136, "compression_ratio": 1.0133333333333334, "no_speech_prob": 9.294922347180545e-05}, {"id": 127, "seek": 131112, "start": 1316.12, "end": 1330.12, "text": " Okay, so let's find, sometimes it's just easiest to look at the code.", "tokens": [1033, 11, 370, 718, 311, 915, 11, 2171, 309, 311, 445, 12889, 281, 574, 412, 264, 3089, 13], "temperature": 0.0, "avg_logprob": -0.5302187846257136, "compression_ratio": 1.0133333333333334, "no_speech_prob": 9.294922347180545e-05}, {"id": 128, "seek": 133012, "start": 1330.12, "end": 1344.7199999999998, "text": " I see.", "tokens": [286, 536, 13], "temperature": 0.0, "avg_logprob": -0.38486694567131274, "compression_ratio": 1.075, "no_speech_prob": 5.7351182476850227e-05}, {"id": 129, "seek": 133012, "start": 1344.7199999999998, "end": 1349.12, "text": " So it's just calling add.", "tokens": [407, 309, 311, 445, 5141, 909, 13], "temperature": 0.0, "avg_logprob": -0.38486694567131274, "compression_ratio": 1.075, "no_speech_prob": 5.7351182476850227e-05}, {"id": 130, "seek": 133012, "start": 1349.12, "end": 1351.7199999999998, "text": " I see for this particular event.", "tokens": [286, 536, 337, 341, 1729, 2280, 13], "temperature": 0.0, "avg_logprob": -0.38486694567131274, "compression_ratio": 1.075, "no_speech_prob": 5.7351182476850227e-05}, {"id": 131, "seek": 133012, "start": 1351.7199999999998, "end": 1357.6, "text": " And we're adding it.", "tokens": [400, 321, 434, 5127, 309, 13], "temperature": 0.0, "avg_logprob": -0.38486694567131274, "compression_ratio": 1.075, "no_speech_prob": 5.7351182476850227e-05}, {"id": 132, "seek": 135760, "start": 1357.6, "end": 1360.1999999999998, "text": " I see we're adding it to the after batch event.", "tokens": [286, 536, 321, 434, 5127, 309, 281, 264, 934, 15245, 2280, 13], "temperature": 0.0, "avg_logprob": -0.21213857433463953, "compression_ratio": 1.558139534883721, "no_speech_prob": 8.663455446367152e-06}, {"id": 133, "seek": 135760, "start": 1360.1999999999998, "end": 1366.6, "text": " So we should find there's a after batch event.", "tokens": [407, 321, 820, 915, 456, 311, 257, 934, 15245, 2280, 13], "temperature": 0.0, "avg_logprob": -0.21213857433463953, "compression_ratio": 1.558139534883721, "no_speech_prob": 8.663455446367152e-06}, {"id": 134, "seek": 135760, "start": 1366.6, "end": 1367.6, "text": " Here we are.", "tokens": [1692, 321, 366, 13], "temperature": 0.0, "avg_logprob": -0.21213857433463953, "compression_ratio": 1.558139534883721, "no_speech_prob": 8.663455446367152e-06}, {"id": 135, "seek": 135760, "start": 1367.6, "end": 1369.7199999999998, "text": " I see. And there's our transforms.", "tokens": [286, 536, 13, 400, 456, 311, 527, 35592, 13], "temperature": 0.0, "avg_logprob": -0.21213857433463953, "compression_ratio": 1.558139534883721, "no_speech_prob": 8.663455446367152e-06}, {"id": 136, "seek": 135760, "start": 1369.7199999999998, "end": 1378.0, "text": " So if we call vision learner, that should change our data loader.", "tokens": [407, 498, 321, 818, 5201, 33347, 11, 300, 820, 1319, 527, 1412, 3677, 260, 13], "temperature": 0.0, "avg_logprob": -0.21213857433463953, "compression_ratio": 1.558139534883721, "no_speech_prob": 8.663455446367152e-06}, {"id": 137, "seek": 135760, "start": 1378.0, "end": 1384.52, "text": " Yep. And it's now got normalized using the image net stats.", "tokens": [7010, 13, 400, 309, 311, 586, 658, 48704, 1228, 264, 3256, 2533, 18152, 13], "temperature": 0.0, "avg_logprob": -0.21213857433463953, "compression_ratio": 1.558139534883721, "no_speech_prob": 8.663455446367152e-06}, {"id": 138, "seek": 138452, "start": 1384.52, "end": 1393.52, "text": " And if we now try it for a string version.", "tokens": [400, 498, 321, 586, 853, 309, 337, 257, 6798, 3037, 13], "temperature": 0.0, "avg_logprob": -0.25914478302001953, "compression_ratio": 1.1111111111111112, "no_speech_prob": 2.1109182853251696e-05}, {"id": 139, "seek": 138452, "start": 1393.52, "end": 1404.52, "text": " Now that's interesting.", "tokens": [823, 300, 311, 1880, 13], "temperature": 0.0, "avg_logprob": -0.25914478302001953, "compression_ratio": 1.1111111111111112, "no_speech_prob": 2.1109182853251696e-05}, {"id": 140, "seek": 138452, "start": 1404.52, "end": 1409.52, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.25914478302001953, "compression_ratio": 1.1111111111111112, "no_speech_prob": 2.1109182853251696e-05}, {"id": 141, "seek": 138452, "start": 1409.52, "end": 1411.52, "text": " What happened differently?", "tokens": [708, 2011, 7614, 30], "temperature": 0.0, "avg_logprob": -0.25914478302001953, "compression_ratio": 1.1111111111111112, "no_speech_prob": 2.1109182853251696e-05}, {"id": 142, "seek": 138452, "start": 1411.52, "end": 1412.52, "text": " Oh, I see.", "tokens": [876, 11, 286, 536, 13], "temperature": 0.0, "avg_logprob": -0.25914478302001953, "compression_ratio": 1.1111111111111112, "no_speech_prob": 2.1109182853251696e-05}, {"id": 143, "seek": 141252, "start": 1412.52, "end": 1420.52, "text": " We need to recreate the data loaders for this test.", "tokens": [492, 643, 281, 25833, 264, 1412, 3677, 433, 337, 341, 1500, 13], "temperature": 0.0, "avg_logprob": -0.13361796736717224, "compression_ratio": 1.474025974025974, "no_speech_prob": 1.2804645848518703e-05}, {"id": 144, "seek": 141252, "start": 1420.52, "end": 1426.52, "text": " So that doesn't have normalized anymore.", "tokens": [407, 300, 1177, 380, 362, 48704, 3602, 13], "temperature": 0.0, "avg_logprob": -0.13361796736717224, "compression_ratio": 1.474025974025974, "no_speech_prob": 1.2804645848518703e-05}, {"id": 145, "seek": 141252, "start": 1426.52, "end": 1429.52, "text": " And that gives us, okay, that gives us an error.", "tokens": [400, 300, 2709, 505, 11, 1392, 11, 300, 2709, 505, 364, 6713, 13], "temperature": 0.0, "avg_logprob": -0.13361796736717224, "compression_ratio": 1.474025974025974, "no_speech_prob": 1.2804645848518703e-05}, {"id": 146, "seek": 141252, "start": 1429.52, "end": 1432.52, "text": " And that's because it says we're passing a sequential object.", "tokens": [400, 300, 311, 570, 309, 1619, 321, 434, 8437, 257, 42881, 2657, 13], "temperature": 0.0, "avg_logprob": -0.13361796736717224, "compression_ratio": 1.474025974025974, "no_speech_prob": 1.2804645848518703e-05}, {"id": 147, "seek": 141252, "start": 1432.52, "end": 1436.52, "text": " Okay, that makes sense.", "tokens": [1033, 11, 300, 1669, 2020, 13], "temperature": 0.0, "avg_logprob": -0.13361796736717224, "compression_ratio": 1.474025974025974, "no_speech_prob": 1.2804645848518703e-05}, {"id": 148, "seek": 143652, "start": 1436.52, "end": 1444.52, "text": " Because create Tim model actually.", "tokens": [1436, 1884, 7172, 2316, 767, 13], "temperature": 0.0, "avg_logprob": -0.15828451243313876, "compression_ratio": 1.2956521739130435, "no_speech_prob": 3.966727945226012e-06}, {"id": 149, "seek": 143652, "start": 1444.52, "end": 1447.52, "text": " Yeah, modifies things.", "tokens": [865, 11, 1072, 11221, 721, 13], "temperature": 0.0, "avg_logprob": -0.15828451243313876, "compression_ratio": 1.2956521739130435, "no_speech_prob": 3.966727945226012e-06}, {"id": 150, "seek": 143652, "start": 1447.52, "end": 1450.52, "text": " That's why.", "tokens": [663, 311, 983, 13], "temperature": 0.0, "avg_logprob": -0.15828451243313876, "compression_ratio": 1.2956521739130435, "no_speech_prob": 3.966727945226012e-06}, {"id": 151, "seek": 143652, "start": 1450.52, "end": 1456.52, "text": " And it creates a sequential model because it's got the head and the body in it.", "tokens": [400, 309, 7829, 257, 42881, 2316, 570, 309, 311, 658, 264, 1378, 293, 264, 1772, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.15828451243313876, "compression_ratio": 1.2956521739130435, "no_speech_prob": 3.966727945226012e-06}, {"id": 152, "seek": 145652, "start": 1456.52, "end": 1467.52, "text": " So we need to change how we do this.", "tokens": [407, 321, 643, 281, 1319, 577, 321, 360, 341, 13], "temperature": 0.0, "avg_logprob": -0.13552417357762656, "compression_ratio": 1.2521739130434784, "no_speech_prob": 1.3081419183436083e-06}, {"id": 153, "seek": 145652, "start": 1467.52, "end": 1472.52, "text": " All right. This is Tim body.", "tokens": [1057, 558, 13, 639, 307, 7172, 1772, 13], "temperature": 0.0, "avg_logprob": -0.13552417357762656, "compression_ratio": 1.2521739130434784, "no_speech_prob": 1.3081419183436083e-06}, {"id": 154, "seek": 145652, "start": 1472.52, "end": 1477.52, "text": " Here is the model.", "tokens": [1692, 307, 264, 2316, 13], "temperature": 0.0, "avg_logprob": -0.13552417357762656, "compression_ratio": 1.2521739130434784, "no_speech_prob": 1.3081419183436083e-06}, {"id": 155, "seek": 145652, "start": 1477.52, "end": 1485.52, "text": " And oh, look, here we use default config to get stuff here.", "tokens": [400, 1954, 11, 574, 11, 510, 321, 764, 7576, 6662, 281, 483, 1507, 510, 13], "temperature": 0.0, "avg_logprob": -0.13552417357762656, "compression_ratio": 1.2521739130434784, "no_speech_prob": 1.3081419183436083e-06}, {"id": 156, "seek": 148552, "start": 1485.52, "end": 1494.52, "text": " Interesting.", "tokens": [14711, 13], "temperature": 0.0, "avg_logprob": -0.16453739007314047, "compression_ratio": 0.6, "no_speech_prob": 0.00010549955914029852}, {"id": 157, "seek": 149452, "start": 1494.52, "end": 1520.52, "text": " So Tim body is called from here.", "tokens": [407, 7172, 1772, 307, 1219, 490, 510, 13], "temperature": 0.0, "avg_logprob": -0.1456932326157888, "compression_ratio": 0.8, "no_speech_prob": 2.1440640921355225e-05}, {"id": 158, "seek": 152052, "start": 1520.52, "end": 1526.52, "text": " I guess like it would be nice to know how Tim does this exactly.", "tokens": [286, 2041, 411, 309, 576, 312, 1481, 281, 458, 577, 7172, 775, 341, 2293, 13], "temperature": 0.0, "avg_logprob": -0.17211103439331055, "compression_ratio": 1.127659574468085, "no_speech_prob": 1.2026502190565225e-05}, {"id": 159, "seek": 152052, "start": 1526.52, "end": 1531.52, "text": " Where does that default config come from?", "tokens": [2305, 775, 300, 7576, 6662, 808, 490, 30], "temperature": 0.0, "avg_logprob": -0.17211103439331055, "compression_ratio": 1.127659574468085, "no_speech_prob": 1.2026502190565225e-05}, {"id": 160, "seek": 153152, "start": 1531.52, "end": 1549.52, "text": " So when we call Tim.createModel,", "tokens": [407, 562, 321, 818, 7172, 13, 14066, 473, 44, 41147, 11], "temperature": 0.0, "avg_logprob": -0.311655634925479, "compression_ratio": 0.8620689655172413, "no_speech_prob": 1.7772314095054753e-05}, {"id": 161, "seek": 153152, "start": 1549.52, "end": 1559.52, "text": " set layer config.", "tokens": [992, 4583, 6662, 13], "temperature": 0.0, "avg_logprob": -0.311655634925479, "compression_ratio": 0.8620689655172413, "no_speech_prob": 1.7772314095054753e-05}, {"id": 162, "seek": 155952, "start": 1559.52, "end": 1577.52, "text": " I wonder if we should take a look.", "tokens": [286, 2441, 498, 321, 820, 747, 257, 574, 13], "temperature": 0.0, "avg_logprob": -0.14023709297180176, "compression_ratio": 0.8620689655172413, "no_speech_prob": 5.0145326895290054e-06}, {"id": 163, "seek": 155952, "start": 1577.52, "end": 1582.52, "text": " Default config.", "tokens": [9548, 5107, 6662, 13], "temperature": 0.0, "avg_logprob": -0.14023709297180176, "compression_ratio": 0.8620689655172413, "no_speech_prob": 5.0145326895290054e-06}, {"id": 164, "seek": 158252, "start": 1582.52, "end": 1590.52, "text": " Here's data config.py.", "tokens": [1692, 311, 1412, 6662, 13, 8200, 13], "temperature": 0.0, "avg_logprob": -0.17383744499900125, "compression_ratio": 0.7333333333333333, "no_speech_prob": 1.9523300579749048e-05}, {"id": 165, "seek": 159052, "start": 1590.52, "end": 1619.52, "text": " So where does it get set?", "tokens": [407, 689, 775, 309, 483, 992, 30], "temperature": 0.0, "avg_logprob": -0.1550226645036177, "compression_ratio": 0.7575757575757576, "no_speech_prob": 8.742058707866818e-05}, {"id": 166, "seek": 161952, "start": 1619.52, "end": 1646.52, "text": " Maybe bottles help us.", "tokens": [2704, 15923, 854, 505, 13], "temperature": 0.0, "avg_logprob": -0.42518825001186794, "compression_ratio": 0.7333333333333333, "no_speech_prob": 0.0009503016481176019}, {"id": 167, "seek": 164652, "start": 1646.52, "end": 1653.52, "text": " Build model with config.", "tokens": [11875, 2316, 365, 6662, 13], "temperature": 0.0, "avg_logprob": -0.46735880925105167, "compression_ratio": 0.7894736842105263, "no_speech_prob": 5.421896730695153e-06}, {"id": 168, "seek": 164652, "start": 1653.52, "end": 1660.52, "text": " Well,", "tokens": [1042, 11], "temperature": 0.0, "avg_logprob": -0.46735880925105167, "compression_ratio": 0.7894736842105263, "no_speech_prob": 5.421896730695153e-06}, {"id": 169, "seek": 166052, "start": 1660.52, "end": 1681.52, "text": " seems like this button needs restructuring.", "tokens": [2544, 411, 341, 2960, 2203, 1472, 1757, 1345, 13], "temperature": 0.0, "avg_logprob": -0.36082568535437953, "compression_ratio": 0.86, "no_speech_prob": 1.6440115359728225e-05}, {"id": 170, "seek": 168152, "start": 1681.52, "end": 1691.52, "text": " Create vision model.", "tokens": [20248, 5201, 2316, 13], "temperature": 0.0, "avg_logprob": -0.47579618862697054, "compression_ratio": 1.2465753424657535, "no_speech_prob": 6.74748253004509e-06}, {"id": 171, "seek": 168152, "start": 1691.52, "end": 1698.52, "text": " Create body and create body.", "tokens": [20248, 1772, 293, 1884, 1772, 13], "temperature": 0.0, "avg_logprob": -0.47579618862697054, "compression_ratio": 1.2465753424657535, "no_speech_prob": 6.74748253004509e-06}, {"id": 172, "seek": 168152, "start": 1698.52, "end": 1707.52, "text": " Here, this is where it creates the model.", "tokens": [1692, 11, 341, 307, 689, 309, 7829, 264, 2316, 13], "temperature": 0.0, "avg_logprob": -0.47579618862697054, "compression_ratio": 1.2465753424657535, "no_speech_prob": 6.74748253004509e-06}, {"id": 173, "seek": 170752, "start": 1707.52, "end": 1722.52, "text": " So let's do some,", "tokens": [407, 718, 311, 360, 512, 11], "temperature": 0.0, "avg_logprob": -0.8276069641113282, "compression_ratio": 0.68, "no_speech_prob": 9.513988516118843e-06}, {"id": 174, "seek": 172252, "start": 1722.52, "end": 1737.52, "text": " I guess, redesign maybe.", "tokens": [286, 2041, 11, 39853, 1310, 13], "temperature": 0.0, "avg_logprob": -0.8412010192871093, "compression_ratio": 0.75, "no_speech_prob": 1.8335260392632335e-05}, {"id": 175, "seek": 173752, "start": 1737.52, "end": 1758.52, "text": " But it assumes it's already instantiated.", "tokens": [583, 309, 37808, 309, 311, 1217, 9836, 72, 770, 13], "temperature": 0.0, "avg_logprob": -0.10342061519622803, "compression_ratio": 0.9428571428571428, "no_speech_prob": 8.66360824147705e-06}, {"id": 176, "seek": 173752, "start": 1758.52, "end": 1765.52, "text": " So we would remove that.", "tokens": [407, 321, 576, 4159, 300, 13], "temperature": 0.0, "avg_logprob": -0.10342061519622803, "compression_ratio": 0.9428571428571428, "no_speech_prob": 8.66360824147705e-06}, {"id": 177, "seek": 176552, "start": 1765.52, "end": 1768.52, "text": " So that's now not going to work, of course.", "tokens": [407, 300, 311, 586, 406, 516, 281, 589, 11, 295, 1164, 13], "temperature": 0.0, "avg_logprob": -0.2036127690915708, "compression_ratio": 1.064102564102564, "no_speech_prob": 1.6699468687875196e-05}, {"id": 178, "seek": 176552, "start": 1768.52, "end": 1778.52, "text": " So then we're creating body with model.", "tokens": [407, 550, 321, 434, 4084, 1772, 365, 2316, 13], "temperature": 0.0, "avg_logprob": -0.2036127690915708, "compression_ratio": 1.064102564102564, "no_speech_prob": 1.6699468687875196e-05}, {"id": 179, "seek": 177852, "start": 1778.52, "end": 1807.52, "text": " And so then we have to instantiate that.", "tokens": [400, 370, 550, 321, 362, 281, 9836, 13024, 300, 13], "temperature": 0.0, "avg_logprob": -0.05803125670978001, "compression_ratio": 0.8695652173913043, "no_speech_prob": 1.7496829968877137e-05}, {"id": 180, "seek": 180752, "start": 1807.52, "end": 1828.52, "text": " So we may as well just do that directly, right?", "tokens": [407, 321, 815, 382, 731, 445, 360, 300, 3838, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.20568236708641052, "compression_ratio": 0.9038461538461539, "no_speech_prob": 6.397193646989763e-05}, {"id": 181, "seek": 182852, "start": 1828.52, "end": 1855.52, "text": " Maybe make this a function.", "tokens": [2704, 652, 341, 257, 2445, 13], "temperature": 0.0, "avg_logprob": -0.4081864356994629, "compression_ratio": 0.7714285714285715, "no_speech_prob": 0.0001069589052349329}, {"id": 182, "seek": 185552, "start": 1855.52, "end": 1872.52, "text": " So it gets new on each time.", "tokens": [407, 309, 2170, 777, 322, 1184, 565, 13], "temperature": 0.0, "avg_logprob": -0.5397511720657349, "compression_ratio": 0.7777777777777778, "no_speech_prob": 3.023140197910834e-05}, {"id": 183, "seek": 187252, "start": 1872.52, "end": 1892.52, "text": " Okay. So in this refactoring,", "tokens": [1033, 13, 407, 294, 341, 1895, 578, 3662, 11], "temperature": 0.0, "avg_logprob": -0.2934149412008432, "compression_ratio": 0.7837837837837838, "no_speech_prob": 1.7227610442205332e-05}, {"id": 184, "seek": 189252, "start": 1892.52, "end": 1906.52, "text": " create head won't change.", "tokens": [1884, 1378, 1582, 380, 1319, 13], "temperature": 0.0, "avg_logprob": -0.12323928701466527, "compression_ratio": 1.1428571428571428, "no_speech_prob": 1.6185606000362895e-05}, {"id": 185, "seek": 189252, "start": 1906.52, "end": 1909.52, "text": " Or the model meta stuff doesn't change.", "tokens": [1610, 264, 2316, 19616, 1507, 1177, 380, 1319, 13], "temperature": 0.0, "avg_logprob": -0.12323928701466527, "compression_ratio": 1.1428571428571428, "no_speech_prob": 1.6185606000362895e-05}, {"id": 186, "seek": 189252, "start": 1909.52, "end": 1920.52, "text": " Okay. So this changes.", "tokens": [1033, 13, 407, 341, 2962, 13], "temperature": 0.0, "avg_logprob": -0.12323928701466527, "compression_ratio": 1.1428571428571428, "no_speech_prob": 1.6185606000362895e-05}, {"id": 187, "seek": 192052, "start": 1920.52, "end": 1927.52, "text": " So now we say model equals arch pretrained.", "tokens": [407, 586, 321, 584, 2316, 6915, 3912, 1162, 31774, 13], "temperature": 0.0, "avg_logprob": -0.25181088624177156, "compression_ratio": 1.0128205128205128, "no_speech_prob": 2.4678200134076178e-05}, {"id": 188, "seek": 192052, "start": 1927.52, "end": 1937.52, "text": " Pass in model.", "tokens": [10319, 294, 2316, 13], "temperature": 0.0, "avg_logprob": -0.25181088624177156, "compression_ratio": 1.0128205128205128, "no_speech_prob": 2.4678200134076178e-05}, {"id": 189, "seek": 192052, "start": 1937.52, "end": 1946.52, "text": " Okay. Looks hopeful.", "tokens": [1033, 13, 10027, 20531, 13], "temperature": 0.0, "avg_logprob": -0.25181088624177156, "compression_ratio": 1.0128205128205128, "no_speech_prob": 2.4678200134076178e-05}, {"id": 190, "seek": 194652, "start": 1946.52, "end": 1951.52, "text": " So we're going to do the same thing for Tim.", "tokens": [407, 321, 434, 516, 281, 360, 264, 912, 551, 337, 7172, 13], "temperature": 0.0, "avg_logprob": -0.06428530067205429, "compression_ratio": 0.9166666666666666, "no_speech_prob": 2.795566433633212e-05}, {"id": 191, "seek": 195152, "start": 1951.52, "end": 1976.52, "text": " We're going to pass in a model.", "tokens": [492, 434, 516, 281, 1320, 294, 257, 2316, 13], "temperature": 0.0, "avg_logprob": -0.14074077972998986, "compression_ratio": 0.7948717948717948, "no_speech_prob": 0.00018198000907432288}, {"id": 192, "seek": 197652, "start": 1976.52, "end": 1987.52, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.1569142242272695, "compression_ratio": 0.38461538461538464, "no_speech_prob": 3.474688855931163e-05}, {"id": 193, "seek": 198752, "start": 1987.52, "end": 2008.52, "text": " So it's going to be the same here.", "tokens": [407, 309, 311, 516, 281, 312, 264, 912, 510, 13], "temperature": 0.0, "avg_logprob": -0.21577049255371095, "compression_ratio": 1.0273972602739727, "no_speech_prob": 2.2449092284659855e-05}, {"id": 194, "seek": 198752, "start": 2008.52, "end": 2015.52, "text": " Let's see if vision learner still works.", "tokens": [961, 311, 536, 498, 5201, 33347, 920, 1985, 13], "temperature": 0.0, "avg_logprob": -0.21577049255371095, "compression_ratio": 1.0273972602739727, "no_speech_prob": 2.2449092284659855e-05}, {"id": 195, "seek": 201552, "start": 2015.52, "end": 2035.52, "text": " Okay. It does.", "tokens": [1033, 13, 467, 775, 13], "temperature": 0.0, "avg_logprob": -0.19019419617123073, "compression_ratio": 0.6363636363636364, "no_speech_prob": 7.131131133064628e-05}, {"id": 196, "seek": 203552, "start": 2035.52, "end": 2055.52, "text": " So maybe we should move, keep moving this back further and further.", "tokens": [407, 1310, 321, 820, 1286, 11, 1066, 2684, 341, 646, 3052, 293, 3052, 13], "temperature": 0.0, "avg_logprob": -0.062379823790656194, "compression_ratio": 1.0151515151515151, "no_speech_prob": 1.3418220078165177e-05}, {"id": 197, "seek": 205552, "start": 2055.52, "end": 2073.52, "text": " So to make Tim work,", "tokens": [407, 281, 652, 7172, 589, 11], "temperature": 0.0, "avg_logprob": -0.16755764484405516, "compression_ratio": 0.7142857142857143, "no_speech_prob": 1.362995317322202e-05}, {"id": 198, "seek": 207352, "start": 2073.52, "end": 2089.52, "text": " do that.", "tokens": [360, 300, 13], "temperature": 0.0, "avg_logprob": -0.36750991003853933, "compression_ratio": 0.5, "no_speech_prob": 2.0140676497248933e-05}, {"id": 199, "seek": 208952, "start": 2089.52, "end": 2113.52, "text": " Tim.", "tokens": [7172, 13], "temperature": 0.0, "avg_logprob": -0.5777310927708944, "compression_ratio": 0.3333333333333333, "no_speech_prob": 0.030531736090779305}, {"id": 200, "seek": 211352, "start": 2113.52, "end": 2128.52, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.3220462432274452, "compression_ratio": 1.4188034188034189, "no_speech_prob": 1.0782041499624029e-05}, {"id": 201, "seek": 211352, "start": 2128.52, "end": 2132.52, "text": " Problem with that is the keyword arguments.", "tokens": [11676, 365, 300, 307, 264, 20428, 12869, 13], "temperature": 0.0, "avg_logprob": -0.3220462432274452, "compression_ratio": 1.4188034188034189, "no_speech_prob": 1.0782041499624029e-05}, {"id": 202, "seek": 211352, "start": 2132.52, "end": 2134.52, "text": " So there's a lot of, this is,", "tokens": [407, 456, 311, 257, 688, 295, 11, 341, 307, 11], "temperature": 0.0, "avg_logprob": -0.3220462432274452, "compression_ratio": 1.4188034188034189, "no_speech_prob": 1.0782041499624029e-05}, {"id": 203, "seek": 211352, "start": 2134.52, "end": 2135.52, "text": " this gets a bit crazy.", "tokens": [341, 2170, 257, 857, 3219, 13], "temperature": 0.0, "avg_logprob": -0.3220462432274452, "compression_ratio": 1.4188034188034189, "no_speech_prob": 1.0782041499624029e-05}, {"id": 204, "seek": 211352, "start": 2135.52, "end": 2140.52, "text": " There's a lot of keyword arguments when you create a model and.", "tokens": [821, 311, 257, 688, 295, 20428, 12869, 562, 291, 1884, 257, 2316, 293, 13], "temperature": 0.0, "avg_logprob": -0.3220462432274452, "compression_ratio": 1.4188034188034189, "no_speech_prob": 1.0782041499624029e-05}, {"id": 205, "seek": 214052, "start": 2140.52, "end": 2144.52, "text": " And then you can actually do it.", "tokens": [400, 550, 291, 393, 767, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.7864241600036621, "compression_ratio": 1.0273972602739727, "no_speech_prob": 2.3921460524434224e-05}, {"id": 206, "seek": 214052, "start": 2144.52, "end": 2155.52, "text": " To Tim.", "tokens": [1407, 7172, 13], "temperature": 0.0, "avg_logprob": -0.7864241600036621, "compression_ratio": 1.0273972602739727, "no_speech_prob": 2.3921460524434224e-05}, {"id": 207, "seek": 214052, "start": 2155.52, "end": 2164.52, "text": " So I think actually what we'll do.", "tokens": [407, 286, 519, 767, 437, 321, 603, 360, 13], "temperature": 0.0, "avg_logprob": -0.7864241600036621, "compression_ratio": 1.0273972602739727, "no_speech_prob": 2.3921460524434224e-05}, {"id": 208, "seek": 216452, "start": 2164.52, "end": 2180.52, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.3817123633164626, "compression_ratio": 0.7333333333333333, "no_speech_prob": 2.2113659724709578e-05}, {"id": 209, "seek": 216452, "start": 2180.52, "end": 2183.52, "text": " And so Tim body.", "tokens": [400, 370, 7172, 1772, 13], "temperature": 0.0, "avg_logprob": -0.3817123633164626, "compression_ratio": 0.7333333333333333, "no_speech_prob": 2.2113659724709578e-05}, {"id": 210, "seek": 218352, "start": 2183.52, "end": 2194.52, "text": " Doesn't need quags anymore.", "tokens": [12955, 380, 643, 421, 12109, 3602, 13], "temperature": 0.0, "avg_logprob": -0.2779619455337524, "compression_ratio": 1.1030927835051547, "no_speech_prob": 8.939487088355236e-06}, {"id": 211, "seek": 218352, "start": 2194.52, "end": 2198.52, "text": " What we might do.", "tokens": [708, 321, 1062, 360, 13], "temperature": 0.0, "avg_logprob": -0.2779619455337524, "compression_ratio": 1.1030927835051547, "no_speech_prob": 8.939487088355236e-06}, {"id": 212, "seek": 218352, "start": 2198.52, "end": 2201.52, "text": " Let's say this is the result.", "tokens": [961, 311, 584, 341, 307, 264, 1874, 13], "temperature": 0.0, "avg_logprob": -0.2779619455337524, "compression_ratio": 1.1030927835051547, "no_speech_prob": 8.939487088355236e-06}, {"id": 213, "seek": 218352, "start": 2201.52, "end": 2203.52, "text": " And we'll return.", "tokens": [400, 321, 603, 2736, 13], "temperature": 0.0, "avg_logprob": -0.2779619455337524, "compression_ratio": 1.1030927835051547, "no_speech_prob": 8.939487088355236e-06}, {"id": 214, "seek": 218352, "start": 2203.52, "end": 2207.52, "text": " Those things.", "tokens": [3950, 721, 13], "temperature": 0.0, "avg_logprob": -0.2779619455337524, "compression_ratio": 1.1030927835051547, "no_speech_prob": 8.939487088355236e-06}, {"id": 215, "seek": 220752, "start": 2207.52, "end": 2216.52, "text": " Or even return.", "tokens": [1610, 754, 2736, 13], "temperature": 0.0, "avg_logprob": -0.1897079414791531, "compression_ratio": 0.8461538461538461, "no_speech_prob": 3.0415399123739917e-06}, {"id": 216, "seek": 220752, "start": 2216.52, "end": 2226.52, "text": " So now we've got the config.", "tokens": [407, 586, 321, 600, 658, 264, 6662, 13], "temperature": 0.0, "avg_logprob": -0.1897079414791531, "compression_ratio": 0.8461538461538461, "no_speech_prob": 3.0415399123739917e-06}, {"id": 217, "seek": 222652, "start": 2226.52, "end": 2240.52, "text": " And so we can pass the config.", "tokens": [400, 370, 321, 393, 1320, 264, 6662, 13], "temperature": 0.0, "avg_logprob": -0.10371460517247517, "compression_ratio": 0.8333333333333334, "no_speech_prob": 2.8397385904099792e-05}, {"id": 218, "seek": 224052, "start": 2240.52, "end": 2262.52, "text": " So let's see how much we.", "tokens": [407, 718, 311, 536, 577, 709, 321, 13], "temperature": 0.2, "avg_logprob": -0.991864283879598, "compression_ratio": 0.7575757575757576, "no_speech_prob": 3.1160572689259425e-05}, {"id": 219, "seek": 226252, "start": 2262.52, "end": 2270.52, "text": " We can pass it.", "tokens": [492, 393, 1320, 309, 13], "temperature": 0.0, "avg_logprob": -0.4196800138892197, "compression_ratio": 1.15, "no_speech_prob": 7.0673504524165764e-06}, {"id": 220, "seek": 226252, "start": 2270.52, "end": 2271.52, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.4196800138892197, "compression_ratio": 1.15, "no_speech_prob": 7.0673504524165764e-06}, {"id": 221, "seek": 226252, "start": 2271.52, "end": 2279.52, "text": " So create Tim model.", "tokens": [407, 1884, 7172, 2316, 13], "temperature": 0.0, "avg_logprob": -0.4196800138892197, "compression_ratio": 1.15, "no_speech_prob": 7.0673504524165764e-06}, {"id": 222, "seek": 226252, "start": 2279.52, "end": 2282.52, "text": " Yes, we do pass it in architecture after all.", "tokens": [1079, 11, 321, 360, 1320, 309, 294, 9482, 934, 439, 13], "temperature": 0.0, "avg_logprob": -0.4196800138892197, "compression_ratio": 1.15, "no_speech_prob": 7.0673504524165764e-06}, {"id": 223, "seek": 226252, "start": 2282.52, "end": 2291.52, "text": " We just changed that back.", "tokens": [492, 445, 3105, 300, 646, 13], "temperature": 0.0, "avg_logprob": -0.4196800138892197, "compression_ratio": 1.15, "no_speech_prob": 7.0673504524165764e-06}, {"id": 224, "seek": 229152, "start": 2291.52, "end": 2299.52, "text": " So we should find that if we create a.", "tokens": [407, 321, 820, 915, 300, 498, 321, 1884, 257, 13], "temperature": 0.0, "avg_logprob": -0.2525382428555875, "compression_ratio": 1.101123595505618, "no_speech_prob": 6.143864538898924e-06}, {"id": 225, "seek": 229152, "start": 2299.52, "end": 2305.52, "text": " The IT.", "tokens": [440, 6783, 13], "temperature": 0.0, "avg_logprob": -0.2525382428555875, "compression_ratio": 1.101123595505618, "no_speech_prob": 6.143864538898924e-06}, {"id": 226, "seek": 229152, "start": 2305.52, "end": 2309.52, "text": " And check its default config.", "tokens": [400, 1520, 1080, 7576, 6662, 13], "temperature": 0.0, "avg_logprob": -0.2525382428555875, "compression_ratio": 1.101123595505618, "no_speech_prob": 6.143864538898924e-06}, {"id": 227, "seek": 229152, "start": 2309.52, "end": 2310.52, "text": " Yep.", "tokens": [7010, 13], "temperature": 0.0, "avg_logprob": -0.2525382428555875, "compression_ratio": 1.101123595505618, "no_speech_prob": 6.143864538898924e-06}, {"id": 228, "seek": 229152, "start": 2310.52, "end": 2319.52, "text": " That looks good.", "tokens": [663, 1542, 665, 13], "temperature": 0.0, "avg_logprob": -0.2525382428555875, "compression_ratio": 1.101123595505618, "no_speech_prob": 6.143864538898924e-06}, {"id": 229, "seek": 231952, "start": 2319.52, "end": 2324.52, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.37860023975372314, "compression_ratio": 1.0978260869565217, "no_speech_prob": 1.3210581528255716e-05}, {"id": 230, "seek": 231952, "start": 2324.52, "end": 2328.52, "text": " Now conflicts tiny on the other hand, uses image net stats.", "tokens": [823, 19807, 5870, 322, 264, 661, 1011, 11, 4960, 3256, 2533, 18152, 13], "temperature": 0.0, "avg_logprob": -0.37860023975372314, "compression_ratio": 1.0978260869565217, "no_speech_prob": 1.3210581528255716e-05}, {"id": 231, "seek": 231952, "start": 2328.52, "end": 2337.52, "text": " Excellent.", "tokens": [16723, 13], "temperature": 0.0, "avg_logprob": -0.37860023975372314, "compression_ratio": 1.0978260869565217, "no_speech_prob": 1.3210581528255716e-05}, {"id": 232, "seek": 231952, "start": 2337.52, "end": 2348.52, "text": " That looks very hopeful.", "tokens": [663, 1542, 588, 20531, 13], "temperature": 0.0, "avg_logprob": -0.37860023975372314, "compression_ratio": 1.0978260869565217, "no_speech_prob": 1.3210581528255716e-05}, {"id": 233, "seek": 234852, "start": 2348.52, "end": 2351.52, "text": " So I think that's a very interesting and.", "tokens": [407, 286, 519, 300, 311, 257, 588, 1880, 293, 13], "temperature": 0.0, "avg_logprob": -0.37582482610430035, "compression_ratio": 1.3687943262411348, "no_speech_prob": 2.2469454052043147e-05}, {"id": 234, "seek": 234852, "start": 2351.52, "end": 2356.52, "text": " Valuable problem to solve.", "tokens": [691, 38060, 1154, 281, 5039, 13], "temperature": 0.0, "avg_logprob": -0.37582482610430035, "compression_ratio": 1.3687943262411348, "no_speech_prob": 2.2469454052043147e-05}, {"id": 235, "seek": 234852, "start": 2356.52, "end": 2361.52, "text": " Making create unit model work with Tim would be super helpful.", "tokens": [14595, 1884, 4985, 2316, 589, 365, 7172, 576, 312, 1687, 4961, 13], "temperature": 0.0, "avg_logprob": -0.37582482610430035, "compression_ratio": 1.3687943262411348, "no_speech_prob": 2.2469454052043147e-05}, {"id": 236, "seek": 234852, "start": 2361.52, "end": 2371.52, "text": " All right. Now create unit model.", "tokens": [1057, 558, 13, 823, 1884, 4985, 2316, 13], "temperature": 0.0, "avg_logprob": -0.37582482610430035, "compression_ratio": 1.3687943262411348, "no_speech_prob": 2.2469454052043147e-05}, {"id": 237, "seek": 234852, "start": 2371.52, "end": 2375.52, "text": " Needs to do the same thing.", "tokens": [1734, 5147, 281, 360, 264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.37582482610430035, "compression_ratio": 1.3687943262411348, "no_speech_prob": 2.2469454052043147e-05}, {"id": 238, "seek": 237552, "start": 2375.52, "end": 2378.52, "text": " So let's see if we can.", "tokens": [407, 718, 311, 536, 498, 321, 393, 13], "temperature": 0.0, "avg_logprob": -0.23635338013430676, "compression_ratio": 1.4285714285714286, "no_speech_prob": 9.221511390933301e-06}, {"id": 239, "seek": 237552, "start": 2378.52, "end": 2385.52, "text": " Instantiate the model.", "tokens": [2730, 11520, 473, 264, 2316, 13], "temperature": 0.0, "avg_logprob": -0.23635338013430676, "compression_ratio": 1.4285714285714286, "no_speech_prob": 9.221511390933301e-06}, {"id": 240, "seek": 237552, "start": 2385.52, "end": 2388.52, "text": " Is anybody potentially interested in having a go at doing unit models", "tokens": [1119, 4472, 7263, 3102, 294, 1419, 257, 352, 412, 884, 4985, 5245], "temperature": 0.0, "avg_logprob": -0.23635338013430676, "compression_ratio": 1.4285714285714286, "no_speech_prob": 9.221511390933301e-06}, {"id": 241, "seek": 237552, "start": 2388.52, "end": 2390.52, "text": " with Tim? If so, did you want to talk about it?", "tokens": [365, 7172, 30, 759, 370, 11, 630, 291, 528, 281, 751, 466, 309, 30], "temperature": 0.0, "avg_logprob": -0.23635338013430676, "compression_ratio": 1.4285714285714286, "no_speech_prob": 9.221511390933301e-06}, {"id": 242, "seek": 237552, "start": 2390.52, "end": 2391.52, "text": " I'd be interested.", "tokens": [286, 1116, 312, 3102, 13], "temperature": 0.0, "avg_logprob": -0.23635338013430676, "compression_ratio": 1.4285714285714286, "no_speech_prob": 9.221511390933301e-06}, {"id": 243, "seek": 237552, "start": 2391.52, "end": 2392.52, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.23635338013430676, "compression_ratio": 1.4285714285714286, "no_speech_prob": 9.221511390933301e-06}, {"id": 244, "seek": 237552, "start": 2392.52, "end": 2399.52, "text": " So.", "tokens": [407, 13], "temperature": 0.0, "avg_logprob": -0.23635338013430676, "compression_ratio": 1.4285714285714286, "no_speech_prob": 9.221511390933301e-06}, {"id": 245, "seek": 237552, "start": 2399.52, "end": 2402.52, "text": " All right. Let's just get this working first.", "tokens": [1057, 558, 13, 961, 311, 445, 483, 341, 1364, 700, 13], "temperature": 0.0, "avg_logprob": -0.23635338013430676, "compression_ratio": 1.4285714285714286, "no_speech_prob": 9.221511390933301e-06}, {"id": 246, "seek": 237552, "start": 2402.52, "end": 2404.52, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.23635338013430676, "compression_ratio": 1.4285714285714286, "no_speech_prob": 9.221511390933301e-06}, {"id": 247, "seek": 240452, "start": 2404.52, "end": 2405.52, "text": " So Tim,", "tokens": [407, 7172, 11], "temperature": 0.0, "avg_logprob": -0.38111283702235066, "compression_ratio": 1.36, "no_speech_prob": 1.11233193820226e-05}, {"id": 248, "seek": 240452, "start": 2405.52, "end": 2408.52, "text": " do you have any experience with using units?", "tokens": [360, 291, 362, 604, 1752, 365, 1228, 6815, 30], "temperature": 0.0, "avg_logprob": -0.38111283702235066, "compression_ratio": 1.36, "no_speech_prob": 1.11233193820226e-05}, {"id": 249, "seek": 240452, "start": 2408.52, "end": 2410.52, "text": " In general.", "tokens": [682, 2674, 13], "temperature": 0.0, "avg_logprob": -0.38111283702235066, "compression_ratio": 1.36, "no_speech_prob": 1.11233193820226e-05}, {"id": 250, "seek": 240452, "start": 2410.52, "end": 2414.52, "text": " Dynamic unit.", "tokens": [45440, 4985, 13], "temperature": 0.0, "avg_logprob": -0.38111283702235066, "compression_ratio": 1.36, "no_speech_prob": 1.11233193820226e-05}, {"id": 251, "seek": 240452, "start": 2414.52, "end": 2416.52, "text": " A little bit. I'm training one at the moment.", "tokens": [316, 707, 857, 13, 286, 478, 3097, 472, 412, 264, 1623, 13], "temperature": 0.0, "avg_logprob": -0.38111283702235066, "compression_ratio": 1.36, "no_speech_prob": 1.11233193820226e-05}, {"id": 252, "seek": 240452, "start": 2416.52, "end": 2418.52, "text": " That's my maximum experience.", "tokens": [663, 311, 452, 6674, 1752, 13], "temperature": 0.0, "avg_logprob": -0.38111283702235066, "compression_ratio": 1.36, "no_speech_prob": 1.11233193820226e-05}, {"id": 253, "seek": 240452, "start": 2418.52, "end": 2420.52, "text": " I've been through some notebooks to walk through.", "tokens": [286, 600, 668, 807, 512, 43782, 281, 1792, 807, 13], "temperature": 0.0, "avg_logprob": -0.38111283702235066, "compression_ratio": 1.36, "no_speech_prob": 1.11233193820226e-05}, {"id": 254, "seek": 242052, "start": 2420.52, "end": 2435.52, "text": " And I've been able to do some.", "tokens": [400, 286, 600, 668, 1075, 281, 360, 512, 13], "temperature": 0.0, "avg_logprob": -0.3941649160077495, "compression_ratio": 1.335483870967742, "no_speech_prob": 1.147483999375254e-05}, {"id": 255, "seek": 242052, "start": 2435.52, "end": 2439.52, "text": " The interesting, okay. So, you know, the basic idea of a unit is.", "tokens": [440, 1880, 11, 1392, 13, 407, 11, 291, 458, 11, 264, 3875, 1558, 295, 257, 4985, 307, 13], "temperature": 0.0, "avg_logprob": -0.3941649160077495, "compression_ratio": 1.335483870967742, "no_speech_prob": 1.147483999375254e-05}, {"id": 256, "seek": 242052, "start": 2439.52, "end": 2440.52, "text": " That it has.", "tokens": [663, 309, 575, 13], "temperature": 0.0, "avg_logprob": -0.3941649160077495, "compression_ratio": 1.335483870967742, "no_speech_prob": 1.147483999375254e-05}, {"id": 257, "seek": 242052, "start": 2440.52, "end": 2446.52, "text": " Not just the usual kind of.", "tokens": [1726, 445, 264, 7713, 733, 295, 13], "temperature": 0.0, "avg_logprob": -0.3941649160077495, "compression_ratio": 1.335483870967742, "no_speech_prob": 1.147483999375254e-05}, {"id": 258, "seek": 242052, "start": 2446.52, "end": 2449.52, "text": " Downward sampling path where the image is getting kind of effectively", "tokens": [9506, 1007, 21179, 3100, 689, 264, 3256, 307, 1242, 733, 295, 8659], "temperature": 0.0, "avg_logprob": -0.3941649160077495, "compression_ratio": 1.335483870967742, "no_speech_prob": 1.147483999375254e-05}, {"id": 259, "seek": 244952, "start": 2449.52, "end": 2451.52, "text": " through the input from the previous layer.", "tokens": [807, 264, 4846, 490, 264, 3894, 4583, 13], "temperature": 0.0, "avg_logprob": -0.37866609236773324, "compression_ratio": 2.2447257383966246, "no_speech_prob": 2.9298371373442933e-05}, {"id": 260, "seek": 244952, "start": 2451.52, "end": 2454.52, "text": " And then we also take the input from the previous layer.", "tokens": [400, 550, 321, 611, 747, 264, 4846, 490, 264, 3894, 4583, 13], "temperature": 0.0, "avg_logprob": -0.37866609236773324, "compression_ratio": 2.2447257383966246, "no_speech_prob": 2.9298371373442933e-05}, {"id": 261, "seek": 244952, "start": 2454.52, "end": 2456.52, "text": " And then we also take the input from the previous layer.", "tokens": [400, 550, 321, 611, 747, 264, 4846, 490, 264, 3894, 4583, 13], "temperature": 0.0, "avg_logprob": -0.37866609236773324, "compression_ratio": 2.2447257383966246, "no_speech_prob": 2.9298371373442933e-05}, {"id": 262, "seek": 244952, "start": 2456.52, "end": 2460.52, "text": " And then rather than averaging those to get a vector and using those as", "tokens": [400, 550, 2831, 813, 47308, 729, 281, 483, 257, 8062, 293, 1228, 729, 382], "temperature": 0.0, "avg_logprob": -0.37866609236773324, "compression_ratio": 2.2447257383966246, "no_speech_prob": 2.9298371373442933e-05}, {"id": 263, "seek": 244952, "start": 2460.52, "end": 2461.52, "text": " our features for our head.", "tokens": [527, 4122, 337, 527, 1378, 13], "temperature": 0.0, "avg_logprob": -0.37866609236773324, "compression_ratio": 2.2447257383966246, "no_speech_prob": 2.9298371373442933e-05}, {"id": 264, "seek": 244952, "start": 2461.52, "end": 2463.52, "text": " Instead, we go through reverse convolutions,", "tokens": [7156, 11, 321, 352, 807, 9943, 3754, 15892, 11], "temperature": 0.0, "avg_logprob": -0.37866609236773324, "compression_ratio": 2.2447257383966246, "no_speech_prob": 2.9298371373442933e-05}, {"id": 265, "seek": 244952, "start": 2463.52, "end": 2465.52, "text": " which are things which make it bigger and bigger.", "tokens": [597, 366, 721, 597, 652, 309, 3801, 293, 3801, 13], "temperature": 0.0, "avg_logprob": -0.37866609236773324, "compression_ratio": 2.2447257383966246, "no_speech_prob": 2.9298371373442933e-05}, {"id": 266, "seek": 244952, "start": 2465.52, "end": 2470.52, "text": " And as we do that, we also.", "tokens": [400, 382, 321, 360, 300, 11, 321, 611, 13], "temperature": 0.0, "avg_logprob": -0.37866609236773324, "compression_ratio": 2.2447257383966246, "no_speech_prob": 2.9298371373442933e-05}, {"id": 267, "seek": 244952, "start": 2470.52, "end": 2473.52, "text": " Don't just take the input from the previous layer of the up sampling,", "tokens": [1468, 380, 445, 747, 264, 4846, 490, 264, 3894, 4583, 295, 264, 493, 21179, 11], "temperature": 0.0, "avg_logprob": -0.37866609236773324, "compression_ratio": 2.2447257383966246, "no_speech_prob": 2.9298371373442933e-05}, {"id": 268, "seek": 244952, "start": 2473.52, "end": 2476.52, "text": " but also the input from the equivalently sized down sample,", "tokens": [457, 611, 264, 4846, 490, 264, 9052, 2276, 20004, 760, 6889, 11], "temperature": 0.0, "avg_logprob": -0.37866609236773324, "compression_ratio": 2.2447257383966246, "no_speech_prob": 2.9298371373442933e-05}, {"id": 269, "seek": 244952, "start": 2476.52, "end": 2477.52, "text": " the size down sampling.", "tokens": [264, 2744, 760, 21179, 13], "temperature": 0.0, "avg_logprob": -0.37866609236773324, "compression_ratio": 2.2447257383966246, "no_speech_prob": 2.9298371373442933e-05}, {"id": 270, "seek": 247752, "start": 2477.52, "end": 2479.52, "text": " And then we also take the input from the previous layer.", "tokens": [400, 550, 321, 611, 747, 264, 4846, 490, 264, 3894, 4583, 13], "temperature": 0.4, "avg_logprob": -0.4212319240097172, "compression_ratio": 1.8695652173913044, "no_speech_prob": 3.3202704798895866e-05}, {"id": 271, "seek": 247752, "start": 2479.52, "end": 2481.52, "text": " And then we also take the input from the previous layer.", "tokens": [400, 550, 321, 611, 747, 264, 4846, 490, 264, 3894, 4583, 13], "temperature": 0.4, "avg_logprob": -0.4212319240097172, "compression_ratio": 1.8695652173913044, "no_speech_prob": 3.3202704798895866e-05}, {"id": 272, "seek": 247752, "start": 2481.52, "end": 2482.52, "text": " And so that's how we do it.", "tokens": [400, 370, 300, 311, 577, 321, 360, 309, 13], "temperature": 0.4, "avg_logprob": -0.4212319240097172, "compression_ratio": 1.8695652173913044, "no_speech_prob": 3.3202704798895866e-05}, {"id": 273, "seek": 247752, "start": 2482.52, "end": 2483.52, "text": " So we had to be.", "tokens": [407, 321, 632, 281, 312, 13], "temperature": 0.4, "avg_logprob": -0.4212319240097172, "compression_ratio": 1.8695652173913044, "no_speech_prob": 3.3202704798895866e-05}, {"id": 274, "seek": 247752, "start": 2483.52, "end": 2487.52, "text": " Only handled a fixed size.", "tokens": [5686, 18033, 257, 6806, 2744, 13], "temperature": 0.4, "avg_logprob": -0.4212319240097172, "compression_ratio": 1.8695652173913044, "no_speech_prob": 3.3202704798895866e-05}, {"id": 275, "seek": 247752, "start": 2487.52, "end": 2490.52, "text": " But Keram did was he created this thing called the dynamic unit,", "tokens": [583, 20706, 335, 630, 390, 415, 2942, 341, 551, 1219, 264, 8546, 4985, 11], "temperature": 0.4, "avg_logprob": -0.4212319240097172, "compression_ratio": 1.8695652173913044, "no_speech_prob": 3.3202704798895866e-05}, {"id": 276, "seek": 247752, "start": 2490.52, "end": 2491.52, "text": " which would look to see how big.", "tokens": [597, 576, 574, 281, 536, 577, 955, 13], "temperature": 0.4, "avg_logprob": -0.4212319240097172, "compression_ratio": 1.8695652173913044, "no_speech_prob": 3.3202704798895866e-05}, {"id": 277, "seek": 247752, "start": 2491.52, "end": 2494.52, "text": " Each size was on the downward path and automatically create.", "tokens": [6947, 2744, 390, 322, 264, 24805, 3100, 293, 6772, 1884, 13], "temperature": 0.4, "avg_logprob": -0.4212319240097172, "compression_ratio": 1.8695652173913044, "no_speech_prob": 3.3202704798895866e-05}, {"id": 278, "seek": 247752, "start": 2494.52, "end": 2495.52, "text": " An appropriate size thing.", "tokens": [1107, 6854, 2744, 551, 13], "temperature": 0.4, "avg_logprob": -0.4212319240097172, "compression_ratio": 1.8695652173913044, "no_speech_prob": 3.3202704798895866e-05}, {"id": 279, "seek": 247752, "start": 2495.52, "end": 2500.52, "text": " On the upward path.", "tokens": [1282, 264, 23452, 3100, 13], "temperature": 0.4, "avg_logprob": -0.4212319240097172, "compression_ratio": 1.8695652173913044, "no_speech_prob": 3.3202704798895866e-05}, {"id": 280, "seek": 247752, "start": 2500.52, "end": 2505.52, "text": " And that's what the dynamic unit does.", "tokens": [400, 300, 311, 437, 264, 8546, 4985, 775, 13], "temperature": 0.4, "avg_logprob": -0.4212319240097172, "compression_ratio": 1.8695652173913044, "no_speech_prob": 3.3202704798895866e-05}, {"id": 281, "seek": 250552, "start": 2505.52, "end": 2508.52, "text": " And so it's not something that's necessarily.", "tokens": [400, 370, 309, 311, 406, 746, 300, 311, 4725, 13], "temperature": 0.4, "avg_logprob": -0.37741802584740425, "compression_ratio": 1.5435897435897437, "no_speech_prob": 1.7499285604571924e-05}, {"id": 282, "seek": 250552, "start": 2508.52, "end": 2509.52, "text": " Aggressive.", "tokens": [41512, 22733, 13], "temperature": 0.4, "avg_logprob": -0.37741802584740425, "compression_ratio": 1.5435897435897437, "no_speech_prob": 1.7499285604571924e-05}, {"id": 283, "seek": 250552, "start": 2509.52, "end": 2511.52, "text": " In like using pre-trained models everywhere.", "tokens": [682, 411, 1228, 659, 12, 17227, 2001, 5245, 5315, 13], "temperature": 0.4, "avg_logprob": -0.37741802584740425, "compression_ratio": 1.5435897435897437, "no_speech_prob": 1.7499285604571924e-05}, {"id": 284, "seek": 250552, "start": 2511.52, "end": 2513.52, "text": " So something we added to this idea is this idea that.", "tokens": [407, 746, 321, 3869, 281, 341, 1558, 307, 341, 1558, 300, 13], "temperature": 0.4, "avg_logprob": -0.37741802584740425, "compression_ratio": 1.5435897435897437, "no_speech_prob": 1.7499285604571924e-05}, {"id": 285, "seek": 250552, "start": 2513.52, "end": 2514.52, "text": " The downward sampling path.", "tokens": [440, 24805, 21179, 3100, 13], "temperature": 0.4, "avg_logprob": -0.37741802584740425, "compression_ratio": 1.5435897435897437, "no_speech_prob": 1.7499285604571924e-05}, {"id": 286, "seek": 250552, "start": 2514.52, "end": 2515.52, "text": " Can be.", "tokens": [1664, 312, 13], "temperature": 0.4, "avg_logprob": -0.37741802584740425, "compression_ratio": 1.5435897435897437, "no_speech_prob": 1.7499285604571924e-05}, {"id": 287, "seek": 250552, "start": 2515.52, "end": 2517.52, "text": " Can have a pre-trained model.", "tokens": [1664, 362, 257, 659, 12, 17227, 2001, 2316, 13], "temperature": 0.4, "avg_logprob": -0.37741802584740425, "compression_ratio": 1.5435897435897437, "no_speech_prob": 1.7499285604571924e-05}, {"id": 288, "seek": 250552, "start": 2517.52, "end": 2518.52, "text": " Which is.", "tokens": [3013, 307, 13], "temperature": 0.4, "avg_logprob": -0.37741802584740425, "compression_ratio": 1.5435897435897437, "no_speech_prob": 1.7499285604571924e-05}, {"id": 289, "seek": 250552, "start": 2518.52, "end": 2519.52, "text": " Not rocket science.", "tokens": [1726, 13012, 3497, 13], "temperature": 0.4, "avg_logprob": -0.37741802584740425, "compression_ratio": 1.5435897435897437, "no_speech_prob": 1.7499285604571924e-05}, {"id": 290, "seek": 250552, "start": 2519.52, "end": 2524.52, "text": " Obviously it's like this, this one line of code.", "tokens": [7580, 309, 311, 411, 341, 11, 341, 472, 1622, 295, 3089, 13], "temperature": 0.4, "avg_logprob": -0.37741802584740425, "compression_ratio": 1.5435897435897437, "no_speech_prob": 1.7499285604571924e-05}, {"id": 291, "seek": 252452, "start": 2524.52, "end": 2541.52, "text": " And then we also have the.", "tokens": [400, 550, 321, 611, 362, 264, 13], "temperature": 0.0, "avg_logprob": -0.48945333087255083, "compression_ratio": 1.4155844155844155, "no_speech_prob": 5.593396508629667e-06}, {"id": 292, "seek": 252452, "start": 2541.52, "end": 2544.52, "text": " So to understand like at the moment I'm using say like a ResNet", "tokens": [407, 281, 1223, 411, 412, 264, 1623, 286, 478, 1228, 584, 411, 257, 5015, 31890], "temperature": 0.0, "avg_logprob": -0.48945333087255083, "compression_ratio": 1.4155844155844155, "no_speech_prob": 5.593396508629667e-06}, {"id": 293, "seek": 252452, "start": 2544.52, "end": 2546.52, "text": " 34. Does that mean the down path?", "tokens": [12790, 13, 4402, 300, 914, 264, 760, 3100, 30], "temperature": 0.0, "avg_logprob": -0.48945333087255083, "compression_ratio": 1.4155844155844155, "no_speech_prob": 5.593396508629667e-06}, {"id": 294, "seek": 252452, "start": 2546.52, "end": 2548.52, "text": " Is it ResNet 34 backbone?", "tokens": [1119, 309, 5015, 31890, 12790, 34889, 30], "temperature": 0.0, "avg_logprob": -0.48945333087255083, "compression_ratio": 1.4155844155844155, "no_speech_prob": 5.593396508629667e-06}, {"id": 295, "seek": 252452, "start": 2548.52, "end": 2551.52, "text": " And then there's a reverse ResNet 34 being automatically generated.", "tokens": [400, 550, 456, 311, 257, 9943, 5015, 31890, 12790, 885, 6772, 10833, 13], "temperature": 0.0, "avg_logprob": -0.48945333087255083, "compression_ratio": 1.4155844155844155, "no_speech_prob": 5.593396508629667e-06}, {"id": 296, "seek": 255152, "start": 2551.52, "end": 2554.52, "text": " And then there's a reverse ResNet 34.", "tokens": [400, 550, 456, 311, 257, 9943, 5015, 31890, 12790, 13], "temperature": 0.0, "avg_logprob": -0.3011133066813151, "compression_ratio": 1.4928571428571429, "no_speech_prob": 4.859849923377624e-06}, {"id": 297, "seek": 255152, "start": 2554.52, "end": 2556.52, "text": " It's not a reverse ResNet 34.", "tokens": [467, 311, 406, 257, 9943, 5015, 31890, 12790, 13], "temperature": 0.0, "avg_logprob": -0.3011133066813151, "compression_ratio": 1.4928571428571429, "no_speech_prob": 4.859849923377624e-06}, {"id": 298, "seek": 255152, "start": 2556.52, "end": 2559.52, "text": " It's it is a ResNet 34 backbone.", "tokens": [467, 311, 309, 307, 257, 5015, 31890, 12790, 34889, 13], "temperature": 0.0, "avg_logprob": -0.3011133066813151, "compression_ratio": 1.4928571428571429, "no_speech_prob": 4.859849923377624e-06}, {"id": 299, "seek": 255152, "start": 2559.52, "end": 2560.52, "text": " So here's our dynamic unit.", "tokens": [407, 510, 311, 527, 8546, 4985, 13], "temperature": 0.0, "avg_logprob": -0.3011133066813151, "compression_ratio": 1.4928571428571429, "no_speech_prob": 4.859849923377624e-06}, {"id": 300, "seek": 255152, "start": 2560.52, "end": 2562.52, "text": " The upward sample.", "tokens": [440, 23452, 6889, 13], "temperature": 0.0, "avg_logprob": -0.3011133066813151, "compression_ratio": 1.4928571428571429, "no_speech_prob": 4.859849923377624e-06}, {"id": 301, "seek": 255152, "start": 2562.52, "end": 2564.52, "text": " The up sampling path.", "tokens": [440, 493, 21179, 3100, 13], "temperature": 0.0, "avg_logprob": -0.3011133066813151, "compression_ratio": 1.4928571428571429, "no_speech_prob": 4.859849923377624e-06}, {"id": 302, "seek": 255152, "start": 2564.52, "end": 2566.52, "text": " Is.", "tokens": [1119, 13], "temperature": 0.0, "avg_logprob": -0.3011133066813151, "compression_ratio": 1.4928571428571429, "no_speech_prob": 4.859849923377624e-06}, {"id": 303, "seek": 255152, "start": 2566.52, "end": 2571.52, "text": " Has a fixed architecture.", "tokens": [8646, 257, 6806, 9482, 13], "temperature": 0.0, "avg_logprob": -0.3011133066813151, "compression_ratio": 1.4928571428571429, "no_speech_prob": 4.859849923377624e-06}, {"id": 304, "seek": 255152, "start": 2571.52, "end": 2580.52, "text": " Which is.", "tokens": [3013, 307, 13], "temperature": 0.0, "avg_logprob": -0.3011133066813151, "compression_ratio": 1.4928571428571429, "no_speech_prob": 4.859849923377624e-06}, {"id": 305, "seek": 258052, "start": 2580.52, "end": 2581.52, "text": " A reverse VIT.", "tokens": [316, 9943, 691, 3927, 13], "temperature": 0.0, "avg_logprob": -0.2771056239590323, "compression_ratio": 1.7919708029197081, "no_speech_prob": 1.341838014923269e-05}, {"id": 306, "seek": 258052, "start": 2581.52, "end": 2583.52, "text": " So it's not a reverse VIT.", "tokens": [407, 309, 311, 406, 257, 9943, 691, 3927, 13], "temperature": 0.0, "avg_logprob": -0.2771056239590323, "compression_ratio": 1.7919708029197081, "no_speech_prob": 1.341838014923269e-05}, {"id": 307, "seek": 258052, "start": 2583.52, "end": 2584.52, "text": " But they're not like.", "tokens": [583, 436, 434, 406, 411, 13], "temperature": 0.0, "avg_logprob": -0.2771056239590323, "compression_ratio": 1.7919708029197081, "no_speech_prob": 1.341838014923269e-05}, {"id": 308, "seek": 258052, "start": 2584.52, "end": 2586.52, "text": " If you use as a downward sampling path.", "tokens": [759, 291, 764, 382, 257, 24805, 21179, 3100, 13], "temperature": 0.0, "avg_logprob": -0.2771056239590323, "compression_ratio": 1.7919708029197081, "no_speech_prob": 1.341838014923269e-05}, {"id": 309, "seek": 258052, "start": 2586.52, "end": 2587.52, "text": " You know,", "tokens": [509, 458, 11], "temperature": 0.0, "avg_logprob": -0.2771056239590323, "compression_ratio": 1.7919708029197081, "no_speech_prob": 1.341838014923269e-05}, {"id": 310, "seek": 258052, "start": 2587.52, "end": 2589.52, "text": " Downward sampling of VIT the upward sampling is not going to be a", "tokens": [9506, 1007, 21179, 295, 691, 3927, 264, 23452, 21179, 307, 406, 516, 281, 312, 257], "temperature": 0.0, "avg_logprob": -0.2771056239590323, "compression_ratio": 1.7919708029197081, "no_speech_prob": 1.341838014923269e-05}, {"id": 311, "seek": 258052, "start": 2589.52, "end": 2590.52, "text": " reverse VIT.", "tokens": [9943, 691, 3927, 13], "temperature": 0.0, "avg_logprob": -0.2771056239590323, "compression_ratio": 1.7919708029197081, "no_speech_prob": 1.341838014923269e-05}, {"id": 312, "seek": 258052, "start": 2590.52, "end": 2591.52, "text": " It's not a mirror.", "tokens": [467, 311, 406, 257, 8013, 13], "temperature": 0.0, "avg_logprob": -0.2771056239590323, "compression_ratio": 1.7919708029197081, "no_speech_prob": 1.341838014923269e-05}, {"id": 313, "seek": 258052, "start": 2591.52, "end": 2592.52, "text": " No, exactly.", "tokens": [883, 11, 2293, 13], "temperature": 0.0, "avg_logprob": -0.2771056239590323, "compression_ratio": 1.7919708029197081, "no_speech_prob": 1.341838014923269e-05}, {"id": 314, "seek": 258052, "start": 2592.52, "end": 2594.52, "text": " Would there be an advantage in doing that?", "tokens": [6068, 456, 312, 364, 5002, 294, 884, 300, 30], "temperature": 0.0, "avg_logprob": -0.2771056239590323, "compression_ratio": 1.7919708029197081, "no_speech_prob": 1.341838014923269e-05}, {"id": 315, "seek": 258052, "start": 2594.52, "end": 2595.52, "text": " Or is it just not really helpful?", "tokens": [1610, 307, 309, 445, 406, 534, 4961, 30], "temperature": 0.0, "avg_logprob": -0.2771056239590323, "compression_ratio": 1.7919708029197081, "no_speech_prob": 1.341838014923269e-05}, {"id": 316, "seek": 258052, "start": 2595.52, "end": 2597.52, "text": " I don't see why there would be.", "tokens": [286, 500, 380, 536, 983, 456, 576, 312, 13], "temperature": 0.0, "avg_logprob": -0.2771056239590323, "compression_ratio": 1.7919708029197081, "no_speech_prob": 1.341838014923269e-05}, {"id": 317, "seek": 258052, "start": 2597.52, "end": 2599.52, "text": " I'd also don't see why there wouldn't be.", "tokens": [286, 1116, 611, 500, 380, 536, 983, 456, 2759, 380, 312, 13], "temperature": 0.0, "avg_logprob": -0.2771056239590323, "compression_ratio": 1.7919708029197081, "no_speech_prob": 1.341838014923269e-05}, {"id": 318, "seek": 258052, "start": 2599.52, "end": 2601.52, "text": " Nobody's tried it as far as I know.", "tokens": [9297, 311, 3031, 309, 382, 1400, 382, 286, 458, 13], "temperature": 0.0, "avg_logprob": -0.2771056239590323, "compression_ratio": 1.7919708029197081, "no_speech_prob": 1.341838014923269e-05}, {"id": 319, "seek": 258052, "start": 2601.52, "end": 2604.52, "text": " I don't even know if there's such a thing as an up sampling.", "tokens": [286, 500, 380, 754, 458, 498, 456, 311, 1270, 257, 551, 382, 364, 493, 21179, 13], "temperature": 0.0, "avg_logprob": -0.2771056239590323, "compression_ratio": 1.7919708029197081, "no_speech_prob": 1.341838014923269e-05}, {"id": 320, "seek": 258052, "start": 2604.52, "end": 2605.52, "text": " Transformer block.", "tokens": [27938, 260, 3461, 13], "temperature": 0.0, "avg_logprob": -0.2771056239590323, "compression_ratio": 1.7919708029197081, "no_speech_prob": 1.341838014923269e-05}, {"id": 321, "seek": 260552, "start": 2605.52, "end": 2610.52, "text": " There may well be.", "tokens": [821, 815, 731, 312, 13], "temperature": 0.0, "avg_logprob": -0.2184292653973183, "compression_ratio": 1.5776397515527951, "no_speech_prob": 7.071055733831599e-06}, {"id": 322, "seek": 260552, "start": 2610.52, "end": 2611.52, "text": " Without digressing.", "tokens": [9129, 2528, 18605, 13], "temperature": 0.0, "avg_logprob": -0.2184292653973183, "compression_ratio": 1.5776397515527951, "no_speech_prob": 7.071055733831599e-06}, {"id": 323, "seek": 260552, "start": 2611.52, "end": 2613.52, "text": " There's no need to worry about that.", "tokens": [821, 311, 572, 643, 281, 3292, 466, 300, 13], "temperature": 0.0, "avg_logprob": -0.2184292653973183, "compression_ratio": 1.5776397515527951, "no_speech_prob": 7.071055733831599e-06}, {"id": 324, "seek": 260552, "start": 2613.52, "end": 2615.52, "text": " The key thing is that.", "tokens": [440, 2141, 551, 307, 300, 13], "temperature": 0.0, "avg_logprob": -0.2184292653973183, "compression_ratio": 1.5776397515527951, "no_speech_prob": 7.071055733831599e-06}, {"id": 325, "seek": 260552, "start": 2615.52, "end": 2619.52, "text": " In the downward sampling path.", "tokens": [682, 264, 24805, 21179, 3100, 13], "temperature": 0.0, "avg_logprob": -0.2184292653973183, "compression_ratio": 1.5776397515527951, "no_speech_prob": 7.071055733831599e-06}, {"id": 326, "seek": 260552, "start": 2619.52, "end": 2623.52, "text": " What we do is we.", "tokens": [708, 321, 360, 307, 321, 13], "temperature": 0.0, "avg_logprob": -0.2184292653973183, "compression_ratio": 1.5776397515527951, "no_speech_prob": 7.071055733831599e-06}, {"id": 327, "seek": 260552, "start": 2623.52, "end": 2624.52, "text": " We have the downward sampling bit.", "tokens": [492, 362, 264, 24805, 21179, 857, 13], "temperature": 0.0, "avg_logprob": -0.2184292653973183, "compression_ratio": 1.5776397515527951, "no_speech_prob": 7.071055733831599e-06}, {"id": 328, "seek": 260552, "start": 2624.52, "end": 2626.52, "text": " We call the encoder.", "tokens": [492, 818, 264, 2058, 19866, 13], "temperature": 0.0, "avg_logprob": -0.2184292653973183, "compression_ratio": 1.5776397515527951, "no_speech_prob": 7.071055733831599e-06}, {"id": 329, "seek": 260552, "start": 2626.52, "end": 2627.52, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.2184292653973183, "compression_ratio": 1.5776397515527951, "no_speech_prob": 7.071055733831599e-06}, {"id": 330, "seek": 260552, "start": 2627.52, "end": 2629.52, "text": " And what we do is we do a.", "tokens": [400, 437, 321, 360, 307, 321, 360, 257, 13], "temperature": 0.0, "avg_logprob": -0.2184292653973183, "compression_ratio": 1.5776397515527951, "no_speech_prob": 7.071055733831599e-06}, {"id": 331, "seek": 260552, "start": 2629.52, "end": 2630.52, "text": " A dummy of vowel.", "tokens": [316, 35064, 295, 29410, 13], "temperature": 0.0, "avg_logprob": -0.2184292653973183, "compression_ratio": 1.5776397515527951, "no_speech_prob": 7.071055733831599e-06}, {"id": 332, "seek": 263052, "start": 2630.52, "end": 2635.52, "text": " Now a dummy of vowel is basically to take a, I can't remember.", "tokens": [823, 257, 35064, 295, 29410, 307, 1936, 281, 747, 257, 11, 286, 393, 380, 1604, 13], "temperature": 0.0, "avg_logprob": -0.17742742466021189, "compression_ratio": 1.4529411764705882, "no_speech_prob": 7.29496105122962e-06}, {"id": 333, "seek": 263052, "start": 2635.52, "end": 2637.52, "text": " Like I either a zero length batch or a one length batch,", "tokens": [1743, 286, 2139, 257, 4018, 4641, 15245, 420, 257, 472, 4641, 15245, 11], "temperature": 0.0, "avg_logprob": -0.17742742466021189, "compression_ratio": 1.4529411764705882, "no_speech_prob": 7.29496105122962e-06}, {"id": 334, "seek": 263052, "start": 2637.52, "end": 2638.52, "text": " like a very small batch.", "tokens": [411, 257, 588, 1359, 15245, 13], "temperature": 0.0, "avg_logprob": -0.17742742466021189, "compression_ratio": 1.4529411764705882, "no_speech_prob": 7.29496105122962e-06}, {"id": 335, "seek": 263052, "start": 2638.52, "end": 2639.52, "text": " And pass it through.", "tokens": [400, 1320, 309, 807, 13], "temperature": 0.0, "avg_logprob": -0.17742742466021189, "compression_ratio": 1.4529411764705882, "no_speech_prob": 7.29496105122962e-06}, {"id": 336, "seek": 263052, "start": 2639.52, "end": 2641.52, "text": " At some image size.", "tokens": [1711, 512, 3256, 2744, 13], "temperature": 0.0, "avg_logprob": -0.17742742466021189, "compression_ratio": 1.4529411764705882, "no_speech_prob": 7.29496105122962e-06}, {"id": 337, "seek": 263052, "start": 2641.52, "end": 2645.52, "text": " And.", "tokens": [400, 13], "temperature": 0.0, "avg_logprob": -0.17742742466021189, "compression_ratio": 1.4529411764705882, "no_speech_prob": 7.29496105122962e-06}, {"id": 338, "seek": 263052, "start": 2645.52, "end": 2647.52, "text": " We use, I believe we use hooks.", "tokens": [492, 764, 11, 286, 1697, 321, 764, 26485, 13], "temperature": 0.0, "avg_logprob": -0.17742742466021189, "compression_ratio": 1.4529411764705882, "no_speech_prob": 7.29496105122962e-06}, {"id": 339, "seek": 263052, "start": 2647.52, "end": 2658.52, "text": " If I remember correctly.", "tokens": [759, 286, 1604, 8944, 13], "temperature": 0.0, "avg_logprob": -0.17742742466021189, "compression_ratio": 1.4529411764705882, "no_speech_prob": 7.29496105122962e-06}, {"id": 340, "seek": 265852, "start": 2658.52, "end": 2660.52, "text": " I think I remember that.", "tokens": [286, 519, 286, 1604, 300, 13], "temperature": 0.0, "avg_logprob": -0.4850094754930953, "compression_ratio": 1.3448275862068966, "no_speech_prob": 1.2407037502271123e-05}, {"id": 341, "seek": 265852, "start": 2660.52, "end": 2661.52, "text": " What's happening to my screen.", "tokens": [708, 311, 2737, 281, 452, 2568, 13], "temperature": 0.0, "avg_logprob": -0.4850094754930953, "compression_ratio": 1.3448275862068966, "no_speech_prob": 1.2407037502271123e-05}, {"id": 342, "seek": 265852, "start": 2661.52, "end": 2666.52, "text": " My screen's gone crazy.", "tokens": [1222, 2568, 311, 2780, 3219, 13], "temperature": 0.0, "avg_logprob": -0.4850094754930953, "compression_ratio": 1.3448275862068966, "no_speech_prob": 1.2407037502271123e-05}, {"id": 343, "seek": 265852, "start": 2666.52, "end": 2670.52, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.4850094754930953, "compression_ratio": 1.3448275862068966, "no_speech_prob": 1.2407037502271123e-05}, {"id": 344, "seek": 265852, "start": 2670.52, "end": 2672.52, "text": " Yeah, so we've got these hawks.", "tokens": [865, 11, 370, 321, 600, 658, 613, 33634, 1694, 13], "temperature": 0.0, "avg_logprob": -0.4850094754930953, "compression_ratio": 1.3448275862068966, "no_speech_prob": 1.2407037502271123e-05}, {"id": 345, "seek": 265852, "start": 2672.52, "end": 2674.52, "text": " With a pie torch hawks.", "tokens": [2022, 257, 1730, 27822, 33634, 1694, 13], "temperature": 0.0, "avg_logprob": -0.4850094754930953, "compression_ratio": 1.3448275862068966, "no_speech_prob": 1.2407037502271123e-05}, {"id": 346, "seek": 265852, "start": 2674.52, "end": 2680.52, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.4850094754930953, "compression_ratio": 1.3448275862068966, "no_speech_prob": 1.2407037502271123e-05}, {"id": 347, "seek": 265852, "start": 2680.52, "end": 2686.52, "text": " Okay. So we use fast AI's hawk outputs function.", "tokens": [1033, 13, 407, 321, 764, 2370, 7318, 311, 33634, 74, 23930, 2445, 13], "temperature": 0.0, "avg_logprob": -0.4850094754930953, "compression_ratio": 1.3448275862068966, "no_speech_prob": 1.2407037502271123e-05}, {"id": 348, "seek": 268652, "start": 2686.52, "end": 2688.52, "text": " And we use the same.", "tokens": [400, 321, 764, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.4469383145556038, "compression_ratio": 1.4965034965034965, "no_speech_prob": 6.143319751572562e-06}, {"id": 349, "seek": 268652, "start": 2688.52, "end": 2696.52, "text": " And so.", "tokens": [400, 370, 13], "temperature": 0.0, "avg_logprob": -0.4469383145556038, "compression_ratio": 1.4965034965034965, "no_speech_prob": 6.143319751572562e-06}, {"id": 350, "seek": 268652, "start": 2696.52, "end": 2698.52, "text": " What is SCCGCHG indexes?", "tokens": [708, 307, 9028, 34, 38, 5462, 38, 8186, 279, 30], "temperature": 0.0, "avg_logprob": -0.4469383145556038, "compression_ratio": 1.4965034965034965, "no_speech_prob": 6.143319751572562e-06}, {"id": 351, "seek": 268652, "start": 2698.52, "end": 2700.52, "text": " So this is, yeah. Okay.", "tokens": [407, 341, 307, 11, 1338, 13, 1033, 13], "temperature": 0.0, "avg_logprob": -0.4469383145556038, "compression_ratio": 1.4965034965034965, "no_speech_prob": 6.143319751572562e-06}, {"id": 352, "seek": 268652, "start": 2700.52, "end": 2701.52, "text": " So that's a great question.", "tokens": [407, 300, 311, 257, 869, 1168, 13], "temperature": 0.0, "avg_logprob": -0.4469383145556038, "compression_ratio": 1.4965034965034965, "no_speech_prob": 6.143319751572562e-06}, {"id": 353, "seek": 268652, "start": 2701.52, "end": 2703.52, "text": " So this is the.", "tokens": [407, 341, 307, 264, 13], "temperature": 0.0, "avg_logprob": -0.4469383145556038, "compression_ratio": 1.4965034965034965, "no_speech_prob": 6.143319751572562e-06}, {"id": 354, "seek": 268652, "start": 2703.52, "end": 2704.52, "text": " Indices of this is the key thing.", "tokens": [2333, 1473, 295, 341, 307, 264, 2141, 551, 13], "temperature": 0.0, "avg_logprob": -0.4469383145556038, "compression_ratio": 1.4965034965034965, "no_speech_prob": 6.143319751572562e-06}, {"id": 355, "seek": 268652, "start": 2704.52, "end": 2709.52, "text": " This is the indices of the layers.", "tokens": [639, 307, 264, 43840, 295, 264, 7914, 13], "temperature": 0.0, "avg_logprob": -0.4469383145556038, "compression_ratio": 1.4965034965034965, "no_speech_prob": 6.143319751572562e-06}, {"id": 356, "seek": 268652, "start": 2709.52, "end": 2713.52, "text": " Where the size changes.", "tokens": [2305, 264, 2744, 2962, 13], "temperature": 0.0, "avg_logprob": -0.4469383145556038, "compression_ratio": 1.4965034965034965, "no_speech_prob": 6.143319751572562e-06}, {"id": 357, "seek": 271352, "start": 2713.52, "end": 2717.52, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.24054107150515994, "compression_ratio": 1.4246575342465753, "no_speech_prob": 6.142687198007479e-06}, {"id": 358, "seek": 271352, "start": 2717.52, "end": 2719.52, "text": " Either just before that or just after that, you know.", "tokens": [13746, 445, 949, 300, 420, 445, 934, 300, 11, 291, 458, 13], "temperature": 0.0, "avg_logprob": -0.24054107150515994, "compression_ratio": 1.4246575342465753, "no_speech_prob": 6.142687198007479e-06}, {"id": 359, "seek": 271352, "start": 2719.52, "end": 2722.52, "text": " So get, get, get the indices with the size changes.", "tokens": [407, 483, 11, 483, 11, 483, 264, 43840, 365, 264, 2744, 2962, 13], "temperature": 0.0, "avg_logprob": -0.24054107150515994, "compression_ratio": 1.4246575342465753, "no_speech_prob": 6.142687198007479e-06}, {"id": 360, "seek": 271352, "start": 2722.52, "end": 2727.52, "text": " So the sizes.", "tokens": [407, 264, 11602, 13], "temperature": 0.0, "avg_logprob": -0.24054107150515994, "compression_ratio": 1.4246575342465753, "no_speech_prob": 6.142687198007479e-06}, {"id": 361, "seek": 271352, "start": 2727.52, "end": 2728.52, "text": " Here.", "tokens": [1692, 13], "temperature": 0.0, "avg_logprob": -0.24054107150515994, "compression_ratio": 1.4246575342465753, "no_speech_prob": 6.142687198007479e-06}, {"id": 362, "seek": 271352, "start": 2728.52, "end": 2734.52, "text": " Model sizes.", "tokens": [17105, 11602, 13], "temperature": 0.0, "avg_logprob": -0.24054107150515994, "compression_ratio": 1.4246575342465753, "no_speech_prob": 6.142687198007479e-06}, {"id": 363, "seek": 271352, "start": 2734.52, "end": 2737.52, "text": " So we hook outputs.", "tokens": [407, 321, 6328, 23930, 13], "temperature": 0.0, "avg_logprob": -0.24054107150515994, "compression_ratio": 1.4246575342465753, "no_speech_prob": 6.142687198007479e-06}, {"id": 364, "seek": 271352, "start": 2737.52, "end": 2739.52, "text": " We do a dummy of L.", "tokens": [492, 360, 257, 35064, 295, 441, 13], "temperature": 0.0, "avg_logprob": -0.24054107150515994, "compression_ratio": 1.4246575342465753, "no_speech_prob": 6.142687198007479e-06}, {"id": 365, "seek": 271352, "start": 2739.52, "end": 2741.52, "text": " And we find the shape.", "tokens": [400, 321, 915, 264, 3909, 13], "temperature": 0.0, "avg_logprob": -0.24054107150515994, "compression_ratio": 1.4246575342465753, "no_speech_prob": 6.142687198007479e-06}, {"id": 366, "seek": 274152, "start": 2741.52, "end": 2743.52, "text": " Of each thing.", "tokens": [2720, 1184, 551, 13], "temperature": 0.0, "avg_logprob": -0.1678981197123625, "compression_ratio": 1.5252525252525253, "no_speech_prob": 1.6437083104392514e-05}, {"id": 367, "seek": 274152, "start": 2743.52, "end": 2744.52, "text": " And yeah.", "tokens": [400, 1338, 13], "temperature": 0.0, "avg_logprob": -0.1678981197123625, "compression_ratio": 1.5252525252525253, "no_speech_prob": 1.6437083104392514e-05}, {"id": 368, "seek": 274152, "start": 2744.52, "end": 2746.52, "text": " So here you can see dummy of L is using just a single.", "tokens": [407, 510, 291, 393, 536, 35064, 295, 441, 307, 1228, 445, 257, 2167, 13], "temperature": 0.0, "avg_logprob": -0.1678981197123625, "compression_ratio": 1.5252525252525253, "no_speech_prob": 1.6437083104392514e-05}, {"id": 369, "seek": 274152, "start": 2746.52, "end": 2749.52, "text": " Image.", "tokens": [29903, 13], "temperature": 0.0, "avg_logprob": -0.1678981197123625, "compression_ratio": 1.5252525252525253, "no_speech_prob": 1.6437083104392514e-05}, {"id": 370, "seek": 274152, "start": 2749.52, "end": 2751.52, "text": " And so, yeah, this just returns the shape.", "tokens": [400, 370, 11, 1338, 11, 341, 445, 11247, 264, 3909, 13], "temperature": 0.0, "avg_logprob": -0.1678981197123625, "compression_ratio": 1.5252525252525253, "no_speech_prob": 1.6437083104392514e-05}, {"id": 371, "seek": 274152, "start": 2751.52, "end": 2757.52, "text": " Of the output of every layer.", "tokens": [2720, 264, 5598, 295, 633, 4583, 13], "temperature": 0.0, "avg_logprob": -0.1678981197123625, "compression_ratio": 1.5252525252525253, "no_speech_prob": 1.6437083104392514e-05}, {"id": 372, "seek": 274152, "start": 2757.52, "end": 2758.52, "text": " That's going to be in sizes.", "tokens": [663, 311, 516, 281, 312, 294, 11602, 13], "temperature": 0.0, "avg_logprob": -0.1678981197123625, "compression_ratio": 1.5252525252525253, "no_speech_prob": 1.6437083104392514e-05}, {"id": 373, "seek": 274152, "start": 2758.52, "end": 2761.52, "text": " And so then this is just a very simple function.", "tokens": [400, 370, 550, 341, 307, 445, 257, 588, 2199, 2445, 13], "temperature": 0.0, "avg_logprob": -0.1678981197123625, "compression_ratio": 1.5252525252525253, "no_speech_prob": 1.6437083104392514e-05}, {"id": 374, "seek": 274152, "start": 2761.52, "end": 2763.52, "text": " Which just goes through and finds.", "tokens": [3013, 445, 1709, 807, 293, 10704, 13], "temperature": 0.0, "avg_logprob": -0.1678981197123625, "compression_ratio": 1.5252525252525253, "no_speech_prob": 1.6437083104392514e-05}, {"id": 375, "seek": 274152, "start": 2763.52, "end": 2766.52, "text": " Where the size changes.", "tokens": [2305, 264, 2744, 2962, 13], "temperature": 0.0, "avg_logprob": -0.1678981197123625, "compression_ratio": 1.5252525252525253, "no_speech_prob": 1.6437083104392514e-05}, {"id": 376, "seek": 274152, "start": 2766.52, "end": 2769.52, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.1678981197123625, "compression_ratio": 1.5252525252525253, "no_speech_prob": 1.6437083104392514e-05}, {"id": 377, "seek": 276952, "start": 2769.52, "end": 2774.52, "text": " And so this is the indices of those.", "tokens": [400, 370, 341, 307, 264, 43840, 295, 729, 13], "temperature": 0.0, "avg_logprob": -0.1874738757529955, "compression_ratio": 1.6649214659685865, "no_speech_prob": 1.8629707483341917e-05}, {"id": 378, "seek": 276952, "start": 2774.52, "end": 2775.52, "text": " Things.", "tokens": [9514, 13], "temperature": 0.0, "avg_logprob": -0.1874738757529955, "compression_ratio": 1.6649214659685865, "no_speech_prob": 1.8629707483341917e-05}, {"id": 379, "seek": 276952, "start": 2775.52, "end": 2780.52, "text": " So now that we know where the size changes, we know where we want our cross connections to be.", "tokens": [407, 586, 300, 321, 458, 689, 264, 2744, 2962, 11, 321, 458, 689, 321, 528, 527, 3278, 9271, 281, 312, 13], "temperature": 0.0, "avg_logprob": -0.1874738757529955, "compression_ratio": 1.6649214659685865, "no_speech_prob": 1.8629707483341917e-05}, {"id": 380, "seek": 276952, "start": 2780.52, "end": 2785.52, "text": " Now for each of the cross connections, we need to.", "tokens": [823, 337, 1184, 295, 264, 3278, 9271, 11, 321, 643, 281, 13], "temperature": 0.0, "avg_logprob": -0.1874738757529955, "compression_ratio": 1.6649214659685865, "no_speech_prob": 1.8629707483341917e-05}, {"id": 381, "seek": 276952, "start": 2785.52, "end": 2793.52, "text": " Store the output of the model at that point, because that's that's going to be an input in the up sampling block.", "tokens": [17242, 264, 5598, 295, 264, 2316, 412, 300, 935, 11, 570, 300, 311, 300, 311, 516, 281, 312, 364, 4846, 294, 264, 493, 21179, 3461, 13], "temperature": 0.0, "avg_logprob": -0.1874738757529955, "compression_ratio": 1.6649214659685865, "no_speech_prob": 1.8629707483341917e-05}, {"id": 382, "seek": 276952, "start": 2793.52, "end": 2797.52, "text": " So these SFs.", "tokens": [407, 613, 31095, 82, 13], "temperature": 0.0, "avg_logprob": -0.1874738757529955, "compression_ratio": 1.6649214659685865, "no_speech_prob": 1.8629707483341917e-05}, {"id": 383, "seek": 279752, "start": 2797.52, "end": 2802.52, "text": " And so we can see that the unit block we create.", "tokens": [400, 370, 321, 393, 536, 300, 264, 4985, 3461, 321, 1884, 13], "temperature": 0.0, "avg_logprob": -0.36144432654747594, "compression_ratio": 1.5941176470588236, "no_speech_prob": 9.079994924832135e-06}, {"id": 384, "seek": 279752, "start": 2802.52, "end": 2807.52, "text": " So for each change in the index, for each up sampling block, you have to pass in.", "tokens": [407, 337, 1184, 1319, 294, 264, 8186, 11, 337, 1184, 493, 21179, 3461, 11, 291, 362, 281, 1320, 294, 13], "temperature": 0.0, "avg_logprob": -0.36144432654747594, "compression_ratio": 1.5941176470588236, "no_speech_prob": 9.079994924832135e-06}, {"id": 385, "seek": 279752, "start": 2807.52, "end": 2810.52, "text": " That.", "tokens": [663, 13], "temperature": 0.0, "avg_logprob": -0.36144432654747594, "compression_ratio": 1.5941176470588236, "no_speech_prob": 9.079994924832135e-06}, {"id": 386, "seek": 279752, "start": 2810.52, "end": 2813.52, "text": " Those.", "tokens": [3950, 13], "temperature": 0.0, "avg_logprob": -0.36144432654747594, "compression_ratio": 1.5941176470588236, "no_speech_prob": 9.079994924832135e-06}, {"id": 387, "seek": 279752, "start": 2813.52, "end": 2814.52, "text": " Outputs.", "tokens": [5925, 2582, 82, 13], "temperature": 0.0, "avg_logprob": -0.36144432654747594, "compression_ratio": 1.5941176470588236, "no_speech_prob": 9.079994924832135e-06}, {"id": 388, "seek": 279752, "start": 2814.52, "end": 2815.52, "text": " Sampling.", "tokens": [4832, 11970, 13], "temperature": 0.0, "avg_logprob": -0.36144432654747594, "compression_ratio": 1.5941176470588236, "no_speech_prob": 9.079994924832135e-06}, {"id": 389, "seek": 279752, "start": 2815.52, "end": 2817.52, "text": " Side. So this is the index where it happened.", "tokens": [19026, 13, 407, 341, 307, 264, 8186, 689, 309, 2011, 13], "temperature": 0.0, "avg_logprob": -0.36144432654747594, "compression_ratio": 1.5941176470588236, "no_speech_prob": 9.079994924832135e-06}, {"id": 390, "seek": 279752, "start": 2817.52, "end": 2819.52, "text": " And so this will be the actual.", "tokens": [400, 370, 341, 486, 312, 264, 3539, 13], "temperature": 0.0, "avg_logprob": -0.36144432654747594, "compression_ratio": 1.5941176470588236, "no_speech_prob": 9.079994924832135e-06}, {"id": 391, "seek": 279752, "start": 2819.52, "end": 2823.52, "text": " So if we go to the unit block.", "tokens": [407, 498, 321, 352, 281, 264, 4985, 3461, 13], "temperature": 0.0, "avg_logprob": -0.36144432654747594, "compression_ratio": 1.5941176470588236, "no_speech_prob": 9.079994924832135e-06}, {"id": 392, "seek": 282352, "start": 2823.52, "end": 2828.52, "text": " And then we can see that the blocks get created on the other side.", "tokens": [400, 550, 321, 393, 536, 300, 264, 8474, 483, 2942, 322, 264, 661, 1252, 13], "temperature": 0.0, "avg_logprob": -0.31268645977151804, "compression_ratio": 1.6488095238095237, "no_speech_prob": 9.515461897535715e-06}, {"id": 393, "seek": 282352, "start": 2828.52, "end": 2831.52, "text": " So it's going to be passed the hook, right?", "tokens": [407, 309, 311, 516, 281, 312, 4678, 264, 6328, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.31268645977151804, "compression_ratio": 1.6488095238095237, "no_speech_prob": 9.515461897535715e-06}, {"id": 394, "seek": 282352, "start": 2831.52, "end": 2832.52, "text": " Which is.", "tokens": [3013, 307, 13], "temperature": 0.0, "avg_logprob": -0.31268645977151804, "compression_ratio": 1.6488095238095237, "no_speech_prob": 9.515461897535715e-06}, {"id": 395, "seek": 282352, "start": 2832.52, "end": 2839.52, "text": " And so that that's just the hook that was used.", "tokens": [400, 370, 300, 300, 311, 445, 264, 6328, 300, 390, 1143, 13], "temperature": 0.0, "avg_logprob": -0.31268645977151804, "compression_ratio": 1.6488095238095237, "no_speech_prob": 9.515461897535715e-06}, {"id": 396, "seek": 282352, "start": 2839.52, "end": 2842.52, "text": " That's the hook that was used.", "tokens": [663, 311, 264, 6328, 300, 390, 1143, 13], "temperature": 0.0, "avg_logprob": -0.31268645977151804, "compression_ratio": 1.6488095238095237, "no_speech_prob": 9.515461897535715e-06}, {"id": 397, "seek": 282352, "start": 2842.52, "end": 2845.52, "text": " On the down sampling side.", "tokens": [1282, 264, 760, 21179, 1252, 13], "temperature": 0.0, "avg_logprob": -0.31268645977151804, "compression_ratio": 1.6488095238095237, "no_speech_prob": 9.515461897535715e-06}, {"id": 398, "seek": 282352, "start": 2845.52, "end": 2848.52, "text": " And from that, we can get the stored.", "tokens": [400, 490, 300, 11, 321, 393, 483, 264, 12187, 13], "temperature": 0.0, "avg_logprob": -0.31268645977151804, "compression_ratio": 1.6488095238095237, "no_speech_prob": 9.515461897535715e-06}, {"id": 399, "seek": 282352, "start": 2848.52, "end": 2851.52, "text": " Activations.", "tokens": [28550, 763, 13], "temperature": 0.0, "avg_logprob": -0.31268645977151804, "compression_ratio": 1.6488095238095237, "no_speech_prob": 9.515461897535715e-06}, {"id": 400, "seek": 285152, "start": 2851.52, "end": 2855.52, "text": " So this is the.", "tokens": [407, 341, 307, 264, 13], "temperature": 0.0, "avg_logprob": -0.1787339405841138, "compression_ratio": 1.5647668393782384, "no_speech_prob": 6.240110906219343e-06}, {"id": 401, "seek": 285152, "start": 2855.52, "end": 2858.52, "text": " Shape of those stored activations.", "tokens": [49148, 295, 729, 12187, 2430, 763, 13], "temperature": 0.0, "avg_logprob": -0.1787339405841138, "compression_ratio": 1.5647668393782384, "no_speech_prob": 6.240110906219343e-06}, {"id": 402, "seek": 285152, "start": 2858.52, "end": 2863.52, "text": " And this is a minor tweak. So let's just ignore this if block for a moment.", "tokens": [400, 341, 307, 257, 6696, 29879, 13, 407, 718, 311, 445, 11200, 341, 498, 3461, 337, 257, 1623, 13], "temperature": 0.0, "avg_logprob": -0.1787339405841138, "compression_ratio": 1.5647668393782384, "no_speech_prob": 6.240110906219343e-06}, {"id": 403, "seek": 285152, "start": 2863.52, "end": 2867.52, "text": " Basically, all we then do is we take those activations, stick them through a batch norm.", "tokens": [8537, 11, 439, 321, 550, 360, 307, 321, 747, 729, 2430, 763, 11, 2897, 552, 807, 257, 15245, 2026, 13], "temperature": 0.0, "avg_logprob": -0.1787339405841138, "compression_ratio": 1.5647668393782384, "no_speech_prob": 6.240110906219343e-06}, {"id": 404, "seek": 285152, "start": 2867.52, "end": 2872.52, "text": " Concatenate them with the previous layers up sampling.", "tokens": [18200, 7186, 473, 552, 365, 264, 3894, 7914, 493, 21179, 13], "temperature": 0.0, "avg_logprob": -0.1787339405841138, "compression_ratio": 1.5647668393782384, "no_speech_prob": 6.240110906219343e-06}, {"id": 405, "seek": 285152, "start": 2872.52, "end": 2875.52, "text": " And chuck that through a value.", "tokens": [400, 20870, 300, 807, 257, 2158, 13], "temperature": 0.0, "avg_logprob": -0.1787339405841138, "compression_ratio": 1.5647668393782384, "no_speech_prob": 6.240110906219343e-06}, {"id": 406, "seek": 287552, "start": 2875.52, "end": 2881.52, "text": " And then we do.", "tokens": [400, 550, 321, 360, 13], "temperature": 0.0, "avg_logprob": -0.39375833023426143, "compression_ratio": 1.2376237623762376, "no_speech_prob": 7.1806884989200626e-06}, {"id": 407, "seek": 287552, "start": 2881.52, "end": 2883.52, "text": " Some cons.", "tokens": [2188, 1014, 13], "temperature": 0.0, "avg_logprob": -0.39375833023426143, "compression_ratio": 1.2376237623762376, "no_speech_prob": 7.1806884989200626e-06}, {"id": 408, "seek": 287552, "start": 2883.52, "end": 2890.52, "text": " And the com cons aren't just coms their fast AI coms, which.", "tokens": [400, 264, 395, 1014, 3212, 380, 445, 395, 82, 641, 2370, 7318, 395, 82, 11, 597, 13], "temperature": 0.0, "avg_logprob": -0.39375833023426143, "compression_ratio": 1.2376237623762376, "no_speech_prob": 7.1806884989200626e-06}, {"id": 409, "seek": 287552, "start": 2890.52, "end": 2893.52, "text": " Can include all kinds of things like.", "tokens": [1664, 4090, 439, 3685, 295, 721, 411, 13], "temperature": 0.0, "avg_logprob": -0.39375833023426143, "compression_ratio": 1.2376237623762376, "no_speech_prob": 7.1806884989200626e-06}, {"id": 410, "seek": 289352, "start": 2893.52, "end": 2906.52, "text": " Activation, whatever. So it's it's a.", "tokens": [28550, 399, 11, 2035, 13, 407, 309, 311, 309, 311, 257, 13], "temperature": 0.0, "avg_logprob": -0.36299898889329696, "compression_ratio": 1.6, "no_speech_prob": 1.3003521416976582e-05}, {"id": 411, "seek": 289352, "start": 2906.52, "end": 2910.52, "text": " Some combination of batch norm, you know, activation convolution.", "tokens": [2188, 6562, 295, 15245, 2026, 11, 291, 458, 11, 24433, 45216, 13], "temperature": 0.0, "avg_logprob": -0.36299898889329696, "compression_ratio": 1.6, "no_speech_prob": 1.3003521416976582e-05}, {"id": 412, "seek": 289352, "start": 2910.52, "end": 2914.52, "text": " You can also do up sampling. So it's transpose.", "tokens": [509, 393, 611, 360, 493, 21179, 13, 407, 309, 311, 25167, 13], "temperature": 0.0, "avg_logprob": -0.36299898889329696, "compression_ratio": 1.6, "no_speech_prob": 1.3003521416976582e-05}, {"id": 413, "seek": 289352, "start": 2914.52, "end": 2922.52, "text": " That's not can go first or last, whatever. So that's quite as you know, a very rich convolutional layer.", "tokens": [663, 311, 406, 393, 352, 700, 420, 1036, 11, 2035, 13, 407, 300, 311, 1596, 382, 291, 458, 11, 257, 588, 4593, 45216, 304, 4583, 13], "temperature": 0.0, "avg_logprob": -0.36299898889329696, "compression_ratio": 1.6, "no_speech_prob": 1.3003521416976582e-05}, {"id": 414, "seek": 292252, "start": 2922.52, "end": 2927.52, "text": " Okay, so then this if part here is that it's possible that.", "tokens": [1033, 11, 370, 550, 341, 498, 644, 510, 307, 300, 309, 311, 1944, 300, 13], "temperature": 0.0, "avg_logprob": -0.09080483436584473, "compression_ratio": 1.64, "no_speech_prob": 5.9537196648307145e-06}, {"id": 415, "seek": 292252, "start": 2927.52, "end": 2933.52, "text": " Things didn't quite round off nicely so that the cross connection doesn't quite have the right size.", "tokens": [9514, 994, 380, 1596, 3098, 766, 9594, 370, 300, 264, 3278, 4984, 1177, 380, 1596, 362, 264, 558, 2744, 13], "temperature": 0.0, "avg_logprob": -0.09080483436584473, "compression_ratio": 1.64, "no_speech_prob": 5.9537196648307145e-06}, {"id": 416, "seek": 292252, "start": 2933.52, "end": 2937.52, "text": " And if that happens, then we'll interpolate.", "tokens": [400, 498, 300, 2314, 11, 550, 321, 603, 44902, 473, 13], "temperature": 0.0, "avg_logprob": -0.09080483436584473, "compression_ratio": 1.64, "no_speech_prob": 5.9537196648307145e-06}, {"id": 417, "seek": 292252, "start": 2937.52, "end": 2942.52, "text": " The cross connection to be the same shape as the up sampling.", "tokens": [440, 3278, 4984, 281, 312, 264, 912, 3909, 382, 264, 493, 21179, 13], "temperature": 0.0, "avg_logprob": -0.09080483436584473, "compression_ratio": 1.64, "no_speech_prob": 5.9537196648307145e-06}, {"id": 418, "seek": 292252, "start": 2942.52, "end": 2943.52, "text": " Connection.", "tokens": [11653, 313, 13], "temperature": 0.0, "avg_logprob": -0.09080483436584473, "compression_ratio": 1.64, "no_speech_prob": 5.9537196648307145e-06}, {"id": 419, "seek": 292252, "start": 2943.52, "end": 2948.52, "text": " And again, I don't know if anybody else does this, but this is to try to make it so that.", "tokens": [400, 797, 11, 286, 500, 380, 458, 498, 4472, 1646, 775, 341, 11, 457, 341, 307, 281, 853, 281, 652, 309, 370, 300, 13], "temperature": 0.0, "avg_logprob": -0.09080483436584473, "compression_ratio": 1.64, "no_speech_prob": 5.9537196648307145e-06}, {"id": 420, "seek": 294852, "start": 2948.52, "end": 2952.52, "text": " The dynamic unit always just works.", "tokens": [440, 8546, 4985, 1009, 445, 1985, 13], "temperature": 0.0, "avg_logprob": -0.1434999704360962, "compression_ratio": 1.18, "no_speech_prob": 2.1906373603997054e-06}, {"id": 421, "seek": 294852, "start": 2952.52, "end": 2954.52, "text": " That's the basic idea.", "tokens": [663, 311, 264, 3875, 1558, 13], "temperature": 0.0, "avg_logprob": -0.1434999704360962, "compression_ratio": 1.18, "no_speech_prob": 2.1906373603997054e-06}, {"id": 422, "seek": 294852, "start": 2954.52, "end": 2960.52, "text": " Yeah, so to make this work for Tim.", "tokens": [865, 11, 370, 281, 652, 341, 589, 337, 7172, 13], "temperature": 0.0, "avg_logprob": -0.1434999704360962, "compression_ratio": 1.18, "no_speech_prob": 2.1906373603997054e-06}, {"id": 423, "seek": 294852, "start": 2960.52, "end": 2966.52, "text": " You know, this encoder.", "tokens": [509, 458, 11, 341, 2058, 19866, 13], "temperature": 0.0, "avg_logprob": -0.1434999704360962, "compression_ratio": 1.18, "no_speech_prob": 2.1906373603997054e-06}, {"id": 424, "seek": 296652, "start": 2966.52, "end": 2979.52, "text": " You know, you need to know about the spots right.", "tokens": [509, 458, 11, 291, 643, 281, 458, 466, 264, 10681, 558, 13], "temperature": 0.0, "avg_logprob": -0.5773187081019083, "compression_ratio": 0.971830985915493, "no_speech_prob": 3.844500952254748e-06}, {"id": 425, "seek": 296652, "start": 2979.52, "end": 2983.52, "text": " But yeah, it would.", "tokens": [583, 1338, 11, 309, 576, 13], "temperature": 0.0, "avg_logprob": -0.5773187081019083, "compression_ratio": 0.971830985915493, "no_speech_prob": 3.844500952254748e-06}, {"id": 426, "seek": 298352, "start": 2983.52, "end": 3004.52, "text": " You know, to figure out what doesn't work, you know, you would need to change this line to say, oh, if it's a string, create trim model otherwise do this, you know, and then you'd like create body would need to be create Tim body if it's a string so like at minimum do the same stuff that create vision model does.", "tokens": [509, 458, 11, 281, 2573, 484, 437, 1177, 380, 589, 11, 291, 458, 11, 291, 576, 643, 281, 1319, 341, 1622, 281, 584, 11, 1954, 11, 498, 309, 311, 257, 6798, 11, 1884, 10445, 2316, 5911, 360, 341, 11, 291, 458, 11, 293, 550, 291, 1116, 411, 1884, 1772, 576, 643, 281, 312, 1884, 7172, 1772, 498, 309, 311, 257, 6798, 370, 411, 412, 7285, 360, 264, 912, 1507, 300, 1884, 5201, 2316, 775, 13], "temperature": 0.0, "avg_logprob": -0.23794823658617237, "compression_ratio": 1.7348066298342542, "no_speech_prob": 5.420981779025169e-06}, {"id": 427, "seek": 300452, "start": 3004.52, "end": 3013.52, "text": " And then, yeah, and then see if this works. Right. And it might well. Now, I will say, if you do get it working.", "tokens": [400, 550, 11, 1338, 11, 293, 550, 536, 498, 341, 1985, 13, 1779, 13, 400, 309, 1062, 731, 13, 823, 11, 286, 486, 584, 11, 498, 291, 360, 483, 309, 1364, 13], "temperature": 0.0, "avg_logprob": -0.09955610077956627, "compression_ratio": 1.502145922746781, "no_speech_prob": 4.859667569689918e-06}, {"id": 428, "seek": 300452, "start": 3013.52, "end": 3029.52, "text": " Tim does have an API to actually tell you where the feature sizes change. So like you could actually optimize out that dummy of L stuff but I don't even know if I'd bother because it makes the code more complex for no particular benefit.", "tokens": [7172, 775, 362, 364, 9362, 281, 767, 980, 291, 689, 264, 4111, 11602, 1319, 13, 407, 411, 291, 727, 767, 19719, 484, 300, 35064, 295, 441, 1507, 457, 286, 500, 380, 754, 458, 498, 286, 1116, 8677, 570, 309, 1669, 264, 3089, 544, 3997, 337, 572, 1729, 5121, 13], "temperature": 0.0, "avg_logprob": -0.09955610077956627, "compression_ratio": 1.502145922746781, "no_speech_prob": 4.859667569689918e-06}, {"id": 429, "seek": 302952, "start": 3029.52, "end": 3042.52, "text": " So, look, I think if you know this you commit this as a PR I'll definitely be looking at it. I was actually going to try confidence in my unit so I had no idea it wouldn't work actually so that would have been, I would have noticed that already but I just haven't had time.", "tokens": [407, 11, 574, 11, 286, 519, 498, 291, 458, 341, 291, 5599, 341, 382, 257, 11568, 286, 603, 2138, 312, 1237, 412, 309, 13, 286, 390, 767, 516, 281, 853, 6687, 294, 452, 4985, 370, 286, 632, 572, 1558, 309, 2759, 380, 589, 767, 370, 300, 576, 362, 668, 11, 286, 576, 362, 5694, 300, 1217, 457, 286, 445, 2378, 380, 632, 565, 13], "temperature": 0.0, "avg_logprob": -0.15868189735134153, "compression_ratio": 1.6913183279742765, "no_speech_prob": 5.6819785640982445e-06}, {"id": 430, "seek": 302952, "start": 3042.52, "end": 3051.52, "text": " So I'd love to because I, you know, tried and resident 32 I've got particular results and I'd like to see we can push it with a different model. Yeah, no I mean I think there'd be a lot of benefit to that.", "tokens": [407, 286, 1116, 959, 281, 570, 286, 11, 291, 458, 11, 3031, 293, 10832, 8858, 286, 600, 658, 1729, 3542, 293, 286, 1116, 411, 281, 536, 321, 393, 2944, 309, 365, 257, 819, 2316, 13, 865, 11, 572, 286, 914, 286, 519, 456, 1116, 312, 257, 688, 295, 5121, 281, 300, 13], "temperature": 0.0, "avg_logprob": -0.15868189735134153, "compression_ratio": 1.6913183279742765, "no_speech_prob": 5.6819785640982445e-06}, {"id": 431, "seek": 302952, "start": 3051.52, "end": 3057.52, "text": " So, all right. So now we should run the tests.", "tokens": [407, 11, 439, 558, 13, 407, 586, 321, 820, 1190, 264, 6921, 13], "temperature": 0.0, "avg_logprob": -0.15868189735134153, "compression_ratio": 1.6913183279742765, "no_speech_prob": 5.6819785640982445e-06}, {"id": 432, "seek": 305752, "start": 3057.52, "end": 3065.52, "text": " Just, just to know with that all likely be in the same notebook that you're editing the vision letter is that when most of the source code is unit.", "tokens": [1449, 11, 445, 281, 458, 365, 300, 439, 3700, 312, 294, 264, 912, 21060, 300, 291, 434, 10000, 264, 5201, 5063, 307, 300, 562, 881, 295, 264, 4009, 3089, 307, 4985, 13], "temperature": 0.0, "avg_logprob": -0.2587649676264549, "compression_ratio": 1.6396396396396395, "no_speech_prob": 1.5204746887320653e-05}, {"id": 433, "seek": 305752, "start": 3065.52, "end": 3070.52, "text": " Let us, or is it a different.", "tokens": [961, 505, 11, 420, 307, 309, 257, 819, 13], "temperature": 0.0, "avg_logprob": -0.2587649676264549, "compression_ratio": 1.6396396396396395, "no_speech_prob": 1.5204746887320653e-05}, {"id": 434, "seek": 305752, "start": 3070.52, "end": 3084.52, "text": " I don't know I was just using this right, I'll find it jump jump jump to whatever automatically in Vim so I was using Vim see tags to jump around, so I don't, I have no idea where I was.", "tokens": [286, 500, 380, 458, 286, 390, 445, 1228, 341, 558, 11, 286, 603, 915, 309, 3012, 3012, 3012, 281, 2035, 6772, 294, 691, 332, 370, 286, 390, 1228, 691, 332, 536, 18632, 281, 3012, 926, 11, 370, 286, 500, 380, 11, 286, 362, 572, 1558, 689, 286, 390, 13], "temperature": 0.0, "avg_logprob": -0.2587649676264549, "compression_ratio": 1.6396396396396395, "no_speech_prob": 1.5204746887320653e-05}, {"id": 435, "seek": 308452, "start": 3084.52, "end": 3097.52, "text": " Actually,", "tokens": [5135, 11], "temperature": 0.0, "avg_logprob": -0.28052112330561096, "compression_ratio": 1.028169014084507, "no_speech_prob": 2.4291393856401555e-05}, {"id": 436, "seek": 308452, "start": 3097.52, "end": 3106.52, "text": " so yeah so there's a models unit, where the dynamic unit lives.", "tokens": [370, 1338, 370, 456, 311, 257, 5245, 4985, 11, 689, 264, 8546, 4985, 2909, 13], "temperature": 0.0, "avg_logprob": -0.28052112330561096, "compression_ratio": 1.028169014084507, "no_speech_prob": 2.4291393856401555e-05}, {"id": 437, "seek": 310652, "start": 3106.52, "end": 3119.52, "text": " Okay. Is there anything unique about the fact that the team model doesn't that sort of an option there to cut the tail and head off. Does that need to be done with the unit architecture.", "tokens": [1033, 13, 1119, 456, 1340, 3845, 466, 264, 1186, 300, 264, 1469, 2316, 1177, 380, 300, 1333, 295, 364, 3614, 456, 281, 1723, 264, 6838, 293, 1378, 766, 13, 4402, 300, 643, 281, 312, 1096, 365, 264, 4985, 9482, 13], "temperature": 0.0, "avg_logprob": -0.1464177560115206, "compression_ratio": 1.5588235294117647, "no_speech_prob": 8.937687198340427e-06}, {"id": 438, "seek": 310652, "start": 3119.52, "end": 3124.52, "text": " Oh, got an error here.", "tokens": [876, 11, 658, 364, 6713, 510, 13], "temperature": 0.0, "avg_logprob": -0.1464177560115206, "compression_ratio": 1.5588235294117647, "no_speech_prob": 8.937687198340427e-06}, {"id": 439, "seek": 310652, "start": 3124.52, "end": 3130.52, "text": " Yeah, so yeah, you absolutely have to cut the head off.", "tokens": [865, 11, 370, 1338, 11, 291, 3122, 362, 281, 1723, 264, 1378, 766, 13], "temperature": 0.0, "avg_logprob": -0.1464177560115206, "compression_ratio": 1.5588235294117647, "no_speech_prob": 8.937687198340427e-06}, {"id": 440, "seek": 313052, "start": 3130.52, "end": 3144.52, "text": " That's because it comes with a default classifier head. So you will need, you know, so you know you once you get it working, you'll probably find you can factor out some duplicate code between the unit and the vision letter.", "tokens": [663, 311, 570, 309, 1487, 365, 257, 7576, 1508, 9902, 1378, 13, 407, 291, 486, 643, 11, 291, 458, 11, 370, 291, 458, 291, 1564, 291, 483, 309, 1364, 11, 291, 603, 1391, 915, 291, 393, 5952, 484, 512, 23976, 3089, 1296, 264, 4985, 293, 264, 5201, 5063, 13], "temperature": 0.0, "avg_logprob": -0.1522718487363873, "compression_ratio": 1.6305220883534137, "no_speech_prob": 7.070064839354018e-06}, {"id": 441, "seek": 313052, "start": 3144.52, "end": 3152.52, "text": " But yeah, you basically have to cut off the classifier head in the same way that create Tim body does.", "tokens": [583, 1338, 11, 291, 1936, 362, 281, 1723, 766, 264, 1508, 9902, 1378, 294, 264, 912, 636, 300, 1884, 7172, 1772, 775, 13], "temperature": 0.0, "avg_logprob": -0.1522718487363873, "compression_ratio": 1.6305220883534137, "no_speech_prob": 7.070064839354018e-06}, {"id": 442, "seek": 313052, "start": 3152.52, "end": 3159.52, "text": " And I don't think you'll need to change any input processing as far as I know.", "tokens": [400, 286, 500, 380, 519, 291, 603, 643, 281, 1319, 604, 4846, 9007, 382, 1400, 382, 286, 458, 13], "temperature": 0.0, "avg_logprob": -0.1522718487363873, "compression_ratio": 1.6305220883534137, "no_speech_prob": 7.070064839354018e-06}, {"id": 443, "seek": 315952, "start": 3159.52, "end": 3163.52, "text": " So the vision, create vision model.", "tokens": [407, 264, 5201, 11, 1884, 5201, 2316, 13], "temperature": 0.0, "avg_logprob": -0.141081174214681, "compression_ratio": 1.6736842105263159, "no_speech_prob": 9.2216132543399e-06}, {"id": 444, "seek": 315952, "start": 3163.52, "end": 3167.52, "text": " You know, handles, like,", "tokens": [509, 458, 11, 18722, 11, 411, 11], "temperature": 0.0, "avg_logprob": -0.141081174214681, "compression_ratio": 1.6736842105263159, "no_speech_prob": 9.2216132543399e-06}, {"id": 445, "seek": 315952, "start": 3167.52, "end": 3181.52, "text": " you know, if you've only got one or two or four channel inputs and the models are three channel input it handles that automatically, but Tim actually, I think, Ross and I independently invented this as far as I know we both kind of automatically handle like", "tokens": [291, 458, 11, 498, 291, 600, 787, 658, 472, 420, 732, 420, 1451, 2269, 15743, 293, 264, 5245, 366, 1045, 2269, 4846, 309, 18722, 300, 6772, 11, 457, 7172, 767, 11, 286, 519, 11, 16140, 293, 286, 21761, 14479, 341, 382, 1400, 382, 286, 458, 321, 1293, 733, 295, 6772, 4813, 411], "temperature": 0.0, "avg_logprob": -0.141081174214681, "compression_ratio": 1.6736842105263159, "no_speech_prob": 9.2216132543399e-06}, {"id": 446, "seek": 318152, "start": 3181.52, "end": 3192.52, "text": " copying weights if necessary or deleting weights if necessary or whatever but yeah so the same stuff from vision when I should should work there as well.", "tokens": [27976, 17443, 498, 4818, 420, 48946, 17443, 498, 4818, 420, 2035, 457, 1338, 370, 264, 912, 1507, 490, 5201, 562, 286, 820, 820, 589, 456, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.16838693618774414, "compression_ratio": 1.6011904761904763, "no_speech_prob": 1.4822710454609478e-06}, {"id": 447, "seek": 318152, "start": 3192.52, "end": 3201.52, "text": " So interestingly layers, the layers notebook", "tokens": [407, 25873, 7914, 11, 264, 7914, 21060], "temperature": 0.0, "avg_logprob": -0.16838693618774414, "compression_ratio": 1.6011904761904763, "no_speech_prob": 1.4822710454609478e-06}, {"id": 448, "seek": 318152, "start": 3201.52, "end": 3210.52, "text": " doesn't work because it's actually creating a model, which is curious.", "tokens": [1177, 380, 589, 570, 309, 311, 767, 4084, 257, 2316, 11, 597, 307, 6369, 13], "temperature": 0.0, "avg_logprob": -0.16838693618774414, "compression_ratio": 1.6011904761904763, "no_speech_prob": 1.4822710454609478e-06}, {"id": 449, "seek": 321052, "start": 3210.52, "end": 3217.52, "text": " It's easily fixed.", "tokens": [467, 311, 3612, 6806, 13], "temperature": 0.0, "avg_logprob": -0.2164889517284575, "compression_ratio": 0.8928571428571429, "no_speech_prob": 7.954883039928973e-05}, {"id": 450, "seek": 321052, "start": 3217.52, "end": 3234.52, "text": " Yeah, that's interesting.", "tokens": [865, 11, 300, 311, 1880, 13], "temperature": 0.0, "avg_logprob": -0.2164889517284575, "compression_ratio": 0.8928571428571429, "no_speech_prob": 7.954883039928973e-05}, {"id": 451, "seek": 321052, "start": 3234.52, "end": 3236.52, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.2164889517284575, "compression_ratio": 0.8928571428571429, "no_speech_prob": 7.954883039928973e-05}, {"id": 452, "seek": 323652, "start": 3236.52, "end": 3241.52, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.14715512096881866, "compression_ratio": 0.8536585365853658, "no_speech_prob": 1.2602185961441137e-05}, {"id": 453, "seek": 323652, "start": 3241.52, "end": 3251.52, "text": " So, the big question then is,", "tokens": [407, 11, 264, 955, 1168, 550, 307, 11], "temperature": 0.0, "avg_logprob": -0.14715512096881866, "compression_ratio": 0.8536585365853658, "no_speech_prob": 1.2602185961441137e-05}, {"id": 454, "seek": 325152, "start": 3251.52, "end": 3278.52, "text": " can we still predict race disease.", "tokens": [393, 321, 920, 6069, 4569, 4752, 13], "temperature": 0.0, "avg_logprob": -0.2722747976129705, "compression_ratio": 0.8095238095238095, "no_speech_prob": 0.00013318817946128547}, {"id": 455, "seek": 327852, "start": 3278.52, "end": 3303.52, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.49501999219258624, "compression_ratio": 0.38461538461538464, "no_speech_prob": 6.388063775375485e-05}, {"id": 456, "seek": 330352, "start": 3303.52, "end": 3308.52, "text": " So, I don't know if it's going to make much difference or not.", "tokens": [407, 11, 286, 500, 380, 458, 498, 309, 311, 516, 281, 652, 709, 2649, 420, 406, 13], "temperature": 0.0, "avg_logprob": -0.22742530878852396, "compression_ratio": 1.4835164835164836, "no_speech_prob": 4.222455117997015e-06}, {"id": 457, "seek": 330352, "start": 3308.52, "end": 3314.52, "text": " You know, because we're pretty careful about fine tuning the batch norm layers.", "tokens": [509, 458, 11, 570, 321, 434, 1238, 5026, 466, 2489, 15164, 264, 15245, 2026, 7914, 13], "temperature": 0.0, "avg_logprob": -0.22742530878852396, "compression_ratio": 1.4835164835164836, "no_speech_prob": 4.222455117997015e-06}, {"id": 458, "seek": 330352, "start": 3314.52, "end": 3319.52, "text": " It would actually be interesting to see whether normalization matters as much as it used to.", "tokens": [467, 576, 767, 312, 1880, 281, 536, 1968, 2710, 2144, 7001, 382, 709, 382, 309, 1143, 281, 13], "temperature": 0.0, "avg_logprob": -0.22742530878852396, "compression_ratio": 1.4835164835164836, "no_speech_prob": 4.222455117997015e-06}, {"id": 459, "seek": 330352, "start": 3319.52, "end": 3330.52, "text": " It used to be absolutely critical.", "tokens": [467, 1143, 281, 312, 3122, 4924, 13], "temperature": 0.0, "avg_logprob": -0.22742530878852396, "compression_ratio": 1.4835164835164836, "no_speech_prob": 4.222455117997015e-06}, {"id": 460, "seek": 333052, "start": 3330.52, "end": 3345.52, "text": " So, it's like a layer that learns the normalization sort of thing. Yeah, I mean that's basically what batch norm does, you know,", "tokens": [407, 11, 309, 311, 411, 257, 4583, 300, 27152, 264, 2710, 2144, 1333, 295, 551, 13, 865, 11, 286, 914, 300, 311, 1936, 437, 15245, 2026, 775, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.2355474665545035, "compression_ratio": 1.6573033707865168, "no_speech_prob": 2.144231621059589e-05}, {"id": 461, "seek": 333052, "start": 3345.52, "end": 3355.52, "text": " to understand it's a those weights in the batch norm layer basically learning the aggregate of that batch that optimally give the best activations for the next layer.", "tokens": [281, 1223, 309, 311, 257, 729, 17443, 294, 264, 15245, 2026, 4583, 1936, 2539, 264, 26118, 295, 300, 15245, 300, 5028, 379, 976, 264, 1151, 2430, 763, 337, 264, 958, 4583, 13], "temperature": 0.0, "avg_logprob": -0.2355474665545035, "compression_ratio": 1.6573033707865168, "no_speech_prob": 2.144231621059589e-05}, {"id": 462, "seek": 335552, "start": 3355.52, "end": 3363.52, "text": " Exactly. Yeah, yeah, it's just, it's just, you know, multiply by something and add something.", "tokens": [7587, 13, 865, 11, 1338, 11, 309, 311, 445, 11, 309, 311, 445, 11, 291, 458, 11, 12972, 538, 746, 293, 909, 746, 13], "temperature": 0.0, "avg_logprob": -0.15424645465353262, "compression_ratio": 1.4933333333333334, "no_speech_prob": 1.80527895281557e-05}, {"id": 463, "seek": 335552, "start": 3363.52, "end": 3367.52, "text": " So it's finding what's the best thing to multiply by and add by.", "tokens": [407, 309, 311, 5006, 437, 311, 264, 1151, 551, 281, 12972, 538, 293, 909, 538, 13], "temperature": 0.0, "avg_logprob": -0.15424645465353262, "compression_ratio": 1.4933333333333334, "no_speech_prob": 1.80527895281557e-05}, {"id": 464, "seek": 335552, "start": 3367.52, "end": 3376.52, "text": " So, let's take a look. So I mean, alright, so this got 47% error.", "tokens": [407, 11, 718, 311, 747, 257, 574, 13, 407, 286, 914, 11, 5845, 11, 370, 341, 658, 16953, 4, 6713, 13], "temperature": 0.0, "avg_logprob": -0.15424645465353262, "compression_ratio": 1.4933333333333334, "no_speech_prob": 1.80527895281557e-05}, {"id": 465, "seek": 337652, "start": 3376.52, "end": 3388.52, "text": " Yeah. So I mean, it's a bit disappointing after all that work it doesn't actually, I mean this is fascinating, like, yeah, when you find tune the way we do.", "tokens": [865, 13, 407, 286, 914, 11, 309, 311, 257, 857, 25054, 934, 439, 300, 589, 309, 1177, 380, 767, 11, 286, 914, 341, 307, 10343, 11, 411, 11, 1338, 11, 562, 291, 915, 10864, 264, 636, 321, 360, 13], "temperature": 0.0, "avg_logprob": -0.1768291646784002, "compression_ratio": 1.4550898203592815, "no_speech_prob": 4.09257381761563e-06}, {"id": 466, "seek": 337652, "start": 3388.52, "end": 3395.52, "text": " Basically doesn't really matter, you know.", "tokens": [8537, 1177, 380, 534, 1871, 11, 291, 458, 13], "temperature": 0.0, "avg_logprob": -0.1768291646784002, "compression_ratio": 1.4550898203592815, "no_speech_prob": 4.09257381761563e-06}, {"id": 467, "seek": 337652, "start": 3395.52, "end": 3402.52, "text": " And let's just double check it actually is.", "tokens": [400, 718, 311, 445, 3834, 1520, 309, 767, 307, 13], "temperature": 0.0, "avg_logprob": -0.1768291646784002, "compression_ratio": 1.4550898203592815, "no_speech_prob": 4.09257381761563e-06}, {"id": 468, "seek": 340252, "start": 3402.52, "end": 3408.52, "text": " It actually is working.", "tokens": [467, 767, 307, 1364, 13], "temperature": 0.0, "avg_logprob": -0.11457013430660717, "compression_ratio": 1.5235602094240839, "no_speech_prob": 2.2826427084510215e-05}, {"id": 469, "seek": 340252, "start": 3408.52, "end": 3419.52, "text": " Would it be fair to say that the one advantage would be if you wanted to use pre trained models without fine tuning you definitely want the statistics in there right. Yes, absolutely.", "tokens": [6068, 309, 312, 3143, 281, 584, 300, 264, 472, 5002, 576, 312, 498, 291, 1415, 281, 764, 659, 8895, 5245, 1553, 2489, 15164, 291, 2138, 528, 264, 12523, 294, 456, 558, 13, 1079, 11, 3122, 13], "temperature": 0.0, "avg_logprob": -0.11457013430660717, "compression_ratio": 1.5235602094240839, "no_speech_prob": 2.2826427084510215e-05}, {"id": 470, "seek": 340252, "start": 3419.52, "end": 3427.52, "text": " I mean, I don't know if that's an actual thing that people do. But yes, if you did.", "tokens": [286, 914, 11, 286, 500, 380, 458, 498, 300, 311, 364, 3539, 551, 300, 561, 360, 13, 583, 2086, 11, 498, 291, 630, 13], "temperature": 0.0, "avg_logprob": -0.11457013430660717, "compression_ratio": 1.5235602094240839, "no_speech_prob": 2.2826427084510215e-05}, {"id": 471, "seek": 342752, "start": 3427.52, "end": 3432.52, "text": " Alright, so we did deals train after batch.", "tokens": [2798, 11, 370, 321, 630, 11215, 3847, 934, 15245, 13], "temperature": 0.0, "avg_logprob": -0.15377385943543678, "compression_ratio": 1.2537313432835822, "no_speech_prob": 1.5445017197635025e-05}, {"id": 472, "seek": 342752, "start": 3432.52, "end": 3435.52, "text": " Yep. There it is.", "tokens": [7010, 13, 821, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.15377385943543678, "compression_ratio": 1.2537313432835822, "no_speech_prob": 1.5445017197635025e-05}, {"id": 473, "seek": 342752, "start": 3435.52, "end": 3437.52, "text": " Groovy.", "tokens": [12981, 38223, 13], "temperature": 0.0, "avg_logprob": -0.15377385943543678, "compression_ratio": 1.2537313432835822, "no_speech_prob": 1.5445017197635025e-05}, {"id": 474, "seek": 342752, "start": 3437.52, "end": 3445.52, "text": " Yeah, it's funny these things that you know we've been doing for years and I guess never question.", "tokens": [865, 11, 309, 311, 4074, 613, 721, 300, 291, 458, 321, 600, 668, 884, 337, 924, 293, 286, 2041, 1128, 1168, 13], "temperature": 0.0, "avg_logprob": -0.15377385943543678, "compression_ratio": 1.2537313432835822, "no_speech_prob": 1.5445017197635025e-05}, {"id": 475, "seek": 344552, "start": 3445.52, "end": 3457.52, "text": " I have a question relating to that because one of the things I wanted to do is get this unit into a mobile app so I use the latest torch script, and it works with the demo app to fill around the locks is broken from pytorch.", "tokens": [286, 362, 257, 1168, 23968, 281, 300, 570, 472, 295, 264, 721, 286, 1415, 281, 360, 307, 483, 341, 4985, 666, 257, 6013, 724, 370, 286, 764, 264, 6792, 27822, 5755, 11, 293, 309, 1985, 365, 264, 10723, 724, 281, 2836, 926, 264, 20703, 307, 5463, 490, 25878, 284, 339, 13], "temperature": 0.0, "avg_logprob": -0.09928856492042541, "compression_ratio": 1.5534883720930233, "no_speech_prob": 1.669919402047526e-05}, {"id": 476, "seek": 344552, "start": 3457.52, "end": 3463.52, "text": " But of course in there you need to provide the averaging statistics for the app. So it's like inference mode.", "tokens": [583, 295, 1164, 294, 456, 291, 643, 281, 2893, 264, 47308, 12523, 337, 264, 724, 13, 407, 309, 311, 411, 38253, 4391, 13], "temperature": 0.0, "avg_logprob": -0.09928856492042541, "compression_ratio": 1.5534883720930233, "no_speech_prob": 1.669919402047526e-05}, {"id": 477, "seek": 346352, "start": 3463.52, "end": 3481.52, "text": " So I wonder, I know that at the moment, the fast AI is kind of idea is that you dump everything is like a pickle, but conceivably it would be helpful if you could maybe extract those new fine tuned statistics or something for your deployment in particular environments,", "tokens": [407, 286, 2441, 11, 286, 458, 300, 412, 264, 1623, 11, 264, 2370, 7318, 307, 733, 295, 1558, 307, 300, 291, 11430, 1203, 307, 411, 257, 31433, 11, 457, 10413, 592, 1188, 309, 576, 312, 4961, 498, 291, 727, 1310, 8947, 729, 777, 2489, 10870, 12523, 420, 746, 337, 428, 19317, 294, 1729, 12388, 11], "temperature": 0.0, "avg_logprob": -0.14000486003028023, "compression_ratio": 1.5119617224880382, "no_speech_prob": 4.1570096982468385e-06}, {"id": 478, "seek": 346352, "start": 3481.52, "end": 3485.52, "text": " because that, how would I go about doing that.", "tokens": [570, 300, 11, 577, 576, 286, 352, 466, 884, 300, 13], "temperature": 0.0, "avg_logprob": -0.14000486003028023, "compression_ratio": 1.5119617224880382, "no_speech_prob": 4.1570096982468385e-06}, {"id": 479, "seek": 348552, "start": 3485.52, "end": 3498.52, "text": " I mean, they're just parameters and batch normally is, you know, they're just parameters. So there'll be in the parameters attribute of the model.", "tokens": [286, 914, 11, 436, 434, 445, 9834, 293, 15245, 5646, 307, 11, 291, 458, 11, 436, 434, 445, 9834, 13, 407, 456, 603, 312, 294, 264, 9834, 19667, 295, 264, 2316, 13], "temperature": 0.0, "avg_logprob": -0.12639335736836474, "compression_ratio": 1.7777777777777777, "no_speech_prob": 4.157181137998123e-06}, {"id": 480, "seek": 348552, "start": 3498.52, "end": 3507.52, "text": " But like they're not, they're not really parameters that makes sense independently of all the other parameters at all. So I don't think you would treat them any differently.", "tokens": [583, 411, 436, 434, 406, 11, 436, 434, 406, 534, 9834, 300, 1669, 2020, 21761, 295, 439, 264, 661, 9834, 412, 439, 13, 407, 286, 500, 380, 519, 291, 576, 2387, 552, 604, 7614, 13], "temperature": 0.0, "avg_logprob": -0.12639335736836474, "compression_ratio": 1.7777777777777777, "no_speech_prob": 4.157181137998123e-06}, {"id": 481, "seek": 350752, "start": 3507.52, "end": 3516.52, "text": " If you use, say, image nets statistics when you're fine tuning and that's the result of your model right you're going to use that down the track as well.", "tokens": [759, 291, 764, 11, 584, 11, 3256, 36170, 12523, 562, 291, 434, 2489, 15164, 293, 300, 311, 264, 1874, 295, 428, 2316, 558, 291, 434, 516, 281, 764, 300, 760, 264, 2837, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.15478161589740075, "compression_ratio": 1.5968586387434556, "no_speech_prob": 1.8161132402383373e-06}, {"id": 482, "seek": 350752, "start": 3516.52, "end": 3532.52, "text": " Well, yes and no, like that's what you normalize with but, but you've got batch normal layers which then obviously dividing and subtracting themselves.", "tokens": [1042, 11, 2086, 293, 572, 11, 411, 300, 311, 437, 291, 2710, 1125, 365, 457, 11, 457, 291, 600, 658, 15245, 2710, 7914, 597, 550, 2745, 26764, 293, 16390, 278, 2969, 13], "temperature": 0.0, "avg_logprob": -0.15478161589740075, "compression_ratio": 1.5968586387434556, "no_speech_prob": 1.8161132402383373e-06}, {"id": 483, "seek": 353252, "start": 3532.52, "end": 3545.52, "text": " So yeah, I mean, you're those normalization stats are going to change but there isn't really any reason to, you know, it would only be if you", "tokens": [407, 1338, 11, 286, 914, 11, 291, 434, 729, 2710, 2144, 18152, 366, 516, 281, 1319, 457, 456, 1943, 380, 534, 604, 1778, 281, 11, 291, 458, 11, 309, 576, 787, 312, 498, 291], "temperature": 0.0, "avg_logprob": -0.1258988786250987, "compression_ratio": 1.3257575757575757, "no_speech_prob": 3.668483259389177e-06}, {"id": 484, "seek": 353252, "start": 3545.52, "end": 3550.52, "text": " trained a new model from scratch.", "tokens": [8895, 257, 777, 2316, 490, 8459, 13], "temperature": 0.0, "avg_logprob": -0.1258988786250987, "compression_ratio": 1.3257575757575757, "no_speech_prob": 3.668483259389177e-06}, {"id": 485, "seek": 355052, "start": 3550.52, "end": 3566.52, "text": " I just want to have a look at this next one. So this is 27 to 1824. Yeah, this is actually kind of what I thought might happen is on a slightly better model, you know, we may be getting slightly better errors, initially, then as it trains a bit.", "tokens": [286, 445, 528, 281, 362, 257, 574, 412, 341, 958, 472, 13, 407, 341, 307, 7634, 281, 2443, 7911, 13, 865, 11, 341, 307, 767, 733, 295, 437, 286, 1194, 1062, 1051, 307, 322, 257, 4748, 1101, 2316, 11, 291, 458, 11, 321, 815, 312, 1242, 4748, 1101, 13603, 11, 9105, 11, 550, 382, 309, 16329, 257, 857, 13], "temperature": 0.0, "avg_logprob": -0.13994507167650305, "compression_ratio": 1.4615384615384615, "no_speech_prob": 2.994299393321853e-06}, {"id": 486, "seek": 355052, "start": 3566.52, "end": 3576.52, "text": " Makes no difference.", "tokens": [25245, 572, 2649, 13], "temperature": 0.0, "avg_logprob": -0.13994507167650305, "compression_ratio": 1.4615384615384615, "no_speech_prob": 2.994299393321853e-06}, {"id": 487, "seek": 357652, "start": 3576.52, "end": 3580.52, "text": " Cool.", "tokens": [8561, 13], "temperature": 0.0, "avg_logprob": -0.2191930917593149, "compression_ratio": 1.0253164556962024, "no_speech_prob": 1.112454538088059e-05}, {"id": 488, "seek": 357652, "start": 3580.52, "end": 3591.52, "text": " Alright, so, yeah, I'd love people to try out faster AI from master because", "tokens": [2798, 11, 370, 11, 1338, 11, 286, 1116, 959, 561, 281, 853, 484, 4663, 7318, 490, 4505, 570], "temperature": 0.0, "avg_logprob": -0.2191930917593149, "compression_ratio": 1.0253164556962024, "no_speech_prob": 1.112454538088059e-05}, {"id": 489, "seek": 359152, "start": 3591.52, "end": 3610.52, "text": " many of your models look substantially better or even more important substantially worse or to normalize Tim models.", "tokens": [867, 295, 428, 5245, 574, 30797, 1101, 420, 754, 544, 1021, 30797, 5324, 420, 281, 2710, 1125, 7172, 5245, 13], "temperature": 0.0, "avg_logprob": -0.20433861868722097, "compression_ratio": 1.2842105263157895, "no_speech_prob": 8.013058504729997e-06}, {"id": 490, "seek": 359152, "start": 3610.52, "end": 3616.52, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.20433861868722097, "compression_ratio": 1.2842105263157895, "no_speech_prob": 8.013058504729997e-06}, {"id": 491, "seek": 361652, "start": 3616.52, "end": 3639.52, "text": " This is 3716.", "tokens": [639, 307, 13435, 6866, 13], "temperature": 0.0, "avg_logprob": -0.28065962261623806, "compression_ratio": 0.6842105263157895, "no_speech_prob": 3.423987072892487e-05}, {"id": 492, "seek": 363952, "start": 3639.52, "end": 3647.52, "text": " Does anybody have any questions before we wrap it up.", "tokens": [4402, 4472, 362, 604, 1651, 949, 321, 7019, 309, 493, 13], "temperature": 0.0, "avg_logprob": -0.20954618880997844, "compression_ratio": 1.4107142857142858, "no_speech_prob": 4.784572411153931e-06}, {"id": 493, "seek": 363952, "start": 3647.52, "end": 3649.52, "text": " Just with normalize.", "tokens": [1449, 365, 2710, 1125, 13], "temperature": 0.0, "avg_logprob": -0.20954618880997844, "compression_ratio": 1.4107142857142858, "no_speech_prob": 4.784572411153931e-06}, {"id": 494, "seek": 363952, "start": 3649.52, "end": 3655.52, "text": " It's just the initial error it will be a bit more less than earlier approach right.", "tokens": [467, 311, 445, 264, 5883, 6713, 309, 486, 312, 257, 857, 544, 1570, 813, 3071, 3109, 558, 13], "temperature": 0.0, "avg_logprob": -0.20954618880997844, "compression_ratio": 1.4107142857142858, "no_speech_prob": 4.784572411153931e-06}, {"id": 495, "seek": 363952, "start": 3655.52, "end": 3660.52, "text": " Yeah, so like that that that, you know, well,", "tokens": [865, 11, 370, 411, 300, 300, 300, 11, 291, 458, 11, 731, 11], "temperature": 0.0, "avg_logprob": -0.20954618880997844, "compression_ratio": 1.4107142857142858, "no_speech_prob": 4.784572411153931e-06}, {"id": 496, "seek": 363952, "start": 3660.52, "end": 3663.52, "text": " at first you have a random head.", "tokens": [412, 700, 291, 362, 257, 4974, 1378, 13], "temperature": 0.0, "avg_logprob": -0.20954618880997844, "compression_ratio": 1.4107142857142858, "no_speech_prob": 4.784572411153931e-06}, {"id": 497, "seek": 366352, "start": 3663.52, "end": 3669.52, "text": " It doesn't actually matter right it randoms random whether you normalize or not.", "tokens": [467, 1177, 380, 767, 1871, 558, 309, 4974, 82, 4974, 1968, 291, 2710, 1125, 420, 406, 13], "temperature": 0.0, "avg_logprob": -0.12465423696181353, "compression_ratio": 1.4046242774566473, "no_speech_prob": 5.506539309863001e-06}, {"id": 498, "seek": 366352, "start": 3669.52, "end": 3673.52, "text": " So,", "tokens": [407, 11], "temperature": 0.0, "avg_logprob": -0.12465423696181353, "compression_ratio": 1.4046242774566473, "no_speech_prob": 5.506539309863001e-06}, {"id": 499, "seek": 366352, "start": 3673.52, "end": 3677.52, "text": " maybe you know the fastest 10 batches.", "tokens": [1310, 291, 458, 264, 14573, 1266, 15245, 279, 13], "temperature": 0.0, "avg_logprob": -0.12465423696181353, "compression_ratio": 1.4046242774566473, "no_speech_prob": 5.506539309863001e-06}, {"id": 500, "seek": 366352, "start": 3677.52, "end": 3688.52, "text": " It's better or something. But, yeah, I don't know, like, I mean, be interesting to see if anybody notices a difference.", "tokens": [467, 311, 1101, 420, 746, 13, 583, 11, 1338, 11, 286, 500, 380, 458, 11, 411, 11, 286, 914, 11, 312, 1880, 281, 536, 498, 4472, 32978, 257, 2649, 13], "temperature": 0.0, "avg_logprob": -0.12465423696181353, "compression_ratio": 1.4046242774566473, "no_speech_prob": 5.506539309863001e-06}, {"id": 501, "seek": 368852, "start": 3688.52, "end": 3702.52, "text": " It's just this used to matter a lot right for a couple of reasons. One is that most people didn't find chain models most people train most models and scratch until until fast AI came along, pretty much.", "tokens": [467, 311, 445, 341, 1143, 281, 1871, 257, 688, 558, 337, 257, 1916, 295, 4112, 13, 1485, 307, 300, 881, 561, 994, 380, 915, 5021, 5245, 881, 561, 3847, 881, 5245, 293, 8459, 1826, 1826, 2370, 7318, 1361, 2051, 11, 1238, 709, 13], "temperature": 0.0, "avg_logprob": -0.21355687811019572, "compression_ratio": 1.7342342342342343, "no_speech_prob": 4.63758988189511e-06}, {"id": 502, "seek": 368852, "start": 3702.52, "end": 3705.52, "text": " And then secondly,", "tokens": [400, 550, 26246, 11], "temperature": 0.0, "avg_logprob": -0.21355687811019572, "compression_ratio": 1.7342342342342343, "no_speech_prob": 4.63758988189511e-06}, {"id": 503, "seek": 368852, "start": 3705.52, "end": 3707.52, "text": " what we didn't have batch norm.", "tokens": [437, 321, 994, 380, 362, 15245, 2026, 13], "temperature": 0.0, "avg_logprob": -0.21355687811019572, "compression_ratio": 1.7342342342342343, "no_speech_prob": 4.63758988189511e-06}, {"id": 504, "seek": 368852, "start": 3707.52, "end": 3716.52, "text": " Right, so it was totally critical. And then even when batch norm came along we didn't know how to find your models with batch norm.", "tokens": [1779, 11, 370, 309, 390, 3879, 4924, 13, 400, 550, 754, 562, 15245, 2026, 1361, 2051, 321, 994, 380, 458, 577, 281, 915, 428, 5245, 365, 15245, 2026, 13], "temperature": 0.0, "avg_logprob": -0.21355687811019572, "compression_ratio": 1.7342342342342343, "no_speech_prob": 4.63758988189511e-06}, {"id": 505, "seek": 371652, "start": 3716.52, "end": 3719.52, "text": " So we just fine tuned the head.", "tokens": [407, 321, 445, 2489, 10870, 264, 1378, 13], "temperature": 0.0, "avg_logprob": -0.21962671279907225, "compression_ratio": 1.5572916666666667, "no_speech_prob": 1.4281824405770749e-05}, {"id": 506, "seek": 371652, "start": 3719.52, "end": 3725.52, "text": " At that point, we didn't realize that you had to find tune the batch norm layers as well.", "tokens": [1711, 300, 935, 11, 321, 994, 380, 4325, 300, 291, 632, 281, 915, 10864, 264, 15245, 2026, 7914, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.21962671279907225, "compression_ratio": 1.5572916666666667, "no_speech_prob": 1.4281824405770749e-05}, {"id": 507, "seek": 371652, "start": 3725.52, "end": 3741.52, "text": " So, I remember emailing Francois the creator of care us and I was saying to him like I'm trying to find tune your care us model and it's like bizarrely bad like why why is that.", "tokens": [407, 11, 286, 1604, 3796, 278, 34695, 271, 264, 14181, 295, 1127, 505, 293, 286, 390, 1566, 281, 796, 411, 286, 478, 1382, 281, 915, 10864, 428, 1127, 505, 2316, 293, 309, 311, 411, 18265, 356, 1578, 411, 983, 983, 307, 300, 13], "temperature": 0.0, "avg_logprob": -0.21962671279907225, "compression_ratio": 1.5572916666666667, "no_speech_prob": 1.4281824405770749e-05}, {"id": 508, "seek": 374152, "start": 3741.52, "end": 3750.52, "text": " I'm probably doing the wrong thing here's documentation whatever like no I'm pretty sure I'm doing the right thing and I spent like three months, trying to answer this question.", "tokens": [286, 478, 1391, 884, 264, 2085, 551, 510, 311, 14333, 2035, 411, 572, 286, 478, 1238, 988, 286, 478, 884, 264, 558, 551, 293, 286, 4418, 411, 1045, 2493, 11, 1382, 281, 1867, 341, 1168, 13], "temperature": 0.0, "avg_logprob": -0.26810920238494873, "compression_ratio": 1.6721311475409837, "no_speech_prob": 8.138423254422378e-06}, {"id": 509, "seek": 374152, "start": 3750.52, "end": 3765.52, "text": " Eventually I realized it's like, holy shit it's the best normalize I sent him an email and said oh we can't find tune care us models like this, which you have to find in batch normalize, which I don't think they changed for years.", "tokens": [17586, 286, 5334, 309, 311, 411, 11, 10622, 4611, 309, 311, 264, 1151, 2710, 1125, 286, 2279, 796, 364, 3796, 293, 848, 1954, 321, 393, 380, 915, 10864, 1127, 505, 5245, 411, 341, 11, 597, 291, 362, 281, 915, 294, 15245, 2710, 1125, 11, 597, 286, 500, 380, 519, 436, 3105, 337, 924, 13], "temperature": 0.0, "avg_logprob": -0.26810920238494873, "compression_ratio": 1.6721311475409837, "no_speech_prob": 8.138423254422378e-06}, {"id": 510, "seek": 376552, "start": 3765.52, "end": 3781.52, "text": " Anyway, so those there so those changes is why I guess this whole normalization layer thing is much less interesting than I guess we thought, which is why we hadn't really noticed it wasn't working before.", "tokens": [5684, 11, 370, 729, 456, 370, 729, 2962, 307, 983, 286, 2041, 341, 1379, 2710, 2144, 4583, 551, 307, 709, 1570, 1880, 813, 286, 2041, 321, 1194, 11, 597, 307, 983, 321, 8782, 380, 534, 5694, 309, 2067, 380, 1364, 949, 13], "temperature": 0.0, "avg_logprob": -0.11673933375965465, "compression_ratio": 1.48125, "no_speech_prob": 2.885257708840072e-05}, {"id": 511, "seek": 376552, "start": 3781.52, "end": 3790.52, "text": " Because I'm also training fine.", "tokens": [1436, 286, 478, 611, 3097, 2489, 13], "temperature": 0.0, "avg_logprob": -0.11673933375965465, "compression_ratio": 1.48125, "no_speech_prob": 2.885257708840072e-05}, {"id": 512, "seek": 379052, "start": 3790.52, "end": 3799.52, "text": " Anybody else have any questions before we wrap up.", "tokens": [19082, 1646, 362, 604, 1651, 949, 321, 7019, 493, 13], "temperature": 0.0, "avg_logprob": -0.23841040062181879, "compression_ratio": 1.0618556701030928, "no_speech_prob": 0.00021563876362051815}, {"id": 513, "seek": 379052, "start": 3799.52, "end": 3801.52, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.23841040062181879, "compression_ratio": 1.0618556701030928, "no_speech_prob": 0.00021563876362051815}, {"id": 514, "seek": 380152, "start": 3801.52, "end": 3821.52, "text": " Thank you. Let's see you all. Good luck with unit. Bye.", "tokens": [50364, 1044, 291, 13, 961, 311, 536, 291, 439, 13, 2205, 3668, 365, 4985, 13, 4621, 13, 51364], "temperature": 0.0, "avg_logprob": -0.4243153521889134, "compression_ratio": 0.9166666666666666, "no_speech_prob": 0.00010976037447107956}], "language": "en"}