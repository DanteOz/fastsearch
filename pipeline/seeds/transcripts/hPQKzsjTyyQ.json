{"text": " Well, welcome back. Welcome to lesson 11 where we're going to be talking mainly about data loading and optimizers. I said we would be talking about fastai.audio, but that's going to be a little bit later in the course. We haven't quite got to where I wanted to get to yet. So everything I said we'd talk about last week, we will talk about, but it might take a few more lessons to get there than I said. So this is kind of where we're up to, is the last little bit of our CNN. And specifically these were the things we were going to dive into when we've done the first four of these. CUDA, convolutions, hooks, normalization. So we're going to keep going through this process to try to create our state of the art image net model. The specific items that we're working with are images, but everything we've covered so far is equally valid, equally used for tabular collaborative filtering and text and pretty much everything else as well. Last week we talked about batch norm, and I just wanted to mention that at the end of the batch norm notebook there's another bit called simplified running batch norm. We talked a little bit about de-biasing last week, we'll talk about it more today. But Stas Begman pointed out something which is kind of obvious in hindsight, but I didn't notice at the time, which is that we had sums divided by de-bias and we had count divided by de-bias, and then we go sum divided by count, and sum divided by de-bias divided by count divided by de-bias, the two de-biases cancel each other out, so we can remove all of them. So we're still going to cover de-biasing today for a different purpose, but actually we didn't really need it for last week. So we can remove all the de-biasing and end up with something much simpler. So that's the version that we're going to go with. And also thanks to Tom Veman who pointed out that the last step where we went subtract mean divided by standard deviation multiplied by mults at adds, you can just rearrange that into this form, mults divided by variances and adds minus means times that factor. And if you do it this way, then you don't actually have to touch x until you've done all of those things, and so that's going to end up being faster. If you think through the broadcasting operations there, then you're doing a lot less computation this way. So that was a good idea as well. And I should mention Tom has been helping us out quite a bit with some of this batch norm stuff. One of a few people involved in the PyTorch community who's been amazingly helpful who I'd like to call out. So thanks also to Sumit Chintala who was one of the original founders of PyTorch who's been super helpful in sorting some things out for this course and also Francisco Massa also super helpful. They're both part of the official Facebook engineering team and Tom's not, but he does so much work for PyTorch. He kind of seems like he must be sometimes. So thanks to all of you for your great help. Okay. Before we moved on to data blocks, I wanted to mention one other approach to making sure that your model trains nicely. And to me, this is the most fast AI-ish method. I wish I had come up with it, but I didn't. Wonderful researcher named Dmitry came up with it in a paper called All You Need Is a Good Unit, Dmitry Mishkin, and this is the paper. And he came up with this technique called LSUV, Layer-Wise Sequential Unit Variance. And the basic idea is this. You've seen now how fiddly it is to get your unit variances all the way through your network and little things can change that. So if you change your activation function or something we haven't mentioned, if you add dropout or change the amount of dropout, these are all going to impact the variances of your layer outputs. And if they're just a little bit different to one, you'll get exponentially worse, as we saw, through the model. So the normal approach to fixing this is to think really carefully about your architecture and exactly, analytically figure out how to initialize everything so it works. And Dmitry's idea, which I like a lot better, is let the computer figure it out. And here's how you let the computer figure it out. We create our MNIST data set in the same way as before. We create a bunch of layers with these number of filters like before. And what I'm going to do is I'm going to create a conv layer class, which contains our convolution and our relu. The idea is that we're going to use this because now we can basically say this whole combined conv plus relu has kind of a, I'm calling it bias, but actually I'm taking that general relu and just saying how much are we subtracting from it. So this is kind of like something we can add or remove. And then the weight is just the conv weights. And you'll see why we're doing this in a moment. Basically what we'll do is we'll create our learner in the usual way, right? And however it initializes is fine. And so we can train it. That's fine. But let's try and now train it in a better way. So let's recreate our learner. And let's grab a single minibatch. And here's a function that will let us grab a single minibatch, making sure we're using all our callbacks, that the minibatch does all the things we need it to do. So here's one minibatch of X and Y. And what we're going to do is we're going to find all of the modules that are, we're going to find all of the modules which are of type conv layer. And so it's just a little function that does that. And generally speaking, when you're working with PyTorch modules or with neural nets more generally, you need to use recursion a lot because modules can contain modules can contain modules, right? So you can see here, find modules, calls find modules to find out all the modules throughout your kind of tree, you know, because really a module is like a tree. Modules have modules have modules. And so here's our list of all of our conv layers. And then what we do is we create a hook, right? And the hook is just going to grab the mean and standard deviation of a particular module. And so we can just, we can first of all just print those out. And we can see that the means and standard deviations are not zero one. The means are too high, right? As we know, because we've got the relus and the standard deviations are too low. So rather than coming up with our perfect in it, instead, we just create a loop. And the loop calls the model passing in that mini batch we have, right? And remember this is, so first of all, we hook it, right? And then we call the model in a while loop. We check whether the mean, the absolute value of the mean is close to zero. And if it's not, we subtract the mean from the bias. And so it just keeps looping through, calling the model again with the hook, subtracting from the bias until we get about zero mean. And then we do the same thing for the standard deviation. Keep checking whether standard deviation minus one is nearly zero. And as long as it isn't, we'll keep dividing by the standard deviation. And so those two loops, if we run this function, then it's going to eventually give us what we want. Now it's not perfect, right? The means are still not quite zero because we do the means first and then the standard deviations. And the standard deviation changes, we'll slightly change the mean. But you can see our means are, standard deviations are perfectly one. And our means are pretty close. And that's it. This is called LSUV. And this is how, without thinking at all, you can initialize any neural network, pretty much, to get the unit variance all the way through. And this is much easier than having to think about whether you've got ReLU or ELU or whether you've got dropout or whatever else. So here's a super cool trick. Yeah, and then we can train it, and it trains very nicely. Particularly useful for complex and deeper architectures. So there's kind of, for me, the fast AI approach to initializing your neural nets, which is no math, no thinking, just a simple little for loop, or in this case, a while loop. All right. So I think we've done enough with MNIST because we're getting really good results. It's running fast. It's looking good. It's looking good, but it's looking a little bit harder. So what are we going to try? Well, we're not quite ready to try ImageNet because ImageNet takes quite a lot of time, you know, a few days if you've got just one GPU to train. And that's really frustrating and an expensive way to try to practice things or learn things or try things out. I kept finding this problem of not knowing what data set I should try for my research or for my practice or for my learning. You know, it seemed like at one end there was MNIST, which is kind of too easy. There was Sci-Fi 10 that a lot of people use, but these are 32 by 32 pixel images. And it turns out, and this is something I haven't seen really well written about, but our research clearly shows, it turns out that small images, 32 by 32, have very different characteristics to larger images. And specifically, it seems like once you get beneath about 96 by 96, things behave really differently. So stuff that works well on Sci-Fi 10 tends not to work well on normal sized images, because 32 by 32 is tiny, right? And stuff that tends to work well on Sci-Fi 10 doesn't necessarily work well on ImageNet. There's this kind of gap of like something with, you know, with normal sized images, which I can train in a sane amount of time, but also gives me a good sense of whether something's going to work well or not. And actually, Dmitro, who wrote that LSUV paper we just looked at, also had a fantastic paper called systematic evaluation, something like systematic evaluation of convolutional neural networks. And he noticed that if you use 128 by 128 images with ImageNet, then the kind of things that he found works well or doesn't work well, all of those discoveries applied equally well to the full sized ImageNet. It still takes too long, 128 by 128 for 1.5, well, 1.3 million images, still too long. So I thought that was a good step, but I wanted to go even further. So I tried creating two new datasets. And my two new datasets are subsets of ImageNet, and there's kind of like multiple versions in here, really, but they're both subsets of ImageNet. They both contain just 10 classes out of the thousand. So they're 1, 100th of the number of images of ImageNet. And I create a number of versions, full size, 320 pixel size, and 160 pixel size. One dataset is specifically designed to be easy. It contains 10 classes that are all very different to each other. So this is like my starting point. I thought, well, what if I create this dataset, then maybe I could train it for like just an epoch or two, like just a couple of minutes, and see whether something was going to work. And then the second one I created was one designed to be hard, which is 10 categories are designed to be very similar to each other. So they're all dog breeds. So the first dataset is called ImageNet, which is very French, as you can hear. And there's some helpful pronunciation tips here. And the second is called ImageWolf. And you can see here I've created a leaderboard for ImageNet and for ImageWolf. And I've discovered that in my very quick experiments with this, the exact observations I find about what works well for the full ImageNet, also I see the same results here. It is also fascinating to see how some things are the same between the two datasets, and some are different. And I found working with these two datasets has given me more insight into computer vision model training than anything else that I've done. So check them out. And I really wanted to mention this to say a big part of getting good at using deep learning in your domain is knowing how to create small, workable, useful datasets. So once I decided to make this, it took me about three hours. It's not at all hard to create a dataset. It's a quick little Python script to grab the things I wanted. How did I decide which 10 things? I just looked at a list of categories and picked 10 things that I knew were different. How did I decide to pick these things? I just looked at 10 things that I knew were dogs. So it's like, throw something together, get it working, and then on your domain area, whether it's audio or Sanskrit texts or whatever, or genomic sequences, try to come up with your version of a toy problem or two, which you hope might give insight into your full problem. So this has been super helpful for me. And if you're interested in computer vision, I would strongly recommend trying this out. And specifically, try to beat me, right? Because trying to beat me, and these are not great, they're just okay, but trying to beat me will give you a sense of whether the things you're thinking about are in the ballpark of what a moderately competent practitioner is able to do in a small amount of time. It's also interesting to see that with a 1 one hundredth the size of ImageNet, like a tiny data set, I was able to create a 90% accurate dog breed classifier from random weights. So you can do a lot pretty quickly without much data, even if you don't have transfer learning, which is kind of amazing. So we're going to use this data set now. Oh, sorry, you had a question. So before we look at the data set, let's do the question. So just to confirm, LSUV is something you run on all the layers once at the beginning, not during training. What if your batch size is small? Could you overfit to that batch? Yeah, that's right. So you'd run it once at the start of training to initialize your weights, just so that that initial set of steps gives you sensible gradients. Because it's those first few mini batches that are everything. Remember how we saw that if we didn't have a very good first few mini batches that we ended up with 90% of the activations being inactive? So that's why we want to make sure we start well. And yeah, if you've got a small mini batch, just run five mini batches and take the mean. There's nothing special about the one mini batch. It's just a fast way to do the computation. It's not like we're doing any gradient descent or anything. It's just a forward pass. Thanks, that was a good question. So ImageNet is too big to read it all into RAM at once. It's not huge, but it's too big to do that. So we're going to need to be able to read it in one image at a time, which is going to be true of most of our deep learning projects. So we need some way to do that from scratch, because that's the rules. So let's start working through that process. And in the process, we're going to end up building a data block API, which you're all familiar with. But most people using the data block API feel familiar enough with it to do small tweaks for things that they kind of know they can do. But most people I speak to don't know how to really change what's going on. So by the end of this notebook, you'll see how incredibly simple the data block API is. And you'll be able to either write your own, maybe based on this one, or modify the one in Fast AI, because this is a very direct translation of the one that's in Fast AI. So you should be able to get going. So the first thing to do is to read in our data. And we'll see a similar thing when we build FastAI.audio. But whatever process you use, you're going to have to find some library that can read the kind of data that you want. So in our case, we have images. And there's a library called PIL, or Pillow, Python Imaging Library, which can read images. So let's import it. We'll grab the data set and untie it, import Pillow. And we want to see what's inside our ImageNet data set. Typing list x.editor is far too complicated for me. I just want to type ls. So be lazy. This is how easy it is to add stuff to the standard library. You can just take the class and add a function to it. So now we have ls. So here's ls. So we've got a training and a validation directory. In validation, we have one directory for each category. And then if we look at one category, we could grab one file name. And if we look at one file name, we have a tench. So if you want to know whether somebody is actually a deep learning practitioner, show them this photo. If they don't know it's a tench, they're lying to you, because this is the first category in ImageNet. So if you're ever using ImageNet, you know your tenches. They're generally being held up by middle-aged men, or sometimes they're in nets. That's pretty much how it always looks in ImageNet. So that's why we have them in ImageNet too, because it's such a classic computer vision fish. We're cheating in importing NumPy for a moment, just so I can show you what an image contains, just to turn it into an array so I can print it for you. This is really important. It contains bytes. It contains numbers between 0 and 255 that are integer. They're not float. So this is what we get when we load up an image. And it's got a geometry, and it's got a number of channels, and in this case it's RGB, three channels. So we want to have some way to read in lots of images, which means we need to know what images there are in this directory structure. And in the full ImageNet there's going to be 1.3 million of them, so we need to be able to do that fast. So the first thing we need to know is which things are images. So we need a list of image extensions. Your computer already has a list of image extensions. It's your MIME types database, so you can query Python for your MIME types database for all of the images. So here's a list of the image extensions that my computer knows about. So now what I want to do is I want to loop through all the files in a directory and find out which ones are one of these. The fastest way to check whether something's in a list is to first of all turn it into a set, and of course therefore we need Setify. So Setify simply checks if it is a set, and if it is, it makes it one. Otherwise it first turns it into a list and then turns it into a set. So that's how we can setify things. And here's what I do when I build a little bit of functionality. I just throw together a quick bunch of tests to make sure it seems to be roughly doing the right thing. And remember in lesson one we created our own test framework, so we can now run any notebook as a test suite, so it'll automatically check if we break this at some point. Okay, so now we need a way to go through a single directory and grab all of the images in that. So here we can say get files. I always like to make sure that you can pass any of these things, either a path, lib path, or a string to make it convenient. So if you just say p equals path p, if it's already a path lib object, that doesn't do anything. So this is a nice easy way to make sure that works. So we just go through, here's our path lib object. And so you'll see in a moment how we actually grab the list of files, but this is our parent directory. This is going to be our list of files. We go through the list of files. We check that it doesn't start with dot. If it does, that's a Unix hidden file or Mac hidden file. And we also check either they didn't ask for some particular extensions or that the extension is in the list of extensions we asked for. So that will allow us to grab just the image files. Python has something called Scander, which will grab a path and list all of the files in that path. So here is how we can call get files. We go Scander, and then we go get files, and it looks something like that. So that's just for one directory. So we can put all this together like so. So this is something where we say, for some path, give me things with these extensions, optionally recurse, optionally only include these folder names, and this is it. I will go through it in detail, but I'll just point out a couple of things because being able to rapidly look through files is important. The first is that Scander is super, super fast. This is Python's thin wrapper over a C API. So this is a really great way to quickly grab stuff for a single directory. If you need to recurse, check out os.walk. This is the thing that uses Scander internally to walk through recursively through a folder tree. You can do cool stuff like change the list of directories that it's going to look at, and it basically returns all the information that you need. It's super great. So os.walk and os.scander are the things that you want to be using if you're playing with directories and files in Python and you need it to be fast. We do. So here's get files, and so now we can say get files, path tench, just the image extensions. There we go. And then we're going to need recurse because we've got a few levels of directory structure. So here's with recurse. So if we try to get all of the file names, we have 13,000. And specifically, it takes 70 milliseconds to get 13,000 file names. For me to look at 13,000 files in Windows Explorer seems to take about four minutes. So this is unbelievably fast. So the full ImageNet, which is 100 times bigger, it's going to be literally just a few seconds. So this gives you a sense of how incredibly fast these os.walk and scander functions are. Yes, questions are good. I've often been confused as to whether the code Jeremy is writing in the notebooks are functionality that will be integrated into the Fast AI library or whether the functions and classes are meant to be written and used by the user interactively and on the fly. Well I guess that's really a question about what's the purpose of this deep learning from the foundations course. And different people will get different things out of it. But for me, it's about demystifying what's going on so that you can take what's in your head and turn it into something real. And to do that would be always some combination of using things that are in existing libraries, which might be Fast AI or PyTorch or TensorFlow or whatever. And partly will be things that aren't in existing libraries. And I don't want you to be in a situation where you say, well, that's not in Fast AI, therefore I don't know how to do it. So really the goal is, this is why it's also called impractical deep learning for coders, is to give you the underlying expertise and tools that you need. In practice, I would expect a lot of the stuff I'm showing you to end up in the Fast AI library, because that's like literally I'm showing you my research, basically. This is like my research journal of the last six months. And that's what happens is Fast AI library is silver and I take our research and turn it into a library. And some of it, like this function, is pretty much copied and pasted from the existing Fast AI V1 code base, because I spent at least a week figuring out how to make this fast. I'm sure most people can do it faster, but I'm slow and it took me a long time. And this is what I came up with. So yeah, I mean, it's going to map pretty closely to what's in Fast AI already. Where things are new, we're telling you, like running batch norm is new. Today we're going to be seeing a whole new kind of optimizer. But otherwise, things are going to be pretty similar to what's in Fast AI, so it'll make you be able to quickly hack it Fast AI V1. And as Fast AI changes, it's not going to surprise you because you'll know what's going on. Sure. How does Scander compare to Glob? Scander should be much faster than Glob. It's a little more awkward to work with because it doesn't try to do so much. It's the lowest level thing. I suspect Glob probably uses it behind the scenes. You should try. Do a time it with Glob, time it with Scander. Probably depends how you use Glob exactly. But I remember I used to use Glob and it was quite a bit slower. And when I say quite a bit, for those of you that have been using Fast AI for a while, you might have noticed that the speed at which you can grab the ImageNet folder is some orders of magnitude faster than it used to be. That's quite a big difference. Okay. So the reason that Fast AI has a data blocks API and nobody else does is because I got so frustrated in the last course at having to create every possible combination of independent and dependent variable that I actually sat back for a while and did some thinking. And specifically, this is what I did when I thought was I wrote this down. It's like what do you actually need to do? So let's go ahead and do these things. So we've already got files. We need some way to split the validation set or multiple validation sets out, some way to do labeling, optionally some augmentation, transform it to a tensor, make it into data, into batches, optionally transform the batches, and then combine the data loaders together into a data bunch and optionally add a test set. And so when I wrote it down like that, I just went ahead and implemented an API for each of those things to say like, okay, you can plug in anything you like to that part of the API. So let's do it. Right? So we've already got the basic functionality to get the files. So now we have to put them somewhere. We already created that list container. Right? So we basically can just dump our files into a list container. But in the end, what we actually want is an image list for this one. And an image list, when you call... So we're going to have this get method. And when you get something from the image list, it should open the image. So pil.image.open is how you open an image. But we could get all kinds of different objects. So therefore, we have this superclass, which has a get method that you override. And by default, it just returns whatever you put in there, which in this case would be the file name. So this is basically all item list does. Right? It's got a list of items. Right? So in this case, it's going to be our file names. The path that they came from. And then optionally also, there could be a list of transforms. Right? And transforms are some kind of functions. And we'll look at this in more detail in a moment. But basically, what will happen is when you index into your item list, remember done to get item does that. We'll pass that back up to list container, get item. And that will return either a single item or a list of items. And if it's a single item, we'll just call self.underscoreget. If it's a list of items, we'll call self.underscoreget on all of them. And what that's going to do is it's going to call the get method, which in the case of an image list, will open the image. And then it'll compose the transforms. So for those of you that haven't done any kind of more functional style programming, compose is just a concept that says go through a list of functions and call the function and replace myself with the result of that. And then call the next function and replace myself with the result of that and so forth. So in other words, a deep neural network is just a composition of functions. Each layer is a function. We compose them all together. This compose does a little bit more than most composers. Specifically, you can optionally say I want to order them in some way. And it checks to see whether the things have an underscore order key and sorts them. And also, you can pass in some keyword arguments. And if you do, it'll just keep passing in those keyword arguments. But it's basically, other than that, it's a pretty standard function composition function. If you haven't seen compose used elsewhere in programming before, Google that because it's a super useful concept. Comes up all the time. We use it all the time. And as you can see, in this case, it means I can just pass in a list of transforms. And this will simply call each of those transforms in turn, modifying, in this case, the image that I had. So here's how you create an image list. And then here's a method to create an image list from a path. And it's just going to call that get files. And then that's going to give us a list of files, which we will then pass to the class constructor, which expects a list of files or a list of something. So this is basically the same as item list in Fast AI version 1. It's just a list. It's just a list where when you try to index into it, it will call something which subclasses overread. OK. So now we've got an image list. We can use it. Now, one thing that happens all the time is you try to create a mini batch of images. One of your images was black and white. And when Pillow opens up a black and white image, it gives you back by default a rank two tensor, just the x and y, no channel axis. And then you can't stack them into a mini batch because they're not all the same shape. So what you can do is you can call the Pillow.convert and RGB. If something's not RGB, it'll turn it into RGB. So here's our first transform. So a transform is just a class with an underscore order. And then makeRGB is a transform that when you call it will call convert. Or you could just do it this way. Just make it a function. Both is fine. These are both going to do the same thing. And this is often the case, right? You can, we've seen it a bunch of times before. You can have done to call or you can have a function. So here's our first transform. And so here's the simplest version. It's just a function. And so if we create a image list from files using our path and pass in that transform, then now we have an item list. And remember that item list inherits from list container, which we gave a Dundar Repra to. So it's going to give us nice printing. This is why we create these little convenient things to subclass from, because we get all this behavior for free. So we can now see that we've got 13,000 items. And here's a few of them. Is the path. And we can index into it. And when we index into it, it calls get and get calls image.open. And pillow automatically displays images in Jupyter. And so there it is. And this, of course, is a man with a tange. Yes. Thank you. OK. He looks very happy with it. We're going to be seeing him a lot. And because we're using the functionality that we wrote last time for list container, we can also index with a list of Booleans, with a slice, with a list of ints, and so forth. So here's a slice containing one item, for instance. All right. So that's step one. Step two, split validation set. So to do that, we look and we see here's a path. Here's the file name. Here's the parent. Here's the grandparent. So here's the grandparent's name. That's the thing we use to split. So let's create a function called grandparent splitter that grabs the grandparent's name. And you call it, telling it the name of your validation set and the name of your training set. And it returns true if it's the validation set or false if it's the training set or none if it's neither. And so here's something that will create a mask containing your pass at some function. So we're going to be using grandparent splitter. And it will just grab all the things where that mask is false. That's the training set. All the things where the mask is true. That's the validation set and it will return them. Okay. So there's our splitter. Remember, we used partial. So here's a splitter that splits on grandparents and where the validation name is val because that's what it is for image net. And let's check that that seems to work. Yes, it does. We've now got a validation set with 500 things and a training set with 12,800 things. So that's looking good. So let's use it. So split data object is just something with a training set and a validation set. You pass it in. You save them away. And then that's basically it. Everything else from here is just convenience. So we'll give it a representation so that you can print it. We'll define dunder get attribute so that if you pass it some attribute that it doesn't know about, it'll grab it from the training set. And then let's add a split by funk method that just calls that split by funk thing we just had. There's one trick here though, which is we want split by funk to return item lists of the same type that we gave it. In this case, it would be an image list. So we call item list.new. And that's why in our item list we defined something called new. And this is a really handy trick. PyTorch has the concept of a new method as well. It says, all right, let's look at this object. Let's see what class it is. Because it might not be item list, right? It might be image list or some other subclass. It doesn't exist yet. And this is now the constructor for that class. And it's just passing in the items that we asked for. And then passing in our path and our transforms. So new is going to create a new item list of the same type, with the same path, and the same transforms, but with these new items. And so that's why this is now going to give us a training set and a validation set with the same path, the same transforms, and the same type. And so if we call split data, split by func, now you can see we've got our training set and our validation set. Easy. So next in our list of things to do is labeling. Labeling is a little more tricky. And the reason it's tricky is because we need processes. These are things which are first applied to the training set. They get some state. And then they get applied to the validation set. For example, our labels should not be tench and french horn. They should be like zero and two. Because when we go to do a cross entropy loss, we expect to see a long there, not a string there. So we need to be able to map tench to zero, or french horn to two. We need the training set to have the same mapping as the validation set. And for any inference we do in the future, it's going to have the same mapping as well. Because otherwise, the different data sets are going to be talking about completely different things when they see the number zero, for instance. So we're going to create something called a vocab. And a vocab is just the list saying these are our classes, and this is the order they're in. Zero is tench, one is golf ball, two is french horn, and so forth. So we're going to create the vocab from the training set. And then we're going to convert all those strings into ints using the vocab. And then we're going to do the same thing for the validation set, but we'll use the training set's vocab. So that's an example of a processor that converts label strings to numbers in a consistent and reproducible ways. Other things we could do would be processing texts to tokenize them and then numericalize them. Numericalizing them is a lot like converting the label strings to numbers. Or taking tabular data and filling the missing values with a median computed on the training set. Or whatever. So most things we do in this labeling process is going to require some kind of processor. So in our case we want a processor that can convert label strings to numbers. So the first thing we need to know is what are all of the possible labels. And so therefore we need to know all the possible unique things in a list. So here's some list. Here's something that unicifies them. So that's how we can get all the unique values of something. So now that we've got that, we can create a processor. And a processor is just something that can process some items. And so let's create a category processor. And this is the thing that's going to create our list of all of the possible categories. So basically when you say process, we're going to see if there's a vocab yet. And if there's not, this must be the training set. So we'll create a vocab. And it's just the unique values of all the items. And then we'll create the thing that goes not from int to object, but goes from object to int. So it's the reverse mapping. So we just enumerate the vocabulary and create a dictionary with the reverse mapping. So now that we have a vocab, we can then go through all the items and process one of them at a time. And process one of them simply means look in that reverse mapping. We could also de-process, which would take a bunch of indexes. We would use this, for example, to print out the inferences that we're doing. So we better make sure we've got a vocab by now. Otherwise we can't do anything. And then we just de-process one for each index. And de-process one just looks it up in the vocab. So that's all we need. And so with this, we can now combine it all together. And let's create a processed item list. And it's just a list container that contains a processor. And the items in it, whatever we were given after being processed. And so then, as well as being able to index in it to grab those processed items, we'll also define something called object. And that's just the thing that's going to de-process the items again. So that's all the stuff we need to label things. So we already know that for splitting, we needed the grandparent. For labeling, we need the parent. So here's a parent labeler. And here is something which labels things using a function. It just calls a function for each thing. And so here is our class. And we're going to have to pass it some independent variable and some dependent variable and store them away. And then we need a indexer to grab the x and grab the y at those indexes. We need a length. We may as well make it print out nicely. And then we'll just add something just like we did before, which does the labeling and passes those to a processed item list to grab the labels. And then passes the inputs and outputs to our constructor to give us our labeled data. So that's basically it. So with that, we have a label by function where we can create our category processor. We can label the training set. We can label the validation set. And we can return the result, the split data result. So the main thing to notice here is that when we say train equals labeled data dot label passing in this processor, this processor has no vocab. So it goes to that bit we saw. It says, oh, there's no vocab. So let's create a list of all the unique possibilities. On the other hand, when it goes to the validation set, proc now does have a vocab. So it will skip that step and use the training set's vocab. So this is really important, right? People get mixed up by this all the time in machine learning and deep learning. It's like very often when somebody says, my model's no better than random, the most common reason is that they're using some kind of different mapping between their training set and their validation set. So if you use a process like this, that's never going to happen because you're ensuring that you're always using the same mapping. So the details of the code aren't particularly important. The important idea is that your labeling process needs to include some kind of processor idea. And if you're doing this stuff manually, which basically every other machine learning and deep learning framework does, you're asking for difficult to fix bugs because any time your computer's not doing something for you, it means you have to remember to do it yourself. So whatever framework you're using, I don't know if any other frameworks have something quite like this. So create something like this for yourself so that you don't have that problem. All right, let's go. In the case of online streaming data, how do you deal with having new categories in the test set that you don't see in training? Yeah, great question. It's not just online streaming data. It happens all the time. You do inference either on your validation set or test set or in production where you see something you haven't seen before. For labels, it's less of a problem in inference because for inference, you don't have labels by definition. But you could certainly have that problem in your validation set. So what I tend to like to do is if I have something where there's lots and lots of categories and some of them don't occur very often and I know that in the future there might be new categories appearing, I'll take the few least common and I'll group them together into a group called other. And that way I now have some way to ensure that my model can handle these rare other cases. Something like that tends to work pretty well. But you do have to think of it ahead of time. For many kinds of problems, you know that there's a fixed set of possibilities. And if you know that it's not a fixed set, yeah, I would generally try to create an other category with a few examples. So make sure you train with some things in that other category. In the label data class, what is the class method decorator doing? Sure. So I'll be quick because you can Google it, but basically this is the difference between an instance method and a class method. So you'll see it's not getting passed self. So you'll see that I am not going to call this on an object of type label data, but I'm calling it on the label data class itself. So it's just a convenience really, class methods. The thing that they get passed in is the actual class that was requested. So I could create a subclass of this and then ask for that subclass. So anyway, they're called class methods. You should Google them. Pretty much every language supports class methods or something like it. They're pretty convenient. You can get away without them, but they're pretty convenient. Great. So now we've got our labeled list. And if we print it out, it's got a training set and a validation set, and each one has an X and a Y. Our category items are a little less convenient than the Fast AI version 1 ones, because the Fast AI ones will actually print out the name of each category. We haven't done anything to make that happen. So if we want the name of each category, we would actually have to refer to the.obsh, which you can see we're doing here. Y.obsh or Y.obsh with a slice. So in Fast AI version 1, there's one extra thing we have, which is this concept of an item base, and you can actually define things like category items that know how to print themselves out. Whether that convenience is worth the extra complexity is up to you if you're designing something similar yourself. So we still can't train a model with these, because we have pillow objects. We need tenses. So here's our labeled list, training set, zeroth object, and that has an X and a Y, so the zeroth thing in that tuple is the X. If they're all going to be in the batch together, they have to be the same size, so we can just go dot resize, no problem. I mean, that's not a great way to do it, but it's a start. So here's a transform that resizes things, and it has to be after all the other transforms we've seen so far, because we want conversion to RGB to happen beforehand probably, stuff like that. So we'll give this an order of 10, and this is something you pass in a size. If you pass in an integer, we'll turn it into a tuple, and when you call it, it'll call resize, and it'll do bilinear resizing for you. So there's a transform. Once you've turned them all into the same size, then we can turn them into tenses. I stole this from TorchVision. This is how TorchVision turns pillow objects into tenses, and this has to happen after the resizing, so we'll give this a lesser order. And you see these two ways here of adding kind of class level state or transform level state. I can actually attach state to a function. This is really underused in Python, but it's super handy, right? We've got a function, we just want to say, like, what's the order of the function, or we can put it in the class. And then that's turned it into a byte tensor. We actually need a float tensor, so here's how you turn it into a float, and we don't want it to be between 0 and 255. We want it between 0 and 1, so we'll divide it in place by 255. And that has to happen after it's a byte, so we'll give that a higher order again. So now here's our list of transforms. It doesn't matter what order they're in the array, because they're going to order them by the underscore order attribute. So we can pass that to our image list. We can split it. We can label it. Here's a little convenience to permute the order back again. I don't know if you noticed this, but in to byte tensor I had to permute 201, because pillow has the channel last, where else PyTorch assumes the channel comes first. So this is just going to pop the channel first. So to print them out, we have to put the channel last again. So now we can grab something from that list and show image. Here it is, and you can see that it is something of a torch thing of this size. So that's looking good. So we now have tensors that are floats and are all the same size, so we can train a model. So we've got a batch size. We'll use the get data loaders we had before. We can just pass in train invalid directly from our labeled list. Let's grab a mini batch, and here it is, 64 by 3 by 128 by 128. And we can have a look at it, and we can see the vocab for it. We can see the whole mini batch of Y values. So now we can create a data bunch. That's going to have our data loaders. And to make life even easier for the future, let's add two optional things, channels in and channels out, and that way any models that want to be automatically created can automatically create themselves with the correct number of inputs and the correct number of outputs for our data set. And let's create an add to our split data, something called to data bunch, which is just this function. It just calls that get DLs we saw before. So like in practice, in your actual module, you would go back and you would paste the contents of this back into your split data definition. But this is kind of a nice way when you're just iteratively building stuff, you can't only monkey patch PyTorch things or standard library things, you can monkey patch your own things. So here's how you can add something to a previous class when you realize later that you want it. So let's go through and see what happens. So here are all the steps, literally all the steps. Grab the path, untar the data, grab the transforms, grab the item list, pass in the transforms, split the data using the grandparent, using this validation name, label it using parent labeler, and then turn it into a data bunch with this batch size, three channels in, ten channels out, and we'll use four processes. Here's our callback functions from last time. Let's make sure that we normalize. In the past, we've normalized things that have had only one channel being MNIST. Now we've got three channels, so we need to make sure that we take the mean over the other channels so that we get a three channel mean and a three channel standard deviation. So let's define a function that normalizes things that are three channels. So we're just broadcasting here. So here's the mean and standard deviation of this ImageNet batch. So here's a function called norm ImageNet, which we can use from now on to normalize anything with this data set. So let's add that as a callback using the batch transform we built earlier. We will create a conv net with this number of layers. And here's the conv net. We're going to come back to that. And then we will do our one cycle scheduling using cosine one cycle annealing, pass that into our get learn run, and train. And that's going to give us 72.6%, which if we look at the ImageNet leaderboard for 128 pixels for I5 epochs, the best is 84.6 so far. So this is looking pretty good. We're very much on the right track. So let's take a look and see what model we built. Because it's kind of interesting. There's a few interesting features of this model, and we're going to be looking at these features quite a lot in the next two lessons. The model knows how big its first layer has to start out, because we pass in data, and data has the channels in. So this is nice. Already this is a model which you don't have to change its definition if you have hyperspectral imaging with four channels, or you have black and white with one channel, or whatever. So this is going to change itself. Now what's the second layer going to be? Or I should say what's the output of the first layer going to be? The input's going to be CN. What's the output going to be? Is it going to be 16, 32, 64? Well, what we're going to do is we're going to say, well, our input has, we don't know, some number of channels. But we do know that the first layer is going to be a 3 by 3 kernel, and then there's going to be some number of channels, CN channels, which in our case is 3. So as the convolution kernel kind of scrolls over the input image, at each time the number of things that it's multiplying together is going to be 3 by 3 by CN. So 9 by CN. So remember we talked about this last week, right? We basically want to put that, we basically want to make sure that our first convolution actually has something useful to do. So if we're getting 9 by CN coming in, you wouldn't want more than that going out, because it's basically a wasted time. So we discussed that briefly last week. So what I'm going to do is I'm going to say, okay, let's take that value, CN by 3 by 3, and let's just look for the next largest number that's a power of 2, and we'll use that. So then that's how I do that. And then I'll just go ahead and multiply by 2 for each of the next two layers. So this way we've got these vital first three layers are going to work out pretty well. So back in the old days, we used to use 5 by 5 and 7 by 7 kernels, okay? We'd have the first layer would be one of those. But we know now that's not a good idea. Still most people do it, because people stick with what they know. But when you look at the bag of tricks for image classification paper, which in turn refers to many previous citations, many of which are state of the art and competition winning models, the message is always clear. Three by three kernels give you more bang for your buck. You get deeper. You end up with the same receptive field. It's faster, because you've got less work going on, right? And really this goes all the way back to the classic Zeiler and Fergus paper that we've looked at so many times over the years that we've been doing this course. And even before that to the VGG paper, it really is three by three kernels everywhere. So any place you see something that's not a three by three kernel, have a big think about whether that makes sense. Okay. So that's basically what we have for those critical first three layers. That's where that initial feature representation is happening. And then the rest of the layers is whatever we've asked for. And so then we can build those layers up, just saying number of filters in to number of filters out for each filter. And then as usual, average pooling, flatten, and a linear layer to however many classes is in our data. That's it. It's very hard to... Every time I write something like this, I break it the first 12 times. And the only way to debug it is to see exactly what's going on. To see exactly what's going on, you need to see that what module is there at each point, and what is the output shape at each module. So that's why we've created this model summary. So model summary is going to use that get batch that we added in the LSUV notebook to grab one batch of data. We will make sure that that batch is on the correct device. We will use the find modules thing that we used in the LSUV to find all of the places that there's a linear layer. If you said find all, otherwise we will grab just the immediate children. We will grab a hook for every layer using the hooks that we made. And so now we can pass that model through. And the function that we've used for hooking simply prints out the module and the output shape. That's how easy it is to create this wonderfully useful model summary. So to answer your question of earlier, another reason like why are we doing this or what are you meant to be getting out of it, is to say like you don't have to write much code to create really useful tools and telemetry. So we've seen how to create per layer histogram viewers, how to create model summaries. With the tools that you have at your disposal now, I really hope that you can dig inside your models what they are and what they're doing. And you see that it's all about hooks. So this hooks thing we have is just super, super useful. I'm very grateful to the PyTorch team for adding this fantastic functionality. So you can see here we start. The input is 128 because that's the batch size, 128 by 3 by 128 by 128. And then we gradually go through these convolutions. The first one has a stride of 1. The next two have a stride of 2. So that goes 64, 32. And you can see after each one, they have a stride of 2, gets smaller and smaller. And then an average pull that to a 1 by 1, and then we flatten it, and then we have a linear. So that's, it's a really, it's like as basic a conv net as you could get. It really is. It's just a bunch of 3 by 3 conv value batch norms. But it does terrifically well. You know, it's pretty, it's deep enough. So I think that's a good start. All right. I think that's a good time to take a break. So let's come back at 7.45. This is one of the bits I'm most excited about in this course, actually. But hopefully it's gonna be like totally unexciting to you, because it's just gonna be so obvious that you should do it this way. But the reason I'm excited is that we're gonna be talking about optimizers. And anybody who's done work with kind of optimizers in deep learning in the past will know that every library treats every optimizer as a totally different thing. So there's an atom optimizer, like in PyTorch, there's an atom optimizer and a SGD optimizer and an RMS prop optimizer. And somebody comes along and says, hey, we've invented this thing called decoupled weight decay, also known as Adam W. And the PyTorch folks go, oh, damn, what are we gonna do? And they have to add a parameter to every one of their optimizers, and they have to change every one of their optimizers. And then somebody else comes along and says, oh, we've invented a thing called AMS grad. There's another parameter we have to put into any one of those optimizers. And it's not just inefficient and frustrating, but it holds back research, because it starts feeling like there are all these things called different kinds of optimizers, but there's not. I'm gonna show you there's not. There's one optimizer. And there's one optimizer in which you can inject different pieces of behavior in a very, very small number of ways. And what we're gonna do is we're gonna start with this generic optimizer, and we're gonna end up with this. This came out last week, and it's a massive improvement, as you see, in what we can do with natural language processing. This is the equation set that we're gonna end up implementing from the paper. And what if I told you that not only I think are we the first library to have this implemented, but this is the total amount of code that we're gonna write to do it. So that's where we're going. So we're gonna continue with ImageNet, and we're gonna continue with the basic set of transforms we had before, and the basic set of stuff to create our data bunch. This is our model, and this is something to pop it on CUDA to get our statistics written out to do our batch transform with the normalization. And so we're gonna start here, 52% after an epoch. And so let's try to create an optimizer. Now in PyTorch, the base thing called optimizer is just a dictionary that stores away some hyperparameters, and we've actually already used it. And I deeply apologize for this. We cheated. We used something that is not part of our approved set of foundations without building it ourselves, and we did it here. We never wrote param groups. We never wrote param groups. So we're gonna go back and do it now, right? Because the reason we did this is because we were using Torch's optim.optimizer. We've already built the main part of that, which is the thing that multiplies by the learning rate and subtracts from the gradients. But we didn't build param groups. So let's do it here. So here's what's gonna happen. As always, we need something called zero grad, which is gonna go through some parameters and zero them out and also remove any gradient computation history. And we're gonna have a step function that does some kind of step. The main difference here, though, is our step function isn't actually gonna do anything. It's going to use composition on some things that we pass on and ask them to do something. So this optimizer's gonna do nothing at all until we build on top of it. But we're gonna set it up to be able to handle things like discriminative learning rates and one-cycle annealing and stuff like that. And so to be able to do that, we need some way to create parameter groups. This is what we call in Fast AI layer groups. And I kind of wish I hadn't called them layer groups. I should call them parameter groups because we have a perfectly good name for them already in PyTorch. So I'm not gonna call them layer groups anymore. I'm just gonna call them parameter groups. But it's the same thing, okay? Parameter groups and layer groups. So a parameter group, so remember what is in, when we say parameters in PyTorch, remember right back to when we've created our first linear layer, we had a weight tensor and we had a bias tensor. And each one of those is a parameter, right? It's a parameter tensor. So in order to optimize something, we need to know what all the parameter tensors are in a model. And you can just say model.parameters to grab them all in PyTorch. And that's gonna give us, it gives us a generator, but as soon as you call list on a generator, it turns it into an actual list. So that's gonna give us a list of all of the tensors, all of the weights and all of the biases basically. But we might wanna be able to say, the last two layers should have a different learning rate to all the other layers. And so the way we can do that is rather than just passing in a list of parameters, we'll pass in a list of lists. And so let's say our list of lists has two items. The first item contains all the parameters in the main body of the architecture and the last item contains just the parameters from the last two layers. So if we make this, decide that this is a list of lists, then that lets us do parameter groups. Now that's how we tell the optimizer these sets of parameters should be handled differently with discriminative learning rates and stuff. And so that's what we're gonna do. We're gonna assume that this thing being passed in is a list of lists. But we won't quite assume, we'll check. If it's not, then we'll turn it into a list of lists by just wrapping it in a list. So if it only has one thing in it, we'll just make it a list with one item containing a list. So now, param groups is a list of lists of parameter tensors. And so you could either pass in, so you could decide how you wanna split them up into different parameter groups, or you could just have them turn into a single parameter group for you. So that's the first thing we need. So now we have, our optimizer object has a param groups attribute containing our parameter groups. So just keep remembering that's a list of lists. All right. Each parameter group can have its own set of hyperparameters. So hyperparameters could be learning rate, momentum, beta in atom, epsilon in atom, and so forth. So those hyperparameters are gonna be stored as a dictionary. And so there's gonna be one dictionary for each parameter group. So here's where we create it. Self.hypers contains for each parameter group a dictionary. And what's in the dictionary? What's in the dictionary is whatever you pass to the constructor. Okay? So this is how you just pass a single bunch of keyword arguments to the constructor, and it's gonna construct a dictionary for every one. And this is just a way of cloning a dictionary so that they're not all referring to the same reference, but they all have their own reference. All right. So that's doing much the same stuff as torches.optim.optimizer, and here's the new bit, stepper. In order to see what a stepper is, let's write one. Here's a stepper. It's a function. It's called SGD step. What does it do? It does the SGD step. We've seen it before. So in other words, to create an SGD optimizer, we create a partial with our optimizer with the steppers being SGD step. So now when we call step, it goes through our parameters, composes together our steppers, which is just one thing, right, and calls the parameter. So the parameter is gonna go p.data.add minus learning rate p.grad.data. So that's how we can create SGD. So with that optimization function, we can fit. It's not doing anything different at all, right? But what we have done is we've done the same thing we've done a thousand times without ever creating an SGD optimizer. It's an optimizer with an SGD step. I've created this thing called gradparams, which is just a little convenience. Basically when we like zero the gradients, we have to go through every parameter. To go through every parameter, we have to go through every parameter group, and then within each parameter group, we have to go through every parameter in that group where the gradient exists. They're the ones that we have to zero. And ditto for when we do a step. That's why I just refactored it. And also when we call the stepper, we want to pass to it all of our hyperparameters, right, because the stepper might want them. Like it'll probably want learning rate, right, and learning rate's just one of the things that we've listed in our hyperparameters. So remember how I said that our compose is a bit special, that it passes along any keyword arguments it got to everything that it composes? Here's a nice way to use that, right? So that's how calm SGD step can say, oh, I need the learning rate. And so as long as hyper has a learning rate in it, it's going to end up here. And it'll be here as long as you passed it here. And then you can change it for each different layer group. You can anneal it and so forth. So we're going to need to change our parameter scheduler to use our new generic optimizer. It's simply now that we have to say go through each hyperparameter in self.opt.hypers and schedule it. So that's basically the same as what we had in parameter scheduler before, but for our new thing. And ditto for recorder, this used to use param groups, now it uses hypers. So a minor change to make these keep working. So now I was super excited when we first got this working. So it's like, wow, we've just built an SGD optimizer that works without ever writing an SGD optimizer. So now when we want to add weight decay, right? So weight decay, remember, is the thing where we don't want something that fits this. We want something that fits this. And the way we do it is we use L2 regularization, which just is we add the sum of squared's weights times some parameter we choose. And remember that the derivative of that is actually just WTD times weight. So you could either add an L2 regularization to the loss, or you can add WD times weight to the gradients. If you've forgotten this, go back and look at weight decay in part one to remind yourself. And so if we want to add either this or this, we can do it. We can add a stepper. So weight decay is going to get an LR and a WD, and it's going to simply do that. There it is. OK. Or L2 regularization is going to just do that. By the way, if you haven't seen this before, add in PyTorch. Normally it just adds this tensor to this tensor. But if you add a scalar here, it multiplies these together first. This is a nice fast way to go WD times parameter and add that to the gradient. So there's that. OK. So we've got our L2 regularization. We've got our weight decay. What we need to be able to do now is to be able to somehow have the idea of defaults, because we don't want to have to say weight decay equals zero every time we want to turn it off. So see how we've attached some state here to our function object. So the function now has something called defaults that says it's a dictionary with WD equals zero. So let's just grab exactly the same optimizer we had before. But what we're going to do is we're going to maybe update our defaults with whatever self.steppers has in their defaults. And the reason it's maybe update is that it's not going to replace. If you explicitly say, I want this weight decay, it's not going to update it. It'll only update it if it's missing. And so that's just what this little loop does. Just goes through each of the things and then goes through each of the things in the dictionary. And it just checks if it's not there, then it updates it. So everything else here is exactly the same as before. So now we can say, let's create an SGD optimizer. It's just an optimizer with a SGD step and weight decay. And so let's create a learner. And let's try creating an optimizer, which is an SGD optimizer, with our model's parameters, with some learning rate, and make sure that the hyperparameter for weight decay should be 0. The hyperparameter for LR should be 0.1. Yep, it passes. Let's try giving it a different weight decay. Make sure it's there. OK, it passes as well. So we've now got an ability to basically add any step functions we want. And those step functions can have their own state that gets added automatically to our optimization object. And we can go ahead and fit. So that's fine. So now we've got an SGD optimizer with weight decay is one line of code. Let's now add momentum. So momentum is going to require a slightly better optimizer or a slightly different optimizer, because momentum needs some more state. It doesn't just have parameters and hyperparameters. But also momentum knows that for every set of activations, it knows what were they updated by last time. Because remember, the momentum equation is, if momentum is 0.9, then it would be 0.9 times whatever you did last time plus this step. So we actually need to track for every single parameter what happened last time. And that's actually quite a bit of state, right? If you've got 10 million activations in your network, you've now got 10 million more floats that you have to store, because that's your momentum. So we're going to store that in a dictionary called state. So a stateful optimizer is just an optimizer that has state. And then we're going to have to have some stats. And stats are a lot like steppers. They're objects that we're going to pass in to say, when we create this state, how do you create it? So when you're doing momentum, what's the function that you run to calculate momentum? So that's going to be called something of a stat class. So for example, momentum is calculated by simply averaging the gradient like so. We take whatever the gradient averaged before, we multiply it by momentum, and we add the current gradient. That's the definition of momentum. So this is an example of a stat class. So it's not enough just to have update, because we actually need this to be something at the start. We can't multiply by something that doesn't exist. So we're also going to define something called init state that will create a dictionary containing the initial state. So that's all that stateful optimizer is going to do, right? It's going to look at each of our parameters, and it's going to check to see whether that parameter already exists in the state dictionary. And if it doesn't, it hasn't been initialized. So we'll initialize it with an empty dictionary, and then we'll update it with the results of that init state call we just saw. So now that we have every parameter can now be looked up in this state dictionary to find out its state, and we can now therefore grab it, and then we can call update like so. Well, this one's not opening, like so, to do, for example, average gradients. And then we can call compose with our parameter and our steppers. And now we don't just pass in our hyperparameters, but we also pass in our state. So now that we have average gradients, which is sticking it into this thing called grad average, and it's going to be passed into our steppers, we can now do a momentum step. And the momentum step takes not just LR, but it's now going to be getting this grad average. And here is the momentum step. It's just this grad average times the learning rate. That's all you do. So now we can create an SGD with momentum optimizer with a line of code. It can have a momentum step. It can have a weight decay step. It can have an average grad stat. We can even give it some default weight decay. And away we go. So here's something that might just blow your mind. Let me read it to you. Here is a paper. L2 regularization versus batch and weight norm. Batch normalization is a commonly used trick to improve training of deep neural networks. And they also use L2 regularization ostensibly to prevent overfitting. However, we show that L2 regularization has no regularizing effect. What? Okay. It's true. Watch this. I realized this when I was chatting to Sylvain at NeurIPS, and like we were walking around the poster session, and I suddenly said to him, wait, Sylvain, if there's batch norm, how can L2 regularization possibly work? And I'll tell you what I laid out to him. This is before I discovered this paper. We've got some layer of activations, right? And some layer. And we've got some weights that was used to create that layer of activations. So these are our weights, and these are our activations. And then we pass it through some batch norm layer, right? And the batch norm layer does two things. It's got a bunch of adds, and it's got a bunch of multipliers, right? It also normalizes, but these are the learned parameters. Okay. So we come along and we say, okay, weight decay time, your weight decay is a million. And it goes, uh-oh, what do I do? Because now the squared of these, the sum of the squares of these gets multiplied by 1E6. My loss function's destroyed. I can't possibly learn anything. But then the batch norm layer goes, oh no, don't worry, friends. And it fills every single one of these with 1 divided by a million. Okay? So what just happened? Well, no, sorry, it multiplies by positive. Sorry, it multiplies them all by a million. So what now happens? Oh, these now have to get the same activations we had before. All of our weights, so like W1, now have to get divided by a million to get the same result. And so now our weight decay basically is nothing. So in other words, we can decide exactly how much weight decay loss there is. By simply using the batch norm malts. Now the batch norm malts get a tiny bit of weight decay applied to them, unless you turn it off, which people often do. But it's tiny, right? Because there's very few parameters here, and there's lots of parameters here. So it's true. It's true. L2 regularization has no regularizing effect. Which is not what I've been telling people who have been listening to these lessons the last three years, for which I apologize. I was wrong. I feel a little bit better in knowing that pretty much everybody in the community is wrong. We've all been doing it wrong. So Tuan Van Laahovan mentioned this in the middle of 2017. Basically nobody noticed. There's a couple more papers I've mentioned in today's lesson notes from the last few months where people are finally starting to really think about this. But I'm not aware of any other course which has actually pointed out we're all doing it wrong. So you know how I keep mentioning how none of us know what we're doing? We don't even know what L2 regularization does because it doesn't even do anything. But it does do something. Because if you change it, something happens. So this guy's wrong too. It doesn't do nothing. So a more recent paper by a team led by Roger Gross has found three kind of ways in which maybe regularization happens but it's not the way you think. This is one of the papers in the lesson notes. But even in his paper which is just a few months old, the abstract says basically, or the introduction says basically no one really understands what L2 regularization does. So we have no idea what we're doing. There's this thing that every model ever always has and it totally doesn't work. At least it doesn't work in the way we thought it did. So that should make you feel better about can I contribute to deep learning? Obviously you can because none of us have any idea what we're doing. And this is a great place to contribute. Use all this telemetry that I'm showing you, activations of different layers, and see what happens experimentally. Because the people who study this stuff, like what actually happens with batch norm and rate decay, most of them don't know how to train models. They're like the theory people. And then there's the practitioners who forget about actually thinking about the foundations at all. But if you can combine the two and say like, oh, let's actually try some experiments. Let's see what happens really when we change weight decay now that I assume we don't know what we're doing. I'm sure you can find some really interesting results. So momentum is also interesting. And we really don't understand much about how things like momentum work. But here's some nice pictures for you. And hopefully it'll give you a bit of a sense of momentum. Let's create 200 numbers equally spaced between minus 4 and 4. And then let's create another 200 random numbers that average 0.3. And then let's create something that plots some function for these numbers. And we're going to look at this function for each value of something called beta. And this is the function we're going to try plotting. And this is the momentum function. Okay. So what happens if we plot this function for each value of beta for our data where the y is random and average is 0.3? So beta here is going to be our different values of momentum. And you can see what happens is with very little momentum, you just get very bumpy, very bumpy. Once you get up to a high momentum, you get a totally wrong answer. Why is this? Because if you think about it, right, we're constantly saying 0.9 times whatever we had before plus the new thing. Then basically you're continuing to say like, oh, the thing I had before times 0.9 plus the new thing and the things are all above 0. So you end up with a number that's too high. And this is why if your momentum is too high and basically you're way away from where you need to be in weight space. So it keeps on saying go that way, go that way, go that way. If you get that enough with a high momentum, it will literally shoot off far faster than is reasonable. Okay. So this will give you a sense of why you've got to be really careful with high momentums. It's literally biased to end up being a higher gradient than the actual gradient. So we can fix that. Like when you think about it, this is kind of dumb, right? Because we shouldn't be saying beta times average plus yi. We should be saying beta times average plus 1 minus beta times the other thing. So like dampen the thing that we're adding in. And that's called an exponentially weighted moving average, as we know. Or LERP in PyTorch speak. So let's plot the same thing as before, but this time with exponentially weighted moving average. Ah, perfect. Okay. So we're done, right? Not quite. Not quite.What if the thing that we're trying to match isn't just random, but is some function? So it looks something like this. Well, if we use a very small momentum with exponentially weighted moving averages, we're fine. And I've added an outlier at the start just to show you what happens. Even with beta 0.7, we're fine. But uh-oh, now we've got trouble. And the reason we've got trouble is that the second, third, fourth, fifth observations all have a whole other lot of this item number one in, right? Because remember item number two is 0.99 times item number one plus 0.01 times item number two. And so item number one is massively biasing the start. Even here. It takes a very long time. And the second thing that goes wrong is with this momentum is that you see how we're a bit to the right of where we should be? We're always running a bit behind where we should be. Which makes perfect sense, right? Because we're always only taking 0.1 times the new thing. So we can use de-biasing. De-biasing is what we saw last week. And it turned out, thanks to Stas Beckman's discovery, we didn't really need it. But we do need it now. And de-biasing is to divide by 1 minus beta to the power of whatever batch number we're up to. So you can kind of tell, right? If your initial starting point is 0, and that's what we use always when we're de-biasing. We always start at 0. And beta is 0.9. Then your first step is going to be 0.9 times 0 plus 0.1 times your item. So in other words, you'll end up at 0.1 times your item. So you're going to end up 10 times lower than you should be. So you need to divide by 0.1. And if you kind of work through it, you'll see that each step is simply 0.1 to the power of 1, 2, 3, 4, 5, and so forth. And in fact, we have, of course, a spreadsheet showing you this. So if you have a look at the momentum bias spreadsheet, there we go. So basically, here's our batch number. And let's say these are the values that are coming in, our gradients, 5, 1, 1, 1, 1, 1, 5, 1. Then basically, this is our exponentially weighted moving average. And here is our debiasing correction. And then here is our resulting debiased exponentially weighted moving average. And then you can compare it to an actual moving average of the last few. So that's basically how this works. And Sylvain loves writing LaTeX. So he wrote all this LaTeX that basically points out that if you say what I just said, which is beta times this plus 1 minus beta times that, and you keep doing it to itself lots and lots of times, you end up with something that they all cancel out to that. So this is all we need to do to take our exponentially weighted moving average, divide it by 1 minus beta to the power of i plus 1. And look at that. It's pretty good, right? It debiases very quickly, even if you have a bad starting point. And it looks pretty good. It's not magic, right? But you can see why a beta of 0.9 is popular. It's kind of got a pretty nice behavior. So let's use all that to create Adam. So what's Adam? Adam is dampened debiased momentum. That's the numerator. Dampened by dampened debiased root sum of squared gradients. And so we talked about why Adam does that before. We won't go into the details. But here's our average gradient again. But this time we've added optional dampening. So if you say I want dampening, then we'll set momentum dampening to that. Otherwise, we'll set it to 1. And then so this is exactly the same as before, but with dampening. Average squared gradients is exactly the same as average gradients. We could definitely refactor these a lot. So this is all exactly the same as before, except we'll call them different things. We'll call it squared dampening. We'll call it squared averages. And this time, rather than just adding in the p grad data, we will multiply p grad data by itself. In other words, we get the squids. This is the only difference. We throw it in a different name. So with those, we're also going to need to debias, which means we need to know what step we're up to. So here's a stat which just literally counts. So here's our debias function, the one we just saw. And so here's Adam. Right? Once that's in place, Adam is just the debiased momentum with momentum dampening, the debiased squared momentum with squared momentum dampening, and then we just take the parameter and then our learning rate, and we've got the debiasing here, our gradient average, and divided by the squared. And we also have our epsilon. Oh, this is in the wrong spot. Be careful. Epsilon should always go inside the square root. So, that's an Adam step. So now we can create an Adam optimizer in one line of code. And so there's our Adam optimizer. It has average grads. It's got average squared grads. It's got a step. And we can now try it out. Okay? So, here's Lam. By the way, these equations are a little nicer than these equations, and I want to point something out. Mathematicians hate refactoring. Don't be like them. Look at this. M over V plus epsilon root lambda, it's the same as this. So like, it's just so complicated when things appear the same way in multiple places. So when we did this equation, we gave that a new name. And so now we can just look at R2 goes from all that to just that. And Wt goes from all that to just that. And so when you pull these things out, when you refactor your math, it's much easier to see what's going on. So here's the cool thing, right? When we look at this, even if you're a terrible mathematician like me, you're going to start to recognize some patterns. And that's the trick to being a less terrible mathematician is recognizing patterns. Beta times something plus 1 minus beta times another thing is exponentially weighted moving average, right? So here's one exponentially weighted moving average. Here's another exponentially weighted moving average. This one has a gradient. This one has gradient squared. This means element-wise multiplication. So these are the exponentially weighted moving average of the gradient and the gradient squared. Oh, beta to the t, de-biasing. So that's the de-biased version of m. There's the de-biased version of v. Not to move the epsilon? Really? Sylvan has a message. Don't... Sylvan has a message, don't move the epsilon. Don't listen to Jeremy. Don't listen to Jeremy. Okay. Sylvan is an actual math guy. So...In an atom, the epsilon goes outside the square root. No way. I always thought epsilon should always go inside the square root. Jeremy just did a fix I pushed a week ago where our atom wasn't working. Let's press control Z a few times. There we go. That's great. So to explain why this matters and why there is no right answer, here's the difference. So if epsilon is 1e-7, then having it here versus having it here. So like the square root of 1e-7 is very different to 1e-7. And in batch norm, they do put it inside the square root. And according to Sylvan and Adam, they don't. Neither is like the right place to put it or the wrong place to put it. If you don't put it in the same place as they do in the paper, it's just a totally different number. And this is a good time to talk about epsilon and atom. Because I love epsilon and atom. Because like what if we put made epsilon equal to 1, right? And we've got the kind of momentum term on the numerator. And the denominator, we've got the root sum of squares of the root of the exponentially weighted average squared gradients. So we're dividing by that plus 1. And most of the time, the gradients are going to be smaller than 1. And the squared version is going to be much smaller than 1. So basically then, the 1 is going to be much bigger than this. So it basically makes this go away. So if epsilon is 1, it's pretty close to being standard SGD with momentum, or at least debiased, dampened momentum. Where else is epsilon is 1e-7? Then we're basically saying, oh, we want to really use these different exponentially weighted moving average squared gradients. And this is really important. Because if you have some activation that has had a very small squared gradients for a while, this could well be like 1e-6. Which means when you divide by it, you're multiplying by a million. And that could absolutely kill your optimizer. So the trick to making atom and atom-like things work well is to make this about 0.1, somewhere between 1e-3 and 1e-1 tends to work pretty well. Most people use 1e-7, which just makes no sense. There's no way that you want to be able to multiply your step by 10 million times. That's just never going to be a good idea. So it's another place that epsilon is a super important thing to think about. So LAM, then, is stuff that we've all seen before. So it's debiased. This is atom. Just exponentially weighted moving averages of gradients and gradients squared. This here is the norm of the weights. The norm is just the sum of the roots of the squares. So this is just weight decay. So LAM has weight decay built in. This one here, hopefully you recognize as being the atom step. And so this is the norm of the atom step. So basically what LAM is doing is it's atom, but what we do is we average all the steps over a whole layer. That's why these L's are really important, because these things are happening over a layer. And so basically we're taking, so here's our d by cementum, d by squared momentum, and then here's our 1. And look, here's this mean. So it's for a layer, because remember each stepper is created for a layer, for a parameter. I shouldn't say a layer, for a parameter. So this is kind of both exciting and annoying, because I had been working on this exact idea, which is basically atom, but averaged out over a layer for the previous week. And then this LAM paper came out, and I was like, oh, that's cool. Some paper about BERT training. I'll check it out. And it's like, oh, we do it with a new optimizer. And I looked at the new optimizer, it's like, it's just the optimizer I wrote a week before we were going to present it. So I'm thrilled that this thing exists. I think it's exactly what we need. And you should definitely check out LAM, because it makes so much sense to use the average over the layer of that step as a kind of a, you can see here it's kind of got this normalization going on. Because it's just really unlikely that every individual parameter in that tensor, you don't want to divide it by its squared gradients, because it's going to vary too much. There's just too much chance that there's going to be a 1e neg 7 in there somewhere or something. Right? So this to me is exactly the right way to do it. And this is kind of like the first optimizer I've seen, where I just kind of think like, oh, finally I feel like people are heading in the right direction. But when you really study this optimizer, you'll realize that everything we thought about optimizers kind of doesn't make sense. The way optimizers are going with things like LAM is the whole idea of like, what is the magnitude of our step? It just looks very different to everything we kind of thought of before. So check out this paper. This path might look slightly intimidating at first, but now you know all of these things. What they all are and you know why they exist. So I think you'll be fine. So here's how we create a LAM optimizer, and here's how we fit with it. Okay. That is that. Unless Sylvia says otherwise. All right. So as I was building this, I got so sick of runner, because I kept on wondering when do I pass the runner, when do I pass the learner? And then I kind of suddenly thought, again, once every month or two, I actually sit and think. And it's only when I get really frustrated. So I was getting really frustrated with runners, and I actually decided to sit and think. And I looked at the definition of learner, and I thought, wait, it doesn't do anything at all. It stores three things. What kind of class just stores three things? And then a runner has a learner in it that stores three things. But why don't we store the three things in the runner? So I took the runner, I took that line of code, I copied it, and I pasted it just here. I then renamed runner to learner. I then found everything that said self.learn and removed the.learn, and I was done. And now there's no more runner. And it's like, oh, it's just one of those obvious refactorings that as soon as I did it, Sylvan was like, why didn't you do it that way in the first place? And I was like, why didn't you fix it that way in the first place? But now that we've done it, this is so much easier. There's no more get learn run, there's no more having to match these things together. It's just super simple. So one of the nice things I like about this kind of Jupyter style of development is I spend a month or two just immersing myself in the code in this very experimental way, and I feel totally fine throwing it all away and changing everything, because everything's small and I can fiddle around with it. And then after a couple of months, Sylvan and I will just go like, okay, there's a bunch of things here that work nicely together, and we turn it into some modules. And so that's how Fast AI version 1 happened. And people often say to us like, oh, turning it into modules, what a nightmare that must have been. So here's what was required for me to do that. I typed into Skype, Sylvan, please turn this into a module. So that was pretty easy. And then three hours later, Sylvan typed back and he said, done. It was three hours of work. It was four, five, six months of development in notebooks, three hours to convert it into modules. So it's really, it's not a hassle. And I find this quite delightful. It works super well. So no more runner. Thank God. Runner is now called learner. We're kind of back to where we were. We want progress bars. So Sylvan wrote this fantastic package called Fast Progress, which you should totally check out. And we're allowed to import it, because remember, we're allowed to import modules that are not data science modules. Progress bar is not a data science module. But now we need to attach this progress bar to our callback system. So let's grab our ImageNet data as before, create a little thing with, I don't know, 432 filled layers. Let's rewrite our stats callback. It's basically exactly the same as it was before, except now we're storing our stats in an array. And we're just passing off the array to logger. Remember logger is just a print statement at this stage. And then we will create our progress bar callback. And that is actually the entirety of it. That's all we need. So with that, we can now add progress callback to our callback functions and grab our learner. No runner. Fit. Now that's kind of magic, right? That's all the code we needed to make this happen. And look at the end, oh, creates a nice little table. Pretty good. So this is thanks to just careful, simple, decoupled software engineering. We just said, okay, when you start fitting, you've got to create the master bar. So that's the thing that tracks the epochs. And then tell the master bar we're starting. And then replace the logger function not with print, but with master bar dot write. So it's going to print the HTML into there. And then after we've done a batch, update our progress bar. When we begin an epoch or begin validating, we'll have to create a new progress bar. And when we're done fitting, tell the master bar we're finished. That's it. All right. So it's very easy to, once you have a system like this, to integrate with other libraries if you want to use TensorBoard or Visdom or send yourself a Twilio message or whatever. Right? It's super easy. Okay. So we're going to finish, I think we're going to finish, unless this goes faster than I expect, with data augmentation. So so far we've seen how to create our optimizers. We've seen how to create our data blocks API. And we can use all that to train a reasonably good ImageNet model. But to make a better ImageNet model, it's a bit short of data. So we should use data augmentation as we all know. Now so let's load it in as before. And let's just grab an image list for now. Okay. And the only transforms we're going to use is resize fixed. And here's our chap with a tensh. And let's just actually open the original Pillow image without resizing it to see what he looks like full size. So here he is. And I want to point something out. When you resize, there are various resampling methods you can use. So basically when you go from one size image to another size image, do you like take the pixels and take the average of them? Or do you put a little cubic spline through them? Or what? And so these are called resampling methods, and Pillow has a few. They suggest when down sampling, so going from big to small, you should use anti-alias. So here's what you do. When you're augmenting your data, and this is like nothing I'm going to say today is really focused on vision. If you're doing audio, if you're doing text, if you're doing music, whatever, augment your data and look at or listen to or understand your augmented data. So don't just chuck this into a model, but look at what's going on. So if I want to know what's going on here, I need to be able to see the texture of this tensch. Now I'm not very good at tenches, but I do know a bit about clothes. So let's say if we were trying to see what this guy's wearing, it's a checkered shirt. So let's zoom in and see what this guy's wearing. I have no idea. The checkered shirt's gone. So I can tell that this is going to totally break my model if we use this kind of image augmentation. So let's try a few more. What if instead of anti-aliasing, we use bilinear, which is the most common. No, I still don't know what he's wearing. What if we use nearest neighbors, which nobody uses because everybody knows it's terrible. Oh, it totally works. So yeah, just look at stuff and try and find something that you can study to see whether it works. Here's something interesting though. This looks better still, don't you think? And this is interesting because what I did here was I did two steps. I first of all resized to 256 by 256 with bicubic, and then I resized to my final 128 by 128 with nearest neighbors. And so sometimes you can combine things together in steps to get really good results. Anyway, I didn't want to go into the details here. I'm just saying that when we talk about image augmentation, your test is to look at or listen to or whatever your augmented data. So resizing is very important for vision. Flipping is a great data augmentation for vision. I don't particularly care about flipping. The main thing I want to point out is this. At this point, our tensors contain bytes. Calculating with bytes and moving bytes around is very, very fast. And we really care about this because when we were doing the DawnBench competition, one of our biggest issues for speed was getting our data augmentation running fast enough. And doing stuff on floats is slow. If you're flipping something, flipping bytes is identical to flipping floats in terms of the outcome. So you should definitely do your flip while it's still a byte. So image augmentation isn't just about throwing some transformation functions in there. But think about when you're going to do it. Because you've got this pipeline where you start with bytes, and you start with bytes in a pillow thing, and then they become bytes in a tensor, and then they become floats, and then they get turned into a batch. Where are you going to do the work? And so you should do whatever you can while they're still bytes. But be careful, don't do things that are going to cause rounding errors or saturation problems, whatever. But flips, definitely good. So let's do our flips. So there's a thing called pio.x.transpose, pio.image.flipLeftRight. Let's check if a random number is less than.5. Let's create an item list. And let's replace that. Like, we built this ourselves, so we know how to do this stuff now. Let's replace the items with just the first item with 64 copies of it. And so that way, we can now use this to create the same picture lots of times. So show batch is just something that's just going to go through our batch and show all the images. Everything we're using, we've built ourselves. So you never have to wonder what's going on. So we can show batch with no augmentation, or remember how we created our transforms? We can add pio.randomFlip, and now some of them are backwards. It might be nice to turn this into a class that you actually pass a p into to decide what the probability of a flip is. You probably want to give it an order, because we need to make sure it happens after we've got the image and after we've converted it to RGB, but before we've turned it into a tensor. Since all of our pio.transforms are going to want to be that order, we may as well create a pio.transform class and give it that order. And then we can just inherit from that class every time we want a pio.transform. So now we've got a pio.transform class. We've got a pio.randomFlip. It's got this state. It's going to be random. We can try it out, giving it p of.8. And so now, yep, most of them are flipped. Okay. Or maybe we want to be able to do all these other flips. So actually, pio.transpose, you can pass it all kinds of different things, and they're basically just numbers between 0 and 6. So here are all the options. So let's turn that into another transform, where we just pick any one of those at random. And there it is. So this is how we can do data augmentation. All right. Now's a good time. It's easy to evaluate data augmentation for images. How would you handle tabular text or time series? For text, you read it. How would you handle the data augmentation? Yeah. You would read the augmented text. So if you're augmenting text, then you read the augmented text. For time series, you would look at the signal of the time series. For tabular, you would graph or however you normally visualize that kind of tabular data, you would visualize that tabular data in the same way. So you just kind of come and try and, as a domain expert, hopefully you understand your data and you have to come up with a way, you know, what are the ways you normally visualize that kind of data? And you use the same thing for your augmented data. Make sure it makes sense. Yeah. Make sure it seems reasonable. Sorry. I think I misread. How would you do the augmentation for tabular data, text or time series? How would you do the augmentation? I mean, again, it kind of requires your domain expertise. So just before class today, actually, one of our alumni, Christine Payne, came in. She's at OpenAI now working on music analysis and music generation. And she was talking about her data augmentation, saying she's like pitch shifting and volume changing and slicing bits off the front and the end and stuff like that. So there isn't an answer, you know. It's just a case of thinking about like, oh, what kinds of things could change in your data that would almost certainly cause the label to not change but would still be a reasonable data item? And that just requires your domain expertise. Oh, except for the thing I'm going to show you next, which is going to be a magic trick that works for everything. So we'll come to that. Okay. We can do random cropping. And this is, again, something to be very careful of. We very often want to grab a small piece of an image and zoom into that piece. It's a great way to do data augmentation. One way would be to crop and then resize. And if we do crop and resize, oh, we've lost his checked shirt. But very often you can do both in one step. So for example, with Pillow, there's a transform called Extent where you tell it what crop and what resize, and it does it in one step. And now it's much more clear. So generally speaking, you've got to be super careful, particularly when your data is still bytes, not to do destructive transformations, particularly multiple destructive transformations. Do them all in one go or wait until they're floats. Because bytes round off and disappear or saturate, whereas floats don't. And the cropping one takes 193 microseconds. The better one takes 500 microseconds. So one approach would be to say, oh, crap, it's more than twice as long. We're screwed. But that's not how to think. How to think is, what's your time budget? Does it matter? So here's how I thought through our time budget for this little augmentation project. I know that for Dawnbench, the best we could get down to is five minutes per batch of ImageNet on eight GPUs. And so that's 1.25 million images. So that's on one GPU per minute, that's 31,000 or 500 per second, assuming four cores per GPU, that's 125 per second. So we're going to try to stay under 10 milliseconds. I said 10 milliseconds per image. I think I mean 10 milliseconds per batch. So it's actually still a pretty small number. So we're not too worried at this point about 500 microseconds. But this is always kind of the thing to think about is how much time have you got? And sometimes these times really add up. But yeah, 520 per second, we've got some time, especially since we've got normally a few cores per GPU. So we can just write some code to do a kind of a general crop transform. For ImageNet and things like that, for the validation set, what we normally do is we grab the center of the image. We remove 14% from each side and grab the center. So we can zoom in a little bit so we have a center crop. So here we show all that. That's what we do for the validation set. And obviously they're all the same because validation set doesn't have the randomness. But for the training set, the most useful transformation by far, like all the competition winners, grab a small piece of the image and zoom into it. This is called a random resize crop. And this is going to be really useful to know about for any domain. So for example, in NLP, a really useful thing to do is to grab different sized chunks of contiguous text. With audio, if you're doing speech recognition, grab different sized pieces of the utterances and so forth. So if you can find a way to get different slices of your data, it's a fantastically useful data augmentation approach. And so this is by far the main, most important augmentation used in every ImageNet winner for the last six years or so. It's a bit weird though because what they do in this approach is this little ratio here says squish it by between a three over four aspect ratio to a four over three aspect ratio. And so it literally makes the person, see here he's looking quite thin and see here he's looking quite wide. It doesn't actually make any sense, this transformation, because optically speaking, there's no way of looking at something in normal day-to-day life that causes them to expand outwards or contract inwards. So when we looked at this, we thought, I think what happened here is that they were, this is the best they could do with the tools they had, but probably what they really want to do is to do the thing that's kind of like physically reasonable. And so the physically reasonable thing is like you might be a bit above somebody or a bit below somebody or left of somebody or right of somebody, causing your perspective to change. So our guess is that what we actually want is not this, but this. So perspective warping is basically something that looks like this. You basically have four points, right, and you think about how would those four points map to four other points if they were going through some angle. So it's like as you look from different directions, roughly speaking. And the reason that I really like this idea is because when you're doing data augmentation in any domain, as I mentioned, the idea is to try and create physically reasonable in your domain inputs. And these just aren't. Like you can't make somebody squishier in real world. But you can shift their perspective. So if we do a perspective transform, then they look like this. And this is true, right? If you're a bit underneath them, the fish will look a bit closer. Or if you're a bit over here, then the hat's a bit closer from that side. So these perspective transforms make a lot more sense. So if you're interested in perspective transforms, we have some details here on how you actually do them mathematically. The details aren't important, but what are interesting is the transform actually requires solving a system of linear equations. And did you know that PyTorch has a function for solving systems of linear equations? It's amazing how much stuff is in PyTorch, right? So for like lots of the things you'll need in your domain, you might be surprised to find what's already there. Question? And with the cropping and resizing, what happens when you lose the object of interest, so when the fish has been cropped out? That's a great question. It's not just a fish. It's a tensch. Yeah. So there's no tensch here. And so these are noisy labels. And interestingly, the kind of image net winning strategy is to randomly pick between 8% and 100% of the pixels. So literally, they are very often picking 8% of the pixels. And that's the image net winning strategy. So they very often have no tensch. So very often they'll have just the fin or just the eye. So this tells us that if we want to use this really effective augmentation strategy really well, we have to be very good at handling noisy labels, which we're going to learn about in the next lesson. And it also hopefully tells you that if you already have noisy labels, don't worry about it. All of the research we have tells us that we can handle labels where the thing's totally missing or sometimes it's wrong, as long as it's not biased. So yeah, it's okay. And one of the things it'll do is it'll learn to find things associated with a tensch. So if there's a middle-aged man looking very happy outside, could well be a tensch. Okay. So this is a bit of research that we're currently working on. And hopefully I'll have some results to show you soon. But our view is that this image warping approach is probably going to give us better results than the traditional image net style augmentations. So here's our final transform for tilting in arbitrary directions. And here's the result. Not bad. So a couple of things to finish on. The first is that it's really important to measure everything. And I and many people have been shocked to discover that actually the time it takes to convert an image into a float tensor is significantly longer than the amount of time it takes to do something as complicated as a warp. So you may be thinking this image warping thing sounds really hard and slow, but be careful, right? Just converting bytes to floats is really hard and slow. And then this is the one, as I mentioned, this one we're using here is the one that comes from TorchVision. We found another version that's like twice as fast, which goes directly to float. So this is the one that we're going to be using. So time everything if you're running, you know, if things are running not fast enough. Okay. Here's the thing I'm really excited about for augmentation is this stuff's all still too slow. What if I told you, you could do arbitrary affine transformations, so warping, zooming, rotating, shifting at a speed which would compare this is the normal speed, this is our speed. So up to like, you know, an order of magnitude or more faster. How do we do it? We figured out how to do it on the GPU. So we can actually do augmentation on the GPU. And the trick is that PyTorch gives us all the functionality to make it happen. So the key thing we have to do is to actually realize that our transforms, our augmentation should happen after you create a batch. So here's what we do. For our augmentation, we don't create one random number, we create a mini batch of random numbers, which is fine because PyTorch has the ability to generate batches of random numbers on the GPU. And so then once we've got a mini batch of random numbers, then we just have to use that to generate a mini batch of augmented images. I won't kind of bore you with the details. I find them very interesting details, but if you're not a computer vision person, maybe not. But basically, we create something called an affine grid, which is just the coordinates of where is every pixel. So like literally, it's coordinates from minus one to one. And then what we do is we multiply it by this matrix, which is called an affine transform. And there are various kinds of affine transforms you can do. For example, you can do a rotation transform by using this particular matrix, but these are all just matrix multiplications. And then you just, as you see here, you just do the matrix multiplication. And this is how you can rotate. So a rotation, believe it or not, is just a matrix multiplication by this tiny little matrix. If you do that, normally it's going to take you about 17 milliseconds. We can speed it up a bit with Ironsum. Or we could speed it up a little bit more with Batch Matrix Multiply. Or we could stick the whole thing on the GPU and do it there. And that's going to go from 11 milliseconds to 81 microseconds. So if we can put things on the GPU, it's totally different. And suddenly, we don't have to worry about how long our augmentation is taking. So this is the thing that actually rotates the coordinates to say where the coordinates are now. Then we have to do the interpolation. And believe it or not, PyTorch has an optimized batch-wise interpolation function. It's called grid sample. And so here it is. We run it. There it is. And not only do they have a grid sample, but this is actually even better than pillows because you don't have to have these black edges. You can say padding mode equals reflection. And the black edges are gone. It just reflects what was there, which most of the time is better. And so reflection padding is one of these little things we find definitely helps models. So now we can put this all together into a rotate batch. We can do any kind of coordinate transform here. One of them is rotate batch. To do it a batch at a time. And yeah, as I say, it's dramatically faster. Or in fact, we can do it all in one step because PyTorch has a thing called affine grid that will actually do the multiplication as it creates the coordinate grid. And this is where we get down to this incredibly fast speed. So I feel like there's a whole big opportunity here. There are currently no kind of hackable anybody can write their own augmentation, run on the GPU libraries out there. The entire fast AI dot vision library is written using PyTorch tensor operations. We did it so that we could eventually do it this way. But currently they all run on the CPU, on an image at a time. But this is our template now. So now you can do them a batch at a time. And so whatever domain you're working in, you can hopefully start to try out these randomized GPU batch-wise augmentations. And next week, we're going to show you this magic data augmentation called mixup that's going to work on the GPU. It's going to work on every kind of domain that you can think of. And we'll possibly make most of these irrelevant because it's so good you possibly don't need any others. So that and much more next week. We'll see you then. Thank you. Please, please be seated.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 4.84, "text": " Well, welcome back.", "tokens": [1042, 11, 2928, 646, 13], "temperature": 0.0, "avg_logprob": -0.1340902970761669, "compression_ratio": 1.6574074074074074, "no_speech_prob": 0.018525147810578346}, {"id": 1, "seek": 0, "start": 4.84, "end": 13.24, "text": " Welcome to lesson 11 where we're going to be talking mainly about data loading and optimizers.", "tokens": [4027, 281, 6898, 2975, 689, 321, 434, 516, 281, 312, 1417, 8704, 466, 1412, 15114, 293, 5028, 22525, 13], "temperature": 0.0, "avg_logprob": -0.1340902970761669, "compression_ratio": 1.6574074074074074, "no_speech_prob": 0.018525147810578346}, {"id": 2, "seek": 0, "start": 13.24, "end": 18.52, "text": " I said we would be talking about fastai.audio, but that's going to be a little bit later", "tokens": [286, 848, 321, 576, 312, 1417, 466, 2370, 1301, 13, 46069, 11, 457, 300, 311, 516, 281, 312, 257, 707, 857, 1780], "temperature": 0.0, "avg_logprob": -0.1340902970761669, "compression_ratio": 1.6574074074074074, "no_speech_prob": 0.018525147810578346}, {"id": 3, "seek": 0, "start": 18.52, "end": 19.52, "text": " in the course.", "tokens": [294, 264, 1164, 13], "temperature": 0.0, "avg_logprob": -0.1340902970761669, "compression_ratio": 1.6574074074074074, "no_speech_prob": 0.018525147810578346}, {"id": 4, "seek": 0, "start": 19.52, "end": 21.68, "text": " We haven't quite got to where I wanted to get to yet.", "tokens": [492, 2378, 380, 1596, 658, 281, 689, 286, 1415, 281, 483, 281, 1939, 13], "temperature": 0.0, "avg_logprob": -0.1340902970761669, "compression_ratio": 1.6574074074074074, "no_speech_prob": 0.018525147810578346}, {"id": 5, "seek": 0, "start": 21.68, "end": 27.12, "text": " So everything I said we'd talk about last week, we will talk about, but it might take", "tokens": [407, 1203, 286, 848, 321, 1116, 751, 466, 1036, 1243, 11, 321, 486, 751, 466, 11, 457, 309, 1062, 747], "temperature": 0.0, "avg_logprob": -0.1340902970761669, "compression_ratio": 1.6574074074074074, "no_speech_prob": 0.018525147810578346}, {"id": 6, "seek": 2712, "start": 27.12, "end": 31.64, "text": " a few more lessons to get there than I said.", "tokens": [257, 1326, 544, 8820, 281, 483, 456, 813, 286, 848, 13], "temperature": 0.0, "avg_logprob": -0.15054646291230855, "compression_ratio": 1.4456521739130435, "no_speech_prob": 5.55769911443349e-05}, {"id": 7, "seek": 2712, "start": 31.64, "end": 38.84, "text": " So this is kind of where we're up to, is the last little bit of our CNN.", "tokens": [407, 341, 307, 733, 295, 689, 321, 434, 493, 281, 11, 307, 264, 1036, 707, 857, 295, 527, 24859, 13], "temperature": 0.0, "avg_logprob": -0.15054646291230855, "compression_ratio": 1.4456521739130435, "no_speech_prob": 5.55769911443349e-05}, {"id": 8, "seek": 2712, "start": 38.84, "end": 43.96, "text": " And specifically these were the things we were going to dive into when we've done the", "tokens": [400, 4682, 613, 645, 264, 721, 321, 645, 516, 281, 9192, 666, 562, 321, 600, 1096, 264], "temperature": 0.0, "avg_logprob": -0.15054646291230855, "compression_ratio": 1.4456521739130435, "no_speech_prob": 5.55769911443349e-05}, {"id": 9, "seek": 2712, "start": 43.96, "end": 46.0, "text": " first four of these.", "tokens": [700, 1451, 295, 613, 13], "temperature": 0.0, "avg_logprob": -0.15054646291230855, "compression_ratio": 1.4456521739130435, "no_speech_prob": 5.55769911443349e-05}, {"id": 10, "seek": 2712, "start": 46.0, "end": 51.08, "text": " CUDA, convolutions, hooks, normalization.", "tokens": [29777, 7509, 11, 3754, 15892, 11, 26485, 11, 2710, 2144, 13], "temperature": 0.0, "avg_logprob": -0.15054646291230855, "compression_ratio": 1.4456521739130435, "no_speech_prob": 5.55769911443349e-05}, {"id": 11, "seek": 5108, "start": 51.08, "end": 57.199999999999996, "text": " So we're going to keep going through this process to try to create our state of the", "tokens": [407, 321, 434, 516, 281, 1066, 516, 807, 341, 1399, 281, 853, 281, 1884, 527, 1785, 295, 264], "temperature": 0.0, "avg_logprob": -0.12258819631628089, "compression_ratio": 1.583756345177665, "no_speech_prob": 5.7698753153090365e-06}, {"id": 12, "seek": 5108, "start": 57.199999999999996, "end": 60.08, "text": " art image net model.", "tokens": [1523, 3256, 2533, 2316, 13], "temperature": 0.0, "avg_logprob": -0.12258819631628089, "compression_ratio": 1.583756345177665, "no_speech_prob": 5.7698753153090365e-06}, {"id": 13, "seek": 5108, "start": 60.08, "end": 68.03999999999999, "text": " The specific items that we're working with are images, but everything we've covered so", "tokens": [440, 2685, 4754, 300, 321, 434, 1364, 365, 366, 5267, 11, 457, 1203, 321, 600, 5343, 370], "temperature": 0.0, "avg_logprob": -0.12258819631628089, "compression_ratio": 1.583756345177665, "no_speech_prob": 5.7698753153090365e-06}, {"id": 14, "seek": 5108, "start": 68.03999999999999, "end": 75.5, "text": " far is equally valid, equally used for tabular collaborative filtering and text and pretty", "tokens": [1400, 307, 12309, 7363, 11, 12309, 1143, 337, 4421, 1040, 16555, 30822, 293, 2487, 293, 1238], "temperature": 0.0, "avg_logprob": -0.12258819631628089, "compression_ratio": 1.583756345177665, "no_speech_prob": 5.7698753153090365e-06}, {"id": 15, "seek": 5108, "start": 75.5, "end": 80.4, "text": " much everything else as well.", "tokens": [709, 1203, 1646, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.12258819631628089, "compression_ratio": 1.583756345177665, "no_speech_prob": 5.7698753153090365e-06}, {"id": 16, "seek": 8040, "start": 80.4, "end": 86.80000000000001, "text": " Last week we talked about batch norm, and I just wanted to mention that at the end of", "tokens": [5264, 1243, 321, 2825, 466, 15245, 2026, 11, 293, 286, 445, 1415, 281, 2152, 300, 412, 264, 917, 295], "temperature": 0.0, "avg_logprob": -0.1201053584387543, "compression_ratio": 1.7338709677419355, "no_speech_prob": 1.9220229660277255e-05}, {"id": 17, "seek": 8040, "start": 86.80000000000001, "end": 92.24000000000001, "text": " the batch norm notebook there's another bit called simplified running batch norm.", "tokens": [264, 15245, 2026, 21060, 456, 311, 1071, 857, 1219, 26335, 2614, 15245, 2026, 13], "temperature": 0.0, "avg_logprob": -0.1201053584387543, "compression_ratio": 1.7338709677419355, "no_speech_prob": 1.9220229660277255e-05}, {"id": 18, "seek": 8040, "start": 92.24000000000001, "end": 97.32000000000001, "text": " We talked a little bit about de-biasing last week, we'll talk about it more today.", "tokens": [492, 2825, 257, 707, 857, 466, 368, 12, 5614, 3349, 1036, 1243, 11, 321, 603, 751, 466, 309, 544, 965, 13], "temperature": 0.0, "avg_logprob": -0.1201053584387543, "compression_ratio": 1.7338709677419355, "no_speech_prob": 1.9220229660277255e-05}, {"id": 19, "seek": 8040, "start": 97.32000000000001, "end": 101.64000000000001, "text": " But Stas Begman pointed out something which is kind of obvious in hindsight, but I didn't", "tokens": [583, 745, 296, 879, 70, 1601, 10932, 484, 746, 597, 307, 733, 295, 6322, 294, 44357, 11, 457, 286, 994, 380], "temperature": 0.0, "avg_logprob": -0.1201053584387543, "compression_ratio": 1.7338709677419355, "no_speech_prob": 1.9220229660277255e-05}, {"id": 20, "seek": 8040, "start": 101.64000000000001, "end": 109.08000000000001, "text": " notice at the time, which is that we had sums divided by de-bias and we had count divided", "tokens": [3449, 412, 264, 565, 11, 597, 307, 300, 321, 632, 34499, 6666, 538, 368, 12, 65, 4609, 293, 321, 632, 1207, 6666], "temperature": 0.0, "avg_logprob": -0.1201053584387543, "compression_ratio": 1.7338709677419355, "no_speech_prob": 1.9220229660277255e-05}, {"id": 21, "seek": 10908, "start": 109.08, "end": 114.8, "text": " by de-bias, and then we go sum divided by count, and sum divided by de-bias divided", "tokens": [538, 368, 12, 65, 4609, 11, 293, 550, 321, 352, 2408, 6666, 538, 1207, 11, 293, 2408, 6666, 538, 368, 12, 65, 4609, 6666], "temperature": 0.0, "avg_logprob": -0.09244751511958607, "compression_ratio": 1.8373205741626795, "no_speech_prob": 4.06648323405534e-05}, {"id": 22, "seek": 10908, "start": 114.8, "end": 120.72, "text": " by count divided by de-bias, the two de-biases cancel each other out, so we can remove all", "tokens": [538, 1207, 6666, 538, 368, 12, 65, 4609, 11, 264, 732, 368, 12, 65, 4609, 279, 10373, 1184, 661, 484, 11, 370, 321, 393, 4159, 439], "temperature": 0.0, "avg_logprob": -0.09244751511958607, "compression_ratio": 1.8373205741626795, "no_speech_prob": 4.06648323405534e-05}, {"id": 23, "seek": 10908, "start": 120.72, "end": 122.16, "text": " of them.", "tokens": [295, 552, 13], "temperature": 0.0, "avg_logprob": -0.09244751511958607, "compression_ratio": 1.8373205741626795, "no_speech_prob": 4.06648323405534e-05}, {"id": 24, "seek": 10908, "start": 122.16, "end": 126.36, "text": " So we're still going to cover de-biasing today for a different purpose, but actually we didn't", "tokens": [407, 321, 434, 920, 516, 281, 2060, 368, 12, 5614, 3349, 965, 337, 257, 819, 4334, 11, 457, 767, 321, 994, 380], "temperature": 0.0, "avg_logprob": -0.09244751511958607, "compression_ratio": 1.8373205741626795, "no_speech_prob": 4.06648323405534e-05}, {"id": 25, "seek": 10908, "start": 126.36, "end": 127.84, "text": " really need it for last week.", "tokens": [534, 643, 309, 337, 1036, 1243, 13], "temperature": 0.0, "avg_logprob": -0.09244751511958607, "compression_ratio": 1.8373205741626795, "no_speech_prob": 4.06648323405534e-05}, {"id": 26, "seek": 10908, "start": 127.84, "end": 134.07999999999998, "text": " So we can remove all the de-biasing and end up with something much simpler.", "tokens": [407, 321, 393, 4159, 439, 264, 368, 12, 5614, 3349, 293, 917, 493, 365, 746, 709, 18587, 13], "temperature": 0.0, "avg_logprob": -0.09244751511958607, "compression_ratio": 1.8373205741626795, "no_speech_prob": 4.06648323405534e-05}, {"id": 27, "seek": 13408, "start": 134.08, "end": 140.72000000000003, "text": " So that's the version that we're going to go with.", "tokens": [407, 300, 311, 264, 3037, 300, 321, 434, 516, 281, 352, 365, 13], "temperature": 0.0, "avg_logprob": -0.20436196667807444, "compression_ratio": 1.43125, "no_speech_prob": 7.888013897172641e-06}, {"id": 28, "seek": 13408, "start": 140.72000000000003, "end": 150.48000000000002, "text": " And also thanks to Tom Veman who pointed out that the last step where we went subtract", "tokens": [400, 611, 3231, 281, 5041, 691, 15023, 567, 10932, 484, 300, 264, 1036, 1823, 689, 321, 1437, 16390], "temperature": 0.0, "avg_logprob": -0.20436196667807444, "compression_ratio": 1.43125, "no_speech_prob": 7.888013897172641e-06}, {"id": 29, "seek": 13408, "start": 150.48000000000002, "end": 157.4, "text": " mean divided by standard deviation multiplied by mults at adds, you can just rearrange that", "tokens": [914, 6666, 538, 3832, 25163, 17207, 538, 275, 33361, 412, 10860, 11, 291, 393, 445, 39568, 300], "temperature": 0.0, "avg_logprob": -0.20436196667807444, "compression_ratio": 1.43125, "no_speech_prob": 7.888013897172641e-06}, {"id": 30, "seek": 15740, "start": 157.4, "end": 166.20000000000002, "text": " into this form, mults divided by variances and adds minus means times that factor.", "tokens": [666, 341, 1254, 11, 275, 33361, 6666, 538, 1374, 21518, 293, 10860, 3175, 1355, 1413, 300, 5952, 13], "temperature": 0.0, "avg_logprob": -0.09724872413722949, "compression_ratio": 1.6066350710900474, "no_speech_prob": 5.594012236542767e-06}, {"id": 31, "seek": 15740, "start": 166.20000000000002, "end": 173.64000000000001, "text": " And if you do it this way, then you don't actually have to touch x until you've done", "tokens": [400, 498, 291, 360, 309, 341, 636, 11, 550, 291, 500, 380, 767, 362, 281, 2557, 2031, 1826, 291, 600, 1096], "temperature": 0.0, "avg_logprob": -0.09724872413722949, "compression_ratio": 1.6066350710900474, "no_speech_prob": 5.594012236542767e-06}, {"id": 32, "seek": 15740, "start": 173.64000000000001, "end": 176.20000000000002, "text": " all of those things, and so that's going to end up being faster.", "tokens": [439, 295, 729, 721, 11, 293, 370, 300, 311, 516, 281, 917, 493, 885, 4663, 13], "temperature": 0.0, "avg_logprob": -0.09724872413722949, "compression_ratio": 1.6066350710900474, "no_speech_prob": 5.594012236542767e-06}, {"id": 33, "seek": 15740, "start": 176.20000000000002, "end": 181.88, "text": " If you think through the broadcasting operations there, then you're doing a lot less computation", "tokens": [759, 291, 519, 807, 264, 30024, 7705, 456, 11, 550, 291, 434, 884, 257, 688, 1570, 24903], "temperature": 0.0, "avg_logprob": -0.09724872413722949, "compression_ratio": 1.6066350710900474, "no_speech_prob": 5.594012236542767e-06}, {"id": 34, "seek": 15740, "start": 181.88, "end": 182.88, "text": " this way.", "tokens": [341, 636, 13], "temperature": 0.0, "avg_logprob": -0.09724872413722949, "compression_ratio": 1.6066350710900474, "no_speech_prob": 5.594012236542767e-06}, {"id": 35, "seek": 18288, "start": 182.88, "end": 187.79999999999998, "text": " So that was a good idea as well.", "tokens": [407, 300, 390, 257, 665, 1558, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.15495647922638925, "compression_ratio": 1.4976958525345623, "no_speech_prob": 1.2027558113913983e-05}, {"id": 36, "seek": 18288, "start": 187.79999999999998, "end": 192.68, "text": " And I should mention Tom has been helping us out quite a bit with some of this batch", "tokens": [400, 286, 820, 2152, 5041, 575, 668, 4315, 505, 484, 1596, 257, 857, 365, 512, 295, 341, 15245], "temperature": 0.0, "avg_logprob": -0.15495647922638925, "compression_ratio": 1.4976958525345623, "no_speech_prob": 1.2027558113913983e-05}, {"id": 37, "seek": 18288, "start": 192.68, "end": 193.68, "text": " norm stuff.", "tokens": [2026, 1507, 13], "temperature": 0.0, "avg_logprob": -0.15495647922638925, "compression_ratio": 1.4976958525345623, "no_speech_prob": 1.2027558113913983e-05}, {"id": 38, "seek": 18288, "start": 193.68, "end": 198.28, "text": " One of a few people involved in the PyTorch community who's been amazingly helpful who", "tokens": [1485, 295, 257, 1326, 561, 3288, 294, 264, 9953, 51, 284, 339, 1768, 567, 311, 668, 31762, 4961, 567], "temperature": 0.0, "avg_logprob": -0.15495647922638925, "compression_ratio": 1.4976958525345623, "no_speech_prob": 1.2027558113913983e-05}, {"id": 39, "seek": 18288, "start": 198.28, "end": 200.48, "text": " I'd like to call out.", "tokens": [286, 1116, 411, 281, 818, 484, 13], "temperature": 0.0, "avg_logprob": -0.15495647922638925, "compression_ratio": 1.4976958525345623, "no_speech_prob": 1.2027558113913983e-05}, {"id": 40, "seek": 18288, "start": 200.48, "end": 207.68, "text": " So thanks also to Sumit Chintala who was one of the original founders of PyTorch who's", "tokens": [407, 3231, 611, 281, 8626, 270, 761, 686, 5159, 567, 390, 472, 295, 264, 3380, 25608, 295, 9953, 51, 284, 339, 567, 311], "temperature": 0.0, "avg_logprob": -0.15495647922638925, "compression_ratio": 1.4976958525345623, "no_speech_prob": 1.2027558113913983e-05}, {"id": 41, "seek": 20768, "start": 207.68, "end": 213.08, "text": " been super helpful in sorting some things out for this course and also Francisco Massa", "tokens": [668, 1687, 4961, 294, 32411, 512, 721, 484, 337, 341, 1164, 293, 611, 12279, 10482, 64], "temperature": 0.0, "avg_logprob": -0.2269785016082054, "compression_ratio": 1.5023923444976077, "no_speech_prob": 2.9297752917045727e-05}, {"id": 42, "seek": 20768, "start": 213.08, "end": 216.28, "text": " also super helpful.", "tokens": [611, 1687, 4961, 13], "temperature": 0.0, "avg_logprob": -0.2269785016082054, "compression_ratio": 1.5023923444976077, "no_speech_prob": 2.9297752917045727e-05}, {"id": 43, "seek": 20768, "start": 216.28, "end": 222.04000000000002, "text": " They're both part of the official Facebook engineering team and Tom's not, but he does", "tokens": [814, 434, 1293, 644, 295, 264, 4783, 4384, 7043, 1469, 293, 5041, 311, 406, 11, 457, 415, 775], "temperature": 0.0, "avg_logprob": -0.2269785016082054, "compression_ratio": 1.5023923444976077, "no_speech_prob": 2.9297752917045727e-05}, {"id": 44, "seek": 20768, "start": 222.04000000000002, "end": 223.32, "text": " so much work for PyTorch.", "tokens": [370, 709, 589, 337, 9953, 51, 284, 339, 13], "temperature": 0.0, "avg_logprob": -0.2269785016082054, "compression_ratio": 1.5023923444976077, "no_speech_prob": 2.9297752917045727e-05}, {"id": 45, "seek": 20768, "start": 223.32, "end": 226.20000000000002, "text": " He kind of seems like he must be sometimes.", "tokens": [634, 733, 295, 2544, 411, 415, 1633, 312, 2171, 13], "temperature": 0.0, "avg_logprob": -0.2269785016082054, "compression_ratio": 1.5023923444976077, "no_speech_prob": 2.9297752917045727e-05}, {"id": 46, "seek": 20768, "start": 226.20000000000002, "end": 229.92000000000002, "text": " So thanks to all of you for your great help.", "tokens": [407, 3231, 281, 439, 295, 291, 337, 428, 869, 854, 13], "temperature": 0.0, "avg_logprob": -0.2269785016082054, "compression_ratio": 1.5023923444976077, "no_speech_prob": 2.9297752917045727e-05}, {"id": 47, "seek": 20768, "start": 229.92000000000002, "end": 232.12, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.2269785016082054, "compression_ratio": 1.5023923444976077, "no_speech_prob": 2.9297752917045727e-05}, {"id": 48, "seek": 23212, "start": 232.12, "end": 239.36, "text": " Before we moved on to data blocks, I wanted to mention one other approach to making sure", "tokens": [4546, 321, 4259, 322, 281, 1412, 8474, 11, 286, 1415, 281, 2152, 472, 661, 3109, 281, 1455, 988], "temperature": 0.0, "avg_logprob": -0.16232665379842123, "compression_ratio": 1.4660194174757282, "no_speech_prob": 1.4728809219377581e-05}, {"id": 49, "seek": 23212, "start": 239.36, "end": 242.24, "text": " that your model trains nicely.", "tokens": [300, 428, 2316, 16329, 9594, 13], "temperature": 0.0, "avg_logprob": -0.16232665379842123, "compression_ratio": 1.4660194174757282, "no_speech_prob": 1.4728809219377581e-05}, {"id": 50, "seek": 23212, "start": 242.24, "end": 246.84, "text": " And to me, this is the most fast AI-ish method.", "tokens": [400, 281, 385, 11, 341, 307, 264, 881, 2370, 7318, 12, 742, 3170, 13], "temperature": 0.0, "avg_logprob": -0.16232665379842123, "compression_ratio": 1.4660194174757282, "no_speech_prob": 1.4728809219377581e-05}, {"id": 51, "seek": 23212, "start": 246.84, "end": 252.32, "text": " I wish I had come up with it, but I didn't.", "tokens": [286, 3172, 286, 632, 808, 493, 365, 309, 11, 457, 286, 994, 380, 13], "temperature": 0.0, "avg_logprob": -0.16232665379842123, "compression_ratio": 1.4660194174757282, "no_speech_prob": 1.4728809219377581e-05}, {"id": 52, "seek": 23212, "start": 252.32, "end": 257.12, "text": " Wonderful researcher named Dmitry came up with it in a paper called All You Need Is a Good", "tokens": [22768, 21751, 4926, 413, 3508, 627, 1361, 493, 365, 309, 294, 257, 3035, 1219, 1057, 509, 16984, 1119, 257, 2205], "temperature": 0.0, "avg_logprob": -0.16232665379842123, "compression_ratio": 1.4660194174757282, "no_speech_prob": 1.4728809219377581e-05}, {"id": 53, "seek": 25712, "start": 257.12, "end": 264.64, "text": " Unit, Dmitry Mishkin, and this is the paper.", "tokens": [27894, 11, 413, 3508, 627, 376, 742, 5843, 11, 293, 341, 307, 264, 3035, 13], "temperature": 0.0, "avg_logprob": -0.18408300736371208, "compression_ratio": 1.4393939393939394, "no_speech_prob": 1.618209353182465e-05}, {"id": 54, "seek": 25712, "start": 264.64, "end": 268.8, "text": " And he came up with this technique called LSUV, Layer-Wise Sequential Unit Variance.", "tokens": [400, 415, 1361, 493, 365, 341, 6532, 1219, 441, 20214, 53, 11, 35166, 12, 54, 908, 46859, 2549, 27894, 32511, 719, 13], "temperature": 0.0, "avg_logprob": -0.18408300736371208, "compression_ratio": 1.4393939393939394, "no_speech_prob": 1.618209353182465e-05}, {"id": 55, "seek": 25712, "start": 268.8, "end": 271.12, "text": " And the basic idea is this.", "tokens": [400, 264, 3875, 1558, 307, 341, 13], "temperature": 0.0, "avg_logprob": -0.18408300736371208, "compression_ratio": 1.4393939393939394, "no_speech_prob": 1.618209353182465e-05}, {"id": 56, "seek": 25712, "start": 271.12, "end": 278.52, "text": " You've seen now how fiddly it is to get your unit variances all the way through your network", "tokens": [509, 600, 1612, 586, 577, 283, 14273, 356, 309, 307, 281, 483, 428, 4985, 1374, 21518, 439, 264, 636, 807, 428, 3209], "temperature": 0.0, "avg_logprob": -0.18408300736371208, "compression_ratio": 1.4393939393939394, "no_speech_prob": 1.618209353182465e-05}, {"id": 57, "seek": 25712, "start": 278.52, "end": 282.2, "text": " and little things can change that.", "tokens": [293, 707, 721, 393, 1319, 300, 13], "temperature": 0.0, "avg_logprob": -0.18408300736371208, "compression_ratio": 1.4393939393939394, "no_speech_prob": 1.618209353182465e-05}, {"id": 58, "seek": 28220, "start": 282.2, "end": 287.8, "text": " So if you change your activation function or something we haven't mentioned, if you", "tokens": [407, 498, 291, 1319, 428, 24433, 2445, 420, 746, 321, 2378, 380, 2835, 11, 498, 291], "temperature": 0.0, "avg_logprob": -0.13103332122166952, "compression_ratio": 1.6352459016393444, "no_speech_prob": 1.4737091987626627e-05}, {"id": 59, "seek": 28220, "start": 287.8, "end": 293.64, "text": " add dropout or change the amount of dropout, these are all going to impact the variances", "tokens": [909, 3270, 346, 420, 1319, 264, 2372, 295, 3270, 346, 11, 613, 366, 439, 516, 281, 2712, 264, 1374, 21518], "temperature": 0.0, "avg_logprob": -0.13103332122166952, "compression_ratio": 1.6352459016393444, "no_speech_prob": 1.4737091987626627e-05}, {"id": 60, "seek": 28220, "start": 293.64, "end": 296.12, "text": " of your layer outputs.", "tokens": [295, 428, 4583, 23930, 13], "temperature": 0.0, "avg_logprob": -0.13103332122166952, "compression_ratio": 1.6352459016393444, "no_speech_prob": 1.4737091987626627e-05}, {"id": 61, "seek": 28220, "start": 296.12, "end": 300.44, "text": " And if they're just a little bit different to one, you'll get exponentially worse, as", "tokens": [400, 498, 436, 434, 445, 257, 707, 857, 819, 281, 472, 11, 291, 603, 483, 37330, 5324, 11, 382], "temperature": 0.0, "avg_logprob": -0.13103332122166952, "compression_ratio": 1.6352459016393444, "no_speech_prob": 1.4737091987626627e-05}, {"id": 62, "seek": 28220, "start": 300.44, "end": 303.44, "text": " we saw, through the model.", "tokens": [321, 1866, 11, 807, 264, 2316, 13], "temperature": 0.0, "avg_logprob": -0.13103332122166952, "compression_ratio": 1.6352459016393444, "no_speech_prob": 1.4737091987626627e-05}, {"id": 63, "seek": 28220, "start": 303.44, "end": 307.88, "text": " So the normal approach to fixing this is to think really carefully about your architecture", "tokens": [407, 264, 2710, 3109, 281, 19442, 341, 307, 281, 519, 534, 7500, 466, 428, 9482], "temperature": 0.0, "avg_logprob": -0.13103332122166952, "compression_ratio": 1.6352459016393444, "no_speech_prob": 1.4737091987626627e-05}, {"id": 64, "seek": 30788, "start": 307.88, "end": 312.76, "text": " and exactly, analytically figure out how to initialize everything so it works.", "tokens": [293, 2293, 11, 10783, 984, 2573, 484, 577, 281, 5883, 1125, 1203, 370, 309, 1985, 13], "temperature": 0.0, "avg_logprob": -0.12289699179227234, "compression_ratio": 1.7470817120622568, "no_speech_prob": 9.972406587621663e-06}, {"id": 65, "seek": 30788, "start": 312.76, "end": 319.36, "text": " And Dmitry's idea, which I like a lot better, is let the computer figure it out.", "tokens": [400, 413, 3508, 627, 311, 1558, 11, 597, 286, 411, 257, 688, 1101, 11, 307, 718, 264, 3820, 2573, 309, 484, 13], "temperature": 0.0, "avg_logprob": -0.12289699179227234, "compression_ratio": 1.7470817120622568, "no_speech_prob": 9.972406587621663e-06}, {"id": 66, "seek": 30788, "start": 319.36, "end": 321.0, "text": " And here's how you let the computer figure it out.", "tokens": [400, 510, 311, 577, 291, 718, 264, 3820, 2573, 309, 484, 13], "temperature": 0.0, "avg_logprob": -0.12289699179227234, "compression_ratio": 1.7470817120622568, "no_speech_prob": 9.972406587621663e-06}, {"id": 67, "seek": 30788, "start": 321.0, "end": 323.48, "text": " We create our MNIST data set in the same way as before.", "tokens": [492, 1884, 527, 376, 45, 19756, 1412, 992, 294, 264, 912, 636, 382, 949, 13], "temperature": 0.0, "avg_logprob": -0.12289699179227234, "compression_ratio": 1.7470817120622568, "no_speech_prob": 9.972406587621663e-06}, {"id": 68, "seek": 30788, "start": 323.48, "end": 327.92, "text": " We create a bunch of layers with these number of filters like before.", "tokens": [492, 1884, 257, 3840, 295, 7914, 365, 613, 1230, 295, 15995, 411, 949, 13], "temperature": 0.0, "avg_logprob": -0.12289699179227234, "compression_ratio": 1.7470817120622568, "no_speech_prob": 9.972406587621663e-06}, {"id": 69, "seek": 30788, "start": 327.92, "end": 333.2, "text": " And what I'm going to do is I'm going to create a conv layer class, which contains our convolution", "tokens": [400, 437, 286, 478, 516, 281, 360, 307, 286, 478, 516, 281, 1884, 257, 3754, 4583, 1508, 11, 597, 8306, 527, 45216], "temperature": 0.0, "avg_logprob": -0.12289699179227234, "compression_ratio": 1.7470817120622568, "no_speech_prob": 9.972406587621663e-06}, {"id": 70, "seek": 30788, "start": 333.2, "end": 335.04, "text": " and our relu.", "tokens": [293, 527, 1039, 84, 13], "temperature": 0.0, "avg_logprob": -0.12289699179227234, "compression_ratio": 1.7470817120622568, "no_speech_prob": 9.972406587621663e-06}, {"id": 71, "seek": 33504, "start": 335.04, "end": 340.16, "text": " The idea is that we're going to use this because now we can basically say this whole combined", "tokens": [440, 1558, 307, 300, 321, 434, 516, 281, 764, 341, 570, 586, 321, 393, 1936, 584, 341, 1379, 9354], "temperature": 0.0, "avg_logprob": -0.1504129409790039, "compression_ratio": 1.652542372881356, "no_speech_prob": 8.267592420452274e-06}, {"id": 72, "seek": 33504, "start": 340.16, "end": 347.16, "text": " conv plus relu has kind of a, I'm calling it bias, but actually I'm taking that general", "tokens": [3754, 1804, 1039, 84, 575, 733, 295, 257, 11, 286, 478, 5141, 309, 12577, 11, 457, 767, 286, 478, 1940, 300, 2674], "temperature": 0.0, "avg_logprob": -0.1504129409790039, "compression_ratio": 1.652542372881356, "no_speech_prob": 8.267592420452274e-06}, {"id": 73, "seek": 33504, "start": 347.16, "end": 351.52000000000004, "text": " relu and just saying how much are we subtracting from it.", "tokens": [1039, 84, 293, 445, 1566, 577, 709, 366, 321, 16390, 278, 490, 309, 13], "temperature": 0.0, "avg_logprob": -0.1504129409790039, "compression_ratio": 1.652542372881356, "no_speech_prob": 8.267592420452274e-06}, {"id": 74, "seek": 33504, "start": 351.52000000000004, "end": 356.76, "text": " So this is kind of like something we can add or remove.", "tokens": [407, 341, 307, 733, 295, 411, 746, 321, 393, 909, 420, 4159, 13], "temperature": 0.0, "avg_logprob": -0.1504129409790039, "compression_ratio": 1.652542372881356, "no_speech_prob": 8.267592420452274e-06}, {"id": 75, "seek": 33504, "start": 356.76, "end": 359.0, "text": " And then the weight is just the conv weights.", "tokens": [400, 550, 264, 3364, 307, 445, 264, 3754, 17443, 13], "temperature": 0.0, "avg_logprob": -0.1504129409790039, "compression_ratio": 1.652542372881356, "no_speech_prob": 8.267592420452274e-06}, {"id": 76, "seek": 33504, "start": 359.0, "end": 361.24, "text": " And you'll see why we're doing this in a moment.", "tokens": [400, 291, 603, 536, 983, 321, 434, 884, 341, 294, 257, 1623, 13], "temperature": 0.0, "avg_logprob": -0.1504129409790039, "compression_ratio": 1.652542372881356, "no_speech_prob": 8.267592420452274e-06}, {"id": 77, "seek": 36124, "start": 361.24, "end": 365.48, "text": " Basically what we'll do is we'll create our learner in the usual way, right?", "tokens": [8537, 437, 321, 603, 360, 307, 321, 603, 1884, 527, 33347, 294, 264, 7713, 636, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1301799714565277, "compression_ratio": 1.7941176470588236, "no_speech_prob": 1.5534764088442898e-06}, {"id": 78, "seek": 36124, "start": 365.48, "end": 369.40000000000003, "text": " And however it initializes is fine.", "tokens": [400, 4461, 309, 5883, 5660, 307, 2489, 13], "temperature": 0.0, "avg_logprob": -0.1301799714565277, "compression_ratio": 1.7941176470588236, "no_speech_prob": 1.5534764088442898e-06}, {"id": 79, "seek": 36124, "start": 369.40000000000003, "end": 373.0, "text": " And so we can train it.", "tokens": [400, 370, 321, 393, 3847, 309, 13], "temperature": 0.0, "avg_logprob": -0.1301799714565277, "compression_ratio": 1.7941176470588236, "no_speech_prob": 1.5534764088442898e-06}, {"id": 80, "seek": 36124, "start": 373.0, "end": 374.0, "text": " That's fine.", "tokens": [663, 311, 2489, 13], "temperature": 0.0, "avg_logprob": -0.1301799714565277, "compression_ratio": 1.7941176470588236, "no_speech_prob": 1.5534764088442898e-06}, {"id": 81, "seek": 36124, "start": 374.0, "end": 376.92, "text": " But let's try and now train it in a better way.", "tokens": [583, 718, 311, 853, 293, 586, 3847, 309, 294, 257, 1101, 636, 13], "temperature": 0.0, "avg_logprob": -0.1301799714565277, "compression_ratio": 1.7941176470588236, "no_speech_prob": 1.5534764088442898e-06}, {"id": 82, "seek": 36124, "start": 376.92, "end": 380.0, "text": " So let's recreate our learner.", "tokens": [407, 718, 311, 25833, 527, 33347, 13], "temperature": 0.0, "avg_logprob": -0.1301799714565277, "compression_ratio": 1.7941176470588236, "no_speech_prob": 1.5534764088442898e-06}, {"id": 83, "seek": 36124, "start": 380.0, "end": 385.0, "text": " And let's grab a single minibatch.", "tokens": [400, 718, 311, 4444, 257, 2167, 923, 897, 852, 13], "temperature": 0.0, "avg_logprob": -0.1301799714565277, "compression_ratio": 1.7941176470588236, "no_speech_prob": 1.5534764088442898e-06}, {"id": 84, "seek": 36124, "start": 385.0, "end": 388.16, "text": " And here's a function that will let us grab a single minibatch, making sure we're using", "tokens": [400, 510, 311, 257, 2445, 300, 486, 718, 505, 4444, 257, 2167, 923, 897, 852, 11, 1455, 988, 321, 434, 1228], "temperature": 0.0, "avg_logprob": -0.1301799714565277, "compression_ratio": 1.7941176470588236, "no_speech_prob": 1.5534764088442898e-06}, {"id": 85, "seek": 36124, "start": 388.16, "end": 391.08, "text": " all our callbacks, that the minibatch does all the things we need it to do.", "tokens": [439, 527, 818, 17758, 11, 300, 264, 923, 897, 852, 775, 439, 264, 721, 321, 643, 309, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.1301799714565277, "compression_ratio": 1.7941176470588236, "no_speech_prob": 1.5534764088442898e-06}, {"id": 86, "seek": 39108, "start": 391.08, "end": 397.76, "text": " So here's one minibatch of X and Y.", "tokens": [407, 510, 311, 472, 923, 897, 852, 295, 1783, 293, 398, 13], "temperature": 0.0, "avg_logprob": -0.12322396172417535, "compression_ratio": 1.7150537634408602, "no_speech_prob": 2.3320235413848422e-06}, {"id": 87, "seek": 39108, "start": 397.76, "end": 405.2, "text": " And what we're going to do is we're going to find all of the modules that are, we're", "tokens": [400, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 915, 439, 295, 264, 16679, 300, 366, 11, 321, 434], "temperature": 0.0, "avg_logprob": -0.12322396172417535, "compression_ratio": 1.7150537634408602, "no_speech_prob": 2.3320235413848422e-06}, {"id": 88, "seek": 39108, "start": 405.2, "end": 411.56, "text": " going to find all of the modules which are of type conv layer.", "tokens": [516, 281, 915, 439, 295, 264, 16679, 597, 366, 295, 2010, 3754, 4583, 13], "temperature": 0.0, "avg_logprob": -0.12322396172417535, "compression_ratio": 1.7150537634408602, "no_speech_prob": 2.3320235413848422e-06}, {"id": 89, "seek": 39108, "start": 411.56, "end": 415.44, "text": " And so it's just a little function that does that.", "tokens": [400, 370, 309, 311, 445, 257, 707, 2445, 300, 775, 300, 13], "temperature": 0.0, "avg_logprob": -0.12322396172417535, "compression_ratio": 1.7150537634408602, "no_speech_prob": 2.3320235413848422e-06}, {"id": 90, "seek": 39108, "start": 415.44, "end": 420.44, "text": " And generally speaking, when you're working with PyTorch modules or with neural nets", "tokens": [400, 5101, 4124, 11, 562, 291, 434, 1364, 365, 9953, 51, 284, 339, 16679, 420, 365, 18161, 36170], "temperature": 0.0, "avg_logprob": -0.12322396172417535, "compression_ratio": 1.7150537634408602, "no_speech_prob": 2.3320235413848422e-06}, {"id": 91, "seek": 42044, "start": 420.44, "end": 424.6, "text": " more generally, you need to use recursion a lot because modules can contain modules", "tokens": [544, 5101, 11, 291, 643, 281, 764, 20560, 313, 257, 688, 570, 16679, 393, 5304, 16679], "temperature": 0.0, "avg_logprob": -0.136766736752519, "compression_ratio": 1.8709677419354838, "no_speech_prob": 1.3925218809163198e-06}, {"id": 92, "seek": 42044, "start": 424.6, "end": 425.6, "text": " can contain modules, right?", "tokens": [393, 5304, 16679, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.136766736752519, "compression_ratio": 1.8709677419354838, "no_speech_prob": 1.3925218809163198e-06}, {"id": 93, "seek": 42044, "start": 425.6, "end": 431.44, "text": " So you can see here, find modules, calls find modules to find out all the modules throughout", "tokens": [407, 291, 393, 536, 510, 11, 915, 16679, 11, 5498, 915, 16679, 281, 915, 484, 439, 264, 16679, 3710], "temperature": 0.0, "avg_logprob": -0.136766736752519, "compression_ratio": 1.8709677419354838, "no_speech_prob": 1.3925218809163198e-06}, {"id": 94, "seek": 42044, "start": 431.44, "end": 434.72, "text": " your kind of tree, you know, because really a module is like a tree.", "tokens": [428, 733, 295, 4230, 11, 291, 458, 11, 570, 534, 257, 10088, 307, 411, 257, 4230, 13], "temperature": 0.0, "avg_logprob": -0.136766736752519, "compression_ratio": 1.8709677419354838, "no_speech_prob": 1.3925218809163198e-06}, {"id": 95, "seek": 42044, "start": 434.72, "end": 438.04, "text": " Modules have modules have modules.", "tokens": [6583, 3473, 362, 16679, 362, 16679, 13], "temperature": 0.0, "avg_logprob": -0.136766736752519, "compression_ratio": 1.8709677419354838, "no_speech_prob": 1.3925218809163198e-06}, {"id": 96, "seek": 42044, "start": 438.04, "end": 440.9, "text": " And so here's our list of all of our conv layers.", "tokens": [400, 370, 510, 311, 527, 1329, 295, 439, 295, 527, 3754, 7914, 13], "temperature": 0.0, "avg_logprob": -0.136766736752519, "compression_ratio": 1.8709677419354838, "no_speech_prob": 1.3925218809163198e-06}, {"id": 97, "seek": 42044, "start": 440.9, "end": 445.76, "text": " And then what we do is we create a hook, right?", "tokens": [400, 550, 437, 321, 360, 307, 321, 1884, 257, 6328, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.136766736752519, "compression_ratio": 1.8709677419354838, "no_speech_prob": 1.3925218809163198e-06}, {"id": 98, "seek": 44576, "start": 445.76, "end": 451.26, "text": " And the hook is just going to grab the mean and standard deviation of a particular module.", "tokens": [400, 264, 6328, 307, 445, 516, 281, 4444, 264, 914, 293, 3832, 25163, 295, 257, 1729, 10088, 13], "temperature": 0.0, "avg_logprob": -0.17598902384440104, "compression_ratio": 1.768421052631579, "no_speech_prob": 5.5073105613701046e-06}, {"id": 99, "seek": 44576, "start": 451.26, "end": 453.71999999999997, "text": " And so we can just, we can first of all just print those out.", "tokens": [400, 370, 321, 393, 445, 11, 321, 393, 700, 295, 439, 445, 4482, 729, 484, 13], "temperature": 0.0, "avg_logprob": -0.17598902384440104, "compression_ratio": 1.768421052631579, "no_speech_prob": 5.5073105613701046e-06}, {"id": 100, "seek": 44576, "start": 453.71999999999997, "end": 458.92, "text": " And we can see that the means and standard deviations are not zero one.", "tokens": [400, 321, 393, 536, 300, 264, 1355, 293, 3832, 31219, 763, 366, 406, 4018, 472, 13], "temperature": 0.0, "avg_logprob": -0.17598902384440104, "compression_ratio": 1.768421052631579, "no_speech_prob": 5.5073105613701046e-06}, {"id": 101, "seek": 44576, "start": 458.92, "end": 462.06, "text": " The means are too high, right?", "tokens": [440, 1355, 366, 886, 1090, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.17598902384440104, "compression_ratio": 1.768421052631579, "no_speech_prob": 5.5073105613701046e-06}, {"id": 102, "seek": 44576, "start": 462.06, "end": 469.52, "text": " As we know, because we've got the relus and the standard deviations are too low.", "tokens": [1018, 321, 458, 11, 570, 321, 600, 658, 264, 1039, 301, 293, 264, 3832, 31219, 763, 366, 886, 2295, 13], "temperature": 0.0, "avg_logprob": -0.17598902384440104, "compression_ratio": 1.768421052631579, "no_speech_prob": 5.5073105613701046e-06}, {"id": 103, "seek": 46952, "start": 469.52, "end": 477.79999999999995, "text": " So rather than coming up with our perfect in it, instead, we just create a loop.", "tokens": [407, 2831, 813, 1348, 493, 365, 527, 2176, 294, 309, 11, 2602, 11, 321, 445, 1884, 257, 6367, 13], "temperature": 0.0, "avg_logprob": -0.15064481588510367, "compression_ratio": 1.6176470588235294, "no_speech_prob": 5.337927632353967e-06}, {"id": 104, "seek": 46952, "start": 477.79999999999995, "end": 483.44, "text": " And the loop calls the model passing in that mini batch we have, right?", "tokens": [400, 264, 6367, 5498, 264, 2316, 8437, 294, 300, 8382, 15245, 321, 362, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.15064481588510367, "compression_ratio": 1.6176470588235294, "no_speech_prob": 5.337927632353967e-06}, {"id": 105, "seek": 46952, "start": 483.44, "end": 488.79999999999995, "text": " And remember this is, so first of all, we hook it, right?", "tokens": [400, 1604, 341, 307, 11, 370, 700, 295, 439, 11, 321, 6328, 309, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.15064481588510367, "compression_ratio": 1.6176470588235294, "no_speech_prob": 5.337927632353967e-06}, {"id": 106, "seek": 46952, "start": 488.79999999999995, "end": 492.84, "text": " And then we call the model in a while loop.", "tokens": [400, 550, 321, 818, 264, 2316, 294, 257, 1339, 6367, 13], "temperature": 0.0, "avg_logprob": -0.15064481588510367, "compression_ratio": 1.6176470588235294, "no_speech_prob": 5.337927632353967e-06}, {"id": 107, "seek": 46952, "start": 492.84, "end": 497.84, "text": " We check whether the mean, the absolute value of the mean is close to zero.", "tokens": [492, 1520, 1968, 264, 914, 11, 264, 8236, 2158, 295, 264, 914, 307, 1998, 281, 4018, 13], "temperature": 0.0, "avg_logprob": -0.15064481588510367, "compression_ratio": 1.6176470588235294, "no_speech_prob": 5.337927632353967e-06}, {"id": 108, "seek": 49784, "start": 497.84, "end": 501.9, "text": " And if it's not, we subtract the mean from the bias.", "tokens": [400, 498, 309, 311, 406, 11, 321, 16390, 264, 914, 490, 264, 12577, 13], "temperature": 0.0, "avg_logprob": -0.1011969477859969, "compression_ratio": 1.7685185185185186, "no_speech_prob": 4.565669769363012e-06}, {"id": 109, "seek": 49784, "start": 501.9, "end": 506.2, "text": " And so it just keeps looping through, calling the model again with the hook, subtracting", "tokens": [400, 370, 309, 445, 5965, 6367, 278, 807, 11, 5141, 264, 2316, 797, 365, 264, 6328, 11, 16390, 278], "temperature": 0.0, "avg_logprob": -0.1011969477859969, "compression_ratio": 1.7685185185185186, "no_speech_prob": 4.565669769363012e-06}, {"id": 110, "seek": 49784, "start": 506.2, "end": 510.2, "text": " from the bias until we get about zero mean.", "tokens": [490, 264, 12577, 1826, 321, 483, 466, 4018, 914, 13], "temperature": 0.0, "avg_logprob": -0.1011969477859969, "compression_ratio": 1.7685185185185186, "no_speech_prob": 4.565669769363012e-06}, {"id": 111, "seek": 49784, "start": 510.2, "end": 513.0, "text": " And then we do the same thing for the standard deviation.", "tokens": [400, 550, 321, 360, 264, 912, 551, 337, 264, 3832, 25163, 13], "temperature": 0.0, "avg_logprob": -0.1011969477859969, "compression_ratio": 1.7685185185185186, "no_speech_prob": 4.565669769363012e-06}, {"id": 112, "seek": 49784, "start": 513.0, "end": 517.3199999999999, "text": " Keep checking whether standard deviation minus one is nearly zero.", "tokens": [5527, 8568, 1968, 3832, 25163, 3175, 472, 307, 6217, 4018, 13], "temperature": 0.0, "avg_logprob": -0.1011969477859969, "compression_ratio": 1.7685185185185186, "no_speech_prob": 4.565669769363012e-06}, {"id": 113, "seek": 49784, "start": 517.3199999999999, "end": 522.16, "text": " And as long as it isn't, we'll keep dividing by the standard deviation.", "tokens": [400, 382, 938, 382, 309, 1943, 380, 11, 321, 603, 1066, 26764, 538, 264, 3832, 25163, 13], "temperature": 0.0, "avg_logprob": -0.1011969477859969, "compression_ratio": 1.7685185185185186, "no_speech_prob": 4.565669769363012e-06}, {"id": 114, "seek": 52216, "start": 522.16, "end": 532.7199999999999, "text": " And so those two loops, if we run this function, then it's going to eventually give us what", "tokens": [400, 370, 729, 732, 16121, 11, 498, 321, 1190, 341, 2445, 11, 550, 309, 311, 516, 281, 4728, 976, 505, 437], "temperature": 0.0, "avg_logprob": -0.14310555798666819, "compression_ratio": 1.764957264957265, "no_speech_prob": 3.844880666292738e-06}, {"id": 115, "seek": 52216, "start": 532.7199999999999, "end": 533.7199999999999, "text": " we want.", "tokens": [321, 528, 13], "temperature": 0.0, "avg_logprob": -0.14310555798666819, "compression_ratio": 1.764957264957265, "no_speech_prob": 3.844880666292738e-06}, {"id": 116, "seek": 52216, "start": 533.7199999999999, "end": 535.3199999999999, "text": " Now it's not perfect, right?", "tokens": [823, 309, 311, 406, 2176, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.14310555798666819, "compression_ratio": 1.764957264957265, "no_speech_prob": 3.844880666292738e-06}, {"id": 117, "seek": 52216, "start": 535.3199999999999, "end": 540.24, "text": " The means are still not quite zero because we do the means first and then the standard", "tokens": [440, 1355, 366, 920, 406, 1596, 4018, 570, 321, 360, 264, 1355, 700, 293, 550, 264, 3832], "temperature": 0.0, "avg_logprob": -0.14310555798666819, "compression_ratio": 1.764957264957265, "no_speech_prob": 3.844880666292738e-06}, {"id": 118, "seek": 52216, "start": 540.24, "end": 541.24, "text": " deviations.", "tokens": [31219, 763, 13], "temperature": 0.0, "avg_logprob": -0.14310555798666819, "compression_ratio": 1.764957264957265, "no_speech_prob": 3.844880666292738e-06}, {"id": 119, "seek": 52216, "start": 541.24, "end": 543.36, "text": " And the standard deviation changes, we'll slightly change the mean.", "tokens": [400, 264, 3832, 25163, 2962, 11, 321, 603, 4748, 1319, 264, 914, 13], "temperature": 0.0, "avg_logprob": -0.14310555798666819, "compression_ratio": 1.764957264957265, "no_speech_prob": 3.844880666292738e-06}, {"id": 120, "seek": 52216, "start": 543.36, "end": 547.6999999999999, "text": " But you can see our means are, standard deviations are perfectly one.", "tokens": [583, 291, 393, 536, 527, 1355, 366, 11, 3832, 31219, 763, 366, 6239, 472, 13], "temperature": 0.0, "avg_logprob": -0.14310555798666819, "compression_ratio": 1.764957264957265, "no_speech_prob": 3.844880666292738e-06}, {"id": 121, "seek": 52216, "start": 547.6999999999999, "end": 550.64, "text": " And our means are pretty close.", "tokens": [400, 527, 1355, 366, 1238, 1998, 13], "temperature": 0.0, "avg_logprob": -0.14310555798666819, "compression_ratio": 1.764957264957265, "no_speech_prob": 3.844880666292738e-06}, {"id": 122, "seek": 52216, "start": 550.64, "end": 551.64, "text": " And that's it.", "tokens": [400, 300, 311, 309, 13], "temperature": 0.0, "avg_logprob": -0.14310555798666819, "compression_ratio": 1.764957264957265, "no_speech_prob": 3.844880666292738e-06}, {"id": 123, "seek": 55164, "start": 551.64, "end": 553.08, "text": " This is called LSUV.", "tokens": [639, 307, 1219, 441, 20214, 53, 13], "temperature": 0.0, "avg_logprob": -0.21108111628779658, "compression_ratio": 1.6196581196581197, "no_speech_prob": 6.96175857228809e-06}, {"id": 124, "seek": 55164, "start": 553.08, "end": 560.52, "text": " And this is how, without thinking at all, you can initialize any neural network, pretty", "tokens": [400, 341, 307, 577, 11, 1553, 1953, 412, 439, 11, 291, 393, 5883, 1125, 604, 18161, 3209, 11, 1238], "temperature": 0.0, "avg_logprob": -0.21108111628779658, "compression_ratio": 1.6196581196581197, "no_speech_prob": 6.96175857228809e-06}, {"id": 125, "seek": 55164, "start": 560.52, "end": 564.52, "text": " much, to get the unit variance all the way through.", "tokens": [709, 11, 281, 483, 264, 4985, 21977, 439, 264, 636, 807, 13], "temperature": 0.0, "avg_logprob": -0.21108111628779658, "compression_ratio": 1.6196581196581197, "no_speech_prob": 6.96175857228809e-06}, {"id": 126, "seek": 55164, "start": 564.52, "end": 571.04, "text": " And this is much easier than having to think about whether you've got ReLU or ELU or whether", "tokens": [400, 341, 307, 709, 3571, 813, 1419, 281, 519, 466, 1968, 291, 600, 658, 1300, 43, 52, 420, 14426, 52, 420, 1968], "temperature": 0.0, "avg_logprob": -0.21108111628779658, "compression_ratio": 1.6196581196581197, "no_speech_prob": 6.96175857228809e-06}, {"id": 127, "seek": 55164, "start": 571.04, "end": 575.12, "text": " you've got dropout or whatever else.", "tokens": [291, 600, 658, 3270, 346, 420, 2035, 1646, 13], "temperature": 0.0, "avg_logprob": -0.21108111628779658, "compression_ratio": 1.6196581196581197, "no_speech_prob": 6.96175857228809e-06}, {"id": 128, "seek": 55164, "start": 575.12, "end": 576.6, "text": " So here's a super cool trick.", "tokens": [407, 510, 311, 257, 1687, 1627, 4282, 13], "temperature": 0.0, "avg_logprob": -0.21108111628779658, "compression_ratio": 1.6196581196581197, "no_speech_prob": 6.96175857228809e-06}, {"id": 129, "seek": 55164, "start": 576.6, "end": 580.84, "text": " Yeah, and then we can train it, and it trains very nicely.", "tokens": [865, 11, 293, 550, 321, 393, 3847, 309, 11, 293, 309, 16329, 588, 9594, 13], "temperature": 0.0, "avg_logprob": -0.21108111628779658, "compression_ratio": 1.6196581196581197, "no_speech_prob": 6.96175857228809e-06}, {"id": 130, "seek": 58084, "start": 580.84, "end": 584.44, "text": " Particularly useful for complex and deeper architectures.", "tokens": [32281, 4420, 337, 3997, 293, 7731, 6331, 1303, 13], "temperature": 0.0, "avg_logprob": -0.16595253141799776, "compression_ratio": 1.5082644628099173, "no_speech_prob": 2.5609322165109916e-06}, {"id": 131, "seek": 58084, "start": 584.44, "end": 592.08, "text": " So there's kind of, for me, the fast AI approach to initializing your neural nets, which is", "tokens": [407, 456, 311, 733, 295, 11, 337, 385, 11, 264, 2370, 7318, 3109, 281, 5883, 3319, 428, 18161, 36170, 11, 597, 307], "temperature": 0.0, "avg_logprob": -0.16595253141799776, "compression_ratio": 1.5082644628099173, "no_speech_prob": 2.5609322165109916e-06}, {"id": 132, "seek": 58084, "start": 592.08, "end": 598.64, "text": " no math, no thinking, just a simple little for loop, or in this case, a while loop.", "tokens": [572, 5221, 11, 572, 1953, 11, 445, 257, 2199, 707, 337, 6367, 11, 420, 294, 341, 1389, 11, 257, 1339, 6367, 13], "temperature": 0.0, "avg_logprob": -0.16595253141799776, "compression_ratio": 1.5082644628099173, "no_speech_prob": 2.5609322165109916e-06}, {"id": 133, "seek": 58084, "start": 598.64, "end": 599.64, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.16595253141799776, "compression_ratio": 1.5082644628099173, "no_speech_prob": 2.5609322165109916e-06}, {"id": 134, "seek": 58084, "start": 599.64, "end": 606.4000000000001, "text": " So I think we've done enough with MNIST because we're getting really good results.", "tokens": [407, 286, 519, 321, 600, 1096, 1547, 365, 376, 45, 19756, 570, 321, 434, 1242, 534, 665, 3542, 13], "temperature": 0.0, "avg_logprob": -0.16595253141799776, "compression_ratio": 1.5082644628099173, "no_speech_prob": 2.5609322165109916e-06}, {"id": 135, "seek": 58084, "start": 606.4000000000001, "end": 607.4000000000001, "text": " It's running fast.", "tokens": [467, 311, 2614, 2370, 13], "temperature": 0.0, "avg_logprob": -0.16595253141799776, "compression_ratio": 1.5082644628099173, "no_speech_prob": 2.5609322165109916e-06}, {"id": 136, "seek": 58084, "start": 607.4000000000001, "end": 608.4000000000001, "text": " It's looking good.", "tokens": [467, 311, 1237, 665, 13], "temperature": 0.0, "avg_logprob": -0.16595253141799776, "compression_ratio": 1.5082644628099173, "no_speech_prob": 2.5609322165109916e-06}, {"id": 137, "seek": 60840, "start": 608.4, "end": 612.28, "text": " It's looking good, but it's looking a little bit harder.", "tokens": [467, 311, 1237, 665, 11, 457, 309, 311, 1237, 257, 707, 857, 6081, 13], "temperature": 0.0, "avg_logprob": -0.2255324313515111, "compression_ratio": 1.6374045801526718, "no_speech_prob": 7.296070634765783e-06}, {"id": 138, "seek": 60840, "start": 612.28, "end": 613.68, "text": " So what are we going to try?", "tokens": [407, 437, 366, 321, 516, 281, 853, 30], "temperature": 0.0, "avg_logprob": -0.2255324313515111, "compression_ratio": 1.6374045801526718, "no_speech_prob": 7.296070634765783e-06}, {"id": 139, "seek": 60840, "start": 613.68, "end": 618.92, "text": " Well, we're not quite ready to try ImageNet because ImageNet takes quite a lot of time,", "tokens": [1042, 11, 321, 434, 406, 1596, 1919, 281, 853, 29903, 31890, 570, 29903, 31890, 2516, 1596, 257, 688, 295, 565, 11], "temperature": 0.0, "avg_logprob": -0.2255324313515111, "compression_ratio": 1.6374045801526718, "no_speech_prob": 7.296070634765783e-06}, {"id": 140, "seek": 60840, "start": 618.92, "end": 622.48, "text": " you know, a few days if you've got just one GPU to train.", "tokens": [291, 458, 11, 257, 1326, 1708, 498, 291, 600, 658, 445, 472, 18407, 281, 3847, 13], "temperature": 0.0, "avg_logprob": -0.2255324313515111, "compression_ratio": 1.6374045801526718, "no_speech_prob": 7.296070634765783e-06}, {"id": 141, "seek": 60840, "start": 622.48, "end": 628.0799999999999, "text": " And that's really frustrating and an expensive way to try to practice things or learn things", "tokens": [400, 300, 311, 534, 16522, 293, 364, 5124, 636, 281, 853, 281, 3124, 721, 420, 1466, 721], "temperature": 0.0, "avg_logprob": -0.2255324313515111, "compression_ratio": 1.6374045801526718, "no_speech_prob": 7.296070634765783e-06}, {"id": 142, "seek": 60840, "start": 628.0799999999999, "end": 630.68, "text": " or try things out.", "tokens": [420, 853, 721, 484, 13], "temperature": 0.0, "avg_logprob": -0.2255324313515111, "compression_ratio": 1.6374045801526718, "no_speech_prob": 7.296070634765783e-06}, {"id": 143, "seek": 60840, "start": 630.68, "end": 636.16, "text": " I kept finding this problem of not knowing what data set I should try for my research", "tokens": [286, 4305, 5006, 341, 1154, 295, 406, 5276, 437, 1412, 992, 286, 820, 853, 337, 452, 2132], "temperature": 0.0, "avg_logprob": -0.2255324313515111, "compression_ratio": 1.6374045801526718, "no_speech_prob": 7.296070634765783e-06}, {"id": 144, "seek": 63616, "start": 636.16, "end": 639.0, "text": " or for my practice or for my learning.", "tokens": [420, 337, 452, 3124, 420, 337, 452, 2539, 13], "temperature": 0.0, "avg_logprob": -0.1299957275390625, "compression_ratio": 1.6480263157894737, "no_speech_prob": 3.5907029086956754e-05}, {"id": 145, "seek": 63616, "start": 639.0, "end": 642.4399999999999, "text": " You know, it seemed like at one end there was MNIST, which is kind of too easy.", "tokens": [509, 458, 11, 309, 6576, 411, 412, 472, 917, 456, 390, 376, 45, 19756, 11, 597, 307, 733, 295, 886, 1858, 13], "temperature": 0.0, "avg_logprob": -0.1299957275390625, "compression_ratio": 1.6480263157894737, "no_speech_prob": 3.5907029086956754e-05}, {"id": 146, "seek": 63616, "start": 642.4399999999999, "end": 647.8, "text": " There was Sci-Fi 10 that a lot of people use, but these are 32 by 32 pixel images.", "tokens": [821, 390, 16942, 12, 13229, 1266, 300, 257, 688, 295, 561, 764, 11, 457, 613, 366, 8858, 538, 8858, 19261, 5267, 13], "temperature": 0.0, "avg_logprob": -0.1299957275390625, "compression_ratio": 1.6480263157894737, "no_speech_prob": 3.5907029086956754e-05}, {"id": 147, "seek": 63616, "start": 647.8, "end": 652.0, "text": " And it turns out, and this is something I haven't seen really well written about, but", "tokens": [400, 309, 4523, 484, 11, 293, 341, 307, 746, 286, 2378, 380, 1612, 534, 731, 3720, 466, 11, 457], "temperature": 0.0, "avg_logprob": -0.1299957275390625, "compression_ratio": 1.6480263157894737, "no_speech_prob": 3.5907029086956754e-05}, {"id": 148, "seek": 63616, "start": 652.0, "end": 657.86, "text": " our research clearly shows, it turns out that small images, 32 by 32, have very different", "tokens": [527, 2132, 4448, 3110, 11, 309, 4523, 484, 300, 1359, 5267, 11, 8858, 538, 8858, 11, 362, 588, 819], "temperature": 0.0, "avg_logprob": -0.1299957275390625, "compression_ratio": 1.6480263157894737, "no_speech_prob": 3.5907029086956754e-05}, {"id": 149, "seek": 63616, "start": 657.86, "end": 660.92, "text": " characteristics to larger images.", "tokens": [10891, 281, 4833, 5267, 13], "temperature": 0.0, "avg_logprob": -0.1299957275390625, "compression_ratio": 1.6480263157894737, "no_speech_prob": 3.5907029086956754e-05}, {"id": 150, "seek": 63616, "start": 660.92, "end": 665.56, "text": " And specifically, it seems like once you get beneath about 96 by 96, things behave really", "tokens": [400, 4682, 11, 309, 2544, 411, 1564, 291, 483, 17149, 466, 24124, 538, 24124, 11, 721, 15158, 534], "temperature": 0.0, "avg_logprob": -0.1299957275390625, "compression_ratio": 1.6480263157894737, "no_speech_prob": 3.5907029086956754e-05}, {"id": 151, "seek": 66556, "start": 665.56, "end": 666.56, "text": " differently.", "tokens": [7614, 13], "temperature": 0.0, "avg_logprob": -0.13797029955633755, "compression_ratio": 1.72, "no_speech_prob": 2.9021127829764737e-06}, {"id": 152, "seek": 66556, "start": 666.56, "end": 672.0, "text": " So stuff that works well on Sci-Fi 10 tends not to work well on normal sized images, because", "tokens": [407, 1507, 300, 1985, 731, 322, 16942, 12, 13229, 1266, 12258, 406, 281, 589, 731, 322, 2710, 20004, 5267, 11, 570], "temperature": 0.0, "avg_logprob": -0.13797029955633755, "compression_ratio": 1.72, "no_speech_prob": 2.9021127829764737e-06}, {"id": 153, "seek": 66556, "start": 672.0, "end": 674.8399999999999, "text": " 32 by 32 is tiny, right?", "tokens": [8858, 538, 8858, 307, 5870, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.13797029955633755, "compression_ratio": 1.72, "no_speech_prob": 2.9021127829764737e-06}, {"id": 154, "seek": 66556, "start": 674.8399999999999, "end": 680.4399999999999, "text": " And stuff that tends to work well on Sci-Fi 10 doesn't necessarily work well on ImageNet.", "tokens": [400, 1507, 300, 12258, 281, 589, 731, 322, 16942, 12, 13229, 1266, 1177, 380, 4725, 589, 731, 322, 29903, 31890, 13], "temperature": 0.0, "avg_logprob": -0.13797029955633755, "compression_ratio": 1.72, "no_speech_prob": 2.9021127829764737e-06}, {"id": 155, "seek": 66556, "start": 680.4399999999999, "end": 686.76, "text": " There's this kind of gap of like something with, you know, with normal sized images,", "tokens": [821, 311, 341, 733, 295, 7417, 295, 411, 746, 365, 11, 291, 458, 11, 365, 2710, 20004, 5267, 11], "temperature": 0.0, "avg_logprob": -0.13797029955633755, "compression_ratio": 1.72, "no_speech_prob": 2.9021127829764737e-06}, {"id": 156, "seek": 66556, "start": 686.76, "end": 692.3599999999999, "text": " which I can train in a sane amount of time, but also gives me a good sense of whether", "tokens": [597, 286, 393, 3847, 294, 257, 45610, 2372, 295, 565, 11, 457, 611, 2709, 385, 257, 665, 2020, 295, 1968], "temperature": 0.0, "avg_logprob": -0.13797029955633755, "compression_ratio": 1.72, "no_speech_prob": 2.9021127829764737e-06}, {"id": 157, "seek": 66556, "start": 692.3599999999999, "end": 694.28, "text": " something's going to work well or not.", "tokens": [746, 311, 516, 281, 589, 731, 420, 406, 13], "temperature": 0.0, "avg_logprob": -0.13797029955633755, "compression_ratio": 1.72, "no_speech_prob": 2.9021127829764737e-06}, {"id": 158, "seek": 69428, "start": 694.28, "end": 700.9599999999999, "text": " And actually, Dmitro, who wrote that LSUV paper we just looked at, also had a fantastic", "tokens": [400, 767, 11, 413, 3508, 340, 11, 567, 4114, 300, 441, 20214, 53, 3035, 321, 445, 2956, 412, 11, 611, 632, 257, 5456], "temperature": 0.0, "avg_logprob": -0.12350277065001812, "compression_ratio": 1.6184738955823292, "no_speech_prob": 6.96161487212521e-06}, {"id": 159, "seek": 69428, "start": 700.9599999999999, "end": 706.8, "text": " paper called systematic evaluation, something like systematic evaluation of convolutional", "tokens": [3035, 1219, 27249, 13344, 11, 746, 411, 27249, 13344, 295, 45216, 304], "temperature": 0.0, "avg_logprob": -0.12350277065001812, "compression_ratio": 1.6184738955823292, "no_speech_prob": 6.96161487212521e-06}, {"id": 160, "seek": 69428, "start": 706.8, "end": 708.12, "text": " neural networks.", "tokens": [18161, 9590, 13], "temperature": 0.0, "avg_logprob": -0.12350277065001812, "compression_ratio": 1.6184738955823292, "no_speech_prob": 6.96161487212521e-06}, {"id": 161, "seek": 69428, "start": 708.12, "end": 715.64, "text": " And he noticed that if you use 128 by 128 images with ImageNet, then the kind of things", "tokens": [400, 415, 5694, 300, 498, 291, 764, 29810, 538, 29810, 5267, 365, 29903, 31890, 11, 550, 264, 733, 295, 721], "temperature": 0.0, "avg_logprob": -0.12350277065001812, "compression_ratio": 1.6184738955823292, "no_speech_prob": 6.96161487212521e-06}, {"id": 162, "seek": 69428, "start": 715.64, "end": 721.1999999999999, "text": " that he found works well or doesn't work well, all of those discoveries applied equally well", "tokens": [300, 415, 1352, 1985, 731, 420, 1177, 380, 589, 731, 11, 439, 295, 729, 28400, 6456, 12309, 731], "temperature": 0.0, "avg_logprob": -0.12350277065001812, "compression_ratio": 1.6184738955823292, "no_speech_prob": 6.96161487212521e-06}, {"id": 163, "seek": 69428, "start": 721.1999999999999, "end": 722.9599999999999, "text": " to the full sized ImageNet.", "tokens": [281, 264, 1577, 20004, 29903, 31890, 13], "temperature": 0.0, "avg_logprob": -0.12350277065001812, "compression_ratio": 1.6184738955823292, "no_speech_prob": 6.96161487212521e-06}, {"id": 164, "seek": 72296, "start": 722.96, "end": 729.72, "text": " It still takes too long, 128 by 128 for 1.5, well, 1.3 million images, still too long.", "tokens": [467, 920, 2516, 886, 938, 11, 29810, 538, 29810, 337, 502, 13, 20, 11, 731, 11, 502, 13, 18, 2459, 5267, 11, 920, 886, 938, 13], "temperature": 0.0, "avg_logprob": -0.15126157880903365, "compression_ratio": 1.6416666666666666, "no_speech_prob": 2.0143634174019098e-05}, {"id": 165, "seek": 72296, "start": 729.72, "end": 732.8000000000001, "text": " So I thought that was a good step, but I wanted to go even further.", "tokens": [407, 286, 1194, 300, 390, 257, 665, 1823, 11, 457, 286, 1415, 281, 352, 754, 3052, 13], "temperature": 0.0, "avg_logprob": -0.15126157880903365, "compression_ratio": 1.6416666666666666, "no_speech_prob": 2.0143634174019098e-05}, {"id": 166, "seek": 72296, "start": 732.8000000000001, "end": 737.08, "text": " So I tried creating two new datasets.", "tokens": [407, 286, 3031, 4084, 732, 777, 42856, 13], "temperature": 0.0, "avg_logprob": -0.15126157880903365, "compression_ratio": 1.6416666666666666, "no_speech_prob": 2.0143634174019098e-05}, {"id": 167, "seek": 72296, "start": 737.08, "end": 744.08, "text": " And my two new datasets are subsets of ImageNet, and there's kind of like multiple versions", "tokens": [400, 452, 732, 777, 42856, 366, 2090, 1385, 295, 29903, 31890, 11, 293, 456, 311, 733, 295, 411, 3866, 9606], "temperature": 0.0, "avg_logprob": -0.15126157880903365, "compression_ratio": 1.6416666666666666, "no_speech_prob": 2.0143634174019098e-05}, {"id": 168, "seek": 72296, "start": 744.08, "end": 747.14, "text": " in here, really, but they're both subsets of ImageNet.", "tokens": [294, 510, 11, 534, 11, 457, 436, 434, 1293, 2090, 1385, 295, 29903, 31890, 13], "temperature": 0.0, "avg_logprob": -0.15126157880903365, "compression_ratio": 1.6416666666666666, "no_speech_prob": 2.0143634174019098e-05}, {"id": 169, "seek": 72296, "start": 747.14, "end": 750.1, "text": " They both contain just 10 classes out of the thousand.", "tokens": [814, 1293, 5304, 445, 1266, 5359, 484, 295, 264, 4714, 13], "temperature": 0.0, "avg_logprob": -0.15126157880903365, "compression_ratio": 1.6416666666666666, "no_speech_prob": 2.0143634174019098e-05}, {"id": 170, "seek": 75010, "start": 750.1, "end": 754.12, "text": " So they're 1, 100th of the number of images of ImageNet.", "tokens": [407, 436, 434, 502, 11, 2319, 392, 295, 264, 1230, 295, 5267, 295, 29903, 31890, 13], "temperature": 0.0, "avg_logprob": -0.16040953079072556, "compression_ratio": 1.5555555555555556, "no_speech_prob": 2.5215006189682754e-06}, {"id": 171, "seek": 75010, "start": 754.12, "end": 761.12, "text": " And I create a number of versions, full size, 320 pixel size, and 160 pixel size.", "tokens": [400, 286, 1884, 257, 1230, 295, 9606, 11, 1577, 2744, 11, 42429, 19261, 2744, 11, 293, 21243, 19261, 2744, 13], "temperature": 0.0, "avg_logprob": -0.16040953079072556, "compression_ratio": 1.5555555555555556, "no_speech_prob": 2.5215006189682754e-06}, {"id": 172, "seek": 75010, "start": 761.12, "end": 764.28, "text": " One dataset is specifically designed to be easy.", "tokens": [1485, 28872, 307, 4682, 4761, 281, 312, 1858, 13], "temperature": 0.0, "avg_logprob": -0.16040953079072556, "compression_ratio": 1.5555555555555556, "no_speech_prob": 2.5215006189682754e-06}, {"id": 173, "seek": 75010, "start": 764.28, "end": 770.28, "text": " It contains 10 classes that are all very different to each other.", "tokens": [467, 8306, 1266, 5359, 300, 366, 439, 588, 819, 281, 1184, 661, 13], "temperature": 0.0, "avg_logprob": -0.16040953079072556, "compression_ratio": 1.5555555555555556, "no_speech_prob": 2.5215006189682754e-06}, {"id": 174, "seek": 75010, "start": 770.28, "end": 771.6800000000001, "text": " So this is like my starting point.", "tokens": [407, 341, 307, 411, 452, 2891, 935, 13], "temperature": 0.0, "avg_logprob": -0.16040953079072556, "compression_ratio": 1.5555555555555556, "no_speech_prob": 2.5215006189682754e-06}, {"id": 175, "seek": 75010, "start": 771.6800000000001, "end": 775.76, "text": " I thought, well, what if I create this dataset, then maybe I could train it for like just", "tokens": [286, 1194, 11, 731, 11, 437, 498, 286, 1884, 341, 28872, 11, 550, 1310, 286, 727, 3847, 309, 337, 411, 445], "temperature": 0.0, "avg_logprob": -0.16040953079072556, "compression_ratio": 1.5555555555555556, "no_speech_prob": 2.5215006189682754e-06}, {"id": 176, "seek": 77576, "start": 775.76, "end": 781.76, "text": " an epoch or two, like just a couple of minutes, and see whether something was going to work.", "tokens": [364, 30992, 339, 420, 732, 11, 411, 445, 257, 1916, 295, 2077, 11, 293, 536, 1968, 746, 390, 516, 281, 589, 13], "temperature": 0.0, "avg_logprob": -0.11424680189652876, "compression_ratio": 1.5850622406639003, "no_speech_prob": 1.362945022265194e-05}, {"id": 177, "seek": 77576, "start": 781.76, "end": 788.3199999999999, "text": " And then the second one I created was one designed to be hard, which is 10 categories", "tokens": [400, 550, 264, 1150, 472, 286, 2942, 390, 472, 4761, 281, 312, 1152, 11, 597, 307, 1266, 10479], "temperature": 0.0, "avg_logprob": -0.11424680189652876, "compression_ratio": 1.5850622406639003, "no_speech_prob": 1.362945022265194e-05}, {"id": 178, "seek": 77576, "start": 788.3199999999999, "end": 790.08, "text": " are designed to be very similar to each other.", "tokens": [366, 4761, 281, 312, 588, 2531, 281, 1184, 661, 13], "temperature": 0.0, "avg_logprob": -0.11424680189652876, "compression_ratio": 1.5850622406639003, "no_speech_prob": 1.362945022265194e-05}, {"id": 179, "seek": 77576, "start": 790.08, "end": 792.14, "text": " So they're all dog breeds.", "tokens": [407, 436, 434, 439, 3000, 41609, 13], "temperature": 0.0, "avg_logprob": -0.11424680189652876, "compression_ratio": 1.5850622406639003, "no_speech_prob": 1.362945022265194e-05}, {"id": 180, "seek": 77576, "start": 792.14, "end": 799.12, "text": " So the first dataset is called ImageNet, which is very French, as you can hear.", "tokens": [407, 264, 700, 28872, 307, 1219, 29903, 31890, 11, 597, 307, 588, 5522, 11, 382, 291, 393, 1568, 13], "temperature": 0.0, "avg_logprob": -0.11424680189652876, "compression_ratio": 1.5850622406639003, "no_speech_prob": 1.362945022265194e-05}, {"id": 181, "seek": 77576, "start": 799.12, "end": 802.3199999999999, "text": " And there's some helpful pronunciation tips here.", "tokens": [400, 456, 311, 512, 4961, 23338, 6082, 510, 13], "temperature": 0.0, "avg_logprob": -0.11424680189652876, "compression_ratio": 1.5850622406639003, "no_speech_prob": 1.362945022265194e-05}, {"id": 182, "seek": 80232, "start": 802.32, "end": 806.24, "text": " And the second is called ImageWolf.", "tokens": [400, 264, 1150, 307, 1219, 29903, 54, 7491, 13], "temperature": 0.0, "avg_logprob": -0.08807483037312826, "compression_ratio": 1.513089005235602, "no_speech_prob": 1.9221799448132515e-05}, {"id": 183, "seek": 80232, "start": 806.24, "end": 814.6600000000001, "text": " And you can see here I've created a leaderboard for ImageNet and for ImageWolf.", "tokens": [400, 291, 393, 536, 510, 286, 600, 2942, 257, 5263, 3787, 337, 29903, 31890, 293, 337, 29903, 54, 7491, 13], "temperature": 0.0, "avg_logprob": -0.08807483037312826, "compression_ratio": 1.513089005235602, "no_speech_prob": 1.9221799448132515e-05}, {"id": 184, "seek": 80232, "start": 814.6600000000001, "end": 820.48, "text": " And I've discovered that in my very quick experiments with this, the exact observations", "tokens": [400, 286, 600, 6941, 300, 294, 452, 588, 1702, 12050, 365, 341, 11, 264, 1900, 18163], "temperature": 0.0, "avg_logprob": -0.08807483037312826, "compression_ratio": 1.513089005235602, "no_speech_prob": 1.9221799448132515e-05}, {"id": 185, "seek": 80232, "start": 820.48, "end": 826.8000000000001, "text": " I find about what works well for the full ImageNet, also I see the same results here.", "tokens": [286, 915, 466, 437, 1985, 731, 337, 264, 1577, 29903, 31890, 11, 611, 286, 536, 264, 912, 3542, 510, 13], "temperature": 0.0, "avg_logprob": -0.08807483037312826, "compression_ratio": 1.513089005235602, "no_speech_prob": 1.9221799448132515e-05}, {"id": 186, "seek": 82680, "start": 826.8, "end": 834.68, "text": " It is also fascinating to see how some things are the same between the two datasets, and", "tokens": [467, 307, 611, 10343, 281, 536, 577, 512, 721, 366, 264, 912, 1296, 264, 732, 42856, 11, 293], "temperature": 0.0, "avg_logprob": -0.12307448719823083, "compression_ratio": 1.6044444444444443, "no_speech_prob": 4.4251737563172355e-06}, {"id": 187, "seek": 82680, "start": 834.68, "end": 835.9599999999999, "text": " some are different.", "tokens": [512, 366, 819, 13], "temperature": 0.0, "avg_logprob": -0.12307448719823083, "compression_ratio": 1.6044444444444443, "no_speech_prob": 4.4251737563172355e-06}, {"id": 188, "seek": 82680, "start": 835.9599999999999, "end": 843.2199999999999, "text": " And I found working with these two datasets has given me more insight into computer vision", "tokens": [400, 286, 1352, 1364, 365, 613, 732, 42856, 575, 2212, 385, 544, 11269, 666, 3820, 5201], "temperature": 0.0, "avg_logprob": -0.12307448719823083, "compression_ratio": 1.6044444444444443, "no_speech_prob": 4.4251737563172355e-06}, {"id": 189, "seek": 82680, "start": 843.2199999999999, "end": 847.6999999999999, "text": " model training than anything else that I've done.", "tokens": [2316, 3097, 813, 1340, 1646, 300, 286, 600, 1096, 13], "temperature": 0.0, "avg_logprob": -0.12307448719823083, "compression_ratio": 1.6044444444444443, "no_speech_prob": 4.4251737563172355e-06}, {"id": 190, "seek": 82680, "start": 847.6999999999999, "end": 849.64, "text": " So check them out.", "tokens": [407, 1520, 552, 484, 13], "temperature": 0.0, "avg_logprob": -0.12307448719823083, "compression_ratio": 1.6044444444444443, "no_speech_prob": 4.4251737563172355e-06}, {"id": 191, "seek": 82680, "start": 849.64, "end": 855.88, "text": " And I really wanted to mention this to say a big part of getting good at using deep learning", "tokens": [400, 286, 534, 1415, 281, 2152, 341, 281, 584, 257, 955, 644, 295, 1242, 665, 412, 1228, 2452, 2539], "temperature": 0.0, "avg_logprob": -0.12307448719823083, "compression_ratio": 1.6044444444444443, "no_speech_prob": 4.4251737563172355e-06}, {"id": 192, "seek": 85588, "start": 855.88, "end": 863.36, "text": " in your domain is knowing how to create small, workable, useful datasets.", "tokens": [294, 428, 9274, 307, 5276, 577, 281, 1884, 1359, 11, 589, 712, 11, 4420, 42856, 13], "temperature": 0.0, "avg_logprob": -0.18171975322972947, "compression_ratio": 1.639344262295082, "no_speech_prob": 1.9518050976330414e-05}, {"id": 193, "seek": 85588, "start": 863.36, "end": 866.08, "text": " So once I decided to make this, it took me about three hours.", "tokens": [407, 1564, 286, 3047, 281, 652, 341, 11, 309, 1890, 385, 466, 1045, 2496, 13], "temperature": 0.0, "avg_logprob": -0.18171975322972947, "compression_ratio": 1.639344262295082, "no_speech_prob": 1.9518050976330414e-05}, {"id": 194, "seek": 85588, "start": 866.08, "end": 868.88, "text": " It's not at all hard to create a dataset.", "tokens": [467, 311, 406, 412, 439, 1152, 281, 1884, 257, 28872, 13], "temperature": 0.0, "avg_logprob": -0.18171975322972947, "compression_ratio": 1.639344262295082, "no_speech_prob": 1.9518050976330414e-05}, {"id": 195, "seek": 85588, "start": 868.88, "end": 873.16, "text": " It's a quick little Python script to grab the things I wanted.", "tokens": [467, 311, 257, 1702, 707, 15329, 5755, 281, 4444, 264, 721, 286, 1415, 13], "temperature": 0.0, "avg_logprob": -0.18171975322972947, "compression_ratio": 1.639344262295082, "no_speech_prob": 1.9518050976330414e-05}, {"id": 196, "seek": 85588, "start": 873.16, "end": 876.68, "text": " How did I decide which 10 things?", "tokens": [1012, 630, 286, 4536, 597, 1266, 721, 30], "temperature": 0.0, "avg_logprob": -0.18171975322972947, "compression_ratio": 1.639344262295082, "no_speech_prob": 1.9518050976330414e-05}, {"id": 197, "seek": 85588, "start": 876.68, "end": 880.48, "text": " I just looked at a list of categories and picked 10 things that I knew were different.", "tokens": [286, 445, 2956, 412, 257, 1329, 295, 10479, 293, 6183, 1266, 721, 300, 286, 2586, 645, 819, 13], "temperature": 0.0, "avg_logprob": -0.18171975322972947, "compression_ratio": 1.639344262295082, "no_speech_prob": 1.9518050976330414e-05}, {"id": 198, "seek": 85588, "start": 880.48, "end": 882.56, "text": " How did I decide to pick these things?", "tokens": [1012, 630, 286, 4536, 281, 1888, 613, 721, 30], "temperature": 0.0, "avg_logprob": -0.18171975322972947, "compression_ratio": 1.639344262295082, "no_speech_prob": 1.9518050976330414e-05}, {"id": 199, "seek": 88256, "start": 882.56, "end": 886.4799999999999, "text": " I just looked at 10 things that I knew were dogs.", "tokens": [286, 445, 2956, 412, 1266, 721, 300, 286, 2586, 645, 7197, 13], "temperature": 0.0, "avg_logprob": -0.14345300574051706, "compression_ratio": 1.553648068669528, "no_speech_prob": 4.934776370646432e-06}, {"id": 200, "seek": 88256, "start": 886.4799999999999, "end": 893.04, "text": " So it's like, throw something together, get it working, and then on your domain area,", "tokens": [407, 309, 311, 411, 11, 3507, 746, 1214, 11, 483, 309, 1364, 11, 293, 550, 322, 428, 9274, 1859, 11], "temperature": 0.0, "avg_logprob": -0.14345300574051706, "compression_ratio": 1.553648068669528, "no_speech_prob": 4.934776370646432e-06}, {"id": 201, "seek": 88256, "start": 893.04, "end": 901.8, "text": " whether it's audio or Sanskrit texts or whatever, or genomic sequences, try to come up with", "tokens": [1968, 309, 311, 6278, 420, 44392, 15765, 420, 2035, 11, 420, 1049, 21401, 22978, 11, 853, 281, 808, 493, 365], "temperature": 0.0, "avg_logprob": -0.14345300574051706, "compression_ratio": 1.553648068669528, "no_speech_prob": 4.934776370646432e-06}, {"id": 202, "seek": 88256, "start": 901.8, "end": 906.5999999999999, "text": " your version of a toy problem or two, which you hope might give insight into your full", "tokens": [428, 3037, 295, 257, 12058, 1154, 420, 732, 11, 597, 291, 1454, 1062, 976, 11269, 666, 428, 1577], "temperature": 0.0, "avg_logprob": -0.14345300574051706, "compression_ratio": 1.553648068669528, "no_speech_prob": 4.934776370646432e-06}, {"id": 203, "seek": 88256, "start": 906.5999999999999, "end": 907.5999999999999, "text": " problem.", "tokens": [1154, 13], "temperature": 0.0, "avg_logprob": -0.14345300574051706, "compression_ratio": 1.553648068669528, "no_speech_prob": 4.934776370646432e-06}, {"id": 204, "seek": 88256, "start": 907.5999999999999, "end": 909.68, "text": " So this has been super helpful for me.", "tokens": [407, 341, 575, 668, 1687, 4961, 337, 385, 13], "temperature": 0.0, "avg_logprob": -0.14345300574051706, "compression_ratio": 1.553648068669528, "no_speech_prob": 4.934776370646432e-06}, {"id": 205, "seek": 90968, "start": 909.68, "end": 913.76, "text": " And if you're interested in computer vision, I would strongly recommend trying this out.", "tokens": [400, 498, 291, 434, 3102, 294, 3820, 5201, 11, 286, 576, 10613, 2748, 1382, 341, 484, 13], "temperature": 0.0, "avg_logprob": -0.11252936137091253, "compression_ratio": 1.6443514644351465, "no_speech_prob": 8.396666999033187e-06}, {"id": 206, "seek": 90968, "start": 913.76, "end": 915.64, "text": " And specifically, try to beat me, right?", "tokens": [400, 4682, 11, 853, 281, 4224, 385, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.11252936137091253, "compression_ratio": 1.6443514644351465, "no_speech_prob": 8.396666999033187e-06}, {"id": 207, "seek": 90968, "start": 915.64, "end": 919.8399999999999, "text": " Because trying to beat me, and these are not great, they're just okay, but trying to beat", "tokens": [1436, 1382, 281, 4224, 385, 11, 293, 613, 366, 406, 869, 11, 436, 434, 445, 1392, 11, 457, 1382, 281, 4224], "temperature": 0.0, "avg_logprob": -0.11252936137091253, "compression_ratio": 1.6443514644351465, "no_speech_prob": 8.396666999033187e-06}, {"id": 208, "seek": 90968, "start": 919.8399999999999, "end": 926.4799999999999, "text": " me will give you a sense of whether the things you're thinking about are in the ballpark", "tokens": [385, 486, 976, 291, 257, 2020, 295, 1968, 264, 721, 291, 434, 1953, 466, 366, 294, 264, 2594, 31239], "temperature": 0.0, "avg_logprob": -0.11252936137091253, "compression_ratio": 1.6443514644351465, "no_speech_prob": 8.396666999033187e-06}, {"id": 209, "seek": 90968, "start": 926.4799999999999, "end": 932.9599999999999, "text": " of what a moderately competent practitioner is able to do in a small amount of time.", "tokens": [295, 437, 257, 10494, 1592, 29998, 32125, 307, 1075, 281, 360, 294, 257, 1359, 2372, 295, 565, 13], "temperature": 0.0, "avg_logprob": -0.11252936137091253, "compression_ratio": 1.6443514644351465, "no_speech_prob": 8.396666999033187e-06}, {"id": 210, "seek": 93296, "start": 932.96, "end": 940.08, "text": " It's also interesting to see that with a 1 one hundredth the size of ImageNet, like a", "tokens": [467, 311, 611, 1880, 281, 536, 300, 365, 257, 502, 472, 3262, 392, 264, 2744, 295, 29903, 31890, 11, 411, 257], "temperature": 0.0, "avg_logprob": -0.18668127841636784, "compression_ratio": 1.575091575091575, "no_speech_prob": 2.1232783637969987e-06}, {"id": 211, "seek": 93296, "start": 940.08, "end": 945.12, "text": " tiny data set, I was able to create a 90% accurate dog breed classifier from random", "tokens": [5870, 1412, 992, 11, 286, 390, 1075, 281, 1884, 257, 4289, 4, 8559, 3000, 18971, 1508, 9902, 490, 4974], "temperature": 0.0, "avg_logprob": -0.18668127841636784, "compression_ratio": 1.575091575091575, "no_speech_prob": 2.1232783637969987e-06}, {"id": 212, "seek": 93296, "start": 945.12, "end": 946.12, "text": " weights.", "tokens": [17443, 13], "temperature": 0.0, "avg_logprob": -0.18668127841636784, "compression_ratio": 1.575091575091575, "no_speech_prob": 2.1232783637969987e-06}, {"id": 213, "seek": 93296, "start": 946.12, "end": 951.4000000000001, "text": " So you can do a lot pretty quickly without much data, even if you don't have transfer", "tokens": [407, 291, 393, 360, 257, 688, 1238, 2661, 1553, 709, 1412, 11, 754, 498, 291, 500, 380, 362, 5003], "temperature": 0.0, "avg_logprob": -0.18668127841636784, "compression_ratio": 1.575091575091575, "no_speech_prob": 2.1232783637969987e-06}, {"id": 214, "seek": 93296, "start": 951.4000000000001, "end": 953.96, "text": " learning, which is kind of amazing.", "tokens": [2539, 11, 597, 307, 733, 295, 2243, 13], "temperature": 0.0, "avg_logprob": -0.18668127841636784, "compression_ratio": 1.575091575091575, "no_speech_prob": 2.1232783637969987e-06}, {"id": 215, "seek": 93296, "start": 953.96, "end": 955.8000000000001, "text": " So we're going to use this data set now.", "tokens": [407, 321, 434, 516, 281, 764, 341, 1412, 992, 586, 13], "temperature": 0.0, "avg_logprob": -0.18668127841636784, "compression_ratio": 1.575091575091575, "no_speech_prob": 2.1232783637969987e-06}, {"id": 216, "seek": 93296, "start": 955.8000000000001, "end": 958.08, "text": " Oh, sorry, you had a question.", "tokens": [876, 11, 2597, 11, 291, 632, 257, 1168, 13], "temperature": 0.0, "avg_logprob": -0.18668127841636784, "compression_ratio": 1.575091575091575, "no_speech_prob": 2.1232783637969987e-06}, {"id": 217, "seek": 93296, "start": 958.08, "end": 961.6800000000001, "text": " So before we look at the data set, let's do the question.", "tokens": [407, 949, 321, 574, 412, 264, 1412, 992, 11, 718, 311, 360, 264, 1168, 13], "temperature": 0.0, "avg_logprob": -0.18668127841636784, "compression_ratio": 1.575091575091575, "no_speech_prob": 2.1232783637969987e-06}, {"id": 218, "seek": 96168, "start": 961.68, "end": 970.04, "text": " So just to confirm, LSUV is something you run on all the layers once at the beginning,", "tokens": [407, 445, 281, 9064, 11, 441, 20214, 53, 307, 746, 291, 1190, 322, 439, 264, 7914, 1564, 412, 264, 2863, 11], "temperature": 0.0, "avg_logprob": -0.1588679070168353, "compression_ratio": 1.6047619047619048, "no_speech_prob": 2.177363603550475e-05}, {"id": 219, "seek": 96168, "start": 970.04, "end": 971.4799999999999, "text": " not during training.", "tokens": [406, 1830, 3097, 13], "temperature": 0.0, "avg_logprob": -0.1588679070168353, "compression_ratio": 1.6047619047619048, "no_speech_prob": 2.177363603550475e-05}, {"id": 220, "seek": 96168, "start": 971.4799999999999, "end": 973.0, "text": " What if your batch size is small?", "tokens": [708, 498, 428, 15245, 2744, 307, 1359, 30], "temperature": 0.0, "avg_logprob": -0.1588679070168353, "compression_ratio": 1.6047619047619048, "no_speech_prob": 2.177363603550475e-05}, {"id": 221, "seek": 96168, "start": 973.0, "end": 975.88, "text": " Could you overfit to that batch?", "tokens": [7497, 291, 670, 6845, 281, 300, 15245, 30], "temperature": 0.0, "avg_logprob": -0.1588679070168353, "compression_ratio": 1.6047619047619048, "no_speech_prob": 2.177363603550475e-05}, {"id": 222, "seek": 96168, "start": 975.88, "end": 979.8, "text": " Yeah, that's right.", "tokens": [865, 11, 300, 311, 558, 13], "temperature": 0.0, "avg_logprob": -0.1588679070168353, "compression_ratio": 1.6047619047619048, "no_speech_prob": 2.177363603550475e-05}, {"id": 223, "seek": 96168, "start": 979.8, "end": 985.16, "text": " So you'd run it once at the start of training to initialize your weights, just so that that", "tokens": [407, 291, 1116, 1190, 309, 1564, 412, 264, 722, 295, 3097, 281, 5883, 1125, 428, 17443, 11, 445, 370, 300, 300], "temperature": 0.0, "avg_logprob": -0.1588679070168353, "compression_ratio": 1.6047619047619048, "no_speech_prob": 2.177363603550475e-05}, {"id": 224, "seek": 96168, "start": 985.16, "end": 989.3199999999999, "text": " initial set of steps gives you sensible gradients.", "tokens": [5883, 992, 295, 4439, 2709, 291, 25380, 2771, 2448, 13], "temperature": 0.0, "avg_logprob": -0.1588679070168353, "compression_ratio": 1.6047619047619048, "no_speech_prob": 2.177363603550475e-05}, {"id": 225, "seek": 98932, "start": 989.32, "end": 992.48, "text": " Because it's those first few mini batches that are everything.", "tokens": [1436, 309, 311, 729, 700, 1326, 8382, 15245, 279, 300, 366, 1203, 13], "temperature": 0.0, "avg_logprob": -0.11971083683754082, "compression_ratio": 1.7307692307692308, "no_speech_prob": 2.3178608898888342e-05}, {"id": 226, "seek": 98932, "start": 992.48, "end": 997.12, "text": " Remember how we saw that if we didn't have a very good first few mini batches that we", "tokens": [5459, 577, 321, 1866, 300, 498, 321, 994, 380, 362, 257, 588, 665, 700, 1326, 8382, 15245, 279, 300, 321], "temperature": 0.0, "avg_logprob": -0.11971083683754082, "compression_ratio": 1.7307692307692308, "no_speech_prob": 2.3178608898888342e-05}, {"id": 227, "seek": 98932, "start": 997.12, "end": 1003.36, "text": " ended up with 90% of the activations being inactive?", "tokens": [4590, 493, 365, 4289, 4, 295, 264, 2430, 763, 885, 294, 12596, 30], "temperature": 0.0, "avg_logprob": -0.11971083683754082, "compression_ratio": 1.7307692307692308, "no_speech_prob": 2.3178608898888342e-05}, {"id": 228, "seek": 98932, "start": 1003.36, "end": 1007.24, "text": " So that's why we want to make sure we start well.", "tokens": [407, 300, 311, 983, 321, 528, 281, 652, 988, 321, 722, 731, 13], "temperature": 0.0, "avg_logprob": -0.11971083683754082, "compression_ratio": 1.7307692307692308, "no_speech_prob": 2.3178608898888342e-05}, {"id": 229, "seek": 98932, "start": 1007.24, "end": 1012.0, "text": " And yeah, if you've got a small mini batch, just run five mini batches and take the mean.", "tokens": [400, 1338, 11, 498, 291, 600, 658, 257, 1359, 8382, 15245, 11, 445, 1190, 1732, 8382, 15245, 279, 293, 747, 264, 914, 13], "temperature": 0.0, "avg_logprob": -0.11971083683754082, "compression_ratio": 1.7307692307692308, "no_speech_prob": 2.3178608898888342e-05}, {"id": 230, "seek": 98932, "start": 1012.0, "end": 1013.72, "text": " There's nothing special about the one mini batch.", "tokens": [821, 311, 1825, 2121, 466, 264, 472, 8382, 15245, 13], "temperature": 0.0, "avg_logprob": -0.11971083683754082, "compression_ratio": 1.7307692307692308, "no_speech_prob": 2.3178608898888342e-05}, {"id": 231, "seek": 98932, "start": 1013.72, "end": 1016.32, "text": " It's just a fast way to do the computation.", "tokens": [467, 311, 445, 257, 2370, 636, 281, 360, 264, 24903, 13], "temperature": 0.0, "avg_logprob": -0.11971083683754082, "compression_ratio": 1.7307692307692308, "no_speech_prob": 2.3178608898888342e-05}, {"id": 232, "seek": 98932, "start": 1016.32, "end": 1018.8800000000001, "text": " It's not like we're doing any gradient descent or anything.", "tokens": [467, 311, 406, 411, 321, 434, 884, 604, 16235, 23475, 420, 1340, 13], "temperature": 0.0, "avg_logprob": -0.11971083683754082, "compression_ratio": 1.7307692307692308, "no_speech_prob": 2.3178608898888342e-05}, {"id": 233, "seek": 101888, "start": 1018.88, "end": 1020.48, "text": " It's just a forward pass.", "tokens": [467, 311, 445, 257, 2128, 1320, 13], "temperature": 0.0, "avg_logprob": -0.1389358436668312, "compression_ratio": 1.5260416666666667, "no_speech_prob": 3.137840622002841e-06}, {"id": 234, "seek": 101888, "start": 1020.48, "end": 1025.24, "text": " Thanks, that was a good question.", "tokens": [2561, 11, 300, 390, 257, 665, 1168, 13], "temperature": 0.0, "avg_logprob": -0.1389358436668312, "compression_ratio": 1.5260416666666667, "no_speech_prob": 3.137840622002841e-06}, {"id": 235, "seek": 101888, "start": 1025.24, "end": 1036.36, "text": " So ImageNet is too big to read it all into RAM at once.", "tokens": [407, 29903, 31890, 307, 886, 955, 281, 1401, 309, 439, 666, 14561, 412, 1564, 13], "temperature": 0.0, "avg_logprob": -0.1389358436668312, "compression_ratio": 1.5260416666666667, "no_speech_prob": 3.137840622002841e-06}, {"id": 236, "seek": 101888, "start": 1036.36, "end": 1038.32, "text": " It's not huge, but it's too big to do that.", "tokens": [467, 311, 406, 2603, 11, 457, 309, 311, 886, 955, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.1389358436668312, "compression_ratio": 1.5260416666666667, "no_speech_prob": 3.137840622002841e-06}, {"id": 237, "seek": 101888, "start": 1038.32, "end": 1042.2, "text": " So we're going to need to be able to read it in one image at a time, which is going", "tokens": [407, 321, 434, 516, 281, 643, 281, 312, 1075, 281, 1401, 309, 294, 472, 3256, 412, 257, 565, 11, 597, 307, 516], "temperature": 0.0, "avg_logprob": -0.1389358436668312, "compression_ratio": 1.5260416666666667, "no_speech_prob": 3.137840622002841e-06}, {"id": 238, "seek": 101888, "start": 1042.2, "end": 1044.88, "text": " to be true of most of our deep learning projects.", "tokens": [281, 312, 2074, 295, 881, 295, 527, 2452, 2539, 4455, 13], "temperature": 0.0, "avg_logprob": -0.1389358436668312, "compression_ratio": 1.5260416666666667, "no_speech_prob": 3.137840622002841e-06}, {"id": 239, "seek": 104488, "start": 1044.88, "end": 1051.1200000000001, "text": " So we need some way to do that from scratch, because that's the rules.", "tokens": [407, 321, 643, 512, 636, 281, 360, 300, 490, 8459, 11, 570, 300, 311, 264, 4474, 13], "temperature": 0.0, "avg_logprob": -0.10464452845709664, "compression_ratio": 1.7327935222672064, "no_speech_prob": 1.2407833310135175e-05}, {"id": 240, "seek": 104488, "start": 1051.1200000000001, "end": 1053.44, "text": " So let's start working through that process.", "tokens": [407, 718, 311, 722, 1364, 807, 300, 1399, 13], "temperature": 0.0, "avg_logprob": -0.10464452845709664, "compression_ratio": 1.7327935222672064, "no_speech_prob": 1.2407833310135175e-05}, {"id": 241, "seek": 104488, "start": 1053.44, "end": 1058.1200000000001, "text": " And in the process, we're going to end up building a data block API, which you're all", "tokens": [400, 294, 264, 1399, 11, 321, 434, 516, 281, 917, 493, 2390, 257, 1412, 3461, 9362, 11, 597, 291, 434, 439], "temperature": 0.0, "avg_logprob": -0.10464452845709664, "compression_ratio": 1.7327935222672064, "no_speech_prob": 1.2407833310135175e-05}, {"id": 242, "seek": 104488, "start": 1058.1200000000001, "end": 1059.24, "text": " familiar with.", "tokens": [4963, 365, 13], "temperature": 0.0, "avg_logprob": -0.10464452845709664, "compression_ratio": 1.7327935222672064, "no_speech_prob": 1.2407833310135175e-05}, {"id": 243, "seek": 104488, "start": 1059.24, "end": 1065.96, "text": " But most people using the data block API feel familiar enough with it to do small tweaks", "tokens": [583, 881, 561, 1228, 264, 1412, 3461, 9362, 841, 4963, 1547, 365, 309, 281, 360, 1359, 46664], "temperature": 0.0, "avg_logprob": -0.10464452845709664, "compression_ratio": 1.7327935222672064, "no_speech_prob": 1.2407833310135175e-05}, {"id": 244, "seek": 104488, "start": 1065.96, "end": 1068.24, "text": " for things that they kind of know they can do.", "tokens": [337, 721, 300, 436, 733, 295, 458, 436, 393, 360, 13], "temperature": 0.0, "avg_logprob": -0.10464452845709664, "compression_ratio": 1.7327935222672064, "no_speech_prob": 1.2407833310135175e-05}, {"id": 245, "seek": 104488, "start": 1068.24, "end": 1074.16, "text": " But most people I speak to don't know how to really change what's going on.", "tokens": [583, 881, 561, 286, 1710, 281, 500, 380, 458, 577, 281, 534, 1319, 437, 311, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.10464452845709664, "compression_ratio": 1.7327935222672064, "no_speech_prob": 1.2407833310135175e-05}, {"id": 246, "seek": 107416, "start": 1074.16, "end": 1079.72, "text": " So by the end of this notebook, you'll see how incredibly simple the data block API is.", "tokens": [407, 538, 264, 917, 295, 341, 21060, 11, 291, 603, 536, 577, 6252, 2199, 264, 1412, 3461, 9362, 307, 13], "temperature": 0.0, "avg_logprob": -0.09074758228502776, "compression_ratio": 1.5852534562211982, "no_speech_prob": 2.354208118049428e-05}, {"id": 247, "seek": 107416, "start": 1079.72, "end": 1084.3600000000001, "text": " And you'll be able to either write your own, maybe based on this one, or modify the one", "tokens": [400, 291, 603, 312, 1075, 281, 2139, 2464, 428, 1065, 11, 1310, 2361, 322, 341, 472, 11, 420, 16927, 264, 472], "temperature": 0.0, "avg_logprob": -0.09074758228502776, "compression_ratio": 1.5852534562211982, "no_speech_prob": 2.354208118049428e-05}, {"id": 248, "seek": 107416, "start": 1084.3600000000001, "end": 1092.52, "text": " in Fast AI, because this is a very direct translation of the one that's in Fast AI.", "tokens": [294, 15968, 7318, 11, 570, 341, 307, 257, 588, 2047, 12853, 295, 264, 472, 300, 311, 294, 15968, 7318, 13], "temperature": 0.0, "avg_logprob": -0.09074758228502776, "compression_ratio": 1.5852534562211982, "no_speech_prob": 2.354208118049428e-05}, {"id": 249, "seek": 107416, "start": 1092.52, "end": 1095.76, "text": " So you should be able to get going.", "tokens": [407, 291, 820, 312, 1075, 281, 483, 516, 13], "temperature": 0.0, "avg_logprob": -0.09074758228502776, "compression_ratio": 1.5852534562211982, "no_speech_prob": 2.354208118049428e-05}, {"id": 250, "seek": 107416, "start": 1095.76, "end": 1099.8000000000002, "text": " So the first thing to do is to read in our data.", "tokens": [407, 264, 700, 551, 281, 360, 307, 281, 1401, 294, 527, 1412, 13], "temperature": 0.0, "avg_logprob": -0.09074758228502776, "compression_ratio": 1.5852534562211982, "no_speech_prob": 2.354208118049428e-05}, {"id": 251, "seek": 109980, "start": 1099.8, "end": 1104.32, "text": " And we'll see a similar thing when we build FastAI.audio.", "tokens": [400, 321, 603, 536, 257, 2531, 551, 562, 321, 1322, 15968, 48698, 13, 46069, 13], "temperature": 0.0, "avg_logprob": -0.13226417194713247, "compression_ratio": 1.5630252100840336, "no_speech_prob": 1.3628780834551435e-05}, {"id": 252, "seek": 109980, "start": 1104.32, "end": 1108.48, "text": " But whatever process you use, you're going to have to find some library that can read", "tokens": [583, 2035, 1399, 291, 764, 11, 291, 434, 516, 281, 362, 281, 915, 512, 6405, 300, 393, 1401], "temperature": 0.0, "avg_logprob": -0.13226417194713247, "compression_ratio": 1.5630252100840336, "no_speech_prob": 1.3628780834551435e-05}, {"id": 253, "seek": 109980, "start": 1108.48, "end": 1110.18, "text": " the kind of data that you want.", "tokens": [264, 733, 295, 1412, 300, 291, 528, 13], "temperature": 0.0, "avg_logprob": -0.13226417194713247, "compression_ratio": 1.5630252100840336, "no_speech_prob": 1.3628780834551435e-05}, {"id": 254, "seek": 109980, "start": 1110.18, "end": 1112.36, "text": " So in our case, we have images.", "tokens": [407, 294, 527, 1389, 11, 321, 362, 5267, 13], "temperature": 0.0, "avg_logprob": -0.13226417194713247, "compression_ratio": 1.5630252100840336, "no_speech_prob": 1.3628780834551435e-05}, {"id": 255, "seek": 109980, "start": 1112.36, "end": 1117.56, "text": " And there's a library called PIL, or Pillow, Python Imaging Library, which can read images.", "tokens": [400, 456, 311, 257, 6405, 1219, 430, 4620, 11, 420, 44656, 305, 11, 15329, 4331, 3568, 12806, 11, 597, 393, 1401, 5267, 13], "temperature": 0.0, "avg_logprob": -0.13226417194713247, "compression_ratio": 1.5630252100840336, "no_speech_prob": 1.3628780834551435e-05}, {"id": 256, "seek": 109980, "start": 1117.56, "end": 1119.72, "text": " So let's import it.", "tokens": [407, 718, 311, 974, 309, 13], "temperature": 0.0, "avg_logprob": -0.13226417194713247, "compression_ratio": 1.5630252100840336, "no_speech_prob": 1.3628780834551435e-05}, {"id": 257, "seek": 109980, "start": 1119.72, "end": 1124.04, "text": " We'll grab the data set and untie it, import Pillow.", "tokens": [492, 603, 4444, 264, 1412, 992, 293, 1701, 414, 309, 11, 974, 44656, 305, 13], "temperature": 0.0, "avg_logprob": -0.13226417194713247, "compression_ratio": 1.5630252100840336, "no_speech_prob": 1.3628780834551435e-05}, {"id": 258, "seek": 112404, "start": 1124.04, "end": 1131.1599999999999, "text": " And we want to see what's inside our ImageNet data set.", "tokens": [400, 321, 528, 281, 536, 437, 311, 1854, 527, 29903, 31890, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.09380204827935847, "compression_ratio": 1.53125, "no_speech_prob": 8.80050447449321e-06}, {"id": 259, "seek": 112404, "start": 1131.1599999999999, "end": 1134.6399999999999, "text": " Typing list x.editor is far too complicated for me.", "tokens": [5569, 3381, 1329, 2031, 13, 292, 3029, 307, 1400, 886, 6179, 337, 385, 13], "temperature": 0.0, "avg_logprob": -0.09380204827935847, "compression_ratio": 1.53125, "no_speech_prob": 8.80050447449321e-06}, {"id": 260, "seek": 112404, "start": 1134.6399999999999, "end": 1136.26, "text": " I just want to type ls.", "tokens": [286, 445, 528, 281, 2010, 287, 82, 13], "temperature": 0.0, "avg_logprob": -0.09380204827935847, "compression_ratio": 1.53125, "no_speech_prob": 8.80050447449321e-06}, {"id": 261, "seek": 112404, "start": 1136.26, "end": 1137.36, "text": " So be lazy.", "tokens": [407, 312, 14847, 13], "temperature": 0.0, "avg_logprob": -0.09380204827935847, "compression_ratio": 1.53125, "no_speech_prob": 8.80050447449321e-06}, {"id": 262, "seek": 112404, "start": 1137.36, "end": 1140.68, "text": " This is how easy it is to add stuff to the standard library.", "tokens": [639, 307, 577, 1858, 309, 307, 281, 909, 1507, 281, 264, 3832, 6405, 13], "temperature": 0.0, "avg_logprob": -0.09380204827935847, "compression_ratio": 1.53125, "no_speech_prob": 8.80050447449321e-06}, {"id": 263, "seek": 112404, "start": 1140.68, "end": 1144.08, "text": " You can just take the class and add a function to it.", "tokens": [509, 393, 445, 747, 264, 1508, 293, 909, 257, 2445, 281, 309, 13], "temperature": 0.0, "avg_logprob": -0.09380204827935847, "compression_ratio": 1.53125, "no_speech_prob": 8.80050447449321e-06}, {"id": 264, "seek": 112404, "start": 1144.08, "end": 1146.96, "text": " So now we have ls.", "tokens": [407, 586, 321, 362, 287, 82, 13], "temperature": 0.0, "avg_logprob": -0.09380204827935847, "compression_ratio": 1.53125, "no_speech_prob": 8.80050447449321e-06}, {"id": 265, "seek": 112404, "start": 1146.96, "end": 1147.96, "text": " So here's ls.", "tokens": [407, 510, 311, 287, 82, 13], "temperature": 0.0, "avg_logprob": -0.09380204827935847, "compression_ratio": 1.53125, "no_speech_prob": 8.80050447449321e-06}, {"id": 266, "seek": 112404, "start": 1147.96, "end": 1151.84, "text": " So we've got a training and a validation directory.", "tokens": [407, 321, 600, 658, 257, 3097, 293, 257, 24071, 21120, 13], "temperature": 0.0, "avg_logprob": -0.09380204827935847, "compression_ratio": 1.53125, "no_speech_prob": 8.80050447449321e-06}, {"id": 267, "seek": 115184, "start": 1151.84, "end": 1157.6399999999999, "text": " In validation, we have one directory for each category.", "tokens": [682, 24071, 11, 321, 362, 472, 21120, 337, 1184, 7719, 13], "temperature": 0.0, "avg_logprob": -0.09498939357820105, "compression_ratio": 1.7469879518072289, "no_speech_prob": 2.9770984838251024e-05}, {"id": 268, "seek": 115184, "start": 1157.6399999999999, "end": 1162.6399999999999, "text": " And then if we look at one category, we could grab one file name.", "tokens": [400, 550, 498, 321, 574, 412, 472, 7719, 11, 321, 727, 4444, 472, 3991, 1315, 13], "temperature": 0.0, "avg_logprob": -0.09498939357820105, "compression_ratio": 1.7469879518072289, "no_speech_prob": 2.9770984838251024e-05}, {"id": 269, "seek": 115184, "start": 1162.6399999999999, "end": 1167.1599999999999, "text": " And if we look at one file name, we have a tench.", "tokens": [400, 498, 321, 574, 412, 472, 3991, 1315, 11, 321, 362, 257, 2064, 339, 13], "temperature": 0.0, "avg_logprob": -0.09498939357820105, "compression_ratio": 1.7469879518072289, "no_speech_prob": 2.9770984838251024e-05}, {"id": 270, "seek": 115184, "start": 1167.1599999999999, "end": 1171.36, "text": " So if you want to know whether somebody is actually a deep learning practitioner, show", "tokens": [407, 498, 291, 528, 281, 458, 1968, 2618, 307, 767, 257, 2452, 2539, 32125, 11, 855], "temperature": 0.0, "avg_logprob": -0.09498939357820105, "compression_ratio": 1.7469879518072289, "no_speech_prob": 2.9770984838251024e-05}, {"id": 271, "seek": 115184, "start": 1171.36, "end": 1172.74, "text": " them this photo.", "tokens": [552, 341, 5052, 13], "temperature": 0.0, "avg_logprob": -0.09498939357820105, "compression_ratio": 1.7469879518072289, "no_speech_prob": 2.9770984838251024e-05}, {"id": 272, "seek": 115184, "start": 1172.74, "end": 1176.28, "text": " If they don't know it's a tench, they're lying to you, because this is the first category", "tokens": [759, 436, 500, 380, 458, 309, 311, 257, 2064, 339, 11, 436, 434, 8493, 281, 291, 11, 570, 341, 307, 264, 700, 7719], "temperature": 0.0, "avg_logprob": -0.09498939357820105, "compression_ratio": 1.7469879518072289, "no_speech_prob": 2.9770984838251024e-05}, {"id": 273, "seek": 115184, "start": 1176.28, "end": 1177.28, "text": " in ImageNet.", "tokens": [294, 29903, 31890, 13], "temperature": 0.0, "avg_logprob": -0.09498939357820105, "compression_ratio": 1.7469879518072289, "no_speech_prob": 2.9770984838251024e-05}, {"id": 274, "seek": 115184, "start": 1177.28, "end": 1181.6799999999998, "text": " So if you're ever using ImageNet, you know your tenches.", "tokens": [407, 498, 291, 434, 1562, 1228, 29903, 31890, 11, 291, 458, 428, 2064, 3781, 13], "temperature": 0.0, "avg_logprob": -0.09498939357820105, "compression_ratio": 1.7469879518072289, "no_speech_prob": 2.9770984838251024e-05}, {"id": 275, "seek": 118168, "start": 1181.68, "end": 1188.5600000000002, "text": " They're generally being held up by middle-aged men, or sometimes they're in nets.", "tokens": [814, 434, 5101, 885, 5167, 493, 538, 2808, 12, 2980, 1706, 11, 420, 2171, 436, 434, 294, 36170, 13], "temperature": 0.0, "avg_logprob": -0.09180852345057897, "compression_ratio": 1.594488188976378, "no_speech_prob": 4.565829385683173e-06}, {"id": 276, "seek": 118168, "start": 1188.5600000000002, "end": 1192.9, "text": " That's pretty much how it always looks in ImageNet.", "tokens": [663, 311, 1238, 709, 577, 309, 1009, 1542, 294, 29903, 31890, 13], "temperature": 0.0, "avg_logprob": -0.09180852345057897, "compression_ratio": 1.594488188976378, "no_speech_prob": 4.565829385683173e-06}, {"id": 277, "seek": 118168, "start": 1192.9, "end": 1197.6000000000001, "text": " So that's why we have them in ImageNet too, because it's such a classic computer vision", "tokens": [407, 300, 311, 983, 321, 362, 552, 294, 29903, 31890, 886, 11, 570, 309, 311, 1270, 257, 7230, 3820, 5201], "temperature": 0.0, "avg_logprob": -0.09180852345057897, "compression_ratio": 1.594488188976378, "no_speech_prob": 4.565829385683173e-06}, {"id": 278, "seek": 118168, "start": 1197.6000000000001, "end": 1200.52, "text": " fish.", "tokens": [3506, 13], "temperature": 0.0, "avg_logprob": -0.09180852345057897, "compression_ratio": 1.594488188976378, "no_speech_prob": 4.565829385683173e-06}, {"id": 279, "seek": 118168, "start": 1200.52, "end": 1205.68, "text": " We're cheating in importing NumPy for a moment, just so I can show you what an image contains,", "tokens": [492, 434, 18309, 294, 43866, 22592, 47, 88, 337, 257, 1623, 11, 445, 370, 286, 393, 855, 291, 437, 364, 3256, 8306, 11], "temperature": 0.0, "avg_logprob": -0.09180852345057897, "compression_ratio": 1.594488188976378, "no_speech_prob": 4.565829385683173e-06}, {"id": 280, "seek": 118168, "start": 1205.68, "end": 1208.98, "text": " just to turn it into an array so I can print it for you.", "tokens": [445, 281, 1261, 309, 666, 364, 10225, 370, 286, 393, 4482, 309, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.09180852345057897, "compression_ratio": 1.594488188976378, "no_speech_prob": 4.565829385683173e-06}, {"id": 281, "seek": 118168, "start": 1208.98, "end": 1210.3600000000001, "text": " This is really important.", "tokens": [639, 307, 534, 1021, 13], "temperature": 0.0, "avg_logprob": -0.09180852345057897, "compression_ratio": 1.594488188976378, "no_speech_prob": 4.565829385683173e-06}, {"id": 282, "seek": 121036, "start": 1210.36, "end": 1211.9599999999998, "text": " It contains bytes.", "tokens": [467, 8306, 36088, 13], "temperature": 0.0, "avg_logprob": -0.20545899709065754, "compression_ratio": 1.5030674846625767, "no_speech_prob": 4.35675337939756e-06}, {"id": 283, "seek": 121036, "start": 1211.9599999999998, "end": 1216.52, "text": " It contains numbers between 0 and 255 that are integer.", "tokens": [467, 8306, 3547, 1296, 1958, 293, 3552, 20, 300, 366, 24922, 13], "temperature": 0.0, "avg_logprob": -0.20545899709065754, "compression_ratio": 1.5030674846625767, "no_speech_prob": 4.35675337939756e-06}, {"id": 284, "seek": 121036, "start": 1216.52, "end": 1218.3999999999999, "text": " They're not float.", "tokens": [814, 434, 406, 15706, 13], "temperature": 0.0, "avg_logprob": -0.20545899709065754, "compression_ratio": 1.5030674846625767, "no_speech_prob": 4.35675337939756e-06}, {"id": 285, "seek": 121036, "start": 1218.3999999999999, "end": 1224.3999999999999, "text": " So this is what we get when we load up an image.", "tokens": [407, 341, 307, 437, 321, 483, 562, 321, 3677, 493, 364, 3256, 13], "temperature": 0.0, "avg_logprob": -0.20545899709065754, "compression_ratio": 1.5030674846625767, "no_speech_prob": 4.35675337939756e-06}, {"id": 286, "seek": 121036, "start": 1224.3999999999999, "end": 1233.08, "text": " And it's got a geometry, and it's got a number of channels, and in this case it's RGB, three", "tokens": [400, 309, 311, 658, 257, 18426, 11, 293, 309, 311, 658, 257, 1230, 295, 9235, 11, 293, 294, 341, 1389, 309, 311, 31231, 11, 1045], "temperature": 0.0, "avg_logprob": -0.20545899709065754, "compression_ratio": 1.5030674846625767, "no_speech_prob": 4.35675337939756e-06}, {"id": 287, "seek": 121036, "start": 1233.08, "end": 1234.1599999999999, "text": " channels.", "tokens": [9235, 13], "temperature": 0.0, "avg_logprob": -0.20545899709065754, "compression_ratio": 1.5030674846625767, "no_speech_prob": 4.35675337939756e-06}, {"id": 288, "seek": 123416, "start": 1234.16, "end": 1240.6000000000001, "text": " So we want to have some way to read in lots of images, which means we need to know what", "tokens": [407, 321, 528, 281, 362, 512, 636, 281, 1401, 294, 3195, 295, 5267, 11, 597, 1355, 321, 643, 281, 458, 437], "temperature": 0.0, "avg_logprob": -0.09851931792039137, "compression_ratio": 1.8106060606060606, "no_speech_prob": 6.14389819020289e-06}, {"id": 289, "seek": 123416, "start": 1240.6000000000001, "end": 1244.3200000000002, "text": " images there are in this directory structure.", "tokens": [5267, 456, 366, 294, 341, 21120, 3877, 13], "temperature": 0.0, "avg_logprob": -0.09851931792039137, "compression_ratio": 1.8106060606060606, "no_speech_prob": 6.14389819020289e-06}, {"id": 290, "seek": 123416, "start": 1244.3200000000002, "end": 1247.8400000000001, "text": " And in the full ImageNet there's going to be 1.3 million of them, so we need to be able", "tokens": [400, 294, 264, 1577, 29903, 31890, 456, 311, 516, 281, 312, 502, 13, 18, 2459, 295, 552, 11, 370, 321, 643, 281, 312, 1075], "temperature": 0.0, "avg_logprob": -0.09851931792039137, "compression_ratio": 1.8106060606060606, "no_speech_prob": 6.14389819020289e-06}, {"id": 291, "seek": 123416, "start": 1247.8400000000001, "end": 1249.2, "text": " to do that fast.", "tokens": [281, 360, 300, 2370, 13], "temperature": 0.0, "avg_logprob": -0.09851931792039137, "compression_ratio": 1.8106060606060606, "no_speech_prob": 6.14389819020289e-06}, {"id": 292, "seek": 123416, "start": 1249.2, "end": 1252.5600000000002, "text": " So the first thing we need to know is which things are images.", "tokens": [407, 264, 700, 551, 321, 643, 281, 458, 307, 597, 721, 366, 5267, 13], "temperature": 0.0, "avg_logprob": -0.09851931792039137, "compression_ratio": 1.8106060606060606, "no_speech_prob": 6.14389819020289e-06}, {"id": 293, "seek": 123416, "start": 1252.5600000000002, "end": 1254.8000000000002, "text": " So we need a list of image extensions.", "tokens": [407, 321, 643, 257, 1329, 295, 3256, 25129, 13], "temperature": 0.0, "avg_logprob": -0.09851931792039137, "compression_ratio": 1.8106060606060606, "no_speech_prob": 6.14389819020289e-06}, {"id": 294, "seek": 123416, "start": 1254.8000000000002, "end": 1257.0800000000002, "text": " Your computer already has a list of image extensions.", "tokens": [2260, 3820, 1217, 575, 257, 1329, 295, 3256, 25129, 13], "temperature": 0.0, "avg_logprob": -0.09851931792039137, "compression_ratio": 1.8106060606060606, "no_speech_prob": 6.14389819020289e-06}, {"id": 295, "seek": 123416, "start": 1257.0800000000002, "end": 1263.0800000000002, "text": " It's your MIME types database, so you can query Python for your MIME types database", "tokens": [467, 311, 428, 376, 6324, 36, 3467, 8149, 11, 370, 291, 393, 14581, 15329, 337, 428, 376, 6324, 36, 3467, 8149], "temperature": 0.0, "avg_logprob": -0.09851931792039137, "compression_ratio": 1.8106060606060606, "no_speech_prob": 6.14389819020289e-06}, {"id": 296, "seek": 126308, "start": 1263.08, "end": 1265.1999999999998, "text": " for all of the images.", "tokens": [337, 439, 295, 264, 5267, 13], "temperature": 0.0, "avg_logprob": -0.10554067967301708, "compression_ratio": 1.7355371900826446, "no_speech_prob": 1.56883161253063e-05}, {"id": 297, "seek": 126308, "start": 1265.1999999999998, "end": 1270.04, "text": " So here's a list of the image extensions that my computer knows about.", "tokens": [407, 510, 311, 257, 1329, 295, 264, 3256, 25129, 300, 452, 3820, 3255, 466, 13], "temperature": 0.0, "avg_logprob": -0.10554067967301708, "compression_ratio": 1.7355371900826446, "no_speech_prob": 1.56883161253063e-05}, {"id": 298, "seek": 126308, "start": 1270.04, "end": 1275.9199999999998, "text": " So now what I want to do is I want to loop through all the files in a directory and find", "tokens": [407, 586, 437, 286, 528, 281, 360, 307, 286, 528, 281, 6367, 807, 439, 264, 7098, 294, 257, 21120, 293, 915], "temperature": 0.0, "avg_logprob": -0.10554067967301708, "compression_ratio": 1.7355371900826446, "no_speech_prob": 1.56883161253063e-05}, {"id": 299, "seek": 126308, "start": 1275.9199999999998, "end": 1278.6, "text": " out which ones are one of these.", "tokens": [484, 597, 2306, 366, 472, 295, 613, 13], "temperature": 0.0, "avg_logprob": -0.10554067967301708, "compression_ratio": 1.7355371900826446, "no_speech_prob": 1.56883161253063e-05}, {"id": 300, "seek": 126308, "start": 1278.6, "end": 1282.28, "text": " The fastest way to check whether something's in a list is to first of all turn it into", "tokens": [440, 14573, 636, 281, 1520, 1968, 746, 311, 294, 257, 1329, 307, 281, 700, 295, 439, 1261, 309, 666], "temperature": 0.0, "avg_logprob": -0.10554067967301708, "compression_ratio": 1.7355371900826446, "no_speech_prob": 1.56883161253063e-05}, {"id": 301, "seek": 126308, "start": 1282.28, "end": 1286.6799999999998, "text": " a set, and of course therefore we need Setify.", "tokens": [257, 992, 11, 293, 295, 1164, 4412, 321, 643, 8928, 2505, 13], "temperature": 0.0, "avg_logprob": -0.10554067967301708, "compression_ratio": 1.7355371900826446, "no_speech_prob": 1.56883161253063e-05}, {"id": 302, "seek": 126308, "start": 1286.6799999999998, "end": 1290.6, "text": " So Setify simply checks if it is a set, and if it is, it makes it one.", "tokens": [407, 8928, 2505, 2935, 13834, 498, 309, 307, 257, 992, 11, 293, 498, 309, 307, 11, 309, 1669, 309, 472, 13], "temperature": 0.0, "avg_logprob": -0.10554067967301708, "compression_ratio": 1.7355371900826446, "no_speech_prob": 1.56883161253063e-05}, {"id": 303, "seek": 129060, "start": 1290.6, "end": 1294.8, "text": " Otherwise it first turns it into a list and then turns it into a set.", "tokens": [10328, 309, 700, 4523, 309, 666, 257, 1329, 293, 550, 4523, 309, 666, 257, 992, 13], "temperature": 0.0, "avg_logprob": -0.12298121945611362, "compression_ratio": 1.6666666666666667, "no_speech_prob": 8.800636351224966e-06}, {"id": 304, "seek": 129060, "start": 1294.8, "end": 1297.08, "text": " So that's how we can setify things.", "tokens": [407, 300, 311, 577, 321, 393, 992, 2505, 721, 13], "temperature": 0.0, "avg_logprob": -0.12298121945611362, "compression_ratio": 1.6666666666666667, "no_speech_prob": 8.800636351224966e-06}, {"id": 305, "seek": 129060, "start": 1297.08, "end": 1299.8, "text": " And here's what I do when I build a little bit of functionality.", "tokens": [400, 510, 311, 437, 286, 360, 562, 286, 1322, 257, 707, 857, 295, 14980, 13], "temperature": 0.0, "avg_logprob": -0.12298121945611362, "compression_ratio": 1.6666666666666667, "no_speech_prob": 8.800636351224966e-06}, {"id": 306, "seek": 129060, "start": 1299.8, "end": 1304.4399999999998, "text": " I just throw together a quick bunch of tests to make sure it seems to be roughly doing", "tokens": [286, 445, 3507, 1214, 257, 1702, 3840, 295, 6921, 281, 652, 988, 309, 2544, 281, 312, 9810, 884], "temperature": 0.0, "avg_logprob": -0.12298121945611362, "compression_ratio": 1.6666666666666667, "no_speech_prob": 8.800636351224966e-06}, {"id": 307, "seek": 129060, "start": 1304.4399999999998, "end": 1305.4399999999998, "text": " the right thing.", "tokens": [264, 558, 551, 13], "temperature": 0.0, "avg_logprob": -0.12298121945611362, "compression_ratio": 1.6666666666666667, "no_speech_prob": 8.800636351224966e-06}, {"id": 308, "seek": 129060, "start": 1305.4399999999998, "end": 1311.36, "text": " And remember in lesson one we created our own test framework, so we can now run any", "tokens": [400, 1604, 294, 6898, 472, 321, 2942, 527, 1065, 1500, 8388, 11, 370, 321, 393, 586, 1190, 604], "temperature": 0.0, "avg_logprob": -0.12298121945611362, "compression_ratio": 1.6666666666666667, "no_speech_prob": 8.800636351224966e-06}, {"id": 309, "seek": 129060, "start": 1311.36, "end": 1317.6, "text": " notebook as a test suite, so it'll automatically check if we break this at some point.", "tokens": [21060, 382, 257, 1500, 14205, 11, 370, 309, 603, 6772, 1520, 498, 321, 1821, 341, 412, 512, 935, 13], "temperature": 0.0, "avg_logprob": -0.12298121945611362, "compression_ratio": 1.6666666666666667, "no_speech_prob": 8.800636351224966e-06}, {"id": 310, "seek": 131760, "start": 1317.6, "end": 1325.04, "text": " Okay, so now we need a way to go through a single directory and grab all of the images", "tokens": [1033, 11, 370, 586, 321, 643, 257, 636, 281, 352, 807, 257, 2167, 21120, 293, 4444, 439, 295, 264, 5267], "temperature": 0.0, "avg_logprob": -0.1367362992376344, "compression_ratio": 1.6877637130801688, "no_speech_prob": 1.0129624570254236e-05}, {"id": 311, "seek": 131760, "start": 1325.04, "end": 1326.04, "text": " in that.", "tokens": [294, 300, 13], "temperature": 0.0, "avg_logprob": -0.1367362992376344, "compression_ratio": 1.6877637130801688, "no_speech_prob": 1.0129624570254236e-05}, {"id": 312, "seek": 131760, "start": 1326.04, "end": 1329.3999999999999, "text": " So here we can say get files.", "tokens": [407, 510, 321, 393, 584, 483, 7098, 13], "temperature": 0.0, "avg_logprob": -0.1367362992376344, "compression_ratio": 1.6877637130801688, "no_speech_prob": 1.0129624570254236e-05}, {"id": 313, "seek": 131760, "start": 1329.3999999999999, "end": 1333.76, "text": " I always like to make sure that you can pass any of these things, either a path, lib path,", "tokens": [286, 1009, 411, 281, 652, 988, 300, 291, 393, 1320, 604, 295, 613, 721, 11, 2139, 257, 3100, 11, 22854, 3100, 11], "temperature": 0.0, "avg_logprob": -0.1367362992376344, "compression_ratio": 1.6877637130801688, "no_speech_prob": 1.0129624570254236e-05}, {"id": 314, "seek": 131760, "start": 1333.76, "end": 1336.32, "text": " or a string to make it convenient.", "tokens": [420, 257, 6798, 281, 652, 309, 10851, 13], "temperature": 0.0, "avg_logprob": -0.1367362992376344, "compression_ratio": 1.6877637130801688, "no_speech_prob": 1.0129624570254236e-05}, {"id": 315, "seek": 131760, "start": 1336.32, "end": 1341.9199999999998, "text": " So if you just say p equals path p, if it's already a path lib object, that doesn't do", "tokens": [407, 498, 291, 445, 584, 280, 6915, 3100, 280, 11, 498, 309, 311, 1217, 257, 3100, 22854, 2657, 11, 300, 1177, 380, 360], "temperature": 0.0, "avg_logprob": -0.1367362992376344, "compression_ratio": 1.6877637130801688, "no_speech_prob": 1.0129624570254236e-05}, {"id": 316, "seek": 131760, "start": 1341.9199999999998, "end": 1342.9199999999998, "text": " anything.", "tokens": [1340, 13], "temperature": 0.0, "avg_logprob": -0.1367362992376344, "compression_ratio": 1.6877637130801688, "no_speech_prob": 1.0129624570254236e-05}, {"id": 317, "seek": 131760, "start": 1342.9199999999998, "end": 1345.6799999999998, "text": " So this is a nice easy way to make sure that works.", "tokens": [407, 341, 307, 257, 1481, 1858, 636, 281, 652, 988, 300, 1985, 13], "temperature": 0.0, "avg_logprob": -0.1367362992376344, "compression_ratio": 1.6877637130801688, "no_speech_prob": 1.0129624570254236e-05}, {"id": 318, "seek": 134568, "start": 1345.68, "end": 1353.72, "text": " So we just go through, here's our path lib object.", "tokens": [407, 321, 445, 352, 807, 11, 510, 311, 527, 3100, 22854, 2657, 13], "temperature": 0.0, "avg_logprob": -0.12584324181079865, "compression_ratio": 1.8046875, "no_speech_prob": 6.7474693423719145e-06}, {"id": 319, "seek": 134568, "start": 1353.72, "end": 1357.2, "text": " And so you'll see in a moment how we actually grab the list of files, but this is our parent", "tokens": [400, 370, 291, 603, 536, 294, 257, 1623, 577, 321, 767, 4444, 264, 1329, 295, 7098, 11, 457, 341, 307, 527, 2596], "temperature": 0.0, "avg_logprob": -0.12584324181079865, "compression_ratio": 1.8046875, "no_speech_prob": 6.7474693423719145e-06}, {"id": 320, "seek": 134568, "start": 1357.2, "end": 1358.2, "text": " directory.", "tokens": [21120, 13], "temperature": 0.0, "avg_logprob": -0.12584324181079865, "compression_ratio": 1.8046875, "no_speech_prob": 6.7474693423719145e-06}, {"id": 321, "seek": 134568, "start": 1358.2, "end": 1359.74, "text": " This is going to be our list of files.", "tokens": [639, 307, 516, 281, 312, 527, 1329, 295, 7098, 13], "temperature": 0.0, "avg_logprob": -0.12584324181079865, "compression_ratio": 1.8046875, "no_speech_prob": 6.7474693423719145e-06}, {"id": 322, "seek": 134568, "start": 1359.74, "end": 1361.2, "text": " We go through the list of files.", "tokens": [492, 352, 807, 264, 1329, 295, 7098, 13], "temperature": 0.0, "avg_logprob": -0.12584324181079865, "compression_ratio": 1.8046875, "no_speech_prob": 6.7474693423719145e-06}, {"id": 323, "seek": 134568, "start": 1361.2, "end": 1362.64, "text": " We check that it doesn't start with dot.", "tokens": [492, 1520, 300, 309, 1177, 380, 722, 365, 5893, 13], "temperature": 0.0, "avg_logprob": -0.12584324181079865, "compression_ratio": 1.8046875, "no_speech_prob": 6.7474693423719145e-06}, {"id": 324, "seek": 134568, "start": 1362.64, "end": 1367.76, "text": " If it does, that's a Unix hidden file or Mac hidden file.", "tokens": [759, 309, 775, 11, 300, 311, 257, 1156, 970, 7633, 3991, 420, 5707, 7633, 3991, 13], "temperature": 0.0, "avg_logprob": -0.12584324181079865, "compression_ratio": 1.8046875, "no_speech_prob": 6.7474693423719145e-06}, {"id": 325, "seek": 134568, "start": 1367.76, "end": 1372.4, "text": " And we also check either they didn't ask for some particular extensions or that the extension", "tokens": [400, 321, 611, 1520, 2139, 436, 994, 380, 1029, 337, 512, 1729, 25129, 420, 300, 264, 10320], "temperature": 0.0, "avg_logprob": -0.12584324181079865, "compression_ratio": 1.8046875, "no_speech_prob": 6.7474693423719145e-06}, {"id": 326, "seek": 134568, "start": 1372.4, "end": 1375.4, "text": " is in the list of extensions we asked for.", "tokens": [307, 294, 264, 1329, 295, 25129, 321, 2351, 337, 13], "temperature": 0.0, "avg_logprob": -0.12584324181079865, "compression_ratio": 1.8046875, "no_speech_prob": 6.7474693423719145e-06}, {"id": 327, "seek": 137540, "start": 1375.4, "end": 1380.52, "text": " So that will allow us to grab just the image files.", "tokens": [407, 300, 486, 2089, 505, 281, 4444, 445, 264, 3256, 7098, 13], "temperature": 0.0, "avg_logprob": -0.14009951561996617, "compression_ratio": 1.7208121827411167, "no_speech_prob": 3.844874299829826e-06}, {"id": 328, "seek": 137540, "start": 1380.52, "end": 1385.0800000000002, "text": " Python has something called Scander, which will grab a path and list all of the files", "tokens": [15329, 575, 746, 1219, 2747, 4483, 11, 597, 486, 4444, 257, 3100, 293, 1329, 439, 295, 264, 7098], "temperature": 0.0, "avg_logprob": -0.14009951561996617, "compression_ratio": 1.7208121827411167, "no_speech_prob": 3.844874299829826e-06}, {"id": 329, "seek": 137540, "start": 1385.0800000000002, "end": 1386.24, "text": " in that path.", "tokens": [294, 300, 3100, 13], "temperature": 0.0, "avg_logprob": -0.14009951561996617, "compression_ratio": 1.7208121827411167, "no_speech_prob": 3.844874299829826e-06}, {"id": 330, "seek": 137540, "start": 1386.24, "end": 1388.92, "text": " So here is how we can call get files.", "tokens": [407, 510, 307, 577, 321, 393, 818, 483, 7098, 13], "temperature": 0.0, "avg_logprob": -0.14009951561996617, "compression_ratio": 1.7208121827411167, "no_speech_prob": 3.844874299829826e-06}, {"id": 331, "seek": 137540, "start": 1388.92, "end": 1393.8000000000002, "text": " We go Scander, and then we go get files, and it looks something like that.", "tokens": [492, 352, 2747, 4483, 11, 293, 550, 321, 352, 483, 7098, 11, 293, 309, 1542, 746, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.14009951561996617, "compression_ratio": 1.7208121827411167, "no_speech_prob": 3.844874299829826e-06}, {"id": 332, "seek": 137540, "start": 1393.8000000000002, "end": 1396.64, "text": " So that's just for one directory.", "tokens": [407, 300, 311, 445, 337, 472, 21120, 13], "temperature": 0.0, "avg_logprob": -0.14009951561996617, "compression_ratio": 1.7208121827411167, "no_speech_prob": 3.844874299829826e-06}, {"id": 333, "seek": 137540, "start": 1396.64, "end": 1400.64, "text": " So we can put all this together like so.", "tokens": [407, 321, 393, 829, 439, 341, 1214, 411, 370, 13], "temperature": 0.0, "avg_logprob": -0.14009951561996617, "compression_ratio": 1.7208121827411167, "no_speech_prob": 3.844874299829826e-06}, {"id": 334, "seek": 140064, "start": 1400.64, "end": 1406.0, "text": " So this is something where we say, for some path, give me things with these extensions,", "tokens": [407, 341, 307, 746, 689, 321, 584, 11, 337, 512, 3100, 11, 976, 385, 721, 365, 613, 25129, 11], "temperature": 0.0, "avg_logprob": -0.140396201494828, "compression_ratio": 1.6072874493927125, "no_speech_prob": 1.7330156651951256e-06}, {"id": 335, "seek": 140064, "start": 1406.0, "end": 1411.2800000000002, "text": " optionally recurse, optionally only include these folder names, and this is it.", "tokens": [3614, 379, 18680, 405, 11, 3614, 379, 787, 4090, 613, 10820, 5288, 11, 293, 341, 307, 309, 13], "temperature": 0.0, "avg_logprob": -0.140396201494828, "compression_ratio": 1.6072874493927125, "no_speech_prob": 1.7330156651951256e-06}, {"id": 336, "seek": 140064, "start": 1411.2800000000002, "end": 1414.88, "text": " I will go through it in detail, but I'll just point out a couple of things because being", "tokens": [286, 486, 352, 807, 309, 294, 2607, 11, 457, 286, 603, 445, 935, 484, 257, 1916, 295, 721, 570, 885], "temperature": 0.0, "avg_logprob": -0.140396201494828, "compression_ratio": 1.6072874493927125, "no_speech_prob": 1.7330156651951256e-06}, {"id": 337, "seek": 140064, "start": 1414.88, "end": 1418.64, "text": " able to rapidly look through files is important.", "tokens": [1075, 281, 12910, 574, 807, 7098, 307, 1021, 13], "temperature": 0.0, "avg_logprob": -0.140396201494828, "compression_ratio": 1.6072874493927125, "no_speech_prob": 1.7330156651951256e-06}, {"id": 338, "seek": 140064, "start": 1418.64, "end": 1421.92, "text": " The first is that Scander is super, super fast.", "tokens": [440, 700, 307, 300, 2747, 4483, 307, 1687, 11, 1687, 2370, 13], "temperature": 0.0, "avg_logprob": -0.140396201494828, "compression_ratio": 1.6072874493927125, "no_speech_prob": 1.7330156651951256e-06}, {"id": 339, "seek": 140064, "start": 1421.92, "end": 1426.72, "text": " This is Python's thin wrapper over a C API.", "tokens": [639, 307, 15329, 311, 5862, 46906, 670, 257, 383, 9362, 13], "temperature": 0.0, "avg_logprob": -0.140396201494828, "compression_ratio": 1.6072874493927125, "no_speech_prob": 1.7330156651951256e-06}, {"id": 340, "seek": 142672, "start": 1426.72, "end": 1433.56, "text": " So this is a really great way to quickly grab stuff for a single directory.", "tokens": [407, 341, 307, 257, 534, 869, 636, 281, 2661, 4444, 1507, 337, 257, 2167, 21120, 13], "temperature": 0.0, "avg_logprob": -0.12797078132629394, "compression_ratio": 1.6379310344827587, "no_speech_prob": 5.682335995516041e-06}, {"id": 341, "seek": 142672, "start": 1433.56, "end": 1437.6000000000001, "text": " If you need to recurse, check out os.walk.", "tokens": [759, 291, 643, 281, 18680, 405, 11, 1520, 484, 3003, 13, 12490, 13], "temperature": 0.0, "avg_logprob": -0.12797078132629394, "compression_ratio": 1.6379310344827587, "no_speech_prob": 5.682335995516041e-06}, {"id": 342, "seek": 142672, "start": 1437.6000000000001, "end": 1443.44, "text": " This is the thing that uses Scander internally to walk through recursively through a folder", "tokens": [639, 307, 264, 551, 300, 4960, 2747, 4483, 19501, 281, 1792, 807, 20560, 3413, 807, 257, 10820], "temperature": 0.0, "avg_logprob": -0.12797078132629394, "compression_ratio": 1.6379310344827587, "no_speech_prob": 5.682335995516041e-06}, {"id": 343, "seek": 142672, "start": 1443.44, "end": 1444.44, "text": " tree.", "tokens": [4230, 13], "temperature": 0.0, "avg_logprob": -0.12797078132629394, "compression_ratio": 1.6379310344827587, "no_speech_prob": 5.682335995516041e-06}, {"id": 344, "seek": 142672, "start": 1444.44, "end": 1451.3600000000001, "text": " You can do cool stuff like change the list of directories that it's going to look at,", "tokens": [509, 393, 360, 1627, 1507, 411, 1319, 264, 1329, 295, 5391, 530, 300, 309, 311, 516, 281, 574, 412, 11], "temperature": 0.0, "avg_logprob": -0.12797078132629394, "compression_ratio": 1.6379310344827587, "no_speech_prob": 5.682335995516041e-06}, {"id": 345, "seek": 142672, "start": 1451.3600000000001, "end": 1454.2, "text": " and it basically returns all the information that you need.", "tokens": [293, 309, 1936, 11247, 439, 264, 1589, 300, 291, 643, 13], "temperature": 0.0, "avg_logprob": -0.12797078132629394, "compression_ratio": 1.6379310344827587, "no_speech_prob": 5.682335995516041e-06}, {"id": 346, "seek": 142672, "start": 1454.2, "end": 1455.2, "text": " It's super great.", "tokens": [467, 311, 1687, 869, 13], "temperature": 0.0, "avg_logprob": -0.12797078132629394, "compression_ratio": 1.6379310344827587, "no_speech_prob": 5.682335995516041e-06}, {"id": 347, "seek": 145520, "start": 1455.2, "end": 1459.92, "text": " So os.walk and os.scander are the things that you want to be using if you're playing with", "tokens": [407, 3003, 13, 12490, 293, 3003, 13, 4417, 4483, 366, 264, 721, 300, 291, 528, 281, 312, 1228, 498, 291, 434, 2433, 365], "temperature": 0.0, "avg_logprob": -0.1550367544363211, "compression_ratio": 1.6506550218340612, "no_speech_prob": 1.6962907466222532e-05}, {"id": 348, "seek": 145520, "start": 1459.92, "end": 1466.0800000000002, "text": " directories and files in Python and you need it to be fast.", "tokens": [5391, 530, 293, 7098, 294, 15329, 293, 291, 643, 309, 281, 312, 2370, 13], "temperature": 0.0, "avg_logprob": -0.1550367544363211, "compression_ratio": 1.6506550218340612, "no_speech_prob": 1.6962907466222532e-05}, {"id": 349, "seek": 145520, "start": 1466.0800000000002, "end": 1467.0800000000002, "text": " We do.", "tokens": [492, 360, 13], "temperature": 0.0, "avg_logprob": -0.1550367544363211, "compression_ratio": 1.6506550218340612, "no_speech_prob": 1.6962907466222532e-05}, {"id": 350, "seek": 145520, "start": 1467.0800000000002, "end": 1473.64, "text": " So here's get files, and so now we can say get files, path tench, just the image extensions.", "tokens": [407, 510, 311, 483, 7098, 11, 293, 370, 586, 321, 393, 584, 483, 7098, 11, 3100, 2064, 339, 11, 445, 264, 3256, 25129, 13], "temperature": 0.0, "avg_logprob": -0.1550367544363211, "compression_ratio": 1.6506550218340612, "no_speech_prob": 1.6962907466222532e-05}, {"id": 351, "seek": 145520, "start": 1473.64, "end": 1475.52, "text": " There we go.", "tokens": [821, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.1550367544363211, "compression_ratio": 1.6506550218340612, "no_speech_prob": 1.6962907466222532e-05}, {"id": 352, "seek": 145520, "start": 1475.52, "end": 1480.2, "text": " And then we're going to need recurse because we've got a few levels of directory structure.", "tokens": [400, 550, 321, 434, 516, 281, 643, 18680, 405, 570, 321, 600, 658, 257, 1326, 4358, 295, 21120, 3877, 13], "temperature": 0.0, "avg_logprob": -0.1550367544363211, "compression_ratio": 1.6506550218340612, "no_speech_prob": 1.6962907466222532e-05}, {"id": 353, "seek": 145520, "start": 1480.2, "end": 1482.48, "text": " So here's with recurse.", "tokens": [407, 510, 311, 365, 18680, 405, 13], "temperature": 0.0, "avg_logprob": -0.1550367544363211, "compression_ratio": 1.6506550218340612, "no_speech_prob": 1.6962907466222532e-05}, {"id": 354, "seek": 148248, "start": 1482.48, "end": 1487.64, "text": " So if we try to get all of the file names, we have 13,000.", "tokens": [407, 498, 321, 853, 281, 483, 439, 295, 264, 3991, 5288, 11, 321, 362, 3705, 11, 1360, 13], "temperature": 0.0, "avg_logprob": -0.13057793853103475, "compression_ratio": 1.5089285714285714, "no_speech_prob": 5.422065896709682e-06}, {"id": 355, "seek": 148248, "start": 1487.64, "end": 1495.8, "text": " And specifically, it takes 70 milliseconds to get 13,000 file names.", "tokens": [400, 4682, 11, 309, 2516, 5285, 34184, 281, 483, 3705, 11, 1360, 3991, 5288, 13], "temperature": 0.0, "avg_logprob": -0.13057793853103475, "compression_ratio": 1.5089285714285714, "no_speech_prob": 5.422065896709682e-06}, {"id": 356, "seek": 148248, "start": 1495.8, "end": 1503.08, "text": " For me to look at 13,000 files in Windows Explorer seems to take about four minutes.", "tokens": [1171, 385, 281, 574, 412, 3705, 11, 1360, 7098, 294, 8591, 31895, 2544, 281, 747, 466, 1451, 2077, 13], "temperature": 0.0, "avg_logprob": -0.13057793853103475, "compression_ratio": 1.5089285714285714, "no_speech_prob": 5.422065896709682e-06}, {"id": 357, "seek": 148248, "start": 1503.08, "end": 1506.16, "text": " So this is unbelievably fast.", "tokens": [407, 341, 307, 43593, 2370, 13], "temperature": 0.0, "avg_logprob": -0.13057793853103475, "compression_ratio": 1.5089285714285714, "no_speech_prob": 5.422065896709682e-06}, {"id": 358, "seek": 148248, "start": 1506.16, "end": 1511.64, "text": " So the full ImageNet, which is 100 times bigger, it's going to be literally just a few seconds.", "tokens": [407, 264, 1577, 29903, 31890, 11, 597, 307, 2319, 1413, 3801, 11, 309, 311, 516, 281, 312, 3736, 445, 257, 1326, 3949, 13], "temperature": 0.0, "avg_logprob": -0.13057793853103475, "compression_ratio": 1.5089285714285714, "no_speech_prob": 5.422065896709682e-06}, {"id": 359, "seek": 151164, "start": 1511.64, "end": 1518.6000000000001, "text": " So this gives you a sense of how incredibly fast these os.walk and scander functions are.", "tokens": [407, 341, 2709, 291, 257, 2020, 295, 577, 6252, 2370, 613, 3003, 13, 12490, 293, 795, 4483, 6828, 366, 13], "temperature": 0.0, "avg_logprob": -0.17754669622941452, "compression_ratio": 1.6419213973799127, "no_speech_prob": 2.1108988221385516e-05}, {"id": 360, "seek": 151164, "start": 1518.6000000000001, "end": 1523.76, "text": " Yes, questions are good.", "tokens": [1079, 11, 1651, 366, 665, 13], "temperature": 0.0, "avg_logprob": -0.17754669622941452, "compression_ratio": 1.6419213973799127, "no_speech_prob": 2.1108988221385516e-05}, {"id": 361, "seek": 151164, "start": 1523.76, "end": 1527.64, "text": " I've often been confused as to whether the code Jeremy is writing in the notebooks are", "tokens": [286, 600, 2049, 668, 9019, 382, 281, 1968, 264, 3089, 17809, 307, 3579, 294, 264, 43782, 366], "temperature": 0.0, "avg_logprob": -0.17754669622941452, "compression_ratio": 1.6419213973799127, "no_speech_prob": 2.1108988221385516e-05}, {"id": 362, "seek": 151164, "start": 1527.64, "end": 1532.5600000000002, "text": " functionality that will be integrated into the Fast AI library or whether the functions", "tokens": [14980, 300, 486, 312, 10919, 666, 264, 15968, 7318, 6405, 420, 1968, 264, 6828], "temperature": 0.0, "avg_logprob": -0.17754669622941452, "compression_ratio": 1.6419213973799127, "no_speech_prob": 2.1108988221385516e-05}, {"id": 363, "seek": 151164, "start": 1532.5600000000002, "end": 1541.5600000000002, "text": " and classes are meant to be written and used by the user interactively and on the fly.", "tokens": [293, 5359, 366, 4140, 281, 312, 3720, 293, 1143, 538, 264, 4195, 4648, 3413, 293, 322, 264, 3603, 13], "temperature": 0.0, "avg_logprob": -0.17754669622941452, "compression_ratio": 1.6419213973799127, "no_speech_prob": 2.1108988221385516e-05}, {"id": 364, "seek": 154156, "start": 1541.56, "end": 1545.96, "text": " Well I guess that's really a question about what's the purpose of this deep learning from", "tokens": [1042, 286, 2041, 300, 311, 534, 257, 1168, 466, 437, 311, 264, 4334, 295, 341, 2452, 2539, 490], "temperature": 0.0, "avg_logprob": -0.12422367534806243, "compression_ratio": 1.644927536231884, "no_speech_prob": 2.2468615497928113e-05}, {"id": 365, "seek": 154156, "start": 1545.96, "end": 1549.08, "text": " the foundations course.", "tokens": [264, 22467, 1164, 13], "temperature": 0.0, "avg_logprob": -0.12422367534806243, "compression_ratio": 1.644927536231884, "no_speech_prob": 2.2468615497928113e-05}, {"id": 366, "seek": 154156, "start": 1549.08, "end": 1551.08, "text": " And different people will get different things out of it.", "tokens": [400, 819, 561, 486, 483, 819, 721, 484, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.12422367534806243, "compression_ratio": 1.644927536231884, "no_speech_prob": 2.2468615497928113e-05}, {"id": 367, "seek": 154156, "start": 1551.08, "end": 1558.6399999999999, "text": " But for me, it's about demystifying what's going on so that you can take what's in your", "tokens": [583, 337, 385, 11, 309, 311, 466, 1371, 38593, 5489, 437, 311, 516, 322, 370, 300, 291, 393, 747, 437, 311, 294, 428], "temperature": 0.0, "avg_logprob": -0.12422367534806243, "compression_ratio": 1.644927536231884, "no_speech_prob": 2.2468615497928113e-05}, {"id": 368, "seek": 154156, "start": 1558.6399999999999, "end": 1562.0, "text": " head and turn it into something real.", "tokens": [1378, 293, 1261, 309, 666, 746, 957, 13], "temperature": 0.0, "avg_logprob": -0.12422367534806243, "compression_ratio": 1.644927536231884, "no_speech_prob": 2.2468615497928113e-05}, {"id": 369, "seek": 154156, "start": 1562.0, "end": 1566.08, "text": " And to do that would be always some combination of using things that are in existing libraries,", "tokens": [400, 281, 360, 300, 576, 312, 1009, 512, 6562, 295, 1228, 721, 300, 366, 294, 6741, 15148, 11], "temperature": 0.0, "avg_logprob": -0.12422367534806243, "compression_ratio": 1.644927536231884, "no_speech_prob": 2.2468615497928113e-05}, {"id": 370, "seek": 154156, "start": 1566.08, "end": 1570.28, "text": " which might be Fast AI or PyTorch or TensorFlow or whatever.", "tokens": [597, 1062, 312, 15968, 7318, 420, 9953, 51, 284, 339, 420, 37624, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.12422367534806243, "compression_ratio": 1.644927536231884, "no_speech_prob": 2.2468615497928113e-05}, {"id": 371, "seek": 157028, "start": 1570.28, "end": 1572.72, "text": " And partly will be things that aren't in existing libraries.", "tokens": [400, 17031, 486, 312, 721, 300, 3212, 380, 294, 6741, 15148, 13], "temperature": 0.0, "avg_logprob": -0.1251305251562295, "compression_ratio": 1.6539923954372624, "no_speech_prob": 7.88820307207061e-06}, {"id": 372, "seek": 157028, "start": 1572.72, "end": 1577.48, "text": " And I don't want you to be in a situation where you say, well, that's not in Fast AI,", "tokens": [400, 286, 500, 380, 528, 291, 281, 312, 294, 257, 2590, 689, 291, 584, 11, 731, 11, 300, 311, 406, 294, 15968, 7318, 11], "temperature": 0.0, "avg_logprob": -0.1251305251562295, "compression_ratio": 1.6539923954372624, "no_speech_prob": 7.88820307207061e-06}, {"id": 373, "seek": 157028, "start": 1577.48, "end": 1579.92, "text": " therefore I don't know how to do it.", "tokens": [4412, 286, 500, 380, 458, 577, 281, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.1251305251562295, "compression_ratio": 1.6539923954372624, "no_speech_prob": 7.88820307207061e-06}, {"id": 374, "seek": 157028, "start": 1579.92, "end": 1585.68, "text": " So really the goal is, this is why it's also called impractical deep learning for coders,", "tokens": [407, 534, 264, 3387, 307, 11, 341, 307, 983, 309, 311, 611, 1219, 704, 1897, 804, 2452, 2539, 337, 17656, 433, 11], "temperature": 0.0, "avg_logprob": -0.1251305251562295, "compression_ratio": 1.6539923954372624, "no_speech_prob": 7.88820307207061e-06}, {"id": 375, "seek": 157028, "start": 1585.68, "end": 1591.92, "text": " is to give you the underlying expertise and tools that you need.", "tokens": [307, 281, 976, 291, 264, 14217, 11769, 293, 3873, 300, 291, 643, 13], "temperature": 0.0, "avg_logprob": -0.1251305251562295, "compression_ratio": 1.6539923954372624, "no_speech_prob": 7.88820307207061e-06}, {"id": 376, "seek": 157028, "start": 1591.92, "end": 1598.8, "text": " In practice, I would expect a lot of the stuff I'm showing you to end up in the Fast AI library,", "tokens": [682, 3124, 11, 286, 576, 2066, 257, 688, 295, 264, 1507, 286, 478, 4099, 291, 281, 917, 493, 294, 264, 15968, 7318, 6405, 11], "temperature": 0.0, "avg_logprob": -0.1251305251562295, "compression_ratio": 1.6539923954372624, "no_speech_prob": 7.88820307207061e-06}, {"id": 377, "seek": 159880, "start": 1598.8, "end": 1603.48, "text": " because that's like literally I'm showing you my research, basically.", "tokens": [570, 300, 311, 411, 3736, 286, 478, 4099, 291, 452, 2132, 11, 1936, 13], "temperature": 0.0, "avg_logprob": -0.15807309517493615, "compression_ratio": 1.6653061224489796, "no_speech_prob": 1.4501043551717885e-05}, {"id": 378, "seek": 159880, "start": 1603.48, "end": 1606.76, "text": " This is like my research journal of the last six months.", "tokens": [639, 307, 411, 452, 2132, 6708, 295, 264, 1036, 2309, 2493, 13], "temperature": 0.0, "avg_logprob": -0.15807309517493615, "compression_ratio": 1.6653061224489796, "no_speech_prob": 1.4501043551717885e-05}, {"id": 379, "seek": 159880, "start": 1606.76, "end": 1611.3999999999999, "text": " And that's what happens is Fast AI library is silver and I take our research and turn", "tokens": [400, 300, 311, 437, 2314, 307, 15968, 7318, 6405, 307, 8753, 293, 286, 747, 527, 2132, 293, 1261], "temperature": 0.0, "avg_logprob": -0.15807309517493615, "compression_ratio": 1.6653061224489796, "no_speech_prob": 1.4501043551717885e-05}, {"id": 380, "seek": 159880, "start": 1611.3999999999999, "end": 1613.6, "text": " it into a library.", "tokens": [309, 666, 257, 6405, 13], "temperature": 0.0, "avg_logprob": -0.15807309517493615, "compression_ratio": 1.6653061224489796, "no_speech_prob": 1.4501043551717885e-05}, {"id": 381, "seek": 159880, "start": 1613.6, "end": 1618.84, "text": " And some of it, like this function, is pretty much copied and pasted from the existing Fast", "tokens": [400, 512, 295, 309, 11, 411, 341, 2445, 11, 307, 1238, 709, 25365, 293, 1791, 292, 490, 264, 6741, 15968], "temperature": 0.0, "avg_logprob": -0.15807309517493615, "compression_ratio": 1.6653061224489796, "no_speech_prob": 1.4501043551717885e-05}, {"id": 382, "seek": 159880, "start": 1618.84, "end": 1626.56, "text": " AI V1 code base, because I spent at least a week figuring out how to make this fast.", "tokens": [7318, 691, 16, 3089, 3096, 11, 570, 286, 4418, 412, 1935, 257, 1243, 15213, 484, 577, 281, 652, 341, 2370, 13], "temperature": 0.0, "avg_logprob": -0.15807309517493615, "compression_ratio": 1.6653061224489796, "no_speech_prob": 1.4501043551717885e-05}, {"id": 383, "seek": 162656, "start": 1626.56, "end": 1630.12, "text": " I'm sure most people can do it faster, but I'm slow and it took me a long time.", "tokens": [286, 478, 988, 881, 561, 393, 360, 309, 4663, 11, 457, 286, 478, 2964, 293, 309, 1890, 385, 257, 938, 565, 13], "temperature": 0.0, "avg_logprob": -0.14372696553854117, "compression_ratio": 1.6451612903225807, "no_speech_prob": 2.0132012650719844e-05}, {"id": 384, "seek": 162656, "start": 1630.12, "end": 1632.24, "text": " And this is what I came up with.", "tokens": [400, 341, 307, 437, 286, 1361, 493, 365, 13], "temperature": 0.0, "avg_logprob": -0.14372696553854117, "compression_ratio": 1.6451612903225807, "no_speech_prob": 2.0132012650719844e-05}, {"id": 385, "seek": 162656, "start": 1632.24, "end": 1640.8799999999999, "text": " So yeah, I mean, it's going to map pretty closely to what's in Fast AI already.", "tokens": [407, 1338, 11, 286, 914, 11, 309, 311, 516, 281, 4471, 1238, 8185, 281, 437, 311, 294, 15968, 7318, 1217, 13], "temperature": 0.0, "avg_logprob": -0.14372696553854117, "compression_ratio": 1.6451612903225807, "no_speech_prob": 2.0132012650719844e-05}, {"id": 386, "seek": 162656, "start": 1640.8799999999999, "end": 1644.1599999999999, "text": " Where things are new, we're telling you, like running batch norm is new.", "tokens": [2305, 721, 366, 777, 11, 321, 434, 3585, 291, 11, 411, 2614, 15245, 2026, 307, 777, 13], "temperature": 0.0, "avg_logprob": -0.14372696553854117, "compression_ratio": 1.6451612903225807, "no_speech_prob": 2.0132012650719844e-05}, {"id": 387, "seek": 162656, "start": 1644.1599999999999, "end": 1647.6, "text": " Today we're going to be seeing a whole new kind of optimizer.", "tokens": [2692, 321, 434, 516, 281, 312, 2577, 257, 1379, 777, 733, 295, 5028, 6545, 13], "temperature": 0.0, "avg_logprob": -0.14372696553854117, "compression_ratio": 1.6451612903225807, "no_speech_prob": 2.0132012650719844e-05}, {"id": 388, "seek": 162656, "start": 1647.6, "end": 1651.34, "text": " But otherwise, things are going to be pretty similar to what's in Fast AI, so it'll make", "tokens": [583, 5911, 11, 721, 366, 516, 281, 312, 1238, 2531, 281, 437, 311, 294, 15968, 7318, 11, 370, 309, 603, 652], "temperature": 0.0, "avg_logprob": -0.14372696553854117, "compression_ratio": 1.6451612903225807, "no_speech_prob": 2.0132012650719844e-05}, {"id": 389, "seek": 162656, "start": 1651.34, "end": 1655.4199999999998, "text": " you be able to quickly hack it Fast AI V1.", "tokens": [291, 312, 1075, 281, 2661, 10339, 309, 15968, 7318, 691, 16, 13], "temperature": 0.0, "avg_logprob": -0.14372696553854117, "compression_ratio": 1.6451612903225807, "no_speech_prob": 2.0132012650719844e-05}, {"id": 390, "seek": 165542, "start": 1655.42, "end": 1660.3200000000002, "text": " And as Fast AI changes, it's not going to surprise you because you'll know what's going", "tokens": [400, 382, 15968, 7318, 2962, 11, 309, 311, 406, 516, 281, 6365, 291, 570, 291, 603, 458, 437, 311, 516], "temperature": 0.0, "avg_logprob": -0.16101409687715418, "compression_ratio": 1.4761904761904763, "no_speech_prob": 3.4789623896358535e-05}, {"id": 391, "seek": 165542, "start": 1660.3200000000002, "end": 1661.3200000000002, "text": " on.", "tokens": [322, 13], "temperature": 0.0, "avg_logprob": -0.16101409687715418, "compression_ratio": 1.4761904761904763, "no_speech_prob": 3.4789623896358535e-05}, {"id": 392, "seek": 165542, "start": 1661.3200000000002, "end": 1662.3200000000002, "text": " Sure.", "tokens": [4894, 13], "temperature": 0.0, "avg_logprob": -0.16101409687715418, "compression_ratio": 1.4761904761904763, "no_speech_prob": 3.4789623896358535e-05}, {"id": 393, "seek": 165542, "start": 1662.3200000000002, "end": 1671.04, "text": " How does Scander compare to Glob?", "tokens": [1012, 775, 2747, 4483, 6794, 281, 10786, 65, 30], "temperature": 0.0, "avg_logprob": -0.16101409687715418, "compression_ratio": 1.4761904761904763, "no_speech_prob": 3.4789623896358535e-05}, {"id": 394, "seek": 165542, "start": 1671.04, "end": 1673.92, "text": " Scander should be much faster than Glob.", "tokens": [2747, 4483, 820, 312, 709, 4663, 813, 10786, 65, 13], "temperature": 0.0, "avg_logprob": -0.16101409687715418, "compression_ratio": 1.4761904761904763, "no_speech_prob": 3.4789623896358535e-05}, {"id": 395, "seek": 165542, "start": 1673.92, "end": 1680.52, "text": " It's a little more awkward to work with because it doesn't try to do so much.", "tokens": [467, 311, 257, 707, 544, 11411, 281, 589, 365, 570, 309, 1177, 380, 853, 281, 360, 370, 709, 13], "temperature": 0.0, "avg_logprob": -0.16101409687715418, "compression_ratio": 1.4761904761904763, "no_speech_prob": 3.4789623896358535e-05}, {"id": 396, "seek": 165542, "start": 1680.52, "end": 1681.92, "text": " It's the lowest level thing.", "tokens": [467, 311, 264, 12437, 1496, 551, 13], "temperature": 0.0, "avg_logprob": -0.16101409687715418, "compression_ratio": 1.4761904761904763, "no_speech_prob": 3.4789623896358535e-05}, {"id": 397, "seek": 168192, "start": 1681.92, "end": 1687.1200000000001, "text": " I suspect Glob probably uses it behind the scenes.", "tokens": [286, 9091, 10786, 65, 1391, 4960, 309, 2261, 264, 8026, 13], "temperature": 0.0, "avg_logprob": -0.15029922733461953, "compression_ratio": 1.6679245283018869, "no_speech_prob": 1.3208492418925744e-05}, {"id": 398, "seek": 168192, "start": 1687.1200000000001, "end": 1688.1200000000001, "text": " You should try.", "tokens": [509, 820, 853, 13], "temperature": 0.0, "avg_logprob": -0.15029922733461953, "compression_ratio": 1.6679245283018869, "no_speech_prob": 1.3208492418925744e-05}, {"id": 399, "seek": 168192, "start": 1688.1200000000001, "end": 1691.04, "text": " Do a time it with Glob, time it with Scander.", "tokens": [1144, 257, 565, 309, 365, 10786, 65, 11, 565, 309, 365, 2747, 4483, 13], "temperature": 0.0, "avg_logprob": -0.15029922733461953, "compression_ratio": 1.6679245283018869, "no_speech_prob": 1.3208492418925744e-05}, {"id": 400, "seek": 168192, "start": 1691.04, "end": 1692.8000000000002, "text": " Probably depends how you use Glob exactly.", "tokens": [9210, 5946, 577, 291, 764, 10786, 65, 2293, 13], "temperature": 0.0, "avg_logprob": -0.15029922733461953, "compression_ratio": 1.6679245283018869, "no_speech_prob": 1.3208492418925744e-05}, {"id": 401, "seek": 168192, "start": 1692.8000000000002, "end": 1696.72, "text": " But I remember I used to use Glob and it was quite a bit slower.", "tokens": [583, 286, 1604, 286, 1143, 281, 764, 10786, 65, 293, 309, 390, 1596, 257, 857, 14009, 13], "temperature": 0.0, "avg_logprob": -0.15029922733461953, "compression_ratio": 1.6679245283018869, "no_speech_prob": 1.3208492418925744e-05}, {"id": 402, "seek": 168192, "start": 1696.72, "end": 1703.0, "text": " And when I say quite a bit, for those of you that have been using Fast AI for a while,", "tokens": [400, 562, 286, 584, 1596, 257, 857, 11, 337, 729, 295, 291, 300, 362, 668, 1228, 15968, 7318, 337, 257, 1339, 11], "temperature": 0.0, "avg_logprob": -0.15029922733461953, "compression_ratio": 1.6679245283018869, "no_speech_prob": 1.3208492418925744e-05}, {"id": 403, "seek": 168192, "start": 1703.0, "end": 1709.5600000000002, "text": " you might have noticed that the speed at which you can grab the ImageNet folder is some orders", "tokens": [291, 1062, 362, 5694, 300, 264, 3073, 412, 597, 291, 393, 4444, 264, 29903, 31890, 10820, 307, 512, 9470], "temperature": 0.0, "avg_logprob": -0.15029922733461953, "compression_ratio": 1.6679245283018869, "no_speech_prob": 1.3208492418925744e-05}, {"id": 404, "seek": 168192, "start": 1709.5600000000002, "end": 1711.72, "text": " of magnitude faster than it used to be.", "tokens": [295, 15668, 4663, 813, 309, 1143, 281, 312, 13], "temperature": 0.0, "avg_logprob": -0.15029922733461953, "compression_ratio": 1.6679245283018869, "no_speech_prob": 1.3208492418925744e-05}, {"id": 405, "seek": 171172, "start": 1711.72, "end": 1714.1200000000001, "text": " That's quite a big difference.", "tokens": [663, 311, 1596, 257, 955, 2649, 13], "temperature": 0.0, "avg_logprob": -0.1663089222378201, "compression_ratio": 1.5284552845528456, "no_speech_prob": 1.2804235666408204e-05}, {"id": 406, "seek": 171172, "start": 1714.1200000000001, "end": 1715.44, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.1663089222378201, "compression_ratio": 1.5284552845528456, "no_speech_prob": 1.2804235666408204e-05}, {"id": 407, "seek": 171172, "start": 1715.44, "end": 1724.84, "text": " So the reason that Fast AI has a data blocks API and nobody else does is because I got", "tokens": [407, 264, 1778, 300, 15968, 7318, 575, 257, 1412, 8474, 9362, 293, 5079, 1646, 775, 307, 570, 286, 658], "temperature": 0.0, "avg_logprob": -0.1663089222378201, "compression_ratio": 1.5284552845528456, "no_speech_prob": 1.2804235666408204e-05}, {"id": 408, "seek": 171172, "start": 1724.84, "end": 1730.2, "text": " so frustrated in the last course at having to create every possible combination of independent", "tokens": [370, 15751, 294, 264, 1036, 1164, 412, 1419, 281, 1884, 633, 1944, 6562, 295, 6695], "temperature": 0.0, "avg_logprob": -0.1663089222378201, "compression_ratio": 1.5284552845528456, "no_speech_prob": 1.2804235666408204e-05}, {"id": 409, "seek": 171172, "start": 1730.2, "end": 1736.52, "text": " and dependent variable that I actually sat back for a while and did some thinking.", "tokens": [293, 12334, 7006, 300, 286, 767, 3227, 646, 337, 257, 1339, 293, 630, 512, 1953, 13], "temperature": 0.0, "avg_logprob": -0.1663089222378201, "compression_ratio": 1.5284552845528456, "no_speech_prob": 1.2804235666408204e-05}, {"id": 410, "seek": 171172, "start": 1736.52, "end": 1739.48, "text": " And specifically, this is what I did when I thought was I wrote this down.", "tokens": [400, 4682, 11, 341, 307, 437, 286, 630, 562, 286, 1194, 390, 286, 4114, 341, 760, 13], "temperature": 0.0, "avg_logprob": -0.1663089222378201, "compression_ratio": 1.5284552845528456, "no_speech_prob": 1.2804235666408204e-05}, {"id": 411, "seek": 173948, "start": 1739.48, "end": 1743.2, "text": " It's like what do you actually need to do?", "tokens": [467, 311, 411, 437, 360, 291, 767, 643, 281, 360, 30], "temperature": 0.0, "avg_logprob": -0.13763984747692548, "compression_ratio": 1.786610878661088, "no_speech_prob": 2.2824748157290742e-05}, {"id": 412, "seek": 173948, "start": 1743.2, "end": 1744.56, "text": " So let's go ahead and do these things.", "tokens": [407, 718, 311, 352, 2286, 293, 360, 613, 721, 13], "temperature": 0.0, "avg_logprob": -0.13763984747692548, "compression_ratio": 1.786610878661088, "no_speech_prob": 2.2824748157290742e-05}, {"id": 413, "seek": 173948, "start": 1744.56, "end": 1746.5, "text": " So we've already got files.", "tokens": [407, 321, 600, 1217, 658, 7098, 13], "temperature": 0.0, "avg_logprob": -0.13763984747692548, "compression_ratio": 1.786610878661088, "no_speech_prob": 2.2824748157290742e-05}, {"id": 414, "seek": 173948, "start": 1746.5, "end": 1751.68, "text": " We need some way to split the validation set or multiple validation sets out, some way", "tokens": [492, 643, 512, 636, 281, 7472, 264, 24071, 992, 420, 3866, 24071, 6352, 484, 11, 512, 636], "temperature": 0.0, "avg_logprob": -0.13763984747692548, "compression_ratio": 1.786610878661088, "no_speech_prob": 2.2824748157290742e-05}, {"id": 415, "seek": 173948, "start": 1751.68, "end": 1758.44, "text": " to do labeling, optionally some augmentation, transform it to a tensor, make it into data,", "tokens": [281, 360, 40244, 11, 3614, 379, 512, 14501, 19631, 11, 4088, 309, 281, 257, 40863, 11, 652, 309, 666, 1412, 11], "temperature": 0.0, "avg_logprob": -0.13763984747692548, "compression_ratio": 1.786610878661088, "no_speech_prob": 2.2824748157290742e-05}, {"id": 416, "seek": 173948, "start": 1758.44, "end": 1765.54, "text": " into batches, optionally transform the batches, and then combine the data loaders together", "tokens": [666, 15245, 279, 11, 3614, 379, 4088, 264, 15245, 279, 11, 293, 550, 10432, 264, 1412, 3677, 433, 1214], "temperature": 0.0, "avg_logprob": -0.13763984747692548, "compression_ratio": 1.786610878661088, "no_speech_prob": 2.2824748157290742e-05}, {"id": 417, "seek": 173948, "start": 1765.54, "end": 1769.1200000000001, "text": " into a data bunch and optionally add a test set.", "tokens": [666, 257, 1412, 3840, 293, 3614, 379, 909, 257, 1500, 992, 13], "temperature": 0.0, "avg_logprob": -0.13763984747692548, "compression_ratio": 1.786610878661088, "no_speech_prob": 2.2824748157290742e-05}, {"id": 418, "seek": 176912, "start": 1769.12, "end": 1773.9599999999998, "text": " And so when I wrote it down like that, I just went ahead and implemented an API for each", "tokens": [400, 370, 562, 286, 4114, 309, 760, 411, 300, 11, 286, 445, 1437, 2286, 293, 12270, 364, 9362, 337, 1184], "temperature": 0.0, "avg_logprob": -0.1261070121047843, "compression_ratio": 1.6842105263157894, "no_speech_prob": 8.267477824119851e-06}, {"id": 419, "seek": 176912, "start": 1773.9599999999998, "end": 1778.0, "text": " of those things to say like, okay, you can plug in anything you like to that part of", "tokens": [295, 729, 721, 281, 584, 411, 11, 1392, 11, 291, 393, 5452, 294, 1340, 291, 411, 281, 300, 644, 295], "temperature": 0.0, "avg_logprob": -0.1261070121047843, "compression_ratio": 1.6842105263157894, "no_speech_prob": 8.267477824119851e-06}, {"id": 420, "seek": 176912, "start": 1778.0, "end": 1779.0, "text": " the API.", "tokens": [264, 9362, 13], "temperature": 0.0, "avg_logprob": -0.1261070121047843, "compression_ratio": 1.6842105263157894, "no_speech_prob": 8.267477824119851e-06}, {"id": 421, "seek": 176912, "start": 1779.0, "end": 1780.0, "text": " So let's do it.", "tokens": [407, 718, 311, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.1261070121047843, "compression_ratio": 1.6842105263157894, "no_speech_prob": 8.267477824119851e-06}, {"id": 422, "seek": 176912, "start": 1780.0, "end": 1781.0, "text": " Right?", "tokens": [1779, 30], "temperature": 0.0, "avg_logprob": -0.1261070121047843, "compression_ratio": 1.6842105263157894, "no_speech_prob": 8.267477824119851e-06}, {"id": 423, "seek": 176912, "start": 1781.0, "end": 1785.28, "text": " So we've already got the basic functionality to get the files.", "tokens": [407, 321, 600, 1217, 658, 264, 3875, 14980, 281, 483, 264, 7098, 13], "temperature": 0.0, "avg_logprob": -0.1261070121047843, "compression_ratio": 1.6842105263157894, "no_speech_prob": 8.267477824119851e-06}, {"id": 424, "seek": 176912, "start": 1785.28, "end": 1788.1599999999999, "text": " So now we have to put them somewhere.", "tokens": [407, 586, 321, 362, 281, 829, 552, 4079, 13], "temperature": 0.0, "avg_logprob": -0.1261070121047843, "compression_ratio": 1.6842105263157894, "no_speech_prob": 8.267477824119851e-06}, {"id": 425, "seek": 176912, "start": 1788.1599999999999, "end": 1790.6399999999999, "text": " We already created that list container.", "tokens": [492, 1217, 2942, 300, 1329, 10129, 13], "temperature": 0.0, "avg_logprob": -0.1261070121047843, "compression_ratio": 1.6842105263157894, "no_speech_prob": 8.267477824119851e-06}, {"id": 426, "seek": 176912, "start": 1790.6399999999999, "end": 1791.6399999999999, "text": " Right?", "tokens": [1779, 30], "temperature": 0.0, "avg_logprob": -0.1261070121047843, "compression_ratio": 1.6842105263157894, "no_speech_prob": 8.267477824119851e-06}, {"id": 427, "seek": 176912, "start": 1791.6399999999999, "end": 1797.12, "text": " So we basically can just dump our files into a list container.", "tokens": [407, 321, 1936, 393, 445, 11430, 527, 7098, 666, 257, 1329, 10129, 13], "temperature": 0.0, "avg_logprob": -0.1261070121047843, "compression_ratio": 1.6842105263157894, "no_speech_prob": 8.267477824119851e-06}, {"id": 428, "seek": 179712, "start": 1797.12, "end": 1801.52, "text": " But in the end, what we actually want is an image list for this one.", "tokens": [583, 294, 264, 917, 11, 437, 321, 767, 528, 307, 364, 3256, 1329, 337, 341, 472, 13], "temperature": 0.0, "avg_logprob": -0.1762132791372446, "compression_ratio": 1.8638132295719845, "no_speech_prob": 9.22322942642495e-06}, {"id": 429, "seek": 179712, "start": 1801.52, "end": 1803.56, "text": " And an image list, when you call...", "tokens": [400, 364, 3256, 1329, 11, 562, 291, 818, 485], "temperature": 0.0, "avg_logprob": -0.1762132791372446, "compression_ratio": 1.8638132295719845, "no_speech_prob": 9.22322942642495e-06}, {"id": 430, "seek": 179712, "start": 1803.56, "end": 1805.52, "text": " So we're going to have this get method.", "tokens": [407, 321, 434, 516, 281, 362, 341, 483, 3170, 13], "temperature": 0.0, "avg_logprob": -0.1762132791372446, "compression_ratio": 1.8638132295719845, "no_speech_prob": 9.22322942642495e-06}, {"id": 431, "seek": 179712, "start": 1805.52, "end": 1810.12, "text": " And when you get something from the image list, it should open the image.", "tokens": [400, 562, 291, 483, 746, 490, 264, 3256, 1329, 11, 309, 820, 1269, 264, 3256, 13], "temperature": 0.0, "avg_logprob": -0.1762132791372446, "compression_ratio": 1.8638132295719845, "no_speech_prob": 9.22322942642495e-06}, {"id": 432, "seek": 179712, "start": 1810.12, "end": 1813.6, "text": " So pil.image.open is how you open an image.", "tokens": [407, 6429, 13, 26624, 13, 15752, 307, 577, 291, 1269, 364, 3256, 13], "temperature": 0.0, "avg_logprob": -0.1762132791372446, "compression_ratio": 1.8638132295719845, "no_speech_prob": 9.22322942642495e-06}, {"id": 433, "seek": 179712, "start": 1813.6, "end": 1816.82, "text": " But we could get all kinds of different objects.", "tokens": [583, 321, 727, 483, 439, 3685, 295, 819, 6565, 13], "temperature": 0.0, "avg_logprob": -0.1762132791372446, "compression_ratio": 1.8638132295719845, "no_speech_prob": 9.22322942642495e-06}, {"id": 434, "seek": 179712, "start": 1816.82, "end": 1822.32, "text": " So therefore, we have this superclass, which has a get method that you override.", "tokens": [407, 4412, 11, 321, 362, 341, 1687, 11665, 11, 597, 575, 257, 483, 3170, 300, 291, 42321, 13], "temperature": 0.0, "avg_logprob": -0.1762132791372446, "compression_ratio": 1.8638132295719845, "no_speech_prob": 9.22322942642495e-06}, {"id": 435, "seek": 179712, "start": 1822.32, "end": 1826.32, "text": " And by default, it just returns whatever you put in there, which in this case would be", "tokens": [400, 538, 7576, 11, 309, 445, 11247, 2035, 291, 829, 294, 456, 11, 597, 294, 341, 1389, 576, 312], "temperature": 0.0, "avg_logprob": -0.1762132791372446, "compression_ratio": 1.8638132295719845, "no_speech_prob": 9.22322942642495e-06}, {"id": 436, "seek": 182632, "start": 1826.32, "end": 1829.36, "text": " the file name.", "tokens": [264, 3991, 1315, 13], "temperature": 0.0, "avg_logprob": -0.1785012943538155, "compression_ratio": 1.714859437751004, "no_speech_prob": 1.450955642212648e-05}, {"id": 437, "seek": 182632, "start": 1829.36, "end": 1831.56, "text": " So this is basically all item list does.", "tokens": [407, 341, 307, 1936, 439, 3174, 1329, 775, 13], "temperature": 0.0, "avg_logprob": -0.1785012943538155, "compression_ratio": 1.714859437751004, "no_speech_prob": 1.450955642212648e-05}, {"id": 438, "seek": 182632, "start": 1831.56, "end": 1832.56, "text": " Right?", "tokens": [1779, 30], "temperature": 0.0, "avg_logprob": -0.1785012943538155, "compression_ratio": 1.714859437751004, "no_speech_prob": 1.450955642212648e-05}, {"id": 439, "seek": 182632, "start": 1832.56, "end": 1833.56, "text": " It's got a list of items.", "tokens": [467, 311, 658, 257, 1329, 295, 4754, 13], "temperature": 0.0, "avg_logprob": -0.1785012943538155, "compression_ratio": 1.714859437751004, "no_speech_prob": 1.450955642212648e-05}, {"id": 440, "seek": 182632, "start": 1833.56, "end": 1834.56, "text": " Right?", "tokens": [1779, 30], "temperature": 0.0, "avg_logprob": -0.1785012943538155, "compression_ratio": 1.714859437751004, "no_speech_prob": 1.450955642212648e-05}, {"id": 441, "seek": 182632, "start": 1834.56, "end": 1837.6399999999999, "text": " So in this case, it's going to be our file names.", "tokens": [407, 294, 341, 1389, 11, 309, 311, 516, 281, 312, 527, 3991, 5288, 13], "temperature": 0.0, "avg_logprob": -0.1785012943538155, "compression_ratio": 1.714859437751004, "no_speech_prob": 1.450955642212648e-05}, {"id": 442, "seek": 182632, "start": 1837.6399999999999, "end": 1840.2, "text": " The path that they came from.", "tokens": [440, 3100, 300, 436, 1361, 490, 13], "temperature": 0.0, "avg_logprob": -0.1785012943538155, "compression_ratio": 1.714859437751004, "no_speech_prob": 1.450955642212648e-05}, {"id": 443, "seek": 182632, "start": 1840.2, "end": 1843.1599999999999, "text": " And then optionally also, there could be a list of transforms.", "tokens": [400, 550, 3614, 379, 611, 11, 456, 727, 312, 257, 1329, 295, 35592, 13], "temperature": 0.0, "avg_logprob": -0.1785012943538155, "compression_ratio": 1.714859437751004, "no_speech_prob": 1.450955642212648e-05}, {"id": 444, "seek": 182632, "start": 1843.1599999999999, "end": 1844.1599999999999, "text": " Right?", "tokens": [1779, 30], "temperature": 0.0, "avg_logprob": -0.1785012943538155, "compression_ratio": 1.714859437751004, "no_speech_prob": 1.450955642212648e-05}, {"id": 445, "seek": 182632, "start": 1844.1599999999999, "end": 1847.02, "text": " And transforms are some kind of functions.", "tokens": [400, 35592, 366, 512, 733, 295, 6828, 13], "temperature": 0.0, "avg_logprob": -0.1785012943538155, "compression_ratio": 1.714859437751004, "no_speech_prob": 1.450955642212648e-05}, {"id": 446, "seek": 182632, "start": 1847.02, "end": 1848.84, "text": " And we'll look at this in more detail in a moment.", "tokens": [400, 321, 603, 574, 412, 341, 294, 544, 2607, 294, 257, 1623, 13], "temperature": 0.0, "avg_logprob": -0.1785012943538155, "compression_ratio": 1.714859437751004, "no_speech_prob": 1.450955642212648e-05}, {"id": 447, "seek": 182632, "start": 1848.84, "end": 1854.4399999999998, "text": " But basically, what will happen is when you index into your item list, remember done to", "tokens": [583, 1936, 11, 437, 486, 1051, 307, 562, 291, 8186, 666, 428, 3174, 1329, 11, 1604, 1096, 281], "temperature": 0.0, "avg_logprob": -0.1785012943538155, "compression_ratio": 1.714859437751004, "no_speech_prob": 1.450955642212648e-05}, {"id": 448, "seek": 185444, "start": 1854.44, "end": 1857.56, "text": " get item does that.", "tokens": [483, 3174, 775, 300, 13], "temperature": 0.0, "avg_logprob": -0.09791352748870849, "compression_ratio": 1.9166666666666667, "no_speech_prob": 8.013281330931932e-06}, {"id": 449, "seek": 185444, "start": 1857.56, "end": 1860.56, "text": " We'll pass that back up to list container, get item.", "tokens": [492, 603, 1320, 300, 646, 493, 281, 1329, 10129, 11, 483, 3174, 13], "temperature": 0.0, "avg_logprob": -0.09791352748870849, "compression_ratio": 1.9166666666666667, "no_speech_prob": 8.013281330931932e-06}, {"id": 450, "seek": 185444, "start": 1860.56, "end": 1865.44, "text": " And that will return either a single item or a list of items.", "tokens": [400, 300, 486, 2736, 2139, 257, 2167, 3174, 420, 257, 1329, 295, 4754, 13], "temperature": 0.0, "avg_logprob": -0.09791352748870849, "compression_ratio": 1.9166666666666667, "no_speech_prob": 8.013281330931932e-06}, {"id": 451, "seek": 185444, "start": 1865.44, "end": 1869.3600000000001, "text": " And if it's a single item, we'll just call self.underscoreget.", "tokens": [400, 498, 309, 311, 257, 2167, 3174, 11, 321, 603, 445, 818, 2698, 13, 997, 433, 12352, 847, 13], "temperature": 0.0, "avg_logprob": -0.09791352748870849, "compression_ratio": 1.9166666666666667, "no_speech_prob": 8.013281330931932e-06}, {"id": 452, "seek": 185444, "start": 1869.3600000000001, "end": 1873.48, "text": " If it's a list of items, we'll call self.underscoreget on all of them.", "tokens": [759, 309, 311, 257, 1329, 295, 4754, 11, 321, 603, 818, 2698, 13, 997, 433, 12352, 847, 322, 439, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.09791352748870849, "compression_ratio": 1.9166666666666667, "no_speech_prob": 8.013281330931932e-06}, {"id": 453, "seek": 185444, "start": 1873.48, "end": 1879.0, "text": " And what that's going to do is it's going to call the get method, which in the case", "tokens": [400, 437, 300, 311, 516, 281, 360, 307, 309, 311, 516, 281, 818, 264, 483, 3170, 11, 597, 294, 264, 1389], "temperature": 0.0, "avg_logprob": -0.09791352748870849, "compression_ratio": 1.9166666666666667, "no_speech_prob": 8.013281330931932e-06}, {"id": 454, "seek": 185444, "start": 1879.0, "end": 1881.68, "text": " of an image list, will open the image.", "tokens": [295, 364, 3256, 1329, 11, 486, 1269, 264, 3256, 13], "temperature": 0.0, "avg_logprob": -0.09791352748870849, "compression_ratio": 1.9166666666666667, "no_speech_prob": 8.013281330931932e-06}, {"id": 455, "seek": 188168, "start": 1881.68, "end": 1885.1200000000001, "text": " And then it'll compose the transforms.", "tokens": [400, 550, 309, 603, 35925, 264, 35592, 13], "temperature": 0.0, "avg_logprob": -0.10897525740258487, "compression_ratio": 1.8351063829787233, "no_speech_prob": 5.093236268294277e-06}, {"id": 456, "seek": 188168, "start": 1885.1200000000001, "end": 1889.72, "text": " So for those of you that haven't done any kind of more functional style programming,", "tokens": [407, 337, 729, 295, 291, 300, 2378, 380, 1096, 604, 733, 295, 544, 11745, 3758, 9410, 11], "temperature": 0.0, "avg_logprob": -0.10897525740258487, "compression_ratio": 1.8351063829787233, "no_speech_prob": 5.093236268294277e-06}, {"id": 457, "seek": 188168, "start": 1889.72, "end": 1900.76, "text": " compose is just a concept that says go through a list of functions and call the function", "tokens": [35925, 307, 445, 257, 3410, 300, 1619, 352, 807, 257, 1329, 295, 6828, 293, 818, 264, 2445], "temperature": 0.0, "avg_logprob": -0.10897525740258487, "compression_ratio": 1.8351063829787233, "no_speech_prob": 5.093236268294277e-06}, {"id": 458, "seek": 188168, "start": 1900.76, "end": 1904.1000000000001, "text": " and replace myself with the result of that.", "tokens": [293, 7406, 2059, 365, 264, 1874, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.10897525740258487, "compression_ratio": 1.8351063829787233, "no_speech_prob": 5.093236268294277e-06}, {"id": 459, "seek": 188168, "start": 1904.1000000000001, "end": 1908.44, "text": " And then call the next function and replace myself with the result of that and so forth.", "tokens": [400, 550, 818, 264, 958, 2445, 293, 7406, 2059, 365, 264, 1874, 295, 300, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.10897525740258487, "compression_ratio": 1.8351063829787233, "no_speech_prob": 5.093236268294277e-06}, {"id": 460, "seek": 190844, "start": 1908.44, "end": 1913.52, "text": " So in other words, a deep neural network is just a composition of functions.", "tokens": [407, 294, 661, 2283, 11, 257, 2452, 18161, 3209, 307, 445, 257, 12686, 295, 6828, 13], "temperature": 0.0, "avg_logprob": -0.14777188465513033, "compression_ratio": 1.75, "no_speech_prob": 1.1842208550660871e-05}, {"id": 461, "seek": 190844, "start": 1913.52, "end": 1915.1200000000001, "text": " Each layer is a function.", "tokens": [6947, 4583, 307, 257, 2445, 13], "temperature": 0.0, "avg_logprob": -0.14777188465513033, "compression_ratio": 1.75, "no_speech_prob": 1.1842208550660871e-05}, {"id": 462, "seek": 190844, "start": 1915.1200000000001, "end": 1916.8600000000001, "text": " We compose them all together.", "tokens": [492, 35925, 552, 439, 1214, 13], "temperature": 0.0, "avg_logprob": -0.14777188465513033, "compression_ratio": 1.75, "no_speech_prob": 1.1842208550660871e-05}, {"id": 463, "seek": 190844, "start": 1916.8600000000001, "end": 1920.0, "text": " This compose does a little bit more than most composers.", "tokens": [639, 35925, 775, 257, 707, 857, 544, 813, 881, 43872, 13], "temperature": 0.0, "avg_logprob": -0.14777188465513033, "compression_ratio": 1.75, "no_speech_prob": 1.1842208550660871e-05}, {"id": 464, "seek": 190844, "start": 1920.0, "end": 1925.8400000000001, "text": " Specifically, you can optionally say I want to order them in some way.", "tokens": [26058, 11, 291, 393, 3614, 379, 584, 286, 528, 281, 1668, 552, 294, 512, 636, 13], "temperature": 0.0, "avg_logprob": -0.14777188465513033, "compression_ratio": 1.75, "no_speech_prob": 1.1842208550660871e-05}, {"id": 465, "seek": 190844, "start": 1925.8400000000001, "end": 1932.0800000000002, "text": " And it checks to see whether the things have an underscore order key and sorts them.", "tokens": [400, 309, 13834, 281, 536, 1968, 264, 721, 362, 364, 37556, 1668, 2141, 293, 7527, 552, 13], "temperature": 0.0, "avg_logprob": -0.14777188465513033, "compression_ratio": 1.75, "no_speech_prob": 1.1842208550660871e-05}, {"id": 466, "seek": 190844, "start": 1932.0800000000002, "end": 1934.6000000000001, "text": " And also, you can pass in some keyword arguments.", "tokens": [400, 611, 11, 291, 393, 1320, 294, 512, 20428, 12869, 13], "temperature": 0.0, "avg_logprob": -0.14777188465513033, "compression_ratio": 1.75, "no_speech_prob": 1.1842208550660871e-05}, {"id": 467, "seek": 190844, "start": 1934.6000000000001, "end": 1937.28, "text": " And if you do, it'll just keep passing in those keyword arguments.", "tokens": [400, 498, 291, 360, 11, 309, 603, 445, 1066, 8437, 294, 729, 20428, 12869, 13], "temperature": 0.0, "avg_logprob": -0.14777188465513033, "compression_ratio": 1.75, "no_speech_prob": 1.1842208550660871e-05}, {"id": 468, "seek": 193728, "start": 1937.28, "end": 1942.8, "text": " But it's basically, other than that, it's a pretty standard function composition function.", "tokens": [583, 309, 311, 1936, 11, 661, 813, 300, 11, 309, 311, 257, 1238, 3832, 2445, 12686, 2445, 13], "temperature": 0.0, "avg_logprob": -0.10316422454312317, "compression_ratio": 1.7170542635658914, "no_speech_prob": 1.5934278053464368e-05}, {"id": 469, "seek": 193728, "start": 1942.8, "end": 1949.24, "text": " If you haven't seen compose used elsewhere in programming before, Google that because", "tokens": [759, 291, 2378, 380, 1612, 35925, 1143, 14517, 294, 9410, 949, 11, 3329, 300, 570], "temperature": 0.0, "avg_logprob": -0.10316422454312317, "compression_ratio": 1.7170542635658914, "no_speech_prob": 1.5934278053464368e-05}, {"id": 470, "seek": 193728, "start": 1949.24, "end": 1951.84, "text": " it's a super useful concept.", "tokens": [309, 311, 257, 1687, 4420, 3410, 13], "temperature": 0.0, "avg_logprob": -0.10316422454312317, "compression_ratio": 1.7170542635658914, "no_speech_prob": 1.5934278053464368e-05}, {"id": 471, "seek": 193728, "start": 1951.84, "end": 1952.84, "text": " Comes up all the time.", "tokens": [47290, 493, 439, 264, 565, 13], "temperature": 0.0, "avg_logprob": -0.10316422454312317, "compression_ratio": 1.7170542635658914, "no_speech_prob": 1.5934278053464368e-05}, {"id": 472, "seek": 193728, "start": 1952.84, "end": 1953.84, "text": " We use it all the time.", "tokens": [492, 764, 309, 439, 264, 565, 13], "temperature": 0.0, "avg_logprob": -0.10316422454312317, "compression_ratio": 1.7170542635658914, "no_speech_prob": 1.5934278053464368e-05}, {"id": 473, "seek": 193728, "start": 1953.84, "end": 1957.84, "text": " And as you can see, in this case, it means I can just pass in a list of transforms.", "tokens": [400, 382, 291, 393, 536, 11, 294, 341, 1389, 11, 309, 1355, 286, 393, 445, 1320, 294, 257, 1329, 295, 35592, 13], "temperature": 0.0, "avg_logprob": -0.10316422454312317, "compression_ratio": 1.7170542635658914, "no_speech_prob": 1.5934278053464368e-05}, {"id": 474, "seek": 193728, "start": 1957.84, "end": 1965.16, "text": " And this will simply call each of those transforms in turn, modifying, in this case, the image", "tokens": [400, 341, 486, 2935, 818, 1184, 295, 729, 35592, 294, 1261, 11, 42626, 11, 294, 341, 1389, 11, 264, 3256], "temperature": 0.0, "avg_logprob": -0.10316422454312317, "compression_ratio": 1.7170542635658914, "no_speech_prob": 1.5934278053464368e-05}, {"id": 475, "seek": 193728, "start": 1965.16, "end": 1966.2, "text": " that I had.", "tokens": [300, 286, 632, 13], "temperature": 0.0, "avg_logprob": -0.10316422454312317, "compression_ratio": 1.7170542635658914, "no_speech_prob": 1.5934278053464368e-05}, {"id": 476, "seek": 196620, "start": 1966.2, "end": 1969.1200000000001, "text": " So here's how you create an image list.", "tokens": [407, 510, 311, 577, 291, 1884, 364, 3256, 1329, 13], "temperature": 0.0, "avg_logprob": -0.1329306983947754, "compression_ratio": 1.775609756097561, "no_speech_prob": 7.295850082300603e-06}, {"id": 477, "seek": 196620, "start": 1969.1200000000001, "end": 1975.74, "text": " And then here's a method to create an image list from a path.", "tokens": [400, 550, 510, 311, 257, 3170, 281, 1884, 364, 3256, 1329, 490, 257, 3100, 13], "temperature": 0.0, "avg_logprob": -0.1329306983947754, "compression_ratio": 1.775609756097561, "no_speech_prob": 7.295850082300603e-06}, {"id": 478, "seek": 196620, "start": 1975.74, "end": 1978.06, "text": " And it's just going to call that get files.", "tokens": [400, 309, 311, 445, 516, 281, 818, 300, 483, 7098, 13], "temperature": 0.0, "avg_logprob": -0.1329306983947754, "compression_ratio": 1.775609756097561, "no_speech_prob": 7.295850082300603e-06}, {"id": 479, "seek": 196620, "start": 1978.06, "end": 1982.8, "text": " And then that's going to give us a list of files, which we will then pass to the class", "tokens": [400, 550, 300, 311, 516, 281, 976, 505, 257, 1329, 295, 7098, 11, 597, 321, 486, 550, 1320, 281, 264, 1508], "temperature": 0.0, "avg_logprob": -0.1329306983947754, "compression_ratio": 1.775609756097561, "no_speech_prob": 7.295850082300603e-06}, {"id": 480, "seek": 196620, "start": 1982.8, "end": 1987.72, "text": " constructor, which expects a list of files or a list of something.", "tokens": [47479, 11, 597, 33280, 257, 1329, 295, 7098, 420, 257, 1329, 295, 746, 13], "temperature": 0.0, "avg_logprob": -0.1329306983947754, "compression_ratio": 1.775609756097561, "no_speech_prob": 7.295850082300603e-06}, {"id": 481, "seek": 196620, "start": 1987.72, "end": 1993.16, "text": " So this is basically the same as item list in Fast AI version 1.", "tokens": [407, 341, 307, 1936, 264, 912, 382, 3174, 1329, 294, 15968, 7318, 3037, 502, 13], "temperature": 0.0, "avg_logprob": -0.1329306983947754, "compression_ratio": 1.775609756097561, "no_speech_prob": 7.295850082300603e-06}, {"id": 482, "seek": 199316, "start": 1993.16, "end": 1997.24, "text": " It's just a list.", "tokens": [467, 311, 445, 257, 1329, 13], "temperature": 0.0, "avg_logprob": -0.153975279934435, "compression_ratio": 1.5, "no_speech_prob": 6.54032146485406e-06}, {"id": 483, "seek": 199316, "start": 1997.24, "end": 2003.0400000000002, "text": " It's just a list where when you try to index into it, it will call something which subclasses", "tokens": [467, 311, 445, 257, 1329, 689, 562, 291, 853, 281, 8186, 666, 309, 11, 309, 486, 818, 746, 597, 1422, 11665, 279], "temperature": 0.0, "avg_logprob": -0.153975279934435, "compression_ratio": 1.5, "no_speech_prob": 6.54032146485406e-06}, {"id": 484, "seek": 199316, "start": 2003.0400000000002, "end": 2004.0400000000002, "text": " overread.", "tokens": [670, 2538, 13], "temperature": 0.0, "avg_logprob": -0.153975279934435, "compression_ratio": 1.5, "no_speech_prob": 6.54032146485406e-06}, {"id": 485, "seek": 199316, "start": 2004.0400000000002, "end": 2005.0400000000002, "text": " OK.", "tokens": [2264, 13], "temperature": 0.0, "avg_logprob": -0.153975279934435, "compression_ratio": 1.5, "no_speech_prob": 6.54032146485406e-06}, {"id": 486, "seek": 199316, "start": 2005.0400000000002, "end": 2010.8000000000002, "text": " So now we've got an image list.", "tokens": [407, 586, 321, 600, 658, 364, 3256, 1329, 13], "temperature": 0.0, "avg_logprob": -0.153975279934435, "compression_ratio": 1.5, "no_speech_prob": 6.54032146485406e-06}, {"id": 487, "seek": 199316, "start": 2010.8000000000002, "end": 2012.68, "text": " We can use it.", "tokens": [492, 393, 764, 309, 13], "temperature": 0.0, "avg_logprob": -0.153975279934435, "compression_ratio": 1.5, "no_speech_prob": 6.54032146485406e-06}, {"id": 488, "seek": 199316, "start": 2012.68, "end": 2019.92, "text": " Now, one thing that happens all the time is you try to create a mini batch of images.", "tokens": [823, 11, 472, 551, 300, 2314, 439, 264, 565, 307, 291, 853, 281, 1884, 257, 8382, 15245, 295, 5267, 13], "temperature": 0.0, "avg_logprob": -0.153975279934435, "compression_ratio": 1.5, "no_speech_prob": 6.54032146485406e-06}, {"id": 489, "seek": 201992, "start": 2019.92, "end": 2023.48, "text": " One of your images was black and white.", "tokens": [1485, 295, 428, 5267, 390, 2211, 293, 2418, 13], "temperature": 0.0, "avg_logprob": -0.15376263079435928, "compression_ratio": 1.575609756097561, "no_speech_prob": 5.9547446653596126e-06}, {"id": 490, "seek": 201992, "start": 2023.48, "end": 2030.3600000000001, "text": " And when Pillow opens up a black and white image, it gives you back by default a rank", "tokens": [400, 562, 44656, 305, 9870, 493, 257, 2211, 293, 2418, 3256, 11, 309, 2709, 291, 646, 538, 7576, 257, 6181], "temperature": 0.0, "avg_logprob": -0.15376263079435928, "compression_ratio": 1.575609756097561, "no_speech_prob": 5.9547446653596126e-06}, {"id": 491, "seek": 201992, "start": 2030.3600000000001, "end": 2035.2, "text": " two tensor, just the x and y, no channel axis.", "tokens": [732, 40863, 11, 445, 264, 2031, 293, 288, 11, 572, 2269, 10298, 13], "temperature": 0.0, "avg_logprob": -0.15376263079435928, "compression_ratio": 1.575609756097561, "no_speech_prob": 5.9547446653596126e-06}, {"id": 492, "seek": 201992, "start": 2035.2, "end": 2041.0800000000002, "text": " And then you can't stack them into a mini batch because they're not all the same shape.", "tokens": [400, 550, 291, 393, 380, 8630, 552, 666, 257, 8382, 15245, 570, 436, 434, 406, 439, 264, 912, 3909, 13], "temperature": 0.0, "avg_logprob": -0.15376263079435928, "compression_ratio": 1.575609756097561, "no_speech_prob": 5.9547446653596126e-06}, {"id": 493, "seek": 201992, "start": 2041.0800000000002, "end": 2047.76, "text": " So what you can do is you can call the Pillow.convert and RGB.", "tokens": [407, 437, 291, 393, 360, 307, 291, 393, 818, 264, 44656, 305, 13, 1671, 3281, 293, 31231, 13], "temperature": 0.0, "avg_logprob": -0.15376263079435928, "compression_ratio": 1.575609756097561, "no_speech_prob": 5.9547446653596126e-06}, {"id": 494, "seek": 204776, "start": 2047.76, "end": 2051.92, "text": " If something's not RGB, it'll turn it into RGB.", "tokens": [759, 746, 311, 406, 31231, 11, 309, 603, 1261, 309, 666, 31231, 13], "temperature": 0.0, "avg_logprob": -0.15476499682795392, "compression_ratio": 1.6763636363636363, "no_speech_prob": 3.668743374873884e-06}, {"id": 495, "seek": 204776, "start": 2051.92, "end": 2055.6, "text": " So here's our first transform.", "tokens": [407, 510, 311, 527, 700, 4088, 13], "temperature": 0.0, "avg_logprob": -0.15476499682795392, "compression_ratio": 1.6763636363636363, "no_speech_prob": 3.668743374873884e-06}, {"id": 496, "seek": 204776, "start": 2055.6, "end": 2059.12, "text": " So a transform is just a class with an underscore order.", "tokens": [407, 257, 4088, 307, 445, 257, 1508, 365, 364, 37556, 1668, 13], "temperature": 0.0, "avg_logprob": -0.15476499682795392, "compression_ratio": 1.6763636363636363, "no_speech_prob": 3.668743374873884e-06}, {"id": 497, "seek": 204776, "start": 2059.12, "end": 2063.96, "text": " And then makeRGB is a transform that when you call it will call convert.", "tokens": [400, 550, 652, 49, 8769, 307, 257, 4088, 300, 562, 291, 818, 309, 486, 818, 7620, 13], "temperature": 0.0, "avg_logprob": -0.15476499682795392, "compression_ratio": 1.6763636363636363, "no_speech_prob": 3.668743374873884e-06}, {"id": 498, "seek": 204776, "start": 2063.96, "end": 2065.36, "text": " Or you could just do it this way.", "tokens": [1610, 291, 727, 445, 360, 309, 341, 636, 13], "temperature": 0.0, "avg_logprob": -0.15476499682795392, "compression_ratio": 1.6763636363636363, "no_speech_prob": 3.668743374873884e-06}, {"id": 499, "seek": 204776, "start": 2065.36, "end": 2067.04, "text": " Just make it a function.", "tokens": [1449, 652, 309, 257, 2445, 13], "temperature": 0.0, "avg_logprob": -0.15476499682795392, "compression_ratio": 1.6763636363636363, "no_speech_prob": 3.668743374873884e-06}, {"id": 500, "seek": 204776, "start": 2067.04, "end": 2068.36, "text": " Both is fine.", "tokens": [6767, 307, 2489, 13], "temperature": 0.0, "avg_logprob": -0.15476499682795392, "compression_ratio": 1.6763636363636363, "no_speech_prob": 3.668743374873884e-06}, {"id": 501, "seek": 204776, "start": 2068.36, "end": 2070.66, "text": " These are both going to do the same thing.", "tokens": [1981, 366, 1293, 516, 281, 360, 264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.15476499682795392, "compression_ratio": 1.6763636363636363, "no_speech_prob": 3.668743374873884e-06}, {"id": 502, "seek": 204776, "start": 2070.66, "end": 2072.0, "text": " And this is often the case, right?", "tokens": [400, 341, 307, 2049, 264, 1389, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.15476499682795392, "compression_ratio": 1.6763636363636363, "no_speech_prob": 3.668743374873884e-06}, {"id": 503, "seek": 204776, "start": 2072.0, "end": 2073.76, "text": " You can, we've seen it a bunch of times before.", "tokens": [509, 393, 11, 321, 600, 1612, 309, 257, 3840, 295, 1413, 949, 13], "temperature": 0.0, "avg_logprob": -0.15476499682795392, "compression_ratio": 1.6763636363636363, "no_speech_prob": 3.668743374873884e-06}, {"id": 504, "seek": 204776, "start": 2073.76, "end": 2077.32, "text": " You can have done to call or you can have a function.", "tokens": [509, 393, 362, 1096, 281, 818, 420, 291, 393, 362, 257, 2445, 13], "temperature": 0.0, "avg_logprob": -0.15476499682795392, "compression_ratio": 1.6763636363636363, "no_speech_prob": 3.668743374873884e-06}, {"id": 505, "seek": 207732, "start": 2077.32, "end": 2081.28, "text": " So here's our first transform.", "tokens": [407, 510, 311, 527, 700, 4088, 13], "temperature": 0.0, "avg_logprob": -0.16435913163788465, "compression_ratio": 1.6280193236714975, "no_speech_prob": 4.22273978983867e-06}, {"id": 506, "seek": 207732, "start": 2081.28, "end": 2082.28, "text": " And so here's the simplest version.", "tokens": [400, 370, 510, 311, 264, 22811, 3037, 13], "temperature": 0.0, "avg_logprob": -0.16435913163788465, "compression_ratio": 1.6280193236714975, "no_speech_prob": 4.22273978983867e-06}, {"id": 507, "seek": 207732, "start": 2082.28, "end": 2083.92, "text": " It's just a function.", "tokens": [467, 311, 445, 257, 2445, 13], "temperature": 0.0, "avg_logprob": -0.16435913163788465, "compression_ratio": 1.6280193236714975, "no_speech_prob": 4.22273978983867e-06}, {"id": 508, "seek": 207732, "start": 2083.92, "end": 2091.6200000000003, "text": " And so if we create a image list from files using our path and pass in that transform,", "tokens": [400, 370, 498, 321, 1884, 257, 3256, 1329, 490, 7098, 1228, 527, 3100, 293, 1320, 294, 300, 4088, 11], "temperature": 0.0, "avg_logprob": -0.16435913163788465, "compression_ratio": 1.6280193236714975, "no_speech_prob": 4.22273978983867e-06}, {"id": 509, "seek": 207732, "start": 2091.6200000000003, "end": 2094.4, "text": " then now we have an item list.", "tokens": [550, 586, 321, 362, 364, 3174, 1329, 13], "temperature": 0.0, "avg_logprob": -0.16435913163788465, "compression_ratio": 1.6280193236714975, "no_speech_prob": 4.22273978983867e-06}, {"id": 510, "seek": 207732, "start": 2094.4, "end": 2100.6400000000003, "text": " And remember that item list inherits from list container, which we gave a Dundar Repra", "tokens": [400, 1604, 300, 3174, 1329, 9484, 1208, 490, 1329, 10129, 11, 597, 321, 2729, 257, 413, 997, 289, 3696, 424], "temperature": 0.0, "avg_logprob": -0.16435913163788465, "compression_ratio": 1.6280193236714975, "no_speech_prob": 4.22273978983867e-06}, {"id": 511, "seek": 207732, "start": 2100.6400000000003, "end": 2101.6400000000003, "text": " to.", "tokens": [281, 13], "temperature": 0.0, "avg_logprob": -0.16435913163788465, "compression_ratio": 1.6280193236714975, "no_speech_prob": 4.22273978983867e-06}, {"id": 512, "seek": 207732, "start": 2101.6400000000003, "end": 2104.0, "text": " So it's going to give us nice printing.", "tokens": [407, 309, 311, 516, 281, 976, 505, 1481, 14699, 13], "temperature": 0.0, "avg_logprob": -0.16435913163788465, "compression_ratio": 1.6280193236714975, "no_speech_prob": 4.22273978983867e-06}, {"id": 513, "seek": 210400, "start": 2104.0, "end": 2109.2, "text": " This is why we create these little convenient things to subclass from, because we get all", "tokens": [639, 307, 983, 321, 1884, 613, 707, 10851, 721, 281, 1422, 11665, 490, 11, 570, 321, 483, 439], "temperature": 0.0, "avg_logprob": -0.15668338810631988, "compression_ratio": 1.625, "no_speech_prob": 4.71071643914911e-06}, {"id": 514, "seek": 210400, "start": 2109.2, "end": 2110.92, "text": " this behavior for free.", "tokens": [341, 5223, 337, 1737, 13], "temperature": 0.0, "avg_logprob": -0.15668338810631988, "compression_ratio": 1.625, "no_speech_prob": 4.71071643914911e-06}, {"id": 515, "seek": 210400, "start": 2110.92, "end": 2113.76, "text": " So we can now see that we've got 13,000 items.", "tokens": [407, 321, 393, 586, 536, 300, 321, 600, 658, 3705, 11, 1360, 4754, 13], "temperature": 0.0, "avg_logprob": -0.15668338810631988, "compression_ratio": 1.625, "no_speech_prob": 4.71071643914911e-06}, {"id": 516, "seek": 210400, "start": 2113.76, "end": 2115.4, "text": " And here's a few of them.", "tokens": [400, 510, 311, 257, 1326, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.15668338810631988, "compression_ratio": 1.625, "no_speech_prob": 4.71071643914911e-06}, {"id": 517, "seek": 210400, "start": 2115.4, "end": 2117.8, "text": " Is the path.", "tokens": [1119, 264, 3100, 13], "temperature": 0.0, "avg_logprob": -0.15668338810631988, "compression_ratio": 1.625, "no_speech_prob": 4.71071643914911e-06}, {"id": 518, "seek": 210400, "start": 2117.8, "end": 2119.6, "text": " And we can index into it.", "tokens": [400, 321, 393, 8186, 666, 309, 13], "temperature": 0.0, "avg_logprob": -0.15668338810631988, "compression_ratio": 1.625, "no_speech_prob": 4.71071643914911e-06}, {"id": 519, "seek": 210400, "start": 2119.6, "end": 2126.52, "text": " And when we index into it, it calls get and get calls image.open.", "tokens": [400, 562, 321, 8186, 666, 309, 11, 309, 5498, 483, 293, 483, 5498, 3256, 13, 15752, 13], "temperature": 0.0, "avg_logprob": -0.15668338810631988, "compression_ratio": 1.625, "no_speech_prob": 4.71071643914911e-06}, {"id": 520, "seek": 210400, "start": 2126.52, "end": 2129.34, "text": " And pillow automatically displays images in Jupyter.", "tokens": [400, 18581, 6772, 20119, 5267, 294, 22125, 88, 391, 13], "temperature": 0.0, "avg_logprob": -0.15668338810631988, "compression_ratio": 1.625, "no_speech_prob": 4.71071643914911e-06}, {"id": 521, "seek": 210400, "start": 2129.34, "end": 2130.72, "text": " And so there it is.", "tokens": [400, 370, 456, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.15668338810631988, "compression_ratio": 1.625, "no_speech_prob": 4.71071643914911e-06}, {"id": 522, "seek": 213072, "start": 2130.72, "end": 2134.2799999999997, "text": " And this, of course, is a man with a tange.", "tokens": [400, 341, 11, 295, 1164, 11, 307, 257, 587, 365, 257, 256, 933, 13], "temperature": 0.0, "avg_logprob": -0.17543694952956768, "compression_ratio": 1.5677966101694916, "no_speech_prob": 3.0239549232646823e-05}, {"id": 523, "seek": 213072, "start": 2134.2799999999997, "end": 2135.2799999999997, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.17543694952956768, "compression_ratio": 1.5677966101694916, "no_speech_prob": 3.0239549232646823e-05}, {"id": 524, "seek": 213072, "start": 2135.2799999999997, "end": 2136.2799999999997, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.17543694952956768, "compression_ratio": 1.5677966101694916, "no_speech_prob": 3.0239549232646823e-05}, {"id": 525, "seek": 213072, "start": 2136.2799999999997, "end": 2137.2799999999997, "text": " OK.", "tokens": [2264, 13], "temperature": 0.0, "avg_logprob": -0.17543694952956768, "compression_ratio": 1.5677966101694916, "no_speech_prob": 3.0239549232646823e-05}, {"id": 526, "seek": 213072, "start": 2137.2799999999997, "end": 2139.8399999999997, "text": " He looks very happy with it.", "tokens": [634, 1542, 588, 2055, 365, 309, 13], "temperature": 0.0, "avg_logprob": -0.17543694952956768, "compression_ratio": 1.5677966101694916, "no_speech_prob": 3.0239549232646823e-05}, {"id": 527, "seek": 213072, "start": 2139.8399999999997, "end": 2142.7999999999997, "text": " We're going to be seeing him a lot.", "tokens": [492, 434, 516, 281, 312, 2577, 796, 257, 688, 13], "temperature": 0.0, "avg_logprob": -0.17543694952956768, "compression_ratio": 1.5677966101694916, "no_speech_prob": 3.0239549232646823e-05}, {"id": 528, "seek": 213072, "start": 2142.7999999999997, "end": 2147.72, "text": " And because we're using the functionality that we wrote last time for list container,", "tokens": [400, 570, 321, 434, 1228, 264, 14980, 300, 321, 4114, 1036, 565, 337, 1329, 10129, 11], "temperature": 0.0, "avg_logprob": -0.17543694952956768, "compression_ratio": 1.5677966101694916, "no_speech_prob": 3.0239549232646823e-05}, {"id": 529, "seek": 213072, "start": 2147.72, "end": 2154.24, "text": " we can also index with a list of Booleans, with a slice, with a list of ints, and so", "tokens": [321, 393, 611, 8186, 365, 257, 1329, 295, 23351, 24008, 11, 365, 257, 13153, 11, 365, 257, 1329, 295, 560, 82, 11, 293, 370], "temperature": 0.0, "avg_logprob": -0.17543694952956768, "compression_ratio": 1.5677966101694916, "no_speech_prob": 3.0239549232646823e-05}, {"id": 530, "seek": 213072, "start": 2154.24, "end": 2155.24, "text": " forth.", "tokens": [5220, 13], "temperature": 0.0, "avg_logprob": -0.17543694952956768, "compression_ratio": 1.5677966101694916, "no_speech_prob": 3.0239549232646823e-05}, {"id": 531, "seek": 213072, "start": 2155.24, "end": 2159.2799999999997, "text": " So here's a slice containing one item, for instance.", "tokens": [407, 510, 311, 257, 13153, 19273, 472, 3174, 11, 337, 5197, 13], "temperature": 0.0, "avg_logprob": -0.17543694952956768, "compression_ratio": 1.5677966101694916, "no_speech_prob": 3.0239549232646823e-05}, {"id": 532, "seek": 213072, "start": 2159.2799999999997, "end": 2160.2799999999997, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.17543694952956768, "compression_ratio": 1.5677966101694916, "no_speech_prob": 3.0239549232646823e-05}, {"id": 533, "seek": 216028, "start": 2160.28, "end": 2162.92, "text": " So that's step one.", "tokens": [407, 300, 311, 1823, 472, 13], "temperature": 0.0, "avg_logprob": -0.12598374310661764, "compression_ratio": 1.9457831325301205, "no_speech_prob": 1.165850153483916e-05}, {"id": 534, "seek": 216028, "start": 2162.92, "end": 2166.6800000000003, "text": " Step two, split validation set.", "tokens": [5470, 732, 11, 7472, 24071, 992, 13], "temperature": 0.0, "avg_logprob": -0.12598374310661764, "compression_ratio": 1.9457831325301205, "no_speech_prob": 1.165850153483916e-05}, {"id": 535, "seek": 216028, "start": 2166.6800000000003, "end": 2171.44, "text": " So to do that, we look and we see here's a path.", "tokens": [407, 281, 360, 300, 11, 321, 574, 293, 321, 536, 510, 311, 257, 3100, 13], "temperature": 0.0, "avg_logprob": -0.12598374310661764, "compression_ratio": 1.9457831325301205, "no_speech_prob": 1.165850153483916e-05}, {"id": 536, "seek": 216028, "start": 2171.44, "end": 2173.28, "text": " Here's the file name.", "tokens": [1692, 311, 264, 3991, 1315, 13], "temperature": 0.0, "avg_logprob": -0.12598374310661764, "compression_ratio": 1.9457831325301205, "no_speech_prob": 1.165850153483916e-05}, {"id": 537, "seek": 216028, "start": 2173.28, "end": 2174.28, "text": " Here's the parent.", "tokens": [1692, 311, 264, 2596, 13], "temperature": 0.0, "avg_logprob": -0.12598374310661764, "compression_ratio": 1.9457831325301205, "no_speech_prob": 1.165850153483916e-05}, {"id": 538, "seek": 216028, "start": 2174.28, "end": 2176.9, "text": " Here's the grandparent.", "tokens": [1692, 311, 264, 2697, 38321, 13], "temperature": 0.0, "avg_logprob": -0.12598374310661764, "compression_ratio": 1.9457831325301205, "no_speech_prob": 1.165850153483916e-05}, {"id": 539, "seek": 216028, "start": 2176.9, "end": 2179.6000000000004, "text": " So here's the grandparent's name.", "tokens": [407, 510, 311, 264, 2697, 38321, 311, 1315, 13], "temperature": 0.0, "avg_logprob": -0.12598374310661764, "compression_ratio": 1.9457831325301205, "no_speech_prob": 1.165850153483916e-05}, {"id": 540, "seek": 216028, "start": 2179.6000000000004, "end": 2182.0, "text": " That's the thing we use to split.", "tokens": [663, 311, 264, 551, 321, 764, 281, 7472, 13], "temperature": 0.0, "avg_logprob": -0.12598374310661764, "compression_ratio": 1.9457831325301205, "no_speech_prob": 1.165850153483916e-05}, {"id": 541, "seek": 216028, "start": 2182.0, "end": 2187.6000000000004, "text": " So let's create a function called grandparent splitter that grabs the grandparent's name.", "tokens": [407, 718, 311, 1884, 257, 2445, 1219, 2697, 38321, 4732, 3904, 300, 30028, 264, 2697, 38321, 311, 1315, 13], "temperature": 0.0, "avg_logprob": -0.12598374310661764, "compression_ratio": 1.9457831325301205, "no_speech_prob": 1.165850153483916e-05}, {"id": 542, "seek": 218760, "start": 2187.6, "end": 2191.0, "text": " And you call it, telling it the name of your validation set and the name of your training", "tokens": [400, 291, 818, 309, 11, 3585, 309, 264, 1315, 295, 428, 24071, 992, 293, 264, 1315, 295, 428, 3097], "temperature": 0.0, "avg_logprob": -0.08710676623928931, "compression_ratio": 2.0130434782608697, "no_speech_prob": 7.296182957361452e-06}, {"id": 543, "seek": 218760, "start": 2191.0, "end": 2192.0, "text": " set.", "tokens": [992, 13], "temperature": 0.0, "avg_logprob": -0.08710676623928931, "compression_ratio": 2.0130434782608697, "no_speech_prob": 7.296182957361452e-06}, {"id": 544, "seek": 218760, "start": 2192.0, "end": 2199.52, "text": " And it returns true if it's the validation set or false if it's the training set or none", "tokens": [400, 309, 11247, 2074, 498, 309, 311, 264, 24071, 992, 420, 7908, 498, 309, 311, 264, 3097, 992, 420, 6022], "temperature": 0.0, "avg_logprob": -0.08710676623928931, "compression_ratio": 2.0130434782608697, "no_speech_prob": 7.296182957361452e-06}, {"id": 545, "seek": 218760, "start": 2199.52, "end": 2201.7599999999998, "text": " if it's neither.", "tokens": [498, 309, 311, 9662, 13], "temperature": 0.0, "avg_logprob": -0.08710676623928931, "compression_ratio": 2.0130434782608697, "no_speech_prob": 7.296182957361452e-06}, {"id": 546, "seek": 218760, "start": 2201.7599999999998, "end": 2209.12, "text": " And so here's something that will create a mask containing your pass at some function.", "tokens": [400, 370, 510, 311, 746, 300, 486, 1884, 257, 6094, 19273, 428, 1320, 412, 512, 2445, 13], "temperature": 0.0, "avg_logprob": -0.08710676623928931, "compression_ratio": 2.0130434782608697, "no_speech_prob": 7.296182957361452e-06}, {"id": 547, "seek": 218760, "start": 2209.12, "end": 2211.48, "text": " So we're going to be using grandparent splitter.", "tokens": [407, 321, 434, 516, 281, 312, 1228, 2697, 38321, 4732, 3904, 13], "temperature": 0.0, "avg_logprob": -0.08710676623928931, "compression_ratio": 2.0130434782608697, "no_speech_prob": 7.296182957361452e-06}, {"id": 548, "seek": 218760, "start": 2211.48, "end": 2214.92, "text": " And it will just grab all the things where that mask is false.", "tokens": [400, 309, 486, 445, 4444, 439, 264, 721, 689, 300, 6094, 307, 7908, 13], "temperature": 0.0, "avg_logprob": -0.08710676623928931, "compression_ratio": 2.0130434782608697, "no_speech_prob": 7.296182957361452e-06}, {"id": 549, "seek": 218760, "start": 2214.92, "end": 2215.92, "text": " That's the training set.", "tokens": [663, 311, 264, 3097, 992, 13], "temperature": 0.0, "avg_logprob": -0.08710676623928931, "compression_ratio": 2.0130434782608697, "no_speech_prob": 7.296182957361452e-06}, {"id": 550, "seek": 218760, "start": 2215.92, "end": 2217.48, "text": " All the things where the mask is true.", "tokens": [1057, 264, 721, 689, 264, 6094, 307, 2074, 13], "temperature": 0.0, "avg_logprob": -0.08710676623928931, "compression_ratio": 2.0130434782608697, "no_speech_prob": 7.296182957361452e-06}, {"id": 551, "seek": 221748, "start": 2217.48, "end": 2219.92, "text": " That's the validation set and it will return them.", "tokens": [663, 311, 264, 24071, 992, 293, 309, 486, 2736, 552, 13], "temperature": 0.0, "avg_logprob": -0.1866082816288389, "compression_ratio": 1.6877637130801688, "no_speech_prob": 5.862719262950122e-06}, {"id": 552, "seek": 221748, "start": 2219.92, "end": 2220.92, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.1866082816288389, "compression_ratio": 1.6877637130801688, "no_speech_prob": 5.862719262950122e-06}, {"id": 553, "seek": 221748, "start": 2220.92, "end": 2222.28, "text": " So there's our splitter.", "tokens": [407, 456, 311, 527, 4732, 3904, 13], "temperature": 0.0, "avg_logprob": -0.1866082816288389, "compression_ratio": 1.6877637130801688, "no_speech_prob": 5.862719262950122e-06}, {"id": 554, "seek": 221748, "start": 2222.28, "end": 2225.0, "text": " Remember, we used partial.", "tokens": [5459, 11, 321, 1143, 14641, 13], "temperature": 0.0, "avg_logprob": -0.1866082816288389, "compression_ratio": 1.6877637130801688, "no_speech_prob": 5.862719262950122e-06}, {"id": 555, "seek": 221748, "start": 2225.0, "end": 2230.84, "text": " So here's a splitter that splits on grandparents and where the validation name is val because", "tokens": [407, 510, 311, 257, 4732, 3904, 300, 37741, 322, 21876, 293, 689, 264, 24071, 1315, 307, 1323, 570], "temperature": 0.0, "avg_logprob": -0.1866082816288389, "compression_ratio": 1.6877637130801688, "no_speech_prob": 5.862719262950122e-06}, {"id": 556, "seek": 221748, "start": 2230.84, "end": 2234.48, "text": " that's what it is for image net.", "tokens": [300, 311, 437, 309, 307, 337, 3256, 2533, 13], "temperature": 0.0, "avg_logprob": -0.1866082816288389, "compression_ratio": 1.6877637130801688, "no_speech_prob": 5.862719262950122e-06}, {"id": 557, "seek": 221748, "start": 2234.48, "end": 2236.72, "text": " And let's check that that seems to work.", "tokens": [400, 718, 311, 1520, 300, 300, 2544, 281, 589, 13], "temperature": 0.0, "avg_logprob": -0.1866082816288389, "compression_ratio": 1.6877637130801688, "no_speech_prob": 5.862719262950122e-06}, {"id": 558, "seek": 221748, "start": 2236.72, "end": 2237.72, "text": " Yes, it does.", "tokens": [1079, 11, 309, 775, 13], "temperature": 0.0, "avg_logprob": -0.1866082816288389, "compression_ratio": 1.6877637130801688, "no_speech_prob": 5.862719262950122e-06}, {"id": 559, "seek": 221748, "start": 2237.72, "end": 2244.12, "text": " We've now got a validation set with 500 things and a training set with 12,800 things.", "tokens": [492, 600, 586, 658, 257, 24071, 992, 365, 5923, 721, 293, 257, 3097, 992, 365, 2272, 11, 14423, 721, 13], "temperature": 0.0, "avg_logprob": -0.1866082816288389, "compression_ratio": 1.6877637130801688, "no_speech_prob": 5.862719262950122e-06}, {"id": 560, "seek": 221748, "start": 2244.12, "end": 2246.72, "text": " So that's looking good.", "tokens": [407, 300, 311, 1237, 665, 13], "temperature": 0.0, "avg_logprob": -0.1866082816288389, "compression_ratio": 1.6877637130801688, "no_speech_prob": 5.862719262950122e-06}, {"id": 561, "seek": 224672, "start": 2246.72, "end": 2248.4399999999996, "text": " So let's use it.", "tokens": [407, 718, 311, 764, 309, 13], "temperature": 0.0, "avg_logprob": -0.1351743425641741, "compression_ratio": 1.6272727272727272, "no_speech_prob": 4.092752078577178e-06}, {"id": 562, "seek": 224672, "start": 2248.4399999999996, "end": 2253.3199999999997, "text": " So split data object is just something with a training set and a validation set.", "tokens": [407, 7472, 1412, 2657, 307, 445, 746, 365, 257, 3097, 992, 293, 257, 24071, 992, 13], "temperature": 0.0, "avg_logprob": -0.1351743425641741, "compression_ratio": 1.6272727272727272, "no_speech_prob": 4.092752078577178e-06}, {"id": 563, "seek": 224672, "start": 2253.3199999999997, "end": 2254.3199999999997, "text": " You pass it in.", "tokens": [509, 1320, 309, 294, 13], "temperature": 0.0, "avg_logprob": -0.1351743425641741, "compression_ratio": 1.6272727272727272, "no_speech_prob": 4.092752078577178e-06}, {"id": 564, "seek": 224672, "start": 2254.3199999999997, "end": 2257.2, "text": " You save them away.", "tokens": [509, 3155, 552, 1314, 13], "temperature": 0.0, "avg_logprob": -0.1351743425641741, "compression_ratio": 1.6272727272727272, "no_speech_prob": 4.092752078577178e-06}, {"id": 565, "seek": 224672, "start": 2257.2, "end": 2261.3199999999997, "text": " And then that's basically it.", "tokens": [400, 550, 300, 311, 1936, 309, 13], "temperature": 0.0, "avg_logprob": -0.1351743425641741, "compression_ratio": 1.6272727272727272, "no_speech_prob": 4.092752078577178e-06}, {"id": 566, "seek": 224672, "start": 2261.3199999999997, "end": 2264.2, "text": " Everything else from here is just convenience.", "tokens": [5471, 1646, 490, 510, 307, 445, 19283, 13], "temperature": 0.0, "avg_logprob": -0.1351743425641741, "compression_ratio": 1.6272727272727272, "no_speech_prob": 4.092752078577178e-06}, {"id": 567, "seek": 224672, "start": 2264.2, "end": 2268.56, "text": " So we'll give it a representation so that you can print it.", "tokens": [407, 321, 603, 976, 309, 257, 10290, 370, 300, 291, 393, 4482, 309, 13], "temperature": 0.0, "avg_logprob": -0.1351743425641741, "compression_ratio": 1.6272727272727272, "no_speech_prob": 4.092752078577178e-06}, {"id": 568, "seek": 224672, "start": 2268.56, "end": 2272.3999999999996, "text": " We'll define dunder get attribute so that if you pass it some attribute that it doesn't", "tokens": [492, 603, 6964, 274, 6617, 483, 19667, 370, 300, 498, 291, 1320, 309, 512, 19667, 300, 309, 1177, 380], "temperature": 0.0, "avg_logprob": -0.1351743425641741, "compression_ratio": 1.6272727272727272, "no_speech_prob": 4.092752078577178e-06}, {"id": 569, "seek": 227240, "start": 2272.4, "end": 2276.92, "text": " know about, it'll grab it from the training set.", "tokens": [458, 466, 11, 309, 603, 4444, 309, 490, 264, 3097, 992, 13], "temperature": 0.0, "avg_logprob": -0.11427161859911542, "compression_ratio": 1.581151832460733, "no_speech_prob": 6.048732302588178e-06}, {"id": 570, "seek": 227240, "start": 2276.92, "end": 2285.8, "text": " And then let's add a split by funk method that just calls that split by funk thing we", "tokens": [400, 550, 718, 311, 909, 257, 7472, 538, 26476, 3170, 300, 445, 5498, 300, 7472, 538, 26476, 551, 321], "temperature": 0.0, "avg_logprob": -0.11427161859911542, "compression_ratio": 1.581151832460733, "no_speech_prob": 6.048732302588178e-06}, {"id": 571, "seek": 227240, "start": 2285.8, "end": 2287.12, "text": " just had.", "tokens": [445, 632, 13], "temperature": 0.0, "avg_logprob": -0.11427161859911542, "compression_ratio": 1.581151832460733, "no_speech_prob": 6.048732302588178e-06}, {"id": 572, "seek": 227240, "start": 2287.12, "end": 2294.0, "text": " There's one trick here though, which is we want split by funk to return item lists of", "tokens": [821, 311, 472, 4282, 510, 1673, 11, 597, 307, 321, 528, 7472, 538, 26476, 281, 2736, 3174, 14511, 295], "temperature": 0.0, "avg_logprob": -0.11427161859911542, "compression_ratio": 1.581151832460733, "no_speech_prob": 6.048732302588178e-06}, {"id": 573, "seek": 227240, "start": 2294.0, "end": 2297.1600000000003, "text": " the same type that we gave it.", "tokens": [264, 912, 2010, 300, 321, 2729, 309, 13], "temperature": 0.0, "avg_logprob": -0.11427161859911542, "compression_ratio": 1.581151832460733, "no_speech_prob": 6.048732302588178e-06}, {"id": 574, "seek": 227240, "start": 2297.1600000000003, "end": 2300.2400000000002, "text": " In this case, it would be an image list.", "tokens": [682, 341, 1389, 11, 309, 576, 312, 364, 3256, 1329, 13], "temperature": 0.0, "avg_logprob": -0.11427161859911542, "compression_ratio": 1.581151832460733, "no_speech_prob": 6.048732302588178e-06}, {"id": 575, "seek": 230024, "start": 2300.24, "end": 2303.56, "text": " So we call item list.new.", "tokens": [407, 321, 818, 3174, 1329, 13, 7686, 13], "temperature": 0.0, "avg_logprob": -0.14961227348872594, "compression_ratio": 1.6026785714285714, "no_speech_prob": 2.684153287191293e-06}, {"id": 576, "seek": 230024, "start": 2303.56, "end": 2313.08, "text": " And that's why in our item list we defined something called new.", "tokens": [400, 300, 311, 983, 294, 527, 3174, 1329, 321, 7642, 746, 1219, 777, 13], "temperature": 0.0, "avg_logprob": -0.14961227348872594, "compression_ratio": 1.6026785714285714, "no_speech_prob": 2.684153287191293e-06}, {"id": 577, "seek": 230024, "start": 2313.08, "end": 2315.4799999999996, "text": " And this is a really handy trick.", "tokens": [400, 341, 307, 257, 534, 13239, 4282, 13], "temperature": 0.0, "avg_logprob": -0.14961227348872594, "compression_ratio": 1.6026785714285714, "no_speech_prob": 2.684153287191293e-06}, {"id": 578, "seek": 230024, "start": 2315.4799999999996, "end": 2319.3999999999996, "text": " PyTorch has the concept of a new method as well.", "tokens": [9953, 51, 284, 339, 575, 264, 3410, 295, 257, 777, 3170, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.14961227348872594, "compression_ratio": 1.6026785714285714, "no_speech_prob": 2.684153287191293e-06}, {"id": 579, "seek": 230024, "start": 2319.3999999999996, "end": 2322.24, "text": " It says, all right, let's look at this object.", "tokens": [467, 1619, 11, 439, 558, 11, 718, 311, 574, 412, 341, 2657, 13], "temperature": 0.0, "avg_logprob": -0.14961227348872594, "compression_ratio": 1.6026785714285714, "no_speech_prob": 2.684153287191293e-06}, {"id": 580, "seek": 230024, "start": 2322.24, "end": 2323.9199999999996, "text": " Let's see what class it is.", "tokens": [961, 311, 536, 437, 1508, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.14961227348872594, "compression_ratio": 1.6026785714285714, "no_speech_prob": 2.684153287191293e-06}, {"id": 581, "seek": 230024, "start": 2323.9199999999996, "end": 2325.16, "text": " Because it might not be item list, right?", "tokens": [1436, 309, 1062, 406, 312, 3174, 1329, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.14961227348872594, "compression_ratio": 1.6026785714285714, "no_speech_prob": 2.684153287191293e-06}, {"id": 582, "seek": 230024, "start": 2325.16, "end": 2327.72, "text": " It might be image list or some other subclass.", "tokens": [467, 1062, 312, 3256, 1329, 420, 512, 661, 1422, 11665, 13], "temperature": 0.0, "avg_logprob": -0.14961227348872594, "compression_ratio": 1.6026785714285714, "no_speech_prob": 2.684153287191293e-06}, {"id": 583, "seek": 230024, "start": 2327.72, "end": 2329.8799999999997, "text": " It doesn't exist yet.", "tokens": [467, 1177, 380, 2514, 1939, 13], "temperature": 0.0, "avg_logprob": -0.14961227348872594, "compression_ratio": 1.6026785714285714, "no_speech_prob": 2.684153287191293e-06}, {"id": 584, "seek": 232988, "start": 2329.88, "end": 2333.0, "text": " And this is now the constructor for that class.", "tokens": [400, 341, 307, 586, 264, 47479, 337, 300, 1508, 13], "temperature": 0.0, "avg_logprob": -0.14109617350052814, "compression_ratio": 1.8267326732673268, "no_speech_prob": 1.012967277347343e-05}, {"id": 585, "seek": 232988, "start": 2333.0, "end": 2336.7200000000003, "text": " And it's just passing in the items that we asked for.", "tokens": [400, 309, 311, 445, 8437, 294, 264, 4754, 300, 321, 2351, 337, 13], "temperature": 0.0, "avg_logprob": -0.14109617350052814, "compression_ratio": 1.8267326732673268, "no_speech_prob": 1.012967277347343e-05}, {"id": 586, "seek": 232988, "start": 2336.7200000000003, "end": 2339.6800000000003, "text": " And then passing in our path and our transforms.", "tokens": [400, 550, 8437, 294, 527, 3100, 293, 527, 35592, 13], "temperature": 0.0, "avg_logprob": -0.14109617350052814, "compression_ratio": 1.8267326732673268, "no_speech_prob": 1.012967277347343e-05}, {"id": 587, "seek": 232988, "start": 2339.6800000000003, "end": 2345.2000000000003, "text": " So new is going to create a new item list of the same type, with the same path, and", "tokens": [407, 777, 307, 516, 281, 1884, 257, 777, 3174, 1329, 295, 264, 912, 2010, 11, 365, 264, 912, 3100, 11, 293], "temperature": 0.0, "avg_logprob": -0.14109617350052814, "compression_ratio": 1.8267326732673268, "no_speech_prob": 1.012967277347343e-05}, {"id": 588, "seek": 232988, "start": 2345.2000000000003, "end": 2348.4, "text": " the same transforms, but with these new items.", "tokens": [264, 912, 35592, 11, 457, 365, 613, 777, 4754, 13], "temperature": 0.0, "avg_logprob": -0.14109617350052814, "compression_ratio": 1.8267326732673268, "no_speech_prob": 1.012967277347343e-05}, {"id": 589, "seek": 232988, "start": 2348.4, "end": 2357.6800000000003, "text": " And so that's why this is now going to give us a training set and a validation set with", "tokens": [400, 370, 300, 311, 983, 341, 307, 586, 516, 281, 976, 505, 257, 3097, 992, 293, 257, 24071, 992, 365], "temperature": 0.0, "avg_logprob": -0.14109617350052814, "compression_ratio": 1.8267326732673268, "no_speech_prob": 1.012967277347343e-05}, {"id": 590, "seek": 235768, "start": 2357.68, "end": 2360.66, "text": " the same path, the same transforms, and the same type.", "tokens": [264, 912, 3100, 11, 264, 912, 35592, 11, 293, 264, 912, 2010, 13], "temperature": 0.0, "avg_logprob": -0.1316435782463996, "compression_ratio": 1.5757575757575757, "no_speech_prob": 5.771752057626145e-06}, {"id": 591, "seek": 235768, "start": 2360.66, "end": 2365.8799999999997, "text": " And so if we call split data, split by func, now you can see we've got our training set", "tokens": [400, 370, 498, 321, 818, 7472, 1412, 11, 7472, 538, 1019, 66, 11, 586, 291, 393, 536, 321, 600, 658, 527, 3097, 992], "temperature": 0.0, "avg_logprob": -0.1316435782463996, "compression_ratio": 1.5757575757575757, "no_speech_prob": 5.771752057626145e-06}, {"id": 592, "seek": 235768, "start": 2365.8799999999997, "end": 2368.52, "text": " and our validation set.", "tokens": [293, 527, 24071, 992, 13], "temperature": 0.0, "avg_logprob": -0.1316435782463996, "compression_ratio": 1.5757575757575757, "no_speech_prob": 5.771752057626145e-06}, {"id": 593, "seek": 235768, "start": 2368.52, "end": 2370.48, "text": " Easy.", "tokens": [16002, 13], "temperature": 0.0, "avg_logprob": -0.1316435782463996, "compression_ratio": 1.5757575757575757, "no_speech_prob": 5.771752057626145e-06}, {"id": 594, "seek": 235768, "start": 2370.48, "end": 2377.08, "text": " So next in our list of things to do is labeling.", "tokens": [407, 958, 294, 527, 1329, 295, 721, 281, 360, 307, 40244, 13], "temperature": 0.0, "avg_logprob": -0.1316435782463996, "compression_ratio": 1.5757575757575757, "no_speech_prob": 5.771752057626145e-06}, {"id": 595, "seek": 235768, "start": 2377.08, "end": 2381.12, "text": " Labeling is a little more tricky.", "tokens": [10137, 11031, 307, 257, 707, 544, 12414, 13], "temperature": 0.0, "avg_logprob": -0.1316435782463996, "compression_ratio": 1.5757575757575757, "no_speech_prob": 5.771752057626145e-06}, {"id": 596, "seek": 235768, "start": 2381.12, "end": 2385.72, "text": " And the reason it's tricky is because we need processes.", "tokens": [400, 264, 1778, 309, 311, 12414, 307, 570, 321, 643, 7555, 13], "temperature": 0.0, "avg_logprob": -0.1316435782463996, "compression_ratio": 1.5757575757575757, "no_speech_prob": 5.771752057626145e-06}, {"id": 597, "seek": 238572, "start": 2385.72, "end": 2389.7999999999997, "text": " These are things which are first applied to the training set.", "tokens": [1981, 366, 721, 597, 366, 700, 6456, 281, 264, 3097, 992, 13], "temperature": 0.0, "avg_logprob": -0.1369376771905449, "compression_ratio": 1.588235294117647, "no_speech_prob": 4.7107532736845315e-06}, {"id": 598, "seek": 238572, "start": 2389.7999999999997, "end": 2391.68, "text": " They get some state.", "tokens": [814, 483, 512, 1785, 13], "temperature": 0.0, "avg_logprob": -0.1369376771905449, "compression_ratio": 1.588235294117647, "no_speech_prob": 4.7107532736845315e-06}, {"id": 599, "seek": 238572, "start": 2391.68, "end": 2394.72, "text": " And then they get applied to the validation set.", "tokens": [400, 550, 436, 483, 6456, 281, 264, 24071, 992, 13], "temperature": 0.0, "avg_logprob": -0.1369376771905449, "compression_ratio": 1.588235294117647, "no_speech_prob": 4.7107532736845315e-06}, {"id": 600, "seek": 238572, "start": 2394.72, "end": 2402.4599999999996, "text": " For example, our labels should not be tench and french horn.", "tokens": [1171, 1365, 11, 527, 16949, 820, 406, 312, 2064, 339, 293, 27598, 13482, 13], "temperature": 0.0, "avg_logprob": -0.1369376771905449, "compression_ratio": 1.588235294117647, "no_speech_prob": 4.7107532736845315e-06}, {"id": 601, "seek": 238572, "start": 2402.4599999999996, "end": 2405.72, "text": " They should be like zero and two.", "tokens": [814, 820, 312, 411, 4018, 293, 732, 13], "temperature": 0.0, "avg_logprob": -0.1369376771905449, "compression_ratio": 1.588235294117647, "no_speech_prob": 4.7107532736845315e-06}, {"id": 602, "seek": 238572, "start": 2405.72, "end": 2410.64, "text": " Because when we go to do a cross entropy loss, we expect to see a long there, not a string", "tokens": [1436, 562, 321, 352, 281, 360, 257, 3278, 30867, 4470, 11, 321, 2066, 281, 536, 257, 938, 456, 11, 406, 257, 6798], "temperature": 0.0, "avg_logprob": -0.1369376771905449, "compression_ratio": 1.588235294117647, "no_speech_prob": 4.7107532736845315e-06}, {"id": 603, "seek": 238572, "start": 2410.64, "end": 2412.68, "text": " there.", "tokens": [456, 13], "temperature": 0.0, "avg_logprob": -0.1369376771905449, "compression_ratio": 1.588235294117647, "no_speech_prob": 4.7107532736845315e-06}, {"id": 604, "seek": 241268, "start": 2412.68, "end": 2418.48, "text": " So we need to be able to map tench to zero, or french horn to two.", "tokens": [407, 321, 643, 281, 312, 1075, 281, 4471, 2064, 339, 281, 4018, 11, 420, 27598, 13482, 281, 732, 13], "temperature": 0.0, "avg_logprob": -0.07690527728784864, "compression_ratio": 1.781512605042017, "no_speech_prob": 1.3496835435944377e-06}, {"id": 605, "seek": 241268, "start": 2418.48, "end": 2422.94, "text": " We need the training set to have the same mapping as the validation set.", "tokens": [492, 643, 264, 3097, 992, 281, 362, 264, 912, 18350, 382, 264, 24071, 992, 13], "temperature": 0.0, "avg_logprob": -0.07690527728784864, "compression_ratio": 1.781512605042017, "no_speech_prob": 1.3496835435944377e-06}, {"id": 606, "seek": 241268, "start": 2422.94, "end": 2426.8399999999997, "text": " And for any inference we do in the future, it's going to have the same mapping as well.", "tokens": [400, 337, 604, 38253, 321, 360, 294, 264, 2027, 11, 309, 311, 516, 281, 362, 264, 912, 18350, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.07690527728784864, "compression_ratio": 1.781512605042017, "no_speech_prob": 1.3496835435944377e-06}, {"id": 607, "seek": 241268, "start": 2426.8399999999997, "end": 2432.16, "text": " Because otherwise, the different data sets are going to be talking about completely different", "tokens": [1436, 5911, 11, 264, 819, 1412, 6352, 366, 516, 281, 312, 1417, 466, 2584, 819], "temperature": 0.0, "avg_logprob": -0.07690527728784864, "compression_ratio": 1.781512605042017, "no_speech_prob": 1.3496835435944377e-06}, {"id": 608, "seek": 241268, "start": 2432.16, "end": 2435.68, "text": " things when they see the number zero, for instance.", "tokens": [721, 562, 436, 536, 264, 1230, 4018, 11, 337, 5197, 13], "temperature": 0.0, "avg_logprob": -0.07690527728784864, "compression_ratio": 1.781512605042017, "no_speech_prob": 1.3496835435944377e-06}, {"id": 609, "seek": 241268, "start": 2435.68, "end": 2438.7, "text": " So we're going to create something called a vocab.", "tokens": [407, 321, 434, 516, 281, 1884, 746, 1219, 257, 2329, 455, 13], "temperature": 0.0, "avg_logprob": -0.07690527728784864, "compression_ratio": 1.781512605042017, "no_speech_prob": 1.3496835435944377e-06}, {"id": 610, "seek": 243870, "start": 2438.7, "end": 2443.56, "text": " And a vocab is just the list saying these are our classes, and this is the order they're", "tokens": [400, 257, 2329, 455, 307, 445, 264, 1329, 1566, 613, 366, 527, 5359, 11, 293, 341, 307, 264, 1668, 436, 434], "temperature": 0.0, "avg_logprob": -0.0905397760457006, "compression_ratio": 1.8604651162790697, "no_speech_prob": 7.411053957184777e-06}, {"id": 611, "seek": 243870, "start": 2443.56, "end": 2444.56, "text": " in.", "tokens": [294, 13], "temperature": 0.0, "avg_logprob": -0.0905397760457006, "compression_ratio": 1.8604651162790697, "no_speech_prob": 7.411053957184777e-06}, {"id": 612, "seek": 243870, "start": 2444.56, "end": 2450.48, "text": " Zero is tench, one is golf ball, two is french horn, and so forth.", "tokens": [17182, 307, 2064, 339, 11, 472, 307, 12880, 2594, 11, 732, 307, 27598, 13482, 11, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.0905397760457006, "compression_ratio": 1.8604651162790697, "no_speech_prob": 7.411053957184777e-06}, {"id": 613, "seek": 243870, "start": 2450.48, "end": 2454.3599999999997, "text": " So we're going to create the vocab from the training set.", "tokens": [407, 321, 434, 516, 281, 1884, 264, 2329, 455, 490, 264, 3097, 992, 13], "temperature": 0.0, "avg_logprob": -0.0905397760457006, "compression_ratio": 1.8604651162790697, "no_speech_prob": 7.411053957184777e-06}, {"id": 614, "seek": 243870, "start": 2454.3599999999997, "end": 2459.8399999999997, "text": " And then we're going to convert all those strings into ints using the vocab.", "tokens": [400, 550, 321, 434, 516, 281, 7620, 439, 729, 13985, 666, 560, 82, 1228, 264, 2329, 455, 13], "temperature": 0.0, "avg_logprob": -0.0905397760457006, "compression_ratio": 1.8604651162790697, "no_speech_prob": 7.411053957184777e-06}, {"id": 615, "seek": 243870, "start": 2459.8399999999997, "end": 2463.2, "text": " And then we're going to do the same thing for the validation set, but we'll use the", "tokens": [400, 550, 321, 434, 516, 281, 360, 264, 912, 551, 337, 264, 24071, 992, 11, 457, 321, 603, 764, 264], "temperature": 0.0, "avg_logprob": -0.0905397760457006, "compression_ratio": 1.8604651162790697, "no_speech_prob": 7.411053957184777e-06}, {"id": 616, "seek": 243870, "start": 2463.2, "end": 2466.0, "text": " training set's vocab.", "tokens": [3097, 992, 311, 2329, 455, 13], "temperature": 0.0, "avg_logprob": -0.0905397760457006, "compression_ratio": 1.8604651162790697, "no_speech_prob": 7.411053957184777e-06}, {"id": 617, "seek": 246600, "start": 2466.0, "end": 2472.2, "text": " So that's an example of a processor that converts label strings to numbers in a consistent and", "tokens": [407, 300, 311, 364, 1365, 295, 257, 15321, 300, 38874, 7645, 13985, 281, 3547, 294, 257, 8398, 293], "temperature": 0.0, "avg_logprob": -0.10213135198219535, "compression_ratio": 1.7873303167420815, "no_speech_prob": 3.0894268547854153e-06}, {"id": 618, "seek": 246600, "start": 2472.2, "end": 2474.48, "text": " reproducible ways.", "tokens": [11408, 32128, 2098, 13], "temperature": 0.0, "avg_logprob": -0.10213135198219535, "compression_ratio": 1.7873303167420815, "no_speech_prob": 3.0894268547854153e-06}, {"id": 619, "seek": 246600, "start": 2474.48, "end": 2480.0, "text": " Other things we could do would be processing texts to tokenize them and then numericalize", "tokens": [5358, 721, 321, 727, 360, 576, 312, 9007, 15765, 281, 14862, 1125, 552, 293, 550, 29054, 1125], "temperature": 0.0, "avg_logprob": -0.10213135198219535, "compression_ratio": 1.7873303167420815, "no_speech_prob": 3.0894268547854153e-06}, {"id": 620, "seek": 246600, "start": 2480.0, "end": 2481.0, "text": " them.", "tokens": [552, 13], "temperature": 0.0, "avg_logprob": -0.10213135198219535, "compression_ratio": 1.7873303167420815, "no_speech_prob": 3.0894268547854153e-06}, {"id": 621, "seek": 246600, "start": 2481.0, "end": 2484.56, "text": " Numericalizing them is a lot like converting the label strings to numbers.", "tokens": [426, 15583, 804, 3319, 552, 307, 257, 688, 411, 29942, 264, 7645, 13985, 281, 3547, 13], "temperature": 0.0, "avg_logprob": -0.10213135198219535, "compression_ratio": 1.7873303167420815, "no_speech_prob": 3.0894268547854153e-06}, {"id": 622, "seek": 246600, "start": 2484.56, "end": 2489.72, "text": " Or taking tabular data and filling the missing values with a median computed on the training", "tokens": [1610, 1940, 4421, 1040, 1412, 293, 10623, 264, 5361, 4190, 365, 257, 26779, 40610, 322, 264, 3097], "temperature": 0.0, "avg_logprob": -0.10213135198219535, "compression_ratio": 1.7873303167420815, "no_speech_prob": 3.0894268547854153e-06}, {"id": 623, "seek": 246600, "start": 2489.72, "end": 2490.72, "text": " set.", "tokens": [992, 13], "temperature": 0.0, "avg_logprob": -0.10213135198219535, "compression_ratio": 1.7873303167420815, "no_speech_prob": 3.0894268547854153e-06}, {"id": 624, "seek": 246600, "start": 2490.72, "end": 2491.72, "text": " Or whatever.", "tokens": [1610, 2035, 13], "temperature": 0.0, "avg_logprob": -0.10213135198219535, "compression_ratio": 1.7873303167420815, "no_speech_prob": 3.0894268547854153e-06}, {"id": 625, "seek": 249172, "start": 2491.72, "end": 2500.72, "text": " So most things we do in this labeling process is going to require some kind of processor.", "tokens": [407, 881, 721, 321, 360, 294, 341, 40244, 1399, 307, 516, 281, 3651, 512, 733, 295, 15321, 13], "temperature": 0.0, "avg_logprob": -0.10341817140579224, "compression_ratio": 1.8431372549019607, "no_speech_prob": 4.029366664326517e-06}, {"id": 626, "seek": 249172, "start": 2500.72, "end": 2505.0, "text": " So in our case we want a processor that can convert label strings to numbers.", "tokens": [407, 294, 527, 1389, 321, 528, 257, 15321, 300, 393, 7620, 7645, 13985, 281, 3547, 13], "temperature": 0.0, "avg_logprob": -0.10341817140579224, "compression_ratio": 1.8431372549019607, "no_speech_prob": 4.029366664326517e-06}, {"id": 627, "seek": 249172, "start": 2505.0, "end": 2508.9199999999996, "text": " So the first thing we need to know is what are all of the possible labels.", "tokens": [407, 264, 700, 551, 321, 643, 281, 458, 307, 437, 366, 439, 295, 264, 1944, 16949, 13], "temperature": 0.0, "avg_logprob": -0.10341817140579224, "compression_ratio": 1.8431372549019607, "no_speech_prob": 4.029366664326517e-06}, {"id": 628, "seek": 249172, "start": 2508.9199999999996, "end": 2513.08, "text": " And so therefore we need to know all the possible unique things in a list.", "tokens": [400, 370, 4412, 321, 643, 281, 458, 439, 264, 1944, 3845, 721, 294, 257, 1329, 13], "temperature": 0.0, "avg_logprob": -0.10341817140579224, "compression_ratio": 1.8431372549019607, "no_speech_prob": 4.029366664326517e-06}, {"id": 629, "seek": 249172, "start": 2513.08, "end": 2514.68, "text": " So here's some list.", "tokens": [407, 510, 311, 512, 1329, 13], "temperature": 0.0, "avg_logprob": -0.10341817140579224, "compression_ratio": 1.8431372549019607, "no_speech_prob": 4.029366664326517e-06}, {"id": 630, "seek": 249172, "start": 2514.68, "end": 2517.04, "text": " Here's something that unicifies them.", "tokens": [1692, 311, 746, 300, 517, 299, 11221, 552, 13], "temperature": 0.0, "avg_logprob": -0.10341817140579224, "compression_ratio": 1.8431372549019607, "no_speech_prob": 4.029366664326517e-06}, {"id": 631, "seek": 251704, "start": 2517.04, "end": 2523.08, "text": " So that's how we can get all the unique values of something.", "tokens": [407, 300, 311, 577, 321, 393, 483, 439, 264, 3845, 4190, 295, 746, 13], "temperature": 0.0, "avg_logprob": -0.07499076805862726, "compression_ratio": 1.8605769230769231, "no_speech_prob": 1.0289303645549808e-05}, {"id": 632, "seek": 251704, "start": 2523.08, "end": 2526.44, "text": " So now that we've got that, we can create a processor.", "tokens": [407, 586, 300, 321, 600, 658, 300, 11, 321, 393, 1884, 257, 15321, 13], "temperature": 0.0, "avg_logprob": -0.07499076805862726, "compression_ratio": 1.8605769230769231, "no_speech_prob": 1.0289303645549808e-05}, {"id": 633, "seek": 251704, "start": 2526.44, "end": 2531.2599999999998, "text": " And a processor is just something that can process some items.", "tokens": [400, 257, 15321, 307, 445, 746, 300, 393, 1399, 512, 4754, 13], "temperature": 0.0, "avg_logprob": -0.07499076805862726, "compression_ratio": 1.8605769230769231, "no_speech_prob": 1.0289303645549808e-05}, {"id": 634, "seek": 251704, "start": 2531.2599999999998, "end": 2534.2799999999997, "text": " And so let's create a category processor.", "tokens": [400, 370, 718, 311, 1884, 257, 7719, 15321, 13], "temperature": 0.0, "avg_logprob": -0.07499076805862726, "compression_ratio": 1.8605769230769231, "no_speech_prob": 1.0289303645549808e-05}, {"id": 635, "seek": 251704, "start": 2534.2799999999997, "end": 2539.92, "text": " And this is the thing that's going to create our list of all of the possible categories.", "tokens": [400, 341, 307, 264, 551, 300, 311, 516, 281, 1884, 527, 1329, 295, 439, 295, 264, 1944, 10479, 13], "temperature": 0.0, "avg_logprob": -0.07499076805862726, "compression_ratio": 1.8605769230769231, "no_speech_prob": 1.0289303645549808e-05}, {"id": 636, "seek": 251704, "start": 2539.92, "end": 2545.68, "text": " So basically when you say process, we're going to see if there's a vocab yet.", "tokens": [407, 1936, 562, 291, 584, 1399, 11, 321, 434, 516, 281, 536, 498, 456, 311, 257, 2329, 455, 1939, 13], "temperature": 0.0, "avg_logprob": -0.07499076805862726, "compression_ratio": 1.8605769230769231, "no_speech_prob": 1.0289303645549808e-05}, {"id": 637, "seek": 254568, "start": 2545.68, "end": 2548.7599999999998, "text": " And if there's not, this must be the training set.", "tokens": [400, 498, 456, 311, 406, 11, 341, 1633, 312, 264, 3097, 992, 13], "temperature": 0.0, "avg_logprob": -0.08266635131835938, "compression_ratio": 1.8553191489361702, "no_speech_prob": 4.157300281804055e-06}, {"id": 638, "seek": 254568, "start": 2548.7599999999998, "end": 2550.24, "text": " So we'll create a vocab.", "tokens": [407, 321, 603, 1884, 257, 2329, 455, 13], "temperature": 0.0, "avg_logprob": -0.08266635131835938, "compression_ratio": 1.8553191489361702, "no_speech_prob": 4.157300281804055e-06}, {"id": 639, "seek": 254568, "start": 2550.24, "end": 2554.24, "text": " And it's just the unique values of all the items.", "tokens": [400, 309, 311, 445, 264, 3845, 4190, 295, 439, 264, 4754, 13], "temperature": 0.0, "avg_logprob": -0.08266635131835938, "compression_ratio": 1.8553191489361702, "no_speech_prob": 4.157300281804055e-06}, {"id": 640, "seek": 254568, "start": 2554.24, "end": 2558.3599999999997, "text": " And then we'll create the thing that goes not from int to object, but goes from object", "tokens": [400, 550, 321, 603, 1884, 264, 551, 300, 1709, 406, 490, 560, 281, 2657, 11, 457, 1709, 490, 2657], "temperature": 0.0, "avg_logprob": -0.08266635131835938, "compression_ratio": 1.8553191489361702, "no_speech_prob": 4.157300281804055e-06}, {"id": 641, "seek": 254568, "start": 2558.3599999999997, "end": 2559.3599999999997, "text": " to int.", "tokens": [281, 560, 13], "temperature": 0.0, "avg_logprob": -0.08266635131835938, "compression_ratio": 1.8553191489361702, "no_speech_prob": 4.157300281804055e-06}, {"id": 642, "seek": 254568, "start": 2559.3599999999997, "end": 2560.3599999999997, "text": " So it's the reverse mapping.", "tokens": [407, 309, 311, 264, 9943, 18350, 13], "temperature": 0.0, "avg_logprob": -0.08266635131835938, "compression_ratio": 1.8553191489361702, "no_speech_prob": 4.157300281804055e-06}, {"id": 643, "seek": 254568, "start": 2560.3599999999997, "end": 2566.16, "text": " So we just enumerate the vocabulary and create a dictionary with the reverse mapping.", "tokens": [407, 321, 445, 465, 15583, 473, 264, 19864, 293, 1884, 257, 25890, 365, 264, 9943, 18350, 13], "temperature": 0.0, "avg_logprob": -0.08266635131835938, "compression_ratio": 1.8553191489361702, "no_speech_prob": 4.157300281804055e-06}, {"id": 644, "seek": 254568, "start": 2566.16, "end": 2572.48, "text": " So now that we have a vocab, we can then go through all the items and process one of them", "tokens": [407, 586, 300, 321, 362, 257, 2329, 455, 11, 321, 393, 550, 352, 807, 439, 264, 4754, 293, 1399, 472, 295, 552], "temperature": 0.0, "avg_logprob": -0.08266635131835938, "compression_ratio": 1.8553191489361702, "no_speech_prob": 4.157300281804055e-06}, {"id": 645, "seek": 254568, "start": 2572.48, "end": 2574.24, "text": " at a time.", "tokens": [412, 257, 565, 13], "temperature": 0.0, "avg_logprob": -0.08266635131835938, "compression_ratio": 1.8553191489361702, "no_speech_prob": 4.157300281804055e-06}, {"id": 646, "seek": 257424, "start": 2574.24, "end": 2579.9199999999996, "text": " And process one of them simply means look in that reverse mapping.", "tokens": [400, 1399, 472, 295, 552, 2935, 1355, 574, 294, 300, 9943, 18350, 13], "temperature": 0.0, "avg_logprob": -0.13342844645182292, "compression_ratio": 1.6693877551020408, "no_speech_prob": 5.682326445821673e-06}, {"id": 647, "seek": 257424, "start": 2579.9199999999996, "end": 2583.1, "text": " We could also de-process, which would take a bunch of indexes.", "tokens": [492, 727, 611, 368, 12, 41075, 11, 597, 576, 747, 257, 3840, 295, 8186, 279, 13], "temperature": 0.0, "avg_logprob": -0.13342844645182292, "compression_ratio": 1.6693877551020408, "no_speech_prob": 5.682326445821673e-06}, {"id": 648, "seek": 257424, "start": 2583.1, "end": 2589.12, "text": " We would use this, for example, to print out the inferences that we're doing.", "tokens": [492, 576, 764, 341, 11, 337, 1365, 11, 281, 4482, 484, 264, 13596, 2667, 300, 321, 434, 884, 13], "temperature": 0.0, "avg_logprob": -0.13342844645182292, "compression_ratio": 1.6693877551020408, "no_speech_prob": 5.682326445821673e-06}, {"id": 649, "seek": 257424, "start": 2589.12, "end": 2591.12, "text": " So we better make sure we've got a vocab by now.", "tokens": [407, 321, 1101, 652, 988, 321, 600, 658, 257, 2329, 455, 538, 586, 13], "temperature": 0.0, "avg_logprob": -0.13342844645182292, "compression_ratio": 1.6693877551020408, "no_speech_prob": 5.682326445821673e-06}, {"id": 650, "seek": 257424, "start": 2591.12, "end": 2593.16, "text": " Otherwise we can't do anything.", "tokens": [10328, 321, 393, 380, 360, 1340, 13], "temperature": 0.0, "avg_logprob": -0.13342844645182292, "compression_ratio": 1.6693877551020408, "no_speech_prob": 5.682326445821673e-06}, {"id": 651, "seek": 257424, "start": 2593.16, "end": 2597.2999999999997, "text": " And then we just de-process one for each index.", "tokens": [400, 550, 321, 445, 368, 12, 41075, 472, 337, 1184, 8186, 13], "temperature": 0.0, "avg_logprob": -0.13342844645182292, "compression_ratio": 1.6693877551020408, "no_speech_prob": 5.682326445821673e-06}, {"id": 652, "seek": 257424, "start": 2597.2999999999997, "end": 2601.12, "text": " And de-process one just looks it up in the vocab.", "tokens": [400, 368, 12, 41075, 472, 445, 1542, 309, 493, 294, 264, 2329, 455, 13], "temperature": 0.0, "avg_logprob": -0.13342844645182292, "compression_ratio": 1.6693877551020408, "no_speech_prob": 5.682326445821673e-06}, {"id": 653, "seek": 257424, "start": 2601.12, "end": 2603.8799999999997, "text": " So that's all we need.", "tokens": [407, 300, 311, 439, 321, 643, 13], "temperature": 0.0, "avg_logprob": -0.13342844645182292, "compression_ratio": 1.6693877551020408, "no_speech_prob": 5.682326445821673e-06}, {"id": 654, "seek": 260388, "start": 2603.88, "end": 2606.56, "text": " And so with this, we can now combine it all together.", "tokens": [400, 370, 365, 341, 11, 321, 393, 586, 10432, 309, 439, 1214, 13], "temperature": 0.0, "avg_logprob": -0.08953027510910891, "compression_ratio": 1.71, "no_speech_prob": 1.0129758265975397e-05}, {"id": 655, "seek": 260388, "start": 2606.56, "end": 2609.6400000000003, "text": " And let's create a processed item list.", "tokens": [400, 718, 311, 1884, 257, 18846, 3174, 1329, 13], "temperature": 0.0, "avg_logprob": -0.08953027510910891, "compression_ratio": 1.71, "no_speech_prob": 1.0129758265975397e-05}, {"id": 656, "seek": 260388, "start": 2609.6400000000003, "end": 2614.6800000000003, "text": " And it's just a list container that contains a processor.", "tokens": [400, 309, 311, 445, 257, 1329, 10129, 300, 8306, 257, 15321, 13], "temperature": 0.0, "avg_logprob": -0.08953027510910891, "compression_ratio": 1.71, "no_speech_prob": 1.0129758265975397e-05}, {"id": 657, "seek": 260388, "start": 2614.6800000000003, "end": 2621.28, "text": " And the items in it, whatever we were given after being processed.", "tokens": [400, 264, 4754, 294, 309, 11, 2035, 321, 645, 2212, 934, 885, 18846, 13], "temperature": 0.0, "avg_logprob": -0.08953027510910891, "compression_ratio": 1.71, "no_speech_prob": 1.0129758265975397e-05}, {"id": 658, "seek": 260388, "start": 2621.28, "end": 2626.92, "text": " And so then, as well as being able to index in it to grab those processed items, we'll", "tokens": [400, 370, 550, 11, 382, 731, 382, 885, 1075, 281, 8186, 294, 309, 281, 4444, 729, 18846, 4754, 11, 321, 603], "temperature": 0.0, "avg_logprob": -0.08953027510910891, "compression_ratio": 1.71, "no_speech_prob": 1.0129758265975397e-05}, {"id": 659, "seek": 260388, "start": 2626.92, "end": 2629.2000000000003, "text": " also define something called object.", "tokens": [611, 6964, 746, 1219, 2657, 13], "temperature": 0.0, "avg_logprob": -0.08953027510910891, "compression_ratio": 1.71, "no_speech_prob": 1.0129758265975397e-05}, {"id": 660, "seek": 262920, "start": 2629.2, "end": 2634.7999999999997, "text": " And that's just the thing that's going to de-process the items again.", "tokens": [400, 300, 311, 445, 264, 551, 300, 311, 516, 281, 368, 12, 41075, 264, 4754, 797, 13], "temperature": 0.0, "avg_logprob": -0.09882813440242284, "compression_ratio": 1.64, "no_speech_prob": 5.862491434527328e-06}, {"id": 661, "seek": 262920, "start": 2634.7999999999997, "end": 2641.2799999999997, "text": " So that's all the stuff we need to label things.", "tokens": [407, 300, 311, 439, 264, 1507, 321, 643, 281, 7645, 721, 13], "temperature": 0.0, "avg_logprob": -0.09882813440242284, "compression_ratio": 1.64, "no_speech_prob": 5.862491434527328e-06}, {"id": 662, "seek": 262920, "start": 2641.2799999999997, "end": 2647.52, "text": " So we already know that for splitting, we needed the grandparent.", "tokens": [407, 321, 1217, 458, 300, 337, 30348, 11, 321, 2978, 264, 2697, 38321, 13], "temperature": 0.0, "avg_logprob": -0.09882813440242284, "compression_ratio": 1.64, "no_speech_prob": 5.862491434527328e-06}, {"id": 663, "seek": 262920, "start": 2647.52, "end": 2651.2799999999997, "text": " For labeling, we need the parent.", "tokens": [1171, 40244, 11, 321, 643, 264, 2596, 13], "temperature": 0.0, "avg_logprob": -0.09882813440242284, "compression_ratio": 1.64, "no_speech_prob": 5.862491434527328e-06}, {"id": 664, "seek": 262920, "start": 2651.2799999999997, "end": 2658.3199999999997, "text": " So here's a parent labeler.", "tokens": [407, 510, 311, 257, 2596, 2715, 6185, 13], "temperature": 0.0, "avg_logprob": -0.09882813440242284, "compression_ratio": 1.64, "no_speech_prob": 5.862491434527328e-06}, {"id": 665, "seek": 265832, "start": 2658.32, "end": 2661.96, "text": " And here is something which labels things using a function.", "tokens": [400, 510, 307, 746, 597, 16949, 721, 1228, 257, 2445, 13], "temperature": 0.0, "avg_logprob": -0.09743343310409718, "compression_ratio": 1.7634408602150538, "no_speech_prob": 5.6823391787474975e-06}, {"id": 666, "seek": 265832, "start": 2661.96, "end": 2666.96, "text": " It just calls a function for each thing.", "tokens": [467, 445, 5498, 257, 2445, 337, 1184, 551, 13], "temperature": 0.0, "avg_logprob": -0.09743343310409718, "compression_ratio": 1.7634408602150538, "no_speech_prob": 5.6823391787474975e-06}, {"id": 667, "seek": 265832, "start": 2666.96, "end": 2670.6200000000003, "text": " And so here is our class.", "tokens": [400, 370, 510, 307, 527, 1508, 13], "temperature": 0.0, "avg_logprob": -0.09743343310409718, "compression_ratio": 1.7634408602150538, "no_speech_prob": 5.6823391787474975e-06}, {"id": 668, "seek": 265832, "start": 2670.6200000000003, "end": 2674.6800000000003, "text": " And we're going to have to pass it some independent variable and some dependent variable and store", "tokens": [400, 321, 434, 516, 281, 362, 281, 1320, 309, 512, 6695, 7006, 293, 512, 12334, 7006, 293, 3531], "temperature": 0.0, "avg_logprob": -0.09743343310409718, "compression_ratio": 1.7634408602150538, "no_speech_prob": 5.6823391787474975e-06}, {"id": 669, "seek": 265832, "start": 2674.6800000000003, "end": 2676.98, "text": " them away.", "tokens": [552, 1314, 13], "temperature": 0.0, "avg_logprob": -0.09743343310409718, "compression_ratio": 1.7634408602150538, "no_speech_prob": 5.6823391787474975e-06}, {"id": 670, "seek": 265832, "start": 2676.98, "end": 2683.5, "text": " And then we need a indexer to grab the x and grab the y at those indexes.", "tokens": [400, 550, 321, 643, 257, 8186, 260, 281, 4444, 264, 2031, 293, 4444, 264, 288, 412, 729, 8186, 279, 13], "temperature": 0.0, "avg_logprob": -0.09743343310409718, "compression_ratio": 1.7634408602150538, "no_speech_prob": 5.6823391787474975e-06}, {"id": 671, "seek": 265832, "start": 2683.5, "end": 2685.7000000000003, "text": " We need a length.", "tokens": [492, 643, 257, 4641, 13], "temperature": 0.0, "avg_logprob": -0.09743343310409718, "compression_ratio": 1.7634408602150538, "no_speech_prob": 5.6823391787474975e-06}, {"id": 672, "seek": 268570, "start": 2685.7, "end": 2689.04, "text": " We may as well make it print out nicely.", "tokens": [492, 815, 382, 731, 652, 309, 4482, 484, 9594, 13], "temperature": 0.0, "avg_logprob": -0.11176376342773438, "compression_ratio": 1.574468085106383, "no_speech_prob": 1.1015870313713094e-06}, {"id": 673, "seek": 268570, "start": 2689.04, "end": 2694.9199999999996, "text": " And then we'll just add something just like we did before, which does the labeling and", "tokens": [400, 550, 321, 603, 445, 909, 746, 445, 411, 321, 630, 949, 11, 597, 775, 264, 40244, 293], "temperature": 0.0, "avg_logprob": -0.11176376342773438, "compression_ratio": 1.574468085106383, "no_speech_prob": 1.1015870313713094e-06}, {"id": 674, "seek": 268570, "start": 2694.9199999999996, "end": 2700.96, "text": " passes those to a processed item list to grab the labels.", "tokens": [11335, 729, 281, 257, 18846, 3174, 1329, 281, 4444, 264, 16949, 13], "temperature": 0.0, "avg_logprob": -0.11176376342773438, "compression_ratio": 1.574468085106383, "no_speech_prob": 1.1015870313713094e-06}, {"id": 675, "seek": 268570, "start": 2700.96, "end": 2709.3599999999997, "text": " And then passes the inputs and outputs to our constructor to give us our labeled data.", "tokens": [400, 550, 11335, 264, 15743, 293, 23930, 281, 527, 47479, 281, 976, 505, 527, 21335, 1412, 13], "temperature": 0.0, "avg_logprob": -0.11176376342773438, "compression_ratio": 1.574468085106383, "no_speech_prob": 1.1015870313713094e-06}, {"id": 676, "seek": 268570, "start": 2709.3599999999997, "end": 2711.46, "text": " So that's basically it.", "tokens": [407, 300, 311, 1936, 309, 13], "temperature": 0.0, "avg_logprob": -0.11176376342773438, "compression_ratio": 1.574468085106383, "no_speech_prob": 1.1015870313713094e-06}, {"id": 677, "seek": 271146, "start": 2711.46, "end": 2717.6, "text": " So with that, we have a label by function where we can create our category processor.", "tokens": [407, 365, 300, 11, 321, 362, 257, 7645, 538, 2445, 689, 321, 393, 1884, 527, 7719, 15321, 13], "temperature": 0.0, "avg_logprob": -0.10418031896863665, "compression_ratio": 1.8177777777777777, "no_speech_prob": 5.954928383289371e-06}, {"id": 678, "seek": 271146, "start": 2717.6, "end": 2719.98, "text": " We can label the training set.", "tokens": [492, 393, 7645, 264, 3097, 992, 13], "temperature": 0.0, "avg_logprob": -0.10418031896863665, "compression_ratio": 1.8177777777777777, "no_speech_prob": 5.954928383289371e-06}, {"id": 679, "seek": 271146, "start": 2719.98, "end": 2721.88, "text": " We can label the validation set.", "tokens": [492, 393, 7645, 264, 24071, 992, 13], "temperature": 0.0, "avg_logprob": -0.10418031896863665, "compression_ratio": 1.8177777777777777, "no_speech_prob": 5.954928383289371e-06}, {"id": 680, "seek": 271146, "start": 2721.88, "end": 2725.84, "text": " And we can return the result, the split data result.", "tokens": [400, 321, 393, 2736, 264, 1874, 11, 264, 7472, 1412, 1874, 13], "temperature": 0.0, "avg_logprob": -0.10418031896863665, "compression_ratio": 1.8177777777777777, "no_speech_prob": 5.954928383289371e-06}, {"id": 681, "seek": 271146, "start": 2725.84, "end": 2733.32, "text": " So the main thing to notice here is that when we say train equals labeled data dot label", "tokens": [407, 264, 2135, 551, 281, 3449, 510, 307, 300, 562, 321, 584, 3847, 6915, 21335, 1412, 5893, 7645], "temperature": 0.0, "avg_logprob": -0.10418031896863665, "compression_ratio": 1.8177777777777777, "no_speech_prob": 5.954928383289371e-06}, {"id": 682, "seek": 271146, "start": 2733.32, "end": 2737.56, "text": " passing in this processor, this processor has no vocab.", "tokens": [8437, 294, 341, 15321, 11, 341, 15321, 575, 572, 2329, 455, 13], "temperature": 0.0, "avg_logprob": -0.10418031896863665, "compression_ratio": 1.8177777777777777, "no_speech_prob": 5.954928383289371e-06}, {"id": 683, "seek": 271146, "start": 2737.56, "end": 2739.08, "text": " So it goes to that bit we saw.", "tokens": [407, 309, 1709, 281, 300, 857, 321, 1866, 13], "temperature": 0.0, "avg_logprob": -0.10418031896863665, "compression_ratio": 1.8177777777777777, "no_speech_prob": 5.954928383289371e-06}, {"id": 684, "seek": 271146, "start": 2739.08, "end": 2740.64, "text": " It says, oh, there's no vocab.", "tokens": [467, 1619, 11, 1954, 11, 456, 311, 572, 2329, 455, 13], "temperature": 0.0, "avg_logprob": -0.10418031896863665, "compression_ratio": 1.8177777777777777, "no_speech_prob": 5.954928383289371e-06}, {"id": 685, "seek": 274064, "start": 2740.64, "end": 2744.0, "text": " So let's create a list of all the unique possibilities.", "tokens": [407, 718, 311, 1884, 257, 1329, 295, 439, 264, 3845, 12178, 13], "temperature": 0.0, "avg_logprob": -0.0999820997130196, "compression_ratio": 1.594488188976378, "no_speech_prob": 3.5008229133381974e-06}, {"id": 686, "seek": 274064, "start": 2744.0, "end": 2749.4, "text": " On the other hand, when it goes to the validation set, proc now does have a vocab.", "tokens": [1282, 264, 661, 1011, 11, 562, 309, 1709, 281, 264, 24071, 992, 11, 9510, 586, 775, 362, 257, 2329, 455, 13], "temperature": 0.0, "avg_logprob": -0.0999820997130196, "compression_ratio": 1.594488188976378, "no_speech_prob": 3.5008229133381974e-06}, {"id": 687, "seek": 274064, "start": 2749.4, "end": 2754.06, "text": " So it will skip that step and use the training set's vocab.", "tokens": [407, 309, 486, 10023, 300, 1823, 293, 764, 264, 3097, 992, 311, 2329, 455, 13], "temperature": 0.0, "avg_logprob": -0.0999820997130196, "compression_ratio": 1.594488188976378, "no_speech_prob": 3.5008229133381974e-06}, {"id": 688, "seek": 274064, "start": 2754.06, "end": 2755.7599999999998, "text": " So this is really important, right?", "tokens": [407, 341, 307, 534, 1021, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.0999820997130196, "compression_ratio": 1.594488188976378, "no_speech_prob": 3.5008229133381974e-06}, {"id": 689, "seek": 274064, "start": 2755.7599999999998, "end": 2760.04, "text": " People get mixed up by this all the time in machine learning and deep learning.", "tokens": [3432, 483, 7467, 493, 538, 341, 439, 264, 565, 294, 3479, 2539, 293, 2452, 2539, 13], "temperature": 0.0, "avg_logprob": -0.0999820997130196, "compression_ratio": 1.594488188976378, "no_speech_prob": 3.5008229133381974e-06}, {"id": 690, "seek": 274064, "start": 2760.04, "end": 2766.0, "text": " It's like very often when somebody says, my model's no better than random, the most common", "tokens": [467, 311, 411, 588, 2049, 562, 2618, 1619, 11, 452, 2316, 311, 572, 1101, 813, 4974, 11, 264, 881, 2689], "temperature": 0.0, "avg_logprob": -0.0999820997130196, "compression_ratio": 1.594488188976378, "no_speech_prob": 3.5008229133381974e-06}, {"id": 691, "seek": 276600, "start": 2766.0, "end": 2771.28, "text": " reason is that they're using some kind of different mapping between their training set", "tokens": [1778, 307, 300, 436, 434, 1228, 512, 733, 295, 819, 18350, 1296, 641, 3097, 992], "temperature": 0.0, "avg_logprob": -0.11814223544698366, "compression_ratio": 1.5925925925925926, "no_speech_prob": 2.3686998247285374e-06}, {"id": 692, "seek": 276600, "start": 2771.28, "end": 2773.36, "text": " and their validation set.", "tokens": [293, 641, 24071, 992, 13], "temperature": 0.0, "avg_logprob": -0.11814223544698366, "compression_ratio": 1.5925925925925926, "no_speech_prob": 2.3686998247285374e-06}, {"id": 693, "seek": 276600, "start": 2773.36, "end": 2777.92, "text": " So if you use a process like this, that's never going to happen because you're ensuring", "tokens": [407, 498, 291, 764, 257, 1399, 411, 341, 11, 300, 311, 1128, 516, 281, 1051, 570, 291, 434, 16882], "temperature": 0.0, "avg_logprob": -0.11814223544698366, "compression_ratio": 1.5925925925925926, "no_speech_prob": 2.3686998247285374e-06}, {"id": 694, "seek": 276600, "start": 2777.92, "end": 2784.04, "text": " that you're always using the same mapping.", "tokens": [300, 291, 434, 1009, 1228, 264, 912, 18350, 13], "temperature": 0.0, "avg_logprob": -0.11814223544698366, "compression_ratio": 1.5925925925925926, "no_speech_prob": 2.3686998247285374e-06}, {"id": 695, "seek": 276600, "start": 2784.04, "end": 2788.84, "text": " So the details of the code aren't particularly important.", "tokens": [407, 264, 4365, 295, 264, 3089, 3212, 380, 4098, 1021, 13], "temperature": 0.0, "avg_logprob": -0.11814223544698366, "compression_ratio": 1.5925925925925926, "no_speech_prob": 2.3686998247285374e-06}, {"id": 696, "seek": 278884, "start": 2788.84, "end": 2798.4, "text": " The important idea is that your labeling process needs to include some kind of processor idea.", "tokens": [440, 1021, 1558, 307, 300, 428, 40244, 1399, 2203, 281, 4090, 512, 733, 295, 15321, 1558, 13], "temperature": 0.0, "avg_logprob": -0.07045948656299446, "compression_ratio": 1.6079295154185023, "no_speech_prob": 4.156980594416382e-06}, {"id": 697, "seek": 278884, "start": 2798.4, "end": 2804.6400000000003, "text": " And if you're doing this stuff manually, which basically every other machine learning and", "tokens": [400, 498, 291, 434, 884, 341, 1507, 16945, 11, 597, 1936, 633, 661, 3479, 2539, 293], "temperature": 0.0, "avg_logprob": -0.07045948656299446, "compression_ratio": 1.6079295154185023, "no_speech_prob": 4.156980594416382e-06}, {"id": 698, "seek": 278884, "start": 2804.6400000000003, "end": 2811.48, "text": " deep learning framework does, you're asking for difficult to fix bugs because any time", "tokens": [2452, 2539, 8388, 775, 11, 291, 434, 3365, 337, 2252, 281, 3191, 15120, 570, 604, 565], "temperature": 0.0, "avg_logprob": -0.07045948656299446, "compression_ratio": 1.6079295154185023, "no_speech_prob": 4.156980594416382e-06}, {"id": 699, "seek": 278884, "start": 2811.48, "end": 2815.44, "text": " your computer's not doing something for you, it means you have to remember to do it yourself.", "tokens": [428, 3820, 311, 406, 884, 746, 337, 291, 11, 309, 1355, 291, 362, 281, 1604, 281, 360, 309, 1803, 13], "temperature": 0.0, "avg_logprob": -0.07045948656299446, "compression_ratio": 1.6079295154185023, "no_speech_prob": 4.156980594416382e-06}, {"id": 700, "seek": 281544, "start": 2815.44, "end": 2822.64, "text": " So whatever framework you're using, I don't know if any other frameworks have something", "tokens": [407, 2035, 8388, 291, 434, 1228, 11, 286, 500, 380, 458, 498, 604, 661, 29834, 362, 746], "temperature": 0.0, "avg_logprob": -0.17040334164517598, "compression_ratio": 1.7236842105263157, "no_speech_prob": 1.1299306606815662e-05}, {"id": 701, "seek": 281544, "start": 2822.64, "end": 2823.64, "text": " quite like this.", "tokens": [1596, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.17040334164517598, "compression_ratio": 1.7236842105263157, "no_speech_prob": 1.1299306606815662e-05}, {"id": 702, "seek": 281544, "start": 2823.64, "end": 2828.84, "text": " So create something like this for yourself so that you don't have that problem.", "tokens": [407, 1884, 746, 411, 341, 337, 1803, 370, 300, 291, 500, 380, 362, 300, 1154, 13], "temperature": 0.0, "avg_logprob": -0.17040334164517598, "compression_ratio": 1.7236842105263157, "no_speech_prob": 1.1299306606815662e-05}, {"id": 703, "seek": 281544, "start": 2828.84, "end": 2834.32, "text": " All right, let's go.", "tokens": [1057, 558, 11, 718, 311, 352, 13], "temperature": 0.0, "avg_logprob": -0.17040334164517598, "compression_ratio": 1.7236842105263157, "no_speech_prob": 1.1299306606815662e-05}, {"id": 704, "seek": 281544, "start": 2834.32, "end": 2838.52, "text": " In the case of online streaming data, how do you deal with having new categories in", "tokens": [682, 264, 1389, 295, 2950, 11791, 1412, 11, 577, 360, 291, 2028, 365, 1419, 777, 10479, 294], "temperature": 0.0, "avg_logprob": -0.17040334164517598, "compression_ratio": 1.7236842105263157, "no_speech_prob": 1.1299306606815662e-05}, {"id": 705, "seek": 281544, "start": 2838.52, "end": 2841.44, "text": " the test set that you don't see in training?", "tokens": [264, 1500, 992, 300, 291, 500, 380, 536, 294, 3097, 30], "temperature": 0.0, "avg_logprob": -0.17040334164517598, "compression_ratio": 1.7236842105263157, "no_speech_prob": 1.1299306606815662e-05}, {"id": 706, "seek": 281544, "start": 2841.44, "end": 2843.04, "text": " Yeah, great question.", "tokens": [865, 11, 869, 1168, 13], "temperature": 0.0, "avg_logprob": -0.17040334164517598, "compression_ratio": 1.7236842105263157, "no_speech_prob": 1.1299306606815662e-05}, {"id": 707, "seek": 281544, "start": 2843.04, "end": 2845.12, "text": " It's not just online streaming data.", "tokens": [467, 311, 406, 445, 2950, 11791, 1412, 13], "temperature": 0.0, "avg_logprob": -0.17040334164517598, "compression_ratio": 1.7236842105263157, "no_speech_prob": 1.1299306606815662e-05}, {"id": 708, "seek": 284512, "start": 2845.12, "end": 2847.0, "text": " It happens all the time.", "tokens": [467, 2314, 439, 264, 565, 13], "temperature": 0.0, "avg_logprob": -0.18356389637234843, "compression_ratio": 1.6649484536082475, "no_speech_prob": 1.568703919474501e-05}, {"id": 709, "seek": 284512, "start": 2847.0, "end": 2855.52, "text": " You do inference either on your validation set or test set or in production where you", "tokens": [509, 360, 38253, 2139, 322, 428, 24071, 992, 420, 1500, 992, 420, 294, 4265, 689, 291], "temperature": 0.0, "avg_logprob": -0.18356389637234843, "compression_ratio": 1.6649484536082475, "no_speech_prob": 1.568703919474501e-05}, {"id": 710, "seek": 284512, "start": 2855.52, "end": 2859.4, "text": " see something you haven't seen before.", "tokens": [536, 746, 291, 2378, 380, 1612, 949, 13], "temperature": 0.0, "avg_logprob": -0.18356389637234843, "compression_ratio": 1.6649484536082475, "no_speech_prob": 1.568703919474501e-05}, {"id": 711, "seek": 284512, "start": 2859.4, "end": 2864.44, "text": " For labels, it's less of a problem in inference because for inference, you don't have labels", "tokens": [1171, 16949, 11, 309, 311, 1570, 295, 257, 1154, 294, 38253, 570, 337, 38253, 11, 291, 500, 380, 362, 16949], "temperature": 0.0, "avg_logprob": -0.18356389637234843, "compression_ratio": 1.6649484536082475, "no_speech_prob": 1.568703919474501e-05}, {"id": 712, "seek": 284512, "start": 2864.44, "end": 2866.64, "text": " by definition.", "tokens": [538, 7123, 13], "temperature": 0.0, "avg_logprob": -0.18356389637234843, "compression_ratio": 1.6649484536082475, "no_speech_prob": 1.568703919474501e-05}, {"id": 713, "seek": 284512, "start": 2866.64, "end": 2870.4, "text": " But you could certainly have that problem in your validation set.", "tokens": [583, 291, 727, 3297, 362, 300, 1154, 294, 428, 24071, 992, 13], "temperature": 0.0, "avg_logprob": -0.18356389637234843, "compression_ratio": 1.6649484536082475, "no_speech_prob": 1.568703919474501e-05}, {"id": 714, "seek": 287040, "start": 2870.4, "end": 2876.64, "text": " So what I tend to like to do is if I have something where there's lots and lots of categories", "tokens": [407, 437, 286, 3928, 281, 411, 281, 360, 307, 498, 286, 362, 746, 689, 456, 311, 3195, 293, 3195, 295, 10479], "temperature": 0.0, "avg_logprob": -0.13492127701088233, "compression_ratio": 1.7551020408163265, "no_speech_prob": 5.014097041566856e-06}, {"id": 715, "seek": 287040, "start": 2876.64, "end": 2881.88, "text": " and some of them don't occur very often and I know that in the future there might be new", "tokens": [293, 512, 295, 552, 500, 380, 5160, 588, 2049, 293, 286, 458, 300, 294, 264, 2027, 456, 1062, 312, 777], "temperature": 0.0, "avg_logprob": -0.13492127701088233, "compression_ratio": 1.7551020408163265, "no_speech_prob": 5.014097041566856e-06}, {"id": 716, "seek": 287040, "start": 2881.88, "end": 2887.6800000000003, "text": " categories appearing, I'll take the few least common and I'll group them together into a", "tokens": [10479, 19870, 11, 286, 603, 747, 264, 1326, 1935, 2689, 293, 286, 603, 1594, 552, 1214, 666, 257], "temperature": 0.0, "avg_logprob": -0.13492127701088233, "compression_ratio": 1.7551020408163265, "no_speech_prob": 5.014097041566856e-06}, {"id": 717, "seek": 287040, "start": 2887.6800000000003, "end": 2890.7200000000003, "text": " group called other.", "tokens": [1594, 1219, 661, 13], "temperature": 0.0, "avg_logprob": -0.13492127701088233, "compression_ratio": 1.7551020408163265, "no_speech_prob": 5.014097041566856e-06}, {"id": 718, "seek": 287040, "start": 2890.7200000000003, "end": 2895.1600000000003, "text": " And that way I now have some way to ensure that my model can handle these rare other", "tokens": [400, 300, 636, 286, 586, 362, 512, 636, 281, 5586, 300, 452, 2316, 393, 4813, 613, 5892, 661], "temperature": 0.0, "avg_logprob": -0.13492127701088233, "compression_ratio": 1.7551020408163265, "no_speech_prob": 5.014097041566856e-06}, {"id": 719, "seek": 287040, "start": 2895.1600000000003, "end": 2897.1, "text": " cases.", "tokens": [3331, 13], "temperature": 0.0, "avg_logprob": -0.13492127701088233, "compression_ratio": 1.7551020408163265, "no_speech_prob": 5.014097041566856e-06}, {"id": 720, "seek": 287040, "start": 2897.1, "end": 2900.32, "text": " Something like that tends to work pretty well.", "tokens": [6595, 411, 300, 12258, 281, 589, 1238, 731, 13], "temperature": 0.0, "avg_logprob": -0.13492127701088233, "compression_ratio": 1.7551020408163265, "no_speech_prob": 5.014097041566856e-06}, {"id": 721, "seek": 290032, "start": 2900.32, "end": 2903.6000000000004, "text": " But you do have to think of it ahead of time.", "tokens": [583, 291, 360, 362, 281, 519, 295, 309, 2286, 295, 565, 13], "temperature": 0.0, "avg_logprob": -0.14385643237974585, "compression_ratio": 1.592783505154639, "no_speech_prob": 8.267060366051737e-06}, {"id": 722, "seek": 290032, "start": 2903.6000000000004, "end": 2909.2400000000002, "text": " For many kinds of problems, you know that there's a fixed set of possibilities.", "tokens": [1171, 867, 3685, 295, 2740, 11, 291, 458, 300, 456, 311, 257, 6806, 992, 295, 12178, 13], "temperature": 0.0, "avg_logprob": -0.14385643237974585, "compression_ratio": 1.592783505154639, "no_speech_prob": 8.267060366051737e-06}, {"id": 723, "seek": 290032, "start": 2909.2400000000002, "end": 2914.44, "text": " And if you know that it's not a fixed set, yeah, I would generally try to create an other", "tokens": [400, 498, 291, 458, 300, 309, 311, 406, 257, 6806, 992, 11, 1338, 11, 286, 576, 5101, 853, 281, 1884, 364, 661], "temperature": 0.0, "avg_logprob": -0.14385643237974585, "compression_ratio": 1.592783505154639, "no_speech_prob": 8.267060366051737e-06}, {"id": 724, "seek": 290032, "start": 2914.44, "end": 2916.36, "text": " category with a few examples.", "tokens": [7719, 365, 257, 1326, 5110, 13], "temperature": 0.0, "avg_logprob": -0.14385643237974585, "compression_ratio": 1.592783505154639, "no_speech_prob": 8.267060366051737e-06}, {"id": 725, "seek": 290032, "start": 2916.36, "end": 2924.28, "text": " So make sure you train with some things in that other category.", "tokens": [407, 652, 988, 291, 3847, 365, 512, 721, 294, 300, 661, 7719, 13], "temperature": 0.0, "avg_logprob": -0.14385643237974585, "compression_ratio": 1.592783505154639, "no_speech_prob": 8.267060366051737e-06}, {"id": 726, "seek": 292428, "start": 2924.28, "end": 2930.5600000000004, "text": " In the label data class, what is the class method decorator doing?", "tokens": [682, 264, 7645, 1412, 1508, 11, 437, 307, 264, 1508, 3170, 7919, 1639, 884, 30], "temperature": 0.0, "avg_logprob": -0.13017349028855227, "compression_ratio": 1.6482412060301508, "no_speech_prob": 3.647155608632602e-05}, {"id": 727, "seek": 292428, "start": 2930.5600000000004, "end": 2933.5600000000004, "text": " Sure.", "tokens": [4894, 13], "temperature": 0.0, "avg_logprob": -0.13017349028855227, "compression_ratio": 1.6482412060301508, "no_speech_prob": 3.647155608632602e-05}, {"id": 728, "seek": 292428, "start": 2933.5600000000004, "end": 2937.48, "text": " So I'll be quick because you can Google it, but basically this is the difference between", "tokens": [407, 286, 603, 312, 1702, 570, 291, 393, 3329, 309, 11, 457, 1936, 341, 307, 264, 2649, 1296], "temperature": 0.0, "avg_logprob": -0.13017349028855227, "compression_ratio": 1.6482412060301508, "no_speech_prob": 3.647155608632602e-05}, {"id": 729, "seek": 292428, "start": 2937.48, "end": 2939.78, "text": " an instance method and a class method.", "tokens": [364, 5197, 3170, 293, 257, 1508, 3170, 13], "temperature": 0.0, "avg_logprob": -0.13017349028855227, "compression_ratio": 1.6482412060301508, "no_speech_prob": 3.647155608632602e-05}, {"id": 730, "seek": 292428, "start": 2939.78, "end": 2942.92, "text": " So you'll see it's not getting passed self.", "tokens": [407, 291, 603, 536, 309, 311, 406, 1242, 4678, 2698, 13], "temperature": 0.0, "avg_logprob": -0.13017349028855227, "compression_ratio": 1.6482412060301508, "no_speech_prob": 3.647155608632602e-05}, {"id": 731, "seek": 292428, "start": 2942.92, "end": 2952.92, "text": " So you'll see that I am not going to call this on an object of type label data, but", "tokens": [407, 291, 603, 536, 300, 286, 669, 406, 516, 281, 818, 341, 322, 364, 2657, 295, 2010, 7645, 1412, 11, 457], "temperature": 0.0, "avg_logprob": -0.13017349028855227, "compression_ratio": 1.6482412060301508, "no_speech_prob": 3.647155608632602e-05}, {"id": 732, "seek": 295292, "start": 2952.92, "end": 2957.08, "text": " I'm calling it on the label data class itself.", "tokens": [286, 478, 5141, 309, 322, 264, 7645, 1412, 1508, 2564, 13], "temperature": 0.0, "avg_logprob": -0.1550495647689671, "compression_ratio": 1.6861924686192469, "no_speech_prob": 9.515450983599294e-06}, {"id": 733, "seek": 295292, "start": 2957.08, "end": 2959.96, "text": " So it's just a convenience really, class methods.", "tokens": [407, 309, 311, 445, 257, 19283, 534, 11, 1508, 7150, 13], "temperature": 0.0, "avg_logprob": -0.1550495647689671, "compression_ratio": 1.6861924686192469, "no_speech_prob": 9.515450983599294e-06}, {"id": 734, "seek": 295292, "start": 2959.96, "end": 2964.04, "text": " The thing that they get passed in is the actual class that was requested.", "tokens": [440, 551, 300, 436, 483, 4678, 294, 307, 264, 3539, 1508, 300, 390, 16436, 13], "temperature": 0.0, "avg_logprob": -0.1550495647689671, "compression_ratio": 1.6861924686192469, "no_speech_prob": 9.515450983599294e-06}, {"id": 735, "seek": 295292, "start": 2964.04, "end": 2968.7200000000003, "text": " So I could create a subclass of this and then ask for that subclass.", "tokens": [407, 286, 727, 1884, 257, 1422, 11665, 295, 341, 293, 550, 1029, 337, 300, 1422, 11665, 13], "temperature": 0.0, "avg_logprob": -0.1550495647689671, "compression_ratio": 1.6861924686192469, "no_speech_prob": 9.515450983599294e-06}, {"id": 736, "seek": 295292, "start": 2968.7200000000003, "end": 2971.0, "text": " So anyway, they're called class methods.", "tokens": [407, 4033, 11, 436, 434, 1219, 1508, 7150, 13], "temperature": 0.0, "avg_logprob": -0.1550495647689671, "compression_ratio": 1.6861924686192469, "no_speech_prob": 9.515450983599294e-06}, {"id": 737, "seek": 295292, "start": 2971.0, "end": 2972.2400000000002, "text": " You should Google them.", "tokens": [509, 820, 3329, 552, 13], "temperature": 0.0, "avg_logprob": -0.1550495647689671, "compression_ratio": 1.6861924686192469, "no_speech_prob": 9.515450983599294e-06}, {"id": 738, "seek": 295292, "start": 2972.2400000000002, "end": 2976.7200000000003, "text": " Pretty much every language supports class methods or something like it.", "tokens": [10693, 709, 633, 2856, 9346, 1508, 7150, 420, 746, 411, 309, 13], "temperature": 0.0, "avg_logprob": -0.1550495647689671, "compression_ratio": 1.6861924686192469, "no_speech_prob": 9.515450983599294e-06}, {"id": 739, "seek": 295292, "start": 2976.7200000000003, "end": 2978.56, "text": " They're pretty convenient.", "tokens": [814, 434, 1238, 10851, 13], "temperature": 0.0, "avg_logprob": -0.1550495647689671, "compression_ratio": 1.6861924686192469, "no_speech_prob": 9.515450983599294e-06}, {"id": 740, "seek": 297856, "start": 2978.56, "end": 2983.7599999999998, "text": " You can get away without them, but they're pretty convenient.", "tokens": [509, 393, 483, 1314, 1553, 552, 11, 457, 436, 434, 1238, 10851, 13], "temperature": 0.0, "avg_logprob": -0.17193636440095447, "compression_ratio": 1.4702970297029703, "no_speech_prob": 6.143880455056205e-06}, {"id": 741, "seek": 297856, "start": 2983.7599999999998, "end": 2986.12, "text": " Great.", "tokens": [3769, 13], "temperature": 0.0, "avg_logprob": -0.17193636440095447, "compression_ratio": 1.4702970297029703, "no_speech_prob": 6.143880455056205e-06}, {"id": 742, "seek": 297856, "start": 2986.12, "end": 2989.6, "text": " So now we've got our labeled list.", "tokens": [407, 586, 321, 600, 658, 527, 21335, 1329, 13], "temperature": 0.0, "avg_logprob": -0.17193636440095447, "compression_ratio": 1.4702970297029703, "no_speech_prob": 6.143880455056205e-06}, {"id": 743, "seek": 297856, "start": 2989.6, "end": 2994.56, "text": " And if we print it out, it's got a training set and a validation set, and each one has", "tokens": [400, 498, 321, 4482, 309, 484, 11, 309, 311, 658, 257, 3097, 992, 293, 257, 24071, 992, 11, 293, 1184, 472, 575], "temperature": 0.0, "avg_logprob": -0.17193636440095447, "compression_ratio": 1.4702970297029703, "no_speech_prob": 6.143880455056205e-06}, {"id": 744, "seek": 297856, "start": 2994.56, "end": 2998.18, "text": " an X and a Y.", "tokens": [364, 1783, 293, 257, 398, 13], "temperature": 0.0, "avg_logprob": -0.17193636440095447, "compression_ratio": 1.4702970297029703, "no_speech_prob": 6.143880455056205e-06}, {"id": 745, "seek": 297856, "start": 2998.18, "end": 3004.04, "text": " Our category items are a little less convenient than the Fast AI version 1 ones, because the", "tokens": [2621, 7719, 4754, 366, 257, 707, 1570, 10851, 813, 264, 15968, 7318, 3037, 502, 2306, 11, 570, 264], "temperature": 0.0, "avg_logprob": -0.17193636440095447, "compression_ratio": 1.4702970297029703, "no_speech_prob": 6.143880455056205e-06}, {"id": 746, "seek": 300404, "start": 3004.04, "end": 3008.64, "text": " Fast AI ones will actually print out the name of each category.", "tokens": [15968, 7318, 2306, 486, 767, 4482, 484, 264, 1315, 295, 1184, 7719, 13], "temperature": 0.0, "avg_logprob": -0.15177322836483226, "compression_ratio": 1.6571428571428573, "no_speech_prob": 1.1297723176539876e-05}, {"id": 747, "seek": 300404, "start": 3008.64, "end": 3010.22, "text": " We haven't done anything to make that happen.", "tokens": [492, 2378, 380, 1096, 1340, 281, 652, 300, 1051, 13], "temperature": 0.0, "avg_logprob": -0.15177322836483226, "compression_ratio": 1.6571428571428573, "no_speech_prob": 1.1297723176539876e-05}, {"id": 748, "seek": 300404, "start": 3010.22, "end": 3016.92, "text": " So if we want the name of each category, we would actually have to refer to the.obsh,", "tokens": [407, 498, 321, 528, 264, 1315, 295, 1184, 7719, 11, 321, 576, 767, 362, 281, 2864, 281, 264, 2411, 16537, 71, 11], "temperature": 0.0, "avg_logprob": -0.15177322836483226, "compression_ratio": 1.6571428571428573, "no_speech_prob": 1.1297723176539876e-05}, {"id": 749, "seek": 300404, "start": 3016.92, "end": 3018.44, "text": " which you can see we're doing here.", "tokens": [597, 291, 393, 536, 321, 434, 884, 510, 13], "temperature": 0.0, "avg_logprob": -0.15177322836483226, "compression_ratio": 1.6571428571428573, "no_speech_prob": 1.1297723176539876e-05}, {"id": 750, "seek": 300404, "start": 3018.44, "end": 3024.2799999999997, "text": " Y.obsh or Y.obsh with a slice.", "tokens": [398, 13, 16537, 71, 420, 398, 13, 16537, 71, 365, 257, 13153, 13], "temperature": 0.0, "avg_logprob": -0.15177322836483226, "compression_ratio": 1.6571428571428573, "no_speech_prob": 1.1297723176539876e-05}, {"id": 751, "seek": 300404, "start": 3024.2799999999997, "end": 3030.64, "text": " So in Fast AI version 1, there's one extra thing we have, which is this concept of an", "tokens": [407, 294, 15968, 7318, 3037, 502, 11, 456, 311, 472, 2857, 551, 321, 362, 11, 597, 307, 341, 3410, 295, 364], "temperature": 0.0, "avg_logprob": -0.15177322836483226, "compression_ratio": 1.6571428571428573, "no_speech_prob": 1.1297723176539876e-05}, {"id": 752, "seek": 303064, "start": 3030.64, "end": 3035.0, "text": " item base, and you can actually define things like category items that know how to print", "tokens": [3174, 3096, 11, 293, 291, 393, 767, 6964, 721, 411, 7719, 4754, 300, 458, 577, 281, 4482], "temperature": 0.0, "avg_logprob": -0.09990835189819336, "compression_ratio": 1.5219512195121951, "no_speech_prob": 4.157048806519015e-06}, {"id": 753, "seek": 303064, "start": 3035.0, "end": 3037.44, "text": " themselves out.", "tokens": [2969, 484, 13], "temperature": 0.0, "avg_logprob": -0.09990835189819336, "compression_ratio": 1.5219512195121951, "no_speech_prob": 4.157048806519015e-06}, {"id": 754, "seek": 303064, "start": 3037.44, "end": 3043.4, "text": " Whether that convenience is worth the extra complexity is up to you if you're designing", "tokens": [8503, 300, 19283, 307, 3163, 264, 2857, 14024, 307, 493, 281, 291, 498, 291, 434, 14685], "temperature": 0.0, "avg_logprob": -0.09990835189819336, "compression_ratio": 1.5219512195121951, "no_speech_prob": 4.157048806519015e-06}, {"id": 755, "seek": 303064, "start": 3043.4, "end": 3046.9, "text": " something similar yourself.", "tokens": [746, 2531, 1803, 13], "temperature": 0.0, "avg_logprob": -0.09990835189819336, "compression_ratio": 1.5219512195121951, "no_speech_prob": 4.157048806519015e-06}, {"id": 756, "seek": 303064, "start": 3046.9, "end": 3051.8399999999997, "text": " So we still can't train a model with these, because we have pillow objects.", "tokens": [407, 321, 920, 393, 380, 3847, 257, 2316, 365, 613, 11, 570, 321, 362, 18581, 6565, 13], "temperature": 0.0, "avg_logprob": -0.09990835189819336, "compression_ratio": 1.5219512195121951, "no_speech_prob": 4.157048806519015e-06}, {"id": 757, "seek": 303064, "start": 3051.8399999999997, "end": 3054.8399999999997, "text": " We need tenses.", "tokens": [492, 643, 256, 9085, 13], "temperature": 0.0, "avg_logprob": -0.09990835189819336, "compression_ratio": 1.5219512195121951, "no_speech_prob": 4.157048806519015e-06}, {"id": 758, "seek": 305484, "start": 3054.84, "end": 3065.26, "text": " So here's our labeled list, training set, zeroth object, and that has an X and a Y,", "tokens": [407, 510, 311, 527, 21335, 1329, 11, 3097, 992, 11, 44746, 900, 2657, 11, 293, 300, 575, 364, 1783, 293, 257, 398, 11], "temperature": 0.0, "avg_logprob": -0.143912850594034, "compression_ratio": 1.5685279187817258, "no_speech_prob": 2.190755139963585e-06}, {"id": 759, "seek": 305484, "start": 3065.26, "end": 3069.6200000000003, "text": " so the zeroth thing in that tuple is the X.", "tokens": [370, 264, 44746, 900, 551, 294, 300, 2604, 781, 307, 264, 1783, 13], "temperature": 0.0, "avg_logprob": -0.143912850594034, "compression_ratio": 1.5685279187817258, "no_speech_prob": 2.190755139963585e-06}, {"id": 760, "seek": 305484, "start": 3069.6200000000003, "end": 3073.08, "text": " If they're all going to be in the batch together, they have to be the same size, so we can just", "tokens": [759, 436, 434, 439, 516, 281, 312, 294, 264, 15245, 1214, 11, 436, 362, 281, 312, 264, 912, 2744, 11, 370, 321, 393, 445], "temperature": 0.0, "avg_logprob": -0.143912850594034, "compression_ratio": 1.5685279187817258, "no_speech_prob": 2.190755139963585e-06}, {"id": 761, "seek": 305484, "start": 3073.08, "end": 3075.2000000000003, "text": " go dot resize, no problem.", "tokens": [352, 5893, 50069, 11, 572, 1154, 13], "temperature": 0.0, "avg_logprob": -0.143912850594034, "compression_ratio": 1.5685279187817258, "no_speech_prob": 2.190755139963585e-06}, {"id": 762, "seek": 305484, "start": 3075.2000000000003, "end": 3081.9, "text": " I mean, that's not a great way to do it, but it's a start.", "tokens": [286, 914, 11, 300, 311, 406, 257, 869, 636, 281, 360, 309, 11, 457, 309, 311, 257, 722, 13], "temperature": 0.0, "avg_logprob": -0.143912850594034, "compression_ratio": 1.5685279187817258, "no_speech_prob": 2.190755139963585e-06}, {"id": 763, "seek": 308190, "start": 3081.9, "end": 3088.36, "text": " So here's a transform that resizes things, and it has to be after all the other transforms", "tokens": [407, 510, 311, 257, 4088, 300, 725, 5660, 721, 11, 293, 309, 575, 281, 312, 934, 439, 264, 661, 35592], "temperature": 0.0, "avg_logprob": -0.0961466672127707, "compression_ratio": 1.6489795918367347, "no_speech_prob": 9.516157660982572e-06}, {"id": 764, "seek": 308190, "start": 3088.36, "end": 3095.04, "text": " we've seen so far, because we want conversion to RGB to happen beforehand probably, stuff", "tokens": [321, 600, 1612, 370, 1400, 11, 570, 321, 528, 14298, 281, 31231, 281, 1051, 22893, 1391, 11, 1507], "temperature": 0.0, "avg_logprob": -0.0961466672127707, "compression_ratio": 1.6489795918367347, "no_speech_prob": 9.516157660982572e-06}, {"id": 765, "seek": 308190, "start": 3095.04, "end": 3096.04, "text": " like that.", "tokens": [411, 300, 13], "temperature": 0.0, "avg_logprob": -0.0961466672127707, "compression_ratio": 1.6489795918367347, "no_speech_prob": 9.516157660982572e-06}, {"id": 766, "seek": 308190, "start": 3096.04, "end": 3101.48, "text": " So we'll give this an order of 10, and this is something you pass in a size.", "tokens": [407, 321, 603, 976, 341, 364, 1668, 295, 1266, 11, 293, 341, 307, 746, 291, 1320, 294, 257, 2744, 13], "temperature": 0.0, "avg_logprob": -0.0961466672127707, "compression_ratio": 1.6489795918367347, "no_speech_prob": 9.516157660982572e-06}, {"id": 767, "seek": 308190, "start": 3101.48, "end": 3106.6, "text": " If you pass in an integer, we'll turn it into a tuple, and when you call it, it'll call", "tokens": [759, 291, 1320, 294, 364, 24922, 11, 321, 603, 1261, 309, 666, 257, 2604, 781, 11, 293, 562, 291, 818, 309, 11, 309, 603, 818], "temperature": 0.0, "avg_logprob": -0.0961466672127707, "compression_ratio": 1.6489795918367347, "no_speech_prob": 9.516157660982572e-06}, {"id": 768, "seek": 308190, "start": 3106.6, "end": 3111.2400000000002, "text": " resize, and it'll do bilinear resizing for you.", "tokens": [50069, 11, 293, 309, 603, 360, 8588, 533, 289, 725, 3319, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.0961466672127707, "compression_ratio": 1.6489795918367347, "no_speech_prob": 9.516157660982572e-06}, {"id": 769, "seek": 311124, "start": 3111.24, "end": 3113.64, "text": " So there's a transform.", "tokens": [407, 456, 311, 257, 4088, 13], "temperature": 0.0, "avg_logprob": -0.12960766301010596, "compression_ratio": 1.6216216216216217, "no_speech_prob": 2.2958954559726408e-06}, {"id": 770, "seek": 311124, "start": 3113.64, "end": 3119.7999999999997, "text": " Once you've turned them all into the same size, then we can turn them into tenses.", "tokens": [3443, 291, 600, 3574, 552, 439, 666, 264, 912, 2744, 11, 550, 321, 393, 1261, 552, 666, 256, 9085, 13], "temperature": 0.0, "avg_logprob": -0.12960766301010596, "compression_ratio": 1.6216216216216217, "no_speech_prob": 2.2958954559726408e-06}, {"id": 771, "seek": 311124, "start": 3119.7999999999997, "end": 3122.8799999999997, "text": " I stole this from TorchVision.", "tokens": [286, 16326, 341, 490, 7160, 339, 53, 1991, 13], "temperature": 0.0, "avg_logprob": -0.12960766301010596, "compression_ratio": 1.6216216216216217, "no_speech_prob": 2.2958954559726408e-06}, {"id": 772, "seek": 311124, "start": 3122.8799999999997, "end": 3130.4799999999996, "text": " This is how TorchVision turns pillow objects into tenses, and this has to happen after", "tokens": [639, 307, 577, 7160, 339, 53, 1991, 4523, 18581, 6565, 666, 256, 9085, 11, 293, 341, 575, 281, 1051, 934], "temperature": 0.0, "avg_logprob": -0.12960766301010596, "compression_ratio": 1.6216216216216217, "no_speech_prob": 2.2958954559726408e-06}, {"id": 773, "seek": 311124, "start": 3130.4799999999996, "end": 3134.9599999999996, "text": " the resizing, so we'll give this a lesser order.", "tokens": [264, 725, 3319, 11, 370, 321, 603, 976, 341, 257, 22043, 1668, 13], "temperature": 0.0, "avg_logprob": -0.12960766301010596, "compression_ratio": 1.6216216216216217, "no_speech_prob": 2.2958954559726408e-06}, {"id": 774, "seek": 311124, "start": 3134.9599999999996, "end": 3140.4799999999996, "text": " And you see these two ways here of adding kind of class level state or transform level", "tokens": [400, 291, 536, 613, 732, 2098, 510, 295, 5127, 733, 295, 1508, 1496, 1785, 420, 4088, 1496], "temperature": 0.0, "avg_logprob": -0.12960766301010596, "compression_ratio": 1.6216216216216217, "no_speech_prob": 2.2958954559726408e-06}, {"id": 775, "seek": 314048, "start": 3140.48, "end": 3141.48, "text": " state.", "tokens": [1785, 13], "temperature": 0.0, "avg_logprob": -0.12289928366070145, "compression_ratio": 1.8181818181818181, "no_speech_prob": 9.368435712531209e-06}, {"id": 776, "seek": 314048, "start": 3141.48, "end": 3143.48, "text": " I can actually attach state to a function.", "tokens": [286, 393, 767, 5085, 1785, 281, 257, 2445, 13], "temperature": 0.0, "avg_logprob": -0.12289928366070145, "compression_ratio": 1.8181818181818181, "no_speech_prob": 9.368435712531209e-06}, {"id": 777, "seek": 314048, "start": 3143.48, "end": 3146.36, "text": " This is really underused in Python, but it's super handy, right?", "tokens": [639, 307, 534, 833, 4717, 294, 15329, 11, 457, 309, 311, 1687, 13239, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.12289928366070145, "compression_ratio": 1.8181818181818181, "no_speech_prob": 9.368435712531209e-06}, {"id": 778, "seek": 314048, "start": 3146.36, "end": 3149.76, "text": " We've got a function, we just want to say, like, what's the order of the function, or", "tokens": [492, 600, 658, 257, 2445, 11, 321, 445, 528, 281, 584, 11, 411, 11, 437, 311, 264, 1668, 295, 264, 2445, 11, 420], "temperature": 0.0, "avg_logprob": -0.12289928366070145, "compression_ratio": 1.8181818181818181, "no_speech_prob": 9.368435712531209e-06}, {"id": 779, "seek": 314048, "start": 3149.76, "end": 3152.44, "text": " we can put it in the class.", "tokens": [321, 393, 829, 309, 294, 264, 1508, 13], "temperature": 0.0, "avg_logprob": -0.12289928366070145, "compression_ratio": 1.8181818181818181, "no_speech_prob": 9.368435712531209e-06}, {"id": 780, "seek": 314048, "start": 3152.44, "end": 3154.84, "text": " And then that's turned it into a byte tensor.", "tokens": [400, 550, 300, 311, 3574, 309, 666, 257, 40846, 40863, 13], "temperature": 0.0, "avg_logprob": -0.12289928366070145, "compression_ratio": 1.8181818181818181, "no_speech_prob": 9.368435712531209e-06}, {"id": 781, "seek": 314048, "start": 3154.84, "end": 3158.8, "text": " We actually need a float tensor, so here's how you turn it into a float, and we don't", "tokens": [492, 767, 643, 257, 15706, 40863, 11, 370, 510, 311, 577, 291, 1261, 309, 666, 257, 15706, 11, 293, 321, 500, 380], "temperature": 0.0, "avg_logprob": -0.12289928366070145, "compression_ratio": 1.8181818181818181, "no_speech_prob": 9.368435712531209e-06}, {"id": 782, "seek": 314048, "start": 3158.8, "end": 3161.0, "text": " want it to be between 0 and 255.", "tokens": [528, 309, 281, 312, 1296, 1958, 293, 3552, 20, 13], "temperature": 0.0, "avg_logprob": -0.12289928366070145, "compression_ratio": 1.8181818181818181, "no_speech_prob": 9.368435712531209e-06}, {"id": 783, "seek": 314048, "start": 3161.0, "end": 3165.28, "text": " We want it between 0 and 1, so we'll divide it in place by 255.", "tokens": [492, 528, 309, 1296, 1958, 293, 502, 11, 370, 321, 603, 9845, 309, 294, 1081, 538, 3552, 20, 13], "temperature": 0.0, "avg_logprob": -0.12289928366070145, "compression_ratio": 1.8181818181818181, "no_speech_prob": 9.368435712531209e-06}, {"id": 784, "seek": 314048, "start": 3165.28, "end": 3170.36, "text": " And that has to happen after it's a byte, so we'll give that a higher order again.", "tokens": [400, 300, 575, 281, 1051, 934, 309, 311, 257, 40846, 11, 370, 321, 603, 976, 300, 257, 2946, 1668, 797, 13], "temperature": 0.0, "avg_logprob": -0.12289928366070145, "compression_ratio": 1.8181818181818181, "no_speech_prob": 9.368435712531209e-06}, {"id": 785, "seek": 317036, "start": 3170.36, "end": 3172.1600000000003, "text": " So now here's our list of transforms.", "tokens": [407, 586, 510, 311, 527, 1329, 295, 35592, 13], "temperature": 0.0, "avg_logprob": -0.18295255999698817, "compression_ratio": 1.662280701754386, "no_speech_prob": 7.646380254300311e-06}, {"id": 786, "seek": 317036, "start": 3172.1600000000003, "end": 3175.0, "text": " It doesn't matter what order they're in the array, because they're going to order them", "tokens": [467, 1177, 380, 1871, 437, 1668, 436, 434, 294, 264, 10225, 11, 570, 436, 434, 516, 281, 1668, 552], "temperature": 0.0, "avg_logprob": -0.18295255999698817, "compression_ratio": 1.662280701754386, "no_speech_prob": 7.646380254300311e-06}, {"id": 787, "seek": 317036, "start": 3175.0, "end": 3178.36, "text": " by the underscore order attribute.", "tokens": [538, 264, 37556, 1668, 19667, 13], "temperature": 0.0, "avg_logprob": -0.18295255999698817, "compression_ratio": 1.662280701754386, "no_speech_prob": 7.646380254300311e-06}, {"id": 788, "seek": 317036, "start": 3178.36, "end": 3180.84, "text": " So we can pass that to our image list.", "tokens": [407, 321, 393, 1320, 300, 281, 527, 3256, 1329, 13], "temperature": 0.0, "avg_logprob": -0.18295255999698817, "compression_ratio": 1.662280701754386, "no_speech_prob": 7.646380254300311e-06}, {"id": 789, "seek": 317036, "start": 3180.84, "end": 3182.1200000000003, "text": " We can split it.", "tokens": [492, 393, 7472, 309, 13], "temperature": 0.0, "avg_logprob": -0.18295255999698817, "compression_ratio": 1.662280701754386, "no_speech_prob": 7.646380254300311e-06}, {"id": 790, "seek": 317036, "start": 3182.1200000000003, "end": 3183.7200000000003, "text": " We can label it.", "tokens": [492, 393, 7645, 309, 13], "temperature": 0.0, "avg_logprob": -0.18295255999698817, "compression_ratio": 1.662280701754386, "no_speech_prob": 7.646380254300311e-06}, {"id": 791, "seek": 317036, "start": 3183.7200000000003, "end": 3188.6, "text": " Here's a little convenience to permute the order back again.", "tokens": [1692, 311, 257, 707, 19283, 281, 4784, 1169, 264, 1668, 646, 797, 13], "temperature": 0.0, "avg_logprob": -0.18295255999698817, "compression_ratio": 1.662280701754386, "no_speech_prob": 7.646380254300311e-06}, {"id": 792, "seek": 317036, "start": 3188.6, "end": 3195.84, "text": " I don't know if you noticed this, but in to byte tensor I had to permute 201, because", "tokens": [286, 500, 380, 458, 498, 291, 5694, 341, 11, 457, 294, 281, 40846, 40863, 286, 632, 281, 4784, 1169, 1525, 11, 570], "temperature": 0.0, "avg_logprob": -0.18295255999698817, "compression_ratio": 1.662280701754386, "no_speech_prob": 7.646380254300311e-06}, {"id": 793, "seek": 319584, "start": 3195.84, "end": 3202.7200000000003, "text": " pillow has the channel last, where else PyTorch assumes the channel comes first.", "tokens": [18581, 575, 264, 2269, 1036, 11, 689, 1646, 9953, 51, 284, 339, 37808, 264, 2269, 1487, 700, 13], "temperature": 0.0, "avg_logprob": -0.1332348990686161, "compression_ratio": 1.756218905472637, "no_speech_prob": 4.356744284450542e-06}, {"id": 794, "seek": 319584, "start": 3202.7200000000003, "end": 3204.56, "text": " So this is just going to pop the channel first.", "tokens": [407, 341, 307, 445, 516, 281, 1665, 264, 2269, 700, 13], "temperature": 0.0, "avg_logprob": -0.1332348990686161, "compression_ratio": 1.756218905472637, "no_speech_prob": 4.356744284450542e-06}, {"id": 795, "seek": 319584, "start": 3204.56, "end": 3208.56, "text": " So to print them out, we have to put the channel last again.", "tokens": [407, 281, 4482, 552, 484, 11, 321, 362, 281, 829, 264, 2269, 1036, 797, 13], "temperature": 0.0, "avg_logprob": -0.1332348990686161, "compression_ratio": 1.756218905472637, "no_speech_prob": 4.356744284450542e-06}, {"id": 796, "seek": 319584, "start": 3208.56, "end": 3213.76, "text": " So now we can grab something from that list and show image.", "tokens": [407, 586, 321, 393, 4444, 746, 490, 300, 1329, 293, 855, 3256, 13], "temperature": 0.0, "avg_logprob": -0.1332348990686161, "compression_ratio": 1.756218905472637, "no_speech_prob": 4.356744284450542e-06}, {"id": 797, "seek": 319584, "start": 3213.76, "end": 3219.38, "text": " Here it is, and you can see that it is something of a torch thing of this size.", "tokens": [1692, 309, 307, 11, 293, 291, 393, 536, 300, 309, 307, 746, 295, 257, 27822, 551, 295, 341, 2744, 13], "temperature": 0.0, "avg_logprob": -0.1332348990686161, "compression_ratio": 1.756218905472637, "no_speech_prob": 4.356744284450542e-06}, {"id": 798, "seek": 319584, "start": 3219.38, "end": 3220.52, "text": " So that's looking good.", "tokens": [407, 300, 311, 1237, 665, 13], "temperature": 0.0, "avg_logprob": -0.1332348990686161, "compression_ratio": 1.756218905472637, "no_speech_prob": 4.356744284450542e-06}, {"id": 799, "seek": 322052, "start": 3220.52, "end": 3227.96, "text": " So we now have tensors that are floats and are all the same size, so we can train a model.", "tokens": [407, 321, 586, 362, 10688, 830, 300, 366, 37878, 293, 366, 439, 264, 912, 2744, 11, 370, 321, 393, 3847, 257, 2316, 13], "temperature": 0.0, "avg_logprob": -0.12415732812444004, "compression_ratio": 1.5682819383259912, "no_speech_prob": 2.3687327939114766e-06}, {"id": 800, "seek": 322052, "start": 3227.96, "end": 3229.64, "text": " So we've got a batch size.", "tokens": [407, 321, 600, 658, 257, 15245, 2744, 13], "temperature": 0.0, "avg_logprob": -0.12415732812444004, "compression_ratio": 1.5682819383259912, "no_speech_prob": 2.3687327939114766e-06}, {"id": 801, "seek": 322052, "start": 3229.64, "end": 3232.04, "text": " We'll use the get data loaders we had before.", "tokens": [492, 603, 764, 264, 483, 1412, 3677, 433, 321, 632, 949, 13], "temperature": 0.0, "avg_logprob": -0.12415732812444004, "compression_ratio": 1.5682819383259912, "no_speech_prob": 2.3687327939114766e-06}, {"id": 802, "seek": 322052, "start": 3232.04, "end": 3238.32, "text": " We can just pass in train invalid directly from our labeled list.", "tokens": [492, 393, 445, 1320, 294, 3847, 34702, 3838, 490, 527, 21335, 1329, 13], "temperature": 0.0, "avg_logprob": -0.12415732812444004, "compression_ratio": 1.5682819383259912, "no_speech_prob": 2.3687327939114766e-06}, {"id": 803, "seek": 322052, "start": 3238.32, "end": 3243.56, "text": " Let's grab a mini batch, and here it is, 64 by 3 by 128 by 128.", "tokens": [961, 311, 4444, 257, 8382, 15245, 11, 293, 510, 309, 307, 11, 12145, 538, 805, 538, 29810, 538, 29810, 13], "temperature": 0.0, "avg_logprob": -0.12415732812444004, "compression_ratio": 1.5682819383259912, "no_speech_prob": 2.3687327939114766e-06}, {"id": 804, "seek": 322052, "start": 3243.56, "end": 3248.84, "text": " And we can have a look at it, and we can see the vocab for it.", "tokens": [400, 321, 393, 362, 257, 574, 412, 309, 11, 293, 321, 393, 536, 264, 2329, 455, 337, 309, 13], "temperature": 0.0, "avg_logprob": -0.12415732812444004, "compression_ratio": 1.5682819383259912, "no_speech_prob": 2.3687327939114766e-06}, {"id": 805, "seek": 324884, "start": 3248.84, "end": 3253.32, "text": " We can see the whole mini batch of Y values.", "tokens": [492, 393, 536, 264, 1379, 8382, 15245, 295, 398, 4190, 13], "temperature": 0.0, "avg_logprob": -0.09982813558270855, "compression_ratio": 1.6977777777777778, "no_speech_prob": 2.0580334876285633e-06}, {"id": 806, "seek": 324884, "start": 3253.32, "end": 3257.88, "text": " So now we can create a data bunch.", "tokens": [407, 586, 321, 393, 1884, 257, 1412, 3840, 13], "temperature": 0.0, "avg_logprob": -0.09982813558270855, "compression_ratio": 1.6977777777777778, "no_speech_prob": 2.0580334876285633e-06}, {"id": 807, "seek": 324884, "start": 3257.88, "end": 3260.52, "text": " That's going to have our data loaders.", "tokens": [663, 311, 516, 281, 362, 527, 1412, 3677, 433, 13], "temperature": 0.0, "avg_logprob": -0.09982813558270855, "compression_ratio": 1.6977777777777778, "no_speech_prob": 2.0580334876285633e-06}, {"id": 808, "seek": 324884, "start": 3260.52, "end": 3266.56, "text": " And to make life even easier for the future, let's add two optional things, channels in", "tokens": [400, 281, 652, 993, 754, 3571, 337, 264, 2027, 11, 718, 311, 909, 732, 17312, 721, 11, 9235, 294], "temperature": 0.0, "avg_logprob": -0.09982813558270855, "compression_ratio": 1.6977777777777778, "no_speech_prob": 2.0580334876285633e-06}, {"id": 809, "seek": 324884, "start": 3266.56, "end": 3272.2400000000002, "text": " and channels out, and that way any models that want to be automatically created can", "tokens": [293, 9235, 484, 11, 293, 300, 636, 604, 5245, 300, 528, 281, 312, 6772, 2942, 393], "temperature": 0.0, "avg_logprob": -0.09982813558270855, "compression_ratio": 1.6977777777777778, "no_speech_prob": 2.0580334876285633e-06}, {"id": 810, "seek": 324884, "start": 3272.2400000000002, "end": 3275.6400000000003, "text": " automatically create themselves with the correct number of inputs and the correct number of", "tokens": [6772, 1884, 2969, 365, 264, 3006, 1230, 295, 15743, 293, 264, 3006, 1230, 295], "temperature": 0.0, "avg_logprob": -0.09982813558270855, "compression_ratio": 1.6977777777777778, "no_speech_prob": 2.0580334876285633e-06}, {"id": 811, "seek": 327564, "start": 3275.64, "end": 3280.16, "text": " outputs for our data set.", "tokens": [23930, 337, 527, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.12789644135369194, "compression_ratio": 1.4685714285714286, "no_speech_prob": 5.954838343313895e-06}, {"id": 812, "seek": 327564, "start": 3280.16, "end": 3289.96, "text": " And let's create an add to our split data, something called to data bunch, which is just", "tokens": [400, 718, 311, 1884, 364, 909, 281, 527, 7472, 1412, 11, 746, 1219, 281, 1412, 3840, 11, 597, 307, 445], "temperature": 0.0, "avg_logprob": -0.12789644135369194, "compression_ratio": 1.4685714285714286, "no_speech_prob": 5.954838343313895e-06}, {"id": 813, "seek": 327564, "start": 3289.96, "end": 3290.96, "text": " this function.", "tokens": [341, 2445, 13], "temperature": 0.0, "avg_logprob": -0.12789644135369194, "compression_ratio": 1.4685714285714286, "no_speech_prob": 5.954838343313895e-06}, {"id": 814, "seek": 327564, "start": 3290.96, "end": 3294.8799999999997, "text": " It just calls that get DLs we saw before.", "tokens": [467, 445, 5498, 300, 483, 413, 43, 82, 321, 1866, 949, 13], "temperature": 0.0, "avg_logprob": -0.12789644135369194, "compression_ratio": 1.4685714285714286, "no_speech_prob": 5.954838343313895e-06}, {"id": 815, "seek": 327564, "start": 3294.8799999999997, "end": 3302.12, "text": " So like in practice, in your actual module, you would go back and you would paste the", "tokens": [407, 411, 294, 3124, 11, 294, 428, 3539, 10088, 11, 291, 576, 352, 646, 293, 291, 576, 9163, 264], "temperature": 0.0, "avg_logprob": -0.12789644135369194, "compression_ratio": 1.4685714285714286, "no_speech_prob": 5.954838343313895e-06}, {"id": 816, "seek": 330212, "start": 3302.12, "end": 3305.96, "text": " contents of this back into your split data definition.", "tokens": [15768, 295, 341, 646, 666, 428, 7472, 1412, 7123, 13], "temperature": 0.0, "avg_logprob": -0.14299362532946527, "compression_ratio": 1.6293103448275863, "no_speech_prob": 1.1842021194752306e-05}, {"id": 817, "seek": 330212, "start": 3305.96, "end": 3312.8399999999997, "text": " But this is kind of a nice way when you're just iteratively building stuff, you can't", "tokens": [583, 341, 307, 733, 295, 257, 1481, 636, 562, 291, 434, 445, 17138, 19020, 2390, 1507, 11, 291, 393, 380], "temperature": 0.0, "avg_logprob": -0.14299362532946527, "compression_ratio": 1.6293103448275863, "no_speech_prob": 1.1842021194752306e-05}, {"id": 818, "seek": 330212, "start": 3312.8399999999997, "end": 3317.52, "text": " only monkey patch PyTorch things or standard library things, you can monkey patch your", "tokens": [787, 17847, 9972, 9953, 51, 284, 339, 721, 420, 3832, 6405, 721, 11, 291, 393, 17847, 9972, 428], "temperature": 0.0, "avg_logprob": -0.14299362532946527, "compression_ratio": 1.6293103448275863, "no_speech_prob": 1.1842021194752306e-05}, {"id": 819, "seek": 330212, "start": 3317.52, "end": 3318.52, "text": " own things.", "tokens": [1065, 721, 13], "temperature": 0.0, "avg_logprob": -0.14299362532946527, "compression_ratio": 1.6293103448275863, "no_speech_prob": 1.1842021194752306e-05}, {"id": 820, "seek": 330212, "start": 3318.52, "end": 3322.64, "text": " So here's how you can add something to a previous class when you realize later that you want", "tokens": [407, 510, 311, 577, 291, 393, 909, 746, 281, 257, 3894, 1508, 562, 291, 4325, 1780, 300, 291, 528], "temperature": 0.0, "avg_logprob": -0.14299362532946527, "compression_ratio": 1.6293103448275863, "no_speech_prob": 1.1842021194752306e-05}, {"id": 821, "seek": 330212, "start": 3322.64, "end": 3326.08, "text": " it.", "tokens": [309, 13], "temperature": 0.0, "avg_logprob": -0.14299362532946527, "compression_ratio": 1.6293103448275863, "no_speech_prob": 1.1842021194752306e-05}, {"id": 822, "seek": 330212, "start": 3326.08, "end": 3330.98, "text": " So let's go through and see what happens.", "tokens": [407, 718, 311, 352, 807, 293, 536, 437, 2314, 13], "temperature": 0.0, "avg_logprob": -0.14299362532946527, "compression_ratio": 1.6293103448275863, "no_speech_prob": 1.1842021194752306e-05}, {"id": 823, "seek": 333098, "start": 3330.98, "end": 3335.42, "text": " So here are all the steps, literally all the steps.", "tokens": [407, 510, 366, 439, 264, 4439, 11, 3736, 439, 264, 4439, 13], "temperature": 0.0, "avg_logprob": -0.13856863688273602, "compression_ratio": 1.8, "no_speech_prob": 2.178178147005383e-05}, {"id": 824, "seek": 333098, "start": 3335.42, "end": 3343.4, "text": " Grab the path, untar the data, grab the transforms, grab the item list, pass in the transforms,", "tokens": [20357, 264, 3100, 11, 1701, 289, 264, 1412, 11, 4444, 264, 35592, 11, 4444, 264, 3174, 1329, 11, 1320, 294, 264, 35592, 11], "temperature": 0.0, "avg_logprob": -0.13856863688273602, "compression_ratio": 1.8, "no_speech_prob": 2.178178147005383e-05}, {"id": 825, "seek": 333098, "start": 3343.4, "end": 3350.28, "text": " split the data using the grandparent, using this validation name, label it using parent", "tokens": [7472, 264, 1412, 1228, 264, 2697, 38321, 11, 1228, 341, 24071, 1315, 11, 7645, 309, 1228, 2596], "temperature": 0.0, "avg_logprob": -0.13856863688273602, "compression_ratio": 1.8, "no_speech_prob": 2.178178147005383e-05}, {"id": 826, "seek": 333098, "start": 3350.28, "end": 3357.32, "text": " labeler, and then turn it into a data bunch with this batch size, three channels in, ten", "tokens": [2715, 6185, 11, 293, 550, 1261, 309, 666, 257, 1412, 3840, 365, 341, 15245, 2744, 11, 1045, 9235, 294, 11, 2064], "temperature": 0.0, "avg_logprob": -0.13856863688273602, "compression_ratio": 1.8, "no_speech_prob": 2.178178147005383e-05}, {"id": 827, "seek": 335732, "start": 3357.32, "end": 3362.6400000000003, "text": " channels out, and we'll use four processes.", "tokens": [9235, 484, 11, 293, 321, 603, 764, 1451, 7555, 13], "temperature": 0.0, "avg_logprob": -0.11185078669076014, "compression_ratio": 1.7638888888888888, "no_speech_prob": 9.368565770273563e-06}, {"id": 828, "seek": 335732, "start": 3362.6400000000003, "end": 3367.52, "text": " Here's our callback functions from last time.", "tokens": [1692, 311, 527, 818, 3207, 6828, 490, 1036, 565, 13], "temperature": 0.0, "avg_logprob": -0.11185078669076014, "compression_ratio": 1.7638888888888888, "no_speech_prob": 9.368565770273563e-06}, {"id": 829, "seek": 335732, "start": 3367.52, "end": 3369.44, "text": " Let's make sure that we normalize.", "tokens": [961, 311, 652, 988, 300, 321, 2710, 1125, 13], "temperature": 0.0, "avg_logprob": -0.11185078669076014, "compression_ratio": 1.7638888888888888, "no_speech_prob": 9.368565770273563e-06}, {"id": 830, "seek": 335732, "start": 3369.44, "end": 3374.5, "text": " In the past, we've normalized things that have had only one channel being MNIST.", "tokens": [682, 264, 1791, 11, 321, 600, 48704, 721, 300, 362, 632, 787, 472, 2269, 885, 376, 45, 19756, 13], "temperature": 0.0, "avg_logprob": -0.11185078669076014, "compression_ratio": 1.7638888888888888, "no_speech_prob": 9.368565770273563e-06}, {"id": 831, "seek": 335732, "start": 3374.5, "end": 3378.6400000000003, "text": " Now we've got three channels, so we need to make sure that we take the mean over the other", "tokens": [823, 321, 600, 658, 1045, 9235, 11, 370, 321, 643, 281, 652, 988, 300, 321, 747, 264, 914, 670, 264, 661], "temperature": 0.0, "avg_logprob": -0.11185078669076014, "compression_ratio": 1.7638888888888888, "no_speech_prob": 9.368565770273563e-06}, {"id": 832, "seek": 335732, "start": 3378.6400000000003, "end": 3385.36, "text": " channels so that we get a three channel mean and a three channel standard deviation.", "tokens": [9235, 370, 300, 321, 483, 257, 1045, 2269, 914, 293, 257, 1045, 2269, 3832, 25163, 13], "temperature": 0.0, "avg_logprob": -0.11185078669076014, "compression_ratio": 1.7638888888888888, "no_speech_prob": 9.368565770273563e-06}, {"id": 833, "seek": 338536, "start": 3385.36, "end": 3390.6, "text": " So let's define a function that normalizes things that are three channels.", "tokens": [407, 718, 311, 6964, 257, 2445, 300, 2710, 5660, 721, 300, 366, 1045, 9235, 13], "temperature": 0.0, "avg_logprob": -0.08332416062713952, "compression_ratio": 1.672811059907834, "no_speech_prob": 5.5074951887945645e-06}, {"id": 834, "seek": 338536, "start": 3390.6, "end": 3393.4, "text": " So we're just broadcasting here.", "tokens": [407, 321, 434, 445, 30024, 510, 13], "temperature": 0.0, "avg_logprob": -0.08332416062713952, "compression_ratio": 1.672811059907834, "no_speech_prob": 5.5074951887945645e-06}, {"id": 835, "seek": 338536, "start": 3393.4, "end": 3398.6, "text": " So here's the mean and standard deviation of this ImageNet batch.", "tokens": [407, 510, 311, 264, 914, 293, 3832, 25163, 295, 341, 29903, 31890, 15245, 13], "temperature": 0.0, "avg_logprob": -0.08332416062713952, "compression_ratio": 1.672811059907834, "no_speech_prob": 5.5074951887945645e-06}, {"id": 836, "seek": 338536, "start": 3398.6, "end": 3404.1600000000003, "text": " So here's a function called norm ImageNet, which we can use from now on to normalize", "tokens": [407, 510, 311, 257, 2445, 1219, 2026, 29903, 31890, 11, 597, 321, 393, 764, 490, 586, 322, 281, 2710, 1125], "temperature": 0.0, "avg_logprob": -0.08332416062713952, "compression_ratio": 1.672811059907834, "no_speech_prob": 5.5074951887945645e-06}, {"id": 837, "seek": 338536, "start": 3404.1600000000003, "end": 3407.2000000000003, "text": " anything with this data set.", "tokens": [1340, 365, 341, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.08332416062713952, "compression_ratio": 1.672811059907834, "no_speech_prob": 5.5074951887945645e-06}, {"id": 838, "seek": 338536, "start": 3407.2000000000003, "end": 3414.4, "text": " So let's add that as a callback using the batch transform we built earlier.", "tokens": [407, 718, 311, 909, 300, 382, 257, 818, 3207, 1228, 264, 15245, 4088, 321, 3094, 3071, 13], "temperature": 0.0, "avg_logprob": -0.08332416062713952, "compression_ratio": 1.672811059907834, "no_speech_prob": 5.5074951887945645e-06}, {"id": 839, "seek": 341440, "start": 3414.4, "end": 3421.92, "text": " We will create a conv net with this number of layers.", "tokens": [492, 486, 1884, 257, 3754, 2533, 365, 341, 1230, 295, 7914, 13], "temperature": 0.0, "avg_logprob": -0.18538169006803143, "compression_ratio": 1.5031847133757963, "no_speech_prob": 2.7264227355772164e-06}, {"id": 840, "seek": 341440, "start": 3421.92, "end": 3423.4, "text": " And here's the conv net.", "tokens": [400, 510, 311, 264, 3754, 2533, 13], "temperature": 0.0, "avg_logprob": -0.18538169006803143, "compression_ratio": 1.5031847133757963, "no_speech_prob": 2.7264227355772164e-06}, {"id": 841, "seek": 341440, "start": 3423.4, "end": 3425.96, "text": " We're going to come back to that.", "tokens": [492, 434, 516, 281, 808, 646, 281, 300, 13], "temperature": 0.0, "avg_logprob": -0.18538169006803143, "compression_ratio": 1.5031847133757963, "no_speech_prob": 2.7264227355772164e-06}, {"id": 842, "seek": 341440, "start": 3425.96, "end": 3434.4, "text": " And then we will do our one cycle scheduling using cosine one cycle annealing, pass that", "tokens": [400, 550, 321, 486, 360, 527, 472, 6586, 29055, 1228, 23565, 472, 6586, 22256, 4270, 11, 1320, 300], "temperature": 0.0, "avg_logprob": -0.18538169006803143, "compression_ratio": 1.5031847133757963, "no_speech_prob": 2.7264227355772164e-06}, {"id": 843, "seek": 341440, "start": 3434.4, "end": 3440.76, "text": " into our get learn run, and train.", "tokens": [666, 527, 483, 1466, 1190, 11, 293, 3847, 13], "temperature": 0.0, "avg_logprob": -0.18538169006803143, "compression_ratio": 1.5031847133757963, "no_speech_prob": 2.7264227355772164e-06}, {"id": 844, "seek": 344076, "start": 3440.76, "end": 3453.48, "text": " And that's going to give us 72.6%, which if we look at the ImageNet leaderboard for 128", "tokens": [400, 300, 311, 516, 281, 976, 505, 18731, 13, 21, 8923, 597, 498, 321, 574, 412, 264, 29903, 31890, 5263, 3787, 337, 29810], "temperature": 0.0, "avg_logprob": -0.14315885908148263, "compression_ratio": 1.375, "no_speech_prob": 6.338980256259674e-06}, {"id": 845, "seek": 344076, "start": 3453.48, "end": 3458.32, "text": " pixels for I5 epochs, the best is 84.6 so far.", "tokens": [18668, 337, 286, 20, 30992, 28346, 11, 264, 1151, 307, 29018, 13, 21, 370, 1400, 13], "temperature": 0.0, "avg_logprob": -0.14315885908148263, "compression_ratio": 1.375, "no_speech_prob": 6.338980256259674e-06}, {"id": 846, "seek": 344076, "start": 3458.32, "end": 3461.7200000000003, "text": " So this is looking pretty good.", "tokens": [407, 341, 307, 1237, 1238, 665, 13], "temperature": 0.0, "avg_logprob": -0.14315885908148263, "compression_ratio": 1.375, "no_speech_prob": 6.338980256259674e-06}, {"id": 847, "seek": 344076, "start": 3461.7200000000003, "end": 3464.1400000000003, "text": " We're very much on the right track.", "tokens": [492, 434, 588, 709, 322, 264, 558, 2837, 13], "temperature": 0.0, "avg_logprob": -0.14315885908148263, "compression_ratio": 1.375, "no_speech_prob": 6.338980256259674e-06}, {"id": 848, "seek": 344076, "start": 3464.1400000000003, "end": 3467.28, "text": " So let's take a look and see what model we built.", "tokens": [407, 718, 311, 747, 257, 574, 293, 536, 437, 2316, 321, 3094, 13], "temperature": 0.0, "avg_logprob": -0.14315885908148263, "compression_ratio": 1.375, "no_speech_prob": 6.338980256259674e-06}, {"id": 849, "seek": 344076, "start": 3467.28, "end": 3468.28, "text": " Because it's kind of interesting.", "tokens": [1436, 309, 311, 733, 295, 1880, 13], "temperature": 0.0, "avg_logprob": -0.14315885908148263, "compression_ratio": 1.375, "no_speech_prob": 6.338980256259674e-06}, {"id": 850, "seek": 346828, "start": 3468.28, "end": 3471.52, "text": " There's a few interesting features of this model, and we're going to be looking at these", "tokens": [821, 311, 257, 1326, 1880, 4122, 295, 341, 2316, 11, 293, 321, 434, 516, 281, 312, 1237, 412, 613], "temperature": 0.0, "avg_logprob": -0.12692729548404091, "compression_ratio": 1.6143497757847534, "no_speech_prob": 8.139323654177133e-06}, {"id": 851, "seek": 346828, "start": 3471.52, "end": 3476.8, "text": " features quite a lot in the next two lessons.", "tokens": [4122, 1596, 257, 688, 294, 264, 958, 732, 8820, 13], "temperature": 0.0, "avg_logprob": -0.12692729548404091, "compression_ratio": 1.6143497757847534, "no_speech_prob": 8.139323654177133e-06}, {"id": 852, "seek": 346828, "start": 3476.8, "end": 3484.76, "text": " The model knows how big its first layer has to start out, because we pass in data, and", "tokens": [440, 2316, 3255, 577, 955, 1080, 700, 4583, 575, 281, 722, 484, 11, 570, 321, 1320, 294, 1412, 11, 293], "temperature": 0.0, "avg_logprob": -0.12692729548404091, "compression_ratio": 1.6143497757847534, "no_speech_prob": 8.139323654177133e-06}, {"id": 853, "seek": 346828, "start": 3484.76, "end": 3486.7200000000003, "text": " data has the channels in.", "tokens": [1412, 575, 264, 9235, 294, 13], "temperature": 0.0, "avg_logprob": -0.12692729548404091, "compression_ratio": 1.6143497757847534, "no_speech_prob": 8.139323654177133e-06}, {"id": 854, "seek": 346828, "start": 3486.7200000000003, "end": 3487.7200000000003, "text": " So this is nice.", "tokens": [407, 341, 307, 1481, 13], "temperature": 0.0, "avg_logprob": -0.12692729548404091, "compression_ratio": 1.6143497757847534, "no_speech_prob": 8.139323654177133e-06}, {"id": 855, "seek": 346828, "start": 3487.7200000000003, "end": 3493.8, "text": " Already this is a model which you don't have to change its definition if you have hyperspectral", "tokens": [23741, 341, 307, 257, 2316, 597, 291, 500, 380, 362, 281, 1319, 1080, 7123, 498, 291, 362, 7420, 433, 1043, 2155], "temperature": 0.0, "avg_logprob": -0.12692729548404091, "compression_ratio": 1.6143497757847534, "no_speech_prob": 8.139323654177133e-06}, {"id": 856, "seek": 349380, "start": 3493.8, "end": 3498.6400000000003, "text": " imaging with four channels, or you have black and white with one channel, or whatever.", "tokens": [25036, 365, 1451, 9235, 11, 420, 291, 362, 2211, 293, 2418, 365, 472, 2269, 11, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.17623030010022614, "compression_ratio": 1.7228260869565217, "no_speech_prob": 1.0289019883202855e-05}, {"id": 857, "seek": 349380, "start": 3498.6400000000003, "end": 3501.6000000000004, "text": " So this is going to change itself.", "tokens": [407, 341, 307, 516, 281, 1319, 2564, 13], "temperature": 0.0, "avg_logprob": -0.17623030010022614, "compression_ratio": 1.7228260869565217, "no_speech_prob": 1.0289019883202855e-05}, {"id": 858, "seek": 349380, "start": 3501.6000000000004, "end": 3504.7200000000003, "text": " Now what's the second layer going to be?", "tokens": [823, 437, 311, 264, 1150, 4583, 516, 281, 312, 30], "temperature": 0.0, "avg_logprob": -0.17623030010022614, "compression_ratio": 1.7228260869565217, "no_speech_prob": 1.0289019883202855e-05}, {"id": 859, "seek": 349380, "start": 3504.7200000000003, "end": 3507.0800000000004, "text": " Or I should say what's the output of the first layer going to be?", "tokens": [1610, 286, 820, 584, 437, 311, 264, 5598, 295, 264, 700, 4583, 516, 281, 312, 30], "temperature": 0.0, "avg_logprob": -0.17623030010022614, "compression_ratio": 1.7228260869565217, "no_speech_prob": 1.0289019883202855e-05}, {"id": 860, "seek": 349380, "start": 3507.0800000000004, "end": 3508.6000000000004, "text": " The input's going to be CN.", "tokens": [440, 4846, 311, 516, 281, 312, 14589, 13], "temperature": 0.0, "avg_logprob": -0.17623030010022614, "compression_ratio": 1.7228260869565217, "no_speech_prob": 1.0289019883202855e-05}, {"id": 861, "seek": 349380, "start": 3508.6000000000004, "end": 3509.6000000000004, "text": " What's the output going to be?", "tokens": [708, 311, 264, 5598, 516, 281, 312, 30], "temperature": 0.0, "avg_logprob": -0.17623030010022614, "compression_ratio": 1.7228260869565217, "no_speech_prob": 1.0289019883202855e-05}, {"id": 862, "seek": 349380, "start": 3509.6000000000004, "end": 3514.28, "text": " Is it going to be 16, 32, 64?", "tokens": [1119, 309, 516, 281, 312, 3165, 11, 8858, 11, 12145, 30], "temperature": 0.0, "avg_logprob": -0.17623030010022614, "compression_ratio": 1.7228260869565217, "no_speech_prob": 1.0289019883202855e-05}, {"id": 863, "seek": 351428, "start": 3514.28, "end": 3524.6000000000004, "text": " Well, what we're going to do is we're going to say, well, our input has, we don't know,", "tokens": [1042, 11, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 584, 11, 731, 11, 527, 4846, 575, 11, 321, 500, 380, 458, 11], "temperature": 0.0, "avg_logprob": -0.10794845281862746, "compression_ratio": 1.7320574162679425, "no_speech_prob": 6.540284175571287e-06}, {"id": 864, "seek": 351428, "start": 3524.6000000000004, "end": 3526.32, "text": " some number of channels.", "tokens": [512, 1230, 295, 9235, 13], "temperature": 0.0, "avg_logprob": -0.10794845281862746, "compression_ratio": 1.7320574162679425, "no_speech_prob": 6.540284175571287e-06}, {"id": 865, "seek": 351428, "start": 3526.32, "end": 3530.76, "text": " But we do know that the first layer is going to be a 3 by 3 kernel, and then there's going", "tokens": [583, 321, 360, 458, 300, 264, 700, 4583, 307, 516, 281, 312, 257, 805, 538, 805, 28256, 11, 293, 550, 456, 311, 516], "temperature": 0.0, "avg_logprob": -0.10794845281862746, "compression_ratio": 1.7320574162679425, "no_speech_prob": 6.540284175571287e-06}, {"id": 866, "seek": 351428, "start": 3530.76, "end": 3538.2000000000003, "text": " to be some number of channels, CN channels, which in our case is 3.", "tokens": [281, 312, 512, 1230, 295, 9235, 11, 14589, 9235, 11, 597, 294, 527, 1389, 307, 805, 13], "temperature": 0.0, "avg_logprob": -0.10794845281862746, "compression_ratio": 1.7320574162679425, "no_speech_prob": 6.540284175571287e-06}, {"id": 867, "seek": 351428, "start": 3538.2000000000003, "end": 3544.1600000000003, "text": " So as the convolution kernel kind of scrolls over the input image, at each time the number", "tokens": [407, 382, 264, 45216, 28256, 733, 295, 11369, 82, 670, 264, 4846, 3256, 11, 412, 1184, 565, 264, 1230], "temperature": 0.0, "avg_logprob": -0.10794845281862746, "compression_ratio": 1.7320574162679425, "no_speech_prob": 6.540284175571287e-06}, {"id": 868, "seek": 354416, "start": 3544.16, "end": 3550.48, "text": " of things that it's multiplying together is going to be 3 by 3 by CN.", "tokens": [295, 721, 300, 309, 311, 30955, 1214, 307, 516, 281, 312, 805, 538, 805, 538, 14589, 13], "temperature": 0.0, "avg_logprob": -0.12921416241189707, "compression_ratio": 1.5087719298245614, "no_speech_prob": 6.438838227040833e-06}, {"id": 869, "seek": 354416, "start": 3550.48, "end": 3553.7999999999997, "text": " So 9 by CN.", "tokens": [407, 1722, 538, 14589, 13], "temperature": 0.0, "avg_logprob": -0.12921416241189707, "compression_ratio": 1.5087719298245614, "no_speech_prob": 6.438838227040833e-06}, {"id": 870, "seek": 354416, "start": 3553.7999999999997, "end": 3556.3199999999997, "text": " So remember we talked about this last week, right?", "tokens": [407, 1604, 321, 2825, 466, 341, 1036, 1243, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.12921416241189707, "compression_ratio": 1.5087719298245614, "no_speech_prob": 6.438838227040833e-06}, {"id": 871, "seek": 354416, "start": 3556.3199999999997, "end": 3566.3999999999996, "text": " We basically want to put that, we basically want to make sure that our first convolution", "tokens": [492, 1936, 528, 281, 829, 300, 11, 321, 1936, 528, 281, 652, 988, 300, 527, 700, 45216], "temperature": 0.0, "avg_logprob": -0.12921416241189707, "compression_ratio": 1.5087719298245614, "no_speech_prob": 6.438838227040833e-06}, {"id": 872, "seek": 354416, "start": 3566.3999999999996, "end": 3571.7999999999997, "text": " actually has something useful to do.", "tokens": [767, 575, 746, 4420, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.12921416241189707, "compression_ratio": 1.5087719298245614, "no_speech_prob": 6.438838227040833e-06}, {"id": 873, "seek": 357180, "start": 3571.8, "end": 3578.48, "text": " So if we're getting 9 by CN coming in, you wouldn't want more than that going out, because", "tokens": [407, 498, 321, 434, 1242, 1722, 538, 14589, 1348, 294, 11, 291, 2759, 380, 528, 544, 813, 300, 516, 484, 11, 570], "temperature": 0.0, "avg_logprob": -0.08123614336993243, "compression_ratio": 1.6167400881057268, "no_speech_prob": 7.766281669319142e-06}, {"id": 874, "seek": 357180, "start": 3578.48, "end": 3581.04, "text": " it's basically a wasted time.", "tokens": [309, 311, 1936, 257, 19496, 565, 13], "temperature": 0.0, "avg_logprob": -0.08123614336993243, "compression_ratio": 1.6167400881057268, "no_speech_prob": 7.766281669319142e-06}, {"id": 875, "seek": 357180, "start": 3581.04, "end": 3582.92, "text": " So we discussed that briefly last week.", "tokens": [407, 321, 7152, 300, 10515, 1036, 1243, 13], "temperature": 0.0, "avg_logprob": -0.08123614336993243, "compression_ratio": 1.6167400881057268, "no_speech_prob": 7.766281669319142e-06}, {"id": 876, "seek": 357180, "start": 3582.92, "end": 3590.96, "text": " So what I'm going to do is I'm going to say, okay, let's take that value, CN by 3 by 3,", "tokens": [407, 437, 286, 478, 516, 281, 360, 307, 286, 478, 516, 281, 584, 11, 1392, 11, 718, 311, 747, 300, 2158, 11, 14589, 538, 805, 538, 805, 11], "temperature": 0.0, "avg_logprob": -0.08123614336993243, "compression_ratio": 1.6167400881057268, "no_speech_prob": 7.766281669319142e-06}, {"id": 877, "seek": 357180, "start": 3590.96, "end": 3598.6800000000003, "text": " and let's just look for the next largest number that's a power of 2, and we'll use that.", "tokens": [293, 718, 311, 445, 574, 337, 264, 958, 6443, 1230, 300, 311, 257, 1347, 295, 568, 11, 293, 321, 603, 764, 300, 13], "temperature": 0.0, "avg_logprob": -0.08123614336993243, "compression_ratio": 1.6167400881057268, "no_speech_prob": 7.766281669319142e-06}, {"id": 878, "seek": 357180, "start": 3598.6800000000003, "end": 3601.2400000000002, "text": " So then that's how I do that.", "tokens": [407, 550, 300, 311, 577, 286, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.08123614336993243, "compression_ratio": 1.6167400881057268, "no_speech_prob": 7.766281669319142e-06}, {"id": 879, "seek": 360124, "start": 3601.24, "end": 3605.3999999999996, "text": " And then I'll just go ahead and multiply by 2 for each of the next two layers.", "tokens": [400, 550, 286, 603, 445, 352, 2286, 293, 12972, 538, 568, 337, 1184, 295, 264, 958, 732, 7914, 13], "temperature": 0.0, "avg_logprob": -0.1115308403968811, "compression_ratio": 1.5612648221343874, "no_speech_prob": 1.5205749150482006e-05}, {"id": 880, "seek": 360124, "start": 3605.3999999999996, "end": 3611.68, "text": " So this way we've got these vital first three layers are going to work out pretty well.", "tokens": [407, 341, 636, 321, 600, 658, 613, 11707, 700, 1045, 7914, 366, 516, 281, 589, 484, 1238, 731, 13], "temperature": 0.0, "avg_logprob": -0.1115308403968811, "compression_ratio": 1.5612648221343874, "no_speech_prob": 1.5205749150482006e-05}, {"id": 881, "seek": 360124, "start": 3611.68, "end": 3619.08, "text": " So back in the old days, we used to use 5 by 5 and 7 by 7 kernels, okay?", "tokens": [407, 646, 294, 264, 1331, 1708, 11, 321, 1143, 281, 764, 1025, 538, 1025, 293, 1614, 538, 1614, 23434, 1625, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.1115308403968811, "compression_ratio": 1.5612648221343874, "no_speech_prob": 1.5205749150482006e-05}, {"id": 882, "seek": 360124, "start": 3619.08, "end": 3622.72, "text": " We'd have the first layer would be one of those.", "tokens": [492, 1116, 362, 264, 700, 4583, 576, 312, 472, 295, 729, 13], "temperature": 0.0, "avg_logprob": -0.1115308403968811, "compression_ratio": 1.5612648221343874, "no_speech_prob": 1.5205749150482006e-05}, {"id": 883, "seek": 360124, "start": 3622.72, "end": 3625.7599999999998, "text": " But we know now that's not a good idea.", "tokens": [583, 321, 458, 586, 300, 311, 406, 257, 665, 1558, 13], "temperature": 0.0, "avg_logprob": -0.1115308403968811, "compression_ratio": 1.5612648221343874, "no_speech_prob": 1.5205749150482006e-05}, {"id": 884, "seek": 360124, "start": 3625.7599999999998, "end": 3630.2, "text": " Still most people do it, because people stick with what they know.", "tokens": [8291, 881, 561, 360, 309, 11, 570, 561, 2897, 365, 437, 436, 458, 13], "temperature": 0.0, "avg_logprob": -0.1115308403968811, "compression_ratio": 1.5612648221343874, "no_speech_prob": 1.5205749150482006e-05}, {"id": 885, "seek": 363020, "start": 3630.2, "end": 3637.12, "text": " But when you look at the bag of tricks for image classification paper, which in turn", "tokens": [583, 562, 291, 574, 412, 264, 3411, 295, 11733, 337, 3256, 21538, 3035, 11, 597, 294, 1261], "temperature": 0.0, "avg_logprob": -0.11636043548583984, "compression_ratio": 1.556, "no_speech_prob": 7.527350135205779e-06}, {"id": 886, "seek": 363020, "start": 3637.12, "end": 3643.48, "text": " refers to many previous citations, many of which are state of the art and competition", "tokens": [14942, 281, 867, 3894, 4814, 763, 11, 867, 295, 597, 366, 1785, 295, 264, 1523, 293, 6211], "temperature": 0.0, "avg_logprob": -0.11636043548583984, "compression_ratio": 1.556, "no_speech_prob": 7.527350135205779e-06}, {"id": 887, "seek": 363020, "start": 3643.48, "end": 3647.08, "text": " winning models, the message is always clear.", "tokens": [8224, 5245, 11, 264, 3636, 307, 1009, 1850, 13], "temperature": 0.0, "avg_logprob": -0.11636043548583984, "compression_ratio": 1.556, "no_speech_prob": 7.527350135205779e-06}, {"id": 888, "seek": 363020, "start": 3647.08, "end": 3651.0, "text": " Three by three kernels give you more bang for your buck.", "tokens": [6244, 538, 1045, 23434, 1625, 976, 291, 544, 8550, 337, 428, 14894, 13], "temperature": 0.0, "avg_logprob": -0.11636043548583984, "compression_ratio": 1.556, "no_speech_prob": 7.527350135205779e-06}, {"id": 889, "seek": 363020, "start": 3651.0, "end": 3652.16, "text": " You get deeper.", "tokens": [509, 483, 7731, 13], "temperature": 0.0, "avg_logprob": -0.11636043548583984, "compression_ratio": 1.556, "no_speech_prob": 7.527350135205779e-06}, {"id": 890, "seek": 363020, "start": 3652.16, "end": 3654.16, "text": " You end up with the same receptive field.", "tokens": [509, 917, 493, 365, 264, 912, 45838, 2519, 13], "temperature": 0.0, "avg_logprob": -0.11636043548583984, "compression_ratio": 1.556, "no_speech_prob": 7.527350135205779e-06}, {"id": 891, "seek": 363020, "start": 3654.16, "end": 3658.24, "text": " It's faster, because you've got less work going on, right?", "tokens": [467, 311, 4663, 11, 570, 291, 600, 658, 1570, 589, 516, 322, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.11636043548583984, "compression_ratio": 1.556, "no_speech_prob": 7.527350135205779e-06}, {"id": 892, "seek": 365824, "start": 3658.24, "end": 3663.56, "text": " And really this goes all the way back to the classic Zeiler and Fergus paper that we've", "tokens": [400, 534, 341, 1709, 439, 264, 636, 646, 281, 264, 7230, 4853, 5441, 293, 36790, 3035, 300, 321, 600], "temperature": 0.0, "avg_logprob": -0.14186121026674905, "compression_ratio": 1.6517857142857142, "no_speech_prob": 3.041057425434701e-06}, {"id": 893, "seek": 365824, "start": 3663.56, "end": 3668.16, "text": " looked at so many times over the years that we've been doing this course.", "tokens": [2956, 412, 370, 867, 1413, 670, 264, 924, 300, 321, 600, 668, 884, 341, 1164, 13], "temperature": 0.0, "avg_logprob": -0.14186121026674905, "compression_ratio": 1.6517857142857142, "no_speech_prob": 3.041057425434701e-06}, {"id": 894, "seek": 365824, "start": 3668.16, "end": 3676.08, "text": " And even before that to the VGG paper, it really is three by three kernels everywhere.", "tokens": [400, 754, 949, 300, 281, 264, 691, 27561, 3035, 11, 309, 534, 307, 1045, 538, 1045, 23434, 1625, 5315, 13], "temperature": 0.0, "avg_logprob": -0.14186121026674905, "compression_ratio": 1.6517857142857142, "no_speech_prob": 3.041057425434701e-06}, {"id": 895, "seek": 365824, "start": 3676.08, "end": 3682.7999999999997, "text": " So any place you see something that's not a three by three kernel, have a big think", "tokens": [407, 604, 1081, 291, 536, 746, 300, 311, 406, 257, 1045, 538, 1045, 28256, 11, 362, 257, 955, 519], "temperature": 0.0, "avg_logprob": -0.14186121026674905, "compression_ratio": 1.6517857142857142, "no_speech_prob": 3.041057425434701e-06}, {"id": 896, "seek": 365824, "start": 3682.7999999999997, "end": 3685.6, "text": " about whether that makes sense.", "tokens": [466, 1968, 300, 1669, 2020, 13], "temperature": 0.0, "avg_logprob": -0.14186121026674905, "compression_ratio": 1.6517857142857142, "no_speech_prob": 3.041057425434701e-06}, {"id": 897, "seek": 365824, "start": 3685.6, "end": 3687.24, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.14186121026674905, "compression_ratio": 1.6517857142857142, "no_speech_prob": 3.041057425434701e-06}, {"id": 898, "seek": 368724, "start": 3687.24, "end": 3692.6, "text": " So that's basically what we have for those critical first three layers.", "tokens": [407, 300, 311, 1936, 437, 321, 362, 337, 729, 4924, 700, 1045, 7914, 13], "temperature": 0.0, "avg_logprob": -0.13275530731793747, "compression_ratio": 1.71900826446281, "no_speech_prob": 6.960858172533335e-06}, {"id": 899, "seek": 368724, "start": 3692.6, "end": 3696.52, "text": " That's where that initial feature representation is happening.", "tokens": [663, 311, 689, 300, 5883, 4111, 10290, 307, 2737, 13], "temperature": 0.0, "avg_logprob": -0.13275530731793747, "compression_ratio": 1.71900826446281, "no_speech_prob": 6.960858172533335e-06}, {"id": 900, "seek": 368724, "start": 3696.52, "end": 3703.2, "text": " And then the rest of the layers is whatever we've asked for.", "tokens": [400, 550, 264, 1472, 295, 264, 7914, 307, 2035, 321, 600, 2351, 337, 13], "temperature": 0.0, "avg_logprob": -0.13275530731793747, "compression_ratio": 1.71900826446281, "no_speech_prob": 6.960858172533335e-06}, {"id": 901, "seek": 368724, "start": 3703.2, "end": 3706.52, "text": " And so then we can build those layers up, just saying number of filters in to number", "tokens": [400, 370, 550, 321, 393, 1322, 729, 7914, 493, 11, 445, 1566, 1230, 295, 15995, 294, 281, 1230], "temperature": 0.0, "avg_logprob": -0.13275530731793747, "compression_ratio": 1.71900826446281, "no_speech_prob": 6.960858172533335e-06}, {"id": 902, "seek": 368724, "start": 3706.52, "end": 3709.04, "text": " of filters out for each filter.", "tokens": [295, 15995, 484, 337, 1184, 6608, 13], "temperature": 0.0, "avg_logprob": -0.13275530731793747, "compression_ratio": 1.71900826446281, "no_speech_prob": 6.960858172533335e-06}, {"id": 903, "seek": 368724, "start": 3709.04, "end": 3714.8399999999997, "text": " And then as usual, average pooling, flatten, and a linear layer to however many classes", "tokens": [400, 550, 382, 7713, 11, 4274, 7005, 278, 11, 24183, 11, 293, 257, 8213, 4583, 281, 4461, 867, 5359], "temperature": 0.0, "avg_logprob": -0.13275530731793747, "compression_ratio": 1.71900826446281, "no_speech_prob": 6.960858172533335e-06}, {"id": 904, "seek": 368724, "start": 3714.8399999999997, "end": 3716.8399999999997, "text": " is in our data.", "tokens": [307, 294, 527, 1412, 13], "temperature": 0.0, "avg_logprob": -0.13275530731793747, "compression_ratio": 1.71900826446281, "no_speech_prob": 6.960858172533335e-06}, {"id": 905, "seek": 371684, "start": 3716.84, "end": 3723.2400000000002, "text": " That's it.", "tokens": [663, 311, 309, 13], "temperature": 0.0, "avg_logprob": -0.11498199112113865, "compression_ratio": 1.6483516483516483, "no_speech_prob": 5.68216592000681e-06}, {"id": 906, "seek": 371684, "start": 3723.2400000000002, "end": 3726.52, "text": " It's very hard to...", "tokens": [467, 311, 588, 1152, 281, 485], "temperature": 0.0, "avg_logprob": -0.11498199112113865, "compression_ratio": 1.6483516483516483, "no_speech_prob": 5.68216592000681e-06}, {"id": 907, "seek": 371684, "start": 3726.52, "end": 3731.0, "text": " Every time I write something like this, I break it the first 12 times.", "tokens": [2048, 565, 286, 2464, 746, 411, 341, 11, 286, 1821, 309, 264, 700, 2272, 1413, 13], "temperature": 0.0, "avg_logprob": -0.11498199112113865, "compression_ratio": 1.6483516483516483, "no_speech_prob": 5.68216592000681e-06}, {"id": 908, "seek": 371684, "start": 3731.0, "end": 3734.88, "text": " And the only way to debug it is to see exactly what's going on.", "tokens": [400, 264, 787, 636, 281, 24083, 309, 307, 281, 536, 2293, 437, 311, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.11498199112113865, "compression_ratio": 1.6483516483516483, "no_speech_prob": 5.68216592000681e-06}, {"id": 909, "seek": 371684, "start": 3734.88, "end": 3740.4, "text": " To see exactly what's going on, you need to see that what module is there at each point,", "tokens": [1407, 536, 2293, 437, 311, 516, 322, 11, 291, 643, 281, 536, 300, 437, 10088, 307, 456, 412, 1184, 935, 11], "temperature": 0.0, "avg_logprob": -0.11498199112113865, "compression_ratio": 1.6483516483516483, "no_speech_prob": 5.68216592000681e-06}, {"id": 910, "seek": 371684, "start": 3740.4, "end": 3745.06, "text": " and what is the output shape at each module.", "tokens": [293, 437, 307, 264, 5598, 3909, 412, 1184, 10088, 13], "temperature": 0.0, "avg_logprob": -0.11498199112113865, "compression_ratio": 1.6483516483516483, "no_speech_prob": 5.68216592000681e-06}, {"id": 911, "seek": 374506, "start": 3745.06, "end": 3748.4, "text": " So that's why we've created this model summary.", "tokens": [407, 300, 311, 983, 321, 600, 2942, 341, 2316, 12691, 13], "temperature": 0.0, "avg_logprob": -0.11998290600983993, "compression_ratio": 1.7025641025641025, "no_speech_prob": 2.156757773263962e-06}, {"id": 912, "seek": 374506, "start": 3748.4, "end": 3753.36, "text": " So model summary is going to use that get batch that we added in the LSUV notebook to", "tokens": [407, 2316, 12691, 307, 516, 281, 764, 300, 483, 15245, 300, 321, 3869, 294, 264, 441, 20214, 53, 21060, 281], "temperature": 0.0, "avg_logprob": -0.11998290600983993, "compression_ratio": 1.7025641025641025, "no_speech_prob": 2.156757773263962e-06}, {"id": 913, "seek": 374506, "start": 3753.36, "end": 3757.52, "text": " grab one batch of data.", "tokens": [4444, 472, 15245, 295, 1412, 13], "temperature": 0.0, "avg_logprob": -0.11998290600983993, "compression_ratio": 1.7025641025641025, "no_speech_prob": 2.156757773263962e-06}, {"id": 914, "seek": 374506, "start": 3757.52, "end": 3762.08, "text": " We will make sure that that batch is on the correct device.", "tokens": [492, 486, 652, 988, 300, 300, 15245, 307, 322, 264, 3006, 4302, 13], "temperature": 0.0, "avg_logprob": -0.11998290600983993, "compression_ratio": 1.7025641025641025, "no_speech_prob": 2.156757773263962e-06}, {"id": 915, "seek": 374506, "start": 3762.08, "end": 3767.86, "text": " We will use the find modules thing that we used in the LSUV to find all of the places", "tokens": [492, 486, 764, 264, 915, 16679, 551, 300, 321, 1143, 294, 264, 441, 20214, 53, 281, 915, 439, 295, 264, 3190], "temperature": 0.0, "avg_logprob": -0.11998290600983993, "compression_ratio": 1.7025641025641025, "no_speech_prob": 2.156757773263962e-06}, {"id": 916, "seek": 374506, "start": 3767.86, "end": 3772.48, "text": " that there's a linear layer.", "tokens": [300, 456, 311, 257, 8213, 4583, 13], "temperature": 0.0, "avg_logprob": -0.11998290600983993, "compression_ratio": 1.7025641025641025, "no_speech_prob": 2.156757773263962e-06}, {"id": 917, "seek": 377248, "start": 3772.48, "end": 3780.72, "text": " If you said find all, otherwise we will grab just the immediate children.", "tokens": [759, 291, 848, 915, 439, 11, 5911, 321, 486, 4444, 445, 264, 11629, 2227, 13], "temperature": 0.0, "avg_logprob": -0.06757518980238172, "compression_ratio": 1.5359116022099448, "no_speech_prob": 3.7852425975870574e-06}, {"id": 918, "seek": 377248, "start": 3780.72, "end": 3786.32, "text": " We will grab a hook for every layer using the hooks that we made.", "tokens": [492, 486, 4444, 257, 6328, 337, 633, 4583, 1228, 264, 26485, 300, 321, 1027, 13], "temperature": 0.0, "avg_logprob": -0.06757518980238172, "compression_ratio": 1.5359116022099448, "no_speech_prob": 3.7852425975870574e-06}, {"id": 919, "seek": 377248, "start": 3786.32, "end": 3789.4, "text": " And so now we can pass that model through.", "tokens": [400, 370, 586, 321, 393, 1320, 300, 2316, 807, 13], "temperature": 0.0, "avg_logprob": -0.06757518980238172, "compression_ratio": 1.5359116022099448, "no_speech_prob": 3.7852425975870574e-06}, {"id": 920, "seek": 377248, "start": 3789.4, "end": 3797.44, "text": " And the function that we've used for hooking simply prints out the module and the output", "tokens": [400, 264, 2445, 300, 321, 600, 1143, 337, 1106, 5953, 2935, 22305, 484, 264, 10088, 293, 264, 5598], "temperature": 0.0, "avg_logprob": -0.06757518980238172, "compression_ratio": 1.5359116022099448, "no_speech_prob": 3.7852425975870574e-06}, {"id": 921, "seek": 377248, "start": 3797.44, "end": 3798.44, "text": " shape.", "tokens": [3909, 13], "temperature": 0.0, "avg_logprob": -0.06757518980238172, "compression_ratio": 1.5359116022099448, "no_speech_prob": 3.7852425975870574e-06}, {"id": 922, "seek": 379844, "start": 3798.44, "end": 3803.36, "text": " That's how easy it is to create this wonderfully useful model summary.", "tokens": [663, 311, 577, 1858, 309, 307, 281, 1884, 341, 38917, 4420, 2316, 12691, 13], "temperature": 0.0, "avg_logprob": -0.13796506179006476, "compression_ratio": 1.7272727272727273, "no_speech_prob": 5.014564067096217e-06}, {"id": 923, "seek": 379844, "start": 3803.36, "end": 3809.8, "text": " So to answer your question of earlier, another reason like why are we doing this or what", "tokens": [407, 281, 1867, 428, 1168, 295, 3071, 11, 1071, 1778, 411, 983, 366, 321, 884, 341, 420, 437], "temperature": 0.0, "avg_logprob": -0.13796506179006476, "compression_ratio": 1.7272727272727273, "no_speech_prob": 5.014564067096217e-06}, {"id": 924, "seek": 379844, "start": 3809.8, "end": 3816.44, "text": " are you meant to be getting out of it, is to say like you don't have to write much code", "tokens": [366, 291, 4140, 281, 312, 1242, 484, 295, 309, 11, 307, 281, 584, 411, 291, 500, 380, 362, 281, 2464, 709, 3089], "temperature": 0.0, "avg_logprob": -0.13796506179006476, "compression_ratio": 1.7272727272727273, "no_speech_prob": 5.014564067096217e-06}, {"id": 925, "seek": 379844, "start": 3816.44, "end": 3820.44, "text": " to create really useful tools and telemetry.", "tokens": [281, 1884, 534, 4420, 3873, 293, 4304, 5537, 627, 13], "temperature": 0.0, "avg_logprob": -0.13796506179006476, "compression_ratio": 1.7272727272727273, "no_speech_prob": 5.014564067096217e-06}, {"id": 926, "seek": 379844, "start": 3820.44, "end": 3827.56, "text": " So we've seen how to create per layer histogram viewers, how to create model summaries.", "tokens": [407, 321, 600, 1612, 577, 281, 1884, 680, 4583, 49816, 8499, 11, 577, 281, 1884, 2316, 8367, 4889, 13], "temperature": 0.0, "avg_logprob": -0.13796506179006476, "compression_ratio": 1.7272727272727273, "no_speech_prob": 5.014564067096217e-06}, {"id": 927, "seek": 382756, "start": 3827.56, "end": 3832.16, "text": " With the tools that you have at your disposal now, I really hope that you can dig inside", "tokens": [2022, 264, 3873, 300, 291, 362, 412, 428, 26400, 586, 11, 286, 534, 1454, 300, 291, 393, 2528, 1854], "temperature": 0.0, "avg_logprob": -0.15063233997510828, "compression_ratio": 1.5953488372093023, "no_speech_prob": 1.520407840871485e-05}, {"id": 928, "seek": 382756, "start": 3832.16, "end": 3836.64, "text": " your models what they are and what they're doing.", "tokens": [428, 5245, 437, 436, 366, 293, 437, 436, 434, 884, 13], "temperature": 0.0, "avg_logprob": -0.15063233997510828, "compression_ratio": 1.5953488372093023, "no_speech_prob": 1.520407840871485e-05}, {"id": 929, "seek": 382756, "start": 3836.64, "end": 3839.12, "text": " And you see that it's all about hooks.", "tokens": [400, 291, 536, 300, 309, 311, 439, 466, 26485, 13], "temperature": 0.0, "avg_logprob": -0.15063233997510828, "compression_ratio": 1.5953488372093023, "no_speech_prob": 1.520407840871485e-05}, {"id": 930, "seek": 382756, "start": 3839.12, "end": 3843.96, "text": " So this hooks thing we have is just super, super useful.", "tokens": [407, 341, 26485, 551, 321, 362, 307, 445, 1687, 11, 1687, 4420, 13], "temperature": 0.0, "avg_logprob": -0.15063233997510828, "compression_ratio": 1.5953488372093023, "no_speech_prob": 1.520407840871485e-05}, {"id": 931, "seek": 382756, "start": 3843.96, "end": 3848.6, "text": " I'm very grateful to the PyTorch team for adding this fantastic functionality.", "tokens": [286, 478, 588, 7941, 281, 264, 9953, 51, 284, 339, 1469, 337, 5127, 341, 5456, 14980, 13], "temperature": 0.0, "avg_logprob": -0.15063233997510828, "compression_ratio": 1.5953488372093023, "no_speech_prob": 1.520407840871485e-05}, {"id": 932, "seek": 382756, "start": 3848.6, "end": 3851.2, "text": " So you can see here we start.", "tokens": [407, 291, 393, 536, 510, 321, 722, 13], "temperature": 0.0, "avg_logprob": -0.15063233997510828, "compression_ratio": 1.5953488372093023, "no_speech_prob": 1.520407840871485e-05}, {"id": 933, "seek": 385120, "start": 3851.2, "end": 3859.16, "text": " The input is 128 because that's the batch size, 128 by 3 by 128 by 128.", "tokens": [440, 4846, 307, 29810, 570, 300, 311, 264, 15245, 2744, 11, 29810, 538, 805, 538, 29810, 538, 29810, 13], "temperature": 0.0, "avg_logprob": -0.20377437884990984, "compression_ratio": 1.5638297872340425, "no_speech_prob": 7.182501121860696e-06}, {"id": 934, "seek": 385120, "start": 3859.16, "end": 3862.8399999999997, "text": " And then we gradually go through these convolutions.", "tokens": [400, 550, 321, 13145, 352, 807, 613, 3754, 15892, 13], "temperature": 0.0, "avg_logprob": -0.20377437884990984, "compression_ratio": 1.5638297872340425, "no_speech_prob": 7.182501121860696e-06}, {"id": 935, "seek": 385120, "start": 3862.8399999999997, "end": 3868.0, "text": " The first one has a stride of 1.", "tokens": [440, 700, 472, 575, 257, 1056, 482, 295, 502, 13], "temperature": 0.0, "avg_logprob": -0.20377437884990984, "compression_ratio": 1.5638297872340425, "no_speech_prob": 7.182501121860696e-06}, {"id": 936, "seek": 385120, "start": 3868.0, "end": 3869.6, "text": " The next two have a stride of 2.", "tokens": [440, 958, 732, 362, 257, 1056, 482, 295, 568, 13], "temperature": 0.0, "avg_logprob": -0.20377437884990984, "compression_ratio": 1.5638297872340425, "no_speech_prob": 7.182501121860696e-06}, {"id": 937, "seek": 385120, "start": 3869.6, "end": 3872.04, "text": " So that goes 64, 32.", "tokens": [407, 300, 1709, 12145, 11, 8858, 13], "temperature": 0.0, "avg_logprob": -0.20377437884990984, "compression_ratio": 1.5638297872340425, "no_speech_prob": 7.182501121860696e-06}, {"id": 938, "seek": 385120, "start": 3872.04, "end": 3876.9399999999996, "text": " And you can see after each one, they have a stride of 2, gets smaller and smaller.", "tokens": [400, 291, 393, 536, 934, 1184, 472, 11, 436, 362, 257, 1056, 482, 295, 568, 11, 2170, 4356, 293, 4356, 13], "temperature": 0.0, "avg_logprob": -0.20377437884990984, "compression_ratio": 1.5638297872340425, "no_speech_prob": 7.182501121860696e-06}, {"id": 939, "seek": 387694, "start": 3876.94, "end": 3882.4, "text": " And then an average pull that to a 1 by 1, and then we flatten it, and then we have a", "tokens": [400, 550, 364, 4274, 2235, 300, 281, 257, 502, 538, 502, 11, 293, 550, 321, 24183, 309, 11, 293, 550, 321, 362, 257], "temperature": 0.0, "avg_logprob": -0.24835339108028928, "compression_ratio": 1.5821596244131455, "no_speech_prob": 3.0412172691285377e-06}, {"id": 940, "seek": 387694, "start": 3882.4, "end": 3883.4, "text": " linear.", "tokens": [8213, 13], "temperature": 0.0, "avg_logprob": -0.24835339108028928, "compression_ratio": 1.5821596244131455, "no_speech_prob": 3.0412172691285377e-06}, {"id": 941, "seek": 387694, "start": 3883.4, "end": 3889.52, "text": " So that's, it's a really, it's like as basic a conv net as you could get.", "tokens": [407, 300, 311, 11, 309, 311, 257, 534, 11, 309, 311, 411, 382, 3875, 257, 3754, 2533, 382, 291, 727, 483, 13], "temperature": 0.0, "avg_logprob": -0.24835339108028928, "compression_ratio": 1.5821596244131455, "no_speech_prob": 3.0412172691285377e-06}, {"id": 942, "seek": 387694, "start": 3889.52, "end": 3890.52, "text": " It really is.", "tokens": [467, 534, 307, 13], "temperature": 0.0, "avg_logprob": -0.24835339108028928, "compression_ratio": 1.5821596244131455, "no_speech_prob": 3.0412172691285377e-06}, {"id": 943, "seek": 387694, "start": 3890.52, "end": 3894.52, "text": " It's just a bunch of 3 by 3 conv value batch norms.", "tokens": [467, 311, 445, 257, 3840, 295, 805, 538, 805, 3754, 2158, 15245, 24357, 13], "temperature": 0.0, "avg_logprob": -0.24835339108028928, "compression_ratio": 1.5821596244131455, "no_speech_prob": 3.0412172691285377e-06}, {"id": 944, "seek": 387694, "start": 3894.52, "end": 3897.04, "text": " But it does terrifically well.", "tokens": [583, 309, 775, 7245, 4278, 731, 13], "temperature": 0.0, "avg_logprob": -0.24835339108028928, "compression_ratio": 1.5821596244131455, "no_speech_prob": 3.0412172691285377e-06}, {"id": 945, "seek": 387694, "start": 3897.04, "end": 3900.8, "text": " You know, it's pretty, it's deep enough.", "tokens": [509, 458, 11, 309, 311, 1238, 11, 309, 311, 2452, 1547, 13], "temperature": 0.0, "avg_logprob": -0.24835339108028928, "compression_ratio": 1.5821596244131455, "no_speech_prob": 3.0412172691285377e-06}, {"id": 946, "seek": 387694, "start": 3900.8, "end": 3904.2400000000002, "text": " So I think that's a good start.", "tokens": [407, 286, 519, 300, 311, 257, 665, 722, 13], "temperature": 0.0, "avg_logprob": -0.24835339108028928, "compression_ratio": 1.5821596244131455, "no_speech_prob": 3.0412172691285377e-06}, {"id": 947, "seek": 390424, "start": 3904.24, "end": 3908.0, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.13165628342401414, "compression_ratio": 1.5695652173913044, "no_speech_prob": 2.642559365995112e-06}, {"id": 948, "seek": 390424, "start": 3908.0, "end": 3910.68, "text": " I think that's a good time to take a break.", "tokens": [286, 519, 300, 311, 257, 665, 565, 281, 747, 257, 1821, 13], "temperature": 0.0, "avg_logprob": -0.13165628342401414, "compression_ratio": 1.5695652173913044, "no_speech_prob": 2.642559365995112e-06}, {"id": 949, "seek": 390424, "start": 3910.68, "end": 3918.4799999999996, "text": " So let's come back at 7.45.", "tokens": [407, 718, 311, 808, 646, 412, 1614, 13, 8465, 13], "temperature": 0.0, "avg_logprob": -0.13165628342401414, "compression_ratio": 1.5695652173913044, "no_speech_prob": 2.642559365995112e-06}, {"id": 950, "seek": 390424, "start": 3918.4799999999996, "end": 3923.3599999999997, "text": " This is one of the bits I'm most excited about in this course, actually.", "tokens": [639, 307, 472, 295, 264, 9239, 286, 478, 881, 2919, 466, 294, 341, 1164, 11, 767, 13], "temperature": 0.0, "avg_logprob": -0.13165628342401414, "compression_ratio": 1.5695652173913044, "no_speech_prob": 2.642559365995112e-06}, {"id": 951, "seek": 390424, "start": 3923.3599999999997, "end": 3927.4399999999996, "text": " But hopefully it's gonna be like totally unexciting to you, because it's just gonna be so obvious", "tokens": [583, 4696, 309, 311, 799, 312, 411, 3879, 11572, 66, 1748, 281, 291, 11, 570, 309, 311, 445, 799, 312, 370, 6322], "temperature": 0.0, "avg_logprob": -0.13165628342401414, "compression_ratio": 1.5695652173913044, "no_speech_prob": 2.642559365995112e-06}, {"id": 952, "seek": 390424, "start": 3927.4399999999996, "end": 3928.9199999999996, "text": " that you should do it this way.", "tokens": [300, 291, 820, 360, 309, 341, 636, 13], "temperature": 0.0, "avg_logprob": -0.13165628342401414, "compression_ratio": 1.5695652173913044, "no_speech_prob": 2.642559365995112e-06}, {"id": 953, "seek": 390424, "start": 3928.9199999999996, "end": 3933.04, "text": " But the reason I'm excited is that we're gonna be talking about optimizers.", "tokens": [583, 264, 1778, 286, 478, 2919, 307, 300, 321, 434, 799, 312, 1417, 466, 5028, 22525, 13], "temperature": 0.0, "avg_logprob": -0.13165628342401414, "compression_ratio": 1.5695652173913044, "no_speech_prob": 2.642559365995112e-06}, {"id": 954, "seek": 393304, "start": 3933.04, "end": 3939.04, "text": " And anybody who's done work with kind of optimizers in deep learning in the past will know that", "tokens": [400, 4472, 567, 311, 1096, 589, 365, 733, 295, 5028, 22525, 294, 2452, 2539, 294, 264, 1791, 486, 458, 300], "temperature": 0.0, "avg_logprob": -0.17875565422905815, "compression_ratio": 1.6639004149377594, "no_speech_prob": 2.902181222452782e-06}, {"id": 955, "seek": 393304, "start": 3939.04, "end": 3943.6, "text": " every library treats every optimizer as a totally different thing.", "tokens": [633, 6405, 19566, 633, 5028, 6545, 382, 257, 3879, 819, 551, 13], "temperature": 0.0, "avg_logprob": -0.17875565422905815, "compression_ratio": 1.6639004149377594, "no_speech_prob": 2.902181222452782e-06}, {"id": 956, "seek": 393304, "start": 3943.6, "end": 3949.58, "text": " So there's an atom optimizer, like in PyTorch, there's an atom optimizer and a SGD optimizer", "tokens": [407, 456, 311, 364, 12018, 5028, 6545, 11, 411, 294, 9953, 51, 284, 339, 11, 456, 311, 364, 12018, 5028, 6545, 293, 257, 34520, 35, 5028, 6545], "temperature": 0.0, "avg_logprob": -0.17875565422905815, "compression_ratio": 1.6639004149377594, "no_speech_prob": 2.902181222452782e-06}, {"id": 957, "seek": 393304, "start": 3949.58, "end": 3952.6, "text": " and an RMS prop optimizer.", "tokens": [293, 364, 497, 10288, 2365, 5028, 6545, 13], "temperature": 0.0, "avg_logprob": -0.17875565422905815, "compression_ratio": 1.6639004149377594, "no_speech_prob": 2.902181222452782e-06}, {"id": 958, "seek": 393304, "start": 3952.6, "end": 3957.96, "text": " And somebody comes along and says, hey, we've invented this thing called decoupled weight", "tokens": [400, 2618, 1487, 2051, 293, 1619, 11, 4177, 11, 321, 600, 14479, 341, 551, 1219, 979, 263, 15551, 3364], "temperature": 0.0, "avg_logprob": -0.17875565422905815, "compression_ratio": 1.6639004149377594, "no_speech_prob": 2.902181222452782e-06}, {"id": 959, "seek": 393304, "start": 3957.96, "end": 3960.92, "text": " decay, also known as Adam W.", "tokens": [21039, 11, 611, 2570, 382, 7938, 343, 13], "temperature": 0.0, "avg_logprob": -0.17875565422905815, "compression_ratio": 1.6639004149377594, "no_speech_prob": 2.902181222452782e-06}, {"id": 960, "seek": 396092, "start": 3960.92, "end": 3964.44, "text": " And the PyTorch folks go, oh, damn, what are we gonna do?", "tokens": [400, 264, 9953, 51, 284, 339, 4024, 352, 11, 1954, 11, 8151, 11, 437, 366, 321, 799, 360, 30], "temperature": 0.0, "avg_logprob": -0.1218737132513701, "compression_ratio": 1.8327526132404182, "no_speech_prob": 8.397109013458248e-06}, {"id": 961, "seek": 396092, "start": 3964.44, "end": 3967.32, "text": " And they have to add a parameter to every one of their optimizers, and they have to", "tokens": [400, 436, 362, 281, 909, 257, 13075, 281, 633, 472, 295, 641, 5028, 22525, 11, 293, 436, 362, 281], "temperature": 0.0, "avg_logprob": -0.1218737132513701, "compression_ratio": 1.8327526132404182, "no_speech_prob": 8.397109013458248e-06}, {"id": 962, "seek": 396092, "start": 3967.32, "end": 3968.88, "text": " change every one of their optimizers.", "tokens": [1319, 633, 472, 295, 641, 5028, 22525, 13], "temperature": 0.0, "avg_logprob": -0.1218737132513701, "compression_ratio": 1.8327526132404182, "no_speech_prob": 8.397109013458248e-06}, {"id": 963, "seek": 396092, "start": 3968.88, "end": 3973.7200000000003, "text": " And then somebody else comes along and says, oh, we've invented a thing called AMS grad.", "tokens": [400, 550, 2618, 1646, 1487, 2051, 293, 1619, 11, 1954, 11, 321, 600, 14479, 257, 551, 1219, 6475, 50, 2771, 13], "temperature": 0.0, "avg_logprob": -0.1218737132513701, "compression_ratio": 1.8327526132404182, "no_speech_prob": 8.397109013458248e-06}, {"id": 964, "seek": 396092, "start": 3973.7200000000003, "end": 3976.28, "text": " There's another parameter we have to put into any one of those optimizers.", "tokens": [821, 311, 1071, 13075, 321, 362, 281, 829, 666, 604, 472, 295, 729, 5028, 22525, 13], "temperature": 0.0, "avg_logprob": -0.1218737132513701, "compression_ratio": 1.8327526132404182, "no_speech_prob": 8.397109013458248e-06}, {"id": 965, "seek": 396092, "start": 3976.28, "end": 3985.56, "text": " And it's not just inefficient and frustrating, but it holds back research, because it starts", "tokens": [400, 309, 311, 406, 445, 43495, 293, 16522, 11, 457, 309, 9190, 646, 2132, 11, 570, 309, 3719], "temperature": 0.0, "avg_logprob": -0.1218737132513701, "compression_ratio": 1.8327526132404182, "no_speech_prob": 8.397109013458248e-06}, {"id": 966, "seek": 396092, "start": 3985.56, "end": 3990.6800000000003, "text": " feeling like there are all these things called different kinds of optimizers, but there's", "tokens": [2633, 411, 456, 366, 439, 613, 721, 1219, 819, 3685, 295, 5028, 22525, 11, 457, 456, 311], "temperature": 0.0, "avg_logprob": -0.1218737132513701, "compression_ratio": 1.8327526132404182, "no_speech_prob": 8.397109013458248e-06}, {"id": 967, "seek": 399068, "start": 3990.68, "end": 3991.68, "text": " not.", "tokens": [406, 13], "temperature": 0.0, "avg_logprob": -0.09911522243333899, "compression_ratio": 1.7456896551724137, "no_speech_prob": 1.1125018318125512e-05}, {"id": 968, "seek": 399068, "start": 3991.68, "end": 3993.0, "text": " I'm gonna show you there's not.", "tokens": [286, 478, 799, 855, 291, 456, 311, 406, 13], "temperature": 0.0, "avg_logprob": -0.09911522243333899, "compression_ratio": 1.7456896551724137, "no_speech_prob": 1.1125018318125512e-05}, {"id": 969, "seek": 399068, "start": 3993.0, "end": 3995.72, "text": " There's one optimizer.", "tokens": [821, 311, 472, 5028, 6545, 13], "temperature": 0.0, "avg_logprob": -0.09911522243333899, "compression_ratio": 1.7456896551724137, "no_speech_prob": 1.1125018318125512e-05}, {"id": 970, "seek": 399068, "start": 3995.72, "end": 4000.72, "text": " And there's one optimizer in which you can inject different pieces of behavior in a very,", "tokens": [400, 456, 311, 472, 5028, 6545, 294, 597, 291, 393, 10711, 819, 3755, 295, 5223, 294, 257, 588, 11], "temperature": 0.0, "avg_logprob": -0.09911522243333899, "compression_ratio": 1.7456896551724137, "no_speech_prob": 1.1125018318125512e-05}, {"id": 971, "seek": 399068, "start": 4000.72, "end": 4002.6, "text": " very small number of ways.", "tokens": [588, 1359, 1230, 295, 2098, 13], "temperature": 0.0, "avg_logprob": -0.09911522243333899, "compression_ratio": 1.7456896551724137, "no_speech_prob": 1.1125018318125512e-05}, {"id": 972, "seek": 399068, "start": 4002.6, "end": 4006.8399999999997, "text": " And what we're gonna do is we're gonna start with this generic optimizer, and we're gonna", "tokens": [400, 437, 321, 434, 799, 360, 307, 321, 434, 799, 722, 365, 341, 19577, 5028, 6545, 11, 293, 321, 434, 799], "temperature": 0.0, "avg_logprob": -0.09911522243333899, "compression_ratio": 1.7456896551724137, "no_speech_prob": 1.1125018318125512e-05}, {"id": 973, "seek": 399068, "start": 4006.8399999999997, "end": 4009.44, "text": " end up with this.", "tokens": [917, 493, 365, 341, 13], "temperature": 0.0, "avg_logprob": -0.09911522243333899, "compression_ratio": 1.7456896551724137, "no_speech_prob": 1.1125018318125512e-05}, {"id": 974, "seek": 399068, "start": 4009.44, "end": 4015.2799999999997, "text": " This came out last week, and it's a massive improvement, as you see, in what we can do", "tokens": [639, 1361, 484, 1036, 1243, 11, 293, 309, 311, 257, 5994, 10444, 11, 382, 291, 536, 11, 294, 437, 321, 393, 360], "temperature": 0.0, "avg_logprob": -0.09911522243333899, "compression_ratio": 1.7456896551724137, "no_speech_prob": 1.1125018318125512e-05}, {"id": 975, "seek": 399068, "start": 4015.2799999999997, "end": 4019.3599999999997, "text": " with natural language processing.", "tokens": [365, 3303, 2856, 9007, 13], "temperature": 0.0, "avg_logprob": -0.09911522243333899, "compression_ratio": 1.7456896551724137, "no_speech_prob": 1.1125018318125512e-05}, {"id": 976, "seek": 401936, "start": 4019.36, "end": 4027.92, "text": " This is the equation set that we're gonna end up implementing from the paper.", "tokens": [639, 307, 264, 5367, 992, 300, 321, 434, 799, 917, 493, 18114, 490, 264, 3035, 13], "temperature": 0.0, "avg_logprob": -0.07501917415195042, "compression_ratio": 1.5722543352601157, "no_speech_prob": 5.862535545020364e-06}, {"id": 977, "seek": 401936, "start": 4027.92, "end": 4035.84, "text": " And what if I told you that not only I think are we the first library to have this implemented,", "tokens": [400, 437, 498, 286, 1907, 291, 300, 406, 787, 286, 519, 366, 321, 264, 700, 6405, 281, 362, 341, 12270, 11], "temperature": 0.0, "avg_logprob": -0.07501917415195042, "compression_ratio": 1.5722543352601157, "no_speech_prob": 5.862535545020364e-06}, {"id": 978, "seek": 401936, "start": 4035.84, "end": 4041.28, "text": " but this is the total amount of code that we're gonna write to do it.", "tokens": [457, 341, 307, 264, 3217, 2372, 295, 3089, 300, 321, 434, 799, 2464, 281, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.07501917415195042, "compression_ratio": 1.5722543352601157, "no_speech_prob": 5.862535545020364e-06}, {"id": 979, "seek": 401936, "start": 4041.28, "end": 4046.28, "text": " So that's where we're going.", "tokens": [407, 300, 311, 689, 321, 434, 516, 13], "temperature": 0.0, "avg_logprob": -0.07501917415195042, "compression_ratio": 1.5722543352601157, "no_speech_prob": 5.862535545020364e-06}, {"id": 980, "seek": 404628, "start": 4046.28, "end": 4051.1600000000003, "text": " So we're gonna continue with ImageNet, and we're gonna continue with the basic set of", "tokens": [407, 321, 434, 799, 2354, 365, 29903, 31890, 11, 293, 321, 434, 799, 2354, 365, 264, 3875, 992, 295], "temperature": 0.0, "avg_logprob": -0.11337483393681513, "compression_ratio": 1.6775956284153006, "no_speech_prob": 6.438625405280618e-06}, {"id": 981, "seek": 404628, "start": 4051.1600000000003, "end": 4058.8, "text": " transforms we had before, and the basic set of stuff to create our data bunch.", "tokens": [35592, 321, 632, 949, 11, 293, 264, 3875, 992, 295, 1507, 281, 1884, 527, 1412, 3840, 13], "temperature": 0.0, "avg_logprob": -0.11337483393681513, "compression_ratio": 1.6775956284153006, "no_speech_prob": 6.438625405280618e-06}, {"id": 982, "seek": 404628, "start": 4058.8, "end": 4066.1600000000003, "text": " This is our model, and this is something to pop it on CUDA to get our statistics written", "tokens": [639, 307, 527, 2316, 11, 293, 341, 307, 746, 281, 1665, 309, 322, 29777, 7509, 281, 483, 527, 12523, 3720], "temperature": 0.0, "avg_logprob": -0.11337483393681513, "compression_ratio": 1.6775956284153006, "no_speech_prob": 6.438625405280618e-06}, {"id": 983, "seek": 404628, "start": 4066.1600000000003, "end": 4070.0400000000004, "text": " out to do our batch transform with the normalization.", "tokens": [484, 281, 360, 527, 15245, 4088, 365, 264, 2710, 2144, 13], "temperature": 0.0, "avg_logprob": -0.11337483393681513, "compression_ratio": 1.6775956284153006, "no_speech_prob": 6.438625405280618e-06}, {"id": 984, "seek": 407004, "start": 4070.04, "end": 4076.68, "text": " And so we're gonna start here, 52% after an epoch.", "tokens": [400, 370, 321, 434, 799, 722, 510, 11, 18079, 4, 934, 364, 30992, 339, 13], "temperature": 0.0, "avg_logprob": -0.13173290661403111, "compression_ratio": 1.407035175879397, "no_speech_prob": 2.947953134935233e-06}, {"id": 985, "seek": 407004, "start": 4076.68, "end": 4079.84, "text": " And so let's try to create an optimizer.", "tokens": [400, 370, 718, 311, 853, 281, 1884, 364, 5028, 6545, 13], "temperature": 0.0, "avg_logprob": -0.13173290661403111, "compression_ratio": 1.407035175879397, "no_speech_prob": 2.947953134935233e-06}, {"id": 986, "seek": 407004, "start": 4079.84, "end": 4088.6, "text": " Now in PyTorch, the base thing called optimizer is just a dictionary that stores away some", "tokens": [823, 294, 9953, 51, 284, 339, 11, 264, 3096, 551, 1219, 5028, 6545, 307, 445, 257, 25890, 300, 9512, 1314, 512], "temperature": 0.0, "avg_logprob": -0.13173290661403111, "compression_ratio": 1.407035175879397, "no_speech_prob": 2.947953134935233e-06}, {"id": 987, "seek": 407004, "start": 4088.6, "end": 4093.24, "text": " hyperparameters, and we've actually already used it.", "tokens": [9848, 2181, 335, 6202, 11, 293, 321, 600, 767, 1217, 1143, 309, 13], "temperature": 0.0, "avg_logprob": -0.13173290661403111, "compression_ratio": 1.407035175879397, "no_speech_prob": 2.947953134935233e-06}, {"id": 988, "seek": 407004, "start": 4093.24, "end": 4095.96, "text": " And I deeply apologize for this.", "tokens": [400, 286, 8760, 12328, 337, 341, 13], "temperature": 0.0, "avg_logprob": -0.13173290661403111, "compression_ratio": 1.407035175879397, "no_speech_prob": 2.947953134935233e-06}, {"id": 989, "seek": 407004, "start": 4095.96, "end": 4097.72, "text": " We cheated.", "tokens": [492, 28079, 13], "temperature": 0.0, "avg_logprob": -0.13173290661403111, "compression_ratio": 1.407035175879397, "no_speech_prob": 2.947953134935233e-06}, {"id": 990, "seek": 409772, "start": 4097.72, "end": 4103.320000000001, "text": " We used something that is not part of our approved set of foundations without building", "tokens": [492, 1143, 746, 300, 307, 406, 644, 295, 527, 10826, 992, 295, 22467, 1553, 2390], "temperature": 0.0, "avg_logprob": -0.11299533110398513, "compression_ratio": 1.691304347826087, "no_speech_prob": 1.4144295619189506e-06}, {"id": 991, "seek": 409772, "start": 4103.320000000001, "end": 4109.72, "text": " it ourselves, and we did it here.", "tokens": [309, 4175, 11, 293, 321, 630, 309, 510, 13], "temperature": 0.0, "avg_logprob": -0.11299533110398513, "compression_ratio": 1.691304347826087, "no_speech_prob": 1.4144295619189506e-06}, {"id": 992, "seek": 409772, "start": 4109.72, "end": 4111.96, "text": " We never wrote param groups.", "tokens": [492, 1128, 4114, 6220, 3935, 13], "temperature": 0.0, "avg_logprob": -0.11299533110398513, "compression_ratio": 1.691304347826087, "no_speech_prob": 1.4144295619189506e-06}, {"id": 993, "seek": 409772, "start": 4111.96, "end": 4113.96, "text": " We never wrote param groups.", "tokens": [492, 1128, 4114, 6220, 3935, 13], "temperature": 0.0, "avg_logprob": -0.11299533110398513, "compression_ratio": 1.691304347826087, "no_speech_prob": 1.4144295619189506e-06}, {"id": 994, "seek": 409772, "start": 4113.96, "end": 4117.0, "text": " So we're gonna go back and do it now, right?", "tokens": [407, 321, 434, 799, 352, 646, 293, 360, 309, 586, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.11299533110398513, "compression_ratio": 1.691304347826087, "no_speech_prob": 1.4144295619189506e-06}, {"id": 995, "seek": 409772, "start": 4117.0, "end": 4122.280000000001, "text": " Because the reason we did this is because we were using Torch's optim.optimizer.", "tokens": [1436, 264, 1778, 321, 630, 341, 307, 570, 321, 645, 1228, 7160, 339, 311, 5028, 13, 5747, 332, 6545, 13], "temperature": 0.0, "avg_logprob": -0.11299533110398513, "compression_ratio": 1.691304347826087, "no_speech_prob": 1.4144295619189506e-06}, {"id": 996, "seek": 409772, "start": 4122.280000000001, "end": 4127.240000000001, "text": " We've already built the main part of that, which is the thing that multiplies by the", "tokens": [492, 600, 1217, 3094, 264, 2135, 644, 295, 300, 11, 597, 307, 264, 551, 300, 12788, 530, 538, 264], "temperature": 0.0, "avg_logprob": -0.11299533110398513, "compression_ratio": 1.691304347826087, "no_speech_prob": 1.4144295619189506e-06}, {"id": 997, "seek": 412724, "start": 4127.24, "end": 4132.5599999999995, "text": " learning rate and subtracts from the gradients.", "tokens": [2539, 3314, 293, 16390, 82, 490, 264, 2771, 2448, 13], "temperature": 0.0, "avg_logprob": -0.09870001064833775, "compression_ratio": 1.6181818181818182, "no_speech_prob": 8.267670636996627e-06}, {"id": 998, "seek": 412724, "start": 4132.5599999999995, "end": 4134.5199999999995, "text": " But we didn't build param groups.", "tokens": [583, 321, 994, 380, 1322, 6220, 3935, 13], "temperature": 0.0, "avg_logprob": -0.09870001064833775, "compression_ratio": 1.6181818181818182, "no_speech_prob": 8.267670636996627e-06}, {"id": 999, "seek": 412724, "start": 4134.5199999999995, "end": 4136.679999999999, "text": " So let's do it here.", "tokens": [407, 718, 311, 360, 309, 510, 13], "temperature": 0.0, "avg_logprob": -0.09870001064833775, "compression_ratio": 1.6181818181818182, "no_speech_prob": 8.267670636996627e-06}, {"id": 1000, "seek": 412724, "start": 4136.679999999999, "end": 4139.96, "text": " So here's what's gonna happen.", "tokens": [407, 510, 311, 437, 311, 799, 1051, 13], "temperature": 0.0, "avg_logprob": -0.09870001064833775, "compression_ratio": 1.6181818181818182, "no_speech_prob": 8.267670636996627e-06}, {"id": 1001, "seek": 412724, "start": 4139.96, "end": 4144.5199999999995, "text": " As always, we need something called zero grad, which is gonna go through some parameters", "tokens": [1018, 1009, 11, 321, 643, 746, 1219, 4018, 2771, 11, 597, 307, 799, 352, 807, 512, 9834], "temperature": 0.0, "avg_logprob": -0.09870001064833775, "compression_ratio": 1.6181818181818182, "no_speech_prob": 8.267670636996627e-06}, {"id": 1002, "seek": 412724, "start": 4144.5199999999995, "end": 4150.04, "text": " and zero them out and also remove any gradient computation history.", "tokens": [293, 4018, 552, 484, 293, 611, 4159, 604, 16235, 24903, 2503, 13], "temperature": 0.0, "avg_logprob": -0.09870001064833775, "compression_ratio": 1.6181818181818182, "no_speech_prob": 8.267670636996627e-06}, {"id": 1003, "seek": 412724, "start": 4150.04, "end": 4153.76, "text": " And we're gonna have a step function that does some kind of step.", "tokens": [400, 321, 434, 799, 362, 257, 1823, 2445, 300, 775, 512, 733, 295, 1823, 13], "temperature": 0.0, "avg_logprob": -0.09870001064833775, "compression_ratio": 1.6181818181818182, "no_speech_prob": 8.267670636996627e-06}, {"id": 1004, "seek": 415376, "start": 4153.76, "end": 4160.4800000000005, "text": " The main difference here, though, is our step function isn't actually gonna do anything.", "tokens": [440, 2135, 2649, 510, 11, 1673, 11, 307, 527, 1823, 2445, 1943, 380, 767, 799, 360, 1340, 13], "temperature": 0.0, "avg_logprob": -0.10836485243335213, "compression_ratio": 1.628691983122363, "no_speech_prob": 1.191100295727665e-06}, {"id": 1005, "seek": 415376, "start": 4160.4800000000005, "end": 4166.4800000000005, "text": " It's going to use composition on some things that we pass on and ask them to do something.", "tokens": [467, 311, 516, 281, 764, 12686, 322, 512, 721, 300, 321, 1320, 322, 293, 1029, 552, 281, 360, 746, 13], "temperature": 0.0, "avg_logprob": -0.10836485243335213, "compression_ratio": 1.628691983122363, "no_speech_prob": 1.191100295727665e-06}, {"id": 1006, "seek": 415376, "start": 4166.4800000000005, "end": 4173.400000000001, "text": " So this optimizer's gonna do nothing at all until we build on top of it.", "tokens": [407, 341, 5028, 6545, 311, 799, 360, 1825, 412, 439, 1826, 321, 1322, 322, 1192, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.10836485243335213, "compression_ratio": 1.628691983122363, "no_speech_prob": 1.191100295727665e-06}, {"id": 1007, "seek": 415376, "start": 4173.400000000001, "end": 4179.320000000001, "text": " But we're gonna set it up to be able to handle things like discriminative learning rates and", "tokens": [583, 321, 434, 799, 992, 309, 493, 281, 312, 1075, 281, 4813, 721, 411, 20828, 1166, 2539, 6846, 293], "temperature": 0.0, "avg_logprob": -0.10836485243335213, "compression_ratio": 1.628691983122363, "no_speech_prob": 1.191100295727665e-06}, {"id": 1008, "seek": 415376, "start": 4179.320000000001, "end": 4182.320000000001, "text": " one-cycle annealing and stuff like that.", "tokens": [472, 12, 14796, 22256, 4270, 293, 1507, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.10836485243335213, "compression_ratio": 1.628691983122363, "no_speech_prob": 1.191100295727665e-06}, {"id": 1009, "seek": 418232, "start": 4182.32, "end": 4188.2, "text": " And so to be able to do that, we need some way to create parameter groups.", "tokens": [400, 370, 281, 312, 1075, 281, 360, 300, 11, 321, 643, 512, 636, 281, 1884, 13075, 3935, 13], "temperature": 0.0, "avg_logprob": -0.15179432233174642, "compression_ratio": 1.858974358974359, "no_speech_prob": 1.4285186807683203e-05}, {"id": 1010, "seek": 418232, "start": 4188.2, "end": 4192.88, "text": " This is what we call in Fast AI layer groups.", "tokens": [639, 307, 437, 321, 818, 294, 15968, 7318, 4583, 3935, 13], "temperature": 0.0, "avg_logprob": -0.15179432233174642, "compression_ratio": 1.858974358974359, "no_speech_prob": 1.4285186807683203e-05}, {"id": 1011, "seek": 418232, "start": 4192.88, "end": 4196.44, "text": " And I kind of wish I hadn't called them layer groups.", "tokens": [400, 286, 733, 295, 3172, 286, 8782, 380, 1219, 552, 4583, 3935, 13], "temperature": 0.0, "avg_logprob": -0.15179432233174642, "compression_ratio": 1.858974358974359, "no_speech_prob": 1.4285186807683203e-05}, {"id": 1012, "seek": 418232, "start": 4196.44, "end": 4200.36, "text": " I should call them parameter groups because we have a perfectly good name for them already", "tokens": [286, 820, 818, 552, 13075, 3935, 570, 321, 362, 257, 6239, 665, 1315, 337, 552, 1217], "temperature": 0.0, "avg_logprob": -0.15179432233174642, "compression_ratio": 1.858974358974359, "no_speech_prob": 1.4285186807683203e-05}, {"id": 1013, "seek": 418232, "start": 4200.36, "end": 4201.759999999999, "text": " in PyTorch.", "tokens": [294, 9953, 51, 284, 339, 13], "temperature": 0.0, "avg_logprob": -0.15179432233174642, "compression_ratio": 1.858974358974359, "no_speech_prob": 1.4285186807683203e-05}, {"id": 1014, "seek": 418232, "start": 4201.759999999999, "end": 4203.84, "text": " So I'm not gonna call them layer groups anymore.", "tokens": [407, 286, 478, 406, 799, 818, 552, 4583, 3935, 3602, 13], "temperature": 0.0, "avg_logprob": -0.15179432233174642, "compression_ratio": 1.858974358974359, "no_speech_prob": 1.4285186807683203e-05}, {"id": 1015, "seek": 418232, "start": 4203.84, "end": 4206.32, "text": " I'm just gonna call them parameter groups.", "tokens": [286, 478, 445, 799, 818, 552, 13075, 3935, 13], "temperature": 0.0, "avg_logprob": -0.15179432233174642, "compression_ratio": 1.858974358974359, "no_speech_prob": 1.4285186807683203e-05}, {"id": 1016, "seek": 418232, "start": 4206.32, "end": 4207.32, "text": " But it's the same thing, okay?", "tokens": [583, 309, 311, 264, 912, 551, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.15179432233174642, "compression_ratio": 1.858974358974359, "no_speech_prob": 1.4285186807683203e-05}, {"id": 1017, "seek": 418232, "start": 4207.32, "end": 4208.96, "text": " Parameter groups and layer groups.", "tokens": [34882, 2398, 3935, 293, 4583, 3935, 13], "temperature": 0.0, "avg_logprob": -0.15179432233174642, "compression_ratio": 1.858974358974359, "no_speech_prob": 1.4285186807683203e-05}, {"id": 1018, "seek": 420896, "start": 4208.96, "end": 4216.64, "text": " So a parameter group, so remember what is in, when we say parameters in PyTorch, remember", "tokens": [407, 257, 13075, 1594, 11, 370, 1604, 437, 307, 294, 11, 562, 321, 584, 9834, 294, 9953, 51, 284, 339, 11, 1604], "temperature": 0.0, "avg_logprob": -0.1531263766902508, "compression_ratio": 1.7122641509433962, "no_speech_prob": 2.9479847398761194e-06}, {"id": 1019, "seek": 420896, "start": 4216.64, "end": 4220.72, "text": " right back to when we've created our first linear layer, we had a weight tensor and we", "tokens": [558, 646, 281, 562, 321, 600, 2942, 527, 700, 8213, 4583, 11, 321, 632, 257, 3364, 40863, 293, 321], "temperature": 0.0, "avg_logprob": -0.1531263766902508, "compression_ratio": 1.7122641509433962, "no_speech_prob": 2.9479847398761194e-06}, {"id": 1020, "seek": 420896, "start": 4220.72, "end": 4222.44, "text": " had a bias tensor.", "tokens": [632, 257, 12577, 40863, 13], "temperature": 0.0, "avg_logprob": -0.1531263766902508, "compression_ratio": 1.7122641509433962, "no_speech_prob": 2.9479847398761194e-06}, {"id": 1021, "seek": 420896, "start": 4222.44, "end": 4224.8, "text": " And each one of those is a parameter, right?", "tokens": [400, 1184, 472, 295, 729, 307, 257, 13075, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1531263766902508, "compression_ratio": 1.7122641509433962, "no_speech_prob": 2.9479847398761194e-06}, {"id": 1022, "seek": 420896, "start": 4224.8, "end": 4227.74, "text": " It's a parameter tensor.", "tokens": [467, 311, 257, 13075, 40863, 13], "temperature": 0.0, "avg_logprob": -0.1531263766902508, "compression_ratio": 1.7122641509433962, "no_speech_prob": 2.9479847398761194e-06}, {"id": 1023, "seek": 420896, "start": 4227.74, "end": 4232.0, "text": " So in order to optimize something, we need to know what all the parameter tensors are", "tokens": [407, 294, 1668, 281, 19719, 746, 11, 321, 643, 281, 458, 437, 439, 264, 13075, 10688, 830, 366], "temperature": 0.0, "avg_logprob": -0.1531263766902508, "compression_ratio": 1.7122641509433962, "no_speech_prob": 2.9479847398761194e-06}, {"id": 1024, "seek": 420896, "start": 4232.0, "end": 4233.0, "text": " in a model.", "tokens": [294, 257, 2316, 13], "temperature": 0.0, "avg_logprob": -0.1531263766902508, "compression_ratio": 1.7122641509433962, "no_speech_prob": 2.9479847398761194e-06}, {"id": 1025, "seek": 423300, "start": 4233.0, "end": 4240.0, "text": " And you can just say model.parameters to grab them all in PyTorch.", "tokens": [400, 291, 393, 445, 584, 2316, 13, 2181, 335, 6202, 281, 4444, 552, 439, 294, 9953, 51, 284, 339, 13], "temperature": 0.0, "avg_logprob": -0.14474899503919814, "compression_ratio": 1.681564245810056, "no_speech_prob": 2.123352942362544e-06}, {"id": 1026, "seek": 423300, "start": 4240.0, "end": 4245.44, "text": " And that's gonna give us, it gives us a generator, but as soon as you call list on a generator,", "tokens": [400, 300, 311, 799, 976, 505, 11, 309, 2709, 505, 257, 19265, 11, 457, 382, 2321, 382, 291, 818, 1329, 322, 257, 19265, 11], "temperature": 0.0, "avg_logprob": -0.14474899503919814, "compression_ratio": 1.681564245810056, "no_speech_prob": 2.123352942362544e-06}, {"id": 1027, "seek": 423300, "start": 4245.44, "end": 4246.84, "text": " it turns it into an actual list.", "tokens": [309, 4523, 309, 666, 364, 3539, 1329, 13], "temperature": 0.0, "avg_logprob": -0.14474899503919814, "compression_ratio": 1.681564245810056, "no_speech_prob": 2.123352942362544e-06}, {"id": 1028, "seek": 423300, "start": 4246.84, "end": 4253.4, "text": " So that's gonna give us a list of all of the tensors, all of the weights and all of the", "tokens": [407, 300, 311, 799, 976, 505, 257, 1329, 295, 439, 295, 264, 10688, 830, 11, 439, 295, 264, 17443, 293, 439, 295, 264], "temperature": 0.0, "avg_logprob": -0.14474899503919814, "compression_ratio": 1.681564245810056, "no_speech_prob": 2.123352942362544e-06}, {"id": 1029, "seek": 423300, "start": 4253.4, "end": 4258.88, "text": " biases basically.", "tokens": [32152, 1936, 13], "temperature": 0.0, "avg_logprob": -0.14474899503919814, "compression_ratio": 1.681564245810056, "no_speech_prob": 2.123352942362544e-06}, {"id": 1030, "seek": 425888, "start": 4258.88, "end": 4266.6, "text": " But we might wanna be able to say, the last two layers should have a different learning", "tokens": [583, 321, 1062, 1948, 312, 1075, 281, 584, 11, 264, 1036, 732, 7914, 820, 362, 257, 819, 2539], "temperature": 0.0, "avg_logprob": -0.09513260920842488, "compression_ratio": 1.7571428571428571, "no_speech_prob": 9.818045327847358e-06}, {"id": 1031, "seek": 425888, "start": 4266.6, "end": 4269.04, "text": " rate to all the other layers.", "tokens": [3314, 281, 439, 264, 661, 7914, 13], "temperature": 0.0, "avg_logprob": -0.09513260920842488, "compression_ratio": 1.7571428571428571, "no_speech_prob": 9.818045327847358e-06}, {"id": 1032, "seek": 425888, "start": 4269.04, "end": 4274.12, "text": " And so the way we can do that is rather than just passing in a list of parameters, we'll", "tokens": [400, 370, 264, 636, 321, 393, 360, 300, 307, 2831, 813, 445, 8437, 294, 257, 1329, 295, 9834, 11, 321, 603], "temperature": 0.0, "avg_logprob": -0.09513260920842488, "compression_ratio": 1.7571428571428571, "no_speech_prob": 9.818045327847358e-06}, {"id": 1033, "seek": 425888, "start": 4274.12, "end": 4276.6, "text": " pass in a list of lists.", "tokens": [1320, 294, 257, 1329, 295, 14511, 13], "temperature": 0.0, "avg_logprob": -0.09513260920842488, "compression_ratio": 1.7571428571428571, "no_speech_prob": 9.818045327847358e-06}, {"id": 1034, "seek": 425888, "start": 4276.6, "end": 4279.36, "text": " And so let's say our list of lists has two items.", "tokens": [400, 370, 718, 311, 584, 527, 1329, 295, 14511, 575, 732, 4754, 13], "temperature": 0.0, "avg_logprob": -0.09513260920842488, "compression_ratio": 1.7571428571428571, "no_speech_prob": 9.818045327847358e-06}, {"id": 1035, "seek": 425888, "start": 4279.36, "end": 4285.56, "text": " The first item contains all the parameters in the main body of the architecture and the", "tokens": [440, 700, 3174, 8306, 439, 264, 9834, 294, 264, 2135, 1772, 295, 264, 9482, 293, 264], "temperature": 0.0, "avg_logprob": -0.09513260920842488, "compression_ratio": 1.7571428571428571, "no_speech_prob": 9.818045327847358e-06}, {"id": 1036, "seek": 428556, "start": 4285.56, "end": 4290.240000000001, "text": " last item contains just the parameters from the last two layers.", "tokens": [1036, 3174, 8306, 445, 264, 9834, 490, 264, 1036, 732, 7914, 13], "temperature": 0.0, "avg_logprob": -0.09611132565666647, "compression_ratio": 1.7191489361702128, "no_speech_prob": 5.0147004913014825e-06}, {"id": 1037, "seek": 428556, "start": 4290.240000000001, "end": 4298.240000000001, "text": " So if we make this, decide that this is a list of lists, then that lets us do parameter", "tokens": [407, 498, 321, 652, 341, 11, 4536, 300, 341, 307, 257, 1329, 295, 14511, 11, 550, 300, 6653, 505, 360, 13075], "temperature": 0.0, "avg_logprob": -0.09611132565666647, "compression_ratio": 1.7191489361702128, "no_speech_prob": 5.0147004913014825e-06}, {"id": 1038, "seek": 428556, "start": 4298.240000000001, "end": 4299.240000000001, "text": " groups.", "tokens": [3935, 13], "temperature": 0.0, "avg_logprob": -0.09611132565666647, "compression_ratio": 1.7191489361702128, "no_speech_prob": 5.0147004913014825e-06}, {"id": 1039, "seek": 428556, "start": 4299.240000000001, "end": 4304.8, "text": " Now that's how we tell the optimizer these sets of parameters should be handled differently", "tokens": [823, 300, 311, 577, 321, 980, 264, 5028, 6545, 613, 6352, 295, 9834, 820, 312, 18033, 7614], "temperature": 0.0, "avg_logprob": -0.09611132565666647, "compression_ratio": 1.7191489361702128, "no_speech_prob": 5.0147004913014825e-06}, {"id": 1040, "seek": 428556, "start": 4304.8, "end": 4308.0, "text": " with discriminative learning rates and stuff.", "tokens": [365, 20828, 1166, 2539, 6846, 293, 1507, 13], "temperature": 0.0, "avg_logprob": -0.09611132565666647, "compression_ratio": 1.7191489361702128, "no_speech_prob": 5.0147004913014825e-06}, {"id": 1041, "seek": 428556, "start": 4308.0, "end": 4309.0, "text": " And so that's what we're gonna do.", "tokens": [400, 370, 300, 311, 437, 321, 434, 799, 360, 13], "temperature": 0.0, "avg_logprob": -0.09611132565666647, "compression_ratio": 1.7191489361702128, "no_speech_prob": 5.0147004913014825e-06}, {"id": 1042, "seek": 428556, "start": 4309.0, "end": 4314.120000000001, "text": " We're gonna assume that this thing being passed in is a list of lists.", "tokens": [492, 434, 799, 6552, 300, 341, 551, 885, 4678, 294, 307, 257, 1329, 295, 14511, 13], "temperature": 0.0, "avg_logprob": -0.09611132565666647, "compression_ratio": 1.7191489361702128, "no_speech_prob": 5.0147004913014825e-06}, {"id": 1043, "seek": 431412, "start": 4314.12, "end": 4316.599999999999, "text": " But we won't quite assume, we'll check.", "tokens": [583, 321, 1582, 380, 1596, 6552, 11, 321, 603, 1520, 13], "temperature": 0.0, "avg_logprob": -0.12259224864924065, "compression_ratio": 1.7045454545454546, "no_speech_prob": 6.962209226912819e-06}, {"id": 1044, "seek": 431412, "start": 4316.599999999999, "end": 4322.599999999999, "text": " If it's not, then we'll turn it into a list of lists by just wrapping it in a list.", "tokens": [759, 309, 311, 406, 11, 550, 321, 603, 1261, 309, 666, 257, 1329, 295, 14511, 538, 445, 21993, 309, 294, 257, 1329, 13], "temperature": 0.0, "avg_logprob": -0.12259224864924065, "compression_ratio": 1.7045454545454546, "no_speech_prob": 6.962209226912819e-06}, {"id": 1045, "seek": 431412, "start": 4322.599999999999, "end": 4327.5199999999995, "text": " So if it only has one thing in it, we'll just make it a list with one item containing a", "tokens": [407, 498, 309, 787, 575, 472, 551, 294, 309, 11, 321, 603, 445, 652, 309, 257, 1329, 365, 472, 3174, 19273, 257], "temperature": 0.0, "avg_logprob": -0.12259224864924065, "compression_ratio": 1.7045454545454546, "no_speech_prob": 6.962209226912819e-06}, {"id": 1046, "seek": 431412, "start": 4327.5199999999995, "end": 4329.16, "text": " list.", "tokens": [1329, 13], "temperature": 0.0, "avg_logprob": -0.12259224864924065, "compression_ratio": 1.7045454545454546, "no_speech_prob": 6.962209226912819e-06}, {"id": 1047, "seek": 431412, "start": 4329.16, "end": 4335.92, "text": " So now, param groups is a list of lists of parameter tensors.", "tokens": [407, 586, 11, 6220, 3935, 307, 257, 1329, 295, 14511, 295, 13075, 10688, 830, 13], "temperature": 0.0, "avg_logprob": -0.12259224864924065, "compression_ratio": 1.7045454545454546, "no_speech_prob": 6.962209226912819e-06}, {"id": 1048, "seek": 431412, "start": 4335.92, "end": 4340.88, "text": " And so you could either pass in, so you could decide how you wanna split them up into different", "tokens": [400, 370, 291, 727, 2139, 1320, 294, 11, 370, 291, 727, 4536, 577, 291, 1948, 7472, 552, 493, 666, 819], "temperature": 0.0, "avg_logprob": -0.12259224864924065, "compression_ratio": 1.7045454545454546, "no_speech_prob": 6.962209226912819e-06}, {"id": 1049, "seek": 434088, "start": 4340.88, "end": 4348.32, "text": " parameter groups, or you could just have them turn into a single parameter group for you.", "tokens": [13075, 3935, 11, 420, 291, 727, 445, 362, 552, 1261, 666, 257, 2167, 13075, 1594, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.16372886233859593, "compression_ratio": 1.7079207920792079, "no_speech_prob": 1.147862622019602e-05}, {"id": 1050, "seek": 434088, "start": 4348.32, "end": 4350.16, "text": " So that's the first thing we need.", "tokens": [407, 300, 311, 264, 700, 551, 321, 643, 13], "temperature": 0.0, "avg_logprob": -0.16372886233859593, "compression_ratio": 1.7079207920792079, "no_speech_prob": 1.147862622019602e-05}, {"id": 1051, "seek": 434088, "start": 4350.16, "end": 4357.92, "text": " So now we have, our optimizer object has a param groups attribute containing our parameter", "tokens": [407, 586, 321, 362, 11, 527, 5028, 6545, 2657, 575, 257, 6220, 3935, 19667, 19273, 527, 13075], "temperature": 0.0, "avg_logprob": -0.16372886233859593, "compression_ratio": 1.7079207920792079, "no_speech_prob": 1.147862622019602e-05}, {"id": 1052, "seek": 434088, "start": 4357.92, "end": 4358.92, "text": " groups.", "tokens": [3935, 13], "temperature": 0.0, "avg_logprob": -0.16372886233859593, "compression_ratio": 1.7079207920792079, "no_speech_prob": 1.147862622019602e-05}, {"id": 1053, "seek": 434088, "start": 4358.92, "end": 4360.88, "text": " So just keep remembering that's a list of lists.", "tokens": [407, 445, 1066, 20719, 300, 311, 257, 1329, 295, 14511, 13], "temperature": 0.0, "avg_logprob": -0.16372886233859593, "compression_ratio": 1.7079207920792079, "no_speech_prob": 1.147862622019602e-05}, {"id": 1054, "seek": 434088, "start": 4360.88, "end": 4362.12, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.16372886233859593, "compression_ratio": 1.7079207920792079, "no_speech_prob": 1.147862622019602e-05}, {"id": 1055, "seek": 434088, "start": 4362.12, "end": 4367.6, "text": " Each parameter group can have its own set of hyperparameters.", "tokens": [6947, 13075, 1594, 393, 362, 1080, 1065, 992, 295, 9848, 2181, 335, 6202, 13], "temperature": 0.0, "avg_logprob": -0.16372886233859593, "compression_ratio": 1.7079207920792079, "no_speech_prob": 1.147862622019602e-05}, {"id": 1056, "seek": 436760, "start": 4367.6, "end": 4374.88, "text": " So hyperparameters could be learning rate, momentum, beta in atom, epsilon in atom, and", "tokens": [407, 9848, 2181, 335, 6202, 727, 312, 2539, 3314, 11, 11244, 11, 9861, 294, 12018, 11, 17889, 294, 12018, 11, 293], "temperature": 0.0, "avg_logprob": -0.11128471252766062, "compression_ratio": 1.9005524861878453, "no_speech_prob": 3.50082177646982e-06}, {"id": 1057, "seek": 436760, "start": 4374.88, "end": 4376.360000000001, "text": " so forth.", "tokens": [370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.11128471252766062, "compression_ratio": 1.9005524861878453, "no_speech_prob": 3.50082177646982e-06}, {"id": 1058, "seek": 436760, "start": 4376.360000000001, "end": 4380.6, "text": " So those hyperparameters are gonna be stored as a dictionary.", "tokens": [407, 729, 9848, 2181, 335, 6202, 366, 799, 312, 12187, 382, 257, 25890, 13], "temperature": 0.0, "avg_logprob": -0.11128471252766062, "compression_ratio": 1.9005524861878453, "no_speech_prob": 3.50082177646982e-06}, {"id": 1059, "seek": 436760, "start": 4380.6, "end": 4384.92, "text": " And so there's gonna be one dictionary for each parameter group.", "tokens": [400, 370, 456, 311, 799, 312, 472, 25890, 337, 1184, 13075, 1594, 13], "temperature": 0.0, "avg_logprob": -0.11128471252766062, "compression_ratio": 1.9005524861878453, "no_speech_prob": 3.50082177646982e-06}, {"id": 1060, "seek": 436760, "start": 4384.92, "end": 4387.320000000001, "text": " So here's where we create it.", "tokens": [407, 510, 311, 689, 321, 1884, 309, 13], "temperature": 0.0, "avg_logprob": -0.11128471252766062, "compression_ratio": 1.9005524861878453, "no_speech_prob": 3.50082177646982e-06}, {"id": 1061, "seek": 436760, "start": 4387.320000000001, "end": 4394.56, "text": " Self.hypers contains for each parameter group a dictionary.", "tokens": [16348, 13, 3495, 21819, 8306, 337, 1184, 13075, 1594, 257, 25890, 13], "temperature": 0.0, "avg_logprob": -0.11128471252766062, "compression_ratio": 1.9005524861878453, "no_speech_prob": 3.50082177646982e-06}, {"id": 1062, "seek": 436760, "start": 4394.56, "end": 4396.4400000000005, "text": " And what's in the dictionary?", "tokens": [400, 437, 311, 294, 264, 25890, 30], "temperature": 0.0, "avg_logprob": -0.11128471252766062, "compression_ratio": 1.9005524861878453, "no_speech_prob": 3.50082177646982e-06}, {"id": 1063, "seek": 439644, "start": 4396.44, "end": 4400.16, "text": " What's in the dictionary is whatever you pass to the constructor.", "tokens": [708, 311, 294, 264, 25890, 307, 2035, 291, 1320, 281, 264, 47479, 13], "temperature": 0.0, "avg_logprob": -0.15376411971225534, "compression_ratio": 1.736842105263158, "no_speech_prob": 8.397911187785212e-06}, {"id": 1064, "seek": 439644, "start": 4400.16, "end": 4401.16, "text": " Okay?", "tokens": [1033, 30], "temperature": 0.0, "avg_logprob": -0.15376411971225534, "compression_ratio": 1.736842105263158, "no_speech_prob": 8.397911187785212e-06}, {"id": 1065, "seek": 439644, "start": 4401.16, "end": 4406.48, "text": " So this is how you just pass a single bunch of keyword arguments to the constructor, and", "tokens": [407, 341, 307, 577, 291, 445, 1320, 257, 2167, 3840, 295, 20428, 12869, 281, 264, 47479, 11, 293], "temperature": 0.0, "avg_logprob": -0.15376411971225534, "compression_ratio": 1.736842105263158, "no_speech_prob": 8.397911187785212e-06}, {"id": 1066, "seek": 439644, "start": 4406.48, "end": 4409.16, "text": " it's gonna construct a dictionary for every one.", "tokens": [309, 311, 799, 7690, 257, 25890, 337, 633, 472, 13], "temperature": 0.0, "avg_logprob": -0.15376411971225534, "compression_ratio": 1.736842105263158, "no_speech_prob": 8.397911187785212e-06}, {"id": 1067, "seek": 439644, "start": 4409.16, "end": 4414.4, "text": " And this is just a way of cloning a dictionary so that they're not all referring to the same", "tokens": [400, 341, 307, 445, 257, 636, 295, 596, 16638, 257, 25890, 370, 300, 436, 434, 406, 439, 13761, 281, 264, 912], "temperature": 0.0, "avg_logprob": -0.15376411971225534, "compression_ratio": 1.736842105263158, "no_speech_prob": 8.397911187785212e-06}, {"id": 1068, "seek": 439644, "start": 4414.4, "end": 4417.2, "text": " reference, but they all have their own reference.", "tokens": [6408, 11, 457, 436, 439, 362, 641, 1065, 6408, 13], "temperature": 0.0, "avg_logprob": -0.15376411971225534, "compression_ratio": 1.736842105263158, "no_speech_prob": 8.397911187785212e-06}, {"id": 1069, "seek": 439644, "start": 4417.2, "end": 4419.28, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.15376411971225534, "compression_ratio": 1.736842105263158, "no_speech_prob": 8.397911187785212e-06}, {"id": 1070, "seek": 441928, "start": 4419.28, "end": 4429.4, "text": " So that's doing much the same stuff as torches.optim.optimizer, and here's the new bit, stepper.", "tokens": [407, 300, 311, 884, 709, 264, 912, 1507, 382, 3930, 3781, 13, 5747, 332, 13, 5747, 332, 6545, 11, 293, 510, 311, 264, 777, 857, 11, 2126, 3717, 13], "temperature": 0.0, "avg_logprob": -0.18585358833780094, "compression_ratio": 1.5465116279069768, "no_speech_prob": 4.42541704614996e-06}, {"id": 1071, "seek": 441928, "start": 4429.4, "end": 4431.639999999999, "text": " In order to see what a stepper is, let's write one.", "tokens": [682, 1668, 281, 536, 437, 257, 2126, 3717, 307, 11, 718, 311, 2464, 472, 13], "temperature": 0.0, "avg_logprob": -0.18585358833780094, "compression_ratio": 1.5465116279069768, "no_speech_prob": 4.42541704614996e-06}, {"id": 1072, "seek": 441928, "start": 4431.639999999999, "end": 4433.679999999999, "text": " Here's a stepper.", "tokens": [1692, 311, 257, 2126, 3717, 13], "temperature": 0.0, "avg_logprob": -0.18585358833780094, "compression_ratio": 1.5465116279069768, "no_speech_prob": 4.42541704614996e-06}, {"id": 1073, "seek": 441928, "start": 4433.679999999999, "end": 4435.259999999999, "text": " It's a function.", "tokens": [467, 311, 257, 2445, 13], "temperature": 0.0, "avg_logprob": -0.18585358833780094, "compression_ratio": 1.5465116279069768, "no_speech_prob": 4.42541704614996e-06}, {"id": 1074, "seek": 441928, "start": 4435.259999999999, "end": 4437.16, "text": " It's called SGD step.", "tokens": [467, 311, 1219, 34520, 35, 1823, 13], "temperature": 0.0, "avg_logprob": -0.18585358833780094, "compression_ratio": 1.5465116279069768, "no_speech_prob": 4.42541704614996e-06}, {"id": 1075, "seek": 441928, "start": 4437.16, "end": 4438.759999999999, "text": " What does it do?", "tokens": [708, 775, 309, 360, 30], "temperature": 0.0, "avg_logprob": -0.18585358833780094, "compression_ratio": 1.5465116279069768, "no_speech_prob": 4.42541704614996e-06}, {"id": 1076, "seek": 441928, "start": 4438.759999999999, "end": 4440.24, "text": " It does the SGD step.", "tokens": [467, 775, 264, 34520, 35, 1823, 13], "temperature": 0.0, "avg_logprob": -0.18585358833780094, "compression_ratio": 1.5465116279069768, "no_speech_prob": 4.42541704614996e-06}, {"id": 1077, "seek": 441928, "start": 4440.24, "end": 4443.5599999999995, "text": " We've seen it before.", "tokens": [492, 600, 1612, 309, 949, 13], "temperature": 0.0, "avg_logprob": -0.18585358833780094, "compression_ratio": 1.5465116279069768, "no_speech_prob": 4.42541704614996e-06}, {"id": 1078, "seek": 444356, "start": 4443.56, "end": 4451.96, "text": " So in other words, to create an SGD optimizer, we create a partial with our optimizer with", "tokens": [407, 294, 661, 2283, 11, 281, 1884, 364, 34520, 35, 5028, 6545, 11, 321, 1884, 257, 14641, 365, 527, 5028, 6545, 365], "temperature": 0.0, "avg_logprob": -0.09148233884001432, "compression_ratio": 1.612121212121212, "no_speech_prob": 8.714284831512487e-07}, {"id": 1079, "seek": 444356, "start": 4451.96, "end": 4455.280000000001, "text": " the steppers being SGD step.", "tokens": [264, 2126, 15226, 885, 34520, 35, 1823, 13], "temperature": 0.0, "avg_logprob": -0.09148233884001432, "compression_ratio": 1.612121212121212, "no_speech_prob": 8.714284831512487e-07}, {"id": 1080, "seek": 444356, "start": 4455.280000000001, "end": 4463.360000000001, "text": " So now when we call step, it goes through our parameters, composes together our steppers,", "tokens": [407, 586, 562, 321, 818, 1823, 11, 309, 1709, 807, 527, 9834, 11, 715, 4201, 1214, 527, 2126, 15226, 11], "temperature": 0.0, "avg_logprob": -0.09148233884001432, "compression_ratio": 1.612121212121212, "no_speech_prob": 8.714284831512487e-07}, {"id": 1081, "seek": 444356, "start": 4463.360000000001, "end": 4469.080000000001, "text": " which is just one thing, right, and calls the parameter.", "tokens": [597, 307, 445, 472, 551, 11, 558, 11, 293, 5498, 264, 13075, 13], "temperature": 0.0, "avg_logprob": -0.09148233884001432, "compression_ratio": 1.612121212121212, "no_speech_prob": 8.714284831512487e-07}, {"id": 1082, "seek": 446908, "start": 4469.08, "end": 4475.32, "text": " So the parameter is gonna go p.data.add minus learning rate p.grad.data.", "tokens": [407, 264, 13075, 307, 799, 352, 280, 13, 67, 3274, 13, 25224, 3175, 2539, 3314, 280, 13, 7165, 13, 67, 3274, 13], "temperature": 0.0, "avg_logprob": -0.16401279540289015, "compression_ratio": 1.5210526315789474, "no_speech_prob": 3.011585363310587e-07}, {"id": 1083, "seek": 446908, "start": 4475.32, "end": 4480.16, "text": " So that's how we can create SGD.", "tokens": [407, 300, 311, 577, 321, 393, 1884, 34520, 35, 13], "temperature": 0.0, "avg_logprob": -0.16401279540289015, "compression_ratio": 1.5210526315789474, "no_speech_prob": 3.011585363310587e-07}, {"id": 1084, "seek": 446908, "start": 4480.16, "end": 4485.68, "text": " So with that optimization function, we can fit.", "tokens": [407, 365, 300, 19618, 2445, 11, 321, 393, 3318, 13], "temperature": 0.0, "avg_logprob": -0.16401279540289015, "compression_ratio": 1.5210526315789474, "no_speech_prob": 3.011585363310587e-07}, {"id": 1085, "seek": 446908, "start": 4485.68, "end": 4489.4, "text": " It's not doing anything different at all, right?", "tokens": [467, 311, 406, 884, 1340, 819, 412, 439, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.16401279540289015, "compression_ratio": 1.5210526315789474, "no_speech_prob": 3.011585363310587e-07}, {"id": 1086, "seek": 446908, "start": 4489.4, "end": 4493.82, "text": " But what we have done is we've done the same thing we've done a thousand times without", "tokens": [583, 437, 321, 362, 1096, 307, 321, 600, 1096, 264, 912, 551, 321, 600, 1096, 257, 4714, 1413, 1553], "temperature": 0.0, "avg_logprob": -0.16401279540289015, "compression_ratio": 1.5210526315789474, "no_speech_prob": 3.011585363310587e-07}, {"id": 1087, "seek": 449382, "start": 4493.82, "end": 4499.12, "text": " ever creating an SGD optimizer.", "tokens": [1562, 4084, 364, 34520, 35, 5028, 6545, 13], "temperature": 0.0, "avg_logprob": -0.1356740951538086, "compression_ratio": 2.0408163265306123, "no_speech_prob": 2.902273536165012e-06}, {"id": 1088, "seek": 449382, "start": 4499.12, "end": 4505.32, "text": " It's an optimizer with an SGD step.", "tokens": [467, 311, 364, 5028, 6545, 365, 364, 34520, 35, 1823, 13], "temperature": 0.0, "avg_logprob": -0.1356740951538086, "compression_ratio": 2.0408163265306123, "no_speech_prob": 2.902273536165012e-06}, {"id": 1089, "seek": 449382, "start": 4505.32, "end": 4509.0, "text": " I've created this thing called gradparams, which is just a little convenience.", "tokens": [286, 600, 2942, 341, 551, 1219, 2771, 2181, 4070, 11, 597, 307, 445, 257, 707, 19283, 13], "temperature": 0.0, "avg_logprob": -0.1356740951538086, "compression_ratio": 2.0408163265306123, "no_speech_prob": 2.902273536165012e-06}, {"id": 1090, "seek": 449382, "start": 4509.0, "end": 4514.099999999999, "text": " Basically when we like zero the gradients, we have to go through every parameter.", "tokens": [8537, 562, 321, 411, 4018, 264, 2771, 2448, 11, 321, 362, 281, 352, 807, 633, 13075, 13], "temperature": 0.0, "avg_logprob": -0.1356740951538086, "compression_ratio": 2.0408163265306123, "no_speech_prob": 2.902273536165012e-06}, {"id": 1091, "seek": 449382, "start": 4514.099999999999, "end": 4518.5599999999995, "text": " To go through every parameter, we have to go through every parameter group, and then", "tokens": [1407, 352, 807, 633, 13075, 11, 321, 362, 281, 352, 807, 633, 13075, 1594, 11, 293, 550], "temperature": 0.0, "avg_logprob": -0.1356740951538086, "compression_ratio": 2.0408163265306123, "no_speech_prob": 2.902273536165012e-06}, {"id": 1092, "seek": 449382, "start": 4518.5599999999995, "end": 4522.5599999999995, "text": " within each parameter group, we have to go through every parameter in that group where", "tokens": [1951, 1184, 13075, 1594, 11, 321, 362, 281, 352, 807, 633, 13075, 294, 300, 1594, 689], "temperature": 0.0, "avg_logprob": -0.1356740951538086, "compression_ratio": 2.0408163265306123, "no_speech_prob": 2.902273536165012e-06}, {"id": 1093, "seek": 452256, "start": 4522.56, "end": 4525.04, "text": " the gradient exists.", "tokens": [264, 16235, 8198, 13], "temperature": 0.0, "avg_logprob": -0.16517358862835427, "compression_ratio": 1.7079646017699115, "no_speech_prob": 3.966933945775963e-06}, {"id": 1094, "seek": 452256, "start": 4525.04, "end": 4526.92, "text": " They're the ones that we have to zero.", "tokens": [814, 434, 264, 2306, 300, 321, 362, 281, 4018, 13], "temperature": 0.0, "avg_logprob": -0.16517358862835427, "compression_ratio": 1.7079646017699115, "no_speech_prob": 3.966933945775963e-06}, {"id": 1095, "seek": 452256, "start": 4526.92, "end": 4529.76, "text": " And ditto for when we do a step.", "tokens": [400, 274, 34924, 337, 562, 321, 360, 257, 1823, 13], "temperature": 0.0, "avg_logprob": -0.16517358862835427, "compression_ratio": 1.7079646017699115, "no_speech_prob": 3.966933945775963e-06}, {"id": 1096, "seek": 452256, "start": 4529.76, "end": 4531.96, "text": " That's why I just refactored it.", "tokens": [663, 311, 983, 286, 445, 1895, 578, 2769, 309, 13], "temperature": 0.0, "avg_logprob": -0.16517358862835427, "compression_ratio": 1.7079646017699115, "no_speech_prob": 3.966933945775963e-06}, {"id": 1097, "seek": 452256, "start": 4531.96, "end": 4541.8, "text": " And also when we call the stepper, we want to pass to it all of our hyperparameters,", "tokens": [400, 611, 562, 321, 818, 264, 2126, 3717, 11, 321, 528, 281, 1320, 281, 309, 439, 295, 527, 9848, 2181, 335, 6202, 11], "temperature": 0.0, "avg_logprob": -0.16517358862835427, "compression_ratio": 1.7079646017699115, "no_speech_prob": 3.966933945775963e-06}, {"id": 1098, "seek": 452256, "start": 4541.8, "end": 4544.84, "text": " right, because the stepper might want them.", "tokens": [558, 11, 570, 264, 2126, 3717, 1062, 528, 552, 13], "temperature": 0.0, "avg_logprob": -0.16517358862835427, "compression_ratio": 1.7079646017699115, "no_speech_prob": 3.966933945775963e-06}, {"id": 1099, "seek": 452256, "start": 4544.84, "end": 4548.72, "text": " Like it'll probably want learning rate, right, and learning rate's just one of the things", "tokens": [1743, 309, 603, 1391, 528, 2539, 3314, 11, 558, 11, 293, 2539, 3314, 311, 445, 472, 295, 264, 721], "temperature": 0.0, "avg_logprob": -0.16517358862835427, "compression_ratio": 1.7079646017699115, "no_speech_prob": 3.966933945775963e-06}, {"id": 1100, "seek": 452256, "start": 4548.72, "end": 4552.04, "text": " that we've listed in our hyperparameters.", "tokens": [300, 321, 600, 10052, 294, 527, 9848, 2181, 335, 6202, 13], "temperature": 0.0, "avg_logprob": -0.16517358862835427, "compression_ratio": 1.7079646017699115, "no_speech_prob": 3.966933945775963e-06}, {"id": 1101, "seek": 455204, "start": 4552.04, "end": 4556.96, "text": " So remember how I said that our compose is a bit special, that it passes along any keyword", "tokens": [407, 1604, 577, 286, 848, 300, 527, 35925, 307, 257, 857, 2121, 11, 300, 309, 11335, 2051, 604, 20428], "temperature": 0.0, "avg_logprob": -0.1074700861266165, "compression_ratio": 1.7047970479704797, "no_speech_prob": 4.157280272920616e-06}, {"id": 1102, "seek": 455204, "start": 4556.96, "end": 4559.48, "text": " arguments it got to everything that it composes?", "tokens": [12869, 309, 658, 281, 1203, 300, 309, 715, 4201, 30], "temperature": 0.0, "avg_logprob": -0.1074700861266165, "compression_ratio": 1.7047970479704797, "no_speech_prob": 4.157280272920616e-06}, {"id": 1103, "seek": 455204, "start": 4559.48, "end": 4561.84, "text": " Here's a nice way to use that, right?", "tokens": [1692, 311, 257, 1481, 636, 281, 764, 300, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1074700861266165, "compression_ratio": 1.7047970479704797, "no_speech_prob": 4.157280272920616e-06}, {"id": 1104, "seek": 455204, "start": 4561.84, "end": 4566.44, "text": " So that's how calm SGD step can say, oh, I need the learning rate.", "tokens": [407, 300, 311, 577, 7151, 34520, 35, 1823, 393, 584, 11, 1954, 11, 286, 643, 264, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.1074700861266165, "compression_ratio": 1.7047970479704797, "no_speech_prob": 4.157280272920616e-06}, {"id": 1105, "seek": 455204, "start": 4566.44, "end": 4571.04, "text": " And so as long as hyper has a learning rate in it, it's going to end up here.", "tokens": [400, 370, 382, 938, 382, 9848, 575, 257, 2539, 3314, 294, 309, 11, 309, 311, 516, 281, 917, 493, 510, 13], "temperature": 0.0, "avg_logprob": -0.1074700861266165, "compression_ratio": 1.7047970479704797, "no_speech_prob": 4.157280272920616e-06}, {"id": 1106, "seek": 455204, "start": 4571.04, "end": 4574.78, "text": " And it'll be here as long as you passed it here.", "tokens": [400, 309, 603, 312, 510, 382, 938, 382, 291, 4678, 309, 510, 13], "temperature": 0.0, "avg_logprob": -0.1074700861266165, "compression_ratio": 1.7047970479704797, "no_speech_prob": 4.157280272920616e-06}, {"id": 1107, "seek": 455204, "start": 4574.78, "end": 4576.88, "text": " And then you can change it for each different layer group.", "tokens": [400, 550, 291, 393, 1319, 309, 337, 1184, 819, 4583, 1594, 13], "temperature": 0.0, "avg_logprob": -0.1074700861266165, "compression_ratio": 1.7047970479704797, "no_speech_prob": 4.157280272920616e-06}, {"id": 1108, "seek": 455204, "start": 4576.88, "end": 4580.34, "text": " You can anneal it and so forth.", "tokens": [509, 393, 22256, 304, 309, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.1074700861266165, "compression_ratio": 1.7047970479704797, "no_speech_prob": 4.157280272920616e-06}, {"id": 1109, "seek": 458034, "start": 4580.34, "end": 4589.84, "text": " So we're going to need to change our parameter scheduler to use our new generic optimizer.", "tokens": [407, 321, 434, 516, 281, 643, 281, 1319, 527, 13075, 12000, 260, 281, 764, 527, 777, 19577, 5028, 6545, 13], "temperature": 0.0, "avg_logprob": -0.13050004547717525, "compression_ratio": 1.6866359447004609, "no_speech_prob": 8.800582691037562e-06}, {"id": 1110, "seek": 458034, "start": 4589.84, "end": 4597.72, "text": " It's simply now that we have to say go through each hyperparameter in self.opt.hypers and", "tokens": [467, 311, 2935, 586, 300, 321, 362, 281, 584, 352, 807, 1184, 9848, 2181, 335, 2398, 294, 2698, 13, 5747, 13, 3495, 21819, 293], "temperature": 0.0, "avg_logprob": -0.13050004547717525, "compression_ratio": 1.6866359447004609, "no_speech_prob": 8.800582691037562e-06}, {"id": 1111, "seek": 458034, "start": 4597.72, "end": 4599.24, "text": " schedule it.", "tokens": [7567, 309, 13], "temperature": 0.0, "avg_logprob": -0.13050004547717525, "compression_ratio": 1.6866359447004609, "no_speech_prob": 8.800582691037562e-06}, {"id": 1112, "seek": 458034, "start": 4599.24, "end": 4602.360000000001, "text": " So that's basically the same as what we had in parameter scheduler before, but for our", "tokens": [407, 300, 311, 1936, 264, 912, 382, 437, 321, 632, 294, 13075, 12000, 260, 949, 11, 457, 337, 527], "temperature": 0.0, "avg_logprob": -0.13050004547717525, "compression_ratio": 1.6866359447004609, "no_speech_prob": 8.800582691037562e-06}, {"id": 1113, "seek": 458034, "start": 4602.360000000001, "end": 4603.56, "text": " new thing.", "tokens": [777, 551, 13], "temperature": 0.0, "avg_logprob": -0.13050004547717525, "compression_ratio": 1.6866359447004609, "no_speech_prob": 8.800582691037562e-06}, {"id": 1114, "seek": 458034, "start": 4603.56, "end": 4610.08, "text": " And ditto for recorder, this used to use param groups, now it uses hypers.", "tokens": [400, 274, 34924, 337, 37744, 11, 341, 1143, 281, 764, 6220, 3935, 11, 586, 309, 4960, 2477, 21819, 13], "temperature": 0.0, "avg_logprob": -0.13050004547717525, "compression_ratio": 1.6866359447004609, "no_speech_prob": 8.800582691037562e-06}, {"id": 1115, "seek": 461008, "start": 4610.08, "end": 4612.96, "text": " So a minor change to make these keep working.", "tokens": [407, 257, 6696, 1319, 281, 652, 613, 1066, 1364, 13], "temperature": 0.0, "avg_logprob": -0.1541721930870643, "compression_ratio": 1.749063670411985, "no_speech_prob": 1.1841610103147104e-05}, {"id": 1116, "seek": 461008, "start": 4612.96, "end": 4617.5199999999995, "text": " So now I was super excited when we first got this working.", "tokens": [407, 586, 286, 390, 1687, 2919, 562, 321, 700, 658, 341, 1364, 13], "temperature": 0.0, "avg_logprob": -0.1541721930870643, "compression_ratio": 1.749063670411985, "no_speech_prob": 1.1841610103147104e-05}, {"id": 1117, "seek": 461008, "start": 4617.5199999999995, "end": 4622.5199999999995, "text": " So it's like, wow, we've just built an SGD optimizer that works without ever writing", "tokens": [407, 309, 311, 411, 11, 6076, 11, 321, 600, 445, 3094, 364, 34520, 35, 5028, 6545, 300, 1985, 1553, 1562, 3579], "temperature": 0.0, "avg_logprob": -0.1541721930870643, "compression_ratio": 1.749063670411985, "no_speech_prob": 1.1841610103147104e-05}, {"id": 1118, "seek": 461008, "start": 4622.5199999999995, "end": 4624.32, "text": " an SGD optimizer.", "tokens": [364, 34520, 35, 5028, 6545, 13], "temperature": 0.0, "avg_logprob": -0.1541721930870643, "compression_ratio": 1.749063670411985, "no_speech_prob": 1.1841610103147104e-05}, {"id": 1119, "seek": 461008, "start": 4624.32, "end": 4626.76, "text": " So now when we want to add weight decay, right?", "tokens": [407, 586, 562, 321, 528, 281, 909, 3364, 21039, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1541721930870643, "compression_ratio": 1.749063670411985, "no_speech_prob": 1.1841610103147104e-05}, {"id": 1120, "seek": 461008, "start": 4626.76, "end": 4630.88, "text": " So weight decay, remember, is the thing where we don't want something that fits this.", "tokens": [407, 3364, 21039, 11, 1604, 11, 307, 264, 551, 689, 321, 500, 380, 528, 746, 300, 9001, 341, 13], "temperature": 0.0, "avg_logprob": -0.1541721930870643, "compression_ratio": 1.749063670411985, "no_speech_prob": 1.1841610103147104e-05}, {"id": 1121, "seek": 461008, "start": 4630.88, "end": 4633.08, "text": " We want something that fits this.", "tokens": [492, 528, 746, 300, 9001, 341, 13], "temperature": 0.0, "avg_logprob": -0.1541721930870643, "compression_ratio": 1.749063670411985, "no_speech_prob": 1.1841610103147104e-05}, {"id": 1122, "seek": 461008, "start": 4633.08, "end": 4639.88, "text": " And the way we do it is we use L2 regularization, which just is we add the sum of squared's", "tokens": [400, 264, 636, 321, 360, 309, 307, 321, 764, 441, 17, 3890, 2144, 11, 597, 445, 307, 321, 909, 264, 2408, 295, 8889, 311], "temperature": 0.0, "avg_logprob": -0.1541721930870643, "compression_ratio": 1.749063670411985, "no_speech_prob": 1.1841610103147104e-05}, {"id": 1123, "seek": 463988, "start": 4639.88, "end": 4644.4400000000005, "text": " weights times some parameter we choose.", "tokens": [17443, 1413, 512, 13075, 321, 2826, 13], "temperature": 0.0, "avg_logprob": -0.12475722010542707, "compression_ratio": 1.5048076923076923, "no_speech_prob": 1.3006409062654711e-05}, {"id": 1124, "seek": 463988, "start": 4644.4400000000005, "end": 4649.68, "text": " And remember that the derivative of that is actually just WTD times weight.", "tokens": [400, 1604, 300, 264, 13760, 295, 300, 307, 767, 445, 343, 51, 35, 1413, 3364, 13], "temperature": 0.0, "avg_logprob": -0.12475722010542707, "compression_ratio": 1.5048076923076923, "no_speech_prob": 1.3006409062654711e-05}, {"id": 1125, "seek": 463988, "start": 4649.68, "end": 4658.28, "text": " So you could either add an L2 regularization to the loss, or you can add WD times weight", "tokens": [407, 291, 727, 2139, 909, 364, 441, 17, 3890, 2144, 281, 264, 4470, 11, 420, 291, 393, 909, 343, 35, 1413, 3364], "temperature": 0.0, "avg_logprob": -0.12475722010542707, "compression_ratio": 1.5048076923076923, "no_speech_prob": 1.3006409062654711e-05}, {"id": 1126, "seek": 463988, "start": 4658.28, "end": 4659.52, "text": " to the gradients.", "tokens": [281, 264, 2771, 2448, 13], "temperature": 0.0, "avg_logprob": -0.12475722010542707, "compression_ratio": 1.5048076923076923, "no_speech_prob": 1.3006409062654711e-05}, {"id": 1127, "seek": 463988, "start": 4659.52, "end": 4666.56, "text": " If you've forgotten this, go back and look at weight decay in part one to remind yourself.", "tokens": [759, 291, 600, 11832, 341, 11, 352, 646, 293, 574, 412, 3364, 21039, 294, 644, 472, 281, 4160, 1803, 13], "temperature": 0.0, "avg_logprob": -0.12475722010542707, "compression_ratio": 1.5048076923076923, "no_speech_prob": 1.3006409062654711e-05}, {"id": 1128, "seek": 466656, "start": 4666.56, "end": 4671.84, "text": " And so if we want to add either this or this, we can do it.", "tokens": [400, 370, 498, 321, 528, 281, 909, 2139, 341, 420, 341, 11, 321, 393, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.15327707611688293, "compression_ratio": 1.4845360824742269, "no_speech_prob": 2.7693422453012317e-06}, {"id": 1129, "seek": 466656, "start": 4671.84, "end": 4673.400000000001, "text": " We can add a stepper.", "tokens": [492, 393, 909, 257, 2126, 3717, 13], "temperature": 0.0, "avg_logprob": -0.15327707611688293, "compression_ratio": 1.4845360824742269, "no_speech_prob": 2.7693422453012317e-06}, {"id": 1130, "seek": 466656, "start": 4673.400000000001, "end": 4680.52, "text": " So weight decay is going to get an LR and a WD, and it's going to simply do that.", "tokens": [407, 3364, 21039, 307, 516, 281, 483, 364, 441, 49, 293, 257, 343, 35, 11, 293, 309, 311, 516, 281, 2935, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.15327707611688293, "compression_ratio": 1.4845360824742269, "no_speech_prob": 2.7693422453012317e-06}, {"id": 1131, "seek": 466656, "start": 4680.52, "end": 4683.160000000001, "text": " There it is.", "tokens": [821, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.15327707611688293, "compression_ratio": 1.4845360824742269, "no_speech_prob": 2.7693422453012317e-06}, {"id": 1132, "seek": 466656, "start": 4683.160000000001, "end": 4684.160000000001, "text": " OK.", "tokens": [2264, 13], "temperature": 0.0, "avg_logprob": -0.15327707611688293, "compression_ratio": 1.4845360824742269, "no_speech_prob": 2.7693422453012317e-06}, {"id": 1133, "seek": 466656, "start": 4684.160000000001, "end": 4690.160000000001, "text": " Or L2 regularization is going to just do that.", "tokens": [1610, 441, 17, 3890, 2144, 307, 516, 281, 445, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.15327707611688293, "compression_ratio": 1.4845360824742269, "no_speech_prob": 2.7693422453012317e-06}, {"id": 1134, "seek": 466656, "start": 4690.160000000001, "end": 4696.0, "text": " By the way, if you haven't seen this before, add in PyTorch.", "tokens": [3146, 264, 636, 11, 498, 291, 2378, 380, 1612, 341, 949, 11, 909, 294, 9953, 51, 284, 339, 13], "temperature": 0.0, "avg_logprob": -0.15327707611688293, "compression_ratio": 1.4845360824742269, "no_speech_prob": 2.7693422453012317e-06}, {"id": 1135, "seek": 469600, "start": 4696.0, "end": 4699.28, "text": " Normally it just adds this tensor to this tensor.", "tokens": [17424, 309, 445, 10860, 341, 40863, 281, 341, 40863, 13], "temperature": 0.0, "avg_logprob": -0.17866496434287418, "compression_ratio": 1.396103896103896, "no_speech_prob": 5.682286428054795e-06}, {"id": 1136, "seek": 469600, "start": 4699.28, "end": 4704.08, "text": " But if you add a scalar here, it multiplies these together first.", "tokens": [583, 498, 291, 909, 257, 39684, 510, 11, 309, 12788, 530, 613, 1214, 700, 13], "temperature": 0.0, "avg_logprob": -0.17866496434287418, "compression_ratio": 1.396103896103896, "no_speech_prob": 5.682286428054795e-06}, {"id": 1137, "seek": 469600, "start": 4704.08, "end": 4711.72, "text": " This is a nice fast way to go WD times parameter and add that to the gradient.", "tokens": [639, 307, 257, 1481, 2370, 636, 281, 352, 343, 35, 1413, 13075, 293, 909, 300, 281, 264, 16235, 13], "temperature": 0.0, "avg_logprob": -0.17866496434287418, "compression_ratio": 1.396103896103896, "no_speech_prob": 5.682286428054795e-06}, {"id": 1138, "seek": 469600, "start": 4711.72, "end": 4716.52, "text": " So there's that.", "tokens": [407, 456, 311, 300, 13], "temperature": 0.0, "avg_logprob": -0.17866496434287418, "compression_ratio": 1.396103896103896, "no_speech_prob": 5.682286428054795e-06}, {"id": 1139, "seek": 469600, "start": 4716.52, "end": 4720.84, "text": " OK.", "tokens": [2264, 13], "temperature": 0.0, "avg_logprob": -0.17866496434287418, "compression_ratio": 1.396103896103896, "no_speech_prob": 5.682286428054795e-06}, {"id": 1140, "seek": 472084, "start": 4720.84, "end": 4726.360000000001, "text": " So we've got our L2 regularization.", "tokens": [407, 321, 600, 658, 527, 441, 17, 3890, 2144, 13], "temperature": 0.0, "avg_logprob": -0.09933301847274989, "compression_ratio": 1.5569620253164558, "no_speech_prob": 4.092813469469547e-06}, {"id": 1141, "seek": 472084, "start": 4726.360000000001, "end": 4733.18, "text": " We've got our weight decay.", "tokens": [492, 600, 658, 527, 3364, 21039, 13], "temperature": 0.0, "avg_logprob": -0.09933301847274989, "compression_ratio": 1.5569620253164558, "no_speech_prob": 4.092813469469547e-06}, {"id": 1142, "seek": 472084, "start": 4733.18, "end": 4740.88, "text": " What we need to be able to do now is to be able to somehow have the idea of defaults,", "tokens": [708, 321, 643, 281, 312, 1075, 281, 360, 586, 307, 281, 312, 1075, 281, 6063, 362, 264, 1558, 295, 7576, 82, 11], "temperature": 0.0, "avg_logprob": -0.09933301847274989, "compression_ratio": 1.5569620253164558, "no_speech_prob": 4.092813469469547e-06}, {"id": 1143, "seek": 472084, "start": 4740.88, "end": 4746.04, "text": " because we don't want to have to say weight decay equals zero every time we want to turn", "tokens": [570, 321, 500, 380, 528, 281, 362, 281, 584, 3364, 21039, 6915, 4018, 633, 565, 321, 528, 281, 1261], "temperature": 0.0, "avg_logprob": -0.09933301847274989, "compression_ratio": 1.5569620253164558, "no_speech_prob": 4.092813469469547e-06}, {"id": 1144, "seek": 472084, "start": 4746.04, "end": 4747.16, "text": " it off.", "tokens": [309, 766, 13], "temperature": 0.0, "avg_logprob": -0.09933301847274989, "compression_ratio": 1.5569620253164558, "no_speech_prob": 4.092813469469547e-06}, {"id": 1145, "seek": 474716, "start": 4747.16, "end": 4752.08, "text": " So see how we've attached some state here to our function object.", "tokens": [407, 536, 577, 321, 600, 8570, 512, 1785, 510, 281, 527, 2445, 2657, 13], "temperature": 0.0, "avg_logprob": -0.08582037428151006, "compression_ratio": 1.6291079812206573, "no_speech_prob": 6.854019375168718e-06}, {"id": 1146, "seek": 474716, "start": 4752.08, "end": 4757.08, "text": " So the function now has something called defaults that says it's a dictionary with WD equals", "tokens": [407, 264, 2445, 586, 575, 746, 1219, 7576, 82, 300, 1619, 309, 311, 257, 25890, 365, 343, 35, 6915], "temperature": 0.0, "avg_logprob": -0.08582037428151006, "compression_ratio": 1.6291079812206573, "no_speech_prob": 6.854019375168718e-06}, {"id": 1147, "seek": 474716, "start": 4757.08, "end": 4759.12, "text": " zero.", "tokens": [4018, 13], "temperature": 0.0, "avg_logprob": -0.08582037428151006, "compression_ratio": 1.6291079812206573, "no_speech_prob": 6.854019375168718e-06}, {"id": 1148, "seek": 474716, "start": 4759.12, "end": 4763.16, "text": " So let's just grab exactly the same optimizer we had before.", "tokens": [407, 718, 311, 445, 4444, 2293, 264, 912, 5028, 6545, 321, 632, 949, 13], "temperature": 0.0, "avg_logprob": -0.08582037428151006, "compression_ratio": 1.6291079812206573, "no_speech_prob": 6.854019375168718e-06}, {"id": 1149, "seek": 474716, "start": 4763.16, "end": 4770.94, "text": " But what we're going to do is we're going to maybe update our defaults with whatever", "tokens": [583, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 1310, 5623, 527, 7576, 82, 365, 2035], "temperature": 0.0, "avg_logprob": -0.08582037428151006, "compression_ratio": 1.6291079812206573, "no_speech_prob": 6.854019375168718e-06}, {"id": 1150, "seek": 474716, "start": 4770.94, "end": 4774.24, "text": " self.steppers has in their defaults.", "tokens": [2698, 13, 2941, 15226, 575, 294, 641, 7576, 82, 13], "temperature": 0.0, "avg_logprob": -0.08582037428151006, "compression_ratio": 1.6291079812206573, "no_speech_prob": 6.854019375168718e-06}, {"id": 1151, "seek": 477424, "start": 4774.24, "end": 4777.76, "text": " And the reason it's maybe update is that it's not going to replace.", "tokens": [400, 264, 1778, 309, 311, 1310, 5623, 307, 300, 309, 311, 406, 516, 281, 7406, 13], "temperature": 0.0, "avg_logprob": -0.1297565199371077, "compression_ratio": 1.8181818181818181, "no_speech_prob": 1.4738630852662027e-05}, {"id": 1152, "seek": 477424, "start": 4777.76, "end": 4781.76, "text": " If you explicitly say, I want this weight decay, it's not going to update it.", "tokens": [759, 291, 20803, 584, 11, 286, 528, 341, 3364, 21039, 11, 309, 311, 406, 516, 281, 5623, 309, 13], "temperature": 0.0, "avg_logprob": -0.1297565199371077, "compression_ratio": 1.8181818181818181, "no_speech_prob": 1.4738630852662027e-05}, {"id": 1153, "seek": 477424, "start": 4781.76, "end": 4784.32, "text": " It'll only update it if it's missing.", "tokens": [467, 603, 787, 5623, 309, 498, 309, 311, 5361, 13], "temperature": 0.0, "avg_logprob": -0.1297565199371077, "compression_ratio": 1.8181818181818181, "no_speech_prob": 1.4738630852662027e-05}, {"id": 1154, "seek": 477424, "start": 4784.32, "end": 4786.4, "text": " And so that's just what this little loop does.", "tokens": [400, 370, 300, 311, 445, 437, 341, 707, 6367, 775, 13], "temperature": 0.0, "avg_logprob": -0.1297565199371077, "compression_ratio": 1.8181818181818181, "no_speech_prob": 1.4738630852662027e-05}, {"id": 1155, "seek": 477424, "start": 4786.4, "end": 4791.44, "text": " Just goes through each of the things and then goes through each of the things in the dictionary.", "tokens": [1449, 1709, 807, 1184, 295, 264, 721, 293, 550, 1709, 807, 1184, 295, 264, 721, 294, 264, 25890, 13], "temperature": 0.0, "avg_logprob": -0.1297565199371077, "compression_ratio": 1.8181818181818181, "no_speech_prob": 1.4738630852662027e-05}, {"id": 1156, "seek": 477424, "start": 4791.44, "end": 4795.639999999999, "text": " And it just checks if it's not there, then it updates it.", "tokens": [400, 309, 445, 13834, 498, 309, 311, 406, 456, 11, 550, 309, 9205, 309, 13], "temperature": 0.0, "avg_logprob": -0.1297565199371077, "compression_ratio": 1.8181818181818181, "no_speech_prob": 1.4738630852662027e-05}, {"id": 1157, "seek": 477424, "start": 4795.639999999999, "end": 4800.96, "text": " So everything else here is exactly the same as before.", "tokens": [407, 1203, 1646, 510, 307, 2293, 264, 912, 382, 949, 13], "temperature": 0.0, "avg_logprob": -0.1297565199371077, "compression_ratio": 1.8181818181818181, "no_speech_prob": 1.4738630852662027e-05}, {"id": 1158, "seek": 480096, "start": 4800.96, "end": 4808.6, "text": " So now we can say, let's create an SGD optimizer.", "tokens": [407, 586, 321, 393, 584, 11, 718, 311, 1884, 364, 34520, 35, 5028, 6545, 13], "temperature": 0.0, "avg_logprob": -0.12859813260360503, "compression_ratio": 1.6083916083916083, "no_speech_prob": 1.459359964428586e-06}, {"id": 1159, "seek": 480096, "start": 4808.6, "end": 4814.28, "text": " It's just an optimizer with a SGD step and weight decay.", "tokens": [467, 311, 445, 364, 5028, 6545, 365, 257, 34520, 35, 1823, 293, 3364, 21039, 13], "temperature": 0.0, "avg_logprob": -0.12859813260360503, "compression_ratio": 1.6083916083916083, "no_speech_prob": 1.459359964428586e-06}, {"id": 1160, "seek": 480096, "start": 4814.28, "end": 4819.32, "text": " And so let's create a learner.", "tokens": [400, 370, 718, 311, 1884, 257, 33347, 13], "temperature": 0.0, "avg_logprob": -0.12859813260360503, "compression_ratio": 1.6083916083916083, "no_speech_prob": 1.459359964428586e-06}, {"id": 1161, "seek": 480096, "start": 4819.32, "end": 4826.2, "text": " And let's try creating an optimizer, which is an SGD optimizer, with our model's parameters,", "tokens": [400, 718, 311, 853, 4084, 364, 5028, 6545, 11, 597, 307, 364, 34520, 35, 5028, 6545, 11, 365, 527, 2316, 311, 9834, 11], "temperature": 0.0, "avg_logprob": -0.12859813260360503, "compression_ratio": 1.6083916083916083, "no_speech_prob": 1.459359964428586e-06}, {"id": 1162, "seek": 482620, "start": 4826.2, "end": 4831.4, "text": " with some learning rate, and make sure that the hyperparameter for weight decay should", "tokens": [365, 512, 2539, 3314, 11, 293, 652, 988, 300, 264, 9848, 2181, 335, 2398, 337, 3364, 21039, 820], "temperature": 0.0, "avg_logprob": -0.16629214035837273, "compression_ratio": 1.6736401673640167, "no_speech_prob": 8.939417966757901e-06}, {"id": 1163, "seek": 482620, "start": 4831.4, "end": 4832.88, "text": " be 0.", "tokens": [312, 1958, 13], "temperature": 0.0, "avg_logprob": -0.16629214035837273, "compression_ratio": 1.6736401673640167, "no_speech_prob": 8.939417966757901e-06}, {"id": 1164, "seek": 482620, "start": 4832.88, "end": 4834.96, "text": " The hyperparameter for LR should be 0.1.", "tokens": [440, 9848, 2181, 335, 2398, 337, 441, 49, 820, 312, 1958, 13, 16, 13], "temperature": 0.0, "avg_logprob": -0.16629214035837273, "compression_ratio": 1.6736401673640167, "no_speech_prob": 8.939417966757901e-06}, {"id": 1165, "seek": 482620, "start": 4834.96, "end": 4836.96, "text": " Yep, it passes.", "tokens": [7010, 11, 309, 11335, 13], "temperature": 0.0, "avg_logprob": -0.16629214035837273, "compression_ratio": 1.6736401673640167, "no_speech_prob": 8.939417966757901e-06}, {"id": 1166, "seek": 482620, "start": 4836.96, "end": 4838.44, "text": " Let's try giving it a different weight decay.", "tokens": [961, 311, 853, 2902, 309, 257, 819, 3364, 21039, 13], "temperature": 0.0, "avg_logprob": -0.16629214035837273, "compression_ratio": 1.6736401673640167, "no_speech_prob": 8.939417966757901e-06}, {"id": 1167, "seek": 482620, "start": 4838.44, "end": 4839.44, "text": " Make sure it's there.", "tokens": [4387, 988, 309, 311, 456, 13], "temperature": 0.0, "avg_logprob": -0.16629214035837273, "compression_ratio": 1.6736401673640167, "no_speech_prob": 8.939417966757901e-06}, {"id": 1168, "seek": 482620, "start": 4839.44, "end": 4841.16, "text": " OK, it passes as well.", "tokens": [2264, 11, 309, 11335, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.16629214035837273, "compression_ratio": 1.6736401673640167, "no_speech_prob": 8.939417966757901e-06}, {"id": 1169, "seek": 482620, "start": 4841.16, "end": 4848.4, "text": " So we've now got an ability to basically add any step functions we want.", "tokens": [407, 321, 600, 586, 658, 364, 3485, 281, 1936, 909, 604, 1823, 6828, 321, 528, 13], "temperature": 0.0, "avg_logprob": -0.16629214035837273, "compression_ratio": 1.6736401673640167, "no_speech_prob": 8.939417966757901e-06}, {"id": 1170, "seek": 482620, "start": 4848.4, "end": 4853.8, "text": " And those step functions can have their own state that gets added automatically to our", "tokens": [400, 729, 1823, 6828, 393, 362, 641, 1065, 1785, 300, 2170, 3869, 6772, 281, 527], "temperature": 0.0, "avg_logprob": -0.16629214035837273, "compression_ratio": 1.6736401673640167, "no_speech_prob": 8.939417966757901e-06}, {"id": 1171, "seek": 485380, "start": 4853.8, "end": 4856.360000000001, "text": " optimization object.", "tokens": [19618, 2657, 13], "temperature": 0.0, "avg_logprob": -0.13471602230537227, "compression_ratio": 1.6141304347826086, "no_speech_prob": 1.3925305211159866e-06}, {"id": 1172, "seek": 485380, "start": 4856.360000000001, "end": 4858.08, "text": " And we can go ahead and fit.", "tokens": [400, 321, 393, 352, 2286, 293, 3318, 13], "temperature": 0.0, "avg_logprob": -0.13471602230537227, "compression_ratio": 1.6141304347826086, "no_speech_prob": 1.3925305211159866e-06}, {"id": 1173, "seek": 485380, "start": 4858.08, "end": 4861.14, "text": " So that's fine.", "tokens": [407, 300, 311, 2489, 13], "temperature": 0.0, "avg_logprob": -0.13471602230537227, "compression_ratio": 1.6141304347826086, "no_speech_prob": 1.3925305211159866e-06}, {"id": 1174, "seek": 485380, "start": 4861.14, "end": 4871.6, "text": " So now we've got an SGD optimizer with weight decay is one line of code.", "tokens": [407, 586, 321, 600, 658, 364, 34520, 35, 5028, 6545, 365, 3364, 21039, 307, 472, 1622, 295, 3089, 13], "temperature": 0.0, "avg_logprob": -0.13471602230537227, "compression_ratio": 1.6141304347826086, "no_speech_prob": 1.3925305211159866e-06}, {"id": 1175, "seek": 485380, "start": 4871.6, "end": 4874.72, "text": " Let's now add momentum.", "tokens": [961, 311, 586, 909, 11244, 13], "temperature": 0.0, "avg_logprob": -0.13471602230537227, "compression_ratio": 1.6141304347826086, "no_speech_prob": 1.3925305211159866e-06}, {"id": 1176, "seek": 485380, "start": 4874.72, "end": 4879.84, "text": " So momentum is going to require a slightly better optimizer or a slightly different optimizer,", "tokens": [407, 11244, 307, 516, 281, 3651, 257, 4748, 1101, 5028, 6545, 420, 257, 4748, 819, 5028, 6545, 11], "temperature": 0.0, "avg_logprob": -0.13471602230537227, "compression_ratio": 1.6141304347826086, "no_speech_prob": 1.3925305211159866e-06}, {"id": 1177, "seek": 485380, "start": 4879.84, "end": 4883.26, "text": " because momentum needs some more state.", "tokens": [570, 11244, 2203, 512, 544, 1785, 13], "temperature": 0.0, "avg_logprob": -0.13471602230537227, "compression_ratio": 1.6141304347826086, "no_speech_prob": 1.3925305211159866e-06}, {"id": 1178, "seek": 488326, "start": 4883.26, "end": 4887.4800000000005, "text": " It doesn't just have parameters and hyperparameters.", "tokens": [467, 1177, 380, 445, 362, 9834, 293, 9848, 2181, 335, 6202, 13], "temperature": 0.0, "avg_logprob": -0.13828556354229266, "compression_ratio": 1.6077348066298343, "no_speech_prob": 7.183119123510551e-06}, {"id": 1179, "seek": 488326, "start": 4887.4800000000005, "end": 4894.360000000001, "text": " But also momentum knows that for every set of activations, it knows what were they updated", "tokens": [583, 611, 11244, 3255, 300, 337, 633, 992, 295, 2430, 763, 11, 309, 3255, 437, 645, 436, 10588], "temperature": 0.0, "avg_logprob": -0.13828556354229266, "compression_ratio": 1.6077348066298343, "no_speech_prob": 7.183119123510551e-06}, {"id": 1180, "seek": 488326, "start": 4894.360000000001, "end": 4896.2, "text": " by last time.", "tokens": [538, 1036, 565, 13], "temperature": 0.0, "avg_logprob": -0.13828556354229266, "compression_ratio": 1.6077348066298343, "no_speech_prob": 7.183119123510551e-06}, {"id": 1181, "seek": 488326, "start": 4896.2, "end": 4902.6, "text": " Because remember, the momentum equation is, if momentum is 0.9, then it would be 0.9 times", "tokens": [1436, 1604, 11, 264, 11244, 5367, 307, 11, 498, 11244, 307, 1958, 13, 24, 11, 550, 309, 576, 312, 1958, 13, 24, 1413], "temperature": 0.0, "avg_logprob": -0.13828556354229266, "compression_ratio": 1.6077348066298343, "no_speech_prob": 7.183119123510551e-06}, {"id": 1182, "seek": 488326, "start": 4902.6, "end": 4908.04, "text": " whatever you did last time plus this step.", "tokens": [2035, 291, 630, 1036, 565, 1804, 341, 1823, 13], "temperature": 0.0, "avg_logprob": -0.13828556354229266, "compression_ratio": 1.6077348066298343, "no_speech_prob": 7.183119123510551e-06}, {"id": 1183, "seek": 490804, "start": 4908.04, "end": 4914.4, "text": " So we actually need to track for every single parameter what happened last time.", "tokens": [407, 321, 767, 643, 281, 2837, 337, 633, 2167, 13075, 437, 2011, 1036, 565, 13], "temperature": 0.0, "avg_logprob": -0.07551977747962588, "compression_ratio": 1.5754716981132075, "no_speech_prob": 7.00211160165054e-07}, {"id": 1184, "seek": 490804, "start": 4914.4, "end": 4916.58, "text": " And that's actually quite a bit of state, right?", "tokens": [400, 300, 311, 767, 1596, 257, 857, 295, 1785, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.07551977747962588, "compression_ratio": 1.5754716981132075, "no_speech_prob": 7.00211160165054e-07}, {"id": 1185, "seek": 490804, "start": 4916.58, "end": 4923.04, "text": " If you've got 10 million activations in your network, you've now got 10 million more floats", "tokens": [759, 291, 600, 658, 1266, 2459, 2430, 763, 294, 428, 3209, 11, 291, 600, 586, 658, 1266, 2459, 544, 37878], "temperature": 0.0, "avg_logprob": -0.07551977747962588, "compression_ratio": 1.5754716981132075, "no_speech_prob": 7.00211160165054e-07}, {"id": 1186, "seek": 490804, "start": 4923.04, "end": 4926.76, "text": " that you have to store, because that's your momentum.", "tokens": [300, 291, 362, 281, 3531, 11, 570, 300, 311, 428, 11244, 13], "temperature": 0.0, "avg_logprob": -0.07551977747962588, "compression_ratio": 1.5754716981132075, "no_speech_prob": 7.00211160165054e-07}, {"id": 1187, "seek": 490804, "start": 4926.76, "end": 4931.84, "text": " So we're going to store that in a dictionary called state.", "tokens": [407, 321, 434, 516, 281, 3531, 300, 294, 257, 25890, 1219, 1785, 13], "temperature": 0.0, "avg_logprob": -0.07551977747962588, "compression_ratio": 1.5754716981132075, "no_speech_prob": 7.00211160165054e-07}, {"id": 1188, "seek": 493184, "start": 4931.84, "end": 4939.16, "text": " So a stateful optimizer is just an optimizer that has state.", "tokens": [407, 257, 1785, 906, 5028, 6545, 307, 445, 364, 5028, 6545, 300, 575, 1785, 13], "temperature": 0.0, "avg_logprob": -0.08486103684934851, "compression_ratio": 1.6225165562913908, "no_speech_prob": 1.760280269991199e-06}, {"id": 1189, "seek": 493184, "start": 4939.16, "end": 4945.4800000000005, "text": " And then we're going to have to have some stats.", "tokens": [400, 550, 321, 434, 516, 281, 362, 281, 362, 512, 18152, 13], "temperature": 0.0, "avg_logprob": -0.08486103684934851, "compression_ratio": 1.6225165562913908, "no_speech_prob": 1.760280269991199e-06}, {"id": 1190, "seek": 493184, "start": 4945.4800000000005, "end": 4949.96, "text": " And stats are a lot like steppers.", "tokens": [400, 18152, 366, 257, 688, 411, 2126, 15226, 13], "temperature": 0.0, "avg_logprob": -0.08486103684934851, "compression_ratio": 1.6225165562913908, "no_speech_prob": 1.760280269991199e-06}, {"id": 1191, "seek": 493184, "start": 4949.96, "end": 4955.2, "text": " They're objects that we're going to pass in to say, when we create this state, how do", "tokens": [814, 434, 6565, 300, 321, 434, 516, 281, 1320, 294, 281, 584, 11, 562, 321, 1884, 341, 1785, 11, 577, 360], "temperature": 0.0, "avg_logprob": -0.08486103684934851, "compression_ratio": 1.6225165562913908, "no_speech_prob": 1.760280269991199e-06}, {"id": 1192, "seek": 493184, "start": 4955.2, "end": 4956.96, "text": " you create it?", "tokens": [291, 1884, 309, 30], "temperature": 0.0, "avg_logprob": -0.08486103684934851, "compression_ratio": 1.6225165562913908, "no_speech_prob": 1.760280269991199e-06}, {"id": 1193, "seek": 495696, "start": 4956.96, "end": 4962.24, "text": " So when you're doing momentum, what's the function that you run to calculate momentum?", "tokens": [407, 562, 291, 434, 884, 11244, 11, 437, 311, 264, 2445, 300, 291, 1190, 281, 8873, 11244, 30], "temperature": 0.0, "avg_logprob": -0.07656751589828663, "compression_ratio": 1.7345971563981042, "no_speech_prob": 3.0894464089215035e-06}, {"id": 1194, "seek": 495696, "start": 4962.24, "end": 4965.92, "text": " So that's going to be called something of a stat class.", "tokens": [407, 300, 311, 516, 281, 312, 1219, 746, 295, 257, 2219, 1508, 13], "temperature": 0.0, "avg_logprob": -0.07656751589828663, "compression_ratio": 1.7345971563981042, "no_speech_prob": 3.0894464089215035e-06}, {"id": 1195, "seek": 495696, "start": 4965.92, "end": 4974.04, "text": " So for example, momentum is calculated by simply averaging the gradient like so.", "tokens": [407, 337, 1365, 11, 11244, 307, 15598, 538, 2935, 47308, 264, 16235, 411, 370, 13], "temperature": 0.0, "avg_logprob": -0.07656751589828663, "compression_ratio": 1.7345971563981042, "no_speech_prob": 3.0894464089215035e-06}, {"id": 1196, "seek": 495696, "start": 4974.04, "end": 4980.4, "text": " We take whatever the gradient averaged before, we multiply it by momentum, and we add the", "tokens": [492, 747, 2035, 264, 16235, 18247, 2980, 949, 11, 321, 12972, 309, 538, 11244, 11, 293, 321, 909, 264], "temperature": 0.0, "avg_logprob": -0.07656751589828663, "compression_ratio": 1.7345971563981042, "no_speech_prob": 3.0894464089215035e-06}, {"id": 1197, "seek": 495696, "start": 4980.4, "end": 4981.76, "text": " current gradient.", "tokens": [2190, 16235, 13], "temperature": 0.0, "avg_logprob": -0.07656751589828663, "compression_ratio": 1.7345971563981042, "no_speech_prob": 3.0894464089215035e-06}, {"id": 1198, "seek": 495696, "start": 4981.76, "end": 4986.22, "text": " That's the definition of momentum.", "tokens": [663, 311, 264, 7123, 295, 11244, 13], "temperature": 0.0, "avg_logprob": -0.07656751589828663, "compression_ratio": 1.7345971563981042, "no_speech_prob": 3.0894464089215035e-06}, {"id": 1199, "seek": 498622, "start": 4986.22, "end": 4990.56, "text": " So this is an example of a stat class.", "tokens": [407, 341, 307, 364, 1365, 295, 257, 2219, 1508, 13], "temperature": 0.0, "avg_logprob": -0.10162023703257243, "compression_ratio": 1.6502242152466369, "no_speech_prob": 1.816205212890054e-06}, {"id": 1200, "seek": 498622, "start": 4990.56, "end": 4995.88, "text": " So it's not enough just to have update, because we actually need this to be something at the", "tokens": [407, 309, 311, 406, 1547, 445, 281, 362, 5623, 11, 570, 321, 767, 643, 341, 281, 312, 746, 412, 264], "temperature": 0.0, "avg_logprob": -0.10162023703257243, "compression_ratio": 1.6502242152466369, "no_speech_prob": 1.816205212890054e-06}, {"id": 1201, "seek": 498622, "start": 4995.88, "end": 4996.88, "text": " start.", "tokens": [722, 13], "temperature": 0.0, "avg_logprob": -0.10162023703257243, "compression_ratio": 1.6502242152466369, "no_speech_prob": 1.816205212890054e-06}, {"id": 1202, "seek": 498622, "start": 4996.88, "end": 4998.76, "text": " We can't multiply by something that doesn't exist.", "tokens": [492, 393, 380, 12972, 538, 746, 300, 1177, 380, 2514, 13], "temperature": 0.0, "avg_logprob": -0.10162023703257243, "compression_ratio": 1.6502242152466369, "no_speech_prob": 1.816205212890054e-06}, {"id": 1203, "seek": 498622, "start": 4998.76, "end": 5004.76, "text": " So we're also going to define something called init state that will create a dictionary containing", "tokens": [407, 321, 434, 611, 516, 281, 6964, 746, 1219, 3157, 1785, 300, 486, 1884, 257, 25890, 19273], "temperature": 0.0, "avg_logprob": -0.10162023703257243, "compression_ratio": 1.6502242152466369, "no_speech_prob": 1.816205212890054e-06}, {"id": 1204, "seek": 498622, "start": 5004.76, "end": 5006.6, "text": " the initial state.", "tokens": [264, 5883, 1785, 13], "temperature": 0.0, "avg_logprob": -0.10162023703257243, "compression_ratio": 1.6502242152466369, "no_speech_prob": 1.816205212890054e-06}, {"id": 1205, "seek": 498622, "start": 5006.6, "end": 5011.08, "text": " So that's all that stateful optimizer is going to do, right?", "tokens": [407, 300, 311, 439, 300, 1785, 906, 5028, 6545, 307, 516, 281, 360, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.10162023703257243, "compression_ratio": 1.6502242152466369, "no_speech_prob": 1.816205212890054e-06}, {"id": 1206, "seek": 501108, "start": 5011.08, "end": 5018.5199999999995, "text": " It's going to look at each of our parameters, and it's going to check to see whether that", "tokens": [467, 311, 516, 281, 574, 412, 1184, 295, 527, 9834, 11, 293, 309, 311, 516, 281, 1520, 281, 536, 1968, 300], "temperature": 0.0, "avg_logprob": -0.08633528403865481, "compression_ratio": 1.7797356828193833, "no_speech_prob": 8.939447070588358e-06}, {"id": 1207, "seek": 501108, "start": 5018.5199999999995, "end": 5021.5199999999995, "text": " parameter already exists in the state dictionary.", "tokens": [13075, 1217, 8198, 294, 264, 1785, 25890, 13], "temperature": 0.0, "avg_logprob": -0.08633528403865481, "compression_ratio": 1.7797356828193833, "no_speech_prob": 8.939447070588358e-06}, {"id": 1208, "seek": 501108, "start": 5021.5199999999995, "end": 5023.8, "text": " And if it doesn't, it hasn't been initialized.", "tokens": [400, 498, 309, 1177, 380, 11, 309, 6132, 380, 668, 5883, 1602, 13], "temperature": 0.0, "avg_logprob": -0.08633528403865481, "compression_ratio": 1.7797356828193833, "no_speech_prob": 8.939447070588358e-06}, {"id": 1209, "seek": 501108, "start": 5023.8, "end": 5028.76, "text": " So we'll initialize it with an empty dictionary, and then we'll update it with the results", "tokens": [407, 321, 603, 5883, 1125, 309, 365, 364, 6707, 25890, 11, 293, 550, 321, 603, 5623, 309, 365, 264, 3542], "temperature": 0.0, "avg_logprob": -0.08633528403865481, "compression_ratio": 1.7797356828193833, "no_speech_prob": 8.939447070588358e-06}, {"id": 1210, "seek": 501108, "start": 5028.76, "end": 5031.62, "text": " of that init state call we just saw.", "tokens": [295, 300, 3157, 1785, 818, 321, 445, 1866, 13], "temperature": 0.0, "avg_logprob": -0.08633528403865481, "compression_ratio": 1.7797356828193833, "no_speech_prob": 8.939447070588358e-06}, {"id": 1211, "seek": 501108, "start": 5031.62, "end": 5037.2, "text": " So now that we have every parameter can now be looked up in this state dictionary to find", "tokens": [407, 586, 300, 321, 362, 633, 13075, 393, 586, 312, 2956, 493, 294, 341, 1785, 25890, 281, 915], "temperature": 0.0, "avg_logprob": -0.08633528403865481, "compression_ratio": 1.7797356828193833, "no_speech_prob": 8.939447070588358e-06}, {"id": 1212, "seek": 503720, "start": 5037.2, "end": 5045.5599999999995, "text": " out its state, and we can now therefore grab it, and then we can call update like so.", "tokens": [484, 1080, 1785, 11, 293, 321, 393, 586, 4412, 4444, 309, 11, 293, 550, 321, 393, 818, 5623, 411, 370, 13], "temperature": 0.0, "avg_logprob": -0.1499312682585283, "compression_ratio": 1.6631016042780749, "no_speech_prob": 4.785025339515414e-06}, {"id": 1213, "seek": 503720, "start": 5045.5599999999995, "end": 5051.12, "text": " Well, this one's not opening, like so, to do, for example, average gradients.", "tokens": [1042, 11, 341, 472, 311, 406, 5193, 11, 411, 370, 11, 281, 360, 11, 337, 1365, 11, 4274, 2771, 2448, 13], "temperature": 0.0, "avg_logprob": -0.1499312682585283, "compression_ratio": 1.6631016042780749, "no_speech_prob": 4.785025339515414e-06}, {"id": 1214, "seek": 503720, "start": 5051.12, "end": 5057.96, "text": " And then we can call compose with our parameter and our steppers.", "tokens": [400, 550, 321, 393, 818, 35925, 365, 527, 13075, 293, 527, 2126, 15226, 13], "temperature": 0.0, "avg_logprob": -0.1499312682585283, "compression_ratio": 1.6631016042780749, "no_speech_prob": 4.785025339515414e-06}, {"id": 1215, "seek": 503720, "start": 5057.96, "end": 5063.5199999999995, "text": " And now we don't just pass in our hyperparameters, but we also pass in our state.", "tokens": [400, 586, 321, 500, 380, 445, 1320, 294, 527, 9848, 2181, 335, 6202, 11, 457, 321, 611, 1320, 294, 527, 1785, 13], "temperature": 0.0, "avg_logprob": -0.1499312682585283, "compression_ratio": 1.6631016042780749, "no_speech_prob": 4.785025339515414e-06}, {"id": 1216, "seek": 506352, "start": 5063.52, "end": 5070.64, "text": " So now that we have average gradients, which is sticking it into this thing called grad", "tokens": [407, 586, 300, 321, 362, 4274, 2771, 2448, 11, 597, 307, 13465, 309, 666, 341, 551, 1219, 2771], "temperature": 0.0, "avg_logprob": -0.09348625183105469, "compression_ratio": 1.7788461538461537, "no_speech_prob": 4.710863777290797e-06}, {"id": 1217, "seek": 506352, "start": 5070.64, "end": 5077.900000000001, "text": " average, and it's going to be passed into our steppers, we can now do a momentum step.", "tokens": [4274, 11, 293, 309, 311, 516, 281, 312, 4678, 666, 527, 2126, 15226, 11, 321, 393, 586, 360, 257, 11244, 1823, 13], "temperature": 0.0, "avg_logprob": -0.09348625183105469, "compression_ratio": 1.7788461538461537, "no_speech_prob": 4.710863777290797e-06}, {"id": 1218, "seek": 506352, "start": 5077.900000000001, "end": 5082.52, "text": " And the momentum step takes not just LR, but it's now going to be getting this grad average.", "tokens": [400, 264, 11244, 1823, 2516, 406, 445, 441, 49, 11, 457, 309, 311, 586, 516, 281, 312, 1242, 341, 2771, 4274, 13], "temperature": 0.0, "avg_logprob": -0.09348625183105469, "compression_ratio": 1.7788461538461537, "no_speech_prob": 4.710863777290797e-06}, {"id": 1219, "seek": 506352, "start": 5082.52, "end": 5086.22, "text": " And here is the momentum step.", "tokens": [400, 510, 307, 264, 11244, 1823, 13], "temperature": 0.0, "avg_logprob": -0.09348625183105469, "compression_ratio": 1.7788461538461537, "no_speech_prob": 4.710863777290797e-06}, {"id": 1220, "seek": 506352, "start": 5086.22, "end": 5091.4800000000005, "text": " It's just this grad average times the learning rate.", "tokens": [467, 311, 445, 341, 2771, 4274, 1413, 264, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.09348625183105469, "compression_ratio": 1.7788461538461537, "no_speech_prob": 4.710863777290797e-06}, {"id": 1221, "seek": 506352, "start": 5091.4800000000005, "end": 5093.200000000001, "text": " That's all you do.", "tokens": [663, 311, 439, 291, 360, 13], "temperature": 0.0, "avg_logprob": -0.09348625183105469, "compression_ratio": 1.7788461538461537, "no_speech_prob": 4.710863777290797e-06}, {"id": 1222, "seek": 509320, "start": 5093.2, "end": 5099.96, "text": " So now we can create an SGD with momentum optimizer with a line of code.", "tokens": [407, 586, 321, 393, 1884, 364, 34520, 35, 365, 11244, 5028, 6545, 365, 257, 1622, 295, 3089, 13], "temperature": 0.0, "avg_logprob": -0.13636294773646762, "compression_ratio": 1.5931034482758621, "no_speech_prob": 2.8129600195825333e-06}, {"id": 1223, "seek": 509320, "start": 5099.96, "end": 5102.32, "text": " It can have a momentum step.", "tokens": [467, 393, 362, 257, 11244, 1823, 13], "temperature": 0.0, "avg_logprob": -0.13636294773646762, "compression_ratio": 1.5931034482758621, "no_speech_prob": 2.8129600195825333e-06}, {"id": 1224, "seek": 509320, "start": 5102.32, "end": 5104.0, "text": " It can have a weight decay step.", "tokens": [467, 393, 362, 257, 3364, 21039, 1823, 13], "temperature": 0.0, "avg_logprob": -0.13636294773646762, "compression_ratio": 1.5931034482758621, "no_speech_prob": 2.8129600195825333e-06}, {"id": 1225, "seek": 509320, "start": 5104.0, "end": 5106.36, "text": " It can have an average grad stat.", "tokens": [467, 393, 362, 364, 4274, 2771, 2219, 13], "temperature": 0.0, "avg_logprob": -0.13636294773646762, "compression_ratio": 1.5931034482758621, "no_speech_prob": 2.8129600195825333e-06}, {"id": 1226, "seek": 509320, "start": 5106.36, "end": 5111.66, "text": " We can even give it some default weight decay.", "tokens": [492, 393, 754, 976, 309, 512, 7576, 3364, 21039, 13], "temperature": 0.0, "avg_logprob": -0.13636294773646762, "compression_ratio": 1.5931034482758621, "no_speech_prob": 2.8129600195825333e-06}, {"id": 1227, "seek": 509320, "start": 5111.66, "end": 5114.88, "text": " And away we go.", "tokens": [400, 1314, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.13636294773646762, "compression_ratio": 1.5931034482758621, "no_speech_prob": 2.8129600195825333e-06}, {"id": 1228, "seek": 511488, "start": 5114.88, "end": 5125.56, "text": " So here's something that might just blow your mind.", "tokens": [407, 510, 311, 746, 300, 1062, 445, 6327, 428, 1575, 13], "temperature": 0.0, "avg_logprob": -0.13981422301261656, "compression_ratio": 1.387878787878788, "no_speech_prob": 2.561230530773173e-06}, {"id": 1229, "seek": 511488, "start": 5125.56, "end": 5129.64, "text": " Let me read it to you.", "tokens": [961, 385, 1401, 309, 281, 291, 13], "temperature": 0.0, "avg_logprob": -0.13981422301261656, "compression_ratio": 1.387878787878788, "no_speech_prob": 2.561230530773173e-06}, {"id": 1230, "seek": 511488, "start": 5129.64, "end": 5130.64, "text": " Here is a paper.", "tokens": [1692, 307, 257, 3035, 13], "temperature": 0.0, "avg_logprob": -0.13981422301261656, "compression_ratio": 1.387878787878788, "no_speech_prob": 2.561230530773173e-06}, {"id": 1231, "seek": 511488, "start": 5130.64, "end": 5138.08, "text": " L2 regularization versus batch and weight norm.", "tokens": [441, 17, 3890, 2144, 5717, 15245, 293, 3364, 2026, 13], "temperature": 0.0, "avg_logprob": -0.13981422301261656, "compression_ratio": 1.387878787878788, "no_speech_prob": 2.561230530773173e-06}, {"id": 1232, "seek": 511488, "start": 5138.08, "end": 5142.8, "text": " Batch normalization is a commonly used trick to improve training of deep neural networks.", "tokens": [363, 852, 2710, 2144, 307, 257, 12719, 1143, 4282, 281, 3470, 3097, 295, 2452, 18161, 9590, 13], "temperature": 0.0, "avg_logprob": -0.13981422301261656, "compression_ratio": 1.387878787878788, "no_speech_prob": 2.561230530773173e-06}, {"id": 1233, "seek": 514280, "start": 5142.8, "end": 5148.28, "text": " And they also use L2 regularization ostensibly to prevent overfitting.", "tokens": [400, 436, 611, 764, 441, 17, 3890, 2144, 32946, 694, 3545, 281, 4871, 670, 69, 2414, 13], "temperature": 0.0, "avg_logprob": -0.21871559794356182, "compression_ratio": 1.4193548387096775, "no_speech_prob": 7.887903848313726e-06}, {"id": 1234, "seek": 514280, "start": 5148.28, "end": 5155.68, "text": " However, we show that L2 regularization has no regularizing effect.", "tokens": [2908, 11, 321, 855, 300, 441, 17, 3890, 2144, 575, 572, 3890, 3319, 1802, 13], "temperature": 0.0, "avg_logprob": -0.21871559794356182, "compression_ratio": 1.4193548387096775, "no_speech_prob": 7.887903848313726e-06}, {"id": 1235, "seek": 514280, "start": 5155.68, "end": 5158.08, "text": " What?", "tokens": [708, 30], "temperature": 0.0, "avg_logprob": -0.21871559794356182, "compression_ratio": 1.4193548387096775, "no_speech_prob": 7.887903848313726e-06}, {"id": 1236, "seek": 514280, "start": 5158.08, "end": 5161.360000000001, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.21871559794356182, "compression_ratio": 1.4193548387096775, "no_speech_prob": 7.887903848313726e-06}, {"id": 1237, "seek": 514280, "start": 5161.360000000001, "end": 5162.92, "text": " It's true.", "tokens": [467, 311, 2074, 13], "temperature": 0.0, "avg_logprob": -0.21871559794356182, "compression_ratio": 1.4193548387096775, "no_speech_prob": 7.887903848313726e-06}, {"id": 1238, "seek": 514280, "start": 5162.92, "end": 5166.72, "text": " Watch this.", "tokens": [7277, 341, 13], "temperature": 0.0, "avg_logprob": -0.21871559794356182, "compression_ratio": 1.4193548387096775, "no_speech_prob": 7.887903848313726e-06}, {"id": 1239, "seek": 514280, "start": 5166.72, "end": 5172.4800000000005, "text": " I realized this when I was chatting to Sylvain at NeurIPS, and like we were walking around", "tokens": [286, 5334, 341, 562, 286, 390, 24654, 281, 3902, 14574, 491, 412, 1734, 374, 40, 6273, 11, 293, 411, 321, 645, 4494, 926], "temperature": 0.0, "avg_logprob": -0.21871559794356182, "compression_ratio": 1.4193548387096775, "no_speech_prob": 7.887903848313726e-06}, {"id": 1240, "seek": 517248, "start": 5172.48, "end": 5181.5199999999995, "text": " the poster session, and I suddenly said to him, wait, Sylvain, if there's batch norm,", "tokens": [264, 17171, 5481, 11, 293, 286, 5800, 848, 281, 796, 11, 1699, 11, 3902, 14574, 491, 11, 498, 456, 311, 15245, 2026, 11], "temperature": 0.0, "avg_logprob": -0.13560671865204235, "compression_ratio": 1.4031413612565444, "no_speech_prob": 1.1477355656097643e-05}, {"id": 1241, "seek": 517248, "start": 5181.5199999999995, "end": 5184.719999999999, "text": " how can L2 regularization possibly work?", "tokens": [577, 393, 441, 17, 3890, 2144, 6264, 589, 30], "temperature": 0.0, "avg_logprob": -0.13560671865204235, "compression_ratio": 1.4031413612565444, "no_speech_prob": 1.1477355656097643e-05}, {"id": 1242, "seek": 517248, "start": 5184.719999999999, "end": 5186.919999999999, "text": " And I'll tell you what I laid out to him.", "tokens": [400, 286, 603, 980, 291, 437, 286, 9897, 484, 281, 796, 13], "temperature": 0.0, "avg_logprob": -0.13560671865204235, "compression_ratio": 1.4031413612565444, "no_speech_prob": 1.1477355656097643e-05}, {"id": 1243, "seek": 517248, "start": 5186.919999999999, "end": 5191.2, "text": " This is before I discovered this paper.", "tokens": [639, 307, 949, 286, 6941, 341, 3035, 13], "temperature": 0.0, "avg_logprob": -0.13560671865204235, "compression_ratio": 1.4031413612565444, "no_speech_prob": 1.1477355656097643e-05}, {"id": 1244, "seek": 517248, "start": 5191.2, "end": 5195.959999999999, "text": " We've got some layer of activations, right?", "tokens": [492, 600, 658, 512, 4583, 295, 2430, 763, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.13560671865204235, "compression_ratio": 1.4031413612565444, "no_speech_prob": 1.1477355656097643e-05}, {"id": 1245, "seek": 517248, "start": 5195.959999999999, "end": 5197.599999999999, "text": " And some layer.", "tokens": [400, 512, 4583, 13], "temperature": 0.0, "avg_logprob": -0.13560671865204235, "compression_ratio": 1.4031413612565444, "no_speech_prob": 1.1477355656097643e-05}, {"id": 1246, "seek": 519760, "start": 5197.6, "end": 5203.56, "text": " And we've got some weights that was used to create that layer of activations.", "tokens": [400, 321, 600, 658, 512, 17443, 300, 390, 1143, 281, 1884, 300, 4583, 295, 2430, 763, 13], "temperature": 0.0, "avg_logprob": -0.17445734435436772, "compression_ratio": 1.812807881773399, "no_speech_prob": 9.368302016810048e-06}, {"id": 1247, "seek": 519760, "start": 5203.56, "end": 5207.400000000001, "text": " So these are our weights, and these are our activations.", "tokens": [407, 613, 366, 527, 17443, 11, 293, 613, 366, 527, 2430, 763, 13], "temperature": 0.0, "avg_logprob": -0.17445734435436772, "compression_ratio": 1.812807881773399, "no_speech_prob": 9.368302016810048e-06}, {"id": 1248, "seek": 519760, "start": 5207.400000000001, "end": 5210.360000000001, "text": " And then we pass it through some batch norm layer, right?", "tokens": [400, 550, 321, 1320, 309, 807, 512, 15245, 2026, 4583, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.17445734435436772, "compression_ratio": 1.812807881773399, "no_speech_prob": 9.368302016810048e-06}, {"id": 1249, "seek": 519760, "start": 5210.360000000001, "end": 5212.68, "text": " And the batch norm layer does two things.", "tokens": [400, 264, 15245, 2026, 4583, 775, 732, 721, 13], "temperature": 0.0, "avg_logprob": -0.17445734435436772, "compression_ratio": 1.812807881773399, "no_speech_prob": 9.368302016810048e-06}, {"id": 1250, "seek": 519760, "start": 5212.68, "end": 5219.360000000001, "text": " It's got a bunch of adds, and it's got a bunch of multipliers, right?", "tokens": [467, 311, 658, 257, 3840, 295, 10860, 11, 293, 309, 311, 658, 257, 3840, 295, 12788, 4890, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.17445734435436772, "compression_ratio": 1.812807881773399, "no_speech_prob": 9.368302016810048e-06}, {"id": 1251, "seek": 519760, "start": 5219.360000000001, "end": 5222.8, "text": " It also normalizes, but these are the learned parameters.", "tokens": [467, 611, 2710, 5660, 11, 457, 613, 366, 264, 3264, 9834, 13], "temperature": 0.0, "avg_logprob": -0.17445734435436772, "compression_ratio": 1.812807881773399, "no_speech_prob": 9.368302016810048e-06}, {"id": 1252, "seek": 519760, "start": 5222.8, "end": 5223.8, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.17445734435436772, "compression_ratio": 1.812807881773399, "no_speech_prob": 9.368302016810048e-06}, {"id": 1253, "seek": 522380, "start": 5223.8, "end": 5233.4800000000005, "text": " So we come along and we say, okay, weight decay time, your weight decay is a million.", "tokens": [407, 321, 808, 2051, 293, 321, 584, 11, 1392, 11, 3364, 21039, 565, 11, 428, 3364, 21039, 307, 257, 2459, 13], "temperature": 0.0, "avg_logprob": -0.1986935778361995, "compression_ratio": 1.431578947368421, "no_speech_prob": 5.422062258730875e-06}, {"id": 1254, "seek": 522380, "start": 5233.4800000000005, "end": 5237.400000000001, "text": " And it goes, uh-oh, what do I do?", "tokens": [400, 309, 1709, 11, 2232, 12, 1445, 11, 437, 360, 286, 360, 30], "temperature": 0.0, "avg_logprob": -0.1986935778361995, "compression_ratio": 1.431578947368421, "no_speech_prob": 5.422062258730875e-06}, {"id": 1255, "seek": 522380, "start": 5237.400000000001, "end": 5244.12, "text": " Because now the squared of these, the sum of the squares of these gets multiplied by", "tokens": [1436, 586, 264, 8889, 295, 613, 11, 264, 2408, 295, 264, 19368, 295, 613, 2170, 17207, 538], "temperature": 0.0, "avg_logprob": -0.1986935778361995, "compression_ratio": 1.431578947368421, "no_speech_prob": 5.422062258730875e-06}, {"id": 1256, "seek": 522380, "start": 5244.12, "end": 5246.2, "text": " 1E6.", "tokens": [502, 36, 21, 13], "temperature": 0.0, "avg_logprob": -0.1986935778361995, "compression_ratio": 1.431578947368421, "no_speech_prob": 5.422062258730875e-06}, {"id": 1257, "seek": 522380, "start": 5246.2, "end": 5248.52, "text": " My loss function's destroyed.", "tokens": [1222, 4470, 2445, 311, 8937, 13], "temperature": 0.0, "avg_logprob": -0.1986935778361995, "compression_ratio": 1.431578947368421, "no_speech_prob": 5.422062258730875e-06}, {"id": 1258, "seek": 522380, "start": 5248.52, "end": 5250.64, "text": " I can't possibly learn anything.", "tokens": [286, 393, 380, 6264, 1466, 1340, 13], "temperature": 0.0, "avg_logprob": -0.1986935778361995, "compression_ratio": 1.431578947368421, "no_speech_prob": 5.422062258730875e-06}, {"id": 1259, "seek": 525064, "start": 5250.64, "end": 5256.240000000001, "text": " But then the batch norm layer goes, oh no, don't worry, friends.", "tokens": [583, 550, 264, 15245, 2026, 4583, 1709, 11, 1954, 572, 11, 500, 380, 3292, 11, 1855, 13], "temperature": 0.0, "avg_logprob": -0.24853934739765368, "compression_ratio": 1.447674418604651, "no_speech_prob": 3.393063479961711e-06}, {"id": 1260, "seek": 525064, "start": 5256.240000000001, "end": 5263.320000000001, "text": " And it fills every single one of these with 1 divided by a million.", "tokens": [400, 309, 22498, 633, 2167, 472, 295, 613, 365, 502, 6666, 538, 257, 2459, 13], "temperature": 0.0, "avg_logprob": -0.24853934739765368, "compression_ratio": 1.447674418604651, "no_speech_prob": 3.393063479961711e-06}, {"id": 1261, "seek": 525064, "start": 5263.320000000001, "end": 5265.4800000000005, "text": " Okay?", "tokens": [1033, 30], "temperature": 0.0, "avg_logprob": -0.24853934739765368, "compression_ratio": 1.447674418604651, "no_speech_prob": 3.393063479961711e-06}, {"id": 1262, "seek": 525064, "start": 5265.4800000000005, "end": 5268.76, "text": " So what just happened?", "tokens": [407, 437, 445, 2011, 30], "temperature": 0.0, "avg_logprob": -0.24853934739765368, "compression_ratio": 1.447674418604651, "no_speech_prob": 3.393063479961711e-06}, {"id": 1263, "seek": 525064, "start": 5268.76, "end": 5273.4400000000005, "text": " Well, no, sorry, it multiplies by positive.", "tokens": [1042, 11, 572, 11, 2597, 11, 309, 12788, 530, 538, 3353, 13], "temperature": 0.0, "avg_logprob": -0.24853934739765368, "compression_ratio": 1.447674418604651, "no_speech_prob": 3.393063479961711e-06}, {"id": 1264, "seek": 525064, "start": 5273.4400000000005, "end": 5278.88, "text": " Sorry, it multiplies them all by a million.", "tokens": [4919, 11, 309, 12788, 530, 552, 439, 538, 257, 2459, 13], "temperature": 0.0, "avg_logprob": -0.24853934739765368, "compression_ratio": 1.447674418604651, "no_speech_prob": 3.393063479961711e-06}, {"id": 1265, "seek": 527888, "start": 5278.88, "end": 5280.76, "text": " So what now happens?", "tokens": [407, 437, 586, 2314, 30], "temperature": 0.0, "avg_logprob": -0.12131032599024026, "compression_ratio": 1.6031746031746033, "no_speech_prob": 5.6823391787474975e-06}, {"id": 1266, "seek": 527888, "start": 5280.76, "end": 5284.92, "text": " Oh, these now have to get the same activations we had before.", "tokens": [876, 11, 613, 586, 362, 281, 483, 264, 912, 2430, 763, 321, 632, 949, 13], "temperature": 0.0, "avg_logprob": -0.12131032599024026, "compression_ratio": 1.6031746031746033, "no_speech_prob": 5.6823391787474975e-06}, {"id": 1267, "seek": 527888, "start": 5284.92, "end": 5293.6, "text": " All of our weights, so like W1, now have to get divided by a million to get the same result.", "tokens": [1057, 295, 527, 17443, 11, 370, 411, 343, 16, 11, 586, 362, 281, 483, 6666, 538, 257, 2459, 281, 483, 264, 912, 1874, 13], "temperature": 0.0, "avg_logprob": -0.12131032599024026, "compression_ratio": 1.6031746031746033, "no_speech_prob": 5.6823391787474975e-06}, {"id": 1268, "seek": 527888, "start": 5293.6, "end": 5299.84, "text": " And so now our weight decay basically is nothing.", "tokens": [400, 370, 586, 527, 3364, 21039, 1936, 307, 1825, 13], "temperature": 0.0, "avg_logprob": -0.12131032599024026, "compression_ratio": 1.6031746031746033, "no_speech_prob": 5.6823391787474975e-06}, {"id": 1269, "seek": 527888, "start": 5299.84, "end": 5308.84, "text": " So in other words, we can decide exactly how much weight decay loss there is.", "tokens": [407, 294, 661, 2283, 11, 321, 393, 4536, 2293, 577, 709, 3364, 21039, 4470, 456, 307, 13], "temperature": 0.0, "avg_logprob": -0.12131032599024026, "compression_ratio": 1.6031746031746033, "no_speech_prob": 5.6823391787474975e-06}, {"id": 1270, "seek": 530884, "start": 5308.84, "end": 5311.24, "text": " By simply using the batch norm malts.", "tokens": [3146, 2935, 1228, 264, 15245, 2026, 2806, 1373, 13], "temperature": 0.0, "avg_logprob": -0.18911294111116664, "compression_ratio": 1.6962962962962962, "no_speech_prob": 6.747964107489679e-06}, {"id": 1271, "seek": 530884, "start": 5311.24, "end": 5316.24, "text": " Now the batch norm malts get a tiny bit of weight decay applied to them, unless you turn", "tokens": [823, 264, 15245, 2026, 2806, 1373, 483, 257, 5870, 857, 295, 3364, 21039, 6456, 281, 552, 11, 5969, 291, 1261], "temperature": 0.0, "avg_logprob": -0.18911294111116664, "compression_ratio": 1.6962962962962962, "no_speech_prob": 6.747964107489679e-06}, {"id": 1272, "seek": 530884, "start": 5316.24, "end": 5317.6, "text": " it off, which people often do.", "tokens": [309, 766, 11, 597, 561, 2049, 360, 13], "temperature": 0.0, "avg_logprob": -0.18911294111116664, "compression_ratio": 1.6962962962962962, "no_speech_prob": 6.747964107489679e-06}, {"id": 1273, "seek": 530884, "start": 5317.6, "end": 5318.8, "text": " But it's tiny, right?", "tokens": [583, 309, 311, 5870, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.18911294111116664, "compression_ratio": 1.6962962962962962, "no_speech_prob": 6.747964107489679e-06}, {"id": 1274, "seek": 530884, "start": 5318.8, "end": 5324.56, "text": " Because there's very few parameters here, and there's lots of parameters here.", "tokens": [1436, 456, 311, 588, 1326, 9834, 510, 11, 293, 456, 311, 3195, 295, 9834, 510, 13], "temperature": 0.0, "avg_logprob": -0.18911294111116664, "compression_ratio": 1.6962962962962962, "no_speech_prob": 6.747964107489679e-06}, {"id": 1275, "seek": 530884, "start": 5324.56, "end": 5326.360000000001, "text": " So it's true.", "tokens": [407, 309, 311, 2074, 13], "temperature": 0.0, "avg_logprob": -0.18911294111116664, "compression_ratio": 1.6962962962962962, "no_speech_prob": 6.747964107489679e-06}, {"id": 1276, "seek": 530884, "start": 5326.360000000001, "end": 5327.360000000001, "text": " It's true.", "tokens": [467, 311, 2074, 13], "temperature": 0.0, "avg_logprob": -0.18911294111116664, "compression_ratio": 1.6962962962962962, "no_speech_prob": 6.747964107489679e-06}, {"id": 1277, "seek": 530884, "start": 5327.360000000001, "end": 5333.0, "text": " L2 regularization has no regularizing effect.", "tokens": [441, 17, 3890, 2144, 575, 572, 3890, 3319, 1802, 13], "temperature": 0.0, "avg_logprob": -0.18911294111116664, "compression_ratio": 1.6962962962962962, "no_speech_prob": 6.747964107489679e-06}, {"id": 1278, "seek": 530884, "start": 5333.0, "end": 5335.92, "text": " Which is not what I've been telling people who have been listening to these lessons the", "tokens": [3013, 307, 406, 437, 286, 600, 668, 3585, 561, 567, 362, 668, 4764, 281, 613, 8820, 264], "temperature": 0.0, "avg_logprob": -0.18911294111116664, "compression_ratio": 1.6962962962962962, "no_speech_prob": 6.747964107489679e-06}, {"id": 1279, "seek": 530884, "start": 5335.92, "end": 5338.72, "text": " last three years, for which I apologize.", "tokens": [1036, 1045, 924, 11, 337, 597, 286, 12328, 13], "temperature": 0.0, "avg_logprob": -0.18911294111116664, "compression_ratio": 1.6962962962962962, "no_speech_prob": 6.747964107489679e-06}, {"id": 1280, "seek": 533872, "start": 5338.72, "end": 5340.320000000001, "text": " I was wrong.", "tokens": [286, 390, 2085, 13], "temperature": 0.0, "avg_logprob": -0.20091787974039713, "compression_ratio": 1.4641148325358853, "no_speech_prob": 4.637395250028931e-06}, {"id": 1281, "seek": 533872, "start": 5340.320000000001, "end": 5344.360000000001, "text": " I feel a little bit better in knowing that pretty much everybody in the community is", "tokens": [286, 841, 257, 707, 857, 1101, 294, 5276, 300, 1238, 709, 2201, 294, 264, 1768, 307], "temperature": 0.0, "avg_logprob": -0.20091787974039713, "compression_ratio": 1.4641148325358853, "no_speech_prob": 4.637395250028931e-06}, {"id": 1282, "seek": 533872, "start": 5344.360000000001, "end": 5345.360000000001, "text": " wrong.", "tokens": [2085, 13], "temperature": 0.0, "avg_logprob": -0.20091787974039713, "compression_ratio": 1.4641148325358853, "no_speech_prob": 4.637395250028931e-06}, {"id": 1283, "seek": 533872, "start": 5345.360000000001, "end": 5348.400000000001, "text": " We've all been doing it wrong.", "tokens": [492, 600, 439, 668, 884, 309, 2085, 13], "temperature": 0.0, "avg_logprob": -0.20091787974039713, "compression_ratio": 1.4641148325358853, "no_speech_prob": 4.637395250028931e-06}, {"id": 1284, "seek": 533872, "start": 5348.400000000001, "end": 5354.4400000000005, "text": " So Tuan Van Laahovan mentioned this in the middle of 2017.", "tokens": [407, 314, 6139, 8979, 2369, 545, 5179, 282, 2835, 341, 294, 264, 2808, 295, 6591, 13], "temperature": 0.0, "avg_logprob": -0.20091787974039713, "compression_ratio": 1.4641148325358853, "no_speech_prob": 4.637395250028931e-06}, {"id": 1285, "seek": 533872, "start": 5354.4400000000005, "end": 5357.12, "text": " Basically nobody noticed.", "tokens": [8537, 5079, 5694, 13], "temperature": 0.0, "avg_logprob": -0.20091787974039713, "compression_ratio": 1.4641148325358853, "no_speech_prob": 4.637395250028931e-06}, {"id": 1286, "seek": 533872, "start": 5357.12, "end": 5363.2, "text": " There's a couple more papers I've mentioned in today's lesson notes from the last few", "tokens": [821, 311, 257, 1916, 544, 10577, 286, 600, 2835, 294, 965, 311, 6898, 5570, 490, 264, 1036, 1326], "temperature": 0.0, "avg_logprob": -0.20091787974039713, "compression_ratio": 1.4641148325358853, "no_speech_prob": 4.637395250028931e-06}, {"id": 1287, "seek": 536320, "start": 5363.2, "end": 5368.84, "text": " months where people are finally starting to really think about this.", "tokens": [2493, 689, 561, 366, 2721, 2891, 281, 534, 519, 466, 341, 13], "temperature": 0.0, "avg_logprob": -0.12073599354604657, "compression_ratio": 1.6046511627906976, "no_speech_prob": 2.3180265998234972e-05}, {"id": 1288, "seek": 536320, "start": 5368.84, "end": 5373.4, "text": " But I'm not aware of any other course which has actually pointed out we're all doing it", "tokens": [583, 286, 478, 406, 3650, 295, 604, 661, 1164, 597, 575, 767, 10932, 484, 321, 434, 439, 884, 309], "temperature": 0.0, "avg_logprob": -0.12073599354604657, "compression_ratio": 1.6046511627906976, "no_speech_prob": 2.3180265998234972e-05}, {"id": 1289, "seek": 536320, "start": 5373.4, "end": 5374.4, "text": " wrong.", "tokens": [2085, 13], "temperature": 0.0, "avg_logprob": -0.12073599354604657, "compression_ratio": 1.6046511627906976, "no_speech_prob": 2.3180265998234972e-05}, {"id": 1290, "seek": 536320, "start": 5374.4, "end": 5379.0, "text": " So you know how I keep mentioning how none of us know what we're doing?", "tokens": [407, 291, 458, 577, 286, 1066, 18315, 577, 6022, 295, 505, 458, 437, 321, 434, 884, 30], "temperature": 0.0, "avg_logprob": -0.12073599354604657, "compression_ratio": 1.6046511627906976, "no_speech_prob": 2.3180265998234972e-05}, {"id": 1291, "seek": 536320, "start": 5379.0, "end": 5386.72, "text": " We don't even know what L2 regularization does because it doesn't even do anything.", "tokens": [492, 500, 380, 754, 458, 437, 441, 17, 3890, 2144, 775, 570, 309, 1177, 380, 754, 360, 1340, 13], "temperature": 0.0, "avg_logprob": -0.12073599354604657, "compression_ratio": 1.6046511627906976, "no_speech_prob": 2.3180265998234972e-05}, {"id": 1292, "seek": 536320, "start": 5386.72, "end": 5388.4, "text": " But it does do something.", "tokens": [583, 309, 775, 360, 746, 13], "temperature": 0.0, "avg_logprob": -0.12073599354604657, "compression_ratio": 1.6046511627906976, "no_speech_prob": 2.3180265998234972e-05}, {"id": 1293, "seek": 538840, "start": 5388.4, "end": 5393.24, "text": " Because if you change it, something happens.", "tokens": [1436, 498, 291, 1319, 309, 11, 746, 2314, 13], "temperature": 0.0, "avg_logprob": -0.10768618583679199, "compression_ratio": 1.4871794871794872, "no_speech_prob": 3.21785882988479e-05}, {"id": 1294, "seek": 538840, "start": 5393.24, "end": 5396.759999999999, "text": " So this guy's wrong too.", "tokens": [407, 341, 2146, 311, 2085, 886, 13], "temperature": 0.0, "avg_logprob": -0.10768618583679199, "compression_ratio": 1.4871794871794872, "no_speech_prob": 3.21785882988479e-05}, {"id": 1295, "seek": 538840, "start": 5396.759999999999, "end": 5398.16, "text": " It doesn't do nothing.", "tokens": [467, 1177, 380, 360, 1825, 13], "temperature": 0.0, "avg_logprob": -0.10768618583679199, "compression_ratio": 1.4871794871794872, "no_speech_prob": 3.21785882988479e-05}, {"id": 1296, "seek": 538840, "start": 5398.16, "end": 5406.08, "text": " So a more recent paper by a team led by Roger Gross has found three kind of ways in which", "tokens": [407, 257, 544, 5162, 3035, 538, 257, 1469, 4684, 538, 17666, 34256, 575, 1352, 1045, 733, 295, 2098, 294, 597], "temperature": 0.0, "avg_logprob": -0.10768618583679199, "compression_ratio": 1.4871794871794872, "no_speech_prob": 3.21785882988479e-05}, {"id": 1297, "seek": 538840, "start": 5406.08, "end": 5410.599999999999, "text": " maybe regularization happens but it's not the way you think.", "tokens": [1310, 3890, 2144, 2314, 457, 309, 311, 406, 264, 636, 291, 519, 13], "temperature": 0.0, "avg_logprob": -0.10768618583679199, "compression_ratio": 1.4871794871794872, "no_speech_prob": 3.21785882988479e-05}, {"id": 1298, "seek": 538840, "start": 5410.599999999999, "end": 5413.5599999999995, "text": " This is one of the papers in the lesson notes.", "tokens": [639, 307, 472, 295, 264, 10577, 294, 264, 6898, 5570, 13], "temperature": 0.0, "avg_logprob": -0.10768618583679199, "compression_ratio": 1.4871794871794872, "no_speech_prob": 3.21785882988479e-05}, {"id": 1299, "seek": 541356, "start": 5413.56, "end": 5420.4800000000005, "text": " But even in his paper which is just a few months old, the abstract says basically, or", "tokens": [583, 754, 294, 702, 3035, 597, 307, 445, 257, 1326, 2493, 1331, 11, 264, 12649, 1619, 1936, 11, 420], "temperature": 0.0, "avg_logprob": -0.15332038923241625, "compression_ratio": 1.5898617511520738, "no_speech_prob": 2.3548969693365507e-05}, {"id": 1300, "seek": 541356, "start": 5420.4800000000005, "end": 5426.280000000001, "text": " the introduction says basically no one really understands what L2 regularization does.", "tokens": [264, 9339, 1619, 1936, 572, 472, 534, 15146, 437, 441, 17, 3890, 2144, 775, 13], "temperature": 0.0, "avg_logprob": -0.15332038923241625, "compression_ratio": 1.5898617511520738, "no_speech_prob": 2.3548969693365507e-05}, {"id": 1301, "seek": 541356, "start": 5426.280000000001, "end": 5428.92, "text": " So we have no idea what we're doing.", "tokens": [407, 321, 362, 572, 1558, 437, 321, 434, 884, 13], "temperature": 0.0, "avg_logprob": -0.15332038923241625, "compression_ratio": 1.5898617511520738, "no_speech_prob": 2.3548969693365507e-05}, {"id": 1302, "seek": 541356, "start": 5428.92, "end": 5435.56, "text": " There's this thing that every model ever always has and it totally doesn't work.", "tokens": [821, 311, 341, 551, 300, 633, 2316, 1562, 1009, 575, 293, 309, 3879, 1177, 380, 589, 13], "temperature": 0.0, "avg_logprob": -0.15332038923241625, "compression_ratio": 1.5898617511520738, "no_speech_prob": 2.3548969693365507e-05}, {"id": 1303, "seek": 541356, "start": 5435.56, "end": 5438.4800000000005, "text": " At least it doesn't work in the way we thought it did.", "tokens": [1711, 1935, 309, 1177, 380, 589, 294, 264, 636, 321, 1194, 309, 630, 13], "temperature": 0.0, "avg_logprob": -0.15332038923241625, "compression_ratio": 1.5898617511520738, "no_speech_prob": 2.3548969693365507e-05}, {"id": 1304, "seek": 543848, "start": 5438.48, "end": 5445.0, "text": " So that should make you feel better about can I contribute to deep learning?", "tokens": [407, 300, 820, 652, 291, 841, 1101, 466, 393, 286, 10586, 281, 2452, 2539, 30], "temperature": 0.0, "avg_logprob": -0.15436314669522372, "compression_ratio": 1.5985663082437276, "no_speech_prob": 7.571049422949727e-07}, {"id": 1305, "seek": 543848, "start": 5445.0, "end": 5449.879999999999, "text": " Obviously you can because none of us have any idea what we're doing.", "tokens": [7580, 291, 393, 570, 6022, 295, 505, 362, 604, 1558, 437, 321, 434, 884, 13], "temperature": 0.0, "avg_logprob": -0.15436314669522372, "compression_ratio": 1.5985663082437276, "no_speech_prob": 7.571049422949727e-07}, {"id": 1306, "seek": 543848, "start": 5449.879999999999, "end": 5452.36, "text": " And this is a great place to contribute.", "tokens": [400, 341, 307, 257, 869, 1081, 281, 10586, 13], "temperature": 0.0, "avg_logprob": -0.15436314669522372, "compression_ratio": 1.5985663082437276, "no_speech_prob": 7.571049422949727e-07}, {"id": 1307, "seek": 543848, "start": 5452.36, "end": 5456.759999999999, "text": " Use all this telemetry that I'm showing you, activations of different layers, and see what", "tokens": [8278, 439, 341, 4304, 5537, 627, 300, 286, 478, 4099, 291, 11, 2430, 763, 295, 819, 7914, 11, 293, 536, 437], "temperature": 0.0, "avg_logprob": -0.15436314669522372, "compression_ratio": 1.5985663082437276, "no_speech_prob": 7.571049422949727e-07}, {"id": 1308, "seek": 543848, "start": 5456.759999999999, "end": 5457.759999999999, "text": " happens experimentally.", "tokens": [2314, 5120, 379, 13], "temperature": 0.0, "avg_logprob": -0.15436314669522372, "compression_ratio": 1.5985663082437276, "no_speech_prob": 7.571049422949727e-07}, {"id": 1309, "seek": 543848, "start": 5457.759999999999, "end": 5463.44, "text": " Because the people who study this stuff, like what actually happens with batch norm and", "tokens": [1436, 264, 561, 567, 2979, 341, 1507, 11, 411, 437, 767, 2314, 365, 15245, 2026, 293], "temperature": 0.0, "avg_logprob": -0.15436314669522372, "compression_ratio": 1.5985663082437276, "no_speech_prob": 7.571049422949727e-07}, {"id": 1310, "seek": 543848, "start": 5463.44, "end": 5467.32, "text": " rate decay, most of them don't know how to train models.", "tokens": [3314, 21039, 11, 881, 295, 552, 500, 380, 458, 577, 281, 3847, 5245, 13], "temperature": 0.0, "avg_logprob": -0.15436314669522372, "compression_ratio": 1.5985663082437276, "no_speech_prob": 7.571049422949727e-07}, {"id": 1311, "seek": 546732, "start": 5467.32, "end": 5468.88, "text": " They're like the theory people.", "tokens": [814, 434, 411, 264, 5261, 561, 13], "temperature": 0.0, "avg_logprob": -0.14645727141564632, "compression_ratio": 1.7173144876325088, "no_speech_prob": 6.240607490326511e-06}, {"id": 1312, "seek": 546732, "start": 5468.88, "end": 5473.84, "text": " And then there's the practitioners who forget about actually thinking about the foundations", "tokens": [400, 550, 456, 311, 264, 25742, 567, 2870, 466, 767, 1953, 466, 264, 22467], "temperature": 0.0, "avg_logprob": -0.14645727141564632, "compression_ratio": 1.7173144876325088, "no_speech_prob": 6.240607490326511e-06}, {"id": 1313, "seek": 546732, "start": 5473.84, "end": 5474.84, "text": " at all.", "tokens": [412, 439, 13], "temperature": 0.0, "avg_logprob": -0.14645727141564632, "compression_ratio": 1.7173144876325088, "no_speech_prob": 6.240607490326511e-06}, {"id": 1314, "seek": 546732, "start": 5474.84, "end": 5479.12, "text": " But if you can combine the two and say like, oh, let's actually try some experiments.", "tokens": [583, 498, 291, 393, 10432, 264, 732, 293, 584, 411, 11, 1954, 11, 718, 311, 767, 853, 512, 12050, 13], "temperature": 0.0, "avg_logprob": -0.14645727141564632, "compression_ratio": 1.7173144876325088, "no_speech_prob": 6.240607490326511e-06}, {"id": 1315, "seek": 546732, "start": 5479.12, "end": 5483.32, "text": " Let's see what happens really when we change weight decay now that I assume we don't know", "tokens": [961, 311, 536, 437, 2314, 534, 562, 321, 1319, 3364, 21039, 586, 300, 286, 6552, 321, 500, 380, 458], "temperature": 0.0, "avg_logprob": -0.14645727141564632, "compression_ratio": 1.7173144876325088, "no_speech_prob": 6.240607490326511e-06}, {"id": 1316, "seek": 546732, "start": 5483.32, "end": 5484.88, "text": " what we're doing.", "tokens": [437, 321, 434, 884, 13], "temperature": 0.0, "avg_logprob": -0.14645727141564632, "compression_ratio": 1.7173144876325088, "no_speech_prob": 6.240607490326511e-06}, {"id": 1317, "seek": 546732, "start": 5484.88, "end": 5489.08, "text": " I'm sure you can find some really interesting results.", "tokens": [286, 478, 988, 291, 393, 915, 512, 534, 1880, 3542, 13], "temperature": 0.0, "avg_logprob": -0.14645727141564632, "compression_ratio": 1.7173144876325088, "no_speech_prob": 6.240607490326511e-06}, {"id": 1318, "seek": 546732, "start": 5489.08, "end": 5492.36, "text": " So momentum is also interesting.", "tokens": [407, 11244, 307, 611, 1880, 13], "temperature": 0.0, "avg_logprob": -0.14645727141564632, "compression_ratio": 1.7173144876325088, "no_speech_prob": 6.240607490326511e-06}, {"id": 1319, "seek": 546732, "start": 5492.36, "end": 5495.5599999999995, "text": " And we really don't understand much about how things like momentum work.", "tokens": [400, 321, 534, 500, 380, 1223, 709, 466, 577, 721, 411, 11244, 589, 13], "temperature": 0.0, "avg_logprob": -0.14645727141564632, "compression_ratio": 1.7173144876325088, "no_speech_prob": 6.240607490326511e-06}, {"id": 1320, "seek": 549556, "start": 5495.56, "end": 5498.8, "text": " But here's some nice pictures for you.", "tokens": [583, 510, 311, 512, 1481, 5242, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.08432085483105152, "compression_ratio": 1.6031746031746033, "no_speech_prob": 4.222720235702582e-06}, {"id": 1321, "seek": 549556, "start": 5498.8, "end": 5502.04, "text": " And hopefully it'll give you a bit of a sense of momentum.", "tokens": [400, 4696, 309, 603, 976, 291, 257, 857, 295, 257, 2020, 295, 11244, 13], "temperature": 0.0, "avg_logprob": -0.08432085483105152, "compression_ratio": 1.6031746031746033, "no_speech_prob": 4.222720235702582e-06}, {"id": 1322, "seek": 549556, "start": 5502.04, "end": 5506.900000000001, "text": " Let's create 200 numbers equally spaced between minus 4 and 4.", "tokens": [961, 311, 1884, 2331, 3547, 12309, 43766, 1296, 3175, 1017, 293, 1017, 13], "temperature": 0.0, "avg_logprob": -0.08432085483105152, "compression_ratio": 1.6031746031746033, "no_speech_prob": 4.222720235702582e-06}, {"id": 1323, "seek": 549556, "start": 5506.900000000001, "end": 5512.88, "text": " And then let's create another 200 random numbers that average 0.3.", "tokens": [400, 550, 718, 311, 1884, 1071, 2331, 4974, 3547, 300, 4274, 1958, 13, 18, 13], "temperature": 0.0, "avg_logprob": -0.08432085483105152, "compression_ratio": 1.6031746031746033, "no_speech_prob": 4.222720235702582e-06}, {"id": 1324, "seek": 549556, "start": 5512.88, "end": 5523.280000000001, "text": " And then let's create something that plots some function for these numbers.", "tokens": [400, 550, 718, 311, 1884, 746, 300, 28609, 512, 2445, 337, 613, 3547, 13], "temperature": 0.0, "avg_logprob": -0.08432085483105152, "compression_ratio": 1.6031746031746033, "no_speech_prob": 4.222720235702582e-06}, {"id": 1325, "seek": 552328, "start": 5523.28, "end": 5527.92, "text": " And we're going to look at this function for each value of something called beta.", "tokens": [400, 321, 434, 516, 281, 574, 412, 341, 2445, 337, 1184, 2158, 295, 746, 1219, 9861, 13], "temperature": 0.0, "avg_logprob": -0.11218464500025699, "compression_ratio": 1.8307692307692307, "no_speech_prob": 3.7852630612178473e-06}, {"id": 1326, "seek": 552328, "start": 5527.92, "end": 5531.36, "text": " And this is the function we're going to try plotting.", "tokens": [400, 341, 307, 264, 2445, 321, 434, 516, 281, 853, 41178, 13], "temperature": 0.0, "avg_logprob": -0.11218464500025699, "compression_ratio": 1.8307692307692307, "no_speech_prob": 3.7852630612178473e-06}, {"id": 1327, "seek": 552328, "start": 5531.36, "end": 5533.639999999999, "text": " And this is the momentum function.", "tokens": [400, 341, 307, 264, 11244, 2445, 13], "temperature": 0.0, "avg_logprob": -0.11218464500025699, "compression_ratio": 1.8307692307692307, "no_speech_prob": 3.7852630612178473e-06}, {"id": 1328, "seek": 552328, "start": 5533.639999999999, "end": 5534.639999999999, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.11218464500025699, "compression_ratio": 1.8307692307692307, "no_speech_prob": 3.7852630612178473e-06}, {"id": 1329, "seek": 552328, "start": 5534.639999999999, "end": 5544.24, "text": " So what happens if we plot this function for each value of beta for our data where the", "tokens": [407, 437, 2314, 498, 321, 7542, 341, 2445, 337, 1184, 2158, 295, 9861, 337, 527, 1412, 689, 264], "temperature": 0.0, "avg_logprob": -0.11218464500025699, "compression_ratio": 1.8307692307692307, "no_speech_prob": 3.7852630612178473e-06}, {"id": 1330, "seek": 552328, "start": 5544.24, "end": 5548.719999999999, "text": " y is random and average is 0.3?", "tokens": [288, 307, 4974, 293, 4274, 307, 1958, 13, 18, 30], "temperature": 0.0, "avg_logprob": -0.11218464500025699, "compression_ratio": 1.8307692307692307, "no_speech_prob": 3.7852630612178473e-06}, {"id": 1331, "seek": 552328, "start": 5548.719999999999, "end": 5552.24, "text": " So beta here is going to be our different values of momentum.", "tokens": [407, 9861, 510, 307, 516, 281, 312, 527, 819, 4190, 295, 11244, 13], "temperature": 0.0, "avg_logprob": -0.11218464500025699, "compression_ratio": 1.8307692307692307, "no_speech_prob": 3.7852630612178473e-06}, {"id": 1332, "seek": 555224, "start": 5552.24, "end": 5557.36, "text": " And you can see what happens is with very little momentum, you just get very bumpy,", "tokens": [400, 291, 393, 536, 437, 2314, 307, 365, 588, 707, 11244, 11, 291, 445, 483, 588, 49400, 11], "temperature": 0.0, "avg_logprob": -0.12630743246812087, "compression_ratio": 1.6196581196581197, "no_speech_prob": 1.482339598624094e-06}, {"id": 1333, "seek": 555224, "start": 5557.36, "end": 5558.44, "text": " very bumpy.", "tokens": [588, 49400, 13], "temperature": 0.0, "avg_logprob": -0.12630743246812087, "compression_ratio": 1.6196581196581197, "no_speech_prob": 1.482339598624094e-06}, {"id": 1334, "seek": 555224, "start": 5558.44, "end": 5563.44, "text": " Once you get up to a high momentum, you get a totally wrong answer.", "tokens": [3443, 291, 483, 493, 281, 257, 1090, 11244, 11, 291, 483, 257, 3879, 2085, 1867, 13], "temperature": 0.0, "avg_logprob": -0.12630743246812087, "compression_ratio": 1.6196581196581197, "no_speech_prob": 1.482339598624094e-06}, {"id": 1335, "seek": 555224, "start": 5563.44, "end": 5564.8, "text": " Why is this?", "tokens": [1545, 307, 341, 30], "temperature": 0.0, "avg_logprob": -0.12630743246812087, "compression_ratio": 1.6196581196581197, "no_speech_prob": 1.482339598624094e-06}, {"id": 1336, "seek": 555224, "start": 5564.8, "end": 5570.5599999999995, "text": " Because if you think about it, right, we're constantly saying 0.9 times whatever we had", "tokens": [1436, 498, 291, 519, 466, 309, 11, 558, 11, 321, 434, 6460, 1566, 1958, 13, 24, 1413, 2035, 321, 632], "temperature": 0.0, "avg_logprob": -0.12630743246812087, "compression_ratio": 1.6196581196581197, "no_speech_prob": 1.482339598624094e-06}, {"id": 1337, "seek": 555224, "start": 5570.5599999999995, "end": 5575.08, "text": " before plus the new thing.", "tokens": [949, 1804, 264, 777, 551, 13], "temperature": 0.0, "avg_logprob": -0.12630743246812087, "compression_ratio": 1.6196581196581197, "no_speech_prob": 1.482339598624094e-06}, {"id": 1338, "seek": 555224, "start": 5575.08, "end": 5579.12, "text": " Then basically you're continuing to say like, oh, the thing I had before times 0.9 plus", "tokens": [1396, 1936, 291, 434, 9289, 281, 584, 411, 11, 1954, 11, 264, 551, 286, 632, 949, 1413, 1958, 13, 24, 1804], "temperature": 0.0, "avg_logprob": -0.12630743246812087, "compression_ratio": 1.6196581196581197, "no_speech_prob": 1.482339598624094e-06}, {"id": 1339, "seek": 557912, "start": 5579.12, "end": 5582.76, "text": " the new thing and the things are all above 0.", "tokens": [264, 777, 551, 293, 264, 721, 366, 439, 3673, 1958, 13], "temperature": 0.0, "avg_logprob": -0.10422578224769005, "compression_ratio": 1.7765151515151516, "no_speech_prob": 9.972443876904435e-06}, {"id": 1340, "seek": 557912, "start": 5582.76, "end": 5586.0, "text": " So you end up with a number that's too high.", "tokens": [407, 291, 917, 493, 365, 257, 1230, 300, 311, 886, 1090, 13], "temperature": 0.0, "avg_logprob": -0.10422578224769005, "compression_ratio": 1.7765151515151516, "no_speech_prob": 9.972443876904435e-06}, {"id": 1341, "seek": 557912, "start": 5586.0, "end": 5592.44, "text": " And this is why if your momentum is too high and basically you're way away from where you", "tokens": [400, 341, 307, 983, 498, 428, 11244, 307, 886, 1090, 293, 1936, 291, 434, 636, 1314, 490, 689, 291], "temperature": 0.0, "avg_logprob": -0.10422578224769005, "compression_ratio": 1.7765151515151516, "no_speech_prob": 9.972443876904435e-06}, {"id": 1342, "seek": 557912, "start": 5592.44, "end": 5593.64, "text": " need to be in weight space.", "tokens": [643, 281, 312, 294, 3364, 1901, 13], "temperature": 0.0, "avg_logprob": -0.10422578224769005, "compression_ratio": 1.7765151515151516, "no_speech_prob": 9.972443876904435e-06}, {"id": 1343, "seek": 557912, "start": 5593.64, "end": 5596.64, "text": " So it keeps on saying go that way, go that way, go that way.", "tokens": [407, 309, 5965, 322, 1566, 352, 300, 636, 11, 352, 300, 636, 11, 352, 300, 636, 13], "temperature": 0.0, "avg_logprob": -0.10422578224769005, "compression_ratio": 1.7765151515151516, "no_speech_prob": 9.972443876904435e-06}, {"id": 1344, "seek": 557912, "start": 5596.64, "end": 5602.76, "text": " If you get that enough with a high momentum, it will literally shoot off far faster than", "tokens": [759, 291, 483, 300, 1547, 365, 257, 1090, 11244, 11, 309, 486, 3736, 3076, 766, 1400, 4663, 813], "temperature": 0.0, "avg_logprob": -0.10422578224769005, "compression_ratio": 1.7765151515151516, "no_speech_prob": 9.972443876904435e-06}, {"id": 1345, "seek": 557912, "start": 5602.76, "end": 5603.76, "text": " is reasonable.", "tokens": [307, 10585, 13], "temperature": 0.0, "avg_logprob": -0.10422578224769005, "compression_ratio": 1.7765151515151516, "no_speech_prob": 9.972443876904435e-06}, {"id": 1346, "seek": 557912, "start": 5603.76, "end": 5604.76, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.10422578224769005, "compression_ratio": 1.7765151515151516, "no_speech_prob": 9.972443876904435e-06}, {"id": 1347, "seek": 557912, "start": 5604.76, "end": 5608.42, "text": " So this will give you a sense of why you've got to be really careful with high momentums.", "tokens": [407, 341, 486, 976, 291, 257, 2020, 295, 983, 291, 600, 658, 281, 312, 534, 5026, 365, 1090, 1623, 8099, 13], "temperature": 0.0, "avg_logprob": -0.10422578224769005, "compression_ratio": 1.7765151515151516, "no_speech_prob": 9.972443876904435e-06}, {"id": 1348, "seek": 560842, "start": 5608.42, "end": 5616.92, "text": " It's literally biased to end up being a higher gradient than the actual gradient.", "tokens": [467, 311, 3736, 28035, 281, 917, 493, 885, 257, 2946, 16235, 813, 264, 3539, 16235, 13], "temperature": 0.0, "avg_logprob": -0.12352240391266651, "compression_ratio": 1.60752688172043, "no_speech_prob": 1.0952659067697823e-05}, {"id": 1349, "seek": 560842, "start": 5616.92, "end": 5619.64, "text": " So we can fix that.", "tokens": [407, 321, 393, 3191, 300, 13], "temperature": 0.0, "avg_logprob": -0.12352240391266651, "compression_ratio": 1.60752688172043, "no_speech_prob": 1.0952659067697823e-05}, {"id": 1350, "seek": 560842, "start": 5619.64, "end": 5623.22, "text": " Like when you think about it, this is kind of dumb, right?", "tokens": [1743, 562, 291, 519, 466, 309, 11, 341, 307, 733, 295, 10316, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.12352240391266651, "compression_ratio": 1.60752688172043, "no_speech_prob": 1.0952659067697823e-05}, {"id": 1351, "seek": 560842, "start": 5623.22, "end": 5626.68, "text": " Because we shouldn't be saying beta times average plus yi.", "tokens": [1436, 321, 4659, 380, 312, 1566, 9861, 1413, 4274, 1804, 288, 72, 13], "temperature": 0.0, "avg_logprob": -0.12352240391266651, "compression_ratio": 1.60752688172043, "no_speech_prob": 1.0952659067697823e-05}, {"id": 1352, "seek": 560842, "start": 5626.68, "end": 5634.54, "text": " We should be saying beta times average plus 1 minus beta times the other thing.", "tokens": [492, 820, 312, 1566, 9861, 1413, 4274, 1804, 502, 3175, 9861, 1413, 264, 661, 551, 13], "temperature": 0.0, "avg_logprob": -0.12352240391266651, "compression_ratio": 1.60752688172043, "no_speech_prob": 1.0952659067697823e-05}, {"id": 1353, "seek": 563454, "start": 5634.54, "end": 5638.48, "text": " So like dampen the thing that we're adding in.", "tokens": [407, 411, 19498, 268, 264, 551, 300, 321, 434, 5127, 294, 13], "temperature": 0.0, "avg_logprob": -0.2317975500355596, "compression_ratio": 1.5473684210526315, "no_speech_prob": 2.260285327793099e-06}, {"id": 1354, "seek": 563454, "start": 5638.48, "end": 5642.8, "text": " And that's called an exponentially weighted moving average, as we know.", "tokens": [400, 300, 311, 1219, 364, 37330, 32807, 2684, 4274, 11, 382, 321, 458, 13], "temperature": 0.0, "avg_logprob": -0.2317975500355596, "compression_ratio": 1.5473684210526315, "no_speech_prob": 2.260285327793099e-06}, {"id": 1355, "seek": 563454, "start": 5642.8, "end": 5646.96, "text": " Or LERP in PyTorch speak.", "tokens": [1610, 441, 1598, 47, 294, 9953, 51, 284, 339, 1710, 13], "temperature": 0.0, "avg_logprob": -0.2317975500355596, "compression_ratio": 1.5473684210526315, "no_speech_prob": 2.260285327793099e-06}, {"id": 1356, "seek": 563454, "start": 5646.96, "end": 5650.16, "text": " So let's plot the same thing as before, but this time with exponentially weighted moving", "tokens": [407, 718, 311, 7542, 264, 912, 551, 382, 949, 11, 457, 341, 565, 365, 37330, 32807, 2684], "temperature": 0.0, "avg_logprob": -0.2317975500355596, "compression_ratio": 1.5473684210526315, "no_speech_prob": 2.260285327793099e-06}, {"id": 1357, "seek": 563454, "start": 5650.16, "end": 5651.16, "text": " average.", "tokens": [4274, 13], "temperature": 0.0, "avg_logprob": -0.2317975500355596, "compression_ratio": 1.5473684210526315, "no_speech_prob": 2.260285327793099e-06}, {"id": 1358, "seek": 563454, "start": 5651.16, "end": 5652.16, "text": " Ah, perfect.", "tokens": [2438, 11, 2176, 13], "temperature": 0.0, "avg_logprob": -0.2317975500355596, "compression_ratio": 1.5473684210526315, "no_speech_prob": 2.260285327793099e-06}, {"id": 1359, "seek": 563454, "start": 5652.16, "end": 5653.16, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.2317975500355596, "compression_ratio": 1.5473684210526315, "no_speech_prob": 2.260285327793099e-06}, {"id": 1360, "seek": 563454, "start": 5653.16, "end": 5655.16, "text": " So we're done, right?", "tokens": [407, 321, 434, 1096, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2317975500355596, "compression_ratio": 1.5473684210526315, "no_speech_prob": 2.260285327793099e-06}, {"id": 1361, "seek": 563454, "start": 5655.16, "end": 5658.16, "text": " Not quite.", "tokens": [1726, 1596, 13], "temperature": 0.0, "avg_logprob": -0.2317975500355596, "compression_ratio": 1.5473684210526315, "no_speech_prob": 2.260285327793099e-06}, {"id": 1362, "seek": 565816, "start": 5658.16, "end": 5666.5599999999995, "text": " Not quite.What if the thing that we're trying to match isn't just random, but is some function?", "tokens": [1726, 1596, 13, 3748, 498, 264, 551, 300, 321, 434, 1382, 281, 2995, 1943, 380, 445, 4974, 11, 457, 307, 512, 2445, 30], "temperature": 0.0, "avg_logprob": -0.16896942945627066, "compression_ratio": 1.4958333333333333, "no_speech_prob": 4.092854396731127e-06}, {"id": 1363, "seek": 565816, "start": 5666.5599999999995, "end": 5668.5199999999995, "text": " So it looks something like this.", "tokens": [407, 309, 1542, 746, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.16896942945627066, "compression_ratio": 1.4958333333333333, "no_speech_prob": 4.092854396731127e-06}, {"id": 1364, "seek": 565816, "start": 5668.5199999999995, "end": 5674.08, "text": " Well, if we use a very small momentum with exponentially weighted moving averages, we're", "tokens": [1042, 11, 498, 321, 764, 257, 588, 1359, 11244, 365, 37330, 32807, 2684, 42257, 11, 321, 434], "temperature": 0.0, "avg_logprob": -0.16896942945627066, "compression_ratio": 1.4958333333333333, "no_speech_prob": 4.092854396731127e-06}, {"id": 1365, "seek": 565816, "start": 5674.08, "end": 5675.08, "text": " fine.", "tokens": [2489, 13], "temperature": 0.0, "avg_logprob": -0.16896942945627066, "compression_ratio": 1.4958333333333333, "no_speech_prob": 4.092854396731127e-06}, {"id": 1366, "seek": 565816, "start": 5675.08, "end": 5680.08, "text": " And I've added an outlier at the start just to show you what happens.", "tokens": [400, 286, 600, 3869, 364, 484, 2753, 412, 264, 722, 445, 281, 855, 291, 437, 2314, 13], "temperature": 0.0, "avg_logprob": -0.16896942945627066, "compression_ratio": 1.4958333333333333, "no_speech_prob": 4.092854396731127e-06}, {"id": 1367, "seek": 565816, "start": 5680.08, "end": 5682.68, "text": " Even with beta 0.7, we're fine.", "tokens": [2754, 365, 9861, 1958, 13, 22, 11, 321, 434, 2489, 13], "temperature": 0.0, "avg_logprob": -0.16896942945627066, "compression_ratio": 1.4958333333333333, "no_speech_prob": 4.092854396731127e-06}, {"id": 1368, "seek": 565816, "start": 5682.68, "end": 5686.32, "text": " But uh-oh, now we've got trouble.", "tokens": [583, 2232, 12, 1445, 11, 586, 321, 600, 658, 5253, 13], "temperature": 0.0, "avg_logprob": -0.16896942945627066, "compression_ratio": 1.4958333333333333, "no_speech_prob": 4.092854396731127e-06}, {"id": 1369, "seek": 568632, "start": 5686.32, "end": 5694.32, "text": " And the reason we've got trouble is that the second, third, fourth, fifth observations", "tokens": [400, 264, 1778, 321, 600, 658, 5253, 307, 300, 264, 1150, 11, 2636, 11, 6409, 11, 9266, 18163], "temperature": 0.0, "avg_logprob": -0.14204722359066918, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.862626949237892e-06}, {"id": 1370, "seek": 568632, "start": 5694.32, "end": 5698.88, "text": " all have a whole other lot of this item number one in, right?", "tokens": [439, 362, 257, 1379, 661, 688, 295, 341, 3174, 1230, 472, 294, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.14204722359066918, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.862626949237892e-06}, {"id": 1371, "seek": 568632, "start": 5698.88, "end": 5706.66, "text": " Because remember item number two is 0.99 times item number one plus 0.01 times item number", "tokens": [1436, 1604, 3174, 1230, 732, 307, 1958, 13, 8494, 1413, 3174, 1230, 472, 1804, 1958, 13, 10607, 1413, 3174, 1230], "temperature": 0.0, "avg_logprob": -0.14204722359066918, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.862626949237892e-06}, {"id": 1372, "seek": 568632, "start": 5706.66, "end": 5708.44, "text": " two.", "tokens": [732, 13], "temperature": 0.0, "avg_logprob": -0.14204722359066918, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.862626949237892e-06}, {"id": 1373, "seek": 568632, "start": 5708.44, "end": 5713.719999999999, "text": " And so item number one is massively biasing the start.", "tokens": [400, 370, 3174, 1230, 472, 307, 29379, 3228, 3349, 264, 722, 13], "temperature": 0.0, "avg_logprob": -0.14204722359066918, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.862626949237892e-06}, {"id": 1374, "seek": 568632, "start": 5713.719999999999, "end": 5714.719999999999, "text": " Even here.", "tokens": [2754, 510, 13], "temperature": 0.0, "avg_logprob": -0.14204722359066918, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.862626949237892e-06}, {"id": 1375, "seek": 571472, "start": 5714.72, "end": 5716.96, "text": " It takes a very long time.", "tokens": [467, 2516, 257, 588, 938, 565, 13], "temperature": 0.0, "avg_logprob": -0.15944969396797015, "compression_ratio": 1.713235294117647, "no_speech_prob": 1.6441130355815403e-05}, {"id": 1376, "seek": 571472, "start": 5716.96, "end": 5720.400000000001, "text": " And the second thing that goes wrong is with this momentum is that you see how we're a", "tokens": [400, 264, 1150, 551, 300, 1709, 2085, 307, 365, 341, 11244, 307, 300, 291, 536, 577, 321, 434, 257], "temperature": 0.0, "avg_logprob": -0.15944969396797015, "compression_ratio": 1.713235294117647, "no_speech_prob": 1.6441130355815403e-05}, {"id": 1377, "seek": 571472, "start": 5720.400000000001, "end": 5722.56, "text": " bit to the right of where we should be?", "tokens": [857, 281, 264, 558, 295, 689, 321, 820, 312, 30], "temperature": 0.0, "avg_logprob": -0.15944969396797015, "compression_ratio": 1.713235294117647, "no_speech_prob": 1.6441130355815403e-05}, {"id": 1378, "seek": 571472, "start": 5722.56, "end": 5725.240000000001, "text": " We're always running a bit behind where we should be.", "tokens": [492, 434, 1009, 2614, 257, 857, 2261, 689, 321, 820, 312, 13], "temperature": 0.0, "avg_logprob": -0.15944969396797015, "compression_ratio": 1.713235294117647, "no_speech_prob": 1.6441130355815403e-05}, {"id": 1379, "seek": 571472, "start": 5725.240000000001, "end": 5726.64, "text": " Which makes perfect sense, right?", "tokens": [3013, 1669, 2176, 2020, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.15944969396797015, "compression_ratio": 1.713235294117647, "no_speech_prob": 1.6441130355815403e-05}, {"id": 1380, "seek": 571472, "start": 5726.64, "end": 5731.280000000001, "text": " Because we're always only taking 0.1 times the new thing.", "tokens": [1436, 321, 434, 1009, 787, 1940, 1958, 13, 16, 1413, 264, 777, 551, 13], "temperature": 0.0, "avg_logprob": -0.15944969396797015, "compression_ratio": 1.713235294117647, "no_speech_prob": 1.6441130355815403e-05}, {"id": 1381, "seek": 571472, "start": 5731.280000000001, "end": 5734.0, "text": " So we can use de-biasing.", "tokens": [407, 321, 393, 764, 368, 12, 5614, 3349, 13], "temperature": 0.0, "avg_logprob": -0.15944969396797015, "compression_ratio": 1.713235294117647, "no_speech_prob": 1.6441130355815403e-05}, {"id": 1382, "seek": 571472, "start": 5734.0, "end": 5736.280000000001, "text": " De-biasing is what we saw last week.", "tokens": [1346, 12, 5614, 3349, 307, 437, 321, 1866, 1036, 1243, 13], "temperature": 0.0, "avg_logprob": -0.15944969396797015, "compression_ratio": 1.713235294117647, "no_speech_prob": 1.6441130355815403e-05}, {"id": 1383, "seek": 571472, "start": 5736.280000000001, "end": 5741.0, "text": " And it turned out, thanks to Stas Beckman's discovery, we didn't really need it.", "tokens": [400, 309, 3574, 484, 11, 3231, 281, 745, 296, 19184, 1601, 311, 12114, 11, 321, 994, 380, 534, 643, 309, 13], "temperature": 0.0, "avg_logprob": -0.15944969396797015, "compression_ratio": 1.713235294117647, "no_speech_prob": 1.6441130355815403e-05}, {"id": 1384, "seek": 571472, "start": 5741.0, "end": 5742.400000000001, "text": " But we do need it now.", "tokens": [583, 321, 360, 643, 309, 586, 13], "temperature": 0.0, "avg_logprob": -0.15944969396797015, "compression_ratio": 1.713235294117647, "no_speech_prob": 1.6441130355815403e-05}, {"id": 1385, "seek": 574240, "start": 5742.4, "end": 5748.799999999999, "text": " And de-biasing is to divide by 1 minus beta to the power of whatever batch number we're", "tokens": [400, 368, 12, 5614, 3349, 307, 281, 9845, 538, 502, 3175, 9861, 281, 264, 1347, 295, 2035, 15245, 1230, 321, 434], "temperature": 0.0, "avg_logprob": -0.11905235614416734, "compression_ratio": 1.5666666666666667, "no_speech_prob": 7.646224730706308e-06}, {"id": 1386, "seek": 574240, "start": 5748.799999999999, "end": 5750.599999999999, "text": " up to.", "tokens": [493, 281, 13], "temperature": 0.0, "avg_logprob": -0.11905235614416734, "compression_ratio": 1.5666666666666667, "no_speech_prob": 7.646224730706308e-06}, {"id": 1387, "seek": 574240, "start": 5750.599999999999, "end": 5752.44, "text": " So you can kind of tell, right?", "tokens": [407, 291, 393, 733, 295, 980, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.11905235614416734, "compression_ratio": 1.5666666666666667, "no_speech_prob": 7.646224730706308e-06}, {"id": 1388, "seek": 574240, "start": 5752.44, "end": 5758.32, "text": " If your initial starting point is 0, and that's what we use always when we're de-biasing.", "tokens": [759, 428, 5883, 2891, 935, 307, 1958, 11, 293, 300, 311, 437, 321, 764, 1009, 562, 321, 434, 368, 12, 5614, 3349, 13], "temperature": 0.0, "avg_logprob": -0.11905235614416734, "compression_ratio": 1.5666666666666667, "no_speech_prob": 7.646224730706308e-06}, {"id": 1389, "seek": 574240, "start": 5758.32, "end": 5760.2, "text": " We always start at 0.", "tokens": [492, 1009, 722, 412, 1958, 13], "temperature": 0.0, "avg_logprob": -0.11905235614416734, "compression_ratio": 1.5666666666666667, "no_speech_prob": 7.646224730706308e-06}, {"id": 1390, "seek": 574240, "start": 5760.2, "end": 5762.24, "text": " And beta is 0.9.", "tokens": [400, 9861, 307, 1958, 13, 24, 13], "temperature": 0.0, "avg_logprob": -0.11905235614416734, "compression_ratio": 1.5666666666666667, "no_speech_prob": 7.646224730706308e-06}, {"id": 1391, "seek": 574240, "start": 5762.24, "end": 5770.5199999999995, "text": " Then your first step is going to be 0.9 times 0 plus 0.1 times your item.", "tokens": [1396, 428, 700, 1823, 307, 516, 281, 312, 1958, 13, 24, 1413, 1958, 1804, 1958, 13, 16, 1413, 428, 3174, 13], "temperature": 0.0, "avg_logprob": -0.11905235614416734, "compression_ratio": 1.5666666666666667, "no_speech_prob": 7.646224730706308e-06}, {"id": 1392, "seek": 577052, "start": 5770.52, "end": 5773.040000000001, "text": " So in other words, you'll end up at 0.1 times your item.", "tokens": [407, 294, 661, 2283, 11, 291, 603, 917, 493, 412, 1958, 13, 16, 1413, 428, 3174, 13], "temperature": 0.0, "avg_logprob": -0.11495707346045453, "compression_ratio": 1.5056179775280898, "no_speech_prob": 3.0414798857236747e-06}, {"id": 1393, "seek": 577052, "start": 5773.040000000001, "end": 5775.92, "text": " So you're going to end up 10 times lower than you should be.", "tokens": [407, 291, 434, 516, 281, 917, 493, 1266, 1413, 3126, 813, 291, 820, 312, 13], "temperature": 0.0, "avg_logprob": -0.11495707346045453, "compression_ratio": 1.5056179775280898, "no_speech_prob": 3.0414798857236747e-06}, {"id": 1394, "seek": 577052, "start": 5775.92, "end": 5778.76, "text": " So you need to divide by 0.1.", "tokens": [407, 291, 643, 281, 9845, 538, 1958, 13, 16, 13], "temperature": 0.0, "avg_logprob": -0.11495707346045453, "compression_ratio": 1.5056179775280898, "no_speech_prob": 3.0414798857236747e-06}, {"id": 1395, "seek": 577052, "start": 5778.76, "end": 5790.320000000001, "text": " And if you kind of work through it, you'll see that each step is simply 0.1 to the power", "tokens": [400, 498, 291, 733, 295, 589, 807, 309, 11, 291, 603, 536, 300, 1184, 1823, 307, 2935, 1958, 13, 16, 281, 264, 1347], "temperature": 0.0, "avg_logprob": -0.11495707346045453, "compression_ratio": 1.5056179775280898, "no_speech_prob": 3.0414798857236747e-06}, {"id": 1396, "seek": 577052, "start": 5790.320000000001, "end": 5794.4400000000005, "text": " of 1, 2, 3, 4, 5, and so forth.", "tokens": [295, 502, 11, 568, 11, 805, 11, 1017, 11, 1025, 11, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.11495707346045453, "compression_ratio": 1.5056179775280898, "no_speech_prob": 3.0414798857236747e-06}, {"id": 1397, "seek": 579444, "start": 5794.44, "end": 5800.759999999999, "text": " And in fact, we have, of course, a spreadsheet showing you this.", "tokens": [400, 294, 1186, 11, 321, 362, 11, 295, 1164, 11, 257, 27733, 4099, 291, 341, 13], "temperature": 0.0, "avg_logprob": -0.1336430636319247, "compression_ratio": 1.5933014354066986, "no_speech_prob": 9.817958016355988e-06}, {"id": 1398, "seek": 579444, "start": 5800.759999999999, "end": 5807.32, "text": " So if you have a look at the momentum bias spreadsheet, there we go.", "tokens": [407, 498, 291, 362, 257, 574, 412, 264, 11244, 12577, 27733, 11, 456, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.1336430636319247, "compression_ratio": 1.5933014354066986, "no_speech_prob": 9.817958016355988e-06}, {"id": 1399, "seek": 579444, "start": 5807.32, "end": 5809.679999999999, "text": " So basically, here's our batch number.", "tokens": [407, 1936, 11, 510, 311, 527, 15245, 1230, 13], "temperature": 0.0, "avg_logprob": -0.1336430636319247, "compression_ratio": 1.5933014354066986, "no_speech_prob": 9.817958016355988e-06}, {"id": 1400, "seek": 579444, "start": 5809.679999999999, "end": 5815.719999999999, "text": " And let's say these are the values that are coming in, our gradients, 5, 1, 1, 1, 1, 1,", "tokens": [400, 718, 311, 584, 613, 366, 264, 4190, 300, 366, 1348, 294, 11, 527, 2771, 2448, 11, 1025, 11, 502, 11, 502, 11, 502, 11, 502, 11, 502, 11], "temperature": 0.0, "avg_logprob": -0.1336430636319247, "compression_ratio": 1.5933014354066986, "no_speech_prob": 9.817958016355988e-06}, {"id": 1401, "seek": 579444, "start": 5815.719999999999, "end": 5816.719999999999, "text": " 5, 1.", "tokens": [1025, 11, 502, 13], "temperature": 0.0, "avg_logprob": -0.1336430636319247, "compression_ratio": 1.5933014354066986, "no_speech_prob": 9.817958016355988e-06}, {"id": 1402, "seek": 579444, "start": 5816.719999999999, "end": 5821.96, "text": " Then basically, this is our exponentially weighted moving average.", "tokens": [1396, 1936, 11, 341, 307, 527, 37330, 32807, 2684, 4274, 13], "temperature": 0.0, "avg_logprob": -0.1336430636319247, "compression_ratio": 1.5933014354066986, "no_speech_prob": 9.817958016355988e-06}, {"id": 1403, "seek": 582196, "start": 5821.96, "end": 5826.32, "text": " And here is our debiasing correction.", "tokens": [400, 510, 307, 527, 3001, 72, 3349, 19984, 13], "temperature": 0.0, "avg_logprob": -0.08348574845687202, "compression_ratio": 1.6666666666666667, "no_speech_prob": 7.410989383060951e-06}, {"id": 1404, "seek": 582196, "start": 5826.32, "end": 5831.28, "text": " And then here is our resulting debiased exponentially weighted moving average.", "tokens": [400, 550, 510, 307, 527, 16505, 3001, 72, 1937, 37330, 32807, 2684, 4274, 13], "temperature": 0.0, "avg_logprob": -0.08348574845687202, "compression_ratio": 1.6666666666666667, "no_speech_prob": 7.410989383060951e-06}, {"id": 1405, "seek": 582196, "start": 5831.28, "end": 5837.64, "text": " And then you can compare it to an actual moving average of the last few.", "tokens": [400, 550, 291, 393, 6794, 309, 281, 364, 3539, 2684, 4274, 295, 264, 1036, 1326, 13], "temperature": 0.0, "avg_logprob": -0.08348574845687202, "compression_ratio": 1.6666666666666667, "no_speech_prob": 7.410989383060951e-06}, {"id": 1406, "seek": 582196, "start": 5837.64, "end": 5841.04, "text": " So that's basically how this works.", "tokens": [407, 300, 311, 1936, 577, 341, 1985, 13], "temperature": 0.0, "avg_logprob": -0.08348574845687202, "compression_ratio": 1.6666666666666667, "no_speech_prob": 7.410989383060951e-06}, {"id": 1407, "seek": 582196, "start": 5841.04, "end": 5843.6, "text": " And Sylvain loves writing LaTeX.", "tokens": [400, 3902, 14574, 491, 6752, 3579, 2369, 14233, 55, 13], "temperature": 0.0, "avg_logprob": -0.08348574845687202, "compression_ratio": 1.6666666666666667, "no_speech_prob": 7.410989383060951e-06}, {"id": 1408, "seek": 582196, "start": 5843.6, "end": 5849.28, "text": " So he wrote all this LaTeX that basically points out that if you say what I just said,", "tokens": [407, 415, 4114, 439, 341, 2369, 14233, 55, 300, 1936, 2793, 484, 300, 498, 291, 584, 437, 286, 445, 848, 11], "temperature": 0.0, "avg_logprob": -0.08348574845687202, "compression_ratio": 1.6666666666666667, "no_speech_prob": 7.410989383060951e-06}, {"id": 1409, "seek": 584928, "start": 5849.28, "end": 5854.84, "text": " which is beta times this plus 1 minus beta times that, and you keep doing it to itself", "tokens": [597, 307, 9861, 1413, 341, 1804, 502, 3175, 9861, 1413, 300, 11, 293, 291, 1066, 884, 309, 281, 2564], "temperature": 0.0, "avg_logprob": -0.10266331369562666, "compression_ratio": 1.7386363636363635, "no_speech_prob": 1.520591558801243e-05}, {"id": 1410, "seek": 584928, "start": 5854.84, "end": 5860.08, "text": " lots and lots of times, you end up with something that they all cancel out to that.", "tokens": [3195, 293, 3195, 295, 1413, 11, 291, 917, 493, 365, 746, 300, 436, 439, 10373, 484, 281, 300, 13], "temperature": 0.0, "avg_logprob": -0.10266331369562666, "compression_ratio": 1.7386363636363635, "no_speech_prob": 1.520591558801243e-05}, {"id": 1411, "seek": 584928, "start": 5860.08, "end": 5865.639999999999, "text": " So this is all we need to do to take our exponentially weighted moving average, divide it by 1 minus", "tokens": [407, 341, 307, 439, 321, 643, 281, 360, 281, 747, 527, 37330, 32807, 2684, 4274, 11, 9845, 309, 538, 502, 3175], "temperature": 0.0, "avg_logprob": -0.10266331369562666, "compression_ratio": 1.7386363636363635, "no_speech_prob": 1.520591558801243e-05}, {"id": 1412, "seek": 584928, "start": 5865.639999999999, "end": 5869.28, "text": " beta to the power of i plus 1.", "tokens": [9861, 281, 264, 1347, 295, 741, 1804, 502, 13], "temperature": 0.0, "avg_logprob": -0.10266331369562666, "compression_ratio": 1.7386363636363635, "no_speech_prob": 1.520591558801243e-05}, {"id": 1413, "seek": 584928, "start": 5869.28, "end": 5870.28, "text": " And look at that.", "tokens": [400, 574, 412, 300, 13], "temperature": 0.0, "avg_logprob": -0.10266331369562666, "compression_ratio": 1.7386363636363635, "no_speech_prob": 1.520591558801243e-05}, {"id": 1414, "seek": 584928, "start": 5870.28, "end": 5871.719999999999, "text": " It's pretty good, right?", "tokens": [467, 311, 1238, 665, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.10266331369562666, "compression_ratio": 1.7386363636363635, "no_speech_prob": 1.520591558801243e-05}, {"id": 1415, "seek": 584928, "start": 5871.719999999999, "end": 5874.88, "text": " It debiases very quickly, even if you have a bad starting point.", "tokens": [467, 3001, 72, 1957, 588, 2661, 11, 754, 498, 291, 362, 257, 1578, 2891, 935, 13], "temperature": 0.0, "avg_logprob": -0.10266331369562666, "compression_ratio": 1.7386363636363635, "no_speech_prob": 1.520591558801243e-05}, {"id": 1416, "seek": 584928, "start": 5874.88, "end": 5876.8, "text": " And it looks pretty good.", "tokens": [400, 309, 1542, 1238, 665, 13], "temperature": 0.0, "avg_logprob": -0.10266331369562666, "compression_ratio": 1.7386363636363635, "no_speech_prob": 1.520591558801243e-05}, {"id": 1417, "seek": 584928, "start": 5876.8, "end": 5878.599999999999, "text": " It's not magic, right?", "tokens": [467, 311, 406, 5585, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.10266331369562666, "compression_ratio": 1.7386363636363635, "no_speech_prob": 1.520591558801243e-05}, {"id": 1418, "seek": 587860, "start": 5878.6, "end": 5883.08, "text": " But you can see why a beta of 0.9 is popular.", "tokens": [583, 291, 393, 536, 983, 257, 9861, 295, 1958, 13, 24, 307, 3743, 13], "temperature": 0.0, "avg_logprob": -0.18537293619184353, "compression_ratio": 1.356164383561644, "no_speech_prob": 1.0348456953579444e-06}, {"id": 1419, "seek": 587860, "start": 5883.08, "end": 5886.320000000001, "text": " It's kind of got a pretty nice behavior.", "tokens": [467, 311, 733, 295, 658, 257, 1238, 1481, 5223, 13], "temperature": 0.0, "avg_logprob": -0.18537293619184353, "compression_ratio": 1.356164383561644, "no_speech_prob": 1.0348456953579444e-06}, {"id": 1420, "seek": 587860, "start": 5886.320000000001, "end": 5890.360000000001, "text": " So let's use all that to create Adam.", "tokens": [407, 718, 311, 764, 439, 300, 281, 1884, 7938, 13], "temperature": 0.0, "avg_logprob": -0.18537293619184353, "compression_ratio": 1.356164383561644, "no_speech_prob": 1.0348456953579444e-06}, {"id": 1421, "seek": 587860, "start": 5890.360000000001, "end": 5891.84, "text": " So what's Adam?", "tokens": [407, 437, 311, 7938, 30], "temperature": 0.0, "avg_logprob": -0.18537293619184353, "compression_ratio": 1.356164383561644, "no_speech_prob": 1.0348456953579444e-06}, {"id": 1422, "seek": 587860, "start": 5891.84, "end": 5897.76, "text": " Adam is dampened debiased momentum.", "tokens": [7938, 307, 19498, 5320, 3001, 72, 1937, 11244, 13], "temperature": 0.0, "avg_logprob": -0.18537293619184353, "compression_ratio": 1.356164383561644, "no_speech_prob": 1.0348456953579444e-06}, {"id": 1423, "seek": 587860, "start": 5897.76, "end": 5899.76, "text": " That's the numerator.", "tokens": [663, 311, 264, 30380, 13], "temperature": 0.0, "avg_logprob": -0.18537293619184353, "compression_ratio": 1.356164383561644, "no_speech_prob": 1.0348456953579444e-06}, {"id": 1424, "seek": 589976, "start": 5899.76, "end": 5909.4400000000005, "text": " Dampened by dampened debiased root sum of squared gradients.", "tokens": [413, 1215, 5320, 538, 19498, 5320, 3001, 72, 1937, 5593, 2408, 295, 8889, 2771, 2448, 13], "temperature": 0.0, "avg_logprob": -0.17376010934102168, "compression_ratio": 1.5566037735849056, "no_speech_prob": 4.029407591588097e-06}, {"id": 1425, "seek": 589976, "start": 5909.4400000000005, "end": 5911.84, "text": " And so we talked about why Adam does that before.", "tokens": [400, 370, 321, 2825, 466, 983, 7938, 775, 300, 949, 13], "temperature": 0.0, "avg_logprob": -0.17376010934102168, "compression_ratio": 1.5566037735849056, "no_speech_prob": 4.029407591588097e-06}, {"id": 1426, "seek": 589976, "start": 5911.84, "end": 5914.400000000001, "text": " We won't go into the details.", "tokens": [492, 1582, 380, 352, 666, 264, 4365, 13], "temperature": 0.0, "avg_logprob": -0.17376010934102168, "compression_ratio": 1.5566037735849056, "no_speech_prob": 4.029407591588097e-06}, {"id": 1427, "seek": 589976, "start": 5914.400000000001, "end": 5916.92, "text": " But here's our average gradient again.", "tokens": [583, 510, 311, 527, 4274, 16235, 797, 13], "temperature": 0.0, "avg_logprob": -0.17376010934102168, "compression_ratio": 1.5566037735849056, "no_speech_prob": 4.029407591588097e-06}, {"id": 1428, "seek": 589976, "start": 5916.92, "end": 5920.52, "text": " But this time we've added optional dampening.", "tokens": [583, 341, 565, 321, 600, 3869, 17312, 19498, 4559, 13], "temperature": 0.0, "avg_logprob": -0.17376010934102168, "compression_ratio": 1.5566037735849056, "no_speech_prob": 4.029407591588097e-06}, {"id": 1429, "seek": 589976, "start": 5920.52, "end": 5925.72, "text": " So if you say I want dampening, then we'll set momentum dampening to that.", "tokens": [407, 498, 291, 584, 286, 528, 19498, 4559, 11, 550, 321, 603, 992, 11244, 19498, 4559, 281, 300, 13], "temperature": 0.0, "avg_logprob": -0.17376010934102168, "compression_ratio": 1.5566037735849056, "no_speech_prob": 4.029407591588097e-06}, {"id": 1430, "seek": 589976, "start": 5925.72, "end": 5929.52, "text": " Otherwise, we'll set it to 1.", "tokens": [10328, 11, 321, 603, 992, 309, 281, 502, 13], "temperature": 0.0, "avg_logprob": -0.17376010934102168, "compression_ratio": 1.5566037735849056, "no_speech_prob": 4.029407591588097e-06}, {"id": 1431, "seek": 592952, "start": 5929.52, "end": 5934.76, "text": " And then so this is exactly the same as before, but with dampening.", "tokens": [400, 550, 370, 341, 307, 2293, 264, 912, 382, 949, 11, 457, 365, 19498, 4559, 13], "temperature": 0.0, "avg_logprob": -0.11826626586914063, "compression_ratio": 1.9327731092436975, "no_speech_prob": 9.817867976380512e-06}, {"id": 1432, "seek": 592952, "start": 5934.76, "end": 5938.040000000001, "text": " Average squared gradients is exactly the same as average gradients.", "tokens": [316, 3623, 8889, 2771, 2448, 307, 2293, 264, 912, 382, 4274, 2771, 2448, 13], "temperature": 0.0, "avg_logprob": -0.11826626586914063, "compression_ratio": 1.9327731092436975, "no_speech_prob": 9.817867976380512e-06}, {"id": 1433, "seek": 592952, "start": 5938.040000000001, "end": 5941.0, "text": " We could definitely refactor these a lot.", "tokens": [492, 727, 2138, 1895, 15104, 613, 257, 688, 13], "temperature": 0.0, "avg_logprob": -0.11826626586914063, "compression_ratio": 1.9327731092436975, "no_speech_prob": 9.817867976380512e-06}, {"id": 1434, "seek": 592952, "start": 5941.0, "end": 5944.240000000001, "text": " So this is all exactly the same as before, except we'll call them different things.", "tokens": [407, 341, 307, 439, 2293, 264, 912, 382, 949, 11, 3993, 321, 603, 818, 552, 819, 721, 13], "temperature": 0.0, "avg_logprob": -0.11826626586914063, "compression_ratio": 1.9327731092436975, "no_speech_prob": 9.817867976380512e-06}, {"id": 1435, "seek": 592952, "start": 5944.240000000001, "end": 5945.240000000001, "text": " We'll call it squared dampening.", "tokens": [492, 603, 818, 309, 8889, 19498, 4559, 13], "temperature": 0.0, "avg_logprob": -0.11826626586914063, "compression_ratio": 1.9327731092436975, "no_speech_prob": 9.817867976380512e-06}, {"id": 1436, "seek": 592952, "start": 5945.240000000001, "end": 5947.320000000001, "text": " We'll call it squared averages.", "tokens": [492, 603, 818, 309, 8889, 42257, 13], "temperature": 0.0, "avg_logprob": -0.11826626586914063, "compression_ratio": 1.9327731092436975, "no_speech_prob": 9.817867976380512e-06}, {"id": 1437, "seek": 592952, "start": 5947.320000000001, "end": 5955.160000000001, "text": " And this time, rather than just adding in the p grad data, we will multiply p grad data", "tokens": [400, 341, 565, 11, 2831, 813, 445, 5127, 294, 264, 280, 2771, 1412, 11, 321, 486, 12972, 280, 2771, 1412], "temperature": 0.0, "avg_logprob": -0.11826626586914063, "compression_ratio": 1.9327731092436975, "no_speech_prob": 9.817867976380512e-06}, {"id": 1438, "seek": 592952, "start": 5955.160000000001, "end": 5956.160000000001, "text": " by itself.", "tokens": [538, 2564, 13], "temperature": 0.0, "avg_logprob": -0.11826626586914063, "compression_ratio": 1.9327731092436975, "no_speech_prob": 9.817867976380512e-06}, {"id": 1439, "seek": 592952, "start": 5956.160000000001, "end": 5958.200000000001, "text": " In other words, we get the squids.", "tokens": [682, 661, 2283, 11, 321, 483, 264, 2339, 3742, 13], "temperature": 0.0, "avg_logprob": -0.11826626586914063, "compression_ratio": 1.9327731092436975, "no_speech_prob": 9.817867976380512e-06}, {"id": 1440, "seek": 595820, "start": 5958.2, "end": 5960.599999999999, "text": " This is the only difference.", "tokens": [639, 307, 264, 787, 2649, 13], "temperature": 0.0, "avg_logprob": -0.1413241090445683, "compression_ratio": 1.6946902654867257, "no_speech_prob": 6.240577022254001e-06}, {"id": 1441, "seek": 595820, "start": 5960.599999999999, "end": 5962.0, "text": " We throw it in a different name.", "tokens": [492, 3507, 309, 294, 257, 819, 1315, 13], "temperature": 0.0, "avg_logprob": -0.1413241090445683, "compression_ratio": 1.6946902654867257, "no_speech_prob": 6.240577022254001e-06}, {"id": 1442, "seek": 595820, "start": 5962.0, "end": 5967.16, "text": " So with those, we're also going to need to debias, which means we need to know what step", "tokens": [407, 365, 729, 11, 321, 434, 611, 516, 281, 643, 281, 3001, 4609, 11, 597, 1355, 321, 643, 281, 458, 437, 1823], "temperature": 0.0, "avg_logprob": -0.1413241090445683, "compression_ratio": 1.6946902654867257, "no_speech_prob": 6.240577022254001e-06}, {"id": 1443, "seek": 595820, "start": 5967.16, "end": 5968.34, "text": " we're up to.", "tokens": [321, 434, 493, 281, 13], "temperature": 0.0, "avg_logprob": -0.1413241090445683, "compression_ratio": 1.6946902654867257, "no_speech_prob": 6.240577022254001e-06}, {"id": 1444, "seek": 595820, "start": 5968.34, "end": 5974.58, "text": " So here's a stat which just literally counts.", "tokens": [407, 510, 311, 257, 2219, 597, 445, 3736, 14893, 13], "temperature": 0.0, "avg_logprob": -0.1413241090445683, "compression_ratio": 1.6946902654867257, "no_speech_prob": 6.240577022254001e-06}, {"id": 1445, "seek": 595820, "start": 5974.58, "end": 5978.76, "text": " So here's our debias function, the one we just saw.", "tokens": [407, 510, 311, 527, 3001, 4609, 2445, 11, 264, 472, 321, 445, 1866, 13], "temperature": 0.0, "avg_logprob": -0.1413241090445683, "compression_ratio": 1.6946902654867257, "no_speech_prob": 6.240577022254001e-06}, {"id": 1446, "seek": 595820, "start": 5978.76, "end": 5979.76, "text": " And so here's Adam.", "tokens": [400, 370, 510, 311, 7938, 13], "temperature": 0.0, "avg_logprob": -0.1413241090445683, "compression_ratio": 1.6946902654867257, "no_speech_prob": 6.240577022254001e-06}, {"id": 1447, "seek": 595820, "start": 5979.76, "end": 5980.76, "text": " Right?", "tokens": [1779, 30], "temperature": 0.0, "avg_logprob": -0.1413241090445683, "compression_ratio": 1.6946902654867257, "no_speech_prob": 6.240577022254001e-06}, {"id": 1448, "seek": 595820, "start": 5980.76, "end": 5985.08, "text": " Once that's in place, Adam is just the debiased momentum with momentum dampening, the debiased", "tokens": [3443, 300, 311, 294, 1081, 11, 7938, 307, 445, 264, 3001, 4609, 292, 11244, 365, 11244, 19498, 4559, 11, 264, 3001, 4609, 292], "temperature": 0.0, "avg_logprob": -0.1413241090445683, "compression_ratio": 1.6946902654867257, "no_speech_prob": 6.240577022254001e-06}, {"id": 1449, "seek": 598508, "start": 5985.08, "end": 5992.5199999999995, "text": " squared momentum with squared momentum dampening, and then we just take the parameter and then", "tokens": [8889, 11244, 365, 8889, 11244, 19498, 4559, 11, 293, 550, 321, 445, 747, 264, 13075, 293, 550], "temperature": 0.0, "avg_logprob": -0.21321301127589026, "compression_ratio": 1.6443298969072164, "no_speech_prob": 5.09358187628095e-06}, {"id": 1450, "seek": 598508, "start": 5992.5199999999995, "end": 6001.4, "text": " our learning rate, and we've got the debiasing here, our gradient average, and divided by", "tokens": [527, 2539, 3314, 11, 293, 321, 600, 658, 264, 3001, 72, 3349, 510, 11, 527, 16235, 4274, 11, 293, 6666, 538], "temperature": 0.0, "avg_logprob": -0.21321301127589026, "compression_ratio": 1.6443298969072164, "no_speech_prob": 5.09358187628095e-06}, {"id": 1451, "seek": 598508, "start": 6001.4, "end": 6003.44, "text": " the squared.", "tokens": [264, 8889, 13], "temperature": 0.0, "avg_logprob": -0.21321301127589026, "compression_ratio": 1.6443298969072164, "no_speech_prob": 5.09358187628095e-06}, {"id": 1452, "seek": 598508, "start": 6003.44, "end": 6004.76, "text": " And we also have our epsilon.", "tokens": [400, 321, 611, 362, 527, 17889, 13], "temperature": 0.0, "avg_logprob": -0.21321301127589026, "compression_ratio": 1.6443298969072164, "no_speech_prob": 5.09358187628095e-06}, {"id": 1453, "seek": 598508, "start": 6004.76, "end": 6007.84, "text": " Oh, this is in the wrong spot.", "tokens": [876, 11, 341, 307, 294, 264, 2085, 4008, 13], "temperature": 0.0, "avg_logprob": -0.21321301127589026, "compression_ratio": 1.6443298969072164, "no_speech_prob": 5.09358187628095e-06}, {"id": 1454, "seek": 598508, "start": 6007.84, "end": 6008.84, "text": " Be careful.", "tokens": [879, 5026, 13], "temperature": 0.0, "avg_logprob": -0.21321301127589026, "compression_ratio": 1.6443298969072164, "no_speech_prob": 5.09358187628095e-06}, {"id": 1455, "seek": 598508, "start": 6008.84, "end": 6013.64, "text": " Epsilon should always go inside the square root.", "tokens": [462, 16592, 820, 1009, 352, 1854, 264, 3732, 5593, 13], "temperature": 0.0, "avg_logprob": -0.21321301127589026, "compression_ratio": 1.6443298969072164, "no_speech_prob": 5.09358187628095e-06}, {"id": 1456, "seek": 601364, "start": 6013.64, "end": 6017.64, "text": " So, that's an Adam step.", "tokens": [407, 11, 300, 311, 364, 7938, 1823, 13], "temperature": 0.0, "avg_logprob": -0.22344300963661887, "compression_ratio": 1.551948051948052, "no_speech_prob": 8.186279387700779e-07}, {"id": 1457, "seek": 601364, "start": 6017.64, "end": 6023.160000000001, "text": " So now we can create an Adam optimizer in one line of code.", "tokens": [407, 586, 321, 393, 1884, 364, 7938, 5028, 6545, 294, 472, 1622, 295, 3089, 13], "temperature": 0.0, "avg_logprob": -0.22344300963661887, "compression_ratio": 1.551948051948052, "no_speech_prob": 8.186279387700779e-07}, {"id": 1458, "seek": 601364, "start": 6023.160000000001, "end": 6025.320000000001, "text": " And so there's our Adam optimizer.", "tokens": [400, 370, 456, 311, 527, 7938, 5028, 6545, 13], "temperature": 0.0, "avg_logprob": -0.22344300963661887, "compression_ratio": 1.551948051948052, "no_speech_prob": 8.186279387700779e-07}, {"id": 1459, "seek": 601364, "start": 6025.320000000001, "end": 6026.320000000001, "text": " It has average grads.", "tokens": [467, 575, 4274, 2771, 82, 13], "temperature": 0.0, "avg_logprob": -0.22344300963661887, "compression_ratio": 1.551948051948052, "no_speech_prob": 8.186279387700779e-07}, {"id": 1460, "seek": 601364, "start": 6026.320000000001, "end": 6027.8, "text": " It's got average squared grads.", "tokens": [467, 311, 658, 4274, 8889, 2771, 82, 13], "temperature": 0.0, "avg_logprob": -0.22344300963661887, "compression_ratio": 1.551948051948052, "no_speech_prob": 8.186279387700779e-07}, {"id": 1461, "seek": 601364, "start": 6027.8, "end": 6029.4800000000005, "text": " It's got a step.", "tokens": [467, 311, 658, 257, 1823, 13], "temperature": 0.0, "avg_logprob": -0.22344300963661887, "compression_ratio": 1.551948051948052, "no_speech_prob": 8.186279387700779e-07}, {"id": 1462, "seek": 601364, "start": 6029.4800000000005, "end": 6034.56, "text": " And we can now try it out.", "tokens": [400, 321, 393, 586, 853, 309, 484, 13], "temperature": 0.0, "avg_logprob": -0.22344300963661887, "compression_ratio": 1.551948051948052, "no_speech_prob": 8.186279387700779e-07}, {"id": 1463, "seek": 601364, "start": 6034.56, "end": 6036.08, "text": " Okay?", "tokens": [1033, 30], "temperature": 0.0, "avg_logprob": -0.22344300963661887, "compression_ratio": 1.551948051948052, "no_speech_prob": 8.186279387700779e-07}, {"id": 1464, "seek": 601364, "start": 6036.08, "end": 6041.92, "text": " So, here's Lam.", "tokens": [407, 11, 510, 311, 18825, 13], "temperature": 0.0, "avg_logprob": -0.22344300963661887, "compression_ratio": 1.551948051948052, "no_speech_prob": 8.186279387700779e-07}, {"id": 1465, "seek": 604192, "start": 6041.92, "end": 6047.56, "text": " By the way, these equations are a little nicer than these equations, and I want to point", "tokens": [3146, 264, 636, 11, 613, 11787, 366, 257, 707, 22842, 813, 613, 11787, 11, 293, 286, 528, 281, 935], "temperature": 0.0, "avg_logprob": -0.153315916515532, "compression_ratio": 1.5905172413793103, "no_speech_prob": 4.157229341217317e-06}, {"id": 1466, "seek": 604192, "start": 6047.56, "end": 6049.72, "text": " something out.", "tokens": [746, 484, 13], "temperature": 0.0, "avg_logprob": -0.153315916515532, "compression_ratio": 1.5905172413793103, "no_speech_prob": 4.157229341217317e-06}, {"id": 1467, "seek": 604192, "start": 6049.72, "end": 6052.04, "text": " Mathematicians hate refactoring.", "tokens": [15776, 14911, 2567, 4700, 1895, 578, 3662, 13], "temperature": 0.0, "avg_logprob": -0.153315916515532, "compression_ratio": 1.5905172413793103, "no_speech_prob": 4.157229341217317e-06}, {"id": 1468, "seek": 604192, "start": 6052.04, "end": 6053.8, "text": " Don't be like them.", "tokens": [1468, 380, 312, 411, 552, 13], "temperature": 0.0, "avg_logprob": -0.153315916515532, "compression_ratio": 1.5905172413793103, "no_speech_prob": 4.157229341217317e-06}, {"id": 1469, "seek": 604192, "start": 6053.8, "end": 6054.8, "text": " Look at this.", "tokens": [2053, 412, 341, 13], "temperature": 0.0, "avg_logprob": -0.153315916515532, "compression_ratio": 1.5905172413793103, "no_speech_prob": 4.157229341217317e-06}, {"id": 1470, "seek": 604192, "start": 6054.8, "end": 6061.04, "text": " M over V plus epsilon root lambda, it's the same as this.", "tokens": [376, 670, 691, 1804, 17889, 5593, 13607, 11, 309, 311, 264, 912, 382, 341, 13], "temperature": 0.0, "avg_logprob": -0.153315916515532, "compression_ratio": 1.5905172413793103, "no_speech_prob": 4.157229341217317e-06}, {"id": 1471, "seek": 604192, "start": 6061.04, "end": 6066.04, "text": " So like, it's just so complicated when things appear the same way in multiple places.", "tokens": [407, 411, 11, 309, 311, 445, 370, 6179, 562, 721, 4204, 264, 912, 636, 294, 3866, 3190, 13], "temperature": 0.0, "avg_logprob": -0.153315916515532, "compression_ratio": 1.5905172413793103, "no_speech_prob": 4.157229341217317e-06}, {"id": 1472, "seek": 604192, "start": 6066.04, "end": 6070.24, "text": " So when we did this equation, we gave that a new name.", "tokens": [407, 562, 321, 630, 341, 5367, 11, 321, 2729, 300, 257, 777, 1315, 13], "temperature": 0.0, "avg_logprob": -0.153315916515532, "compression_ratio": 1.5905172413793103, "no_speech_prob": 4.157229341217317e-06}, {"id": 1473, "seek": 607024, "start": 6070.24, "end": 6076.36, "text": " And so now we can just look at R2 goes from all that to just that.", "tokens": [400, 370, 586, 321, 393, 445, 574, 412, 497, 17, 1709, 490, 439, 300, 281, 445, 300, 13], "temperature": 0.0, "avg_logprob": -0.11499141978326245, "compression_ratio": 1.7083333333333333, "no_speech_prob": 3.66875406143663e-06}, {"id": 1474, "seek": 607024, "start": 6076.36, "end": 6080.599999999999, "text": " And Wt goes from all that to just that.", "tokens": [400, 343, 83, 1709, 490, 439, 300, 281, 445, 300, 13], "temperature": 0.0, "avg_logprob": -0.11499141978326245, "compression_ratio": 1.7083333333333333, "no_speech_prob": 3.66875406143663e-06}, {"id": 1475, "seek": 607024, "start": 6080.599999999999, "end": 6085.599999999999, "text": " And so when you pull these things out, when you refactor your math, it's much easier to", "tokens": [400, 370, 562, 291, 2235, 613, 721, 484, 11, 562, 291, 1895, 15104, 428, 5221, 11, 309, 311, 709, 3571, 281], "temperature": 0.0, "avg_logprob": -0.11499141978326245, "compression_ratio": 1.7083333333333333, "no_speech_prob": 3.66875406143663e-06}, {"id": 1476, "seek": 607024, "start": 6085.599999999999, "end": 6088.08, "text": " see what's going on.", "tokens": [536, 437, 311, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.11499141978326245, "compression_ratio": 1.7083333333333333, "no_speech_prob": 3.66875406143663e-06}, {"id": 1477, "seek": 607024, "start": 6088.08, "end": 6090.66, "text": " So here's the cool thing, right?", "tokens": [407, 510, 311, 264, 1627, 551, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.11499141978326245, "compression_ratio": 1.7083333333333333, "no_speech_prob": 3.66875406143663e-06}, {"id": 1478, "seek": 607024, "start": 6090.66, "end": 6095.08, "text": " When we look at this, even if you're a terrible mathematician like me, you're going to start", "tokens": [1133, 321, 574, 412, 341, 11, 754, 498, 291, 434, 257, 6237, 48281, 411, 385, 11, 291, 434, 516, 281, 722], "temperature": 0.0, "avg_logprob": -0.11499141978326245, "compression_ratio": 1.7083333333333333, "no_speech_prob": 3.66875406143663e-06}, {"id": 1479, "seek": 607024, "start": 6095.08, "end": 6096.08, "text": " to recognize some patterns.", "tokens": [281, 5521, 512, 8294, 13], "temperature": 0.0, "avg_logprob": -0.11499141978326245, "compression_ratio": 1.7083333333333333, "no_speech_prob": 3.66875406143663e-06}, {"id": 1480, "seek": 609608, "start": 6096.08, "end": 6100.64, "text": " And that's the trick to being a less terrible mathematician is recognizing patterns.", "tokens": [400, 300, 311, 264, 4282, 281, 885, 257, 1570, 6237, 48281, 307, 18538, 8294, 13], "temperature": 0.0, "avg_logprob": -0.1496689206077939, "compression_ratio": 2.139130434782609, "no_speech_prob": 7.071811069181422e-06}, {"id": 1481, "seek": 609608, "start": 6100.64, "end": 6105.96, "text": " Beta times something plus 1 minus beta times another thing is exponentially weighted moving", "tokens": [33286, 1413, 746, 1804, 502, 3175, 9861, 1413, 1071, 551, 307, 37330, 32807, 2684], "temperature": 0.0, "avg_logprob": -0.1496689206077939, "compression_ratio": 2.139130434782609, "no_speech_prob": 7.071811069181422e-06}, {"id": 1482, "seek": 609608, "start": 6105.96, "end": 6108.44, "text": " average, right?", "tokens": [4274, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1496689206077939, "compression_ratio": 2.139130434782609, "no_speech_prob": 7.071811069181422e-06}, {"id": 1483, "seek": 609608, "start": 6108.44, "end": 6110.08, "text": " So here's one exponentially weighted moving average.", "tokens": [407, 510, 311, 472, 37330, 32807, 2684, 4274, 13], "temperature": 0.0, "avg_logprob": -0.1496689206077939, "compression_ratio": 2.139130434782609, "no_speech_prob": 7.071811069181422e-06}, {"id": 1484, "seek": 609608, "start": 6110.08, "end": 6112.1, "text": " Here's another exponentially weighted moving average.", "tokens": [1692, 311, 1071, 37330, 32807, 2684, 4274, 13], "temperature": 0.0, "avg_logprob": -0.1496689206077939, "compression_ratio": 2.139130434782609, "no_speech_prob": 7.071811069181422e-06}, {"id": 1485, "seek": 609608, "start": 6112.1, "end": 6113.72, "text": " This one has a gradient.", "tokens": [639, 472, 575, 257, 16235, 13], "temperature": 0.0, "avg_logprob": -0.1496689206077939, "compression_ratio": 2.139130434782609, "no_speech_prob": 7.071811069181422e-06}, {"id": 1486, "seek": 609608, "start": 6113.72, "end": 6115.36, "text": " This one has gradient squared.", "tokens": [639, 472, 575, 16235, 8889, 13], "temperature": 0.0, "avg_logprob": -0.1496689206077939, "compression_ratio": 2.139130434782609, "no_speech_prob": 7.071811069181422e-06}, {"id": 1487, "seek": 609608, "start": 6115.36, "end": 6119.04, "text": " This means element-wise multiplication.", "tokens": [639, 1355, 4478, 12, 3711, 27290, 13], "temperature": 0.0, "avg_logprob": -0.1496689206077939, "compression_ratio": 2.139130434782609, "no_speech_prob": 7.071811069181422e-06}, {"id": 1488, "seek": 609608, "start": 6119.04, "end": 6123.48, "text": " So these are the exponentially weighted moving average of the gradient and the gradient squared.", "tokens": [407, 613, 366, 264, 37330, 32807, 2684, 4274, 295, 264, 16235, 293, 264, 16235, 8889, 13], "temperature": 0.0, "avg_logprob": -0.1496689206077939, "compression_ratio": 2.139130434782609, "no_speech_prob": 7.071811069181422e-06}, {"id": 1489, "seek": 612348, "start": 6123.48, "end": 6127.32, "text": " Oh, beta to the t, de-biasing.", "tokens": [876, 11, 9861, 281, 264, 256, 11, 368, 12, 5614, 3349, 13], "temperature": 0.0, "avg_logprob": -0.28803420066833496, "compression_ratio": 1.8395061728395061, "no_speech_prob": 1.3630832654598635e-05}, {"id": 1490, "seek": 612348, "start": 6127.32, "end": 6129.32, "text": " So that's the de-biased version of m.", "tokens": [407, 300, 311, 264, 368, 12, 5614, 1937, 3037, 295, 275, 13], "temperature": 0.0, "avg_logprob": -0.28803420066833496, "compression_ratio": 1.8395061728395061, "no_speech_prob": 1.3630832654598635e-05}, {"id": 1491, "seek": 612348, "start": 6129.32, "end": 6136.5599999999995, "text": " There's the de-biased version of v.", "tokens": [821, 311, 264, 368, 12, 5614, 1937, 3037, 295, 371, 13], "temperature": 0.0, "avg_logprob": -0.28803420066833496, "compression_ratio": 1.8395061728395061, "no_speech_prob": 1.3630832654598635e-05}, {"id": 1492, "seek": 612348, "start": 6136.5599999999995, "end": 6138.5599999999995, "text": " Not to move the epsilon?", "tokens": [1726, 281, 1286, 264, 17889, 30], "temperature": 0.0, "avg_logprob": -0.28803420066833496, "compression_ratio": 1.8395061728395061, "no_speech_prob": 1.3630832654598635e-05}, {"id": 1493, "seek": 612348, "start": 6138.5599999999995, "end": 6139.5599999999995, "text": " Really?", "tokens": [4083, 30], "temperature": 0.0, "avg_logprob": -0.28803420066833496, "compression_ratio": 1.8395061728395061, "no_speech_prob": 1.3630832654598635e-05}, {"id": 1494, "seek": 612348, "start": 6139.5599999999995, "end": 6142.5599999999995, "text": " Sylvan has a message.", "tokens": [33349, 9768, 575, 257, 3636, 13], "temperature": 0.0, "avg_logprob": -0.28803420066833496, "compression_ratio": 1.8395061728395061, "no_speech_prob": 1.3630832654598635e-05}, {"id": 1495, "seek": 612348, "start": 6142.5599999999995, "end": 6143.5599999999995, "text": " Don't...", "tokens": [1468, 380, 485], "temperature": 0.0, "avg_logprob": -0.28803420066833496, "compression_ratio": 1.8395061728395061, "no_speech_prob": 1.3630832654598635e-05}, {"id": 1496, "seek": 612348, "start": 6143.5599999999995, "end": 6145.5599999999995, "text": " Sylvan has a message, don't move the epsilon.", "tokens": [33349, 9768, 575, 257, 3636, 11, 500, 380, 1286, 264, 17889, 13], "temperature": 0.0, "avg_logprob": -0.28803420066833496, "compression_ratio": 1.8395061728395061, "no_speech_prob": 1.3630832654598635e-05}, {"id": 1497, "seek": 612348, "start": 6145.5599999999995, "end": 6146.5599999999995, "text": " Don't listen to Jeremy.", "tokens": [1468, 380, 2140, 281, 17809, 13], "temperature": 0.0, "avg_logprob": -0.28803420066833496, "compression_ratio": 1.8395061728395061, "no_speech_prob": 1.3630832654598635e-05}, {"id": 1498, "seek": 612348, "start": 6146.5599999999995, "end": 6147.5599999999995, "text": " Don't listen to Jeremy.", "tokens": [1468, 380, 2140, 281, 17809, 13], "temperature": 0.0, "avg_logprob": -0.28803420066833496, "compression_ratio": 1.8395061728395061, "no_speech_prob": 1.3630832654598635e-05}, {"id": 1499, "seek": 612348, "start": 6147.5599999999995, "end": 6148.5599999999995, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.28803420066833496, "compression_ratio": 1.8395061728395061, "no_speech_prob": 1.3630832654598635e-05}, {"id": 1500, "seek": 612348, "start": 6148.5599999999995, "end": 6150.5599999999995, "text": " Sylvan is an actual math guy.", "tokens": [33349, 9768, 307, 364, 3539, 5221, 2146, 13], "temperature": 0.0, "avg_logprob": -0.28803420066833496, "compression_ratio": 1.8395061728395061, "no_speech_prob": 1.3630832654598635e-05}, {"id": 1501, "seek": 615056, "start": 6150.56, "end": 6153.84, "text": " So...In an atom, the epsilon goes outside the square root.", "tokens": [407, 485, 4575, 364, 12018, 11, 264, 17889, 1709, 2380, 264, 3732, 5593, 13], "temperature": 0.0, "avg_logprob": -0.2588052177429199, "compression_ratio": 1.5919282511210762, "no_speech_prob": 2.026115680564544e-06}, {"id": 1502, "seek": 615056, "start": 6153.84, "end": 6154.84, "text": " No way.", "tokens": [883, 636, 13], "temperature": 0.0, "avg_logprob": -0.2588052177429199, "compression_ratio": 1.5919282511210762, "no_speech_prob": 2.026115680564544e-06}, {"id": 1503, "seek": 615056, "start": 6154.84, "end": 6159.400000000001, "text": " I always thought epsilon should always go inside the square root.", "tokens": [286, 1009, 1194, 17889, 820, 1009, 352, 1854, 264, 3732, 5593, 13], "temperature": 0.0, "avg_logprob": -0.2588052177429199, "compression_ratio": 1.5919282511210762, "no_speech_prob": 2.026115680564544e-06}, {"id": 1504, "seek": 615056, "start": 6159.400000000001, "end": 6165.64, "text": " Jeremy just did a fix I pushed a week ago where our atom wasn't working.", "tokens": [17809, 445, 630, 257, 3191, 286, 9152, 257, 1243, 2057, 689, 527, 12018, 2067, 380, 1364, 13], "temperature": 0.0, "avg_logprob": -0.2588052177429199, "compression_ratio": 1.5919282511210762, "no_speech_prob": 2.026115680564544e-06}, {"id": 1505, "seek": 615056, "start": 6165.64, "end": 6167.400000000001, "text": " Let's press control Z a few times.", "tokens": [961, 311, 1886, 1969, 1176, 257, 1326, 1413, 13], "temperature": 0.0, "avg_logprob": -0.2588052177429199, "compression_ratio": 1.5919282511210762, "no_speech_prob": 2.026115680564544e-06}, {"id": 1506, "seek": 615056, "start": 6167.400000000001, "end": 6168.72, "text": " There we go.", "tokens": [821, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.2588052177429199, "compression_ratio": 1.5919282511210762, "no_speech_prob": 2.026115680564544e-06}, {"id": 1507, "seek": 615056, "start": 6168.72, "end": 6169.72, "text": " That's great.", "tokens": [663, 311, 869, 13], "temperature": 0.0, "avg_logprob": -0.2588052177429199, "compression_ratio": 1.5919282511210762, "no_speech_prob": 2.026115680564544e-06}, {"id": 1508, "seek": 615056, "start": 6169.72, "end": 6176.52, "text": " So to explain why this matters and why there is no right answer, here's the difference.", "tokens": [407, 281, 2903, 983, 341, 7001, 293, 983, 456, 307, 572, 558, 1867, 11, 510, 311, 264, 2649, 13], "temperature": 0.0, "avg_logprob": -0.2588052177429199, "compression_ratio": 1.5919282511210762, "no_speech_prob": 2.026115680564544e-06}, {"id": 1509, "seek": 617652, "start": 6176.52, "end": 6184.64, "text": " So if epsilon is 1e-7, then having it here versus having it here.", "tokens": [407, 498, 17889, 307, 502, 68, 12, 22, 11, 550, 1419, 309, 510, 5717, 1419, 309, 510, 13], "temperature": 0.0, "avg_logprob": -0.14577552063824378, "compression_ratio": 1.5405405405405406, "no_speech_prob": 3.7265688206389314e-06}, {"id": 1510, "seek": 617652, "start": 6184.64, "end": 6194.360000000001, "text": " So like the square root of 1e-7 is very different to 1e-7.", "tokens": [407, 411, 264, 3732, 5593, 295, 502, 68, 12, 22, 307, 588, 819, 281, 502, 68, 12, 22, 13], "temperature": 0.0, "avg_logprob": -0.14577552063824378, "compression_ratio": 1.5405405405405406, "no_speech_prob": 3.7265688206389314e-06}, {"id": 1511, "seek": 617652, "start": 6194.360000000001, "end": 6198.76, "text": " And in batch norm, they do put it inside the square root.", "tokens": [400, 294, 15245, 2026, 11, 436, 360, 829, 309, 1854, 264, 3732, 5593, 13], "temperature": 0.0, "avg_logprob": -0.14577552063824378, "compression_ratio": 1.5405405405405406, "no_speech_prob": 3.7265688206389314e-06}, {"id": 1512, "seek": 617652, "start": 6198.76, "end": 6202.400000000001, "text": " And according to Sylvan and Adam, they don't.", "tokens": [400, 4650, 281, 33349, 9768, 293, 7938, 11, 436, 500, 380, 13], "temperature": 0.0, "avg_logprob": -0.14577552063824378, "compression_ratio": 1.5405405405405406, "no_speech_prob": 3.7265688206389314e-06}, {"id": 1513, "seek": 620240, "start": 6202.4, "end": 6207.0, "text": " Neither is like the right place to put it or the wrong place to put it.", "tokens": [23956, 307, 411, 264, 558, 1081, 281, 829, 309, 420, 264, 2085, 1081, 281, 829, 309, 13], "temperature": 0.0, "avg_logprob": -0.1387173546685113, "compression_ratio": 1.6546391752577319, "no_speech_prob": 5.862728812644491e-06}, {"id": 1514, "seek": 620240, "start": 6207.0, "end": 6211.24, "text": " If you don't put it in the same place as they do in the paper, it's just a totally different", "tokens": [759, 291, 500, 380, 829, 309, 294, 264, 912, 1081, 382, 436, 360, 294, 264, 3035, 11, 309, 311, 445, 257, 3879, 819], "temperature": 0.0, "avg_logprob": -0.1387173546685113, "compression_ratio": 1.6546391752577319, "no_speech_prob": 5.862728812644491e-06}, {"id": 1515, "seek": 620240, "start": 6211.24, "end": 6212.24, "text": " number.", "tokens": [1230, 13], "temperature": 0.0, "avg_logprob": -0.1387173546685113, "compression_ratio": 1.6546391752577319, "no_speech_prob": 5.862728812644491e-06}, {"id": 1516, "seek": 620240, "start": 6212.24, "end": 6216.839999999999, "text": " And this is a good time to talk about epsilon and atom.", "tokens": [400, 341, 307, 257, 665, 565, 281, 751, 466, 17889, 293, 12018, 13], "temperature": 0.0, "avg_logprob": -0.1387173546685113, "compression_ratio": 1.6546391752577319, "no_speech_prob": 5.862728812644491e-06}, {"id": 1517, "seek": 620240, "start": 6216.839999999999, "end": 6219.32, "text": " Because I love epsilon and atom.", "tokens": [1436, 286, 959, 17889, 293, 12018, 13], "temperature": 0.0, "avg_logprob": -0.1387173546685113, "compression_ratio": 1.6546391752577319, "no_speech_prob": 5.862728812644491e-06}, {"id": 1518, "seek": 620240, "start": 6219.32, "end": 6226.0, "text": " Because like what if we put made epsilon equal to 1, right?", "tokens": [1436, 411, 437, 498, 321, 829, 1027, 17889, 2681, 281, 502, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1387173546685113, "compression_ratio": 1.6546391752577319, "no_speech_prob": 5.862728812644491e-06}, {"id": 1519, "seek": 622600, "start": 6226.0, "end": 6234.32, "text": " And we've got the kind of momentum term on the numerator.", "tokens": [400, 321, 600, 658, 264, 733, 295, 11244, 1433, 322, 264, 30380, 13], "temperature": 0.0, "avg_logprob": -0.15656917360093858, "compression_ratio": 1.7864583333333333, "no_speech_prob": 1.6963042071438394e-05}, {"id": 1520, "seek": 622600, "start": 6234.32, "end": 6242.24, "text": " And the denominator, we've got the root sum of squares of the root of the exponentially", "tokens": [400, 264, 20687, 11, 321, 600, 658, 264, 5593, 2408, 295, 19368, 295, 264, 5593, 295, 264, 37330], "temperature": 0.0, "avg_logprob": -0.15656917360093858, "compression_ratio": 1.7864583333333333, "no_speech_prob": 1.6963042071438394e-05}, {"id": 1521, "seek": 622600, "start": 6242.24, "end": 6244.92, "text": " weighted average squared gradients.", "tokens": [32807, 4274, 8889, 2771, 2448, 13], "temperature": 0.0, "avg_logprob": -0.15656917360093858, "compression_ratio": 1.7864583333333333, "no_speech_prob": 1.6963042071438394e-05}, {"id": 1522, "seek": 622600, "start": 6244.92, "end": 6248.2, "text": " So we're dividing by that plus 1.", "tokens": [407, 321, 434, 26764, 538, 300, 1804, 502, 13], "temperature": 0.0, "avg_logprob": -0.15656917360093858, "compression_ratio": 1.7864583333333333, "no_speech_prob": 1.6963042071438394e-05}, {"id": 1523, "seek": 622600, "start": 6248.2, "end": 6252.6, "text": " And most of the time, the gradients are going to be smaller than 1.", "tokens": [400, 881, 295, 264, 565, 11, 264, 2771, 2448, 366, 516, 281, 312, 4356, 813, 502, 13], "temperature": 0.0, "avg_logprob": -0.15656917360093858, "compression_ratio": 1.7864583333333333, "no_speech_prob": 1.6963042071438394e-05}, {"id": 1524, "seek": 622600, "start": 6252.6, "end": 6255.2, "text": " And the squared version is going to be much smaller than 1.", "tokens": [400, 264, 8889, 3037, 307, 516, 281, 312, 709, 4356, 813, 502, 13], "temperature": 0.0, "avg_logprob": -0.15656917360093858, "compression_ratio": 1.7864583333333333, "no_speech_prob": 1.6963042071438394e-05}, {"id": 1525, "seek": 625520, "start": 6255.2, "end": 6262.04, "text": " So basically then, the 1 is going to be much bigger than this.", "tokens": [407, 1936, 550, 11, 264, 502, 307, 516, 281, 312, 709, 3801, 813, 341, 13], "temperature": 0.0, "avg_logprob": -0.2229534355369774, "compression_ratio": 1.4759036144578312, "no_speech_prob": 7.296226613107137e-06}, {"id": 1526, "seek": 625520, "start": 6262.04, "end": 6264.639999999999, "text": " So it basically makes this go away.", "tokens": [407, 309, 1936, 1669, 341, 352, 1314, 13], "temperature": 0.0, "avg_logprob": -0.2229534355369774, "compression_ratio": 1.4759036144578312, "no_speech_prob": 7.296226613107137e-06}, {"id": 1527, "seek": 625520, "start": 6264.639999999999, "end": 6272.96, "text": " So if epsilon is 1, it's pretty close to being standard SGD with momentum, or at least debiased,", "tokens": [407, 498, 17889, 307, 502, 11, 309, 311, 1238, 1998, 281, 885, 3832, 34520, 35, 365, 11244, 11, 420, 412, 1935, 3001, 72, 1937, 11], "temperature": 0.0, "avg_logprob": -0.2229534355369774, "compression_ratio": 1.4759036144578312, "no_speech_prob": 7.296226613107137e-06}, {"id": 1528, "seek": 625520, "start": 6272.96, "end": 6273.96, "text": " dampened momentum.", "tokens": [19498, 5320, 11244, 13], "temperature": 0.0, "avg_logprob": -0.2229534355369774, "compression_ratio": 1.4759036144578312, "no_speech_prob": 7.296226613107137e-06}, {"id": 1529, "seek": 625520, "start": 6273.96, "end": 6279.599999999999, "text": " Where else is epsilon is 1e-7?", "tokens": [2305, 1646, 307, 17889, 307, 502, 68, 12, 22, 30], "temperature": 0.0, "avg_logprob": -0.2229534355369774, "compression_ratio": 1.4759036144578312, "no_speech_prob": 7.296226613107137e-06}, {"id": 1530, "seek": 627960, "start": 6279.6, "end": 6287.88, "text": " Then we're basically saying, oh, we want to really use these different exponentially", "tokens": [1396, 321, 434, 1936, 1566, 11, 1954, 11, 321, 528, 281, 534, 764, 613, 819, 37330], "temperature": 0.0, "avg_logprob": -0.09850525174822126, "compression_ratio": 1.4893617021276595, "no_speech_prob": 1.3287642559589585e-06}, {"id": 1531, "seek": 627960, "start": 6287.88, "end": 6290.84, "text": " weighted moving average squared gradients.", "tokens": [32807, 2684, 4274, 8889, 2771, 2448, 13], "temperature": 0.0, "avg_logprob": -0.09850525174822126, "compression_ratio": 1.4893617021276595, "no_speech_prob": 1.3287642559589585e-06}, {"id": 1532, "seek": 627960, "start": 6290.84, "end": 6293.0, "text": " And this is really important.", "tokens": [400, 341, 307, 534, 1021, 13], "temperature": 0.0, "avg_logprob": -0.09850525174822126, "compression_ratio": 1.4893617021276595, "no_speech_prob": 1.3287642559589585e-06}, {"id": 1533, "seek": 627960, "start": 6293.0, "end": 6304.46, "text": " Because if you have some activation that has had a very small squared gradients for a while,", "tokens": [1436, 498, 291, 362, 512, 24433, 300, 575, 632, 257, 588, 1359, 8889, 2771, 2448, 337, 257, 1339, 11], "temperature": 0.0, "avg_logprob": -0.09850525174822126, "compression_ratio": 1.4893617021276595, "no_speech_prob": 1.3287642559589585e-06}, {"id": 1534, "seek": 627960, "start": 6304.46, "end": 6307.58, "text": " this could well be like 1e-6.", "tokens": [341, 727, 731, 312, 411, 502, 68, 12, 21, 13], "temperature": 0.0, "avg_logprob": -0.09850525174822126, "compression_ratio": 1.4893617021276595, "no_speech_prob": 1.3287642559589585e-06}, {"id": 1535, "seek": 630758, "start": 6307.58, "end": 6311.5599999999995, "text": " Which means when you divide by it, you're multiplying by a million.", "tokens": [3013, 1355, 562, 291, 9845, 538, 309, 11, 291, 434, 30955, 538, 257, 2459, 13], "temperature": 0.0, "avg_logprob": -0.1110662610343333, "compression_ratio": 1.4597156398104265, "no_speech_prob": 5.955026608717162e-06}, {"id": 1536, "seek": 630758, "start": 6311.5599999999995, "end": 6314.74, "text": " And that could absolutely kill your optimizer.", "tokens": [400, 300, 727, 3122, 1961, 428, 5028, 6545, 13], "temperature": 0.0, "avg_logprob": -0.1110662610343333, "compression_ratio": 1.4597156398104265, "no_speech_prob": 5.955026608717162e-06}, {"id": 1537, "seek": 630758, "start": 6314.74, "end": 6323.16, "text": " So the trick to making atom and atom-like things work well is to make this about 0.1,", "tokens": [407, 264, 4282, 281, 1455, 12018, 293, 12018, 12, 4092, 721, 589, 731, 307, 281, 652, 341, 466, 1958, 13, 16, 11], "temperature": 0.0, "avg_logprob": -0.1110662610343333, "compression_ratio": 1.4597156398104265, "no_speech_prob": 5.955026608717162e-06}, {"id": 1538, "seek": 630758, "start": 6323.16, "end": 6327.12, "text": " somewhere between 1e-3 and 1e-1 tends to work pretty well.", "tokens": [4079, 1296, 502, 68, 12, 18, 293, 502, 68, 12, 16, 12258, 281, 589, 1238, 731, 13], "temperature": 0.0, "avg_logprob": -0.1110662610343333, "compression_ratio": 1.4597156398104265, "no_speech_prob": 5.955026608717162e-06}, {"id": 1539, "seek": 630758, "start": 6327.12, "end": 6332.0, "text": " Most people use 1e-7, which just makes no sense.", "tokens": [4534, 561, 764, 502, 68, 12, 22, 11, 597, 445, 1669, 572, 2020, 13], "temperature": 0.0, "avg_logprob": -0.1110662610343333, "compression_ratio": 1.4597156398104265, "no_speech_prob": 5.955026608717162e-06}, {"id": 1540, "seek": 633200, "start": 6332.0, "end": 6338.88, "text": " There's no way that you want to be able to multiply your step by 10 million times.", "tokens": [821, 311, 572, 636, 300, 291, 528, 281, 312, 1075, 281, 12972, 428, 1823, 538, 1266, 2459, 1413, 13], "temperature": 0.0, "avg_logprob": -0.1517239947651708, "compression_ratio": 1.5052631578947369, "no_speech_prob": 5.9549774960032664e-06}, {"id": 1541, "seek": 633200, "start": 6338.88, "end": 6340.6, "text": " That's just never going to be a good idea.", "tokens": [663, 311, 445, 1128, 516, 281, 312, 257, 665, 1558, 13], "temperature": 0.0, "avg_logprob": -0.1517239947651708, "compression_ratio": 1.5052631578947369, "no_speech_prob": 5.9549774960032664e-06}, {"id": 1542, "seek": 633200, "start": 6340.6, "end": 6349.76, "text": " So it's another place that epsilon is a super important thing to think about.", "tokens": [407, 309, 311, 1071, 1081, 300, 17889, 307, 257, 1687, 1021, 551, 281, 519, 466, 13], "temperature": 0.0, "avg_logprob": -0.1517239947651708, "compression_ratio": 1.5052631578947369, "no_speech_prob": 5.9549774960032664e-06}, {"id": 1543, "seek": 633200, "start": 6349.76, "end": 6356.08, "text": " So LAM, then, is stuff that we've all seen before.", "tokens": [407, 441, 2865, 11, 550, 11, 307, 1507, 300, 321, 600, 439, 1612, 949, 13], "temperature": 0.0, "avg_logprob": -0.1517239947651708, "compression_ratio": 1.5052631578947369, "no_speech_prob": 5.9549774960032664e-06}, {"id": 1544, "seek": 633200, "start": 6356.08, "end": 6358.04, "text": " So it's debiased.", "tokens": [407, 309, 311, 3001, 72, 1937, 13], "temperature": 0.0, "avg_logprob": -0.1517239947651708, "compression_ratio": 1.5052631578947369, "no_speech_prob": 5.9549774960032664e-06}, {"id": 1545, "seek": 633200, "start": 6358.04, "end": 6360.56, "text": " This is atom.", "tokens": [639, 307, 12018, 13], "temperature": 0.0, "avg_logprob": -0.1517239947651708, "compression_ratio": 1.5052631578947369, "no_speech_prob": 5.9549774960032664e-06}, {"id": 1546, "seek": 636056, "start": 6360.56, "end": 6365.8, "text": " Just exponentially weighted moving averages of gradients and gradients squared.", "tokens": [1449, 37330, 32807, 2684, 42257, 295, 2771, 2448, 293, 2771, 2448, 8889, 13], "temperature": 0.0, "avg_logprob": -0.16429802087637094, "compression_ratio": 1.664804469273743, "no_speech_prob": 9.516091267869342e-06}, {"id": 1547, "seek": 636056, "start": 6365.8, "end": 6369.88, "text": " This here is the norm of the weights.", "tokens": [639, 510, 307, 264, 2026, 295, 264, 17443, 13], "temperature": 0.0, "avg_logprob": -0.16429802087637094, "compression_ratio": 1.664804469273743, "no_speech_prob": 9.516091267869342e-06}, {"id": 1548, "seek": 636056, "start": 6369.88, "end": 6373.56, "text": " The norm is just the sum of the roots of the squares.", "tokens": [440, 2026, 307, 445, 264, 2408, 295, 264, 10669, 295, 264, 19368, 13], "temperature": 0.0, "avg_logprob": -0.16429802087637094, "compression_ratio": 1.664804469273743, "no_speech_prob": 9.516091267869342e-06}, {"id": 1549, "seek": 636056, "start": 6373.56, "end": 6375.06, "text": " So this is just weight decay.", "tokens": [407, 341, 307, 445, 3364, 21039, 13], "temperature": 0.0, "avg_logprob": -0.16429802087637094, "compression_ratio": 1.664804469273743, "no_speech_prob": 9.516091267869342e-06}, {"id": 1550, "seek": 636056, "start": 6375.06, "end": 6379.080000000001, "text": " So LAM has weight decay built in.", "tokens": [407, 441, 2865, 575, 3364, 21039, 3094, 294, 13], "temperature": 0.0, "avg_logprob": -0.16429802087637094, "compression_ratio": 1.664804469273743, "no_speech_prob": 9.516091267869342e-06}, {"id": 1551, "seek": 636056, "start": 6379.080000000001, "end": 6385.06, "text": " This one here, hopefully you recognize as being the atom step.", "tokens": [639, 472, 510, 11, 4696, 291, 5521, 382, 885, 264, 12018, 1823, 13], "temperature": 0.0, "avg_logprob": -0.16429802087637094, "compression_ratio": 1.664804469273743, "no_speech_prob": 9.516091267869342e-06}, {"id": 1552, "seek": 638506, "start": 6385.06, "end": 6391.1, "text": " And so this is the norm of the atom step.", "tokens": [400, 370, 341, 307, 264, 2026, 295, 264, 12018, 1823, 13], "temperature": 0.0, "avg_logprob": -0.1176913441091344, "compression_ratio": 1.4968944099378882, "no_speech_prob": 1.2878880397693138e-06}, {"id": 1553, "seek": 638506, "start": 6391.1, "end": 6401.56, "text": " So basically what LAM is doing is it's atom, but what we do is we average all the steps", "tokens": [407, 1936, 437, 441, 2865, 307, 884, 307, 309, 311, 12018, 11, 457, 437, 321, 360, 307, 321, 4274, 439, 264, 4439], "temperature": 0.0, "avg_logprob": -0.1176913441091344, "compression_ratio": 1.4968944099378882, "no_speech_prob": 1.2878880397693138e-06}, {"id": 1554, "seek": 638506, "start": 6401.56, "end": 6404.4800000000005, "text": " over a whole layer.", "tokens": [670, 257, 1379, 4583, 13], "temperature": 0.0, "avg_logprob": -0.1176913441091344, "compression_ratio": 1.4968944099378882, "no_speech_prob": 1.2878880397693138e-06}, {"id": 1555, "seek": 638506, "start": 6404.4800000000005, "end": 6408.04, "text": " That's why these L's are really important, because these things are happening over a", "tokens": [663, 311, 983, 613, 441, 311, 366, 534, 1021, 11, 570, 613, 721, 366, 2737, 670, 257], "temperature": 0.0, "avg_logprob": -0.1176913441091344, "compression_ratio": 1.4968944099378882, "no_speech_prob": 1.2878880397693138e-06}, {"id": 1556, "seek": 638506, "start": 6408.04, "end": 6409.04, "text": " layer.", "tokens": [4583, 13], "temperature": 0.0, "avg_logprob": -0.1176913441091344, "compression_ratio": 1.4968944099378882, "no_speech_prob": 1.2878880397693138e-06}, {"id": 1557, "seek": 640904, "start": 6409.04, "end": 6417.64, "text": " And so basically we're taking, so here's our d by cementum, d by squared momentum, and", "tokens": [400, 370, 1936, 321, 434, 1940, 11, 370, 510, 311, 527, 274, 538, 19729, 449, 11, 274, 538, 8889, 11244, 11, 293], "temperature": 0.0, "avg_logprob": -0.237021844363907, "compression_ratio": 1.6948356807511737, "no_speech_prob": 8.397913916269317e-06}, {"id": 1558, "seek": 640904, "start": 6417.64, "end": 6420.24, "text": " then here's our 1.", "tokens": [550, 510, 311, 527, 502, 13], "temperature": 0.0, "avg_logprob": -0.237021844363907, "compression_ratio": 1.6948356807511737, "no_speech_prob": 8.397913916269317e-06}, {"id": 1559, "seek": 640904, "start": 6420.24, "end": 6422.04, "text": " And look, here's this mean.", "tokens": [400, 574, 11, 510, 311, 341, 914, 13], "temperature": 0.0, "avg_logprob": -0.237021844363907, "compression_ratio": 1.6948356807511737, "no_speech_prob": 8.397913916269317e-06}, {"id": 1560, "seek": 640904, "start": 6422.04, "end": 6427.48, "text": " So it's for a layer, because remember each stepper is created for a layer, for a parameter.", "tokens": [407, 309, 311, 337, 257, 4583, 11, 570, 1604, 1184, 2126, 3717, 307, 2942, 337, 257, 4583, 11, 337, 257, 13075, 13], "temperature": 0.0, "avg_logprob": -0.237021844363907, "compression_ratio": 1.6948356807511737, "no_speech_prob": 8.397913916269317e-06}, {"id": 1561, "seek": 640904, "start": 6427.48, "end": 6431.2, "text": " I shouldn't say a layer, for a parameter.", "tokens": [286, 4659, 380, 584, 257, 4583, 11, 337, 257, 13075, 13], "temperature": 0.0, "avg_logprob": -0.237021844363907, "compression_ratio": 1.6948356807511737, "no_speech_prob": 8.397913916269317e-06}, {"id": 1562, "seek": 640904, "start": 6431.2, "end": 6438.8, "text": " So this is kind of both exciting and annoying, because I had been working on this exact idea,", "tokens": [407, 341, 307, 733, 295, 1293, 4670, 293, 11304, 11, 570, 286, 632, 668, 1364, 322, 341, 1900, 1558, 11], "temperature": 0.0, "avg_logprob": -0.237021844363907, "compression_ratio": 1.6948356807511737, "no_speech_prob": 8.397913916269317e-06}, {"id": 1563, "seek": 643880, "start": 6438.8, "end": 6445.4400000000005, "text": " which is basically atom, but averaged out over a layer for the previous week.", "tokens": [597, 307, 1936, 12018, 11, 457, 18247, 2980, 484, 670, 257, 4583, 337, 264, 3894, 1243, 13], "temperature": 0.0, "avg_logprob": -0.14864544338650174, "compression_ratio": 1.6578947368421053, "no_speech_prob": 2.7533995307749137e-05}, {"id": 1564, "seek": 643880, "start": 6445.4400000000005, "end": 6448.8, "text": " And then this LAM paper came out, and I was like, oh, that's cool.", "tokens": [400, 550, 341, 441, 2865, 3035, 1361, 484, 11, 293, 286, 390, 411, 11, 1954, 11, 300, 311, 1627, 13], "temperature": 0.0, "avg_logprob": -0.14864544338650174, "compression_ratio": 1.6578947368421053, "no_speech_prob": 2.7533995307749137e-05}, {"id": 1565, "seek": 643880, "start": 6448.8, "end": 6450.0, "text": " Some paper about BERT training.", "tokens": [2188, 3035, 466, 363, 31479, 3097, 13], "temperature": 0.0, "avg_logprob": -0.14864544338650174, "compression_ratio": 1.6578947368421053, "no_speech_prob": 2.7533995307749137e-05}, {"id": 1566, "seek": 643880, "start": 6450.0, "end": 6451.0, "text": " I'll check it out.", "tokens": [286, 603, 1520, 309, 484, 13], "temperature": 0.0, "avg_logprob": -0.14864544338650174, "compression_ratio": 1.6578947368421053, "no_speech_prob": 2.7533995307749137e-05}, {"id": 1567, "seek": 643880, "start": 6451.0, "end": 6452.52, "text": " And it's like, oh, we do it with a new optimizer.", "tokens": [400, 309, 311, 411, 11, 1954, 11, 321, 360, 309, 365, 257, 777, 5028, 6545, 13], "temperature": 0.0, "avg_logprob": -0.14864544338650174, "compression_ratio": 1.6578947368421053, "no_speech_prob": 2.7533995307749137e-05}, {"id": 1568, "seek": 643880, "start": 6452.52, "end": 6457.84, "text": " And I looked at the new optimizer, it's like, it's just the optimizer I wrote a week before", "tokens": [400, 286, 2956, 412, 264, 777, 5028, 6545, 11, 309, 311, 411, 11, 309, 311, 445, 264, 5028, 6545, 286, 4114, 257, 1243, 949], "temperature": 0.0, "avg_logprob": -0.14864544338650174, "compression_ratio": 1.6578947368421053, "no_speech_prob": 2.7533995307749137e-05}, {"id": 1569, "seek": 643880, "start": 6457.84, "end": 6459.360000000001, "text": " we were going to present it.", "tokens": [321, 645, 516, 281, 1974, 309, 13], "temperature": 0.0, "avg_logprob": -0.14864544338650174, "compression_ratio": 1.6578947368421053, "no_speech_prob": 2.7533995307749137e-05}, {"id": 1570, "seek": 643880, "start": 6459.360000000001, "end": 6462.400000000001, "text": " So I'm thrilled that this thing exists.", "tokens": [407, 286, 478, 18744, 300, 341, 551, 8198, 13], "temperature": 0.0, "avg_logprob": -0.14864544338650174, "compression_ratio": 1.6578947368421053, "no_speech_prob": 2.7533995307749137e-05}, {"id": 1571, "seek": 643880, "start": 6462.400000000001, "end": 6465.34, "text": " I think it's exactly what we need.", "tokens": [286, 519, 309, 311, 2293, 437, 321, 643, 13], "temperature": 0.0, "avg_logprob": -0.14864544338650174, "compression_ratio": 1.6578947368421053, "no_speech_prob": 2.7533995307749137e-05}, {"id": 1572, "seek": 646534, "start": 6465.34, "end": 6473.4800000000005, "text": " And you should definitely check out LAM, because it makes so much sense to use the average", "tokens": [400, 291, 820, 2138, 1520, 484, 441, 2865, 11, 570, 309, 1669, 370, 709, 2020, 281, 764, 264, 4274], "temperature": 0.0, "avg_logprob": -0.11369599687292221, "compression_ratio": 1.6228070175438596, "no_speech_prob": 1.0451170965097845e-05}, {"id": 1573, "seek": 646534, "start": 6473.4800000000005, "end": 6480.28, "text": " over the layer of that step as a kind of a, you can see here it's kind of got this normalization", "tokens": [670, 264, 4583, 295, 300, 1823, 382, 257, 733, 295, 257, 11, 291, 393, 536, 510, 309, 311, 733, 295, 658, 341, 2710, 2144], "temperature": 0.0, "avg_logprob": -0.11369599687292221, "compression_ratio": 1.6228070175438596, "no_speech_prob": 1.0451170965097845e-05}, {"id": 1574, "seek": 646534, "start": 6480.28, "end": 6482.56, "text": " going on.", "tokens": [516, 322, 13], "temperature": 0.0, "avg_logprob": -0.11369599687292221, "compression_ratio": 1.6228070175438596, "no_speech_prob": 1.0451170965097845e-05}, {"id": 1575, "seek": 646534, "start": 6482.56, "end": 6489.4800000000005, "text": " Because it's just really unlikely that every individual parameter in that tensor, you don't", "tokens": [1436, 309, 311, 445, 534, 17518, 300, 633, 2609, 13075, 294, 300, 40863, 11, 291, 500, 380], "temperature": 0.0, "avg_logprob": -0.11369599687292221, "compression_ratio": 1.6228070175438596, "no_speech_prob": 1.0451170965097845e-05}, {"id": 1576, "seek": 646534, "start": 6489.4800000000005, "end": 6494.12, "text": " want to divide it by its squared gradients, because it's going to vary too much.", "tokens": [528, 281, 9845, 309, 538, 1080, 8889, 2771, 2448, 11, 570, 309, 311, 516, 281, 10559, 886, 709, 13], "temperature": 0.0, "avg_logprob": -0.11369599687292221, "compression_ratio": 1.6228070175438596, "no_speech_prob": 1.0451170965097845e-05}, {"id": 1577, "seek": 649412, "start": 6494.12, "end": 6498.24, "text": " There's just too much chance that there's going to be a 1e neg 7 in there somewhere or", "tokens": [821, 311, 445, 886, 709, 2931, 300, 456, 311, 516, 281, 312, 257, 502, 68, 2485, 1614, 294, 456, 4079, 420], "temperature": 0.0, "avg_logprob": -0.170194027787548, "compression_ratio": 1.697674418604651, "no_speech_prob": 5.255070391285699e-06}, {"id": 1578, "seek": 649412, "start": 6498.24, "end": 6499.24, "text": " something.", "tokens": [746, 13], "temperature": 0.0, "avg_logprob": -0.170194027787548, "compression_ratio": 1.697674418604651, "no_speech_prob": 5.255070391285699e-06}, {"id": 1579, "seek": 649412, "start": 6499.24, "end": 6500.24, "text": " Right?", "tokens": [1779, 30], "temperature": 0.0, "avg_logprob": -0.170194027787548, "compression_ratio": 1.697674418604651, "no_speech_prob": 5.255070391285699e-06}, {"id": 1580, "seek": 649412, "start": 6500.24, "end": 6502.84, "text": " So this to me is exactly the right way to do it.", "tokens": [407, 341, 281, 385, 307, 2293, 264, 558, 636, 281, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.170194027787548, "compression_ratio": 1.697674418604651, "no_speech_prob": 5.255070391285699e-06}, {"id": 1581, "seek": 649412, "start": 6502.84, "end": 6508.5599999999995, "text": " And this is kind of like the first optimizer I've seen, where I just kind of think like,", "tokens": [400, 341, 307, 733, 295, 411, 264, 700, 5028, 6545, 286, 600, 1612, 11, 689, 286, 445, 733, 295, 519, 411, 11], "temperature": 0.0, "avg_logprob": -0.170194027787548, "compression_ratio": 1.697674418604651, "no_speech_prob": 5.255070391285699e-06}, {"id": 1582, "seek": 649412, "start": 6508.5599999999995, "end": 6511.8, "text": " oh, finally I feel like people are heading in the right direction.", "tokens": [1954, 11, 2721, 286, 841, 411, 561, 366, 9864, 294, 264, 558, 3513, 13], "temperature": 0.0, "avg_logprob": -0.170194027787548, "compression_ratio": 1.697674418604651, "no_speech_prob": 5.255070391285699e-06}, {"id": 1583, "seek": 649412, "start": 6511.8, "end": 6515.88, "text": " But when you really study this optimizer, you'll realize that everything we thought about optimizers", "tokens": [583, 562, 291, 534, 2979, 341, 5028, 6545, 11, 291, 603, 4325, 300, 1203, 321, 1194, 466, 5028, 22525], "temperature": 0.0, "avg_logprob": -0.170194027787548, "compression_ratio": 1.697674418604651, "no_speech_prob": 5.255070391285699e-06}, {"id": 1584, "seek": 649412, "start": 6515.88, "end": 6517.96, "text": " kind of doesn't make sense.", "tokens": [733, 295, 1177, 380, 652, 2020, 13], "temperature": 0.0, "avg_logprob": -0.170194027787548, "compression_ratio": 1.697674418604651, "no_speech_prob": 5.255070391285699e-06}, {"id": 1585, "seek": 651796, "start": 6517.96, "end": 6527.0, "text": " The way optimizers are going with things like LAM is the whole idea of like, what is the", "tokens": [440, 636, 5028, 22525, 366, 516, 365, 721, 411, 441, 2865, 307, 264, 1379, 1558, 295, 411, 11, 437, 307, 264], "temperature": 0.0, "avg_logprob": -0.1387652064977067, "compression_ratio": 1.5772727272727274, "no_speech_prob": 2.2602230274060275e-06}, {"id": 1586, "seek": 651796, "start": 6527.0, "end": 6528.92, "text": " magnitude of our step?", "tokens": [15668, 295, 527, 1823, 30], "temperature": 0.0, "avg_logprob": -0.1387652064977067, "compression_ratio": 1.5772727272727274, "no_speech_prob": 2.2602230274060275e-06}, {"id": 1587, "seek": 651796, "start": 6528.92, "end": 6533.36, "text": " It just looks very different to everything we kind of thought of before.", "tokens": [467, 445, 1542, 588, 819, 281, 1203, 321, 733, 295, 1194, 295, 949, 13], "temperature": 0.0, "avg_logprob": -0.1387652064977067, "compression_ratio": 1.5772727272727274, "no_speech_prob": 2.2602230274060275e-06}, {"id": 1588, "seek": 651796, "start": 6533.36, "end": 6537.2, "text": " So check out this paper.", "tokens": [407, 1520, 484, 341, 3035, 13], "temperature": 0.0, "avg_logprob": -0.1387652064977067, "compression_ratio": 1.5772727272727274, "no_speech_prob": 2.2602230274060275e-06}, {"id": 1589, "seek": 651796, "start": 6537.2, "end": 6541.56, "text": " This path might look slightly intimidating at first, but now you know all of these things.", "tokens": [639, 3100, 1062, 574, 4748, 29714, 412, 700, 11, 457, 586, 291, 458, 439, 295, 613, 721, 13], "temperature": 0.0, "avg_logprob": -0.1387652064977067, "compression_ratio": 1.5772727272727274, "no_speech_prob": 2.2602230274060275e-06}, {"id": 1590, "seek": 651796, "start": 6541.56, "end": 6544.92, "text": " What they all are and you know why they exist.", "tokens": [708, 436, 439, 366, 293, 291, 458, 983, 436, 2514, 13], "temperature": 0.0, "avg_logprob": -0.1387652064977067, "compression_ratio": 1.5772727272727274, "no_speech_prob": 2.2602230274060275e-06}, {"id": 1591, "seek": 654492, "start": 6544.92, "end": 6548.06, "text": " So I think you'll be fine.", "tokens": [407, 286, 519, 291, 603, 312, 2489, 13], "temperature": 0.0, "avg_logprob": -0.23673747498312114, "compression_ratio": 1.3798882681564246, "no_speech_prob": 4.2892816054518335e-06}, {"id": 1592, "seek": 654492, "start": 6548.06, "end": 6554.2, "text": " So here's how we create a LAM optimizer, and here's how we fit with it.", "tokens": [407, 510, 311, 577, 321, 1884, 257, 441, 2865, 5028, 6545, 11, 293, 510, 311, 577, 321, 3318, 365, 309, 13], "temperature": 0.0, "avg_logprob": -0.23673747498312114, "compression_ratio": 1.3798882681564246, "no_speech_prob": 4.2892816054518335e-06}, {"id": 1593, "seek": 654492, "start": 6554.2, "end": 6556.24, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.23673747498312114, "compression_ratio": 1.3798882681564246, "no_speech_prob": 4.2892816054518335e-06}, {"id": 1594, "seek": 654492, "start": 6556.24, "end": 6560.68, "text": " That is that.", "tokens": [663, 307, 300, 13], "temperature": 0.0, "avg_logprob": -0.23673747498312114, "compression_ratio": 1.3798882681564246, "no_speech_prob": 4.2892816054518335e-06}, {"id": 1595, "seek": 654492, "start": 6560.68, "end": 6565.16, "text": " Unless Sylvia says otherwise.", "tokens": [16581, 33349, 11617, 1619, 5911, 13], "temperature": 0.0, "avg_logprob": -0.23673747498312114, "compression_ratio": 1.3798882681564246, "no_speech_prob": 4.2892816054518335e-06}, {"id": 1596, "seek": 654492, "start": 6565.16, "end": 6568.58, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.23673747498312114, "compression_ratio": 1.3798882681564246, "no_speech_prob": 4.2892816054518335e-06}, {"id": 1597, "seek": 654492, "start": 6568.58, "end": 6574.8, "text": " So as I was building this, I got so sick of runner, because I kept on wondering when do", "tokens": [407, 382, 286, 390, 2390, 341, 11, 286, 658, 370, 4998, 295, 24376, 11, 570, 286, 4305, 322, 6359, 562, 360], "temperature": 0.0, "avg_logprob": -0.23673747498312114, "compression_ratio": 1.3798882681564246, "no_speech_prob": 4.2892816054518335e-06}, {"id": 1598, "seek": 657480, "start": 6574.8, "end": 6578.52, "text": " I pass the runner, when do I pass the learner?", "tokens": [286, 1320, 264, 24376, 11, 562, 360, 286, 1320, 264, 33347, 30], "temperature": 0.0, "avg_logprob": -0.13087874119824702, "compression_ratio": 1.770334928229665, "no_speech_prob": 4.49508115707431e-06}, {"id": 1599, "seek": 657480, "start": 6578.52, "end": 6586.88, "text": " And then I kind of suddenly thought, again, once every month or two, I actually sit and", "tokens": [400, 550, 286, 733, 295, 5800, 1194, 11, 797, 11, 1564, 633, 1618, 420, 732, 11, 286, 767, 1394, 293], "temperature": 0.0, "avg_logprob": -0.13087874119824702, "compression_ratio": 1.770334928229665, "no_speech_prob": 4.49508115707431e-06}, {"id": 1600, "seek": 657480, "start": 6586.88, "end": 6587.88, "text": " think.", "tokens": [519, 13], "temperature": 0.0, "avg_logprob": -0.13087874119824702, "compression_ratio": 1.770334928229665, "no_speech_prob": 4.49508115707431e-06}, {"id": 1601, "seek": 657480, "start": 6587.88, "end": 6591.6, "text": " And it's only when I get really frustrated.", "tokens": [400, 309, 311, 787, 562, 286, 483, 534, 15751, 13], "temperature": 0.0, "avg_logprob": -0.13087874119824702, "compression_ratio": 1.770334928229665, "no_speech_prob": 4.49508115707431e-06}, {"id": 1602, "seek": 657480, "start": 6591.6, "end": 6596.38, "text": " So I was getting really frustrated with runners, and I actually decided to sit and think.", "tokens": [407, 286, 390, 1242, 534, 15751, 365, 33892, 11, 293, 286, 767, 3047, 281, 1394, 293, 519, 13], "temperature": 0.0, "avg_logprob": -0.13087874119824702, "compression_ratio": 1.770334928229665, "no_speech_prob": 4.49508115707431e-06}, {"id": 1603, "seek": 657480, "start": 6596.38, "end": 6602.52, "text": " And I looked at the definition of learner, and I thought, wait, it doesn't do anything", "tokens": [400, 286, 2956, 412, 264, 7123, 295, 33347, 11, 293, 286, 1194, 11, 1699, 11, 309, 1177, 380, 360, 1340], "temperature": 0.0, "avg_logprob": -0.13087874119824702, "compression_ratio": 1.770334928229665, "no_speech_prob": 4.49508115707431e-06}, {"id": 1604, "seek": 657480, "start": 6602.52, "end": 6604.16, "text": " at all.", "tokens": [412, 439, 13], "temperature": 0.0, "avg_logprob": -0.13087874119824702, "compression_ratio": 1.770334928229665, "no_speech_prob": 4.49508115707431e-06}, {"id": 1605, "seek": 660416, "start": 6604.16, "end": 6606.2, "text": " It stores three things.", "tokens": [467, 9512, 1045, 721, 13], "temperature": 0.0, "avg_logprob": -0.14018720867990078, "compression_ratio": 1.8070175438596492, "no_speech_prob": 4.029405317851342e-06}, {"id": 1606, "seek": 660416, "start": 6606.2, "end": 6608.599999999999, "text": " What kind of class just stores three things?", "tokens": [708, 733, 295, 1508, 445, 9512, 1045, 721, 30], "temperature": 0.0, "avg_logprob": -0.14018720867990078, "compression_ratio": 1.8070175438596492, "no_speech_prob": 4.029405317851342e-06}, {"id": 1607, "seek": 660416, "start": 6608.599999999999, "end": 6614.12, "text": " And then a runner has a learner in it that stores three things.", "tokens": [400, 550, 257, 24376, 575, 257, 33347, 294, 309, 300, 9512, 1045, 721, 13], "temperature": 0.0, "avg_logprob": -0.14018720867990078, "compression_ratio": 1.8070175438596492, "no_speech_prob": 4.029405317851342e-06}, {"id": 1608, "seek": 660416, "start": 6614.12, "end": 6618.92, "text": " But why don't we store the three things in the runner?", "tokens": [583, 983, 500, 380, 321, 3531, 264, 1045, 721, 294, 264, 24376, 30], "temperature": 0.0, "avg_logprob": -0.14018720867990078, "compression_ratio": 1.8070175438596492, "no_speech_prob": 4.029405317851342e-06}, {"id": 1609, "seek": 660416, "start": 6618.92, "end": 6626.28, "text": " So I took the runner, I took that line of code, I copied it, and I pasted it just here.", "tokens": [407, 286, 1890, 264, 24376, 11, 286, 1890, 300, 1622, 295, 3089, 11, 286, 25365, 309, 11, 293, 286, 1791, 292, 309, 445, 510, 13], "temperature": 0.0, "avg_logprob": -0.14018720867990078, "compression_ratio": 1.8070175438596492, "no_speech_prob": 4.029405317851342e-06}, {"id": 1610, "seek": 660416, "start": 6626.28, "end": 6629.16, "text": " I then renamed runner to learner.", "tokens": [286, 550, 40949, 24376, 281, 33347, 13], "temperature": 0.0, "avg_logprob": -0.14018720867990078, "compression_ratio": 1.8070175438596492, "no_speech_prob": 4.029405317851342e-06}, {"id": 1611, "seek": 662916, "start": 6629.16, "end": 6635.84, "text": " I then found everything that said self.learn and removed the.learn, and I was done.", "tokens": [286, 550, 1352, 1203, 300, 848, 2698, 13, 306, 1083, 293, 7261, 264, 2411, 306, 1083, 11, 293, 286, 390, 1096, 13], "temperature": 0.0, "avg_logprob": -0.11417191737406962, "compression_ratio": 1.893939393939394, "no_speech_prob": 3.39306302521436e-06}, {"id": 1612, "seek": 662916, "start": 6635.84, "end": 6637.72, "text": " And now there's no more runner.", "tokens": [400, 586, 456, 311, 572, 544, 24376, 13], "temperature": 0.0, "avg_logprob": -0.11417191737406962, "compression_ratio": 1.893939393939394, "no_speech_prob": 3.39306302521436e-06}, {"id": 1613, "seek": 662916, "start": 6637.72, "end": 6642.44, "text": " And it's like, oh, it's just one of those obvious refactorings that as soon as I did", "tokens": [400, 309, 311, 411, 11, 1954, 11, 309, 311, 445, 472, 295, 729, 6322, 1895, 15104, 1109, 300, 382, 2321, 382, 286, 630], "temperature": 0.0, "avg_logprob": -0.11417191737406962, "compression_ratio": 1.893939393939394, "no_speech_prob": 3.39306302521436e-06}, {"id": 1614, "seek": 662916, "start": 6642.44, "end": 6647.48, "text": " it, Sylvan was like, why didn't you do it that way in the first place?", "tokens": [309, 11, 33349, 9768, 390, 411, 11, 983, 994, 380, 291, 360, 309, 300, 636, 294, 264, 700, 1081, 30], "temperature": 0.0, "avg_logprob": -0.11417191737406962, "compression_ratio": 1.893939393939394, "no_speech_prob": 3.39306302521436e-06}, {"id": 1615, "seek": 662916, "start": 6647.48, "end": 6649.68, "text": " And I was like, why didn't you fix it that way in the first place?", "tokens": [400, 286, 390, 411, 11, 983, 994, 380, 291, 3191, 309, 300, 636, 294, 264, 700, 1081, 30], "temperature": 0.0, "avg_logprob": -0.11417191737406962, "compression_ratio": 1.893939393939394, "no_speech_prob": 3.39306302521436e-06}, {"id": 1616, "seek": 662916, "start": 6649.68, "end": 6652.92, "text": " But now that we've done it, this is so much easier.", "tokens": [583, 586, 300, 321, 600, 1096, 309, 11, 341, 307, 370, 709, 3571, 13], "temperature": 0.0, "avg_logprob": -0.11417191737406962, "compression_ratio": 1.893939393939394, "no_speech_prob": 3.39306302521436e-06}, {"id": 1617, "seek": 662916, "start": 6652.92, "end": 6656.84, "text": " There's no more get learn run, there's no more having to match these things together.", "tokens": [821, 311, 572, 544, 483, 1466, 1190, 11, 456, 311, 572, 544, 1419, 281, 2995, 613, 721, 1214, 13], "temperature": 0.0, "avg_logprob": -0.11417191737406962, "compression_ratio": 1.893939393939394, "no_speech_prob": 3.39306302521436e-06}, {"id": 1618, "seek": 662916, "start": 6656.84, "end": 6658.92, "text": " It's just super simple.", "tokens": [467, 311, 445, 1687, 2199, 13], "temperature": 0.0, "avg_logprob": -0.11417191737406962, "compression_ratio": 1.893939393939394, "no_speech_prob": 3.39306302521436e-06}, {"id": 1619, "seek": 665892, "start": 6658.92, "end": 6664.4800000000005, "text": " So one of the nice things I like about this kind of Jupyter style of development is I", "tokens": [407, 472, 295, 264, 1481, 721, 286, 411, 466, 341, 733, 295, 22125, 88, 391, 3758, 295, 3250, 307, 286], "temperature": 0.0, "avg_logprob": -0.13509366969869593, "compression_ratio": 1.6182572614107884, "no_speech_prob": 4.006084782304242e-05}, {"id": 1620, "seek": 665892, "start": 6664.4800000000005, "end": 6671.32, "text": " spend a month or two just immersing myself in the code in this very experimental way,", "tokens": [3496, 257, 1618, 420, 732, 445, 16787, 278, 2059, 294, 264, 3089, 294, 341, 588, 17069, 636, 11], "temperature": 0.0, "avg_logprob": -0.13509366969869593, "compression_ratio": 1.6182572614107884, "no_speech_prob": 4.006084782304242e-05}, {"id": 1621, "seek": 665892, "start": 6671.32, "end": 6675.84, "text": " and I feel totally fine throwing it all away and changing everything, because everything's", "tokens": [293, 286, 841, 3879, 2489, 10238, 309, 439, 1314, 293, 4473, 1203, 11, 570, 1203, 311], "temperature": 0.0, "avg_logprob": -0.13509366969869593, "compression_ratio": 1.6182572614107884, "no_speech_prob": 4.006084782304242e-05}, {"id": 1622, "seek": 665892, "start": 6675.84, "end": 6679.16, "text": " small and I can fiddle around with it.", "tokens": [1359, 293, 286, 393, 24553, 2285, 926, 365, 309, 13], "temperature": 0.0, "avg_logprob": -0.13509366969869593, "compression_ratio": 1.6182572614107884, "no_speech_prob": 4.006084782304242e-05}, {"id": 1623, "seek": 665892, "start": 6679.16, "end": 6685.04, "text": " And then after a couple of months, Sylvan and I will just go like, okay, there's a bunch", "tokens": [400, 550, 934, 257, 1916, 295, 2493, 11, 33349, 9768, 293, 286, 486, 445, 352, 411, 11, 1392, 11, 456, 311, 257, 3840], "temperature": 0.0, "avg_logprob": -0.13509366969869593, "compression_ratio": 1.6182572614107884, "no_speech_prob": 4.006084782304242e-05}, {"id": 1624, "seek": 668504, "start": 6685.04, "end": 6689.36, "text": " of things here that work nicely together, and we turn it into some modules.", "tokens": [295, 721, 510, 300, 589, 9594, 1214, 11, 293, 321, 1261, 309, 666, 512, 16679, 13], "temperature": 0.0, "avg_logprob": -0.15026389227973092, "compression_ratio": 1.6653386454183268, "no_speech_prob": 1.6184549167519435e-05}, {"id": 1625, "seek": 668504, "start": 6689.36, "end": 6693.5199999999995, "text": " And so that's how Fast AI version 1 happened.", "tokens": [400, 370, 300, 311, 577, 15968, 7318, 3037, 502, 2011, 13], "temperature": 0.0, "avg_logprob": -0.15026389227973092, "compression_ratio": 1.6653386454183268, "no_speech_prob": 1.6184549167519435e-05}, {"id": 1626, "seek": 668504, "start": 6693.5199999999995, "end": 6697.96, "text": " And people often say to us like, oh, turning it into modules, what a nightmare that must", "tokens": [400, 561, 2049, 584, 281, 505, 411, 11, 1954, 11, 6246, 309, 666, 16679, 11, 437, 257, 18724, 300, 1633], "temperature": 0.0, "avg_logprob": -0.15026389227973092, "compression_ratio": 1.6653386454183268, "no_speech_prob": 1.6184549167519435e-05}, {"id": 1627, "seek": 668504, "start": 6697.96, "end": 6699.08, "text": " have been.", "tokens": [362, 668, 13], "temperature": 0.0, "avg_logprob": -0.15026389227973092, "compression_ratio": 1.6653386454183268, "no_speech_prob": 1.6184549167519435e-05}, {"id": 1628, "seek": 668504, "start": 6699.08, "end": 6703.64, "text": " So here's what was required for me to do that.", "tokens": [407, 510, 311, 437, 390, 4739, 337, 385, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.15026389227973092, "compression_ratio": 1.6653386454183268, "no_speech_prob": 1.6184549167519435e-05}, {"id": 1629, "seek": 668504, "start": 6703.64, "end": 6707.94, "text": " I typed into Skype, Sylvan, please turn this into a module.", "tokens": [286, 33941, 666, 31743, 11, 33349, 9768, 11, 1767, 1261, 341, 666, 257, 10088, 13], "temperature": 0.0, "avg_logprob": -0.15026389227973092, "compression_ratio": 1.6653386454183268, "no_speech_prob": 1.6184549167519435e-05}, {"id": 1630, "seek": 668504, "start": 6707.94, "end": 6709.96, "text": " So that was pretty easy.", "tokens": [407, 300, 390, 1238, 1858, 13], "temperature": 0.0, "avg_logprob": -0.15026389227973092, "compression_ratio": 1.6653386454183268, "no_speech_prob": 1.6184549167519435e-05}, {"id": 1631, "seek": 668504, "start": 6709.96, "end": 6714.0, "text": " And then three hours later, Sylvan typed back and he said, done.", "tokens": [400, 550, 1045, 2496, 1780, 11, 33349, 9768, 33941, 646, 293, 415, 848, 11, 1096, 13], "temperature": 0.0, "avg_logprob": -0.15026389227973092, "compression_ratio": 1.6653386454183268, "no_speech_prob": 1.6184549167519435e-05}, {"id": 1632, "seek": 671400, "start": 6714.0, "end": 6717.12, "text": " It was three hours of work.", "tokens": [467, 390, 1045, 2496, 295, 589, 13], "temperature": 0.0, "avg_logprob": -0.23490077933085332, "compression_ratio": 1.5048076923076923, "no_speech_prob": 3.392843382243882e-06}, {"id": 1633, "seek": 671400, "start": 6717.12, "end": 6723.56, "text": " It was four, five, six months of development in notebooks, three hours to convert it into", "tokens": [467, 390, 1451, 11, 1732, 11, 2309, 2493, 295, 3250, 294, 43782, 11, 1045, 2496, 281, 7620, 309, 666], "temperature": 0.0, "avg_logprob": -0.23490077933085332, "compression_ratio": 1.5048076923076923, "no_speech_prob": 3.392843382243882e-06}, {"id": 1634, "seek": 671400, "start": 6723.56, "end": 6724.56, "text": " modules.", "tokens": [16679, 13], "temperature": 0.0, "avg_logprob": -0.23490077933085332, "compression_ratio": 1.5048076923076923, "no_speech_prob": 3.392843382243882e-06}, {"id": 1635, "seek": 671400, "start": 6724.56, "end": 6727.16, "text": " So it's really, it's not a hassle.", "tokens": [407, 309, 311, 534, 11, 309, 311, 406, 257, 39526, 13], "temperature": 0.0, "avg_logprob": -0.23490077933085332, "compression_ratio": 1.5048076923076923, "no_speech_prob": 3.392843382243882e-06}, {"id": 1636, "seek": 671400, "start": 6727.16, "end": 6728.96, "text": " And I find this quite delightful.", "tokens": [400, 286, 915, 341, 1596, 35194, 13], "temperature": 0.0, "avg_logprob": -0.23490077933085332, "compression_ratio": 1.5048076923076923, "no_speech_prob": 3.392843382243882e-06}, {"id": 1637, "seek": 671400, "start": 6728.96, "end": 6731.72, "text": " It works super well.", "tokens": [467, 1985, 1687, 731, 13], "temperature": 0.0, "avg_logprob": -0.23490077933085332, "compression_ratio": 1.5048076923076923, "no_speech_prob": 3.392843382243882e-06}, {"id": 1638, "seek": 671400, "start": 6731.72, "end": 6733.44, "text": " So no more runner.", "tokens": [407, 572, 544, 24376, 13], "temperature": 0.0, "avg_logprob": -0.23490077933085332, "compression_ratio": 1.5048076923076923, "no_speech_prob": 3.392843382243882e-06}, {"id": 1639, "seek": 671400, "start": 6733.44, "end": 6734.44, "text": " Thank God.", "tokens": [1044, 1265, 13], "temperature": 0.0, "avg_logprob": -0.23490077933085332, "compression_ratio": 1.5048076923076923, "no_speech_prob": 3.392843382243882e-06}, {"id": 1640, "seek": 671400, "start": 6734.44, "end": 6736.24, "text": " Runner is now called learner.", "tokens": [50105, 307, 586, 1219, 33347, 13], "temperature": 0.0, "avg_logprob": -0.23490077933085332, "compression_ratio": 1.5048076923076923, "no_speech_prob": 3.392843382243882e-06}, {"id": 1641, "seek": 671400, "start": 6736.24, "end": 6739.32, "text": " We're kind of back to where we were.", "tokens": [492, 434, 733, 295, 646, 281, 689, 321, 645, 13], "temperature": 0.0, "avg_logprob": -0.23490077933085332, "compression_ratio": 1.5048076923076923, "no_speech_prob": 3.392843382243882e-06}, {"id": 1642, "seek": 673932, "start": 6739.32, "end": 6744.84, "text": " We want progress bars.", "tokens": [492, 528, 4205, 10228, 13], "temperature": 0.0, "avg_logprob": -0.1397475648200375, "compression_ratio": 1.7009803921568627, "no_speech_prob": 6.5399658524256665e-06}, {"id": 1643, "seek": 673932, "start": 6744.84, "end": 6749.24, "text": " So Sylvan wrote this fantastic package called Fast Progress, which you should totally check", "tokens": [407, 33349, 9768, 4114, 341, 5456, 7372, 1219, 15968, 32587, 11, 597, 291, 820, 3879, 1520], "temperature": 0.0, "avg_logprob": -0.1397475648200375, "compression_ratio": 1.7009803921568627, "no_speech_prob": 6.5399658524256665e-06}, {"id": 1644, "seek": 673932, "start": 6749.24, "end": 6750.719999999999, "text": " out.", "tokens": [484, 13], "temperature": 0.0, "avg_logprob": -0.1397475648200375, "compression_ratio": 1.7009803921568627, "no_speech_prob": 6.5399658524256665e-06}, {"id": 1645, "seek": 673932, "start": 6750.719999999999, "end": 6755.32, "text": " And we're allowed to import it, because remember, we're allowed to import modules that are not", "tokens": [400, 321, 434, 4350, 281, 974, 309, 11, 570, 1604, 11, 321, 434, 4350, 281, 974, 16679, 300, 366, 406], "temperature": 0.0, "avg_logprob": -0.1397475648200375, "compression_ratio": 1.7009803921568627, "no_speech_prob": 6.5399658524256665e-06}, {"id": 1646, "seek": 673932, "start": 6755.32, "end": 6757.04, "text": " data science modules.", "tokens": [1412, 3497, 16679, 13], "temperature": 0.0, "avg_logprob": -0.1397475648200375, "compression_ratio": 1.7009803921568627, "no_speech_prob": 6.5399658524256665e-06}, {"id": 1647, "seek": 673932, "start": 6757.04, "end": 6759.24, "text": " Progress bar is not a data science module.", "tokens": [32587, 2159, 307, 406, 257, 1412, 3497, 10088, 13], "temperature": 0.0, "avg_logprob": -0.1397475648200375, "compression_ratio": 1.7009803921568627, "no_speech_prob": 6.5399658524256665e-06}, {"id": 1648, "seek": 673932, "start": 6759.24, "end": 6764.28, "text": " But now we need to attach this progress bar to our callback system.", "tokens": [583, 586, 321, 643, 281, 5085, 341, 4205, 2159, 281, 527, 818, 3207, 1185, 13], "temperature": 0.0, "avg_logprob": -0.1397475648200375, "compression_ratio": 1.7009803921568627, "no_speech_prob": 6.5399658524256665e-06}, {"id": 1649, "seek": 676428, "start": 6764.28, "end": 6769.48, "text": " So let's grab our ImageNet data as before, create a little thing with, I don't know,", "tokens": [407, 718, 311, 4444, 527, 29903, 31890, 1412, 382, 949, 11, 1884, 257, 707, 551, 365, 11, 286, 500, 380, 458, 11], "temperature": 0.0, "avg_logprob": -0.14428600822527385, "compression_ratio": 1.5384615384615385, "no_speech_prob": 6.961789495107951e-06}, {"id": 1650, "seek": 676428, "start": 6769.48, "end": 6772.08, "text": " 432 filled layers.", "tokens": [1017, 11440, 6412, 7914, 13], "temperature": 0.0, "avg_logprob": -0.14428600822527385, "compression_ratio": 1.5384615384615385, "no_speech_prob": 6.961789495107951e-06}, {"id": 1651, "seek": 676428, "start": 6772.08, "end": 6776.759999999999, "text": " Let's rewrite our stats callback.", "tokens": [961, 311, 28132, 527, 18152, 818, 3207, 13], "temperature": 0.0, "avg_logprob": -0.14428600822527385, "compression_ratio": 1.5384615384615385, "no_speech_prob": 6.961789495107951e-06}, {"id": 1652, "seek": 676428, "start": 6776.759999999999, "end": 6781.46, "text": " It's basically exactly the same as it was before, except now we're storing our stats", "tokens": [467, 311, 1936, 2293, 264, 912, 382, 309, 390, 949, 11, 3993, 586, 321, 434, 26085, 527, 18152], "temperature": 0.0, "avg_logprob": -0.14428600822527385, "compression_ratio": 1.5384615384615385, "no_speech_prob": 6.961789495107951e-06}, {"id": 1653, "seek": 676428, "start": 6781.46, "end": 6784.28, "text": " in an array.", "tokens": [294, 364, 10225, 13], "temperature": 0.0, "avg_logprob": -0.14428600822527385, "compression_ratio": 1.5384615384615385, "no_speech_prob": 6.961789495107951e-06}, {"id": 1654, "seek": 676428, "start": 6784.28, "end": 6787.599999999999, "text": " And we're just passing off the array to logger.", "tokens": [400, 321, 434, 445, 8437, 766, 264, 10225, 281, 3565, 1321, 13], "temperature": 0.0, "avg_logprob": -0.14428600822527385, "compression_ratio": 1.5384615384615385, "no_speech_prob": 6.961789495107951e-06}, {"id": 1655, "seek": 676428, "start": 6787.599999999999, "end": 6792.28, "text": " Remember logger is just a print statement at this stage.", "tokens": [5459, 3565, 1321, 307, 445, 257, 4482, 5629, 412, 341, 3233, 13], "temperature": 0.0, "avg_logprob": -0.14428600822527385, "compression_ratio": 1.5384615384615385, "no_speech_prob": 6.961789495107951e-06}, {"id": 1656, "seek": 679228, "start": 6792.28, "end": 6798.639999999999, "text": " And then we will create our progress bar callback.", "tokens": [400, 550, 321, 486, 1884, 527, 4205, 2159, 818, 3207, 13], "temperature": 0.0, "avg_logprob": -0.1504814965384347, "compression_ratio": 1.577319587628866, "no_speech_prob": 1.067689481715206e-06}, {"id": 1657, "seek": 679228, "start": 6798.639999999999, "end": 6800.96, "text": " And that is actually the entirety of it.", "tokens": [400, 300, 307, 767, 264, 31557, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.1504814965384347, "compression_ratio": 1.577319587628866, "no_speech_prob": 1.067689481715206e-06}, {"id": 1658, "seek": 679228, "start": 6800.96, "end": 6802.179999999999, "text": " That's all we need.", "tokens": [663, 311, 439, 321, 643, 13], "temperature": 0.0, "avg_logprob": -0.1504814965384347, "compression_ratio": 1.577319587628866, "no_speech_prob": 1.067689481715206e-06}, {"id": 1659, "seek": 679228, "start": 6802.179999999999, "end": 6809.4, "text": " So with that, we can now add progress callback to our callback functions and grab our learner.", "tokens": [407, 365, 300, 11, 321, 393, 586, 909, 4205, 818, 3207, 281, 527, 818, 3207, 6828, 293, 4444, 527, 33347, 13], "temperature": 0.0, "avg_logprob": -0.1504814965384347, "compression_ratio": 1.577319587628866, "no_speech_prob": 1.067689481715206e-06}, {"id": 1660, "seek": 679228, "start": 6809.4, "end": 6811.4, "text": " No runner.", "tokens": [883, 24376, 13], "temperature": 0.0, "avg_logprob": -0.1504814965384347, "compression_ratio": 1.577319587628866, "no_speech_prob": 1.067689481715206e-06}, {"id": 1661, "seek": 679228, "start": 6811.4, "end": 6813.5199999999995, "text": " Fit.", "tokens": [29263, 13], "temperature": 0.0, "avg_logprob": -0.1504814965384347, "compression_ratio": 1.577319587628866, "no_speech_prob": 1.067689481715206e-06}, {"id": 1662, "seek": 679228, "start": 6813.5199999999995, "end": 6815.36, "text": " Now that's kind of magic, right?", "tokens": [823, 300, 311, 733, 295, 5585, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1504814965384347, "compression_ratio": 1.577319587628866, "no_speech_prob": 1.067689481715206e-06}, {"id": 1663, "seek": 679228, "start": 6815.36, "end": 6819.24, "text": " That's all the code we needed to make this happen.", "tokens": [663, 311, 439, 264, 3089, 321, 2978, 281, 652, 341, 1051, 13], "temperature": 0.0, "avg_logprob": -0.1504814965384347, "compression_ratio": 1.577319587628866, "no_speech_prob": 1.067689481715206e-06}, {"id": 1664, "seek": 681924, "start": 6819.24, "end": 6822.4, "text": " And look at the end, oh, creates a nice little table.", "tokens": [400, 574, 412, 264, 917, 11, 1954, 11, 7829, 257, 1481, 707, 3199, 13], "temperature": 0.0, "avg_logprob": -0.19703241630836768, "compression_ratio": 1.6458333333333333, "no_speech_prob": 1.4509333595924545e-05}, {"id": 1665, "seek": 681924, "start": 6822.4, "end": 6823.58, "text": " Pretty good.", "tokens": [10693, 665, 13], "temperature": 0.0, "avg_logprob": -0.19703241630836768, "compression_ratio": 1.6458333333333333, "no_speech_prob": 1.4509333595924545e-05}, {"id": 1666, "seek": 681924, "start": 6823.58, "end": 6830.12, "text": " So this is thanks to just careful, simple, decoupled software engineering.", "tokens": [407, 341, 307, 3231, 281, 445, 5026, 11, 2199, 11, 979, 263, 15551, 4722, 7043, 13], "temperature": 0.0, "avg_logprob": -0.19703241630836768, "compression_ratio": 1.6458333333333333, "no_speech_prob": 1.4509333595924545e-05}, {"id": 1667, "seek": 681924, "start": 6830.12, "end": 6834.12, "text": " We just said, okay, when you start fitting, you've got to create the master bar.", "tokens": [492, 445, 848, 11, 1392, 11, 562, 291, 722, 15669, 11, 291, 600, 658, 281, 1884, 264, 4505, 2159, 13], "temperature": 0.0, "avg_logprob": -0.19703241630836768, "compression_ratio": 1.6458333333333333, "no_speech_prob": 1.4509333595924545e-05}, {"id": 1668, "seek": 681924, "start": 6834.12, "end": 6838.36, "text": " So that's the thing that tracks the epochs.", "tokens": [407, 300, 311, 264, 551, 300, 10218, 264, 30992, 28346, 13], "temperature": 0.0, "avg_logprob": -0.19703241630836768, "compression_ratio": 1.6458333333333333, "no_speech_prob": 1.4509333595924545e-05}, {"id": 1669, "seek": 681924, "start": 6838.36, "end": 6840.96, "text": " And then tell the master bar we're starting.", "tokens": [400, 550, 980, 264, 4505, 2159, 321, 434, 2891, 13], "temperature": 0.0, "avg_logprob": -0.19703241630836768, "compression_ratio": 1.6458333333333333, "no_speech_prob": 1.4509333595924545e-05}, {"id": 1670, "seek": 681924, "start": 6840.96, "end": 6845.48, "text": " And then replace the logger function not with print, but with master bar dot write.", "tokens": [400, 550, 7406, 264, 3565, 1321, 2445, 406, 365, 4482, 11, 457, 365, 4505, 2159, 5893, 2464, 13], "temperature": 0.0, "avg_logprob": -0.19703241630836768, "compression_ratio": 1.6458333333333333, "no_speech_prob": 1.4509333595924545e-05}, {"id": 1671, "seek": 684548, "start": 6845.48, "end": 6849.24, "text": " So it's going to print the HTML into there.", "tokens": [407, 309, 311, 516, 281, 4482, 264, 17995, 666, 456, 13], "temperature": 0.0, "avg_logprob": -0.09515118598937988, "compression_ratio": 1.6150442477876106, "no_speech_prob": 6.643009783147136e-06}, {"id": 1672, "seek": 684548, "start": 6849.24, "end": 6855.4, "text": " And then after we've done a batch, update our progress bar.", "tokens": [400, 550, 934, 321, 600, 1096, 257, 15245, 11, 5623, 527, 4205, 2159, 13], "temperature": 0.0, "avg_logprob": -0.09515118598937988, "compression_ratio": 1.6150442477876106, "no_speech_prob": 6.643009783147136e-06}, {"id": 1673, "seek": 684548, "start": 6855.4, "end": 6860.2, "text": " When we begin an epoch or begin validating, we'll have to create a new progress bar.", "tokens": [1133, 321, 1841, 364, 30992, 339, 420, 1841, 7363, 990, 11, 321, 603, 362, 281, 1884, 257, 777, 4205, 2159, 13], "temperature": 0.0, "avg_logprob": -0.09515118598937988, "compression_ratio": 1.6150442477876106, "no_speech_prob": 6.643009783147136e-06}, {"id": 1674, "seek": 684548, "start": 6860.2, "end": 6863.2, "text": " And when we're done fitting, tell the master bar we're finished.", "tokens": [400, 562, 321, 434, 1096, 15669, 11, 980, 264, 4505, 2159, 321, 434, 4335, 13], "temperature": 0.0, "avg_logprob": -0.09515118598937988, "compression_ratio": 1.6150442477876106, "no_speech_prob": 6.643009783147136e-06}, {"id": 1675, "seek": 684548, "start": 6863.2, "end": 6864.2, "text": " That's it.", "tokens": [663, 311, 309, 13], "temperature": 0.0, "avg_logprob": -0.09515118598937988, "compression_ratio": 1.6150442477876106, "no_speech_prob": 6.643009783147136e-06}, {"id": 1676, "seek": 684548, "start": 6864.2, "end": 6865.2, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.09515118598937988, "compression_ratio": 1.6150442477876106, "no_speech_prob": 6.643009783147136e-06}, {"id": 1677, "seek": 684548, "start": 6865.2, "end": 6869.44, "text": " So it's very easy to, once you have a system like this, to integrate with other libraries", "tokens": [407, 309, 311, 588, 1858, 281, 11, 1564, 291, 362, 257, 1185, 411, 341, 11, 281, 13365, 365, 661, 15148], "temperature": 0.0, "avg_logprob": -0.09515118598937988, "compression_ratio": 1.6150442477876106, "no_speech_prob": 6.643009783147136e-06}, {"id": 1678, "seek": 686944, "start": 6869.44, "end": 6878.12, "text": " if you want to use TensorBoard or Visdom or send yourself a Twilio message or whatever.", "tokens": [498, 291, 528, 281, 764, 34306, 22493, 515, 420, 10410, 4121, 420, 2845, 1803, 257, 2574, 388, 1004, 3636, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.239659054444568, "compression_ratio": 1.5904761904761904, "no_speech_prob": 4.637514393834863e-06}, {"id": 1679, "seek": 686944, "start": 6878.12, "end": 6879.12, "text": " Right?", "tokens": [1779, 30], "temperature": 0.0, "avg_logprob": -0.239659054444568, "compression_ratio": 1.5904761904761904, "no_speech_prob": 4.637514393834863e-06}, {"id": 1680, "seek": 686944, "start": 6879.12, "end": 6881.44, "text": " It's super easy.", "tokens": [467, 311, 1687, 1858, 13], "temperature": 0.0, "avg_logprob": -0.239659054444568, "compression_ratio": 1.5904761904761904, "no_speech_prob": 4.637514393834863e-06}, {"id": 1681, "seek": 686944, "start": 6881.44, "end": 6883.719999999999, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.239659054444568, "compression_ratio": 1.5904761904761904, "no_speech_prob": 4.637514393834863e-06}, {"id": 1682, "seek": 686944, "start": 6883.719999999999, "end": 6889.44, "text": " So we're going to finish, I think we're going to finish, unless this goes faster than I", "tokens": [407, 321, 434, 516, 281, 2413, 11, 286, 519, 321, 434, 516, 281, 2413, 11, 5969, 341, 1709, 4663, 813, 286], "temperature": 0.0, "avg_logprob": -0.239659054444568, "compression_ratio": 1.5904761904761904, "no_speech_prob": 4.637514393834863e-06}, {"id": 1683, "seek": 686944, "start": 6889.44, "end": 6892.0599999999995, "text": " expect, with data augmentation.", "tokens": [2066, 11, 365, 1412, 14501, 19631, 13], "temperature": 0.0, "avg_logprob": -0.239659054444568, "compression_ratio": 1.5904761904761904, "no_speech_prob": 4.637514393834863e-06}, {"id": 1684, "seek": 686944, "start": 6892.0599999999995, "end": 6895.639999999999, "text": " So so far we've seen how to create our optimizers.", "tokens": [407, 370, 1400, 321, 600, 1612, 577, 281, 1884, 527, 5028, 22525, 13], "temperature": 0.0, "avg_logprob": -0.239659054444568, "compression_ratio": 1.5904761904761904, "no_speech_prob": 4.637514393834863e-06}, {"id": 1685, "seek": 686944, "start": 6895.639999999999, "end": 6898.2, "text": " We've seen how to create our data blocks API.", "tokens": [492, 600, 1612, 577, 281, 1884, 527, 1412, 8474, 9362, 13], "temperature": 0.0, "avg_logprob": -0.239659054444568, "compression_ratio": 1.5904761904761904, "no_speech_prob": 4.637514393834863e-06}, {"id": 1686, "seek": 689820, "start": 6898.2, "end": 6904.12, "text": " And we can use all that to train a reasonably good ImageNet model.", "tokens": [400, 321, 393, 764, 439, 300, 281, 3847, 257, 23551, 665, 29903, 31890, 2316, 13], "temperature": 0.0, "avg_logprob": -0.19539068363330983, "compression_ratio": 1.577092511013216, "no_speech_prob": 1.1659092706395313e-05}, {"id": 1687, "seek": 689820, "start": 6904.12, "end": 6907.4, "text": " But to make a better ImageNet model, it's a bit short of data.", "tokens": [583, 281, 652, 257, 1101, 29903, 31890, 2316, 11, 309, 311, 257, 857, 2099, 295, 1412, 13], "temperature": 0.0, "avg_logprob": -0.19539068363330983, "compression_ratio": 1.577092511013216, "no_speech_prob": 1.1659092706395313e-05}, {"id": 1688, "seek": 689820, "start": 6907.4, "end": 6911.4, "text": " So we should use data augmentation as we all know.", "tokens": [407, 321, 820, 764, 1412, 14501, 19631, 382, 321, 439, 458, 13], "temperature": 0.0, "avg_logprob": -0.19539068363330983, "compression_ratio": 1.577092511013216, "no_speech_prob": 1.1659092706395313e-05}, {"id": 1689, "seek": 689820, "start": 6911.4, "end": 6917.12, "text": " Now so let's load it in as before.", "tokens": [823, 370, 718, 311, 3677, 309, 294, 382, 949, 13], "temperature": 0.0, "avg_logprob": -0.19539068363330983, "compression_ratio": 1.577092511013216, "no_speech_prob": 1.1659092706395313e-05}, {"id": 1690, "seek": 689820, "start": 6917.12, "end": 6919.24, "text": " And let's just grab an image list for now.", "tokens": [400, 718, 311, 445, 4444, 364, 3256, 1329, 337, 586, 13], "temperature": 0.0, "avg_logprob": -0.19539068363330983, "compression_ratio": 1.577092511013216, "no_speech_prob": 1.1659092706395313e-05}, {"id": 1691, "seek": 689820, "start": 6919.24, "end": 6920.24, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.19539068363330983, "compression_ratio": 1.577092511013216, "no_speech_prob": 1.1659092706395313e-05}, {"id": 1692, "seek": 689820, "start": 6920.24, "end": 6924.16, "text": " And the only transforms we're going to use is resize fixed.", "tokens": [400, 264, 787, 35592, 321, 434, 516, 281, 764, 307, 50069, 6806, 13], "temperature": 0.0, "avg_logprob": -0.19539068363330983, "compression_ratio": 1.577092511013216, "no_speech_prob": 1.1659092706395313e-05}, {"id": 1693, "seek": 689820, "start": 6924.16, "end": 6927.04, "text": " And here's our chap with a tensh.", "tokens": [400, 510, 311, 527, 13223, 365, 257, 10688, 71, 13], "temperature": 0.0, "avg_logprob": -0.19539068363330983, "compression_ratio": 1.577092511013216, "no_speech_prob": 1.1659092706395313e-05}, {"id": 1694, "seek": 692704, "start": 6927.04, "end": 6931.24, "text": " And let's just actually open the original Pillow image without resizing it to see what", "tokens": [400, 718, 311, 445, 767, 1269, 264, 3380, 44656, 305, 3256, 1553, 725, 3319, 309, 281, 536, 437], "temperature": 0.0, "avg_logprob": -0.140851054275245, "compression_ratio": 1.652, "no_speech_prob": 4.710806024377234e-06}, {"id": 1695, "seek": 692704, "start": 6931.24, "end": 6932.56, "text": " he looks like full size.", "tokens": [415, 1542, 411, 1577, 2744, 13], "temperature": 0.0, "avg_logprob": -0.140851054275245, "compression_ratio": 1.652, "no_speech_prob": 4.710806024377234e-06}, {"id": 1696, "seek": 692704, "start": 6932.56, "end": 6935.28, "text": " So here he is.", "tokens": [407, 510, 415, 307, 13], "temperature": 0.0, "avg_logprob": -0.140851054275245, "compression_ratio": 1.652, "no_speech_prob": 4.710806024377234e-06}, {"id": 1697, "seek": 692704, "start": 6935.28, "end": 6938.4, "text": " And I want to point something out.", "tokens": [400, 286, 528, 281, 935, 746, 484, 13], "temperature": 0.0, "avg_logprob": -0.140851054275245, "compression_ratio": 1.652, "no_speech_prob": 4.710806024377234e-06}, {"id": 1698, "seek": 692704, "start": 6938.4, "end": 6943.36, "text": " When you resize, there are various resampling methods you can use.", "tokens": [1133, 291, 50069, 11, 456, 366, 3683, 725, 335, 11970, 7150, 291, 393, 764, 13], "temperature": 0.0, "avg_logprob": -0.140851054275245, "compression_ratio": 1.652, "no_speech_prob": 4.710806024377234e-06}, {"id": 1699, "seek": 692704, "start": 6943.36, "end": 6947.74, "text": " So basically when you go from one size image to another size image, do you like take the", "tokens": [407, 1936, 562, 291, 352, 490, 472, 2744, 3256, 281, 1071, 2744, 3256, 11, 360, 291, 411, 747, 264], "temperature": 0.0, "avg_logprob": -0.140851054275245, "compression_ratio": 1.652, "no_speech_prob": 4.710806024377234e-06}, {"id": 1700, "seek": 692704, "start": 6947.74, "end": 6950.24, "text": " pixels and take the average of them?", "tokens": [18668, 293, 747, 264, 4274, 295, 552, 30], "temperature": 0.0, "avg_logprob": -0.140851054275245, "compression_ratio": 1.652, "no_speech_prob": 4.710806024377234e-06}, {"id": 1701, "seek": 692704, "start": 6950.24, "end": 6953.92, "text": " Or do you put a little cubic spline through them?", "tokens": [1610, 360, 291, 829, 257, 707, 28733, 4732, 533, 807, 552, 30], "temperature": 0.0, "avg_logprob": -0.140851054275245, "compression_ratio": 1.652, "no_speech_prob": 4.710806024377234e-06}, {"id": 1702, "seek": 692704, "start": 6953.92, "end": 6954.96, "text": " Or what?", "tokens": [1610, 437, 30], "temperature": 0.0, "avg_logprob": -0.140851054275245, "compression_ratio": 1.652, "no_speech_prob": 4.710806024377234e-06}, {"id": 1703, "seek": 695496, "start": 6954.96, "end": 6958.76, "text": " And so these are called resampling methods, and Pillow has a few.", "tokens": [400, 370, 613, 366, 1219, 725, 335, 11970, 7150, 11, 293, 44656, 305, 575, 257, 1326, 13], "temperature": 0.0, "avg_logprob": -0.1014136795170051, "compression_ratio": 1.7170542635658914, "no_speech_prob": 5.682315531885251e-06}, {"id": 1704, "seek": 695496, "start": 6958.76, "end": 6964.24, "text": " They suggest when down sampling, so going from big to small, you should use anti-alias.", "tokens": [814, 3402, 562, 760, 21179, 11, 370, 516, 490, 955, 281, 1359, 11, 291, 820, 764, 6061, 12, 304, 4609, 13], "temperature": 0.0, "avg_logprob": -0.1014136795170051, "compression_ratio": 1.7170542635658914, "no_speech_prob": 5.682315531885251e-06}, {"id": 1705, "seek": 695496, "start": 6964.24, "end": 6966.64, "text": " So here's what you do.", "tokens": [407, 510, 311, 437, 291, 360, 13], "temperature": 0.0, "avg_logprob": -0.1014136795170051, "compression_ratio": 1.7170542635658914, "no_speech_prob": 5.682315531885251e-06}, {"id": 1706, "seek": 695496, "start": 6966.64, "end": 6970.84, "text": " When you're augmenting your data, and this is like nothing I'm going to say today is", "tokens": [1133, 291, 434, 29919, 278, 428, 1412, 11, 293, 341, 307, 411, 1825, 286, 478, 516, 281, 584, 965, 307], "temperature": 0.0, "avg_logprob": -0.1014136795170051, "compression_ratio": 1.7170542635658914, "no_speech_prob": 5.682315531885251e-06}, {"id": 1707, "seek": 695496, "start": 6970.84, "end": 6972.38, "text": " really focused on vision.", "tokens": [534, 5178, 322, 5201, 13], "temperature": 0.0, "avg_logprob": -0.1014136795170051, "compression_ratio": 1.7170542635658914, "no_speech_prob": 5.682315531885251e-06}, {"id": 1708, "seek": 695496, "start": 6972.38, "end": 6978.4, "text": " If you're doing audio, if you're doing text, if you're doing music, whatever, augment your", "tokens": [759, 291, 434, 884, 6278, 11, 498, 291, 434, 884, 2487, 11, 498, 291, 434, 884, 1318, 11, 2035, 11, 29919, 428], "temperature": 0.0, "avg_logprob": -0.1014136795170051, "compression_ratio": 1.7170542635658914, "no_speech_prob": 5.682315531885251e-06}, {"id": 1709, "seek": 695496, "start": 6978.4, "end": 6983.66, "text": " data and look at or listen to or understand your augmented data.", "tokens": [1412, 293, 574, 412, 420, 2140, 281, 420, 1223, 428, 36155, 1412, 13], "temperature": 0.0, "avg_logprob": -0.1014136795170051, "compression_ratio": 1.7170542635658914, "no_speech_prob": 5.682315531885251e-06}, {"id": 1710, "seek": 698366, "start": 6983.66, "end": 6988.72, "text": " So don't just chuck this into a model, but look at what's going on.", "tokens": [407, 500, 380, 445, 20870, 341, 666, 257, 2316, 11, 457, 574, 412, 437, 311, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.10672399239946706, "compression_ratio": 1.7426160337552743, "no_speech_prob": 5.42191583008389e-06}, {"id": 1711, "seek": 698366, "start": 6988.72, "end": 6994.88, "text": " So if I want to know what's going on here, I need to be able to see the texture of this", "tokens": [407, 498, 286, 528, 281, 458, 437, 311, 516, 322, 510, 11, 286, 643, 281, 312, 1075, 281, 536, 264, 8091, 295, 341], "temperature": 0.0, "avg_logprob": -0.10672399239946706, "compression_ratio": 1.7426160337552743, "no_speech_prob": 5.42191583008389e-06}, {"id": 1712, "seek": 698366, "start": 6994.88, "end": 6995.88, "text": " tensch.", "tokens": [256, 26590, 13], "temperature": 0.0, "avg_logprob": -0.10672399239946706, "compression_ratio": 1.7426160337552743, "no_speech_prob": 5.42191583008389e-06}, {"id": 1713, "seek": 698366, "start": 6995.88, "end": 6999.46, "text": " Now I'm not very good at tenches, but I do know a bit about clothes.", "tokens": [823, 286, 478, 406, 588, 665, 412, 2064, 3781, 11, 457, 286, 360, 458, 257, 857, 466, 5534, 13], "temperature": 0.0, "avg_logprob": -0.10672399239946706, "compression_ratio": 1.7426160337552743, "no_speech_prob": 5.42191583008389e-06}, {"id": 1714, "seek": 698366, "start": 6999.46, "end": 7004.12, "text": " So let's say if we were trying to see what this guy's wearing, it's a checkered shirt.", "tokens": [407, 718, 311, 584, 498, 321, 645, 1382, 281, 536, 437, 341, 2146, 311, 4769, 11, 309, 311, 257, 1520, 4073, 8336, 13], "temperature": 0.0, "avg_logprob": -0.10672399239946706, "compression_ratio": 1.7426160337552743, "no_speech_prob": 5.42191583008389e-06}, {"id": 1715, "seek": 698366, "start": 7004.12, "end": 7007.72, "text": " So let's zoom in and see what this guy's wearing.", "tokens": [407, 718, 311, 8863, 294, 293, 536, 437, 341, 2146, 311, 4769, 13], "temperature": 0.0, "avg_logprob": -0.10672399239946706, "compression_ratio": 1.7426160337552743, "no_speech_prob": 5.42191583008389e-06}, {"id": 1716, "seek": 698366, "start": 7007.72, "end": 7008.72, "text": " I have no idea.", "tokens": [286, 362, 572, 1558, 13], "temperature": 0.0, "avg_logprob": -0.10672399239946706, "compression_ratio": 1.7426160337552743, "no_speech_prob": 5.42191583008389e-06}, {"id": 1717, "seek": 698366, "start": 7008.72, "end": 7011.72, "text": " The checkered shirt's gone.", "tokens": [440, 1520, 4073, 8336, 311, 2780, 13], "temperature": 0.0, "avg_logprob": -0.10672399239946706, "compression_ratio": 1.7426160337552743, "no_speech_prob": 5.42191583008389e-06}, {"id": 1718, "seek": 701172, "start": 7011.72, "end": 7019.16, "text": " So I can tell that this is going to totally break my model if we use this kind of image", "tokens": [407, 286, 393, 980, 300, 341, 307, 516, 281, 3879, 1821, 452, 2316, 498, 321, 764, 341, 733, 295, 3256], "temperature": 0.0, "avg_logprob": -0.19410590755129323, "compression_ratio": 1.5911111111111111, "no_speech_prob": 1.994694457607693e-06}, {"id": 1719, "seek": 701172, "start": 7019.16, "end": 7021.400000000001, "text": " augmentation.", "tokens": [14501, 19631, 13], "temperature": 0.0, "avg_logprob": -0.19410590755129323, "compression_ratio": 1.5911111111111111, "no_speech_prob": 1.994694457607693e-06}, {"id": 1720, "seek": 701172, "start": 7021.400000000001, "end": 7022.8, "text": " So let's try a few more.", "tokens": [407, 718, 311, 853, 257, 1326, 544, 13], "temperature": 0.0, "avg_logprob": -0.19410590755129323, "compression_ratio": 1.5911111111111111, "no_speech_prob": 1.994694457607693e-06}, {"id": 1721, "seek": 701172, "start": 7022.8, "end": 7026.52, "text": " What if instead of anti-aliasing, we use bilinear, which is the most common.", "tokens": [708, 498, 2602, 295, 6061, 12, 5103, 3349, 11, 321, 764, 8588, 533, 289, 11, 597, 307, 264, 881, 2689, 13], "temperature": 0.0, "avg_logprob": -0.19410590755129323, "compression_ratio": 1.5911111111111111, "no_speech_prob": 1.994694457607693e-06}, {"id": 1722, "seek": 701172, "start": 7026.52, "end": 7031.92, "text": " No, I still don't know what he's wearing.", "tokens": [883, 11, 286, 920, 500, 380, 458, 437, 415, 311, 4769, 13], "temperature": 0.0, "avg_logprob": -0.19410590755129323, "compression_ratio": 1.5911111111111111, "no_speech_prob": 1.994694457607693e-06}, {"id": 1723, "seek": 701172, "start": 7031.92, "end": 7036.92, "text": " What if we use nearest neighbors, which nobody uses because everybody knows it's terrible.", "tokens": [708, 498, 321, 764, 23831, 12512, 11, 597, 5079, 4960, 570, 2201, 3255, 309, 311, 6237, 13], "temperature": 0.0, "avg_logprob": -0.19410590755129323, "compression_ratio": 1.5911111111111111, "no_speech_prob": 1.994694457607693e-06}, {"id": 1724, "seek": 701172, "start": 7036.92, "end": 7040.6, "text": " Oh, it totally works.", "tokens": [876, 11, 309, 3879, 1985, 13], "temperature": 0.0, "avg_logprob": -0.19410590755129323, "compression_ratio": 1.5911111111111111, "no_speech_prob": 1.994694457607693e-06}, {"id": 1725, "seek": 704060, "start": 7040.6, "end": 7048.92, "text": " So yeah, just look at stuff and try and find something that you can study to see whether", "tokens": [407, 1338, 11, 445, 574, 412, 1507, 293, 853, 293, 915, 746, 300, 291, 393, 2979, 281, 536, 1968], "temperature": 0.0, "avg_logprob": -0.13983919162942907, "compression_ratio": 1.5921052631578947, "no_speech_prob": 8.800075192993972e-06}, {"id": 1726, "seek": 704060, "start": 7048.92, "end": 7049.92, "text": " it works.", "tokens": [309, 1985, 13], "temperature": 0.0, "avg_logprob": -0.13983919162942907, "compression_ratio": 1.5921052631578947, "no_speech_prob": 8.800075192993972e-06}, {"id": 1727, "seek": 704060, "start": 7049.92, "end": 7053.88, "text": " Here's something interesting though.", "tokens": [1692, 311, 746, 1880, 1673, 13], "temperature": 0.0, "avg_logprob": -0.13983919162942907, "compression_ratio": 1.5921052631578947, "no_speech_prob": 8.800075192993972e-06}, {"id": 1728, "seek": 704060, "start": 7053.88, "end": 7056.76, "text": " This looks better still, don't you think?", "tokens": [639, 1542, 1101, 920, 11, 500, 380, 291, 519, 30], "temperature": 0.0, "avg_logprob": -0.13983919162942907, "compression_ratio": 1.5921052631578947, "no_speech_prob": 8.800075192993972e-06}, {"id": 1729, "seek": 704060, "start": 7056.76, "end": 7059.64, "text": " And this is interesting because what I did here was I did two steps.", "tokens": [400, 341, 307, 1880, 570, 437, 286, 630, 510, 390, 286, 630, 732, 4439, 13], "temperature": 0.0, "avg_logprob": -0.13983919162942907, "compression_ratio": 1.5921052631578947, "no_speech_prob": 8.800075192993972e-06}, {"id": 1730, "seek": 704060, "start": 7059.64, "end": 7067.88, "text": " I first of all resized to 256 by 256 with bicubic, and then I resized to my final 128", "tokens": [286, 700, 295, 439, 725, 1602, 281, 38882, 538, 38882, 365, 34472, 836, 299, 11, 293, 550, 286, 725, 1602, 281, 452, 2572, 29810], "temperature": 0.0, "avg_logprob": -0.13983919162942907, "compression_ratio": 1.5921052631578947, "no_speech_prob": 8.800075192993972e-06}, {"id": 1731, "seek": 704060, "start": 7067.88, "end": 7070.120000000001, "text": " by 128 with nearest neighbors.", "tokens": [538, 29810, 365, 23831, 12512, 13], "temperature": 0.0, "avg_logprob": -0.13983919162942907, "compression_ratio": 1.5921052631578947, "no_speech_prob": 8.800075192993972e-06}, {"id": 1732, "seek": 707012, "start": 7070.12, "end": 7075.599999999999, "text": " And so sometimes you can combine things together in steps to get really good results.", "tokens": [400, 370, 2171, 291, 393, 10432, 721, 1214, 294, 4439, 281, 483, 534, 665, 3542, 13], "temperature": 0.0, "avg_logprob": -0.13314222252887228, "compression_ratio": 1.6629213483146068, "no_speech_prob": 2.7693317861121614e-06}, {"id": 1733, "seek": 707012, "start": 7075.599999999999, "end": 7078.48, "text": " Anyway, I didn't want to go into the details here.", "tokens": [5684, 11, 286, 994, 380, 528, 281, 352, 666, 264, 4365, 510, 13], "temperature": 0.0, "avg_logprob": -0.13314222252887228, "compression_ratio": 1.6629213483146068, "no_speech_prob": 2.7693317861121614e-06}, {"id": 1734, "seek": 707012, "start": 7078.48, "end": 7084.8, "text": " I'm just saying that when we talk about image augmentation, your test is to look at or listen", "tokens": [286, 478, 445, 1566, 300, 562, 321, 751, 466, 3256, 14501, 19631, 11, 428, 1500, 307, 281, 574, 412, 420, 2140], "temperature": 0.0, "avg_logprob": -0.13314222252887228, "compression_ratio": 1.6629213483146068, "no_speech_prob": 2.7693317861121614e-06}, {"id": 1735, "seek": 707012, "start": 7084.8, "end": 7088.12, "text": " to or whatever your augmented data.", "tokens": [281, 420, 2035, 428, 36155, 1412, 13], "temperature": 0.0, "avg_logprob": -0.13314222252887228, "compression_ratio": 1.6629213483146068, "no_speech_prob": 2.7693317861121614e-06}, {"id": 1736, "seek": 707012, "start": 7088.12, "end": 7092.64, "text": " So resizing is very important for vision.", "tokens": [407, 725, 3319, 307, 588, 1021, 337, 5201, 13], "temperature": 0.0, "avg_logprob": -0.13314222252887228, "compression_ratio": 1.6629213483146068, "no_speech_prob": 2.7693317861121614e-06}, {"id": 1737, "seek": 707012, "start": 7092.64, "end": 7096.5199999999995, "text": " Flipping is a great data augmentation for vision.", "tokens": [479, 2081, 3759, 307, 257, 869, 1412, 14501, 19631, 337, 5201, 13], "temperature": 0.0, "avg_logprob": -0.13314222252887228, "compression_ratio": 1.6629213483146068, "no_speech_prob": 2.7693317861121614e-06}, {"id": 1738, "seek": 707012, "start": 7096.5199999999995, "end": 7098.0199999999995, "text": " I don't particularly care about flipping.", "tokens": [286, 500, 380, 4098, 1127, 466, 26886, 13], "temperature": 0.0, "avg_logprob": -0.13314222252887228, "compression_ratio": 1.6629213483146068, "no_speech_prob": 2.7693317861121614e-06}, {"id": 1739, "seek": 707012, "start": 7098.0199999999995, "end": 7099.94, "text": " The main thing I want to point out is this.", "tokens": [440, 2135, 551, 286, 528, 281, 935, 484, 307, 341, 13], "temperature": 0.0, "avg_logprob": -0.13314222252887228, "compression_ratio": 1.6629213483146068, "no_speech_prob": 2.7693317861121614e-06}, {"id": 1740, "seek": 709994, "start": 7099.94, "end": 7104.639999999999, "text": " At this point, our tensors contain bytes.", "tokens": [1711, 341, 935, 11, 527, 10688, 830, 5304, 36088, 13], "temperature": 0.0, "avg_logprob": -0.09832528058220358, "compression_ratio": 1.6600790513833992, "no_speech_prob": 1.3006827430217527e-05}, {"id": 1741, "seek": 709994, "start": 7104.639999999999, "end": 7108.44, "text": " Calculating with bytes and moving bytes around is very, very fast.", "tokens": [3511, 2444, 990, 365, 36088, 293, 2684, 36088, 926, 307, 588, 11, 588, 2370, 13], "temperature": 0.0, "avg_logprob": -0.09832528058220358, "compression_ratio": 1.6600790513833992, "no_speech_prob": 1.3006827430217527e-05}, {"id": 1742, "seek": 709994, "start": 7108.44, "end": 7113.12, "text": " And we really care about this because when we were doing the DawnBench competition, one", "tokens": [400, 321, 534, 1127, 466, 341, 570, 562, 321, 645, 884, 264, 26001, 21736, 339, 6211, 11, 472], "temperature": 0.0, "avg_logprob": -0.09832528058220358, "compression_ratio": 1.6600790513833992, "no_speech_prob": 1.3006827430217527e-05}, {"id": 1743, "seek": 709994, "start": 7113.12, "end": 7118.0, "text": " of our biggest issues for speed was getting our data augmentation running fast enough.", "tokens": [295, 527, 3880, 2663, 337, 3073, 390, 1242, 527, 1412, 14501, 19631, 2614, 2370, 1547, 13], "temperature": 0.0, "avg_logprob": -0.09832528058220358, "compression_ratio": 1.6600790513833992, "no_speech_prob": 1.3006827430217527e-05}, {"id": 1744, "seek": 709994, "start": 7118.0, "end": 7121.32, "text": " And doing stuff on floats is slow.", "tokens": [400, 884, 1507, 322, 37878, 307, 2964, 13], "temperature": 0.0, "avg_logprob": -0.09832528058220358, "compression_ratio": 1.6600790513833992, "no_speech_prob": 1.3006827430217527e-05}, {"id": 1745, "seek": 709994, "start": 7121.32, "end": 7126.32, "text": " If you're flipping something, flipping bytes is identical to flipping floats in terms of", "tokens": [759, 291, 434, 26886, 746, 11, 26886, 36088, 307, 14800, 281, 26886, 37878, 294, 2115, 295], "temperature": 0.0, "avg_logprob": -0.09832528058220358, "compression_ratio": 1.6600790513833992, "no_speech_prob": 1.3006827430217527e-05}, {"id": 1746, "seek": 709994, "start": 7126.32, "end": 7127.5599999999995, "text": " the outcome.", "tokens": [264, 9700, 13], "temperature": 0.0, "avg_logprob": -0.09832528058220358, "compression_ratio": 1.6600790513833992, "no_speech_prob": 1.3006827430217527e-05}, {"id": 1747, "seek": 712756, "start": 7127.56, "end": 7131.96, "text": " So you should definitely do your flip while it's still a byte.", "tokens": [407, 291, 820, 2138, 360, 428, 7929, 1339, 309, 311, 920, 257, 40846, 13], "temperature": 0.0, "avg_logprob": -0.12596743876539815, "compression_ratio": 1.8884758364312269, "no_speech_prob": 2.8572567316587083e-06}, {"id": 1748, "seek": 712756, "start": 7131.96, "end": 7138.4800000000005, "text": " So image augmentation isn't just about throwing some transformation functions in there.", "tokens": [407, 3256, 14501, 19631, 1943, 380, 445, 466, 10238, 512, 9887, 6828, 294, 456, 13], "temperature": 0.0, "avg_logprob": -0.12596743876539815, "compression_ratio": 1.8884758364312269, "no_speech_prob": 2.8572567316587083e-06}, {"id": 1749, "seek": 712756, "start": 7138.4800000000005, "end": 7140.200000000001, "text": " But think about when you're going to do it.", "tokens": [583, 519, 466, 562, 291, 434, 516, 281, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.12596743876539815, "compression_ratio": 1.8884758364312269, "no_speech_prob": 2.8572567316587083e-06}, {"id": 1750, "seek": 712756, "start": 7140.200000000001, "end": 7144.88, "text": " Because you've got this pipeline where you start with bytes, and you start with bytes", "tokens": [1436, 291, 600, 658, 341, 15517, 689, 291, 722, 365, 36088, 11, 293, 291, 722, 365, 36088], "temperature": 0.0, "avg_logprob": -0.12596743876539815, "compression_ratio": 1.8884758364312269, "no_speech_prob": 2.8572567316587083e-06}, {"id": 1751, "seek": 712756, "start": 7144.88, "end": 7149.080000000001, "text": " in a pillow thing, and then they become bytes in a tensor, and then they become floats,", "tokens": [294, 257, 18581, 551, 11, 293, 550, 436, 1813, 36088, 294, 257, 40863, 11, 293, 550, 436, 1813, 37878, 11], "temperature": 0.0, "avg_logprob": -0.12596743876539815, "compression_ratio": 1.8884758364312269, "no_speech_prob": 2.8572567316587083e-06}, {"id": 1752, "seek": 712756, "start": 7149.080000000001, "end": 7151.56, "text": " and then they get turned into a batch.", "tokens": [293, 550, 436, 483, 3574, 666, 257, 15245, 13], "temperature": 0.0, "avg_logprob": -0.12596743876539815, "compression_ratio": 1.8884758364312269, "no_speech_prob": 2.8572567316587083e-06}, {"id": 1753, "seek": 712756, "start": 7151.56, "end": 7153.080000000001, "text": " Where are you going to do the work?", "tokens": [2305, 366, 291, 516, 281, 360, 264, 589, 30], "temperature": 0.0, "avg_logprob": -0.12596743876539815, "compression_ratio": 1.8884758364312269, "no_speech_prob": 2.8572567316587083e-06}, {"id": 1754, "seek": 712756, "start": 7153.080000000001, "end": 7156.06, "text": " And so you should do whatever you can while they're still bytes.", "tokens": [400, 370, 291, 820, 360, 2035, 291, 393, 1339, 436, 434, 920, 36088, 13], "temperature": 0.0, "avg_logprob": -0.12596743876539815, "compression_ratio": 1.8884758364312269, "no_speech_prob": 2.8572567316587083e-06}, {"id": 1755, "seek": 715606, "start": 7156.06, "end": 7161.84, "text": " But be careful, don't do things that are going to cause rounding errors or saturation problems,", "tokens": [583, 312, 5026, 11, 500, 380, 360, 721, 300, 366, 516, 281, 3082, 48237, 13603, 420, 27090, 2740, 11], "temperature": 0.0, "avg_logprob": -0.1829649972133949, "compression_ratio": 1.56, "no_speech_prob": 6.747910447302274e-06}, {"id": 1756, "seek": 715606, "start": 7161.84, "end": 7162.84, "text": " whatever.", "tokens": [2035, 13], "temperature": 0.0, "avg_logprob": -0.1829649972133949, "compression_ratio": 1.56, "no_speech_prob": 6.747910447302274e-06}, {"id": 1757, "seek": 715606, "start": 7162.84, "end": 7165.0, "text": " But flips, definitely good.", "tokens": [583, 40249, 11, 2138, 665, 13], "temperature": 0.0, "avg_logprob": -0.1829649972133949, "compression_ratio": 1.56, "no_speech_prob": 6.747910447302274e-06}, {"id": 1758, "seek": 715606, "start": 7165.0, "end": 7167.8, "text": " So let's do our flips.", "tokens": [407, 718, 311, 360, 527, 40249, 13], "temperature": 0.0, "avg_logprob": -0.1829649972133949, "compression_ratio": 1.56, "no_speech_prob": 6.747910447302274e-06}, {"id": 1759, "seek": 715606, "start": 7167.8, "end": 7172.400000000001, "text": " So there's a thing called pio.x.transpose, pio.image.flipLeftRight.", "tokens": [407, 456, 311, 257, 551, 1219, 280, 1004, 13, 87, 13, 24999, 43501, 11, 280, 1004, 13, 26624, 13, 3423, 647, 11020, 844, 16964, 13], "temperature": 0.0, "avg_logprob": -0.1829649972133949, "compression_ratio": 1.56, "no_speech_prob": 6.747910447302274e-06}, {"id": 1760, "seek": 715606, "start": 7172.400000000001, "end": 7175.620000000001, "text": " Let's check if a random number is less than.5.", "tokens": [961, 311, 1520, 498, 257, 4974, 1230, 307, 1570, 813, 2411, 20, 13], "temperature": 0.0, "avg_logprob": -0.1829649972133949, "compression_ratio": 1.56, "no_speech_prob": 6.747910447302274e-06}, {"id": 1761, "seek": 715606, "start": 7175.620000000001, "end": 7177.6, "text": " Let's create an item list.", "tokens": [961, 311, 1884, 364, 3174, 1329, 13], "temperature": 0.0, "avg_logprob": -0.1829649972133949, "compression_ratio": 1.56, "no_speech_prob": 6.747910447302274e-06}, {"id": 1762, "seek": 715606, "start": 7177.6, "end": 7178.6, "text": " And let's replace that.", "tokens": [400, 718, 311, 7406, 300, 13], "temperature": 0.0, "avg_logprob": -0.1829649972133949, "compression_ratio": 1.56, "no_speech_prob": 6.747910447302274e-06}, {"id": 1763, "seek": 715606, "start": 7178.6, "end": 7181.240000000001, "text": " Like, we built this ourselves, so we know how to do this stuff now.", "tokens": [1743, 11, 321, 3094, 341, 4175, 11, 370, 321, 458, 577, 281, 360, 341, 1507, 586, 13], "temperature": 0.0, "avg_logprob": -0.1829649972133949, "compression_ratio": 1.56, "no_speech_prob": 6.747910447302274e-06}, {"id": 1764, "seek": 718124, "start": 7181.24, "end": 7186.92, "text": " Let's replace the items with just the first item with 64 copies of it.", "tokens": [961, 311, 7406, 264, 4754, 365, 445, 264, 700, 3174, 365, 12145, 14341, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.11987369984119862, "compression_ratio": 1.697211155378486, "no_speech_prob": 4.2892615965683945e-06}, {"id": 1765, "seek": 718124, "start": 7186.92, "end": 7193.639999999999, "text": " And so that way, we can now use this to create the same picture lots of times.", "tokens": [400, 370, 300, 636, 11, 321, 393, 586, 764, 341, 281, 1884, 264, 912, 3036, 3195, 295, 1413, 13], "temperature": 0.0, "avg_logprob": -0.11987369984119862, "compression_ratio": 1.697211155378486, "no_speech_prob": 4.2892615965683945e-06}, {"id": 1766, "seek": 718124, "start": 7193.639999999999, "end": 7197.08, "text": " So show batch is just something that's just going to go through our batch and show all", "tokens": [407, 855, 15245, 307, 445, 746, 300, 311, 445, 516, 281, 352, 807, 527, 15245, 293, 855, 439], "temperature": 0.0, "avg_logprob": -0.11987369984119862, "compression_ratio": 1.697211155378486, "no_speech_prob": 4.2892615965683945e-06}, {"id": 1767, "seek": 718124, "start": 7197.08, "end": 7199.32, "text": " the images.", "tokens": [264, 5267, 13], "temperature": 0.0, "avg_logprob": -0.11987369984119862, "compression_ratio": 1.697211155378486, "no_speech_prob": 4.2892615965683945e-06}, {"id": 1768, "seek": 718124, "start": 7199.32, "end": 7201.26, "text": " Everything we're using, we've built ourselves.", "tokens": [5471, 321, 434, 1228, 11, 321, 600, 3094, 4175, 13], "temperature": 0.0, "avg_logprob": -0.11987369984119862, "compression_ratio": 1.697211155378486, "no_speech_prob": 4.2892615965683945e-06}, {"id": 1769, "seek": 718124, "start": 7201.26, "end": 7204.0, "text": " So you never have to wonder what's going on.", "tokens": [407, 291, 1128, 362, 281, 2441, 437, 311, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.11987369984119862, "compression_ratio": 1.697211155378486, "no_speech_prob": 4.2892615965683945e-06}, {"id": 1770, "seek": 718124, "start": 7204.0, "end": 7209.24, "text": " So we can show batch with no augmentation, or remember how we created our transforms?", "tokens": [407, 321, 393, 855, 15245, 365, 572, 14501, 19631, 11, 420, 1604, 577, 321, 2942, 527, 35592, 30], "temperature": 0.0, "avg_logprob": -0.11987369984119862, "compression_ratio": 1.697211155378486, "no_speech_prob": 4.2892615965683945e-06}, {"id": 1771, "seek": 720924, "start": 7209.24, "end": 7215.24, "text": " We can add pio.randomFlip, and now some of them are backwards.", "tokens": [492, 393, 909, 280, 1004, 13, 3699, 298, 25680, 647, 11, 293, 586, 512, 295, 552, 366, 12204, 13], "temperature": 0.0, "avg_logprob": -0.11592872440814972, "compression_ratio": 1.6462093862815885, "no_speech_prob": 3.7852169043617323e-06}, {"id": 1772, "seek": 720924, "start": 7215.24, "end": 7219.5199999999995, "text": " It might be nice to turn this into a class that you actually pass a p into to decide", "tokens": [467, 1062, 312, 1481, 281, 1261, 341, 666, 257, 1508, 300, 291, 767, 1320, 257, 280, 666, 281, 4536], "temperature": 0.0, "avg_logprob": -0.11592872440814972, "compression_ratio": 1.6462093862815885, "no_speech_prob": 3.7852169043617323e-06}, {"id": 1773, "seek": 720924, "start": 7219.5199999999995, "end": 7223.32, "text": " what the probability of a flip is.", "tokens": [437, 264, 8482, 295, 257, 7929, 307, 13], "temperature": 0.0, "avg_logprob": -0.11592872440814972, "compression_ratio": 1.6462093862815885, "no_speech_prob": 3.7852169043617323e-06}, {"id": 1774, "seek": 720924, "start": 7223.32, "end": 7227.719999999999, "text": " You probably want to give it an order, because we need to make sure it happens after we've", "tokens": [509, 1391, 528, 281, 976, 309, 364, 1668, 11, 570, 321, 643, 281, 652, 988, 309, 2314, 934, 321, 600], "temperature": 0.0, "avg_logprob": -0.11592872440814972, "compression_ratio": 1.6462093862815885, "no_speech_prob": 3.7852169043617323e-06}, {"id": 1775, "seek": 720924, "start": 7227.719999999999, "end": 7232.4, "text": " got the image and after we've converted it to RGB, but before we've turned it into a", "tokens": [658, 264, 3256, 293, 934, 321, 600, 16424, 309, 281, 31231, 11, 457, 949, 321, 600, 3574, 309, 666, 257], "temperature": 0.0, "avg_logprob": -0.11592872440814972, "compression_ratio": 1.6462093862815885, "no_speech_prob": 3.7852169043617323e-06}, {"id": 1776, "seek": 720924, "start": 7232.4, "end": 7233.4, "text": " tensor.", "tokens": [40863, 13], "temperature": 0.0, "avg_logprob": -0.11592872440814972, "compression_ratio": 1.6462093862815885, "no_speech_prob": 3.7852169043617323e-06}, {"id": 1777, "seek": 720924, "start": 7233.4, "end": 7237.92, "text": " Since all of our pio.transforms are going to want to be that order, we may as well create", "tokens": [4162, 439, 295, 527, 280, 1004, 13, 24999, 837, 82, 366, 516, 281, 528, 281, 312, 300, 1668, 11, 321, 815, 382, 731, 1884], "temperature": 0.0, "avg_logprob": -0.11592872440814972, "compression_ratio": 1.6462093862815885, "no_speech_prob": 3.7852169043617323e-06}, {"id": 1778, "seek": 723792, "start": 7237.92, "end": 7241.04, "text": " a pio.transform class and give it that order.", "tokens": [257, 280, 1004, 13, 24999, 837, 1508, 293, 976, 309, 300, 1668, 13], "temperature": 0.0, "avg_logprob": -0.12839120893336053, "compression_ratio": 1.7330316742081449, "no_speech_prob": 2.1111669411766343e-05}, {"id": 1779, "seek": 723792, "start": 7241.04, "end": 7245.6, "text": " And then we can just inherit from that class every time we want a pio.transform.", "tokens": [400, 550, 321, 393, 445, 21389, 490, 300, 1508, 633, 565, 321, 528, 257, 280, 1004, 13, 24999, 837, 13], "temperature": 0.0, "avg_logprob": -0.12839120893336053, "compression_ratio": 1.7330316742081449, "no_speech_prob": 2.1111669411766343e-05}, {"id": 1780, "seek": 723792, "start": 7245.6, "end": 7247.4, "text": " So now we've got a pio.transform class.", "tokens": [407, 586, 321, 600, 658, 257, 280, 1004, 13, 24999, 837, 1508, 13], "temperature": 0.0, "avg_logprob": -0.12839120893336053, "compression_ratio": 1.7330316742081449, "no_speech_prob": 2.1111669411766343e-05}, {"id": 1781, "seek": 723792, "start": 7247.4, "end": 7249.24, "text": " We've got a pio.randomFlip.", "tokens": [492, 600, 658, 257, 280, 1004, 13, 3699, 298, 25680, 647, 13], "temperature": 0.0, "avg_logprob": -0.12839120893336053, "compression_ratio": 1.7330316742081449, "no_speech_prob": 2.1111669411766343e-05}, {"id": 1782, "seek": 723792, "start": 7249.24, "end": 7250.9800000000005, "text": " It's got this state.", "tokens": [467, 311, 658, 341, 1785, 13], "temperature": 0.0, "avg_logprob": -0.12839120893336053, "compression_ratio": 1.7330316742081449, "no_speech_prob": 2.1111669411766343e-05}, {"id": 1783, "seek": 723792, "start": 7250.9800000000005, "end": 7253.12, "text": " It's going to be random.", "tokens": [467, 311, 516, 281, 312, 4974, 13], "temperature": 0.0, "avg_logprob": -0.12839120893336053, "compression_ratio": 1.7330316742081449, "no_speech_prob": 2.1111669411766343e-05}, {"id": 1784, "seek": 723792, "start": 7253.12, "end": 7256.4, "text": " We can try it out, giving it p of.8.", "tokens": [492, 393, 853, 309, 484, 11, 2902, 309, 280, 295, 2411, 23, 13], "temperature": 0.0, "avg_logprob": -0.12839120893336053, "compression_ratio": 1.7330316742081449, "no_speech_prob": 2.1111669411766343e-05}, {"id": 1785, "seek": 723792, "start": 7256.4, "end": 7261.36, "text": " And so now, yep, most of them are flipped.", "tokens": [400, 370, 586, 11, 18633, 11, 881, 295, 552, 366, 26273, 13], "temperature": 0.0, "avg_logprob": -0.12839120893336053, "compression_ratio": 1.7330316742081449, "no_speech_prob": 2.1111669411766343e-05}, {"id": 1786, "seek": 723792, "start": 7261.36, "end": 7262.7, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.12839120893336053, "compression_ratio": 1.7330316742081449, "no_speech_prob": 2.1111669411766343e-05}, {"id": 1787, "seek": 723792, "start": 7262.7, "end": 7267.22, "text": " Or maybe we want to be able to do all these other flips.", "tokens": [1610, 1310, 321, 528, 281, 312, 1075, 281, 360, 439, 613, 661, 40249, 13], "temperature": 0.0, "avg_logprob": -0.12839120893336053, "compression_ratio": 1.7330316742081449, "no_speech_prob": 2.1111669411766343e-05}, {"id": 1788, "seek": 726722, "start": 7267.22, "end": 7272.64, "text": " So actually, pio.transpose, you can pass it all kinds of different things, and they're", "tokens": [407, 767, 11, 280, 1004, 13, 24999, 43501, 11, 291, 393, 1320, 309, 439, 3685, 295, 819, 721, 11, 293, 436, 434], "temperature": 0.0, "avg_logprob": -0.176984920501709, "compression_ratio": 1.5272727272727273, "no_speech_prob": 3.6687897591036744e-06}, {"id": 1789, "seek": 726722, "start": 7272.64, "end": 7277.08, "text": " basically just numbers between 0 and 6.", "tokens": [1936, 445, 3547, 1296, 1958, 293, 1386, 13], "temperature": 0.0, "avg_logprob": -0.176984920501709, "compression_ratio": 1.5272727272727273, "no_speech_prob": 3.6687897591036744e-06}, {"id": 1790, "seek": 726722, "start": 7277.08, "end": 7281.26, "text": " So here are all the options.", "tokens": [407, 510, 366, 439, 264, 3956, 13], "temperature": 0.0, "avg_logprob": -0.176984920501709, "compression_ratio": 1.5272727272727273, "no_speech_prob": 3.6687897591036744e-06}, {"id": 1791, "seek": 726722, "start": 7281.26, "end": 7289.92, "text": " So let's turn that into another transform, where we just pick any one of those at random.", "tokens": [407, 718, 311, 1261, 300, 666, 1071, 4088, 11, 689, 321, 445, 1888, 604, 472, 295, 729, 412, 4974, 13], "temperature": 0.0, "avg_logprob": -0.176984920501709, "compression_ratio": 1.5272727272727273, "no_speech_prob": 3.6687897591036744e-06}, {"id": 1792, "seek": 726722, "start": 7289.92, "end": 7291.42, "text": " And there it is.", "tokens": [400, 456, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.176984920501709, "compression_ratio": 1.5272727272727273, "no_speech_prob": 3.6687897591036744e-06}, {"id": 1793, "seek": 726722, "start": 7291.42, "end": 7293.320000000001, "text": " So this is how we can do data augmentation.", "tokens": [407, 341, 307, 577, 321, 393, 360, 1412, 14501, 19631, 13], "temperature": 0.0, "avg_logprob": -0.176984920501709, "compression_ratio": 1.5272727272727273, "no_speech_prob": 3.6687897591036744e-06}, {"id": 1794, "seek": 726722, "start": 7293.320000000001, "end": 7294.320000000001, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.176984920501709, "compression_ratio": 1.5272727272727273, "no_speech_prob": 3.6687897591036744e-06}, {"id": 1795, "seek": 726722, "start": 7294.320000000001, "end": 7296.320000000001, "text": " Now's a good time.", "tokens": [823, 311, 257, 665, 565, 13], "temperature": 0.0, "avg_logprob": -0.176984920501709, "compression_ratio": 1.5272727272727273, "no_speech_prob": 3.6687897591036744e-06}, {"id": 1796, "seek": 729632, "start": 7296.32, "end": 7301.679999999999, "text": " It's easy to evaluate data augmentation for images.", "tokens": [467, 311, 1858, 281, 13059, 1412, 14501, 19631, 337, 5267, 13], "temperature": 0.0, "avg_logprob": -0.23739785414475661, "compression_ratio": 1.82, "no_speech_prob": 9.368326573166996e-06}, {"id": 1797, "seek": 729632, "start": 7301.679999999999, "end": 7309.12, "text": " How would you handle tabular text or time series?", "tokens": [1012, 576, 291, 4813, 4421, 1040, 2487, 420, 565, 2638, 30], "temperature": 0.0, "avg_logprob": -0.23739785414475661, "compression_ratio": 1.82, "no_speech_prob": 9.368326573166996e-06}, {"id": 1798, "seek": 729632, "start": 7309.12, "end": 7312.5599999999995, "text": " For text, you read it.", "tokens": [1171, 2487, 11, 291, 1401, 309, 13], "temperature": 0.0, "avg_logprob": -0.23739785414475661, "compression_ratio": 1.82, "no_speech_prob": 9.368326573166996e-06}, {"id": 1799, "seek": 729632, "start": 7312.5599999999995, "end": 7315.44, "text": " How would you handle the data augmentation?", "tokens": [1012, 576, 291, 4813, 264, 1412, 14501, 19631, 30], "temperature": 0.0, "avg_logprob": -0.23739785414475661, "compression_ratio": 1.82, "no_speech_prob": 9.368326573166996e-06}, {"id": 1800, "seek": 729632, "start": 7315.44, "end": 7316.44, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.23739785414475661, "compression_ratio": 1.82, "no_speech_prob": 9.368326573166996e-06}, {"id": 1801, "seek": 729632, "start": 7316.44, "end": 7318.599999999999, "text": " You would read the augmented text.", "tokens": [509, 576, 1401, 264, 36155, 2487, 13], "temperature": 0.0, "avg_logprob": -0.23739785414475661, "compression_ratio": 1.82, "no_speech_prob": 9.368326573166996e-06}, {"id": 1802, "seek": 729632, "start": 7318.599999999999, "end": 7323.5199999999995, "text": " So if you're augmenting text, then you read the augmented text.", "tokens": [407, 498, 291, 434, 29919, 278, 2487, 11, 550, 291, 1401, 264, 36155, 2487, 13], "temperature": 0.0, "avg_logprob": -0.23739785414475661, "compression_ratio": 1.82, "no_speech_prob": 9.368326573166996e-06}, {"id": 1803, "seek": 732352, "start": 7323.52, "end": 7329.4400000000005, "text": " For time series, you would look at the signal of the time series.", "tokens": [1171, 565, 2638, 11, 291, 576, 574, 412, 264, 6358, 295, 264, 565, 2638, 13], "temperature": 0.0, "avg_logprob": -0.17721020973334878, "compression_ratio": 1.9700854700854702, "no_speech_prob": 1.9830360542982817e-05}, {"id": 1804, "seek": 732352, "start": 7329.4400000000005, "end": 7335.72, "text": " For tabular, you would graph or however you normally visualize that kind of tabular data,", "tokens": [1171, 4421, 1040, 11, 291, 576, 4295, 420, 4461, 291, 5646, 23273, 300, 733, 295, 4421, 1040, 1412, 11], "temperature": 0.0, "avg_logprob": -0.17721020973334878, "compression_ratio": 1.9700854700854702, "no_speech_prob": 1.9830360542982817e-05}, {"id": 1805, "seek": 732352, "start": 7335.72, "end": 7338.360000000001, "text": " you would visualize that tabular data in the same way.", "tokens": [291, 576, 23273, 300, 4421, 1040, 1412, 294, 264, 912, 636, 13], "temperature": 0.0, "avg_logprob": -0.17721020973334878, "compression_ratio": 1.9700854700854702, "no_speech_prob": 1.9830360542982817e-05}, {"id": 1806, "seek": 732352, "start": 7338.360000000001, "end": 7343.040000000001, "text": " So you just kind of come and try and, as a domain expert, hopefully you understand your", "tokens": [407, 291, 445, 733, 295, 808, 293, 853, 293, 11, 382, 257, 9274, 5844, 11, 4696, 291, 1223, 428], "temperature": 0.0, "avg_logprob": -0.17721020973334878, "compression_ratio": 1.9700854700854702, "no_speech_prob": 1.9830360542982817e-05}, {"id": 1807, "seek": 732352, "start": 7343.040000000001, "end": 7348.68, "text": " data and you have to come up with a way, you know, what are the ways you normally visualize", "tokens": [1412, 293, 291, 362, 281, 808, 493, 365, 257, 636, 11, 291, 458, 11, 437, 366, 264, 2098, 291, 5646, 23273], "temperature": 0.0, "avg_logprob": -0.17721020973334878, "compression_ratio": 1.9700854700854702, "no_speech_prob": 1.9830360542982817e-05}, {"id": 1808, "seek": 732352, "start": 7348.68, "end": 7349.68, "text": " that kind of data?", "tokens": [300, 733, 295, 1412, 30], "temperature": 0.0, "avg_logprob": -0.17721020973334878, "compression_ratio": 1.9700854700854702, "no_speech_prob": 1.9830360542982817e-05}, {"id": 1809, "seek": 732352, "start": 7349.68, "end": 7351.96, "text": " And you use the same thing for your augmented data.", "tokens": [400, 291, 764, 264, 912, 551, 337, 428, 36155, 1412, 13], "temperature": 0.0, "avg_logprob": -0.17721020973334878, "compression_ratio": 1.9700854700854702, "no_speech_prob": 1.9830360542982817e-05}, {"id": 1810, "seek": 735196, "start": 7351.96, "end": 7353.6, "text": " Make sure it makes sense.", "tokens": [4387, 988, 309, 1669, 2020, 13], "temperature": 0.0, "avg_logprob": -0.18108589172363282, "compression_ratio": 1.574766355140187, "no_speech_prob": 4.936237019137479e-06}, {"id": 1811, "seek": 735196, "start": 7353.6, "end": 7354.6, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.18108589172363282, "compression_ratio": 1.574766355140187, "no_speech_prob": 4.936237019137479e-06}, {"id": 1812, "seek": 735196, "start": 7354.6, "end": 7355.6, "text": " Make sure it seems reasonable.", "tokens": [4387, 988, 309, 2544, 10585, 13], "temperature": 0.0, "avg_logprob": -0.18108589172363282, "compression_ratio": 1.574766355140187, "no_speech_prob": 4.936237019137479e-06}, {"id": 1813, "seek": 735196, "start": 7355.6, "end": 7356.6, "text": " Sorry.", "tokens": [4919, 13], "temperature": 0.0, "avg_logprob": -0.18108589172363282, "compression_ratio": 1.574766355140187, "no_speech_prob": 4.936237019137479e-06}, {"id": 1814, "seek": 735196, "start": 7356.6, "end": 7360.16, "text": " I think I misread.", "tokens": [286, 519, 286, 3346, 2538, 13], "temperature": 0.0, "avg_logprob": -0.18108589172363282, "compression_ratio": 1.574766355140187, "no_speech_prob": 4.936237019137479e-06}, {"id": 1815, "seek": 735196, "start": 7360.16, "end": 7364.0, "text": " How would you do the augmentation for tabular data, text or time series?", "tokens": [1012, 576, 291, 360, 264, 14501, 19631, 337, 4421, 1040, 1412, 11, 2487, 420, 565, 2638, 30], "temperature": 0.0, "avg_logprob": -0.18108589172363282, "compression_ratio": 1.574766355140187, "no_speech_prob": 4.936237019137479e-06}, {"id": 1816, "seek": 735196, "start": 7364.0, "end": 7367.8, "text": " How would you do the augmentation?", "tokens": [1012, 576, 291, 360, 264, 14501, 19631, 30], "temperature": 0.0, "avg_logprob": -0.18108589172363282, "compression_ratio": 1.574766355140187, "no_speech_prob": 4.936237019137479e-06}, {"id": 1817, "seek": 735196, "start": 7367.8, "end": 7372.12, "text": " I mean, again, it kind of requires your domain expertise.", "tokens": [286, 914, 11, 797, 11, 309, 733, 295, 7029, 428, 9274, 11769, 13], "temperature": 0.0, "avg_logprob": -0.18108589172363282, "compression_ratio": 1.574766355140187, "no_speech_prob": 4.936237019137479e-06}, {"id": 1818, "seek": 735196, "start": 7372.12, "end": 7379.2, "text": " So just before class today, actually, one of our alumni, Christine Payne, came in.", "tokens": [407, 445, 949, 1508, 965, 11, 767, 11, 472, 295, 527, 16347, 11, 24038, 11431, 716, 11, 1361, 294, 13], "temperature": 0.0, "avg_logprob": -0.18108589172363282, "compression_ratio": 1.574766355140187, "no_speech_prob": 4.936237019137479e-06}, {"id": 1819, "seek": 737920, "start": 7379.2, "end": 7384.48, "text": " She's at OpenAI now working on music analysis and music generation.", "tokens": [1240, 311, 412, 7238, 48698, 586, 1364, 322, 1318, 5215, 293, 1318, 5125, 13], "temperature": 0.0, "avg_logprob": -0.1487174087695861, "compression_ratio": 1.6181818181818182, "no_speech_prob": 1.1473367521830369e-05}, {"id": 1820, "seek": 737920, "start": 7384.48, "end": 7389.84, "text": " And she was talking about her data augmentation, saying she's like pitch shifting and volume", "tokens": [400, 750, 390, 1417, 466, 720, 1412, 14501, 19631, 11, 1566, 750, 311, 411, 7293, 17573, 293, 5523], "temperature": 0.0, "avg_logprob": -0.1487174087695861, "compression_ratio": 1.6181818181818182, "no_speech_prob": 1.1473367521830369e-05}, {"id": 1821, "seek": 737920, "start": 7389.84, "end": 7394.24, "text": " changing and slicing bits off the front and the end and stuff like that.", "tokens": [4473, 293, 46586, 9239, 766, 264, 1868, 293, 264, 917, 293, 1507, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.1487174087695861, "compression_ratio": 1.6181818181818182, "no_speech_prob": 1.1473367521830369e-05}, {"id": 1822, "seek": 737920, "start": 7394.24, "end": 7398.599999999999, "text": " So there isn't an answer, you know.", "tokens": [407, 456, 1943, 380, 364, 1867, 11, 291, 458, 13], "temperature": 0.0, "avg_logprob": -0.1487174087695861, "compression_ratio": 1.6181818181818182, "no_speech_prob": 1.1473367521830369e-05}, {"id": 1823, "seek": 737920, "start": 7398.599999999999, "end": 7402.72, "text": " It's just a case of thinking about like, oh, what kinds of things could change in your", "tokens": [467, 311, 445, 257, 1389, 295, 1953, 466, 411, 11, 1954, 11, 437, 3685, 295, 721, 727, 1319, 294, 428], "temperature": 0.0, "avg_logprob": -0.1487174087695861, "compression_ratio": 1.6181818181818182, "no_speech_prob": 1.1473367521830369e-05}, {"id": 1824, "seek": 740272, "start": 7402.72, "end": 7410.64, "text": " data that would almost certainly cause the label to not change but would still be a reasonable", "tokens": [1412, 300, 576, 1920, 3297, 3082, 264, 7645, 281, 406, 1319, 457, 576, 920, 312, 257, 10585], "temperature": 0.0, "avg_logprob": -0.14924642672905555, "compression_ratio": 1.5690376569037656, "no_speech_prob": 8.529298611392733e-06}, {"id": 1825, "seek": 740272, "start": 7410.64, "end": 7411.68, "text": " data item?", "tokens": [1412, 3174, 30], "temperature": 0.0, "avg_logprob": -0.14924642672905555, "compression_ratio": 1.5690376569037656, "no_speech_prob": 8.529298611392733e-06}, {"id": 1826, "seek": 740272, "start": 7411.68, "end": 7413.240000000001, "text": " And that just requires your domain expertise.", "tokens": [400, 300, 445, 7029, 428, 9274, 11769, 13], "temperature": 0.0, "avg_logprob": -0.14924642672905555, "compression_ratio": 1.5690376569037656, "no_speech_prob": 8.529298611392733e-06}, {"id": 1827, "seek": 740272, "start": 7413.240000000001, "end": 7416.360000000001, "text": " Oh, except for the thing I'm going to show you next, which is going to be a magic trick", "tokens": [876, 11, 3993, 337, 264, 551, 286, 478, 516, 281, 855, 291, 958, 11, 597, 307, 516, 281, 312, 257, 5585, 4282], "temperature": 0.0, "avg_logprob": -0.14924642672905555, "compression_ratio": 1.5690376569037656, "no_speech_prob": 8.529298611392733e-06}, {"id": 1828, "seek": 740272, "start": 7416.360000000001, "end": 7417.6, "text": " that works for everything.", "tokens": [300, 1985, 337, 1203, 13], "temperature": 0.0, "avg_logprob": -0.14924642672905555, "compression_ratio": 1.5690376569037656, "no_speech_prob": 8.529298611392733e-06}, {"id": 1829, "seek": 740272, "start": 7417.6, "end": 7421.400000000001, "text": " So we'll come to that.", "tokens": [407, 321, 603, 808, 281, 300, 13], "temperature": 0.0, "avg_logprob": -0.14924642672905555, "compression_ratio": 1.5690376569037656, "no_speech_prob": 8.529298611392733e-06}, {"id": 1830, "seek": 740272, "start": 7421.400000000001, "end": 7423.64, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.14924642672905555, "compression_ratio": 1.5690376569037656, "no_speech_prob": 8.529298611392733e-06}, {"id": 1831, "seek": 740272, "start": 7423.64, "end": 7426.280000000001, "text": " We can do random cropping.", "tokens": [492, 393, 360, 4974, 4848, 3759, 13], "temperature": 0.0, "avg_logprob": -0.14924642672905555, "compression_ratio": 1.5690376569037656, "no_speech_prob": 8.529298611392733e-06}, {"id": 1832, "seek": 740272, "start": 7426.280000000001, "end": 7430.280000000001, "text": " And this is, again, something to be very careful of.", "tokens": [400, 341, 307, 11, 797, 11, 746, 281, 312, 588, 5026, 295, 13], "temperature": 0.0, "avg_logprob": -0.14924642672905555, "compression_ratio": 1.5690376569037656, "no_speech_prob": 8.529298611392733e-06}, {"id": 1833, "seek": 743028, "start": 7430.28, "end": 7435.48, "text": " We very often want to grab a small piece of an image and zoom into that piece.", "tokens": [492, 588, 2049, 528, 281, 4444, 257, 1359, 2522, 295, 364, 3256, 293, 8863, 666, 300, 2522, 13], "temperature": 0.0, "avg_logprob": -0.08373449362960517, "compression_ratio": 1.5296610169491525, "no_speech_prob": 6.438456694013439e-06}, {"id": 1834, "seek": 743028, "start": 7435.48, "end": 7437.599999999999, "text": " It's a great way to do data augmentation.", "tokens": [467, 311, 257, 869, 636, 281, 360, 1412, 14501, 19631, 13], "temperature": 0.0, "avg_logprob": -0.08373449362960517, "compression_ratio": 1.5296610169491525, "no_speech_prob": 6.438456694013439e-06}, {"id": 1835, "seek": 743028, "start": 7437.599999999999, "end": 7441.719999999999, "text": " One way would be to crop and then resize.", "tokens": [1485, 636, 576, 312, 281, 9086, 293, 550, 50069, 13], "temperature": 0.0, "avg_logprob": -0.08373449362960517, "compression_ratio": 1.5296610169491525, "no_speech_prob": 6.438456694013439e-06}, {"id": 1836, "seek": 743028, "start": 7441.719999999999, "end": 7447.08, "text": " And if we do crop and resize, oh, we've lost his checked shirt.", "tokens": [400, 498, 321, 360, 9086, 293, 50069, 11, 1954, 11, 321, 600, 2731, 702, 10033, 8336, 13], "temperature": 0.0, "avg_logprob": -0.08373449362960517, "compression_ratio": 1.5296610169491525, "no_speech_prob": 6.438456694013439e-06}, {"id": 1837, "seek": 743028, "start": 7447.08, "end": 7449.5599999999995, "text": " But very often you can do both in one step.", "tokens": [583, 588, 2049, 291, 393, 360, 1293, 294, 472, 1823, 13], "temperature": 0.0, "avg_logprob": -0.08373449362960517, "compression_ratio": 1.5296610169491525, "no_speech_prob": 6.438456694013439e-06}, {"id": 1838, "seek": 743028, "start": 7449.5599999999995, "end": 7457.44, "text": " So for example, with Pillow, there's a transform called Extent where you tell it what crop", "tokens": [407, 337, 1365, 11, 365, 44656, 305, 11, 456, 311, 257, 4088, 1219, 9881, 317, 689, 291, 980, 309, 437, 9086], "temperature": 0.0, "avg_logprob": -0.08373449362960517, "compression_ratio": 1.5296610169491525, "no_speech_prob": 6.438456694013439e-06}, {"id": 1839, "seek": 745744, "start": 7457.44, "end": 7461.36, "text": " and what resize, and it does it in one step.", "tokens": [293, 437, 50069, 11, 293, 309, 775, 309, 294, 472, 1823, 13], "temperature": 0.0, "avg_logprob": -0.12406561451573525, "compression_ratio": 1.6302521008403361, "no_speech_prob": 8.39786389406072e-06}, {"id": 1840, "seek": 745744, "start": 7461.36, "end": 7464.2, "text": " And now it's much more clear.", "tokens": [400, 586, 309, 311, 709, 544, 1850, 13], "temperature": 0.0, "avg_logprob": -0.12406561451573525, "compression_ratio": 1.6302521008403361, "no_speech_prob": 8.39786389406072e-06}, {"id": 1841, "seek": 745744, "start": 7464.2, "end": 7468.719999999999, "text": " So generally speaking, you've got to be super careful, particularly when your data is still", "tokens": [407, 5101, 4124, 11, 291, 600, 658, 281, 312, 1687, 5026, 11, 4098, 562, 428, 1412, 307, 920], "temperature": 0.0, "avg_logprob": -0.12406561451573525, "compression_ratio": 1.6302521008403361, "no_speech_prob": 8.39786389406072e-06}, {"id": 1842, "seek": 745744, "start": 7468.719999999999, "end": 7476.0, "text": " bytes, not to do destructive transformations, particularly multiple destructive transformations.", "tokens": [36088, 11, 406, 281, 360, 26960, 34852, 11, 4098, 3866, 26960, 34852, 13], "temperature": 0.0, "avg_logprob": -0.12406561451573525, "compression_ratio": 1.6302521008403361, "no_speech_prob": 8.39786389406072e-06}, {"id": 1843, "seek": 745744, "start": 7476.0, "end": 7481.36, "text": " Do them all in one go or wait until they're floats.", "tokens": [1144, 552, 439, 294, 472, 352, 420, 1699, 1826, 436, 434, 37878, 13], "temperature": 0.0, "avg_logprob": -0.12406561451573525, "compression_ratio": 1.6302521008403361, "no_speech_prob": 8.39786389406072e-06}, {"id": 1844, "seek": 745744, "start": 7481.36, "end": 7487.139999999999, "text": " Because bytes round off and disappear or saturate, whereas floats don't.", "tokens": [1436, 36088, 3098, 766, 293, 11596, 420, 21160, 473, 11, 9735, 37878, 500, 380, 13], "temperature": 0.0, "avg_logprob": -0.12406561451573525, "compression_ratio": 1.6302521008403361, "no_speech_prob": 8.39786389406072e-06}, {"id": 1845, "seek": 748714, "start": 7487.14, "end": 7494.92, "text": " And the cropping one takes 193 microseconds.", "tokens": [400, 264, 4848, 3759, 472, 2516, 1294, 18, 3123, 37841, 28750, 13], "temperature": 0.0, "avg_logprob": -0.12326028509047425, "compression_ratio": 1.5779816513761469, "no_speech_prob": 5.771666110376827e-06}, {"id": 1846, "seek": 748714, "start": 7494.92, "end": 7497.38, "text": " The better one takes 500 microseconds.", "tokens": [440, 1101, 472, 2516, 5923, 3123, 37841, 28750, 13], "temperature": 0.0, "avg_logprob": -0.12326028509047425, "compression_ratio": 1.5779816513761469, "no_speech_prob": 5.771666110376827e-06}, {"id": 1847, "seek": 748714, "start": 7497.38, "end": 7501.12, "text": " So one approach would be to say, oh, crap, it's more than twice as long.", "tokens": [407, 472, 3109, 576, 312, 281, 584, 11, 1954, 11, 12426, 11, 309, 311, 544, 813, 6091, 382, 938, 13], "temperature": 0.0, "avg_logprob": -0.12326028509047425, "compression_ratio": 1.5779816513761469, "no_speech_prob": 5.771666110376827e-06}, {"id": 1848, "seek": 748714, "start": 7501.12, "end": 7502.12, "text": " We're screwed.", "tokens": [492, 434, 20331, 13], "temperature": 0.0, "avg_logprob": -0.12326028509047425, "compression_ratio": 1.5779816513761469, "no_speech_prob": 5.771666110376827e-06}, {"id": 1849, "seek": 748714, "start": 7502.12, "end": 7504.240000000001, "text": " But that's not how to think.", "tokens": [583, 300, 311, 406, 577, 281, 519, 13], "temperature": 0.0, "avg_logprob": -0.12326028509047425, "compression_ratio": 1.5779816513761469, "no_speech_prob": 5.771666110376827e-06}, {"id": 1850, "seek": 748714, "start": 7504.240000000001, "end": 7507.02, "text": " How to think is, what's your time budget?", "tokens": [1012, 281, 519, 307, 11, 437, 311, 428, 565, 4706, 30], "temperature": 0.0, "avg_logprob": -0.12326028509047425, "compression_ratio": 1.5779816513761469, "no_speech_prob": 5.771666110376827e-06}, {"id": 1851, "seek": 748714, "start": 7507.02, "end": 7508.400000000001, "text": " Does it matter?", "tokens": [4402, 309, 1871, 30], "temperature": 0.0, "avg_logprob": -0.12326028509047425, "compression_ratio": 1.5779816513761469, "no_speech_prob": 5.771666110376827e-06}, {"id": 1852, "seek": 748714, "start": 7508.400000000001, "end": 7512.64, "text": " So here's how I thought through our time budget for this little augmentation project.", "tokens": [407, 510, 311, 577, 286, 1194, 807, 527, 565, 4706, 337, 341, 707, 14501, 19631, 1716, 13], "temperature": 0.0, "avg_logprob": -0.12326028509047425, "compression_ratio": 1.5779816513761469, "no_speech_prob": 5.771666110376827e-06}, {"id": 1853, "seek": 751264, "start": 7512.64, "end": 7522.76, "text": " I know that for Dawnbench, the best we could get down to is five minutes per batch of ImageNet", "tokens": [286, 458, 300, 337, 26001, 47244, 11, 264, 1151, 321, 727, 483, 760, 281, 307, 1732, 2077, 680, 15245, 295, 29903, 31890], "temperature": 0.0, "avg_logprob": -0.12055678897433811, "compression_ratio": 1.152, "no_speech_prob": 6.0487691371236e-06}, {"id": 1854, "seek": 751264, "start": 7522.76, "end": 7525.92, "text": " on eight GPUs.", "tokens": [322, 3180, 18407, 82, 13], "temperature": 0.0, "avg_logprob": -0.12055678897433811, "compression_ratio": 1.152, "no_speech_prob": 6.0487691371236e-06}, {"id": 1855, "seek": 751264, "start": 7525.92, "end": 7532.0, "text": " And so that's 1.25 million images.", "tokens": [400, 370, 300, 311, 502, 13, 6074, 2459, 5267, 13], "temperature": 0.0, "avg_logprob": -0.12055678897433811, "compression_ratio": 1.152, "no_speech_prob": 6.0487691371236e-06}, {"id": 1856, "seek": 753200, "start": 7532.0, "end": 7546.04, "text": " So that's on one GPU per minute, that's 31,000 or 500 per second, assuming four cores per", "tokens": [407, 300, 311, 322, 472, 18407, 680, 3456, 11, 300, 311, 10353, 11, 1360, 420, 5923, 680, 1150, 11, 11926, 1451, 24826, 680], "temperature": 0.0, "avg_logprob": -0.1562344188421545, "compression_ratio": 1.5769230769230769, "no_speech_prob": 3.72652698388265e-06}, {"id": 1857, "seek": 753200, "start": 7546.04, "end": 7549.22, "text": " GPU, that's 125 per second.", "tokens": [18407, 11, 300, 311, 25276, 680, 1150, 13], "temperature": 0.0, "avg_logprob": -0.1562344188421545, "compression_ratio": 1.5769230769230769, "no_speech_prob": 3.72652698388265e-06}, {"id": 1858, "seek": 753200, "start": 7549.22, "end": 7552.76, "text": " So we're going to try to stay under 10 milliseconds.", "tokens": [407, 321, 434, 516, 281, 853, 281, 1754, 833, 1266, 34184, 13], "temperature": 0.0, "avg_logprob": -0.1562344188421545, "compression_ratio": 1.5769230769230769, "no_speech_prob": 3.72652698388265e-06}, {"id": 1859, "seek": 753200, "start": 7552.76, "end": 7554.2, "text": " I said 10 milliseconds per image.", "tokens": [286, 848, 1266, 34184, 680, 3256, 13], "temperature": 0.0, "avg_logprob": -0.1562344188421545, "compression_ratio": 1.5769230769230769, "no_speech_prob": 3.72652698388265e-06}, {"id": 1860, "seek": 753200, "start": 7554.2, "end": 7558.48, "text": " I think I mean 10 milliseconds per batch.", "tokens": [286, 519, 286, 914, 1266, 34184, 680, 15245, 13], "temperature": 0.0, "avg_logprob": -0.1562344188421545, "compression_ratio": 1.5769230769230769, "no_speech_prob": 3.72652698388265e-06}, {"id": 1861, "seek": 755848, "start": 7558.48, "end": 7565.04, "text": " So it's actually still a pretty small number.", "tokens": [407, 309, 311, 767, 920, 257, 1238, 1359, 1230, 13], "temperature": 0.0, "avg_logprob": -0.14902034882576234, "compression_ratio": 1.4320987654320987, "no_speech_prob": 5.285472752802889e-07}, {"id": 1862, "seek": 755848, "start": 7565.04, "end": 7571.28, "text": " So we're not too worried at this point about 500 microseconds.", "tokens": [407, 321, 434, 406, 886, 5804, 412, 341, 935, 466, 5923, 3123, 37841, 28750, 13], "temperature": 0.0, "avg_logprob": -0.14902034882576234, "compression_ratio": 1.4320987654320987, "no_speech_prob": 5.285472752802889e-07}, {"id": 1863, "seek": 755848, "start": 7571.28, "end": 7580.5, "text": " But this is always kind of the thing to think about is how much time have you got?", "tokens": [583, 341, 307, 1009, 733, 295, 264, 551, 281, 519, 466, 307, 577, 709, 565, 362, 291, 658, 30], "temperature": 0.0, "avg_logprob": -0.14902034882576234, "compression_ratio": 1.4320987654320987, "no_speech_prob": 5.285472752802889e-07}, {"id": 1864, "seek": 755848, "start": 7580.5, "end": 7583.44, "text": " And sometimes these times really add up.", "tokens": [400, 2171, 613, 1413, 534, 909, 493, 13], "temperature": 0.0, "avg_logprob": -0.14902034882576234, "compression_ratio": 1.4320987654320987, "no_speech_prob": 5.285472752802889e-07}, {"id": 1865, "seek": 758344, "start": 7583.44, "end": 7589.44, "text": " But yeah, 520 per second, we've got some time, especially since we've got normally a few", "tokens": [583, 1338, 11, 1025, 2009, 680, 1150, 11, 321, 600, 658, 512, 565, 11, 2318, 1670, 321, 600, 658, 5646, 257, 1326], "temperature": 0.0, "avg_logprob": -0.13002323809965158, "compression_ratio": 1.4477611940298507, "no_speech_prob": 1.1477711268526036e-05}, {"id": 1866, "seek": 758344, "start": 7589.44, "end": 7592.919999999999, "text": " cores per GPU.", "tokens": [24826, 680, 18407, 13], "temperature": 0.0, "avg_logprob": -0.13002323809965158, "compression_ratio": 1.4477611940298507, "no_speech_prob": 1.1477711268526036e-05}, {"id": 1867, "seek": 758344, "start": 7592.919999999999, "end": 7599.599999999999, "text": " So we can just write some code to do a kind of a general crop transform.", "tokens": [407, 321, 393, 445, 2464, 512, 3089, 281, 360, 257, 733, 295, 257, 2674, 9086, 4088, 13], "temperature": 0.0, "avg_logprob": -0.13002323809965158, "compression_ratio": 1.4477611940298507, "no_speech_prob": 1.1477711268526036e-05}, {"id": 1868, "seek": 758344, "start": 7599.599999999999, "end": 7608.12, "text": " For ImageNet and things like that, for the validation set, what we normally do is we", "tokens": [1171, 29903, 31890, 293, 721, 411, 300, 11, 337, 264, 24071, 992, 11, 437, 321, 5646, 360, 307, 321], "temperature": 0.0, "avg_logprob": -0.13002323809965158, "compression_ratio": 1.4477611940298507, "no_speech_prob": 1.1477711268526036e-05}, {"id": 1869, "seek": 758344, "start": 7608.12, "end": 7610.639999999999, "text": " grab the center of the image.", "tokens": [4444, 264, 3056, 295, 264, 3256, 13], "temperature": 0.0, "avg_logprob": -0.13002323809965158, "compression_ratio": 1.4477611940298507, "no_speech_prob": 1.1477711268526036e-05}, {"id": 1870, "seek": 761064, "start": 7610.64, "end": 7616.4800000000005, "text": " We remove 14% from each side and grab the center.", "tokens": [492, 4159, 3499, 4, 490, 1184, 1252, 293, 4444, 264, 3056, 13], "temperature": 0.0, "avg_logprob": -0.13551542033319888, "compression_ratio": 1.588235294117647, "no_speech_prob": 1.0288957128068432e-05}, {"id": 1871, "seek": 761064, "start": 7616.4800000000005, "end": 7620.360000000001, "text": " So we can zoom in a little bit so we have a center crop.", "tokens": [407, 321, 393, 8863, 294, 257, 707, 857, 370, 321, 362, 257, 3056, 9086, 13], "temperature": 0.0, "avg_logprob": -0.13551542033319888, "compression_ratio": 1.588235294117647, "no_speech_prob": 1.0288957128068432e-05}, {"id": 1872, "seek": 761064, "start": 7620.360000000001, "end": 7623.96, "text": " So here we show all that.", "tokens": [407, 510, 321, 855, 439, 300, 13], "temperature": 0.0, "avg_logprob": -0.13551542033319888, "compression_ratio": 1.588235294117647, "no_speech_prob": 1.0288957128068432e-05}, {"id": 1873, "seek": 761064, "start": 7623.96, "end": 7625.400000000001, "text": " That's what we do for the validation set.", "tokens": [663, 311, 437, 321, 360, 337, 264, 24071, 992, 13], "temperature": 0.0, "avg_logprob": -0.13551542033319888, "compression_ratio": 1.588235294117647, "no_speech_prob": 1.0288957128068432e-05}, {"id": 1874, "seek": 761064, "start": 7625.400000000001, "end": 7629.4800000000005, "text": " And obviously they're all the same because validation set doesn't have the randomness.", "tokens": [400, 2745, 436, 434, 439, 264, 912, 570, 24071, 992, 1177, 380, 362, 264, 4974, 1287, 13], "temperature": 0.0, "avg_logprob": -0.13551542033319888, "compression_ratio": 1.588235294117647, "no_speech_prob": 1.0288957128068432e-05}, {"id": 1875, "seek": 761064, "start": 7629.4800000000005, "end": 7636.56, "text": " But for the training set, the most useful transformation by far, like all the competition", "tokens": [583, 337, 264, 3097, 992, 11, 264, 881, 4420, 9887, 538, 1400, 11, 411, 439, 264, 6211], "temperature": 0.0, "avg_logprob": -0.13551542033319888, "compression_ratio": 1.588235294117647, "no_speech_prob": 1.0288957128068432e-05}, {"id": 1876, "seek": 763656, "start": 7636.56, "end": 7642.56, "text": " winners, grab a small piece of the image and zoom into it.", "tokens": [17193, 11, 4444, 257, 1359, 2522, 295, 264, 3256, 293, 8863, 666, 309, 13], "temperature": 0.0, "avg_logprob": -0.09375439643859863, "compression_ratio": 1.625, "no_speech_prob": 3.611884039855795e-06}, {"id": 1877, "seek": 763656, "start": 7642.56, "end": 7644.64, "text": " This is called a random resize crop.", "tokens": [639, 307, 1219, 257, 4974, 50069, 9086, 13], "temperature": 0.0, "avg_logprob": -0.09375439643859863, "compression_ratio": 1.625, "no_speech_prob": 3.611884039855795e-06}, {"id": 1878, "seek": 763656, "start": 7644.64, "end": 7649.92, "text": " And this is going to be really useful to know about for any domain.", "tokens": [400, 341, 307, 516, 281, 312, 534, 4420, 281, 458, 466, 337, 604, 9274, 13], "temperature": 0.0, "avg_logprob": -0.09375439643859863, "compression_ratio": 1.625, "no_speech_prob": 3.611884039855795e-06}, {"id": 1879, "seek": 763656, "start": 7649.92, "end": 7655.76, "text": " So for example, in NLP, a really useful thing to do is to grab different sized chunks of", "tokens": [407, 337, 1365, 11, 294, 426, 45196, 11, 257, 534, 4420, 551, 281, 360, 307, 281, 4444, 819, 20004, 24004, 295], "temperature": 0.0, "avg_logprob": -0.09375439643859863, "compression_ratio": 1.625, "no_speech_prob": 3.611884039855795e-06}, {"id": 1880, "seek": 763656, "start": 7655.76, "end": 7657.92, "text": " contiguous text.", "tokens": [660, 30525, 2487, 13], "temperature": 0.0, "avg_logprob": -0.09375439643859863, "compression_ratio": 1.625, "no_speech_prob": 3.611884039855795e-06}, {"id": 1881, "seek": 763656, "start": 7657.92, "end": 7663.240000000001, "text": " With audio, if you're doing speech recognition, grab different sized pieces of the utterances", "tokens": [2022, 6278, 11, 498, 291, 434, 884, 6218, 11150, 11, 4444, 819, 20004, 3755, 295, 264, 17567, 2676], "temperature": 0.0, "avg_logprob": -0.09375439643859863, "compression_ratio": 1.625, "no_speech_prob": 3.611884039855795e-06}, {"id": 1882, "seek": 763656, "start": 7663.240000000001, "end": 7664.240000000001, "text": " and so forth.", "tokens": [293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.09375439643859863, "compression_ratio": 1.625, "no_speech_prob": 3.611884039855795e-06}, {"id": 1883, "seek": 766424, "start": 7664.24, "end": 7671.04, "text": " So if you can find a way to get different slices of your data, it's a fantastically", "tokens": [407, 498, 291, 393, 915, 257, 636, 281, 483, 819, 19793, 295, 428, 1412, 11, 309, 311, 257, 4115, 22808], "temperature": 0.0, "avg_logprob": -0.11210654155317559, "compression_ratio": 1.5450236966824644, "no_speech_prob": 5.4219135563471355e-06}, {"id": 1884, "seek": 766424, "start": 7671.04, "end": 7673.76, "text": " useful data augmentation approach.", "tokens": [4420, 1412, 14501, 19631, 3109, 13], "temperature": 0.0, "avg_logprob": -0.11210654155317559, "compression_ratio": 1.5450236966824644, "no_speech_prob": 5.4219135563471355e-06}, {"id": 1885, "seek": 766424, "start": 7673.76, "end": 7681.2, "text": " And so this is by far the main, most important augmentation used in every ImageNet winner", "tokens": [400, 370, 341, 307, 538, 1400, 264, 2135, 11, 881, 1021, 14501, 19631, 1143, 294, 633, 29903, 31890, 8507], "temperature": 0.0, "avg_logprob": -0.11210654155317559, "compression_ratio": 1.5450236966824644, "no_speech_prob": 5.4219135563471355e-06}, {"id": 1886, "seek": 766424, "start": 7681.2, "end": 7685.88, "text": " for the last six years or so.", "tokens": [337, 264, 1036, 2309, 924, 420, 370, 13], "temperature": 0.0, "avg_logprob": -0.11210654155317559, "compression_ratio": 1.5450236966824644, "no_speech_prob": 5.4219135563471355e-06}, {"id": 1887, "seek": 766424, "start": 7685.88, "end": 7691.84, "text": " It's a bit weird though because what they do in this approach is this little ratio here", "tokens": [467, 311, 257, 857, 3657, 1673, 570, 437, 436, 360, 294, 341, 3109, 307, 341, 707, 8509, 510], "temperature": 0.0, "avg_logprob": -0.11210654155317559, "compression_ratio": 1.5450236966824644, "no_speech_prob": 5.4219135563471355e-06}, {"id": 1888, "seek": 769184, "start": 7691.84, "end": 7699.6, "text": " says squish it by between a three over four aspect ratio to a four over three aspect ratio.", "tokens": [1619, 31379, 309, 538, 1296, 257, 1045, 670, 1451, 4171, 8509, 281, 257, 1451, 670, 1045, 4171, 8509, 13], "temperature": 0.0, "avg_logprob": -0.12752188632362768, "compression_ratio": 1.728888888888889, "no_speech_prob": 1.1842399544548243e-05}, {"id": 1889, "seek": 769184, "start": 7699.6, "end": 7705.16, "text": " And so it literally makes the person, see here he's looking quite thin and see here", "tokens": [400, 370, 309, 3736, 1669, 264, 954, 11, 536, 510, 415, 311, 1237, 1596, 5862, 293, 536, 510], "temperature": 0.0, "avg_logprob": -0.12752188632362768, "compression_ratio": 1.728888888888889, "no_speech_prob": 1.1842399544548243e-05}, {"id": 1890, "seek": 769184, "start": 7705.16, "end": 7708.16, "text": " he's looking quite wide.", "tokens": [415, 311, 1237, 1596, 4874, 13], "temperature": 0.0, "avg_logprob": -0.12752188632362768, "compression_ratio": 1.728888888888889, "no_speech_prob": 1.1842399544548243e-05}, {"id": 1891, "seek": 769184, "start": 7708.16, "end": 7713.860000000001, "text": " It doesn't actually make any sense, this transformation, because optically speaking, there's no way", "tokens": [467, 1177, 380, 767, 652, 604, 2020, 11, 341, 9887, 11, 570, 2427, 984, 4124, 11, 456, 311, 572, 636], "temperature": 0.0, "avg_logprob": -0.12752188632362768, "compression_ratio": 1.728888888888889, "no_speech_prob": 1.1842399544548243e-05}, {"id": 1892, "seek": 769184, "start": 7713.860000000001, "end": 7720.2, "text": " of looking at something in normal day-to-day life that causes them to expand outwards or", "tokens": [295, 1237, 412, 746, 294, 2710, 786, 12, 1353, 12, 810, 993, 300, 7700, 552, 281, 5268, 484, 2015, 420], "temperature": 0.0, "avg_logprob": -0.12752188632362768, "compression_ratio": 1.728888888888889, "no_speech_prob": 1.1842399544548243e-05}, {"id": 1893, "seek": 772020, "start": 7720.2, "end": 7723.2, "text": " contract inwards.", "tokens": [4364, 294, 2015, 13], "temperature": 0.0, "avg_logprob": -0.10744436946483928, "compression_ratio": 1.8271604938271604, "no_speech_prob": 1.260633234778652e-05}, {"id": 1894, "seek": 772020, "start": 7723.2, "end": 7728.24, "text": " So when we looked at this, we thought, I think what happened here is that they were, this", "tokens": [407, 562, 321, 2956, 412, 341, 11, 321, 1194, 11, 286, 519, 437, 2011, 510, 307, 300, 436, 645, 11, 341], "temperature": 0.0, "avg_logprob": -0.10744436946483928, "compression_ratio": 1.8271604938271604, "no_speech_prob": 1.260633234778652e-05}, {"id": 1895, "seek": 772020, "start": 7728.24, "end": 7732.12, "text": " is the best they could do with the tools they had, but probably what they really want to", "tokens": [307, 264, 1151, 436, 727, 360, 365, 264, 3873, 436, 632, 11, 457, 1391, 437, 436, 534, 528, 281], "temperature": 0.0, "avg_logprob": -0.10744436946483928, "compression_ratio": 1.8271604938271604, "no_speech_prob": 1.260633234778652e-05}, {"id": 1896, "seek": 772020, "start": 7732.12, "end": 7737.22, "text": " do is to do the thing that's kind of like physically reasonable.", "tokens": [360, 307, 281, 360, 264, 551, 300, 311, 733, 295, 411, 9762, 10585, 13], "temperature": 0.0, "avg_logprob": -0.10744436946483928, "compression_ratio": 1.8271604938271604, "no_speech_prob": 1.260633234778652e-05}, {"id": 1897, "seek": 772020, "start": 7737.22, "end": 7741.0, "text": " And so the physically reasonable thing is like you might be a bit above somebody or", "tokens": [400, 370, 264, 9762, 10585, 551, 307, 411, 291, 1062, 312, 257, 857, 3673, 2618, 420], "temperature": 0.0, "avg_logprob": -0.10744436946483928, "compression_ratio": 1.8271604938271604, "no_speech_prob": 1.260633234778652e-05}, {"id": 1898, "seek": 772020, "start": 7741.0, "end": 7745.04, "text": " a bit below somebody or left of somebody or right of somebody, causing your perspective", "tokens": [257, 857, 2507, 2618, 420, 1411, 295, 2618, 420, 558, 295, 2618, 11, 9853, 428, 4585], "temperature": 0.0, "avg_logprob": -0.10744436946483928, "compression_ratio": 1.8271604938271604, "no_speech_prob": 1.260633234778652e-05}, {"id": 1899, "seek": 772020, "start": 7745.04, "end": 7746.8, "text": " to change.", "tokens": [281, 1319, 13], "temperature": 0.0, "avg_logprob": -0.10744436946483928, "compression_ratio": 1.8271604938271604, "no_speech_prob": 1.260633234778652e-05}, {"id": 1900, "seek": 774680, "start": 7746.8, "end": 7752.78, "text": " So our guess is that what we actually want is not this, but this.", "tokens": [407, 527, 2041, 307, 300, 437, 321, 767, 528, 307, 406, 341, 11, 457, 341, 13], "temperature": 0.0, "avg_logprob": -0.11386174247378394, "compression_ratio": 1.6777251184834123, "no_speech_prob": 6.338923412840813e-06}, {"id": 1901, "seek": 774680, "start": 7752.78, "end": 7759.5, "text": " So perspective warping is basically something that looks like this.", "tokens": [407, 4585, 1516, 3381, 307, 1936, 746, 300, 1542, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.11386174247378394, "compression_ratio": 1.6777251184834123, "no_speech_prob": 6.338923412840813e-06}, {"id": 1902, "seek": 774680, "start": 7759.5, "end": 7763.92, "text": " You basically have four points, right, and you think about how would those four points", "tokens": [509, 1936, 362, 1451, 2793, 11, 558, 11, 293, 291, 519, 466, 577, 576, 729, 1451, 2793], "temperature": 0.0, "avg_logprob": -0.11386174247378394, "compression_ratio": 1.6777251184834123, "no_speech_prob": 6.338923412840813e-06}, {"id": 1903, "seek": 774680, "start": 7763.92, "end": 7767.68, "text": " map to four other points if they were going through some angle.", "tokens": [4471, 281, 1451, 661, 2793, 498, 436, 645, 516, 807, 512, 5802, 13], "temperature": 0.0, "avg_logprob": -0.11386174247378394, "compression_ratio": 1.6777251184834123, "no_speech_prob": 6.338923412840813e-06}, {"id": 1904, "seek": 774680, "start": 7767.68, "end": 7772.900000000001, "text": " So it's like as you look from different directions, roughly speaking.", "tokens": [407, 309, 311, 411, 382, 291, 574, 490, 819, 11095, 11, 9810, 4124, 13], "temperature": 0.0, "avg_logprob": -0.11386174247378394, "compression_ratio": 1.6777251184834123, "no_speech_prob": 6.338923412840813e-06}, {"id": 1905, "seek": 777290, "start": 7772.9, "end": 7779.16, "text": " And the reason that I really like this idea is because when you're doing data augmentation", "tokens": [400, 264, 1778, 300, 286, 534, 411, 341, 1558, 307, 570, 562, 291, 434, 884, 1412, 14501, 19631], "temperature": 0.0, "avg_logprob": -0.12169839158842835, "compression_ratio": 1.5422885572139304, "no_speech_prob": 5.337940365279792e-06}, {"id": 1906, "seek": 777290, "start": 7779.16, "end": 7786.98, "text": " in any domain, as I mentioned, the idea is to try and create physically reasonable in", "tokens": [294, 604, 9274, 11, 382, 286, 2835, 11, 264, 1558, 307, 281, 853, 293, 1884, 9762, 10585, 294], "temperature": 0.0, "avg_logprob": -0.12169839158842835, "compression_ratio": 1.5422885572139304, "no_speech_prob": 5.337940365279792e-06}, {"id": 1907, "seek": 777290, "start": 7786.98, "end": 7789.44, "text": " your domain inputs.", "tokens": [428, 9274, 15743, 13], "temperature": 0.0, "avg_logprob": -0.12169839158842835, "compression_ratio": 1.5422885572139304, "no_speech_prob": 5.337940365279792e-06}, {"id": 1908, "seek": 777290, "start": 7789.44, "end": 7791.5199999999995, "text": " And these just aren't.", "tokens": [400, 613, 445, 3212, 380, 13], "temperature": 0.0, "avg_logprob": -0.12169839158842835, "compression_ratio": 1.5422885572139304, "no_speech_prob": 5.337940365279792e-06}, {"id": 1909, "seek": 777290, "start": 7791.5199999999995, "end": 7796.66, "text": " Like you can't make somebody squishier in real world.", "tokens": [1743, 291, 393, 380, 652, 2618, 31379, 811, 294, 957, 1002, 13], "temperature": 0.0, "avg_logprob": -0.12169839158842835, "compression_ratio": 1.5422885572139304, "no_speech_prob": 5.337940365279792e-06}, {"id": 1910, "seek": 777290, "start": 7796.66, "end": 7798.799999999999, "text": " But you can shift their perspective.", "tokens": [583, 291, 393, 5513, 641, 4585, 13], "temperature": 0.0, "avg_logprob": -0.12169839158842835, "compression_ratio": 1.5422885572139304, "no_speech_prob": 5.337940365279792e-06}, {"id": 1911, "seek": 779880, "start": 7798.8, "end": 7805.2, "text": " So if we do a perspective transform, then they look like this.", "tokens": [407, 498, 321, 360, 257, 4585, 4088, 11, 550, 436, 574, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.10815274374825613, "compression_ratio": 1.7991071428571428, "no_speech_prob": 5.954958396614529e-06}, {"id": 1912, "seek": 779880, "start": 7805.2, "end": 7806.8, "text": " And this is true, right?", "tokens": [400, 341, 307, 2074, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.10815274374825613, "compression_ratio": 1.7991071428571428, "no_speech_prob": 5.954958396614529e-06}, {"id": 1913, "seek": 779880, "start": 7806.8, "end": 7810.12, "text": " If you're a bit underneath them, the fish will look a bit closer.", "tokens": [759, 291, 434, 257, 857, 7223, 552, 11, 264, 3506, 486, 574, 257, 857, 4966, 13], "temperature": 0.0, "avg_logprob": -0.10815274374825613, "compression_ratio": 1.7991071428571428, "no_speech_prob": 5.954958396614529e-06}, {"id": 1914, "seek": 779880, "start": 7810.12, "end": 7813.76, "text": " Or if you're a bit over here, then the hat's a bit closer from that side.", "tokens": [1610, 498, 291, 434, 257, 857, 670, 510, 11, 550, 264, 2385, 311, 257, 857, 4966, 490, 300, 1252, 13], "temperature": 0.0, "avg_logprob": -0.10815274374825613, "compression_ratio": 1.7991071428571428, "no_speech_prob": 5.954958396614529e-06}, {"id": 1915, "seek": 779880, "start": 7813.76, "end": 7819.72, "text": " So these perspective transforms make a lot more sense.", "tokens": [407, 613, 4585, 35592, 652, 257, 688, 544, 2020, 13], "temperature": 0.0, "avg_logprob": -0.10815274374825613, "compression_ratio": 1.7991071428571428, "no_speech_prob": 5.954958396614529e-06}, {"id": 1916, "seek": 779880, "start": 7819.72, "end": 7823.64, "text": " So if you're interested in perspective transforms, we have some details here on how you actually", "tokens": [407, 498, 291, 434, 3102, 294, 4585, 35592, 11, 321, 362, 512, 4365, 510, 322, 577, 291, 767], "temperature": 0.0, "avg_logprob": -0.10815274374825613, "compression_ratio": 1.7991071428571428, "no_speech_prob": 5.954958396614529e-06}, {"id": 1917, "seek": 779880, "start": 7823.64, "end": 7825.8, "text": " do them mathematically.", "tokens": [360, 552, 44003, 13], "temperature": 0.0, "avg_logprob": -0.10815274374825613, "compression_ratio": 1.7991071428571428, "no_speech_prob": 5.954958396614529e-06}, {"id": 1918, "seek": 782580, "start": 7825.8, "end": 7830.64, "text": " The details aren't important, but what are interesting is the transform actually requires", "tokens": [440, 4365, 3212, 380, 1021, 11, 457, 437, 366, 1880, 307, 264, 4088, 767, 7029], "temperature": 0.0, "avg_logprob": -0.13791748008342705, "compression_ratio": 1.610878661087866, "no_speech_prob": 1.1299307516310364e-05}, {"id": 1919, "seek": 782580, "start": 7830.64, "end": 7833.28, "text": " solving a system of linear equations.", "tokens": [12606, 257, 1185, 295, 8213, 11787, 13], "temperature": 0.0, "avg_logprob": -0.13791748008342705, "compression_ratio": 1.610878661087866, "no_speech_prob": 1.1299307516310364e-05}, {"id": 1920, "seek": 782580, "start": 7833.28, "end": 7837.4800000000005, "text": " And did you know that PyTorch has a function for solving systems of linear equations?", "tokens": [400, 630, 291, 458, 300, 9953, 51, 284, 339, 575, 257, 2445, 337, 12606, 3652, 295, 8213, 11787, 30], "temperature": 0.0, "avg_logprob": -0.13791748008342705, "compression_ratio": 1.610878661087866, "no_speech_prob": 1.1299307516310364e-05}, {"id": 1921, "seek": 782580, "start": 7837.4800000000005, "end": 7841.28, "text": " It's amazing how much stuff is in PyTorch, right?", "tokens": [467, 311, 2243, 577, 709, 1507, 307, 294, 9953, 51, 284, 339, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.13791748008342705, "compression_ratio": 1.610878661087866, "no_speech_prob": 1.1299307516310364e-05}, {"id": 1922, "seek": 782580, "start": 7841.28, "end": 7844.92, "text": " So for like lots of the things you'll need in your domain, you might be surprised to", "tokens": [407, 337, 411, 3195, 295, 264, 721, 291, 603, 643, 294, 428, 9274, 11, 291, 1062, 312, 6100, 281], "temperature": 0.0, "avg_logprob": -0.13791748008342705, "compression_ratio": 1.610878661087866, "no_speech_prob": 1.1299307516310364e-05}, {"id": 1923, "seek": 782580, "start": 7844.92, "end": 7846.92, "text": " find what's already there.", "tokens": [915, 437, 311, 1217, 456, 13], "temperature": 0.0, "avg_logprob": -0.13791748008342705, "compression_ratio": 1.610878661087866, "no_speech_prob": 1.1299307516310364e-05}, {"id": 1924, "seek": 782580, "start": 7846.92, "end": 7850.72, "text": " Question?", "tokens": [14464, 30], "temperature": 0.0, "avg_logprob": -0.13791748008342705, "compression_ratio": 1.610878661087866, "no_speech_prob": 1.1299307516310364e-05}, {"id": 1925, "seek": 785072, "start": 7850.72, "end": 7856.2, "text": " And with the cropping and resizing, what happens when you lose the object of interest, so when", "tokens": [400, 365, 264, 4848, 3759, 293, 725, 3319, 11, 437, 2314, 562, 291, 3624, 264, 2657, 295, 1179, 11, 370, 562], "temperature": 0.0, "avg_logprob": -0.18888067984366202, "compression_ratio": 1.5555555555555556, "no_speech_prob": 2.2468242605100386e-05}, {"id": 1926, "seek": 785072, "start": 7856.2, "end": 7858.84, "text": " the fish has been cropped out?", "tokens": [264, 3506, 575, 668, 4848, 3320, 484, 30], "temperature": 0.0, "avg_logprob": -0.18888067984366202, "compression_ratio": 1.5555555555555556, "no_speech_prob": 2.2468242605100386e-05}, {"id": 1927, "seek": 785072, "start": 7858.84, "end": 7859.84, "text": " That's a great question.", "tokens": [663, 311, 257, 869, 1168, 13], "temperature": 0.0, "avg_logprob": -0.18888067984366202, "compression_ratio": 1.5555555555555556, "no_speech_prob": 2.2468242605100386e-05}, {"id": 1928, "seek": 785072, "start": 7859.84, "end": 7861.72, "text": " It's not just a fish.", "tokens": [467, 311, 406, 445, 257, 3506, 13], "temperature": 0.0, "avg_logprob": -0.18888067984366202, "compression_ratio": 1.5555555555555556, "no_speech_prob": 2.2468242605100386e-05}, {"id": 1929, "seek": 785072, "start": 7861.72, "end": 7863.52, "text": " It's a tensch.", "tokens": [467, 311, 257, 256, 26590, 13], "temperature": 0.0, "avg_logprob": -0.18888067984366202, "compression_ratio": 1.5555555555555556, "no_speech_prob": 2.2468242605100386e-05}, {"id": 1930, "seek": 785072, "start": 7863.52, "end": 7864.52, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.18888067984366202, "compression_ratio": 1.5555555555555556, "no_speech_prob": 2.2468242605100386e-05}, {"id": 1931, "seek": 785072, "start": 7864.52, "end": 7866.96, "text": " So there's no tensch here.", "tokens": [407, 456, 311, 572, 256, 26590, 510, 13], "temperature": 0.0, "avg_logprob": -0.18888067984366202, "compression_ratio": 1.5555555555555556, "no_speech_prob": 2.2468242605100386e-05}, {"id": 1932, "seek": 785072, "start": 7866.96, "end": 7869.64, "text": " And so these are noisy labels.", "tokens": [400, 370, 613, 366, 24518, 16949, 13], "temperature": 0.0, "avg_logprob": -0.18888067984366202, "compression_ratio": 1.5555555555555556, "no_speech_prob": 2.2468242605100386e-05}, {"id": 1933, "seek": 785072, "start": 7869.64, "end": 7877.6, "text": " And interestingly, the kind of image net winning strategy is to randomly pick between 8% and", "tokens": [400, 25873, 11, 264, 733, 295, 3256, 2533, 8224, 5206, 307, 281, 16979, 1888, 1296, 1649, 4, 293], "temperature": 0.0, "avg_logprob": -0.18888067984366202, "compression_ratio": 1.5555555555555556, "no_speech_prob": 2.2468242605100386e-05}, {"id": 1934, "seek": 785072, "start": 7877.6, "end": 7879.84, "text": " 100% of the pixels.", "tokens": [2319, 4, 295, 264, 18668, 13], "temperature": 0.0, "avg_logprob": -0.18888067984366202, "compression_ratio": 1.5555555555555556, "no_speech_prob": 2.2468242605100386e-05}, {"id": 1935, "seek": 787984, "start": 7879.84, "end": 7885.52, "text": " So literally, they are very often picking 8% of the pixels.", "tokens": [407, 3736, 11, 436, 366, 588, 2049, 8867, 1649, 4, 295, 264, 18668, 13], "temperature": 0.0, "avg_logprob": -0.09194739965292123, "compression_ratio": 1.7056277056277056, "no_speech_prob": 2.769255615930888e-06}, {"id": 1936, "seek": 787984, "start": 7885.52, "end": 7887.7, "text": " And that's the image net winning strategy.", "tokens": [400, 300, 311, 264, 3256, 2533, 8224, 5206, 13], "temperature": 0.0, "avg_logprob": -0.09194739965292123, "compression_ratio": 1.7056277056277056, "no_speech_prob": 2.769255615930888e-06}, {"id": 1937, "seek": 787984, "start": 7887.7, "end": 7890.400000000001, "text": " So they very often have no tensch.", "tokens": [407, 436, 588, 2049, 362, 572, 256, 26590, 13], "temperature": 0.0, "avg_logprob": -0.09194739965292123, "compression_ratio": 1.7056277056277056, "no_speech_prob": 2.769255615930888e-06}, {"id": 1938, "seek": 787984, "start": 7890.400000000001, "end": 7896.76, "text": " So very often they'll have just the fin or just the eye.", "tokens": [407, 588, 2049, 436, 603, 362, 445, 264, 962, 420, 445, 264, 3313, 13], "temperature": 0.0, "avg_logprob": -0.09194739965292123, "compression_ratio": 1.7056277056277056, "no_speech_prob": 2.769255615930888e-06}, {"id": 1939, "seek": 787984, "start": 7896.76, "end": 7901.0, "text": " So this tells us that if we want to use this really effective augmentation strategy really", "tokens": [407, 341, 5112, 505, 300, 498, 321, 528, 281, 764, 341, 534, 4942, 14501, 19631, 5206, 534], "temperature": 0.0, "avg_logprob": -0.09194739965292123, "compression_ratio": 1.7056277056277056, "no_speech_prob": 2.769255615930888e-06}, {"id": 1940, "seek": 787984, "start": 7901.0, "end": 7904.76, "text": " well, we have to be very good at handling noisy labels, which we're going to learn about", "tokens": [731, 11, 321, 362, 281, 312, 588, 665, 412, 13175, 24518, 16949, 11, 597, 321, 434, 516, 281, 1466, 466], "temperature": 0.0, "avg_logprob": -0.09194739965292123, "compression_ratio": 1.7056277056277056, "no_speech_prob": 2.769255615930888e-06}, {"id": 1941, "seek": 787984, "start": 7904.76, "end": 7907.64, "text": " in the next lesson.", "tokens": [294, 264, 958, 6898, 13], "temperature": 0.0, "avg_logprob": -0.09194739965292123, "compression_ratio": 1.7056277056277056, "no_speech_prob": 2.769255615930888e-06}, {"id": 1942, "seek": 790764, "start": 7907.64, "end": 7913.6, "text": " And it also hopefully tells you that if you already have noisy labels, don't worry about", "tokens": [400, 309, 611, 4696, 5112, 291, 300, 498, 291, 1217, 362, 24518, 16949, 11, 500, 380, 3292, 466], "temperature": 0.0, "avg_logprob": -0.10561731238114207, "compression_ratio": 1.6384976525821595, "no_speech_prob": 1.9637388959381497e-06}, {"id": 1943, "seek": 790764, "start": 7913.6, "end": 7914.6, "text": " it.", "tokens": [309, 13], "temperature": 0.0, "avg_logprob": -0.10561731238114207, "compression_ratio": 1.6384976525821595, "no_speech_prob": 1.9637388959381497e-06}, {"id": 1944, "seek": 790764, "start": 7914.6, "end": 7920.92, "text": " All of the research we have tells us that we can handle labels where the thing's totally", "tokens": [1057, 295, 264, 2132, 321, 362, 5112, 505, 300, 321, 393, 4813, 16949, 689, 264, 551, 311, 3879], "temperature": 0.0, "avg_logprob": -0.10561731238114207, "compression_ratio": 1.6384976525821595, "no_speech_prob": 1.9637388959381497e-06}, {"id": 1945, "seek": 790764, "start": 7920.92, "end": 7925.64, "text": " missing or sometimes it's wrong, as long as it's not biased.", "tokens": [5361, 420, 2171, 309, 311, 2085, 11, 382, 938, 382, 309, 311, 406, 28035, 13], "temperature": 0.0, "avg_logprob": -0.10561731238114207, "compression_ratio": 1.6384976525821595, "no_speech_prob": 1.9637388959381497e-06}, {"id": 1946, "seek": 790764, "start": 7925.64, "end": 7926.64, "text": " So yeah, it's okay.", "tokens": [407, 1338, 11, 309, 311, 1392, 13], "temperature": 0.0, "avg_logprob": -0.10561731238114207, "compression_ratio": 1.6384976525821595, "no_speech_prob": 1.9637388959381497e-06}, {"id": 1947, "seek": 790764, "start": 7926.64, "end": 7932.8, "text": " And one of the things it'll do is it'll learn to find things associated with a tensch.", "tokens": [400, 472, 295, 264, 721, 309, 603, 360, 307, 309, 603, 1466, 281, 915, 721, 6615, 365, 257, 256, 26590, 13], "temperature": 0.0, "avg_logprob": -0.10561731238114207, "compression_ratio": 1.6384976525821595, "no_speech_prob": 1.9637388959381497e-06}, {"id": 1948, "seek": 793280, "start": 7932.8, "end": 7940.24, "text": " So if there's a middle-aged man looking very happy outside, could well be a tensch.", "tokens": [407, 498, 456, 311, 257, 2808, 12, 2980, 587, 1237, 588, 2055, 2380, 11, 727, 731, 312, 257, 256, 26590, 13], "temperature": 0.0, "avg_logprob": -0.13651600943671333, "compression_ratio": 1.5129310344827587, "no_speech_prob": 2.2251847440202255e-06}, {"id": 1949, "seek": 793280, "start": 7940.24, "end": 7942.2, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.13651600943671333, "compression_ratio": 1.5129310344827587, "no_speech_prob": 2.2251847440202255e-06}, {"id": 1950, "seek": 793280, "start": 7942.2, "end": 7945.46, "text": " So this is a bit of research that we're currently working on.", "tokens": [407, 341, 307, 257, 857, 295, 2132, 300, 321, 434, 4362, 1364, 322, 13], "temperature": 0.0, "avg_logprob": -0.13651600943671333, "compression_ratio": 1.5129310344827587, "no_speech_prob": 2.2251847440202255e-06}, {"id": 1951, "seek": 793280, "start": 7945.46, "end": 7948.64, "text": " And hopefully I'll have some results to show you soon.", "tokens": [400, 4696, 286, 603, 362, 512, 3542, 281, 855, 291, 2321, 13], "temperature": 0.0, "avg_logprob": -0.13651600943671333, "compression_ratio": 1.5129310344827587, "no_speech_prob": 2.2251847440202255e-06}, {"id": 1952, "seek": 793280, "start": 7948.64, "end": 7955.4800000000005, "text": " But our view is that this image warping approach is probably going to give us better results", "tokens": [583, 527, 1910, 307, 300, 341, 3256, 1516, 3381, 3109, 307, 1391, 516, 281, 976, 505, 1101, 3542], "temperature": 0.0, "avg_logprob": -0.13651600943671333, "compression_ratio": 1.5129310344827587, "no_speech_prob": 2.2251847440202255e-06}, {"id": 1953, "seek": 793280, "start": 7955.4800000000005, "end": 7959.900000000001, "text": " than the traditional image net style augmentations.", "tokens": [813, 264, 5164, 3256, 2533, 3758, 29919, 763, 13], "temperature": 0.0, "avg_logprob": -0.13651600943671333, "compression_ratio": 1.5129310344827587, "no_speech_prob": 2.2251847440202255e-06}, {"id": 1954, "seek": 795990, "start": 7959.9, "end": 7964.9, "text": " So here's our final transform for tilting in arbitrary directions.", "tokens": [407, 510, 311, 527, 2572, 4088, 337, 8440, 783, 294, 23211, 11095, 13], "temperature": 0.0, "avg_logprob": -0.09075556574641047, "compression_ratio": 1.5026455026455026, "no_speech_prob": 6.4384789766336326e-06}, {"id": 1955, "seek": 795990, "start": 7964.9, "end": 7967.48, "text": " And here's the result.", "tokens": [400, 510, 311, 264, 1874, 13], "temperature": 0.0, "avg_logprob": -0.09075556574641047, "compression_ratio": 1.5026455026455026, "no_speech_prob": 6.4384789766336326e-06}, {"id": 1956, "seek": 795990, "start": 7967.48, "end": 7968.639999999999, "text": " Not bad.", "tokens": [1726, 1578, 13], "temperature": 0.0, "avg_logprob": -0.09075556574641047, "compression_ratio": 1.5026455026455026, "no_speech_prob": 6.4384789766336326e-06}, {"id": 1957, "seek": 795990, "start": 7968.639999999999, "end": 7972.4, "text": " So a couple of things to finish on.", "tokens": [407, 257, 1916, 295, 721, 281, 2413, 322, 13], "temperature": 0.0, "avg_logprob": -0.09075556574641047, "compression_ratio": 1.5026455026455026, "no_speech_prob": 6.4384789766336326e-06}, {"id": 1958, "seek": 795990, "start": 7972.4, "end": 7978.0, "text": " The first is that it's really important to measure everything.", "tokens": [440, 700, 307, 300, 309, 311, 534, 1021, 281, 3481, 1203, 13], "temperature": 0.0, "avg_logprob": -0.09075556574641047, "compression_ratio": 1.5026455026455026, "no_speech_prob": 6.4384789766336326e-06}, {"id": 1959, "seek": 795990, "start": 7978.0, "end": 7983.5199999999995, "text": " And I and many people have been shocked to discover that actually the time it takes to", "tokens": [400, 286, 293, 867, 561, 362, 668, 12763, 281, 4411, 300, 767, 264, 565, 309, 2516, 281], "temperature": 0.0, "avg_logprob": -0.09075556574641047, "compression_ratio": 1.5026455026455026, "no_speech_prob": 6.4384789766336326e-06}, {"id": 1960, "seek": 798352, "start": 7983.52, "end": 7993.280000000001, "text": " convert an image into a float tensor is significantly longer than the amount of time it takes to", "tokens": [7620, 364, 3256, 666, 257, 15706, 40863, 307, 10591, 2854, 813, 264, 2372, 295, 565, 309, 2516, 281], "temperature": 0.0, "avg_logprob": -0.12464171396174901, "compression_ratio": 1.5869565217391304, "no_speech_prob": 7.811382829459035e-07}, {"id": 1961, "seek": 798352, "start": 7993.280000000001, "end": 7998.660000000001, "text": " do something as complicated as a warp.", "tokens": [360, 746, 382, 6179, 382, 257, 36030, 13], "temperature": 0.0, "avg_logprob": -0.12464171396174901, "compression_ratio": 1.5869565217391304, "no_speech_prob": 7.811382829459035e-07}, {"id": 1962, "seek": 798352, "start": 7998.660000000001, "end": 8005.320000000001, "text": " So you may be thinking this image warping thing sounds really hard and slow, but be", "tokens": [407, 291, 815, 312, 1953, 341, 3256, 1516, 3381, 551, 3263, 534, 1152, 293, 2964, 11, 457, 312], "temperature": 0.0, "avg_logprob": -0.12464171396174901, "compression_ratio": 1.5869565217391304, "no_speech_prob": 7.811382829459035e-07}, {"id": 1963, "seek": 798352, "start": 8005.320000000001, "end": 8006.820000000001, "text": " careful, right?", "tokens": [5026, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.12464171396174901, "compression_ratio": 1.5869565217391304, "no_speech_prob": 7.811382829459035e-07}, {"id": 1964, "seek": 798352, "start": 8006.820000000001, "end": 8010.820000000001, "text": " Just converting bytes to floats is really hard and slow.", "tokens": [1449, 29942, 36088, 281, 37878, 307, 534, 1152, 293, 2964, 13], "temperature": 0.0, "avg_logprob": -0.12464171396174901, "compression_ratio": 1.5869565217391304, "no_speech_prob": 7.811382829459035e-07}, {"id": 1965, "seek": 801082, "start": 8010.82, "end": 8013.42, "text": " And then this is the one, as I mentioned, this one we're using here is the one that", "tokens": [400, 550, 341, 307, 264, 472, 11, 382, 286, 2835, 11, 341, 472, 321, 434, 1228, 510, 307, 264, 472, 300], "temperature": 0.0, "avg_logprob": -0.1145241543398065, "compression_ratio": 1.683794466403162, "no_speech_prob": 8.012929356482346e-06}, {"id": 1966, "seek": 801082, "start": 8013.42, "end": 8016.16, "text": " comes from TorchVision.", "tokens": [1487, 490, 7160, 339, 53, 1991, 13], "temperature": 0.0, "avg_logprob": -0.1145241543398065, "compression_ratio": 1.683794466403162, "no_speech_prob": 8.012929356482346e-06}, {"id": 1967, "seek": 801082, "start": 8016.16, "end": 8022.5, "text": " We found another version that's like twice as fast, which goes directly to float.", "tokens": [492, 1352, 1071, 3037, 300, 311, 411, 6091, 382, 2370, 11, 597, 1709, 3838, 281, 15706, 13], "temperature": 0.0, "avg_logprob": -0.1145241543398065, "compression_ratio": 1.683794466403162, "no_speech_prob": 8.012929356482346e-06}, {"id": 1968, "seek": 801082, "start": 8022.5, "end": 8025.44, "text": " So this is the one that we're going to be using.", "tokens": [407, 341, 307, 264, 472, 300, 321, 434, 516, 281, 312, 1228, 13], "temperature": 0.0, "avg_logprob": -0.1145241543398065, "compression_ratio": 1.683794466403162, "no_speech_prob": 8.012929356482346e-06}, {"id": 1969, "seek": 801082, "start": 8025.44, "end": 8029.88, "text": " So time everything if you're running, you know, if things are running not fast enough.", "tokens": [407, 565, 1203, 498, 291, 434, 2614, 11, 291, 458, 11, 498, 721, 366, 2614, 406, 2370, 1547, 13], "temperature": 0.0, "avg_logprob": -0.1145241543398065, "compression_ratio": 1.683794466403162, "no_speech_prob": 8.012929356482346e-06}, {"id": 1970, "seek": 801082, "start": 8029.88, "end": 8030.88, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.1145241543398065, "compression_ratio": 1.683794466403162, "no_speech_prob": 8.012929356482346e-06}, {"id": 1971, "seek": 801082, "start": 8030.88, "end": 8037.759999999999, "text": " Here's the thing I'm really excited about for augmentation is this stuff's all still", "tokens": [1692, 311, 264, 551, 286, 478, 534, 2919, 466, 337, 14501, 19631, 307, 341, 1507, 311, 439, 920], "temperature": 0.0, "avg_logprob": -0.1145241543398065, "compression_ratio": 1.683794466403162, "no_speech_prob": 8.012929356482346e-06}, {"id": 1972, "seek": 801082, "start": 8037.759999999999, "end": 8039.88, "text": " too slow.", "tokens": [886, 2964, 13], "temperature": 0.0, "avg_logprob": -0.1145241543398065, "compression_ratio": 1.683794466403162, "no_speech_prob": 8.012929356482346e-06}, {"id": 1973, "seek": 803988, "start": 8039.88, "end": 8049.8, "text": " What if I told you, you could do arbitrary affine transformations, so warping, zooming,", "tokens": [708, 498, 286, 1907, 291, 11, 291, 727, 360, 23211, 2096, 533, 34852, 11, 370, 1516, 3381, 11, 48226, 11], "temperature": 0.0, "avg_logprob": -0.1907186508178711, "compression_ratio": 1.4939024390243902, "no_speech_prob": 1.7880313407658832e-06}, {"id": 1974, "seek": 803988, "start": 8049.8, "end": 8063.4400000000005, "text": " rotating, shifting at a speed which would compare this is the normal speed, this is", "tokens": [19627, 11, 17573, 412, 257, 3073, 597, 576, 6794, 341, 307, 264, 2710, 3073, 11, 341, 307], "temperature": 0.0, "avg_logprob": -0.1907186508178711, "compression_ratio": 1.4939024390243902, "no_speech_prob": 1.7880313407658832e-06}, {"id": 1975, "seek": 803988, "start": 8063.4400000000005, "end": 8065.12, "text": " our speed.", "tokens": [527, 3073, 13], "temperature": 0.0, "avg_logprob": -0.1907186508178711, "compression_ratio": 1.4939024390243902, "no_speech_prob": 1.7880313407658832e-06}, {"id": 1976, "seek": 803988, "start": 8065.12, "end": 8069.32, "text": " So up to like, you know, an order of magnitude or more faster.", "tokens": [407, 493, 281, 411, 11, 291, 458, 11, 364, 1668, 295, 15668, 420, 544, 4663, 13], "temperature": 0.0, "avg_logprob": -0.1907186508178711, "compression_ratio": 1.4939024390243902, "no_speech_prob": 1.7880313407658832e-06}, {"id": 1977, "seek": 806932, "start": 8069.32, "end": 8071.0, "text": " How do we do it?", "tokens": [1012, 360, 321, 360, 309, 30], "temperature": 0.0, "avg_logprob": -0.08687177225321281, "compression_ratio": 1.757201646090535, "no_speech_prob": 9.368383871333208e-06}, {"id": 1978, "seek": 806932, "start": 8071.0, "end": 8074.16, "text": " We figured out how to do it on the GPU.", "tokens": [492, 8932, 484, 577, 281, 360, 309, 322, 264, 18407, 13], "temperature": 0.0, "avg_logprob": -0.08687177225321281, "compression_ratio": 1.757201646090535, "no_speech_prob": 9.368383871333208e-06}, {"id": 1979, "seek": 806932, "start": 8074.16, "end": 8076.88, "text": " So we can actually do augmentation on the GPU.", "tokens": [407, 321, 393, 767, 360, 14501, 19631, 322, 264, 18407, 13], "temperature": 0.0, "avg_logprob": -0.08687177225321281, "compression_ratio": 1.757201646090535, "no_speech_prob": 9.368383871333208e-06}, {"id": 1980, "seek": 806932, "start": 8076.88, "end": 8082.44, "text": " And the trick is that PyTorch gives us all the functionality to make it happen.", "tokens": [400, 264, 4282, 307, 300, 9953, 51, 284, 339, 2709, 505, 439, 264, 14980, 281, 652, 309, 1051, 13], "temperature": 0.0, "avg_logprob": -0.08687177225321281, "compression_ratio": 1.757201646090535, "no_speech_prob": 9.368383871333208e-06}, {"id": 1981, "seek": 806932, "start": 8082.44, "end": 8088.24, "text": " So the key thing we have to do is to actually realize that our transforms, our augmentation", "tokens": [407, 264, 2141, 551, 321, 362, 281, 360, 307, 281, 767, 4325, 300, 527, 35592, 11, 527, 14501, 19631], "temperature": 0.0, "avg_logprob": -0.08687177225321281, "compression_ratio": 1.757201646090535, "no_speech_prob": 9.368383871333208e-06}, {"id": 1982, "seek": 806932, "start": 8088.24, "end": 8092.2, "text": " should happen after you create a batch.", "tokens": [820, 1051, 934, 291, 1884, 257, 15245, 13], "temperature": 0.0, "avg_logprob": -0.08687177225321281, "compression_ratio": 1.757201646090535, "no_speech_prob": 9.368383871333208e-06}, {"id": 1983, "seek": 806932, "start": 8092.2, "end": 8093.36, "text": " So here's what we do.", "tokens": [407, 510, 311, 437, 321, 360, 13], "temperature": 0.0, "avg_logprob": -0.08687177225321281, "compression_ratio": 1.757201646090535, "no_speech_prob": 9.368383871333208e-06}, {"id": 1984, "seek": 806932, "start": 8093.36, "end": 8097.88, "text": " For our augmentation, we don't create one random number, we create a mini batch of random", "tokens": [1171, 527, 14501, 19631, 11, 321, 500, 380, 1884, 472, 4974, 1230, 11, 321, 1884, 257, 8382, 15245, 295, 4974], "temperature": 0.0, "avg_logprob": -0.08687177225321281, "compression_ratio": 1.757201646090535, "no_speech_prob": 9.368383871333208e-06}, {"id": 1985, "seek": 809788, "start": 8097.88, "end": 8102.72, "text": " numbers, which is fine because PyTorch has the ability to generate batches of random", "tokens": [3547, 11, 597, 307, 2489, 570, 9953, 51, 284, 339, 575, 264, 3485, 281, 8460, 15245, 279, 295, 4974], "temperature": 0.0, "avg_logprob": -0.12261928717295328, "compression_ratio": 1.7084870848708487, "no_speech_prob": 1.1842304957099259e-05}, {"id": 1986, "seek": 809788, "start": 8102.72, "end": 8104.88, "text": " numbers on the GPU.", "tokens": [3547, 322, 264, 18407, 13], "temperature": 0.0, "avg_logprob": -0.12261928717295328, "compression_ratio": 1.7084870848708487, "no_speech_prob": 1.1842304957099259e-05}, {"id": 1987, "seek": 809788, "start": 8104.88, "end": 8110.2, "text": " And so then once we've got a mini batch of random numbers, then we just have to use that", "tokens": [400, 370, 550, 1564, 321, 600, 658, 257, 8382, 15245, 295, 4974, 3547, 11, 550, 321, 445, 362, 281, 764, 300], "temperature": 0.0, "avg_logprob": -0.12261928717295328, "compression_ratio": 1.7084870848708487, "no_speech_prob": 1.1842304957099259e-05}, {"id": 1988, "seek": 809788, "start": 8110.2, "end": 8114.400000000001, "text": " to generate a mini batch of augmented images.", "tokens": [281, 8460, 257, 8382, 15245, 295, 36155, 5267, 13], "temperature": 0.0, "avg_logprob": -0.12261928717295328, "compression_ratio": 1.7084870848708487, "no_speech_prob": 1.1842304957099259e-05}, {"id": 1989, "seek": 809788, "start": 8114.400000000001, "end": 8117.04, "text": " I won't kind of bore you with the details.", "tokens": [286, 1582, 380, 733, 295, 26002, 291, 365, 264, 4365, 13], "temperature": 0.0, "avg_logprob": -0.12261928717295328, "compression_ratio": 1.7084870848708487, "no_speech_prob": 1.1842304957099259e-05}, {"id": 1990, "seek": 809788, "start": 8117.04, "end": 8120.6, "text": " I find them very interesting details, but if you're not a computer vision person, maybe", "tokens": [286, 915, 552, 588, 1880, 4365, 11, 457, 498, 291, 434, 406, 257, 3820, 5201, 954, 11, 1310], "temperature": 0.0, "avg_logprob": -0.12261928717295328, "compression_ratio": 1.7084870848708487, "no_speech_prob": 1.1842304957099259e-05}, {"id": 1991, "seek": 809788, "start": 8120.6, "end": 8121.6, "text": " not.", "tokens": [406, 13], "temperature": 0.0, "avg_logprob": -0.12261928717295328, "compression_ratio": 1.7084870848708487, "no_speech_prob": 1.1842304957099259e-05}, {"id": 1992, "seek": 809788, "start": 8121.6, "end": 8126.8, "text": " But basically, we create something called an affine grid, which is just the coordinates", "tokens": [583, 1936, 11, 321, 1884, 746, 1219, 364, 2096, 533, 10748, 11, 597, 307, 445, 264, 21056], "temperature": 0.0, "avg_logprob": -0.12261928717295328, "compression_ratio": 1.7084870848708487, "no_speech_prob": 1.1842304957099259e-05}, {"id": 1993, "seek": 812680, "start": 8126.8, "end": 8128.64, "text": " of where is every pixel.", "tokens": [295, 689, 307, 633, 19261, 13], "temperature": 0.0, "avg_logprob": -0.11491586511785334, "compression_ratio": 1.8200836820083681, "no_speech_prob": 1.2804359357687645e-05}, {"id": 1994, "seek": 812680, "start": 8128.64, "end": 8133.72, "text": " So like literally, it's coordinates from minus one to one.", "tokens": [407, 411, 3736, 11, 309, 311, 21056, 490, 3175, 472, 281, 472, 13], "temperature": 0.0, "avg_logprob": -0.11491586511785334, "compression_ratio": 1.8200836820083681, "no_speech_prob": 1.2804359357687645e-05}, {"id": 1995, "seek": 812680, "start": 8133.72, "end": 8141.84, "text": " And then what we do is we multiply it by this matrix, which is called an affine transform.", "tokens": [400, 550, 437, 321, 360, 307, 321, 12972, 309, 538, 341, 8141, 11, 597, 307, 1219, 364, 2096, 533, 4088, 13], "temperature": 0.0, "avg_logprob": -0.11491586511785334, "compression_ratio": 1.8200836820083681, "no_speech_prob": 1.2804359357687645e-05}, {"id": 1996, "seek": 812680, "start": 8141.84, "end": 8144.4400000000005, "text": " And there are various kinds of affine transforms you can do.", "tokens": [400, 456, 366, 3683, 3685, 295, 2096, 533, 35592, 291, 393, 360, 13], "temperature": 0.0, "avg_logprob": -0.11491586511785334, "compression_ratio": 1.8200836820083681, "no_speech_prob": 1.2804359357687645e-05}, {"id": 1997, "seek": 812680, "start": 8144.4400000000005, "end": 8148.6, "text": " For example, you can do a rotation transform by using this particular matrix, but these", "tokens": [1171, 1365, 11, 291, 393, 360, 257, 12447, 4088, 538, 1228, 341, 1729, 8141, 11, 457, 613], "temperature": 0.0, "avg_logprob": -0.11491586511785334, "compression_ratio": 1.8200836820083681, "no_speech_prob": 1.2804359357687645e-05}, {"id": 1998, "seek": 812680, "start": 8148.6, "end": 8152.320000000001, "text": " are all just matrix multiplications.", "tokens": [366, 439, 445, 8141, 17596, 763, 13], "temperature": 0.0, "avg_logprob": -0.11491586511785334, "compression_ratio": 1.8200836820083681, "no_speech_prob": 1.2804359357687645e-05}, {"id": 1999, "seek": 812680, "start": 8152.320000000001, "end": 8155.92, "text": " And then you just, as you see here, you just do the matrix multiplication.", "tokens": [400, 550, 291, 445, 11, 382, 291, 536, 510, 11, 291, 445, 360, 264, 8141, 27290, 13], "temperature": 0.0, "avg_logprob": -0.11491586511785334, "compression_ratio": 1.8200836820083681, "no_speech_prob": 1.2804359357687645e-05}, {"id": 2000, "seek": 815592, "start": 8155.92, "end": 8158.24, "text": " And this is how you can rotate.", "tokens": [400, 341, 307, 577, 291, 393, 13121, 13], "temperature": 0.0, "avg_logprob": -0.22694247892533226, "compression_ratio": 1.5174129353233832, "no_speech_prob": 1.2029256140522193e-05}, {"id": 2001, "seek": 815592, "start": 8158.24, "end": 8164.04, "text": " So a rotation, believe it or not, is just a matrix multiplication by this tiny little", "tokens": [407, 257, 12447, 11, 1697, 309, 420, 406, 11, 307, 445, 257, 8141, 27290, 538, 341, 5870, 707], "temperature": 0.0, "avg_logprob": -0.22694247892533226, "compression_ratio": 1.5174129353233832, "no_speech_prob": 1.2029256140522193e-05}, {"id": 2002, "seek": 815592, "start": 8164.04, "end": 8166.32, "text": " matrix.", "tokens": [8141, 13], "temperature": 0.0, "avg_logprob": -0.22694247892533226, "compression_ratio": 1.5174129353233832, "no_speech_prob": 1.2029256140522193e-05}, {"id": 2003, "seek": 815592, "start": 8166.32, "end": 8172.4800000000005, "text": " If you do that, normally it's going to take you about 17 milliseconds.", "tokens": [759, 291, 360, 300, 11, 5646, 309, 311, 516, 281, 747, 291, 466, 3282, 34184, 13], "temperature": 0.0, "avg_logprob": -0.22694247892533226, "compression_ratio": 1.5174129353233832, "no_speech_prob": 1.2029256140522193e-05}, {"id": 2004, "seek": 815592, "start": 8172.4800000000005, "end": 8175.76, "text": " We can speed it up a bit with Ironsum.", "tokens": [492, 393, 3073, 309, 493, 257, 857, 365, 9151, 892, 449, 13], "temperature": 0.0, "avg_logprob": -0.22694247892533226, "compression_ratio": 1.5174129353233832, "no_speech_prob": 1.2029256140522193e-05}, {"id": 2005, "seek": 815592, "start": 8175.76, "end": 8181.6, "text": " Or we could speed it up a little bit more with Batch Matrix Multiply.", "tokens": [1610, 321, 727, 3073, 309, 493, 257, 707, 857, 544, 365, 363, 852, 36274, 31150, 356, 13], "temperature": 0.0, "avg_logprob": -0.22694247892533226, "compression_ratio": 1.5174129353233832, "no_speech_prob": 1.2029256140522193e-05}, {"id": 2006, "seek": 818160, "start": 8181.6, "end": 8186.04, "text": " Or we could stick the whole thing on the GPU and do it there.", "tokens": [1610, 321, 727, 2897, 264, 1379, 551, 322, 264, 18407, 293, 360, 309, 456, 13], "temperature": 0.0, "avg_logprob": -0.09299201785393481, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.2553482419170905e-06}, {"id": 2007, "seek": 818160, "start": 8186.04, "end": 8192.56, "text": " And that's going to go from 11 milliseconds to 81 microseconds.", "tokens": [400, 300, 311, 516, 281, 352, 490, 2975, 34184, 281, 30827, 3123, 37841, 28750, 13], "temperature": 0.0, "avg_logprob": -0.09299201785393481, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.2553482419170905e-06}, {"id": 2008, "seek": 818160, "start": 8192.56, "end": 8197.16, "text": " So if we can put things on the GPU, it's totally different.", "tokens": [407, 498, 321, 393, 829, 721, 322, 264, 18407, 11, 309, 311, 3879, 819, 13], "temperature": 0.0, "avg_logprob": -0.09299201785393481, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.2553482419170905e-06}, {"id": 2009, "seek": 818160, "start": 8197.16, "end": 8201.800000000001, "text": " And suddenly, we don't have to worry about how long our augmentation is taking.", "tokens": [400, 5800, 11, 321, 500, 380, 362, 281, 3292, 466, 577, 938, 527, 14501, 19631, 307, 1940, 13], "temperature": 0.0, "avg_logprob": -0.09299201785393481, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.2553482419170905e-06}, {"id": 2010, "seek": 818160, "start": 8201.800000000001, "end": 8204.92, "text": " So this is the thing that actually rotates the coordinates to say where the coordinates", "tokens": [407, 341, 307, 264, 551, 300, 767, 42133, 264, 21056, 281, 584, 689, 264, 21056], "temperature": 0.0, "avg_logprob": -0.09299201785393481, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.2553482419170905e-06}, {"id": 2011, "seek": 818160, "start": 8204.92, "end": 8205.92, "text": " are now.", "tokens": [366, 586, 13], "temperature": 0.0, "avg_logprob": -0.09299201785393481, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.2553482419170905e-06}, {"id": 2012, "seek": 818160, "start": 8205.92, "end": 8208.1, "text": " Then we have to do the interpolation.", "tokens": [1396, 321, 362, 281, 360, 264, 44902, 399, 13], "temperature": 0.0, "avg_logprob": -0.09299201785393481, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.2553482419170905e-06}, {"id": 2013, "seek": 820810, "start": 8208.1, "end": 8214.76, "text": " And believe it or not, PyTorch has an optimized batch-wise interpolation function.", "tokens": [400, 1697, 309, 420, 406, 11, 9953, 51, 284, 339, 575, 364, 26941, 15245, 12, 3711, 44902, 399, 2445, 13], "temperature": 0.0, "avg_logprob": -0.082876000522582, "compression_ratio": 1.633587786259542, "no_speech_prob": 1.1125137461931445e-05}, {"id": 2014, "seek": 820810, "start": 8214.76, "end": 8216.640000000001, "text": " It's called grid sample.", "tokens": [467, 311, 1219, 10748, 6889, 13], "temperature": 0.0, "avg_logprob": -0.082876000522582, "compression_ratio": 1.633587786259542, "no_speech_prob": 1.1125137461931445e-05}, {"id": 2015, "seek": 820810, "start": 8216.640000000001, "end": 8217.640000000001, "text": " And so here it is.", "tokens": [400, 370, 510, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.082876000522582, "compression_ratio": 1.633587786259542, "no_speech_prob": 1.1125137461931445e-05}, {"id": 2016, "seek": 820810, "start": 8217.640000000001, "end": 8218.640000000001, "text": " We run it.", "tokens": [492, 1190, 309, 13], "temperature": 0.0, "avg_logprob": -0.082876000522582, "compression_ratio": 1.633587786259542, "no_speech_prob": 1.1125137461931445e-05}, {"id": 2017, "seek": 820810, "start": 8218.640000000001, "end": 8219.640000000001, "text": " There it is.", "tokens": [821, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.082876000522582, "compression_ratio": 1.633587786259542, "no_speech_prob": 1.1125137461931445e-05}, {"id": 2018, "seek": 820810, "start": 8219.640000000001, "end": 8223.12, "text": " And not only do they have a grid sample, but this is actually even better than pillows", "tokens": [400, 406, 787, 360, 436, 362, 257, 10748, 6889, 11, 457, 341, 307, 767, 754, 1101, 813, 38630], "temperature": 0.0, "avg_logprob": -0.082876000522582, "compression_ratio": 1.633587786259542, "no_speech_prob": 1.1125137461931445e-05}, {"id": 2019, "seek": 820810, "start": 8223.12, "end": 8225.56, "text": " because you don't have to have these black edges.", "tokens": [570, 291, 500, 380, 362, 281, 362, 613, 2211, 8819, 13], "temperature": 0.0, "avg_logprob": -0.082876000522582, "compression_ratio": 1.633587786259542, "no_speech_prob": 1.1125137461931445e-05}, {"id": 2020, "seek": 820810, "start": 8225.56, "end": 8228.880000000001, "text": " You can say padding mode equals reflection.", "tokens": [509, 393, 584, 39562, 4391, 6915, 12914, 13], "temperature": 0.0, "avg_logprob": -0.082876000522582, "compression_ratio": 1.633587786259542, "no_speech_prob": 1.1125137461931445e-05}, {"id": 2021, "seek": 820810, "start": 8228.880000000001, "end": 8230.4, "text": " And the black edges are gone.", "tokens": [400, 264, 2211, 8819, 366, 2780, 13], "temperature": 0.0, "avg_logprob": -0.082876000522582, "compression_ratio": 1.633587786259542, "no_speech_prob": 1.1125137461931445e-05}, {"id": 2022, "seek": 820810, "start": 8230.4, "end": 8234.960000000001, "text": " It just reflects what was there, which most of the time is better.", "tokens": [467, 445, 18926, 437, 390, 456, 11, 597, 881, 295, 264, 565, 307, 1101, 13], "temperature": 0.0, "avg_logprob": -0.082876000522582, "compression_ratio": 1.633587786259542, "no_speech_prob": 1.1125137461931445e-05}, {"id": 2023, "seek": 823496, "start": 8234.96, "end": 8240.22, "text": " And so reflection padding is one of these little things we find definitely helps models.", "tokens": [400, 370, 12914, 39562, 307, 472, 295, 613, 707, 721, 321, 915, 2138, 3665, 5245, 13], "temperature": 0.0, "avg_logprob": -0.09227252889562536, "compression_ratio": 1.6182572614107884, "no_speech_prob": 3.187527227055398e-06}, {"id": 2024, "seek": 823496, "start": 8240.22, "end": 8245.0, "text": " So now we can put this all together into a rotate batch.", "tokens": [407, 586, 321, 393, 829, 341, 439, 1214, 666, 257, 13121, 15245, 13], "temperature": 0.0, "avg_logprob": -0.09227252889562536, "compression_ratio": 1.6182572614107884, "no_speech_prob": 3.187527227055398e-06}, {"id": 2025, "seek": 823496, "start": 8245.0, "end": 8247.32, "text": " We can do any kind of coordinate transform here.", "tokens": [492, 393, 360, 604, 733, 295, 15670, 4088, 510, 13], "temperature": 0.0, "avg_logprob": -0.09227252889562536, "compression_ratio": 1.6182572614107884, "no_speech_prob": 3.187527227055398e-06}, {"id": 2026, "seek": 823496, "start": 8247.32, "end": 8249.64, "text": " One of them is rotate batch.", "tokens": [1485, 295, 552, 307, 13121, 15245, 13], "temperature": 0.0, "avg_logprob": -0.09227252889562536, "compression_ratio": 1.6182572614107884, "no_speech_prob": 3.187527227055398e-06}, {"id": 2027, "seek": 823496, "start": 8249.64, "end": 8252.24, "text": " To do it a batch at a time.", "tokens": [1407, 360, 309, 257, 15245, 412, 257, 565, 13], "temperature": 0.0, "avg_logprob": -0.09227252889562536, "compression_ratio": 1.6182572614107884, "no_speech_prob": 3.187527227055398e-06}, {"id": 2028, "seek": 823496, "start": 8252.24, "end": 8258.779999999999, "text": " And yeah, as I say, it's dramatically faster.", "tokens": [400, 1338, 11, 382, 286, 584, 11, 309, 311, 17548, 4663, 13], "temperature": 0.0, "avg_logprob": -0.09227252889562536, "compression_ratio": 1.6182572614107884, "no_speech_prob": 3.187527227055398e-06}, {"id": 2029, "seek": 823496, "start": 8258.779999999999, "end": 8264.8, "text": " Or in fact, we can do it all in one step because PyTorch has a thing called affine grid that", "tokens": [1610, 294, 1186, 11, 321, 393, 360, 309, 439, 294, 472, 1823, 570, 9953, 51, 284, 339, 575, 257, 551, 1219, 2096, 533, 10748, 300], "temperature": 0.0, "avg_logprob": -0.09227252889562536, "compression_ratio": 1.6182572614107884, "no_speech_prob": 3.187527227055398e-06}, {"id": 2030, "seek": 826480, "start": 8264.8, "end": 8269.32, "text": " will actually do the multiplication as it creates the coordinate grid.", "tokens": [486, 767, 360, 264, 27290, 382, 309, 7829, 264, 15670, 10748, 13], "temperature": 0.0, "avg_logprob": -0.14554838938255832, "compression_ratio": 1.473170731707317, "no_speech_prob": 3.905396170011954e-06}, {"id": 2031, "seek": 826480, "start": 8269.32, "end": 8273.08, "text": " And this is where we get down to this incredibly fast speed.", "tokens": [400, 341, 307, 689, 321, 483, 760, 281, 341, 6252, 2370, 3073, 13], "temperature": 0.0, "avg_logprob": -0.14554838938255832, "compression_ratio": 1.473170731707317, "no_speech_prob": 3.905396170011954e-06}, {"id": 2032, "seek": 826480, "start": 8273.08, "end": 8280.48, "text": " So I feel like there's a whole big opportunity here.", "tokens": [407, 286, 841, 411, 456, 311, 257, 1379, 955, 2650, 510, 13], "temperature": 0.0, "avg_logprob": -0.14554838938255832, "compression_ratio": 1.473170731707317, "no_speech_prob": 3.905396170011954e-06}, {"id": 2033, "seek": 826480, "start": 8280.48, "end": 8289.199999999999, "text": " There are currently no kind of hackable anybody can write their own augmentation, run on the", "tokens": [821, 366, 4362, 572, 733, 295, 10339, 712, 4472, 393, 2464, 641, 1065, 14501, 19631, 11, 1190, 322, 264], "temperature": 0.0, "avg_logprob": -0.14554838938255832, "compression_ratio": 1.473170731707317, "no_speech_prob": 3.905396170011954e-06}, {"id": 2034, "seek": 826480, "start": 8289.199999999999, "end": 8291.5, "text": " GPU libraries out there.", "tokens": [18407, 15148, 484, 456, 13], "temperature": 0.0, "avg_logprob": -0.14554838938255832, "compression_ratio": 1.473170731707317, "no_speech_prob": 3.905396170011954e-06}, {"id": 2035, "seek": 829150, "start": 8291.5, "end": 8297.52, "text": " The entire fast AI dot vision library is written using PyTorch tensor operations.", "tokens": [440, 2302, 2370, 7318, 5893, 5201, 6405, 307, 3720, 1228, 9953, 51, 284, 339, 40863, 7705, 13], "temperature": 0.0, "avg_logprob": -0.1575046912911012, "compression_ratio": 1.5252100840336134, "no_speech_prob": 3.8446669350378215e-06}, {"id": 2036, "seek": 829150, "start": 8297.52, "end": 8300.04, "text": " We did it so that we could eventually do it this way.", "tokens": [492, 630, 309, 370, 300, 321, 727, 4728, 360, 309, 341, 636, 13], "temperature": 0.0, "avg_logprob": -0.1575046912911012, "compression_ratio": 1.5252100840336134, "no_speech_prob": 3.8446669350378215e-06}, {"id": 2037, "seek": 829150, "start": 8300.04, "end": 8304.72, "text": " But currently they all run on the CPU, on an image at a time.", "tokens": [583, 4362, 436, 439, 1190, 322, 264, 13199, 11, 322, 364, 3256, 412, 257, 565, 13], "temperature": 0.0, "avg_logprob": -0.1575046912911012, "compression_ratio": 1.5252100840336134, "no_speech_prob": 3.8446669350378215e-06}, {"id": 2038, "seek": 829150, "start": 8304.72, "end": 8307.32, "text": " But this is our template now.", "tokens": [583, 341, 307, 527, 12379, 586, 13], "temperature": 0.0, "avg_logprob": -0.1575046912911012, "compression_ratio": 1.5252100840336134, "no_speech_prob": 3.8446669350378215e-06}, {"id": 2039, "seek": 829150, "start": 8307.32, "end": 8310.8, "text": " So now you can do them a batch at a time.", "tokens": [407, 586, 291, 393, 360, 552, 257, 15245, 412, 257, 565, 13], "temperature": 0.0, "avg_logprob": -0.1575046912911012, "compression_ratio": 1.5252100840336134, "no_speech_prob": 3.8446669350378215e-06}, {"id": 2040, "seek": 829150, "start": 8310.8, "end": 8319.52, "text": " And so whatever domain you're working in, you can hopefully start to try out these randomized", "tokens": [400, 370, 2035, 9274, 291, 434, 1364, 294, 11, 291, 393, 4696, 722, 281, 853, 484, 613, 38513], "temperature": 0.0, "avg_logprob": -0.1575046912911012, "compression_ratio": 1.5252100840336134, "no_speech_prob": 3.8446669350378215e-06}, {"id": 2041, "seek": 831952, "start": 8319.52, "end": 8324.28, "text": " GPU batch-wise augmentations.", "tokens": [18407, 15245, 12, 3711, 29919, 763, 13], "temperature": 0.0, "avg_logprob": -0.17876793788029596, "compression_ratio": 1.6069868995633187, "no_speech_prob": 2.246833719254937e-05}, {"id": 2042, "seek": 831952, "start": 8324.28, "end": 8331.32, "text": " And next week, we're going to show you this magic data augmentation called mixup that's", "tokens": [400, 958, 1243, 11, 321, 434, 516, 281, 855, 291, 341, 5585, 1412, 14501, 19631, 1219, 2890, 1010, 300, 311], "temperature": 0.0, "avg_logprob": -0.17876793788029596, "compression_ratio": 1.6069868995633187, "no_speech_prob": 2.246833719254937e-05}, {"id": 2043, "seek": 831952, "start": 8331.32, "end": 8333.16, "text": " going to work on the GPU.", "tokens": [516, 281, 589, 322, 264, 18407, 13], "temperature": 0.0, "avg_logprob": -0.17876793788029596, "compression_ratio": 1.6069868995633187, "no_speech_prob": 2.246833719254937e-05}, {"id": 2044, "seek": 831952, "start": 8333.16, "end": 8338.76, "text": " It's going to work on every kind of domain that you can think of.", "tokens": [467, 311, 516, 281, 589, 322, 633, 733, 295, 9274, 300, 291, 393, 519, 295, 13], "temperature": 0.0, "avg_logprob": -0.17876793788029596, "compression_ratio": 1.6069868995633187, "no_speech_prob": 2.246833719254937e-05}, {"id": 2045, "seek": 831952, "start": 8338.76, "end": 8342.960000000001, "text": " And we'll possibly make most of these irrelevant because it's so good you possibly don't need", "tokens": [400, 321, 603, 6264, 652, 881, 295, 613, 28682, 570, 309, 311, 370, 665, 291, 6264, 500, 380, 643], "temperature": 0.0, "avg_logprob": -0.17876793788029596, "compression_ratio": 1.6069868995633187, "no_speech_prob": 2.246833719254937e-05}, {"id": 2046, "seek": 831952, "start": 8342.960000000001, "end": 8344.42, "text": " any others.", "tokens": [604, 2357, 13], "temperature": 0.0, "avg_logprob": -0.17876793788029596, "compression_ratio": 1.6069868995633187, "no_speech_prob": 2.246833719254937e-05}, {"id": 2047, "seek": 831952, "start": 8344.42, "end": 8347.4, "text": " So that and much more next week.", "tokens": [407, 300, 293, 709, 544, 958, 1243, 13], "temperature": 0.0, "avg_logprob": -0.17876793788029596, "compression_ratio": 1.6069868995633187, "no_speech_prob": 2.246833719254937e-05}, {"id": 2048, "seek": 831952, "start": 8347.4, "end": 8348.4, "text": " We'll see you then.", "tokens": [492, 603, 536, 291, 550, 13], "temperature": 0.0, "avg_logprob": -0.17876793788029596, "compression_ratio": 1.6069868995633187, "no_speech_prob": 2.246833719254937e-05}, {"id": 2049, "seek": 834840, "start": 8348.4, "end": 8349.4, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.709012204950506, "compression_ratio": 1.0, "no_speech_prob": 0.0008129567140713334}, {"id": 2050, "seek": 834940, "start": 8349.4, "end": 8378.619999999999, "text": " Please, please be seated.", "tokens": [2555, 11, 1767, 312, 20959, 13], "temperature": 1.0, "avg_logprob": -2.153065872192383, "compression_ratio": 0.8620689655172413, "no_speech_prob": 0.0010215641232207417}], "language": "en"}