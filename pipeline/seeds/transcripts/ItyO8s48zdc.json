{"text": " Okay. Hi everybody. And this is lesson 19 with extremely special guests Tanishq and Jono. Hi guys. How are you? Hello. Hey Jeremy. Good to be here. And it's New Year's Eve 2022, finishing off 2022 with a bang. Or at least a really cool lesson. And most of this lesson is going to be Tanishq and Jono. But I'm going to start with a quick update from the last lesson. What I wanted to show you is that Christopher Thomas on the forum, what I want to show you is that Christopher Thomas on the forum came up with a better winning result for our challenge, the Fashion MNIST challenge, which we are tracking here. And be sure to check out this forum thread for the latest results. And he found that he was able to get better results with dropout. Then Peter on the forum noticed I had a bug in my code. And the bug in my code for ResNets, actually I won't show you, I'll just tell you, is that in the res block I was not passing along the batch norm parameter. And as a result all the results I had were without batch norm. So then when I fixed batch norm and added dropout at Christopher's suggestion, I got better results still. And then Christopher came up with a better dropout and got better results still for 50 epochs. So let me show you the 93.2 for 5 epochs improvement. I won't show the change to batch norm because that's actually, that'll just be in the repo now. So the batch norm is already fixed. So I'm going to tell you about what dropout is, and then show that to you. So dropout is a simple but powerful idea where what we do with some particular probability, so here let's say a probability of 0.1, we randomly delete some activations. And when I say delete, what I actually mean is we change them to zero. So one easy way to do this is to create a binomial distribution object where the probabilities are 1 minus p, and then sample from that. And that will give you a 0.1 probability. So in this case, oh this is perfect, I have exactly one zero. Of course randomly that's not always going to be the case, but since I asked for 10 samples and 0.1 of the time should be 0, I so happen to get, yeah, exactly one of them. And so if we took a tensor like this and multiplied it by our activations, that will set about a tenth of them to 0, because multiplying by 0 gives you 0. So here's a dropout class. So you pass it and you say what probability of dropout there is, store it away. Now we're only going to do this during training time, so at evaluation time we're not going to randomly delete activations. But during training time we will create our binomial distribution object. We will pass in the 1 minus p probability, and then you say how many binomial trials do you want to run? So how like how many coin tosses or dice rolls or whatever each time, and so it's just one. And this is a cool little trick, if you put that one onto your accelerator, you know, GPU or NPS or whatever, it's actually going to create a binomial distribution that runs on the GPU. That's a really cool trick that not many people know about. And so then if I, yeah, sample, and I make a sample exactly the same size as my input, then that's going to give me a bunch of ones and zeros in a tensor, the same size as my activations. And then another cool trick is this is going to result in activations that are on average about 1 tenth smaller. So if I multiply by 1 over 1 minus 0.9, so multiply in this case by that, then that's going to scale up my, to undo that difference. Jeremy? Yeah? In the line above where you have props equals 1 minus p, should that be 1 minus self dot p? Oh, it absolutely should. Thank you very much, Jonah. Not that it matters too much, because, yeah, you can always just use nn.dropout at this point, and I don't use 0.1, which is why I didn't even see that. So as you can see, I'm not even bothering to export this, because I'm just showing how to repeat what's already available in PyTorch. So yeah, thanks Jonah, that's a good fix. Yeah, so if we're in evaluation mode, it's just going to return the original. If p equals 0, then these are all going to be just ones anyway. So we'll be multiplying by 1 divided by 1, so there's nothing to change. So with p of 0, it does nothing in effect. Yeah, and otherwise it's going to kind of zero out some of our activations. So we can, a pretty common place to add dropout is before your last linear layer. So that's what I've done here. So yeah, if I run the exact same epochs, I get 93.2, which is a very slight improvement. And so there is the reason for that, is that it's not going to be able to kind of memorize the data or the activations, you know, because there's a little bit of randomness. So it's going to force it to try to identify just the actual underlying differences. There's a lot of different ways of thinking about this. It's all, you can almost think of it as a bagging thing, a bit like a random forest, you know, it's each time it's giving a slightly different kind of random subset. Yeah, but that's what it does. There's a, I also added a drop 2d layer right at the start, which is not particularly common. I was just kind of like showing it. This is also how Christopher Thomas's idea tried it as well, although he didn't use dropout 2d. What's the difference between dropout 2d and dropout? So this is actually something I'd like you to do to implement yourself as an exercise, is to implement dropout 2d. The difference is that with dropout 2d, rather than using x.size as our tensor of ones and zeros, so in other words, potentially dropping out every single batch, every single channel, every single xy independently. Instead, we want to drop out an entire kind of grid area, all of the channels together. So if any of them are zero, then they're all zero. So you can look up the docs for dropout 2d for more details about exactly what that looks like. But yeah, so the exercise is to try and implement that from scratch, and come up with a way to test it. So like actually check that it's working correctly, because it's a very easy thing to think that it's working and then realize it's not. So then, yeah, Christopher Thomas actually found that if you remove this entirely, and only keep this, then you end up with a better results for 50 epochs. And so he's the first to break 95%. So I feel like we should insert some kind of animation or trumpet sounds or something at this point. I'm not sure if I'm clever enough to do that in the video editor, but I'll see. Okay. Hooray. Okay, so that's about it for me. Did you guys have any other things to add about dropout? How to understand it or what it does or interesting things? Oh, I did have one more thing before. I bet you go ahead if you've got anything to mention. So I was gonna ask just because I think the standard is to set it, like remove the dropout before you do inference. But I was wondering if there's anyone you know of or if it works to use it for some sort of test time augmentation. Oh, dude, thank you. Because I wrote a callback for that. Did you see this? Or are you just like, okay, so test time dropout callback. Nice. So yeah, before epoch, if you remember in learner, we put it into, you know, training mode, which actually what it does is it puts every individual layer into training mode. So that's why for the module itself, we can check whether that module is in training mode. So what we can actually do is after that's happened, we can then go back in this callback and apply a lambda that says if this is a dropout, then this is, yeah, then put it in training mode all the time, including a devaluation. And so then you can run it multiple times, just like we did for TTA, but with this callback. Now, that's very unlikely to give you a better result, because it's not kind of showing it different versions or anything like that, like TTA does that are kind of meant to be the same. But what it does do is it gives you some a sense of how confident it is. If it kind of has no idea, then that little bit of dropouts quite often going to lead to different predictions. So this is a way of kind of doing some kind of confidence measure. You'd have to calibrate it by kind of looking at things that it should be confident about and not confident about, and seeing how that dropout, test time dropout changes. But the basic idea, it's been used in medical models before. I wouldn't say it's totally popular, which is why I didn't even bother to show it being used, but I just want to add it here, because I think it's an interesting idea, and maybe could be more used than it is, or at least more studied than it has been. A lot of stuff that gets used in the medical world is less well known in the rest of the world, so maybe that's part of the problem. Cool, all right, so I will stop my sharing, and we're going to switch to Tanishq, who's going to do something much more exciting, which is to show that we are now at a point where we can do DDPM from scratch, or at least everything except the model. And so to remind you DDPM doesn't have the latent VAE thing, and we're not going to do conditional, so it's not going to be like, we're not going to get to tell it what to draw. And the UNet model itself is the one bit we're not going to do today. We're going to do that next lesson. But other than the UNet, it's going to be unconditional DDPM from scratch, so Tanishq, take it away. Okay, hi, welcome back. Sorry for the slight continuity problem, you may notice people look a little bit different, that's because we had some zoom issues. So we have a couple of days have passed, and we're back again, and then John recorded his bit before we do Tanishq's bit, and then we're going to post them in backwards. So hopefully there's not too many confusing continuity problems as a result, and it all goes smoothly. But it's time to turn it over to Tanishq to talk about DDPM. So we've reached the point where we have this mini AI framework, and I guess it's time to now, you know, start using it to build more, I guess, sophisticated models. And as we'll see here, we can start putting together a diffusion model from scratch using the mini AI library, and we'll see how it makes our life a lot easier. And also, it'd be very nice to see how, you know, the equations in the papers correspond to the code. So I have here, of course, the notebook that we are, that we'll be working from. The paper, which, you know, we have the diffusion model paper, the Noising Diffusion Probabilistic Models, which is the paper that was published in 2020. It was one of the original diffusion model papers that kind of set off the entire trend of diffusion models, and it's a good starting point as we delve into this topic further. And also, I have some diagrams and drawings that I will also show later on. But yeah, basically, let's just get started with the code here, and of course, the paper. So just to provide some context with this paper, you know, this paper was published from this group in UC Berkeley. I think a few of them have gone on now to work at Google, and this is a peer review. Yes, a big lab at UC Berkeley. And so diffusion models were actually originally introduced in 2015. But this paper in 2020, greatly simplified the diffusion models and made it a lot easier to work with. And, you know, got these amazing results, as you can see here, when they trained on faces, and in this case, CIFAR-10. And, you know, this really was very, kind of a big leap in terms of the progress of diffusion models. And so just to kind of briefly provide, I guess, kind of an overview. If I could just quickly step, just mention something, which is, you know, when we started this course, we, you know, we talked a bit about how perhaps the diffusion part of diffusion models is not actually all that. Everybody's been talking about diffusion models, because that's, particularly because that's the open source thing we have that works really well. But this week, actually a model that appears to be quite a lot better than stable diffusion was released, that doesn't use diffusion at all. Having said that, the basic ideas, like most of the stuff that Tanisha talks about today, will still appear in some kind of form, you know, but a lot of the details will be different. But strictly speaking, actually, yeah, I don't even know if we've got a word anymore for the kind of like modern generative model things we're doing. So in some ways, when we're talking about diffusion models, you should maybe replace it in your head with some other word, which is more general and includes this paper that Tanisha is looking at here. Iterative refinement, perhaps. That's what I've heard. Yeah, that's not bad, iterative refinement. I'm sure by the time people watch this video, probably, you know, somebody will have decided on something. We will keep our course website up to date. Yeah, yeah, this is the paper that Jeremy was talking about. And yeah, every week, there seems to be another state of the art model. So, but yeah, like Jeremy said, a lot of the principles are the same, but you know, the details can be different for each paper. But yeah. And just to, I just want to again, also, like Jeremy was saying, kind of zoom back a little bit and kind of talk about a little bit about what, you know, just kind of provide a review of what we're trying to do here. Right. So let me just, I guess, yeah. So with this task, we were trying to, in this case, I would try to do image generation. Of course, it could be other forms of generation, like text generation or whatever. And the general idea is that, of course, we have some, you know, data points. You know, in this case, we have some images of dogs and we want to produce more like these, the data points that were given. So in this case, maybe the dog image generation or something like this. And so the overall idea that a lot of these approaches take for image, you know, for some sort of generative modeling task is they have, they try to, not over there, I want to get over here. They try to, oops, what happened here? Maybe my, yeah. Yeah. So let me just end a bit. P of X, which is our, which is basically the sort of likelihood, what's going to happen here? Likelihood of data point X, of X. So let's say X is some image. Then P of X tells us, like, what is the probability that you would see that image in real life? And like, if you, we can take like a simpler example, which may be easier to think about of like a one-dimensional data point, like height, for example. And if we were to look at height, of course we know, like we have a data distribution, that's kind of a bell curve. And, you know, you have maybe some, you know, mean height, which is like something like five, nine, five, 10, yeah. I guess it'd be at five feet, nine, 10 inches or something like that, or five feet, nine inches, whatever. And then of course we have some, it was like, you have some more unlikely points, but that is still possible. Like for example, you have seven feet or you have something that's maybe not as likely, it was just like, you know, like three feet or something like this. So here's like. X-axis is height and the Y-axis is the probability of some random person you meet being that tall. Exactly. So, you know, you, you know, the, yeah, this is basically the probability. And so of course you have this sort of peak, which is where, you know, you have higher probability. And so those are the sorts of, you know, values that you would see more often. So this is, this is our, what we do call our P of X. And like the important part about P of X is that you can use this now to sample new values. If you know what P of X is, or if you have some sort of information about P of X. So for example, here, you can think of like, if you were to like, say, maybe have some, let's say you have some game and you have some human characters in the game, and you just want to randomly generate a height for this human character. You know, you could, you wouldn't want to of course select a random height between three and seven, that's kind of uniformly distributed. You would instead maybe want to, you would want to have the height dependent on this sort of function where you would more likely sample values, you know, in the middle and less likely sample these sorts of extreme points. So it's dependent on this function P of X. So having some information about P of X will allow you to sample more data points. And so that's kind of the overall goal of generative modeling is to get some information about P of X that then allows us to sample new points and, you know, create new generations. So that's kind of a high level kind of description of what we're trying to do when we're doing generative modeling. And of course, there are many different approaches. We, you know, we have our famous scans, which, you know, used to be the common method back in the day before diffusion models. You know, we have BAEs, which I think we'll probably talk a little bit more about that later as well. We'll be talking about both of those techniques later. Yeah. Yeah. And so there are many different other techniques. There are also some niche techniques that are out there as well. But of course, now the popular one is, are these diffusion models or, you know, as we talked about, maybe a better term might be, a hereditary refinement or whenever, you know, whatever the term ends to be. But yeah, so there are many different techniques and yeah. So let's just, so this is kind of the general diagram that shows what diffusion models are. And if we can look at the paper here, which let's pull up the paper. Yeah. And you see here, this is the sort of, they call it a directed graphical model. It's a very complicated term. It's just kind of showing what's going on in this, you know, in this process. And there's a lot of complicated math here, but we'll highlight some of the key variables and equations here. So basically the idea is that, okay, so let's see here. So we, so this is like, so this is an image that we want to generate, right? And so X0 is basically, you know, these are actually the samples that we want. So we want to, X0 is what we want to generate. And, you know, these would be, yeah, these are images. And we start out with pure noise. So that's what X, uppercase T, pure noise. And the whole idea is that we have two processes. We have this process where we're going from pure noise to our image. And we have this process within from our image to pure noise. So the process where we're going from our image to pure noise, this is called the forward process. Forward, sorry, my typing is, my handwriting is not so good in it. So hopefully it's clear enough. Let me know if it's not. So we have the forward process, which is mostly just used for our cleaning. Then we also have our reverse process. So this is the reverse process. Should I go right up here? Reverse process. So this is a bit of a summary, I guess, of what you and Waseem talked about in lesson 9b, right? Yes. And just, it's just mostly to highlight now what are the different variables as we look at the code and see, you know, the different variables in the code. Okay, so we'll be focusing today on the code, but the code will be referring to things by name, and those names won't make sense very much unless we see what they're used for in the math. Okay. I won't dive too much into the math. I just want to focus on these sorts of variables and equations that we see in the code. So basically, the general idea is that we do these in multiple different steps. We have here from time step 0 all the way to time steps, uppercase T. And so there's some fixed number of steps, but then we have this intermediate process where we're going from some particular time step. Yeah, we have this time step at lowercase t, which is noisy image. And yes, we're transitioning between these two different noisy images. So we have this, what is sometimes called the transition. We have this one here. This is like sometimes called the transition kernel, or yeah, whatever it is, it basically is just telling us, you know, how do we go from, you know, one, in this case, we're going from a less noisy image to a more noisy image, and then going backwards is going from a more noisy image to a less noisy image. So let's look at the equations. So the forward direction is driven easily to make it something more noisy, just add a bit more noise to it. And the reverse direction is incredibly difficult, which is to particularly to go from the far left to the far right is strictly speaking impossible because none of that person's face exists anymore. But somewhere in between, you could certainly go from something that's partially noisy to less noisy by a learned model. Exactly. And that's like what I'm going to write down right now in terms of, you know, in terms of, I guess, the symbols in the map. So yeah, basically, I'm just trying to pull out the, just to write down the equations here. So we have, let me zoom in a bit. On. Yeah, so we have our two, oops, let's see here. Okay. Two of XT, XT minus one. Or actually, you know what, maybe it's just better if I just snip. Yeah, just snip it from here. So the one that is going from our, the one that is going from our forward process is this equation here. So I'll just make that a little smaller for you guys. Just so right there. So that is going, and basically to explain, we have this sort of script, a little bit of a, maybe a little bit confusing notation here, but basically this is referring to a normal distribution or a Gaussian distribution. And this is just saying, okay, this is a Gaussian distribution that's describing this particular variable. So it's just saying, okay, you know, N is our normal or Gaussian distribution, and it's representing this variable X of T, or X, sorry, XT. And then we have here is the mean, and this is the variance. So just to again clarify, I think we've talked about this before as well, but like, you know, this is a, you know, this is of course a bad drawing of a Gaussian, but you know, our mean is just, oops, our mean is just, you know, this, you know, the middle point here is the mean, and the variance is kind of describes the sort of spread of the Gaussian distribution. So if you think about this a little further, you have this beta, which is one of the important variables that kind of describes the diffusion process, beta.T. So you'll see the beta T in the code, and basically beta T increases as T increases. So basically your beta T will be greater than your beta T minus one. So if you think about that a little bit more carefully, you can see that, okay, so at, you know, T minus one, at this time point here, and then you're going to the next time point, you're going to increase your beta T, so you're increasing the variance, but then you have this one minus beta T, and take the square root of that, and multiply it by XT minus one. So as your T is increasing, this term actually decreases. So your mean is actually decreasing, and you're getting less of the original image, because the original image is clearly part of XT minus one. So as you... Just to let you know, Kanishka, just to let you know, we can't see your pointer. So if you want to point at things, you would need to highlight them or something. Yeah, so I'll just, let's see, yeah. Or, yeah, basically, I mean, I wasn't particularly pointing at anything in specific, I was just saying that, yeah, basically, if we have our X of T here, as the time step increases, you know, you're getting less contribution from your XT minus one, and so that means your mean is going towards zero, and so you're going to have a mean of zero, and, you know, the variance keeps increasing, and basically, you just have a Gaussian distribution, and you lose any contribution from the original image as your time step increases. So that's why, when we start out from X of zero, and go all the way to our X of T here, this becomes pure noise. It's because we're doing this iterative process where we keep adding noise, we lose that contribution from the original image, and, you know, that leads to the image having pure noise at the end of the process. So just something I find useful here is to consider one extreme, which is to consider X1. So at X1, the mean is going to be root one minus beta T times X naught, and the reason that's interesting is X naught is the original image. So we're taking the original image, and at this point, one minus beta T will be pretty close to one. So at X1, we're going to have something that's the mean is very close to the image, and the variance will be very small, and so that's why we will have an image that just has a tiny bit of noise. Right, right. And then another thing that sometimes is easier to write out is, sometimes you can write out, in this case, you can write out Q of XT directly, because these are all independent in terms of, like, Q of XT is only dependent on XT minus one, and then XT minus one is only dependent on XT minus two, and you can, you can, it's just independent, like, all each of these steps are independent. So based on, you know, the different laws of probability, you can get your Q of XT in closed form. So yeah, that's what's shown here. Q of XT given the original image. So this is also another way of kind of seeing this more clearly, where you can see, you can see that. Anyway, so I'm going back here. Yeah, so this is another way to see here more directly. So this is, of course, our clean image, and this is our, just make that all clear, our noisy image. And so you can also see again, now alpha bar T is dependent on beta T. Basically, it's like one minus, like, the cumulative, this is, I mean, we'll see the code for it, I guess, so maybe, yes, yes. So it might be clear to see that this is alpha bar T or something like this. But basically, basically, the idea is that alpha bar T, alpha bar T is going to be, again, less, this is what is going to be less than alpha bar T minus one. So basically, alpha, this, this keeps decreasing, right? This decreases as, as time step increases. And on the other hand, this is going to be increasing as time step increases. So again, you can see the contribution from the original image decreases as time step increases, while the noise, you know, as shown by the Dariens is increasing while, you know, the time step is increased. Anyway, so that hopefully clarifies the forward process. And then the reverse process is this basically a neural network as we, as Jeremy had mentioned. And yeah, I'll just screenshot this. That's, oops, paste this. That's, yes, this is our, this is our reverse process. And basically, the idea is, well, this is a neural network. And this is also a neural network, neural network. And we learned it during the training of the model. But the nice thing about this particular diffusion model paper that made it so simple was actually, we completely ignored this and actually set it to constants just based on, you know, big data. We can't see what you're pointing at. So I think it's important to mention what this is here. This term here. So this one, we just kind of ignore it. And it's just a constant dependent, dependent, dependent, dependent on beta t. So you only have one neural network that you need to train, which is basically referring to this mean. And when the nice thing about this diffusion model process is that it also re-paraphrases mean into this easier form, where you do a lot of complicated math, which we'll not get into here. But basically, you get this kind of simplified training objective, where, let's see here. Yeah, you see the simplified training objective, you instead have this epsilon beta function. And let me just screenshot that again. screenshot. This is our loss function that we train, and we have this epsilon beta function. And you can see it's a very simple loss function, right? This is just a, let me just write this down. This is just an MSC loss. And we have this epsilon beta function here. That is our... I mean, to folks like me who are less mathy, it might not be obvious that it's a simple thing, because it looks quite complicated to me. But once we see it in code, it'll be simple. Yes, yes. Basically, you're just doing like, and you'll be, yeah, you'll see in code how simple it is. But this is like, just an MSC loss. So we've seen MSC loss before, but you'll see how, yeah, this is basically MSC. So the nice, so just to kind of take a step back again, what is this epsilon theta? Because this is like a new thing that like, seems a little bit confusing. Basically, epsilon, you can see here, basically, yeah, so this here is saying, this is actually equivalent to this equation here. These two are equivalent. This is just another way of saying that, because basically it's saying, that's x of t. So this is giving x of t, just in a different way. But epsilon is actually this normal distribution with a mean of zero and a variance of one. And then you have all these scaling terms that, you know, changes the mean to be the same as this equation that we have over here. So that's, this is our x of t. And so what epsilon is, it's actually the noise that we're adding to our image to make it into a noisy image. And what this neural network is doing is trying to predict that noise. So what this is actually doing is, this is actually a noise predictor. And it is predicting the noise in the image. And why is that important? Basically, the general idea is, like, if we were to think about our distribution of data, let's just think about it in a 2D space. Just here, you know, each data point here represents an image, and they're in this blob area, which represents a distribution. So this is in distribution. And this is out of the distribution. Out, well, it's distribution. And basically, the idea is that, okay, if we take an image, and we want to generate, yeah, we want to generate some random image. If we were to take a random data point, it would most likely be noisy images, right? So if we take some random data point, it would be more, you know, the way to generate a random data point, it's going to be just noise. But we want to, you know, keep adjusting this data point to make it look more like an image from your distribution. That's kind of the whole idea of this iterative process that we're doing in our diffusion model. So the way to get that information is actually to take images from your dataset, and actually add noise to it. So that's what we try to do in this process. So we have an image here, and we add noise to it. And then what we do is we try to play a neural network to predict the noise. And by predicting the noise and subtracting it out, we're going back to the distribution. So adding the noise takes you away from the distribution, and then predicting the noise brings you back to the distribution. So then if we know at any given point in this space, how much noise to remove, that tells you how to keep going towards the data distribution and get a point that lies within the distribution. So that's why we have noise prediction. And that's the importance of doing this noise prediction, is to be able to then do this iterative process where we can start out at a random point, which would be, for example, pure noise, and keep predicting and removing that noise, and walking towards the data distribution. Okay. So yeah, let's get started with the code. And so here, we of course have our imports, and we're going to load our dataset. We're going to work with our fashion MNIST dataset, which is what we've been working with for a while already. And this is just basically a similar code that we've seen from before, in terms of loading the dataset. And then we have our model. So we remove the noise from the image. So what our model is going to take in, is it's going to take in the previous image, the noisy image, and predict the noise. So the shapes of the input and the output are the same. They're going to be in the shape of an image. So what we use is, we use a UNet neural network, which takes in kind of an input image. And we do see your pointer now, by the way. So feel free to point at things. Yeah. So yeah, it takes in, you know, an input image. And in this case, UNet is the purpose, but they can also be used for any sort of image-to-image path, where we're going from an input image, and then outputting some other image of some sort. And we'll talk more about new architecture, which we haven't learned about yet, and we will be learning about in the next lesson. But broadly speaking, those gray arrows going from left to right, are a lot like ResNet, very much like ResNet skip connections. But they're being used in a different way. Everything else is stuff that we've seen before. So it's basically, we can pretend those don't exist. For now, it's a neural network that the output is the same size, or a similar size, to the input. And therefore you can use it to learn how to go from one image to a different image. Yeah. So that's what UNet is. And yeah, like Jeb has said, we'll talk about it more. The sort of UNets that are used for diffusion models also tend to have some additional tricks, which again, we'll talk about them later on as well. But yeah, we'll just, for the time being, we will just import a UNet from the diffusers library, which is the Hudinface library for diffusion models. So they have a UNet implementation, and we'll just be using that for now. And so, yeah, of course, strictly speaking, we're cheating at this point, because we're using something we haven't written from scratch, but we're only cheating temporarily, because we will be writing it from scratch. Yeah. And yeah, so and then of course, we're working with one channel images, our fashion MNIST images are one channel images, so we just have to specify that. And then of course, the channels of the different blocks within the UNet are also specified. And then let's go into the training process. So basically, the general idea, of course, is we want to train with this MSE loss. What we do is we select a random time step, and then we add noise to our image based on that time step. So of course, if we have a very high time step, we're adding a lot of noise. If we have a lower time step, then we're adding very little noise. So we're going to randomly choose a time step. And then yeah, we add the noise accordingly to the image. And then we pass it the noisy image to a model as well as the time step. And we are trying to predict the amount of noise that was in the image, and we predict it with the MSE loss. So we can see all the- I have some pictures of some of these variables I could share if that would be useful. So I have a version, so I think Tanishka is sharing notebook number 15. Is that right? And I've got here notebook number 17. And so I took Tanishka's notebook, and just as I was starting to understand it, I added, I like to draw pictures for myself to understand what's going on. So I took the things which are in Tanishka's class, and just put them into a cell. So I just copied and pasted them, although I replaced the Greek letters with English written out versions. And then I just plotted them to see what they look like. So in Tanishka's class, he has this thing called beta, which is just lin space. So that's just literally a line. So beta, there's going to be a thousand of them, and they're just going to be equally spaced from 0.01 to 0.02. And then there's something called sigma, which is the square root of that. So that's what sigma is going to look like. And then he's also got alpha bar, which is the cumulative product of 1 minus this. And that's what alpha bar looks like. So you can see here, as Tanishka was describing earlier, that when T is higher, that's this is T, the x-axis, beta is higher. And when T is higher, alpha bar is lower. So yeah, so if you want to remind yourself, so each of these things, beta, sigma, alpha bar, they're each, they've each got a thousand things in them. And this is the shape of those thousand things. So this is the amount of variance, I guess, added at each step. This is the square root of that. So it's the standard deviation added at each step. And then if we do 1 minus that, it's just, you know, the exact opposite. And then this is what happens if you multiply them all together up to that point. And the reason you do that is because if you add noise to something, you add noise to something, that you add noise to something, that you add noise to something, then you have to multiply together all that amount of noise to say how much noise you would get. So yeah, those are my pictures, if that's helpful. Yep, good to see the diagram, or see how it, the actual values, and how it changes over time. So yeah, let's see here. Sorry. Yeah, so like Jeremy was showing, we have our length space for our beta. In this case, we're using kind of more of the Greek letters. So you can see, you know, the Greek letters that, you know, we see in the paper, as well as now we have it here in the code as well. And you know, we have our length space from our minimum value to our maximum value. And we have some number of steps. So this is the number of time steps. So here we use 1000 time steps, but that can depend on the type of model that you're training. And that's one of the parameters of your model, or how to parameters of your model. And this is the callback you've got here. So yeah, this callback is going to be used to, to set up the data, I guess, so that you're going to be using this to add the noise, so that the models then got the, the data that we're trying to get it to learn to then denoise. Yeah. So the callback, of course, makes, makes a life, a life lot easier in terms of, yeah, setting up everything and, you know, still being able to use, I guess, the mini AI learner with the mini AI learner with maybe some of these more complicated, and maybe a little bit more unique training loops. So yeah, in this case, we're just able to use the, the, the callback, and in order to, you know, set up the, the data, the data, I guess, the batch that we are passing into our learner. I just wanted to mention, when you first did this, you, you wrote out the Greek letters in English, alpha and beta and so forth. And at least for my brain, I was finding it difficult to read because they were literally going off the edge of the page and I couldn't see it all at once. And so we've did a search in a place to replace it with the actual Greek letters. I still don't know how I feel about it. Like it's, I find it, I'm finding it easier to read because I can see it all at once. And I don't know if it's a scroll and my, I don't get as overwhelmed. But when I need to edit the code, I kind of just tend to copy and paste the Greek letters, which is why we use the actual word beta in the init parameter list, so that somebody using this never has to type a Greek letter. But I don't know, Jono or Tanishka, if you had any thoughts over the last week or two, since we made that change about whether you guys like having the Greek letters in there or, or not. I like it for this demo in particular. I don't know that I do this in my code, but because we're looking back and forth between the paper and the implementation here, I think it works in this case just fine. Yeah, I agree. I think it's good for like, yeah, when you're trying to study something or try to implement something, having the Greek letters is very useful to be able to, I guess, match the math more closely. And it's just easy just to pick the equation and put it into code, or, you know, vice versa, looking at the code and try to match to the equation. So I think for like, yeah, educational purpose, I tend to like, I guess the Greek letters. So yeah. Um, yeah, so, you know, we have our initialization, but we're just defining all these variables. We'll get to the predict in just a moment. But first, I just want to go over the before batch, where we're setting up our batch to pass into the model. So remember that the model is taking in our noisy image and the time step. So, and of course the target is the actual amount of noise that we are adding to the image. So basically we generate that noise. So that's what's... So epsilon is that target. So epsilon is the amount of noise, is not the amount of, is the actual noise. Yes. Epsilon is the actual noise that we're adding. So we're just adding the noise to the image. So epsilon is the actual noise that we're adding. And that's the target as well, because our model is a noise predicting model. It's predicting the noise in the image. And so our target should be the noise during, that we're adding to the image during training. So we have our epsilon and, you know, we're just generating it with this random function. It's, you know, the normal random normal distribution with a mean of zero variance of one. So that's what that's doing. And, you know, adding the appropriate shape and device. Then the batch that we get originally will contain the clean images, right? These are the original images from our dataset. So that's X zero. And then what we want to do is we want to add noise. So we have our alpha bar and we have a random time step that we select. And then we just simply follow that equation, which again, I'll just show in just a moment. That equation, you can make a tiny bit easier to read. I think if you were to double click on that first alpha bar underscore T, cut it and then paste it, sorry, in the XT equals torch dot square root, take the thing inside the square root, double click it and paste it over the top of the word torch. That would be a little bit easier to read. That's ingenious. **JASON LENGSTORF** And then do the same for the next one. **BEN HONG** Are we done? **JASON LENGSTORF** Those parentheses. **BEN HONG** Yeah. So basically, yeah. So yeah, I guess let's just pull up the equation. So let's see. So there's a section in the paper that has the nice algorithm. Let's see if I can find it. No, not here. It's I think earlier. Yes, training. So we're just following these same sort of training steps here, right? We select a clean image that we take from our dataset. This fancy kind of equation here is just saying take an image from your dataset, take a random time step between this range. And this is our epsilon that we're getting, just say get some epsilon value. And then we have our equation for X of T, right? This is the equation here. You can see that it is square root of alpha bar T X zero plus one square root of one minus alpha bar T times epsilon. So that's the same equation that we have right here, right? And then what we need to do is we need to pass this into our model. So we have X, T and T. So we set up our batch accordingly. So this is the two things that we pass into our model. And of course, we also have our target, which is our epsilon. And so that's what this is showing here. We pass in our X of T, as well as our T here, right? And we pass that into a model. The model is represented here as epsilon theta. And theta is often used to represent like this is a neural network with some parameters, and the parameters are represented by theta. So epsilon theta is just representing our noise predicting model. So this is our neural network. So we have passed in our X of T and our T into a neural network, and we are comparing it to our target here, which is the actual epsilon. And so that's what we're doing here. We have our batch where we have our X of T and T and epsilon. And then here we have our prediction function. And because we actually have, I guess in this case, we have two things that are in a tuple that we need to pass into our model. So we just kind of, you know, get those elements from our tuple with this. Yeah, we get the elements from the tuple, pass it into the model, and then Hugging Face has its own API in terms of getting the output. So you need to call dot sample in order to get the predictions from your model. So we just do that. And then we do, yeah, we have learned dot preds. And, you know, that's what's going to be used later than when we're trying to do our loss function calculation. So the, just so, I mean, it's just worth looking at that a little bit more since we haven't quite seen something like this before. And it's something which I'm not aware of any other framework that would let you do this, you know, literally replace how prediction works. So many AI's kind of really fun for this. So because you inherited from train CB, train CB has predict, you know, defined. And you've defined a new version. So it's not going to use the train CB version anymore. It's going to use your version. And what you're doing is instead of passing learn dot batch zero to the model, you've got a star in front of it. So the key thing is that star is going to, you know, and is, we know actually learn dot batch zero has two things in it. Because that learn dot batch that you showed at the end of the before batch method has two things in learn dot zero. So that star will unpack them and send each one of those as a separate argument. So our model needs to take two things, which the diffusers unit does take two things. So that's the main interesting point. And then something I find a bit awkward, honestly, about a lot of hacking face stuff, including diffusers is that generally their models don't just return the result, but they put it inside some name. And so that's what happens here. They put it in something inside something called sample. So that's why tanishq added dot sample at the end of the predict, because of this somewhat awkward thing, which hugging face like to do for some reason. But yeah, now that you know, I mean, this is something that people often get stuck on. I see on Kaggle and stuff like that. It's like, how on earth do I use these models? Because they take things in weird forms and they give back things with weird forms. Well, this is how, you know, if you inherit from train cb, you can change predict to do whatever you want, which I think is quite sweet. Yep. So yeah, that's the training loop. And then of course, you have your regular training loop that's implemented in many AI where you are going to have, yeah, you have your loss function calculation. And you get the predictions learned up threads. And of course, the target is our learned up batch one, which is our epsilon. So you know, we have those and we pass it into the loss function and calculates the loss function and does the back propagation. So I'll just go over that. We'll get back to the sampling in just a moment. But just to show the training loop. So most of this is copied from our, I think it's 14 augment notebook, the way you've got the tmax and the shed. The only thing I think you've added here is the ddpm callback, right? Yes, the ddpm callback. And you've changed the loss function. Yes. So basically, we have to initialize our ddpm callback with the appropriate arguments. So like the number of time steps and the minimum beta and maximum beta. And then yeah, and then of course, we're using an MSC loss as we talked about. It just becomes a regular training loop. And everything else is from before. Yeah, we have your scheduler, your progress bar, all of that we've seen before. I think it's really cool that we're using basically the same code to train a diffusion model as we've used to train a classifier just with, yeah, an extra callback. Yeah, yeah. Yeah, that's why I think callbacks are very powerful for being, you know, for allowing us to do such things. It's like pretty, you can take all this code and now we have a diffusion training loop. And we can just call it learn.fit. And yeah, you can see, you've got a nice training loop. Nice loss curve. We can save our model. On a torch, saving functionality to be able to save our model and we could load it in. But now that we have our trained model, then the question is, what can we do to use it to sample, you know, the dataset? So the basic idea, of course, was that, you know, we have, we have, like basically over here, right, we have, let's see here, okay, so we have a, basic idea is that we start out with a random data point. And of course, that's not going to be within the distribution at first. But now we've learned how to move from, you know, one point towards the data distribution. That's what our noise prediction, predicting function does. It basically tells you how, you know, in what direction and how much to, so the basic idea is that, yeah, I guess I'll start from maybe a new drawing here. Again, we have, the distribution is, and we have a random point. And we use our noise predicting model that we have trained to tell us which direction to move. So it tells us some direction. Or I guess, let's, Zach, actually, that's not an area, other area, like, okay, so here, like here, okay, so it tells us some direction to move. At first, that direction is not going to be, like, you cannot follow that direction all the way to get the correct data point. Because basically, what we were doing is we're trying to reverse the path that we were following when we were adding noise. So like, because we had originally a data point, and we kept adding noise to the data point, and maybe, you know, followed some path like this. And we want to reverse that path to get to. So our noise predicting function will give us an original direction, which, you know, would be, you know, some kind of, it's going to be kind of tangential to the actual path at that location. So what we would do is, you know, we would maybe follow that data point all the way towards the, you know, we're just going to keep following that data point. You know, we're going to try to predict the fully denoised image by following this, this noise prediction. But our fully denoised image is also not going to be a real image. So what we, so let me, I'll show an example of that over here in the paper on where they show this a little bit more carefully. Let's see here. So x0, yeah, so basically, you can see the different data, you can see the different data points here. It's not going to look anything like a real image. So you can see all these points, you know, it doesn't look anything. That we would do is, oh, we actually had a little bit of noise back to it. And we start, we have a new point, where then we could maybe estimate a bit, get a better estimate of which direction to move. Follow that all the way again, we follow a new point. And then I get add back a little bit of noise, you get a new estimate, you make a new estimate of, you know, this noise prediction and removing the noise, you know, follow that all again, completely, and add a little bit of noise again to the image, and merge onto the image. So that's kind of what we're showing here. It's a lot like SGD, with SGD, we don't take the gradient and jump all the way, we use a learning rate to go some of the way, because each of those estimates of where we want to go, you know, not that great, but we just do it slowly. Exactly. And at the end of the day, that's, yeah, that's what we're doing with this noise prediction, we are predicting the sort of gradient of this P of X. But of course, we need to keep making estimates of that gradient as we're progressing. So we have to keep evaluating our noise prediction function to get updated and better estimates of our gradient in order to finally converge onto our image. So that, and then you can see that here, you know, we have this, maybe this fully predicted denoise image, which at the beginning doesn't look anything like a real image. But then as we continue throughout the sampling process, we finally converge on something that looks like an actual image. Again, these are CIFAR-10 images, and it's still a little bit maybe unclear about how realistic these images, these very small images look, but that's kind of the general principle, I would say. And so that's what I can show in the code. This idea of, we're going to start out basically with a random image, right? And this random image is going to be like a pure noise image, and it's not going to be part of the data distribution. You know, this is not anything like a real image, it's just a rounded image. And so this is going to be our X, I guess, X uppercase T, right? That's what we start out with. And we want to go from X uppercase T all the way to X zero. So what we do is we go through each of the time steps and create, we have to put it in this sort of batch format, because that's what our neural network expects, so we just have to format it appropriately. And we'll get to Z in just a moment, I'll explain that in just a moment. But of course, we just, I could have similar alpha bar, beta bar, which is getting those variables that we... And we faked beta bar, because we couldn't figure out how to type it, so we used B bar instead. Yes, yes. So yeah, in, yeah, we were able to get beta bar to work, I guess. But anyway, at each step, what we're trying to do is to try to predict what direction we need to go, and that direction is given by our noise predicting model, right? So what we do is we pass in X of T, and our, our noise prediction is given by our noise prediction model. So what we do is we pass in X of T, and our current time step into our model, and we get this noise prediction, and that's the direction that we need to move in. So basically, we take X of T, we first attempt to completely remove the noise, right? That's what this is doing. That's what X zero hat is, that's completely removing the noise. And of course, as we said, that estimate at the beginning won't be very accurate. So we have some coefficients here where we have a coefficient of how much that we keep of this estimate of our denoise image, and how much of the originally noisy image we keep. And on top of that, we're going to add in some additional noise. So that's what we do here. We have X zero hat, and so, and we multiply by its coefficient, and we have X of T, we multiply by some coefficient, and we also add some additional noise. That's what Z is. It's just, it's basically a weighted average of the two, plus the noise. And then the whole idea is that, as our, as we get closer and closer to get closer and closer to a time step equals to zero, our estimate of X zero will be more and more accurate. So our X zero coefficient will get closer and closer as we're, you know, increasing our, you know, going through the process. And then our X T coefficient will get closer and closer to zero. So basically, we're going to be weighting more and more of the X zero hat estimate, and less and less of the X T as we're getting closer and closer to our final time step. And so at the end of the day, we will have our estimated generated image. So that's kind of an overview of the sampling process. So yeah. So yeah, as basically the way I implemented it here was I had the sample function that's part of our callback. And it will take in the model and the kind of shape that you want for your images that you're producing. So like, if you want to specify how many images you produce, you know, that's going to be part of your back size or whatever. And you'll just see that in a moment. But yeah, it's just part of the callback. So then we basically have our DDPM callback. And then we could just call the sample method of our DDPM callback. And we pass in our model. And then here you can see we're going to produce, for example, 16 images. And it just has to be a one channel image of shape 32 by 32. And we get our samples. And one thing I forgot to note was that I am collecting each of the time step, the estimate, the effects of t. So the predictions here, you can see that there are a thousand of them. We want the last one because that is our final generation. So we want the last one. And that's what we're saying. And it's sad actually. Yeah. And this is. You know, I mean, we've come a long way since DDPM. So this is like slower and less great than it could be. But considering that, except for Unet, we've done this from scratch, you know, literally from matrix multiplication. I think those are pretty decent. Yeah. And we're only trained for about five epochs. It took like, you know, maybe like four minutes to train this model, something like that. It's pretty quick. And this is what we get with very little training. And it's, yeah, pretty decent. You can see, of course, some clear shirts and shoes and pants and whatever else. Yeah. And you can see fabric and it's got texture and things have buckles. Yeah. You know, I mean, to compare, like we did generative modeling in the first time we did part two, back in the days when something called Vassusky and Gann was just new, which is actually created by the same guy that created PipeTorch or one of the two guys, Sumith. And we trained for hours and hours and hours and got things that I'm not sure were any better than this. So things have come a long way. Yeah. Yeah. And of course, Ben, yeah, so we can see Ben, like how this sampling progresses over the multiple time steps. So that's what I'm showing here because I collected during the sampling process, we are collecting at each time step what that estimate looks like. And you can kind of see here. And so this is an estimate out of like the noisy image over the time steps. Oops. And I guess I had to pause. Yeah, you can kind of see. But you'll notice that actually, so we actually, what we did is like, okay, so we selected an image, which is like the ninth image. So that's this image here. So we're looking at this image, particularly here. And we're going over. Yeah, we have a function here that's showing the I time step during the sampling process of that image. And we're just getting the images. And what we are doing is we're only showing basically from time step 800 to 1000. And here, we're just having it like where it's like, okay, we're looking at like, maybe every five steps, and we're going from 800 to 9. And this time, it'll make it visually easier to see the transition. But what you'll notice is I didn't start all the way from zero, I started from 800. And the reason we do that is because actually, between zero and 800, there's very little change in terms of like, it's just mostly a noisy image. And it turns out, but yeah, as I make a note of this here, it's actually a limitation of the noise schedule that is used in the original DDPM paper. And especially when applied to some of these smaller images, when we're working with images of like size 32 by 32, or whatever. And so there are some other papers like the improved DDPM paper that propose other sorts of noise schedules. And what I mean by noise schedule is basically how beta is defined, basically. So you know, we had this definition of torch dot lens space for our beta, but people have different ways of defining beta that lead to different properties. So you know, things like that, people have come up with different improvements. And those sorts of improvements work well when we're working with these smaller images. And basically, the point is if we are working from zero to 800, and it's just mostly just noise that entire time, we're not actually making full use of all those time steps. So it would be nice if we could actually make full use of those time steps, and actually have it do something during that time period. So there are some papers that examine this a little bit more carefully. And it would be kind of interesting for maybe some of you folks to also look at these papers and see if you can try to implement those sorts of models with this notebook as a starting point. And it should be a fairly simple change in terms of like noise schedule or something like that. So I actually think, you know, this is the start of our next journey, you know, which is our previous journey was, you know, going from being totally rubbish at fashion MNIST classification to being really good at it. I would say now we're like a little bit rubbish at doing fashion MNIST generation. And yeah, I think we should all now work from here over the next few lessons and so forth, and people, you know, trying things at home and all of us trying to make better and better generative models, you know, initially a fashion MNIST, and hopefully we'll get to the point where we're so good at that, that we're like, oh, this is too easy. And then we'll pick something harder. Yeah. And eventually that'll take us to stable diffusion and beyond, I imagine. Yeah. That's cool. I got some stuff to show you guys. If you're interested, I tried to better understand what was going on in Tunisia's notebook and tried doing it in a thousand different ways, and also see if I could just start to make it a bit faster. So that's what's in notebook 17, which I will share. So we've already seen the start of notebook 17. Well, one thing I did do is I did a little bit just drew a picture for myself, partly just to remind myself what the real ones look like, and they definitely have more detail than the samples that Tunisia was showing. But they're not, you know, they're just 28 by 28. I mean, they're not super amazing images, and they're just black and white. So even if we're fantastic at this, they're never going to look great, because we're using a small simple data set. As you always should, when you're doing any kind of R&D or experiments, you should always use a small and simple data set, up until you're so good at it that it's not challenging anymore. And even then, when you're exploring new ideas, you should explore them on small simple data sets first. Yeah. So after I drew the various things, what I like to do is, one thing I found challenging about working with your class Tunisia, because I find when stuff is inside a class, it's harder for me to explore. So I copied and pasted it, the before batch contents, and called it Noisify. And so one of the things that's fun to do that is it forces you to figure out what are the actual parameters to it. And so now that I, rather than putting it in the class, now that I've got all of my, you know, various things to do with, so these are the three parameters to the DDPM callbacks in it. So then these things we can calculate from that. So with those, then actually all we need is, yeah, what's the image that we're going to noisify, and then what's the what's the alpha bar, which I mean we can get from here, but it's a bit more general if you can pass in your alpha bar. So yeah, this is just copying and pasting from the class. But the nice thing is then I could experiment with it. So I can call Noisify on my first 25 images, and with a random T, each one's got a different random T, and so I can print out the T, and then I could actually use those as titles. And so this lets me, I thought this was quite nice. I might actually rerun this, because actually none of these look like anything, because as it turns out in this particular case, all of the T's are over 200. And as Tanishq mentioned, once you're over 200, it's almost impossible to see anything. So let me just rerun this, and see if we get a better, there we go, there's a better one. So with a T of 7, right, so remember T naught, T equals naught is the pure image. So T equals 7, it's just a slightly speckled-y image. And by 67, it's a pretty bad image. And by 94, it's very hard to see what it is at all. And by 293, maybe I can see a pair of pants? I'm not sure I can see anything. So yeah, by the way, there's a handy little, so we've, I think we've looked at map before in the course. There's an extended version of map in fastcore, and one of the nice things is you can pass it a string, and it basically just calls this format string, if you pass it a string rather than a function. And so this is going to stringify everything using its representations. This is how I got the titles out of it, just by the way. So yeah, I found this useful to be able to draw a picture of everything. And then I wanted to, yeah, look at what else can I do. So then I took, not so you won't be surprised to see, I took the sample method and turned that into a function. And I actually decided to pass everything that it needs, even, I mean, you could actually calculate pretty much all of these. But I thought since I've calculated them before, I'll just pass them in. So this is all copied and pasted from Dinesh's version. And so that means the callback now is tiny, right? Because before batch is just noisify, and the sample method just calls the sample function. Now what I did do is I decided just to, yeah, I wanted to try like as many different ways of doing this as possible. Partly as an exercise to help everybody like see all the different ways we can work with our framework, you know. So I decided not to inherit from train.cb, but instead I inherited from callback. So that means I can't use Dinesh's nifty trick of replacing predict. So instead I now need some way to pass in the two parts of the first element of the tuple, add separate things to the model, and return the sample. So how else could we do that? Well what we could do is we could actually inherit from unit2d model, which is what Dinesh used directly, unit2d model, and we can replace the model. And so we could replace specifically the forward function, that's the thing that gets called, and we could just call the original forward function, but rather than passing in x, we're passing star x. And rather than returning that, we'll return that dot sample. Okay so if we do that, then we don't need the train.cb anymore, and we don't need the predict. And so if you're not working with something as beautifully flexible as mini-ai, you can always do this, you know, to make, to replace your model so that it has the interface that you need it to have. So now again we did the same as Dinesh cad of create the callback, and now when we create the model, we'll use our unit class, which we just created. I wanted to see if I can make things faster. I tried dividing all of Dinesh's channels by two, and I found it worked just as well. One thing I noticed is that it uses group norm in the unit, which we have briefly learned about before. And in group norm, it splits the channels up into a certain number of groups, and I needed to make sure that those groups had more than one thing in, so you can actually pass in how many groups do you want to use in the normalization. So that's what this is for. You've got to be a little bit careful with these things. I didn't think of it at first, and I ended up, I think the num groups might have been 32, and I got an error saying you can't split 16 things into 32 groups. But it also made me realize actually, even in Dinesh, maybe you probably had 32 in the first with 32 groups, and so maybe the group norm wouldn't have been working as well. So they're little subtle things to look out for. So now that we're not using anything inherited from train CB, that means we either need to use train CB itself, or just use our train learner, and that everything else is the same as what Dinesh had. So then I wanted to look at the results of noisify here, and we've seen this trick before, which is we call fit, but don't call the training part of the fit, and use the single batch CB callback that we created way back when we first created learner, and now learn.batch will contain the tuple of tuples, which we can then use that trick to show. So I mean, obviously we'd expect it to look the same as before, but it's nice. I always like to draw pictures of everything all along the way, because it's very, very often. I mean, the first six to seven times I do anything, I do it wrong. So given that I know that, I might as well draw a picture to try and see how it's wrong, until it's fixed. It also tells me when it's not wrong. Isn't there a show batch function now that does something similar? Yes, you wrote that show image batch, didn't you? I can't quite remember. Yeah, we should remind ourselves how that worked. That's a good point. Thanks for reminding. Okay, so then I'll just go ahead and do the same thing that Dinesh did, but then the next thing I looked at was, I looked at the, you know, how am I going to make this train faster? I want a bigger, I want a higher learning rate, and I realized, oddly enough, the diffuser's code does not initialize anything at all. They use the defaults, which just goes to show, like even, you know, the experts at Hugging Face, they don't necessarily think, like, oh, maybe the PyTorch defaults aren't, you know, perfect for my model. Of course they're not, because they depend on what activation function do you have, and what res box do you have, and so forth. So I wasn't exactly sure how to initialize it. I, partly by chatting to Kat Crowley, who's the author of K-diffusion, and partly by looking at papers, and partly by thinking about my own experience, I ended up doing a few things. One is, I did do the thing that we talked about a while ago, which is to take every second convolutional layer and zero it out. You could do the same thing with using batch norm, which is what we tried, and since we've got quite a deep network, you know, that seemed like it might, you know, it helps basically by having the non-ID path in the res nets do nothing at first. So they can't cause problems. We haven't talked about orthogonalized weights before, and we probably won't, because you would need to take our computational linear algebra course to learn about that, which is a great course. Rachel Thomas did a fantastic job of it. I highly recommend it, but I don't want to make it a prerequisite. But Kat mentioned she thought that using orthogonal weights for the downsamplers was a good idea. Then for the up blocks, they also set the second comms to zero, and something Kat mentioned she found useful, which is also from, I think it's from the Darawal Google paper, is to also zero out the weights of basically the very last layer. So it's going to start by predicting zero as the noise, which is, you know, something that can't hurt. So that's how I initialized the weights. So call init ddpm on my model. Something that I found that a huge difference is I replaced the normal atom optimizer with one that has an epsilon of 1e neg 5. The default, I think, is 1e neg 8. And so to remind you, this is when we divide by the kind of exponentially weighted moving average of the squared gradients. When we divide by that, if that's a very, very small number, then it makes the effective learning rate huge. And so we add this to it to make it not too huge. And it's nearly always a good idea to make this bigger than the default. I don't know why the default is so small. And I found until I did this, anytime I tried to use a reasonably large learning rate, somewhere around the middle of the one cycle training, it would explode. So that makes a big difference. So this way, yeah, I could train, I could get 0.016 after five epochs. And then sampling, so it looks all pretty similar. We got some pretty nice textures, I think. So then I was thinking, how do I get faster? So one way we can make it faster is we can take advantage of something called mixed precision. So currently, we're using 32-bit floating point values. That's the defaults, and also known as single precision. And GPUs are pretty fast at doing 32-bit floating point values, but they're much, much, much, much faster at doing 16-bit floating point values. So 16-bit floating point values aren't able to represent a very, you know, wide range of numbers, or much precision at the difference between numbers. And so they're quite difficult to use. But if you can, you'll get a huge benefit, because modern GPUs, modern NVIDIA GPUs specifically, have special units that do matrix multiplies of 16-bit values extremely quickly. You can't just cast everything to 16-bit, because then you, there's not enough precision to calculate gradients and stuff properly. So we have to use something called mixed precision. Depending on how enthusiastic I'm feeling, I guess we ought to do this from scratch as well. We'll see. We do have an implementation from scratch, because we actually implemented this before NVIDIA implemented it in an earlier version of fast.ai. Anyway, we'll see. So basically the idea is that we use 32-bit for things where we need 32-bit, and we use 16-bit for things where we use 16-bit. So that's what we're going to do, is we're going to use this mixed precision. But for now we're going to use NVIDIA's, you know, semi-automatic or fairly automatic code to do that for us. Actually we had a slight change of plan at this point, when we realized this lesson was going to be over three hours in length, and we should actually split it into two. So we're going to wrap up this lesson here, and we're going to come back and implement this mixed precision thing in lesson 20. So we'll see you then.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.8, "text": " Okay. Hi everybody. And this is lesson 19 with extremely special guests Tanishq and", "tokens": [50364, 1033, 13, 2421, 2201, 13, 400, 341, 307, 6898, 1294, 365, 4664, 2121, 9804, 314, 7524, 80, 293, 50754], "temperature": 0.0, "avg_logprob": -0.3422339630126953, "compression_ratio": 1.3979057591623036, "no_speech_prob": 0.013208186253905296}, {"id": 1, "seek": 0, "start": 7.8, "end": 17.28, "text": " Jono. Hi guys. How are you? Hello. Hey Jeremy. Good to be here. And it's New Year's Eve 2022,", "tokens": [50754, 7745, 78, 13, 2421, 1074, 13, 1012, 366, 291, 30, 2425, 13, 1911, 17809, 13, 2205, 281, 312, 510, 13, 400, 309, 311, 1873, 10289, 311, 15544, 20229, 11, 51228], "temperature": 0.0, "avg_logprob": -0.3422339630126953, "compression_ratio": 1.3979057591623036, "no_speech_prob": 0.013208186253905296}, {"id": 2, "seek": 0, "start": 17.28, "end": 27.84, "text": " finishing off 2022 with a bang. Or at least a really cool lesson. And most of this lesson", "tokens": [51228, 12693, 766, 20229, 365, 257, 8550, 13, 1610, 412, 1935, 257, 534, 1627, 6898, 13, 400, 881, 295, 341, 6898, 51756], "temperature": 0.0, "avg_logprob": -0.3422339630126953, "compression_ratio": 1.3979057591623036, "no_speech_prob": 0.013208186253905296}, {"id": 3, "seek": 2784, "start": 27.84, "end": 34.519999999999996, "text": " is going to be Tanishq and Jono. But I'm going to start with a quick update from the last lesson.", "tokens": [50364, 307, 516, 281, 312, 314, 7524, 80, 293, 7745, 78, 13, 583, 286, 478, 516, 281, 722, 365, 257, 1702, 5623, 490, 264, 1036, 6898, 13, 50698], "temperature": 0.0, "avg_logprob": -0.26519507282185106, "compression_ratio": 1.4360902255639099, "no_speech_prob": 0.019694121554493904}, {"id": 4, "seek": 2784, "start": 34.519999999999996, "end": 49.6, "text": " What I wanted to show you is that Christopher Thomas on the forum, what I want to show you is", "tokens": [50698, 708, 286, 1415, 281, 855, 291, 307, 300, 20649, 8500, 322, 264, 17542, 11, 437, 286, 528, 281, 855, 291, 307, 51452], "temperature": 0.0, "avg_logprob": -0.26519507282185106, "compression_ratio": 1.4360902255639099, "no_speech_prob": 0.019694121554493904}, {"id": 5, "seek": 4960, "start": 49.6, "end": 59.0, "text": " that Christopher Thomas on the forum came up with a better winning result for our challenge,", "tokens": [50364, 300, 20649, 8500, 322, 264, 17542, 1361, 493, 365, 257, 1101, 8224, 1874, 337, 527, 3430, 11, 50834], "temperature": 0.0, "avg_logprob": -0.28327281891353545, "compression_ratio": 1.5402298850574712, "no_speech_prob": 0.023313695564866066}, {"id": 6, "seek": 4960, "start": 59.0, "end": 67.8, "text": " the Fashion MNIST challenge, which we are tracking here. And be sure to check out this", "tokens": [50834, 264, 32782, 376, 45, 19756, 3430, 11, 597, 321, 366, 11603, 510, 13, 400, 312, 988, 281, 1520, 484, 341, 51274], "temperature": 0.0, "avg_logprob": -0.28327281891353545, "compression_ratio": 1.5402298850574712, "no_speech_prob": 0.023313695564866066}, {"id": 7, "seek": 4960, "start": 67.8, "end": 75.32, "text": " forum thread for the latest results. And he found that he was able to get better results", "tokens": [51274, 17542, 7207, 337, 264, 6792, 3542, 13, 400, 415, 1352, 300, 415, 390, 1075, 281, 483, 1101, 3542, 51650], "temperature": 0.0, "avg_logprob": -0.28327281891353545, "compression_ratio": 1.5402298850574712, "no_speech_prob": 0.023313695564866066}, {"id": 8, "seek": 7532, "start": 75.32, "end": 84.96, "text": " with dropout. Then Peter on the forum noticed I had a bug in my code. And the bug in my", "tokens": [50364, 365, 3270, 346, 13, 1396, 6508, 322, 264, 17542, 5694, 286, 632, 257, 7426, 294, 452, 3089, 13, 400, 264, 7426, 294, 452, 50846], "temperature": 0.0, "avg_logprob": -0.24932479858398438, "compression_ratio": 1.611353711790393, "no_speech_prob": 0.0007672885549254715}, {"id": 9, "seek": 7532, "start": 84.96, "end": 91.44, "text": " code for ResNets, actually I won't show you, I'll just tell you, is that in the res block", "tokens": [50846, 3089, 337, 5015, 45, 1385, 11, 767, 286, 1582, 380, 855, 291, 11, 286, 603, 445, 980, 291, 11, 307, 300, 294, 264, 725, 3461, 51170], "temperature": 0.0, "avg_logprob": -0.24932479858398438, "compression_ratio": 1.611353711790393, "no_speech_prob": 0.0007672885549254715}, {"id": 10, "seek": 7532, "start": 91.44, "end": 96.67999999999999, "text": " I was not passing along the batch norm parameter. And as a result all the results I had were", "tokens": [51170, 286, 390, 406, 8437, 2051, 264, 15245, 2026, 13075, 13, 400, 382, 257, 1874, 439, 264, 3542, 286, 632, 645, 51432], "temperature": 0.0, "avg_logprob": -0.24932479858398438, "compression_ratio": 1.611353711790393, "no_speech_prob": 0.0007672885549254715}, {"id": 11, "seek": 7532, "start": 96.67999999999999, "end": 104.63999999999999, "text": " without batch norm. So then when I fixed batch norm and added dropout at Christopher's suggestion,", "tokens": [51432, 1553, 15245, 2026, 13, 407, 550, 562, 286, 6806, 15245, 2026, 293, 3869, 3270, 346, 412, 20649, 311, 16541, 11, 51830], "temperature": 0.0, "avg_logprob": -0.24932479858398438, "compression_ratio": 1.611353711790393, "no_speech_prob": 0.0007672885549254715}, {"id": 12, "seek": 10464, "start": 104.68, "end": 110.32, "text": " I got better results still. And then Christopher came up with a better dropout and got better", "tokens": [50366, 286, 658, 1101, 3542, 920, 13, 400, 550, 20649, 1361, 493, 365, 257, 1101, 3270, 346, 293, 658, 1101, 50648], "temperature": 0.0, "avg_logprob": -0.20846887429555258, "compression_ratio": 1.6216216216216217, "no_speech_prob": 0.0002269325195811689}, {"id": 13, "seek": 10464, "start": 110.32, "end": 120.48, "text": " results still for 50 epochs. So let me show you the 93.2 for 5 epochs improvement. I won't", "tokens": [50648, 3542, 920, 337, 2625, 30992, 28346, 13, 407, 718, 385, 855, 291, 264, 28876, 13, 17, 337, 1025, 30992, 28346, 10444, 13, 286, 1582, 380, 51156], "temperature": 0.0, "avg_logprob": -0.20846887429555258, "compression_ratio": 1.6216216216216217, "no_speech_prob": 0.0002269325195811689}, {"id": 14, "seek": 10464, "start": 120.48, "end": 125.32, "text": " show the change to batch norm because that's actually, that'll just be in the repo now.", "tokens": [51156, 855, 264, 1319, 281, 15245, 2026, 570, 300, 311, 767, 11, 300, 603, 445, 312, 294, 264, 49040, 586, 13, 51398], "temperature": 0.0, "avg_logprob": -0.20846887429555258, "compression_ratio": 1.6216216216216217, "no_speech_prob": 0.0002269325195811689}, {"id": 15, "seek": 10464, "start": 125.32, "end": 132.24, "text": " So the batch norm is already fixed. So I'm going to tell you about what dropout is, and", "tokens": [51398, 407, 264, 15245, 2026, 307, 1217, 6806, 13, 407, 286, 478, 516, 281, 980, 291, 466, 437, 3270, 346, 307, 11, 293, 51744], "temperature": 0.0, "avg_logprob": -0.20846887429555258, "compression_ratio": 1.6216216216216217, "no_speech_prob": 0.0002269325195811689}, {"id": 16, "seek": 13224, "start": 132.24, "end": 141.20000000000002, "text": " then show that to you. So dropout is a simple but powerful idea where what we do with some", "tokens": [50364, 550, 855, 300, 281, 291, 13, 407, 3270, 346, 307, 257, 2199, 457, 4005, 1558, 689, 437, 321, 360, 365, 512, 50812], "temperature": 0.0, "avg_logprob": -0.2767479938009511, "compression_ratio": 1.4640883977900552, "no_speech_prob": 0.0007321772282011807}, {"id": 17, "seek": 13224, "start": 141.20000000000002, "end": 148.24, "text": " particular probability, so here let's say a probability of 0.1, we randomly delete some", "tokens": [50812, 1729, 8482, 11, 370, 510, 718, 311, 584, 257, 8482, 295, 1958, 13, 16, 11, 321, 16979, 12097, 512, 51164], "temperature": 0.0, "avg_logprob": -0.2767479938009511, "compression_ratio": 1.4640883977900552, "no_speech_prob": 0.0007321772282011807}, {"id": 18, "seek": 13224, "start": 148.24, "end": 156.32000000000002, "text": " activations. And when I say delete, what I actually mean is we change them to zero. So", "tokens": [51164, 2430, 763, 13, 400, 562, 286, 584, 12097, 11, 437, 286, 767, 914, 307, 321, 1319, 552, 281, 4018, 13, 407, 51568], "temperature": 0.0, "avg_logprob": -0.2767479938009511, "compression_ratio": 1.4640883977900552, "no_speech_prob": 0.0007321772282011807}, {"id": 19, "seek": 15632, "start": 156.32, "end": 165.35999999999999, "text": " one easy way to do this is to create a binomial distribution object where the probabilities", "tokens": [50364, 472, 1858, 636, 281, 360, 341, 307, 281, 1884, 257, 5171, 47429, 7316, 2657, 689, 264, 33783, 50816], "temperature": 0.0, "avg_logprob": -0.2876671708148459, "compression_ratio": 1.530701754385965, "no_speech_prob": 0.0037068971432745457}, {"id": 20, "seek": 15632, "start": 165.35999999999999, "end": 176.72, "text": " are 1 minus p, and then sample from that. And that will give you a 0.1 probability.", "tokens": [50816, 366, 502, 3175, 280, 11, 293, 550, 6889, 490, 300, 13, 400, 300, 486, 976, 291, 257, 1958, 13, 16, 8482, 13, 51384], "temperature": 0.0, "avg_logprob": -0.2876671708148459, "compression_ratio": 1.530701754385965, "no_speech_prob": 0.0037068971432745457}, {"id": 21, "seek": 15632, "start": 176.72, "end": 181.24, "text": " So in this case, oh this is perfect, I have exactly one zero. Of course randomly that's", "tokens": [51384, 407, 294, 341, 1389, 11, 1954, 341, 307, 2176, 11, 286, 362, 2293, 472, 4018, 13, 2720, 1164, 16979, 300, 311, 51610], "temperature": 0.0, "avg_logprob": -0.2876671708148459, "compression_ratio": 1.530701754385965, "no_speech_prob": 0.0037068971432745457}, {"id": 22, "seek": 15632, "start": 181.24, "end": 186.07999999999998, "text": " not always going to be the case, but since I asked for 10 samples and 0.1 of the time", "tokens": [51610, 406, 1009, 516, 281, 312, 264, 1389, 11, 457, 1670, 286, 2351, 337, 1266, 10938, 293, 1958, 13, 16, 295, 264, 565, 51852], "temperature": 0.0, "avg_logprob": -0.2876671708148459, "compression_ratio": 1.530701754385965, "no_speech_prob": 0.0037068971432745457}, {"id": 23, "seek": 18608, "start": 186.16000000000003, "end": 192.92000000000002, "text": " should be 0, I so happen to get, yeah, exactly one of them. And so if we took a tensor like", "tokens": [50368, 820, 312, 1958, 11, 286, 370, 1051, 281, 483, 11, 1338, 11, 2293, 472, 295, 552, 13, 400, 370, 498, 321, 1890, 257, 40863, 411, 50706], "temperature": 0.0, "avg_logprob": -0.29665537884360865, "compression_ratio": 1.4806629834254144, "no_speech_prob": 0.00045121010043658316}, {"id": 24, "seek": 18608, "start": 192.92000000000002, "end": 203.44, "text": " this and multiplied it by our activations, that will set about a tenth of them to 0,", "tokens": [50706, 341, 293, 17207, 309, 538, 527, 2430, 763, 11, 300, 486, 992, 466, 257, 27269, 295, 552, 281, 1958, 11, 51232], "temperature": 0.0, "avg_logprob": -0.29665537884360865, "compression_ratio": 1.4806629834254144, "no_speech_prob": 0.00045121010043658316}, {"id": 25, "seek": 18608, "start": 203.44, "end": 209.32000000000002, "text": " because multiplying by 0 gives you 0. So here's a dropout class. So you pass it and you say", "tokens": [51232, 570, 30955, 538, 1958, 2709, 291, 1958, 13, 407, 510, 311, 257, 3270, 346, 1508, 13, 407, 291, 1320, 309, 293, 291, 584, 51526], "temperature": 0.0, "avg_logprob": -0.29665537884360865, "compression_ratio": 1.4806629834254144, "no_speech_prob": 0.00045121010043658316}, {"id": 26, "seek": 20932, "start": 209.35999999999999, "end": 216.76, "text": " what probability of dropout there is, store it away. Now we're only going to do this during", "tokens": [50366, 437, 8482, 295, 3270, 346, 456, 307, 11, 3531, 309, 1314, 13, 823, 321, 434, 787, 516, 281, 360, 341, 1830, 50736], "temperature": 0.0, "avg_logprob": -0.26854530970255536, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0791807621717453}, {"id": 27, "seek": 20932, "start": 216.76, "end": 223.44, "text": " training time, so at evaluation time we're not going to randomly delete activations.", "tokens": [50736, 3097, 565, 11, 370, 412, 13344, 565, 321, 434, 406, 516, 281, 16979, 12097, 2430, 763, 13, 51070], "temperature": 0.0, "avg_logprob": -0.26854530970255536, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0791807621717453}, {"id": 28, "seek": 20932, "start": 223.44, "end": 229.64, "text": " But during training time we will create our binomial distribution object. We will pass", "tokens": [51070, 583, 1830, 3097, 565, 321, 486, 1884, 527, 5171, 47429, 7316, 2657, 13, 492, 486, 1320, 51380], "temperature": 0.0, "avg_logprob": -0.26854530970255536, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0791807621717453}, {"id": 29, "seek": 20932, "start": 229.64, "end": 236.56, "text": " in the 1 minus p probability, and then you say how many binomial trials do you want to", "tokens": [51380, 294, 264, 502, 3175, 280, 8482, 11, 293, 550, 291, 584, 577, 867, 5171, 47429, 12450, 360, 291, 528, 281, 51726], "temperature": 0.0, "avg_logprob": -0.26854530970255536, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0791807621717453}, {"id": 30, "seek": 23656, "start": 236.56, "end": 241.28, "text": " run? So how like how many coin tosses or dice rolls or whatever each time, and so it's", "tokens": [50364, 1190, 30, 407, 577, 411, 577, 867, 11464, 14432, 279, 420, 10313, 15767, 420, 2035, 1184, 565, 11, 293, 370, 309, 311, 50600], "temperature": 0.0, "avg_logprob": -0.2902377128601074, "compression_ratio": 1.6433823529411764, "no_speech_prob": 0.041460346430540085}, {"id": 31, "seek": 23656, "start": 241.28, "end": 247.8, "text": " just one. And this is a cool little trick, if you put that one onto your accelerator,", "tokens": [50600, 445, 472, 13, 400, 341, 307, 257, 1627, 707, 4282, 11, 498, 291, 829, 300, 472, 3911, 428, 39889, 11, 50926], "temperature": 0.0, "avg_logprob": -0.2902377128601074, "compression_ratio": 1.6433823529411764, "no_speech_prob": 0.041460346430540085}, {"id": 32, "seek": 23656, "start": 247.8, "end": 252.52, "text": " you know, GPU or NPS or whatever, it's actually going to create a binomial distribution that", "tokens": [50926, 291, 458, 11, 18407, 420, 426, 6273, 420, 2035, 11, 309, 311, 767, 516, 281, 1884, 257, 5171, 47429, 7316, 300, 51162], "temperature": 0.0, "avg_logprob": -0.2902377128601074, "compression_ratio": 1.6433823529411764, "no_speech_prob": 0.041460346430540085}, {"id": 33, "seek": 23656, "start": 252.52, "end": 259.36, "text": " runs on the GPU. That's a really cool trick that not many people know about. And so then", "tokens": [51162, 6676, 322, 264, 18407, 13, 663, 311, 257, 534, 1627, 4282, 300, 406, 867, 561, 458, 466, 13, 400, 370, 550, 51504], "temperature": 0.0, "avg_logprob": -0.2902377128601074, "compression_ratio": 1.6433823529411764, "no_speech_prob": 0.041460346430540085}, {"id": 34, "seek": 23656, "start": 259.36, "end": 265.84000000000003, "text": " if I, yeah, sample, and I make a sample exactly the same size as my input, then that's going", "tokens": [51504, 498, 286, 11, 1338, 11, 6889, 11, 293, 286, 652, 257, 6889, 2293, 264, 912, 2744, 382, 452, 4846, 11, 550, 300, 311, 516, 51828], "temperature": 0.0, "avg_logprob": -0.2902377128601074, "compression_ratio": 1.6433823529411764, "no_speech_prob": 0.041460346430540085}, {"id": 35, "seek": 26584, "start": 265.91999999999996, "end": 272.32, "text": " to give me a bunch of ones and zeros in a tensor, the same size as my activations. And", "tokens": [50368, 281, 976, 385, 257, 3840, 295, 2306, 293, 35193, 294, 257, 40863, 11, 264, 912, 2744, 382, 452, 2430, 763, 13, 400, 50688], "temperature": 0.0, "avg_logprob": -0.2492180415562221, "compression_ratio": 1.4772727272727273, "no_speech_prob": 0.00011591915972530842}, {"id": 36, "seek": 26584, "start": 272.32, "end": 278.44, "text": " then another cool trick is this is going to result in activations that are on average", "tokens": [50688, 550, 1071, 1627, 4282, 307, 341, 307, 516, 281, 1874, 294, 2430, 763, 300, 366, 322, 4274, 50994], "temperature": 0.0, "avg_logprob": -0.2492180415562221, "compression_ratio": 1.4772727272727273, "no_speech_prob": 0.00011591915972530842}, {"id": 37, "seek": 26584, "start": 278.44, "end": 290.0, "text": " about 1 tenth smaller. So if I multiply by 1 over 1 minus 0.9, so multiply in this case", "tokens": [50994, 466, 502, 27269, 4356, 13, 407, 498, 286, 12972, 538, 502, 670, 502, 3175, 1958, 13, 24, 11, 370, 12972, 294, 341, 1389, 51572], "temperature": 0.0, "avg_logprob": -0.2492180415562221, "compression_ratio": 1.4772727272727273, "no_speech_prob": 0.00011591915972530842}, {"id": 38, "seek": 29000, "start": 290.0, "end": 305.2, "text": " by that, then that's going to scale up my, to undo that difference. Jeremy? Yeah? In", "tokens": [50364, 538, 300, 11, 550, 300, 311, 516, 281, 4373, 493, 452, 11, 281, 23779, 300, 2649, 13, 17809, 30, 865, 30, 682, 51124], "temperature": 0.0, "avg_logprob": -0.30250014721507756, "compression_ratio": 1.4277777777777778, "no_speech_prob": 0.4843350648880005}, {"id": 39, "seek": 29000, "start": 305.2, "end": 309.12, "text": " the line above where you have props equals 1 minus p, should that be 1 minus self dot", "tokens": [51124, 264, 1622, 3673, 689, 291, 362, 26173, 6915, 502, 3175, 280, 11, 820, 300, 312, 502, 3175, 2698, 5893, 51320], "temperature": 0.0, "avg_logprob": -0.30250014721507756, "compression_ratio": 1.4277777777777778, "no_speech_prob": 0.4843350648880005}, {"id": 40, "seek": 29000, "start": 309.12, "end": 317.76, "text": " p? Oh, it absolutely should. Thank you very much, Jonah. Not that it matters too much,", "tokens": [51320, 280, 30, 876, 11, 309, 3122, 820, 13, 1044, 291, 588, 709, 11, 42353, 13, 1726, 300, 309, 7001, 886, 709, 11, 51752], "temperature": 0.0, "avg_logprob": -0.30250014721507756, "compression_ratio": 1.4277777777777778, "no_speech_prob": 0.4843350648880005}, {"id": 41, "seek": 31776, "start": 317.76, "end": 323.28, "text": " because, yeah, you can always just use nn.dropout at this point, and I don't use 0.1, which", "tokens": [50364, 570, 11, 1338, 11, 291, 393, 1009, 445, 764, 297, 77, 13, 23332, 346, 412, 341, 935, 11, 293, 286, 500, 380, 764, 1958, 13, 16, 11, 597, 50640], "temperature": 0.0, "avg_logprob": -0.2556342902006926, "compression_ratio": 1.5807860262008733, "no_speech_prob": 0.004681766498833895}, {"id": 42, "seek": 31776, "start": 323.28, "end": 327.12, "text": " is why I didn't even see that. So as you can see, I'm not even bothering to export this,", "tokens": [50640, 307, 983, 286, 994, 380, 754, 536, 300, 13, 407, 382, 291, 393, 536, 11, 286, 478, 406, 754, 31432, 281, 10725, 341, 11, 50832], "temperature": 0.0, "avg_logprob": -0.2556342902006926, "compression_ratio": 1.5807860262008733, "no_speech_prob": 0.004681766498833895}, {"id": 43, "seek": 31776, "start": 327.12, "end": 336.36, "text": " because I'm just showing how to repeat what's already available in PyTorch. So yeah, thanks", "tokens": [50832, 570, 286, 478, 445, 4099, 577, 281, 7149, 437, 311, 1217, 2435, 294, 9953, 51, 284, 339, 13, 407, 1338, 11, 3231, 51294], "temperature": 0.0, "avg_logprob": -0.2556342902006926, "compression_ratio": 1.5807860262008733, "no_speech_prob": 0.004681766498833895}, {"id": 44, "seek": 31776, "start": 336.36, "end": 344.08, "text": " Jonah, that's a good fix. Yeah, so if we're in evaluation mode, it's just going to return", "tokens": [51294, 42353, 11, 300, 311, 257, 665, 3191, 13, 865, 11, 370, 498, 321, 434, 294, 13344, 4391, 11, 309, 311, 445, 516, 281, 2736, 51680], "temperature": 0.0, "avg_logprob": -0.2556342902006926, "compression_ratio": 1.5807860262008733, "no_speech_prob": 0.004681766498833895}, {"id": 45, "seek": 34408, "start": 344.08, "end": 353.64, "text": " the original. If p equals 0, then these are all going to be just ones anyway. So we'll", "tokens": [50364, 264, 3380, 13, 759, 280, 6915, 1958, 11, 550, 613, 366, 439, 516, 281, 312, 445, 2306, 4033, 13, 407, 321, 603, 50842], "temperature": 0.0, "avg_logprob": -0.24011933683144926, "compression_ratio": 1.5695652173913044, "no_speech_prob": 0.022285709157586098}, {"id": 46, "seek": 34408, "start": 353.64, "end": 359.68, "text": " be multiplying by 1 divided by 1, so there's nothing to change. So with p of 0, it does", "tokens": [50842, 312, 30955, 538, 502, 6666, 538, 502, 11, 370, 456, 311, 1825, 281, 1319, 13, 407, 365, 280, 295, 1958, 11, 309, 775, 51144], "temperature": 0.0, "avg_logprob": -0.24011933683144926, "compression_ratio": 1.5695652173913044, "no_speech_prob": 0.022285709157586098}, {"id": 47, "seek": 34408, "start": 359.68, "end": 365.28, "text": " nothing in effect. Yeah, and otherwise it's going to kind of zero out some of our activations.", "tokens": [51144, 1825, 294, 1802, 13, 865, 11, 293, 5911, 309, 311, 516, 281, 733, 295, 4018, 484, 512, 295, 527, 2430, 763, 13, 51424], "temperature": 0.0, "avg_logprob": -0.24011933683144926, "compression_ratio": 1.5695652173913044, "no_speech_prob": 0.022285709157586098}, {"id": 48, "seek": 34408, "start": 365.28, "end": 371.47999999999996, "text": " So we can, a pretty common place to add dropout is before your last linear layer. So that's", "tokens": [51424, 407, 321, 393, 11, 257, 1238, 2689, 1081, 281, 909, 3270, 346, 307, 949, 428, 1036, 8213, 4583, 13, 407, 300, 311, 51734], "temperature": 0.0, "avg_logprob": -0.24011933683144926, "compression_ratio": 1.5695652173913044, "no_speech_prob": 0.022285709157586098}, {"id": 49, "seek": 37148, "start": 371.48, "end": 381.56, "text": " what I've done here. So yeah, if I run the exact same epochs, I get 93.2, which is a", "tokens": [50364, 437, 286, 600, 1096, 510, 13, 407, 1338, 11, 498, 286, 1190, 264, 1900, 912, 30992, 28346, 11, 286, 483, 28876, 13, 17, 11, 597, 307, 257, 50868], "temperature": 0.0, "avg_logprob": -0.22372780348125257, "compression_ratio": 1.411764705882353, "no_speech_prob": 0.02128690667450428}, {"id": 50, "seek": 37148, "start": 381.56, "end": 388.48, "text": " very slight improvement. And so there is the reason for that, is that it's not going to", "tokens": [50868, 588, 4036, 10444, 13, 400, 370, 456, 307, 264, 1778, 337, 300, 11, 307, 300, 309, 311, 406, 516, 281, 51214], "temperature": 0.0, "avg_logprob": -0.22372780348125257, "compression_ratio": 1.411764705882353, "no_speech_prob": 0.02128690667450428}, {"id": 51, "seek": 37148, "start": 388.48, "end": 398.0, "text": " be able to kind of memorize the data or the activations, you know, because there's a little", "tokens": [51214, 312, 1075, 281, 733, 295, 27478, 264, 1412, 420, 264, 2430, 763, 11, 291, 458, 11, 570, 456, 311, 257, 707, 51690], "temperature": 0.0, "avg_logprob": -0.22372780348125257, "compression_ratio": 1.411764705882353, "no_speech_prob": 0.02128690667450428}, {"id": 52, "seek": 39800, "start": 398.0, "end": 402.84, "text": " bit of randomness. So it's going to force it to try to identify just the actual underlying", "tokens": [50364, 857, 295, 4974, 1287, 13, 407, 309, 311, 516, 281, 3464, 309, 281, 853, 281, 5876, 445, 264, 3539, 14217, 50606], "temperature": 0.0, "avg_logprob": -0.23815551170935997, "compression_ratio": 1.6870229007633588, "no_speech_prob": 0.054196253418922424}, {"id": 53, "seek": 39800, "start": 402.84, "end": 407.64, "text": " differences. There's a lot of different ways of thinking about this. It's all, you can", "tokens": [50606, 7300, 13, 821, 311, 257, 688, 295, 819, 2098, 295, 1953, 466, 341, 13, 467, 311, 439, 11, 291, 393, 50846], "temperature": 0.0, "avg_logprob": -0.23815551170935997, "compression_ratio": 1.6870229007633588, "no_speech_prob": 0.054196253418922424}, {"id": 54, "seek": 39800, "start": 407.64, "end": 411.04, "text": " almost think of it as a bagging thing, a bit like a random forest, you know, it's each", "tokens": [50846, 1920, 519, 295, 309, 382, 257, 3411, 3249, 551, 11, 257, 857, 411, 257, 4974, 6719, 11, 291, 458, 11, 309, 311, 1184, 51016], "temperature": 0.0, "avg_logprob": -0.23815551170935997, "compression_ratio": 1.6870229007633588, "no_speech_prob": 0.054196253418922424}, {"id": 55, "seek": 39800, "start": 411.04, "end": 420.36, "text": " time it's giving a slightly different kind of random subset. Yeah, but that's what it", "tokens": [51016, 565, 309, 311, 2902, 257, 4748, 819, 733, 295, 4974, 25993, 13, 865, 11, 457, 300, 311, 437, 309, 51482], "temperature": 0.0, "avg_logprob": -0.23815551170935997, "compression_ratio": 1.6870229007633588, "no_speech_prob": 0.054196253418922424}, {"id": 56, "seek": 39800, "start": 420.36, "end": 427.64, "text": " does. There's a, I also added a drop 2d layer right at the start, which is not particularly", "tokens": [51482, 775, 13, 821, 311, 257, 11, 286, 611, 3869, 257, 3270, 568, 67, 4583, 558, 412, 264, 722, 11, 597, 307, 406, 4098, 51846], "temperature": 0.0, "avg_logprob": -0.23815551170935997, "compression_ratio": 1.6870229007633588, "no_speech_prob": 0.054196253418922424}, {"id": 57, "seek": 42764, "start": 427.64, "end": 432.03999999999996, "text": " common. I was just kind of like showing it. This is also how Christopher Thomas's idea", "tokens": [50364, 2689, 13, 286, 390, 445, 733, 295, 411, 4099, 309, 13, 639, 307, 611, 577, 20649, 8500, 311, 1558, 50584], "temperature": 0.0, "avg_logprob": -0.23966466619613322, "compression_ratio": 1.6359447004608294, "no_speech_prob": 0.0025114528834819794}, {"id": 58, "seek": 42764, "start": 432.03999999999996, "end": 435.91999999999996, "text": " tried it as well, although he didn't use dropout 2d. What's the difference between dropout", "tokens": [50584, 3031, 309, 382, 731, 11, 4878, 415, 994, 380, 764, 3270, 346, 568, 67, 13, 708, 311, 264, 2649, 1296, 3270, 346, 50778], "temperature": 0.0, "avg_logprob": -0.23966466619613322, "compression_ratio": 1.6359447004608294, "no_speech_prob": 0.0025114528834819794}, {"id": 59, "seek": 42764, "start": 435.91999999999996, "end": 440.91999999999996, "text": " 2d and dropout? So this is actually something I'd like you to do to implement yourself as", "tokens": [50778, 568, 67, 293, 3270, 346, 30, 407, 341, 307, 767, 746, 286, 1116, 411, 291, 281, 360, 281, 4445, 1803, 382, 51028], "temperature": 0.0, "avg_logprob": -0.23966466619613322, "compression_ratio": 1.6359447004608294, "no_speech_prob": 0.0025114528834819794}, {"id": 60, "seek": 42764, "start": 440.91999999999996, "end": 448.24, "text": " an exercise, is to implement dropout 2d. The difference is that with dropout 2d, rather", "tokens": [51028, 364, 5380, 11, 307, 281, 4445, 3270, 346, 568, 67, 13, 440, 2649, 307, 300, 365, 3270, 346, 568, 67, 11, 2831, 51394], "temperature": 0.0, "avg_logprob": -0.23966466619613322, "compression_ratio": 1.6359447004608294, "no_speech_prob": 0.0025114528834819794}, {"id": 61, "seek": 44824, "start": 448.24, "end": 457.92, "text": " than using x.size as our tensor of ones and zeros, so in other words, potentially dropping", "tokens": [50364, 813, 1228, 2031, 13, 27553, 382, 527, 40863, 295, 2306, 293, 35193, 11, 370, 294, 661, 2283, 11, 7263, 13601, 50848], "temperature": 0.0, "avg_logprob": -0.31022749374161906, "compression_ratio": 1.576470588235294, "no_speech_prob": 0.08150005340576172}, {"id": 62, "seek": 44824, "start": 457.92, "end": 464.2, "text": " out every single batch, every single channel, every single xy independently. Instead, we", "tokens": [50848, 484, 633, 2167, 15245, 11, 633, 2167, 2269, 11, 633, 2167, 2031, 88, 21761, 13, 7156, 11, 321, 51162], "temperature": 0.0, "avg_logprob": -0.31022749374161906, "compression_ratio": 1.576470588235294, "no_speech_prob": 0.08150005340576172}, {"id": 63, "seek": 44824, "start": 464.2, "end": 474.12, "text": " want to drop out an entire kind of grid area, all of the channels together. So if any of", "tokens": [51162, 528, 281, 3270, 484, 364, 2302, 733, 295, 10748, 1859, 11, 439, 295, 264, 9235, 1214, 13, 407, 498, 604, 295, 51658], "temperature": 0.0, "avg_logprob": -0.31022749374161906, "compression_ratio": 1.576470588235294, "no_speech_prob": 0.08150005340576172}, {"id": 64, "seek": 47412, "start": 474.12, "end": 481.4, "text": " them are zero, then they're all zero. So you can look up the docs for dropout 2d for", "tokens": [50364, 552, 366, 4018, 11, 550, 436, 434, 439, 4018, 13, 407, 291, 393, 574, 493, 264, 45623, 337, 3270, 346, 568, 67, 337, 50728], "temperature": 0.0, "avg_logprob": -0.24916651007834445, "compression_ratio": 1.5767441860465117, "no_speech_prob": 0.17548561096191406}, {"id": 65, "seek": 47412, "start": 481.4, "end": 485.72, "text": " more details about exactly what that looks like. But yeah, so the exercise is to try", "tokens": [50728, 544, 4365, 466, 2293, 437, 300, 1542, 411, 13, 583, 1338, 11, 370, 264, 5380, 307, 281, 853, 50944], "temperature": 0.0, "avg_logprob": -0.24916651007834445, "compression_ratio": 1.5767441860465117, "no_speech_prob": 0.17548561096191406}, {"id": 66, "seek": 47412, "start": 485.72, "end": 493.52, "text": " and implement that from scratch, and come up with a way to test it. So like actually", "tokens": [50944, 293, 4445, 300, 490, 8459, 11, 293, 808, 493, 365, 257, 636, 281, 1500, 309, 13, 407, 411, 767, 51334], "temperature": 0.0, "avg_logprob": -0.24916651007834445, "compression_ratio": 1.5767441860465117, "no_speech_prob": 0.17548561096191406}, {"id": 67, "seek": 47412, "start": 493.52, "end": 497.72, "text": " check that it's working correctly, because it's a very easy thing to think that it's", "tokens": [51334, 1520, 300, 309, 311, 1364, 8944, 11, 570, 309, 311, 257, 588, 1858, 551, 281, 519, 300, 309, 311, 51544], "temperature": 0.0, "avg_logprob": -0.24916651007834445, "compression_ratio": 1.5767441860465117, "no_speech_prob": 0.17548561096191406}, {"id": 68, "seek": 49772, "start": 497.72, "end": 506.8, "text": " working and then realize it's not. So then, yeah, Christopher Thomas actually found that", "tokens": [50364, 1364, 293, 550, 4325, 309, 311, 406, 13, 407, 550, 11, 1338, 11, 20649, 8500, 767, 1352, 300, 50818], "temperature": 0.0, "avg_logprob": -0.2452855110168457, "compression_ratio": 1.398936170212766, "no_speech_prob": 0.09944604337215424}, {"id": 69, "seek": 49772, "start": 506.8, "end": 514.4, "text": " if you remove this entirely, and only keep this, then you end up with a better results", "tokens": [50818, 498, 291, 4159, 341, 7696, 11, 293, 787, 1066, 341, 11, 550, 291, 917, 493, 365, 257, 1101, 3542, 51198], "temperature": 0.0, "avg_logprob": -0.2452855110168457, "compression_ratio": 1.398936170212766, "no_speech_prob": 0.09944604337215424}, {"id": 70, "seek": 49772, "start": 514.4, "end": 523.48, "text": " for 50 epochs. And so he's the first to break 95%. So I feel like we should insert some", "tokens": [51198, 337, 2625, 30992, 28346, 13, 400, 370, 415, 311, 264, 700, 281, 1821, 13420, 6856, 407, 286, 841, 411, 321, 820, 8969, 512, 51652], "temperature": 0.0, "avg_logprob": -0.2452855110168457, "compression_ratio": 1.398936170212766, "no_speech_prob": 0.09944604337215424}, {"id": 71, "seek": 52348, "start": 523.48, "end": 528.6, "text": " kind of animation or trumpet sounds or something at this point. I'm not sure if I'm clever", "tokens": [50364, 733, 295, 9603, 420, 35160, 3263, 420, 746, 412, 341, 935, 13, 286, 478, 406, 988, 498, 286, 478, 13494, 50620], "temperature": 0.0, "avg_logprob": -0.27435329982212614, "compression_ratio": 1.5707964601769913, "no_speech_prob": 0.31397774815559387}, {"id": 72, "seek": 52348, "start": 528.6, "end": 539.28, "text": " enough to do that in the video editor, but I'll see. Okay. Hooray. Okay, so that's about", "tokens": [50620, 1547, 281, 360, 300, 294, 264, 960, 9839, 11, 457, 286, 603, 536, 13, 1033, 13, 3631, 284, 320, 13, 1033, 11, 370, 300, 311, 466, 51154], "temperature": 0.0, "avg_logprob": -0.27435329982212614, "compression_ratio": 1.5707964601769913, "no_speech_prob": 0.31397774815559387}, {"id": 73, "seek": 52348, "start": 539.28, "end": 544.16, "text": " it for me. Did you guys have any other things to add about dropout? How to understand it", "tokens": [51154, 309, 337, 385, 13, 2589, 291, 1074, 362, 604, 661, 721, 281, 909, 466, 3270, 346, 30, 1012, 281, 1223, 309, 51398], "temperature": 0.0, "avg_logprob": -0.27435329982212614, "compression_ratio": 1.5707964601769913, "no_speech_prob": 0.31397774815559387}, {"id": 74, "seek": 52348, "start": 544.16, "end": 548.52, "text": " or what it does or interesting things? Oh, I did have one more thing before. I bet you", "tokens": [51398, 420, 437, 309, 775, 420, 1880, 721, 30, 876, 11, 286, 630, 362, 472, 544, 551, 949, 13, 286, 778, 291, 51616], "temperature": 0.0, "avg_logprob": -0.27435329982212614, "compression_ratio": 1.5707964601769913, "no_speech_prob": 0.31397774815559387}, {"id": 75, "seek": 54852, "start": 548.52, "end": 555.0799999999999, "text": " go ahead if you've got anything to mention. So I was gonna ask just because I think the", "tokens": [50364, 352, 2286, 498, 291, 600, 658, 1340, 281, 2152, 13, 407, 286, 390, 799, 1029, 445, 570, 286, 519, 264, 50692], "temperature": 0.0, "avg_logprob": -0.3169305178583885, "compression_ratio": 1.5751072961373391, "no_speech_prob": 0.18239013850688934}, {"id": 76, "seek": 54852, "start": 555.0799999999999, "end": 559.4, "text": " standard is to set it, like remove the dropout before you do inference. But I was wondering", "tokens": [50692, 3832, 307, 281, 992, 309, 11, 411, 4159, 264, 3270, 346, 949, 291, 360, 38253, 13, 583, 286, 390, 6359, 50908], "temperature": 0.0, "avg_logprob": -0.3169305178583885, "compression_ratio": 1.5751072961373391, "no_speech_prob": 0.18239013850688934}, {"id": 77, "seek": 54852, "start": 559.4, "end": 564.92, "text": " if there's anyone you know of or if it works to use it for some sort of test time augmentation.", "tokens": [50908, 498, 456, 311, 2878, 291, 458, 295, 420, 498, 309, 1985, 281, 764, 309, 337, 512, 1333, 295, 1500, 565, 14501, 19631, 13, 51184], "temperature": 0.0, "avg_logprob": -0.3169305178583885, "compression_ratio": 1.5751072961373391, "no_speech_prob": 0.18239013850688934}, {"id": 78, "seek": 54852, "start": 564.92, "end": 570.24, "text": " Oh, dude, thank you. Because I wrote a callback for that. Did you see this? Or are you just", "tokens": [51184, 876, 11, 6449, 11, 1309, 291, 13, 1436, 286, 4114, 257, 818, 3207, 337, 300, 13, 2589, 291, 536, 341, 30, 1610, 366, 291, 445, 51450], "temperature": 0.0, "avg_logprob": -0.3169305178583885, "compression_ratio": 1.5751072961373391, "no_speech_prob": 0.18239013850688934}, {"id": 79, "seek": 57024, "start": 570.24, "end": 579.88, "text": " like, okay, so test time dropout callback. Nice. So yeah, before epoch, if you remember", "tokens": [50364, 411, 11, 1392, 11, 370, 1500, 565, 3270, 346, 818, 3207, 13, 5490, 13, 407, 1338, 11, 949, 30992, 339, 11, 498, 291, 1604, 50846], "temperature": 0.0, "avg_logprob": -0.31508692105611164, "compression_ratio": 1.502824858757062, "no_speech_prob": 0.0618688240647316}, {"id": 80, "seek": 57024, "start": 579.88, "end": 589.5600000000001, "text": " in learner, we put it into, you know, training mode, which actually what it does is it puts", "tokens": [50846, 294, 33347, 11, 321, 829, 309, 666, 11, 291, 458, 11, 3097, 4391, 11, 597, 767, 437, 309, 775, 307, 309, 8137, 51330], "temperature": 0.0, "avg_logprob": -0.31508692105611164, "compression_ratio": 1.502824858757062, "no_speech_prob": 0.0618688240647316}, {"id": 81, "seek": 57024, "start": 589.5600000000001, "end": 595.52, "text": " every individual layer into training mode. So that's why for the module itself, we can", "tokens": [51330, 633, 2609, 4583, 666, 3097, 4391, 13, 407, 300, 311, 983, 337, 264, 10088, 2564, 11, 321, 393, 51628], "temperature": 0.0, "avg_logprob": -0.31508692105611164, "compression_ratio": 1.502824858757062, "no_speech_prob": 0.0618688240647316}, {"id": 82, "seek": 59552, "start": 595.52, "end": 600.52, "text": " check whether that module is in training mode. So what we can actually do is after", "tokens": [50364, 1520, 1968, 300, 10088, 307, 294, 3097, 4391, 13, 407, 437, 321, 393, 767, 360, 307, 934, 50614], "temperature": 0.0, "avg_logprob": -0.2827909133013557, "compression_ratio": 1.6560509554140128, "no_speech_prob": 0.18007922172546387}, {"id": 83, "seek": 59552, "start": 600.52, "end": 608.0799999999999, "text": " that's happened, we can then go back in this callback and apply a lambda that says if this", "tokens": [50614, 300, 311, 2011, 11, 321, 393, 550, 352, 646, 294, 341, 818, 3207, 293, 3079, 257, 13607, 300, 1619, 498, 341, 50992], "temperature": 0.0, "avg_logprob": -0.2827909133013557, "compression_ratio": 1.6560509554140128, "no_speech_prob": 0.18007922172546387}, {"id": 84, "seek": 59552, "start": 608.0799999999999, "end": 624.56, "text": " is a dropout, then this is, yeah, then put it in training mode all the time, including", "tokens": [50992, 307, 257, 3270, 346, 11, 550, 341, 307, 11, 1338, 11, 550, 829, 309, 294, 3097, 4391, 439, 264, 565, 11, 3009, 51816], "temperature": 0.0, "avg_logprob": -0.2827909133013557, "compression_ratio": 1.6560509554140128, "no_speech_prob": 0.18007922172546387}, {"id": 85, "seek": 62456, "start": 624.5999999999999, "end": 632.8399999999999, "text": " a devaluation. And so then you can run it multiple times, just like we did for TTA,", "tokens": [50366, 257, 1905, 4929, 399, 13, 400, 370, 550, 291, 393, 1190, 309, 3866, 1413, 11, 445, 411, 321, 630, 337, 314, 8241, 11, 50778], "temperature": 0.0, "avg_logprob": -0.31226536887032647, "compression_ratio": 1.430939226519337, "no_speech_prob": 0.16883856058120728}, {"id": 86, "seek": 62456, "start": 632.8399999999999, "end": 644.1999999999999, "text": " but with this callback. Now, that's very unlikely to give you a better result, because it's", "tokens": [50778, 457, 365, 341, 818, 3207, 13, 823, 11, 300, 311, 588, 17518, 281, 976, 291, 257, 1101, 1874, 11, 570, 309, 311, 51346], "temperature": 0.0, "avg_logprob": -0.31226536887032647, "compression_ratio": 1.430939226519337, "no_speech_prob": 0.16883856058120728}, {"id": 87, "seek": 62456, "start": 644.1999999999999, "end": 649.9599999999999, "text": " not kind of showing it different versions or anything like that, like TTA does that", "tokens": [51346, 406, 733, 295, 4099, 309, 819, 9606, 420, 1340, 411, 300, 11, 411, 314, 8241, 775, 300, 51634], "temperature": 0.0, "avg_logprob": -0.31226536887032647, "compression_ratio": 1.430939226519337, "no_speech_prob": 0.16883856058120728}, {"id": 88, "seek": 64996, "start": 649.96, "end": 659.44, "text": " are kind of meant to be the same. But what it does do is it gives you some a sense of", "tokens": [50364, 366, 733, 295, 4140, 281, 312, 264, 912, 13, 583, 437, 309, 775, 360, 307, 309, 2709, 291, 512, 257, 2020, 295, 50838], "temperature": 0.0, "avg_logprob": -0.23519178118024553, "compression_ratio": 1.5321637426900585, "no_speech_prob": 0.19675315916538239}, {"id": 89, "seek": 64996, "start": 659.44, "end": 670.12, "text": " how confident it is. If it kind of has no idea, then that little bit of dropouts quite", "tokens": [50838, 577, 6679, 309, 307, 13, 759, 309, 733, 295, 575, 572, 1558, 11, 550, 300, 707, 857, 295, 3270, 7711, 1596, 51372], "temperature": 0.0, "avg_logprob": -0.23519178118024553, "compression_ratio": 1.5321637426900585, "no_speech_prob": 0.19675315916538239}, {"id": 90, "seek": 64996, "start": 670.12, "end": 676.08, "text": " often going to lead to different predictions. So this is a way of kind of doing some kind", "tokens": [51372, 2049, 516, 281, 1477, 281, 819, 21264, 13, 407, 341, 307, 257, 636, 295, 733, 295, 884, 512, 733, 51670], "temperature": 0.0, "avg_logprob": -0.23519178118024553, "compression_ratio": 1.5321637426900585, "no_speech_prob": 0.19675315916538239}, {"id": 91, "seek": 67608, "start": 676.08, "end": 681.48, "text": " of confidence measure. You'd have to calibrate it by kind of looking at things that it should", "tokens": [50364, 295, 6687, 3481, 13, 509, 1116, 362, 281, 21583, 4404, 309, 538, 733, 295, 1237, 412, 721, 300, 309, 820, 50634], "temperature": 0.0, "avg_logprob": -0.2474361295285432, "compression_ratio": 1.6177777777777778, "no_speech_prob": 0.15609021484851837}, {"id": 92, "seek": 67608, "start": 681.48, "end": 686.32, "text": " be confident about and not confident about, and seeing how that dropout, test time dropout", "tokens": [50634, 312, 6679, 466, 293, 406, 6679, 466, 11, 293, 2577, 577, 300, 3270, 346, 11, 1500, 565, 3270, 346, 50876], "temperature": 0.0, "avg_logprob": -0.2474361295285432, "compression_ratio": 1.6177777777777778, "no_speech_prob": 0.15609021484851837}, {"id": 93, "seek": 67608, "start": 686.32, "end": 695.8000000000001, "text": " changes. But the basic idea, it's been used in medical models before. I wouldn't say it's", "tokens": [50876, 2962, 13, 583, 264, 3875, 1558, 11, 309, 311, 668, 1143, 294, 4625, 5245, 949, 13, 286, 2759, 380, 584, 309, 311, 51350], "temperature": 0.0, "avg_logprob": -0.2474361295285432, "compression_ratio": 1.6177777777777778, "no_speech_prob": 0.15609021484851837}, {"id": 94, "seek": 67608, "start": 695.8000000000001, "end": 703.0, "text": " totally popular, which is why I didn't even bother to show it being used, but I just want", "tokens": [51350, 3879, 3743, 11, 597, 307, 983, 286, 994, 380, 754, 8677, 281, 855, 309, 885, 1143, 11, 457, 286, 445, 528, 51710], "temperature": 0.0, "avg_logprob": -0.2474361295285432, "compression_ratio": 1.6177777777777778, "no_speech_prob": 0.15609021484851837}, {"id": 95, "seek": 70300, "start": 703.0, "end": 710.96, "text": " to add it here, because I think it's an interesting idea, and maybe could be more used than it", "tokens": [50364, 281, 909, 309, 510, 11, 570, 286, 519, 309, 311, 364, 1880, 1558, 11, 293, 1310, 727, 312, 544, 1143, 813, 309, 50762], "temperature": 0.0, "avg_logprob": -0.3000190160045885, "compression_ratio": 1.6023391812865497, "no_speech_prob": 0.25373372435569763}, {"id": 96, "seek": 70300, "start": 710.96, "end": 717.16, "text": " is, or at least more studied than it has been. A lot of stuff that gets used in the medical", "tokens": [50762, 307, 11, 420, 412, 1935, 544, 9454, 813, 309, 575, 668, 13, 316, 688, 295, 1507, 300, 2170, 1143, 294, 264, 4625, 51072], "temperature": 0.0, "avg_logprob": -0.3000190160045885, "compression_ratio": 1.6023391812865497, "no_speech_prob": 0.25373372435569763}, {"id": 97, "seek": 70300, "start": 717.16, "end": 724.08, "text": " world is less well known in the rest of the world, so maybe that's part of the problem.", "tokens": [51072, 1002, 307, 1570, 731, 2570, 294, 264, 1472, 295, 264, 1002, 11, 370, 1310, 300, 311, 644, 295, 264, 1154, 13, 51418], "temperature": 0.0, "avg_logprob": -0.3000190160045885, "compression_ratio": 1.6023391812865497, "no_speech_prob": 0.25373372435569763}, {"id": 98, "seek": 72408, "start": 724.08, "end": 733.2800000000001, "text": " Cool, all right, so I will stop my sharing, and we're going to switch to Tanishq, who's", "tokens": [50364, 8561, 11, 439, 558, 11, 370, 286, 486, 1590, 452, 5414, 11, 293, 321, 434, 516, 281, 3679, 281, 314, 7524, 80, 11, 567, 311, 50824], "temperature": 0.0, "avg_logprob": -0.2665351403726114, "compression_ratio": 1.4456521739130435, "no_speech_prob": 0.0017821803921833634}, {"id": 99, "seek": 72408, "start": 733.2800000000001, "end": 737.8000000000001, "text": " going to do something much more exciting, which is to show that we are now at a point", "tokens": [50824, 516, 281, 360, 746, 709, 544, 4670, 11, 597, 307, 281, 855, 300, 321, 366, 586, 412, 257, 935, 51050], "temperature": 0.0, "avg_logprob": -0.2665351403726114, "compression_ratio": 1.4456521739130435, "no_speech_prob": 0.0017821803921833634}, {"id": 100, "seek": 72408, "start": 737.8000000000001, "end": 747.12, "text": " where we can do DDPM from scratch, or at least everything except the model. And so to remind", "tokens": [51050, 689, 321, 393, 360, 413, 11373, 44, 490, 8459, 11, 420, 412, 1935, 1203, 3993, 264, 2316, 13, 400, 370, 281, 4160, 51516], "temperature": 0.0, "avg_logprob": -0.2665351403726114, "compression_ratio": 1.4456521739130435, "no_speech_prob": 0.0017821803921833634}, {"id": 101, "seek": 74712, "start": 747.12, "end": 754.04, "text": " you DDPM doesn't have the latent VAE thing, and we're not going to do conditional, so", "tokens": [50364, 291, 413, 11373, 44, 1177, 380, 362, 264, 48994, 18527, 36, 551, 11, 293, 321, 434, 406, 516, 281, 360, 27708, 11, 370, 50710], "temperature": 0.0, "avg_logprob": -0.2811288954336432, "compression_ratio": 1.694267515923567, "no_speech_prob": 0.02635188214480877}, {"id": 102, "seek": 74712, "start": 754.04, "end": 760.96, "text": " it's not going to be like, we're not going to get to tell it what to draw. And the UNet", "tokens": [50710, 309, 311, 406, 516, 281, 312, 411, 11, 321, 434, 406, 516, 281, 483, 281, 980, 309, 437, 281, 2642, 13, 400, 264, 8229, 302, 51056], "temperature": 0.0, "avg_logprob": -0.2811288954336432, "compression_ratio": 1.694267515923567, "no_speech_prob": 0.02635188214480877}, {"id": 103, "seek": 74712, "start": 760.96, "end": 770.28, "text": " model itself is the one bit we're not going to do today. We're going to do that next lesson.", "tokens": [51056, 2316, 2564, 307, 264, 472, 857, 321, 434, 406, 516, 281, 360, 965, 13, 492, 434, 516, 281, 360, 300, 958, 6898, 13, 51522], "temperature": 0.0, "avg_logprob": -0.2811288954336432, "compression_ratio": 1.694267515923567, "no_speech_prob": 0.02635188214480877}, {"id": 104, "seek": 77028, "start": 770.28, "end": 777.4399999999999, "text": " But other than the UNet, it's going to be unconditional DDPM from scratch, so Tanishq,", "tokens": [50364, 583, 661, 813, 264, 8229, 302, 11, 309, 311, 516, 281, 312, 47916, 413, 11373, 44, 490, 8459, 11, 370, 314, 7524, 80, 11, 50722], "temperature": 0.0, "avg_logprob": -0.2926507618116296, "compression_ratio": 1.453781512605042, "no_speech_prob": 0.025176893919706345}, {"id": 105, "seek": 77028, "start": 777.4399999999999, "end": 785.8, "text": " take it away. Okay, hi, welcome back. Sorry for the slight continuity problem, you may", "tokens": [50722, 747, 309, 1314, 13, 1033, 11, 4879, 11, 2928, 646, 13, 4919, 337, 264, 4036, 23807, 1154, 11, 291, 815, 51140], "temperature": 0.0, "avg_logprob": -0.2926507618116296, "compression_ratio": 1.453781512605042, "no_speech_prob": 0.025176893919706345}, {"id": 106, "seek": 77028, "start": 785.8, "end": 790.16, "text": " notice people look a little bit different, that's because we had some zoom issues. So", "tokens": [51140, 3449, 561, 574, 257, 707, 857, 819, 11, 300, 311, 570, 321, 632, 512, 8863, 2663, 13, 407, 51358], "temperature": 0.0, "avg_logprob": -0.2926507618116296, "compression_ratio": 1.453781512605042, "no_speech_prob": 0.025176893919706345}, {"id": 107, "seek": 77028, "start": 790.16, "end": 796.64, "text": " we have a couple of days have passed, and we're back again, and then John recorded his", "tokens": [51358, 321, 362, 257, 1916, 295, 1708, 362, 4678, 11, 293, 321, 434, 646, 797, 11, 293, 550, 2619, 8287, 702, 51682], "temperature": 0.0, "avg_logprob": -0.2926507618116296, "compression_ratio": 1.453781512605042, "no_speech_prob": 0.025176893919706345}, {"id": 108, "seek": 79664, "start": 796.64, "end": 800.76, "text": " bit before we do Tanishq's bit, and then we're going to post them in backwards. So", "tokens": [50364, 857, 949, 321, 360, 314, 7524, 80, 311, 857, 11, 293, 550, 321, 434, 516, 281, 2183, 552, 294, 12204, 13, 407, 50570], "temperature": 0.0, "avg_logprob": -0.24281495871003142, "compression_ratio": 1.51931330472103, "no_speech_prob": 0.13657166063785553}, {"id": 109, "seek": 79664, "start": 800.76, "end": 806.56, "text": " hopefully there's not too many confusing continuity problems as a result, and it all goes smoothly.", "tokens": [50570, 4696, 456, 311, 406, 886, 867, 13181, 23807, 2740, 382, 257, 1874, 11, 293, 309, 439, 1709, 19565, 13, 50860], "temperature": 0.0, "avg_logprob": -0.24281495871003142, "compression_ratio": 1.51931330472103, "no_speech_prob": 0.13657166063785553}, {"id": 110, "seek": 79664, "start": 806.56, "end": 816.84, "text": " But it's time to turn it over to Tanishq to talk about DDPM. So we've reached the point", "tokens": [50860, 583, 309, 311, 565, 281, 1261, 309, 670, 281, 314, 7524, 80, 281, 751, 466, 413, 11373, 44, 13, 407, 321, 600, 6488, 264, 935, 51374], "temperature": 0.0, "avg_logprob": -0.24281495871003142, "compression_ratio": 1.51931330472103, "no_speech_prob": 0.13657166063785553}, {"id": 111, "seek": 79664, "start": 816.84, "end": 824.76, "text": " where we have this mini AI framework, and I guess it's time to now, you know, start", "tokens": [51374, 689, 321, 362, 341, 8382, 7318, 8388, 11, 293, 286, 2041, 309, 311, 565, 281, 586, 11, 291, 458, 11, 722, 51770], "temperature": 0.0, "avg_logprob": -0.24281495871003142, "compression_ratio": 1.51931330472103, "no_speech_prob": 0.13657166063785553}, {"id": 112, "seek": 82476, "start": 824.8, "end": 831.24, "text": " using it to build more, I guess, sophisticated models. And as we'll see here, we can start", "tokens": [50366, 1228, 309, 281, 1322, 544, 11, 286, 2041, 11, 16950, 5245, 13, 400, 382, 321, 603, 536, 510, 11, 321, 393, 722, 50688], "temperature": 0.0, "avg_logprob": -0.23023346931703628, "compression_ratio": 1.579646017699115, "no_speech_prob": 0.029288165271282196}, {"id": 113, "seek": 82476, "start": 831.24, "end": 837.28, "text": " putting together a diffusion model from scratch using the mini AI library, and we'll see how", "tokens": [50688, 3372, 1214, 257, 25242, 2316, 490, 8459, 1228, 264, 8382, 7318, 6405, 11, 293, 321, 603, 536, 577, 50990], "temperature": 0.0, "avg_logprob": -0.23023346931703628, "compression_ratio": 1.579646017699115, "no_speech_prob": 0.029288165271282196}, {"id": 114, "seek": 82476, "start": 837.28, "end": 842.2, "text": " it makes our life a lot easier. And also, it'd be very nice to see how, you know, the", "tokens": [50990, 309, 1669, 527, 993, 257, 688, 3571, 13, 400, 611, 11, 309, 1116, 312, 588, 1481, 281, 536, 577, 11, 291, 458, 11, 264, 51236], "temperature": 0.0, "avg_logprob": -0.23023346931703628, "compression_ratio": 1.579646017699115, "no_speech_prob": 0.029288165271282196}, {"id": 115, "seek": 82476, "start": 842.2, "end": 847.96, "text": " equations in the papers correspond to the code. So I have here, of course, the notebook", "tokens": [51236, 11787, 294, 264, 10577, 6805, 281, 264, 3089, 13, 407, 286, 362, 510, 11, 295, 1164, 11, 264, 21060, 51524], "temperature": 0.0, "avg_logprob": -0.23023346931703628, "compression_ratio": 1.579646017699115, "no_speech_prob": 0.029288165271282196}, {"id": 116, "seek": 84796, "start": 847.96, "end": 857.4000000000001, "text": " that we are, that we'll be working from. The paper, which, you know, we have the diffusion", "tokens": [50364, 300, 321, 366, 11, 300, 321, 603, 312, 1364, 490, 13, 440, 3035, 11, 597, 11, 291, 458, 11, 321, 362, 264, 25242, 50836], "temperature": 0.0, "avg_logprob": -0.27878728120223334, "compression_ratio": 1.6837209302325582, "no_speech_prob": 0.6920737624168396}, {"id": 117, "seek": 84796, "start": 857.4000000000001, "end": 863.48, "text": " model paper, the Noising Diffusion Probabilistic Models, which is the paper that was published", "tokens": [50836, 2316, 3035, 11, 264, 883, 3436, 413, 3661, 5704, 8736, 5177, 3142, 6583, 1625, 11, 597, 307, 264, 3035, 300, 390, 6572, 51140], "temperature": 0.0, "avg_logprob": -0.27878728120223334, "compression_ratio": 1.6837209302325582, "no_speech_prob": 0.6920737624168396}, {"id": 118, "seek": 84796, "start": 863.48, "end": 870.4000000000001, "text": " in 2020. It was one of the original diffusion model papers that kind of set off the entire", "tokens": [51140, 294, 4808, 13, 467, 390, 472, 295, 264, 3380, 25242, 2316, 10577, 300, 733, 295, 992, 766, 264, 2302, 51486], "temperature": 0.0, "avg_logprob": -0.27878728120223334, "compression_ratio": 1.6837209302325582, "no_speech_prob": 0.6920737624168396}, {"id": 119, "seek": 84796, "start": 870.4000000000001, "end": 875.8000000000001, "text": " trend of diffusion models, and it's a good starting point as we delve into this topic", "tokens": [51486, 6028, 295, 25242, 5245, 11, 293, 309, 311, 257, 665, 2891, 935, 382, 321, 43098, 666, 341, 4829, 51756], "temperature": 0.0, "avg_logprob": -0.27878728120223334, "compression_ratio": 1.6837209302325582, "no_speech_prob": 0.6920737624168396}, {"id": 120, "seek": 87580, "start": 875.8, "end": 885.8399999999999, "text": " further. And also, I have some diagrams and drawings that I will also show later on. But", "tokens": [50364, 3052, 13, 400, 611, 11, 286, 362, 512, 36709, 293, 18618, 300, 286, 486, 611, 855, 1780, 322, 13, 583, 50866], "temperature": 0.0, "avg_logprob": -0.24837201574574347, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.10225530713796616}, {"id": 121, "seek": 87580, "start": 885.8399999999999, "end": 893.76, "text": " yeah, basically, let's just get started with the code here, and of course, the paper. So", "tokens": [50866, 1338, 11, 1936, 11, 718, 311, 445, 483, 1409, 365, 264, 3089, 510, 11, 293, 295, 1164, 11, 264, 3035, 13, 407, 51262], "temperature": 0.0, "avg_logprob": -0.24837201574574347, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.10225530713796616}, {"id": 122, "seek": 87580, "start": 893.76, "end": 898.8399999999999, "text": " just to provide some context with this paper, you know, this paper was published from this", "tokens": [51262, 445, 281, 2893, 512, 4319, 365, 341, 3035, 11, 291, 458, 11, 341, 3035, 390, 6572, 490, 341, 51516], "temperature": 0.0, "avg_logprob": -0.24837201574574347, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.10225530713796616}, {"id": 123, "seek": 87580, "start": 898.8399999999999, "end": 905.4, "text": " group in UC Berkeley. I think a few of them have gone on now to work at Google, and this", "tokens": [51516, 1594, 294, 14079, 23684, 13, 286, 519, 257, 1326, 295, 552, 362, 2780, 322, 586, 281, 589, 412, 3329, 11, 293, 341, 51844], "temperature": 0.0, "avg_logprob": -0.24837201574574347, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.10225530713796616}, {"id": 124, "seek": 90540, "start": 905.4, "end": 912.8, "text": " is a peer review. Yes, a big lab at UC Berkeley. And so diffusion models were actually originally", "tokens": [50364, 307, 257, 15108, 3131, 13, 1079, 11, 257, 955, 2715, 412, 14079, 23684, 13, 400, 370, 25242, 5245, 645, 767, 7993, 50734], "temperature": 0.0, "avg_logprob": -0.3242435656095806, "compression_ratio": 1.5341880341880343, "no_speech_prob": 0.0022167630959302187}, {"id": 125, "seek": 90540, "start": 912.8, "end": 919.28, "text": " introduced in 2015. But this paper in 2020, greatly simplified the diffusion models and", "tokens": [50734, 7268, 294, 7546, 13, 583, 341, 3035, 294, 4808, 11, 14147, 26335, 264, 25242, 5245, 293, 51058], "temperature": 0.0, "avg_logprob": -0.3242435656095806, "compression_ratio": 1.5341880341880343, "no_speech_prob": 0.0022167630959302187}, {"id": 126, "seek": 90540, "start": 919.28, "end": 923.36, "text": " made it a lot easier to work with. And, you know, got these amazing results, as you can", "tokens": [51058, 1027, 309, 257, 688, 3571, 281, 589, 365, 13, 400, 11, 291, 458, 11, 658, 613, 2243, 3542, 11, 382, 291, 393, 51262], "temperature": 0.0, "avg_logprob": -0.3242435656095806, "compression_ratio": 1.5341880341880343, "no_speech_prob": 0.0022167630959302187}, {"id": 127, "seek": 90540, "start": 923.36, "end": 930.12, "text": " see here, when they trained on faces, and in this case, CIFAR-10. And, you know, this", "tokens": [51262, 536, 510, 11, 562, 436, 8895, 322, 8475, 11, 293, 294, 341, 1389, 11, 383, 12775, 1899, 12, 3279, 13, 400, 11, 291, 458, 11, 341, 51600], "temperature": 0.0, "avg_logprob": -0.3242435656095806, "compression_ratio": 1.5341880341880343, "no_speech_prob": 0.0022167630959302187}, {"id": 128, "seek": 93012, "start": 930.12, "end": 938.12, "text": " really was very, kind of a big leap in terms of the progress of diffusion models. And so", "tokens": [50364, 534, 390, 588, 11, 733, 295, 257, 955, 19438, 294, 2115, 295, 264, 4205, 295, 25242, 5245, 13, 400, 370, 50764], "temperature": 0.0, "avg_logprob": -0.3032283506531646, "compression_ratio": 1.4886363636363635, "no_speech_prob": 0.05183859169483185}, {"id": 129, "seek": 93012, "start": 938.12, "end": 943.96, "text": " just to kind of briefly provide, I guess, kind of an overview. If I could just quickly", "tokens": [50764, 445, 281, 733, 295, 10515, 2893, 11, 286, 2041, 11, 733, 295, 364, 12492, 13, 759, 286, 727, 445, 2661, 51056], "temperature": 0.0, "avg_logprob": -0.3032283506531646, "compression_ratio": 1.4886363636363635, "no_speech_prob": 0.05183859169483185}, {"id": 130, "seek": 93012, "start": 943.96, "end": 953.12, "text": " step, just mention something, which is, you know, when we started this course, we, you", "tokens": [51056, 1823, 11, 445, 2152, 746, 11, 597, 307, 11, 291, 458, 11, 562, 321, 1409, 341, 1164, 11, 321, 11, 291, 51514], "temperature": 0.0, "avg_logprob": -0.3032283506531646, "compression_ratio": 1.4886363636363635, "no_speech_prob": 0.05183859169483185}, {"id": 131, "seek": 95312, "start": 953.12, "end": 959.68, "text": " know, we talked a bit about how perhaps the diffusion part of diffusion models is not", "tokens": [50364, 458, 11, 321, 2825, 257, 857, 466, 577, 4317, 264, 25242, 644, 295, 25242, 5245, 307, 406, 50692], "temperature": 0.0, "avg_logprob": -0.2737183221956579, "compression_ratio": 1.7452830188679245, "no_speech_prob": 0.0685102790594101}, {"id": 132, "seek": 95312, "start": 959.68, "end": 964.16, "text": " actually all that. Everybody's been talking about diffusion models, because that's, particularly", "tokens": [50692, 767, 439, 300, 13, 7646, 311, 668, 1417, 466, 25242, 5245, 11, 570, 300, 311, 11, 4098, 50916], "temperature": 0.0, "avg_logprob": -0.2737183221956579, "compression_ratio": 1.7452830188679245, "no_speech_prob": 0.0685102790594101}, {"id": 133, "seek": 95312, "start": 964.16, "end": 970.92, "text": " because that's the open source thing we have that works really well. But this week, actually", "tokens": [50916, 570, 300, 311, 264, 1269, 4009, 551, 321, 362, 300, 1985, 534, 731, 13, 583, 341, 1243, 11, 767, 51254], "temperature": 0.0, "avg_logprob": -0.2737183221956579, "compression_ratio": 1.7452830188679245, "no_speech_prob": 0.0685102790594101}, {"id": 134, "seek": 95312, "start": 970.92, "end": 976.68, "text": " a model that appears to be quite a lot better than stable diffusion was released, that doesn't", "tokens": [51254, 257, 2316, 300, 7038, 281, 312, 1596, 257, 688, 1101, 813, 8351, 25242, 390, 4736, 11, 300, 1177, 380, 51542], "temperature": 0.0, "avg_logprob": -0.2737183221956579, "compression_ratio": 1.7452830188679245, "no_speech_prob": 0.0685102790594101}, {"id": 135, "seek": 97668, "start": 976.68, "end": 986.16, "text": " use diffusion at all. Having said that, the basic ideas, like most of the stuff that Tanisha", "tokens": [50364, 764, 25242, 412, 439, 13, 10222, 848, 300, 11, 264, 3875, 3487, 11, 411, 881, 295, 264, 1507, 300, 314, 7524, 64, 50838], "temperature": 0.0, "avg_logprob": -0.30222588115268284, "compression_ratio": 1.4728260869565217, "no_speech_prob": 0.04879997670650482}, {"id": 136, "seek": 97668, "start": 986.16, "end": 994.4, "text": " talks about today, will still appear in some kind of form, you know, but a lot of the details", "tokens": [50838, 6686, 466, 965, 11, 486, 920, 4204, 294, 512, 733, 295, 1254, 11, 291, 458, 11, 457, 257, 688, 295, 264, 4365, 51250], "temperature": 0.0, "avg_logprob": -0.30222588115268284, "compression_ratio": 1.4728260869565217, "no_speech_prob": 0.04879997670650482}, {"id": 137, "seek": 97668, "start": 994.4, "end": 1000.0799999999999, "text": " will be different. But strictly speaking, actually, yeah, I don't even know if we've", "tokens": [51250, 486, 312, 819, 13, 583, 20792, 4124, 11, 767, 11, 1338, 11, 286, 500, 380, 754, 458, 498, 321, 600, 51534], "temperature": 0.0, "avg_logprob": -0.30222588115268284, "compression_ratio": 1.4728260869565217, "no_speech_prob": 0.04879997670650482}, {"id": 138, "seek": 100008, "start": 1000.08, "end": 1007.12, "text": " got a word anymore for the kind of like modern generative model things we're doing. So in", "tokens": [50364, 658, 257, 1349, 3602, 337, 264, 733, 295, 411, 4363, 1337, 1166, 2316, 721, 321, 434, 884, 13, 407, 294, 50716], "temperature": 0.0, "avg_logprob": -0.2958069994933623, "compression_ratio": 1.698051948051948, "no_speech_prob": 0.6745489835739136}, {"id": 139, "seek": 100008, "start": 1007.12, "end": 1012.0, "text": " some ways, when we're talking about diffusion models, you should maybe replace it in your", "tokens": [50716, 512, 2098, 11, 562, 321, 434, 1417, 466, 25242, 5245, 11, 291, 820, 1310, 7406, 309, 294, 428, 50960], "temperature": 0.0, "avg_logprob": -0.2958069994933623, "compression_ratio": 1.698051948051948, "no_speech_prob": 0.6745489835739136}, {"id": 140, "seek": 100008, "start": 1012.0, "end": 1016.8000000000001, "text": " head with some other word, which is more general and includes this paper that Tanisha is looking", "tokens": [50960, 1378, 365, 512, 661, 1349, 11, 597, 307, 544, 2674, 293, 5974, 341, 3035, 300, 314, 7524, 64, 307, 1237, 51200], "temperature": 0.0, "avg_logprob": -0.2958069994933623, "compression_ratio": 1.698051948051948, "no_speech_prob": 0.6745489835739136}, {"id": 141, "seek": 100008, "start": 1016.8000000000001, "end": 1017.8000000000001, "text": " at here.", "tokens": [51200, 412, 510, 13, 51250], "temperature": 0.0, "avg_logprob": -0.2958069994933623, "compression_ratio": 1.698051948051948, "no_speech_prob": 0.6745489835739136}, {"id": 142, "seek": 100008, "start": 1017.8000000000001, "end": 1019.5200000000001, "text": " Iterative refinement, perhaps. That's what I've heard.", "tokens": [51250, 286, 391, 1166, 1895, 30229, 11, 4317, 13, 663, 311, 437, 286, 600, 2198, 13, 51336], "temperature": 0.0, "avg_logprob": -0.2958069994933623, "compression_ratio": 1.698051948051948, "no_speech_prob": 0.6745489835739136}, {"id": 143, "seek": 100008, "start": 1019.5200000000001, "end": 1024.24, "text": " Yeah, that's not bad, iterative refinement. I'm sure by the time people watch this video,", "tokens": [51336, 865, 11, 300, 311, 406, 1578, 11, 17138, 1166, 1895, 30229, 13, 286, 478, 988, 538, 264, 565, 561, 1159, 341, 960, 11, 51572], "temperature": 0.0, "avg_logprob": -0.2958069994933623, "compression_ratio": 1.698051948051948, "no_speech_prob": 0.6745489835739136}, {"id": 144, "seek": 100008, "start": 1024.24, "end": 1029.92, "text": " probably, you know, somebody will have decided on something. We will keep our course website", "tokens": [51572, 1391, 11, 291, 458, 11, 2618, 486, 362, 3047, 322, 746, 13, 492, 486, 1066, 527, 1164, 3144, 51856], "temperature": 0.0, "avg_logprob": -0.2958069994933623, "compression_ratio": 1.698051948051948, "no_speech_prob": 0.6745489835739136}, {"id": 145, "seek": 102992, "start": 1029.92, "end": 1030.92, "text": " up to date.", "tokens": [50364, 493, 281, 4002, 13, 50414], "temperature": 0.0, "avg_logprob": -0.3260088153913909, "compression_ratio": 1.641255605381166, "no_speech_prob": 0.3552243113517761}, {"id": 146, "seek": 102992, "start": 1030.92, "end": 1037.0, "text": " Yeah, yeah, this is the paper that Jeremy was talking about. And yeah, every week, there", "tokens": [50414, 865, 11, 1338, 11, 341, 307, 264, 3035, 300, 17809, 390, 1417, 466, 13, 400, 1338, 11, 633, 1243, 11, 456, 50718], "temperature": 0.0, "avg_logprob": -0.3260088153913909, "compression_ratio": 1.641255605381166, "no_speech_prob": 0.3552243113517761}, {"id": 147, "seek": 102992, "start": 1037.0, "end": 1042.92, "text": " seems to be another state of the art model. So, but yeah, like Jeremy said, a lot of the", "tokens": [50718, 2544, 281, 312, 1071, 1785, 295, 264, 1523, 2316, 13, 407, 11, 457, 1338, 11, 411, 17809, 848, 11, 257, 688, 295, 264, 51014], "temperature": 0.0, "avg_logprob": -0.3260088153913909, "compression_ratio": 1.641255605381166, "no_speech_prob": 0.3552243113517761}, {"id": 148, "seek": 102992, "start": 1042.92, "end": 1048.28, "text": " principles are the same, but you know, the details can be different for each paper. But", "tokens": [51014, 9156, 366, 264, 912, 11, 457, 291, 458, 11, 264, 4365, 393, 312, 819, 337, 1184, 3035, 13, 583, 51282], "temperature": 0.0, "avg_logprob": -0.3260088153913909, "compression_ratio": 1.641255605381166, "no_speech_prob": 0.3552243113517761}, {"id": 149, "seek": 102992, "start": 1048.28, "end": 1054.16, "text": " yeah. And just to, I just want to again, also, like Jeremy was saying, kind of zoom back", "tokens": [51282, 1338, 13, 400, 445, 281, 11, 286, 445, 528, 281, 797, 11, 611, 11, 411, 17809, 390, 1566, 11, 733, 295, 8863, 646, 51576], "temperature": 0.0, "avg_logprob": -0.3260088153913909, "compression_ratio": 1.641255605381166, "no_speech_prob": 0.3552243113517761}, {"id": 150, "seek": 105416, "start": 1054.16, "end": 1059.76, "text": " a little bit and kind of talk about a little bit about what, you know, just kind of provide", "tokens": [50364, 257, 707, 857, 293, 733, 295, 751, 466, 257, 707, 857, 466, 437, 11, 291, 458, 11, 445, 733, 295, 2893, 50644], "temperature": 0.0, "avg_logprob": -0.33585254351298016, "compression_ratio": 1.669767441860465, "no_speech_prob": 0.6546695232391357}, {"id": 151, "seek": 105416, "start": 1059.76, "end": 1066.96, "text": " a review of what we're trying to do here. Right. So let me just, I guess, yeah. So with", "tokens": [50644, 257, 3131, 295, 437, 321, 434, 1382, 281, 360, 510, 13, 1779, 13, 407, 718, 385, 445, 11, 286, 2041, 11, 1338, 13, 407, 365, 51004], "temperature": 0.0, "avg_logprob": -0.33585254351298016, "compression_ratio": 1.669767441860465, "no_speech_prob": 0.6546695232391357}, {"id": 152, "seek": 105416, "start": 1066.96, "end": 1076.72, "text": " this task, we were trying to, in this case, I would try to do image generation. Of course,", "tokens": [51004, 341, 5633, 11, 321, 645, 1382, 281, 11, 294, 341, 1389, 11, 286, 576, 853, 281, 360, 3256, 5125, 13, 2720, 1164, 11, 51492], "temperature": 0.0, "avg_logprob": -0.33585254351298016, "compression_ratio": 1.669767441860465, "no_speech_prob": 0.6546695232391357}, {"id": 153, "seek": 105416, "start": 1076.72, "end": 1081.44, "text": " it could be other forms of generation, like text generation or whatever. And the general", "tokens": [51492, 309, 727, 312, 661, 6422, 295, 5125, 11, 411, 2487, 5125, 420, 2035, 13, 400, 264, 2674, 51728], "temperature": 0.0, "avg_logprob": -0.33585254351298016, "compression_ratio": 1.669767441860465, "no_speech_prob": 0.6546695232391357}, {"id": 154, "seek": 108144, "start": 1081.44, "end": 1087.76, "text": " idea is that, of course, we have some, you know, data points. You know, in this case,", "tokens": [50364, 1558, 307, 300, 11, 295, 1164, 11, 321, 362, 512, 11, 291, 458, 11, 1412, 2793, 13, 509, 458, 11, 294, 341, 1389, 11, 50680], "temperature": 0.0, "avg_logprob": -0.2921857116042927, "compression_ratio": 1.78, "no_speech_prob": 0.14411841332912445}, {"id": 155, "seek": 108144, "start": 1087.76, "end": 1091.52, "text": " we have some images of dogs and we want to produce more like these, the data points that", "tokens": [50680, 321, 362, 512, 5267, 295, 7197, 293, 321, 528, 281, 5258, 544, 411, 613, 11, 264, 1412, 2793, 300, 50868], "temperature": 0.0, "avg_logprob": -0.2921857116042927, "compression_ratio": 1.78, "no_speech_prob": 0.14411841332912445}, {"id": 156, "seek": 108144, "start": 1091.52, "end": 1097.2, "text": " were given. So in this case, maybe the dog image generation or something like this. And", "tokens": [50868, 645, 2212, 13, 407, 294, 341, 1389, 11, 1310, 264, 3000, 3256, 5125, 420, 746, 411, 341, 13, 400, 51152], "temperature": 0.0, "avg_logprob": -0.2921857116042927, "compression_ratio": 1.78, "no_speech_prob": 0.14411841332912445}, {"id": 157, "seek": 108144, "start": 1097.2, "end": 1104.28, "text": " so the overall idea that a lot of these approaches take for image, you know, for some sort of", "tokens": [51152, 370, 264, 4787, 1558, 300, 257, 688, 295, 613, 11587, 747, 337, 3256, 11, 291, 458, 11, 337, 512, 1333, 295, 51506], "temperature": 0.0, "avg_logprob": -0.2921857116042927, "compression_ratio": 1.78, "no_speech_prob": 0.14411841332912445}, {"id": 158, "seek": 110428, "start": 1104.28, "end": 1110.12, "text": " generative modeling task is they have, they try to, not over there, I want to get over", "tokens": [50364, 1337, 1166, 15983, 5633, 307, 436, 362, 11, 436, 853, 281, 11, 406, 670, 456, 11, 286, 528, 281, 483, 670, 50656], "temperature": 0.0, "avg_logprob": -0.5391141718084161, "compression_ratio": 1.5170454545454546, "no_speech_prob": 0.859347939491272}, {"id": 159, "seek": 110428, "start": 1110.12, "end": 1117.32, "text": " here. They try to, oops, what happened here? Maybe my, yeah. Yeah. So let me just end a", "tokens": [50656, 510, 13, 814, 853, 281, 11, 34166, 11, 437, 2011, 510, 30, 2704, 452, 11, 1338, 13, 865, 13, 407, 718, 385, 445, 917, 257, 51016], "temperature": 0.0, "avg_logprob": -0.5391141718084161, "compression_ratio": 1.5170454545454546, "no_speech_prob": 0.859347939491272}, {"id": 160, "seek": 110428, "start": 1117.32, "end": 1129.48, "text": " bit. P of X, which is our, which is basically the sort of likelihood, what's going to happen", "tokens": [51016, 857, 13, 430, 295, 1783, 11, 597, 307, 527, 11, 597, 307, 1936, 264, 1333, 295, 22119, 11, 437, 311, 516, 281, 1051, 51624], "temperature": 0.0, "avg_logprob": -0.5391141718084161, "compression_ratio": 1.5170454545454546, "no_speech_prob": 0.859347939491272}, {"id": 161, "seek": 112948, "start": 1129.48, "end": 1141.48, "text": " here? Likelihood of data point X, of X. So let's say X is some image. Then P of X tells us,", "tokens": [50364, 510, 30, 1743, 21648, 295, 1412, 935, 1783, 11, 295, 1783, 13, 407, 718, 311, 584, 1783, 307, 512, 3256, 13, 1396, 430, 295, 1783, 5112, 505, 11, 50964], "temperature": 0.0, "avg_logprob": -0.23776488968088658, "compression_ratio": 1.4736842105263157, "no_speech_prob": 0.052615951746702194}, {"id": 162, "seek": 112948, "start": 1141.48, "end": 1148.28, "text": " like, what is the probability that you would see that image in real life? And like, if you,", "tokens": [50964, 411, 11, 437, 307, 264, 8482, 300, 291, 576, 536, 300, 3256, 294, 957, 993, 30, 400, 411, 11, 498, 291, 11, 51304], "temperature": 0.0, "avg_logprob": -0.23776488968088658, "compression_ratio": 1.4736842105263157, "no_speech_prob": 0.052615951746702194}, {"id": 163, "seek": 112948, "start": 1148.28, "end": 1153.8, "text": " we can take like a simpler example, which may be easier to think about of like a one-dimensional", "tokens": [51304, 321, 393, 747, 411, 257, 18587, 1365, 11, 597, 815, 312, 3571, 281, 519, 466, 295, 411, 257, 472, 12, 18759, 51580], "temperature": 0.0, "avg_logprob": -0.23776488968088658, "compression_ratio": 1.4736842105263157, "no_speech_prob": 0.052615951746702194}, {"id": 164, "seek": 115380, "start": 1153.8, "end": 1159.1599999999999, "text": " data point, like height, for example. And if we were to look at height, of course we know,", "tokens": [50364, 1412, 935, 11, 411, 6681, 11, 337, 1365, 13, 400, 498, 321, 645, 281, 574, 412, 6681, 11, 295, 1164, 321, 458, 11, 50632], "temperature": 0.0, "avg_logprob": -0.27305230293565125, "compression_ratio": 1.874015748031496, "no_speech_prob": 0.18473799526691437}, {"id": 165, "seek": 115380, "start": 1159.1599999999999, "end": 1164.28, "text": " like we have a data distribution, that's kind of a bell curve. And, you know, you have maybe some,", "tokens": [50632, 411, 321, 362, 257, 1412, 7316, 11, 300, 311, 733, 295, 257, 4549, 7605, 13, 400, 11, 291, 458, 11, 291, 362, 1310, 512, 11, 50888], "temperature": 0.0, "avg_logprob": -0.27305230293565125, "compression_ratio": 1.874015748031496, "no_speech_prob": 0.18473799526691437}, {"id": 166, "seek": 115380, "start": 1164.28, "end": 1169.72, "text": " you know, mean height, which is like something like five, nine, five, 10, yeah. I guess it'd be", "tokens": [50888, 291, 458, 11, 914, 6681, 11, 597, 307, 411, 746, 411, 1732, 11, 4949, 11, 1732, 11, 1266, 11, 1338, 13, 286, 2041, 309, 1116, 312, 51160], "temperature": 0.0, "avg_logprob": -0.27305230293565125, "compression_ratio": 1.874015748031496, "no_speech_prob": 0.18473799526691437}, {"id": 167, "seek": 115380, "start": 1169.72, "end": 1174.84, "text": " at five feet, nine, 10 inches or something like that, or five feet, nine inches, whatever. And", "tokens": [51160, 412, 1732, 3521, 11, 4949, 11, 1266, 8478, 420, 746, 411, 300, 11, 420, 1732, 3521, 11, 4949, 8478, 11, 2035, 13, 400, 51416], "temperature": 0.0, "avg_logprob": -0.27305230293565125, "compression_ratio": 1.874015748031496, "no_speech_prob": 0.18473799526691437}, {"id": 168, "seek": 115380, "start": 1174.84, "end": 1179.1599999999999, "text": " then of course we have some, it was like, you have some more unlikely points, but that is still", "tokens": [51416, 550, 295, 1164, 321, 362, 512, 11, 309, 390, 411, 11, 291, 362, 512, 544, 17518, 2793, 11, 457, 300, 307, 920, 51632], "temperature": 0.0, "avg_logprob": -0.27305230293565125, "compression_ratio": 1.874015748031496, "no_speech_prob": 0.18473799526691437}, {"id": 169, "seek": 117916, "start": 1179.16, "end": 1184.8400000000001, "text": " possible. Like for example, you have seven feet or you have something that's maybe not as likely,", "tokens": [50364, 1944, 13, 1743, 337, 1365, 11, 291, 362, 3407, 3521, 420, 291, 362, 746, 300, 311, 1310, 406, 382, 3700, 11, 50648], "temperature": 0.0, "avg_logprob": -0.25098336329225635, "compression_ratio": 1.8634538152610443, "no_speech_prob": 0.026757292449474335}, {"id": 170, "seek": 117916, "start": 1184.8400000000001, "end": 1188.2, "text": " it was just like, you know, like three feet or something like this. So here's like.", "tokens": [50648, 309, 390, 445, 411, 11, 291, 458, 11, 411, 1045, 3521, 420, 746, 411, 341, 13, 407, 510, 311, 411, 13, 50816], "temperature": 0.0, "avg_logprob": -0.25098336329225635, "compression_ratio": 1.8634538152610443, "no_speech_prob": 0.026757292449474335}, {"id": 171, "seek": 117916, "start": 1188.2, "end": 1195.48, "text": " X-axis is height and the Y-axis is the probability of some random person you meet being that tall.", "tokens": [50816, 1783, 12, 24633, 307, 6681, 293, 264, 398, 12, 24633, 307, 264, 8482, 295, 512, 4974, 954, 291, 1677, 885, 300, 6764, 13, 51180], "temperature": 0.0, "avg_logprob": -0.25098336329225635, "compression_ratio": 1.8634538152610443, "no_speech_prob": 0.026757292449474335}, {"id": 172, "seek": 117916, "start": 1196.6000000000001, "end": 1202.3600000000001, "text": " Exactly. So, you know, you, you know, the, yeah, this is basically the probability. And so of", "tokens": [51236, 7587, 13, 407, 11, 291, 458, 11, 291, 11, 291, 458, 11, 264, 11, 1338, 11, 341, 307, 1936, 264, 8482, 13, 400, 370, 295, 51524], "temperature": 0.0, "avg_logprob": -0.25098336329225635, "compression_ratio": 1.8634538152610443, "no_speech_prob": 0.026757292449474335}, {"id": 173, "seek": 117916, "start": 1202.3600000000001, "end": 1206.68, "text": " course you have this sort of peak, which is where, you know, you have higher probability.", "tokens": [51524, 1164, 291, 362, 341, 1333, 295, 10651, 11, 597, 307, 689, 11, 291, 458, 11, 291, 362, 2946, 8482, 13, 51740], "temperature": 0.0, "avg_logprob": -0.25098336329225635, "compression_ratio": 1.8634538152610443, "no_speech_prob": 0.026757292449474335}, {"id": 174, "seek": 120668, "start": 1206.68, "end": 1212.1200000000001, "text": " And so those are the sorts of, you know, values that you would see more often. So this is, this", "tokens": [50364, 400, 370, 729, 366, 264, 7527, 295, 11, 291, 458, 11, 4190, 300, 291, 576, 536, 544, 2049, 13, 407, 341, 307, 11, 341, 50636], "temperature": 0.0, "avg_logprob": -0.20839694537947664, "compression_ratio": 1.8110599078341014, "no_speech_prob": 0.0004172970075160265}, {"id": 175, "seek": 120668, "start": 1212.1200000000001, "end": 1220.52, "text": " is our, what we do call our P of X. And like the important part about P of X is that you can use", "tokens": [50636, 307, 527, 11, 437, 321, 360, 818, 527, 430, 295, 1783, 13, 400, 411, 264, 1021, 644, 466, 430, 295, 1783, 307, 300, 291, 393, 764, 51056], "temperature": 0.0, "avg_logprob": -0.20839694537947664, "compression_ratio": 1.8110599078341014, "no_speech_prob": 0.0004172970075160265}, {"id": 176, "seek": 120668, "start": 1220.52, "end": 1226.68, "text": " this now to sample new values. If you know what P of X is, or if you have some sort of information", "tokens": [51056, 341, 586, 281, 6889, 777, 4190, 13, 759, 291, 458, 437, 430, 295, 1783, 307, 11, 420, 498, 291, 362, 512, 1333, 295, 1589, 51364], "temperature": 0.0, "avg_logprob": -0.20839694537947664, "compression_ratio": 1.8110599078341014, "no_speech_prob": 0.0004172970075160265}, {"id": 177, "seek": 120668, "start": 1226.68, "end": 1232.76, "text": " about P of X. So for example, here, you can think of like, if you were to like, say, maybe have some,", "tokens": [51364, 466, 430, 295, 1783, 13, 407, 337, 1365, 11, 510, 11, 291, 393, 519, 295, 411, 11, 498, 291, 645, 281, 411, 11, 584, 11, 1310, 362, 512, 11, 51668], "temperature": 0.0, "avg_logprob": -0.20839694537947664, "compression_ratio": 1.8110599078341014, "no_speech_prob": 0.0004172970075160265}, {"id": 178, "seek": 123276, "start": 1232.76, "end": 1236.76, "text": " let's say you have some game and you have some human characters in the game, and you just want", "tokens": [50364, 718, 311, 584, 291, 362, 512, 1216, 293, 291, 362, 512, 1952, 4342, 294, 264, 1216, 11, 293, 291, 445, 528, 50564], "temperature": 0.0, "avg_logprob": -0.20968026247891514, "compression_ratio": 1.94331983805668, "no_speech_prob": 0.0002002605324378237}, {"id": 179, "seek": 123276, "start": 1236.76, "end": 1242.92, "text": " to randomly generate a height for this human character. You know, you could, you wouldn't", "tokens": [50564, 281, 16979, 8460, 257, 6681, 337, 341, 1952, 2517, 13, 509, 458, 11, 291, 727, 11, 291, 2759, 380, 50872], "temperature": 0.0, "avg_logprob": -0.20968026247891514, "compression_ratio": 1.94331983805668, "no_speech_prob": 0.0002002605324378237}, {"id": 180, "seek": 123276, "start": 1242.92, "end": 1247.4, "text": " want to of course select a random height between three and seven, that's kind of uniformly distributed.", "tokens": [50872, 528, 281, 295, 1164, 3048, 257, 4974, 6681, 1296, 1045, 293, 3407, 11, 300, 311, 733, 295, 48806, 12631, 13, 51096], "temperature": 0.0, "avg_logprob": -0.20968026247891514, "compression_ratio": 1.94331983805668, "no_speech_prob": 0.0002002605324378237}, {"id": 181, "seek": 123276, "start": 1247.4, "end": 1255.08, "text": " You would instead maybe want to, you would want to have the height dependent on this sort of", "tokens": [51096, 509, 576, 2602, 1310, 528, 281, 11, 291, 576, 528, 281, 362, 264, 6681, 12334, 322, 341, 1333, 295, 51480], "temperature": 0.0, "avg_logprob": -0.20968026247891514, "compression_ratio": 1.94331983805668, "no_speech_prob": 0.0002002605324378237}, {"id": 182, "seek": 123276, "start": 1255.08, "end": 1261.4, "text": " function where you would more likely sample values, you know, in the middle and less likely sample", "tokens": [51480, 2445, 689, 291, 576, 544, 3700, 6889, 4190, 11, 291, 458, 11, 294, 264, 2808, 293, 1570, 3700, 6889, 51796], "temperature": 0.0, "avg_logprob": -0.20968026247891514, "compression_ratio": 1.94331983805668, "no_speech_prob": 0.0002002605324378237}, {"id": 183, "seek": 126140, "start": 1261.4, "end": 1266.92, "text": " these sorts of extreme points. So it's dependent on this function P of X. So having some information", "tokens": [50364, 613, 7527, 295, 8084, 2793, 13, 407, 309, 311, 12334, 322, 341, 2445, 430, 295, 1783, 13, 407, 1419, 512, 1589, 50640], "temperature": 0.0, "avg_logprob": -0.17976584333054563, "compression_ratio": 1.8093023255813954, "no_speech_prob": 0.0006070529925636947}, {"id": 184, "seek": 126140, "start": 1266.92, "end": 1274.1200000000001, "text": " about P of X will allow you to sample more data points. And so that's kind of the overall goal", "tokens": [50640, 466, 430, 295, 1783, 486, 2089, 291, 281, 6889, 544, 1412, 2793, 13, 400, 370, 300, 311, 733, 295, 264, 4787, 3387, 51000], "temperature": 0.0, "avg_logprob": -0.17976584333054563, "compression_ratio": 1.8093023255813954, "no_speech_prob": 0.0006070529925636947}, {"id": 185, "seek": 126140, "start": 1274.1200000000001, "end": 1280.92, "text": " of generative modeling is to get some information about P of X that then allows us to sample new", "tokens": [51000, 295, 1337, 1166, 15983, 307, 281, 483, 512, 1589, 466, 430, 295, 1783, 300, 550, 4045, 505, 281, 6889, 777, 51340], "temperature": 0.0, "avg_logprob": -0.17976584333054563, "compression_ratio": 1.8093023255813954, "no_speech_prob": 0.0006070529925636947}, {"id": 186, "seek": 126140, "start": 1280.92, "end": 1288.0400000000002, "text": " points and, you know, create new generations. So that's kind of a high level kind of description", "tokens": [51340, 2793, 293, 11, 291, 458, 11, 1884, 777, 10593, 13, 407, 300, 311, 733, 295, 257, 1090, 1496, 733, 295, 3855, 51696], "temperature": 0.0, "avg_logprob": -0.17976584333054563, "compression_ratio": 1.8093023255813954, "no_speech_prob": 0.0006070529925636947}, {"id": 187, "seek": 128804, "start": 1288.04, "end": 1293.48, "text": " of what we're trying to do when we're doing generative modeling. And of course, there are", "tokens": [50364, 295, 437, 321, 434, 1382, 281, 360, 562, 321, 434, 884, 1337, 1166, 15983, 13, 400, 295, 1164, 11, 456, 366, 50636], "temperature": 0.0, "avg_logprob": -0.2216213862101237, "compression_ratio": 1.7851851851851852, "no_speech_prob": 0.02479475364089012}, {"id": 188, "seek": 128804, "start": 1293.48, "end": 1298.68, "text": " many different approaches. We, you know, we have our famous scans, which, you know, used to be", "tokens": [50636, 867, 819, 11587, 13, 492, 11, 291, 458, 11, 321, 362, 527, 4618, 35116, 11, 597, 11, 291, 458, 11, 1143, 281, 312, 50896], "temperature": 0.0, "avg_logprob": -0.2216213862101237, "compression_ratio": 1.7851851851851852, "no_speech_prob": 0.02479475364089012}, {"id": 189, "seek": 128804, "start": 1298.68, "end": 1305.72, "text": " the common method back in the day before diffusion models. You know, we have BAEs, which I think we'll", "tokens": [50896, 264, 2689, 3170, 646, 294, 264, 786, 949, 25242, 5245, 13, 509, 458, 11, 321, 362, 21050, 20442, 11, 597, 286, 519, 321, 603, 51248], "temperature": 0.0, "avg_logprob": -0.2216213862101237, "compression_ratio": 1.7851851851851852, "no_speech_prob": 0.02479475364089012}, {"id": 190, "seek": 128804, "start": 1305.72, "end": 1309.72, "text": " probably talk a little bit more about that later as well. We'll be talking about both of those", "tokens": [51248, 1391, 751, 257, 707, 857, 544, 466, 300, 1780, 382, 731, 13, 492, 603, 312, 1417, 466, 1293, 295, 729, 51448], "temperature": 0.0, "avg_logprob": -0.2216213862101237, "compression_ratio": 1.7851851851851852, "no_speech_prob": 0.02479475364089012}, {"id": 191, "seek": 128804, "start": 1309.72, "end": 1314.6, "text": " techniques later. Yeah. Yeah. And so there are many different other techniques. There are also some", "tokens": [51448, 7512, 1780, 13, 865, 13, 865, 13, 400, 370, 456, 366, 867, 819, 661, 7512, 13, 821, 366, 611, 512, 51692], "temperature": 0.0, "avg_logprob": -0.2216213862101237, "compression_ratio": 1.7851851851851852, "no_speech_prob": 0.02479475364089012}, {"id": 192, "seek": 131460, "start": 1314.6, "end": 1319.56, "text": " niche techniques that are out there as well. But of course, now the popular one is, are these", "tokens": [50364, 19956, 7512, 300, 366, 484, 456, 382, 731, 13, 583, 295, 1164, 11, 586, 264, 3743, 472, 307, 11, 366, 613, 50612], "temperature": 0.0, "avg_logprob": -0.2960402639288651, "compression_ratio": 1.6327433628318584, "no_speech_prob": 0.0015486108604818583}, {"id": 193, "seek": 131460, "start": 1319.56, "end": 1324.12, "text": " diffusion models or, you know, as we talked about, maybe a better term might be, a hereditary", "tokens": [50612, 25242, 5245, 420, 11, 291, 458, 11, 382, 321, 2825, 466, 11, 1310, 257, 1101, 1433, 1062, 312, 11, 257, 415, 986, 4109, 50840], "temperature": 0.0, "avg_logprob": -0.2960402639288651, "compression_ratio": 1.6327433628318584, "no_speech_prob": 0.0015486108604818583}, {"id": 194, "seek": 131460, "start": 1324.12, "end": 1332.28, "text": " refinement or whenever, you know, whatever the term ends to be. But yeah, so there are many", "tokens": [50840, 1895, 30229, 420, 5699, 11, 291, 458, 11, 2035, 264, 1433, 5314, 281, 312, 13, 583, 1338, 11, 370, 456, 366, 867, 51248], "temperature": 0.0, "avg_logprob": -0.2960402639288651, "compression_ratio": 1.6327433628318584, "no_speech_prob": 0.0015486108604818583}, {"id": 195, "seek": 131460, "start": 1332.28, "end": 1340.76, "text": " different techniques and yeah. So let's just, so this is kind of the general diagram that", "tokens": [51248, 819, 7512, 293, 1338, 13, 407, 718, 311, 445, 11, 370, 341, 307, 733, 295, 264, 2674, 10686, 300, 51672], "temperature": 0.0, "avg_logprob": -0.2960402639288651, "compression_ratio": 1.6327433628318584, "no_speech_prob": 0.0015486108604818583}, {"id": 196, "seek": 134076, "start": 1340.76, "end": 1346.84, "text": " shows what diffusion models are. And if we can look at the paper here, which let's pull up the", "tokens": [50364, 3110, 437, 25242, 5245, 366, 13, 400, 498, 321, 393, 574, 412, 264, 3035, 510, 11, 597, 718, 311, 2235, 493, 264, 50668], "temperature": 0.0, "avg_logprob": -0.21593652987012676, "compression_ratio": 1.6483050847457628, "no_speech_prob": 0.00035141187254339457}, {"id": 197, "seek": 134076, "start": 1346.84, "end": 1352.84, "text": " paper. Yeah. And you see here, this is the sort of, they call it a directed graphical model. It's a", "tokens": [50668, 3035, 13, 865, 13, 400, 291, 536, 510, 11, 341, 307, 264, 1333, 295, 11, 436, 818, 309, 257, 12898, 35942, 2316, 13, 467, 311, 257, 50968], "temperature": 0.0, "avg_logprob": -0.21593652987012676, "compression_ratio": 1.6483050847457628, "no_speech_prob": 0.00035141187254339457}, {"id": 198, "seek": 134076, "start": 1352.84, "end": 1359.48, "text": " very complicated term. It's just kind of showing what's going on in this, you know, in this process.", "tokens": [50968, 588, 6179, 1433, 13, 467, 311, 445, 733, 295, 4099, 437, 311, 516, 322, 294, 341, 11, 291, 458, 11, 294, 341, 1399, 13, 51300], "temperature": 0.0, "avg_logprob": -0.21593652987012676, "compression_ratio": 1.6483050847457628, "no_speech_prob": 0.00035141187254339457}, {"id": 199, "seek": 134076, "start": 1360.28, "end": 1366.28, "text": " And there's a lot of complicated math here, but we'll highlight some of the key variables and", "tokens": [51340, 400, 456, 311, 257, 688, 295, 6179, 5221, 510, 11, 457, 321, 603, 5078, 512, 295, 264, 2141, 9102, 293, 51640], "temperature": 0.0, "avg_logprob": -0.21593652987012676, "compression_ratio": 1.6483050847457628, "no_speech_prob": 0.00035141187254339457}, {"id": 200, "seek": 136628, "start": 1366.28, "end": 1373.8799999999999, "text": " equations here. So basically the idea is that, okay, so let's see here. So we, so this is like,", "tokens": [50364, 11787, 510, 13, 407, 1936, 264, 1558, 307, 300, 11, 1392, 11, 370, 718, 311, 536, 510, 13, 407, 321, 11, 370, 341, 307, 411, 11, 50744], "temperature": 0.0, "avg_logprob": -0.2105562686920166, "compression_ratio": 1.7232704402515724, "no_speech_prob": 0.0013668701285496354}, {"id": 201, "seek": 136628, "start": 1373.8799999999999, "end": 1382.52, "text": " so this is an image that we want to generate, right? And so X0 is basically, you know,", "tokens": [50744, 370, 341, 307, 364, 3256, 300, 321, 528, 281, 8460, 11, 558, 30, 400, 370, 1783, 15, 307, 1936, 11, 291, 458, 11, 51176], "temperature": 0.0, "avg_logprob": -0.2105562686920166, "compression_ratio": 1.7232704402515724, "no_speech_prob": 0.0013668701285496354}, {"id": 202, "seek": 136628, "start": 1382.52, "end": 1389.0, "text": " these are actually the samples that we want. So we want to, X0 is what we want to generate.", "tokens": [51176, 613, 366, 767, 264, 10938, 300, 321, 528, 13, 407, 321, 528, 281, 11, 1783, 15, 307, 437, 321, 528, 281, 8460, 13, 51500], "temperature": 0.0, "avg_logprob": -0.2105562686920166, "compression_ratio": 1.7232704402515724, "no_speech_prob": 0.0013668701285496354}, {"id": 203, "seek": 138900, "start": 1389.0, "end": 1397.24, "text": " And, you know, these would be, yeah, these are images. And we start out with pure noise.", "tokens": [50364, 400, 11, 291, 458, 11, 613, 576, 312, 11, 1338, 11, 613, 366, 5267, 13, 400, 321, 722, 484, 365, 6075, 5658, 13, 50776], "temperature": 0.0, "avg_logprob": -0.26451316632722555, "compression_ratio": 1.6369047619047619, "no_speech_prob": 0.0019266910385340452}, {"id": 204, "seek": 138900, "start": 1397.24, "end": 1409.88, "text": " So that's what X, uppercase T, pure noise. And the whole idea is that we have two processes.", "tokens": [50776, 407, 300, 311, 437, 1783, 11, 11775, 2869, 651, 314, 11, 6075, 5658, 13, 400, 264, 1379, 1558, 307, 300, 321, 362, 732, 7555, 13, 51408], "temperature": 0.0, "avg_logprob": -0.26451316632722555, "compression_ratio": 1.6369047619047619, "no_speech_prob": 0.0019266910385340452}, {"id": 205, "seek": 138900, "start": 1410.52, "end": 1418.44, "text": " We have this process where we're going from pure noise to our image. And we have this process", "tokens": [51440, 492, 362, 341, 1399, 689, 321, 434, 516, 490, 6075, 5658, 281, 527, 3256, 13, 400, 321, 362, 341, 1399, 51836], "temperature": 0.0, "avg_logprob": -0.26451316632722555, "compression_ratio": 1.6369047619047619, "no_speech_prob": 0.0019266910385340452}, {"id": 206, "seek": 141844, "start": 1418.44, "end": 1424.3600000000001, "text": " within from our image to pure noise. So the process where we're going from our image to pure noise,", "tokens": [50364, 1951, 490, 527, 3256, 281, 6075, 5658, 13, 407, 264, 1399, 689, 321, 434, 516, 490, 527, 3256, 281, 6075, 5658, 11, 50660], "temperature": 0.0, "avg_logprob": -0.25001656381707443, "compression_ratio": 1.7314814814814814, "no_speech_prob": 0.00010889475379372016}, {"id": 207, "seek": 141844, "start": 1424.3600000000001, "end": 1432.8400000000001, "text": " this is called the forward process. Forward, sorry, my typing is, my handwriting is not so good in it.", "tokens": [50660, 341, 307, 1219, 264, 2128, 1399, 13, 35524, 11, 2597, 11, 452, 18444, 307, 11, 452, 39179, 307, 406, 370, 665, 294, 309, 13, 51084], "temperature": 0.0, "avg_logprob": -0.25001656381707443, "compression_ratio": 1.7314814814814814, "no_speech_prob": 0.00010889475379372016}, {"id": 208, "seek": 141844, "start": 1432.8400000000001, "end": 1439.4, "text": " So hopefully it's clear enough. Let me know if it's not. So we have the forward process,", "tokens": [51084, 407, 4696, 309, 311, 1850, 1547, 13, 961, 385, 458, 498, 309, 311, 406, 13, 407, 321, 362, 264, 2128, 1399, 11, 51412], "temperature": 0.0, "avg_logprob": -0.25001656381707443, "compression_ratio": 1.7314814814814814, "no_speech_prob": 0.00010889475379372016}, {"id": 209, "seek": 141844, "start": 1439.4, "end": 1445.4, "text": " which is mostly just used for our cleaning. Then we also have our reverse process.", "tokens": [51412, 597, 307, 5240, 445, 1143, 337, 527, 8924, 13, 1396, 321, 611, 362, 527, 9943, 1399, 13, 51712], "temperature": 0.0, "avg_logprob": -0.25001656381707443, "compression_ratio": 1.7314814814814814, "no_speech_prob": 0.00010889475379372016}, {"id": 210, "seek": 144540, "start": 1445.4, "end": 1455.8000000000002, "text": " So this is the reverse process. Should I go right up here? Reverse process.", "tokens": [50364, 407, 341, 307, 264, 9943, 1399, 13, 6454, 286, 352, 558, 493, 510, 30, 26314, 405, 1399, 13, 50884], "temperature": 0.0, "avg_logprob": -0.320302734375, "compression_ratio": 1.488888888888889, "no_speech_prob": 0.0006263097748160362}, {"id": 211, "seek": 144540, "start": 1455.8000000000002, "end": 1463.88, "text": " So this is a bit of a summary, I guess, of what you and Waseem talked about in lesson 9b, right?", "tokens": [50884, 407, 341, 307, 257, 857, 295, 257, 12691, 11, 286, 2041, 11, 295, 437, 291, 293, 343, 651, 443, 2825, 466, 294, 6898, 1722, 65, 11, 558, 30, 51288], "temperature": 0.0, "avg_logprob": -0.320302734375, "compression_ratio": 1.488888888888889, "no_speech_prob": 0.0006263097748160362}, {"id": 212, "seek": 144540, "start": 1464.68, "end": 1471.3200000000002, "text": " Yes. And just, it's just mostly to highlight now what are the different variables as we look at", "tokens": [51328, 1079, 13, 400, 445, 11, 309, 311, 445, 5240, 281, 5078, 586, 437, 366, 264, 819, 9102, 382, 321, 574, 412, 51660], "temperature": 0.0, "avg_logprob": -0.320302734375, "compression_ratio": 1.488888888888889, "no_speech_prob": 0.0006263097748160362}, {"id": 213, "seek": 147132, "start": 1471.96, "end": 1477.72, "text": " the code and see, you know, the different variables in the code. Okay, so we'll be focusing today on", "tokens": [50396, 264, 3089, 293, 536, 11, 291, 458, 11, 264, 819, 9102, 294, 264, 3089, 13, 1033, 11, 370, 321, 603, 312, 8416, 965, 322, 50684], "temperature": 0.0, "avg_logprob": -0.26157258987426757, "compression_ratio": 1.7568807339449541, "no_speech_prob": 0.002631575334817171}, {"id": 214, "seek": 147132, "start": 1477.72, "end": 1483.24, "text": " the code, but the code will be referring to things by name, and those names won't make sense very", "tokens": [50684, 264, 3089, 11, 457, 264, 3089, 486, 312, 13761, 281, 721, 538, 1315, 11, 293, 729, 5288, 1582, 380, 652, 2020, 588, 50960], "temperature": 0.0, "avg_logprob": -0.26157258987426757, "compression_ratio": 1.7568807339449541, "no_speech_prob": 0.002631575334817171}, {"id": 215, "seek": 147132, "start": 1483.24, "end": 1492.28, "text": " much unless we see what they're used for in the math. Okay. I won't dive too much into the math.", "tokens": [50960, 709, 5969, 321, 536, 437, 436, 434, 1143, 337, 294, 264, 5221, 13, 1033, 13, 286, 1582, 380, 9192, 886, 709, 666, 264, 5221, 13, 51412], "temperature": 0.0, "avg_logprob": -0.26157258987426757, "compression_ratio": 1.7568807339449541, "no_speech_prob": 0.002631575334817171}, {"id": 216, "seek": 147132, "start": 1492.28, "end": 1496.9199999999998, "text": " I just want to focus on these sorts of variables and equations that we see in the code.", "tokens": [51412, 286, 445, 528, 281, 1879, 322, 613, 7527, 295, 9102, 293, 11787, 300, 321, 536, 294, 264, 3089, 13, 51644], "temperature": 0.0, "avg_logprob": -0.26157258987426757, "compression_ratio": 1.7568807339449541, "no_speech_prob": 0.002631575334817171}, {"id": 217, "seek": 149692, "start": 1497.64, "end": 1504.52, "text": " So basically, the general idea is that we do these in multiple different steps.", "tokens": [50400, 407, 1936, 11, 264, 2674, 1558, 307, 300, 321, 360, 613, 294, 3866, 819, 4439, 13, 50744], "temperature": 0.0, "avg_logprob": -0.30773027637336825, "compression_ratio": 1.6102564102564103, "no_speech_prob": 0.0037068172823637724}, {"id": 218, "seek": 149692, "start": 1504.52, "end": 1511.64, "text": " We have here from time step 0 all the way to time steps, uppercase T.", "tokens": [50744, 492, 362, 510, 490, 565, 1823, 1958, 439, 264, 636, 281, 565, 4439, 11, 11775, 2869, 651, 314, 13, 51100], "temperature": 0.0, "avg_logprob": -0.30773027637336825, "compression_ratio": 1.6102564102564103, "no_speech_prob": 0.0037068172823637724}, {"id": 219, "seek": 149692, "start": 1511.64, "end": 1516.68, "text": " And so there's some fixed number of steps, but then we have this intermediate process where", "tokens": [51100, 400, 370, 456, 311, 512, 6806, 1230, 295, 4439, 11, 457, 550, 321, 362, 341, 19376, 1399, 689, 51352], "temperature": 0.0, "avg_logprob": -0.30773027637336825, "compression_ratio": 1.6102564102564103, "no_speech_prob": 0.0037068172823637724}, {"id": 220, "seek": 149692, "start": 1516.68, "end": 1523.24, "text": " we're going from some particular time step. Yeah, we have this time step", "tokens": [51352, 321, 434, 516, 490, 512, 1729, 565, 1823, 13, 865, 11, 321, 362, 341, 565, 1823, 51680], "temperature": 0.0, "avg_logprob": -0.30773027637336825, "compression_ratio": 1.6102564102564103, "no_speech_prob": 0.0037068172823637724}, {"id": 221, "seek": 152324, "start": 1523.72, "end": 1535.32, "text": " at lowercase t, which is noisy image. And yes, we're transitioning between these two", "tokens": [50388, 412, 3126, 9765, 256, 11, 597, 307, 24518, 3256, 13, 400, 2086, 11, 321, 434, 33777, 1296, 613, 732, 50968], "temperature": 0.0, "avg_logprob": -0.27227090752643085, "compression_ratio": 1.7549019607843137, "no_speech_prob": 5.6497257901355624e-05}, {"id": 222, "seek": 152324, "start": 1535.32, "end": 1539.08, "text": " different noisy images. So we have this, what is sometimes called the transition.", "tokens": [50968, 819, 24518, 5267, 13, 407, 321, 362, 341, 11, 437, 307, 2171, 1219, 264, 6034, 13, 51156], "temperature": 0.0, "avg_logprob": -0.27227090752643085, "compression_ratio": 1.7549019607843137, "no_speech_prob": 5.6497257901355624e-05}, {"id": 223, "seek": 152324, "start": 1539.72, "end": 1544.44, "text": " We have this one here. This is like sometimes called the transition kernel, or yeah, whatever", "tokens": [51188, 492, 362, 341, 472, 510, 13, 639, 307, 411, 2171, 1219, 264, 6034, 28256, 11, 420, 1338, 11, 2035, 51424], "temperature": 0.0, "avg_logprob": -0.27227090752643085, "compression_ratio": 1.7549019607843137, "no_speech_prob": 5.6497257901355624e-05}, {"id": 224, "seek": 152324, "start": 1544.44, "end": 1550.28, "text": " it is, it basically is just telling us, you know, how do we go from, you know, one, in this case,", "tokens": [51424, 309, 307, 11, 309, 1936, 307, 445, 3585, 505, 11, 291, 458, 11, 577, 360, 321, 352, 490, 11, 291, 458, 11, 472, 11, 294, 341, 1389, 11, 51716], "temperature": 0.0, "avg_logprob": -0.27227090752643085, "compression_ratio": 1.7549019607843137, "no_speech_prob": 5.6497257901355624e-05}, {"id": 225, "seek": 155028, "start": 1550.28, "end": 1554.2, "text": " we're going from a less noisy image to a more noisy image, and then going backwards is going", "tokens": [50364, 321, 434, 516, 490, 257, 1570, 24518, 3256, 281, 257, 544, 24518, 3256, 11, 293, 550, 516, 12204, 307, 516, 50560], "temperature": 0.0, "avg_logprob": -0.23357041098854758, "compression_ratio": 1.8565891472868217, "no_speech_prob": 0.007937660440802574}, {"id": 226, "seek": 155028, "start": 1554.2, "end": 1559.48, "text": " from a more noisy image to a less noisy image. So let's look at the equations. So the forward", "tokens": [50560, 490, 257, 544, 24518, 3256, 281, 257, 1570, 24518, 3256, 13, 407, 718, 311, 574, 412, 264, 11787, 13, 407, 264, 2128, 50824], "temperature": 0.0, "avg_logprob": -0.23357041098854758, "compression_ratio": 1.8565891472868217, "no_speech_prob": 0.007937660440802574}, {"id": 227, "seek": 155028, "start": 1559.48, "end": 1564.12, "text": " direction is driven easily to make it something more noisy, just add a bit more noise to it.", "tokens": [50824, 3513, 307, 9555, 3612, 281, 652, 309, 746, 544, 24518, 11, 445, 909, 257, 857, 544, 5658, 281, 309, 13, 51056], "temperature": 0.0, "avg_logprob": -0.23357041098854758, "compression_ratio": 1.8565891472868217, "no_speech_prob": 0.007937660440802574}, {"id": 228, "seek": 155028, "start": 1564.12, "end": 1569.32, "text": " And the reverse direction is incredibly difficult, which is to particularly to go from the far left", "tokens": [51056, 400, 264, 9943, 3513, 307, 6252, 2252, 11, 597, 307, 281, 4098, 281, 352, 490, 264, 1400, 1411, 51316], "temperature": 0.0, "avg_logprob": -0.23357041098854758, "compression_ratio": 1.8565891472868217, "no_speech_prob": 0.007937660440802574}, {"id": 229, "seek": 155028, "start": 1569.32, "end": 1576.2, "text": " to the far right is strictly speaking impossible because none of that person's face exists anymore.", "tokens": [51316, 281, 264, 1400, 558, 307, 20792, 4124, 6243, 570, 6022, 295, 300, 954, 311, 1851, 8198, 3602, 13, 51660], "temperature": 0.0, "avg_logprob": -0.23357041098854758, "compression_ratio": 1.8565891472868217, "no_speech_prob": 0.007937660440802574}, {"id": 230, "seek": 157620, "start": 1576.76, "end": 1582.1200000000001, "text": " But somewhere in between, you could certainly go from something that's partially noisy to less noisy", "tokens": [50392, 583, 4079, 294, 1296, 11, 291, 727, 3297, 352, 490, 746, 300, 311, 18886, 24518, 281, 1570, 24518, 50660], "temperature": 0.0, "avg_logprob": -0.24297330875207881, "compression_ratio": 1.6059322033898304, "no_speech_prob": 0.01717549003660679}, {"id": 231, "seek": 157620, "start": 1583.24, "end": 1590.6000000000001, "text": " by a learned model. Exactly. And that's like what I'm going to write down right now in terms of,", "tokens": [50716, 538, 257, 3264, 2316, 13, 7587, 13, 400, 300, 311, 411, 437, 286, 478, 516, 281, 2464, 760, 558, 586, 294, 2115, 295, 11, 51084], "temperature": 0.0, "avg_logprob": -0.24297330875207881, "compression_ratio": 1.6059322033898304, "no_speech_prob": 0.01717549003660679}, {"id": 232, "seek": 157620, "start": 1590.6000000000001, "end": 1595.72, "text": " you know, in terms of, I guess, the symbols in the map. So yeah, basically, I'm just trying to", "tokens": [51084, 291, 458, 11, 294, 2115, 295, 11, 286, 2041, 11, 264, 16944, 294, 264, 4471, 13, 407, 1338, 11, 1936, 11, 286, 478, 445, 1382, 281, 51340], "temperature": 0.0, "avg_logprob": -0.24297330875207881, "compression_ratio": 1.6059322033898304, "no_speech_prob": 0.01717549003660679}, {"id": 233, "seek": 157620, "start": 1595.72, "end": 1601.56, "text": " pull out the, just to write down the equations here. So we have, let me zoom in a bit.", "tokens": [51340, 2235, 484, 264, 11, 445, 281, 2464, 760, 264, 11787, 510, 13, 407, 321, 362, 11, 718, 385, 8863, 294, 257, 857, 13, 51632], "temperature": 0.0, "avg_logprob": -0.24297330875207881, "compression_ratio": 1.6059322033898304, "no_speech_prob": 0.01717549003660679}, {"id": 234, "seek": 160156, "start": 1601.8799999999999, "end": 1616.04, "text": " On. Yeah, so we have our two, oops, let's see here. Okay. Two of XT, XT minus one.", "tokens": [50380, 1282, 13, 865, 11, 370, 321, 362, 527, 732, 11, 34166, 11, 718, 311, 536, 510, 13, 1033, 13, 4453, 295, 1783, 51, 11, 1783, 51, 3175, 472, 13, 51088], "temperature": 0.0, "avg_logprob": -0.35349483489990235, "compression_ratio": 1.335820895522388, "no_speech_prob": 0.013846365734934807}, {"id": 235, "seek": 160156, "start": 1618.12, "end": 1626.28, "text": " Or actually, you know what, maybe it's just better if I just snip. Yeah, just snip it from here.", "tokens": [51192, 1610, 767, 11, 291, 458, 437, 11, 1310, 309, 311, 445, 1101, 498, 286, 445, 37482, 13, 865, 11, 445, 37482, 309, 490, 510, 13, 51600], "temperature": 0.0, "avg_logprob": -0.35349483489990235, "compression_ratio": 1.335820895522388, "no_speech_prob": 0.013846365734934807}, {"id": 236, "seek": 162628, "start": 1626.92, "end": 1630.28, "text": " So the one that is going from our,", "tokens": [50396, 407, 264, 472, 300, 307, 516, 490, 527, 11, 50564], "temperature": 0.0, "avg_logprob": -0.2766537747140658, "compression_ratio": 1.5815602836879432, "no_speech_prob": 0.0013669600011780858}, {"id": 237, "seek": 162628, "start": 1634.12, "end": 1641.56, "text": " the one that is going from our forward process is this equation here. So I'll just make that a", "tokens": [50756, 264, 472, 300, 307, 516, 490, 527, 2128, 1399, 307, 341, 5367, 510, 13, 407, 286, 603, 445, 652, 300, 257, 51128], "temperature": 0.0, "avg_logprob": -0.2766537747140658, "compression_ratio": 1.5815602836879432, "no_speech_prob": 0.0013669600011780858}, {"id": 238, "seek": 162628, "start": 1641.56, "end": 1650.36, "text": " little smaller for you guys. Just so right there. So that is going, and basically to explain,", "tokens": [51128, 707, 4356, 337, 291, 1074, 13, 1449, 370, 558, 456, 13, 407, 300, 307, 516, 11, 293, 1936, 281, 2903, 11, 51568], "temperature": 0.0, "avg_logprob": -0.2766537747140658, "compression_ratio": 1.5815602836879432, "no_speech_prob": 0.0013669600011780858}, {"id": 239, "seek": 165036, "start": 1650.36, "end": 1659.3999999999999, "text": " we have this sort of script, a little bit of a, maybe a little bit confusing notation here,", "tokens": [50364, 321, 362, 341, 1333, 295, 5755, 11, 257, 707, 857, 295, 257, 11, 1310, 257, 707, 857, 13181, 24657, 510, 11, 50816], "temperature": 0.0, "avg_logprob": -0.21786249251592726, "compression_ratio": 1.8564102564102565, "no_speech_prob": 0.00023413221060764045}, {"id": 240, "seek": 165036, "start": 1659.3999999999999, "end": 1663.9599999999998, "text": " but basically this is referring to a normal distribution or a Gaussian distribution.", "tokens": [50816, 457, 1936, 341, 307, 13761, 281, 257, 2710, 7316, 420, 257, 39148, 7316, 13, 51044], "temperature": 0.0, "avg_logprob": -0.21786249251592726, "compression_ratio": 1.8564102564102565, "no_speech_prob": 0.00023413221060764045}, {"id": 241, "seek": 165036, "start": 1665.08, "end": 1670.04, "text": " And this is just saying, okay, this is a Gaussian distribution that's describing this particular", "tokens": [51100, 400, 341, 307, 445, 1566, 11, 1392, 11, 341, 307, 257, 39148, 7316, 300, 311, 16141, 341, 1729, 51348], "temperature": 0.0, "avg_logprob": -0.21786249251592726, "compression_ratio": 1.8564102564102565, "no_speech_prob": 0.00023413221060764045}, {"id": 242, "seek": 165036, "start": 1670.04, "end": 1676.84, "text": " variable. So it's just saying, okay, you know, N is our normal or Gaussian distribution,", "tokens": [51348, 7006, 13, 407, 309, 311, 445, 1566, 11, 1392, 11, 291, 458, 11, 426, 307, 527, 2710, 420, 39148, 7316, 11, 51688], "temperature": 0.0, "avg_logprob": -0.21786249251592726, "compression_ratio": 1.8564102564102565, "no_speech_prob": 0.00023413221060764045}, {"id": 243, "seek": 167684, "start": 1676.84, "end": 1686.4399999999998, "text": " and it's representing this variable X of T, or X, sorry, XT. And then we have here is the mean,", "tokens": [50364, 293, 309, 311, 13460, 341, 7006, 1783, 295, 314, 11, 420, 1783, 11, 2597, 11, 1783, 51, 13, 400, 550, 321, 362, 510, 307, 264, 914, 11, 50844], "temperature": 0.0, "avg_logprob": -0.21972821833013179, "compression_ratio": 1.5765306122448979, "no_speech_prob": 4.611249460140243e-05}, {"id": 244, "seek": 167684, "start": 1689.1599999999999, "end": 1690.76, "text": " and this is the variance.", "tokens": [50980, 293, 341, 307, 264, 21977, 13, 51060], "temperature": 0.0, "avg_logprob": -0.21972821833013179, "compression_ratio": 1.5765306122448979, "no_speech_prob": 4.611249460140243e-05}, {"id": 245, "seek": 167684, "start": 1694.76, "end": 1700.12, "text": " So just to again clarify, I think we've talked about this before as well, but like, you know,", "tokens": [51260, 407, 445, 281, 797, 17594, 11, 286, 519, 321, 600, 2825, 466, 341, 949, 382, 731, 11, 457, 411, 11, 291, 458, 11, 51528], "temperature": 0.0, "avg_logprob": -0.21972821833013179, "compression_ratio": 1.5765306122448979, "no_speech_prob": 4.611249460140243e-05}, {"id": 246, "seek": 167684, "start": 1700.12, "end": 1704.36, "text": " this is a, you know, this is of course a bad drawing of a Gaussian, but you know, our mean is", "tokens": [51528, 341, 307, 257, 11, 291, 458, 11, 341, 307, 295, 1164, 257, 1578, 6316, 295, 257, 39148, 11, 457, 291, 458, 11, 527, 914, 307, 51740], "temperature": 0.0, "avg_logprob": -0.21972821833013179, "compression_ratio": 1.5765306122448979, "no_speech_prob": 4.611249460140243e-05}, {"id": 247, "seek": 170436, "start": 1704.4399999999998, "end": 1711.1599999999999, "text": " just, oops, our mean is just, you know, this, you know, the middle point here is the mean,", "tokens": [50368, 445, 11, 34166, 11, 527, 914, 307, 445, 11, 291, 458, 11, 341, 11, 291, 458, 11, 264, 2808, 935, 510, 307, 264, 914, 11, 50704], "temperature": 0.0, "avg_logprob": -0.23564839625096584, "compression_ratio": 1.7259615384615385, "no_speech_prob": 0.00010720783757278696}, {"id": 248, "seek": 170436, "start": 1711.1599999999999, "end": 1717.8, "text": " and the variance is kind of describes the sort of spread of the Gaussian distribution. So", "tokens": [50704, 293, 264, 21977, 307, 733, 295, 15626, 264, 1333, 295, 3974, 295, 264, 39148, 7316, 13, 407, 51036], "temperature": 0.0, "avg_logprob": -0.23564839625096584, "compression_ratio": 1.7259615384615385, "no_speech_prob": 0.00010720783757278696}, {"id": 249, "seek": 170436, "start": 1718.76, "end": 1723.3999999999999, "text": " if you think about this a little further, you have this beta, which is one of the", "tokens": [51084, 498, 291, 519, 466, 341, 257, 707, 3052, 11, 291, 362, 341, 9861, 11, 597, 307, 472, 295, 264, 51316], "temperature": 0.0, "avg_logprob": -0.23564839625096584, "compression_ratio": 1.7259615384615385, "no_speech_prob": 0.00010720783757278696}, {"id": 250, "seek": 170436, "start": 1723.3999999999999, "end": 1731.08, "text": " important variables that kind of describes the diffusion process, beta.T. So you'll see the beta", "tokens": [51316, 1021, 9102, 300, 733, 295, 15626, 264, 25242, 1399, 11, 9861, 13, 51, 13, 407, 291, 603, 536, 264, 9861, 51700], "temperature": 0.0, "avg_logprob": -0.23564839625096584, "compression_ratio": 1.7259615384615385, "no_speech_prob": 0.00010720783757278696}, {"id": 251, "seek": 173108, "start": 1731.08, "end": 1742.36, "text": " T in the code, and basically beta T increases as T increases. So basically your beta T will be", "tokens": [50364, 314, 294, 264, 3089, 11, 293, 1936, 9861, 314, 8637, 382, 314, 8637, 13, 407, 1936, 428, 9861, 314, 486, 312, 50928], "temperature": 0.0, "avg_logprob": -0.19243832224423124, "compression_ratio": 1.8894736842105264, "no_speech_prob": 0.002115634037181735}, {"id": 252, "seek": 173108, "start": 1742.36, "end": 1748.4399999999998, "text": " greater than your beta T minus one. So if you think about that a little bit more carefully,", "tokens": [50928, 5044, 813, 428, 9861, 314, 3175, 472, 13, 407, 498, 291, 519, 466, 300, 257, 707, 857, 544, 7500, 11, 51232], "temperature": 0.0, "avg_logprob": -0.19243832224423124, "compression_ratio": 1.8894736842105264, "no_speech_prob": 0.002115634037181735}, {"id": 253, "seek": 173108, "start": 1748.4399999999998, "end": 1754.28, "text": " you can see that, okay, so at, you know, T minus one, at this time point here,", "tokens": [51232, 291, 393, 536, 300, 11, 1392, 11, 370, 412, 11, 291, 458, 11, 314, 3175, 472, 11, 412, 341, 565, 935, 510, 11, 51524], "temperature": 0.0, "avg_logprob": -0.19243832224423124, "compression_ratio": 1.8894736842105264, "no_speech_prob": 0.002115634037181735}, {"id": 254, "seek": 173108, "start": 1756.28, "end": 1760.4399999999998, "text": " and then you're going to the next time point, you're going to increase your beta T, so you're", "tokens": [51624, 293, 550, 291, 434, 516, 281, 264, 958, 565, 935, 11, 291, 434, 516, 281, 3488, 428, 9861, 314, 11, 370, 291, 434, 51832], "temperature": 0.0, "avg_logprob": -0.19243832224423124, "compression_ratio": 1.8894736842105264, "no_speech_prob": 0.002115634037181735}, {"id": 255, "seek": 176044, "start": 1760.44, "end": 1766.04, "text": " increasing the variance, but then you have this one minus beta T, and take the square root of that,", "tokens": [50364, 5662, 264, 21977, 11, 457, 550, 291, 362, 341, 472, 3175, 9861, 314, 11, 293, 747, 264, 3732, 5593, 295, 300, 11, 50644], "temperature": 0.0, "avg_logprob": -0.21725621716729526, "compression_ratio": 1.7979274611398963, "no_speech_prob": 0.0003982108610216528}, {"id": 256, "seek": 176044, "start": 1767.16, "end": 1775.0800000000002, "text": " and multiply it by XT minus one. So as your T is increasing, this term actually decreases.", "tokens": [50700, 293, 12972, 309, 538, 1783, 51, 3175, 472, 13, 407, 382, 428, 314, 307, 5662, 11, 341, 1433, 767, 24108, 13, 51096], "temperature": 0.0, "avg_logprob": -0.21725621716729526, "compression_ratio": 1.7979274611398963, "no_speech_prob": 0.0003982108610216528}, {"id": 257, "seek": 176044, "start": 1775.0800000000002, "end": 1779.56, "text": " So your mean is actually decreasing, and you're getting less of the original image,", "tokens": [51096, 407, 428, 914, 307, 767, 23223, 11, 293, 291, 434, 1242, 1570, 295, 264, 3380, 3256, 11, 51320], "temperature": 0.0, "avg_logprob": -0.21725621716729526, "compression_ratio": 1.7979274611398963, "no_speech_prob": 0.0003982108610216528}, {"id": 258, "seek": 176044, "start": 1779.56, "end": 1784.3600000000001, "text": " because the original image is clearly part of XT minus one. So as you...", "tokens": [51320, 570, 264, 3380, 3256, 307, 4448, 644, 295, 1783, 51, 3175, 472, 13, 407, 382, 291, 485, 51560], "temperature": 0.0, "avg_logprob": -0.21725621716729526, "compression_ratio": 1.7979274611398963, "no_speech_prob": 0.0003982108610216528}, {"id": 259, "seek": 178436, "start": 1784.52, "end": 1792.12, "text": " Just to let you know, Kanishka, just to let you know, we can't see your pointer. So if you want", "tokens": [50372, 1449, 281, 718, 291, 458, 11, 591, 7524, 2330, 11, 445, 281, 718, 291, 458, 11, 321, 393, 380, 536, 428, 23918, 13, 407, 498, 291, 528, 50752], "temperature": 0.0, "avg_logprob": -0.34848922280704275, "compression_ratio": 1.597883597883598, "no_speech_prob": 0.014061891473829746}, {"id": 260, "seek": 178436, "start": 1792.12, "end": 1799.7199999999998, "text": " to point at things, you would need to highlight them or something. Yeah, so I'll just, let's see, yeah.", "tokens": [50752, 281, 935, 412, 721, 11, 291, 576, 643, 281, 5078, 552, 420, 746, 13, 865, 11, 370, 286, 603, 445, 11, 718, 311, 536, 11, 1338, 13, 51132], "temperature": 0.0, "avg_logprob": -0.34848922280704275, "compression_ratio": 1.597883597883598, "no_speech_prob": 0.014061891473829746}, {"id": 261, "seek": 178436, "start": 1802.04, "end": 1807.1599999999999, "text": " Or, yeah, basically, I mean, I wasn't particularly pointing at anything in specific, I was just saying", "tokens": [51248, 1610, 11, 1338, 11, 1936, 11, 286, 914, 11, 286, 2067, 380, 4098, 12166, 412, 1340, 294, 2685, 11, 286, 390, 445, 1566, 51504], "temperature": 0.0, "avg_logprob": -0.34848922280704275, "compression_ratio": 1.597883597883598, "no_speech_prob": 0.014061891473829746}, {"id": 262, "seek": 180716, "start": 1807.48, "end": 1814.68, "text": " that, yeah, basically, if we have our X of T here, as the time step increases,", "tokens": [50380, 300, 11, 1338, 11, 1936, 11, 498, 321, 362, 527, 1783, 295, 314, 510, 11, 382, 264, 565, 1823, 8637, 11, 50740], "temperature": 0.0, "avg_logprob": -0.20660410536096452, "compression_ratio": 1.8137254901960784, "no_speech_prob": 0.0008830309379845858}, {"id": 263, "seek": 180716, "start": 1816.0400000000002, "end": 1822.44, "text": " you know, you're getting less contribution from your XT minus one, and so that means your", "tokens": [50808, 291, 458, 11, 291, 434, 1242, 1570, 13150, 490, 428, 1783, 51, 3175, 472, 11, 293, 370, 300, 1355, 428, 51128], "temperature": 0.0, "avg_logprob": -0.20660410536096452, "compression_ratio": 1.8137254901960784, "no_speech_prob": 0.0008830309379845858}, {"id": 264, "seek": 180716, "start": 1822.44, "end": 1827.24, "text": " mean is going towards zero, and so you're going to have a mean of zero, and, you know, the variance", "tokens": [51128, 914, 307, 516, 3030, 4018, 11, 293, 370, 291, 434, 516, 281, 362, 257, 914, 295, 4018, 11, 293, 11, 291, 458, 11, 264, 21977, 51368], "temperature": 0.0, "avg_logprob": -0.20660410536096452, "compression_ratio": 1.8137254901960784, "no_speech_prob": 0.0008830309379845858}, {"id": 265, "seek": 180716, "start": 1827.24, "end": 1831.8000000000002, "text": " keeps increasing, and basically, you just have a Gaussian distribution, and you lose any contribution", "tokens": [51368, 5965, 5662, 11, 293, 1936, 11, 291, 445, 362, 257, 39148, 7316, 11, 293, 291, 3624, 604, 13150, 51596], "temperature": 0.0, "avg_logprob": -0.20660410536096452, "compression_ratio": 1.8137254901960784, "no_speech_prob": 0.0008830309379845858}, {"id": 266, "seek": 183180, "start": 1831.8, "end": 1837.8799999999999, "text": " from the original image as your time step increases. So that's why, when we start out from X of zero,", "tokens": [50364, 490, 264, 3380, 3256, 382, 428, 565, 1823, 8637, 13, 407, 300, 311, 983, 11, 562, 321, 722, 484, 490, 1783, 295, 4018, 11, 50668], "temperature": 0.0, "avg_logprob": -0.21988457441329956, "compression_ratio": 1.6711111111111112, "no_speech_prob": 0.00818712543696165}, {"id": 267, "seek": 183180, "start": 1837.8799999999999, "end": 1843.48, "text": " and go all the way to our X of T here, this becomes pure noise. It's because we're doing this", "tokens": [50668, 293, 352, 439, 264, 636, 281, 527, 1783, 295, 314, 510, 11, 341, 3643, 6075, 5658, 13, 467, 311, 570, 321, 434, 884, 341, 50948], "temperature": 0.0, "avg_logprob": -0.21988457441329956, "compression_ratio": 1.6711111111111112, "no_speech_prob": 0.00818712543696165}, {"id": 268, "seek": 183180, "start": 1843.48, "end": 1847.8799999999999, "text": " iterative process where we keep adding noise, we lose that contribution from the original image,", "tokens": [50948, 17138, 1166, 1399, 689, 321, 1066, 5127, 5658, 11, 321, 3624, 300, 13150, 490, 264, 3380, 3256, 11, 51168], "temperature": 0.0, "avg_logprob": -0.21988457441329956, "compression_ratio": 1.6711111111111112, "no_speech_prob": 0.00818712543696165}, {"id": 269, "seek": 183180, "start": 1847.8799999999999, "end": 1855.8799999999999, "text": " and, you know, that leads to the image having pure noise at the end of the process.", "tokens": [51168, 293, 11, 291, 458, 11, 300, 6689, 281, 264, 3256, 1419, 6075, 5658, 412, 264, 917, 295, 264, 1399, 13, 51568], "temperature": 0.0, "avg_logprob": -0.21988457441329956, "compression_ratio": 1.6711111111111112, "no_speech_prob": 0.00818712543696165}, {"id": 270, "seek": 185588, "start": 1856.6000000000001, "end": 1865.16, "text": " So just something I find useful here is to consider one extreme, which is to consider", "tokens": [50400, 407, 445, 746, 286, 915, 4420, 510, 307, 281, 1949, 472, 8084, 11, 597, 307, 281, 1949, 50828], "temperature": 0.0, "avg_logprob": -0.26625670849437444, "compression_ratio": 1.6167664670658684, "no_speech_prob": 0.011685943230986595}, {"id": 271, "seek": 185588, "start": 1866.1200000000001, "end": 1875.4, "text": " X1. So at X1, the mean is going to be root one minus beta T times X naught, and the reason", "tokens": [50876, 1783, 16, 13, 407, 412, 1783, 16, 11, 264, 914, 307, 516, 281, 312, 5593, 472, 3175, 9861, 314, 1413, 1783, 13138, 11, 293, 264, 1778, 51340], "temperature": 0.0, "avg_logprob": -0.26625670849437444, "compression_ratio": 1.6167664670658684, "no_speech_prob": 0.011685943230986595}, {"id": 272, "seek": 185588, "start": 1875.4, "end": 1881.16, "text": " that's interesting is X naught is the original image. So we're taking the original image, and", "tokens": [51340, 300, 311, 1880, 307, 1783, 13138, 307, 264, 3380, 3256, 13, 407, 321, 434, 1940, 264, 3380, 3256, 11, 293, 51628], "temperature": 0.0, "avg_logprob": -0.26625670849437444, "compression_ratio": 1.6167664670658684, "no_speech_prob": 0.011685943230986595}, {"id": 273, "seek": 188116, "start": 1881.72, "end": 1890.52, "text": " at this point, one minus beta T will be pretty close to one. So at X1, we're going to have something", "tokens": [50392, 412, 341, 935, 11, 472, 3175, 9861, 314, 486, 312, 1238, 1998, 281, 472, 13, 407, 412, 1783, 16, 11, 321, 434, 516, 281, 362, 746, 50832], "temperature": 0.0, "avg_logprob": -0.20839485895066034, "compression_ratio": 1.7757847533632287, "no_speech_prob": 0.0002611859526950866}, {"id": 274, "seek": 188116, "start": 1890.52, "end": 1897.16, "text": " that's the mean is very close to the image, and the variance will be very small, and so that's why", "tokens": [50832, 300, 311, 264, 914, 307, 588, 1998, 281, 264, 3256, 11, 293, 264, 21977, 486, 312, 588, 1359, 11, 293, 370, 300, 311, 983, 51164], "temperature": 0.0, "avg_logprob": -0.20839485895066034, "compression_ratio": 1.7757847533632287, "no_speech_prob": 0.0002611859526950866}, {"id": 275, "seek": 188116, "start": 1897.16, "end": 1905.24, "text": " we will have an image that just has a tiny bit of noise. Right, right. And then another thing that", "tokens": [51164, 321, 486, 362, 364, 3256, 300, 445, 575, 257, 5870, 857, 295, 5658, 13, 1779, 11, 558, 13, 400, 550, 1071, 551, 300, 51568], "temperature": 0.0, "avg_logprob": -0.20839485895066034, "compression_ratio": 1.7757847533632287, "no_speech_prob": 0.0002611859526950866}, {"id": 276, "seek": 188116, "start": 1905.24, "end": 1910.1200000000001, "text": " sometimes is easier to write out is, sometimes you can write out, in this case, you can write out", "tokens": [51568, 2171, 307, 3571, 281, 2464, 484, 307, 11, 2171, 291, 393, 2464, 484, 11, 294, 341, 1389, 11, 291, 393, 2464, 484, 51812], "temperature": 0.0, "avg_logprob": -0.20839485895066034, "compression_ratio": 1.7757847533632287, "no_speech_prob": 0.0002611859526950866}, {"id": 277, "seek": 191012, "start": 1910.12, "end": 1920.76, "text": " Q of XT directly, because these are all independent in terms of, like, Q of XT is only", "tokens": [50364, 1249, 295, 1783, 51, 3838, 11, 570, 613, 366, 439, 6695, 294, 2115, 295, 11, 411, 11, 1249, 295, 1783, 51, 307, 787, 50896], "temperature": 0.0, "avg_logprob": -0.2464596284638851, "compression_ratio": 1.900990099009901, "no_speech_prob": 0.0005192990065552294}, {"id": 278, "seek": 191012, "start": 1920.76, "end": 1926.36, "text": " dependent on XT minus one, and then XT minus one is only dependent on XT minus two, and you can,", "tokens": [50896, 12334, 322, 1783, 51, 3175, 472, 11, 293, 550, 1783, 51, 3175, 472, 307, 787, 12334, 322, 1783, 51, 3175, 732, 11, 293, 291, 393, 11, 51176], "temperature": 0.0, "avg_logprob": -0.2464596284638851, "compression_ratio": 1.900990099009901, "no_speech_prob": 0.0005192990065552294}, {"id": 279, "seek": 191012, "start": 1926.36, "end": 1931.3999999999999, "text": " you can, it's just independent, like, all each of these steps are independent. So based on, you know,", "tokens": [51176, 291, 393, 11, 309, 311, 445, 6695, 11, 411, 11, 439, 1184, 295, 613, 4439, 366, 6695, 13, 407, 2361, 322, 11, 291, 458, 11, 51428], "temperature": 0.0, "avg_logprob": -0.2464596284638851, "compression_ratio": 1.900990099009901, "no_speech_prob": 0.0005192990065552294}, {"id": 280, "seek": 191012, "start": 1931.3999999999999, "end": 1938.9199999999998, "text": " the different laws of probability, you can get your Q of XT in closed form. So yeah, that's what's", "tokens": [51428, 264, 819, 6064, 295, 8482, 11, 291, 393, 483, 428, 1249, 295, 1783, 51, 294, 5395, 1254, 13, 407, 1338, 11, 300, 311, 437, 311, 51804], "temperature": 0.0, "avg_logprob": -0.2464596284638851, "compression_ratio": 1.900990099009901, "no_speech_prob": 0.0005192990065552294}, {"id": 281, "seek": 193892, "start": 1939.0, "end": 1944.52, "text": " shown here. Q of XT given the original image. So this is also another way of kind of seeing this", "tokens": [50368, 4898, 510, 13, 1249, 295, 1783, 51, 290, 592, 268, 264, 3380, 3256, 13, 407, 341, 307, 611, 1071, 636, 295, 733, 295, 2577, 341, 50644], "temperature": 0.0, "avg_logprob": -0.25194609919680827, "compression_ratio": 1.608187134502924, "no_speech_prob": 0.001896791160106659}, {"id": 282, "seek": 193892, "start": 1944.52, "end": 1954.76, "text": " more clearly, where you can see, you can see that. Anyway, so I'm going back here. Yeah, so", "tokens": [50644, 544, 4448, 11, 689, 291, 393, 536, 11, 291, 393, 536, 300, 13, 5684, 11, 370, 286, 478, 516, 646, 510, 13, 865, 11, 370, 51156], "temperature": 0.0, "avg_logprob": -0.25194609919680827, "compression_ratio": 1.608187134502924, "no_speech_prob": 0.001896791160106659}, {"id": 283, "seek": 193892, "start": 1956.04, "end": 1963.3200000000002, "text": " this is another way to see here more directly. So this is, of course, our clean image,", "tokens": [51220, 341, 307, 1071, 636, 281, 536, 510, 544, 3838, 13, 407, 341, 307, 11, 295, 1164, 11, 527, 2541, 3256, 11, 51584], "temperature": 0.0, "avg_logprob": -0.25194609919680827, "compression_ratio": 1.608187134502924, "no_speech_prob": 0.001896791160106659}, {"id": 284, "seek": 196332, "start": 1963.8799999999999, "end": 1975.32, "text": " and this is our, just make that all clear, our noisy image. And so you can also see again,", "tokens": [50392, 293, 341, 307, 527, 11, 445, 652, 300, 439, 1850, 11, 527, 24518, 3256, 13, 400, 370, 291, 393, 611, 536, 797, 11, 50964], "temperature": 0.0, "avg_logprob": -0.3907162769731269, "compression_ratio": 1.470899470899471, "no_speech_prob": 0.0031724912114441395}, {"id": 285, "seek": 196332, "start": 1975.8799999999999, "end": 1984.76, "text": " now alpha bar T is dependent on beta T. Basically, it's like one minus, like, the cumulative,", "tokens": [50992, 586, 8961, 2159, 314, 307, 12334, 322, 9861, 314, 13, 8537, 11, 309, 311, 411, 472, 3175, 11, 411, 11, 264, 38379, 11, 51436], "temperature": 0.0, "avg_logprob": -0.3907162769731269, "compression_ratio": 1.470899470899471, "no_speech_prob": 0.0031724912114441395}, {"id": 286, "seek": 196332, "start": 1986.04, "end": 1991.1599999999999, "text": " this is, I mean, we'll see the code for it, I guess, so maybe, yes, yes. So it might be clear", "tokens": [51500, 341, 307, 11, 286, 914, 11, 321, 603, 536, 264, 3089, 337, 309, 11, 286, 2041, 11, 370, 1310, 11, 2086, 11, 2086, 13, 407, 309, 1062, 312, 1850, 51756], "temperature": 0.0, "avg_logprob": -0.3907162769731269, "compression_ratio": 1.470899470899471, "no_speech_prob": 0.0031724912114441395}, {"id": 287, "seek": 199116, "start": 1992.1200000000001, "end": 1997.3200000000002, "text": " to see that this is alpha bar T or something like this. But basically, basically, the idea is that", "tokens": [50412, 281, 536, 300, 341, 307, 8961, 2159, 314, 420, 746, 411, 341, 13, 583, 1936, 11, 1936, 11, 264, 1558, 307, 300, 50672], "temperature": 0.0, "avg_logprob": -0.23682575599820005, "compression_ratio": 2.0471204188481678, "no_speech_prob": 0.00033014820655807853}, {"id": 288, "seek": 199116, "start": 1997.3200000000002, "end": 2007.88, "text": " alpha bar T, alpha bar T is going to be, again, less, this is what is going to be less than alpha", "tokens": [50672, 8961, 2159, 314, 11, 8961, 2159, 314, 307, 516, 281, 312, 11, 797, 11, 1570, 11, 341, 307, 437, 307, 516, 281, 312, 1570, 813, 8961, 51200], "temperature": 0.0, "avg_logprob": -0.23682575599820005, "compression_ratio": 2.0471204188481678, "no_speech_prob": 0.00033014820655807853}, {"id": 289, "seek": 199116, "start": 2007.88, "end": 2014.92, "text": " bar T minus one. So basically, alpha, this, this keeps decreasing, right? This decreases as, as", "tokens": [51200, 2159, 314, 3175, 472, 13, 407, 1936, 11, 8961, 11, 341, 11, 341, 5965, 23223, 11, 558, 30, 639, 24108, 382, 11, 382, 51552], "temperature": 0.0, "avg_logprob": -0.23682575599820005, "compression_ratio": 2.0471204188481678, "no_speech_prob": 0.00033014820655807853}, {"id": 290, "seek": 199116, "start": 2014.92, "end": 2020.3600000000001, "text": " time step increases. And on the other hand, this is going to be increasing as time step increases.", "tokens": [51552, 565, 1823, 8637, 13, 400, 322, 264, 661, 1011, 11, 341, 307, 516, 281, 312, 5662, 382, 565, 1823, 8637, 13, 51824], "temperature": 0.0, "avg_logprob": -0.23682575599820005, "compression_ratio": 2.0471204188481678, "no_speech_prob": 0.00033014820655807853}, {"id": 291, "seek": 202036, "start": 2020.36, "end": 2026.6799999999998, "text": " So again, you can see the contribution from the original image decreases as time step increases,", "tokens": [50364, 407, 797, 11, 291, 393, 536, 264, 13150, 490, 264, 3380, 3256, 24108, 382, 565, 1823, 8637, 11, 50680], "temperature": 0.0, "avg_logprob": -0.285271487393222, "compression_ratio": 1.676991150442478, "no_speech_prob": 0.0004878330510109663}, {"id": 292, "seek": 202036, "start": 2026.6799999999998, "end": 2031.9599999999998, "text": " while the noise, you know, as shown by the Dariens is increasing while, you know, the time step is", "tokens": [50680, 1339, 264, 5658, 11, 291, 458, 11, 382, 4898, 538, 264, 413, 3504, 694, 307, 5662, 1339, 11, 291, 458, 11, 264, 565, 1823, 307, 50944], "temperature": 0.0, "avg_logprob": -0.285271487393222, "compression_ratio": 1.676991150442478, "no_speech_prob": 0.0004878330510109663}, {"id": 293, "seek": 202036, "start": 2031.9599999999998, "end": 2038.12, "text": " increased. Anyway, so that hopefully clarifies the forward process. And then the reverse process is", "tokens": [50944, 6505, 13, 5684, 11, 370, 300, 4696, 6093, 11221, 264, 2128, 1399, 13, 400, 550, 264, 9943, 1399, 307, 51252], "temperature": 0.0, "avg_logprob": -0.285271487393222, "compression_ratio": 1.676991150442478, "no_speech_prob": 0.0004878330510109663}, {"id": 294, "seek": 202036, "start": 2038.12, "end": 2046.76, "text": " this basically a neural network as we, as Jeremy had mentioned. And yeah, I'll just", "tokens": [51252, 341, 1936, 257, 18161, 3209, 382, 321, 11, 382, 17809, 632, 2835, 13, 400, 1338, 11, 286, 603, 445, 51684], "temperature": 0.0, "avg_logprob": -0.285271487393222, "compression_ratio": 1.676991150442478, "no_speech_prob": 0.0004878330510109663}, {"id": 295, "seek": 204676, "start": 2047.32, "end": 2056.36, "text": " screenshot this. That's, oops, paste this. That's, yes, this is our, this is our reverse process.", "tokens": [50392, 27712, 341, 13, 663, 311, 11, 34166, 11, 9163, 341, 13, 663, 311, 11, 2086, 11, 341, 307, 527, 11, 341, 307, 527, 9943, 1399, 13, 50844], "temperature": 0.0, "avg_logprob": -0.38186341837832805, "compression_ratio": 1.7924528301886793, "no_speech_prob": 0.0002269237593282014}, {"id": 296, "seek": 204676, "start": 2056.36, "end": 2062.28, "text": " And basically, the idea is, well, this is a neural network. And this is also a neural network,", "tokens": [50844, 400, 1936, 11, 264, 1558, 307, 11, 731, 11, 341, 307, 257, 18161, 3209, 13, 400, 341, 307, 611, 257, 18161, 3209, 11, 51140], "temperature": 0.0, "avg_logprob": -0.38186341837832805, "compression_ratio": 1.7924528301886793, "no_speech_prob": 0.0002269237593282014}, {"id": 297, "seek": 204676, "start": 2062.28, "end": 2072.04, "text": " neural network. And we learned it during the training of the model. But the nice thing about", "tokens": [51140, 18161, 3209, 13, 400, 321, 3264, 309, 1830, 264, 3097, 295, 264, 2316, 13, 583, 264, 1481, 551, 466, 51628], "temperature": 0.0, "avg_logprob": -0.38186341837832805, "compression_ratio": 1.7924528301886793, "no_speech_prob": 0.0002269237593282014}, {"id": 298, "seek": 207204, "start": 2073.0, "end": 2080.2, "text": " this particular diffusion model paper that made it so simple was actually, we completely ignored", "tokens": [50412, 341, 1729, 25242, 2316, 3035, 300, 1027, 309, 370, 2199, 390, 767, 11, 321, 2584, 19735, 50772], "temperature": 0.0, "avg_logprob": -0.43682188628822244, "compression_ratio": 1.6755555555555555, "no_speech_prob": 0.0627606213092804}, {"id": 299, "seek": 207204, "start": 2080.2, "end": 2085.4, "text": " this and actually set it to constants just based on, you know, big data. We can't see what you're", "tokens": [50772, 341, 293, 767, 992, 309, 281, 35870, 445, 2361, 322, 11, 291, 458, 11, 955, 1412, 13, 492, 393, 380, 536, 437, 291, 434, 51032], "temperature": 0.0, "avg_logprob": -0.43682188628822244, "compression_ratio": 1.6755555555555555, "no_speech_prob": 0.0627606213092804}, {"id": 300, "seek": 207204, "start": 2085.4, "end": 2091.4, "text": " pointing at. So I think it's important to mention what this is here. This term here. So this one,", "tokens": [51032, 12166, 412, 13, 407, 286, 519, 309, 311, 1021, 281, 2152, 437, 341, 307, 510, 13, 639, 1433, 510, 13, 407, 341, 472, 11, 51332], "temperature": 0.0, "avg_logprob": -0.43682188628822244, "compression_ratio": 1.6755555555555555, "no_speech_prob": 0.0627606213092804}, {"id": 301, "seek": 207204, "start": 2091.4, "end": 2101.16, "text": " we just kind of ignore it. And it's just a constant dependent, dependent, dependent,", "tokens": [51332, 321, 445, 733, 295, 11200, 309, 13, 400, 309, 311, 445, 257, 5754, 12334, 11, 12334, 11, 12334, 11, 51820], "temperature": 0.0, "avg_logprob": -0.43682188628822244, "compression_ratio": 1.6755555555555555, "no_speech_prob": 0.0627606213092804}, {"id": 302, "seek": 210116, "start": 2102.12, "end": 2111.72, "text": " dependent on beta t. So you only have one neural network that you need to train, which is basically", "tokens": [50412, 12334, 322, 9861, 256, 13, 407, 291, 787, 362, 472, 18161, 3209, 300, 291, 643, 281, 3847, 11, 597, 307, 1936, 50892], "temperature": 0.0, "avg_logprob": -0.2526177565256755, "compression_ratio": 1.5260416666666667, "no_speech_prob": 0.0005112255457788706}, {"id": 303, "seek": 210116, "start": 2111.72, "end": 2119.3199999999997, "text": " referring to this mean. And when the nice thing about this diffusion model process is that it", "tokens": [50892, 13761, 281, 341, 914, 13, 400, 562, 264, 1481, 551, 466, 341, 25242, 2316, 1399, 307, 300, 309, 51272], "temperature": 0.0, "avg_logprob": -0.2526177565256755, "compression_ratio": 1.5260416666666667, "no_speech_prob": 0.0005112255457788706}, {"id": 304, "seek": 210116, "start": 2119.3199999999997, "end": 2127.3199999999997, "text": " also re-paraphrases mean into this easier form, where you do a lot of complicated math, which we'll", "tokens": [51272, 611, 319, 12, 2181, 569, 1703, 1957, 914, 666, 341, 3571, 1254, 11, 689, 291, 360, 257, 688, 295, 6179, 5221, 11, 597, 321, 603, 51672], "temperature": 0.0, "avg_logprob": -0.2526177565256755, "compression_ratio": 1.5260416666666667, "no_speech_prob": 0.0005112255457788706}, {"id": 305, "seek": 212732, "start": 2127.32, "end": 2134.84, "text": " not get into here. But basically, you get this kind of simplified training objective, where,", "tokens": [50364, 406, 483, 666, 510, 13, 583, 1936, 11, 291, 483, 341, 733, 295, 26335, 3097, 10024, 11, 689, 11, 50740], "temperature": 0.0, "avg_logprob": -0.23630274832248688, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.030672352761030197}, {"id": 306, "seek": 212732, "start": 2136.2000000000003, "end": 2143.0800000000004, "text": " let's see here. Yeah, you see the simplified training objective, you instead have this", "tokens": [50808, 718, 311, 536, 510, 13, 865, 11, 291, 536, 264, 26335, 3097, 10024, 11, 291, 2602, 362, 341, 51152], "temperature": 0.0, "avg_logprob": -0.23630274832248688, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.030672352761030197}, {"id": 307, "seek": 212732, "start": 2143.0800000000004, "end": 2153.88, "text": " epsilon beta function. And let me just screenshot that again. screenshot. This is our loss function", "tokens": [51152, 17889, 9861, 2445, 13, 400, 718, 385, 445, 27712, 300, 797, 13, 27712, 13, 639, 307, 527, 4470, 2445, 51692], "temperature": 0.0, "avg_logprob": -0.23630274832248688, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.030672352761030197}, {"id": 308, "seek": 215388, "start": 2153.88, "end": 2161.4, "text": " that we train, and we have this epsilon beta function. And you can see it's a very simple", "tokens": [50364, 300, 321, 3847, 11, 293, 321, 362, 341, 17889, 9861, 2445, 13, 400, 291, 393, 536, 309, 311, 257, 588, 2199, 50740], "temperature": 0.0, "avg_logprob": -0.2573375888899261, "compression_ratio": 1.676991150442478, "no_speech_prob": 0.010818088427186012}, {"id": 309, "seek": 215388, "start": 2161.4, "end": 2167.48, "text": " loss function, right? This is just a, let me just write this down. This is just an MSC loss.", "tokens": [50740, 4470, 2445, 11, 558, 30, 639, 307, 445, 257, 11, 718, 385, 445, 2464, 341, 760, 13, 639, 307, 445, 364, 7395, 34, 4470, 13, 51044], "temperature": 0.0, "avg_logprob": -0.2573375888899261, "compression_ratio": 1.676991150442478, "no_speech_prob": 0.010818088427186012}, {"id": 310, "seek": 215388, "start": 2168.04, "end": 2173.8, "text": " And we have this epsilon beta function here. That is our... I mean, to folks like me who are less", "tokens": [51072, 400, 321, 362, 341, 17889, 9861, 2445, 510, 13, 663, 307, 527, 485, 286, 914, 11, 281, 4024, 411, 385, 567, 366, 1570, 51360], "temperature": 0.0, "avg_logprob": -0.2573375888899261, "compression_ratio": 1.676991150442478, "no_speech_prob": 0.010818088427186012}, {"id": 311, "seek": 215388, "start": 2173.8, "end": 2178.12, "text": " mathy, it might not be obvious that it's a simple thing, because it looks quite complicated to me.", "tokens": [51360, 5221, 88, 11, 309, 1062, 406, 312, 6322, 300, 309, 311, 257, 2199, 551, 11, 570, 309, 1542, 1596, 6179, 281, 385, 13, 51576], "temperature": 0.0, "avg_logprob": -0.2573375888899261, "compression_ratio": 1.676991150442478, "no_speech_prob": 0.010818088427186012}, {"id": 312, "seek": 217812, "start": 2178.12, "end": 2185.16, "text": " But once we see it in code, it'll be simple. Yes, yes. Basically, you're just doing like,", "tokens": [50364, 583, 1564, 321, 536, 309, 294, 3089, 11, 309, 603, 312, 2199, 13, 1079, 11, 2086, 13, 8537, 11, 291, 434, 445, 884, 411, 11, 50716], "temperature": 0.0, "avg_logprob": -0.22590344412284985, "compression_ratio": 1.7431192660550459, "no_speech_prob": 0.010327612981200218}, {"id": 313, "seek": 217812, "start": 2185.16, "end": 2190.52, "text": " and you'll be, yeah, you'll see in code how simple it is. But this is like, just an MSC loss. So we've", "tokens": [50716, 293, 291, 603, 312, 11, 1338, 11, 291, 603, 536, 294, 3089, 577, 2199, 309, 307, 13, 583, 341, 307, 411, 11, 445, 364, 7395, 34, 4470, 13, 407, 321, 600, 50984], "temperature": 0.0, "avg_logprob": -0.22590344412284985, "compression_ratio": 1.7431192660550459, "no_speech_prob": 0.010327612981200218}, {"id": 314, "seek": 217812, "start": 2190.52, "end": 2197.16, "text": " seen MSC loss before, but you'll see how, yeah, this is basically MSC. So the nice, so just to", "tokens": [50984, 1612, 7395, 34, 4470, 949, 11, 457, 291, 603, 536, 577, 11, 1338, 11, 341, 307, 1936, 7395, 34, 13, 407, 264, 1481, 11, 370, 445, 281, 51316], "temperature": 0.0, "avg_logprob": -0.22590344412284985, "compression_ratio": 1.7431192660550459, "no_speech_prob": 0.010327612981200218}, {"id": 315, "seek": 217812, "start": 2197.16, "end": 2201.88, "text": " kind of take a step back again, what is this epsilon theta? Because this is like a new thing", "tokens": [51316, 733, 295, 747, 257, 1823, 646, 797, 11, 437, 307, 341, 17889, 9725, 30, 1436, 341, 307, 411, 257, 777, 551, 51552], "temperature": 0.0, "avg_logprob": -0.22590344412284985, "compression_ratio": 1.7431192660550459, "no_speech_prob": 0.010327612981200218}, {"id": 316, "seek": 220188, "start": 2201.88, "end": 2209.7200000000003, "text": " that like, seems a little bit confusing. Basically, epsilon, you can see here, basically, yeah, so", "tokens": [50364, 300, 411, 11, 2544, 257, 707, 857, 13181, 13, 8537, 11, 17889, 11, 291, 393, 536, 510, 11, 1936, 11, 1338, 11, 370, 50756], "temperature": 0.0, "avg_logprob": -0.24649140641495987, "compression_ratio": 1.699421965317919, "no_speech_prob": 0.11594849079847336}, {"id": 317, "seek": 220188, "start": 2211.32, "end": 2223.0, "text": " this here is saying, this is actually equivalent to this equation here. These two are equivalent.", "tokens": [50836, 341, 510, 307, 1566, 11, 341, 307, 767, 10344, 281, 341, 5367, 510, 13, 1981, 732, 366, 10344, 13, 51420], "temperature": 0.0, "avg_logprob": -0.24649140641495987, "compression_ratio": 1.699421965317919, "no_speech_prob": 0.11594849079847336}, {"id": 318, "seek": 220188, "start": 2223.0, "end": 2231.0, "text": " This is just another way of saying that, because basically it's saying, that's x of t. So this is", "tokens": [51420, 639, 307, 445, 1071, 636, 295, 1566, 300, 11, 570, 1936, 309, 311, 1566, 11, 300, 311, 2031, 295, 256, 13, 407, 341, 307, 51820], "temperature": 0.0, "avg_logprob": -0.24649140641495987, "compression_ratio": 1.699421965317919, "no_speech_prob": 0.11594849079847336}, {"id": 319, "seek": 223100, "start": 2231.0, "end": 2241.08, "text": " giving x of t, just in a different way. But epsilon is actually this normal distribution", "tokens": [50364, 2902, 2031, 295, 256, 11, 445, 294, 257, 819, 636, 13, 583, 17889, 307, 767, 341, 2710, 7316, 50868], "temperature": 0.0, "avg_logprob": -0.21499950327771775, "compression_ratio": 1.6318181818181818, "no_speech_prob": 0.00028684682911261916}, {"id": 320, "seek": 223100, "start": 2241.72, "end": 2246.92, "text": " with a mean of zero and a variance of one. And then you have all these scaling terms that, you", "tokens": [50900, 365, 257, 914, 295, 4018, 293, 257, 21977, 295, 472, 13, 400, 550, 291, 362, 439, 613, 21589, 2115, 300, 11, 291, 51160], "temperature": 0.0, "avg_logprob": -0.21499950327771775, "compression_ratio": 1.6318181818181818, "no_speech_prob": 0.00028684682911261916}, {"id": 321, "seek": 223100, "start": 2246.92, "end": 2253.8, "text": " know, changes the mean to be the same as this equation that we have over here. So that's,", "tokens": [51160, 458, 11, 2962, 264, 914, 281, 312, 264, 912, 382, 341, 5367, 300, 321, 362, 670, 510, 13, 407, 300, 311, 11, 51504], "temperature": 0.0, "avg_logprob": -0.21499950327771775, "compression_ratio": 1.6318181818181818, "no_speech_prob": 0.00028684682911261916}, {"id": 322, "seek": 223100, "start": 2253.8, "end": 2260.04, "text": " this is our x of t. And so what epsilon is, it's actually the noise that we're adding", "tokens": [51504, 341, 307, 527, 2031, 295, 256, 13, 400, 370, 437, 17889, 307, 11, 309, 311, 767, 264, 5658, 300, 321, 434, 5127, 51816], "temperature": 0.0, "avg_logprob": -0.21499950327771775, "compression_ratio": 1.6318181818181818, "no_speech_prob": 0.00028684682911261916}, {"id": 323, "seek": 226004, "start": 2260.12, "end": 2265.32, "text": " to our image to make it into a noisy image. And what this neural network is doing is trying to", "tokens": [50368, 281, 527, 3256, 281, 652, 309, 666, 257, 24518, 3256, 13, 400, 437, 341, 18161, 3209, 307, 884, 307, 1382, 281, 50628], "temperature": 0.0, "avg_logprob": -0.18486131940569198, "compression_ratio": 1.7701863354037266, "no_speech_prob": 5.5621338105993345e-05}, {"id": 324, "seek": 226004, "start": 2265.32, "end": 2271.96, "text": " predict that noise. So what this is actually doing is, this is actually a noise predictor.", "tokens": [50628, 6069, 300, 5658, 13, 407, 437, 341, 307, 767, 884, 307, 11, 341, 307, 767, 257, 5658, 6069, 284, 13, 50960], "temperature": 0.0, "avg_logprob": -0.18486131940569198, "compression_ratio": 1.7701863354037266, "no_speech_prob": 5.5621338105993345e-05}, {"id": 325, "seek": 226004, "start": 2272.6, "end": 2284.2, "text": " And it is predicting the noise in the image. And why is that important? Basically, the general idea", "tokens": [50992, 400, 309, 307, 32884, 264, 5658, 294, 264, 3256, 13, 400, 983, 307, 300, 1021, 30, 8537, 11, 264, 2674, 1558, 51572], "temperature": 0.0, "avg_logprob": -0.18486131940569198, "compression_ratio": 1.7701863354037266, "no_speech_prob": 5.5621338105993345e-05}, {"id": 326, "seek": 228420, "start": 2284.2, "end": 2293.24, "text": " is, like, if we were to think about our distribution of data, let's just think about", "tokens": [50364, 307, 11, 411, 11, 498, 321, 645, 281, 519, 466, 527, 7316, 295, 1412, 11, 718, 311, 445, 519, 466, 50816], "temperature": 0.0, "avg_logprob": -0.20920418758018344, "compression_ratio": 1.3740458015267176, "no_speech_prob": 0.0010484576923772693}, {"id": 327, "seek": 228420, "start": 2293.24, "end": 2303.7999999999997, "text": " it in a 2D space. Just here, you know, each data point here represents an image, and they're in", "tokens": [50816, 309, 294, 257, 568, 35, 1901, 13, 1449, 510, 11, 291, 458, 11, 1184, 1412, 935, 510, 8855, 364, 3256, 11, 293, 436, 434, 294, 51344], "temperature": 0.0, "avg_logprob": -0.20920418758018344, "compression_ratio": 1.3740458015267176, "no_speech_prob": 0.0010484576923772693}, {"id": 328, "seek": 230380, "start": 2303.8, "end": 2311.32, "text": " this blob area, which represents a distribution. So this is in distribution.", "tokens": [50364, 341, 46115, 1859, 11, 597, 8855, 257, 7316, 13, 407, 341, 307, 294, 7316, 13, 50740], "temperature": 0.0, "avg_logprob": -0.2759185228191438, "compression_ratio": 1.5944055944055944, "no_speech_prob": 0.10666903108358383}, {"id": 329, "seek": 230380, "start": 2315.8, "end": 2321.6400000000003, "text": " And this is out of the distribution. Out, well, it's distribution.", "tokens": [50964, 400, 341, 307, 484, 295, 264, 7316, 13, 5925, 11, 731, 11, 309, 311, 7316, 13, 51256], "temperature": 0.0, "avg_logprob": -0.2759185228191438, "compression_ratio": 1.5944055944055944, "no_speech_prob": 0.10666903108358383}, {"id": 330, "seek": 230380, "start": 2324.92, "end": 2332.6000000000004, "text": " And basically, the idea is that, okay, if we take an image, and we want to generate,", "tokens": [51420, 400, 1936, 11, 264, 1558, 307, 300, 11, 1392, 11, 498, 321, 747, 364, 3256, 11, 293, 321, 528, 281, 8460, 11, 51804], "temperature": 0.0, "avg_logprob": -0.2759185228191438, "compression_ratio": 1.5944055944055944, "no_speech_prob": 0.10666903108358383}, {"id": 331, "seek": 233260, "start": 2332.6, "end": 2337.08, "text": " yeah, we want to generate some random image. If we were to take a random data point,", "tokens": [50364, 1338, 11, 321, 528, 281, 8460, 512, 4974, 3256, 13, 759, 321, 645, 281, 747, 257, 4974, 1412, 935, 11, 50588], "temperature": 0.0, "avg_logprob": -0.16835799146054395, "compression_ratio": 1.934065934065934, "no_speech_prob": 0.0006361763225868344}, {"id": 332, "seek": 233260, "start": 2337.08, "end": 2341.16, "text": " it would most likely be noisy images, right? So if we take some random data point,", "tokens": [50588, 309, 576, 881, 3700, 312, 24518, 5267, 11, 558, 30, 407, 498, 321, 747, 512, 4974, 1412, 935, 11, 50792], "temperature": 0.0, "avg_logprob": -0.16835799146054395, "compression_ratio": 1.934065934065934, "no_speech_prob": 0.0006361763225868344}, {"id": 333, "seek": 233260, "start": 2341.16, "end": 2346.52, "text": " it would be more, you know, the way to generate a random data point, it's going to be just noise.", "tokens": [50792, 309, 576, 312, 544, 11, 291, 458, 11, 264, 636, 281, 8460, 257, 4974, 1412, 935, 11, 309, 311, 516, 281, 312, 445, 5658, 13, 51060], "temperature": 0.0, "avg_logprob": -0.16835799146054395, "compression_ratio": 1.934065934065934, "no_speech_prob": 0.0006361763225868344}, {"id": 334, "seek": 233260, "start": 2346.52, "end": 2351.48, "text": " But we want to, you know, keep adjusting this data point to make it look more like", "tokens": [51060, 583, 321, 528, 281, 11, 291, 458, 11, 1066, 23559, 341, 1412, 935, 281, 652, 309, 574, 544, 411, 51308], "temperature": 0.0, "avg_logprob": -0.16835799146054395, "compression_ratio": 1.934065934065934, "no_speech_prob": 0.0006361763225868344}, {"id": 335, "seek": 233260, "start": 2352.12, "end": 2356.12, "text": " an image from your distribution. That's kind of the whole idea of this iterative process that", "tokens": [51340, 364, 3256, 490, 428, 7316, 13, 663, 311, 733, 295, 264, 1379, 1558, 295, 341, 17138, 1166, 1399, 300, 51540], "temperature": 0.0, "avg_logprob": -0.16835799146054395, "compression_ratio": 1.934065934065934, "no_speech_prob": 0.0006361763225868344}, {"id": 336, "seek": 233260, "start": 2356.12, "end": 2361.56, "text": " we're doing in our diffusion model. So the way to get that information is actually to", "tokens": [51540, 321, 434, 884, 294, 527, 25242, 2316, 13, 407, 264, 636, 281, 483, 300, 1589, 307, 767, 281, 51812], "temperature": 0.0, "avg_logprob": -0.16835799146054395, "compression_ratio": 1.934065934065934, "no_speech_prob": 0.0006361763225868344}, {"id": 337, "seek": 236156, "start": 2362.2, "end": 2369.7999999999997, "text": " take images from your dataset, and actually add noise to it. So that's what we try to do", "tokens": [50396, 747, 5267, 490, 428, 28872, 11, 293, 767, 909, 5658, 281, 309, 13, 407, 300, 311, 437, 321, 853, 281, 360, 50776], "temperature": 0.0, "avg_logprob": -0.20716627712907462, "compression_ratio": 2.071111111111111, "no_speech_prob": 0.00021995048155076802}, {"id": 338, "seek": 236156, "start": 2369.7999999999997, "end": 2375.48, "text": " in this process. So we have an image here, and we add noise to it. And then what we do is we", "tokens": [50776, 294, 341, 1399, 13, 407, 321, 362, 364, 3256, 510, 11, 293, 321, 909, 5658, 281, 309, 13, 400, 550, 437, 321, 360, 307, 321, 51060], "temperature": 0.0, "avg_logprob": -0.20716627712907462, "compression_ratio": 2.071111111111111, "no_speech_prob": 0.00021995048155076802}, {"id": 339, "seek": 236156, "start": 2375.48, "end": 2380.6, "text": " try to play a neural network to predict the noise. And by predicting the noise and subtracting it out,", "tokens": [51060, 853, 281, 862, 257, 18161, 3209, 281, 6069, 264, 5658, 13, 400, 538, 32884, 264, 5658, 293, 16390, 278, 309, 484, 11, 51316], "temperature": 0.0, "avg_logprob": -0.20716627712907462, "compression_ratio": 2.071111111111111, "no_speech_prob": 0.00021995048155076802}, {"id": 340, "seek": 236156, "start": 2380.6, "end": 2386.12, "text": " we're going back to the distribution. So adding the noise takes you away from the distribution,", "tokens": [51316, 321, 434, 516, 646, 281, 264, 7316, 13, 407, 5127, 264, 5658, 2516, 291, 1314, 490, 264, 7316, 11, 51592], "temperature": 0.0, "avg_logprob": -0.20716627712907462, "compression_ratio": 2.071111111111111, "no_speech_prob": 0.00021995048155076802}, {"id": 341, "seek": 236156, "start": 2386.12, "end": 2391.24, "text": " and then predicting the noise brings you back to the distribution. So then if we know", "tokens": [51592, 293, 550, 32884, 264, 5658, 5607, 291, 646, 281, 264, 7316, 13, 407, 550, 498, 321, 458, 51848], "temperature": 0.0, "avg_logprob": -0.20716627712907462, "compression_ratio": 2.071111111111111, "no_speech_prob": 0.00021995048155076802}, {"id": 342, "seek": 239124, "start": 2392.12, "end": 2399.72, "text": " at any given point in this space, how much noise to remove, that tells you how to", "tokens": [50408, 412, 604, 2212, 935, 294, 341, 1901, 11, 577, 709, 5658, 281, 4159, 11, 300, 5112, 291, 577, 281, 50788], "temperature": 0.0, "avg_logprob": -0.21696390107620595, "compression_ratio": 1.6948356807511737, "no_speech_prob": 6.922055763425305e-05}, {"id": 343, "seek": 239124, "start": 2400.7599999999998, "end": 2406.7599999999998, "text": " keep going towards the data distribution and get a point that lies within the distribution.", "tokens": [50840, 1066, 516, 3030, 264, 1412, 7316, 293, 483, 257, 935, 300, 9134, 1951, 264, 7316, 13, 51140], "temperature": 0.0, "avg_logprob": -0.21696390107620595, "compression_ratio": 1.6948356807511737, "no_speech_prob": 6.922055763425305e-05}, {"id": 344, "seek": 239124, "start": 2408.6, "end": 2412.8399999999997, "text": " So that's why we have noise prediction. And that's the importance of doing this noise prediction,", "tokens": [51232, 407, 300, 311, 983, 321, 362, 5658, 17630, 13, 400, 300, 311, 264, 7379, 295, 884, 341, 5658, 17630, 11, 51444], "temperature": 0.0, "avg_logprob": -0.21696390107620595, "compression_ratio": 1.6948356807511737, "no_speech_prob": 6.922055763425305e-05}, {"id": 345, "seek": 239124, "start": 2413.64, "end": 2417.4799999999996, "text": " is to be able to then do this iterative process where we can start out at a random point,", "tokens": [51484, 307, 281, 312, 1075, 281, 550, 360, 341, 17138, 1166, 1399, 689, 321, 393, 722, 484, 412, 257, 4974, 935, 11, 51676], "temperature": 0.0, "avg_logprob": -0.21696390107620595, "compression_ratio": 1.6948356807511737, "no_speech_prob": 6.922055763425305e-05}, {"id": 346, "seek": 241748, "start": 2417.48, "end": 2422.6, "text": " which would be, for example, pure noise, and keep predicting and removing that noise,", "tokens": [50364, 597, 576, 312, 11, 337, 1365, 11, 6075, 5658, 11, 293, 1066, 32884, 293, 12720, 300, 5658, 11, 50620], "temperature": 0.0, "avg_logprob": -0.24589675985356813, "compression_ratio": 1.6, "no_speech_prob": 8.219980372814462e-05}, {"id": 347, "seek": 241748, "start": 2422.6, "end": 2431.2400000000002, "text": " and walking towards the data distribution. Okay. So yeah, let's get started with the code.", "tokens": [50620, 293, 4494, 3030, 264, 1412, 7316, 13, 1033, 13, 407, 1338, 11, 718, 311, 483, 1409, 365, 264, 3089, 13, 51052], "temperature": 0.0, "avg_logprob": -0.24589675985356813, "compression_ratio": 1.6, "no_speech_prob": 8.219980372814462e-05}, {"id": 348, "seek": 241748, "start": 2433.4, "end": 2439.16, "text": " And so here, we of course have our imports, and we're going to load our dataset. We're going to", "tokens": [51160, 400, 370, 510, 11, 321, 295, 1164, 362, 527, 41596, 11, 293, 321, 434, 516, 281, 3677, 527, 28872, 13, 492, 434, 516, 281, 51448], "temperature": 0.0, "avg_logprob": -0.24589675985356813, "compression_ratio": 1.6, "no_speech_prob": 8.219980372814462e-05}, {"id": 349, "seek": 241748, "start": 2439.16, "end": 2446.28, "text": " work with our fashion MNIST dataset, which is what we've been working with for a while already.", "tokens": [51448, 589, 365, 527, 6700, 376, 45, 19756, 28872, 11, 597, 307, 437, 321, 600, 668, 1364, 365, 337, 257, 1339, 1217, 13, 51804], "temperature": 0.0, "avg_logprob": -0.24589675985356813, "compression_ratio": 1.6, "no_speech_prob": 8.219980372814462e-05}, {"id": 350, "seek": 244748, "start": 2448.12, "end": 2454.68, "text": " And this is just basically a similar code that we've seen from before, in terms of loading the", "tokens": [50396, 400, 341, 307, 445, 1936, 257, 2531, 3089, 300, 321, 600, 1612, 490, 949, 11, 294, 2115, 295, 15114, 264, 50724], "temperature": 0.0, "avg_logprob": -0.2509011443780393, "compression_ratio": 1.8038277511961722, "no_speech_prob": 0.0003569528053048998}, {"id": 351, "seek": 244748, "start": 2454.68, "end": 2462.52, "text": " dataset. And then we have our model. So we remove the noise from the image. So what our model is", "tokens": [50724, 28872, 13, 400, 550, 321, 362, 527, 2316, 13, 407, 321, 4159, 264, 5658, 490, 264, 3256, 13, 407, 437, 527, 2316, 307, 51116], "temperature": 0.0, "avg_logprob": -0.2509011443780393, "compression_ratio": 1.8038277511961722, "no_speech_prob": 0.0003569528053048998}, {"id": 352, "seek": 244748, "start": 2462.52, "end": 2469.48, "text": " going to take in, is it's going to take in the previous image, the noisy image, and predict", "tokens": [51116, 516, 281, 747, 294, 11, 307, 309, 311, 516, 281, 747, 294, 264, 3894, 3256, 11, 264, 24518, 3256, 11, 293, 6069, 51464], "temperature": 0.0, "avg_logprob": -0.2509011443780393, "compression_ratio": 1.8038277511961722, "no_speech_prob": 0.0003569528053048998}, {"id": 353, "seek": 244748, "start": 2469.48, "end": 2474.52, "text": " the noise. So the shapes of the input and the output are the same. They're going to be in the", "tokens": [51464, 264, 5658, 13, 407, 264, 10854, 295, 264, 4846, 293, 264, 5598, 366, 264, 912, 13, 814, 434, 516, 281, 312, 294, 264, 51716], "temperature": 0.0, "avg_logprob": -0.2509011443780393, "compression_ratio": 1.8038277511961722, "no_speech_prob": 0.0003569528053048998}, {"id": 354, "seek": 247452, "start": 2474.52, "end": 2481.88, "text": " shape of an image. So what we use is, we use a UNet neural network, which takes in kind of an", "tokens": [50364, 3909, 295, 364, 3256, 13, 407, 437, 321, 764, 307, 11, 321, 764, 257, 8229, 302, 18161, 3209, 11, 597, 2516, 294, 733, 295, 364, 50732], "temperature": 0.0, "avg_logprob": -0.2531270980834961, "compression_ratio": 1.6567164179104477, "no_speech_prob": 0.0012842786964029074}, {"id": 355, "seek": 247452, "start": 2482.44, "end": 2487.0, "text": " input image. And we do see your pointer now, by the way. So feel free to point at things.", "tokens": [50760, 4846, 3256, 13, 400, 321, 360, 536, 428, 23918, 586, 11, 538, 264, 636, 13, 407, 841, 1737, 281, 935, 412, 721, 13, 50988], "temperature": 0.0, "avg_logprob": -0.2531270980834961, "compression_ratio": 1.6567164179104477, "no_speech_prob": 0.0012842786964029074}, {"id": 356, "seek": 247452, "start": 2488.12, "end": 2494.28, "text": " Yeah. So yeah, it takes in, you know, an input image. And in this case, UNet", "tokens": [51044, 865, 13, 407, 1338, 11, 309, 2516, 294, 11, 291, 458, 11, 364, 4846, 3256, 13, 400, 294, 341, 1389, 11, 8229, 302, 51352], "temperature": 0.0, "avg_logprob": -0.2531270980834961, "compression_ratio": 1.6567164179104477, "no_speech_prob": 0.0012842786964029074}, {"id": 357, "seek": 247452, "start": 2496.52, "end": 2501.32, "text": " is the purpose, but they can also be used for any sort of image-to-image", "tokens": [51464, 307, 264, 4334, 11, 457, 436, 393, 611, 312, 1143, 337, 604, 1333, 295, 3256, 12, 1353, 12, 26624, 51704], "temperature": 0.0, "avg_logprob": -0.2531270980834961, "compression_ratio": 1.6567164179104477, "no_speech_prob": 0.0012842786964029074}, {"id": 358, "seek": 250132, "start": 2501.32, "end": 2507.7200000000003, "text": " path, where we're going from an input image, and then outputting some other image of some sort.", "tokens": [50364, 3100, 11, 689, 321, 434, 516, 490, 364, 4846, 3256, 11, 293, 550, 5598, 783, 512, 661, 3256, 295, 512, 1333, 13, 50684], "temperature": 0.0, "avg_logprob": -0.2492125862523129, "compression_ratio": 1.6324786324786325, "no_speech_prob": 0.00033534682006575167}, {"id": 359, "seek": 250132, "start": 2507.7200000000003, "end": 2511.88, "text": " And we'll talk more about new architecture, which we haven't learned about yet, and we will be", "tokens": [50684, 400, 321, 603, 751, 544, 466, 777, 9482, 11, 597, 321, 2378, 380, 3264, 466, 1939, 11, 293, 321, 486, 312, 50892], "temperature": 0.0, "avg_logprob": -0.2492125862523129, "compression_ratio": 1.6324786324786325, "no_speech_prob": 0.00033534682006575167}, {"id": 360, "seek": 250132, "start": 2511.88, "end": 2519.7200000000003, "text": " learning about in the next lesson. But broadly speaking, those gray arrows going from left to", "tokens": [50892, 2539, 466, 294, 264, 958, 6898, 13, 583, 19511, 4124, 11, 729, 10855, 19669, 516, 490, 1411, 281, 51284], "temperature": 0.0, "avg_logprob": -0.2492125862523129, "compression_ratio": 1.6324786324786325, "no_speech_prob": 0.00033534682006575167}, {"id": 361, "seek": 250132, "start": 2519.7200000000003, "end": 2528.44, "text": " right, are a lot like ResNet, very much like ResNet skip connections. But they're being used in a", "tokens": [51284, 558, 11, 366, 257, 688, 411, 5015, 31890, 11, 588, 709, 411, 5015, 31890, 10023, 9271, 13, 583, 436, 434, 885, 1143, 294, 257, 51720], "temperature": 0.0, "avg_logprob": -0.2492125862523129, "compression_ratio": 1.6324786324786325, "no_speech_prob": 0.00033534682006575167}, {"id": 362, "seek": 252844, "start": 2528.44, "end": 2535.7200000000003, "text": " different way. Everything else is stuff that we've seen before. So it's basically, we can", "tokens": [50364, 819, 636, 13, 5471, 1646, 307, 1507, 300, 321, 600, 1612, 949, 13, 407, 309, 311, 1936, 11, 321, 393, 50728], "temperature": 0.0, "avg_logprob": -0.2366662555270725, "compression_ratio": 1.4648648648648648, "no_speech_prob": 0.0006263194954954088}, {"id": 363, "seek": 252844, "start": 2536.28, "end": 2544.76, "text": " pretend those don't exist. For now, it's a neural network that the output is the same", "tokens": [50756, 11865, 729, 500, 380, 2514, 13, 1171, 586, 11, 309, 311, 257, 18161, 3209, 300, 264, 5598, 307, 264, 912, 51180], "temperature": 0.0, "avg_logprob": -0.2366662555270725, "compression_ratio": 1.4648648648648648, "no_speech_prob": 0.0006263194954954088}, {"id": 364, "seek": 252844, "start": 2545.32, "end": 2551.56, "text": " size, or a similar size, to the input. And therefore you can use it to learn how to go from one", "tokens": [51208, 2744, 11, 420, 257, 2531, 2744, 11, 281, 264, 4846, 13, 400, 4412, 291, 393, 764, 309, 281, 1466, 577, 281, 352, 490, 472, 51520], "temperature": 0.0, "avg_logprob": -0.2366662555270725, "compression_ratio": 1.4648648648648648, "no_speech_prob": 0.0006263194954954088}, {"id": 365, "seek": 255156, "start": 2552.2799999999997, "end": 2553.32, "text": " image to a different image.", "tokens": [50400, 3256, 281, 257, 819, 3256, 13, 50452], "temperature": 0.0, "avg_logprob": -0.27035404915033384, "compression_ratio": 1.5175879396984924, "no_speech_prob": 0.0008294408908113837}, {"id": 366, "seek": 255156, "start": 2556.44, "end": 2562.2, "text": " Yeah. So that's what UNet is. And yeah, like Jeb has said, we'll talk about it more.", "tokens": [50608, 865, 13, 407, 300, 311, 437, 8229, 302, 307, 13, 400, 1338, 11, 411, 2588, 65, 575, 848, 11, 321, 603, 751, 466, 309, 544, 13, 50896], "temperature": 0.0, "avg_logprob": -0.27035404915033384, "compression_ratio": 1.5175879396984924, "no_speech_prob": 0.0008294408908113837}, {"id": 367, "seek": 255156, "start": 2565.08, "end": 2570.36, "text": " The sort of UNets that are used for diffusion models also tend to have some additional tricks,", "tokens": [51040, 440, 1333, 295, 8229, 1385, 300, 366, 1143, 337, 25242, 5245, 611, 3928, 281, 362, 512, 4497, 11733, 11, 51304], "temperature": 0.0, "avg_logprob": -0.27035404915033384, "compression_ratio": 1.5175879396984924, "no_speech_prob": 0.0008294408908113837}, {"id": 368, "seek": 255156, "start": 2570.36, "end": 2576.84, "text": " which again, we'll talk about them later on as well. But yeah, we'll just, for the time being,", "tokens": [51304, 597, 797, 11, 321, 603, 751, 466, 552, 1780, 322, 382, 731, 13, 583, 1338, 11, 321, 603, 445, 11, 337, 264, 565, 885, 11, 51628], "temperature": 0.0, "avg_logprob": -0.27035404915033384, "compression_ratio": 1.5175879396984924, "no_speech_prob": 0.0008294408908113837}, {"id": 369, "seek": 257684, "start": 2576.84, "end": 2585.6400000000003, "text": " we will just import a UNet from the diffusers library, which is the Hudinface library for", "tokens": [50364, 321, 486, 445, 974, 257, 8229, 302, 490, 264, 7593, 301, 433, 6405, 11, 597, 307, 264, 389, 532, 259, 2868, 6405, 337, 50804], "temperature": 0.0, "avg_logprob": -0.2772442837978931, "compression_ratio": 1.6343612334801763, "no_speech_prob": 0.0010004694340750575}, {"id": 370, "seek": 257684, "start": 2585.6400000000003, "end": 2589.96, "text": " diffusion models. So they have a UNet implementation, and we'll just be using that for now.", "tokens": [50804, 25242, 5245, 13, 407, 436, 362, 257, 8229, 302, 11420, 11, 293, 321, 603, 445, 312, 1228, 300, 337, 586, 13, 51020], "temperature": 0.0, "avg_logprob": -0.2772442837978931, "compression_ratio": 1.6343612334801763, "no_speech_prob": 0.0010004694340750575}, {"id": 371, "seek": 257684, "start": 2592.84, "end": 2597.56, "text": " And so, yeah, of course, strictly speaking, we're cheating at this point, because we're using", "tokens": [51164, 400, 370, 11, 1338, 11, 295, 1164, 11, 20792, 4124, 11, 321, 434, 18309, 412, 341, 935, 11, 570, 321, 434, 1228, 51400], "temperature": 0.0, "avg_logprob": -0.2772442837978931, "compression_ratio": 1.6343612334801763, "no_speech_prob": 0.0010004694340750575}, {"id": 372, "seek": 257684, "start": 2597.56, "end": 2601.8, "text": " something we haven't written from scratch, but we're only cheating temporarily, because we will", "tokens": [51400, 746, 321, 2378, 380, 3720, 490, 8459, 11, 457, 321, 434, 787, 18309, 23750, 11, 570, 321, 486, 51612], "temperature": 0.0, "avg_logprob": -0.2772442837978931, "compression_ratio": 1.6343612334801763, "no_speech_prob": 0.0010004694340750575}, {"id": 373, "seek": 260180, "start": 2601.88, "end": 2609.7200000000003, "text": " be writing it from scratch. Yeah. And yeah, so and then of course, we're working with one channel", "tokens": [50368, 312, 3579, 309, 490, 8459, 13, 865, 13, 400, 1338, 11, 370, 293, 550, 295, 1164, 11, 321, 434, 1364, 365, 472, 2269, 50760], "temperature": 0.0, "avg_logprob": -0.27927116394042967, "compression_ratio": 1.6120218579234973, "no_speech_prob": 0.010162838734686375}, {"id": 374, "seek": 260180, "start": 2609.7200000000003, "end": 2615.48, "text": " images, our fashion MNIST images are one channel images, so we just have to specify that. And then", "tokens": [50760, 5267, 11, 527, 6700, 376, 45, 19756, 5267, 366, 472, 2269, 5267, 11, 370, 321, 445, 362, 281, 16500, 300, 13, 400, 550, 51048], "temperature": 0.0, "avg_logprob": -0.27927116394042967, "compression_ratio": 1.6120218579234973, "no_speech_prob": 0.010162838734686375}, {"id": 375, "seek": 260180, "start": 2615.48, "end": 2622.44, "text": " of course, the channels of the different blocks within the UNet are also specified. And then let's", "tokens": [51048, 295, 1164, 11, 264, 9235, 295, 264, 819, 8474, 1951, 264, 8229, 302, 366, 611, 22206, 13, 400, 550, 718, 311, 51396], "temperature": 0.0, "avg_logprob": -0.27927116394042967, "compression_ratio": 1.6120218579234973, "no_speech_prob": 0.010162838734686375}, {"id": 376, "seek": 262244, "start": 2622.44, "end": 2632.68, "text": " go into the training process. So basically, the general idea, of course, is we want to train with", "tokens": [50364, 352, 666, 264, 3097, 1399, 13, 407, 1936, 11, 264, 2674, 1558, 11, 295, 1164, 11, 307, 321, 528, 281, 3847, 365, 50876], "temperature": 0.0, "avg_logprob": -0.18004025353325737, "compression_ratio": 1.60989010989011, "no_speech_prob": 0.25966668128967285}, {"id": 377, "seek": 262244, "start": 2632.68, "end": 2643.4, "text": " this MSE loss. What we do is we select a random time step, and then we add noise to our image", "tokens": [50876, 341, 376, 5879, 4470, 13, 708, 321, 360, 307, 321, 3048, 257, 4974, 565, 1823, 11, 293, 550, 321, 909, 5658, 281, 527, 3256, 51412], "temperature": 0.0, "avg_logprob": -0.18004025353325737, "compression_ratio": 1.60989010989011, "no_speech_prob": 0.25966668128967285}, {"id": 378, "seek": 262244, "start": 2643.4, "end": 2648.68, "text": " based on that time step. So of course, if we have a very high time step, we're adding a lot of noise.", "tokens": [51412, 2361, 322, 300, 565, 1823, 13, 407, 295, 1164, 11, 498, 321, 362, 257, 588, 1090, 565, 1823, 11, 321, 434, 5127, 257, 688, 295, 5658, 13, 51676], "temperature": 0.0, "avg_logprob": -0.18004025353325737, "compression_ratio": 1.60989010989011, "no_speech_prob": 0.25966668128967285}, {"id": 379, "seek": 264868, "start": 2648.68, "end": 2657.24, "text": " If we have a lower time step, then we're adding very little noise. So we're going to randomly", "tokens": [50364, 759, 321, 362, 257, 3126, 565, 1823, 11, 550, 321, 434, 5127, 588, 707, 5658, 13, 407, 321, 434, 516, 281, 16979, 50792], "temperature": 0.0, "avg_logprob": -0.28915669658396503, "compression_ratio": 1.7735849056603774, "no_speech_prob": 0.0011512825731188059}, {"id": 380, "seek": 264868, "start": 2657.24, "end": 2665.3999999999996, "text": " choose a time step. And then yeah, we add the noise accordingly to the image. And then we pass", "tokens": [50792, 2826, 257, 565, 1823, 13, 400, 550, 1338, 11, 321, 909, 264, 5658, 19717, 281, 264, 3256, 13, 400, 550, 321, 1320, 51200], "temperature": 0.0, "avg_logprob": -0.28915669658396503, "compression_ratio": 1.7735849056603774, "no_speech_prob": 0.0011512825731188059}, {"id": 381, "seek": 264868, "start": 2665.3999999999996, "end": 2672.9199999999996, "text": " it the noisy image to a model as well as the time step. And we are trying to predict the amount of", "tokens": [51200, 309, 264, 24518, 3256, 281, 257, 2316, 382, 731, 382, 264, 565, 1823, 13, 400, 321, 366, 1382, 281, 6069, 264, 2372, 295, 51576], "temperature": 0.0, "avg_logprob": -0.28915669658396503, "compression_ratio": 1.7735849056603774, "no_speech_prob": 0.0011512825731188059}, {"id": 382, "seek": 264868, "start": 2672.9199999999996, "end": 2677.3999999999996, "text": " noise that was in the image, and we predict it with the MSE loss. So we can see all the-", "tokens": [51576, 5658, 300, 390, 294, 264, 3256, 11, 293, 321, 6069, 309, 365, 264, 376, 5879, 4470, 13, 407, 321, 393, 536, 439, 264, 12, 51800], "temperature": 0.0, "avg_logprob": -0.28915669658396503, "compression_ratio": 1.7735849056603774, "no_speech_prob": 0.0011512825731188059}, {"id": 383, "seek": 267740, "start": 2677.4, "end": 2681.48, "text": " I have some pictures of some of these variables I could share if that would be useful.", "tokens": [50364, 286, 362, 512, 5242, 295, 512, 295, 613, 9102, 286, 727, 2073, 498, 300, 576, 312, 4420, 13, 50568], "temperature": 0.0, "avg_logprob": -0.22567212699663522, "compression_ratio": 1.668103448275862, "no_speech_prob": 0.001080149319022894}, {"id": 384, "seek": 267740, "start": 2684.84, "end": 2692.52, "text": " So I have a version, so I think Tanishka is sharing notebook number 15. Is that right? And I've got", "tokens": [50736, 407, 286, 362, 257, 3037, 11, 370, 286, 519, 314, 7524, 2330, 307, 5414, 21060, 1230, 2119, 13, 1119, 300, 558, 30, 400, 286, 600, 658, 51120], "temperature": 0.0, "avg_logprob": -0.22567212699663522, "compression_ratio": 1.668103448275862, "no_speech_prob": 0.001080149319022894}, {"id": 385, "seek": 267740, "start": 2692.52, "end": 2698.6800000000003, "text": " here notebook number 17. And so I took Tanishka's notebook, and just as I was starting to understand", "tokens": [51120, 510, 21060, 1230, 3282, 13, 400, 370, 286, 1890, 314, 7524, 2330, 311, 21060, 11, 293, 445, 382, 286, 390, 2891, 281, 1223, 51428], "temperature": 0.0, "avg_logprob": -0.22567212699663522, "compression_ratio": 1.668103448275862, "no_speech_prob": 0.001080149319022894}, {"id": 386, "seek": 267740, "start": 2698.6800000000003, "end": 2704.2000000000003, "text": " it, I added, I like to draw pictures for myself to understand what's going on. So I took the things", "tokens": [51428, 309, 11, 286, 3869, 11, 286, 411, 281, 2642, 5242, 337, 2059, 281, 1223, 437, 311, 516, 322, 13, 407, 286, 1890, 264, 721, 51704], "temperature": 0.0, "avg_logprob": -0.22567212699663522, "compression_ratio": 1.668103448275862, "no_speech_prob": 0.001080149319022894}, {"id": 387, "seek": 270420, "start": 2704.2, "end": 2712.2799999999997, "text": " which are in Tanishka's class, and just put them into a cell. So I just copied and pasted them,", "tokens": [50364, 597, 366, 294, 314, 7524, 2330, 311, 1508, 11, 293, 445, 829, 552, 666, 257, 2815, 13, 407, 286, 445, 25365, 293, 1791, 292, 552, 11, 50768], "temperature": 0.0, "avg_logprob": -0.18788499956007126, "compression_ratio": 1.572192513368984, "no_speech_prob": 0.0003569554537534714}, {"id": 388, "seek": 270420, "start": 2712.2799999999997, "end": 2720.12, "text": " although I replaced the Greek letters with English written out versions. And then I just plotted them", "tokens": [50768, 4878, 286, 10772, 264, 10281, 7825, 365, 3669, 3720, 484, 9606, 13, 400, 550, 286, 445, 43288, 552, 51160], "temperature": 0.0, "avg_logprob": -0.18788499956007126, "compression_ratio": 1.572192513368984, "no_speech_prob": 0.0003569554537534714}, {"id": 389, "seek": 270420, "start": 2720.12, "end": 2727.7999999999997, "text": " to see what they look like. So in Tanishka's class, he has this thing called beta, which is just", "tokens": [51160, 281, 536, 437, 436, 574, 411, 13, 407, 294, 314, 7524, 2330, 311, 1508, 11, 415, 575, 341, 551, 1219, 9861, 11, 597, 307, 445, 51544], "temperature": 0.0, "avg_logprob": -0.18788499956007126, "compression_ratio": 1.572192513368984, "no_speech_prob": 0.0003569554537534714}, {"id": 390, "seek": 272780, "start": 2728.76, "end": 2735.4, "text": " lin space. So that's just literally a line. So beta, there's going to be a thousand of them,", "tokens": [50412, 22896, 1901, 13, 407, 300, 311, 445, 3736, 257, 1622, 13, 407, 9861, 11, 456, 311, 516, 281, 312, 257, 4714, 295, 552, 11, 50744], "temperature": 0.0, "avg_logprob": -0.25882935523986816, "compression_ratio": 1.5875, "no_speech_prob": 0.00016865180805325508}, {"id": 391, "seek": 272780, "start": 2735.4, "end": 2743.5600000000004, "text": " and they're just going to be equally spaced from 0.01 to 0.02. And then", "tokens": [50744, 293, 436, 434, 445, 516, 281, 312, 12309, 43766, 490, 1958, 13, 10607, 281, 1958, 13, 12756, 13, 400, 550, 51152], "temperature": 0.0, "avg_logprob": -0.25882935523986816, "compression_ratio": 1.5875, "no_speech_prob": 0.00016865180805325508}, {"id": 392, "seek": 272780, "start": 2746.04, "end": 2752.36, "text": " there's something called sigma, which is the square root of that. So that's what sigma is", "tokens": [51276, 456, 311, 746, 1219, 12771, 11, 597, 307, 264, 3732, 5593, 295, 300, 13, 407, 300, 311, 437, 12771, 307, 51592], "temperature": 0.0, "avg_logprob": -0.25882935523986816, "compression_ratio": 1.5875, "no_speech_prob": 0.00016865180805325508}, {"id": 393, "seek": 275236, "start": 2752.44, "end": 2763.56, "text": " going to look like. And then he's also got alpha bar, which is the cumulative product of", "tokens": [50368, 516, 281, 574, 411, 13, 400, 550, 415, 311, 611, 658, 8961, 2159, 11, 597, 307, 264, 38379, 1674, 295, 50924], "temperature": 0.0, "avg_logprob": -0.20428136825561524, "compression_ratio": 1.3511450381679388, "no_speech_prob": 7.031156565062702e-05}, {"id": 394, "seek": 275236, "start": 2764.6800000000003, "end": 2773.56, "text": " 1 minus this. And that's what alpha bar looks like. So you can see here, as Tanishka was", "tokens": [50980, 502, 3175, 341, 13, 400, 300, 311, 437, 8961, 2159, 1542, 411, 13, 407, 291, 393, 536, 510, 11, 382, 314, 7524, 2330, 390, 51424], "temperature": 0.0, "avg_logprob": -0.20428136825561524, "compression_ratio": 1.3511450381679388, "no_speech_prob": 7.031156565062702e-05}, {"id": 395, "seek": 277356, "start": 2773.56, "end": 2781.56, "text": " describing earlier, that when T is higher, that's this is T, the x-axis, beta is higher.", "tokens": [50364, 16141, 3071, 11, 300, 562, 314, 307, 2946, 11, 300, 311, 341, 307, 314, 11, 264, 2031, 12, 24633, 11, 9861, 307, 2946, 13, 50764], "temperature": 0.0, "avg_logprob": -0.20745051990855823, "compression_ratio": 1.6358024691358024, "no_speech_prob": 0.0005614717956632376}, {"id": 396, "seek": 277356, "start": 2783.24, "end": 2792.44, "text": " And when T is higher, alpha bar is lower. So yeah, so if you want to remind yourself,", "tokens": [50848, 400, 562, 314, 307, 2946, 11, 8961, 2159, 307, 3126, 13, 407, 1338, 11, 370, 498, 291, 528, 281, 4160, 1803, 11, 51308], "temperature": 0.0, "avg_logprob": -0.20745051990855823, "compression_ratio": 1.6358024691358024, "no_speech_prob": 0.0005614717956632376}, {"id": 397, "seek": 277356, "start": 2792.44, "end": 2800.2799999999997, "text": " so each of these things, beta, sigma, alpha bar, they're each, they've each got a thousand", "tokens": [51308, 370, 1184, 295, 613, 721, 11, 9861, 11, 12771, 11, 8961, 2159, 11, 436, 434, 1184, 11, 436, 600, 1184, 658, 257, 4714, 51700], "temperature": 0.0, "avg_logprob": -0.20745051990855823, "compression_ratio": 1.6358024691358024, "no_speech_prob": 0.0005614717956632376}, {"id": 398, "seek": 280028, "start": 2800.28, "end": 2805.0800000000004, "text": " things in them. And this is the shape of those thousand things. So this is the", "tokens": [50364, 721, 294, 552, 13, 400, 341, 307, 264, 3909, 295, 729, 4714, 721, 13, 407, 341, 307, 264, 50604], "temperature": 0.0, "avg_logprob": -0.19564214471268326, "compression_ratio": 1.6296296296296295, "no_speech_prob": 1.5206793250399642e-05}, {"id": 399, "seek": 280028, "start": 2809.6400000000003, "end": 2814.76, "text": " amount of variance, I guess, added at each step. This is the square root of that. So it's the", "tokens": [50832, 2372, 295, 21977, 11, 286, 2041, 11, 3869, 412, 1184, 1823, 13, 639, 307, 264, 3732, 5593, 295, 300, 13, 407, 309, 311, 264, 51088], "temperature": 0.0, "avg_logprob": -0.19564214471268326, "compression_ratio": 1.6296296296296295, "no_speech_prob": 1.5206793250399642e-05}, {"id": 400, "seek": 280028, "start": 2815.88, "end": 2825.0800000000004, "text": " standard deviation added at each step. And then if we do 1 minus that, it's just, you know,", "tokens": [51144, 3832, 25163, 3869, 412, 1184, 1823, 13, 400, 550, 498, 321, 360, 502, 3175, 300, 11, 309, 311, 445, 11, 291, 458, 11, 51604], "temperature": 0.0, "avg_logprob": -0.19564214471268326, "compression_ratio": 1.6296296296296295, "no_speech_prob": 1.5206793250399642e-05}, {"id": 401, "seek": 282508, "start": 2825.08, "end": 2830.04, "text": " the exact opposite. And then this is what happens if you multiply them all together up to that", "tokens": [50364, 264, 1900, 6182, 13, 400, 550, 341, 307, 437, 2314, 498, 291, 12972, 552, 439, 1214, 493, 281, 300, 50612], "temperature": 0.0, "avg_logprob": -0.2191301927728168, "compression_ratio": 2.025531914893617, "no_speech_prob": 0.0007321531302295625}, {"id": 402, "seek": 282508, "start": 2830.04, "end": 2834.6, "text": " point. And the reason you do that is because if you add noise to something, you add noise to", "tokens": [50612, 935, 13, 400, 264, 1778, 291, 360, 300, 307, 570, 498, 291, 909, 5658, 281, 746, 11, 291, 909, 5658, 281, 50840], "temperature": 0.0, "avg_logprob": -0.2191301927728168, "compression_ratio": 2.025531914893617, "no_speech_prob": 0.0007321531302295625}, {"id": 403, "seek": 282508, "start": 2834.6, "end": 2837.24, "text": " something, that you add noise to something, that you add noise to something, then you have to", "tokens": [50840, 746, 11, 300, 291, 909, 5658, 281, 746, 11, 300, 291, 909, 5658, 281, 746, 11, 550, 291, 362, 281, 50972], "temperature": 0.0, "avg_logprob": -0.2191301927728168, "compression_ratio": 2.025531914893617, "no_speech_prob": 0.0007321531302295625}, {"id": 404, "seek": 282508, "start": 2837.24, "end": 2843.08, "text": " multiply together all that amount of noise to say how much noise you would get. So yeah, those are", "tokens": [50972, 12972, 1214, 439, 300, 2372, 295, 5658, 281, 584, 577, 709, 5658, 291, 576, 483, 13, 407, 1338, 11, 729, 366, 51264], "temperature": 0.0, "avg_logprob": -0.2191301927728168, "compression_ratio": 2.025531914893617, "no_speech_prob": 0.0007321531302295625}, {"id": 405, "seek": 282508, "start": 2843.08, "end": 2852.2799999999997, "text": " my pictures, if that's helpful. Yep, good to see the diagram, or see how it, the actual values,", "tokens": [51264, 452, 5242, 11, 498, 300, 311, 4961, 13, 7010, 11, 665, 281, 536, 264, 10686, 11, 420, 536, 577, 309, 11, 264, 3539, 4190, 11, 51724], "temperature": 0.0, "avg_logprob": -0.2191301927728168, "compression_ratio": 2.025531914893617, "no_speech_prob": 0.0007321531302295625}, {"id": 406, "seek": 285228, "start": 2853.0800000000004, "end": 2862.6000000000004, "text": " and how it changes over time. So yeah, let's see here. Sorry. Yeah, so like Jeremy was showing,", "tokens": [50404, 293, 577, 309, 2962, 670, 565, 13, 407, 1338, 11, 718, 311, 536, 510, 13, 4919, 13, 865, 11, 370, 411, 17809, 390, 4099, 11, 50880], "temperature": 0.0, "avg_logprob": -0.23206874302455358, "compression_ratio": 1.7630331753554502, "no_speech_prob": 0.00025313947116956115}, {"id": 407, "seek": 285228, "start": 2862.6000000000004, "end": 2867.7200000000003, "text": " we have our length space for our beta. In this case, we're using kind of more of the Greek", "tokens": [50880, 321, 362, 527, 4641, 1901, 337, 527, 9861, 13, 682, 341, 1389, 11, 321, 434, 1228, 733, 295, 544, 295, 264, 10281, 51136], "temperature": 0.0, "avg_logprob": -0.23206874302455358, "compression_ratio": 1.7630331753554502, "no_speech_prob": 0.00025313947116956115}, {"id": 408, "seek": 285228, "start": 2867.7200000000003, "end": 2872.2000000000003, "text": " letters. So you can see, you know, the Greek letters that, you know, we see in the paper,", "tokens": [51136, 7825, 13, 407, 291, 393, 536, 11, 291, 458, 11, 264, 10281, 7825, 300, 11, 291, 458, 11, 321, 536, 294, 264, 3035, 11, 51360], "temperature": 0.0, "avg_logprob": -0.23206874302455358, "compression_ratio": 1.7630331753554502, "no_speech_prob": 0.00025313947116956115}, {"id": 409, "seek": 285228, "start": 2872.2000000000003, "end": 2878.6000000000004, "text": " as well as now we have it here in the code as well. And you know, we have our length space from", "tokens": [51360, 382, 731, 382, 586, 321, 362, 309, 510, 294, 264, 3089, 382, 731, 13, 400, 291, 458, 11, 321, 362, 527, 4641, 1901, 490, 51680], "temperature": 0.0, "avg_logprob": -0.23206874302455358, "compression_ratio": 1.7630331753554502, "no_speech_prob": 0.00025313947116956115}, {"id": 410, "seek": 287860, "start": 2878.6, "end": 2883.3199999999997, "text": " our minimum value to our maximum value. And we have some number of steps. So this is the number", "tokens": [50364, 527, 7285, 2158, 281, 527, 6674, 2158, 13, 400, 321, 362, 512, 1230, 295, 4439, 13, 407, 341, 307, 264, 1230, 50600], "temperature": 0.0, "avg_logprob": -0.21315816879272462, "compression_ratio": 1.826086956521739, "no_speech_prob": 0.012053441256284714}, {"id": 411, "seek": 287860, "start": 2883.3199999999997, "end": 2889.96, "text": " of time steps. So here we use 1000 time steps, but that can depend on the type of model that", "tokens": [50600, 295, 565, 4439, 13, 407, 510, 321, 764, 9714, 565, 4439, 11, 457, 300, 393, 5672, 322, 264, 2010, 295, 2316, 300, 50932], "temperature": 0.0, "avg_logprob": -0.21315816879272462, "compression_ratio": 1.826086956521739, "no_speech_prob": 0.012053441256284714}, {"id": 412, "seek": 287860, "start": 2889.96, "end": 2894.68, "text": " you're training. And that's one of the parameters of your model, or how to parameters of your model.", "tokens": [50932, 291, 434, 3097, 13, 400, 300, 311, 472, 295, 264, 9834, 295, 428, 2316, 11, 420, 577, 281, 9834, 295, 428, 2316, 13, 51168], "temperature": 0.0, "avg_logprob": -0.21315816879272462, "compression_ratio": 1.826086956521739, "no_speech_prob": 0.012053441256284714}, {"id": 413, "seek": 287860, "start": 2896.04, "end": 2902.2799999999997, "text": " And this is the callback you've got here. So yeah, this callback is going to be used to,", "tokens": [51236, 400, 341, 307, 264, 818, 3207, 291, 600, 658, 510, 13, 407, 1338, 11, 341, 818, 3207, 307, 516, 281, 312, 1143, 281, 11, 51548], "temperature": 0.0, "avg_logprob": -0.21315816879272462, "compression_ratio": 1.826086956521739, "no_speech_prob": 0.012053441256284714}, {"id": 414, "seek": 290228, "start": 2902.84, "end": 2909.1600000000003, "text": " to set up the data, I guess, so that you're going to be using this to add the noise,", "tokens": [50392, 281, 992, 493, 264, 1412, 11, 286, 2041, 11, 370, 300, 291, 434, 516, 281, 312, 1228, 341, 281, 909, 264, 5658, 11, 50708], "temperature": 0.0, "avg_logprob": -0.3275168859041654, "compression_ratio": 1.6962616822429906, "no_speech_prob": 0.0017273561097681522}, {"id": 415, "seek": 290228, "start": 2909.1600000000003, "end": 2915.4, "text": " so that the models then got the, the data that we're trying to get it to learn to then denoise.", "tokens": [50708, 370, 300, 264, 5245, 550, 658, 264, 11, 264, 1412, 300, 321, 434, 1382, 281, 483, 309, 281, 1466, 281, 550, 1441, 38800, 13, 51020], "temperature": 0.0, "avg_logprob": -0.3275168859041654, "compression_ratio": 1.6962616822429906, "no_speech_prob": 0.0017273561097681522}, {"id": 416, "seek": 290228, "start": 2916.92, "end": 2923.0, "text": " Yeah. So the callback, of course, makes, makes a life, a life lot easier in terms of,", "tokens": [51096, 865, 13, 407, 264, 818, 3207, 11, 295, 1164, 11, 1669, 11, 1669, 257, 993, 11, 257, 993, 688, 3571, 294, 2115, 295, 11, 51400], "temperature": 0.0, "avg_logprob": -0.3275168859041654, "compression_ratio": 1.6962616822429906, "no_speech_prob": 0.0017273561097681522}, {"id": 417, "seek": 290228, "start": 2923.8, "end": 2929.48, "text": " yeah, setting up everything and, you know, still being able to use, I guess, the mini AI learner", "tokens": [51440, 1338, 11, 3287, 493, 1203, 293, 11, 291, 458, 11, 920, 885, 1075, 281, 764, 11, 286, 2041, 11, 264, 8382, 7318, 33347, 51724], "temperature": 0.0, "avg_logprob": -0.3275168859041654, "compression_ratio": 1.6962616822429906, "no_speech_prob": 0.0017273561097681522}, {"id": 418, "seek": 292948, "start": 2929.56, "end": 2935.2400000000002, "text": " with the mini AI learner with maybe some of these more complicated, and maybe a little bit more", "tokens": [50368, 365, 264, 8382, 7318, 33347, 365, 1310, 512, 295, 613, 544, 6179, 11, 293, 1310, 257, 707, 857, 544, 50652], "temperature": 0.0, "avg_logprob": -0.2660207187428194, "compression_ratio": 1.6475770925110131, "no_speech_prob": 0.00020987876632716507}, {"id": 419, "seek": 292948, "start": 2935.2400000000002, "end": 2943.56, "text": " unique training loops. So yeah, in this case, we're just able to use the, the, the callback,", "tokens": [50652, 3845, 3097, 16121, 13, 407, 1338, 11, 294, 341, 1389, 11, 321, 434, 445, 1075, 281, 764, 264, 11, 264, 11, 264, 818, 3207, 11, 51068], "temperature": 0.0, "avg_logprob": -0.2660207187428194, "compression_ratio": 1.6475770925110131, "no_speech_prob": 0.00020987876632716507}, {"id": 420, "seek": 292948, "start": 2943.56, "end": 2949.72, "text": " and in order to, you know, set up the, the data, the data, I guess, the batch that we are passing", "tokens": [51068, 293, 294, 1668, 281, 11, 291, 458, 11, 992, 493, 264, 11, 264, 1412, 11, 264, 1412, 11, 286, 2041, 11, 264, 15245, 300, 321, 366, 8437, 51376], "temperature": 0.0, "avg_logprob": -0.2660207187428194, "compression_ratio": 1.6475770925110131, "no_speech_prob": 0.00020987876632716507}, {"id": 421, "seek": 292948, "start": 2949.72, "end": 2957.64, "text": " into our learner. I just wanted to mention, when you first did this, you, you wrote out", "tokens": [51376, 666, 527, 33347, 13, 286, 445, 1415, 281, 2152, 11, 562, 291, 700, 630, 341, 11, 291, 11, 291, 4114, 484, 51772], "temperature": 0.0, "avg_logprob": -0.2660207187428194, "compression_ratio": 1.6475770925110131, "no_speech_prob": 0.00020987876632716507}, {"id": 422, "seek": 295764, "start": 2957.64, "end": 2965.0, "text": " the Greek letters in English, alpha and beta and so forth. And at least for my brain, I was finding", "tokens": [50364, 264, 10281, 7825, 294, 3669, 11, 8961, 293, 9861, 293, 370, 5220, 13, 400, 412, 1935, 337, 452, 3567, 11, 286, 390, 5006, 50732], "temperature": 0.0, "avg_logprob": -0.18968384011277875, "compression_ratio": 1.6302521008403361, "no_speech_prob": 0.00010228078463114798}, {"id": 423, "seek": 295764, "start": 2965.0, "end": 2969.72, "text": " it difficult to read because they were literally going off the edge of the page and I couldn't see", "tokens": [50732, 309, 2252, 281, 1401, 570, 436, 645, 3736, 516, 766, 264, 4691, 295, 264, 3028, 293, 286, 2809, 380, 536, 50968], "temperature": 0.0, "avg_logprob": -0.18968384011277875, "compression_ratio": 1.6302521008403361, "no_speech_prob": 0.00010228078463114798}, {"id": 424, "seek": 295764, "start": 2969.72, "end": 2975.24, "text": " it all at once. And so we've did a search in a place to replace it with the actual Greek letters.", "tokens": [50968, 309, 439, 412, 1564, 13, 400, 370, 321, 600, 630, 257, 3164, 294, 257, 1081, 281, 7406, 309, 365, 264, 3539, 10281, 7825, 13, 51244], "temperature": 0.0, "avg_logprob": -0.18968384011277875, "compression_ratio": 1.6302521008403361, "no_speech_prob": 0.00010228078463114798}, {"id": 425, "seek": 295764, "start": 2976.44, "end": 2983.0, "text": " I still don't know how I feel about it. Like it's, I find it, I'm finding it easier to read", "tokens": [51304, 286, 920, 500, 380, 458, 577, 286, 841, 466, 309, 13, 1743, 309, 311, 11, 286, 915, 309, 11, 286, 478, 5006, 309, 3571, 281, 1401, 51632], "temperature": 0.0, "avg_logprob": -0.18968384011277875, "compression_ratio": 1.6302521008403361, "no_speech_prob": 0.00010228078463114798}, {"id": 426, "seek": 298300, "start": 2983.0, "end": 2987.8, "text": " because I can see it all at once. And I don't know if it's a scroll and my, I don't get as overwhelmed.", "tokens": [50364, 570, 286, 393, 536, 309, 439, 412, 1564, 13, 400, 286, 500, 380, 458, 498, 309, 311, 257, 11369, 293, 452, 11, 286, 500, 380, 483, 382, 19042, 13, 50604], "temperature": 0.0, "avg_logprob": -0.2745723546108353, "compression_ratio": 1.5975103734439835, "no_speech_prob": 0.004608991090208292}, {"id": 427, "seek": 298300, "start": 2989.16, "end": 2994.76, "text": " But when I need to edit the code, I kind of just tend to copy and paste the Greek letters,", "tokens": [50672, 583, 562, 286, 643, 281, 8129, 264, 3089, 11, 286, 733, 295, 445, 3928, 281, 5055, 293, 9163, 264, 10281, 7825, 11, 50952], "temperature": 0.0, "avg_logprob": -0.2745723546108353, "compression_ratio": 1.5975103734439835, "no_speech_prob": 0.004608991090208292}, {"id": 428, "seek": 298300, "start": 2995.72, "end": 3002.76, "text": " which is why we use the actual word beta in the init parameter list, so that somebody using", "tokens": [51000, 597, 307, 983, 321, 764, 264, 3539, 1349, 9861, 294, 264, 3157, 13075, 1329, 11, 370, 300, 2618, 1228, 51352], "temperature": 0.0, "avg_logprob": -0.2745723546108353, "compression_ratio": 1.5975103734439835, "no_speech_prob": 0.004608991090208292}, {"id": 429, "seek": 298300, "start": 3002.76, "end": 3009.08, "text": " this never has to type a Greek letter. But I don't know, Jono or Tanishka, if you had any thoughts", "tokens": [51352, 341, 1128, 575, 281, 2010, 257, 10281, 5063, 13, 583, 286, 500, 380, 458, 11, 7745, 78, 420, 314, 7524, 2330, 11, 498, 291, 632, 604, 4598, 51668], "temperature": 0.0, "avg_logprob": -0.2745723546108353, "compression_ratio": 1.5975103734439835, "no_speech_prob": 0.004608991090208292}, {"id": 430, "seek": 300908, "start": 3009.08, "end": 3013.48, "text": " over the last week or two, since we made that change about whether you guys like having the", "tokens": [50364, 670, 264, 1036, 1243, 420, 732, 11, 1670, 321, 1027, 300, 1319, 466, 1968, 291, 1074, 411, 1419, 264, 50584], "temperature": 0.0, "avg_logprob": -0.2261612926210676, "compression_ratio": 1.6576923076923078, "no_speech_prob": 0.17542749643325806}, {"id": 431, "seek": 300908, "start": 3013.48, "end": 3021.0, "text": " Greek letters in there or, or not. I like it for this demo in particular. I don't know that I do", "tokens": [50584, 10281, 7825, 294, 456, 420, 11, 420, 406, 13, 286, 411, 309, 337, 341, 10723, 294, 1729, 13, 286, 500, 380, 458, 300, 286, 360, 50960], "temperature": 0.0, "avg_logprob": -0.2261612926210676, "compression_ratio": 1.6576923076923078, "no_speech_prob": 0.17542749643325806}, {"id": 432, "seek": 300908, "start": 3021.0, "end": 3024.92, "text": " this in my code, but because we're looking back and forth between the paper and the", "tokens": [50960, 341, 294, 452, 3089, 11, 457, 570, 321, 434, 1237, 646, 293, 5220, 1296, 264, 3035, 293, 264, 51156], "temperature": 0.0, "avg_logprob": -0.2261612926210676, "compression_ratio": 1.6576923076923078, "no_speech_prob": 0.17542749643325806}, {"id": 433, "seek": 300908, "start": 3025.7999999999997, "end": 3028.2, "text": " implementation here, I think it works in this case just fine.", "tokens": [51200, 11420, 510, 11, 286, 519, 309, 1985, 294, 341, 1389, 445, 2489, 13, 51320], "temperature": 0.0, "avg_logprob": -0.2261612926210676, "compression_ratio": 1.6576923076923078, "no_speech_prob": 0.17542749643325806}, {"id": 434, "seek": 300908, "start": 3031.0, "end": 3036.2799999999997, "text": " Yeah, I agree. I think it's good for like, yeah, when you're trying to study something or try to", "tokens": [51460, 865, 11, 286, 3986, 13, 286, 519, 309, 311, 665, 337, 411, 11, 1338, 11, 562, 291, 434, 1382, 281, 2979, 746, 420, 853, 281, 51724], "temperature": 0.0, "avg_logprob": -0.2261612926210676, "compression_ratio": 1.6576923076923078, "no_speech_prob": 0.17542749643325806}, {"id": 435, "seek": 303628, "start": 3036.28, "end": 3043.4, "text": " implement something, having the Greek letters is very useful to be able to, I guess, match the math", "tokens": [50364, 4445, 746, 11, 1419, 264, 10281, 7825, 307, 588, 4420, 281, 312, 1075, 281, 11, 286, 2041, 11, 2995, 264, 5221, 50720], "temperature": 0.0, "avg_logprob": -0.22267947949861225, "compression_ratio": 1.651376146788991, "no_speech_prob": 0.0018965658964589238}, {"id": 436, "seek": 303628, "start": 3043.4, "end": 3049.96, "text": " more closely. And it's just easy just to pick the equation and put it into code, or, you know,", "tokens": [50720, 544, 8185, 13, 400, 309, 311, 445, 1858, 445, 281, 1888, 264, 5367, 293, 829, 309, 666, 3089, 11, 420, 11, 291, 458, 11, 51048], "temperature": 0.0, "avg_logprob": -0.22267947949861225, "compression_ratio": 1.651376146788991, "no_speech_prob": 0.0018965658964589238}, {"id": 437, "seek": 303628, "start": 3049.96, "end": 3054.92, "text": " vice versa, looking at the code and try to match to the equation. So I think for like, yeah,", "tokens": [51048, 11964, 25650, 11, 1237, 412, 264, 3089, 293, 853, 281, 2995, 281, 264, 5367, 13, 407, 286, 519, 337, 411, 11, 1338, 11, 51296], "temperature": 0.0, "avg_logprob": -0.22267947949861225, "compression_ratio": 1.651376146788991, "no_speech_prob": 0.0018965658964589238}, {"id": 438, "seek": 303628, "start": 3054.92, "end": 3059.88, "text": " educational purpose, I tend to like, I guess the Greek letters. So yeah.", "tokens": [51296, 10189, 4334, 11, 286, 3928, 281, 411, 11, 286, 2041, 264, 10281, 7825, 13, 407, 1338, 13, 51544], "temperature": 0.0, "avg_logprob": -0.22267947949861225, "compression_ratio": 1.651376146788991, "no_speech_prob": 0.0018965658964589238}, {"id": 439, "seek": 305988, "start": 3060.12, "end": 3066.44, "text": " Um, yeah, so, you know, we have our initialization, but we're just defining all these variables.", "tokens": [50376, 3301, 11, 1338, 11, 370, 11, 291, 458, 11, 321, 362, 527, 5883, 2144, 11, 457, 321, 434, 445, 17827, 439, 613, 9102, 13, 50692], "temperature": 0.0, "avg_logprob": -0.36652919224330355, "compression_ratio": 1.5078534031413613, "no_speech_prob": 0.002511002356186509}, {"id": 440, "seek": 305988, "start": 3066.44, "end": 3073.7200000000003, "text": " We'll get to the predict in just a moment. But first, I just want to go over the before batch,", "tokens": [50692, 492, 603, 483, 281, 264, 6069, 294, 445, 257, 1623, 13, 583, 700, 11, 286, 445, 528, 281, 352, 670, 264, 949, 15245, 11, 51056], "temperature": 0.0, "avg_logprob": -0.36652919224330355, "compression_ratio": 1.5078534031413613, "no_speech_prob": 0.002511002356186509}, {"id": 441, "seek": 305988, "start": 3073.7200000000003, "end": 3083.32, "text": " where we're setting up our batch to pass into the model. So remember that the model is taking in", "tokens": [51056, 689, 321, 434, 3287, 493, 527, 15245, 281, 1320, 666, 264, 2316, 13, 407, 1604, 300, 264, 2316, 307, 1940, 294, 51536], "temperature": 0.0, "avg_logprob": -0.36652919224330355, "compression_ratio": 1.5078534031413613, "no_speech_prob": 0.002511002356186509}, {"id": 442, "seek": 308332, "start": 3083.4, "end": 3090.92, "text": " our noisy image and the time step. So, and of course the target is the actual amount of noise", "tokens": [50368, 527, 24518, 3256, 293, 264, 565, 1823, 13, 407, 11, 293, 295, 1164, 264, 3779, 307, 264, 3539, 2372, 295, 5658, 50744], "temperature": 0.0, "avg_logprob": -0.5134884942438185, "compression_ratio": 2.0726256983240225, "no_speech_prob": 0.1008346676826477}, {"id": 443, "seek": 308332, "start": 3090.92, "end": 3099.6400000000003, "text": " that we are adding to the image. So basically we generate that noise. So that's what's...", "tokens": [50744, 300, 321, 366, 5127, 281, 264, 3256, 13, 407, 1936, 321, 8460, 300, 5658, 13, 407, 300, 311, 437, 311, 485, 51180], "temperature": 0.0, "avg_logprob": -0.5134884942438185, "compression_ratio": 2.0726256983240225, "no_speech_prob": 0.1008346676826477}, {"id": 444, "seek": 308332, "start": 3099.6400000000003, "end": 3104.04, "text": " So epsilon is that target. So epsilon is the amount of noise, is not the amount of, is the", "tokens": [51180, 407, 17889, 307, 300, 3779, 13, 407, 17889, 307, 264, 2372, 295, 5658, 11, 307, 406, 264, 2372, 295, 11, 307, 264, 51400], "temperature": 0.0, "avg_logprob": -0.5134884942438185, "compression_ratio": 2.0726256983240225, "no_speech_prob": 0.1008346676826477}, {"id": 445, "seek": 308332, "start": 3104.04, "end": 3111.0, "text": " actual noise. Yes. Epsilon is the actual noise that we're adding. So we're just adding the noise", "tokens": [51400, 3539, 5658, 13, 1079, 13, 462, 16592, 307, 264, 3539, 5658, 300, 321, 434, 5127, 13, 407, 321, 434, 445, 5127, 264, 5658, 51748], "temperature": 0.0, "avg_logprob": -0.5134884942438185, "compression_ratio": 2.0726256983240225, "no_speech_prob": 0.1008346676826477}, {"id": 446, "seek": 311100, "start": 3111.08, "end": 3114.6, "text": " to the image. So epsilon is the actual noise that we're adding. And that's the target as well,", "tokens": [50368, 281, 264, 3256, 13, 407, 17889, 307, 264, 3539, 5658, 300, 321, 434, 5127, 13, 400, 300, 311, 264, 3779, 382, 731, 11, 50544], "temperature": 0.0, "avg_logprob": -0.28530180043187636, "compression_ratio": 1.933609958506224, "no_speech_prob": 0.0009695736225694418}, {"id": 447, "seek": 311100, "start": 3114.6, "end": 3120.6, "text": " because our model is a noise predicting model. It's predicting the noise in the image. And so", "tokens": [50544, 570, 527, 2316, 307, 257, 5658, 32884, 2316, 13, 467, 311, 32884, 264, 5658, 294, 264, 3256, 13, 400, 370, 50844], "temperature": 0.0, "avg_logprob": -0.28530180043187636, "compression_ratio": 1.933609958506224, "no_speech_prob": 0.0009695736225694418}, {"id": 448, "seek": 311100, "start": 3120.6, "end": 3124.92, "text": " our target should be the noise during, that we're adding to the image during training.", "tokens": [50844, 527, 3779, 820, 312, 264, 5658, 1830, 11, 300, 321, 434, 5127, 281, 264, 3256, 1830, 3097, 13, 51060], "temperature": 0.0, "avg_logprob": -0.28530180043187636, "compression_ratio": 1.933609958506224, "no_speech_prob": 0.0009695736225694418}, {"id": 449, "seek": 311100, "start": 3125.48, "end": 3131.0, "text": " So we have our epsilon and, you know, we're just generating it with this random function. It's,", "tokens": [51088, 407, 321, 362, 527, 17889, 293, 11, 291, 458, 11, 321, 434, 445, 17746, 309, 365, 341, 4974, 2445, 13, 467, 311, 11, 51364], "temperature": 0.0, "avg_logprob": -0.28530180043187636, "compression_ratio": 1.933609958506224, "no_speech_prob": 0.0009695736225694418}, {"id": 450, "seek": 311100, "start": 3131.0, "end": 3136.28, "text": " you know, the normal random normal distribution with a mean of zero variance of one. So that's", "tokens": [51364, 291, 458, 11, 264, 2710, 4974, 2710, 7316, 365, 257, 914, 295, 4018, 21977, 295, 472, 13, 407, 300, 311, 51628], "temperature": 0.0, "avg_logprob": -0.28530180043187636, "compression_ratio": 1.933609958506224, "no_speech_prob": 0.0009695736225694418}, {"id": 451, "seek": 313628, "start": 3136.28, "end": 3143.8, "text": " what that's doing. And, you know, adding the appropriate shape and device. Then the batch", "tokens": [50364, 437, 300, 311, 884, 13, 400, 11, 291, 458, 11, 5127, 264, 6854, 3909, 293, 4302, 13, 1396, 264, 15245, 50740], "temperature": 0.0, "avg_logprob": -0.19998831390052713, "compression_ratio": 1.6743119266055047, "no_speech_prob": 0.007120477966964245}, {"id": 452, "seek": 313628, "start": 3143.8, "end": 3150.6000000000004, "text": " that we get originally will contain the clean images, right? These are the original images", "tokens": [50740, 300, 321, 483, 7993, 486, 5304, 264, 2541, 5267, 11, 558, 30, 1981, 366, 264, 3380, 5267, 51080], "temperature": 0.0, "avg_logprob": -0.19998831390052713, "compression_ratio": 1.6743119266055047, "no_speech_prob": 0.007120477966964245}, {"id": 453, "seek": 313628, "start": 3150.6000000000004, "end": 3157.0, "text": " from our dataset. So that's X zero. And then what we want to do is we want to add noise.", "tokens": [51080, 490, 527, 28872, 13, 407, 300, 311, 1783, 4018, 13, 400, 550, 437, 321, 528, 281, 360, 307, 321, 528, 281, 909, 5658, 13, 51400], "temperature": 0.0, "avg_logprob": -0.19998831390052713, "compression_ratio": 1.6743119266055047, "no_speech_prob": 0.007120477966964245}, {"id": 454, "seek": 313628, "start": 3157.0, "end": 3164.0400000000004, "text": " So we have our alpha bar and we have a random time step that we select. And then we just simply", "tokens": [51400, 407, 321, 362, 527, 8961, 2159, 293, 321, 362, 257, 4974, 565, 1823, 300, 321, 3048, 13, 400, 550, 321, 445, 2935, 51752], "temperature": 0.0, "avg_logprob": -0.19998831390052713, "compression_ratio": 1.6743119266055047, "no_speech_prob": 0.007120477966964245}, {"id": 455, "seek": 316404, "start": 3164.04, "end": 3168.92, "text": " follow that equation, which again, I'll just show in just a moment. That equation, you can make a", "tokens": [50364, 1524, 300, 5367, 11, 597, 797, 11, 286, 603, 445, 855, 294, 445, 257, 1623, 13, 663, 5367, 11, 291, 393, 652, 257, 50608], "temperature": 0.0, "avg_logprob": -0.2468899509363007, "compression_ratio": 1.7695473251028806, "no_speech_prob": 0.0071209087036550045}, {"id": 456, "seek": 316404, "start": 3168.92, "end": 3173.4, "text": " tiny bit easier to read. I think if you were to double click on that first alpha bar underscore", "tokens": [50608, 5870, 857, 3571, 281, 1401, 13, 286, 519, 498, 291, 645, 281, 3834, 2052, 322, 300, 700, 8961, 2159, 37556, 50832], "temperature": 0.0, "avg_logprob": -0.2468899509363007, "compression_ratio": 1.7695473251028806, "no_speech_prob": 0.0071209087036550045}, {"id": 457, "seek": 316404, "start": 3173.4, "end": 3179.72, "text": " T, cut it and then paste it, sorry, in the XT equals torch dot square root, take the thing", "tokens": [50832, 314, 11, 1723, 309, 293, 550, 9163, 309, 11, 2597, 11, 294, 264, 1783, 51, 6915, 27822, 5893, 3732, 5593, 11, 747, 264, 551, 51148], "temperature": 0.0, "avg_logprob": -0.2468899509363007, "compression_ratio": 1.7695473251028806, "no_speech_prob": 0.0071209087036550045}, {"id": 458, "seek": 316404, "start": 3179.72, "end": 3184.6, "text": " inside the square root, double click it and paste it over the top of the word torch.", "tokens": [51148, 1854, 264, 3732, 5593, 11, 3834, 2052, 309, 293, 9163, 309, 670, 264, 1192, 295, 264, 1349, 27822, 13, 51392], "temperature": 0.0, "avg_logprob": -0.2468899509363007, "compression_ratio": 1.7695473251028806, "no_speech_prob": 0.0071209087036550045}, {"id": 459, "seek": 316404, "start": 3186.7599999999998, "end": 3190.12, "text": " That would be a little bit easier to read. That's ingenious.", "tokens": [51500, 663, 576, 312, 257, 707, 857, 3571, 281, 1401, 13, 663, 311, 294, 1766, 851, 13, 51668], "temperature": 0.0, "avg_logprob": -0.2468899509363007, "compression_ratio": 1.7695473251028806, "no_speech_prob": 0.0071209087036550045}, {"id": 460, "seek": 319012, "start": 3190.12, "end": 3195.08, "text": " **JASON LENGSTORF** And then do the same for the next one.", "tokens": [50364, 22723, 41, 26556, 441, 2195, 38, 6840, 2483, 37, 4852, 400, 550, 360, 264, 912, 337, 264, 958, 472, 13, 50612], "temperature": 0.0, "avg_logprob": -0.3614294005603325, "compression_ratio": 1.3870967741935485, "no_speech_prob": 0.005730143282562494}, {"id": 461, "seek": 319012, "start": 3195.08, "end": 3196.8399999999997, "text": " **BEN HONG**", "tokens": [50612, 22723, 33, 2195, 389, 12661, 4852, 50700], "temperature": 0.0, "avg_logprob": -0.3614294005603325, "compression_ratio": 1.3870967741935485, "no_speech_prob": 0.005730143282562494}, {"id": 462, "seek": 319012, "start": 3202.68, "end": 3203.24, "text": " Are we done?", "tokens": [50992, 2014, 321, 1096, 30, 51020], "temperature": 0.0, "avg_logprob": -0.3614294005603325, "compression_ratio": 1.3870967741935485, "no_speech_prob": 0.005730143282562494}, {"id": 463, "seek": 319012, "start": 3203.24, "end": 3205.56, "text": " **JASON LENGSTORF** Those parentheses.", "tokens": [51020, 22723, 41, 26556, 441, 2195, 38, 6840, 2483, 37, 4852, 3950, 34153, 13, 51136], "temperature": 0.0, "avg_logprob": -0.3614294005603325, "compression_ratio": 1.3870967741935485, "no_speech_prob": 0.005730143282562494}, {"id": 464, "seek": 319012, "start": 3205.56, "end": 3216.3599999999997, "text": " **BEN HONG** Yeah. So basically, yeah. So yeah, I guess let's just pull up the equation. So", "tokens": [51136, 22723, 33, 2195, 389, 12661, 4852, 865, 13, 407, 1936, 11, 1338, 13, 407, 1338, 11, 286, 2041, 718, 311, 445, 2235, 493, 264, 5367, 13, 407, 51676], "temperature": 0.0, "avg_logprob": -0.3614294005603325, "compression_ratio": 1.3870967741935485, "no_speech_prob": 0.005730143282562494}, {"id": 465, "seek": 321636, "start": 3217.08, "end": 3224.44, "text": " let's see. So there's a section in the paper that has the nice algorithm. Let's see if I can find it.", "tokens": [50400, 718, 311, 536, 13, 407, 456, 311, 257, 3541, 294, 264, 3035, 300, 575, 264, 1481, 9284, 13, 961, 311, 536, 498, 286, 393, 915, 309, 13, 50768], "temperature": 0.0, "avg_logprob": -0.3079609448396707, "compression_ratio": 1.467005076142132, "no_speech_prob": 0.008315172046422958}, {"id": 466, "seek": 321636, "start": 3224.44, "end": 3234.52, "text": " No, not here. It's I think earlier. Yes, training. So we're just following these same sort of", "tokens": [50768, 883, 11, 406, 510, 13, 467, 311, 286, 519, 3071, 13, 1079, 11, 3097, 13, 407, 321, 434, 445, 3480, 613, 912, 1333, 295, 51272], "temperature": 0.0, "avg_logprob": -0.3079609448396707, "compression_ratio": 1.467005076142132, "no_speech_prob": 0.008315172046422958}, {"id": 467, "seek": 321636, "start": 3235.32, "end": 3242.36, "text": " training steps here, right? We select a clean image that we take from our dataset. This fancy", "tokens": [51312, 3097, 4439, 510, 11, 558, 30, 492, 3048, 257, 2541, 3256, 300, 321, 747, 490, 527, 28872, 13, 639, 10247, 51664], "temperature": 0.0, "avg_logprob": -0.3079609448396707, "compression_ratio": 1.467005076142132, "no_speech_prob": 0.008315172046422958}, {"id": 468, "seek": 324236, "start": 3243.32, "end": 3248.84, "text": " kind of equation here is just saying take an image from your dataset, take a random time step", "tokens": [50412, 733, 295, 5367, 510, 307, 445, 1566, 747, 364, 3256, 490, 428, 28872, 11, 747, 257, 4974, 565, 1823, 50688], "temperature": 0.0, "avg_logprob": -0.3166955785548433, "compression_ratio": 1.7201834862385321, "no_speech_prob": 0.00298097706399858}, {"id": 469, "seek": 324236, "start": 3249.8, "end": 3256.36, "text": " between this range. And this is our epsilon that we're getting, just say get some epsilon value.", "tokens": [50736, 1296, 341, 3613, 13, 400, 341, 307, 527, 17889, 300, 321, 434, 1242, 11, 445, 584, 483, 512, 17889, 2158, 13, 51064], "temperature": 0.0, "avg_logprob": -0.3166955785548433, "compression_ratio": 1.7201834862385321, "no_speech_prob": 0.00298097706399858}, {"id": 470, "seek": 324236, "start": 3256.36, "end": 3263.48, "text": " And then we have our equation for X of T, right? This is the equation here. You can see that", "tokens": [51064, 400, 550, 321, 362, 527, 5367, 337, 1783, 295, 314, 11, 558, 30, 639, 307, 264, 5367, 510, 13, 509, 393, 536, 300, 51420], "temperature": 0.0, "avg_logprob": -0.3166955785548433, "compression_ratio": 1.7201834862385321, "no_speech_prob": 0.00298097706399858}, {"id": 471, "seek": 324236, "start": 3263.48, "end": 3268.76, "text": " it is square root of alpha bar T X zero plus one square root of one minus alpha bar T times", "tokens": [51420, 309, 307, 3732, 5593, 295, 8961, 2159, 314, 1783, 4018, 1804, 472, 3732, 5593, 295, 472, 3175, 8961, 2159, 314, 1413, 51684], "temperature": 0.0, "avg_logprob": -0.3166955785548433, "compression_ratio": 1.7201834862385321, "no_speech_prob": 0.00298097706399858}, {"id": 472, "seek": 326876, "start": 3268.76, "end": 3275.0800000000004, "text": " epsilon. So that's the same equation that we have right here, right? And then what we need to do is", "tokens": [50364, 17889, 13, 407, 300, 311, 264, 912, 5367, 300, 321, 362, 558, 510, 11, 558, 30, 400, 550, 437, 321, 643, 281, 360, 307, 50680], "temperature": 0.0, "avg_logprob": -0.18250769868903205, "compression_ratio": 1.8254716981132075, "no_speech_prob": 0.0001123494075727649}, {"id": 473, "seek": 326876, "start": 3275.0800000000004, "end": 3281.32, "text": " we need to pass this into our model. So we have X, T and T. So we set up our batch accordingly.", "tokens": [50680, 321, 643, 281, 1320, 341, 666, 527, 2316, 13, 407, 321, 362, 1783, 11, 314, 293, 314, 13, 407, 321, 992, 493, 527, 15245, 19717, 13, 50992], "temperature": 0.0, "avg_logprob": -0.18250769868903205, "compression_ratio": 1.8254716981132075, "no_speech_prob": 0.0001123494075727649}, {"id": 474, "seek": 326876, "start": 3281.32, "end": 3286.5200000000004, "text": " So this is the two things that we pass into our model. And of course, we also have our target,", "tokens": [50992, 407, 341, 307, 264, 732, 721, 300, 321, 1320, 666, 527, 2316, 13, 400, 295, 1164, 11, 321, 611, 362, 527, 3779, 11, 51252], "temperature": 0.0, "avg_logprob": -0.18250769868903205, "compression_ratio": 1.8254716981132075, "no_speech_prob": 0.0001123494075727649}, {"id": 475, "seek": 326876, "start": 3286.5200000000004, "end": 3292.2000000000003, "text": " which is our epsilon. And so that's what this is showing here. We pass in our X of T, as well as", "tokens": [51252, 597, 307, 527, 17889, 13, 400, 370, 300, 311, 437, 341, 307, 4099, 510, 13, 492, 1320, 294, 527, 1783, 295, 314, 11, 382, 731, 382, 51536], "temperature": 0.0, "avg_logprob": -0.18250769868903205, "compression_ratio": 1.8254716981132075, "no_speech_prob": 0.0001123494075727649}, {"id": 476, "seek": 329220, "start": 3292.2, "end": 3299.0, "text": " our T here, right? And we pass that into a model. The model is represented here as epsilon theta.", "tokens": [50364, 527, 314, 510, 11, 558, 30, 400, 321, 1320, 300, 666, 257, 2316, 13, 440, 2316, 307, 10379, 510, 382, 17889, 9725, 13, 50704], "temperature": 0.0, "avg_logprob": -0.2097548763308905, "compression_ratio": 1.9227642276422765, "no_speech_prob": 0.10667037963867188}, {"id": 477, "seek": 329220, "start": 3299.0, "end": 3303.3199999999997, "text": " And theta is often used to represent like this is a neural network with some parameters,", "tokens": [50704, 400, 9725, 307, 2049, 1143, 281, 2906, 411, 341, 307, 257, 18161, 3209, 365, 512, 9834, 11, 50920], "temperature": 0.0, "avg_logprob": -0.2097548763308905, "compression_ratio": 1.9227642276422765, "no_speech_prob": 0.10667037963867188}, {"id": 478, "seek": 329220, "start": 3303.3199999999997, "end": 3307.8799999999997, "text": " and the parameters are represented by theta. So epsilon theta is just representing our noise", "tokens": [50920, 293, 264, 9834, 366, 10379, 538, 9725, 13, 407, 17889, 9725, 307, 445, 13460, 527, 5658, 51148], "temperature": 0.0, "avg_logprob": -0.2097548763308905, "compression_ratio": 1.9227642276422765, "no_speech_prob": 0.10667037963867188}, {"id": 479, "seek": 329220, "start": 3307.8799999999997, "end": 3312.9199999999996, "text": " predicting model. So this is our neural network. So we have passed in our X of T and our T into", "tokens": [51148, 32884, 2316, 13, 407, 341, 307, 527, 18161, 3209, 13, 407, 321, 362, 4678, 294, 527, 1783, 295, 314, 293, 527, 314, 666, 51400], "temperature": 0.0, "avg_logprob": -0.2097548763308905, "compression_ratio": 1.9227642276422765, "no_speech_prob": 0.10667037963867188}, {"id": 480, "seek": 329220, "start": 3312.9199999999996, "end": 3319.3199999999997, "text": " a neural network, and we are comparing it to our target here, which is the actual epsilon. And so", "tokens": [51400, 257, 18161, 3209, 11, 293, 321, 366, 15763, 309, 281, 527, 3779, 510, 11, 597, 307, 264, 3539, 17889, 13, 400, 370, 51720], "temperature": 0.0, "avg_logprob": -0.2097548763308905, "compression_ratio": 1.9227642276422765, "no_speech_prob": 0.10667037963867188}, {"id": 481, "seek": 331932, "start": 3319.32, "end": 3326.76, "text": " that's what we're doing here. We have our batch where we have our X of T and T and epsilon.", "tokens": [50364, 300, 311, 437, 321, 434, 884, 510, 13, 492, 362, 527, 15245, 689, 321, 362, 527, 1783, 295, 314, 293, 314, 293, 17889, 13, 50736], "temperature": 0.0, "avg_logprob": -0.21721427257244402, "compression_ratio": 1.7098214285714286, "no_speech_prob": 0.0006563390488736331}, {"id": 482, "seek": 331932, "start": 3326.76, "end": 3332.6800000000003, "text": " And then here we have our prediction function. And because we actually have, I guess in this case,", "tokens": [50736, 400, 550, 510, 321, 362, 527, 17630, 2445, 13, 400, 570, 321, 767, 362, 11, 286, 2041, 294, 341, 1389, 11, 51032], "temperature": 0.0, "avg_logprob": -0.21721427257244402, "compression_ratio": 1.7098214285714286, "no_speech_prob": 0.0006563390488736331}, {"id": 483, "seek": 331932, "start": 3332.6800000000003, "end": 3339.0, "text": " we have two things that are in a tuple that we need to pass into our model. So we just kind of,", "tokens": [51032, 321, 362, 732, 721, 300, 366, 294, 257, 2604, 781, 300, 321, 643, 281, 1320, 666, 527, 2316, 13, 407, 321, 445, 733, 295, 11, 51348], "temperature": 0.0, "avg_logprob": -0.21721427257244402, "compression_ratio": 1.7098214285714286, "no_speech_prob": 0.0006563390488736331}, {"id": 484, "seek": 331932, "start": 3339.0, "end": 3345.0, "text": " you know, get those elements from our tuple with this. Yeah, we get the elements from the tuple,", "tokens": [51348, 291, 458, 11, 483, 729, 4959, 490, 527, 2604, 781, 365, 341, 13, 865, 11, 321, 483, 264, 4959, 490, 264, 2604, 781, 11, 51648], "temperature": 0.0, "avg_logprob": -0.21721427257244402, "compression_ratio": 1.7098214285714286, "no_speech_prob": 0.0006563390488736331}, {"id": 485, "seek": 334500, "start": 3345.0, "end": 3350.92, "text": " pass it into the model, and then Hugging Face has its own API in terms of getting the output.", "tokens": [50364, 1320, 309, 666, 264, 2316, 11, 293, 550, 46892, 3249, 4047, 575, 1080, 1065, 9362, 294, 2115, 295, 1242, 264, 5598, 13, 50660], "temperature": 0.0, "avg_logprob": -0.24335945977105033, "compression_ratio": 1.6736842105263159, "no_speech_prob": 0.0029807959217578173}, {"id": 486, "seek": 334500, "start": 3350.92, "end": 3356.84, "text": " So you need to call dot sample in order to get the predictions from your model. So we just do that.", "tokens": [50660, 407, 291, 643, 281, 818, 5893, 6889, 294, 1668, 281, 483, 264, 21264, 490, 428, 2316, 13, 407, 321, 445, 360, 300, 13, 50956], "temperature": 0.0, "avg_logprob": -0.24335945977105033, "compression_ratio": 1.6736842105263159, "no_speech_prob": 0.0029807959217578173}, {"id": 487, "seek": 334500, "start": 3356.84, "end": 3362.28, "text": " And then we do, yeah, we have learned dot preds. And, you know, that's what's going to be used", "tokens": [50956, 400, 550, 321, 360, 11, 1338, 11, 321, 362, 3264, 5893, 3852, 82, 13, 400, 11, 291, 458, 11, 300, 311, 437, 311, 516, 281, 312, 1143, 51228], "temperature": 0.0, "avg_logprob": -0.24335945977105033, "compression_ratio": 1.6736842105263159, "no_speech_prob": 0.0029807959217578173}, {"id": 488, "seek": 334500, "start": 3362.28, "end": 3368.68, "text": " later than when we're trying to do our loss function calculation. So the, just so, I mean,", "tokens": [51228, 1780, 813, 562, 321, 434, 1382, 281, 360, 527, 4470, 2445, 17108, 13, 407, 264, 11, 445, 370, 11, 286, 914, 11, 51548], "temperature": 0.0, "avg_logprob": -0.24335945977105033, "compression_ratio": 1.6736842105263159, "no_speech_prob": 0.0029807959217578173}, {"id": 489, "seek": 334500, "start": 3368.68, "end": 3372.04, "text": " it's just worth looking at that a little bit more since we haven't quite seen something like this", "tokens": [51548, 309, 311, 445, 3163, 1237, 412, 300, 257, 707, 857, 544, 1670, 321, 2378, 380, 1596, 1612, 746, 411, 341, 51716], "temperature": 0.0, "avg_logprob": -0.24335945977105033, "compression_ratio": 1.6736842105263159, "no_speech_prob": 0.0029807959217578173}, {"id": 490, "seek": 337204, "start": 3372.04, "end": 3377.32, "text": " before. And it's something which I'm not aware of any other framework that would let you do this,", "tokens": [50364, 949, 13, 400, 309, 311, 746, 597, 286, 478, 406, 3650, 295, 604, 661, 8388, 300, 576, 718, 291, 360, 341, 11, 50628], "temperature": 0.0, "avg_logprob": -0.24518059239243017, "compression_ratio": 1.7149122807017543, "no_speech_prob": 0.0003569659311324358}, {"id": 491, "seek": 337204, "start": 3377.32, "end": 3383.72, "text": " you know, literally replace how prediction works. So many AI's kind of really fun for this. So", "tokens": [50628, 291, 458, 11, 3736, 7406, 577, 17630, 1985, 13, 407, 867, 7318, 311, 733, 295, 534, 1019, 337, 341, 13, 407, 50948], "temperature": 0.0, "avg_logprob": -0.24518059239243017, "compression_ratio": 1.7149122807017543, "no_speech_prob": 0.0003569659311324358}, {"id": 492, "seek": 337204, "start": 3383.72, "end": 3390.36, "text": " because you inherited from train CB, train CB has predict, you know, defined. And you've defined a", "tokens": [50948, 570, 291, 27091, 490, 3847, 18745, 11, 3847, 18745, 575, 6069, 11, 291, 458, 11, 7642, 13, 400, 291, 600, 7642, 257, 51280], "temperature": 0.0, "avg_logprob": -0.24518059239243017, "compression_ratio": 1.7149122807017543, "no_speech_prob": 0.0003569659311324358}, {"id": 493, "seek": 337204, "start": 3390.36, "end": 3394.7599999999998, "text": " new version. So it's not going to use the train CB version anymore. It's going to use your version.", "tokens": [51280, 777, 3037, 13, 407, 309, 311, 406, 516, 281, 764, 264, 3847, 18745, 3037, 3602, 13, 467, 311, 516, 281, 764, 428, 3037, 13, 51500], "temperature": 0.0, "avg_logprob": -0.24518059239243017, "compression_ratio": 1.7149122807017543, "no_speech_prob": 0.0003569659311324358}, {"id": 494, "seek": 339476, "start": 3395.5600000000004, "end": 3402.28, "text": " And what you're doing is instead of passing learn dot batch zero to the model,", "tokens": [50404, 400, 437, 291, 434, 884, 307, 2602, 295, 8437, 1466, 5893, 15245, 4018, 281, 264, 2316, 11, 50740], "temperature": 0.0, "avg_logprob": -0.2628289892318401, "compression_ratio": 1.8520408163265305, "no_speech_prob": 0.022281289100646973}, {"id": 495, "seek": 339476, "start": 3403.1600000000003, "end": 3408.92, "text": " you've got a star in front of it. So the key thing is that star is going to, you know, and is,", "tokens": [50784, 291, 600, 658, 257, 3543, 294, 1868, 295, 309, 13, 407, 264, 2141, 551, 307, 300, 3543, 307, 516, 281, 11, 291, 458, 11, 293, 307, 11, 51072], "temperature": 0.0, "avg_logprob": -0.2628289892318401, "compression_ratio": 1.8520408163265305, "no_speech_prob": 0.022281289100646973}, {"id": 496, "seek": 339476, "start": 3408.92, "end": 3414.6000000000004, "text": " we know actually learn dot batch zero has two things in it. Because that learn dot batch that", "tokens": [51072, 321, 458, 767, 1466, 5893, 15245, 4018, 575, 732, 721, 294, 309, 13, 1436, 300, 1466, 5893, 15245, 300, 51356], "temperature": 0.0, "avg_logprob": -0.2628289892318401, "compression_ratio": 1.8520408163265305, "no_speech_prob": 0.022281289100646973}, {"id": 497, "seek": 339476, "start": 3414.6000000000004, "end": 3419.32, "text": " you showed at the end of the before batch method has two things in learn dot zero. So that star", "tokens": [51356, 291, 4712, 412, 264, 917, 295, 264, 949, 15245, 3170, 575, 732, 721, 294, 1466, 5893, 4018, 13, 407, 300, 3543, 51592], "temperature": 0.0, "avg_logprob": -0.2628289892318401, "compression_ratio": 1.8520408163265305, "no_speech_prob": 0.022281289100646973}, {"id": 498, "seek": 341932, "start": 3419.32, "end": 3425.6400000000003, "text": " will unpack them and send each one of those as a separate argument. So our model needs to take", "tokens": [50364, 486, 26699, 552, 293, 2845, 1184, 472, 295, 729, 382, 257, 4994, 6770, 13, 407, 527, 2316, 2203, 281, 747, 50680], "temperature": 0.0, "avg_logprob": -0.19519859811534052, "compression_ratio": 1.6282051282051282, "no_speech_prob": 0.0018967996584251523}, {"id": 499, "seek": 341932, "start": 3425.6400000000003, "end": 3431.96, "text": " two things, which the diffusers unit does take two things. So that's the main interesting point.", "tokens": [50680, 732, 721, 11, 597, 264, 7593, 301, 433, 4985, 775, 747, 732, 721, 13, 407, 300, 311, 264, 2135, 1880, 935, 13, 50996], "temperature": 0.0, "avg_logprob": -0.19519859811534052, "compression_ratio": 1.6282051282051282, "no_speech_prob": 0.0018967996584251523}, {"id": 500, "seek": 341932, "start": 3431.96, "end": 3440.28, "text": " And then something I find a bit awkward, honestly, about a lot of hacking face stuff, including", "tokens": [50996, 400, 550, 746, 286, 915, 257, 857, 11411, 11, 6095, 11, 466, 257, 688, 295, 31422, 1851, 1507, 11, 3009, 51412], "temperature": 0.0, "avg_logprob": -0.19519859811534052, "compression_ratio": 1.6282051282051282, "no_speech_prob": 0.0018967996584251523}, {"id": 501, "seek": 341932, "start": 3440.28, "end": 3445.88, "text": " diffusers is that generally their models don't just return the result, but they put it inside", "tokens": [51412, 7593, 301, 433, 307, 300, 5101, 641, 5245, 500, 380, 445, 2736, 264, 1874, 11, 457, 436, 829, 309, 1854, 51692], "temperature": 0.0, "avg_logprob": -0.19519859811534052, "compression_ratio": 1.6282051282051282, "no_speech_prob": 0.0018967996584251523}, {"id": 502, "seek": 344588, "start": 3445.88, "end": 3450.92, "text": " some name. And so that's what happens here. They put it in something inside something called sample.", "tokens": [50364, 512, 1315, 13, 400, 370, 300, 311, 437, 2314, 510, 13, 814, 829, 309, 294, 746, 1854, 746, 1219, 6889, 13, 50616], "temperature": 0.0, "avg_logprob": -0.23679690361022948, "compression_ratio": 1.6953405017921146, "no_speech_prob": 0.003884267993271351}, {"id": 503, "seek": 344588, "start": 3451.48, "end": 3456.36, "text": " So that's why tanishq added dot sample at the end of the predict, because of this", "tokens": [50644, 407, 300, 311, 983, 256, 7524, 80, 3869, 5893, 6889, 412, 264, 917, 295, 264, 6069, 11, 570, 295, 341, 50888], "temperature": 0.0, "avg_logprob": -0.23679690361022948, "compression_ratio": 1.6953405017921146, "no_speech_prob": 0.003884267993271351}, {"id": 504, "seek": 344588, "start": 3457.48, "end": 3463.88, "text": " somewhat awkward thing, which hugging face like to do for some reason. But yeah, now that you know,", "tokens": [50944, 8344, 11411, 551, 11, 597, 41706, 1851, 411, 281, 360, 337, 512, 1778, 13, 583, 1338, 11, 586, 300, 291, 458, 11, 51264], "temperature": 0.0, "avg_logprob": -0.23679690361022948, "compression_ratio": 1.6953405017921146, "no_speech_prob": 0.003884267993271351}, {"id": 505, "seek": 344588, "start": 3463.88, "end": 3469.2400000000002, "text": " I mean, this is something that people often get stuck on. I see on Kaggle and stuff like that.", "tokens": [51264, 286, 914, 11, 341, 307, 746, 300, 561, 2049, 483, 5541, 322, 13, 286, 536, 322, 48751, 22631, 293, 1507, 411, 300, 13, 51532], "temperature": 0.0, "avg_logprob": -0.23679690361022948, "compression_ratio": 1.6953405017921146, "no_speech_prob": 0.003884267993271351}, {"id": 506, "seek": 344588, "start": 3469.2400000000002, "end": 3473.48, "text": " It's like, how on earth do I use these models? Because they take things in weird forms and they", "tokens": [51532, 467, 311, 411, 11, 577, 322, 4120, 360, 286, 764, 613, 5245, 30, 1436, 436, 747, 721, 294, 3657, 6422, 293, 436, 51744], "temperature": 0.0, "avg_logprob": -0.23679690361022948, "compression_ratio": 1.6953405017921146, "no_speech_prob": 0.003884267993271351}, {"id": 507, "seek": 347348, "start": 3473.48, "end": 3479.2400000000002, "text": " give back things with weird forms. Well, this is how, you know, if you inherit from train cb,", "tokens": [50364, 976, 646, 721, 365, 3657, 6422, 13, 1042, 11, 341, 307, 577, 11, 291, 458, 11, 498, 291, 21389, 490, 3847, 269, 65, 11, 50652], "temperature": 0.0, "avg_logprob": -0.24629711068194846, "compression_ratio": 1.610091743119266, "no_speech_prob": 0.0007321537705138326}, {"id": 508, "seek": 347348, "start": 3479.88, "end": 3485.48, "text": " you can change predict to do whatever you want, which I think is quite sweet.", "tokens": [50684, 291, 393, 1319, 6069, 281, 360, 2035, 291, 528, 11, 597, 286, 519, 307, 1596, 3844, 13, 50964], "temperature": 0.0, "avg_logprob": -0.24629711068194846, "compression_ratio": 1.610091743119266, "no_speech_prob": 0.0007321537705138326}, {"id": 509, "seek": 347348, "start": 3487.48, "end": 3494.52, "text": " Yep. So yeah, that's the training loop. And then of course, you have your regular", "tokens": [51064, 7010, 13, 407, 1338, 11, 300, 311, 264, 3097, 6367, 13, 400, 550, 295, 1164, 11, 291, 362, 428, 3890, 51416], "temperature": 0.0, "avg_logprob": -0.24629711068194846, "compression_ratio": 1.610091743119266, "no_speech_prob": 0.0007321537705138326}, {"id": 510, "seek": 347348, "start": 3494.52, "end": 3501.48, "text": " training loop that's implemented in many AI where you are going to have, yeah, you have your loss", "tokens": [51416, 3097, 6367, 300, 311, 12270, 294, 867, 7318, 689, 291, 366, 516, 281, 362, 11, 1338, 11, 291, 362, 428, 4470, 51764], "temperature": 0.0, "avg_logprob": -0.24629711068194846, "compression_ratio": 1.610091743119266, "no_speech_prob": 0.0007321537705138326}, {"id": 511, "seek": 350148, "start": 3501.48, "end": 3510.76, "text": " function calculation. And you get the predictions learned up threads. And of course, the target is", "tokens": [50364, 2445, 17108, 13, 400, 291, 483, 264, 21264, 3264, 493, 19314, 13, 400, 295, 1164, 11, 264, 3779, 307, 50828], "temperature": 0.0, "avg_logprob": -0.2883199401523756, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.00045820389641448855}, {"id": 512, "seek": 350148, "start": 3510.76, "end": 3517.8, "text": " our learned up batch one, which is our epsilon. So you know, we have those and we pass it into", "tokens": [50828, 527, 3264, 493, 15245, 472, 11, 597, 307, 527, 17889, 13, 407, 291, 458, 11, 321, 362, 729, 293, 321, 1320, 309, 666, 51180], "temperature": 0.0, "avg_logprob": -0.2883199401523756, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.00045820389641448855}, {"id": 513, "seek": 350148, "start": 3517.8, "end": 3521.72, "text": " the loss function and calculates the loss function and does the back propagation.", "tokens": [51180, 264, 4470, 2445, 293, 4322, 1024, 264, 4470, 2445, 293, 775, 264, 646, 38377, 13, 51376], "temperature": 0.0, "avg_logprob": -0.2883199401523756, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.00045820389641448855}, {"id": 514, "seek": 350148, "start": 3521.72, "end": 3526.84, "text": " So I'll just go over that. We'll get back to the sampling in just a moment. But just to show the", "tokens": [51376, 407, 286, 603, 445, 352, 670, 300, 13, 492, 603, 483, 646, 281, 264, 21179, 294, 445, 257, 1623, 13, 583, 445, 281, 855, 264, 51632], "temperature": 0.0, "avg_logprob": -0.2883199401523756, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.00045820389641448855}, {"id": 515, "seek": 352684, "start": 3527.7200000000003, "end": 3538.6800000000003, "text": " training loop. So most of this is copied from our, I think it's 14 augment notebook, the way", "tokens": [50408, 3097, 6367, 13, 407, 881, 295, 341, 307, 25365, 490, 527, 11, 286, 519, 309, 311, 3499, 29919, 21060, 11, 264, 636, 50956], "temperature": 0.0, "avg_logprob": -0.283916384674782, "compression_ratio": 1.5025125628140703, "no_speech_prob": 0.010169262997806072}, {"id": 516, "seek": 352684, "start": 3538.6800000000003, "end": 3546.36, "text": " you've got the tmax and the shed. The only thing I think you've added here is the ddpm callback, right?", "tokens": [50956, 291, 600, 658, 264, 256, 41167, 293, 264, 14951, 13, 440, 787, 551, 286, 519, 291, 600, 3869, 510, 307, 264, 274, 67, 14395, 818, 3207, 11, 558, 30, 51340], "temperature": 0.0, "avg_logprob": -0.283916384674782, "compression_ratio": 1.5025125628140703, "no_speech_prob": 0.010169262997806072}, {"id": 517, "seek": 352684, "start": 3547.2400000000002, "end": 3554.36, "text": " Yes, the ddpm callback. And you've changed the loss function. Yes. So basically, we have to initialize", "tokens": [51384, 1079, 11, 264, 274, 67, 14395, 818, 3207, 13, 400, 291, 600, 3105, 264, 4470, 2445, 13, 1079, 13, 407, 1936, 11, 321, 362, 281, 5883, 1125, 51740], "temperature": 0.0, "avg_logprob": -0.283916384674782, "compression_ratio": 1.5025125628140703, "no_speech_prob": 0.010169262997806072}, {"id": 518, "seek": 355436, "start": 3554.36, "end": 3560.84, "text": " our ddpm callback with the appropriate arguments. So like the number of time steps and the minimum", "tokens": [50364, 527, 274, 67, 14395, 818, 3207, 365, 264, 6854, 12869, 13, 407, 411, 264, 1230, 295, 565, 4439, 293, 264, 7285, 50688], "temperature": 0.0, "avg_logprob": -0.2683366277943487, "compression_ratio": 1.5521739130434782, "no_speech_prob": 0.014955620281398296}, {"id": 519, "seek": 355436, "start": 3561.96, "end": 3569.2400000000002, "text": " beta and maximum beta. And then yeah, and then of course, we're using an MSC loss as we talked about.", "tokens": [50744, 9861, 293, 6674, 9861, 13, 400, 550, 1338, 11, 293, 550, 295, 1164, 11, 321, 434, 1228, 364, 7395, 34, 4470, 382, 321, 2825, 466, 13, 51108], "temperature": 0.0, "avg_logprob": -0.2683366277943487, "compression_ratio": 1.5521739130434782, "no_speech_prob": 0.014955620281398296}, {"id": 520, "seek": 355436, "start": 3572.1200000000003, "end": 3578.04, "text": " It just becomes a regular training loop. And everything else is from before. Yeah, we have", "tokens": [51252, 467, 445, 3643, 257, 3890, 3097, 6367, 13, 400, 1203, 1646, 307, 490, 949, 13, 865, 11, 321, 362, 51548], "temperature": 0.0, "avg_logprob": -0.2683366277943487, "compression_ratio": 1.5521739130434782, "no_speech_prob": 0.014955620281398296}, {"id": 521, "seek": 355436, "start": 3578.04, "end": 3582.1200000000003, "text": " your scheduler, your progress bar, all of that we've seen before.", "tokens": [51548, 428, 12000, 260, 11, 428, 4205, 2159, 11, 439, 295, 300, 321, 600, 1612, 949, 13, 51752], "temperature": 0.0, "avg_logprob": -0.2683366277943487, "compression_ratio": 1.5521739130434782, "no_speech_prob": 0.014955620281398296}, {"id": 522, "seek": 358212, "start": 3582.6, "end": 3586.8399999999997, "text": " I think it's really cool that we're using basically the same code to train a diffusion", "tokens": [50388, 286, 519, 309, 311, 534, 1627, 300, 321, 434, 1228, 1936, 264, 912, 3089, 281, 3847, 257, 25242, 50600], "temperature": 0.0, "avg_logprob": -0.30196851299655053, "compression_ratio": 1.7709923664122138, "no_speech_prob": 0.0026729570236057043}, {"id": 523, "seek": 358212, "start": 3586.8399999999997, "end": 3592.44, "text": " model as we've used to train a classifier just with, yeah, an extra callback. Yeah,", "tokens": [50600, 2316, 382, 321, 600, 1143, 281, 3847, 257, 1508, 9902, 445, 365, 11, 1338, 11, 364, 2857, 818, 3207, 13, 865, 11, 50880], "temperature": 0.0, "avg_logprob": -0.30196851299655053, "compression_ratio": 1.7709923664122138, "no_speech_prob": 0.0026729570236057043}, {"id": 524, "seek": 358212, "start": 3592.44, "end": 3597.88, "text": " yeah. Yeah, that's why I think callbacks are very powerful for being, you know, for allowing us to", "tokens": [50880, 1338, 13, 865, 11, 300, 311, 983, 286, 519, 818, 17758, 366, 588, 4005, 337, 885, 11, 291, 458, 11, 337, 8293, 505, 281, 51152], "temperature": 0.0, "avg_logprob": -0.30196851299655053, "compression_ratio": 1.7709923664122138, "no_speech_prob": 0.0026729570236057043}, {"id": 525, "seek": 358212, "start": 3597.88, "end": 3605.4, "text": " do such things. It's like pretty, you can take all this code and now we have a diffusion training", "tokens": [51152, 360, 1270, 721, 13, 467, 311, 411, 1238, 11, 291, 393, 747, 439, 341, 3089, 293, 586, 321, 362, 257, 25242, 3097, 51528], "temperature": 0.0, "avg_logprob": -0.30196851299655053, "compression_ratio": 1.7709923664122138, "no_speech_prob": 0.0026729570236057043}, {"id": 526, "seek": 358212, "start": 3605.4, "end": 3611.96, "text": " loop. And we can just call it learn.fit. And yeah, you can see, you've got a nice training loop.", "tokens": [51528, 6367, 13, 400, 321, 393, 445, 818, 309, 1466, 13, 6845, 13, 400, 1338, 11, 291, 393, 536, 11, 291, 600, 658, 257, 1481, 3097, 6367, 13, 51856], "temperature": 0.0, "avg_logprob": -0.30196851299655053, "compression_ratio": 1.7709923664122138, "no_speech_prob": 0.0026729570236057043}, {"id": 527, "seek": 361212, "start": 3612.2, "end": 3620.6, "text": " Nice loss curve. We can save our model. On a torch, saving functionality to be able to save our model", "tokens": [50368, 5490, 4470, 7605, 13, 492, 393, 3155, 527, 2316, 13, 1282, 257, 27822, 11, 6816, 14980, 281, 312, 1075, 281, 3155, 527, 2316, 50788], "temperature": 0.0, "avg_logprob": -0.28993659683420686, "compression_ratio": 1.53475935828877, "no_speech_prob": 9.460504952585325e-05}, {"id": 528, "seek": 361212, "start": 3620.6, "end": 3625.96, "text": " and we could load it in. But now that we have our trained model, then the question is,", "tokens": [50788, 293, 321, 727, 3677, 309, 294, 13, 583, 586, 300, 321, 362, 527, 8895, 2316, 11, 550, 264, 1168, 307, 11, 51056], "temperature": 0.0, "avg_logprob": -0.28993659683420686, "compression_ratio": 1.53475935828877, "no_speech_prob": 9.460504952585325e-05}, {"id": 529, "seek": 361212, "start": 3626.6, "end": 3635.08, "text": " what can we do to use it to sample, you know, the dataset? So the basic idea, of course, was that,", "tokens": [51088, 437, 393, 321, 360, 281, 764, 309, 281, 6889, 11, 291, 458, 11, 264, 28872, 30, 407, 264, 3875, 1558, 11, 295, 1164, 11, 390, 300, 11, 51512], "temperature": 0.0, "avg_logprob": -0.28993659683420686, "compression_ratio": 1.53475935828877, "no_speech_prob": 9.460504952585325e-05}, {"id": 530, "seek": 363508, "start": 3635.16, "end": 3635.96, "text": " you know, we have,", "tokens": [50368, 291, 458, 11, 321, 362, 11, 50408], "temperature": 0.0, "avg_logprob": -0.3093991672291475, "compression_ratio": 1.620879120879121, "no_speech_prob": 0.0019262632122263312}, {"id": 531, "seek": 363508, "start": 3639.88, "end": 3647.24, "text": " we have, like basically over here, right, we have, let's see here, okay, so we have a,", "tokens": [50604, 321, 362, 11, 411, 1936, 670, 510, 11, 558, 11, 321, 362, 11, 718, 311, 536, 510, 11, 1392, 11, 370, 321, 362, 257, 11, 50972], "temperature": 0.0, "avg_logprob": -0.3093991672291475, "compression_ratio": 1.620879120879121, "no_speech_prob": 0.0019262632122263312}, {"id": 532, "seek": 363508, "start": 3648.2799999999997, "end": 3652.6, "text": " basic idea is that we start out with a random data point. And of course, that's not going to be", "tokens": [51024, 3875, 1558, 307, 300, 321, 722, 484, 365, 257, 4974, 1412, 935, 13, 400, 295, 1164, 11, 300, 311, 406, 516, 281, 312, 51240], "temperature": 0.0, "avg_logprob": -0.3093991672291475, "compression_ratio": 1.620879120879121, "no_speech_prob": 0.0019262632122263312}, {"id": 533, "seek": 363508, "start": 3652.6, "end": 3661.4, "text": " within the distribution at first. But now we've learned how to move from, you know, one point", "tokens": [51240, 1951, 264, 7316, 412, 700, 13, 583, 586, 321, 600, 3264, 577, 281, 1286, 490, 11, 291, 458, 11, 472, 935, 51680], "temperature": 0.0, "avg_logprob": -0.3093991672291475, "compression_ratio": 1.620879120879121, "no_speech_prob": 0.0019262632122263312}, {"id": 534, "seek": 366140, "start": 3661.4, "end": 3666.36, "text": " towards the data distribution. That's what our noise prediction, predicting function does.", "tokens": [50364, 3030, 264, 1412, 7316, 13, 663, 311, 437, 527, 5658, 17630, 11, 32884, 2445, 775, 13, 50612], "temperature": 0.0, "avg_logprob": -0.2608059443784564, "compression_ratio": 1.6604651162790698, "no_speech_prob": 0.0002823963004630059}, {"id": 535, "seek": 366140, "start": 3666.36, "end": 3670.36, "text": " It basically tells you how, you know, in what direction and how much to,", "tokens": [50612, 467, 1936, 5112, 291, 577, 11, 291, 458, 11, 294, 437, 3513, 293, 577, 709, 281, 11, 50812], "temperature": 0.0, "avg_logprob": -0.2608059443784564, "compression_ratio": 1.6604651162790698, "no_speech_prob": 0.0002823963004630059}, {"id": 536, "seek": 366140, "start": 3673.56, "end": 3681.4, "text": " so the basic idea is that, yeah, I guess I'll start from maybe a new drawing here. Again, we have,", "tokens": [50972, 370, 264, 3875, 1558, 307, 300, 11, 1338, 11, 286, 2041, 286, 603, 722, 490, 1310, 257, 777, 6316, 510, 13, 3764, 11, 321, 362, 11, 51364], "temperature": 0.0, "avg_logprob": -0.2608059443784564, "compression_ratio": 1.6604651162790698, "no_speech_prob": 0.0002823963004630059}, {"id": 537, "seek": 366140, "start": 3683.96, "end": 3690.6800000000003, "text": " the distribution is, and we have a random point. And we use our noise predicting model that we", "tokens": [51492, 264, 7316, 307, 11, 293, 321, 362, 257, 4974, 935, 13, 400, 321, 764, 527, 5658, 32884, 2316, 300, 321, 51828], "temperature": 0.0, "avg_logprob": -0.2608059443784564, "compression_ratio": 1.6604651162790698, "no_speech_prob": 0.0002823963004630059}, {"id": 538, "seek": 369068, "start": 3690.68, "end": 3697.56, "text": " have trained to tell us which direction to move. So it tells us some direction. Or I guess, let's,", "tokens": [50364, 362, 8895, 281, 980, 505, 597, 3513, 281, 1286, 13, 407, 309, 5112, 505, 512, 3513, 13, 1610, 286, 2041, 11, 718, 311, 11, 50708], "temperature": 0.0, "avg_logprob": -0.2614635285877046, "compression_ratio": 1.9166666666666667, "no_speech_prob": 0.00025713216746225953}, {"id": 539, "seek": 369068, "start": 3697.56, "end": 3704.52, "text": " Zach, actually, that's not an area, other area, like, okay, so here, like here, okay, so it tells", "tokens": [50708, 21028, 11, 767, 11, 300, 311, 406, 364, 1859, 11, 661, 1859, 11, 411, 11, 1392, 11, 370, 510, 11, 411, 510, 11, 1392, 11, 370, 309, 5112, 51056], "temperature": 0.0, "avg_logprob": -0.2614635285877046, "compression_ratio": 1.9166666666666667, "no_speech_prob": 0.00025713216746225953}, {"id": 540, "seek": 369068, "start": 3704.52, "end": 3709.24, "text": " us some direction to move. At first, that direction is not going to be, like, you cannot follow that", "tokens": [51056, 505, 512, 3513, 281, 1286, 13, 1711, 700, 11, 300, 3513, 307, 406, 516, 281, 312, 11, 411, 11, 291, 2644, 1524, 300, 51292], "temperature": 0.0, "avg_logprob": -0.2614635285877046, "compression_ratio": 1.9166666666666667, "no_speech_prob": 0.00025713216746225953}, {"id": 541, "seek": 369068, "start": 3709.24, "end": 3714.8399999999997, "text": " direction all the way to get the correct data point. Because basically, what we were doing is", "tokens": [51292, 3513, 439, 264, 636, 281, 483, 264, 3006, 1412, 935, 13, 1436, 1936, 11, 437, 321, 645, 884, 307, 51572], "temperature": 0.0, "avg_logprob": -0.2614635285877046, "compression_ratio": 1.9166666666666667, "no_speech_prob": 0.00025713216746225953}, {"id": 542, "seek": 369068, "start": 3714.8399999999997, "end": 3718.68, "text": " we're trying to reverse the path that we were following when we were adding noise. So like,", "tokens": [51572, 321, 434, 1382, 281, 9943, 264, 3100, 300, 321, 645, 3480, 562, 321, 645, 5127, 5658, 13, 407, 411, 11, 51764], "temperature": 0.0, "avg_logprob": -0.2614635285877046, "compression_ratio": 1.9166666666666667, "no_speech_prob": 0.00025713216746225953}, {"id": 543, "seek": 371868, "start": 3718.68, "end": 3722.8399999999997, "text": " because we had originally a data point, and we kept adding noise to the data point, and maybe,", "tokens": [50364, 570, 321, 632, 7993, 257, 1412, 935, 11, 293, 321, 4305, 5127, 5658, 281, 264, 1412, 935, 11, 293, 1310, 11, 50572], "temperature": 0.0, "avg_logprob": -0.20354034554245126, "compression_ratio": 1.859437751004016, "no_speech_prob": 5.649646482197568e-05}, {"id": 544, "seek": 371868, "start": 3722.8399999999997, "end": 3727.96, "text": " you know, followed some path like this. And we want to reverse that path to get to.", "tokens": [50572, 291, 458, 11, 6263, 512, 3100, 411, 341, 13, 400, 321, 528, 281, 9943, 300, 3100, 281, 483, 281, 13, 50828], "temperature": 0.0, "avg_logprob": -0.20354034554245126, "compression_ratio": 1.859437751004016, "no_speech_prob": 5.649646482197568e-05}, {"id": 545, "seek": 371868, "start": 3731.0, "end": 3736.3599999999997, "text": " So our noise predicting function will give us an original direction, which, you know, would be,", "tokens": [50980, 407, 527, 5658, 32884, 2445, 486, 976, 505, 364, 3380, 3513, 11, 597, 11, 291, 458, 11, 576, 312, 11, 51248], "temperature": 0.0, "avg_logprob": -0.20354034554245126, "compression_ratio": 1.859437751004016, "no_speech_prob": 5.649646482197568e-05}, {"id": 546, "seek": 371868, "start": 3736.3599999999997, "end": 3743.24, "text": " you know, some kind of, it's going to be kind of tangential to the actual path at that location.", "tokens": [51248, 291, 458, 11, 512, 733, 295, 11, 309, 311, 516, 281, 312, 733, 295, 10266, 2549, 281, 264, 3539, 3100, 412, 300, 4914, 13, 51592], "temperature": 0.0, "avg_logprob": -0.20354034554245126, "compression_ratio": 1.859437751004016, "no_speech_prob": 5.649646482197568e-05}, {"id": 547, "seek": 371868, "start": 3743.24, "end": 3747.72, "text": " So what we would do is, you know, we would maybe follow that data point all the way towards", "tokens": [51592, 407, 437, 321, 576, 360, 307, 11, 291, 458, 11, 321, 576, 1310, 1524, 300, 1412, 935, 439, 264, 636, 3030, 51816], "temperature": 0.0, "avg_logprob": -0.20354034554245126, "compression_ratio": 1.859437751004016, "no_speech_prob": 5.649646482197568e-05}, {"id": 548, "seek": 374868, "start": 3748.68, "end": 3751.64, "text": " the, you know, we're just going to keep following that data point.", "tokens": [50364, 264, 11, 291, 458, 11, 321, 434, 445, 516, 281, 1066, 3480, 300, 1412, 935, 13, 50512], "temperature": 0.0, "avg_logprob": -0.22295190890630087, "compression_ratio": 1.7586206896551724, "no_speech_prob": 5.829069414176047e-05}, {"id": 549, "seek": 374868, "start": 3754.6, "end": 3759.3199999999997, "text": " You know, we're going to try to predict the fully denoised image by following this, this noise", "tokens": [50660, 509, 458, 11, 321, 434, 516, 281, 853, 281, 6069, 264, 4498, 1441, 78, 2640, 3256, 538, 3480, 341, 11, 341, 5658, 50896], "temperature": 0.0, "avg_logprob": -0.22295190890630087, "compression_ratio": 1.7586206896551724, "no_speech_prob": 5.829069414176047e-05}, {"id": 550, "seek": 374868, "start": 3759.3199999999997, "end": 3766.2799999999997, "text": " prediction. But our fully denoised image is also not going to be a real image. So what we, so let", "tokens": [50896, 17630, 13, 583, 527, 4498, 1441, 78, 2640, 3256, 307, 611, 406, 516, 281, 312, 257, 957, 3256, 13, 407, 437, 321, 11, 370, 718, 51244], "temperature": 0.0, "avg_logprob": -0.22295190890630087, "compression_ratio": 1.7586206896551724, "no_speech_prob": 5.829069414176047e-05}, {"id": 551, "seek": 374868, "start": 3766.2799999999997, "end": 3771.96, "text": " me, I'll show an example of that over here in the paper on where they show this a little bit more", "tokens": [51244, 385, 11, 286, 603, 855, 364, 1365, 295, 300, 670, 510, 294, 264, 3035, 322, 689, 436, 855, 341, 257, 707, 857, 544, 51528], "temperature": 0.0, "avg_logprob": -0.22295190890630087, "compression_ratio": 1.7586206896551724, "no_speech_prob": 5.829069414176047e-05}, {"id": 552, "seek": 377196, "start": 3771.96, "end": 3781.7200000000003, "text": " carefully. Let's see here. So x0, yeah, so basically, you can see the different data,", "tokens": [50364, 7500, 13, 961, 311, 536, 510, 13, 407, 2031, 15, 11, 1338, 11, 370, 1936, 11, 291, 393, 536, 264, 819, 1412, 11, 50852], "temperature": 0.0, "avg_logprob": -0.31190213521321614, "compression_ratio": 1.6875, "no_speech_prob": 0.04740035533905029}, {"id": 553, "seek": 377196, "start": 3785.7200000000003, "end": 3790.52, "text": " you can see the different data points here. It's not going to look anything like a real image. So", "tokens": [51052, 291, 393, 536, 264, 819, 1412, 2793, 510, 13, 467, 311, 406, 516, 281, 574, 1340, 411, 257, 957, 3256, 13, 407, 51292], "temperature": 0.0, "avg_logprob": -0.31190213521321614, "compression_ratio": 1.6875, "no_speech_prob": 0.04740035533905029}, {"id": 554, "seek": 377196, "start": 3790.52, "end": 3797.32, "text": " you can see all these points, you know, it doesn't look anything. That we would do is,", "tokens": [51292, 291, 393, 536, 439, 613, 2793, 11, 291, 458, 11, 309, 1177, 380, 574, 1340, 13, 663, 321, 576, 360, 307, 11, 51632], "temperature": 0.0, "avg_logprob": -0.31190213521321614, "compression_ratio": 1.6875, "no_speech_prob": 0.04740035533905029}, {"id": 555, "seek": 379732, "start": 3798.28, "end": 3804.92, "text": " oh, we actually had a little bit of noise back to it. And we start, we have a new point,", "tokens": [50412, 1954, 11, 321, 767, 632, 257, 707, 857, 295, 5658, 646, 281, 309, 13, 400, 321, 722, 11, 321, 362, 257, 777, 935, 11, 50744], "temperature": 0.0, "avg_logprob": -0.23617663383483886, "compression_ratio": 1.9741379310344827, "no_speech_prob": 0.0020504705607891083}, {"id": 556, "seek": 379732, "start": 3804.92, "end": 3809.32, "text": " where then we could maybe estimate a bit, get a better estimate of which direction to move.", "tokens": [50744, 689, 550, 321, 727, 1310, 12539, 257, 857, 11, 483, 257, 1101, 12539, 295, 597, 3513, 281, 1286, 13, 50964], "temperature": 0.0, "avg_logprob": -0.23617663383483886, "compression_ratio": 1.9741379310344827, "no_speech_prob": 0.0020504705607891083}, {"id": 557, "seek": 379732, "start": 3810.1200000000003, "end": 3815.2400000000002, "text": " Follow that all the way again, we follow a new point. And then I get add back a little bit of", "tokens": [51004, 9876, 300, 439, 264, 636, 797, 11, 321, 1524, 257, 777, 935, 13, 400, 550, 286, 483, 909, 646, 257, 707, 857, 295, 51260], "temperature": 0.0, "avg_logprob": -0.23617663383483886, "compression_ratio": 1.9741379310344827, "no_speech_prob": 0.0020504705607891083}, {"id": 558, "seek": 379732, "start": 3815.2400000000002, "end": 3821.0, "text": " noise, you get a new estimate, you make a new estimate of, you know, this noise prediction", "tokens": [51260, 5658, 11, 291, 483, 257, 777, 12539, 11, 291, 652, 257, 777, 12539, 295, 11, 291, 458, 11, 341, 5658, 17630, 51548], "temperature": 0.0, "avg_logprob": -0.23617663383483886, "compression_ratio": 1.9741379310344827, "no_speech_prob": 0.0020504705607891083}, {"id": 559, "seek": 379732, "start": 3821.0, "end": 3825.4, "text": " and removing the noise, you know, follow that all again, completely, and add a little bit of", "tokens": [51548, 293, 12720, 264, 5658, 11, 291, 458, 11, 1524, 300, 439, 797, 11, 2584, 11, 293, 909, 257, 707, 857, 295, 51768], "temperature": 0.0, "avg_logprob": -0.23617663383483886, "compression_ratio": 1.9741379310344827, "no_speech_prob": 0.0020504705607891083}, {"id": 560, "seek": 382540, "start": 3825.4, "end": 3833.48, "text": " noise again to the image, and merge onto the image. So that's kind of what we're showing here.", "tokens": [50364, 5658, 797, 281, 264, 3256, 11, 293, 22183, 3911, 264, 3256, 13, 407, 300, 311, 733, 295, 437, 321, 434, 4099, 510, 13, 50768], "temperature": 0.0, "avg_logprob": -0.2384021282196045, "compression_ratio": 1.6812227074235808, "no_speech_prob": 0.00041084244730882347}, {"id": 561, "seek": 382540, "start": 3833.48, "end": 3839.08, "text": " It's a lot like SGD, with SGD, we don't take the gradient and jump all the way, we use a learning", "tokens": [50768, 467, 311, 257, 688, 411, 34520, 35, 11, 365, 34520, 35, 11, 321, 500, 380, 747, 264, 16235, 293, 3012, 439, 264, 636, 11, 321, 764, 257, 2539, 51048], "temperature": 0.0, "avg_logprob": -0.2384021282196045, "compression_ratio": 1.6812227074235808, "no_speech_prob": 0.00041084244730882347}, {"id": 562, "seek": 382540, "start": 3839.08, "end": 3845.32, "text": " rate to go some of the way, because each of those estimates of where we want to go, you know,", "tokens": [51048, 3314, 281, 352, 512, 295, 264, 636, 11, 570, 1184, 295, 729, 20561, 295, 689, 321, 528, 281, 352, 11, 291, 458, 11, 51360], "temperature": 0.0, "avg_logprob": -0.2384021282196045, "compression_ratio": 1.6812227074235808, "no_speech_prob": 0.00041084244730882347}, {"id": 563, "seek": 382540, "start": 3845.32, "end": 3851.1600000000003, "text": " not that great, but we just do it slowly. Exactly. And at the end of the day, that's, yeah, that's", "tokens": [51360, 406, 300, 869, 11, 457, 321, 445, 360, 309, 5692, 13, 7587, 13, 400, 412, 264, 917, 295, 264, 786, 11, 300, 311, 11, 1338, 11, 300, 311, 51652], "temperature": 0.0, "avg_logprob": -0.2384021282196045, "compression_ratio": 1.6812227074235808, "no_speech_prob": 0.00041084244730882347}, {"id": 564, "seek": 385116, "start": 3851.16, "end": 3855.8799999999997, "text": " what we're doing with this noise prediction, we are predicting the sort of gradient of this", "tokens": [50364, 437, 321, 434, 884, 365, 341, 5658, 17630, 11, 321, 366, 32884, 264, 1333, 295, 16235, 295, 341, 50600], "temperature": 0.0, "avg_logprob": -0.2342455500648135, "compression_ratio": 1.7380952380952381, "no_speech_prob": 0.014280654489994049}, {"id": 565, "seek": 385116, "start": 3857.3199999999997, "end": 3863.3199999999997, "text": " P of X. But of course, we need to keep making estimates of that gradient as we're progressing.", "tokens": [50672, 430, 295, 1783, 13, 583, 295, 1164, 11, 321, 643, 281, 1066, 1455, 20561, 295, 300, 16235, 382, 321, 434, 36305, 13, 50972], "temperature": 0.0, "avg_logprob": -0.2342455500648135, "compression_ratio": 1.7380952380952381, "no_speech_prob": 0.014280654489994049}, {"id": 566, "seek": 385116, "start": 3863.3199999999997, "end": 3869.48, "text": " So we have to keep evaluating our noise prediction function to get updated and better", "tokens": [50972, 407, 321, 362, 281, 1066, 27479, 527, 5658, 17630, 2445, 281, 483, 10588, 293, 1101, 51280], "temperature": 0.0, "avg_logprob": -0.2342455500648135, "compression_ratio": 1.7380952380952381, "no_speech_prob": 0.014280654489994049}, {"id": 567, "seek": 385116, "start": 3869.48, "end": 3876.7599999999998, "text": " estimates of our gradient in order to finally converge onto our image. So that, and then you", "tokens": [51280, 20561, 295, 527, 16235, 294, 1668, 281, 2721, 41881, 3911, 527, 3256, 13, 407, 300, 11, 293, 550, 291, 51644], "temperature": 0.0, "avg_logprob": -0.2342455500648135, "compression_ratio": 1.7380952380952381, "no_speech_prob": 0.014280654489994049}, {"id": 568, "seek": 387676, "start": 3876.76, "end": 3883.1600000000003, "text": " can see that here, you know, we have this, maybe this fully predicted denoise image, which at the", "tokens": [50364, 393, 536, 300, 510, 11, 291, 458, 11, 321, 362, 341, 11, 1310, 341, 4498, 19147, 1441, 38800, 3256, 11, 597, 412, 264, 50684], "temperature": 0.0, "avg_logprob": -0.23087374903574712, "compression_ratio": 1.6815068493150684, "no_speech_prob": 0.023684341460466385}, {"id": 569, "seek": 387676, "start": 3883.1600000000003, "end": 3888.36, "text": " beginning doesn't look anything like a real image. But then as we continue throughout the sampling", "tokens": [50684, 2863, 1177, 380, 574, 1340, 411, 257, 957, 3256, 13, 583, 550, 382, 321, 2354, 3710, 264, 21179, 50944], "temperature": 0.0, "avg_logprob": -0.23087374903574712, "compression_ratio": 1.6815068493150684, "no_speech_prob": 0.023684341460466385}, {"id": 570, "seek": 387676, "start": 3888.36, "end": 3893.5600000000004, "text": " process, we finally converge on something that looks like an actual image. Again, these are CIFAR-10", "tokens": [50944, 1399, 11, 321, 2721, 41881, 322, 746, 300, 1542, 411, 364, 3539, 3256, 13, 3764, 11, 613, 366, 383, 12775, 1899, 12, 3279, 51204], "temperature": 0.0, "avg_logprob": -0.23087374903574712, "compression_ratio": 1.6815068493150684, "no_speech_prob": 0.023684341460466385}, {"id": 571, "seek": 387676, "start": 3893.5600000000004, "end": 3899.1600000000003, "text": " images, and it's still a little bit maybe unclear about how realistic these images, these very small", "tokens": [51204, 5267, 11, 293, 309, 311, 920, 257, 707, 857, 1310, 25636, 466, 577, 12465, 613, 5267, 11, 613, 588, 1359, 51484], "temperature": 0.0, "avg_logprob": -0.23087374903574712, "compression_ratio": 1.6815068493150684, "no_speech_prob": 0.023684341460466385}, {"id": 572, "seek": 387676, "start": 3899.1600000000003, "end": 3906.0400000000004, "text": " images look, but that's kind of the general principle, I would say. And so that's what I can", "tokens": [51484, 5267, 574, 11, 457, 300, 311, 733, 295, 264, 2674, 8665, 11, 286, 576, 584, 13, 400, 370, 300, 311, 437, 286, 393, 51828], "temperature": 0.0, "avg_logprob": -0.23087374903574712, "compression_ratio": 1.6815068493150684, "no_speech_prob": 0.023684341460466385}, {"id": 573, "seek": 390604, "start": 3906.04, "end": 3916.6, "text": " show in the code. This idea of, we're going to start out basically with a random image, right?", "tokens": [50364, 855, 294, 264, 3089, 13, 639, 1558, 295, 11, 321, 434, 516, 281, 722, 484, 1936, 365, 257, 4974, 3256, 11, 558, 30, 50892], "temperature": 0.0, "avg_logprob": -0.18809473956072773, "compression_ratio": 1.742081447963801, "no_speech_prob": 0.00010229700274067}, {"id": 574, "seek": 390604, "start": 3916.6, "end": 3922.04, "text": " And this random image is going to be like a pure noise image, and it's not going to be part of the", "tokens": [50892, 400, 341, 4974, 3256, 307, 516, 281, 312, 411, 257, 6075, 5658, 3256, 11, 293, 309, 311, 406, 516, 281, 312, 644, 295, 264, 51164], "temperature": 0.0, "avg_logprob": -0.18809473956072773, "compression_ratio": 1.742081447963801, "no_speech_prob": 0.00010229700274067}, {"id": 575, "seek": 390604, "start": 3922.04, "end": 3926.2799999999997, "text": " data distribution. You know, this is not anything like a real image, it's just a rounded image.", "tokens": [51164, 1412, 7316, 13, 509, 458, 11, 341, 307, 406, 1340, 411, 257, 957, 3256, 11, 309, 311, 445, 257, 23382, 3256, 13, 51376], "temperature": 0.0, "avg_logprob": -0.18809473956072773, "compression_ratio": 1.742081447963801, "no_speech_prob": 0.00010229700274067}, {"id": 576, "seek": 390604, "start": 3926.2799999999997, "end": 3932.44, "text": " And so this is going to be our X, I guess, X uppercase T, right? That's what we start out with.", "tokens": [51376, 400, 370, 341, 307, 516, 281, 312, 527, 1783, 11, 286, 2041, 11, 1783, 11775, 2869, 651, 314, 11, 558, 30, 663, 311, 437, 321, 722, 484, 365, 13, 51684], "temperature": 0.0, "avg_logprob": -0.18809473956072773, "compression_ratio": 1.742081447963801, "no_speech_prob": 0.00010229700274067}, {"id": 577, "seek": 393244, "start": 3932.44, "end": 3939.4, "text": " And we want to go from X uppercase T all the way to X zero. So what we do is we go through each of", "tokens": [50364, 400, 321, 528, 281, 352, 490, 1783, 11775, 2869, 651, 314, 439, 264, 636, 281, 1783, 4018, 13, 407, 437, 321, 360, 307, 321, 352, 807, 1184, 295, 50712], "temperature": 0.0, "avg_logprob": -0.27476853870210194, "compression_ratio": 1.6160337552742616, "no_speech_prob": 0.0002304987283423543}, {"id": 578, "seek": 393244, "start": 3939.4, "end": 3946.92, "text": " the time steps and create, we have to put it in this sort of batch format, because that's what our", "tokens": [50712, 264, 565, 4439, 293, 1884, 11, 321, 362, 281, 829, 309, 294, 341, 1333, 295, 15245, 7877, 11, 570, 300, 311, 437, 527, 51088], "temperature": 0.0, "avg_logprob": -0.27476853870210194, "compression_ratio": 1.6160337552742616, "no_speech_prob": 0.0002304987283423543}, {"id": 579, "seek": 393244, "start": 3947.48, "end": 3955.32, "text": " neural network expects, so we just have to format it appropriately. And we'll get to Z", "tokens": [51116, 18161, 3209, 33280, 11, 370, 321, 445, 362, 281, 7877, 309, 23505, 13, 400, 321, 603, 483, 281, 1176, 51508], "temperature": 0.0, "avg_logprob": -0.27476853870210194, "compression_ratio": 1.6160337552742616, "no_speech_prob": 0.0002304987283423543}, {"id": 580, "seek": 393244, "start": 3955.32, "end": 3959.96, "text": " in just a moment, I'll explain that in just a moment. But of course, we just, I could have similar", "tokens": [51508, 294, 445, 257, 1623, 11, 286, 603, 2903, 300, 294, 445, 257, 1623, 13, 583, 295, 1164, 11, 321, 445, 11, 286, 727, 362, 2531, 51740], "temperature": 0.0, "avg_logprob": -0.27476853870210194, "compression_ratio": 1.6160337552742616, "no_speech_prob": 0.0002304987283423543}, {"id": 581, "seek": 395996, "start": 3960.12, "end": 3965.2400000000002, "text": " alpha bar, beta bar, which is getting those variables that we... And we faked beta bar,", "tokens": [50372, 8961, 2159, 11, 9861, 2159, 11, 597, 307, 1242, 729, 9102, 300, 321, 485, 400, 321, 283, 7301, 9861, 2159, 11, 50628], "temperature": 0.0, "avg_logprob": -0.39619140042603473, "compression_ratio": 1.7306273062730628, "no_speech_prob": 0.0010322093730792403}, {"id": 582, "seek": 395996, "start": 3965.2400000000002, "end": 3971.16, "text": " because we couldn't figure out how to type it, so we used B bar instead. Yes, yes. So yeah,", "tokens": [50628, 570, 321, 2809, 380, 2573, 484, 577, 281, 2010, 309, 11, 370, 321, 1143, 363, 2159, 2602, 13, 1079, 11, 2086, 13, 407, 1338, 11, 50924], "temperature": 0.0, "avg_logprob": -0.39619140042603473, "compression_ratio": 1.7306273062730628, "no_speech_prob": 0.0010322093730792403}, {"id": 583, "seek": 395996, "start": 3971.16, "end": 3977.2400000000002, "text": " in, yeah, we were able to get beta bar to work, I guess. But anyway, at each step, what we're", "tokens": [50924, 294, 11, 1338, 11, 321, 645, 1075, 281, 483, 9861, 2159, 281, 589, 11, 286, 2041, 13, 583, 4033, 11, 412, 1184, 1823, 11, 437, 321, 434, 51228], "temperature": 0.0, "avg_logprob": -0.39619140042603473, "compression_ratio": 1.7306273062730628, "no_speech_prob": 0.0010322093730792403}, {"id": 584, "seek": 395996, "start": 3977.2400000000002, "end": 3981.56, "text": " trying to do is to try to predict what direction we need to go, and that direction is given by", "tokens": [51228, 1382, 281, 360, 307, 281, 853, 281, 6069, 437, 3513, 321, 643, 281, 352, 11, 293, 300, 3513, 307, 2212, 538, 51444], "temperature": 0.0, "avg_logprob": -0.39619140042603473, "compression_ratio": 1.7306273062730628, "no_speech_prob": 0.0010322093730792403}, {"id": 585, "seek": 395996, "start": 3981.56, "end": 3987.96, "text": " our noise predicting model, right? So what we do is we pass in X of T, and our, our noise prediction", "tokens": [51444, 527, 5658, 32884, 2316, 11, 558, 30, 407, 437, 321, 360, 307, 321, 1320, 294, 1783, 295, 314, 11, 293, 527, 11, 527, 5658, 17630, 51764], "temperature": 0.0, "avg_logprob": -0.39619140042603473, "compression_ratio": 1.7306273062730628, "no_speech_prob": 0.0010322093730792403}, {"id": 586, "seek": 398796, "start": 3987.96, "end": 3994.2, "text": " is given by our noise prediction model. So what we do is we pass in X of T, and our current time", "tokens": [50364, 307, 2212, 538, 527, 5658, 17630, 2316, 13, 407, 437, 321, 360, 307, 321, 1320, 294, 1783, 295, 314, 11, 293, 527, 2190, 565, 50676], "temperature": 0.0, "avg_logprob": -0.3539407903497869, "compression_ratio": 1.807843137254902, "no_speech_prob": 0.0010161937680095434}, {"id": 587, "seek": 398796, "start": 3994.2, "end": 3998.68, "text": " step into our model, and we get this noise prediction, and that's the direction that we", "tokens": [50676, 1823, 666, 527, 2316, 11, 293, 321, 483, 341, 5658, 17630, 11, 293, 300, 311, 264, 3513, 300, 321, 50900], "temperature": 0.0, "avg_logprob": -0.3539407903497869, "compression_ratio": 1.807843137254902, "no_speech_prob": 0.0010161937680095434}, {"id": 588, "seek": 398796, "start": 3998.68, "end": 4005.16, "text": " need to move in. So basically, we take X of T, we first attempt to completely remove the noise,", "tokens": [50900, 643, 281, 1286, 294, 13, 407, 1936, 11, 321, 747, 1783, 295, 314, 11, 321, 700, 5217, 281, 2584, 4159, 264, 5658, 11, 51224], "temperature": 0.0, "avg_logprob": -0.3539407903497869, "compression_ratio": 1.807843137254902, "no_speech_prob": 0.0010161937680095434}, {"id": 589, "seek": 398796, "start": 4005.16, "end": 4009.48, "text": " right? That's what this is doing. That's what X zero hat is, that's completely removing the noise.", "tokens": [51224, 558, 30, 663, 311, 437, 341, 307, 884, 13, 663, 311, 437, 1783, 4018, 2385, 307, 11, 300, 311, 2584, 12720, 264, 5658, 13, 51440], "temperature": 0.0, "avg_logprob": -0.3539407903497869, "compression_ratio": 1.807843137254902, "no_speech_prob": 0.0010161937680095434}, {"id": 590, "seek": 398796, "start": 4011.32, "end": 4014.68, "text": " And of course, as we said, that estimate at the beginning won't be very accurate.", "tokens": [51532, 400, 295, 1164, 11, 382, 321, 848, 11, 300, 12539, 412, 264, 2863, 1582, 380, 312, 588, 8559, 13, 51700], "temperature": 0.0, "avg_logprob": -0.3539407903497869, "compression_ratio": 1.807843137254902, "no_speech_prob": 0.0010161937680095434}, {"id": 591, "seek": 401468, "start": 4014.68, "end": 4020.68, "text": " So we have some coefficients here where we have a coefficient of how much that we keep of this estimate", "tokens": [50364, 407, 321, 362, 512, 31994, 510, 689, 321, 362, 257, 17619, 295, 577, 709, 300, 321, 1066, 295, 341, 12539, 50664], "temperature": 0.0, "avg_logprob": -0.256203130086263, "compression_ratio": 1.6420454545454546, "no_speech_prob": 0.041456323117017746}, {"id": 592, "seek": 401468, "start": 4020.68, "end": 4029.7999999999997, "text": " of our denoise image, and how much of the originally noisy image we keep. And on top of that,", "tokens": [50664, 295, 527, 1441, 38800, 3256, 11, 293, 577, 709, 295, 264, 7993, 24518, 3256, 321, 1066, 13, 400, 322, 1192, 295, 300, 11, 51120], "temperature": 0.0, "avg_logprob": -0.256203130086263, "compression_ratio": 1.6420454545454546, "no_speech_prob": 0.041456323117017746}, {"id": 593, "seek": 401468, "start": 4029.7999999999997, "end": 4036.68, "text": " we're going to add in some additional noise. So that's what we do here. We have X zero hat,", "tokens": [51120, 321, 434, 516, 281, 909, 294, 512, 4497, 5658, 13, 407, 300, 311, 437, 321, 360, 510, 13, 492, 362, 1783, 4018, 2385, 11, 51464], "temperature": 0.0, "avg_logprob": -0.256203130086263, "compression_ratio": 1.6420454545454546, "no_speech_prob": 0.041456323117017746}, {"id": 594, "seek": 403668, "start": 4037.24, "end": 4043.8799999999997, "text": " and so, and we multiply by its coefficient, and we have X of T, we multiply by some coefficient,", "tokens": [50392, 293, 370, 11, 293, 321, 12972, 538, 1080, 17619, 11, 293, 321, 362, 1783, 295, 314, 11, 321, 12972, 538, 512, 17619, 11, 50724], "temperature": 0.0, "avg_logprob": -0.4219355642059703, "compression_ratio": 1.664804469273743, "no_speech_prob": 0.0004655226948671043}, {"id": 595, "seek": 403668, "start": 4043.8799999999997, "end": 4048.8399999999997, "text": " and we also add some additional noise. That's what Z is. It's just, it's basically a weighted average", "tokens": [50724, 293, 321, 611, 909, 512, 4497, 5658, 13, 663, 311, 437, 1176, 307, 13, 467, 311, 445, 11, 309, 311, 1936, 257, 32807, 4274, 50972], "temperature": 0.0, "avg_logprob": -0.4219355642059703, "compression_ratio": 1.664804469273743, "no_speech_prob": 0.0004655226948671043}, {"id": 596, "seek": 403668, "start": 4048.8399999999997, "end": 4060.8399999999997, "text": " of the two, plus the noise. And then the whole idea is that, as our, as we get closer and closer to", "tokens": [50972, 295, 264, 732, 11, 1804, 264, 5658, 13, 400, 550, 264, 1379, 1558, 307, 300, 11, 382, 527, 11, 382, 321, 483, 4966, 293, 4966, 281, 51572], "temperature": 0.0, "avg_logprob": -0.4219355642059703, "compression_ratio": 1.664804469273743, "no_speech_prob": 0.0004655226948671043}, {"id": 597, "seek": 406084, "start": 4061.08, "end": 4069.08, "text": " get closer and closer to a time step equals to zero, our estimate of X zero will be more and", "tokens": [50376, 483, 4966, 293, 4966, 281, 257, 565, 1823, 6915, 281, 4018, 11, 527, 12539, 295, 1783, 4018, 486, 312, 544, 293, 50776], "temperature": 0.0, "avg_logprob": -0.2425258282533626, "compression_ratio": 1.9494949494949494, "no_speech_prob": 0.06852992624044418}, {"id": 598, "seek": 406084, "start": 4069.08, "end": 4077.4, "text": " more accurate. So our X zero coefficient will get closer and closer as we're, you know, increasing", "tokens": [50776, 544, 8559, 13, 407, 527, 1783, 4018, 17619, 486, 483, 4966, 293, 4966, 382, 321, 434, 11, 291, 458, 11, 5662, 51192], "temperature": 0.0, "avg_logprob": -0.2425258282533626, "compression_ratio": 1.9494949494949494, "no_speech_prob": 0.06852992624044418}, {"id": 599, "seek": 406084, "start": 4077.4, "end": 4083.2400000000002, "text": " our, you know, going through the process. And then our X T coefficient will get closer and closer to", "tokens": [51192, 527, 11, 291, 458, 11, 516, 807, 264, 1399, 13, 400, 550, 527, 1783, 314, 17619, 486, 483, 4966, 293, 4966, 281, 51484], "temperature": 0.0, "avg_logprob": -0.2425258282533626, "compression_ratio": 1.9494949494949494, "no_speech_prob": 0.06852992624044418}, {"id": 600, "seek": 406084, "start": 4083.2400000000002, "end": 4089.7200000000003, "text": " zero. So basically, we're going to be weighting more and more of the X zero hat estimate, and", "tokens": [51484, 4018, 13, 407, 1936, 11, 321, 434, 516, 281, 312, 3364, 278, 544, 293, 544, 295, 264, 1783, 4018, 2385, 12539, 11, 293, 51808], "temperature": 0.0, "avg_logprob": -0.2425258282533626, "compression_ratio": 1.9494949494949494, "no_speech_prob": 0.06852992624044418}, {"id": 601, "seek": 408972, "start": 4089.7999999999997, "end": 4095.48, "text": " less and less of the X T as we're getting closer and closer to our final time step. And so at the", "tokens": [50368, 1570, 293, 1570, 295, 264, 1783, 314, 382, 321, 434, 1242, 4966, 293, 4966, 281, 527, 2572, 565, 1823, 13, 400, 370, 412, 264, 50652], "temperature": 0.0, "avg_logprob": -0.2065188691422746, "compression_ratio": 1.5635359116022098, "no_speech_prob": 4.198543683742173e-05}, {"id": 602, "seek": 408972, "start": 4095.48, "end": 4103.32, "text": " end of the day, we will have our estimated generated image. So that's kind of an overview", "tokens": [50652, 917, 295, 264, 786, 11, 321, 486, 362, 527, 14109, 10833, 3256, 13, 407, 300, 311, 733, 295, 364, 12492, 51044], "temperature": 0.0, "avg_logprob": -0.2065188691422746, "compression_ratio": 1.5635359116022098, "no_speech_prob": 4.198543683742173e-05}, {"id": 603, "seek": 408972, "start": 4103.32, "end": 4114.2, "text": " of the sampling process. So yeah. So yeah, as basically the way I implemented it here was I had", "tokens": [51044, 295, 264, 21179, 1399, 13, 407, 1338, 13, 407, 1338, 11, 382, 1936, 264, 636, 286, 12270, 309, 510, 390, 286, 632, 51588], "temperature": 0.0, "avg_logprob": -0.2065188691422746, "compression_ratio": 1.5635359116022098, "no_speech_prob": 4.198543683742173e-05}, {"id": 604, "seek": 411420, "start": 4114.2, "end": 4123.5599999999995, "text": " the sample function that's part of our callback. And it will take in the model and the kind of", "tokens": [50364, 264, 6889, 2445, 300, 311, 644, 295, 527, 818, 3207, 13, 400, 309, 486, 747, 294, 264, 2316, 293, 264, 733, 295, 50832], "temperature": 0.0, "avg_logprob": -0.19138043469721727, "compression_ratio": 1.7035398230088497, "no_speech_prob": 0.14411665499210358}, {"id": 605, "seek": 411420, "start": 4123.5599999999995, "end": 4128.44, "text": " shape that you want for your images that you're producing. So like, if you want to specify how", "tokens": [50832, 3909, 300, 291, 528, 337, 428, 5267, 300, 291, 434, 10501, 13, 407, 411, 11, 498, 291, 528, 281, 16500, 577, 51076], "temperature": 0.0, "avg_logprob": -0.19138043469721727, "compression_ratio": 1.7035398230088497, "no_speech_prob": 0.14411665499210358}, {"id": 606, "seek": 411420, "start": 4128.44, "end": 4132.36, "text": " many images you produce, you know, that's going to be part of your back size or whatever. And you'll", "tokens": [51076, 867, 5267, 291, 5258, 11, 291, 458, 11, 300, 311, 516, 281, 312, 644, 295, 428, 646, 2744, 420, 2035, 13, 400, 291, 603, 51272], "temperature": 0.0, "avg_logprob": -0.19138043469721727, "compression_ratio": 1.7035398230088497, "no_speech_prob": 0.14411665499210358}, {"id": 607, "seek": 411420, "start": 4132.36, "end": 4137.8, "text": " just see that in a moment. But yeah, it's just part of the callback. So then we basically have", "tokens": [51272, 445, 536, 300, 294, 257, 1623, 13, 583, 1338, 11, 309, 311, 445, 644, 295, 264, 818, 3207, 13, 407, 550, 321, 1936, 362, 51544], "temperature": 0.0, "avg_logprob": -0.19138043469721727, "compression_ratio": 1.7035398230088497, "no_speech_prob": 0.14411665499210358}, {"id": 608, "seek": 413780, "start": 4137.8, "end": 4147.8, "text": " our DDPM callback. And then we could just call the sample method of our DDPM callback. And we", "tokens": [50364, 527, 413, 11373, 44, 818, 3207, 13, 400, 550, 321, 727, 445, 818, 264, 6889, 3170, 295, 527, 413, 11373, 44, 818, 3207, 13, 400, 321, 50864], "temperature": 0.0, "avg_logprob": -0.19776998247419084, "compression_ratio": 1.5978260869565217, "no_speech_prob": 0.11120414733886719}, {"id": 609, "seek": 413780, "start": 4147.8, "end": 4153.24, "text": " pass in our model. And then here you can see we're going to produce, for example, 16 images. And it", "tokens": [50864, 1320, 294, 527, 2316, 13, 400, 550, 510, 291, 393, 536, 321, 434, 516, 281, 5258, 11, 337, 1365, 11, 3165, 5267, 13, 400, 309, 51136], "temperature": 0.0, "avg_logprob": -0.19776998247419084, "compression_ratio": 1.5978260869565217, "no_speech_prob": 0.11120414733886719}, {"id": 610, "seek": 413780, "start": 4153.24, "end": 4161.56, "text": " just has to be a one channel image of shape 32 by 32. And we get our samples. And one thing I forgot", "tokens": [51136, 445, 575, 281, 312, 257, 472, 2269, 3256, 295, 3909, 8858, 538, 8858, 13, 400, 321, 483, 527, 10938, 13, 400, 472, 551, 286, 5298, 51552], "temperature": 0.0, "avg_logprob": -0.19776998247419084, "compression_ratio": 1.5978260869565217, "no_speech_prob": 0.11120414733886719}, {"id": 611, "seek": 416156, "start": 4161.56, "end": 4169.080000000001, "text": " to note was that I am collecting each of the time step, the estimate, the effects of t. So", "tokens": [50364, 281, 3637, 390, 300, 286, 669, 12510, 1184, 295, 264, 565, 1823, 11, 264, 12539, 11, 264, 5065, 295, 256, 13, 407, 50740], "temperature": 0.0, "avg_logprob": -0.3799985901922242, "compression_ratio": 1.6720647773279351, "no_speech_prob": 0.265774667263031}, {"id": 612, "seek": 416156, "start": 4170.200000000001, "end": 4175.400000000001, "text": " the predictions here, you can see that there are a thousand of them. We want the last one because", "tokens": [50796, 264, 21264, 510, 11, 291, 393, 536, 300, 456, 366, 257, 4714, 295, 552, 13, 492, 528, 264, 1036, 472, 570, 51056], "temperature": 0.0, "avg_logprob": -0.3799985901922242, "compression_ratio": 1.6720647773279351, "no_speech_prob": 0.265774667263031}, {"id": 613, "seek": 416156, "start": 4175.400000000001, "end": 4180.68, "text": " that is our final generation. So we want the last one. And that's what we're saying.", "tokens": [51056, 300, 307, 527, 2572, 5125, 13, 407, 321, 528, 264, 1036, 472, 13, 400, 300, 311, 437, 321, 434, 1566, 13, 51320], "temperature": 0.0, "avg_logprob": -0.3799985901922242, "compression_ratio": 1.6720647773279351, "no_speech_prob": 0.265774667263031}, {"id": 614, "seek": 416156, "start": 4180.68, "end": 4181.56, "text": " And it's sad actually.", "tokens": [51320, 400, 309, 311, 4227, 767, 13, 51364], "temperature": 0.0, "avg_logprob": -0.3799985901922242, "compression_ratio": 1.6720647773279351, "no_speech_prob": 0.265774667263031}, {"id": 615, "seek": 416156, "start": 4182.76, "end": 4183.4800000000005, "text": " Yeah. And this is.", "tokens": [51424, 865, 13, 400, 341, 307, 13, 51460], "temperature": 0.0, "avg_logprob": -0.3799985901922242, "compression_ratio": 1.6720647773279351, "no_speech_prob": 0.265774667263031}, {"id": 616, "seek": 416156, "start": 4183.4800000000005, "end": 4188.84, "text": " You know, I mean, we've come a long way since DDPM. So this is like slower and less great than it", "tokens": [51460, 509, 458, 11, 286, 914, 11, 321, 600, 808, 257, 938, 636, 1670, 413, 11373, 44, 13, 407, 341, 307, 411, 14009, 293, 1570, 869, 813, 309, 51728], "temperature": 0.0, "avg_logprob": -0.3799985901922242, "compression_ratio": 1.6720647773279351, "no_speech_prob": 0.265774667263031}, {"id": 617, "seek": 418884, "start": 4188.84, "end": 4193.96, "text": " could be. But considering that, except for Unet, we've done this from scratch,", "tokens": [50364, 727, 312, 13, 583, 8079, 300, 11, 3993, 337, 1156, 302, 11, 321, 600, 1096, 341, 490, 8459, 11, 50620], "temperature": 0.0, "avg_logprob": -0.24358712255427267, "compression_ratio": 1.649056603773585, "no_speech_prob": 0.0007672760402783751}, {"id": 618, "seek": 418884, "start": 4193.96, "end": 4199.08, "text": " you know, literally from matrix multiplication. I think those are pretty decent.", "tokens": [50620, 291, 458, 11, 3736, 490, 8141, 27290, 13, 286, 519, 729, 366, 1238, 8681, 13, 50876], "temperature": 0.0, "avg_logprob": -0.24358712255427267, "compression_ratio": 1.649056603773585, "no_speech_prob": 0.0007672760402783751}, {"id": 619, "seek": 418884, "start": 4200.28, "end": 4205.56, "text": " Yeah. And we're only trained for about five epochs. It took like, you know, maybe like four", "tokens": [50936, 865, 13, 400, 321, 434, 787, 8895, 337, 466, 1732, 30992, 28346, 13, 467, 1890, 411, 11, 291, 458, 11, 1310, 411, 1451, 51200], "temperature": 0.0, "avg_logprob": -0.24358712255427267, "compression_ratio": 1.649056603773585, "no_speech_prob": 0.0007672760402783751}, {"id": 620, "seek": 418884, "start": 4205.56, "end": 4210.68, "text": " minutes to train this model, something like that. It's pretty quick. And this is what we get with", "tokens": [51200, 2077, 281, 3847, 341, 2316, 11, 746, 411, 300, 13, 467, 311, 1238, 1702, 13, 400, 341, 307, 437, 321, 483, 365, 51456], "temperature": 0.0, "avg_logprob": -0.24358712255427267, "compression_ratio": 1.649056603773585, "no_speech_prob": 0.0007672760402783751}, {"id": 621, "seek": 418884, "start": 4210.68, "end": 4215.96, "text": " very little training. And it's, yeah, pretty decent. You can see, of course, some clear", "tokens": [51456, 588, 707, 3097, 13, 400, 309, 311, 11, 1338, 11, 1238, 8681, 13, 509, 393, 536, 11, 295, 1164, 11, 512, 1850, 51720], "temperature": 0.0, "avg_logprob": -0.24358712255427267, "compression_ratio": 1.649056603773585, "no_speech_prob": 0.0007672760402783751}, {"id": 622, "seek": 421596, "start": 4216.68, "end": 4220.52, "text": " shirts and shoes and pants and whatever else.", "tokens": [50400, 20832, 293, 6654, 293, 10082, 293, 2035, 1646, 13, 50592], "temperature": 0.0, "avg_logprob": -0.35731155927791153, "compression_ratio": 1.5072463768115942, "no_speech_prob": 0.0008040166576392949}, {"id": 623, "seek": 421596, "start": 4220.52, "end": 4224.76, "text": " Yeah. And you can see fabric and it's got texture and things have buckles.", "tokens": [50592, 865, 13, 400, 291, 393, 536, 7253, 293, 309, 311, 658, 8091, 293, 721, 362, 14894, 904, 13, 50804], "temperature": 0.0, "avg_logprob": -0.35731155927791153, "compression_ratio": 1.5072463768115942, "no_speech_prob": 0.0008040166576392949}, {"id": 624, "seek": 421596, "start": 4226.76, "end": 4227.26, "text": " Yeah.", "tokens": [50904, 865, 13, 50929], "temperature": 0.0, "avg_logprob": -0.35731155927791153, "compression_ratio": 1.5072463768115942, "no_speech_prob": 0.0008040166576392949}, {"id": 625, "seek": 421596, "start": 4228.2, "end": 4233.88, "text": " You know, I mean, to compare, like we did generative modeling in the first time we did", "tokens": [50976, 509, 458, 11, 286, 914, 11, 281, 6794, 11, 411, 321, 630, 1337, 1166, 15983, 294, 264, 700, 565, 321, 630, 51260], "temperature": 0.0, "avg_logprob": -0.35731155927791153, "compression_ratio": 1.5072463768115942, "no_speech_prob": 0.0008040166576392949}, {"id": 626, "seek": 421596, "start": 4234.44, "end": 4241.0, "text": " part two, back in the days when something called Vassusky and Gann was just new, which is actually", "tokens": [51288, 644, 732, 11, 646, 294, 264, 1708, 562, 746, 1219, 691, 640, 301, 4133, 293, 460, 969, 390, 445, 777, 11, 597, 307, 767, 51616], "temperature": 0.0, "avg_logprob": -0.35731155927791153, "compression_ratio": 1.5072463768115942, "no_speech_prob": 0.0008040166576392949}, {"id": 627, "seek": 424100, "start": 4241.0, "end": 4246.44, "text": " created by the same guy that created PipeTorch or one of the two guys, Sumith. And we trained", "tokens": [50364, 2942, 538, 264, 912, 2146, 300, 2942, 430, 6527, 51, 284, 339, 420, 472, 295, 264, 732, 1074, 11, 8626, 355, 13, 400, 321, 8895, 50636], "temperature": 0.0, "avg_logprob": -0.2513684897587217, "compression_ratio": 1.5786802030456852, "no_speech_prob": 0.08875512331724167}, {"id": 628, "seek": 424100, "start": 4246.44, "end": 4253.64, "text": " for hours and hours and hours and got things that I'm not sure were any better than this.", "tokens": [50636, 337, 2496, 293, 2496, 293, 2496, 293, 658, 721, 300, 286, 478, 406, 988, 645, 604, 1101, 813, 341, 13, 50996], "temperature": 0.0, "avg_logprob": -0.2513684897587217, "compression_ratio": 1.5786802030456852, "no_speech_prob": 0.08875512331724167}, {"id": 629, "seek": 424100, "start": 4253.64, "end": 4255.0, "text": " So things have come a long way.", "tokens": [50996, 407, 721, 362, 808, 257, 938, 636, 13, 51064], "temperature": 0.0, "avg_logprob": -0.2513684897587217, "compression_ratio": 1.5786802030456852, "no_speech_prob": 0.08875512331724167}, {"id": 630, "seek": 424100, "start": 4255.96, "end": 4267.72, "text": " Yeah. Yeah. And of course, Ben, yeah, so we can see Ben, like how this sampling progresses over", "tokens": [51112, 865, 13, 865, 13, 400, 295, 1164, 11, 3964, 11, 1338, 11, 370, 321, 393, 536, 3964, 11, 411, 577, 341, 21179, 41929, 670, 51700], "temperature": 0.0, "avg_logprob": -0.2513684897587217, "compression_ratio": 1.5786802030456852, "no_speech_prob": 0.08875512331724167}, {"id": 631, "seek": 426772, "start": 4268.52, "end": 4273.96, "text": " the multiple time steps. So that's what I'm showing here because I collected during the", "tokens": [50404, 264, 3866, 565, 4439, 13, 407, 300, 311, 437, 286, 478, 4099, 510, 570, 286, 11087, 1830, 264, 50676], "temperature": 0.0, "avg_logprob": -0.27488926000762404, "compression_ratio": 1.7813765182186234, "no_speech_prob": 0.03566647320985794}, {"id": 632, "seek": 426772, "start": 4273.96, "end": 4277.88, "text": " sampling process, we are collecting at each time step what that estimate looks like.", "tokens": [50676, 21179, 1399, 11, 321, 366, 12510, 412, 1184, 565, 1823, 437, 300, 12539, 1542, 411, 13, 50872], "temperature": 0.0, "avg_logprob": -0.27488926000762404, "compression_ratio": 1.7813765182186234, "no_speech_prob": 0.03566647320985794}, {"id": 633, "seek": 426772, "start": 4279.88, "end": 4284.280000000001, "text": " And you can kind of see here. And so this is an estimate out of like the noisy image", "tokens": [50972, 400, 291, 393, 733, 295, 536, 510, 13, 400, 370, 341, 307, 364, 12539, 484, 295, 411, 264, 24518, 3256, 51192], "temperature": 0.0, "avg_logprob": -0.27488926000762404, "compression_ratio": 1.7813765182186234, "no_speech_prob": 0.03566647320985794}, {"id": 634, "seek": 426772, "start": 4284.280000000001, "end": 4289.08, "text": " over the time steps. Oops. And I guess I had to pause. Yeah, you can kind of see.", "tokens": [51192, 670, 264, 565, 4439, 13, 21726, 13, 400, 286, 2041, 286, 632, 281, 10465, 13, 865, 11, 291, 393, 733, 295, 536, 13, 51432], "temperature": 0.0, "avg_logprob": -0.27488926000762404, "compression_ratio": 1.7813765182186234, "no_speech_prob": 0.03566647320985794}, {"id": 635, "seek": 426772, "start": 4290.2, "end": 4294.6, "text": " But you'll notice that actually, so we actually, what we did is like, okay, so we selected an image,", "tokens": [51488, 583, 291, 603, 3449, 300, 767, 11, 370, 321, 767, 11, 437, 321, 630, 307, 411, 11, 1392, 11, 370, 321, 8209, 364, 3256, 11, 51708], "temperature": 0.0, "avg_logprob": -0.27488926000762404, "compression_ratio": 1.7813765182186234, "no_speech_prob": 0.03566647320985794}, {"id": 636, "seek": 429460, "start": 4295.320000000001, "end": 4299.56, "text": " which is like the ninth image. So that's this image here. So we're looking at this image,", "tokens": [50400, 597, 307, 411, 264, 28207, 3256, 13, 407, 300, 311, 341, 3256, 510, 13, 407, 321, 434, 1237, 412, 341, 3256, 11, 50612], "temperature": 0.0, "avg_logprob": -0.22861144853674847, "compression_ratio": 1.7355769230769231, "no_speech_prob": 0.0006563340430147946}, {"id": 637, "seek": 429460, "start": 4299.56, "end": 4304.84, "text": " particularly here. And we're going over. Yeah, we have a function here that's showing", "tokens": [50612, 4098, 510, 13, 400, 321, 434, 516, 670, 13, 865, 11, 321, 362, 257, 2445, 510, 300, 311, 4099, 50876], "temperature": 0.0, "avg_logprob": -0.22861144853674847, "compression_ratio": 1.7355769230769231, "no_speech_prob": 0.0006563340430147946}, {"id": 638, "seek": 429460, "start": 4305.4800000000005, "end": 4312.280000000001, "text": " the I time step during the sampling process of that image. And we're just getting the images.", "tokens": [50908, 264, 286, 565, 1823, 1830, 264, 21179, 1399, 295, 300, 3256, 13, 400, 321, 434, 445, 1242, 264, 5267, 13, 51248], "temperature": 0.0, "avg_logprob": -0.22861144853674847, "compression_ratio": 1.7355769230769231, "no_speech_prob": 0.0006563340430147946}, {"id": 639, "seek": 429460, "start": 4313.96, "end": 4322.04, "text": " And what we are doing is we're only showing basically from time step 800 to 1000. And here,", "tokens": [51332, 400, 437, 321, 366, 884, 307, 321, 434, 787, 4099, 1936, 490, 565, 1823, 13083, 281, 9714, 13, 400, 510, 11, 51736], "temperature": 0.0, "avg_logprob": -0.22861144853674847, "compression_ratio": 1.7355769230769231, "no_speech_prob": 0.0006563340430147946}, {"id": 640, "seek": 432204, "start": 4322.6, "end": 4326.44, "text": " we're just having it like where it's like, okay, we're looking at like, maybe every five steps,", "tokens": [50392, 321, 434, 445, 1419, 309, 411, 689, 309, 311, 411, 11, 1392, 11, 321, 434, 1237, 412, 411, 11, 1310, 633, 1732, 4439, 11, 50584], "temperature": 0.0, "avg_logprob": -0.2508448591135969, "compression_ratio": 1.5914893617021277, "no_speech_prob": 0.003884230274707079}, {"id": 641, "seek": 432204, "start": 4326.44, "end": 4333.48, "text": " and we're going from 800 to 9. And this time, it'll make it visually easier to see the transition.", "tokens": [50584, 293, 321, 434, 516, 490, 13083, 281, 1722, 13, 400, 341, 565, 11, 309, 603, 652, 309, 19622, 3571, 281, 536, 264, 6034, 13, 50936], "temperature": 0.0, "avg_logprob": -0.2508448591135969, "compression_ratio": 1.5914893617021277, "no_speech_prob": 0.003884230274707079}, {"id": 642, "seek": 432204, "start": 4333.48, "end": 4337.48, "text": " But what you'll notice is I didn't start all the way from zero, I started from 800.", "tokens": [50936, 583, 437, 291, 603, 3449, 307, 286, 994, 380, 722, 439, 264, 636, 490, 4018, 11, 286, 1409, 490, 13083, 13, 51136], "temperature": 0.0, "avg_logprob": -0.2508448591135969, "compression_ratio": 1.5914893617021277, "no_speech_prob": 0.003884230274707079}, {"id": 643, "seek": 432204, "start": 4338.04, "end": 4345.16, "text": " And the reason we do that is because actually, between zero and 800, there's very little change", "tokens": [51164, 400, 264, 1778, 321, 360, 300, 307, 570, 767, 11, 1296, 4018, 293, 13083, 11, 456, 311, 588, 707, 1319, 51520], "temperature": 0.0, "avg_logprob": -0.2508448591135969, "compression_ratio": 1.5914893617021277, "no_speech_prob": 0.003884230274707079}, {"id": 644, "seek": 434516, "start": 4345.24, "end": 4352.36, "text": " in terms of like, it's just mostly a noisy image. And it turns out, but yeah, as I make a note of", "tokens": [50368, 294, 2115, 295, 411, 11, 309, 311, 445, 5240, 257, 24518, 3256, 13, 400, 309, 4523, 484, 11, 457, 1338, 11, 382, 286, 652, 257, 3637, 295, 50724], "temperature": 0.0, "avg_logprob": -0.21194334030151368, "compression_ratio": 1.6455696202531647, "no_speech_prob": 0.4223882555961609}, {"id": 645, "seek": 434516, "start": 4352.36, "end": 4358.76, "text": " this here, it's actually a limitation of the noise schedule that is used in the original DDPM paper.", "tokens": [50724, 341, 510, 11, 309, 311, 767, 257, 27432, 295, 264, 5658, 7567, 300, 307, 1143, 294, 264, 3380, 30778, 18819, 3035, 13, 51044], "temperature": 0.0, "avg_logprob": -0.21194334030151368, "compression_ratio": 1.6455696202531647, "no_speech_prob": 0.4223882555961609}, {"id": 646, "seek": 434516, "start": 4358.76, "end": 4363.24, "text": " And especially when applied to some of these smaller images, when we're working with images", "tokens": [51044, 400, 2318, 562, 6456, 281, 512, 295, 613, 4356, 5267, 11, 562, 321, 434, 1364, 365, 5267, 51268], "temperature": 0.0, "avg_logprob": -0.21194334030151368, "compression_ratio": 1.6455696202531647, "no_speech_prob": 0.4223882555961609}, {"id": 647, "seek": 434516, "start": 4363.24, "end": 4371.4, "text": " of like size 32 by 32, or whatever. And so there are some other papers like the improved DDPM paper", "tokens": [51268, 295, 411, 2744, 8858, 538, 8858, 11, 420, 2035, 13, 400, 370, 456, 366, 512, 661, 10577, 411, 264, 9689, 413, 11373, 44, 3035, 51676], "temperature": 0.0, "avg_logprob": -0.21194334030151368, "compression_ratio": 1.6455696202531647, "no_speech_prob": 0.4223882555961609}, {"id": 648, "seek": 437140, "start": 4371.4, "end": 4376.12, "text": " that propose other sorts of noise schedules. And what I mean by noise schedule is basically", "tokens": [50364, 300, 17421, 661, 7527, 295, 5658, 28078, 13, 400, 437, 286, 914, 538, 5658, 7567, 307, 1936, 50600], "temperature": 0.0, "avg_logprob": -0.2458108629499163, "compression_ratio": 1.859375, "no_speech_prob": 0.012429896742105484}, {"id": 649, "seek": 437140, "start": 4376.839999999999, "end": 4383.719999999999, "text": " how beta is defined, basically. So you know, we had this definition of torch dot lens space for", "tokens": [50636, 577, 9861, 307, 7642, 11, 1936, 13, 407, 291, 458, 11, 321, 632, 341, 7123, 295, 27822, 5893, 6765, 1901, 337, 50980], "temperature": 0.0, "avg_logprob": -0.2458108629499163, "compression_ratio": 1.859375, "no_speech_prob": 0.012429896742105484}, {"id": 650, "seek": 437140, "start": 4383.719999999999, "end": 4388.92, "text": " our beta, but people have different ways of defining beta that lead to different properties.", "tokens": [50980, 527, 9861, 11, 457, 561, 362, 819, 2098, 295, 17827, 9861, 300, 1477, 281, 819, 7221, 13, 51240], "temperature": 0.0, "avg_logprob": -0.2458108629499163, "compression_ratio": 1.859375, "no_speech_prob": 0.012429896742105484}, {"id": 651, "seek": 437140, "start": 4388.92, "end": 4393.799999999999, "text": " So you know, things like that, people have come up with different improvements. And those sorts", "tokens": [51240, 407, 291, 458, 11, 721, 411, 300, 11, 561, 362, 808, 493, 365, 819, 13797, 13, 400, 729, 7527, 51484], "temperature": 0.0, "avg_logprob": -0.2458108629499163, "compression_ratio": 1.859375, "no_speech_prob": 0.012429896742105484}, {"id": 652, "seek": 437140, "start": 4393.799999999999, "end": 4398.599999999999, "text": " of improvements work well when we're working with these smaller images. And basically, the point is", "tokens": [51484, 295, 13797, 589, 731, 562, 321, 434, 1364, 365, 613, 4356, 5267, 13, 400, 1936, 11, 264, 935, 307, 51724], "temperature": 0.0, "avg_logprob": -0.2458108629499163, "compression_ratio": 1.859375, "no_speech_prob": 0.012429896742105484}, {"id": 653, "seek": 439860, "start": 4398.84, "end": 4402.68, "text": " if we are working from zero to 800, and it's just mostly just noise that entire time,", "tokens": [50376, 498, 321, 366, 1364, 490, 4018, 281, 13083, 11, 293, 309, 311, 445, 5240, 445, 5658, 300, 2302, 565, 11, 50568], "temperature": 0.0, "avg_logprob": -0.20448268949985504, "compression_ratio": 1.8344594594594594, "no_speech_prob": 0.015186646021902561}, {"id": 654, "seek": 439860, "start": 4403.96, "end": 4408.200000000001, "text": " we're not actually making full use of all those time steps. So it would be nice if we could", "tokens": [50632, 321, 434, 406, 767, 1455, 1577, 764, 295, 439, 729, 565, 4439, 13, 407, 309, 576, 312, 1481, 498, 321, 727, 50844], "temperature": 0.0, "avg_logprob": -0.20448268949985504, "compression_ratio": 1.8344594594594594, "no_speech_prob": 0.015186646021902561}, {"id": 655, "seek": 439860, "start": 4408.200000000001, "end": 4411.96, "text": " actually make full use of those time steps, and actually have it do something during that", "tokens": [50844, 767, 652, 1577, 764, 295, 729, 565, 4439, 11, 293, 767, 362, 309, 360, 746, 1830, 300, 51032], "temperature": 0.0, "avg_logprob": -0.20448268949985504, "compression_ratio": 1.8344594594594594, "no_speech_prob": 0.015186646021902561}, {"id": 656, "seek": 439860, "start": 4411.96, "end": 4416.92, "text": " time period. So there are some papers that examine this a little bit more carefully.", "tokens": [51032, 565, 2896, 13, 407, 456, 366, 512, 10577, 300, 17496, 341, 257, 707, 857, 544, 7500, 13, 51280], "temperature": 0.0, "avg_logprob": -0.20448268949985504, "compression_ratio": 1.8344594594594594, "no_speech_prob": 0.015186646021902561}, {"id": 657, "seek": 439860, "start": 4416.92, "end": 4421.88, "text": " And it would be kind of interesting for maybe some of you folks to also look at these papers and see", "tokens": [51280, 400, 309, 576, 312, 733, 295, 1880, 337, 1310, 512, 295, 291, 4024, 281, 611, 574, 412, 613, 10577, 293, 536, 51528], "temperature": 0.0, "avg_logprob": -0.20448268949985504, "compression_ratio": 1.8344594594594594, "no_speech_prob": 0.015186646021902561}, {"id": 658, "seek": 439860, "start": 4421.88, "end": 4428.280000000001, "text": " if you can try to implement those sorts of models with this notebook as a starting point.", "tokens": [51528, 498, 291, 393, 853, 281, 4445, 729, 7527, 295, 5245, 365, 341, 21060, 382, 257, 2891, 935, 13, 51848], "temperature": 0.0, "avg_logprob": -0.20448268949985504, "compression_ratio": 1.8344594594594594, "no_speech_prob": 0.015186646021902561}, {"id": 659, "seek": 442828, "start": 4428.28, "end": 4431.88, "text": " And it should be a fairly simple change in terms of like noise schedule or something like that.", "tokens": [50364, 400, 309, 820, 312, 257, 6457, 2199, 1319, 294, 2115, 295, 411, 5658, 7567, 420, 746, 411, 300, 13, 50544], "temperature": 0.0, "avg_logprob": -0.2125601869948367, "compression_ratio": 1.6623376623376624, "no_speech_prob": 0.00031999219208955765}, {"id": 660, "seek": 442828, "start": 4432.5199999999995, "end": 4437.48, "text": " So I actually think, you know, this is the start of our next journey, you know, which is our", "tokens": [50576, 407, 286, 767, 519, 11, 291, 458, 11, 341, 307, 264, 722, 295, 527, 958, 4671, 11, 291, 458, 11, 597, 307, 527, 50824], "temperature": 0.0, "avg_logprob": -0.2125601869948367, "compression_ratio": 1.6623376623376624, "no_speech_prob": 0.00031999219208955765}, {"id": 661, "seek": 442828, "start": 4437.48, "end": 4445.08, "text": " previous journey was, you know, going from being totally rubbish at fashion MNIST classification", "tokens": [50824, 3894, 4671, 390, 11, 291, 458, 11, 516, 490, 885, 3879, 29978, 412, 6700, 376, 45, 19756, 21538, 51204], "temperature": 0.0, "avg_logprob": -0.2125601869948367, "compression_ratio": 1.6623376623376624, "no_speech_prob": 0.00031999219208955765}, {"id": 662, "seek": 442828, "start": 4445.639999999999, "end": 4452.44, "text": " to being really good at it. I would say now we're like a little bit rubbish at doing fashion MNIST", "tokens": [51232, 281, 885, 534, 665, 412, 309, 13, 286, 576, 584, 586, 321, 434, 411, 257, 707, 857, 29978, 412, 884, 6700, 376, 45, 19756, 51572], "temperature": 0.0, "avg_logprob": -0.2125601869948367, "compression_ratio": 1.6623376623376624, "no_speech_prob": 0.00031999219208955765}, {"id": 663, "seek": 445244, "start": 4452.44, "end": 4463.879999999999, "text": " generation. And yeah, I think we should all now work from here over the next few lessons and so", "tokens": [50364, 5125, 13, 400, 1338, 11, 286, 519, 321, 820, 439, 586, 589, 490, 510, 670, 264, 958, 1326, 8820, 293, 370, 50936], "temperature": 0.0, "avg_logprob": -0.2412416046740962, "compression_ratio": 1.6265560165975104, "no_speech_prob": 0.0006164612714201212}, {"id": 664, "seek": 445244, "start": 4463.879999999999, "end": 4471.96, "text": " forth, and people, you know, trying things at home and all of us trying to make better and better", "tokens": [50936, 5220, 11, 293, 561, 11, 291, 458, 11, 1382, 721, 412, 1280, 293, 439, 295, 505, 1382, 281, 652, 1101, 293, 1101, 51340], "temperature": 0.0, "avg_logprob": -0.2412416046740962, "compression_ratio": 1.6265560165975104, "no_speech_prob": 0.0006164612714201212}, {"id": 665, "seek": 445244, "start": 4472.839999999999, "end": 4476.679999999999, "text": " generative models, you know, initially a fashion MNIST, and hopefully we'll get to the point where", "tokens": [51384, 1337, 1166, 5245, 11, 291, 458, 11, 9105, 257, 6700, 376, 45, 19756, 11, 293, 4696, 321, 603, 483, 281, 264, 935, 689, 51576], "temperature": 0.0, "avg_logprob": -0.2412416046740962, "compression_ratio": 1.6265560165975104, "no_speech_prob": 0.0006164612714201212}, {"id": 666, "seek": 445244, "start": 4476.679999999999, "end": 4481.4, "text": " we're so good at that, that we're like, oh, this is too easy. And then we'll pick something harder.", "tokens": [51576, 321, 434, 370, 665, 412, 300, 11, 300, 321, 434, 411, 11, 1954, 11, 341, 307, 886, 1858, 13, 400, 550, 321, 603, 1888, 746, 6081, 13, 51812], "temperature": 0.0, "avg_logprob": -0.2412416046740962, "compression_ratio": 1.6265560165975104, "no_speech_prob": 0.0006164612714201212}, {"id": 667, "seek": 448244, "start": 4483.4, "end": 4491.24, "text": " Yeah. And eventually that'll take us to stable diffusion and beyond, I imagine.", "tokens": [50412, 865, 13, 400, 4728, 300, 603, 747, 505, 281, 8351, 25242, 293, 4399, 11, 286, 3811, 13, 50804], "temperature": 0.0, "avg_logprob": -0.3340148058804599, "compression_ratio": 1.256, "no_speech_prob": 0.0001253290247404948}, {"id": 668, "seek": 448244, "start": 4492.44, "end": 4503.879999999999, "text": " Yeah. That's cool. I got some stuff to show you guys. If you're interested, I", "tokens": [50864, 865, 13, 663, 311, 1627, 13, 286, 658, 512, 1507, 281, 855, 291, 1074, 13, 759, 291, 434, 3102, 11, 286, 51436], "temperature": 0.0, "avg_logprob": -0.3340148058804599, "compression_ratio": 1.256, "no_speech_prob": 0.0001253290247404948}, {"id": 669, "seek": 450388, "start": 4504.4400000000005, "end": 4511.4800000000005, "text": " tried to better understand what was going on in Tunisia's notebook and tried doing it in a", "tokens": [50392, 3031, 281, 1101, 1223, 437, 390, 516, 322, 294, 21363, 15877, 311, 21060, 293, 3031, 884, 309, 294, 257, 50744], "temperature": 0.0, "avg_logprob": -0.48487299542094386, "compression_ratio": 1.575609756097561, "no_speech_prob": 0.03020423650741577}, {"id": 670, "seek": 450388, "start": 4511.4800000000005, "end": 4514.4400000000005, "text": " thousand different ways, and also see if I could just start to make it a bit faster.", "tokens": [50744, 4714, 819, 2098, 11, 293, 611, 536, 498, 286, 727, 445, 722, 281, 652, 309, 257, 857, 4663, 13, 50892], "temperature": 0.0, "avg_logprob": -0.48487299542094386, "compression_ratio": 1.575609756097561, "no_speech_prob": 0.03020423650741577}, {"id": 671, "seek": 450388, "start": 4516.84, "end": 4523.32, "text": " So that's what's in notebook 17, which I will share.", "tokens": [51012, 407, 300, 311, 437, 311, 294, 21060, 3282, 11, 597, 286, 486, 2073, 13, 51336], "temperature": 0.0, "avg_logprob": -0.48487299542094386, "compression_ratio": 1.575609756097561, "no_speech_prob": 0.03020423650741577}, {"id": 672, "seek": 450388, "start": 4528.04, "end": 4533.64, "text": " So we've already seen the start of notebook 17. Well, one thing I did do is I did a little bit", "tokens": [51572, 407, 321, 600, 1217, 1612, 264, 722, 295, 21060, 3282, 13, 1042, 11, 472, 551, 286, 630, 360, 307, 286, 630, 257, 707, 857, 51852], "temperature": 0.0, "avg_logprob": -0.48487299542094386, "compression_ratio": 1.575609756097561, "no_speech_prob": 0.03020423650741577}, {"id": 673, "seek": 453364, "start": 4534.360000000001, "end": 4538.92, "text": " just drew a picture for myself, partly just to remind myself what the real ones look like,", "tokens": [50400, 445, 12804, 257, 3036, 337, 2059, 11, 17031, 445, 281, 4160, 2059, 437, 264, 957, 2306, 574, 411, 11, 50628], "temperature": 0.0, "avg_logprob": -0.22848695257435675, "compression_ratio": 1.6, "no_speech_prob": 5.475700891111046e-05}, {"id": 674, "seek": 453364, "start": 4538.92, "end": 4545.64, "text": " and they definitely have more detail than the samples that Tunisia was showing.", "tokens": [50628, 293, 436, 2138, 362, 544, 2607, 813, 264, 10938, 300, 21363, 15877, 390, 4099, 13, 50964], "temperature": 0.0, "avg_logprob": -0.22848695257435675, "compression_ratio": 1.6, "no_speech_prob": 5.475700891111046e-05}, {"id": 675, "seek": 453364, "start": 4548.360000000001, "end": 4552.52, "text": " But they're not, you know, they're just 28 by 28. I mean, they're not super amazing images,", "tokens": [51100, 583, 436, 434, 406, 11, 291, 458, 11, 436, 434, 445, 7562, 538, 7562, 13, 286, 914, 11, 436, 434, 406, 1687, 2243, 5267, 11, 51308], "temperature": 0.0, "avg_logprob": -0.22848695257435675, "compression_ratio": 1.6, "no_speech_prob": 5.475700891111046e-05}, {"id": 676, "seek": 453364, "start": 4552.52, "end": 4556.84, "text": " and they're just black and white. So even if we're fantastic at this, they're never going to look", "tokens": [51308, 293, 436, 434, 445, 2211, 293, 2418, 13, 407, 754, 498, 321, 434, 5456, 412, 341, 11, 436, 434, 1128, 516, 281, 574, 51524], "temperature": 0.0, "avg_logprob": -0.22848695257435675, "compression_ratio": 1.6, "no_speech_prob": 5.475700891111046e-05}, {"id": 677, "seek": 455684, "start": 4557.72, "end": 4564.28, "text": " great, because we're using a small simple data set. As you always should, when you're doing any", "tokens": [50408, 869, 11, 570, 321, 434, 1228, 257, 1359, 2199, 1412, 992, 13, 1018, 291, 1009, 820, 11, 562, 291, 434, 884, 604, 50736], "temperature": 0.0, "avg_logprob": -0.20705131608612684, "compression_ratio": 1.7545454545454546, "no_speech_prob": 0.000410836364608258}, {"id": 678, "seek": 455684, "start": 4564.28, "end": 4570.28, "text": " kind of R&D or experiments, you should always use a small and simple data set, up until you're so", "tokens": [50736, 733, 295, 497, 5, 35, 420, 12050, 11, 291, 820, 1009, 764, 257, 1359, 293, 2199, 1412, 992, 11, 493, 1826, 291, 434, 370, 51036], "temperature": 0.0, "avg_logprob": -0.20705131608612684, "compression_ratio": 1.7545454545454546, "no_speech_prob": 0.000410836364608258}, {"id": 679, "seek": 455684, "start": 4570.28, "end": 4576.52, "text": " good at it that it's not challenging anymore. And even then, when you're exploring new ideas,", "tokens": [51036, 665, 412, 309, 300, 309, 311, 406, 7595, 3602, 13, 400, 754, 550, 11, 562, 291, 434, 12736, 777, 3487, 11, 51348], "temperature": 0.0, "avg_logprob": -0.20705131608612684, "compression_ratio": 1.7545454545454546, "no_speech_prob": 0.000410836364608258}, {"id": 680, "seek": 455684, "start": 4576.52, "end": 4582.360000000001, "text": " you should explore them on small simple data sets first. Yeah. So after I drew the various things,", "tokens": [51348, 291, 820, 6839, 552, 322, 1359, 2199, 1412, 6352, 700, 13, 865, 13, 407, 934, 286, 12804, 264, 3683, 721, 11, 51640], "temperature": 0.0, "avg_logprob": -0.20705131608612684, "compression_ratio": 1.7545454545454546, "no_speech_prob": 0.000410836364608258}, {"id": 681, "seek": 458236, "start": 4582.36, "end": 4589.4, "text": " what I like to do is, one thing I found challenging about working with your class", "tokens": [50364, 437, 286, 411, 281, 360, 307, 11, 472, 551, 286, 1352, 7595, 466, 1364, 365, 428, 1508, 50716], "temperature": 0.0, "avg_logprob": -0.22747039794921875, "compression_ratio": 1.6016949152542372, "no_speech_prob": 0.00031997982296161354}, {"id": 682, "seek": 458236, "start": 4589.4, "end": 4594.679999999999, "text": " Tunisia, because I find when stuff is inside a class, it's harder for me to explore. So I copied", "tokens": [50716, 21363, 15877, 11, 570, 286, 915, 562, 1507, 307, 1854, 257, 1508, 11, 309, 311, 6081, 337, 385, 281, 6839, 13, 407, 286, 25365, 50980], "temperature": 0.0, "avg_logprob": -0.22747039794921875, "compression_ratio": 1.6016949152542372, "no_speech_prob": 0.00031997982296161354}, {"id": 683, "seek": 458236, "start": 4594.679999999999, "end": 4605.32, "text": " and pasted it, the before batch contents, and called it Noisify. And so one of the things that's", "tokens": [50980, 293, 1791, 292, 309, 11, 264, 949, 15245, 15768, 11, 293, 1219, 309, 883, 271, 2505, 13, 400, 370, 472, 295, 264, 721, 300, 311, 51512], "temperature": 0.0, "avg_logprob": -0.22747039794921875, "compression_ratio": 1.6016949152542372, "no_speech_prob": 0.00031997982296161354}, {"id": 684, "seek": 458236, "start": 4605.32, "end": 4610.839999999999, "text": " fun to do that is it forces you to figure out what are the actual parameters to it. And so now that I,", "tokens": [51512, 1019, 281, 360, 300, 307, 309, 5874, 291, 281, 2573, 484, 437, 366, 264, 3539, 9834, 281, 309, 13, 400, 370, 586, 300, 286, 11, 51788], "temperature": 0.0, "avg_logprob": -0.22747039794921875, "compression_ratio": 1.6016949152542372, "no_speech_prob": 0.00031997982296161354}, {"id": 685, "seek": 461084, "start": 4610.92, "end": 4618.12, "text": " rather than putting it in the class, now that I've got all of my, you know, various things to do with,", "tokens": [50368, 2831, 813, 3372, 309, 294, 264, 1508, 11, 586, 300, 286, 600, 658, 439, 295, 452, 11, 291, 458, 11, 3683, 721, 281, 360, 365, 11, 50728], "temperature": 0.0, "avg_logprob": -0.24684718803123193, "compression_ratio": 1.6581196581196582, "no_speech_prob": 6.502748874481767e-05}, {"id": 686, "seek": 461084, "start": 4618.12, "end": 4624.04, "text": " so these are the three parameters to the DDPM callbacks in it. So then these things we can", "tokens": [50728, 370, 613, 366, 264, 1045, 9834, 281, 264, 413, 11373, 44, 818, 17758, 294, 309, 13, 407, 550, 613, 721, 321, 393, 51024], "temperature": 0.0, "avg_logprob": -0.24684718803123193, "compression_ratio": 1.6581196581196582, "no_speech_prob": 6.502748874481767e-05}, {"id": 687, "seek": 461084, "start": 4624.04, "end": 4633.16, "text": " calculate from that. So with those, then actually all we need is, yeah, what's the image that we're", "tokens": [51024, 8873, 490, 300, 13, 407, 365, 729, 11, 550, 767, 439, 321, 643, 307, 11, 1338, 11, 437, 311, 264, 3256, 300, 321, 434, 51480], "temperature": 0.0, "avg_logprob": -0.24684718803123193, "compression_ratio": 1.6581196581196582, "no_speech_prob": 6.502748874481767e-05}, {"id": 688, "seek": 461084, "start": 4633.16, "end": 4638.92, "text": " going to noisify, and then what's the what's the alpha bar, which I mean we can get from here,", "tokens": [51480, 516, 281, 572, 271, 2505, 11, 293, 550, 437, 311, 264, 437, 311, 264, 8961, 2159, 11, 597, 286, 914, 321, 393, 483, 490, 510, 11, 51768], "temperature": 0.0, "avg_logprob": -0.24684718803123193, "compression_ratio": 1.6581196581196582, "no_speech_prob": 6.502748874481767e-05}, {"id": 689, "seek": 463892, "start": 4638.92, "end": 4643.64, "text": " but it's a bit more general if you can pass in your alpha bar. So yeah, this is just copying and", "tokens": [50364, 457, 309, 311, 257, 857, 544, 2674, 498, 291, 393, 1320, 294, 428, 8961, 2159, 13, 407, 1338, 11, 341, 307, 445, 27976, 293, 50600], "temperature": 0.0, "avg_logprob": -0.22695548381280461, "compression_ratio": 1.6188524590163935, "no_speech_prob": 0.00019716464157681912}, {"id": 690, "seek": 463892, "start": 4643.64, "end": 4651.8, "text": " pasting from the class. But the nice thing is then I could experiment with it. So I can call Noisify", "tokens": [50600, 1791, 278, 490, 264, 1508, 13, 583, 264, 1481, 551, 307, 550, 286, 727, 5120, 365, 309, 13, 407, 286, 393, 818, 883, 271, 2505, 51008], "temperature": 0.0, "avg_logprob": -0.22695548381280461, "compression_ratio": 1.6188524590163935, "no_speech_prob": 0.00019716464157681912}, {"id": 691, "seek": 463892, "start": 4651.8, "end": 4661.24, "text": " on my first 25 images, and with a random T, each one's got a different random T, and so I can print", "tokens": [51008, 322, 452, 700, 3552, 5267, 11, 293, 365, 257, 4974, 314, 11, 1184, 472, 311, 658, 257, 819, 4974, 314, 11, 293, 370, 286, 393, 4482, 51480], "temperature": 0.0, "avg_logprob": -0.22695548381280461, "compression_ratio": 1.6188524590163935, "no_speech_prob": 0.00019716464157681912}, {"id": 692, "seek": 463892, "start": 4661.24, "end": 4666.2, "text": " out the T, and then I could actually use those as titles. And so this lets me, I thought this was", "tokens": [51480, 484, 264, 314, 11, 293, 550, 286, 727, 767, 764, 729, 382, 12992, 13, 400, 370, 341, 6653, 385, 11, 286, 1194, 341, 390, 51728], "temperature": 0.0, "avg_logprob": -0.22695548381280461, "compression_ratio": 1.6188524590163935, "no_speech_prob": 0.00019716464157681912}, {"id": 693, "seek": 466620, "start": 4666.2, "end": 4672.28, "text": " quite nice. I might actually rerun this, because actually none of these look like anything, because", "tokens": [50364, 1596, 1481, 13, 286, 1062, 767, 43819, 409, 341, 11, 570, 767, 6022, 295, 613, 574, 411, 1340, 11, 570, 50668], "temperature": 0.0, "avg_logprob": -0.2076872283337163, "compression_ratio": 1.6590909090909092, "no_speech_prob": 3.1693220080342144e-05}, {"id": 694, "seek": 466620, "start": 4672.28, "end": 4678.84, "text": " as it turns out in this particular case, all of the T's are over 200. And as Tanishq mentioned,", "tokens": [50668, 382, 309, 4523, 484, 294, 341, 1729, 1389, 11, 439, 295, 264, 314, 311, 366, 670, 2331, 13, 400, 382, 314, 7524, 80, 2835, 11, 50996], "temperature": 0.0, "avg_logprob": -0.2076872283337163, "compression_ratio": 1.6590909090909092, "no_speech_prob": 3.1693220080342144e-05}, {"id": 695, "seek": 466620, "start": 4678.84, "end": 4684.04, "text": " once you're over 200, it's almost impossible to see anything. So let me just rerun this,", "tokens": [50996, 1564, 291, 434, 670, 2331, 11, 309, 311, 1920, 6243, 281, 536, 1340, 13, 407, 718, 385, 445, 43819, 409, 341, 11, 51256], "temperature": 0.0, "avg_logprob": -0.2076872283337163, "compression_ratio": 1.6590909090909092, "no_speech_prob": 3.1693220080342144e-05}, {"id": 696, "seek": 466620, "start": 4684.04, "end": 4691.88, "text": " and see if we get a better, there we go, there's a better one. So with a T of 7,", "tokens": [51256, 293, 536, 498, 321, 483, 257, 1101, 11, 456, 321, 352, 11, 456, 311, 257, 1101, 472, 13, 407, 365, 257, 314, 295, 1614, 11, 51648], "temperature": 0.0, "avg_logprob": -0.2076872283337163, "compression_ratio": 1.6590909090909092, "no_speech_prob": 3.1693220080342144e-05}, {"id": 697, "seek": 469188, "start": 4692.84, "end": 4699.0, "text": " right, so remember T naught, T equals naught is the pure image. So T equals 7, it's just a slightly", "tokens": [50412, 558, 11, 370, 1604, 314, 13138, 11, 314, 6915, 13138, 307, 264, 6075, 3256, 13, 407, 314, 6915, 1614, 11, 309, 311, 445, 257, 4748, 50720], "temperature": 0.0, "avg_logprob": -0.2741695322016234, "compression_ratio": 1.5869565217391304, "no_speech_prob": 0.00013341786689125001}, {"id": 698, "seek": 469188, "start": 4699.56, "end": 4707.0, "text": " speckled-y image. And by 67, it's a pretty bad image. And by 94, it's very hard to see what it is at all.", "tokens": [50748, 768, 547, 1493, 12, 88, 3256, 13, 400, 538, 23879, 11, 309, 311, 257, 1238, 1578, 3256, 13, 400, 538, 30849, 11, 309, 311, 588, 1152, 281, 536, 437, 309, 307, 412, 439, 13, 51120], "temperature": 0.0, "avg_logprob": -0.2741695322016234, "compression_ratio": 1.5869565217391304, "no_speech_prob": 0.00013341786689125001}, {"id": 699, "seek": 469188, "start": 4708.36, "end": 4719.24, "text": " And by 293, maybe I can see a pair of pants? I'm not sure I can see anything. So yeah,", "tokens": [51188, 400, 538, 9413, 18, 11, 1310, 286, 393, 536, 257, 6119, 295, 10082, 30, 286, 478, 406, 988, 286, 393, 536, 1340, 13, 407, 1338, 11, 51732], "temperature": 0.0, "avg_logprob": -0.2741695322016234, "compression_ratio": 1.5869565217391304, "no_speech_prob": 0.00013341786689125001}, {"id": 700, "seek": 471924, "start": 4720.04, "end": 4726.2, "text": " by the way, there's a handy little, so we've, I think we've looked at map before in the course.", "tokens": [50404, 538, 264, 636, 11, 456, 311, 257, 13239, 707, 11, 370, 321, 600, 11, 286, 519, 321, 600, 2956, 412, 4471, 949, 294, 264, 1164, 13, 50712], "temperature": 0.0, "avg_logprob": -0.21497119140625, "compression_ratio": 1.7236363636363636, "no_speech_prob": 0.0001055456668836996}, {"id": 701, "seek": 471924, "start": 4726.2, "end": 4730.36, "text": " There's an extended version of map in fastcore, and one of the nice things is you can pass it a", "tokens": [50712, 821, 311, 364, 10913, 3037, 295, 4471, 294, 2370, 12352, 11, 293, 472, 295, 264, 1481, 721, 307, 291, 393, 1320, 309, 257, 50920], "temperature": 0.0, "avg_logprob": -0.21497119140625, "compression_ratio": 1.7236363636363636, "no_speech_prob": 0.0001055456668836996}, {"id": 702, "seek": 471924, "start": 4730.36, "end": 4736.04, "text": " string, and it basically just calls this format string, if you pass it a string rather than a", "tokens": [50920, 6798, 11, 293, 309, 1936, 445, 5498, 341, 7877, 6798, 11, 498, 291, 1320, 309, 257, 6798, 2831, 813, 257, 51204], "temperature": 0.0, "avg_logprob": -0.21497119140625, "compression_ratio": 1.7236363636363636, "no_speech_prob": 0.0001055456668836996}, {"id": 703, "seek": 471924, "start": 4736.04, "end": 4741.0, "text": " function. And so this is going to stringify everything using its representations. This is", "tokens": [51204, 2445, 13, 400, 370, 341, 307, 516, 281, 6798, 2505, 1203, 1228, 1080, 33358, 13, 639, 307, 51452], "temperature": 0.0, "avg_logprob": -0.21497119140625, "compression_ratio": 1.7236363636363636, "no_speech_prob": 0.0001055456668836996}, {"id": 704, "seek": 471924, "start": 4741.0, "end": 4748.92, "text": " how I got the titles out of it, just by the way. So yeah, I found this useful to be able to draw a", "tokens": [51452, 577, 286, 658, 264, 12992, 484, 295, 309, 11, 445, 538, 264, 636, 13, 407, 1338, 11, 286, 1352, 341, 4420, 281, 312, 1075, 281, 2642, 257, 51848], "temperature": 0.0, "avg_logprob": -0.21497119140625, "compression_ratio": 1.7236363636363636, "no_speech_prob": 0.0001055456668836996}, {"id": 705, "seek": 474892, "start": 4748.92, "end": 4757.8, "text": " picture of everything. And then I wanted to, yeah, look at what else can I do. So then I took,", "tokens": [50364, 3036, 295, 1203, 13, 400, 550, 286, 1415, 281, 11, 1338, 11, 574, 412, 437, 1646, 393, 286, 360, 13, 407, 550, 286, 1890, 11, 50808], "temperature": 0.0, "avg_logprob": -0.24247625139024523, "compression_ratio": 1.725, "no_speech_prob": 2.6688359866966493e-05}, {"id": 706, "seek": 474892, "start": 4757.8, "end": 4761.56, "text": " not so you won't be surprised to see, I took the sample method and turned that into a function.", "tokens": [50808, 406, 370, 291, 1582, 380, 312, 6100, 281, 536, 11, 286, 1890, 264, 6889, 3170, 293, 3574, 300, 666, 257, 2445, 13, 50996], "temperature": 0.0, "avg_logprob": -0.24247625139024523, "compression_ratio": 1.725, "no_speech_prob": 2.6688359866966493e-05}, {"id": 707, "seek": 474892, "start": 4762.68, "end": 4767.88, "text": " And I actually decided to pass everything that it needs, even, I mean, you could actually calculate", "tokens": [51052, 400, 286, 767, 3047, 281, 1320, 1203, 300, 309, 2203, 11, 754, 11, 286, 914, 11, 291, 727, 767, 8873, 51312], "temperature": 0.0, "avg_logprob": -0.24247625139024523, "compression_ratio": 1.725, "no_speech_prob": 2.6688359866966493e-05}, {"id": 708, "seek": 474892, "start": 4767.88, "end": 4772.04, "text": " pretty much all of these. But I thought since I've calculated them before, I'll just pass them in.", "tokens": [51312, 1238, 709, 439, 295, 613, 13, 583, 286, 1194, 1670, 286, 600, 15598, 552, 949, 11, 286, 603, 445, 1320, 552, 294, 13, 51520], "temperature": 0.0, "avg_logprob": -0.24247625139024523, "compression_ratio": 1.725, "no_speech_prob": 2.6688359866966493e-05}, {"id": 709, "seek": 474892, "start": 4772.04, "end": 4777.4800000000005, "text": " So this is all copied and pasted from Dinesh's version. And so that means the callback now is", "tokens": [51520, 407, 341, 307, 439, 25365, 293, 1791, 292, 490, 413, 1652, 71, 311, 3037, 13, 400, 370, 300, 1355, 264, 818, 3207, 586, 307, 51792], "temperature": 0.0, "avg_logprob": -0.24247625139024523, "compression_ratio": 1.725, "no_speech_prob": 2.6688359866966493e-05}, {"id": 710, "seek": 477748, "start": 4777.48, "end": 4784.759999999999, "text": " tiny, right? Because before batch is just noisify, and the sample method just calls the sample", "tokens": [50364, 5870, 11, 558, 30, 1436, 949, 15245, 307, 445, 572, 271, 2505, 11, 293, 264, 6889, 3170, 445, 5498, 264, 6889, 50728], "temperature": 0.0, "avg_logprob": -0.2416235605875651, "compression_ratio": 1.6115702479338843, "no_speech_prob": 0.00011061107215937227}, {"id": 711, "seek": 477748, "start": 4784.759999999999, "end": 4792.2, "text": " function. Now what I did do is I decided just to, yeah, I wanted to try like as many different ways", "tokens": [50728, 2445, 13, 823, 437, 286, 630, 360, 307, 286, 3047, 445, 281, 11, 1338, 11, 286, 1415, 281, 853, 411, 382, 867, 819, 2098, 51100], "temperature": 0.0, "avg_logprob": -0.2416235605875651, "compression_ratio": 1.6115702479338843, "no_speech_prob": 0.00011061107215937227}, {"id": 712, "seek": 477748, "start": 4792.2, "end": 4800.2, "text": " of doing this as possible. Partly as an exercise to help everybody like see all the different ways", "tokens": [51100, 295, 884, 341, 382, 1944, 13, 4100, 356, 382, 364, 5380, 281, 854, 2201, 411, 536, 439, 264, 819, 2098, 51500], "temperature": 0.0, "avg_logprob": -0.2416235605875651, "compression_ratio": 1.6115702479338843, "no_speech_prob": 0.00011061107215937227}, {"id": 713, "seek": 477748, "start": 4800.2, "end": 4806.12, "text": " we can work with our framework, you know. So I decided not to inherit from train.cb, but instead", "tokens": [51500, 321, 393, 589, 365, 527, 8388, 11, 291, 458, 13, 407, 286, 3047, 406, 281, 21389, 490, 3847, 13, 66, 65, 11, 457, 2602, 51796], "temperature": 0.0, "avg_logprob": -0.2416235605875651, "compression_ratio": 1.6115702479338843, "no_speech_prob": 0.00011061107215937227}, {"id": 714, "seek": 480612, "start": 4806.12, "end": 4815.08, "text": " I inherited from callback. So that means I can't use Dinesh's nifty trick of replacing predict.", "tokens": [50364, 286, 27091, 490, 818, 3207, 13, 407, 300, 1355, 286, 393, 380, 764, 413, 1652, 71, 311, 297, 37177, 4282, 295, 19139, 6069, 13, 50812], "temperature": 0.0, "avg_logprob": -0.19903351131238436, "compression_ratio": 1.4739583333333333, "no_speech_prob": 0.00013765388575848192}, {"id": 715, "seek": 480612, "start": 4815.72, "end": 4822.76, "text": " So instead I now need some way to pass in the two parts of the first element of the tuple,", "tokens": [50844, 407, 2602, 286, 586, 643, 512, 636, 281, 1320, 294, 264, 732, 3166, 295, 264, 700, 4478, 295, 264, 2604, 781, 11, 51196], "temperature": 0.0, "avg_logprob": -0.19903351131238436, "compression_ratio": 1.4739583333333333, "no_speech_prob": 0.00013765388575848192}, {"id": 716, "seek": 480612, "start": 4823.4, "end": 4829.8, "text": " add separate things to the model, and return the sample. So how else could we do that? Well what", "tokens": [51228, 909, 4994, 721, 281, 264, 2316, 11, 293, 2736, 264, 6889, 13, 407, 577, 1646, 727, 321, 360, 300, 30, 1042, 437, 51548], "temperature": 0.0, "avg_logprob": -0.19903351131238436, "compression_ratio": 1.4739583333333333, "no_speech_prob": 0.00013765388575848192}, {"id": 717, "seek": 482980, "start": 4829.8, "end": 4836.92, "text": " we could do is we could actually inherit from unit2d model, which is what Dinesh used directly,", "tokens": [50364, 321, 727, 360, 307, 321, 727, 767, 21389, 490, 4985, 17, 67, 2316, 11, 597, 307, 437, 413, 1652, 71, 1143, 3838, 11, 50720], "temperature": 0.0, "avg_logprob": -0.25726984927528784, "compression_ratio": 1.8731707317073172, "no_speech_prob": 0.0008693289128132164}, {"id": 718, "seek": 482980, "start": 4836.92, "end": 4842.4400000000005, "text": " unit2d model, and we can replace the model. And so we could replace specifically the forward", "tokens": [50720, 4985, 17, 67, 2316, 11, 293, 321, 393, 7406, 264, 2316, 13, 400, 370, 321, 727, 7406, 4682, 264, 2128, 50996], "temperature": 0.0, "avg_logprob": -0.25726984927528784, "compression_ratio": 1.8731707317073172, "no_speech_prob": 0.0008693289128132164}, {"id": 719, "seek": 482980, "start": 4842.4400000000005, "end": 4847.88, "text": " function, that's the thing that gets called, and we could just call the original forward function,", "tokens": [50996, 2445, 11, 300, 311, 264, 551, 300, 2170, 1219, 11, 293, 321, 727, 445, 818, 264, 3380, 2128, 2445, 11, 51268], "temperature": 0.0, "avg_logprob": -0.25726984927528784, "compression_ratio": 1.8731707317073172, "no_speech_prob": 0.0008693289128132164}, {"id": 720, "seek": 482980, "start": 4848.68, "end": 4854.04, "text": " but rather than passing in x, we're passing star x. And rather than returning that, we'll return", "tokens": [51308, 457, 2831, 813, 8437, 294, 2031, 11, 321, 434, 8437, 3543, 2031, 13, 400, 2831, 813, 12678, 300, 11, 321, 603, 2736, 51576], "temperature": 0.0, "avg_logprob": -0.25726984927528784, "compression_ratio": 1.8731707317073172, "no_speech_prob": 0.0008693289128132164}, {"id": 721, "seek": 485404, "start": 4854.04, "end": 4861.32, "text": " that dot sample. Okay so if we do that, then we don't need the train.cb anymore, and we don't need", "tokens": [50364, 300, 5893, 6889, 13, 1033, 370, 498, 321, 360, 300, 11, 550, 321, 500, 380, 643, 264, 3847, 13, 66, 65, 3602, 11, 293, 321, 500, 380, 643, 50728], "temperature": 0.0, "avg_logprob": -0.24420042492094493, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.00030061486177146435}, {"id": 722, "seek": 485404, "start": 4861.32, "end": 4868.2, "text": " the predict. And so if you're not working with something as beautifully flexible as mini-ai,", "tokens": [50728, 264, 6069, 13, 400, 370, 498, 291, 434, 406, 1364, 365, 746, 382, 16525, 11358, 382, 8382, 12, 1301, 11, 51072], "temperature": 0.0, "avg_logprob": -0.24420042492094493, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.00030061486177146435}, {"id": 723, "seek": 485404, "start": 4868.76, "end": 4876.04, "text": " you can always do this, you know, to make, to replace your model so that it has the interface", "tokens": [51100, 291, 393, 1009, 360, 341, 11, 291, 458, 11, 281, 652, 11, 281, 7406, 428, 2316, 370, 300, 309, 575, 264, 9226, 51464], "temperature": 0.0, "avg_logprob": -0.24420042492094493, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.00030061486177146435}, {"id": 724, "seek": 485404, "start": 4876.04, "end": 4882.28, "text": " that you need it to have. So now again we did the same as Dinesh cad of create the callback,", "tokens": [51464, 300, 291, 643, 309, 281, 362, 13, 407, 586, 797, 321, 630, 264, 912, 382, 413, 1652, 71, 12209, 295, 1884, 264, 818, 3207, 11, 51776], "temperature": 0.0, "avg_logprob": -0.24420042492094493, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.00030061486177146435}, {"id": 725, "seek": 488228, "start": 4882.92, "end": 4886.92, "text": " and now when we create the model, we'll use our unit class, which we just created.", "tokens": [50396, 293, 586, 562, 321, 1884, 264, 2316, 11, 321, 603, 764, 527, 4985, 1508, 11, 597, 321, 445, 2942, 13, 50596], "temperature": 0.0, "avg_logprob": -0.2267361879348755, "compression_ratio": 1.6167400881057268, "no_speech_prob": 5.3910494898445904e-05}, {"id": 726, "seek": 488228, "start": 4888.36, "end": 4895.24, "text": " I wanted to see if I can make things faster. I tried dividing all of Dinesh's channels by two,", "tokens": [50668, 286, 1415, 281, 536, 498, 286, 393, 652, 721, 4663, 13, 286, 3031, 26764, 439, 295, 413, 1652, 71, 311, 9235, 538, 732, 11, 51012], "temperature": 0.0, "avg_logprob": -0.2267361879348755, "compression_ratio": 1.6167400881057268, "no_speech_prob": 5.3910494898445904e-05}, {"id": 727, "seek": 488228, "start": 4895.24, "end": 4902.28, "text": " and I found it worked just as well. One thing I noticed is that it uses group norm in the unit,", "tokens": [51012, 293, 286, 1352, 309, 2732, 445, 382, 731, 13, 1485, 551, 286, 5694, 307, 300, 309, 4960, 1594, 2026, 294, 264, 4985, 11, 51364], "temperature": 0.0, "avg_logprob": -0.2267361879348755, "compression_ratio": 1.6167400881057268, "no_speech_prob": 5.3910494898445904e-05}, {"id": 728, "seek": 488228, "start": 4902.28, "end": 4908.28, "text": " which we have briefly learned about before. And in group norm, it splits the channels up into", "tokens": [51364, 597, 321, 362, 10515, 3264, 466, 949, 13, 400, 294, 1594, 2026, 11, 309, 37741, 264, 9235, 493, 666, 51664], "temperature": 0.0, "avg_logprob": -0.2267361879348755, "compression_ratio": 1.6167400881057268, "no_speech_prob": 5.3910494898445904e-05}, {"id": 729, "seek": 490828, "start": 4908.84, "end": 4917.4, "text": " a certain number of groups, and I needed to make sure that those groups had more than one thing in,", "tokens": [50392, 257, 1629, 1230, 295, 3935, 11, 293, 286, 2978, 281, 652, 988, 300, 729, 3935, 632, 544, 813, 472, 551, 294, 11, 50820], "temperature": 0.0, "avg_logprob": -0.20707654490054234, "compression_ratio": 1.641350210970464, "no_speech_prob": 0.0001634639484109357}, {"id": 730, "seek": 490828, "start": 4917.4, "end": 4923.16, "text": " so you can actually pass in how many groups do you want to use in the normalization. So that's", "tokens": [50820, 370, 291, 393, 767, 1320, 294, 577, 867, 3935, 360, 291, 528, 281, 764, 294, 264, 2710, 2144, 13, 407, 300, 311, 51108], "temperature": 0.0, "avg_logprob": -0.20707654490054234, "compression_ratio": 1.641350210970464, "no_speech_prob": 0.0001634639484109357}, {"id": 731, "seek": 490828, "start": 4923.16, "end": 4927.8, "text": " what this is for. You've got to be a little bit careful with these things. I didn't think of it", "tokens": [51108, 437, 341, 307, 337, 13, 509, 600, 658, 281, 312, 257, 707, 857, 5026, 365, 613, 721, 13, 286, 994, 380, 519, 295, 309, 51340], "temperature": 0.0, "avg_logprob": -0.20707654490054234, "compression_ratio": 1.641350210970464, "no_speech_prob": 0.0001634639484109357}, {"id": 732, "seek": 490828, "start": 4927.8, "end": 4934.599999999999, "text": " at first, and I ended up, I think the num groups might have been 32, and I got an error saying you", "tokens": [51340, 412, 700, 11, 293, 286, 4590, 493, 11, 286, 519, 264, 1031, 3935, 1062, 362, 668, 8858, 11, 293, 286, 658, 364, 6713, 1566, 291, 51680], "temperature": 0.0, "avg_logprob": -0.20707654490054234, "compression_ratio": 1.641350210970464, "no_speech_prob": 0.0001634639484109357}, {"id": 733, "seek": 493460, "start": 4934.6, "end": 4940.280000000001, "text": " can't split 16 things into 32 groups. But it also made me realize actually, even in Dinesh,", "tokens": [50364, 393, 380, 7472, 3165, 721, 666, 8858, 3935, 13, 583, 309, 611, 1027, 385, 4325, 767, 11, 754, 294, 413, 1652, 71, 11, 50648], "temperature": 0.0, "avg_logprob": -0.19846132066514757, "compression_ratio": 1.6570397111913358, "no_speech_prob": 0.0010649290634319186}, {"id": 734, "seek": 493460, "start": 4940.280000000001, "end": 4945.0, "text": " maybe you probably had 32 in the first with 32 groups, and so maybe the group norm wouldn't", "tokens": [50648, 1310, 291, 1391, 632, 8858, 294, 264, 700, 365, 8858, 3935, 11, 293, 370, 1310, 264, 1594, 2026, 2759, 380, 50884], "temperature": 0.0, "avg_logprob": -0.19846132066514757, "compression_ratio": 1.6570397111913358, "no_speech_prob": 0.0010649290634319186}, {"id": 735, "seek": 493460, "start": 4945.0, "end": 4952.280000000001, "text": " have been working as well. So they're little subtle things to look out for. So now that we're", "tokens": [50884, 362, 668, 1364, 382, 731, 13, 407, 436, 434, 707, 13743, 721, 281, 574, 484, 337, 13, 407, 586, 300, 321, 434, 51248], "temperature": 0.0, "avg_logprob": -0.19846132066514757, "compression_ratio": 1.6570397111913358, "no_speech_prob": 0.0010649290634319186}, {"id": 736, "seek": 493460, "start": 4952.280000000001, "end": 4957.8, "text": " not using anything inherited from train CB, that means we either need to use train CB itself,", "tokens": [51248, 406, 1228, 1340, 27091, 490, 3847, 18745, 11, 300, 1355, 321, 2139, 643, 281, 764, 3847, 18745, 2564, 11, 51524], "temperature": 0.0, "avg_logprob": -0.19846132066514757, "compression_ratio": 1.6570397111913358, "no_speech_prob": 0.0010649290634319186}, {"id": 737, "seek": 493460, "start": 4957.8, "end": 4962.68, "text": " or just use our train learner, and that everything else is the same as what Dinesh had.", "tokens": [51524, 420, 445, 764, 527, 3847, 33347, 11, 293, 300, 1203, 1646, 307, 264, 912, 382, 437, 413, 1652, 71, 632, 13, 51768], "temperature": 0.0, "avg_logprob": -0.19846132066514757, "compression_ratio": 1.6570397111913358, "no_speech_prob": 0.0010649290634319186}, {"id": 738, "seek": 496268, "start": 4963.64, "end": 4971.400000000001, "text": " So then I wanted to look at the results of noisify here, and we've seen this trick before,", "tokens": [50412, 407, 550, 286, 1415, 281, 574, 412, 264, 3542, 295, 572, 271, 2505, 510, 11, 293, 321, 600, 1612, 341, 4282, 949, 11, 50800], "temperature": 0.0, "avg_logprob": -0.2395470428466797, "compression_ratio": 1.5635359116022098, "no_speech_prob": 0.00026947748847305775}, {"id": 739, "seek": 496268, "start": 4971.400000000001, "end": 4978.12, "text": " which is we call fit, but don't call the training part of the fit, and use the single batch CB", "tokens": [50800, 597, 307, 321, 818, 3318, 11, 457, 500, 380, 818, 264, 3097, 644, 295, 264, 3318, 11, 293, 764, 264, 2167, 15245, 18745, 51136], "temperature": 0.0, "avg_logprob": -0.2395470428466797, "compression_ratio": 1.5635359116022098, "no_speech_prob": 0.00026947748847305775}, {"id": 740, "seek": 496268, "start": 4978.12, "end": 4984.200000000001, "text": " callback that we created way back when we first created learner, and now learn.batch will contain", "tokens": [51136, 818, 3207, 300, 321, 2942, 636, 646, 562, 321, 700, 2942, 33347, 11, 293, 586, 1466, 13, 65, 852, 486, 5304, 51440], "temperature": 0.0, "avg_logprob": -0.2395470428466797, "compression_ratio": 1.5635359116022098, "no_speech_prob": 0.00026947748847305775}, {"id": 741, "seek": 498420, "start": 4985.16, "end": 4993.16, "text": " the tuple of tuples, which we can then use that trick to show. So I mean, obviously we'd expect", "tokens": [50412, 264, 2604, 781, 295, 2604, 2622, 11, 597, 321, 393, 550, 764, 300, 4282, 281, 855, 13, 407, 286, 914, 11, 2745, 321, 1116, 2066, 50812], "temperature": 0.0, "avg_logprob": -0.20279031329684788, "compression_ratio": 1.6452991452991452, "no_speech_prob": 0.057461295276880264}, {"id": 742, "seek": 498420, "start": 4993.16, "end": 4997.8, "text": " it to look the same as before, but it's nice. I always like to draw pictures of everything", "tokens": [50812, 309, 281, 574, 264, 912, 382, 949, 11, 457, 309, 311, 1481, 13, 286, 1009, 411, 281, 2642, 5242, 295, 1203, 51044], "temperature": 0.0, "avg_logprob": -0.20279031329684788, "compression_ratio": 1.6452991452991452, "no_speech_prob": 0.057461295276880264}, {"id": 743, "seek": 498420, "start": 4997.8, "end": 5003.16, "text": " all along the way, because it's very, very often. I mean, the first six to seven times I do anything,", "tokens": [51044, 439, 2051, 264, 636, 11, 570, 309, 311, 588, 11, 588, 2049, 13, 286, 914, 11, 264, 700, 2309, 281, 3407, 1413, 286, 360, 1340, 11, 51312], "temperature": 0.0, "avg_logprob": -0.20279031329684788, "compression_ratio": 1.6452991452991452, "no_speech_prob": 0.057461295276880264}, {"id": 744, "seek": 498420, "start": 5003.16, "end": 5008.2, "text": " I do it wrong. So given that I know that, I might as well draw a picture to try and see how it's", "tokens": [51312, 286, 360, 309, 2085, 13, 407, 2212, 300, 286, 458, 300, 11, 286, 1062, 382, 731, 2642, 257, 3036, 281, 853, 293, 536, 577, 309, 311, 51564], "temperature": 0.0, "avg_logprob": -0.20279031329684788, "compression_ratio": 1.6452991452991452, "no_speech_prob": 0.057461295276880264}, {"id": 745, "seek": 500820, "start": 5008.2, "end": 5016.28, "text": " wrong, until it's fixed. It also tells me when it's not wrong. Isn't there a show batch function", "tokens": [50364, 2085, 11, 1826, 309, 311, 6806, 13, 467, 611, 5112, 385, 562, 309, 311, 406, 2085, 13, 6998, 380, 456, 257, 855, 15245, 2445, 50768], "temperature": 0.0, "avg_logprob": -0.19254872639973958, "compression_ratio": 1.4921465968586387, "no_speech_prob": 0.09533604234457016}, {"id": 746, "seek": 500820, "start": 5016.28, "end": 5024.76, "text": " now that does something similar? Yes, you wrote that show image batch, didn't you? I can't quite", "tokens": [50768, 586, 300, 775, 746, 2531, 30, 1079, 11, 291, 4114, 300, 855, 3256, 15245, 11, 994, 380, 291, 30, 286, 393, 380, 1596, 51192], "temperature": 0.0, "avg_logprob": -0.19254872639973958, "compression_ratio": 1.4921465968586387, "no_speech_prob": 0.09533604234457016}, {"id": 747, "seek": 500820, "start": 5024.76, "end": 5034.92, "text": " remember. Yeah, we should remind ourselves how that worked. That's a good point. Thanks for", "tokens": [51192, 1604, 13, 865, 11, 321, 820, 4160, 4175, 577, 300, 2732, 13, 663, 311, 257, 665, 935, 13, 2561, 337, 51700], "temperature": 0.0, "avg_logprob": -0.19254872639973958, "compression_ratio": 1.4921465968586387, "no_speech_prob": 0.09533604234457016}, {"id": 748, "seek": 503492, "start": 5034.92, "end": 5042.12, "text": " reminding. Okay, so then I'll just go ahead and do the same thing that Dinesh did, but then the", "tokens": [50364, 27639, 13, 1033, 11, 370, 550, 286, 603, 445, 352, 2286, 293, 360, 264, 912, 551, 300, 413, 1652, 71, 630, 11, 457, 550, 264, 50724], "temperature": 0.0, "avg_logprob": -0.24347907806111274, "compression_ratio": 1.6016597510373445, "no_speech_prob": 0.003376467153429985}, {"id": 749, "seek": 503492, "start": 5042.12, "end": 5047.24, "text": " next thing I looked at was, I looked at the, you know, how am I going to make this train faster? I", "tokens": [50724, 958, 551, 286, 2956, 412, 390, 11, 286, 2956, 412, 264, 11, 291, 458, 11, 577, 669, 286, 516, 281, 652, 341, 3847, 4663, 30, 286, 50980], "temperature": 0.0, "avg_logprob": -0.24347907806111274, "compression_ratio": 1.6016597510373445, "no_speech_prob": 0.003376467153429985}, {"id": 750, "seek": 503492, "start": 5047.24, "end": 5056.6, "text": " want a bigger, I want a higher learning rate, and I realized, oddly enough, the diffuser's code does", "tokens": [50980, 528, 257, 3801, 11, 286, 528, 257, 2946, 2539, 3314, 11, 293, 286, 5334, 11, 46083, 1547, 11, 264, 7593, 18088, 311, 3089, 775, 51448], "temperature": 0.0, "avg_logprob": -0.24347907806111274, "compression_ratio": 1.6016597510373445, "no_speech_prob": 0.003376467153429985}, {"id": 751, "seek": 503492, "start": 5056.6, "end": 5062.68, "text": " not initialize anything at all. They use the defaults, which just goes to show, like even,", "tokens": [51448, 406, 5883, 1125, 1340, 412, 439, 13, 814, 764, 264, 7576, 82, 11, 597, 445, 1709, 281, 855, 11, 411, 754, 11, 51752], "temperature": 0.0, "avg_logprob": -0.24347907806111274, "compression_ratio": 1.6016597510373445, "no_speech_prob": 0.003376467153429985}, {"id": 752, "seek": 506268, "start": 5062.68, "end": 5068.68, "text": " you know, the experts at Hugging Face, they don't necessarily think, like, oh, maybe the", "tokens": [50364, 291, 458, 11, 264, 8572, 412, 46892, 3249, 4047, 11, 436, 500, 380, 4725, 519, 11, 411, 11, 1954, 11, 1310, 264, 50664], "temperature": 0.0, "avg_logprob": -0.23026723861694337, "compression_ratio": 1.5240641711229947, "no_speech_prob": 0.00010391000978415832}, {"id": 753, "seek": 506268, "start": 5070.200000000001, "end": 5076.12, "text": " PyTorch defaults aren't, you know, perfect for my model. Of course they're not, because they depend", "tokens": [50740, 9953, 51, 284, 339, 7576, 82, 3212, 380, 11, 291, 458, 11, 2176, 337, 452, 2316, 13, 2720, 1164, 436, 434, 406, 11, 570, 436, 5672, 51036], "temperature": 0.0, "avg_logprob": -0.23026723861694337, "compression_ratio": 1.5240641711229947, "no_speech_prob": 0.00010391000978415832}, {"id": 754, "seek": 506268, "start": 5076.12, "end": 5084.68, "text": " on what activation function do you have, and what res box do you have, and so forth. So I wasn't", "tokens": [51036, 322, 437, 24433, 2445, 360, 291, 362, 11, 293, 437, 725, 2424, 360, 291, 362, 11, 293, 370, 5220, 13, 407, 286, 2067, 380, 51464], "temperature": 0.0, "avg_logprob": -0.23026723861694337, "compression_ratio": 1.5240641711229947, "no_speech_prob": 0.00010391000978415832}, {"id": 755, "seek": 508468, "start": 5084.68, "end": 5092.76, "text": " exactly sure how to initialize it. I, partly by chatting to Kat Crowley, who's the author of", "tokens": [50364, 2293, 988, 577, 281, 5883, 1125, 309, 13, 286, 11, 17031, 538, 24654, 281, 8365, 27072, 3420, 11, 567, 311, 264, 3793, 295, 50768], "temperature": 0.0, "avg_logprob": -0.20254894294361075, "compression_ratio": 1.6, "no_speech_prob": 0.008445296436548233}, {"id": 756, "seek": 508468, "start": 5092.76, "end": 5099.0, "text": " K-diffusion, and partly by looking at papers, and partly by thinking about my own experience, I", "tokens": [50768, 591, 12, 67, 3661, 5704, 11, 293, 17031, 538, 1237, 412, 10577, 11, 293, 17031, 538, 1953, 466, 452, 1065, 1752, 11, 286, 51080], "temperature": 0.0, "avg_logprob": -0.20254894294361075, "compression_ratio": 1.6, "no_speech_prob": 0.008445296436548233}, {"id": 757, "seek": 508468, "start": 5099.0, "end": 5106.280000000001, "text": " ended up doing a few things. One is, I did do the thing that we talked about a while ago, which is", "tokens": [51080, 4590, 493, 884, 257, 1326, 721, 13, 1485, 307, 11, 286, 630, 360, 264, 551, 300, 321, 2825, 466, 257, 1339, 2057, 11, 597, 307, 51444], "temperature": 0.0, "avg_logprob": -0.20254894294361075, "compression_ratio": 1.6, "no_speech_prob": 0.008445296436548233}, {"id": 758, "seek": 508468, "start": 5106.280000000001, "end": 5112.12, "text": " to take every second convolutional layer and zero it out. You could do the same thing with using", "tokens": [51444, 281, 747, 633, 1150, 45216, 304, 4583, 293, 4018, 309, 484, 13, 509, 727, 360, 264, 912, 551, 365, 1228, 51736], "temperature": 0.0, "avg_logprob": -0.20254894294361075, "compression_ratio": 1.6, "no_speech_prob": 0.008445296436548233}, {"id": 759, "seek": 511212, "start": 5112.12, "end": 5115.96, "text": " batch norm, which is what we tried, and since we've got quite a deep network, you know, that", "tokens": [50364, 15245, 2026, 11, 597, 307, 437, 321, 3031, 11, 293, 1670, 321, 600, 658, 1596, 257, 2452, 3209, 11, 291, 458, 11, 300, 50556], "temperature": 0.0, "avg_logprob": -0.2069168501002814, "compression_ratio": 1.5491803278688525, "no_speech_prob": 3.2192241633310914e-05}, {"id": 760, "seek": 511212, "start": 5115.96, "end": 5125.0, "text": " seemed like it might, you know, it helps basically by having the non-ID path in the res nets do", "tokens": [50556, 6576, 411, 309, 1062, 11, 291, 458, 11, 309, 3665, 1936, 538, 1419, 264, 2107, 12, 2777, 3100, 294, 264, 725, 36170, 360, 51008], "temperature": 0.0, "avg_logprob": -0.2069168501002814, "compression_ratio": 1.5491803278688525, "no_speech_prob": 3.2192241633310914e-05}, {"id": 761, "seek": 511212, "start": 5125.0, "end": 5133.32, "text": " nothing at first. So they can't cause problems. We haven't talked about orthogonalized weights", "tokens": [51008, 1825, 412, 700, 13, 407, 436, 393, 380, 3082, 2740, 13, 492, 2378, 380, 2825, 466, 41488, 1602, 17443, 51424], "temperature": 0.0, "avg_logprob": -0.2069168501002814, "compression_ratio": 1.5491803278688525, "no_speech_prob": 3.2192241633310914e-05}, {"id": 762, "seek": 511212, "start": 5133.32, "end": 5140.68, "text": " before, and we probably won't, because you would need to take our computational linear algebra", "tokens": [51424, 949, 11, 293, 321, 1391, 1582, 380, 11, 570, 291, 576, 643, 281, 747, 527, 28270, 8213, 21989, 51792], "temperature": 0.0, "avg_logprob": -0.2069168501002814, "compression_ratio": 1.5491803278688525, "no_speech_prob": 3.2192241633310914e-05}, {"id": 763, "seek": 514068, "start": 5140.68, "end": 5145.320000000001, "text": " course to learn about that, which is a great course. Rachel Thomas did a fantastic job of it.", "tokens": [50364, 1164, 281, 1466, 466, 300, 11, 597, 307, 257, 869, 1164, 13, 14246, 8500, 630, 257, 5456, 1691, 295, 309, 13, 50596], "temperature": 0.0, "avg_logprob": -0.21554630318867793, "compression_ratio": 1.5901639344262295, "no_speech_prob": 6.502786709461361e-05}, {"id": 764, "seek": 514068, "start": 5145.320000000001, "end": 5149.72, "text": " I highly recommend it, but I don't want to make it a prerequisite. But Kat mentioned she thought", "tokens": [50596, 286, 5405, 2748, 309, 11, 457, 286, 500, 380, 528, 281, 652, 309, 257, 38333, 34152, 13, 583, 8365, 2835, 750, 1194, 50816], "temperature": 0.0, "avg_logprob": -0.21554630318867793, "compression_ratio": 1.5901639344262295, "no_speech_prob": 6.502786709461361e-05}, {"id": 765, "seek": 514068, "start": 5149.72, "end": 5160.04, "text": " that using orthogonal weights for the downsamplers was a good idea. Then for the up blocks, they", "tokens": [50816, 300, 1228, 41488, 17443, 337, 264, 760, 19988, 564, 433, 390, 257, 665, 1558, 13, 1396, 337, 264, 493, 8474, 11, 436, 51332], "temperature": 0.0, "avg_logprob": -0.21554630318867793, "compression_ratio": 1.5901639344262295, "no_speech_prob": 6.502786709461361e-05}, {"id": 766, "seek": 514068, "start": 5160.04, "end": 5165.96, "text": " also set the second comms to zero, and something Kat mentioned she found useful, which is also from,", "tokens": [51332, 611, 992, 264, 1150, 800, 82, 281, 4018, 11, 293, 746, 8365, 2835, 750, 1352, 4420, 11, 597, 307, 611, 490, 11, 51628], "temperature": 0.0, "avg_logprob": -0.21554630318867793, "compression_ratio": 1.5901639344262295, "no_speech_prob": 6.502786709461361e-05}, {"id": 767, "seek": 516596, "start": 5166.2, "end": 5173.0, "text": " I think it's from the Darawal Google paper, is to also zero out the weights of basically the", "tokens": [50376, 286, 519, 309, 311, 490, 264, 7803, 1607, 304, 3329, 3035, 11, 307, 281, 611, 4018, 484, 264, 17443, 295, 1936, 264, 50716], "temperature": 0.0, "avg_logprob": -0.2704215194239761, "compression_ratio": 1.6059322033898304, "no_speech_prob": 0.0011878805235028267}, {"id": 768, "seek": 516596, "start": 5173.0, "end": 5180.76, "text": " very last layer. So it's going to start by predicting zero as the noise, which is, you know,", "tokens": [50716, 588, 1036, 4583, 13, 407, 309, 311, 516, 281, 722, 538, 32884, 4018, 382, 264, 5658, 11, 597, 307, 11, 291, 458, 11, 51104], "temperature": 0.0, "avg_logprob": -0.2704215194239761, "compression_ratio": 1.6059322033898304, "no_speech_prob": 0.0011878805235028267}, {"id": 769, "seek": 516596, "start": 5180.76, "end": 5188.92, "text": " something that can't hurt. So that's how I initialized the weights. So call init ddpm on my", "tokens": [51104, 746, 300, 393, 380, 4607, 13, 407, 300, 311, 577, 286, 5883, 1602, 264, 17443, 13, 407, 818, 3157, 274, 67, 14395, 322, 452, 51512], "temperature": 0.0, "avg_logprob": -0.2704215194239761, "compression_ratio": 1.6059322033898304, "no_speech_prob": 0.0011878805235028267}, {"id": 770, "seek": 516596, "start": 5188.92, "end": 5195.08, "text": " model. Something that I found that a huge difference is I replaced the normal atom optimizer with one", "tokens": [51512, 2316, 13, 6595, 300, 286, 1352, 300, 257, 2603, 2649, 307, 286, 10772, 264, 2710, 12018, 5028, 6545, 365, 472, 51820], "temperature": 0.0, "avg_logprob": -0.2704215194239761, "compression_ratio": 1.6059322033898304, "no_speech_prob": 0.0011878805235028267}, {"id": 771, "seek": 519508, "start": 5195.08, "end": 5201.16, "text": " that has an epsilon of 1e neg 5. The default, I think, is 1e neg 8. And so to remind you,", "tokens": [50364, 300, 575, 364, 17889, 295, 502, 68, 2485, 1025, 13, 440, 7576, 11, 286, 519, 11, 307, 502, 68, 2485, 1649, 13, 400, 370, 281, 4160, 291, 11, 50668], "temperature": 0.0, "avg_logprob": -0.21747930844624838, "compression_ratio": 1.6055045871559632, "no_speech_prob": 0.00010229901090497151}, {"id": 772, "seek": 519508, "start": 5202.28, "end": 5209.96, "text": " this is when we divide by the kind of exponentially weighted moving average of", "tokens": [50724, 341, 307, 562, 321, 9845, 538, 264, 733, 295, 37330, 32807, 2684, 4274, 295, 51108], "temperature": 0.0, "avg_logprob": -0.21747930844624838, "compression_ratio": 1.6055045871559632, "no_speech_prob": 0.00010229901090497151}, {"id": 773, "seek": 519508, "start": 5209.96, "end": 5213.72, "text": " the squared gradients. When we divide by that, if that's a very, very small number,", "tokens": [51108, 264, 8889, 2771, 2448, 13, 1133, 321, 9845, 538, 300, 11, 498, 300, 311, 257, 588, 11, 588, 1359, 1230, 11, 51296], "temperature": 0.0, "avg_logprob": -0.21747930844624838, "compression_ratio": 1.6055045871559632, "no_speech_prob": 0.00010229901090497151}, {"id": 774, "seek": 519508, "start": 5216.12, "end": 5223.96, "text": " then it makes the effective learning rate huge. And so we add this to it to make it not too huge.", "tokens": [51416, 550, 309, 1669, 264, 4942, 2539, 3314, 2603, 13, 400, 370, 321, 909, 341, 281, 309, 281, 652, 309, 406, 886, 2603, 13, 51808], "temperature": 0.0, "avg_logprob": -0.21747930844624838, "compression_ratio": 1.6055045871559632, "no_speech_prob": 0.00010229901090497151}, {"id": 775, "seek": 522396, "start": 5223.96, "end": 5227.96, "text": " And it's nearly always a good idea to make this bigger than the default. I don't know why the", "tokens": [50364, 400, 309, 311, 6217, 1009, 257, 665, 1558, 281, 652, 341, 3801, 813, 264, 7576, 13, 286, 500, 380, 458, 983, 264, 50564], "temperature": 0.0, "avg_logprob": -0.1854739385108425, "compression_ratio": 1.5076923076923077, "no_speech_prob": 2.4300175937241875e-05}, {"id": 776, "seek": 522396, "start": 5227.96, "end": 5233.72, "text": " default is so small. And I found until I did this, anytime I tried to use a reasonably large learning", "tokens": [50564, 7576, 307, 370, 1359, 13, 400, 286, 1352, 1826, 286, 630, 341, 11, 13038, 286, 3031, 281, 764, 257, 23551, 2416, 2539, 50852], "temperature": 0.0, "avg_logprob": -0.1854739385108425, "compression_ratio": 1.5076923076923077, "no_speech_prob": 2.4300175937241875e-05}, {"id": 777, "seek": 522396, "start": 5233.72, "end": 5240.68, "text": " rate, somewhere around the middle of the one cycle training, it would explode. So that makes a big", "tokens": [50852, 3314, 11, 4079, 926, 264, 2808, 295, 264, 472, 6586, 3097, 11, 309, 576, 21411, 13, 407, 300, 1669, 257, 955, 51200], "temperature": 0.0, "avg_logprob": -0.1854739385108425, "compression_ratio": 1.5076923076923077, "no_speech_prob": 2.4300175937241875e-05}, {"id": 778, "seek": 524068, "start": 5240.68, "end": 5256.200000000001, "text": " difference. So this way, yeah, I could train, I could get 0.016 after five epochs. And then", "tokens": [50364, 2649, 13, 407, 341, 636, 11, 1338, 11, 286, 727, 3847, 11, 286, 727, 483, 1958, 13, 15, 6866, 934, 1732, 30992, 28346, 13, 400, 550, 51140], "temperature": 0.0, "avg_logprob": -0.21521550730655067, "compression_ratio": 1.4585635359116023, "no_speech_prob": 0.025165896862745285}, {"id": 779, "seek": 524068, "start": 5256.200000000001, "end": 5259.72, "text": " sampling, so it looks all pretty similar. We got some pretty nice textures, I think.", "tokens": [51140, 21179, 11, 370, 309, 1542, 439, 1238, 2531, 13, 492, 658, 512, 1238, 1481, 24501, 11, 286, 519, 13, 51316], "temperature": 0.0, "avg_logprob": -0.21521550730655067, "compression_ratio": 1.4585635359116023, "no_speech_prob": 0.025165896862745285}, {"id": 780, "seek": 524068, "start": 5261.0, "end": 5266.12, "text": " So then I was thinking, how do I get faster? So one way we can make it faster is we can", "tokens": [51380, 407, 550, 286, 390, 1953, 11, 577, 360, 286, 483, 4663, 30, 407, 472, 636, 321, 393, 652, 309, 4663, 307, 321, 393, 51636], "temperature": 0.0, "avg_logprob": -0.21521550730655067, "compression_ratio": 1.4585635359116023, "no_speech_prob": 0.025165896862745285}, {"id": 781, "seek": 526612, "start": 5266.12, "end": 5275.48, "text": " take advantage of something called mixed precision. So currently, we're using 32-bit", "tokens": [50364, 747, 5002, 295, 746, 1219, 7467, 18356, 13, 407, 4362, 11, 321, 434, 1228, 8858, 12, 5260, 50832], "temperature": 0.0, "avg_logprob": -0.2256746700831822, "compression_ratio": 1.6127167630057804, "no_speech_prob": 3.8228925404837355e-05}, {"id": 782, "seek": 526612, "start": 5275.48, "end": 5284.28, "text": " floating point values. That's the defaults, and also known as single precision. And GPUs are", "tokens": [50832, 12607, 935, 4190, 13, 663, 311, 264, 7576, 82, 11, 293, 611, 2570, 382, 2167, 18356, 13, 400, 18407, 82, 366, 51272], "temperature": 0.0, "avg_logprob": -0.2256746700831822, "compression_ratio": 1.6127167630057804, "no_speech_prob": 3.8228925404837355e-05}, {"id": 783, "seek": 526612, "start": 5284.28, "end": 5290.12, "text": " pretty fast at doing 32-bit floating point values, but they're much, much, much, much faster at doing", "tokens": [51272, 1238, 2370, 412, 884, 8858, 12, 5260, 12607, 935, 4190, 11, 457, 436, 434, 709, 11, 709, 11, 709, 11, 709, 4663, 412, 884, 51564], "temperature": 0.0, "avg_logprob": -0.2256746700831822, "compression_ratio": 1.6127167630057804, "no_speech_prob": 3.8228925404837355e-05}, {"id": 784, "seek": 529012, "start": 5290.12, "end": 5298.68, "text": " 16-bit floating point values. So 16-bit floating point values aren't able to represent a very,", "tokens": [50364, 3165, 12, 5260, 12607, 935, 4190, 13, 407, 3165, 12, 5260, 12607, 935, 4190, 3212, 380, 1075, 281, 2906, 257, 588, 11, 50792], "temperature": 0.0, "avg_logprob": -0.18634672422666806, "compression_ratio": 1.5104166666666667, "no_speech_prob": 0.00030061363941058517}, {"id": 785, "seek": 529012, "start": 5298.68, "end": 5303.96, "text": " you know, wide range of numbers, or much precision at the difference between numbers. And so they're", "tokens": [50792, 291, 458, 11, 4874, 3613, 295, 3547, 11, 420, 709, 18356, 412, 264, 2649, 1296, 3547, 13, 400, 370, 436, 434, 51056], "temperature": 0.0, "avg_logprob": -0.18634672422666806, "compression_ratio": 1.5104166666666667, "no_speech_prob": 0.00030061363941058517}, {"id": 786, "seek": 529012, "start": 5303.96, "end": 5311.5599999999995, "text": " quite difficult to use. But if you can, you'll get a huge benefit, because modern GPUs, modern", "tokens": [51056, 1596, 2252, 281, 764, 13, 583, 498, 291, 393, 11, 291, 603, 483, 257, 2603, 5121, 11, 570, 4363, 18407, 82, 11, 4363, 51436], "temperature": 0.0, "avg_logprob": -0.18634672422666806, "compression_ratio": 1.5104166666666667, "no_speech_prob": 0.00030061363941058517}, {"id": 787, "seek": 531156, "start": 5311.56, "end": 5319.64, "text": " NVIDIA GPUs specifically, have special units that do matrix multiplies of 16-bit values", "tokens": [50364, 426, 3958, 6914, 18407, 82, 4682, 11, 362, 2121, 6815, 300, 360, 8141, 12788, 530, 295, 3165, 12, 5260, 4190, 50768], "temperature": 0.0, "avg_logprob": -0.18489908449577563, "compression_ratio": 1.3514851485148516, "no_speech_prob": 0.002472422318533063}, {"id": 788, "seek": 531156, "start": 5319.64, "end": 5327.64, "text": " extremely quickly. You can't just cast everything to 16-bit, because then you, there's not enough", "tokens": [50768, 4664, 2661, 13, 509, 393, 380, 445, 4193, 1203, 281, 3165, 12, 5260, 11, 570, 550, 291, 11, 456, 311, 406, 1547, 51168], "temperature": 0.0, "avg_logprob": -0.18489908449577563, "compression_ratio": 1.3514851485148516, "no_speech_prob": 0.002472422318533063}, {"id": 789, "seek": 531156, "start": 5327.64, "end": 5331.88, "text": " precision to calculate gradients and stuff properly. So we have to use something called", "tokens": [51168, 18356, 281, 8873, 2771, 2448, 293, 1507, 6108, 13, 407, 321, 362, 281, 764, 746, 1219, 51380], "temperature": 0.0, "avg_logprob": -0.18489908449577563, "compression_ratio": 1.3514851485148516, "no_speech_prob": 0.002472422318533063}, {"id": 790, "seek": 533188, "start": 5331.88, "end": 5342.68, "text": " mixed precision. Depending on how enthusiastic I'm feeling, I guess we ought to do this from", "tokens": [50364, 7467, 18356, 13, 22539, 322, 577, 28574, 286, 478, 2633, 11, 286, 2041, 321, 13416, 281, 360, 341, 490, 50904], "temperature": 0.0, "avg_logprob": -0.20605475660683453, "compression_ratio": 1.5161290322580645, "no_speech_prob": 0.006289278622716665}, {"id": 791, "seek": 533188, "start": 5342.68, "end": 5349.64, "text": " scratch as well. We'll see. We do have an implementation from scratch, because we actually", "tokens": [50904, 8459, 382, 731, 13, 492, 603, 536, 13, 492, 360, 362, 364, 11420, 490, 8459, 11, 570, 321, 767, 51252], "temperature": 0.0, "avg_logprob": -0.20605475660683453, "compression_ratio": 1.5161290322580645, "no_speech_prob": 0.006289278622716665}, {"id": 792, "seek": 533188, "start": 5349.64, "end": 5359.72, "text": " implemented this before NVIDIA implemented it in an earlier version of fast.ai. Anyway, we'll see.", "tokens": [51252, 12270, 341, 949, 426, 3958, 6914, 12270, 309, 294, 364, 3071, 3037, 295, 2370, 13, 1301, 13, 5684, 11, 321, 603, 536, 13, 51756], "temperature": 0.0, "avg_logprob": -0.20605475660683453, "compression_ratio": 1.5161290322580645, "no_speech_prob": 0.006289278622716665}, {"id": 793, "seek": 535972, "start": 5359.72, "end": 5365.320000000001, "text": " So basically the idea is that we use 32-bit for things where we need 32-bit, and we use 16-bit", "tokens": [50364, 407, 1936, 264, 1558, 307, 300, 321, 764, 8858, 12, 5260, 337, 721, 689, 321, 643, 8858, 12, 5260, 11, 293, 321, 764, 3165, 12, 5260, 50644], "temperature": 0.0, "avg_logprob": -0.1803755393395057, "compression_ratio": 1.7954545454545454, "no_speech_prob": 1.777849865902681e-05}, {"id": 794, "seek": 535972, "start": 5365.320000000001, "end": 5369.0, "text": " for things where we use 16-bit. So that's what we're going to do, is we're going to use this", "tokens": [50644, 337, 721, 689, 321, 764, 3165, 12, 5260, 13, 407, 300, 311, 437, 321, 434, 516, 281, 360, 11, 307, 321, 434, 516, 281, 764, 341, 50828], "temperature": 0.0, "avg_logprob": -0.1803755393395057, "compression_ratio": 1.7954545454545454, "no_speech_prob": 1.777849865902681e-05}, {"id": 795, "seek": 535972, "start": 5369.0, "end": 5377.0, "text": " mixed precision. But for now we're going to use NVIDIA's, you know, semi-automatic or fairly", "tokens": [50828, 7467, 18356, 13, 583, 337, 586, 321, 434, 516, 281, 764, 426, 3958, 6914, 311, 11, 291, 458, 11, 12909, 12, 1375, 13143, 420, 6457, 51228], "temperature": 0.0, "avg_logprob": -0.1803755393395057, "compression_ratio": 1.7954545454545454, "no_speech_prob": 1.777849865902681e-05}, {"id": 796, "seek": 535972, "start": 5377.0, "end": 5382.6, "text": " automatic code to do that for us. Actually we had a slight change of plan at this point, when we", "tokens": [51228, 12509, 3089, 281, 360, 300, 337, 505, 13, 5135, 321, 632, 257, 4036, 1319, 295, 1393, 412, 341, 935, 11, 562, 321, 51508], "temperature": 0.0, "avg_logprob": -0.1803755393395057, "compression_ratio": 1.7954545454545454, "no_speech_prob": 1.777849865902681e-05}, {"id": 797, "seek": 535972, "start": 5382.6, "end": 5387.64, "text": " realized this lesson was going to be over three hours in length, and we should actually split it", "tokens": [51508, 5334, 341, 6898, 390, 516, 281, 312, 670, 1045, 2496, 294, 4641, 11, 293, 321, 820, 767, 7472, 309, 51760], "temperature": 0.0, "avg_logprob": -0.1803755393395057, "compression_ratio": 1.7954545454545454, "no_speech_prob": 1.777849865902681e-05}, {"id": 798, "seek": 538764, "start": 5387.64, "end": 5395.88, "text": " into two. So we're going to wrap up this lesson here, and we're going to come back and implement", "tokens": [50364, 666, 732, 13, 407, 321, 434, 516, 281, 7019, 493, 341, 6898, 510, 11, 293, 321, 434, 516, 281, 808, 646, 293, 4445, 50776], "temperature": 0.0, "avg_logprob": -0.18709923500238462, "compression_ratio": 1.391304347826087, "no_speech_prob": 0.0004173096676822752}, {"id": 799, "seek": 538764, "start": 5395.88, "end": 5401.96, "text": " this mixed precision thing in lesson 20. So we'll see you then.", "tokens": [50776, 341, 7467, 18356, 551, 294, 6898, 945, 13, 407, 321, 603, 536, 291, 550, 13, 51080], "temperature": 0.0, "avg_logprob": -0.18709923500238462, "compression_ratio": 1.391304347826087, "no_speech_prob": 0.0004173096676822752}], "language": "en"}