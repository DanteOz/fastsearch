{"text": " So what is NLP? NLP is a really broad field. It includes a huge variety of tasks such as part of speech tagging, so identifying if something's a noun or a verb or an adjective, something called named entity recognition, which is identifying people's names, organizations, locations, medical codes, question answering, speech recognition, text to speech, speech to text, and then the last four are the applications we're going to cover in this course. This is a relatively short course, so we can't cover as many as we might like, but topic modeling, sentiment classification, language modeling, and translation. I just wanted to highlight that NLP techniques are useful in a lot of contexts. Even if you're working with tabular data, you may have several of your columns involving textual data, and so this isn't just for purely unstructured text. There are also interesting techniques that go between text and images, and so here looking up the words tensch and net, tensch is a type of fish, you can input text and get back a picture of a tensch and a net, and that's from the devise research as taught in the FASTA-AI class. So I'll be using a top-down teaching approach, which is different than many technical courses, so many technical courses or math courses, my background is in math, which is very bottom-up. If you have to learn each component, you're going to be using before you can put them together to do something interesting, and you kind of start with these fundamental components and are building them, and that's very different from how things like music or sports are taught, which are a bit more experiential. Kids can be playing baseball without knowing all the formal rules, but they can get a sense of the game, and so my goal here in these Jupyter notebooks is to kind of get you into a working application, and then we'll often go into more detail later about what's going on underneath the hood, but that can feel a little bit uncomfortable at first to kind of be using something when you don't initially know all the underlying details. Then NLP is a changing field, which I think makes it really exciting, so this is getting into our kind of a what is NLP overview of the field. Originally, NLP relied on hard-coded rules about a language, and then in the 90s there was a move towards more statistical and machine learning approaches, and we're now in the midst of this major change towards deep learning approaches, and deep learning is now achieving kind of state-of-the-art in a lot of tasks, but you'll see a lot of variation among NLP researchers, among computational linguistics or linguists, and how they approach things, and so I think that's interesting to kind of have this field that's really still changing, and best practices are still being established. One example of this change comes from Peter Norvick, so he talked about spell checkers, and he has a really fun notebook, so I linked to it here, that I encourage you to check out on how to write a spell corrector, but something historically, if you wanted to write a spell checker, and this is an NLP problem of finding spelling mistakes and fixing them, you could have a bunch of rules of checking, you know, if this, else this, of what we know about spelling and how things are spelled, and he shows kind of a more machine learning statistical approach of just learn from a bunch of a bunch of tasks, of okay, that gives you a sense of what words are out there, and then when you find a word that's not in your vocabulary, you can look at how far away it is from a word in your vocabulary, how many edits you would have to make to get there, as well as how common the word is, and so that's important information that you could have learned from having this kind of corpus of text that you started with, of you know, if it's a common word, it's probably more likely, although also if it's, you know, if it's just one letter away from a real word, you know, that's more likely than something, you know, what if it's two edits away from a more common word and kind of making a decision of what do we think that this misspelled word should be. So that's an interesting case study, and something that's really cool is that Peter Norvig shares this example of a spell checker written using this, you know, machine learning approach of having learned from a bunch of corpuses, or sorry, a corpus of text, can be done in 17 lines of code. So it's a much shorter approach, and so this idea of using data, sorry, I should be scrolling down, and feel free to stop me if there are any questions. And so then I'll talk about this in the next lesson, but there's some things you might have heard of, stemming or stop word removal as standard techniques in NLP, which was the case historically, but that's not the case for deep learning and some newer approaches but it's something where I think best practices are still not widely accepted or established, and I think you could hear differences depending on who you're talking to and what their background is. So that's something to keep in mind when reading about NLP or talking with people. So there's an interesting quote debate, Norvig versus Chomsky, that I wanted to talk about just because it captures the tension between two approaches, and you see this in a lot of fields, not just NLP, I think probably anywhere that machine learning is being applied, but the difference between modeling the underlying mechanism of a phenomenon and using machine learning to predict outputs without necessarily understanding the phenomena underneath it. And I meant to add a picture, but when I was in graduate school, I did a PhD in math, but I did some work together with a biologist and we were kind of modeling how metabolism works within a cell. And so this is a cellular process involving kind of all these different substrates and enzymes, and we were using the first approach of modeling the underlying mechanism. And so we had information of what rate the enzyme converts at, and we're kind of building this huge system of differential equations to represent how are different things converting and to see if we could capture the overall behavior that you would see in the human body with dealing with different nutrients. And it was something that it was kind of a revelation to me, this is before I ever learned about machine learning a very long time ago, to realize like, oh, there's this totally different approach, because there was something that was a little bit dissatisfying to me about we ended up with a very complex system, and you're kind of fiddling with it to get the outputs to align with what you would expect, versus whereas machine learning really starts with we have all the data, and we're modeling the outputs, but we don't necessarily know about every chemical reaction, but we do know that we're going to be able to know about every chemical reaction going on within. Anyway, so this tension is very much alive in NLP. So Noam Chomsky, who is incredibly famous in the field of linguistics and even beyond, is kind of considered the father of modern linguistics. And when I say modern linguistics, that means kind of latter half of the 20th century, spoke at a panel at MIT, and was critical of purely statistical methods that don't understand, try to understand the meaning of the behavior. And so Chomsky compared such researchers to scientists who might study the dance made by a bee returning to the hive, and who could produce a statistically based simulation of such a dance without attempting to understand the way why the bee behaved that way. And then Peter Norvick, and so Peter Norvick is head of research at Google, wrote this response on Chomsky and the two cultures of statistical learning, kind of in favor of using statistics and using data. And I think this is, actually I should just click on the link, can be interesting to read through. So I'll leave this for you if you want to kind of read more about this debate. And again, this applies beyond NLP, but to anywhere that machine learning and more kind of data driven statistical approaches are being applied. And these, I'd also note that it's not always a binary, like you're completely doing one or you're completely doing the other. There are definitely ways to draw from both. And another debate or quote debate was Jan Lacoon versus Chris Manning, had a debate at Stanford last year and Jan Lacoon, you know, is one of the kind of fathers of deep learning and this, you know, deep learning renaissance that we're having now. And Chris Manning is a very famous computational linguist, teaches the NLP course at Stanford. And they talk about this, kind of how much linguistic structure you want to incorporate into deep learning systems. I liked the line, Manning described structure as a necessary good, whereas Lacoon described structure as a necessary evil. So this is just kind of some background to get situated kind of in the field of NLP. I won't be using a textbook in the course, but I have links to, and the first two are free online, available as PDFs if you're interested in a reference, and then also some of my favorite NLP related blogs. And in particular, Sebastian Ritter has some great posts. Just get a drink of water. So I wanted to talk about some of the tools that you'll see in NLP. One is RegEx, and we'll be covering RegEx, but RegEx lets you solve problems such as find all phone numbers. And you'll notice that phone numbers can have different formats. Here I have one just written in English, and then I have one in English, and then I have one in English, and then I have one in English, and numbers can have different formats. Here I have one just written with dashes. This one's written with parentheses, but there are these common themes among phone numbers, and we'll get into this, but RegEx is a way to pick out what are all the phone numbers, even though they have slightly different formats. Tokenization, how do we split text into meaningful units, which is necessary for processing. Word embeddings, which I believe you've covered twice in the program so far, depending on which boot camp you took, but they're a useful tool. Linear algebra and matrix decomposition, neural nets, hidden Markov models, which we will not be covering. Parse trees will also not cover. I wanted to show a parse tree just so you know, because they are, or can be a big thing in linguistics, and there are plenty of people working on them, but that's kind of coming up with the diagram of a sentence. Actually, I'm curious, who here had to diagram sentences in school when they were younger? Okay, so still a few people. We did this when I was in school. I feel like it might be going out of fashion. I don't know, but that's coming up with, so there is this, yeah, you could have the sentence, she killed the man with the tie. Is it that the man was wearing a tie, who she killed, or was the tie the murder weapon that she used to kill the man? And it's ambiguous from the sentence, but the sentence structure of the parse tree would show you. And basically it comes down to the prepositional phrase with the tie. Is that part of this noun phrase, the man, or is that part of the verb phrase, killed? And so, and here just broadly, S stands for sentence, NP is a noun phrase, VP is a verb phrase. And we won't really be covering this, but I wanted to show you that this existed. I minored in linguistics in college, and I took an entire semester on syntax that was kind of all about drawing these rules to document the rules to diagram sentences. So some of the popular Python libraries for NLP, there's NLTK, which was first released in 2001. It's a very broad NLP library. I think it's less state of the art, but it does include kind of a lot of different things and has been around for a while. Spacey, which is great for creating parse trees. It has a great tokenizer. It's an opinionated library, which I think is a good thing. It kind of will give you one way to do something and to do it very well, as opposed to NLTK is more kind of this broad umbrella of here are a ton of ways to do things. Gensim, which I have not used much, but that's for topic modeling and similarity detection. There are specialized tools such as PyText or FastText, and then also general machine learning libraries have some NLP functionality such as IKITlearn and FastAI, both of which we'll be using in here. And then I just want to share a few NLP applications to close out kind of this intro to what is the field of NLP before we get into the first kind of programming notebook of looking at a specific application. Some things you can do with NLP. This was a project how QUID uses deep learning with small data. So QUID is a startup that has a database of company descriptions, and they wanted to identify which company descriptions were low quality, meaning that they were kind of generic and used a lot of marketing language. And so they did this with deep learning. Just to give some examples, actually, I guess this is still a little bit small, saying something like, launches quickly with configuration in as little as a week doesn't really give you any information about the product, whereas spatial scanning software for mobile devices, that's giving you specific information. So that was a good description, the first one was a bad one. So that's, and that's also an example of a nice blog post let me open it up. Yeah, kind of going through going through this problem set, and I'm always interested in things on little data as well. Another application comes from a law student in Singapore who classified legal documents by category. So knowing whether a document related to civil law, criminal law, contract, family, tort. And this is this is an application that or this problem of classification we will be covering later, although not this application specifically. And I thought this was neat, this is a journalism grad student analyze the Twitter sentiment of different different people running for political office. And this is a this is a neat article introducing metadata enhanced ULM fit. So they were classifying quotes from articles and they used metadata such as the publication, the country, and the source together with the text. And so this is an example of pairing what we would often think of as more structured data with unstructured data to improve your accuracy. And that's something to keep in mind and kind of relates to you don't have to think of NLP necessarily as separate from tabular data, but you can use them together. And finally, and we'll come back to this kind of in the last week or two in more detail, I want to talk about some of the ethical issues in NLP. So bias is a huge issue. And I spoke about this on the Friday seminar in the fall. There's bias and word embedding, and I think that's a really important issue. And so this is word embeddings. And so word embeddings are a useful tool, but they also produce biased results because they're learning from historical data, which is biased. And so Google Translate, if you enter she is a doctor, he is a nurse, translated that into Turkish, which has a gender neutral singular pronoun, and then back into English, it comes back he is a doctor, she is a nurse. So the genders have been flipped to fit the stereotype. And you can see this with several things, babysitters end up being she. And so there's a good article in the MIT Tech Review a few years ago on that. There have been some proposals of ways to try to de-bias word embeddings, but there was a paper earlier this year that I liked suggesting that de-biasing word embeddings is just kind of covering them up, but doesn't remove them, and you still have the underlying structure. If you want a lot more detail, I gave a kind of a two hour workshop on this several years ago that comes with the Jupyter Notebook and really goes through kind of letting you see how certain words are closer to each other versus not. Language models have been getting more attention recently, and this is where you have an algorithm that's generating text, specifically a neural network. And so they looked at OpenAI's GPT-2, which has been a kind of state of the art language model that came out recently. And if you prompt it, my wife just got an exciting new job, it suggests that she's going to be a housekeeper and full-time mom. If you say my husband got an exciting new job, he's going to be a doctor and a financial consultant simultaneously. And so that's just issues to know about, and we'll talk about this more later. And then in addition to bias, there's the concern around fakery. So now that language models are getting better and better at producing kind of convincing text, it will be, so to quote Jeremy Howard, who's here, said, we have the technology to totally fill Twitter email and the web up with reasonable sounding, context appropriate prose, which could drown out all other speech and be impossible to filter. So that's kind of something scary and something to be thinking about as you take kind of take your first jobs in the field, the ethical implications and kind of preparing for how we want to address this. There's a list of just to make this a little bit more concrete of some things that could go wrong with having kind of more convincing generated language. So you think of combining this with deep fakes, videos, you know, having a world leader declaring war or saying something against another country. This could be very bad politically. It doesn't have to be perfect, just good enough to make the enemy think something has happened. You can also think of the issue politically where legislators are flooded with fraudulent and attention. And this actually, I guess, happens in a smaller scale. There was a great blog post. I'll update this to include the link of, I think, there was a way for people to respond around that neutrality and a data scientist went through the comments and discovered that a ton of them seem to be created by bots. But you can think if humans are going to have to start competing with fake campaigns or bots and even go to the with fake campaigns or bots and even getting legislation, legislator attention around issues. And then also spam is going to become much more compelling. Like right now, most of us probably think, oh, it's easy to spot spam. It looks pretty fake. But if people can imitate the way your loved one talks in a much more appropriate way, that's kind of scary. And so, and then I also wanted to link to this was a good article by Oren Ecziani, who's the head of the Allen Institute on AI. And he was saying that we basically need, AI is poised to make high fidelity forgery inexpensive and automated. We need to start kind of getting the norm out there that any item that isn't signed is potentially forged, but that would kind of require a huge, huge cultural shift. So these are just some issues to kind of have on the back burner as we get into our study of NLP and then we will talk about them more later in the course. Cool. Any, any questions so far? This is just a very broad, broad intro.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 8.64, "text": " So what is NLP? NLP is a really broad field. It includes a huge variety of tasks such as", "tokens": [407, 437, 307, 426, 45196, 30, 426, 45196, 307, 257, 534, 4152, 2519, 13, 467, 5974, 257, 2603, 5673, 295, 9608, 1270, 382], "temperature": 0.0, "avg_logprob": -0.22156721970130658, "compression_ratio": 1.654708520179372, "no_speech_prob": 0.021268514916300774}, {"id": 1, "seek": 0, "start": 8.64, "end": 15.56, "text": " part of speech tagging, so identifying if something's a noun or a verb or an adjective,", "tokens": [644, 295, 6218, 6162, 3249, 11, 370, 16696, 498, 746, 311, 257, 23307, 420, 257, 9595, 420, 364, 44129, 11], "temperature": 0.0, "avg_logprob": -0.22156721970130658, "compression_ratio": 1.654708520179372, "no_speech_prob": 0.021268514916300774}, {"id": 2, "seek": 0, "start": 15.56, "end": 22.16, "text": " something called named entity recognition, which is identifying people's names, organizations,", "tokens": [746, 1219, 4926, 13977, 11150, 11, 597, 307, 16696, 561, 311, 5288, 11, 6150, 11], "temperature": 0.0, "avg_logprob": -0.22156721970130658, "compression_ratio": 1.654708520179372, "no_speech_prob": 0.021268514916300774}, {"id": 3, "seek": 0, "start": 22.16, "end": 29.28, "text": " locations, medical codes, question answering, speech recognition, text to speech, speech to text,", "tokens": [9253, 11, 4625, 14211, 11, 1168, 13430, 11, 6218, 11150, 11, 2487, 281, 6218, 11, 6218, 281, 2487, 11], "temperature": 0.0, "avg_logprob": -0.22156721970130658, "compression_ratio": 1.654708520179372, "no_speech_prob": 0.021268514916300774}, {"id": 4, "seek": 2928, "start": 29.28, "end": 34.84, "text": " and then the last four are the applications we're going to cover in this course. This is a relatively", "tokens": [293, 550, 264, 1036, 1451, 366, 264, 5821, 321, 434, 516, 281, 2060, 294, 341, 1164, 13, 639, 307, 257, 7226], "temperature": 0.0, "avg_logprob": -0.13960433536105685, "compression_ratio": 1.5546558704453441, "no_speech_prob": 1.450861782359425e-05}, {"id": 5, "seek": 2928, "start": 34.84, "end": 40.84, "text": " short course, so we can't cover as many as we might like, but topic modeling, sentiment", "tokens": [2099, 1164, 11, 370, 321, 393, 380, 2060, 382, 867, 382, 321, 1062, 411, 11, 457, 4829, 15983, 11, 16149], "temperature": 0.0, "avg_logprob": -0.13960433536105685, "compression_ratio": 1.5546558704453441, "no_speech_prob": 1.450861782359425e-05}, {"id": 6, "seek": 2928, "start": 40.84, "end": 49.120000000000005, "text": " classification, language modeling, and translation. I just wanted to highlight that NLP techniques", "tokens": [21538, 11, 2856, 15983, 11, 293, 12853, 13, 286, 445, 1415, 281, 5078, 300, 426, 45196, 7512], "temperature": 0.0, "avg_logprob": -0.13960433536105685, "compression_ratio": 1.5546558704453441, "no_speech_prob": 1.450861782359425e-05}, {"id": 7, "seek": 2928, "start": 49.120000000000005, "end": 55.64, "text": " are useful in a lot of contexts. Even if you're working with tabular data, you may have several", "tokens": [366, 4420, 294, 257, 688, 295, 30628, 13, 2754, 498, 291, 434, 1364, 365, 4421, 1040, 1412, 11, 291, 815, 362, 2940], "temperature": 0.0, "avg_logprob": -0.13960433536105685, "compression_ratio": 1.5546558704453441, "no_speech_prob": 1.450861782359425e-05}, {"id": 8, "seek": 5564, "start": 55.64, "end": 62.28, "text": " of your columns involving textual data, and so this isn't just for purely unstructured text.", "tokens": [295, 428, 13766, 17030, 2487, 901, 1412, 11, 293, 370, 341, 1943, 380, 445, 337, 17491, 18799, 46847, 2487, 13], "temperature": 0.0, "avg_logprob": -0.20233551336794484, "compression_ratio": 1.6103896103896105, "no_speech_prob": 3.7628331483574584e-05}, {"id": 9, "seek": 5564, "start": 62.28, "end": 70.04, "text": " There are also interesting techniques that go between text and images, and so here looking up", "tokens": [821, 366, 611, 1880, 7512, 300, 352, 1296, 2487, 293, 5267, 11, 293, 370, 510, 1237, 493], "temperature": 0.0, "avg_logprob": -0.20233551336794484, "compression_ratio": 1.6103896103896105, "no_speech_prob": 3.7628331483574584e-05}, {"id": 10, "seek": 5564, "start": 70.04, "end": 77.0, "text": " the words tensch and net, tensch is a type of fish, you can input text and get back a picture", "tokens": [264, 2283, 256, 26590, 293, 2533, 11, 256, 26590, 307, 257, 2010, 295, 3506, 11, 291, 393, 4846, 2487, 293, 483, 646, 257, 3036], "temperature": 0.0, "avg_logprob": -0.20233551336794484, "compression_ratio": 1.6103896103896105, "no_speech_prob": 3.7628331483574584e-05}, {"id": 11, "seek": 5564, "start": 77.0, "end": 82.96000000000001, "text": " of a tensch and a net, and that's from the devise research as taught in the FASTA-AI class.", "tokens": [295, 257, 256, 26590, 293, 257, 2533, 11, 293, 300, 311, 490, 264, 1905, 908, 2132, 382, 5928, 294, 264, 479, 3160, 8241, 12, 48698, 1508, 13], "temperature": 0.0, "avg_logprob": -0.20233551336794484, "compression_ratio": 1.6103896103896105, "no_speech_prob": 3.7628331483574584e-05}, {"id": 12, "seek": 8296, "start": 82.96, "end": 90.55999999999999, "text": " So I'll be using a top-down teaching approach, which is different than many technical courses,", "tokens": [407, 286, 603, 312, 1228, 257, 1192, 12, 5093, 4571, 3109, 11, 597, 307, 819, 813, 867, 6191, 7712, 11], "temperature": 0.0, "avg_logprob": -0.13769315599321244, "compression_ratio": 1.7720588235294117, "no_speech_prob": 7.600479875691235e-05}, {"id": 13, "seek": 8296, "start": 90.55999999999999, "end": 96.0, "text": " so many technical courses or math courses, my background is in math, which is very bottom-up.", "tokens": [370, 867, 6191, 7712, 420, 5221, 7712, 11, 452, 3678, 307, 294, 5221, 11, 597, 307, 588, 2767, 12, 1010, 13], "temperature": 0.0, "avg_logprob": -0.13769315599321244, "compression_ratio": 1.7720588235294117, "no_speech_prob": 7.600479875691235e-05}, {"id": 14, "seek": 8296, "start": 96.0, "end": 101.86, "text": " If you have to learn each component, you're going to be using before you can put them together to do", "tokens": [759, 291, 362, 281, 1466, 1184, 6542, 11, 291, 434, 516, 281, 312, 1228, 949, 291, 393, 829, 552, 1214, 281, 360], "temperature": 0.0, "avg_logprob": -0.13769315599321244, "compression_ratio": 1.7720588235294117, "no_speech_prob": 7.600479875691235e-05}, {"id": 15, "seek": 8296, "start": 101.86, "end": 106.47999999999999, "text": " something interesting, and you kind of start with these fundamental components and are building", "tokens": [746, 1880, 11, 293, 291, 733, 295, 722, 365, 613, 8088, 6677, 293, 366, 2390], "temperature": 0.0, "avg_logprob": -0.13769315599321244, "compression_ratio": 1.7720588235294117, "no_speech_prob": 7.600479875691235e-05}, {"id": 16, "seek": 8296, "start": 106.47999999999999, "end": 111.75999999999999, "text": " them, and that's very different from how things like music or sports are taught, which are a bit", "tokens": [552, 11, 293, 300, 311, 588, 819, 490, 577, 721, 411, 1318, 420, 6573, 366, 5928, 11, 597, 366, 257, 857], "temperature": 0.0, "avg_logprob": -0.13769315599321244, "compression_ratio": 1.7720588235294117, "no_speech_prob": 7.600479875691235e-05}, {"id": 17, "seek": 11176, "start": 111.76, "end": 118.72, "text": " more experiential. Kids can be playing baseball without knowing all the formal rules, but they", "tokens": [544, 49611, 831, 13, 15694, 393, 312, 2433, 14323, 1553, 5276, 439, 264, 9860, 4474, 11, 457, 436], "temperature": 0.0, "avg_logprob": -0.060557318770367165, "compression_ratio": 1.6340425531914893, "no_speech_prob": 2.212277649960015e-05}, {"id": 18, "seek": 11176, "start": 118.72, "end": 123.84, "text": " can get a sense of the game, and so my goal here in these Jupyter notebooks is to kind of get you", "tokens": [393, 483, 257, 2020, 295, 264, 1216, 11, 293, 370, 452, 3387, 510, 294, 613, 22125, 88, 391, 43782, 307, 281, 733, 295, 483, 291], "temperature": 0.0, "avg_logprob": -0.060557318770367165, "compression_ratio": 1.6340425531914893, "no_speech_prob": 2.212277649960015e-05}, {"id": 19, "seek": 11176, "start": 123.84, "end": 130.0, "text": " into a working application, and then we'll often go into more detail later about what's going on", "tokens": [666, 257, 1364, 3861, 11, 293, 550, 321, 603, 2049, 352, 666, 544, 2607, 1780, 466, 437, 311, 516, 322], "temperature": 0.0, "avg_logprob": -0.060557318770367165, "compression_ratio": 1.6340425531914893, "no_speech_prob": 2.212277649960015e-05}, {"id": 20, "seek": 11176, "start": 130.0, "end": 134.72, "text": " underneath the hood, but that can feel a little bit uncomfortable at first to kind of be using", "tokens": [7223, 264, 13376, 11, 457, 300, 393, 841, 257, 707, 857, 10532, 412, 700, 281, 733, 295, 312, 1228], "temperature": 0.0, "avg_logprob": -0.060557318770367165, "compression_ratio": 1.6340425531914893, "no_speech_prob": 2.212277649960015e-05}, {"id": 21, "seek": 13472, "start": 134.72, "end": 146.96, "text": " something when you don't initially know all the underlying details. Then NLP is a changing field,", "tokens": [746, 562, 291, 500, 380, 9105, 458, 439, 264, 14217, 4365, 13, 1396, 426, 45196, 307, 257, 4473, 2519, 11], "temperature": 0.0, "avg_logprob": -0.09339219645449989, "compression_ratio": 1.4702970297029703, "no_speech_prob": 7.888856089266483e-06}, {"id": 22, "seek": 13472, "start": 146.96, "end": 153.6, "text": " which I think makes it really exciting, so this is getting into our kind of a what is NLP overview", "tokens": [597, 286, 519, 1669, 309, 534, 4670, 11, 370, 341, 307, 1242, 666, 527, 733, 295, 257, 437, 307, 426, 45196, 12492], "temperature": 0.0, "avg_logprob": -0.09339219645449989, "compression_ratio": 1.4702970297029703, "no_speech_prob": 7.888856089266483e-06}, {"id": 23, "seek": 13472, "start": 153.6, "end": 161.28, "text": " of the field. Originally, NLP relied on hard-coded rules about a language, and then in the 90s there", "tokens": [295, 264, 2519, 13, 28696, 11, 426, 45196, 35463, 322, 1152, 12, 66, 12340, 4474, 466, 257, 2856, 11, 293, 550, 294, 264, 4289, 82, 456], "temperature": 0.0, "avg_logprob": -0.09339219645449989, "compression_ratio": 1.4702970297029703, "no_speech_prob": 7.888856089266483e-06}, {"id": 24, "seek": 16128, "start": 161.28, "end": 167.36, "text": " was a move towards more statistical and machine learning approaches, and we're now in the midst", "tokens": [390, 257, 1286, 3030, 544, 22820, 293, 3479, 2539, 11587, 11, 293, 321, 434, 586, 294, 264, 18629], "temperature": 0.0, "avg_logprob": -0.0679114731875333, "compression_ratio": 1.8521400778210118, "no_speech_prob": 3.0234039513743483e-05}, {"id": 25, "seek": 16128, "start": 167.36, "end": 175.12, "text": " of this major change towards deep learning approaches, and deep learning is now achieving", "tokens": [295, 341, 2563, 1319, 3030, 2452, 2539, 11587, 11, 293, 2452, 2539, 307, 586, 19626], "temperature": 0.0, "avg_logprob": -0.0679114731875333, "compression_ratio": 1.8521400778210118, "no_speech_prob": 3.0234039513743483e-05}, {"id": 26, "seek": 16128, "start": 175.12, "end": 180.0, "text": " kind of state-of-the-art in a lot of tasks, but you'll see a lot of variation among NLP researchers,", "tokens": [733, 295, 1785, 12, 2670, 12, 3322, 12, 446, 294, 257, 688, 295, 9608, 11, 457, 291, 603, 536, 257, 688, 295, 12990, 3654, 426, 45196, 10309, 11], "temperature": 0.0, "avg_logprob": -0.0679114731875333, "compression_ratio": 1.8521400778210118, "no_speech_prob": 3.0234039513743483e-05}, {"id": 27, "seek": 16128, "start": 180.0, "end": 186.24, "text": " among computational linguistics or linguists, and how they approach things, and so I think that's", "tokens": [3654, 28270, 21766, 6006, 420, 21766, 1751, 11, 293, 577, 436, 3109, 721, 11, 293, 370, 286, 519, 300, 311], "temperature": 0.0, "avg_logprob": -0.0679114731875333, "compression_ratio": 1.8521400778210118, "no_speech_prob": 3.0234039513743483e-05}, {"id": 28, "seek": 16128, "start": 186.24, "end": 191.04, "text": " interesting to kind of have this field that's really still changing, and best practices are", "tokens": [1880, 281, 733, 295, 362, 341, 2519, 300, 311, 534, 920, 4473, 11, 293, 1151, 7525, 366], "temperature": 0.0, "avg_logprob": -0.0679114731875333, "compression_ratio": 1.8521400778210118, "no_speech_prob": 3.0234039513743483e-05}, {"id": 29, "seek": 19104, "start": 191.04, "end": 200.39999999999998, "text": " still being established. One example of this change comes from Peter Norvick, so he talked about", "tokens": [920, 885, 7545, 13, 1485, 1365, 295, 341, 1319, 1487, 490, 6508, 6966, 85, 618, 11, 370, 415, 2825, 466], "temperature": 0.0, "avg_logprob": -0.14133607773553758, "compression_ratio": 1.4397590361445782, "no_speech_prob": 1.0952742741210386e-05}, {"id": 30, "seek": 19104, "start": 200.39999999999998, "end": 207.44, "text": " spell checkers, and he has a really fun notebook, so I linked to it here, that I encourage you to check out", "tokens": [9827, 1520, 433, 11, 293, 415, 575, 257, 534, 1019, 21060, 11, 370, 286, 9408, 281, 309, 510, 11, 300, 286, 5373, 291, 281, 1520, 484], "temperature": 0.0, "avg_logprob": -0.14133607773553758, "compression_ratio": 1.4397590361445782, "no_speech_prob": 1.0952742741210386e-05}, {"id": 31, "seek": 19104, "start": 211.6, "end": 213.76, "text": " on how to write a spell corrector,", "tokens": [322, 577, 281, 2464, 257, 9827, 29731, 1672, 11], "temperature": 0.0, "avg_logprob": -0.14133607773553758, "compression_ratio": 1.4397590361445782, "no_speech_prob": 1.0952742741210386e-05}, {"id": 32, "seek": 21376, "start": 213.76, "end": 219.51999999999998, "text": " but something historically, if you wanted to write a spell checker, and this is an NLP problem of", "tokens": [457, 746, 16180, 11, 498, 291, 1415, 281, 2464, 257, 9827, 1520, 260, 11, 293, 341, 307, 364, 426, 45196, 1154, 295], "temperature": 0.0, "avg_logprob": -0.2023183350921959, "compression_ratio": 1.7399103139013452, "no_speech_prob": 2.8853261028416455e-05}, {"id": 33, "seek": 21376, "start": 221.12, "end": 226.39999999999998, "text": " finding spelling mistakes and fixing them, you could have a bunch of rules of checking, you know,", "tokens": [5006, 22254, 8038, 293, 19442, 552, 11, 291, 727, 362, 257, 3840, 295, 4474, 295, 8568, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.2023183350921959, "compression_ratio": 1.7399103139013452, "no_speech_prob": 2.8853261028416455e-05}, {"id": 34, "seek": 21376, "start": 226.39999999999998, "end": 233.68, "text": " if this, else this, of what we know about spelling and how things are spelled, and he shows kind of a", "tokens": [498, 341, 11, 1646, 341, 11, 295, 437, 321, 458, 466, 22254, 293, 577, 721, 366, 34388, 11, 293, 415, 3110, 733, 295, 257], "temperature": 0.0, "avg_logprob": -0.2023183350921959, "compression_ratio": 1.7399103139013452, "no_speech_prob": 2.8853261028416455e-05}, {"id": 35, "seek": 21376, "start": 233.68, "end": 240.88, "text": " more machine learning statistical approach of just learn from a bunch of a bunch of tasks,", "tokens": [544, 3479, 2539, 22820, 3109, 295, 445, 1466, 490, 257, 3840, 295, 257, 3840, 295, 9608, 11], "temperature": 0.0, "avg_logprob": -0.2023183350921959, "compression_ratio": 1.7399103139013452, "no_speech_prob": 2.8853261028416455e-05}, {"id": 36, "seek": 24088, "start": 240.88, "end": 245.84, "text": " of okay, that gives you a sense of what words are out there, and then when you find a word that's not", "tokens": [295, 1392, 11, 300, 2709, 291, 257, 2020, 295, 437, 2283, 366, 484, 456, 11, 293, 550, 562, 291, 915, 257, 1349, 300, 311, 406], "temperature": 0.0, "avg_logprob": -0.11546611785888672, "compression_ratio": 1.8494208494208495, "no_speech_prob": 1.6963589587248862e-05}, {"id": 37, "seek": 24088, "start": 245.84, "end": 251.28, "text": " in your vocabulary, you can look at how far away it is from a word in your vocabulary, how many", "tokens": [294, 428, 19864, 11, 291, 393, 574, 412, 577, 1400, 1314, 309, 307, 490, 257, 1349, 294, 428, 19864, 11, 577, 867], "temperature": 0.0, "avg_logprob": -0.11546611785888672, "compression_ratio": 1.8494208494208495, "no_speech_prob": 1.6963589587248862e-05}, {"id": 38, "seek": 24088, "start": 251.28, "end": 259.84, "text": " edits you would have to make to get there, as well as how common the word is, and so that's", "tokens": [41752, 291, 576, 362, 281, 652, 281, 483, 456, 11, 382, 731, 382, 577, 2689, 264, 1349, 307, 11, 293, 370, 300, 311], "temperature": 0.0, "avg_logprob": -0.11546611785888672, "compression_ratio": 1.8494208494208495, "no_speech_prob": 1.6963589587248862e-05}, {"id": 39, "seek": 24088, "start": 259.84, "end": 263.52, "text": " important information that you could have learned from having this kind of corpus of text that you", "tokens": [1021, 1589, 300, 291, 727, 362, 3264, 490, 1419, 341, 733, 295, 1181, 31624, 295, 2487, 300, 291], "temperature": 0.0, "avg_logprob": -0.11546611785888672, "compression_ratio": 1.8494208494208495, "no_speech_prob": 1.6963589587248862e-05}, {"id": 40, "seek": 24088, "start": 263.52, "end": 268.0, "text": " started with, of you know, if it's a common word, it's probably more likely, although also", "tokens": [1409, 365, 11, 295, 291, 458, 11, 498, 309, 311, 257, 2689, 1349, 11, 309, 311, 1391, 544, 3700, 11, 4878, 611], "temperature": 0.0, "avg_logprob": -0.11546611785888672, "compression_ratio": 1.8494208494208495, "no_speech_prob": 1.6963589587248862e-05}, {"id": 41, "seek": 26800, "start": 268.0, "end": 274.4, "text": " if it's, you know, if it's just one letter away from a real word, you know, that's more likely", "tokens": [498, 309, 311, 11, 291, 458, 11, 498, 309, 311, 445, 472, 5063, 1314, 490, 257, 957, 1349, 11, 291, 458, 11, 300, 311, 544, 3700], "temperature": 0.0, "avg_logprob": -0.11616806983947754, "compression_ratio": 1.8352490421455938, "no_speech_prob": 1.1124970114906318e-05}, {"id": 42, "seek": 26800, "start": 274.4, "end": 278.8, "text": " than something, you know, what if it's two edits away from a more common word and kind of making a", "tokens": [813, 746, 11, 291, 458, 11, 437, 498, 309, 311, 732, 41752, 1314, 490, 257, 544, 2689, 1349, 293, 733, 295, 1455, 257], "temperature": 0.0, "avg_logprob": -0.11616806983947754, "compression_ratio": 1.8352490421455938, "no_speech_prob": 1.1124970114906318e-05}, {"id": 43, "seek": 26800, "start": 278.8, "end": 283.68, "text": " decision of what do we think that this misspelled word should be. So that's an interesting case", "tokens": [3537, 295, 437, 360, 321, 519, 300, 341, 1713, 33000, 1349, 820, 312, 13, 407, 300, 311, 364, 1880, 1389], "temperature": 0.0, "avg_logprob": -0.11616806983947754, "compression_ratio": 1.8352490421455938, "no_speech_prob": 1.1124970114906318e-05}, {"id": 44, "seek": 26800, "start": 283.68, "end": 289.12, "text": " study, and something that's really cool is that Peter Norvig shares this example of a spell checker", "tokens": [2979, 11, 293, 746, 300, 311, 534, 1627, 307, 300, 6508, 6966, 85, 328, 12182, 341, 1365, 295, 257, 9827, 1520, 260], "temperature": 0.0, "avg_logprob": -0.11616806983947754, "compression_ratio": 1.8352490421455938, "no_speech_prob": 1.1124970114906318e-05}, {"id": 45, "seek": 26800, "start": 289.12, "end": 294.0, "text": " written using this, you know, machine learning approach of having learned from a bunch of", "tokens": [3720, 1228, 341, 11, 291, 458, 11, 3479, 2539, 3109, 295, 1419, 3264, 490, 257, 3840, 295], "temperature": 0.0, "avg_logprob": -0.11616806983947754, "compression_ratio": 1.8352490421455938, "no_speech_prob": 1.1124970114906318e-05}, {"id": 46, "seek": 29400, "start": 294.0, "end": 303.92, "text": " corpuses, or sorry, a corpus of text, can be done in 17 lines of code. So it's a much shorter", "tokens": [1181, 79, 8355, 11, 420, 2597, 11, 257, 1181, 31624, 295, 2487, 11, 393, 312, 1096, 294, 3282, 3876, 295, 3089, 13, 407, 309, 311, 257, 709, 11639], "temperature": 0.0, "avg_logprob": -0.2819164569561298, "compression_ratio": 1.4113924050632911, "no_speech_prob": 1.983252150239423e-05}, {"id": 47, "seek": 29400, "start": 303.92, "end": 314.16, "text": " approach, and so this idea of using data, sorry, I should be scrolling down, and feel free to stop", "tokens": [3109, 11, 293, 370, 341, 1558, 295, 1228, 1412, 11, 2597, 11, 286, 820, 312, 29053, 760, 11, 293, 841, 1737, 281, 1590], "temperature": 0.0, "avg_logprob": -0.2819164569561298, "compression_ratio": 1.4113924050632911, "no_speech_prob": 1.983252150239423e-05}, {"id": 48, "seek": 31416, "start": 314.16, "end": 327.04, "text": " me if there are any questions. And so then I'll talk about this in the next lesson, but there's", "tokens": [385, 498, 456, 366, 604, 1651, 13, 400, 370, 550, 286, 603, 751, 466, 341, 294, 264, 958, 6898, 11, 457, 456, 311], "temperature": 0.0, "avg_logprob": -0.17465598146680375, "compression_ratio": 1.4848484848484849, "no_speech_prob": 2.627240974106826e-05}, {"id": 49, "seek": 31416, "start": 327.04, "end": 333.52000000000004, "text": " some things you might have heard of, stemming or stop word removal as standard techniques in NLP,", "tokens": [512, 721, 291, 1062, 362, 2198, 295, 11, 12312, 2810, 420, 1590, 1349, 17933, 382, 3832, 7512, 294, 426, 45196, 11], "temperature": 0.0, "avg_logprob": -0.17465598146680375, "compression_ratio": 1.4848484848484849, "no_speech_prob": 2.627240974106826e-05}, {"id": 50, "seek": 31416, "start": 333.52000000000004, "end": 340.40000000000003, "text": " which was the case historically, but that's not the case for deep learning and some newer approaches", "tokens": [597, 390, 264, 1389, 16180, 11, 457, 300, 311, 406, 264, 1389, 337, 2452, 2539, 293, 512, 17628, 11587], "temperature": 0.0, "avg_logprob": -0.17465598146680375, "compression_ratio": 1.4848484848484849, "no_speech_prob": 2.627240974106826e-05}, {"id": 51, "seek": 34040, "start": 340.4, "end": 346.23999999999995, "text": " but it's something where I think best practices are still not widely accepted or established,", "tokens": [457, 309, 311, 746, 689, 286, 519, 1151, 7525, 366, 920, 406, 13371, 9035, 420, 7545, 11], "temperature": 0.0, "avg_logprob": -0.15399370410225607, "compression_ratio": 1.5732217573221758, "no_speech_prob": 1.0781907803902868e-05}, {"id": 52, "seek": 34040, "start": 346.23999999999995, "end": 350.4, "text": " and I think you could hear differences depending on who you're talking to and what their background", "tokens": [293, 286, 519, 291, 727, 1568, 7300, 5413, 322, 567, 291, 434, 1417, 281, 293, 437, 641, 3678], "temperature": 0.0, "avg_logprob": -0.15399370410225607, "compression_ratio": 1.5732217573221758, "no_speech_prob": 1.0781907803902868e-05}, {"id": 53, "seek": 34040, "start": 350.4, "end": 356.23999999999995, "text": " is. So that's something to keep in mind when reading about NLP or talking with people.", "tokens": [307, 13, 407, 300, 311, 746, 281, 1066, 294, 1575, 562, 3760, 466, 426, 45196, 420, 1417, 365, 561, 13], "temperature": 0.0, "avg_logprob": -0.15399370410225607, "compression_ratio": 1.5732217573221758, "no_speech_prob": 1.0781907803902868e-05}, {"id": 54, "seek": 34040, "start": 357.76, "end": 366.0, "text": " So there's an interesting quote debate, Norvig versus Chomsky, that I wanted to talk about just", "tokens": [407, 456, 311, 364, 1880, 6513, 7958, 11, 6966, 85, 328, 5717, 761, 4785, 4133, 11, 300, 286, 1415, 281, 751, 466, 445], "temperature": 0.0, "avg_logprob": -0.15399370410225607, "compression_ratio": 1.5732217573221758, "no_speech_prob": 1.0781907803902868e-05}, {"id": 55, "seek": 36600, "start": 366.0, "end": 372.96, "text": " because it captures the tension between two approaches, and you see this in a lot of fields,", "tokens": [570, 309, 27986, 264, 8980, 1296, 732, 11587, 11, 293, 291, 536, 341, 294, 257, 688, 295, 7909, 11], "temperature": 0.0, "avg_logprob": -0.23692279075508688, "compression_ratio": 1.6009615384615385, "no_speech_prob": 4.425088263815269e-06}, {"id": 56, "seek": 36600, "start": 372.96, "end": 380.0, "text": " not just NLP, I think probably anywhere that machine learning is being applied, but the", "tokens": [406, 445, 426, 45196, 11, 286, 519, 1391, 4992, 300, 3479, 2539, 307, 885, 6456, 11, 457, 264], "temperature": 0.0, "avg_logprob": -0.23692279075508688, "compression_ratio": 1.6009615384615385, "no_speech_prob": 4.425088263815269e-06}, {"id": 57, "seek": 36600, "start": 381.2, "end": 385.12, "text": " difference between modeling the underlying mechanism of a phenomenon", "tokens": [2649, 1296, 15983, 264, 14217, 7513, 295, 257, 14029], "temperature": 0.0, "avg_logprob": -0.23692279075508688, "compression_ratio": 1.6009615384615385, "no_speech_prob": 4.425088263815269e-06}, {"id": 58, "seek": 36600, "start": 386.32, "end": 392.48, "text": " and using machine learning to predict outputs without necessarily understanding the", "tokens": [293, 1228, 3479, 2539, 281, 6069, 23930, 1553, 4725, 3701, 264], "temperature": 0.0, "avg_logprob": -0.23692279075508688, "compression_ratio": 1.6009615384615385, "no_speech_prob": 4.425088263815269e-06}, {"id": 59, "seek": 39248, "start": 392.48, "end": 396.64000000000004, "text": " phenomena underneath it. And I meant to add a picture, but when I was in graduate school,", "tokens": [22004, 7223, 309, 13, 400, 286, 4140, 281, 909, 257, 3036, 11, 457, 562, 286, 390, 294, 8080, 1395, 11], "temperature": 0.0, "avg_logprob": -0.1690991326664271, "compression_ratio": 1.6422413793103448, "no_speech_prob": 7.4098652476095594e-06}, {"id": 60, "seek": 39248, "start": 397.52000000000004, "end": 405.36, "text": " I did a PhD in math, but I did some work together with a biologist and we were kind of modeling how", "tokens": [286, 630, 257, 14476, 294, 5221, 11, 457, 286, 630, 512, 589, 1214, 365, 257, 3228, 9201, 293, 321, 645, 733, 295, 15983, 577], "temperature": 0.0, "avg_logprob": -0.1690991326664271, "compression_ratio": 1.6422413793103448, "no_speech_prob": 7.4098652476095594e-06}, {"id": 61, "seek": 39248, "start": 405.36, "end": 410.48, "text": " metabolism works within a cell. And so this is a cellular process involving kind of all these", "tokens": [31190, 1985, 1951, 257, 2815, 13, 400, 370, 341, 307, 257, 29267, 1399, 17030, 733, 295, 439, 613], "temperature": 0.0, "avg_logprob": -0.1690991326664271, "compression_ratio": 1.6422413793103448, "no_speech_prob": 7.4098652476095594e-06}, {"id": 62, "seek": 39248, "start": 410.48, "end": 416.40000000000003, "text": " different substrates and enzymes, and we were using the first approach of modeling the underlying", "tokens": [819, 4594, 12507, 293, 29299, 11, 293, 321, 645, 1228, 264, 700, 3109, 295, 15983, 264, 14217], "temperature": 0.0, "avg_logprob": -0.1690991326664271, "compression_ratio": 1.6422413793103448, "no_speech_prob": 7.4098652476095594e-06}, {"id": 63, "seek": 41640, "start": 416.4, "end": 422.32, "text": " mechanism. And so we had information of what rate the enzyme converts at, and we're kind of", "tokens": [7513, 13, 400, 370, 321, 632, 1589, 295, 437, 3314, 264, 24521, 38874, 412, 11, 293, 321, 434, 733, 295], "temperature": 0.0, "avg_logprob": -0.16238843477689302, "compression_ratio": 1.6805555555555556, "no_speech_prob": 4.263988012098707e-05}, {"id": 64, "seek": 41640, "start": 422.32, "end": 426.96, "text": " building this huge system of differential equations to represent how are different things", "tokens": [2390, 341, 2603, 1185, 295, 15756, 11787, 281, 2906, 577, 366, 819, 721], "temperature": 0.0, "avg_logprob": -0.16238843477689302, "compression_ratio": 1.6805555555555556, "no_speech_prob": 4.263988012098707e-05}, {"id": 65, "seek": 41640, "start": 426.96, "end": 434.23999999999995, "text": " converting and to see if we could capture the overall behavior that you would see in the human", "tokens": [29942, 293, 281, 536, 498, 321, 727, 7983, 264, 4787, 5223, 300, 291, 576, 536, 294, 264, 1952], "temperature": 0.0, "avg_logprob": -0.16238843477689302, "compression_ratio": 1.6805555555555556, "no_speech_prob": 4.263988012098707e-05}, {"id": 66, "seek": 41640, "start": 434.23999999999995, "end": 442.0, "text": " body with dealing with different nutrients. And it was something that it was kind of a", "tokens": [1772, 365, 6260, 365, 819, 17617, 13, 400, 309, 390, 746, 300, 309, 390, 733, 295, 257], "temperature": 0.0, "avg_logprob": -0.16238843477689302, "compression_ratio": 1.6805555555555556, "no_speech_prob": 4.263988012098707e-05}, {"id": 67, "seek": 44200, "start": 442.0, "end": 446.32, "text": " revelation to me, this is before I ever learned about machine learning a very long time ago, to", "tokens": [23456, 281, 385, 11, 341, 307, 949, 286, 1562, 3264, 466, 3479, 2539, 257, 588, 938, 565, 2057, 11, 281], "temperature": 0.0, "avg_logprob": -0.26414235432942706, "compression_ratio": 1.8164556962025316, "no_speech_prob": 2.3184580641100183e-05}, {"id": 68, "seek": 44200, "start": 446.32, "end": 449.92, "text": " realize like, oh, there's this totally different approach, because there was something that was a", "tokens": [4325, 411, 11, 1954, 11, 456, 311, 341, 3879, 819, 3109, 11, 570, 456, 390, 746, 300, 390, 257], "temperature": 0.0, "avg_logprob": -0.26414235432942706, "compression_ratio": 1.8164556962025316, "no_speech_prob": 2.3184580641100183e-05}, {"id": 69, "seek": 44200, "start": 449.92, "end": 453.84, "text": " little bit dissatisfying to me about we ended up with a very complex system, and you're kind of", "tokens": [707, 857, 7802, 25239, 1840, 281, 385, 466, 321, 4590, 493, 365, 257, 588, 3997, 1185, 11, 293, 291, 434, 733, 295], "temperature": 0.0, "avg_logprob": -0.26414235432942706, "compression_ratio": 1.8164556962025316, "no_speech_prob": 2.3184580641100183e-05}, {"id": 70, "seek": 44200, "start": 453.84, "end": 460.56, "text": " fiddling with it to get the outputs to align with what you would expect, versus whereas machine", "tokens": [283, 14273, 1688, 365, 309, 281, 483, 264, 23930, 281, 7975, 365, 437, 291, 576, 2066, 11, 5717, 9735, 3479], "temperature": 0.0, "avg_logprob": -0.26414235432942706, "compression_ratio": 1.8164556962025316, "no_speech_prob": 2.3184580641100183e-05}, {"id": 71, "seek": 44200, "start": 460.56, "end": 464.56, "text": " learning really starts with we have all the data, and we're modeling the outputs, but we don't", "tokens": [2539, 534, 3719, 365, 321, 362, 439, 264, 1412, 11, 293, 321, 434, 15983, 264, 23930, 11, 457, 321, 500, 380], "temperature": 0.0, "avg_logprob": -0.26414235432942706, "compression_ratio": 1.8164556962025316, "no_speech_prob": 2.3184580641100183e-05}, {"id": 72, "seek": 44200, "start": 464.56, "end": 469.92, "text": " necessarily know about every chemical reaction, but we do know that we're going to be able to", "tokens": [4725, 458, 466, 633, 7313, 5480, 11, 457, 321, 360, 458, 300, 321, 434, 516, 281, 312, 1075, 281], "temperature": 0.0, "avg_logprob": -0.26414235432942706, "compression_ratio": 1.8164556962025316, "no_speech_prob": 2.3184580641100183e-05}, {"id": 73, "seek": 46992, "start": 469.92, "end": 479.04, "text": " know about every chemical reaction going on within. Anyway, so this tension is very much alive in NLP.", "tokens": [458, 466, 633, 7313, 5480, 516, 322, 1951, 13, 5684, 11, 370, 341, 8980, 307, 588, 709, 5465, 294, 426, 45196, 13], "temperature": 0.0, "avg_logprob": -0.0745668819972447, "compression_ratio": 1.5135135135135136, "no_speech_prob": 9.817644240683876e-06}, {"id": 74, "seek": 46992, "start": 479.76, "end": 486.88, "text": " So Noam Chomsky, who is incredibly famous in the field of linguistics and even beyond,", "tokens": [407, 883, 335, 761, 4785, 4133, 11, 567, 307, 6252, 4618, 294, 264, 2519, 295, 21766, 6006, 293, 754, 4399, 11], "temperature": 0.0, "avg_logprob": -0.0745668819972447, "compression_ratio": 1.5135135135135136, "no_speech_prob": 9.817644240683876e-06}, {"id": 75, "seek": 46992, "start": 486.88, "end": 491.6, "text": " is kind of considered the father of modern linguistics. And when I say modern linguistics,", "tokens": [307, 733, 295, 4888, 264, 3086, 295, 4363, 21766, 6006, 13, 400, 562, 286, 584, 4363, 21766, 6006, 11], "temperature": 0.0, "avg_logprob": -0.0745668819972447, "compression_ratio": 1.5135135135135136, "no_speech_prob": 9.817644240683876e-06}, {"id": 76, "seek": 49160, "start": 491.6, "end": 501.6, "text": " that means kind of latter half of the 20th century, spoke at a panel at MIT, and was", "tokens": [300, 1355, 733, 295, 18481, 1922, 295, 264, 945, 392, 4901, 11, 7179, 412, 257, 4831, 412, 13100, 11, 293, 390], "temperature": 0.0, "avg_logprob": -0.09721171494686243, "compression_ratio": 1.4564102564102563, "no_speech_prob": 5.337586571840802e-06}, {"id": 77, "seek": 49160, "start": 502.32000000000005, "end": 508.64000000000004, "text": " critical of purely statistical methods that don't understand, try to understand the meaning of the", "tokens": [4924, 295, 17491, 22820, 7150, 300, 500, 380, 1223, 11, 853, 281, 1223, 264, 3620, 295, 264], "temperature": 0.0, "avg_logprob": -0.09721171494686243, "compression_ratio": 1.4564102564102563, "no_speech_prob": 5.337586571840802e-06}, {"id": 78, "seek": 49160, "start": 508.64000000000004, "end": 518.96, "text": " behavior. And so Chomsky compared such researchers to scientists who might study the dance made by a", "tokens": [5223, 13, 400, 370, 761, 4785, 4133, 5347, 1270, 10309, 281, 7708, 567, 1062, 2979, 264, 4489, 1027, 538, 257], "temperature": 0.0, "avg_logprob": -0.09721171494686243, "compression_ratio": 1.4564102564102563, "no_speech_prob": 5.337586571840802e-06}, {"id": 79, "seek": 51896, "start": 518.96, "end": 524.24, "text": " bee returning to the hive, and who could produce a statistically based simulation of such a dance", "tokens": [17479, 12678, 281, 264, 42523, 11, 293, 567, 727, 5258, 257, 36478, 2361, 16575, 295, 1270, 257, 4489], "temperature": 0.0, "avg_logprob": -0.09139859335763113, "compression_ratio": 1.5179487179487179, "no_speech_prob": 2.3919168597785756e-05}, {"id": 80, "seek": 51896, "start": 524.24, "end": 531.2800000000001, "text": " without attempting to understand the way why the bee behaved that way. And then Peter Norvick, and", "tokens": [1553, 22001, 281, 1223, 264, 636, 983, 264, 17479, 48249, 300, 636, 13, 400, 550, 6508, 6966, 85, 618, 11, 293], "temperature": 0.0, "avg_logprob": -0.09139859335763113, "compression_ratio": 1.5179487179487179, "no_speech_prob": 2.3919168597785756e-05}, {"id": 81, "seek": 51896, "start": 531.2800000000001, "end": 540.32, "text": " so Peter Norvick is head of research at Google, wrote this response on Chomsky and the two cultures", "tokens": [370, 6508, 6966, 85, 618, 307, 1378, 295, 2132, 412, 3329, 11, 4114, 341, 4134, 322, 761, 4785, 4133, 293, 264, 732, 12951], "temperature": 0.0, "avg_logprob": -0.09139859335763113, "compression_ratio": 1.5179487179487179, "no_speech_prob": 2.3919168597785756e-05}, {"id": 82, "seek": 54032, "start": 540.32, "end": 551.7600000000001, "text": " of statistical learning, kind of in favor of using statistics and using data. And I think this is,", "tokens": [295, 22820, 2539, 11, 733, 295, 294, 2294, 295, 1228, 12523, 293, 1228, 1412, 13, 400, 286, 519, 341, 307, 11], "temperature": 0.0, "avg_logprob": -0.10456222958034939, "compression_ratio": 1.6523605150214593, "no_speech_prob": 3.071265382459387e-05}, {"id": 83, "seek": 54032, "start": 551.7600000000001, "end": 557.6800000000001, "text": " actually I should just click on the link, can be interesting to read through. So I'll leave this", "tokens": [767, 286, 820, 445, 2052, 322, 264, 2113, 11, 393, 312, 1880, 281, 1401, 807, 13, 407, 286, 603, 1856, 341], "temperature": 0.0, "avg_logprob": -0.10456222958034939, "compression_ratio": 1.6523605150214593, "no_speech_prob": 3.071265382459387e-05}, {"id": 84, "seek": 54032, "start": 557.6800000000001, "end": 563.84, "text": " for you if you want to kind of read more about this debate. And again, this applies beyond NLP,", "tokens": [337, 291, 498, 291, 528, 281, 733, 295, 1401, 544, 466, 341, 7958, 13, 400, 797, 11, 341, 13165, 4399, 426, 45196, 11], "temperature": 0.0, "avg_logprob": -0.10456222958034939, "compression_ratio": 1.6523605150214593, "no_speech_prob": 3.071265382459387e-05}, {"id": 85, "seek": 54032, "start": 563.84, "end": 569.5200000000001, "text": " but to anywhere that machine learning and more kind of data driven statistical approaches are", "tokens": [457, 281, 4992, 300, 3479, 2539, 293, 544, 733, 295, 1412, 9555, 22820, 11587, 366], "temperature": 0.0, "avg_logprob": -0.10456222958034939, "compression_ratio": 1.6523605150214593, "no_speech_prob": 3.071265382459387e-05}, {"id": 86, "seek": 56952, "start": 569.52, "end": 575.6, "text": " being applied. And these, I'd also note that it's not always a binary, like you're completely doing", "tokens": [885, 6456, 13, 400, 613, 11, 286, 1116, 611, 3637, 300, 309, 311, 406, 1009, 257, 17434, 11, 411, 291, 434, 2584, 884], "temperature": 0.0, "avg_logprob": -0.15851656595865884, "compression_ratio": 1.5635359116022098, "no_speech_prob": 3.168942566844635e-05}, {"id": 87, "seek": 56952, "start": 575.6, "end": 580.3199999999999, "text": " one or you're completely doing the other. There are definitely ways to draw from both.", "tokens": [472, 420, 291, 434, 2584, 884, 264, 661, 13, 821, 366, 2138, 2098, 281, 2642, 490, 1293, 13], "temperature": 0.0, "avg_logprob": -0.15851656595865884, "compression_ratio": 1.5635359116022098, "no_speech_prob": 3.168942566844635e-05}, {"id": 88, "seek": 56952, "start": 585.84, "end": 594.0, "text": " And another debate or quote debate was Jan Lacoon versus Chris Manning, had a debate at Stanford", "tokens": [400, 1071, 7958, 420, 6513, 7958, 390, 4956, 40113, 4106, 5717, 6688, 2458, 773, 11, 632, 257, 7958, 412, 20374], "temperature": 0.0, "avg_logprob": -0.15851656595865884, "compression_ratio": 1.5635359116022098, "no_speech_prob": 3.168942566844635e-05}, {"id": 89, "seek": 59400, "start": 594.0, "end": 602.88, "text": " last year and Jan Lacoon, you know, is one of the kind of fathers of deep learning and this, you know,", "tokens": [1036, 1064, 293, 4956, 40113, 4106, 11, 291, 458, 11, 307, 472, 295, 264, 733, 295, 23450, 295, 2452, 2539, 293, 341, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.11114820179186369, "compression_ratio": 1.6652360515021458, "no_speech_prob": 1.0450577974552289e-05}, {"id": 90, "seek": 59400, "start": 602.88, "end": 609.12, "text": " deep learning renaissance that we're having now. And Chris Manning is a very famous computational", "tokens": [2452, 2539, 319, 629, 14431, 300, 321, 434, 1419, 586, 13, 400, 6688, 2458, 773, 307, 257, 588, 4618, 28270], "temperature": 0.0, "avg_logprob": -0.11114820179186369, "compression_ratio": 1.6652360515021458, "no_speech_prob": 1.0450577974552289e-05}, {"id": 91, "seek": 59400, "start": 609.12, "end": 615.6, "text": " linguist, teaches the NLP course at Stanford. And they talk about this, kind of how much linguistic", "tokens": [21766, 468, 11, 16876, 264, 426, 45196, 1164, 412, 20374, 13, 400, 436, 751, 466, 341, 11, 733, 295, 577, 709, 43002], "temperature": 0.0, "avg_logprob": -0.11114820179186369, "compression_ratio": 1.6652360515021458, "no_speech_prob": 1.0450577974552289e-05}, {"id": 92, "seek": 59400, "start": 615.6, "end": 621.52, "text": " structure you want to incorporate into deep learning systems. I liked the line, Manning", "tokens": [3877, 291, 528, 281, 16091, 666, 2452, 2539, 3652, 13, 286, 4501, 264, 1622, 11, 2458, 773], "temperature": 0.0, "avg_logprob": -0.11114820179186369, "compression_ratio": 1.6652360515021458, "no_speech_prob": 1.0450577974552289e-05}, {"id": 93, "seek": 62152, "start": 621.52, "end": 627.84, "text": " described structure as a necessary good, whereas Lacoon described structure as a necessary evil.", "tokens": [7619, 3877, 382, 257, 4818, 665, 11, 9735, 40113, 4106, 7619, 3877, 382, 257, 4818, 6724, 13], "temperature": 0.0, "avg_logprob": -0.10702503749302456, "compression_ratio": 1.5730337078651686, "no_speech_prob": 2.355112519580871e-05}, {"id": 94, "seek": 62152, "start": 629.52, "end": 634.88, "text": " So this is just kind of some background to get situated kind of in the field of NLP.", "tokens": [407, 341, 307, 445, 733, 295, 512, 3678, 281, 483, 30143, 733, 295, 294, 264, 2519, 295, 426, 45196, 13], "temperature": 0.0, "avg_logprob": -0.10702503749302456, "compression_ratio": 1.5730337078651686, "no_speech_prob": 2.355112519580871e-05}, {"id": 95, "seek": 62152, "start": 637.1999999999999, "end": 645.12, "text": " I won't be using a textbook in the course, but I have links to, and the first two are free online,", "tokens": [286, 1582, 380, 312, 1228, 257, 25591, 294, 264, 1164, 11, 457, 286, 362, 6123, 281, 11, 293, 264, 700, 732, 366, 1737, 2950, 11], "temperature": 0.0, "avg_logprob": -0.10702503749302456, "compression_ratio": 1.5730337078651686, "no_speech_prob": 2.355112519580871e-05}, {"id": 96, "seek": 64512, "start": 645.12, "end": 650.48, "text": " available as PDFs if you're interested in a reference, and then also some of my favorite", "tokens": [2435, 382, 17752, 82, 498, 291, 434, 3102, 294, 257, 6408, 11, 293, 550, 611, 512, 295, 452, 2954], "temperature": 0.0, "avg_logprob": -0.3545028462129481, "compression_ratio": 1.28, "no_speech_prob": 2.0784065782208927e-05}, {"id": 97, "seek": 64512, "start": 650.48, "end": 656.32, "text": " NLP related blogs. And in particular, Sebastian Ritter has some great posts.", "tokens": [426, 45196, 4077, 31038, 13, 400, 294, 1729, 11, 31102, 497, 3904, 575, 512, 869, 12300, 13], "temperature": 0.0, "avg_logprob": -0.3545028462129481, "compression_ratio": 1.28, "no_speech_prob": 2.0784065782208927e-05}, {"id": 98, "seek": 64512, "start": 659.04, "end": 661.04, "text": " Just get a drink of water.", "tokens": [1449, 483, 257, 2822, 295, 1281, 13], "temperature": 0.0, "avg_logprob": -0.3545028462129481, "compression_ratio": 1.28, "no_speech_prob": 2.0784065782208927e-05}, {"id": 99, "seek": 66104, "start": 661.04, "end": 668.7199999999999, "text": " So I wanted to talk about some of the tools that you'll see in NLP. One is RegEx, and we'll be", "tokens": [407, 286, 1415, 281, 751, 466, 512, 295, 264, 3873, 300, 291, 603, 536, 294, 426, 45196, 13, 1485, 307, 4791, 11149, 11, 293, 321, 603, 312], "temperature": 0.0, "avg_logprob": -0.39221644637608294, "compression_ratio": 1.8817733990147782, "no_speech_prob": 2.212399704148993e-05}, {"id": 100, "seek": 66104, "start": 668.7199999999999, "end": 676.0, "text": " covering RegEx, but RegEx lets you solve problems such as find all phone numbers. And you'll notice", "tokens": [10322, 4791, 11149, 11, 457, 4791, 11149, 6653, 291, 5039, 2740, 1270, 382, 915, 439, 2593, 3547, 13, 400, 291, 603, 3449], "temperature": 0.0, "avg_logprob": -0.39221644637608294, "compression_ratio": 1.8817733990147782, "no_speech_prob": 2.212399704148993e-05}, {"id": 101, "seek": 66104, "start": 676.0, "end": 681.12, "text": " that phone numbers can have different formats. Here I have one just written in English, and", "tokens": [300, 2593, 3547, 393, 362, 819, 25879, 13, 1692, 286, 362, 472, 445, 3720, 294, 3669, 11, 293], "temperature": 0.0, "avg_logprob": -0.39221644637608294, "compression_ratio": 1.8817733990147782, "no_speech_prob": 2.212399704148993e-05}, {"id": 102, "seek": 66104, "start": 681.12, "end": 687.4399999999999, "text": " then I have one in English, and then I have one in English, and then I have one in English, and", "tokens": [550, 286, 362, 472, 294, 3669, 11, 293, 550, 286, 362, 472, 294, 3669, 11, 293, 550, 286, 362, 472, 294, 3669, 11, 293], "temperature": 0.0, "avg_logprob": -0.39221644637608294, "compression_ratio": 1.8817733990147782, "no_speech_prob": 2.212399704148993e-05}, {"id": 103, "seek": 68744, "start": 687.44, "end": 693.12, "text": " numbers can have different formats. Here I have one just written with dashes. This one's written", "tokens": [3547, 393, 362, 819, 25879, 13, 1692, 286, 362, 472, 445, 3720, 365, 8240, 279, 13, 639, 472, 311, 3720], "temperature": 0.0, "avg_logprob": -0.07233203112424075, "compression_ratio": 1.6869565217391305, "no_speech_prob": 0.0002034059289144352}, {"id": 104, "seek": 68744, "start": 693.12, "end": 698.08, "text": " with parentheses, but there are these common themes among phone numbers, and we'll get into", "tokens": [365, 34153, 11, 457, 456, 366, 613, 2689, 13544, 3654, 2593, 3547, 11, 293, 321, 603, 483, 666], "temperature": 0.0, "avg_logprob": -0.07233203112424075, "compression_ratio": 1.6869565217391305, "no_speech_prob": 0.0002034059289144352}, {"id": 105, "seek": 68744, "start": 698.08, "end": 703.7600000000001, "text": " this, but RegEx is a way to pick out what are all the phone numbers, even though they have slightly", "tokens": [341, 11, 457, 4791, 11149, 307, 257, 636, 281, 1888, 484, 437, 366, 439, 264, 2593, 3547, 11, 754, 1673, 436, 362, 4748], "temperature": 0.0, "avg_logprob": -0.07233203112424075, "compression_ratio": 1.6869565217391305, "no_speech_prob": 0.0002034059289144352}, {"id": 106, "seek": 68744, "start": 703.7600000000001, "end": 710.96, "text": " different formats. Tokenization, how do we split text into meaningful units, which is necessary for", "tokens": [819, 25879, 13, 314, 8406, 2144, 11, 577, 360, 321, 7472, 2487, 666, 10995, 6815, 11, 597, 307, 4818, 337], "temperature": 0.0, "avg_logprob": -0.07233203112424075, "compression_ratio": 1.6869565217391305, "no_speech_prob": 0.0002034059289144352}, {"id": 107, "seek": 71096, "start": 710.96, "end": 718.64, "text": " processing. Word embeddings, which I believe you've covered twice in the program so far,", "tokens": [9007, 13, 8725, 12240, 29432, 11, 597, 286, 1697, 291, 600, 5343, 6091, 294, 264, 1461, 370, 1400, 11], "temperature": 0.0, "avg_logprob": -0.11757652885035465, "compression_ratio": 1.5666666666666667, "no_speech_prob": 6.604756345041096e-05}, {"id": 108, "seek": 71096, "start": 718.64, "end": 725.12, "text": " depending on which boot camp you took, but they're a useful tool. Linear algebra and matrix", "tokens": [5413, 322, 597, 11450, 2255, 291, 1890, 11, 457, 436, 434, 257, 4420, 2290, 13, 14670, 289, 21989, 293, 8141], "temperature": 0.0, "avg_logprob": -0.11757652885035465, "compression_ratio": 1.5666666666666667, "no_speech_prob": 6.604756345041096e-05}, {"id": 109, "seek": 71096, "start": 725.12, "end": 731.12, "text": " decomposition, neural nets, hidden Markov models, which we will not be covering. Parse trees will", "tokens": [48356, 11, 18161, 36170, 11, 7633, 3934, 5179, 5245, 11, 597, 321, 486, 406, 312, 10322, 13, 3457, 405, 5852, 486], "temperature": 0.0, "avg_logprob": -0.11757652885035465, "compression_ratio": 1.5666666666666667, "no_speech_prob": 6.604756345041096e-05}, {"id": 110, "seek": 71096, "start": 731.12, "end": 737.36, "text": " also not cover. I wanted to show a parse tree just so you know, because they are, or can be a big", "tokens": [611, 406, 2060, 13, 286, 1415, 281, 855, 257, 48377, 4230, 445, 370, 291, 458, 11, 570, 436, 366, 11, 420, 393, 312, 257, 955], "temperature": 0.0, "avg_logprob": -0.11757652885035465, "compression_ratio": 1.5666666666666667, "no_speech_prob": 6.604756345041096e-05}, {"id": 111, "seek": 73736, "start": 737.36, "end": 742.8000000000001, "text": " thing in linguistics, and there are plenty of people working on them, but that's kind of coming", "tokens": [551, 294, 21766, 6006, 11, 293, 456, 366, 7140, 295, 561, 1364, 322, 552, 11, 457, 300, 311, 733, 295, 1348], "temperature": 0.0, "avg_logprob": -0.07988473622485845, "compression_ratio": 1.654867256637168, "no_speech_prob": 4.2643805500119925e-05}, {"id": 112, "seek": 73736, "start": 742.8000000000001, "end": 750.08, "text": " up with the diagram of a sentence. Actually, I'm curious, who here had to diagram sentences", "tokens": [493, 365, 264, 10686, 295, 257, 8174, 13, 5135, 11, 286, 478, 6369, 11, 567, 510, 632, 281, 10686, 16579], "temperature": 0.0, "avg_logprob": -0.07988473622485845, "compression_ratio": 1.654867256637168, "no_speech_prob": 4.2643805500119925e-05}, {"id": 113, "seek": 73736, "start": 750.08, "end": 756.4, "text": " in school when they were younger? Okay, so still a few people. We did this when I was in school. I", "tokens": [294, 1395, 562, 436, 645, 7037, 30, 1033, 11, 370, 920, 257, 1326, 561, 13, 492, 630, 341, 562, 286, 390, 294, 1395, 13, 286], "temperature": 0.0, "avg_logprob": -0.07988473622485845, "compression_ratio": 1.654867256637168, "no_speech_prob": 4.2643805500119925e-05}, {"id": 114, "seek": 73736, "start": 756.4, "end": 761.6800000000001, "text": " feel like it might be going out of fashion. I don't know, but that's coming up with, so", "tokens": [841, 411, 309, 1062, 312, 516, 484, 295, 6700, 13, 286, 500, 380, 458, 11, 457, 300, 311, 1348, 493, 365, 11, 370], "temperature": 0.0, "avg_logprob": -0.07988473622485845, "compression_ratio": 1.654867256637168, "no_speech_prob": 4.2643805500119925e-05}, {"id": 115, "seek": 76168, "start": 761.68, "end": 767.52, "text": " there is this, yeah, you could have the sentence, she killed the man with the tie. Is it that the", "tokens": [456, 307, 341, 11, 1338, 11, 291, 727, 362, 264, 8174, 11, 750, 4652, 264, 587, 365, 264, 7582, 13, 1119, 309, 300, 264], "temperature": 0.0, "avg_logprob": -0.16577354431152344, "compression_ratio": 1.7954545454545454, "no_speech_prob": 7.411038950522197e-06}, {"id": 116, "seek": 76168, "start": 767.52, "end": 774.0799999999999, "text": " man was wearing a tie, who she killed, or was the tie the murder weapon that she used to kill the", "tokens": [587, 390, 4769, 257, 7582, 11, 567, 750, 4652, 11, 420, 390, 264, 7582, 264, 6568, 7463, 300, 750, 1143, 281, 1961, 264], "temperature": 0.0, "avg_logprob": -0.16577354431152344, "compression_ratio": 1.7954545454545454, "no_speech_prob": 7.411038950522197e-06}, {"id": 117, "seek": 76168, "start": 774.0799999999999, "end": 779.5999999999999, "text": " man? And it's ambiguous from the sentence, but the sentence structure of the parse tree would show", "tokens": [587, 30, 400, 309, 311, 39465, 490, 264, 8174, 11, 457, 264, 8174, 3877, 295, 264, 48377, 4230, 576, 855], "temperature": 0.0, "avg_logprob": -0.16577354431152344, "compression_ratio": 1.7954545454545454, "no_speech_prob": 7.411038950522197e-06}, {"id": 118, "seek": 76168, "start": 779.5999999999999, "end": 787.52, "text": " you. And basically it comes down to the prepositional phrase with the tie. Is that part of this noun", "tokens": [291, 13, 400, 1936, 309, 1487, 760, 281, 264, 2666, 329, 2628, 9535, 365, 264, 7582, 13, 1119, 300, 644, 295, 341, 23307], "temperature": 0.0, "avg_logprob": -0.16577354431152344, "compression_ratio": 1.7954545454545454, "no_speech_prob": 7.411038950522197e-06}, {"id": 119, "seek": 78752, "start": 787.52, "end": 796.0799999999999, "text": " phrase, the man, or is that part of the verb phrase, killed? And so, and here just broadly,", "tokens": [9535, 11, 264, 587, 11, 420, 307, 300, 644, 295, 264, 9595, 9535, 11, 4652, 30, 400, 370, 11, 293, 510, 445, 19511, 11], "temperature": 0.0, "avg_logprob": -0.1862072800145005, "compression_ratio": 1.5815899581589958, "no_speech_prob": 1.1125203855044674e-05}, {"id": 120, "seek": 78752, "start": 796.0799999999999, "end": 803.76, "text": " S stands for sentence, NP is a noun phrase, VP is a verb phrase. And we won't really be covering", "tokens": [318, 7382, 337, 8174, 11, 38611, 307, 257, 23307, 9535, 11, 35812, 307, 257, 9595, 9535, 13, 400, 321, 1582, 380, 534, 312, 10322], "temperature": 0.0, "avg_logprob": -0.1862072800145005, "compression_ratio": 1.5815899581589958, "no_speech_prob": 1.1125203855044674e-05}, {"id": 121, "seek": 78752, "start": 803.76, "end": 810.24, "text": " this, but I wanted to show you that this existed. I minored in linguistics in college, and I took", "tokens": [341, 11, 457, 286, 1415, 281, 855, 291, 300, 341, 13135, 13, 286, 923, 2769, 294, 21766, 6006, 294, 3859, 11, 293, 286, 1890], "temperature": 0.0, "avg_logprob": -0.1862072800145005, "compression_ratio": 1.5815899581589958, "no_speech_prob": 1.1125203855044674e-05}, {"id": 122, "seek": 78752, "start": 810.24, "end": 816.72, "text": " an entire semester on syntax that was kind of all about drawing these rules to document the", "tokens": [364, 2302, 11894, 322, 28431, 300, 390, 733, 295, 439, 466, 6316, 613, 4474, 281, 4166, 264], "temperature": 0.0, "avg_logprob": -0.1862072800145005, "compression_ratio": 1.5815899581589958, "no_speech_prob": 1.1125203855044674e-05}, {"id": 123, "seek": 81672, "start": 816.72, "end": 828.24, "text": " rules to diagram sentences. So some of the popular Python libraries for NLP, there's NLTK,", "tokens": [4474, 281, 10686, 16579, 13, 407, 512, 295, 264, 3743, 15329, 15148, 337, 426, 45196, 11, 456, 311, 426, 43, 51, 42, 11], "temperature": 0.0, "avg_logprob": -0.09231553579631605, "compression_ratio": 1.4329896907216495, "no_speech_prob": 1.0288676094205584e-05}, {"id": 124, "seek": 81672, "start": 829.2, "end": 835.6800000000001, "text": " which was first released in 2001. It's a very broad NLP library. I think it's less state of", "tokens": [597, 390, 700, 4736, 294, 16382, 13, 467, 311, 257, 588, 4152, 426, 45196, 6405, 13, 286, 519, 309, 311, 1570, 1785, 295], "temperature": 0.0, "avg_logprob": -0.09231553579631605, "compression_ratio": 1.4329896907216495, "no_speech_prob": 1.0288676094205584e-05}, {"id": 125, "seek": 81672, "start": 835.6800000000001, "end": 839.6800000000001, "text": " the art, but it does include kind of a lot of different things and has been around for a while.", "tokens": [264, 1523, 11, 457, 309, 775, 4090, 733, 295, 257, 688, 295, 819, 721, 293, 575, 668, 926, 337, 257, 1339, 13], "temperature": 0.0, "avg_logprob": -0.09231553579631605, "compression_ratio": 1.4329896907216495, "no_speech_prob": 1.0288676094205584e-05}, {"id": 126, "seek": 83968, "start": 839.68, "end": 847.12, "text": " Spacey, which is great for creating parse trees. It has a great tokenizer. It's an opinionated", "tokens": [8705, 88, 11, 597, 307, 869, 337, 4084, 48377, 5852, 13, 467, 575, 257, 869, 14862, 6545, 13, 467, 311, 364, 4800, 770], "temperature": 0.0, "avg_logprob": -0.1459768567766462, "compression_ratio": 1.6160337552742616, "no_speech_prob": 2.1778640075353906e-05}, {"id": 127, "seek": 83968, "start": 847.12, "end": 851.76, "text": " library, which I think is a good thing. It kind of will give you one way to do something and to do", "tokens": [6405, 11, 597, 286, 519, 307, 257, 665, 551, 13, 467, 733, 295, 486, 976, 291, 472, 636, 281, 360, 746, 293, 281, 360], "temperature": 0.0, "avg_logprob": -0.1459768567766462, "compression_ratio": 1.6160337552742616, "no_speech_prob": 2.1778640075353906e-05}, {"id": 128, "seek": 83968, "start": 851.76, "end": 857.28, "text": " it very well, as opposed to NLTK is more kind of this broad umbrella of here are a ton of ways to", "tokens": [309, 588, 731, 11, 382, 8851, 281, 426, 43, 51, 42, 307, 544, 733, 295, 341, 4152, 21925, 295, 510, 366, 257, 2952, 295, 2098, 281], "temperature": 0.0, "avg_logprob": -0.1459768567766462, "compression_ratio": 1.6160337552742616, "no_speech_prob": 2.1778640075353906e-05}, {"id": 129, "seek": 83968, "start": 857.28, "end": 863.92, "text": " do things. Gensim, which I have not used much, but that's for topic modeling and similarity", "tokens": [360, 721, 13, 460, 694, 332, 11, 597, 286, 362, 406, 1143, 709, 11, 457, 300, 311, 337, 4829, 15983, 293, 32194], "temperature": 0.0, "avg_logprob": -0.1459768567766462, "compression_ratio": 1.6160337552742616, "no_speech_prob": 2.1778640075353906e-05}, {"id": 130, "seek": 86392, "start": 863.92, "end": 871.76, "text": " detection. There are specialized tools such as PyText or FastText, and then also general", "tokens": [17784, 13, 821, 366, 19813, 3873, 1270, 382, 9953, 50198, 420, 15968, 50198, 11, 293, 550, 611, 2674], "temperature": 0.0, "avg_logprob": -0.2617735332912869, "compression_ratio": 1.5281385281385282, "no_speech_prob": 1.2605830306711141e-05}, {"id": 131, "seek": 86392, "start": 871.76, "end": 877.4399999999999, "text": " machine learning libraries have some NLP functionality such as IKITlearn and FastAI,", "tokens": [3479, 2539, 15148, 362, 512, 426, 45196, 14980, 1270, 382, 286, 42, 3927, 306, 1083, 293, 15968, 48698, 11], "temperature": 0.0, "avg_logprob": -0.2617735332912869, "compression_ratio": 1.5281385281385282, "no_speech_prob": 1.2605830306711141e-05}, {"id": 132, "seek": 86392, "start": 878.16, "end": 886.7199999999999, "text": " both of which we'll be using in here. And then I just want to share a few NLP applications to", "tokens": [1293, 295, 597, 321, 603, 312, 1228, 294, 510, 13, 400, 550, 286, 445, 528, 281, 2073, 257, 1326, 426, 45196, 5821, 281], "temperature": 0.0, "avg_logprob": -0.2617735332912869, "compression_ratio": 1.5281385281385282, "no_speech_prob": 1.2605830306711141e-05}, {"id": 133, "seek": 86392, "start": 886.7199999999999, "end": 892.24, "text": " close out kind of this intro to what is the field of NLP before we get into the first", "tokens": [1998, 484, 733, 295, 341, 12897, 281, 437, 307, 264, 2519, 295, 426, 45196, 949, 321, 483, 666, 264, 700], "temperature": 0.0, "avg_logprob": -0.2617735332912869, "compression_ratio": 1.5281385281385282, "no_speech_prob": 1.2605830306711141e-05}, {"id": 134, "seek": 89224, "start": 892.24, "end": 898.32, "text": " kind of programming notebook of looking at a specific application. Some things you can do with", "tokens": [733, 295, 9410, 21060, 295, 1237, 412, 257, 2685, 3861, 13, 2188, 721, 291, 393, 360, 365], "temperature": 0.0, "avg_logprob": -0.14954310113733466, "compression_ratio": 1.589430894308943, "no_speech_prob": 1.2804612197214738e-05}, {"id": 135, "seek": 89224, "start": 898.32, "end": 906.4, "text": " NLP. This was a project how QUID uses deep learning with small data. So QUID is a startup that has", "tokens": [426, 45196, 13, 639, 390, 257, 1716, 577, 7246, 2777, 4960, 2452, 2539, 365, 1359, 1412, 13, 407, 7246, 2777, 307, 257, 18578, 300, 575], "temperature": 0.0, "avg_logprob": -0.14954310113733466, "compression_ratio": 1.589430894308943, "no_speech_prob": 1.2804612197214738e-05}, {"id": 136, "seek": 89224, "start": 906.4, "end": 912.16, "text": " a database of company descriptions, and they wanted to identify which company descriptions were low", "tokens": [257, 8149, 295, 2237, 24406, 11, 293, 436, 1415, 281, 5876, 597, 2237, 24406, 645, 2295], "temperature": 0.0, "avg_logprob": -0.14954310113733466, "compression_ratio": 1.589430894308943, "no_speech_prob": 1.2804612197214738e-05}, {"id": 137, "seek": 89224, "start": 912.16, "end": 917.84, "text": " quality, meaning that they were kind of generic and used a lot of marketing language. And so they", "tokens": [3125, 11, 3620, 300, 436, 645, 733, 295, 19577, 293, 1143, 257, 688, 295, 6370, 2856, 13, 400, 370, 436], "temperature": 0.0, "avg_logprob": -0.14954310113733466, "compression_ratio": 1.589430894308943, "no_speech_prob": 1.2804612197214738e-05}, {"id": 138, "seek": 91784, "start": 917.84, "end": 922.8000000000001, "text": " did this with deep learning. Just to give some examples, actually, I guess this is still a", "tokens": [630, 341, 365, 2452, 2539, 13, 1449, 281, 976, 512, 5110, 11, 767, 11, 286, 2041, 341, 307, 920, 257], "temperature": 0.0, "avg_logprob": -0.20130020717404923, "compression_ratio": 1.68, "no_speech_prob": 1.1659040865197312e-05}, {"id": 139, "seek": 91784, "start": 922.8000000000001, "end": 930.48, "text": " little bit small, saying something like, launches quickly with configuration in as little as a week", "tokens": [707, 857, 1359, 11, 1566, 746, 411, 11, 31841, 2661, 365, 11694, 294, 382, 707, 382, 257, 1243], "temperature": 0.0, "avg_logprob": -0.20130020717404923, "compression_ratio": 1.68, "no_speech_prob": 1.1659040865197312e-05}, {"id": 140, "seek": 91784, "start": 930.48, "end": 935.44, "text": " doesn't really give you any information about the product, whereas spatial scanning software for", "tokens": [1177, 380, 534, 976, 291, 604, 1589, 466, 264, 1674, 11, 9735, 23598, 27019, 4722, 337], "temperature": 0.0, "avg_logprob": -0.20130020717404923, "compression_ratio": 1.68, "no_speech_prob": 1.1659040865197312e-05}, {"id": 141, "seek": 91784, "start": 935.44, "end": 939.6, "text": " mobile devices, that's giving you specific information. So that was a good description,", "tokens": [6013, 5759, 11, 300, 311, 2902, 291, 2685, 1589, 13, 407, 300, 390, 257, 665, 3855, 11], "temperature": 0.0, "avg_logprob": -0.20130020717404923, "compression_ratio": 1.68, "no_speech_prob": 1.1659040865197312e-05}, {"id": 142, "seek": 91784, "start": 939.6, "end": 945.12, "text": " the first one was a bad one. So that's, and that's also an example of a nice blog post", "tokens": [264, 700, 472, 390, 257, 1578, 472, 13, 407, 300, 311, 11, 293, 300, 311, 611, 364, 1365, 295, 257, 1481, 6968, 2183], "temperature": 0.0, "avg_logprob": -0.20130020717404923, "compression_ratio": 1.68, "no_speech_prob": 1.1659040865197312e-05}, {"id": 143, "seek": 94512, "start": 945.12, "end": 952.96, "text": " let me open it up. Yeah, kind of going through going through this problem set, and I'm always", "tokens": [718, 385, 1269, 309, 493, 13, 865, 11, 733, 295, 516, 807, 516, 807, 341, 1154, 992, 11, 293, 286, 478, 1009], "temperature": 0.0, "avg_logprob": -0.19931270962669737, "compression_ratio": 1.6157205240174672, "no_speech_prob": 1.1477914995339233e-05}, {"id": 144, "seek": 94512, "start": 952.96, "end": 959.84, "text": " interested in things on little data as well. Another application comes from a law student", "tokens": [3102, 294, 721, 322, 707, 1412, 382, 731, 13, 3996, 3861, 1487, 490, 257, 2101, 3107], "temperature": 0.0, "avg_logprob": -0.19931270962669737, "compression_ratio": 1.6157205240174672, "no_speech_prob": 1.1477914995339233e-05}, {"id": 145, "seek": 94512, "start": 959.84, "end": 967.12, "text": " in Singapore who classified legal documents by category. So knowing whether a document related", "tokens": [294, 14491, 567, 20627, 5089, 8512, 538, 7719, 13, 407, 5276, 1968, 257, 4166, 4077], "temperature": 0.0, "avg_logprob": -0.19931270962669737, "compression_ratio": 1.6157205240174672, "no_speech_prob": 1.1477914995339233e-05}, {"id": 146, "seek": 94512, "start": 967.12, "end": 973.44, "text": " to civil law, criminal law, contract, family, tort. And this is this is an application that", "tokens": [281, 5605, 2101, 11, 8628, 2101, 11, 4364, 11, 1605, 11, 10806, 13, 400, 341, 307, 341, 307, 364, 3861, 300], "temperature": 0.0, "avg_logprob": -0.19931270962669737, "compression_ratio": 1.6157205240174672, "no_speech_prob": 1.1477914995339233e-05}, {"id": 147, "seek": 97344, "start": 973.44, "end": 978.96, "text": " or this problem of classification we will be covering later, although not this application", "tokens": [420, 341, 1154, 295, 21538, 321, 486, 312, 10322, 1780, 11, 4878, 406, 341, 3861], "temperature": 0.0, "avg_logprob": -0.22421800273738496, "compression_ratio": 1.6168224299065421, "no_speech_prob": 1.3004309039388318e-05}, {"id": 148, "seek": 97344, "start": 978.96, "end": 985.44, "text": " specifically. And I thought this was neat, this is a journalism grad student analyze the Twitter", "tokens": [4682, 13, 400, 286, 1194, 341, 390, 10654, 11, 341, 307, 257, 23191, 2771, 3107, 12477, 264, 5794], "temperature": 0.0, "avg_logprob": -0.22421800273738496, "compression_ratio": 1.6168224299065421, "no_speech_prob": 1.3004309039388318e-05}, {"id": 149, "seek": 97344, "start": 985.44, "end": 989.36, "text": " sentiment of different different people running for political office.", "tokens": [16149, 295, 819, 819, 561, 2614, 337, 3905, 3398, 13], "temperature": 0.0, "avg_logprob": -0.22421800273738496, "compression_ratio": 1.6168224299065421, "no_speech_prob": 1.3004309039388318e-05}, {"id": 150, "seek": 97344, "start": 994.4000000000001, "end": 999.9200000000001, "text": " And this is a this is a neat article introducing metadata enhanced ULM fit. So they were", "tokens": [400, 341, 307, 257, 341, 307, 257, 10654, 7222, 15424, 26603, 21191, 624, 43, 44, 3318, 13, 407, 436, 645], "temperature": 0.0, "avg_logprob": -0.22421800273738496, "compression_ratio": 1.6168224299065421, "no_speech_prob": 1.3004309039388318e-05}, {"id": 151, "seek": 99992, "start": 999.92, "end": 1005.1999999999999, "text": " classifying quotes from articles and they used metadata such as the publication, the country,", "tokens": [1508, 5489, 19963, 490, 11290, 293, 436, 1143, 26603, 1270, 382, 264, 19953, 11, 264, 1941, 11], "temperature": 0.0, "avg_logprob": -0.1303896511302275, "compression_ratio": 1.6051502145922747, "no_speech_prob": 6.012333324179053e-05}, {"id": 152, "seek": 99992, "start": 1005.1999999999999, "end": 1011.4399999999999, "text": " and the source together with the text. And so this is an example of pairing what we would often", "tokens": [293, 264, 4009, 1214, 365, 264, 2487, 13, 400, 370, 341, 307, 364, 1365, 295, 32735, 437, 321, 576, 2049], "temperature": 0.0, "avg_logprob": -0.1303896511302275, "compression_ratio": 1.6051502145922747, "no_speech_prob": 6.012333324179053e-05}, {"id": 153, "seek": 99992, "start": 1011.4399999999999, "end": 1016.4799999999999, "text": " think of as more structured data with unstructured data to improve your accuracy. And that's", "tokens": [519, 295, 382, 544, 18519, 1412, 365, 18799, 46847, 1412, 281, 3470, 428, 14170, 13, 400, 300, 311], "temperature": 0.0, "avg_logprob": -0.1303896511302275, "compression_ratio": 1.6051502145922747, "no_speech_prob": 6.012333324179053e-05}, {"id": 154, "seek": 99992, "start": 1016.4799999999999, "end": 1021.1999999999999, "text": " something to keep in mind and kind of relates to you don't have to think of NLP necessarily", "tokens": [746, 281, 1066, 294, 1575, 293, 733, 295, 16155, 281, 291, 500, 380, 362, 281, 519, 295, 426, 45196, 4725], "temperature": 0.0, "avg_logprob": -0.1303896511302275, "compression_ratio": 1.6051502145922747, "no_speech_prob": 6.012333324179053e-05}, {"id": 155, "seek": 102120, "start": 1021.2, "end": 1031.28, "text": " as separate from tabular data, but you can use them together. And finally, and we'll come back", "tokens": [382, 4994, 490, 4421, 1040, 1412, 11, 457, 291, 393, 764, 552, 1214, 13, 400, 2721, 11, 293, 321, 603, 808, 646], "temperature": 0.0, "avg_logprob": -0.4043235778808594, "compression_ratio": 1.5916666666666666, "no_speech_prob": 5.47462550457567e-05}, {"id": 156, "seek": 102120, "start": 1031.28, "end": 1036.0800000000002, "text": " to this kind of in the last week or two in more detail, I want to talk about some of the ethical", "tokens": [281, 341, 733, 295, 294, 264, 1036, 1243, 420, 732, 294, 544, 2607, 11, 286, 528, 281, 751, 466, 512, 295, 264, 18890], "temperature": 0.0, "avg_logprob": -0.4043235778808594, "compression_ratio": 1.5916666666666666, "no_speech_prob": 5.47462550457567e-05}, {"id": 157, "seek": 102120, "start": 1036.0800000000002, "end": 1043.1200000000001, "text": " issues in NLP. So bias is a huge issue. And I spoke about this on the Friday seminar in the fall.", "tokens": [2663, 294, 426, 45196, 13, 407, 12577, 307, 257, 2603, 2734, 13, 400, 286, 7179, 466, 341, 322, 264, 6984, 29235, 294, 264, 2100, 13], "temperature": 0.0, "avg_logprob": -0.4043235778808594, "compression_ratio": 1.5916666666666666, "no_speech_prob": 5.47462550457567e-05}, {"id": 158, "seek": 102120, "start": 1043.68, "end": 1050.56, "text": " There's bias and word embedding, and I think that's a really important issue. And so this is", "tokens": [821, 311, 12577, 293, 1349, 12240, 3584, 11, 293, 286, 519, 300, 311, 257, 534, 1021, 2734, 13, 400, 370, 341, 307], "temperature": 0.0, "avg_logprob": -0.4043235778808594, "compression_ratio": 1.5916666666666666, "no_speech_prob": 5.47462550457567e-05}, {"id": 159, "seek": 105056, "start": 1050.56, "end": 1057.04, "text": " word embeddings. And so word embeddings are a useful tool, but they also produce biased results", "tokens": [1349, 12240, 29432, 13, 400, 370, 1349, 12240, 29432, 366, 257, 4420, 2290, 11, 457, 436, 611, 5258, 28035, 3542], "temperature": 0.0, "avg_logprob": -0.08496681336433656, "compression_ratio": 1.726027397260274, "no_speech_prob": 2.7960415536654182e-05}, {"id": 160, "seek": 105056, "start": 1057.04, "end": 1063.12, "text": " because they're learning from historical data, which is biased. And so Google Translate, if you", "tokens": [570, 436, 434, 2539, 490, 8584, 1412, 11, 597, 307, 28035, 13, 400, 370, 3329, 6531, 17593, 11, 498, 291], "temperature": 0.0, "avg_logprob": -0.08496681336433656, "compression_ratio": 1.726027397260274, "no_speech_prob": 2.7960415536654182e-05}, {"id": 161, "seek": 105056, "start": 1063.12, "end": 1069.76, "text": " enter she is a doctor, he is a nurse, translated that into Turkish, which has a gender neutral", "tokens": [3242, 750, 307, 257, 4631, 11, 415, 307, 257, 14012, 11, 16805, 300, 666, 18565, 11, 597, 575, 257, 7898, 10598], "temperature": 0.0, "avg_logprob": -0.08496681336433656, "compression_ratio": 1.726027397260274, "no_speech_prob": 2.7960415536654182e-05}, {"id": 162, "seek": 105056, "start": 1069.76, "end": 1075.36, "text": " singular pronoun, and then back into English, it comes back he is a doctor, she is a nurse.", "tokens": [20010, 14144, 11, 293, 550, 646, 666, 3669, 11, 309, 1487, 646, 415, 307, 257, 4631, 11, 750, 307, 257, 14012, 13], "temperature": 0.0, "avg_logprob": -0.08496681336433656, "compression_ratio": 1.726027397260274, "no_speech_prob": 2.7960415536654182e-05}, {"id": 163, "seek": 107536, "start": 1075.36, "end": 1081.6, "text": " So the genders have been flipped to fit the stereotype. And you can see this with several", "tokens": [407, 264, 290, 16292, 362, 668, 26273, 281, 3318, 264, 38229, 13, 400, 291, 393, 536, 341, 365, 2940], "temperature": 0.0, "avg_logprob": -0.10063285696996402, "compression_ratio": 1.4761904761904763, "no_speech_prob": 2.6676720153773203e-05}, {"id": 164, "seek": 107536, "start": 1081.6, "end": 1092.08, "text": " things, babysitters end up being she. And so there's a good article in the MIT Tech Review", "tokens": [721, 11, 39764, 38873, 917, 493, 885, 750, 13, 400, 370, 456, 311, 257, 665, 7222, 294, 264, 13100, 13795, 19954], "temperature": 0.0, "avg_logprob": -0.10063285696996402, "compression_ratio": 1.4761904761904763, "no_speech_prob": 2.6676720153773203e-05}, {"id": 165, "seek": 107536, "start": 1092.08, "end": 1102.3999999999999, "text": " a few years ago on that. There have been some proposals of ways to try to de-bias word embeddings,", "tokens": [257, 1326, 924, 2057, 322, 300, 13, 821, 362, 668, 512, 20198, 295, 2098, 281, 853, 281, 368, 12, 65, 4609, 1349, 12240, 29432, 11], "temperature": 0.0, "avg_logprob": -0.10063285696996402, "compression_ratio": 1.4761904761904763, "no_speech_prob": 2.6676720153773203e-05}, {"id": 166, "seek": 110240, "start": 1102.4, "end": 1109.6000000000001, "text": " but there was a paper earlier this year that I liked suggesting that de-biasing word embeddings", "tokens": [457, 456, 390, 257, 3035, 3071, 341, 1064, 300, 286, 4501, 18094, 300, 368, 12, 5614, 3349, 1349, 12240, 29432], "temperature": 0.0, "avg_logprob": -0.17849414244942044, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.00010884884977713227}, {"id": 167, "seek": 110240, "start": 1111.1200000000001, "end": 1114.72, "text": " is just kind of covering them up, but doesn't remove them, and you still have the underlying", "tokens": [307, 445, 733, 295, 10322, 552, 493, 11, 457, 1177, 380, 4159, 552, 11, 293, 291, 920, 362, 264, 14217], "temperature": 0.0, "avg_logprob": -0.17849414244942044, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.00010884884977713227}, {"id": 168, "seek": 110240, "start": 1114.72, "end": 1124.5600000000002, "text": " structure. If you want a lot more detail, I gave a kind of a two hour workshop on this several years", "tokens": [3877, 13, 759, 291, 528, 257, 688, 544, 2607, 11, 286, 2729, 257, 733, 295, 257, 732, 1773, 13541, 322, 341, 2940, 924], "temperature": 0.0, "avg_logprob": -0.17849414244942044, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.00010884884977713227}, {"id": 169, "seek": 110240, "start": 1124.5600000000002, "end": 1128.64, "text": " ago that comes with the Jupyter Notebook and really goes through kind of letting you see", "tokens": [2057, 300, 1487, 365, 264, 22125, 88, 391, 11633, 2939, 293, 534, 1709, 807, 733, 295, 8295, 291, 536], "temperature": 0.0, "avg_logprob": -0.17849414244942044, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.00010884884977713227}, {"id": 170, "seek": 112864, "start": 1128.64, "end": 1134.8000000000002, "text": " how certain words are closer to each other versus not. Language models have been getting more", "tokens": [577, 1629, 2283, 366, 4966, 281, 1184, 661, 5717, 406, 13, 24445, 5245, 362, 668, 1242, 544], "temperature": 0.0, "avg_logprob": -0.153634399496099, "compression_ratio": 1.5236220472440944, "no_speech_prob": 5.304667502059601e-05}, {"id": 171, "seek": 112864, "start": 1134.8000000000002, "end": 1141.5200000000002, "text": " attention recently, and this is where you have an algorithm that's generating text, specifically a", "tokens": [3202, 3938, 11, 293, 341, 307, 689, 291, 362, 364, 9284, 300, 311, 17746, 2487, 11, 4682, 257], "temperature": 0.0, "avg_logprob": -0.153634399496099, "compression_ratio": 1.5236220472440944, "no_speech_prob": 5.304667502059601e-05}, {"id": 172, "seek": 112864, "start": 1141.5200000000002, "end": 1149.0400000000002, "text": " neural network. And so they looked at OpenAI's GPT-2, which has been a kind of state of the art", "tokens": [18161, 3209, 13, 400, 370, 436, 2956, 412, 7238, 48698, 311, 26039, 51, 12, 17, 11, 597, 575, 668, 257, 733, 295, 1785, 295, 264, 1523], "temperature": 0.0, "avg_logprob": -0.153634399496099, "compression_ratio": 1.5236220472440944, "no_speech_prob": 5.304667502059601e-05}, {"id": 173, "seek": 112864, "start": 1149.0400000000002, "end": 1156.0, "text": " language model that came out recently. And if you prompt it, my wife just got an exciting new job,", "tokens": [2856, 2316, 300, 1361, 484, 3938, 13, 400, 498, 291, 12391, 309, 11, 452, 3836, 445, 658, 364, 4670, 777, 1691, 11], "temperature": 0.0, "avg_logprob": -0.153634399496099, "compression_ratio": 1.5236220472440944, "no_speech_prob": 5.304667502059601e-05}, {"id": 174, "seek": 115600, "start": 1156.0, "end": 1161.04, "text": " it suggests that she's going to be a housekeeper and full-time mom. If you say my husband got an", "tokens": [309, 13409, 300, 750, 311, 516, 281, 312, 257, 1782, 23083, 293, 1577, 12, 3766, 1225, 13, 759, 291, 584, 452, 5213, 658, 364], "temperature": 0.0, "avg_logprob": -0.14529430866241455, "compression_ratio": 1.651063829787234, "no_speech_prob": 5.389977741288021e-05}, {"id": 175, "seek": 115600, "start": 1161.04, "end": 1168.48, "text": " exciting new job, he's going to be a doctor and a financial consultant simultaneously. And so that's", "tokens": [4670, 777, 1691, 11, 415, 311, 516, 281, 312, 257, 4631, 293, 257, 4669, 24676, 16561, 13, 400, 370, 300, 311], "temperature": 0.0, "avg_logprob": -0.14529430866241455, "compression_ratio": 1.651063829787234, "no_speech_prob": 5.389977741288021e-05}, {"id": 176, "seek": 115600, "start": 1168.48, "end": 1175.28, "text": " just issues to know about, and we'll talk about this more later. And then in addition to bias,", "tokens": [445, 2663, 281, 458, 466, 11, 293, 321, 603, 751, 466, 341, 544, 1780, 13, 400, 550, 294, 4500, 281, 12577, 11], "temperature": 0.0, "avg_logprob": -0.14529430866241455, "compression_ratio": 1.651063829787234, "no_speech_prob": 5.389977741288021e-05}, {"id": 177, "seek": 115600, "start": 1175.28, "end": 1182.08, "text": " there's the concern around fakery. So now that language models are getting better and better at", "tokens": [456, 311, 264, 3136, 926, 33647, 2109, 13, 407, 586, 300, 2856, 5245, 366, 1242, 1101, 293, 1101, 412], "temperature": 0.0, "avg_logprob": -0.14529430866241455, "compression_ratio": 1.651063829787234, "no_speech_prob": 5.389977741288021e-05}, {"id": 178, "seek": 118208, "start": 1182.08, "end": 1191.1999999999998, "text": " producing kind of convincing text, it will be, so to quote Jeremy Howard, who's here, said,", "tokens": [10501, 733, 295, 24823, 2487, 11, 309, 486, 312, 11, 370, 281, 6513, 17809, 17626, 11, 567, 311, 510, 11, 848, 11], "temperature": 0.0, "avg_logprob": -0.21211806008982104, "compression_ratio": 1.5924369747899159, "no_speech_prob": 1.4062697118788492e-05}, {"id": 179, "seek": 118208, "start": 1191.1999999999998, "end": 1196.24, "text": " we have the technology to totally fill Twitter email and the web up with reasonable sounding,", "tokens": [321, 362, 264, 2899, 281, 3879, 2836, 5794, 3796, 293, 264, 3670, 493, 365, 10585, 24931, 11], "temperature": 0.0, "avg_logprob": -0.21211806008982104, "compression_ratio": 1.5924369747899159, "no_speech_prob": 1.4062697118788492e-05}, {"id": 180, "seek": 118208, "start": 1196.24, "end": 1202.72, "text": " context appropriate prose, which could drown out all other speech and be impossible to filter.", "tokens": [4319, 6854, 12505, 11, 597, 727, 20337, 484, 439, 661, 6218, 293, 312, 6243, 281, 6608, 13], "temperature": 0.0, "avg_logprob": -0.21211806008982104, "compression_ratio": 1.5924369747899159, "no_speech_prob": 1.4062697118788492e-05}, {"id": 181, "seek": 118208, "start": 1203.28, "end": 1207.6, "text": " So that's kind of something scary and something to be thinking about as you take kind of take your", "tokens": [407, 300, 311, 733, 295, 746, 6958, 293, 746, 281, 312, 1953, 466, 382, 291, 747, 733, 295, 747, 428], "temperature": 0.0, "avg_logprob": -0.21211806008982104, "compression_ratio": 1.5924369747899159, "no_speech_prob": 1.4062697118788492e-05}, {"id": 182, "seek": 120760, "start": 1207.6, "end": 1213.9199999999998, "text": " first jobs in the field, the ethical implications and kind of preparing for how we want to address", "tokens": [700, 4782, 294, 264, 2519, 11, 264, 18890, 16602, 293, 733, 295, 10075, 337, 577, 321, 528, 281, 2985], "temperature": 0.0, "avg_logprob": -0.17589589321252072, "compression_ratio": 1.5638297872340425, "no_speech_prob": 1.5202987015072722e-05}, {"id": 183, "seek": 120760, "start": 1213.9199999999998, "end": 1224.8799999999999, "text": " this. There's a list of just to make this a little bit more concrete of some things that could go", "tokens": [341, 13, 821, 311, 257, 1329, 295, 445, 281, 652, 341, 257, 707, 857, 544, 9859, 295, 512, 721, 300, 727, 352], "temperature": 0.0, "avg_logprob": -0.17589589321252072, "compression_ratio": 1.5638297872340425, "no_speech_prob": 1.5202987015072722e-05}, {"id": 184, "seek": 120760, "start": 1224.8799999999999, "end": 1233.4399999999998, "text": " wrong with having kind of more convincing generated language. So you think of combining this with", "tokens": [2085, 365, 1419, 733, 295, 544, 24823, 10833, 2856, 13, 407, 291, 519, 295, 21928, 341, 365], "temperature": 0.0, "avg_logprob": -0.17589589321252072, "compression_ratio": 1.5638297872340425, "no_speech_prob": 1.5202987015072722e-05}, {"id": 185, "seek": 123344, "start": 1233.44, "end": 1240.56, "text": " deep fakes, videos, you know, having a world leader declaring war or saying something against", "tokens": [2452, 283, 3419, 11, 2145, 11, 291, 458, 11, 1419, 257, 1002, 5263, 40374, 1516, 420, 1566, 746, 1970], "temperature": 0.0, "avg_logprob": -0.19905164382036994, "compression_ratio": 1.5958333333333334, "no_speech_prob": 3.117482992820442e-05}, {"id": 186, "seek": 123344, "start": 1240.56, "end": 1246.88, "text": " another country. This could be very bad politically. It doesn't have to be perfect, just good enough to", "tokens": [1071, 1941, 13, 639, 727, 312, 588, 1578, 21154, 13, 467, 1177, 380, 362, 281, 312, 2176, 11, 445, 665, 1547, 281], "temperature": 0.0, "avg_logprob": -0.19905164382036994, "compression_ratio": 1.5958333333333334, "no_speech_prob": 3.117482992820442e-05}, {"id": 187, "seek": 123344, "start": 1246.88, "end": 1252.8, "text": " make the enemy think something has happened. You can also think of the issue politically", "tokens": [652, 264, 5945, 519, 746, 575, 2011, 13, 509, 393, 611, 519, 295, 264, 2734, 21154], "temperature": 0.0, "avg_logprob": -0.19905164382036994, "compression_ratio": 1.5958333333333334, "no_speech_prob": 3.117482992820442e-05}, {"id": 188, "seek": 123344, "start": 1252.8, "end": 1259.6000000000001, "text": " where legislators are flooded with fraudulent and attention. And this actually, I guess, happens", "tokens": [689, 39264, 366, 31594, 365, 14560, 23405, 293, 3202, 13, 400, 341, 767, 11, 286, 2041, 11, 2314], "temperature": 0.0, "avg_logprob": -0.19905164382036994, "compression_ratio": 1.5958333333333334, "no_speech_prob": 3.117482992820442e-05}, {"id": 189, "seek": 125960, "start": 1259.6, "end": 1266.9599999999998, "text": " in a smaller scale. There was a great blog post. I'll update this to include the link of, I think,", "tokens": [294, 257, 4356, 4373, 13, 821, 390, 257, 869, 6968, 2183, 13, 286, 603, 5623, 341, 281, 4090, 264, 2113, 295, 11, 286, 519, 11], "temperature": 0.0, "avg_logprob": -0.26729565081389056, "compression_ratio": 1.6101694915254237, "no_speech_prob": 4.2625742935342714e-05}, {"id": 190, "seek": 125960, "start": 1267.9199999999998, "end": 1274.32, "text": " there was a way for people to respond around that neutrality and a data scientist went through", "tokens": [456, 390, 257, 636, 337, 561, 281, 4196, 926, 300, 39913, 1860, 293, 257, 1412, 12662, 1437, 807], "temperature": 0.0, "avg_logprob": -0.26729565081389056, "compression_ratio": 1.6101694915254237, "no_speech_prob": 4.2625742935342714e-05}, {"id": 191, "seek": 125960, "start": 1274.32, "end": 1279.9199999999998, "text": " the comments and discovered that a ton of them seem to be created by bots. But you can think", "tokens": [264, 3053, 293, 6941, 300, 257, 2952, 295, 552, 1643, 281, 312, 2942, 538, 35410, 13, 583, 291, 393, 519], "temperature": 0.0, "avg_logprob": -0.26729565081389056, "compression_ratio": 1.6101694915254237, "no_speech_prob": 4.2625742935342714e-05}, {"id": 192, "seek": 125960, "start": 1279.9199999999998, "end": 1286.24, "text": " if humans are going to have to start competing with fake campaigns or bots and even go to the", "tokens": [498, 6255, 366, 516, 281, 362, 281, 722, 15439, 365, 7592, 16840, 420, 35410, 293, 754, 352, 281, 264], "temperature": 0.0, "avg_logprob": -0.26729565081389056, "compression_ratio": 1.6101694915254237, "no_speech_prob": 4.2625742935342714e-05}, {"id": 193, "seek": 128624, "start": 1286.24, "end": 1293.36, "text": " with fake campaigns or bots and even getting legislation, legislator attention around issues.", "tokens": [365, 7592, 16840, 420, 35410, 293, 754, 1242, 11329, 11, 6593, 1639, 3202, 926, 2663, 13], "temperature": 0.0, "avg_logprob": -0.16115781872771506, "compression_ratio": 1.5502183406113537, "no_speech_prob": 3.535021096467972e-05}, {"id": 194, "seek": 128624, "start": 1294.32, "end": 1298.88, "text": " And then also spam is going to become much more compelling. Like right now,", "tokens": [400, 550, 611, 24028, 307, 516, 281, 1813, 709, 544, 20050, 13, 1743, 558, 586, 11], "temperature": 0.0, "avg_logprob": -0.16115781872771506, "compression_ratio": 1.5502183406113537, "no_speech_prob": 3.535021096467972e-05}, {"id": 195, "seek": 128624, "start": 1299.76, "end": 1305.36, "text": " most of us probably think, oh, it's easy to spot spam. It looks pretty fake. But if people can", "tokens": [881, 295, 505, 1391, 519, 11, 1954, 11, 309, 311, 1858, 281, 4008, 24028, 13, 467, 1542, 1238, 7592, 13, 583, 498, 561, 393], "temperature": 0.0, "avg_logprob": -0.16115781872771506, "compression_ratio": 1.5502183406113537, "no_speech_prob": 3.535021096467972e-05}, {"id": 196, "seek": 128624, "start": 1306.32, "end": 1312.08, "text": " imitate the way your loved one talks in a much more appropriate way, that's kind of scary.", "tokens": [35556, 264, 636, 428, 4333, 472, 6686, 294, 257, 709, 544, 6854, 636, 11, 300, 311, 733, 295, 6958, 13], "temperature": 0.0, "avg_logprob": -0.16115781872771506, "compression_ratio": 1.5502183406113537, "no_speech_prob": 3.535021096467972e-05}, {"id": 197, "seek": 131208, "start": 1312.08, "end": 1316.8799999999999, "text": " And so, and then I also wanted to link to this was a good article by Oren Ecziani, who's the head of", "tokens": [400, 370, 11, 293, 550, 286, 611, 1415, 281, 2113, 281, 341, 390, 257, 665, 7222, 538, 422, 1095, 462, 3689, 21309, 11, 567, 311, 264, 1378, 295], "temperature": 0.0, "avg_logprob": -0.1830644412916534, "compression_ratio": 1.5909090909090908, "no_speech_prob": 2.1442789147840813e-05}, {"id": 198, "seek": 131208, "start": 1316.8799999999999, "end": 1323.36, "text": " the Allen Institute on AI. And he was saying that we basically need, AI is poised to make high", "tokens": [264, 17160, 9446, 322, 7318, 13, 400, 415, 390, 1566, 300, 321, 1936, 643, 11, 7318, 307, 714, 2640, 281, 652, 1090], "temperature": 0.0, "avg_logprob": -0.1830644412916534, "compression_ratio": 1.5909090909090908, "no_speech_prob": 2.1442789147840813e-05}, {"id": 199, "seek": 131208, "start": 1323.36, "end": 1331.76, "text": " fidelity forgery inexpensive and automated. We need to start kind of getting the norm out there", "tokens": [46404, 337, 7337, 28382, 293, 18473, 13, 492, 643, 281, 722, 733, 295, 1242, 264, 2026, 484, 456], "temperature": 0.0, "avg_logprob": -0.1830644412916534, "compression_ratio": 1.5909090909090908, "no_speech_prob": 2.1442789147840813e-05}, {"id": 200, "seek": 131208, "start": 1331.76, "end": 1336.56, "text": " that any item that isn't signed is potentially forged, but that would kind of require a huge,", "tokens": [300, 604, 3174, 300, 1943, 380, 8175, 307, 7263, 40226, 11, 457, 300, 576, 733, 295, 3651, 257, 2603, 11], "temperature": 0.0, "avg_logprob": -0.1830644412916534, "compression_ratio": 1.5909090909090908, "no_speech_prob": 2.1442789147840813e-05}, {"id": 201, "seek": 133656, "start": 1336.56, "end": 1341.76, "text": " huge cultural shift. So these are just some issues to kind of have on the back burner as we", "tokens": [2603, 6988, 5513, 13, 407, 613, 366, 445, 512, 2663, 281, 733, 295, 362, 322, 264, 646, 36116, 382, 321], "temperature": 0.0, "avg_logprob": -0.24584607517018037, "compression_ratio": 1.398876404494382, "no_speech_prob": 1.7228032447746955e-05}, {"id": 202, "seek": 133656, "start": 1341.76, "end": 1346.08, "text": " get into our study of NLP and then we will talk about them more later in the course.", "tokens": [483, 666, 527, 2979, 295, 426, 45196, 293, 550, 321, 486, 751, 466, 552, 544, 1780, 294, 264, 1164, 13], "temperature": 0.0, "avg_logprob": -0.24584607517018037, "compression_ratio": 1.398876404494382, "no_speech_prob": 1.7228032447746955e-05}, {"id": 203, "seek": 133656, "start": 1348.48, "end": 1350.48, "text": " Cool. Any, any questions so far?", "tokens": [8561, 13, 2639, 11, 604, 1651, 370, 1400, 30], "temperature": 0.0, "avg_logprob": -0.24584607517018037, "compression_ratio": 1.398876404494382, "no_speech_prob": 1.7228032447746955e-05}, {"id": 204, "seek": 135048, "start": 1350.48, "end": 1363.76, "text": " This is just a very broad, broad intro.", "tokens": [50364, 639, 307, 445, 257, 588, 4152, 11, 4152, 12897, 13, 51028], "temperature": 0.0, "avg_logprob": -0.3884548040536734, "compression_ratio": 0.9512195121951219, "no_speech_prob": 1.642901224840898e-05}], "language": "en"}