{"text": " So, we've looked at a lot of different random forest interpretation techniques. A question that's come up a little bit on the forums is like, what are these for, really? How do these help me get a better score on Kaggle? And my answer's kind of been like, they don't necessarily. So I want to talk more about why do we do machine learning? What's the point? To answer this question, I'm going to put this PowerPoint in the GitHub repo so you can have a look. I want to show you something really important, which is examples of how people have used machine learning mainly in business, because that's where most of you are probably going to end up after this is working for some company. I'm going to show you applications of machine learning which are either based on things that I've been personally involved in myself or know of people who are doing them directly. So none of these are going to be like hypotheticals, these are all actual things that people are doing and I've got direct or secondhand knowledge of. I'm going to split them into two groups, horizontal and vertical. So in business, horizontal means something that you do across different kinds of business, whereas vertical means something that you do within a business or within a supply chain or within a process. So in other words, an example of horizontal applications is everything involving marketing. So every company pretty much has to try to sell more products to its customers and so therefore does marketing. And so each of these boxes are examples of some of the things that people are using machine learning for in marketing. So let's take an example. Let's take churn. So churn refers to a model which attempts to predict who's going to leave. So I've done some churn modeling fairly recently in telecommunications. And so we're trying to figure out for this big cell phone company which customers are going to leave. That is not of itself that interesting. Like building a highly predictive model that says Jeremy Howard is almost certainly going to leave next month is probably not that helpful because if I'm almost certainly going to leave next month, there's probably nothing you can do about it. It's too late. It's going to cost you too much to keep me. So in order to understand why would we do churn modeling, I've got a little framework that you might find helpful. So if you Google for Jeremy Howard data products, I think I've mentioned this thing before, there's a paper you can find, Designing Great Data Products that I wrote with a couple of colleagues a few years ago. And in it, I describe my experience of actually turning machine learning models into like stuff that makes money. And the basic trick is this thing I call the drivetrain approach, which is these 4 steps. The starting point to actually turn a machine learning project into something that's actually useful is to know what am I trying to achieve. And that doesn't mean like I'm trying to achieve a high area under the ROC curve, or I'm trying to achieve a large difference between classes. No, it would be I'm trying to sell more books, or I'm trying to reduce the number of customers that leave next month, or I'm trying to detect lung cancer earlier. These are things, these are objectives. So the objective is something that absolutely directly is the thing that the company or the organization actually wants. No company or organization lives in order to create a more accurate predictive model for some reason. So that's your objective. Now that's obviously the most important thing. If you don't know the purpose of what you're modeling for, then you can't possibly do a good job of it. And hopefully people are starting to pick that up out there in the world of data science. But interestingly, what very few people are talking about, but it's just as important as the next thing, which is levers. A lever is a thing that the organization can do to actually drive the objective. So let's take the example of churn modeling. What is a lever that an organization could use to reduce the number of customers that are leaving? They could take a closer look at the model and do some of this random forest interpretation and see some of the causes that are causing people to leave and potentially change those issues in the company. So that's a data scientist answer. But I want you to go to the next level. What are the things, the levers are the things they can do. Do you want to put it past behind you? What are the things that they can do? Just outreach, like calling or sending emails. They could call someone and say like, are you happy, anything we could do. They can provide incentives to increase engagement with the product. So they could give them a free pen or something if they buy 20 bucks worth of product next month. You want to do that as well? You guys are the giving out carrots rather than handing out sticks. These are levers. And so whenever you're working as a data scientist, keep coming back and thinking, what are we trying to achieve, we being the organization, and how are we trying to achieve it being like what are the actual things we can do to make that objective happen. So building a model is never ever a lever. But it could help you with the lever. So then the next step is, what data does the organization have that could possibly help them to set that lever to achieve that objective. And so this is not what data did they give you when you started the project, but think about it from a first principles point of view. I'm working for a telecommunications company, they gave me some certain set of data, but I'm sure they must know where their customers live, how many phone calls they made last month, how many times they called customer service, whatever. And so have a think about, if we're trying to decide who should we give a special offer to proactively, then we want to figure out what information do we have that might help us to identify who's going to react well or badly to that. Perhaps more interestingly would be, what if we were doing a fraud algorithm, and so we're trying to figure out who's going to not pay for the phone that they take out of the store, they're on some 12 month payment plan, we never see them again. Now in that case, the data we have available, it doesn't matter what's in the database, what matters is what's the data that we can get when the customer is in the shop. So there's often constraints around the data that we can actually use. So we need to know, what am I trying to achieve? What can this organization actually do specifically to change that outcome? And at the point that that decision is being made, what data do they have or could they collect? And so then the way I put that all together is with a model. And this is not a model in the sense of a predictive model, but it's a model in the sense of a simulation model. So one of the main examples I give in this paper is one I spent many years building, which is if an insurance company changes their prices, how does that impact their profitability? And so generally your simulation model contains a number of predictive models. So I had for example a predictive model called an elasticity model that said for a specific customer, if we charge them a specific price for a specific product, what's the probability that they would say yes, both when it's new business and then a year later, what's the probability that they're new? And then there's another predictive model, which is what's the probability that they're going to make a claim and how much is that claim going to be? And so you can combine these models together then to say, all right, if we changed our pricing by reducing it by 10% for everybody between 18 and 25, and we can run it through these models that combine together into a simulation, then the overall impact on our market share in 10 years time is X and our cost is Y and our profit is Z and so forth. So in practice, most of the time you really are going to care more about the results of that simulation than you do about the predictive model directly. But most people are not doing this effectively at the moment. So for example, when I go to Amazon, I read all of Douglas Adams' books. And so having read all of Douglas Adams' books, the next time I went to Amazon, they said, would you like to buy the collected works of Douglas Adams? This is after I had bought every one of his books. So from a machine learning point of view, some data scientist had said, oh, people that buy one of Douglas Adams' books often go on to buy the collected works, but recommending to me that I buy the collected works of Douglas Adams isn't smart. And it's actually not smart at a number of levels. Not only is it unlikely I'm going to buy a box set of something of which I have everyone individually, but furthermore, it's not going to change my buying behavior. I already know about Douglas Adams, I already know I like him, so taking up your valuable web space to tell me, hey, maybe you should buy more of the author who you're already familiar with and have bought lots of times, isn't actually going to change my behavior. So what if instead of creating a predictive model, Amazon had built an optimization model that could simulate and said, if we show Jeremy this ad, how likely is he then to go on to buy this book? And if I don't show him this ad, how likely is he to go on to buy this book? And so that's the counterfactual, right? The counterfactual is what would have happened otherwise. And then you can take the difference and say, okay, what should we recommend him that is going to maximally change his behavior, so maximally result in more books. And so you'd probably say, oh, he's never bought me Terry Pratchett books, he probably doesn't know about Terry Pratchett, but lots of people that liked Douglas Adams did turn out to like Terry Pratchett, so let's introduce him to a new author. So it's a difference between a predictive model on the one hand versus an optimization model on the other hand. So the two tend to go hand in hand. The optimization model basically is saying, first of all you have a simulation model. The simulation model is saying, in a world where we put Terry Pratchett's book on the front page of Amazon for Jeremy Howard, this is what would have happened. He would have bought it with a 94% probability. And that then tells us with this lever of like, what do I put on my home page for Jeremy today, we say, okay, well the different settings of that lever that put Terry Pratchett on the home page has the highest simulated outcome. And then that's the thing which maximizes our profit from Jeremy's visit to Amazon.com today. So generally speaking, your predictive models kind of feed into this simulation model, but you've kind of got to think about how do they all work together. So for example, let's go back to Churn. So it turned out that Jeremy Howard is very likely to leave his cell phone company next month. What are we going to do about it? Oh, let's call him. And I can tell you, if my cell phone company calls me right now and says, just calling to say we love you, I'd be like, I'm canceling right now. That would be a terrible idea. So again, you'd want a simulation model that says, what's the probability that Jeremy is going to change his behavior as a result of calling him right now. One of the levers I have is call him. On the other hand, if I got a piece of mail tomorrow that said, for each month you stay with us, we're going to give you $100,000, okay, then that's going to definitely change my behavior. But then, feeding that into the simulation model, it turns out that overall that would be an unprofitable choice to make. So do you see how all this fits in together? So when we look at something like Churn, we want to be thinking, what are the levers we can pull, and so what are the kind of models that we could build with what kinds of data to help us pull those levers better to achieve our objectives. And so when you think about it that way, you realize that the vast majority of these applications are not largely about a predictive model at all, they're about interpretation. They're about understanding what happens if. So if we kind of take the cross-product, not the cross-product, sorry, the intersection between on the one hand, here are all the levers that we could pull, here are all the things we can do, and then here are all of the features from our random forest feature importance that turn out to be strong drivers of the outcome, and so then the intersection of those is here are the levers we could pull that actually matter. Because if you can't change the thing, then it's not very interesting, and if it's not actually a significant driver, it's not very interesting. So we can actually use our random forest feature importance to tell us what can we actually do to make a difference. And then we can use the partial dependence to actually build this kind of simulation model to say, okay, well if we did change that, what would happen? So there are examples, lots and lots of these vertical examples, and so what I want you to kind of think about as you think about the machine learning problems you're working on is like, why does somebody care about this, right? And like what would a good answer to them look like? And how could you actually positively impact this business? So if you're creating like a Kaggle kernel, try to think about from the point of view of the competition organizer, like what would they want to know, and how can you give them that information? So something like fraud detection, on the other hand, you probably just basically want to know who's fraudulent, right? So you probably do just care about the predictive model, but then you do have to think carefully about the data availability here. So it's like, okay, but we need to know who's fraudulent at the point that we're about to deliver them a product, right? So it's no point like looking at data that's available like a month later, for instance. So you've kind of got this key issue of thinking about the actual operational constraints that you're working under. You know, lots of interesting applications in human resources, but like employee churn, it's another kind of churn model where finding out that Jeremy Howard's sick of lecturing, he's going to leave tomorrow, what are you going to do about it? Well, knowing that wouldn't actually be helpful. It'd be too late, right? You would actually want a model that said, what kinds of people are leaving USF? And it turns out that like, oh, everybody that goes to the downstairs cafe leaves USF, you know, I guess their food is awful, or whatever, right? Or everybody that we're paying less than half a million dollars a year is leaving USF, you know, because they can't afford basic housing in San Francisco. So like you could use your employee churn model, not so much to say like which employees hide us, but why do employees leave? And so again, it's really the interpretation there that matters. Lead prioritization is a really interesting one, right? This is one where a lot of companies, yes, Dana, can you pass that over there? Yeah, exactly. So this is what this simulation model is all about. So it's a great question. So like you kind of figure out this objective we're trying to maximize, which is like company profitability, you can kind of create like a pretty simple like Excel model or something that says like here's the revenues and here's the costs and the cost is equal to the number of people we employ multiplied by their salaries, blah blah blah blah blah blah. And so inside that kind of Excel model, there are certain cells, there are certain inputs where you're like oh that thing's kind of stochastic, or that thing is kind of uncertain but we could predict it with a model. And so that's kind of what I do then, is I ask them, we need a predictive model for how likely somebody is to stay if we change their salary, how likely they are to leave with their current salary, how likely they are to leave next year if I increase their salary now blah blah blah. So you kind of build a bunch of these models and then you can combine them together with simple business logic and then you can optimize that. You can then say okay, if I pay Jeremy Howard half a million dollars, that's probably a really good idea, and if I pay him less, then it's probably not. You can figure out the overall impact. So it's really shocking to me how few people do this, like most people in industry measure their models using like AUC or RMSE or whatever, which is never actually what you want. Yes, can you pass it over here? I wanted to stress the point that you made before, in my experience, a lot of the problem was to define the problem, right? So you are in a company, you are talking to somebody that doesn't have like this mentality that you have. They don't know that you have to have X and Y and so on. So you have to try to get that out of them. What exactly do you want? And try to go through a few iterations of understanding what they want and then you know the data, you know where the data is, you know actually where you can measure, which is often know what they want. So you have to kind of get a proxy for what they want. And then so a lot of what you do is not that much of like, well, some people do actually just work on really good models for, you know, but a lot of people also just work on this kind of how do you put this as a, you know, classification regression or some other type of modeling. That's actually kind of the most interesting, I think. And also kind of what's kind of what you have to do well. The best people do both, the best people understand the technical model building deeply, but also understand the kind of strategic context deeply. And so this is one way to think about it. And as I say, I actually think, you know, there aren't many articles I wrote in 2012 I'm still recommending, but this one I think is still equally valid today. So yeah, so like another great example is lead prioritization, right? So like a lot of companies, like every one of these boxes I'm showing, you can generally find a company or many companies whose sole job in life is to build models of that thing, right? So lots of companies that sell lead prioritization systems. But again, like the question is, how would we use that information, right? So if it's like, oh, our best lead is Jeremy, you know, he's our highest probability of buying. Does that mean I should send a salesperson out to Jeremy or I shouldn't? Like if he's highly probable to buy, why waste my time with him? You know, so like again, it's like you really want some kind of simulation that says like, what's the change, the likely change in Jeremy's behavior if I send my best salesperson, Yanet, out to go and like encourage him to sign? So yeah, I think this is like there are many, many opportunities for data scientists in the world today to move beyond predictive modeling to actually bringing it all together, you know, with the kind of stuff that Dina was talking about in her question. So as well as these horizontal applications that basically apply to like every company, there's a whole bunch of applications that are specific to like every part of the world, right? So for those of you that end up in healthcare, some of you will become experts in one or more of these areas like readmission risk. So what's the probability that this patient is going to come back to the hospital? And readmission is, depending on the details of the jurisdiction and so forth, it can be a disaster for hospitals when somebody is readmitted, right? So if you find out that this patient has a high probability of readmission, what do you do about it? Well again, the predictive model is helpful of itself, right? It rather suggests like we just shouldn't send them home yet because they're going to come back. But wouldn't it be nice if we had the tree interpreter and it said to us the reason that they're at high risk is because we don't have a recent EKG for them and without a recent EKG we can't have a high confidence about their cardiac health. In which case it wouldn't be like, let's keep them in the hospital for two weeks, it'll be like let's give them an EKG. So this is interaction between interpretation and predictive accuracy. So correct me if I'm wrong, but what I'm understanding you saying is that the predictive models are a really great starting point, but in order to actually answer these questions, we really need to focus on the interpretability of these models. Yes, I think so, and more specifically I'm saying we just learned a whole raft of random forest interpretation techniques, so I'm trying to justify why. The reason why is because actually, I'd say most of the time, the interpretation is the thing we care about. You can create a chart or a table without machine learning, and indeed that's how most of the world works. Most managers build all kinds of tables and charts without any machine learning behind them. But they often make terrible decisions because they don't know the feature importance of the objective they're interested in, so the table they create is of things that actually are the least important things anyway, or they just do a univariate chart rather than a partial dependence plot so they don't actually realize that the relationship they thought they're looking at is due entirely to something else. So I'm kind of arguing for data scientists getting much more deeply involved in strategy and in trying to use machine learning to really help a business with all of its objectives. There are companies like Dunn-Humby, a huge company that does nothing but retail applications with machine learning. And so I believe there's a Dunn-Humby product you can buy which will help you figure out like if I put my new store in this location versus that location, how many people are going to shop there. Or if I put my diapers in this part of the shop versus that part of the shop, how's that going to impact purchasing behavior or whatever. So I think it's also good to realize that the subset of machine learning applications you tend to hear about in the tech press or whatever is this massively biased tiny subset of stuff which Google and Facebook do, whereas the vast majority of stuff that actually makes the world go round is these kinds of applications that actually help people make things, buy things, sell things, build things, so forth. So about tree interpretation, the way we looked at the tree was we manually checked which feature was more important for particular observation. But for businesses, they would have huge amount of data and they want this interpretation for a lot of observations. So how do they automate it? I don't think the automation is at all difficult. You can run any of these algorithms like looping through the rows or doing them in parallel. It's all just code. Am I misunderstanding your question? Is it like they set a threshold that if some feature is above, different people will have different behavior? That's a good question. This is a really important issue actually. The vast majority of machine learning models don't automate anything. They're designed to provide information to humans. So for example, if you're a point of sales customer service phone operator for an insurance company and your customer asks you why is my renewal $500 more expensive than last time, then hopefully the insurance company provides in your terminal a little screen that shows the result of the tree interpreter, so that you can jump there and tell the customer, okay, last year you were in this different zip code which has lower amounts of car theft and this year also you've actually changed your vehicle to a more expensive one or whatever. So it's not so much about thresholds of automation, but about making these model outputs available to the decision makers in an organization, whether they be at the top strategic level of like are we going to shut down this whole product or not, all the way to the operational level of that individual discussion with the customer. So another example is aircraft scheduling and gate management. There's lots of companies that do that. Basically what happens is that there are people at an airport whose job it is to basically tell each aircraft what gate to go to, to figure out when to close the doors, stuff like that. So the idea is you're giving them software which has the information they need to make good decisions. So the machine learning models end up embedded in that software to say, okay, that plane that's currently coming in from Miami, there's a 48% chance that it's going to be over 5 minutes late. If it does, then this is going to be the knock-on impact through the rest of the terminal. So that's kind of how these things tend to fit together. So there's so many of these. There's lots and lots. I don't expect you to remember all these applications, but what I do want you to do is to spend some time thinking about them. Sit down with one of your friends and talk about a few examples. Like okay, how would we go about doing failure analysis in manufacturing? Who would be doing that? Why would they be doing it? What kind of models might they use? What kind of data might they use? Start to practice this and get a sense. Because then as you're interviewing and then when you're at the workplace and you're talking to managers, you want to be straight away able to recognize that the person you're talking to, what are they trying to achieve, what are the levers that they have to pull, what is the data they have available to pull those levers to achieve that thing, and therefore how could we build models to help them do that, and what kind of predictions would they have to be making. And so then you can have this really thoughtful, empathetic conversation with those people and saying, in order to reduce the number of customers that are leaving, I guess you're trying to figure out who should you be providing better pricing to or whatever. So what I'm noticing from your beautiful little chart above is that a lot of this, to me at least, still seems like the primary purpose is at least base level, is predictive power. And so I guess my thing is, for explanatory problems, a lot of the ones that people are faced with in social sciences, is that something machine learning can be used for or is used for or is that not really the realm that it is? That's a great question. And I've had a lot of conversations about this with people in social sciences, and currently machine learning is not well applied in economics or psychology or whatever on the whole. But I'm convinced it can be, for the exact reasons we're talking about. So if you're trying to figure out, if you're trying to do some kind of behavioral economics and you're trying to understand why some people behave differently to other people, a random forest with a feature importance plot would be a great way to start. Or more interestingly, if you're trying to do some kind of sociology experiment or analysis based on a large social network data set where you have an observational study, you really want to try and pull out all of the sources of kind of exogenous variables, all the stuff that's going on outside. And so if you use a partial dependence plot with a random forest, that happens automatically. So I actually gave a talk at MIT a couple of years ago for the first conference on digital experimentation which was really talking about how do we experiment in things like social networks and these digital environments. Economists all do things with classic statistical tests. But anyway, in this case, the economists I talked to were absolutely fascinated by this and they actually asked me to give an introduction to machine learning session at MIT to these various faculty and graduate folks in the economics department. And some of those folks have gone on to write some pretty famous books and stuff and so hopefully it's been useful. It's definitely early days, but it's a big, big opportunity. But as Yannett says, there's plenty of skepticism still out there. Well the skepticism comes from unfamiliarity basically with this totally different approach. So like if you spent 20 years studying econometrics and somebody comes along and says, here's a totally different approach to all the stuff that econometricians do, naturally your first reaction will be like, prove it. So that's fair enough. But I think over time, the next generation of people who are growing up with machine learning, some of them will move into the social sciences, they'll make huge impacts that nobody's ever managed to make before, and people will start going, wow. Just like what happened in computer vision. When computer vision spent a long time of people saying, hey maybe you should use deep learning for computer vision, and everybody in computer vision is like, prove it. We have decades of work on amazing feature detectors for computer vision. And then finally in 2012 Hinton and Kredesky came along and said, okay, our model is like twice as good as yours, and we've only just started on this, and everybody was like, oh okay, that's pretty convincing. Nowadays every computer vision researcher basically uses deep learning. So I think that time will come in this area too. I think what we might do then is take a break and we're going to come back and talk about these random forest interpretation techniques and do a bit of a review. So let's come back at 2 o'clock. So let's have a go at talking about these different random forest interpretation methods, having talked about why they're important. So let's now remind ourselves what they are. So I'm going to let you folks have a go. So let's start with confidence based on tree variance. So can one of you tell me one or more of the following things about confidence based on tree variance? What does it tell us? Why would we be interested in that? And how is it calculated? This is going back a ways, because it was the first one we looked at. Even if you're not sure, you only know a little piece of it, give us your piece and we'll build on it together. I think I got a piece of it. It's getting the variance of our predictions from random forests. That's true, that's the how. Can you be more specific? So what is it the variance of? I think it's, if I'm remembering correctly, I think it's just the overall prediction. The variance of the predictions of the trees. So normally the prediction is just the average, this is the variance of the trees. So it kind of just gives you an idea of how much your prediction is going to vary. So maybe you want to minimize variance, maybe that's your goal for whatever reason that could be. That's not so much the reason. So I like your calculation description. Let's see if somebody else can tell us how you might use that. So I remember that we talked about the independence of the trees. So maybe something about if the variance of the trees is higher or lower than, you know. No not so much that. That's an interesting question, but it's not what we're going to see here. Do you want to pass it back behind you? So to remind you, just to fill in a detail here, what we generally do here is we take just one row, like one observation often, and find out how confident we are about that, like how much variance there are in the trees for that. Or we can do it as we did here for different groups. So according to me, the idea is like for each row, we calculate the standard deviation that we get from the random forest model, and then maybe group according to different variables or predictors and see for which particular predictor the standard deviation is high. Then go deep down as why it is happening. Maybe it is because a particular category of that variable has very less number of observations. Yeah, that's great. So that would be one approach. What we've done here is to say, is there any groups where we're very unconfident? Something that I think is even more important would be when you're using this like operationally, let's say you're doing a credit decisioning algorithm. So we're trying to say like, okay, is Jeremy a good risk or a bad risk? Should we loan him a million dollars? And the random forest says, I think he's a good risk, but I'm not at all confident. In which case we might say, okay, maybe I shouldn't give him a million dollars. Or else if the random forest said, I think he's a good risk, I am very sure of that, then we're much more comfortable giving him a million dollars. And I'm a very good risk, so feel free to give me a million dollars. I checked the random forest before, different notebook, not in the repo. So like this is like, it's quite hard for me to give you folks direct experience with this kind of like, single observation, interpretation stuff, because it's really like the kind of stuff that you actually need to be putting out to the front line. Do you know what I mean? Like it's not something which you can really use so much in a kind of Kaggle context, but it's more like, okay, if you're actually putting out some algorithm which is making like big decisions that could cost a lot of money, you probably don't so much care about the average prediction of the random forest, but maybe you actually care about like the average minus a couple of standard deviations. You know, like what's the kind of worst case prediction? And so as Chika mentioned, it's like maybe there's a whole group that we're kind of unconfident about. So that's confidence based on tree variance. Who wants to have a go at answering feature importance? What is it? Why is it interesting? How do we calculate it? Or any subset thereof? Dina? I think it's like, it's basically to find out which features are important for your model. So you take each feature and you like randomly sample all the values in the feature and you see how the predictions are. If it's very different, it means that that feature was actually important. Else, if it's fine to take any random values for that feature, it means that maybe probably it's not very important. Okay. That was terrific. That was all exactly right. There was some details that maybe were skimmed over a little bit. I wonder if anybody else wants to jump into like a more detailed description of how it's calculated because I know this morning some people were not quite sure. Is there anybody who's like not quite sure maybe who wants to like have a go or want to just put it next to you there? Let's see. How exactly do we calculate feature importance for a particular feature? I think after you're done building the random forest model, you take each column and randomly shuffle it and generate a prediction and check the validation score. If it gets pretty bad for after shuffling one of the columns, that means that column was important. So that has higher importance. I'm not exactly sure how we quantify the feature importance. Okay great. Dina, do you know how we quantify the feature importance? That was a great description. I think we take the difference in the first place. Or score of some sort. Exactly. Yeah, so let's say we've got our dependent variable which is price, right, and there's a bunch of independent variables including year made, right, and so we basically we use the whole lot to build a random forest, right, and then that gives us our predictions, right, and so then we can compare that to get R squared, RMSE, whatever you're interested in from the model. Now the key thing here is I don't want to have to retrain my whole random forest. That's kind of slow and boring, right, so using the existing random forest, how can I figure out how important year made was, right, and so the suggestion was let's randomly shuffle the whole column, right. So now that column is totally useless. It's got the same mean, same distribution, everything about it is the same, but there's no connection at all between particular people, actual year made, and what's now in that column. I've randomly shuffled it. And so now I put that new version through the same random forest, so there's no retraining done, to get some new y hat, I call it y hat, ym, right, and then I can compare that to my actuals to get like an RMSE, ym, right, and so now I can start to create a little table. So now I can create a little table where I've basically got like the original here, RMSE, and then I've got with year made scrambled. So this one had an RMSE of like 3, this one had an RMSE of like 2, enclosure, you know, scrambling that had an RMSE of like 2.5, right. And so then I just take these differences. So I'd say year made, the importance is 1, 3-2, enclosure is 0.5, 3-2.5, and so forth, right. So how much worse did my model get after I shuffled that variable? Does anybody have any questions about that? Can you pass that to Danielle please? I assume you just chose those numbers randomly, but my question I guess is, does it, do all of them theoretically have a perfect model to start out with? Like are they, will all the importances sum to 1, or is that not? No. They're just... Honestly, I've never actually looked at what the units are, so I'm actually not quite sure. Sorry. We can check it out during the week. If somebody's interested, have a look at this SKLearn code and see exactly what those units of measure are, because I've never bothered to check. Although I don't check the units of measure specifically, what I do check is the relative importance. And so like here's an example. So rather than just saying like what are the top 10, yesterday one of the practicum students asked me about a feature importance where they said like I think these 3 are important. And I pointed out that the top one was 1000 times more important than the second one. So like look at the relative numbers here. And so in that case it's like no, don't look at the top 3, look at the one that's 1000 times more important and ignore all the rest. And so this is where sometimes the kind of, your natural tendency to want to be like precise and careful, you need to override that and be very practical. It's like okay this thing's 1000 times more important, don't spend any time on anything else. So then you can go and talk to the manager of your project and say like okay this thing's 1000 times more important, and then they might say oh, that was a mistake, it shouldn't have been in there, we don't actually have that information at the decision time. Or for whatever reason we can't actually use that variable, so then you could remove it and have a look. Or they might say gosh, I had no idea that that was by far more important than everything else put together, so let's forget this random Boris thing and just focus on understanding how we can better collect that one variable and better use that one variable. So that's like something which comes up quite a lot. And actually another place that came up just yesterday, again, another practicum student asked me, hey I'm doing this medical diagnostics project and my r squared is.95 for a disease which I was told is very hard to diagnose. You know, is this random forest a genius or is something going wrong? And I said like remember the second thing you do after you build a random forest is to do feature importance. So do feature importance and what you'll probably find is that the top column is something that shouldn't be there. And so that's what happened. He came back to me half an hour later, he said yeah I did the feature importance, you were right, the top column was basically something that was another encoding of the dependent variable, I've removed it, and now my r squared is negative 0.1. So that's an improvement. Okay, the other thing I like to look at is this chart, is to basically say where do things kind of flatten off in terms of which ones should I be really focusing on. So that's the most important one. And so when I did credit scoring in telecommunications, I found there were 9 variables that basically predicted very accurately who was going to end up paying for their phone and who wasn't. And apart from ending up with a model that saved them $3 billion a year in fraud and credit costs, it also let them basically rejig their process so they focused on collecting those 9 variables much better. Alright who wants to do partial dependence? This is an interesting one, very important, but in some ways kind of tricky to think about it. Go ahead and try. Yeah, please do. So from my understanding of what partial dependence is, is that there's not always necessarily like a relationship between the strictly the dependent variable and this independent variable that necessarily like is showing importance, but rather than interaction between two variables that are working together. Something like this, right? Yeah. Where we're like, oh, that's weird, like, you'd expect this to be kind of flat and there's a weird pokey bit. Yeah, and so for this example what we found was that it's not necessarily year made or when the sale was elapsed, but it's actually the age of the model and so it's that is easier to just like to tell a like company well obviously your younger models are going to sell for more and it's less about when the year was made. Yeah, exactly. So let's come back to how we calculate this in a moment. But the first thing to realize is that the vast majority of the time, you know, post your course here when somebody shows you a chart, it'll be like a univariate chart. That'll just like grab the data from the database and they'll plot X against Y and then managers have a tendency to want to like make a decision. So be like, oh, there's this like drop-off here so we should like stop dealing in equipment made between 1990 and 1995 or whatever. And this is like a big problem because like real world data has lots of these interactions going on. So like, you know, maybe there was a recession going on around the time that those things are being sold or maybe around that time people were buying more of a different type of equipment or whatever. Right. So generally what we actually want to know is all other things being equal. What's the relationship between year made and sale price? Right, because like if you think about the drivetrain approach idea of like the levers, you really want to model that says if I change this lever, how will it change my objective? And so it's by pulling them apart using partial dependence that you can say, okay, actually this is the relationship between year made and sale price, all other things being equal. So how do we calculate that? For the variable year made, for example, you're going to train, you keep every other variable constants and then you're going to pass every single value of the year made and then train the model after that. So for every model, you're going to have the light blue for the values of it and the median is going to be the yellow line. Good. Okay. So let's try and draw that. So by leave everything else constant, what she means is leave them at whatever they are in the dataset. So just like when we did feature importance, we're going to leave the rest of the dataset as it is and we're going to do partial dependence plot for year made. So we've got all of these other rows of data that we'll just leave as they are. And so instead of randomly shuffling year made, instead what we're going to do is replace every single value with exactly the same thing. 1960. Okay. And just like before, we now pass that through our existing random forest, which we have not retrained or changed in any way to get back out a set of predictions. Why 1960? And so then we can plot that on a chart. Year made against partial dependence, 1960 year. Okay. Then we can do it for 1961, 2, 3, 4, 5, and so forth. And so we can do that for on average for all of them, or we could do it just for one of them. And so when we do it for just one of them and we change its year made and pass that single thing through our model, that gives us one of these blue lines. So each one of these blue lines is a single row as we change its year made from 1960 up to 2008. And so then we can just take the median of all of those blue lines to say, on average, what's the relationship between year made and price, all other things being equal. So why is it that this works? Why is it that this process tells us the relationship between year made and price, all other things being equal? Well, maybe it's good to think about a really simplified approach. A really simplified approach would say, what's the average auction? What's the average sale date? What's the most common type of machine we sell? Which location do we mostly sell things? We could come up with a single row that represents the average auction, and then we could say, let's run that row through the random forest, replace its year made with 1960, and then do it again with 1961, and then do it again with 1962. And we could plot those on our little chart. And that would give us a version of the relationship between year made and sale price, all other things being equal. But what if tractors looked like that, and backhoe loaders looked like that? Then taking the average one would hide the fact that there are these totally different relationships. So instead, we basically say, our data tells us what kinds of things we tend to sell, and who we tend to sell them to, and when we tend to sell them, so let's use that. So then we actually find out for every blue line, here are actual examples of these relationships. And so then what we can do is, as well as plotting the median, we can do a cluster analysis to find out a few different shapes. And so we may find, in this case they all look like pretty much different versions of the same thing with different slopes. So my main takeaway from this would be that the relationship between sale price and year is basically a straight line. And remember, this was log of sale price, so this is actually showing us an exponential. And so this is where I would then bring in the domain expertise, which is like, okay, things depreciate over time by a constant ratio, so therefore I would expect older stuff year made to have this exponential shape. So this is where, I kind of mentioned the very start of my machine learning project, I generally try to avoid using as much domain expertise as I can and let the data do the talking. So one of the questions I got this morning was, if there's a sale ID, a model ID, I should throw those away, because they're just IDs. No, don't assume anything about your data. Leave them in and if they turn out to be super important predictors, you want to find out why is that. But then, now I'm at the other end of my project. I've done my feature importance, I've pulled out the stuff which is like from that dendrogram, the redundant features, I'm looking at the partial dependence, and now I'm thinking, okay, is this shape what I expected? So even better, before you plot this, first of all think, what shape would I expect this to be? Because it's always easy to justify to yourself after the fact, oh I knew it would look like this. So what shape do you expect, and then is it that shape? So in this case I'd be like, yeah, this is what I would expect. Where else? This is definitely not what I'd expect. So the partial dependence plot has really pulled out the underlying truth. Does anybody have any questions about why we use partial dependence or how we calculate it? Who's got the, oh you've got it. If there are 20 features that are important, then I will do the partial dependence for all of them. Where important means like, it's a lever I can actually pull, it's like the magnitude of its size is not much smaller than the other 19. Based on all of these things, it's like yeah, it's a feature I ought to care about, then I will want to know how it's related. It's pretty unusual to have that many features that are important both operationally and from a modeling point of view in my experience. So important means it's a lever, so it's something I can change, and it's like kind of at the spiky end of this tale. Or maybe it's not a lever directly, maybe it's like zip code, and I can't actually tell my customers where to live, but I could focus my new marketing attention on a different zip code. Would it make sense to do pairwise shuffling for every combination of two features and hold everything else constant like in feature importance to see interactions and compare scores? So you wouldn't do that so much for partial dependence. I think your question is really getting to the question of could we do that for feature importance. So I think interaction feature importance is a very important and interesting question. But doing it by randomly shuffling every pair of columns, if you've got 100 columns, sounds computationally intensive, possibly infeasible. So what I'm going to do is after we talk about tree interpreter, I'll talk about an interesting but largely unexplored approach that will probably work. Who wants to do tree interpreter? Alright over here, Prince. Can you pass that over here to Prince? I was thinking this to be more like feature importance, but feature importance is for complete random forest model and this tree interpreter is for feature importance for particular observation. So if that, let's say it's about hospital readmission. So if a patient A1 is going to be readmitted to a hospital, which feature for that particular patient is going to impact and how can we change that? And it is calculated starting from the prediction of mean, then seeing how each feature is changing the behavior of that particular patient. I'm smiling because that was one of the best examples of technical communication I've heard in a long time. So it's really good to think about why was that effective. So what Prince did there was he used as specific an example as possible. So humans are much less good at understanding abstractions. So if you kind of say, oh it takes some kind of feature and then there's an observation in that feature, it's a hospital readmission. So we take a specific example. The other thing he did which was very effective was to kind of take an analogy to something we already understand. So we already understand the idea of feature importance across all of the rows in the data set. So now we're going to do it for a single row. So one of the things I was really hoping we would learn from this experience is how to become effective technical communicators. So that was a really great role model from Prince of using all of the tricks we have at our disposal for effective technical communication. So hopefully you found that a useful explanation. I don't have a hell of a lot to add to that other than to show you what that looks like. So with the tree interpreter, we picked out a row. And so remember when we talked about the confidence intervals at the very start, the confidence based on tree variance, we mainly said you'd probably mainly use that for a row. So this would also be for a row. So it's like, okay, why is this patient likely to be readmitted? So here is all of the information we have about that patient, or in this case this auction. Why is this auction so expensive? So then we call treeinterpreter.predict and we get back the prediction of the price, the bias, which is the root of the tree. So this is just the average price for everybody. So this is always going to be the same. And then the contributions, which is how important is each of these things. And so the way we calculated that was to say, okay, at the very start the average price was 10, and then we split on enclosure. And for those with disenclosure, the average was 9.5. And then we split on year made, less than 1990, and for those with that year made, the average price was 9.7. And then we split on the number of hours on the meter, and for this branch we got 9.4. And so we then have a particular auction, which we pass it through the tree and it just so happens that it takes this path. So one row can only have one path through the tree. And so we ended up at this point. So then we can create a little table. And so as we go through, we start at the top and we start with 10, that's our bias, and we said enclosure resulted in a change from 10 to 9.5, minus 0.5. Year made changed it from 9.5 to 9.7, so plus 0.2. And then meter changed it from 9.7 down to 9.4, which is minus 0.3. And then if we add all that together, 10 minus 1 half is 9.5, plus 0.2 is 9.7, minus 0.3 is 9.4, and lo and behold that's that number, which takes us to our Excel spreadsheet. Where's Chris, who did our waterfall? There you are. So last week we had to use Excel for this because there isn't a good Python library for doing waterfall charts. And so we saw we got our starting point, this is the bias, and then we had each of our contributions and we ended up with our total. The world is now a better place because Chris has created a Python waterfall chart module for us and put it on PIC, so never again will we have to use Excel for this. And I wanted to point out that waterfall charts have been very important in business communications at least as long as I've been in business, so that's about 25 years. Python is a couple of decades old, a little bit less, maybe a couple of decades old. But despite that, no one in the Python world ever got to the point where they actually thought you know I'm going to make a waterfall chart. So they didn't exist until two days ago, which is to say like the world is full of stuff which ought to exist and doesn't, and doesn't necessarily take a lot of time to build. Chris, how long did it take you to build the first Python waterfall chart? Well there was a you know a gist of it, yeah. About 8 hours, okay, so you know a hefty time amount but not unreasonable. And now forevermore, people when they want the Python waterfall chart will end up at Chris's GitHub repo and hopefully find lots of other USF contributors who have made it even better. So in order for you to help improve Chris's Python waterfall, you need to know how to do that. And so you're going to need to submit a pull request. Life becomes very easy for submitting pull requests if you use something called hub. So if you go to github.hub, that will send you over here. And what they suggest you do is that you alias git to hub, because it turns out that hub actually is a strict superset of git. But what it lets you do is you can go git fork, git push, git pull request, and you've now sent Chris a pull request. Without hub, this is actually a pain and requires like going to the website and filling in forms and stuff. So this gives you no reason not to do pull requests. And I mention this because like when you're interviewing for a job or whatever, I can promise you that the person you're talking to will check your GitHub. And if they see you have a history of submitting thoughtful pull requests that are accepted to interesting libraries, that looks great. It looks great because it shows you're somebody who actually contributes. It also shows that if they're being accepted, that you know how to create code that fits with people's coding standards, has appropriate documentation, passes their tests and coverage and so forth. So when people look at you and they say, oh, here's somebody with a history of successfully contributing accepted pull requests to open source libraries, that's a great part of your portfolio. And you can specifically refer to it. So either I'm the person who built Python waterfall, here is my repo. Or I'm the person who contributed currency number formatting to Python waterfall, here's my pull request. Anytime you see something that doesn't work right in any open source software you use is not a problem. It's a great opportunity because you can fix it and send in the pull request. So give it a go. It actually feels great the first time you have a pull request accepted. And of course, one big opportunity is the FastAI library. Thanks to one of our students, we now have docstrings for most of the FastAI.structured library and that again came via a pull request. Does anybody have any questions about how to calculate any of these random forest interpretation methods or why we might want to use any of these random forest interpretation methods? Towards the end of the week, you're going to need to be able to build all of these yourself from scratch. Just looking at the tree interpreter, I noticed that some of the values are NANs. How can an NAN have a feature importance? Okay, let me pass it back to you. Why not? So in other words, how is NAN handled in pandas and therefore in the tree? Set to some default value? Anybody remember how pandas, notice these are all in categorical variables. How does pandas handle NANs in categorical variables and how does FastAI deal with them? Can somebody pass it to the person who's talking? Negative one for pandas? Yeah, pandas sets them to negative one category code. And do you have to remember what we then do? Doesn't matter really, we add one to all of the category codes so it ends up being zero. So in other words, we have a category with, remember by the time it hits the random forest it's just a number and it's just a number zero. And we map it back to the descriptions back here. So the question really is why shouldn't the random forest be able to split on zero? It's just another number. So it could be NAN, high, medium or low, 0, 1, 2, 3, 4. And so missing values are one of these things that are generally taught really badly. Often people get taught, here are some ways to remove columns with missing values or remove rows with missing values or to replace missing values. That's never what we want because missingness is very, very, very often interesting. And so we actually learned from our feature importance that coupler system NAN is like one of the most important features. And so for some reason, well I could guess, coupler system NAN presumably means this is the kind of industrial equipment that doesn't have a coupler system. Now I don't know what kind that is, but apparently it's a more expensive kind. Does that make sense? So I did this competition for university grant research success where by far the most important predictors were whether or not some of the fields were null. And it turned out that this was data leakage, that these fields only got filled in most of the time after a research grant was accepted. So it allowed me to win that Kaggle competition, but didn't actually help the university very much. So let's talk about extrapolation. And I am going to do something risky and dangerous, which is we're going to do some live coding. And the reason we're going to do some live coding is I want to explore extrapolation together with you, and I kind of also want to help give you a feel of how you might go about writing code quickly in this notebook environment. And this is the kind of stuff that you're going to need to be able to do in the real world and in the exam, is kind of quickly create the kind of code that we're going to talk about. So I really like creating synthetic datasets. Anytime I'm trying to investigate the behavior of something, because if I have a synthetic dataset, I know how it should behave. Which reminds me, before we do this, I promised that we would talk about interaction importance, and I just about forgot. Tree interpreter tells us the contributions for a particular row based on the difference in the tree. We could calculate that for every row in our dataset and add them up, and that would tell us feature importance. It would tell us feature importance in a different way. One way of doing feature importance is by shuffling the columns one at a time. Another way is by doing tree interpreter for every row and adding them up. Neither is more right than the others. They're actually both quite widely used. So this is kind of type 1 and type 2 feature importance. So we could try to expand this a little bit to do not just single variable feature importance, but interaction feature importance. Now here's the thing. What I'm going to describe is very easy to describe. It was described by Breiman right back when random forests were first invented, and it is part of the commercial software product from Salford Systems who have the trademark on random forests. But it is not part of any open source library I'm aware of, and I've never seen an academic paper that actually studies it closely. So what I'm going to describe here is a huge opportunity, but it's also like there's lots and lots of details that kind of need to be fleshed out. But here's the basic idea. This particular difference here is not just because of year made, but because of a combination of year made and enclosure. The fact that this is 9.7 is because enclosure was in this branch and year made was in this branch. So in other words we could say the contribution of enclosure interacted with year made is minus 0.3. And so what about that difference? Well that's an interaction of year made and hours on the meter. So year made interacted with, I'm using star here not to mean times, but to mean interacted with. It's kind of a common way of doing things, like ours formulas do it this way as well. Year made interacted with meter has a contribution of minus 0.1. Perhaps we could also say from here to here that this also shows an interaction between meter and enclosure, like with one thing in between them. So maybe we could say meter by enclosure equals, and then what should it be? Should it be minus 0.6? I mean in some ways that kind of seems unfair because we're also including the impact of year made. So maybe it should be minus 0.6, maybe we should add back this 0.2. And these are like details that I actually don't know the answer to. How should we best assign a contribution to each pair of variables in this path? But clearly, conceptually we can. The pairs of variables in that path all represent interactions. I'm not going to say it's the wrong approach. I don't think it's the right approach though because it feels like this path here, meter and enclosure are interacting. So it seems like not recognizing that contribution is throwing away information. But I'm not sure. I had one of my staff at CADL actually do some R&D on this a few years ago, and I wasn't close enough to know how they dealt with these details, but they got it working pretty well. But unfortunately it never saw the light of day as a software product. This is something which maybe a group of you could get together and build. Do some Googling to check, but I really don't think that there are any interaction feature importance parts of any open source library. Wouldn't this exclude interactions though between variables that don't matter until they interact? So say your row never chooses to split down that path, but that variable interacting with another one becomes your most important split. I don't think that happens, right, because if there's an interaction that's important only because it's an interaction and not on a univariate basis, it will appear sometimes, assuming that you set max features to less than 1. And so therefore it will appear in some paths. What is meant by interaction? Is it multiplication, ratio, addition? Interaction means branches appears on the same path through a tree. Like an interaction, in this case the tree, there's an interaction between enclosure and year made, because we branch on enclosure and then we branch on year made. So to get to here, we have to have some specific value of enclosure and some specific value of year made. What if you went down the middle leafs between the two things you were trying to observe and you could just sort of norm, and you would also take into account what the final measure is. If you extend the tree downwards, you'd have many measures, both of the two things you're trying to look at and also the in-between steps. There seems to be a way to average information out in between them? There could be. So I think what we should do is talk about this on the forum. I think this is fascinating and I hope we build something. Great. I need to do my live coding. So that was a great discussion. Big thinking about it. Yeah, these are experiments. And so to experiment with that, you almost certainly want to create a synthetic dataset first. It's like y equals x1 plus x2 plus x1 times x2 or something. Like something where you know there's this interaction effect and there isn't that interaction effect and you want to make sure that the feature importance you get at the end is what you expected. And so probably the first step would be to do single variable feature importance using the tree interpreter style approach. And one nice thing about this is it doesn't really matter how much data you have, all you have to do to calculate feature importance is just slide through the tree. So you should be able to write in a way that's actually pretty fast. And so even writing it in pure Python might be fast enough, depending on your tree size. So we're going to talk about extrapolation. And so the first thing I want to do is create a synthetic dataset that has a simple linear relationship. We're going to pretend it's like a time series. So we need to basically create some x values. So the easiest way to create some synthetic data of this type is to use linspace, which just creates some evenly spaced data between start and stop, by default 50 observations. So if we just do that, there it is. And so then we're going to create a dependent variable. And so let's assume there's just a linear relationship between x and y, and let's add a little bit of randomness to it. So uniform random between low and high, so we could add somewhere between like minus 0.2 and 0.2. And so the next thing we need is a shape, which is basically what dimensions do you want these random numbers to be. And obviously we want them to be the same shape as x's shape. So we can just say x.shape. So in other words, that's x.shape. Remember when you see something in parentheses with a comma, that's a tuple with just one thing in it. So this is of shape 50, and so we've added 50 random numbers, and so now we could plot those. So shift-tab, x, y. So there's our data. So for when you're both working as a data scientist or for doing your exams in this course, you need to be able to quickly whip up a dataset like that, throw it up in a plot without thinking too much. And as you can see, you don't have to really remember much, if anything, you just have to know how to hit shift-tab to check the names of the parameters. And everything in the exam will be open, bulk open internet, so you can always google for something to try and find linspace if you've got what it's called. So let's assume that's our data. And so we're now going to build a random forest model, and what I want to do is build a random forest model that kind of acts as if this is a time series. So I'm going to take this as a training set, I'm going to take this as our validation or test set, just like we did in groceries or bulldozers or whatever. So we can use exactly the same kind of code that we used in split-vowels. So we can basically say X train, X vowel equals X up to 40, X from 40. So that just splits it into the first 40 versus the last 10. And so we can do the same thing for Y. So the next thing to do is we want to create a random forest and fit it. And that's going to require Xs and Ys. Now that's actually going to give an error, and the reason why is that it expects X to be a matrix, not a vector, because it expects X to have a number of columns of data. So it's important to know that a matrix with one column is not the same thing as a vector. So if I try to run this, expected 2D array got 1D array instead. So we need to convert our 2D array into a 1D array. So remember I said X.shape is 50, right? So X has one axis. So here's the important thing to make sure, X's rank is 1. The rank of a variable is equal to the length of its shape. How many axes does it have? So a vector we can think of as an array of rank 1. A matrix is an array of rank 2. I very rarely use words like vector and matrix because they're kind of meaningless, specific examples of something more general, which is they're all n-dimensional tensors or n-dimensional arrays. So an n-dimensional array, we can say it's a tensor of rank n. They basically mean kind of the same thing. N-terms get crazy when you say that because to a physicist, a tensor has quite a specific meaning, but in machine learning we generally use it in the same way. So how do we turn a one-dimensional array into a two-dimensional array? There's a couple of ways we can do it, but basically we slice it. So colon means give me everything in that axis. Colon comma none means give me everything in the first axis, which is the only axis we have. And then none is a special indexer, which means add a unit axis here. So let me show you. That is of shape 50,1. So it's of rank 2. It has two axes. One of them is a very boring axis, right? It's a length 1 axis. So let's move this over here. There's 1,50. And then to remind you, the original is just 50. So you can see I can put none as a special indexer to introduce a new unit axis there. So this thing has 1 row and 50 columns. This thing has 50 rows and 1 column. So that's what we want, right? We want 50 rows and 1 column. This kind of playing around with ranks and dimensions is going to become increasingly important in this course and in the deep learning course. So spend a lot of time slicing with none, slicing with other things, try to create three-dimensional, four-dimensional tensors and so forth. I'll show you two tricks. The first is you never ever need to write comma colon. It's always assumed. So if I delete that, this is exactly the same thing. And you'll see that in code all the time, so you need to recognize it. The second trick is this is adding an axis in the second dimension, or I guess the index 1 dimension. What if I always want to put it in the last dimension? And often our tensors change dimensions without us looking, because you went from a 1-channel image to a 3-channel image, or you went from a single image to a mini batch of images. Like suddenly you get new dimensions appearing. So to make things general, I would say this, dot dot dot. Dot dot dot means as many dimensions as you need to fill this up. And so in this case, it's exactly the same thing, but I would always try to write it that way, because it means it's going to continue to work as I get higher dimensional tensors. So in this case, I want 50 rows in one column, so I'll call that x1. So let's now use that here. And so this is now a 2D array, and so I can create my random forest. So then I could plot that, and this is where you're going to have to turn your brains on, because the folks this morning got this very quickly, which was super impressive. I'm going to plot y train against m.predict x train. Before I hit go, what is this going to look like? It should basically be the same. Our predictions hopefully are the same as the actuals. So this should fall on the line. But there's some randomness, so it won't quite. So that's cool. That was the easy one. Let's now do the hard one, the fun one. What's that going to look like? So I'm going to say no, but nice try. It's like, hey, we're extrapolating to the validation. That's what I'd like it to look like, but that's not what it is going to look like. Think about what trees do, and think about the fact that we have a validation set here and a training set here. So think about a forest is just a bunch of trees. The first tree is going to have a go. Can you pass that to Melissa? Will it start grouping the dots? Yeah, that's what it does. But let's think about how it groups the dots. I'm guessing since all the new data is actually outside of the original scope, it's all going to be basically the same. It's like one huge group. Forget the forest, let's create one tree. So we're probably going to split somewhere around here first, and then we're going to probably split somewhere around here, and then we're going to split somewhere around here and somewhere around here. And so our final split is here. So our prediction, when we say, okay, let's take this one, and so it's going to put that through the forest and end up predicting this average. It can't predict anything higher than that, because there is nothing higher than that to average. So this is really important to realize. A random forest is not magic. It's just returning the average of nearby observations where nearby is kind of in this tree space. So let's run it. Let's see if Tim's right. Holy shit, that's awful. And if you don't know how random forests work, then this is going to totally screw. If you think that it's actually going to be able to extrapolate to any kind of data it hasn't seen before, particularly future time periods, it's just not. It just can't. It's just averaging stuff it's already seen. That's all it can do. So we're going to be talking about how to avoid this problem. We talked a little bit in the last lesson about trying to avoid it by just avoiding unnecessary time-dependent variables where we can. But in the end, if you really have a time series that looks like this, we actually have to deal with the problem. So one way we could deal with the problem would be use like a neural net. Use something that actually has a function or shape that can actually fit something like this. So then it will extrapolate nicely. Another approach would be to use all the time series techniques you guys are learning about in the morning class to fit some kind of time series and then detrend it. And so then you'll end up with detrended dots and then use the random forest to predict those. And that's particularly cool, because if you're, imagine that your random forest was actually trying to predict data that, I don't know, maybe it was two different states. And so the blue ones are down here and the red ones are up here. Now if you try to use a random forest, it's going to do a pretty crappy job because time is going to seem much more important. So it's basically still going to split like this, and then it's going to split like this. And then finally, once it kind of gets down to this piece, it'll be like, oh, now I can see the difference between the states. So in other words, when you've got this big time piece going on, you're not going to see the other relationships in the random forest until every tree deals with time. So one way to fix this would be with a gradient boosting machine, GBM. And what a GBM does is it creates a little tree and runs everything through that first little tree, which could be like the time tree, and then it calculates the residuals, and then the next little tree just predicts the residuals. So it would be kind of like detrending it. So GBMs handle this. GBMs still can't extrapolate to the future, but at least they can deal with time-dependent data more conveniently. So we're going to be talking about this quite a lot more over the next couple of weeks. And in the end, the solution is going to be just use neural nets. But for now, using some kind of time-series analysis, detrend it, and then use a random forest on that isn't a bad technique at all. And if you're playing around with something like the Ecuador Groceries Competition, that would be a really good thing to fiddle around with. All right, see you next time.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 13.42, "text": " So, we've looked at a lot of different random forest interpretation techniques.", "tokens": [407, 11, 321, 600, 2956, 412, 257, 688, 295, 819, 4974, 6719, 14174, 7512, 13], "temperature": 0.0, "avg_logprob": -0.2220252784522804, "compression_ratio": 1.4684210526315788, "no_speech_prob": 0.0007672314532101154}, {"id": 1, "seek": 0, "start": 13.42, "end": 21.42, "text": " A question that's come up a little bit on the forums is like, what are these for, really?", "tokens": [316, 1168, 300, 311, 808, 493, 257, 707, 857, 322, 264, 26998, 307, 411, 11, 437, 366, 613, 337, 11, 534, 30], "temperature": 0.0, "avg_logprob": -0.2220252784522804, "compression_ratio": 1.4684210526315788, "no_speech_prob": 0.0007672314532101154}, {"id": 2, "seek": 0, "start": 21.42, "end": 24.82, "text": " How do these help me get a better score on Kaggle?", "tokens": [1012, 360, 613, 854, 385, 483, 257, 1101, 6175, 322, 48751, 22631, 30], "temperature": 0.0, "avg_logprob": -0.2220252784522804, "compression_ratio": 1.4684210526315788, "no_speech_prob": 0.0007672314532101154}, {"id": 3, "seek": 0, "start": 24.82, "end": 29.46, "text": " And my answer's kind of been like, they don't necessarily.", "tokens": [400, 452, 1867, 311, 733, 295, 668, 411, 11, 436, 500, 380, 4725, 13], "temperature": 0.0, "avg_logprob": -0.2220252784522804, "compression_ratio": 1.4684210526315788, "no_speech_prob": 0.0007672314532101154}, {"id": 4, "seek": 2946, "start": 29.46, "end": 36.36, "text": " So I want to talk more about why do we do machine learning?", "tokens": [407, 286, 528, 281, 751, 544, 466, 983, 360, 321, 360, 3479, 2539, 30], "temperature": 0.0, "avg_logprob": -0.14832210540771484, "compression_ratio": 1.4126984126984128, "no_speech_prob": 2.7535157641978003e-05}, {"id": 5, "seek": 2946, "start": 36.36, "end": 41.04, "text": " What's the point?", "tokens": [708, 311, 264, 935, 30], "temperature": 0.0, "avg_logprob": -0.14832210540771484, "compression_ratio": 1.4126984126984128, "no_speech_prob": 2.7535157641978003e-05}, {"id": 6, "seek": 2946, "start": 41.04, "end": 45.480000000000004, "text": " To answer this question, I'm going to put this PowerPoint in the GitHub repo so you", "tokens": [1407, 1867, 341, 1168, 11, 286, 478, 516, 281, 829, 341, 25584, 294, 264, 23331, 49040, 370, 291], "temperature": 0.0, "avg_logprob": -0.14832210540771484, "compression_ratio": 1.4126984126984128, "no_speech_prob": 2.7535157641978003e-05}, {"id": 7, "seek": 2946, "start": 45.480000000000004, "end": 48.38, "text": " can have a look.", "tokens": [393, 362, 257, 574, 13], "temperature": 0.0, "avg_logprob": -0.14832210540771484, "compression_ratio": 1.4126984126984128, "no_speech_prob": 2.7535157641978003e-05}, {"id": 8, "seek": 2946, "start": 48.38, "end": 57.0, "text": " I want to show you something really important, which is examples of how people have used", "tokens": [286, 528, 281, 855, 291, 746, 534, 1021, 11, 597, 307, 5110, 295, 577, 561, 362, 1143], "temperature": 0.0, "avg_logprob": -0.14832210540771484, "compression_ratio": 1.4126984126984128, "no_speech_prob": 2.7535157641978003e-05}, {"id": 9, "seek": 5700, "start": 57.0, "end": 62.92, "text": " machine learning mainly in business, because that's where most of you are probably going", "tokens": [3479, 2539, 8704, 294, 1606, 11, 570, 300, 311, 689, 881, 295, 291, 366, 1391, 516], "temperature": 0.0, "avg_logprob": -0.17910165684197538, "compression_ratio": 1.7008196721311475, "no_speech_prob": 1.5206380339805037e-05}, {"id": 10, "seek": 5700, "start": 62.92, "end": 65.76, "text": " to end up after this is working for some company.", "tokens": [281, 917, 493, 934, 341, 307, 1364, 337, 512, 2237, 13], "temperature": 0.0, "avg_logprob": -0.17910165684197538, "compression_ratio": 1.7008196721311475, "no_speech_prob": 1.5206380339805037e-05}, {"id": 11, "seek": 5700, "start": 65.76, "end": 70.6, "text": " I'm going to show you applications of machine learning which are either based on things", "tokens": [286, 478, 516, 281, 855, 291, 5821, 295, 3479, 2539, 597, 366, 2139, 2361, 322, 721], "temperature": 0.0, "avg_logprob": -0.17910165684197538, "compression_ratio": 1.7008196721311475, "no_speech_prob": 1.5206380339805037e-05}, {"id": 12, "seek": 5700, "start": 70.6, "end": 75.32, "text": " that I've been personally involved in myself or know of people who are doing them directly.", "tokens": [300, 286, 600, 668, 5665, 3288, 294, 2059, 420, 458, 295, 561, 567, 366, 884, 552, 3838, 13], "temperature": 0.0, "avg_logprob": -0.17910165684197538, "compression_ratio": 1.7008196721311475, "no_speech_prob": 1.5206380339805037e-05}, {"id": 13, "seek": 5700, "start": 75.32, "end": 82.12, "text": " So none of these are going to be like hypotheticals, these are all actual things that people are", "tokens": [407, 6022, 295, 613, 366, 516, 281, 312, 411, 33053, 82, 11, 613, 366, 439, 3539, 721, 300, 561, 366], "temperature": 0.0, "avg_logprob": -0.17910165684197538, "compression_ratio": 1.7008196721311475, "no_speech_prob": 1.5206380339805037e-05}, {"id": 14, "seek": 8212, "start": 82.12, "end": 87.2, "text": " doing and I've got direct or secondhand knowledge of.", "tokens": [884, 293, 286, 600, 658, 2047, 420, 1150, 5543, 3601, 295, 13], "temperature": 0.0, "avg_logprob": -0.12692863876755173, "compression_ratio": 1.7624309392265194, "no_speech_prob": 1.0451407433720306e-05}, {"id": 15, "seek": 8212, "start": 87.2, "end": 89.96000000000001, "text": " I'm going to split them into two groups, horizontal and vertical.", "tokens": [286, 478, 516, 281, 7472, 552, 666, 732, 3935, 11, 12750, 293, 9429, 13], "temperature": 0.0, "avg_logprob": -0.12692863876755173, "compression_ratio": 1.7624309392265194, "no_speech_prob": 1.0451407433720306e-05}, {"id": 16, "seek": 8212, "start": 89.96000000000001, "end": 98.0, "text": " So in business, horizontal means something that you do across different kinds of business,", "tokens": [407, 294, 1606, 11, 12750, 1355, 746, 300, 291, 360, 2108, 819, 3685, 295, 1606, 11], "temperature": 0.0, "avg_logprob": -0.12692863876755173, "compression_ratio": 1.7624309392265194, "no_speech_prob": 1.0451407433720306e-05}, {"id": 17, "seek": 8212, "start": 98.0, "end": 103.92, "text": " whereas vertical means something that you do within a business or within a supply chain", "tokens": [9735, 9429, 1355, 746, 300, 291, 360, 1951, 257, 1606, 420, 1951, 257, 5847, 5021], "temperature": 0.0, "avg_logprob": -0.12692863876755173, "compression_ratio": 1.7624309392265194, "no_speech_prob": 1.0451407433720306e-05}, {"id": 18, "seek": 8212, "start": 103.92, "end": 105.52000000000001, "text": " or within a process.", "tokens": [420, 1951, 257, 1399, 13], "temperature": 0.0, "avg_logprob": -0.12692863876755173, "compression_ratio": 1.7624309392265194, "no_speech_prob": 1.0451407433720306e-05}, {"id": 19, "seek": 10552, "start": 105.52, "end": 113.92, "text": " So in other words, an example of horizontal applications is everything involving marketing.", "tokens": [407, 294, 661, 2283, 11, 364, 1365, 295, 12750, 5821, 307, 1203, 17030, 6370, 13], "temperature": 0.0, "avg_logprob": -0.1051865577697754, "compression_ratio": 1.6730769230769231, "no_speech_prob": 3.2887360248423647e-06}, {"id": 20, "seek": 10552, "start": 113.92, "end": 120.47999999999999, "text": " So every company pretty much has to try to sell more products to its customers and so", "tokens": [407, 633, 2237, 1238, 709, 575, 281, 853, 281, 3607, 544, 3383, 281, 1080, 4581, 293, 370], "temperature": 0.0, "avg_logprob": -0.1051865577697754, "compression_ratio": 1.6730769230769231, "no_speech_prob": 3.2887360248423647e-06}, {"id": 21, "seek": 10552, "start": 120.47999999999999, "end": 122.0, "text": " therefore does marketing.", "tokens": [4412, 775, 6370, 13], "temperature": 0.0, "avg_logprob": -0.1051865577697754, "compression_ratio": 1.6730769230769231, "no_speech_prob": 3.2887360248423647e-06}, {"id": 22, "seek": 10552, "start": 122.0, "end": 127.39999999999999, "text": " And so each of these boxes are examples of some of the things that people are using machine", "tokens": [400, 370, 1184, 295, 613, 9002, 366, 5110, 295, 512, 295, 264, 721, 300, 561, 366, 1228, 3479], "temperature": 0.0, "avg_logprob": -0.1051865577697754, "compression_ratio": 1.6730769230769231, "no_speech_prob": 3.2887360248423647e-06}, {"id": 23, "seek": 10552, "start": 127.39999999999999, "end": 131.14, "text": " learning for in marketing.", "tokens": [2539, 337, 294, 6370, 13], "temperature": 0.0, "avg_logprob": -0.1051865577697754, "compression_ratio": 1.6730769230769231, "no_speech_prob": 3.2887360248423647e-06}, {"id": 24, "seek": 10552, "start": 131.14, "end": 133.68, "text": " So let's take an example.", "tokens": [407, 718, 311, 747, 364, 1365, 13], "temperature": 0.0, "avg_logprob": -0.1051865577697754, "compression_ratio": 1.6730769230769231, "no_speech_prob": 3.2887360248423647e-06}, {"id": 25, "seek": 13368, "start": 133.68, "end": 138.76000000000002, "text": " Let's take churn.", "tokens": [961, 311, 747, 417, 925, 13], "temperature": 0.0, "avg_logprob": -0.14772251505910614, "compression_ratio": 1.5612244897959184, "no_speech_prob": 4.3568670662352815e-06}, {"id": 26, "seek": 13368, "start": 138.76000000000002, "end": 146.92000000000002, "text": " So churn refers to a model which attempts to predict who's going to leave.", "tokens": [407, 417, 925, 14942, 281, 257, 2316, 597, 15257, 281, 6069, 567, 311, 516, 281, 1856, 13], "temperature": 0.0, "avg_logprob": -0.14772251505910614, "compression_ratio": 1.5612244897959184, "no_speech_prob": 4.3568670662352815e-06}, {"id": 27, "seek": 13368, "start": 146.92000000000002, "end": 152.12, "text": " So I've done some churn modeling fairly recently in telecommunications.", "tokens": [407, 286, 600, 1096, 512, 417, 925, 15983, 6457, 3938, 294, 4304, 25451, 24847, 13], "temperature": 0.0, "avg_logprob": -0.14772251505910614, "compression_ratio": 1.5612244897959184, "no_speech_prob": 4.3568670662352815e-06}, {"id": 28, "seek": 13368, "start": 152.12, "end": 155.28, "text": " And so we're trying to figure out for this big cell phone company which customers are", "tokens": [400, 370, 321, 434, 1382, 281, 2573, 484, 337, 341, 955, 2815, 2593, 2237, 597, 4581, 366], "temperature": 0.0, "avg_logprob": -0.14772251505910614, "compression_ratio": 1.5612244897959184, "no_speech_prob": 4.3568670662352815e-06}, {"id": 29, "seek": 13368, "start": 155.28, "end": 158.92000000000002, "text": " going to leave.", "tokens": [516, 281, 1856, 13], "temperature": 0.0, "avg_logprob": -0.14772251505910614, "compression_ratio": 1.5612244897959184, "no_speech_prob": 4.3568670662352815e-06}, {"id": 30, "seek": 13368, "start": 158.92000000000002, "end": 162.98000000000002, "text": " That is not of itself that interesting.", "tokens": [663, 307, 406, 295, 2564, 300, 1880, 13], "temperature": 0.0, "avg_logprob": -0.14772251505910614, "compression_ratio": 1.5612244897959184, "no_speech_prob": 4.3568670662352815e-06}, {"id": 31, "seek": 16298, "start": 162.98, "end": 169.1, "text": " Like building a highly predictive model that says Jeremy Howard is almost certainly going", "tokens": [1743, 2390, 257, 5405, 35521, 2316, 300, 1619, 17809, 17626, 307, 1920, 3297, 516], "temperature": 0.0, "avg_logprob": -0.11630472164709592, "compression_ratio": 1.7521008403361344, "no_speech_prob": 4.157354851486161e-06}, {"id": 32, "seek": 16298, "start": 169.1, "end": 175.48, "text": " to leave next month is probably not that helpful because if I'm almost certainly going to leave", "tokens": [281, 1856, 958, 1618, 307, 1391, 406, 300, 4961, 570, 498, 286, 478, 1920, 3297, 516, 281, 1856], "temperature": 0.0, "avg_logprob": -0.11630472164709592, "compression_ratio": 1.7521008403361344, "no_speech_prob": 4.157354851486161e-06}, {"id": 33, "seek": 16298, "start": 175.48, "end": 178.82, "text": " next month, there's probably nothing you can do about it.", "tokens": [958, 1618, 11, 456, 311, 1391, 1825, 291, 393, 360, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.11630472164709592, "compression_ratio": 1.7521008403361344, "no_speech_prob": 4.157354851486161e-06}, {"id": 34, "seek": 16298, "start": 178.82, "end": 179.82, "text": " It's too late.", "tokens": [467, 311, 886, 3469, 13], "temperature": 0.0, "avg_logprob": -0.11630472164709592, "compression_ratio": 1.7521008403361344, "no_speech_prob": 4.157354851486161e-06}, {"id": 35, "seek": 16298, "start": 179.82, "end": 182.95999999999998, "text": " It's going to cost you too much to keep me.", "tokens": [467, 311, 516, 281, 2063, 291, 886, 709, 281, 1066, 385, 13], "temperature": 0.0, "avg_logprob": -0.11630472164709592, "compression_ratio": 1.7521008403361344, "no_speech_prob": 4.157354851486161e-06}, {"id": 36, "seek": 16298, "start": 182.95999999999998, "end": 189.56, "text": " So in order to understand why would we do churn modeling, I've got a little framework", "tokens": [407, 294, 1668, 281, 1223, 983, 576, 321, 360, 417, 925, 15983, 11, 286, 600, 658, 257, 707, 8388], "temperature": 0.0, "avg_logprob": -0.11630472164709592, "compression_ratio": 1.7521008403361344, "no_speech_prob": 4.157354851486161e-06}, {"id": 37, "seek": 16298, "start": 189.56, "end": 191.23999999999998, "text": " that you might find helpful.", "tokens": [300, 291, 1062, 915, 4961, 13], "temperature": 0.0, "avg_logprob": -0.11630472164709592, "compression_ratio": 1.7521008403361344, "no_speech_prob": 4.157354851486161e-06}, {"id": 38, "seek": 19124, "start": 191.24, "end": 199.52, "text": " So if you Google for Jeremy Howard data products, I think I've mentioned this thing before,", "tokens": [407, 498, 291, 3329, 337, 17809, 17626, 1412, 3383, 11, 286, 519, 286, 600, 2835, 341, 551, 949, 11], "temperature": 0.0, "avg_logprob": -0.1294093132019043, "compression_ratio": 1.4748858447488584, "no_speech_prob": 2.9944355901534436e-06}, {"id": 39, "seek": 19124, "start": 199.52, "end": 204.20000000000002, "text": " there's a paper you can find, Designing Great Data Products that I wrote with a couple of", "tokens": [456, 311, 257, 3035, 291, 393, 915, 11, 3885, 9676, 3769, 11888, 47699, 300, 286, 4114, 365, 257, 1916, 295], "temperature": 0.0, "avg_logprob": -0.1294093132019043, "compression_ratio": 1.4748858447488584, "no_speech_prob": 2.9944355901534436e-06}, {"id": 40, "seek": 19124, "start": 204.20000000000002, "end": 206.88, "text": " colleagues a few years ago.", "tokens": [7734, 257, 1326, 924, 2057, 13], "temperature": 0.0, "avg_logprob": -0.1294093132019043, "compression_ratio": 1.4748858447488584, "no_speech_prob": 2.9944355901534436e-06}, {"id": 41, "seek": 19124, "start": 206.88, "end": 215.48000000000002, "text": " And in it, I describe my experience of actually turning machine learning models into like", "tokens": [400, 294, 309, 11, 286, 6786, 452, 1752, 295, 767, 6246, 3479, 2539, 5245, 666, 411], "temperature": 0.0, "avg_logprob": -0.1294093132019043, "compression_ratio": 1.4748858447488584, "no_speech_prob": 2.9944355901534436e-06}, {"id": 42, "seek": 19124, "start": 215.48000000000002, "end": 217.92000000000002, "text": " stuff that makes money.", "tokens": [1507, 300, 1669, 1460, 13], "temperature": 0.0, "avg_logprob": -0.1294093132019043, "compression_ratio": 1.4748858447488584, "no_speech_prob": 2.9944355901534436e-06}, {"id": 43, "seek": 21792, "start": 217.92, "end": 229.48, "text": " And the basic trick is this thing I call the drivetrain approach, which is these 4 steps.", "tokens": [400, 264, 3875, 4282, 307, 341, 551, 286, 818, 264, 1630, 9771, 7146, 3109, 11, 597, 307, 613, 1017, 4439, 13], "temperature": 0.0, "avg_logprob": -0.09561344429298684, "compression_ratio": 1.5673076923076923, "no_speech_prob": 3.340525609019096e-06}, {"id": 44, "seek": 21792, "start": 229.48, "end": 235.04, "text": " The starting point to actually turn a machine learning project into something that's actually", "tokens": [440, 2891, 935, 281, 767, 1261, 257, 3479, 2539, 1716, 666, 746, 300, 311, 767], "temperature": 0.0, "avg_logprob": -0.09561344429298684, "compression_ratio": 1.5673076923076923, "no_speech_prob": 3.340525609019096e-06}, {"id": 45, "seek": 21792, "start": 235.04, "end": 238.51999999999998, "text": " useful is to know what am I trying to achieve.", "tokens": [4420, 307, 281, 458, 437, 669, 286, 1382, 281, 4584, 13], "temperature": 0.0, "avg_logprob": -0.09561344429298684, "compression_ratio": 1.5673076923076923, "no_speech_prob": 3.340525609019096e-06}, {"id": 46, "seek": 21792, "start": 238.51999999999998, "end": 244.48, "text": " And that doesn't mean like I'm trying to achieve a high area under the ROC curve, or I'm trying", "tokens": [400, 300, 1177, 380, 914, 411, 286, 478, 1382, 281, 4584, 257, 1090, 1859, 833, 264, 9025, 34, 7605, 11, 420, 286, 478, 1382], "temperature": 0.0, "avg_logprob": -0.09561344429298684, "compression_ratio": 1.5673076923076923, "no_speech_prob": 3.340525609019096e-06}, {"id": 47, "seek": 24448, "start": 244.48, "end": 248.92, "text": " to achieve a large difference between classes.", "tokens": [281, 4584, 257, 2416, 2649, 1296, 5359, 13], "temperature": 0.0, "avg_logprob": -0.11721858111294833, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.4593737205359503e-06}, {"id": 48, "seek": 24448, "start": 248.92, "end": 256.76, "text": " No, it would be I'm trying to sell more books, or I'm trying to reduce the number of customers", "tokens": [883, 11, 309, 576, 312, 286, 478, 1382, 281, 3607, 544, 3642, 11, 420, 286, 478, 1382, 281, 5407, 264, 1230, 295, 4581], "temperature": 0.0, "avg_logprob": -0.11721858111294833, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.4593737205359503e-06}, {"id": 49, "seek": 24448, "start": 256.76, "end": 263.96, "text": " that leave next month, or I'm trying to detect lung cancer earlier.", "tokens": [300, 1856, 958, 1618, 11, 420, 286, 478, 1382, 281, 5531, 16730, 5592, 3071, 13], "temperature": 0.0, "avg_logprob": -0.11721858111294833, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.4593737205359503e-06}, {"id": 50, "seek": 24448, "start": 263.96, "end": 265.52, "text": " These are things, these are objectives.", "tokens": [1981, 366, 721, 11, 613, 366, 15961, 13], "temperature": 0.0, "avg_logprob": -0.11721858111294833, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.4593737205359503e-06}, {"id": 51, "seek": 24448, "start": 265.52, "end": 270.59999999999997, "text": " So the objective is something that absolutely directly is the thing that the company or", "tokens": [407, 264, 10024, 307, 746, 300, 3122, 3838, 307, 264, 551, 300, 264, 2237, 420], "temperature": 0.0, "avg_logprob": -0.11721858111294833, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.4593737205359503e-06}, {"id": 52, "seek": 24448, "start": 270.59999999999997, "end": 273.0, "text": " the organization actually wants.", "tokens": [264, 4475, 767, 2738, 13], "temperature": 0.0, "avg_logprob": -0.11721858111294833, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.4593737205359503e-06}, {"id": 53, "seek": 27300, "start": 273.0, "end": 281.52, "text": " No company or organization lives in order to create a more accurate predictive model", "tokens": [883, 2237, 420, 4475, 2909, 294, 1668, 281, 1884, 257, 544, 8559, 35521, 2316], "temperature": 0.0, "avg_logprob": -0.10994028193610055, "compression_ratio": 1.6272401433691757, "no_speech_prob": 9.080413292394951e-06}, {"id": 54, "seek": 27300, "start": 281.52, "end": 283.82, "text": " for some reason.", "tokens": [337, 512, 1778, 13], "temperature": 0.0, "avg_logprob": -0.10994028193610055, "compression_ratio": 1.6272401433691757, "no_speech_prob": 9.080413292394951e-06}, {"id": 55, "seek": 27300, "start": 283.82, "end": 285.82, "text": " So that's your objective.", "tokens": [407, 300, 311, 428, 10024, 13], "temperature": 0.0, "avg_logprob": -0.10994028193610055, "compression_ratio": 1.6272401433691757, "no_speech_prob": 9.080413292394951e-06}, {"id": 56, "seek": 27300, "start": 285.82, "end": 287.36, "text": " Now that's obviously the most important thing.", "tokens": [823, 300, 311, 2745, 264, 881, 1021, 551, 13], "temperature": 0.0, "avg_logprob": -0.10994028193610055, "compression_ratio": 1.6272401433691757, "no_speech_prob": 9.080413292394951e-06}, {"id": 57, "seek": 27300, "start": 287.36, "end": 290.72, "text": " If you don't know the purpose of what you're modeling for, then you can't possibly do a", "tokens": [759, 291, 500, 380, 458, 264, 4334, 295, 437, 291, 434, 15983, 337, 11, 550, 291, 393, 380, 6264, 360, 257], "temperature": 0.0, "avg_logprob": -0.10994028193610055, "compression_ratio": 1.6272401433691757, "no_speech_prob": 9.080413292394951e-06}, {"id": 58, "seek": 27300, "start": 290.72, "end": 292.92, "text": " good job of it.", "tokens": [665, 1691, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.10994028193610055, "compression_ratio": 1.6272401433691757, "no_speech_prob": 9.080413292394951e-06}, {"id": 59, "seek": 27300, "start": 292.92, "end": 298.36, "text": " And hopefully people are starting to pick that up out there in the world of data science.", "tokens": [400, 4696, 561, 366, 2891, 281, 1888, 300, 493, 484, 456, 294, 264, 1002, 295, 1412, 3497, 13], "temperature": 0.0, "avg_logprob": -0.10994028193610055, "compression_ratio": 1.6272401433691757, "no_speech_prob": 9.080413292394951e-06}, {"id": 60, "seek": 27300, "start": 298.36, "end": 302.12, "text": " But interestingly, what very few people are talking about, but it's just as important", "tokens": [583, 25873, 11, 437, 588, 1326, 561, 366, 1417, 466, 11, 457, 309, 311, 445, 382, 1021], "temperature": 0.0, "avg_logprob": -0.10994028193610055, "compression_ratio": 1.6272401433691757, "no_speech_prob": 9.080413292394951e-06}, {"id": 61, "seek": 30212, "start": 302.12, "end": 304.84000000000003, "text": " as the next thing, which is levers.", "tokens": [382, 264, 958, 551, 11, 597, 307, 45571, 13], "temperature": 0.0, "avg_logprob": -0.1353133045025726, "compression_ratio": 1.5568862275449102, "no_speech_prob": 3.187556103512179e-06}, {"id": 62, "seek": 30212, "start": 304.84000000000003, "end": 311.16, "text": " A lever is a thing that the organization can do to actually drive the objective.", "tokens": [316, 12451, 307, 257, 551, 300, 264, 4475, 393, 360, 281, 767, 3332, 264, 10024, 13], "temperature": 0.0, "avg_logprob": -0.1353133045025726, "compression_ratio": 1.5568862275449102, "no_speech_prob": 3.187556103512179e-06}, {"id": 63, "seek": 30212, "start": 311.16, "end": 314.58, "text": " So let's take the example of churn modeling.", "tokens": [407, 718, 311, 747, 264, 1365, 295, 417, 925, 15983, 13], "temperature": 0.0, "avg_logprob": -0.1353133045025726, "compression_ratio": 1.5568862275449102, "no_speech_prob": 3.187556103512179e-06}, {"id": 64, "seek": 30212, "start": 314.58, "end": 322.16, "text": " What is a lever that an organization could use to reduce the number of customers that", "tokens": [708, 307, 257, 12451, 300, 364, 4475, 727, 764, 281, 5407, 264, 1230, 295, 4581, 300], "temperature": 0.0, "avg_logprob": -0.1353133045025726, "compression_ratio": 1.5568862275449102, "no_speech_prob": 3.187556103512179e-06}, {"id": 65, "seek": 30212, "start": 322.16, "end": 326.88, "text": " are leaving?", "tokens": [366, 5012, 30], "temperature": 0.0, "avg_logprob": -0.1353133045025726, "compression_ratio": 1.5568862275449102, "no_speech_prob": 3.187556103512179e-06}, {"id": 66, "seek": 32688, "start": 326.88, "end": 332.52, "text": " They could take a closer look at the model and do some of this random forest interpretation", "tokens": [814, 727, 747, 257, 4966, 574, 412, 264, 2316, 293, 360, 512, 295, 341, 4974, 6719, 14174], "temperature": 0.0, "avg_logprob": -0.22241659725413604, "compression_ratio": 1.7238805970149254, "no_speech_prob": 1.1478721717139706e-05}, {"id": 67, "seek": 32688, "start": 332.52, "end": 338.76, "text": " and see some of the causes that are causing people to leave and potentially change those", "tokens": [293, 536, 512, 295, 264, 7700, 300, 366, 9853, 561, 281, 1856, 293, 7263, 1319, 729], "temperature": 0.0, "avg_logprob": -0.22241659725413604, "compression_ratio": 1.7238805970149254, "no_speech_prob": 1.1478721717139706e-05}, {"id": 68, "seek": 32688, "start": 338.76, "end": 340.84, "text": " issues in the company.", "tokens": [2663, 294, 264, 2237, 13], "temperature": 0.0, "avg_logprob": -0.22241659725413604, "compression_ratio": 1.7238805970149254, "no_speech_prob": 1.1478721717139706e-05}, {"id": 69, "seek": 32688, "start": 340.84, "end": 343.4, "text": " So that's a data scientist answer.", "tokens": [407, 300, 311, 257, 1412, 12662, 1867, 13], "temperature": 0.0, "avg_logprob": -0.22241659725413604, "compression_ratio": 1.7238805970149254, "no_speech_prob": 1.1478721717139706e-05}, {"id": 70, "seek": 32688, "start": 343.4, "end": 344.76, "text": " But I want you to go to the next level.", "tokens": [583, 286, 528, 291, 281, 352, 281, 264, 958, 1496, 13], "temperature": 0.0, "avg_logprob": -0.22241659725413604, "compression_ratio": 1.7238805970149254, "no_speech_prob": 1.1478721717139706e-05}, {"id": 71, "seek": 32688, "start": 344.76, "end": 347.24, "text": " What are the things, the levers are the things they can do.", "tokens": [708, 366, 264, 721, 11, 264, 45571, 366, 264, 721, 436, 393, 360, 13], "temperature": 0.0, "avg_logprob": -0.22241659725413604, "compression_ratio": 1.7238805970149254, "no_speech_prob": 1.1478721717139706e-05}, {"id": 72, "seek": 32688, "start": 347.24, "end": 348.44, "text": " Do you want to put it past behind you?", "tokens": [1144, 291, 528, 281, 829, 309, 1791, 2261, 291, 30], "temperature": 0.0, "avg_logprob": -0.22241659725413604, "compression_ratio": 1.7238805970149254, "no_speech_prob": 1.1478721717139706e-05}, {"id": 73, "seek": 32688, "start": 348.44, "end": 350.48, "text": " What are the things that they can do?", "tokens": [708, 366, 264, 721, 300, 436, 393, 360, 30], "temperature": 0.0, "avg_logprob": -0.22241659725413604, "compression_ratio": 1.7238805970149254, "no_speech_prob": 1.1478721717139706e-05}, {"id": 74, "seek": 32688, "start": 350.48, "end": 354.14, "text": " Just outreach, like calling or sending emails.", "tokens": [1449, 19638, 11, 411, 5141, 420, 7750, 12524, 13], "temperature": 0.0, "avg_logprob": -0.22241659725413604, "compression_ratio": 1.7238805970149254, "no_speech_prob": 1.1478721717139706e-05}, {"id": 75, "seek": 35414, "start": 354.14, "end": 360.96, "text": " They could call someone and say like, are you happy, anything we could do.", "tokens": [814, 727, 818, 1580, 293, 584, 411, 11, 366, 291, 2055, 11, 1340, 321, 727, 360, 13], "temperature": 0.0, "avg_logprob": -0.3050802203192227, "compression_ratio": 1.4835164835164836, "no_speech_prob": 7.254174124682322e-05}, {"id": 76, "seek": 35414, "start": 360.96, "end": 367.2, "text": " They can provide incentives to increase engagement with the product.", "tokens": [814, 393, 2893, 23374, 281, 3488, 8742, 365, 264, 1674, 13], "temperature": 0.0, "avg_logprob": -0.3050802203192227, "compression_ratio": 1.4835164835164836, "no_speech_prob": 7.254174124682322e-05}, {"id": 77, "seek": 35414, "start": 367.2, "end": 375.47999999999996, "text": " So they could give them a free pen or something if they buy 20 bucks worth of product next", "tokens": [407, 436, 727, 976, 552, 257, 1737, 3435, 420, 746, 498, 436, 2256, 945, 11829, 3163, 295, 1674, 958], "temperature": 0.0, "avg_logprob": -0.3050802203192227, "compression_ratio": 1.4835164835164836, "no_speech_prob": 7.254174124682322e-05}, {"id": 78, "seek": 35414, "start": 375.47999999999996, "end": 376.47999999999996, "text": " month.", "tokens": [1618, 13], "temperature": 0.0, "avg_logprob": -0.3050802203192227, "compression_ratio": 1.4835164835164836, "no_speech_prob": 7.254174124682322e-05}, {"id": 79, "seek": 35414, "start": 376.47999999999996, "end": 380.0, "text": " You want to do that as well?", "tokens": [509, 528, 281, 360, 300, 382, 731, 30], "temperature": 0.0, "avg_logprob": -0.3050802203192227, "compression_ratio": 1.4835164835164836, "no_speech_prob": 7.254174124682322e-05}, {"id": 80, "seek": 38000, "start": 380.0, "end": 385.64, "text": " You guys are the giving out carrots rather than handing out sticks.", "tokens": [509, 1074, 366, 264, 2902, 484, 21005, 2831, 813, 34774, 484, 12518, 13], "temperature": 0.0, "avg_logprob": -0.22903848794790416, "compression_ratio": 1.5952380952380953, "no_speech_prob": 2.753524313447997e-05}, {"id": 81, "seek": 38000, "start": 385.64, "end": 398.12, "text": " These are levers.", "tokens": [1981, 366, 45571, 13], "temperature": 0.0, "avg_logprob": -0.22903848794790416, "compression_ratio": 1.5952380952380953, "no_speech_prob": 2.753524313447997e-05}, {"id": 82, "seek": 38000, "start": 398.12, "end": 403.48, "text": " And so whenever you're working as a data scientist, keep coming back and thinking, what are we", "tokens": [400, 370, 5699, 291, 434, 1364, 382, 257, 1412, 12662, 11, 1066, 1348, 646, 293, 1953, 11, 437, 366, 321], "temperature": 0.0, "avg_logprob": -0.22903848794790416, "compression_ratio": 1.5952380952380953, "no_speech_prob": 2.753524313447997e-05}, {"id": 83, "seek": 38000, "start": 403.48, "end": 408.32, "text": " trying to achieve, we being the organization, and how are we trying to achieve it being", "tokens": [1382, 281, 4584, 11, 321, 885, 264, 4475, 11, 293, 577, 366, 321, 1382, 281, 4584, 309, 885], "temperature": 0.0, "avg_logprob": -0.22903848794790416, "compression_ratio": 1.5952380952380953, "no_speech_prob": 2.753524313447997e-05}, {"id": 84, "seek": 40832, "start": 408.32, "end": 413.48, "text": " like what are the actual things we can do to make that objective happen.", "tokens": [411, 437, 366, 264, 3539, 721, 321, 393, 360, 281, 652, 300, 10024, 1051, 13], "temperature": 0.0, "avg_logprob": -0.12740747895959306, "compression_ratio": 1.6256983240223464, "no_speech_prob": 1.8738714970822912e-06}, {"id": 85, "seek": 40832, "start": 413.48, "end": 418.84, "text": " So building a model is never ever a lever.", "tokens": [407, 2390, 257, 2316, 307, 1128, 1562, 257, 12451, 13], "temperature": 0.0, "avg_logprob": -0.12740747895959306, "compression_ratio": 1.6256983240223464, "no_speech_prob": 1.8738714970822912e-06}, {"id": 86, "seek": 40832, "start": 418.84, "end": 422.96, "text": " But it could help you with the lever.", "tokens": [583, 309, 727, 854, 291, 365, 264, 12451, 13], "temperature": 0.0, "avg_logprob": -0.12740747895959306, "compression_ratio": 1.6256983240223464, "no_speech_prob": 1.8738714970822912e-06}, {"id": 87, "seek": 40832, "start": 422.96, "end": 429.52, "text": " So then the next step is, what data does the organization have that could possibly help", "tokens": [407, 550, 264, 958, 1823, 307, 11, 437, 1412, 775, 264, 4475, 362, 300, 727, 6264, 854], "temperature": 0.0, "avg_logprob": -0.12740747895959306, "compression_ratio": 1.6256983240223464, "no_speech_prob": 1.8738714970822912e-06}, {"id": 88, "seek": 40832, "start": 429.52, "end": 434.15999999999997, "text": " them to set that lever to achieve that objective.", "tokens": [552, 281, 992, 300, 12451, 281, 4584, 300, 10024, 13], "temperature": 0.0, "avg_logprob": -0.12740747895959306, "compression_ratio": 1.6256983240223464, "no_speech_prob": 1.8738714970822912e-06}, {"id": 89, "seek": 43416, "start": 434.16, "end": 440.0, "text": " And so this is not what data did they give you when you started the project, but think", "tokens": [400, 370, 341, 307, 406, 437, 1412, 630, 436, 976, 291, 562, 291, 1409, 264, 1716, 11, 457, 519], "temperature": 0.0, "avg_logprob": -0.1301262928889348, "compression_ratio": 1.6288209606986899, "no_speech_prob": 1.9223025446990505e-05}, {"id": 90, "seek": 43416, "start": 440.0, "end": 442.48, "text": " about it from a first principles point of view.", "tokens": [466, 309, 490, 257, 700, 9156, 935, 295, 1910, 13], "temperature": 0.0, "avg_logprob": -0.1301262928889348, "compression_ratio": 1.6288209606986899, "no_speech_prob": 1.9223025446990505e-05}, {"id": 91, "seek": 43416, "start": 442.48, "end": 447.36, "text": " I'm working for a telecommunications company, they gave me some certain set of data, but", "tokens": [286, 478, 1364, 337, 257, 4304, 25451, 24847, 2237, 11, 436, 2729, 385, 512, 1629, 992, 295, 1412, 11, 457], "temperature": 0.0, "avg_logprob": -0.1301262928889348, "compression_ratio": 1.6288209606986899, "no_speech_prob": 1.9223025446990505e-05}, {"id": 92, "seek": 43416, "start": 447.36, "end": 452.56, "text": " I'm sure they must know where their customers live, how many phone calls they made last", "tokens": [286, 478, 988, 436, 1633, 458, 689, 641, 4581, 1621, 11, 577, 867, 2593, 5498, 436, 1027, 1036], "temperature": 0.0, "avg_logprob": -0.1301262928889348, "compression_ratio": 1.6288209606986899, "no_speech_prob": 1.9223025446990505e-05}, {"id": 93, "seek": 43416, "start": 452.56, "end": 456.76000000000005, "text": " month, how many times they called customer service, whatever.", "tokens": [1618, 11, 577, 867, 1413, 436, 1219, 5474, 2643, 11, 2035, 13], "temperature": 0.0, "avg_logprob": -0.1301262928889348, "compression_ratio": 1.6288209606986899, "no_speech_prob": 1.9223025446990505e-05}, {"id": 94, "seek": 45676, "start": 456.76, "end": 465.48, "text": " And so have a think about, if we're trying to decide who should we give a special offer", "tokens": [400, 370, 362, 257, 519, 466, 11, 498, 321, 434, 1382, 281, 4536, 567, 820, 321, 976, 257, 2121, 2626], "temperature": 0.0, "avg_logprob": -0.14315305782269827, "compression_ratio": 1.5665024630541873, "no_speech_prob": 4.092902599950321e-06}, {"id": 95, "seek": 45676, "start": 465.48, "end": 472.62, "text": " to proactively, then we want to figure out what information do we have that might help", "tokens": [281, 447, 45679, 11, 550, 321, 528, 281, 2573, 484, 437, 1589, 360, 321, 362, 300, 1062, 854], "temperature": 0.0, "avg_logprob": -0.14315305782269827, "compression_ratio": 1.5665024630541873, "no_speech_prob": 4.092902599950321e-06}, {"id": 96, "seek": 45676, "start": 472.62, "end": 477.4, "text": " us to identify who's going to react well or badly to that.", "tokens": [505, 281, 5876, 567, 311, 516, 281, 4515, 731, 420, 13425, 281, 300, 13], "temperature": 0.0, "avg_logprob": -0.14315305782269827, "compression_ratio": 1.5665024630541873, "no_speech_prob": 4.092902599950321e-06}, {"id": 97, "seek": 45676, "start": 477.4, "end": 484.48, "text": " Perhaps more interestingly would be, what if we were doing a fraud algorithm, and so", "tokens": [10517, 544, 25873, 576, 312, 11, 437, 498, 321, 645, 884, 257, 14560, 9284, 11, 293, 370], "temperature": 0.0, "avg_logprob": -0.14315305782269827, "compression_ratio": 1.5665024630541873, "no_speech_prob": 4.092902599950321e-06}, {"id": 98, "seek": 48448, "start": 484.48, "end": 488.8, "text": " we're trying to figure out who's going to not pay for the phone that they take out of", "tokens": [321, 434, 1382, 281, 2573, 484, 567, 311, 516, 281, 406, 1689, 337, 264, 2593, 300, 436, 747, 484, 295], "temperature": 0.0, "avg_logprob": -0.12488484582981142, "compression_ratio": 1.7786561264822134, "no_speech_prob": 9.818281796469819e-06}, {"id": 99, "seek": 48448, "start": 488.8, "end": 493.8, "text": " the store, they're on some 12 month payment plan, we never see them again.", "tokens": [264, 3531, 11, 436, 434, 322, 512, 2272, 1618, 10224, 1393, 11, 321, 1128, 536, 552, 797, 13], "temperature": 0.0, "avg_logprob": -0.12488484582981142, "compression_ratio": 1.7786561264822134, "no_speech_prob": 9.818281796469819e-06}, {"id": 100, "seek": 48448, "start": 493.8, "end": 497.88, "text": " Now in that case, the data we have available, it doesn't matter what's in the database,", "tokens": [823, 294, 300, 1389, 11, 264, 1412, 321, 362, 2435, 11, 309, 1177, 380, 1871, 437, 311, 294, 264, 8149, 11], "temperature": 0.0, "avg_logprob": -0.12488484582981142, "compression_ratio": 1.7786561264822134, "no_speech_prob": 9.818281796469819e-06}, {"id": 101, "seek": 48448, "start": 497.88, "end": 503.0, "text": " what matters is what's the data that we can get when the customer is in the shop.", "tokens": [437, 7001, 307, 437, 311, 264, 1412, 300, 321, 393, 483, 562, 264, 5474, 307, 294, 264, 3945, 13], "temperature": 0.0, "avg_logprob": -0.12488484582981142, "compression_ratio": 1.7786561264822134, "no_speech_prob": 9.818281796469819e-06}, {"id": 102, "seek": 48448, "start": 503.0, "end": 508.92, "text": " So there's often constraints around the data that we can actually use.", "tokens": [407, 456, 311, 2049, 18491, 926, 264, 1412, 300, 321, 393, 767, 764, 13], "temperature": 0.0, "avg_logprob": -0.12488484582981142, "compression_ratio": 1.7786561264822134, "no_speech_prob": 9.818281796469819e-06}, {"id": 103, "seek": 48448, "start": 508.92, "end": 513.64, "text": " So we need to know, what am I trying to achieve?", "tokens": [407, 321, 643, 281, 458, 11, 437, 669, 286, 1382, 281, 4584, 30], "temperature": 0.0, "avg_logprob": -0.12488484582981142, "compression_ratio": 1.7786561264822134, "no_speech_prob": 9.818281796469819e-06}, {"id": 104, "seek": 51364, "start": 513.64, "end": 518.4, "text": " What can this organization actually do specifically to change that outcome?", "tokens": [708, 393, 341, 4475, 767, 360, 4682, 281, 1319, 300, 9700, 30], "temperature": 0.0, "avg_logprob": -0.1244560415094549, "compression_ratio": 1.7520325203252032, "no_speech_prob": 1.4593747437174898e-06}, {"id": 105, "seek": 51364, "start": 518.4, "end": 523.0, "text": " And at the point that that decision is being made, what data do they have or could they", "tokens": [400, 412, 264, 935, 300, 300, 3537, 307, 885, 1027, 11, 437, 1412, 360, 436, 362, 420, 727, 436], "temperature": 0.0, "avg_logprob": -0.1244560415094549, "compression_ratio": 1.7520325203252032, "no_speech_prob": 1.4593747437174898e-06}, {"id": 106, "seek": 51364, "start": 523.0, "end": 525.76, "text": " collect?", "tokens": [2500, 30], "temperature": 0.0, "avg_logprob": -0.1244560415094549, "compression_ratio": 1.7520325203252032, "no_speech_prob": 1.4593747437174898e-06}, {"id": 107, "seek": 51364, "start": 525.76, "end": 529.1999999999999, "text": " And so then the way I put that all together is with a model.", "tokens": [400, 370, 550, 264, 636, 286, 829, 300, 439, 1214, 307, 365, 257, 2316, 13], "temperature": 0.0, "avg_logprob": -0.1244560415094549, "compression_ratio": 1.7520325203252032, "no_speech_prob": 1.4593747437174898e-06}, {"id": 108, "seek": 51364, "start": 529.1999999999999, "end": 533.04, "text": " And this is not a model in the sense of a predictive model, but it's a model in the", "tokens": [400, 341, 307, 406, 257, 2316, 294, 264, 2020, 295, 257, 35521, 2316, 11, 457, 309, 311, 257, 2316, 294, 264], "temperature": 0.0, "avg_logprob": -0.1244560415094549, "compression_ratio": 1.7520325203252032, "no_speech_prob": 1.4593747437174898e-06}, {"id": 109, "seek": 51364, "start": 533.04, "end": 535.24, "text": " sense of a simulation model.", "tokens": [2020, 295, 257, 16575, 2316, 13], "temperature": 0.0, "avg_logprob": -0.1244560415094549, "compression_ratio": 1.7520325203252032, "no_speech_prob": 1.4593747437174898e-06}, {"id": 110, "seek": 51364, "start": 535.24, "end": 540.3199999999999, "text": " So one of the main examples I give in this paper is one I spent many years building,", "tokens": [407, 472, 295, 264, 2135, 5110, 286, 976, 294, 341, 3035, 307, 472, 286, 4418, 867, 924, 2390, 11], "temperature": 0.0, "avg_logprob": -0.1244560415094549, "compression_ratio": 1.7520325203252032, "no_speech_prob": 1.4593747437174898e-06}, {"id": 111, "seek": 54032, "start": 540.32, "end": 549.1600000000001, "text": " which is if an insurance company changes their prices, how does that impact their profitability?", "tokens": [597, 307, 498, 364, 7214, 2237, 2962, 641, 7901, 11, 577, 775, 300, 2712, 641, 46249, 30], "temperature": 0.0, "avg_logprob": -0.11504119756270428, "compression_ratio": 1.7490196078431373, "no_speech_prob": 1.2411413081281353e-05}, {"id": 112, "seek": 54032, "start": 549.1600000000001, "end": 553.7600000000001, "text": " And so generally your simulation model contains a number of predictive models.", "tokens": [400, 370, 5101, 428, 16575, 2316, 8306, 257, 1230, 295, 35521, 5245, 13], "temperature": 0.0, "avg_logprob": -0.11504119756270428, "compression_ratio": 1.7490196078431373, "no_speech_prob": 1.2411413081281353e-05}, {"id": 113, "seek": 54032, "start": 553.7600000000001, "end": 558.6400000000001, "text": " So I had for example a predictive model called an elasticity model that said for a specific", "tokens": [407, 286, 632, 337, 1365, 257, 35521, 2316, 1219, 364, 46260, 2316, 300, 848, 337, 257, 2685], "temperature": 0.0, "avg_logprob": -0.11504119756270428, "compression_ratio": 1.7490196078431373, "no_speech_prob": 1.2411413081281353e-05}, {"id": 114, "seek": 54032, "start": 558.6400000000001, "end": 564.2, "text": " customer, if we charge them a specific price for a specific product, what's the probability", "tokens": [5474, 11, 498, 321, 4602, 552, 257, 2685, 3218, 337, 257, 2685, 1674, 11, 437, 311, 264, 8482], "temperature": 0.0, "avg_logprob": -0.11504119756270428, "compression_ratio": 1.7490196078431373, "no_speech_prob": 1.2411413081281353e-05}, {"id": 115, "seek": 54032, "start": 564.2, "end": 569.8000000000001, "text": " that they would say yes, both when it's new business and then a year later, what's the", "tokens": [300, 436, 576, 584, 2086, 11, 1293, 562, 309, 311, 777, 1606, 293, 550, 257, 1064, 1780, 11, 437, 311, 264], "temperature": 0.0, "avg_logprob": -0.11504119756270428, "compression_ratio": 1.7490196078431373, "no_speech_prob": 1.2411413081281353e-05}, {"id": 116, "seek": 56980, "start": 569.8, "end": 572.28, "text": " probability that they're new?", "tokens": [8482, 300, 436, 434, 777, 30], "temperature": 0.0, "avg_logprob": -0.15199051263197413, "compression_ratio": 1.752988047808765, "no_speech_prob": 2.7694020445778733e-06}, {"id": 117, "seek": 56980, "start": 572.28, "end": 576.1999999999999, "text": " And then there's another predictive model, which is what's the probability that they're", "tokens": [400, 550, 456, 311, 1071, 35521, 2316, 11, 597, 307, 437, 311, 264, 8482, 300, 436, 434], "temperature": 0.0, "avg_logprob": -0.15199051263197413, "compression_ratio": 1.752988047808765, "no_speech_prob": 2.7694020445778733e-06}, {"id": 118, "seek": 56980, "start": 576.1999999999999, "end": 580.7199999999999, "text": " going to make a claim and how much is that claim going to be?", "tokens": [516, 281, 652, 257, 3932, 293, 577, 709, 307, 300, 3932, 516, 281, 312, 30], "temperature": 0.0, "avg_logprob": -0.15199051263197413, "compression_ratio": 1.752988047808765, "no_speech_prob": 2.7694020445778733e-06}, {"id": 119, "seek": 56980, "start": 580.7199999999999, "end": 584.64, "text": " And so you can combine these models together then to say, all right, if we changed our", "tokens": [400, 370, 291, 393, 10432, 613, 5245, 1214, 550, 281, 584, 11, 439, 558, 11, 498, 321, 3105, 527], "temperature": 0.0, "avg_logprob": -0.15199051263197413, "compression_ratio": 1.752988047808765, "no_speech_prob": 2.7694020445778733e-06}, {"id": 120, "seek": 56980, "start": 584.64, "end": 590.4399999999999, "text": " pricing by reducing it by 10% for everybody between 18 and 25, and we can run it through", "tokens": [17621, 538, 12245, 309, 538, 1266, 4, 337, 2201, 1296, 2443, 293, 3552, 11, 293, 321, 393, 1190, 309, 807], "temperature": 0.0, "avg_logprob": -0.15199051263197413, "compression_ratio": 1.752988047808765, "no_speech_prob": 2.7694020445778733e-06}, {"id": 121, "seek": 56980, "start": 590.4399999999999, "end": 594.4, "text": " these models that combine together into a simulation, then the overall impact on our", "tokens": [613, 5245, 300, 10432, 1214, 666, 257, 16575, 11, 550, 264, 4787, 2712, 322, 527], "temperature": 0.0, "avg_logprob": -0.15199051263197413, "compression_ratio": 1.752988047808765, "no_speech_prob": 2.7694020445778733e-06}, {"id": 122, "seek": 59440, "start": 594.4, "end": 601.64, "text": " market share in 10 years time is X and our cost is Y and our profit is Z and so forth.", "tokens": [2142, 2073, 294, 1266, 924, 565, 307, 1783, 293, 527, 2063, 307, 398, 293, 527, 7475, 307, 1176, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.11085655882551863, "compression_ratio": 1.601063829787234, "no_speech_prob": 7.33820570530952e-07}, {"id": 123, "seek": 59440, "start": 601.64, "end": 610.4399999999999, "text": " So in practice, most of the time you really are going to care more about the results of", "tokens": [407, 294, 3124, 11, 881, 295, 264, 565, 291, 534, 366, 516, 281, 1127, 544, 466, 264, 3542, 295], "temperature": 0.0, "avg_logprob": -0.11085655882551863, "compression_ratio": 1.601063829787234, "no_speech_prob": 7.33820570530952e-07}, {"id": 124, "seek": 59440, "start": 610.4399999999999, "end": 615.84, "text": " that simulation than you do about the predictive model directly.", "tokens": [300, 16575, 813, 291, 360, 466, 264, 35521, 2316, 3838, 13], "temperature": 0.0, "avg_logprob": -0.11085655882551863, "compression_ratio": 1.601063829787234, "no_speech_prob": 7.33820570530952e-07}, {"id": 125, "seek": 59440, "start": 615.84, "end": 619.62, "text": " But most people are not doing this effectively at the moment.", "tokens": [583, 881, 561, 366, 406, 884, 341, 8659, 412, 264, 1623, 13], "temperature": 0.0, "avg_logprob": -0.11085655882551863, "compression_ratio": 1.601063829787234, "no_speech_prob": 7.33820570530952e-07}, {"id": 126, "seek": 61962, "start": 619.62, "end": 627.6, "text": " So for example, when I go to Amazon, I read all of Douglas Adams' books.", "tokens": [407, 337, 1365, 11, 562, 286, 352, 281, 6795, 11, 286, 1401, 439, 295, 23010, 25214, 6, 3642, 13], "temperature": 0.0, "avg_logprob": -0.12141403555870056, "compression_ratio": 1.6380090497737556, "no_speech_prob": 4.785073542734608e-06}, {"id": 127, "seek": 61962, "start": 627.6, "end": 632.94, "text": " And so having read all of Douglas Adams' books, the next time I went to Amazon, they said,", "tokens": [400, 370, 1419, 1401, 439, 295, 23010, 25214, 6, 3642, 11, 264, 958, 565, 286, 1437, 281, 6795, 11, 436, 848, 11], "temperature": 0.0, "avg_logprob": -0.12141403555870056, "compression_ratio": 1.6380090497737556, "no_speech_prob": 4.785073542734608e-06}, {"id": 128, "seek": 61962, "start": 632.94, "end": 637.16, "text": " would you like to buy the collected works of Douglas Adams?", "tokens": [576, 291, 411, 281, 2256, 264, 11087, 1985, 295, 23010, 25214, 30], "temperature": 0.0, "avg_logprob": -0.12141403555870056, "compression_ratio": 1.6380090497737556, "no_speech_prob": 4.785073542734608e-06}, {"id": 129, "seek": 61962, "start": 637.16, "end": 640.32, "text": " This is after I had bought every one of his books.", "tokens": [639, 307, 934, 286, 632, 4243, 633, 472, 295, 702, 3642, 13], "temperature": 0.0, "avg_logprob": -0.12141403555870056, "compression_ratio": 1.6380090497737556, "no_speech_prob": 4.785073542734608e-06}, {"id": 130, "seek": 61962, "start": 640.32, "end": 646.6800000000001, "text": " So from a machine learning point of view, some data scientist had said, oh, people that", "tokens": [407, 490, 257, 3479, 2539, 935, 295, 1910, 11, 512, 1412, 12662, 632, 848, 11, 1954, 11, 561, 300], "temperature": 0.0, "avg_logprob": -0.12141403555870056, "compression_ratio": 1.6380090497737556, "no_speech_prob": 4.785073542734608e-06}, {"id": 131, "seek": 64668, "start": 646.68, "end": 654.3, "text": " buy one of Douglas Adams' books often go on to buy the collected works, but recommending", "tokens": [2256, 472, 295, 23010, 25214, 6, 3642, 2049, 352, 322, 281, 2256, 264, 11087, 1985, 11, 457, 30559], "temperature": 0.0, "avg_logprob": -0.11736413052207545, "compression_ratio": 1.7595419847328244, "no_speech_prob": 7.296343028428964e-06}, {"id": 132, "seek": 64668, "start": 654.3, "end": 659.52, "text": " to me that I buy the collected works of Douglas Adams isn't smart.", "tokens": [281, 385, 300, 286, 2256, 264, 11087, 1985, 295, 23010, 25214, 1943, 380, 4069, 13], "temperature": 0.0, "avg_logprob": -0.11736413052207545, "compression_ratio": 1.7595419847328244, "no_speech_prob": 7.296343028428964e-06}, {"id": 133, "seek": 64668, "start": 659.52, "end": 662.68, "text": " And it's actually not smart at a number of levels.", "tokens": [400, 309, 311, 767, 406, 4069, 412, 257, 1230, 295, 4358, 13], "temperature": 0.0, "avg_logprob": -0.11736413052207545, "compression_ratio": 1.7595419847328244, "no_speech_prob": 7.296343028428964e-06}, {"id": 134, "seek": 64668, "start": 662.68, "end": 666.1999999999999, "text": " Not only is it unlikely I'm going to buy a box set of something of which I have everyone", "tokens": [1726, 787, 307, 309, 17518, 286, 478, 516, 281, 2256, 257, 2424, 992, 295, 746, 295, 597, 286, 362, 1518], "temperature": 0.0, "avg_logprob": -0.11736413052207545, "compression_ratio": 1.7595419847328244, "no_speech_prob": 7.296343028428964e-06}, {"id": 135, "seek": 64668, "start": 666.1999999999999, "end": 670.88, "text": " individually, but furthermore, it's not going to change my buying behavior.", "tokens": [16652, 11, 457, 3052, 3138, 11, 309, 311, 406, 516, 281, 1319, 452, 6382, 5223, 13], "temperature": 0.0, "avg_logprob": -0.11736413052207545, "compression_ratio": 1.7595419847328244, "no_speech_prob": 7.296343028428964e-06}, {"id": 136, "seek": 64668, "start": 670.88, "end": 676.18, "text": " I already know about Douglas Adams, I already know I like him, so taking up your valuable", "tokens": [286, 1217, 458, 466, 23010, 25214, 11, 286, 1217, 458, 286, 411, 796, 11, 370, 1940, 493, 428, 8263], "temperature": 0.0, "avg_logprob": -0.11736413052207545, "compression_ratio": 1.7595419847328244, "no_speech_prob": 7.296343028428964e-06}, {"id": 137, "seek": 67618, "start": 676.18, "end": 680.5799999999999, "text": " web space to tell me, hey, maybe you should buy more of the author who you're already", "tokens": [3670, 1901, 281, 980, 385, 11, 4177, 11, 1310, 291, 820, 2256, 544, 295, 264, 3793, 567, 291, 434, 1217], "temperature": 0.0, "avg_logprob": -0.10724887640579887, "compression_ratio": 1.5416666666666667, "no_speech_prob": 6.240874881768832e-06}, {"id": 138, "seek": 67618, "start": 680.5799999999999, "end": 686.7199999999999, "text": " familiar with and have bought lots of times, isn't actually going to change my behavior.", "tokens": [4963, 365, 293, 362, 4243, 3195, 295, 1413, 11, 1943, 380, 767, 516, 281, 1319, 452, 5223, 13], "temperature": 0.0, "avg_logprob": -0.10724887640579887, "compression_ratio": 1.5416666666666667, "no_speech_prob": 6.240874881768832e-06}, {"id": 139, "seek": 67618, "start": 686.7199999999999, "end": 692.52, "text": " So what if instead of creating a predictive model, Amazon had built an optimization model", "tokens": [407, 437, 498, 2602, 295, 4084, 257, 35521, 2316, 11, 6795, 632, 3094, 364, 19618, 2316], "temperature": 0.0, "avg_logprob": -0.10724887640579887, "compression_ratio": 1.5416666666666667, "no_speech_prob": 6.240874881768832e-06}, {"id": 140, "seek": 67618, "start": 692.52, "end": 699.8, "text": " that could simulate and said, if we show Jeremy this ad, how likely is he then to go on to", "tokens": [300, 727, 27817, 293, 848, 11, 498, 321, 855, 17809, 341, 614, 11, 577, 3700, 307, 415, 550, 281, 352, 322, 281], "temperature": 0.0, "avg_logprob": -0.10724887640579887, "compression_ratio": 1.5416666666666667, "no_speech_prob": 6.240874881768832e-06}, {"id": 141, "seek": 67618, "start": 699.8, "end": 701.0799999999999, "text": " buy this book?", "tokens": [2256, 341, 1446, 30], "temperature": 0.0, "avg_logprob": -0.10724887640579887, "compression_ratio": 1.5416666666666667, "no_speech_prob": 6.240874881768832e-06}, {"id": 142, "seek": 70108, "start": 701.08, "end": 706.72, "text": " And if I don't show him this ad, how likely is he to go on to buy this book?", "tokens": [400, 498, 286, 500, 380, 855, 796, 341, 614, 11, 577, 3700, 307, 415, 281, 352, 322, 281, 2256, 341, 1446, 30], "temperature": 0.0, "avg_logprob": -0.12559863380763842, "compression_ratio": 1.6447876447876448, "no_speech_prob": 5.255371888779337e-06}, {"id": 143, "seek": 70108, "start": 706.72, "end": 708.36, "text": " And so that's the counterfactual, right?", "tokens": [400, 370, 300, 311, 264, 5682, 44919, 901, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.12559863380763842, "compression_ratio": 1.6447876447876448, "no_speech_prob": 5.255371888779337e-06}, {"id": 144, "seek": 70108, "start": 708.36, "end": 711.88, "text": " The counterfactual is what would have happened otherwise.", "tokens": [440, 5682, 44919, 901, 307, 437, 576, 362, 2011, 5911, 13], "temperature": 0.0, "avg_logprob": -0.12559863380763842, "compression_ratio": 1.6447876447876448, "no_speech_prob": 5.255371888779337e-06}, {"id": 145, "seek": 70108, "start": 711.88, "end": 717.08, "text": " And then you can take the difference and say, okay, what should we recommend him that is", "tokens": [400, 550, 291, 393, 747, 264, 2649, 293, 584, 11, 1392, 11, 437, 820, 321, 2748, 796, 300, 307], "temperature": 0.0, "avg_logprob": -0.12559863380763842, "compression_ratio": 1.6447876447876448, "no_speech_prob": 5.255371888779337e-06}, {"id": 146, "seek": 70108, "start": 717.08, "end": 722.6, "text": " going to maximally change his behavior, so maximally result in more books.", "tokens": [516, 281, 5138, 379, 1319, 702, 5223, 11, 370, 5138, 379, 1874, 294, 544, 3642, 13], "temperature": 0.0, "avg_logprob": -0.12559863380763842, "compression_ratio": 1.6447876447876448, "no_speech_prob": 5.255371888779337e-06}, {"id": 147, "seek": 70108, "start": 722.6, "end": 728.12, "text": " And so you'd probably say, oh, he's never bought me Terry Pratchett books, he probably", "tokens": [400, 370, 291, 1116, 1391, 584, 11, 1954, 11, 415, 311, 1128, 4243, 385, 21983, 2114, 852, 3093, 3642, 11, 415, 1391], "temperature": 0.0, "avg_logprob": -0.12559863380763842, "compression_ratio": 1.6447876447876448, "no_speech_prob": 5.255371888779337e-06}, {"id": 148, "seek": 72812, "start": 728.12, "end": 732.76, "text": " doesn't know about Terry Pratchett, but lots of people that liked Douglas Adams did turn", "tokens": [1177, 380, 458, 466, 21983, 2114, 852, 3093, 11, 457, 3195, 295, 561, 300, 4501, 23010, 25214, 630, 1261], "temperature": 0.0, "avg_logprob": -0.1139476467864682, "compression_ratio": 1.6926406926406927, "no_speech_prob": 2.156811206077691e-06}, {"id": 149, "seek": 72812, "start": 732.76, "end": 737.36, "text": " out to like Terry Pratchett, so let's introduce him to a new author.", "tokens": [484, 281, 411, 21983, 2114, 852, 3093, 11, 370, 718, 311, 5366, 796, 281, 257, 777, 3793, 13], "temperature": 0.0, "avg_logprob": -0.1139476467864682, "compression_ratio": 1.6926406926406927, "no_speech_prob": 2.156811206077691e-06}, {"id": 150, "seek": 72812, "start": 737.36, "end": 741.48, "text": " So it's a difference between a predictive model on the one hand versus an optimization", "tokens": [407, 309, 311, 257, 2649, 1296, 257, 35521, 2316, 322, 264, 472, 1011, 5717, 364, 19618], "temperature": 0.0, "avg_logprob": -0.1139476467864682, "compression_ratio": 1.6926406926406927, "no_speech_prob": 2.156811206077691e-06}, {"id": 151, "seek": 72812, "start": 741.48, "end": 743.48, "text": " model on the other hand.", "tokens": [2316, 322, 264, 661, 1011, 13], "temperature": 0.0, "avg_logprob": -0.1139476467864682, "compression_ratio": 1.6926406926406927, "no_speech_prob": 2.156811206077691e-06}, {"id": 152, "seek": 72812, "start": 743.48, "end": 746.52, "text": " So the two tend to go hand in hand.", "tokens": [407, 264, 732, 3928, 281, 352, 1011, 294, 1011, 13], "temperature": 0.0, "avg_logprob": -0.1139476467864682, "compression_ratio": 1.6926406926406927, "no_speech_prob": 2.156811206077691e-06}, {"id": 153, "seek": 72812, "start": 746.52, "end": 754.5600000000001, "text": " The optimization model basically is saying, first of all you have a simulation model.", "tokens": [440, 19618, 2316, 1936, 307, 1566, 11, 700, 295, 439, 291, 362, 257, 16575, 2316, 13], "temperature": 0.0, "avg_logprob": -0.1139476467864682, "compression_ratio": 1.6926406926406927, "no_speech_prob": 2.156811206077691e-06}, {"id": 154, "seek": 75456, "start": 754.56, "end": 761.28, "text": " The simulation model is saying, in a world where we put Terry Pratchett's book on the", "tokens": [440, 16575, 2316, 307, 1566, 11, 294, 257, 1002, 689, 321, 829, 21983, 2114, 852, 3093, 311, 1446, 322, 264], "temperature": 0.0, "avg_logprob": -0.16112869564849552, "compression_ratio": 1.615702479338843, "no_speech_prob": 3.5559780826588394e-06}, {"id": 155, "seek": 75456, "start": 761.28, "end": 766.3599999999999, "text": " front page of Amazon for Jeremy Howard, this is what would have happened.", "tokens": [1868, 3028, 295, 6795, 337, 17809, 17626, 11, 341, 307, 437, 576, 362, 2011, 13], "temperature": 0.0, "avg_logprob": -0.16112869564849552, "compression_ratio": 1.615702479338843, "no_speech_prob": 3.5559780826588394e-06}, {"id": 156, "seek": 75456, "start": 766.3599999999999, "end": 770.68, "text": " He would have bought it with a 94% probability.", "tokens": [634, 576, 362, 4243, 309, 365, 257, 30849, 4, 8482, 13], "temperature": 0.0, "avg_logprob": -0.16112869564849552, "compression_ratio": 1.615702479338843, "no_speech_prob": 3.5559780826588394e-06}, {"id": 157, "seek": 75456, "start": 770.68, "end": 778.52, "text": " And that then tells us with this lever of like, what do I put on my home page for Jeremy", "tokens": [400, 300, 550, 5112, 505, 365, 341, 12451, 295, 411, 11, 437, 360, 286, 829, 322, 452, 1280, 3028, 337, 17809], "temperature": 0.0, "avg_logprob": -0.16112869564849552, "compression_ratio": 1.615702479338843, "no_speech_prob": 3.5559780826588394e-06}, {"id": 158, "seek": 75456, "start": 778.52, "end": 783.76, "text": " today, we say, okay, well the different settings of that lever that put Terry Pratchett on the", "tokens": [965, 11, 321, 584, 11, 1392, 11, 731, 264, 819, 6257, 295, 300, 12451, 300, 829, 21983, 2114, 852, 3093, 322, 264], "temperature": 0.0, "avg_logprob": -0.16112869564849552, "compression_ratio": 1.615702479338843, "no_speech_prob": 3.5559780826588394e-06}, {"id": 159, "seek": 78376, "start": 783.76, "end": 787.72, "text": " home page has the highest simulated outcome.", "tokens": [1280, 3028, 575, 264, 6343, 41713, 9700, 13], "temperature": 0.0, "avg_logprob": -0.16348476188127384, "compression_ratio": 1.5251141552511416, "no_speech_prob": 1.5294122022169176e-06}, {"id": 160, "seek": 78376, "start": 787.72, "end": 793.36, "text": " And then that's the thing which maximizes our profit from Jeremy's visit to Amazon.com", "tokens": [400, 550, 300, 311, 264, 551, 597, 5138, 5660, 527, 7475, 490, 17809, 311, 3441, 281, 6795, 13, 1112], "temperature": 0.0, "avg_logprob": -0.16348476188127384, "compression_ratio": 1.5251141552511416, "no_speech_prob": 1.5294122022169176e-06}, {"id": 161, "seek": 78376, "start": 793.36, "end": 795.78, "text": " today.", "tokens": [965, 13], "temperature": 0.0, "avg_logprob": -0.16348476188127384, "compression_ratio": 1.5251141552511416, "no_speech_prob": 1.5294122022169176e-06}, {"id": 162, "seek": 78376, "start": 795.78, "end": 802.24, "text": " So generally speaking, your predictive models kind of feed into this simulation model, but", "tokens": [407, 5101, 4124, 11, 428, 35521, 5245, 733, 295, 3154, 666, 341, 16575, 2316, 11, 457], "temperature": 0.0, "avg_logprob": -0.16348476188127384, "compression_ratio": 1.5251141552511416, "no_speech_prob": 1.5294122022169176e-06}, {"id": 163, "seek": 78376, "start": 802.24, "end": 805.08, "text": " you've kind of got to think about how do they all work together.", "tokens": [291, 600, 733, 295, 658, 281, 519, 466, 577, 360, 436, 439, 589, 1214, 13], "temperature": 0.0, "avg_logprob": -0.16348476188127384, "compression_ratio": 1.5251141552511416, "no_speech_prob": 1.5294122022169176e-06}, {"id": 164, "seek": 78376, "start": 805.08, "end": 808.52, "text": " So for example, let's go back to Churn.", "tokens": [407, 337, 1365, 11, 718, 311, 352, 646, 281, 761, 925, 13], "temperature": 0.0, "avg_logprob": -0.16348476188127384, "compression_ratio": 1.5251141552511416, "no_speech_prob": 1.5294122022169176e-06}, {"id": 165, "seek": 80852, "start": 808.52, "end": 815.92, "text": " So it turned out that Jeremy Howard is very likely to leave his cell phone company next", "tokens": [407, 309, 3574, 484, 300, 17809, 17626, 307, 588, 3700, 281, 1856, 702, 2815, 2593, 2237, 958], "temperature": 0.0, "avg_logprob": -0.1686361034711202, "compression_ratio": 1.542857142857143, "no_speech_prob": 8.80106927070301e-06}, {"id": 166, "seek": 80852, "start": 815.92, "end": 816.92, "text": " month.", "tokens": [1618, 13], "temperature": 0.0, "avg_logprob": -0.1686361034711202, "compression_ratio": 1.542857142857143, "no_speech_prob": 8.80106927070301e-06}, {"id": 167, "seek": 80852, "start": 816.92, "end": 818.68, "text": " What are we going to do about it?", "tokens": [708, 366, 321, 516, 281, 360, 466, 309, 30], "temperature": 0.0, "avg_logprob": -0.1686361034711202, "compression_ratio": 1.542857142857143, "no_speech_prob": 8.80106927070301e-06}, {"id": 168, "seek": 80852, "start": 818.68, "end": 821.12, "text": " Oh, let's call him.", "tokens": [876, 11, 718, 311, 818, 796, 13], "temperature": 0.0, "avg_logprob": -0.1686361034711202, "compression_ratio": 1.542857142857143, "no_speech_prob": 8.80106927070301e-06}, {"id": 169, "seek": 80852, "start": 821.12, "end": 825.5799999999999, "text": " And I can tell you, if my cell phone company calls me right now and says, just calling", "tokens": [400, 286, 393, 980, 291, 11, 498, 452, 2815, 2593, 2237, 5498, 385, 558, 586, 293, 1619, 11, 445, 5141], "temperature": 0.0, "avg_logprob": -0.1686361034711202, "compression_ratio": 1.542857142857143, "no_speech_prob": 8.80106927070301e-06}, {"id": 170, "seek": 80852, "start": 825.5799999999999, "end": 832.4, "text": " to say we love you, I'd be like, I'm canceling right now.", "tokens": [281, 584, 321, 959, 291, 11, 286, 1116, 312, 411, 11, 286, 478, 10373, 278, 558, 586, 13], "temperature": 0.0, "avg_logprob": -0.1686361034711202, "compression_ratio": 1.542857142857143, "no_speech_prob": 8.80106927070301e-06}, {"id": 171, "seek": 80852, "start": 832.4, "end": 833.96, "text": " That would be a terrible idea.", "tokens": [663, 576, 312, 257, 6237, 1558, 13], "temperature": 0.0, "avg_logprob": -0.1686361034711202, "compression_ratio": 1.542857142857143, "no_speech_prob": 8.80106927070301e-06}, {"id": 172, "seek": 83396, "start": 833.96, "end": 838.76, "text": " So again, you'd want a simulation model that says, what's the probability that Jeremy", "tokens": [407, 797, 11, 291, 1116, 528, 257, 16575, 2316, 300, 1619, 11, 437, 311, 264, 8482, 300, 17809], "temperature": 0.0, "avg_logprob": -0.13456986500666693, "compression_ratio": 1.5916666666666666, "no_speech_prob": 1.994725835174904e-06}, {"id": 173, "seek": 83396, "start": 838.76, "end": 842.8000000000001, "text": " is going to change his behavior as a result of calling him right now.", "tokens": [307, 516, 281, 1319, 702, 5223, 382, 257, 1874, 295, 5141, 796, 558, 586, 13], "temperature": 0.0, "avg_logprob": -0.13456986500666693, "compression_ratio": 1.5916666666666666, "no_speech_prob": 1.994725835174904e-06}, {"id": 174, "seek": 83396, "start": 842.8000000000001, "end": 845.4200000000001, "text": " One of the levers I have is call him.", "tokens": [1485, 295, 264, 45571, 286, 362, 307, 818, 796, 13], "temperature": 0.0, "avg_logprob": -0.13456986500666693, "compression_ratio": 1.5916666666666666, "no_speech_prob": 1.994725835174904e-06}, {"id": 175, "seek": 83396, "start": 845.4200000000001, "end": 851.5600000000001, "text": " On the other hand, if I got a piece of mail tomorrow that said, for each month you stay", "tokens": [1282, 264, 661, 1011, 11, 498, 286, 658, 257, 2522, 295, 10071, 4153, 300, 848, 11, 337, 1184, 1618, 291, 1754], "temperature": 0.0, "avg_logprob": -0.13456986500666693, "compression_ratio": 1.5916666666666666, "no_speech_prob": 1.994725835174904e-06}, {"id": 176, "seek": 83396, "start": 851.5600000000001, "end": 856.5600000000001, "text": " with us, we're going to give you $100,000, okay, then that's going to definitely change", "tokens": [365, 505, 11, 321, 434, 516, 281, 976, 291, 1848, 6879, 11, 1360, 11, 1392, 11, 550, 300, 311, 516, 281, 2138, 1319], "temperature": 0.0, "avg_logprob": -0.13456986500666693, "compression_ratio": 1.5916666666666666, "no_speech_prob": 1.994725835174904e-06}, {"id": 177, "seek": 83396, "start": 856.5600000000001, "end": 858.36, "text": " my behavior.", "tokens": [452, 5223, 13], "temperature": 0.0, "avg_logprob": -0.13456986500666693, "compression_ratio": 1.5916666666666666, "no_speech_prob": 1.994725835174904e-06}, {"id": 178, "seek": 85836, "start": 858.36, "end": 864.64, "text": " But then, feeding that into the simulation model, it turns out that overall that would", "tokens": [583, 550, 11, 12919, 300, 666, 264, 16575, 2316, 11, 309, 4523, 484, 300, 4787, 300, 576], "temperature": 0.0, "avg_logprob": -0.15137851503160266, "compression_ratio": 1.6333333333333333, "no_speech_prob": 1.7061804555851268e-06}, {"id": 179, "seek": 85836, "start": 864.64, "end": 867.28, "text": " be an unprofitable choice to make.", "tokens": [312, 364, 517, 14583, 712, 3922, 281, 652, 13], "temperature": 0.0, "avg_logprob": -0.15137851503160266, "compression_ratio": 1.6333333333333333, "no_speech_prob": 1.7061804555851268e-06}, {"id": 180, "seek": 85836, "start": 867.28, "end": 874.2, "text": " So do you see how all this fits in together?", "tokens": [407, 360, 291, 536, 577, 439, 341, 9001, 294, 1214, 30], "temperature": 0.0, "avg_logprob": -0.15137851503160266, "compression_ratio": 1.6333333333333333, "no_speech_prob": 1.7061804555851268e-06}, {"id": 181, "seek": 85836, "start": 874.2, "end": 881.7, "text": " So when we look at something like Churn, we want to be thinking, what are the levers we", "tokens": [407, 562, 321, 574, 412, 746, 411, 761, 925, 11, 321, 528, 281, 312, 1953, 11, 437, 366, 264, 45571, 321], "temperature": 0.0, "avg_logprob": -0.15137851503160266, "compression_ratio": 1.6333333333333333, "no_speech_prob": 1.7061804555851268e-06}, {"id": 182, "seek": 85836, "start": 881.7, "end": 887.14, "text": " can pull, and so what are the kind of models that we could build with what kinds of data", "tokens": [393, 2235, 11, 293, 370, 437, 366, 264, 733, 295, 5245, 300, 321, 727, 1322, 365, 437, 3685, 295, 1412], "temperature": 0.0, "avg_logprob": -0.15137851503160266, "compression_ratio": 1.6333333333333333, "no_speech_prob": 1.7061804555851268e-06}, {"id": 183, "seek": 88714, "start": 887.14, "end": 891.76, "text": " to help us pull those levers better to achieve our objectives.", "tokens": [281, 854, 505, 2235, 729, 45571, 1101, 281, 4584, 527, 15961, 13], "temperature": 0.0, "avg_logprob": -0.13139487625262058, "compression_ratio": 1.748091603053435, "no_speech_prob": 5.093681920698145e-06}, {"id": 184, "seek": 88714, "start": 891.76, "end": 897.3, "text": " And so when you think about it that way, you realize that the vast majority of these applications", "tokens": [400, 370, 562, 291, 519, 466, 309, 300, 636, 11, 291, 4325, 300, 264, 8369, 6286, 295, 613, 5821], "temperature": 0.0, "avg_logprob": -0.13139487625262058, "compression_ratio": 1.748091603053435, "no_speech_prob": 5.093681920698145e-06}, {"id": 185, "seek": 88714, "start": 897.3, "end": 903.0, "text": " are not largely about a predictive model at all, they're about interpretation.", "tokens": [366, 406, 11611, 466, 257, 35521, 2316, 412, 439, 11, 436, 434, 466, 14174, 13], "temperature": 0.0, "avg_logprob": -0.13139487625262058, "compression_ratio": 1.748091603053435, "no_speech_prob": 5.093681920698145e-06}, {"id": 186, "seek": 88714, "start": 903.0, "end": 907.4399999999999, "text": " They're about understanding what happens if.", "tokens": [814, 434, 466, 3701, 437, 2314, 498, 13], "temperature": 0.0, "avg_logprob": -0.13139487625262058, "compression_ratio": 1.748091603053435, "no_speech_prob": 5.093681920698145e-06}, {"id": 187, "seek": 88714, "start": 907.4399999999999, "end": 912.48, "text": " So if we kind of take the cross-product, not the cross-product, sorry, the intersection", "tokens": [407, 498, 321, 733, 295, 747, 264, 3278, 12, 33244, 11, 406, 264, 3278, 12, 33244, 11, 2597, 11, 264, 15236], "temperature": 0.0, "avg_logprob": -0.13139487625262058, "compression_ratio": 1.748091603053435, "no_speech_prob": 5.093681920698145e-06}, {"id": 188, "seek": 88714, "start": 912.48, "end": 916.78, "text": " between on the one hand, here are all the levers that we could pull, here are all the", "tokens": [1296, 322, 264, 472, 1011, 11, 510, 366, 439, 264, 45571, 300, 321, 727, 2235, 11, 510, 366, 439, 264], "temperature": 0.0, "avg_logprob": -0.13139487625262058, "compression_ratio": 1.748091603053435, "no_speech_prob": 5.093681920698145e-06}, {"id": 189, "seek": 91678, "start": 916.78, "end": 921.8, "text": " things we can do, and then here are all of the features from our random forest feature", "tokens": [721, 321, 393, 360, 11, 293, 550, 510, 366, 439, 295, 264, 4122, 490, 527, 4974, 6719, 4111], "temperature": 0.0, "avg_logprob": -0.08969626888152092, "compression_ratio": 1.8396226415094339, "no_speech_prob": 8.530280865670647e-06}, {"id": 190, "seek": 91678, "start": 921.8, "end": 927.0799999999999, "text": " importance that turn out to be strong drivers of the outcome, and so then the intersection", "tokens": [7379, 300, 1261, 484, 281, 312, 2068, 11590, 295, 264, 9700, 11, 293, 370, 550, 264, 15236], "temperature": 0.0, "avg_logprob": -0.08969626888152092, "compression_ratio": 1.8396226415094339, "no_speech_prob": 8.530280865670647e-06}, {"id": 191, "seek": 91678, "start": 927.0799999999999, "end": 933.54, "text": " of those is here are the levers we could pull that actually matter.", "tokens": [295, 729, 307, 510, 366, 264, 45571, 321, 727, 2235, 300, 767, 1871, 13], "temperature": 0.0, "avg_logprob": -0.08969626888152092, "compression_ratio": 1.8396226415094339, "no_speech_prob": 8.530280865670647e-06}, {"id": 192, "seek": 91678, "start": 933.54, "end": 939.4399999999999, "text": " Because if you can't change the thing, then it's not very interesting, and if it's not", "tokens": [1436, 498, 291, 393, 380, 1319, 264, 551, 11, 550, 309, 311, 406, 588, 1880, 11, 293, 498, 309, 311, 406], "temperature": 0.0, "avg_logprob": -0.08969626888152092, "compression_ratio": 1.8396226415094339, "no_speech_prob": 8.530280865670647e-06}, {"id": 193, "seek": 91678, "start": 939.4399999999999, "end": 942.92, "text": " actually a significant driver, it's not very interesting.", "tokens": [767, 257, 4776, 6787, 11, 309, 311, 406, 588, 1880, 13], "temperature": 0.0, "avg_logprob": -0.08969626888152092, "compression_ratio": 1.8396226415094339, "no_speech_prob": 8.530280865670647e-06}, {"id": 194, "seek": 94292, "start": 942.92, "end": 948.9399999999999, "text": " So we can actually use our random forest feature importance to tell us what can we actually", "tokens": [407, 321, 393, 767, 764, 527, 4974, 6719, 4111, 7379, 281, 980, 505, 437, 393, 321, 767], "temperature": 0.0, "avg_logprob": -0.1426376963770667, "compression_ratio": 1.6330275229357798, "no_speech_prob": 1.3287741467138403e-06}, {"id": 195, "seek": 94292, "start": 948.9399999999999, "end": 951.8199999999999, "text": " do to make a difference.", "tokens": [360, 281, 652, 257, 2649, 13], "temperature": 0.0, "avg_logprob": -0.1426376963770667, "compression_ratio": 1.6330275229357798, "no_speech_prob": 1.3287741467138403e-06}, {"id": 196, "seek": 94292, "start": 951.8199999999999, "end": 955.4799999999999, "text": " And then we can use the partial dependence to actually build this kind of simulation", "tokens": [400, 550, 321, 393, 764, 264, 14641, 31704, 281, 767, 1322, 341, 733, 295, 16575], "temperature": 0.0, "avg_logprob": -0.1426376963770667, "compression_ratio": 1.6330275229357798, "no_speech_prob": 1.3287741467138403e-06}, {"id": 197, "seek": 94292, "start": 955.4799999999999, "end": 962.76, "text": " model to say, okay, well if we did change that, what would happen?", "tokens": [2316, 281, 584, 11, 1392, 11, 731, 498, 321, 630, 1319, 300, 11, 437, 576, 1051, 30], "temperature": 0.0, "avg_logprob": -0.1426376963770667, "compression_ratio": 1.6330275229357798, "no_speech_prob": 1.3287741467138403e-06}, {"id": 198, "seek": 94292, "start": 962.76, "end": 968.64, "text": " So there are examples, lots and lots of these vertical examples, and so what I want you", "tokens": [407, 456, 366, 5110, 11, 3195, 293, 3195, 295, 613, 9429, 5110, 11, 293, 370, 437, 286, 528, 291], "temperature": 0.0, "avg_logprob": -0.1426376963770667, "compression_ratio": 1.6330275229357798, "no_speech_prob": 1.3287741467138403e-06}, {"id": 199, "seek": 96864, "start": 968.64, "end": 973.28, "text": " to kind of think about as you think about the machine learning problems you're working", "tokens": [281, 733, 295, 519, 466, 382, 291, 519, 466, 264, 3479, 2539, 2740, 291, 434, 1364], "temperature": 0.0, "avg_logprob": -0.16118038617647612, "compression_ratio": 1.7131474103585658, "no_speech_prob": 4.425474799063522e-06}, {"id": 200, "seek": 96864, "start": 973.28, "end": 978.68, "text": " on is like, why does somebody care about this, right?", "tokens": [322, 307, 411, 11, 983, 775, 2618, 1127, 466, 341, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.16118038617647612, "compression_ratio": 1.7131474103585658, "no_speech_prob": 4.425474799063522e-06}, {"id": 201, "seek": 96864, "start": 978.68, "end": 981.96, "text": " And like what would a good answer to them look like?", "tokens": [400, 411, 437, 576, 257, 665, 1867, 281, 552, 574, 411, 30], "temperature": 0.0, "avg_logprob": -0.16118038617647612, "compression_ratio": 1.7131474103585658, "no_speech_prob": 4.425474799063522e-06}, {"id": 202, "seek": 96864, "start": 981.96, "end": 985.48, "text": " And how could you actually positively impact this business?", "tokens": [400, 577, 727, 291, 767, 25795, 2712, 341, 1606, 30], "temperature": 0.0, "avg_logprob": -0.16118038617647612, "compression_ratio": 1.7131474103585658, "no_speech_prob": 4.425474799063522e-06}, {"id": 203, "seek": 96864, "start": 985.48, "end": 991.8199999999999, "text": " So if you're creating like a Kaggle kernel, try to think about from the point of view", "tokens": [407, 498, 291, 434, 4084, 411, 257, 48751, 22631, 28256, 11, 853, 281, 519, 466, 490, 264, 935, 295, 1910], "temperature": 0.0, "avg_logprob": -0.16118038617647612, "compression_ratio": 1.7131474103585658, "no_speech_prob": 4.425474799063522e-06}, {"id": 204, "seek": 96864, "start": 991.8199999999999, "end": 997.12, "text": " of the competition organizer, like what would they want to know, and how can you give them", "tokens": [295, 264, 6211, 41363, 11, 411, 437, 576, 436, 528, 281, 458, 11, 293, 577, 393, 291, 976, 552], "temperature": 0.0, "avg_logprob": -0.16118038617647612, "compression_ratio": 1.7131474103585658, "no_speech_prob": 4.425474799063522e-06}, {"id": 205, "seek": 99712, "start": 997.12, "end": 999.02, "text": " that information?", "tokens": [300, 1589, 30], "temperature": 0.0, "avg_logprob": -0.11528739772859167, "compression_ratio": 1.8244274809160306, "no_speech_prob": 2.1233756797300885e-06}, {"id": 206, "seek": 99712, "start": 999.02, "end": 1006.12, "text": " So something like fraud detection, on the other hand, you probably just basically want", "tokens": [407, 746, 411, 14560, 17784, 11, 322, 264, 661, 1011, 11, 291, 1391, 445, 1936, 528], "temperature": 0.0, "avg_logprob": -0.11528739772859167, "compression_ratio": 1.8244274809160306, "no_speech_prob": 2.1233756797300885e-06}, {"id": 207, "seek": 99712, "start": 1006.12, "end": 1009.34, "text": " to know who's fraudulent, right?", "tokens": [281, 458, 567, 311, 14560, 23405, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.11528739772859167, "compression_ratio": 1.8244274809160306, "no_speech_prob": 2.1233756797300885e-06}, {"id": 208, "seek": 99712, "start": 1009.34, "end": 1012.88, "text": " So you probably do just care about the predictive model, but then you do have to think carefully", "tokens": [407, 291, 1391, 360, 445, 1127, 466, 264, 35521, 2316, 11, 457, 550, 291, 360, 362, 281, 519, 7500], "temperature": 0.0, "avg_logprob": -0.11528739772859167, "compression_ratio": 1.8244274809160306, "no_speech_prob": 2.1233756797300885e-06}, {"id": 209, "seek": 99712, "start": 1012.88, "end": 1015.02, "text": " about the data availability here.", "tokens": [466, 264, 1412, 17945, 510, 13], "temperature": 0.0, "avg_logprob": -0.11528739772859167, "compression_ratio": 1.8244274809160306, "no_speech_prob": 2.1233756797300885e-06}, {"id": 210, "seek": 99712, "start": 1015.02, "end": 1020.44, "text": " So it's like, okay, but we need to know who's fraudulent at the point that we're about to", "tokens": [407, 309, 311, 411, 11, 1392, 11, 457, 321, 643, 281, 458, 567, 311, 14560, 23405, 412, 264, 935, 300, 321, 434, 466, 281], "temperature": 0.0, "avg_logprob": -0.11528739772859167, "compression_ratio": 1.8244274809160306, "no_speech_prob": 2.1233756797300885e-06}, {"id": 211, "seek": 99712, "start": 1020.44, "end": 1022.8, "text": " deliver them a product, right?", "tokens": [4239, 552, 257, 1674, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.11528739772859167, "compression_ratio": 1.8244274809160306, "no_speech_prob": 2.1233756797300885e-06}, {"id": 212, "seek": 99712, "start": 1022.8, "end": 1027.08, "text": " So it's no point like looking at data that's available like a month later, for instance.", "tokens": [407, 309, 311, 572, 935, 411, 1237, 412, 1412, 300, 311, 2435, 411, 257, 1618, 1780, 11, 337, 5197, 13], "temperature": 0.0, "avg_logprob": -0.11528739772859167, "compression_ratio": 1.8244274809160306, "no_speech_prob": 2.1233756797300885e-06}, {"id": 213, "seek": 102708, "start": 1027.08, "end": 1033.04, "text": " So you've kind of got this key issue of thinking about the actual operational constraints that", "tokens": [407, 291, 600, 733, 295, 658, 341, 2141, 2734, 295, 1953, 466, 264, 3539, 16607, 18491, 300], "temperature": 0.0, "avg_logprob": -0.1744353968069094, "compression_ratio": 1.631578947368421, "no_speech_prob": 1.0289461897627916e-05}, {"id": 214, "seek": 102708, "start": 1033.04, "end": 1037.6399999999999, "text": " you're working under.", "tokens": [291, 434, 1364, 833, 13], "temperature": 0.0, "avg_logprob": -0.1744353968069094, "compression_ratio": 1.631578947368421, "no_speech_prob": 1.0289461897627916e-05}, {"id": 215, "seek": 102708, "start": 1037.6399999999999, "end": 1043.48, "text": " You know, lots of interesting applications in human resources, but like employee churn,", "tokens": [509, 458, 11, 3195, 295, 1880, 5821, 294, 1952, 3593, 11, 457, 411, 10738, 417, 925, 11], "temperature": 0.0, "avg_logprob": -0.1744353968069094, "compression_ratio": 1.631578947368421, "no_speech_prob": 1.0289461897627916e-05}, {"id": 216, "seek": 102708, "start": 1043.48, "end": 1048.8799999999999, "text": " it's another kind of churn model where finding out that Jeremy Howard's sick of lecturing,", "tokens": [309, 311, 1071, 733, 295, 417, 925, 2316, 689, 5006, 484, 300, 17809, 17626, 311, 4998, 295, 5899, 1345, 11], "temperature": 0.0, "avg_logprob": -0.1744353968069094, "compression_ratio": 1.631578947368421, "no_speech_prob": 1.0289461897627916e-05}, {"id": 217, "seek": 102708, "start": 1048.8799999999999, "end": 1052.12, "text": " he's going to leave tomorrow, what are you going to do about it?", "tokens": [415, 311, 516, 281, 1856, 4153, 11, 437, 366, 291, 516, 281, 360, 466, 309, 30], "temperature": 0.0, "avg_logprob": -0.1744353968069094, "compression_ratio": 1.631578947368421, "no_speech_prob": 1.0289461897627916e-05}, {"id": 218, "seek": 102708, "start": 1052.12, "end": 1054.24, "text": " Well, knowing that wouldn't actually be helpful.", "tokens": [1042, 11, 5276, 300, 2759, 380, 767, 312, 4961, 13], "temperature": 0.0, "avg_logprob": -0.1744353968069094, "compression_ratio": 1.631578947368421, "no_speech_prob": 1.0289461897627916e-05}, {"id": 219, "seek": 102708, "start": 1054.24, "end": 1056.0, "text": " It'd be too late, right?", "tokens": [467, 1116, 312, 886, 3469, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1744353968069094, "compression_ratio": 1.631578947368421, "no_speech_prob": 1.0289461897627916e-05}, {"id": 220, "seek": 105600, "start": 1056.0, "end": 1061.92, "text": " You would actually want a model that said, what kinds of people are leaving USF?", "tokens": [509, 576, 767, 528, 257, 2316, 300, 848, 11, 437, 3685, 295, 561, 366, 5012, 2546, 37, 30], "temperature": 0.0, "avg_logprob": -0.15411734094425122, "compression_ratio": 1.5809128630705394, "no_speech_prob": 5.594248705165228e-06}, {"id": 221, "seek": 105600, "start": 1061.92, "end": 1066.96, "text": " And it turns out that like, oh, everybody that goes to the downstairs cafe leaves USF,", "tokens": [400, 309, 4523, 484, 300, 411, 11, 1954, 11, 2201, 300, 1709, 281, 264, 20148, 17773, 5510, 2546, 37, 11], "temperature": 0.0, "avg_logprob": -0.15411734094425122, "compression_ratio": 1.5809128630705394, "no_speech_prob": 5.594248705165228e-06}, {"id": 222, "seek": 105600, "start": 1066.96, "end": 1071.2, "text": " you know, I guess their food is awful, or whatever, right?", "tokens": [291, 458, 11, 286, 2041, 641, 1755, 307, 11232, 11, 420, 2035, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.15411734094425122, "compression_ratio": 1.5809128630705394, "no_speech_prob": 5.594248705165228e-06}, {"id": 223, "seek": 105600, "start": 1071.2, "end": 1075.48, "text": " Or everybody that we're paying less than half a million dollars a year is leaving USF, you", "tokens": [1610, 2201, 300, 321, 434, 6229, 1570, 813, 1922, 257, 2459, 3808, 257, 1064, 307, 5012, 2546, 37, 11, 291], "temperature": 0.0, "avg_logprob": -0.15411734094425122, "compression_ratio": 1.5809128630705394, "no_speech_prob": 5.594248705165228e-06}, {"id": 224, "seek": 105600, "start": 1075.48, "end": 1080.48, "text": " know, because they can't afford basic housing in San Francisco.", "tokens": [458, 11, 570, 436, 393, 380, 6157, 3875, 6849, 294, 5271, 12279, 13], "temperature": 0.0, "avg_logprob": -0.15411734094425122, "compression_ratio": 1.5809128630705394, "no_speech_prob": 5.594248705165228e-06}, {"id": 225, "seek": 108048, "start": 1080.48, "end": 1085.88, "text": " So like you could use your employee churn model, not so much to say like which employees", "tokens": [407, 411, 291, 727, 764, 428, 10738, 417, 925, 2316, 11, 406, 370, 709, 281, 584, 411, 597, 6619], "temperature": 0.0, "avg_logprob": -0.20463295905820786, "compression_ratio": 1.5, "no_speech_prob": 1.392540980305057e-06}, {"id": 226, "seek": 108048, "start": 1085.88, "end": 1092.4, "text": " hide us, but why do employees leave?", "tokens": [6479, 505, 11, 457, 983, 360, 6619, 1856, 30], "temperature": 0.0, "avg_logprob": -0.20463295905820786, "compression_ratio": 1.5, "no_speech_prob": 1.392540980305057e-06}, {"id": 227, "seek": 108048, "start": 1092.4, "end": 1101.24, "text": " And so again, it's really the interpretation there that matters.", "tokens": [400, 370, 797, 11, 309, 311, 534, 264, 14174, 456, 300, 7001, 13], "temperature": 0.0, "avg_logprob": -0.20463295905820786, "compression_ratio": 1.5, "no_speech_prob": 1.392540980305057e-06}, {"id": 228, "seek": 108048, "start": 1101.24, "end": 1103.56, "text": " Lead prioritization is a really interesting one, right?", "tokens": [31025, 14846, 2144, 307, 257, 534, 1880, 472, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.20463295905820786, "compression_ratio": 1.5, "no_speech_prob": 1.392540980305057e-06}, {"id": 229, "seek": 110356, "start": 1103.56, "end": 1110.56, "text": " This is one where a lot of companies, yes, Dana, can you pass that over there?", "tokens": [639, 307, 472, 689, 257, 688, 295, 3431, 11, 2086, 11, 23759, 11, 393, 291, 1320, 300, 670, 456, 30], "temperature": 0.0, "avg_logprob": -0.36009395954220796, "compression_ratio": 1.25, "no_speech_prob": 3.844912953354651e-06}, {"id": 230, "seek": 110356, "start": 1110.56, "end": 1129.3999999999999, "text": " Yeah, exactly.", "tokens": [865, 11, 2293, 13], "temperature": 0.0, "avg_logprob": -0.36009395954220796, "compression_ratio": 1.25, "no_speech_prob": 3.844912953354651e-06}, {"id": 231, "seek": 110356, "start": 1129.3999999999999, "end": 1133.12, "text": " So this is what this simulation model is all about.", "tokens": [407, 341, 307, 437, 341, 16575, 2316, 307, 439, 466, 13], "temperature": 0.0, "avg_logprob": -0.36009395954220796, "compression_ratio": 1.25, "no_speech_prob": 3.844912953354651e-06}, {"id": 232, "seek": 113312, "start": 1133.12, "end": 1134.26, "text": " So it's a great question.", "tokens": [407, 309, 311, 257, 869, 1168, 13], "temperature": 0.0, "avg_logprob": -0.1733605298128995, "compression_ratio": 1.7773584905660378, "no_speech_prob": 3.0717801564605907e-05}, {"id": 233, "seek": 113312, "start": 1134.26, "end": 1139.8799999999999, "text": " So like you kind of figure out this objective we're trying to maximize, which is like company", "tokens": [407, 411, 291, 733, 295, 2573, 484, 341, 10024, 321, 434, 1382, 281, 19874, 11, 597, 307, 411, 2237], "temperature": 0.0, "avg_logprob": -0.1733605298128995, "compression_ratio": 1.7773584905660378, "no_speech_prob": 3.0717801564605907e-05}, {"id": 234, "seek": 113312, "start": 1139.8799999999999, "end": 1144.9599999999998, "text": " profitability, you can kind of create like a pretty simple like Excel model or something", "tokens": [46249, 11, 291, 393, 733, 295, 1884, 411, 257, 1238, 2199, 411, 19060, 2316, 420, 746], "temperature": 0.0, "avg_logprob": -0.1733605298128995, "compression_ratio": 1.7773584905660378, "no_speech_prob": 3.0717801564605907e-05}, {"id": 235, "seek": 113312, "start": 1144.9599999999998, "end": 1149.4799999999998, "text": " that says like here's the revenues and here's the costs and the cost is equal to the number", "tokens": [300, 1619, 411, 510, 311, 264, 27299, 293, 510, 311, 264, 5497, 293, 264, 2063, 307, 2681, 281, 264, 1230], "temperature": 0.0, "avg_logprob": -0.1733605298128995, "compression_ratio": 1.7773584905660378, "no_speech_prob": 3.0717801564605907e-05}, {"id": 236, "seek": 113312, "start": 1149.4799999999998, "end": 1153.84, "text": " of people we employ multiplied by their salaries, blah blah blah blah blah blah.", "tokens": [295, 561, 321, 3188, 17207, 538, 641, 35057, 11, 12288, 12288, 12288, 12288, 12288, 12288, 13], "temperature": 0.0, "avg_logprob": -0.1733605298128995, "compression_ratio": 1.7773584905660378, "no_speech_prob": 3.0717801564605907e-05}, {"id": 237, "seek": 113312, "start": 1153.84, "end": 1159.9199999999998, "text": " And so inside that kind of Excel model, there are certain cells, there are certain inputs", "tokens": [400, 370, 1854, 300, 733, 295, 19060, 2316, 11, 456, 366, 1629, 5438, 11, 456, 366, 1629, 15743], "temperature": 0.0, "avg_logprob": -0.1733605298128995, "compression_ratio": 1.7773584905660378, "no_speech_prob": 3.0717801564605907e-05}, {"id": 238, "seek": 115992, "start": 1159.92, "end": 1165.92, "text": " where you're like oh that thing's kind of stochastic, or that thing is kind of uncertain", "tokens": [689, 291, 434, 411, 1954, 300, 551, 311, 733, 295, 342, 8997, 2750, 11, 420, 300, 551, 307, 733, 295, 11308], "temperature": 0.0, "avg_logprob": -0.15929072380065917, "compression_ratio": 1.8571428571428572, "no_speech_prob": 4.785081728186924e-06}, {"id": 239, "seek": 115992, "start": 1165.92, "end": 1168.04, "text": " but we could predict it with a model.", "tokens": [457, 321, 727, 6069, 309, 365, 257, 2316, 13], "temperature": 0.0, "avg_logprob": -0.15929072380065917, "compression_ratio": 1.8571428571428572, "no_speech_prob": 4.785081728186924e-06}, {"id": 240, "seek": 115992, "start": 1168.04, "end": 1174.64, "text": " And so that's kind of what I do then, is I ask them, we need a predictive model for how", "tokens": [400, 370, 300, 311, 733, 295, 437, 286, 360, 550, 11, 307, 286, 1029, 552, 11, 321, 643, 257, 35521, 2316, 337, 577], "temperature": 0.0, "avg_logprob": -0.15929072380065917, "compression_ratio": 1.8571428571428572, "no_speech_prob": 4.785081728186924e-06}, {"id": 241, "seek": 115992, "start": 1174.64, "end": 1181.3600000000001, "text": " likely somebody is to stay if we change their salary, how likely they are to leave with", "tokens": [3700, 2618, 307, 281, 1754, 498, 321, 1319, 641, 15360, 11, 577, 3700, 436, 366, 281, 1856, 365], "temperature": 0.0, "avg_logprob": -0.15929072380065917, "compression_ratio": 1.8571428571428572, "no_speech_prob": 4.785081728186924e-06}, {"id": 242, "seek": 115992, "start": 1181.3600000000001, "end": 1188.4, "text": " their current salary, how likely they are to leave next year if I increase their salary", "tokens": [641, 2190, 15360, 11, 577, 3700, 436, 366, 281, 1856, 958, 1064, 498, 286, 3488, 641, 15360], "temperature": 0.0, "avg_logprob": -0.15929072380065917, "compression_ratio": 1.8571428571428572, "no_speech_prob": 4.785081728186924e-06}, {"id": 243, "seek": 118840, "start": 1188.4, "end": 1190.0400000000002, "text": " now blah blah blah.", "tokens": [586, 12288, 12288, 12288, 13], "temperature": 0.0, "avg_logprob": -0.16512976522031036, "compression_ratio": 1.669811320754717, "no_speech_prob": 4.1573543967388105e-06}, {"id": 244, "seek": 118840, "start": 1190.0400000000002, "end": 1194.4, "text": " So you kind of build a bunch of these models and then you can combine them together with", "tokens": [407, 291, 733, 295, 1322, 257, 3840, 295, 613, 5245, 293, 550, 291, 393, 10432, 552, 1214, 365], "temperature": 0.0, "avg_logprob": -0.16512976522031036, "compression_ratio": 1.669811320754717, "no_speech_prob": 4.1573543967388105e-06}, {"id": 245, "seek": 118840, "start": 1194.4, "end": 1199.3200000000002, "text": " simple business logic and then you can optimize that.", "tokens": [2199, 1606, 9952, 293, 550, 291, 393, 19719, 300, 13], "temperature": 0.0, "avg_logprob": -0.16512976522031036, "compression_ratio": 1.669811320754717, "no_speech_prob": 4.1573543967388105e-06}, {"id": 246, "seek": 118840, "start": 1199.3200000000002, "end": 1205.0, "text": " You can then say okay, if I pay Jeremy Howard half a million dollars, that's probably a", "tokens": [509, 393, 550, 584, 1392, 11, 498, 286, 1689, 17809, 17626, 1922, 257, 2459, 3808, 11, 300, 311, 1391, 257], "temperature": 0.0, "avg_logprob": -0.16512976522031036, "compression_ratio": 1.669811320754717, "no_speech_prob": 4.1573543967388105e-06}, {"id": 247, "seek": 118840, "start": 1205.0, "end": 1209.48, "text": " really good idea, and if I pay him less, then it's probably not.", "tokens": [534, 665, 1558, 11, 293, 498, 286, 1689, 796, 1570, 11, 550, 309, 311, 1391, 406, 13], "temperature": 0.0, "avg_logprob": -0.16512976522031036, "compression_ratio": 1.669811320754717, "no_speech_prob": 4.1573543967388105e-06}, {"id": 248, "seek": 118840, "start": 1209.48, "end": 1212.98, "text": " You can figure out the overall impact.", "tokens": [509, 393, 2573, 484, 264, 4787, 2712, 13], "temperature": 0.0, "avg_logprob": -0.16512976522031036, "compression_ratio": 1.669811320754717, "no_speech_prob": 4.1573543967388105e-06}, {"id": 249, "seek": 121298, "start": 1212.98, "end": 1221.04, "text": " So it's really shocking to me how few people do this, like most people in industry measure", "tokens": [407, 309, 311, 534, 18776, 281, 385, 577, 1326, 561, 360, 341, 11, 411, 881, 561, 294, 3518, 3481], "temperature": 0.0, "avg_logprob": -0.17140786988394602, "compression_ratio": 1.3375796178343948, "no_speech_prob": 6.240868060558569e-06}, {"id": 250, "seek": 121298, "start": 1221.04, "end": 1231.64, "text": " their models using like AUC or RMSE or whatever, which is never actually what you want.", "tokens": [641, 5245, 1228, 411, 7171, 34, 420, 23790, 5879, 420, 2035, 11, 597, 307, 1128, 767, 437, 291, 528, 13], "temperature": 0.0, "avg_logprob": -0.17140786988394602, "compression_ratio": 1.3375796178343948, "no_speech_prob": 6.240868060558569e-06}, {"id": 251, "seek": 121298, "start": 1231.64, "end": 1238.32, "text": " Yes, can you pass it over here?", "tokens": [1079, 11, 393, 291, 1320, 309, 670, 510, 30], "temperature": 0.0, "avg_logprob": -0.17140786988394602, "compression_ratio": 1.3375796178343948, "no_speech_prob": 6.240868060558569e-06}, {"id": 252, "seek": 123832, "start": 1238.32, "end": 1244.8799999999999, "text": " I wanted to stress the point that you made before, in my experience, a lot of the problem", "tokens": [286, 1415, 281, 4244, 264, 935, 300, 291, 1027, 949, 11, 294, 452, 1752, 11, 257, 688, 295, 264, 1154], "temperature": 0.0, "avg_logprob": -0.12694481924070533, "compression_ratio": 1.8402777777777777, "no_speech_prob": 3.425816976232454e-05}, {"id": 253, "seek": 123832, "start": 1244.8799999999999, "end": 1247.24, "text": " was to define the problem, right?", "tokens": [390, 281, 6964, 264, 1154, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.12694481924070533, "compression_ratio": 1.8402777777777777, "no_speech_prob": 3.425816976232454e-05}, {"id": 254, "seek": 123832, "start": 1247.24, "end": 1250.8799999999999, "text": " So you are in a company, you are talking to somebody that doesn't have like this mentality", "tokens": [407, 291, 366, 294, 257, 2237, 11, 291, 366, 1417, 281, 2618, 300, 1177, 380, 362, 411, 341, 21976], "temperature": 0.0, "avg_logprob": -0.12694481924070533, "compression_ratio": 1.8402777777777777, "no_speech_prob": 3.425816976232454e-05}, {"id": 255, "seek": 123832, "start": 1250.8799999999999, "end": 1251.8799999999999, "text": " that you have.", "tokens": [300, 291, 362, 13], "temperature": 0.0, "avg_logprob": -0.12694481924070533, "compression_ratio": 1.8402777777777777, "no_speech_prob": 3.425816976232454e-05}, {"id": 256, "seek": 123832, "start": 1251.8799999999999, "end": 1254.9199999999998, "text": " They don't know that you have to have X and Y and so on.", "tokens": [814, 500, 380, 458, 300, 291, 362, 281, 362, 1783, 293, 398, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.12694481924070533, "compression_ratio": 1.8402777777777777, "no_speech_prob": 3.425816976232454e-05}, {"id": 257, "seek": 123832, "start": 1254.9199999999998, "end": 1257.36, "text": " So you have to try to get that out of them.", "tokens": [407, 291, 362, 281, 853, 281, 483, 300, 484, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.12694481924070533, "compression_ratio": 1.8402777777777777, "no_speech_prob": 3.425816976232454e-05}, {"id": 258, "seek": 123832, "start": 1257.36, "end": 1258.36, "text": " What exactly do you want?", "tokens": [708, 2293, 360, 291, 528, 30], "temperature": 0.0, "avg_logprob": -0.12694481924070533, "compression_ratio": 1.8402777777777777, "no_speech_prob": 3.425816976232454e-05}, {"id": 259, "seek": 123832, "start": 1258.36, "end": 1263.2, "text": " And try to go through a few iterations of understanding what they want and then you", "tokens": [400, 853, 281, 352, 807, 257, 1326, 36540, 295, 3701, 437, 436, 528, 293, 550, 291], "temperature": 0.0, "avg_logprob": -0.12694481924070533, "compression_ratio": 1.8402777777777777, "no_speech_prob": 3.425816976232454e-05}, {"id": 260, "seek": 123832, "start": 1263.2, "end": 1266.76, "text": " know the data, you know where the data is, you know actually where you can measure, which", "tokens": [458, 264, 1412, 11, 291, 458, 689, 264, 1412, 307, 11, 291, 458, 767, 689, 291, 393, 3481, 11, 597], "temperature": 0.0, "avg_logprob": -0.12694481924070533, "compression_ratio": 1.8402777777777777, "no_speech_prob": 3.425816976232454e-05}, {"id": 261, "seek": 126676, "start": 1266.76, "end": 1268.44, "text": " is often know what they want.", "tokens": [307, 2049, 458, 437, 436, 528, 13], "temperature": 0.0, "avg_logprob": -0.1542713805919385, "compression_ratio": 1.8671875, "no_speech_prob": 0.00020236230921000242}, {"id": 262, "seek": 126676, "start": 1268.44, "end": 1271.2, "text": " So you have to kind of get a proxy for what they want.", "tokens": [407, 291, 362, 281, 733, 295, 483, 257, 29690, 337, 437, 436, 528, 13], "temperature": 0.0, "avg_logprob": -0.1542713805919385, "compression_ratio": 1.8671875, "no_speech_prob": 0.00020236230921000242}, {"id": 263, "seek": 126676, "start": 1271.2, "end": 1276.0, "text": " And then so a lot of what you do is not that much of like, well, some people do actually", "tokens": [400, 550, 370, 257, 688, 295, 437, 291, 360, 307, 406, 300, 709, 295, 411, 11, 731, 11, 512, 561, 360, 767], "temperature": 0.0, "avg_logprob": -0.1542713805919385, "compression_ratio": 1.8671875, "no_speech_prob": 0.00020236230921000242}, {"id": 264, "seek": 126676, "start": 1276.0, "end": 1281.4, "text": " just work on really good models for, you know, but a lot of people also just work on this", "tokens": [445, 589, 322, 534, 665, 5245, 337, 11, 291, 458, 11, 457, 257, 688, 295, 561, 611, 445, 589, 322, 341], "temperature": 0.0, "avg_logprob": -0.1542713805919385, "compression_ratio": 1.8671875, "no_speech_prob": 0.00020236230921000242}, {"id": 265, "seek": 126676, "start": 1281.4, "end": 1286.36, "text": " kind of how do you put this as a, you know, classification regression or some other type", "tokens": [733, 295, 577, 360, 291, 829, 341, 382, 257, 11, 291, 458, 11, 21538, 24590, 420, 512, 661, 2010], "temperature": 0.0, "avg_logprob": -0.1542713805919385, "compression_ratio": 1.8671875, "no_speech_prob": 0.00020236230921000242}, {"id": 266, "seek": 126676, "start": 1286.36, "end": 1287.8799999999999, "text": " of modeling.", "tokens": [295, 15983, 13], "temperature": 0.0, "avg_logprob": -0.1542713805919385, "compression_ratio": 1.8671875, "no_speech_prob": 0.00020236230921000242}, {"id": 267, "seek": 126676, "start": 1287.8799999999999, "end": 1290.84, "text": " That's actually kind of the most interesting, I think.", "tokens": [663, 311, 767, 733, 295, 264, 881, 1880, 11, 286, 519, 13], "temperature": 0.0, "avg_logprob": -0.1542713805919385, "compression_ratio": 1.8671875, "no_speech_prob": 0.00020236230921000242}, {"id": 268, "seek": 126676, "start": 1290.84, "end": 1293.24, "text": " And also kind of what's kind of what you have to do well.", "tokens": [400, 611, 733, 295, 437, 311, 733, 295, 437, 291, 362, 281, 360, 731, 13], "temperature": 0.0, "avg_logprob": -0.1542713805919385, "compression_ratio": 1.8671875, "no_speech_prob": 0.00020236230921000242}, {"id": 269, "seek": 129324, "start": 1293.24, "end": 1304.76, "text": " The best people do both, the best people understand the technical model building deeply, but also", "tokens": [440, 1151, 561, 360, 1293, 11, 264, 1151, 561, 1223, 264, 6191, 2316, 2390, 8760, 11, 457, 611], "temperature": 0.0, "avg_logprob": -0.2251458448522231, "compression_ratio": 1.5337078651685394, "no_speech_prob": 4.13342313549947e-05}, {"id": 270, "seek": 129324, "start": 1304.76, "end": 1307.76, "text": " understand the kind of strategic context deeply.", "tokens": [1223, 264, 733, 295, 10924, 4319, 8760, 13], "temperature": 0.0, "avg_logprob": -0.2251458448522231, "compression_ratio": 1.5337078651685394, "no_speech_prob": 4.13342313549947e-05}, {"id": 271, "seek": 129324, "start": 1307.76, "end": 1310.72, "text": " And so this is one way to think about it.", "tokens": [400, 370, 341, 307, 472, 636, 281, 519, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.2251458448522231, "compression_ratio": 1.5337078651685394, "no_speech_prob": 4.13342313549947e-05}, {"id": 272, "seek": 129324, "start": 1310.72, "end": 1319.04, "text": " And as I say, I actually think, you know, there aren't many articles I wrote in 2012", "tokens": [400, 382, 286, 584, 11, 286, 767, 519, 11, 291, 458, 11, 456, 3212, 380, 867, 11290, 286, 4114, 294, 9125], "temperature": 0.0, "avg_logprob": -0.2251458448522231, "compression_ratio": 1.5337078651685394, "no_speech_prob": 4.13342313549947e-05}, {"id": 273, "seek": 131904, "start": 1319.04, "end": 1326.96, "text": " I'm still recommending, but this one I think is still equally valid today.", "tokens": [286, 478, 920, 30559, 11, 457, 341, 472, 286, 519, 307, 920, 12309, 7363, 965, 13], "temperature": 0.0, "avg_logprob": -0.14677622204735166, "compression_ratio": 1.5639810426540284, "no_speech_prob": 3.0717728805029765e-05}, {"id": 274, "seek": 131904, "start": 1326.96, "end": 1330.8, "text": " So yeah, so like another great example is lead prioritization, right?", "tokens": [407, 1338, 11, 370, 411, 1071, 869, 1365, 307, 1477, 14846, 2144, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.14677622204735166, "compression_ratio": 1.5639810426540284, "no_speech_prob": 3.0717728805029765e-05}, {"id": 275, "seek": 131904, "start": 1330.8, "end": 1336.22, "text": " So like a lot of companies, like every one of these boxes I'm showing, you can generally", "tokens": [407, 411, 257, 688, 295, 3431, 11, 411, 633, 472, 295, 613, 9002, 286, 478, 4099, 11, 291, 393, 5101], "temperature": 0.0, "avg_logprob": -0.14677622204735166, "compression_ratio": 1.5639810426540284, "no_speech_prob": 3.0717728805029765e-05}, {"id": 276, "seek": 131904, "start": 1336.22, "end": 1343.52, "text": " find a company or many companies whose sole job in life is to build models of that thing,", "tokens": [915, 257, 2237, 420, 867, 3431, 6104, 12321, 1691, 294, 993, 307, 281, 1322, 5245, 295, 300, 551, 11], "temperature": 0.0, "avg_logprob": -0.14677622204735166, "compression_ratio": 1.5639810426540284, "no_speech_prob": 3.0717728805029765e-05}, {"id": 277, "seek": 131904, "start": 1343.52, "end": 1344.52, "text": " right?", "tokens": [558, 30], "temperature": 0.0, "avg_logprob": -0.14677622204735166, "compression_ratio": 1.5639810426540284, "no_speech_prob": 3.0717728805029765e-05}, {"id": 278, "seek": 134452, "start": 1344.52, "end": 1350.08, "text": " So lots of companies that sell lead prioritization systems.", "tokens": [407, 3195, 295, 3431, 300, 3607, 1477, 14846, 2144, 3652, 13], "temperature": 0.0, "avg_logprob": -0.1738176155090332, "compression_ratio": 1.5659574468085107, "no_speech_prob": 5.093680101708742e-06}, {"id": 279, "seek": 134452, "start": 1350.08, "end": 1355.96, "text": " But again, like the question is, how would we use that information, right?", "tokens": [583, 797, 11, 411, 264, 1168, 307, 11, 577, 576, 321, 764, 300, 1589, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1738176155090332, "compression_ratio": 1.5659574468085107, "no_speech_prob": 5.093680101708742e-06}, {"id": 280, "seek": 134452, "start": 1355.96, "end": 1361.56, "text": " So if it's like, oh, our best lead is Jeremy, you know, he's our highest probability of", "tokens": [407, 498, 309, 311, 411, 11, 1954, 11, 527, 1151, 1477, 307, 17809, 11, 291, 458, 11, 415, 311, 527, 6343, 8482, 295], "temperature": 0.0, "avg_logprob": -0.1738176155090332, "compression_ratio": 1.5659574468085107, "no_speech_prob": 5.093680101708742e-06}, {"id": 281, "seek": 134452, "start": 1361.56, "end": 1362.56, "text": " buying.", "tokens": [6382, 13], "temperature": 0.0, "avg_logprob": -0.1738176155090332, "compression_ratio": 1.5659574468085107, "no_speech_prob": 5.093680101708742e-06}, {"id": 282, "seek": 134452, "start": 1362.56, "end": 1368.16, "text": " Does that mean I should send a salesperson out to Jeremy or I shouldn't?", "tokens": [4402, 300, 914, 286, 820, 2845, 257, 5763, 10813, 484, 281, 17809, 420, 286, 4659, 380, 30], "temperature": 0.0, "avg_logprob": -0.1738176155090332, "compression_ratio": 1.5659574468085107, "no_speech_prob": 5.093680101708742e-06}, {"id": 283, "seek": 134452, "start": 1368.16, "end": 1373.1399999999999, "text": " Like if he's highly probable to buy, why waste my time with him?", "tokens": [1743, 498, 415, 311, 5405, 21759, 281, 2256, 11, 983, 5964, 452, 565, 365, 796, 30], "temperature": 0.0, "avg_logprob": -0.1738176155090332, "compression_ratio": 1.5659574468085107, "no_speech_prob": 5.093680101708742e-06}, {"id": 284, "seek": 137314, "start": 1373.14, "end": 1378.72, "text": " You know, so like again, it's like you really want some kind of simulation that says like,", "tokens": [509, 458, 11, 370, 411, 797, 11, 309, 311, 411, 291, 534, 528, 512, 733, 295, 16575, 300, 1619, 411, 11], "temperature": 0.0, "avg_logprob": -0.19840084634176114, "compression_ratio": 1.5414634146341464, "no_speech_prob": 3.7266136132529937e-06}, {"id": 285, "seek": 137314, "start": 1378.72, "end": 1386.0, "text": " what's the change, the likely change in Jeremy's behavior if I send my best salesperson, Yanet,", "tokens": [437, 311, 264, 1319, 11, 264, 3700, 1319, 294, 17809, 311, 5223, 498, 286, 2845, 452, 1151, 5763, 10813, 11, 13633, 302, 11], "temperature": 0.0, "avg_logprob": -0.19840084634176114, "compression_ratio": 1.5414634146341464, "no_speech_prob": 3.7266136132529937e-06}, {"id": 286, "seek": 137314, "start": 1386.0, "end": 1391.0800000000002, "text": " out to go and like encourage him to sign?", "tokens": [484, 281, 352, 293, 411, 5373, 796, 281, 1465, 30], "temperature": 0.0, "avg_logprob": -0.19840084634176114, "compression_ratio": 1.5414634146341464, "no_speech_prob": 3.7266136132529937e-06}, {"id": 287, "seek": 137314, "start": 1391.0800000000002, "end": 1398.5200000000002, "text": " So yeah, I think this is like there are many, many opportunities for data scientists in", "tokens": [407, 1338, 11, 286, 519, 341, 307, 411, 456, 366, 867, 11, 867, 4786, 337, 1412, 7708, 294], "temperature": 0.0, "avg_logprob": -0.19840084634176114, "compression_ratio": 1.5414634146341464, "no_speech_prob": 3.7266136132529937e-06}, {"id": 288, "seek": 139852, "start": 1398.52, "end": 1404.6399999999999, "text": " the world today to move beyond predictive modeling to actually bringing it all together,", "tokens": [264, 1002, 965, 281, 1286, 4399, 35521, 15983, 281, 767, 5062, 309, 439, 1214, 11], "temperature": 0.0, "avg_logprob": -0.12807454314886355, "compression_ratio": 1.681992337164751, "no_speech_prob": 5.014718226448167e-06}, {"id": 289, "seek": 139852, "start": 1404.6399999999999, "end": 1409.48, "text": " you know, with the kind of stuff that Dina was talking about in her question.", "tokens": [291, 458, 11, 365, 264, 733, 295, 1507, 300, 413, 1426, 390, 1417, 466, 294, 720, 1168, 13], "temperature": 0.0, "avg_logprob": -0.12807454314886355, "compression_ratio": 1.681992337164751, "no_speech_prob": 5.014718226448167e-06}, {"id": 290, "seek": 139852, "start": 1409.48, "end": 1417.56, "text": " So as well as these horizontal applications that basically apply to like every company,", "tokens": [407, 382, 731, 382, 613, 12750, 5821, 300, 1936, 3079, 281, 411, 633, 2237, 11], "temperature": 0.0, "avg_logprob": -0.12807454314886355, "compression_ratio": 1.681992337164751, "no_speech_prob": 5.014718226448167e-06}, {"id": 291, "seek": 139852, "start": 1417.56, "end": 1421.4, "text": " there's a whole bunch of applications that are specific to like every part of the world,", "tokens": [456, 311, 257, 1379, 3840, 295, 5821, 300, 366, 2685, 281, 411, 633, 644, 295, 264, 1002, 11], "temperature": 0.0, "avg_logprob": -0.12807454314886355, "compression_ratio": 1.681992337164751, "no_speech_prob": 5.014718226448167e-06}, {"id": 292, "seek": 139852, "start": 1421.4, "end": 1422.4, "text": " right?", "tokens": [558, 30], "temperature": 0.0, "avg_logprob": -0.12807454314886355, "compression_ratio": 1.681992337164751, "no_speech_prob": 5.014718226448167e-06}, {"id": 293, "seek": 139852, "start": 1422.4, "end": 1426.92, "text": " So for those of you that end up in healthcare, some of you will become experts in one or", "tokens": [407, 337, 729, 295, 291, 300, 917, 493, 294, 8884, 11, 512, 295, 291, 486, 1813, 8572, 294, 472, 420], "temperature": 0.0, "avg_logprob": -0.12807454314886355, "compression_ratio": 1.681992337164751, "no_speech_prob": 5.014718226448167e-06}, {"id": 294, "seek": 142692, "start": 1426.92, "end": 1433.48, "text": " more of these areas like readmission risk.", "tokens": [544, 295, 613, 3179, 411, 1401, 29797, 3148, 13], "temperature": 0.0, "avg_logprob": -0.0798954963684082, "compression_ratio": 1.7122641509433962, "no_speech_prob": 6.375545353876078e-07}, {"id": 295, "seek": 142692, "start": 1433.48, "end": 1438.44, "text": " So what's the probability that this patient is going to come back to the hospital?", "tokens": [407, 437, 311, 264, 8482, 300, 341, 4537, 307, 516, 281, 808, 646, 281, 264, 4530, 30], "temperature": 0.0, "avg_logprob": -0.0798954963684082, "compression_ratio": 1.7122641509433962, "no_speech_prob": 6.375545353876078e-07}, {"id": 296, "seek": 142692, "start": 1438.44, "end": 1445.92, "text": " And readmission is, depending on the details of the jurisdiction and so forth, it can be", "tokens": [400, 1401, 29797, 307, 11, 5413, 322, 264, 4365, 295, 264, 27285, 293, 370, 5220, 11, 309, 393, 312], "temperature": 0.0, "avg_logprob": -0.0798954963684082, "compression_ratio": 1.7122641509433962, "no_speech_prob": 6.375545353876078e-07}, {"id": 297, "seek": 142692, "start": 1445.92, "end": 1450.5600000000002, "text": " a disaster for hospitals when somebody is readmitted, right?", "tokens": [257, 11293, 337, 13014, 562, 2618, 307, 1401, 76, 3944, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.0798954963684082, "compression_ratio": 1.7122641509433962, "no_speech_prob": 6.375545353876078e-07}, {"id": 298, "seek": 142692, "start": 1450.5600000000002, "end": 1456.3600000000001, "text": " So if you find out that this patient has a high probability of readmission, what do you", "tokens": [407, 498, 291, 915, 484, 300, 341, 4537, 575, 257, 1090, 8482, 295, 1401, 29797, 11, 437, 360, 291], "temperature": 0.0, "avg_logprob": -0.0798954963684082, "compression_ratio": 1.7122641509433962, "no_speech_prob": 6.375545353876078e-07}, {"id": 299, "seek": 145636, "start": 1456.36, "end": 1457.6399999999999, "text": " do about it?", "tokens": [360, 466, 309, 30], "temperature": 0.0, "avg_logprob": -0.17656467579029225, "compression_ratio": 1.652, "no_speech_prob": 4.860424724029144e-06}, {"id": 300, "seek": 145636, "start": 1457.6399999999999, "end": 1461.0, "text": " Well again, the predictive model is helpful of itself, right?", "tokens": [1042, 797, 11, 264, 35521, 2316, 307, 4961, 295, 2564, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.17656467579029225, "compression_ratio": 1.652, "no_speech_prob": 4.860424724029144e-06}, {"id": 301, "seek": 145636, "start": 1461.0, "end": 1465.24, "text": " It rather suggests like we just shouldn't send them home yet because they're going to", "tokens": [467, 2831, 13409, 411, 321, 445, 4659, 380, 2845, 552, 1280, 1939, 570, 436, 434, 516, 281], "temperature": 0.0, "avg_logprob": -0.17656467579029225, "compression_ratio": 1.652, "no_speech_prob": 4.860424724029144e-06}, {"id": 302, "seek": 145636, "start": 1465.24, "end": 1466.7199999999998, "text": " come back.", "tokens": [808, 646, 13], "temperature": 0.0, "avg_logprob": -0.17656467579029225, "compression_ratio": 1.652, "no_speech_prob": 4.860424724029144e-06}, {"id": 303, "seek": 145636, "start": 1466.7199999999998, "end": 1471.8, "text": " But wouldn't it be nice if we had the tree interpreter and it said to us the reason that", "tokens": [583, 2759, 380, 309, 312, 1481, 498, 321, 632, 264, 4230, 34132, 293, 309, 848, 281, 505, 264, 1778, 300], "temperature": 0.0, "avg_logprob": -0.17656467579029225, "compression_ratio": 1.652, "no_speech_prob": 4.860424724029144e-06}, {"id": 304, "seek": 145636, "start": 1471.8, "end": 1478.8, "text": " they're at high risk is because we don't have a recent EKG for them and without a recent", "tokens": [436, 434, 412, 1090, 3148, 307, 570, 321, 500, 380, 362, 257, 5162, 46078, 38, 337, 552, 293, 1553, 257, 5162], "temperature": 0.0, "avg_logprob": -0.17656467579029225, "compression_ratio": 1.652, "no_speech_prob": 4.860424724029144e-06}, {"id": 305, "seek": 145636, "start": 1478.8, "end": 1485.24, "text": " EKG we can't have a high confidence about their cardiac health.", "tokens": [46078, 38, 321, 393, 380, 362, 257, 1090, 6687, 466, 641, 32129, 1585, 13], "temperature": 0.0, "avg_logprob": -0.17656467579029225, "compression_ratio": 1.652, "no_speech_prob": 4.860424724029144e-06}, {"id": 306, "seek": 148524, "start": 1485.24, "end": 1488.72, "text": " In which case it wouldn't be like, let's keep them in the hospital for two weeks, it'll", "tokens": [682, 597, 1389, 309, 2759, 380, 312, 411, 11, 718, 311, 1066, 552, 294, 264, 4530, 337, 732, 3259, 11, 309, 603], "temperature": 0.0, "avg_logprob": -0.2230235326857794, "compression_ratio": 1.6795366795366795, "no_speech_prob": 3.089470737904776e-06}, {"id": 307, "seek": 148524, "start": 1488.72, "end": 1491.56, "text": " be like let's give them an EKG.", "tokens": [312, 411, 718, 311, 976, 552, 364, 46078, 38, 13], "temperature": 0.0, "avg_logprob": -0.2230235326857794, "compression_ratio": 1.6795366795366795, "no_speech_prob": 3.089470737904776e-06}, {"id": 308, "seek": 148524, "start": 1491.56, "end": 1496.84, "text": " So this is interaction between interpretation and predictive accuracy.", "tokens": [407, 341, 307, 9285, 1296, 14174, 293, 35521, 14170, 13], "temperature": 0.0, "avg_logprob": -0.2230235326857794, "compression_ratio": 1.6795366795366795, "no_speech_prob": 3.089470737904776e-06}, {"id": 309, "seek": 148524, "start": 1496.84, "end": 1505.56, "text": " So correct me if I'm wrong, but what I'm understanding you saying is that the predictive models are", "tokens": [407, 3006, 385, 498, 286, 478, 2085, 11, 457, 437, 286, 478, 3701, 291, 1566, 307, 300, 264, 35521, 5245, 366], "temperature": 0.0, "avg_logprob": -0.2230235326857794, "compression_ratio": 1.6795366795366795, "no_speech_prob": 3.089470737904776e-06}, {"id": 310, "seek": 148524, "start": 1505.56, "end": 1510.72, "text": " a really great starting point, but in order to actually answer these questions, we really", "tokens": [257, 534, 869, 2891, 935, 11, 457, 294, 1668, 281, 767, 1867, 613, 1651, 11, 321, 534], "temperature": 0.0, "avg_logprob": -0.2230235326857794, "compression_ratio": 1.6795366795366795, "no_speech_prob": 3.089470737904776e-06}, {"id": 311, "seek": 148524, "start": 1510.72, "end": 1513.48, "text": " need to focus on the interpretability of these models.", "tokens": [643, 281, 1879, 322, 264, 7302, 2310, 295, 613, 5245, 13], "temperature": 0.0, "avg_logprob": -0.2230235326857794, "compression_ratio": 1.6795366795366795, "no_speech_prob": 3.089470737904776e-06}, {"id": 312, "seek": 151348, "start": 1513.48, "end": 1521.0, "text": " Yes, I think so, and more specifically I'm saying we just learned a whole raft of random", "tokens": [1079, 11, 286, 519, 370, 11, 293, 544, 4682, 286, 478, 1566, 321, 445, 3264, 257, 1379, 43863, 295, 4974], "temperature": 0.0, "avg_logprob": -0.24709531032677853, "compression_ratio": 1.5174418604651163, "no_speech_prob": 1.670130222919397e-05}, {"id": 313, "seek": 151348, "start": 1521.0, "end": 1530.16, "text": " forest interpretation techniques, so I'm trying to justify why.", "tokens": [6719, 14174, 7512, 11, 370, 286, 478, 1382, 281, 20833, 983, 13], "temperature": 0.0, "avg_logprob": -0.24709531032677853, "compression_ratio": 1.5174418604651163, "no_speech_prob": 1.670130222919397e-05}, {"id": 314, "seek": 151348, "start": 1530.16, "end": 1536.48, "text": " The reason why is because actually, I'd say most of the time, the interpretation is the", "tokens": [440, 1778, 983, 307, 570, 767, 11, 286, 1116, 584, 881, 295, 264, 565, 11, 264, 14174, 307, 264], "temperature": 0.0, "avg_logprob": -0.24709531032677853, "compression_ratio": 1.5174418604651163, "no_speech_prob": 1.670130222919397e-05}, {"id": 315, "seek": 151348, "start": 1536.48, "end": 1540.64, "text": " thing we care about.", "tokens": [551, 321, 1127, 466, 13], "temperature": 0.0, "avg_logprob": -0.24709531032677853, "compression_ratio": 1.5174418604651163, "no_speech_prob": 1.670130222919397e-05}, {"id": 316, "seek": 154064, "start": 1540.64, "end": 1549.2, "text": " You can create a chart or a table without machine learning, and indeed that's how most", "tokens": [509, 393, 1884, 257, 6927, 420, 257, 3199, 1553, 3479, 2539, 11, 293, 6451, 300, 311, 577, 881], "temperature": 0.0, "avg_logprob": -0.1577935499303481, "compression_ratio": 1.6755555555555555, "no_speech_prob": 7.296305739146192e-06}, {"id": 317, "seek": 154064, "start": 1549.2, "end": 1550.3600000000001, "text": " of the world works.", "tokens": [295, 264, 1002, 1985, 13], "temperature": 0.0, "avg_logprob": -0.1577935499303481, "compression_ratio": 1.6755555555555555, "no_speech_prob": 7.296305739146192e-06}, {"id": 318, "seek": 154064, "start": 1550.3600000000001, "end": 1554.88, "text": " Most managers build all kinds of tables and charts without any machine learning behind", "tokens": [4534, 14084, 1322, 439, 3685, 295, 8020, 293, 17767, 1553, 604, 3479, 2539, 2261], "temperature": 0.0, "avg_logprob": -0.1577935499303481, "compression_ratio": 1.6755555555555555, "no_speech_prob": 7.296305739146192e-06}, {"id": 319, "seek": 154064, "start": 1554.88, "end": 1556.92, "text": " them.", "tokens": [552, 13], "temperature": 0.0, "avg_logprob": -0.1577935499303481, "compression_ratio": 1.6755555555555555, "no_speech_prob": 7.296305739146192e-06}, {"id": 320, "seek": 154064, "start": 1556.92, "end": 1561.64, "text": " But they often make terrible decisions because they don't know the feature importance of", "tokens": [583, 436, 2049, 652, 6237, 5327, 570, 436, 500, 380, 458, 264, 4111, 7379, 295], "temperature": 0.0, "avg_logprob": -0.1577935499303481, "compression_ratio": 1.6755555555555555, "no_speech_prob": 7.296305739146192e-06}, {"id": 321, "seek": 154064, "start": 1561.64, "end": 1565.0800000000002, "text": " the objective they're interested in, so the table they create is of things that actually", "tokens": [264, 10024, 436, 434, 3102, 294, 11, 370, 264, 3199, 436, 1884, 307, 295, 721, 300, 767], "temperature": 0.0, "avg_logprob": -0.1577935499303481, "compression_ratio": 1.6755555555555555, "no_speech_prob": 7.296305739146192e-06}, {"id": 322, "seek": 156508, "start": 1565.08, "end": 1571.04, "text": " are the least important things anyway, or they just do a univariate chart rather than", "tokens": [366, 264, 1935, 1021, 721, 4033, 11, 420, 436, 445, 360, 257, 517, 592, 3504, 473, 6927, 2831, 813], "temperature": 0.0, "avg_logprob": -0.11396285849557795, "compression_ratio": 1.5384615384615385, "no_speech_prob": 2.5215622372343205e-06}, {"id": 323, "seek": 156508, "start": 1571.04, "end": 1575.52, "text": " a partial dependence plot so they don't actually realize that the relationship they thought", "tokens": [257, 14641, 31704, 7542, 370, 436, 500, 380, 767, 4325, 300, 264, 2480, 436, 1194], "temperature": 0.0, "avg_logprob": -0.11396285849557795, "compression_ratio": 1.5384615384615385, "no_speech_prob": 2.5215622372343205e-06}, {"id": 324, "seek": 156508, "start": 1575.52, "end": 1580.0, "text": " they're looking at is due entirely to something else.", "tokens": [436, 434, 1237, 412, 307, 3462, 7696, 281, 746, 1646, 13], "temperature": 0.0, "avg_logprob": -0.11396285849557795, "compression_ratio": 1.5384615384615385, "no_speech_prob": 2.5215622372343205e-06}, {"id": 325, "seek": 156508, "start": 1580.0, "end": 1588.36, "text": " So I'm kind of arguing for data scientists getting much more deeply involved in strategy", "tokens": [407, 286, 478, 733, 295, 19697, 337, 1412, 7708, 1242, 709, 544, 8760, 3288, 294, 5206], "temperature": 0.0, "avg_logprob": -0.11396285849557795, "compression_ratio": 1.5384615384615385, "no_speech_prob": 2.5215622372343205e-06}, {"id": 326, "seek": 158836, "start": 1588.36, "end": 1600.6799999999998, "text": " and in trying to use machine learning to really help a business with all of its objectives.", "tokens": [293, 294, 1382, 281, 764, 3479, 2539, 281, 534, 854, 257, 1606, 365, 439, 295, 1080, 15961, 13], "temperature": 0.0, "avg_logprob": -0.20002365112304688, "compression_ratio": 1.523076923076923, "no_speech_prob": 9.368679457111284e-06}, {"id": 327, "seek": 158836, "start": 1600.6799999999998, "end": 1606.08, "text": " There are companies like Dunn-Humby, a huge company that does nothing but retail applications", "tokens": [821, 366, 3431, 411, 11959, 77, 12, 39, 449, 2322, 11, 257, 2603, 2237, 300, 775, 1825, 457, 10800, 5821], "temperature": 0.0, "avg_logprob": -0.20002365112304688, "compression_ratio": 1.523076923076923, "no_speech_prob": 9.368679457111284e-06}, {"id": 328, "seek": 158836, "start": 1606.08, "end": 1607.32, "text": " with machine learning.", "tokens": [365, 3479, 2539, 13], "temperature": 0.0, "avg_logprob": -0.20002365112304688, "compression_ratio": 1.523076923076923, "no_speech_prob": 9.368679457111284e-06}, {"id": 329, "seek": 158836, "start": 1607.32, "end": 1616.32, "text": " And so I believe there's a Dunn-Humby product you can buy which will help you figure out", "tokens": [400, 370, 286, 1697, 456, 311, 257, 11959, 77, 12, 39, 449, 2322, 1674, 291, 393, 2256, 597, 486, 854, 291, 2573, 484], "temperature": 0.0, "avg_logprob": -0.20002365112304688, "compression_ratio": 1.523076923076923, "no_speech_prob": 9.368679457111284e-06}, {"id": 330, "seek": 161632, "start": 1616.32, "end": 1622.36, "text": " like if I put my new store in this location versus that location, how many people are", "tokens": [411, 498, 286, 829, 452, 777, 3531, 294, 341, 4914, 5717, 300, 4914, 11, 577, 867, 561, 366], "temperature": 0.0, "avg_logprob": -0.1343942571569372, "compression_ratio": 1.7277486910994764, "no_speech_prob": 1.4970984011597466e-05}, {"id": 331, "seek": 161632, "start": 1622.36, "end": 1624.48, "text": " going to shop there.", "tokens": [516, 281, 3945, 456, 13], "temperature": 0.0, "avg_logprob": -0.1343942571569372, "compression_ratio": 1.7277486910994764, "no_speech_prob": 1.4970984011597466e-05}, {"id": 332, "seek": 161632, "start": 1624.48, "end": 1630.56, "text": " Or if I put my diapers in this part of the shop versus that part of the shop, how's that", "tokens": [1610, 498, 286, 829, 452, 48496, 294, 341, 644, 295, 264, 3945, 5717, 300, 644, 295, 264, 3945, 11, 577, 311, 300], "temperature": 0.0, "avg_logprob": -0.1343942571569372, "compression_ratio": 1.7277486910994764, "no_speech_prob": 1.4970984011597466e-05}, {"id": 333, "seek": 161632, "start": 1630.56, "end": 1634.6399999999999, "text": " going to impact purchasing behavior or whatever.", "tokens": [516, 281, 2712, 20906, 5223, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.1343942571569372, "compression_ratio": 1.7277486910994764, "no_speech_prob": 1.4970984011597466e-05}, {"id": 334, "seek": 161632, "start": 1634.6399999999999, "end": 1640.6, "text": " So I think it's also good to realize that the subset of machine learning applications", "tokens": [407, 286, 519, 309, 311, 611, 665, 281, 4325, 300, 264, 25993, 295, 3479, 2539, 5821], "temperature": 0.0, "avg_logprob": -0.1343942571569372, "compression_ratio": 1.7277486910994764, "no_speech_prob": 1.4970984011597466e-05}, {"id": 335, "seek": 164060, "start": 1640.6, "end": 1650.3999999999999, "text": " you tend to hear about in the tech press or whatever is this massively biased tiny subset", "tokens": [291, 3928, 281, 1568, 466, 294, 264, 7553, 1886, 420, 2035, 307, 341, 29379, 28035, 5870, 25993], "temperature": 0.0, "avg_logprob": -0.16003594156038964, "compression_ratio": 1.541899441340782, "no_speech_prob": 1.6964242604444735e-05}, {"id": 336, "seek": 164060, "start": 1650.3999999999999, "end": 1656.6, "text": " of stuff which Google and Facebook do, whereas the vast majority of stuff that actually makes", "tokens": [295, 1507, 597, 3329, 293, 4384, 360, 11, 9735, 264, 8369, 6286, 295, 1507, 300, 767, 1669], "temperature": 0.0, "avg_logprob": -0.16003594156038964, "compression_ratio": 1.541899441340782, "no_speech_prob": 1.6964242604444735e-05}, {"id": 337, "seek": 164060, "start": 1656.6, "end": 1663.32, "text": " the world go round is these kinds of applications that actually help people make things, buy", "tokens": [264, 1002, 352, 3098, 307, 613, 3685, 295, 5821, 300, 767, 854, 561, 652, 721, 11, 2256], "temperature": 0.0, "avg_logprob": -0.16003594156038964, "compression_ratio": 1.541899441340782, "no_speech_prob": 1.6964242604444735e-05}, {"id": 338, "seek": 166332, "start": 1663.32, "end": 1672.6, "text": " things, sell things, build things, so forth.", "tokens": [721, 11, 3607, 721, 11, 1322, 721, 11, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.12378783907209123, "compression_ratio": 1.6451612903225807, "no_speech_prob": 7.137283682823181e-05}, {"id": 339, "seek": 166332, "start": 1672.6, "end": 1679.28, "text": " So about tree interpretation, the way we looked at the tree was we manually checked which", "tokens": [407, 466, 4230, 14174, 11, 264, 636, 321, 2956, 412, 264, 4230, 390, 321, 16945, 10033, 597], "temperature": 0.0, "avg_logprob": -0.12378783907209123, "compression_ratio": 1.6451612903225807, "no_speech_prob": 7.137283682823181e-05}, {"id": 340, "seek": 166332, "start": 1679.28, "end": 1685.76, "text": " feature was more important for particular observation.", "tokens": [4111, 390, 544, 1021, 337, 1729, 14816, 13], "temperature": 0.0, "avg_logprob": -0.12378783907209123, "compression_ratio": 1.6451612903225807, "no_speech_prob": 7.137283682823181e-05}, {"id": 341, "seek": 166332, "start": 1685.76, "end": 1691.24, "text": " But for businesses, they would have huge amount of data and they want this interpretation", "tokens": [583, 337, 6011, 11, 436, 576, 362, 2603, 2372, 295, 1412, 293, 436, 528, 341, 14174], "temperature": 0.0, "avg_logprob": -0.12378783907209123, "compression_ratio": 1.6451612903225807, "no_speech_prob": 7.137283682823181e-05}, {"id": 342, "seek": 166332, "start": 1691.24, "end": 1693.04, "text": " for a lot of observations.", "tokens": [337, 257, 688, 295, 18163, 13], "temperature": 0.0, "avg_logprob": -0.12378783907209123, "compression_ratio": 1.6451612903225807, "no_speech_prob": 7.137283682823181e-05}, {"id": 343, "seek": 169304, "start": 1693.04, "end": 1697.68, "text": " So how do they automate it?", "tokens": [407, 577, 360, 436, 31605, 309, 30], "temperature": 0.0, "avg_logprob": -0.28544949376305867, "compression_ratio": 1.538812785388128, "no_speech_prob": 5.390992373577319e-05}, {"id": 344, "seek": 169304, "start": 1697.68, "end": 1700.84, "text": " I don't think the automation is at all difficult.", "tokens": [286, 500, 380, 519, 264, 17769, 307, 412, 439, 2252, 13], "temperature": 0.0, "avg_logprob": -0.28544949376305867, "compression_ratio": 1.538812785388128, "no_speech_prob": 5.390992373577319e-05}, {"id": 345, "seek": 169304, "start": 1700.84, "end": 1705.48, "text": " You can run any of these algorithms like looping through the rows or doing them in parallel.", "tokens": [509, 393, 1190, 604, 295, 613, 14642, 411, 6367, 278, 807, 264, 13241, 420, 884, 552, 294, 8952, 13], "temperature": 0.0, "avg_logprob": -0.28544949376305867, "compression_ratio": 1.538812785388128, "no_speech_prob": 5.390992373577319e-05}, {"id": 346, "seek": 169304, "start": 1705.48, "end": 1707.52, "text": " It's all just code.", "tokens": [467, 311, 439, 445, 3089, 13], "temperature": 0.0, "avg_logprob": -0.28544949376305867, "compression_ratio": 1.538812785388128, "no_speech_prob": 5.390992373577319e-05}, {"id": 347, "seek": 169304, "start": 1707.52, "end": 1709.6, "text": " Am I misunderstanding your question?", "tokens": [2012, 286, 29227, 428, 1168, 30], "temperature": 0.0, "avg_logprob": -0.28544949376305867, "compression_ratio": 1.538812785388128, "no_speech_prob": 5.390992373577319e-05}, {"id": 348, "seek": 169304, "start": 1709.6, "end": 1717.76, "text": " Is it like they set a threshold that if some feature is above, different people will have", "tokens": [1119, 309, 411, 436, 992, 257, 14678, 300, 498, 512, 4111, 307, 3673, 11, 819, 561, 486, 362], "temperature": 0.0, "avg_logprob": -0.28544949376305867, "compression_ratio": 1.538812785388128, "no_speech_prob": 5.390992373577319e-05}, {"id": 349, "seek": 169304, "start": 1717.76, "end": 1718.76, "text": " different behavior?", "tokens": [819, 5223, 30], "temperature": 0.0, "avg_logprob": -0.28544949376305867, "compression_ratio": 1.538812785388128, "no_speech_prob": 5.390992373577319e-05}, {"id": 350, "seek": 171876, "start": 1718.76, "end": 1724.84, "text": " That's a good question.", "tokens": [663, 311, 257, 665, 1168, 13], "temperature": 0.0, "avg_logprob": -0.20703230637770434, "compression_ratio": 1.4840425531914894, "no_speech_prob": 1.4510323126160074e-05}, {"id": 351, "seek": 171876, "start": 1724.84, "end": 1727.52, "text": " This is a really important issue actually.", "tokens": [639, 307, 257, 534, 1021, 2734, 767, 13], "temperature": 0.0, "avg_logprob": -0.20703230637770434, "compression_ratio": 1.4840425531914894, "no_speech_prob": 1.4510323126160074e-05}, {"id": 352, "seek": 171876, "start": 1727.52, "end": 1731.76, "text": " The vast majority of machine learning models don't automate anything.", "tokens": [440, 8369, 6286, 295, 3479, 2539, 5245, 500, 380, 31605, 1340, 13], "temperature": 0.0, "avg_logprob": -0.20703230637770434, "compression_ratio": 1.4840425531914894, "no_speech_prob": 1.4510323126160074e-05}, {"id": 353, "seek": 171876, "start": 1731.76, "end": 1735.46, "text": " They're designed to provide information to humans.", "tokens": [814, 434, 4761, 281, 2893, 1589, 281, 6255, 13], "temperature": 0.0, "avg_logprob": -0.20703230637770434, "compression_ratio": 1.4840425531914894, "no_speech_prob": 1.4510323126160074e-05}, {"id": 354, "seek": 171876, "start": 1735.46, "end": 1745.04, "text": " So for example, if you're a point of sales customer service phone operator for an insurance", "tokens": [407, 337, 1365, 11, 498, 291, 434, 257, 935, 295, 5763, 5474, 2643, 2593, 12973, 337, 364, 7214], "temperature": 0.0, "avg_logprob": -0.20703230637770434, "compression_ratio": 1.4840425531914894, "no_speech_prob": 1.4510323126160074e-05}, {"id": 355, "seek": 174504, "start": 1745.04, "end": 1752.8799999999999, "text": " company and your customer asks you why is my renewal $500 more expensive than last time,", "tokens": [2237, 293, 428, 5474, 8962, 291, 983, 307, 452, 35516, 1848, 7526, 544, 5124, 813, 1036, 565, 11], "temperature": 0.0, "avg_logprob": -0.227017818353115, "compression_ratio": 1.6045454545454545, "no_speech_prob": 6.540383765241131e-06}, {"id": 356, "seek": 174504, "start": 1752.8799999999999, "end": 1758.68, "text": " then hopefully the insurance company provides in your terminal a little screen that shows", "tokens": [550, 4696, 264, 7214, 2237, 6417, 294, 428, 14709, 257, 707, 2568, 300, 3110], "temperature": 0.0, "avg_logprob": -0.227017818353115, "compression_ratio": 1.6045454545454545, "no_speech_prob": 6.540383765241131e-06}, {"id": 357, "seek": 174504, "start": 1758.68, "end": 1764.1599999999999, "text": " the result of the tree interpreter, so that you can jump there and tell the customer,", "tokens": [264, 1874, 295, 264, 4230, 34132, 11, 370, 300, 291, 393, 3012, 456, 293, 980, 264, 5474, 11], "temperature": 0.0, "avg_logprob": -0.227017818353115, "compression_ratio": 1.6045454545454545, "no_speech_prob": 6.540383765241131e-06}, {"id": 358, "seek": 174504, "start": 1764.1599999999999, "end": 1773.2, "text": " okay, last year you were in this different zip code which has lower amounts of car theft", "tokens": [1392, 11, 1036, 1064, 291, 645, 294, 341, 819, 20730, 3089, 597, 575, 3126, 11663, 295, 1032, 28508], "temperature": 0.0, "avg_logprob": -0.227017818353115, "compression_ratio": 1.6045454545454545, "no_speech_prob": 6.540383765241131e-06}, {"id": 359, "seek": 177320, "start": 1773.2, "end": 1779.56, "text": " and this year also you've actually changed your vehicle to a more expensive one or whatever.", "tokens": [293, 341, 1064, 611, 291, 600, 767, 3105, 428, 5864, 281, 257, 544, 5124, 472, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.13837317960808077, "compression_ratio": 1.5836909871244635, "no_speech_prob": 1.130068540078355e-05}, {"id": 360, "seek": 177320, "start": 1779.56, "end": 1789.0, "text": " So it's not so much about thresholds of automation, but about making these model outputs available", "tokens": [407, 309, 311, 406, 370, 709, 466, 14678, 82, 295, 17769, 11, 457, 466, 1455, 613, 2316, 23930, 2435], "temperature": 0.0, "avg_logprob": -0.13837317960808077, "compression_ratio": 1.5836909871244635, "no_speech_prob": 1.130068540078355e-05}, {"id": 361, "seek": 177320, "start": 1789.0, "end": 1793.8400000000001, "text": " to the decision makers in an organization, whether they be at the top strategic level", "tokens": [281, 264, 3537, 19323, 294, 364, 4475, 11, 1968, 436, 312, 412, 264, 1192, 10924, 1496], "temperature": 0.0, "avg_logprob": -0.13837317960808077, "compression_ratio": 1.5836909871244635, "no_speech_prob": 1.130068540078355e-05}, {"id": 362, "seek": 177320, "start": 1793.8400000000001, "end": 1800.16, "text": " of like are we going to shut down this whole product or not, all the way to the operational", "tokens": [295, 411, 366, 321, 516, 281, 5309, 760, 341, 1379, 1674, 420, 406, 11, 439, 264, 636, 281, 264, 16607], "temperature": 0.0, "avg_logprob": -0.13837317960808077, "compression_ratio": 1.5836909871244635, "no_speech_prob": 1.130068540078355e-05}, {"id": 363, "seek": 180016, "start": 1800.16, "end": 1806.8000000000002, "text": " level of that individual discussion with the customer.", "tokens": [1496, 295, 300, 2609, 5017, 365, 264, 5474, 13], "temperature": 0.0, "avg_logprob": -0.1988244663585316, "compression_ratio": 1.4761904761904763, "no_speech_prob": 1.723136483633425e-05}, {"id": 364, "seek": 180016, "start": 1806.8000000000002, "end": 1811.92, "text": " So another example is aircraft scheduling and gate management.", "tokens": [407, 1071, 1365, 307, 9465, 29055, 293, 8539, 4592, 13], "temperature": 0.0, "avg_logprob": -0.1988244663585316, "compression_ratio": 1.4761904761904763, "no_speech_prob": 1.723136483633425e-05}, {"id": 365, "seek": 180016, "start": 1811.92, "end": 1816.52, "text": " There's lots of companies that do that.", "tokens": [821, 311, 3195, 295, 3431, 300, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.1988244663585316, "compression_ratio": 1.4761904761904763, "no_speech_prob": 1.723136483633425e-05}, {"id": 366, "seek": 180016, "start": 1816.52, "end": 1827.2, "text": " Basically what happens is that there are people at an airport whose job it is to basically", "tokens": [8537, 437, 2314, 307, 300, 456, 366, 561, 412, 364, 10155, 6104, 1691, 309, 307, 281, 1936], "temperature": 0.0, "avg_logprob": -0.1988244663585316, "compression_ratio": 1.4761904761904763, "no_speech_prob": 1.723136483633425e-05}, {"id": 367, "seek": 182720, "start": 1827.2, "end": 1832.96, "text": " tell each aircraft what gate to go to, to figure out when to close the doors, stuff", "tokens": [980, 1184, 9465, 437, 8539, 281, 352, 281, 11, 281, 2573, 484, 562, 281, 1998, 264, 8077, 11, 1507], "temperature": 0.0, "avg_logprob": -0.14852471870951134, "compression_ratio": 1.5942622950819672, "no_speech_prob": 1.9947244709328515e-06}, {"id": 368, "seek": 182720, "start": 1832.96, "end": 1833.96, "text": " like that.", "tokens": [411, 300, 13], "temperature": 0.0, "avg_logprob": -0.14852471870951134, "compression_ratio": 1.5942622950819672, "no_speech_prob": 1.9947244709328515e-06}, {"id": 369, "seek": 182720, "start": 1833.96, "end": 1838.64, "text": " So the idea is you're giving them software which has the information they need to make", "tokens": [407, 264, 1558, 307, 291, 434, 2902, 552, 4722, 597, 575, 264, 1589, 436, 643, 281, 652], "temperature": 0.0, "avg_logprob": -0.14852471870951134, "compression_ratio": 1.5942622950819672, "no_speech_prob": 1.9947244709328515e-06}, {"id": 370, "seek": 182720, "start": 1838.64, "end": 1839.64, "text": " good decisions.", "tokens": [665, 5327, 13], "temperature": 0.0, "avg_logprob": -0.14852471870951134, "compression_ratio": 1.5942622950819672, "no_speech_prob": 1.9947244709328515e-06}, {"id": 371, "seek": 182720, "start": 1839.64, "end": 1846.0800000000002, "text": " So the machine learning models end up embedded in that software to say, okay, that plane", "tokens": [407, 264, 3479, 2539, 5245, 917, 493, 16741, 294, 300, 4722, 281, 584, 11, 1392, 11, 300, 5720], "temperature": 0.0, "avg_logprob": -0.14852471870951134, "compression_ratio": 1.5942622950819672, "no_speech_prob": 1.9947244709328515e-06}, {"id": 372, "seek": 182720, "start": 1846.0800000000002, "end": 1851.96, "text": " that's currently coming in from Miami, there's a 48% chance that it's going to be over 5", "tokens": [300, 311, 4362, 1348, 294, 490, 18367, 11, 456, 311, 257, 11174, 4, 2931, 300, 309, 311, 516, 281, 312, 670, 1025], "temperature": 0.0, "avg_logprob": -0.14852471870951134, "compression_ratio": 1.5942622950819672, "no_speech_prob": 1.9947244709328515e-06}, {"id": 373, "seek": 182720, "start": 1851.96, "end": 1852.96, "text": " minutes late.", "tokens": [2077, 3469, 13], "temperature": 0.0, "avg_logprob": -0.14852471870951134, "compression_ratio": 1.5942622950819672, "no_speech_prob": 1.9947244709328515e-06}, {"id": 374, "seek": 185296, "start": 1852.96, "end": 1858.04, "text": " If it does, then this is going to be the knock-on impact through the rest of the terminal.", "tokens": [759, 309, 775, 11, 550, 341, 307, 516, 281, 312, 264, 6728, 12, 266, 2712, 807, 264, 1472, 295, 264, 14709, 13], "temperature": 0.0, "avg_logprob": -0.2221147412451628, "compression_ratio": 1.649789029535865, "no_speech_prob": 4.157355760980863e-06}, {"id": 375, "seek": 185296, "start": 1858.04, "end": 1863.2, "text": " So that's kind of how these things tend to fit together.", "tokens": [407, 300, 311, 733, 295, 577, 613, 721, 3928, 281, 3318, 1214, 13], "temperature": 0.0, "avg_logprob": -0.2221147412451628, "compression_ratio": 1.649789029535865, "no_speech_prob": 4.157355760980863e-06}, {"id": 376, "seek": 185296, "start": 1863.2, "end": 1865.6000000000001, "text": " So there's so many of these.", "tokens": [407, 456, 311, 370, 867, 295, 613, 13], "temperature": 0.0, "avg_logprob": -0.2221147412451628, "compression_ratio": 1.649789029535865, "no_speech_prob": 4.157355760980863e-06}, {"id": 377, "seek": 185296, "start": 1865.6000000000001, "end": 1866.6000000000001, "text": " There's lots and lots.", "tokens": [821, 311, 3195, 293, 3195, 13], "temperature": 0.0, "avg_logprob": -0.2221147412451628, "compression_ratio": 1.649789029535865, "no_speech_prob": 4.157355760980863e-06}, {"id": 378, "seek": 185296, "start": 1866.6000000000001, "end": 1873.1200000000001, "text": " I don't expect you to remember all these applications, but what I do want you to do is to spend", "tokens": [286, 500, 380, 2066, 291, 281, 1604, 439, 613, 5821, 11, 457, 437, 286, 360, 528, 291, 281, 360, 307, 281, 3496], "temperature": 0.0, "avg_logprob": -0.2221147412451628, "compression_ratio": 1.649789029535865, "no_speech_prob": 4.157355760980863e-06}, {"id": 379, "seek": 185296, "start": 1873.1200000000001, "end": 1876.24, "text": " some time thinking about them.", "tokens": [512, 565, 1953, 466, 552, 13], "temperature": 0.0, "avg_logprob": -0.2221147412451628, "compression_ratio": 1.649789029535865, "no_speech_prob": 4.157355760980863e-06}, {"id": 380, "seek": 185296, "start": 1876.24, "end": 1879.72, "text": " Sit down with one of your friends and talk about a few examples.", "tokens": [14523, 760, 365, 472, 295, 428, 1855, 293, 751, 466, 257, 1326, 5110, 13], "temperature": 0.0, "avg_logprob": -0.2221147412451628, "compression_ratio": 1.649789029535865, "no_speech_prob": 4.157355760980863e-06}, {"id": 381, "seek": 187972, "start": 1879.72, "end": 1888.64, "text": " Like okay, how would we go about doing failure analysis in manufacturing?", "tokens": [1743, 1392, 11, 577, 576, 321, 352, 466, 884, 7763, 5215, 294, 11096, 30], "temperature": 0.0, "avg_logprob": -0.20999374029771337, "compression_ratio": 1.7872340425531914, "no_speech_prob": 4.936978257319424e-06}, {"id": 382, "seek": 187972, "start": 1888.64, "end": 1889.64, "text": " Who would be doing that?", "tokens": [2102, 576, 312, 884, 300, 30], "temperature": 0.0, "avg_logprob": -0.20999374029771337, "compression_ratio": 1.7872340425531914, "no_speech_prob": 4.936978257319424e-06}, {"id": 383, "seek": 187972, "start": 1889.64, "end": 1890.8, "text": " Why would they be doing it?", "tokens": [1545, 576, 436, 312, 884, 309, 30], "temperature": 0.0, "avg_logprob": -0.20999374029771337, "compression_ratio": 1.7872340425531914, "no_speech_prob": 4.936978257319424e-06}, {"id": 384, "seek": 187972, "start": 1890.8, "end": 1892.16, "text": " What kind of models might they use?", "tokens": [708, 733, 295, 5245, 1062, 436, 764, 30], "temperature": 0.0, "avg_logprob": -0.20999374029771337, "compression_ratio": 1.7872340425531914, "no_speech_prob": 4.936978257319424e-06}, {"id": 385, "seek": 187972, "start": 1892.16, "end": 1893.76, "text": " What kind of data might they use?", "tokens": [708, 733, 295, 1412, 1062, 436, 764, 30], "temperature": 0.0, "avg_logprob": -0.20999374029771337, "compression_ratio": 1.7872340425531914, "no_speech_prob": 4.936978257319424e-06}, {"id": 386, "seek": 187972, "start": 1893.76, "end": 1897.68, "text": " Start to practice this and get a sense.", "tokens": [6481, 281, 3124, 341, 293, 483, 257, 2020, 13], "temperature": 0.0, "avg_logprob": -0.20999374029771337, "compression_ratio": 1.7872340425531914, "no_speech_prob": 4.936978257319424e-06}, {"id": 387, "seek": 187972, "start": 1897.68, "end": 1903.18, "text": " Because then as you're interviewing and then when you're at the workplace and you're talking", "tokens": [1436, 550, 382, 291, 434, 26524, 293, 550, 562, 291, 434, 412, 264, 15328, 293, 291, 434, 1417], "temperature": 0.0, "avg_logprob": -0.20999374029771337, "compression_ratio": 1.7872340425531914, "no_speech_prob": 4.936978257319424e-06}, {"id": 388, "seek": 187972, "start": 1903.18, "end": 1909.7, "text": " to managers, you want to be straight away able to recognize that the person you're talking", "tokens": [281, 14084, 11, 291, 528, 281, 312, 2997, 1314, 1075, 281, 5521, 300, 264, 954, 291, 434, 1417], "temperature": 0.0, "avg_logprob": -0.20999374029771337, "compression_ratio": 1.7872340425531914, "no_speech_prob": 4.936978257319424e-06}, {"id": 389, "seek": 190970, "start": 1909.7, "end": 1914.52, "text": " to, what are they trying to achieve, what are the levers that they have to pull, what", "tokens": [281, 11, 437, 366, 436, 1382, 281, 4584, 11, 437, 366, 264, 45571, 300, 436, 362, 281, 2235, 11, 437], "temperature": 0.0, "avg_logprob": -0.18351715857829523, "compression_ratio": 1.8366533864541832, "no_speech_prob": 7.411169917759253e-06}, {"id": 390, "seek": 190970, "start": 1914.52, "end": 1918.52, "text": " is the data they have available to pull those levers to achieve that thing, and therefore", "tokens": [307, 264, 1412, 436, 362, 2435, 281, 2235, 729, 45571, 281, 4584, 300, 551, 11, 293, 4412], "temperature": 0.0, "avg_logprob": -0.18351715857829523, "compression_ratio": 1.8366533864541832, "no_speech_prob": 7.411169917759253e-06}, {"id": 391, "seek": 190970, "start": 1918.52, "end": 1922.68, "text": " how could we build models to help them do that, and what kind of predictions would they", "tokens": [577, 727, 321, 1322, 5245, 281, 854, 552, 360, 300, 11, 293, 437, 733, 295, 21264, 576, 436], "temperature": 0.0, "avg_logprob": -0.18351715857829523, "compression_ratio": 1.8366533864541832, "no_speech_prob": 7.411169917759253e-06}, {"id": 392, "seek": 190970, "start": 1922.68, "end": 1925.4, "text": " have to be making.", "tokens": [362, 281, 312, 1455, 13], "temperature": 0.0, "avg_logprob": -0.18351715857829523, "compression_ratio": 1.8366533864541832, "no_speech_prob": 7.411169917759253e-06}, {"id": 393, "seek": 190970, "start": 1925.4, "end": 1930.32, "text": " And so then you can have this really thoughtful, empathetic conversation with those people", "tokens": [400, 370, 550, 291, 393, 362, 341, 534, 21566, 11, 27155, 3532, 3761, 365, 729, 561], "temperature": 0.0, "avg_logprob": -0.18351715857829523, "compression_ratio": 1.8366533864541832, "no_speech_prob": 7.411169917759253e-06}, {"id": 394, "seek": 190970, "start": 1930.32, "end": 1937.72, "text": " and saying, in order to reduce the number of customers that are leaving, I guess you're", "tokens": [293, 1566, 11, 294, 1668, 281, 5407, 264, 1230, 295, 4581, 300, 366, 5012, 11, 286, 2041, 291, 434], "temperature": 0.0, "avg_logprob": -0.18351715857829523, "compression_ratio": 1.8366533864541832, "no_speech_prob": 7.411169917759253e-06}, {"id": 395, "seek": 193772, "start": 1937.72, "end": 1950.78, "text": " trying to figure out who should you be providing better pricing to or whatever.", "tokens": [1382, 281, 2573, 484, 567, 820, 291, 312, 6530, 1101, 17621, 281, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.20308365360383066, "compression_ratio": 1.5204678362573099, "no_speech_prob": 1.1842724234156776e-05}, {"id": 396, "seek": 193772, "start": 1950.78, "end": 1957.1200000000001, "text": " So what I'm noticing from your beautiful little chart above is that a lot of this, to me at", "tokens": [407, 437, 286, 478, 21814, 490, 428, 2238, 707, 6927, 3673, 307, 300, 257, 688, 295, 341, 11, 281, 385, 412], "temperature": 0.0, "avg_logprob": -0.20308365360383066, "compression_ratio": 1.5204678362573099, "no_speech_prob": 1.1842724234156776e-05}, {"id": 397, "seek": 193772, "start": 1957.1200000000001, "end": 1965.56, "text": " least, still seems like the primary purpose is at least base level, is predictive power.", "tokens": [1935, 11, 920, 2544, 411, 264, 6194, 4334, 307, 412, 1935, 3096, 1496, 11, 307, 35521, 1347, 13], "temperature": 0.0, "avg_logprob": -0.20308365360383066, "compression_ratio": 1.5204678362573099, "no_speech_prob": 1.1842724234156776e-05}, {"id": 398, "seek": 196556, "start": 1965.56, "end": 1971.12, "text": " And so I guess my thing is, for explanatory problems, a lot of the ones that people are", "tokens": [400, 370, 286, 2041, 452, 551, 307, 11, 337, 9045, 4745, 2740, 11, 257, 688, 295, 264, 2306, 300, 561, 366], "temperature": 0.0, "avg_logprob": -0.18704190747491245, "compression_ratio": 1.6634615384615385, "no_speech_prob": 4.425447514222469e-06}, {"id": 399, "seek": 196556, "start": 1971.12, "end": 1975.52, "text": " faced with in social sciences, is that something machine learning can be used for or is used", "tokens": [11446, 365, 294, 2093, 17677, 11, 307, 300, 746, 3479, 2539, 393, 312, 1143, 337, 420, 307, 1143], "temperature": 0.0, "avg_logprob": -0.18704190747491245, "compression_ratio": 1.6634615384615385, "no_speech_prob": 4.425447514222469e-06}, {"id": 400, "seek": 196556, "start": 1975.52, "end": 1978.76, "text": " for or is that not really the realm that it is?", "tokens": [337, 420, 307, 300, 406, 534, 264, 15355, 300, 309, 307, 30], "temperature": 0.0, "avg_logprob": -0.18704190747491245, "compression_ratio": 1.6634615384615385, "no_speech_prob": 4.425447514222469e-06}, {"id": 401, "seek": 196556, "start": 1978.76, "end": 1981.76, "text": " That's a great question.", "tokens": [663, 311, 257, 869, 1168, 13], "temperature": 0.0, "avg_logprob": -0.18704190747491245, "compression_ratio": 1.6634615384615385, "no_speech_prob": 4.425447514222469e-06}, {"id": 402, "seek": 196556, "start": 1981.76, "end": 1988.44, "text": " And I've had a lot of conversations about this with people in social sciences, and currently", "tokens": [400, 286, 600, 632, 257, 688, 295, 7315, 466, 341, 365, 561, 294, 2093, 17677, 11, 293, 4362], "temperature": 0.0, "avg_logprob": -0.18704190747491245, "compression_ratio": 1.6634615384615385, "no_speech_prob": 4.425447514222469e-06}, {"id": 403, "seek": 198844, "start": 1988.44, "end": 1995.96, "text": " machine learning is not well applied in economics or psychology or whatever on the whole.", "tokens": [3479, 2539, 307, 406, 731, 6456, 294, 14564, 420, 15105, 420, 2035, 322, 264, 1379, 13], "temperature": 0.0, "avg_logprob": -0.09793440500895183, "compression_ratio": 1.6938775510204083, "no_speech_prob": 2.857303798009525e-06}, {"id": 404, "seek": 198844, "start": 1995.96, "end": 1999.76, "text": " But I'm convinced it can be, for the exact reasons we're talking about.", "tokens": [583, 286, 478, 12561, 309, 393, 312, 11, 337, 264, 1900, 4112, 321, 434, 1417, 466, 13], "temperature": 0.0, "avg_logprob": -0.09793440500895183, "compression_ratio": 1.6938775510204083, "no_speech_prob": 2.857303798009525e-06}, {"id": 405, "seek": 198844, "start": 1999.76, "end": 2004.3600000000001, "text": " So if you're trying to figure out, if you're trying to do some kind of behavioral economics", "tokens": [407, 498, 291, 434, 1382, 281, 2573, 484, 11, 498, 291, 434, 1382, 281, 360, 512, 733, 295, 19124, 14564], "temperature": 0.0, "avg_logprob": -0.09793440500895183, "compression_ratio": 1.6938775510204083, "no_speech_prob": 2.857303798009525e-06}, {"id": 406, "seek": 198844, "start": 2004.3600000000001, "end": 2009.44, "text": " and you're trying to understand why some people behave differently to other people, a random", "tokens": [293, 291, 434, 1382, 281, 1223, 983, 512, 561, 15158, 7614, 281, 661, 561, 11, 257, 4974], "temperature": 0.0, "avg_logprob": -0.09793440500895183, "compression_ratio": 1.6938775510204083, "no_speech_prob": 2.857303798009525e-06}, {"id": 407, "seek": 198844, "start": 2009.44, "end": 2014.1200000000001, "text": " forest with a feature importance plot would be a great way to start.", "tokens": [6719, 365, 257, 4111, 7379, 7542, 576, 312, 257, 869, 636, 281, 722, 13], "temperature": 0.0, "avg_logprob": -0.09793440500895183, "compression_ratio": 1.6938775510204083, "no_speech_prob": 2.857303798009525e-06}, {"id": 408, "seek": 201412, "start": 2014.12, "end": 2021.1599999999999, "text": " Or more interestingly, if you're trying to do some kind of sociology experiment or analysis", "tokens": [1610, 544, 25873, 11, 498, 291, 434, 1382, 281, 360, 512, 733, 295, 41744, 5120, 420, 5215], "temperature": 0.0, "avg_logprob": -0.09292572147243626, "compression_ratio": 1.5959183673469388, "no_speech_prob": 3.041589252461563e-06}, {"id": 409, "seek": 201412, "start": 2021.1599999999999, "end": 2026.4799999999998, "text": " based on a large social network data set where you have an observational study, you really", "tokens": [2361, 322, 257, 2416, 2093, 3209, 1412, 992, 689, 291, 362, 364, 9951, 1478, 2979, 11, 291, 534], "temperature": 0.0, "avg_logprob": -0.09292572147243626, "compression_ratio": 1.5959183673469388, "no_speech_prob": 3.041589252461563e-06}, {"id": 410, "seek": 201412, "start": 2026.4799999999998, "end": 2033.7199999999998, "text": " want to try and pull out all of the sources of kind of exogenous variables, all the stuff", "tokens": [528, 281, 853, 293, 2235, 484, 439, 295, 264, 7139, 295, 733, 295, 454, 45519, 9102, 11, 439, 264, 1507], "temperature": 0.0, "avg_logprob": -0.09292572147243626, "compression_ratio": 1.5959183673469388, "no_speech_prob": 3.041589252461563e-06}, {"id": 411, "seek": 201412, "start": 2033.7199999999998, "end": 2035.8, "text": " that's going on outside.", "tokens": [300, 311, 516, 322, 2380, 13], "temperature": 0.0, "avg_logprob": -0.09292572147243626, "compression_ratio": 1.5959183673469388, "no_speech_prob": 3.041589252461563e-06}, {"id": 412, "seek": 201412, "start": 2035.8, "end": 2040.34, "text": " And so if you use a partial dependence plot with a random forest, that happens automatically.", "tokens": [400, 370, 498, 291, 764, 257, 14641, 31704, 7542, 365, 257, 4974, 6719, 11, 300, 2314, 6772, 13], "temperature": 0.0, "avg_logprob": -0.09292572147243626, "compression_ratio": 1.5959183673469388, "no_speech_prob": 3.041589252461563e-06}, {"id": 413, "seek": 204034, "start": 2040.34, "end": 2048.36, "text": " So I actually gave a talk at MIT a couple of years ago for the first conference on digital", "tokens": [407, 286, 767, 2729, 257, 751, 412, 13100, 257, 1916, 295, 924, 2057, 337, 264, 700, 7586, 322, 4562], "temperature": 0.0, "avg_logprob": -0.1954804759914592, "compression_ratio": 1.5108695652173914, "no_speech_prob": 7.071845629980089e-06}, {"id": 414, "seek": 204034, "start": 2048.36, "end": 2054.22, "text": " experimentation which was really talking about how do we experiment in things like social", "tokens": [37142, 597, 390, 534, 1417, 466, 577, 360, 321, 5120, 294, 721, 411, 2093], "temperature": 0.0, "avg_logprob": -0.1954804759914592, "compression_ratio": 1.5108695652173914, "no_speech_prob": 7.071845629980089e-06}, {"id": 415, "seek": 204034, "start": 2054.22, "end": 2063.64, "text": " networks and these digital environments.", "tokens": [9590, 293, 613, 4562, 12388, 13], "temperature": 0.0, "avg_logprob": -0.1954804759914592, "compression_ratio": 1.5108695652173914, "no_speech_prob": 7.071845629980089e-06}, {"id": 416, "seek": 204034, "start": 2063.64, "end": 2067.52, "text": " Economists all do things with classic statistical tests.", "tokens": [14821, 1751, 439, 360, 721, 365, 7230, 22820, 6921, 13], "temperature": 0.0, "avg_logprob": -0.1954804759914592, "compression_ratio": 1.5108695652173914, "no_speech_prob": 7.071845629980089e-06}, {"id": 417, "seek": 206752, "start": 2067.52, "end": 2080.94, "text": " But anyway, in this case, the economists I talked to were absolutely fascinated by this", "tokens": [583, 4033, 11, 294, 341, 1389, 11, 264, 32431, 286, 2825, 281, 645, 3122, 24597, 538, 341], "temperature": 0.0, "avg_logprob": -0.13466081252464882, "compression_ratio": 1.4470588235294117, "no_speech_prob": 7.527827165176859e-06}, {"id": 418, "seek": 206752, "start": 2080.94, "end": 2089.22, "text": " and they actually asked me to give an introduction to machine learning session at MIT to these", "tokens": [293, 436, 767, 2351, 385, 281, 976, 364, 9339, 281, 3479, 2539, 5481, 412, 13100, 281, 613], "temperature": 0.0, "avg_logprob": -0.13466081252464882, "compression_ratio": 1.4470588235294117, "no_speech_prob": 7.527827165176859e-06}, {"id": 419, "seek": 206752, "start": 2089.22, "end": 2093.5, "text": " various faculty and graduate folks in the economics department.", "tokens": [3683, 6389, 293, 8080, 4024, 294, 264, 14564, 5882, 13], "temperature": 0.0, "avg_logprob": -0.13466081252464882, "compression_ratio": 1.4470588235294117, "no_speech_prob": 7.527827165176859e-06}, {"id": 420, "seek": 209350, "start": 2093.5, "end": 2098.76, "text": " And some of those folks have gone on to write some pretty famous books and stuff and so", "tokens": [400, 512, 295, 729, 4024, 362, 2780, 322, 281, 2464, 512, 1238, 4618, 3642, 293, 1507, 293, 370], "temperature": 0.0, "avg_logprob": -0.25021749554258405, "compression_ratio": 1.4464285714285714, "no_speech_prob": 3.1692332413513213e-05}, {"id": 421, "seek": 209350, "start": 2098.76, "end": 2100.44, "text": " hopefully it's been useful.", "tokens": [4696, 309, 311, 668, 4420, 13], "temperature": 0.0, "avg_logprob": -0.25021749554258405, "compression_ratio": 1.4464285714285714, "no_speech_prob": 3.1692332413513213e-05}, {"id": 422, "seek": 209350, "start": 2100.44, "end": 2105.28, "text": " It's definitely early days, but it's a big, big opportunity.", "tokens": [467, 311, 2138, 2440, 1708, 11, 457, 309, 311, 257, 955, 11, 955, 2650, 13], "temperature": 0.0, "avg_logprob": -0.25021749554258405, "compression_ratio": 1.4464285714285714, "no_speech_prob": 3.1692332413513213e-05}, {"id": 423, "seek": 209350, "start": 2105.28, "end": 2118.68, "text": " But as Yannett says, there's plenty of skepticism still out there.", "tokens": [583, 382, 398, 969, 3093, 1619, 11, 456, 311, 7140, 295, 19128, 26356, 920, 484, 456, 13], "temperature": 0.0, "avg_logprob": -0.25021749554258405, "compression_ratio": 1.4464285714285714, "no_speech_prob": 3.1692332413513213e-05}, {"id": 424, "seek": 211868, "start": 2118.68, "end": 2128.8399999999997, "text": " Well the skepticism comes from unfamiliarity basically with this totally different approach.", "tokens": [1042, 264, 19128, 26356, 1487, 490, 29415, 507, 1936, 365, 341, 3879, 819, 3109, 13], "temperature": 0.0, "avg_logprob": -0.13868792851765951, "compression_ratio": 1.5977011494252873, "no_speech_prob": 1.6701191270840354e-05}, {"id": 425, "seek": 211868, "start": 2128.8399999999997, "end": 2137.62, "text": " So like if you spent 20 years studying econometrics and somebody comes along and says, here's", "tokens": [407, 411, 498, 291, 4418, 945, 924, 7601, 23692, 649, 10716, 293, 2618, 1487, 2051, 293, 1619, 11, 510, 311], "temperature": 0.0, "avg_logprob": -0.13868792851765951, "compression_ratio": 1.5977011494252873, "no_speech_prob": 1.6701191270840354e-05}, {"id": 426, "seek": 211868, "start": 2137.62, "end": 2144.46, "text": " a totally different approach to all the stuff that econometricians do, naturally your first", "tokens": [257, 3879, 819, 3109, 281, 439, 264, 1507, 300, 23692, 29470, 2567, 360, 11, 8195, 428, 700], "temperature": 0.0, "avg_logprob": -0.13868792851765951, "compression_ratio": 1.5977011494252873, "no_speech_prob": 1.6701191270840354e-05}, {"id": 427, "seek": 214446, "start": 2144.46, "end": 2149.2400000000002, "text": " reaction will be like, prove it.", "tokens": [5480, 486, 312, 411, 11, 7081, 309, 13], "temperature": 0.0, "avg_logprob": -0.15780440144155217, "compression_ratio": 1.5446428571428572, "no_speech_prob": 1.8924993128166534e-05}, {"id": 428, "seek": 214446, "start": 2149.2400000000002, "end": 2152.48, "text": " So that's fair enough.", "tokens": [407, 300, 311, 3143, 1547, 13], "temperature": 0.0, "avg_logprob": -0.15780440144155217, "compression_ratio": 1.5446428571428572, "no_speech_prob": 1.8924993128166534e-05}, {"id": 429, "seek": 214446, "start": 2152.48, "end": 2159.58, "text": " But I think over time, the next generation of people who are growing up with machine", "tokens": [583, 286, 519, 670, 565, 11, 264, 958, 5125, 295, 561, 567, 366, 4194, 493, 365, 3479], "temperature": 0.0, "avg_logprob": -0.15780440144155217, "compression_ratio": 1.5446428571428572, "no_speech_prob": 1.8924993128166534e-05}, {"id": 430, "seek": 214446, "start": 2159.58, "end": 2164.04, "text": " learning, some of them will move into the social sciences, they'll make huge impacts", "tokens": [2539, 11, 512, 295, 552, 486, 1286, 666, 264, 2093, 17677, 11, 436, 603, 652, 2603, 11606], "temperature": 0.0, "avg_logprob": -0.15780440144155217, "compression_ratio": 1.5446428571428572, "no_speech_prob": 1.8924993128166534e-05}, {"id": 431, "seek": 214446, "start": 2164.04, "end": 2168.56, "text": " that nobody's ever managed to make before, and people will start going, wow.", "tokens": [300, 5079, 311, 1562, 6453, 281, 652, 949, 11, 293, 561, 486, 722, 516, 11, 6076, 13], "temperature": 0.0, "avg_logprob": -0.15780440144155217, "compression_ratio": 1.5446428571428572, "no_speech_prob": 1.8924993128166534e-05}, {"id": 432, "seek": 214446, "start": 2168.56, "end": 2170.4, "text": " Just like what happened in computer vision.", "tokens": [1449, 411, 437, 2011, 294, 3820, 5201, 13], "temperature": 0.0, "avg_logprob": -0.15780440144155217, "compression_ratio": 1.5446428571428572, "no_speech_prob": 1.8924993128166534e-05}, {"id": 433, "seek": 217040, "start": 2170.4, "end": 2175.88, "text": " When computer vision spent a long time of people saying, hey maybe you should use deep", "tokens": [1133, 3820, 5201, 4418, 257, 938, 565, 295, 561, 1566, 11, 4177, 1310, 291, 820, 764, 2452], "temperature": 0.0, "avg_logprob": -0.21288572383832327, "compression_ratio": 1.6116504854368932, "no_speech_prob": 2.355219294258859e-05}, {"id": 434, "seek": 217040, "start": 2175.88, "end": 2180.88, "text": " learning for computer vision, and everybody in computer vision is like, prove it.", "tokens": [2539, 337, 3820, 5201, 11, 293, 2201, 294, 3820, 5201, 307, 411, 11, 7081, 309, 13], "temperature": 0.0, "avg_logprob": -0.21288572383832327, "compression_ratio": 1.6116504854368932, "no_speech_prob": 2.355219294258859e-05}, {"id": 435, "seek": 217040, "start": 2180.88, "end": 2187.6, "text": " We have decades of work on amazing feature detectors for computer vision.", "tokens": [492, 362, 7878, 295, 589, 322, 2243, 4111, 46866, 337, 3820, 5201, 13], "temperature": 0.0, "avg_logprob": -0.21288572383832327, "compression_ratio": 1.6116504854368932, "no_speech_prob": 2.355219294258859e-05}, {"id": 436, "seek": 217040, "start": 2187.6, "end": 2194.2000000000003, "text": " And then finally in 2012 Hinton and Kredesky came along and said, okay, our model is like", "tokens": [400, 550, 2721, 294, 9125, 389, 12442, 293, 591, 986, 279, 4133, 1361, 2051, 293, 848, 11, 1392, 11, 527, 2316, 307, 411], "temperature": 0.0, "avg_logprob": -0.21288572383832327, "compression_ratio": 1.6116504854368932, "no_speech_prob": 2.355219294258859e-05}, {"id": 437, "seek": 219420, "start": 2194.2, "end": 2201.3999999999996, "text": " twice as good as yours, and we've only just started on this, and everybody was like, oh", "tokens": [6091, 382, 665, 382, 6342, 11, 293, 321, 600, 787, 445, 1409, 322, 341, 11, 293, 2201, 390, 411, 11, 1954], "temperature": 0.0, "avg_logprob": -0.18106102081666509, "compression_ratio": 1.5714285714285714, "no_speech_prob": 3.905464382114587e-06}, {"id": 438, "seek": 219420, "start": 2201.3999999999996, "end": 2205.2, "text": " okay, that's pretty convincing.", "tokens": [1392, 11, 300, 311, 1238, 24823, 13], "temperature": 0.0, "avg_logprob": -0.18106102081666509, "compression_ratio": 1.5714285714285714, "no_speech_prob": 3.905464382114587e-06}, {"id": 439, "seek": 219420, "start": 2205.2, "end": 2208.72, "text": " Nowadays every computer vision researcher basically uses deep learning.", "tokens": [28908, 633, 3820, 5201, 21751, 1936, 4960, 2452, 2539, 13], "temperature": 0.0, "avg_logprob": -0.18106102081666509, "compression_ratio": 1.5714285714285714, "no_speech_prob": 3.905464382114587e-06}, {"id": 440, "seek": 219420, "start": 2208.72, "end": 2216.16, "text": " So I think that time will come in this area too.", "tokens": [407, 286, 519, 300, 565, 486, 808, 294, 341, 1859, 886, 13], "temperature": 0.0, "avg_logprob": -0.18106102081666509, "compression_ratio": 1.5714285714285714, "no_speech_prob": 3.905464382114587e-06}, {"id": 441, "seek": 219420, "start": 2216.16, "end": 2223.6, "text": " I think what we might do then is take a break and we're going to come back and talk about", "tokens": [286, 519, 437, 321, 1062, 360, 550, 307, 747, 257, 1821, 293, 321, 434, 516, 281, 808, 646, 293, 751, 466], "temperature": 0.0, "avg_logprob": -0.18106102081666509, "compression_ratio": 1.5714285714285714, "no_speech_prob": 3.905464382114587e-06}, {"id": 442, "seek": 222360, "start": 2223.6, "end": 2229.16, "text": " these random forest interpretation techniques and do a bit of a review.", "tokens": [613, 4974, 6719, 14174, 7512, 293, 360, 257, 857, 295, 257, 3131, 13], "temperature": 0.0, "avg_logprob": -0.11026169004894439, "compression_ratio": 1.6910994764397906, "no_speech_prob": 1.2029539902869146e-05}, {"id": 443, "seek": 222360, "start": 2229.16, "end": 2238.12, "text": " So let's come back at 2 o'clock.", "tokens": [407, 718, 311, 808, 646, 412, 568, 277, 6, 9023, 13], "temperature": 0.0, "avg_logprob": -0.11026169004894439, "compression_ratio": 1.6910994764397906, "no_speech_prob": 1.2029539902869146e-05}, {"id": 444, "seek": 222360, "start": 2238.12, "end": 2244.56, "text": " So let's have a go at talking about these different random forest interpretation methods,", "tokens": [407, 718, 311, 362, 257, 352, 412, 1417, 466, 613, 819, 4974, 6719, 14174, 7150, 11], "temperature": 0.0, "avg_logprob": -0.11026169004894439, "compression_ratio": 1.6910994764397906, "no_speech_prob": 1.2029539902869146e-05}, {"id": 445, "seek": 222360, "start": 2244.56, "end": 2246.96, "text": " having talked about why they're important.", "tokens": [1419, 2825, 466, 983, 436, 434, 1021, 13], "temperature": 0.0, "avg_logprob": -0.11026169004894439, "compression_ratio": 1.6910994764397906, "no_speech_prob": 1.2029539902869146e-05}, {"id": 446, "seek": 222360, "start": 2246.96, "end": 2249.4, "text": " So let's now remind ourselves what they are.", "tokens": [407, 718, 311, 586, 4160, 4175, 437, 436, 366, 13], "temperature": 0.0, "avg_logprob": -0.11026169004894439, "compression_ratio": 1.6910994764397906, "no_speech_prob": 1.2029539902869146e-05}, {"id": 447, "seek": 222360, "start": 2249.4, "end": 2253.4, "text": " So I'm going to let you folks have a go.", "tokens": [407, 286, 478, 516, 281, 718, 291, 4024, 362, 257, 352, 13], "temperature": 0.0, "avg_logprob": -0.11026169004894439, "compression_ratio": 1.6910994764397906, "no_speech_prob": 1.2029539902869146e-05}, {"id": 448, "seek": 225340, "start": 2253.4, "end": 2260.6800000000003, "text": " So let's start with confidence based on tree variance.", "tokens": [407, 718, 311, 722, 365, 6687, 2361, 322, 4230, 21977, 13], "temperature": 0.0, "avg_logprob": -0.1627708210664637, "compression_ratio": 1.6368421052631579, "no_speech_prob": 5.3910007409285754e-05}, {"id": 449, "seek": 225340, "start": 2260.6800000000003, "end": 2266.2000000000003, "text": " So can one of you tell me one or more of the following things about confidence based on", "tokens": [407, 393, 472, 295, 291, 980, 385, 472, 420, 544, 295, 264, 3480, 721, 466, 6687, 2361, 322], "temperature": 0.0, "avg_logprob": -0.1627708210664637, "compression_ratio": 1.6368421052631579, "no_speech_prob": 5.3910007409285754e-05}, {"id": 450, "seek": 225340, "start": 2266.2000000000003, "end": 2268.6800000000003, "text": " tree variance?", "tokens": [4230, 21977, 30], "temperature": 0.0, "avg_logprob": -0.1627708210664637, "compression_ratio": 1.6368421052631579, "no_speech_prob": 5.3910007409285754e-05}, {"id": 451, "seek": 225340, "start": 2268.6800000000003, "end": 2270.7400000000002, "text": " What does it tell us?", "tokens": [708, 775, 309, 980, 505, 30], "temperature": 0.0, "avg_logprob": -0.1627708210664637, "compression_ratio": 1.6368421052631579, "no_speech_prob": 5.3910007409285754e-05}, {"id": 452, "seek": 225340, "start": 2270.7400000000002, "end": 2273.26, "text": " Why would we be interested in that?", "tokens": [1545, 576, 321, 312, 3102, 294, 300, 30], "temperature": 0.0, "avg_logprob": -0.1627708210664637, "compression_ratio": 1.6368421052631579, "no_speech_prob": 5.3910007409285754e-05}, {"id": 453, "seek": 225340, "start": 2273.26, "end": 2275.2000000000003, "text": " And how is it calculated?", "tokens": [400, 577, 307, 309, 15598, 30], "temperature": 0.0, "avg_logprob": -0.1627708210664637, "compression_ratio": 1.6368421052631579, "no_speech_prob": 5.3910007409285754e-05}, {"id": 454, "seek": 225340, "start": 2275.2000000000003, "end": 2281.52, "text": " This is going back a ways, because it was the first one we looked at.", "tokens": [639, 307, 516, 646, 257, 2098, 11, 570, 309, 390, 264, 700, 472, 321, 2956, 412, 13], "temperature": 0.0, "avg_logprob": -0.1627708210664637, "compression_ratio": 1.6368421052631579, "no_speech_prob": 5.3910007409285754e-05}, {"id": 455, "seek": 228152, "start": 2281.52, "end": 2285.24, "text": " Even if you're not sure, you only know a little piece of it, give us your piece and we'll", "tokens": [2754, 498, 291, 434, 406, 988, 11, 291, 787, 458, 257, 707, 2522, 295, 309, 11, 976, 505, 428, 2522, 293, 321, 603], "temperature": 0.0, "avg_logprob": -0.21115004799582743, "compression_ratio": 1.6578947368421053, "no_speech_prob": 3.2190902857109904e-05}, {"id": 456, "seek": 228152, "start": 2285.24, "end": 2286.24, "text": " build on it together.", "tokens": [1322, 322, 309, 1214, 13], "temperature": 0.0, "avg_logprob": -0.21115004799582743, "compression_ratio": 1.6578947368421053, "no_speech_prob": 3.2190902857109904e-05}, {"id": 457, "seek": 228152, "start": 2286.24, "end": 2293.44, "text": " I think I got a piece of it.", "tokens": [286, 519, 286, 658, 257, 2522, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.21115004799582743, "compression_ratio": 1.6578947368421053, "no_speech_prob": 3.2190902857109904e-05}, {"id": 458, "seek": 228152, "start": 2293.44, "end": 2299.88, "text": " It's getting the variance of our predictions from random forests.", "tokens": [467, 311, 1242, 264, 21977, 295, 527, 21264, 490, 4974, 21700, 13], "temperature": 0.0, "avg_logprob": -0.21115004799582743, "compression_ratio": 1.6578947368421053, "no_speech_prob": 3.2190902857109904e-05}, {"id": 459, "seek": 228152, "start": 2299.88, "end": 2300.88, "text": " That's true, that's the how.", "tokens": [663, 311, 2074, 11, 300, 311, 264, 577, 13], "temperature": 0.0, "avg_logprob": -0.21115004799582743, "compression_ratio": 1.6578947368421053, "no_speech_prob": 3.2190902857109904e-05}, {"id": 460, "seek": 228152, "start": 2300.88, "end": 2302.32, "text": " Can you be more specific?", "tokens": [1664, 291, 312, 544, 2685, 30], "temperature": 0.0, "avg_logprob": -0.21115004799582743, "compression_ratio": 1.6578947368421053, "no_speech_prob": 3.2190902857109904e-05}, {"id": 461, "seek": 228152, "start": 2302.32, "end": 2304.6, "text": " So what is it the variance of?", "tokens": [407, 437, 307, 309, 264, 21977, 295, 30], "temperature": 0.0, "avg_logprob": -0.21115004799582743, "compression_ratio": 1.6578947368421053, "no_speech_prob": 3.2190902857109904e-05}, {"id": 462, "seek": 228152, "start": 2304.6, "end": 2311.32, "text": " I think it's, if I'm remembering correctly, I think it's just the overall prediction.", "tokens": [286, 519, 309, 311, 11, 498, 286, 478, 20719, 8944, 11, 286, 519, 309, 311, 445, 264, 4787, 17630, 13], "temperature": 0.0, "avg_logprob": -0.21115004799582743, "compression_ratio": 1.6578947368421053, "no_speech_prob": 3.2190902857109904e-05}, {"id": 463, "seek": 231132, "start": 2311.32, "end": 2314.32, "text": " The variance of the predictions of the trees.", "tokens": [440, 21977, 295, 264, 21264, 295, 264, 5852, 13], "temperature": 0.0, "avg_logprob": -0.1732913851737976, "compression_ratio": 1.8095238095238095, "no_speech_prob": 1.0616049621603452e-05}, {"id": 464, "seek": 231132, "start": 2314.32, "end": 2318.92, "text": " So normally the prediction is just the average, this is the variance of the trees.", "tokens": [407, 5646, 264, 17630, 307, 445, 264, 4274, 11, 341, 307, 264, 21977, 295, 264, 5852, 13], "temperature": 0.0, "avg_logprob": -0.1732913851737976, "compression_ratio": 1.8095238095238095, "no_speech_prob": 1.0616049621603452e-05}, {"id": 465, "seek": 231132, "start": 2318.92, "end": 2322.96, "text": " So it kind of just gives you an idea of how much your prediction is going to vary.", "tokens": [407, 309, 733, 295, 445, 2709, 291, 364, 1558, 295, 577, 709, 428, 17630, 307, 516, 281, 10559, 13], "temperature": 0.0, "avg_logprob": -0.1732913851737976, "compression_ratio": 1.8095238095238095, "no_speech_prob": 1.0616049621603452e-05}, {"id": 466, "seek": 231132, "start": 2322.96, "end": 2326.92, "text": " So maybe you want to minimize variance, maybe that's your goal for whatever reason that", "tokens": [407, 1310, 291, 528, 281, 17522, 21977, 11, 1310, 300, 311, 428, 3387, 337, 2035, 1778, 300], "temperature": 0.0, "avg_logprob": -0.1732913851737976, "compression_ratio": 1.8095238095238095, "no_speech_prob": 1.0616049621603452e-05}, {"id": 467, "seek": 231132, "start": 2326.92, "end": 2328.4, "text": " could be.", "tokens": [727, 312, 13], "temperature": 0.0, "avg_logprob": -0.1732913851737976, "compression_ratio": 1.8095238095238095, "no_speech_prob": 1.0616049621603452e-05}, {"id": 468, "seek": 231132, "start": 2328.4, "end": 2329.88, "text": " That's not so much the reason.", "tokens": [663, 311, 406, 370, 709, 264, 1778, 13], "temperature": 0.0, "avg_logprob": -0.1732913851737976, "compression_ratio": 1.8095238095238095, "no_speech_prob": 1.0616049621603452e-05}, {"id": 469, "seek": 231132, "start": 2329.88, "end": 2331.84, "text": " So I like your calculation description.", "tokens": [407, 286, 411, 428, 17108, 3855, 13], "temperature": 0.0, "avg_logprob": -0.1732913851737976, "compression_ratio": 1.8095238095238095, "no_speech_prob": 1.0616049621603452e-05}, {"id": 470, "seek": 233184, "start": 2331.84, "end": 2348.8, "text": " Let's see if somebody else can tell us how you might use that.", "tokens": [961, 311, 536, 498, 2618, 1646, 393, 980, 505, 577, 291, 1062, 764, 300, 13], "temperature": 0.0, "avg_logprob": -0.25571595921235923, "compression_ratio": 1.2190476190476192, "no_speech_prob": 8.801056537777185e-06}, {"id": 471, "seek": 233184, "start": 2348.8, "end": 2355.6400000000003, "text": " So I remember that we talked about the independence of the trees.", "tokens": [407, 286, 1604, 300, 321, 2825, 466, 264, 14640, 295, 264, 5852, 13], "temperature": 0.0, "avg_logprob": -0.25571595921235923, "compression_ratio": 1.2190476190476192, "no_speech_prob": 8.801056537777185e-06}, {"id": 472, "seek": 235564, "start": 2355.64, "end": 2365.3199999999997, "text": " So maybe something about if the variance of the trees is higher or lower than, you know.", "tokens": [407, 1310, 746, 466, 498, 264, 21977, 295, 264, 5852, 307, 2946, 420, 3126, 813, 11, 291, 458, 13], "temperature": 0.0, "avg_logprob": -0.2328266321226608, "compression_ratio": 1.5346534653465347, "no_speech_prob": 9.666031473898329e-06}, {"id": 473, "seek": 235564, "start": 2365.3199999999997, "end": 2368.64, "text": " No not so much that.", "tokens": [883, 406, 370, 709, 300, 13], "temperature": 0.0, "avg_logprob": -0.2328266321226608, "compression_ratio": 1.5346534653465347, "no_speech_prob": 9.666031473898329e-06}, {"id": 474, "seek": 235564, "start": 2368.64, "end": 2372.2, "text": " That's an interesting question, but it's not what we're going to see here.", "tokens": [663, 311, 364, 1880, 1168, 11, 457, 309, 311, 406, 437, 321, 434, 516, 281, 536, 510, 13], "temperature": 0.0, "avg_logprob": -0.2328266321226608, "compression_ratio": 1.5346534653465347, "no_speech_prob": 9.666031473898329e-06}, {"id": 475, "seek": 235564, "start": 2372.2, "end": 2375.0, "text": " Do you want to pass it back behind you?", "tokens": [1144, 291, 528, 281, 1320, 309, 646, 2261, 291, 30], "temperature": 0.0, "avg_logprob": -0.2328266321226608, "compression_ratio": 1.5346534653465347, "no_speech_prob": 9.666031473898329e-06}, {"id": 476, "seek": 235564, "start": 2375.0, "end": 2379.96, "text": " So to remind you, just to fill in a detail here, what we generally do here is we take", "tokens": [407, 281, 4160, 291, 11, 445, 281, 2836, 294, 257, 2607, 510, 11, 437, 321, 5101, 360, 510, 307, 321, 747], "temperature": 0.0, "avg_logprob": -0.2328266321226608, "compression_ratio": 1.5346534653465347, "no_speech_prob": 9.666031473898329e-06}, {"id": 477, "seek": 237996, "start": 2379.96, "end": 2387.76, "text": " just one row, like one observation often, and find out how confident we are about that,", "tokens": [445, 472, 5386, 11, 411, 472, 14816, 2049, 11, 293, 915, 484, 577, 6679, 321, 366, 466, 300, 11], "temperature": 0.0, "avg_logprob": -0.17363587054577503, "compression_ratio": 1.7227272727272727, "no_speech_prob": 7.64646665629698e-06}, {"id": 478, "seek": 237996, "start": 2387.76, "end": 2390.32, "text": " like how much variance there are in the trees for that.", "tokens": [411, 577, 709, 21977, 456, 366, 294, 264, 5852, 337, 300, 13], "temperature": 0.0, "avg_logprob": -0.17363587054577503, "compression_ratio": 1.7227272727272727, "no_speech_prob": 7.64646665629698e-06}, {"id": 479, "seek": 237996, "start": 2390.32, "end": 2395.0, "text": " Or we can do it as we did here for different groups.", "tokens": [1610, 321, 393, 360, 309, 382, 321, 630, 510, 337, 819, 3935, 13], "temperature": 0.0, "avg_logprob": -0.17363587054577503, "compression_ratio": 1.7227272727272727, "no_speech_prob": 7.64646665629698e-06}, {"id": 480, "seek": 237996, "start": 2395.0, "end": 2401.48, "text": " So according to me, the idea is like for each row, we calculate the standard deviation that", "tokens": [407, 4650, 281, 385, 11, 264, 1558, 307, 411, 337, 1184, 5386, 11, 321, 8873, 264, 3832, 25163, 300], "temperature": 0.0, "avg_logprob": -0.17363587054577503, "compression_ratio": 1.7227272727272727, "no_speech_prob": 7.64646665629698e-06}, {"id": 481, "seek": 237996, "start": 2401.48, "end": 2408.2400000000002, "text": " we get from the random forest model, and then maybe group according to different variables", "tokens": [321, 483, 490, 264, 4974, 6719, 2316, 11, 293, 550, 1310, 1594, 4650, 281, 819, 9102], "temperature": 0.0, "avg_logprob": -0.17363587054577503, "compression_ratio": 1.7227272727272727, "no_speech_prob": 7.64646665629698e-06}, {"id": 482, "seek": 240824, "start": 2408.24, "end": 2413.68, "text": " or predictors and see for which particular predictor the standard deviation is high.", "tokens": [420, 6069, 830, 293, 536, 337, 597, 1729, 6069, 284, 264, 3832, 25163, 307, 1090, 13], "temperature": 0.0, "avg_logprob": -0.22283910395024897, "compression_ratio": 1.5892857142857142, "no_speech_prob": 1.7502707123639993e-05}, {"id": 483, "seek": 240824, "start": 2413.68, "end": 2417.16, "text": " Then go deep down as why it is happening.", "tokens": [1396, 352, 2452, 760, 382, 983, 309, 307, 2737, 13], "temperature": 0.0, "avg_logprob": -0.22283910395024897, "compression_ratio": 1.5892857142857142, "no_speech_prob": 1.7502707123639993e-05}, {"id": 484, "seek": 240824, "start": 2417.16, "end": 2423.3199999999997, "text": " Maybe it is because a particular category of that variable has very less number of observations.", "tokens": [2704, 309, 307, 570, 257, 1729, 7719, 295, 300, 7006, 575, 588, 1570, 1230, 295, 18163, 13], "temperature": 0.0, "avg_logprob": -0.22283910395024897, "compression_ratio": 1.5892857142857142, "no_speech_prob": 1.7502707123639993e-05}, {"id": 485, "seek": 240824, "start": 2423.3199999999997, "end": 2424.3199999999997, "text": " Yeah, that's great.", "tokens": [865, 11, 300, 311, 869, 13], "temperature": 0.0, "avg_logprob": -0.22283910395024897, "compression_ratio": 1.5892857142857142, "no_speech_prob": 1.7502707123639993e-05}, {"id": 486, "seek": 240824, "start": 2424.3199999999997, "end": 2426.2799999999997, "text": " So that would be one approach.", "tokens": [407, 300, 576, 312, 472, 3109, 13], "temperature": 0.0, "avg_logprob": -0.22283910395024897, "compression_ratio": 1.5892857142857142, "no_speech_prob": 1.7502707123639993e-05}, {"id": 487, "seek": 240824, "start": 2426.2799999999997, "end": 2434.3199999999997, "text": " What we've done here is to say, is there any groups where we're very unconfident?", "tokens": [708, 321, 600, 1096, 510, 307, 281, 584, 11, 307, 456, 604, 3935, 689, 321, 434, 588, 517, 24697, 1078, 30], "temperature": 0.0, "avg_logprob": -0.22283910395024897, "compression_ratio": 1.5892857142857142, "no_speech_prob": 1.7502707123639993e-05}, {"id": 488, "seek": 243432, "start": 2434.32, "end": 2442.0, "text": " Something that I think is even more important would be when you're using this like operationally,", "tokens": [6595, 300, 286, 519, 307, 754, 544, 1021, 576, 312, 562, 291, 434, 1228, 341, 411, 6916, 379, 11], "temperature": 0.0, "avg_logprob": -0.1346858137397356, "compression_ratio": 1.5656108597285068, "no_speech_prob": 6.540417416545097e-06}, {"id": 489, "seek": 243432, "start": 2442.0, "end": 2446.36, "text": " let's say you're doing a credit decisioning algorithm.", "tokens": [718, 311, 584, 291, 434, 884, 257, 5397, 3537, 278, 9284, 13], "temperature": 0.0, "avg_logprob": -0.1346858137397356, "compression_ratio": 1.5656108597285068, "no_speech_prob": 6.540417416545097e-06}, {"id": 490, "seek": 243432, "start": 2446.36, "end": 2450.28, "text": " So we're trying to say like, okay, is Jeremy a good risk or a bad risk?", "tokens": [407, 321, 434, 1382, 281, 584, 411, 11, 1392, 11, 307, 17809, 257, 665, 3148, 420, 257, 1578, 3148, 30], "temperature": 0.0, "avg_logprob": -0.1346858137397356, "compression_ratio": 1.5656108597285068, "no_speech_prob": 6.540417416545097e-06}, {"id": 491, "seek": 243432, "start": 2450.28, "end": 2454.04, "text": " Should we loan him a million dollars?", "tokens": [6454, 321, 10529, 796, 257, 2459, 3808, 30], "temperature": 0.0, "avg_logprob": -0.1346858137397356, "compression_ratio": 1.5656108597285068, "no_speech_prob": 6.540417416545097e-06}, {"id": 492, "seek": 243432, "start": 2454.04, "end": 2462.9, "text": " And the random forest says, I think he's a good risk, but I'm not at all confident.", "tokens": [400, 264, 4974, 6719, 1619, 11, 286, 519, 415, 311, 257, 665, 3148, 11, 457, 286, 478, 406, 412, 439, 6679, 13], "temperature": 0.0, "avg_logprob": -0.1346858137397356, "compression_ratio": 1.5656108597285068, "no_speech_prob": 6.540417416545097e-06}, {"id": 493, "seek": 246290, "start": 2462.9, "end": 2465.92, "text": " In which case we might say, okay, maybe I shouldn't give him a million dollars.", "tokens": [682, 597, 1389, 321, 1062, 584, 11, 1392, 11, 1310, 286, 4659, 380, 976, 796, 257, 2459, 3808, 13], "temperature": 0.0, "avg_logprob": -0.16688861268939395, "compression_ratio": 1.740566037735849, "no_speech_prob": 9.223430424754042e-06}, {"id": 494, "seek": 246290, "start": 2465.92, "end": 2472.2400000000002, "text": " Or else if the random forest said, I think he's a good risk, I am very sure of that,", "tokens": [1610, 1646, 498, 264, 4974, 6719, 848, 11, 286, 519, 415, 311, 257, 665, 3148, 11, 286, 669, 588, 988, 295, 300, 11], "temperature": 0.0, "avg_logprob": -0.16688861268939395, "compression_ratio": 1.740566037735849, "no_speech_prob": 9.223430424754042e-06}, {"id": 495, "seek": 246290, "start": 2472.2400000000002, "end": 2476.96, "text": " then we're much more comfortable giving him a million dollars.", "tokens": [550, 321, 434, 709, 544, 4619, 2902, 796, 257, 2459, 3808, 13], "temperature": 0.0, "avg_logprob": -0.16688861268939395, "compression_ratio": 1.740566037735849, "no_speech_prob": 9.223430424754042e-06}, {"id": 496, "seek": 246290, "start": 2476.96, "end": 2481.88, "text": " And I'm a very good risk, so feel free to give me a million dollars.", "tokens": [400, 286, 478, 257, 588, 665, 3148, 11, 370, 841, 1737, 281, 976, 385, 257, 2459, 3808, 13], "temperature": 0.0, "avg_logprob": -0.16688861268939395, "compression_ratio": 1.740566037735849, "no_speech_prob": 9.223430424754042e-06}, {"id": 497, "seek": 246290, "start": 2481.88, "end": 2488.92, "text": " I checked the random forest before, different notebook, not in the repo.", "tokens": [286, 10033, 264, 4974, 6719, 949, 11, 819, 21060, 11, 406, 294, 264, 49040, 13], "temperature": 0.0, "avg_logprob": -0.16688861268939395, "compression_ratio": 1.740566037735849, "no_speech_prob": 9.223430424754042e-06}, {"id": 498, "seek": 248892, "start": 2488.92, "end": 2496.56, "text": " So like this is like, it's quite hard for me to give you folks direct experience with", "tokens": [407, 411, 341, 307, 411, 11, 309, 311, 1596, 1152, 337, 385, 281, 976, 291, 4024, 2047, 1752, 365], "temperature": 0.0, "avg_logprob": -0.15237453709477963, "compression_ratio": 1.6177777777777778, "no_speech_prob": 4.2228089114360046e-06}, {"id": 499, "seek": 248892, "start": 2496.56, "end": 2504.96, "text": " this kind of like, single observation, interpretation stuff, because it's really like the kind of", "tokens": [341, 733, 295, 411, 11, 2167, 14816, 11, 14174, 1507, 11, 570, 309, 311, 534, 411, 264, 733, 295], "temperature": 0.0, "avg_logprob": -0.15237453709477963, "compression_ratio": 1.6177777777777778, "no_speech_prob": 4.2228089114360046e-06}, {"id": 500, "seek": 248892, "start": 2504.96, "end": 2508.6, "text": " stuff that you actually need to be putting out to the front line.", "tokens": [1507, 300, 291, 767, 643, 281, 312, 3372, 484, 281, 264, 1868, 1622, 13], "temperature": 0.0, "avg_logprob": -0.15237453709477963, "compression_ratio": 1.6177777777777778, "no_speech_prob": 4.2228089114360046e-06}, {"id": 501, "seek": 248892, "start": 2508.6, "end": 2509.6, "text": " Do you know what I mean?", "tokens": [1144, 291, 458, 437, 286, 914, 30], "temperature": 0.0, "avg_logprob": -0.15237453709477963, "compression_ratio": 1.6177777777777778, "no_speech_prob": 4.2228089114360046e-06}, {"id": 502, "seek": 248892, "start": 2509.6, "end": 2514.76, "text": " Like it's not something which you can really use so much in a kind of Kaggle context, but", "tokens": [1743, 309, 311, 406, 746, 597, 291, 393, 534, 764, 370, 709, 294, 257, 733, 295, 48751, 22631, 4319, 11, 457], "temperature": 0.0, "avg_logprob": -0.15237453709477963, "compression_ratio": 1.6177777777777778, "no_speech_prob": 4.2228089114360046e-06}, {"id": 503, "seek": 251476, "start": 2514.76, "end": 2521.6400000000003, "text": " it's more like, okay, if you're actually putting out some algorithm which is making like big", "tokens": [309, 311, 544, 411, 11, 1392, 11, 498, 291, 434, 767, 3372, 484, 512, 9284, 597, 307, 1455, 411, 955], "temperature": 0.0, "avg_logprob": -0.16447591233527523, "compression_ratio": 1.6545454545454545, "no_speech_prob": 5.68240193388192e-06}, {"id": 504, "seek": 251476, "start": 2521.6400000000003, "end": 2527.36, "text": " decisions that could cost a lot of money, you probably don't so much care about the", "tokens": [5327, 300, 727, 2063, 257, 688, 295, 1460, 11, 291, 1391, 500, 380, 370, 709, 1127, 466, 264], "temperature": 0.0, "avg_logprob": -0.16447591233527523, "compression_ratio": 1.6545454545454545, "no_speech_prob": 5.68240193388192e-06}, {"id": 505, "seek": 251476, "start": 2527.36, "end": 2532.48, "text": " average prediction of the random forest, but maybe you actually care about like the average", "tokens": [4274, 17630, 295, 264, 4974, 6719, 11, 457, 1310, 291, 767, 1127, 466, 411, 264, 4274], "temperature": 0.0, "avg_logprob": -0.16447591233527523, "compression_ratio": 1.6545454545454545, "no_speech_prob": 5.68240193388192e-06}, {"id": 506, "seek": 251476, "start": 2532.48, "end": 2535.92, "text": " minus a couple of standard deviations.", "tokens": [3175, 257, 1916, 295, 3832, 31219, 763, 13], "temperature": 0.0, "avg_logprob": -0.16447591233527523, "compression_ratio": 1.6545454545454545, "no_speech_prob": 5.68240193388192e-06}, {"id": 507, "seek": 251476, "start": 2535.92, "end": 2540.76, "text": " You know, like what's the kind of worst case prediction?", "tokens": [509, 458, 11, 411, 437, 311, 264, 733, 295, 5855, 1389, 17630, 30], "temperature": 0.0, "avg_logprob": -0.16447591233527523, "compression_ratio": 1.6545454545454545, "no_speech_prob": 5.68240193388192e-06}, {"id": 508, "seek": 254076, "start": 2540.76, "end": 2549.76, "text": " And so as Chika mentioned, it's like maybe there's a whole group that we're kind of unconfident", "tokens": [400, 370, 382, 761, 5439, 2835, 11, 309, 311, 411, 1310, 456, 311, 257, 1379, 1594, 300, 321, 434, 733, 295, 517, 24697, 1078], "temperature": 0.0, "avg_logprob": -0.2054618623521593, "compression_ratio": 1.4477611940298507, "no_speech_prob": 8.13961923995521e-06}, {"id": 509, "seek": 254076, "start": 2549.76, "end": 2552.96, "text": " about.", "tokens": [466, 13], "temperature": 0.0, "avg_logprob": -0.2054618623521593, "compression_ratio": 1.4477611940298507, "no_speech_prob": 8.13961923995521e-06}, {"id": 510, "seek": 254076, "start": 2552.96, "end": 2557.1200000000003, "text": " So that's confidence based on tree variance.", "tokens": [407, 300, 311, 6687, 2361, 322, 4230, 21977, 13], "temperature": 0.0, "avg_logprob": -0.2054618623521593, "compression_ratio": 1.4477611940298507, "no_speech_prob": 8.13961923995521e-06}, {"id": 511, "seek": 254076, "start": 2557.1200000000003, "end": 2560.32, "text": " Who wants to have a go at answering feature importance?", "tokens": [2102, 2738, 281, 362, 257, 352, 412, 13430, 4111, 7379, 30], "temperature": 0.0, "avg_logprob": -0.2054618623521593, "compression_ratio": 1.4477611940298507, "no_speech_prob": 8.13961923995521e-06}, {"id": 512, "seek": 254076, "start": 2560.32, "end": 2561.32, "text": " What is it?", "tokens": [708, 307, 309, 30], "temperature": 0.0, "avg_logprob": -0.2054618623521593, "compression_ratio": 1.4477611940298507, "no_speech_prob": 8.13961923995521e-06}, {"id": 513, "seek": 254076, "start": 2561.32, "end": 2562.32, "text": " Why is it interesting?", "tokens": [1545, 307, 309, 1880, 30], "temperature": 0.0, "avg_logprob": -0.2054618623521593, "compression_ratio": 1.4477611940298507, "no_speech_prob": 8.13961923995521e-06}, {"id": 514, "seek": 254076, "start": 2562.32, "end": 2563.32, "text": " How do we calculate it?", "tokens": [1012, 360, 321, 8873, 309, 30], "temperature": 0.0, "avg_logprob": -0.2054618623521593, "compression_ratio": 1.4477611940298507, "no_speech_prob": 8.13961923995521e-06}, {"id": 515, "seek": 254076, "start": 2563.32, "end": 2564.32, "text": " Or any subset thereof?", "tokens": [1610, 604, 25993, 456, 2670, 30], "temperature": 0.0, "avg_logprob": -0.2054618623521593, "compression_ratio": 1.4477611940298507, "no_speech_prob": 8.13961923995521e-06}, {"id": 516, "seek": 254076, "start": 2564.32, "end": 2565.32, "text": " Dina?", "tokens": [413, 1426, 30], "temperature": 0.0, "avg_logprob": -0.2054618623521593, "compression_ratio": 1.4477611940298507, "no_speech_prob": 8.13961923995521e-06}, {"id": 517, "seek": 256532, "start": 2565.32, "end": 2575.0, "text": " I think it's like, it's basically to find out which features are important for your", "tokens": [286, 519, 309, 311, 411, 11, 309, 311, 1936, 281, 915, 484, 597, 4122, 366, 1021, 337, 428], "temperature": 0.0, "avg_logprob": -0.19350756396044483, "compression_ratio": 1.8369098712446352, "no_speech_prob": 1.0952270713460166e-05}, {"id": 518, "seek": 256532, "start": 2575.0, "end": 2576.0, "text": " model.", "tokens": [2316, 13], "temperature": 0.0, "avg_logprob": -0.19350756396044483, "compression_ratio": 1.8369098712446352, "no_speech_prob": 1.0952270713460166e-05}, {"id": 519, "seek": 256532, "start": 2576.0, "end": 2581.7200000000003, "text": " So you take each feature and you like randomly sample all the values in the feature and you", "tokens": [407, 291, 747, 1184, 4111, 293, 291, 411, 16979, 6889, 439, 264, 4190, 294, 264, 4111, 293, 291], "temperature": 0.0, "avg_logprob": -0.19350756396044483, "compression_ratio": 1.8369098712446352, "no_speech_prob": 1.0952270713460166e-05}, {"id": 520, "seek": 256532, "start": 2581.7200000000003, "end": 2583.44, "text": " see how the predictions are.", "tokens": [536, 577, 264, 21264, 366, 13], "temperature": 0.0, "avg_logprob": -0.19350756396044483, "compression_ratio": 1.8369098712446352, "no_speech_prob": 1.0952270713460166e-05}, {"id": 521, "seek": 256532, "start": 2583.44, "end": 2587.2000000000003, "text": " If it's very different, it means that that feature was actually important.", "tokens": [759, 309, 311, 588, 819, 11, 309, 1355, 300, 300, 4111, 390, 767, 1021, 13], "temperature": 0.0, "avg_logprob": -0.19350756396044483, "compression_ratio": 1.8369098712446352, "no_speech_prob": 1.0952270713460166e-05}, {"id": 522, "seek": 256532, "start": 2587.2000000000003, "end": 2591.44, "text": " Else, if it's fine to take any random values for that feature, it means that maybe probably", "tokens": [45472, 11, 498, 309, 311, 2489, 281, 747, 604, 4974, 4190, 337, 300, 4111, 11, 309, 1355, 300, 1310, 1391], "temperature": 0.0, "avg_logprob": -0.19350756396044483, "compression_ratio": 1.8369098712446352, "no_speech_prob": 1.0952270713460166e-05}, {"id": 523, "seek": 256532, "start": 2591.44, "end": 2592.44, "text": " it's not very important.", "tokens": [309, 311, 406, 588, 1021, 13], "temperature": 0.0, "avg_logprob": -0.19350756396044483, "compression_ratio": 1.8369098712446352, "no_speech_prob": 1.0952270713460166e-05}, {"id": 524, "seek": 256532, "start": 2592.44, "end": 2593.44, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.19350756396044483, "compression_ratio": 1.8369098712446352, "no_speech_prob": 1.0952270713460166e-05}, {"id": 525, "seek": 256532, "start": 2593.44, "end": 2594.44, "text": " That was terrific.", "tokens": [663, 390, 20899, 13], "temperature": 0.0, "avg_logprob": -0.19350756396044483, "compression_ratio": 1.8369098712446352, "no_speech_prob": 1.0952270713460166e-05}, {"id": 526, "seek": 259444, "start": 2594.44, "end": 2599.7200000000003, "text": " That was all exactly right.", "tokens": [663, 390, 439, 2293, 558, 13], "temperature": 0.0, "avg_logprob": -0.1935437774658203, "compression_ratio": 1.6623376623376624, "no_speech_prob": 8.013460501388181e-06}, {"id": 527, "seek": 259444, "start": 2599.7200000000003, "end": 2603.08, "text": " There was some details that maybe were skimmed over a little bit.", "tokens": [821, 390, 512, 4365, 300, 1310, 645, 1110, 332, 1912, 670, 257, 707, 857, 13], "temperature": 0.0, "avg_logprob": -0.1935437774658203, "compression_ratio": 1.6623376623376624, "no_speech_prob": 8.013460501388181e-06}, {"id": 528, "seek": 259444, "start": 2603.08, "end": 2607.98, "text": " I wonder if anybody else wants to jump into like a more detailed description of how it's", "tokens": [286, 2441, 498, 4472, 1646, 2738, 281, 3012, 666, 411, 257, 544, 9942, 3855, 295, 577, 309, 311], "temperature": 0.0, "avg_logprob": -0.1935437774658203, "compression_ratio": 1.6623376623376624, "no_speech_prob": 8.013460501388181e-06}, {"id": 529, "seek": 259444, "start": 2607.98, "end": 2613.32, "text": " calculated because I know this morning some people were not quite sure.", "tokens": [15598, 570, 286, 458, 341, 2446, 512, 561, 645, 406, 1596, 988, 13], "temperature": 0.0, "avg_logprob": -0.1935437774658203, "compression_ratio": 1.6623376623376624, "no_speech_prob": 8.013460501388181e-06}, {"id": 530, "seek": 259444, "start": 2613.32, "end": 2617.52, "text": " Is there anybody who's like not quite sure maybe who wants to like have a go or want", "tokens": [1119, 456, 4472, 567, 311, 411, 406, 1596, 988, 1310, 567, 2738, 281, 411, 362, 257, 352, 420, 528], "temperature": 0.0, "avg_logprob": -0.1935437774658203, "compression_ratio": 1.6623376623376624, "no_speech_prob": 8.013460501388181e-06}, {"id": 531, "seek": 259444, "start": 2617.52, "end": 2618.52, "text": " to just put it next to you there?", "tokens": [281, 445, 829, 309, 958, 281, 291, 456, 30], "temperature": 0.0, "avg_logprob": -0.1935437774658203, "compression_ratio": 1.6623376623376624, "no_speech_prob": 8.013460501388181e-06}, {"id": 532, "seek": 259444, "start": 2618.52, "end": 2619.52, "text": " Let's see.", "tokens": [961, 311, 536, 13], "temperature": 0.0, "avg_logprob": -0.1935437774658203, "compression_ratio": 1.6623376623376624, "no_speech_prob": 8.013460501388181e-06}, {"id": 533, "seek": 261952, "start": 2619.52, "end": 2624.6, "text": " How exactly do we calculate feature importance for a particular feature?", "tokens": [1012, 2293, 360, 321, 8873, 4111, 7379, 337, 257, 1729, 4111, 30], "temperature": 0.0, "avg_logprob": -0.3067839724346272, "compression_ratio": 1.7283464566929134, "no_speech_prob": 8.013453225430567e-06}, {"id": 534, "seek": 261952, "start": 2624.6, "end": 2629.44, "text": " I think after you're done building the random forest model, you take each column and randomly", "tokens": [286, 519, 934, 291, 434, 1096, 2390, 264, 4974, 6719, 2316, 11, 291, 747, 1184, 7738, 293, 16979], "temperature": 0.0, "avg_logprob": -0.3067839724346272, "compression_ratio": 1.7283464566929134, "no_speech_prob": 8.013453225430567e-06}, {"id": 535, "seek": 261952, "start": 2629.44, "end": 2634.16, "text": " shuffle it and generate a prediction and check the validation score.", "tokens": [39426, 309, 293, 8460, 257, 17630, 293, 1520, 264, 24071, 6175, 13], "temperature": 0.0, "avg_logprob": -0.3067839724346272, "compression_ratio": 1.7283464566929134, "no_speech_prob": 8.013453225430567e-06}, {"id": 536, "seek": 261952, "start": 2634.16, "end": 2639.56, "text": " If it gets pretty bad for after shuffling one of the columns, that means that column", "tokens": [759, 309, 2170, 1238, 1578, 337, 934, 402, 1245, 1688, 472, 295, 264, 13766, 11, 300, 1355, 300, 7738], "temperature": 0.0, "avg_logprob": -0.3067839724346272, "compression_ratio": 1.7283464566929134, "no_speech_prob": 8.013453225430567e-06}, {"id": 537, "seek": 261952, "start": 2639.56, "end": 2640.56, "text": " was important.", "tokens": [390, 1021, 13], "temperature": 0.0, "avg_logprob": -0.3067839724346272, "compression_ratio": 1.7283464566929134, "no_speech_prob": 8.013453225430567e-06}, {"id": 538, "seek": 261952, "start": 2640.56, "end": 2643.96, "text": " So that has higher importance.", "tokens": [407, 300, 575, 2946, 7379, 13], "temperature": 0.0, "avg_logprob": -0.3067839724346272, "compression_ratio": 1.7283464566929134, "no_speech_prob": 8.013453225430567e-06}, {"id": 539, "seek": 261952, "start": 2643.96, "end": 2647.44, "text": " I'm not exactly sure how we quantify the feature importance.", "tokens": [286, 478, 406, 2293, 988, 577, 321, 40421, 264, 4111, 7379, 13], "temperature": 0.0, "avg_logprob": -0.3067839724346272, "compression_ratio": 1.7283464566929134, "no_speech_prob": 8.013453225430567e-06}, {"id": 540, "seek": 261952, "start": 2647.44, "end": 2648.44, "text": " Okay great.", "tokens": [1033, 869, 13], "temperature": 0.0, "avg_logprob": -0.3067839724346272, "compression_ratio": 1.7283464566929134, "no_speech_prob": 8.013453225430567e-06}, {"id": 541, "seek": 264844, "start": 2648.44, "end": 2655.0, "text": " Dina, do you know how we quantify the feature importance?", "tokens": [413, 1426, 11, 360, 291, 458, 577, 321, 40421, 264, 4111, 7379, 30], "temperature": 0.0, "avg_logprob": -0.2603391149769659, "compression_ratio": 1.5265486725663717, "no_speech_prob": 6.643406322837109e-06}, {"id": 542, "seek": 264844, "start": 2655.0, "end": 2656.0, "text": " That was a great description.", "tokens": [663, 390, 257, 869, 3855, 13], "temperature": 0.0, "avg_logprob": -0.2603391149769659, "compression_ratio": 1.5265486725663717, "no_speech_prob": 6.643406322837109e-06}, {"id": 543, "seek": 264844, "start": 2656.0, "end": 2660.44, "text": " I think we take the difference in the first place.", "tokens": [286, 519, 321, 747, 264, 2649, 294, 264, 700, 1081, 13], "temperature": 0.0, "avg_logprob": -0.2603391149769659, "compression_ratio": 1.5265486725663717, "no_speech_prob": 6.643406322837109e-06}, {"id": 544, "seek": 264844, "start": 2660.44, "end": 2662.0, "text": " Or score of some sort.", "tokens": [1610, 6175, 295, 512, 1333, 13], "temperature": 0.0, "avg_logprob": -0.2603391149769659, "compression_ratio": 1.5265486725663717, "no_speech_prob": 6.643406322837109e-06}, {"id": 545, "seek": 264844, "start": 2662.0, "end": 2663.0, "text": " Exactly.", "tokens": [7587, 13], "temperature": 0.0, "avg_logprob": -0.2603391149769659, "compression_ratio": 1.5265486725663717, "no_speech_prob": 6.643406322837109e-06}, {"id": 546, "seek": 264844, "start": 2663.0, "end": 2666.6, "text": " Yeah, so let's say we've got our dependent variable which is price, right, and there's", "tokens": [865, 11, 370, 718, 311, 584, 321, 600, 658, 527, 12334, 7006, 597, 307, 3218, 11, 558, 11, 293, 456, 311], "temperature": 0.0, "avg_logprob": -0.2603391149769659, "compression_ratio": 1.5265486725663717, "no_speech_prob": 6.643406322837109e-06}, {"id": 547, "seek": 264844, "start": 2666.6, "end": 2673.16, "text": " a bunch of independent variables including year made, right, and so we basically we use", "tokens": [257, 3840, 295, 6695, 9102, 3009, 1064, 1027, 11, 558, 11, 293, 370, 321, 1936, 321, 764], "temperature": 0.0, "avg_logprob": -0.2603391149769659, "compression_ratio": 1.5265486725663717, "no_speech_prob": 6.643406322837109e-06}, {"id": 548, "seek": 267316, "start": 2673.16, "end": 2684.3999999999996, "text": " the whole lot to build a random forest, right, and then that gives us our predictions, right,", "tokens": [264, 1379, 688, 281, 1322, 257, 4974, 6719, 11, 558, 11, 293, 550, 300, 2709, 505, 527, 21264, 11, 558, 11], "temperature": 0.0, "avg_logprob": -0.15102919260660808, "compression_ratio": 1.521978021978022, "no_speech_prob": 5.86280020797858e-06}, {"id": 549, "seek": 267316, "start": 2684.3999999999996, "end": 2695.0, "text": " and so then we can compare that to get R squared, RMSE, whatever you're interested in from the", "tokens": [293, 370, 550, 321, 393, 6794, 300, 281, 483, 497, 8889, 11, 23790, 5879, 11, 2035, 291, 434, 3102, 294, 490, 264], "temperature": 0.0, "avg_logprob": -0.15102919260660808, "compression_ratio": 1.521978021978022, "no_speech_prob": 5.86280020797858e-06}, {"id": 550, "seek": 267316, "start": 2695.0, "end": 2696.0, "text": " model.", "tokens": [2316, 13], "temperature": 0.0, "avg_logprob": -0.15102919260660808, "compression_ratio": 1.521978021978022, "no_speech_prob": 5.86280020797858e-06}, {"id": 551, "seek": 267316, "start": 2696.0, "end": 2701.92, "text": " Now the key thing here is I don't want to have to retrain my whole random forest.", "tokens": [823, 264, 2141, 551, 510, 307, 286, 500, 380, 528, 281, 362, 281, 1533, 7146, 452, 1379, 4974, 6719, 13], "temperature": 0.0, "avg_logprob": -0.15102919260660808, "compression_ratio": 1.521978021978022, "no_speech_prob": 5.86280020797858e-06}, {"id": 552, "seek": 270192, "start": 2701.92, "end": 2706.6800000000003, "text": " That's kind of slow and boring, right, so using the existing random forest, how can", "tokens": [663, 311, 733, 295, 2964, 293, 9989, 11, 558, 11, 370, 1228, 264, 6741, 4974, 6719, 11, 577, 393], "temperature": 0.0, "avg_logprob": -0.12707319752923374, "compression_ratio": 1.7509578544061302, "no_speech_prob": 8.93966353032738e-06}, {"id": 553, "seek": 270192, "start": 2706.6800000000003, "end": 2713.48, "text": " I figure out how important year made was, right, and so the suggestion was let's randomly", "tokens": [286, 2573, 484, 577, 1021, 1064, 1027, 390, 11, 558, 11, 293, 370, 264, 16541, 390, 718, 311, 16979], "temperature": 0.0, "avg_logprob": -0.12707319752923374, "compression_ratio": 1.7509578544061302, "no_speech_prob": 8.93966353032738e-06}, {"id": 554, "seek": 270192, "start": 2713.48, "end": 2716.2400000000002, "text": " shuffle the whole column, right.", "tokens": [39426, 264, 1379, 7738, 11, 558, 13], "temperature": 0.0, "avg_logprob": -0.12707319752923374, "compression_ratio": 1.7509578544061302, "no_speech_prob": 8.93966353032738e-06}, {"id": 555, "seek": 270192, "start": 2716.2400000000002, "end": 2718.08, "text": " So now that column is totally useless.", "tokens": [407, 586, 300, 7738, 307, 3879, 14115, 13], "temperature": 0.0, "avg_logprob": -0.12707319752923374, "compression_ratio": 1.7509578544061302, "no_speech_prob": 8.93966353032738e-06}, {"id": 556, "seek": 270192, "start": 2718.08, "end": 2722.64, "text": " It's got the same mean, same distribution, everything about it is the same, but there's", "tokens": [467, 311, 658, 264, 912, 914, 11, 912, 7316, 11, 1203, 466, 309, 307, 264, 912, 11, 457, 456, 311], "temperature": 0.0, "avg_logprob": -0.12707319752923374, "compression_ratio": 1.7509578544061302, "no_speech_prob": 8.93966353032738e-06}, {"id": 557, "seek": 270192, "start": 2722.64, "end": 2727.96, "text": " no connection at all between particular people, actual year made, and what's now in that column.", "tokens": [572, 4984, 412, 439, 1296, 1729, 561, 11, 3539, 1064, 1027, 11, 293, 437, 311, 586, 294, 300, 7738, 13], "temperature": 0.0, "avg_logprob": -0.12707319752923374, "compression_ratio": 1.7509578544061302, "no_speech_prob": 8.93966353032738e-06}, {"id": 558, "seek": 270192, "start": 2727.96, "end": 2730.54, "text": " I've randomly shuffled it.", "tokens": [286, 600, 16979, 402, 33974, 309, 13], "temperature": 0.0, "avg_logprob": -0.12707319752923374, "compression_ratio": 1.7509578544061302, "no_speech_prob": 8.93966353032738e-06}, {"id": 559, "seek": 273054, "start": 2730.54, "end": 2741.0, "text": " And so now I put that new version through the same random forest, so there's no retraining", "tokens": [400, 370, 586, 286, 829, 300, 777, 3037, 807, 264, 912, 4974, 6719, 11, 370, 456, 311, 572, 49356, 1760], "temperature": 0.0, "avg_logprob": -0.17951700687408448, "compression_ratio": 1.5654761904761905, "no_speech_prob": 7.527965408371529e-06}, {"id": 560, "seek": 273054, "start": 2741.0, "end": 2751.68, "text": " done, to get some new y hat, I call it y hat, ym, right, and then I can compare that to", "tokens": [1096, 11, 281, 483, 512, 777, 288, 2385, 11, 286, 818, 309, 288, 2385, 11, 288, 76, 11, 558, 11, 293, 550, 286, 393, 6794, 300, 281], "temperature": 0.0, "avg_logprob": -0.17951700687408448, "compression_ratio": 1.5654761904761905, "no_speech_prob": 7.527965408371529e-06}, {"id": 561, "seek": 273054, "start": 2751.68, "end": 2758.84, "text": " my actuals to get like an RMSE, ym, right, and so now I can start to create a little", "tokens": [452, 3539, 82, 281, 483, 411, 364, 23790, 5879, 11, 288, 76, 11, 558, 11, 293, 370, 586, 286, 393, 722, 281, 1884, 257, 707], "temperature": 0.0, "avg_logprob": -0.17951700687408448, "compression_ratio": 1.5654761904761905, "no_speech_prob": 7.527965408371529e-06}, {"id": 562, "seek": 275884, "start": 2758.84, "end": 2764.6400000000003, "text": " table.", "tokens": [3199, 13], "temperature": 0.0, "avg_logprob": -0.1516300356665323, "compression_ratio": 1.6428571428571428, "no_speech_prob": 2.212549043179024e-05}, {"id": 563, "seek": 275884, "start": 2764.6400000000003, "end": 2771.6400000000003, "text": " So now I can create a little table where I've basically got like the original here, RMSE,", "tokens": [407, 586, 286, 393, 1884, 257, 707, 3199, 689, 286, 600, 1936, 658, 411, 264, 3380, 510, 11, 23790, 5879, 11], "temperature": 0.0, "avg_logprob": -0.1516300356665323, "compression_ratio": 1.6428571428571428, "no_speech_prob": 2.212549043179024e-05}, {"id": 564, "seek": 275884, "start": 2771.6400000000003, "end": 2774.84, "text": " and then I've got with year made scrambled.", "tokens": [293, 550, 286, 600, 658, 365, 1064, 1027, 49127, 13], "temperature": 0.0, "avg_logprob": -0.1516300356665323, "compression_ratio": 1.6428571428571428, "no_speech_prob": 2.212549043179024e-05}, {"id": 565, "seek": 275884, "start": 2774.84, "end": 2784.28, "text": " So this one had an RMSE of like 3, this one had an RMSE of like 2, enclosure, you know,", "tokens": [407, 341, 472, 632, 364, 23790, 5879, 295, 411, 805, 11, 341, 472, 632, 364, 23790, 5879, 295, 411, 568, 11, 34093, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.1516300356665323, "compression_ratio": 1.6428571428571428, "no_speech_prob": 2.212549043179024e-05}, {"id": 566, "seek": 275884, "start": 2784.28, "end": 2788.36, "text": " scrambling that had an RMSE of like 2.5, right.", "tokens": [5918, 19391, 300, 632, 364, 23790, 5879, 295, 411, 568, 13, 20, 11, 558, 13], "temperature": 0.0, "avg_logprob": -0.1516300356665323, "compression_ratio": 1.6428571428571428, "no_speech_prob": 2.212549043179024e-05}, {"id": 567, "seek": 278836, "start": 2788.36, "end": 2789.88, "text": " And so then I just take these differences.", "tokens": [400, 370, 550, 286, 445, 747, 613, 7300, 13], "temperature": 0.0, "avg_logprob": -0.2217104218222878, "compression_ratio": 1.4179104477611941, "no_speech_prob": 2.2473957869806327e-05}, {"id": 568, "seek": 278836, "start": 2789.88, "end": 2800.04, "text": " So I'd say year made, the importance is 1, 3-2, enclosure is 0.5, 3-2.5, and so forth,", "tokens": [407, 286, 1116, 584, 1064, 1027, 11, 264, 7379, 307, 502, 11, 805, 12, 17, 11, 34093, 307, 1958, 13, 20, 11, 805, 12, 17, 13, 20, 11, 293, 370, 5220, 11], "temperature": 0.0, "avg_logprob": -0.2217104218222878, "compression_ratio": 1.4179104477611941, "no_speech_prob": 2.2473957869806327e-05}, {"id": 569, "seek": 278836, "start": 2800.04, "end": 2801.04, "text": " right.", "tokens": [558, 13], "temperature": 0.0, "avg_logprob": -0.2217104218222878, "compression_ratio": 1.4179104477611941, "no_speech_prob": 2.2473957869806327e-05}, {"id": 570, "seek": 278836, "start": 2801.04, "end": 2806.1200000000003, "text": " So how much worse did my model get after I shuffled that variable?", "tokens": [407, 577, 709, 5324, 630, 452, 2316, 483, 934, 286, 402, 33974, 300, 7006, 30], "temperature": 0.0, "avg_logprob": -0.2217104218222878, "compression_ratio": 1.4179104477611941, "no_speech_prob": 2.2473957869806327e-05}, {"id": 571, "seek": 278836, "start": 2806.1200000000003, "end": 2809.28, "text": " Does anybody have any questions about that?", "tokens": [4402, 4472, 362, 604, 1651, 466, 300, 30], "temperature": 0.0, "avg_logprob": -0.2217104218222878, "compression_ratio": 1.4179104477611941, "no_speech_prob": 2.2473957869806327e-05}, {"id": 572, "seek": 278836, "start": 2809.28, "end": 2815.28, "text": " Can you pass that to Danielle please?", "tokens": [1664, 291, 1320, 300, 281, 21182, 1767, 30], "temperature": 0.0, "avg_logprob": -0.2217104218222878, "compression_ratio": 1.4179104477611941, "no_speech_prob": 2.2473957869806327e-05}, {"id": 573, "seek": 281528, "start": 2815.28, "end": 2821.8, "text": " I assume you just chose those numbers randomly, but my question I guess is, does it, do all", "tokens": [286, 6552, 291, 445, 5111, 729, 3547, 16979, 11, 457, 452, 1168, 286, 2041, 307, 11, 775, 309, 11, 360, 439], "temperature": 0.0, "avg_logprob": -0.32858390451591707, "compression_ratio": 1.5120967741935485, "no_speech_prob": 2.3552353013656102e-05}, {"id": 574, "seek": 281528, "start": 2821.8, "end": 2824.6800000000003, "text": " of them theoretically have a perfect model to start out with?", "tokens": [295, 552, 29400, 362, 257, 2176, 2316, 281, 722, 484, 365, 30], "temperature": 0.0, "avg_logprob": -0.32858390451591707, "compression_ratio": 1.5120967741935485, "no_speech_prob": 2.3552353013656102e-05}, {"id": 575, "seek": 281528, "start": 2824.6800000000003, "end": 2828.8, "text": " Like are they, will all the importances sum to 1, or is that not?", "tokens": [1743, 366, 436, 11, 486, 439, 264, 974, 2676, 2408, 281, 502, 11, 420, 307, 300, 406, 30], "temperature": 0.0, "avg_logprob": -0.32858390451591707, "compression_ratio": 1.5120967741935485, "no_speech_prob": 2.3552353013656102e-05}, {"id": 576, "seek": 281528, "start": 2828.8, "end": 2829.8, "text": " No.", "tokens": [883, 13], "temperature": 0.0, "avg_logprob": -0.32858390451591707, "compression_ratio": 1.5120967741935485, "no_speech_prob": 2.3552353013656102e-05}, {"id": 577, "seek": 281528, "start": 2829.8, "end": 2830.8, "text": " They're just...", "tokens": [814, 434, 445, 485], "temperature": 0.0, "avg_logprob": -0.32858390451591707, "compression_ratio": 1.5120967741935485, "no_speech_prob": 2.3552353013656102e-05}, {"id": 578, "seek": 281528, "start": 2830.8, "end": 2835.48, "text": " Honestly, I've never actually looked at what the units are, so I'm actually not quite sure.", "tokens": [12348, 11, 286, 600, 1128, 767, 2956, 412, 437, 264, 6815, 366, 11, 370, 286, 478, 767, 406, 1596, 988, 13], "temperature": 0.0, "avg_logprob": -0.32858390451591707, "compression_ratio": 1.5120967741935485, "no_speech_prob": 2.3552353013656102e-05}, {"id": 579, "seek": 281528, "start": 2835.48, "end": 2836.48, "text": " Sorry.", "tokens": [4919, 13], "temperature": 0.0, "avg_logprob": -0.32858390451591707, "compression_ratio": 1.5120967741935485, "no_speech_prob": 2.3552353013656102e-05}, {"id": 580, "seek": 281528, "start": 2836.48, "end": 2837.84, "text": " We can check it out during the week.", "tokens": [492, 393, 1520, 309, 484, 1830, 264, 1243, 13], "temperature": 0.0, "avg_logprob": -0.32858390451591707, "compression_ratio": 1.5120967741935485, "no_speech_prob": 2.3552353013656102e-05}, {"id": 581, "seek": 283784, "start": 2837.84, "end": 2845.86, "text": " If somebody's interested, have a look at this SKLearn code and see exactly what those units", "tokens": [759, 2618, 311, 3102, 11, 362, 257, 574, 412, 341, 21483, 11020, 1083, 3089, 293, 536, 2293, 437, 729, 6815], "temperature": 0.0, "avg_logprob": -0.18869893094326587, "compression_ratio": 1.5720338983050848, "no_speech_prob": 7.2963343882292975e-06}, {"id": 582, "seek": 283784, "start": 2845.86, "end": 2849.1600000000003, "text": " of measure are, because I've never bothered to check.", "tokens": [295, 3481, 366, 11, 570, 286, 600, 1128, 22996, 281, 1520, 13], "temperature": 0.0, "avg_logprob": -0.18869893094326587, "compression_ratio": 1.5720338983050848, "no_speech_prob": 7.2963343882292975e-06}, {"id": 583, "seek": 283784, "start": 2849.1600000000003, "end": 2855.1600000000003, "text": " Although I don't check the units of measure specifically, what I do check is the relative", "tokens": [5780, 286, 500, 380, 1520, 264, 6815, 295, 3481, 4682, 11, 437, 286, 360, 1520, 307, 264, 4972], "temperature": 0.0, "avg_logprob": -0.18869893094326587, "compression_ratio": 1.5720338983050848, "no_speech_prob": 7.2963343882292975e-06}, {"id": 584, "seek": 283784, "start": 2855.1600000000003, "end": 2856.52, "text": " importance.", "tokens": [7379, 13], "temperature": 0.0, "avg_logprob": -0.18869893094326587, "compression_ratio": 1.5720338983050848, "no_speech_prob": 7.2963343882292975e-06}, {"id": 585, "seek": 283784, "start": 2856.52, "end": 2858.04, "text": " And so like here's an example.", "tokens": [400, 370, 411, 510, 311, 364, 1365, 13], "temperature": 0.0, "avg_logprob": -0.18869893094326587, "compression_ratio": 1.5720338983050848, "no_speech_prob": 7.2963343882292975e-06}, {"id": 586, "seek": 283784, "start": 2858.04, "end": 2865.8, "text": " So rather than just saying like what are the top 10, yesterday one of the practicum students", "tokens": [407, 2831, 813, 445, 1566, 411, 437, 366, 264, 1192, 1266, 11, 5186, 472, 295, 264, 1927, 299, 449, 1731], "temperature": 0.0, "avg_logprob": -0.18869893094326587, "compression_ratio": 1.5720338983050848, "no_speech_prob": 7.2963343882292975e-06}, {"id": 587, "seek": 286580, "start": 2865.8, "end": 2871.44, "text": " asked me about a feature importance where they said like I think these 3 are important.", "tokens": [2351, 385, 466, 257, 4111, 7379, 689, 436, 848, 411, 286, 519, 613, 805, 366, 1021, 13], "temperature": 0.0, "avg_logprob": -0.15179352287773615, "compression_ratio": 1.8571428571428572, "no_speech_prob": 3.966975327784894e-06}, {"id": 588, "seek": 286580, "start": 2871.44, "end": 2877.6600000000003, "text": " And I pointed out that the top one was 1000 times more important than the second one.", "tokens": [400, 286, 10932, 484, 300, 264, 1192, 472, 390, 9714, 1413, 544, 1021, 813, 264, 1150, 472, 13], "temperature": 0.0, "avg_logprob": -0.15179352287773615, "compression_ratio": 1.8571428571428572, "no_speech_prob": 3.966975327784894e-06}, {"id": 589, "seek": 286580, "start": 2877.6600000000003, "end": 2879.92, "text": " So like look at the relative numbers here.", "tokens": [407, 411, 574, 412, 264, 4972, 3547, 510, 13], "temperature": 0.0, "avg_logprob": -0.15179352287773615, "compression_ratio": 1.8571428571428572, "no_speech_prob": 3.966975327784894e-06}, {"id": 590, "seek": 286580, "start": 2879.92, "end": 2884.0800000000004, "text": " And so in that case it's like no, don't look at the top 3, look at the one that's 1000", "tokens": [400, 370, 294, 300, 1389, 309, 311, 411, 572, 11, 500, 380, 574, 412, 264, 1192, 805, 11, 574, 412, 264, 472, 300, 311, 9714], "temperature": 0.0, "avg_logprob": -0.15179352287773615, "compression_ratio": 1.8571428571428572, "no_speech_prob": 3.966975327784894e-06}, {"id": 591, "seek": 286580, "start": 2884.0800000000004, "end": 2887.2400000000002, "text": " times more important and ignore all the rest.", "tokens": [1413, 544, 1021, 293, 11200, 439, 264, 1472, 13], "temperature": 0.0, "avg_logprob": -0.15179352287773615, "compression_ratio": 1.8571428571428572, "no_speech_prob": 3.966975327784894e-06}, {"id": 592, "seek": 286580, "start": 2887.2400000000002, "end": 2892.84, "text": " And so this is where sometimes the kind of, your natural tendency to want to be like precise", "tokens": [400, 370, 341, 307, 689, 2171, 264, 733, 295, 11, 428, 3303, 18187, 281, 528, 281, 312, 411, 13600], "temperature": 0.0, "avg_logprob": -0.15179352287773615, "compression_ratio": 1.8571428571428572, "no_speech_prob": 3.966975327784894e-06}, {"id": 593, "seek": 289284, "start": 2892.84, "end": 2896.2400000000002, "text": " and careful, you need to override that and be very practical.", "tokens": [293, 5026, 11, 291, 643, 281, 42321, 300, 293, 312, 588, 8496, 13], "temperature": 0.0, "avg_logprob": -0.18645556767781576, "compression_ratio": 1.8439716312056738, "no_speech_prob": 5.17389526066836e-06}, {"id": 594, "seek": 289284, "start": 2896.2400000000002, "end": 2900.2000000000003, "text": " It's like okay this thing's 1000 times more important, don't spend any time on anything", "tokens": [467, 311, 411, 1392, 341, 551, 311, 9714, 1413, 544, 1021, 11, 500, 380, 3496, 604, 565, 322, 1340], "temperature": 0.0, "avg_logprob": -0.18645556767781576, "compression_ratio": 1.8439716312056738, "no_speech_prob": 5.17389526066836e-06}, {"id": 595, "seek": 289284, "start": 2900.2000000000003, "end": 2901.2000000000003, "text": " else.", "tokens": [1646, 13], "temperature": 0.0, "avg_logprob": -0.18645556767781576, "compression_ratio": 1.8439716312056738, "no_speech_prob": 5.17389526066836e-06}, {"id": 596, "seek": 289284, "start": 2901.2000000000003, "end": 2905.52, "text": " So then you can go and talk to the manager of your project and say like okay this thing's", "tokens": [407, 550, 291, 393, 352, 293, 751, 281, 264, 6598, 295, 428, 1716, 293, 584, 411, 1392, 341, 551, 311], "temperature": 0.0, "avg_logprob": -0.18645556767781576, "compression_ratio": 1.8439716312056738, "no_speech_prob": 5.17389526066836e-06}, {"id": 597, "seek": 289284, "start": 2905.52, "end": 2911.6000000000004, "text": " 1000 times more important, and then they might say oh, that was a mistake, it shouldn't have", "tokens": [9714, 1413, 544, 1021, 11, 293, 550, 436, 1062, 584, 1954, 11, 300, 390, 257, 6146, 11, 309, 4659, 380, 362], "temperature": 0.0, "avg_logprob": -0.18645556767781576, "compression_ratio": 1.8439716312056738, "no_speech_prob": 5.17389526066836e-06}, {"id": 598, "seek": 289284, "start": 2911.6000000000004, "end": 2915.08, "text": " been in there, we don't actually have that information at the decision time.", "tokens": [668, 294, 456, 11, 321, 500, 380, 767, 362, 300, 1589, 412, 264, 3537, 565, 13], "temperature": 0.0, "avg_logprob": -0.18645556767781576, "compression_ratio": 1.8439716312056738, "no_speech_prob": 5.17389526066836e-06}, {"id": 599, "seek": 289284, "start": 2915.08, "end": 2921.2400000000002, "text": " Or for whatever reason we can't actually use that variable, so then you could remove it", "tokens": [1610, 337, 2035, 1778, 321, 393, 380, 767, 764, 300, 7006, 11, 370, 550, 291, 727, 4159, 309], "temperature": 0.0, "avg_logprob": -0.18645556767781576, "compression_ratio": 1.8439716312056738, "no_speech_prob": 5.17389526066836e-06}, {"id": 600, "seek": 289284, "start": 2921.2400000000002, "end": 2922.6800000000003, "text": " and have a look.", "tokens": [293, 362, 257, 574, 13], "temperature": 0.0, "avg_logprob": -0.18645556767781576, "compression_ratio": 1.8439716312056738, "no_speech_prob": 5.17389526066836e-06}, {"id": 601, "seek": 292268, "start": 2922.68, "end": 2927.7999999999997, "text": " Or they might say gosh, I had no idea that that was by far more important than everything", "tokens": [1610, 436, 1062, 584, 6502, 11, 286, 632, 572, 1558, 300, 300, 390, 538, 1400, 544, 1021, 813, 1203], "temperature": 0.0, "avg_logprob": -0.14485093821649966, "compression_ratio": 1.6528925619834711, "no_speech_prob": 6.540412869071588e-06}, {"id": 602, "seek": 292268, "start": 2927.7999999999997, "end": 2935.3199999999997, "text": " else put together, so let's forget this random Boris thing and just focus on understanding", "tokens": [1646, 829, 1214, 11, 370, 718, 311, 2870, 341, 4974, 27158, 551, 293, 445, 1879, 322, 3701], "temperature": 0.0, "avg_logprob": -0.14485093821649966, "compression_ratio": 1.6528925619834711, "no_speech_prob": 6.540412869071588e-06}, {"id": 603, "seek": 292268, "start": 2935.3199999999997, "end": 2939.2799999999997, "text": " how we can better collect that one variable and better use that one variable.", "tokens": [577, 321, 393, 1101, 2500, 300, 472, 7006, 293, 1101, 764, 300, 472, 7006, 13], "temperature": 0.0, "avg_logprob": -0.14485093821649966, "compression_ratio": 1.6528925619834711, "no_speech_prob": 6.540412869071588e-06}, {"id": 604, "seek": 292268, "start": 2939.2799999999997, "end": 2943.96, "text": " So that's like something which comes up quite a lot.", "tokens": [407, 300, 311, 411, 746, 597, 1487, 493, 1596, 257, 688, 13], "temperature": 0.0, "avg_logprob": -0.14485093821649966, "compression_ratio": 1.6528925619834711, "no_speech_prob": 6.540412869071588e-06}, {"id": 605, "seek": 292268, "start": 2943.96, "end": 2947.7599999999998, "text": " And actually another place that came up just yesterday, again, another practicum student", "tokens": [400, 767, 1071, 1081, 300, 1361, 493, 445, 5186, 11, 797, 11, 1071, 1927, 299, 449, 3107], "temperature": 0.0, "avg_logprob": -0.14485093821649966, "compression_ratio": 1.6528925619834711, "no_speech_prob": 6.540412869071588e-06}, {"id": 606, "seek": 294776, "start": 2947.76, "end": 2959.6000000000004, "text": " asked me, hey I'm doing this medical diagnostics project and my r squared is.95 for a disease", "tokens": [2351, 385, 11, 4177, 286, 478, 884, 341, 4625, 43215, 1167, 1716, 293, 452, 367, 8889, 307, 2411, 15718, 337, 257, 4752], "temperature": 0.0, "avg_logprob": -0.1348284379935559, "compression_ratio": 1.5192307692307692, "no_speech_prob": 3.785307626458234e-06}, {"id": 607, "seek": 294776, "start": 2959.6000000000004, "end": 2962.78, "text": " which I was told is very hard to diagnose.", "tokens": [597, 286, 390, 1907, 307, 588, 1152, 281, 36238, 13], "temperature": 0.0, "avg_logprob": -0.1348284379935559, "compression_ratio": 1.5192307692307692, "no_speech_prob": 3.785307626458234e-06}, {"id": 608, "seek": 294776, "start": 2962.78, "end": 2967.1200000000003, "text": " You know, is this random forest a genius or is something going wrong?", "tokens": [509, 458, 11, 307, 341, 4974, 6719, 257, 14017, 420, 307, 746, 516, 2085, 30], "temperature": 0.0, "avg_logprob": -0.1348284379935559, "compression_ratio": 1.5192307692307692, "no_speech_prob": 3.785307626458234e-06}, {"id": 609, "seek": 294776, "start": 2967.1200000000003, "end": 2971.2000000000003, "text": " And I said like remember the second thing you do after you build a random forest is", "tokens": [400, 286, 848, 411, 1604, 264, 1150, 551, 291, 360, 934, 291, 1322, 257, 4974, 6719, 307], "temperature": 0.0, "avg_logprob": -0.1348284379935559, "compression_ratio": 1.5192307692307692, "no_speech_prob": 3.785307626458234e-06}, {"id": 610, "seek": 294776, "start": 2971.2000000000003, "end": 2973.0, "text": " to do feature importance.", "tokens": [281, 360, 4111, 7379, 13], "temperature": 0.0, "avg_logprob": -0.1348284379935559, "compression_ratio": 1.5192307692307692, "no_speech_prob": 3.785307626458234e-06}, {"id": 611, "seek": 297300, "start": 2973.0, "end": 2978.76, "text": " So do feature importance and what you'll probably find is that the top column is something that", "tokens": [407, 360, 4111, 7379, 293, 437, 291, 603, 1391, 915, 307, 300, 264, 1192, 7738, 307, 746, 300], "temperature": 0.0, "avg_logprob": -0.17454924673404335, "compression_ratio": 1.6733870967741935, "no_speech_prob": 6.854254479549127e-06}, {"id": 612, "seek": 297300, "start": 2978.76, "end": 2980.4, "text": " shouldn't be there.", "tokens": [4659, 380, 312, 456, 13], "temperature": 0.0, "avg_logprob": -0.17454924673404335, "compression_ratio": 1.6733870967741935, "no_speech_prob": 6.854254479549127e-06}, {"id": 613, "seek": 297300, "start": 2980.4, "end": 2981.4, "text": " And so that's what happened.", "tokens": [400, 370, 300, 311, 437, 2011, 13], "temperature": 0.0, "avg_logprob": -0.17454924673404335, "compression_ratio": 1.6733870967741935, "no_speech_prob": 6.854254479549127e-06}, {"id": 614, "seek": 297300, "start": 2981.4, "end": 2984.84, "text": " He came back to me half an hour later, he said yeah I did the feature importance, you", "tokens": [634, 1361, 646, 281, 385, 1922, 364, 1773, 1780, 11, 415, 848, 1338, 286, 630, 264, 4111, 7379, 11, 291], "temperature": 0.0, "avg_logprob": -0.17454924673404335, "compression_ratio": 1.6733870967741935, "no_speech_prob": 6.854254479549127e-06}, {"id": 615, "seek": 297300, "start": 2984.84, "end": 2990.48, "text": " were right, the top column was basically something that was another encoding of the dependent", "tokens": [645, 558, 11, 264, 1192, 7738, 390, 1936, 746, 300, 390, 1071, 43430, 295, 264, 12334], "temperature": 0.0, "avg_logprob": -0.17454924673404335, "compression_ratio": 1.6733870967741935, "no_speech_prob": 6.854254479549127e-06}, {"id": 616, "seek": 297300, "start": 2990.48, "end": 2995.44, "text": " variable, I've removed it, and now my r squared is negative 0.1.", "tokens": [7006, 11, 286, 600, 7261, 309, 11, 293, 586, 452, 367, 8889, 307, 3671, 1958, 13, 16, 13], "temperature": 0.0, "avg_logprob": -0.17454924673404335, "compression_ratio": 1.6733870967741935, "no_speech_prob": 6.854254479549127e-06}, {"id": 617, "seek": 297300, "start": 2995.44, "end": 3000.96, "text": " So that's an improvement.", "tokens": [407, 300, 311, 364, 10444, 13], "temperature": 0.0, "avg_logprob": -0.17454924673404335, "compression_ratio": 1.6733870967741935, "no_speech_prob": 6.854254479549127e-06}, {"id": 618, "seek": 300096, "start": 3000.96, "end": 3010.2, "text": " Okay, the other thing I like to look at is this chart, is to basically say where do things", "tokens": [1033, 11, 264, 661, 551, 286, 411, 281, 574, 412, 307, 341, 6927, 11, 307, 281, 1936, 584, 689, 360, 721], "temperature": 0.0, "avg_logprob": -0.16380477704499896, "compression_ratio": 1.5918367346938775, "no_speech_prob": 1.0129938345926348e-05}, {"id": 619, "seek": 300096, "start": 3010.2, "end": 3015.56, "text": " kind of flatten off in terms of which ones should I be really focusing on.", "tokens": [733, 295, 24183, 766, 294, 2115, 295, 597, 2306, 820, 286, 312, 534, 8416, 322, 13], "temperature": 0.0, "avg_logprob": -0.16380477704499896, "compression_ratio": 1.5918367346938775, "no_speech_prob": 1.0129938345926348e-05}, {"id": 620, "seek": 300096, "start": 3015.56, "end": 3017.64, "text": " So that's the most important one.", "tokens": [407, 300, 311, 264, 881, 1021, 472, 13], "temperature": 0.0, "avg_logprob": -0.16380477704499896, "compression_ratio": 1.5918367346938775, "no_speech_prob": 1.0129938345926348e-05}, {"id": 621, "seek": 300096, "start": 3017.64, "end": 3023.68, "text": " And so when I did credit scoring in telecommunications, I found there were 9 variables that basically", "tokens": [400, 370, 562, 286, 630, 5397, 22358, 294, 4304, 25451, 24847, 11, 286, 1352, 456, 645, 1722, 9102, 300, 1936], "temperature": 0.0, "avg_logprob": -0.16380477704499896, "compression_ratio": 1.5918367346938775, "no_speech_prob": 1.0129938345926348e-05}, {"id": 622, "seek": 300096, "start": 3023.68, "end": 3029.28, "text": " predicted very accurately who was going to end up paying for their phone and who wasn't.", "tokens": [19147, 588, 20095, 567, 390, 516, 281, 917, 493, 6229, 337, 641, 2593, 293, 567, 2067, 380, 13], "temperature": 0.0, "avg_logprob": -0.16380477704499896, "compression_ratio": 1.5918367346938775, "no_speech_prob": 1.0129938345926348e-05}, {"id": 623, "seek": 302928, "start": 3029.28, "end": 3035.1400000000003, "text": " And apart from ending up with a model that saved them $3 billion a year in fraud and", "tokens": [400, 4936, 490, 8121, 493, 365, 257, 2316, 300, 6624, 552, 1848, 18, 5218, 257, 1064, 294, 14560, 293], "temperature": 0.0, "avg_logprob": -0.13355964567603135, "compression_ratio": 1.4761904761904763, "no_speech_prob": 2.05804212782823e-06}, {"id": 624, "seek": 302928, "start": 3035.1400000000003, "end": 3041.28, "text": " credit costs, it also let them basically rejig their process so they focused on collecting", "tokens": [5397, 5497, 11, 309, 611, 718, 552, 1936, 319, 73, 328, 641, 1399, 370, 436, 5178, 322, 12510], "temperature": 0.0, "avg_logprob": -0.13355964567603135, "compression_ratio": 1.4761904761904763, "no_speech_prob": 2.05804212782823e-06}, {"id": 625, "seek": 302928, "start": 3041.28, "end": 3047.0800000000004, "text": " those 9 variables much better.", "tokens": [729, 1722, 9102, 709, 1101, 13], "temperature": 0.0, "avg_logprob": -0.13355964567603135, "compression_ratio": 1.4761904761904763, "no_speech_prob": 2.05804212782823e-06}, {"id": 626, "seek": 302928, "start": 3047.0800000000004, "end": 3053.0800000000004, "text": " Alright who wants to do partial dependence?", "tokens": [2798, 567, 2738, 281, 360, 14641, 31704, 30], "temperature": 0.0, "avg_logprob": -0.13355964567603135, "compression_ratio": 1.4761904761904763, "no_speech_prob": 2.05804212782823e-06}, {"id": 627, "seek": 302928, "start": 3053.0800000000004, "end": 3059.2000000000003, "text": " This is an interesting one, very important, but in some ways kind of tricky to think about", "tokens": [639, 307, 364, 1880, 472, 11, 588, 1021, 11, 457, 294, 512, 2098, 733, 295, 12414, 281, 519, 466], "temperature": 0.0, "avg_logprob": -0.13355964567603135, "compression_ratio": 1.4761904761904763, "no_speech_prob": 2.05804212782823e-06}, {"id": 628, "seek": 305920, "start": 3059.2, "end": 3060.2, "text": " it.", "tokens": [309, 13], "temperature": 0.0, "avg_logprob": -0.3059007583125945, "compression_ratio": 1.7544483985765125, "no_speech_prob": 8.61403823364526e-05}, {"id": 629, "seek": 305920, "start": 3060.2, "end": 3061.2, "text": " Go ahead and try.", "tokens": [1037, 2286, 293, 853, 13], "temperature": 0.0, "avg_logprob": -0.3059007583125945, "compression_ratio": 1.7544483985765125, "no_speech_prob": 8.61403823364526e-05}, {"id": 630, "seek": 305920, "start": 3061.2, "end": 3062.2, "text": " Yeah, please do.", "tokens": [865, 11, 1767, 360, 13], "temperature": 0.0, "avg_logprob": -0.3059007583125945, "compression_ratio": 1.7544483985765125, "no_speech_prob": 8.61403823364526e-05}, {"id": 631, "seek": 305920, "start": 3062.2, "end": 3067.8399999999997, "text": " So from my understanding of what partial dependence is, is that there's not always necessarily", "tokens": [407, 490, 452, 3701, 295, 437, 14641, 31704, 307, 11, 307, 300, 456, 311, 406, 1009, 4725], "temperature": 0.0, "avg_logprob": -0.3059007583125945, "compression_ratio": 1.7544483985765125, "no_speech_prob": 8.61403823364526e-05}, {"id": 632, "seek": 305920, "start": 3067.8399999999997, "end": 3073.2, "text": " like a relationship between the strictly the dependent variable and this independent variable", "tokens": [411, 257, 2480, 1296, 264, 20792, 264, 12334, 7006, 293, 341, 6695, 7006], "temperature": 0.0, "avg_logprob": -0.3059007583125945, "compression_ratio": 1.7544483985765125, "no_speech_prob": 8.61403823364526e-05}, {"id": 633, "seek": 305920, "start": 3073.2, "end": 3080.3599999999997, "text": " that necessarily like is showing importance, but rather than interaction between two variables", "tokens": [300, 4725, 411, 307, 4099, 7379, 11, 457, 2831, 813, 9285, 1296, 732, 9102], "temperature": 0.0, "avg_logprob": -0.3059007583125945, "compression_ratio": 1.7544483985765125, "no_speech_prob": 8.61403823364526e-05}, {"id": 634, "seek": 305920, "start": 3080.3599999999997, "end": 3081.3599999999997, "text": " that are working together.", "tokens": [300, 366, 1364, 1214, 13], "temperature": 0.0, "avg_logprob": -0.3059007583125945, "compression_ratio": 1.7544483985765125, "no_speech_prob": 8.61403823364526e-05}, {"id": 635, "seek": 305920, "start": 3081.3599999999997, "end": 3082.3599999999997, "text": " Something like this, right?", "tokens": [6595, 411, 341, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.3059007583125945, "compression_ratio": 1.7544483985765125, "no_speech_prob": 8.61403823364526e-05}, {"id": 636, "seek": 305920, "start": 3082.3599999999997, "end": 3083.3599999999997, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.3059007583125945, "compression_ratio": 1.7544483985765125, "no_speech_prob": 8.61403823364526e-05}, {"id": 637, "seek": 305920, "start": 3083.3599999999997, "end": 3086.16, "text": " Where we're like, oh, that's weird, like, you'd expect this to be kind of flat and there's", "tokens": [2305, 321, 434, 411, 11, 1954, 11, 300, 311, 3657, 11, 411, 11, 291, 1116, 2066, 341, 281, 312, 733, 295, 4962, 293, 456, 311], "temperature": 0.0, "avg_logprob": -0.3059007583125945, "compression_ratio": 1.7544483985765125, "no_speech_prob": 8.61403823364526e-05}, {"id": 638, "seek": 305920, "start": 3086.16, "end": 3088.04, "text": " a weird pokey bit.", "tokens": [257, 3657, 19712, 88, 857, 13], "temperature": 0.0, "avg_logprob": -0.3059007583125945, "compression_ratio": 1.7544483985765125, "no_speech_prob": 8.61403823364526e-05}, {"id": 639, "seek": 308804, "start": 3088.04, "end": 3095.08, "text": " Yeah, and so for this example what we found was that it's not necessarily year made or", "tokens": [865, 11, 293, 370, 337, 341, 1365, 437, 321, 1352, 390, 300, 309, 311, 406, 4725, 1064, 1027, 420], "temperature": 0.0, "avg_logprob": -0.19868776230585009, "compression_ratio": 1.672340425531915, "no_speech_prob": 4.785061264556134e-06}, {"id": 640, "seek": 308804, "start": 3095.08, "end": 3099.9, "text": " when the sale was elapsed, but it's actually the age of the model and so it's that is easier", "tokens": [562, 264, 8680, 390, 806, 2382, 292, 11, 457, 309, 311, 767, 264, 3205, 295, 264, 2316, 293, 370, 309, 311, 300, 307, 3571], "temperature": 0.0, "avg_logprob": -0.19868776230585009, "compression_ratio": 1.672340425531915, "no_speech_prob": 4.785061264556134e-06}, {"id": 641, "seek": 308804, "start": 3099.9, "end": 3105.48, "text": " to just like to tell a like company well obviously your younger models are going to sell for", "tokens": [281, 445, 411, 281, 980, 257, 411, 2237, 731, 2745, 428, 7037, 5245, 366, 516, 281, 3607, 337], "temperature": 0.0, "avg_logprob": -0.19868776230585009, "compression_ratio": 1.672340425531915, "no_speech_prob": 4.785061264556134e-06}, {"id": 642, "seek": 308804, "start": 3105.48, "end": 3108.44, "text": " more and it's less about when the year was made.", "tokens": [544, 293, 309, 311, 1570, 466, 562, 264, 1064, 390, 1027, 13], "temperature": 0.0, "avg_logprob": -0.19868776230585009, "compression_ratio": 1.672340425531915, "no_speech_prob": 4.785061264556134e-06}, {"id": 643, "seek": 308804, "start": 3108.44, "end": 3110.22, "text": " Yeah, exactly.", "tokens": [865, 11, 2293, 13], "temperature": 0.0, "avg_logprob": -0.19868776230585009, "compression_ratio": 1.672340425531915, "no_speech_prob": 4.785061264556134e-06}, {"id": 644, "seek": 308804, "start": 3110.22, "end": 3113.48, "text": " So let's come back to how we calculate this in a moment.", "tokens": [407, 718, 311, 808, 646, 281, 577, 321, 8873, 341, 294, 257, 1623, 13], "temperature": 0.0, "avg_logprob": -0.19868776230585009, "compression_ratio": 1.672340425531915, "no_speech_prob": 4.785061264556134e-06}, {"id": 645, "seek": 311348, "start": 3113.48, "end": 3118.04, "text": " But the first thing to realize is that the vast majority of the time, you know, post", "tokens": [583, 264, 700, 551, 281, 4325, 307, 300, 264, 8369, 6286, 295, 264, 565, 11, 291, 458, 11, 2183], "temperature": 0.0, "avg_logprob": -0.18071109937584917, "compression_ratio": 1.60431654676259, "no_speech_prob": 7.527922662120545e-06}, {"id": 646, "seek": 311348, "start": 3118.04, "end": 3123.48, "text": " your course here when somebody shows you a chart, it'll be like a univariate chart.", "tokens": [428, 1164, 510, 562, 2618, 3110, 291, 257, 6927, 11, 309, 603, 312, 411, 257, 517, 592, 3504, 473, 6927, 13], "temperature": 0.0, "avg_logprob": -0.18071109937584917, "compression_ratio": 1.60431654676259, "no_speech_prob": 7.527922662120545e-06}, {"id": 647, "seek": 311348, "start": 3123.48, "end": 3128.4, "text": " That'll just like grab the data from the database and they'll plot X against Y and then managers", "tokens": [663, 603, 445, 411, 4444, 264, 1412, 490, 264, 8149, 293, 436, 603, 7542, 1783, 1970, 398, 293, 550, 14084], "temperature": 0.0, "avg_logprob": -0.18071109937584917, "compression_ratio": 1.60431654676259, "no_speech_prob": 7.527922662120545e-06}, {"id": 648, "seek": 311348, "start": 3128.4, "end": 3131.28, "text": " have a tendency to want to like make a decision.", "tokens": [362, 257, 18187, 281, 528, 281, 411, 652, 257, 3537, 13], "temperature": 0.0, "avg_logprob": -0.18071109937584917, "compression_ratio": 1.60431654676259, "no_speech_prob": 7.527922662120545e-06}, {"id": 649, "seek": 311348, "start": 3131.28, "end": 3137.02, "text": " So be like, oh, there's this like drop-off here so we should like stop dealing in equipment", "tokens": [407, 312, 411, 11, 1954, 11, 456, 311, 341, 411, 3270, 12, 4506, 510, 370, 321, 820, 411, 1590, 6260, 294, 5927], "temperature": 0.0, "avg_logprob": -0.18071109937584917, "compression_ratio": 1.60431654676259, "no_speech_prob": 7.527922662120545e-06}, {"id": 650, "seek": 311348, "start": 3137.02, "end": 3140.76, "text": " made between 1990 and 1995 or whatever.", "tokens": [1027, 1296, 13384, 293, 22601, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.18071109937584917, "compression_ratio": 1.60431654676259, "no_speech_prob": 7.527922662120545e-06}, {"id": 651, "seek": 314076, "start": 3140.76, "end": 3148.44, "text": " And this is like a big problem because like real world data has lots of these interactions", "tokens": [400, 341, 307, 411, 257, 955, 1154, 570, 411, 957, 1002, 1412, 575, 3195, 295, 613, 13280], "temperature": 0.0, "avg_logprob": -0.13863132584769772, "compression_ratio": 1.6934865900383143, "no_speech_prob": 8.664591405249666e-06}, {"id": 652, "seek": 314076, "start": 3148.44, "end": 3149.44, "text": " going on.", "tokens": [516, 322, 13], "temperature": 0.0, "avg_logprob": -0.13863132584769772, "compression_ratio": 1.6934865900383143, "no_speech_prob": 8.664591405249666e-06}, {"id": 653, "seek": 314076, "start": 3149.44, "end": 3154.4, "text": " So like, you know, maybe there was a recession going on around the time that those things", "tokens": [407, 411, 11, 291, 458, 11, 1310, 456, 390, 257, 24828, 516, 322, 926, 264, 565, 300, 729, 721], "temperature": 0.0, "avg_logprob": -0.13863132584769772, "compression_ratio": 1.6934865900383143, "no_speech_prob": 8.664591405249666e-06}, {"id": 654, "seek": 314076, "start": 3154.4, "end": 3159.0400000000004, "text": " are being sold or maybe around that time people were buying more of a different type of equipment", "tokens": [366, 885, 3718, 420, 1310, 926, 300, 565, 561, 645, 6382, 544, 295, 257, 819, 2010, 295, 5927], "temperature": 0.0, "avg_logprob": -0.13863132584769772, "compression_ratio": 1.6934865900383143, "no_speech_prob": 8.664591405249666e-06}, {"id": 655, "seek": 314076, "start": 3159.0400000000004, "end": 3160.32, "text": " or whatever.", "tokens": [420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.13863132584769772, "compression_ratio": 1.6934865900383143, "no_speech_prob": 8.664591405249666e-06}, {"id": 656, "seek": 314076, "start": 3160.32, "end": 3161.32, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.13863132584769772, "compression_ratio": 1.6934865900383143, "no_speech_prob": 8.664591405249666e-06}, {"id": 657, "seek": 314076, "start": 3161.32, "end": 3165.92, "text": " So generally what we actually want to know is all other things being equal.", "tokens": [407, 5101, 437, 321, 767, 528, 281, 458, 307, 439, 661, 721, 885, 2681, 13], "temperature": 0.0, "avg_logprob": -0.13863132584769772, "compression_ratio": 1.6934865900383143, "no_speech_prob": 8.664591405249666e-06}, {"id": 658, "seek": 314076, "start": 3165.92, "end": 3169.8, "text": " What's the relationship between year made and sale price?", "tokens": [708, 311, 264, 2480, 1296, 1064, 1027, 293, 8680, 3218, 30], "temperature": 0.0, "avg_logprob": -0.13863132584769772, "compression_ratio": 1.6934865900383143, "no_speech_prob": 8.664591405249666e-06}, {"id": 659, "seek": 316980, "start": 3169.8, "end": 3175.7000000000003, "text": " Right, because like if you think about the drivetrain approach idea of like the levers,", "tokens": [1779, 11, 570, 411, 498, 291, 519, 466, 264, 1630, 9771, 7146, 3109, 1558, 295, 411, 264, 45571, 11], "temperature": 0.0, "avg_logprob": -0.1362085454604205, "compression_ratio": 1.5584415584415585, "no_speech_prob": 2.2603205707127927e-06}, {"id": 660, "seek": 316980, "start": 3175.7000000000003, "end": 3183.92, "text": " you really want to model that says if I change this lever, how will it change my objective?", "tokens": [291, 534, 528, 281, 2316, 300, 1619, 498, 286, 1319, 341, 12451, 11, 577, 486, 309, 1319, 452, 10024, 30], "temperature": 0.0, "avg_logprob": -0.1362085454604205, "compression_ratio": 1.5584415584415585, "no_speech_prob": 2.2603205707127927e-06}, {"id": 661, "seek": 316980, "start": 3183.92, "end": 3190.6800000000003, "text": " And so it's by pulling them apart using partial dependence that you can say, okay, actually", "tokens": [400, 370, 309, 311, 538, 8407, 552, 4936, 1228, 14641, 31704, 300, 291, 393, 584, 11, 1392, 11, 767], "temperature": 0.0, "avg_logprob": -0.1362085454604205, "compression_ratio": 1.5584415584415585, "no_speech_prob": 2.2603205707127927e-06}, {"id": 662, "seek": 316980, "start": 3190.6800000000003, "end": 3197.5600000000004, "text": " this is the relationship between year made and sale price, all other things being equal.", "tokens": [341, 307, 264, 2480, 1296, 1064, 1027, 293, 8680, 3218, 11, 439, 661, 721, 885, 2681, 13], "temperature": 0.0, "avg_logprob": -0.1362085454604205, "compression_ratio": 1.5584415584415585, "no_speech_prob": 2.2603205707127927e-06}, {"id": 663, "seek": 319756, "start": 3197.56, "end": 3202.4, "text": " So how do we calculate that?", "tokens": [407, 577, 360, 321, 8873, 300, 30], "temperature": 0.0, "avg_logprob": -0.17537893163095605, "compression_ratio": 1.8009708737864079, "no_speech_prob": 1.3419856259133667e-05}, {"id": 664, "seek": 319756, "start": 3202.4, "end": 3208.0, "text": " For the variable year made, for example, you're going to train, you keep every other variable", "tokens": [1171, 264, 7006, 1064, 1027, 11, 337, 1365, 11, 291, 434, 516, 281, 3847, 11, 291, 1066, 633, 661, 7006], "temperature": 0.0, "avg_logprob": -0.17537893163095605, "compression_ratio": 1.8009708737864079, "no_speech_prob": 1.3419856259133667e-05}, {"id": 665, "seek": 319756, "start": 3208.0, "end": 3212.2, "text": " constants and then you're going to pass every single value of the year made and then train", "tokens": [35870, 293, 550, 291, 434, 516, 281, 1320, 633, 2167, 2158, 295, 264, 1064, 1027, 293, 550, 3847], "temperature": 0.0, "avg_logprob": -0.17537893163095605, "compression_ratio": 1.8009708737864079, "no_speech_prob": 1.3419856259133667e-05}, {"id": 666, "seek": 319756, "start": 3212.2, "end": 3213.52, "text": " the model after that.", "tokens": [264, 2316, 934, 300, 13], "temperature": 0.0, "avg_logprob": -0.17537893163095605, "compression_ratio": 1.8009708737864079, "no_speech_prob": 1.3419856259133667e-05}, {"id": 667, "seek": 319756, "start": 3213.52, "end": 3220.2799999999997, "text": " So for every model, you're going to have the light blue for the values of it and the median", "tokens": [407, 337, 633, 2316, 11, 291, 434, 516, 281, 362, 264, 1442, 3344, 337, 264, 4190, 295, 309, 293, 264, 26779], "temperature": 0.0, "avg_logprob": -0.17537893163095605, "compression_ratio": 1.8009708737864079, "no_speech_prob": 1.3419856259133667e-05}, {"id": 668, "seek": 319756, "start": 3220.2799999999997, "end": 3223.48, "text": " is going to be the yellow line.", "tokens": [307, 516, 281, 312, 264, 5566, 1622, 13], "temperature": 0.0, "avg_logprob": -0.17537893163095605, "compression_ratio": 1.8009708737864079, "no_speech_prob": 1.3419856259133667e-05}, {"id": 669, "seek": 319756, "start": 3223.48, "end": 3224.48, "text": " Good.", "tokens": [2205, 13], "temperature": 0.0, "avg_logprob": -0.17537893163095605, "compression_ratio": 1.8009708737864079, "no_speech_prob": 1.3419856259133667e-05}, {"id": 670, "seek": 319756, "start": 3224.48, "end": 3225.48, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.17537893163095605, "compression_ratio": 1.8009708737864079, "no_speech_prob": 1.3419856259133667e-05}, {"id": 671, "seek": 322548, "start": 3225.48, "end": 3229.48, "text": " So let's try and draw that.", "tokens": [407, 718, 311, 853, 293, 2642, 300, 13], "temperature": 0.0, "avg_logprob": -0.12373748117563677, "compression_ratio": 1.7582938388625593, "no_speech_prob": 3.6688559248432284e-06}, {"id": 672, "seek": 322548, "start": 3229.48, "end": 3235.44, "text": " So by leave everything else constant, what she means is leave them at whatever they are", "tokens": [407, 538, 1856, 1203, 1646, 5754, 11, 437, 750, 1355, 307, 1856, 552, 412, 2035, 436, 366], "temperature": 0.0, "avg_logprob": -0.12373748117563677, "compression_ratio": 1.7582938388625593, "no_speech_prob": 3.6688559248432284e-06}, {"id": 673, "seek": 322548, "start": 3235.44, "end": 3236.52, "text": " in the dataset.", "tokens": [294, 264, 28872, 13], "temperature": 0.0, "avg_logprob": -0.12373748117563677, "compression_ratio": 1.7582938388625593, "no_speech_prob": 3.6688559248432284e-06}, {"id": 674, "seek": 322548, "start": 3236.52, "end": 3241.16, "text": " So just like when we did feature importance, we're going to leave the rest of the dataset", "tokens": [407, 445, 411, 562, 321, 630, 4111, 7379, 11, 321, 434, 516, 281, 1856, 264, 1472, 295, 264, 28872], "temperature": 0.0, "avg_logprob": -0.12373748117563677, "compression_ratio": 1.7582938388625593, "no_speech_prob": 3.6688559248432284e-06}, {"id": 675, "seek": 322548, "start": 3241.16, "end": 3245.36, "text": " as it is and we're going to do partial dependence plot for year made.", "tokens": [382, 309, 307, 293, 321, 434, 516, 281, 360, 14641, 31704, 7542, 337, 1064, 1027, 13], "temperature": 0.0, "avg_logprob": -0.12373748117563677, "compression_ratio": 1.7582938388625593, "no_speech_prob": 3.6688559248432284e-06}, {"id": 676, "seek": 322548, "start": 3245.36, "end": 3251.04, "text": " So we've got all of these other rows of data that we'll just leave as they are.", "tokens": [407, 321, 600, 658, 439, 295, 613, 661, 13241, 295, 1412, 300, 321, 603, 445, 1856, 382, 436, 366, 13], "temperature": 0.0, "avg_logprob": -0.12373748117563677, "compression_ratio": 1.7582938388625593, "no_speech_prob": 3.6688559248432284e-06}, {"id": 677, "seek": 325104, "start": 3251.04, "end": 3258.7599999999998, "text": " And so instead of randomly shuffling year made, instead what we're going to do is replace", "tokens": [400, 370, 2602, 295, 16979, 402, 1245, 1688, 1064, 1027, 11, 2602, 437, 321, 434, 516, 281, 360, 307, 7406], "temperature": 0.0, "avg_logprob": -0.1829545904950398, "compression_ratio": 1.4511627906976745, "no_speech_prob": 4.785080363944871e-06}, {"id": 678, "seek": 325104, "start": 3258.7599999999998, "end": 3262.88, "text": " every single value with exactly the same thing.", "tokens": [633, 2167, 2158, 365, 2293, 264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.1829545904950398, "compression_ratio": 1.4511627906976745, "no_speech_prob": 4.785080363944871e-06}, {"id": 679, "seek": 325104, "start": 3262.88, "end": 3265.24, "text": " 1960.", "tokens": [16157, 13], "temperature": 0.0, "avg_logprob": -0.1829545904950398, "compression_ratio": 1.4511627906976745, "no_speech_prob": 4.785080363944871e-06}, {"id": 680, "seek": 325104, "start": 3265.24, "end": 3269.4, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.1829545904950398, "compression_ratio": 1.4511627906976745, "no_speech_prob": 4.785080363944871e-06}, {"id": 681, "seek": 325104, "start": 3269.4, "end": 3273.72, "text": " And just like before, we now pass that through our existing random forest, which we have", "tokens": [400, 445, 411, 949, 11, 321, 586, 1320, 300, 807, 527, 6741, 4974, 6719, 11, 597, 321, 362], "temperature": 0.0, "avg_logprob": -0.1829545904950398, "compression_ratio": 1.4511627906976745, "no_speech_prob": 4.785080363944871e-06}, {"id": 682, "seek": 325104, "start": 3273.72, "end": 3279.72, "text": " not retrained or changed in any way to get back out a set of predictions.", "tokens": [406, 1533, 31774, 420, 3105, 294, 604, 636, 281, 483, 646, 484, 257, 992, 295, 21264, 13], "temperature": 0.0, "avg_logprob": -0.1829545904950398, "compression_ratio": 1.4511627906976745, "no_speech_prob": 4.785080363944871e-06}, {"id": 683, "seek": 327972, "start": 3279.72, "end": 3284.6, "text": " Why 1960?", "tokens": [1545, 16157, 30], "temperature": 0.0, "avg_logprob": -0.34956968876353484, "compression_ratio": 1.2421875, "no_speech_prob": 1.0129937436431646e-05}, {"id": 684, "seek": 327972, "start": 3284.6, "end": 3288.08, "text": " And so then we can plot that on a chart.", "tokens": [400, 370, 550, 321, 393, 7542, 300, 322, 257, 6927, 13], "temperature": 0.0, "avg_logprob": -0.34956968876353484, "compression_ratio": 1.2421875, "no_speech_prob": 1.0129937436431646e-05}, {"id": 685, "seek": 327972, "start": 3288.08, "end": 3294.2, "text": " Year made against partial dependence, 1960 year.", "tokens": [10289, 1027, 1970, 14641, 31704, 11, 16157, 1064, 13], "temperature": 0.0, "avg_logprob": -0.34956968876353484, "compression_ratio": 1.2421875, "no_speech_prob": 1.0129937436431646e-05}, {"id": 686, "seek": 327972, "start": 3294.2, "end": 3295.2, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.34956968876353484, "compression_ratio": 1.2421875, "no_speech_prob": 1.0129937436431646e-05}, {"id": 687, "seek": 327972, "start": 3295.2, "end": 3303.56, "text": " Then we can do it for 1961, 2, 3, 4, 5, and so forth.", "tokens": [1396, 321, 393, 360, 309, 337, 41720, 11, 568, 11, 805, 11, 1017, 11, 1025, 11, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.34956968876353484, "compression_ratio": 1.2421875, "no_speech_prob": 1.0129937436431646e-05}, {"id": 688, "seek": 330356, "start": 3303.56, "end": 3311.84, "text": " And so we can do that for on average for all of them, or we could do it just for one of", "tokens": [400, 370, 321, 393, 360, 300, 337, 322, 4274, 337, 439, 295, 552, 11, 420, 321, 727, 360, 309, 445, 337, 472, 295], "temperature": 0.0, "avg_logprob": -0.13999941038048785, "compression_ratio": 1.8666666666666667, "no_speech_prob": 1.4144729902909603e-06}, {"id": 689, "seek": 330356, "start": 3311.84, "end": 3314.08, "text": " them.", "tokens": [552, 13], "temperature": 0.0, "avg_logprob": -0.13999941038048785, "compression_ratio": 1.8666666666666667, "no_speech_prob": 1.4144729902909603e-06}, {"id": 690, "seek": 330356, "start": 3314.08, "end": 3318.12, "text": " And so when we do it for just one of them and we change its year made and pass that", "tokens": [400, 370, 562, 321, 360, 309, 337, 445, 472, 295, 552, 293, 321, 1319, 1080, 1064, 1027, 293, 1320, 300], "temperature": 0.0, "avg_logprob": -0.13999941038048785, "compression_ratio": 1.8666666666666667, "no_speech_prob": 1.4144729902909603e-06}, {"id": 691, "seek": 330356, "start": 3318.12, "end": 3322.96, "text": " single thing through our model, that gives us one of these blue lines.", "tokens": [2167, 551, 807, 527, 2316, 11, 300, 2709, 505, 472, 295, 613, 3344, 3876, 13], "temperature": 0.0, "avg_logprob": -0.13999941038048785, "compression_ratio": 1.8666666666666667, "no_speech_prob": 1.4144729902909603e-06}, {"id": 692, "seek": 330356, "start": 3322.96, "end": 3329.92, "text": " So each one of these blue lines is a single row as we change its year made from 1960 up", "tokens": [407, 1184, 472, 295, 613, 3344, 3876, 307, 257, 2167, 5386, 382, 321, 1319, 1080, 1064, 1027, 490, 16157, 493], "temperature": 0.0, "avg_logprob": -0.13999941038048785, "compression_ratio": 1.8666666666666667, "no_speech_prob": 1.4144729902909603e-06}, {"id": 693, "seek": 332992, "start": 3329.92, "end": 3334.04, "text": " to 2008.", "tokens": [281, 10389, 13], "temperature": 0.0, "avg_logprob": -0.13762850988478886, "compression_ratio": 1.8554913294797688, "no_speech_prob": 1.6028085383368307e-06}, {"id": 694, "seek": 332992, "start": 3334.04, "end": 3340.76, "text": " And so then we can just take the median of all of those blue lines to say, on average,", "tokens": [400, 370, 550, 321, 393, 445, 747, 264, 26779, 295, 439, 295, 729, 3344, 3876, 281, 584, 11, 322, 4274, 11], "temperature": 0.0, "avg_logprob": -0.13762850988478886, "compression_ratio": 1.8554913294797688, "no_speech_prob": 1.6028085383368307e-06}, {"id": 695, "seek": 332992, "start": 3340.76, "end": 3347.04, "text": " what's the relationship between year made and price, all other things being equal.", "tokens": [437, 311, 264, 2480, 1296, 1064, 1027, 293, 3218, 11, 439, 661, 721, 885, 2681, 13], "temperature": 0.0, "avg_logprob": -0.13762850988478886, "compression_ratio": 1.8554913294797688, "no_speech_prob": 1.6028085383368307e-06}, {"id": 696, "seek": 332992, "start": 3347.04, "end": 3352.84, "text": " So why is it that this works?", "tokens": [407, 983, 307, 309, 300, 341, 1985, 30], "temperature": 0.0, "avg_logprob": -0.13762850988478886, "compression_ratio": 1.8554913294797688, "no_speech_prob": 1.6028085383368307e-06}, {"id": 697, "seek": 332992, "start": 3352.84, "end": 3358.3, "text": " Why is it that this process tells us the relationship between year made and price, all other things", "tokens": [1545, 307, 309, 300, 341, 1399, 5112, 505, 264, 2480, 1296, 1064, 1027, 293, 3218, 11, 439, 661, 721], "temperature": 0.0, "avg_logprob": -0.13762850988478886, "compression_ratio": 1.8554913294797688, "no_speech_prob": 1.6028085383368307e-06}, {"id": 698, "seek": 332992, "start": 3358.3, "end": 3359.3, "text": " being equal?", "tokens": [885, 2681, 30], "temperature": 0.0, "avg_logprob": -0.13762850988478886, "compression_ratio": 1.8554913294797688, "no_speech_prob": 1.6028085383368307e-06}, {"id": 699, "seek": 335930, "start": 3359.3, "end": 3363.6800000000003, "text": " Well, maybe it's good to think about a really simplified approach.", "tokens": [1042, 11, 1310, 309, 311, 665, 281, 519, 466, 257, 534, 26335, 3109, 13], "temperature": 0.0, "avg_logprob": -0.21570545503462868, "compression_ratio": 1.7938144329896908, "no_speech_prob": 2.190777195210103e-06}, {"id": 700, "seek": 335930, "start": 3363.6800000000003, "end": 3369.1200000000003, "text": " A really simplified approach would say, what's the average auction?", "tokens": [316, 534, 26335, 3109, 576, 584, 11, 437, 311, 264, 4274, 24139, 30], "temperature": 0.0, "avg_logprob": -0.21570545503462868, "compression_ratio": 1.7938144329896908, "no_speech_prob": 2.190777195210103e-06}, {"id": 701, "seek": 335930, "start": 3369.1200000000003, "end": 3371.28, "text": " What's the average sale date?", "tokens": [708, 311, 264, 4274, 8680, 4002, 30], "temperature": 0.0, "avg_logprob": -0.21570545503462868, "compression_ratio": 1.7938144329896908, "no_speech_prob": 2.190777195210103e-06}, {"id": 702, "seek": 335930, "start": 3371.28, "end": 3374.5800000000004, "text": " What's the most common type of machine we sell?", "tokens": [708, 311, 264, 881, 2689, 2010, 295, 3479, 321, 3607, 30], "temperature": 0.0, "avg_logprob": -0.21570545503462868, "compression_ratio": 1.7938144329896908, "no_speech_prob": 2.190777195210103e-06}, {"id": 703, "seek": 335930, "start": 3374.5800000000004, "end": 3377.32, "text": " Which location do we mostly sell things?", "tokens": [3013, 4914, 360, 321, 5240, 3607, 721, 30], "temperature": 0.0, "avg_logprob": -0.21570545503462868, "compression_ratio": 1.7938144329896908, "no_speech_prob": 2.190777195210103e-06}, {"id": 704, "seek": 335930, "start": 3377.32, "end": 3383.28, "text": " We could come up with a single row that represents the average auction, and then we could say,", "tokens": [492, 727, 808, 493, 365, 257, 2167, 5386, 300, 8855, 264, 4274, 24139, 11, 293, 550, 321, 727, 584, 11], "temperature": 0.0, "avg_logprob": -0.21570545503462868, "compression_ratio": 1.7938144329896908, "no_speech_prob": 2.190777195210103e-06}, {"id": 705, "seek": 338328, "start": 3383.28, "end": 3389.44, "text": " let's run that row through the random forest, replace its year made with 1960, and then", "tokens": [718, 311, 1190, 300, 5386, 807, 264, 4974, 6719, 11, 7406, 1080, 1064, 1027, 365, 16157, 11, 293, 550], "temperature": 0.0, "avg_logprob": -0.1392206167563414, "compression_ratio": 1.6432432432432433, "no_speech_prob": 5.682416485797148e-06}, {"id": 706, "seek": 338328, "start": 3389.44, "end": 3392.76, "text": " do it again with 1961, and then do it again with 1962.", "tokens": [360, 309, 797, 365, 41720, 11, 293, 550, 360, 309, 797, 365, 39498, 13], "temperature": 0.0, "avg_logprob": -0.1392206167563414, "compression_ratio": 1.6432432432432433, "no_speech_prob": 5.682416485797148e-06}, {"id": 707, "seek": 338328, "start": 3392.76, "end": 3397.6800000000003, "text": " And we could plot those on our little chart.", "tokens": [400, 321, 727, 7542, 729, 322, 527, 707, 6927, 13], "temperature": 0.0, "avg_logprob": -0.1392206167563414, "compression_ratio": 1.6432432432432433, "no_speech_prob": 5.682416485797148e-06}, {"id": 708, "seek": 338328, "start": 3397.6800000000003, "end": 3406.2400000000002, "text": " And that would give us a version of the relationship between year made and sale price, all other", "tokens": [400, 300, 576, 976, 505, 257, 3037, 295, 264, 2480, 1296, 1064, 1027, 293, 8680, 3218, 11, 439, 661], "temperature": 0.0, "avg_logprob": -0.1392206167563414, "compression_ratio": 1.6432432432432433, "no_speech_prob": 5.682416485797148e-06}, {"id": 709, "seek": 338328, "start": 3406.2400000000002, "end": 3408.5600000000004, "text": " things being equal.", "tokens": [721, 885, 2681, 13], "temperature": 0.0, "avg_logprob": -0.1392206167563414, "compression_ratio": 1.6432432432432433, "no_speech_prob": 5.682416485797148e-06}, {"id": 710, "seek": 340856, "start": 3408.56, "end": 3424.24, "text": " But what if tractors looked like that, and backhoe loaders looked like that?", "tokens": [583, 437, 498, 24207, 830, 2956, 411, 300, 11, 293, 646, 33810, 3677, 433, 2956, 411, 300, 30], "temperature": 0.0, "avg_logprob": -0.1394265751506007, "compression_ratio": 1.424, "no_speech_prob": 1.0845155884453561e-06}, {"id": 711, "seek": 340856, "start": 3424.24, "end": 3429.72, "text": " Then taking the average one would hide the fact that there are these totally different", "tokens": [1396, 1940, 264, 4274, 472, 576, 6479, 264, 1186, 300, 456, 366, 613, 3879, 819], "temperature": 0.0, "avg_logprob": -0.1394265751506007, "compression_ratio": 1.424, "no_speech_prob": 1.0845155884453561e-06}, {"id": 712, "seek": 340856, "start": 3429.72, "end": 3431.16, "text": " relationships.", "tokens": [6159, 13], "temperature": 0.0, "avg_logprob": -0.1394265751506007, "compression_ratio": 1.424, "no_speech_prob": 1.0845155884453561e-06}, {"id": 713, "seek": 343116, "start": 3431.16, "end": 3438.8399999999997, "text": " So instead, we basically say, our data tells us what kinds of things we tend to sell, and", "tokens": [407, 2602, 11, 321, 1936, 584, 11, 527, 1412, 5112, 505, 437, 3685, 295, 721, 321, 3928, 281, 3607, 11, 293], "temperature": 0.0, "avg_logprob": -0.14741759604596077, "compression_ratio": 1.7177033492822966, "no_speech_prob": 1.5534964177277288e-06}, {"id": 714, "seek": 343116, "start": 3438.8399999999997, "end": 3442.7799999999997, "text": " who we tend to sell them to, and when we tend to sell them, so let's use that.", "tokens": [567, 321, 3928, 281, 3607, 552, 281, 11, 293, 562, 321, 3928, 281, 3607, 552, 11, 370, 718, 311, 764, 300, 13], "temperature": 0.0, "avg_logprob": -0.14741759604596077, "compression_ratio": 1.7177033492822966, "no_speech_prob": 1.5534964177277288e-06}, {"id": 715, "seek": 343116, "start": 3442.7799999999997, "end": 3453.04, "text": " So then we actually find out for every blue line, here are actual examples of these relationships.", "tokens": [407, 550, 321, 767, 915, 484, 337, 633, 3344, 1622, 11, 510, 366, 3539, 5110, 295, 613, 6159, 13], "temperature": 0.0, "avg_logprob": -0.14741759604596077, "compression_ratio": 1.7177033492822966, "no_speech_prob": 1.5534964177277288e-06}, {"id": 716, "seek": 343116, "start": 3453.04, "end": 3457.8599999999997, "text": " And so then what we can do is, as well as plotting the median, we can do a cluster analysis", "tokens": [400, 370, 550, 437, 321, 393, 360, 307, 11, 382, 731, 382, 41178, 264, 26779, 11, 321, 393, 360, 257, 13630, 5215], "temperature": 0.0, "avg_logprob": -0.14741759604596077, "compression_ratio": 1.7177033492822966, "no_speech_prob": 1.5534964177277288e-06}, {"id": 717, "seek": 345786, "start": 3457.86, "end": 3464.0, "text": " to find out a few different shapes.", "tokens": [281, 915, 484, 257, 1326, 819, 10854, 13], "temperature": 0.0, "avg_logprob": -0.15557758843720848, "compression_ratio": 1.5706214689265536, "no_speech_prob": 7.889238986535929e-06}, {"id": 718, "seek": 345786, "start": 3464.0, "end": 3469.1600000000003, "text": " And so we may find, in this case they all look like pretty much different versions of", "tokens": [400, 370, 321, 815, 915, 11, 294, 341, 1389, 436, 439, 574, 411, 1238, 709, 819, 9606, 295], "temperature": 0.0, "avg_logprob": -0.15557758843720848, "compression_ratio": 1.5706214689265536, "no_speech_prob": 7.889238986535929e-06}, {"id": 719, "seek": 345786, "start": 3469.1600000000003, "end": 3473.08, "text": " the same thing with different slopes.", "tokens": [264, 912, 551, 365, 819, 37725, 13], "temperature": 0.0, "avg_logprob": -0.15557758843720848, "compression_ratio": 1.5706214689265536, "no_speech_prob": 7.889238986535929e-06}, {"id": 720, "seek": 345786, "start": 3473.08, "end": 3480.56, "text": " So my main takeaway from this would be that the relationship between sale price and year", "tokens": [407, 452, 2135, 30681, 490, 341, 576, 312, 300, 264, 2480, 1296, 8680, 3218, 293, 1064], "temperature": 0.0, "avg_logprob": -0.15557758843720848, "compression_ratio": 1.5706214689265536, "no_speech_prob": 7.889238986535929e-06}, {"id": 721, "seek": 345786, "start": 3480.56, "end": 3483.84, "text": " is basically a straight line.", "tokens": [307, 1936, 257, 2997, 1622, 13], "temperature": 0.0, "avg_logprob": -0.15557758843720848, "compression_ratio": 1.5706214689265536, "no_speech_prob": 7.889238986535929e-06}, {"id": 722, "seek": 348384, "start": 3483.84, "end": 3491.0, "text": " And remember, this was log of sale price, so this is actually showing us an exponential.", "tokens": [400, 1604, 11, 341, 390, 3565, 295, 8680, 3218, 11, 370, 341, 307, 767, 4099, 505, 364, 21510, 13], "temperature": 0.0, "avg_logprob": -0.13595913551949165, "compression_ratio": 1.605263157894737, "no_speech_prob": 2.3687957764195744e-06}, {"id": 723, "seek": 348384, "start": 3491.0, "end": 3497.84, "text": " And so this is where I would then bring in the domain expertise, which is like, okay,", "tokens": [400, 370, 341, 307, 689, 286, 576, 550, 1565, 294, 264, 9274, 11769, 11, 597, 307, 411, 11, 1392, 11], "temperature": 0.0, "avg_logprob": -0.13595913551949165, "compression_ratio": 1.605263157894737, "no_speech_prob": 2.3687957764195744e-06}, {"id": 724, "seek": 348384, "start": 3497.84, "end": 3506.6800000000003, "text": " things depreciate over time by a constant ratio, so therefore I would expect older stuff", "tokens": [721, 40609, 473, 670, 565, 538, 257, 5754, 8509, 11, 370, 4412, 286, 576, 2066, 4906, 1507], "temperature": 0.0, "avg_logprob": -0.13595913551949165, "compression_ratio": 1.605263157894737, "no_speech_prob": 2.3687957764195744e-06}, {"id": 725, "seek": 348384, "start": 3506.6800000000003, "end": 3510.96, "text": " year made to have this exponential shape.", "tokens": [1064, 1027, 281, 362, 341, 21510, 3909, 13], "temperature": 0.0, "avg_logprob": -0.13595913551949165, "compression_ratio": 1.605263157894737, "no_speech_prob": 2.3687957764195744e-06}, {"id": 726, "seek": 351096, "start": 3510.96, "end": 3516.84, "text": " So this is where, I kind of mentioned the very start of my machine learning project,", "tokens": [407, 341, 307, 689, 11, 286, 733, 295, 2835, 264, 588, 722, 295, 452, 3479, 2539, 1716, 11], "temperature": 0.0, "avg_logprob": -0.1972326494983791, "compression_ratio": 1.5384615384615385, "no_speech_prob": 4.5659608076675795e-06}, {"id": 727, "seek": 351096, "start": 3516.84, "end": 3522.92, "text": " I generally try to avoid using as much domain expertise as I can and let the data do the", "tokens": [286, 5101, 853, 281, 5042, 1228, 382, 709, 9274, 11769, 382, 286, 393, 293, 718, 264, 1412, 360, 264], "temperature": 0.0, "avg_logprob": -0.1972326494983791, "compression_ratio": 1.5384615384615385, "no_speech_prob": 4.5659608076675795e-06}, {"id": 728, "seek": 351096, "start": 3522.92, "end": 3524.2400000000002, "text": " talking.", "tokens": [1417, 13], "temperature": 0.0, "avg_logprob": -0.1972326494983791, "compression_ratio": 1.5384615384615385, "no_speech_prob": 4.5659608076675795e-06}, {"id": 729, "seek": 351096, "start": 3524.2400000000002, "end": 3529.84, "text": " So one of the questions I got this morning was, if there's a sale ID, a model ID, I should", "tokens": [407, 472, 295, 264, 1651, 286, 658, 341, 2446, 390, 11, 498, 456, 311, 257, 8680, 7348, 11, 257, 2316, 7348, 11, 286, 820], "temperature": 0.0, "avg_logprob": -0.1972326494983791, "compression_ratio": 1.5384615384615385, "no_speech_prob": 4.5659608076675795e-06}, {"id": 730, "seek": 351096, "start": 3529.84, "end": 3533.0, "text": " throw those away, because they're just IDs.", "tokens": [3507, 729, 1314, 11, 570, 436, 434, 445, 48212, 13], "temperature": 0.0, "avg_logprob": -0.1972326494983791, "compression_ratio": 1.5384615384615385, "no_speech_prob": 4.5659608076675795e-06}, {"id": 731, "seek": 351096, "start": 3533.0, "end": 3537.2, "text": " No, don't assume anything about your data.", "tokens": [883, 11, 500, 380, 6552, 1340, 466, 428, 1412, 13], "temperature": 0.0, "avg_logprob": -0.1972326494983791, "compression_ratio": 1.5384615384615385, "no_speech_prob": 4.5659608076675795e-06}, {"id": 732, "seek": 353720, "start": 3537.2, "end": 3542.52, "text": " Leave them in and if they turn out to be super important predictors, you want to find out", "tokens": [9825, 552, 294, 293, 498, 436, 1261, 484, 281, 312, 1687, 1021, 6069, 830, 11, 291, 528, 281, 915, 484], "temperature": 0.0, "avg_logprob": -0.21946046866622626, "compression_ratio": 1.6043478260869566, "no_speech_prob": 1.577960006216017e-06}, {"id": 733, "seek": 353720, "start": 3542.52, "end": 3543.52, "text": " why is that.", "tokens": [983, 307, 300, 13], "temperature": 0.0, "avg_logprob": -0.21946046866622626, "compression_ratio": 1.6043478260869566, "no_speech_prob": 1.577960006216017e-06}, {"id": 734, "seek": 353720, "start": 3543.52, "end": 3546.48, "text": " But then, now I'm at the other end of my project.", "tokens": [583, 550, 11, 586, 286, 478, 412, 264, 661, 917, 295, 452, 1716, 13], "temperature": 0.0, "avg_logprob": -0.21946046866622626, "compression_ratio": 1.6043478260869566, "no_speech_prob": 1.577960006216017e-06}, {"id": 735, "seek": 353720, "start": 3546.48, "end": 3553.12, "text": " I've done my feature importance, I've pulled out the stuff which is like from that dendrogram,", "tokens": [286, 600, 1096, 452, 4111, 7379, 11, 286, 600, 7373, 484, 264, 1507, 597, 307, 411, 490, 300, 274, 521, 340, 1342, 11], "temperature": 0.0, "avg_logprob": -0.21946046866622626, "compression_ratio": 1.6043478260869566, "no_speech_prob": 1.577960006216017e-06}, {"id": 736, "seek": 353720, "start": 3553.12, "end": 3557.7599999999998, "text": " the redundant features, I'm looking at the partial dependence, and now I'm thinking,", "tokens": [264, 40997, 4122, 11, 286, 478, 1237, 412, 264, 14641, 31704, 11, 293, 586, 286, 478, 1953, 11], "temperature": 0.0, "avg_logprob": -0.21946046866622626, "compression_ratio": 1.6043478260869566, "no_speech_prob": 1.577960006216017e-06}, {"id": 737, "seek": 353720, "start": 3557.7599999999998, "end": 3562.08, "text": " okay, is this shape what I expected?", "tokens": [1392, 11, 307, 341, 3909, 437, 286, 5176, 30], "temperature": 0.0, "avg_logprob": -0.21946046866622626, "compression_ratio": 1.6043478260869566, "no_speech_prob": 1.577960006216017e-06}, {"id": 738, "seek": 356208, "start": 3562.08, "end": 3568.08, "text": " So even better, before you plot this, first of all think, what shape would I expect this", "tokens": [407, 754, 1101, 11, 949, 291, 7542, 341, 11, 700, 295, 439, 519, 11, 437, 3909, 576, 286, 2066, 341], "temperature": 0.0, "avg_logprob": -0.18211261561659517, "compression_ratio": 1.6896551724137931, "no_speech_prob": 3.6119756714469986e-06}, {"id": 739, "seek": 356208, "start": 3568.08, "end": 3569.08, "text": " to be?", "tokens": [281, 312, 30], "temperature": 0.0, "avg_logprob": -0.18211261561659517, "compression_ratio": 1.6896551724137931, "no_speech_prob": 3.6119756714469986e-06}, {"id": 740, "seek": 356208, "start": 3569.08, "end": 3572.2, "text": " Because it's always easy to justify to yourself after the fact, oh I knew it would look like", "tokens": [1436, 309, 311, 1009, 1858, 281, 20833, 281, 1803, 934, 264, 1186, 11, 1954, 286, 2586, 309, 576, 574, 411], "temperature": 0.0, "avg_logprob": -0.18211261561659517, "compression_ratio": 1.6896551724137931, "no_speech_prob": 3.6119756714469986e-06}, {"id": 741, "seek": 356208, "start": 3572.2, "end": 3573.2, "text": " this.", "tokens": [341, 13], "temperature": 0.0, "avg_logprob": -0.18211261561659517, "compression_ratio": 1.6896551724137931, "no_speech_prob": 3.6119756714469986e-06}, {"id": 742, "seek": 356208, "start": 3573.2, "end": 3575.72, "text": " So what shape do you expect, and then is it that shape?", "tokens": [407, 437, 3909, 360, 291, 2066, 11, 293, 550, 307, 309, 300, 3909, 30], "temperature": 0.0, "avg_logprob": -0.18211261561659517, "compression_ratio": 1.6896551724137931, "no_speech_prob": 3.6119756714469986e-06}, {"id": 743, "seek": 356208, "start": 3575.72, "end": 3581.04, "text": " So in this case I'd be like, yeah, this is what I would expect.", "tokens": [407, 294, 341, 1389, 286, 1116, 312, 411, 11, 1338, 11, 341, 307, 437, 286, 576, 2066, 13], "temperature": 0.0, "avg_logprob": -0.18211261561659517, "compression_ratio": 1.6896551724137931, "no_speech_prob": 3.6119756714469986e-06}, {"id": 744, "seek": 356208, "start": 3581.04, "end": 3583.24, "text": " Where else?", "tokens": [2305, 1646, 30], "temperature": 0.0, "avg_logprob": -0.18211261561659517, "compression_ratio": 1.6896551724137931, "no_speech_prob": 3.6119756714469986e-06}, {"id": 745, "seek": 356208, "start": 3583.24, "end": 3585.86, "text": " This is definitely not what I'd expect.", "tokens": [639, 307, 2138, 406, 437, 286, 1116, 2066, 13], "temperature": 0.0, "avg_logprob": -0.18211261561659517, "compression_ratio": 1.6896551724137931, "no_speech_prob": 3.6119756714469986e-06}, {"id": 746, "seek": 356208, "start": 3585.86, "end": 3590.08, "text": " So the partial dependence plot has really pulled out the underlying truth.", "tokens": [407, 264, 14641, 31704, 7542, 575, 534, 7373, 484, 264, 14217, 3494, 13], "temperature": 0.0, "avg_logprob": -0.18211261561659517, "compression_ratio": 1.6896551724137931, "no_speech_prob": 3.6119756714469986e-06}, {"id": 747, "seek": 359008, "start": 3590.08, "end": 3598.88, "text": " Does anybody have any questions about why we use partial dependence or how we calculate", "tokens": [4402, 4472, 362, 604, 1651, 466, 983, 321, 764, 14641, 31704, 420, 577, 321, 8873], "temperature": 0.0, "avg_logprob": -0.4176624615987142, "compression_ratio": 1.169811320754717, "no_speech_prob": 2.9772574634989724e-05}, {"id": 748, "seek": 359008, "start": 3598.88, "end": 3599.88, "text": " it?", "tokens": [309, 30], "temperature": 0.0, "avg_logprob": -0.4176624615987142, "compression_ratio": 1.169811320754717, "no_speech_prob": 2.9772574634989724e-05}, {"id": 749, "seek": 359008, "start": 3599.88, "end": 3604.44, "text": " Who's got the, oh you've got it.", "tokens": [2102, 311, 658, 264, 11, 1954, 291, 600, 658, 309, 13], "temperature": 0.0, "avg_logprob": -0.4176624615987142, "compression_ratio": 1.169811320754717, "no_speech_prob": 2.9772574634989724e-05}, {"id": 750, "seek": 360444, "start": 3604.44, "end": 3627.16, "text": " If there are 20 features that are important, then I will do the partial dependence for", "tokens": [759, 456, 366, 945, 4122, 300, 366, 1021, 11, 550, 286, 486, 360, 264, 14641, 31704, 337], "temperature": 0.0, "avg_logprob": -0.164619074927436, "compression_ratio": 1.1511627906976745, "no_speech_prob": 5.507556124939583e-06}, {"id": 751, "seek": 360444, "start": 3627.16, "end": 3628.56, "text": " all of them.", "tokens": [439, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.164619074927436, "compression_ratio": 1.1511627906976745, "no_speech_prob": 5.507556124939583e-06}, {"id": 752, "seek": 362856, "start": 3628.56, "end": 3636.36, "text": " Where important means like, it's a lever I can actually pull, it's like the magnitude", "tokens": [2305, 1021, 1355, 411, 11, 309, 311, 257, 12451, 286, 393, 767, 2235, 11, 309, 311, 411, 264, 15668], "temperature": 0.0, "avg_logprob": -0.17886874986731488, "compression_ratio": 1.592760180995475, "no_speech_prob": 1.618747046450153e-05}, {"id": 753, "seek": 362856, "start": 3636.36, "end": 3640.08, "text": " of its size is not much smaller than the other 19.", "tokens": [295, 1080, 2744, 307, 406, 709, 4356, 813, 264, 661, 1294, 13], "temperature": 0.0, "avg_logprob": -0.17886874986731488, "compression_ratio": 1.592760180995475, "no_speech_prob": 1.618747046450153e-05}, {"id": 754, "seek": 362856, "start": 3640.08, "end": 3645.0, "text": " Based on all of these things, it's like yeah, it's a feature I ought to care about, then", "tokens": [18785, 322, 439, 295, 613, 721, 11, 309, 311, 411, 1338, 11, 309, 311, 257, 4111, 286, 13416, 281, 1127, 466, 11, 550], "temperature": 0.0, "avg_logprob": -0.17886874986731488, "compression_ratio": 1.592760180995475, "no_speech_prob": 1.618747046450153e-05}, {"id": 755, "seek": 362856, "start": 3645.0, "end": 3648.48, "text": " I will want to know how it's related.", "tokens": [286, 486, 528, 281, 458, 577, 309, 311, 4077, 13], "temperature": 0.0, "avg_logprob": -0.17886874986731488, "compression_ratio": 1.592760180995475, "no_speech_prob": 1.618747046450153e-05}, {"id": 756, "seek": 362856, "start": 3648.48, "end": 3655.4, "text": " It's pretty unusual to have that many features that are important both operationally and", "tokens": [467, 311, 1238, 10901, 281, 362, 300, 867, 4122, 300, 366, 1021, 1293, 6916, 379, 293], "temperature": 0.0, "avg_logprob": -0.17886874986731488, "compression_ratio": 1.592760180995475, "no_speech_prob": 1.618747046450153e-05}, {"id": 757, "seek": 365540, "start": 3655.4, "end": 3664.88, "text": " from a modeling point of view in my experience.", "tokens": [490, 257, 15983, 935, 295, 1910, 294, 452, 1752, 13], "temperature": 0.0, "avg_logprob": -0.14667373895645142, "compression_ratio": 1.247787610619469, "no_speech_prob": 7.527958587161265e-06}, {"id": 758, "seek": 365540, "start": 3664.88, "end": 3679.96, "text": " So important means it's a lever, so it's something I can change, and it's like kind of at the", "tokens": [407, 1021, 1355, 309, 311, 257, 12451, 11, 370, 309, 311, 746, 286, 393, 1319, 11, 293, 309, 311, 411, 733, 295, 412, 264], "temperature": 0.0, "avg_logprob": -0.14667373895645142, "compression_ratio": 1.247787610619469, "no_speech_prob": 7.527958587161265e-06}, {"id": 759, "seek": 367996, "start": 3679.96, "end": 3687.44, "text": " spiky end of this tale.", "tokens": [637, 1035, 88, 917, 295, 341, 17172, 13], "temperature": 0.0, "avg_logprob": -0.1450316270192464, "compression_ratio": 1.412162162162162, "no_speech_prob": 1.723145214782562e-05}, {"id": 760, "seek": 367996, "start": 3687.44, "end": 3694.84, "text": " Or maybe it's not a lever directly, maybe it's like zip code, and I can't actually tell", "tokens": [1610, 1310, 309, 311, 406, 257, 12451, 3838, 11, 1310, 309, 311, 411, 20730, 3089, 11, 293, 286, 393, 380, 767, 980], "temperature": 0.0, "avg_logprob": -0.1450316270192464, "compression_ratio": 1.412162162162162, "no_speech_prob": 1.723145214782562e-05}, {"id": 761, "seek": 367996, "start": 3694.84, "end": 3701.56, "text": " my customers where to live, but I could focus my new marketing attention on a different", "tokens": [452, 4581, 689, 281, 1621, 11, 457, 286, 727, 1879, 452, 777, 6370, 3202, 322, 257, 819], "temperature": 0.0, "avg_logprob": -0.1450316270192464, "compression_ratio": 1.412162162162162, "no_speech_prob": 1.723145214782562e-05}, {"id": 762, "seek": 367996, "start": 3701.56, "end": 3705.52, "text": " zip code.", "tokens": [20730, 3089, 13], "temperature": 0.0, "avg_logprob": -0.1450316270192464, "compression_ratio": 1.412162162162162, "no_speech_prob": 1.723145214782562e-05}, {"id": 763, "seek": 370552, "start": 3705.52, "end": 3712.16, "text": " Would it make sense to do pairwise shuffling for every combination of two features and", "tokens": [6068, 309, 652, 2020, 281, 360, 6119, 3711, 402, 1245, 1688, 337, 633, 6562, 295, 732, 4122, 293], "temperature": 0.0, "avg_logprob": -0.15564615298540163, "compression_ratio": 1.6536585365853658, "no_speech_prob": 2.668803426786326e-05}, {"id": 764, "seek": 370552, "start": 3712.16, "end": 3716.7599999999998, "text": " hold everything else constant like in feature importance to see interactions and compare", "tokens": [1797, 1203, 1646, 5754, 411, 294, 4111, 7379, 281, 536, 13280, 293, 6794], "temperature": 0.0, "avg_logprob": -0.15564615298540163, "compression_ratio": 1.6536585365853658, "no_speech_prob": 2.668803426786326e-05}, {"id": 765, "seek": 370552, "start": 3716.7599999999998, "end": 3720.56, "text": " scores?", "tokens": [13444, 30], "temperature": 0.0, "avg_logprob": -0.15564615298540163, "compression_ratio": 1.6536585365853658, "no_speech_prob": 2.668803426786326e-05}, {"id": 766, "seek": 370552, "start": 3720.56, "end": 3724.96, "text": " So you wouldn't do that so much for partial dependence.", "tokens": [407, 291, 2759, 380, 360, 300, 370, 709, 337, 14641, 31704, 13], "temperature": 0.0, "avg_logprob": -0.15564615298540163, "compression_ratio": 1.6536585365853658, "no_speech_prob": 2.668803426786326e-05}, {"id": 767, "seek": 370552, "start": 3724.96, "end": 3731.16, "text": " I think your question is really getting to the question of could we do that for feature", "tokens": [286, 519, 428, 1168, 307, 534, 1242, 281, 264, 1168, 295, 727, 321, 360, 300, 337, 4111], "temperature": 0.0, "avg_logprob": -0.15564615298540163, "compression_ratio": 1.6536585365853658, "no_speech_prob": 2.668803426786326e-05}, {"id": 768, "seek": 370552, "start": 3731.16, "end": 3733.6, "text": " importance.", "tokens": [7379, 13], "temperature": 0.0, "avg_logprob": -0.15564615298540163, "compression_ratio": 1.6536585365853658, "no_speech_prob": 2.668803426786326e-05}, {"id": 769, "seek": 373360, "start": 3733.6, "end": 3741.4, "text": " So I think interaction feature importance is a very important and interesting question.", "tokens": [407, 286, 519, 9285, 4111, 7379, 307, 257, 588, 1021, 293, 1880, 1168, 13], "temperature": 0.0, "avg_logprob": -0.12686713118302195, "compression_ratio": 1.5911330049261083, "no_speech_prob": 3.256307650190138e-07}, {"id": 770, "seek": 373360, "start": 3741.4, "end": 3750.2799999999997, "text": " But doing it by randomly shuffling every pair of columns, if you've got 100 columns, sounds", "tokens": [583, 884, 309, 538, 16979, 402, 1245, 1688, 633, 6119, 295, 13766, 11, 498, 291, 600, 658, 2319, 13766, 11, 3263], "temperature": 0.0, "avg_logprob": -0.12686713118302195, "compression_ratio": 1.5911330049261083, "no_speech_prob": 3.256307650190138e-07}, {"id": 771, "seek": 373360, "start": 3750.2799999999997, "end": 3754.08, "text": " computationally intensive, possibly infeasible.", "tokens": [24903, 379, 18957, 11, 6264, 1536, 68, 296, 964, 13], "temperature": 0.0, "avg_logprob": -0.12686713118302195, "compression_ratio": 1.5911330049261083, "no_speech_prob": 3.256307650190138e-07}, {"id": 772, "seek": 373360, "start": 3754.08, "end": 3759.3199999999997, "text": " So what I'm going to do is after we talk about tree interpreter, I'll talk about an interesting", "tokens": [407, 437, 286, 478, 516, 281, 360, 307, 934, 321, 751, 466, 4230, 34132, 11, 286, 603, 751, 466, 364, 1880], "temperature": 0.0, "avg_logprob": -0.12686713118302195, "compression_ratio": 1.5911330049261083, "no_speech_prob": 3.256307650190138e-07}, {"id": 773, "seek": 375932, "start": 3759.32, "end": 3765.52, "text": " but largely unexplored approach that will probably work.", "tokens": [457, 11611, 11572, 564, 2769, 3109, 300, 486, 1391, 589, 13], "temperature": 0.0, "avg_logprob": -0.3173495433369621, "compression_ratio": 1.475609756097561, "no_speech_prob": 5.390979640651494e-05}, {"id": 774, "seek": 375932, "start": 3765.52, "end": 3769.36, "text": " Who wants to do tree interpreter?", "tokens": [2102, 2738, 281, 360, 4230, 34132, 30], "temperature": 0.0, "avg_logprob": -0.3173495433369621, "compression_ratio": 1.475609756097561, "no_speech_prob": 5.390979640651494e-05}, {"id": 775, "seek": 375932, "start": 3769.36, "end": 3771.36, "text": " Alright over here, Prince.", "tokens": [2798, 670, 510, 11, 9821, 13], "temperature": 0.0, "avg_logprob": -0.3173495433369621, "compression_ratio": 1.475609756097561, "no_speech_prob": 5.390979640651494e-05}, {"id": 776, "seek": 375932, "start": 3771.36, "end": 3779.6800000000003, "text": " Can you pass that over here to Prince?", "tokens": [1664, 291, 1320, 300, 670, 510, 281, 9821, 30], "temperature": 0.0, "avg_logprob": -0.3173495433369621, "compression_ratio": 1.475609756097561, "no_speech_prob": 5.390979640651494e-05}, {"id": 777, "seek": 375932, "start": 3779.6800000000003, "end": 3784.1600000000003, "text": " I was thinking this to be more like feature importance, but feature importance is for", "tokens": [286, 390, 1953, 341, 281, 312, 544, 411, 4111, 7379, 11, 457, 4111, 7379, 307, 337], "temperature": 0.0, "avg_logprob": -0.3173495433369621, "compression_ratio": 1.475609756097561, "no_speech_prob": 5.390979640651494e-05}, {"id": 778, "seek": 378416, "start": 3784.16, "end": 3789.48, "text": " complete random forest model and this tree interpreter is for feature importance for", "tokens": [3566, 4974, 6719, 2316, 293, 341, 4230, 34132, 307, 337, 4111, 7379, 337], "temperature": 0.0, "avg_logprob": -0.12266769011815389, "compression_ratio": 1.7758620689655173, "no_speech_prob": 1.8341443137614988e-05}, {"id": 779, "seek": 378416, "start": 3789.48, "end": 3790.92, "text": " particular observation.", "tokens": [1729, 14816, 13], "temperature": 0.0, "avg_logprob": -0.12266769011815389, "compression_ratio": 1.7758620689655173, "no_speech_prob": 1.8341443137614988e-05}, {"id": 780, "seek": 378416, "start": 3790.92, "end": 3795.48, "text": " So if that, let's say it's about hospital readmission.", "tokens": [407, 498, 300, 11, 718, 311, 584, 309, 311, 466, 4530, 1401, 29797, 13], "temperature": 0.0, "avg_logprob": -0.12266769011815389, "compression_ratio": 1.7758620689655173, "no_speech_prob": 1.8341443137614988e-05}, {"id": 781, "seek": 378416, "start": 3795.48, "end": 3802.44, "text": " So if a patient A1 is going to be readmitted to a hospital, which feature for that particular", "tokens": [407, 498, 257, 4537, 316, 16, 307, 516, 281, 312, 1401, 76, 3944, 281, 257, 4530, 11, 597, 4111, 337, 300, 1729], "temperature": 0.0, "avg_logprob": -0.12266769011815389, "compression_ratio": 1.7758620689655173, "no_speech_prob": 1.8341443137614988e-05}, {"id": 782, "seek": 378416, "start": 3802.44, "end": 3807.44, "text": " patient is going to impact and how can we change that?", "tokens": [4537, 307, 516, 281, 2712, 293, 577, 393, 321, 1319, 300, 30], "temperature": 0.0, "avg_logprob": -0.12266769011815389, "compression_ratio": 1.7758620689655173, "no_speech_prob": 1.8341443137614988e-05}, {"id": 783, "seek": 378416, "start": 3807.44, "end": 3814.0, "text": " And it is calculated starting from the prediction of mean, then seeing how each feature is changing", "tokens": [400, 309, 307, 15598, 2891, 490, 264, 17630, 295, 914, 11, 550, 2577, 577, 1184, 4111, 307, 4473], "temperature": 0.0, "avg_logprob": -0.12266769011815389, "compression_ratio": 1.7758620689655173, "no_speech_prob": 1.8341443137614988e-05}, {"id": 784, "seek": 381400, "start": 3814.0, "end": 3817.64, "text": " the behavior of that particular patient.", "tokens": [264, 5223, 295, 300, 1729, 4537, 13], "temperature": 0.0, "avg_logprob": -0.11529346791709341, "compression_ratio": 1.5740740740740742, "no_speech_prob": 7.889202606747858e-06}, {"id": 785, "seek": 381400, "start": 3817.64, "end": 3823.8, "text": " I'm smiling because that was one of the best examples of technical communication I've heard", "tokens": [286, 478, 16005, 570, 300, 390, 472, 295, 264, 1151, 5110, 295, 6191, 6101, 286, 600, 2198], "temperature": 0.0, "avg_logprob": -0.11529346791709341, "compression_ratio": 1.5740740740740742, "no_speech_prob": 7.889202606747858e-06}, {"id": 786, "seek": 381400, "start": 3823.8, "end": 3824.8, "text": " in a long time.", "tokens": [294, 257, 938, 565, 13], "temperature": 0.0, "avg_logprob": -0.11529346791709341, "compression_ratio": 1.5740740740740742, "no_speech_prob": 7.889202606747858e-06}, {"id": 787, "seek": 381400, "start": 3824.8, "end": 3829.28, "text": " So it's really good to think about why was that effective.", "tokens": [407, 309, 311, 534, 665, 281, 519, 466, 983, 390, 300, 4942, 13], "temperature": 0.0, "avg_logprob": -0.11529346791709341, "compression_ratio": 1.5740740740740742, "no_speech_prob": 7.889202606747858e-06}, {"id": 788, "seek": 381400, "start": 3829.28, "end": 3835.92, "text": " So what Prince did there was he used as specific an example as possible.", "tokens": [407, 437, 9821, 630, 456, 390, 415, 1143, 382, 2685, 364, 1365, 382, 1944, 13], "temperature": 0.0, "avg_logprob": -0.11529346791709341, "compression_ratio": 1.5740740740740742, "no_speech_prob": 7.889202606747858e-06}, {"id": 789, "seek": 381400, "start": 3835.92, "end": 3839.84, "text": " So humans are much less good at understanding abstractions.", "tokens": [407, 6255, 366, 709, 1570, 665, 412, 3701, 12649, 626, 13], "temperature": 0.0, "avg_logprob": -0.11529346791709341, "compression_ratio": 1.5740740740740742, "no_speech_prob": 7.889202606747858e-06}, {"id": 790, "seek": 383984, "start": 3839.84, "end": 3845.1600000000003, "text": " So if you kind of say, oh it takes some kind of feature and then there's an observation", "tokens": [407, 498, 291, 733, 295, 584, 11, 1954, 309, 2516, 512, 733, 295, 4111, 293, 550, 456, 311, 364, 14816], "temperature": 0.0, "avg_logprob": -0.16085902127352628, "compression_ratio": 1.7468879668049793, "no_speech_prob": 4.157345301791793e-06}, {"id": 791, "seek": 383984, "start": 3845.1600000000003, "end": 3849.84, "text": " in that feature, it's a hospital readmission.", "tokens": [294, 300, 4111, 11, 309, 311, 257, 4530, 1401, 29797, 13], "temperature": 0.0, "avg_logprob": -0.16085902127352628, "compression_ratio": 1.7468879668049793, "no_speech_prob": 4.157345301791793e-06}, {"id": 792, "seek": 383984, "start": 3849.84, "end": 3853.1200000000003, "text": " So we take a specific example.", "tokens": [407, 321, 747, 257, 2685, 1365, 13], "temperature": 0.0, "avg_logprob": -0.16085902127352628, "compression_ratio": 1.7468879668049793, "no_speech_prob": 4.157345301791793e-06}, {"id": 793, "seek": 383984, "start": 3853.1200000000003, "end": 3857.6400000000003, "text": " The other thing he did which was very effective was to kind of take an analogy to something", "tokens": [440, 661, 551, 415, 630, 597, 390, 588, 4942, 390, 281, 733, 295, 747, 364, 21663, 281, 746], "temperature": 0.0, "avg_logprob": -0.16085902127352628, "compression_ratio": 1.7468879668049793, "no_speech_prob": 4.157345301791793e-06}, {"id": 794, "seek": 383984, "start": 3857.6400000000003, "end": 3858.98, "text": " we already understand.", "tokens": [321, 1217, 1223, 13], "temperature": 0.0, "avg_logprob": -0.16085902127352628, "compression_ratio": 1.7468879668049793, "no_speech_prob": 4.157345301791793e-06}, {"id": 795, "seek": 383984, "start": 3858.98, "end": 3864.76, "text": " So we already understand the idea of feature importance across all of the rows in the data", "tokens": [407, 321, 1217, 1223, 264, 1558, 295, 4111, 7379, 2108, 439, 295, 264, 13241, 294, 264, 1412], "temperature": 0.0, "avg_logprob": -0.16085902127352628, "compression_ratio": 1.7468879668049793, "no_speech_prob": 4.157345301791793e-06}, {"id": 796, "seek": 383984, "start": 3864.76, "end": 3866.02, "text": " set.", "tokens": [992, 13], "temperature": 0.0, "avg_logprob": -0.16085902127352628, "compression_ratio": 1.7468879668049793, "no_speech_prob": 4.157345301791793e-06}, {"id": 797, "seek": 383984, "start": 3866.02, "end": 3869.8, "text": " So now we're going to do it for a single row.", "tokens": [407, 586, 321, 434, 516, 281, 360, 309, 337, 257, 2167, 5386, 13], "temperature": 0.0, "avg_logprob": -0.16085902127352628, "compression_ratio": 1.7468879668049793, "no_speech_prob": 4.157345301791793e-06}, {"id": 798, "seek": 386980, "start": 3869.8, "end": 3875.44, "text": " So one of the things I was really hoping we would learn from this experience is how to", "tokens": [407, 472, 295, 264, 721, 286, 390, 534, 7159, 321, 576, 1466, 490, 341, 1752, 307, 577, 281], "temperature": 0.0, "avg_logprob": -0.08844487865765889, "compression_ratio": 1.777292576419214, "no_speech_prob": 1.9833232727251016e-05}, {"id": 799, "seek": 386980, "start": 3875.44, "end": 3878.6800000000003, "text": " become effective technical communicators.", "tokens": [1813, 4942, 6191, 3363, 3391, 13], "temperature": 0.0, "avg_logprob": -0.08844487865765889, "compression_ratio": 1.777292576419214, "no_speech_prob": 1.9833232727251016e-05}, {"id": 800, "seek": 386980, "start": 3878.6800000000003, "end": 3884.8, "text": " So that was a really great role model from Prince of using all of the tricks we have", "tokens": [407, 300, 390, 257, 534, 869, 3090, 2316, 490, 9821, 295, 1228, 439, 295, 264, 11733, 321, 362], "temperature": 0.0, "avg_logprob": -0.08844487865765889, "compression_ratio": 1.777292576419214, "no_speech_prob": 1.9833232727251016e-05}, {"id": 801, "seek": 386980, "start": 3884.8, "end": 3888.02, "text": " at our disposal for effective technical communication.", "tokens": [412, 527, 26400, 337, 4942, 6191, 6101, 13], "temperature": 0.0, "avg_logprob": -0.08844487865765889, "compression_ratio": 1.777292576419214, "no_speech_prob": 1.9833232727251016e-05}, {"id": 802, "seek": 386980, "start": 3888.02, "end": 3890.36, "text": " So hopefully you found that a useful explanation.", "tokens": [407, 4696, 291, 1352, 300, 257, 4420, 10835, 13], "temperature": 0.0, "avg_logprob": -0.08844487865765889, "compression_ratio": 1.777292576419214, "no_speech_prob": 1.9833232727251016e-05}, {"id": 803, "seek": 386980, "start": 3890.36, "end": 3897.0, "text": " I don't have a hell of a lot to add to that other than to show you what that looks like.", "tokens": [286, 500, 380, 362, 257, 4921, 295, 257, 688, 281, 909, 281, 300, 661, 813, 281, 855, 291, 437, 300, 1542, 411, 13], "temperature": 0.0, "avg_logprob": -0.08844487865765889, "compression_ratio": 1.777292576419214, "no_speech_prob": 1.9833232727251016e-05}, {"id": 804, "seek": 389700, "start": 3897.0, "end": 3903.4, "text": " So with the tree interpreter, we picked out a row.", "tokens": [407, 365, 264, 4230, 34132, 11, 321, 6183, 484, 257, 5386, 13], "temperature": 0.0, "avg_logprob": -0.12926920070204623, "compression_ratio": 1.6581632653061225, "no_speech_prob": 3.0415856144827558e-06}, {"id": 805, "seek": 389700, "start": 3903.4, "end": 3910.92, "text": " And so remember when we talked about the confidence intervals at the very start, the confidence", "tokens": [400, 370, 1604, 562, 321, 2825, 466, 264, 6687, 26651, 412, 264, 588, 722, 11, 264, 6687], "temperature": 0.0, "avg_logprob": -0.12926920070204623, "compression_ratio": 1.6581632653061225, "no_speech_prob": 3.0415856144827558e-06}, {"id": 806, "seek": 389700, "start": 3910.92, "end": 3915.72, "text": " based on tree variance, we mainly said you'd probably mainly use that for a row.", "tokens": [2361, 322, 4230, 21977, 11, 321, 8704, 848, 291, 1116, 1391, 8704, 764, 300, 337, 257, 5386, 13], "temperature": 0.0, "avg_logprob": -0.12926920070204623, "compression_ratio": 1.6581632653061225, "no_speech_prob": 3.0415856144827558e-06}, {"id": 807, "seek": 389700, "start": 3915.72, "end": 3917.6, "text": " So this would also be for a row.", "tokens": [407, 341, 576, 611, 312, 337, 257, 5386, 13], "temperature": 0.0, "avg_logprob": -0.12926920070204623, "compression_ratio": 1.6581632653061225, "no_speech_prob": 3.0415856144827558e-06}, {"id": 808, "seek": 389700, "start": 3917.6, "end": 3923.88, "text": " So it's like, okay, why is this patient likely to be readmitted?", "tokens": [407, 309, 311, 411, 11, 1392, 11, 983, 307, 341, 4537, 3700, 281, 312, 1401, 76, 3944, 30], "temperature": 0.0, "avg_logprob": -0.12926920070204623, "compression_ratio": 1.6581632653061225, "no_speech_prob": 3.0415856144827558e-06}, {"id": 809, "seek": 392388, "start": 3923.88, "end": 3930.56, "text": " So here is all of the information we have about that patient, or in this case this auction.", "tokens": [407, 510, 307, 439, 295, 264, 1589, 321, 362, 466, 300, 4537, 11, 420, 294, 341, 1389, 341, 24139, 13], "temperature": 0.0, "avg_logprob": -0.12422952344340663, "compression_ratio": 1.683168316831683, "no_speech_prob": 2.7693981792253908e-06}, {"id": 810, "seek": 392388, "start": 3930.56, "end": 3935.2200000000003, "text": " Why is this auction so expensive?", "tokens": [1545, 307, 341, 24139, 370, 5124, 30], "temperature": 0.0, "avg_logprob": -0.12422952344340663, "compression_ratio": 1.683168316831683, "no_speech_prob": 2.7693981792253908e-06}, {"id": 811, "seek": 392388, "start": 3935.2200000000003, "end": 3941.48, "text": " So then we call treeinterpreter.predict and we get back the prediction of the price, the", "tokens": [407, 550, 321, 818, 4230, 5106, 3712, 391, 13, 79, 24945, 293, 321, 483, 646, 264, 17630, 295, 264, 3218, 11, 264], "temperature": 0.0, "avg_logprob": -0.12422952344340663, "compression_ratio": 1.683168316831683, "no_speech_prob": 2.7693981792253908e-06}, {"id": 812, "seek": 392388, "start": 3941.48, "end": 3944.6800000000003, "text": " bias, which is the root of the tree.", "tokens": [12577, 11, 597, 307, 264, 5593, 295, 264, 4230, 13], "temperature": 0.0, "avg_logprob": -0.12422952344340663, "compression_ratio": 1.683168316831683, "no_speech_prob": 2.7693981792253908e-06}, {"id": 813, "seek": 392388, "start": 3944.6800000000003, "end": 3947.08, "text": " So this is just the average price for everybody.", "tokens": [407, 341, 307, 445, 264, 4274, 3218, 337, 2201, 13], "temperature": 0.0, "avg_logprob": -0.12422952344340663, "compression_ratio": 1.683168316831683, "no_speech_prob": 2.7693981792253908e-06}, {"id": 814, "seek": 392388, "start": 3947.08, "end": 3949.6400000000003, "text": " So this is always going to be the same.", "tokens": [407, 341, 307, 1009, 516, 281, 312, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.12422952344340663, "compression_ratio": 1.683168316831683, "no_speech_prob": 2.7693981792253908e-06}, {"id": 815, "seek": 394964, "start": 3949.64, "end": 3959.08, "text": " And then the contributions, which is how important is each of these things.", "tokens": [400, 550, 264, 15725, 11, 597, 307, 577, 1021, 307, 1184, 295, 613, 721, 13], "temperature": 0.0, "avg_logprob": -0.15759753017890743, "compression_ratio": 1.3697478991596639, "no_speech_prob": 8.398051249969285e-06}, {"id": 816, "seek": 394964, "start": 3959.08, "end": 3974.7999999999997, "text": " And so the way we calculated that was to say, okay, at the very start the average price", "tokens": [400, 370, 264, 636, 321, 15598, 300, 390, 281, 584, 11, 1392, 11, 412, 264, 588, 722, 264, 4274, 3218], "temperature": 0.0, "avg_logprob": -0.15759753017890743, "compression_ratio": 1.3697478991596639, "no_speech_prob": 8.398051249969285e-06}, {"id": 817, "seek": 397480, "start": 3974.8, "end": 3981.84, "text": " was 10, and then we split on enclosure.", "tokens": [390, 1266, 11, 293, 550, 321, 7472, 322, 34093, 13], "temperature": 0.0, "avg_logprob": -0.13902722299098969, "compression_ratio": 1.6504065040650406, "no_speech_prob": 1.5689483916503377e-05}, {"id": 818, "seek": 397480, "start": 3981.84, "end": 3989.48, "text": " And for those with disenclosure, the average was 9.5.", "tokens": [400, 337, 729, 365, 717, 268, 3474, 7641, 11, 264, 4274, 390, 1722, 13, 20, 13], "temperature": 0.0, "avg_logprob": -0.13902722299098969, "compression_ratio": 1.6504065040650406, "no_speech_prob": 1.5689483916503377e-05}, {"id": 819, "seek": 397480, "start": 3989.48, "end": 3995.52, "text": " And then we split on year made, less than 1990, and for those with that year made, the", "tokens": [400, 550, 321, 7472, 322, 1064, 1027, 11, 1570, 813, 13384, 11, 293, 337, 729, 365, 300, 1064, 1027, 11, 264], "temperature": 0.0, "avg_logprob": -0.13902722299098969, "compression_ratio": 1.6504065040650406, "no_speech_prob": 1.5689483916503377e-05}, {"id": 820, "seek": 397480, "start": 3995.52, "end": 4000.48, "text": " average price was 9.7.", "tokens": [4274, 3218, 390, 1722, 13, 22, 13], "temperature": 0.0, "avg_logprob": -0.13902722299098969, "compression_ratio": 1.6504065040650406, "no_speech_prob": 1.5689483916503377e-05}, {"id": 821, "seek": 400048, "start": 4000.48, "end": 4011.72, "text": " And then we split on the number of hours on the meter, and for this branch we got 9.4.", "tokens": [400, 550, 321, 7472, 322, 264, 1230, 295, 2496, 322, 264, 9255, 11, 293, 337, 341, 9819, 321, 658, 1722, 13, 19, 13], "temperature": 0.0, "avg_logprob": -0.10366325245963202, "compression_ratio": 1.5783132530120483, "no_speech_prob": 3.844922048301669e-06}, {"id": 822, "seek": 400048, "start": 4011.72, "end": 4018.08, "text": " And so we then have a particular auction, which we pass it through the tree and it just", "tokens": [400, 370, 321, 550, 362, 257, 1729, 24139, 11, 597, 321, 1320, 309, 807, 264, 4230, 293, 309, 445], "temperature": 0.0, "avg_logprob": -0.10366325245963202, "compression_ratio": 1.5783132530120483, "no_speech_prob": 3.844922048301669e-06}, {"id": 823, "seek": 400048, "start": 4018.08, "end": 4021.56, "text": " so happens that it takes this path.", "tokens": [370, 2314, 300, 309, 2516, 341, 3100, 13], "temperature": 0.0, "avg_logprob": -0.10366325245963202, "compression_ratio": 1.5783132530120483, "no_speech_prob": 3.844922048301669e-06}, {"id": 824, "seek": 400048, "start": 4021.56, "end": 4026.84, "text": " So one row can only have one path through the tree.", "tokens": [407, 472, 5386, 393, 787, 362, 472, 3100, 807, 264, 4230, 13], "temperature": 0.0, "avg_logprob": -0.10366325245963202, "compression_ratio": 1.5783132530120483, "no_speech_prob": 3.844922048301669e-06}, {"id": 825, "seek": 402684, "start": 4026.84, "end": 4031.2000000000003, "text": " And so we ended up at this point.", "tokens": [400, 370, 321, 4590, 493, 412, 341, 935, 13], "temperature": 0.0, "avg_logprob": -0.18927527145600656, "compression_ratio": 1.4331210191082802, "no_speech_prob": 1.0451518392073922e-05}, {"id": 826, "seek": 402684, "start": 4031.2000000000003, "end": 4037.8, "text": " So then we can create a little table.", "tokens": [407, 550, 321, 393, 1884, 257, 707, 3199, 13], "temperature": 0.0, "avg_logprob": -0.18927527145600656, "compression_ratio": 1.4331210191082802, "no_speech_prob": 1.0451518392073922e-05}, {"id": 827, "seek": 402684, "start": 4037.8, "end": 4044.4, "text": " And so as we go through, we start at the top and we start with 10, that's our bias, and", "tokens": [400, 370, 382, 321, 352, 807, 11, 321, 722, 412, 264, 1192, 293, 321, 722, 365, 1266, 11, 300, 311, 527, 12577, 11, 293], "temperature": 0.0, "avg_logprob": -0.18927527145600656, "compression_ratio": 1.4331210191082802, "no_speech_prob": 1.0451518392073922e-05}, {"id": 828, "seek": 402684, "start": 4044.4, "end": 4051.96, "text": " we said enclosure resulted in a change from 10 to 9.5, minus 0.5.", "tokens": [321, 848, 34093, 18753, 294, 257, 1319, 490, 1266, 281, 1722, 13, 20, 11, 3175, 1958, 13, 20, 13], "temperature": 0.0, "avg_logprob": -0.18927527145600656, "compression_ratio": 1.4331210191082802, "no_speech_prob": 1.0451518392073922e-05}, {"id": 829, "seek": 405196, "start": 4051.96, "end": 4059.44, "text": " Year made changed it from 9.5 to 9.7, so plus 0.2.", "tokens": [10289, 1027, 3105, 309, 490, 1722, 13, 20, 281, 1722, 13, 22, 11, 370, 1804, 1958, 13, 17, 13], "temperature": 0.0, "avg_logprob": -0.11544131055290316, "compression_ratio": 1.532846715328467, "no_speech_prob": 7.18324190529529e-06}, {"id": 830, "seek": 405196, "start": 4059.44, "end": 4071.4, "text": " And then meter changed it from 9.7 down to 9.4, which is minus 0.3.", "tokens": [400, 550, 9255, 3105, 309, 490, 1722, 13, 22, 760, 281, 1722, 13, 19, 11, 597, 307, 3175, 1958, 13, 18, 13], "temperature": 0.0, "avg_logprob": -0.11544131055290316, "compression_ratio": 1.532846715328467, "no_speech_prob": 7.18324190529529e-06}, {"id": 831, "seek": 405196, "start": 4071.4, "end": 4079.96, "text": " And then if we add all that together, 10 minus 1 half is 9.5, plus 0.2 is 9.7, minus 0.3 is", "tokens": [400, 550, 498, 321, 909, 439, 300, 1214, 11, 1266, 3175, 502, 1922, 307, 1722, 13, 20, 11, 1804, 1958, 13, 17, 307, 1722, 13, 22, 11, 3175, 1958, 13, 18, 307], "temperature": 0.0, "avg_logprob": -0.11544131055290316, "compression_ratio": 1.532846715328467, "no_speech_prob": 7.18324190529529e-06}, {"id": 832, "seek": 407996, "start": 4079.96, "end": 4095.48, "text": " 9.4, and lo and behold that's that number, which takes us to our Excel spreadsheet.", "tokens": [1722, 13, 19, 11, 293, 450, 293, 27234, 300, 311, 300, 1230, 11, 597, 2516, 505, 281, 527, 19060, 27733, 13], "temperature": 0.0, "avg_logprob": -0.37332816351027714, "compression_ratio": 1.182608695652174, "no_speech_prob": 3.2887430734263035e-06}, {"id": 833, "seek": 407996, "start": 4095.48, "end": 4100.92, "text": " Where's Chris, who did our waterfall?", "tokens": [2305, 311, 6688, 11, 567, 630, 527, 27848, 30], "temperature": 0.0, "avg_logprob": -0.37332816351027714, "compression_ratio": 1.182608695652174, "no_speech_prob": 3.2887430734263035e-06}, {"id": 834, "seek": 407996, "start": 4100.92, "end": 4102.12, "text": " There you are.", "tokens": [821, 291, 366, 13], "temperature": 0.0, "avg_logprob": -0.37332816351027714, "compression_ratio": 1.182608695652174, "no_speech_prob": 3.2887430734263035e-06}, {"id": 835, "seek": 410212, "start": 4102.12, "end": 4109.88, "text": " So last week we had to use Excel for this because there isn't a good Python library", "tokens": [407, 1036, 1243, 321, 632, 281, 764, 19060, 337, 341, 570, 456, 1943, 380, 257, 665, 15329, 6405], "temperature": 0.0, "avg_logprob": -0.0992153076898484, "compression_ratio": 1.670731707317073, "no_speech_prob": 6.6433995016268454e-06}, {"id": 836, "seek": 410212, "start": 4109.88, "end": 4111.44, "text": " for doing waterfall charts.", "tokens": [337, 884, 27848, 17767, 13], "temperature": 0.0, "avg_logprob": -0.0992153076898484, "compression_ratio": 1.670731707317073, "no_speech_prob": 6.6433995016268454e-06}, {"id": 837, "seek": 410212, "start": 4111.44, "end": 4117.04, "text": " And so we saw we got our starting point, this is the bias, and then we had each of our contributions", "tokens": [400, 370, 321, 1866, 321, 658, 527, 2891, 935, 11, 341, 307, 264, 12577, 11, 293, 550, 321, 632, 1184, 295, 527, 15725], "temperature": 0.0, "avg_logprob": -0.0992153076898484, "compression_ratio": 1.670731707317073, "no_speech_prob": 6.6433995016268454e-06}, {"id": 838, "seek": 410212, "start": 4117.04, "end": 4120.08, "text": " and we ended up with our total.", "tokens": [293, 321, 4590, 493, 365, 527, 3217, 13], "temperature": 0.0, "avg_logprob": -0.0992153076898484, "compression_ratio": 1.670731707317073, "no_speech_prob": 6.6433995016268454e-06}, {"id": 839, "seek": 410212, "start": 4120.08, "end": 4124.72, "text": " The world is now a better place because Chris has created a Python waterfall chart module", "tokens": [440, 1002, 307, 586, 257, 1101, 1081, 570, 6688, 575, 2942, 257, 15329, 27848, 6927, 10088], "temperature": 0.0, "avg_logprob": -0.0992153076898484, "compression_ratio": 1.670731707317073, "no_speech_prob": 6.6433995016268454e-06}, {"id": 840, "seek": 410212, "start": 4124.72, "end": 4130.34, "text": " for us and put it on PIC, so never again will we have to use Excel for this.", "tokens": [337, 505, 293, 829, 309, 322, 430, 2532, 11, 370, 1128, 797, 486, 321, 362, 281, 764, 19060, 337, 341, 13], "temperature": 0.0, "avg_logprob": -0.0992153076898484, "compression_ratio": 1.670731707317073, "no_speech_prob": 6.6433995016268454e-06}, {"id": 841, "seek": 413034, "start": 4130.34, "end": 4137.08, "text": " And I wanted to point out that waterfall charts have been very important in business communications", "tokens": [400, 286, 1415, 281, 935, 484, 300, 27848, 17767, 362, 668, 588, 1021, 294, 1606, 15163], "temperature": 0.0, "avg_logprob": -0.12937971440757193, "compression_ratio": 1.6037735849056605, "no_speech_prob": 3.6688488762592897e-06}, {"id": 842, "seek": 413034, "start": 4137.08, "end": 4144.14, "text": " at least as long as I've been in business, so that's about 25 years.", "tokens": [412, 1935, 382, 938, 382, 286, 600, 668, 294, 1606, 11, 370, 300, 311, 466, 3552, 924, 13], "temperature": 0.0, "avg_logprob": -0.12937971440757193, "compression_ratio": 1.6037735849056605, "no_speech_prob": 3.6688488762592897e-06}, {"id": 843, "seek": 413034, "start": 4144.14, "end": 4150.52, "text": " Python is a couple of decades old, a little bit less, maybe a couple of decades old.", "tokens": [15329, 307, 257, 1916, 295, 7878, 1331, 11, 257, 707, 857, 1570, 11, 1310, 257, 1916, 295, 7878, 1331, 13], "temperature": 0.0, "avg_logprob": -0.12937971440757193, "compression_ratio": 1.6037735849056605, "no_speech_prob": 3.6688488762592897e-06}, {"id": 844, "seek": 413034, "start": 4150.52, "end": 4157.4800000000005, "text": " But despite that, no one in the Python world ever got to the point where they actually", "tokens": [583, 7228, 300, 11, 572, 472, 294, 264, 15329, 1002, 1562, 658, 281, 264, 935, 689, 436, 767], "temperature": 0.0, "avg_logprob": -0.12937971440757193, "compression_ratio": 1.6037735849056605, "no_speech_prob": 3.6688488762592897e-06}, {"id": 845, "seek": 415748, "start": 4157.48, "end": 4160.32, "text": " thought you know I'm going to make a waterfall chart.", "tokens": [1194, 291, 458, 286, 478, 516, 281, 652, 257, 27848, 6927, 13], "temperature": 0.0, "avg_logprob": -0.23148396140650698, "compression_ratio": 1.627906976744186, "no_speech_prob": 7.183115030784393e-06}, {"id": 846, "seek": 415748, "start": 4160.32, "end": 4167.2, "text": " So they didn't exist until two days ago, which is to say like the world is full of stuff", "tokens": [407, 436, 994, 380, 2514, 1826, 732, 1708, 2057, 11, 597, 307, 281, 584, 411, 264, 1002, 307, 1577, 295, 1507], "temperature": 0.0, "avg_logprob": -0.23148396140650698, "compression_ratio": 1.627906976744186, "no_speech_prob": 7.183115030784393e-06}, {"id": 847, "seek": 415748, "start": 4167.2, "end": 4172.48, "text": " which ought to exist and doesn't, and doesn't necessarily take a lot of time to build.", "tokens": [597, 13416, 281, 2514, 293, 1177, 380, 11, 293, 1177, 380, 4725, 747, 257, 688, 295, 565, 281, 1322, 13], "temperature": 0.0, "avg_logprob": -0.23148396140650698, "compression_ratio": 1.627906976744186, "no_speech_prob": 7.183115030784393e-06}, {"id": 848, "seek": 415748, "start": 4172.48, "end": 4179.28, "text": " Chris, how long did it take you to build the first Python waterfall chart?", "tokens": [6688, 11, 577, 938, 630, 309, 747, 291, 281, 1322, 264, 700, 15329, 27848, 6927, 30], "temperature": 0.0, "avg_logprob": -0.23148396140650698, "compression_ratio": 1.627906976744186, "no_speech_prob": 7.183115030784393e-06}, {"id": 849, "seek": 415748, "start": 4179.28, "end": 4181.4, "text": " Well there was a you know a gist of it, yeah.", "tokens": [1042, 456, 390, 257, 291, 458, 257, 290, 468, 295, 309, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.23148396140650698, "compression_ratio": 1.627906976744186, "no_speech_prob": 7.183115030784393e-06}, {"id": 850, "seek": 418140, "start": 4181.4, "end": 4194.599999999999, "text": " About 8 hours, okay, so you know a hefty time amount but not unreasonable.", "tokens": [7769, 1649, 2496, 11, 1392, 11, 370, 291, 458, 257, 43674, 88, 565, 2372, 457, 406, 41730, 13], "temperature": 0.0, "avg_logprob": -0.2016983619103065, "compression_ratio": 1.377659574468085, "no_speech_prob": 6.540371941810008e-06}, {"id": 851, "seek": 418140, "start": 4194.599999999999, "end": 4199.44, "text": " And now forevermore, people when they want the Python waterfall chart will end up at", "tokens": [400, 586, 5680, 3138, 11, 561, 562, 436, 528, 264, 15329, 27848, 6927, 486, 917, 493, 412], "temperature": 0.0, "avg_logprob": -0.2016983619103065, "compression_ratio": 1.377659574468085, "no_speech_prob": 6.540371941810008e-06}, {"id": 852, "seek": 418140, "start": 4199.44, "end": 4204.879999999999, "text": " Chris's GitHub repo and hopefully find lots of other USF contributors who have made it", "tokens": [6688, 311, 23331, 49040, 293, 4696, 915, 3195, 295, 661, 2546, 37, 45627, 567, 362, 1027, 309], "temperature": 0.0, "avg_logprob": -0.2016983619103065, "compression_ratio": 1.377659574468085, "no_speech_prob": 6.540371941810008e-06}, {"id": 853, "seek": 418140, "start": 4204.879999999999, "end": 4208.04, "text": " even better.", "tokens": [754, 1101, 13], "temperature": 0.0, "avg_logprob": -0.2016983619103065, "compression_ratio": 1.377659574468085, "no_speech_prob": 6.540371941810008e-06}, {"id": 854, "seek": 420804, "start": 4208.04, "end": 4214.56, "text": " So in order for you to help improve Chris's Python waterfall, you need to know how to", "tokens": [407, 294, 1668, 337, 291, 281, 854, 3470, 6688, 311, 15329, 27848, 11, 291, 643, 281, 458, 577, 281], "temperature": 0.0, "avg_logprob": -0.15651406182183158, "compression_ratio": 1.5077720207253886, "no_speech_prob": 4.637851816369221e-06}, {"id": 855, "seek": 420804, "start": 4214.56, "end": 4215.76, "text": " do that.", "tokens": [360, 300, 13], "temperature": 0.0, "avg_logprob": -0.15651406182183158, "compression_ratio": 1.5077720207253886, "no_speech_prob": 4.637851816369221e-06}, {"id": 856, "seek": 420804, "start": 4215.76, "end": 4220.44, "text": " And so you're going to need to submit a pull request.", "tokens": [400, 370, 291, 434, 516, 281, 643, 281, 10315, 257, 2235, 5308, 13], "temperature": 0.0, "avg_logprob": -0.15651406182183158, "compression_ratio": 1.5077720207253886, "no_speech_prob": 4.637851816369221e-06}, {"id": 857, "seek": 420804, "start": 4220.44, "end": 4224.64, "text": " Life becomes very easy for submitting pull requests if you use something called hub.", "tokens": [7720, 3643, 588, 1858, 337, 31836, 2235, 12475, 498, 291, 764, 746, 1219, 11838, 13], "temperature": 0.0, "avg_logprob": -0.15651406182183158, "compression_ratio": 1.5077720207253886, "no_speech_prob": 4.637851816369221e-06}, {"id": 858, "seek": 420804, "start": 4224.64, "end": 4233.26, "text": " So if you go to github.hub, that will send you over here.", "tokens": [407, 498, 291, 352, 281, 290, 355, 836, 13, 71, 836, 11, 300, 486, 2845, 291, 670, 510, 13], "temperature": 0.0, "avg_logprob": -0.15651406182183158, "compression_ratio": 1.5077720207253886, "no_speech_prob": 4.637851816369221e-06}, {"id": 859, "seek": 423326, "start": 4233.26, "end": 4237.92, "text": " And what they suggest you do is that you alias git to hub, because it turns out that hub", "tokens": [400, 437, 436, 3402, 291, 360, 307, 300, 291, 419, 4609, 18331, 281, 11838, 11, 570, 309, 4523, 484, 300, 11838], "temperature": 0.0, "avg_logprob": -0.10502404967943828, "compression_ratio": 1.6842105263157894, "no_speech_prob": 2.3687912289460655e-06}, {"id": 860, "seek": 423326, "start": 4237.92, "end": 4241.56, "text": " actually is a strict superset of git.", "tokens": [767, 307, 257, 10910, 37906, 302, 295, 18331, 13], "temperature": 0.0, "avg_logprob": -0.10502404967943828, "compression_ratio": 1.6842105263157894, "no_speech_prob": 2.3687912289460655e-06}, {"id": 861, "seek": 423326, "start": 4241.56, "end": 4251.34, "text": " But what it lets you do is you can go git fork, git push, git pull request, and you've", "tokens": [583, 437, 309, 6653, 291, 360, 307, 291, 393, 352, 18331, 17716, 11, 18331, 2944, 11, 18331, 2235, 5308, 11, 293, 291, 600], "temperature": 0.0, "avg_logprob": -0.10502404967943828, "compression_ratio": 1.6842105263157894, "no_speech_prob": 2.3687912289460655e-06}, {"id": 862, "seek": 423326, "start": 4251.34, "end": 4254.280000000001, "text": " now sent Chris a pull request.", "tokens": [586, 2279, 6688, 257, 2235, 5308, 13], "temperature": 0.0, "avg_logprob": -0.10502404967943828, "compression_ratio": 1.6842105263157894, "no_speech_prob": 2.3687912289460655e-06}, {"id": 863, "seek": 423326, "start": 4254.280000000001, "end": 4258.84, "text": " Without hub, this is actually a pain and requires like going to the website and filling in forms", "tokens": [9129, 11838, 11, 341, 307, 767, 257, 1822, 293, 7029, 411, 516, 281, 264, 3144, 293, 10623, 294, 6422], "temperature": 0.0, "avg_logprob": -0.10502404967943828, "compression_ratio": 1.6842105263157894, "no_speech_prob": 2.3687912289460655e-06}, {"id": 864, "seek": 423326, "start": 4258.84, "end": 4260.08, "text": " and stuff.", "tokens": [293, 1507, 13], "temperature": 0.0, "avg_logprob": -0.10502404967943828, "compression_ratio": 1.6842105263157894, "no_speech_prob": 2.3687912289460655e-06}, {"id": 865, "seek": 426008, "start": 4260.08, "end": 4263.84, "text": " So this gives you no reason not to do pull requests.", "tokens": [407, 341, 2709, 291, 572, 1778, 406, 281, 360, 2235, 12475, 13], "temperature": 0.0, "avg_logprob": -0.10485535654528387, "compression_ratio": 1.754325259515571, "no_speech_prob": 5.014706857764395e-06}, {"id": 866, "seek": 426008, "start": 4263.84, "end": 4268.98, "text": " And I mention this because like when you're interviewing for a job or whatever, I can", "tokens": [400, 286, 2152, 341, 570, 411, 562, 291, 434, 26524, 337, 257, 1691, 420, 2035, 11, 286, 393], "temperature": 0.0, "avg_logprob": -0.10485535654528387, "compression_ratio": 1.754325259515571, "no_speech_prob": 5.014706857764395e-06}, {"id": 867, "seek": 426008, "start": 4268.98, "end": 4273.2, "text": " promise you that the person you're talking to will check your GitHub.", "tokens": [6228, 291, 300, 264, 954, 291, 434, 1417, 281, 486, 1520, 428, 23331, 13], "temperature": 0.0, "avg_logprob": -0.10485535654528387, "compression_ratio": 1.754325259515571, "no_speech_prob": 5.014706857764395e-06}, {"id": 868, "seek": 426008, "start": 4273.2, "end": 4277.98, "text": " And if they see you have a history of submitting thoughtful pull requests that are accepted", "tokens": [400, 498, 436, 536, 291, 362, 257, 2503, 295, 31836, 21566, 2235, 12475, 300, 366, 9035], "temperature": 0.0, "avg_logprob": -0.10485535654528387, "compression_ratio": 1.754325259515571, "no_speech_prob": 5.014706857764395e-06}, {"id": 869, "seek": 426008, "start": 4277.98, "end": 4281.0, "text": " to interesting libraries, that looks great.", "tokens": [281, 1880, 15148, 11, 300, 1542, 869, 13], "temperature": 0.0, "avg_logprob": -0.10485535654528387, "compression_ratio": 1.754325259515571, "no_speech_prob": 5.014706857764395e-06}, {"id": 870, "seek": 426008, "start": 4281.0, "end": 4284.64, "text": " It looks great because it shows you're somebody who actually contributes.", "tokens": [467, 1542, 869, 570, 309, 3110, 291, 434, 2618, 567, 767, 32035, 13], "temperature": 0.0, "avg_logprob": -0.10485535654528387, "compression_ratio": 1.754325259515571, "no_speech_prob": 5.014706857764395e-06}, {"id": 871, "seek": 426008, "start": 4284.64, "end": 4288.96, "text": " It also shows that if they're being accepted, that you know how to create code that fits", "tokens": [467, 611, 3110, 300, 498, 436, 434, 885, 9035, 11, 300, 291, 458, 577, 281, 1884, 3089, 300, 9001], "temperature": 0.0, "avg_logprob": -0.10485535654528387, "compression_ratio": 1.754325259515571, "no_speech_prob": 5.014706857764395e-06}, {"id": 872, "seek": 428896, "start": 4288.96, "end": 4294.58, "text": " with people's coding standards, has appropriate documentation, passes their tests and coverage", "tokens": [365, 561, 311, 17720, 7787, 11, 575, 6854, 14333, 11, 11335, 641, 6921, 293, 9645], "temperature": 0.0, "avg_logprob": -0.13842670204713173, "compression_ratio": 1.573076923076923, "no_speech_prob": 4.637862275558291e-06}, {"id": 873, "seek": 428896, "start": 4294.58, "end": 4296.04, "text": " and so forth.", "tokens": [293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.13842670204713173, "compression_ratio": 1.573076923076923, "no_speech_prob": 4.637862275558291e-06}, {"id": 874, "seek": 428896, "start": 4296.04, "end": 4301.12, "text": " So when people look at you and they say, oh, here's somebody with a history of successfully", "tokens": [407, 562, 561, 574, 412, 291, 293, 436, 584, 11, 1954, 11, 510, 311, 2618, 365, 257, 2503, 295, 10727], "temperature": 0.0, "avg_logprob": -0.13842670204713173, "compression_ratio": 1.573076923076923, "no_speech_prob": 4.637862275558291e-06}, {"id": 875, "seek": 428896, "start": 4301.12, "end": 4306.4, "text": " contributing accepted pull requests to open source libraries, that's a great part of your", "tokens": [19270, 9035, 2235, 12475, 281, 1269, 4009, 15148, 11, 300, 311, 257, 869, 644, 295, 428], "temperature": 0.0, "avg_logprob": -0.13842670204713173, "compression_ratio": 1.573076923076923, "no_speech_prob": 4.637862275558291e-06}, {"id": 876, "seek": 428896, "start": 4306.4, "end": 4308.2, "text": " portfolio.", "tokens": [12583, 13], "temperature": 0.0, "avg_logprob": -0.13842670204713173, "compression_ratio": 1.573076923076923, "no_speech_prob": 4.637862275558291e-06}, {"id": 877, "seek": 428896, "start": 4308.2, "end": 4310.56, "text": " And you can specifically refer to it.", "tokens": [400, 291, 393, 4682, 2864, 281, 309, 13], "temperature": 0.0, "avg_logprob": -0.13842670204713173, "compression_ratio": 1.573076923076923, "no_speech_prob": 4.637862275558291e-06}, {"id": 878, "seek": 428896, "start": 4310.56, "end": 4317.44, "text": " So either I'm the person who built Python waterfall, here is my repo.", "tokens": [407, 2139, 286, 478, 264, 954, 567, 3094, 15329, 27848, 11, 510, 307, 452, 49040, 13], "temperature": 0.0, "avg_logprob": -0.13842670204713173, "compression_ratio": 1.573076923076923, "no_speech_prob": 4.637862275558291e-06}, {"id": 879, "seek": 431744, "start": 4317.44, "end": 4324.04, "text": " Or I'm the person who contributed currency number formatting to Python waterfall, here's", "tokens": [1610, 286, 478, 264, 954, 567, 18434, 13346, 1230, 39366, 281, 15329, 27848, 11, 510, 311], "temperature": 0.0, "avg_logprob": -0.11959669437814266, "compression_ratio": 1.590717299578059, "no_speech_prob": 5.862785656063352e-06}, {"id": 880, "seek": 431744, "start": 4324.04, "end": 4327.0, "text": " my pull request.", "tokens": [452, 2235, 5308, 13], "temperature": 0.0, "avg_logprob": -0.11959669437814266, "compression_ratio": 1.590717299578059, "no_speech_prob": 5.862785656063352e-06}, {"id": 881, "seek": 431744, "start": 4327.0, "end": 4332.2, "text": " Anytime you see something that doesn't work right in any open source software you use", "tokens": [39401, 291, 536, 746, 300, 1177, 380, 589, 558, 294, 604, 1269, 4009, 4722, 291, 764], "temperature": 0.0, "avg_logprob": -0.11959669437814266, "compression_ratio": 1.590717299578059, "no_speech_prob": 5.862785656063352e-06}, {"id": 882, "seek": 431744, "start": 4332.2, "end": 4333.879999999999, "text": " is not a problem.", "tokens": [307, 406, 257, 1154, 13], "temperature": 0.0, "avg_logprob": -0.11959669437814266, "compression_ratio": 1.590717299578059, "no_speech_prob": 5.862785656063352e-06}, {"id": 883, "seek": 431744, "start": 4333.879999999999, "end": 4339.219999999999, "text": " It's a great opportunity because you can fix it and send in the pull request.", "tokens": [467, 311, 257, 869, 2650, 570, 291, 393, 3191, 309, 293, 2845, 294, 264, 2235, 5308, 13], "temperature": 0.0, "avg_logprob": -0.11959669437814266, "compression_ratio": 1.590717299578059, "no_speech_prob": 5.862785656063352e-06}, {"id": 884, "seek": 431744, "start": 4339.219999999999, "end": 4340.679999999999, "text": " So give it a go.", "tokens": [407, 976, 309, 257, 352, 13], "temperature": 0.0, "avg_logprob": -0.11959669437814266, "compression_ratio": 1.590717299578059, "no_speech_prob": 5.862785656063352e-06}, {"id": 885, "seek": 431744, "start": 4340.679999999999, "end": 4344.28, "text": " It actually feels great the first time you have a pull request accepted.", "tokens": [467, 767, 3417, 869, 264, 700, 565, 291, 362, 257, 2235, 5308, 9035, 13], "temperature": 0.0, "avg_logprob": -0.11959669437814266, "compression_ratio": 1.590717299578059, "no_speech_prob": 5.862785656063352e-06}, {"id": 886, "seek": 434428, "start": 4344.28, "end": 4348.88, "text": " And of course, one big opportunity is the FastAI library.", "tokens": [400, 295, 1164, 11, 472, 955, 2650, 307, 264, 15968, 48698, 6405, 13], "temperature": 0.0, "avg_logprob": -0.3139262566199669, "compression_ratio": 1.3591549295774648, "no_speech_prob": 3.535595169523731e-05}, {"id": 887, "seek": 434428, "start": 4348.88, "end": 4362.92, "text": " Thanks to one of our students, we now have docstrings for most of the FastAI.structured", "tokens": [2561, 281, 472, 295, 527, 1731, 11, 321, 586, 362, 3211, 50035, 337, 881, 295, 264, 15968, 48698, 13, 372, 46847], "temperature": 0.0, "avg_logprob": -0.3139262566199669, "compression_ratio": 1.3591549295774648, "no_speech_prob": 3.535595169523731e-05}, {"id": 888, "seek": 434428, "start": 4362.92, "end": 4372.2, "text": " library and that again came via a pull request.", "tokens": [6405, 293, 300, 797, 1361, 5766, 257, 2235, 5308, 13], "temperature": 0.0, "avg_logprob": -0.3139262566199669, "compression_ratio": 1.3591549295774648, "no_speech_prob": 3.535595169523731e-05}, {"id": 889, "seek": 437220, "start": 4372.2, "end": 4379.72, "text": " Does anybody have any questions about how to calculate any of these random forest interpretation", "tokens": [4402, 4472, 362, 604, 1651, 466, 577, 281, 8873, 604, 295, 613, 4974, 6719, 14174], "temperature": 0.0, "avg_logprob": -0.1396754925067608, "compression_ratio": 1.7305389221556886, "no_speech_prob": 2.9310647732927464e-05}, {"id": 890, "seek": 437220, "start": 4379.72, "end": 4385.4, "text": " methods or why we might want to use any of these random forest interpretation methods?", "tokens": [7150, 420, 983, 321, 1062, 528, 281, 764, 604, 295, 613, 4974, 6719, 14174, 7150, 30], "temperature": 0.0, "avg_logprob": -0.1396754925067608, "compression_ratio": 1.7305389221556886, "no_speech_prob": 2.9310647732927464e-05}, {"id": 891, "seek": 437220, "start": 4385.4, "end": 4391.84, "text": " Towards the end of the week, you're going to need to be able to build all of these yourself", "tokens": [48938, 264, 917, 295, 264, 1243, 11, 291, 434, 516, 281, 643, 281, 312, 1075, 281, 1322, 439, 295, 613, 1803], "temperature": 0.0, "avg_logprob": -0.1396754925067608, "compression_ratio": 1.7305389221556886, "no_speech_prob": 2.9310647732927464e-05}, {"id": 892, "seek": 437220, "start": 4391.84, "end": 4400.92, "text": " from scratch.", "tokens": [490, 8459, 13], "temperature": 0.0, "avg_logprob": -0.1396754925067608, "compression_ratio": 1.7305389221556886, "no_speech_prob": 2.9310647732927464e-05}, {"id": 893, "seek": 440092, "start": 4400.92, "end": 4409.4800000000005, "text": " Just looking at the tree interpreter, I noticed that some of the values are NANs.", "tokens": [1449, 1237, 412, 264, 4230, 34132, 11, 286, 5694, 300, 512, 295, 264, 4190, 366, 426, 1770, 82, 13], "temperature": 0.0, "avg_logprob": -0.24906905709880672, "compression_ratio": 1.3850574712643677, "no_speech_prob": 1.1478517080831807e-05}, {"id": 894, "seek": 440092, "start": 4409.4800000000005, "end": 4416.2, "text": " How can an NAN have a feature importance?", "tokens": [1012, 393, 364, 426, 1770, 362, 257, 4111, 7379, 30], "temperature": 0.0, "avg_logprob": -0.24906905709880672, "compression_ratio": 1.3850574712643677, "no_speech_prob": 1.1478517080831807e-05}, {"id": 895, "seek": 440092, "start": 4416.2, "end": 4421.08, "text": " Okay, let me pass it back to you.", "tokens": [1033, 11, 718, 385, 1320, 309, 646, 281, 291, 13], "temperature": 0.0, "avg_logprob": -0.24906905709880672, "compression_ratio": 1.3850574712643677, "no_speech_prob": 1.1478517080831807e-05}, {"id": 896, "seek": 440092, "start": 4421.08, "end": 4422.84, "text": " Why not?", "tokens": [1545, 406, 30], "temperature": 0.0, "avg_logprob": -0.24906905709880672, "compression_ratio": 1.3850574712643677, "no_speech_prob": 1.1478517080831807e-05}, {"id": 897, "seek": 440092, "start": 4422.84, "end": 4429.08, "text": " So in other words, how is NAN handled in pandas and therefore in the tree?", "tokens": [407, 294, 661, 2283, 11, 577, 307, 426, 1770, 18033, 294, 4565, 296, 293, 4412, 294, 264, 4230, 30], "temperature": 0.0, "avg_logprob": -0.24906905709880672, "compression_ratio": 1.3850574712643677, "no_speech_prob": 1.1478517080831807e-05}, {"id": 898, "seek": 442908, "start": 4429.08, "end": 4434.5199999999995, "text": " Set to some default value?", "tokens": [8928, 281, 512, 7576, 2158, 30], "temperature": 0.0, "avg_logprob": -0.22309509588747609, "compression_ratio": 1.6367713004484306, "no_speech_prob": 3.9441634726244956e-05}, {"id": 899, "seek": 442908, "start": 4434.5199999999995, "end": 4438.5199999999995, "text": " Anybody remember how pandas, notice these are all in categorical variables.", "tokens": [19082, 1604, 577, 4565, 296, 11, 3449, 613, 366, 439, 294, 19250, 804, 9102, 13], "temperature": 0.0, "avg_logprob": -0.22309509588747609, "compression_ratio": 1.6367713004484306, "no_speech_prob": 3.9441634726244956e-05}, {"id": 900, "seek": 442908, "start": 4438.5199999999995, "end": 4444.72, "text": " How does pandas handle NANs in categorical variables and how does FastAI deal with them?", "tokens": [1012, 775, 4565, 296, 4813, 426, 1770, 82, 294, 19250, 804, 9102, 293, 577, 775, 15968, 48698, 2028, 365, 552, 30], "temperature": 0.0, "avg_logprob": -0.22309509588747609, "compression_ratio": 1.6367713004484306, "no_speech_prob": 3.9441634726244956e-05}, {"id": 901, "seek": 442908, "start": 4444.72, "end": 4450.64, "text": " Can somebody pass it to the person who's talking?", "tokens": [1664, 2618, 1320, 309, 281, 264, 954, 567, 311, 1417, 30], "temperature": 0.0, "avg_logprob": -0.22309509588747609, "compression_ratio": 1.6367713004484306, "no_speech_prob": 3.9441634726244956e-05}, {"id": 902, "seek": 442908, "start": 4450.64, "end": 4451.64, "text": " Negative one for pandas?", "tokens": [43230, 472, 337, 4565, 296, 30], "temperature": 0.0, "avg_logprob": -0.22309509588747609, "compression_ratio": 1.6367713004484306, "no_speech_prob": 3.9441634726244956e-05}, {"id": 903, "seek": 442908, "start": 4451.64, "end": 4455.32, "text": " Yeah, pandas sets them to negative one category code.", "tokens": [865, 11, 4565, 296, 6352, 552, 281, 3671, 472, 7719, 3089, 13], "temperature": 0.0, "avg_logprob": -0.22309509588747609, "compression_ratio": 1.6367713004484306, "no_speech_prob": 3.9441634726244956e-05}, {"id": 904, "seek": 442908, "start": 4455.32, "end": 4458.12, "text": " And do you have to remember what we then do?", "tokens": [400, 360, 291, 362, 281, 1604, 437, 321, 550, 360, 30], "temperature": 0.0, "avg_logprob": -0.22309509588747609, "compression_ratio": 1.6367713004484306, "no_speech_prob": 3.9441634726244956e-05}, {"id": 905, "seek": 445812, "start": 4458.12, "end": 4462.96, "text": " Doesn't matter really, we add one to all of the category codes so it ends up being zero.", "tokens": [12955, 380, 1871, 534, 11, 321, 909, 472, 281, 439, 295, 264, 7719, 14211, 370, 309, 5314, 493, 885, 4018, 13], "temperature": 0.0, "avg_logprob": -0.15782283246517181, "compression_ratio": 1.7104247104247103, "no_speech_prob": 2.507130557205528e-05}, {"id": 906, "seek": 445812, "start": 4462.96, "end": 4467.0, "text": " So in other words, we have a category with, remember by the time it hits the random forest", "tokens": [407, 294, 661, 2283, 11, 321, 362, 257, 7719, 365, 11, 1604, 538, 264, 565, 309, 8664, 264, 4974, 6719], "temperature": 0.0, "avg_logprob": -0.15782283246517181, "compression_ratio": 1.7104247104247103, "no_speech_prob": 2.507130557205528e-05}, {"id": 907, "seek": 445812, "start": 4467.0, "end": 4470.96, "text": " it's just a number and it's just a number zero.", "tokens": [309, 311, 445, 257, 1230, 293, 309, 311, 445, 257, 1230, 4018, 13], "temperature": 0.0, "avg_logprob": -0.15782283246517181, "compression_ratio": 1.7104247104247103, "no_speech_prob": 2.507130557205528e-05}, {"id": 908, "seek": 445812, "start": 4470.96, "end": 4473.92, "text": " And we map it back to the descriptions back here.", "tokens": [400, 321, 4471, 309, 646, 281, 264, 24406, 646, 510, 13], "temperature": 0.0, "avg_logprob": -0.15782283246517181, "compression_ratio": 1.7104247104247103, "no_speech_prob": 2.507130557205528e-05}, {"id": 909, "seek": 445812, "start": 4473.92, "end": 4480.599999999999, "text": " So the question really is why shouldn't the random forest be able to split on zero?", "tokens": [407, 264, 1168, 534, 307, 983, 4659, 380, 264, 4974, 6719, 312, 1075, 281, 7472, 322, 4018, 30], "temperature": 0.0, "avg_logprob": -0.15782283246517181, "compression_ratio": 1.7104247104247103, "no_speech_prob": 2.507130557205528e-05}, {"id": 910, "seek": 445812, "start": 4480.599999999999, "end": 4482.5599999999995, "text": " It's just another number.", "tokens": [467, 311, 445, 1071, 1230, 13], "temperature": 0.0, "avg_logprob": -0.15782283246517181, "compression_ratio": 1.7104247104247103, "no_speech_prob": 2.507130557205528e-05}, {"id": 911, "seek": 445812, "start": 4482.5599999999995, "end": 4487.08, "text": " So it could be NAN, high, medium or low, 0, 1, 2, 3, 4.", "tokens": [407, 309, 727, 312, 426, 1770, 11, 1090, 11, 6399, 420, 2295, 11, 1958, 11, 502, 11, 568, 11, 805, 11, 1017, 13], "temperature": 0.0, "avg_logprob": -0.15782283246517181, "compression_ratio": 1.7104247104247103, "no_speech_prob": 2.507130557205528e-05}, {"id": 912, "seek": 448708, "start": 4487.08, "end": 4495.08, "text": " And so missing values are one of these things that are generally taught really badly.", "tokens": [400, 370, 5361, 4190, 366, 472, 295, 613, 721, 300, 366, 5101, 5928, 534, 13425, 13], "temperature": 0.0, "avg_logprob": -0.12516737655854562, "compression_ratio": 1.7513812154696133, "no_speech_prob": 5.1739039008680265e-06}, {"id": 913, "seek": 448708, "start": 4495.08, "end": 4499.28, "text": " Often people get taught, here are some ways to remove columns with missing values or remove", "tokens": [20043, 561, 483, 5928, 11, 510, 366, 512, 2098, 281, 4159, 13766, 365, 5361, 4190, 420, 4159], "temperature": 0.0, "avg_logprob": -0.12516737655854562, "compression_ratio": 1.7513812154696133, "no_speech_prob": 5.1739039008680265e-06}, {"id": 914, "seek": 448708, "start": 4499.28, "end": 4504.04, "text": " rows with missing values or to replace missing values.", "tokens": [13241, 365, 5361, 4190, 420, 281, 7406, 5361, 4190, 13], "temperature": 0.0, "avg_logprob": -0.12516737655854562, "compression_ratio": 1.7513812154696133, "no_speech_prob": 5.1739039008680265e-06}, {"id": 915, "seek": 448708, "start": 4504.04, "end": 4510.96, "text": " That's never what we want because missingness is very, very, very often interesting.", "tokens": [663, 311, 1128, 437, 321, 528, 570, 5361, 1287, 307, 588, 11, 588, 11, 588, 2049, 1880, 13], "temperature": 0.0, "avg_logprob": -0.12516737655854562, "compression_ratio": 1.7513812154696133, "no_speech_prob": 5.1739039008680265e-06}, {"id": 916, "seek": 451096, "start": 4510.96, "end": 4517.52, "text": " And so we actually learned from our feature importance that coupler system NAN is like", "tokens": [400, 370, 321, 767, 3264, 490, 527, 4111, 7379, 300, 1384, 22732, 1185, 426, 1770, 307, 411], "temperature": 0.0, "avg_logprob": -0.17018590370814005, "compression_ratio": 1.6309012875536482, "no_speech_prob": 6.854289495095145e-06}, {"id": 917, "seek": 451096, "start": 4517.52, "end": 4519.92, "text": " one of the most important features.", "tokens": [472, 295, 264, 881, 1021, 4122, 13], "temperature": 0.0, "avg_logprob": -0.17018590370814005, "compression_ratio": 1.6309012875536482, "no_speech_prob": 6.854289495095145e-06}, {"id": 918, "seek": 451096, "start": 4519.92, "end": 4527.24, "text": " And so for some reason, well I could guess, coupler system NAN presumably means this is", "tokens": [400, 370, 337, 512, 1778, 11, 731, 286, 727, 2041, 11, 1384, 22732, 1185, 426, 1770, 26742, 1355, 341, 307], "temperature": 0.0, "avg_logprob": -0.17018590370814005, "compression_ratio": 1.6309012875536482, "no_speech_prob": 6.854289495095145e-06}, {"id": 919, "seek": 451096, "start": 4527.24, "end": 4530.64, "text": " the kind of industrial equipment that doesn't have a coupler system.", "tokens": [264, 733, 295, 9987, 5927, 300, 1177, 380, 362, 257, 1384, 22732, 1185, 13], "temperature": 0.0, "avg_logprob": -0.17018590370814005, "compression_ratio": 1.6309012875536482, "no_speech_prob": 6.854289495095145e-06}, {"id": 920, "seek": 451096, "start": 4530.64, "end": 4536.04, "text": " Now I don't know what kind that is, but apparently it's a more expensive kind.", "tokens": [823, 286, 500, 380, 458, 437, 733, 300, 307, 11, 457, 7970, 309, 311, 257, 544, 5124, 733, 13], "temperature": 0.0, "avg_logprob": -0.17018590370814005, "compression_ratio": 1.6309012875536482, "no_speech_prob": 6.854289495095145e-06}, {"id": 921, "seek": 451096, "start": 4536.04, "end": 4540.44, "text": " Does that make sense?", "tokens": [4402, 300, 652, 2020, 30], "temperature": 0.0, "avg_logprob": -0.17018590370814005, "compression_ratio": 1.6309012875536482, "no_speech_prob": 6.854289495095145e-06}, {"id": 922, "seek": 454044, "start": 4540.44, "end": 4551.5199999999995, "text": " So I did this competition for university grant research success where by far the most important", "tokens": [407, 286, 630, 341, 6211, 337, 5454, 6386, 2132, 2245, 689, 538, 1400, 264, 881, 1021], "temperature": 0.0, "avg_logprob": -0.07905329519243383, "compression_ratio": 1.5837837837837838, "no_speech_prob": 7.646506674063858e-06}, {"id": 923, "seek": 454044, "start": 4551.5199999999995, "end": 4556.48, "text": " predictors were whether or not some of the fields were null.", "tokens": [6069, 830, 645, 1968, 420, 406, 512, 295, 264, 7909, 645, 18184, 13], "temperature": 0.0, "avg_logprob": -0.07905329519243383, "compression_ratio": 1.5837837837837838, "no_speech_prob": 7.646506674063858e-06}, {"id": 924, "seek": 454044, "start": 4556.48, "end": 4561.96, "text": " And it turned out that this was data leakage, that these fields only got filled in most", "tokens": [400, 309, 3574, 484, 300, 341, 390, 1412, 47799, 11, 300, 613, 7909, 787, 658, 6412, 294, 881], "temperature": 0.0, "avg_logprob": -0.07905329519243383, "compression_ratio": 1.5837837837837838, "no_speech_prob": 7.646506674063858e-06}, {"id": 925, "seek": 454044, "start": 4561.96, "end": 4565.82, "text": " of the time after a research grant was accepted.", "tokens": [295, 264, 565, 934, 257, 2132, 6386, 390, 9035, 13], "temperature": 0.0, "avg_logprob": -0.07905329519243383, "compression_ratio": 1.5837837837837838, "no_speech_prob": 7.646506674063858e-06}, {"id": 926, "seek": 456582, "start": 4565.82, "end": 4571.2, "text": " So it allowed me to win that Kaggle competition, but didn't actually help the university very", "tokens": [407, 309, 4350, 385, 281, 1942, 300, 48751, 22631, 6211, 11, 457, 994, 380, 767, 854, 264, 5454, 588], "temperature": 0.0, "avg_logprob": -0.17738720628081775, "compression_ratio": 1.4171779141104295, "no_speech_prob": 1.459373038414924e-06}, {"id": 927, "seek": 456582, "start": 4571.2, "end": 4574.2, "text": " much.", "tokens": [709, 13], "temperature": 0.0, "avg_logprob": -0.17738720628081775, "compression_ratio": 1.4171779141104295, "no_speech_prob": 1.459373038414924e-06}, {"id": 928, "seek": 456582, "start": 4574.2, "end": 4583.08, "text": " So let's talk about extrapolation.", "tokens": [407, 718, 311, 751, 466, 48224, 399, 13], "temperature": 0.0, "avg_logprob": -0.17738720628081775, "compression_ratio": 1.4171779141104295, "no_speech_prob": 1.459373038414924e-06}, {"id": 929, "seek": 456582, "start": 4583.08, "end": 4592.799999999999, "text": " And I am going to do something risky and dangerous, which is we're going to do some live coding.", "tokens": [400, 286, 669, 516, 281, 360, 746, 21137, 293, 5795, 11, 597, 307, 321, 434, 516, 281, 360, 512, 1621, 17720, 13], "temperature": 0.0, "avg_logprob": -0.17738720628081775, "compression_ratio": 1.4171779141104295, "no_speech_prob": 1.459373038414924e-06}, {"id": 930, "seek": 459280, "start": 4592.8, "end": 4598.360000000001, "text": " And the reason we're going to do some live coding is I want to explore extrapolation", "tokens": [400, 264, 1778, 321, 434, 516, 281, 360, 512, 1621, 17720, 307, 286, 528, 281, 6839, 48224, 399], "temperature": 0.0, "avg_logprob": -0.11230529029414339, "compression_ratio": 1.771551724137931, "no_speech_prob": 1.2029509889543988e-05}, {"id": 931, "seek": 459280, "start": 4598.360000000001, "end": 4606.02, "text": " together with you, and I kind of also want to help give you a feel of how you might go", "tokens": [1214, 365, 291, 11, 293, 286, 733, 295, 611, 528, 281, 854, 976, 291, 257, 841, 295, 577, 291, 1062, 352], "temperature": 0.0, "avg_logprob": -0.11230529029414339, "compression_ratio": 1.771551724137931, "no_speech_prob": 1.2029509889543988e-05}, {"id": 932, "seek": 459280, "start": 4606.02, "end": 4612.2, "text": " about writing code quickly in this notebook environment.", "tokens": [466, 3579, 3089, 2661, 294, 341, 21060, 2823, 13], "temperature": 0.0, "avg_logprob": -0.11230529029414339, "compression_ratio": 1.771551724137931, "no_speech_prob": 1.2029509889543988e-05}, {"id": 933, "seek": 459280, "start": 4612.2, "end": 4615.400000000001, "text": " And this is the kind of stuff that you're going to need to be able to do in the real", "tokens": [400, 341, 307, 264, 733, 295, 1507, 300, 291, 434, 516, 281, 643, 281, 312, 1075, 281, 360, 294, 264, 957], "temperature": 0.0, "avg_logprob": -0.11230529029414339, "compression_ratio": 1.771551724137931, "no_speech_prob": 1.2029509889543988e-05}, {"id": 934, "seek": 459280, "start": 4615.400000000001, "end": 4619.04, "text": " world and in the exam, is kind of quickly create the kind of code that we're going to", "tokens": [1002, 293, 294, 264, 1139, 11, 307, 733, 295, 2661, 1884, 264, 733, 295, 3089, 300, 321, 434, 516, 281], "temperature": 0.0, "avg_logprob": -0.11230529029414339, "compression_ratio": 1.771551724137931, "no_speech_prob": 1.2029509889543988e-05}, {"id": 935, "seek": 459280, "start": 4619.04, "end": 4620.24, "text": " talk about.", "tokens": [751, 466, 13], "temperature": 0.0, "avg_logprob": -0.11230529029414339, "compression_ratio": 1.771551724137931, "no_speech_prob": 1.2029509889543988e-05}, {"id": 936, "seek": 462024, "start": 4620.24, "end": 4625.32, "text": " So I really like creating synthetic datasets.", "tokens": [407, 286, 534, 411, 4084, 23420, 42856, 13], "temperature": 0.0, "avg_logprob": -0.1384106353974678, "compression_ratio": 1.5179487179487179, "no_speech_prob": 7.646463018318173e-06}, {"id": 937, "seek": 462024, "start": 4625.32, "end": 4629.08, "text": " Anytime I'm trying to investigate the behavior of something, because if I have a synthetic", "tokens": [39401, 286, 478, 1382, 281, 15013, 264, 5223, 295, 746, 11, 570, 498, 286, 362, 257, 23420], "temperature": 0.0, "avg_logprob": -0.1384106353974678, "compression_ratio": 1.5179487179487179, "no_speech_prob": 7.646463018318173e-06}, {"id": 938, "seek": 462024, "start": 4629.08, "end": 4633.0, "text": " dataset, I know how it should behave.", "tokens": [28872, 11, 286, 458, 577, 309, 820, 15158, 13], "temperature": 0.0, "avg_logprob": -0.1384106353974678, "compression_ratio": 1.5179487179487179, "no_speech_prob": 7.646463018318173e-06}, {"id": 939, "seek": 462024, "start": 4633.0, "end": 4641.24, "text": " Which reminds me, before we do this, I promised that we would talk about interaction importance,", "tokens": [3013, 12025, 385, 11, 949, 321, 360, 341, 11, 286, 10768, 300, 321, 576, 751, 466, 9285, 7379, 11], "temperature": 0.0, "avg_logprob": -0.1384106353974678, "compression_ratio": 1.5179487179487179, "no_speech_prob": 7.646463018318173e-06}, {"id": 940, "seek": 462024, "start": 4641.24, "end": 4645.44, "text": " and I just about forgot.", "tokens": [293, 286, 445, 466, 5298, 13], "temperature": 0.0, "avg_logprob": -0.1384106353974678, "compression_ratio": 1.5179487179487179, "no_speech_prob": 7.646463018318173e-06}, {"id": 941, "seek": 464544, "start": 4645.44, "end": 4653.379999999999, "text": " Tree interpreter tells us the contributions for a particular row based on the difference", "tokens": [22291, 34132, 5112, 505, 264, 15725, 337, 257, 1729, 5386, 2361, 322, 264, 2649], "temperature": 0.0, "avg_logprob": -0.18782614171504974, "compression_ratio": 1.6981132075471699, "no_speech_prob": 1.4063954040466342e-05}, {"id": 942, "seek": 464544, "start": 4653.379999999999, "end": 4656.0199999999995, "text": " in the tree.", "tokens": [294, 264, 4230, 13], "temperature": 0.0, "avg_logprob": -0.18782614171504974, "compression_ratio": 1.6981132075471699, "no_speech_prob": 1.4063954040466342e-05}, {"id": 943, "seek": 464544, "start": 4656.0199999999995, "end": 4664.0, "text": " We could calculate that for every row in our dataset and add them up, and that would tell", "tokens": [492, 727, 8873, 300, 337, 633, 5386, 294, 527, 28872, 293, 909, 552, 493, 11, 293, 300, 576, 980], "temperature": 0.0, "avg_logprob": -0.18782614171504974, "compression_ratio": 1.6981132075471699, "no_speech_prob": 1.4063954040466342e-05}, {"id": 944, "seek": 464544, "start": 4664.0, "end": 4666.679999999999, "text": " us feature importance.", "tokens": [505, 4111, 7379, 13], "temperature": 0.0, "avg_logprob": -0.18782614171504974, "compression_ratio": 1.6981132075471699, "no_speech_prob": 1.4063954040466342e-05}, {"id": 945, "seek": 464544, "start": 4666.679999999999, "end": 4670.799999999999, "text": " It would tell us feature importance in a different way.", "tokens": [467, 576, 980, 505, 4111, 7379, 294, 257, 819, 636, 13], "temperature": 0.0, "avg_logprob": -0.18782614171504974, "compression_ratio": 1.6981132075471699, "no_speech_prob": 1.4063954040466342e-05}, {"id": 946, "seek": 467080, "start": 4670.8, "end": 4675.62, "text": " One way of doing feature importance is by shuffling the columns one at a time.", "tokens": [1485, 636, 295, 884, 4111, 7379, 307, 538, 402, 1245, 1688, 264, 13766, 472, 412, 257, 565, 13], "temperature": 0.0, "avg_logprob": -0.1815906215358425, "compression_ratio": 1.5343915343915344, "no_speech_prob": 3.50084724232147e-06}, {"id": 947, "seek": 467080, "start": 4675.62, "end": 4681.8, "text": " Another way is by doing tree interpreter for every row and adding them up.", "tokens": [3996, 636, 307, 538, 884, 4230, 34132, 337, 633, 5386, 293, 5127, 552, 493, 13], "temperature": 0.0, "avg_logprob": -0.1815906215358425, "compression_ratio": 1.5343915343915344, "no_speech_prob": 3.50084724232147e-06}, {"id": 948, "seek": 467080, "start": 4681.8, "end": 4683.84, "text": " Neither is more right than the others.", "tokens": [23956, 307, 544, 558, 813, 264, 2357, 13], "temperature": 0.0, "avg_logprob": -0.1815906215358425, "compression_ratio": 1.5343915343915344, "no_speech_prob": 3.50084724232147e-06}, {"id": 949, "seek": 467080, "start": 4683.84, "end": 4686.16, "text": " They're actually both quite widely used.", "tokens": [814, 434, 767, 1293, 1596, 13371, 1143, 13], "temperature": 0.0, "avg_logprob": -0.1815906215358425, "compression_ratio": 1.5343915343915344, "no_speech_prob": 3.50084724232147e-06}, {"id": 950, "seek": 467080, "start": 4686.16, "end": 4691.84, "text": " So this is kind of type 1 and type 2 feature importance.", "tokens": [407, 341, 307, 733, 295, 2010, 502, 293, 2010, 568, 4111, 7379, 13], "temperature": 0.0, "avg_logprob": -0.1815906215358425, "compression_ratio": 1.5343915343915344, "no_speech_prob": 3.50084724232147e-06}, {"id": 951, "seek": 469184, "start": 4691.84, "end": 4706.16, "text": " So we could try to expand this a little bit to do not just single variable feature importance,", "tokens": [407, 321, 727, 853, 281, 5268, 341, 257, 707, 857, 281, 360, 406, 445, 2167, 7006, 4111, 7379, 11], "temperature": 0.0, "avg_logprob": -0.15503543531390981, "compression_ratio": 1.5208333333333333, "no_speech_prob": 1.482355514781375e-06}, {"id": 952, "seek": 469184, "start": 4706.16, "end": 4708.0, "text": " but interaction feature importance.", "tokens": [457, 9285, 4111, 7379, 13], "temperature": 0.0, "avg_logprob": -0.15503543531390981, "compression_ratio": 1.5208333333333333, "no_speech_prob": 1.482355514781375e-06}, {"id": 953, "seek": 469184, "start": 4708.0, "end": 4712.64, "text": " Now here's the thing.", "tokens": [823, 510, 311, 264, 551, 13], "temperature": 0.0, "avg_logprob": -0.15503543531390981, "compression_ratio": 1.5208333333333333, "no_speech_prob": 1.482355514781375e-06}, {"id": 954, "seek": 469184, "start": 4712.64, "end": 4716.2, "text": " What I'm going to describe is very easy to describe.", "tokens": [708, 286, 478, 516, 281, 6786, 307, 588, 1858, 281, 6786, 13], "temperature": 0.0, "avg_logprob": -0.15503543531390981, "compression_ratio": 1.5208333333333333, "no_speech_prob": 1.482355514781375e-06}, {"id": 955, "seek": 469184, "start": 4716.2, "end": 4720.92, "text": " It was described by Breiman right back when random forests were first invented, and it", "tokens": [467, 390, 7619, 538, 7090, 25504, 558, 646, 562, 4974, 21700, 645, 700, 14479, 11, 293, 309], "temperature": 0.0, "avg_logprob": -0.15503543531390981, "compression_ratio": 1.5208333333333333, "no_speech_prob": 1.482355514781375e-06}, {"id": 956, "seek": 472092, "start": 4720.92, "end": 4727.12, "text": " is part of the commercial software product from Salford Systems who have the trademark", "tokens": [307, 644, 295, 264, 6841, 4722, 1674, 490, 318, 1678, 765, 27059, 567, 362, 264, 31361], "temperature": 0.0, "avg_logprob": -0.12641177858625138, "compression_ratio": 1.5737704918032787, "no_speech_prob": 1.1478677151899319e-05}, {"id": 957, "seek": 472092, "start": 4727.12, "end": 4729.08, "text": " on random forests.", "tokens": [322, 4974, 21700, 13], "temperature": 0.0, "avg_logprob": -0.12641177858625138, "compression_ratio": 1.5737704918032787, "no_speech_prob": 1.1478677151899319e-05}, {"id": 958, "seek": 472092, "start": 4729.08, "end": 4736.0, "text": " But it is not part of any open source library I'm aware of, and I've never seen an academic", "tokens": [583, 309, 307, 406, 644, 295, 604, 1269, 4009, 6405, 286, 478, 3650, 295, 11, 293, 286, 600, 1128, 1612, 364, 7778], "temperature": 0.0, "avg_logprob": -0.12641177858625138, "compression_ratio": 1.5737704918032787, "no_speech_prob": 1.1478677151899319e-05}, {"id": 959, "seek": 472092, "start": 4736.0, "end": 4738.92, "text": " paper that actually studies it closely.", "tokens": [3035, 300, 767, 5313, 309, 8185, 13], "temperature": 0.0, "avg_logprob": -0.12641177858625138, "compression_ratio": 1.5737704918032787, "no_speech_prob": 1.1478677151899319e-05}, {"id": 960, "seek": 472092, "start": 4738.92, "end": 4745.5, "text": " So what I'm going to describe here is a huge opportunity, but it's also like there's lots", "tokens": [407, 437, 286, 478, 516, 281, 6786, 510, 307, 257, 2603, 2650, 11, 457, 309, 311, 611, 411, 456, 311, 3195], "temperature": 0.0, "avg_logprob": -0.12641177858625138, "compression_ratio": 1.5737704918032787, "no_speech_prob": 1.1478677151899319e-05}, {"id": 961, "seek": 472092, "start": 4745.5, "end": 4748.2, "text": " and lots of details that kind of need to be fleshed out.", "tokens": [293, 3195, 295, 4365, 300, 733, 295, 643, 281, 312, 12497, 292, 484, 13], "temperature": 0.0, "avg_logprob": -0.12641177858625138, "compression_ratio": 1.5737704918032787, "no_speech_prob": 1.1478677151899319e-05}, {"id": 962, "seek": 474820, "start": 4748.2, "end": 4756.36, "text": " But here's the basic idea.", "tokens": [583, 510, 311, 264, 3875, 1558, 13], "temperature": 0.0, "avg_logprob": -0.16607826634457237, "compression_ratio": 1.3669724770642202, "no_speech_prob": 9.223405868397094e-06}, {"id": 963, "seek": 474820, "start": 4756.36, "end": 4767.84, "text": " This particular difference here is not just because of year made, but because of a combination", "tokens": [639, 1729, 2649, 510, 307, 406, 445, 570, 295, 1064, 1027, 11, 457, 570, 295, 257, 6562], "temperature": 0.0, "avg_logprob": -0.16607826634457237, "compression_ratio": 1.3669724770642202, "no_speech_prob": 9.223405868397094e-06}, {"id": 964, "seek": 474820, "start": 4767.84, "end": 4772.42, "text": " of year made and enclosure.", "tokens": [295, 1064, 1027, 293, 34093, 13], "temperature": 0.0, "avg_logprob": -0.16607826634457237, "compression_ratio": 1.3669724770642202, "no_speech_prob": 9.223405868397094e-06}, {"id": 965, "seek": 477242, "start": 4772.42, "end": 4778.16, "text": " The fact that this is 9.7 is because enclosure was in this branch and year made was in this", "tokens": [440, 1186, 300, 341, 307, 1722, 13, 22, 307, 570, 34093, 390, 294, 341, 9819, 293, 1064, 1027, 390, 294, 341], "temperature": 0.0, "avg_logprob": -0.17236513561672634, "compression_ratio": 1.5037593984962405, "no_speech_prob": 5.682410574081587e-06}, {"id": 966, "seek": 477242, "start": 4778.16, "end": 4779.46, "text": " branch.", "tokens": [9819, 13], "temperature": 0.0, "avg_logprob": -0.17236513561672634, "compression_ratio": 1.5037593984962405, "no_speech_prob": 5.682410574081587e-06}, {"id": 967, "seek": 477242, "start": 4779.46, "end": 4790.92, "text": " So in other words we could say the contribution of enclosure interacted with year made is", "tokens": [407, 294, 661, 2283, 321, 727, 584, 264, 13150, 295, 34093, 49621, 365, 1064, 1027, 307], "temperature": 0.0, "avg_logprob": -0.17236513561672634, "compression_ratio": 1.5037593984962405, "no_speech_prob": 5.682410574081587e-06}, {"id": 968, "seek": 477242, "start": 4790.92, "end": 4796.28, "text": " minus 0.3.", "tokens": [3175, 1958, 13, 18, 13], "temperature": 0.0, "avg_logprob": -0.17236513561672634, "compression_ratio": 1.5037593984962405, "no_speech_prob": 5.682410574081587e-06}, {"id": 969, "seek": 479628, "start": 4796.28, "end": 4803.28, "text": " And so what about that difference?", "tokens": [400, 370, 437, 466, 300, 2649, 30], "temperature": 0.0, "avg_logprob": -0.13588966821369372, "compression_ratio": 1.5754189944134078, "no_speech_prob": 5.0936855586769525e-06}, {"id": 970, "seek": 479628, "start": 4803.28, "end": 4808.66, "text": " Well that's an interaction of year made and hours on the meter.", "tokens": [1042, 300, 311, 364, 9285, 295, 1064, 1027, 293, 2496, 322, 264, 9255, 13], "temperature": 0.0, "avg_logprob": -0.13588966821369372, "compression_ratio": 1.5754189944134078, "no_speech_prob": 5.0936855586769525e-06}, {"id": 971, "seek": 479628, "start": 4808.66, "end": 4814.719999999999, "text": " So year made interacted with, I'm using star here not to mean times, but to mean interacted", "tokens": [407, 1064, 1027, 49621, 365, 11, 286, 478, 1228, 3543, 510, 406, 281, 914, 1413, 11, 457, 281, 914, 49621], "temperature": 0.0, "avg_logprob": -0.13588966821369372, "compression_ratio": 1.5754189944134078, "no_speech_prob": 5.0936855586769525e-06}, {"id": 972, "seek": 479628, "start": 4814.719999999999, "end": 4815.719999999999, "text": " with.", "tokens": [365, 13], "temperature": 0.0, "avg_logprob": -0.13588966821369372, "compression_ratio": 1.5754189944134078, "no_speech_prob": 5.0936855586769525e-06}, {"id": 973, "seek": 479628, "start": 4815.719999999999, "end": 4820.5599999999995, "text": " It's kind of a common way of doing things, like ours formulas do it this way as well.", "tokens": [467, 311, 733, 295, 257, 2689, 636, 295, 884, 721, 11, 411, 11896, 30546, 360, 309, 341, 636, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.13588966821369372, "compression_ratio": 1.5754189944134078, "no_speech_prob": 5.0936855586769525e-06}, {"id": 974, "seek": 482056, "start": 4820.56, "end": 4834.320000000001, "text": " Year made interacted with meter has a contribution of minus 0.1.", "tokens": [10289, 1027, 49621, 365, 9255, 575, 257, 13150, 295, 3175, 1958, 13, 16, 13], "temperature": 0.0, "avg_logprob": -0.12017513275146484, "compression_ratio": 1.4893617021276595, "no_speech_prob": 2.2959127363719745e-06}, {"id": 975, "seek": 482056, "start": 4834.320000000001, "end": 4841.88, "text": " Perhaps we could also say from here to here that this also shows an interaction between", "tokens": [10517, 321, 727, 611, 584, 490, 510, 281, 510, 300, 341, 611, 3110, 364, 9285, 1296], "temperature": 0.0, "avg_logprob": -0.12017513275146484, "compression_ratio": 1.4893617021276595, "no_speech_prob": 2.2959127363719745e-06}, {"id": 976, "seek": 482056, "start": 4841.88, "end": 4846.080000000001, "text": " meter and enclosure, like with one thing in between them.", "tokens": [9255, 293, 34093, 11, 411, 365, 472, 551, 294, 1296, 552, 13], "temperature": 0.0, "avg_logprob": -0.12017513275146484, "compression_ratio": 1.4893617021276595, "no_speech_prob": 2.2959127363719745e-06}, {"id": 977, "seek": 484608, "start": 4846.08, "end": 4855.4, "text": " So maybe we could say meter by enclosure equals, and then what should it be?", "tokens": [407, 1310, 321, 727, 584, 9255, 538, 34093, 6915, 11, 293, 550, 437, 820, 309, 312, 30], "temperature": 0.0, "avg_logprob": -0.21154148238045828, "compression_ratio": 1.3605442176870748, "no_speech_prob": 1.0953083801723551e-05}, {"id": 978, "seek": 484608, "start": 4855.4, "end": 4862.08, "text": " Should it be minus 0.6?", "tokens": [6454, 309, 312, 3175, 1958, 13, 21, 30], "temperature": 0.0, "avg_logprob": -0.21154148238045828, "compression_ratio": 1.3605442176870748, "no_speech_prob": 1.0953083801723551e-05}, {"id": 979, "seek": 484608, "start": 4862.08, "end": 4868.44, "text": " I mean in some ways that kind of seems unfair because we're also including the impact of", "tokens": [286, 914, 294, 512, 2098, 300, 733, 295, 2544, 17019, 570, 321, 434, 611, 3009, 264, 2712, 295], "temperature": 0.0, "avg_logprob": -0.21154148238045828, "compression_ratio": 1.3605442176870748, "no_speech_prob": 1.0953083801723551e-05}, {"id": 980, "seek": 484608, "start": 4868.44, "end": 4869.68, "text": " year made.", "tokens": [1064, 1027, 13], "temperature": 0.0, "avg_logprob": -0.21154148238045828, "compression_ratio": 1.3605442176870748, "no_speech_prob": 1.0953083801723551e-05}, {"id": 981, "seek": 486968, "start": 4869.68, "end": 4878.8, "text": " So maybe it should be minus 0.6, maybe we should add back this 0.2.", "tokens": [407, 1310, 309, 820, 312, 3175, 1958, 13, 21, 11, 1310, 321, 820, 909, 646, 341, 1958, 13, 17, 13], "temperature": 0.0, "avg_logprob": -0.16449162579964902, "compression_ratio": 1.4508670520231215, "no_speech_prob": 3.0894859719410306e-06}, {"id": 982, "seek": 486968, "start": 4878.8, "end": 4883.72, "text": " And these are like details that I actually don't know the answer to.", "tokens": [400, 613, 366, 411, 4365, 300, 286, 767, 500, 380, 458, 264, 1867, 281, 13], "temperature": 0.0, "avg_logprob": -0.16449162579964902, "compression_ratio": 1.4508670520231215, "no_speech_prob": 3.0894859719410306e-06}, {"id": 983, "seek": 486968, "start": 4883.72, "end": 4893.76, "text": " How should we best assign a contribution to each pair of variables in this path?", "tokens": [1012, 820, 321, 1151, 6269, 257, 13150, 281, 1184, 6119, 295, 9102, 294, 341, 3100, 30], "temperature": 0.0, "avg_logprob": -0.16449162579964902, "compression_ratio": 1.4508670520231215, "no_speech_prob": 3.0894859719410306e-06}, {"id": 984, "seek": 486968, "start": 4893.76, "end": 4896.92, "text": " But clearly, conceptually we can.", "tokens": [583, 4448, 11, 3410, 671, 321, 393, 13], "temperature": 0.0, "avg_logprob": -0.16449162579964902, "compression_ratio": 1.4508670520231215, "no_speech_prob": 3.0894859719410306e-06}, {"id": 985, "seek": 489692, "start": 4896.92, "end": 4915.36, "text": " The pairs of variables in that path all represent interactions.", "tokens": [440, 15494, 295, 9102, 294, 300, 3100, 439, 2906, 13280, 13], "temperature": 0.0, "avg_logprob": -0.18345027923583984, "compression_ratio": 1.4244604316546763, "no_speech_prob": 1.8631548300618306e-05}, {"id": 986, "seek": 489692, "start": 4915.36, "end": 4918.64, "text": " I'm not going to say it's the wrong approach.", "tokens": [286, 478, 406, 516, 281, 584, 309, 311, 264, 2085, 3109, 13], "temperature": 0.0, "avg_logprob": -0.18345027923583984, "compression_ratio": 1.4244604316546763, "no_speech_prob": 1.8631548300618306e-05}, {"id": 987, "seek": 489692, "start": 4918.64, "end": 4925.16, "text": " I don't think it's the right approach though because it feels like this path here, meter", "tokens": [286, 500, 380, 519, 309, 311, 264, 558, 3109, 1673, 570, 309, 3417, 411, 341, 3100, 510, 11, 9255], "temperature": 0.0, "avg_logprob": -0.18345027923583984, "compression_ratio": 1.4244604316546763, "no_speech_prob": 1.8631548300618306e-05}, {"id": 988, "seek": 492516, "start": 4925.16, "end": 4928.84, "text": " and enclosure are interacting.", "tokens": [293, 34093, 366, 18017, 13], "temperature": 0.0, "avg_logprob": -0.1835028330485026, "compression_ratio": 1.433179723502304, "no_speech_prob": 3.0415853871090803e-06}, {"id": 989, "seek": 492516, "start": 4928.84, "end": 4937.08, "text": " So it seems like not recognizing that contribution is throwing away information.", "tokens": [407, 309, 2544, 411, 406, 18538, 300, 13150, 307, 10238, 1314, 1589, 13], "temperature": 0.0, "avg_logprob": -0.1835028330485026, "compression_ratio": 1.433179723502304, "no_speech_prob": 3.0415853871090803e-06}, {"id": 990, "seek": 492516, "start": 4937.08, "end": 4940.08, "text": " But I'm not sure.", "tokens": [583, 286, 478, 406, 988, 13], "temperature": 0.0, "avg_logprob": -0.1835028330485026, "compression_ratio": 1.433179723502304, "no_speech_prob": 3.0415853871090803e-06}, {"id": 991, "seek": 492516, "start": 4940.08, "end": 4947.4, "text": " I had one of my staff at CADL actually do some R&D on this a few years ago, and I wasn't", "tokens": [286, 632, 472, 295, 452, 3525, 412, 41143, 43, 767, 360, 512, 497, 5, 35, 322, 341, 257, 1326, 924, 2057, 11, 293, 286, 2067, 380], "temperature": 0.0, "avg_logprob": -0.1835028330485026, "compression_ratio": 1.433179723502304, "no_speech_prob": 3.0415853871090803e-06}, {"id": 992, "seek": 492516, "start": 4947.4, "end": 4951.32, "text": " close enough to know how they dealt with these details, but they got it working pretty well.", "tokens": [1998, 1547, 281, 458, 577, 436, 15991, 365, 613, 4365, 11, 457, 436, 658, 309, 1364, 1238, 731, 13], "temperature": 0.0, "avg_logprob": -0.1835028330485026, "compression_ratio": 1.433179723502304, "no_speech_prob": 3.0415853871090803e-06}, {"id": 993, "seek": 495132, "start": 4951.32, "end": 4955.92, "text": " But unfortunately it never saw the light of day as a software product.", "tokens": [583, 7015, 309, 1128, 1866, 264, 1442, 295, 786, 382, 257, 4722, 1674, 13], "temperature": 0.0, "avg_logprob": -0.2138836311571526, "compression_ratio": 1.433673469387755, "no_speech_prob": 5.6824110288289376e-06}, {"id": 994, "seek": 495132, "start": 4955.92, "end": 4962.799999999999, "text": " This is something which maybe a group of you could get together and build.", "tokens": [639, 307, 746, 597, 1310, 257, 1594, 295, 291, 727, 483, 1214, 293, 1322, 13], "temperature": 0.0, "avg_logprob": -0.2138836311571526, "compression_ratio": 1.433673469387755, "no_speech_prob": 5.6824110288289376e-06}, {"id": 995, "seek": 495132, "start": 4962.799999999999, "end": 4968.24, "text": " Do some Googling to check, but I really don't think that there are any interaction feature", "tokens": [1144, 512, 45005, 1688, 281, 1520, 11, 457, 286, 534, 500, 380, 519, 300, 456, 366, 604, 9285, 4111], "temperature": 0.0, "avg_logprob": -0.2138836311571526, "compression_ratio": 1.433673469387755, "no_speech_prob": 5.6824110288289376e-06}, {"id": 996, "seek": 495132, "start": 4968.24, "end": 4976.799999999999, "text": " importance parts of any open source library.", "tokens": [7379, 3166, 295, 604, 1269, 4009, 6405, 13], "temperature": 0.0, "avg_logprob": -0.2138836311571526, "compression_ratio": 1.433673469387755, "no_speech_prob": 5.6824110288289376e-06}, {"id": 997, "seek": 497680, "start": 4976.8, "end": 4982.22, "text": " Wouldn't this exclude interactions though between variables that don't matter until", "tokens": [26291, 380, 341, 33536, 13280, 1673, 1296, 9102, 300, 500, 380, 1871, 1826], "temperature": 0.0, "avg_logprob": -0.12450344004529587, "compression_ratio": 1.7639484978540771, "no_speech_prob": 2.482475110809901e-06}, {"id": 998, "seek": 497680, "start": 4982.22, "end": 4983.68, "text": " they interact?", "tokens": [436, 4648, 30], "temperature": 0.0, "avg_logprob": -0.12450344004529587, "compression_ratio": 1.7639484978540771, "no_speech_prob": 2.482475110809901e-06}, {"id": 999, "seek": 497680, "start": 4983.68, "end": 4989.8, "text": " So say your row never chooses to split down that path, but that variable interacting with", "tokens": [407, 584, 428, 5386, 1128, 25963, 281, 7472, 760, 300, 3100, 11, 457, 300, 7006, 18017, 365], "temperature": 0.0, "avg_logprob": -0.12450344004529587, "compression_ratio": 1.7639484978540771, "no_speech_prob": 2.482475110809901e-06}, {"id": 1000, "seek": 497680, "start": 4989.8, "end": 4993.96, "text": " another one becomes your most important split.", "tokens": [1071, 472, 3643, 428, 881, 1021, 7472, 13], "temperature": 0.0, "avg_logprob": -0.12450344004529587, "compression_ratio": 1.7639484978540771, "no_speech_prob": 2.482475110809901e-06}, {"id": 1001, "seek": 497680, "start": 4993.96, "end": 4998.12, "text": " I don't think that happens, right, because if there's an interaction that's important", "tokens": [286, 500, 380, 519, 300, 2314, 11, 558, 11, 570, 498, 456, 311, 364, 9285, 300, 311, 1021], "temperature": 0.0, "avg_logprob": -0.12450344004529587, "compression_ratio": 1.7639484978540771, "no_speech_prob": 2.482475110809901e-06}, {"id": 1002, "seek": 497680, "start": 4998.12, "end": 5004.6, "text": " only because it's an interaction and not on a univariate basis, it will appear sometimes,", "tokens": [787, 570, 309, 311, 364, 9285, 293, 406, 322, 257, 517, 592, 3504, 473, 5143, 11, 309, 486, 4204, 2171, 11], "temperature": 0.0, "avg_logprob": -0.12450344004529587, "compression_ratio": 1.7639484978540771, "no_speech_prob": 2.482475110809901e-06}, {"id": 1003, "seek": 500460, "start": 5004.6, "end": 5007.72, "text": " assuming that you set max features to less than 1.", "tokens": [11926, 300, 291, 992, 11469, 4122, 281, 1570, 813, 502, 13], "temperature": 0.0, "avg_logprob": -0.23523678357088113, "compression_ratio": 1.6069651741293531, "no_speech_prob": 9.818240869208239e-06}, {"id": 1004, "seek": 500460, "start": 5007.72, "end": 5012.6, "text": " And so therefore it will appear in some paths.", "tokens": [400, 370, 4412, 309, 486, 4204, 294, 512, 14518, 13], "temperature": 0.0, "avg_logprob": -0.23523678357088113, "compression_ratio": 1.6069651741293531, "no_speech_prob": 9.818240869208239e-06}, {"id": 1005, "seek": 500460, "start": 5012.6, "end": 5013.8, "text": " What is meant by interaction?", "tokens": [708, 307, 4140, 538, 9285, 30], "temperature": 0.0, "avg_logprob": -0.23523678357088113, "compression_ratio": 1.6069651741293531, "no_speech_prob": 9.818240869208239e-06}, {"id": 1006, "seek": 500460, "start": 5013.8, "end": 5017.360000000001, "text": " Is it multiplication, ratio, addition?", "tokens": [1119, 309, 27290, 11, 8509, 11, 4500, 30], "temperature": 0.0, "avg_logprob": -0.23523678357088113, "compression_ratio": 1.6069651741293531, "no_speech_prob": 9.818240869208239e-06}, {"id": 1007, "seek": 500460, "start": 5017.360000000001, "end": 5027.200000000001, "text": " Interaction means branches appears on the same path through a tree.", "tokens": [5751, 2894, 1355, 14770, 7038, 322, 264, 912, 3100, 807, 257, 4230, 13], "temperature": 0.0, "avg_logprob": -0.23523678357088113, "compression_ratio": 1.6069651741293531, "no_speech_prob": 9.818240869208239e-06}, {"id": 1008, "seek": 500460, "start": 5027.200000000001, "end": 5031.0, "text": " Like an interaction, in this case the tree, there's an interaction between enclosure and", "tokens": [1743, 364, 9285, 11, 294, 341, 1389, 264, 4230, 11, 456, 311, 364, 9285, 1296, 34093, 293], "temperature": 0.0, "avg_logprob": -0.23523678357088113, "compression_ratio": 1.6069651741293531, "no_speech_prob": 9.818240869208239e-06}, {"id": 1009, "seek": 503100, "start": 5031.0, "end": 5035.0, "text": " year made, because we branch on enclosure and then we branch on year made.", "tokens": [1064, 1027, 11, 570, 321, 9819, 322, 34093, 293, 550, 321, 9819, 322, 1064, 1027, 13], "temperature": 0.0, "avg_logprob": -0.21992395235144574, "compression_ratio": 1.7853658536585366, "no_speech_prob": 9.972857696993742e-06}, {"id": 1010, "seek": 503100, "start": 5035.0, "end": 5039.8, "text": " So to get to here, we have to have some specific value of enclosure and some specific value", "tokens": [407, 281, 483, 281, 510, 11, 321, 362, 281, 362, 512, 2685, 2158, 295, 34093, 293, 512, 2685, 2158], "temperature": 0.0, "avg_logprob": -0.21992395235144574, "compression_ratio": 1.7853658536585366, "no_speech_prob": 9.972857696993742e-06}, {"id": 1011, "seek": 503100, "start": 5039.8, "end": 5042.8, "text": " of year made.", "tokens": [295, 1064, 1027, 13], "temperature": 0.0, "avg_logprob": -0.21992395235144574, "compression_ratio": 1.7853658536585366, "no_speech_prob": 9.972857696993742e-06}, {"id": 1012, "seek": 503100, "start": 5042.8, "end": 5052.84, "text": " What if you went down the middle leafs between the two things you were trying to observe", "tokens": [708, 498, 291, 1437, 760, 264, 2808, 10871, 82, 1296, 264, 732, 721, 291, 645, 1382, 281, 11441], "temperature": 0.0, "avg_logprob": -0.21992395235144574, "compression_ratio": 1.7853658536585366, "no_speech_prob": 9.972857696993742e-06}, {"id": 1013, "seek": 503100, "start": 5052.84, "end": 5057.12, "text": " and you could just sort of norm, and you would also take into account what the final measure", "tokens": [293, 291, 727, 445, 1333, 295, 2026, 11, 293, 291, 576, 611, 747, 666, 2696, 437, 264, 2572, 3481], "temperature": 0.0, "avg_logprob": -0.21992395235144574, "compression_ratio": 1.7853658536585366, "no_speech_prob": 9.972857696993742e-06}, {"id": 1014, "seek": 503100, "start": 5057.12, "end": 5058.12, "text": " is.", "tokens": [307, 13], "temperature": 0.0, "avg_logprob": -0.21992395235144574, "compression_ratio": 1.7853658536585366, "no_speech_prob": 9.972857696993742e-06}, {"id": 1015, "seek": 505812, "start": 5058.12, "end": 5062.12, "text": " If you extend the tree downwards, you'd have many measures, both of the two things you're", "tokens": [759, 291, 10101, 264, 4230, 39880, 11, 291, 1116, 362, 867, 8000, 11, 1293, 295, 264, 732, 721, 291, 434], "temperature": 0.0, "avg_logprob": -0.23228571707742257, "compression_ratio": 1.644, "no_speech_prob": 2.1112138711032458e-05}, {"id": 1016, "seek": 505812, "start": 5062.12, "end": 5067.0, "text": " trying to look at and also the in-between steps.", "tokens": [1382, 281, 574, 412, 293, 611, 264, 294, 12, 32387, 4439, 13], "temperature": 0.0, "avg_logprob": -0.23228571707742257, "compression_ratio": 1.644, "no_speech_prob": 2.1112138711032458e-05}, {"id": 1017, "seek": 505812, "start": 5067.0, "end": 5070.08, "text": " There seems to be a way to average information out in between them?", "tokens": [821, 2544, 281, 312, 257, 636, 281, 4274, 1589, 484, 294, 1296, 552, 30], "temperature": 0.0, "avg_logprob": -0.23228571707742257, "compression_ratio": 1.644, "no_speech_prob": 2.1112138711032458e-05}, {"id": 1018, "seek": 505812, "start": 5070.08, "end": 5071.08, "text": " There could be.", "tokens": [821, 727, 312, 13], "temperature": 0.0, "avg_logprob": -0.23228571707742257, "compression_ratio": 1.644, "no_speech_prob": 2.1112138711032458e-05}, {"id": 1019, "seek": 505812, "start": 5071.08, "end": 5072.92, "text": " So I think what we should do is talk about this on the forum.", "tokens": [407, 286, 519, 437, 321, 820, 360, 307, 751, 466, 341, 322, 264, 17542, 13], "temperature": 0.0, "avg_logprob": -0.23228571707742257, "compression_ratio": 1.644, "no_speech_prob": 2.1112138711032458e-05}, {"id": 1020, "seek": 505812, "start": 5072.92, "end": 5076.92, "text": " I think this is fascinating and I hope we build something.", "tokens": [286, 519, 341, 307, 10343, 293, 286, 1454, 321, 1322, 746, 13], "temperature": 0.0, "avg_logprob": -0.23228571707742257, "compression_ratio": 1.644, "no_speech_prob": 2.1112138711032458e-05}, {"id": 1021, "seek": 505812, "start": 5076.92, "end": 5077.92, "text": " Great.", "tokens": [3769, 13], "temperature": 0.0, "avg_logprob": -0.23228571707742257, "compression_ratio": 1.644, "no_speech_prob": 2.1112138711032458e-05}, {"id": 1022, "seek": 505812, "start": 5077.92, "end": 5080.48, "text": " I need to do my live coding.", "tokens": [286, 643, 281, 360, 452, 1621, 17720, 13], "temperature": 0.0, "avg_logprob": -0.23228571707742257, "compression_ratio": 1.644, "no_speech_prob": 2.1112138711032458e-05}, {"id": 1023, "seek": 505812, "start": 5080.48, "end": 5087.12, "text": " So that was a great discussion.", "tokens": [407, 300, 390, 257, 869, 5017, 13], "temperature": 0.0, "avg_logprob": -0.23228571707742257, "compression_ratio": 1.644, "no_speech_prob": 2.1112138711032458e-05}, {"id": 1024, "seek": 508712, "start": 5087.12, "end": 5088.12, "text": " Big thinking about it.", "tokens": [5429, 1953, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.20644254591858502, "compression_ratio": 1.6833333333333333, "no_speech_prob": 2.5071078198379837e-05}, {"id": 1025, "seek": 508712, "start": 5088.12, "end": 5090.16, "text": " Yeah, these are experiments.", "tokens": [865, 11, 613, 366, 12050, 13], "temperature": 0.0, "avg_logprob": -0.20644254591858502, "compression_ratio": 1.6833333333333333, "no_speech_prob": 2.5071078198379837e-05}, {"id": 1026, "seek": 508712, "start": 5090.16, "end": 5096.24, "text": " And so to experiment with that, you almost certainly want to create a synthetic dataset", "tokens": [400, 370, 281, 5120, 365, 300, 11, 291, 1920, 3297, 528, 281, 1884, 257, 23420, 28872], "temperature": 0.0, "avg_logprob": -0.20644254591858502, "compression_ratio": 1.6833333333333333, "no_speech_prob": 2.5071078198379837e-05}, {"id": 1027, "seek": 508712, "start": 5096.24, "end": 5097.24, "text": " first.", "tokens": [700, 13], "temperature": 0.0, "avg_logprob": -0.20644254591858502, "compression_ratio": 1.6833333333333333, "no_speech_prob": 2.5071078198379837e-05}, {"id": 1028, "seek": 508712, "start": 5097.24, "end": 5103.88, "text": " It's like y equals x1 plus x2 plus x1 times x2 or something.", "tokens": [467, 311, 411, 288, 6915, 2031, 16, 1804, 2031, 17, 1804, 2031, 16, 1413, 2031, 17, 420, 746, 13], "temperature": 0.0, "avg_logprob": -0.20644254591858502, "compression_ratio": 1.6833333333333333, "no_speech_prob": 2.5071078198379837e-05}, {"id": 1029, "seek": 508712, "start": 5103.88, "end": 5107.599999999999, "text": " Like something where you know there's this interaction effect and there isn't that interaction", "tokens": [1743, 746, 689, 291, 458, 456, 311, 341, 9285, 1802, 293, 456, 1943, 380, 300, 9285], "temperature": 0.0, "avg_logprob": -0.20644254591858502, "compression_ratio": 1.6833333333333333, "no_speech_prob": 2.5071078198379837e-05}, {"id": 1030, "seek": 508712, "start": 5107.599999999999, "end": 5112.74, "text": " effect and you want to make sure that the feature importance you get at the end is what", "tokens": [1802, 293, 291, 528, 281, 652, 988, 300, 264, 4111, 7379, 291, 483, 412, 264, 917, 307, 437], "temperature": 0.0, "avg_logprob": -0.20644254591858502, "compression_ratio": 1.6833333333333333, "no_speech_prob": 2.5071078198379837e-05}, {"id": 1031, "seek": 508712, "start": 5112.74, "end": 5114.4, "text": " you expected.", "tokens": [291, 5176, 13], "temperature": 0.0, "avg_logprob": -0.20644254591858502, "compression_ratio": 1.6833333333333333, "no_speech_prob": 2.5071078198379837e-05}, {"id": 1032, "seek": 511440, "start": 5114.4, "end": 5120.839999999999, "text": " And so probably the first step would be to do single variable feature importance using", "tokens": [400, 370, 1391, 264, 700, 1823, 576, 312, 281, 360, 2167, 7006, 4111, 7379, 1228], "temperature": 0.0, "avg_logprob": -0.1294867618974433, "compression_ratio": 1.676056338028169, "no_speech_prob": 5.173889348952798e-06}, {"id": 1033, "seek": 511440, "start": 5120.839999999999, "end": 5126.799999999999, "text": " the tree interpreter style approach.", "tokens": [264, 4230, 34132, 3758, 3109, 13], "temperature": 0.0, "avg_logprob": -0.1294867618974433, "compression_ratio": 1.676056338028169, "no_speech_prob": 5.173889348952798e-06}, {"id": 1034, "seek": 511440, "start": 5126.799999999999, "end": 5133.679999999999, "text": " And one nice thing about this is it doesn't really matter how much data you have, all", "tokens": [400, 472, 1481, 551, 466, 341, 307, 309, 1177, 380, 534, 1871, 577, 709, 1412, 291, 362, 11, 439], "temperature": 0.0, "avg_logprob": -0.1294867618974433, "compression_ratio": 1.676056338028169, "no_speech_prob": 5.173889348952798e-06}, {"id": 1035, "seek": 511440, "start": 5133.679999999999, "end": 5137.599999999999, "text": " you have to do to calculate feature importance is just slide through the tree.", "tokens": [291, 362, 281, 360, 281, 8873, 4111, 7379, 307, 445, 4137, 807, 264, 4230, 13], "temperature": 0.0, "avg_logprob": -0.1294867618974433, "compression_ratio": 1.676056338028169, "no_speech_prob": 5.173889348952798e-06}, {"id": 1036, "seek": 511440, "start": 5137.599999999999, "end": 5141.2, "text": " So you should be able to write in a way that's actually pretty fast.", "tokens": [407, 291, 820, 312, 1075, 281, 2464, 294, 257, 636, 300, 311, 767, 1238, 2370, 13], "temperature": 0.0, "avg_logprob": -0.1294867618974433, "compression_ratio": 1.676056338028169, "no_speech_prob": 5.173889348952798e-06}, {"id": 1037, "seek": 514120, "start": 5141.2, "end": 5149.32, "text": " And so even writing it in pure Python might be fast enough, depending on your tree size.", "tokens": [400, 370, 754, 3579, 309, 294, 6075, 15329, 1062, 312, 2370, 1547, 11, 5413, 322, 428, 4230, 2744, 13], "temperature": 0.0, "avg_logprob": -0.11243837933207668, "compression_ratio": 1.5467289719626167, "no_speech_prob": 6.643392225669231e-06}, {"id": 1038, "seek": 514120, "start": 5149.32, "end": 5152.639999999999, "text": " So we're going to talk about extrapolation.", "tokens": [407, 321, 434, 516, 281, 751, 466, 48224, 399, 13], "temperature": 0.0, "avg_logprob": -0.11243837933207668, "compression_ratio": 1.5467289719626167, "no_speech_prob": 6.643392225669231e-06}, {"id": 1039, "seek": 514120, "start": 5152.639999999999, "end": 5156.599999999999, "text": " And so the first thing I want to do is create a synthetic dataset that has a simple linear", "tokens": [400, 370, 264, 700, 551, 286, 528, 281, 360, 307, 1884, 257, 23420, 28872, 300, 575, 257, 2199, 8213], "temperature": 0.0, "avg_logprob": -0.11243837933207668, "compression_ratio": 1.5467289719626167, "no_speech_prob": 6.643392225669231e-06}, {"id": 1040, "seek": 514120, "start": 5156.599999999999, "end": 5157.599999999999, "text": " relationship.", "tokens": [2480, 13], "temperature": 0.0, "avg_logprob": -0.11243837933207668, "compression_ratio": 1.5467289719626167, "no_speech_prob": 6.643392225669231e-06}, {"id": 1041, "seek": 514120, "start": 5157.599999999999, "end": 5161.76, "text": " We're going to pretend it's like a time series.", "tokens": [492, 434, 516, 281, 11865, 309, 311, 411, 257, 565, 2638, 13], "temperature": 0.0, "avg_logprob": -0.11243837933207668, "compression_ratio": 1.5467289719626167, "no_speech_prob": 6.643392225669231e-06}, {"id": 1042, "seek": 514120, "start": 5161.76, "end": 5164.92, "text": " So we need to basically create some x values.", "tokens": [407, 321, 643, 281, 1936, 1884, 512, 2031, 4190, 13], "temperature": 0.0, "avg_logprob": -0.11243837933207668, "compression_ratio": 1.5467289719626167, "no_speech_prob": 6.643392225669231e-06}, {"id": 1043, "seek": 516492, "start": 5164.92, "end": 5172.04, "text": " So the easiest way to create some synthetic data of this type is to use linspace, which", "tokens": [407, 264, 12889, 636, 281, 1884, 512, 23420, 1412, 295, 341, 2010, 307, 281, 764, 287, 1292, 17940, 11, 597], "temperature": 0.0, "avg_logprob": -0.1472142826427113, "compression_ratio": 1.4324324324324325, "no_speech_prob": 2.561278279245016e-06}, {"id": 1044, "seek": 516492, "start": 5172.04, "end": 5181.56, "text": " just creates some evenly spaced data between start and stop, by default 50 observations.", "tokens": [445, 7829, 512, 17658, 43766, 1412, 1296, 722, 293, 1590, 11, 538, 7576, 2625, 18163, 13], "temperature": 0.0, "avg_logprob": -0.1472142826427113, "compression_ratio": 1.4324324324324325, "no_speech_prob": 2.561278279245016e-06}, {"id": 1045, "seek": 516492, "start": 5181.56, "end": 5192.52, "text": " So if we just do that, there it is.", "tokens": [407, 498, 321, 445, 360, 300, 11, 456, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.1472142826427113, "compression_ratio": 1.4324324324324325, "no_speech_prob": 2.561278279245016e-06}, {"id": 1046, "seek": 519252, "start": 5192.52, "end": 5195.4800000000005, "text": " And so then we're going to create a dependent variable.", "tokens": [400, 370, 550, 321, 434, 516, 281, 1884, 257, 12334, 7006, 13], "temperature": 0.0, "avg_logprob": -0.1258135980634547, "compression_ratio": 1.5294117647058822, "no_speech_prob": 9.972893167287111e-06}, {"id": 1047, "seek": 519252, "start": 5195.4800000000005, "end": 5200.84, "text": " And so let's assume there's just a linear relationship between x and y, and let's add", "tokens": [400, 370, 718, 311, 6552, 456, 311, 445, 257, 8213, 2480, 1296, 2031, 293, 288, 11, 293, 718, 311, 909], "temperature": 0.0, "avg_logprob": -0.1258135980634547, "compression_ratio": 1.5294117647058822, "no_speech_prob": 9.972893167287111e-06}, {"id": 1048, "seek": 519252, "start": 5200.84, "end": 5207.120000000001, "text": " a little bit of randomness to it.", "tokens": [257, 707, 857, 295, 4974, 1287, 281, 309, 13], "temperature": 0.0, "avg_logprob": -0.1258135980634547, "compression_ratio": 1.5294117647058822, "no_speech_prob": 9.972893167287111e-06}, {"id": 1049, "seek": 519252, "start": 5207.120000000001, "end": 5215.76, "text": " So uniform random between low and high, so we could add somewhere between like minus", "tokens": [407, 9452, 4974, 1296, 2295, 293, 1090, 11, 370, 321, 727, 909, 4079, 1296, 411, 3175], "temperature": 0.0, "avg_logprob": -0.1258135980634547, "compression_ratio": 1.5294117647058822, "no_speech_prob": 9.972893167287111e-06}, {"id": 1050, "seek": 521576, "start": 5215.76, "end": 5222.72, "text": " 0.2 and 0.2.", "tokens": [1958, 13, 17, 293, 1958, 13, 17, 13], "temperature": 0.0, "avg_logprob": -0.12125161034720285, "compression_ratio": 1.4797297297297298, "no_speech_prob": 3.844926141027827e-06}, {"id": 1051, "seek": 521576, "start": 5222.72, "end": 5230.24, "text": " And so the next thing we need is a shape, which is basically what dimensions do you", "tokens": [400, 370, 264, 958, 551, 321, 643, 307, 257, 3909, 11, 597, 307, 1936, 437, 12819, 360, 291], "temperature": 0.0, "avg_logprob": -0.12125161034720285, "compression_ratio": 1.4797297297297298, "no_speech_prob": 3.844926141027827e-06}, {"id": 1052, "seek": 521576, "start": 5230.24, "end": 5233.96, "text": " want these random numbers to be.", "tokens": [528, 613, 4974, 3547, 281, 312, 13], "temperature": 0.0, "avg_logprob": -0.12125161034720285, "compression_ratio": 1.4797297297297298, "no_speech_prob": 3.844926141027827e-06}, {"id": 1053, "seek": 521576, "start": 5233.96, "end": 5237.38, "text": " And obviously we want them to be the same shape as x's shape.", "tokens": [400, 2745, 321, 528, 552, 281, 312, 264, 912, 3909, 382, 2031, 311, 3909, 13], "temperature": 0.0, "avg_logprob": -0.12125161034720285, "compression_ratio": 1.4797297297297298, "no_speech_prob": 3.844926141027827e-06}, {"id": 1054, "seek": 521576, "start": 5237.38, "end": 5242.12, "text": " So we can just say x.shape.", "tokens": [407, 321, 393, 445, 584, 2031, 13, 82, 42406, 13], "temperature": 0.0, "avg_logprob": -0.12125161034720285, "compression_ratio": 1.4797297297297298, "no_speech_prob": 3.844926141027827e-06}, {"id": 1055, "seek": 524212, "start": 5242.12, "end": 5245.96, "text": " So in other words, that's x.shape.", "tokens": [407, 294, 661, 2283, 11, 300, 311, 2031, 13, 82, 42406, 13], "temperature": 0.0, "avg_logprob": -0.1619866733819666, "compression_ratio": 1.5098039215686274, "no_speech_prob": 1.11255758383777e-05}, {"id": 1056, "seek": 524212, "start": 5245.96, "end": 5251.44, "text": " Remember when you see something in parentheses with a comma, that's a tuple with just one", "tokens": [5459, 562, 291, 536, 746, 294, 34153, 365, 257, 22117, 11, 300, 311, 257, 2604, 781, 365, 445, 472], "temperature": 0.0, "avg_logprob": -0.1619866733819666, "compression_ratio": 1.5098039215686274, "no_speech_prob": 1.11255758383777e-05}, {"id": 1057, "seek": 524212, "start": 5251.44, "end": 5253.599999999999, "text": " thing in it.", "tokens": [551, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.1619866733819666, "compression_ratio": 1.5098039215686274, "no_speech_prob": 1.11255758383777e-05}, {"id": 1058, "seek": 524212, "start": 5253.599999999999, "end": 5260.36, "text": " So this is of shape 50, and so we've added 50 random numbers, and so now we could plot", "tokens": [407, 341, 307, 295, 3909, 2625, 11, 293, 370, 321, 600, 3869, 2625, 4974, 3547, 11, 293, 370, 586, 321, 727, 7542], "temperature": 0.0, "avg_logprob": -0.1619866733819666, "compression_ratio": 1.5098039215686274, "no_speech_prob": 1.11255758383777e-05}, {"id": 1059, "seek": 524212, "start": 5260.36, "end": 5266.76, "text": " those.", "tokens": [729, 13], "temperature": 0.0, "avg_logprob": -0.1619866733819666, "compression_ratio": 1.5098039215686274, "no_speech_prob": 1.11255758383777e-05}, {"id": 1060, "seek": 526676, "start": 5266.76, "end": 5275.280000000001, "text": " So shift-tab, x, y.", "tokens": [407, 5513, 12, 83, 455, 11, 2031, 11, 288, 13], "temperature": 0.0, "avg_logprob": -0.19143301820101805, "compression_ratio": 1.4695121951219512, "no_speech_prob": 4.710899702331517e-06}, {"id": 1061, "seek": 526676, "start": 5275.280000000001, "end": 5277.360000000001, "text": " So there's our data.", "tokens": [407, 456, 311, 527, 1412, 13], "temperature": 0.0, "avg_logprob": -0.19143301820101805, "compression_ratio": 1.4695121951219512, "no_speech_prob": 4.710899702331517e-06}, {"id": 1062, "seek": 526676, "start": 5277.360000000001, "end": 5283.88, "text": " So for when you're both working as a data scientist or for doing your exams in this", "tokens": [407, 337, 562, 291, 434, 1293, 1364, 382, 257, 1412, 12662, 420, 337, 884, 428, 20514, 294, 341], "temperature": 0.0, "avg_logprob": -0.19143301820101805, "compression_ratio": 1.4695121951219512, "no_speech_prob": 4.710899702331517e-06}, {"id": 1063, "seek": 526676, "start": 5283.88, "end": 5288.88, "text": " course, you need to be able to quickly whip up a dataset like that, throw it up in a plot", "tokens": [1164, 11, 291, 643, 281, 312, 1075, 281, 2661, 22377, 493, 257, 28872, 411, 300, 11, 3507, 309, 493, 294, 257, 7542], "temperature": 0.0, "avg_logprob": -0.19143301820101805, "compression_ratio": 1.4695121951219512, "no_speech_prob": 4.710899702331517e-06}, {"id": 1064, "seek": 526676, "start": 5288.88, "end": 5292.22, "text": " without thinking too much.", "tokens": [1553, 1953, 886, 709, 13], "temperature": 0.0, "avg_logprob": -0.19143301820101805, "compression_ratio": 1.4695121951219512, "no_speech_prob": 4.710899702331517e-06}, {"id": 1065, "seek": 529222, "start": 5292.22, "end": 5297.0, "text": " And as you can see, you don't have to really remember much, if anything, you just have", "tokens": [400, 382, 291, 393, 536, 11, 291, 500, 380, 362, 281, 534, 1604, 709, 11, 498, 1340, 11, 291, 445, 362], "temperature": 0.0, "avg_logprob": -0.21949398643092105, "compression_ratio": 1.5662100456621004, "no_speech_prob": 5.862792477273615e-06}, {"id": 1066, "seek": 529222, "start": 5297.0, "end": 5301.96, "text": " to know how to hit shift-tab to check the names of the parameters.", "tokens": [281, 458, 577, 281, 2045, 5513, 12, 83, 455, 281, 1520, 264, 5288, 295, 264, 9834, 13], "temperature": 0.0, "avg_logprob": -0.21949398643092105, "compression_ratio": 1.5662100456621004, "no_speech_prob": 5.862792477273615e-06}, {"id": 1067, "seek": 529222, "start": 5301.96, "end": 5308.400000000001, "text": " And everything in the exam will be open, bulk open internet, so you can always google for", "tokens": [400, 1203, 294, 264, 1139, 486, 312, 1269, 11, 16139, 1269, 4705, 11, 370, 291, 393, 1009, 20742, 337], "temperature": 0.0, "avg_logprob": -0.21949398643092105, "compression_ratio": 1.5662100456621004, "no_speech_prob": 5.862792477273615e-06}, {"id": 1068, "seek": 529222, "start": 5308.400000000001, "end": 5313.9800000000005, "text": " something to try and find linspace if you've got what it's called.", "tokens": [746, 281, 853, 293, 915, 287, 1292, 17940, 498, 291, 600, 658, 437, 309, 311, 1219, 13], "temperature": 0.0, "avg_logprob": -0.21949398643092105, "compression_ratio": 1.5662100456621004, "no_speech_prob": 5.862792477273615e-06}, {"id": 1069, "seek": 529222, "start": 5313.9800000000005, "end": 5316.8, "text": " So let's assume that's our data.", "tokens": [407, 718, 311, 6552, 300, 311, 527, 1412, 13], "temperature": 0.0, "avg_logprob": -0.21949398643092105, "compression_ratio": 1.5662100456621004, "no_speech_prob": 5.862792477273615e-06}, {"id": 1070, "seek": 531680, "start": 5316.8, "end": 5323.8, "text": " And so we're now going to build a random forest model, and what I want to do is build a random", "tokens": [400, 370, 321, 434, 586, 516, 281, 1322, 257, 4974, 6719, 2316, 11, 293, 437, 286, 528, 281, 360, 307, 1322, 257, 4974], "temperature": 0.0, "avg_logprob": -0.07432873108807732, "compression_ratio": 1.7613636363636365, "no_speech_prob": 2.4439864318992477e-06}, {"id": 1071, "seek": 531680, "start": 5323.8, "end": 5327.72, "text": " forest model that kind of acts as if this is a time series.", "tokens": [6719, 2316, 300, 733, 295, 10672, 382, 498, 341, 307, 257, 565, 2638, 13], "temperature": 0.0, "avg_logprob": -0.07432873108807732, "compression_ratio": 1.7613636363636365, "no_speech_prob": 2.4439864318992477e-06}, {"id": 1072, "seek": 531680, "start": 5327.72, "end": 5336.68, "text": " So I'm going to take this as a training set, I'm going to take this as our validation or", "tokens": [407, 286, 478, 516, 281, 747, 341, 382, 257, 3097, 992, 11, 286, 478, 516, 281, 747, 341, 382, 527, 24071, 420], "temperature": 0.0, "avg_logprob": -0.07432873108807732, "compression_ratio": 1.7613636363636365, "no_speech_prob": 2.4439864318992477e-06}, {"id": 1073, "seek": 531680, "start": 5336.68, "end": 5345.7, "text": " test set, just like we did in groceries or bulldozers or whatever.", "tokens": [1500, 992, 11, 445, 411, 321, 630, 294, 31391, 420, 4693, 2595, 41698, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.07432873108807732, "compression_ratio": 1.7613636363636365, "no_speech_prob": 2.4439864318992477e-06}, {"id": 1074, "seek": 534570, "start": 5345.7, "end": 5349.5199999999995, "text": " So we can use exactly the same kind of code that we used in split-vowels.", "tokens": [407, 321, 393, 764, 2293, 264, 912, 733, 295, 3089, 300, 321, 1143, 294, 7472, 12, 85, 305, 1625, 13], "temperature": 0.0, "avg_logprob": -0.17095013587705551, "compression_ratio": 1.4137931034482758, "no_speech_prob": 9.368681276100688e-06}, {"id": 1075, "seek": 534570, "start": 5349.5199999999995, "end": 5370.0, "text": " So we can basically say X train, X vowel equals X up to 40, X from 40.", "tokens": [407, 321, 393, 1936, 584, 1783, 3847, 11, 1783, 29410, 6915, 1783, 493, 281, 3356, 11, 1783, 490, 3356, 13], "temperature": 0.0, "avg_logprob": -0.17095013587705551, "compression_ratio": 1.4137931034482758, "no_speech_prob": 9.368681276100688e-06}, {"id": 1076, "seek": 534570, "start": 5370.0, "end": 5375.08, "text": " So that just splits it into the first 40 versus the last 10.", "tokens": [407, 300, 445, 37741, 309, 666, 264, 700, 3356, 5717, 264, 1036, 1266, 13], "temperature": 0.0, "avg_logprob": -0.17095013587705551, "compression_ratio": 1.4137931034482758, "no_speech_prob": 9.368681276100688e-06}, {"id": 1077, "seek": 537508, "start": 5375.08, "end": 5387.2, "text": " And so we can do the same thing for Y.", "tokens": [400, 370, 321, 393, 360, 264, 912, 551, 337, 398, 13], "temperature": 0.0, "avg_logprob": -0.12126245304029815, "compression_ratio": 1.3513513513513513, "no_speech_prob": 4.6378563638427295e-06}, {"id": 1078, "seek": 537508, "start": 5387.2, "end": 5397.32, "text": " So the next thing to do is we want to create a random forest and fit it.", "tokens": [407, 264, 958, 551, 281, 360, 307, 321, 528, 281, 1884, 257, 4974, 6719, 293, 3318, 309, 13], "temperature": 0.0, "avg_logprob": -0.12126245304029815, "compression_ratio": 1.3513513513513513, "no_speech_prob": 4.6378563638427295e-06}, {"id": 1079, "seek": 537508, "start": 5397.32, "end": 5400.96, "text": " And that's going to require Xs and Ys.", "tokens": [400, 300, 311, 516, 281, 3651, 1783, 82, 293, 398, 82, 13], "temperature": 0.0, "avg_logprob": -0.12126245304029815, "compression_ratio": 1.3513513513513513, "no_speech_prob": 4.6378563638427295e-06}, {"id": 1080, "seek": 540096, "start": 5400.96, "end": 5405.92, "text": " Now that's actually going to give an error, and the reason why is that it expects X to", "tokens": [823, 300, 311, 767, 516, 281, 976, 364, 6713, 11, 293, 264, 1778, 983, 307, 300, 309, 33280, 1783, 281], "temperature": 0.0, "avg_logprob": -0.09644648688180106, "compression_ratio": 1.587878787878788, "no_speech_prob": 2.769398861346417e-06}, {"id": 1081, "seek": 540096, "start": 5405.92, "end": 5414.32, "text": " be a matrix, not a vector, because it expects X to have a number of columns of data.", "tokens": [312, 257, 8141, 11, 406, 257, 8062, 11, 570, 309, 33280, 1783, 281, 362, 257, 1230, 295, 13766, 295, 1412, 13], "temperature": 0.0, "avg_logprob": -0.09644648688180106, "compression_ratio": 1.587878787878788, "no_speech_prob": 2.769398861346417e-06}, {"id": 1082, "seek": 540096, "start": 5414.32, "end": 5422.24, "text": " So it's important to know that a matrix with one column is not the same thing as a vector.", "tokens": [407, 309, 311, 1021, 281, 458, 300, 257, 8141, 365, 472, 7738, 307, 406, 264, 912, 551, 382, 257, 8062, 13], "temperature": 0.0, "avg_logprob": -0.09644648688180106, "compression_ratio": 1.587878787878788, "no_speech_prob": 2.769398861346417e-06}, {"id": 1083, "seek": 542224, "start": 5422.24, "end": 5431.84, "text": " So if I try to run this, expected 2D array got 1D array instead.", "tokens": [407, 498, 286, 853, 281, 1190, 341, 11, 5176, 568, 35, 10225, 658, 502, 35, 10225, 2602, 13], "temperature": 0.0, "avg_logprob": -0.1745566552685153, "compression_ratio": 1.3333333333333333, "no_speech_prob": 1.7061804555851268e-06}, {"id": 1084, "seek": 542224, "start": 5431.84, "end": 5435.44, "text": " So we need to convert our 2D array into a 1D array.", "tokens": [407, 321, 643, 281, 7620, 527, 568, 35, 10225, 666, 257, 502, 35, 10225, 13], "temperature": 0.0, "avg_logprob": -0.1745566552685153, "compression_ratio": 1.3333333333333333, "no_speech_prob": 1.7061804555851268e-06}, {"id": 1085, "seek": 542224, "start": 5435.44, "end": 5441.38, "text": " So remember I said X.shape is 50, right?", "tokens": [407, 1604, 286, 848, 1783, 13, 82, 42406, 307, 2625, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1745566552685153, "compression_ratio": 1.3333333333333333, "no_speech_prob": 1.7061804555851268e-06}, {"id": 1086, "seek": 542224, "start": 5441.38, "end": 5446.36, "text": " So X has one axis.", "tokens": [407, 1783, 575, 472, 10298, 13], "temperature": 0.0, "avg_logprob": -0.1745566552685153, "compression_ratio": 1.3333333333333333, "no_speech_prob": 1.7061804555851268e-06}, {"id": 1087, "seek": 544636, "start": 5446.36, "end": 5452.24, "text": " So here's the important thing to make sure, X's rank is 1.", "tokens": [407, 510, 311, 264, 1021, 551, 281, 652, 988, 11, 1783, 311, 6181, 307, 502, 13], "temperature": 0.0, "avg_logprob": -0.17875779999627006, "compression_ratio": 1.4585987261146496, "no_speech_prob": 9.132543254963821e-07}, {"id": 1088, "seek": 544636, "start": 5452.24, "end": 5459.639999999999, "text": " The rank of a variable is equal to the length of its shape.", "tokens": [440, 6181, 295, 257, 7006, 307, 2681, 281, 264, 4641, 295, 1080, 3909, 13], "temperature": 0.0, "avg_logprob": -0.17875779999627006, "compression_ratio": 1.4585987261146496, "no_speech_prob": 9.132543254963821e-07}, {"id": 1089, "seek": 544636, "start": 5459.639999999999, "end": 5461.599999999999, "text": " How many axes does it have?", "tokens": [1012, 867, 35387, 775, 309, 362, 30], "temperature": 0.0, "avg_logprob": -0.17875779999627006, "compression_ratio": 1.4585987261146496, "no_speech_prob": 9.132543254963821e-07}, {"id": 1090, "seek": 544636, "start": 5461.599999999999, "end": 5467.2, "text": " So a vector we can think of as an array of rank 1.", "tokens": [407, 257, 8062, 321, 393, 519, 295, 382, 364, 10225, 295, 6181, 502, 13], "temperature": 0.0, "avg_logprob": -0.17875779999627006, "compression_ratio": 1.4585987261146496, "no_speech_prob": 9.132543254963821e-07}, {"id": 1091, "seek": 544636, "start": 5467.2, "end": 5471.24, "text": " A matrix is an array of rank 2.", "tokens": [316, 8141, 307, 364, 10225, 295, 6181, 568, 13], "temperature": 0.0, "avg_logprob": -0.17875779999627006, "compression_ratio": 1.4585987261146496, "no_speech_prob": 9.132543254963821e-07}, {"id": 1092, "seek": 547124, "start": 5471.24, "end": 5480.48, "text": " I very rarely use words like vector and matrix because they're kind of meaningless, specific", "tokens": [286, 588, 13752, 764, 2283, 411, 8062, 293, 8141, 570, 436, 434, 733, 295, 33232, 11, 2685], "temperature": 0.0, "avg_logprob": -0.17067689162034255, "compression_ratio": 1.6378378378378378, "no_speech_prob": 2.2603226170758717e-06}, {"id": 1093, "seek": 547124, "start": 5480.48, "end": 5486.84, "text": " examples of something more general, which is they're all n-dimensional tensors or n-dimensional", "tokens": [5110, 295, 746, 544, 2674, 11, 597, 307, 436, 434, 439, 297, 12, 18759, 10688, 830, 420, 297, 12, 18759], "temperature": 0.0, "avg_logprob": -0.17067689162034255, "compression_ratio": 1.6378378378378378, "no_speech_prob": 2.2603226170758717e-06}, {"id": 1094, "seek": 547124, "start": 5486.84, "end": 5489.32, "text": " arrays.", "tokens": [41011, 13], "temperature": 0.0, "avg_logprob": -0.17067689162034255, "compression_ratio": 1.6378378378378378, "no_speech_prob": 2.2603226170758717e-06}, {"id": 1095, "seek": 547124, "start": 5489.32, "end": 5495.48, "text": " So an n-dimensional array, we can say it's a tensor of rank n.", "tokens": [407, 364, 297, 12, 18759, 10225, 11, 321, 393, 584, 309, 311, 257, 40863, 295, 6181, 297, 13], "temperature": 0.0, "avg_logprob": -0.17067689162034255, "compression_ratio": 1.6378378378378378, "no_speech_prob": 2.2603226170758717e-06}, {"id": 1096, "seek": 547124, "start": 5495.48, "end": 5499.639999999999, "text": " They basically mean kind of the same thing.", "tokens": [814, 1936, 914, 733, 295, 264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.17067689162034255, "compression_ratio": 1.6378378378378378, "no_speech_prob": 2.2603226170758717e-06}, {"id": 1097, "seek": 549964, "start": 5499.64, "end": 5503.200000000001, "text": " N-terms get crazy when you say that because to a physicist, a tensor has quite a specific", "tokens": [426, 12, 391, 2592, 483, 3219, 562, 291, 584, 300, 570, 281, 257, 42466, 11, 257, 40863, 575, 1596, 257, 2685], "temperature": 0.0, "avg_logprob": -0.2341805540997049, "compression_ratio": 1.5681818181818181, "no_speech_prob": 4.0294485188496765e-06}, {"id": 1098, "seek": 549964, "start": 5503.200000000001, "end": 5508.780000000001, "text": " meaning, but in machine learning we generally use it in the same way.", "tokens": [3620, 11, 457, 294, 3479, 2539, 321, 5101, 764, 309, 294, 264, 912, 636, 13], "temperature": 0.0, "avg_logprob": -0.2341805540997049, "compression_ratio": 1.5681818181818181, "no_speech_prob": 4.0294485188496765e-06}, {"id": 1099, "seek": 549964, "start": 5508.780000000001, "end": 5518.0, "text": " So how do we turn a one-dimensional array into a two-dimensional array?", "tokens": [407, 577, 360, 321, 1261, 257, 472, 12, 18759, 10225, 666, 257, 732, 12, 18759, 10225, 30], "temperature": 0.0, "avg_logprob": -0.2341805540997049, "compression_ratio": 1.5681818181818181, "no_speech_prob": 4.0294485188496765e-06}, {"id": 1100, "seek": 549964, "start": 5518.0, "end": 5522.160000000001, "text": " There's a couple of ways we can do it, but basically we slice it.", "tokens": [821, 311, 257, 1916, 295, 2098, 321, 393, 360, 309, 11, 457, 1936, 321, 13153, 309, 13], "temperature": 0.0, "avg_logprob": -0.2341805540997049, "compression_ratio": 1.5681818181818181, "no_speech_prob": 4.0294485188496765e-06}, {"id": 1101, "seek": 549964, "start": 5522.160000000001, "end": 5529.280000000001, "text": " So colon means give me everything in that axis.", "tokens": [407, 8255, 1355, 976, 385, 1203, 294, 300, 10298, 13], "temperature": 0.0, "avg_logprob": -0.2341805540997049, "compression_ratio": 1.5681818181818181, "no_speech_prob": 4.0294485188496765e-06}, {"id": 1102, "seek": 552928, "start": 5529.28, "end": 5534.5599999999995, "text": " Colon comma none means give me everything in the first axis, which is the only axis", "tokens": [21408, 22117, 6022, 1355, 976, 385, 1203, 294, 264, 700, 10298, 11, 597, 307, 264, 787, 10298], "temperature": 0.0, "avg_logprob": -0.1671810527839283, "compression_ratio": 1.4950980392156863, "no_speech_prob": 1.5689496649429202e-05}, {"id": 1103, "seek": 552928, "start": 5534.5599999999995, "end": 5535.5599999999995, "text": " we have.", "tokens": [321, 362, 13], "temperature": 0.0, "avg_logprob": -0.1671810527839283, "compression_ratio": 1.4950980392156863, "no_speech_prob": 1.5689496649429202e-05}, {"id": 1104, "seek": 552928, "start": 5535.5599999999995, "end": 5542.48, "text": " And then none is a special indexer, which means add a unit axis here.", "tokens": [400, 550, 6022, 307, 257, 2121, 8186, 260, 11, 597, 1355, 909, 257, 4985, 10298, 510, 13], "temperature": 0.0, "avg_logprob": -0.1671810527839283, "compression_ratio": 1.4950980392156863, "no_speech_prob": 1.5689496649429202e-05}, {"id": 1105, "seek": 552928, "start": 5542.48, "end": 5545.4, "text": " So let me show you.", "tokens": [407, 718, 385, 855, 291, 13], "temperature": 0.0, "avg_logprob": -0.1671810527839283, "compression_ratio": 1.4950980392156863, "no_speech_prob": 1.5689496649429202e-05}, {"id": 1106, "seek": 552928, "start": 5545.4, "end": 5549.679999999999, "text": " That is of shape 50,1.", "tokens": [663, 307, 295, 3909, 2625, 11, 16, 13], "temperature": 0.0, "avg_logprob": -0.1671810527839283, "compression_ratio": 1.4950980392156863, "no_speech_prob": 1.5689496649429202e-05}, {"id": 1107, "seek": 552928, "start": 5549.679999999999, "end": 5552.2, "text": " So it's of rank 2.", "tokens": [407, 309, 311, 295, 6181, 568, 13], "temperature": 0.0, "avg_logprob": -0.1671810527839283, "compression_ratio": 1.4950980392156863, "no_speech_prob": 1.5689496649429202e-05}, {"id": 1108, "seek": 552928, "start": 5552.2, "end": 5554.179999999999, "text": " It has two axes.", "tokens": [467, 575, 732, 35387, 13], "temperature": 0.0, "avg_logprob": -0.1671810527839283, "compression_ratio": 1.4950980392156863, "no_speech_prob": 1.5689496649429202e-05}, {"id": 1109, "seek": 552928, "start": 5554.179999999999, "end": 5556.44, "text": " One of them is a very boring axis, right?", "tokens": [1485, 295, 552, 307, 257, 588, 9989, 10298, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1671810527839283, "compression_ratio": 1.4950980392156863, "no_speech_prob": 1.5689496649429202e-05}, {"id": 1110, "seek": 552928, "start": 5556.44, "end": 5558.7, "text": " It's a length 1 axis.", "tokens": [467, 311, 257, 4641, 502, 10298, 13], "temperature": 0.0, "avg_logprob": -0.1671810527839283, "compression_ratio": 1.4950980392156863, "no_speech_prob": 1.5689496649429202e-05}, {"id": 1111, "seek": 555870, "start": 5558.7, "end": 5561.4, "text": " So let's move this over here.", "tokens": [407, 718, 311, 1286, 341, 670, 510, 13], "temperature": 0.0, "avg_logprob": -0.15710529382678046, "compression_ratio": 1.3860759493670887, "no_speech_prob": 1.1478729902592022e-05}, {"id": 1112, "seek": 555870, "start": 5561.4, "end": 5565.639999999999, "text": " There's 1,50.", "tokens": [821, 311, 502, 11, 2803, 13], "temperature": 0.0, "avg_logprob": -0.15710529382678046, "compression_ratio": 1.3860759493670887, "no_speech_prob": 1.1478729902592022e-05}, {"id": 1113, "seek": 555870, "start": 5565.639999999999, "end": 5571.76, "text": " And then to remind you, the original is just 50.", "tokens": [400, 550, 281, 4160, 291, 11, 264, 3380, 307, 445, 2625, 13], "temperature": 0.0, "avg_logprob": -0.15710529382678046, "compression_ratio": 1.3860759493670887, "no_speech_prob": 1.1478729902592022e-05}, {"id": 1114, "seek": 555870, "start": 5571.76, "end": 5580.84, "text": " So you can see I can put none as a special indexer to introduce a new unit axis there.", "tokens": [407, 291, 393, 536, 286, 393, 829, 6022, 382, 257, 2121, 8186, 260, 281, 5366, 257, 777, 4985, 10298, 456, 13], "temperature": 0.0, "avg_logprob": -0.15710529382678046, "compression_ratio": 1.3860759493670887, "no_speech_prob": 1.1478729902592022e-05}, {"id": 1115, "seek": 555870, "start": 5580.84, "end": 5587.2, "text": " So this thing has 1 row and 50 columns.", "tokens": [407, 341, 551, 575, 502, 5386, 293, 2625, 13766, 13], "temperature": 0.0, "avg_logprob": -0.15710529382678046, "compression_ratio": 1.3860759493670887, "no_speech_prob": 1.1478729902592022e-05}, {"id": 1116, "seek": 558720, "start": 5587.2, "end": 5590.4, "text": " This thing has 50 rows and 1 column.", "tokens": [639, 551, 575, 2625, 13241, 293, 502, 7738, 13], "temperature": 0.0, "avg_logprob": -0.16578325933339644, "compression_ratio": 1.7285067873303168, "no_speech_prob": 2.3687964585406007e-06}, {"id": 1117, "seek": 558720, "start": 5590.4, "end": 5591.4, "text": " So that's what we want, right?", "tokens": [407, 300, 311, 437, 321, 528, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.16578325933339644, "compression_ratio": 1.7285067873303168, "no_speech_prob": 2.3687964585406007e-06}, {"id": 1118, "seek": 558720, "start": 5591.4, "end": 5595.639999999999, "text": " We want 50 rows and 1 column.", "tokens": [492, 528, 2625, 13241, 293, 502, 7738, 13], "temperature": 0.0, "avg_logprob": -0.16578325933339644, "compression_ratio": 1.7285067873303168, "no_speech_prob": 2.3687964585406007e-06}, {"id": 1119, "seek": 558720, "start": 5595.639999999999, "end": 5602.58, "text": " This kind of playing around with ranks and dimensions is going to become increasingly", "tokens": [639, 733, 295, 2433, 926, 365, 21406, 293, 12819, 307, 516, 281, 1813, 12980], "temperature": 0.0, "avg_logprob": -0.16578325933339644, "compression_ratio": 1.7285067873303168, "no_speech_prob": 2.3687964585406007e-06}, {"id": 1120, "seek": 558720, "start": 5602.58, "end": 5607.28, "text": " important in this course and in the deep learning course.", "tokens": [1021, 294, 341, 1164, 293, 294, 264, 2452, 2539, 1164, 13], "temperature": 0.0, "avg_logprob": -0.16578325933339644, "compression_ratio": 1.7285067873303168, "no_speech_prob": 2.3687964585406007e-06}, {"id": 1121, "seek": 558720, "start": 5607.28, "end": 5613.679999999999, "text": " So spend a lot of time slicing with none, slicing with other things, try to create three-dimensional,", "tokens": [407, 3496, 257, 688, 295, 565, 46586, 365, 6022, 11, 46586, 365, 661, 721, 11, 853, 281, 1884, 1045, 12, 18759, 11], "temperature": 0.0, "avg_logprob": -0.16578325933339644, "compression_ratio": 1.7285067873303168, "no_speech_prob": 2.3687964585406007e-06}, {"id": 1122, "seek": 558720, "start": 5613.679999999999, "end": 5615.48, "text": " four-dimensional tensors and so forth.", "tokens": [1451, 12, 18759, 10688, 830, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.16578325933339644, "compression_ratio": 1.7285067873303168, "no_speech_prob": 2.3687964585406007e-06}, {"id": 1123, "seek": 561548, "start": 5615.48, "end": 5617.24, "text": " I'll show you two tricks.", "tokens": [286, 603, 855, 291, 732, 11733, 13], "temperature": 0.0, "avg_logprob": -0.14326000213623047, "compression_ratio": 1.6608695652173913, "no_speech_prob": 4.28931571150315e-06}, {"id": 1124, "seek": 561548, "start": 5617.24, "end": 5621.4, "text": " The first is you never ever need to write comma colon.", "tokens": [440, 700, 307, 291, 1128, 1562, 643, 281, 2464, 22117, 8255, 13], "temperature": 0.0, "avg_logprob": -0.14326000213623047, "compression_ratio": 1.6608695652173913, "no_speech_prob": 4.28931571150315e-06}, {"id": 1125, "seek": 561548, "start": 5621.4, "end": 5622.679999999999, "text": " It's always assumed.", "tokens": [467, 311, 1009, 15895, 13], "temperature": 0.0, "avg_logprob": -0.14326000213623047, "compression_ratio": 1.6608695652173913, "no_speech_prob": 4.28931571150315e-06}, {"id": 1126, "seek": 561548, "start": 5622.679999999999, "end": 5628.04, "text": " So if I delete that, this is exactly the same thing.", "tokens": [407, 498, 286, 12097, 300, 11, 341, 307, 2293, 264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.14326000213623047, "compression_ratio": 1.6608695652173913, "no_speech_prob": 4.28931571150315e-06}, {"id": 1127, "seek": 561548, "start": 5628.04, "end": 5631.959999999999, "text": " And you'll see that in code all the time, so you need to recognize it.", "tokens": [400, 291, 603, 536, 300, 294, 3089, 439, 264, 565, 11, 370, 291, 643, 281, 5521, 309, 13], "temperature": 0.0, "avg_logprob": -0.14326000213623047, "compression_ratio": 1.6608695652173913, "no_speech_prob": 4.28931571150315e-06}, {"id": 1128, "seek": 561548, "start": 5631.959999999999, "end": 5639.24, "text": " The second trick is this is adding an axis in the second dimension, or I guess the index", "tokens": [440, 1150, 4282, 307, 341, 307, 5127, 364, 10298, 294, 264, 1150, 10139, 11, 420, 286, 2041, 264, 8186], "temperature": 0.0, "avg_logprob": -0.14326000213623047, "compression_ratio": 1.6608695652173913, "no_speech_prob": 4.28931571150315e-06}, {"id": 1129, "seek": 561548, "start": 5639.24, "end": 5640.5599999999995, "text": " 1 dimension.", "tokens": [502, 10139, 13], "temperature": 0.0, "avg_logprob": -0.14326000213623047, "compression_ratio": 1.6608695652173913, "no_speech_prob": 4.28931571150315e-06}, {"id": 1130, "seek": 561548, "start": 5640.5599999999995, "end": 5644.0599999999995, "text": " What if I always want to put it in the last dimension?", "tokens": [708, 498, 286, 1009, 528, 281, 829, 309, 294, 264, 1036, 10139, 30], "temperature": 0.0, "avg_logprob": -0.14326000213623047, "compression_ratio": 1.6608695652173913, "no_speech_prob": 4.28931571150315e-06}, {"id": 1131, "seek": 564406, "start": 5644.06, "end": 5650.56, "text": " And often our tensors change dimensions without us looking, because you went from a 1-channel", "tokens": [400, 2049, 527, 10688, 830, 1319, 12819, 1553, 505, 1237, 11, 570, 291, 1437, 490, 257, 502, 12, 339, 11444], "temperature": 0.0, "avg_logprob": -0.1337207761304132, "compression_ratio": 1.7410358565737052, "no_speech_prob": 3.8449074963864405e-06}, {"id": 1132, "seek": 564406, "start": 5650.56, "end": 5655.400000000001, "text": " image to a 3-channel image, or you went from a single image to a mini batch of images.", "tokens": [3256, 281, 257, 805, 12, 339, 11444, 3256, 11, 420, 291, 1437, 490, 257, 2167, 3256, 281, 257, 8382, 15245, 295, 5267, 13], "temperature": 0.0, "avg_logprob": -0.1337207761304132, "compression_ratio": 1.7410358565737052, "no_speech_prob": 3.8449074963864405e-06}, {"id": 1133, "seek": 564406, "start": 5655.400000000001, "end": 5658.280000000001, "text": " Like suddenly you get new dimensions appearing.", "tokens": [1743, 5800, 291, 483, 777, 12819, 19870, 13], "temperature": 0.0, "avg_logprob": -0.1337207761304132, "compression_ratio": 1.7410358565737052, "no_speech_prob": 3.8449074963864405e-06}, {"id": 1134, "seek": 564406, "start": 5658.280000000001, "end": 5662.84, "text": " So to make things general, I would say this, dot dot dot.", "tokens": [407, 281, 652, 721, 2674, 11, 286, 576, 584, 341, 11, 5893, 5893, 5893, 13], "temperature": 0.0, "avg_logprob": -0.1337207761304132, "compression_ratio": 1.7410358565737052, "no_speech_prob": 3.8449074963864405e-06}, {"id": 1135, "seek": 564406, "start": 5662.84, "end": 5668.080000000001, "text": " Dot dot dot means as many dimensions as you need to fill this up.", "tokens": [38753, 5893, 5893, 1355, 382, 867, 12819, 382, 291, 643, 281, 2836, 341, 493, 13], "temperature": 0.0, "avg_logprob": -0.1337207761304132, "compression_ratio": 1.7410358565737052, "no_speech_prob": 3.8449074963864405e-06}, {"id": 1136, "seek": 564406, "start": 5668.080000000001, "end": 5672.4800000000005, "text": " And so in this case, it's exactly the same thing, but I would always try to write it", "tokens": [400, 370, 294, 341, 1389, 11, 309, 311, 2293, 264, 912, 551, 11, 457, 286, 576, 1009, 853, 281, 2464, 309], "temperature": 0.0, "avg_logprob": -0.1337207761304132, "compression_ratio": 1.7410358565737052, "no_speech_prob": 3.8449074963864405e-06}, {"id": 1137, "seek": 567248, "start": 5672.48, "end": 5678.679999999999, "text": " that way, because it means it's going to continue to work as I get higher dimensional tensors.", "tokens": [300, 636, 11, 570, 309, 1355, 309, 311, 516, 281, 2354, 281, 589, 382, 286, 483, 2946, 18795, 10688, 830, 13], "temperature": 0.0, "avg_logprob": -0.14583989131597824, "compression_ratio": 1.4364640883977902, "no_speech_prob": 2.482475792930927e-06}, {"id": 1138, "seek": 567248, "start": 5678.679999999999, "end": 5687.0, "text": " So in this case, I want 50 rows in one column, so I'll call that x1.", "tokens": [407, 294, 341, 1389, 11, 286, 528, 2625, 13241, 294, 472, 7738, 11, 370, 286, 603, 818, 300, 2031, 16, 13], "temperature": 0.0, "avg_logprob": -0.14583989131597824, "compression_ratio": 1.4364640883977902, "no_speech_prob": 2.482475792930927e-06}, {"id": 1139, "seek": 567248, "start": 5687.0, "end": 5690.08, "text": " So let's now use that here.", "tokens": [407, 718, 311, 586, 764, 300, 510, 13], "temperature": 0.0, "avg_logprob": -0.14583989131597824, "compression_ratio": 1.4364640883977902, "no_speech_prob": 2.482475792930927e-06}, {"id": 1140, "seek": 567248, "start": 5690.08, "end": 5700.2, "text": " And so this is now a 2D array, and so I can create my", "tokens": [400, 370, 341, 307, 586, 257, 568, 35, 10225, 11, 293, 370, 286, 393, 1884, 452], "temperature": 0.0, "avg_logprob": -0.14583989131597824, "compression_ratio": 1.4364640883977902, "no_speech_prob": 2.482475792930927e-06}, {"id": 1141, "seek": 567248, "start": 5700.2, "end": 5702.24, "text": " random forest.", "tokens": [4974, 6719, 13], "temperature": 0.0, "avg_logprob": -0.14583989131597824, "compression_ratio": 1.4364640883977902, "no_speech_prob": 2.482475792930927e-06}, {"id": 1142, "seek": 570224, "start": 5702.24, "end": 5709.4, "text": " So then I could plot that, and this is where you're going to have to turn your brains on,", "tokens": [407, 550, 286, 727, 7542, 300, 11, 293, 341, 307, 689, 291, 434, 516, 281, 362, 281, 1261, 428, 15442, 322, 11], "temperature": 0.0, "avg_logprob": -0.17245426177978515, "compression_ratio": 1.4267515923566878, "no_speech_prob": 1.3007066627324093e-05}, {"id": 1143, "seek": 570224, "start": 5709.4, "end": 5713.88, "text": " because the folks this morning got this very quickly, which was super impressive.", "tokens": [570, 264, 4024, 341, 2446, 658, 341, 588, 2661, 11, 597, 390, 1687, 8992, 13], "temperature": 0.0, "avg_logprob": -0.17245426177978515, "compression_ratio": 1.4267515923566878, "no_speech_prob": 1.3007066627324093e-05}, {"id": 1144, "seek": 570224, "start": 5713.88, "end": 5722.679999999999, "text": " I'm going to plot y train against m.predict x train.", "tokens": [286, 478, 516, 281, 7542, 288, 3847, 1970, 275, 13, 79, 24945, 2031, 3847, 13], "temperature": 0.0, "avg_logprob": -0.17245426177978515, "compression_ratio": 1.4267515923566878, "no_speech_prob": 1.3007066627324093e-05}, {"id": 1145, "seek": 572268, "start": 5722.68, "end": 5734.72, "text": " Before I hit go, what is this going to look like?", "tokens": [4546, 286, 2045, 352, 11, 437, 307, 341, 516, 281, 574, 411, 30], "temperature": 0.0, "avg_logprob": -0.2589391767978668, "compression_ratio": 1.379746835443038, "no_speech_prob": 1.3631262845592573e-05}, {"id": 1146, "seek": 572268, "start": 5734.72, "end": 5737.4400000000005, "text": " It should basically be the same.", "tokens": [467, 820, 1936, 312, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.2589391767978668, "compression_ratio": 1.379746835443038, "no_speech_prob": 1.3631262845592573e-05}, {"id": 1147, "seek": 572268, "start": 5737.4400000000005, "end": 5740.740000000001, "text": " Our predictions hopefully are the same as the actuals.", "tokens": [2621, 21264, 4696, 366, 264, 912, 382, 264, 3539, 82, 13], "temperature": 0.0, "avg_logprob": -0.2589391767978668, "compression_ratio": 1.379746835443038, "no_speech_prob": 1.3631262845592573e-05}, {"id": 1148, "seek": 572268, "start": 5740.740000000001, "end": 5742.3, "text": " So this should fall on the line.", "tokens": [407, 341, 820, 2100, 322, 264, 1622, 13], "temperature": 0.0, "avg_logprob": -0.2589391767978668, "compression_ratio": 1.379746835443038, "no_speech_prob": 1.3631262845592573e-05}, {"id": 1149, "seek": 572268, "start": 5742.3, "end": 5752.200000000001, "text": " But there's some randomness, so it won't quite.", "tokens": [583, 456, 311, 512, 4974, 1287, 11, 370, 309, 1582, 380, 1596, 13], "temperature": 0.0, "avg_logprob": -0.2589391767978668, "compression_ratio": 1.379746835443038, "no_speech_prob": 1.3631262845592573e-05}, {"id": 1150, "seek": 575220, "start": 5752.2, "end": 5754.8, "text": " So that's cool.", "tokens": [407, 300, 311, 1627, 13], "temperature": 0.0, "avg_logprob": -0.156321851218619, "compression_ratio": 1.1956521739130435, "no_speech_prob": 2.3552473066956736e-05}, {"id": 1151, "seek": 575220, "start": 5754.8, "end": 5758.72, "text": " That was the easy one.", "tokens": [663, 390, 264, 1858, 472, 13], "temperature": 0.0, "avg_logprob": -0.156321851218619, "compression_ratio": 1.1956521739130435, "no_speech_prob": 2.3552473066956736e-05}, {"id": 1152, "seek": 575220, "start": 5758.72, "end": 5769.24, "text": " Let's now do the hard one, the fun one.", "tokens": [961, 311, 586, 360, 264, 1152, 472, 11, 264, 1019, 472, 13], "temperature": 0.0, "avg_logprob": -0.156321851218619, "compression_ratio": 1.1956521739130435, "no_speech_prob": 2.3552473066956736e-05}, {"id": 1153, "seek": 575220, "start": 5769.24, "end": 5781.32, "text": " What's that going to look like?", "tokens": [708, 311, 300, 516, 281, 574, 411, 30], "temperature": 0.0, "avg_logprob": -0.156321851218619, "compression_ratio": 1.1956521739130435, "no_speech_prob": 2.3552473066956736e-05}, {"id": 1154, "seek": 578132, "start": 5781.32, "end": 5784.799999999999, "text": " So I'm going to say no, but nice try.", "tokens": [407, 286, 478, 516, 281, 584, 572, 11, 457, 1481, 853, 13], "temperature": 0.0, "avg_logprob": -0.1344698849846335, "compression_ratio": 1.6941176470588235, "no_speech_prob": 8.139651981764473e-06}, {"id": 1155, "seek": 578132, "start": 5784.799999999999, "end": 5788.2, "text": " It's like, hey, we're extrapolating to the validation.", "tokens": [467, 311, 411, 11, 4177, 11, 321, 434, 48224, 990, 281, 264, 24071, 13], "temperature": 0.0, "avg_logprob": -0.1344698849846335, "compression_ratio": 1.6941176470588235, "no_speech_prob": 8.139651981764473e-06}, {"id": 1156, "seek": 578132, "start": 5788.2, "end": 5797.96, "text": " That's what I'd like it to look like, but that's not what it is going to look like.", "tokens": [663, 311, 437, 286, 1116, 411, 309, 281, 574, 411, 11, 457, 300, 311, 406, 437, 309, 307, 516, 281, 574, 411, 13], "temperature": 0.0, "avg_logprob": -0.1344698849846335, "compression_ratio": 1.6941176470588235, "no_speech_prob": 8.139651981764473e-06}, {"id": 1157, "seek": 578132, "start": 5797.96, "end": 5806.16, "text": " Think about what trees do, and think about the fact that we have a validation set here", "tokens": [6557, 466, 437, 5852, 360, 11, 293, 519, 466, 264, 1186, 300, 321, 362, 257, 24071, 992, 510], "temperature": 0.0, "avg_logprob": -0.1344698849846335, "compression_ratio": 1.6941176470588235, "no_speech_prob": 8.139651981764473e-06}, {"id": 1158, "seek": 578132, "start": 5806.16, "end": 5810.16, "text": " and a training set here.", "tokens": [293, 257, 3097, 992, 510, 13], "temperature": 0.0, "avg_logprob": -0.1344698849846335, "compression_ratio": 1.6941176470588235, "no_speech_prob": 8.139651981764473e-06}, {"id": 1159, "seek": 581016, "start": 5810.16, "end": 5814.36, "text": " So think about a forest is just a bunch of trees.", "tokens": [407, 519, 466, 257, 6719, 307, 445, 257, 3840, 295, 5852, 13], "temperature": 0.0, "avg_logprob": -0.33387647356305805, "compression_ratio": 1.5314009661835748, "no_speech_prob": 2.2125446776044555e-05}, {"id": 1160, "seek": 581016, "start": 5814.36, "end": 5817.96, "text": " The first tree is going to have a go.", "tokens": [440, 700, 4230, 307, 516, 281, 362, 257, 352, 13], "temperature": 0.0, "avg_logprob": -0.33387647356305805, "compression_ratio": 1.5314009661835748, "no_speech_prob": 2.2125446776044555e-05}, {"id": 1161, "seek": 581016, "start": 5817.96, "end": 5824.44, "text": " Can you pass that to Melissa?", "tokens": [1664, 291, 1320, 300, 281, 22844, 30], "temperature": 0.0, "avg_logprob": -0.33387647356305805, "compression_ratio": 1.5314009661835748, "no_speech_prob": 2.2125446776044555e-05}, {"id": 1162, "seek": 581016, "start": 5824.44, "end": 5827.2, "text": " Will it start grouping the dots?", "tokens": [3099, 309, 722, 40149, 264, 15026, 30], "temperature": 0.0, "avg_logprob": -0.33387647356305805, "compression_ratio": 1.5314009661835748, "no_speech_prob": 2.2125446776044555e-05}, {"id": 1163, "seek": 581016, "start": 5827.2, "end": 5829.76, "text": " Yeah, that's what it does.", "tokens": [865, 11, 300, 311, 437, 309, 775, 13], "temperature": 0.0, "avg_logprob": -0.33387647356305805, "compression_ratio": 1.5314009661835748, "no_speech_prob": 2.2125446776044555e-05}, {"id": 1164, "seek": 581016, "start": 5829.76, "end": 5835.5599999999995, "text": " But let's think about how it groups the dots.", "tokens": [583, 718, 311, 519, 466, 577, 309, 3935, 264, 15026, 13], "temperature": 0.0, "avg_logprob": -0.33387647356305805, "compression_ratio": 1.5314009661835748, "no_speech_prob": 2.2125446776044555e-05}, {"id": 1165, "seek": 581016, "start": 5835.5599999999995, "end": 5839.0, "text": " I'm guessing since all the new data is actually outside of the original scope, it's all going", "tokens": [286, 478, 17939, 1670, 439, 264, 777, 1412, 307, 767, 2380, 295, 264, 3380, 11923, 11, 309, 311, 439, 516], "temperature": 0.0, "avg_logprob": -0.33387647356305805, "compression_ratio": 1.5314009661835748, "no_speech_prob": 2.2125446776044555e-05}, {"id": 1166, "seek": 583900, "start": 5839.0, "end": 5840.48, "text": " to be basically the same.", "tokens": [281, 312, 1936, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.19811107431139266, "compression_ratio": 2.0541871921182264, "no_speech_prob": 7.889237167546526e-06}, {"id": 1167, "seek": 583900, "start": 5840.48, "end": 5843.48, "text": " It's like one huge group.", "tokens": [467, 311, 411, 472, 2603, 1594, 13], "temperature": 0.0, "avg_logprob": -0.19811107431139266, "compression_ratio": 2.0541871921182264, "no_speech_prob": 7.889237167546526e-06}, {"id": 1168, "seek": 583900, "start": 5843.48, "end": 5846.92, "text": " Forget the forest, let's create one tree.", "tokens": [18675, 264, 6719, 11, 718, 311, 1884, 472, 4230, 13], "temperature": 0.0, "avg_logprob": -0.19811107431139266, "compression_ratio": 2.0541871921182264, "no_speech_prob": 7.889237167546526e-06}, {"id": 1169, "seek": 583900, "start": 5846.92, "end": 5850.72, "text": " So we're probably going to split somewhere around here first, and then we're going to", "tokens": [407, 321, 434, 1391, 516, 281, 7472, 4079, 926, 510, 700, 11, 293, 550, 321, 434, 516, 281], "temperature": 0.0, "avg_logprob": -0.19811107431139266, "compression_ratio": 2.0541871921182264, "no_speech_prob": 7.889237167546526e-06}, {"id": 1170, "seek": 583900, "start": 5850.72, "end": 5853.84, "text": " probably split somewhere around here, and then we're going to split somewhere around", "tokens": [1391, 7472, 4079, 926, 510, 11, 293, 550, 321, 434, 516, 281, 7472, 4079, 926], "temperature": 0.0, "avg_logprob": -0.19811107431139266, "compression_ratio": 2.0541871921182264, "no_speech_prob": 7.889237167546526e-06}, {"id": 1171, "seek": 583900, "start": 5853.84, "end": 5856.18, "text": " here and somewhere around here.", "tokens": [510, 293, 4079, 926, 510, 13], "temperature": 0.0, "avg_logprob": -0.19811107431139266, "compression_ratio": 2.0541871921182264, "no_speech_prob": 7.889237167546526e-06}, {"id": 1172, "seek": 583900, "start": 5856.18, "end": 5860.74, "text": " And so our final split is here.", "tokens": [400, 370, 527, 2572, 7472, 307, 510, 13], "temperature": 0.0, "avg_logprob": -0.19811107431139266, "compression_ratio": 2.0541871921182264, "no_speech_prob": 7.889237167546526e-06}, {"id": 1173, "seek": 583900, "start": 5860.74, "end": 5868.16, "text": " So our prediction, when we say, okay, let's take this one, and so it's going to put that", "tokens": [407, 527, 17630, 11, 562, 321, 584, 11, 1392, 11, 718, 311, 747, 341, 472, 11, 293, 370, 309, 311, 516, 281, 829, 300], "temperature": 0.0, "avg_logprob": -0.19811107431139266, "compression_ratio": 2.0541871921182264, "no_speech_prob": 7.889237167546526e-06}, {"id": 1174, "seek": 586816, "start": 5868.16, "end": 5875.0, "text": " through the forest and end up predicting this average.", "tokens": [807, 264, 6719, 293, 917, 493, 32884, 341, 4274, 13], "temperature": 0.0, "avg_logprob": -0.18605484359565821, "compression_ratio": 1.6984924623115578, "no_speech_prob": 5.955115739197936e-06}, {"id": 1175, "seek": 586816, "start": 5875.0, "end": 5878.88, "text": " It can't predict anything higher than that, because there is nothing higher than that", "tokens": [467, 393, 380, 6069, 1340, 2946, 813, 300, 11, 570, 456, 307, 1825, 2946, 813, 300], "temperature": 0.0, "avg_logprob": -0.18605484359565821, "compression_ratio": 1.6984924623115578, "no_speech_prob": 5.955115739197936e-06}, {"id": 1176, "seek": 586816, "start": 5878.88, "end": 5879.88, "text": " to average.", "tokens": [281, 4274, 13], "temperature": 0.0, "avg_logprob": -0.18605484359565821, "compression_ratio": 1.6984924623115578, "no_speech_prob": 5.955115739197936e-06}, {"id": 1177, "seek": 586816, "start": 5879.88, "end": 5882.2, "text": " So this is really important to realize.", "tokens": [407, 341, 307, 534, 1021, 281, 4325, 13], "temperature": 0.0, "avg_logprob": -0.18605484359565821, "compression_ratio": 1.6984924623115578, "no_speech_prob": 5.955115739197936e-06}, {"id": 1178, "seek": 586816, "start": 5882.2, "end": 5884.48, "text": " A random forest is not magic.", "tokens": [316, 4974, 6719, 307, 406, 5585, 13], "temperature": 0.0, "avg_logprob": -0.18605484359565821, "compression_ratio": 1.6984924623115578, "no_speech_prob": 5.955115739197936e-06}, {"id": 1179, "seek": 586816, "start": 5884.48, "end": 5890.2, "text": " It's just returning the average of nearby observations where nearby is kind of in this", "tokens": [467, 311, 445, 12678, 264, 4274, 295, 11184, 18163, 689, 11184, 307, 733, 295, 294, 341], "temperature": 0.0, "avg_logprob": -0.18605484359565821, "compression_ratio": 1.6984924623115578, "no_speech_prob": 5.955115739197936e-06}, {"id": 1180, "seek": 586816, "start": 5890.2, "end": 5892.24, "text": " tree space.", "tokens": [4230, 1901, 13], "temperature": 0.0, "avg_logprob": -0.18605484359565821, "compression_ratio": 1.6984924623115578, "no_speech_prob": 5.955115739197936e-06}, {"id": 1181, "seek": 586816, "start": 5892.24, "end": 5893.92, "text": " So let's run it.", "tokens": [407, 718, 311, 1190, 309, 13], "temperature": 0.0, "avg_logprob": -0.18605484359565821, "compression_ratio": 1.6984924623115578, "no_speech_prob": 5.955115739197936e-06}, {"id": 1182, "seek": 589392, "start": 5893.92, "end": 5898.36, "text": " Let's see if Tim's right.", "tokens": [961, 311, 536, 498, 7172, 311, 558, 13], "temperature": 0.0, "avg_logprob": -0.16513994216918945, "compression_ratio": 1.583710407239819, "no_speech_prob": 9.223444067174569e-06}, {"id": 1183, "seek": 589392, "start": 5898.36, "end": 5903.32, "text": " Holy shit, that's awful.", "tokens": [6295, 4611, 11, 300, 311, 11232, 13], "temperature": 0.0, "avg_logprob": -0.16513994216918945, "compression_ratio": 1.583710407239819, "no_speech_prob": 9.223444067174569e-06}, {"id": 1184, "seek": 589392, "start": 5903.32, "end": 5908.64, "text": " And if you don't know how random forests work, then this is going to totally screw.", "tokens": [400, 498, 291, 500, 380, 458, 577, 4974, 21700, 589, 11, 550, 341, 307, 516, 281, 3879, 5630, 13], "temperature": 0.0, "avg_logprob": -0.16513994216918945, "compression_ratio": 1.583710407239819, "no_speech_prob": 9.223444067174569e-06}, {"id": 1185, "seek": 589392, "start": 5908.64, "end": 5913.4400000000005, "text": " If you think that it's actually going to be able to extrapolate to any kind of data it", "tokens": [759, 291, 519, 300, 309, 311, 767, 516, 281, 312, 1075, 281, 48224, 473, 281, 604, 733, 295, 1412, 309], "temperature": 0.0, "avg_logprob": -0.16513994216918945, "compression_ratio": 1.583710407239819, "no_speech_prob": 9.223444067174569e-06}, {"id": 1186, "seek": 589392, "start": 5913.4400000000005, "end": 5919.16, "text": " hasn't seen before, particularly future time periods, it's just not.", "tokens": [6132, 380, 1612, 949, 11, 4098, 2027, 565, 13804, 11, 309, 311, 445, 406, 13], "temperature": 0.0, "avg_logprob": -0.16513994216918945, "compression_ratio": 1.583710407239819, "no_speech_prob": 9.223444067174569e-06}, {"id": 1187, "seek": 589392, "start": 5919.16, "end": 5920.46, "text": " It just can't.", "tokens": [467, 445, 393, 380, 13], "temperature": 0.0, "avg_logprob": -0.16513994216918945, "compression_ratio": 1.583710407239819, "no_speech_prob": 9.223444067174569e-06}, {"id": 1188, "seek": 589392, "start": 5920.46, "end": 5922.76, "text": " It's just averaging stuff it's already seen.", "tokens": [467, 311, 445, 47308, 1507, 309, 311, 1217, 1612, 13], "temperature": 0.0, "avg_logprob": -0.16513994216918945, "compression_ratio": 1.583710407239819, "no_speech_prob": 9.223444067174569e-06}, {"id": 1189, "seek": 592276, "start": 5922.76, "end": 5925.4800000000005, "text": " That's all it can do.", "tokens": [663, 311, 439, 309, 393, 360, 13], "temperature": 0.0, "avg_logprob": -0.09601043462753296, "compression_ratio": 1.5612244897959184, "no_speech_prob": 1.8162201058657956e-06}, {"id": 1190, "seek": 592276, "start": 5925.4800000000005, "end": 5931.6, "text": " So we're going to be talking about how to avoid this problem.", "tokens": [407, 321, 434, 516, 281, 312, 1417, 466, 577, 281, 5042, 341, 1154, 13], "temperature": 0.0, "avg_logprob": -0.09601043462753296, "compression_ratio": 1.5612244897959184, "no_speech_prob": 1.8162201058657956e-06}, {"id": 1191, "seek": 592276, "start": 5931.6, "end": 5938.8, "text": " We talked a little bit in the last lesson about trying to avoid it by just avoiding", "tokens": [492, 2825, 257, 707, 857, 294, 264, 1036, 6898, 466, 1382, 281, 5042, 309, 538, 445, 20220], "temperature": 0.0, "avg_logprob": -0.09601043462753296, "compression_ratio": 1.5612244897959184, "no_speech_prob": 1.8162201058657956e-06}, {"id": 1192, "seek": 592276, "start": 5938.8, "end": 5945.24, "text": " unnecessary time-dependent variables where we can.", "tokens": [19350, 565, 12, 36763, 317, 9102, 689, 321, 393, 13], "temperature": 0.0, "avg_logprob": -0.09601043462753296, "compression_ratio": 1.5612244897959184, "no_speech_prob": 1.8162201058657956e-06}, {"id": 1193, "seek": 592276, "start": 5945.24, "end": 5951.56, "text": " But in the end, if you really have a time series that looks like this, we actually have", "tokens": [583, 294, 264, 917, 11, 498, 291, 534, 362, 257, 565, 2638, 300, 1542, 411, 341, 11, 321, 767, 362], "temperature": 0.0, "avg_logprob": -0.09601043462753296, "compression_ratio": 1.5612244897959184, "no_speech_prob": 1.8162201058657956e-06}, {"id": 1194, "seek": 595156, "start": 5951.56, "end": 5954.160000000001, "text": " to deal with the problem.", "tokens": [281, 2028, 365, 264, 1154, 13], "temperature": 0.0, "avg_logprob": -0.1880805553534092, "compression_ratio": 1.6770833333333333, "no_speech_prob": 1.2219024029036518e-05}, {"id": 1195, "seek": 595156, "start": 5954.160000000001, "end": 5959.120000000001, "text": " So one way we could deal with the problem would be use like a neural net.", "tokens": [407, 472, 636, 321, 727, 2028, 365, 264, 1154, 576, 312, 764, 411, 257, 18161, 2533, 13], "temperature": 0.0, "avg_logprob": -0.1880805553534092, "compression_ratio": 1.6770833333333333, "no_speech_prob": 1.2219024029036518e-05}, {"id": 1196, "seek": 595156, "start": 5959.120000000001, "end": 5967.240000000001, "text": " Use something that actually has a function or shape that can actually fit something like", "tokens": [8278, 746, 300, 767, 575, 257, 2445, 420, 3909, 300, 393, 767, 3318, 746, 411], "temperature": 0.0, "avg_logprob": -0.1880805553534092, "compression_ratio": 1.6770833333333333, "no_speech_prob": 1.2219024029036518e-05}, {"id": 1197, "seek": 595156, "start": 5967.240000000001, "end": 5968.240000000001, "text": " this.", "tokens": [341, 13], "temperature": 0.0, "avg_logprob": -0.1880805553534092, "compression_ratio": 1.6770833333333333, "no_speech_prob": 1.2219024029036518e-05}, {"id": 1198, "seek": 595156, "start": 5968.240000000001, "end": 5971.04, "text": " So then it will extrapolate nicely.", "tokens": [407, 550, 309, 486, 48224, 473, 9594, 13], "temperature": 0.0, "avg_logprob": -0.1880805553534092, "compression_ratio": 1.6770833333333333, "no_speech_prob": 1.2219024029036518e-05}, {"id": 1199, "seek": 595156, "start": 5971.04, "end": 5975.52, "text": " Another approach would be to use all the time series techniques you guys are learning about", "tokens": [3996, 3109, 576, 312, 281, 764, 439, 264, 565, 2638, 7512, 291, 1074, 366, 2539, 466], "temperature": 0.0, "avg_logprob": -0.1880805553534092, "compression_ratio": 1.6770833333333333, "no_speech_prob": 1.2219024029036518e-05}, {"id": 1200, "seek": 597552, "start": 5975.52, "end": 5983.0, "text": " in the morning class to fit some kind of time series and then detrend it.", "tokens": [294, 264, 2446, 1508, 281, 3318, 512, 733, 295, 565, 2638, 293, 550, 1141, 4542, 309, 13], "temperature": 0.0, "avg_logprob": -0.1764368956116424, "compression_ratio": 1.6585365853658536, "no_speech_prob": 2.4439905246254057e-06}, {"id": 1201, "seek": 597552, "start": 5983.0, "end": 5987.64, "text": " And so then you'll end up with detrended dots and then use the random forest to predict", "tokens": [400, 370, 550, 291, 603, 917, 493, 365, 1141, 4542, 292, 15026, 293, 550, 764, 264, 4974, 6719, 281, 6069], "temperature": 0.0, "avg_logprob": -0.1764368956116424, "compression_ratio": 1.6585365853658536, "no_speech_prob": 2.4439905246254057e-06}, {"id": 1202, "seek": 597552, "start": 5987.64, "end": 5989.84, "text": " those.", "tokens": [729, 13], "temperature": 0.0, "avg_logprob": -0.1764368956116424, "compression_ratio": 1.6585365853658536, "no_speech_prob": 2.4439905246254057e-06}, {"id": 1203, "seek": 597552, "start": 5989.84, "end": 5996.52, "text": " And that's particularly cool, because if you're, imagine that your random forest was actually", "tokens": [400, 300, 311, 4098, 1627, 11, 570, 498, 291, 434, 11, 3811, 300, 428, 4974, 6719, 390, 767], "temperature": 0.0, "avg_logprob": -0.1764368956116424, "compression_ratio": 1.6585365853658536, "no_speech_prob": 2.4439905246254057e-06}, {"id": 1204, "seek": 597552, "start": 5996.52, "end": 6003.320000000001, "text": " trying to predict data that, I don't know, maybe it was two different states.", "tokens": [1382, 281, 6069, 1412, 300, 11, 286, 500, 380, 458, 11, 1310, 309, 390, 732, 819, 4368, 13], "temperature": 0.0, "avg_logprob": -0.1764368956116424, "compression_ratio": 1.6585365853658536, "no_speech_prob": 2.4439905246254057e-06}, {"id": 1205, "seek": 600332, "start": 6003.32, "end": 6009.5199999999995, "text": " And so the blue ones are down here and the red ones are up here.", "tokens": [400, 370, 264, 3344, 2306, 366, 760, 510, 293, 264, 2182, 2306, 366, 493, 510, 13], "temperature": 0.0, "avg_logprob": -0.17731841834815773, "compression_ratio": 1.7435897435897436, "no_speech_prob": 9.818247235671151e-06}, {"id": 1206, "seek": 600332, "start": 6009.5199999999995, "end": 6015.36, "text": " Now if you try to use a random forest, it's going to do a pretty crappy job because time", "tokens": [823, 498, 291, 853, 281, 764, 257, 4974, 6719, 11, 309, 311, 516, 281, 360, 257, 1238, 36531, 1691, 570, 565], "temperature": 0.0, "avg_logprob": -0.17731841834815773, "compression_ratio": 1.7435897435897436, "no_speech_prob": 9.818247235671151e-06}, {"id": 1207, "seek": 600332, "start": 6015.36, "end": 6016.759999999999, "text": " is going to seem much more important.", "tokens": [307, 516, 281, 1643, 709, 544, 1021, 13], "temperature": 0.0, "avg_logprob": -0.17731841834815773, "compression_ratio": 1.7435897435897436, "no_speech_prob": 9.818247235671151e-06}, {"id": 1208, "seek": 600332, "start": 6016.759999999999, "end": 6021.759999999999, "text": " So it's basically still going to split like this, and then it's going to split like this.", "tokens": [407, 309, 311, 1936, 920, 516, 281, 7472, 411, 341, 11, 293, 550, 309, 311, 516, 281, 7472, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.17731841834815773, "compression_ratio": 1.7435897435897436, "no_speech_prob": 9.818247235671151e-06}, {"id": 1209, "seek": 600332, "start": 6021.759999999999, "end": 6026.08, "text": " And then finally, once it kind of gets down to this piece, it'll be like, oh, now I can", "tokens": [400, 550, 2721, 11, 1564, 309, 733, 295, 2170, 760, 281, 341, 2522, 11, 309, 603, 312, 411, 11, 1954, 11, 586, 286, 393], "temperature": 0.0, "avg_logprob": -0.17731841834815773, "compression_ratio": 1.7435897435897436, "no_speech_prob": 9.818247235671151e-06}, {"id": 1210, "seek": 600332, "start": 6026.08, "end": 6028.98, "text": " see the difference between the states.", "tokens": [536, 264, 2649, 1296, 264, 4368, 13], "temperature": 0.0, "avg_logprob": -0.17731841834815773, "compression_ratio": 1.7435897435897436, "no_speech_prob": 9.818247235671151e-06}, {"id": 1211, "seek": 602898, "start": 6028.98, "end": 6034.719999999999, "text": " So in other words, when you've got this big time piece going on, you're not going to see", "tokens": [407, 294, 661, 2283, 11, 562, 291, 600, 658, 341, 955, 565, 2522, 516, 322, 11, 291, 434, 406, 516, 281, 536], "temperature": 0.0, "avg_logprob": -0.09493784256923346, "compression_ratio": 1.5402843601895735, "no_speech_prob": 2.5612773697503144e-06}, {"id": 1212, "seek": 602898, "start": 6034.719999999999, "end": 6041.759999999999, "text": " the other relationships in the random forest until every tree deals with time.", "tokens": [264, 661, 6159, 294, 264, 4974, 6719, 1826, 633, 4230, 11215, 365, 565, 13], "temperature": 0.0, "avg_logprob": -0.09493784256923346, "compression_ratio": 1.5402843601895735, "no_speech_prob": 2.5612773697503144e-06}, {"id": 1213, "seek": 602898, "start": 6041.759999999999, "end": 6047.599999999999, "text": " So one way to fix this would be with a gradient boosting machine, GBM.", "tokens": [407, 472, 636, 281, 3191, 341, 576, 312, 365, 257, 16235, 43117, 3479, 11, 460, 18345, 13], "temperature": 0.0, "avg_logprob": -0.09493784256923346, "compression_ratio": 1.5402843601895735, "no_speech_prob": 2.5612773697503144e-06}, {"id": 1214, "seek": 602898, "start": 6047.599999999999, "end": 6052.719999999999, "text": " And what a GBM does is it creates a little tree and runs everything through that first", "tokens": [400, 437, 257, 460, 18345, 775, 307, 309, 7829, 257, 707, 4230, 293, 6676, 1203, 807, 300, 700], "temperature": 0.0, "avg_logprob": -0.09493784256923346, "compression_ratio": 1.5402843601895735, "no_speech_prob": 2.5612773697503144e-06}, {"id": 1215, "seek": 605272, "start": 6052.72, "end": 6059.52, "text": " little tree, which could be like the time tree, and then it calculates the residuals,", "tokens": [707, 4230, 11, 597, 727, 312, 411, 264, 565, 4230, 11, 293, 550, 309, 4322, 1024, 264, 27980, 82, 11], "temperature": 0.0, "avg_logprob": -0.14347063147503397, "compression_ratio": 1.6983471074380165, "no_speech_prob": 1.459373038414924e-06}, {"id": 1216, "seek": 605272, "start": 6059.52, "end": 6063.4400000000005, "text": " and then the next little tree just predicts the residuals.", "tokens": [293, 550, 264, 958, 707, 4230, 445, 6069, 82, 264, 27980, 82, 13], "temperature": 0.0, "avg_logprob": -0.14347063147503397, "compression_ratio": 1.6983471074380165, "no_speech_prob": 1.459373038414924e-06}, {"id": 1217, "seek": 605272, "start": 6063.4400000000005, "end": 6065.52, "text": " So it would be kind of like detrending it.", "tokens": [407, 309, 576, 312, 733, 295, 411, 1141, 4542, 278, 309, 13], "temperature": 0.0, "avg_logprob": -0.14347063147503397, "compression_ratio": 1.6983471074380165, "no_speech_prob": 1.459373038414924e-06}, {"id": 1218, "seek": 605272, "start": 6065.52, "end": 6067.240000000001, "text": " So GBMs handle this.", "tokens": [407, 460, 18345, 82, 4813, 341, 13], "temperature": 0.0, "avg_logprob": -0.14347063147503397, "compression_ratio": 1.6983471074380165, "no_speech_prob": 1.459373038414924e-06}, {"id": 1219, "seek": 605272, "start": 6067.240000000001, "end": 6071.96, "text": " GBMs still can't extrapolate to the future, but at least they can deal with time-dependent", "tokens": [460, 18345, 82, 920, 393, 380, 48224, 473, 281, 264, 2027, 11, 457, 412, 1935, 436, 393, 2028, 365, 565, 12, 36763, 317], "temperature": 0.0, "avg_logprob": -0.14347063147503397, "compression_ratio": 1.6983471074380165, "no_speech_prob": 1.459373038414924e-06}, {"id": 1220, "seek": 605272, "start": 6071.96, "end": 6075.4800000000005, "text": " data more conveniently.", "tokens": [1412, 544, 44375, 13], "temperature": 0.0, "avg_logprob": -0.14347063147503397, "compression_ratio": 1.6983471074380165, "no_speech_prob": 1.459373038414924e-06}, {"id": 1221, "seek": 605272, "start": 6075.4800000000005, "end": 6078.360000000001, "text": " So we're going to be talking about this quite a lot more over the next couple of weeks.", "tokens": [407, 321, 434, 516, 281, 312, 1417, 466, 341, 1596, 257, 688, 544, 670, 264, 958, 1916, 295, 3259, 13], "temperature": 0.0, "avg_logprob": -0.14347063147503397, "compression_ratio": 1.6983471074380165, "no_speech_prob": 1.459373038414924e-06}, {"id": 1222, "seek": 607836, "start": 6078.36, "end": 6084.12, "text": " And in the end, the solution is going to be just use neural nets.", "tokens": [400, 294, 264, 917, 11, 264, 3827, 307, 516, 281, 312, 445, 764, 18161, 36170, 13], "temperature": 0.0, "avg_logprob": -0.17875178173334913, "compression_ratio": 1.5751072961373391, "no_speech_prob": 3.905458470399026e-06}, {"id": 1223, "seek": 607836, "start": 6084.12, "end": 6090.08, "text": " But for now, using some kind of time-series analysis, detrend it, and then use a random", "tokens": [583, 337, 586, 11, 1228, 512, 733, 295, 565, 12, 12484, 530, 5215, 11, 1141, 4542, 309, 11, 293, 550, 764, 257, 4974], "temperature": 0.0, "avg_logprob": -0.17875178173334913, "compression_ratio": 1.5751072961373391, "no_speech_prob": 3.905458470399026e-06}, {"id": 1224, "seek": 607836, "start": 6090.08, "end": 6093.28, "text": " forest on that isn't a bad technique at all.", "tokens": [6719, 322, 300, 1943, 380, 257, 1578, 6532, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.17875178173334913, "compression_ratio": 1.5751072961373391, "no_speech_prob": 3.905458470399026e-06}, {"id": 1225, "seek": 607836, "start": 6093.28, "end": 6097.12, "text": " And if you're playing around with something like the Ecuador Groceries Competition, that", "tokens": [400, 498, 291, 434, 2433, 926, 365, 746, 411, 264, 41558, 12981, 1776, 530, 43634, 11, 300], "temperature": 0.0, "avg_logprob": -0.17875178173334913, "compression_ratio": 1.5751072961373391, "no_speech_prob": 3.905458470399026e-06}, {"id": 1226, "seek": 607836, "start": 6097.12, "end": 6100.5199999999995, "text": " would be a really good thing to fiddle around with.", "tokens": [576, 312, 257, 534, 665, 551, 281, 24553, 2285, 926, 365, 13], "temperature": 0.0, "avg_logprob": -0.17875178173334913, "compression_ratio": 1.5751072961373391, "no_speech_prob": 3.905458470399026e-06}, {"id": 1227, "seek": 610052, "start": 6100.52, "end": 6108.68, "text": " All right, see you next time.", "tokens": [50364, 1057, 558, 11, 536, 291, 958, 565, 13, 50772], "temperature": 0.0, "avg_logprob": -0.5770281444896351, "compression_ratio": 0.7837837837837838, "no_speech_prob": 3.94099042750895e-05}], "language": "en"}