{"text": " Hi everybody and welcome to lesson 13, where we're gonna start talking about back propagation. Before we do, I'll just mention that there was some great success amongst the folks in the class during the week on working with flexing their tensor manipulation muscles. So far the fastest mean shift algorithm, which has a similar accuracy to the one I displayed, is one that actually randomly chooses data points as subset. And I actually think that's a great approach. Very often random sampling and random projections are two excellent ways of speeding up algorithms. So it'll be interesting to see if anybody during the rest of the course comes up with anything faster than random sampling. Also been seeing some good Einstein summation examples and implementations, and continuing to see lots of good diff edit implementations. So congratulations to all the students and I hope those of you following along the videos in the MOOC will be working on the same homework as well and sharing your results on the fast.ai forums. So now we're going to take a look at notebook number three in the normal repo, course 22 p1 repo. And we're going to be looking at the forward and backward passes of a simple multi-layer perceptron neural network. The initial stuff up here is just importing things and just setting some stuff that they're just copying and pasting some stuff from previous notebooks around paths and parameters and stuff like that. So we'll skip over this. So we'll often be kind of copying and pasting stuff from one notebook to another's kind of first cell to get things set up. And I'm also loading in our data for MNIST as tensors. Okay so we to start with need to create the basic architecture of our neural network. And I did mention at the start of the course that we will briefly review everything that we need to cover. So we should briefly review what basic neural networks are and why they are what they are. So to start with let's consider a a linear model. Oops that's not how I do it. So let's start by considering a linear model of well let's take the most simple example possible which is we're going to pick a single pixel from from our MNIST pictures. And we're going to pick a single pixel from our MNIST from from our MNIST pictures. And so that will be our x. And for our for our y values then we'll have some some loss function of how good is this model. Sorry not some loss function. That's created even simpler. For our y value we're going to be looking at how likely is it that this is say the number three based on the value of this one pixel. So the pixel its value will be x and the probability of being the number three we'll call y. And if we just have a linear model then it's going to look like this. And so in this case it's it's saying that the brighter this pixel is the more likely it is that it's the number three. And so there's a few problems with this. The first one obviously is that as a linear model it's very limiting because maybe you know we actually are trying to draw something that looks more like this. So how would you do that? Well there's actually a neat trick we can use to do that. What we could do is well let's first of all talk about something we can't do. Something we can't do is to add a bunch of additional lines. So consider what happens if we say okay well let's add a few different lines. So let's also add this line. So what would be the sum of our two lines? Well the answer is of course that the sum of the two lines will itself be a line. So it's not going to help us at all match the actual curve that we want. So here's the trick. Instead we could create a line like this that actually we could create this line. And now consider what happens if we add this original line with this new what's not a line right it's a it's a two line segments. So what we would get is this everything to the left of this point is going to not be changed if I add these two lines together. Because this is zero all the way and everything to the right of it is going to be reduced. It looks like they've got similar slopes. So we might end up with instead so this would all disappear here and instead we would end up with something like this. And then we could do that again right we could add an additional line that looks a bit like that. So it would go but this time it could go even further out here and it could be something like this. So what if we added that? Well again at the point underneath here it's always zero so it won't do anything at all. But after that it's going to make it even more negatively sloped. And if you can see using this approach we could add up lots of these rectified lines. These lines that truncate at zero. And we could create any shape we want with enough of them. And these lines are very easy to create because actually all we need to do is to create just a regular line. Just create a regular line right which we can move up down left right change its angle whatever. And then just say if it's greater than zero truncate it to zero. Or we could do the opposite for a line going the opposite direction. If it's less than zero we could say truncate it to zero. And that would get rid of as we want this whole section here and make it flat. Okay so these are rectified lines. And so we can sum up a bunch of these together to basically match any arbitrary curve. So let's start by doing that. Oh the other thing we should mention of course is that we're going to have not just one pixel but we're going to have lots of pixels. So to start with the kind of most you know slightly you know the only slightly less simple approach. We could have something where we've got you know pixel number one and pixel number two. We're looking at two different pixels to see how likely they are to be the number three. And so that would allow us to draw more complex shapes that have some kind of surface between them. Okay and then we can do exactly the same thing is to create these surfaces. We can add up lots of these rectified lines together. But now they're going to be kind of rectified planes. But it's going to be exactly the same thing. We're going to be adding together a bunch of lines each one of which is truncated at zero. Okay so that's the quick review. And so to do that we'll start out by just defining a few variables. So n is the number of training examples. m is the number of pixels. c is the number of possible values of our digits. And so here they are. 50,000 samples, 784 pixels and 10 possible outputs. Okay so what we do is to is we basically decide ahead of time how many of these line segment thingies to add up. And so the number that we create in a layer is called the number of hidden nodes or activations. So we'll call that nh. So let's just arbitrarily decide on creating 50 of those. So in order to create lots of lines which we're going to truncate at zero, we can do a matrix multiplication. So with a matrix multiplication we're going to have something where we've got 50,000 rows by 700, was it 784? Yeah by 784 columns. And we're going to multiply that by something with 784 rows and 10 columns. And why is that? Well that's because if we take this very first line of this first vector here, row one, we have 784 values. They're the pixel values of the first image. Okay so this is our first image and so they're each going to each of those 784 values will be multiplied by each of these 784 values in the first column, the zero index column. And that's going to give us a number in our output. So our output is going to be 50,000 images by 10. And so that result, we'll multiply those together and we'll add them up and that result's going to end up over here in this first cell. And so each of these columns is going to eventually represent, if this is a linear model, in this case this is just the example of doing a linear model, each of these cells is going to represent the probability. So this first column will be the probability of being zero and the second column will be the probability of one. The third column will be the probability of being a two and so forth. So that's why we're going to have these 10 columns, each one allowing us to weight the 784 inputs. Now of course we're going to do something a bit more tricky than that, which is actually we're going to have a 784 by 50 input going into a 784 by 50 output to create the 50 hidden layers. Then we're going to truncate those at zero and then multiply that by a 50 by 10 to create our 10 output. So we do it in two steps. So the way SGD works is we start with just, this is our weight matrix here, and this is our data, this is our outputs. The way it works is that this weight matrix is initially filled with random values. Of course this contains our pixel values, this contains results. So W is going to start with random values. So here's our weight matrix. It's going to have, as we discussed, 50,000 by 50 random values. And it's not enough just to multiply, we also have to add. So that's what makes it a linear function. So we call those the biases, the things we add. We can just start those at zeros. So we'll need one for each output, so 50 of those. And so that'll be layer one. And then as we just mentioned, layer two will be a matrix that goes from 50 hidden. And now I'm going to do something totally cheating to simplify some of the calculations for the calculus. I'm only going to create one output. Why am I going to create one output? That's because I'm not going to use cross entropy just yet. Instead I'm going to use MSE. So actually I'm going to create one output, which will literally just be, what number do I think it is? From zero to ten. And so then we're going to compare those to the actual, so these will be our Y predictors. We normally use a little hat for that. And we're going to compare that to our actuals. And yeah, in this case, we're going to compare those to the actuals. To our actuals. And yeah, in this very hacky approach, let's say we predict over here the number nine and the actual is the number two. And we'll compare those together using MSE. Which will be a stupid way to do it, because it's saying that nine is further away from being two than two. Nine is further away from two than it is from four, in terms of how correct it is. Which is not what we want at all. But this is what we're going to do just to simplify our starting point. So that's why we're going to have a single output for this weight matrix, and a single output for this bias. So a linear, let's create a function for putting X through a linear layer with these weights and these biases. So it's a matrix multiply and an add. All right, so we can now try it. So if we multiply our X, oh we're doing X valid this time. So just to clarify, X valid is 10,000 by 784. So if we put X valid through our weights and biases with a linear layer, we end up with a 10,000 by 50. So 10,050 long hidden activations. They're not quite ready yet, because we have to put them through ReLU. And so we're going to clamp at zero. So everything under zero will become zero. And so here's what it looks like when we go through the linear layer, and then the ReLU. And you can see here's a tensor with a bunch of things, some of which are zero, or they're positive. And so that's the result of this matrix multiplication. Okay, so to create our basic MLP multi-layer perceptron from scratch, we will take our mini-batch of X's. XB is a X match. We will create our first layer's output with a linear, and then we will put that through a ReLU. And then that will go through the second linear. So the first one uses the W1B1, okay, these ones. And the second one uses the W2B2. And so we've now got a simple model. And as we hoped, when we pass in the validation set, we get back 10,000 digits. So 10,000 by one. Great. So that's a good start. Okay, so let's use our ridiculous loss function of MSC. So our results is 10,000 by one. And our Y valid is just a vector. Now what's going to happen if I do res minus Y valid? So before you continue in the video, have a think about that. What's going to happen if I do res minus Y valid by thinking about the NumPy broadcasting rules we've learnt? Okay, let's try it. Oh, terrible. We've ended up with a 10,000 by 10,000 matrix. So 100 million points. Now we would expect an MSC to contain a thousand points. Why did that happen? The reason it happened is because we have to start out at the last dimension and go right to left. And we compare the 10,000 to the one and say, are they compatible? And the answer is, that's right, Alexei in the chat's got it right, broadcasting rules. So the answer is that this one will be broadcast over these 10,000. So this pair here will give us 10,000 outputs. And then we'll move to the next one. And we'll also move here to the next one. Uh-oh, there is no next one. What happens? Now if you remember the rules, it inserts a unit axis for us. So we now have 10,000 by one. So that means each of the 10,000 outputs from here will end up being broadcast across the 10,000 rows here. So that means that we'll end up for each of those 10,000, we'll have another 10,000. So we'll end up with a 10,000 by 10,000 output. So that's not what we want. So how could we fix that? Well what we really would want, would we want this to be 10,000 comma one here. If that was 10,000 comma one, then we'd compare these two, right to left, and they're both one. So those match. And there's nothing to broadcast, because they're the same. And then we'll go to the next one, 10,000 to 10,000. Those match. So they just go element-wise for those. And we'd end up with exactly what we want. We'd end up with 10,000 results. Or, alternatively, we could remove this dimension. And then again, same thing. We're then going to add right to left, compatible 10,000. So they'll get element-wise operation. So in this case, I got rid of the trailing comma one. There's a couple of ways you could do that. One is just to say, okay, grab every row and the zeroth column of res. And that's going to turn it from a 10,000 by one into a 10,000. Or alternatively, we can say dot squeeze. Now dot squeeze removes all trailing unit vectors, and possibly also prefix unit vectors. I can't quite recall. I guess we should try. So let's say res none comma colon comma none q.shape Okay, so if I go q.squeeze dot shape. Okay, so all the unit vectors get removed. Sorry, all the unit dimensions get removed, I should say. Okay, so now that we've got a way to remove that axis that we didn't want, we can use it. And if we do the subtraction, now we get 10,000, just like we wanted. So now let's get our training and validation-wise. We'll turn them into floats because we're using MSE. So let's calculate our predictions for the training set, which is 50,000 by one. And so if we create an MSE function that just does what we just said we wanted. So it does the subtraction and then squares it and then takes the mean. That's MSE. So there we go. We now have a loss function being applied to our training set. Okay. Now we need gradients. So as we briefly discussed last time, gradients are slopes. And in fact, maybe it would even be easier to look at last time. So this was last time's notebook. And so we saw how the gradient at this point is the slope here. And so it's the, as we discussed, rise over run. Now, so that means as we increase, in this case, time by one, the distance increases by how much? Distance increases by how much? That's what the slope is. So why is this interesting? The reason it's interesting is because let's consider our neural network. Our neural network is some function that takes two things, two groups of things. It contains a matrix of our inputs. And it contains our weight matrix. And we want to and let's assume we're also putting it through a loss function. So let's say well, I mean, I guess we can be explicit about that. So we could say, we then take the result of that and we put it through some loss function. So these are the predictions. And we compare it to our actual dependent variable. So that's our neural net. And that's our loss function. Okay. So if we can get the derivative of the loss with respect to, let's say, one particular weight. So let's say weight number zero. And what is that doing? Well, it's saying as I increase the weight by a little bit, what happens to the loss? And if it says, oh, well, that would make the loss go down, then obviously I want to increase the weight by a little bit. And if it says, oh, it makes the loss go up, then obviously I want to do the opposite. So the derivative of the loss with respect to the weights, each one of those tells us how to change the weights. And so to remind you, we then change each weight by that derivative times a little bit and subtract it from the original weights. And we do that a bunch of times. And that's called SGD. Now, there's something interesting going on here, which is that in this case, there's a single input and a single output. And so the derivative is a single number at any point. It's the speed. In this case, the vehicle's going. But consider a more complex function. Like, say, this one. Now, in this case, there's one output, but there's two inputs. And so if we want to take the derivative of this function, then we actually need to say, well, what happens if we increase X by a little bit? And also what happens if we increase Y by a little bit? And in each case, what happens to Z? And so in that case, the derivative is actually going to contain two numbers, right? It's going to contain the derivative of Z with respect to Y. And it's going to contain the derivative of Z with respect to X. What happens if we change each of these two numbers? So for example, these could be, as we discussed, two different weights in our neural network. And Z could be our loss, for example. Now, we've got actually 784 inputs, right? So we would actually have 784 of these. So we don't normally write them all like that. We would just say, use this little squiggly symbol to say the derivative of the loss across all of them with respect to all of the weights. Okay. And that's just saying that there's a whole bunch of them. It's a shorthand way of writing this. Okay. So it gets more complicated still, though. Because think about what happens if, for example, you're in the first layer where we've got a weight matrix that's going to end up giving us 50 outputs, right? So for every image, we're going to have 784 inputs to our function. And we're going to have 50 outputs to our function. And so in that case, I can't even draw it, right? Because like for every, even if I had two inputs and two outputs, then as I increase my first input, I'd actually need to say, how does that change both of the two outputs? And as I change my second input, how does that change both of my two outputs? So for the full thing, you actually are going to end up with a matrix of derivatives. It basically says for every input that you change by a little bit, how much does it change every output of that function? So you're going to end up with a matrix. So that's what we're going to be doing, is we're going to be calculating these derivatives. But rather than being single numbers, they're going to actually contain matrices with a row for every input and a column for every output. And a single cell in that matrix will tell us, as I change this input by a little bit, how does it change this output? Now, eventually, we will end up with a single number for every input. And that's because our loss in the end is going to be a single number. And this is like a requirement that you'll find when you try to use SGD, is that your loss has to be a single number. And so we generally get it by either doing the sum or a mean or something like that. But as you'll see on the way there, we're going to have to be dealing with these matrix of derivatives. So I just want to mention, as I might have said before, I can't even remember, there is this paper that Terence Parr and I wrote a while ago, which goes through all this. And it basically assumes that you only know high school calculus. And if you don't, check out Khan Academy. But then it describes matrix calculus in those terms. So it's going to explain to you exactly. And it works through lots and lots of examples. So, for example, as it mentions here, when you have this matrix of derivatives, we call that a Jacobian matrix. So there's all these words. It doesn't matter too much if you know them or not. But it's convenient to be able to talk about the matrix of all of the derivatives if somebody just says the Jacobian. It's a little bit easier than saying the matrix of all of the derivatives where all of the rows are the inputs and all the columns are the outputs. So, yeah, if you want to really understand, get to a point where papers are easier to read, in particular, it's quite useful to know this notation and definitions of words. You can certainly get away without it. It's just something to consider. OK. So we need to be able to calculate derivatives of at least a single variable. And I am not going to worry too much about that. A, because that is something you do in high school math. And B, because your computer can do it for you. And so you can do it symbolically using something called sympi, which is really great. If you create two symbols called x and y, you can say please differentiate x squared with respect to x. And if you do that, sympi will tell you the answer is 2x. If you say differentiate 3x squared plus 9 with respect to x, sympi will tell you that's 6x. And a lot of you probably will have used Wolfram Alpha, that does something very similar. I kind of quite like this because I can quickly do it inside my notebook and include it in my prose. So I think sympi is pretty cool. So, you know, basically, yeah, if you, you know, you can quickly calculate derivatives on a computer. Having said that, I do want to talk about why the derivative of 3x squared plus 9 equals 6x, because that's going to be very important. So 3x squared plus 9. So we're going to start with the the information that the derivative of a to the b with respect to a equals b times a. So for example, the derivative of x squared with respect to x equals 2x. So that's just something I'm hoping you'll remember from high school or refresh your memory using Kahn Academy or similar. So there, that is there. So what we could now do is we could rewrite this derivative as 3u plus 9. And then we'll write u equals x squared. Okay. Now this is getting easier. The derivative of two things being added together is simply the sum of their derivatives. Oh, forgot b minus 1 in the exponent. Thank you. So b a to the power of b minus 1. That's what it should be, which would be 2x to the power of 1. And the 1 is not needed. Thank you for fixing that. All right. So we just sum them up. So we get the derivative of 3u is actually just, well, it's going to be the derivative of that plus the derivative of that. Now the derivative of any constant with respect to a variable is zero, because if I change something, an input, it doesn't change the constant. It's always 9. So that's going to end up as zero. And so we're going to end up with dy du equals something plus zero. And the derivative of 3u with respect to u is just 3, because it's just a line. So that's its slope. Okay. But that's not dy dx. We want dy dx. Well, the cool thing is that dy dx is actually just equal to dy du du dx. So I'll explain why in a moment. But for now, then let's recognize we've got dy, sorry, du dx. We know that one, 2x. So we can now multiply these two bits together. And we will end up with 2x times 3, which is 6x, which is what Simpai told us. So fantastic. Okay. This is something we need to know really well. And it's called the chain rule. And it's best to understand it intuitively. So to understand it intuitively, we're going to take a look at an interactive animation. So I found this nice interactive animation on this page here. Webspace.ship.edu. Geogebra calculus. Okay. And the idea here is that we've got a wheel spinning around. And each time it spins around, this is x going up. Okay. So at the moment, there's some change in x, dx, over a period of time. All right. Now, this wheel is eight times bigger than this wheel. So each time this goes around once, if we connect the two together, this wheel would be going around four times faster. Because the difference between the multiple between eight and two is four. Maybe I'll bring this up to here. So now that this wheel is, has got twice as big a circumference as the u wheel, each time this goes around once, this is going around two times. So the change in u, each time x goes around once, the change in u will be two. So that's what du dx is saying. The change in u for each change in x is two. Now, we could make this interesting by connecting this wheel to this wheel. Now, this wheel is twice as small as this wheel. So now we can see that, again, each time this spins around once, this spins around twice, because this has twice the circumference of this. So therefore, dy du equals two. But now that means every time this goes around once, this goes around twice. Every time this one goes around once, this gun goes around twice. So therefore, every time this one goes around once, this one goes around four times. So dy dx equals four. So you can see here how the two, well, how the dx du dx has to be multiplied with the dy du to get the total. So this is what's going on in the chain rule. And this is what you want to be thinking about, is this idea that you've got one function that is kind of this intermediary, and so you have to multiply the two impacts to get the impact of the x wheel on the y wheel. So I hope you find that useful. I find this, personally, I find this intuition quite useful. So why do we care about this? Well, the reason we care about this is because we want to calculate the gradient of our MSE applied to our model. And so our inputs are going through a linear, they're going through a ReLU, they're going through another ReLU, they're going through a ReLU, they're going through another linear, and then they're going through an MSE. So there's four different steps going on. And so we're going to have to combine those all together. And so we can do that with the chain rule. So if our steps are that loss function is, so we've got the loss function, which is some function of the predictions and the actuals. And then we've got, we've got the second layer, we've got the second layer is a function of, actually, let's say, let's call this the output of the second layer. Slightly weird notation, but hopefully it's not too bad. It's going to be a function of the ReLU, of the ReLU activations. And the ReLU activations are a function of the first layer. And the first layer is a function of the inputs. Oh, and of course, this also has weights and biases. So we're basically going to have to calculate the derivative of that. Okay. But then remember that this is itself a function. So then we'll need to multiply that derivative by the derivative of that, but that's also a function. So we have to multiply that derivative by this, but that's also a function. So we have to multiply that derivative by this. So that's going to be our approach. We're going to start at the end. We're going to take its derivative, and then we're going to gradually keep multiplying as we go each step through. And this is called backpropagation. So backpropagation sounds pretty fancy, but it's actually just using the chain rule. Gosh, I didn't spell that very well. Prop-agation. It's just using the chain rule. And as you'll see, it's also just taking advantage of a computational trick of memorizing some things on the way. And in our chat, Siva made a very good point about understanding nonlinear functions in this case, which is just to consider that the wheels could be growing and shrinking all the time as they're moving. But you're still going to have the same compound effect, which I really like that. Thank you, Siva. There's also a question in the chat about why is this colon comma zero being placed in the function, given that we can do it outside the function? Well, the point is we want an MSE function that will apply to any output. We're not using it once. We want it to work any time. So we haven't actually modified preds or anything like that, or ytrain. So we want this to be able to apply to anything without us having to pre-process it, is basically the idea here. OK. So let's take a look at the basic idea. So here's going to do a forward pass and a backward pass. So the forward pass is where we calculate the loss. So the loss is oh, I've got an error here. That should be diff. There we go. So the loss is going to be the output of our neural net minus our target squared, and then take the mean. OK. And then our output is going to be the output of the second linear layer. The second linear layer's input will be the ReLU. The ReLU's input will be the first layer. So we're going to take our input, put it through a linear layer, put that through a ReLU, put that through a linear layer, and calculate the MSE. OK. That bit hopefully is pretty straightforward. So what about the backward pass? So the backward pass, what I'm going to do, and you'll see why in a moment, is I'm going to store the gradients of each layer. So for example, the gradients of the loss with respect to its inputs are in the layer itself. So I'm going to create a new attribute. I could call it anything I like. I'm just going to call it dot g. So I'm going to create a new layer, a new attribute called out.g, which is going to contain the gradients. You don't have to do it this way, but as you'll see, it turns out pretty convenient. So that's just going to be two times the difference, because we've got difference squared. All right. So that's just the derivative. And then we have taken the mean here, so we have to do the same thing here, divided by the input shape. And so that's those gradients. That's good. And now what we need to do is multiply by the gradients of the previous layer. So here's the previous layer. So what are the gradients of a linear layer? I've created a function for that here. So the gradient of a linear layer, we're going to need to know the weights of the layer. We're going to need to know the biases of the layer. And then we're also going to know the input to the linear layer, because that's the thing that's actually being manipulated here. And then we're also going to need the output, because we have to multiply by the gradients, because we've got the chain rule. So again, we're going to store the gradients of our input. So this would be the gradients of our output with respect to the input. And that's simply the weights, because the weights... So a matrix multiplier is just a whole bunch of linear functions. So each one's slope is just its weight. But you have to multiply it by the gradient of the outputs, because of the chain rule. And then the gradient of the outputs with respect to the weights is going to be the input times the output summed up. I'll talk more about that in a moment. The derivatives of the bias is very straightforward. It's the gradients of the output and the output added together, because the bias is just a constant value. So for the chain rule, we simply just use output times one, which is output. So for this one here, again, we have to do the same thing we've been doing before, which is multiply by the output gradients, because of the chain rule. And then we've got the input weights. So every single one of those has to be multiplied by the outputs. And so that's why we have to do an unsqueeze minus one. So what I'm going to do now is I'm going to show you how I would experiment with this code in order to understand it. And I would encourage you to do the same thing. It's a little harder to do this one cell by cell, because we kind of want to put it all into this function like this. So we need a way to explore the calculations interactively. And the way we do that is by using the Python debugger. Here is how you, let me see a few ways to do this. Here's one way to use the Python debugger. The Python debugger is called PDB. So if you say PDB.setTrace in your code, then that tells the debugger to stop execution when it reaches this line. So it sets a break point. So if I call forward and backward, you can see here it's stopped. And the interactive Python debugger, IPDB, has popped up. With an arrow pointing at the line of code it's about to run. And at this point, there's a whole range of things we can do to find out what they are. We hit H for help. Understanding how to use the Python debugger is one of the most powerful things I think you can do to improve your coding. So one of the most useful things you can do is to print something. You see all these single letter things? They're just shortcuts. But in a debugger, you want to be able to do things quickly. So instead of typing print, I just type P. So for example, let's take a look at the shape of the input. So I type P for print, input.shape. So I've got a 50,000 by 50 input to the last layer. That makes sense. These are the hidden activations coming into the last layer for every one of our images. What about the output gradients? And there's that as well. And actually, a little trick. You can ignore the you don't have to use the P at all if your variable name is not the same as any of these commands. So I could have just typed out.g.shape. Get the same thing. OK. So you can also put in expressions. So let's have a look at the shape of this. So the output of this is let's see if it makes sense. We've got the input, 50,000 by 50. We put a new axis on the end. Unsqueeze minus one is the same as doing dot as indexing it with dot, dot, dot, comma, none. So that would have become 50,000 by 50 by one. And then the out g.unsqueeze we're putting in the first dimension. So we're going to have 50,000 by 50 by one times 50,000 by one by one. And so we're going to end up getting this broadcasting happening over these last two dimensions. Which is why we end up with 50,000 by 50 by one. And then we're summing up. This makes sense, right? We want to sum up over all of the inputs. Each image is individually contributing to the derivative. And so we want to add them all up to find their total impact. Because remember the sum of a bunch of the derivative of the sum of functions is the sum of the derivatives of the functions. So we can just sum them up. Now, this is one of these situations where if you see a times and a sum and an unsqueeze, it's not a bad idea to think about Einstein summation notation. Maybe there's a way to simplify this. So first of all, let's just see how we can do some more stuff in the debugger. I'm going to continue. So just continue running. So press C for continue. And it keeps running until it comes back again to the same spot. And the reason we've come to the same spot twice is because lin grad is called two times. So we would expect that the second time we're going to get a different bunch of inputs and outputs. And so I can print out a tuple of the inputs and output gradient. So now, yeah, so this is the first layer. Going into the second layer. So that's exactly what we would expect. To find out what called this function, you just type W. W is where am I? And so you can see here, where am I? Oh, forward and backward was called. See the arrow? That called lin grad the second time. And now we're here in W.G equals. If we want to find out what W.G ends up being equal to, I can press N to say go to the next line. And so now we've moved from line 5 to line 6. So the instruction point is now looking at line 6. So I can print out W.G.shape. And there's the shape of our weights. One person on the chat has pointed out that you can use breakpoint instead of this import PDB business. Unfortunately, the breakpoint keyword doesn't currently work in Jupyter or in IPython. So we actually can't, sadly. That's why I'm doing it the old fashioned way. So this way, maybe they'll fix the bug at some point. But for now, we have to type all this. OK. So those are a few things to know about. But I would definitely suggest looking up a Python tutorial to become familiar with this incredibly powerful tool. Because it really is so very handy. So if I just press continue again, it keeps running all the way to the end. And it's now finished, running forward and backward. So when it's finished, we would find that there will now be, for example, a W1.G. Because this is the gradients that it just calculated. And there would also be a X train.G, and so forth. OK. So let's see if we can simplify this a little bit. So I would be inclined to take these out and give them their own variable names, just to make life a bit easier. It would have been better if I'd actually done this before the debugging. So it'd be a bit easier to type. So let's set I and O equal to input and output.G.unsqueeze. Oh. OK. So we'll get rid of our breakpoint. And double check that we've got our gradients. OK. And I guess before we rerun it, we should probably set those to zero. OK. What I would do here to try things out is I'd put my breakpoint there. And then I would try things. So let's go next. And so I realize here that what we're actually doing is we're basically doing exactly the same thing as an Einstein would do. So I could test that out by trying an ionsum, right? Because I've just got this is being replicated, and then I'm summing over that dimension, because that's the multiplication that I'm doing. So I'm basically multiplying the first dimension of each, and then summing over that dimension. So I could try running that and, ah, it works. So that's interesting. Oh, and I've got zeros because I did X train dot zero. That was silly. That should be dot gradients dot zero. OK. So let's try doing an ionsum. And there we go. That seems to be working. That's pretty cool. So we've multiplied this repeating index. So we were just multiplying the first dimensions together, and then summing over them. So there's no i here. Now, that's not quite the same thing as a matrix multiplication, but we could turn it into the same thing as matrix multiplication just by swapping i and j, so that they're the other way around. So I could do that. And then I could do that. And then I could do that. And then I could do that. Just by swapping i and j, so that they're the other way around. And that way we'd have j i comma i k. And we can swap into dimensions very easily. That's what's called the transpose. So that would become a matrix multiplication if we just use the transpose. And in NumPy, the transpose is the capital T attribute. So here is exactly the same thing, using a matrix multiply and a transpose. And let's check. Yeah, that's the same thing as well. Okay, cool. So that tells us that now we've checked in our debugger that we can actually replace all this with a matrix multiply. We don't need that anymore. And let's see if it works. It does. All right. X train dot g. Cool. Okay. So hopefully that's convinced you that the debugger is a really handy thing for playing around with numeric programming ideas or coding in general. And so I think now's a good time to take a break. So let's take a eight minute break. And I'll see you back here. Actually, seven minute break. I'll see you back here in seven minutes. Thank you. Okay. Welcome back. So we've calculated our derivatives. And we want to test them. Luckily, PyTorch already has derivatives implemented. So I'm going to totally cheat and use PyTorch to calculate the same derivatives. So don't worry about how this works yet. Because we're actually going to be doing all this from scratch anyway. For now, I'm just going to run it all through PyTorch and check that their derivatives are the same as ours. And they are. So we're on the right track. Okay. So this is all pretty clunky. I think we can all agree. And obviously it's clunkier than what we do in PyTorch. So how do we simplify things? There's some really cool refactoring that we can do. So what we're going to do is we're going to create a whole class for each of our functions. For the value function and for the linear function. So the way that we're going to do this is we're going to create a Dunder call. What does Dunder call do? Let me show you. So if I create a class. And we're just going to set that to print hello. So if I create an instance of that class. And then I call it as if it was a function. Oops. Missing the Dunder bit here. Call it as if it's a function. It says hi. So in other words, you know, everything can be changed in Python. You can change how a class behaves. You can make it look, work like a function. And to do that, you simply define Dunder call. You could pass it an argument. Like so. Okay. So that's what Dunder call does. It just says it's just a little bit of syntax sugary kind of stuff to say I want to be able to treat it as if it's a function without any method at all. You can still do it the method way. You could have done this. Don't know why you'd want to. But you can. Because it's got this special magic named Dunder call. You don't have to write the Dunder call at all. So here, if we create an instance of the value class, we can treat it as a function. And what it's going to do is it's going to take its input and do the value on it. But if you look back at the forward and backward, there's something very interesting about the backward pass. Which is that it has to know about, for example, this intermediate calculation gets passed over here. This intermediate calculation gets passed over here. Because of the chain rule, we're going to need some of the intermediate calculations. And not just because of the chain rule, but because of how the derivatives are calculated. So we need to actually store each of the layer intermediate calculations. And so that's why relu doesn't just calculate and return the output. But it also stores its output. And it also stores its input. So that way then when we call backward, we know how to calculate that. We set the inputs gradient. Because remember we stored the input. So we can do that. Right? And it's going to just be, oh, input greater than zero dot float. Right? So that's the definition, okay, of the derivative of a relu. And then chain rule. So that's how we can calculate the forward pass and the backward pass for relu. And we're not going to have to then store all this intermediate stuff separately. It's going to happen automatically. So we can do the same thing for a linear layer. Now linear layer needs some additional state, weights and biases. Relu doesn't. Right? So there's no in it. So when we create a linear layer, we have to say what are its weights, what are its biases. We store them away. And then when we call it in the forward pass, just like before, we store the input. So that's exactly the same line here. And just like before, we calculate the output and store it and then return it. Okay? And this time, of course, we just call lin. And then for the backward pass, it's the same thing. Okay? So the input gradients we calculate just like before. Oh, dot t brackets is exactly the same with a little t as big T is as a property. So that's the same thing. That's just the transpose. Calculate the gradients of the weights. Again, with a chain rule and the bias, just like we did it before. And they're all being stored in the appropriate places. And then for MSE, we can do the same thing. We don't just calculate the MSE, but we also store it. And we also, now the MSE needs just needs two things, an input and a target. So we'll store those as well. So then in the backward pass, we can calculate its gradient of the input as being two times the difference. And there it all is. Okay. So our model now, it's much easier to define. We can just create a bunch of layers, linear w1, b1, relu, linear w2, b2. And then we can store an instance of the MSE. So this is not calling MSE. It's creating an instance of the MSE class. And this is an instance of the lin class. This is an instance of the relu class. So they're just being stored. So then when we call the model, we pass it our inputs and our target. We go through each layer, set x equal to the result of calling that layer, and then pass that to the loss. So there's something kind of interesting here that you might have noticed, which is that we don't have, um, um, there we do it. Something interesting here is that we don't have two separate functions inside our, inside our model. The loss function being applied to a separate neural net. But we've actually integrated the loss function directly into the neural net, into the model. See how the loss is being calculated inside the model? Now, that's neither better nor worse than having it separately. It's just different. And so generally a lot of hugging face stuff does it this way. They actually put the loss inside the forward. Most stuff in fast AI and a lot of other libraries does it separately, which is the loss is a whole separate function and the model only returns the result of putting it through the layers. So for this model, we're going to actually do the loss function inside the model. So for backward, we just do each thing. So self.loss.backward. So self.loss is the MSE object. So that's going to call backward, right? And it's stored when it was called here, it was storing, remember the inputs, the targets, the outputs, so it can calculate the backward. And then we go through each layer is in reverse, right? This is back propagation, backwards, reversed, calling backward on each one. So that's pretty interesting, I think. So now we can calculate the model, we can calculate the loss, we can call backward, and then we can check that each of the gradients that we stored earlier are equal to each of our new gradients. Okay, so Williams asked a very good question. That is, if you do put the loss inside here, how on earth do you actually get predictions? So generally, what happens is, in practice, Hugging Face models do something like this. They'll say self.preds equals x. And then they'll say, well, self.finalLoss equals that, and then return self.finalLoss. And that way, I guess you don't even need that last bit. Well, that's with them anyway, that is what they do. So we'll leave it there. And so that way you can kind of check like model.preds, for example. So it'll be something like that. Or alternatively, you can return not just the loss, but both as a dictionary, stuff like that. So there's a few different ways you could do it. Actually, now I think about it, I think that's what they do, is they actually return both as a dictionary. So it would be, um, it'd be like return dictionaryLoss equals that, comma, preds equals that. Something like that, I guess, is what they would do. Anyway, there's a few different ways to do it. Um, okay. So hopefully you can see that this is really, um, making it nice and easy for us to do our forward pass and our backward pass without all of this manual fiddling around. Every class now can be totally separately considered, um, and can be combined however we want. We could create layers. So you could try creating a bigger neural net if you want to. But we can refactor it more. So basically, as a rule of thumb, when you see repeated code, self.imp equals imp, self.imp equals imp, self.out equals return self.out, self.out equals return self.out, that's a sign you can refactor things. And so what we can do is a simple refactoring, is to create a new class called module. And module's going to do those things we just said. It's going to store the inputs, and it's going to call something called self.forward in order to create our self.out, because remember that was one of the things we had again and again and again, self.out, self.out, and then return it. And so now there's going to be a thing called forward, which actually, in this, it doesn't do anything, because the whole purpose of this module is to be inherited. When we call backward, it's going to call self.backward passing in self.out, because notice all of our backwards always wanted to get hold of self.out, right, self.out, self.out, because we need it for the chain rule. So let's pass that in, and pass in those arguments that we stored earlier. And so star means take all of the arguments, regardless whether it's zero, one, two, or more, and put them into a list. And then that's what happens when it's inside the actual signature. And then when you call a function using star, it says take this list and expand them into separate arguments, calling backward with each one separately. So now for relu, look how much simpler it is. Let's copy the old relu to the new relu. So the old relu had to do all this storing stuff manually, and it had all the self.stuff as well. But now we can get rid of all of that, and just implement forward, because that's the thing that's being called, and that's the thing that we need to implement. And so now the forward of relu just does the one thing we want, which also makes the code much cleaner and more understandable. Ditto for backward, it just does the one thing we want. So that's nice. Now we still have to multiply it, but I still have to do the chain rule manually. But so same thing for linear, same thing for MSE. So these all look a lot nicer. And one thing to point out here is that there's often opportunities to manually speed things up when you create custom autograd functions in PyTorch. And here's an example. Look, this calculation is being done twice, which seems like a waste, doesn't it? So at the cost of some memory, we could instead store that calculation as diff. Right? And I guess we'd have to store it for use later, so it would need to be self.diff. And at the cost of that memory, we could now remove this redundant calculation, because we've done it once before already and stored it, and just use it directly. And this is something that you can often do in neural nets. So there's this compromise between storing things, the memory use of that, and then the computational speed up of not having to recalculate it. This is something we come across a lot. And so now we can call it the same way, create our model, passing in all of those layers. Right? So you can see with our model, we're just so the model hasn't changed at this point. The definition was up here. We just pass in the layers. Sorry, not the layers, the weights for the layers. Calculate the loss, call backward. And look, it's the same. Hooray! Okay. So, um, thankfully, PyTorch has written all this for us. And remember, according to rules of our game, once we've re-implemented it, we're allowed to use PyTorch's version. So PyTorch calls their version nn.module. And so it's exactly the same. You inherit from nn.module. So if we want to create a linear layer, just like this one, rather than inheriting from our module, we will inherit from their module. But everything's exactly the same. So we create our, we can create our random numbers. So in this case, rather than passing in the already randomized weights, we're actually going to generate the random weights ourselves. And the zeroed biases. And then here's our linear layer, which you could also use lin for that, of course. So we've defined our forward. And why don't we need to define backward? Because PyTorch already knows the derivatives of all of the functions in PyTorch. And it knows how to use the chain rule. So we don't have to do the backward at all. It'll actually do that entirely for us. Which is very cool. So we only need forward. We don't need backward. So let's create a model that uses that nn.module. Otherwise, it's exactly the same as before. And now we're going to use PyTorch's mseloss, because we've already implemented ourselves. It's very common to use torch.nn.functional as capital F. This is where lots of these handy functions live, including mseloss. And so now you know why we need the colon common none, because you saw the problem if we don't have it. And so create the model, call backward. And remember, we stored our gradients in something called dot G. PyTorch stores them in something called dot grad. But it's doing exactly the same thing. So there is the exact same values. So let's take stock of where we're up to. So we've we've created a matrix multiplication from scratch. We've created linear layers. We've created a complete backprop system of modules. We can now calculate both the forward pass and the backward pass for linear layers and values. So we can create a multi-layer perceptron. So we're now up to a point where we can train a model. So let's do that. So many batch training, notebook number four. So same first cell as before. We won't go through it. This cell is also the same as before. So we won't go through it. Here's the same model that we had before. So we won't go through it. So just rerunning all that to see. OK. So the first thing we should do, I think, is to improve our loss function. So it's not total rubbish anymore. So if you watched part one, you might recall that there are some Excel notebooks. One of those Excel notebooks is Entropy Example. OK. So this is what we looked at. So just to remind you, what we're doing now is we're saying, OK, rather than outputting a single number for each image, we're going to instead output 10 numbers for each image. And so that's going to be a one-hot encoded set of, it'll be like 1, 0, 0, 0, et cetera. And so then that's going to be, so well, actually, the outputs won't be 1, 0, 0. They'll be basically probabilities, won't they? So it'll be like 0.99, comma, you know, 0.01, et cetera. And the targets will be one-hot encoded. So if it's the digit 0, for example, it might be 1, 0, 0, 0, 0, dot, dot, dot, for all the 10 possibilities. And so to see, you know, how good is it? So in this case, it's really good. It had a 0.99 probability prediction that it's 0. And indeed it is, because this is the one-hot encoded version. And so the way we implement that is we don't even need to actually do the one-hot encoding, thanks to some tricks. We can actually just directly store the integer, but we can treat it as if it's one-hot encoded. So we can just store the actual target 0 as an integer. So the way we do that is we say, for example, for a single output, oh, it could be cat, let's say, cat, dog, plane, fish, building. The neural net spits out a bunch of outputs. What we do for Softmax is we go e to the power of each of those outputs. We sum up all of those e to the power ofs. So here's the e to the power of each of those outputs. Here's the sum of them. And then we divide one, each one by the sum. So divide each one by the sum. That gives us our Softmaxes. And then for the loss function, we then compare those Softmaxes to the one-hot encoded version. So let's say it was a dog, then it's going to have a 1 for dog. And 0 everywhere else. And then Softmax, this is from this nice blog post here. This is the calculation sum of the 1s and 0s. So each of the 1s and 0s, multiplied by the log of the probabilities. So here is the log probability times the actuals. And since the actuals are either 0 or 1, and only one of them is going to be a 1, we're only going to end up with one value here. And so if we add them up, it's all 0 except for one of them. So that's cross entropy. So in this special case where the output's one-hot encoded, then doing the one-hot encoded multiplied by the log Softmax is actually identical to simply saying, oh, dog is in this row. Let's just look it up directly and take its log Softmax. We can just index directly into it. So it's exactly the same thing. So that's just review. So if you haven't seen that before, then yeah, go and watch the part one video where we went into that in a lot more detail. Okay, so here's our Softmax calculation. It's e to the power of each output divided by the sum of them. Or we can use sigma notation to say exactly the same thing. And as you can see, Jupyter Notebook lets us use LaTeX. If you haven't used LaTeX before, it's actually surprisingly easy to learn. You just put dollar signs around your equations like this. And your equations backslash is going to be kind of like your functions, if you like. And curly parentheses, curly, curly is used to kind of for up and down, and curly is used to kind of for down. And curly parentheses, curly, curly is used to kind of for arguments. So you can see here, here is e to the power of, and then underscore is used for subscript. So this is x subscript i. And power of is used for superscripts. So here's dots. You can see here it is, dots. So it's actually, yeah, learning LaTeX is easier than you might expect. It can be quite convenient for writing these functions when you want to. So anyway, that's what softmax is. As we'll see in a moment, well, actually, as you've already seen, in cross entropy, we don't really want softmax, we want log of softmax. So log of softmax is, here it is. So we've got x dot exp, so e to the x, divided by x dot exp dot sum. And we're going to sum up over the last dimension. And then we actually want to keep that dimension, so that when we do the divided by, we want a trailing unit axis, for exactly the same reason we saw when we did our MSE loss function. So if you sum with keep dim equals true, it leaves a unit axis in that last position. So we don't have to put it back to avoid that horrible out of product issue. So this is the equivalent of this, and then dot log. So that's log of softmax. So there is the log of the softmax with the predictions. Now, in terms of high score math that you may have forgotten, but you definitely are going to want to know, a key piece that in that list of things is log and exponent rules. So check out Khan Academy or similar if you've forgotten them. But a quick reminder is, for example, the one we mentioned here. log of a over b equals log of a minus log of b. And equivalently, log of a times b equals log of a plus log of b. And these are very handy, because, for example, division can take a long time, multiply can create really big numbers that have lots of floating point error, being able to replace these things with pluses and minuses is very handy indeed. In fact, I used to give people an interview question 20 years ago, a company which I did a lot of stuff with SQL and math. SQL actually only has a sum function for group by clauses. And I used to ask people how you would deal with calculating a compound interest column, where the answer is basically that you have to say, because this compound interest is taking products, so it has to be the sum of the log. Of the column, and then e to the power of all that. So there's like all kinds of little places that these things come in handy, but they come in to neural nets all the time. So we're going to take advantage of that, because we've got a divided by, it's being logged. And also, rather handily, we're going to have, therefore, the log of exp dot exp minus the log of this. But exp and log are opposites. So that is going to end up just being x minus. So log softmax is just x minus all this logged. And here it is. All this logged. So that's nice. So here's our simplified version. Okay. Now, there's another very cool trick. Which, it's one of these things I figured out myself and then discovered other people had known it for years. So not my trick, but it's always nice to rediscover things. The trick is what's written here. Let me explain what's going on. This piece here, the log of this sum, right? This sum here. We've got x dot exp dot sum. Now, x could be some pretty big numbers. And e to the power of that's going to be really big numbers. And e to the power of things creating really big numbers. Well, really big numbers, there's much less precision in your computer's floating point handling. The further you get away from zero, basically. So we don't want really big numbers. Particularly because we're going to be taking derivatives. And so if you're in an area that's not very precise, as far as floating point math is concerned, then the derivatives are going to be a disaster. They might even be zero. Because you've got two numbers that the computer can't even recognize as different. So this is bad. But there's a nice trick we can do to make it a lot better. What we can do is we can calculate the max of a, sorry, the max of x, right? And we'll call that a. And so then rather than doing the log of the sum of e to the x i, we're instead going to define a as being the minimum, sorry, the maximum of all of our x values. It's our biggest number. Now if we then subtract that from every number, that means none of the numbers are going to be big, by definition. Because we've subtracted it from all of them. Now the problem is that's given us a different result, right? But if you think about it, let's expand this sum. It's e to the power of x1, if we don't put include L minus a, plus e to the power of x2, plus e to the power of x3, and so forth. Okay, now we just subtracted a from our exponents, which has made, meant we're now wrong. But I've got good news, I've got good news and bad news. The bad news is that you've got more high school math to remember, which is exponent rules. So a, x to the a plus b, equals x to the a, times x to the b. And similarly, x to the a minus b, equals x to the a, divided by x to the b. And to convince yourself that's true, consider for example, 2 to the power of 2 plus 3. What is that? Well you've got 2 to the power of 2, is just 2 times 2. And 2 to the power of 2 plus 3, well it's 2 times 2 times, is 2 to the power of 5. So you've got 2 to the power of 2, you've got 2 of them here, and you've got another 3 of them here. So we're just adding up the number to get the total index. So we can take advantage of this here, and say like, oh well this is equal to e to the x1, over e to the a, plus e to the x2, over e to the a, plus e to the x3, oops, plus e to the x3, over e to the a. And this is a common denominator, so we can put all that together. A to the a. And why did we do all that? Because if we now multiply that all by e to the a, these would cancel out, and we get the thing we originally wanted. So that means we simply have to multiply this, by that, and this gives us exactly the same thing as we had before. But with, critically, this is no longer ever going to be a giant number. So this might seem a bit weird, we're doing extra calculations. It's not a simplification, it's a complexification. But it's one that's going to make it easier for our floating point unit. So that's our trick, is rather than doing log of this sum, what we actually do is log of e to the a times the sum of e to the x minus a. And since we've got log of a product, that's just a log, that's just the sum of the logs, and log of e to the a is just a. So it's a plus that. So this here is called the log sum exp trick. Oops, people pointing out that I've made a mistake, thank you. That, of course, should have been inside the log. You can't just go sticking it on the outside like a crazy person. That's what I meant to say. Okay, so here is the log sum exp trick. Oh, I called it m instead of a, which is a bit silly. I should have called it a, but anyway. So we find the maximum on the last dimension, and then here is the m plus a. That exact thing. Okay, so that's just another way of doing that. Okay, so that's the log sum exp. So now we can rewrite log softmax as x minus log sum exp. And we're not going to use our version because PyTorch already has one. So we'll just use PyTorch's. And if we check, we, here we go. Here's our results. And so then as we've discussed, the cross entropy loss is the sum of the outputs times the log probabilities. And as we discussed, our outputs are one-hot encoded, or actually they're just the integers, better still. So what we can do is we can, I guess I should make that more clear. Actually, they're just the integer indices. So we can simply rewrite that as negative log of the target. So that's what we have in our Excel. And so how do we do that in PyTorch? So this is quite interesting. There's a lot of cool things you can do with array indexing in PyTorch and NumPy. So basically they use the same approaches. Let's take a look. Here is the first three actual values in y-train. They're five, zero, and four. Now what we want to do is we want to find in our softmax predictions, we want to get five, the fifth prediction in the zeroth row, the zeroth prediction in the first row, and the fourth prediction in the index two row. So these are the numbers that we want. This is going to be what we add up for the first two rows of our loss function. So how do we do that in all in one go? Well, here's a cool trick. See here I've got zero, one, two. If we index using a two lists, we can put here zero, one, two. And for the second list, we can put y-train, column three, five, zero, four. And this is actually going to return zero, comma, zero, one, comma, sorry, it's going to be, sorry, it's going to be zero, comma, five, one, comma, zero, and two, comma, four, which is, as you see, exactly the same thing. So therefore, this is actually giving us what we need for the cross-entropy loss. So if we take range of our target's first dimension, or zero index dimension, which is all this is, and the target, and then take the negative of that dot mean, that gives us our cross-entropy loss, which is pretty neat, in my opinion. All right, so PyTorch calls this negative log likelihood loss. But that's all it is. And so if we take the negative log likelihood, and we pass that to that, the log softmax, then we get the loss. And this particular combination in PyTorch is called f dot cross-entropy. So just check. Yep, f dot cross-entropy gives us exactly the same thing. So that's cool. So we have now re-implemented the cross-entropy loss. And there's a lot of confusing things going on there, a lot. And so this is one of those places where you should pause the video and go back and look at each step and think, not just like, what is it doing, but why is it doing it? And also try typing in lots of different values yourself to see if you can see what's going on. And then put this aside and test yourself by re-implementing the cross-entropy loss. So let's do that. So now that we've got that, we can actually create a training loop. So let's set our loss function to be cross-entropy. Let's create a batch size of 64. And so here's our first mini batch. So xb is the x mini batch. It's going to be from 0 up to 64 from our training set. So we can now calculate our predictions. So that's 64 by 10. So for each of the 64 images in the mini batch, we have 10 probabilities, one for each digit. And our y is just, let's print those out. So there's our first 64 target values. So these are the actual digits. And so our loss function, so we're going to start with a bad loss because it's entirely random at this point. At this point. Okay. So for each of the predictions we made, so those are our predictions. And so remember those predictions are a 64 by 10. What did we predict? So for each one of these 64 rows, we have to go in and see where is the highest number. So if we go through here, we can go through each one. Here's a, there's a 0.1. Okay. It looks like this is the highest number. So it's 0, 1, 2, 3. So it's the highest number is this one. So you've got to find the index of the highest number. The function to find the index of the highest number is called argmax. And yep, here it is, 3. And I guess we could have also written this probably as preds.argmax. Normally you can do them either way. I actually prefer normally to do it this way. Yep. There's the same thing. Okay. And the reason we want this is because we want to be able to calculate accuracy. We don't need it for the actual neural net, but we just like to be able to see how we're going. Cause it's like, it's a metric. It's something that we use for understanding. So we take the argmax, we compare it to the actual. So that's going to give us a bunch of balls. If we turn those into floats, there'll be ones and zeros. And the mean of those floats is the accuracy. So our current accuracy, not surprisingly, is around 10%. It's 9% because it's random. That's what you would expect. So let's train our first neural net. So we'll set a learning rate. We'll do a few epochs. So we're going to go through each epoch and we're going to go through from zero up to N. That's the 50,000 training rows and skipping by 64, the batch size each time. And so we're going to create a slice that starts at I. So starting at zero and goes up to 64, unless we've gone past the end, in which case we'll just go to N. And so then we will slice into our training set for the X and for the Y to get our X and Y batches. We will then calculate our predictions, our loss function and do our backward. So the way I did this originally was I had all of these in separate cells. And I just typed in, you know, I equals zero. And then went through one cell at a time, calculating each one until they all worked. And so then I can put them in a loop. Okay, so once we've got done backward, we can then with torch not go grad, go through each layer. And if that's a layer that has weights, we'll update them to the existing weights minus the gradients times the learning rate. And then zero out. So the weights and biases for the gradients, the gradients of the weights and biases, this underscore means do it in place. So that sets this to zero. So if I run that, oops, got to run all of them. I guess I skipped cell. There we go. It's finished. So you can see that our accuracy on the training sets a bit unfair, but it's only three epochs, is nearly 97%. So we now have a digit recognizer. It trains pretty quickly and is not terrible at all. So that's a pretty good starting point. All right. So what we're going to do next time is we're going to refactor this training loop to make it dramatically, dramatically, dramatically simpler, step by step, until eventually we will get it down to, so we'll get it down to something much, much shorter, and then we're going to add a validation set to it and a multiprocessing data loader. And then, yeah, we'll be in a pretty good position, I think, to, to start training some more interesting models. All right. Hopefully you found that useful and learned some interesting things. And so what I'd really like you to do is at this point, now that you've kind of like got all these key basic pieces in place, is to really try to recreate them without peaking as much as possible. So, you know, recreate your matrix multiply, recreate those forward and backward passes, recreate something that steps through layers, and even see if you can like recreate the idea of the dot forward and the dot backward. Make sure it's all in your head really clearly, so that you fully understand what's going on. You know, at the very least, if you don't have time for that, because that's a big job, you could pick out a smaller part of that, the piece that you're more interested in, or you could just go through and look really closely at these notebooks. So if you go to kernel, restart and clear output, it'll delete all the outputs and like try to think like what are the shapes of things, can you guess what they are, can you check them, and so forth. Okay. Thanks, everybody. Hope you have a great week and I will see you next time. Bye.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 3.52, "text": " Hi everybody and welcome to lesson 13,", "tokens": [50364, 2421, 2201, 293, 2928, 281, 6898, 3705, 11, 50540], "temperature": 0.0, "avg_logprob": -0.5121705796983507, "compression_ratio": 1.4456521739130435, "no_speech_prob": 0.0003946645592804998}, {"id": 1, "seek": 0, "start": 3.52, "end": 8.4, "text": " where we're gonna start talking about back propagation.", "tokens": [50540, 689, 321, 434, 799, 722, 1417, 466, 646, 38377, 13, 50784], "temperature": 0.0, "avg_logprob": -0.5121705796983507, "compression_ratio": 1.4456521739130435, "no_speech_prob": 0.0003946645592804998}, {"id": 2, "seek": 0, "start": 8.4, "end": 13.84, "text": " Before we do, I'll just mention that there was some great success", "tokens": [50784, 4546, 321, 360, 11, 286, 603, 445, 2152, 300, 456, 390, 512, 869, 2245, 51056], "temperature": 0.0, "avg_logprob": -0.5121705796983507, "compression_ratio": 1.4456521739130435, "no_speech_prob": 0.0003946645592804998}, {"id": 3, "seek": 0, "start": 13.84, "end": 17.8, "text": " amongst the folks in the class during the week on", "tokens": [51056, 12918, 264, 4024, 294, 264, 1508, 1830, 264, 1243, 322, 51254], "temperature": 0.0, "avg_logprob": -0.5121705796983507, "compression_ratio": 1.4456521739130435, "no_speech_prob": 0.0003946645592804998}, {"id": 4, "seek": 0, "start": 17.8, "end": 25.72, "text": " working with flexing their tensor manipulation muscles.", "tokens": [51254, 1364, 365, 5896, 278, 641, 40863, 26475, 9530, 13, 51650], "temperature": 0.0, "avg_logprob": -0.5121705796983507, "compression_ratio": 1.4456521739130435, "no_speech_prob": 0.0003946645592804998}, {"id": 5, "seek": 2572, "start": 26.68, "end": 32.04, "text": " So far the fastest mean shift algorithm,", "tokens": [50412, 407, 1400, 264, 14573, 914, 5513, 9284, 11, 50680], "temperature": 0.0, "avg_logprob": -0.3199364525931222, "compression_ratio": 1.5228426395939085, "no_speech_prob": 0.00011591671500355005}, {"id": 6, "seek": 2572, "start": 32.04, "end": 35.96, "text": " which has a similar accuracy to the one I displayed,", "tokens": [50680, 597, 575, 257, 2531, 14170, 281, 264, 472, 286, 16372, 11, 50876], "temperature": 0.0, "avg_logprob": -0.3199364525931222, "compression_ratio": 1.5228426395939085, "no_speech_prob": 0.00011591671500355005}, {"id": 7, "seek": 2572, "start": 35.96, "end": 41.72, "text": " is one that actually randomly chooses data points as subset.", "tokens": [50876, 307, 472, 300, 767, 16979, 25963, 1412, 2793, 382, 25993, 13, 51164], "temperature": 0.0, "avg_logprob": -0.3199364525931222, "compression_ratio": 1.5228426395939085, "no_speech_prob": 0.00011591671500355005}, {"id": 8, "seek": 2572, "start": 41.72, "end": 43.239999999999995, "text": " And I actually think that's a great approach.", "tokens": [51164, 400, 286, 767, 519, 300, 311, 257, 869, 3109, 13, 51240], "temperature": 0.0, "avg_logprob": -0.3199364525931222, "compression_ratio": 1.5228426395939085, "no_speech_prob": 0.00011591671500355005}, {"id": 9, "seek": 2572, "start": 43.239999999999995, "end": 48.68, "text": " Very often random sampling and random projections", "tokens": [51240, 4372, 2049, 4974, 21179, 293, 4974, 32371, 51512], "temperature": 0.0, "avg_logprob": -0.3199364525931222, "compression_ratio": 1.5228426395939085, "no_speech_prob": 0.00011591671500355005}, {"id": 10, "seek": 2572, "start": 48.68, "end": 54.92, "text": " are two excellent ways of speeding up algorithms.", "tokens": [51512, 366, 732, 7103, 2098, 295, 35593, 493, 14642, 13, 51824], "temperature": 0.0, "avg_logprob": -0.3199364525931222, "compression_ratio": 1.5228426395939085, "no_speech_prob": 0.00011591671500355005}, {"id": 11, "seek": 5492, "start": 54.92, "end": 59.32, "text": " So it'll be interesting to see if anybody during the rest of the course", "tokens": [50364, 407, 309, 603, 312, 1880, 281, 536, 498, 4472, 1830, 264, 1472, 295, 264, 1164, 50584], "temperature": 0.0, "avg_logprob": -0.25253077915736605, "compression_ratio": 1.6415929203539823, "no_speech_prob": 0.0005109350895509124}, {"id": 12, "seek": 5492, "start": 59.32, "end": 64.6, "text": " comes up with anything faster than random sampling.", "tokens": [50584, 1487, 493, 365, 1340, 4663, 813, 4974, 21179, 13, 50848], "temperature": 0.0, "avg_logprob": -0.25253077915736605, "compression_ratio": 1.6415929203539823, "no_speech_prob": 0.0005109350895509124}, {"id": 13, "seek": 5492, "start": 64.6, "end": 69.48, "text": " Also been seeing some good Einstein summation examples and implementations,", "tokens": [50848, 2743, 668, 2577, 512, 665, 23486, 28811, 5110, 293, 4445, 763, 11, 51092], "temperature": 0.0, "avg_logprob": -0.25253077915736605, "compression_ratio": 1.6415929203539823, "no_speech_prob": 0.0005109350895509124}, {"id": 14, "seek": 5492, "start": 69.48, "end": 76.28, "text": " and continuing to see lots of good diff edit implementations.", "tokens": [51092, 293, 9289, 281, 536, 3195, 295, 665, 7593, 8129, 4445, 763, 13, 51432], "temperature": 0.0, "avg_logprob": -0.25253077915736605, "compression_ratio": 1.6415929203539823, "no_speech_prob": 0.0005109350895509124}, {"id": 15, "seek": 5492, "start": 76.28, "end": 79.64, "text": " So congratulations to all the students and I hope those of you", "tokens": [51432, 407, 13568, 281, 439, 264, 1731, 293, 286, 1454, 729, 295, 291, 51600], "temperature": 0.0, "avg_logprob": -0.25253077915736605, "compression_ratio": 1.6415929203539823, "no_speech_prob": 0.0005109350895509124}, {"id": 16, "seek": 5492, "start": 79.64, "end": 83.16, "text": " following along the videos in the MOOC will be", "tokens": [51600, 3480, 2051, 264, 2145, 294, 264, 49197, 34, 486, 312, 51776], "temperature": 0.0, "avg_logprob": -0.25253077915736605, "compression_ratio": 1.6415929203539823, "no_speech_prob": 0.0005109350895509124}, {"id": 17, "seek": 8316, "start": 83.16, "end": 87.24, "text": " working on the same homework as well and sharing your results", "tokens": [50364, 1364, 322, 264, 912, 14578, 382, 731, 293, 5414, 428, 3542, 50568], "temperature": 0.0, "avg_logprob": -0.26680246988932294, "compression_ratio": 1.3582089552238805, "no_speech_prob": 3.321177791804075e-05}, {"id": 18, "seek": 8316, "start": 87.24, "end": 90.52, "text": " on the fast.ai forums.", "tokens": [50568, 322, 264, 2370, 13, 1301, 26998, 13, 50732], "temperature": 0.0, "avg_logprob": -0.26680246988932294, "compression_ratio": 1.3582089552238805, "no_speech_prob": 3.321177791804075e-05}, {"id": 19, "seek": 8316, "start": 91.39999999999999, "end": 95.16, "text": " So now we're going to take a look at", "tokens": [50776, 407, 586, 321, 434, 516, 281, 747, 257, 574, 412, 50964], "temperature": 0.0, "avg_logprob": -0.26680246988932294, "compression_ratio": 1.3582089552238805, "no_speech_prob": 3.321177791804075e-05}, {"id": 20, "seek": 8316, "start": 96.12, "end": 99.08, "text": " notebook number three", "tokens": [51012, 21060, 1230, 1045, 51160], "temperature": 0.0, "avg_logprob": -0.26680246988932294, "compression_ratio": 1.3582089552238805, "no_speech_prob": 3.321177791804075e-05}, {"id": 21, "seek": 8316, "start": 101.16, "end": 105.8, "text": " in the normal repo, course 22 p1 repo.", "tokens": [51264, 294, 264, 2710, 49040, 11, 1164, 5853, 280, 16, 49040, 13, 51496], "temperature": 0.0, "avg_logprob": -0.26680246988932294, "compression_ratio": 1.3582089552238805, "no_speech_prob": 3.321177791804075e-05}, {"id": 22, "seek": 10580, "start": 105.88, "end": 111.72, "text": " And we're going to be looking at the forward and backward passes of a", "tokens": [50368, 400, 321, 434, 516, 281, 312, 1237, 412, 264, 2128, 293, 23897, 11335, 295, 257, 50660], "temperature": 0.0, "avg_logprob": -0.4914318642965177, "compression_ratio": 1.6394230769230769, "no_speech_prob": 4.133422771701589e-05}, {"id": 23, "seek": 10580, "start": 111.72, "end": 116.2, "text": " simple multi-layer perceptron neural network.", "tokens": [50660, 2199, 4825, 12, 8376, 260, 43276, 2044, 18161, 3209, 13, 50884], "temperature": 0.0, "avg_logprob": -0.4914318642965177, "compression_ratio": 1.6394230769230769, "no_speech_prob": 4.133422771701589e-05}, {"id": 24, "seek": 10580, "start": 116.2, "end": 122.19999999999999, "text": " The initial stuff up here is just importing things and just setting some", "tokens": [50884, 440, 5883, 1507, 493, 510, 307, 445, 43866, 721, 293, 445, 3287, 512, 51184], "temperature": 0.0, "avg_logprob": -0.4914318642965177, "compression_ratio": 1.6394230769230769, "no_speech_prob": 4.133422771701589e-05}, {"id": 25, "seek": 10580, "start": 122.19999999999999, "end": 124.92, "text": " stuff that they're just copying and pasting some stuff from previous", "tokens": [51184, 1507, 300, 436, 434, 445, 27976, 293, 1791, 278, 512, 1507, 490, 3894, 51320], "temperature": 0.0, "avg_logprob": -0.4914318642965177, "compression_ratio": 1.6394230769230769, "no_speech_prob": 4.133422771701589e-05}, {"id": 26, "seek": 10580, "start": 124.92, "end": 128.68, "text": " notebooks around paths and parameters and stuff like that.", "tokens": [51320, 43782, 926, 14518, 293, 9834, 293, 1507, 411, 300, 13, 51508], "temperature": 0.0, "avg_logprob": -0.4914318642965177, "compression_ratio": 1.6394230769230769, "no_speech_prob": 4.133422771701589e-05}, {"id": 27, "seek": 10580, "start": 128.68, "end": 131.96, "text": " So we'll skip over this.", "tokens": [51508, 407, 321, 603, 10023, 670, 341, 13, 51672], "temperature": 0.0, "avg_logprob": -0.4914318642965177, "compression_ratio": 1.6394230769230769, "no_speech_prob": 4.133422771701589e-05}, {"id": 28, "seek": 13196, "start": 132.04000000000002, "end": 135.24, "text": " So we'll often be kind of copying and pasting stuff from one", "tokens": [50368, 407, 321, 603, 2049, 312, 733, 295, 27976, 293, 1791, 278, 1507, 490, 472, 50528], "temperature": 0.0, "avg_logprob": -0.4776119597970623, "compression_ratio": 1.4505494505494505, "no_speech_prob": 0.01798286661505699}, {"id": 29, "seek": 13196, "start": 135.24, "end": 139.24, "text": " notebook to another's kind of first cell to get things set up.", "tokens": [50528, 21060, 281, 1071, 311, 733, 295, 700, 2815, 281, 483, 721, 992, 493, 13, 50728], "temperature": 0.0, "avg_logprob": -0.4776119597970623, "compression_ratio": 1.4505494505494505, "no_speech_prob": 0.01798286661505699}, {"id": 30, "seek": 13196, "start": 139.24, "end": 147.24, "text": " And I'm also loading in our data for MNIST as tensors.", "tokens": [50728, 400, 286, 478, 611, 15114, 294, 527, 1412, 337, 376, 45, 19756, 382, 10688, 830, 13, 51128], "temperature": 0.0, "avg_logprob": -0.4776119597970623, "compression_ratio": 1.4505494505494505, "no_speech_prob": 0.01798286661505699}, {"id": 31, "seek": 13196, "start": 147.24, "end": 156.20000000000002, "text": " Okay so we to start with need to create the basic architecture of our neural", "tokens": [51128, 1033, 370, 321, 281, 722, 365, 643, 281, 1884, 264, 3875, 9482, 295, 527, 18161, 51576], "temperature": 0.0, "avg_logprob": -0.4776119597970623, "compression_ratio": 1.4505494505494505, "no_speech_prob": 0.01798286661505699}, {"id": 32, "seek": 13196, "start": 156.20000000000002, "end": 159.0, "text": " network.", "tokens": [51576, 3209, 13, 51716], "temperature": 0.0, "avg_logprob": -0.4776119597970623, "compression_ratio": 1.4505494505494505, "no_speech_prob": 0.01798286661505699}, {"id": 33, "seek": 15900, "start": 159.08, "end": 163.64, "text": " And I did mention at the start of the course that we will briefly review", "tokens": [50368, 400, 286, 630, 2152, 412, 264, 722, 295, 264, 1164, 300, 321, 486, 10515, 3131, 50596], "temperature": 0.0, "avg_logprob": -0.47335235595703123, "compression_ratio": 1.5771428571428572, "no_speech_prob": 0.000855731253977865}, {"id": 34, "seek": 15900, "start": 163.64, "end": 167.24, "text": " everything that we need to cover. So we should briefly review", "tokens": [50596, 1203, 300, 321, 643, 281, 2060, 13, 407, 321, 820, 10515, 3131, 50776], "temperature": 0.0, "avg_logprob": -0.47335235595703123, "compression_ratio": 1.5771428571428572, "no_speech_prob": 0.000855731253977865}, {"id": 35, "seek": 15900, "start": 167.24, "end": 171.8, "text": " what basic neural networks are and why they are what they are.", "tokens": [50776, 437, 3875, 18161, 9590, 366, 293, 983, 436, 366, 437, 436, 366, 13, 51004], "temperature": 0.0, "avg_logprob": -0.47335235595703123, "compression_ratio": 1.5771428571428572, "no_speech_prob": 0.000855731253977865}, {"id": 36, "seek": 15900, "start": 171.8, "end": 175.64, "text": " So to start with let's consider", "tokens": [51004, 407, 281, 722, 365, 718, 311, 1949, 51196], "temperature": 0.0, "avg_logprob": -0.47335235595703123, "compression_ratio": 1.5771428571428572, "no_speech_prob": 0.000855731253977865}, {"id": 37, "seek": 15900, "start": 175.64, "end": 178.2, "text": " a", "tokens": [51196, 257, 51324], "temperature": 0.0, "avg_logprob": -0.47335235595703123, "compression_ratio": 1.5771428571428572, "no_speech_prob": 0.000855731253977865}, {"id": 38, "seek": 15900, "start": 180.92000000000002, "end": 186.04, "text": " a linear model. Oops that's not how I do it.", "tokens": [51460, 257, 8213, 2316, 13, 21726, 300, 311, 406, 577, 286, 360, 309, 13, 51716], "temperature": 0.0, "avg_logprob": -0.47335235595703123, "compression_ratio": 1.5771428571428572, "no_speech_prob": 0.000855731253977865}, {"id": 39, "seek": 18604, "start": 187.0, "end": 195.0, "text": " So let's start by considering a linear model of", "tokens": [50412, 407, 718, 311, 722, 538, 8079, 257, 8213, 2316, 295, 50812], "temperature": 0.0, "avg_logprob": -0.5709520558841893, "compression_ratio": 1.6, "no_speech_prob": 5.30741635884624e-05}, {"id": 40, "seek": 18604, "start": 195.0, "end": 199.79999999999998, "text": " well let's take the most simple example possible which is we're going to pick a", "tokens": [50812, 731, 718, 311, 747, 264, 881, 2199, 1365, 1944, 597, 307, 321, 434, 516, 281, 1888, 257, 51052], "temperature": 0.0, "avg_logprob": -0.5709520558841893, "compression_ratio": 1.6, "no_speech_prob": 5.30741635884624e-05}, {"id": 41, "seek": 18604, "start": 199.79999999999998, "end": 206.6, "text": " single pixel from from our MNIST pictures.", "tokens": [51052, 2167, 19261, 490, 490, 527, 376, 45, 19756, 5242, 13, 51392], "temperature": 0.0, "avg_logprob": -0.5709520558841893, "compression_ratio": 1.6, "no_speech_prob": 5.30741635884624e-05}, {"id": 42, "seek": 18604, "start": 206.6, "end": 211.95999999999998, "text": " And we're going to pick a single pixel from our MNIST", "tokens": [51392, 400, 321, 434, 516, 281, 1888, 257, 2167, 19261, 490, 527, 376, 45, 19756, 51660], "temperature": 0.0, "avg_logprob": -0.5709520558841893, "compression_ratio": 1.6, "no_speech_prob": 5.30741635884624e-05}, {"id": 43, "seek": 21196, "start": 212.84, "end": 219.96, "text": " from from our MNIST pictures. And so that will be our x.", "tokens": [50408, 490, 490, 527, 376, 45, 19756, 5242, 13, 400, 370, 300, 486, 312, 527, 2031, 13, 50764], "temperature": 0.0, "avg_logprob": -0.29090081491777975, "compression_ratio": 1.4551724137931035, "no_speech_prob": 0.00012730549497064203}, {"id": 44, "seek": 21196, "start": 219.96, "end": 224.04000000000002, "text": " And for our", "tokens": [50764, 400, 337, 527, 50968], "temperature": 0.0, "avg_logprob": -0.29090081491777975, "compression_ratio": 1.4551724137931035, "no_speech_prob": 0.00012730549497064203}, {"id": 45, "seek": 21196, "start": 224.44, "end": 228.44, "text": " for our y values then we'll have some", "tokens": [50988, 337, 527, 288, 4190, 550, 321, 603, 362, 512, 51188], "temperature": 0.0, "avg_logprob": -0.29090081491777975, "compression_ratio": 1.4551724137931035, "no_speech_prob": 0.00012730549497064203}, {"id": 46, "seek": 21196, "start": 228.44, "end": 233.56, "text": " some loss function of how good is this model.", "tokens": [51188, 512, 4470, 2445, 295, 577, 665, 307, 341, 2316, 13, 51444], "temperature": 0.0, "avg_logprob": -0.29090081491777975, "compression_ratio": 1.4551724137931035, "no_speech_prob": 0.00012730549497064203}, {"id": 47, "seek": 21196, "start": 233.56, "end": 240.04000000000002, "text": " Sorry not some loss function. That's created even simpler.", "tokens": [51444, 4919, 406, 512, 4470, 2445, 13, 663, 311, 2942, 754, 18587, 13, 51768], "temperature": 0.0, "avg_logprob": -0.29090081491777975, "compression_ratio": 1.4551724137931035, "no_speech_prob": 0.00012730549497064203}, {"id": 48, "seek": 24004, "start": 240.04, "end": 246.84, "text": " For our y value we're going to be looking at how likely is it that this is", "tokens": [50364, 1171, 527, 288, 2158, 321, 434, 516, 281, 312, 1237, 412, 577, 3700, 307, 309, 300, 341, 307, 50704], "temperature": 0.0, "avg_logprob": -0.17717467414008248, "compression_ratio": 1.6536312849162011, "no_speech_prob": 1.553494712425163e-06}, {"id": 49, "seek": 24004, "start": 246.84, "end": 252.76, "text": " say the number three based on the value of this one pixel.", "tokens": [50704, 584, 264, 1230, 1045, 2361, 322, 264, 2158, 295, 341, 472, 19261, 13, 51000], "temperature": 0.0, "avg_logprob": -0.17717467414008248, "compression_ratio": 1.6536312849162011, "no_speech_prob": 1.553494712425163e-06}, {"id": 50, "seek": 24004, "start": 252.76, "end": 258.76, "text": " So the pixel its value will be x and the probability of being the", "tokens": [51000, 407, 264, 19261, 1080, 2158, 486, 312, 2031, 293, 264, 8482, 295, 885, 264, 51300], "temperature": 0.0, "avg_logprob": -0.17717467414008248, "compression_ratio": 1.6536312849162011, "no_speech_prob": 1.553494712425163e-06}, {"id": 51, "seek": 24004, "start": 258.76, "end": 264.12, "text": " number three we'll call y. And if we just have a", "tokens": [51300, 1230, 1045, 321, 603, 818, 288, 13, 400, 498, 321, 445, 362, 257, 51568], "temperature": 0.0, "avg_logprob": -0.17717467414008248, "compression_ratio": 1.6536312849162011, "no_speech_prob": 1.553494712425163e-06}, {"id": 52, "seek": 24004, "start": 264.12, "end": 268.52, "text": " linear model then it's going to look like this.", "tokens": [51568, 8213, 2316, 550, 309, 311, 516, 281, 574, 411, 341, 13, 51788], "temperature": 0.0, "avg_logprob": -0.17717467414008248, "compression_ratio": 1.6536312849162011, "no_speech_prob": 1.553494712425163e-06}, {"id": 53, "seek": 26852, "start": 269.47999999999996, "end": 274.12, "text": " And so in this case it's it's saying that the brighter this pixel is the more", "tokens": [50412, 400, 370, 294, 341, 1389, 309, 311, 309, 311, 1566, 300, 264, 19764, 341, 19261, 307, 264, 544, 50644], "temperature": 0.0, "avg_logprob": -0.20226316098813657, "compression_ratio": 1.621761658031088, "no_speech_prob": 5.682405571860727e-06}, {"id": 54, "seek": 26852, "start": 274.12, "end": 277.88, "text": " likely it is that it's the number three.", "tokens": [50644, 3700, 309, 307, 300, 309, 311, 264, 1230, 1045, 13, 50832], "temperature": 0.0, "avg_logprob": -0.20226316098813657, "compression_ratio": 1.621761658031088, "no_speech_prob": 5.682405571860727e-06}, {"id": 55, "seek": 26852, "start": 277.88, "end": 280.59999999999997, "text": " And so", "tokens": [50832, 400, 370, 50968], "temperature": 0.0, "avg_logprob": -0.20226316098813657, "compression_ratio": 1.621761658031088, "no_speech_prob": 5.682405571860727e-06}, {"id": 56, "seek": 26852, "start": 281.88, "end": 285.96, "text": " there's a few problems with this. The first one obviously is that as a", "tokens": [51032, 456, 311, 257, 1326, 2740, 365, 341, 13, 440, 700, 472, 2745, 307, 300, 382, 257, 51236], "temperature": 0.0, "avg_logprob": -0.20226316098813657, "compression_ratio": 1.621761658031088, "no_speech_prob": 5.682405571860727e-06}, {"id": 57, "seek": 26852, "start": 285.96, "end": 292.35999999999996, "text": " linear model it's very limiting because maybe you know we actually", "tokens": [51236, 8213, 2316, 309, 311, 588, 22083, 570, 1310, 291, 458, 321, 767, 51556], "temperature": 0.0, "avg_logprob": -0.20226316098813657, "compression_ratio": 1.621761658031088, "no_speech_prob": 5.682405571860727e-06}, {"id": 58, "seek": 26852, "start": 292.35999999999996, "end": 298.28, "text": " are trying to draw something that looks more like", "tokens": [51556, 366, 1382, 281, 2642, 746, 300, 1542, 544, 411, 51852], "temperature": 0.0, "avg_logprob": -0.20226316098813657, "compression_ratio": 1.621761658031088, "no_speech_prob": 5.682405571860727e-06}, {"id": 59, "seek": 29828, "start": 298.35999999999996, "end": 304.67999999999995, "text": " this. So how would you do that? Well there's actually a neat trick we", "tokens": [50368, 341, 13, 407, 577, 576, 291, 360, 300, 30, 1042, 456, 311, 767, 257, 10654, 4282, 321, 50684], "temperature": 0.0, "avg_logprob": -0.17070557026380903, "compression_ratio": 1.6842105263157894, "no_speech_prob": 3.0415922083193436e-06}, {"id": 60, "seek": 29828, "start": 304.67999999999995, "end": 309.32, "text": " can use to do that. What we could do", "tokens": [50684, 393, 764, 281, 360, 300, 13, 708, 321, 727, 360, 50916], "temperature": 0.0, "avg_logprob": -0.17070557026380903, "compression_ratio": 1.6842105263157894, "no_speech_prob": 3.0415922083193436e-06}, {"id": 61, "seek": 29828, "start": 309.32, "end": 314.03999999999996, "text": " is well let's first of all talk about something we can't do.", "tokens": [50916, 307, 731, 718, 311, 700, 295, 439, 751, 466, 746, 321, 393, 380, 360, 13, 51152], "temperature": 0.0, "avg_logprob": -0.17070557026380903, "compression_ratio": 1.6842105263157894, "no_speech_prob": 3.0415922083193436e-06}, {"id": 62, "seek": 29828, "start": 314.03999999999996, "end": 320.52, "text": " Something we can't do is to add a bunch of additional lines. So consider", "tokens": [51152, 6595, 321, 393, 380, 360, 307, 281, 909, 257, 3840, 295, 4497, 3876, 13, 407, 1949, 51476], "temperature": 0.0, "avg_logprob": -0.17070557026380903, "compression_ratio": 1.6842105263157894, "no_speech_prob": 3.0415922083193436e-06}, {"id": 63, "seek": 29828, "start": 320.52, "end": 324.59999999999997, "text": " what happens if we say okay well let's add a few different lines. So let's also", "tokens": [51476, 437, 2314, 498, 321, 584, 1392, 731, 718, 311, 909, 257, 1326, 819, 3876, 13, 407, 718, 311, 611, 51680], "temperature": 0.0, "avg_logprob": -0.17070557026380903, "compression_ratio": 1.6842105263157894, "no_speech_prob": 3.0415922083193436e-06}, {"id": 64, "seek": 32460, "start": 324.6, "end": 331.56, "text": " add this line. So what would be the sum", "tokens": [50364, 909, 341, 1622, 13, 407, 437, 576, 312, 264, 2408, 50712], "temperature": 0.0, "avg_logprob": -0.1877843475341797, "compression_ratio": 1.5923566878980893, "no_speech_prob": 9.610221604816616e-05}, {"id": 65, "seek": 32460, "start": 331.56, "end": 333.96000000000004, "text": " of", "tokens": [50712, 295, 50832], "temperature": 0.0, "avg_logprob": -0.1877843475341797, "compression_ratio": 1.5923566878980893, "no_speech_prob": 9.610221604816616e-05}, {"id": 66, "seek": 32460, "start": 334.44, "end": 338.68, "text": " our two lines? Well the answer is of course that the sum of the two lines", "tokens": [50856, 527, 732, 3876, 30, 1042, 264, 1867, 307, 295, 1164, 300, 264, 2408, 295, 264, 732, 3876, 51068], "temperature": 0.0, "avg_logprob": -0.1877843475341797, "compression_ratio": 1.5923566878980893, "no_speech_prob": 9.610221604816616e-05}, {"id": 67, "seek": 32460, "start": 338.68, "end": 341.48, "text": " will itself be a line. So it's not going to help us at all", "tokens": [51068, 486, 2564, 312, 257, 1622, 13, 407, 309, 311, 406, 516, 281, 854, 505, 412, 439, 51208], "temperature": 0.0, "avg_logprob": -0.1877843475341797, "compression_ratio": 1.5923566878980893, "no_speech_prob": 9.610221604816616e-05}, {"id": 68, "seek": 32460, "start": 341.48, "end": 345.88, "text": " match the actual curve that we want.", "tokens": [51208, 2995, 264, 3539, 7605, 300, 321, 528, 13, 51428], "temperature": 0.0, "avg_logprob": -0.1877843475341797, "compression_ratio": 1.5923566878980893, "no_speech_prob": 9.610221604816616e-05}, {"id": 69, "seek": 32460, "start": 346.36, "end": 352.12, "text": " So here's the trick. Instead we could", "tokens": [51452, 407, 510, 311, 264, 4282, 13, 7156, 321, 727, 51740], "temperature": 0.0, "avg_logprob": -0.1877843475341797, "compression_ratio": 1.5923566878980893, "no_speech_prob": 9.610221604816616e-05}, {"id": 70, "seek": 35212, "start": 352.36, "end": 357.72, "text": " create a line like this", "tokens": [50376, 1884, 257, 1622, 411, 341, 50644], "temperature": 0.0, "avg_logprob": -0.23262158218695192, "compression_ratio": 1.4957264957264957, "no_speech_prob": 9.08045421965653e-06}, {"id": 71, "seek": 35212, "start": 363.88, "end": 366.84000000000003, "text": " that actually", "tokens": [50952, 300, 767, 51100], "temperature": 0.0, "avg_logprob": -0.23262158218695192, "compression_ratio": 1.4957264957264957, "no_speech_prob": 9.08045421965653e-06}, {"id": 72, "seek": 35212, "start": 370.92, "end": 375.4, "text": " we could create this line. And now consider what happens if we add this", "tokens": [51304, 321, 727, 1884, 341, 1622, 13, 400, 586, 1949, 437, 2314, 498, 321, 909, 341, 51528], "temperature": 0.0, "avg_logprob": -0.23262158218695192, "compression_ratio": 1.4957264957264957, "no_speech_prob": 9.08045421965653e-06}, {"id": 73, "seek": 35212, "start": 375.4, "end": 379.0, "text": " original line with this new what's not a line right it's a it's a", "tokens": [51528, 3380, 1622, 365, 341, 777, 437, 311, 406, 257, 1622, 558, 309, 311, 257, 309, 311, 257, 51708], "temperature": 0.0, "avg_logprob": -0.23262158218695192, "compression_ratio": 1.4957264957264957, "no_speech_prob": 9.08045421965653e-06}, {"id": 74, "seek": 37900, "start": 379.0, "end": 385.16, "text": " two line segments. So what we would get is this everything", "tokens": [50364, 732, 1622, 19904, 13, 407, 437, 321, 576, 483, 307, 341, 1203, 50672], "temperature": 0.0, "avg_logprob": -0.1988506317138672, "compression_ratio": 1.5942857142857143, "no_speech_prob": 0.00020342641801107675}, {"id": 75, "seek": 37900, "start": 385.16, "end": 391.16, "text": " to the left of", "tokens": [50672, 281, 264, 1411, 295, 50972], "temperature": 0.0, "avg_logprob": -0.1988506317138672, "compression_ratio": 1.5942857142857143, "no_speech_prob": 0.00020342641801107675}, {"id": 76, "seek": 37900, "start": 391.48, "end": 397.64, "text": " this point is going to not be changed if I add these two lines together.", "tokens": [50988, 341, 935, 307, 516, 281, 406, 312, 3105, 498, 286, 909, 613, 732, 3876, 1214, 13, 51296], "temperature": 0.0, "avg_logprob": -0.1988506317138672, "compression_ratio": 1.5942857142857143, "no_speech_prob": 0.00020342641801107675}, {"id": 77, "seek": 37900, "start": 397.64, "end": 400.76, "text": " Because this is zero all the way and everything to the right of it", "tokens": [51296, 1436, 341, 307, 4018, 439, 264, 636, 293, 1203, 281, 264, 558, 295, 309, 51452], "temperature": 0.0, "avg_logprob": -0.1988506317138672, "compression_ratio": 1.5942857142857143, "no_speech_prob": 0.00020342641801107675}, {"id": 78, "seek": 37900, "start": 400.76, "end": 404.12, "text": " is going to be reduced. It looks like they've got similar slopes.", "tokens": [51452, 307, 516, 281, 312, 9212, 13, 467, 1542, 411, 436, 600, 658, 2531, 37725, 13, 51620], "temperature": 0.0, "avg_logprob": -0.1988506317138672, "compression_ratio": 1.5942857142857143, "no_speech_prob": 0.00020342641801107675}, {"id": 79, "seek": 40412, "start": 404.12, "end": 409.72, "text": " So we might end up with instead so this would all", "tokens": [50364, 407, 321, 1062, 917, 493, 365, 2602, 370, 341, 576, 439, 50644], "temperature": 0.0, "avg_logprob": -0.21422248898130475, "compression_ratio": 1.6794871794871795, "no_speech_prob": 4.092903054697672e-06}, {"id": 80, "seek": 40412, "start": 409.72, "end": 417.48, "text": " disappear here and instead we would end up with something like", "tokens": [50644, 11596, 510, 293, 2602, 321, 576, 917, 493, 365, 746, 411, 51032], "temperature": 0.0, "avg_logprob": -0.21422248898130475, "compression_ratio": 1.6794871794871795, "no_speech_prob": 4.092903054697672e-06}, {"id": 81, "seek": 40412, "start": 418.36, "end": 426.2, "text": " this. And then we could do that again right we could add an additional", "tokens": [51076, 341, 13, 400, 550, 321, 727, 360, 300, 797, 558, 321, 727, 909, 364, 4497, 51468], "temperature": 0.0, "avg_logprob": -0.21422248898130475, "compression_ratio": 1.6794871794871795, "no_speech_prob": 4.092903054697672e-06}, {"id": 82, "seek": 40412, "start": 426.2, "end": 430.84000000000003, "text": " line that looks a bit like that. So it would go but this time it could go even", "tokens": [51468, 1622, 300, 1542, 257, 857, 411, 300, 13, 407, 309, 576, 352, 457, 341, 565, 309, 727, 352, 754, 51700], "temperature": 0.0, "avg_logprob": -0.21422248898130475, "compression_ratio": 1.6794871794871795, "no_speech_prob": 4.092903054697672e-06}, {"id": 83, "seek": 43084, "start": 430.84, "end": 436.11999999999995, "text": " further out here and it could be", "tokens": [50364, 3052, 484, 510, 293, 309, 727, 312, 50628], "temperature": 0.0, "avg_logprob": -0.21186667222243089, "compression_ratio": 1.4181818181818182, "no_speech_prob": 7.031162385828793e-05}, {"id": 84, "seek": 43084, "start": 437.96, "end": 445.0, "text": " something like this. So what if we added that? Well again", "tokens": [50720, 746, 411, 341, 13, 407, 437, 498, 321, 3869, 300, 30, 1042, 797, 51072], "temperature": 0.0, "avg_logprob": -0.21186667222243089, "compression_ratio": 1.4181818181818182, "no_speech_prob": 7.031162385828793e-05}, {"id": 85, "seek": 43084, "start": 445.0, "end": 449.08, "text": " at the point underneath here it's always zero so it won't do", "tokens": [51072, 412, 264, 935, 7223, 510, 309, 311, 1009, 4018, 370, 309, 1582, 380, 360, 51276], "temperature": 0.0, "avg_logprob": -0.21186667222243089, "compression_ratio": 1.4181818181818182, "no_speech_prob": 7.031162385828793e-05}, {"id": 86, "seek": 43084, "start": 449.08, "end": 452.12, "text": " anything at all. But after that it's going to make it even more negatively", "tokens": [51276, 1340, 412, 439, 13, 583, 934, 300, 309, 311, 516, 281, 652, 309, 754, 544, 29519, 51428], "temperature": 0.0, "avg_logprob": -0.21186667222243089, "compression_ratio": 1.4181818181818182, "no_speech_prob": 7.031162385828793e-05}, {"id": 87, "seek": 43084, "start": 452.12, "end": 454.44, "text": " sloped.", "tokens": [51428, 21254, 292, 13, 51544], "temperature": 0.0, "avg_logprob": -0.21186667222243089, "compression_ratio": 1.4181818181818182, "no_speech_prob": 7.031162385828793e-05}, {"id": 88, "seek": 45444, "start": 455.4, "end": 461.4, "text": " And if you can see using this approach we could add up lots of these", "tokens": [50412, 400, 498, 291, 393, 536, 1228, 341, 3109, 321, 727, 909, 493, 3195, 295, 613, 50712], "temperature": 0.0, "avg_logprob": -0.35309033711751303, "compression_ratio": 1.6, "no_speech_prob": 0.0003301510005258024}, {"id": 89, "seek": 45444, "start": 461.4, "end": 467.16, "text": " rectified lines. These lines that truncate at zero. And we could create", "tokens": [50712, 11048, 2587, 3876, 13, 1981, 3876, 300, 504, 409, 66, 473, 412, 4018, 13, 400, 321, 727, 1884, 51000], "temperature": 0.0, "avg_logprob": -0.35309033711751303, "compression_ratio": 1.6, "no_speech_prob": 0.0003301510005258024}, {"id": 90, "seek": 45444, "start": 467.16, "end": 472.6, "text": " any shape we want with enough of them. And these lines are very easy to create", "tokens": [51000, 604, 3909, 321, 528, 365, 1547, 295, 552, 13, 400, 613, 3876, 366, 588, 1858, 281, 1884, 51272], "temperature": 0.0, "avg_logprob": -0.35309033711751303, "compression_ratio": 1.6, "no_speech_prob": 0.0003301510005258024}, {"id": 91, "seek": 45444, "start": 472.6, "end": 481.48, "text": " because actually all we need to do is to create just a regular line.", "tokens": [51272, 570, 767, 439, 321, 643, 281, 360, 307, 281, 1884, 445, 257, 3890, 1622, 13, 51716], "temperature": 0.0, "avg_logprob": -0.35309033711751303, "compression_ratio": 1.6, "no_speech_prob": 0.0003301510005258024}, {"id": 92, "seek": 48148, "start": 482.44, "end": 489.96000000000004, "text": " Just create a regular line right which we can move up down left right", "tokens": [50412, 1449, 1884, 257, 3890, 1622, 558, 597, 321, 393, 1286, 493, 760, 1411, 558, 50788], "temperature": 0.0, "avg_logprob": -0.31352726248807683, "compression_ratio": 1.691891891891892, "no_speech_prob": 3.6478490073932335e-05}, {"id": 93, "seek": 48148, "start": 489.96000000000004, "end": 495.88, "text": " change its angle whatever. And then just say if it's greater than zero truncate", "tokens": [50788, 1319, 1080, 5802, 2035, 13, 400, 550, 445, 584, 498, 309, 311, 5044, 813, 4018, 504, 409, 66, 473, 51084], "temperature": 0.0, "avg_logprob": -0.31352726248807683, "compression_ratio": 1.691891891891892, "no_speech_prob": 3.6478490073932335e-05}, {"id": 94, "seek": 48148, "start": 495.88, "end": 499.48, "text": " it to zero. Or we could do the opposite for a line", "tokens": [51084, 309, 281, 4018, 13, 1610, 321, 727, 360, 264, 6182, 337, 257, 1622, 51264], "temperature": 0.0, "avg_logprob": -0.31352726248807683, "compression_ratio": 1.691891891891892, "no_speech_prob": 3.6478490073932335e-05}, {"id": 95, "seek": 48148, "start": 499.48, "end": 501.8, "text": " going the opposite direction. If it's less than zero we could say truncate it", "tokens": [51264, 516, 264, 6182, 3513, 13, 759, 309, 311, 1570, 813, 4018, 321, 727, 584, 504, 409, 66, 473, 309, 51380], "temperature": 0.0, "avg_logprob": -0.31352726248807683, "compression_ratio": 1.691891891891892, "no_speech_prob": 3.6478490073932335e-05}, {"id": 96, "seek": 48148, "start": 501.8, "end": 507.32, "text": " to zero. And that would get rid of", "tokens": [51380, 281, 4018, 13, 400, 300, 576, 483, 3973, 295, 51656], "temperature": 0.0, "avg_logprob": -0.31352726248807683, "compression_ratio": 1.691891891891892, "no_speech_prob": 3.6478490073932335e-05}, {"id": 97, "seek": 50732, "start": 508.2, "end": 512.04, "text": " as we want this whole section here", "tokens": [50408, 382, 321, 528, 341, 1379, 3541, 510, 50600], "temperature": 0.0, "avg_logprob": -0.2091289813701923, "compression_ratio": 1.4593023255813953, "no_speech_prob": 3.120162000413984e-05}, {"id": 98, "seek": 50732, "start": 512.28, "end": 519.56, "text": " and make it flat. Okay so these are rectified lines. And so we can sum up a", "tokens": [50612, 293, 652, 309, 4962, 13, 1033, 370, 613, 366, 11048, 2587, 3876, 13, 400, 370, 321, 393, 2408, 493, 257, 50976], "temperature": 0.0, "avg_logprob": -0.2091289813701923, "compression_ratio": 1.4593023255813953, "no_speech_prob": 3.120162000413984e-05}, {"id": 99, "seek": 50732, "start": 519.56, "end": 525.56, "text": " bunch of these together to basically match any arbitrary curve.", "tokens": [50976, 3840, 295, 613, 1214, 281, 1936, 2995, 604, 23211, 7605, 13, 51276], "temperature": 0.0, "avg_logprob": -0.2091289813701923, "compression_ratio": 1.4593023255813953, "no_speech_prob": 3.120162000413984e-05}, {"id": 100, "seek": 50732, "start": 530.76, "end": 534.4399999999999, "text": " So let's start by doing that. Oh the other thing we should mention of course", "tokens": [51536, 407, 718, 311, 722, 538, 884, 300, 13, 876, 264, 661, 551, 321, 820, 2152, 295, 1164, 51720], "temperature": 0.0, "avg_logprob": -0.2091289813701923, "compression_ratio": 1.4593023255813953, "no_speech_prob": 3.120162000413984e-05}, {"id": 101, "seek": 53444, "start": 534.5200000000001, "end": 539.5600000000001, "text": " is that we're going to have not just one pixel but we're going to", "tokens": [50368, 307, 300, 321, 434, 516, 281, 362, 406, 445, 472, 19261, 457, 321, 434, 516, 281, 50620], "temperature": 0.0, "avg_logprob": -0.20505391634427583, "compression_ratio": 1.7209302325581395, "no_speech_prob": 0.0018386006122455}, {"id": 102, "seek": 53444, "start": 539.5600000000001, "end": 544.2, "text": " have lots of pixels. So to start with the", "tokens": [50620, 362, 3195, 295, 18668, 13, 407, 281, 722, 365, 264, 50852], "temperature": 0.0, "avg_logprob": -0.20505391634427583, "compression_ratio": 1.7209302325581395, "no_speech_prob": 0.0018386006122455}, {"id": 103, "seek": 53444, "start": 544.2, "end": 546.5200000000001, "text": " kind of", "tokens": [50852, 733, 295, 50968], "temperature": 0.0, "avg_logprob": -0.20505391634427583, "compression_ratio": 1.7209302325581395, "no_speech_prob": 0.0018386006122455}, {"id": 104, "seek": 53444, "start": 550.36, "end": 554.12, "text": " most you know slightly you know the only slightly", "tokens": [51160, 881, 291, 458, 4748, 291, 458, 264, 787, 4748, 51348], "temperature": 0.0, "avg_logprob": -0.20505391634427583, "compression_ratio": 1.7209302325581395, "no_speech_prob": 0.0018386006122455}, {"id": 105, "seek": 53444, "start": 554.12, "end": 559.96, "text": " less simple approach. We could have something where we've got", "tokens": [51348, 1570, 2199, 3109, 13, 492, 727, 362, 746, 689, 321, 600, 658, 51640], "temperature": 0.0, "avg_logprob": -0.20505391634427583, "compression_ratio": 1.7209302325581395, "no_speech_prob": 0.0018386006122455}, {"id": 106, "seek": 53444, "start": 559.96, "end": 563.32, "text": " you know pixel number one and pixel number two. We're looking at two", "tokens": [51640, 291, 458, 19261, 1230, 472, 293, 19261, 1230, 732, 13, 492, 434, 1237, 412, 732, 51808], "temperature": 0.0, "avg_logprob": -0.20505391634427583, "compression_ratio": 1.7209302325581395, "no_speech_prob": 0.0018386006122455}, {"id": 107, "seek": 56332, "start": 563.32, "end": 565.96, "text": " different pixels to see how likely they are to be the", "tokens": [50364, 819, 18668, 281, 536, 577, 3700, 436, 366, 281, 312, 264, 50496], "temperature": 0.0, "avg_logprob": -0.2244756438515403, "compression_ratio": 1.3577235772357723, "no_speech_prob": 3.480797749944031e-05}, {"id": 108, "seek": 56332, "start": 565.96, "end": 574.0400000000001, "text": " number three. And so that would allow us to draw", "tokens": [50496, 1230, 1045, 13, 400, 370, 300, 576, 2089, 505, 281, 2642, 50900], "temperature": 0.0, "avg_logprob": -0.2244756438515403, "compression_ratio": 1.3577235772357723, "no_speech_prob": 3.480797749944031e-05}, {"id": 109, "seek": 56332, "start": 575.8000000000001, "end": 581.24, "text": " more complex shapes that have some kind of", "tokens": [50988, 544, 3997, 10854, 300, 362, 512, 733, 295, 51260], "temperature": 0.0, "avg_logprob": -0.2244756438515403, "compression_ratio": 1.3577235772357723, "no_speech_prob": 3.480797749944031e-05}, {"id": 110, "seek": 56332, "start": 583.6400000000001, "end": 586.7600000000001, "text": " surface between them.", "tokens": [51380, 3753, 1296, 552, 13, 51536], "temperature": 0.0, "avg_logprob": -0.2244756438515403, "compression_ratio": 1.3577235772357723, "no_speech_prob": 3.480797749944031e-05}, {"id": 111, "seek": 58676, "start": 587.72, "end": 591.0, "text": " Okay and then we can do exactly the same thing", "tokens": [50412, 1033, 293, 550, 321, 393, 360, 2293, 264, 912, 551, 50576], "temperature": 0.0, "avg_logprob": -0.33020735278572005, "compression_ratio": 1.7512437810945274, "no_speech_prob": 5.475906073115766e-05}, {"id": 112, "seek": 58676, "start": 591.0, "end": 596.12, "text": " is to create these surfaces. We can add up", "tokens": [50576, 307, 281, 1884, 613, 16130, 13, 492, 393, 909, 493, 50832], "temperature": 0.0, "avg_logprob": -0.33020735278572005, "compression_ratio": 1.7512437810945274, "no_speech_prob": 5.475906073115766e-05}, {"id": 113, "seek": 58676, "start": 596.12, "end": 601.3199999999999, "text": " lots of these rectified lines together. But now they're going to be kind of", "tokens": [50832, 3195, 295, 613, 11048, 2587, 3876, 1214, 13, 583, 586, 436, 434, 516, 281, 312, 733, 295, 51092], "temperature": 0.0, "avg_logprob": -0.33020735278572005, "compression_ratio": 1.7512437810945274, "no_speech_prob": 5.475906073115766e-05}, {"id": 114, "seek": 58676, "start": 601.3199999999999, "end": 606.4399999999999, "text": " rectified planes. But it's going to be exactly the same", "tokens": [51092, 11048, 2587, 14952, 13, 583, 309, 311, 516, 281, 312, 2293, 264, 912, 51348], "temperature": 0.0, "avg_logprob": -0.33020735278572005, "compression_ratio": 1.7512437810945274, "no_speech_prob": 5.475906073115766e-05}, {"id": 115, "seek": 58676, "start": 606.4399999999999, "end": 610.04, "text": " thing. We're going to be adding together a bunch of lines each one of which is", "tokens": [51348, 551, 13, 492, 434, 516, 281, 312, 5127, 1214, 257, 3840, 295, 3876, 1184, 472, 295, 597, 307, 51528], "temperature": 0.0, "avg_logprob": -0.33020735278572005, "compression_ratio": 1.7512437810945274, "no_speech_prob": 5.475906073115766e-05}, {"id": 116, "seek": 58676, "start": 610.04, "end": 614.76, "text": " truncated at zero. Okay so that's the quick review.", "tokens": [51528, 504, 409, 66, 770, 412, 4018, 13, 1033, 370, 300, 311, 264, 1702, 3131, 13, 51764], "temperature": 0.0, "avg_logprob": -0.33020735278572005, "compression_ratio": 1.7512437810945274, "no_speech_prob": 5.475906073115766e-05}, {"id": 117, "seek": 61476, "start": 615.4, "end": 619.56, "text": " And so to do that", "tokens": [50396, 400, 370, 281, 360, 300, 50604], "temperature": 0.0, "avg_logprob": -0.23415601051459878, "compression_ratio": 1.544776119402985, "no_speech_prob": 0.0011513655772432685}, {"id": 118, "seek": 61476, "start": 620.12, "end": 623.24, "text": " we'll start out by just defining a few variables. So n", "tokens": [50632, 321, 603, 722, 484, 538, 445, 17827, 257, 1326, 9102, 13, 407, 297, 50788], "temperature": 0.0, "avg_logprob": -0.23415601051459878, "compression_ratio": 1.544776119402985, "no_speech_prob": 0.0011513655772432685}, {"id": 119, "seek": 61476, "start": 623.24, "end": 632.84, "text": " is the number of training examples. m is the number of pixels.", "tokens": [50788, 307, 264, 1230, 295, 3097, 5110, 13, 275, 307, 264, 1230, 295, 18668, 13, 51268], "temperature": 0.0, "avg_logprob": -0.23415601051459878, "compression_ratio": 1.544776119402985, "no_speech_prob": 0.0011513655772432685}, {"id": 120, "seek": 61476, "start": 633.48, "end": 640.36, "text": " c is the number of possible values of our digits. And so here they are.", "tokens": [51300, 269, 307, 264, 1230, 295, 1944, 4190, 295, 527, 27011, 13, 400, 370, 510, 436, 366, 13, 51644], "temperature": 0.0, "avg_logprob": -0.23415601051459878, "compression_ratio": 1.544776119402985, "no_speech_prob": 0.0011513655772432685}, {"id": 121, "seek": 64036, "start": 640.36, "end": 646.6, "text": " 50,000 samples, 784 pixels and 10 possible outputs.", "tokens": [50364, 2625, 11, 1360, 10938, 11, 1614, 25494, 18668, 293, 1266, 1944, 23930, 13, 50676], "temperature": 0.0, "avg_logprob": -0.1977757859504086, "compression_ratio": 1.509433962264151, "no_speech_prob": 0.0015247573610395193}, {"id": 122, "seek": 64036, "start": 648.36, "end": 653.88, "text": " Okay so what we do is to is we basically decide ahead of time how", "tokens": [50764, 1033, 370, 437, 321, 360, 307, 281, 307, 321, 1936, 4536, 2286, 295, 565, 577, 51040], "temperature": 0.0, "avg_logprob": -0.1977757859504086, "compression_ratio": 1.509433962264151, "no_speech_prob": 0.0015247573610395193}, {"id": 123, "seek": 64036, "start": 653.88, "end": 659.48, "text": " many of these line segment thingies to add up.", "tokens": [51040, 867, 295, 613, 1622, 9469, 551, 530, 281, 909, 493, 13, 51320], "temperature": 0.0, "avg_logprob": -0.1977757859504086, "compression_ratio": 1.509433962264151, "no_speech_prob": 0.0015247573610395193}, {"id": 124, "seek": 64036, "start": 659.48, "end": 663.96, "text": " And so the number that we create in a layer is called the number of hidden", "tokens": [51320, 400, 370, 264, 1230, 300, 321, 1884, 294, 257, 4583, 307, 1219, 264, 1230, 295, 7633, 51544], "temperature": 0.0, "avg_logprob": -0.1977757859504086, "compression_ratio": 1.509433962264151, "no_speech_prob": 0.0015247573610395193}, {"id": 125, "seek": 64036, "start": 663.96, "end": 668.52, "text": " nodes or activations. So we'll call that nh. So let's just arbitrarily decide on", "tokens": [51544, 13891, 420, 2430, 763, 13, 407, 321, 603, 818, 300, 6245, 13, 407, 718, 311, 445, 19071, 3289, 4536, 322, 51772], "temperature": 0.0, "avg_logprob": -0.1977757859504086, "compression_ratio": 1.509433962264151, "no_speech_prob": 0.0015247573610395193}, {"id": 126, "seek": 66852, "start": 668.52, "end": 675.8, "text": " creating 50 of those. So in order to create lots of lines", "tokens": [50364, 4084, 2625, 295, 729, 13, 407, 294, 1668, 281, 1884, 3195, 295, 3876, 50728], "temperature": 0.0, "avg_logprob": -0.21245011112146212, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.00014883739640936255}, {"id": 127, "seek": 66852, "start": 675.8, "end": 681.96, "text": " which we're going to truncate at zero, we can do a matrix multiplication. So", "tokens": [50728, 597, 321, 434, 516, 281, 504, 409, 66, 473, 412, 4018, 11, 321, 393, 360, 257, 8141, 27290, 13, 407, 51036], "temperature": 0.0, "avg_logprob": -0.21245011112146212, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.00014883739640936255}, {"id": 128, "seek": 66852, "start": 681.96, "end": 685.56, "text": " with a matrix multiplication", "tokens": [51036, 365, 257, 8141, 27290, 51216], "temperature": 0.0, "avg_logprob": -0.21245011112146212, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.00014883739640936255}, {"id": 129, "seek": 66852, "start": 688.1999999999999, "end": 694.04, "text": " we're going to have something where we've got", "tokens": [51348, 321, 434, 516, 281, 362, 746, 689, 321, 600, 658, 51640], "temperature": 0.0, "avg_logprob": -0.21245011112146212, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.00014883739640936255}, {"id": 130, "seek": 69404, "start": 694.68, "end": 699.4, "text": " 50,000 rows", "tokens": [50396, 2625, 11, 1360, 13241, 50632], "temperature": 0.0, "avg_logprob": -0.38514218610875744, "compression_ratio": 0.9887640449438202, "no_speech_prob": 5.829104338772595e-05}, {"id": 131, "seek": 69404, "start": 700.12, "end": 707.56, "text": " by 700, was it 784? Yeah by 784 columns.", "tokens": [50668, 538, 15204, 11, 390, 309, 1614, 25494, 30, 865, 538, 1614, 25494, 13766, 13, 51040], "temperature": 0.0, "avg_logprob": -0.38514218610875744, "compression_ratio": 0.9887640449438202, "no_speech_prob": 5.829104338772595e-05}, {"id": 132, "seek": 69404, "start": 710.36, "end": 716.12, "text": " And we're going to multiply that by", "tokens": [51180, 400, 321, 434, 516, 281, 12972, 300, 538, 51468], "temperature": 0.0, "avg_logprob": -0.38514218610875744, "compression_ratio": 0.9887640449438202, "no_speech_prob": 5.829104338772595e-05}, {"id": 133, "seek": 71612, "start": 717.08, "end": 725.16, "text": " something with 784 rows and 10 columns.", "tokens": [50412, 746, 365, 1614, 25494, 13241, 293, 1266, 13766, 13, 50816], "temperature": 0.0, "avg_logprob": -0.2547222572036936, "compression_ratio": 1.518716577540107, "no_speech_prob": 7.031167478999123e-05}, {"id": 134, "seek": 71612, "start": 725.16, "end": 730.2, "text": " And why is that? Well that's because if we take this very first", "tokens": [50816, 400, 983, 307, 300, 30, 1042, 300, 311, 570, 498, 321, 747, 341, 588, 700, 51068], "temperature": 0.0, "avg_logprob": -0.2547222572036936, "compression_ratio": 1.518716577540107, "no_speech_prob": 7.031167478999123e-05}, {"id": 135, "seek": 71612, "start": 730.2, "end": 733.48, "text": " line of this first vector here, row one,", "tokens": [51068, 1622, 295, 341, 700, 8062, 510, 11, 5386, 472, 11, 51232], "temperature": 0.0, "avg_logprob": -0.2547222572036936, "compression_ratio": 1.518716577540107, "no_speech_prob": 7.031167478999123e-05}, {"id": 136, "seek": 71612, "start": 733.48, "end": 738.2, "text": " we have 784 values. They're the pixel values of the first image.", "tokens": [51232, 321, 362, 1614, 25494, 4190, 13, 814, 434, 264, 19261, 4190, 295, 264, 700, 3256, 13, 51468], "temperature": 0.0, "avg_logprob": -0.2547222572036936, "compression_ratio": 1.518716577540107, "no_speech_prob": 7.031167478999123e-05}, {"id": 137, "seek": 71612, "start": 738.2, "end": 741.5600000000001, "text": " Okay so this is our first image and so they're each going to each of those", "tokens": [51468, 1033, 370, 341, 307, 527, 700, 3256, 293, 370, 436, 434, 1184, 516, 281, 1184, 295, 729, 51636], "temperature": 0.0, "avg_logprob": -0.2547222572036936, "compression_ratio": 1.518716577540107, "no_speech_prob": 7.031167478999123e-05}, {"id": 138, "seek": 74156, "start": 741.56, "end": 748.5999999999999, "text": " 784 values will be multiplied by each of these 784 values in the", "tokens": [50364, 1614, 25494, 4190, 486, 312, 17207, 538, 1184, 295, 613, 1614, 25494, 4190, 294, 264, 50716], "temperature": 0.0, "avg_logprob": -0.25449874483305834, "compression_ratio": 1.3819444444444444, "no_speech_prob": 0.00036258919863030314}, {"id": 139, "seek": 74156, "start": 748.5999999999999, "end": 752.28, "text": " first column, the zero index column. And that's going to give us", "tokens": [50716, 700, 7738, 11, 264, 4018, 8186, 7738, 13, 400, 300, 311, 516, 281, 976, 505, 50900], "temperature": 0.0, "avg_logprob": -0.25449874483305834, "compression_ratio": 1.3819444444444444, "no_speech_prob": 0.00036258919863030314}, {"id": 140, "seek": 74156, "start": 752.28, "end": 759.0, "text": " a number in our output. So our output is going to be", "tokens": [50900, 257, 1230, 294, 527, 5598, 13, 407, 527, 5598, 307, 516, 281, 312, 51236], "temperature": 0.0, "avg_logprob": -0.25449874483305834, "compression_ratio": 1.3819444444444444, "no_speech_prob": 0.00036258919863030314}, {"id": 141, "seek": 74156, "start": 761.4799999999999, "end": 767.88, "text": " 50,000 images by", "tokens": [51360, 2625, 11, 1360, 5267, 538, 51680], "temperature": 0.0, "avg_logprob": -0.25449874483305834, "compression_ratio": 1.3819444444444444, "no_speech_prob": 0.00036258919863030314}, {"id": 142, "seek": 76788, "start": 768.4399999999999, "end": 774.28, "text": " 10. And so that result, we'll multiply those together and we'll add them up", "tokens": [50392, 1266, 13, 400, 370, 300, 1874, 11, 321, 603, 12972, 729, 1214, 293, 321, 603, 909, 552, 493, 50684], "temperature": 0.0, "avg_logprob": -0.19651400125943697, "compression_ratio": 1.8807339449541285, "no_speech_prob": 7.411273145407904e-06}, {"id": 143, "seek": 76788, "start": 774.28, "end": 779.24, "text": " and that result's going to end up over here in this first cell.", "tokens": [50684, 293, 300, 1874, 311, 516, 281, 917, 493, 670, 510, 294, 341, 700, 2815, 13, 50932], "temperature": 0.0, "avg_logprob": -0.19651400125943697, "compression_ratio": 1.8807339449541285, "no_speech_prob": 7.411273145407904e-06}, {"id": 144, "seek": 76788, "start": 779.24, "end": 783.32, "text": " And so each of these columns is going to eventually", "tokens": [50932, 400, 370, 1184, 295, 613, 13766, 307, 516, 281, 4728, 51136], "temperature": 0.0, "avg_logprob": -0.19651400125943697, "compression_ratio": 1.8807339449541285, "no_speech_prob": 7.411273145407904e-06}, {"id": 145, "seek": 76788, "start": 783.32, "end": 786.84, "text": " represent, if this is a linear model,", "tokens": [51136, 2906, 11, 498, 341, 307, 257, 8213, 2316, 11, 51312], "temperature": 0.0, "avg_logprob": -0.19651400125943697, "compression_ratio": 1.8807339449541285, "no_speech_prob": 7.411273145407904e-06}, {"id": 146, "seek": 76788, "start": 786.84, "end": 789.88, "text": " in this case this is just the example of doing a linear model,", "tokens": [51312, 294, 341, 1389, 341, 307, 445, 264, 1365, 295, 884, 257, 8213, 2316, 11, 51464], "temperature": 0.0, "avg_logprob": -0.19651400125943697, "compression_ratio": 1.8807339449541285, "no_speech_prob": 7.411273145407904e-06}, {"id": 147, "seek": 76788, "start": 789.88, "end": 793.08, "text": " each of these cells is going to represent the probability. So this first", "tokens": [51464, 1184, 295, 613, 5438, 307, 516, 281, 2906, 264, 8482, 13, 407, 341, 700, 51624], "temperature": 0.0, "avg_logprob": -0.19651400125943697, "compression_ratio": 1.8807339449541285, "no_speech_prob": 7.411273145407904e-06}, {"id": 148, "seek": 76788, "start": 793.08, "end": 795.24, "text": " column will be the probability of being zero", "tokens": [51624, 7738, 486, 312, 264, 8482, 295, 885, 4018, 51732], "temperature": 0.0, "avg_logprob": -0.19651400125943697, "compression_ratio": 1.8807339449541285, "no_speech_prob": 7.411273145407904e-06}, {"id": 149, "seek": 79524, "start": 795.24, "end": 798.2, "text": " and the second column will be the probability of one. The third column will", "tokens": [50364, 293, 264, 1150, 7738, 486, 312, 264, 8482, 295, 472, 13, 440, 2636, 7738, 486, 50512], "temperature": 0.0, "avg_logprob": -0.16542398734170882, "compression_ratio": 1.8708333333333333, "no_speech_prob": 5.9209083701716736e-05}, {"id": 150, "seek": 79524, "start": 798.2, "end": 801.8, "text": " be the probability of being a two and so forth. So that's why we're going", "tokens": [50512, 312, 264, 8482, 295, 885, 257, 732, 293, 370, 5220, 13, 407, 300, 311, 983, 321, 434, 516, 50692], "temperature": 0.0, "avg_logprob": -0.16542398734170882, "compression_ratio": 1.8708333333333333, "no_speech_prob": 5.9209083701716736e-05}, {"id": 151, "seek": 79524, "start": 801.8, "end": 806.04, "text": " to have these 10 columns, each one allowing us to", "tokens": [50692, 281, 362, 613, 1266, 13766, 11, 1184, 472, 8293, 505, 281, 50904], "temperature": 0.0, "avg_logprob": -0.16542398734170882, "compression_ratio": 1.8708333333333333, "no_speech_prob": 5.9209083701716736e-05}, {"id": 152, "seek": 79524, "start": 806.04, "end": 809.72, "text": " weight the 784 inputs. Now of course we're going to do", "tokens": [50904, 3364, 264, 1614, 25494, 15743, 13, 823, 295, 1164, 321, 434, 516, 281, 360, 51088], "temperature": 0.0, "avg_logprob": -0.16542398734170882, "compression_ratio": 1.8708333333333333, "no_speech_prob": 5.9209083701716736e-05}, {"id": 153, "seek": 79524, "start": 809.72, "end": 812.2, "text": " something a bit more tricky than that, which is actually we're going to have a", "tokens": [51088, 746, 257, 857, 544, 12414, 813, 300, 11, 597, 307, 767, 321, 434, 516, 281, 362, 257, 51212], "temperature": 0.0, "avg_logprob": -0.16542398734170882, "compression_ratio": 1.8708333333333333, "no_speech_prob": 5.9209083701716736e-05}, {"id": 154, "seek": 79524, "start": 812.2, "end": 818.6800000000001, "text": " 784 by 50 input going into a 784 by 50", "tokens": [51212, 1614, 25494, 538, 2625, 4846, 516, 666, 257, 1614, 25494, 538, 2625, 51536], "temperature": 0.0, "avg_logprob": -0.16542398734170882, "compression_ratio": 1.8708333333333333, "no_speech_prob": 5.9209083701716736e-05}, {"id": 155, "seek": 79524, "start": 818.6800000000001, "end": 822.6800000000001, "text": " output to create the 50 hidden layers. Then we're going to truncate those at", "tokens": [51536, 5598, 281, 1884, 264, 2625, 7633, 7914, 13, 1396, 321, 434, 516, 281, 504, 409, 66, 473, 729, 412, 51736], "temperature": 0.0, "avg_logprob": -0.16542398734170882, "compression_ratio": 1.8708333333333333, "no_speech_prob": 5.9209083701716736e-05}, {"id": 156, "seek": 82268, "start": 822.68, "end": 827.0799999999999, "text": " zero and then multiply that by a 50 by 10 to", "tokens": [50364, 4018, 293, 550, 12972, 300, 538, 257, 2625, 538, 1266, 281, 50584], "temperature": 0.0, "avg_logprob": -0.24522797266642252, "compression_ratio": 1.4901960784313726, "no_speech_prob": 5.738745676353574e-05}, {"id": 157, "seek": 82268, "start": 827.0799999999999, "end": 831.0, "text": " create our 10 output. So we do it in two steps.", "tokens": [50584, 1884, 527, 1266, 5598, 13, 407, 321, 360, 309, 294, 732, 4439, 13, 50780], "temperature": 0.0, "avg_logprob": -0.24522797266642252, "compression_ratio": 1.4901960784313726, "no_speech_prob": 5.738745676353574e-05}, {"id": 158, "seek": 82268, "start": 831.16, "end": 835.3199999999999, "text": " So the way SGD works is we start with just,", "tokens": [50788, 407, 264, 636, 34520, 35, 1985, 307, 321, 722, 365, 445, 11, 50996], "temperature": 0.0, "avg_logprob": -0.24522797266642252, "compression_ratio": 1.4901960784313726, "no_speech_prob": 5.738745676353574e-05}, {"id": 159, "seek": 82268, "start": 835.3199999999999, "end": 839.16, "text": " this is our weight matrix here,", "tokens": [50996, 341, 307, 527, 3364, 8141, 510, 11, 51188], "temperature": 0.0, "avg_logprob": -0.24522797266642252, "compression_ratio": 1.4901960784313726, "no_speech_prob": 5.738745676353574e-05}, {"id": 160, "seek": 82268, "start": 841.4, "end": 845.9599999999999, "text": " and this is our data, this is our outputs.", "tokens": [51300, 293, 341, 307, 527, 1412, 11, 341, 307, 527, 23930, 13, 51528], "temperature": 0.0, "avg_logprob": -0.24522797266642252, "compression_ratio": 1.4901960784313726, "no_speech_prob": 5.738745676353574e-05}, {"id": 161, "seek": 82268, "start": 845.9599999999999, "end": 848.92, "text": " The way it works", "tokens": [51528, 440, 636, 309, 1985, 51676], "temperature": 0.0, "avg_logprob": -0.24522797266642252, "compression_ratio": 1.4901960784313726, "no_speech_prob": 5.738745676353574e-05}, {"id": 162, "seek": 84892, "start": 849.4, "end": 853.56, "text": " is that this weight matrix is initially filled with random values.", "tokens": [50388, 307, 300, 341, 3364, 8141, 307, 9105, 6412, 365, 4974, 4190, 13, 50596], "temperature": 0.0, "avg_logprob": -0.21979130489725462, "compression_ratio": 1.654320987654321, "no_speech_prob": 0.00017130750347860157}, {"id": 163, "seek": 84892, "start": 853.56, "end": 856.8399999999999, "text": " Of course this contains our pixel values, this contains results.", "tokens": [50596, 2720, 1164, 341, 8306, 527, 19261, 4190, 11, 341, 8306, 3542, 13, 50760], "temperature": 0.0, "avg_logprob": -0.21979130489725462, "compression_ratio": 1.654320987654321, "no_speech_prob": 0.00017130750347860157}, {"id": 164, "seek": 84892, "start": 856.8399999999999, "end": 861.0, "text": " So W is going to start with random values.", "tokens": [50760, 407, 343, 307, 516, 281, 722, 365, 4974, 4190, 13, 50968], "temperature": 0.0, "avg_logprob": -0.21979130489725462, "compression_ratio": 1.654320987654321, "no_speech_prob": 0.00017130750347860157}, {"id": 165, "seek": 84892, "start": 864.28, "end": 868.4399999999999, "text": " So here's our weight matrix. It's going to have, as we discussed,", "tokens": [51132, 407, 510, 311, 527, 3364, 8141, 13, 467, 311, 516, 281, 362, 11, 382, 321, 7152, 11, 51340], "temperature": 0.0, "avg_logprob": -0.21979130489725462, "compression_ratio": 1.654320987654321, "no_speech_prob": 0.00017130750347860157}, {"id": 166, "seek": 84892, "start": 868.4399999999999, "end": 875.16, "text": " 50,000 by 50 random values.", "tokens": [51340, 2625, 11, 1360, 538, 2625, 4974, 4190, 13, 51676], "temperature": 0.0, "avg_logprob": -0.21979130489725462, "compression_ratio": 1.654320987654321, "no_speech_prob": 0.00017130750347860157}, {"id": 167, "seek": 87516, "start": 875.16, "end": 878.92, "text": " And it's not enough just to multiply, we also have to add.", "tokens": [50364, 400, 309, 311, 406, 1547, 445, 281, 12972, 11, 321, 611, 362, 281, 909, 13, 50552], "temperature": 0.0, "avg_logprob": -0.19233559225207178, "compression_ratio": 1.6130434782608696, "no_speech_prob": 2.5466268198215403e-05}, {"id": 168, "seek": 87516, "start": 878.92, "end": 883.24, "text": " So that's what makes it a linear function. So we call those the biases,", "tokens": [50552, 407, 300, 311, 437, 1669, 309, 257, 8213, 2445, 13, 407, 321, 818, 729, 264, 32152, 11, 50768], "temperature": 0.0, "avg_logprob": -0.19233559225207178, "compression_ratio": 1.6130434782608696, "no_speech_prob": 2.5466268198215403e-05}, {"id": 169, "seek": 87516, "start": 883.24, "end": 888.36, "text": " the things we add. We can just start those at zeros.", "tokens": [50768, 264, 721, 321, 909, 13, 492, 393, 445, 722, 729, 412, 35193, 13, 51024], "temperature": 0.0, "avg_logprob": -0.19233559225207178, "compression_ratio": 1.6130434782608696, "no_speech_prob": 2.5466268198215403e-05}, {"id": 170, "seek": 87516, "start": 888.36, "end": 891.4, "text": " So we'll need one for each output, so 50 of those.", "tokens": [51024, 407, 321, 603, 643, 472, 337, 1184, 5598, 11, 370, 2625, 295, 729, 13, 51176], "temperature": 0.0, "avg_logprob": -0.19233559225207178, "compression_ratio": 1.6130434782608696, "no_speech_prob": 2.5466268198215403e-05}, {"id": 171, "seek": 87516, "start": 891.4, "end": 895.0, "text": " And so that'll be layer one. And then as we just mentioned, layer two", "tokens": [51176, 400, 370, 300, 603, 312, 4583, 472, 13, 400, 550, 382, 321, 445, 2835, 11, 4583, 732, 51356], "temperature": 0.0, "avg_logprob": -0.19233559225207178, "compression_ratio": 1.6130434782608696, "no_speech_prob": 2.5466268198215403e-05}, {"id": 172, "seek": 87516, "start": 895.0, "end": 901.4, "text": " will be a matrix that goes from 50 hidden. And now I'm going to do", "tokens": [51356, 486, 312, 257, 8141, 300, 1709, 490, 2625, 7633, 13, 400, 586, 286, 478, 516, 281, 360, 51676], "temperature": 0.0, "avg_logprob": -0.19233559225207178, "compression_ratio": 1.6130434782608696, "no_speech_prob": 2.5466268198215403e-05}, {"id": 173, "seek": 90140, "start": 901.4, "end": 905.4, "text": " something totally cheating to simplify some of the calculations for the", "tokens": [50364, 746, 3879, 18309, 281, 20460, 512, 295, 264, 20448, 337, 264, 50564], "temperature": 0.0, "avg_logprob": -0.17486896012958728, "compression_ratio": 1.7515151515151515, "no_speech_prob": 0.0015011545037850738}, {"id": 174, "seek": 90140, "start": 905.4, "end": 908.1999999999999, "text": " calculus. I'm only going to create one output.", "tokens": [50564, 33400, 13, 286, 478, 787, 516, 281, 1884, 472, 5598, 13, 50704], "temperature": 0.0, "avg_logprob": -0.17486896012958728, "compression_ratio": 1.7515151515151515, "no_speech_prob": 0.0015011545037850738}, {"id": 175, "seek": 90140, "start": 908.1999999999999, "end": 913.0, "text": " Why am I going to create one output? That's because I'm not going to use", "tokens": [50704, 1545, 669, 286, 516, 281, 1884, 472, 5598, 30, 663, 311, 570, 286, 478, 406, 516, 281, 764, 50944], "temperature": 0.0, "avg_logprob": -0.17486896012958728, "compression_ratio": 1.7515151515151515, "no_speech_prob": 0.0015011545037850738}, {"id": 176, "seek": 90140, "start": 913.0, "end": 918.1999999999999, "text": " cross entropy just yet. Instead I'm going to use MSE.", "tokens": [50944, 3278, 30867, 445, 1939, 13, 7156, 286, 478, 516, 281, 764, 376, 5879, 13, 51204], "temperature": 0.0, "avg_logprob": -0.17486896012958728, "compression_ratio": 1.7515151515151515, "no_speech_prob": 0.0015011545037850738}, {"id": 177, "seek": 90140, "start": 918.1999999999999, "end": 923.64, "text": " So actually I'm going to create one output,", "tokens": [51204, 407, 767, 286, 478, 516, 281, 1884, 472, 5598, 11, 51476], "temperature": 0.0, "avg_logprob": -0.17486896012958728, "compression_ratio": 1.7515151515151515, "no_speech_prob": 0.0015011545037850738}, {"id": 178, "seek": 92364, "start": 923.8, "end": 928.84, "text": " which will literally just be, what number do I think it is?", "tokens": [50372, 597, 486, 3736, 445, 312, 11, 437, 1230, 360, 286, 519, 309, 307, 30, 50624], "temperature": 0.0, "avg_logprob": -0.5038715524876372, "compression_ratio": 1.7234042553191489, "no_speech_prob": 3.6119786273047794e-06}, {"id": 179, "seek": 92364, "start": 928.84, "end": 932.68, "text": " From zero to ten.", "tokens": [50624, 3358, 4018, 281, 2064, 13, 50816], "temperature": 0.0, "avg_logprob": -0.5038715524876372, "compression_ratio": 1.7234042553191489, "no_speech_prob": 3.6119786273047794e-06}, {"id": 180, "seek": 92364, "start": 932.68, "end": 936.68, "text": " And so then we're going to compare those to the actual,", "tokens": [50816, 400, 370, 550, 321, 434, 516, 281, 6794, 729, 281, 264, 3539, 11, 51016], "temperature": 0.0, "avg_logprob": -0.5038715524876372, "compression_ratio": 1.7234042553191489, "no_speech_prob": 3.6119786273047794e-06}, {"id": 181, "seek": 92364, "start": 936.68, "end": 940.68, "text": " so these will be our Y predictors. We normally use a little hat for that.", "tokens": [51016, 370, 613, 486, 312, 527, 398, 6069, 830, 13, 492, 5646, 764, 257, 707, 2385, 337, 300, 13, 51216], "temperature": 0.0, "avg_logprob": -0.5038715524876372, "compression_ratio": 1.7234042553191489, "no_speech_prob": 3.6119786273047794e-06}, {"id": 182, "seek": 92364, "start": 940.68, "end": 944.68, "text": " And we're going to compare that to our actuals.", "tokens": [51216, 400, 321, 434, 516, 281, 6794, 300, 281, 527, 3539, 82, 13, 51416], "temperature": 0.0, "avg_logprob": -0.5038715524876372, "compression_ratio": 1.7234042553191489, "no_speech_prob": 3.6119786273047794e-06}, {"id": 183, "seek": 92364, "start": 944.68, "end": 948.68, "text": " And yeah, in this case, we're going to compare those to the actuals.", "tokens": [51416, 400, 1338, 11, 294, 341, 1389, 11, 321, 434, 516, 281, 6794, 729, 281, 264, 3539, 82, 13, 51616], "temperature": 0.0, "avg_logprob": -0.5038715524876372, "compression_ratio": 1.7234042553191489, "no_speech_prob": 3.6119786273047794e-06}, {"id": 184, "seek": 94868, "start": 948.76, "end": 951.64, "text": " To our actuals.", "tokens": [50368, 1407, 527, 3539, 82, 13, 50512], "temperature": 0.0, "avg_logprob": -0.2251928268917023, "compression_ratio": 1.7392996108949417, "no_speech_prob": 0.007460394408553839}, {"id": 185, "seek": 94868, "start": 952.1999999999999, "end": 956.5999999999999, "text": " And yeah, in this very hacky approach, let's say we predict over here", "tokens": [50540, 400, 1338, 11, 294, 341, 588, 10339, 88, 3109, 11, 718, 311, 584, 321, 6069, 670, 510, 50760], "temperature": 0.0, "avg_logprob": -0.2251928268917023, "compression_ratio": 1.7392996108949417, "no_speech_prob": 0.007460394408553839}, {"id": 186, "seek": 94868, "start": 956.5999999999999, "end": 959.64, "text": " the number nine and the actual is the number two.", "tokens": [50760, 264, 1230, 4949, 293, 264, 3539, 307, 264, 1230, 732, 13, 50912], "temperature": 0.0, "avg_logprob": -0.2251928268917023, "compression_ratio": 1.7392996108949417, "no_speech_prob": 0.007460394408553839}, {"id": 187, "seek": 94868, "start": 959.64, "end": 964.04, "text": " And we'll compare those together using MSE.", "tokens": [50912, 400, 321, 603, 6794, 729, 1214, 1228, 376, 5879, 13, 51132], "temperature": 0.0, "avg_logprob": -0.2251928268917023, "compression_ratio": 1.7392996108949417, "no_speech_prob": 0.007460394408553839}, {"id": 188, "seek": 94868, "start": 964.28, "end": 967.9599999999999, "text": " Which will be a stupid way to do it, because it's saying that", "tokens": [51144, 3013, 486, 312, 257, 6631, 636, 281, 360, 309, 11, 570, 309, 311, 1566, 300, 51328], "temperature": 0.0, "avg_logprob": -0.2251928268917023, "compression_ratio": 1.7392996108949417, "no_speech_prob": 0.007460394408553839}, {"id": 189, "seek": 94868, "start": 967.9599999999999, "end": 971.16, "text": " nine is further away from being two than two.", "tokens": [51328, 4949, 307, 3052, 1314, 490, 885, 732, 813, 732, 13, 51488], "temperature": 0.0, "avg_logprob": -0.2251928268917023, "compression_ratio": 1.7392996108949417, "no_speech_prob": 0.007460394408553839}, {"id": 190, "seek": 94868, "start": 971.16, "end": 975.0799999999999, "text": " Nine is further away from two than it is from four, in terms of how correct it is.", "tokens": [51488, 18939, 307, 3052, 1314, 490, 732, 813, 309, 307, 490, 1451, 11, 294, 2115, 295, 577, 3006, 309, 307, 13, 51684], "temperature": 0.0, "avg_logprob": -0.2251928268917023, "compression_ratio": 1.7392996108949417, "no_speech_prob": 0.007460394408553839}, {"id": 191, "seek": 94868, "start": 975.0799999999999, "end": 978.5999999999999, "text": " Which is not what we want at all. But this is what we're going to do just to", "tokens": [51684, 3013, 307, 406, 437, 321, 528, 412, 439, 13, 583, 341, 307, 437, 321, 434, 516, 281, 360, 445, 281, 51860], "temperature": 0.0, "avg_logprob": -0.2251928268917023, "compression_ratio": 1.7392996108949417, "no_speech_prob": 0.007460394408553839}, {"id": 192, "seek": 97860, "start": 978.6, "end": 981.96, "text": " simplify our starting point. So that's why we're going to have a", "tokens": [50364, 20460, 527, 2891, 935, 13, 407, 300, 311, 983, 321, 434, 516, 281, 362, 257, 50532], "temperature": 0.0, "avg_logprob": -0.18684909654700238, "compression_ratio": 1.7222222222222223, "no_speech_prob": 1.3845971807313617e-05}, {"id": 193, "seek": 97860, "start": 981.96, "end": 985.48, "text": " single output for this weight matrix, and a", "tokens": [50532, 2167, 5598, 337, 341, 3364, 8141, 11, 293, 257, 50708], "temperature": 0.0, "avg_logprob": -0.18684909654700238, "compression_ratio": 1.7222222222222223, "no_speech_prob": 1.3845971807313617e-05}, {"id": 194, "seek": 97860, "start": 985.48, "end": 992.44, "text": " single output for this bias. So a linear, let's create a function for", "tokens": [50708, 2167, 5598, 337, 341, 12577, 13, 407, 257, 8213, 11, 718, 311, 1884, 257, 2445, 337, 51056], "temperature": 0.0, "avg_logprob": -0.18684909654700238, "compression_ratio": 1.7222222222222223, "no_speech_prob": 1.3845971807313617e-05}, {"id": 195, "seek": 97860, "start": 992.44, "end": 997.0, "text": " putting X through a linear layer with these weights and these biases.", "tokens": [51056, 3372, 1783, 807, 257, 8213, 4583, 365, 613, 17443, 293, 613, 32152, 13, 51284], "temperature": 0.0, "avg_logprob": -0.18684909654700238, "compression_ratio": 1.7222222222222223, "no_speech_prob": 1.3845971807313617e-05}, {"id": 196, "seek": 97860, "start": 997.0, "end": 1001.16, "text": " So it's a matrix multiply and an add.", "tokens": [51284, 407, 309, 311, 257, 8141, 12972, 293, 364, 909, 13, 51492], "temperature": 0.0, "avg_logprob": -0.18684909654700238, "compression_ratio": 1.7222222222222223, "no_speech_prob": 1.3845971807313617e-05}, {"id": 197, "seek": 97860, "start": 1001.5600000000001, "end": 1006.36, "text": " All right, so we can now try it. So if we multiply our", "tokens": [51512, 1057, 558, 11, 370, 321, 393, 586, 853, 309, 13, 407, 498, 321, 12972, 527, 51752], "temperature": 0.0, "avg_logprob": -0.18684909654700238, "compression_ratio": 1.7222222222222223, "no_speech_prob": 1.3845971807313617e-05}, {"id": 198, "seek": 100636, "start": 1006.36, "end": 1010.92, "text": " X, oh we're doing X valid this time. So just to clarify,", "tokens": [50364, 1783, 11, 1954, 321, 434, 884, 1783, 7363, 341, 565, 13, 407, 445, 281, 17594, 11, 50592], "temperature": 0.0, "avg_logprob": -0.2298178929154591, "compression_ratio": 1.482233502538071, "no_speech_prob": 7.843768980819732e-05}, {"id": 199, "seek": 100636, "start": 1010.92, "end": 1018.36, "text": " X valid is 10,000 by 784. So if we put X valid through our", "tokens": [50592, 1783, 7363, 307, 1266, 11, 1360, 538, 1614, 25494, 13, 407, 498, 321, 829, 1783, 7363, 807, 527, 50964], "temperature": 0.0, "avg_logprob": -0.2298178929154591, "compression_ratio": 1.482233502538071, "no_speech_prob": 7.843768980819732e-05}, {"id": 200, "seek": 100636, "start": 1018.36, "end": 1025.08, "text": " weights and biases with a linear layer, we end up with a 10,000 by 50. So 10,050", "tokens": [50964, 17443, 293, 32152, 365, 257, 8213, 4583, 11, 321, 917, 493, 365, 257, 1266, 11, 1360, 538, 2625, 13, 407, 1266, 11, 15, 2803, 51300], "temperature": 0.0, "avg_logprob": -0.2298178929154591, "compression_ratio": 1.482233502538071, "no_speech_prob": 7.843768980819732e-05}, {"id": 201, "seek": 100636, "start": 1025.08, "end": 1028.84, "text": " long hidden activations.", "tokens": [51300, 938, 7633, 2430, 763, 13, 51488], "temperature": 0.0, "avg_logprob": -0.2298178929154591, "compression_ratio": 1.482233502538071, "no_speech_prob": 7.843768980819732e-05}, {"id": 202, "seek": 100636, "start": 1028.84, "end": 1032.1200000000001, "text": " They're not quite ready yet, because we have to put them through ReLU.", "tokens": [51488, 814, 434, 406, 1596, 1919, 1939, 11, 570, 321, 362, 281, 829, 552, 807, 1300, 43, 52, 13, 51652], "temperature": 0.0, "avg_logprob": -0.2298178929154591, "compression_ratio": 1.482233502538071, "no_speech_prob": 7.843768980819732e-05}, {"id": 203, "seek": 103212, "start": 1032.12, "end": 1038.36, "text": " And so we're going to clamp at zero. So everything under zero will become zero.", "tokens": [50364, 400, 370, 321, 434, 516, 281, 17690, 412, 4018, 13, 407, 1203, 833, 4018, 486, 1813, 4018, 13, 50676], "temperature": 0.0, "avg_logprob": -0.1988263261427573, "compression_ratio": 1.58, "no_speech_prob": 0.00017130737251136452}, {"id": 204, "seek": 103212, "start": 1038.4399999999998, "end": 1041.8, "text": " And so here's what it looks like when we go through the linear layer,", "tokens": [50680, 400, 370, 510, 311, 437, 309, 1542, 411, 562, 321, 352, 807, 264, 8213, 4583, 11, 50848], "temperature": 0.0, "avg_logprob": -0.1988263261427573, "compression_ratio": 1.58, "no_speech_prob": 0.00017130737251136452}, {"id": 205, "seek": 103212, "start": 1041.8, "end": 1045.6399999999999, "text": " and then the ReLU. And you can see here's a tensor with a bunch of things,", "tokens": [50848, 293, 550, 264, 1300, 43, 52, 13, 400, 291, 393, 536, 510, 311, 257, 40863, 365, 257, 3840, 295, 721, 11, 51040], "temperature": 0.0, "avg_logprob": -0.1988263261427573, "compression_ratio": 1.58, "no_speech_prob": 0.00017130737251136452}, {"id": 206, "seek": 103212, "start": 1045.6399999999999, "end": 1049.08, "text": " some of which are zero, or they're positive. And so that's the result of", "tokens": [51040, 512, 295, 597, 366, 4018, 11, 420, 436, 434, 3353, 13, 400, 370, 300, 311, 264, 1874, 295, 51212], "temperature": 0.0, "avg_logprob": -0.1988263261427573, "compression_ratio": 1.58, "no_speech_prob": 0.00017130737251136452}, {"id": 207, "seek": 103212, "start": 1049.08, "end": 1052.12, "text": " this matrix multiplication.", "tokens": [51212, 341, 8141, 27290, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1988263261427573, "compression_ratio": 1.58, "no_speech_prob": 0.00017130737251136452}, {"id": 208, "seek": 103212, "start": 1052.12, "end": 1058.6799999999998, "text": " Okay, so to create our basic MLP multi-layer perceptron from scratch,", "tokens": [51364, 1033, 11, 370, 281, 1884, 527, 3875, 21601, 47, 4825, 12, 8376, 260, 43276, 2044, 490, 8459, 11, 51692], "temperature": 0.0, "avg_logprob": -0.1988263261427573, "compression_ratio": 1.58, "no_speech_prob": 0.00017130737251136452}, {"id": 209, "seek": 105868, "start": 1058.68, "end": 1065.24, "text": " we will take our mini-batch of X's. XB is a X match.", "tokens": [50364, 321, 486, 747, 527, 8382, 12, 65, 852, 295, 1783, 311, 13, 1783, 33, 307, 257, 1783, 2995, 13, 50692], "temperature": 0.0, "avg_logprob": -0.22974638845406326, "compression_ratio": 1.6443298969072164, "no_speech_prob": 2.5466335500823334e-05}, {"id": 210, "seek": 105868, "start": 1065.24, "end": 1068.76, "text": " We will create our first layer's output with a linear, and then we will put that", "tokens": [50692, 492, 486, 1884, 527, 700, 4583, 311, 5598, 365, 257, 8213, 11, 293, 550, 321, 486, 829, 300, 50868], "temperature": 0.0, "avg_logprob": -0.22974638845406326, "compression_ratio": 1.6443298969072164, "no_speech_prob": 2.5466335500823334e-05}, {"id": 211, "seek": 105868, "start": 1068.76, "end": 1071.3200000000002, "text": " through a ReLU. And then that will go through the second", "tokens": [50868, 807, 257, 1300, 43, 52, 13, 400, 550, 300, 486, 352, 807, 264, 1150, 50996], "temperature": 0.0, "avg_logprob": -0.22974638845406326, "compression_ratio": 1.6443298969072164, "no_speech_prob": 2.5466335500823334e-05}, {"id": 212, "seek": 105868, "start": 1071.3200000000002, "end": 1077.24, "text": " linear. So the first one uses the W1B1, okay, these ones. And the second one", "tokens": [50996, 8213, 13, 407, 264, 700, 472, 4960, 264, 343, 16, 33, 16, 11, 1392, 11, 613, 2306, 13, 400, 264, 1150, 472, 51292], "temperature": 0.0, "avg_logprob": -0.22974638845406326, "compression_ratio": 1.6443298969072164, "no_speech_prob": 2.5466335500823334e-05}, {"id": 213, "seek": 105868, "start": 1077.24, "end": 1084.92, "text": " uses the W2B2. And so we've now got a simple model.", "tokens": [51292, 4960, 264, 343, 17, 33, 17, 13, 400, 370, 321, 600, 586, 658, 257, 2199, 2316, 13, 51676], "temperature": 0.0, "avg_logprob": -0.22974638845406326, "compression_ratio": 1.6443298969072164, "no_speech_prob": 2.5466335500823334e-05}, {"id": 214, "seek": 108492, "start": 1085.16, "end": 1090.28, "text": " And as we hoped, when we pass in the validation set, we get back 10,000", "tokens": [50376, 400, 382, 321, 19737, 11, 562, 321, 1320, 294, 264, 24071, 992, 11, 321, 483, 646, 1266, 11, 1360, 50632], "temperature": 0.0, "avg_logprob": -0.1921933130784468, "compression_ratio": 1.455497382198953, "no_speech_prob": 0.0002378195058554411}, {"id": 215, "seek": 108492, "start": 1090.28, "end": 1094.92, "text": " digits. So 10,000 by one. Great. So that's a good start.", "tokens": [50632, 27011, 13, 407, 1266, 11, 1360, 538, 472, 13, 3769, 13, 407, 300, 311, 257, 665, 722, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1921933130784468, "compression_ratio": 1.455497382198953, "no_speech_prob": 0.0002378195058554411}, {"id": 216, "seek": 108492, "start": 1094.92, "end": 1100.6000000000001, "text": " Okay, so let's use our ridiculous loss function of MSC.", "tokens": [50864, 1033, 11, 370, 718, 311, 764, 527, 11083, 4470, 2445, 295, 7395, 34, 13, 51148], "temperature": 0.0, "avg_logprob": -0.1921933130784468, "compression_ratio": 1.455497382198953, "no_speech_prob": 0.0002378195058554411}, {"id": 217, "seek": 108492, "start": 1100.6000000000001, "end": 1106.52, "text": " So our results is 10,000 by one.", "tokens": [51148, 407, 527, 3542, 307, 1266, 11, 1360, 538, 472, 13, 51444], "temperature": 0.0, "avg_logprob": -0.1921933130784468, "compression_ratio": 1.455497382198953, "no_speech_prob": 0.0002378195058554411}, {"id": 218, "seek": 108492, "start": 1106.52, "end": 1113.64, "text": " And our Y valid is just a vector. Now what's going to happen", "tokens": [51444, 400, 527, 398, 7363, 307, 445, 257, 8062, 13, 823, 437, 311, 516, 281, 1051, 51800], "temperature": 0.0, "avg_logprob": -0.1921933130784468, "compression_ratio": 1.455497382198953, "no_speech_prob": 0.0002378195058554411}, {"id": 219, "seek": 111364, "start": 1113.72, "end": 1122.1200000000001, "text": " if I do res minus Y valid? So before you continue in the video,", "tokens": [50368, 498, 286, 360, 725, 3175, 398, 7363, 30, 407, 949, 291, 2354, 294, 264, 960, 11, 50788], "temperature": 0.0, "avg_logprob": -0.20332127809524536, "compression_ratio": 1.4342105263157894, "no_speech_prob": 1.5689523934270255e-05}, {"id": 220, "seek": 111364, "start": 1122.1200000000001, "end": 1125.96, "text": " have a think about that. What's going to happen if I do res minus Y valid by", "tokens": [50788, 362, 257, 519, 466, 300, 13, 708, 311, 516, 281, 1051, 498, 286, 360, 725, 3175, 398, 7363, 538, 50980], "temperature": 0.0, "avg_logprob": -0.20332127809524536, "compression_ratio": 1.4342105263157894, "no_speech_prob": 1.5689523934270255e-05}, {"id": 221, "seek": 111364, "start": 1125.96, "end": 1131.16, "text": " thinking about the NumPy broadcasting rules we've learnt?", "tokens": [50980, 1953, 466, 264, 22592, 47, 88, 30024, 4474, 321, 600, 18991, 30, 51240], "temperature": 0.0, "avg_logprob": -0.20332127809524536, "compression_ratio": 1.4342105263157894, "no_speech_prob": 1.5689523934270255e-05}, {"id": 222, "seek": 111364, "start": 1134.2800000000002, "end": 1137.24, "text": " Okay, let's try it.", "tokens": [51396, 1033, 11, 718, 311, 853, 309, 13, 51544], "temperature": 0.0, "avg_logprob": -0.20332127809524536, "compression_ratio": 1.4342105263157894, "no_speech_prob": 1.5689523934270255e-05}, {"id": 223, "seek": 113724, "start": 1137.4, "end": 1141.32, "text": " Oh, terrible. We've ended up with a 10,000", "tokens": [50372, 876, 11, 6237, 13, 492, 600, 4590, 493, 365, 257, 1266, 11, 1360, 50568], "temperature": 0.0, "avg_logprob": -0.24397271578429175, "compression_ratio": 1.34640522875817, "no_speech_prob": 0.00014883601397741586}, {"id": 224, "seek": 113724, "start": 1141.32, "end": 1148.1200000000001, "text": " by 10,000 matrix. So 100 million points. Now we would expect an MSC to", "tokens": [50568, 538, 1266, 11, 1360, 8141, 13, 407, 2319, 2459, 2793, 13, 823, 321, 576, 2066, 364, 376, 20839, 281, 50908], "temperature": 0.0, "avg_logprob": -0.24397271578429175, "compression_ratio": 1.34640522875817, "no_speech_prob": 0.00014883601397741586}, {"id": 225, "seek": 113724, "start": 1148.1200000000001, "end": 1153.4, "text": " contain a thousand points. Why did that happen?", "tokens": [50908, 5304, 257, 4714, 2793, 13, 1545, 630, 300, 1051, 30, 51172], "temperature": 0.0, "avg_logprob": -0.24397271578429175, "compression_ratio": 1.34640522875817, "no_speech_prob": 0.00014883601397741586}, {"id": 226, "seek": 113724, "start": 1157.64, "end": 1161.64, "text": " The reason it happened is because we have to", "tokens": [51384, 440, 1778, 309, 2011, 307, 570, 321, 362, 281, 51584], "temperature": 0.0, "avg_logprob": -0.24397271578429175, "compression_ratio": 1.34640522875817, "no_speech_prob": 0.00014883601397741586}, {"id": 227, "seek": 116164, "start": 1161.64, "end": 1168.2800000000002, "text": " start out at the last dimension and go right to left.", "tokens": [50364, 722, 484, 412, 264, 1036, 10139, 293, 352, 558, 281, 1411, 13, 50696], "temperature": 0.0, "avg_logprob": -0.1959960436580157, "compression_ratio": 1.696078431372549, "no_speech_prob": 0.0002737169561441988}, {"id": 228, "seek": 116164, "start": 1168.2800000000002, "end": 1174.92, "text": " And we compare the 10,000 to the one and say, are they compatible?", "tokens": [50696, 400, 321, 6794, 264, 1266, 11, 1360, 281, 264, 472, 293, 584, 11, 366, 436, 18218, 30, 51028], "temperature": 0.0, "avg_logprob": -0.1959960436580157, "compression_ratio": 1.696078431372549, "no_speech_prob": 0.0002737169561441988}, {"id": 229, "seek": 116164, "start": 1174.92, "end": 1178.3600000000001, "text": " And the answer is, that's right, Alexei in the chat's got it right, broadcasting", "tokens": [51028, 400, 264, 1867, 307, 11, 300, 311, 558, 11, 5202, 17067, 294, 264, 5081, 311, 658, 309, 558, 11, 30024, 51200], "temperature": 0.0, "avg_logprob": -0.1959960436580157, "compression_ratio": 1.696078431372549, "no_speech_prob": 0.0002737169561441988}, {"id": 230, "seek": 116164, "start": 1178.3600000000001, "end": 1181.5600000000002, "text": " rules. So the answer is that this one will be", "tokens": [51200, 4474, 13, 407, 264, 1867, 307, 300, 341, 472, 486, 312, 51360], "temperature": 0.0, "avg_logprob": -0.1959960436580157, "compression_ratio": 1.696078431372549, "no_speech_prob": 0.0002737169561441988}, {"id": 231, "seek": 116164, "start": 1181.5600000000002, "end": 1185.96, "text": " broadcast over these 10,000. So this pair here will give us 10,000", "tokens": [51360, 9975, 670, 613, 1266, 11, 1360, 13, 407, 341, 6119, 510, 486, 976, 505, 1266, 11, 1360, 51580], "temperature": 0.0, "avg_logprob": -0.1959960436580157, "compression_ratio": 1.696078431372549, "no_speech_prob": 0.0002737169561441988}, {"id": 232, "seek": 116164, "start": 1185.96, "end": 1190.5200000000002, "text": " outputs. And then we'll move to", "tokens": [51580, 23930, 13, 400, 550, 321, 603, 1286, 281, 51808], "temperature": 0.0, "avg_logprob": -0.1959960436580157, "compression_ratio": 1.696078431372549, "no_speech_prob": 0.0002737169561441988}, {"id": 233, "seek": 119052, "start": 1191.0, "end": 1196.2, "text": " the next one. And we'll also move here to the next one.", "tokens": [50388, 264, 958, 472, 13, 400, 321, 603, 611, 1286, 510, 281, 264, 958, 472, 13, 50648], "temperature": 0.0, "avg_logprob": -0.17774191847792617, "compression_ratio": 1.6990740740740742, "no_speech_prob": 0.0001007124301395379}, {"id": 234, "seek": 119052, "start": 1196.2, "end": 1200.04, "text": " Uh-oh, there is no next one. What happens? Now if you remember the rules,", "tokens": [50648, 4019, 12, 1445, 11, 456, 307, 572, 958, 472, 13, 708, 2314, 30, 823, 498, 291, 1604, 264, 4474, 11, 50840], "temperature": 0.0, "avg_logprob": -0.17774191847792617, "compression_ratio": 1.6990740740740742, "no_speech_prob": 0.0001007124301395379}, {"id": 235, "seek": 119052, "start": 1200.04, "end": 1204.44, "text": " it inserts a unit axis for us. So we now have 10,000 by one.", "tokens": [50840, 309, 49163, 257, 4985, 10298, 337, 505, 13, 407, 321, 586, 362, 1266, 11, 1360, 538, 472, 13, 51060], "temperature": 0.0, "avg_logprob": -0.17774191847792617, "compression_ratio": 1.6990740740740742, "no_speech_prob": 0.0001007124301395379}, {"id": 236, "seek": 119052, "start": 1204.44, "end": 1207.48, "text": " So that means each of the 10,000 outputs from here", "tokens": [51060, 407, 300, 1355, 1184, 295, 264, 1266, 11, 1360, 23930, 490, 510, 51212], "temperature": 0.0, "avg_logprob": -0.17774191847792617, "compression_ratio": 1.6990740740740742, "no_speech_prob": 0.0001007124301395379}, {"id": 237, "seek": 119052, "start": 1207.48, "end": 1212.68, "text": " will end up being broadcast across the 10,000", "tokens": [51212, 486, 917, 493, 885, 9975, 2108, 264, 1266, 11, 1360, 51472], "temperature": 0.0, "avg_logprob": -0.17774191847792617, "compression_ratio": 1.6990740740740742, "no_speech_prob": 0.0001007124301395379}, {"id": 238, "seek": 119052, "start": 1212.68, "end": 1216.6, "text": " rows here. So that means that we'll end up for each of those 10,000, we'll have", "tokens": [51472, 13241, 510, 13, 407, 300, 1355, 300, 321, 603, 917, 493, 337, 1184, 295, 729, 1266, 11, 1360, 11, 321, 603, 362, 51668], "temperature": 0.0, "avg_logprob": -0.17774191847792617, "compression_ratio": 1.6990740740740742, "no_speech_prob": 0.0001007124301395379}, {"id": 239, "seek": 121660, "start": 1216.6, "end": 1219.6399999999999, "text": " another 10,000. So we'll end up with a 10,000", "tokens": [50364, 1071, 1266, 11, 1360, 13, 407, 321, 603, 917, 493, 365, 257, 1266, 11, 1360, 50516], "temperature": 0.0, "avg_logprob": -0.2027053654750931, "compression_ratio": 1.6813725490196079, "no_speech_prob": 0.0015011619543656707}, {"id": 240, "seek": 121660, "start": 1219.6399999999999, "end": 1227.1599999999999, "text": " by 10,000 output. So that's not what we want. So how could we fix that?", "tokens": [50516, 538, 1266, 11, 1360, 5598, 13, 407, 300, 311, 406, 437, 321, 528, 13, 407, 577, 727, 321, 3191, 300, 30, 50892], "temperature": 0.0, "avg_logprob": -0.2027053654750931, "compression_ratio": 1.6813725490196079, "no_speech_prob": 0.0015011619543656707}, {"id": 241, "seek": 121660, "start": 1227.1599999999999, "end": 1231.48, "text": " Well what we really would want, would we want this to be", "tokens": [50892, 1042, 437, 321, 534, 576, 528, 11, 576, 321, 528, 341, 281, 312, 51108], "temperature": 0.0, "avg_logprob": -0.2027053654750931, "compression_ratio": 1.6813725490196079, "no_speech_prob": 0.0015011619543656707}, {"id": 242, "seek": 121660, "start": 1232.04, "end": 1236.84, "text": " 10,000 comma one here. If that was 10,000 comma one,", "tokens": [51136, 1266, 11, 1360, 22117, 472, 510, 13, 759, 300, 390, 1266, 11, 1360, 22117, 472, 11, 51376], "temperature": 0.0, "avg_logprob": -0.2027053654750931, "compression_ratio": 1.6813725490196079, "no_speech_prob": 0.0015011619543656707}, {"id": 243, "seek": 121660, "start": 1236.84, "end": 1239.9599999999998, "text": " then we'd compare these two, right to left,", "tokens": [51376, 550, 321, 1116, 6794, 613, 732, 11, 558, 281, 1411, 11, 51532], "temperature": 0.0, "avg_logprob": -0.2027053654750931, "compression_ratio": 1.6813725490196079, "no_speech_prob": 0.0015011619543656707}, {"id": 244, "seek": 121660, "start": 1239.9599999999998, "end": 1244.6799999999998, "text": " and they're both one. So those match. And there's nothing to broadcast,", "tokens": [51532, 293, 436, 434, 1293, 472, 13, 407, 729, 2995, 13, 400, 456, 311, 1825, 281, 9975, 11, 51768], "temperature": 0.0, "avg_logprob": -0.2027053654750931, "compression_ratio": 1.6813725490196079, "no_speech_prob": 0.0015011619543656707}, {"id": 245, "seek": 124468, "start": 1244.68, "end": 1248.2, "text": " because they're the same. And then we'll go to the next one, 10,000 to 10,000.", "tokens": [50364, 570, 436, 434, 264, 912, 13, 400, 550, 321, 603, 352, 281, 264, 958, 472, 11, 1266, 11, 1360, 281, 1266, 11, 1360, 13, 50540], "temperature": 0.0, "avg_logprob": -0.20898256892651584, "compression_ratio": 1.658008658008658, "no_speech_prob": 0.00014202325837686658}, {"id": 246, "seek": 124468, "start": 1248.2, "end": 1251.48, "text": " Those match. So they just go element-wise for those.", "tokens": [50540, 3950, 2995, 13, 407, 436, 445, 352, 4478, 12, 3711, 337, 729, 13, 50704], "temperature": 0.0, "avg_logprob": -0.20898256892651584, "compression_ratio": 1.658008658008658, "no_speech_prob": 0.00014202325837686658}, {"id": 247, "seek": 124468, "start": 1251.48, "end": 1255.3200000000002, "text": " And we'd end up with exactly what we want. We'd end up with 10,000 results.", "tokens": [50704, 400, 321, 1116, 917, 493, 365, 2293, 437, 321, 528, 13, 492, 1116, 917, 493, 365, 1266, 11, 1360, 3542, 13, 50896], "temperature": 0.0, "avg_logprob": -0.20898256892651584, "compression_ratio": 1.658008658008658, "no_speech_prob": 0.00014202325837686658}, {"id": 248, "seek": 124468, "start": 1255.3200000000002, "end": 1260.04, "text": " Or, alternatively, we could remove this dimension.", "tokens": [50896, 1610, 11, 8535, 356, 11, 321, 727, 4159, 341, 10139, 13, 51132], "temperature": 0.0, "avg_logprob": -0.20898256892651584, "compression_ratio": 1.658008658008658, "no_speech_prob": 0.00014202325837686658}, {"id": 249, "seek": 124468, "start": 1260.04, "end": 1264.52, "text": " And then again, same thing. We're then going to add right to left,", "tokens": [51132, 400, 550, 797, 11, 912, 551, 13, 492, 434, 550, 516, 281, 909, 558, 281, 1411, 11, 51356], "temperature": 0.0, "avg_logprob": -0.20898256892651584, "compression_ratio": 1.658008658008658, "no_speech_prob": 0.00014202325837686658}, {"id": 250, "seek": 124468, "start": 1264.52, "end": 1273.16, "text": " compatible 10,000. So they'll get element-wise operation.", "tokens": [51356, 18218, 1266, 11, 1360, 13, 407, 436, 603, 483, 4478, 12, 3711, 6916, 13, 51788], "temperature": 0.0, "avg_logprob": -0.20898256892651584, "compression_ratio": 1.658008658008658, "no_speech_prob": 0.00014202325837686658}, {"id": 251, "seek": 127468, "start": 1275.64, "end": 1280.28, "text": " So in this case, I got rid of the trailing comma one.", "tokens": [50412, 407, 294, 341, 1389, 11, 286, 658, 3973, 295, 264, 944, 4883, 22117, 472, 13, 50644], "temperature": 0.0, "avg_logprob": -0.22085983348342608, "compression_ratio": 1.548936170212766, "no_speech_prob": 2.4061005206021946e-06}, {"id": 252, "seek": 127468, "start": 1280.28, "end": 1283.24, "text": " There's a couple of ways you could do that. One is just to say, okay, grab every", "tokens": [50644, 821, 311, 257, 1916, 295, 2098, 291, 727, 360, 300, 13, 1485, 307, 445, 281, 584, 11, 1392, 11, 4444, 633, 50792], "temperature": 0.0, "avg_logprob": -0.22085983348342608, "compression_ratio": 1.548936170212766, "no_speech_prob": 2.4061005206021946e-06}, {"id": 253, "seek": 127468, "start": 1283.24, "end": 1287.48, "text": " row and the zeroth column of res.", "tokens": [50792, 5386, 293, 264, 44746, 900, 7738, 295, 725, 13, 51004], "temperature": 0.0, "avg_logprob": -0.22085983348342608, "compression_ratio": 1.548936170212766, "no_speech_prob": 2.4061005206021946e-06}, {"id": 254, "seek": 127468, "start": 1287.48, "end": 1292.44, "text": " And that's going to turn it from a 10,000 by one into a 10,000.", "tokens": [51004, 400, 300, 311, 516, 281, 1261, 309, 490, 257, 1266, 11, 1360, 538, 472, 666, 257, 1266, 11, 1360, 13, 51252], "temperature": 0.0, "avg_logprob": -0.22085983348342608, "compression_ratio": 1.548936170212766, "no_speech_prob": 2.4061005206021946e-06}, {"id": 255, "seek": 127468, "start": 1292.76, "end": 1296.68, "text": " Or alternatively, we can say dot squeeze. Now dot squeeze removes", "tokens": [51268, 1610, 8535, 356, 11, 321, 393, 584, 5893, 13578, 13, 823, 5893, 13578, 30445, 51464], "temperature": 0.0, "avg_logprob": -0.22085983348342608, "compression_ratio": 1.548936170212766, "no_speech_prob": 2.4061005206021946e-06}, {"id": 256, "seek": 127468, "start": 1296.68, "end": 1303.48, "text": " all trailing unit vectors, and possibly also prefix unit vectors.", "tokens": [51464, 439, 944, 4883, 4985, 18875, 11, 293, 6264, 611, 46969, 4985, 18875, 13, 51804], "temperature": 0.0, "avg_logprob": -0.22085983348342608, "compression_ratio": 1.548936170212766, "no_speech_prob": 2.4061005206021946e-06}, {"id": 257, "seek": 130348, "start": 1303.48, "end": 1308.52, "text": " I can't quite recall. I guess we should try.", "tokens": [50364, 286, 393, 380, 1596, 9901, 13, 286, 2041, 321, 820, 853, 13, 50616], "temperature": 0.0, "avg_logprob": -0.2648962338765462, "compression_ratio": 1.127906976744186, "no_speech_prob": 0.00046552091953344643}, {"id": 258, "seek": 130348, "start": 1308.52, "end": 1312.3600000000001, "text": " So let's say", "tokens": [50616, 407, 718, 311, 584, 50808], "temperature": 0.0, "avg_logprob": -0.2648962338765462, "compression_ratio": 1.127906976744186, "no_speech_prob": 0.00046552091953344643}, {"id": 259, "seek": 130348, "start": 1314.84, "end": 1323.72, "text": " res none comma colon comma none", "tokens": [50932, 725, 6022, 22117, 8255, 22117, 6022, 51376], "temperature": 0.0, "avg_logprob": -0.2648962338765462, "compression_ratio": 1.127906976744186, "no_speech_prob": 0.00046552091953344643}, {"id": 260, "seek": 130348, "start": 1323.72, "end": 1326.68, "text": " q.shape", "tokens": [51376, 9505, 13, 82, 42406, 51524], "temperature": 0.0, "avg_logprob": -0.2648962338765462, "compression_ratio": 1.127906976744186, "no_speech_prob": 0.00046552091953344643}, {"id": 261, "seek": 132668, "start": 1327.4, "end": 1334.8400000000001, "text": " Okay, so if I go q.squeeze", "tokens": [50400, 1033, 11, 370, 498, 286, 352, 9505, 13, 44516, 10670, 50772], "temperature": 0.0, "avg_logprob": -0.19960521584126487, "compression_ratio": 1.5319148936170213, "no_speech_prob": 0.00041731176315806806}, {"id": 262, "seek": 132668, "start": 1336.68, "end": 1343.5600000000002, "text": " dot shape. Okay, so all the unit vectors get removed.", "tokens": [50864, 5893, 3909, 13, 1033, 11, 370, 439, 264, 4985, 18875, 483, 7261, 13, 51208], "temperature": 0.0, "avg_logprob": -0.19960521584126487, "compression_ratio": 1.5319148936170213, "no_speech_prob": 0.00041731176315806806}, {"id": 263, "seek": 132668, "start": 1344.76, "end": 1348.8400000000001, "text": " Sorry, all the unit dimensions get removed, I should say.", "tokens": [51268, 4919, 11, 439, 264, 4985, 12819, 483, 7261, 11, 286, 820, 584, 13, 51472], "temperature": 0.0, "avg_logprob": -0.19960521584126487, "compression_ratio": 1.5319148936170213, "no_speech_prob": 0.00041731176315806806}, {"id": 264, "seek": 132668, "start": 1348.8400000000001, "end": 1354.92, "text": " Okay, so now that we've got a way to remove that axis that we didn't want, we", "tokens": [51472, 1033, 11, 370, 586, 300, 321, 600, 658, 257, 636, 281, 4159, 300, 10298, 300, 321, 994, 380, 528, 11, 321, 51776], "temperature": 0.0, "avg_logprob": -0.19960521584126487, "compression_ratio": 1.5319148936170213, "no_speech_prob": 0.00041731176315806806}, {"id": 265, "seek": 135492, "start": 1354.92, "end": 1357.48, "text": " can use it. And if we do the subtraction, now we get", "tokens": [50364, 393, 764, 309, 13, 400, 498, 321, 360, 264, 16390, 313, 11, 586, 321, 483, 50492], "temperature": 0.0, "avg_logprob": -0.21593812499383483, "compression_ratio": 1.5727272727272728, "no_speech_prob": 0.00026118746609427035}, {"id": 266, "seek": 135492, "start": 1357.48, "end": 1364.1200000000001, "text": " 10,000, just like we wanted. So now let's get our", "tokens": [50492, 1266, 11, 1360, 11, 445, 411, 321, 1415, 13, 407, 586, 718, 311, 483, 527, 50824], "temperature": 0.0, "avg_logprob": -0.21593812499383483, "compression_ratio": 1.5727272727272728, "no_speech_prob": 0.00026118746609427035}, {"id": 267, "seek": 135492, "start": 1364.1200000000001, "end": 1369.24, "text": " training and validation-wise. We'll turn them into floats because we're using MSE.", "tokens": [50824, 3097, 293, 24071, 12, 3711, 13, 492, 603, 1261, 552, 666, 37878, 570, 321, 434, 1228, 376, 5879, 13, 51080], "temperature": 0.0, "avg_logprob": -0.21593812499383483, "compression_ratio": 1.5727272727272728, "no_speech_prob": 0.00026118746609427035}, {"id": 268, "seek": 135492, "start": 1369.24, "end": 1375.88, "text": " So let's calculate our predictions for the training set, which is 50,000 by one.", "tokens": [51080, 407, 718, 311, 8873, 527, 21264, 337, 264, 3097, 992, 11, 597, 307, 2625, 11, 1360, 538, 472, 13, 51412], "temperature": 0.0, "avg_logprob": -0.21593812499383483, "compression_ratio": 1.5727272727272728, "no_speech_prob": 0.00026118746609427035}, {"id": 269, "seek": 135492, "start": 1375.88, "end": 1380.04, "text": " And so if we create an MSE function that just does what we just said we wanted.", "tokens": [51412, 400, 370, 498, 321, 1884, 364, 7395, 36, 2445, 300, 445, 775, 437, 321, 445, 848, 321, 1415, 13, 51620], "temperature": 0.0, "avg_logprob": -0.21593812499383483, "compression_ratio": 1.5727272727272728, "no_speech_prob": 0.00026118746609427035}, {"id": 270, "seek": 138004, "start": 1380.04, "end": 1386.04, "text": " So it does the subtraction and then squares it and then takes the mean. That's MSE.", "tokens": [50364, 407, 309, 775, 264, 16390, 313, 293, 550, 19368, 309, 293, 550, 2516, 264, 914, 13, 663, 311, 376, 5879, 13, 50664], "temperature": 0.0, "avg_logprob": -0.26280870745258944, "compression_ratio": 1.4223602484472049, "no_speech_prob": 4.985934720025398e-05}, {"id": 271, "seek": 138004, "start": 1387.96, "end": 1391.24, "text": " So there we go. We now have a loss function being applied to our training set.", "tokens": [50760, 407, 456, 321, 352, 13, 492, 586, 362, 257, 4470, 2445, 885, 6456, 281, 527, 3097, 992, 13, 50924], "temperature": 0.0, "avg_logprob": -0.26280870745258944, "compression_ratio": 1.4223602484472049, "no_speech_prob": 4.985934720025398e-05}, {"id": 272, "seek": 138004, "start": 1395.1599999999999, "end": 1404.36, "text": " Okay. Now we need gradients. So as we briefly discussed last time,", "tokens": [51120, 1033, 13, 823, 321, 643, 2771, 2448, 13, 407, 382, 321, 10515, 7152, 1036, 565, 11, 51580], "temperature": 0.0, "avg_logprob": -0.26280870745258944, "compression_ratio": 1.4223602484472049, "no_speech_prob": 4.985934720025398e-05}, {"id": 273, "seek": 141004, "start": 1410.52, "end": 1417.24, "text": " gradients are slopes. And in fact, maybe it would even be easier to look at last time.", "tokens": [50388, 2771, 2448, 366, 37725, 13, 400, 294, 1186, 11, 1310, 309, 576, 754, 312, 3571, 281, 574, 412, 1036, 565, 13, 50724], "temperature": 0.0, "avg_logprob": -0.2260696291923523, "compression_ratio": 1.3658536585365855, "no_speech_prob": 0.00011412194726290181}, {"id": 274, "seek": 141004, "start": 1421.3999999999999, "end": 1426.2, "text": " So this was last time's notebook. And so we saw how", "tokens": [50932, 407, 341, 390, 1036, 565, 311, 21060, 13, 400, 370, 321, 1866, 577, 51172], "temperature": 0.0, "avg_logprob": -0.2260696291923523, "compression_ratio": 1.3658536585365855, "no_speech_prob": 0.00011412194726290181}, {"id": 275, "seek": 141004, "start": 1429.96, "end": 1433.48, "text": " the gradient at this point is", "tokens": [51360, 264, 16235, 412, 341, 935, 307, 51536], "temperature": 0.0, "avg_logprob": -0.2260696291923523, "compression_ratio": 1.3658536585365855, "no_speech_prob": 0.00011412194726290181}, {"id": 276, "seek": 143348, "start": 1434.44, "end": 1443.08, "text": " the slope here. And so it's the, as we discussed, rise over run.", "tokens": [50412, 264, 13525, 510, 13, 400, 370, 309, 311, 264, 11, 382, 321, 7152, 11, 6272, 670, 1190, 13, 50844], "temperature": 0.0, "avg_logprob": -0.49284303188323975, "compression_ratio": 1.3728813559322033, "no_speech_prob": 1.0129963811777998e-05}, {"id": 277, "seek": 143348, "start": 1445.16, "end": 1458.68, "text": " Now, so that means as we increase, in this case, time by one, the distance increases by how much?", "tokens": [50948, 823, 11, 370, 300, 1355, 382, 321, 3488, 11, 294, 341, 1389, 11, 565, 538, 472, 11, 264, 4560, 8637, 538, 577, 709, 30, 51624], "temperature": 0.0, "avg_logprob": -0.49284303188323975, "compression_ratio": 1.3728813559322033, "no_speech_prob": 1.0129963811777998e-05}, {"id": 278, "seek": 145868, "start": 1459.4, "end": 1464.52, "text": " Distance increases by how much? That's what the slope is.", "tokens": [50400, 9840, 719, 8637, 538, 577, 709, 30, 663, 311, 437, 264, 13525, 307, 13, 50656], "temperature": 0.0, "avg_logprob": -0.23446214766729445, "compression_ratio": 1.3361344537815125, "no_speech_prob": 1.922315095725935e-05}, {"id": 279, "seek": 145868, "start": 1469.3200000000002, "end": 1470.52, "text": " So why is this interesting?", "tokens": [50896, 407, 983, 307, 341, 1880, 30, 50956], "temperature": 0.0, "avg_logprob": -0.23446214766729445, "compression_ratio": 1.3361344537815125, "no_speech_prob": 1.922315095725935e-05}, {"id": 280, "seek": 145868, "start": 1475.16, "end": 1482.92, "text": " The reason it's interesting is because let's consider our neural network.", "tokens": [51188, 440, 1778, 309, 311, 1880, 307, 570, 718, 311, 1949, 527, 18161, 3209, 13, 51576], "temperature": 0.0, "avg_logprob": -0.23446214766729445, "compression_ratio": 1.3361344537815125, "no_speech_prob": 1.922315095725935e-05}, {"id": 281, "seek": 148292, "start": 1482.92, "end": 1488.44, "text": " Our neural network is some function that takes two things, two groups of things.", "tokens": [50364, 2621, 18161, 3209, 307, 512, 2445, 300, 2516, 732, 721, 11, 732, 3935, 295, 721, 13, 50640], "temperature": 0.0, "avg_logprob": -0.20647300657678824, "compression_ratio": 1.51875, "no_speech_prob": 4.9086531362263486e-05}, {"id": 282, "seek": 148292, "start": 1489.3200000000002, "end": 1505.3200000000002, "text": " It contains a matrix of our inputs. And it contains our weight matrix. And we want to", "tokens": [50684, 467, 8306, 257, 8141, 295, 527, 15743, 13, 400, 309, 8306, 527, 3364, 8141, 13, 400, 321, 528, 281, 51484], "temperature": 0.0, "avg_logprob": -0.20647300657678824, "compression_ratio": 1.51875, "no_speech_prob": 4.9086531362263486e-05}, {"id": 283, "seek": 148292, "start": 1508.28, "end": 1511.64, "text": " and let's assume we're also putting it through a loss function. So let's say", "tokens": [51632, 293, 718, 311, 6552, 321, 434, 611, 3372, 309, 807, 257, 4470, 2445, 13, 407, 718, 311, 584, 51800], "temperature": 0.0, "avg_logprob": -0.20647300657678824, "compression_ratio": 1.51875, "no_speech_prob": 4.9086531362263486e-05}, {"id": 284, "seek": 151164, "start": 1512.6000000000001, "end": 1515.5600000000002, "text": " well, I mean, I guess we can be explicit about that. So we could say,", "tokens": [50412, 731, 11, 286, 914, 11, 286, 2041, 321, 393, 312, 13691, 466, 300, 13, 407, 321, 727, 584, 11, 50560], "temperature": 0.0, "avg_logprob": -0.2164549020620493, "compression_ratio": 1.5029940119760479, "no_speech_prob": 7.84376825322397e-05}, {"id": 285, "seek": 151164, "start": 1516.2, "end": 1521.0, "text": " we then take the result of that and we put it through some loss function. So these are the", "tokens": [50592, 321, 550, 747, 264, 1874, 295, 300, 293, 321, 829, 309, 807, 512, 4470, 2445, 13, 407, 613, 366, 264, 50832], "temperature": 0.0, "avg_logprob": -0.2164549020620493, "compression_ratio": 1.5029940119760479, "no_speech_prob": 7.84376825322397e-05}, {"id": 286, "seek": 151164, "start": 1521.0, "end": 1538.0400000000002, "text": " predictions. And we compare it to our actual dependent variable. So that's our neural net.", "tokens": [50832, 21264, 13, 400, 321, 6794, 309, 281, 527, 3539, 12334, 7006, 13, 407, 300, 311, 527, 18161, 2533, 13, 51684], "temperature": 0.0, "avg_logprob": -0.2164549020620493, "compression_ratio": 1.5029940119760479, "no_speech_prob": 7.84376825322397e-05}, {"id": 287, "seek": 154164, "start": 1542.3600000000001, "end": 1546.0400000000002, "text": " And that's our loss function.", "tokens": [50400, 400, 300, 311, 527, 4470, 2445, 13, 50584], "temperature": 0.0, "avg_logprob": -0.21644539528704704, "compression_ratio": 1.314516129032258, "no_speech_prob": 2.1782570911454968e-05}, {"id": 288, "seek": 154164, "start": 1548.3600000000001, "end": 1560.0400000000002, "text": " Okay. So if we can get the derivative of the loss", "tokens": [50700, 1033, 13, 407, 498, 321, 393, 483, 264, 13760, 295, 264, 4470, 51284], "temperature": 0.0, "avg_logprob": -0.21644539528704704, "compression_ratio": 1.314516129032258, "no_speech_prob": 2.1782570911454968e-05}, {"id": 289, "seek": 154164, "start": 1564.2, "end": 1569.24, "text": " with respect to, let's say, one particular weight. So let's say weight number zero.", "tokens": [51492, 365, 3104, 281, 11, 718, 311, 584, 11, 472, 1729, 3364, 13, 407, 718, 311, 584, 3364, 1230, 4018, 13, 51744], "temperature": 0.0, "avg_logprob": -0.21644539528704704, "compression_ratio": 1.314516129032258, "no_speech_prob": 2.1782570911454968e-05}, {"id": 290, "seek": 156924, "start": 1570.2, "end": 1577.4, "text": " And what is that doing? Well, it's saying as I increase the weight by a little bit,", "tokens": [50412, 400, 437, 307, 300, 884, 30, 1042, 11, 309, 311, 1566, 382, 286, 3488, 264, 3364, 538, 257, 707, 857, 11, 50772], "temperature": 0.0, "avg_logprob": -0.1863660124159351, "compression_ratio": 1.8978494623655915, "no_speech_prob": 0.0006361842388287187}, {"id": 291, "seek": 156924, "start": 1578.76, "end": 1584.28, "text": " what happens to the loss? And if it says, oh, well, that would make the loss go down,", "tokens": [50840, 437, 2314, 281, 264, 4470, 30, 400, 498, 309, 1619, 11, 1954, 11, 731, 11, 300, 576, 652, 264, 4470, 352, 760, 11, 51116], "temperature": 0.0, "avg_logprob": -0.1863660124159351, "compression_ratio": 1.8978494623655915, "no_speech_prob": 0.0006361842388287187}, {"id": 292, "seek": 156924, "start": 1584.28, "end": 1590.2, "text": " then obviously I want to increase the weight by a little bit. And if it says, oh, it makes the", "tokens": [51116, 550, 2745, 286, 528, 281, 3488, 264, 3364, 538, 257, 707, 857, 13, 400, 498, 309, 1619, 11, 1954, 11, 309, 1669, 264, 51412], "temperature": 0.0, "avg_logprob": -0.1863660124159351, "compression_ratio": 1.8978494623655915, "no_speech_prob": 0.0006361842388287187}, {"id": 293, "seek": 156924, "start": 1590.2, "end": 1597.32, "text": " loss go up, then obviously I want to do the opposite. So the derivative of the loss with", "tokens": [51412, 4470, 352, 493, 11, 550, 2745, 286, 528, 281, 360, 264, 6182, 13, 407, 264, 13760, 295, 264, 4470, 365, 51768], "temperature": 0.0, "avg_logprob": -0.1863660124159351, "compression_ratio": 1.8978494623655915, "no_speech_prob": 0.0006361842388287187}, {"id": 294, "seek": 159732, "start": 1597.32, "end": 1605.24, "text": " respect to the weights, each one of those tells us how to change the weights. And so to remind you,", "tokens": [50364, 3104, 281, 264, 17443, 11, 1184, 472, 295, 729, 5112, 505, 577, 281, 1319, 264, 17443, 13, 400, 370, 281, 4160, 291, 11, 50760], "temperature": 0.0, "avg_logprob": -0.19247494882612087, "compression_ratio": 1.5843373493975903, "no_speech_prob": 8.220158633776009e-05}, {"id": 295, "seek": 159732, "start": 1605.8, "end": 1613.24, "text": " we then change each weight by that derivative times a little bit and subtract it from the", "tokens": [50788, 321, 550, 1319, 1184, 3364, 538, 300, 13760, 1413, 257, 707, 857, 293, 16390, 309, 490, 264, 51160], "temperature": 0.0, "avg_logprob": -0.19247494882612087, "compression_ratio": 1.5843373493975903, "no_speech_prob": 8.220158633776009e-05}, {"id": 296, "seek": 159732, "start": 1614.28, "end": 1618.6799999999998, "text": " original weights. And we do that a bunch of times. And that's called SGD.", "tokens": [51212, 3380, 17443, 13, 400, 321, 360, 300, 257, 3840, 295, 1413, 13, 400, 300, 311, 1219, 34520, 35, 13, 51432], "temperature": 0.0, "avg_logprob": -0.19247494882612087, "compression_ratio": 1.5843373493975903, "no_speech_prob": 8.220158633776009e-05}, {"id": 297, "seek": 161868, "start": 1618.68, "end": 1631.48, "text": " Now, there's something interesting going on here, which is that in this case,", "tokens": [50364, 823, 11, 456, 311, 746, 1880, 516, 322, 510, 11, 597, 307, 300, 294, 341, 1389, 11, 51004], "temperature": 0.0, "avg_logprob": -0.2143822570345295, "compression_ratio": 1.5963855421686748, "no_speech_prob": 0.00020988131291233003}, {"id": 298, "seek": 161868, "start": 1632.04, "end": 1640.04, "text": " there's a single input and a single output. And so the derivative is a single number at any point.", "tokens": [51032, 456, 311, 257, 2167, 4846, 293, 257, 2167, 5598, 13, 400, 370, 264, 13760, 307, 257, 2167, 1230, 412, 604, 935, 13, 51432], "temperature": 0.0, "avg_logprob": -0.2143822570345295, "compression_ratio": 1.5963855421686748, "no_speech_prob": 0.00020988131291233003}, {"id": 299, "seek": 161868, "start": 1640.04, "end": 1647.64, "text": " It's the speed. In this case, the vehicle's going. But consider a more complex function.", "tokens": [51432, 467, 311, 264, 3073, 13, 682, 341, 1389, 11, 264, 5864, 311, 516, 13, 583, 1949, 257, 544, 3997, 2445, 13, 51812], "temperature": 0.0, "avg_logprob": -0.2143822570345295, "compression_ratio": 1.5963855421686748, "no_speech_prob": 0.00020988131291233003}, {"id": 300, "seek": 164868, "start": 1648.68, "end": 1660.76, "text": " Like, say, this one. Now, in this case, there's one output, but there's two inputs. And so if we", "tokens": [50364, 1743, 11, 584, 11, 341, 472, 13, 823, 11, 294, 341, 1389, 11, 456, 311, 472, 5598, 11, 457, 456, 311, 732, 15743, 13, 400, 370, 498, 321, 50968], "temperature": 0.0, "avg_logprob": -0.20213857109164013, "compression_ratio": 1.69364161849711, "no_speech_prob": 7.484593515982851e-05}, {"id": 301, "seek": 164868, "start": 1660.76, "end": 1667.0800000000002, "text": " want to take the derivative of this function, then we actually need to say, well, what happens if we", "tokens": [50968, 528, 281, 747, 264, 13760, 295, 341, 2445, 11, 550, 321, 767, 643, 281, 584, 11, 731, 11, 437, 2314, 498, 321, 51284], "temperature": 0.0, "avg_logprob": -0.20213857109164013, "compression_ratio": 1.69364161849711, "no_speech_prob": 7.484593515982851e-05}, {"id": 302, "seek": 164868, "start": 1667.0800000000002, "end": 1673.48, "text": " increase X by a little bit? And also what happens if we increase Y by a little bit? And in each", "tokens": [51284, 3488, 1783, 538, 257, 707, 857, 30, 400, 611, 437, 2314, 498, 321, 3488, 398, 538, 257, 707, 857, 30, 400, 294, 1184, 51604], "temperature": 0.0, "avg_logprob": -0.20213857109164013, "compression_ratio": 1.69364161849711, "no_speech_prob": 7.484593515982851e-05}, {"id": 303, "seek": 167348, "start": 1673.48, "end": 1679.8, "text": " case, what happens to Z? And so in that case, the derivative is actually going to contain", "tokens": [50364, 1389, 11, 437, 2314, 281, 1176, 30, 400, 370, 294, 300, 1389, 11, 264, 13760, 307, 767, 516, 281, 5304, 50680], "temperature": 0.0, "avg_logprob": -0.1639969015634188, "compression_ratio": 1.865, "no_speech_prob": 0.00017400534125044942}, {"id": 304, "seek": 167348, "start": 1680.44, "end": 1688.84, "text": " two numbers, right? It's going to contain the derivative of Z with respect to Y. And it's going", "tokens": [50712, 732, 3547, 11, 558, 30, 467, 311, 516, 281, 5304, 264, 13760, 295, 1176, 365, 3104, 281, 398, 13, 400, 309, 311, 516, 51132], "temperature": 0.0, "avg_logprob": -0.1639969015634188, "compression_ratio": 1.865, "no_speech_prob": 0.00017400534125044942}, {"id": 305, "seek": 167348, "start": 1688.84, "end": 1694.1200000000001, "text": " to contain the derivative of Z with respect to X. What happens if we change each of these two", "tokens": [51132, 281, 5304, 264, 13760, 295, 1176, 365, 3104, 281, 1783, 13, 708, 2314, 498, 321, 1319, 1184, 295, 613, 732, 51396], "temperature": 0.0, "avg_logprob": -0.1639969015634188, "compression_ratio": 1.865, "no_speech_prob": 0.00017400534125044942}, {"id": 306, "seek": 167348, "start": 1694.1200000000001, "end": 1698.92, "text": " numbers? So for example, these could be, as we discussed, two different weights in our neural", "tokens": [51396, 3547, 30, 407, 337, 1365, 11, 613, 727, 312, 11, 382, 321, 7152, 11, 732, 819, 17443, 294, 527, 18161, 51636], "temperature": 0.0, "avg_logprob": -0.1639969015634188, "compression_ratio": 1.865, "no_speech_prob": 0.00017400534125044942}, {"id": 307, "seek": 169892, "start": 1698.92, "end": 1709.4, "text": " network. And Z could be our loss, for example. Now, we've got actually 784 inputs, right? So we", "tokens": [50364, 3209, 13, 400, 1176, 727, 312, 527, 4470, 11, 337, 1365, 13, 823, 11, 321, 600, 658, 767, 1614, 25494, 15743, 11, 558, 30, 407, 321, 50888], "temperature": 0.0, "avg_logprob": -0.18323483784993488, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.0001355207059532404}, {"id": 308, "seek": 169892, "start": 1709.4, "end": 1716.6000000000001, "text": " would actually have 784 of these. So we don't normally write them all like that. We would just", "tokens": [50888, 576, 767, 362, 1614, 25494, 295, 613, 13, 407, 321, 500, 380, 5646, 2464, 552, 439, 411, 300, 13, 492, 576, 445, 51248], "temperature": 0.0, "avg_logprob": -0.18323483784993488, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.0001355207059532404}, {"id": 309, "seek": 169892, "start": 1716.6000000000001, "end": 1723.8000000000002, "text": " say, use this little squiggly symbol to say the derivative of the loss across all of them", "tokens": [51248, 584, 11, 764, 341, 707, 2339, 46737, 5986, 281, 584, 264, 13760, 295, 264, 4470, 2108, 439, 295, 552, 51608], "temperature": 0.0, "avg_logprob": -0.18323483784993488, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.0001355207059532404}, {"id": 310, "seek": 172380, "start": 1724.28, "end": 1730.2, "text": " with respect to all of the weights. Okay. And that's just saying that there's a whole bunch", "tokens": [50388, 365, 3104, 281, 439, 295, 264, 17443, 13, 1033, 13, 400, 300, 311, 445, 1566, 300, 456, 311, 257, 1379, 3840, 50684], "temperature": 0.0, "avg_logprob": -0.2825143713700144, "compression_ratio": 1.469387755102041, "no_speech_prob": 4.9086531362263486e-05}, {"id": 311, "seek": 172380, "start": 1730.2, "end": 1741.72, "text": " of them. It's a shorthand way of writing this. Okay. So it gets more complicated still, though.", "tokens": [50684, 295, 552, 13, 467, 311, 257, 402, 2652, 474, 636, 295, 3579, 341, 13, 1033, 13, 407, 309, 2170, 544, 6179, 920, 11, 1673, 13, 51260], "temperature": 0.0, "avg_logprob": -0.2825143713700144, "compression_ratio": 1.469387755102041, "no_speech_prob": 4.9086531362263486e-05}, {"id": 312, "seek": 172380, "start": 1742.28, "end": 1750.76, "text": " Because think about what happens if, for example, you're in the first layer where we've got a weight", "tokens": [51288, 1436, 519, 466, 437, 2314, 498, 11, 337, 1365, 11, 291, 434, 294, 264, 700, 4583, 689, 321, 600, 658, 257, 3364, 51712], "temperature": 0.0, "avg_logprob": -0.2825143713700144, "compression_ratio": 1.469387755102041, "no_speech_prob": 4.9086531362263486e-05}, {"id": 313, "seek": 175076, "start": 1751.72, "end": 1757.56, "text": " matrix that's going to end up giving us 50 outputs, right? So for every image, we're going to", "tokens": [50412, 8141, 300, 311, 516, 281, 917, 493, 2902, 505, 2625, 23930, 11, 558, 30, 407, 337, 633, 3256, 11, 321, 434, 516, 281, 50704], "temperature": 0.0, "avg_logprob": -0.1631664229028019, "compression_ratio": 1.719298245614035, "no_speech_prob": 0.0009849956259131432}, {"id": 314, "seek": 175076, "start": 1757.56, "end": 1766.92, "text": " have 784 inputs to our function. And we're going to have 50 outputs to our function. And so in that", "tokens": [50704, 362, 1614, 25494, 15743, 281, 527, 2445, 13, 400, 321, 434, 516, 281, 362, 2625, 23930, 281, 527, 2445, 13, 400, 370, 294, 300, 51172], "temperature": 0.0, "avg_logprob": -0.1631664229028019, "compression_ratio": 1.719298245614035, "no_speech_prob": 0.0009849956259131432}, {"id": 315, "seek": 175076, "start": 1766.92, "end": 1775.48, "text": " case, I can't even draw it, right? Because like for every, even if I had two inputs and two outputs,", "tokens": [51172, 1389, 11, 286, 393, 380, 754, 2642, 309, 11, 558, 30, 1436, 411, 337, 633, 11, 754, 498, 286, 632, 732, 15743, 293, 732, 23930, 11, 51600], "temperature": 0.0, "avg_logprob": -0.1631664229028019, "compression_ratio": 1.719298245614035, "no_speech_prob": 0.0009849956259131432}, {"id": 316, "seek": 177548, "start": 1775.48, "end": 1783.16, "text": " then as I increase my first input, I'd actually need to say, how does that change both of the two", "tokens": [50364, 550, 382, 286, 3488, 452, 700, 4846, 11, 286, 1116, 767, 643, 281, 584, 11, 577, 775, 300, 1319, 1293, 295, 264, 732, 50748], "temperature": 0.0, "avg_logprob": -0.18709259033203124, "compression_ratio": 1.7696078431372548, "no_speech_prob": 0.00012931523087900132}, {"id": 317, "seek": 177548, "start": 1783.16, "end": 1788.6, "text": " outputs? And as I change my second input, how does that change both of my two outputs?", "tokens": [50748, 23930, 30, 400, 382, 286, 1319, 452, 1150, 4846, 11, 577, 775, 300, 1319, 1293, 295, 452, 732, 23930, 30, 51020], "temperature": 0.0, "avg_logprob": -0.18709259033203124, "compression_ratio": 1.7696078431372548, "no_speech_prob": 0.00012931523087900132}, {"id": 318, "seek": 177548, "start": 1790.1200000000001, "end": 1796.76, "text": " So for the full thing, you actually are going to end up with a matrix of derivatives.", "tokens": [51096, 407, 337, 264, 1577, 551, 11, 291, 767, 366, 516, 281, 917, 493, 365, 257, 8141, 295, 33733, 13, 51428], "temperature": 0.0, "avg_logprob": -0.18709259033203124, "compression_ratio": 1.7696078431372548, "no_speech_prob": 0.00012931523087900132}, {"id": 319, "seek": 177548, "start": 1797.4, "end": 1804.76, "text": " It basically says for every input that you change by a little bit, how much does it change", "tokens": [51460, 467, 1936, 1619, 337, 633, 4846, 300, 291, 1319, 538, 257, 707, 857, 11, 577, 709, 775, 309, 1319, 51828], "temperature": 0.0, "avg_logprob": -0.18709259033203124, "compression_ratio": 1.7696078431372548, "no_speech_prob": 0.00012931523087900132}, {"id": 320, "seek": 180548, "start": 1805.56, "end": 1810.04, "text": " every output of that function? So you're going to end up with a matrix.", "tokens": [50368, 633, 5598, 295, 300, 2445, 30, 407, 291, 434, 516, 281, 917, 493, 365, 257, 8141, 13, 50592], "temperature": 0.0, "avg_logprob": -0.18521932114002315, "compression_ratio": 1.7277227722772277, "no_speech_prob": 1.2029631761834025e-05}, {"id": 321, "seek": 180548, "start": 1811.8, "end": 1817.32, "text": " So that's what we're going to be doing, is we're going to be calculating these derivatives.", "tokens": [50680, 407, 300, 311, 437, 321, 434, 516, 281, 312, 884, 11, 307, 321, 434, 516, 281, 312, 28258, 613, 33733, 13, 50956], "temperature": 0.0, "avg_logprob": -0.18521932114002315, "compression_ratio": 1.7277227722772277, "no_speech_prob": 1.2029631761834025e-05}, {"id": 322, "seek": 180548, "start": 1818.28, "end": 1824.84, "text": " But rather than being single numbers, they're going to actually contain matrices with a row", "tokens": [51004, 583, 2831, 813, 885, 2167, 3547, 11, 436, 434, 516, 281, 767, 5304, 32284, 365, 257, 5386, 51332], "temperature": 0.0, "avg_logprob": -0.18521932114002315, "compression_ratio": 1.7277227722772277, "no_speech_prob": 1.2029631761834025e-05}, {"id": 323, "seek": 180548, "start": 1824.84, "end": 1832.04, "text": " for every input and a column for every output. And a single cell in that matrix will tell us,", "tokens": [51332, 337, 633, 4846, 293, 257, 7738, 337, 633, 5598, 13, 400, 257, 2167, 2815, 294, 300, 8141, 486, 980, 505, 11, 51692], "temperature": 0.0, "avg_logprob": -0.18521932114002315, "compression_ratio": 1.7277227722772277, "no_speech_prob": 1.2029631761834025e-05}, {"id": 324, "seek": 183204, "start": 1832.68, "end": 1837.56, "text": " as I change this input by a little bit, how does it change this output?", "tokens": [50396, 382, 286, 1319, 341, 4846, 538, 257, 707, 857, 11, 577, 775, 309, 1319, 341, 5598, 30, 50640], "temperature": 0.0, "avg_logprob": -0.16225974700030157, "compression_ratio": 1.5294117647058822, "no_speech_prob": 2.4300228687934577e-05}, {"id": 325, "seek": 183204, "start": 1839.3999999999999, "end": 1850.2, "text": " Now, eventually, we will end up with a single number for every input. And that's because our", "tokens": [50732, 823, 11, 4728, 11, 321, 486, 917, 493, 365, 257, 2167, 1230, 337, 633, 4846, 13, 400, 300, 311, 570, 527, 51272], "temperature": 0.0, "avg_logprob": -0.16225974700030157, "compression_ratio": 1.5294117647058822, "no_speech_prob": 2.4300228687934577e-05}, {"id": 326, "seek": 183204, "start": 1850.2, "end": 1856.12, "text": " loss in the end is going to be a single number. And this is like a requirement that you'll find", "tokens": [51272, 4470, 294, 264, 917, 307, 516, 281, 312, 257, 2167, 1230, 13, 400, 341, 307, 411, 257, 11695, 300, 291, 603, 915, 51568], "temperature": 0.0, "avg_logprob": -0.16225974700030157, "compression_ratio": 1.5294117647058822, "no_speech_prob": 2.4300228687934577e-05}, {"id": 327, "seek": 185612, "start": 1856.12, "end": 1862.9199999999998, "text": " when you try to use SGD, is that your loss has to be a single number. And so we generally get it by", "tokens": [50364, 562, 291, 853, 281, 764, 34520, 35, 11, 307, 300, 428, 4470, 575, 281, 312, 257, 2167, 1230, 13, 400, 370, 321, 5101, 483, 309, 538, 50704], "temperature": 0.0, "avg_logprob": -0.1955508194960557, "compression_ratio": 1.4766839378238341, "no_speech_prob": 0.0007321506855078042}, {"id": 328, "seek": 185612, "start": 1863.4799999999998, "end": 1870.6799999999998, "text": " either doing the sum or a mean or something like that. But as you'll see on the way there,", "tokens": [50732, 2139, 884, 264, 2408, 420, 257, 914, 420, 746, 411, 300, 13, 583, 382, 291, 603, 536, 322, 264, 636, 456, 11, 51092], "temperature": 0.0, "avg_logprob": -0.1955508194960557, "compression_ratio": 1.4766839378238341, "no_speech_prob": 0.0007321506855078042}, {"id": 329, "seek": 185612, "start": 1870.6799999999998, "end": 1879.56, "text": " we're going to have to be dealing with these matrix of derivatives. So I just want to mention,", "tokens": [51092, 321, 434, 516, 281, 362, 281, 312, 6260, 365, 613, 8141, 295, 33733, 13, 407, 286, 445, 528, 281, 2152, 11, 51536], "temperature": 0.0, "avg_logprob": -0.1955508194960557, "compression_ratio": 1.4766839378238341, "no_speech_prob": 0.0007321506855078042}, {"id": 330, "seek": 187956, "start": 1879.8799999999999, "end": 1888.36, "text": " as I might have said before, I can't even remember,", "tokens": [50380, 382, 286, 1062, 362, 848, 949, 11, 286, 393, 380, 754, 1604, 11, 50804], "temperature": 0.0, "avg_logprob": -0.2535658597946167, "compression_ratio": 1.2543859649122806, "no_speech_prob": 0.0001511781883891672}, {"id": 331, "seek": 187956, "start": 1892.12, "end": 1900.28, "text": " there is this paper that Terence Parr and I wrote a while ago, which goes through all this.", "tokens": [50992, 456, 307, 341, 3035, 300, 6564, 655, 47890, 293, 286, 4114, 257, 1339, 2057, 11, 597, 1709, 807, 439, 341, 13, 51400], "temperature": 0.0, "avg_logprob": -0.2535658597946167, "compression_ratio": 1.2543859649122806, "no_speech_prob": 0.0001511781883891672}, {"id": 332, "seek": 190028, "start": 1900.44, "end": 1909.32, "text": " And it basically assumes that you only know high school calculus. And if you don't,", "tokens": [50372, 400, 309, 1936, 37808, 300, 291, 787, 458, 1090, 1395, 33400, 13, 400, 498, 291, 500, 380, 11, 50816], "temperature": 0.0, "avg_logprob": -0.24450353716240555, "compression_ratio": 1.4450867052023122, "no_speech_prob": 0.002980954246595502}, {"id": 333, "seek": 190028, "start": 1910.12, "end": 1915.8799999999999, "text": " check out Khan Academy. But then it describes matrix calculus in those terms. So it's going", "tokens": [50856, 1520, 484, 18136, 11735, 13, 583, 550, 309, 15626, 8141, 33400, 294, 729, 2115, 13, 407, 309, 311, 516, 51144], "temperature": 0.0, "avg_logprob": -0.24450353716240555, "compression_ratio": 1.4450867052023122, "no_speech_prob": 0.002980954246595502}, {"id": 334, "seek": 190028, "start": 1915.8799999999999, "end": 1920.28, "text": " to explain to you exactly. And it works through lots and lots of examples.", "tokens": [51144, 281, 2903, 281, 291, 2293, 13, 400, 309, 1985, 807, 3195, 293, 3195, 295, 5110, 13, 51364], "temperature": 0.0, "avg_logprob": -0.24450353716240555, "compression_ratio": 1.4450867052023122, "no_speech_prob": 0.002980954246595502}, {"id": 335, "seek": 192028, "start": 1920.36, "end": 1928.68, "text": " So, for example, as it mentions here, when you have this matrix of derivatives,", "tokens": [50368, 407, 11, 337, 1365, 11, 382, 309, 23844, 510, 11, 562, 291, 362, 341, 8141, 295, 33733, 11, 50784], "temperature": 0.0, "avg_logprob": -0.36959245171345456, "compression_ratio": 1.5337078651685394, "no_speech_prob": 0.01363627053797245}, {"id": 336, "seek": 192028, "start": 1929.96, "end": 1937.8799999999999, "text": " we call that a Jacobian matrix. So there's all these words. It doesn't matter too much if you", "tokens": [50848, 321, 818, 300, 257, 14117, 952, 8141, 13, 407, 456, 311, 439, 613, 2283, 13, 467, 1177, 380, 1871, 886, 709, 498, 291, 51244], "temperature": 0.0, "avg_logprob": -0.36959245171345456, "compression_ratio": 1.5337078651685394, "no_speech_prob": 0.01363627053797245}, {"id": 337, "seek": 192028, "start": 1937.8799999999999, "end": 1944.44, "text": " know them or not. But it's convenient to be able to talk about the matrix of all of the derivatives", "tokens": [51244, 458, 552, 420, 406, 13, 583, 309, 311, 10851, 281, 312, 1075, 281, 751, 466, 264, 8141, 295, 439, 295, 264, 33733, 51572], "temperature": 0.0, "avg_logprob": -0.36959245171345456, "compression_ratio": 1.5337078651685394, "no_speech_prob": 0.01363627053797245}, {"id": 338, "seek": 194444, "start": 1944.44, "end": 1954.8400000000001, "text": " if somebody just says the Jacobian. It's a little bit easier than saying the matrix of all", "tokens": [50364, 498, 2618, 445, 1619, 264, 14117, 952, 13, 467, 311, 257, 707, 857, 3571, 813, 1566, 264, 8141, 295, 439, 50884], "temperature": 0.0, "avg_logprob": -0.36578850124193274, "compression_ratio": 1.5804597701149425, "no_speech_prob": 0.0022517202887684107}, {"id": 339, "seek": 194444, "start": 1954.8400000000001, "end": 1960.44, "text": " of the derivatives where all of the rows are the inputs and all the columns are the outputs.", "tokens": [50884, 295, 264, 33733, 689, 439, 295, 264, 13241, 366, 264, 15743, 293, 439, 264, 13766, 366, 264, 23930, 13, 51164], "temperature": 0.0, "avg_logprob": -0.36578850124193274, "compression_ratio": 1.5804597701149425, "no_speech_prob": 0.0022517202887684107}, {"id": 340, "seek": 194444, "start": 1963.0, "end": 1970.3600000000001, "text": " So, yeah, if you want to really understand, get to a point where papers are easier to read,", "tokens": [51292, 407, 11, 1338, 11, 498, 291, 528, 281, 534, 1223, 11, 483, 281, 257, 935, 689, 10577, 366, 3571, 281, 1401, 11, 51660], "temperature": 0.0, "avg_logprob": -0.36578850124193274, "compression_ratio": 1.5804597701149425, "no_speech_prob": 0.0022517202887684107}, {"id": 341, "seek": 197036, "start": 1970.4399999999998, "end": 1978.9199999999998, "text": " in particular, it's quite useful to know this notation and definitions of words.", "tokens": [50368, 294, 1729, 11, 309, 311, 1596, 4420, 281, 458, 341, 24657, 293, 21988, 295, 2283, 13, 50792], "temperature": 0.0, "avg_logprob": -0.23173502014904487, "compression_ratio": 1.5045045045045045, "no_speech_prob": 0.0005033329362049699}, {"id": 342, "seek": 197036, "start": 1979.4799999999998, "end": 1984.04, "text": " You can certainly get away without it. It's just something to consider.", "tokens": [50820, 509, 393, 3297, 483, 1314, 1553, 309, 13, 467, 311, 445, 746, 281, 1949, 13, 51048], "temperature": 0.0, "avg_logprob": -0.23173502014904487, "compression_ratio": 1.5045045045045045, "no_speech_prob": 0.0005033329362049699}, {"id": 343, "seek": 197036, "start": 1987.56, "end": 1993.56, "text": " OK. So we need to be able to calculate derivatives of at least a single variable.", "tokens": [51224, 2264, 13, 407, 321, 643, 281, 312, 1075, 281, 8873, 33733, 295, 412, 1935, 257, 2167, 7006, 13, 51524], "temperature": 0.0, "avg_logprob": -0.23173502014904487, "compression_ratio": 1.5045045045045045, "no_speech_prob": 0.0005033329362049699}, {"id": 344, "seek": 197036, "start": 1994.12, "end": 1999.6399999999999, "text": " And I am not going to worry too much about that. A, because that is something you do in high school", "tokens": [51552, 400, 286, 669, 406, 516, 281, 3292, 886, 709, 466, 300, 13, 316, 11, 570, 300, 307, 746, 291, 360, 294, 1090, 1395, 51828], "temperature": 0.0, "avg_logprob": -0.23173502014904487, "compression_ratio": 1.5045045045045045, "no_speech_prob": 0.0005033329362049699}, {"id": 345, "seek": 199964, "start": 1999.64, "end": 2008.1200000000001, "text": " math. And B, because your computer can do it for you. And so you can do it symbolically using", "tokens": [50364, 5221, 13, 400, 363, 11, 570, 428, 3820, 393, 360, 309, 337, 291, 13, 400, 370, 291, 393, 360, 309, 5986, 984, 1228, 50788], "temperature": 0.0, "avg_logprob": -0.28148899907651154, "compression_ratio": 1.6, "no_speech_prob": 0.00034599058562889695}, {"id": 346, "seek": 199964, "start": 2008.1200000000001, "end": 2014.1200000000001, "text": " something called sympi, which is really great. If you create two symbols called x and y,", "tokens": [50788, 746, 1219, 6697, 22630, 11, 597, 307, 534, 869, 13, 759, 291, 1884, 732, 16944, 1219, 2031, 293, 288, 11, 51088], "temperature": 0.0, "avg_logprob": -0.28148899907651154, "compression_ratio": 1.6, "no_speech_prob": 0.00034599058562889695}, {"id": 347, "seek": 199964, "start": 2015.5600000000002, "end": 2023.64, "text": " you can say please differentiate x squared with respect to x. And if you do that,", "tokens": [51160, 291, 393, 584, 1767, 23203, 2031, 8889, 365, 3104, 281, 2031, 13, 400, 498, 291, 360, 300, 11, 51564], "temperature": 0.0, "avg_logprob": -0.28148899907651154, "compression_ratio": 1.6, "no_speech_prob": 0.00034599058562889695}, {"id": 348, "seek": 202364, "start": 2024.5200000000002, "end": 2035.16, "text": " sympi will tell you the answer is 2x. If you say differentiate 3x squared plus 9 with respect to x,", "tokens": [50408, 6697, 22630, 486, 980, 291, 264, 1867, 307, 568, 87, 13, 759, 291, 584, 23203, 805, 87, 8889, 1804, 1722, 365, 3104, 281, 2031, 11, 50940], "temperature": 0.0, "avg_logprob": -0.2391881684999208, "compression_ratio": 1.4574468085106382, "no_speech_prob": 0.0013670099433511496}, {"id": 349, "seek": 202364, "start": 2035.96, "end": 2043.0, "text": " sympi will tell you that's 6x. And a lot of you probably will have used Wolfram Alpha,", "tokens": [50980, 6697, 22630, 486, 980, 291, 300, 311, 1386, 87, 13, 400, 257, 688, 295, 291, 1391, 486, 362, 1143, 16634, 2356, 20588, 11, 51332], "temperature": 0.0, "avg_logprob": -0.2391881684999208, "compression_ratio": 1.4574468085106382, "no_speech_prob": 0.0013670099433511496}, {"id": 350, "seek": 202364, "start": 2043.0, "end": 2047.64, "text": " that does something very similar. I kind of quite like this because I can quickly do it", "tokens": [51332, 300, 775, 746, 588, 2531, 13, 286, 733, 295, 1596, 411, 341, 570, 286, 393, 2661, 360, 309, 51564], "temperature": 0.0, "avg_logprob": -0.2391881684999208, "compression_ratio": 1.4574468085106382, "no_speech_prob": 0.0013670099433511496}, {"id": 351, "seek": 204764, "start": 2047.64, "end": 2055.1600000000003, "text": " inside my notebook and include it in my prose. So I think sympi is pretty cool. So, you know,", "tokens": [50364, 1854, 452, 21060, 293, 4090, 309, 294, 452, 12505, 13, 407, 286, 519, 6697, 22630, 307, 1238, 1627, 13, 407, 11, 291, 458, 11, 50740], "temperature": 0.0, "avg_logprob": -0.2077172006879534, "compression_ratio": 1.461111111111111, "no_speech_prob": 0.0003859583812300116}, {"id": 352, "seek": 204764, "start": 2055.1600000000003, "end": 2061.6400000000003, "text": " basically, yeah, if you, you know, you can quickly calculate derivatives on a computer.", "tokens": [50740, 1936, 11, 1338, 11, 498, 291, 11, 291, 458, 11, 291, 393, 2661, 8873, 33733, 322, 257, 3820, 13, 51064], "temperature": 0.0, "avg_logprob": -0.2077172006879534, "compression_ratio": 1.461111111111111, "no_speech_prob": 0.0003859583812300116}, {"id": 353, "seek": 204764, "start": 2064.92, "end": 2070.52, "text": " Having said that, I do want to talk about why the derivative of 3x squared plus 9", "tokens": [51228, 10222, 848, 300, 11, 286, 360, 528, 281, 751, 466, 983, 264, 13760, 295, 805, 87, 8889, 1804, 1722, 51508], "temperature": 0.0, "avg_logprob": -0.2077172006879534, "compression_ratio": 1.461111111111111, "no_speech_prob": 0.0003859583812300116}, {"id": 354, "seek": 207052, "start": 2071.08, "end": 2074.68, "text": " equals 6x, because that's going to be very important.", "tokens": [50392, 6915, 1386, 87, 11, 570, 300, 311, 516, 281, 312, 588, 1021, 13, 50572], "temperature": 0.0, "avg_logprob": -0.22415624724494088, "compression_ratio": 1.1368421052631579, "no_speech_prob": 1.1659482879622374e-05}, {"id": 355, "seek": 207052, "start": 2079.32, "end": 2080.7599999999998, "text": " So 3x squared", "tokens": [50804, 407, 805, 87, 8889, 50876], "temperature": 0.0, "avg_logprob": -0.22415624724494088, "compression_ratio": 1.1368421052631579, "no_speech_prob": 1.1659482879622374e-05}, {"id": 356, "seek": 207052, "start": 2083.96, "end": 2090.6, "text": " plus 9. So we're going to start with the", "tokens": [51036, 1804, 1722, 13, 407, 321, 434, 516, 281, 722, 365, 264, 51368], "temperature": 0.0, "avg_logprob": -0.22415624724494088, "compression_ratio": 1.1368421052631579, "no_speech_prob": 1.1659482879622374e-05}, {"id": 357, "seek": 209060, "start": 2091.24, "end": 2098.2799999999997, "text": " the information that the derivative of a to the b", "tokens": [50396, 264, 1589, 300, 264, 13760, 295, 257, 281, 264, 272, 50748], "temperature": 0.0, "avg_logprob": -0.27974406155672943, "compression_ratio": 1.4766355140186915, "no_speech_prob": 0.00021318723156582564}, {"id": 358, "seek": 209060, "start": 2103.3199999999997, "end": 2104.2799999999997, "text": " with respect to a", "tokens": [51000, 365, 3104, 281, 257, 51048], "temperature": 0.0, "avg_logprob": -0.27974406155672943, "compression_ratio": 1.4766355140186915, "no_speech_prob": 0.00021318723156582564}, {"id": 359, "seek": 209060, "start": 2107.72, "end": 2116.36, "text": " equals b times a. So for example, the derivative of x squared with respect to x equals 2x.", "tokens": [51220, 6915, 272, 1413, 257, 13, 407, 337, 1365, 11, 264, 13760, 295, 2031, 8889, 365, 3104, 281, 2031, 6915, 568, 87, 13, 51652], "temperature": 0.0, "avg_logprob": -0.27974406155672943, "compression_ratio": 1.4766355140186915, "no_speech_prob": 0.00021318723156582564}, {"id": 360, "seek": 211636, "start": 2116.92, "end": 2121.08, "text": " So that's just something I'm hoping you'll remember from high school or refresh your", "tokens": [50392, 407, 300, 311, 445, 746, 286, 478, 7159, 291, 603, 1604, 490, 1090, 1395, 420, 15134, 428, 50600], "temperature": 0.0, "avg_logprob": -0.20508505693122522, "compression_ratio": 1.4636871508379887, "no_speech_prob": 0.00011235308920731768}, {"id": 361, "seek": 211636, "start": 2121.08, "end": 2127.2400000000002, "text": " memory using Kahn Academy or similar. So there, that is there. So what we could now do is we", "tokens": [50600, 4675, 1228, 591, 12140, 11735, 420, 2531, 13, 407, 456, 11, 300, 307, 456, 13, 407, 437, 321, 727, 586, 360, 307, 321, 50908], "temperature": 0.0, "avg_logprob": -0.20508505693122522, "compression_ratio": 1.4636871508379887, "no_speech_prob": 0.00011235308920731768}, {"id": 362, "seek": 211636, "start": 2127.2400000000002, "end": 2144.52, "text": " could rewrite this derivative as 3u plus 9. And then we'll write u equals x squared.", "tokens": [50908, 727, 28132, 341, 13760, 382, 805, 84, 1804, 1722, 13, 400, 550, 321, 603, 2464, 344, 6915, 2031, 8889, 13, 51772], "temperature": 0.0, "avg_logprob": -0.20508505693122522, "compression_ratio": 1.4636871508379887, "no_speech_prob": 0.00011235308920731768}, {"id": 363, "seek": 214636, "start": 2146.6800000000003, "end": 2154.44, "text": " Okay. Now this is getting easier. The derivative of two things being added together", "tokens": [50380, 1033, 13, 823, 341, 307, 1242, 3571, 13, 440, 13760, 295, 732, 721, 885, 3869, 1214, 50768], "temperature": 0.0, "avg_logprob": -0.23451572269588322, "compression_ratio": 1.55, "no_speech_prob": 5.475891521200538e-05}, {"id": 364, "seek": 214636, "start": 2155.1600000000003, "end": 2163.0, "text": " is simply the sum of their derivatives. Oh, forgot b minus 1 in the exponent. Thank you. So b a to", "tokens": [50804, 307, 2935, 264, 2408, 295, 641, 33733, 13, 876, 11, 5298, 272, 3175, 502, 294, 264, 37871, 13, 1044, 291, 13, 407, 272, 257, 281, 51196], "temperature": 0.0, "avg_logprob": -0.23451572269588322, "compression_ratio": 1.55, "no_speech_prob": 5.475891521200538e-05}, {"id": 365, "seek": 214636, "start": 2163.0, "end": 2170.2000000000003, "text": " the power of b minus 1. That's what it should be, which would be 2x to the power of 1. And the 1", "tokens": [51196, 264, 1347, 295, 272, 3175, 502, 13, 663, 311, 437, 309, 820, 312, 11, 597, 576, 312, 568, 87, 281, 264, 1347, 295, 502, 13, 400, 264, 502, 51556], "temperature": 0.0, "avg_logprob": -0.23451572269588322, "compression_ratio": 1.55, "no_speech_prob": 5.475891521200538e-05}, {"id": 366, "seek": 217020, "start": 2170.2, "end": 2180.04, "text": " is not needed. Thank you for fixing that. All right. So we just sum them up. So we get the", "tokens": [50364, 307, 406, 2978, 13, 1044, 291, 337, 19442, 300, 13, 1057, 558, 13, 407, 321, 445, 2408, 552, 493, 13, 407, 321, 483, 264, 50856], "temperature": 0.0, "avg_logprob": -0.16425692867225325, "compression_ratio": 1.6424242424242423, "no_speech_prob": 0.0009110483224503696}, {"id": 367, "seek": 217020, "start": 2180.04, "end": 2191.16, "text": " derivative of 3u is actually just, well, it's going to be the derivative of that plus the", "tokens": [50856, 13760, 295, 805, 84, 307, 767, 445, 11, 731, 11, 309, 311, 516, 281, 312, 264, 13760, 295, 300, 1804, 264, 51412], "temperature": 0.0, "avg_logprob": -0.16425692867225325, "compression_ratio": 1.6424242424242423, "no_speech_prob": 0.0009110483224503696}, {"id": 368, "seek": 217020, "start": 2191.16, "end": 2196.52, "text": " derivative of that. Now the derivative of any constant with respect to a variable is zero,", "tokens": [51412, 13760, 295, 300, 13, 823, 264, 13760, 295, 604, 5754, 365, 3104, 281, 257, 7006, 307, 4018, 11, 51680], "temperature": 0.0, "avg_logprob": -0.16425692867225325, "compression_ratio": 1.6424242424242423, "no_speech_prob": 0.0009110483224503696}, {"id": 369, "seek": 219652, "start": 2196.52, "end": 2201.64, "text": " because if I change something, an input, it doesn't change the constant. It's always 9.", "tokens": [50364, 570, 498, 286, 1319, 746, 11, 364, 4846, 11, 309, 1177, 380, 1319, 264, 5754, 13, 467, 311, 1009, 1722, 13, 50620], "temperature": 0.0, "avg_logprob": -0.20721548171270462, "compression_ratio": 1.672811059907834, "no_speech_prob": 6.502814358100295e-05}, {"id": 370, "seek": 219652, "start": 2201.64, "end": 2209.48, "text": " So that's going to end up as zero. And so we're going to end up with dy du equals", "tokens": [50620, 407, 300, 311, 516, 281, 917, 493, 382, 4018, 13, 400, 370, 321, 434, 516, 281, 917, 493, 365, 14584, 1581, 6915, 51012], "temperature": 0.0, "avg_logprob": -0.20721548171270462, "compression_ratio": 1.672811059907834, "no_speech_prob": 6.502814358100295e-05}, {"id": 371, "seek": 219652, "start": 2210.68, "end": 2215.8, "text": " something plus zero. And the derivative of 3u with respect to u is just 3, because it's just a", "tokens": [51072, 746, 1804, 4018, 13, 400, 264, 13760, 295, 805, 84, 365, 3104, 281, 344, 307, 445, 805, 11, 570, 309, 311, 445, 257, 51328], "temperature": 0.0, "avg_logprob": -0.20721548171270462, "compression_ratio": 1.672811059907834, "no_speech_prob": 6.502814358100295e-05}, {"id": 372, "seek": 219652, "start": 2215.8, "end": 2225.08, "text": " line. So that's its slope. Okay. But that's not dy dx. We want dy dx. Well, the cool thing is that", "tokens": [51328, 1622, 13, 407, 300, 311, 1080, 13525, 13, 1033, 13, 583, 300, 311, 406, 14584, 30017, 13, 492, 528, 14584, 30017, 13, 1042, 11, 264, 1627, 551, 307, 300, 51792], "temperature": 0.0, "avg_logprob": -0.20721548171270462, "compression_ratio": 1.672811059907834, "no_speech_prob": 6.502814358100295e-05}, {"id": 373, "seek": 222508, "start": 2225.08, "end": 2239.48, "text": " dy dx is actually just equal to dy du du dx. So I'll explain why in a moment. But for now,", "tokens": [50364, 14584, 30017, 307, 767, 445, 2681, 281, 14584, 1581, 1581, 30017, 13, 407, 286, 603, 2903, 983, 294, 257, 1623, 13, 583, 337, 586, 11, 51084], "temperature": 0.0, "avg_logprob": -0.20464643938788052, "compression_ratio": 1.295774647887324, "no_speech_prob": 0.00018814101349562407}, {"id": 374, "seek": 222508, "start": 2239.48, "end": 2248.2799999999997, "text": " then let's recognize we've got dy, sorry, du dx. We know that one, 2x. So we can now multiply", "tokens": [51084, 550, 718, 311, 5521, 321, 600, 658, 14584, 11, 2597, 11, 1581, 30017, 13, 492, 458, 300, 472, 11, 568, 87, 13, 407, 321, 393, 586, 12972, 51524], "temperature": 0.0, "avg_logprob": -0.20464643938788052, "compression_ratio": 1.295774647887324, "no_speech_prob": 0.00018814101349562407}, {"id": 375, "seek": 224828, "start": 2248.28, "end": 2258.1200000000003, "text": " these two bits together. And we will end up with 2x times 3, which is 6x, which is what Simpai told", "tokens": [50364, 613, 732, 9239, 1214, 13, 400, 321, 486, 917, 493, 365, 568, 87, 1413, 805, 11, 597, 307, 1386, 87, 11, 597, 307, 437, 3998, 43502, 1907, 50856], "temperature": 0.0, "avg_logprob": -0.1810582680038259, "compression_ratio": 1.5978260869565217, "no_speech_prob": 0.0009697471396066248}, {"id": 376, "seek": 224828, "start": 2258.1200000000003, "end": 2265.0, "text": " us. So fantastic. Okay. This is something we need to know really well. And it's called the chain", "tokens": [50856, 505, 13, 407, 5456, 13, 1033, 13, 639, 307, 746, 321, 643, 281, 458, 534, 731, 13, 400, 309, 311, 1219, 264, 5021, 51200], "temperature": 0.0, "avg_logprob": -0.1810582680038259, "compression_ratio": 1.5978260869565217, "no_speech_prob": 0.0009697471396066248}, {"id": 377, "seek": 224828, "start": 2265.0, "end": 2271.6400000000003, "text": " rule. And it's best to understand it intuitively. So to understand it intuitively, we're going to", "tokens": [51200, 4978, 13, 400, 309, 311, 1151, 281, 1223, 309, 46506, 13, 407, 281, 1223, 309, 46506, 11, 321, 434, 516, 281, 51532], "temperature": 0.0, "avg_logprob": -0.1810582680038259, "compression_ratio": 1.5978260869565217, "no_speech_prob": 0.0009697471396066248}, {"id": 378, "seek": 227164, "start": 2271.64, "end": 2283.0, "text": " take a look at an interactive animation. So I found this nice interactive animation", "tokens": [50364, 747, 257, 574, 412, 364, 15141, 9603, 13, 407, 286, 1352, 341, 1481, 15141, 9603, 50932], "temperature": 0.0, "avg_logprob": -0.2822277108017279, "compression_ratio": 1.3692307692307693, "no_speech_prob": 0.07262754440307617}, {"id": 379, "seek": 227164, "start": 2284.2, "end": 2297.8799999999997, "text": " on this page here. Webspace.ship.edu. Geogebra calculus. Okay. And the idea here is that we've", "tokens": [50992, 322, 341, 3028, 510, 13, 45347, 17940, 13, 2716, 647, 13, 22938, 13, 2876, 78, 19983, 33400, 13, 1033, 13, 400, 264, 1558, 510, 307, 300, 321, 600, 51676], "temperature": 0.0, "avg_logprob": -0.2822277108017279, "compression_ratio": 1.3692307692307693, "no_speech_prob": 0.07262754440307617}, {"id": 380, "seek": 229788, "start": 2297.88, "end": 2307.08, "text": " got a wheel spinning around. And each time it spins around, this is x going up. Okay. So at the", "tokens": [50364, 658, 257, 5589, 15640, 926, 13, 400, 1184, 565, 309, 31587, 926, 11, 341, 307, 2031, 516, 493, 13, 1033, 13, 407, 412, 264, 50824], "temperature": 0.0, "avg_logprob": -0.1774502048244724, "compression_ratio": 1.5819209039548023, "no_speech_prob": 4.4001080823363736e-05}, {"id": 381, "seek": 229788, "start": 2307.08, "end": 2318.36, "text": " moment, there's some change in x, dx, over a period of time. All right. Now, this wheel is", "tokens": [50824, 1623, 11, 456, 311, 512, 1319, 294, 2031, 11, 30017, 11, 670, 257, 2896, 295, 565, 13, 1057, 558, 13, 823, 11, 341, 5589, 307, 51388], "temperature": 0.0, "avg_logprob": -0.1774502048244724, "compression_ratio": 1.5819209039548023, "no_speech_prob": 4.4001080823363736e-05}, {"id": 382, "seek": 229788, "start": 2320.12, "end": 2326.12, "text": " eight times bigger than this wheel. So each time this goes around once, if we connect the two", "tokens": [51476, 3180, 1413, 3801, 813, 341, 5589, 13, 407, 1184, 565, 341, 1709, 926, 1564, 11, 498, 321, 1745, 264, 732, 51776], "temperature": 0.0, "avg_logprob": -0.1774502048244724, "compression_ratio": 1.5819209039548023, "no_speech_prob": 4.4001080823363736e-05}, {"id": 383, "seek": 232612, "start": 2326.12, "end": 2337.88, "text": " together, this wheel would be going around four times faster. Because the difference between the", "tokens": [50364, 1214, 11, 341, 5589, 576, 312, 516, 926, 1451, 1413, 4663, 13, 1436, 264, 2649, 1296, 264, 50952], "temperature": 0.0, "avg_logprob": -0.2234497208526169, "compression_ratio": 1.554945054945055, "no_speech_prob": 1.89251295523718e-05}, {"id": 384, "seek": 232612, "start": 2337.88, "end": 2345.16, "text": " multiple between eight and two is four. Maybe I'll bring this up to here. So now that this wheel is,", "tokens": [50952, 3866, 1296, 3180, 293, 732, 307, 1451, 13, 2704, 286, 603, 1565, 341, 493, 281, 510, 13, 407, 586, 300, 341, 5589, 307, 11, 51316], "temperature": 0.0, "avg_logprob": -0.2234497208526169, "compression_ratio": 1.554945054945055, "no_speech_prob": 1.89251295523718e-05}, {"id": 385, "seek": 232612, "start": 2345.7999999999997, "end": 2350.52, "text": " has got twice as big a circumference as the u wheel, each time this goes around once,", "tokens": [51348, 575, 658, 6091, 382, 955, 257, 7125, 5158, 382, 264, 344, 5589, 11, 1184, 565, 341, 1709, 926, 1564, 11, 51584], "temperature": 0.0, "avg_logprob": -0.2234497208526169, "compression_ratio": 1.554945054945055, "no_speech_prob": 1.89251295523718e-05}, {"id": 386, "seek": 235052, "start": 2351.08, "end": 2361.24, "text": " this is going around two times. So the change in u, each time x goes around once, the change in u", "tokens": [50392, 341, 307, 516, 926, 732, 1413, 13, 407, 264, 1319, 294, 344, 11, 1184, 565, 2031, 1709, 926, 1564, 11, 264, 1319, 294, 344, 50900], "temperature": 0.0, "avg_logprob": -0.3340601554283729, "compression_ratio": 1.7888198757763976, "no_speech_prob": 4.539784276857972e-05}, {"id": 387, "seek": 235052, "start": 2361.24, "end": 2368.12, "text": " will be two. So that's what du dx is saying. The change in u for each change in x is two.", "tokens": [50900, 486, 312, 732, 13, 407, 300, 311, 437, 1581, 30017, 307, 1566, 13, 440, 1319, 294, 344, 337, 1184, 1319, 294, 2031, 307, 732, 13, 51244], "temperature": 0.0, "avg_logprob": -0.3340601554283729, "compression_ratio": 1.7888198757763976, "no_speech_prob": 4.539784276857972e-05}, {"id": 388, "seek": 235052, "start": 2370.04, "end": 2376.7599999999998, "text": " Now, we could make this interesting by connecting this wheel to this wheel. Now, this wheel is twice", "tokens": [51340, 823, 11, 321, 727, 652, 341, 1880, 538, 11015, 341, 5589, 281, 341, 5589, 13, 823, 11, 341, 5589, 307, 6091, 51676], "temperature": 0.0, "avg_logprob": -0.3340601554283729, "compression_ratio": 1.7888198757763976, "no_speech_prob": 4.539784276857972e-05}, {"id": 389, "seek": 237676, "start": 2377.7200000000003, "end": 2388.0400000000004, "text": " as small as this wheel. So now we can see that, again, each time this spins around once,", "tokens": [50412, 382, 1359, 382, 341, 5589, 13, 407, 586, 321, 393, 536, 300, 11, 797, 11, 1184, 565, 341, 31587, 926, 1564, 11, 50928], "temperature": 0.0, "avg_logprob": -0.2211818286350795, "compression_ratio": 1.748427672955975, "no_speech_prob": 0.00015118144801817834}, {"id": 390, "seek": 237676, "start": 2388.6000000000004, "end": 2394.84, "text": " this spins around twice, because this has twice the circumference of this. So therefore, dy du", "tokens": [50956, 341, 31587, 926, 6091, 11, 570, 341, 575, 6091, 264, 7125, 5158, 295, 341, 13, 407, 4412, 11, 14584, 1581, 51268], "temperature": 0.0, "avg_logprob": -0.2211818286350795, "compression_ratio": 1.748427672955975, "no_speech_prob": 0.00015118144801817834}, {"id": 391, "seek": 237676, "start": 2394.84, "end": 2402.36, "text": " equals two. But now that means every time this goes around once, this goes around twice. Every", "tokens": [51268, 6915, 732, 13, 583, 586, 300, 1355, 633, 565, 341, 1709, 926, 1564, 11, 341, 1709, 926, 6091, 13, 2048, 51644], "temperature": 0.0, "avg_logprob": -0.2211818286350795, "compression_ratio": 1.748427672955975, "no_speech_prob": 0.00015118144801817834}, {"id": 392, "seek": 240236, "start": 2402.36, "end": 2406.92, "text": " time this one goes around once, this gun goes around twice. So therefore, every time this one", "tokens": [50364, 565, 341, 472, 1709, 926, 1564, 11, 341, 3874, 1709, 926, 6091, 13, 407, 4412, 11, 633, 565, 341, 472, 50592], "temperature": 0.0, "avg_logprob": -0.21967815447457228, "compression_ratio": 1.826086956521739, "no_speech_prob": 8.6146334069781e-05}, {"id": 393, "seek": 240236, "start": 2406.92, "end": 2419.32, "text": " goes around once, this one goes around four times. So dy dx equals four. So you can see here how the", "tokens": [50592, 1709, 926, 1564, 11, 341, 472, 1709, 926, 1451, 1413, 13, 407, 14584, 30017, 6915, 1451, 13, 407, 291, 393, 536, 510, 577, 264, 51212], "temperature": 0.0, "avg_logprob": -0.21967815447457228, "compression_ratio": 1.826086956521739, "no_speech_prob": 8.6146334069781e-05}, {"id": 394, "seek": 240236, "start": 2419.4, "end": 2428.92, "text": " two, well, how the dx du dx has to be multiplied with the dy du to get the total. So this is what's", "tokens": [51216, 732, 11, 731, 11, 577, 264, 30017, 1581, 30017, 575, 281, 312, 17207, 365, 264, 14584, 1581, 281, 483, 264, 3217, 13, 407, 341, 307, 437, 311, 51692], "temperature": 0.0, "avg_logprob": -0.21967815447457228, "compression_ratio": 1.826086956521739, "no_speech_prob": 8.6146334069781e-05}, {"id": 395, "seek": 242892, "start": 2428.92, "end": 2434.92, "text": " going on in the chain rule. And this is what you want to be thinking about, is this idea that", "tokens": [50364, 516, 322, 294, 264, 5021, 4978, 13, 400, 341, 307, 437, 291, 528, 281, 312, 1953, 466, 11, 307, 341, 1558, 300, 50664], "temperature": 0.0, "avg_logprob": -0.20775660601529208, "compression_ratio": 1.6732673267326732, "no_speech_prob": 0.0028895323630422354}, {"id": 396, "seek": 242892, "start": 2436.36, "end": 2439.48, "text": " you've got one function that is kind of this intermediary,", "tokens": [50736, 291, 600, 658, 472, 2445, 300, 307, 733, 295, 341, 15184, 822, 11, 50892], "temperature": 0.0, "avg_logprob": -0.20775660601529208, "compression_ratio": 1.6732673267326732, "no_speech_prob": 0.0028895323630422354}, {"id": 397, "seek": 242892, "start": 2441.2400000000002, "end": 2447.7200000000003, "text": " and so you have to multiply the two impacts to get the impact of the x wheel on the y wheel.", "tokens": [50980, 293, 370, 291, 362, 281, 12972, 264, 732, 11606, 281, 483, 264, 2712, 295, 264, 2031, 5589, 322, 264, 288, 5589, 13, 51304], "temperature": 0.0, "avg_logprob": -0.20775660601529208, "compression_ratio": 1.6732673267326732, "no_speech_prob": 0.0028895323630422354}, {"id": 398, "seek": 242892, "start": 2448.92, "end": 2455.2400000000002, "text": " So I hope you find that useful. I find this, personally, I find this intuition quite useful.", "tokens": [51364, 407, 286, 1454, 291, 915, 300, 4420, 13, 286, 915, 341, 11, 5665, 11, 286, 915, 341, 24002, 1596, 4420, 13, 51680], "temperature": 0.0, "avg_logprob": -0.20775660601529208, "compression_ratio": 1.6732673267326732, "no_speech_prob": 0.0028895323630422354}, {"id": 399, "seek": 245524, "start": 2455.4799999999996, "end": 2461.4799999999996, "text": " So why do we care about this? Well, the reason we care about this is because we want to calculate", "tokens": [50376, 407, 983, 360, 321, 1127, 466, 341, 30, 1042, 11, 264, 1778, 321, 1127, 466, 341, 307, 570, 321, 528, 281, 8873, 50676], "temperature": 0.0, "avg_logprob": -0.3657593105150306, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0003101488691754639}, {"id": 400, "seek": 245524, "start": 2462.4399999999996, "end": 2479.56, "text": " the gradient of our MSE applied to our model. And so our inputs are going through a linear,", "tokens": [50724, 264, 16235, 295, 527, 376, 5879, 6456, 281, 527, 2316, 13, 400, 370, 527, 15743, 366, 516, 807, 257, 8213, 11, 51580], "temperature": 0.0, "avg_logprob": -0.3657593105150306, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0003101488691754639}, {"id": 401, "seek": 245524, "start": 2479.56, "end": 2482.68, "text": " they're going through a ReLU, they're going through another ReLU,", "tokens": [51580, 436, 434, 516, 807, 257, 1300, 43, 52, 11, 436, 434, 516, 807, 1071, 1300, 43, 52, 11, 51736], "temperature": 0.0, "avg_logprob": -0.3657593105150306, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0003101488691754639}, {"id": 402, "seek": 248268, "start": 2482.68, "end": 2486.6, "text": " they're going through a ReLU, they're going through another linear, and then they're going", "tokens": [50364, 436, 434, 516, 807, 257, 1300, 43, 52, 11, 436, 434, 516, 807, 1071, 8213, 11, 293, 550, 436, 434, 516, 50560], "temperature": 0.0, "avg_logprob": -0.19770871776424043, "compression_ratio": 1.7018633540372672, "no_speech_prob": 6.1441405705409124e-06}, {"id": 403, "seek": 248268, "start": 2486.6, "end": 2491.0, "text": " through an MSE. So there's four different steps going on. And so we're going to have to combine", "tokens": [50560, 807, 364, 376, 5879, 13, 407, 456, 311, 1451, 819, 4439, 516, 322, 13, 400, 370, 321, 434, 516, 281, 362, 281, 10432, 50780], "temperature": 0.0, "avg_logprob": -0.19770871776424043, "compression_ratio": 1.7018633540372672, "no_speech_prob": 6.1441405705409124e-06}, {"id": 404, "seek": 248268, "start": 2491.0, "end": 2498.52, "text": " those all together. And so we can do that with the chain rule. So if our steps are that", "tokens": [50780, 729, 439, 1214, 13, 400, 370, 321, 393, 360, 300, 365, 264, 5021, 4978, 13, 407, 498, 527, 4439, 366, 300, 51156], "temperature": 0.0, "avg_logprob": -0.19770871776424043, "compression_ratio": 1.7018633540372672, "no_speech_prob": 6.1441405705409124e-06}, {"id": 405, "seek": 249852, "start": 2499.48, "end": 2507.32, "text": " loss function is, so we've got the loss function, which is some function of the predictions", "tokens": [50412, 4470, 2445, 307, 11, 370, 321, 600, 658, 264, 4470, 2445, 11, 597, 307, 512, 2445, 295, 264, 21264, 50804], "temperature": 0.0, "avg_logprob": -0.5638260508692542, "compression_ratio": 1.56, "no_speech_prob": 0.002115644747391343}, {"id": 406, "seek": 249852, "start": 2508.28, "end": 2518.36, "text": " and the actuals. And then we've got, we've got the second layer,", "tokens": [50852, 293, 264, 3539, 82, 13, 400, 550, 321, 600, 658, 11, 321, 600, 658, 264, 1150, 4583, 11, 51356], "temperature": 0.0, "avg_logprob": -0.5638260508692542, "compression_ratio": 1.56, "no_speech_prob": 0.002115644747391343}, {"id": 407, "seek": 251836, "start": 2518.6800000000003, "end": 2529.0, "text": " we've got the second layer is a function of,", "tokens": [50380, 321, 600, 658, 264, 1150, 4583, 307, 257, 2445, 295, 11, 50896], "temperature": 0.0, "avg_logprob": -0.24999911444527761, "compression_ratio": 1.4776119402985075, "no_speech_prob": 0.00010229580948362127}, {"id": 408, "seek": 251836, "start": 2532.2000000000003, "end": 2535.8, "text": " actually, let's say, let's call this the output of the second layer.", "tokens": [51056, 767, 11, 718, 311, 584, 11, 718, 311, 818, 341, 264, 5598, 295, 264, 1150, 4583, 13, 51236], "temperature": 0.0, "avg_logprob": -0.24999911444527761, "compression_ratio": 1.4776119402985075, "no_speech_prob": 0.00010229580948362127}, {"id": 409, "seek": 251836, "start": 2538.44, "end": 2542.6, "text": " Slightly weird notation, but hopefully it's not too bad. It's going to be a function", "tokens": [51368, 318, 44872, 3657, 24657, 11, 457, 4696, 309, 311, 406, 886, 1578, 13, 467, 311, 516, 281, 312, 257, 2445, 51576], "temperature": 0.0, "avg_logprob": -0.24999911444527761, "compression_ratio": 1.4776119402985075, "no_speech_prob": 0.00010229580948362127}, {"id": 410, "seek": 254260, "start": 2542.6, "end": 2555.24, "text": " of the ReLU, of the ReLU activations. And the ReLU activations are a function of the first", "tokens": [50364, 295, 264, 1300, 43, 52, 11, 295, 264, 1300, 43, 52, 2430, 763, 13, 400, 264, 1300, 43, 52, 2430, 763, 366, 257, 2445, 295, 264, 700, 50996], "temperature": 0.0, "avg_logprob": -0.19276065485818045, "compression_ratio": 1.654867256637168, "no_speech_prob": 0.0007096354966051877}, {"id": 411, "seek": 254260, "start": 2555.24, "end": 2561.96, "text": " layer. And the first layer is a function of the inputs. Oh, and of course, this also has weights", "tokens": [50996, 4583, 13, 400, 264, 700, 4583, 307, 257, 2445, 295, 264, 15743, 13, 876, 11, 293, 295, 1164, 11, 341, 611, 575, 17443, 51332], "temperature": 0.0, "avg_logprob": -0.19276065485818045, "compression_ratio": 1.654867256637168, "no_speech_prob": 0.0007096354966051877}, {"id": 412, "seek": 256196, "start": 2561.96, "end": 2574.76, "text": " and biases. So we're basically going to have to calculate the derivative of that. Okay. But then", "tokens": [50364, 293, 32152, 13, 407, 321, 434, 1936, 516, 281, 362, 281, 8873, 264, 13760, 295, 300, 13, 1033, 13, 583, 550, 51004], "temperature": 0.0, "avg_logprob": -0.17978412454778497, "compression_ratio": 2.2134146341463414, "no_speech_prob": 0.09668591618537903}, {"id": 413, "seek": 256196, "start": 2574.76, "end": 2579.4, "text": " remember that this is itself a function. So then we'll need to multiply that derivative by the", "tokens": [51004, 1604, 300, 341, 307, 2564, 257, 2445, 13, 407, 550, 321, 603, 643, 281, 12972, 300, 13760, 538, 264, 51236], "temperature": 0.0, "avg_logprob": -0.17978412454778497, "compression_ratio": 2.2134146341463414, "no_speech_prob": 0.09668591618537903}, {"id": 414, "seek": 256196, "start": 2579.4, "end": 2585.48, "text": " derivative of that, but that's also a function. So we have to multiply that derivative by this,", "tokens": [51236, 13760, 295, 300, 11, 457, 300, 311, 611, 257, 2445, 13, 407, 321, 362, 281, 12972, 300, 13760, 538, 341, 11, 51540], "temperature": 0.0, "avg_logprob": -0.17978412454778497, "compression_ratio": 2.2134146341463414, "no_speech_prob": 0.09668591618537903}, {"id": 415, "seek": 256196, "start": 2585.48, "end": 2588.44, "text": " but that's also a function. So we have to multiply that derivative by this.", "tokens": [51540, 457, 300, 311, 611, 257, 2445, 13, 407, 321, 362, 281, 12972, 300, 13760, 538, 341, 13, 51688], "temperature": 0.0, "avg_logprob": -0.17978412454778497, "compression_ratio": 2.2134146341463414, "no_speech_prob": 0.09668591618537903}, {"id": 416, "seek": 258844, "start": 2589.0, "end": 2591.8, "text": " So that's going to be our approach. We're going to start at the end.", "tokens": [50392, 407, 300, 311, 516, 281, 312, 527, 3109, 13, 492, 434, 516, 281, 722, 412, 264, 917, 13, 50532], "temperature": 0.0, "avg_logprob": -0.17891574487453554, "compression_ratio": 1.6170212765957446, "no_speech_prob": 0.0015487397322431207}, {"id": 417, "seek": 258844, "start": 2593.32, "end": 2598.68, "text": " We're going to take its derivative, and then we're going to gradually keep multiplying", "tokens": [50608, 492, 434, 516, 281, 747, 1080, 13760, 11, 293, 550, 321, 434, 516, 281, 13145, 1066, 30955, 50876], "temperature": 0.0, "avg_logprob": -0.17891574487453554, "compression_ratio": 1.6170212765957446, "no_speech_prob": 0.0015487397322431207}, {"id": 418, "seek": 258844, "start": 2598.68, "end": 2602.76, "text": " as we go each step through. And this is called backpropagation.", "tokens": [50876, 382, 321, 352, 1184, 1823, 807, 13, 400, 341, 307, 1219, 646, 79, 1513, 559, 399, 13, 51080], "temperature": 0.0, "avg_logprob": -0.17891574487453554, "compression_ratio": 1.6170212765957446, "no_speech_prob": 0.0015487397322431207}, {"id": 419, "seek": 258844, "start": 2604.76, "end": 2611.4, "text": " So backpropagation sounds pretty fancy, but it's actually just using the chain rule.", "tokens": [51180, 407, 646, 79, 1513, 559, 399, 3263, 1238, 10247, 11, 457, 309, 311, 767, 445, 1228, 264, 5021, 4978, 13, 51512], "temperature": 0.0, "avg_logprob": -0.17891574487453554, "compression_ratio": 1.6170212765957446, "no_speech_prob": 0.0015487397322431207}, {"id": 420, "seek": 261140, "start": 2612.2000000000003, "end": 2619.7200000000003, "text": " Gosh, I didn't spell that very well. Prop-agation. It's just using the chain rule. And as you'll see,", "tokens": [50404, 19185, 11, 286, 994, 380, 9827, 300, 588, 731, 13, 21944, 12, 559, 399, 13, 467, 311, 445, 1228, 264, 5021, 4978, 13, 400, 382, 291, 603, 536, 11, 50780], "temperature": 0.0, "avg_logprob": -0.2268216986405222, "compression_ratio": 1.548780487804878, "no_speech_prob": 0.0014550480991601944}, {"id": 421, "seek": 261140, "start": 2619.7200000000003, "end": 2623.8, "text": " it's also just taking advantage of a computational trick of memorizing some things on the way.", "tokens": [50780, 309, 311, 611, 445, 1940, 5002, 295, 257, 28270, 4282, 295, 10560, 3319, 512, 721, 322, 264, 636, 13, 50984], "temperature": 0.0, "avg_logprob": -0.2268216986405222, "compression_ratio": 1.548780487804878, "no_speech_prob": 0.0014550480991601944}, {"id": 422, "seek": 261140, "start": 2625.64, "end": 2633.32, "text": " And in our chat, Siva made a very good point about understanding nonlinear functions", "tokens": [51076, 400, 294, 527, 5081, 11, 318, 5931, 1027, 257, 588, 665, 935, 466, 3701, 2107, 28263, 6828, 51460], "temperature": 0.0, "avg_logprob": -0.2268216986405222, "compression_ratio": 1.548780487804878, "no_speech_prob": 0.0014550480991601944}, {"id": 423, "seek": 261140, "start": 2634.28, "end": 2640.44, "text": " in this case, which is just to consider that the wheels could be growing and shrinking all the time", "tokens": [51508, 294, 341, 1389, 11, 597, 307, 445, 281, 1949, 300, 264, 10046, 727, 312, 4194, 293, 41684, 439, 264, 565, 51816], "temperature": 0.0, "avg_logprob": -0.2268216986405222, "compression_ratio": 1.548780487804878, "no_speech_prob": 0.0014550480991601944}, {"id": 424, "seek": 264044, "start": 2640.44, "end": 2644.44, "text": " as they're moving. But you're still going to have the same compound effect,", "tokens": [50364, 382, 436, 434, 2684, 13, 583, 291, 434, 920, 516, 281, 362, 264, 912, 14154, 1802, 11, 50564], "temperature": 0.0, "avg_logprob": -0.21009407403334132, "compression_ratio": 1.5903614457831325, "no_speech_prob": 0.00016093019803520292}, {"id": 425, "seek": 264044, "start": 2644.44, "end": 2646.04, "text": " which I really like that. Thank you, Siva.", "tokens": [50564, 597, 286, 534, 411, 300, 13, 1044, 291, 11, 318, 5931, 13, 50644], "temperature": 0.0, "avg_logprob": -0.21009407403334132, "compression_ratio": 1.5903614457831325, "no_speech_prob": 0.00016093019803520292}, {"id": 426, "seek": 264044, "start": 2652.52, "end": 2659.0, "text": " There's also a question in the chat about why is this colon comma zero being placed in the function,", "tokens": [50968, 821, 311, 611, 257, 1168, 294, 264, 5081, 466, 983, 307, 341, 8255, 22117, 4018, 885, 7074, 294, 264, 2445, 11, 51292], "temperature": 0.0, "avg_logprob": -0.21009407403334132, "compression_ratio": 1.5903614457831325, "no_speech_prob": 0.00016093019803520292}, {"id": 427, "seek": 264044, "start": 2659.0, "end": 2663.32, "text": " given that we can do it outside the function? Well, the point is we want an MSE function", "tokens": [51292, 2212, 300, 321, 393, 360, 309, 2380, 264, 2445, 30, 1042, 11, 264, 935, 307, 321, 528, 364, 376, 5879, 2445, 51508], "temperature": 0.0, "avg_logprob": -0.21009407403334132, "compression_ratio": 1.5903614457831325, "no_speech_prob": 0.00016093019803520292}, {"id": 428, "seek": 264044, "start": 2663.96, "end": 2668.84, "text": " that will apply to any output. We're not using it once. We want it to work any time. So", "tokens": [51540, 300, 486, 3079, 281, 604, 5598, 13, 492, 434, 406, 1228, 309, 1564, 13, 492, 528, 309, 281, 589, 604, 565, 13, 407, 51784], "temperature": 0.0, "avg_logprob": -0.21009407403334132, "compression_ratio": 1.5903614457831325, "no_speech_prob": 0.00016093019803520292}, {"id": 429, "seek": 266884, "start": 2669.8, "end": 2681.08, "text": " we haven't actually modified preds or anything like that, or ytrain. So we want this to be able", "tokens": [50412, 321, 2378, 380, 767, 15873, 3852, 82, 420, 1340, 411, 300, 11, 420, 288, 83, 7146, 13, 407, 321, 528, 341, 281, 312, 1075, 50976], "temperature": 0.0, "avg_logprob": -0.2612862141927083, "compression_ratio": 1.5141242937853108, "no_speech_prob": 9.666029654908925e-06}, {"id": 430, "seek": 266884, "start": 2681.08, "end": 2685.8, "text": " to apply to anything without us having to pre-process it, is basically the idea here.", "tokens": [50976, 281, 3079, 281, 1340, 1553, 505, 1419, 281, 659, 12, 41075, 309, 11, 307, 1936, 264, 1558, 510, 13, 51212], "temperature": 0.0, "avg_logprob": -0.2612862141927083, "compression_ratio": 1.5141242937853108, "no_speech_prob": 9.666029654908925e-06}, {"id": 431, "seek": 266884, "start": 2687.96, "end": 2697.4, "text": " OK. So let's take a look at the basic idea. So here's going to do a forward pass and a", "tokens": [51320, 2264, 13, 407, 718, 311, 747, 257, 574, 412, 264, 3875, 1558, 13, 407, 510, 311, 516, 281, 360, 257, 2128, 1320, 293, 257, 51792], "temperature": 0.0, "avg_logprob": -0.2612862141927083, "compression_ratio": 1.5141242937853108, "no_speech_prob": 9.666029654908925e-06}, {"id": 432, "seek": 269740, "start": 2697.4, "end": 2703.2400000000002, "text": " backward pass. So the forward pass is where we calculate the loss. So the loss is", "tokens": [50364, 23897, 1320, 13, 407, 264, 2128, 1320, 307, 689, 321, 8873, 264, 4470, 13, 407, 264, 4470, 307, 50656], "temperature": 0.0, "avg_logprob": -0.22841866162358498, "compression_ratio": 1.4912280701754386, "no_speech_prob": 0.0005274760769680142}, {"id": 433, "seek": 269740, "start": 2705.88, "end": 2717.32, "text": " oh, I've got an error here. That should be diff. There we go. So the loss is going to be", "tokens": [50788, 1954, 11, 286, 600, 658, 364, 6713, 510, 13, 663, 820, 312, 7593, 13, 821, 321, 352, 13, 407, 264, 4470, 307, 516, 281, 312, 51360], "temperature": 0.0, "avg_logprob": -0.22841866162358498, "compression_ratio": 1.4912280701754386, "no_speech_prob": 0.0005274760769680142}, {"id": 434, "seek": 271732, "start": 2718.28, "end": 2725.2400000000002, "text": " the output of our neural net minus our target squared, and then take the mean.", "tokens": [50412, 264, 5598, 295, 527, 18161, 2533, 3175, 527, 3779, 8889, 11, 293, 550, 747, 264, 914, 13, 50760], "temperature": 0.0, "avg_logprob": -0.3020635767185942, "compression_ratio": 1.898876404494382, "no_speech_prob": 0.017985468730330467}, {"id": 435, "seek": 271732, "start": 2727.7200000000003, "end": 2732.6000000000004, "text": " OK. And then our output is going to be the output of the second linear layer.", "tokens": [50884, 2264, 13, 400, 550, 527, 5598, 307, 516, 281, 312, 264, 5598, 295, 264, 1150, 8213, 4583, 13, 51128], "temperature": 0.0, "avg_logprob": -0.3020635767185942, "compression_ratio": 1.898876404494382, "no_speech_prob": 0.017985468730330467}, {"id": 436, "seek": 271732, "start": 2736.1200000000003, "end": 2740.84, "text": " The second linear layer's input will be the ReLU. The ReLU's input will be the first layer.", "tokens": [51304, 440, 1150, 8213, 4583, 311, 4846, 486, 312, 264, 1300, 43, 52, 13, 440, 1300, 43, 52, 311, 4846, 486, 312, 264, 700, 4583, 13, 51540], "temperature": 0.0, "avg_logprob": -0.3020635767185942, "compression_ratio": 1.898876404494382, "no_speech_prob": 0.017985468730330467}, {"id": 437, "seek": 271732, "start": 2740.84, "end": 2744.1200000000003, "text": " So we're going to take our input, put it through a linear layer, put that through a ReLU,", "tokens": [51540, 407, 321, 434, 516, 281, 747, 527, 4846, 11, 829, 309, 807, 257, 8213, 4583, 11, 829, 300, 807, 257, 1300, 43, 52, 11, 51704], "temperature": 0.0, "avg_logprob": -0.3020635767185942, "compression_ratio": 1.898876404494382, "no_speech_prob": 0.017985468730330467}, {"id": 438, "seek": 274412, "start": 2744.3599999999997, "end": 2748.7599999999998, "text": " put that through a linear layer, and calculate the MSE.", "tokens": [50376, 829, 300, 807, 257, 8213, 4583, 11, 293, 8873, 264, 376, 5879, 13, 50596], "temperature": 0.0, "avg_logprob": -0.20182155740672145, "compression_ratio": 1.6213592233009708, "no_speech_prob": 0.0012255457695573568}, {"id": 439, "seek": 274412, "start": 2752.04, "end": 2756.04, "text": " OK. That bit hopefully is pretty straightforward. So what about the backward pass?", "tokens": [50760, 2264, 13, 663, 857, 4696, 307, 1238, 15325, 13, 407, 437, 466, 264, 23897, 1320, 30, 50960], "temperature": 0.0, "avg_logprob": -0.20182155740672145, "compression_ratio": 1.6213592233009708, "no_speech_prob": 0.0012255457695573568}, {"id": 440, "seek": 274412, "start": 2757.56, "end": 2764.52, "text": " So the backward pass, what I'm going to do, and you'll see why in a moment, is I'm going to store", "tokens": [51036, 407, 264, 23897, 1320, 11, 437, 286, 478, 516, 281, 360, 11, 293, 291, 603, 536, 983, 294, 257, 1623, 11, 307, 286, 478, 516, 281, 3531, 51384], "temperature": 0.0, "avg_logprob": -0.20182155740672145, "compression_ratio": 1.6213592233009708, "no_speech_prob": 0.0012255457695573568}, {"id": 441, "seek": 274412, "start": 2764.52, "end": 2773.3199999999997, "text": " the gradients of each layer. So for example, the gradients of the loss with respect to its inputs", "tokens": [51384, 264, 2771, 2448, 295, 1184, 4583, 13, 407, 337, 1365, 11, 264, 2771, 2448, 295, 264, 4470, 365, 3104, 281, 1080, 15743, 51824], "temperature": 0.0, "avg_logprob": -0.20182155740672145, "compression_ratio": 1.6213592233009708, "no_speech_prob": 0.0012255457695573568}, {"id": 442, "seek": 277412, "start": 2774.52, "end": 2780.7599999999998, "text": " are in the layer itself. So I'm going to create a new attribute. I could call it anything I like.", "tokens": [50384, 366, 294, 264, 4583, 2564, 13, 407, 286, 478, 516, 281, 1884, 257, 777, 19667, 13, 286, 727, 818, 309, 1340, 286, 411, 13, 50696], "temperature": 0.0, "avg_logprob": -0.20952705383300782, "compression_ratio": 1.8634538152610443, "no_speech_prob": 3.426846888032742e-05}, {"id": 443, "seek": 277412, "start": 2780.7599999999998, "end": 2786.8399999999997, "text": " I'm just going to call it dot g. So I'm going to create a new layer, a new attribute called out.g,", "tokens": [50696, 286, 478, 445, 516, 281, 818, 309, 5893, 290, 13, 407, 286, 478, 516, 281, 1884, 257, 777, 4583, 11, 257, 777, 19667, 1219, 484, 13, 70, 11, 51000], "temperature": 0.0, "avg_logprob": -0.20952705383300782, "compression_ratio": 1.8634538152610443, "no_speech_prob": 3.426846888032742e-05}, {"id": 444, "seek": 277412, "start": 2786.8399999999997, "end": 2791.08, "text": " which is going to contain the gradients. You don't have to do it this way, but as you'll see,", "tokens": [51000, 597, 307, 516, 281, 5304, 264, 2771, 2448, 13, 509, 500, 380, 362, 281, 360, 309, 341, 636, 11, 457, 382, 291, 603, 536, 11, 51212], "temperature": 0.0, "avg_logprob": -0.20952705383300782, "compression_ratio": 1.8634538152610443, "no_speech_prob": 3.426846888032742e-05}, {"id": 445, "seek": 277412, "start": 2791.08, "end": 2796.7599999999998, "text": " it turns out pretty convenient. So that's just going to be two times the difference,", "tokens": [51212, 309, 4523, 484, 1238, 10851, 13, 407, 300, 311, 445, 516, 281, 312, 732, 1413, 264, 2649, 11, 51496], "temperature": 0.0, "avg_logprob": -0.20952705383300782, "compression_ratio": 1.8634538152610443, "no_speech_prob": 3.426846888032742e-05}, {"id": 446, "seek": 277412, "start": 2796.7599999999998, "end": 2802.04, "text": " because we've got difference squared. All right. So that's just the derivative. And then", "tokens": [51496, 570, 321, 600, 658, 2649, 8889, 13, 1057, 558, 13, 407, 300, 311, 445, 264, 13760, 13, 400, 550, 51760], "temperature": 0.0, "avg_logprob": -0.20952705383300782, "compression_ratio": 1.8634538152610443, "no_speech_prob": 3.426846888032742e-05}, {"id": 447, "seek": 280412, "start": 2805.0, "end": 2812.7599999999998, "text": " we have taken the mean here, so we have to do the same thing here, divided by the input shape.", "tokens": [50408, 321, 362, 2726, 264, 914, 510, 11, 370, 321, 362, 281, 360, 264, 912, 551, 510, 11, 6666, 538, 264, 4846, 3909, 13, 50796], "temperature": 0.0, "avg_logprob": -0.20801392084435572, "compression_ratio": 1.7696969696969698, "no_speech_prob": 4.092909875907935e-06}, {"id": 448, "seek": 280412, "start": 2815.08, "end": 2821.96, "text": " And so that's those gradients. That's good. And now what we need to do is multiply by the gradients", "tokens": [50912, 400, 370, 300, 311, 729, 2771, 2448, 13, 663, 311, 665, 13, 400, 586, 437, 321, 643, 281, 360, 307, 12972, 538, 264, 2771, 2448, 51256], "temperature": 0.0, "avg_logprob": -0.20801392084435572, "compression_ratio": 1.7696969696969698, "no_speech_prob": 4.092909875907935e-06}, {"id": 449, "seek": 280412, "start": 2821.96, "end": 2827.0, "text": " of the previous layer. So here's the previous layer. So what are the gradients of a linear layer?", "tokens": [51256, 295, 264, 3894, 4583, 13, 407, 510, 311, 264, 3894, 4583, 13, 407, 437, 366, 264, 2771, 2448, 295, 257, 8213, 4583, 30, 51508], "temperature": 0.0, "avg_logprob": -0.20801392084435572, "compression_ratio": 1.7696969696969698, "no_speech_prob": 4.092909875907935e-06}, {"id": 450, "seek": 282700, "start": 2827.0, "end": 2835.16, "text": " I've created a function for that here. So the gradient of a linear layer, we're going to need", "tokens": [50364, 286, 600, 2942, 257, 2445, 337, 300, 510, 13, 407, 264, 16235, 295, 257, 8213, 4583, 11, 321, 434, 516, 281, 643, 50772], "temperature": 0.0, "avg_logprob": -0.18691085732501486, "compression_ratio": 1.989071038251366, "no_speech_prob": 3.763638233067468e-05}, {"id": 451, "seek": 282700, "start": 2835.16, "end": 2841.8, "text": " to know the weights of the layer. We're going to need to know the biases of the layer. And then", "tokens": [50772, 281, 458, 264, 17443, 295, 264, 4583, 13, 492, 434, 516, 281, 643, 281, 458, 264, 32152, 295, 264, 4583, 13, 400, 550, 51104], "temperature": 0.0, "avg_logprob": -0.18691085732501486, "compression_ratio": 1.989071038251366, "no_speech_prob": 3.763638233067468e-05}, {"id": 452, "seek": 282700, "start": 2841.8, "end": 2848.68, "text": " we're also going to know the input to the linear layer, because that's the thing that's actually", "tokens": [51104, 321, 434, 611, 516, 281, 458, 264, 4846, 281, 264, 8213, 4583, 11, 570, 300, 311, 264, 551, 300, 311, 767, 51448], "temperature": 0.0, "avg_logprob": -0.18691085732501486, "compression_ratio": 1.989071038251366, "no_speech_prob": 3.763638233067468e-05}, {"id": 453, "seek": 282700, "start": 2849.4, "end": 2854.92, "text": " being manipulated here. And then we're also going to need the output, because", "tokens": [51484, 885, 37161, 510, 13, 400, 550, 321, 434, 611, 516, 281, 643, 264, 5598, 11, 570, 51760], "temperature": 0.0, "avg_logprob": -0.18691085732501486, "compression_ratio": 1.989071038251366, "no_speech_prob": 3.763638233067468e-05}, {"id": 454, "seek": 285492, "start": 2855.2400000000002, "end": 2858.76, "text": " we have to multiply by the gradients, because we've got the chain rule.", "tokens": [50380, 321, 362, 281, 12972, 538, 264, 2771, 2448, 11, 570, 321, 600, 658, 264, 5021, 4978, 13, 50556], "temperature": 0.0, "avg_logprob": -0.24318916733200485, "compression_ratio": 1.9515418502202644, "no_speech_prob": 7.484584784833714e-05}, {"id": 455, "seek": 285492, "start": 2859.96, "end": 2867.64, "text": " So again, we're going to store the gradients of our input. So this would be the gradients of our", "tokens": [50616, 407, 797, 11, 321, 434, 516, 281, 3531, 264, 2771, 2448, 295, 527, 4846, 13, 407, 341, 576, 312, 264, 2771, 2448, 295, 527, 51000], "temperature": 0.0, "avg_logprob": -0.24318916733200485, "compression_ratio": 1.9515418502202644, "no_speech_prob": 7.484584784833714e-05}, {"id": 456, "seek": 285492, "start": 2867.64, "end": 2873.56, "text": " output with respect to the input. And that's simply the weights, because the weights... So", "tokens": [51000, 5598, 365, 3104, 281, 264, 4846, 13, 400, 300, 311, 2935, 264, 17443, 11, 570, 264, 17443, 485, 407, 51296], "temperature": 0.0, "avg_logprob": -0.24318916733200485, "compression_ratio": 1.9515418502202644, "no_speech_prob": 7.484584784833714e-05}, {"id": 457, "seek": 285492, "start": 2873.56, "end": 2878.52, "text": " a matrix multiplier is just a whole bunch of linear functions. So each one's slope is just", "tokens": [51296, 257, 8141, 44106, 307, 445, 257, 1379, 3840, 295, 8213, 6828, 13, 407, 1184, 472, 311, 13525, 307, 445, 51544], "temperature": 0.0, "avg_logprob": -0.24318916733200485, "compression_ratio": 1.9515418502202644, "no_speech_prob": 7.484584784833714e-05}, {"id": 458, "seek": 285492, "start": 2878.52, "end": 2884.36, "text": " its weight. But you have to multiply it by the gradient of the outputs, because of the chain", "tokens": [51544, 1080, 3364, 13, 583, 291, 362, 281, 12972, 309, 538, 264, 16235, 295, 264, 23930, 11, 570, 295, 264, 5021, 51836], "temperature": 0.0, "avg_logprob": -0.24318916733200485, "compression_ratio": 1.9515418502202644, "no_speech_prob": 7.484584784833714e-05}, {"id": 459, "seek": 288436, "start": 2884.36, "end": 2891.7200000000003, "text": " rule. And then the gradient of the outputs with respect to the weights is going to be the", "tokens": [50364, 4978, 13, 400, 550, 264, 16235, 295, 264, 23930, 365, 3104, 281, 264, 17443, 307, 516, 281, 312, 264, 50732], "temperature": 0.0, "avg_logprob": -0.1854649205361643, "compression_ratio": 1.559748427672956, "no_speech_prob": 0.0001177440135506913}, {"id": 460, "seek": 288436, "start": 2894.36, "end": 2901.1600000000003, "text": " input times the output summed up. I'll talk more about that in a moment.", "tokens": [50864, 4846, 1413, 264, 5598, 2408, 1912, 493, 13, 286, 603, 751, 544, 466, 300, 294, 257, 1623, 13, 51204], "temperature": 0.0, "avg_logprob": -0.1854649205361643, "compression_ratio": 1.559748427672956, "no_speech_prob": 0.0001177440135506913}, {"id": 461, "seek": 288436, "start": 2902.28, "end": 2909.88, "text": " The derivatives of the bias is very straightforward. It's the gradients of the output", "tokens": [51260, 440, 33733, 295, 264, 12577, 307, 588, 15325, 13, 467, 311, 264, 2771, 2448, 295, 264, 5598, 51640], "temperature": 0.0, "avg_logprob": -0.1854649205361643, "compression_ratio": 1.559748427672956, "no_speech_prob": 0.0001177440135506913}, {"id": 462, "seek": 290988, "start": 2910.76, "end": 2918.04, "text": " and the output added together, because the bias is just a constant value. So for the chain rule,", "tokens": [50408, 293, 264, 5598, 3869, 1214, 11, 570, 264, 12577, 307, 445, 257, 5754, 2158, 13, 407, 337, 264, 5021, 4978, 11, 50772], "temperature": 0.0, "avg_logprob": -0.23548612286967616, "compression_ratio": 1.796116504854369, "no_speech_prob": 0.00017674453556537628}, {"id": 463, "seek": 290988, "start": 2918.04, "end": 2926.6800000000003, "text": " we simply just use output times one, which is output. So for this one here, again, we have to", "tokens": [50772, 321, 2935, 445, 764, 5598, 1413, 472, 11, 597, 307, 5598, 13, 407, 337, 341, 472, 510, 11, 797, 11, 321, 362, 281, 51204], "temperature": 0.0, "avg_logprob": -0.23548612286967616, "compression_ratio": 1.796116504854369, "no_speech_prob": 0.00017674453556537628}, {"id": 464, "seek": 290988, "start": 2927.2400000000002, "end": 2930.76, "text": " do the same thing we've been doing before, which is multiply by the output gradients,", "tokens": [51232, 360, 264, 912, 551, 321, 600, 668, 884, 949, 11, 597, 307, 12972, 538, 264, 5598, 2771, 2448, 11, 51408], "temperature": 0.0, "avg_logprob": -0.23548612286967616, "compression_ratio": 1.796116504854369, "no_speech_prob": 0.00017674453556537628}, {"id": 465, "seek": 290988, "start": 2930.76, "end": 2939.6400000000003, "text": " because of the chain rule. And then we've got the input weights. So every single one of those", "tokens": [51408, 570, 295, 264, 5021, 4978, 13, 400, 550, 321, 600, 658, 264, 4846, 17443, 13, 407, 633, 2167, 472, 295, 729, 51852], "temperature": 0.0, "avg_logprob": -0.23548612286967616, "compression_ratio": 1.796116504854369, "no_speech_prob": 0.00017674453556537628}, {"id": 466, "seek": 293988, "start": 2939.96, "end": 2947.6400000000003, "text": " has to be multiplied by the outputs. And so that's why we have to do an unsqueeze minus one.", "tokens": [50368, 575, 281, 312, 17207, 538, 264, 23930, 13, 400, 370, 300, 311, 983, 321, 362, 281, 360, 364, 2693, 1077, 10670, 3175, 472, 13, 50752], "temperature": 0.0, "avg_logprob": -0.16429539635067894, "compression_ratio": 1.6623931623931625, "no_speech_prob": 4.0063459891825914e-05}, {"id": 467, "seek": 293988, "start": 2948.36, "end": 2955.96, "text": " So what I'm going to do now is I'm going to show you how I would experiment with this code in order", "tokens": [50788, 407, 437, 286, 478, 516, 281, 360, 586, 307, 286, 478, 516, 281, 855, 291, 577, 286, 576, 5120, 365, 341, 3089, 294, 1668, 51168], "temperature": 0.0, "avg_logprob": -0.16429539635067894, "compression_ratio": 1.6623931623931625, "no_speech_prob": 4.0063459891825914e-05}, {"id": 468, "seek": 293988, "start": 2955.96, "end": 2961.08, "text": " to understand it. And I would encourage you to do the same thing. It's a little harder to do this", "tokens": [51168, 281, 1223, 309, 13, 400, 286, 576, 5373, 291, 281, 360, 264, 912, 551, 13, 467, 311, 257, 707, 6081, 281, 360, 341, 51424], "temperature": 0.0, "avg_logprob": -0.16429539635067894, "compression_ratio": 1.6623931623931625, "no_speech_prob": 4.0063459891825914e-05}, {"id": 469, "seek": 293988, "start": 2961.08, "end": 2966.28, "text": " one cell by cell, because we kind of want to put it all into this function like this. So we need a", "tokens": [51424, 472, 2815, 538, 2815, 11, 570, 321, 733, 295, 528, 281, 829, 309, 439, 666, 341, 2445, 411, 341, 13, 407, 321, 643, 257, 51684], "temperature": 0.0, "avg_logprob": -0.16429539635067894, "compression_ratio": 1.6623931623931625, "no_speech_prob": 4.0063459891825914e-05}, {"id": 470, "seek": 296628, "start": 2966.28, "end": 2973.88, "text": " way to explore the calculations interactively. And the way we do that is by using the Python", "tokens": [50364, 636, 281, 6839, 264, 20448, 4648, 3413, 13, 400, 264, 636, 321, 360, 300, 307, 538, 1228, 264, 15329, 50744], "temperature": 0.0, "avg_logprob": -0.18556606622389804, "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.0007321737939491868}, {"id": 471, "seek": 296628, "start": 2973.88, "end": 2981.8, "text": " debugger. Here is how you, let me see a few ways to do this. Here's one way to use the Python", "tokens": [50744, 24083, 1321, 13, 1692, 307, 577, 291, 11, 718, 385, 536, 257, 1326, 2098, 281, 360, 341, 13, 1692, 311, 472, 636, 281, 764, 264, 15329, 51140], "temperature": 0.0, "avg_logprob": -0.18556606622389804, "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.0007321737939491868}, {"id": 472, "seek": 296628, "start": 2981.8, "end": 2993.0800000000004, "text": " debugger. The Python debugger is called PDB. So if you say PDB.setTrace in your code, then that", "tokens": [51140, 24083, 1321, 13, 440, 15329, 24083, 1321, 307, 1219, 10464, 33, 13, 407, 498, 291, 584, 10464, 33, 13, 3854, 14252, 617, 294, 428, 3089, 11, 550, 300, 51704], "temperature": 0.0, "avg_logprob": -0.18556606622389804, "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.0007321737939491868}, {"id": 473, "seek": 299308, "start": 2993.08, "end": 2997.7999999999997, "text": " tells the debugger to stop execution when it reaches this line. So it sets a break point.", "tokens": [50364, 5112, 264, 24083, 1321, 281, 1590, 15058, 562, 309, 14235, 341, 1622, 13, 407, 309, 6352, 257, 1821, 935, 13, 50600], "temperature": 0.0, "avg_logprob": -0.1975155822501695, "compression_ratio": 1.6750902527075813, "no_speech_prob": 0.0012255426263436675}, {"id": 474, "seek": 299308, "start": 2998.44, "end": 3004.68, "text": " So if I call forward and backward, you can see here it's stopped. And the interactive Python", "tokens": [50632, 407, 498, 286, 818, 2128, 293, 23897, 11, 291, 393, 536, 510, 309, 311, 5936, 13, 400, 264, 15141, 15329, 50944], "temperature": 0.0, "avg_logprob": -0.1975155822501695, "compression_ratio": 1.6750902527075813, "no_speech_prob": 0.0012255426263436675}, {"id": 475, "seek": 299308, "start": 3004.68, "end": 3010.36, "text": " debugger, IPDB, has popped up. With an arrow pointing at the line of code it's about to run.", "tokens": [50944, 24083, 1321, 11, 8671, 27735, 11, 575, 21545, 493, 13, 2022, 364, 11610, 12166, 412, 264, 1622, 295, 3089, 309, 311, 466, 281, 1190, 13, 51228], "temperature": 0.0, "avg_logprob": -0.1975155822501695, "compression_ratio": 1.6750902527075813, "no_speech_prob": 0.0012255426263436675}, {"id": 476, "seek": 299308, "start": 3011.56, "end": 3015.3199999999997, "text": " And at this point, there's a whole range of things we can do to find out what they are. We hit H", "tokens": [51288, 400, 412, 341, 935, 11, 456, 311, 257, 1379, 3613, 295, 721, 321, 393, 360, 281, 915, 484, 437, 436, 366, 13, 492, 2045, 389, 51476], "temperature": 0.0, "avg_logprob": -0.1975155822501695, "compression_ratio": 1.6750902527075813, "no_speech_prob": 0.0012255426263436675}, {"id": 477, "seek": 299308, "start": 3016.36, "end": 3022.2, "text": " for help. Understanding how to use the Python debugger is one of the most powerful things I", "tokens": [51528, 337, 854, 13, 36858, 577, 281, 764, 264, 15329, 24083, 1321, 307, 472, 295, 264, 881, 4005, 721, 286, 51820], "temperature": 0.0, "avg_logprob": -0.1975155822501695, "compression_ratio": 1.6750902527075813, "no_speech_prob": 0.0012255426263436675}, {"id": 478, "seek": 302220, "start": 3022.2, "end": 3030.7599999999998, "text": " think you can do to improve your coding. So one of the most useful things you can do is to print", "tokens": [50364, 519, 291, 393, 360, 281, 3470, 428, 17720, 13, 407, 472, 295, 264, 881, 4420, 721, 291, 393, 360, 307, 281, 4482, 50792], "temperature": 0.0, "avg_logprob": -0.21810406977587407, "compression_ratio": 1.6244541484716157, "no_speech_prob": 9.610158303985372e-05}, {"id": 479, "seek": 302220, "start": 3030.7599999999998, "end": 3034.68, "text": " something. You see all these single letter things? They're just shortcuts. But in a debugger,", "tokens": [50792, 746, 13, 509, 536, 439, 613, 2167, 5063, 721, 30, 814, 434, 445, 34620, 13, 583, 294, 257, 24083, 1321, 11, 50988], "temperature": 0.0, "avg_logprob": -0.21810406977587407, "compression_ratio": 1.6244541484716157, "no_speech_prob": 9.610158303985372e-05}, {"id": 480, "seek": 302220, "start": 3034.68, "end": 3037.96, "text": " you want to be able to do things quickly. So instead of typing print, I just type P.", "tokens": [50988, 291, 528, 281, 312, 1075, 281, 360, 721, 2661, 13, 407, 2602, 295, 18444, 4482, 11, 286, 445, 2010, 430, 13, 51152], "temperature": 0.0, "avg_logprob": -0.21810406977587407, "compression_ratio": 1.6244541484716157, "no_speech_prob": 9.610158303985372e-05}, {"id": 481, "seek": 302220, "start": 3038.9199999999996, "end": 3048.9199999999996, "text": " So for example, let's take a look at the shape of the input. So I type P for print, input.shape.", "tokens": [51200, 407, 337, 1365, 11, 718, 311, 747, 257, 574, 412, 264, 3909, 295, 264, 4846, 13, 407, 286, 2010, 430, 337, 4482, 11, 4846, 13, 82, 42406, 13, 51700], "temperature": 0.0, "avg_logprob": -0.21810406977587407, "compression_ratio": 1.6244541484716157, "no_speech_prob": 9.610158303985372e-05}, {"id": 482, "seek": 305220, "start": 3053.16, "end": 3058.12, "text": " So I've got a 50,000 by 50 input to the last layer. That makes sense. These are the hidden", "tokens": [50412, 407, 286, 600, 658, 257, 2625, 11, 1360, 538, 2625, 4846, 281, 264, 1036, 4583, 13, 663, 1669, 2020, 13, 1981, 366, 264, 7633, 50660], "temperature": 0.0, "avg_logprob": -0.2621609576336749, "compression_ratio": 1.4545454545454546, "no_speech_prob": 8.614623948233202e-05}, {"id": 483, "seek": 305220, "start": 3058.12, "end": 3065.56, "text": " activations coming into the last layer for every one of our images. What about the output gradients?", "tokens": [50660, 2430, 763, 1348, 666, 264, 1036, 4583, 337, 633, 472, 295, 527, 5267, 13, 708, 466, 264, 5598, 2771, 2448, 30, 51032], "temperature": 0.0, "avg_logprob": -0.2621609576336749, "compression_ratio": 1.4545454545454546, "no_speech_prob": 8.614623948233202e-05}, {"id": 484, "seek": 305220, "start": 3071.8799999999997, "end": 3077.16, "text": " And there's that as well. And actually, a little trick. You can ignore the you don't have to use", "tokens": [51348, 400, 456, 311, 300, 382, 731, 13, 400, 767, 11, 257, 707, 4282, 13, 509, 393, 11200, 264, 291, 500, 380, 362, 281, 764, 51612], "temperature": 0.0, "avg_logprob": -0.2621609576336749, "compression_ratio": 1.4545454545454546, "no_speech_prob": 8.614623948233202e-05}, {"id": 485, "seek": 307716, "start": 3077.16, "end": 3084.92, "text": " the P at all if your variable name is not the same as any of these commands. So I could have", "tokens": [50364, 264, 430, 412, 439, 498, 428, 7006, 1315, 307, 406, 264, 912, 382, 604, 295, 613, 16901, 13, 407, 286, 727, 362, 50752], "temperature": 0.0, "avg_logprob": -0.18985467690687913, "compression_ratio": 1.3233082706766917, "no_speech_prob": 0.0028448719531297684}, {"id": 486, "seek": 307716, "start": 3084.92, "end": 3098.04, "text": " just typed out.g.shape. Get the same thing. OK. So you can also put in expressions.", "tokens": [50752, 445, 33941, 484, 13, 70, 13, 82, 42406, 13, 3240, 264, 912, 551, 13, 2264, 13, 407, 291, 393, 611, 829, 294, 15277, 13, 51408], "temperature": 0.0, "avg_logprob": -0.18985467690687913, "compression_ratio": 1.3233082706766917, "no_speech_prob": 0.0028448719531297684}, {"id": 487, "seek": 309804, "start": 3099.0, "end": 3100.7599999999998, "text": " So let's have a look at the shape of this.", "tokens": [50412, 407, 718, 311, 362, 257, 574, 412, 264, 3909, 295, 341, 13, 50500], "temperature": 0.0, "avg_logprob": -0.28364000822368424, "compression_ratio": 1.6096256684491979, "no_speech_prob": 0.006903705187141895}, {"id": 488, "seek": 309804, "start": 3105.88, "end": 3111.4, "text": " So the output of this is let's see if it makes sense. We've got the input, 50,000 by 50.", "tokens": [50756, 407, 264, 5598, 295, 341, 307, 718, 311, 536, 498, 309, 1669, 2020, 13, 492, 600, 658, 264, 4846, 11, 2625, 11, 1360, 538, 2625, 13, 51032], "temperature": 0.0, "avg_logprob": -0.28364000822368424, "compression_ratio": 1.6096256684491979, "no_speech_prob": 0.006903705187141895}, {"id": 489, "seek": 309804, "start": 3111.96, "end": 3118.12, "text": " We put a new axis on the end. Unsqueeze minus one is the same as doing dot as indexing it with dot,", "tokens": [51060, 492, 829, 257, 777, 10298, 322, 264, 917, 13, 25017, 1077, 10670, 3175, 472, 307, 264, 912, 382, 884, 5893, 382, 8186, 278, 309, 365, 5893, 11, 51368], "temperature": 0.0, "avg_logprob": -0.28364000822368424, "compression_ratio": 1.6096256684491979, "no_speech_prob": 0.006903705187141895}, {"id": 490, "seek": 309804, "start": 3118.12, "end": 3126.7599999999998, "text": " dot, dot, comma, none. So that would have become 50,000 by 50 by one.", "tokens": [51368, 5893, 11, 5893, 11, 22117, 11, 6022, 13, 407, 300, 576, 362, 1813, 2625, 11, 1360, 538, 2625, 538, 472, 13, 51800], "temperature": 0.0, "avg_logprob": -0.28364000822368424, "compression_ratio": 1.6096256684491979, "no_speech_prob": 0.006903705187141895}, {"id": 491, "seek": 312804, "start": 3128.52, "end": 3135.24, "text": " And then the out g.unsqueeze we're putting in the first dimension. So we're going to have 50,000", "tokens": [50388, 400, 550, 264, 484, 290, 13, 409, 44516, 10670, 321, 434, 3372, 294, 264, 700, 10139, 13, 407, 321, 434, 516, 281, 362, 2625, 11, 1360, 50724], "temperature": 0.0, "avg_logprob": -0.2605340569107621, "compression_ratio": 1.7058823529411764, "no_speech_prob": 5.307499668560922e-05}, {"id": 492, "seek": 312804, "start": 3135.24, "end": 3144.04, "text": " by 50 by one times 50,000 by one by one. And so we're going to end up getting this broadcasting", "tokens": [50724, 538, 2625, 538, 472, 1413, 2625, 11, 1360, 538, 472, 538, 472, 13, 400, 370, 321, 434, 516, 281, 917, 493, 1242, 341, 4152, 48860, 51164], "temperature": 0.0, "avg_logprob": -0.2605340569107621, "compression_ratio": 1.7058823529411764, "no_speech_prob": 5.307499668560922e-05}, {"id": 493, "seek": 312804, "start": 3144.04, "end": 3149.64, "text": " happening over these last two dimensions. Which is why we end up with 50,000 by 50 by one.", "tokens": [51164, 2737, 670, 613, 1036, 732, 12819, 13, 3013, 307, 983, 321, 917, 493, 365, 2625, 11, 1360, 538, 2625, 538, 472, 13, 51444], "temperature": 0.0, "avg_logprob": -0.2605340569107621, "compression_ratio": 1.7058823529411764, "no_speech_prob": 5.307499668560922e-05}, {"id": 494, "seek": 312804, "start": 3150.36, "end": 3156.7599999999998, "text": " And then we're summing up. This makes sense, right? We want to sum up over all of the inputs.", "tokens": [51480, 400, 550, 321, 434, 2408, 2810, 493, 13, 639, 1669, 2020, 11, 558, 30, 492, 528, 281, 2408, 493, 670, 439, 295, 264, 15743, 13, 51800], "temperature": 0.0, "avg_logprob": -0.2605340569107621, "compression_ratio": 1.7058823529411764, "no_speech_prob": 5.307499668560922e-05}, {"id": 495, "seek": 315676, "start": 3156.76, "end": 3164.5200000000004, "text": " Each image is individually contributing to the derivative. And so we want to add them all up to", "tokens": [50364, 6947, 3256, 307, 16652, 19270, 281, 264, 13760, 13, 400, 370, 321, 528, 281, 909, 552, 439, 493, 281, 50752], "temperature": 0.0, "avg_logprob": -0.20444077385796441, "compression_ratio": 1.7251184834123223, "no_speech_prob": 9.314564522355795e-05}, {"id": 496, "seek": 315676, "start": 3164.5200000000004, "end": 3168.92, "text": " find their total impact. Because remember the sum of a bunch of the derivative of the sum of", "tokens": [50752, 915, 641, 3217, 2712, 13, 1436, 1604, 264, 2408, 295, 257, 3840, 295, 264, 13760, 295, 264, 2408, 295, 50972], "temperature": 0.0, "avg_logprob": -0.20444077385796441, "compression_ratio": 1.7251184834123223, "no_speech_prob": 9.314564522355795e-05}, {"id": 497, "seek": 315676, "start": 3168.92, "end": 3172.92, "text": " functions is the sum of the derivatives of the functions. So we can just sum them up.", "tokens": [50972, 6828, 307, 264, 2408, 295, 264, 33733, 295, 264, 6828, 13, 407, 321, 393, 445, 2408, 552, 493, 13, 51172], "temperature": 0.0, "avg_logprob": -0.20444077385796441, "compression_ratio": 1.7251184834123223, "no_speech_prob": 9.314564522355795e-05}, {"id": 498, "seek": 315676, "start": 3174.5200000000004, "end": 3178.76, "text": " Now, this is one of these situations where if you see a times and a sum and an unsqueeze,", "tokens": [51252, 823, 11, 341, 307, 472, 295, 613, 6851, 689, 498, 291, 536, 257, 1413, 293, 257, 2408, 293, 364, 2693, 1077, 10670, 11, 51464], "temperature": 0.0, "avg_logprob": -0.20444077385796441, "compression_ratio": 1.7251184834123223, "no_speech_prob": 9.314564522355795e-05}, {"id": 499, "seek": 317876, "start": 3179.32, "end": 3186.6800000000003, "text": " it's not a bad idea to think about Einstein summation notation. Maybe there's a way to", "tokens": [50392, 309, 311, 406, 257, 1578, 1558, 281, 519, 466, 23486, 28811, 24657, 13, 2704, 456, 311, 257, 636, 281, 50760], "temperature": 0.0, "avg_logprob": -0.1783098722759046, "compression_ratio": 1.6283185840707965, "no_speech_prob": 0.0028448502998799086}, {"id": 500, "seek": 317876, "start": 3186.6800000000003, "end": 3195.7200000000003, "text": " simplify this. So first of all, let's just see how we can do some more stuff in the debugger.", "tokens": [50760, 20460, 341, 13, 407, 700, 295, 439, 11, 718, 311, 445, 536, 577, 321, 393, 360, 512, 544, 1507, 294, 264, 24083, 1321, 13, 51212], "temperature": 0.0, "avg_logprob": -0.1783098722759046, "compression_ratio": 1.6283185840707965, "no_speech_prob": 0.0028448502998799086}, {"id": 501, "seek": 317876, "start": 3195.7200000000003, "end": 3201.0, "text": " I'm going to continue. So just continue running. So press C for continue. And it keeps running", "tokens": [51212, 286, 478, 516, 281, 2354, 13, 407, 445, 2354, 2614, 13, 407, 1886, 383, 337, 2354, 13, 400, 309, 5965, 2614, 51476], "temperature": 0.0, "avg_logprob": -0.1783098722759046, "compression_ratio": 1.6283185840707965, "no_speech_prob": 0.0028448502998799086}, {"id": 502, "seek": 317876, "start": 3201.0, "end": 3207.0800000000004, "text": " until it comes back again to the same spot. And the reason we've come to the same spot twice", "tokens": [51476, 1826, 309, 1487, 646, 797, 281, 264, 912, 4008, 13, 400, 264, 1778, 321, 600, 808, 281, 264, 912, 4008, 6091, 51780], "temperature": 0.0, "avg_logprob": -0.1783098722759046, "compression_ratio": 1.6283185840707965, "no_speech_prob": 0.0028448502998799086}, {"id": 503, "seek": 320708, "start": 3207.16, "end": 3213.56, "text": " is because lin grad is called two times. So we would expect that the second time", "tokens": [50368, 307, 570, 22896, 2771, 307, 1219, 732, 1413, 13, 407, 321, 576, 2066, 300, 264, 1150, 565, 50688], "temperature": 0.0, "avg_logprob": -0.23903698391384548, "compression_ratio": 1.4873417721518987, "no_speech_prob": 8.888076263247058e-05}, {"id": 504, "seek": 320708, "start": 3215.08, "end": 3222.7599999999998, "text": " we're going to get a different bunch of inputs and outputs.", "tokens": [50764, 321, 434, 516, 281, 483, 257, 819, 3840, 295, 15743, 293, 23930, 13, 51148], "temperature": 0.0, "avg_logprob": -0.23903698391384548, "compression_ratio": 1.4873417721518987, "no_speech_prob": 8.888076263247058e-05}, {"id": 505, "seek": 320708, "start": 3225.48, "end": 3230.7599999999998, "text": " And so I can print out a tuple of the inputs and output gradient. So now, yeah, so this is the", "tokens": [51284, 400, 370, 286, 393, 4482, 484, 257, 2604, 781, 295, 264, 15743, 293, 5598, 16235, 13, 407, 586, 11, 1338, 11, 370, 341, 307, 264, 51548], "temperature": 0.0, "avg_logprob": -0.23903698391384548, "compression_ratio": 1.4873417721518987, "no_speech_prob": 8.888076263247058e-05}, {"id": 506, "seek": 323076, "start": 3230.76, "end": 3235.96, "text": " first layer. Going into the second layer. So that's exactly what we would expect.", "tokens": [50364, 700, 4583, 13, 10963, 666, 264, 1150, 4583, 13, 407, 300, 311, 2293, 437, 321, 576, 2066, 13, 50624], "temperature": 0.0, "avg_logprob": -0.21161660693940662, "compression_ratio": 1.6488888888888888, "no_speech_prob": 0.0049822512082755566}, {"id": 507, "seek": 323076, "start": 3237.88, "end": 3245.2400000000002, "text": " To find out what called this function, you just type W. W is where am I? And so you can see here,", "tokens": [50720, 1407, 915, 484, 437, 1219, 341, 2445, 11, 291, 445, 2010, 343, 13, 343, 307, 689, 669, 286, 30, 400, 370, 291, 393, 536, 510, 11, 51088], "temperature": 0.0, "avg_logprob": -0.21161660693940662, "compression_ratio": 1.6488888888888888, "no_speech_prob": 0.0049822512082755566}, {"id": 508, "seek": 323076, "start": 3245.2400000000002, "end": 3250.6800000000003, "text": " where am I? Oh, forward and backward was called. See the arrow? That called lin grad the second", "tokens": [51088, 689, 669, 286, 30, 876, 11, 2128, 293, 23897, 390, 1219, 13, 3008, 264, 11610, 30, 663, 1219, 22896, 2771, 264, 1150, 51360], "temperature": 0.0, "avg_logprob": -0.21161660693940662, "compression_ratio": 1.6488888888888888, "no_speech_prob": 0.0049822512082755566}, {"id": 509, "seek": 323076, "start": 3250.6800000000003, "end": 3258.76, "text": " time. And now we're here in W.G equals. If we want to find out what W.G ends up being equal to,", "tokens": [51360, 565, 13, 400, 586, 321, 434, 510, 294, 343, 13, 38, 6915, 13, 759, 321, 528, 281, 915, 484, 437, 343, 13, 38, 5314, 493, 885, 2681, 281, 11, 51764], "temperature": 0.0, "avg_logprob": -0.21161660693940662, "compression_ratio": 1.6488888888888888, "no_speech_prob": 0.0049822512082755566}, {"id": 510, "seek": 325876, "start": 3258.76, "end": 3266.44, "text": " I can press N to say go to the next line. And so now we've moved from line 5 to line 6. So", "tokens": [50364, 286, 393, 1886, 426, 281, 584, 352, 281, 264, 958, 1622, 13, 400, 370, 586, 321, 600, 4259, 490, 1622, 1025, 281, 1622, 1386, 13, 407, 50748], "temperature": 0.0, "avg_logprob": -0.24659531116485595, "compression_ratio": 1.5524861878453038, "no_speech_prob": 0.0001253361551789567}, {"id": 511, "seek": 325876, "start": 3266.44, "end": 3273.1600000000003, "text": " the instruction point is now looking at line 6. So I can print out W.G.shape. And there's the", "tokens": [50748, 264, 10951, 935, 307, 586, 1237, 412, 1622, 1386, 13, 407, 286, 393, 4482, 484, 343, 13, 38, 13, 82, 42406, 13, 400, 456, 311, 264, 51084], "temperature": 0.0, "avg_logprob": -0.24659531116485595, "compression_ratio": 1.5524861878453038, "no_speech_prob": 0.0001253361551789567}, {"id": 512, "seek": 325876, "start": 3273.1600000000003, "end": 3281.96, "text": " shape of our weights. One person on the chat has pointed out that you can use breakpoint instead", "tokens": [51084, 3909, 295, 527, 17443, 13, 1485, 954, 322, 264, 5081, 575, 10932, 484, 300, 291, 393, 764, 1821, 6053, 2602, 51524], "temperature": 0.0, "avg_logprob": -0.24659531116485595, "compression_ratio": 1.5524861878453038, "no_speech_prob": 0.0001253361551789567}, {"id": 513, "seek": 328196, "start": 3281.96, "end": 3291.32, "text": " of this import PDB business. Unfortunately, the breakpoint keyword doesn't currently work", "tokens": [50364, 295, 341, 974, 10464, 33, 1606, 13, 8590, 11, 264, 1821, 6053, 20428, 1177, 380, 4362, 589, 50832], "temperature": 0.0, "avg_logprob": -0.21565791009699256, "compression_ratio": 1.4960629921259843, "no_speech_prob": 0.024052459746599197}, {"id": 514, "seek": 328196, "start": 3291.32, "end": 3297.96, "text": " in Jupyter or in IPython. So we actually can't, sadly. That's why I'm doing it the old fashioned", "tokens": [50832, 294, 22125, 88, 391, 420, 294, 8671, 88, 11943, 13, 407, 321, 767, 393, 380, 11, 22023, 13, 663, 311, 983, 286, 478, 884, 309, 264, 1331, 40646, 51164], "temperature": 0.0, "avg_logprob": -0.21565791009699256, "compression_ratio": 1.4960629921259843, "no_speech_prob": 0.024052459746599197}, {"id": 515, "seek": 328196, "start": 3297.96, "end": 3303.64, "text": " way. So this way, maybe they'll fix the bug at some point. But for now, we have to type all this.", "tokens": [51164, 636, 13, 407, 341, 636, 11, 1310, 436, 603, 3191, 264, 7426, 412, 512, 935, 13, 583, 337, 586, 11, 321, 362, 281, 2010, 439, 341, 13, 51448], "temperature": 0.0, "avg_logprob": -0.21565791009699256, "compression_ratio": 1.4960629921259843, "no_speech_prob": 0.024052459746599197}, {"id": 516, "seek": 328196, "start": 3306.68, "end": 3311.7200000000003, "text": " OK. So those are a few things to know about. But I would definitely suggest looking up a Python", "tokens": [51600, 2264, 13, 407, 729, 366, 257, 1326, 721, 281, 458, 466, 13, 583, 286, 576, 2138, 3402, 1237, 493, 257, 15329, 51852], "temperature": 0.0, "avg_logprob": -0.21565791009699256, "compression_ratio": 1.4960629921259843, "no_speech_prob": 0.024052459746599197}, {"id": 517, "seek": 331172, "start": 3312.2799999999997, "end": 3321.72, "text": " tutorial to become familiar with this incredibly powerful tool. Because it really is so very handy.", "tokens": [50392, 7073, 281, 1813, 4963, 365, 341, 6252, 4005, 2290, 13, 1436, 309, 534, 307, 370, 588, 13239, 13, 50864], "temperature": 0.0, "avg_logprob": -0.23936639513288224, "compression_ratio": 1.4974093264248705, "no_speech_prob": 0.0003569626423995942}, {"id": 518, "seek": 331172, "start": 3321.72, "end": 3326.52, "text": " So if I just press continue again, it keeps running all the way to the end. And it's now finished,", "tokens": [50864, 407, 498, 286, 445, 1886, 2354, 797, 11, 309, 5965, 2614, 439, 264, 636, 281, 264, 917, 13, 400, 309, 311, 586, 4335, 11, 51104], "temperature": 0.0, "avg_logprob": -0.23936639513288224, "compression_ratio": 1.4974093264248705, "no_speech_prob": 0.0003569626423995942}, {"id": 519, "seek": 331172, "start": 3327.3199999999997, "end": 3334.2799999999997, "text": " running forward and backward. So when it's finished, we would find that there will now be,", "tokens": [51144, 2614, 2128, 293, 23897, 13, 407, 562, 309, 311, 4335, 11, 321, 576, 915, 300, 456, 486, 586, 312, 11, 51492], "temperature": 0.0, "avg_logprob": -0.23936639513288224, "compression_ratio": 1.4974093264248705, "no_speech_prob": 0.0003569626423995942}, {"id": 520, "seek": 333428, "start": 3334.84, "end": 3344.84, "text": " for example, a W1.G. Because this is the gradients that it just calculated.", "tokens": [50392, 337, 1365, 11, 257, 343, 16, 13, 38, 13, 1436, 341, 307, 264, 2771, 2448, 300, 309, 445, 15598, 13, 50892], "temperature": 0.0, "avg_logprob": -0.215027603448606, "compression_ratio": 1.2, "no_speech_prob": 0.0032729755621403456}, {"id": 521, "seek": 333428, "start": 3347.0800000000004, "end": 3357.6400000000003, "text": " And there would also be a X train.G, and so forth. OK. So let's see if we can simplify", "tokens": [51004, 400, 456, 576, 611, 312, 257, 1783, 3847, 13, 38, 11, 293, 370, 5220, 13, 2264, 13, 407, 718, 311, 536, 498, 321, 393, 20460, 51532], "temperature": 0.0, "avg_logprob": -0.215027603448606, "compression_ratio": 1.2, "no_speech_prob": 0.0032729755621403456}, {"id": 522, "seek": 335764, "start": 3357.64, "end": 3364.7599999999998, "text": " this a little bit. So I would be inclined to take these out and give them their own variable names,", "tokens": [50364, 341, 257, 707, 857, 13, 407, 286, 576, 312, 28173, 281, 747, 613, 484, 293, 976, 552, 641, 1065, 7006, 5288, 11, 50720], "temperature": 0.0, "avg_logprob": -0.2035964330037435, "compression_ratio": 1.5126903553299493, "no_speech_prob": 0.025563707575201988}, {"id": 523, "seek": 335764, "start": 3364.7599999999998, "end": 3367.64, "text": " just to make life a bit easier. It would have been better if I'd actually done this", "tokens": [50720, 445, 281, 652, 993, 257, 857, 3571, 13, 467, 576, 362, 668, 1101, 498, 286, 1116, 767, 1096, 341, 50864], "temperature": 0.0, "avg_logprob": -0.2035964330037435, "compression_ratio": 1.5126903553299493, "no_speech_prob": 0.025563707575201988}, {"id": 524, "seek": 335764, "start": 3369.16, "end": 3372.92, "text": " before the debugging. So it'd be a bit easier to type.", "tokens": [50940, 949, 264, 45592, 13, 407, 309, 1116, 312, 257, 857, 3571, 281, 2010, 13, 51128], "temperature": 0.0, "avg_logprob": -0.2035964330037435, "compression_ratio": 1.5126903553299493, "no_speech_prob": 0.025563707575201988}, {"id": 525, "seek": 335764, "start": 3374.04, "end": 3378.6, "text": " So let's set I and O equal to input and output.G.unsqueeze.", "tokens": [51184, 407, 718, 311, 992, 286, 293, 422, 2681, 281, 4846, 293, 5598, 13, 38, 13, 409, 44516, 10670, 13, 51412], "temperature": 0.0, "avg_logprob": -0.2035964330037435, "compression_ratio": 1.5126903553299493, "no_speech_prob": 0.025563707575201988}, {"id": 526, "seek": 337860, "start": 3378.8399999999997, "end": 3385.48, "text": " Oh. OK. So we'll get rid of our breakpoint.", "tokens": [50376, 876, 13, 2264, 13, 407, 321, 603, 483, 3973, 295, 527, 1821, 6053, 13, 50708], "temperature": 0.0, "avg_logprob": -0.2553925244313366, "compression_ratio": 1.251908396946565, "no_speech_prob": 0.013019234873354435}, {"id": 527, "seek": 337860, "start": 3389.08, "end": 3390.92, "text": " And double check that we've got", "tokens": [50888, 400, 3834, 1520, 300, 321, 600, 658, 50980], "temperature": 0.0, "avg_logprob": -0.2553925244313366, "compression_ratio": 1.251908396946565, "no_speech_prob": 0.013019234873354435}, {"id": 528, "seek": 337860, "start": 3394.44, "end": 3397.88, "text": " our gradients. OK.", "tokens": [51156, 527, 2771, 2448, 13, 2264, 13, 51328], "temperature": 0.0, "avg_logprob": -0.2553925244313366, "compression_ratio": 1.251908396946565, "no_speech_prob": 0.013019234873354435}, {"id": 529, "seek": 337860, "start": 3402.36, "end": 3405.48, "text": " And I guess before we rerun it, we should probably set those to zero.", "tokens": [51552, 400, 286, 2041, 949, 321, 43819, 409, 309, 11, 321, 820, 1391, 992, 729, 281, 4018, 13, 51708], "temperature": 0.0, "avg_logprob": -0.2553925244313366, "compression_ratio": 1.251908396946565, "no_speech_prob": 0.013019234873354435}, {"id": 530, "seek": 340860, "start": 3408.68, "end": 3416.44, "text": " OK. What I would do here to try things out is I'd put my breakpoint there.", "tokens": [50368, 2264, 13, 708, 286, 576, 360, 510, 281, 853, 721, 484, 307, 286, 1116, 829, 452, 1821, 6053, 456, 13, 50756], "temperature": 0.0, "avg_logprob": -0.2422140890092992, "compression_ratio": 1.54375, "no_speech_prob": 0.00013982107338961214}, {"id": 531, "seek": 340860, "start": 3417.64, "end": 3424.36, "text": " And then I would try things. So let's go next. And so I realize here that", "tokens": [50816, 400, 550, 286, 576, 853, 721, 13, 407, 718, 311, 352, 958, 13, 400, 370, 286, 4325, 510, 300, 51152], "temperature": 0.0, "avg_logprob": -0.2422140890092992, "compression_ratio": 1.54375, "no_speech_prob": 0.00013982107338961214}, {"id": 532, "seek": 340860, "start": 3425.64, "end": 3433.4, "text": " what we're actually doing is we're basically doing exactly the same thing as an Einstein would do.", "tokens": [51216, 437, 321, 434, 767, 884, 307, 321, 434, 1936, 884, 2293, 264, 912, 551, 382, 364, 23486, 576, 360, 13, 51604], "temperature": 0.0, "avg_logprob": -0.2422140890092992, "compression_ratio": 1.54375, "no_speech_prob": 0.00013982107338961214}, {"id": 533, "seek": 343340, "start": 3433.56, "end": 3435.32, "text": " So I could test that out", "tokens": [50372, 407, 286, 727, 1500, 300, 484, 50460], "temperature": 0.0, "avg_logprob": -0.3720723594107279, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.0001795278221834451}, {"id": 534, "seek": 343340, "start": 3439.32, "end": 3446.2000000000003, "text": " by trying an ionsum, right? Because I've just got this is being replicated,", "tokens": [50660, 538, 1382, 364, 741, 892, 449, 11, 558, 30, 1436, 286, 600, 445, 658, 341, 307, 885, 46365, 11, 51004], "temperature": 0.0, "avg_logprob": -0.3720723594107279, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.0001795278221834451}, {"id": 535, "seek": 343340, "start": 3447.4, "end": 3451.2400000000002, "text": " and then I'm summing over that dimension, because that's the multiplication that I'm doing.", "tokens": [51064, 293, 550, 286, 478, 2408, 2810, 670, 300, 10139, 11, 570, 300, 311, 264, 27290, 300, 286, 478, 884, 13, 51256], "temperature": 0.0, "avg_logprob": -0.3720723594107279, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.0001795278221834451}, {"id": 536, "seek": 343340, "start": 3451.2400000000002, "end": 3455.32, "text": " So I'm basically multiplying the first dimension of each,", "tokens": [51256, 407, 286, 478, 1936, 30955, 264, 700, 10139, 295, 1184, 11, 51460], "temperature": 0.0, "avg_logprob": -0.3720723594107279, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.0001795278221834451}, {"id": 537, "seek": 343340, "start": 3455.32, "end": 3458.44, "text": " and then summing over that dimension. So I could try running that", "tokens": [51460, 293, 550, 2408, 2810, 670, 300, 10139, 13, 407, 286, 727, 853, 2614, 300, 51616], "temperature": 0.0, "avg_logprob": -0.3720723594107279, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.0001795278221834451}, {"id": 538, "seek": 345844, "start": 3458.52, "end": 3463.56, "text": " and, ah, it works. So that's interesting.", "tokens": [50368, 293, 11, 3716, 11, 309, 1985, 13, 407, 300, 311, 1880, 13, 50620], "temperature": 0.0, "avg_logprob": -0.5668106725660421, "compression_ratio": 1.3503649635036497, "no_speech_prob": 0.00039204087806865573}, {"id": 539, "seek": 345844, "start": 3467.56, "end": 3471.56, "text": " Oh, and I've got zeros because I did X train dot zero. That was silly.", "tokens": [50820, 876, 11, 293, 286, 600, 658, 35193, 570, 286, 630, 1783, 3847, 5893, 4018, 13, 663, 390, 11774, 13, 51020], "temperature": 0.0, "avg_logprob": -0.5668106725660421, "compression_ratio": 1.3503649635036497, "no_speech_prob": 0.00039204087806865573}, {"id": 540, "seek": 345844, "start": 3474.92, "end": 3483.56, "text": " That should be dot gradients dot zero. OK. So let's try doing an ionsum.", "tokens": [51188, 663, 820, 312, 5893, 2771, 2448, 5893, 4018, 13, 2264, 13, 407, 718, 311, 853, 884, 364, 27362, 449, 13, 51620], "temperature": 0.0, "avg_logprob": -0.5668106725660421, "compression_ratio": 1.3503649635036497, "no_speech_prob": 0.00039204087806865573}, {"id": 541, "seek": 348356, "start": 3484.04, "end": 3488.7599999999998, "text": " And there we go. That seems to be working. That's pretty cool. So we've multiplied this", "tokens": [50388, 400, 456, 321, 352, 13, 663, 2544, 281, 312, 1364, 13, 663, 311, 1238, 1627, 13, 407, 321, 600, 17207, 341, 50624], "temperature": 0.0, "avg_logprob": -0.5170015855268999, "compression_ratio": 2.032128514056225, "no_speech_prob": 3.1693205528426915e-05}, {"id": 542, "seek": 348356, "start": 3488.7599999999998, "end": 3493.96, "text": " repeating index. So we were just multiplying the first dimensions together, and then summing over", "tokens": [50624, 18617, 8186, 13, 407, 321, 645, 445, 30955, 264, 700, 12819, 1214, 11, 293, 550, 2408, 2810, 670, 50884], "temperature": 0.0, "avg_logprob": -0.5170015855268999, "compression_ratio": 2.032128514056225, "no_speech_prob": 3.1693205528426915e-05}, {"id": 543, "seek": 348356, "start": 3493.96, "end": 3498.92, "text": " them. So there's no i here. Now, that's not quite the same thing as a matrix multiplication,", "tokens": [50884, 552, 13, 407, 456, 311, 572, 741, 510, 13, 823, 11, 300, 311, 406, 1596, 264, 912, 551, 382, 257, 8141, 27290, 11, 51132], "temperature": 0.0, "avg_logprob": -0.5170015855268999, "compression_ratio": 2.032128514056225, "no_speech_prob": 3.1693205528426915e-05}, {"id": 544, "seek": 348356, "start": 3498.92, "end": 3503.88, "text": " but we could turn it into the same thing as matrix multiplication just by swapping i and j,", "tokens": [51132, 457, 321, 727, 1261, 309, 666, 264, 912, 551, 382, 8141, 27290, 445, 538, 1693, 10534, 741, 293, 361, 11, 51380], "temperature": 0.0, "avg_logprob": -0.5170015855268999, "compression_ratio": 2.032128514056225, "no_speech_prob": 3.1693205528426915e-05}, {"id": 545, "seek": 348356, "start": 3504.52, "end": 3506.6, "text": " so that they're the other way around. So I could do that.", "tokens": [51412, 370, 300, 436, 434, 264, 661, 636, 926, 13, 407, 286, 727, 360, 300, 13, 51516], "temperature": 0.0, "avg_logprob": -0.5170015855268999, "compression_ratio": 2.032128514056225, "no_speech_prob": 3.1693205528426915e-05}, {"id": 546, "seek": 348356, "start": 3506.6, "end": 3509.0, "text": " And then I could do that. And then I could do that. And then I could do that.", "tokens": [51516, 400, 550, 286, 727, 360, 300, 13, 400, 550, 286, 727, 360, 300, 13, 400, 550, 286, 727, 360, 300, 13, 51636], "temperature": 0.0, "avg_logprob": -0.5170015855268999, "compression_ratio": 2.032128514056225, "no_speech_prob": 3.1693205528426915e-05}, {"id": 547, "seek": 350900, "start": 3509.72, "end": 3515.32, "text": " Just by swapping i and j, so that they're the other way around. And that way we'd have", "tokens": [50400, 1449, 538, 1693, 10534, 741, 293, 361, 11, 370, 300, 436, 434, 264, 661, 636, 926, 13, 400, 300, 636, 321, 1116, 362, 50680], "temperature": 0.0, "avg_logprob": -0.22929284883582074, "compression_ratio": 1.6036866359447004, "no_speech_prob": 0.05664857104420662}, {"id": 548, "seek": 350900, "start": 3516.2, "end": 3524.44, "text": " j i comma i k. And we can swap into dimensions very easily. That's what's called the transpose.", "tokens": [50724, 361, 741, 22117, 741, 350, 13, 400, 321, 393, 18135, 666, 12819, 588, 3612, 13, 663, 311, 437, 311, 1219, 264, 25167, 13, 51136], "temperature": 0.0, "avg_logprob": -0.22929284883582074, "compression_ratio": 1.6036866359447004, "no_speech_prob": 0.05664857104420662}, {"id": 549, "seek": 350900, "start": 3524.44, "end": 3529.72, "text": " So that would become a matrix multiplication if we just use the transpose. And in NumPy,", "tokens": [51136, 407, 300, 576, 1813, 257, 8141, 27290, 498, 321, 445, 764, 264, 25167, 13, 400, 294, 22592, 47, 88, 11, 51400], "temperature": 0.0, "avg_logprob": -0.22929284883582074, "compression_ratio": 1.6036866359447004, "no_speech_prob": 0.05664857104420662}, {"id": 550, "seek": 350900, "start": 3529.72, "end": 3535.32, "text": " the transpose is the capital T attribute. So here is exactly the same thing,", "tokens": [51400, 264, 25167, 307, 264, 4238, 314, 19667, 13, 407, 510, 307, 2293, 264, 912, 551, 11, 51680], "temperature": 0.0, "avg_logprob": -0.22929284883582074, "compression_ratio": 1.6036866359447004, "no_speech_prob": 0.05664857104420662}, {"id": 551, "seek": 353532, "start": 3536.04, "end": 3541.88, "text": " using a matrix multiply and a transpose. And let's check.", "tokens": [50400, 1228, 257, 8141, 12972, 293, 257, 25167, 13, 400, 718, 311, 1520, 13, 50692], "temperature": 0.0, "avg_logprob": -0.1779055017413515, "compression_ratio": 1.5089820359281436, "no_speech_prob": 0.00018814110080711544}, {"id": 552, "seek": 353532, "start": 3544.28, "end": 3552.1200000000003, "text": " Yeah, that's the same thing as well. Okay, cool. So that tells us that now we've checked in our", "tokens": [50812, 865, 11, 300, 311, 264, 912, 551, 382, 731, 13, 1033, 11, 1627, 13, 407, 300, 5112, 505, 300, 586, 321, 600, 10033, 294, 527, 51204], "temperature": 0.0, "avg_logprob": -0.1779055017413515, "compression_ratio": 1.5089820359281436, "no_speech_prob": 0.00018814110080711544}, {"id": 553, "seek": 353532, "start": 3552.1200000000003, "end": 3563.8, "text": " debugger that we can actually replace all this with a matrix multiply. We don't need that anymore.", "tokens": [51204, 24083, 1321, 300, 321, 393, 767, 7406, 439, 341, 365, 257, 8141, 12972, 13, 492, 500, 380, 643, 300, 3602, 13, 51788], "temperature": 0.0, "avg_logprob": -0.1779055017413515, "compression_ratio": 1.5089820359281436, "no_speech_prob": 0.00018814110080711544}, {"id": 554, "seek": 356532, "start": 3566.28, "end": 3580.1200000000003, "text": " And let's see if it works. It does. All right. X train dot g. Cool.", "tokens": [50412, 400, 718, 311, 536, 498, 309, 1985, 13, 467, 775, 13, 1057, 558, 13, 1783, 3847, 5893, 290, 13, 8561, 13, 51104], "temperature": 0.0, "avg_logprob": -0.22947782709978629, "compression_ratio": 1.4120879120879122, "no_speech_prob": 0.0003353436477482319}, {"id": 555, "seek": 356532, "start": 3582.28, "end": 3587.0800000000004, "text": " Okay. So hopefully that's convinced you that the debugger is a really handy thing for playing", "tokens": [51212, 1033, 13, 407, 4696, 300, 311, 12561, 291, 300, 264, 24083, 1321, 307, 257, 534, 13239, 551, 337, 2433, 51452], "temperature": 0.0, "avg_logprob": -0.22947782709978629, "compression_ratio": 1.4120879120879122, "no_speech_prob": 0.0003353436477482319}, {"id": 556, "seek": 356532, "start": 3587.0800000000004, "end": 3593.8, "text": " around with numeric programming ideas or coding in general. And so I think now's a good time to", "tokens": [51452, 926, 365, 7866, 299, 9410, 3487, 420, 17720, 294, 2674, 13, 400, 370, 286, 519, 586, 311, 257, 665, 565, 281, 51788], "temperature": 0.0, "avg_logprob": -0.22947782709978629, "compression_ratio": 1.4120879120879122, "no_speech_prob": 0.0003353436477482319}, {"id": 557, "seek": 359380, "start": 3593.8, "end": 3598.6800000000003, "text": " take a break. So let's take a eight minute break. And I'll see you back here. Actually,", "tokens": [50364, 747, 257, 1821, 13, 407, 718, 311, 747, 257, 3180, 3456, 1821, 13, 400, 286, 603, 536, 291, 646, 510, 13, 5135, 11, 50608], "temperature": 0.0, "avg_logprob": -0.2037031092542283, "compression_ratio": 1.674757281553398, "no_speech_prob": 0.0007793479599058628}, {"id": 558, "seek": 359380, "start": 3598.6800000000003, "end": 3603.0, "text": " seven minute break. I'll see you back here in seven minutes. Thank you.", "tokens": [50608, 3407, 3456, 1821, 13, 286, 603, 536, 291, 646, 510, 294, 3407, 2077, 13, 1044, 291, 13, 50824], "temperature": 0.0, "avg_logprob": -0.2037031092542283, "compression_ratio": 1.674757281553398, "no_speech_prob": 0.0007793479599058628}, {"id": 559, "seek": 359380, "start": 3605.88, "end": 3613.7200000000003, "text": " Okay. Welcome back. So we've calculated our derivatives. And we want to test them. Luckily,", "tokens": [50968, 1033, 13, 4027, 646, 13, 407, 321, 600, 15598, 527, 33733, 13, 400, 321, 528, 281, 1500, 552, 13, 19726, 11, 51360], "temperature": 0.0, "avg_logprob": -0.2037031092542283, "compression_ratio": 1.674757281553398, "no_speech_prob": 0.0007793479599058628}, {"id": 560, "seek": 359380, "start": 3613.7200000000003, "end": 3621.5600000000004, "text": " PyTorch already has derivatives implemented. So I'm going to totally cheat and use PyTorch to", "tokens": [51360, 9953, 51, 284, 339, 1217, 575, 33733, 12270, 13, 407, 286, 478, 516, 281, 3879, 17470, 293, 764, 9953, 51, 284, 339, 281, 51752], "temperature": 0.0, "avg_logprob": -0.2037031092542283, "compression_ratio": 1.674757281553398, "no_speech_prob": 0.0007793479599058628}, {"id": 561, "seek": 362156, "start": 3621.56, "end": 3626.52, "text": " calculate the same derivatives. So don't worry about how this works yet. Because we're actually", "tokens": [50364, 8873, 264, 912, 33733, 13, 407, 500, 380, 3292, 466, 577, 341, 1985, 1939, 13, 1436, 321, 434, 767, 50612], "temperature": 0.0, "avg_logprob": -0.1652903298074885, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0005274753784760833}, {"id": 562, "seek": 362156, "start": 3626.52, "end": 3630.68, "text": " going to be doing all this from scratch anyway. For now, I'm just going to run it all through", "tokens": [50612, 516, 281, 312, 884, 439, 341, 490, 8459, 4033, 13, 1171, 586, 11, 286, 478, 445, 516, 281, 1190, 309, 439, 807, 50820], "temperature": 0.0, "avg_logprob": -0.1652903298074885, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0005274753784760833}, {"id": 563, "seek": 362156, "start": 3630.68, "end": 3636.2, "text": " PyTorch and check that their derivatives are the same as ours. And they are. So we're on the right", "tokens": [50820, 9953, 51, 284, 339, 293, 1520, 300, 641, 33733, 366, 264, 912, 382, 11896, 13, 400, 436, 366, 13, 407, 321, 434, 322, 264, 558, 51096], "temperature": 0.0, "avg_logprob": -0.1652903298074885, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0005274753784760833}, {"id": 564, "seek": 362156, "start": 3636.2, "end": 3643.32, "text": " track. Okay. So this is all pretty clunky. I think we can all agree. And obviously it's clunkier than", "tokens": [51096, 2837, 13, 1033, 13, 407, 341, 307, 439, 1238, 596, 25837, 13, 286, 519, 321, 393, 439, 3986, 13, 400, 2745, 309, 311, 596, 3197, 811, 813, 51452], "temperature": 0.0, "avg_logprob": -0.1652903298074885, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0005274753784760833}, {"id": 565, "seek": 362156, "start": 3643.32, "end": 3648.04, "text": " what we do in PyTorch. So how do we simplify things? There's some really cool refactoring", "tokens": [51452, 437, 321, 360, 294, 9953, 51, 284, 339, 13, 407, 577, 360, 321, 20460, 721, 30, 821, 311, 512, 534, 1627, 1895, 578, 3662, 51688], "temperature": 0.0, "avg_logprob": -0.1652903298074885, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0005274753784760833}, {"id": 566, "seek": 364804, "start": 3648.04, "end": 3654.7599999999998, "text": " that we can do. So what we're going to do is we're going to create a whole class for each of", "tokens": [50364, 300, 321, 393, 360, 13, 407, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 1884, 257, 1379, 1508, 337, 1184, 295, 50700], "temperature": 0.0, "avg_logprob": -0.18571836948394777, "compression_ratio": 1.8164556962025316, "no_speech_prob": 0.00045830654562450945}, {"id": 567, "seek": 364804, "start": 3654.7599999999998, "end": 3665.16, "text": " our functions. For the value function and for the linear function. So the way that we're going to", "tokens": [50700, 527, 6828, 13, 1171, 264, 2158, 2445, 293, 337, 264, 8213, 2445, 13, 407, 264, 636, 300, 321, 434, 516, 281, 51220], "temperature": 0.0, "avg_logprob": -0.18571836948394777, "compression_ratio": 1.8164556962025316, "no_speech_prob": 0.00045830654562450945}, {"id": 568, "seek": 364804, "start": 3665.16, "end": 3673.72, "text": " do this is we're going to create a Dunder call. What does Dunder call do? Let me show you. So if", "tokens": [51220, 360, 341, 307, 321, 434, 516, 281, 1884, 257, 413, 6617, 818, 13, 708, 775, 413, 6617, 818, 360, 30, 961, 385, 855, 291, 13, 407, 498, 51648], "temperature": 0.0, "avg_logprob": -0.18571836948394777, "compression_ratio": 1.8164556962025316, "no_speech_prob": 0.00045830654562450945}, {"id": 569, "seek": 367372, "start": 3673.72, "end": 3689.3999999999996, "text": " I create a class. And we're just going to set that to print hello. So if I create an instance", "tokens": [50364, 286, 1884, 257, 1508, 13, 400, 321, 434, 445, 516, 281, 992, 300, 281, 4482, 7751, 13, 407, 498, 286, 1884, 364, 5197, 51148], "temperature": 0.0, "avg_logprob": -0.17418779893354935, "compression_ratio": 1.3955223880597014, "no_speech_prob": 0.0015247699338942766}, {"id": 570, "seek": 367372, "start": 3689.3999999999996, "end": 3699.8799999999997, "text": " of that class. And then I call it as if it was a function. Oops. Missing the Dunder bit here.", "tokens": [51148, 295, 300, 1508, 13, 400, 550, 286, 818, 309, 382, 498, 309, 390, 257, 2445, 13, 21726, 13, 5275, 278, 264, 413, 6617, 857, 510, 13, 51672], "temperature": 0.0, "avg_logprob": -0.17418779893354935, "compression_ratio": 1.3955223880597014, "no_speech_prob": 0.0015247699338942766}, {"id": 571, "seek": 369988, "start": 3700.04, "end": 3707.7200000000003, "text": " Call it as if it's a function. It says hi. So in other words, you know, everything can be", "tokens": [50372, 7807, 309, 382, 498, 309, 311, 257, 2445, 13, 467, 1619, 4879, 13, 407, 294, 661, 2283, 11, 291, 458, 11, 1203, 393, 312, 50756], "temperature": 0.0, "avg_logprob": -0.23359992053057696, "compression_ratio": 1.4860335195530727, "no_speech_prob": 0.00018814076611306518}, {"id": 572, "seek": 369988, "start": 3707.7200000000003, "end": 3713.4, "text": " changed in Python. You can change how a class behaves. You can make it look, work like a", "tokens": [50756, 3105, 294, 15329, 13, 509, 393, 1319, 577, 257, 1508, 36896, 13, 509, 393, 652, 309, 574, 11, 589, 411, 257, 51040], "temperature": 0.0, "avg_logprob": -0.23359992053057696, "compression_ratio": 1.4860335195530727, "no_speech_prob": 0.00018814076611306518}, {"id": 573, "seek": 369988, "start": 3713.4, "end": 3718.84, "text": " function. And to do that, you simply define Dunder call. You could pass it an argument.", "tokens": [51040, 2445, 13, 400, 281, 360, 300, 11, 291, 2935, 6964, 413, 6617, 818, 13, 509, 727, 1320, 309, 364, 6770, 13, 51312], "temperature": 0.0, "avg_logprob": -0.23359992053057696, "compression_ratio": 1.4860335195530727, "no_speech_prob": 0.00018814076611306518}, {"id": 574, "seek": 371884, "start": 3718.84, "end": 3733.4, "text": " Like so. Okay. So that's what Dunder call does. It just says it's just a little bit of syntax", "tokens": [50364, 1743, 370, 13, 1033, 13, 407, 300, 311, 437, 413, 6617, 818, 775, 13, 467, 445, 1619, 309, 311, 445, 257, 707, 857, 295, 28431, 51092], "temperature": 0.0, "avg_logprob": -0.19800907228051162, "compression_ratio": 1.4922279792746114, "no_speech_prob": 0.0032730221282690763}, {"id": 575, "seek": 371884, "start": 3733.4, "end": 3740.1200000000003, "text": " sugary kind of stuff to say I want to be able to treat it as if it's a function without any method", "tokens": [51092, 22802, 822, 733, 295, 1507, 281, 584, 286, 528, 281, 312, 1075, 281, 2387, 309, 382, 498, 309, 311, 257, 2445, 1553, 604, 3170, 51428], "temperature": 0.0, "avg_logprob": -0.19800907228051162, "compression_ratio": 1.4922279792746114, "no_speech_prob": 0.0032730221282690763}, {"id": 576, "seek": 371884, "start": 3740.1200000000003, "end": 3744.52, "text": " at all. You can still do it the method way. You could have done this. Don't know why you'd want", "tokens": [51428, 412, 439, 13, 509, 393, 920, 360, 309, 264, 3170, 636, 13, 509, 727, 362, 1096, 341, 13, 1468, 380, 458, 983, 291, 1116, 528, 51648], "temperature": 0.0, "avg_logprob": -0.19800907228051162, "compression_ratio": 1.4922279792746114, "no_speech_prob": 0.0032730221282690763}, {"id": 577, "seek": 374452, "start": 3744.52, "end": 3750.12, "text": " to. But you can. Because it's got this special magic named Dunder call. You don't have to write", "tokens": [50364, 281, 13, 583, 291, 393, 13, 1436, 309, 311, 658, 341, 2121, 5585, 4926, 413, 6617, 818, 13, 509, 500, 380, 362, 281, 2464, 50644], "temperature": 0.0, "avg_logprob": -0.19596748726040708, "compression_ratio": 1.6710526315789473, "no_speech_prob": 0.013848180882632732}, {"id": 578, "seek": 374452, "start": 3750.12, "end": 3757.8, "text": " the Dunder call at all. So here, if we create an instance of the value class, we can treat it as", "tokens": [50644, 264, 413, 6617, 818, 412, 439, 13, 407, 510, 11, 498, 321, 1884, 364, 5197, 295, 264, 2158, 1508, 11, 321, 393, 2387, 309, 382, 51028], "temperature": 0.0, "avg_logprob": -0.19596748726040708, "compression_ratio": 1.6710526315789473, "no_speech_prob": 0.013848180882632732}, {"id": 579, "seek": 374452, "start": 3757.8, "end": 3763.0, "text": " a function. And what it's going to do is it's going to take its input and do the value on it.", "tokens": [51028, 257, 2445, 13, 400, 437, 309, 311, 516, 281, 360, 307, 309, 311, 516, 281, 747, 1080, 4846, 293, 360, 264, 2158, 322, 309, 13, 51288], "temperature": 0.0, "avg_logprob": -0.19596748726040708, "compression_ratio": 1.6710526315789473, "no_speech_prob": 0.013848180882632732}, {"id": 580, "seek": 374452, "start": 3764.12, "end": 3769.24, "text": " But if you look back at the forward and backward, there's something very interesting about the", "tokens": [51344, 583, 498, 291, 574, 646, 412, 264, 2128, 293, 23897, 11, 456, 311, 746, 588, 1880, 466, 264, 51600], "temperature": 0.0, "avg_logprob": -0.19596748726040708, "compression_ratio": 1.6710526315789473, "no_speech_prob": 0.013848180882632732}, {"id": 581, "seek": 376924, "start": 3769.24, "end": 3777.56, "text": " backward pass. Which is that it has to know about, for example, this intermediate calculation", "tokens": [50364, 23897, 1320, 13, 3013, 307, 300, 309, 575, 281, 458, 466, 11, 337, 1365, 11, 341, 19376, 17108, 50780], "temperature": 0.0, "avg_logprob": -0.21903929710388184, "compression_ratio": 1.945945945945946, "no_speech_prob": 0.0070116701535880566}, {"id": 582, "seek": 376924, "start": 3777.56, "end": 3783.08, "text": " gets passed over here. This intermediate calculation gets passed over here. Because of", "tokens": [50780, 2170, 4678, 670, 510, 13, 639, 19376, 17108, 2170, 4678, 670, 510, 13, 1436, 295, 51056], "temperature": 0.0, "avg_logprob": -0.21903929710388184, "compression_ratio": 1.945945945945946, "no_speech_prob": 0.0070116701535880566}, {"id": 583, "seek": 376924, "start": 3783.08, "end": 3789.4799999999996, "text": " the chain rule, we're going to need some of the intermediate calculations. And not just because", "tokens": [51056, 264, 5021, 4978, 11, 321, 434, 516, 281, 643, 512, 295, 264, 19376, 20448, 13, 400, 406, 445, 570, 51376], "temperature": 0.0, "avg_logprob": -0.21903929710388184, "compression_ratio": 1.945945945945946, "no_speech_prob": 0.0070116701535880566}, {"id": 584, "seek": 376924, "start": 3789.4799999999996, "end": 3794.12, "text": " of the chain rule, but because of how the derivatives are calculated. So we need to", "tokens": [51376, 295, 264, 5021, 4978, 11, 457, 570, 295, 577, 264, 33733, 366, 15598, 13, 407, 321, 643, 281, 51608], "temperature": 0.0, "avg_logprob": -0.21903929710388184, "compression_ratio": 1.945945945945946, "no_speech_prob": 0.0070116701535880566}, {"id": 585, "seek": 379412, "start": 3794.12, "end": 3802.7599999999998, "text": " actually store each of the layer intermediate calculations. And so that's why relu doesn't just", "tokens": [50364, 767, 3531, 1184, 295, 264, 4583, 19376, 20448, 13, 400, 370, 300, 311, 983, 1039, 84, 1177, 380, 445, 50796], "temperature": 0.0, "avg_logprob": -0.224699338277181, "compression_ratio": 1.6424242424242423, "no_speech_prob": 0.000249232049100101}, {"id": 586, "seek": 379412, "start": 3804.2799999999997, "end": 3811.0, "text": " calculate and return the output. But it also stores its output. And it also stores its input.", "tokens": [50872, 8873, 293, 2736, 264, 5598, 13, 583, 309, 611, 9512, 1080, 5598, 13, 400, 309, 611, 9512, 1080, 4846, 13, 51208], "temperature": 0.0, "avg_logprob": -0.224699338277181, "compression_ratio": 1.6424242424242423, "no_speech_prob": 0.000249232049100101}, {"id": 587, "seek": 379412, "start": 3811.88, "end": 3819.7999999999997, "text": " So that way then when we call backward, we know how to calculate that. We set the", "tokens": [51252, 407, 300, 636, 550, 562, 321, 818, 23897, 11, 321, 458, 577, 281, 8873, 300, 13, 492, 992, 264, 51648], "temperature": 0.0, "avg_logprob": -0.224699338277181, "compression_ratio": 1.6424242424242423, "no_speech_prob": 0.000249232049100101}, {"id": 588, "seek": 381980, "start": 3819.8, "end": 3825.48, "text": " inputs gradient. Because remember we stored the input. So we can do that. Right? And it's going", "tokens": [50364, 15743, 16235, 13, 1436, 1604, 321, 12187, 264, 4846, 13, 407, 321, 393, 360, 300, 13, 1779, 30, 400, 309, 311, 516, 50648], "temperature": 0.0, "avg_logprob": -0.252036546405993, "compression_ratio": 1.5297297297297296, "no_speech_prob": 5.829105793964118e-05}, {"id": 589, "seek": 381980, "start": 3825.48, "end": 3832.6800000000003, "text": " to just be, oh, input greater than zero dot float. Right? So that's the definition, okay, of the", "tokens": [50648, 281, 445, 312, 11, 1954, 11, 4846, 5044, 813, 4018, 5893, 15706, 13, 1779, 30, 407, 300, 311, 264, 7123, 11, 1392, 11, 295, 264, 51008], "temperature": 0.0, "avg_logprob": -0.252036546405993, "compression_ratio": 1.5297297297297296, "no_speech_prob": 5.829105793964118e-05}, {"id": 590, "seek": 381980, "start": 3833.4, "end": 3844.92, "text": " derivative of a relu. And then chain rule. So that's how we can calculate the forward pass", "tokens": [51044, 13760, 295, 257, 1039, 84, 13, 400, 550, 5021, 4978, 13, 407, 300, 311, 577, 321, 393, 8873, 264, 2128, 1320, 51620], "temperature": 0.0, "avg_logprob": -0.252036546405993, "compression_ratio": 1.5297297297297296, "no_speech_prob": 5.829105793964118e-05}, {"id": 591, "seek": 384492, "start": 3844.92, "end": 3850.2000000000003, "text": " and the backward pass for relu. And we're not going to have to then store all this intermediate", "tokens": [50364, 293, 264, 23897, 1320, 337, 1039, 84, 13, 400, 321, 434, 406, 516, 281, 362, 281, 550, 3531, 439, 341, 19376, 50628], "temperature": 0.0, "avg_logprob": -0.22903628647327423, "compression_ratio": 1.8086642599277978, "no_speech_prob": 0.0005357793415896595}, {"id": 592, "seek": 384492, "start": 3850.2000000000003, "end": 3854.6, "text": " stuff separately. It's going to happen automatically. So we can do the same thing for a linear layer.", "tokens": [50628, 1507, 14759, 13, 467, 311, 516, 281, 1051, 6772, 13, 407, 321, 393, 360, 264, 912, 551, 337, 257, 8213, 4583, 13, 50848], "temperature": 0.0, "avg_logprob": -0.22903628647327423, "compression_ratio": 1.8086642599277978, "no_speech_prob": 0.0005357793415896595}, {"id": 593, "seek": 384492, "start": 3854.6, "end": 3861.8, "text": " Now linear layer needs some additional state, weights and biases. Relu doesn't. Right? So there's", "tokens": [50848, 823, 8213, 4583, 2203, 512, 4497, 1785, 11, 17443, 293, 32152, 13, 8738, 84, 1177, 380, 13, 1779, 30, 407, 456, 311, 51208], "temperature": 0.0, "avg_logprob": -0.22903628647327423, "compression_ratio": 1.8086642599277978, "no_speech_prob": 0.0005357793415896595}, {"id": 594, "seek": 384492, "start": 3861.8, "end": 3867.0, "text": " no in it. So when we create a linear layer, we have to say what are its weights, what are its biases.", "tokens": [51208, 572, 294, 309, 13, 407, 562, 321, 1884, 257, 8213, 4583, 11, 321, 362, 281, 584, 437, 366, 1080, 17443, 11, 437, 366, 1080, 32152, 13, 51468], "temperature": 0.0, "avg_logprob": -0.22903628647327423, "compression_ratio": 1.8086642599277978, "no_speech_prob": 0.0005357793415896595}, {"id": 595, "seek": 384492, "start": 3867.0, "end": 3873.32, "text": " We store them away. And then when we call it in the forward pass, just like before, we store the input.", "tokens": [51468, 492, 3531, 552, 1314, 13, 400, 550, 562, 321, 818, 309, 294, 264, 2128, 1320, 11, 445, 411, 949, 11, 321, 3531, 264, 4846, 13, 51784], "temperature": 0.0, "avg_logprob": -0.22903628647327423, "compression_ratio": 1.8086642599277978, "no_speech_prob": 0.0005357793415896595}, {"id": 596, "seek": 387332, "start": 3873.32, "end": 3879.56, "text": " So that's exactly the same line here. And just like before, we calculate the output and store it", "tokens": [50364, 407, 300, 311, 2293, 264, 912, 1622, 510, 13, 400, 445, 411, 949, 11, 321, 8873, 264, 5598, 293, 3531, 309, 50676], "temperature": 0.0, "avg_logprob": -0.21056211471557618, "compression_ratio": 1.733644859813084, "no_speech_prob": 0.0010322262533009052}, {"id": 597, "seek": 387332, "start": 3880.36, "end": 3887.0800000000004, "text": " and then return it. Okay? And this time, of course, we just call lin. And then for the backward pass,", "tokens": [50716, 293, 550, 2736, 309, 13, 1033, 30, 400, 341, 565, 11, 295, 1164, 11, 321, 445, 818, 22896, 13, 400, 550, 337, 264, 23897, 1320, 11, 51052], "temperature": 0.0, "avg_logprob": -0.21056211471557618, "compression_ratio": 1.733644859813084, "no_speech_prob": 0.0010322262533009052}, {"id": 598, "seek": 387332, "start": 3888.6800000000003, "end": 3892.6000000000004, "text": " it's the same thing. Okay? So the input gradients we calculate just like before.", "tokens": [51132, 309, 311, 264, 912, 551, 13, 1033, 30, 407, 264, 4846, 2771, 2448, 321, 8873, 445, 411, 949, 13, 51328], "temperature": 0.0, "avg_logprob": -0.21056211471557618, "compression_ratio": 1.733644859813084, "no_speech_prob": 0.0010322262533009052}, {"id": 599, "seek": 387332, "start": 3894.6800000000003, "end": 3901.6400000000003, "text": " Oh, dot t brackets is exactly the same with a little t as big T is as a property. So that's", "tokens": [51432, 876, 11, 5893, 256, 26179, 307, 2293, 264, 912, 365, 257, 707, 256, 382, 955, 314, 307, 382, 257, 4707, 13, 407, 300, 311, 51780], "temperature": 0.0, "avg_logprob": -0.21056211471557618, "compression_ratio": 1.733644859813084, "no_speech_prob": 0.0010322262533009052}, {"id": 600, "seek": 390164, "start": 3901.64, "end": 3908.44, "text": " the same thing. That's just the transpose. Calculate the gradients of the weights.", "tokens": [50364, 264, 912, 551, 13, 663, 311, 445, 264, 25167, 13, 3511, 2444, 473, 264, 2771, 2448, 295, 264, 17443, 13, 50704], "temperature": 0.0, "avg_logprob": -0.20949726838331956, "compression_ratio": 1.7031963470319635, "no_speech_prob": 5.2252111345296726e-05}, {"id": 601, "seek": 390164, "start": 3910.12, "end": 3915.7999999999997, "text": " Again, with a chain rule and the bias, just like we did it before. And they're all being stored in", "tokens": [50788, 3764, 11, 365, 257, 5021, 4978, 293, 264, 12577, 11, 445, 411, 321, 630, 309, 949, 13, 400, 436, 434, 439, 885, 12187, 294, 51072], "temperature": 0.0, "avg_logprob": -0.20949726838331956, "compression_ratio": 1.7031963470319635, "no_speech_prob": 5.2252111345296726e-05}, {"id": 602, "seek": 390164, "start": 3915.7999999999997, "end": 3920.6, "text": " the appropriate places. And then for MSE, we can do the same thing. We don't just calculate the", "tokens": [51072, 264, 6854, 3190, 13, 400, 550, 337, 376, 5879, 11, 321, 393, 360, 264, 912, 551, 13, 492, 500, 380, 445, 8873, 264, 51312], "temperature": 0.0, "avg_logprob": -0.20949726838331956, "compression_ratio": 1.7031963470319635, "no_speech_prob": 5.2252111345296726e-05}, {"id": 603, "seek": 390164, "start": 3920.6, "end": 3926.8399999999997, "text": " MSE, but we also store it. And we also, now the MSE needs just needs two things, an input and a", "tokens": [51312, 376, 5879, 11, 457, 321, 611, 3531, 309, 13, 400, 321, 611, 11, 586, 264, 376, 5879, 2203, 445, 2203, 732, 721, 11, 364, 4846, 293, 257, 51624], "temperature": 0.0, "avg_logprob": -0.20949726838331956, "compression_ratio": 1.7031963470319635, "no_speech_prob": 5.2252111345296726e-05}, {"id": 604, "seek": 392684, "start": 3926.84, "end": 3933.08, "text": " target. So we'll store those as well. So then in the backward pass, we can calculate its gradient", "tokens": [50364, 3779, 13, 407, 321, 603, 3531, 729, 382, 731, 13, 407, 550, 294, 264, 23897, 1320, 11, 321, 393, 8873, 1080, 16235, 50676], "temperature": 0.0, "avg_logprob": -0.21399417555475808, "compression_ratio": 1.4619289340101522, "no_speech_prob": 0.0003353481297381222}, {"id": 605, "seek": 392684, "start": 3933.08, "end": 3946.84, "text": " of the input as being two times the difference. And there it all is. Okay. So our model now,", "tokens": [50676, 295, 264, 4846, 382, 885, 732, 1413, 264, 2649, 13, 400, 456, 309, 439, 307, 13, 1033, 13, 407, 527, 2316, 586, 11, 51364], "temperature": 0.0, "avg_logprob": -0.21399417555475808, "compression_ratio": 1.4619289340101522, "no_speech_prob": 0.0003353481297381222}, {"id": 606, "seek": 392684, "start": 3947.6400000000003, "end": 3955.8, "text": " it's much easier to define. We can just create a bunch of layers, linear w1, b1, relu, linear w2,", "tokens": [51404, 309, 311, 709, 3571, 281, 6964, 13, 492, 393, 445, 1884, 257, 3840, 295, 7914, 11, 8213, 261, 16, 11, 272, 16, 11, 1039, 84, 11, 8213, 261, 17, 11, 51812], "temperature": 0.0, "avg_logprob": -0.21399417555475808, "compression_ratio": 1.4619289340101522, "no_speech_prob": 0.0003353481297381222}, {"id": 607, "seek": 395580, "start": 3955.8, "end": 3962.76, "text": " b2. And then we can store an instance of the MSE. So this is not calling MSE. It's creating an", "tokens": [50364, 272, 17, 13, 400, 550, 321, 393, 3531, 364, 5197, 295, 264, 376, 5879, 13, 407, 341, 307, 406, 5141, 376, 5879, 13, 467, 311, 4084, 364, 50712], "temperature": 0.0, "avg_logprob": -0.24500719706217447, "compression_ratio": 1.8980582524271845, "no_speech_prob": 0.00023050507297739387}, {"id": 608, "seek": 395580, "start": 3962.76, "end": 3967.48, "text": " instance of the MSE class. And this is an instance of the lin class. This is an instance of the", "tokens": [50712, 5197, 295, 264, 376, 5879, 1508, 13, 400, 341, 307, 364, 5197, 295, 264, 22896, 1508, 13, 639, 307, 364, 5197, 295, 264, 50948], "temperature": 0.0, "avg_logprob": -0.24500719706217447, "compression_ratio": 1.8980582524271845, "no_speech_prob": 0.00023050507297739387}, {"id": 609, "seek": 395580, "start": 3967.48, "end": 3975.0800000000004, "text": " relu class. So they're just being stored. So then when we call the model, we pass it our inputs and", "tokens": [50948, 1039, 84, 1508, 13, 407, 436, 434, 445, 885, 12187, 13, 407, 550, 562, 321, 818, 264, 2316, 11, 321, 1320, 309, 527, 15743, 293, 51328], "temperature": 0.0, "avg_logprob": -0.24500719706217447, "compression_ratio": 1.8980582524271845, "no_speech_prob": 0.00023050507297739387}, {"id": 610, "seek": 395580, "start": 3975.0800000000004, "end": 3983.8, "text": " our target. We go through each layer, set x equal to the result of calling that layer, and then pass", "tokens": [51328, 527, 3779, 13, 492, 352, 807, 1184, 4583, 11, 992, 2031, 2681, 281, 264, 1874, 295, 5141, 300, 4583, 11, 293, 550, 1320, 51764], "temperature": 0.0, "avg_logprob": -0.24500719706217447, "compression_ratio": 1.8980582524271845, "no_speech_prob": 0.00023050507297739387}, {"id": 611, "seek": 398380, "start": 3983.8, "end": 3988.04, "text": " that to the loss. So there's something kind of interesting here that you might have noticed,", "tokens": [50364, 300, 281, 264, 4470, 13, 407, 456, 311, 746, 733, 295, 1880, 510, 300, 291, 1062, 362, 5694, 11, 50576], "temperature": 0.0, "avg_logprob": -0.2921104743832447, "compression_ratio": 1.7266187050359711, "no_speech_prob": 0.0006166123785078526}, {"id": 612, "seek": 398380, "start": 3988.6800000000003, "end": 3999.0800000000004, "text": " which is that we don't have, um, um, there we do it.", "tokens": [50608, 597, 307, 300, 321, 500, 380, 362, 11, 1105, 11, 1105, 11, 456, 321, 360, 309, 13, 51128], "temperature": 0.0, "avg_logprob": -0.2921104743832447, "compression_ratio": 1.7266187050359711, "no_speech_prob": 0.0006166123785078526}, {"id": 613, "seek": 398380, "start": 4005.2400000000002, "end": 4010.6800000000003, "text": " Something interesting here is that we don't have two separate functions inside our, inside our", "tokens": [51436, 6595, 1880, 510, 307, 300, 321, 500, 380, 362, 732, 4994, 6828, 1854, 527, 11, 1854, 527, 51708], "temperature": 0.0, "avg_logprob": -0.2921104743832447, "compression_ratio": 1.7266187050359711, "no_speech_prob": 0.0006166123785078526}, {"id": 614, "seek": 401068, "start": 4010.68, "end": 4017.72, "text": " model. The loss function being applied to a separate neural net. But we've actually integrated", "tokens": [50364, 2316, 13, 440, 4470, 2445, 885, 6456, 281, 257, 4994, 18161, 2533, 13, 583, 321, 600, 767, 10919, 50716], "temperature": 0.0, "avg_logprob": -0.2082133943384344, "compression_ratio": 1.7022222222222223, "no_speech_prob": 8.750286360736936e-05}, {"id": 615, "seek": 401068, "start": 4017.72, "end": 4023.3199999999997, "text": " the loss function directly into the neural net, into the model. See how the loss is being", "tokens": [50716, 264, 4470, 2445, 3838, 666, 264, 18161, 2533, 11, 666, 264, 2316, 13, 3008, 577, 264, 4470, 307, 885, 50996], "temperature": 0.0, "avg_logprob": -0.2082133943384344, "compression_ratio": 1.7022222222222223, "no_speech_prob": 8.750286360736936e-05}, {"id": 616, "seek": 401068, "start": 4023.3199999999997, "end": 4029.3199999999997, "text": " calculated inside the model? Now, that's neither better nor worse than having it separately. It's", "tokens": [50996, 15598, 1854, 264, 2316, 30, 823, 11, 300, 311, 9662, 1101, 6051, 5324, 813, 1419, 309, 14759, 13, 467, 311, 51296], "temperature": 0.0, "avg_logprob": -0.2082133943384344, "compression_ratio": 1.7022222222222223, "no_speech_prob": 8.750286360736936e-05}, {"id": 617, "seek": 401068, "start": 4029.3199999999997, "end": 4033.64, "text": " just different. And so generally a lot of hugging face stuff does it this way. They actually put the", "tokens": [51296, 445, 819, 13, 400, 370, 5101, 257, 688, 295, 41706, 1851, 1507, 775, 309, 341, 636, 13, 814, 767, 829, 264, 51512], "temperature": 0.0, "avg_logprob": -0.2082133943384344, "compression_ratio": 1.7022222222222223, "no_speech_prob": 8.750286360736936e-05}, {"id": 618, "seek": 403364, "start": 4033.64, "end": 4041.3199999999997, "text": " loss inside the forward. Most stuff in fast AI and a lot of other libraries does it separately,", "tokens": [50364, 4470, 1854, 264, 2128, 13, 4534, 1507, 294, 2370, 7318, 293, 257, 688, 295, 661, 15148, 775, 309, 14759, 11, 50748], "temperature": 0.0, "avg_logprob": -0.2191109798922397, "compression_ratio": 1.722466960352423, "no_speech_prob": 0.0103279585018754}, {"id": 619, "seek": 403364, "start": 4041.3199999999997, "end": 4045.72, "text": " which is the loss is a whole separate function and the model only returns the result of putting", "tokens": [50748, 597, 307, 264, 4470, 307, 257, 1379, 4994, 2445, 293, 264, 2316, 787, 11247, 264, 1874, 295, 3372, 50968], "temperature": 0.0, "avg_logprob": -0.2191109798922397, "compression_ratio": 1.722466960352423, "no_speech_prob": 0.0103279585018754}, {"id": 620, "seek": 403364, "start": 4045.72, "end": 4050.52, "text": " it through the layers. So for this model, we're going to actually do the loss function inside the", "tokens": [50968, 309, 807, 264, 7914, 13, 407, 337, 341, 2316, 11, 321, 434, 516, 281, 767, 360, 264, 4470, 2445, 1854, 264, 51208], "temperature": 0.0, "avg_logprob": -0.2191109798922397, "compression_ratio": 1.722466960352423, "no_speech_prob": 0.0103279585018754}, {"id": 621, "seek": 403364, "start": 4050.52, "end": 4058.68, "text": " model. So for backward, we just do each thing. So self.loss.backward. So self.loss is the MSE object.", "tokens": [51208, 2316, 13, 407, 337, 23897, 11, 321, 445, 360, 1184, 551, 13, 407, 2698, 13, 75, 772, 13, 3207, 1007, 13, 407, 2698, 13, 75, 772, 307, 264, 376, 5879, 2657, 13, 51616], "temperature": 0.0, "avg_logprob": -0.2191109798922397, "compression_ratio": 1.722466960352423, "no_speech_prob": 0.0103279585018754}, {"id": 622, "seek": 405868, "start": 4058.68, "end": 4064.8399999999997, "text": " So that's going to call backward, right? And it's stored when it was called here,", "tokens": [50364, 407, 300, 311, 516, 281, 818, 23897, 11, 558, 30, 400, 309, 311, 12187, 562, 309, 390, 1219, 510, 11, 50672], "temperature": 0.0, "avg_logprob": -0.2254589124657642, "compression_ratio": 1.6990291262135921, "no_speech_prob": 4.46940612164326e-05}, {"id": 623, "seek": 405868, "start": 4065.8799999999997, "end": 4071.3999999999996, "text": " it was storing, remember the inputs, the targets, the outputs, so it can calculate the backward.", "tokens": [50724, 309, 390, 26085, 11, 1604, 264, 15743, 11, 264, 12911, 11, 264, 23930, 11, 370, 309, 393, 8873, 264, 23897, 13, 51000], "temperature": 0.0, "avg_logprob": -0.2254589124657642, "compression_ratio": 1.6990291262135921, "no_speech_prob": 4.46940612164326e-05}, {"id": 624, "seek": 405868, "start": 4072.7599999999998, "end": 4077.3999999999996, "text": " And then we go through each layer is in reverse, right? This is back propagation,", "tokens": [51068, 400, 550, 321, 352, 807, 1184, 4583, 307, 294, 9943, 11, 558, 30, 639, 307, 646, 38377, 11, 51300], "temperature": 0.0, "avg_logprob": -0.2254589124657642, "compression_ratio": 1.6990291262135921, "no_speech_prob": 4.46940612164326e-05}, {"id": 625, "seek": 405868, "start": 4077.3999999999996, "end": 4084.52, "text": " backwards, reversed, calling backward on each one. So that's pretty interesting, I think.", "tokens": [51300, 12204, 11, 30563, 11, 5141, 23897, 322, 1184, 472, 13, 407, 300, 311, 1238, 1880, 11, 286, 519, 13, 51656], "temperature": 0.0, "avg_logprob": -0.2254589124657642, "compression_ratio": 1.6990291262135921, "no_speech_prob": 4.46940612164326e-05}, {"id": 626, "seek": 408868, "start": 4089.3199999999997, "end": 4100.28, "text": " So now we can calculate the model, we can calculate the loss, we can call backward,", "tokens": [50396, 407, 586, 321, 393, 8873, 264, 2316, 11, 321, 393, 8873, 264, 4470, 11, 321, 393, 818, 23897, 11, 50944], "temperature": 0.0, "avg_logprob": -0.275691582606389, "compression_ratio": 1.6666666666666667, "no_speech_prob": 6.2409171732724644e-06}, {"id": 627, "seek": 408868, "start": 4100.84, "end": 4106.599999999999, "text": " and then we can check that each of the gradients that we stored earlier", "tokens": [50972, 293, 550, 321, 393, 1520, 300, 1184, 295, 264, 2771, 2448, 300, 321, 12187, 3071, 51260], "temperature": 0.0, "avg_logprob": -0.275691582606389, "compression_ratio": 1.6666666666666667, "no_speech_prob": 6.2409171732724644e-06}, {"id": 628, "seek": 408868, "start": 4110.5199999999995, "end": 4113.4, "text": " are equal to each of our new gradients.", "tokens": [51456, 366, 2681, 281, 1184, 295, 527, 777, 2771, 2448, 13, 51600], "temperature": 0.0, "avg_logprob": -0.275691582606389, "compression_ratio": 1.6666666666666667, "no_speech_prob": 6.2409171732724644e-06}, {"id": 629, "seek": 411340, "start": 4114.2, "end": 4122.2, "text": " Okay, so Williams asked a very good question. That is, if you do put the loss inside here,", "tokens": [50404, 1033, 11, 370, 12929, 2351, 257, 588, 665, 1168, 13, 663, 307, 11, 498, 291, 360, 829, 264, 4470, 1854, 510, 11, 50804], "temperature": 0.0, "avg_logprob": -0.42632820129394533, "compression_ratio": 1.4307692307692308, "no_speech_prob": 0.00036829369491897523}, {"id": 630, "seek": 411340, "start": 4122.759999999999, "end": 4131.639999999999, "text": " how on earth do you actually get predictions? So generally, what happens is, in practice,", "tokens": [50832, 577, 322, 4120, 360, 291, 767, 483, 21264, 30, 407, 5101, 11, 437, 2314, 307, 11, 294, 3124, 11, 51276], "temperature": 0.0, "avg_logprob": -0.42632820129394533, "compression_ratio": 1.4307692307692308, "no_speech_prob": 0.00036829369491897523}, {"id": 631, "seek": 411340, "start": 4132.2, "end": 4140.599999999999, "text": " Hugging Face models do something like this. They'll say self.preds equals x. And then they'll say,", "tokens": [51304, 389, 3562, 278, 4047, 5245, 360, 746, 411, 341, 13, 814, 603, 584, 2698, 13, 79, 986, 82, 6915, 2031, 13, 400, 550, 436, 603, 584, 11, 51724], "temperature": 0.0, "avg_logprob": -0.42632820129394533, "compression_ratio": 1.4307692307692308, "no_speech_prob": 0.00036829369491897523}, {"id": 632, "seek": 414340, "start": 4143.4, "end": 4157.08, "text": " well, self.finalLoss equals that, and then return self.finalLoss. And that way,", "tokens": [50364, 731, 11, 2698, 13, 69, 2071, 43, 772, 6915, 300, 11, 293, 550, 2736, 2698, 13, 69, 2071, 43, 772, 13, 400, 300, 636, 11, 51048], "temperature": 0.0, "avg_logprob": -0.28721452894664945, "compression_ratio": 1.5284090909090908, "no_speech_prob": 0.00012148106907261536}, {"id": 633, "seek": 414340, "start": 4158.599999999999, "end": 4161.96, "text": " I guess you don't even need that last bit. Well, that's with them anyway, that is what they do. So", "tokens": [51124, 286, 2041, 291, 500, 380, 754, 643, 300, 1036, 857, 13, 1042, 11, 300, 311, 365, 552, 4033, 11, 300, 307, 437, 436, 360, 13, 407, 51292], "temperature": 0.0, "avg_logprob": -0.28721452894664945, "compression_ratio": 1.5284090909090908, "no_speech_prob": 0.00012148106907261536}, {"id": 634, "seek": 414340, "start": 4161.96, "end": 4171.24, "text": " we'll leave it there. And so that way you can kind of check like model.preds, for example.", "tokens": [51292, 321, 603, 1856, 309, 456, 13, 400, 370, 300, 636, 291, 393, 733, 295, 1520, 411, 2316, 13, 79, 986, 82, 11, 337, 1365, 13, 51756], "temperature": 0.0, "avg_logprob": -0.28721452894664945, "compression_ratio": 1.5284090909090908, "no_speech_prob": 0.00012148106907261536}, {"id": 635, "seek": 417124, "start": 4171.8, "end": 4176.36, "text": " So it'll be something like that. Or alternatively, you can return not just the loss,", "tokens": [50392, 407, 309, 603, 312, 746, 411, 300, 13, 1610, 8535, 356, 11, 291, 393, 2736, 406, 445, 264, 4470, 11, 50620], "temperature": 0.0, "avg_logprob": -0.19652778346364091, "compression_ratio": 1.6162162162162161, "no_speech_prob": 0.0004878545005340129}, {"id": 636, "seek": 417124, "start": 4176.36, "end": 4179.719999999999, "text": " but both as a dictionary, stuff like that. So there's a few different ways you could do it.", "tokens": [50620, 457, 1293, 382, 257, 25890, 11, 1507, 411, 300, 13, 407, 456, 311, 257, 1326, 819, 2098, 291, 727, 360, 309, 13, 50788], "temperature": 0.0, "avg_logprob": -0.19652778346364091, "compression_ratio": 1.6162162162162161, "no_speech_prob": 0.0004878545005340129}, {"id": 637, "seek": 417124, "start": 4182.36, "end": 4184.2, "text": " Actually, now I think about it, I think that's what they do,", "tokens": [50920, 5135, 11, 586, 286, 519, 466, 309, 11, 286, 519, 300, 311, 437, 436, 360, 11, 51012], "temperature": 0.0, "avg_logprob": -0.19652778346364091, "compression_ratio": 1.6162162162162161, "no_speech_prob": 0.0004878545005340129}, {"id": 638, "seek": 417124, "start": 4184.2, "end": 4187.719999999999, "text": " is they actually return both as a dictionary. So it would be,", "tokens": [51012, 307, 436, 767, 2736, 1293, 382, 257, 25890, 13, 407, 309, 576, 312, 11, 51188], "temperature": 0.0, "avg_logprob": -0.19652778346364091, "compression_ratio": 1.6162162162162161, "no_speech_prob": 0.0004878545005340129}, {"id": 639, "seek": 418772, "start": 4188.04, "end": 4208.52, "text": " um, it'd be like return dictionaryLoss equals that, comma, preds equals that.", "tokens": [50380, 1105, 11, 309, 1116, 312, 411, 2736, 25890, 43, 772, 6915, 300, 11, 22117, 11, 280, 986, 82, 6915, 300, 13, 51404], "temperature": 0.0, "avg_logprob": -0.2943324859325702, "compression_ratio": 1.330827067669173, "no_speech_prob": 0.003707203548401594}, {"id": 640, "seek": 418772, "start": 4209.400000000001, "end": 4214.2, "text": " Something like that, I guess, is what they would do. Anyway, there's a few different ways to do it.", "tokens": [51448, 6595, 411, 300, 11, 286, 2041, 11, 307, 437, 436, 576, 360, 13, 5684, 11, 456, 311, 257, 1326, 819, 2098, 281, 360, 309, 13, 51688], "temperature": 0.0, "avg_logprob": -0.2943324859325702, "compression_ratio": 1.330827067669173, "no_speech_prob": 0.003707203548401594}, {"id": 641, "seek": 421420, "start": 4215.0, "end": 4223.96, "text": " Um, okay. So hopefully you can see that this is really, um, making it nice and easy for us to do", "tokens": [50404, 3301, 11, 1392, 13, 407, 4696, 291, 393, 536, 300, 341, 307, 534, 11, 1105, 11, 1455, 309, 1481, 293, 1858, 337, 505, 281, 360, 50852], "temperature": 0.0, "avg_logprob": -0.24990036752488878, "compression_ratio": 1.5104166666666667, "no_speech_prob": 0.00017400427896063775}, {"id": 642, "seek": 421420, "start": 4223.96, "end": 4232.5199999999995, "text": " our forward pass and our backward pass without all of this manual fiddling around. Every class now", "tokens": [50852, 527, 2128, 1320, 293, 527, 23897, 1320, 1553, 439, 295, 341, 9688, 283, 14273, 1688, 926, 13, 2048, 1508, 586, 51280], "temperature": 0.0, "avg_logprob": -0.24990036752488878, "compression_ratio": 1.5104166666666667, "no_speech_prob": 0.00017400427896063775}, {"id": 643, "seek": 421420, "start": 4233.08, "end": 4241.08, "text": " can be totally separately considered, um, and can be combined however we want. We could create", "tokens": [51308, 393, 312, 3879, 14759, 4888, 11, 1105, 11, 293, 393, 312, 9354, 4461, 321, 528, 13, 492, 727, 1884, 51708], "temperature": 0.0, "avg_logprob": -0.24990036752488878, "compression_ratio": 1.5104166666666667, "no_speech_prob": 0.00017400427896063775}, {"id": 644, "seek": 424108, "start": 4241.08, "end": 4246.92, "text": " layers. So you could try creating a bigger neural net if you want to. But we can refactor it more.", "tokens": [50364, 7914, 13, 407, 291, 727, 853, 4084, 257, 3801, 18161, 2533, 498, 291, 528, 281, 13, 583, 321, 393, 1895, 15104, 309, 544, 13, 50656], "temperature": 0.0, "avg_logprob": -0.20260598754882814, "compression_ratio": 1.9059829059829059, "no_speech_prob": 0.020645635202527046}, {"id": 645, "seek": 424108, "start": 4246.92, "end": 4252.04, "text": " So basically, as a rule of thumb, when you see repeated code, self.imp equals imp,", "tokens": [50656, 407, 1936, 11, 382, 257, 4978, 295, 9298, 11, 562, 291, 536, 10477, 3089, 11, 2698, 13, 8814, 6915, 704, 11, 50912], "temperature": 0.0, "avg_logprob": -0.20260598754882814, "compression_ratio": 1.9059829059829059, "no_speech_prob": 0.020645635202527046}, {"id": 646, "seek": 424108, "start": 4253.08, "end": 4259.4, "text": " self.imp equals imp, self.out equals return self.out, self.out equals return self.out,", "tokens": [50964, 2698, 13, 8814, 6915, 704, 11, 2698, 13, 346, 6915, 2736, 2698, 13, 346, 11, 2698, 13, 346, 6915, 2736, 2698, 13, 346, 11, 51280], "temperature": 0.0, "avg_logprob": -0.20260598754882814, "compression_ratio": 1.9059829059829059, "no_speech_prob": 0.020645635202527046}, {"id": 647, "seek": 424108, "start": 4259.4, "end": 4265.0, "text": " that's a sign you can refactor things. And so what we can do is a simple refactoring,", "tokens": [51280, 300, 311, 257, 1465, 291, 393, 1895, 15104, 721, 13, 400, 370, 437, 321, 393, 360, 307, 257, 2199, 1895, 578, 3662, 11, 51560], "temperature": 0.0, "avg_logprob": -0.20260598754882814, "compression_ratio": 1.9059829059829059, "no_speech_prob": 0.020645635202527046}, {"id": 648, "seek": 424108, "start": 4265.0, "end": 4270.04, "text": " is to create a new class called module. And module's going to do those things we just said.", "tokens": [51560, 307, 281, 1884, 257, 777, 1508, 1219, 10088, 13, 400, 10088, 311, 516, 281, 360, 729, 721, 321, 445, 848, 13, 51812], "temperature": 0.0, "avg_logprob": -0.20260598754882814, "compression_ratio": 1.9059829059829059, "no_speech_prob": 0.020645635202527046}, {"id": 649, "seek": 427004, "start": 4270.04, "end": 4277.8, "text": " It's going to store the inputs, and it's going to call something called self.forward in order to", "tokens": [50364, 467, 311, 516, 281, 3531, 264, 15743, 11, 293, 309, 311, 516, 281, 818, 746, 1219, 2698, 13, 13305, 294, 1668, 281, 50752], "temperature": 0.0, "avg_logprob": -0.20220739534585783, "compression_ratio": 1.838095238095238, "no_speech_prob": 0.00012339436216279864}, {"id": 650, "seek": 427004, "start": 4277.8, "end": 4281.56, "text": " create our self.out, because remember that was one of the things we had again and again and again,", "tokens": [50752, 1884, 527, 2698, 13, 346, 11, 570, 1604, 300, 390, 472, 295, 264, 721, 321, 632, 797, 293, 797, 293, 797, 11, 50940], "temperature": 0.0, "avg_logprob": -0.20220739534585783, "compression_ratio": 1.838095238095238, "no_speech_prob": 0.00012339436216279864}, {"id": 651, "seek": 427004, "start": 4281.56, "end": 4289.56, "text": " self.out, self.out, and then return it. And so now there's going to be a thing called forward,", "tokens": [50940, 2698, 13, 346, 11, 2698, 13, 346, 11, 293, 550, 2736, 309, 13, 400, 370, 586, 456, 311, 516, 281, 312, 257, 551, 1219, 2128, 11, 51340], "temperature": 0.0, "avg_logprob": -0.20220739534585783, "compression_ratio": 1.838095238095238, "no_speech_prob": 0.00012339436216279864}, {"id": 652, "seek": 427004, "start": 4290.44, "end": 4295.88, "text": " which actually, in this, it doesn't do anything, because the whole purpose of this module is to", "tokens": [51384, 597, 767, 11, 294, 341, 11, 309, 1177, 380, 360, 1340, 11, 570, 264, 1379, 4334, 295, 341, 10088, 307, 281, 51656], "temperature": 0.0, "avg_logprob": -0.20220739534585783, "compression_ratio": 1.838095238095238, "no_speech_prob": 0.00012339436216279864}, {"id": 653, "seek": 429588, "start": 4295.88, "end": 4303.400000000001, "text": " be inherited. When we call backward, it's going to call self.backward passing in self.out, because", "tokens": [50364, 312, 27091, 13, 1133, 321, 818, 23897, 11, 309, 311, 516, 281, 818, 2698, 13, 3207, 1007, 8437, 294, 2698, 13, 346, 11, 570, 50740], "temperature": 0.0, "avg_logprob": -0.1806957631171504, "compression_ratio": 1.688235294117647, "no_speech_prob": 0.0011335661401972175}, {"id": 654, "seek": 429588, "start": 4303.400000000001, "end": 4312.6, "text": " notice all of our backwards always wanted to get hold of self.out, right, self.out, self.out,", "tokens": [50740, 3449, 439, 295, 527, 12204, 1009, 1415, 281, 483, 1797, 295, 2698, 13, 346, 11, 558, 11, 2698, 13, 346, 11, 2698, 13, 346, 11, 51200], "temperature": 0.0, "avg_logprob": -0.1806957631171504, "compression_ratio": 1.688235294117647, "no_speech_prob": 0.0011335661401972175}, {"id": 655, "seek": 429588, "start": 4312.6, "end": 4319.16, "text": " because we need it for the chain rule. So let's pass that in, and pass in those arguments that", "tokens": [51200, 570, 321, 643, 309, 337, 264, 5021, 4978, 13, 407, 718, 311, 1320, 300, 294, 11, 293, 1320, 294, 729, 12869, 300, 51528], "temperature": 0.0, "avg_logprob": -0.1806957631171504, "compression_ratio": 1.688235294117647, "no_speech_prob": 0.0011335661401972175}, {"id": 656, "seek": 431916, "start": 4319.16, "end": 4326.599999999999, "text": " we stored earlier. And so star means take all of the arguments, regardless whether it's zero, one,", "tokens": [50364, 321, 12187, 3071, 13, 400, 370, 3543, 1355, 747, 439, 295, 264, 12869, 11, 10060, 1968, 309, 311, 4018, 11, 472, 11, 50736], "temperature": 0.0, "avg_logprob": -0.19779037147439937, "compression_ratio": 1.6711111111111112, "no_speech_prob": 0.004198783542960882}, {"id": 657, "seek": 431916, "start": 4326.599999999999, "end": 4332.04, "text": " two, or more, and put them into a list. And then that's what happens when it's inside the actual", "tokens": [50736, 732, 11, 420, 544, 11, 293, 829, 552, 666, 257, 1329, 13, 400, 550, 300, 311, 437, 2314, 562, 309, 311, 1854, 264, 3539, 51008], "temperature": 0.0, "avg_logprob": -0.19779037147439937, "compression_ratio": 1.6711111111111112, "no_speech_prob": 0.004198783542960882}, {"id": 658, "seek": 431916, "start": 4332.04, "end": 4337.8, "text": " signature. And then when you call a function using star, it says take this list and expand", "tokens": [51008, 13397, 13, 400, 550, 562, 291, 818, 257, 2445, 1228, 3543, 11, 309, 1619, 747, 341, 1329, 293, 5268, 51296], "temperature": 0.0, "avg_logprob": -0.19779037147439937, "compression_ratio": 1.6711111111111112, "no_speech_prob": 0.004198783542960882}, {"id": 659, "seek": 431916, "start": 4337.8, "end": 4343.88, "text": " them into separate arguments, calling backward with each one separately. So now for relu,", "tokens": [51296, 552, 666, 4994, 12869, 11, 5141, 23897, 365, 1184, 472, 14759, 13, 407, 586, 337, 1039, 84, 11, 51600], "temperature": 0.0, "avg_logprob": -0.19779037147439937, "compression_ratio": 1.6711111111111112, "no_speech_prob": 0.004198783542960882}, {"id": 660, "seek": 434388, "start": 4343.88, "end": 4347.16, "text": " look how much simpler it is. Let's copy the old relu to the new relu.", "tokens": [50364, 574, 577, 709, 18587, 309, 307, 13, 961, 311, 5055, 264, 1331, 1039, 84, 281, 264, 777, 1039, 84, 13, 50528], "temperature": 0.0, "avg_logprob": -0.22005485866380775, "compression_ratio": 1.7956521739130435, "no_speech_prob": 0.0003150363336317241}, {"id": 661, "seek": 434388, "start": 4349.24, "end": 4352.28, "text": " So the old relu had to do all this storing stuff manually,", "tokens": [50632, 407, 264, 1331, 1039, 84, 632, 281, 360, 439, 341, 26085, 1507, 16945, 11, 50784], "temperature": 0.0, "avg_logprob": -0.22005485866380775, "compression_ratio": 1.7956521739130435, "no_speech_prob": 0.0003150363336317241}, {"id": 662, "seek": 434388, "start": 4353.8, "end": 4359.88, "text": " and it had all the self.stuff as well. But now we can get rid of all of that, and just implement", "tokens": [50860, 293, 309, 632, 439, 264, 2698, 13, 372, 1245, 382, 731, 13, 583, 586, 321, 393, 483, 3973, 295, 439, 295, 300, 11, 293, 445, 4445, 51164], "temperature": 0.0, "avg_logprob": -0.22005485866380775, "compression_ratio": 1.7956521739130435, "no_speech_prob": 0.0003150363336317241}, {"id": 663, "seek": 434388, "start": 4359.88, "end": 4363.96, "text": " forward, because that's the thing that's being called, and that's the thing that we need to", "tokens": [51164, 2128, 11, 570, 300, 311, 264, 551, 300, 311, 885, 1219, 11, 293, 300, 311, 264, 551, 300, 321, 643, 281, 51368], "temperature": 0.0, "avg_logprob": -0.22005485866380775, "compression_ratio": 1.7956521739130435, "no_speech_prob": 0.0003150363336317241}, {"id": 664, "seek": 434388, "start": 4363.96, "end": 4371.08, "text": " implement. And so now the forward of relu just does the one thing we want, which also makes the", "tokens": [51368, 4445, 13, 400, 370, 586, 264, 2128, 295, 1039, 84, 445, 775, 264, 472, 551, 321, 528, 11, 597, 611, 1669, 264, 51724], "temperature": 0.0, "avg_logprob": -0.22005485866380775, "compression_ratio": 1.7956521739130435, "no_speech_prob": 0.0003150363336317241}, {"id": 665, "seek": 437108, "start": 4371.08, "end": 4376.04, "text": " code much cleaner and more understandable. Ditto for backward, it just does the one thing we want.", "tokens": [50364, 3089, 709, 16532, 293, 544, 25648, 13, 413, 34924, 337, 23897, 11, 309, 445, 775, 264, 472, 551, 321, 528, 13, 50612], "temperature": 0.0, "avg_logprob": -0.2140341294117463, "compression_ratio": 1.5473684210526315, "no_speech_prob": 0.0003101537295151502}, {"id": 666, "seek": 437108, "start": 4377.64, "end": 4381.48, "text": " So that's nice. Now we still have to multiply it, but I still have to do the chain rule manually.", "tokens": [50692, 407, 300, 311, 1481, 13, 823, 321, 920, 362, 281, 12972, 309, 11, 457, 286, 920, 362, 281, 360, 264, 5021, 4978, 16945, 13, 50884], "temperature": 0.0, "avg_logprob": -0.2140341294117463, "compression_ratio": 1.5473684210526315, "no_speech_prob": 0.0003101537295151502}, {"id": 667, "seek": 437108, "start": 4383.64, "end": 4390.04, "text": " But so same thing for linear, same thing for MSE. So these all look a lot nicer. And one thing to", "tokens": [50992, 583, 370, 912, 551, 337, 8213, 11, 912, 551, 337, 376, 5879, 13, 407, 613, 439, 574, 257, 688, 22842, 13, 400, 472, 551, 281, 51312], "temperature": 0.0, "avg_logprob": -0.2140341294117463, "compression_ratio": 1.5473684210526315, "no_speech_prob": 0.0003101537295151502}, {"id": 668, "seek": 439004, "start": 4390.04, "end": 4401.4, "text": " point out here is that there's often opportunities to manually speed things up when you create", "tokens": [50364, 935, 484, 510, 307, 300, 456, 311, 2049, 4786, 281, 16945, 3073, 721, 493, 562, 291, 1884, 50932], "temperature": 0.0, "avg_logprob": -0.17639696236812707, "compression_ratio": 1.3934426229508197, "no_speech_prob": 0.0014779048506170511}, {"id": 669, "seek": 439004, "start": 4401.4, "end": 4406.84, "text": " custom autograd functions in PyTorch. And here's an example. Look, this calculation", "tokens": [50932, 2375, 1476, 664, 6206, 6828, 294, 9953, 51, 284, 339, 13, 400, 510, 311, 364, 1365, 13, 2053, 11, 341, 17108, 51204], "temperature": 0.0, "avg_logprob": -0.17639696236812707, "compression_ratio": 1.3934426229508197, "no_speech_prob": 0.0014779048506170511}, {"id": 670, "seek": 439004, "start": 4408.28, "end": 4414.28, "text": " is being done twice, which seems like a waste, doesn't it? So at the cost of", "tokens": [51276, 307, 885, 1096, 6091, 11, 597, 2544, 411, 257, 5964, 11, 1177, 380, 309, 30, 407, 412, 264, 2063, 295, 51576], "temperature": 0.0, "avg_logprob": -0.17639696236812707, "compression_ratio": 1.3934426229508197, "no_speech_prob": 0.0014779048506170511}, {"id": 671, "seek": 441428, "start": 4414.5199999999995, "end": 4421.639999999999, "text": " some memory, we could instead store that calculation as diff.", "tokens": [50376, 512, 4675, 11, 321, 727, 2602, 3531, 300, 17108, 382, 7593, 13, 50732], "temperature": 0.0, "avg_logprob": -0.3403561765497381, "compression_ratio": 1.2583333333333333, "no_speech_prob": 0.00021995203860569745}, {"id": 672, "seek": 441428, "start": 4432.04, "end": 4435.639999999999, "text": " Right? And I guess we'd have to store it for use later, so it would need to be self.diff.", "tokens": [51252, 1779, 30, 400, 286, 2041, 321, 1116, 362, 281, 3531, 309, 337, 764, 1780, 11, 370, 309, 576, 643, 281, 312, 2698, 13, 67, 3661, 13, 51432], "temperature": 0.0, "avg_logprob": -0.3403561765497381, "compression_ratio": 1.2583333333333333, "no_speech_prob": 0.00021995203860569745}, {"id": 673, "seek": 443564, "start": 4435.72, "end": 4439.240000000001, "text": " And at the cost of that memory,", "tokens": [50368, 400, 412, 264, 2063, 295, 300, 4675, 11, 50544], "temperature": 0.0, "avg_logprob": -0.23192350214177912, "compression_ratio": 1.4701986754966887, "no_speech_prob": 0.00026118956157006323}, {"id": 674, "seek": 443564, "start": 4443.08, "end": 4450.04, "text": " we could now remove this redundant calculation, because we've done it once before already and", "tokens": [50736, 321, 727, 586, 4159, 341, 40997, 17108, 11, 570, 321, 600, 1096, 309, 1564, 949, 1217, 293, 51084], "temperature": 0.0, "avg_logprob": -0.23192350214177912, "compression_ratio": 1.4701986754966887, "no_speech_prob": 0.00026118956157006323}, {"id": 675, "seek": 443564, "start": 4450.04, "end": 4461.240000000001, "text": " stored it, and just use it directly. And this is something that you can often do in neural nets.", "tokens": [51084, 12187, 309, 11, 293, 445, 764, 309, 3838, 13, 400, 341, 307, 746, 300, 291, 393, 2049, 360, 294, 18161, 36170, 13, 51644], "temperature": 0.0, "avg_logprob": -0.23192350214177912, "compression_ratio": 1.4701986754966887, "no_speech_prob": 0.00026118956157006323}, {"id": 676, "seek": 446124, "start": 4461.88, "end": 4471.8, "text": " So there's this compromise between storing things, the memory use of that, and then the", "tokens": [50396, 407, 456, 311, 341, 18577, 1296, 26085, 721, 11, 264, 4675, 764, 295, 300, 11, 293, 550, 264, 50892], "temperature": 0.0, "avg_logprob": -0.23923002544202301, "compression_ratio": 1.6008771929824561, "no_speech_prob": 0.0009547182125970721}, {"id": 677, "seek": 446124, "start": 4472.76, "end": 4477.48, "text": " computational speed up of not having to recalculate it. This is something we come across a lot.", "tokens": [50940, 28270, 3073, 493, 295, 406, 1419, 281, 850, 304, 2444, 473, 309, 13, 639, 307, 746, 321, 808, 2108, 257, 688, 13, 51176], "temperature": 0.0, "avg_logprob": -0.23923002544202301, "compression_ratio": 1.6008771929824561, "no_speech_prob": 0.0009547182125970721}, {"id": 678, "seek": 446124, "start": 4478.04, "end": 4482.84, "text": " And so now we can call it the same way, create our model, passing in all of those layers.", "tokens": [51204, 400, 370, 586, 321, 393, 818, 309, 264, 912, 636, 11, 1884, 527, 2316, 11, 8437, 294, 439, 295, 729, 7914, 13, 51444], "temperature": 0.0, "avg_logprob": -0.23923002544202301, "compression_ratio": 1.6008771929824561, "no_speech_prob": 0.0009547182125970721}, {"id": 679, "seek": 446124, "start": 4483.5599999999995, "end": 4488.76, "text": " Right? So you can see with our model, we're just so the model hasn't changed at this point.", "tokens": [51480, 1779, 30, 407, 291, 393, 536, 365, 527, 2316, 11, 321, 434, 445, 370, 264, 2316, 6132, 380, 3105, 412, 341, 935, 13, 51740], "temperature": 0.0, "avg_logprob": -0.23923002544202301, "compression_ratio": 1.6008771929824561, "no_speech_prob": 0.0009547182125970721}, {"id": 680, "seek": 448876, "start": 4489.320000000001, "end": 4497.400000000001, "text": " The definition was up here. We just pass in the layers. Sorry, not the layers, the weights for the", "tokens": [50392, 440, 7123, 390, 493, 510, 13, 492, 445, 1320, 294, 264, 7914, 13, 4919, 11, 406, 264, 7914, 11, 264, 17443, 337, 264, 50796], "temperature": 0.0, "avg_logprob": -0.2672580139977591, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.0004044637898914516}, {"id": 681, "seek": 448876, "start": 4497.400000000001, "end": 4514.2, "text": " layers. Calculate the loss, call backward. And look, it's the same. Hooray! Okay. So,", "tokens": [50796, 7914, 13, 3511, 2444, 473, 264, 4470, 11, 818, 23897, 13, 400, 574, 11, 309, 311, 264, 912, 13, 3631, 284, 320, 0, 1033, 13, 407, 11, 51636], "temperature": 0.0, "avg_logprob": -0.2672580139977591, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.0004044637898914516}, {"id": 682, "seek": 451420, "start": 4515.16, "end": 4520.679999999999, "text": " um, thankfully, PyTorch has written all this for us. And remember, according to rules of our game,", "tokens": [50412, 1105, 11, 27352, 11, 9953, 51, 284, 339, 575, 3720, 439, 341, 337, 505, 13, 400, 1604, 11, 4650, 281, 4474, 295, 527, 1216, 11, 50688], "temperature": 0.0, "avg_logprob": -0.21234399546747623, "compression_ratio": 1.6483050847457628, "no_speech_prob": 0.0005112527287565172}, {"id": 683, "seek": 451420, "start": 4520.679999999999, "end": 4525.08, "text": " once we've re-implemented it, we're allowed to use PyTorch's version. So PyTorch calls their version", "tokens": [50688, 1564, 321, 600, 319, 12, 332, 781, 14684, 309, 11, 321, 434, 4350, 281, 764, 9953, 51, 284, 339, 311, 3037, 13, 407, 9953, 51, 284, 339, 5498, 641, 3037, 50908], "temperature": 0.0, "avg_logprob": -0.21234399546747623, "compression_ratio": 1.6483050847457628, "no_speech_prob": 0.0005112527287565172}, {"id": 684, "seek": 451420, "start": 4525.639999999999, "end": 4534.76, "text": " nn.module. And so it's exactly the same. You inherit from nn.module. So if we want to create", "tokens": [50936, 297, 77, 13, 8014, 2271, 13, 400, 370, 309, 311, 2293, 264, 912, 13, 509, 21389, 490, 297, 77, 13, 8014, 2271, 13, 407, 498, 321, 528, 281, 1884, 51392], "temperature": 0.0, "avg_logprob": -0.21234399546747623, "compression_ratio": 1.6483050847457628, "no_speech_prob": 0.0005112527287565172}, {"id": 685, "seek": 451420, "start": 4534.76, "end": 4541.16, "text": " a linear layer, just like this one, rather than inheriting from our module, we will inherit from", "tokens": [51392, 257, 8213, 4583, 11, 445, 411, 341, 472, 11, 2831, 813, 9484, 1748, 490, 527, 10088, 11, 321, 486, 21389, 490, 51712], "temperature": 0.0, "avg_logprob": -0.21234399546747623, "compression_ratio": 1.6483050847457628, "no_speech_prob": 0.0005112527287565172}, {"id": 686, "seek": 454116, "start": 4541.16, "end": 4549.08, "text": " their module. But everything's exactly the same. So we create our, we can create our random numbers.", "tokens": [50364, 641, 10088, 13, 583, 1203, 311, 2293, 264, 912, 13, 407, 321, 1884, 527, 11, 321, 393, 1884, 527, 4974, 3547, 13, 50760], "temperature": 0.0, "avg_logprob": -0.23779328998766447, "compression_ratio": 1.6767241379310345, "no_speech_prob": 0.00030061547295190394}, {"id": 687, "seek": 454116, "start": 4549.08, "end": 4552.76, "text": " So in this case, rather than passing in the already randomized weights, we're actually going", "tokens": [50760, 407, 294, 341, 1389, 11, 2831, 813, 8437, 294, 264, 1217, 38513, 17443, 11, 321, 434, 767, 516, 50944], "temperature": 0.0, "avg_logprob": -0.23779328998766447, "compression_ratio": 1.6767241379310345, "no_speech_prob": 0.00030061547295190394}, {"id": 688, "seek": 454116, "start": 4552.76, "end": 4559.4, "text": " to generate the random weights ourselves. And the zeroed biases. And then here's our linear layer,", "tokens": [50944, 281, 8460, 264, 4974, 17443, 4175, 13, 400, 264, 4018, 292, 32152, 13, 400, 550, 510, 311, 527, 8213, 4583, 11, 51276], "temperature": 0.0, "avg_logprob": -0.23779328998766447, "compression_ratio": 1.6767241379310345, "no_speech_prob": 0.00030061547295190394}, {"id": 689, "seek": 454116, "start": 4560.2, "end": 4566.2, "text": " which you could also use lin for that, of course. So we've defined our forward. And why don't we", "tokens": [51316, 597, 291, 727, 611, 764, 22896, 337, 300, 11, 295, 1164, 13, 407, 321, 600, 7642, 527, 2128, 13, 400, 983, 500, 380, 321, 51616], "temperature": 0.0, "avg_logprob": -0.23779328998766447, "compression_ratio": 1.6767241379310345, "no_speech_prob": 0.00030061547295190394}, {"id": 690, "seek": 456620, "start": 4566.2, "end": 4577.24, "text": " need to define backward? Because PyTorch already knows the derivatives of all of the functions", "tokens": [50364, 643, 281, 6964, 23897, 30, 1436, 9953, 51, 284, 339, 1217, 3255, 264, 33733, 295, 439, 295, 264, 6828, 50916], "temperature": 0.0, "avg_logprob": -0.1654706319173177, "compression_ratio": 1.6394849785407726, "no_speech_prob": 0.00031503711943514645}, {"id": 691, "seek": 456620, "start": 4577.24, "end": 4582.76, "text": " in PyTorch. And it knows how to use the chain rule. So we don't have to do the backward at all.", "tokens": [50916, 294, 9953, 51, 284, 339, 13, 400, 309, 3255, 577, 281, 764, 264, 5021, 4978, 13, 407, 321, 500, 380, 362, 281, 360, 264, 23897, 412, 439, 13, 51192], "temperature": 0.0, "avg_logprob": -0.1654706319173177, "compression_ratio": 1.6394849785407726, "no_speech_prob": 0.00031503711943514645}, {"id": 692, "seek": 456620, "start": 4582.76, "end": 4589.16, "text": " It'll actually do that entirely for us. Which is very cool. So we only need forward. We don't", "tokens": [51192, 467, 603, 767, 360, 300, 7696, 337, 505, 13, 3013, 307, 588, 1627, 13, 407, 321, 787, 643, 2128, 13, 492, 500, 380, 51512], "temperature": 0.0, "avg_logprob": -0.1654706319173177, "compression_ratio": 1.6394849785407726, "no_speech_prob": 0.00031503711943514645}, {"id": 693, "seek": 456620, "start": 4589.16, "end": 4595.8, "text": " need backward. So let's create a model that uses that nn.module. Otherwise, it's exactly the same", "tokens": [51512, 643, 23897, 13, 407, 718, 311, 1884, 257, 2316, 300, 4960, 300, 297, 77, 13, 8014, 2271, 13, 10328, 11, 309, 311, 2293, 264, 912, 51844], "temperature": 0.0, "avg_logprob": -0.1654706319173177, "compression_ratio": 1.6394849785407726, "no_speech_prob": 0.00031503711943514645}, {"id": 694, "seek": 459580, "start": 4595.8, "end": 4600.84, "text": " as before. And now we're going to use PyTorch's mseloss, because we've already implemented", "tokens": [50364, 382, 949, 13, 400, 586, 321, 434, 516, 281, 764, 9953, 51, 284, 339, 311, 275, 790, 772, 11, 570, 321, 600, 1217, 12270, 50616], "temperature": 0.0, "avg_logprob": -0.2270118168422154, "compression_ratio": 1.5726872246696035, "no_speech_prob": 0.0010817085858434439}, {"id": 695, "seek": 459580, "start": 4600.84, "end": 4607.08, "text": " ourselves. It's very common to use torch.nn.functional as capital F. This is where", "tokens": [50616, 4175, 13, 467, 311, 588, 2689, 281, 764, 27822, 13, 26384, 13, 22845, 304, 382, 4238, 479, 13, 639, 307, 689, 50928], "temperature": 0.0, "avg_logprob": -0.2270118168422154, "compression_ratio": 1.5726872246696035, "no_speech_prob": 0.0010817085858434439}, {"id": 696, "seek": 459580, "start": 4607.8, "end": 4615.16, "text": " lots of these handy functions live, including mseloss. And so now you know why we need the", "tokens": [50964, 3195, 295, 613, 13239, 6828, 1621, 11, 3009, 275, 790, 772, 13, 400, 370, 586, 291, 458, 983, 321, 643, 264, 51332], "temperature": 0.0, "avg_logprob": -0.2270118168422154, "compression_ratio": 1.5726872246696035, "no_speech_prob": 0.0010817085858434439}, {"id": 697, "seek": 459580, "start": 4615.16, "end": 4620.92, "text": " colon common none, because you saw the problem if we don't have it. And so create the model,", "tokens": [51332, 8255, 2689, 6022, 11, 570, 291, 1866, 264, 1154, 498, 321, 500, 380, 362, 309, 13, 400, 370, 1884, 264, 2316, 11, 51620], "temperature": 0.0, "avg_logprob": -0.2270118168422154, "compression_ratio": 1.5726872246696035, "no_speech_prob": 0.0010817085858434439}, {"id": 698, "seek": 462092, "start": 4620.92, "end": 4625.72, "text": " call backward. And remember, we stored our gradients in something called dot G.", "tokens": [50364, 818, 23897, 13, 400, 1604, 11, 321, 12187, 527, 2771, 2448, 294, 746, 1219, 5893, 460, 13, 50604], "temperature": 0.0, "avg_logprob": -0.22516598908797555, "compression_ratio": 1.5679012345679013, "no_speech_prob": 0.0004238830297254026}, {"id": 699, "seek": 462092, "start": 4627.96, "end": 4632.92, "text": " PyTorch stores them in something called dot grad. But it's doing exactly the same thing.", "tokens": [50716, 9953, 51, 284, 339, 9512, 552, 294, 746, 1219, 5893, 2771, 13, 583, 309, 311, 884, 2293, 264, 912, 551, 13, 50964], "temperature": 0.0, "avg_logprob": -0.22516598908797555, "compression_ratio": 1.5679012345679013, "no_speech_prob": 0.0004238830297254026}, {"id": 700, "seek": 462092, "start": 4632.92, "end": 4642.76, "text": " So there is the exact same values. So let's take stock of where we're up to. So we've", "tokens": [50964, 407, 456, 307, 264, 1900, 912, 4190, 13, 407, 718, 311, 747, 4127, 295, 689, 321, 434, 493, 281, 13, 407, 321, 600, 51456], "temperature": 0.0, "avg_logprob": -0.22516598908797555, "compression_ratio": 1.5679012345679013, "no_speech_prob": 0.0004238830297254026}, {"id": 701, "seek": 464276, "start": 4643.72, "end": 4649.88, "text": " we've created a matrix multiplication from scratch. We've created linear layers. We've", "tokens": [50412, 321, 600, 2942, 257, 8141, 27290, 490, 8459, 13, 492, 600, 2942, 8213, 7914, 13, 492, 600, 50720], "temperature": 0.0, "avg_logprob": -0.19059626261393228, "compression_ratio": 1.7461928934010151, "no_speech_prob": 0.011868626810610294}, {"id": 702, "seek": 464276, "start": 4649.88, "end": 4657.400000000001, "text": " created a complete backprop system of modules. We can now calculate both the forward pass and", "tokens": [50720, 2942, 257, 3566, 646, 79, 1513, 1185, 295, 16679, 13, 492, 393, 586, 8873, 1293, 264, 2128, 1320, 293, 51096], "temperature": 0.0, "avg_logprob": -0.19059626261393228, "compression_ratio": 1.7461928934010151, "no_speech_prob": 0.011868626810610294}, {"id": 703, "seek": 464276, "start": 4657.400000000001, "end": 4662.84, "text": " the backward pass for linear layers and values. So we can create a multi-layer perceptron.", "tokens": [51096, 264, 23897, 1320, 337, 8213, 7914, 293, 4190, 13, 407, 321, 393, 1884, 257, 4825, 12, 8376, 260, 43276, 2044, 13, 51368], "temperature": 0.0, "avg_logprob": -0.19059626261393228, "compression_ratio": 1.7461928934010151, "no_speech_prob": 0.011868626810610294}, {"id": 704, "seek": 464276, "start": 4663.88, "end": 4669.64, "text": " So we're now up to a point where we can train a model. So let's do that.", "tokens": [51420, 407, 321, 434, 586, 493, 281, 257, 935, 689, 321, 393, 3847, 257, 2316, 13, 407, 718, 311, 360, 300, 13, 51708], "temperature": 0.0, "avg_logprob": -0.19059626261393228, "compression_ratio": 1.7461928934010151, "no_speech_prob": 0.011868626810610294}, {"id": 705, "seek": 467276, "start": 4673.400000000001, "end": 4679.88, "text": " So many batch training, notebook number four. So same first cell as before. We won't go through it.", "tokens": [50396, 407, 867, 15245, 3097, 11, 21060, 1230, 1451, 13, 407, 912, 700, 2815, 382, 949, 13, 492, 1582, 380, 352, 807, 309, 13, 50720], "temperature": 0.0, "avg_logprob": -0.2746093296768642, "compression_ratio": 1.7427184466019416, "no_speech_prob": 9.46122090681456e-05}, {"id": 706, "seek": 467276, "start": 4680.68, "end": 4685.16, "text": " This cell is also the same as before. So we won't go through it. Here's the same model", "tokens": [50760, 639, 2815, 307, 611, 264, 912, 382, 949, 13, 407, 321, 1582, 380, 352, 807, 309, 13, 1692, 311, 264, 912, 2316, 50984], "temperature": 0.0, "avg_logprob": -0.2746093296768642, "compression_ratio": 1.7427184466019416, "no_speech_prob": 9.46122090681456e-05}, {"id": 707, "seek": 467276, "start": 4685.16, "end": 4689.16, "text": " that we had before. So we won't go through it. So just rerunning all that to see.", "tokens": [50984, 300, 321, 632, 949, 13, 407, 321, 1582, 380, 352, 807, 309, 13, 407, 445, 43819, 25589, 439, 300, 281, 536, 13, 51184], "temperature": 0.0, "avg_logprob": -0.2746093296768642, "compression_ratio": 1.7427184466019416, "no_speech_prob": 9.46122090681456e-05}, {"id": 708, "seek": 467276, "start": 4690.92, "end": 4697.0, "text": " OK. So the first thing we should do, I think, is to improve our loss function. So it's not", "tokens": [51272, 2264, 13, 407, 264, 700, 551, 321, 820, 360, 11, 286, 519, 11, 307, 281, 3470, 527, 4470, 2445, 13, 407, 309, 311, 406, 51576], "temperature": 0.0, "avg_logprob": -0.2746093296768642, "compression_ratio": 1.7427184466019416, "no_speech_prob": 9.46122090681456e-05}, {"id": 709, "seek": 469700, "start": 4697.0, "end": 4706.28, "text": " total rubbish anymore. So if you watched part one, you might recall that there are", "tokens": [50364, 3217, 29978, 3602, 13, 407, 498, 291, 6337, 644, 472, 11, 291, 1062, 9901, 300, 456, 366, 50828], "temperature": 0.0, "avg_logprob": -0.20461082458496094, "compression_ratio": 1.5085714285714287, "no_speech_prob": 0.03308230638504028}, {"id": 710, "seek": 469700, "start": 4706.28, "end": 4712.12, "text": " some Excel notebooks. One of those Excel notebooks is Entropy Example.", "tokens": [50828, 512, 19060, 43782, 13, 1485, 295, 729, 19060, 43782, 307, 3951, 27514, 24755, 781, 13, 51120], "temperature": 0.0, "avg_logprob": -0.20461082458496094, "compression_ratio": 1.5085714285714287, "no_speech_prob": 0.03308230638504028}, {"id": 711, "seek": 469700, "start": 4715.48, "end": 4717.96, "text": " OK. So this is what we looked at. So just to remind you,", "tokens": [51288, 2264, 13, 407, 341, 307, 437, 321, 2956, 412, 13, 407, 445, 281, 4160, 291, 11, 51412], "temperature": 0.0, "avg_logprob": -0.20461082458496094, "compression_ratio": 1.5085714285714287, "no_speech_prob": 0.03308230638504028}, {"id": 712, "seek": 469700, "start": 4719.64, "end": 4723.96, "text": " what we're doing now is we're saying, OK, rather than", "tokens": [51496, 437, 321, 434, 884, 586, 307, 321, 434, 1566, 11, 2264, 11, 2831, 813, 51712], "temperature": 0.0, "avg_logprob": -0.20461082458496094, "compression_ratio": 1.5085714285714287, "no_speech_prob": 0.03308230638504028}, {"id": 713, "seek": 472700, "start": 4727.48, "end": 4737.72, "text": " outputting a single number for each image, we're going to instead output", "tokens": [50388, 5598, 783, 257, 2167, 1230, 337, 1184, 3256, 11, 321, 434, 516, 281, 2602, 5598, 50900], "temperature": 0.0, "avg_logprob": -0.2446064233779907, "compression_ratio": 1.349056603773585, "no_speech_prob": 1.1843071661132853e-05}, {"id": 714, "seek": 472700, "start": 4740.68, "end": 4747.32, "text": " 10 numbers for each image. And so that's going to be a one-hot encoded", "tokens": [51048, 1266, 3547, 337, 1184, 3256, 13, 400, 370, 300, 311, 516, 281, 312, 257, 472, 12, 12194, 2058, 12340, 51380], "temperature": 0.0, "avg_logprob": -0.2446064233779907, "compression_ratio": 1.349056603773585, "no_speech_prob": 1.1843071661132853e-05}, {"id": 715, "seek": 474732, "start": 4748.28, "end": 4757.799999999999, "text": " set of, it'll be like 1, 0, 0, 0, et cetera. And so then that's going to be, so well, actually,", "tokens": [50412, 992, 295, 11, 309, 603, 312, 411, 502, 11, 1958, 11, 1958, 11, 1958, 11, 1030, 11458, 13, 400, 370, 550, 300, 311, 516, 281, 312, 11, 370, 731, 11, 767, 11, 50888], "temperature": 0.0, "avg_logprob": -0.24365660156866517, "compression_ratio": 1.5956284153005464, "no_speech_prob": 0.0007672801730223}, {"id": 716, "seek": 474732, "start": 4757.799999999999, "end": 4762.759999999999, "text": " the outputs won't be 1, 0, 0. They'll be basically probabilities, won't they? So it'll be like 0.99,", "tokens": [50888, 264, 23930, 1582, 380, 312, 502, 11, 1958, 11, 1958, 13, 814, 603, 312, 1936, 33783, 11, 1582, 380, 436, 30, 407, 309, 603, 312, 411, 1958, 13, 8494, 11, 51136], "temperature": 0.0, "avg_logprob": -0.24365660156866517, "compression_ratio": 1.5956284153005464, "no_speech_prob": 0.0007672801730223}, {"id": 717, "seek": 474732, "start": 4766.36, "end": 4773.88, "text": " comma, you know, 0.01, et cetera. And the targets will be one-hot encoded. So if it's the digit", "tokens": [51316, 22117, 11, 291, 458, 11, 1958, 13, 10607, 11, 1030, 11458, 13, 400, 264, 12911, 486, 312, 472, 12, 12194, 2058, 12340, 13, 407, 498, 309, 311, 264, 14293, 51692], "temperature": 0.0, "avg_logprob": -0.24365660156866517, "compression_ratio": 1.5956284153005464, "no_speech_prob": 0.0007672801730223}, {"id": 718, "seek": 477388, "start": 4774.6, "end": 4782.52, "text": " 0, for example, it might be 1, 0, 0, 0, 0, dot, dot, dot, for all the 10 possibilities.", "tokens": [50400, 1958, 11, 337, 1365, 11, 309, 1062, 312, 502, 11, 1958, 11, 1958, 11, 1958, 11, 1958, 11, 5893, 11, 5893, 11, 5893, 11, 337, 439, 264, 1266, 12178, 13, 50796], "temperature": 0.0, "avg_logprob": -0.2001578860812717, "compression_ratio": 1.5081967213114753, "no_speech_prob": 9.027971827890724e-05}, {"id": 719, "seek": 477388, "start": 4783.32, "end": 4788.52, "text": " And so to see, you know, how good is it? So in this case, it's really good. It had a 0.99", "tokens": [50836, 400, 370, 281, 536, 11, 291, 458, 11, 577, 665, 307, 309, 30, 407, 294, 341, 1389, 11, 309, 311, 534, 665, 13, 467, 632, 257, 1958, 13, 8494, 51096], "temperature": 0.0, "avg_logprob": -0.2001578860812717, "compression_ratio": 1.5081967213114753, "no_speech_prob": 9.027971827890724e-05}, {"id": 720, "seek": 477388, "start": 4788.52, "end": 4794.6, "text": " probability prediction that it's 0. And indeed it is, because this is the one-hot encoded version.", "tokens": [51096, 8482, 17630, 300, 309, 311, 1958, 13, 400, 6451, 309, 307, 11, 570, 341, 307, 264, 472, 12, 12194, 2058, 12340, 3037, 13, 51400], "temperature": 0.0, "avg_logprob": -0.2001578860812717, "compression_ratio": 1.5081967213114753, "no_speech_prob": 9.027971827890724e-05}, {"id": 721, "seek": 479460, "start": 4795.56, "end": 4804.280000000001, "text": " And so the way we implement that is we don't even need to actually do the one-hot encoding,", "tokens": [50412, 400, 370, 264, 636, 321, 4445, 300, 307, 321, 500, 380, 754, 643, 281, 767, 360, 264, 472, 12, 12194, 43430, 11, 50848], "temperature": 0.0, "avg_logprob": -0.18229293823242188, "compression_ratio": 1.608433734939759, "no_speech_prob": 0.01717517338693142}, {"id": 722, "seek": 479460, "start": 4805.400000000001, "end": 4810.200000000001, "text": " thanks to some tricks. We can actually just directly store the integer, but we can treat", "tokens": [50904, 3231, 281, 512, 11733, 13, 492, 393, 767, 445, 3838, 3531, 264, 24922, 11, 457, 321, 393, 2387, 51144], "temperature": 0.0, "avg_logprob": -0.18229293823242188, "compression_ratio": 1.608433734939759, "no_speech_prob": 0.01717517338693142}, {"id": 723, "seek": 479460, "start": 4810.200000000001, "end": 4816.6, "text": " it as if it's one-hot encoded. So we can just store the actual target 0 as an integer.", "tokens": [51144, 309, 382, 498, 309, 311, 472, 12, 12194, 2058, 12340, 13, 407, 321, 393, 445, 3531, 264, 3539, 3779, 1958, 382, 364, 24922, 13, 51464], "temperature": 0.0, "avg_logprob": -0.18229293823242188, "compression_ratio": 1.608433734939759, "no_speech_prob": 0.01717517338693142}, {"id": 724, "seek": 481660, "start": 4817.4800000000005, "end": 4825.4800000000005, "text": " So the way we do that is we say, for example, for a single output,", "tokens": [50408, 407, 264, 636, 321, 360, 300, 307, 321, 584, 11, 337, 1365, 11, 337, 257, 2167, 5598, 11, 50808], "temperature": 0.0, "avg_logprob": -0.19287183549669054, "compression_ratio": 1.5748502994011977, "no_speech_prob": 0.002631392562761903}, {"id": 725, "seek": 481660, "start": 4826.200000000001, "end": 4832.6, "text": " oh, it could be cat, let's say, cat, dog, plane, fish, building. The neural net spits out a bunch", "tokens": [50844, 1954, 11, 309, 727, 312, 3857, 11, 718, 311, 584, 11, 3857, 11, 3000, 11, 5720, 11, 3506, 11, 2390, 13, 440, 18161, 2533, 637, 1208, 484, 257, 3840, 51164], "temperature": 0.0, "avg_logprob": -0.19287183549669054, "compression_ratio": 1.5748502994011977, "no_speech_prob": 0.002631392562761903}, {"id": 726, "seek": 481660, "start": 4832.6, "end": 4844.360000000001, "text": " of outputs. What we do for Softmax is we go e to the power of each of those outputs. We sum up all", "tokens": [51164, 295, 23930, 13, 708, 321, 360, 337, 16985, 41167, 307, 321, 352, 308, 281, 264, 1347, 295, 1184, 295, 729, 23930, 13, 492, 2408, 493, 439, 51752], "temperature": 0.0, "avg_logprob": -0.19287183549669054, "compression_ratio": 1.5748502994011977, "no_speech_prob": 0.002631392562761903}, {"id": 727, "seek": 484436, "start": 4844.36, "end": 4849.0, "text": " of those e to the power ofs. So here's the e to the power of each of those outputs. Here's the", "tokens": [50364, 295, 729, 308, 281, 264, 1347, 295, 82, 13, 407, 510, 311, 264, 308, 281, 264, 1347, 295, 1184, 295, 729, 23930, 13, 1692, 311, 264, 50596], "temperature": 0.0, "avg_logprob": -0.1904906532981179, "compression_ratio": 1.8146341463414635, "no_speech_prob": 4.133524271310307e-05}, {"id": 728, "seek": 484436, "start": 4849.0, "end": 4858.36, "text": " sum of them. And then we divide one, each one by the sum. So divide each one by the sum.", "tokens": [50596, 2408, 295, 552, 13, 400, 550, 321, 9845, 472, 11, 1184, 472, 538, 264, 2408, 13, 407, 9845, 1184, 472, 538, 264, 2408, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1904906532981179, "compression_ratio": 1.8146341463414635, "no_speech_prob": 4.133524271310307e-05}, {"id": 729, "seek": 484436, "start": 4860.28, "end": 4868.12, "text": " That gives us our Softmaxes. And then for the loss function, we then compare those Softmaxes", "tokens": [51160, 663, 2709, 505, 527, 16985, 41167, 279, 13, 400, 550, 337, 264, 4470, 2445, 11, 321, 550, 6794, 729, 16985, 41167, 279, 51552], "temperature": 0.0, "avg_logprob": -0.1904906532981179, "compression_ratio": 1.8146341463414635, "no_speech_prob": 4.133524271310307e-05}, {"id": 730, "seek": 484436, "start": 4869.16, "end": 4873.96, "text": " to the one-hot encoded version. So let's say it was a dog, then it's going to have a 1 for dog.", "tokens": [51604, 281, 264, 472, 12, 12194, 2058, 12340, 3037, 13, 407, 718, 311, 584, 309, 390, 257, 3000, 11, 550, 309, 311, 516, 281, 362, 257, 502, 337, 3000, 13, 51844], "temperature": 0.0, "avg_logprob": -0.1904906532981179, "compression_ratio": 1.8146341463414635, "no_speech_prob": 4.133524271310307e-05}, {"id": 731, "seek": 487436, "start": 4874.44, "end": 4883.799999999999, "text": " And 0 everywhere else. And then Softmax, this is from this nice blog post here.", "tokens": [50368, 400, 1958, 5315, 1646, 13, 400, 550, 16985, 41167, 11, 341, 307, 490, 341, 1481, 6968, 2183, 510, 13, 50836], "temperature": 0.0, "avg_logprob": -0.22200538266089656, "compression_ratio": 1.5217391304347827, "no_speech_prob": 1.5936609997879714e-05}, {"id": 732, "seek": 487436, "start": 4887.32, "end": 4894.04, "text": " This is the calculation sum of the 1s and 0s. So each of the 1s and 0s,", "tokens": [51012, 639, 307, 264, 17108, 2408, 295, 264, 502, 82, 293, 1958, 82, 13, 407, 1184, 295, 264, 502, 82, 293, 1958, 82, 11, 51348], "temperature": 0.0, "avg_logprob": -0.22200538266089656, "compression_ratio": 1.5217391304347827, "no_speech_prob": 1.5936609997879714e-05}, {"id": 733, "seek": 487436, "start": 4894.679999999999, "end": 4898.599999999999, "text": " multiplied by the log of the probabilities. So here is the", "tokens": [51380, 17207, 538, 264, 3565, 295, 264, 33783, 13, 407, 510, 307, 264, 51576], "temperature": 0.0, "avg_logprob": -0.22200538266089656, "compression_ratio": 1.5217391304347827, "no_speech_prob": 1.5936609997879714e-05}, {"id": 734, "seek": 489860, "start": 4899.08, "end": 4907.240000000001, "text": " log probability times the actuals. And since the actuals are either 0 or 1, and only one of them", "tokens": [50388, 3565, 8482, 1413, 264, 3539, 82, 13, 400, 1670, 264, 3539, 82, 366, 2139, 1958, 420, 502, 11, 293, 787, 472, 295, 552, 50796], "temperature": 0.0, "avg_logprob": -0.20426459312438966, "compression_ratio": 1.5819209039548023, "no_speech_prob": 9.915124246617779e-05}, {"id": 735, "seek": 489860, "start": 4907.240000000001, "end": 4912.68, "text": " is going to be a 1, we're only going to end up with one value here. And so if we add them up,", "tokens": [50796, 307, 516, 281, 312, 257, 502, 11, 321, 434, 787, 516, 281, 917, 493, 365, 472, 2158, 510, 13, 400, 370, 498, 321, 909, 552, 493, 11, 51068], "temperature": 0.0, "avg_logprob": -0.20426459312438966, "compression_ratio": 1.5819209039548023, "no_speech_prob": 9.915124246617779e-05}, {"id": 736, "seek": 489860, "start": 4912.68, "end": 4921.320000000001, "text": " it's all 0 except for one of them. So that's cross entropy. So in this special case where", "tokens": [51068, 309, 311, 439, 1958, 3993, 337, 472, 295, 552, 13, 407, 300, 311, 3278, 30867, 13, 407, 294, 341, 2121, 1389, 689, 51500], "temperature": 0.0, "avg_logprob": -0.20426459312438966, "compression_ratio": 1.5819209039548023, "no_speech_prob": 9.915124246617779e-05}, {"id": 737, "seek": 492132, "start": 4921.32, "end": 4930.04, "text": " the output's one-hot encoded, then doing the one-hot encoded multiplied by the log Softmax", "tokens": [50364, 264, 5598, 311, 472, 12, 12194, 2058, 12340, 11, 550, 884, 264, 472, 12, 12194, 2058, 12340, 17207, 538, 264, 3565, 16985, 41167, 50800], "temperature": 0.0, "avg_logprob": -0.2058413530650892, "compression_ratio": 1.5942857142857143, "no_speech_prob": 0.016913924366235733}, {"id": 738, "seek": 492132, "start": 4930.679999999999, "end": 4938.92, "text": " is actually identical to simply saying, oh, dog is in this row. Let's just look it up directly and", "tokens": [50832, 307, 767, 14800, 281, 2935, 1566, 11, 1954, 11, 3000, 307, 294, 341, 5386, 13, 961, 311, 445, 574, 309, 493, 3838, 293, 51244], "temperature": 0.0, "avg_logprob": -0.2058413530650892, "compression_ratio": 1.5942857142857143, "no_speech_prob": 0.016913924366235733}, {"id": 739, "seek": 492132, "start": 4938.92, "end": 4945.24, "text": " take its log Softmax. We can just index directly into it. So it's exactly the same thing.", "tokens": [51244, 747, 1080, 3565, 16985, 41167, 13, 492, 393, 445, 8186, 3838, 666, 309, 13, 407, 309, 311, 2293, 264, 912, 551, 13, 51560], "temperature": 0.0, "avg_logprob": -0.2058413530650892, "compression_ratio": 1.5942857142857143, "no_speech_prob": 0.016913924366235733}, {"id": 740, "seek": 494524, "start": 4945.4, "end": 4951.32, "text": " So that's just review. So if you haven't seen that before, then yeah, go and watch the part", "tokens": [50372, 407, 300, 311, 445, 3131, 13, 407, 498, 291, 2378, 380, 1612, 300, 949, 11, 550, 1338, 11, 352, 293, 1159, 264, 644, 50668], "temperature": 0.0, "avg_logprob": -0.34913821796794514, "compression_ratio": 1.518348623853211, "no_speech_prob": 0.0003199967904947698}, {"id": 741, "seek": 494524, "start": 4951.32, "end": 4953.8, "text": " one video where we went into that in a lot more detail.", "tokens": [50668, 472, 960, 689, 321, 1437, 666, 300, 294, 257, 688, 544, 2607, 13, 50792], "temperature": 0.0, "avg_logprob": -0.34913821796794514, "compression_ratio": 1.518348623853211, "no_speech_prob": 0.0003199967904947698}, {"id": 742, "seek": 494524, "start": 4957.8, "end": 4965.24, "text": " Okay, so here's our Softmax calculation. It's e to the power of each output divided by the sum of", "tokens": [50992, 1033, 11, 370, 510, 311, 527, 16985, 41167, 17108, 13, 467, 311, 308, 281, 264, 1347, 295, 1184, 5598, 6666, 538, 264, 2408, 295, 51364], "temperature": 0.0, "avg_logprob": -0.34913821796794514, "compression_ratio": 1.518348623853211, "no_speech_prob": 0.0003199967904947698}, {"id": 743, "seek": 494524, "start": 4965.24, "end": 4970.5199999999995, "text": " them. Or we can use sigma notation to say exactly the same thing. And as you can see,", "tokens": [51364, 552, 13, 1610, 321, 393, 764, 12771, 24657, 281, 584, 2293, 264, 912, 551, 13, 400, 382, 291, 393, 536, 11, 51628], "temperature": 0.0, "avg_logprob": -0.34913821796794514, "compression_ratio": 1.518348623853211, "no_speech_prob": 0.0003199967904947698}, {"id": 744, "seek": 497052, "start": 4971.080000000001, "end": 4976.76, "text": " Jupyter Notebook lets us use LaTeX. If you haven't used LaTeX before, it's actually surprisingly", "tokens": [50392, 22125, 88, 391, 11633, 2939, 6653, 505, 764, 2369, 14233, 55, 13, 759, 291, 2378, 380, 1143, 2369, 14233, 55, 949, 11, 309, 311, 767, 17600, 50676], "temperature": 0.0, "avg_logprob": -0.5612511148258131, "compression_ratio": 1.7149532710280373, "no_speech_prob": 0.0013458486646413803}, {"id": 745, "seek": 497052, "start": 4976.76, "end": 4983.88, "text": " easy to learn. You just put dollar signs around your equations like this. And your equations", "tokens": [50676, 1858, 281, 1466, 13, 509, 445, 829, 7241, 7880, 926, 428, 11787, 411, 341, 13, 400, 428, 11787, 51032], "temperature": 0.0, "avg_logprob": -0.5612511148258131, "compression_ratio": 1.7149532710280373, "no_speech_prob": 0.0013458486646413803}, {"id": 746, "seek": 497052, "start": 4985.240000000001, "end": 4990.92, "text": " backslash is going to be kind of like your functions, if you like. And curly parentheses,", "tokens": [51100, 646, 10418, 1299, 307, 516, 281, 312, 733, 295, 411, 428, 6828, 11, 498, 291, 411, 13, 400, 32066, 34153, 11, 51384], "temperature": 0.0, "avg_logprob": -0.5612511148258131, "compression_ratio": 1.7149532710280373, "no_speech_prob": 0.0013458486646413803}, {"id": 747, "seek": 497052, "start": 4990.92, "end": 4996.4400000000005, "text": " curly, curly is used to kind of for up and down, and curly is used to kind of for down.", "tokens": [51384, 32066, 11, 32066, 307, 1143, 281, 733, 295, 337, 493, 293, 760, 11, 293, 32066, 307, 1143, 281, 733, 295, 337, 760, 13, 51660], "temperature": 0.0, "avg_logprob": -0.5612511148258131, "compression_ratio": 1.7149532710280373, "no_speech_prob": 0.0013458486646413803}, {"id": 748, "seek": 499644, "start": 4997.16, "end": 5002.599999999999, "text": " And curly parentheses, curly, curly is used to kind of for arguments. So you can see here,", "tokens": [50400, 400, 32066, 34153, 11, 32066, 11, 32066, 307, 1143, 281, 733, 295, 337, 12869, 13, 407, 291, 393, 536, 510, 11, 50672], "temperature": 0.0, "avg_logprob": -0.23926203067486101, "compression_ratio": 1.6771300448430493, "no_speech_prob": 0.012624518014490604}, {"id": 749, "seek": 499644, "start": 5002.599999999999, "end": 5008.599999999999, "text": " here is e to the power of, and then underscore is used for subscript. So this is x subscript i.", "tokens": [50672, 510, 307, 308, 281, 264, 1347, 295, 11, 293, 550, 37556, 307, 1143, 337, 2325, 662, 13, 407, 341, 307, 2031, 2325, 662, 741, 13, 50972], "temperature": 0.0, "avg_logprob": -0.23926203067486101, "compression_ratio": 1.6771300448430493, "no_speech_prob": 0.012624518014490604}, {"id": 750, "seek": 499644, "start": 5009.4, "end": 5019.879999999999, "text": " And power of is used for superscripts. So here's dots. You can see here it is, dots. So it's", "tokens": [51012, 400, 1347, 295, 307, 1143, 337, 37906, 5944, 82, 13, 407, 510, 311, 15026, 13, 509, 393, 536, 510, 309, 307, 11, 15026, 13, 407, 309, 311, 51536], "temperature": 0.0, "avg_logprob": -0.23926203067486101, "compression_ratio": 1.6771300448430493, "no_speech_prob": 0.012624518014490604}, {"id": 751, "seek": 499644, "start": 5019.879999999999, "end": 5025.0, "text": " actually, yeah, learning LaTeX is easier than you might expect. It can be quite convenient for", "tokens": [51536, 767, 11, 1338, 11, 2539, 2369, 14233, 55, 307, 3571, 813, 291, 1062, 2066, 13, 467, 393, 312, 1596, 10851, 337, 51792], "temperature": 0.0, "avg_logprob": -0.23926203067486101, "compression_ratio": 1.6771300448430493, "no_speech_prob": 0.012624518014490604}, {"id": 752, "seek": 502500, "start": 5025.0, "end": 5030.36, "text": " writing these functions when you want to. So anyway, that's what softmax is. As we'll see", "tokens": [50364, 3579, 613, 6828, 562, 291, 528, 281, 13, 407, 4033, 11, 300, 311, 437, 2787, 41167, 307, 13, 1018, 321, 603, 536, 50632], "temperature": 0.0, "avg_logprob": -0.194907704624561, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.0007554016192443669}, {"id": 753, "seek": 502500, "start": 5030.36, "end": 5036.28, "text": " in a moment, well, actually, as you've already seen, in cross entropy, we don't really want", "tokens": [50632, 294, 257, 1623, 11, 731, 11, 767, 11, 382, 291, 600, 1217, 1612, 11, 294, 3278, 30867, 11, 321, 500, 380, 534, 528, 50928], "temperature": 0.0, "avg_logprob": -0.194907704624561, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.0007554016192443669}, {"id": 754, "seek": 502500, "start": 5036.28, "end": 5045.72, "text": " softmax, we want log of softmax. So log of softmax is, here it is. So we've got x dot exp,", "tokens": [50928, 2787, 41167, 11, 321, 528, 3565, 295, 2787, 41167, 13, 407, 3565, 295, 2787, 41167, 307, 11, 510, 309, 307, 13, 407, 321, 600, 658, 2031, 5893, 1278, 11, 51400], "temperature": 0.0, "avg_logprob": -0.194907704624561, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.0007554016192443669}, {"id": 755, "seek": 502500, "start": 5045.72, "end": 5054.44, "text": " so e to the x, divided by x dot exp dot sum. And we're going to sum up over the last dimension.", "tokens": [51400, 370, 308, 281, 264, 2031, 11, 6666, 538, 2031, 5893, 1278, 5893, 2408, 13, 400, 321, 434, 516, 281, 2408, 493, 670, 264, 1036, 10139, 13, 51836], "temperature": 0.0, "avg_logprob": -0.194907704624561, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.0007554016192443669}, {"id": 756, "seek": 505500, "start": 5055.96, "end": 5062.84, "text": " And then we actually want to keep that dimension, so that when we do the divided by, we want a", "tokens": [50412, 400, 550, 321, 767, 528, 281, 1066, 300, 10139, 11, 370, 300, 562, 321, 360, 264, 6666, 538, 11, 321, 528, 257, 50756], "temperature": 0.0, "avg_logprob": -0.2249275939633148, "compression_ratio": 1.6536796536796536, "no_speech_prob": 1.8925100448541343e-05}, {"id": 757, "seek": 505500, "start": 5062.84, "end": 5068.44, "text": " trailing unit axis, for exactly the same reason we saw when we did our MSE loss function. So if", "tokens": [50756, 944, 4883, 4985, 10298, 11, 337, 2293, 264, 912, 1778, 321, 1866, 562, 321, 630, 527, 376, 5879, 4470, 2445, 13, 407, 498, 51036], "temperature": 0.0, "avg_logprob": -0.2249275939633148, "compression_ratio": 1.6536796536796536, "no_speech_prob": 1.8925100448541343e-05}, {"id": 758, "seek": 505500, "start": 5068.44, "end": 5075.96, "text": " you sum with keep dim equals true, it leaves a unit axis in that last position. So we don't have", "tokens": [51036, 291, 2408, 365, 1066, 5013, 6915, 2074, 11, 309, 5510, 257, 4985, 10298, 294, 300, 1036, 2535, 13, 407, 321, 500, 380, 362, 51412], "temperature": 0.0, "avg_logprob": -0.2249275939633148, "compression_ratio": 1.6536796536796536, "no_speech_prob": 1.8925100448541343e-05}, {"id": 759, "seek": 505500, "start": 5075.96, "end": 5083.8, "text": " to put it back to avoid that horrible out of product issue. So this is the equivalent of this,", "tokens": [51412, 281, 829, 309, 646, 281, 5042, 300, 9263, 484, 295, 1674, 2734, 13, 407, 341, 307, 264, 10344, 295, 341, 11, 51804], "temperature": 0.0, "avg_logprob": -0.2249275939633148, "compression_ratio": 1.6536796536796536, "no_speech_prob": 1.8925100448541343e-05}, {"id": 760, "seek": 508380, "start": 5084.52, "end": 5093.400000000001, "text": " and then dot log. So that's log of softmax. So there is the log of the softmax with the predictions.", "tokens": [50400, 293, 550, 5893, 3565, 13, 407, 300, 311, 3565, 295, 2787, 41167, 13, 407, 456, 307, 264, 3565, 295, 264, 2787, 41167, 365, 264, 21264, 13, 50844], "temperature": 0.0, "avg_logprob": -0.20211726672028843, "compression_ratio": 1.5804597701149425, "no_speech_prob": 8.349597192136571e-05}, {"id": 761, "seek": 508380, "start": 5095.0, "end": 5101.16, "text": " Now, in terms of high score math that you may have forgotten, but you definitely are", "tokens": [50924, 823, 11, 294, 2115, 295, 1090, 6175, 5221, 300, 291, 815, 362, 11832, 11, 457, 291, 2138, 366, 51232], "temperature": 0.0, "avg_logprob": -0.20211726672028843, "compression_ratio": 1.5804597701149425, "no_speech_prob": 8.349597192136571e-05}, {"id": 762, "seek": 508380, "start": 5101.16, "end": 5111.24, "text": " going to want to know, a key piece that in that list of things is log and exponent rules.", "tokens": [51232, 516, 281, 528, 281, 458, 11, 257, 2141, 2522, 300, 294, 300, 1329, 295, 721, 307, 3565, 293, 37871, 4474, 13, 51736], "temperature": 0.0, "avg_logprob": -0.20211726672028843, "compression_ratio": 1.5804597701149425, "no_speech_prob": 8.349597192136571e-05}, {"id": 763, "seek": 511124, "start": 5111.5599999999995, "end": 5117.96, "text": " So check out Khan Academy or similar if you've forgotten them. But a quick reminder", "tokens": [50380, 407, 1520, 484, 18136, 11735, 420, 2531, 498, 291, 600, 11832, 552, 13, 583, 257, 1702, 13548, 50700], "temperature": 0.0, "avg_logprob": -0.49103910782757926, "compression_ratio": 1.1545454545454545, "no_speech_prob": 9.314568160334602e-05}, {"id": 764, "seek": 511124, "start": 5119.88, "end": 5123.32, "text": " is, for example, the one we mentioned here.", "tokens": [50796, 307, 11, 337, 1365, 11, 264, 472, 321, 2835, 510, 13, 50968], "temperature": 0.0, "avg_logprob": -0.49103910782757926, "compression_ratio": 1.1545454545454545, "no_speech_prob": 9.314568160334602e-05}, {"id": 765, "seek": 512332, "start": 5123.32, "end": 5146.5199999999995, "text": " log of a over b equals log of a minus log of b. And equivalently, log of a times b", "tokens": [50364, 3565, 295, 257, 670, 272, 6915, 3565, 295, 257, 3175, 3565, 295, 272, 13, 400, 9052, 2276, 11, 3565, 295, 257, 1413, 272, 51524], "temperature": 0.0, "avg_logprob": -0.33252797303376375, "compression_ratio": 1.2424242424242424, "no_speech_prob": 0.022285623475909233}, {"id": 766, "seek": 514652, "start": 5147.400000000001, "end": 5160.360000000001, "text": " equals log of a plus log of b. And these are very handy, because, for example, division can take a", "tokens": [50408, 6915, 3565, 295, 257, 1804, 3565, 295, 272, 13, 400, 613, 366, 588, 13239, 11, 570, 11, 337, 1365, 11, 10044, 393, 747, 257, 51056], "temperature": 0.0, "avg_logprob": -0.29910324715279246, "compression_ratio": 1.530612244897959, "no_speech_prob": 0.0015487500932067633}, {"id": 767, "seek": 514652, "start": 5160.360000000001, "end": 5167.320000000001, "text": " long time, multiply can create really big numbers that have lots of floating point error, being able", "tokens": [51056, 938, 565, 11, 12972, 393, 1884, 534, 955, 3547, 300, 362, 3195, 295, 12607, 935, 6713, 11, 885, 1075, 51404], "temperature": 0.0, "avg_logprob": -0.29910324715279246, "compression_ratio": 1.530612244897959, "no_speech_prob": 0.0015487500932067633}, {"id": 768, "seek": 514652, "start": 5167.320000000001, "end": 5173.64, "text": " to replace these things with pluses and minuses is very handy indeed. In fact, I used to give people", "tokens": [51404, 281, 7406, 613, 721, 365, 1804, 279, 293, 3175, 279, 307, 588, 13239, 6451, 13, 682, 1186, 11, 286, 1143, 281, 976, 561, 51720], "temperature": 0.0, "avg_logprob": -0.29910324715279246, "compression_ratio": 1.530612244897959, "no_speech_prob": 0.0015487500932067633}, {"id": 769, "seek": 517364, "start": 5174.04, "end": 5180.92, "text": " an interview question 20 years ago, a company which I did a lot of stuff with SQL and math.", "tokens": [50384, 364, 4049, 1168, 945, 924, 2057, 11, 257, 2237, 597, 286, 630, 257, 688, 295, 1507, 365, 19200, 293, 5221, 13, 50728], "temperature": 0.0, "avg_logprob": -0.21141813042458524, "compression_ratio": 1.575, "no_speech_prob": 0.0010484204394742846}, {"id": 770, "seek": 517364, "start": 5181.72, "end": 5190.4400000000005, "text": " SQL actually only has a sum function for group by clauses. And I used to ask people how you would", "tokens": [50768, 19200, 767, 787, 575, 257, 2408, 2445, 337, 1594, 538, 49072, 13, 400, 286, 1143, 281, 1029, 561, 577, 291, 576, 51204], "temperature": 0.0, "avg_logprob": -0.21141813042458524, "compression_ratio": 1.575, "no_speech_prob": 0.0010484204394742846}, {"id": 771, "seek": 517364, "start": 5190.4400000000005, "end": 5198.280000000001, "text": " deal with calculating a compound interest column, where the answer is basically that you have to say,", "tokens": [51204, 2028, 365, 28258, 257, 14154, 1179, 7738, 11, 689, 264, 1867, 307, 1936, 300, 291, 362, 281, 584, 11, 51596], "temperature": 0.0, "avg_logprob": -0.21141813042458524, "compression_ratio": 1.575, "no_speech_prob": 0.0010484204394742846}, {"id": 772, "seek": 517364, "start": 5199.0, "end": 5202.92, "text": " because this compound interest is taking products, so it has to be the sum of the log.", "tokens": [51632, 570, 341, 14154, 1179, 307, 1940, 3383, 11, 370, 309, 575, 281, 312, 264, 2408, 295, 264, 3565, 13, 51828], "temperature": 0.0, "avg_logprob": -0.21141813042458524, "compression_ratio": 1.575, "no_speech_prob": 0.0010484204394742846}, {"id": 773, "seek": 520364, "start": 5203.72, "end": 5210.280000000001, "text": " Of the column, and then e to the power of all that. So there's like all kinds of little places", "tokens": [50368, 2720, 264, 7738, 11, 293, 550, 308, 281, 264, 1347, 295, 439, 300, 13, 407, 456, 311, 411, 439, 3685, 295, 707, 3190, 50696], "temperature": 0.0, "avg_logprob": -0.24627886878119576, "compression_ratio": 1.5647058823529412, "no_speech_prob": 2.39234996115556e-05}, {"id": 774, "seek": 520364, "start": 5210.280000000001, "end": 5214.92, "text": " that these things come in handy, but they come in to neural nets all the time.", "tokens": [50696, 300, 613, 721, 808, 294, 13239, 11, 457, 436, 808, 294, 281, 18161, 36170, 439, 264, 565, 13, 50928], "temperature": 0.0, "avg_logprob": -0.24627886878119576, "compression_ratio": 1.5647058823529412, "no_speech_prob": 2.39234996115556e-05}, {"id": 775, "seek": 520364, "start": 5221.96, "end": 5228.4400000000005, "text": " So we're going to take advantage of that, because we've got a divided by, it's being logged.", "tokens": [51280, 407, 321, 434, 516, 281, 747, 5002, 295, 300, 11, 570, 321, 600, 658, 257, 6666, 538, 11, 309, 311, 885, 27231, 13, 51604], "temperature": 0.0, "avg_logprob": -0.24627886878119576, "compression_ratio": 1.5647058823529412, "no_speech_prob": 2.39234996115556e-05}, {"id": 776, "seek": 522844, "start": 5229.4, "end": 5241.48, "text": " And also, rather handily, we're going to have, therefore, the log of exp dot exp minus the log", "tokens": [50412, 400, 611, 11, 2831, 1011, 953, 11, 321, 434, 516, 281, 362, 11, 4412, 11, 264, 3565, 295, 1278, 5893, 1278, 3175, 264, 3565, 51016], "temperature": 0.0, "avg_logprob": -0.2801671446415416, "compression_ratio": 1.4848484848484849, "no_speech_prob": 4.400114994496107e-05}, {"id": 777, "seek": 522844, "start": 5241.48, "end": 5249.96, "text": " of this. But exp and log are opposites. So that is going to end up just being x minus. So log softmax", "tokens": [51016, 295, 341, 13, 583, 1278, 293, 3565, 366, 4665, 3324, 13, 407, 300, 307, 516, 281, 917, 493, 445, 885, 2031, 3175, 13, 407, 3565, 2787, 41167, 51440], "temperature": 0.0, "avg_logprob": -0.2801671446415416, "compression_ratio": 1.4848484848484849, "no_speech_prob": 4.400114994496107e-05}, {"id": 778, "seek": 524996, "start": 5250.12, "end": 5258.28, "text": " is just x minus all this logged. And here it is. All this logged. So that's nice.", "tokens": [50372, 307, 445, 2031, 3175, 439, 341, 27231, 13, 400, 510, 309, 307, 13, 1057, 341, 27231, 13, 407, 300, 311, 1481, 13, 50780], "temperature": 0.0, "avg_logprob": -0.2372892086322491, "compression_ratio": 1.4759036144578312, "no_speech_prob": 0.002757592359557748}, {"id": 779, "seek": 524996, "start": 5262.52, "end": 5268.12, "text": " So here's our simplified version. Okay. Now, there's another very cool trick.", "tokens": [50992, 407, 510, 311, 527, 26335, 3037, 13, 1033, 13, 823, 11, 456, 311, 1071, 588, 1627, 4282, 13, 51272], "temperature": 0.0, "avg_logprob": -0.2372892086322491, "compression_ratio": 1.4759036144578312, "no_speech_prob": 0.002757592359557748}, {"id": 780, "seek": 524996, "start": 5271.0, "end": 5275.0, "text": " Which, it's one of these things I figured out myself and then discovered other people", "tokens": [51416, 3013, 11, 309, 311, 472, 295, 613, 721, 286, 8932, 484, 2059, 293, 550, 6941, 661, 561, 51616], "temperature": 0.0, "avg_logprob": -0.2372892086322491, "compression_ratio": 1.4759036144578312, "no_speech_prob": 0.002757592359557748}, {"id": 781, "seek": 527500, "start": 5275.0, "end": 5280.6, "text": " had known it for years. So not my trick, but it's always nice to rediscover things.", "tokens": [50364, 632, 2570, 309, 337, 924, 13, 407, 406, 452, 4282, 11, 457, 309, 311, 1009, 1481, 281, 2182, 40080, 721, 13, 50644], "temperature": 0.0, "avg_logprob": -0.1974183898611167, "compression_ratio": 1.5740740740740742, "no_speech_prob": 0.0003740948741324246}, {"id": 782, "seek": 527500, "start": 5282.68, "end": 5285.4, "text": " The trick is what's written here. Let me explain what's going on.", "tokens": [50748, 440, 4282, 307, 437, 311, 3720, 510, 13, 961, 385, 2903, 437, 311, 516, 322, 13, 50884], "temperature": 0.0, "avg_logprob": -0.1974183898611167, "compression_ratio": 1.5740740740740742, "no_speech_prob": 0.0003740948741324246}, {"id": 783, "seek": 527500, "start": 5287.32, "end": 5296.68, "text": " This piece here, the log of this sum, right? This sum here. We've got x dot exp dot sum. Now,", "tokens": [50980, 639, 2522, 510, 11, 264, 3565, 295, 341, 2408, 11, 558, 30, 639, 2408, 510, 13, 492, 600, 658, 2031, 5893, 1278, 5893, 2408, 13, 823, 11, 51448], "temperature": 0.0, "avg_logprob": -0.1974183898611167, "compression_ratio": 1.5740740740740742, "no_speech_prob": 0.0003740948741324246}, {"id": 784, "seek": 527500, "start": 5296.68, "end": 5301.08, "text": " x could be some pretty big numbers. And e to the power of that's going to be really big numbers.", "tokens": [51448, 2031, 727, 312, 512, 1238, 955, 3547, 13, 400, 308, 281, 264, 1347, 295, 300, 311, 516, 281, 312, 534, 955, 3547, 13, 51668], "temperature": 0.0, "avg_logprob": -0.1974183898611167, "compression_ratio": 1.5740740740740742, "no_speech_prob": 0.0003740948741324246}, {"id": 785, "seek": 530108, "start": 5301.72, "end": 5307.08, "text": " And e to the power of things creating really big numbers. Well, really big numbers, there's much", "tokens": [50396, 400, 308, 281, 264, 1347, 295, 721, 4084, 534, 955, 3547, 13, 1042, 11, 534, 955, 3547, 11, 456, 311, 709, 50664], "temperature": 0.0, "avg_logprob": -0.2097499200275966, "compression_ratio": 1.7472527472527473, "no_speech_prob": 0.0031236193608492613}, {"id": 786, "seek": 530108, "start": 5307.08, "end": 5314.04, "text": " less precision in your computer's floating point handling. The further you get away from zero,", "tokens": [50664, 1570, 18356, 294, 428, 3820, 311, 12607, 935, 13175, 13, 440, 3052, 291, 483, 1314, 490, 4018, 11, 51012], "temperature": 0.0, "avg_logprob": -0.2097499200275966, "compression_ratio": 1.7472527472527473, "no_speech_prob": 0.0031236193608492613}, {"id": 787, "seek": 530108, "start": 5314.04, "end": 5318.44, "text": " basically. So we don't want really big numbers. Particularly because we're going to be taking", "tokens": [51012, 1936, 13, 407, 321, 500, 380, 528, 534, 955, 3547, 13, 32281, 570, 321, 434, 516, 281, 312, 1940, 51232], "temperature": 0.0, "avg_logprob": -0.2097499200275966, "compression_ratio": 1.7472527472527473, "no_speech_prob": 0.0031236193608492613}, {"id": 788, "seek": 530108, "start": 5318.44, "end": 5324.5199999999995, "text": " derivatives. And so if you're in an area that's not very precise, as far as floating point math", "tokens": [51232, 33733, 13, 400, 370, 498, 291, 434, 294, 364, 1859, 300, 311, 406, 588, 13600, 11, 382, 1400, 382, 12607, 935, 5221, 51536], "temperature": 0.0, "avg_logprob": -0.2097499200275966, "compression_ratio": 1.7472527472527473, "no_speech_prob": 0.0031236193608492613}, {"id": 789, "seek": 530108, "start": 5324.5199999999995, "end": 5328.76, "text": " is concerned, then the derivatives are going to be a disaster. They might even be zero. Because", "tokens": [51536, 307, 5922, 11, 550, 264, 33733, 366, 516, 281, 312, 257, 11293, 13, 814, 1062, 754, 312, 4018, 13, 1436, 51748], "temperature": 0.0, "avg_logprob": -0.2097499200275966, "compression_ratio": 1.7472527472527473, "no_speech_prob": 0.0031236193608492613}, {"id": 790, "seek": 532876, "start": 5328.76, "end": 5334.68, "text": " you've got two numbers that the computer can't even recognize as different. So this is bad.", "tokens": [50364, 291, 600, 658, 732, 3547, 300, 264, 3820, 393, 380, 754, 5521, 382, 819, 13, 407, 341, 307, 1578, 13, 50660], "temperature": 0.0, "avg_logprob": -0.20153057726123666, "compression_ratio": 1.4946808510638299, "no_speech_prob": 3.591293716453947e-05}, {"id": 791, "seek": 532876, "start": 5336.280000000001, "end": 5341.56, "text": " But there's a nice trick we can do to make it a lot better. What we can do is we can calculate", "tokens": [50740, 583, 456, 311, 257, 1481, 4282, 321, 393, 360, 281, 652, 309, 257, 688, 1101, 13, 708, 321, 393, 360, 307, 321, 393, 8873, 51004], "temperature": 0.0, "avg_logprob": -0.20153057726123666, "compression_ratio": 1.4946808510638299, "no_speech_prob": 3.591293716453947e-05}, {"id": 792, "seek": 532876, "start": 5341.56, "end": 5348.84, "text": " the max of a, sorry, the max of x, right? And we'll call that a. And so then rather than doing", "tokens": [51004, 264, 11469, 295, 257, 11, 2597, 11, 264, 11469, 295, 2031, 11, 558, 30, 400, 321, 603, 818, 300, 257, 13, 400, 370, 550, 2831, 813, 884, 51368], "temperature": 0.0, "avg_logprob": -0.20153057726123666, "compression_ratio": 1.4946808510638299, "no_speech_prob": 3.591293716453947e-05}, {"id": 793, "seek": 534884, "start": 5349.8, "end": 5369.64, "text": " the log of the sum of e to the x i, we're instead going to define a as being the minimum, sorry,", "tokens": [50412, 264, 3565, 295, 264, 2408, 295, 308, 281, 264, 2031, 741, 11, 321, 434, 2602, 516, 281, 6964, 257, 382, 885, 264, 7285, 11, 2597, 11, 51404], "temperature": 0.0, "avg_logprob": -0.271838124593099, "compression_ratio": 1.1851851851851851, "no_speech_prob": 0.0027149729430675507}, {"id": 794, "seek": 536964, "start": 5369.64, "end": 5377.8, "text": " the maximum of all of our x values. It's our biggest number. Now if we then subtract", "tokens": [50364, 264, 6674, 295, 439, 295, 527, 2031, 4190, 13, 467, 311, 527, 3880, 1230, 13, 823, 498, 321, 550, 16390, 50772], "temperature": 0.0, "avg_logprob": -0.19461406116754237, "compression_ratio": 1.5248618784530388, "no_speech_prob": 0.016402756795287132}, {"id": 795, "seek": 536964, "start": 5380.200000000001, "end": 5386.84, "text": " that from every number, that means none of the numbers are going to be big, by definition.", "tokens": [50892, 300, 490, 633, 1230, 11, 300, 1355, 6022, 295, 264, 3547, 366, 516, 281, 312, 955, 11, 538, 7123, 13, 51224], "temperature": 0.0, "avg_logprob": -0.19461406116754237, "compression_ratio": 1.5248618784530388, "no_speech_prob": 0.016402756795287132}, {"id": 796, "seek": 536964, "start": 5386.84, "end": 5392.76, "text": " Because we've subtracted it from all of them. Now the problem is that's given us a different result,", "tokens": [51224, 1436, 321, 600, 16390, 292, 309, 490, 439, 295, 552, 13, 823, 264, 1154, 307, 300, 311, 2212, 505, 257, 819, 1874, 11, 51520], "temperature": 0.0, "avg_logprob": -0.19461406116754237, "compression_ratio": 1.5248618784530388, "no_speech_prob": 0.016402756795287132}, {"id": 797, "seek": 539276, "start": 5392.76, "end": 5399.08, "text": " right? But if you think about it, let's expand this sum. It's e to the power of x1,", "tokens": [50364, 558, 30, 583, 498, 291, 519, 466, 309, 11, 718, 311, 5268, 341, 2408, 13, 467, 311, 308, 281, 264, 1347, 295, 2031, 16, 11, 50680], "temperature": 0.0, "avg_logprob": -0.20402731214250838, "compression_ratio": 1.511111111111111, "no_speech_prob": 3.1201852834783494e-05}, {"id": 798, "seek": 539276, "start": 5399.64, "end": 5403.8, "text": " if we don't put include L minus a, plus e to the power of x2,", "tokens": [50708, 498, 321, 500, 380, 829, 4090, 441, 3175, 257, 11, 1804, 308, 281, 264, 1347, 295, 2031, 17, 11, 50916], "temperature": 0.0, "avg_logprob": -0.20402731214250838, "compression_ratio": 1.511111111111111, "no_speech_prob": 3.1201852834783494e-05}, {"id": 799, "seek": 539276, "start": 5406.04, "end": 5413.56, "text": " plus e to the power of x3, and so forth. Okay, now we just", "tokens": [51028, 1804, 308, 281, 264, 1347, 295, 2031, 18, 11, 293, 370, 5220, 13, 1033, 11, 586, 321, 445, 51404], "temperature": 0.0, "avg_logprob": -0.20402731214250838, "compression_ratio": 1.511111111111111, "no_speech_prob": 3.1201852834783494e-05}, {"id": 800, "seek": 541356, "start": 5414.52, "end": 5421.56, "text": " subtracted a from our exponents, which has made, meant we're now wrong. But I've got good news,", "tokens": [50412, 16390, 292, 257, 490, 527, 12680, 791, 11, 597, 575, 1027, 11, 4140, 321, 434, 586, 2085, 13, 583, 286, 600, 658, 665, 2583, 11, 50764], "temperature": 0.0, "avg_logprob": -0.32799410532756024, "compression_ratio": 1.630057803468208, "no_speech_prob": 0.0008558975532650948}, {"id": 801, "seek": 541356, "start": 5421.56, "end": 5426.92, "text": " I've got good news and bad news. The bad news is that you've got more high school math to remember,", "tokens": [50764, 286, 600, 658, 665, 2583, 293, 1578, 2583, 13, 440, 1578, 2583, 307, 300, 291, 600, 658, 544, 1090, 1395, 5221, 281, 1604, 11, 51032], "temperature": 0.0, "avg_logprob": -0.32799410532756024, "compression_ratio": 1.630057803468208, "no_speech_prob": 0.0008558975532650948}, {"id": 802, "seek": 541356, "start": 5428.360000000001, "end": 5441.88, "text": " which is exponent rules. So a, x to the a plus b, equals x to the a, times x to the b.", "tokens": [51104, 597, 307, 37871, 4474, 13, 407, 257, 11, 2031, 281, 264, 257, 1804, 272, 11, 6915, 2031, 281, 264, 257, 11, 1413, 2031, 281, 264, 272, 13, 51780], "temperature": 0.0, "avg_logprob": -0.32799410532756024, "compression_ratio": 1.630057803468208, "no_speech_prob": 0.0008558975532650948}, {"id": 803, "seek": 544356, "start": 5444.120000000001, "end": 5452.52, "text": " And similarly, x to the a minus b, equals x to the a, divided by x to the b.", "tokens": [50392, 400, 14138, 11, 2031, 281, 264, 257, 3175, 272, 11, 6915, 2031, 281, 264, 257, 11, 6666, 538, 2031, 281, 264, 272, 13, 50812], "temperature": 0.0, "avg_logprob": -0.173400867132493, "compression_ratio": 1.6645569620253164, "no_speech_prob": 5.1739161790465005e-06}, {"id": 804, "seek": 544356, "start": 5454.360000000001, "end": 5461.160000000001, "text": " And to convince yourself that's true, consider for example, 2 to the power of 2 plus 3.", "tokens": [50904, 400, 281, 13447, 1803, 300, 311, 2074, 11, 1949, 337, 1365, 11, 568, 281, 264, 1347, 295, 568, 1804, 805, 13, 51244], "temperature": 0.0, "avg_logprob": -0.173400867132493, "compression_ratio": 1.6645569620253164, "no_speech_prob": 5.1739161790465005e-06}, {"id": 805, "seek": 544356, "start": 5462.200000000001, "end": 5469.64, "text": " What is that? Well you've got 2 to the power of 2, is just 2 times 2. And 2 to the power of 2 plus", "tokens": [51296, 708, 307, 300, 30, 1042, 291, 600, 658, 568, 281, 264, 1347, 295, 568, 11, 307, 445, 568, 1413, 568, 13, 400, 568, 281, 264, 1347, 295, 568, 1804, 51668], "temperature": 0.0, "avg_logprob": -0.173400867132493, "compression_ratio": 1.6645569620253164, "no_speech_prob": 5.1739161790465005e-06}, {"id": 806, "seek": 546964, "start": 5469.64, "end": 5476.6, "text": " 3, well it's 2 times 2 times, is 2 to the power of 5. So you've got 2 to the power of 2, you've", "tokens": [50364, 805, 11, 731, 309, 311, 568, 1413, 568, 1413, 11, 307, 568, 281, 264, 1347, 295, 1025, 13, 407, 291, 600, 658, 568, 281, 264, 1347, 295, 568, 11, 291, 600, 50712], "temperature": 0.0, "avg_logprob": -0.16808662631294943, "compression_ratio": 1.7425149700598803, "no_speech_prob": 9.46125146583654e-05}, {"id": 807, "seek": 546964, "start": 5476.6, "end": 5481.64, "text": " got 2 of them here, and you've got another 3 of them here. So we're just adding up the number to", "tokens": [50712, 658, 568, 295, 552, 510, 11, 293, 291, 600, 658, 1071, 805, 295, 552, 510, 13, 407, 321, 434, 445, 5127, 493, 264, 1230, 281, 50964], "temperature": 0.0, "avg_logprob": -0.16808662631294943, "compression_ratio": 1.7425149700598803, "no_speech_prob": 9.46125146583654e-05}, {"id": 808, "seek": 546964, "start": 5481.64, "end": 5487.320000000001, "text": " get the total index. So we can take advantage of this here, and say like, oh well this is equal to", "tokens": [50964, 483, 264, 3217, 8186, 13, 407, 321, 393, 747, 5002, 295, 341, 510, 11, 293, 584, 411, 11, 1954, 731, 341, 307, 2681, 281, 51248], "temperature": 0.0, "avg_logprob": -0.16808662631294943, "compression_ratio": 1.7425149700598803, "no_speech_prob": 9.46125146583654e-05}, {"id": 809, "seek": 548732, "start": 5488.2, "end": 5501.0, "text": " e to the x1, over e to the a, plus e to the x2, over e to the a, plus e to the x3, oops,", "tokens": [50408, 308, 281, 264, 2031, 16, 11, 670, 308, 281, 264, 257, 11, 1804, 308, 281, 264, 2031, 17, 11, 670, 308, 281, 264, 257, 11, 1804, 308, 281, 264, 2031, 18, 11, 34166, 11, 51048], "temperature": 0.0, "avg_logprob": -0.16502862818100872, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.022976359352469444}, {"id": 810, "seek": 548732, "start": 5501.0, "end": 5510.759999999999, "text": " plus e to the x3, over e to the a. And this is a common denominator, so we can put all that together.", "tokens": [51048, 1804, 308, 281, 264, 2031, 18, 11, 670, 308, 281, 264, 257, 13, 400, 341, 307, 257, 2689, 20687, 11, 370, 321, 393, 829, 439, 300, 1214, 13, 51536], "temperature": 0.0, "avg_logprob": -0.16502862818100872, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.022976359352469444}, {"id": 811, "seek": 551076, "start": 5510.76, "end": 5522.2, "text": " A to the a. And why did we do all that? Because if we now multiply that all by e to the a,", "tokens": [50364, 316, 281, 264, 257, 13, 400, 983, 630, 321, 360, 439, 300, 30, 1436, 498, 321, 586, 12972, 300, 439, 538, 308, 281, 264, 257, 11, 50936], "temperature": 0.0, "avg_logprob": -0.2461566925048828, "compression_ratio": 1.6094674556213018, "no_speech_prob": 1.6964422684395686e-05}, {"id": 812, "seek": 551076, "start": 5524.2, "end": 5527.08, "text": " these would cancel out, and we get the thing we originally wanted.", "tokens": [51036, 613, 576, 10373, 484, 11, 293, 321, 483, 264, 551, 321, 7993, 1415, 13, 51180], "temperature": 0.0, "avg_logprob": -0.2461566925048828, "compression_ratio": 1.6094674556213018, "no_speech_prob": 1.6964422684395686e-05}, {"id": 813, "seek": 551076, "start": 5527.88, "end": 5530.280000000001, "text": " So that means we simply have to multiply this,", "tokens": [51220, 407, 300, 1355, 321, 2935, 362, 281, 12972, 341, 11, 51340], "temperature": 0.0, "avg_logprob": -0.2461566925048828, "compression_ratio": 1.6094674556213018, "no_speech_prob": 1.6964422684395686e-05}, {"id": 814, "seek": 551076, "start": 5533.8, "end": 5537.400000000001, "text": " by that, and this gives us exactly the same thing as we had before.", "tokens": [51516, 538, 300, 11, 293, 341, 2709, 505, 2293, 264, 912, 551, 382, 321, 632, 949, 13, 51696], "temperature": 0.0, "avg_logprob": -0.2461566925048828, "compression_ratio": 1.6094674556213018, "no_speech_prob": 1.6964422684395686e-05}, {"id": 815, "seek": 553740, "start": 5537.879999999999, "end": 5543.5599999999995, "text": " But with, critically, this is no longer ever going to be a giant number. So this might seem", "tokens": [50388, 583, 365, 11, 22797, 11, 341, 307, 572, 2854, 1562, 516, 281, 312, 257, 7410, 1230, 13, 407, 341, 1062, 1643, 50672], "temperature": 0.0, "avg_logprob": -0.24658389758038265, "compression_ratio": 1.6355140186915889, "no_speech_prob": 0.00014202338934410363}, {"id": 816, "seek": 553740, "start": 5543.5599999999995, "end": 5548.679999999999, "text": " a bit weird, we're doing extra calculations. It's not a simplification, it's a complexification.", "tokens": [50672, 257, 857, 3657, 11, 321, 434, 884, 2857, 20448, 13, 467, 311, 406, 257, 6883, 3774, 11, 309, 311, 257, 3997, 3774, 13, 50928], "temperature": 0.0, "avg_logprob": -0.24658389758038265, "compression_ratio": 1.6355140186915889, "no_speech_prob": 0.00014202338934410363}, {"id": 817, "seek": 553740, "start": 5550.5199999999995, "end": 5553.24, "text": " But it's one that's going to make it easier for our floating point unit.", "tokens": [51020, 583, 309, 311, 472, 300, 311, 516, 281, 652, 309, 3571, 337, 527, 12607, 935, 4985, 13, 51156], "temperature": 0.0, "avg_logprob": -0.24658389758038265, "compression_ratio": 1.6355140186915889, "no_speech_prob": 0.00014202338934410363}, {"id": 818, "seek": 553740, "start": 5554.2, "end": 5559.719999999999, "text": " So that's our trick, is rather than doing log of this sum, what we actually do is log of", "tokens": [51204, 407, 300, 311, 527, 4282, 11, 307, 2831, 813, 884, 3565, 295, 341, 2408, 11, 437, 321, 767, 360, 307, 3565, 295, 51480], "temperature": 0.0, "avg_logprob": -0.24658389758038265, "compression_ratio": 1.6355140186915889, "no_speech_prob": 0.00014202338934410363}, {"id": 819, "seek": 555972, "start": 5559.8, "end": 5569.72, "text": " e to the a times the sum of e to the x minus a. And since we've got log of a product, that's just", "tokens": [50368, 308, 281, 264, 257, 1413, 264, 2408, 295, 308, 281, 264, 2031, 3175, 257, 13, 400, 1670, 321, 600, 658, 3565, 295, 257, 1674, 11, 300, 311, 445, 50864], "temperature": 0.0, "avg_logprob": -0.21085417820857122, "compression_ratio": 1.5901639344262295, "no_speech_prob": 0.018263645470142365}, {"id": 820, "seek": 555972, "start": 5569.72, "end": 5576.92, "text": " a log, that's just the sum of the logs, and log of e to the a is just a. So it's a plus that. So", "tokens": [50864, 257, 3565, 11, 300, 311, 445, 264, 2408, 295, 264, 20820, 11, 293, 3565, 295, 308, 281, 264, 257, 307, 445, 257, 13, 407, 309, 311, 257, 1804, 300, 13, 407, 51224], "temperature": 0.0, "avg_logprob": -0.21085417820857122, "compression_ratio": 1.5901639344262295, "no_speech_prob": 0.018263645470142365}, {"id": 821, "seek": 557692, "start": 5577.72, "end": 5581.88, "text": " this here is", "tokens": [50404, 341, 510, 307, 50612], "temperature": 0.0, "avg_logprob": -0.25267053472584694, "compression_ratio": 1.3666666666666667, "no_speech_prob": 0.004538188222795725}, {"id": 822, "seek": 557692, "start": 5585.8, "end": 5587.72, "text": " called the log sum exp trick.", "tokens": [50808, 1219, 264, 3565, 2408, 1278, 4282, 13, 50904], "temperature": 0.0, "avg_logprob": -0.25267053472584694, "compression_ratio": 1.3666666666666667, "no_speech_prob": 0.004538188222795725}, {"id": 823, "seek": 557692, "start": 5591.8, "end": 5594.6, "text": " Oops, people pointing out that I've made a mistake, thank you.", "tokens": [51108, 21726, 11, 561, 12166, 484, 300, 286, 600, 1027, 257, 6146, 11, 1309, 291, 13, 51248], "temperature": 0.0, "avg_logprob": -0.25267053472584694, "compression_ratio": 1.3666666666666667, "no_speech_prob": 0.004538188222795725}, {"id": 824, "seek": 557692, "start": 5598.52, "end": 5604.92, "text": " That, of course, should have been inside the log. You can't just go sticking it on the outside like", "tokens": [51444, 663, 11, 295, 1164, 11, 820, 362, 668, 1854, 264, 3565, 13, 509, 393, 380, 445, 352, 13465, 309, 322, 264, 2380, 411, 51764], "temperature": 0.0, "avg_logprob": -0.25267053472584694, "compression_ratio": 1.3666666666666667, "no_speech_prob": 0.004538188222795725}, {"id": 825, "seek": 560492, "start": 5604.92, "end": 5616.84, "text": " a crazy person. That's what I meant to say. Okay, so here is the log sum exp trick. Oh,", "tokens": [50364, 257, 3219, 954, 13, 663, 311, 437, 286, 4140, 281, 584, 13, 1033, 11, 370, 510, 307, 264, 3565, 2408, 1278, 4282, 13, 876, 11, 50960], "temperature": 0.0, "avg_logprob": -0.2172490712758657, "compression_ratio": 1.4792899408284024, "no_speech_prob": 0.0003625863464549184}, {"id": 826, "seek": 560492, "start": 5616.84, "end": 5620.28, "text": " I called it m instead of a, which is a bit silly. I should have called it a, but anyway.", "tokens": [50960, 286, 1219, 309, 275, 2602, 295, 257, 11, 597, 307, 257, 857, 11774, 13, 286, 820, 362, 1219, 309, 257, 11, 457, 4033, 13, 51132], "temperature": 0.0, "avg_logprob": -0.2172490712758657, "compression_ratio": 1.4792899408284024, "no_speech_prob": 0.0003625863464549184}, {"id": 827, "seek": 560492, "start": 5620.28, "end": 5625.24, "text": " So we find the maximum on the last dimension, and then here is the m plus", "tokens": [51132, 407, 321, 915, 264, 6674, 322, 264, 1036, 10139, 11, 293, 550, 510, 307, 264, 275, 1804, 51380], "temperature": 0.0, "avg_logprob": -0.2172490712758657, "compression_ratio": 1.4792899408284024, "no_speech_prob": 0.0003625863464549184}, {"id": 828, "seek": 562524, "start": 5626.12, "end": 5640.76, "text": " a. That exact thing. Okay, so that's just another way of doing that. Okay, so that's the log sum exp.", "tokens": [50408, 257, 13, 663, 1900, 551, 13, 1033, 11, 370, 300, 311, 445, 1071, 636, 295, 884, 300, 13, 1033, 11, 370, 300, 311, 264, 3565, 2408, 1278, 13, 51140], "temperature": 0.0, "avg_logprob": -0.24745251839621024, "compression_ratio": 1.4615384615384615, "no_speech_prob": 0.0003625890240073204}, {"id": 829, "seek": 562524, "start": 5643.0, "end": 5651.08, "text": " So now we can rewrite log softmax as x minus log sum exp. And we're not going to use our", "tokens": [51252, 407, 586, 321, 393, 28132, 3565, 2787, 41167, 382, 2031, 3175, 3565, 2408, 1278, 13, 400, 321, 434, 406, 516, 281, 764, 527, 51656], "temperature": 0.0, "avg_logprob": -0.24745251839621024, "compression_ratio": 1.4615384615384615, "no_speech_prob": 0.0003625890240073204}, {"id": 830, "seek": 565108, "start": 5651.08, "end": 5657.8, "text": " version because PyTorch already has one. So we'll just use PyTorch's. And if we check,", "tokens": [50364, 3037, 570, 9953, 51, 284, 339, 1217, 575, 472, 13, 407, 321, 603, 445, 764, 9953, 51, 284, 339, 311, 13, 400, 498, 321, 1520, 11, 50700], "temperature": 0.0, "avg_logprob": -0.23191808064778646, "compression_ratio": 1.5680473372781065, "no_speech_prob": 0.0061927796341478825}, {"id": 831, "seek": 565108, "start": 5660.28, "end": 5669.08, "text": " we, here we go. Here's our results. And so then as we've discussed, the cross entropy loss", "tokens": [50824, 321, 11, 510, 321, 352, 13, 1692, 311, 527, 3542, 13, 400, 370, 550, 382, 321, 600, 7152, 11, 264, 3278, 30867, 4470, 51264], "temperature": 0.0, "avg_logprob": -0.23191808064778646, "compression_ratio": 1.5680473372781065, "no_speech_prob": 0.0061927796341478825}, {"id": 832, "seek": 565108, "start": 5669.08, "end": 5675.5599999999995, "text": " is the sum of the outputs times the log probabilities. And as we discussed, our outputs", "tokens": [51264, 307, 264, 2408, 295, 264, 23930, 1413, 264, 3565, 33783, 13, 400, 382, 321, 7152, 11, 527, 23930, 51588], "temperature": 0.0, "avg_logprob": -0.23191808064778646, "compression_ratio": 1.5680473372781065, "no_speech_prob": 0.0061927796341478825}, {"id": 833, "seek": 567556, "start": 5675.56, "end": 5682.120000000001, "text": " are one-hot encoded, or actually they're just the integers, better still. So what we can do", "tokens": [50364, 366, 472, 12, 12194, 2058, 12340, 11, 420, 767, 436, 434, 445, 264, 41674, 11, 1101, 920, 13, 407, 437, 321, 393, 360, 50692], "temperature": 0.0, "avg_logprob": -0.22245136572390187, "compression_ratio": 1.4426229508196722, "no_speech_prob": 0.0029810022097080946}, {"id": 834, "seek": 567556, "start": 5683.0, "end": 5693.88, "text": " is we can, I guess I should make that more clear. Actually, they're just the integer", "tokens": [50736, 307, 321, 393, 11, 286, 2041, 286, 820, 652, 300, 544, 1850, 13, 5135, 11, 436, 434, 445, 264, 24922, 51280], "temperature": 0.0, "avg_logprob": -0.22245136572390187, "compression_ratio": 1.4426229508196722, "no_speech_prob": 0.0029810022097080946}, {"id": 835, "seek": 569388, "start": 5694.04, "end": 5710.4400000000005, "text": " indices. So we can simply rewrite that as negative log of the target. So that's what we have in our", "tokens": [50372, 43840, 13, 407, 321, 393, 2935, 28132, 300, 382, 3671, 3565, 295, 264, 3779, 13, 407, 300, 311, 437, 321, 362, 294, 527, 51192], "temperature": 0.0, "avg_logprob": -0.2089691162109375, "compression_ratio": 1.3541666666666667, "no_speech_prob": 0.0004955342737957835}, {"id": 836, "seek": 569388, "start": 5710.4400000000005, "end": 5718.4400000000005, "text": " Excel. And so how do we do that in PyTorch? So this is quite interesting. There's a lot of cool", "tokens": [51192, 19060, 13, 400, 370, 577, 360, 321, 360, 300, 294, 9953, 51, 284, 339, 30, 407, 341, 307, 1596, 1880, 13, 821, 311, 257, 688, 295, 1627, 51592], "temperature": 0.0, "avg_logprob": -0.2089691162109375, "compression_ratio": 1.3541666666666667, "no_speech_prob": 0.0004955342737957835}, {"id": 837, "seek": 571844, "start": 5718.44, "end": 5724.04, "text": " things you can do with array indexing in PyTorch and NumPy. So basically they use the same", "tokens": [50364, 721, 291, 393, 360, 365, 10225, 8186, 278, 294, 9953, 51, 284, 339, 293, 22592, 47, 88, 13, 407, 1936, 436, 764, 264, 912, 50644], "temperature": 0.0, "avg_logprob": -0.19591272329982323, "compression_ratio": 1.418848167539267, "no_speech_prob": 0.00045120794675312936}, {"id": 838, "seek": 571844, "start": 5724.04, "end": 5730.839999999999, "text": " approaches. Let's take a look. Here is the first three actual values in y-train. They're five,", "tokens": [50644, 11587, 13, 961, 311, 747, 257, 574, 13, 1692, 307, 264, 700, 1045, 3539, 4190, 294, 288, 12, 83, 7146, 13, 814, 434, 1732, 11, 50984], "temperature": 0.0, "avg_logprob": -0.19591272329982323, "compression_ratio": 1.418848167539267, "no_speech_prob": 0.00045120794675312936}, {"id": 839, "seek": 571844, "start": 5731.639999999999, "end": 5740.36, "text": " zero, and four. Now what we want to do is we want to find in our softmax predictions,", "tokens": [51024, 4018, 11, 293, 1451, 13, 823, 437, 321, 528, 281, 360, 307, 321, 528, 281, 915, 294, 527, 2787, 41167, 21264, 11, 51460], "temperature": 0.0, "avg_logprob": -0.19591272329982323, "compression_ratio": 1.418848167539267, "no_speech_prob": 0.00045120794675312936}, {"id": 840, "seek": 574036, "start": 5741.32, "end": 5752.5199999999995, "text": " we want to get five, the fifth prediction in the zeroth row, the zeroth prediction in the first", "tokens": [50412, 321, 528, 281, 483, 1732, 11, 264, 9266, 17630, 294, 264, 44746, 900, 5386, 11, 264, 44746, 900, 17630, 294, 264, 700, 50972], "temperature": 0.0, "avg_logprob": -0.24114287526983963, "compression_ratio": 1.8113207547169812, "no_speech_prob": 0.001987749245017767}, {"id": 841, "seek": 574036, "start": 5752.5199999999995, "end": 5760.599999999999, "text": " row, and the fourth prediction in the index two row. So these are the numbers that we want. This", "tokens": [50972, 5386, 11, 293, 264, 6409, 17630, 294, 264, 8186, 732, 5386, 13, 407, 613, 366, 264, 3547, 300, 321, 528, 13, 639, 51376], "temperature": 0.0, "avg_logprob": -0.24114287526983963, "compression_ratio": 1.8113207547169812, "no_speech_prob": 0.001987749245017767}, {"id": 842, "seek": 574036, "start": 5760.599999999999, "end": 5766.44, "text": " is going to be what we add up for the first two rows of our loss function. So how do we do that", "tokens": [51376, 307, 516, 281, 312, 437, 321, 909, 493, 337, 264, 700, 732, 13241, 295, 527, 4470, 2445, 13, 407, 577, 360, 321, 360, 300, 51668], "temperature": 0.0, "avg_logprob": -0.24114287526983963, "compression_ratio": 1.8113207547169812, "no_speech_prob": 0.001987749245017767}, {"id": 843, "seek": 576644, "start": 5766.599999999999, "end": 5775.639999999999, "text": " in all in one go? Well, here's a cool trick. See here I've got zero, one, two. If we index using a", "tokens": [50372, 294, 439, 294, 472, 352, 30, 1042, 11, 510, 311, 257, 1627, 4282, 13, 3008, 510, 286, 600, 658, 4018, 11, 472, 11, 732, 13, 759, 321, 8186, 1228, 257, 50824], "temperature": 0.0, "avg_logprob": -0.23627072915263558, "compression_ratio": 1.5747126436781609, "no_speech_prob": 0.008985180407762527}, {"id": 844, "seek": 576644, "start": 5776.679999999999, "end": 5783.4, "text": " two lists, we can put here zero, one, two. And for the second list, we can put y-train,", "tokens": [50876, 732, 14511, 11, 321, 393, 829, 510, 4018, 11, 472, 11, 732, 13, 400, 337, 264, 1150, 1329, 11, 321, 393, 829, 288, 12, 83, 7146, 11, 51212], "temperature": 0.0, "avg_logprob": -0.23627072915263558, "compression_ratio": 1.5747126436781609, "no_speech_prob": 0.008985180407762527}, {"id": 845, "seek": 576644, "start": 5783.4, "end": 5788.919999999999, "text": " column three, five, zero, four. And this is actually going to return zero, comma, zero,", "tokens": [51212, 7738, 1045, 11, 1732, 11, 4018, 11, 1451, 13, 400, 341, 307, 767, 516, 281, 2736, 4018, 11, 22117, 11, 4018, 11, 51488], "temperature": 0.0, "avg_logprob": -0.23627072915263558, "compression_ratio": 1.5747126436781609, "no_speech_prob": 0.008985180407762527}, {"id": 846, "seek": 578892, "start": 5789.88, "end": 5797.24, "text": " one, comma, sorry, it's going to be, sorry, it's going to be zero, comma, five, one, comma, zero,", "tokens": [50412, 472, 11, 22117, 11, 2597, 11, 309, 311, 516, 281, 312, 11, 2597, 11, 309, 311, 516, 281, 312, 4018, 11, 22117, 11, 1732, 11, 472, 11, 22117, 11, 4018, 11, 50780], "temperature": 0.0, "avg_logprob": -0.2767452636322418, "compression_ratio": 1.6357615894039734, "no_speech_prob": 9.91526321740821e-05}, {"id": 847, "seek": 578892, "start": 5797.88, "end": 5805.4800000000005, "text": " and two, comma, four, which is, as you see, exactly the same thing. So therefore,", "tokens": [50812, 293, 732, 11, 22117, 11, 1451, 11, 597, 307, 11, 382, 291, 536, 11, 2293, 264, 912, 551, 13, 407, 4412, 11, 51192], "temperature": 0.0, "avg_logprob": -0.2767452636322418, "compression_ratio": 1.6357615894039734, "no_speech_prob": 9.91526321740821e-05}, {"id": 848, "seek": 578892, "start": 5807.64, "end": 5812.52, "text": " this is actually giving us what we need for the cross-entropy loss.", "tokens": [51300, 341, 307, 767, 2902, 505, 437, 321, 643, 337, 264, 3278, 12, 317, 27514, 4470, 13, 51544], "temperature": 0.0, "avg_logprob": -0.2767452636322418, "compression_ratio": 1.6357615894039734, "no_speech_prob": 9.91526321740821e-05}, {"id": 849, "seek": 581252, "start": 5813.160000000001, "end": 5821.080000000001, "text": " So if we take range of our target's first dimension, or zero index dimension, which is all", "tokens": [50396, 407, 498, 321, 747, 3613, 295, 527, 3779, 311, 700, 10139, 11, 420, 4018, 8186, 10139, 11, 597, 307, 439, 50792], "temperature": 0.0, "avg_logprob": -0.35034018754959106, "compression_ratio": 1.5771812080536913, "no_speech_prob": 0.00048029626486822963}, {"id": 850, "seek": 581252, "start": 5821.080000000001, "end": 5828.52, "text": " this is, and the target, and then take the negative of that dot mean, that gives us our", "tokens": [50792, 341, 307, 11, 293, 264, 3779, 11, 293, 550, 747, 264, 3671, 295, 300, 5893, 914, 11, 300, 2709, 505, 527, 51164], "temperature": 0.0, "avg_logprob": -0.35034018754959106, "compression_ratio": 1.5771812080536913, "no_speech_prob": 0.00048029626486822963}, {"id": 851, "seek": 581252, "start": 5828.52, "end": 5833.56, "text": " cross-entropy loss, which is pretty neat, in my opinion.", "tokens": [51164, 3278, 12, 317, 27514, 4470, 11, 597, 307, 1238, 10654, 11, 294, 452, 4800, 13, 51416], "temperature": 0.0, "avg_logprob": -0.35034018754959106, "compression_ratio": 1.5771812080536913, "no_speech_prob": 0.00048029626486822963}, {"id": 852, "seek": 583356, "start": 5834.52, "end": 5843.96, "text": " All right, so PyTorch calls this negative log likelihood loss. But that's all it is.", "tokens": [50412, 1057, 558, 11, 370, 9953, 51, 284, 339, 5498, 341, 3671, 3565, 22119, 4470, 13, 583, 300, 311, 439, 309, 307, 13, 50884], "temperature": 0.0, "avg_logprob": -0.5391311298717152, "compression_ratio": 1.4645669291338583, "no_speech_prob": 0.015905851498246193}, {"id": 853, "seek": 583356, "start": 5844.84, "end": 5850.360000000001, "text": " And so if we take the negative log likelihood, and we pass that to that, the log softmax,", "tokens": [50928, 400, 370, 498, 321, 747, 264, 3671, 3565, 22119, 11, 293, 321, 1320, 300, 281, 300, 11, 264, 3565, 2787, 41167, 11, 51204], "temperature": 0.0, "avg_logprob": -0.5391311298717152, "compression_ratio": 1.4645669291338583, "no_speech_prob": 0.015905851498246193}, {"id": 854, "seek": 583356, "start": 5854.120000000001, "end": 5855.64, "text": " then we get", "tokens": [51392, 550, 321, 483, 51468], "temperature": 0.0, "avg_logprob": -0.5391311298717152, "compression_ratio": 1.4645669291338583, "no_speech_prob": 0.015905851498246193}, {"id": 855, "seek": 585564, "start": 5856.200000000001, "end": 5863.0, "text": " the loss. And this particular combination in PyTorch is called f dot cross-entropy.", "tokens": [50392, 264, 4470, 13, 400, 341, 1729, 6562, 294, 9953, 51, 284, 339, 307, 1219, 283, 5893, 3278, 12, 317, 27514, 13, 50732], "temperature": 0.0, "avg_logprob": -0.33013920451319495, "compression_ratio": 1.5944444444444446, "no_speech_prob": 0.014728255569934845}, {"id": 856, "seek": 585564, "start": 5863.56, "end": 5867.0, "text": " So just check. Yep, f dot cross-entropy gives us exactly the same thing.", "tokens": [50760, 407, 445, 1520, 13, 7010, 11, 283, 5893, 3278, 12, 317, 27514, 2709, 505, 2293, 264, 912, 551, 13, 50932], "temperature": 0.0, "avg_logprob": -0.33013920451319495, "compression_ratio": 1.5944444444444446, "no_speech_prob": 0.014728255569934845}, {"id": 857, "seek": 585564, "start": 5868.4400000000005, "end": 5871.72, "text": " So that's cool. So we have now re-implemented the cross-entropy loss.", "tokens": [51004, 407, 300, 311, 1627, 13, 407, 321, 362, 586, 319, 12, 332, 781, 14684, 264, 3278, 12, 317, 27514, 4470, 13, 51168], "temperature": 0.0, "avg_logprob": -0.33013920451319495, "compression_ratio": 1.5944444444444446, "no_speech_prob": 0.014728255569934845}, {"id": 858, "seek": 585564, "start": 5872.6, "end": 5877.72, "text": " And there's a lot of confusing things going on there, a lot.", "tokens": [51212, 400, 456, 311, 257, 688, 295, 13181, 721, 516, 322, 456, 11, 257, 688, 13, 51468], "temperature": 0.0, "avg_logprob": -0.33013920451319495, "compression_ratio": 1.5944444444444446, "no_speech_prob": 0.014728255569934845}, {"id": 859, "seek": 587772, "start": 5878.52, "end": 5883.4800000000005, "text": " And so this is one of those places where you should pause the video and go back and look at", "tokens": [50404, 400, 370, 341, 307, 472, 295, 729, 3190, 689, 291, 820, 10465, 264, 960, 293, 352, 646, 293, 574, 412, 50652], "temperature": 0.0, "avg_logprob": -0.4588039511501199, "compression_ratio": 1.6069868995633187, "no_speech_prob": 0.18710513412952423}, {"id": 860, "seek": 587772, "start": 5883.4800000000005, "end": 5889.400000000001, "text": " each step and think, not just like, what is it doing, but why is it doing it? And also try", "tokens": [50652, 1184, 1823, 293, 519, 11, 406, 445, 411, 11, 437, 307, 309, 884, 11, 457, 983, 307, 309, 884, 309, 30, 400, 611, 853, 50948], "temperature": 0.0, "avg_logprob": -0.4588039511501199, "compression_ratio": 1.6069868995633187, "no_speech_prob": 0.18710513412952423}, {"id": 861, "seek": 587772, "start": 5889.400000000001, "end": 5894.280000000001, "text": " typing in lots of different values yourself to see if you can see what's going on.", "tokens": [50948, 18444, 294, 3195, 295, 819, 4190, 1803, 281, 536, 498, 291, 393, 536, 437, 311, 516, 322, 13, 51192], "temperature": 0.0, "avg_logprob": -0.4588039511501199, "compression_ratio": 1.6069868995633187, "no_speech_prob": 0.18710513412952423}, {"id": 862, "seek": 587772, "start": 5895.56, "end": 5901.0, "text": " And then put this aside and test yourself by re-implementing the cross-entropy loss.", "tokens": [51256, 400, 550, 829, 341, 7359, 293, 1500, 1803, 538, 319, 12, 332, 43704, 278, 264, 3278, 12, 317, 27514, 4470, 13, 51528], "temperature": 0.0, "avg_logprob": -0.4588039511501199, "compression_ratio": 1.6069868995633187, "no_speech_prob": 0.18710513412952423}, {"id": 863, "seek": 587772, "start": 5901.0, "end": 5902.04, "text": " So let's do that.", "tokens": [51528, 407, 718, 311, 360, 300, 13, 51580], "temperature": 0.0, "avg_logprob": -0.4588039511501199, "compression_ratio": 1.6069868995633187, "no_speech_prob": 0.18710513412952423}, {"id": 864, "seek": 590204, "start": 5902.04, "end": 5927.08, "text": " So now that we've got that, we can actually create a training loop. So let's set our", "tokens": [50364, 407, 586, 300, 321, 600, 658, 300, 11, 321, 393, 767, 1884, 257, 3097, 6367, 13, 407, 718, 311, 992, 527, 51616], "temperature": 0.0, "avg_logprob": -0.2798114204406738, "compression_ratio": 1.0909090909090908, "no_speech_prob": 0.0503278449177742}, {"id": 865, "seek": 592708, "start": 5927.08, "end": 5933.48, "text": " loss function to be cross-entropy. Let's create a batch size of 64. And so here's our first mini", "tokens": [50364, 4470, 2445, 281, 312, 3278, 12, 317, 27514, 13, 961, 311, 1884, 257, 15245, 2744, 295, 12145, 13, 400, 370, 510, 311, 527, 700, 8382, 50684], "temperature": 0.0, "avg_logprob": -0.18337280896245217, "compression_ratio": 1.586046511627907, "no_speech_prob": 0.013222440145909786}, {"id": 866, "seek": 592708, "start": 5933.48, "end": 5941.96, "text": " batch. So xb is the x mini batch. It's going to be from 0 up to 64 from our training set.", "tokens": [50684, 15245, 13, 407, 2031, 65, 307, 264, 2031, 8382, 15245, 13, 467, 311, 516, 281, 312, 490, 1958, 493, 281, 12145, 490, 527, 3097, 992, 13, 51108], "temperature": 0.0, "avg_logprob": -0.18337280896245217, "compression_ratio": 1.586046511627907, "no_speech_prob": 0.013222440145909786}, {"id": 867, "seek": 592708, "start": 5942.76, "end": 5949.48, "text": " So we can now calculate our predictions. So that's 64 by 10. So for each of the 64", "tokens": [51148, 407, 321, 393, 586, 8873, 527, 21264, 13, 407, 300, 311, 12145, 538, 1266, 13, 407, 337, 1184, 295, 264, 12145, 51484], "temperature": 0.0, "avg_logprob": -0.18337280896245217, "compression_ratio": 1.586046511627907, "no_speech_prob": 0.013222440145909786}, {"id": 868, "seek": 592708, "start": 5949.48, "end": 5952.76, "text": " images in the mini batch, we have 10 probabilities, one for each digit.", "tokens": [51484, 5267, 294, 264, 8382, 15245, 11, 321, 362, 1266, 33783, 11, 472, 337, 1184, 14293, 13, 51648], "temperature": 0.0, "avg_logprob": -0.18337280896245217, "compression_ratio": 1.586046511627907, "no_speech_prob": 0.013222440145909786}, {"id": 869, "seek": 595276, "start": 5952.84, "end": 5958.84, "text": " And our y is just, let's print those out.", "tokens": [50368, 400, 527, 288, 307, 445, 11, 718, 311, 4482, 729, 484, 13, 50668], "temperature": 0.0, "avg_logprob": -0.4001038035408395, "compression_ratio": 1.4230769230769231, "no_speech_prob": 0.0006666966364718974}, {"id": 870, "seek": 595276, "start": 5962.4400000000005, "end": 5969.56, "text": " So there's our first 64 target values. So these are the actual digits. And so our loss function,", "tokens": [50848, 407, 456, 311, 527, 700, 12145, 3779, 4190, 13, 407, 613, 366, 264, 3539, 27011, 13, 400, 370, 527, 4470, 2445, 11, 51204], "temperature": 0.0, "avg_logprob": -0.4001038035408395, "compression_ratio": 1.4230769230769231, "no_speech_prob": 0.0006666966364718974}, {"id": 871, "seek": 595276, "start": 5970.12, "end": 5974.360000000001, "text": " so we're going to start with a bad loss because it's entirely random at this point.", "tokens": [51232, 370, 321, 434, 516, 281, 722, 365, 257, 1578, 4470, 570, 309, 311, 7696, 4974, 412, 341, 935, 13, 51444], "temperature": 0.0, "avg_logprob": -0.4001038035408395, "compression_ratio": 1.4230769230769231, "no_speech_prob": 0.0006666966364718974}, {"id": 872, "seek": 597436, "start": 5975.0, "end": 5975.639999999999, "text": " At this point.", "tokens": [50396, 1711, 341, 935, 13, 50428], "temperature": 0.0, "avg_logprob": -0.2665893766615126, "compression_ratio": 1.3529411764705883, "no_speech_prob": 0.0016484548104926944}, {"id": 873, "seek": 597436, "start": 5979.639999999999, "end": 5988.5199999999995, "text": " Okay. So for each of the predictions we made, so those are our predictions.", "tokens": [50628, 1033, 13, 407, 337, 1184, 295, 264, 21264, 321, 1027, 11, 370, 729, 366, 527, 21264, 13, 51072], "temperature": 0.0, "avg_logprob": -0.2665893766615126, "compression_ratio": 1.3529411764705883, "no_speech_prob": 0.0016484548104926944}, {"id": 874, "seek": 597436, "start": 5989.4, "end": 5995.0, "text": " And so remember those predictions are a 64 by 10. What did we predict?", "tokens": [51116, 400, 370, 1604, 729, 21264, 366, 257, 12145, 538, 1266, 13, 708, 630, 321, 6069, 30, 51396], "temperature": 0.0, "avg_logprob": -0.2665893766615126, "compression_ratio": 1.3529411764705883, "no_speech_prob": 0.0016484548104926944}, {"id": 875, "seek": 599500, "start": 5995.96, "end": 6005.24, "text": " So for each one of these 64 rows, we have to go in and see where is the highest number.", "tokens": [50412, 407, 337, 1184, 472, 295, 613, 12145, 13241, 11, 321, 362, 281, 352, 294, 293, 536, 689, 307, 264, 6343, 1230, 13, 50876], "temperature": 0.0, "avg_logprob": -0.2129444224493844, "compression_ratio": 1.9489795918367347, "no_speech_prob": 0.013848202303051949}, {"id": 876, "seek": 599500, "start": 6005.24, "end": 6014.2, "text": " So if we go through here, we can go through each one. Here's a, there's a 0.1. Okay. It looks like", "tokens": [50876, 407, 498, 321, 352, 807, 510, 11, 321, 393, 352, 807, 1184, 472, 13, 1692, 311, 257, 11, 456, 311, 257, 1958, 13, 16, 13, 1033, 13, 467, 1542, 411, 51324], "temperature": 0.0, "avg_logprob": -0.2129444224493844, "compression_ratio": 1.9489795918367347, "no_speech_prob": 0.013848202303051949}, {"id": 877, "seek": 599500, "start": 6014.2, "end": 6018.6, "text": " this is the highest number. So it's 0, 1, 2, 3. So it's the highest number is this one. So you've", "tokens": [51324, 341, 307, 264, 6343, 1230, 13, 407, 309, 311, 1958, 11, 502, 11, 568, 11, 805, 13, 407, 309, 311, 264, 6343, 1230, 307, 341, 472, 13, 407, 291, 600, 51544], "temperature": 0.0, "avg_logprob": -0.2129444224493844, "compression_ratio": 1.9489795918367347, "no_speech_prob": 0.013848202303051949}, {"id": 878, "seek": 599500, "start": 6018.6, "end": 6022.92, "text": " got to find the index of the highest number. The function to find the index of the highest number", "tokens": [51544, 658, 281, 915, 264, 8186, 295, 264, 6343, 1230, 13, 440, 2445, 281, 915, 264, 8186, 295, 264, 6343, 1230, 51760], "temperature": 0.0, "avg_logprob": -0.2129444224493844, "compression_ratio": 1.9489795918367347, "no_speech_prob": 0.013848202303051949}, {"id": 879, "seek": 602292, "start": 6022.92, "end": 6032.04, "text": " is called argmax. And yep, here it is, 3. And I guess we could have also written this probably as", "tokens": [50364, 307, 1219, 3882, 41167, 13, 400, 18633, 11, 510, 309, 307, 11, 805, 13, 400, 286, 2041, 321, 727, 362, 611, 3720, 341, 1391, 382, 50820], "temperature": 0.0, "avg_logprob": -0.19599592460776274, "compression_ratio": 1.6276150627615062, "no_speech_prob": 0.00016865211364347488}, {"id": 880, "seek": 602292, "start": 6032.04, "end": 6037.4800000000005, "text": " preds.argmax. Normally you can do them either way. I actually prefer normally to do it this way.", "tokens": [50820, 3852, 82, 13, 33544, 41167, 13, 17424, 291, 393, 360, 552, 2139, 636, 13, 286, 767, 4382, 5646, 281, 360, 309, 341, 636, 13, 51092], "temperature": 0.0, "avg_logprob": -0.19599592460776274, "compression_ratio": 1.6276150627615062, "no_speech_prob": 0.00016865211364347488}, {"id": 881, "seek": 602292, "start": 6038.04, "end": 6044.04, "text": " Yep. There's the same thing. Okay. And the reason we want this is because we want to be able to", "tokens": [51120, 7010, 13, 821, 311, 264, 912, 551, 13, 1033, 13, 400, 264, 1778, 321, 528, 341, 307, 570, 321, 528, 281, 312, 1075, 281, 51420], "temperature": 0.0, "avg_logprob": -0.19599592460776274, "compression_ratio": 1.6276150627615062, "no_speech_prob": 0.00016865211364347488}, {"id": 882, "seek": 602292, "start": 6044.04, "end": 6048.68, "text": " calculate accuracy. We don't need it for the actual neural net, but we just like to be able to see", "tokens": [51420, 8873, 14170, 13, 492, 500, 380, 643, 309, 337, 264, 3539, 18161, 2533, 11, 457, 321, 445, 411, 281, 312, 1075, 281, 536, 51652], "temperature": 0.0, "avg_logprob": -0.19599592460776274, "compression_ratio": 1.6276150627615062, "no_speech_prob": 0.00016865211364347488}, {"id": 883, "seek": 604868, "start": 6049.320000000001, "end": 6053.56, "text": " how we're going. Cause it's like, it's a metric. It's something that we use for understanding.", "tokens": [50396, 577, 321, 434, 516, 13, 10865, 309, 311, 411, 11, 309, 311, 257, 20678, 13, 467, 311, 746, 300, 321, 764, 337, 3701, 13, 50608], "temperature": 0.0, "avg_logprob": -0.2074275579978162, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.0028895323630422354}, {"id": 884, "seek": 604868, "start": 6055.64, "end": 6062.04, "text": " So we take the argmax, we compare it to the actual. So that's going to give us a bunch of", "tokens": [50712, 407, 321, 747, 264, 3882, 41167, 11, 321, 6794, 309, 281, 264, 3539, 13, 407, 300, 311, 516, 281, 976, 505, 257, 3840, 295, 51032], "temperature": 0.0, "avg_logprob": -0.2074275579978162, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.0028895323630422354}, {"id": 885, "seek": 604868, "start": 6062.04, "end": 6066.360000000001, "text": " balls. If we turn those into floats, there'll be ones and zeros. And the mean of those floats is", "tokens": [51032, 9803, 13, 759, 321, 1261, 729, 666, 37878, 11, 456, 603, 312, 2306, 293, 35193, 13, 400, 264, 914, 295, 729, 37878, 307, 51248], "temperature": 0.0, "avg_logprob": -0.2074275579978162, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.0028895323630422354}, {"id": 886, "seek": 604868, "start": 6066.360000000001, "end": 6072.360000000001, "text": " the accuracy. So our current accuracy, not surprisingly, is around 10%. It's 9% because", "tokens": [51248, 264, 14170, 13, 407, 527, 2190, 14170, 11, 406, 17600, 11, 307, 926, 1266, 6856, 467, 311, 1722, 4, 570, 51548], "temperature": 0.0, "avg_logprob": -0.2074275579978162, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.0028895323630422354}, {"id": 887, "seek": 604868, "start": 6072.360000000001, "end": 6077.64, "text": " it's random. That's what you would expect. So let's train our first neural net. So we'll set", "tokens": [51548, 309, 311, 4974, 13, 663, 311, 437, 291, 576, 2066, 13, 407, 718, 311, 3847, 527, 700, 18161, 2533, 13, 407, 321, 603, 992, 51812], "temperature": 0.0, "avg_logprob": -0.2074275579978162, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.0028895323630422354}, {"id": 888, "seek": 607764, "start": 6077.64, "end": 6084.280000000001, "text": " a learning rate. We'll do a few epochs. So we're going to go through each epoch and we're going to", "tokens": [50364, 257, 2539, 3314, 13, 492, 603, 360, 257, 1326, 30992, 28346, 13, 407, 321, 434, 516, 281, 352, 807, 1184, 30992, 339, 293, 321, 434, 516, 281, 50696], "temperature": 0.0, "avg_logprob": -0.17834580662738847, "compression_ratio": 1.6187845303867403, "no_speech_prob": 0.00034598971251398325}, {"id": 889, "seek": 607764, "start": 6084.280000000001, "end": 6093.08, "text": " go through from zero up to N. That's the 50,000 training rows and skipping by 64, the batch size", "tokens": [50696, 352, 807, 490, 4018, 493, 281, 426, 13, 663, 311, 264, 2625, 11, 1360, 3097, 13241, 293, 31533, 538, 12145, 11, 264, 15245, 2744, 51136], "temperature": 0.0, "avg_logprob": -0.17834580662738847, "compression_ratio": 1.6187845303867403, "no_speech_prob": 0.00034598971251398325}, {"id": 890, "seek": 607764, "start": 6093.08, "end": 6101.240000000001, "text": " each time. And so we're going to create a slice that starts at I. So starting at zero and goes up", "tokens": [51136, 1184, 565, 13, 400, 370, 321, 434, 516, 281, 1884, 257, 13153, 300, 3719, 412, 286, 13, 407, 2891, 412, 4018, 293, 1709, 493, 51544], "temperature": 0.0, "avg_logprob": -0.17834580662738847, "compression_ratio": 1.6187845303867403, "no_speech_prob": 0.00034598971251398325}, {"id": 891, "seek": 610124, "start": 6101.32, "end": 6109.5599999999995, "text": " to 64, unless we've gone past the end, in which case we'll just go to N. And so then we will slice", "tokens": [50368, 281, 12145, 11, 5969, 321, 600, 2780, 1791, 264, 917, 11, 294, 597, 1389, 321, 603, 445, 352, 281, 426, 13, 400, 370, 550, 321, 486, 13153, 50780], "temperature": 0.0, "avg_logprob": -0.1967552281633208, "compression_ratio": 1.5025906735751295, "no_speech_prob": 0.00302779208868742}, {"id": 892, "seek": 610124, "start": 6109.5599999999995, "end": 6116.599999999999, "text": " into our training set for the X and for the Y to get our X and Y batches. We will then calculate", "tokens": [50780, 666, 527, 3097, 992, 337, 264, 1783, 293, 337, 264, 398, 281, 483, 527, 1783, 293, 398, 15245, 279, 13, 492, 486, 550, 8873, 51132], "temperature": 0.0, "avg_logprob": -0.1967552281633208, "compression_ratio": 1.5025906735751295, "no_speech_prob": 0.00302779208868742}, {"id": 893, "seek": 610124, "start": 6116.599999999999, "end": 6122.84, "text": " our predictions, our loss function and do our backward. So the way I did this originally was I", "tokens": [51132, 527, 21264, 11, 527, 4470, 2445, 293, 360, 527, 23897, 13, 407, 264, 636, 286, 630, 341, 7993, 390, 286, 51444], "temperature": 0.0, "avg_logprob": -0.1967552281633208, "compression_ratio": 1.5025906735751295, "no_speech_prob": 0.00302779208868742}, {"id": 894, "seek": 612284, "start": 6123.72, "end": 6136.52, "text": " had all of these in separate cells. And I just typed in, you know, I equals zero.", "tokens": [50408, 632, 439, 295, 613, 294, 4994, 5438, 13, 400, 286, 445, 33941, 294, 11, 291, 458, 11, 286, 6915, 4018, 13, 51048], "temperature": 0.0, "avg_logprob": -0.24317211574978298, "compression_ratio": 1.336, "no_speech_prob": 0.008187460713088512}, {"id": 895, "seek": 612284, "start": 6138.28, "end": 6143.4800000000005, "text": " And then went through one cell at a time, calculating each one until they all worked.", "tokens": [51136, 400, 550, 1437, 807, 472, 2815, 412, 257, 565, 11, 28258, 1184, 472, 1826, 436, 439, 2732, 13, 51396], "temperature": 0.0, "avg_logprob": -0.24317211574978298, "compression_ratio": 1.336, "no_speech_prob": 0.008187460713088512}, {"id": 896, "seek": 614348, "start": 6143.959999999999, "end": 6154.04, "text": " And so then I can put them in a loop. Okay, so once we've got done backward, we can then", "tokens": [50388, 400, 370, 550, 286, 393, 829, 552, 294, 257, 6367, 13, 1033, 11, 370, 1564, 321, 600, 658, 1096, 23897, 11, 321, 393, 550, 50892], "temperature": 0.0, "avg_logprob": -0.38203963526973017, "compression_ratio": 1.3623188405797102, "no_speech_prob": 0.002018978586420417}, {"id": 897, "seek": 614348, "start": 6156.599999999999, "end": 6164.599999999999, "text": " with torch not go grad, go through each layer. And if that's a layer that has weights, we'll update", "tokens": [51020, 365, 27822, 406, 352, 2771, 11, 352, 807, 1184, 4583, 13, 400, 498, 300, 311, 257, 4583, 300, 575, 17443, 11, 321, 603, 5623, 51420], "temperature": 0.0, "avg_logprob": -0.38203963526973017, "compression_ratio": 1.3623188405797102, "no_speech_prob": 0.002018978586420417}, {"id": 898, "seek": 616460, "start": 6165.56, "end": 6176.4400000000005, "text": " them to the existing weights minus the gradients times the learning rate. And then zero out. So", "tokens": [50412, 552, 281, 264, 6741, 17443, 3175, 264, 2771, 2448, 1413, 264, 2539, 3314, 13, 400, 550, 4018, 484, 13, 407, 50956], "temperature": 0.0, "avg_logprob": -0.2616156108343779, "compression_ratio": 1.728476821192053, "no_speech_prob": 0.022628379985690117}, {"id": 899, "seek": 616460, "start": 6176.4400000000005, "end": 6182.52, "text": " the weights and biases for the gradients, the gradients of the weights and biases, this underscore", "tokens": [50956, 264, 17443, 293, 32152, 337, 264, 2771, 2448, 11, 264, 2771, 2448, 295, 264, 17443, 293, 32152, 11, 341, 37556, 51260], "temperature": 0.0, "avg_logprob": -0.2616156108343779, "compression_ratio": 1.728476821192053, "no_speech_prob": 0.022628379985690117}, {"id": 900, "seek": 616460, "start": 6183.160000000001, "end": 6189.56, "text": " means do it in place. So that sets this to zero. So if I run that,", "tokens": [51292, 1355, 360, 309, 294, 1081, 13, 407, 300, 6352, 341, 281, 4018, 13, 407, 498, 286, 1190, 300, 11, 51612], "temperature": 0.0, "avg_logprob": -0.2616156108343779, "compression_ratio": 1.728476821192053, "no_speech_prob": 0.022628379985690117}, {"id": 901, "seek": 618956, "start": 6190.52, "end": 6199.0, "text": " oops, got to run all of them. I guess I skipped cell. There we go. It's finished.", "tokens": [50412, 34166, 11, 658, 281, 1190, 439, 295, 552, 13, 286, 2041, 286, 30193, 2815, 13, 821, 321, 352, 13, 467, 311, 4335, 13, 50836], "temperature": 0.0, "avg_logprob": -0.3027046260549061, "compression_ratio": 1.3431952662721893, "no_speech_prob": 0.00020027316350024194}, {"id": 902, "seek": 618956, "start": 6203.080000000001, "end": 6210.360000000001, "text": " So you can see that our accuracy on the training sets a bit unfair, but it's only three epochs,", "tokens": [51040, 407, 291, 393, 536, 300, 527, 14170, 322, 264, 3097, 6352, 257, 857, 17019, 11, 457, 309, 311, 787, 1045, 30992, 28346, 11, 51404], "temperature": 0.0, "avg_logprob": -0.3027046260549061, "compression_ratio": 1.3431952662721893, "no_speech_prob": 0.00020027316350024194}, {"id": 903, "seek": 618956, "start": 6210.360000000001, "end": 6214.68, "text": " is nearly 97%. So we now have a digit recognizer.", "tokens": [51404, 307, 6217, 23399, 6856, 407, 321, 586, 362, 257, 14293, 3068, 6545, 13, 51620], "temperature": 0.0, "avg_logprob": -0.3027046260549061, "compression_ratio": 1.3431952662721893, "no_speech_prob": 0.00020027316350024194}, {"id": 904, "seek": 621468, "start": 6215.4800000000005, "end": 6222.280000000001, "text": " It trains pretty quickly and is not terrible at all. So that's a pretty good starting point.", "tokens": [50404, 467, 16329, 1238, 2661, 293, 307, 406, 6237, 412, 439, 13, 407, 300, 311, 257, 1238, 665, 2891, 935, 13, 50744], "temperature": 0.0, "avg_logprob": -0.222668676665335, "compression_ratio": 1.6104651162790697, "no_speech_prob": 0.0001660376728978008}, {"id": 905, "seek": 621468, "start": 6225.16, "end": 6234.200000000001, "text": " All right. So what we're going to do next time is we're going to refactor this training loop", "tokens": [50888, 1057, 558, 13, 407, 437, 321, 434, 516, 281, 360, 958, 565, 307, 321, 434, 516, 281, 1895, 15104, 341, 3097, 6367, 51340], "temperature": 0.0, "avg_logprob": -0.222668676665335, "compression_ratio": 1.6104651162790697, "no_speech_prob": 0.0001660376728978008}, {"id": 906, "seek": 621468, "start": 6235.16, "end": 6240.280000000001, "text": " to make it dramatically, dramatically, dramatically simpler, step by step, until eventually", "tokens": [51388, 281, 652, 309, 17548, 11, 17548, 11, 17548, 18587, 11, 1823, 538, 1823, 11, 1826, 4728, 51644], "temperature": 0.0, "avg_logprob": -0.222668676665335, "compression_ratio": 1.6104651162790697, "no_speech_prob": 0.0001660376728978008}, {"id": 907, "seek": 624028, "start": 6240.28, "end": 6245.16, "text": " we will get it down to,", "tokens": [50364, 321, 486, 483, 309, 760, 281, 11, 50608], "temperature": 0.0, "avg_logprob": -0.24979831554271556, "compression_ratio": 1.5953757225433527, "no_speech_prob": 0.0004583072441164404}, {"id": 908, "seek": 624028, "start": 6248.92, "end": 6253.48, "text": " so we'll get it down to something much, much shorter, and then we're going to", "tokens": [50796, 370, 321, 603, 483, 309, 760, 281, 746, 709, 11, 709, 11639, 11, 293, 550, 321, 434, 516, 281, 51024], "temperature": 0.0, "avg_logprob": -0.24979831554271556, "compression_ratio": 1.5953757225433527, "no_speech_prob": 0.0004583072441164404}, {"id": 909, "seek": 624028, "start": 6253.48, "end": 6257.48, "text": " add a validation set to it and a multiprocessing data loader.", "tokens": [51024, 909, 257, 24071, 992, 281, 309, 293, 257, 3311, 340, 780, 278, 1412, 3677, 260, 13, 51224], "temperature": 0.0, "avg_logprob": -0.24979831554271556, "compression_ratio": 1.5953757225433527, "no_speech_prob": 0.0004583072441164404}, {"id": 910, "seek": 624028, "start": 6259.24, "end": 6262.5199999999995, "text": " And then, yeah, we'll be in a pretty good position, I think, to,", "tokens": [51312, 400, 550, 11, 1338, 11, 321, 603, 312, 294, 257, 1238, 665, 2535, 11, 286, 519, 11, 281, 11, 51476], "temperature": 0.0, "avg_logprob": -0.24979831554271556, "compression_ratio": 1.5953757225433527, "no_speech_prob": 0.0004583072441164404}, {"id": 911, "seek": 624028, "start": 6264.28, "end": 6267.08, "text": " to start training some more interesting models.", "tokens": [51564, 281, 722, 3097, 512, 544, 1880, 5245, 13, 51704], "temperature": 0.0, "avg_logprob": -0.24979831554271556, "compression_ratio": 1.5953757225433527, "no_speech_prob": 0.0004583072441164404}, {"id": 912, "seek": 626708, "start": 6267.8, "end": 6273.64, "text": " All right. Hopefully you found that useful and learned some interesting things. And so", "tokens": [50400, 1057, 558, 13, 10429, 291, 1352, 300, 4420, 293, 3264, 512, 1880, 721, 13, 400, 370, 50692], "temperature": 0.0, "avg_logprob": -0.41917786508236293, "compression_ratio": 1.3945578231292517, "no_speech_prob": 4.90856145916041e-05}, {"id": 913, "seek": 626708, "start": 6278.2, "end": 6279.8, "text": " what I'd really like you to do", "tokens": [50920, 437, 286, 1116, 534, 411, 291, 281, 360, 51000], "temperature": 0.0, "avg_logprob": -0.41917786508236293, "compression_ratio": 1.3945578231292517, "no_speech_prob": 4.90856145916041e-05}, {"id": 914, "seek": 626708, "start": 6283.0, "end": 6289.4, "text": " is at this point, now that you've kind of like got all these key basic pieces in place,", "tokens": [51160, 307, 412, 341, 935, 11, 586, 300, 291, 600, 733, 295, 411, 658, 439, 613, 2141, 3875, 3755, 294, 1081, 11, 51480], "temperature": 0.0, "avg_logprob": -0.41917786508236293, "compression_ratio": 1.3945578231292517, "no_speech_prob": 4.90856145916041e-05}, {"id": 915, "seek": 628940, "start": 6289.799999999999, "end": 6296.36, "text": " is to really try to recreate them without peaking as much as possible.", "tokens": [50384, 307, 281, 534, 853, 281, 25833, 552, 1553, 520, 2456, 382, 709, 382, 1944, 13, 50712], "temperature": 0.0, "avg_logprob": -0.46089901643640857, "compression_ratio": 1.723529411764706, "no_speech_prob": 0.0017007086426019669}, {"id": 916, "seek": 628940, "start": 6296.92, "end": 6303.24, "text": " So, you know, recreate your matrix multiply, recreate those forward and backward passes,", "tokens": [50740, 407, 11, 291, 458, 11, 25833, 428, 8141, 12972, 11, 25833, 729, 2128, 293, 23897, 11335, 11, 51056], "temperature": 0.0, "avg_logprob": -0.46089901643640857, "compression_ratio": 1.723529411764706, "no_speech_prob": 0.0017007086426019669}, {"id": 917, "seek": 628940, "start": 6304.92, "end": 6309.24, "text": " recreate something that steps through layers, and even see if you can like recreate", "tokens": [51140, 25833, 746, 300, 4439, 807, 7914, 11, 293, 754, 536, 498, 291, 393, 411, 25833, 51356], "temperature": 0.0, "avg_logprob": -0.46089901643640857, "compression_ratio": 1.723529411764706, "no_speech_prob": 0.0017007086426019669}, {"id": 918, "seek": 628940, "start": 6310.839999999999, "end": 6313.16, "text": " the idea of the dot forward and the dot backward.", "tokens": [51436, 264, 1558, 295, 264, 5893, 2128, 293, 264, 5893, 23897, 13, 51552], "temperature": 0.0, "avg_logprob": -0.46089901643640857, "compression_ratio": 1.723529411764706, "no_speech_prob": 0.0017007086426019669}, {"id": 919, "seek": 631316, "start": 6313.639999999999, "end": 6324.36, "text": " Make sure it's all in your head really clearly, so that you fully understand what's going on.", "tokens": [50388, 4387, 988, 309, 311, 439, 294, 428, 1378, 534, 4448, 11, 370, 300, 291, 4498, 1223, 437, 311, 516, 322, 13, 50924], "temperature": 0.0, "avg_logprob": -0.20570232651450418, "compression_ratio": 1.599056603773585, "no_speech_prob": 0.0008167271735146642}, {"id": 920, "seek": 631316, "start": 6325.88, "end": 6329.24, "text": " You know, at the very least, if you don't have time for that, because that's a big job,", "tokens": [51000, 509, 458, 11, 412, 264, 588, 1935, 11, 498, 291, 500, 380, 362, 565, 337, 300, 11, 570, 300, 311, 257, 955, 1691, 11, 51168], "temperature": 0.0, "avg_logprob": -0.20570232651450418, "compression_ratio": 1.599056603773585, "no_speech_prob": 0.0008167271735146642}, {"id": 921, "seek": 631316, "start": 6329.24, "end": 6332.5199999999995, "text": " you could pick out a smaller part of that, the piece that you're more interested in,", "tokens": [51168, 291, 727, 1888, 484, 257, 4356, 644, 295, 300, 11, 264, 2522, 300, 291, 434, 544, 3102, 294, 11, 51332], "temperature": 0.0, "avg_logprob": -0.20570232651450418, "compression_ratio": 1.599056603773585, "no_speech_prob": 0.0008167271735146642}, {"id": 922, "seek": 631316, "start": 6333.24, "end": 6338.84, "text": " or you could just go through and look really closely at these notebooks.", "tokens": [51368, 420, 291, 727, 445, 352, 807, 293, 574, 534, 8185, 412, 613, 43782, 13, 51648], "temperature": 0.0, "avg_logprob": -0.20570232651450418, "compression_ratio": 1.599056603773585, "no_speech_prob": 0.0008167271735146642}, {"id": 923, "seek": 633884, "start": 6338.84, "end": 6345.24, "text": " So if you go to kernel, restart and clear output, it'll delete all the outputs and like try to", "tokens": [50364, 407, 498, 291, 352, 281, 28256, 11, 21022, 293, 1850, 5598, 11, 309, 603, 12097, 439, 264, 23930, 293, 411, 853, 281, 50684], "temperature": 0.0, "avg_logprob": -0.26868255514847605, "compression_ratio": 1.5132275132275133, "no_speech_prob": 0.0001376457221340388}, {"id": 924, "seek": 633884, "start": 6345.24, "end": 6350.84, "text": " think like what are the shapes of things, can you guess what they are, can you check them, and so", "tokens": [50684, 519, 411, 437, 366, 264, 10854, 295, 721, 11, 393, 291, 2041, 437, 436, 366, 11, 393, 291, 1520, 552, 11, 293, 370, 50964], "temperature": 0.0, "avg_logprob": -0.26868255514847605, "compression_ratio": 1.5132275132275133, "no_speech_prob": 0.0001376457221340388}, {"id": 925, "seek": 633884, "start": 6350.84, "end": 6360.28, "text": " forth. Okay. Thanks, everybody. Hope you have a great week and I will see you next time. Bye.", "tokens": [50964, 5220, 13, 1033, 13, 2561, 11, 2201, 13, 6483, 291, 362, 257, 869, 1243, 293, 286, 486, 536, 291, 958, 565, 13, 4621, 13, 51436], "temperature": 0.0, "avg_logprob": -0.26868255514847605, "compression_ratio": 1.5132275132275133, "no_speech_prob": 0.0001376457221340388}], "language": "en"}