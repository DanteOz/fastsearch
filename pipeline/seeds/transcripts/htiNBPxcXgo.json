{"text": " So, welcome to the last lesson of part 1 of Practical Deep Learning for Coders. It's been a really fun time doing this course and depending on when you're watching and listening to this you may want to check the forums or the fast.ai website to see whether we have a part 2 planned which is going to be sometime towards the end of 2022. Or if it's already past that then maybe there's even a part 2 already on the website. The part 2 goes a lot deeper than part 1 technically in terms of getting to the point that you should be able to read and implement research papers and deploy models in a very kind of real life situation. So yeah, last lesson we started on the collaborative filtering notebook and we were looking at collaborative filtering and this is where we got to which is creating your own embedding module and this is a very cool place to start the lesson because you're going to learn a lot about what's really going on. And it's really important before you dig into this to make sure that you're really comfortable with the 05 ADM model neural net from scratch notebook. So if parts of this are not totally clear put it aside and redo this notebook because what we're looking at from here are kind of the abstractions that PyTorch and fast.ai add on top of functionality that we've built ourselves from scratch. So if you remember in the neural network from scratch we built, we initialized a number of coefficients, a couple of different layers, you know, and a bias term and then during as the model trained we updated those coefficients by going through each layer of them and subtracting out the gradients by the learning rate. You've probably noticed that in PyTorch we don't have to go to all that trouble and I wanted to show you how PyTorch does this. PyTorch, we don't have to keep track of what our coefficients or parameters or weights are. PyTorch does that for us and the way it does that is it looks inside our module and it tries to find anything that looks like a neural network parameter or a tensor of neural network parameters and it keeps track of them. And so here is a class we've created called T which is a subclass of module and I've created one thing inside it which is something with the attribute a. So this is a in the T module and it just contains three ones. And so the idea is, you know, maybe we're creating a module and this is we're initializing some parameter that we want to train. Now we can find out what trainable parameters or just what parameters in general PyTorch knows about in our model by instantiating our model and then asking for the parameters which you then have to turn that into a list or in fastcore we have a thing called capital L which is like a fancy list which prints out the number of items in the list and shows you those items. Now in this case when we create our object of type T and ask for its parameters we get told there are zero tensors of parameters and a list with nothing in it. Now why is that? We actually said we wanted to create a tensor with three ones in it. How would we make those parameters? Well the answer is that the way you tell PyTorch what your parameters are is you actually just have to put them inside a special object called an nn.parameter. This thing almost doesn't really do anything. In fact last time I checked it really quite literally had almost no code in it. Sometimes these things change but let's take a look. Yeah okay so it's about a dozen lines of code or 20 lines of code which does almost nothing. It's got a way of being copied, it's got a way of printing itself, it's got a way of saving itself, and it's got a way of being initialized. So a parameter hardly does anything. The key thing is though that when PyTorch checks to see which parameters should it update, when it optimizes it just looks for anything that's been wrapped in this parameter class. So if we do exactly the same thing as before which is to set an attribute containing a tensor with three ones in it, but this case we wrap it in a parameter, we now get told okay there's one parameter tensor in this model and it contains a tensor with three ones. And you can see it also actually by default assumes that we're going to want require gradient. It's assuming that anything that's a parameter is something that you want to calculate gradients for. Now most of the time we don't have to do this because PyTorch provides lots of convenient things for us such as what you've seen before nn.linear which is something that also creates a tensor. So this would create a tensor of 1 by 3 without a bias term in it. This is not being wrapped in an nn.parameter but that's okay. PyTorch knows that anything which is basically a layer in a neural net is going to be a parameter. So it automatically considers this a parameter. So here's exactly the same thing again I'll construct my object of type T, I'll check for its parameters and I can see there's one tensor of parameters and there's our three things. And you'll notice that it's also automatically randomly initialized them which again is generally what we want. So PyTorch does go to some effort to try to make things easy for you. So this attribute a is a linear layer and it's got a bunch of things in it. One of the things in it is the weights and that's where you'll actually find the parameters, that is of type parameter. So a linear layer is something that contains attributes of type parameter. Okay so what we want to do is we want to create something that works just like this did which is something that creates a matrix which will be trained as we train the model. Okay so an embedding is something which yeah it's going to create a matrix of this by this and it will be a parameter and it's something that yeah we need to be able to index into as we did here. And so yeah what is happening behind the scenes you know in PyTorch. It's nice to be able to create these things ourselves in Scratch because it means we really understand it. And so let's create that exact same module that we did last time but this time we're going to use a function I've created called createPyrams. We pass in a size so such as in this case n uses by n factors and it's going to call torch.zeros to create a tensor of zeros of the size that you request and then it's going to do a normal random distribution so a Gaussian distribution of mean zero, standard deviation 0.01 to randomly initialize those and it'll put the whole thing into an nn.parameter. So that so this here is going to create an attribute called user factors which will be a parameter containing some tensor of normally distributed random numbers of this size. Excuse me. And because it's a parameter that's going to be stored inside that's going to be available as in parameters in the module. Oh I'm sneezing. So userBias will be a vector of parameters. UserFactors will be a matrix of parameters. MovieFactors will be a matrix and movies by n factors. MovieBias will be a vector of n movies and this is the same as before. So now in the forward we can do exactly what we did before. The thing is when you put a tensor inside a parameter it has all the exact same features that a tensor has. So for example we can index into it. So this whole thing is identical to what we had before and so that's actually believe it or not all that's required to replicate PyTorch's embedding layer from scratch. So let's run those and see if it works. And there it is it's training. So we'll be able to have a look when this is done at for example model dot let's have a look MovieBias. And here it is right. It's a parameter containing a bunch of numbers that have been trained. And as we'd expect it's got 1665 things in because that's how many movies we have. So a question from Jonah Raphael was does torch dot zeros not produce all zeros? Yes torch dot zeros does produce all zeros. But remember a method that ends in underscore changes in place the tensor it's being applied to. And so if you look up PyTorch normal underscore you'll see it fills itself with elements sampled from the normal distribution. So this is actually modifying this tensor in place. And so that's why we end up with something which isn't just zeros. Now this is the bit I find really fun is we train this model but what did it do? How is it going about predicting who's going to like what movie? Well one of the things that's happened is we've created this MovieBias parameter which has been optimized. And what we could do is we could find which movie IDs have the highest numbers here and the lowest numbers. So I think this is going to start lowest and then we can print out we can look inside our data loaders and grab the names of those movies for each of those five lowest numbers. And what's happened here? Well we can see broadly speaking that it has printed out some pretty crappy movies. And why is that? Well that's because when it does that matrix product that we saw in the Excel spreadsheet last week, it's trying to figure out who's going to like what movie based on previous movies people have enjoyed or not. And then it adds MovieBias which can be positive or negative. That's a different number for each movie. So in order to do a good job of predicting whether you're going to like a movie or not it has to know which movies are crap. And so the crap movies are going to end up with a very low MovieBias parameter. And so we can actually find out which movies to people, not only which movies to people really not like, but which movies to people like like less than one would expect given the kind of movie that it is. So Lawnmower Man 2 for example, not only apparently is it a crappy movie, but based on the kind of movie it is, you know it's kind of like a high-tech pop kind of sci-fi movie, people who like those kinds of movies still don't like Lawnmower Man 2. So that's what this is meaning. So it's kind of nice that we can like use a model not just to predict things but to understand things about the data. So if we sort by descending it'll give us the exact opposite. So here are movies that people enjoy even when they don't normally enjoy that kind of movie. So for example LA Confidential, classic kind of film noir detective movie with the Aussie Guy Pearce. Even if you don't really like film noir detective movies you might like this one. You know Silence of the Lambs, classic kind of I guess you'd say like horror kind of, not horror is it a suspense movie. Even people who don't normally like kind of serial killer suspense movies tend to like this one. Now the other thing we can do is not just look at what's happening in the bias. Oh and by the way we could do the same thing with users and find out like which user just loves movies even the crappy ones you know just likes all movies and vice versa. But what about the other thing we didn't just have bias we also had movie factors which has got the number of movies as one axis and the number of factors as the other and we passed in 50. What's in that huge matrix? Well pretty hard to visualize such a huge matrix and we're not going to talk about the details but you can do something called PCA which stands for Principle Component Analysis and that basically tries to compress those 50 columns down into three columns. And then we can draw a chart of the top two. And so this is PCA component number one and this is PCA component number two and here's a bunch of movies and this is a compressed view of these latent factors that it created. And you can see that they obviously have some kind of meaning right. So over here towards the right we've got kind of you know very pop mainstream kind of movies and over here on the left we've got more of the kind of critically acclaimed gritty kind of movies. And then towards the top we've got very kind of action oriented and sci-fi movies and then down towards the bottom we've got very dialogue driven movies. So remember we didn't program in any of these things and we don't have any data at all about what movie is what kind of movie but thanks to the magic of SGD we just told it to please try and optimize these parameters and the way it was able to predict who would like what movie was it had to figure out what kinds of movies are there or what kind of taste is there for each movie. So I think that's pretty interesting. So this is called visualizing embeddings and then this is visualizing the bias. We obviously would rather not do everything both by hand like this or even like this and fast AI provides an application for collaborative learner and so we can create one and this is going to look much the same as what we just had. We're going to say how many latent factors we want and what the y range is to do the sigmoid and the multiply and then we can do fit and away it goes. So let's see how it does. Alright so it's done a bit better than our manual one. Let's take a look at the model it created. The model looks very similar to what we created in terms of the parameters. You can see these are the two embeddings and these are the two biases and we can do exactly the same thing. We can look in that model and we can find the you'll see it's not called movies. It's if items. It's users and items. This is the item bias. So we can look at the item bias, grab the weights, sort and we get a very similar result. In this case it's even more confident that LA Confidential is a movie that you should probably try watching even if you don't like those kind of movies. And Titanic is right up there as well even if you don't really like romancey kind of movies you might like this one. Even if you don't like classic detective you might like this one. You know we can have a look at the source code for Colab Learner and we can see that let's see useNN is false by default. So where our model is going to be of this type embedding.bias. So we can take a look at that. Here it is. And look this does look very similar. Okay it's creating an embedding using the size we requested for each of users by factors and items by factors and users and items. And then it's grabbing each thing from the embedding in the forward and it's doing the model play and it's adding it up and it's doing the sigmoid. So yeah it looks exactly the same. Isn't that neat? So you can see that what's actually happening in real models is not yeah it's not that weird or magic. So Kurian is asking is PCA useful in any other areas? And the answer is absolutely. And what I suggest you do if you're interested is check out our computational linear algebra course. It's five years old now but I mean this is stuff which hasn't changed for decades really. And this will teach you all about things like PCA and stuff like that. It's not nearly as directly practical as practical deep learning for coders but it's definitely like very interesting and it's the kind of thing which if you want to go deeper you know it can become pretty useful later along your path. Okay so here's something else interesting we can do. Let's grab the movie factors. So that's in our model it's the item weights and it's the weight attribute that PyTorch creates. Okay and now we can convert the movie Silence of the Lambs into its class ID and we can do that with object to ID, O to I for the titles. And so that's the movie index of Silence of the Lambs. And what we can do now is we can look through all of the movies in our latent factors and calculate how far apart each vector is, each embedding vector is from this one. And this cosine similarity is very similar to basically the Euclidean distance, you know the kind of the root sum squared of the differences, but it normalizes it. So it's basically the angle between the vectors. So this is going to calculate how similar each movie is to the Silence of the Lambs based on these latent factors. And so then we can find which ID is the closest. Yeah, so based on this embedding distance the closest is dial m for murder, which makes a lot of sense. I'm not going to discuss it today, but in the book there's also some discussion about what's called the bootstrapping problem, which is the question of like if you've got a new company or a new product how would you get started with making recommendations given that you don't have any previous history with which to make recommendations. And that's a very interesting problem that you can read about in the book. Now that's one way to do collaborative filtering, which is where we create that, do that matrix completion exercise using all those dot products. There's a different way, however, which is we can use deep learning. And to do it with deep learning what we could do is we could basically create our user and item embeddings as per usual. And then we could create a sequential model. So sequential model is just layers of a deep learning neural network in order. And what we could do is we could just concatenate, so in forward we could just concatenate the user and item embeddings together and then do a value. So this is basically a single hidden layer neural network and then a linear layer at the end to create a single output. So this is the very world's most simple neural net, exactly the same as the style that we created back here in our neural net from scratch. This is exactly the same. But we're using PyTorch's functionality to do it more easily. So in the forward here we're going to, in exactly the same ways we have before, we'll look up the user embeddings and we'll look up the item embeddings. And then this is new. This is where we concatenate those two things together and put it through our neural network and then finally do our sigmoid. Now one thing different this time is that we're going to ask FastAI to figure out how big our embeddings should be. And so FastAI has something called get embedding sizes and it just uses a rule of thumb that says that for 944 users we recommend 74 factor embeddings and for 1,665 movies, or is it the other way around I can't remember, we recommend 102 factors for your embeddings. So that's what those sizes are. So now we can create that model and we can pop it into a learner and fit in the usual way. And so rather than doing all that from scratch what you can do is you can do exactly the same thing that we've done before which is to call collaborative learner but you can pass in the parameter use neural network equals true. And you can then say how big do you want each layer so this is going to create a two hidden layer deep learning neural net. The first will have 1500 and the second will have 50 and then you can say fit and away it goes. Okay so here is our we've got 0.87 so these are doing less well than our dot product version which is not too surprising because kind of the dot product version is really trying to take advantage of our understanding of the problem domain. In practice nowadays a lot of companies kind of combine they kind of create a combined model that has a dot product component and also has a neural net component. The neural net component is particularly helpful if you've got metadata for example information about your users like when did they sign up how old are they what sex are they you know where are they from and then those are all things that you could concatenate in with your embeddings and ditto with metadata about the movie how old is it what genre is it and so forth. All right so we've got a question from Jonah which I think is interesting and the question is is there an issue where the bias components are overwhelmingly determined by the non-experts in a genre? In general actually there's a more general issue which is in collaborative filtering systems very often a small number of users or a small number of movies overwhelm everybody else and the classic one is anime. A relatively small number of people watch anime and those groups of people watch a lot of anime. So in movie recommendations like there's a classic problem which is every time people try to make a list of well-loved movies all the top ones seem to be anime and so you can imagine what's happening in the matrix completion exercise is that there are yeah some users that just you know really watch this one genre of movie and they watch an awful lot of them. So in general you actually do have to be pretty careful about the you know these subtlety kind of issues and yeah I won't go into details about how to deal with them but they're generally involved taking various kinds of ratios or normalizing things or so forth. All right so that's collaborative filtering and I wanted to show you something interesting then about embeddings which is that embeddings are not just for collaborative filtering and in fact if you've heard about embeddings before you've probably heard about them in the context of natural language processing. So you might have been wondering back when we did the hugging face transformers stuff how did we go about you know using text as inputs to models? And we talked about how you can turn words into integers we make a list so here's the movie sorry movie here's the poem I am Sam I am Daniel I am Sam Sam I am that Sam I am etc etc. We can find a list of all the unique words in that poem and make this list here and then we can give each of those words a unique ID just arbitrarily well actually in this case it's alphabetical order but it doesn't have to be and so we kind of talked about that and that's what we do with categories in general but how do we turn those into like you know lists of random numbers and you might not be surprised to hear what we do is we create an embedding matrix. So here's an embedding matrix containing four latent factors for each word in the vocab. So here's each word in the vocab and here's the embedding matrix. So if we then want to present this poem to a neural net then what we do is we list out our poem I do not like that Sam I am do you like green eggs and ham etc. Then for each word we look it up so in Excel for example we use match so that will find this word over here and find it is word ID 8 and then we will find the eighth word and the first embedding and so that's gives us that's not right 8 oh no that is right sorry here it is it's just weird column it's so it's going to be point two two then point one point zero one and here it is point two two point one point zero one etc. So this is the embedding matrix we end up with for this poem and so if you wanted to train or use a train neural network on this poem you basically turn it into this matrix of numbers and so this is what an embedding matrix looks like in an NLP model and it works exactly the same way as you can see and then you can do exactly the same things in terms of interpretation of an NLP model by looking at both the bias factors and the latent factors in a word embedding matrix. So hopefully you're getting the idea here that our you know our different models you know the inputs to them that they're based on a relatively small number of kind of basic principles and these principles are generally things like lock up something in array and then we know inside the model we're basically multiplying things together adding them up and replacing the negatives and zeros. So hopefully you're getting the idea that what's going on inside a neural network is generally not that complicated but it happens very quickly and at scale. Now it's not just collaborative filtering and NLP but also tabular analysis. So in chapter 9 of the book we've talked about how random forests can be used for this which was for this is for the thing where we're predicting the auction sale price of industrial heavy equipment like bulldozers. Instead of using a random forest we can use a neural net. Now in this data set there are some continuous columns and there are some categorical columns. Now I'm not going to go into the details too much but in short the we can separate out the continuous columns and categorical columns using CONTCAT split and that will automatically find which is which based on their data types. And so in this case it looks like okay so continuous columns the elapsed sale date so I think that's the number of seconds or years or something since the start of the data set is a continuous variable. And then here are the categorical variables so for example there are six different product sizes and two coupler systems, 5059 model descriptions, six enclosures, 17 tire sizes and so forth. So we can use fast AI basically to say okay we'll take that data frame and pass in the categorical and continuous variables and create some random splits and what's the dependent variable and we can create data loaders from that. And from that we can create a tabular learner and basically what that's going to do is it's going to create a pretty regular multi-layer neural network not that different to this one that we created by hand. And each of the categorical variables it's going to create an embedding for it and so I can actually show you this right. So we're going to use tabular learner to create the learner and so tabular learner is one two three four five six seven eight nine lines of code and basically the main thing it does is create a tabular model and so then tabular model you're not going to understand all of it but you might be surprised at how much. So a tabular model is a module. We're going to be passing in how big is each embedding going to be and tabular learner what's that passing in? It's going to call get embedding sizes just like we did manually before automatically. So that's how it gets its embedding sizes and then it's going to create an embedding for each of those embedding sizes from number of inputs to number of factors. Dropout we're going to come back to later. BatchNorm we won't do until part two. So then it's going to create a layer for each of the layers we want which is going to contain a linear layer followed by batchNorm followed by dropout. It's going to add the sigmoid range we've talked about at the very end and so the forward code. This is the entire thing. If there's some embeddings it'll go through and get each of the embeddings using the same indexing approach we've used before. It'll concatenate them all together and then it'll run it through the layers of the neural net which are these. So yeah we don't know all of those details yet but we know quite a few of them. So that's encouraging hopefully. And once we've got that we can do the standard LR find and fit. Now this exact dataset was used in a Kaggle competition. This dataset was in a Kaggle competition and the third place getter published a paper about their technique and it's basically the exact almost the exact one I'm showing you here. So it wasn't this dataset it was a different one. It was about predicting the amount of sales in different stores but they used this basic kind of technique. And one of the interesting things is that they used a lot less manual feature engineering than the other high placed entries. Like they had a much simpler approach. And one of the interesting things they published a paper about their approach. So this is the team from this company. And they basically describe here exactly what I just showed you these different embedding layers being concatenated together and then going through a couple of layers of a neural network. And it's showing here it points out in the paper exactly what we learnt in the last lesson which is embedding layers are exactly equivalent to linear layers on top of a one hot encoded input. And yeah they found that their technique worked really well. One of the interesting things they also showed is that you can take you can create your neural net get your trained embeddings and then you can put those embeddings into a random forest or gradient boosted tree and your mean average percent error will dramatically improve. So you can actually combine random forests and embeddings or gradient boosted trees and embeddings which is really interesting. Now what I really wanted to show you though is what they then did. So as I said this was a thing about the predicted amount that different products would sell for at different shops around Germany. And what they did was they had a so one of their embedding matrices was embeddings by region and then they did I think this is a PCA principal component analysis of the embeddings for their German regions. And when they create a chart of them you can see that the locations that close together in the embedding matrix are the same locations that are close together in Germany. So you can see here's the blue ones and here's the blue ones. And again it's important to recognize that the data that they used had no information about the location of these places. The fact that they are close together geographically is something that was figured out as being something that actually helped it to predict sales. And so in fact they then did a plot showing each of these dots is a shop, a store, and it's showing for each pair of stores how far away is it in real life in metric space and then how far away is it in embedding space. And there's this very strong correlation. So it's kind of reconstructed somehow the geography of Germany by figuring out how people shop and similar for days of the week. So there was no information really about days of the week but when they put it on the embedding matrix the days of the week, Monday, Tuesday, Wednesday, close to each other, Thursday, Friday, close to each other, as you can see Saturday and Sunday close to each other, and ditto for months of the year, January, February, March, April, May, June. So yeah really interesting cool stuff I think. What's actually going on inside a neural network? Alright let's take a 10 minute break and I will see you back here at 7.10. Alright folks this is something I think is really fun which is we're going to we've looked at what goes into the start of a model, the input. We've learned about how they can be categories or embeddings and embeddings are basically kind of one hot encoded categories with a little compute trick or they can just be continuous numbers. We've learned about what comes out the other side which is a bunch of activations, so just a bunch of tensor of numbers which we can use things like softmax to constrain them to add up to one and so forth. And we've looked at what can go in the middle which is the matrix modellplies sandwiched together as rectified linear units. And I mentioned that there are other things that can go in the middle as well but we haven't really talked about what those other things are. So I thought we might look at one of the most important and interesting version of things that can go in the middle. But what you'll see is it turns out it's actually just another kind of matrix modellplication which might not be obvious at first but I'll explain. We're going to look at something called a convolution and convolutions are at the heart of a convolutional neural network. So the first thing to realise is a convolutional neural network is very very very similar to the neural networks we've seen so far. It's got inputs, it's got things that are a lot like or actually are a form of matrix modellplication sandwiched with activation functions which can be rectified linear. But there's a particular thing which makes them very useful for computer vision. And I'm going to show you using this excel spreadsheet that's in our repo called convexample. And we're going to look at it using an image from MNIST. So MNIST is kind of the world's most famous computer vision data set I think because it was like the first one really which really showed image recognition being cracked. It's pretty small by today's standards. It's a data set of handwritten digits. Each one is 28 by 28 pixels. And yeah, back in the mid-90s Jan Likun showed really practically useful performance on this data set and as a result ended up with conv nets being used in the American banking system for reading checks. So here's an example of one of those digits. This is a seven that somebody drew. It's one of those ones with a stroke through it. And this is what it looks like. This is the image. And so I got it from MNIST. This is just one of the images from MNIST which I put into Excel. And what you see in the next column is a version of the image where the horizontal lines are being recognized and another one where the vertical lines are being recognized. And if you think back to that Zyler and Fergus paper that talked about what the layers of a neural net does, this is absolutely an example of something that we know that the first layer of a neural network tends to learn how to do. Now how did I do this? I did this using something called a convolution. And so what we're going to do now is we're going to zoom in to this Excel notebook. We're going to keep zooming in. We're going to keep zooming in. So take a look, keep an eye on this image and you'll see that once we zoom in enough it's actually just made of numbers which as we discussed in the very first lesson we saw how images are made of numbers. So here they are, right? Here are the numbers between 0 and 1. And what I just did is I just used a little trick. I used Microsoft Excel's conditional formatting to basically make things that higher numbers more red. So that's how I turned this Excel sheet. And I've just rounded it off to the nearest decimal but they're actually bigger than that. And so here is the image as numbers. And so let me show you how we went about creating this top edge detector. What we did was we created this formula. Don't worry about the max. Let's focus on this. What it's doing is have a look at the colored in areas. It's taking each of these cells and multiplying them by each of these cells and then adding them up. And then we do the rectified linear part which is if that ends up less than 0 then make it 0. So this is like a rectified linear unit but it's not doing the normal matrix product. It's doing the equivalent of a dot product but just on these nine cells and with just these nine weights. So you might not be surprised to hear that if I move now one to the right then now it's using the next nine cells. So if I move like to the right quite a bit and down quite a bit to here it's using these nine cells. So it's still doing a dot product which as we know is a form of matrix multiplication. But it's doing it in this way where it's kind of taking advantage of the geometry of this situation that the things that are close to each other are being multiplied by this consistent group of the same nine weights each time. Because there's actually 28 by 28 numbers here right which I think is 768. 28 times 28 that's plus enough, 784. But we don't have 784 parameters we only have nine parameters. And so this is called a convolution. So a convolution is where you basically slide this kind of little 3 by 3 matrix across a bigger matrix and at each location you do a dot product of the corresponding elements of that 3 by 3 with the corresponding elements of this 3 by 3 matrix of coefficients. Now why does that create something that finds as you see top edges? Well it's because of the particular way I constructed this 3 by 3 matrix. What I said was that all of the rows just above, so these ones, are going to get a 1 and all of the ones just below are going to get a minus 1 and all of the ones in the middle are going to get a 0. So let's think about what happens somewhere like here. That is, let's try to find the right one, here it is. So here we're going to get 1 times 1 plus 1 times 1 plus 1 times 1 minus 1 times 1 minus 1 times 1 minus 1 times 1 we're going to get 0. But what about up here? Here we're going to get 1 times 1 plus 1 times 1 plus 1 times 1, these do nothing because they're times 0, minus 1 times 0. So we're going to get 3. So we're only going to get 3, the highest possible number, in the situation where these are all as black as possible, or in this case as red as possible, and these are all white. And so that's only going to happen at a horizontal edge. So the one underneath it does exactly the same thing, exactly the same formulas, the one underneath are exactly the same formulas, the 3 by 3 sliding thing here, but this time we've got a different little mini matrix of coefficients, which is all ones going down and all minus ones going down. And so for exactly the same reason, this will only be 3 in situations where they're all 1 here and they're all 0 here. So you can think of a convolution as being a sliding window of little mini dot products of these little 3 by 3 matrices. And they don't have to be 3 by 3, right? We could just have easily done 5 by 5 and then we'd have a 5 by 5 matrix of coefficients or whatever, whatever size you like. So the size of this is called its kernel size. This is a 3 by 3 kernel for this convolution. So then, because this is deep learning, we just repeat these steps again and again and again. So this is this layer I'm calling Conv1, it's the first convolutional layer. So Conv2, it's going to be a little bit different because on Conv1 we only had a single channel input, it's just black and white or, you know, yeah, black and white, grayscale, one channel. But now we've got two channels. We've got the, let's make it a little smaller so we can see better, we've got the horizontal edges channel and the vertical edges channel. And we'd have a similar thing in the first layer if it's color, we'd have a red channel, a green channel and a blue channel. So now our filter, this is called the filter, this little mini matrix is called the filter. Our filter now contains a 3 by 3 by depth 2, or if you want to think of another way, two 3 by 3 kernels or one 3 by 3 by 2 kernel. And we basically do exactly the same thing, which is we're going to multiply each of these by each of these and sum them up. But then we do it for the second bit as well. We multiply each of these by each of these and sum them up. And so that gives us, I think I just picked some random numbers here, right. So this is going to now be something which can combine, oh sorry, the second one, the second set, so it's, sorry, each of the red ones by each of the blue ones, that's here, plus each of the green ones times each of the mauve ones, that's here. So this first filter is being applied to the horizontal edge detector and the second filter is being applied to the vertical edge detector. And as a result we can end up with something that combines features of the two things. And so then we can have a second channel over here, which is just a different bunch of convolutions for each of the two channels, this one times this one. Again, you can see the colours. So what we could do is if, you know, once we kind of get to the end, we'll end up, I'll show you how in a moment, we'll end up with a single set of 10 activations, one per digit we're recognising, zero to nine. Or in this case I think we could just create one, you know, maybe we're just trying to recognise nothing but the number seven or not the number seven, so we could just have one activation. And then we would back propagate through this using SGD in the usual way. And that is going to end up optimising these numbers. So in this case I manually put in the numbers I knew would create edge detectors. In real life you start with random numbers and then you use SGD to optimise these parameters. Okay, so there's a few things we can do next. And I'm going to show you the way that was more common a few years ago and then I'll explain some changes that have been made more recently. What happened a few years ago was we would then take these activations, which as you can see these activations now are kind of in a grid pattern, and we would do something called maxPalling. And maxPalling is kind of like a convolution, it's a sliding window, but this time as the sliding window goes across, so here we're up to here, we don't do a dot product over a filter, but instead we just take a maximum. See here, this is the maximum of these four numbers. And if we go across a little bit, this is the maximum of these four numbers. Go across a bit, go across a bit, and so forth. Oh that goes off the edge. And you can see what happens when this is called a 2x2 maxPalling. So you can see what happens with a 2x2 maxPalling, we end up losing half of our activations on each dimension. So we're going to end up with only one quarter of the number of activations we used to have. And that's actually a good thing, because if we keep on doing convolution maxPall, convolution maxPall, we're going to get fewer and fewer and fewer activations until eventually we'll just have one left, which is what we want. That's effectively what we used to do, but the other thing I mentioned is we didn't normally keep going until there's only one left. What we used to then do is we'd basically say, okay, at some point we're going to take all of the activations that are left and we're going to basically just do a dot product of those with a bunch of coefficients, not as a convolution but just as a normal linear layer. And this is called the dense layer. And then we would add them all up. So we'd basically end up with our final big dot product of all of the maxPulled activations by all of the weights. And we'd do that for each channel. And so that would give us our final activation. And as I say here, MNIST would actually have 10 activations, so you'd have a separate set of weights for each of the digits you're predicting, and then softmax after that. Okay, nowadays we do things very slightly differently. Nowadays we normally don't have maxPull layers, but instead what we normally do is when we do our sliding window, like this one here, we don't normally, let's go back to C. So when I go one to the right, so currently we're starting in cell column G, if I go one to the right, the next one is column H. And if I go one to the right, the next one starts in column I. So you can see it's sliding the window every three by three. Nowadays what we tend to do instead is we generally skip one. So we would normally only look at every second. So we would, after doing column I, we would skip columns J and we'd go straight to column K. And that's called a stride two convolution. We do that both across the rows and down the columns. And what that means is every time we do a convolution, we reduce our effective kind of feature size, grid size, by two on each axis. So it reduces it by four in total. So that's basically instead of doing maxPulling. And then the other thing that we do differently is nowadays we don't normally have a single dense layer at the end, a single matrix multiply at the end. But instead what we do, we generally keep doing stride two convolutions. So each one's going to reduce the grid size by two by two. We keep going down until we've got about a seven by seven grid. And then we do a single pulling at the end. And we don't normally do maxPull nowadays. Instead we do an average pull. So we average the activations of each one of the seven by seven features. This is actually quite important to know because if you think about what that means, it means that something like an ImageNet style image detector is going to end up with a seven by seven grid. Let's say it's trying to say, is this a bear? And in each of the parts of the seven by seven grid, it's basically saying, is there a bear in this part of the photo? Is there a bear in this part of the photo? Is there a bear in this part of the photo? And then it takes the average of those 49 seven by seven predictions to decide whether there's a bear in the photo. That works very well if it's basically a photo of a bear, right? Because most, you know, if the bear is big and takes up most of the frame, then most of those seven by seven bits are bits of a bear. On the other hand, if it's a teeny tiny bear in the corner, then potentially only one of those 49 squares has a bear in it. And even worse, if it's like a picture of lots and lots of different things, only one of which is a bear, it could end up not being a great bear detector. And so this is where like the details of how we construct our model turn out to be important. And so if you're trying to find like just one part of a photo that has a small bear in it, you might decide to use maximum pooling instead of average pooling. Because max pooling will just say, I think this is a picture of a bear if any one of those 49 bits of my grid has something that looks like a bear in it. So these are potentially important details which often get hand waved over. Although you know, again, like the key thing here is that this is happening right at the very end, right? That max pool or that average pool. And actually Fast.ai handles this for you. We do a special thing which we kind of independently invented. I think we did it first, which is we do both max pool and average pool and we concatenate them together. We call that concat pooling. And that has since been reinvented in at least one paper. And so that means that you don't have to think too much about it because we're going to try both for you basically. So I mentioned that this is actually really just matrix multiplication. And to show you that, I'm going to show you some images created by a guy called Matthew Kleinsmith who did this actually, I think this is in our very first ever course. Might have been the part two, first part two course. And he basically pointed out that in a certain way of thinking about it, it turns out that convolution is the same thing as a matrix model player. So I want to show you how he shows this. He basically says, okay, let's take this three by three image and a two by two kernel containing the coefficients alpha, beta, gamma, delta. And so in this, as we slide the window over, each of the colors, each of the colors are multiplied together, red by red plus green by green plus, what is that, orange by orange plus blue by blue gives you this. And so to put it another way, algebraically p equals alpha times a plus beta times b, et cetera. And so then as we slide to this part, we're multiplying again, red by red, green by green so forth, so we can say q equals alpha times b plus beta times c, et cetera. And so this is how we calculate a convolution using the approach we just described as a sliding window. But here's another way of thinking about it. We could say, okay, we've got all these different things, a, b, c, d, e, f, g, h, j. Let's put them all into a single vector and then let's create a single matrix that has alpha, alpha, alpha, alpha, beta, beta, beta, beta, et cetera. And then if we do this matrix multiplied by this vector, we get this with these gray zeros in the appropriate places, which gives us this, which is the same as this. And so this shows that a convolution is actually a special kind of matrix multiplication. It's a matrix multiplication where there are some zeros that are fixed and some numbers that are forced to be the same. Now in practice it's going to be faster to do it this way, but it's a useful kind of thing to think about, I think, that just to realize like, oh, it's just another of these special types of matrix multiplications. Okay, I think, well, let's look at one more thing because there was one other thing that we saw and I mentioned we would look at in the tabular model, which is called dropout. And I actually have this in my Excel spreadsheet. If you go to the conv example dropout page, you'll see we've actually got a little bit more stuff here. We've got the same input as before and the same first convolution as before and the same second convolution as before. And then we've got a bunch of random numbers. They're showing as between 0 and 1, but they're actually, that's just because they're rounding off, they're actually random numbers between, you know, that are floats between 0 and 1. Over here, we're then saying if, let's have a look. So way up here, I'll zoom in a bit, I've got a dropout factor. Let's change this, say to 0.5. There we go. So over here, this is something that says if the random number in the equivalent place is greater than 0.5, then 1, otherwise 0. And so here's a whole bunch of 1s and 0s. Now this thing here is called a dropout mask. Now what happens is we multiply over here, we multiply the dropout mask and we multiply it by our filtered image. And what that means is we end up with exactly the same image we started with. Here's the image we started with. But it's corrupted. Random bits of it have been deleted. And based on the amount of dropout we use, so if we change it to, say, 0.2, not very much of it's deleted at all, so it's still very easy to recognize. Or else if we use lots of dropouts, say 0.8, it's almost impossible to see what the number was. And then we use this as the input to the next layer. So that seems weird. Why would we delete some data at random from our processed image, from our activations after a layer of convolutions? Well the reason is that a human is able to look at this corrupted image and still recognize it's a 7. And the idea is that a computer should be able to as well. And if we randomly delete different bits of the activations each time, then the computer is forced to learn the underlying real representation rather than overfitting. You can think of this as data augmentation, but it's data augmentation not for the inputs, but data augmentation for the activations. So this is called a dropout layer. And so dropout layers are really helpful for avoiding overfitting. And you can decide how much you want to compromise between good generalization, so avoiding overfitting, versus getting something that works really well on the training data. And so the more dropout you use, the less good it's going to be on the training data, but the better it ought to generalize. And so this comes from a paper by Jeffrey Hinton's group quite a few years ago now. Ruslan's now at Apple I think. And then Kujewski and Hinton went on to found Google Brain. And so you can see here they've got this picture of a fully connected neural network, two layers just like the one we built. And here, look, they're kind of randomly deleting some of the activations. And all that's left is these connections. And so that's a different bunch that's going to be deleted each batch. I thought this was an interesting point. So dropout, which is super important, was actually developed in a master's thesis. And it was rejected from the main neural networks conference, then called NIPS, now called NeurIPS. So it ended up being disseminated through Archive, which is a preprint server. And as has just been pointed out on our chat, that Ilya was one of the founders of OpenAI. I don't know what happened to Nitish. I think he went to Google Brain as well, maybe. So peer review is a very fallible thing in both directions. And it's great that we have preprint servers so we can read stuff like this, even if reviewers decide it's not worthy. It's been one of the most important papers ever. OK. Now I think that's given us a good tour now. We've really seen quite a few ways of dealing with input to a neural network, quite a few of the things that can happen in the middle of a neural network. We've only talked about rectified linear units, which is this one here. 0 if x is less than 0 or x otherwise. These are some of the other activations you can use. Don't use this one, of course, because you end up with a linear model. But they're all just different functions. I should mention, it turns out these don't matter very much. Pretty much any non-linearity works fine. So we don't spend much time talking about activation functions, even in part two of the course, just a little bit. So yeah, so we understand there are inputs. They can be one-hot encoded or embeddings, which is a computational shortcut. There are sandwich layers of matrix multipliers and activation functions. The matrix multipliers can sometimes be special cases, such as the convolutions or the embeddings. The output can go through some tweaking, such as the softmax. And then of course, you've got the loss function, such as cross entropy loss or mean squared error or mean absolute error. But there's nothing too crazy going on in there. So I feel like we've got a good sense now of what goes inside a wide range of neural nets. You're not going to see anything too weird from here. And we've also seen a wide range of applications. So before you come back to do part two, you know, what now? And we're going to have a little AMA session here. And in fact, one of the questions was what now? So this is quite good. One thing I strongly suggest is if you've got this far, it's probably worth you investing your time in reading Radek's book, which is meta-learning. And so meta-learning is very heavily based on the kind of teachings of fast AI over the last few years and is all about how to learn deep learning and learn pretty much anything. Yeah, because you know, you've got to this point, you may as well know how to get to the next point as well as possible. And the main thing you'll see that Radek talks about, or one of the main things, is practicing and writing. So if you've kind of zipped through the videos on 2x and haven't done any exercises, go back and watch the videos again. You know, a lot of the best students end up watching them two or three times, probably more like three times. And actually go through and code as you watch and experiment. You know, write posts, blog posts about what you're doing. Spend time on the forum, both helping others and seeing other people's answers to questions. Read the success stories on the forum and of people's projects to get inspiration for things you could try. One of the most important things to do is to get together with other people. For example, you can do a Zoom study group, in fact on our Discord, which you can find through our forum. There's always study groups going on. Or you can create your own. You know, a study group to go through the book together. And of course, build stuff. And sometimes it's tricky to always be able to build stuff for work, because maybe you're not quite in the right area or they're not quite ready to try out deep learning yet. But that's okay. You know, build some hobby projects. Build some stuff just for fun. Or build some stuff that you're passionate about. Yeah, so it's really important to not just put the videos away and go away and do something else because you'll forget everything you've learned and you won't have practiced. So one of our community members went on to create an activation function, for example, which is MISH, which is now, as Tanishka's just reminded me on our forums, is now used in many of the state-of-the-art networks around the world, which is pretty cool. And he's now at Miele, I think, a research, one of the top research labs in the world. I wonder how that's doing. Let's have a look. Go to Google Scholar. Nice. 486 citations. They're doing great. All right. Let's have a look at how our AMA topic is going and pick out some of the highest ranked AMAs. Okay. So the first one is from Lucas, and actually maybe I should, actually let's switch our view here. So our first AMA is from Lucas, and Lucas asks, how do you stay motivated? I often find myself overwhelmed in this field. There are so many new things coming up that I feel like I have to put so much energy just to keep my head above the waterline. That's a very interesting question. I think Lucas, the important thing is to realize you don't have to know everything. In fact, nobody knows everything, and that's okay. What people do is they take an interest in some area, and they follow that, and they try and do the best job they can of keeping up with some little sub area. If your little sub area is too much to keep up on, pick a sub sub area. Yeah, there's no need for it to be demotivating that there's a lot of people doing a lot of interesting work and a lot of different sub fields. That's cool. It used to be kind of dull when there's only basically five labs in the world working on Neuronets. And yeah, from time to time, take a dip into other areas that maybe you're not following as closely. But when you're just starting out, you'll find that things are not changing that fast at all, really. It can kind of look that way because people are always putting out press releases about their new tweaks. But fundamentally, the stuff that is in the course now is not that different to what was in the course five years ago. The foundations haven't changed. And it's not that different, in fact, to the convolutional neural network that Jan LeCun used on MNIST back in 1996. The basic ideas I've described are forever. The way the inputs work and the sandwiches of matrix multipliers and activation functions and the stuff you do to the final layer. Everything else is tweaks. And the more you learn about those basic ideas, the more you'll recognize those tweaks as simple little tweaks that you'll be able to quickly get your head around. So then Lucas goes on to ask or to comment, another thing that constantly bothers me is I feel the field is getting more and more skewed towards bigger and more computationally expensive models and huge amounts of data. I keep wondering if in some years from now I would still be able to train reasonable models with a single GPU or if everything is going to require a compute cluster. Yeah, that's a great question. I get that a lot. But interestingly, I've been teaching people machine learning and data science stuff for nearly 30 years and I've had a variation of this question throughout. And the reason is that engineers always want to push the envelope on the biggest computers they can find. That's just this fun thing engineers love to do. And by definition, they're going to get slightly better results than people doing exactly the same thing on smaller computers. So it always looks like, oh, you need big computers to be state of the art. But that's actually never true, right? Because there's always smarter ways to do things, not just bigger ways to do things. And so when you look at fast AI's Dawnbench success, when we trained ImageNet faster than anybody had trained it before on standard GPUs, me and a bunch of students, that was not meant to happen. Google was working very hard with their TPU introduction to try to show how good they were. Intel was using 256 PCs in parallel or something. But yeah, we used common sense and smarts and showed what can be done. It's also a case of picking the problems you solve. So I would not be probably doing like going head to head up against codecs and trying to create code from English descriptions. Because that's a problem that does probably require very large neural nets and very large amounts of data. But if you pick areas in different domains, there's still huge areas where much smaller models are still going to be state of the art. So hopefully that helped answer your question. Let's see what else we've got here. So Daniel has obviously been following my journey with teaching my daughter math. So I homeschool my daughter. And Daniel asks, how do you homeschool young children, science in general and math in particular? Would you share your experiences by blogging or in lectures someday? Yeah, I could do that. So I actually spent quite a few months just reading research papers about education recently. So I do probably have a lot I probably need to talk about at some stage. But yeah, broadly speaking, I lean into using computers and tablets a lot more than most people. Because actually there's an awful lot of really great apps that are super compelling. They're adaptive, so they go at the right speed for the student. And they're fun. And I really like my daughter to have fun. I really don't like to force her to do things. And for example, there's a really cool app called Dragon Box Algebra 5 Plus, which teaches algebra to five-year-olds by using a really fun computer game involving helping dragon eggs to hatch. And it turns out that yeah, algebra, the basic ideas of algebra are no more complex than the basic ideas that we do in other kindergarten math. And all the parents I know of who have given their kids Dragon Box Algebra 5 Plus, their kids have successfully learned algebra. So that would be an example. But yeah, we should talk about this more at some point. All right. Let's see what else we've got here. So Farah says, the walkthroughs have been a game changer for me. The knowledge and tips you shared in those sessions are skills required to become an effective machine learning practitioner and utilize fast AI more effectively. Have you considered making the walkthroughs a more formal part of the course, doing a separate software engineering course or continuing live coding sessions between part one and two? So yes, I am going to keep doing live coding sessions. At the moment, we've switched to those specifically to focusing on APL. And then in a couple of weeks, they're going to be going to fast AI study groups. And then after that, they'll gradually turn back into more live coding sessions. But yeah, the thing I try to do in my live coding or study groups or whatever is definitely try to show the foundational techniques that just make life easier as a coder or a data scientist. When I say foundational, I mean, yeah, the stuff which you can reuse again and again and again, like learning regular expressions really well or knowing how to use a VM or understanding how to use the terminal and command line, you know, all that kind of stuff never goes out of style. It never gets old. And yeah, I do plan to at some point, hopefully actually do a course really all about that stuff specifically. But yeah, for now, the for now, the best approach is follow along with the live coding and stuff. Okay, WGPUBs, which is Wade, asks, how do you turn a model into a business? Specifically, how does a coder with little or no startup experience turn an ML-based Gradio prototype into a legitimate business venture? Okay, I plan to do a course about this at some point as well. So you know, obviously there isn't a two-minute version to this, but the key thing with creating a legitimate business venture is to solve a legitimate problem, you know, a problem that people need solving and which they will pay you to solve. And so it's important not to start with your fun Gradio prototype as a basis for your business, but instead start with here's a problem I want to solve. And generally speaking, you should try to pick a problem that you understand better than most people. So it's either a problem that you face day to day in your work or in some hobby or passion that you have or that, you know, your club has or your local school has or your spouse deals with in their workplace. You know, it's something where you understand that there's something that doesn't work as well as it ought to, particularly something where you think to yourself, you know, if they just used deep learning here or some algorithm here or some better compute here, that problem would go away. And that's the start of a business. And so then my friend Eric Reese wrote a book called The Lean Startup where he describes what you do next, which is basically you fake it. You create, so he calls it the minimum viable product. You create something that solves that problem that takes you as little time as possible to create. It could be very manual. It can be loss making. It's fine. You know, even the bit in the middle where you're like, oh, there's going to be a neural net here. It's fine to like launch without the neural net and do everything by hand. You're just trying to find out are people going to pay for this and is this actually useful? And then once you have, you know, hopefully confirmed that the need is real and that people will pay for it and you can solve the need, you can gradually make it less and less of a fake, you know, and do, you know, more and more getting the product to where you want it to be. Okay, I don't know how to pronounce the name M-I-W-O-J-C. M-I-W-O-J-C says, Jeremy, can you share some of your productivity hacks from the content you produce? It may seem you work 24 hours a day. Okay I certainly don't do that. I think one of my main productivity hacks actually is not to work too hard or at least no not to work too hard, not to work too much. I spend probably less hours a day working than most people, I would guess. But I think I do a couple of things differently when I'm working. One is I've spent half, at least half of every working day since I was about 18, learning or practicing something new. Could be a new language, could be a new algorithm, could be something I read about. And nearly all of that time therefore I've been doing that thing more slowly than I would if I just used something I already knew. Which often drives my co-workers crazy because they're like, you know, why aren't you focusing on getting that thing done? But in the other 50% of the time I'm constantly building up this kind of exponentially improving base of expertise in a wide range of areas. And so now I do find I can do things, often orders of magnitude faster than people around me or certainly many multiples faster than people around me because I know a whole bunch of tools and skills and ideas which other people don't necessarily know. So I think that's one thing that's been helpful. And then another is trying to really not overdo things, like get good sleep and eat well and exercise well. And also I think it's a case of tenacity. I've noticed a lot of people give up much earlier than I do. So yeah, if you just keep going until something's actually finished then that's going to put you in a small minority to be honest. Most people don't do that. When I say finished, like finish something really nicely. And I try to make it like, so I particularly like coding and so I try to do a lot of coding related stuff. So I create things like NBdev and NBdev makes it much much easier for me to finish something nicely. So in my kind of chosen area I've spent quite a bit of time trying to make sure it's really easy for me to like get out a blog post, get out a Python library, get out a notebook analysis, whatever. So yeah, trying to make these things I want to do easier and so then I'll do them more. So well thank you everybody. That's been a lot of fun. Really appreciate you taking the time to go through this course with me. Yeah, if you enjoyed it, it would really help if you would give a like on YouTube because it really helps other people find the course, goes into the YouTube recommendation system. And please do come and help other beginners on forums.fast.ai. It's a great way to learn yourself is to try to teach other people. And yeah, I hope you'll join us in part two. Thanks everybody very much. I've really enjoyed this process and I hope to get to meet more of you in person in the future. Bye.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 17.22, "text": " So, welcome to the last lesson of part 1 of Practical Deep Learning for Coders.", "tokens": [407, 11, 2928, 281, 264, 1036, 6898, 295, 644, 502, 295, 19170, 804, 14895, 15205, 337, 383, 378, 433, 13], "temperature": 0.0, "avg_logprob": -0.15852929245341907, "compression_ratio": 1.255639097744361, "no_speech_prob": 0.003074588719755411}, {"id": 1, "seek": 0, "start": 17.22, "end": 27.34, "text": " It's been a really fun time doing this course and depending on when you're watching and", "tokens": [467, 311, 668, 257, 534, 1019, 565, 884, 341, 1164, 293, 5413, 322, 562, 291, 434, 1976, 293], "temperature": 0.0, "avg_logprob": -0.15852929245341907, "compression_ratio": 1.255639097744361, "no_speech_prob": 0.003074588719755411}, {"id": 2, "seek": 2734, "start": 27.34, "end": 34.04, "text": " listening to this you may want to check the forums or the fast.ai website to see whether", "tokens": [4764, 281, 341, 291, 815, 528, 281, 1520, 264, 26998, 420, 264, 2370, 13, 1301, 3144, 281, 536, 1968], "temperature": 0.0, "avg_logprob": -0.12066280658428485, "compression_ratio": 1.5393939393939393, "no_speech_prob": 0.00010386348731117323}, {"id": 3, "seek": 2734, "start": 34.04, "end": 42.64, "text": " we have a part 2 planned which is going to be sometime towards the end of 2022.", "tokens": [321, 362, 257, 644, 568, 8589, 597, 307, 516, 281, 312, 15053, 3030, 264, 917, 295, 20229, 13], "temperature": 0.0, "avg_logprob": -0.12066280658428485, "compression_ratio": 1.5393939393939393, "no_speech_prob": 0.00010386348731117323}, {"id": 4, "seek": 2734, "start": 42.64, "end": 49.92, "text": " Or if it's already past that then maybe there's even a part 2 already on the website.", "tokens": [1610, 498, 309, 311, 1217, 1791, 300, 550, 1310, 456, 311, 754, 257, 644, 568, 1217, 322, 264, 3144, 13], "temperature": 0.0, "avg_logprob": -0.12066280658428485, "compression_ratio": 1.5393939393939393, "no_speech_prob": 0.00010386348731117323}, {"id": 5, "seek": 4992, "start": 49.92, "end": 57.64, "text": " The part 2 goes a lot deeper than part 1 technically in terms of getting to the point that you", "tokens": [440, 644, 568, 1709, 257, 688, 7731, 813, 644, 502, 12120, 294, 2115, 295, 1242, 281, 264, 935, 300, 291], "temperature": 0.0, "avg_logprob": -0.17372567313058035, "compression_ratio": 1.4265734265734267, "no_speech_prob": 9.079756637220271e-06}, {"id": 6, "seek": 4992, "start": 57.64, "end": 67.14, "text": " should be able to read and implement research papers and deploy models in a very kind of", "tokens": [820, 312, 1075, 281, 1401, 293, 4445, 2132, 10577, 293, 7274, 5245, 294, 257, 588, 733, 295], "temperature": 0.0, "avg_logprob": -0.17372567313058035, "compression_ratio": 1.4265734265734267, "no_speech_prob": 9.079756637220271e-06}, {"id": 7, "seek": 4992, "start": 67.14, "end": 69.76, "text": " real life situation.", "tokens": [957, 993, 2590, 13], "temperature": 0.0, "avg_logprob": -0.17372567313058035, "compression_ratio": 1.4265734265734267, "no_speech_prob": 9.079756637220271e-06}, {"id": 8, "seek": 6976, "start": 69.76, "end": 87.04, "text": " So yeah, last lesson we started on the collaborative filtering notebook and we were looking at", "tokens": [407, 1338, 11, 1036, 6898, 321, 1409, 322, 264, 16555, 30822, 21060, 293, 321, 645, 1237, 412], "temperature": 0.0, "avg_logprob": -0.1079845564705985, "compression_ratio": 1.7, "no_speech_prob": 7.2961697696882766e-06}, {"id": 9, "seek": 6976, "start": 87.04, "end": 90.80000000000001, "text": " collaborative filtering and this is where we got to which is creating your own embedding", "tokens": [16555, 30822, 293, 341, 307, 689, 321, 658, 281, 597, 307, 4084, 428, 1065, 12240, 3584], "temperature": 0.0, "avg_logprob": -0.1079845564705985, "compression_ratio": 1.7, "no_speech_prob": 7.2961697696882766e-06}, {"id": 10, "seek": 6976, "start": 90.80000000000001, "end": 96.4, "text": " module and this is a very cool place to start the lesson because you're going to learn a", "tokens": [10088, 293, 341, 307, 257, 588, 1627, 1081, 281, 722, 264, 6898, 570, 291, 434, 516, 281, 1466, 257], "temperature": 0.0, "avg_logprob": -0.1079845564705985, "compression_ratio": 1.7, "no_speech_prob": 7.2961697696882766e-06}, {"id": 11, "seek": 6976, "start": 96.4, "end": 98.72, "text": " lot about what's really going on.", "tokens": [688, 466, 437, 311, 534, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.1079845564705985, "compression_ratio": 1.7, "no_speech_prob": 7.2961697696882766e-06}, {"id": 12, "seek": 9872, "start": 98.72, "end": 107.08, "text": " And it's really important before you dig into this to make sure that you're really comfortable", "tokens": [400, 309, 311, 534, 1021, 949, 291, 2528, 666, 341, 281, 652, 988, 300, 291, 434, 534, 4619], "temperature": 0.0, "avg_logprob": -0.1590116024017334, "compression_ratio": 1.4303030303030304, "no_speech_prob": 3.1199626391753554e-05}, {"id": 13, "seek": 9872, "start": 107.08, "end": 114.36, "text": " with the 05 ADM model neural net from scratch notebook.", "tokens": [365, 264, 1958, 20, 9135, 44, 2316, 18161, 2533, 490, 8459, 21060, 13], "temperature": 0.0, "avg_logprob": -0.1590116024017334, "compression_ratio": 1.4303030303030304, "no_speech_prob": 3.1199626391753554e-05}, {"id": 14, "seek": 9872, "start": 114.36, "end": 125.24, "text": " So if parts of this are not totally clear put it aside and redo this notebook because", "tokens": [407, 498, 3166, 295, 341, 366, 406, 3879, 1850, 829, 309, 7359, 293, 29956, 341, 21060, 570], "temperature": 0.0, "avg_logprob": -0.1590116024017334, "compression_ratio": 1.4303030303030304, "no_speech_prob": 3.1199626391753554e-05}, {"id": 15, "seek": 12524, "start": 125.24, "end": 130.07999999999998, "text": " what we're looking at from here are kind of the abstractions that PyTorch and fast.ai", "tokens": [437, 321, 434, 1237, 412, 490, 510, 366, 733, 295, 264, 12649, 626, 300, 9953, 51, 284, 339, 293, 2370, 13, 1301], "temperature": 0.0, "avg_logprob": -0.1349738632760397, "compression_ratio": 1.5687203791469195, "no_speech_prob": 4.637737674784148e-06}, {"id": 16, "seek": 12524, "start": 130.07999999999998, "end": 136.4, "text": " add on top of functionality that we've built ourselves from scratch.", "tokens": [909, 322, 1192, 295, 14980, 300, 321, 600, 3094, 4175, 490, 8459, 13], "temperature": 0.0, "avg_logprob": -0.1349738632760397, "compression_ratio": 1.5687203791469195, "no_speech_prob": 4.637737674784148e-06}, {"id": 17, "seek": 12524, "start": 136.4, "end": 143.6, "text": " So if you remember in the neural network from scratch we built, we initialized a number", "tokens": [407, 498, 291, 1604, 294, 264, 18161, 3209, 490, 8459, 321, 3094, 11, 321, 5883, 1602, 257, 1230], "temperature": 0.0, "avg_logprob": -0.1349738632760397, "compression_ratio": 1.5687203791469195, "no_speech_prob": 4.637737674784148e-06}, {"id": 18, "seek": 12524, "start": 143.6, "end": 153.51999999999998, "text": " of coefficients, a couple of different layers, you know, and a bias term and then during", "tokens": [295, 31994, 11, 257, 1916, 295, 819, 7914, 11, 291, 458, 11, 293, 257, 12577, 1433, 293, 550, 1830], "temperature": 0.0, "avg_logprob": -0.1349738632760397, "compression_ratio": 1.5687203791469195, "no_speech_prob": 4.637737674784148e-06}, {"id": 19, "seek": 15352, "start": 153.52, "end": 160.32000000000002, "text": " as the model trained we updated those coefficients by going through each layer of them and subtracting", "tokens": [382, 264, 2316, 8895, 321, 10588, 729, 31994, 538, 516, 807, 1184, 4583, 295, 552, 293, 16390, 278], "temperature": 0.0, "avg_logprob": -0.1492280037172379, "compression_ratio": 1.6682242990654206, "no_speech_prob": 5.507528385351179e-06}, {"id": 20, "seek": 15352, "start": 160.32000000000002, "end": 166.8, "text": " out the gradients by the learning rate.", "tokens": [484, 264, 2771, 2448, 538, 264, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.1492280037172379, "compression_ratio": 1.6682242990654206, "no_speech_prob": 5.507528385351179e-06}, {"id": 21, "seek": 15352, "start": 166.8, "end": 171.96, "text": " You've probably noticed that in PyTorch we don't have to go to all that trouble and I", "tokens": [509, 600, 1391, 5694, 300, 294, 9953, 51, 284, 339, 321, 500, 380, 362, 281, 352, 281, 439, 300, 5253, 293, 286], "temperature": 0.0, "avg_logprob": -0.1492280037172379, "compression_ratio": 1.6682242990654206, "no_speech_prob": 5.507528385351179e-06}, {"id": 22, "seek": 15352, "start": 171.96, "end": 174.88, "text": " wanted to show you how PyTorch does this.", "tokens": [1415, 281, 855, 291, 577, 9953, 51, 284, 339, 775, 341, 13], "temperature": 0.0, "avg_logprob": -0.1492280037172379, "compression_ratio": 1.6682242990654206, "no_speech_prob": 5.507528385351179e-06}, {"id": 23, "seek": 15352, "start": 174.88, "end": 181.56, "text": " PyTorch, we don't have to keep track of what our coefficients or parameters or weights", "tokens": [9953, 51, 284, 339, 11, 321, 500, 380, 362, 281, 1066, 2837, 295, 437, 527, 31994, 420, 9834, 420, 17443], "temperature": 0.0, "avg_logprob": -0.1492280037172379, "compression_ratio": 1.6682242990654206, "no_speech_prob": 5.507528385351179e-06}, {"id": 24, "seek": 18156, "start": 181.56, "end": 183.64000000000001, "text": " are.", "tokens": [366, 13], "temperature": 0.0, "avg_logprob": -0.1236677974103445, "compression_ratio": 1.696808510638298, "no_speech_prob": 2.332058784304536e-06}, {"id": 25, "seek": 18156, "start": 183.64000000000001, "end": 194.2, "text": " PyTorch does that for us and the way it does that is it looks inside our module and it", "tokens": [9953, 51, 284, 339, 775, 300, 337, 505, 293, 264, 636, 309, 775, 300, 307, 309, 1542, 1854, 527, 10088, 293, 309], "temperature": 0.0, "avg_logprob": -0.1236677974103445, "compression_ratio": 1.696808510638298, "no_speech_prob": 2.332058784304536e-06}, {"id": 26, "seek": 18156, "start": 194.2, "end": 200.6, "text": " tries to find anything that looks like a neural network parameter or a tensor of neural network", "tokens": [9898, 281, 915, 1340, 300, 1542, 411, 257, 18161, 3209, 13075, 420, 257, 40863, 295, 18161, 3209], "temperature": 0.0, "avg_logprob": -0.1236677974103445, "compression_ratio": 1.696808510638298, "no_speech_prob": 2.332058784304536e-06}, {"id": 27, "seek": 18156, "start": 200.6, "end": 204.2, "text": " parameters and it keeps track of them.", "tokens": [9834, 293, 309, 5965, 2837, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.1236677974103445, "compression_ratio": 1.696808510638298, "no_speech_prob": 2.332058784304536e-06}, {"id": 28, "seek": 18156, "start": 204.2, "end": 210.2, "text": " And so here is a class we've created called T which is a subclass of module and I've created", "tokens": [400, 370, 510, 307, 257, 1508, 321, 600, 2942, 1219, 314, 597, 307, 257, 1422, 11665, 295, 10088, 293, 286, 600, 2942], "temperature": 0.0, "avg_logprob": -0.1236677974103445, "compression_ratio": 1.696808510638298, "no_speech_prob": 2.332058784304536e-06}, {"id": 29, "seek": 21020, "start": 210.2, "end": 213.76, "text": " one thing inside it which is something with the attribute a.", "tokens": [472, 551, 1854, 309, 597, 307, 746, 365, 264, 19667, 257, 13], "temperature": 0.0, "avg_logprob": -0.08900039304386485, "compression_ratio": 1.6585365853658536, "no_speech_prob": 6.179379283821618e-07}, {"id": 30, "seek": 21020, "start": 213.76, "end": 220.11999999999998, "text": " So this is a in the T module and it just contains three ones.", "tokens": [407, 341, 307, 257, 294, 264, 314, 10088, 293, 309, 445, 8306, 1045, 2306, 13], "temperature": 0.0, "avg_logprob": -0.08900039304386485, "compression_ratio": 1.6585365853658536, "no_speech_prob": 6.179379283821618e-07}, {"id": 31, "seek": 21020, "start": 220.11999999999998, "end": 224.1, "text": " And so the idea is, you know, maybe we're creating a module and this is we're initializing", "tokens": [400, 370, 264, 1558, 307, 11, 291, 458, 11, 1310, 321, 434, 4084, 257, 10088, 293, 341, 307, 321, 434, 5883, 3319], "temperature": 0.0, "avg_logprob": -0.08900039304386485, "compression_ratio": 1.6585365853658536, "no_speech_prob": 6.179379283821618e-07}, {"id": 32, "seek": 21020, "start": 224.1, "end": 226.76, "text": " some parameter that we want to train.", "tokens": [512, 13075, 300, 321, 528, 281, 3847, 13], "temperature": 0.0, "avg_logprob": -0.08900039304386485, "compression_ratio": 1.6585365853658536, "no_speech_prob": 6.179379283821618e-07}, {"id": 33, "seek": 21020, "start": 226.76, "end": 232.92, "text": " Now we can find out what trainable parameters or just what parameters in general PyTorch", "tokens": [823, 321, 393, 915, 484, 437, 3847, 712, 9834, 420, 445, 437, 9834, 294, 2674, 9953, 51, 284, 339], "temperature": 0.0, "avg_logprob": -0.08900039304386485, "compression_ratio": 1.6585365853658536, "no_speech_prob": 6.179379283821618e-07}, {"id": 34, "seek": 23292, "start": 232.92, "end": 241.92, "text": " knows about in our model by instantiating our model and then asking for the parameters", "tokens": [3255, 466, 294, 527, 2316, 538, 9836, 72, 990, 527, 2316, 293, 550, 3365, 337, 264, 9834], "temperature": 0.0, "avg_logprob": -0.10699490138462611, "compression_ratio": 1.732394366197183, "no_speech_prob": 1.1544575500010978e-06}, {"id": 35, "seek": 23292, "start": 241.92, "end": 246.07999999999998, "text": " which you then have to turn that into a list or in fastcore we have a thing called capital", "tokens": [597, 291, 550, 362, 281, 1261, 300, 666, 257, 1329, 420, 294, 2370, 12352, 321, 362, 257, 551, 1219, 4238], "temperature": 0.0, "avg_logprob": -0.10699490138462611, "compression_ratio": 1.732394366197183, "no_speech_prob": 1.1544575500010978e-06}, {"id": 36, "seek": 23292, "start": 246.07999999999998, "end": 251.39999999999998, "text": " L which is like a fancy list which prints out the number of items in the list and shows", "tokens": [441, 597, 307, 411, 257, 10247, 1329, 597, 22305, 484, 264, 1230, 295, 4754, 294, 264, 1329, 293, 3110], "temperature": 0.0, "avg_logprob": -0.10699490138462611, "compression_ratio": 1.732394366197183, "no_speech_prob": 1.1544575500010978e-06}, {"id": 37, "seek": 23292, "start": 251.39999999999998, "end": 253.0, "text": " you those items.", "tokens": [291, 729, 4754, 13], "temperature": 0.0, "avg_logprob": -0.10699490138462611, "compression_ratio": 1.732394366197183, "no_speech_prob": 1.1544575500010978e-06}, {"id": 38, "seek": 23292, "start": 253.0, "end": 260.08, "text": " Now in this case when we create our object of type T and ask for its parameters we get", "tokens": [823, 294, 341, 1389, 562, 321, 1884, 527, 2657, 295, 2010, 314, 293, 1029, 337, 1080, 9834, 321, 483], "temperature": 0.0, "avg_logprob": -0.10699490138462611, "compression_ratio": 1.732394366197183, "no_speech_prob": 1.1544575500010978e-06}, {"id": 39, "seek": 26008, "start": 260.08, "end": 264.76, "text": " told there are zero tensors of parameters and a list with nothing in it.", "tokens": [1907, 456, 366, 4018, 10688, 830, 295, 9834, 293, 257, 1329, 365, 1825, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.10927352905273438, "compression_ratio": 1.6542056074766356, "no_speech_prob": 2.684188530110987e-06}, {"id": 40, "seek": 26008, "start": 264.76, "end": 266.28, "text": " Now why is that?", "tokens": [823, 983, 307, 300, 30], "temperature": 0.0, "avg_logprob": -0.10927352905273438, "compression_ratio": 1.6542056074766356, "no_speech_prob": 2.684188530110987e-06}, {"id": 41, "seek": 26008, "start": 266.28, "end": 269.84, "text": " We actually said we wanted to create a tensor with three ones in it.", "tokens": [492, 767, 848, 321, 1415, 281, 1884, 257, 40863, 365, 1045, 2306, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.10927352905273438, "compression_ratio": 1.6542056074766356, "no_speech_prob": 2.684188530110987e-06}, {"id": 42, "seek": 26008, "start": 269.84, "end": 273.03999999999996, "text": " How would we make those parameters?", "tokens": [1012, 576, 321, 652, 729, 9834, 30], "temperature": 0.0, "avg_logprob": -0.10927352905273438, "compression_ratio": 1.6542056074766356, "no_speech_prob": 2.684188530110987e-06}, {"id": 43, "seek": 26008, "start": 273.03999999999996, "end": 282.32, "text": " Well the answer is that the way you tell PyTorch what your parameters are is you actually just", "tokens": [1042, 264, 1867, 307, 300, 264, 636, 291, 980, 9953, 51, 284, 339, 437, 428, 9834, 366, 307, 291, 767, 445], "temperature": 0.0, "avg_logprob": -0.10927352905273438, "compression_ratio": 1.6542056074766356, "no_speech_prob": 2.684188530110987e-06}, {"id": 44, "seek": 26008, "start": 282.32, "end": 286.96, "text": " have to put them inside a special object called an nn.parameter.", "tokens": [362, 281, 829, 552, 1854, 257, 2121, 2657, 1219, 364, 297, 77, 13, 2181, 335, 2398, 13], "temperature": 0.0, "avg_logprob": -0.10927352905273438, "compression_ratio": 1.6542056074766356, "no_speech_prob": 2.684188530110987e-06}, {"id": 45, "seek": 28696, "start": 286.96, "end": 291.35999999999996, "text": " This thing almost doesn't really do anything.", "tokens": [639, 551, 1920, 1177, 380, 534, 360, 1340, 13], "temperature": 0.0, "avg_logprob": -0.11622235115538253, "compression_ratio": 1.7115384615384615, "no_speech_prob": 5.7718980315257795e-06}, {"id": 46, "seek": 28696, "start": 291.35999999999996, "end": 295.08, "text": " In fact last time I checked it really quite literally had almost no code in it.", "tokens": [682, 1186, 1036, 565, 286, 10033, 309, 534, 1596, 3736, 632, 1920, 572, 3089, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.11622235115538253, "compression_ratio": 1.7115384615384615, "no_speech_prob": 5.7718980315257795e-06}, {"id": 47, "seek": 28696, "start": 295.08, "end": 302.03999999999996, "text": " Sometimes these things change but let's take a look.", "tokens": [4803, 613, 721, 1319, 457, 718, 311, 747, 257, 574, 13], "temperature": 0.0, "avg_logprob": -0.11622235115538253, "compression_ratio": 1.7115384615384615, "no_speech_prob": 5.7718980315257795e-06}, {"id": 48, "seek": 28696, "start": 302.03999999999996, "end": 310.79999999999995, "text": " Yeah okay so it's about a dozen lines of code or 20 lines of code which does almost nothing.", "tokens": [865, 1392, 370, 309, 311, 466, 257, 16654, 3876, 295, 3089, 420, 945, 3876, 295, 3089, 597, 775, 1920, 1825, 13], "temperature": 0.0, "avg_logprob": -0.11622235115538253, "compression_ratio": 1.7115384615384615, "no_speech_prob": 5.7718980315257795e-06}, {"id": 49, "seek": 28696, "start": 310.79999999999995, "end": 314.52, "text": " It's got a way of being copied, it's got a way of printing itself, it's got a way of", "tokens": [467, 311, 658, 257, 636, 295, 885, 25365, 11, 309, 311, 658, 257, 636, 295, 14699, 2564, 11, 309, 311, 658, 257, 636, 295], "temperature": 0.0, "avg_logprob": -0.11622235115538253, "compression_ratio": 1.7115384615384615, "no_speech_prob": 5.7718980315257795e-06}, {"id": 50, "seek": 31452, "start": 314.52, "end": 319.71999999999997, "text": " saving itself, and it's got a way of being initialized.", "tokens": [6816, 2564, 11, 293, 309, 311, 658, 257, 636, 295, 885, 5883, 1602, 13], "temperature": 0.0, "avg_logprob": -0.072528844469049, "compression_ratio": 1.6363636363636365, "no_speech_prob": 1.5534941439909744e-06}, {"id": 51, "seek": 31452, "start": 319.71999999999997, "end": 321.71999999999997, "text": " So a parameter hardly does anything.", "tokens": [407, 257, 13075, 13572, 775, 1340, 13], "temperature": 0.0, "avg_logprob": -0.072528844469049, "compression_ratio": 1.6363636363636365, "no_speech_prob": 1.5534941439909744e-06}, {"id": 52, "seek": 31452, "start": 321.71999999999997, "end": 328.64, "text": " The key thing is though that when PyTorch checks to see which parameters should it update,", "tokens": [440, 2141, 551, 307, 1673, 300, 562, 9953, 51, 284, 339, 13834, 281, 536, 597, 9834, 820, 309, 5623, 11], "temperature": 0.0, "avg_logprob": -0.072528844469049, "compression_ratio": 1.6363636363636365, "no_speech_prob": 1.5534941439909744e-06}, {"id": 53, "seek": 31452, "start": 328.64, "end": 334.71999999999997, "text": " when it optimizes it just looks for anything that's been wrapped in this parameter class.", "tokens": [562, 309, 5028, 5660, 309, 445, 1542, 337, 1340, 300, 311, 668, 14226, 294, 341, 13075, 1508, 13], "temperature": 0.0, "avg_logprob": -0.072528844469049, "compression_ratio": 1.6363636363636365, "no_speech_prob": 1.5534941439909744e-06}, {"id": 54, "seek": 31452, "start": 334.71999999999997, "end": 338.96, "text": " So if we do exactly the same thing as before which is to set an attribute containing a", "tokens": [407, 498, 321, 360, 2293, 264, 912, 551, 382, 949, 597, 307, 281, 992, 364, 19667, 19273, 257], "temperature": 0.0, "avg_logprob": -0.072528844469049, "compression_ratio": 1.6363636363636365, "no_speech_prob": 1.5534941439909744e-06}, {"id": 55, "seek": 33896, "start": 338.96, "end": 347.64, "text": " tensor with three ones in it, but this case we wrap it in a parameter, we now get told", "tokens": [40863, 365, 1045, 2306, 294, 309, 11, 457, 341, 1389, 321, 7019, 309, 294, 257, 13075, 11, 321, 586, 483, 1907], "temperature": 0.0, "avg_logprob": -0.12804832665816598, "compression_ratio": 1.7429906542056075, "no_speech_prob": 8.186358400052995e-07}, {"id": 56, "seek": 33896, "start": 347.64, "end": 355.44, "text": " okay there's one parameter tensor in this model and it contains a tensor with three", "tokens": [1392, 456, 311, 472, 13075, 40863, 294, 341, 2316, 293, 309, 8306, 257, 40863, 365, 1045], "temperature": 0.0, "avg_logprob": -0.12804832665816598, "compression_ratio": 1.7429906542056075, "no_speech_prob": 8.186358400052995e-07}, {"id": 57, "seek": 33896, "start": 355.44, "end": 357.88, "text": " ones.", "tokens": [2306, 13], "temperature": 0.0, "avg_logprob": -0.12804832665816598, "compression_ratio": 1.7429906542056075, "no_speech_prob": 8.186358400052995e-07}, {"id": 58, "seek": 33896, "start": 357.88, "end": 362.79999999999995, "text": " And you can see it also actually by default assumes that we're going to want require gradient.", "tokens": [400, 291, 393, 536, 309, 611, 767, 538, 7576, 37808, 300, 321, 434, 516, 281, 528, 3651, 16235, 13], "temperature": 0.0, "avg_logprob": -0.12804832665816598, "compression_ratio": 1.7429906542056075, "no_speech_prob": 8.186358400052995e-07}, {"id": 59, "seek": 33896, "start": 362.79999999999995, "end": 366.32, "text": " It's assuming that anything that's a parameter is something that you want to calculate gradients", "tokens": [467, 311, 11926, 300, 1340, 300, 311, 257, 13075, 307, 746, 300, 291, 528, 281, 8873, 2771, 2448], "temperature": 0.0, "avg_logprob": -0.12804832665816598, "compression_ratio": 1.7429906542056075, "no_speech_prob": 8.186358400052995e-07}, {"id": 60, "seek": 33896, "start": 366.32, "end": 368.52, "text": " for.", "tokens": [337, 13], "temperature": 0.0, "avg_logprob": -0.12804832665816598, "compression_ratio": 1.7429906542056075, "no_speech_prob": 8.186358400052995e-07}, {"id": 61, "seek": 36852, "start": 368.52, "end": 374.08, "text": " Now most of the time we don't have to do this because PyTorch provides lots of convenient", "tokens": [823, 881, 295, 264, 565, 321, 500, 380, 362, 281, 360, 341, 570, 9953, 51, 284, 339, 6417, 3195, 295, 10851], "temperature": 0.0, "avg_logprob": -0.12590474348801833, "compression_ratio": 1.5358851674641147, "no_speech_prob": 1.3497003692464205e-06}, {"id": 62, "seek": 36852, "start": 374.08, "end": 381.28, "text": " things for us such as what you've seen before nn.linear which is something that also creates", "tokens": [721, 337, 505, 1270, 382, 437, 291, 600, 1612, 949, 297, 77, 13, 28263, 597, 307, 746, 300, 611, 7829], "temperature": 0.0, "avg_logprob": -0.12590474348801833, "compression_ratio": 1.5358851674641147, "no_speech_prob": 1.3497003692464205e-06}, {"id": 63, "seek": 36852, "start": 381.28, "end": 382.28, "text": " a tensor.", "tokens": [257, 40863, 13], "temperature": 0.0, "avg_logprob": -0.12590474348801833, "compression_ratio": 1.5358851674641147, "no_speech_prob": 1.3497003692464205e-06}, {"id": 64, "seek": 36852, "start": 382.28, "end": 390.64, "text": " So this would create a tensor of 1 by 3 without a bias term in it.", "tokens": [407, 341, 576, 1884, 257, 40863, 295, 502, 538, 805, 1553, 257, 12577, 1433, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.12590474348801833, "compression_ratio": 1.5358851674641147, "no_speech_prob": 1.3497003692464205e-06}, {"id": 65, "seek": 36852, "start": 390.64, "end": 394.59999999999997, "text": " This is not being wrapped in an nn.parameter but that's okay.", "tokens": [639, 307, 406, 885, 14226, 294, 364, 297, 77, 13, 2181, 335, 2398, 457, 300, 311, 1392, 13], "temperature": 0.0, "avg_logprob": -0.12590474348801833, "compression_ratio": 1.5358851674641147, "no_speech_prob": 1.3497003692464205e-06}, {"id": 66, "seek": 39460, "start": 394.6, "end": 400.64000000000004, "text": " PyTorch knows that anything which is basically a layer in a neural net is going to be a parameter.", "tokens": [9953, 51, 284, 339, 3255, 300, 1340, 597, 307, 1936, 257, 4583, 294, 257, 18161, 2533, 307, 516, 281, 312, 257, 13075, 13], "temperature": 0.0, "avg_logprob": -0.11826089576438621, "compression_ratio": 1.7813765182186234, "no_speech_prob": 2.295911372129922e-06}, {"id": 67, "seek": 39460, "start": 400.64000000000004, "end": 405.56, "text": " So it automatically considers this a parameter.", "tokens": [407, 309, 6772, 33095, 341, 257, 13075, 13], "temperature": 0.0, "avg_logprob": -0.11826089576438621, "compression_ratio": 1.7813765182186234, "no_speech_prob": 2.295911372129922e-06}, {"id": 68, "seek": 39460, "start": 405.56, "end": 408.92, "text": " So here's exactly the same thing again I'll construct my object of type T, I'll check", "tokens": [407, 510, 311, 2293, 264, 912, 551, 797, 286, 603, 7690, 452, 2657, 295, 2010, 314, 11, 286, 603, 1520], "temperature": 0.0, "avg_logprob": -0.11826089576438621, "compression_ratio": 1.7813765182186234, "no_speech_prob": 2.295911372129922e-06}, {"id": 69, "seek": 39460, "start": 408.92, "end": 414.52000000000004, "text": " for its parameters and I can see there's one tensor of parameters and there's our three", "tokens": [337, 1080, 9834, 293, 286, 393, 536, 456, 311, 472, 40863, 295, 9834, 293, 456, 311, 527, 1045], "temperature": 0.0, "avg_logprob": -0.11826089576438621, "compression_ratio": 1.7813765182186234, "no_speech_prob": 2.295911372129922e-06}, {"id": 70, "seek": 39460, "start": 414.52000000000004, "end": 415.64000000000004, "text": " things.", "tokens": [721, 13], "temperature": 0.0, "avg_logprob": -0.11826089576438621, "compression_ratio": 1.7813765182186234, "no_speech_prob": 2.295911372129922e-06}, {"id": 71, "seek": 39460, "start": 415.64000000000004, "end": 423.32000000000005, "text": " And you'll notice that it's also automatically randomly initialized them which again is generally", "tokens": [400, 291, 603, 3449, 300, 309, 311, 611, 6772, 16979, 5883, 1602, 552, 597, 797, 307, 5101], "temperature": 0.0, "avg_logprob": -0.11826089576438621, "compression_ratio": 1.7813765182186234, "no_speech_prob": 2.295911372129922e-06}, {"id": 72, "seek": 39460, "start": 423.32000000000005, "end": 424.36, "text": " what we want.", "tokens": [437, 321, 528, 13], "temperature": 0.0, "avg_logprob": -0.11826089576438621, "compression_ratio": 1.7813765182186234, "no_speech_prob": 2.295911372129922e-06}, {"id": 73, "seek": 42436, "start": 424.36, "end": 433.40000000000003, "text": " So PyTorch does go to some effort to try to make things easy for you.", "tokens": [407, 9953, 51, 284, 339, 775, 352, 281, 512, 4630, 281, 853, 281, 652, 721, 1858, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.0975906632163308, "compression_ratio": 1.3063063063063063, "no_speech_prob": 1.6536866951355478e-06}, {"id": 74, "seek": 42436, "start": 433.40000000000003, "end": 448.3, "text": " So this attribute a is a linear layer and it's got a bunch of things in it.", "tokens": [407, 341, 19667, 257, 307, 257, 8213, 4583, 293, 309, 311, 658, 257, 3840, 295, 721, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.0975906632163308, "compression_ratio": 1.3063063063063063, "no_speech_prob": 1.6536866951355478e-06}, {"id": 75, "seek": 44830, "start": 448.3, "end": 455.36, "text": " One of the things in it is the weights and that's where you'll actually find the parameters,", "tokens": [1485, 295, 264, 721, 294, 309, 307, 264, 17443, 293, 300, 311, 689, 291, 603, 767, 915, 264, 9834, 11], "temperature": 0.0, "avg_logprob": -0.12519594551860422, "compression_ratio": 1.7041420118343196, "no_speech_prob": 3.866962288157083e-07}, {"id": 76, "seek": 44830, "start": 455.36, "end": 457.24, "text": " that is of type parameter.", "tokens": [300, 307, 295, 2010, 13075, 13], "temperature": 0.0, "avg_logprob": -0.12519594551860422, "compression_ratio": 1.7041420118343196, "no_speech_prob": 3.866962288157083e-07}, {"id": 77, "seek": 44830, "start": 457.24, "end": 462.76, "text": " So a linear layer is something that contains attributes of type parameter.", "tokens": [407, 257, 8213, 4583, 307, 746, 300, 8306, 17212, 295, 2010, 13075, 13], "temperature": 0.0, "avg_logprob": -0.12519594551860422, "compression_ratio": 1.7041420118343196, "no_speech_prob": 3.866962288157083e-07}, {"id": 78, "seek": 44830, "start": 462.76, "end": 471.12, "text": " Okay so what we want to do is we want to create something that works just like this did which", "tokens": [1033, 370, 437, 321, 528, 281, 360, 307, 321, 528, 281, 1884, 746, 300, 1985, 445, 411, 341, 630, 597], "temperature": 0.0, "avg_logprob": -0.12519594551860422, "compression_ratio": 1.7041420118343196, "no_speech_prob": 3.866962288157083e-07}, {"id": 79, "seek": 47112, "start": 471.12, "end": 482.92, "text": " is something that creates a matrix which will be trained as we train the model.", "tokens": [307, 746, 300, 7829, 257, 8141, 597, 486, 312, 8895, 382, 321, 3847, 264, 2316, 13], "temperature": 0.0, "avg_logprob": -0.10885135892411353, "compression_ratio": 1.729559748427673, "no_speech_prob": 1.0030126986748655e-06}, {"id": 80, "seek": 47112, "start": 482.92, "end": 492.68, "text": " Okay so an embedding is something which yeah it's going to create a matrix of this by this", "tokens": [1033, 370, 364, 12240, 3584, 307, 746, 597, 1338, 309, 311, 516, 281, 1884, 257, 8141, 295, 341, 538, 341], "temperature": 0.0, "avg_logprob": -0.10885135892411353, "compression_ratio": 1.729559748427673, "no_speech_prob": 1.0030126986748655e-06}, {"id": 81, "seek": 47112, "start": 492.68, "end": 498.12, "text": " and it will be a parameter and it's something that yeah we need to be able to index into", "tokens": [293, 309, 486, 312, 257, 13075, 293, 309, 311, 746, 300, 1338, 321, 643, 281, 312, 1075, 281, 8186, 666], "temperature": 0.0, "avg_logprob": -0.10885135892411353, "compression_ratio": 1.729559748427673, "no_speech_prob": 1.0030126986748655e-06}, {"id": 82, "seek": 47112, "start": 498.12, "end": 499.56, "text": " as we did here.", "tokens": [382, 321, 630, 510, 13], "temperature": 0.0, "avg_logprob": -0.10885135892411353, "compression_ratio": 1.729559748427673, "no_speech_prob": 1.0030126986748655e-06}, {"id": 83, "seek": 49956, "start": 499.56, "end": 504.72, "text": " And so yeah what is happening behind the scenes you know in PyTorch.", "tokens": [400, 370, 1338, 437, 307, 2737, 2261, 264, 8026, 291, 458, 294, 9953, 51, 284, 339, 13], "temperature": 0.0, "avg_logprob": -0.14909685920266544, "compression_ratio": 1.536231884057971, "no_speech_prob": 4.936950972478371e-06}, {"id": 84, "seek": 49956, "start": 504.72, "end": 508.84, "text": " It's nice to be able to create these things ourselves in Scratch because it means we really", "tokens": [467, 311, 1481, 281, 312, 1075, 281, 1884, 613, 721, 4175, 294, 34944, 852, 570, 309, 1355, 321, 534], "temperature": 0.0, "avg_logprob": -0.14909685920266544, "compression_ratio": 1.536231884057971, "no_speech_prob": 4.936950972478371e-06}, {"id": 85, "seek": 49956, "start": 508.84, "end": 512.12, "text": " understand it.", "tokens": [1223, 309, 13], "temperature": 0.0, "avg_logprob": -0.14909685920266544, "compression_ratio": 1.536231884057971, "no_speech_prob": 4.936950972478371e-06}, {"id": 86, "seek": 49956, "start": 512.12, "end": 518.76, "text": " And so let's create that exact same module that we did last time but this time we're", "tokens": [400, 370, 718, 311, 1884, 300, 1900, 912, 10088, 300, 321, 630, 1036, 565, 457, 341, 565, 321, 434], "temperature": 0.0, "avg_logprob": -0.14909685920266544, "compression_ratio": 1.536231884057971, "no_speech_prob": 4.936950972478371e-06}, {"id": 87, "seek": 49956, "start": 518.76, "end": 522.4, "text": " going to use a function I've created called createPyrams.", "tokens": [516, 281, 764, 257, 2445, 286, 600, 2942, 1219, 1884, 47, 88, 2356, 82, 13], "temperature": 0.0, "avg_logprob": -0.14909685920266544, "compression_ratio": 1.536231884057971, "no_speech_prob": 4.936950972478371e-06}, {"id": 88, "seek": 52240, "start": 522.4, "end": 532.72, "text": " We pass in a size so such as in this case n uses by n factors and it's going to call", "tokens": [492, 1320, 294, 257, 2744, 370, 1270, 382, 294, 341, 1389, 297, 4960, 538, 297, 6771, 293, 309, 311, 516, 281, 818], "temperature": 0.0, "avg_logprob": -0.17531127360329699, "compression_ratio": 1.5952380952380953, "no_speech_prob": 2.1907678728894098e-06}, {"id": 89, "seek": 52240, "start": 532.72, "end": 542.8, "text": " torch.zeros to create a tensor of zeros of the size that you request and then it's going", "tokens": [27822, 13, 4527, 329, 281, 1884, 257, 40863, 295, 35193, 295, 264, 2744, 300, 291, 5308, 293, 550, 309, 311, 516], "temperature": 0.0, "avg_logprob": -0.17531127360329699, "compression_ratio": 1.5952380952380953, "no_speech_prob": 2.1907678728894098e-06}, {"id": 90, "seek": 52240, "start": 542.8, "end": 551.52, "text": " to do a normal random distribution so a Gaussian distribution of mean zero, standard deviation", "tokens": [281, 360, 257, 2710, 4974, 7316, 370, 257, 39148, 7316, 295, 914, 4018, 11, 3832, 25163], "temperature": 0.0, "avg_logprob": -0.17531127360329699, "compression_ratio": 1.5952380952380953, "no_speech_prob": 2.1907678728894098e-06}, {"id": 91, "seek": 55152, "start": 551.52, "end": 558.0799999999999, "text": " 0.01 to randomly initialize those and it'll put the whole thing into an nn.parameter.", "tokens": [1958, 13, 10607, 281, 16979, 5883, 1125, 729, 293, 309, 603, 829, 264, 1379, 551, 666, 364, 297, 77, 13, 2181, 335, 2398, 13], "temperature": 0.0, "avg_logprob": -0.17296526011298685, "compression_ratio": 1.5055555555555555, "no_speech_prob": 3.905461198883131e-06}, {"id": 92, "seek": 55152, "start": 558.0799999999999, "end": 563.56, "text": " So that so this here is going to create an attribute called user factors which will be", "tokens": [407, 300, 370, 341, 510, 307, 516, 281, 1884, 364, 19667, 1219, 4195, 6771, 597, 486, 312], "temperature": 0.0, "avg_logprob": -0.17296526011298685, "compression_ratio": 1.5055555555555555, "no_speech_prob": 3.905461198883131e-06}, {"id": 93, "seek": 55152, "start": 563.56, "end": 575.76, "text": " a parameter containing some tensor of normally distributed random numbers of this size.", "tokens": [257, 13075, 19273, 512, 40863, 295, 5646, 12631, 4974, 3547, 295, 341, 2744, 13], "temperature": 0.0, "avg_logprob": -0.17296526011298685, "compression_ratio": 1.5055555555555555, "no_speech_prob": 3.905461198883131e-06}, {"id": 94, "seek": 55152, "start": 575.76, "end": 578.96, "text": " Excuse me.", "tokens": [11359, 385, 13], "temperature": 0.0, "avg_logprob": -0.17296526011298685, "compression_ratio": 1.5055555555555555, "no_speech_prob": 3.905461198883131e-06}, {"id": 95, "seek": 57896, "start": 578.96, "end": 583.08, "text": " And because it's a parameter that's going to be stored inside that's going to be available", "tokens": [400, 570, 309, 311, 257, 13075, 300, 311, 516, 281, 312, 12187, 1854, 300, 311, 516, 281, 312, 2435], "temperature": 0.0, "avg_logprob": -0.20749550239712583, "compression_ratio": 1.9184782608695652, "no_speech_prob": 2.6425771011417964e-06}, {"id": 96, "seek": 57896, "start": 583.08, "end": 586.4000000000001, "text": " as in parameters in the module.", "tokens": [382, 294, 9834, 294, 264, 10088, 13], "temperature": 0.0, "avg_logprob": -0.20749550239712583, "compression_ratio": 1.9184782608695652, "no_speech_prob": 2.6425771011417964e-06}, {"id": 97, "seek": 57896, "start": 586.4000000000001, "end": 591.74, "text": " Oh I'm sneezing.", "tokens": [876, 286, 478, 49299, 278, 13], "temperature": 0.0, "avg_logprob": -0.20749550239712583, "compression_ratio": 1.9184782608695652, "no_speech_prob": 2.6425771011417964e-06}, {"id": 98, "seek": 57896, "start": 591.74, "end": 596.5600000000001, "text": " So userBias will be a vector of parameters.", "tokens": [407, 4195, 33, 4609, 486, 312, 257, 8062, 295, 9834, 13], "temperature": 0.0, "avg_logprob": -0.20749550239712583, "compression_ratio": 1.9184782608695652, "no_speech_prob": 2.6425771011417964e-06}, {"id": 99, "seek": 57896, "start": 596.5600000000001, "end": 599.32, "text": " UserFactors will be a matrix of parameters.", "tokens": [32127, 37, 578, 830, 486, 312, 257, 8141, 295, 9834, 13], "temperature": 0.0, "avg_logprob": -0.20749550239712583, "compression_ratio": 1.9184782608695652, "no_speech_prob": 2.6425771011417964e-06}, {"id": 100, "seek": 57896, "start": 599.32, "end": 602.96, "text": " MovieFactors will be a matrix and movies by n factors.", "tokens": [28766, 37, 578, 830, 486, 312, 257, 8141, 293, 6233, 538, 297, 6771, 13], "temperature": 0.0, "avg_logprob": -0.20749550239712583, "compression_ratio": 1.9184782608695652, "no_speech_prob": 2.6425771011417964e-06}, {"id": 101, "seek": 57896, "start": 602.96, "end": 608.6800000000001, "text": " MovieBias will be a vector of n movies and this is the same as before.", "tokens": [28766, 33, 4609, 486, 312, 257, 8062, 295, 297, 6233, 293, 341, 307, 264, 912, 382, 949, 13], "temperature": 0.0, "avg_logprob": -0.20749550239712583, "compression_ratio": 1.9184782608695652, "no_speech_prob": 2.6425771011417964e-06}, {"id": 102, "seek": 60868, "start": 608.68, "end": 611.8, "text": " So now in the forward we can do exactly what we did before.", "tokens": [407, 586, 294, 264, 2128, 321, 393, 360, 2293, 437, 321, 630, 949, 13], "temperature": 0.0, "avg_logprob": -0.08519414265950521, "compression_ratio": 1.6384180790960452, "no_speech_prob": 1.9033806211155024e-06}, {"id": 103, "seek": 60868, "start": 611.8, "end": 620.3199999999999, "text": " The thing is when you put a tensor inside a parameter it has all the exact same features", "tokens": [440, 551, 307, 562, 291, 829, 257, 40863, 1854, 257, 13075, 309, 575, 439, 264, 1900, 912, 4122], "temperature": 0.0, "avg_logprob": -0.08519414265950521, "compression_ratio": 1.6384180790960452, "no_speech_prob": 1.9033806211155024e-06}, {"id": 104, "seek": 60868, "start": 620.3199999999999, "end": 622.8199999999999, "text": " that a tensor has.", "tokens": [300, 257, 40863, 575, 13], "temperature": 0.0, "avg_logprob": -0.08519414265950521, "compression_ratio": 1.6384180790960452, "no_speech_prob": 1.9033806211155024e-06}, {"id": 105, "seek": 60868, "start": 622.8199999999999, "end": 628.78, "text": " So for example we can index into it.", "tokens": [407, 337, 1365, 321, 393, 8186, 666, 309, 13], "temperature": 0.0, "avg_logprob": -0.08519414265950521, "compression_ratio": 1.6384180790960452, "no_speech_prob": 1.9033806211155024e-06}, {"id": 106, "seek": 60868, "start": 628.78, "end": 634.4, "text": " So this whole thing is identical to what we had before and so that's actually believe", "tokens": [407, 341, 1379, 551, 307, 14800, 281, 437, 321, 632, 949, 293, 370, 300, 311, 767, 1697], "temperature": 0.0, "avg_logprob": -0.08519414265950521, "compression_ratio": 1.6384180790960452, "no_speech_prob": 1.9033806211155024e-06}, {"id": 107, "seek": 63440, "start": 634.4, "end": 641.8199999999999, "text": " it or not all that's required to replicate PyTorch's embedding layer from scratch.", "tokens": [309, 420, 406, 439, 300, 311, 4739, 281, 25356, 9953, 51, 284, 339, 311, 12240, 3584, 4583, 490, 8459, 13], "temperature": 0.0, "avg_logprob": -0.14445971080235073, "compression_ratio": 1.4311377245508983, "no_speech_prob": 4.888299258709594e-07}, {"id": 108, "seek": 63440, "start": 641.8199999999999, "end": 647.48, "text": " So let's run those and see if it works.", "tokens": [407, 718, 311, 1190, 729, 293, 536, 498, 309, 1985, 13], "temperature": 0.0, "avg_logprob": -0.14445971080235073, "compression_ratio": 1.4311377245508983, "no_speech_prob": 4.888299258709594e-07}, {"id": 109, "seek": 63440, "start": 647.48, "end": 650.1, "text": " And there it is it's training.", "tokens": [400, 456, 309, 307, 309, 311, 3097, 13], "temperature": 0.0, "avg_logprob": -0.14445971080235073, "compression_ratio": 1.4311377245508983, "no_speech_prob": 4.888299258709594e-07}, {"id": 110, "seek": 63440, "start": 650.1, "end": 659.76, "text": " So we'll be able to have a look when this is done at for example model dot let's have", "tokens": [407, 321, 603, 312, 1075, 281, 362, 257, 574, 562, 341, 307, 1096, 412, 337, 1365, 2316, 5893, 718, 311, 362], "temperature": 0.0, "avg_logprob": -0.14445971080235073, "compression_ratio": 1.4311377245508983, "no_speech_prob": 4.888299258709594e-07}, {"id": 111, "seek": 65976, "start": 659.76, "end": 669.56, "text": " a look MovieBias.", "tokens": [257, 574, 28766, 33, 4609, 13], "temperature": 0.0, "avg_logprob": -0.11985841550325092, "compression_ratio": 1.321917808219178, "no_speech_prob": 1.0677011914594914e-06}, {"id": 112, "seek": 65976, "start": 669.56, "end": 670.64, "text": " And here it is right.", "tokens": [400, 510, 309, 307, 558, 13], "temperature": 0.0, "avg_logprob": -0.11985841550325092, "compression_ratio": 1.321917808219178, "no_speech_prob": 1.0677011914594914e-06}, {"id": 113, "seek": 65976, "start": 670.64, "end": 678.48, "text": " It's a parameter containing a bunch of numbers that have been trained.", "tokens": [467, 311, 257, 13075, 19273, 257, 3840, 295, 3547, 300, 362, 668, 8895, 13], "temperature": 0.0, "avg_logprob": -0.11985841550325092, "compression_ratio": 1.321917808219178, "no_speech_prob": 1.0677011914594914e-06}, {"id": 114, "seek": 65976, "start": 678.48, "end": 685.92, "text": " And as we'd expect it's got 1665 things in because that's how many movies we have.", "tokens": [400, 382, 321, 1116, 2066, 309, 311, 658, 3165, 16824, 721, 294, 570, 300, 311, 577, 867, 6233, 321, 362, 13], "temperature": 0.0, "avg_logprob": -0.11985841550325092, "compression_ratio": 1.321917808219178, "no_speech_prob": 1.0677011914594914e-06}, {"id": 115, "seek": 68592, "start": 685.92, "end": 693.4399999999999, "text": " So a question from Jonah Raphael was does torch dot zeros not produce all zeros?", "tokens": [407, 257, 1168, 490, 42353, 49690, 338, 390, 775, 27822, 5893, 35193, 406, 5258, 439, 35193, 30], "temperature": 0.0, "avg_logprob": -0.20996128429066052, "compression_ratio": 1.5034013605442176, "no_speech_prob": 9.874620445771143e-07}, {"id": 116, "seek": 68592, "start": 693.4399999999999, "end": 697.5999999999999, "text": " Yes torch dot zeros does produce all zeros.", "tokens": [1079, 27822, 5893, 35193, 775, 5258, 439, 35193, 13], "temperature": 0.0, "avg_logprob": -0.20996128429066052, "compression_ratio": 1.5034013605442176, "no_speech_prob": 9.874620445771143e-07}, {"id": 117, "seek": 68592, "start": 697.5999999999999, "end": 705.04, "text": " But remember a method that ends in underscore changes in place the tensor it's being applied", "tokens": [583, 1604, 257, 3170, 300, 5314, 294, 37556, 2962, 294, 1081, 264, 40863, 309, 311, 885, 6456], "temperature": 0.0, "avg_logprob": -0.20996128429066052, "compression_ratio": 1.5034013605442176, "no_speech_prob": 9.874620445771143e-07}, {"id": 118, "seek": 68592, "start": 705.04, "end": 706.4, "text": " to.", "tokens": [281, 13], "temperature": 0.0, "avg_logprob": -0.20996128429066052, "compression_ratio": 1.5034013605442176, "no_speech_prob": 9.874620445771143e-07}, {"id": 119, "seek": 70640, "start": 706.4, "end": 722.24, "text": " And so if you look up PyTorch normal underscore you'll see it fills itself with elements sampled", "tokens": [400, 370, 498, 291, 574, 493, 9953, 51, 284, 339, 2710, 37556, 291, 603, 536, 309, 22498, 2564, 365, 4959, 3247, 15551], "temperature": 0.0, "avg_logprob": -0.09892959594726562, "compression_ratio": 1.3383458646616542, "no_speech_prob": 7.571143214590847e-07}, {"id": 120, "seek": 70640, "start": 722.24, "end": 724.12, "text": " from the normal distribution.", "tokens": [490, 264, 2710, 7316, 13], "temperature": 0.0, "avg_logprob": -0.09892959594726562, "compression_ratio": 1.3383458646616542, "no_speech_prob": 7.571143214590847e-07}, {"id": 121, "seek": 70640, "start": 724.12, "end": 730.52, "text": " So this is actually modifying this tensor in place.", "tokens": [407, 341, 307, 767, 42626, 341, 40863, 294, 1081, 13], "temperature": 0.0, "avg_logprob": -0.09892959594726562, "compression_ratio": 1.3383458646616542, "no_speech_prob": 7.571143214590847e-07}, {"id": 122, "seek": 73052, "start": 730.52, "end": 740.96, "text": " And so that's why we end up with something which isn't just zeros.", "tokens": [400, 370, 300, 311, 983, 321, 917, 493, 365, 746, 597, 1943, 380, 445, 35193, 13], "temperature": 0.0, "avg_logprob": -0.14603942016075397, "compression_ratio": 1.403973509933775, "no_speech_prob": 1.3287744877743535e-06}, {"id": 123, "seek": 73052, "start": 740.96, "end": 752.34, "text": " Now this is the bit I find really fun is we train this model but what did it do?", "tokens": [823, 341, 307, 264, 857, 286, 915, 534, 1019, 307, 321, 3847, 341, 2316, 457, 437, 630, 309, 360, 30], "temperature": 0.0, "avg_logprob": -0.14603942016075397, "compression_ratio": 1.403973509933775, "no_speech_prob": 1.3287744877743535e-06}, {"id": 124, "seek": 73052, "start": 752.34, "end": 756.6, "text": " How is it going about predicting who's going to like what movie?", "tokens": [1012, 307, 309, 516, 466, 32884, 567, 311, 516, 281, 411, 437, 3169, 30], "temperature": 0.0, "avg_logprob": -0.14603942016075397, "compression_ratio": 1.403973509933775, "no_speech_prob": 1.3287744877743535e-06}, {"id": 125, "seek": 75660, "start": 756.6, "end": 765.2, "text": " Well one of the things that's happened is we've created this MovieBias parameter which", "tokens": [1042, 472, 295, 264, 721, 300, 311, 2011, 307, 321, 600, 2942, 341, 28766, 33, 4609, 13075, 597], "temperature": 0.0, "avg_logprob": -0.12024199335198653, "compression_ratio": 1.558974358974359, "no_speech_prob": 2.5612700937927e-06}, {"id": 126, "seek": 75660, "start": 765.2, "end": 768.6, "text": " has been optimized.", "tokens": [575, 668, 26941, 13], "temperature": 0.0, "avg_logprob": -0.12024199335198653, "compression_ratio": 1.558974358974359, "no_speech_prob": 2.5612700937927e-06}, {"id": 127, "seek": 75660, "start": 768.6, "end": 779.96, "text": " And what we could do is we could find which movie IDs have the highest numbers here and", "tokens": [400, 437, 321, 727, 360, 307, 321, 727, 915, 597, 3169, 48212, 362, 264, 6343, 3547, 510, 293], "temperature": 0.0, "avg_logprob": -0.12024199335198653, "compression_ratio": 1.558974358974359, "no_speech_prob": 2.5612700937927e-06}, {"id": 128, "seek": 75660, "start": 779.96, "end": 780.96, "text": " the lowest numbers.", "tokens": [264, 12437, 3547, 13], "temperature": 0.0, "avg_logprob": -0.12024199335198653, "compression_ratio": 1.558974358974359, "no_speech_prob": 2.5612700937927e-06}, {"id": 129, "seek": 75660, "start": 780.96, "end": 784.64, "text": " So I think this is going to start lowest and then we can print out we can look inside our", "tokens": [407, 286, 519, 341, 307, 516, 281, 722, 12437, 293, 550, 321, 393, 4482, 484, 321, 393, 574, 1854, 527], "temperature": 0.0, "avg_logprob": -0.12024199335198653, "compression_ratio": 1.558974358974359, "no_speech_prob": 2.5612700937927e-06}, {"id": 130, "seek": 78464, "start": 784.64, "end": 797.76, "text": " data loaders and grab the names of those movies for each of those five lowest numbers.", "tokens": [1412, 3677, 433, 293, 4444, 264, 5288, 295, 729, 6233, 337, 1184, 295, 729, 1732, 12437, 3547, 13], "temperature": 0.0, "avg_logprob": -0.09457009547465556, "compression_ratio": 1.5538461538461539, "no_speech_prob": 2.561268502176972e-06}, {"id": 131, "seek": 78464, "start": 797.76, "end": 800.1999999999999, "text": " And what's happened here?", "tokens": [400, 437, 311, 2011, 510, 30], "temperature": 0.0, "avg_logprob": -0.09457009547465556, "compression_ratio": 1.5538461538461539, "no_speech_prob": 2.561268502176972e-06}, {"id": 132, "seek": 78464, "start": 800.1999999999999, "end": 804.72, "text": " Well we can see broadly speaking that it has printed out some pretty crappy movies.", "tokens": [1042, 321, 393, 536, 19511, 4124, 300, 309, 575, 13567, 484, 512, 1238, 36531, 6233, 13], "temperature": 0.0, "avg_logprob": -0.09457009547465556, "compression_ratio": 1.5538461538461539, "no_speech_prob": 2.561268502176972e-06}, {"id": 133, "seek": 78464, "start": 804.72, "end": 806.12, "text": " And why is that?", "tokens": [400, 983, 307, 300, 30], "temperature": 0.0, "avg_logprob": -0.09457009547465556, "compression_ratio": 1.5538461538461539, "no_speech_prob": 2.561268502176972e-06}, {"id": 134, "seek": 78464, "start": 806.12, "end": 812.86, "text": " Well that's because when it does that matrix product that we saw in the Excel spreadsheet", "tokens": [1042, 300, 311, 570, 562, 309, 775, 300, 8141, 1674, 300, 321, 1866, 294, 264, 19060, 27733], "temperature": 0.0, "avg_logprob": -0.09457009547465556, "compression_ratio": 1.5538461538461539, "no_speech_prob": 2.561268502176972e-06}, {"id": 135, "seek": 81286, "start": 812.86, "end": 818.28, "text": " last week, it's trying to figure out who's going to like what movie based on previous", "tokens": [1036, 1243, 11, 309, 311, 1382, 281, 2573, 484, 567, 311, 516, 281, 411, 437, 3169, 2361, 322, 3894], "temperature": 0.0, "avg_logprob": -0.07335018900643407, "compression_ratio": 1.7023809523809523, "no_speech_prob": 2.7693888569046976e-06}, {"id": 136, "seek": 81286, "start": 818.28, "end": 821.8000000000001, "text": " movies people have enjoyed or not.", "tokens": [6233, 561, 362, 4626, 420, 406, 13], "temperature": 0.0, "avg_logprob": -0.07335018900643407, "compression_ratio": 1.7023809523809523, "no_speech_prob": 2.7693888569046976e-06}, {"id": 137, "seek": 81286, "start": 821.8000000000001, "end": 824.8000000000001, "text": " And then it adds MovieBias which can be positive or negative.", "tokens": [400, 550, 309, 10860, 28766, 33, 4609, 597, 393, 312, 3353, 420, 3671, 13], "temperature": 0.0, "avg_logprob": -0.07335018900643407, "compression_ratio": 1.7023809523809523, "no_speech_prob": 2.7693888569046976e-06}, {"id": 138, "seek": 81286, "start": 824.8000000000001, "end": 828.12, "text": " That's a different number for each movie.", "tokens": [663, 311, 257, 819, 1230, 337, 1184, 3169, 13], "temperature": 0.0, "avg_logprob": -0.07335018900643407, "compression_ratio": 1.7023809523809523, "no_speech_prob": 2.7693888569046976e-06}, {"id": 139, "seek": 81286, "start": 828.12, "end": 831.92, "text": " So in order to do a good job of predicting whether you're going to like a movie or not", "tokens": [407, 294, 1668, 281, 360, 257, 665, 1691, 295, 32884, 1968, 291, 434, 516, 281, 411, 257, 3169, 420, 406], "temperature": 0.0, "avg_logprob": -0.07335018900643407, "compression_ratio": 1.7023809523809523, "no_speech_prob": 2.7693888569046976e-06}, {"id": 140, "seek": 81286, "start": 831.92, "end": 834.16, "text": " it has to know which movies are crap.", "tokens": [309, 575, 281, 458, 597, 6233, 366, 12426, 13], "temperature": 0.0, "avg_logprob": -0.07335018900643407, "compression_ratio": 1.7023809523809523, "no_speech_prob": 2.7693888569046976e-06}, {"id": 141, "seek": 81286, "start": 834.16, "end": 841.9200000000001, "text": " And so the crap movies are going to end up with a very low MovieBias parameter.", "tokens": [400, 370, 264, 12426, 6233, 366, 516, 281, 917, 493, 365, 257, 588, 2295, 28766, 33, 4609, 13075, 13], "temperature": 0.0, "avg_logprob": -0.07335018900643407, "compression_ratio": 1.7023809523809523, "no_speech_prob": 2.7693888569046976e-06}, {"id": 142, "seek": 84192, "start": 841.92, "end": 849.28, "text": " And so we can actually find out which movies to people, not only which movies to people", "tokens": [400, 370, 321, 393, 767, 915, 484, 597, 6233, 281, 561, 11, 406, 787, 597, 6233, 281, 561], "temperature": 0.0, "avg_logprob": -0.09613013577151608, "compression_ratio": 1.7543859649122806, "no_speech_prob": 1.7061761354852933e-06}, {"id": 143, "seek": 84192, "start": 849.28, "end": 856.5999999999999, "text": " really not like, but which movies to people like like less than one would expect given", "tokens": [534, 406, 411, 11, 457, 597, 6233, 281, 561, 411, 411, 1570, 813, 472, 576, 2066, 2212], "temperature": 0.0, "avg_logprob": -0.09613013577151608, "compression_ratio": 1.7543859649122806, "no_speech_prob": 1.7061761354852933e-06}, {"id": 144, "seek": 84192, "start": 856.5999999999999, "end": 859.4799999999999, "text": " the kind of movie that it is.", "tokens": [264, 733, 295, 3169, 300, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.09613013577151608, "compression_ratio": 1.7543859649122806, "no_speech_prob": 1.7061761354852933e-06}, {"id": 145, "seek": 84192, "start": 859.4799999999999, "end": 867.8399999999999, "text": " So Lawnmower Man 2 for example, not only apparently is it a crappy movie, but based on the kind", "tokens": [407, 2369, 895, 76, 968, 2458, 568, 337, 1365, 11, 406, 787, 7970, 307, 309, 257, 36531, 3169, 11, 457, 2361, 322, 264, 733], "temperature": 0.0, "avg_logprob": -0.09613013577151608, "compression_ratio": 1.7543859649122806, "no_speech_prob": 1.7061761354852933e-06}, {"id": 146, "seek": 86784, "start": 867.84, "end": 875.1600000000001, "text": " of movie it is, you know it's kind of like a high-tech pop kind of sci-fi movie, people", "tokens": [295, 3169, 309, 307, 11, 291, 458, 309, 311, 733, 295, 411, 257, 1090, 12, 25970, 1665, 733, 295, 2180, 12, 13325, 3169, 11, 561], "temperature": 0.0, "avg_logprob": -0.08323406256162204, "compression_ratio": 1.6322869955156951, "no_speech_prob": 1.2289151527511422e-06}, {"id": 147, "seek": 86784, "start": 875.1600000000001, "end": 879.6800000000001, "text": " who like those kinds of movies still don't like Lawnmower Man 2.", "tokens": [567, 411, 729, 3685, 295, 6233, 920, 500, 380, 411, 2369, 895, 76, 968, 2458, 568, 13], "temperature": 0.0, "avg_logprob": -0.08323406256162204, "compression_ratio": 1.6322869955156951, "no_speech_prob": 1.2289151527511422e-06}, {"id": 148, "seek": 86784, "start": 879.6800000000001, "end": 881.24, "text": " So that's what this is meaning.", "tokens": [407, 300, 311, 437, 341, 307, 3620, 13], "temperature": 0.0, "avg_logprob": -0.08323406256162204, "compression_ratio": 1.6322869955156951, "no_speech_prob": 1.2289151527511422e-06}, {"id": 149, "seek": 86784, "start": 881.24, "end": 885.36, "text": " So it's kind of nice that we can like use a model not just to predict things but to", "tokens": [407, 309, 311, 733, 295, 1481, 300, 321, 393, 411, 764, 257, 2316, 406, 445, 281, 6069, 721, 457, 281], "temperature": 0.0, "avg_logprob": -0.08323406256162204, "compression_ratio": 1.6322869955156951, "no_speech_prob": 1.2289151527511422e-06}, {"id": 150, "seek": 86784, "start": 885.36, "end": 887.88, "text": " understand things about the data.", "tokens": [1223, 721, 466, 264, 1412, 13], "temperature": 0.0, "avg_logprob": -0.08323406256162204, "compression_ratio": 1.6322869955156951, "no_speech_prob": 1.2289151527511422e-06}, {"id": 151, "seek": 86784, "start": 887.88, "end": 893.46, "text": " So if we sort by descending it'll give us the exact opposite.", "tokens": [407, 498, 321, 1333, 538, 40182, 309, 603, 976, 505, 264, 1900, 6182, 13], "temperature": 0.0, "avg_logprob": -0.08323406256162204, "compression_ratio": 1.6322869955156951, "no_speech_prob": 1.2289151527511422e-06}, {"id": 152, "seek": 89346, "start": 893.46, "end": 899.76, "text": " So here are movies that people enjoy even when they don't normally enjoy that kind of", "tokens": [407, 510, 366, 6233, 300, 561, 2103, 754, 562, 436, 500, 380, 5646, 2103, 300, 733, 295], "temperature": 0.0, "avg_logprob": -0.1255407064733371, "compression_ratio": 1.5681818181818181, "no_speech_prob": 1.287890427192906e-06}, {"id": 153, "seek": 89346, "start": 899.76, "end": 900.76, "text": " movie.", "tokens": [3169, 13], "temperature": 0.0, "avg_logprob": -0.1255407064733371, "compression_ratio": 1.5681818181818181, "no_speech_prob": 1.287890427192906e-06}, {"id": 154, "seek": 89346, "start": 900.76, "end": 907.64, "text": " So for example LA Confidential, classic kind of film noir detective movie with the Aussie", "tokens": [407, 337, 1365, 9855, 11701, 1078, 831, 11, 7230, 733, 295, 2007, 39359, 25571, 3169, 365, 264, 21286, 414], "temperature": 0.0, "avg_logprob": -0.1255407064733371, "compression_ratio": 1.5681818181818181, "no_speech_prob": 1.287890427192906e-06}, {"id": 155, "seek": 89346, "start": 907.64, "end": 910.38, "text": " Guy Pearce.", "tokens": [14690, 45461, 384, 13], "temperature": 0.0, "avg_logprob": -0.1255407064733371, "compression_ratio": 1.5681818181818181, "no_speech_prob": 1.287890427192906e-06}, {"id": 156, "seek": 89346, "start": 910.38, "end": 918.6, "text": " Even if you don't really like film noir detective movies you might like this one.", "tokens": [2754, 498, 291, 500, 380, 534, 411, 2007, 39359, 25571, 6233, 291, 1062, 411, 341, 472, 13], "temperature": 0.0, "avg_logprob": -0.1255407064733371, "compression_ratio": 1.5681818181818181, "no_speech_prob": 1.287890427192906e-06}, {"id": 157, "seek": 91860, "start": 918.6, "end": 925.36, "text": " You know Silence of the Lambs, classic kind of I guess you'd say like horror kind of,", "tokens": [509, 458, 34570, 295, 264, 18825, 929, 11, 7230, 733, 295, 286, 2041, 291, 1116, 584, 411, 11501, 733, 295, 11], "temperature": 0.0, "avg_logprob": -0.16902202190739093, "compression_ratio": 1.6538461538461537, "no_speech_prob": 1.2878869029009365e-06}, {"id": 158, "seek": 91860, "start": 925.36, "end": 928.12, "text": " not horror is it a suspense movie.", "tokens": [406, 11501, 307, 309, 257, 47803, 3169, 13], "temperature": 0.0, "avg_logprob": -0.16902202190739093, "compression_ratio": 1.6538461538461537, "no_speech_prob": 1.2878869029009365e-06}, {"id": 159, "seek": 91860, "start": 928.12, "end": 931.52, "text": " Even people who don't normally like kind of serial killer suspense movies tend to like", "tokens": [2754, 561, 567, 500, 380, 5646, 411, 733, 295, 17436, 13364, 47803, 6233, 3928, 281, 411], "temperature": 0.0, "avg_logprob": -0.16902202190739093, "compression_ratio": 1.6538461538461537, "no_speech_prob": 1.2878869029009365e-06}, {"id": 160, "seek": 91860, "start": 931.52, "end": 936.9200000000001, "text": " this one.", "tokens": [341, 472, 13], "temperature": 0.0, "avg_logprob": -0.16902202190739093, "compression_ratio": 1.6538461538461537, "no_speech_prob": 1.2878869029009365e-06}, {"id": 161, "seek": 91860, "start": 936.9200000000001, "end": 941.36, "text": " Now the other thing we can do is not just look at what's happening in the bias.", "tokens": [823, 264, 661, 551, 321, 393, 360, 307, 406, 445, 574, 412, 437, 311, 2737, 294, 264, 12577, 13], "temperature": 0.0, "avg_logprob": -0.16902202190739093, "compression_ratio": 1.6538461538461537, "no_speech_prob": 1.2878869029009365e-06}, {"id": 162, "seek": 91860, "start": 941.36, "end": 946.12, "text": " Oh and by the way we could do the same thing with users and find out like which user just", "tokens": [876, 293, 538, 264, 636, 321, 727, 360, 264, 912, 551, 365, 5022, 293, 915, 484, 411, 597, 4195, 445], "temperature": 0.0, "avg_logprob": -0.16902202190739093, "compression_ratio": 1.6538461538461537, "no_speech_prob": 1.2878869029009365e-06}, {"id": 163, "seek": 94612, "start": 946.12, "end": 954.48, "text": " loves movies even the crappy ones you know just likes all movies and vice versa.", "tokens": [6752, 6233, 754, 264, 36531, 2306, 291, 458, 445, 5902, 439, 6233, 293, 11964, 25650, 13], "temperature": 0.0, "avg_logprob": -0.1380433440208435, "compression_ratio": 1.6916299559471366, "no_speech_prob": 3.2887257930269698e-06}, {"id": 164, "seek": 94612, "start": 954.48, "end": 960.64, "text": " But what about the other thing we didn't just have bias we also had movie factors which", "tokens": [583, 437, 466, 264, 661, 551, 321, 994, 380, 445, 362, 12577, 321, 611, 632, 3169, 6771, 597], "temperature": 0.0, "avg_logprob": -0.1380433440208435, "compression_ratio": 1.6916299559471366, "no_speech_prob": 3.2887257930269698e-06}, {"id": 165, "seek": 94612, "start": 960.64, "end": 965.96, "text": " has got the number of movies as one axis and the number of factors as the other and we", "tokens": [575, 658, 264, 1230, 295, 6233, 382, 472, 10298, 293, 264, 1230, 295, 6771, 382, 264, 661, 293, 321], "temperature": 0.0, "avg_logprob": -0.1380433440208435, "compression_ratio": 1.6916299559471366, "no_speech_prob": 3.2887257930269698e-06}, {"id": 166, "seek": 94612, "start": 965.96, "end": 968.08, "text": " passed in 50.", "tokens": [4678, 294, 2625, 13], "temperature": 0.0, "avg_logprob": -0.1380433440208435, "compression_ratio": 1.6916299559471366, "no_speech_prob": 3.2887257930269698e-06}, {"id": 167, "seek": 94612, "start": 968.08, "end": 971.52, "text": " What's in that huge matrix?", "tokens": [708, 311, 294, 300, 2603, 8141, 30], "temperature": 0.0, "avg_logprob": -0.1380433440208435, "compression_ratio": 1.6916299559471366, "no_speech_prob": 3.2887257930269698e-06}, {"id": 168, "seek": 94612, "start": 971.52, "end": 975.44, "text": " Well pretty hard to visualize such a huge matrix and we're not going to talk about the", "tokens": [1042, 1238, 1152, 281, 23273, 1270, 257, 2603, 8141, 293, 321, 434, 406, 516, 281, 751, 466, 264], "temperature": 0.0, "avg_logprob": -0.1380433440208435, "compression_ratio": 1.6916299559471366, "no_speech_prob": 3.2887257930269698e-06}, {"id": 169, "seek": 97544, "start": 975.44, "end": 980.0400000000001, "text": " details but you can do something called PCA which stands for Principle Component Analysis", "tokens": [4365, 457, 291, 393, 360, 746, 1219, 6465, 32, 597, 7382, 337, 38372, 781, 6620, 30365, 38172], "temperature": 0.0, "avg_logprob": -0.1097457572205426, "compression_ratio": 1.627027027027027, "no_speech_prob": 3.2887328416109085e-06}, {"id": 170, "seek": 97544, "start": 980.0400000000001, "end": 988.0, "text": " and that basically tries to compress those 50 columns down into three columns.", "tokens": [293, 300, 1936, 9898, 281, 14778, 729, 2625, 13766, 760, 666, 1045, 13766, 13], "temperature": 0.0, "avg_logprob": -0.1097457572205426, "compression_ratio": 1.627027027027027, "no_speech_prob": 3.2887328416109085e-06}, {"id": 171, "seek": 97544, "start": 988.0, "end": 994.1600000000001, "text": " And then we can draw a chart of the top two.", "tokens": [400, 550, 321, 393, 2642, 257, 6927, 295, 264, 1192, 732, 13], "temperature": 0.0, "avg_logprob": -0.1097457572205426, "compression_ratio": 1.627027027027027, "no_speech_prob": 3.2887328416109085e-06}, {"id": 172, "seek": 97544, "start": 994.1600000000001, "end": 1002.8800000000001, "text": " And so this is PCA component number one and this is PCA component number two and here's", "tokens": [400, 370, 341, 307, 6465, 32, 6542, 1230, 472, 293, 341, 307, 6465, 32, 6542, 1230, 732, 293, 510, 311], "temperature": 0.0, "avg_logprob": -0.1097457572205426, "compression_ratio": 1.627027027027027, "no_speech_prob": 3.2887328416109085e-06}, {"id": 173, "seek": 100288, "start": 1002.88, "end": 1011.76, "text": " a bunch of movies and this is a compressed view of these latent factors that it created.", "tokens": [257, 3840, 295, 6233, 293, 341, 307, 257, 30353, 1910, 295, 613, 48994, 6771, 300, 309, 2942, 13], "temperature": 0.0, "avg_logprob": -0.07667448908783668, "compression_ratio": 1.7241379310344827, "no_speech_prob": 4.2228134589095134e-06}, {"id": 174, "seek": 100288, "start": 1011.76, "end": 1016.12, "text": " And you can see that they obviously have some kind of meaning right.", "tokens": [400, 291, 393, 536, 300, 436, 2745, 362, 512, 733, 295, 3620, 558, 13], "temperature": 0.0, "avg_logprob": -0.07667448908783668, "compression_ratio": 1.7241379310344827, "no_speech_prob": 4.2228134589095134e-06}, {"id": 175, "seek": 100288, "start": 1016.12, "end": 1024.4, "text": " So over here towards the right we've got kind of you know very pop mainstream kind of movies", "tokens": [407, 670, 510, 3030, 264, 558, 321, 600, 658, 733, 295, 291, 458, 588, 1665, 15960, 733, 295, 6233], "temperature": 0.0, "avg_logprob": -0.07667448908783668, "compression_ratio": 1.7241379310344827, "no_speech_prob": 4.2228134589095134e-06}, {"id": 176, "seek": 100288, "start": 1024.4, "end": 1030.58, "text": " and over here on the left we've got more of the kind of critically acclaimed gritty kind", "tokens": [293, 670, 510, 322, 264, 1411, 321, 600, 658, 544, 295, 264, 733, 295, 22797, 1317, 22642, 677, 10016, 733], "temperature": 0.0, "avg_logprob": -0.07667448908783668, "compression_ratio": 1.7241379310344827, "no_speech_prob": 4.2228134589095134e-06}, {"id": 177, "seek": 100288, "start": 1030.58, "end": 1032.68, "text": " of movies.", "tokens": [295, 6233, 13], "temperature": 0.0, "avg_logprob": -0.07667448908783668, "compression_ratio": 1.7241379310344827, "no_speech_prob": 4.2228134589095134e-06}, {"id": 178, "seek": 103268, "start": 1032.68, "end": 1038.92, "text": " And then towards the top we've got very kind of action oriented and sci-fi movies and then", "tokens": [400, 550, 3030, 264, 1192, 321, 600, 658, 588, 733, 295, 3069, 21841, 293, 2180, 12, 13325, 6233, 293, 550], "temperature": 0.0, "avg_logprob": -0.09522530010768346, "compression_ratio": 1.6390243902439023, "no_speech_prob": 1.7330449963992578e-06}, {"id": 179, "seek": 103268, "start": 1038.92, "end": 1042.4, "text": " down towards the bottom we've got very dialogue driven movies.", "tokens": [760, 3030, 264, 2767, 321, 600, 658, 588, 10221, 9555, 6233, 13], "temperature": 0.0, "avg_logprob": -0.09522530010768346, "compression_ratio": 1.6390243902439023, "no_speech_prob": 1.7330449963992578e-06}, {"id": 180, "seek": 103268, "start": 1042.4, "end": 1048.96, "text": " So remember we didn't program in any of these things and we don't have any data at all about", "tokens": [407, 1604, 321, 994, 380, 1461, 294, 604, 295, 613, 721, 293, 321, 500, 380, 362, 604, 1412, 412, 439, 466], "temperature": 0.0, "avg_logprob": -0.09522530010768346, "compression_ratio": 1.6390243902439023, "no_speech_prob": 1.7330449963992578e-06}, {"id": 181, "seek": 103268, "start": 1048.96, "end": 1058.66, "text": " what movie is what kind of movie but thanks to the magic of SGD we just told it to please", "tokens": [437, 3169, 307, 437, 733, 295, 3169, 457, 3231, 281, 264, 5585, 295, 34520, 35, 321, 445, 1907, 309, 281, 1767], "temperature": 0.0, "avg_logprob": -0.09522530010768346, "compression_ratio": 1.6390243902439023, "no_speech_prob": 1.7330449963992578e-06}, {"id": 182, "seek": 105866, "start": 1058.66, "end": 1065.98, "text": " try and optimize these parameters and the way it was able to predict who would like", "tokens": [853, 293, 19719, 613, 9834, 293, 264, 636, 309, 390, 1075, 281, 6069, 567, 576, 411], "temperature": 0.0, "avg_logprob": -0.08807023905091367, "compression_ratio": 1.525974025974026, "no_speech_prob": 1.903376301015669e-06}, {"id": 183, "seek": 105866, "start": 1065.98, "end": 1071.8400000000001, "text": " what movie was it had to figure out what kinds of movies are there or what kind of taste", "tokens": [437, 3169, 390, 309, 632, 281, 2573, 484, 437, 3685, 295, 6233, 366, 456, 420, 437, 733, 295, 3939], "temperature": 0.0, "avg_logprob": -0.08807023905091367, "compression_ratio": 1.525974025974026, "no_speech_prob": 1.903376301015669e-06}, {"id": 184, "seek": 105866, "start": 1071.8400000000001, "end": 1075.7, "text": " is there for each movie.", "tokens": [307, 456, 337, 1184, 3169, 13], "temperature": 0.0, "avg_logprob": -0.08807023905091367, "compression_ratio": 1.525974025974026, "no_speech_prob": 1.903376301015669e-06}, {"id": 185, "seek": 105866, "start": 1075.7, "end": 1078.8400000000001, "text": " So I think that's pretty interesting.", "tokens": [407, 286, 519, 300, 311, 1238, 1880, 13], "temperature": 0.0, "avg_logprob": -0.08807023905091367, "compression_ratio": 1.525974025974026, "no_speech_prob": 1.903376301015669e-06}, {"id": 186, "seek": 107884, "start": 1078.84, "end": 1097.6, "text": " So this is called visualizing embeddings and then this is visualizing the bias.", "tokens": [407, 341, 307, 1219, 5056, 3319, 12240, 29432, 293, 550, 341, 307, 5056, 3319, 264, 12577, 13], "temperature": 0.0, "avg_logprob": -0.11398307085037232, "compression_ratio": 1.4736842105263157, "no_speech_prob": 1.2098563502149773e-06}, {"id": 187, "seek": 107884, "start": 1097.6, "end": 1108.4399999999998, "text": " We obviously would rather not do everything both by hand like this or even like this and", "tokens": [492, 2745, 576, 2831, 406, 360, 1203, 1293, 538, 1011, 411, 341, 420, 754, 411, 341, 293], "temperature": 0.0, "avg_logprob": -0.11398307085037232, "compression_ratio": 1.4736842105263157, "no_speech_prob": 1.2098563502149773e-06}, {"id": 188, "seek": 110844, "start": 1108.44, "end": 1115.1200000000001, "text": " fast AI provides an application for collaborative learner and so we can create one and this", "tokens": [2370, 7318, 6417, 364, 3861, 337, 16555, 33347, 293, 370, 321, 393, 1884, 472, 293, 341], "temperature": 0.0, "avg_logprob": -0.13190649685106778, "compression_ratio": 1.5473684210526315, "no_speech_prob": 1.184303891932359e-05}, {"id": 189, "seek": 110844, "start": 1115.1200000000001, "end": 1117.04, "text": " is going to look much the same as what we just had.", "tokens": [307, 516, 281, 574, 709, 264, 912, 382, 437, 321, 445, 632, 13], "temperature": 0.0, "avg_logprob": -0.13190649685106778, "compression_ratio": 1.5473684210526315, "no_speech_prob": 1.184303891932359e-05}, {"id": 190, "seek": 110844, "start": 1117.04, "end": 1121.56, "text": " We're going to say how many latent factors we want and what the y range is to do the", "tokens": [492, 434, 516, 281, 584, 577, 867, 48994, 6771, 321, 528, 293, 437, 264, 288, 3613, 307, 281, 360, 264], "temperature": 0.0, "avg_logprob": -0.13190649685106778, "compression_ratio": 1.5473684210526315, "no_speech_prob": 1.184303891932359e-05}, {"id": 191, "seek": 110844, "start": 1121.56, "end": 1135.48, "text": " sigmoid and the multiply and then we can do fit and away it goes.", "tokens": [4556, 3280, 327, 293, 264, 12972, 293, 550, 321, 393, 360, 3318, 293, 1314, 309, 1709, 13], "temperature": 0.0, "avg_logprob": -0.13190649685106778, "compression_ratio": 1.5473684210526315, "no_speech_prob": 1.184303891932359e-05}, {"id": 192, "seek": 113548, "start": 1135.48, "end": 1138.88, "text": " So let's see how it does.", "tokens": [407, 718, 311, 536, 577, 309, 775, 13], "temperature": 0.0, "avg_logprob": -0.11631750837664738, "compression_ratio": 1.7017543859649122, "no_speech_prob": 1.8738682001639972e-06}, {"id": 193, "seek": 113548, "start": 1138.88, "end": 1144.28, "text": " Alright so it's done a bit better than our manual one.", "tokens": [2798, 370, 309, 311, 1096, 257, 857, 1101, 813, 527, 9688, 472, 13], "temperature": 0.0, "avg_logprob": -0.11631750837664738, "compression_ratio": 1.7017543859649122, "no_speech_prob": 1.8738682001639972e-06}, {"id": 194, "seek": 113548, "start": 1144.28, "end": 1148.44, "text": " Let's take a look at the model it created.", "tokens": [961, 311, 747, 257, 574, 412, 264, 2316, 309, 2942, 13], "temperature": 0.0, "avg_logprob": -0.11631750837664738, "compression_ratio": 1.7017543859649122, "no_speech_prob": 1.8738682001639972e-06}, {"id": 195, "seek": 113548, "start": 1148.44, "end": 1153.2, "text": " The model looks very similar to what we created in terms of the parameters.", "tokens": [440, 2316, 1542, 588, 2531, 281, 437, 321, 2942, 294, 2115, 295, 264, 9834, 13], "temperature": 0.0, "avg_logprob": -0.11631750837664738, "compression_ratio": 1.7017543859649122, "no_speech_prob": 1.8738682001639972e-06}, {"id": 196, "seek": 113548, "start": 1153.2, "end": 1159.2, "text": " You can see these are the two embeddings and these are the two biases and we can do exactly", "tokens": [509, 393, 536, 613, 366, 264, 732, 12240, 29432, 293, 613, 366, 264, 732, 32152, 293, 321, 393, 360, 2293], "temperature": 0.0, "avg_logprob": -0.11631750837664738, "compression_ratio": 1.7017543859649122, "no_speech_prob": 1.8738682001639972e-06}, {"id": 197, "seek": 113548, "start": 1159.2, "end": 1160.2, "text": " the same thing.", "tokens": [264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.11631750837664738, "compression_ratio": 1.7017543859649122, "no_speech_prob": 1.8738682001639972e-06}, {"id": 198, "seek": 113548, "start": 1160.2, "end": 1165.16, "text": " We can look in that model and we can find the you'll see it's not called movies.", "tokens": [492, 393, 574, 294, 300, 2316, 293, 321, 393, 915, 264, 291, 603, 536, 309, 311, 406, 1219, 6233, 13], "temperature": 0.0, "avg_logprob": -0.11631750837664738, "compression_ratio": 1.7017543859649122, "no_speech_prob": 1.8738682001639972e-06}, {"id": 199, "seek": 116516, "start": 1165.16, "end": 1166.16, "text": " It's if items.", "tokens": [467, 311, 498, 4754, 13], "temperature": 0.0, "avg_logprob": -0.15349846872790107, "compression_ratio": 1.7242798353909465, "no_speech_prob": 2.7108329959446564e-05}, {"id": 200, "seek": 116516, "start": 1166.16, "end": 1167.16, "text": " It's users and items.", "tokens": [467, 311, 5022, 293, 4754, 13], "temperature": 0.0, "avg_logprob": -0.15349846872790107, "compression_ratio": 1.7242798353909465, "no_speech_prob": 2.7108329959446564e-05}, {"id": 201, "seek": 116516, "start": 1167.16, "end": 1168.16, "text": " This is the item bias.", "tokens": [639, 307, 264, 3174, 12577, 13], "temperature": 0.0, "avg_logprob": -0.15349846872790107, "compression_ratio": 1.7242798353909465, "no_speech_prob": 2.7108329959446564e-05}, {"id": 202, "seek": 116516, "start": 1168.16, "end": 1175.68, "text": " So we can look at the item bias, grab the weights, sort and we get a very similar result.", "tokens": [407, 321, 393, 574, 412, 264, 3174, 12577, 11, 4444, 264, 17443, 11, 1333, 293, 321, 483, 257, 588, 2531, 1874, 13], "temperature": 0.0, "avg_logprob": -0.15349846872790107, "compression_ratio": 1.7242798353909465, "no_speech_prob": 2.7108329959446564e-05}, {"id": 203, "seek": 116516, "start": 1175.68, "end": 1179.5600000000002, "text": " In this case it's even more confident that LA Confidential is a movie that you should", "tokens": [682, 341, 1389, 309, 311, 754, 544, 6679, 300, 9855, 11701, 1078, 831, 307, 257, 3169, 300, 291, 820], "temperature": 0.0, "avg_logprob": -0.15349846872790107, "compression_ratio": 1.7242798353909465, "no_speech_prob": 2.7108329959446564e-05}, {"id": 204, "seek": 116516, "start": 1179.5600000000002, "end": 1182.6200000000001, "text": " probably try watching even if you don't like those kind of movies.", "tokens": [1391, 853, 1976, 754, 498, 291, 500, 380, 411, 729, 733, 295, 6233, 13], "temperature": 0.0, "avg_logprob": -0.15349846872790107, "compression_ratio": 1.7242798353909465, "no_speech_prob": 2.7108329959446564e-05}, {"id": 205, "seek": 116516, "start": 1182.6200000000001, "end": 1186.68, "text": " And Titanic is right up there as well even if you don't really like romancey kind of", "tokens": [400, 42183, 307, 558, 493, 456, 382, 731, 754, 498, 291, 500, 380, 534, 411, 19064, 88, 733, 295], "temperature": 0.0, "avg_logprob": -0.15349846872790107, "compression_ratio": 1.7242798353909465, "no_speech_prob": 2.7108329959446564e-05}, {"id": 206, "seek": 116516, "start": 1186.68, "end": 1190.3000000000002, "text": " movies you might like this one.", "tokens": [6233, 291, 1062, 411, 341, 472, 13], "temperature": 0.0, "avg_logprob": -0.15349846872790107, "compression_ratio": 1.7242798353909465, "no_speech_prob": 2.7108329959446564e-05}, {"id": 207, "seek": 119030, "start": 1190.3, "end": 1197.9199999999998, "text": " Even if you don't like classic detective you might like this one.", "tokens": [2754, 498, 291, 500, 380, 411, 7230, 25571, 291, 1062, 411, 341, 472, 13], "temperature": 0.0, "avg_logprob": -0.1821437469900471, "compression_ratio": 1.4285714285714286, "no_speech_prob": 7.571136393380584e-07}, {"id": 208, "seek": 119030, "start": 1197.9199999999998, "end": 1207.32, "text": " You know we can have a look at the source code for Colab Learner and we can see that", "tokens": [509, 458, 321, 393, 362, 257, 574, 412, 264, 4009, 3089, 337, 4004, 455, 17216, 260, 293, 321, 393, 536, 300], "temperature": 0.0, "avg_logprob": -0.1821437469900471, "compression_ratio": 1.4285714285714286, "no_speech_prob": 7.571136393380584e-07}, {"id": 209, "seek": 119030, "start": 1207.32, "end": 1214.6, "text": " let's see useNN is false by default.", "tokens": [718, 311, 536, 764, 45, 45, 307, 7908, 538, 7576, 13], "temperature": 0.0, "avg_logprob": -0.1821437469900471, "compression_ratio": 1.4285714285714286, "no_speech_prob": 7.571136393380584e-07}, {"id": 210, "seek": 119030, "start": 1214.6, "end": 1219.56, "text": " So where our model is going to be of this type embedding.bias.", "tokens": [407, 689, 527, 2316, 307, 516, 281, 312, 295, 341, 2010, 12240, 3584, 13, 65, 4609, 13], "temperature": 0.0, "avg_logprob": -0.1821437469900471, "compression_ratio": 1.4285714285714286, "no_speech_prob": 7.571136393380584e-07}, {"id": 211, "seek": 121956, "start": 1219.56, "end": 1224.24, "text": " So we can take a look at that.", "tokens": [407, 321, 393, 747, 257, 574, 412, 300, 13], "temperature": 0.0, "avg_logprob": -0.1395027254834587, "compression_ratio": 1.664804469273743, "no_speech_prob": 1.2482694273785455e-06}, {"id": 212, "seek": 121956, "start": 1224.24, "end": 1225.24, "text": " Here it is.", "tokens": [1692, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.1395027254834587, "compression_ratio": 1.664804469273743, "no_speech_prob": 1.2482694273785455e-06}, {"id": 213, "seek": 121956, "start": 1225.24, "end": 1227.0, "text": " And look this does look very similar.", "tokens": [400, 574, 341, 775, 574, 588, 2531, 13], "temperature": 0.0, "avg_logprob": -0.1395027254834587, "compression_ratio": 1.664804469273743, "no_speech_prob": 1.2482694273785455e-06}, {"id": 214, "seek": 121956, "start": 1227.0, "end": 1237.8799999999999, "text": " Okay it's creating an embedding using the size we requested for each of users by factors", "tokens": [1033, 309, 311, 4084, 364, 12240, 3584, 1228, 264, 2744, 321, 16436, 337, 1184, 295, 5022, 538, 6771], "temperature": 0.0, "avg_logprob": -0.1395027254834587, "compression_ratio": 1.664804469273743, "no_speech_prob": 1.2482694273785455e-06}, {"id": 215, "seek": 121956, "start": 1237.8799999999999, "end": 1242.24, "text": " and items by factors and users and items.", "tokens": [293, 4754, 538, 6771, 293, 5022, 293, 4754, 13], "temperature": 0.0, "avg_logprob": -0.1395027254834587, "compression_ratio": 1.664804469273743, "no_speech_prob": 1.2482694273785455e-06}, {"id": 216, "seek": 121956, "start": 1242.24, "end": 1247.2, "text": " And then it's grabbing each thing from the embedding in the forward and it's doing the", "tokens": [400, 550, 309, 311, 23771, 1184, 551, 490, 264, 12240, 3584, 294, 264, 2128, 293, 309, 311, 884, 264], "temperature": 0.0, "avg_logprob": -0.1395027254834587, "compression_ratio": 1.664804469273743, "no_speech_prob": 1.2482694273785455e-06}, {"id": 217, "seek": 124720, "start": 1247.2, "end": 1255.0800000000002, "text": " model play and it's adding it up and it's doing the sigmoid.", "tokens": [2316, 862, 293, 309, 311, 5127, 309, 493, 293, 309, 311, 884, 264, 4556, 3280, 327, 13], "temperature": 0.0, "avg_logprob": -0.19165635108947754, "compression_ratio": 1.4963503649635037, "no_speech_prob": 2.857303798009525e-06}, {"id": 218, "seek": 124720, "start": 1255.0800000000002, "end": 1259.24, "text": " So yeah it looks exactly the same.", "tokens": [407, 1338, 309, 1542, 2293, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.19165635108947754, "compression_ratio": 1.4963503649635037, "no_speech_prob": 2.857303798009525e-06}, {"id": 219, "seek": 124720, "start": 1259.24, "end": 1261.46, "text": " Isn't that neat?", "tokens": [6998, 380, 300, 10654, 30], "temperature": 0.0, "avg_logprob": -0.19165635108947754, "compression_ratio": 1.4963503649635037, "no_speech_prob": 2.857303798009525e-06}, {"id": 220, "seek": 124720, "start": 1261.46, "end": 1271.4, "text": " So you can see that what's actually happening in real models is not yeah it's not that weird", "tokens": [407, 291, 393, 536, 300, 437, 311, 767, 2737, 294, 957, 5245, 307, 406, 1338, 309, 311, 406, 300, 3657], "temperature": 0.0, "avg_logprob": -0.19165635108947754, "compression_ratio": 1.4963503649635037, "no_speech_prob": 2.857303798009525e-06}, {"id": 221, "seek": 127140, "start": 1271.4, "end": 1277.88, "text": " or magic.", "tokens": [420, 5585, 13], "temperature": 0.0, "avg_logprob": -0.16457019028840242, "compression_ratio": 1.3108108108108107, "no_speech_prob": 9.132524496635597e-07}, {"id": 222, "seek": 127140, "start": 1277.88, "end": 1282.96, "text": " So Kurian is asking is PCA useful in any other areas?", "tokens": [407, 16481, 952, 307, 3365, 307, 6465, 32, 4420, 294, 604, 661, 3179, 30], "temperature": 0.0, "avg_logprob": -0.16457019028840242, "compression_ratio": 1.3108108108108107, "no_speech_prob": 9.132524496635597e-07}, {"id": 223, "seek": 127140, "start": 1282.96, "end": 1286.16, "text": " And the answer is absolutely.", "tokens": [400, 264, 1867, 307, 3122, 13], "temperature": 0.0, "avg_logprob": -0.16457019028840242, "compression_ratio": 1.3108108108108107, "no_speech_prob": 9.132524496635597e-07}, {"id": 224, "seek": 127140, "start": 1286.16, "end": 1295.52, "text": " And what I suggest you do if you're interested is check out our computational linear algebra", "tokens": [400, 437, 286, 3402, 291, 360, 498, 291, 434, 3102, 307, 1520, 484, 527, 28270, 8213, 21989], "temperature": 0.0, "avg_logprob": -0.16457019028840242, "compression_ratio": 1.3108108108108107, "no_speech_prob": 9.132524496635597e-07}, {"id": 225, "seek": 127140, "start": 1295.52, "end": 1300.1200000000001, "text": " course.", "tokens": [1164, 13], "temperature": 0.0, "avg_logprob": -0.16457019028840242, "compression_ratio": 1.3108108108108107, "no_speech_prob": 9.132524496635597e-07}, {"id": 226, "seek": 130012, "start": 1300.12, "end": 1307.36, "text": " It's five years old now but I mean this is stuff which hasn't changed for decades really.", "tokens": [467, 311, 1732, 924, 1331, 586, 457, 286, 914, 341, 307, 1507, 597, 6132, 380, 3105, 337, 7878, 534, 13], "temperature": 0.0, "avg_logprob": -0.11709487869078855, "compression_ratio": 1.6046511627906976, "no_speech_prob": 9.22298295336077e-06}, {"id": 227, "seek": 130012, "start": 1307.36, "end": 1314.0, "text": " And this will teach you all about things like PCA and stuff like that.", "tokens": [400, 341, 486, 2924, 291, 439, 466, 721, 411, 6465, 32, 293, 1507, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.11709487869078855, "compression_ratio": 1.6046511627906976, "no_speech_prob": 9.22298295336077e-06}, {"id": 228, "seek": 130012, "start": 1314.0, "end": 1320.1599999999999, "text": " It's not nearly as directly practical as practical deep learning for coders but it's definitely", "tokens": [467, 311, 406, 6217, 382, 3838, 8496, 382, 8496, 2452, 2539, 337, 17656, 433, 457, 309, 311, 2138], "temperature": 0.0, "avg_logprob": -0.11709487869078855, "compression_ratio": 1.6046511627906976, "no_speech_prob": 9.22298295336077e-06}, {"id": 229, "seek": 130012, "start": 1320.1599999999999, "end": 1326.3999999999999, "text": " like very interesting and it's the kind of thing which if you want to go deeper you know", "tokens": [411, 588, 1880, 293, 309, 311, 264, 733, 295, 551, 597, 498, 291, 528, 281, 352, 7731, 291, 458], "temperature": 0.0, "avg_logprob": -0.11709487869078855, "compression_ratio": 1.6046511627906976, "no_speech_prob": 9.22298295336077e-06}, {"id": 230, "seek": 132640, "start": 1326.4, "end": 1334.16, "text": " it can become pretty useful later along your path.", "tokens": [309, 393, 1813, 1238, 4420, 1780, 2051, 428, 3100, 13], "temperature": 0.0, "avg_logprob": -0.14027228466300076, "compression_ratio": 1.5742574257425743, "no_speech_prob": 1.4823516494288924e-06}, {"id": 231, "seek": 132640, "start": 1334.16, "end": 1338.92, "text": " Okay so here's something else interesting we can do.", "tokens": [1033, 370, 510, 311, 746, 1646, 1880, 321, 393, 360, 13], "temperature": 0.0, "avg_logprob": -0.14027228466300076, "compression_ratio": 1.5742574257425743, "no_speech_prob": 1.4823516494288924e-06}, {"id": 232, "seek": 132640, "start": 1338.92, "end": 1341.3000000000002, "text": " Let's grab the movie factors.", "tokens": [961, 311, 4444, 264, 3169, 6771, 13], "temperature": 0.0, "avg_logprob": -0.14027228466300076, "compression_ratio": 1.5742574257425743, "no_speech_prob": 1.4823516494288924e-06}, {"id": 233, "seek": 132640, "start": 1341.3000000000002, "end": 1345.3200000000002, "text": " So that's in our model it's the item weights and it's the weight attribute that PyTorch", "tokens": [407, 300, 311, 294, 527, 2316, 309, 311, 264, 3174, 17443, 293, 309, 311, 264, 3364, 19667, 300, 9953, 51, 284, 339], "temperature": 0.0, "avg_logprob": -0.14027228466300076, "compression_ratio": 1.5742574257425743, "no_speech_prob": 1.4823516494288924e-06}, {"id": 234, "seek": 132640, "start": 1345.3200000000002, "end": 1347.16, "text": " creates.", "tokens": [7829, 13], "temperature": 0.0, "avg_logprob": -0.14027228466300076, "compression_ratio": 1.5742574257425743, "no_speech_prob": 1.4823516494288924e-06}, {"id": 235, "seek": 132640, "start": 1347.16, "end": 1356.0800000000002, "text": " Okay and now we can convert the movie Silence of the Lambs into its class ID and we can", "tokens": [1033, 293, 586, 321, 393, 7620, 264, 3169, 34570, 295, 264, 18825, 929, 666, 1080, 1508, 7348, 293, 321, 393], "temperature": 0.0, "avg_logprob": -0.14027228466300076, "compression_ratio": 1.5742574257425743, "no_speech_prob": 1.4823516494288924e-06}, {"id": 236, "seek": 135608, "start": 1356.08, "end": 1361.36, "text": " do that with object to ID, O to I for the titles.", "tokens": [360, 300, 365, 2657, 281, 7348, 11, 422, 281, 286, 337, 264, 12992, 13], "temperature": 0.0, "avg_logprob": -0.10372018814086914, "compression_ratio": 1.5480225988700564, "no_speech_prob": 1.9947217424487462e-06}, {"id": 237, "seek": 135608, "start": 1361.36, "end": 1364.4399999999998, "text": " And so that's the movie index of Silence of the Lambs.", "tokens": [400, 370, 300, 311, 264, 3169, 8186, 295, 34570, 295, 264, 18825, 929, 13], "temperature": 0.0, "avg_logprob": -0.10372018814086914, "compression_ratio": 1.5480225988700564, "no_speech_prob": 1.9947217424487462e-06}, {"id": 238, "seek": 135608, "start": 1364.4399999999998, "end": 1371.4399999999998, "text": " And what we can do now is we can look through all of the movies in our latent factors and", "tokens": [400, 437, 321, 393, 360, 586, 307, 321, 393, 574, 807, 439, 295, 264, 6233, 294, 527, 48994, 6771, 293], "temperature": 0.0, "avg_logprob": -0.10372018814086914, "compression_ratio": 1.5480225988700564, "no_speech_prob": 1.9947217424487462e-06}, {"id": 239, "seek": 135608, "start": 1371.4399999999998, "end": 1381.56, "text": " calculate how far apart each vector is, each embedding vector is from this one.", "tokens": [8873, 577, 1400, 4936, 1184, 8062, 307, 11, 1184, 12240, 3584, 8062, 307, 490, 341, 472, 13], "temperature": 0.0, "avg_logprob": -0.10372018814086914, "compression_ratio": 1.5480225988700564, "no_speech_prob": 1.9947217424487462e-06}, {"id": 240, "seek": 138156, "start": 1381.56, "end": 1386.96, "text": " And this cosine similarity is very similar to basically the Euclidean distance, you know", "tokens": [400, 341, 23565, 32194, 307, 588, 2531, 281, 1936, 264, 462, 1311, 31264, 282, 4560, 11, 291, 458], "temperature": 0.0, "avg_logprob": -0.16145879250985604, "compression_ratio": 1.618811881188119, "no_speech_prob": 1.482351422055217e-06}, {"id": 241, "seek": 138156, "start": 1386.96, "end": 1395.32, "text": " the kind of the root sum squared of the differences, but it normalizes it.", "tokens": [264, 733, 295, 264, 5593, 2408, 8889, 295, 264, 7300, 11, 457, 309, 2710, 5660, 309, 13], "temperature": 0.0, "avg_logprob": -0.16145879250985604, "compression_ratio": 1.618811881188119, "no_speech_prob": 1.482351422055217e-06}, {"id": 242, "seek": 138156, "start": 1395.32, "end": 1399.0, "text": " So it's basically the angle between the vectors.", "tokens": [407, 309, 311, 1936, 264, 5802, 1296, 264, 18875, 13], "temperature": 0.0, "avg_logprob": -0.16145879250985604, "compression_ratio": 1.618811881188119, "no_speech_prob": 1.482351422055217e-06}, {"id": 243, "seek": 138156, "start": 1399.0, "end": 1405.3999999999999, "text": " So this is going to calculate how similar each movie is to the Silence of the Lambs", "tokens": [407, 341, 307, 516, 281, 8873, 577, 2531, 1184, 3169, 307, 281, 264, 34570, 295, 264, 18825, 929], "temperature": 0.0, "avg_logprob": -0.16145879250985604, "compression_ratio": 1.618811881188119, "no_speech_prob": 1.482351422055217e-06}, {"id": 244, "seek": 138156, "start": 1405.3999999999999, "end": 1408.12, "text": " based on these latent factors.", "tokens": [2361, 322, 613, 48994, 6771, 13], "temperature": 0.0, "avg_logprob": -0.16145879250985604, "compression_ratio": 1.618811881188119, "no_speech_prob": 1.482351422055217e-06}, {"id": 245, "seek": 140812, "start": 1408.12, "end": 1415.1999999999998, "text": " And so then we can find which ID is the closest.", "tokens": [400, 370, 550, 321, 393, 915, 597, 7348, 307, 264, 13699, 13], "temperature": 0.0, "avg_logprob": -0.21556568145751953, "compression_ratio": 1.3103448275862069, "no_speech_prob": 6.276701469687396e-07}, {"id": 246, "seek": 140812, "start": 1415.1999999999998, "end": 1424.8, "text": " Yeah, so based on this embedding distance the closest is dial m for murder, which makes", "tokens": [865, 11, 370, 2361, 322, 341, 12240, 3584, 4560, 264, 13699, 307, 5502, 275, 337, 6568, 11, 597, 1669], "temperature": 0.0, "avg_logprob": -0.21556568145751953, "compression_ratio": 1.3103448275862069, "no_speech_prob": 6.276701469687396e-07}, {"id": 247, "seek": 140812, "start": 1424.8, "end": 1436.36, "text": " a lot of sense.", "tokens": [257, 688, 295, 2020, 13], "temperature": 0.0, "avg_logprob": -0.21556568145751953, "compression_ratio": 1.3103448275862069, "no_speech_prob": 6.276701469687396e-07}, {"id": 248, "seek": 143636, "start": 1436.36, "end": 1441.36, "text": " I'm not going to discuss it today, but in the book there's also some discussion about", "tokens": [286, 478, 406, 516, 281, 2248, 309, 965, 11, 457, 294, 264, 1446, 456, 311, 611, 512, 5017, 466], "temperature": 0.0, "avg_logprob": -0.11708257137200771, "compression_ratio": 1.6172248803827751, "no_speech_prob": 1.300668009207584e-05}, {"id": 249, "seek": 143636, "start": 1441.36, "end": 1445.84, "text": " what's called the bootstrapping problem, which is the question of like if you've got a new", "tokens": [437, 311, 1219, 264, 11450, 19639, 3759, 1154, 11, 597, 307, 264, 1168, 295, 411, 498, 291, 600, 658, 257, 777], "temperature": 0.0, "avg_logprob": -0.11708257137200771, "compression_ratio": 1.6172248803827751, "no_speech_prob": 1.300668009207584e-05}, {"id": 250, "seek": 143636, "start": 1445.84, "end": 1453.12, "text": " company or a new product how would you get started with making recommendations given", "tokens": [2237, 420, 257, 777, 1674, 577, 576, 291, 483, 1409, 365, 1455, 10434, 2212], "temperature": 0.0, "avg_logprob": -0.11708257137200771, "compression_ratio": 1.6172248803827751, "no_speech_prob": 1.300668009207584e-05}, {"id": 251, "seek": 143636, "start": 1453.12, "end": 1456.3999999999999, "text": " that you don't have any previous history with which to make recommendations.", "tokens": [300, 291, 500, 380, 362, 604, 3894, 2503, 365, 597, 281, 652, 10434, 13], "temperature": 0.0, "avg_logprob": -0.11708257137200771, "compression_ratio": 1.6172248803827751, "no_speech_prob": 1.300668009207584e-05}, {"id": 252, "seek": 145640, "start": 1456.4, "end": 1466.64, "text": " And that's a very interesting problem that you can read about in the book.", "tokens": [400, 300, 311, 257, 588, 1880, 1154, 300, 291, 393, 1401, 466, 294, 264, 1446, 13], "temperature": 0.0, "avg_logprob": -0.1209335785645705, "compression_ratio": 1.4193548387096775, "no_speech_prob": 2.482462832631427e-06}, {"id": 253, "seek": 145640, "start": 1466.64, "end": 1478.44, "text": " Now that's one way to do collaborative filtering, which is where we create that, do that matrix", "tokens": [823, 300, 311, 472, 636, 281, 360, 16555, 30822, 11, 597, 307, 689, 321, 1884, 300, 11, 360, 300, 8141], "temperature": 0.0, "avg_logprob": -0.1209335785645705, "compression_ratio": 1.4193548387096775, "no_speech_prob": 2.482462832631427e-06}, {"id": 254, "seek": 145640, "start": 1478.44, "end": 1482.0, "text": " completion exercise using all those dot products.", "tokens": [19372, 5380, 1228, 439, 729, 5893, 3383, 13], "temperature": 0.0, "avg_logprob": -0.1209335785645705, "compression_ratio": 1.4193548387096775, "no_speech_prob": 2.482462832631427e-06}, {"id": 255, "seek": 148200, "start": 1482.0, "end": 1487.16, "text": " There's a different way, however, which is we can use deep learning.", "tokens": [821, 311, 257, 819, 636, 11, 4461, 11, 597, 307, 321, 393, 764, 2452, 2539, 13], "temperature": 0.0, "avg_logprob": -0.11055860763941056, "compression_ratio": 1.6737967914438503, "no_speech_prob": 7.889171683927998e-06}, {"id": 256, "seek": 148200, "start": 1487.16, "end": 1497.56, "text": " And to do it with deep learning what we could do is we could basically create our user and", "tokens": [400, 281, 360, 309, 365, 2452, 2539, 437, 321, 727, 360, 307, 321, 727, 1936, 1884, 527, 4195, 293], "temperature": 0.0, "avg_logprob": -0.11055860763941056, "compression_ratio": 1.6737967914438503, "no_speech_prob": 7.889171683927998e-06}, {"id": 257, "seek": 148200, "start": 1497.56, "end": 1501.02, "text": " item embeddings as per usual.", "tokens": [3174, 12240, 29432, 382, 680, 7713, 13], "temperature": 0.0, "avg_logprob": -0.11055860763941056, "compression_ratio": 1.6737967914438503, "no_speech_prob": 7.889171683927998e-06}, {"id": 258, "seek": 148200, "start": 1501.02, "end": 1502.8, "text": " And then we could create a sequential model.", "tokens": [400, 550, 321, 727, 1884, 257, 42881, 2316, 13], "temperature": 0.0, "avg_logprob": -0.11055860763941056, "compression_ratio": 1.6737967914438503, "no_speech_prob": 7.889171683927998e-06}, {"id": 259, "seek": 148200, "start": 1502.8, "end": 1510.54, "text": " So sequential model is just layers of a deep learning neural network in order.", "tokens": [407, 42881, 2316, 307, 445, 7914, 295, 257, 2452, 2539, 18161, 3209, 294, 1668, 13], "temperature": 0.0, "avg_logprob": -0.11055860763941056, "compression_ratio": 1.6737967914438503, "no_speech_prob": 7.889171683927998e-06}, {"id": 260, "seek": 151054, "start": 1510.54, "end": 1517.52, "text": " And what we could do is we could just concatenate, so in forward we could just concatenate the", "tokens": [400, 437, 321, 727, 360, 307, 321, 727, 445, 1588, 7186, 473, 11, 370, 294, 2128, 321, 727, 445, 1588, 7186, 473, 264], "temperature": 0.0, "avg_logprob": -0.12680724426940249, "compression_ratio": 1.7684729064039408, "no_speech_prob": 8.71431211635354e-07}, {"id": 261, "seek": 151054, "start": 1517.52, "end": 1524.1399999999999, "text": " user and item embeddings together and then do a value.", "tokens": [4195, 293, 3174, 12240, 29432, 1214, 293, 550, 360, 257, 2158, 13], "temperature": 0.0, "avg_logprob": -0.12680724426940249, "compression_ratio": 1.7684729064039408, "no_speech_prob": 8.71431211635354e-07}, {"id": 262, "seek": 151054, "start": 1524.1399999999999, "end": 1529.2, "text": " So this is basically a single hidden layer neural network and then a linear layer at", "tokens": [407, 341, 307, 1936, 257, 2167, 7633, 4583, 18161, 3209, 293, 550, 257, 8213, 4583, 412], "temperature": 0.0, "avg_logprob": -0.12680724426940249, "compression_ratio": 1.7684729064039408, "no_speech_prob": 8.71431211635354e-07}, {"id": 263, "seek": 151054, "start": 1529.2, "end": 1531.68, "text": " the end to create a single output.", "tokens": [264, 917, 281, 1884, 257, 2167, 5598, 13], "temperature": 0.0, "avg_logprob": -0.12680724426940249, "compression_ratio": 1.7684729064039408, "no_speech_prob": 8.71431211635354e-07}, {"id": 264, "seek": 151054, "start": 1531.68, "end": 1538.76, "text": " So this is the very world's most simple neural net, exactly the same as the style that we", "tokens": [407, 341, 307, 264, 588, 1002, 311, 881, 2199, 18161, 2533, 11, 2293, 264, 912, 382, 264, 3758, 300, 321], "temperature": 0.0, "avg_logprob": -0.12680724426940249, "compression_ratio": 1.7684729064039408, "no_speech_prob": 8.71431211635354e-07}, {"id": 265, "seek": 153876, "start": 1538.76, "end": 1545.68, "text": " created back here in our neural net from scratch.", "tokens": [2942, 646, 510, 294, 527, 18161, 2533, 490, 8459, 13], "temperature": 0.0, "avg_logprob": -0.12483632162715612, "compression_ratio": 1.6, "no_speech_prob": 1.4593734931622748e-06}, {"id": 266, "seek": 153876, "start": 1545.68, "end": 1549.2, "text": " This is exactly the same.", "tokens": [639, 307, 2293, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.12483632162715612, "compression_ratio": 1.6, "no_speech_prob": 1.4593734931622748e-06}, {"id": 267, "seek": 153876, "start": 1549.2, "end": 1555.68, "text": " But we're using PyTorch's functionality to do it more easily.", "tokens": [583, 321, 434, 1228, 9953, 51, 284, 339, 311, 14980, 281, 360, 309, 544, 3612, 13], "temperature": 0.0, "avg_logprob": -0.12483632162715612, "compression_ratio": 1.6, "no_speech_prob": 1.4593734931622748e-06}, {"id": 268, "seek": 153876, "start": 1555.68, "end": 1560.16, "text": " So in the forward here we're going to, in exactly the same ways we have before, we'll", "tokens": [407, 294, 264, 2128, 510, 321, 434, 516, 281, 11, 294, 2293, 264, 912, 2098, 321, 362, 949, 11, 321, 603], "temperature": 0.0, "avg_logprob": -0.12483632162715612, "compression_ratio": 1.6, "no_speech_prob": 1.4593734931622748e-06}, {"id": 269, "seek": 153876, "start": 1560.16, "end": 1565.24, "text": " look up the user embeddings and we'll look up the item embeddings.", "tokens": [574, 493, 264, 4195, 12240, 29432, 293, 321, 603, 574, 493, 264, 3174, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.12483632162715612, "compression_ratio": 1.6, "no_speech_prob": 1.4593734931622748e-06}, {"id": 270, "seek": 153876, "start": 1565.24, "end": 1566.24, "text": " And then this is new.", "tokens": [400, 550, 341, 307, 777, 13], "temperature": 0.0, "avg_logprob": -0.12483632162715612, "compression_ratio": 1.6, "no_speech_prob": 1.4593734931622748e-06}, {"id": 271, "seek": 156624, "start": 1566.24, "end": 1570.96, "text": " This is where we concatenate those two things together and put it through our neural network", "tokens": [639, 307, 689, 321, 1588, 7186, 473, 729, 732, 721, 1214, 293, 829, 309, 807, 527, 18161, 3209], "temperature": 0.0, "avg_logprob": -0.10732954810647403, "compression_ratio": 1.560747663551402, "no_speech_prob": 1.5056974689287017e-06}, {"id": 272, "seek": 156624, "start": 1570.96, "end": 1575.96, "text": " and then finally do our sigmoid.", "tokens": [293, 550, 2721, 360, 527, 4556, 3280, 327, 13], "temperature": 0.0, "avg_logprob": -0.10732954810647403, "compression_ratio": 1.560747663551402, "no_speech_prob": 1.5056974689287017e-06}, {"id": 273, "seek": 156624, "start": 1575.96, "end": 1584.96, "text": " Now one thing different this time is that we're going to ask FastAI to figure out how", "tokens": [823, 472, 551, 819, 341, 565, 307, 300, 321, 434, 516, 281, 1029, 15968, 48698, 281, 2573, 484, 577], "temperature": 0.0, "avg_logprob": -0.10732954810647403, "compression_ratio": 1.560747663551402, "no_speech_prob": 1.5056974689287017e-06}, {"id": 274, "seek": 156624, "start": 1584.96, "end": 1586.84, "text": " big our embeddings should be.", "tokens": [955, 527, 12240, 29432, 820, 312, 13], "temperature": 0.0, "avg_logprob": -0.10732954810647403, "compression_ratio": 1.560747663551402, "no_speech_prob": 1.5056974689287017e-06}, {"id": 275, "seek": 156624, "start": 1586.84, "end": 1591.24, "text": " And so FastAI has something called get embedding sizes and it just uses a rule of thumb that", "tokens": [400, 370, 15968, 48698, 575, 746, 1219, 483, 12240, 3584, 11602, 293, 309, 445, 4960, 257, 4978, 295, 9298, 300], "temperature": 0.0, "avg_logprob": -0.10732954810647403, "compression_ratio": 1.560747663551402, "no_speech_prob": 1.5056974689287017e-06}, {"id": 276, "seek": 159124, "start": 1591.24, "end": 1601.48, "text": " says that for 944 users we recommend 74 factor embeddings and for 1,665 movies, or is it", "tokens": [1619, 300, 337, 1722, 13912, 5022, 321, 2748, 28868, 5952, 12240, 29432, 293, 337, 502, 11, 15237, 20, 6233, 11, 420, 307, 309], "temperature": 0.0, "avg_logprob": -0.12408077867725227, "compression_ratio": 1.5396825396825398, "no_speech_prob": 1.5779575051055872e-06}, {"id": 277, "seek": 159124, "start": 1601.48, "end": 1607.68, "text": " the other way around I can't remember, we recommend 102 factors for your embeddings.", "tokens": [264, 661, 636, 926, 286, 393, 380, 1604, 11, 321, 2748, 45937, 6771, 337, 428, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.12408077867725227, "compression_ratio": 1.5396825396825398, "no_speech_prob": 1.5779575051055872e-06}, {"id": 278, "seek": 159124, "start": 1607.68, "end": 1611.88, "text": " So that's what those sizes are.", "tokens": [407, 300, 311, 437, 729, 11602, 366, 13], "temperature": 0.0, "avg_logprob": -0.12408077867725227, "compression_ratio": 1.5396825396825398, "no_speech_prob": 1.5779575051055872e-06}, {"id": 279, "seek": 159124, "start": 1611.88, "end": 1619.24, "text": " So now we can create that model and we can pop it into a learner and fit in the usual", "tokens": [407, 586, 321, 393, 1884, 300, 2316, 293, 321, 393, 1665, 309, 666, 257, 33347, 293, 3318, 294, 264, 7713], "temperature": 0.0, "avg_logprob": -0.12408077867725227, "compression_ratio": 1.5396825396825398, "no_speech_prob": 1.5779575051055872e-06}, {"id": 280, "seek": 161924, "start": 1619.24, "end": 1626.84, "text": " way.", "tokens": [636, 13], "temperature": 0.0, "avg_logprob": -0.13996009826660155, "compression_ratio": 1.6281407035175879, "no_speech_prob": 4.425471615832066e-06}, {"id": 281, "seek": 161924, "start": 1626.84, "end": 1632.24, "text": " And so rather than doing all that from scratch what you can do is you can do exactly the", "tokens": [400, 370, 2831, 813, 884, 439, 300, 490, 8459, 437, 291, 393, 360, 307, 291, 393, 360, 2293, 264], "temperature": 0.0, "avg_logprob": -0.13996009826660155, "compression_ratio": 1.6281407035175879, "no_speech_prob": 4.425471615832066e-06}, {"id": 282, "seek": 161924, "start": 1632.24, "end": 1639.0, "text": " same thing that we've done before which is to call collaborative learner but you can", "tokens": [912, 551, 300, 321, 600, 1096, 949, 597, 307, 281, 818, 16555, 33347, 457, 291, 393], "temperature": 0.0, "avg_logprob": -0.13996009826660155, "compression_ratio": 1.6281407035175879, "no_speech_prob": 4.425471615832066e-06}, {"id": 283, "seek": 161924, "start": 1639.0, "end": 1644.2, "text": " pass in the parameter use neural network equals true.", "tokens": [1320, 294, 264, 13075, 764, 18161, 3209, 6915, 2074, 13], "temperature": 0.0, "avg_logprob": -0.13996009826660155, "compression_ratio": 1.6281407035175879, "no_speech_prob": 4.425471615832066e-06}, {"id": 284, "seek": 161924, "start": 1644.2, "end": 1647.64, "text": " And you can then say how big do you want each layer so this is going to create a two hidden", "tokens": [400, 291, 393, 550, 584, 577, 955, 360, 291, 528, 1184, 4583, 370, 341, 307, 516, 281, 1884, 257, 732, 7633], "temperature": 0.0, "avg_logprob": -0.13996009826660155, "compression_ratio": 1.6281407035175879, "no_speech_prob": 4.425471615832066e-06}, {"id": 285, "seek": 164764, "start": 1647.64, "end": 1649.8000000000002, "text": " layer deep learning neural net.", "tokens": [4583, 2452, 2539, 18161, 2533, 13], "temperature": 0.0, "avg_logprob": -0.2057616074879964, "compression_ratio": 1.4193548387096775, "no_speech_prob": 2.601607320684707e-06}, {"id": 286, "seek": 164764, "start": 1649.8000000000002, "end": 1657.5600000000002, "text": " The first will have 1500 and the second will have 50 and then you can say fit and away", "tokens": [440, 700, 486, 362, 22671, 293, 264, 1150, 486, 362, 2625, 293, 550, 291, 393, 584, 3318, 293, 1314], "temperature": 0.0, "avg_logprob": -0.2057616074879964, "compression_ratio": 1.4193548387096775, "no_speech_prob": 2.601607320684707e-06}, {"id": 287, "seek": 164764, "start": 1657.5600000000002, "end": 1668.2, "text": " it goes.", "tokens": [309, 1709, 13], "temperature": 0.0, "avg_logprob": -0.2057616074879964, "compression_ratio": 1.4193548387096775, "no_speech_prob": 2.601607320684707e-06}, {"id": 288, "seek": 164764, "start": 1668.2, "end": 1677.2, "text": " Okay so here is our we've got 0.87 so these are doing less well than our dot product version", "tokens": [1033, 370, 510, 307, 527, 321, 600, 658, 1958, 13, 23853, 370, 613, 366, 884, 1570, 731, 813, 527, 5893, 1674, 3037], "temperature": 0.0, "avg_logprob": -0.2057616074879964, "compression_ratio": 1.4193548387096775, "no_speech_prob": 2.601607320684707e-06}, {"id": 289, "seek": 167720, "start": 1677.2, "end": 1681.52, "text": " which is not too surprising because kind of the dot product version is really trying to", "tokens": [597, 307, 406, 886, 8830, 570, 733, 295, 264, 5893, 1674, 3037, 307, 534, 1382, 281], "temperature": 0.0, "avg_logprob": -0.11013051441737584, "compression_ratio": 1.7307692307692308, "no_speech_prob": 1.4509791981254239e-05}, {"id": 290, "seek": 167720, "start": 1681.52, "end": 1686.04, "text": " take advantage of our understanding of the problem domain.", "tokens": [747, 5002, 295, 527, 3701, 295, 264, 1154, 9274, 13], "temperature": 0.0, "avg_logprob": -0.11013051441737584, "compression_ratio": 1.7307692307692308, "no_speech_prob": 1.4509791981254239e-05}, {"id": 291, "seek": 167720, "start": 1686.04, "end": 1692.4, "text": " In practice nowadays a lot of companies kind of combine they kind of create a combined", "tokens": [682, 3124, 13434, 257, 688, 295, 3431, 733, 295, 10432, 436, 733, 295, 1884, 257, 9354], "temperature": 0.0, "avg_logprob": -0.11013051441737584, "compression_ratio": 1.7307692307692308, "no_speech_prob": 1.4509791981254239e-05}, {"id": 292, "seek": 167720, "start": 1692.4, "end": 1699.68, "text": " model that has a dot product component and also has a neural net component.", "tokens": [2316, 300, 575, 257, 5893, 1674, 6542, 293, 611, 575, 257, 18161, 2533, 6542, 13], "temperature": 0.0, "avg_logprob": -0.11013051441737584, "compression_ratio": 1.7307692307692308, "no_speech_prob": 1.4509791981254239e-05}, {"id": 293, "seek": 167720, "start": 1699.68, "end": 1705.56, "text": " The neural net component is particularly helpful if you've got metadata for example information", "tokens": [440, 18161, 2533, 6542, 307, 4098, 4961, 498, 291, 600, 658, 26603, 337, 1365, 1589], "temperature": 0.0, "avg_logprob": -0.11013051441737584, "compression_ratio": 1.7307692307692308, "no_speech_prob": 1.4509791981254239e-05}, {"id": 294, "seek": 170556, "start": 1705.56, "end": 1711.6399999999999, "text": " about your users like when did they sign up how old are they what sex are they you know", "tokens": [466, 428, 5022, 411, 562, 630, 436, 1465, 493, 577, 1331, 366, 436, 437, 3260, 366, 436, 291, 458], "temperature": 0.0, "avg_logprob": -0.13607735104031032, "compression_ratio": 1.7333333333333334, "no_speech_prob": 4.222776169626741e-06}, {"id": 295, "seek": 170556, "start": 1711.6399999999999, "end": 1717.1599999999999, "text": " where are they from and then those are all things that you could concatenate in with", "tokens": [689, 366, 436, 490, 293, 550, 729, 366, 439, 721, 300, 291, 727, 1588, 7186, 473, 294, 365], "temperature": 0.0, "avg_logprob": -0.13607735104031032, "compression_ratio": 1.7333333333333334, "no_speech_prob": 4.222776169626741e-06}, {"id": 296, "seek": 170556, "start": 1717.1599999999999, "end": 1722.84, "text": " your embeddings and ditto with metadata about the movie how old is it what genre is it and", "tokens": [428, 12240, 29432, 293, 274, 34924, 365, 26603, 466, 264, 3169, 577, 1331, 307, 309, 437, 11022, 307, 309, 293], "temperature": 0.0, "avg_logprob": -0.13607735104031032, "compression_ratio": 1.7333333333333334, "no_speech_prob": 4.222776169626741e-06}, {"id": 297, "seek": 170556, "start": 1722.84, "end": 1723.84, "text": " so forth.", "tokens": [370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.13607735104031032, "compression_ratio": 1.7333333333333334, "no_speech_prob": 4.222776169626741e-06}, {"id": 298, "seek": 170556, "start": 1723.84, "end": 1734.76, "text": " All right so we've got a question from Jonah which I think is interesting and the question", "tokens": [1057, 558, 370, 321, 600, 658, 257, 1168, 490, 42353, 597, 286, 519, 307, 1880, 293, 264, 1168], "temperature": 0.0, "avg_logprob": -0.13607735104031032, "compression_ratio": 1.7333333333333334, "no_speech_prob": 4.222776169626741e-06}, {"id": 299, "seek": 173476, "start": 1734.76, "end": 1742.48, "text": " is is there an issue where the bias components are overwhelmingly determined by the non-experts", "tokens": [307, 307, 456, 364, 2734, 689, 264, 12577, 6677, 366, 42926, 9540, 538, 264, 2107, 12, 3121, 610, 1373], "temperature": 0.0, "avg_logprob": -0.1477306260002984, "compression_ratio": 1.443609022556391, "no_speech_prob": 2.994412170664873e-06}, {"id": 300, "seek": 173476, "start": 1742.48, "end": 1749.12, "text": " in a genre?", "tokens": [294, 257, 11022, 30], "temperature": 0.0, "avg_logprob": -0.1477306260002984, "compression_ratio": 1.443609022556391, "no_speech_prob": 2.994412170664873e-06}, {"id": 301, "seek": 173476, "start": 1749.12, "end": 1756.48, "text": " In general actually there's a more general issue which is in collaborative filtering", "tokens": [682, 2674, 767, 456, 311, 257, 544, 2674, 2734, 597, 307, 294, 16555, 30822], "temperature": 0.0, "avg_logprob": -0.1477306260002984, "compression_ratio": 1.443609022556391, "no_speech_prob": 2.994412170664873e-06}, {"id": 302, "seek": 175648, "start": 1756.48, "end": 1767.52, "text": " systems very often a small number of users or a small number of movies overwhelm everybody", "tokens": [3652, 588, 2049, 257, 1359, 1230, 295, 5022, 420, 257, 1359, 1230, 295, 6233, 9103, 76, 2201], "temperature": 0.0, "avg_logprob": -0.11767732593375192, "compression_ratio": 1.7906976744186047, "no_speech_prob": 2.12334566640493e-06}, {"id": 303, "seek": 175648, "start": 1767.52, "end": 1773.44, "text": " else and the classic one is anime.", "tokens": [1646, 293, 264, 7230, 472, 307, 12435, 13], "temperature": 0.0, "avg_logprob": -0.11767732593375192, "compression_ratio": 1.7906976744186047, "no_speech_prob": 2.12334566640493e-06}, {"id": 304, "seek": 175648, "start": 1773.44, "end": 1778.56, "text": " A relatively small number of people watch anime and those groups of people watch a lot", "tokens": [316, 7226, 1359, 1230, 295, 561, 1159, 12435, 293, 729, 3935, 295, 561, 1159, 257, 688], "temperature": 0.0, "avg_logprob": -0.11767732593375192, "compression_ratio": 1.7906976744186047, "no_speech_prob": 2.12334566640493e-06}, {"id": 305, "seek": 175648, "start": 1778.56, "end": 1780.16, "text": " of anime.", "tokens": [295, 12435, 13], "temperature": 0.0, "avg_logprob": -0.11767732593375192, "compression_ratio": 1.7906976744186047, "no_speech_prob": 2.12334566640493e-06}, {"id": 306, "seek": 175648, "start": 1780.16, "end": 1783.84, "text": " So in movie recommendations like there's a classic problem which is every time people", "tokens": [407, 294, 3169, 10434, 411, 456, 311, 257, 7230, 1154, 597, 307, 633, 565, 561], "temperature": 0.0, "avg_logprob": -0.11767732593375192, "compression_ratio": 1.7906976744186047, "no_speech_prob": 2.12334566640493e-06}, {"id": 307, "seek": 178384, "start": 1783.84, "end": 1789.6399999999999, "text": " try to make a list of well-loved movies all the top ones seem to be anime and so you can", "tokens": [853, 281, 652, 257, 1329, 295, 731, 12, 752, 937, 6233, 439, 264, 1192, 2306, 1643, 281, 312, 12435, 293, 370, 291, 393], "temperature": 0.0, "avg_logprob": -0.07700425386428833, "compression_ratio": 1.6322869955156951, "no_speech_prob": 5.682249138772022e-06}, {"id": 308, "seek": 178384, "start": 1789.6399999999999, "end": 1796.6, "text": " imagine what's happening in the matrix completion exercise is that there are yeah some users", "tokens": [3811, 437, 311, 2737, 294, 264, 8141, 19372, 5380, 307, 300, 456, 366, 1338, 512, 5022], "temperature": 0.0, "avg_logprob": -0.07700425386428833, "compression_ratio": 1.6322869955156951, "no_speech_prob": 5.682249138772022e-06}, {"id": 309, "seek": 178384, "start": 1796.6, "end": 1804.6399999999999, "text": " that just you know really watch this one genre of movie and they watch an awful lot of them.", "tokens": [300, 445, 291, 458, 534, 1159, 341, 472, 11022, 295, 3169, 293, 436, 1159, 364, 11232, 688, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.07700425386428833, "compression_ratio": 1.6322869955156951, "no_speech_prob": 5.682249138772022e-06}, {"id": 310, "seek": 178384, "start": 1804.6399999999999, "end": 1809.9599999999998, "text": " So in general you actually do have to be pretty careful about the you know these subtlety", "tokens": [407, 294, 2674, 291, 767, 360, 362, 281, 312, 1238, 5026, 466, 264, 291, 458, 613, 7257, 75, 2210], "temperature": 0.0, "avg_logprob": -0.07700425386428833, "compression_ratio": 1.6322869955156951, "no_speech_prob": 5.682249138772022e-06}, {"id": 311, "seek": 180996, "start": 1809.96, "end": 1816.56, "text": " kind of issues and yeah I won't go into details about how to deal with them but they're generally", "tokens": [733, 295, 2663, 293, 1338, 286, 1582, 380, 352, 666, 4365, 466, 577, 281, 2028, 365, 552, 457, 436, 434, 5101], "temperature": 0.0, "avg_logprob": -0.12892046468011265, "compression_ratio": 1.5202312138728324, "no_speech_prob": 2.2252115741139278e-06}, {"id": 312, "seek": 180996, "start": 1816.56, "end": 1828.92, "text": " involved taking various kinds of ratios or normalizing things or so forth.", "tokens": [3288, 1940, 3683, 3685, 295, 32435, 420, 2710, 3319, 721, 420, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.12892046468011265, "compression_ratio": 1.5202312138728324, "no_speech_prob": 2.2252115741139278e-06}, {"id": 313, "seek": 180996, "start": 1828.92, "end": 1835.76, "text": " All right so that's collaborative filtering and I wanted to show you something interesting", "tokens": [1057, 558, 370, 300, 311, 16555, 30822, 293, 286, 1415, 281, 855, 291, 746, 1880], "temperature": 0.0, "avg_logprob": -0.12892046468011265, "compression_ratio": 1.5202312138728324, "no_speech_prob": 2.2252115741139278e-06}, {"id": 314, "seek": 183576, "start": 1835.76, "end": 1844.8799999999999, "text": " then about embeddings which is that embeddings are not just for collaborative filtering and", "tokens": [550, 466, 12240, 29432, 597, 307, 300, 12240, 29432, 366, 406, 445, 337, 16555, 30822, 293], "temperature": 0.0, "avg_logprob": -0.11626899533155488, "compression_ratio": 1.6866359447004609, "no_speech_prob": 5.093563231639564e-06}, {"id": 315, "seek": 183576, "start": 1844.8799999999999, "end": 1848.56, "text": " in fact if you've heard about embeddings before you've probably heard about them in the context", "tokens": [294, 1186, 498, 291, 600, 2198, 466, 12240, 29432, 949, 291, 600, 1391, 2198, 466, 552, 294, 264, 4319], "temperature": 0.0, "avg_logprob": -0.11626899533155488, "compression_ratio": 1.6866359447004609, "no_speech_prob": 5.093563231639564e-06}, {"id": 316, "seek": 183576, "start": 1848.56, "end": 1851.18, "text": " of natural language processing.", "tokens": [295, 3303, 2856, 9007, 13], "temperature": 0.0, "avg_logprob": -0.11626899533155488, "compression_ratio": 1.6866359447004609, "no_speech_prob": 5.093563231639564e-06}, {"id": 317, "seek": 183576, "start": 1851.18, "end": 1856.9, "text": " So you might have been wondering back when we did the hugging face transformers stuff", "tokens": [407, 291, 1062, 362, 668, 6359, 646, 562, 321, 630, 264, 41706, 1851, 4088, 433, 1507], "temperature": 0.0, "avg_logprob": -0.11626899533155488, "compression_ratio": 1.6866359447004609, "no_speech_prob": 5.093563231639564e-06}, {"id": 318, "seek": 183576, "start": 1856.9, "end": 1865.56, "text": " how did we go about you know using text as inputs to models?", "tokens": [577, 630, 321, 352, 466, 291, 458, 1228, 2487, 382, 15743, 281, 5245, 30], "temperature": 0.0, "avg_logprob": -0.11626899533155488, "compression_ratio": 1.6866359447004609, "no_speech_prob": 5.093563231639564e-06}, {"id": 319, "seek": 186556, "start": 1865.56, "end": 1871.9199999999998, "text": " And we talked about how you can turn words into integers we make a list so here's the", "tokens": [400, 321, 2825, 466, 577, 291, 393, 1261, 2283, 666, 41674, 321, 652, 257, 1329, 370, 510, 311, 264], "temperature": 0.0, "avg_logprob": -0.1669436518351237, "compression_ratio": 1.6875, "no_speech_prob": 1.4738774552824907e-05}, {"id": 320, "seek": 186556, "start": 1871.9199999999998, "end": 1878.24, "text": " movie sorry movie here's the poem I am Sam I am Daniel I am Sam Sam I am that Sam I", "tokens": [3169, 2597, 3169, 510, 311, 264, 13065, 286, 669, 4832, 286, 669, 8033, 286, 669, 4832, 4832, 286, 669, 300, 4832, 286], "temperature": 0.0, "avg_logprob": -0.1669436518351237, "compression_ratio": 1.6875, "no_speech_prob": 1.4738774552824907e-05}, {"id": 321, "seek": 186556, "start": 1878.24, "end": 1882.62, "text": " am etc etc.", "tokens": [669, 5183, 5183, 13], "temperature": 0.0, "avg_logprob": -0.1669436518351237, "compression_ratio": 1.6875, "no_speech_prob": 1.4738774552824907e-05}, {"id": 322, "seek": 186556, "start": 1882.62, "end": 1887.96, "text": " We can find a list of all the unique words in that poem and make this list here and then", "tokens": [492, 393, 915, 257, 1329, 295, 439, 264, 3845, 2283, 294, 300, 13065, 293, 652, 341, 1329, 510, 293, 550], "temperature": 0.0, "avg_logprob": -0.1669436518351237, "compression_ratio": 1.6875, "no_speech_prob": 1.4738774552824907e-05}, {"id": 323, "seek": 188796, "start": 1887.96, "end": 1895.32, "text": " we can give each of those words a unique ID just arbitrarily well actually in this case", "tokens": [321, 393, 976, 1184, 295, 729, 2283, 257, 3845, 7348, 445, 19071, 3289, 731, 767, 294, 341, 1389], "temperature": 0.0, "avg_logprob": -0.11191527048746745, "compression_ratio": 1.6120689655172413, "no_speech_prob": 1.577954321874131e-06}, {"id": 324, "seek": 188796, "start": 1895.32, "end": 1900.52, "text": " it's alphabetical order but it doesn't have to be and so we kind of talked about that", "tokens": [309, 311, 23339, 804, 1668, 457, 309, 1177, 380, 362, 281, 312, 293, 370, 321, 733, 295, 2825, 466, 300], "temperature": 0.0, "avg_logprob": -0.11191527048746745, "compression_ratio": 1.6120689655172413, "no_speech_prob": 1.577954321874131e-06}, {"id": 325, "seek": 188796, "start": 1900.52, "end": 1906.64, "text": " and that's what we do with categories in general but how do we turn those into like you know", "tokens": [293, 300, 311, 437, 321, 360, 365, 10479, 294, 2674, 457, 577, 360, 321, 1261, 729, 666, 411, 291, 458], "temperature": 0.0, "avg_logprob": -0.11191527048746745, "compression_ratio": 1.6120689655172413, "no_speech_prob": 1.577954321874131e-06}, {"id": 326, "seek": 188796, "start": 1906.64, "end": 1911.92, "text": " lists of random numbers and you might not be surprised to hear what we do is we create", "tokens": [14511, 295, 4974, 3547, 293, 291, 1062, 406, 312, 6100, 281, 1568, 437, 321, 360, 307, 321, 1884], "temperature": 0.0, "avg_logprob": -0.11191527048746745, "compression_ratio": 1.6120689655172413, "no_speech_prob": 1.577954321874131e-06}, {"id": 327, "seek": 188796, "start": 1911.92, "end": 1914.28, "text": " an embedding matrix.", "tokens": [364, 12240, 3584, 8141, 13], "temperature": 0.0, "avg_logprob": -0.11191527048746745, "compression_ratio": 1.6120689655172413, "no_speech_prob": 1.577954321874131e-06}, {"id": 328, "seek": 191428, "start": 1914.28, "end": 1922.84, "text": " So here's an embedding matrix containing four latent factors for each word in the vocab.", "tokens": [407, 510, 311, 364, 12240, 3584, 8141, 19273, 1451, 48994, 6771, 337, 1184, 1349, 294, 264, 2329, 455, 13], "temperature": 0.0, "avg_logprob": -0.0945693456209623, "compression_ratio": 1.6971830985915493, "no_speech_prob": 8.990940614239662e-07}, {"id": 329, "seek": 191428, "start": 1922.84, "end": 1926.8799999999999, "text": " So here's each word in the vocab and here's the embedding matrix.", "tokens": [407, 510, 311, 1184, 1349, 294, 264, 2329, 455, 293, 510, 311, 264, 12240, 3584, 8141, 13], "temperature": 0.0, "avg_logprob": -0.0945693456209623, "compression_ratio": 1.6971830985915493, "no_speech_prob": 8.990940614239662e-07}, {"id": 330, "seek": 191428, "start": 1926.8799999999999, "end": 1939.6, "text": " So if we then want to present this poem to a neural net then what we do is we list out", "tokens": [407, 498, 321, 550, 528, 281, 1974, 341, 13065, 281, 257, 18161, 2533, 550, 437, 321, 360, 307, 321, 1329, 484], "temperature": 0.0, "avg_logprob": -0.0945693456209623, "compression_ratio": 1.6971830985915493, "no_speech_prob": 8.990940614239662e-07}, {"id": 331, "seek": 193960, "start": 1939.6, "end": 1946.0, "text": " our poem I do not like that Sam I am do you like green eggs and ham etc.", "tokens": [527, 13065, 286, 360, 406, 411, 300, 4832, 286, 669, 360, 291, 411, 3092, 6466, 293, 7852, 5183, 13], "temperature": 0.0, "avg_logprob": -0.13886773408348882, "compression_ratio": 1.5534591194968554, "no_speech_prob": 1.1015938525815727e-06}, {"id": 332, "seek": 193960, "start": 1946.0, "end": 1953.52, "text": " Then for each word we look it up so in Excel for example we use match so that will find", "tokens": [1396, 337, 1184, 1349, 321, 574, 309, 493, 370, 294, 19060, 337, 1365, 321, 764, 2995, 370, 300, 486, 915], "temperature": 0.0, "avg_logprob": -0.13886773408348882, "compression_ratio": 1.5534591194968554, "no_speech_prob": 1.1015938525815727e-06}, {"id": 333, "seek": 193960, "start": 1953.52, "end": 1966.36, "text": " this word over here and find it is word ID 8 and then we will find the eighth word and", "tokens": [341, 1349, 670, 510, 293, 915, 309, 307, 1349, 7348, 1649, 293, 550, 321, 486, 915, 264, 19495, 1349, 293], "temperature": 0.0, "avg_logprob": -0.13886773408348882, "compression_ratio": 1.5534591194968554, "no_speech_prob": 1.1015938525815727e-06}, {"id": 334, "seek": 196636, "start": 1966.36, "end": 1985.12, "text": " the first embedding and so that's gives us that's not right 8 oh no that is right sorry", "tokens": [264, 700, 12240, 3584, 293, 370, 300, 311, 2709, 505, 300, 311, 406, 558, 1649, 1954, 572, 300, 307, 558, 2597], "temperature": 0.0, "avg_logprob": -0.2075942824868595, "compression_ratio": 1.8308823529411764, "no_speech_prob": 9.516194950265344e-06}, {"id": 335, "seek": 196636, "start": 1985.12, "end": 1990.6399999999999, "text": " here it is it's just weird column it's so it's going to be point two two then point", "tokens": [510, 309, 307, 309, 311, 445, 3657, 7738, 309, 311, 370, 309, 311, 516, 281, 312, 935, 732, 732, 550, 935], "temperature": 0.0, "avg_logprob": -0.2075942824868595, "compression_ratio": 1.8308823529411764, "no_speech_prob": 9.516194950265344e-06}, {"id": 336, "seek": 196636, "start": 1990.6399999999999, "end": 1994.6799999999998, "text": " one point zero one and here it is point two two point one point zero one etc.", "tokens": [472, 935, 4018, 472, 293, 510, 309, 307, 935, 732, 732, 935, 472, 935, 4018, 472, 5183, 13], "temperature": 0.0, "avg_logprob": -0.2075942824868595, "compression_ratio": 1.8308823529411764, "no_speech_prob": 9.516194950265344e-06}, {"id": 337, "seek": 199468, "start": 1994.68, "end": 2003.44, "text": " So this is the embedding matrix we end up with for this poem and so if you wanted to", "tokens": [407, 341, 307, 264, 12240, 3584, 8141, 321, 917, 493, 365, 337, 341, 13065, 293, 370, 498, 291, 1415, 281], "temperature": 0.0, "avg_logprob": -0.1209481990698612, "compression_ratio": 1.6645569620253164, "no_speech_prob": 1.6280399677270907e-06}, {"id": 338, "seek": 199468, "start": 2003.44, "end": 2009.3400000000001, "text": " train or use a train neural network on this poem you basically turn it into this matrix", "tokens": [3847, 420, 764, 257, 3847, 18161, 3209, 322, 341, 13065, 291, 1936, 1261, 309, 666, 341, 8141], "temperature": 0.0, "avg_logprob": -0.1209481990698612, "compression_ratio": 1.6645569620253164, "no_speech_prob": 1.6280399677270907e-06}, {"id": 339, "seek": 199468, "start": 2009.3400000000001, "end": 2018.6200000000001, "text": " of numbers and so this is what an embedding matrix looks like in an NLP model and it works", "tokens": [295, 3547, 293, 370, 341, 307, 437, 364, 12240, 3584, 8141, 1542, 411, 294, 364, 426, 45196, 2316, 293, 309, 1985], "temperature": 0.0, "avg_logprob": -0.1209481990698612, "compression_ratio": 1.6645569620253164, "no_speech_prob": 1.6280399677270907e-06}, {"id": 340, "seek": 201862, "start": 2018.62, "end": 2025.7199999999998, "text": " exactly the same way as you can see and then you can do exactly the same things in terms", "tokens": [2293, 264, 912, 636, 382, 291, 393, 536, 293, 550, 291, 393, 360, 2293, 264, 912, 721, 294, 2115], "temperature": 0.0, "avg_logprob": -0.056316760870126575, "compression_ratio": 1.548148148148148, "no_speech_prob": 1.760333361744415e-06}, {"id": 341, "seek": 201862, "start": 2025.7199999999998, "end": 2037.4799999999998, "text": " of interpretation of an NLP model by looking at both the bias factors and the latent factors", "tokens": [295, 14174, 295, 364, 426, 45196, 2316, 538, 1237, 412, 1293, 264, 12577, 6771, 293, 264, 48994, 6771], "temperature": 0.0, "avg_logprob": -0.056316760870126575, "compression_ratio": 1.548148148148148, "no_speech_prob": 1.760333361744415e-06}, {"id": 342, "seek": 201862, "start": 2037.4799999999998, "end": 2041.28, "text": " in a word embedding matrix.", "tokens": [294, 257, 1349, 12240, 3584, 8141, 13], "temperature": 0.0, "avg_logprob": -0.056316760870126575, "compression_ratio": 1.548148148148148, "no_speech_prob": 1.760333361744415e-06}, {"id": 343, "seek": 204128, "start": 2041.28, "end": 2052.8, "text": " So hopefully you're getting the idea here that our you know our different models you", "tokens": [407, 4696, 291, 434, 1242, 264, 1558, 510, 300, 527, 291, 458, 527, 819, 5245, 291], "temperature": 0.0, "avg_logprob": -0.11134466406417219, "compression_ratio": 1.729064039408867, "no_speech_prob": 2.332063331778045e-06}, {"id": 344, "seek": 204128, "start": 2052.8, "end": 2057.44, "text": " know the inputs to them that they're based on a relatively small number of kind of basic", "tokens": [458, 264, 15743, 281, 552, 300, 436, 434, 2361, 322, 257, 7226, 1359, 1230, 295, 733, 295, 3875], "temperature": 0.0, "avg_logprob": -0.11134466406417219, "compression_ratio": 1.729064039408867, "no_speech_prob": 2.332063331778045e-06}, {"id": 345, "seek": 204128, "start": 2057.44, "end": 2064.24, "text": " principles and these principles are generally things like lock up something in array and", "tokens": [9156, 293, 613, 9156, 366, 5101, 721, 411, 4017, 493, 746, 294, 10225, 293], "temperature": 0.0, "avg_logprob": -0.11134466406417219, "compression_ratio": 1.729064039408867, "no_speech_prob": 2.332063331778045e-06}, {"id": 346, "seek": 204128, "start": 2064.24, "end": 2069.72, "text": " then we know inside the model we're basically multiplying things together adding them up", "tokens": [550, 321, 458, 1854, 264, 2316, 321, 434, 1936, 30955, 721, 1214, 5127, 552, 493], "temperature": 0.0, "avg_logprob": -0.11134466406417219, "compression_ratio": 1.729064039408867, "no_speech_prob": 2.332063331778045e-06}, {"id": 347, "seek": 206972, "start": 2069.72, "end": 2072.12, "text": " and replacing the negatives and zeros.", "tokens": [293, 19139, 264, 40019, 293, 35193, 13], "temperature": 0.0, "avg_logprob": -0.13768305248684354, "compression_ratio": 1.380281690140845, "no_speech_prob": 7.571125593131001e-07}, {"id": 348, "seek": 206972, "start": 2072.12, "end": 2075.9199999999996, "text": " So hopefully you're getting the idea that what's going on inside a neural network is", "tokens": [407, 4696, 291, 434, 1242, 264, 1558, 300, 437, 311, 516, 322, 1854, 257, 18161, 3209, 307], "temperature": 0.0, "avg_logprob": -0.13768305248684354, "compression_ratio": 1.380281690140845, "no_speech_prob": 7.571125593131001e-07}, {"id": 349, "seek": 206972, "start": 2075.9199999999996, "end": 2088.68, "text": " generally not that complicated but it happens very quickly and at scale.", "tokens": [5101, 406, 300, 6179, 457, 309, 2314, 588, 2661, 293, 412, 4373, 13], "temperature": 0.0, "avg_logprob": -0.13768305248684354, "compression_ratio": 1.380281690140845, "no_speech_prob": 7.571125593131001e-07}, {"id": 350, "seek": 208868, "start": 2088.68, "end": 2102.16, "text": " Now it's not just collaborative filtering and NLP but also tabular analysis.", "tokens": [823, 309, 311, 406, 445, 16555, 30822, 293, 426, 45196, 457, 611, 4421, 1040, 5215, 13], "temperature": 0.0, "avg_logprob": -0.0884164673941476, "compression_ratio": 1.4747474747474747, "no_speech_prob": 1.3287689171193051e-06}, {"id": 351, "seek": 208868, "start": 2102.16, "end": 2108.9199999999996, "text": " So in chapter 9 of the book we've talked about how random forests can be used for this which", "tokens": [407, 294, 7187, 1722, 295, 264, 1446, 321, 600, 2825, 466, 577, 4974, 21700, 393, 312, 1143, 337, 341, 597], "temperature": 0.0, "avg_logprob": -0.0884164673941476, "compression_ratio": 1.4747474747474747, "no_speech_prob": 1.3287689171193051e-06}, {"id": 352, "seek": 208868, "start": 2108.9199999999996, "end": 2115.24, "text": " was for this is for the thing where we're predicting the auction sale price of industrial", "tokens": [390, 337, 341, 307, 337, 264, 551, 689, 321, 434, 32884, 264, 24139, 8680, 3218, 295, 9987], "temperature": 0.0, "avg_logprob": -0.0884164673941476, "compression_ratio": 1.4747474747474747, "no_speech_prob": 1.3287689171193051e-06}, {"id": 353, "seek": 208868, "start": 2115.24, "end": 2118.52, "text": " heavy equipment like bulldozers.", "tokens": [4676, 5927, 411, 4693, 2595, 41698, 13], "temperature": 0.0, "avg_logprob": -0.0884164673941476, "compression_ratio": 1.4747474747474747, "no_speech_prob": 1.3287689171193051e-06}, {"id": 354, "seek": 211852, "start": 2118.52, "end": 2123.32, "text": " Instead of using a random forest we can use a neural net.", "tokens": [7156, 295, 1228, 257, 4974, 6719, 321, 393, 764, 257, 18161, 2533, 13], "temperature": 0.0, "avg_logprob": -0.10257510007438013, "compression_ratio": 1.5827814569536425, "no_speech_prob": 1.3081731822239817e-06}, {"id": 355, "seek": 211852, "start": 2123.32, "end": 2139.08, "text": " Now in this data set there are some continuous columns and there are some categorical columns.", "tokens": [823, 294, 341, 1412, 992, 456, 366, 512, 10957, 13766, 293, 456, 366, 512, 19250, 804, 13766, 13], "temperature": 0.0, "avg_logprob": -0.10257510007438013, "compression_ratio": 1.5827814569536425, "no_speech_prob": 1.3081731822239817e-06}, {"id": 356, "seek": 211852, "start": 2139.08, "end": 2145.4, "text": " Now I'm not going to go into the details too much but in short the we can separate out", "tokens": [823, 286, 478, 406, 516, 281, 352, 666, 264, 4365, 886, 709, 457, 294, 2099, 264, 321, 393, 4994, 484], "temperature": 0.0, "avg_logprob": -0.10257510007438013, "compression_ratio": 1.5827814569536425, "no_speech_prob": 1.3081731822239817e-06}, {"id": 357, "seek": 214540, "start": 2145.4, "end": 2151.56, "text": " the continuous columns and categorical columns using CONTCAT split and that will automatically", "tokens": [264, 10957, 13766, 293, 19250, 804, 13766, 1228, 16596, 51, 34, 2218, 7472, 293, 300, 486, 6772], "temperature": 0.0, "avg_logprob": -0.18592422485351562, "compression_ratio": 1.606060606060606, "no_speech_prob": 1.505698151049728e-06}, {"id": 358, "seek": 214540, "start": 2151.56, "end": 2157.92, "text": " find which is which based on their data types.", "tokens": [915, 597, 307, 597, 2361, 322, 641, 1412, 3467, 13], "temperature": 0.0, "avg_logprob": -0.18592422485351562, "compression_ratio": 1.606060606060606, "no_speech_prob": 1.505698151049728e-06}, {"id": 359, "seek": 214540, "start": 2157.92, "end": 2167.0, "text": " And so in this case it looks like okay so continuous columns the elapsed sale date so", "tokens": [400, 370, 294, 341, 1389, 309, 1542, 411, 1392, 370, 10957, 13766, 264, 806, 2382, 292, 8680, 4002, 370], "temperature": 0.0, "avg_logprob": -0.18592422485351562, "compression_ratio": 1.606060606060606, "no_speech_prob": 1.505698151049728e-06}, {"id": 360, "seek": 214540, "start": 2167.0, "end": 2172.2400000000002, "text": " I think that's the number of seconds or years or something since the start of the data set", "tokens": [286, 519, 300, 311, 264, 1230, 295, 3949, 420, 924, 420, 746, 1670, 264, 722, 295, 264, 1412, 992], "temperature": 0.0, "avg_logprob": -0.18592422485351562, "compression_ratio": 1.606060606060606, "no_speech_prob": 1.505698151049728e-06}, {"id": 361, "seek": 217224, "start": 2172.24, "end": 2176.08, "text": " is a continuous variable.", "tokens": [307, 257, 10957, 7006, 13], "temperature": 0.0, "avg_logprob": -0.13025870756669478, "compression_ratio": 1.4304635761589404, "no_speech_prob": 2.3687896373303374e-06}, {"id": 362, "seek": 217224, "start": 2176.08, "end": 2181.68, "text": " And then here are the categorical variables so for example there are six different product", "tokens": [400, 550, 510, 366, 264, 19250, 804, 9102, 370, 337, 1365, 456, 366, 2309, 819, 1674], "temperature": 0.0, "avg_logprob": -0.13025870756669478, "compression_ratio": 1.4304635761589404, "no_speech_prob": 2.3687896373303374e-06}, {"id": 363, "seek": 217224, "start": 2181.68, "end": 2190.64, "text": " sizes and two coupler systems, 5059 model descriptions, six enclosures, 17 tire sizes", "tokens": [11602, 293, 732, 1384, 22732, 3652, 11, 2625, 19600, 2316, 24406, 11, 2309, 2058, 9389, 1303, 11, 3282, 11756, 11602], "temperature": 0.0, "avg_logprob": -0.13025870756669478, "compression_ratio": 1.4304635761589404, "no_speech_prob": 2.3687896373303374e-06}, {"id": 364, "seek": 217224, "start": 2190.64, "end": 2195.04, "text": " and so forth.", "tokens": [293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.13025870756669478, "compression_ratio": 1.4304635761589404, "no_speech_prob": 2.3687896373303374e-06}, {"id": 365, "seek": 219504, "start": 2195.04, "end": 2206.68, "text": " So we can use fast AI basically to say okay we'll take that data frame and pass in the", "tokens": [407, 321, 393, 764, 2370, 7318, 1936, 281, 584, 1392, 321, 603, 747, 300, 1412, 3920, 293, 1320, 294, 264], "temperature": 0.0, "avg_logprob": -0.11669007937113444, "compression_ratio": 1.5266666666666666, "no_speech_prob": 3.9897142301015265e-07}, {"id": 366, "seek": 219504, "start": 2206.68, "end": 2212.7599999999998, "text": " categorical and continuous variables and create some random splits and what's the dependent", "tokens": [19250, 804, 293, 10957, 9102, 293, 1884, 512, 4974, 37741, 293, 437, 311, 264, 12334], "temperature": 0.0, "avg_logprob": -0.11669007937113444, "compression_ratio": 1.5266666666666666, "no_speech_prob": 3.9897142301015265e-07}, {"id": 367, "seek": 219504, "start": 2212.7599999999998, "end": 2222.32, "text": " variable and we can create data loaders from that.", "tokens": [7006, 293, 321, 393, 1884, 1412, 3677, 433, 490, 300, 13], "temperature": 0.0, "avg_logprob": -0.11669007937113444, "compression_ratio": 1.5266666666666666, "no_speech_prob": 3.9897142301015265e-07}, {"id": 368, "seek": 222232, "start": 2222.32, "end": 2227.88, "text": " And from that we can create a tabular learner and basically what that's going to do is it's", "tokens": [400, 490, 300, 321, 393, 1884, 257, 4421, 1040, 33347, 293, 1936, 437, 300, 311, 516, 281, 360, 307, 309, 311], "temperature": 0.0, "avg_logprob": -0.102567229952131, "compression_ratio": 1.6958762886597938, "no_speech_prob": 1.1015932841473841e-06}, {"id": 369, "seek": 222232, "start": 2227.88, "end": 2237.88, "text": " going to create a pretty regular multi-layer neural network not that different to this", "tokens": [516, 281, 1884, 257, 1238, 3890, 4825, 12, 8376, 260, 18161, 3209, 406, 300, 819, 281, 341], "temperature": 0.0, "avg_logprob": -0.102567229952131, "compression_ratio": 1.6958762886597938, "no_speech_prob": 1.1015932841473841e-06}, {"id": 370, "seek": 222232, "start": 2237.88, "end": 2243.1600000000003, "text": " one that we created by hand.", "tokens": [472, 300, 321, 2942, 538, 1011, 13], "temperature": 0.0, "avg_logprob": -0.102567229952131, "compression_ratio": 1.6958762886597938, "no_speech_prob": 1.1015932841473841e-06}, {"id": 371, "seek": 222232, "start": 2243.1600000000003, "end": 2249.0800000000004, "text": " And each of the categorical variables it's going to create an embedding for it and so", "tokens": [400, 1184, 295, 264, 19250, 804, 9102, 309, 311, 516, 281, 1884, 364, 12240, 3584, 337, 309, 293, 370], "temperature": 0.0, "avg_logprob": -0.102567229952131, "compression_ratio": 1.6958762886597938, "no_speech_prob": 1.1015932841473841e-06}, {"id": 372, "seek": 222232, "start": 2249.0800000000004, "end": 2250.52, "text": " I can actually show you this right.", "tokens": [286, 393, 767, 855, 291, 341, 558, 13], "temperature": 0.0, "avg_logprob": -0.102567229952131, "compression_ratio": 1.6958762886597938, "no_speech_prob": 1.1015932841473841e-06}, {"id": 373, "seek": 225052, "start": 2250.52, "end": 2258.2, "text": " So we're going to use tabular learner to create the learner and so tabular learner is one", "tokens": [407, 321, 434, 516, 281, 764, 4421, 1040, 33347, 281, 1884, 264, 33347, 293, 370, 4421, 1040, 33347, 307, 472], "temperature": 0.0, "avg_logprob": -0.13579039627246642, "compression_ratio": 1.8210526315789475, "no_speech_prob": 5.285510837893526e-07}, {"id": 374, "seek": 225052, "start": 2258.2, "end": 2262.88, "text": " two three four five six seven eight nine lines of code and basically the main thing it does", "tokens": [732, 1045, 1451, 1732, 2309, 3407, 3180, 4949, 3876, 295, 3089, 293, 1936, 264, 2135, 551, 309, 775], "temperature": 0.0, "avg_logprob": -0.13579039627246642, "compression_ratio": 1.8210526315789475, "no_speech_prob": 5.285510837893526e-07}, {"id": 375, "seek": 225052, "start": 2262.88, "end": 2270.52, "text": " is create a tabular model and so then tabular model you're not going to understand all of", "tokens": [307, 1884, 257, 4421, 1040, 2316, 293, 370, 550, 4421, 1040, 2316, 291, 434, 406, 516, 281, 1223, 439, 295], "temperature": 0.0, "avg_logprob": -0.13579039627246642, "compression_ratio": 1.8210526315789475, "no_speech_prob": 5.285510837893526e-07}, {"id": 376, "seek": 225052, "start": 2270.52, "end": 2273.32, "text": " it but you might be surprised at how much.", "tokens": [309, 457, 291, 1062, 312, 6100, 412, 577, 709, 13], "temperature": 0.0, "avg_logprob": -0.13579039627246642, "compression_ratio": 1.8210526315789475, "no_speech_prob": 5.285510837893526e-07}, {"id": 377, "seek": 225052, "start": 2273.32, "end": 2276.04, "text": " So a tabular model is a module.", "tokens": [407, 257, 4421, 1040, 2316, 307, 257, 10088, 13], "temperature": 0.0, "avg_logprob": -0.13579039627246642, "compression_ratio": 1.8210526315789475, "no_speech_prob": 5.285510837893526e-07}, {"id": 378, "seek": 227604, "start": 2276.04, "end": 2285.96, "text": " We're going to be passing in how big is each embedding going to be and tabular learner", "tokens": [492, 434, 516, 281, 312, 8437, 294, 577, 955, 307, 1184, 12240, 3584, 516, 281, 312, 293, 4421, 1040, 33347], "temperature": 0.0, "avg_logprob": -0.08871473024969231, "compression_ratio": 1.7407407407407407, "no_speech_prob": 7.112431603673031e-07}, {"id": 379, "seek": 227604, "start": 2285.96, "end": 2288.36, "text": " what's that passing in?", "tokens": [437, 311, 300, 8437, 294, 30], "temperature": 0.0, "avg_logprob": -0.08871473024969231, "compression_ratio": 1.7407407407407407, "no_speech_prob": 7.112431603673031e-07}, {"id": 380, "seek": 227604, "start": 2288.36, "end": 2294.92, "text": " It's going to call get embedding sizes just like we did manually before automatically.", "tokens": [467, 311, 516, 281, 818, 483, 12240, 3584, 11602, 445, 411, 321, 630, 16945, 949, 6772, 13], "temperature": 0.0, "avg_logprob": -0.08871473024969231, "compression_ratio": 1.7407407407407407, "no_speech_prob": 7.112431603673031e-07}, {"id": 381, "seek": 227604, "start": 2294.92, "end": 2301.6, "text": " So that's how it gets its embedding sizes and then it's going to create an embedding", "tokens": [407, 300, 311, 577, 309, 2170, 1080, 12240, 3584, 11602, 293, 550, 309, 311, 516, 281, 1884, 364, 12240, 3584], "temperature": 0.0, "avg_logprob": -0.08871473024969231, "compression_ratio": 1.7407407407407407, "no_speech_prob": 7.112431603673031e-07}, {"id": 382, "seek": 230160, "start": 2301.6, "end": 2309.48, "text": " for each of those embedding sizes from number of inputs to number of factors.", "tokens": [337, 1184, 295, 729, 12240, 3584, 11602, 490, 1230, 295, 15743, 281, 1230, 295, 6771, 13], "temperature": 0.0, "avg_logprob": -0.1530763643597244, "compression_ratio": 1.7467248908296944, "no_speech_prob": 5.539167773349618e-07}, {"id": 383, "seek": 230160, "start": 2309.48, "end": 2310.96, "text": " Dropout we're going to come back to later.", "tokens": [17675, 346, 321, 434, 516, 281, 808, 646, 281, 1780, 13], "temperature": 0.0, "avg_logprob": -0.1530763643597244, "compression_ratio": 1.7467248908296944, "no_speech_prob": 5.539167773349618e-07}, {"id": 384, "seek": 230160, "start": 2310.96, "end": 2314.7599999999998, "text": " BatchNorm we won't do until part two.", "tokens": [363, 852, 45, 687, 321, 1582, 380, 360, 1826, 644, 732, 13], "temperature": 0.0, "avg_logprob": -0.1530763643597244, "compression_ratio": 1.7467248908296944, "no_speech_prob": 5.539167773349618e-07}, {"id": 385, "seek": 230160, "start": 2314.7599999999998, "end": 2319.56, "text": " So then it's going to create a layer for each of the layers we want which is going to contain", "tokens": [407, 550, 309, 311, 516, 281, 1884, 257, 4583, 337, 1184, 295, 264, 7914, 321, 528, 597, 307, 516, 281, 5304], "temperature": 0.0, "avg_logprob": -0.1530763643597244, "compression_ratio": 1.7467248908296944, "no_speech_prob": 5.539167773349618e-07}, {"id": 386, "seek": 230160, "start": 2319.56, "end": 2324.48, "text": " a linear layer followed by batchNorm followed by dropout.", "tokens": [257, 8213, 4583, 6263, 538, 15245, 45, 687, 6263, 538, 3270, 346, 13], "temperature": 0.0, "avg_logprob": -0.1530763643597244, "compression_ratio": 1.7467248908296944, "no_speech_prob": 5.539167773349618e-07}, {"id": 387, "seek": 230160, "start": 2324.48, "end": 2331.36, "text": " It's going to add the sigmoid range we've talked about at the very end and so the forward", "tokens": [467, 311, 516, 281, 909, 264, 4556, 3280, 327, 3613, 321, 600, 2825, 466, 412, 264, 588, 917, 293, 370, 264, 2128], "temperature": 0.0, "avg_logprob": -0.1530763643597244, "compression_ratio": 1.7467248908296944, "no_speech_prob": 5.539167773349618e-07}, {"id": 388, "seek": 233136, "start": 2331.36, "end": 2332.6400000000003, "text": " code.", "tokens": [3089, 13], "temperature": 0.0, "avg_logprob": -0.15360240346377657, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.804997158520564e-07}, {"id": 389, "seek": 233136, "start": 2332.6400000000003, "end": 2335.08, "text": " This is the entire thing.", "tokens": [639, 307, 264, 2302, 551, 13], "temperature": 0.0, "avg_logprob": -0.15360240346377657, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.804997158520564e-07}, {"id": 390, "seek": 233136, "start": 2335.08, "end": 2339.0, "text": " If there's some embeddings it'll go through and get each of the embeddings using the same", "tokens": [759, 456, 311, 512, 12240, 29432, 309, 603, 352, 807, 293, 483, 1184, 295, 264, 12240, 29432, 1228, 264, 912], "temperature": 0.0, "avg_logprob": -0.15360240346377657, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.804997158520564e-07}, {"id": 391, "seek": 233136, "start": 2339.0, "end": 2341.6, "text": " indexing approach we've used before.", "tokens": [8186, 278, 3109, 321, 600, 1143, 949, 13], "temperature": 0.0, "avg_logprob": -0.15360240346377657, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.804997158520564e-07}, {"id": 392, "seek": 233136, "start": 2341.6, "end": 2348.84, "text": " It'll concatenate them all together and then it'll run it through the layers of the neural", "tokens": [467, 603, 1588, 7186, 473, 552, 439, 1214, 293, 550, 309, 603, 1190, 309, 807, 264, 7914, 295, 264, 18161], "temperature": 0.0, "avg_logprob": -0.15360240346377657, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.804997158520564e-07}, {"id": 393, "seek": 233136, "start": 2348.84, "end": 2353.56, "text": " net which are these.", "tokens": [2533, 597, 366, 613, 13], "temperature": 0.0, "avg_logprob": -0.15360240346377657, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.804997158520564e-07}, {"id": 394, "seek": 233136, "start": 2353.56, "end": 2359.2000000000003, "text": " So yeah we don't know all of those details yet but we know quite a few of them.", "tokens": [407, 1338, 321, 500, 380, 458, 439, 295, 729, 4365, 1939, 457, 321, 458, 1596, 257, 1326, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.15360240346377657, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.804997158520564e-07}, {"id": 395, "seek": 235920, "start": 2359.2, "end": 2372.1, "text": " So that's encouraging hopefully.", "tokens": [407, 300, 311, 14580, 4696, 13], "temperature": 0.0, "avg_logprob": -0.1656691639922386, "compression_ratio": 1.2540983606557377, "no_speech_prob": 7.690353527323168e-07}, {"id": 396, "seek": 235920, "start": 2372.1, "end": 2377.6, "text": " And once we've got that we can do the standard LR find and fit.", "tokens": [400, 1564, 321, 600, 658, 300, 321, 393, 360, 264, 3832, 441, 49, 915, 293, 3318, 13], "temperature": 0.0, "avg_logprob": -0.1656691639922386, "compression_ratio": 1.2540983606557377, "no_speech_prob": 7.690353527323168e-07}, {"id": 397, "seek": 235920, "start": 2377.6, "end": 2388.08, "text": " Now this exact dataset was used in a Kaggle competition.", "tokens": [823, 341, 1900, 28872, 390, 1143, 294, 257, 48751, 22631, 6211, 13], "temperature": 0.0, "avg_logprob": -0.1656691639922386, "compression_ratio": 1.2540983606557377, "no_speech_prob": 7.690353527323168e-07}, {"id": 398, "seek": 238808, "start": 2388.08, "end": 2395.7999999999997, "text": " This dataset was in a Kaggle competition and the third place getter published a paper about", "tokens": [639, 28872, 390, 294, 257, 48751, 22631, 6211, 293, 264, 2636, 1081, 483, 391, 6572, 257, 3035, 466], "temperature": 0.0, "avg_logprob": -0.18987160760003166, "compression_ratio": 1.6307692307692307, "no_speech_prob": 4.029279352835147e-06}, {"id": 399, "seek": 238808, "start": 2395.7999999999997, "end": 2400.16, "text": " their technique and it's basically the exact almost the exact one I'm showing you here.", "tokens": [641, 6532, 293, 309, 311, 1936, 264, 1900, 1920, 264, 1900, 472, 286, 478, 4099, 291, 510, 13], "temperature": 0.0, "avg_logprob": -0.18987160760003166, "compression_ratio": 1.6307692307692307, "no_speech_prob": 4.029279352835147e-06}, {"id": 400, "seek": 238808, "start": 2400.16, "end": 2408.0, "text": " So it wasn't this dataset it was a different one.", "tokens": [407, 309, 2067, 380, 341, 28872, 309, 390, 257, 819, 472, 13], "temperature": 0.0, "avg_logprob": -0.18987160760003166, "compression_ratio": 1.6307692307692307, "no_speech_prob": 4.029279352835147e-06}, {"id": 401, "seek": 238808, "start": 2408.0, "end": 2417.2799999999997, "text": " It was about predicting the amount of sales in different stores but they used this basic", "tokens": [467, 390, 466, 32884, 264, 2372, 295, 5763, 294, 819, 9512, 457, 436, 1143, 341, 3875], "temperature": 0.0, "avg_logprob": -0.18987160760003166, "compression_ratio": 1.6307692307692307, "no_speech_prob": 4.029279352835147e-06}, {"id": 402, "seek": 241728, "start": 2417.28, "end": 2419.7200000000003, "text": " kind of technique.", "tokens": [733, 295, 6532, 13], "temperature": 0.0, "avg_logprob": -0.12834620084918913, "compression_ratio": 1.7236842105263157, "no_speech_prob": 9.422402058589796e-07}, {"id": 403, "seek": 241728, "start": 2419.7200000000003, "end": 2426.36, "text": " And one of the interesting things is that they used a lot less manual feature engineering", "tokens": [400, 472, 295, 264, 1880, 721, 307, 300, 436, 1143, 257, 688, 1570, 9688, 4111, 7043], "temperature": 0.0, "avg_logprob": -0.12834620084918913, "compression_ratio": 1.7236842105263157, "no_speech_prob": 9.422402058589796e-07}, {"id": 404, "seek": 241728, "start": 2426.36, "end": 2429.6000000000004, "text": " than the other high placed entries.", "tokens": [813, 264, 661, 1090, 7074, 23041, 13], "temperature": 0.0, "avg_logprob": -0.12834620084918913, "compression_ratio": 1.7236842105263157, "no_speech_prob": 9.422402058589796e-07}, {"id": 405, "seek": 241728, "start": 2429.6000000000004, "end": 2432.1200000000003, "text": " Like they had a much simpler approach.", "tokens": [1743, 436, 632, 257, 709, 18587, 3109, 13], "temperature": 0.0, "avg_logprob": -0.12834620084918913, "compression_ratio": 1.7236842105263157, "no_speech_prob": 9.422402058589796e-07}, {"id": 406, "seek": 241728, "start": 2432.1200000000003, "end": 2441.42, "text": " And one of the interesting things they published a paper about their approach.", "tokens": [400, 472, 295, 264, 1880, 721, 436, 6572, 257, 3035, 466, 641, 3109, 13], "temperature": 0.0, "avg_logprob": -0.12834620084918913, "compression_ratio": 1.7236842105263157, "no_speech_prob": 9.422402058589796e-07}, {"id": 407, "seek": 244142, "start": 2441.42, "end": 2453.0, "text": " So this is the team from this company.", "tokens": [407, 341, 307, 264, 1469, 490, 341, 2237, 13], "temperature": 0.0, "avg_logprob": -0.11210794706602355, "compression_ratio": 1.6091370558375635, "no_speech_prob": 8.714293358025316e-07}, {"id": 408, "seek": 244142, "start": 2453.0, "end": 2456.96, "text": " And they basically describe here exactly what I just showed you these different embedding", "tokens": [400, 436, 1936, 6786, 510, 2293, 437, 286, 445, 4712, 291, 613, 819, 12240, 3584], "temperature": 0.0, "avg_logprob": -0.11210794706602355, "compression_ratio": 1.6091370558375635, "no_speech_prob": 8.714293358025316e-07}, {"id": 409, "seek": 244142, "start": 2456.96, "end": 2460.88, "text": " layers being concatenated together and then going through a couple of layers of a neural", "tokens": [7914, 885, 1588, 7186, 770, 1214, 293, 550, 516, 807, 257, 1916, 295, 7914, 295, 257, 18161], "temperature": 0.0, "avg_logprob": -0.11210794706602355, "compression_ratio": 1.6091370558375635, "no_speech_prob": 8.714293358025316e-07}, {"id": 410, "seek": 244142, "start": 2460.88, "end": 2463.7400000000002, "text": " network.", "tokens": [3209, 13], "temperature": 0.0, "avg_logprob": -0.11210794706602355, "compression_ratio": 1.6091370558375635, "no_speech_prob": 8.714293358025316e-07}, {"id": 411, "seek": 244142, "start": 2463.7400000000002, "end": 2467.56, "text": " And it's showing here it points out in the paper exactly what we learnt in the last lesson", "tokens": [400, 309, 311, 4099, 510, 309, 2793, 484, 294, 264, 3035, 2293, 437, 321, 18991, 294, 264, 1036, 6898], "temperature": 0.0, "avg_logprob": -0.11210794706602355, "compression_ratio": 1.6091370558375635, "no_speech_prob": 8.714293358025316e-07}, {"id": 412, "seek": 246756, "start": 2467.56, "end": 2473.24, "text": " which is embedding layers are exactly equivalent to linear layers on top of a one hot encoded", "tokens": [597, 307, 12240, 3584, 7914, 366, 2293, 10344, 281, 8213, 7914, 322, 1192, 295, 257, 472, 2368, 2058, 12340], "temperature": 0.0, "avg_logprob": -0.12731873989105225, "compression_ratio": 1.6844660194174756, "no_speech_prob": 2.4060736905084923e-06}, {"id": 413, "seek": 246756, "start": 2473.24, "end": 2478.08, "text": " input.", "tokens": [4846, 13], "temperature": 0.0, "avg_logprob": -0.12731873989105225, "compression_ratio": 1.6844660194174756, "no_speech_prob": 2.4060736905084923e-06}, {"id": 414, "seek": 246756, "start": 2478.08, "end": 2484.54, "text": " And yeah they found that their technique worked really well.", "tokens": [400, 1338, 436, 1352, 300, 641, 6532, 2732, 534, 731, 13], "temperature": 0.0, "avg_logprob": -0.12731873989105225, "compression_ratio": 1.6844660194174756, "no_speech_prob": 2.4060736905084923e-06}, {"id": 415, "seek": 246756, "start": 2484.54, "end": 2490.2, "text": " One of the interesting things they also showed is that you can take you can create your neural", "tokens": [1485, 295, 264, 1880, 721, 436, 611, 4712, 307, 300, 291, 393, 747, 291, 393, 1884, 428, 18161], "temperature": 0.0, "avg_logprob": -0.12731873989105225, "compression_ratio": 1.6844660194174756, "no_speech_prob": 2.4060736905084923e-06}, {"id": 416, "seek": 246756, "start": 2490.2, "end": 2496.52, "text": " net get your trained embeddings and then you can put those embeddings into a random forest", "tokens": [2533, 483, 428, 8895, 12240, 29432, 293, 550, 291, 393, 829, 729, 12240, 29432, 666, 257, 4974, 6719], "temperature": 0.0, "avg_logprob": -0.12731873989105225, "compression_ratio": 1.6844660194174756, "no_speech_prob": 2.4060736905084923e-06}, {"id": 417, "seek": 249652, "start": 2496.52, "end": 2503.7599999999998, "text": " or gradient boosted tree and your mean average percent error will dramatically improve.", "tokens": [420, 16235, 9194, 292, 4230, 293, 428, 914, 4274, 3043, 6713, 486, 17548, 3470, 13], "temperature": 0.0, "avg_logprob": -0.11307867561898581, "compression_ratio": 1.7314814814814814, "no_speech_prob": 3.611942474890384e-06}, {"id": 418, "seek": 249652, "start": 2503.7599999999998, "end": 2510.92, "text": " So you can actually combine random forests and embeddings or gradient boosted trees and", "tokens": [407, 291, 393, 767, 10432, 4974, 21700, 293, 12240, 29432, 420, 16235, 9194, 292, 5852, 293], "temperature": 0.0, "avg_logprob": -0.11307867561898581, "compression_ratio": 1.7314814814814814, "no_speech_prob": 3.611942474890384e-06}, {"id": 419, "seek": 249652, "start": 2510.92, "end": 2513.98, "text": " embeddings which is really interesting.", "tokens": [12240, 29432, 597, 307, 534, 1880, 13], "temperature": 0.0, "avg_logprob": -0.11307867561898581, "compression_ratio": 1.7314814814814814, "no_speech_prob": 3.611942474890384e-06}, {"id": 420, "seek": 249652, "start": 2513.98, "end": 2516.96, "text": " Now what I really wanted to show you though is what they then did.", "tokens": [823, 437, 286, 534, 1415, 281, 855, 291, 1673, 307, 437, 436, 550, 630, 13], "temperature": 0.0, "avg_logprob": -0.11307867561898581, "compression_ratio": 1.7314814814814814, "no_speech_prob": 3.611942474890384e-06}, {"id": 421, "seek": 249652, "start": 2516.96, "end": 2523.0, "text": " So as I said this was a thing about the predicted amount that different products would sell", "tokens": [407, 382, 286, 848, 341, 390, 257, 551, 466, 264, 19147, 2372, 300, 819, 3383, 576, 3607], "temperature": 0.0, "avg_logprob": -0.11307867561898581, "compression_ratio": 1.7314814814814814, "no_speech_prob": 3.611942474890384e-06}, {"id": 422, "seek": 252300, "start": 2523.0, "end": 2527.6, "text": " for at different shops around Germany.", "tokens": [337, 412, 819, 14457, 926, 7244, 13], "temperature": 0.0, "avg_logprob": -0.16136186334151256, "compression_ratio": 1.7025641025641025, "no_speech_prob": 2.642567551447428e-06}, {"id": 423, "seek": 252300, "start": 2527.6, "end": 2533.88, "text": " And what they did was they had a so one of their embedding matrices was embeddings by", "tokens": [400, 437, 436, 630, 390, 436, 632, 257, 370, 472, 295, 641, 12240, 3584, 32284, 390, 12240, 29432, 538], "temperature": 0.0, "avg_logprob": -0.16136186334151256, "compression_ratio": 1.7025641025641025, "no_speech_prob": 2.642567551447428e-06}, {"id": 424, "seek": 252300, "start": 2533.88, "end": 2542.04, "text": " region and then they did I think this is a PCA principal component analysis of the embeddings", "tokens": [4458, 293, 550, 436, 630, 286, 519, 341, 307, 257, 6465, 32, 9716, 6542, 5215, 295, 264, 12240, 29432], "temperature": 0.0, "avg_logprob": -0.16136186334151256, "compression_ratio": 1.7025641025641025, "no_speech_prob": 2.642567551447428e-06}, {"id": 425, "seek": 252300, "start": 2542.04, "end": 2544.72, "text": " for their German regions.", "tokens": [337, 641, 6521, 10682, 13], "temperature": 0.0, "avg_logprob": -0.16136186334151256, "compression_ratio": 1.7025641025641025, "no_speech_prob": 2.642567551447428e-06}, {"id": 426, "seek": 252300, "start": 2544.72, "end": 2552.4, "text": " And when they create a chart of them you can see that the locations that close together", "tokens": [400, 562, 436, 1884, 257, 6927, 295, 552, 291, 393, 536, 300, 264, 9253, 300, 1998, 1214], "temperature": 0.0, "avg_logprob": -0.16136186334151256, "compression_ratio": 1.7025641025641025, "no_speech_prob": 2.642567551447428e-06}, {"id": 427, "seek": 255240, "start": 2552.4, "end": 2558.7200000000003, "text": " in the embedding matrix are the same locations that are close together in Germany.", "tokens": [294, 264, 12240, 3584, 8141, 366, 264, 912, 9253, 300, 366, 1998, 1214, 294, 7244, 13], "temperature": 0.0, "avg_logprob": -0.07591154461815244, "compression_ratio": 1.748792270531401, "no_speech_prob": 2.3687862267252058e-06}, {"id": 428, "seek": 255240, "start": 2558.7200000000003, "end": 2562.12, "text": " So you can see here's the blue ones and here's the blue ones.", "tokens": [407, 291, 393, 536, 510, 311, 264, 3344, 2306, 293, 510, 311, 264, 3344, 2306, 13], "temperature": 0.0, "avg_logprob": -0.07591154461815244, "compression_ratio": 1.748792270531401, "no_speech_prob": 2.3687862267252058e-06}, {"id": 429, "seek": 255240, "start": 2562.12, "end": 2567.08, "text": " And again it's important to recognize that the data that they used had no information", "tokens": [400, 797, 309, 311, 1021, 281, 5521, 300, 264, 1412, 300, 436, 1143, 632, 572, 1589], "temperature": 0.0, "avg_logprob": -0.07591154461815244, "compression_ratio": 1.748792270531401, "no_speech_prob": 2.3687862267252058e-06}, {"id": 430, "seek": 255240, "start": 2567.08, "end": 2570.28, "text": " about the location of these places.", "tokens": [466, 264, 4914, 295, 613, 3190, 13], "temperature": 0.0, "avg_logprob": -0.07591154461815244, "compression_ratio": 1.748792270531401, "no_speech_prob": 2.3687862267252058e-06}, {"id": 431, "seek": 255240, "start": 2570.28, "end": 2577.3, "text": " The fact that they are close together geographically is something that was figured out as being", "tokens": [440, 1186, 300, 436, 366, 1998, 1214, 25435, 984, 307, 746, 300, 390, 8932, 484, 382, 885], "temperature": 0.0, "avg_logprob": -0.07591154461815244, "compression_ratio": 1.748792270531401, "no_speech_prob": 2.3687862267252058e-06}, {"id": 432, "seek": 257730, "start": 2577.3, "end": 2583.8, "text": " something that actually helped it to predict sales.", "tokens": [746, 300, 767, 4254, 309, 281, 6069, 5763, 13], "temperature": 0.0, "avg_logprob": -0.11558495498285061, "compression_ratio": 1.6648936170212767, "no_speech_prob": 8.714322348168935e-07}, {"id": 433, "seek": 257730, "start": 2583.8, "end": 2590.1600000000003, "text": " And so in fact they then did a plot showing each of these dots is a shop, a store, and", "tokens": [400, 370, 294, 1186, 436, 550, 630, 257, 7542, 4099, 1184, 295, 613, 15026, 307, 257, 3945, 11, 257, 3531, 11, 293], "temperature": 0.0, "avg_logprob": -0.11558495498285061, "compression_ratio": 1.6648936170212767, "no_speech_prob": 8.714322348168935e-07}, {"id": 434, "seek": 257730, "start": 2590.1600000000003, "end": 2600.2400000000002, "text": " it's showing for each pair of stores how far away is it in real life in metric space and", "tokens": [309, 311, 4099, 337, 1184, 6119, 295, 9512, 577, 1400, 1314, 307, 309, 294, 957, 993, 294, 20678, 1901, 293], "temperature": 0.0, "avg_logprob": -0.11558495498285061, "compression_ratio": 1.6648936170212767, "no_speech_prob": 8.714322348168935e-07}, {"id": 435, "seek": 257730, "start": 2600.2400000000002, "end": 2604.48, "text": " then how far away is it in embedding space.", "tokens": [550, 577, 1400, 1314, 307, 309, 294, 12240, 3584, 1901, 13], "temperature": 0.0, "avg_logprob": -0.11558495498285061, "compression_ratio": 1.6648936170212767, "no_speech_prob": 8.714322348168935e-07}, {"id": 436, "seek": 257730, "start": 2604.48, "end": 2606.94, "text": " And there's this very strong correlation.", "tokens": [400, 456, 311, 341, 588, 2068, 20009, 13], "temperature": 0.0, "avg_logprob": -0.11558495498285061, "compression_ratio": 1.6648936170212767, "no_speech_prob": 8.714322348168935e-07}, {"id": 437, "seek": 260694, "start": 2606.94, "end": 2615.92, "text": " So it's kind of reconstructed somehow the geography of Germany by figuring out how people", "tokens": [407, 309, 311, 733, 295, 31499, 292, 6063, 264, 26695, 295, 7244, 538, 15213, 484, 577, 561], "temperature": 0.0, "avg_logprob": -0.1414945125579834, "compression_ratio": 1.8169642857142858, "no_speech_prob": 2.7693799893313553e-06}, {"id": 438, "seek": 260694, "start": 2615.92, "end": 2620.6, "text": " shop and similar for days of the week.", "tokens": [3945, 293, 2531, 337, 1708, 295, 264, 1243, 13], "temperature": 0.0, "avg_logprob": -0.1414945125579834, "compression_ratio": 1.8169642857142858, "no_speech_prob": 2.7693799893313553e-06}, {"id": 439, "seek": 260694, "start": 2620.6, "end": 2625.12, "text": " So there was no information really about days of the week but when they put it on the embedding", "tokens": [407, 456, 390, 572, 1589, 534, 466, 1708, 295, 264, 1243, 457, 562, 436, 829, 309, 322, 264, 12240, 3584], "temperature": 0.0, "avg_logprob": -0.1414945125579834, "compression_ratio": 1.8169642857142858, "no_speech_prob": 2.7693799893313553e-06}, {"id": 440, "seek": 260694, "start": 2625.12, "end": 2630.84, "text": " matrix the days of the week, Monday, Tuesday, Wednesday, close to each other, Thursday, Friday,", "tokens": [8141, 264, 1708, 295, 264, 1243, 11, 8138, 11, 10017, 11, 10579, 11, 1998, 281, 1184, 661, 11, 10383, 11, 6984, 11], "temperature": 0.0, "avg_logprob": -0.1414945125579834, "compression_ratio": 1.8169642857142858, "no_speech_prob": 2.7693799893313553e-06}, {"id": 441, "seek": 260694, "start": 2630.84, "end": 2634.96, "text": " close to each other, as you can see Saturday and Sunday close to each other, and ditto", "tokens": [1998, 281, 1184, 661, 11, 382, 291, 393, 536, 8803, 293, 7776, 1998, 281, 1184, 661, 11, 293, 274, 34924], "temperature": 0.0, "avg_logprob": -0.1414945125579834, "compression_ratio": 1.8169642857142858, "no_speech_prob": 2.7693799893313553e-06}, {"id": 442, "seek": 263496, "start": 2634.96, "end": 2639.8, "text": " for months of the year, January, February, March, April, May, June.", "tokens": [337, 2493, 295, 264, 1064, 11, 7061, 11, 8711, 11, 6129, 11, 6929, 11, 1891, 11, 6928, 13], "temperature": 0.0, "avg_logprob": -0.22516180674235026, "compression_ratio": 1.2330827067669172, "no_speech_prob": 1.321155832556542e-05}, {"id": 443, "seek": 263496, "start": 2639.8, "end": 2646.52, "text": " So yeah really interesting cool stuff I think.", "tokens": [407, 1338, 534, 1880, 1627, 1507, 286, 519, 13], "temperature": 0.0, "avg_logprob": -0.22516180674235026, "compression_ratio": 1.2330827067669172, "no_speech_prob": 1.321155832556542e-05}, {"id": 444, "seek": 263496, "start": 2646.52, "end": 2664.8, "text": " What's actually going on inside a neural network?", "tokens": [708, 311, 767, 516, 322, 1854, 257, 18161, 3209, 30], "temperature": 0.0, "avg_logprob": -0.22516180674235026, "compression_ratio": 1.2330827067669172, "no_speech_prob": 1.321155832556542e-05}, {"id": 445, "seek": 266480, "start": 2664.8, "end": 2674.2400000000002, "text": " Alright let's take a 10 minute break and I will see you back here at 7.10.", "tokens": [2798, 718, 311, 747, 257, 1266, 3456, 1821, 293, 286, 486, 536, 291, 646, 510, 412, 1614, 13, 3279, 13], "temperature": 0.0, "avg_logprob": -0.18286747932434083, "compression_ratio": 1.375796178343949, "no_speech_prob": 1.9947199234593427e-06}, {"id": 446, "seek": 266480, "start": 2674.2400000000002, "end": 2681.9, "text": " Alright folks this is something I think is really fun which is we're going to we've looked", "tokens": [2798, 4024, 341, 307, 746, 286, 519, 307, 534, 1019, 597, 307, 321, 434, 516, 281, 321, 600, 2956], "temperature": 0.0, "avg_logprob": -0.18286747932434083, "compression_ratio": 1.375796178343949, "no_speech_prob": 1.9947199234593427e-06}, {"id": 447, "seek": 266480, "start": 2681.9, "end": 2689.2400000000002, "text": " at what goes into the start of a model, the input.", "tokens": [412, 437, 1709, 666, 264, 722, 295, 257, 2316, 11, 264, 4846, 13], "temperature": 0.0, "avg_logprob": -0.18286747932434083, "compression_ratio": 1.375796178343949, "no_speech_prob": 1.9947199234593427e-06}, {"id": 448, "seek": 268924, "start": 2689.24, "end": 2694.9199999999996, "text": " We've learned about how they can be categories or embeddings and embeddings are basically", "tokens": [492, 600, 3264, 466, 577, 436, 393, 312, 10479, 420, 12240, 29432, 293, 12240, 29432, 366, 1936], "temperature": 0.0, "avg_logprob": -0.14206951491686762, "compression_ratio": 1.7533039647577093, "no_speech_prob": 5.142717054695822e-05}, {"id": 449, "seek": 268924, "start": 2694.9199999999996, "end": 2700.16, "text": " kind of one hot encoded categories with a little compute trick or they can just be continuous", "tokens": [733, 295, 472, 2368, 2058, 12340, 10479, 365, 257, 707, 14722, 4282, 420, 436, 393, 445, 312, 10957], "temperature": 0.0, "avg_logprob": -0.14206951491686762, "compression_ratio": 1.7533039647577093, "no_speech_prob": 5.142717054695822e-05}, {"id": 450, "seek": 268924, "start": 2700.16, "end": 2701.3799999999997, "text": " numbers.", "tokens": [3547, 13], "temperature": 0.0, "avg_logprob": -0.14206951491686762, "compression_ratio": 1.7533039647577093, "no_speech_prob": 5.142717054695822e-05}, {"id": 451, "seek": 268924, "start": 2701.3799999999997, "end": 2705.7999999999997, "text": " We've learned about what comes out the other side which is a bunch of activations, so just", "tokens": [492, 600, 3264, 466, 437, 1487, 484, 264, 661, 1252, 597, 307, 257, 3840, 295, 2430, 763, 11, 370, 445], "temperature": 0.0, "avg_logprob": -0.14206951491686762, "compression_ratio": 1.7533039647577093, "no_speech_prob": 5.142717054695822e-05}, {"id": 452, "seek": 268924, "start": 2705.7999999999997, "end": 2712.52, "text": " a bunch of tensor of numbers which we can use things like softmax to constrain them", "tokens": [257, 3840, 295, 40863, 295, 3547, 597, 321, 393, 764, 721, 411, 2787, 41167, 281, 1817, 7146, 552], "temperature": 0.0, "avg_logprob": -0.14206951491686762, "compression_ratio": 1.7533039647577093, "no_speech_prob": 5.142717054695822e-05}, {"id": 453, "seek": 268924, "start": 2712.52, "end": 2719.0, "text": " to add up to one and so forth.", "tokens": [281, 909, 493, 281, 472, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.14206951491686762, "compression_ratio": 1.7533039647577093, "no_speech_prob": 5.142717054695822e-05}, {"id": 454, "seek": 271900, "start": 2719.0, "end": 2727.28, "text": " And we've looked at what can go in the middle which is the matrix modellplies sandwiched", "tokens": [400, 321, 600, 2956, 412, 437, 393, 352, 294, 264, 2808, 597, 307, 264, 8141, 1072, 898, 564, 530, 11141, 292], "temperature": 0.0, "avg_logprob": -0.12831417349881905, "compression_ratio": 1.6713615023474178, "no_speech_prob": 1.3419784409052227e-05}, {"id": 455, "seek": 271900, "start": 2727.28, "end": 2732.56, "text": " together as rectified linear units.", "tokens": [1214, 382, 11048, 2587, 8213, 6815, 13], "temperature": 0.0, "avg_logprob": -0.12831417349881905, "compression_ratio": 1.6713615023474178, "no_speech_prob": 1.3419784409052227e-05}, {"id": 456, "seek": 271900, "start": 2732.56, "end": 2737.28, "text": " And I mentioned that there are other things that can go in the middle as well but we haven't", "tokens": [400, 286, 2835, 300, 456, 366, 661, 721, 300, 393, 352, 294, 264, 2808, 382, 731, 457, 321, 2378, 380], "temperature": 0.0, "avg_logprob": -0.12831417349881905, "compression_ratio": 1.6713615023474178, "no_speech_prob": 1.3419784409052227e-05}, {"id": 457, "seek": 271900, "start": 2737.28, "end": 2741.36, "text": " really talked about what those other things are.", "tokens": [534, 2825, 466, 437, 729, 661, 721, 366, 13], "temperature": 0.0, "avg_logprob": -0.12831417349881905, "compression_ratio": 1.6713615023474178, "no_speech_prob": 1.3419784409052227e-05}, {"id": 458, "seek": 271900, "start": 2741.36, "end": 2746.52, "text": " So I thought we might look at one of the most important and interesting version of things", "tokens": [407, 286, 1194, 321, 1062, 574, 412, 472, 295, 264, 881, 1021, 293, 1880, 3037, 295, 721], "temperature": 0.0, "avg_logprob": -0.12831417349881905, "compression_ratio": 1.6713615023474178, "no_speech_prob": 1.3419784409052227e-05}, {"id": 459, "seek": 274652, "start": 2746.52, "end": 2748.92, "text": " that can go in the middle.", "tokens": [300, 393, 352, 294, 264, 2808, 13], "temperature": 0.0, "avg_logprob": -0.10580516961904672, "compression_ratio": 1.8392857142857142, "no_speech_prob": 4.936950972478371e-06}, {"id": 460, "seek": 274652, "start": 2748.92, "end": 2754.24, "text": " But what you'll see is it turns out it's actually just another kind of matrix modellplication", "tokens": [583, 437, 291, 603, 536, 307, 309, 4523, 484, 309, 311, 767, 445, 1071, 733, 295, 8141, 1072, 898, 4770, 399], "temperature": 0.0, "avg_logprob": -0.10580516961904672, "compression_ratio": 1.8392857142857142, "no_speech_prob": 4.936950972478371e-06}, {"id": 461, "seek": 274652, "start": 2754.24, "end": 2756.88, "text": " which might not be obvious at first but I'll explain.", "tokens": [597, 1062, 406, 312, 6322, 412, 700, 457, 286, 603, 2903, 13], "temperature": 0.0, "avg_logprob": -0.10580516961904672, "compression_ratio": 1.8392857142857142, "no_speech_prob": 4.936950972478371e-06}, {"id": 462, "seek": 274652, "start": 2756.88, "end": 2760.68, "text": " We're going to look at something called a convolution and convolutions are at the heart", "tokens": [492, 434, 516, 281, 574, 412, 746, 1219, 257, 45216, 293, 3754, 15892, 366, 412, 264, 1917], "temperature": 0.0, "avg_logprob": -0.10580516961904672, "compression_ratio": 1.8392857142857142, "no_speech_prob": 4.936950972478371e-06}, {"id": 463, "seek": 274652, "start": 2760.68, "end": 2762.94, "text": " of a convolutional neural network.", "tokens": [295, 257, 45216, 304, 18161, 3209, 13], "temperature": 0.0, "avg_logprob": -0.10580516961904672, "compression_ratio": 1.8392857142857142, "no_speech_prob": 4.936950972478371e-06}, {"id": 464, "seek": 274652, "start": 2762.94, "end": 2767.32, "text": " So the first thing to realise is a convolutional neural network is very very very similar to", "tokens": [407, 264, 700, 551, 281, 18809, 307, 257, 45216, 304, 18161, 3209, 307, 588, 588, 588, 2531, 281], "temperature": 0.0, "avg_logprob": -0.10580516961904672, "compression_ratio": 1.8392857142857142, "no_speech_prob": 4.936950972478371e-06}, {"id": 465, "seek": 274652, "start": 2767.32, "end": 2769.68, "text": " the neural networks we've seen so far.", "tokens": [264, 18161, 9590, 321, 600, 1612, 370, 1400, 13], "temperature": 0.0, "avg_logprob": -0.10580516961904672, "compression_ratio": 1.8392857142857142, "no_speech_prob": 4.936950972478371e-06}, {"id": 466, "seek": 274652, "start": 2769.68, "end": 2774.04, "text": " It's got inputs, it's got things that are a lot like or actually are a form of matrix", "tokens": [467, 311, 658, 15743, 11, 309, 311, 658, 721, 300, 366, 257, 688, 411, 420, 767, 366, 257, 1254, 295, 8141], "temperature": 0.0, "avg_logprob": -0.10580516961904672, "compression_ratio": 1.8392857142857142, "no_speech_prob": 4.936950972478371e-06}, {"id": 467, "seek": 277404, "start": 2774.04, "end": 2781.6, "text": " modellplication sandwiched with activation functions which can be rectified linear.", "tokens": [1072, 898, 4770, 399, 11141, 292, 365, 24433, 6828, 597, 393, 312, 11048, 2587, 8213, 13], "temperature": 0.0, "avg_logprob": -0.1233554067490976, "compression_ratio": 1.4788732394366197, "no_speech_prob": 2.9479929253284354e-06}, {"id": 468, "seek": 277404, "start": 2781.6, "end": 2786.32, "text": " But there's a particular thing which makes them very useful for computer vision.", "tokens": [583, 456, 311, 257, 1729, 551, 597, 1669, 552, 588, 4420, 337, 3820, 5201, 13], "temperature": 0.0, "avg_logprob": -0.1233554067490976, "compression_ratio": 1.4788732394366197, "no_speech_prob": 2.9479929253284354e-06}, {"id": 469, "seek": 277404, "start": 2786.32, "end": 2795.32, "text": " And I'm going to show you using this excel spreadsheet that's in our repo called convexample.", "tokens": [400, 286, 478, 516, 281, 855, 291, 1228, 341, 24015, 27733, 300, 311, 294, 527, 49040, 1219, 3754, 3121, 335, 781, 13], "temperature": 0.0, "avg_logprob": -0.1233554067490976, "compression_ratio": 1.4788732394366197, "no_speech_prob": 2.9479929253284354e-06}, {"id": 470, "seek": 277404, "start": 2795.32, "end": 2798.82, "text": " And we're going to look at it using an image from MNIST.", "tokens": [400, 321, 434, 516, 281, 574, 412, 309, 1228, 364, 3256, 490, 376, 45, 19756, 13], "temperature": 0.0, "avg_logprob": -0.1233554067490976, "compression_ratio": 1.4788732394366197, "no_speech_prob": 2.9479929253284354e-06}, {"id": 471, "seek": 279882, "start": 2798.82, "end": 2805.2000000000003, "text": " So MNIST is kind of the world's most famous computer vision data set I think because it", "tokens": [407, 376, 45, 19756, 307, 733, 295, 264, 1002, 311, 881, 4618, 3820, 5201, 1412, 992, 286, 519, 570, 309], "temperature": 0.0, "avg_logprob": -0.1509802962002689, "compression_ratio": 1.4183673469387754, "no_speech_prob": 2.2252288545132615e-06}, {"id": 472, "seek": 279882, "start": 2805.2000000000003, "end": 2812.84, "text": " was like the first one really which really showed image recognition being cracked.", "tokens": [390, 411, 264, 700, 472, 534, 597, 534, 4712, 3256, 11150, 885, 25140, 13], "temperature": 0.0, "avg_logprob": -0.1509802962002689, "compression_ratio": 1.4183673469387754, "no_speech_prob": 2.2252288545132615e-06}, {"id": 473, "seek": 279882, "start": 2812.84, "end": 2815.04, "text": " It's pretty small by today's standards.", "tokens": [467, 311, 1238, 1359, 538, 965, 311, 7787, 13], "temperature": 0.0, "avg_logprob": -0.1509802962002689, "compression_ratio": 1.4183673469387754, "no_speech_prob": 2.2252288545132615e-06}, {"id": 474, "seek": 279882, "start": 2815.04, "end": 2817.76, "text": " It's a data set of handwritten digits.", "tokens": [467, 311, 257, 1412, 992, 295, 1011, 26859, 27011, 13], "temperature": 0.0, "avg_logprob": -0.1509802962002689, "compression_ratio": 1.4183673469387754, "no_speech_prob": 2.2252288545132615e-06}, {"id": 475, "seek": 279882, "start": 2817.76, "end": 2822.04, "text": " Each one is 28 by 28 pixels.", "tokens": [6947, 472, 307, 7562, 538, 7562, 18668, 13], "temperature": 0.0, "avg_logprob": -0.1509802962002689, "compression_ratio": 1.4183673469387754, "no_speech_prob": 2.2252288545132615e-06}, {"id": 476, "seek": 282204, "start": 2822.04, "end": 2830.16, "text": " And yeah, back in the mid-90s Jan Likun showed really practically useful performance on this", "tokens": [400, 1338, 11, 646, 294, 264, 2062, 12, 7771, 82, 4956, 441, 1035, 409, 4712, 534, 15667, 4420, 3389, 322, 341], "temperature": 0.0, "avg_logprob": -0.2284766605922154, "compression_ratio": 1.5336134453781514, "no_speech_prob": 1.6280091585940681e-06}, {"id": 477, "seek": 282204, "start": 2830.16, "end": 2837.88, "text": " data set and as a result ended up with conv nets being used in the American banking system", "tokens": [1412, 992, 293, 382, 257, 1874, 4590, 493, 365, 3754, 36170, 885, 1143, 294, 264, 2665, 18261, 1185], "temperature": 0.0, "avg_logprob": -0.2284766605922154, "compression_ratio": 1.5336134453781514, "no_speech_prob": 1.6280091585940681e-06}, {"id": 478, "seek": 282204, "start": 2837.88, "end": 2838.88, "text": " for reading checks.", "tokens": [337, 3760, 13834, 13], "temperature": 0.0, "avg_logprob": -0.2284766605922154, "compression_ratio": 1.5336134453781514, "no_speech_prob": 1.6280091585940681e-06}, {"id": 479, "seek": 282204, "start": 2838.88, "end": 2841.84, "text": " So here's an example of one of those digits.", "tokens": [407, 510, 311, 364, 1365, 295, 472, 295, 729, 27011, 13], "temperature": 0.0, "avg_logprob": -0.2284766605922154, "compression_ratio": 1.5336134453781514, "no_speech_prob": 1.6280091585940681e-06}, {"id": 480, "seek": 282204, "start": 2841.84, "end": 2843.2799999999997, "text": " This is a seven that somebody drew.", "tokens": [639, 307, 257, 3407, 300, 2618, 12804, 13], "temperature": 0.0, "avg_logprob": -0.2284766605922154, "compression_ratio": 1.5336134453781514, "no_speech_prob": 1.6280091585940681e-06}, {"id": 481, "seek": 282204, "start": 2843.2799999999997, "end": 2846.36, "text": " It's one of those ones with a stroke through it.", "tokens": [467, 311, 472, 295, 729, 2306, 365, 257, 12403, 807, 309, 13], "temperature": 0.0, "avg_logprob": -0.2284766605922154, "compression_ratio": 1.5336134453781514, "no_speech_prob": 1.6280091585940681e-06}, {"id": 482, "seek": 282204, "start": 2846.36, "end": 2849.6, "text": " And this is what it looks like.", "tokens": [400, 341, 307, 437, 309, 1542, 411, 13], "temperature": 0.0, "avg_logprob": -0.2284766605922154, "compression_ratio": 1.5336134453781514, "no_speech_prob": 1.6280091585940681e-06}, {"id": 483, "seek": 284960, "start": 2849.6, "end": 2852.04, "text": " This is the image.", "tokens": [639, 307, 264, 3256, 13], "temperature": 0.0, "avg_logprob": -0.10566179375899465, "compression_ratio": 1.6964285714285714, "no_speech_prob": 1.788059989848989e-06}, {"id": 484, "seek": 284960, "start": 2852.04, "end": 2853.04, "text": " And so I got it from MNIST.", "tokens": [400, 370, 286, 658, 309, 490, 376, 45, 19756, 13], "temperature": 0.0, "avg_logprob": -0.10566179375899465, "compression_ratio": 1.6964285714285714, "no_speech_prob": 1.788059989848989e-06}, {"id": 485, "seek": 284960, "start": 2853.04, "end": 2858.96, "text": " This is just one of the images from MNIST which I put into Excel.", "tokens": [639, 307, 445, 472, 295, 264, 5267, 490, 376, 45, 19756, 597, 286, 829, 666, 19060, 13], "temperature": 0.0, "avg_logprob": -0.10566179375899465, "compression_ratio": 1.6964285714285714, "no_speech_prob": 1.788059989848989e-06}, {"id": 486, "seek": 284960, "start": 2858.96, "end": 2872.24, "text": " And what you see in the next column is a version of the image where the horizontal lines are", "tokens": [400, 437, 291, 536, 294, 264, 958, 7738, 307, 257, 3037, 295, 264, 3256, 689, 264, 12750, 3876, 366], "temperature": 0.0, "avg_logprob": -0.10566179375899465, "compression_ratio": 1.6964285714285714, "no_speech_prob": 1.788059989848989e-06}, {"id": 487, "seek": 284960, "start": 2872.24, "end": 2878.16, "text": " being recognized and another one where the vertical lines are being recognized.", "tokens": [885, 9823, 293, 1071, 472, 689, 264, 9429, 3876, 366, 885, 9823, 13], "temperature": 0.0, "avg_logprob": -0.10566179375899465, "compression_ratio": 1.6964285714285714, "no_speech_prob": 1.788059989848989e-06}, {"id": 488, "seek": 287816, "start": 2878.16, "end": 2882.0, "text": " And if you think back to that Zyler and Fergus paper that talked about what the layers of", "tokens": [400, 498, 291, 519, 646, 281, 300, 1176, 88, 1918, 293, 36790, 3035, 300, 2825, 466, 437, 264, 7914, 295], "temperature": 0.0, "avg_logprob": -0.1650754755193537, "compression_ratio": 1.8333333333333333, "no_speech_prob": 4.785048531630309e-06}, {"id": 489, "seek": 287816, "start": 2882.0, "end": 2888.68, "text": " a neural net does, this is absolutely an example of something that we know that the first layer", "tokens": [257, 18161, 2533, 775, 11, 341, 307, 3122, 364, 1365, 295, 746, 300, 321, 458, 300, 264, 700, 4583], "temperature": 0.0, "avg_logprob": -0.1650754755193537, "compression_ratio": 1.8333333333333333, "no_speech_prob": 4.785048531630309e-06}, {"id": 490, "seek": 287816, "start": 2888.68, "end": 2891.7999999999997, "text": " of a neural network tends to learn how to do.", "tokens": [295, 257, 18161, 3209, 12258, 281, 1466, 577, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.1650754755193537, "compression_ratio": 1.8333333333333333, "no_speech_prob": 4.785048531630309e-06}, {"id": 491, "seek": 287816, "start": 2891.7999999999997, "end": 2894.48, "text": " Now how did I do this?", "tokens": [823, 577, 630, 286, 360, 341, 30], "temperature": 0.0, "avg_logprob": -0.1650754755193537, "compression_ratio": 1.8333333333333333, "no_speech_prob": 4.785048531630309e-06}, {"id": 492, "seek": 287816, "start": 2894.48, "end": 2897.52, "text": " I did this using something called a convolution.", "tokens": [286, 630, 341, 1228, 746, 1219, 257, 45216, 13], "temperature": 0.0, "avg_logprob": -0.1650754755193537, "compression_ratio": 1.8333333333333333, "no_speech_prob": 4.785048531630309e-06}, {"id": 493, "seek": 287816, "start": 2897.52, "end": 2901.68, "text": " And so what we're going to do now is we're going to zoom in to this Excel notebook.", "tokens": [400, 370, 437, 321, 434, 516, 281, 360, 586, 307, 321, 434, 516, 281, 8863, 294, 281, 341, 19060, 21060, 13], "temperature": 0.0, "avg_logprob": -0.1650754755193537, "compression_ratio": 1.8333333333333333, "no_speech_prob": 4.785048531630309e-06}, {"id": 494, "seek": 287816, "start": 2901.68, "end": 2904.52, "text": " We're going to keep zooming in.", "tokens": [492, 434, 516, 281, 1066, 48226, 294, 13], "temperature": 0.0, "avg_logprob": -0.1650754755193537, "compression_ratio": 1.8333333333333333, "no_speech_prob": 4.785048531630309e-06}, {"id": 495, "seek": 287816, "start": 2904.52, "end": 2906.08, "text": " We're going to keep zooming in.", "tokens": [492, 434, 516, 281, 1066, 48226, 294, 13], "temperature": 0.0, "avg_logprob": -0.1650754755193537, "compression_ratio": 1.8333333333333333, "no_speech_prob": 4.785048531630309e-06}, {"id": 496, "seek": 290608, "start": 2906.08, "end": 2911.16, "text": " So take a look, keep an eye on this image and you'll see that once we zoom in enough", "tokens": [407, 747, 257, 574, 11, 1066, 364, 3313, 322, 341, 3256, 293, 291, 603, 536, 300, 1564, 321, 8863, 294, 1547], "temperature": 0.0, "avg_logprob": -0.11511935544817635, "compression_ratio": 1.5336538461538463, "no_speech_prob": 1.8162173773816903e-06}, {"id": 497, "seek": 290608, "start": 2911.16, "end": 2921.6, "text": " it's actually just made of numbers which as we discussed in the very first lesson we saw", "tokens": [309, 311, 767, 445, 1027, 295, 3547, 597, 382, 321, 7152, 294, 264, 588, 700, 6898, 321, 1866], "temperature": 0.0, "avg_logprob": -0.11511935544817635, "compression_ratio": 1.5336538461538463, "no_speech_prob": 1.8162173773816903e-06}, {"id": 498, "seek": 290608, "start": 2921.6, "end": 2926.0, "text": " how images are made of numbers.", "tokens": [577, 5267, 366, 1027, 295, 3547, 13], "temperature": 0.0, "avg_logprob": -0.11511935544817635, "compression_ratio": 1.5336538461538463, "no_speech_prob": 1.8162173773816903e-06}, {"id": 499, "seek": 290608, "start": 2926.0, "end": 2927.16, "text": " So here they are, right?", "tokens": [407, 510, 436, 366, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.11511935544817635, "compression_ratio": 1.5336538461538463, "no_speech_prob": 1.8162173773816903e-06}, {"id": 500, "seek": 290608, "start": 2927.16, "end": 2930.12, "text": " Here are the numbers between 0 and 1.", "tokens": [1692, 366, 264, 3547, 1296, 1958, 293, 502, 13], "temperature": 0.0, "avg_logprob": -0.11511935544817635, "compression_ratio": 1.5336538461538463, "no_speech_prob": 1.8162173773816903e-06}, {"id": 501, "seek": 290608, "start": 2930.12, "end": 2933.04, "text": " And what I just did is I just used a little trick.", "tokens": [400, 437, 286, 445, 630, 307, 286, 445, 1143, 257, 707, 4282, 13], "temperature": 0.0, "avg_logprob": -0.11511935544817635, "compression_ratio": 1.5336538461538463, "no_speech_prob": 1.8162173773816903e-06}, {"id": 502, "seek": 293304, "start": 2933.04, "end": 2942.72, "text": " I used Microsoft Excel's conditional formatting to basically make things that higher numbers", "tokens": [286, 1143, 8116, 19060, 311, 27708, 39366, 281, 1936, 652, 721, 300, 2946, 3547], "temperature": 0.0, "avg_logprob": -0.16726274632695895, "compression_ratio": 1.456989247311828, "no_speech_prob": 1.0511461141504697e-06}, {"id": 503, "seek": 293304, "start": 2942.72, "end": 2944.32, "text": " more red.", "tokens": [544, 2182, 13], "temperature": 0.0, "avg_logprob": -0.16726274632695895, "compression_ratio": 1.456989247311828, "no_speech_prob": 1.0511461141504697e-06}, {"id": 504, "seek": 293304, "start": 2944.32, "end": 2947.6, "text": " So that's how I turned this Excel sheet.", "tokens": [407, 300, 311, 577, 286, 3574, 341, 19060, 8193, 13], "temperature": 0.0, "avg_logprob": -0.16726274632695895, "compression_ratio": 1.456989247311828, "no_speech_prob": 1.0511461141504697e-06}, {"id": 505, "seek": 293304, "start": 2947.6, "end": 2956.24, "text": " And I've just rounded it off to the nearest decimal but they're actually bigger than that.", "tokens": [400, 286, 600, 445, 23382, 309, 766, 281, 264, 23831, 26601, 457, 436, 434, 767, 3801, 813, 300, 13], "temperature": 0.0, "avg_logprob": -0.16726274632695895, "compression_ratio": 1.456989247311828, "no_speech_prob": 1.0511461141504697e-06}, {"id": 506, "seek": 293304, "start": 2956.24, "end": 2961.92, "text": " And so here is the image as numbers.", "tokens": [400, 370, 510, 307, 264, 3256, 382, 3547, 13], "temperature": 0.0, "avg_logprob": -0.16726274632695895, "compression_ratio": 1.456989247311828, "no_speech_prob": 1.0511461141504697e-06}, {"id": 507, "seek": 296192, "start": 2961.92, "end": 2968.36, "text": " And so let me show you how we went about creating this top edge detector.", "tokens": [400, 370, 718, 385, 855, 291, 577, 321, 1437, 466, 4084, 341, 1192, 4691, 25712, 13], "temperature": 0.0, "avg_logprob": -0.11942639201879501, "compression_ratio": 1.4533333333333334, "no_speech_prob": 5.896409334127384e-07}, {"id": 508, "seek": 296192, "start": 2968.36, "end": 2974.28, "text": " What we did was we created this formula.", "tokens": [708, 321, 630, 390, 321, 2942, 341, 8513, 13], "temperature": 0.0, "avg_logprob": -0.11942639201879501, "compression_ratio": 1.4533333333333334, "no_speech_prob": 5.896409334127384e-07}, {"id": 509, "seek": 296192, "start": 2974.28, "end": 2976.76, "text": " Don't worry about the max.", "tokens": [1468, 380, 3292, 466, 264, 11469, 13], "temperature": 0.0, "avg_logprob": -0.11942639201879501, "compression_ratio": 1.4533333333333334, "no_speech_prob": 5.896409334127384e-07}, {"id": 510, "seek": 296192, "start": 2976.76, "end": 2979.76, "text": " Let's focus on this.", "tokens": [961, 311, 1879, 322, 341, 13], "temperature": 0.0, "avg_logprob": -0.11942639201879501, "compression_ratio": 1.4533333333333334, "no_speech_prob": 5.896409334127384e-07}, {"id": 511, "seek": 296192, "start": 2979.76, "end": 2984.6800000000003, "text": " What it's doing is have a look at the colored in areas.", "tokens": [708, 309, 311, 884, 307, 362, 257, 574, 412, 264, 14332, 294, 3179, 13], "temperature": 0.0, "avg_logprob": -0.11942639201879501, "compression_ratio": 1.4533333333333334, "no_speech_prob": 5.896409334127384e-07}, {"id": 512, "seek": 298468, "start": 2984.68, "end": 2997.2, "text": " It's taking each of these cells and multiplying them by each of these cells and then adding", "tokens": [467, 311, 1940, 1184, 295, 613, 5438, 293, 30955, 552, 538, 1184, 295, 613, 5438, 293, 550, 5127], "temperature": 0.0, "avg_logprob": -0.1322534614139133, "compression_ratio": 1.5396825396825398, "no_speech_prob": 4.5209387167233217e-07}, {"id": 513, "seek": 298468, "start": 2997.2, "end": 3000.3199999999997, "text": " them up.", "tokens": [552, 493, 13], "temperature": 0.0, "avg_logprob": -0.1322534614139133, "compression_ratio": 1.5396825396825398, "no_speech_prob": 4.5209387167233217e-07}, {"id": 514, "seek": 298468, "start": 3000.3199999999997, "end": 3006.08, "text": " And then we do the rectified linear part which is if that ends up less than 0 then make it", "tokens": [400, 550, 321, 360, 264, 11048, 2587, 8213, 644, 597, 307, 498, 300, 5314, 493, 1570, 813, 1958, 550, 652, 309], "temperature": 0.0, "avg_logprob": -0.1322534614139133, "compression_ratio": 1.5396825396825398, "no_speech_prob": 4.5209387167233217e-07}, {"id": 515, "seek": 298468, "start": 3006.08, "end": 3007.56, "text": " 0.", "tokens": [1958, 13], "temperature": 0.0, "avg_logprob": -0.1322534614139133, "compression_ratio": 1.5396825396825398, "no_speech_prob": 4.5209387167233217e-07}, {"id": 516, "seek": 300756, "start": 3007.56, "end": 3017.72, "text": " So this is like a rectified linear unit but it's not doing the normal matrix product.", "tokens": [407, 341, 307, 411, 257, 11048, 2587, 8213, 4985, 457, 309, 311, 406, 884, 264, 2710, 8141, 1674, 13], "temperature": 0.0, "avg_logprob": -0.10054315328598022, "compression_ratio": 1.6630434782608696, "no_speech_prob": 1.5869996161654853e-07}, {"id": 517, "seek": 300756, "start": 3017.72, "end": 3024.0, "text": " It's doing the equivalent of a dot product but just on these nine cells and with just", "tokens": [467, 311, 884, 264, 10344, 295, 257, 5893, 1674, 457, 445, 322, 613, 4949, 5438, 293, 365, 445], "temperature": 0.0, "avg_logprob": -0.10054315328598022, "compression_ratio": 1.6630434782608696, "no_speech_prob": 1.5869996161654853e-07}, {"id": 518, "seek": 300756, "start": 3024.0, "end": 3026.08, "text": " these nine weights.", "tokens": [613, 4949, 17443, 13], "temperature": 0.0, "avg_logprob": -0.10054315328598022, "compression_ratio": 1.6630434782608696, "no_speech_prob": 1.5869996161654853e-07}, {"id": 519, "seek": 300756, "start": 3026.08, "end": 3033.7999999999997, "text": " So you might not be surprised to hear that if I move now one to the right then now it's", "tokens": [407, 291, 1062, 406, 312, 6100, 281, 1568, 300, 498, 286, 1286, 586, 472, 281, 264, 558, 550, 586, 309, 311], "temperature": 0.0, "avg_logprob": -0.10054315328598022, "compression_ratio": 1.6630434782608696, "no_speech_prob": 1.5869996161654853e-07}, {"id": 520, "seek": 300756, "start": 3033.7999999999997, "end": 3036.12, "text": " using the next nine cells.", "tokens": [1228, 264, 958, 4949, 5438, 13], "temperature": 0.0, "avg_logprob": -0.10054315328598022, "compression_ratio": 1.6630434782608696, "no_speech_prob": 1.5869996161654853e-07}, {"id": 521, "seek": 303612, "start": 3036.12, "end": 3043.52, "text": " So if I move like to the right quite a bit and down quite a bit to here it's using these", "tokens": [407, 498, 286, 1286, 411, 281, 264, 558, 1596, 257, 857, 293, 760, 1596, 257, 857, 281, 510, 309, 311, 1228, 613], "temperature": 0.0, "avg_logprob": -0.09642996995345406, "compression_ratio": 1.6607142857142858, "no_speech_prob": 1.7061786365957232e-06}, {"id": 522, "seek": 303612, "start": 3043.52, "end": 3045.72, "text": " nine cells.", "tokens": [4949, 5438, 13], "temperature": 0.0, "avg_logprob": -0.09642996995345406, "compression_ratio": 1.6607142857142858, "no_speech_prob": 1.7061786365957232e-06}, {"id": 523, "seek": 303612, "start": 3045.72, "end": 3052.64, "text": " So it's still doing a dot product which as we know is a form of matrix multiplication.", "tokens": [407, 309, 311, 920, 884, 257, 5893, 1674, 597, 382, 321, 458, 307, 257, 1254, 295, 8141, 27290, 13], "temperature": 0.0, "avg_logprob": -0.09642996995345406, "compression_ratio": 1.6607142857142858, "no_speech_prob": 1.7061786365957232e-06}, {"id": 524, "seek": 303612, "start": 3052.64, "end": 3056.8399999999997, "text": " But it's doing it in this way where it's kind of taking advantage of the geometry of this", "tokens": [583, 309, 311, 884, 309, 294, 341, 636, 689, 309, 311, 733, 295, 1940, 5002, 295, 264, 18426, 295, 341], "temperature": 0.0, "avg_logprob": -0.09642996995345406, "compression_ratio": 1.6607142857142858, "no_speech_prob": 1.7061786365957232e-06}, {"id": 525, "seek": 303612, "start": 3056.8399999999997, "end": 3062.72, "text": " situation that the things that are close to each other are being multiplied by this consistent", "tokens": [2590, 300, 264, 721, 300, 366, 1998, 281, 1184, 661, 366, 885, 17207, 538, 341, 8398], "temperature": 0.0, "avg_logprob": -0.09642996995345406, "compression_ratio": 1.6607142857142858, "no_speech_prob": 1.7061786365957232e-06}, {"id": 526, "seek": 306272, "start": 3062.72, "end": 3066.8399999999997, "text": " group of the same nine weights each time.", "tokens": [1594, 295, 264, 912, 4949, 17443, 1184, 565, 13], "temperature": 0.0, "avg_logprob": -0.2533966386821908, "compression_ratio": 1.4375, "no_speech_prob": 2.1233745428617112e-06}, {"id": 527, "seek": 306272, "start": 3066.8399999999997, "end": 3071.9599999999996, "text": " Because there's actually 28 by 28 numbers here right which I think is 768.", "tokens": [1436, 456, 311, 767, 7562, 538, 7562, 3547, 510, 558, 597, 286, 519, 307, 24733, 23, 13], "temperature": 0.0, "avg_logprob": -0.2533966386821908, "compression_ratio": 1.4375, "no_speech_prob": 2.1233745428617112e-06}, {"id": 528, "seek": 306272, "start": 3071.9599999999996, "end": 3080.8399999999997, "text": " 28 times 28 that's plus enough, 784.", "tokens": [7562, 1413, 7562, 300, 311, 1804, 1547, 11, 1614, 25494, 13], "temperature": 0.0, "avg_logprob": -0.2533966386821908, "compression_ratio": 1.4375, "no_speech_prob": 2.1233745428617112e-06}, {"id": 529, "seek": 306272, "start": 3080.8399999999997, "end": 3085.8799999999997, "text": " But we don't have 784 parameters we only have nine parameters.", "tokens": [583, 321, 500, 380, 362, 1614, 25494, 9834, 321, 787, 362, 4949, 9834, 13], "temperature": 0.0, "avg_logprob": -0.2533966386821908, "compression_ratio": 1.4375, "no_speech_prob": 2.1233745428617112e-06}, {"id": 530, "seek": 306272, "start": 3085.8799999999997, "end": 3087.56, "text": " And so this is called a convolution.", "tokens": [400, 370, 341, 307, 1219, 257, 45216, 13], "temperature": 0.0, "avg_logprob": -0.2533966386821908, "compression_ratio": 1.4375, "no_speech_prob": 2.1233745428617112e-06}, {"id": 531, "seek": 308756, "start": 3087.56, "end": 3096.04, "text": " So a convolution is where you basically slide this kind of little 3 by 3 matrix across a", "tokens": [407, 257, 45216, 307, 689, 291, 1936, 4137, 341, 733, 295, 707, 805, 538, 805, 8141, 2108, 257], "temperature": 0.0, "avg_logprob": -0.10168059031168619, "compression_ratio": 1.682051282051282, "no_speech_prob": 6.179386105031881e-07}, {"id": 532, "seek": 308756, "start": 3096.04, "end": 3101.98, "text": " bigger matrix and at each location you do a dot product of the corresponding elements", "tokens": [3801, 8141, 293, 412, 1184, 4914, 291, 360, 257, 5893, 1674, 295, 264, 11760, 4959], "temperature": 0.0, "avg_logprob": -0.10168059031168619, "compression_ratio": 1.682051282051282, "no_speech_prob": 6.179386105031881e-07}, {"id": 533, "seek": 308756, "start": 3101.98, "end": 3109.0, "text": " of that 3 by 3 with the corresponding elements of this 3 by 3 matrix of coefficients.", "tokens": [295, 300, 805, 538, 805, 365, 264, 11760, 4959, 295, 341, 805, 538, 805, 8141, 295, 31994, 13], "temperature": 0.0, "avg_logprob": -0.10168059031168619, "compression_ratio": 1.682051282051282, "no_speech_prob": 6.179386105031881e-07}, {"id": 534, "seek": 308756, "start": 3109.0, "end": 3114.0, "text": " Now why does that create something that finds as you see top edges?", "tokens": [823, 983, 775, 300, 1884, 746, 300, 10704, 382, 291, 536, 1192, 8819, 30], "temperature": 0.0, "avg_logprob": -0.10168059031168619, "compression_ratio": 1.682051282051282, "no_speech_prob": 6.179386105031881e-07}, {"id": 535, "seek": 311400, "start": 3114.0, "end": 3118.48, "text": " Well it's because of the particular way I constructed this 3 by 3 matrix.", "tokens": [1042, 309, 311, 570, 295, 264, 1729, 636, 286, 17083, 341, 805, 538, 805, 8141, 13], "temperature": 0.0, "avg_logprob": -0.07524256176418728, "compression_ratio": 1.703125, "no_speech_prob": 4.181185602192272e-07}, {"id": 536, "seek": 311400, "start": 3118.48, "end": 3129.56, "text": " What I said was that all of the rows just above, so these ones, are going to get a 1", "tokens": [708, 286, 848, 390, 300, 439, 295, 264, 13241, 445, 3673, 11, 370, 613, 2306, 11, 366, 516, 281, 483, 257, 502], "temperature": 0.0, "avg_logprob": -0.07524256176418728, "compression_ratio": 1.703125, "no_speech_prob": 4.181185602192272e-07}, {"id": 537, "seek": 311400, "start": 3129.56, "end": 3134.36, "text": " and all of the ones just below are going to get a minus 1 and all of the ones in the middle", "tokens": [293, 439, 295, 264, 2306, 445, 2507, 366, 516, 281, 483, 257, 3175, 502, 293, 439, 295, 264, 2306, 294, 264, 2808], "temperature": 0.0, "avg_logprob": -0.07524256176418728, "compression_ratio": 1.703125, "no_speech_prob": 4.181185602192272e-07}, {"id": 538, "seek": 311400, "start": 3134.36, "end": 3136.48, "text": " are going to get a 0.", "tokens": [366, 516, 281, 483, 257, 1958, 13], "temperature": 0.0, "avg_logprob": -0.07524256176418728, "compression_ratio": 1.703125, "no_speech_prob": 4.181185602192272e-07}, {"id": 539, "seek": 311400, "start": 3136.48, "end": 3143.6, "text": " So let's think about what happens somewhere like here.", "tokens": [407, 718, 311, 519, 466, 437, 2314, 4079, 411, 510, 13], "temperature": 0.0, "avg_logprob": -0.07524256176418728, "compression_ratio": 1.703125, "no_speech_prob": 4.181185602192272e-07}, {"id": 540, "seek": 314360, "start": 3143.6, "end": 3151.36, "text": " That is, let's try to find the right one, here it is.", "tokens": [663, 307, 11, 718, 311, 853, 281, 915, 264, 558, 472, 11, 510, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.1264969093212183, "compression_ratio": 1.8135593220338984, "no_speech_prob": 7.0718760980525985e-06}, {"id": 541, "seek": 314360, "start": 3151.36, "end": 3159.04, "text": " So here we're going to get 1 times 1 plus 1 times 1 plus 1 times 1 minus 1 times 1 minus", "tokens": [407, 510, 321, 434, 516, 281, 483, 502, 1413, 502, 1804, 502, 1413, 502, 1804, 502, 1413, 502, 3175, 502, 1413, 502, 3175], "temperature": 0.0, "avg_logprob": -0.1264969093212183, "compression_ratio": 1.8135593220338984, "no_speech_prob": 7.0718760980525985e-06}, {"id": 542, "seek": 314360, "start": 3159.04, "end": 3165.08, "text": " 1 times 1 minus 1 times 1 we're going to get 0.", "tokens": [502, 1413, 502, 3175, 502, 1413, 502, 321, 434, 516, 281, 483, 1958, 13], "temperature": 0.0, "avg_logprob": -0.1264969093212183, "compression_ratio": 1.8135593220338984, "no_speech_prob": 7.0718760980525985e-06}, {"id": 543, "seek": 314360, "start": 3165.08, "end": 3168.6, "text": " But what about up here?", "tokens": [583, 437, 466, 493, 510, 30], "temperature": 0.0, "avg_logprob": -0.1264969093212183, "compression_ratio": 1.8135593220338984, "no_speech_prob": 7.0718760980525985e-06}, {"id": 544, "seek": 316860, "start": 3168.6, "end": 3177.88, "text": " Here we're going to get 1 times 1 plus 1 times 1 plus 1 times 1, these do nothing because", "tokens": [1692, 321, 434, 516, 281, 483, 502, 1413, 502, 1804, 502, 1413, 502, 1804, 502, 1413, 502, 11, 613, 360, 1825, 570], "temperature": 0.0, "avg_logprob": -0.08767094150666267, "compression_ratio": 1.9349112426035502, "no_speech_prob": 7.002165602898458e-07}, {"id": 545, "seek": 316860, "start": 3177.88, "end": 3182.04, "text": " they're times 0, minus 1 times 0.", "tokens": [436, 434, 1413, 1958, 11, 3175, 502, 1413, 1958, 13], "temperature": 0.0, "avg_logprob": -0.08767094150666267, "compression_ratio": 1.9349112426035502, "no_speech_prob": 7.002165602898458e-07}, {"id": 546, "seek": 316860, "start": 3182.04, "end": 3183.52, "text": " So we're going to get 3.", "tokens": [407, 321, 434, 516, 281, 483, 805, 13], "temperature": 0.0, "avg_logprob": -0.08767094150666267, "compression_ratio": 1.9349112426035502, "no_speech_prob": 7.002165602898458e-07}, {"id": 547, "seek": 316860, "start": 3183.52, "end": 3188.88, "text": " So we're only going to get 3, the highest possible number, in the situation where these", "tokens": [407, 321, 434, 787, 516, 281, 483, 805, 11, 264, 6343, 1944, 1230, 11, 294, 264, 2590, 689, 613], "temperature": 0.0, "avg_logprob": -0.08767094150666267, "compression_ratio": 1.9349112426035502, "no_speech_prob": 7.002165602898458e-07}, {"id": 548, "seek": 316860, "start": 3188.88, "end": 3195.8399999999997, "text": " are all as black as possible, or in this case as red as possible, and these are all white.", "tokens": [366, 439, 382, 2211, 382, 1944, 11, 420, 294, 341, 1389, 382, 2182, 382, 1944, 11, 293, 613, 366, 439, 2418, 13], "temperature": 0.0, "avg_logprob": -0.08767094150666267, "compression_ratio": 1.9349112426035502, "no_speech_prob": 7.002165602898458e-07}, {"id": 549, "seek": 319584, "start": 3195.84, "end": 3204.04, "text": " And so that's only going to happen at a horizontal edge.", "tokens": [400, 370, 300, 311, 787, 516, 281, 1051, 412, 257, 12750, 4691, 13], "temperature": 0.0, "avg_logprob": -0.1335476657800507, "compression_ratio": 1.7058823529411764, "no_speech_prob": 1.5294106106011895e-06}, {"id": 550, "seek": 319584, "start": 3204.04, "end": 3213.88, "text": " So the one underneath it does exactly the same thing, exactly the same formulas, the", "tokens": [407, 264, 472, 7223, 309, 775, 2293, 264, 912, 551, 11, 2293, 264, 912, 30546, 11, 264], "temperature": 0.0, "avg_logprob": -0.1335476657800507, "compression_ratio": 1.7058823529411764, "no_speech_prob": 1.5294106106011895e-06}, {"id": 551, "seek": 319584, "start": 3213.88, "end": 3220.76, "text": " one underneath are exactly the same formulas, the 3 by 3 sliding thing here, but this time", "tokens": [472, 7223, 366, 2293, 264, 912, 30546, 11, 264, 805, 538, 805, 21169, 551, 510, 11, 457, 341, 565], "temperature": 0.0, "avg_logprob": -0.1335476657800507, "compression_ratio": 1.7058823529411764, "no_speech_prob": 1.5294106106011895e-06}, {"id": 552, "seek": 322076, "start": 3220.76, "end": 3227.1600000000003, "text": " we've got a different little mini matrix of coefficients, which is all ones going down", "tokens": [321, 600, 658, 257, 819, 707, 8382, 8141, 295, 31994, 11, 597, 307, 439, 2306, 516, 760], "temperature": 0.0, "avg_logprob": -0.10790730700080778, "compression_ratio": 1.6119402985074627, "no_speech_prob": 7.002163897595892e-07}, {"id": 553, "seek": 322076, "start": 3227.1600000000003, "end": 3229.1200000000003, "text": " and all minus ones going down.", "tokens": [293, 439, 3175, 2306, 516, 760, 13], "temperature": 0.0, "avg_logprob": -0.10790730700080778, "compression_ratio": 1.6119402985074627, "no_speech_prob": 7.002163897595892e-07}, {"id": 554, "seek": 322076, "start": 3229.1200000000003, "end": 3236.1200000000003, "text": " And so for exactly the same reason, this will only be 3 in situations where they're all", "tokens": [400, 370, 337, 2293, 264, 912, 1778, 11, 341, 486, 787, 312, 805, 294, 6851, 689, 436, 434, 439], "temperature": 0.0, "avg_logprob": -0.10790730700080778, "compression_ratio": 1.6119402985074627, "no_speech_prob": 7.002163897595892e-07}, {"id": 555, "seek": 322076, "start": 3236.1200000000003, "end": 3240.2000000000003, "text": " 1 here and they're all 0 here.", "tokens": [502, 510, 293, 436, 434, 439, 1958, 510, 13], "temperature": 0.0, "avg_logprob": -0.10790730700080778, "compression_ratio": 1.6119402985074627, "no_speech_prob": 7.002163897595892e-07}, {"id": 556, "seek": 322076, "start": 3240.2000000000003, "end": 3250.2000000000003, "text": " So you can think of a convolution as being a sliding window of little mini dot products", "tokens": [407, 291, 393, 519, 295, 257, 45216, 382, 885, 257, 21169, 4910, 295, 707, 8382, 5893, 3383], "temperature": 0.0, "avg_logprob": -0.10790730700080778, "compression_ratio": 1.6119402985074627, "no_speech_prob": 7.002163897595892e-07}, {"id": 557, "seek": 325020, "start": 3250.2, "end": 3252.68, "text": " of these little 3 by 3 matrices.", "tokens": [295, 613, 707, 805, 538, 805, 32284, 13], "temperature": 0.0, "avg_logprob": -0.13670325833697652, "compression_ratio": 1.5783783783783785, "no_speech_prob": 1.191106889564253e-06}, {"id": 558, "seek": 325020, "start": 3252.68, "end": 3254.9199999999996, "text": " And they don't have to be 3 by 3, right?", "tokens": [400, 436, 500, 380, 362, 281, 312, 805, 538, 805, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.13670325833697652, "compression_ratio": 1.5783783783783785, "no_speech_prob": 1.191106889564253e-06}, {"id": 559, "seek": 325020, "start": 3254.9199999999996, "end": 3261.7599999999998, "text": " We could just have easily done 5 by 5 and then we'd have a 5 by 5 matrix of coefficients", "tokens": [492, 727, 445, 362, 3612, 1096, 1025, 538, 1025, 293, 550, 321, 1116, 362, 257, 1025, 538, 1025, 8141, 295, 31994], "temperature": 0.0, "avg_logprob": -0.13670325833697652, "compression_ratio": 1.5783783783783785, "no_speech_prob": 1.191106889564253e-06}, {"id": 560, "seek": 325020, "start": 3261.7599999999998, "end": 3265.04, "text": " or whatever, whatever size you like.", "tokens": [420, 2035, 11, 2035, 2744, 291, 411, 13], "temperature": 0.0, "avg_logprob": -0.13670325833697652, "compression_ratio": 1.5783783783783785, "no_speech_prob": 1.191106889564253e-06}, {"id": 561, "seek": 325020, "start": 3265.04, "end": 3267.96, "text": " So the size of this is called its kernel size.", "tokens": [407, 264, 2744, 295, 341, 307, 1219, 1080, 28256, 2744, 13], "temperature": 0.0, "avg_logprob": -0.13670325833697652, "compression_ratio": 1.5783783783783785, "no_speech_prob": 1.191106889564253e-06}, {"id": 562, "seek": 325020, "start": 3267.96, "end": 3275.72, "text": " This is a 3 by 3 kernel for this convolution.", "tokens": [639, 307, 257, 805, 538, 805, 28256, 337, 341, 45216, 13], "temperature": 0.0, "avg_logprob": -0.13670325833697652, "compression_ratio": 1.5783783783783785, "no_speech_prob": 1.191106889564253e-06}, {"id": 563, "seek": 327572, "start": 3275.72, "end": 3282.9199999999996, "text": " So then, because this is deep learning, we just repeat these steps again and again and", "tokens": [407, 550, 11, 570, 341, 307, 2452, 2539, 11, 321, 445, 7149, 613, 4439, 797, 293, 797, 293], "temperature": 0.0, "avg_logprob": -0.16281325416227357, "compression_ratio": 1.7043478260869565, "no_speech_prob": 3.7266233903210377e-06}, {"id": 564, "seek": 327572, "start": 3282.9199999999996, "end": 3283.9199999999996, "text": " again.", "tokens": [797, 13], "temperature": 0.0, "avg_logprob": -0.16281325416227357, "compression_ratio": 1.7043478260869565, "no_speech_prob": 3.7266233903210377e-06}, {"id": 565, "seek": 327572, "start": 3283.9199999999996, "end": 3288.68, "text": " So this is this layer I'm calling Conv1, it's the first convolutional layer.", "tokens": [407, 341, 307, 341, 4583, 286, 478, 5141, 2656, 85, 16, 11, 309, 311, 264, 700, 45216, 304, 4583, 13], "temperature": 0.0, "avg_logprob": -0.16281325416227357, "compression_ratio": 1.7043478260869565, "no_speech_prob": 3.7266233903210377e-06}, {"id": 566, "seek": 327572, "start": 3288.68, "end": 3293.9199999999996, "text": " So Conv2, it's going to be a little bit different because on Conv1 we only had a single channel", "tokens": [407, 2656, 85, 17, 11, 309, 311, 516, 281, 312, 257, 707, 857, 819, 570, 322, 2656, 85, 16, 321, 787, 632, 257, 2167, 2269], "temperature": 0.0, "avg_logprob": -0.16281325416227357, "compression_ratio": 1.7043478260869565, "no_speech_prob": 3.7266233903210377e-06}, {"id": 567, "seek": 327572, "start": 3293.9199999999996, "end": 3304.12, "text": " input, it's just black and white or, you know, yeah, black and white, grayscale, one channel.", "tokens": [4846, 11, 309, 311, 445, 2211, 293, 2418, 420, 11, 291, 458, 11, 1338, 11, 2211, 293, 2418, 11, 677, 3772, 37088, 11, 472, 2269, 13], "temperature": 0.0, "avg_logprob": -0.16281325416227357, "compression_ratio": 1.7043478260869565, "no_speech_prob": 3.7266233903210377e-06}, {"id": 568, "seek": 327572, "start": 3304.12, "end": 3305.4399999999996, "text": " But now we've got two channels.", "tokens": [583, 586, 321, 600, 658, 732, 9235, 13], "temperature": 0.0, "avg_logprob": -0.16281325416227357, "compression_ratio": 1.7043478260869565, "no_speech_prob": 3.7266233903210377e-06}, {"id": 569, "seek": 330544, "start": 3305.44, "end": 3312.68, "text": " We've got the, let's make it a little smaller so we can see better, we've got the horizontal", "tokens": [492, 600, 658, 264, 11, 718, 311, 652, 309, 257, 707, 4356, 370, 321, 393, 536, 1101, 11, 321, 600, 658, 264, 12750], "temperature": 0.0, "avg_logprob": -0.1376209664852061, "compression_ratio": 1.8112244897959184, "no_speech_prob": 1.8162193100579316e-06}, {"id": 570, "seek": 330544, "start": 3312.68, "end": 3316.64, "text": " edges channel and the vertical edges channel.", "tokens": [8819, 2269, 293, 264, 9429, 8819, 2269, 13], "temperature": 0.0, "avg_logprob": -0.1376209664852061, "compression_ratio": 1.8112244897959184, "no_speech_prob": 1.8162193100579316e-06}, {"id": 571, "seek": 330544, "start": 3316.64, "end": 3320.0, "text": " And we'd have a similar thing in the first layer if it's color, we'd have a red channel,", "tokens": [400, 321, 1116, 362, 257, 2531, 551, 294, 264, 700, 4583, 498, 309, 311, 2017, 11, 321, 1116, 362, 257, 2182, 2269, 11], "temperature": 0.0, "avg_logprob": -0.1376209664852061, "compression_ratio": 1.8112244897959184, "no_speech_prob": 1.8162193100579316e-06}, {"id": 572, "seek": 330544, "start": 3320.0, "end": 3322.12, "text": " a green channel and a blue channel.", "tokens": [257, 3092, 2269, 293, 257, 3344, 2269, 13], "temperature": 0.0, "avg_logprob": -0.1376209664852061, "compression_ratio": 1.8112244897959184, "no_speech_prob": 1.8162193100579316e-06}, {"id": 573, "seek": 330544, "start": 3322.12, "end": 3333.2000000000003, "text": " So now our filter, this is called the filter, this little mini matrix is called the filter.", "tokens": [407, 586, 527, 6608, 11, 341, 307, 1219, 264, 6608, 11, 341, 707, 8382, 8141, 307, 1219, 264, 6608, 13], "temperature": 0.0, "avg_logprob": -0.1376209664852061, "compression_ratio": 1.8112244897959184, "no_speech_prob": 1.8162193100579316e-06}, {"id": 574, "seek": 333320, "start": 3333.2, "end": 3353.4399999999996, "text": " Our filter now contains a 3 by 3 by depth 2, or if you want to think of another way,", "tokens": [2621, 6608, 586, 8306, 257, 805, 538, 805, 538, 7161, 568, 11, 420, 498, 291, 528, 281, 519, 295, 1071, 636, 11], "temperature": 0.0, "avg_logprob": -0.19081745591274527, "compression_ratio": 1.3265306122448979, "no_speech_prob": 1.602808424649993e-06}, {"id": 575, "seek": 333320, "start": 3353.4399999999996, "end": 3358.48, "text": " two 3 by 3 kernels or one 3 by 3 by 2 kernel.", "tokens": [732, 805, 538, 805, 23434, 1625, 420, 472, 805, 538, 805, 538, 568, 28256, 13], "temperature": 0.0, "avg_logprob": -0.19081745591274527, "compression_ratio": 1.3265306122448979, "no_speech_prob": 1.602808424649993e-06}, {"id": 576, "seek": 335848, "start": 3358.48, "end": 3363.84, "text": " And we basically do exactly the same thing, which is we're going to multiply each of these", "tokens": [400, 321, 1936, 360, 2293, 264, 912, 551, 11, 597, 307, 321, 434, 516, 281, 12972, 1184, 295, 613], "temperature": 0.0, "avg_logprob": -0.12028892085237324, "compression_ratio": 1.8364485981308412, "no_speech_prob": 1.8448198488840717e-06}, {"id": 577, "seek": 335848, "start": 3363.84, "end": 3367.48, "text": " by each of these and sum them up.", "tokens": [538, 1184, 295, 613, 293, 2408, 552, 493, 13], "temperature": 0.0, "avg_logprob": -0.12028892085237324, "compression_ratio": 1.8364485981308412, "no_speech_prob": 1.8448198488840717e-06}, {"id": 578, "seek": 335848, "start": 3367.48, "end": 3369.08, "text": " But then we do it for the second bit as well.", "tokens": [583, 550, 321, 360, 309, 337, 264, 1150, 857, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.12028892085237324, "compression_ratio": 1.8364485981308412, "no_speech_prob": 1.8448198488840717e-06}, {"id": 579, "seek": 335848, "start": 3369.08, "end": 3375.58, "text": " We multiply each of these by each of these and sum them up.", "tokens": [492, 12972, 1184, 295, 613, 538, 1184, 295, 613, 293, 2408, 552, 493, 13], "temperature": 0.0, "avg_logprob": -0.12028892085237324, "compression_ratio": 1.8364485981308412, "no_speech_prob": 1.8448198488840717e-06}, {"id": 580, "seek": 335848, "start": 3375.58, "end": 3380.0, "text": " And so that gives us, I think I just picked some random numbers here, right.", "tokens": [400, 370, 300, 2709, 505, 11, 286, 519, 286, 445, 6183, 512, 4974, 3547, 510, 11, 558, 13], "temperature": 0.0, "avg_logprob": -0.12028892085237324, "compression_ratio": 1.8364485981308412, "no_speech_prob": 1.8448198488840717e-06}, {"id": 581, "seek": 335848, "start": 3380.0, "end": 3384.4, "text": " So this is going to now be something which can combine, oh sorry, the second one, the", "tokens": [407, 341, 307, 516, 281, 586, 312, 746, 597, 393, 10432, 11, 1954, 2597, 11, 264, 1150, 472, 11, 264], "temperature": 0.0, "avg_logprob": -0.12028892085237324, "compression_ratio": 1.8364485981308412, "no_speech_prob": 1.8448198488840717e-06}, {"id": 582, "seek": 338440, "start": 3384.4, "end": 3391.84, "text": " second set, so it's, sorry, each of the red ones by each of the blue ones, that's here,", "tokens": [1150, 992, 11, 370, 309, 311, 11, 2597, 11, 1184, 295, 264, 2182, 2306, 538, 1184, 295, 264, 3344, 2306, 11, 300, 311, 510, 11], "temperature": 0.0, "avg_logprob": -0.09266753099402603, "compression_ratio": 1.8780487804878048, "no_speech_prob": 1.1911065485037398e-06}, {"id": 583, "seek": 338440, "start": 3391.84, "end": 3397.08, "text": " plus each of the green ones times each of the mauve ones, that's here.", "tokens": [1804, 1184, 295, 264, 3092, 2306, 1413, 1184, 295, 264, 22074, 303, 2306, 11, 300, 311, 510, 13], "temperature": 0.0, "avg_logprob": -0.09266753099402603, "compression_ratio": 1.8780487804878048, "no_speech_prob": 1.1911065485037398e-06}, {"id": 584, "seek": 338440, "start": 3397.08, "end": 3403.12, "text": " So this first filter is being applied to the horizontal edge detector and the second filter", "tokens": [407, 341, 700, 6608, 307, 885, 6456, 281, 264, 12750, 4691, 25712, 293, 264, 1150, 6608], "temperature": 0.0, "avg_logprob": -0.09266753099402603, "compression_ratio": 1.8780487804878048, "no_speech_prob": 1.1911065485037398e-06}, {"id": 585, "seek": 338440, "start": 3403.12, "end": 3406.1800000000003, "text": " is being applied to the vertical edge detector.", "tokens": [307, 885, 6456, 281, 264, 9429, 4691, 25712, 13], "temperature": 0.0, "avg_logprob": -0.09266753099402603, "compression_ratio": 1.8780487804878048, "no_speech_prob": 1.1911065485037398e-06}, {"id": 586, "seek": 338440, "start": 3406.1800000000003, "end": 3412.64, "text": " And as a result we can end up with something that combines features of the two things.", "tokens": [400, 382, 257, 1874, 321, 393, 917, 493, 365, 746, 300, 29520, 4122, 295, 264, 732, 721, 13], "temperature": 0.0, "avg_logprob": -0.09266753099402603, "compression_ratio": 1.8780487804878048, "no_speech_prob": 1.1911065485037398e-06}, {"id": 587, "seek": 341264, "start": 3412.64, "end": 3421.4, "text": " And so then we can have a second channel over here, which is just a different bunch of convolutions", "tokens": [400, 370, 550, 321, 393, 362, 257, 1150, 2269, 670, 510, 11, 597, 307, 445, 257, 819, 3840, 295, 3754, 15892], "temperature": 0.0, "avg_logprob": -0.1485480537897424, "compression_ratio": 1.5393258426966292, "no_speech_prob": 9.874618172034388e-07}, {"id": 588, "seek": 341264, "start": 3421.4, "end": 3424.08, "text": " for each of the two channels, this one times this one.", "tokens": [337, 1184, 295, 264, 732, 9235, 11, 341, 472, 1413, 341, 472, 13], "temperature": 0.0, "avg_logprob": -0.1485480537897424, "compression_ratio": 1.5393258426966292, "no_speech_prob": 9.874618172034388e-07}, {"id": 589, "seek": 341264, "start": 3424.08, "end": 3427.92, "text": " Again, you can see the colours.", "tokens": [3764, 11, 291, 393, 536, 264, 16484, 13], "temperature": 0.0, "avg_logprob": -0.1485480537897424, "compression_ratio": 1.5393258426966292, "no_speech_prob": 9.874618172034388e-07}, {"id": 590, "seek": 341264, "start": 3427.92, "end": 3434.16, "text": " So what we could do is if, you know, once we kind of get to the end, we'll end up, I'll", "tokens": [407, 437, 321, 727, 360, 307, 498, 11, 291, 458, 11, 1564, 321, 733, 295, 483, 281, 264, 917, 11, 321, 603, 917, 493, 11, 286, 603], "temperature": 0.0, "avg_logprob": -0.1485480537897424, "compression_ratio": 1.5393258426966292, "no_speech_prob": 9.874618172034388e-07}, {"id": 591, "seek": 343416, "start": 3434.16, "end": 3444.92, "text": " show you how in a moment, we'll end up with a single set of 10 activations, one per digit", "tokens": [855, 291, 577, 294, 257, 1623, 11, 321, 603, 917, 493, 365, 257, 2167, 992, 295, 1266, 2430, 763, 11, 472, 680, 14293], "temperature": 0.0, "avg_logprob": -0.14232932711110532, "compression_ratio": 1.6594827586206897, "no_speech_prob": 8.39797212393023e-06}, {"id": 592, "seek": 343416, "start": 3444.92, "end": 3447.04, "text": " we're recognising, zero to nine.", "tokens": [321, 434, 3068, 3436, 11, 4018, 281, 4949, 13], "temperature": 0.0, "avg_logprob": -0.14232932711110532, "compression_ratio": 1.6594827586206897, "no_speech_prob": 8.39797212393023e-06}, {"id": 593, "seek": 343416, "start": 3447.04, "end": 3449.92, "text": " Or in this case I think we could just create one, you know, maybe we're just trying to", "tokens": [1610, 294, 341, 1389, 286, 519, 321, 727, 445, 1884, 472, 11, 291, 458, 11, 1310, 321, 434, 445, 1382, 281], "temperature": 0.0, "avg_logprob": -0.14232932711110532, "compression_ratio": 1.6594827586206897, "no_speech_prob": 8.39797212393023e-06}, {"id": 594, "seek": 343416, "start": 3449.92, "end": 3453.52, "text": " recognise nothing but the number seven or not the number seven, so we could just have", "tokens": [23991, 1825, 457, 264, 1230, 3407, 420, 406, 264, 1230, 3407, 11, 370, 321, 727, 445, 362], "temperature": 0.0, "avg_logprob": -0.14232932711110532, "compression_ratio": 1.6594827586206897, "no_speech_prob": 8.39797212393023e-06}, {"id": 595, "seek": 343416, "start": 3453.52, "end": 3455.58, "text": " one activation.", "tokens": [472, 24433, 13], "temperature": 0.0, "avg_logprob": -0.14232932711110532, "compression_ratio": 1.6594827586206897, "no_speech_prob": 8.39797212393023e-06}, {"id": 596, "seek": 343416, "start": 3455.58, "end": 3462.8199999999997, "text": " And then we would back propagate through this using SGD in the usual way.", "tokens": [400, 550, 321, 576, 646, 48256, 807, 341, 1228, 34520, 35, 294, 264, 7713, 636, 13], "temperature": 0.0, "avg_logprob": -0.14232932711110532, "compression_ratio": 1.6594827586206897, "no_speech_prob": 8.39797212393023e-06}, {"id": 597, "seek": 346282, "start": 3462.82, "end": 3466.34, "text": " And that is going to end up optimising these numbers.", "tokens": [400, 300, 307, 516, 281, 917, 493, 5028, 3436, 613, 3547, 13], "temperature": 0.0, "avg_logprob": -0.10599929595662054, "compression_ratio": 1.6507936507936507, "no_speech_prob": 1.1015920335921692e-06}, {"id": 598, "seek": 346282, "start": 3466.34, "end": 3471.48, "text": " So in this case I manually put in the numbers I knew would create edge detectors.", "tokens": [407, 294, 341, 1389, 286, 16945, 829, 294, 264, 3547, 286, 2586, 576, 1884, 4691, 46866, 13], "temperature": 0.0, "avg_logprob": -0.10599929595662054, "compression_ratio": 1.6507936507936507, "no_speech_prob": 1.1015920335921692e-06}, {"id": 599, "seek": 346282, "start": 3471.48, "end": 3478.6000000000004, "text": " In real life you start with random numbers and then you use SGD to optimise these parameters.", "tokens": [682, 957, 993, 291, 722, 365, 4974, 3547, 293, 550, 291, 764, 34520, 35, 281, 5028, 908, 613, 9834, 13], "temperature": 0.0, "avg_logprob": -0.10599929595662054, "compression_ratio": 1.6507936507936507, "no_speech_prob": 1.1015920335921692e-06}, {"id": 600, "seek": 346282, "start": 3478.6000000000004, "end": 3483.82, "text": " Okay, so there's a few things we can do next.", "tokens": [1033, 11, 370, 456, 311, 257, 1326, 721, 321, 393, 360, 958, 13], "temperature": 0.0, "avg_logprob": -0.10599929595662054, "compression_ratio": 1.6507936507936507, "no_speech_prob": 1.1015920335921692e-06}, {"id": 601, "seek": 346282, "start": 3483.82, "end": 3489.2200000000003, "text": " And I'm going to show you the way that was more common a few years ago and then I'll", "tokens": [400, 286, 478, 516, 281, 855, 291, 264, 636, 300, 390, 544, 2689, 257, 1326, 924, 2057, 293, 550, 286, 603], "temperature": 0.0, "avg_logprob": -0.10599929595662054, "compression_ratio": 1.6507936507936507, "no_speech_prob": 1.1015920335921692e-06}, {"id": 602, "seek": 346282, "start": 3489.2200000000003, "end": 3492.76, "text": " explain some changes that have been made more recently.", "tokens": [2903, 512, 2962, 300, 362, 668, 1027, 544, 3938, 13], "temperature": 0.0, "avg_logprob": -0.10599929595662054, "compression_ratio": 1.6507936507936507, "no_speech_prob": 1.1015920335921692e-06}, {"id": 603, "seek": 349276, "start": 3492.76, "end": 3500.5200000000004, "text": " What happened a few years ago was we would then take these activations, which as you", "tokens": [708, 2011, 257, 1326, 924, 2057, 390, 321, 576, 550, 747, 613, 2430, 763, 11, 597, 382, 291], "temperature": 0.0, "avg_logprob": -0.13824368506362758, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.237732698835316e-06}, {"id": 604, "seek": 349276, "start": 3500.5200000000004, "end": 3506.88, "text": " can see these activations now are kind of in a grid pattern, and we would do something", "tokens": [393, 536, 613, 2430, 763, 586, 366, 733, 295, 294, 257, 10748, 5102, 11, 293, 321, 576, 360, 746], "temperature": 0.0, "avg_logprob": -0.13824368506362758, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.237732698835316e-06}, {"id": 605, "seek": 349276, "start": 3506.88, "end": 3508.44, "text": " called maxPalling.", "tokens": [1219, 11469, 47, 24021, 13], "temperature": 0.0, "avg_logprob": -0.13824368506362758, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.237732698835316e-06}, {"id": 606, "seek": 349276, "start": 3508.44, "end": 3513.32, "text": " And maxPalling is kind of like a convolution, it's a sliding window, but this time as the", "tokens": [400, 11469, 47, 24021, 307, 733, 295, 411, 257, 45216, 11, 309, 311, 257, 21169, 4910, 11, 457, 341, 565, 382, 264], "temperature": 0.0, "avg_logprob": -0.13824368506362758, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.237732698835316e-06}, {"id": 607, "seek": 349276, "start": 3513.32, "end": 3519.4, "text": " sliding window goes across, so here we're up to here, we don't do a dot product over", "tokens": [21169, 4910, 1709, 2108, 11, 370, 510, 321, 434, 493, 281, 510, 11, 321, 500, 380, 360, 257, 5893, 1674, 670], "temperature": 0.0, "avg_logprob": -0.13824368506362758, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.237732698835316e-06}, {"id": 608, "seek": 351940, "start": 3519.4, "end": 3523.36, "text": " a filter, but instead we just take a maximum.", "tokens": [257, 6608, 11, 457, 2602, 321, 445, 747, 257, 6674, 13], "temperature": 0.0, "avg_logprob": -0.16281022416784408, "compression_ratio": 1.7322404371584699, "no_speech_prob": 3.4465613225620473e-06}, {"id": 609, "seek": 351940, "start": 3523.36, "end": 3527.56, "text": " See here, this is the maximum of these four numbers.", "tokens": [3008, 510, 11, 341, 307, 264, 6674, 295, 613, 1451, 3547, 13], "temperature": 0.0, "avg_logprob": -0.16281022416784408, "compression_ratio": 1.7322404371584699, "no_speech_prob": 3.4465613225620473e-06}, {"id": 610, "seek": 351940, "start": 3527.56, "end": 3531.88, "text": " And if we go across a little bit, this is the maximum of these four numbers.", "tokens": [400, 498, 321, 352, 2108, 257, 707, 857, 11, 341, 307, 264, 6674, 295, 613, 1451, 3547, 13], "temperature": 0.0, "avg_logprob": -0.16281022416784408, "compression_ratio": 1.7322404371584699, "no_speech_prob": 3.4465613225620473e-06}, {"id": 611, "seek": 351940, "start": 3531.88, "end": 3536.6, "text": " Go across a bit, go across a bit, and so forth.", "tokens": [1037, 2108, 257, 857, 11, 352, 2108, 257, 857, 11, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.16281022416784408, "compression_ratio": 1.7322404371584699, "no_speech_prob": 3.4465613225620473e-06}, {"id": 612, "seek": 351940, "start": 3536.6, "end": 3539.5, "text": " Oh that goes off the edge.", "tokens": [876, 300, 1709, 766, 264, 4691, 13], "temperature": 0.0, "avg_logprob": -0.16281022416784408, "compression_ratio": 1.7322404371584699, "no_speech_prob": 3.4465613225620473e-06}, {"id": 613, "seek": 351940, "start": 3539.5, "end": 3548.8, "text": " And you can see what happens when this is called a 2x2 maxPalling.", "tokens": [400, 291, 393, 536, 437, 2314, 562, 341, 307, 1219, 257, 568, 87, 17, 11469, 47, 24021, 13], "temperature": 0.0, "avg_logprob": -0.16281022416784408, "compression_ratio": 1.7322404371584699, "no_speech_prob": 3.4465613225620473e-06}, {"id": 614, "seek": 354880, "start": 3548.8, "end": 3558.88, "text": " So you can see what happens with a 2x2 maxPalling, we end up losing half of our activations on", "tokens": [407, 291, 393, 536, 437, 2314, 365, 257, 568, 87, 17, 11469, 47, 24021, 11, 321, 917, 493, 7027, 1922, 295, 527, 2430, 763, 322], "temperature": 0.0, "avg_logprob": -0.07820258736610412, "compression_ratio": 1.568421052631579, "no_speech_prob": 2.058045083686011e-06}, {"id": 615, "seek": 354880, "start": 3558.88, "end": 3560.44, "text": " each dimension.", "tokens": [1184, 10139, 13], "temperature": 0.0, "avg_logprob": -0.07820258736610412, "compression_ratio": 1.568421052631579, "no_speech_prob": 2.058045083686011e-06}, {"id": 616, "seek": 354880, "start": 3560.44, "end": 3566.6000000000004, "text": " So we're going to end up with only one quarter of the number of activations we used to have.", "tokens": [407, 321, 434, 516, 281, 917, 493, 365, 787, 472, 6555, 295, 264, 1230, 295, 2430, 763, 321, 1143, 281, 362, 13], "temperature": 0.0, "avg_logprob": -0.07820258736610412, "compression_ratio": 1.568421052631579, "no_speech_prob": 2.058045083686011e-06}, {"id": 617, "seek": 354880, "start": 3566.6000000000004, "end": 3573.36, "text": " And that's actually a good thing, because if we keep on doing convolution maxPall, convolution", "tokens": [400, 300, 311, 767, 257, 665, 551, 11, 570, 498, 321, 1066, 322, 884, 45216, 11469, 47, 336, 11, 45216], "temperature": 0.0, "avg_logprob": -0.07820258736610412, "compression_ratio": 1.568421052631579, "no_speech_prob": 2.058045083686011e-06}, {"id": 618, "seek": 357336, "start": 3573.36, "end": 3579.6800000000003, "text": " maxPall, we're going to get fewer and fewer and fewer activations until eventually we'll", "tokens": [11469, 47, 336, 11, 321, 434, 516, 281, 483, 13366, 293, 13366, 293, 13366, 2430, 763, 1826, 4728, 321, 603], "temperature": 0.0, "avg_logprob": -0.10774655037737907, "compression_ratio": 1.703883495145631, "no_speech_prob": 4.637859092326835e-06}, {"id": 619, "seek": 357336, "start": 3579.6800000000003, "end": 3585.6400000000003, "text": " just have one left, which is what we want.", "tokens": [445, 362, 472, 1411, 11, 597, 307, 437, 321, 528, 13], "temperature": 0.0, "avg_logprob": -0.10774655037737907, "compression_ratio": 1.703883495145631, "no_speech_prob": 4.637859092326835e-06}, {"id": 620, "seek": 357336, "start": 3585.6400000000003, "end": 3591.92, "text": " That's effectively what we used to do, but the other thing I mentioned is we didn't normally", "tokens": [663, 311, 8659, 437, 321, 1143, 281, 360, 11, 457, 264, 661, 551, 286, 2835, 307, 321, 994, 380, 5646], "temperature": 0.0, "avg_logprob": -0.10774655037737907, "compression_ratio": 1.703883495145631, "no_speech_prob": 4.637859092326835e-06}, {"id": 621, "seek": 357336, "start": 3591.92, "end": 3594.44, "text": " keep going until there's only one left.", "tokens": [1066, 516, 1826, 456, 311, 787, 472, 1411, 13], "temperature": 0.0, "avg_logprob": -0.10774655037737907, "compression_ratio": 1.703883495145631, "no_speech_prob": 4.637859092326835e-06}, {"id": 622, "seek": 357336, "start": 3594.44, "end": 3599.34, "text": " What we used to then do is we'd basically say, okay, at some point we're going to take", "tokens": [708, 321, 1143, 281, 550, 360, 307, 321, 1116, 1936, 584, 11, 1392, 11, 412, 512, 935, 321, 434, 516, 281, 747], "temperature": 0.0, "avg_logprob": -0.10774655037737907, "compression_ratio": 1.703883495145631, "no_speech_prob": 4.637859092326835e-06}, {"id": 623, "seek": 359934, "start": 3599.34, "end": 3608.88, "text": " all of the activations that are left and we're going to basically just do a dot product of", "tokens": [439, 295, 264, 2430, 763, 300, 366, 1411, 293, 321, 434, 516, 281, 1936, 445, 360, 257, 5893, 1674, 295], "temperature": 0.0, "avg_logprob": -0.1259914566488827, "compression_ratio": 1.5333333333333334, "no_speech_prob": 5.539165499612864e-07}, {"id": 624, "seek": 359934, "start": 3608.88, "end": 3614.2000000000003, "text": " those with a bunch of coefficients, not as a convolution but just as a normal linear", "tokens": [729, 365, 257, 3840, 295, 31994, 11, 406, 382, 257, 45216, 457, 445, 382, 257, 2710, 8213], "temperature": 0.0, "avg_logprob": -0.1259914566488827, "compression_ratio": 1.5333333333333334, "no_speech_prob": 5.539165499612864e-07}, {"id": 625, "seek": 359934, "start": 3614.2000000000003, "end": 3615.2000000000003, "text": " layer.", "tokens": [4583, 13], "temperature": 0.0, "avg_logprob": -0.1259914566488827, "compression_ratio": 1.5333333333333334, "no_speech_prob": 5.539165499612864e-07}, {"id": 626, "seek": 359934, "start": 3615.2000000000003, "end": 3618.1200000000003, "text": " And this is called the dense layer.", "tokens": [400, 341, 307, 1219, 264, 18011, 4583, 13], "temperature": 0.0, "avg_logprob": -0.1259914566488827, "compression_ratio": 1.5333333333333334, "no_speech_prob": 5.539165499612864e-07}, {"id": 627, "seek": 359934, "start": 3618.1200000000003, "end": 3620.02, "text": " And then we would add them all up.", "tokens": [400, 550, 321, 576, 909, 552, 439, 493, 13], "temperature": 0.0, "avg_logprob": -0.1259914566488827, "compression_ratio": 1.5333333333333334, "no_speech_prob": 5.539165499612864e-07}, {"id": 628, "seek": 362002, "start": 3620.02, "end": 3631.04, "text": " So we'd basically end up with our final big dot product of all of the maxPulled activations", "tokens": [407, 321, 1116, 1936, 917, 493, 365, 527, 2572, 955, 5893, 1674, 295, 439, 295, 264, 11469, 47, 858, 292, 2430, 763], "temperature": 0.0, "avg_logprob": -0.14031481275371477, "compression_ratio": 1.669683257918552, "no_speech_prob": 4.381843439205113e-07}, {"id": 629, "seek": 362002, "start": 3631.04, "end": 3633.0, "text": " by all of the weights.", "tokens": [538, 439, 295, 264, 17443, 13], "temperature": 0.0, "avg_logprob": -0.14031481275371477, "compression_ratio": 1.669683257918552, "no_speech_prob": 4.381843439205113e-07}, {"id": 630, "seek": 362002, "start": 3633.0, "end": 3634.8, "text": " And we'd do that for each channel.", "tokens": [400, 321, 1116, 360, 300, 337, 1184, 2269, 13], "temperature": 0.0, "avg_logprob": -0.14031481275371477, "compression_ratio": 1.669683257918552, "no_speech_prob": 4.381843439205113e-07}, {"id": 631, "seek": 362002, "start": 3634.8, "end": 3638.32, "text": " And so that would give us our final activation.", "tokens": [400, 370, 300, 576, 976, 505, 527, 2572, 24433, 13], "temperature": 0.0, "avg_logprob": -0.14031481275371477, "compression_ratio": 1.669683257918552, "no_speech_prob": 4.381843439205113e-07}, {"id": 632, "seek": 362002, "start": 3638.32, "end": 3643.04, "text": " And as I say here, MNIST would actually have 10 activations, so you'd have a separate set", "tokens": [400, 382, 286, 584, 510, 11, 376, 45, 19756, 576, 767, 362, 1266, 2430, 763, 11, 370, 291, 1116, 362, 257, 4994, 992], "temperature": 0.0, "avg_logprob": -0.14031481275371477, "compression_ratio": 1.669683257918552, "no_speech_prob": 4.381843439205113e-07}, {"id": 633, "seek": 362002, "start": 3643.04, "end": 3648.0, "text": " of weights for each of the digits you're predicting, and then softmax after that.", "tokens": [295, 17443, 337, 1184, 295, 264, 27011, 291, 434, 32884, 11, 293, 550, 2787, 41167, 934, 300, 13], "temperature": 0.0, "avg_logprob": -0.14031481275371477, "compression_ratio": 1.669683257918552, "no_speech_prob": 4.381843439205113e-07}, {"id": 634, "seek": 364800, "start": 3648.0, "end": 3653.48, "text": " Okay, nowadays we do things very slightly differently.", "tokens": [1033, 11, 13434, 321, 360, 721, 588, 4748, 7614, 13], "temperature": 0.0, "avg_logprob": -0.12830061136290086, "compression_ratio": 1.5870646766169154, "no_speech_prob": 9.276358809984231e-07}, {"id": 635, "seek": 364800, "start": 3653.48, "end": 3659.52, "text": " Nowadays we normally don't have maxPull layers, but instead what we normally do is when we", "tokens": [28908, 321, 5646, 500, 380, 362, 11469, 47, 858, 7914, 11, 457, 2602, 437, 321, 5646, 360, 307, 562, 321], "temperature": 0.0, "avg_logprob": -0.12830061136290086, "compression_ratio": 1.5870646766169154, "no_speech_prob": 9.276358809984231e-07}, {"id": 636, "seek": 364800, "start": 3659.52, "end": 3669.48, "text": " do our sliding window, like this one here, we don't normally, let's go back to C. So", "tokens": [360, 527, 21169, 4910, 11, 411, 341, 472, 510, 11, 321, 500, 380, 5646, 11, 718, 311, 352, 646, 281, 383, 13, 407], "temperature": 0.0, "avg_logprob": -0.12830061136290086, "compression_ratio": 1.5870646766169154, "no_speech_prob": 9.276358809984231e-07}, {"id": 637, "seek": 364800, "start": 3669.48, "end": 3677.68, "text": " when I go one to the right, so currently we're starting in cell column G, if I go one to", "tokens": [562, 286, 352, 472, 281, 264, 558, 11, 370, 4362, 321, 434, 2891, 294, 2815, 7738, 460, 11, 498, 286, 352, 472, 281], "temperature": 0.0, "avg_logprob": -0.12830061136290086, "compression_ratio": 1.5870646766169154, "no_speech_prob": 9.276358809984231e-07}, {"id": 638, "seek": 367768, "start": 3677.68, "end": 3682.6, "text": " the right, the next one is column H. And if I go one to the right, the next one starts", "tokens": [264, 558, 11, 264, 958, 472, 307, 7738, 389, 13, 400, 498, 286, 352, 472, 281, 264, 558, 11, 264, 958, 472, 3719], "temperature": 0.0, "avg_logprob": -0.12789889526367187, "compression_ratio": 1.7662835249042146, "no_speech_prob": 1.0407801909195769e-07}, {"id": 639, "seek": 367768, "start": 3682.6, "end": 3688.16, "text": " in column I. So you can see it's sliding the window every three by three.", "tokens": [294, 7738, 286, 13, 407, 291, 393, 536, 309, 311, 21169, 264, 4910, 633, 1045, 538, 1045, 13], "temperature": 0.0, "avg_logprob": -0.12789889526367187, "compression_ratio": 1.7662835249042146, "no_speech_prob": 1.0407801909195769e-07}, {"id": 640, "seek": 367768, "start": 3688.16, "end": 3691.8399999999997, "text": " Nowadays what we tend to do instead is we generally skip one.", "tokens": [28908, 437, 321, 3928, 281, 360, 2602, 307, 321, 5101, 10023, 472, 13], "temperature": 0.0, "avg_logprob": -0.12789889526367187, "compression_ratio": 1.7662835249042146, "no_speech_prob": 1.0407801909195769e-07}, {"id": 641, "seek": 367768, "start": 3691.8399999999997, "end": 3695.3199999999997, "text": " So we would normally only look at every second.", "tokens": [407, 321, 576, 5646, 787, 574, 412, 633, 1150, 13], "temperature": 0.0, "avg_logprob": -0.12789889526367187, "compression_ratio": 1.7662835249042146, "no_speech_prob": 1.0407801909195769e-07}, {"id": 642, "seek": 367768, "start": 3695.3199999999997, "end": 3700.8399999999997, "text": " So we would, after doing column I, we would skip columns J and we'd go straight to column", "tokens": [407, 321, 576, 11, 934, 884, 7738, 286, 11, 321, 576, 10023, 13766, 508, 293, 321, 1116, 352, 2997, 281, 7738], "temperature": 0.0, "avg_logprob": -0.12789889526367187, "compression_ratio": 1.7662835249042146, "no_speech_prob": 1.0407801909195769e-07}, {"id": 643, "seek": 367768, "start": 3700.8399999999997, "end": 3704.54, "text": " K. And that's called a stride two convolution.", "tokens": [591, 13, 400, 300, 311, 1219, 257, 1056, 482, 732, 45216, 13], "temperature": 0.0, "avg_logprob": -0.12789889526367187, "compression_ratio": 1.7662835249042146, "no_speech_prob": 1.0407801909195769e-07}, {"id": 644, "seek": 367768, "start": 3704.54, "end": 3707.14, "text": " We do that both across the rows and down the columns.", "tokens": [492, 360, 300, 1293, 2108, 264, 13241, 293, 760, 264, 13766, 13], "temperature": 0.0, "avg_logprob": -0.12789889526367187, "compression_ratio": 1.7662835249042146, "no_speech_prob": 1.0407801909195769e-07}, {"id": 645, "seek": 370714, "start": 3707.14, "end": 3711.8799999999997, "text": " And what that means is every time we do a convolution, we reduce our effective kind", "tokens": [400, 437, 300, 1355, 307, 633, 565, 321, 360, 257, 45216, 11, 321, 5407, 527, 4942, 733], "temperature": 0.0, "avg_logprob": -0.06453748985573098, "compression_ratio": 1.5555555555555556, "no_speech_prob": 8.31527415812161e-07}, {"id": 646, "seek": 370714, "start": 3711.8799999999997, "end": 3718.4, "text": " of feature size, grid size, by two on each axis.", "tokens": [295, 4111, 2744, 11, 10748, 2744, 11, 538, 732, 322, 1184, 10298, 13], "temperature": 0.0, "avg_logprob": -0.06453748985573098, "compression_ratio": 1.5555555555555556, "no_speech_prob": 8.31527415812161e-07}, {"id": 647, "seek": 370714, "start": 3718.4, "end": 3721.72, "text": " So it reduces it by four in total.", "tokens": [407, 309, 18081, 309, 538, 1451, 294, 3217, 13], "temperature": 0.0, "avg_logprob": -0.06453748985573098, "compression_ratio": 1.5555555555555556, "no_speech_prob": 8.31527415812161e-07}, {"id": 648, "seek": 370714, "start": 3721.72, "end": 3726.52, "text": " So that's basically instead of doing maxPulling.", "tokens": [407, 300, 311, 1936, 2602, 295, 884, 11469, 47, 858, 278, 13], "temperature": 0.0, "avg_logprob": -0.06453748985573098, "compression_ratio": 1.5555555555555556, "no_speech_prob": 8.31527415812161e-07}, {"id": 649, "seek": 370714, "start": 3726.52, "end": 3733.7799999999997, "text": " And then the other thing that we do differently is nowadays we don't normally have a single", "tokens": [400, 550, 264, 661, 551, 300, 321, 360, 7614, 307, 13434, 321, 500, 380, 5646, 362, 257, 2167], "temperature": 0.0, "avg_logprob": -0.06453748985573098, "compression_ratio": 1.5555555555555556, "no_speech_prob": 8.31527415812161e-07}, {"id": 650, "seek": 373378, "start": 3733.78, "end": 3737.5600000000004, "text": " dense layer at the end, a single matrix multiply at the end.", "tokens": [18011, 4583, 412, 264, 917, 11, 257, 2167, 8141, 12972, 412, 264, 917, 13], "temperature": 0.0, "avg_logprob": -0.12902035486130486, "compression_ratio": 1.6846846846846846, "no_speech_prob": 2.5612753233872354e-06}, {"id": 651, "seek": 373378, "start": 3737.5600000000004, "end": 3741.2000000000003, "text": " But instead what we do, we generally keep doing stride two convolutions.", "tokens": [583, 2602, 437, 321, 360, 11, 321, 5101, 1066, 884, 1056, 482, 732, 3754, 15892, 13], "temperature": 0.0, "avg_logprob": -0.12902035486130486, "compression_ratio": 1.6846846846846846, "no_speech_prob": 2.5612753233872354e-06}, {"id": 652, "seek": 373378, "start": 3741.2000000000003, "end": 3746.1200000000003, "text": " So each one's going to reduce the grid size by two by two.", "tokens": [407, 1184, 472, 311, 516, 281, 5407, 264, 10748, 2744, 538, 732, 538, 732, 13], "temperature": 0.0, "avg_logprob": -0.12902035486130486, "compression_ratio": 1.6846846846846846, "no_speech_prob": 2.5612753233872354e-06}, {"id": 653, "seek": 373378, "start": 3746.1200000000003, "end": 3751.44, "text": " We keep going down until we've got about a seven by seven grid.", "tokens": [492, 1066, 516, 760, 1826, 321, 600, 658, 466, 257, 3407, 538, 3407, 10748, 13], "temperature": 0.0, "avg_logprob": -0.12902035486130486, "compression_ratio": 1.6846846846846846, "no_speech_prob": 2.5612753233872354e-06}, {"id": 654, "seek": 373378, "start": 3751.44, "end": 3753.48, "text": " And then we do a single pulling at the end.", "tokens": [400, 550, 321, 360, 257, 2167, 8407, 412, 264, 917, 13], "temperature": 0.0, "avg_logprob": -0.12902035486130486, "compression_ratio": 1.6846846846846846, "no_speech_prob": 2.5612753233872354e-06}, {"id": 655, "seek": 373378, "start": 3753.48, "end": 3756.4, "text": " And we don't normally do maxPull nowadays.", "tokens": [400, 321, 500, 380, 5646, 360, 11469, 47, 858, 13434, 13], "temperature": 0.0, "avg_logprob": -0.12902035486130486, "compression_ratio": 1.6846846846846846, "no_speech_prob": 2.5612753233872354e-06}, {"id": 656, "seek": 373378, "start": 3756.4, "end": 3757.84, "text": " Instead we do an average pull.", "tokens": [7156, 321, 360, 364, 4274, 2235, 13], "temperature": 0.0, "avg_logprob": -0.12902035486130486, "compression_ratio": 1.6846846846846846, "no_speech_prob": 2.5612753233872354e-06}, {"id": 657, "seek": 375784, "start": 3757.84, "end": 3767.6800000000003, "text": " So we average the activations of each one of the seven by seven features.", "tokens": [407, 321, 4274, 264, 2430, 763, 295, 1184, 472, 295, 264, 3407, 538, 3407, 4122, 13], "temperature": 0.0, "avg_logprob": -0.08225897834414528, "compression_ratio": 1.7161016949152543, "no_speech_prob": 3.8070120922384376e-07}, {"id": 658, "seek": 375784, "start": 3767.6800000000003, "end": 3773.2000000000003, "text": " This is actually quite important to know because if you think about what that means, it means", "tokens": [639, 307, 767, 1596, 1021, 281, 458, 570, 498, 291, 519, 466, 437, 300, 1355, 11, 309, 1355], "temperature": 0.0, "avg_logprob": -0.08225897834414528, "compression_ratio": 1.7161016949152543, "no_speech_prob": 3.8070120922384376e-07}, {"id": 659, "seek": 375784, "start": 3773.2000000000003, "end": 3780.36, "text": " that something like an ImageNet style image detector is going to end up with a seven by", "tokens": [300, 746, 411, 364, 29903, 31890, 3758, 3256, 25712, 307, 516, 281, 917, 493, 365, 257, 3407, 538], "temperature": 0.0, "avg_logprob": -0.08225897834414528, "compression_ratio": 1.7161016949152543, "no_speech_prob": 3.8070120922384376e-07}, {"id": 660, "seek": 375784, "start": 3780.36, "end": 3781.36, "text": " seven grid.", "tokens": [3407, 10748, 13], "temperature": 0.0, "avg_logprob": -0.08225897834414528, "compression_ratio": 1.7161016949152543, "no_speech_prob": 3.8070120922384376e-07}, {"id": 661, "seek": 375784, "start": 3781.36, "end": 3783.84, "text": " Let's say it's trying to say, is this a bear?", "tokens": [961, 311, 584, 309, 311, 1382, 281, 584, 11, 307, 341, 257, 6155, 30], "temperature": 0.0, "avg_logprob": -0.08225897834414528, "compression_ratio": 1.7161016949152543, "no_speech_prob": 3.8070120922384376e-07}, {"id": 662, "seek": 375784, "start": 3783.84, "end": 3787.6000000000004, "text": " And in each of the parts of the seven by seven grid, it's basically saying, is there a bear", "tokens": [400, 294, 1184, 295, 264, 3166, 295, 264, 3407, 538, 3407, 10748, 11, 309, 311, 1936, 1566, 11, 307, 456, 257, 6155], "temperature": 0.0, "avg_logprob": -0.08225897834414528, "compression_ratio": 1.7161016949152543, "no_speech_prob": 3.8070120922384376e-07}, {"id": 663, "seek": 378760, "start": 3787.6, "end": 3789.12, "text": " in this part of the photo?", "tokens": [294, 341, 644, 295, 264, 5052, 30], "temperature": 0.0, "avg_logprob": -0.09389004707336426, "compression_ratio": 1.9953271028037383, "no_speech_prob": 4.637859092326835e-06}, {"id": 664, "seek": 378760, "start": 3789.12, "end": 3790.7999999999997, "text": " Is there a bear in this part of the photo?", "tokens": [1119, 456, 257, 6155, 294, 341, 644, 295, 264, 5052, 30], "temperature": 0.0, "avg_logprob": -0.09389004707336426, "compression_ratio": 1.9953271028037383, "no_speech_prob": 4.637859092326835e-06}, {"id": 665, "seek": 378760, "start": 3790.7999999999997, "end": 3792.72, "text": " Is there a bear in this part of the photo?", "tokens": [1119, 456, 257, 6155, 294, 341, 644, 295, 264, 5052, 30], "temperature": 0.0, "avg_logprob": -0.09389004707336426, "compression_ratio": 1.9953271028037383, "no_speech_prob": 4.637859092326835e-06}, {"id": 666, "seek": 378760, "start": 3792.72, "end": 3797.64, "text": " And then it takes the average of those 49 seven by seven predictions to decide whether", "tokens": [400, 550, 309, 2516, 264, 4274, 295, 729, 16513, 3407, 538, 3407, 21264, 281, 4536, 1968], "temperature": 0.0, "avg_logprob": -0.09389004707336426, "compression_ratio": 1.9953271028037383, "no_speech_prob": 4.637859092326835e-06}, {"id": 667, "seek": 378760, "start": 3797.64, "end": 3800.58, "text": " there's a bear in the photo.", "tokens": [456, 311, 257, 6155, 294, 264, 5052, 13], "temperature": 0.0, "avg_logprob": -0.09389004707336426, "compression_ratio": 1.9953271028037383, "no_speech_prob": 4.637859092326835e-06}, {"id": 668, "seek": 378760, "start": 3800.58, "end": 3805.64, "text": " That works very well if it's basically a photo of a bear, right?", "tokens": [663, 1985, 588, 731, 498, 309, 311, 1936, 257, 5052, 295, 257, 6155, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.09389004707336426, "compression_ratio": 1.9953271028037383, "no_speech_prob": 4.637859092326835e-06}, {"id": 669, "seek": 378760, "start": 3805.64, "end": 3809.4, "text": " Because most, you know, if the bear is big and takes up most of the frame, then most", "tokens": [1436, 881, 11, 291, 458, 11, 498, 264, 6155, 307, 955, 293, 2516, 493, 881, 295, 264, 3920, 11, 550, 881], "temperature": 0.0, "avg_logprob": -0.09389004707336426, "compression_ratio": 1.9953271028037383, "no_speech_prob": 4.637859092326835e-06}, {"id": 670, "seek": 378760, "start": 3809.4, "end": 3814.72, "text": " of those seven by seven bits are bits of a bear.", "tokens": [295, 729, 3407, 538, 3407, 9239, 366, 9239, 295, 257, 6155, 13], "temperature": 0.0, "avg_logprob": -0.09389004707336426, "compression_ratio": 1.9953271028037383, "no_speech_prob": 4.637859092326835e-06}, {"id": 671, "seek": 381472, "start": 3814.72, "end": 3820.56, "text": " On the other hand, if it's a teeny tiny bear in the corner, then potentially only one of", "tokens": [1282, 264, 661, 1011, 11, 498, 309, 311, 257, 48232, 5870, 6155, 294, 264, 4538, 11, 550, 7263, 787, 472, 295], "temperature": 0.0, "avg_logprob": -0.07428176475293709, "compression_ratio": 1.6607142857142858, "no_speech_prob": 6.577907356586365e-07}, {"id": 672, "seek": 381472, "start": 3820.56, "end": 3824.68, "text": " those 49 squares has a bear in it.", "tokens": [729, 16513, 19368, 575, 257, 6155, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.07428176475293709, "compression_ratio": 1.6607142857142858, "no_speech_prob": 6.577907356586365e-07}, {"id": 673, "seek": 381472, "start": 3824.68, "end": 3828.48, "text": " And even worse, if it's like a picture of lots and lots of different things, only one", "tokens": [400, 754, 5324, 11, 498, 309, 311, 411, 257, 3036, 295, 3195, 293, 3195, 295, 819, 721, 11, 787, 472], "temperature": 0.0, "avg_logprob": -0.07428176475293709, "compression_ratio": 1.6607142857142858, "no_speech_prob": 6.577907356586365e-07}, {"id": 674, "seek": 381472, "start": 3828.48, "end": 3833.74, "text": " of which is a bear, it could end up not being a great bear detector.", "tokens": [295, 597, 307, 257, 6155, 11, 309, 727, 917, 493, 406, 885, 257, 869, 6155, 25712, 13], "temperature": 0.0, "avg_logprob": -0.07428176475293709, "compression_ratio": 1.6607142857142858, "no_speech_prob": 6.577907356586365e-07}, {"id": 675, "seek": 381472, "start": 3833.74, "end": 3840.9599999999996, "text": " And so this is where like the details of how we construct our model turn out to be important.", "tokens": [400, 370, 341, 307, 689, 411, 264, 4365, 295, 577, 321, 7690, 527, 2316, 1261, 484, 281, 312, 1021, 13], "temperature": 0.0, "avg_logprob": -0.07428176475293709, "compression_ratio": 1.6607142857142858, "no_speech_prob": 6.577907356586365e-07}, {"id": 676, "seek": 384096, "start": 3840.96, "end": 3847.76, "text": " And so if you're trying to find like just one part of a photo that has a small bear", "tokens": [400, 370, 498, 291, 434, 1382, 281, 915, 411, 445, 472, 644, 295, 257, 5052, 300, 575, 257, 1359, 6155], "temperature": 0.0, "avg_logprob": -0.10049294290088472, "compression_ratio": 1.56, "no_speech_prob": 3.2887164707062766e-06}, {"id": 677, "seek": 384096, "start": 3847.76, "end": 3852.84, "text": " in it, you might decide to use maximum pooling instead of average pooling.", "tokens": [294, 309, 11, 291, 1062, 4536, 281, 764, 6674, 7005, 278, 2602, 295, 4274, 7005, 278, 13], "temperature": 0.0, "avg_logprob": -0.10049294290088472, "compression_ratio": 1.56, "no_speech_prob": 3.2887164707062766e-06}, {"id": 678, "seek": 384096, "start": 3852.84, "end": 3857.64, "text": " Because max pooling will just say, I think this is a picture of a bear if any one of", "tokens": [1436, 11469, 7005, 278, 486, 445, 584, 11, 286, 519, 341, 307, 257, 3036, 295, 257, 6155, 498, 604, 472, 295], "temperature": 0.0, "avg_logprob": -0.10049294290088472, "compression_ratio": 1.56, "no_speech_prob": 3.2887164707062766e-06}, {"id": 679, "seek": 384096, "start": 3857.64, "end": 3864.4, "text": " those 49 bits of my grid has something that looks like a bear in it.", "tokens": [729, 16513, 9239, 295, 452, 10748, 575, 746, 300, 1542, 411, 257, 6155, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.10049294290088472, "compression_ratio": 1.56, "no_speech_prob": 3.2887164707062766e-06}, {"id": 680, "seek": 386440, "start": 3864.4, "end": 3874.12, "text": " So these are potentially important details which often get hand waved over.", "tokens": [407, 613, 366, 7263, 1021, 4365, 597, 2049, 483, 1011, 261, 12865, 670, 13], "temperature": 0.0, "avg_logprob": -0.1767373899134194, "compression_ratio": 1.5380952380952382, "no_speech_prob": 6.893591262269183e-07}, {"id": 681, "seek": 386440, "start": 3874.12, "end": 3883.1600000000003, "text": " Although you know, again, like the key thing here is that this is happening right at the", "tokens": [5780, 291, 458, 11, 797, 11, 411, 264, 2141, 551, 510, 307, 300, 341, 307, 2737, 558, 412, 264], "temperature": 0.0, "avg_logprob": -0.1767373899134194, "compression_ratio": 1.5380952380952382, "no_speech_prob": 6.893591262269183e-07}, {"id": 682, "seek": 386440, "start": 3883.1600000000003, "end": 3884.7200000000003, "text": " very end, right?", "tokens": [588, 917, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1767373899134194, "compression_ratio": 1.5380952380952382, "no_speech_prob": 6.893591262269183e-07}, {"id": 683, "seek": 386440, "start": 3884.7200000000003, "end": 3886.52, "text": " That max pool or that average pool.", "tokens": [663, 11469, 7005, 420, 300, 4274, 7005, 13], "temperature": 0.0, "avg_logprob": -0.1767373899134194, "compression_ratio": 1.5380952380952382, "no_speech_prob": 6.893591262269183e-07}, {"id": 684, "seek": 386440, "start": 3886.52, "end": 3888.56, "text": " And actually Fast.ai handles this for you.", "tokens": [400, 767, 15968, 13, 1301, 18722, 341, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.1767373899134194, "compression_ratio": 1.5380952380952382, "no_speech_prob": 6.893591262269183e-07}, {"id": 685, "seek": 386440, "start": 3888.56, "end": 3892.7200000000003, "text": " We do a special thing which we kind of independently invented.", "tokens": [492, 360, 257, 2121, 551, 597, 321, 733, 295, 21761, 14479, 13], "temperature": 0.0, "avg_logprob": -0.1767373899134194, "compression_ratio": 1.5380952380952382, "no_speech_prob": 6.893591262269183e-07}, {"id": 686, "seek": 389272, "start": 3892.72, "end": 3898.68, "text": " I think we did it first, which is we do both max pool and average pool and we concatenate", "tokens": [286, 519, 321, 630, 309, 700, 11, 597, 307, 321, 360, 1293, 11469, 7005, 293, 4274, 7005, 293, 321, 1588, 7186, 473], "temperature": 0.0, "avg_logprob": -0.08029786109924317, "compression_ratio": 1.6196581196581197, "no_speech_prob": 3.966959866374964e-06}, {"id": 687, "seek": 389272, "start": 3898.68, "end": 3899.68, "text": " them together.", "tokens": [552, 1214, 13], "temperature": 0.0, "avg_logprob": -0.08029786109924317, "compression_ratio": 1.6196581196581197, "no_speech_prob": 3.966959866374964e-06}, {"id": 688, "seek": 389272, "start": 3899.68, "end": 3901.2799999999997, "text": " We call that concat pooling.", "tokens": [492, 818, 300, 1588, 267, 7005, 278, 13], "temperature": 0.0, "avg_logprob": -0.08029786109924317, "compression_ratio": 1.6196581196581197, "no_speech_prob": 3.966959866374964e-06}, {"id": 689, "seek": 389272, "start": 3901.2799999999997, "end": 3907.8799999999997, "text": " And that has since been reinvented in at least one paper.", "tokens": [400, 300, 575, 1670, 668, 33477, 292, 294, 412, 1935, 472, 3035, 13], "temperature": 0.0, "avg_logprob": -0.08029786109924317, "compression_ratio": 1.6196581196581197, "no_speech_prob": 3.966959866374964e-06}, {"id": 690, "seek": 389272, "start": 3907.8799999999997, "end": 3911.18, "text": " And so that means that you don't have to think too much about it because we're going to try", "tokens": [400, 370, 300, 1355, 300, 291, 500, 380, 362, 281, 519, 886, 709, 466, 309, 570, 321, 434, 516, 281, 853], "temperature": 0.0, "avg_logprob": -0.08029786109924317, "compression_ratio": 1.6196581196581197, "no_speech_prob": 3.966959866374964e-06}, {"id": 691, "seek": 389272, "start": 3911.18, "end": 3915.68, "text": " both for you basically.", "tokens": [1293, 337, 291, 1936, 13], "temperature": 0.0, "avg_logprob": -0.08029786109924317, "compression_ratio": 1.6196581196581197, "no_speech_prob": 3.966959866374964e-06}, {"id": 692, "seek": 389272, "start": 3915.68, "end": 3921.9599999999996, "text": " So I mentioned that this is actually really just matrix multiplication.", "tokens": [407, 286, 2835, 300, 341, 307, 767, 534, 445, 8141, 27290, 13], "temperature": 0.0, "avg_logprob": -0.08029786109924317, "compression_ratio": 1.6196581196581197, "no_speech_prob": 3.966959866374964e-06}, {"id": 693, "seek": 392196, "start": 3921.96, "end": 3929.48, "text": " And to show you that, I'm going to show you some images created by a guy called Matthew", "tokens": [400, 281, 855, 291, 300, 11, 286, 478, 516, 281, 855, 291, 512, 5267, 2942, 538, 257, 2146, 1219, 12434], "temperature": 0.0, "avg_logprob": -0.1900944043231267, "compression_ratio": 1.6211453744493391, "no_speech_prob": 8.664530469104648e-06}, {"id": 694, "seek": 392196, "start": 3929.48, "end": 3933.2, "text": " Kleinsmith who did this actually, I think this is in our very first ever course.", "tokens": [17053, 1292, 41708, 567, 630, 341, 767, 11, 286, 519, 341, 307, 294, 527, 588, 700, 1562, 1164, 13], "temperature": 0.0, "avg_logprob": -0.1900944043231267, "compression_ratio": 1.6211453744493391, "no_speech_prob": 8.664530469104648e-06}, {"id": 695, "seek": 392196, "start": 3933.2, "end": 3939.58, "text": " Might have been the part two, first part two course.", "tokens": [23964, 362, 668, 264, 644, 732, 11, 700, 644, 732, 1164, 13], "temperature": 0.0, "avg_logprob": -0.1900944043231267, "compression_ratio": 1.6211453744493391, "no_speech_prob": 8.664530469104648e-06}, {"id": 696, "seek": 392196, "start": 3939.58, "end": 3944.92, "text": " And he basically pointed out that in a certain way of thinking about it, it turns out that", "tokens": [400, 415, 1936, 10932, 484, 300, 294, 257, 1629, 636, 295, 1953, 466, 309, 11, 309, 4523, 484, 300], "temperature": 0.0, "avg_logprob": -0.1900944043231267, "compression_ratio": 1.6211453744493391, "no_speech_prob": 8.664530469104648e-06}, {"id": 697, "seek": 392196, "start": 3944.92, "end": 3948.92, "text": " convolution is the same thing as a matrix model player.", "tokens": [45216, 307, 264, 912, 551, 382, 257, 8141, 2316, 4256, 13], "temperature": 0.0, "avg_logprob": -0.1900944043231267, "compression_ratio": 1.6211453744493391, "no_speech_prob": 8.664530469104648e-06}, {"id": 698, "seek": 394892, "start": 3948.92, "end": 3952.28, "text": " So I want to show you how he shows this.", "tokens": [407, 286, 528, 281, 855, 291, 577, 415, 3110, 341, 13], "temperature": 0.0, "avg_logprob": -0.11341241888097815, "compression_ratio": 1.563953488372093, "no_speech_prob": 2.026134097832255e-06}, {"id": 699, "seek": 394892, "start": 3952.28, "end": 3959.56, "text": " He basically says, okay, let's take this three by three image and a two by two kernel containing", "tokens": [634, 1936, 1619, 11, 1392, 11, 718, 311, 747, 341, 1045, 538, 1045, 3256, 293, 257, 732, 538, 732, 28256, 19273], "temperature": 0.0, "avg_logprob": -0.11341241888097815, "compression_ratio": 1.563953488372093, "no_speech_prob": 2.026134097832255e-06}, {"id": 700, "seek": 394892, "start": 3959.56, "end": 3965.62, "text": " the coefficients alpha, beta, gamma, delta.", "tokens": [264, 31994, 8961, 11, 9861, 11, 15546, 11, 8289, 13], "temperature": 0.0, "avg_logprob": -0.11341241888097815, "compression_ratio": 1.563953488372093, "no_speech_prob": 2.026134097832255e-06}, {"id": 701, "seek": 394892, "start": 3965.62, "end": 3975.86, "text": " And so in this, as we slide the window over, each of the colors, each of the colors are", "tokens": [400, 370, 294, 341, 11, 382, 321, 4137, 264, 4910, 670, 11, 1184, 295, 264, 4577, 11, 1184, 295, 264, 4577, 366], "temperature": 0.0, "avg_logprob": -0.11341241888097815, "compression_ratio": 1.563953488372093, "no_speech_prob": 2.026134097832255e-06}, {"id": 702, "seek": 397586, "start": 3975.86, "end": 3980.76, "text": " multiplied together, red by red plus green by green plus, what is that, orange by orange", "tokens": [17207, 1214, 11, 2182, 538, 2182, 1804, 3092, 538, 3092, 1804, 11, 437, 307, 300, 11, 7671, 538, 7671], "temperature": 0.0, "avg_logprob": -0.1837939761933826, "compression_ratio": 1.6885245901639345, "no_speech_prob": 2.813009132296429e-06}, {"id": 703, "seek": 397586, "start": 3980.76, "end": 3983.7200000000003, "text": " plus blue by blue gives you this.", "tokens": [1804, 3344, 538, 3344, 2709, 291, 341, 13], "temperature": 0.0, "avg_logprob": -0.1837939761933826, "compression_ratio": 1.6885245901639345, "no_speech_prob": 2.813009132296429e-06}, {"id": 704, "seek": 397586, "start": 3983.7200000000003, "end": 3991.0, "text": " And so to put it another way, algebraically p equals alpha times a plus beta times b,", "tokens": [400, 370, 281, 829, 309, 1071, 636, 11, 21989, 984, 280, 6915, 8961, 1413, 257, 1804, 9861, 1413, 272, 11], "temperature": 0.0, "avg_logprob": -0.1837939761933826, "compression_ratio": 1.6885245901639345, "no_speech_prob": 2.813009132296429e-06}, {"id": 705, "seek": 397586, "start": 3991.0, "end": 3997.28, "text": " et cetera.", "tokens": [1030, 11458, 13], "temperature": 0.0, "avg_logprob": -0.1837939761933826, "compression_ratio": 1.6885245901639345, "no_speech_prob": 2.813009132296429e-06}, {"id": 706, "seek": 397586, "start": 3997.28, "end": 4002.6400000000003, "text": " And so then as we slide to this part, we're multiplying again, red by red, green by green", "tokens": [400, 370, 550, 382, 321, 4137, 281, 341, 644, 11, 321, 434, 30955, 797, 11, 2182, 538, 2182, 11, 3092, 538, 3092], "temperature": 0.0, "avg_logprob": -0.1837939761933826, "compression_ratio": 1.6885245901639345, "no_speech_prob": 2.813009132296429e-06}, {"id": 707, "seek": 400264, "start": 4002.64, "end": 4008.2799999999997, "text": " so forth, so we can say q equals alpha times b plus beta times c, et cetera.", "tokens": [370, 5220, 11, 370, 321, 393, 584, 9505, 6915, 8961, 1413, 272, 1804, 9861, 1413, 269, 11, 1030, 11458, 13], "temperature": 0.0, "avg_logprob": -0.08796107515375665, "compression_ratio": 1.4832535885167464, "no_speech_prob": 8.446220363111934e-07}, {"id": 708, "seek": 400264, "start": 4008.2799999999997, "end": 4013.3199999999997, "text": " And so this is how we calculate a convolution using the approach we just described as a", "tokens": [400, 370, 341, 307, 577, 321, 8873, 257, 45216, 1228, 264, 3109, 321, 445, 7619, 382, 257], "temperature": 0.0, "avg_logprob": -0.08796107515375665, "compression_ratio": 1.4832535885167464, "no_speech_prob": 8.446220363111934e-07}, {"id": 709, "seek": 400264, "start": 4013.3199999999997, "end": 4018.96, "text": " sliding window.", "tokens": [21169, 4910, 13], "temperature": 0.0, "avg_logprob": -0.08796107515375665, "compression_ratio": 1.4832535885167464, "no_speech_prob": 8.446220363111934e-07}, {"id": 710, "seek": 400264, "start": 4018.96, "end": 4022.8799999999997, "text": " But here's another way of thinking about it.", "tokens": [583, 510, 311, 1071, 636, 295, 1953, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.08796107515375665, "compression_ratio": 1.4832535885167464, "no_speech_prob": 8.446220363111934e-07}, {"id": 711, "seek": 400264, "start": 4022.8799999999997, "end": 4031.24, "text": " We could say, okay, we've got all these different things, a, b, c, d, e, f, g, h, j.", "tokens": [492, 727, 584, 11, 1392, 11, 321, 600, 658, 439, 613, 819, 721, 11, 257, 11, 272, 11, 269, 11, 274, 11, 308, 11, 283, 11, 290, 11, 276, 11, 361, 13], "temperature": 0.0, "avg_logprob": -0.08796107515375665, "compression_ratio": 1.4832535885167464, "no_speech_prob": 8.446220363111934e-07}, {"id": 712, "seek": 403124, "start": 4031.24, "end": 4039.04, "text": " Let's put them all into a single vector and then let's create a single matrix that has", "tokens": [961, 311, 829, 552, 439, 666, 257, 2167, 8062, 293, 550, 718, 311, 1884, 257, 2167, 8141, 300, 575], "temperature": 0.0, "avg_logprob": -0.08845539826613207, "compression_ratio": 1.7142857142857142, "no_speech_prob": 4.965280595570221e-07}, {"id": 713, "seek": 403124, "start": 4039.04, "end": 4044.6, "text": " alpha, alpha, alpha, alpha, beta, beta, beta, beta, et cetera.", "tokens": [8961, 11, 8961, 11, 8961, 11, 8961, 11, 9861, 11, 9861, 11, 9861, 11, 9861, 11, 1030, 11458, 13], "temperature": 0.0, "avg_logprob": -0.08845539826613207, "compression_ratio": 1.7142857142857142, "no_speech_prob": 4.965280595570221e-07}, {"id": 714, "seek": 403124, "start": 4044.6, "end": 4054.56, "text": " And then if we do this matrix multiplied by this vector, we get this with these gray zeros", "tokens": [400, 550, 498, 321, 360, 341, 8141, 17207, 538, 341, 8062, 11, 321, 483, 341, 365, 613, 10855, 35193], "temperature": 0.0, "avg_logprob": -0.08845539826613207, "compression_ratio": 1.7142857142857142, "no_speech_prob": 4.965280595570221e-07}, {"id": 715, "seek": 405456, "start": 4054.56, "end": 4063.68, "text": " in the appropriate places, which gives us this, which is the same as this.", "tokens": [294, 264, 6854, 3190, 11, 597, 2709, 505, 341, 11, 597, 307, 264, 912, 382, 341, 13], "temperature": 0.0, "avg_logprob": -0.0659901917870365, "compression_ratio": 1.6608187134502923, "no_speech_prob": 6.786733592889505e-07}, {"id": 716, "seek": 405456, "start": 4063.68, "end": 4070.92, "text": " And so this shows that a convolution is actually a special kind of matrix multiplication.", "tokens": [400, 370, 341, 3110, 300, 257, 45216, 307, 767, 257, 2121, 733, 295, 8141, 27290, 13], "temperature": 0.0, "avg_logprob": -0.0659901917870365, "compression_ratio": 1.6608187134502923, "no_speech_prob": 6.786733592889505e-07}, {"id": 717, "seek": 405456, "start": 4070.92, "end": 4075.84, "text": " It's a matrix multiplication where there are some zeros that are fixed and some numbers", "tokens": [467, 311, 257, 8141, 27290, 689, 456, 366, 512, 35193, 300, 366, 6806, 293, 512, 3547], "temperature": 0.0, "avg_logprob": -0.0659901917870365, "compression_ratio": 1.6608187134502923, "no_speech_prob": 6.786733592889505e-07}, {"id": 718, "seek": 405456, "start": 4075.84, "end": 4078.52, "text": " that are forced to be the same.", "tokens": [300, 366, 7579, 281, 312, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.0659901917870365, "compression_ratio": 1.6608187134502923, "no_speech_prob": 6.786733592889505e-07}, {"id": 719, "seek": 407852, "start": 4078.52, "end": 4086.68, "text": " Now in practice it's going to be faster to do it this way, but it's a useful kind of", "tokens": [823, 294, 3124, 309, 311, 516, 281, 312, 4663, 281, 360, 309, 341, 636, 11, 457, 309, 311, 257, 4420, 733, 295], "temperature": 0.0, "avg_logprob": -0.20534723896091267, "compression_ratio": 1.4557823129251701, "no_speech_prob": 2.813012770275236e-06}, {"id": 720, "seek": 407852, "start": 4086.68, "end": 4091.72, "text": " thing to think about, I think, that just to realize like, oh, it's just another of these", "tokens": [551, 281, 519, 466, 11, 286, 519, 11, 300, 445, 281, 4325, 411, 11, 1954, 11, 309, 311, 445, 1071, 295, 613], "temperature": 0.0, "avg_logprob": -0.20534723896091267, "compression_ratio": 1.4557823129251701, "no_speech_prob": 2.813012770275236e-06}, {"id": 721, "seek": 407852, "start": 4091.72, "end": 4098.84, "text": " special types of matrix multiplications.", "tokens": [2121, 3467, 295, 8141, 17596, 763, 13], "temperature": 0.0, "avg_logprob": -0.20534723896091267, "compression_ratio": 1.4557823129251701, "no_speech_prob": 2.813012770275236e-06}, {"id": 722, "seek": 409884, "start": 4098.84, "end": 4112.12, "text": " Okay, I think, well, let's look at one more thing because there was one other thing that", "tokens": [1033, 11, 286, 519, 11, 731, 11, 718, 311, 574, 412, 472, 544, 551, 570, 456, 390, 472, 661, 551, 300], "temperature": 0.0, "avg_logprob": -0.1017979621887207, "compression_ratio": 1.4331210191082802, "no_speech_prob": 5.014718226448167e-06}, {"id": 723, "seek": 409884, "start": 4112.12, "end": 4118.64, "text": " we saw and I mentioned we would look at in the tabular model, which is called dropout.", "tokens": [321, 1866, 293, 286, 2835, 321, 576, 574, 412, 294, 264, 4421, 1040, 2316, 11, 597, 307, 1219, 3270, 346, 13], "temperature": 0.0, "avg_logprob": -0.1017979621887207, "compression_ratio": 1.4331210191082802, "no_speech_prob": 5.014718226448167e-06}, {"id": 724, "seek": 409884, "start": 4118.64, "end": 4122.2, "text": " And I actually have this in my Excel spreadsheet.", "tokens": [400, 286, 767, 362, 341, 294, 452, 19060, 27733, 13], "temperature": 0.0, "avg_logprob": -0.1017979621887207, "compression_ratio": 1.4331210191082802, "no_speech_prob": 5.014718226448167e-06}, {"id": 725, "seek": 412220, "start": 4122.2, "end": 4132.76, "text": " If you go to the conv example dropout page, you'll see we've actually got a little bit", "tokens": [759, 291, 352, 281, 264, 3754, 1365, 3270, 346, 3028, 11, 291, 603, 536, 321, 600, 767, 658, 257, 707, 857], "temperature": 0.0, "avg_logprob": -0.10142420078146047, "compression_ratio": 1.6323529411764706, "no_speech_prob": 8.186358400052995e-07}, {"id": 726, "seek": 412220, "start": 4132.76, "end": 4133.88, "text": " more stuff here.", "tokens": [544, 1507, 510, 13], "temperature": 0.0, "avg_logprob": -0.10142420078146047, "compression_ratio": 1.6323529411764706, "no_speech_prob": 8.186358400052995e-07}, {"id": 727, "seek": 412220, "start": 4133.88, "end": 4139.2, "text": " We've got the same input as before and the same first convolution as before and the same", "tokens": [492, 600, 658, 264, 912, 4846, 382, 949, 293, 264, 912, 700, 45216, 382, 949, 293, 264, 912], "temperature": 0.0, "avg_logprob": -0.10142420078146047, "compression_ratio": 1.6323529411764706, "no_speech_prob": 8.186358400052995e-07}, {"id": 728, "seek": 412220, "start": 4139.2, "end": 4142.48, "text": " second convolution as before.", "tokens": [1150, 45216, 382, 949, 13], "temperature": 0.0, "avg_logprob": -0.10142420078146047, "compression_ratio": 1.6323529411764706, "no_speech_prob": 8.186358400052995e-07}, {"id": 729, "seek": 414248, "start": 4142.48, "end": 4152.5199999999995, "text": " And then we've got a bunch of random numbers.", "tokens": [400, 550, 321, 600, 658, 257, 3840, 295, 4974, 3547, 13], "temperature": 0.0, "avg_logprob": -0.16333176294962565, "compression_ratio": 1.6126760563380282, "no_speech_prob": 5.203567639000539e-07}, {"id": 730, "seek": 414248, "start": 4152.5199999999995, "end": 4157.5599999999995, "text": " They're showing as between 0 and 1, but they're actually, that's just because they're rounding", "tokens": [814, 434, 4099, 382, 1296, 1958, 293, 502, 11, 457, 436, 434, 767, 11, 300, 311, 445, 570, 436, 434, 48237], "temperature": 0.0, "avg_logprob": -0.16333176294962565, "compression_ratio": 1.6126760563380282, "no_speech_prob": 5.203567639000539e-07}, {"id": 731, "seek": 414248, "start": 4157.5599999999995, "end": 4166.839999999999, "text": " off, they're actually random numbers between, you know, that are floats between 0 and 1.", "tokens": [766, 11, 436, 434, 767, 4974, 3547, 1296, 11, 291, 458, 11, 300, 366, 37878, 1296, 1958, 293, 502, 13], "temperature": 0.0, "avg_logprob": -0.16333176294962565, "compression_ratio": 1.6126760563380282, "no_speech_prob": 5.203567639000539e-07}, {"id": 732, "seek": 416684, "start": 4166.84, "end": 4183.32, "text": " Over here, we're then saying if, let's have a look.", "tokens": [4886, 510, 11, 321, 434, 550, 1566, 498, 11, 718, 311, 362, 257, 574, 13], "temperature": 0.0, "avg_logprob": -0.2466700236002604, "compression_ratio": 1.2153846153846153, "no_speech_prob": 4.785075361724012e-06}, {"id": 733, "seek": 416684, "start": 4183.32, "end": 4189.88, "text": " So way up here, I'll zoom in a bit, I've got a dropout factor.", "tokens": [407, 636, 493, 510, 11, 286, 603, 8863, 294, 257, 857, 11, 286, 600, 658, 257, 3270, 346, 5952, 13], "temperature": 0.0, "avg_logprob": -0.2466700236002604, "compression_ratio": 1.2153846153846153, "no_speech_prob": 4.785075361724012e-06}, {"id": 734, "seek": 416684, "start": 4189.88, "end": 4192.88, "text": " Let's change this, say to 0.5.", "tokens": [961, 311, 1319, 341, 11, 584, 281, 1958, 13, 20, 13], "temperature": 0.0, "avg_logprob": -0.2466700236002604, "compression_ratio": 1.2153846153846153, "no_speech_prob": 4.785075361724012e-06}, {"id": 735, "seek": 416684, "start": 4192.88, "end": 4196.0, "text": " There we go.", "tokens": [821, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.2466700236002604, "compression_ratio": 1.2153846153846153, "no_speech_prob": 4.785075361724012e-06}, {"id": 736, "seek": 419600, "start": 4196.0, "end": 4202.36, "text": " So over here, this is something that says if the random number in the equivalent place", "tokens": [407, 670, 510, 11, 341, 307, 746, 300, 1619, 498, 264, 4974, 1230, 294, 264, 10344, 1081], "temperature": 0.0, "avg_logprob": -0.09404718735638787, "compression_ratio": 1.6521739130434783, "no_speech_prob": 1.3709561699215556e-06}, {"id": 737, "seek": 419600, "start": 4202.36, "end": 4208.32, "text": " is greater than 0.5, then 1, otherwise 0.", "tokens": [307, 5044, 813, 1958, 13, 20, 11, 550, 502, 11, 5911, 1958, 13], "temperature": 0.0, "avg_logprob": -0.09404718735638787, "compression_ratio": 1.6521739130434783, "no_speech_prob": 1.3709561699215556e-06}, {"id": 738, "seek": 419600, "start": 4208.32, "end": 4210.56, "text": " And so here's a whole bunch of 1s and 0s.", "tokens": [400, 370, 510, 311, 257, 1379, 3840, 295, 502, 82, 293, 1958, 82, 13], "temperature": 0.0, "avg_logprob": -0.09404718735638787, "compression_ratio": 1.6521739130434783, "no_speech_prob": 1.3709561699215556e-06}, {"id": 739, "seek": 419600, "start": 4210.56, "end": 4214.72, "text": " Now this thing here is called a dropout mask.", "tokens": [823, 341, 551, 510, 307, 1219, 257, 3270, 346, 6094, 13], "temperature": 0.0, "avg_logprob": -0.09404718735638787, "compression_ratio": 1.6521739130434783, "no_speech_prob": 1.3709561699215556e-06}, {"id": 740, "seek": 419600, "start": 4214.72, "end": 4223.96, "text": " Now what happens is we multiply over here, we multiply the dropout mask and we multiply", "tokens": [823, 437, 2314, 307, 321, 12972, 670, 510, 11, 321, 12972, 264, 3270, 346, 6094, 293, 321, 12972], "temperature": 0.0, "avg_logprob": -0.09404718735638787, "compression_ratio": 1.6521739130434783, "no_speech_prob": 1.3709561699215556e-06}, {"id": 741, "seek": 422396, "start": 4223.96, "end": 4228.08, "text": " it by our filtered image.", "tokens": [309, 538, 527, 37111, 3256, 13], "temperature": 0.0, "avg_logprob": -0.11718236832391649, "compression_ratio": 1.5414364640883977, "no_speech_prob": 8.315265631608781e-07}, {"id": 742, "seek": 422396, "start": 4228.08, "end": 4234.92, "text": " And what that means is we end up with exactly the same image we started with.", "tokens": [400, 437, 300, 1355, 307, 321, 917, 493, 365, 2293, 264, 912, 3256, 321, 1409, 365, 13], "temperature": 0.0, "avg_logprob": -0.11718236832391649, "compression_ratio": 1.5414364640883977, "no_speech_prob": 8.315265631608781e-07}, {"id": 743, "seek": 422396, "start": 4234.92, "end": 4237.04, "text": " Here's the image we started with.", "tokens": [1692, 311, 264, 3256, 321, 1409, 365, 13], "temperature": 0.0, "avg_logprob": -0.11718236832391649, "compression_ratio": 1.5414364640883977, "no_speech_prob": 8.315265631608781e-07}, {"id": 744, "seek": 422396, "start": 4237.04, "end": 4241.24, "text": " But it's corrupted.", "tokens": [583, 309, 311, 39480, 13], "temperature": 0.0, "avg_logprob": -0.11718236832391649, "compression_ratio": 1.5414364640883977, "no_speech_prob": 8.315265631608781e-07}, {"id": 745, "seek": 422396, "start": 4241.24, "end": 4243.36, "text": " Random bits of it have been deleted.", "tokens": [37603, 9239, 295, 309, 362, 668, 22981, 13], "temperature": 0.0, "avg_logprob": -0.11718236832391649, "compression_ratio": 1.5414364640883977, "no_speech_prob": 8.315265631608781e-07}, {"id": 746, "seek": 422396, "start": 4243.36, "end": 4250.6, "text": " And based on the amount of dropout we use, so if we change it to, say, 0.2, not very", "tokens": [400, 2361, 322, 264, 2372, 295, 3270, 346, 321, 764, 11, 370, 498, 321, 1319, 309, 281, 11, 584, 11, 1958, 13, 17, 11, 406, 588], "temperature": 0.0, "avg_logprob": -0.11718236832391649, "compression_ratio": 1.5414364640883977, "no_speech_prob": 8.315265631608781e-07}, {"id": 747, "seek": 425060, "start": 4250.6, "end": 4254.200000000001, "text": " much of it's deleted at all, so it's still very easy to recognize.", "tokens": [709, 295, 309, 311, 22981, 412, 439, 11, 370, 309, 311, 920, 588, 1858, 281, 5521, 13], "temperature": 0.0, "avg_logprob": -0.13867618613047142, "compression_ratio": 1.4216867469879517, "no_speech_prob": 8.059433866947074e-07}, {"id": 748, "seek": 425060, "start": 4254.200000000001, "end": 4259.92, "text": " Or else if we use lots of dropouts, say 0.8, it's almost impossible to see what the number", "tokens": [1610, 1646, 498, 321, 764, 3195, 295, 3270, 7711, 11, 584, 1958, 13, 23, 11, 309, 311, 1920, 6243, 281, 536, 437, 264, 1230], "temperature": 0.0, "avg_logprob": -0.13867618613047142, "compression_ratio": 1.4216867469879517, "no_speech_prob": 8.059433866947074e-07}, {"id": 749, "seek": 425060, "start": 4259.92, "end": 4263.64, "text": " was.", "tokens": [390, 13], "temperature": 0.0, "avg_logprob": -0.13867618613047142, "compression_ratio": 1.4216867469879517, "no_speech_prob": 8.059433866947074e-07}, {"id": 750, "seek": 425060, "start": 4263.64, "end": 4268.92, "text": " And then we use this as the input to the next layer.", "tokens": [400, 550, 321, 764, 341, 382, 264, 4846, 281, 264, 958, 4583, 13], "temperature": 0.0, "avg_logprob": -0.13867618613047142, "compression_ratio": 1.4216867469879517, "no_speech_prob": 8.059433866947074e-07}, {"id": 751, "seek": 425060, "start": 4268.92, "end": 4271.240000000001, "text": " So that seems weird.", "tokens": [407, 300, 2544, 3657, 13], "temperature": 0.0, "avg_logprob": -0.13867618613047142, "compression_ratio": 1.4216867469879517, "no_speech_prob": 8.059433866947074e-07}, {"id": 752, "seek": 427124, "start": 4271.24, "end": 4282.04, "text": " Why would we delete some data at random from our processed image, from our activations", "tokens": [1545, 576, 321, 12097, 512, 1412, 412, 4974, 490, 527, 18846, 3256, 11, 490, 527, 2430, 763], "temperature": 0.0, "avg_logprob": -0.0771592479862579, "compression_ratio": 1.5191256830601092, "no_speech_prob": 6.475933105321019e-07}, {"id": 753, "seek": 427124, "start": 4282.04, "end": 4284.84, "text": " after a layer of convolutions?", "tokens": [934, 257, 4583, 295, 3754, 15892, 30], "temperature": 0.0, "avg_logprob": -0.0771592479862579, "compression_ratio": 1.5191256830601092, "no_speech_prob": 6.475933105321019e-07}, {"id": 754, "seek": 427124, "start": 4284.84, "end": 4291.2, "text": " Well the reason is that a human is able to look at this corrupted image and still recognize", "tokens": [1042, 264, 1778, 307, 300, 257, 1952, 307, 1075, 281, 574, 412, 341, 39480, 3256, 293, 920, 5521], "temperature": 0.0, "avg_logprob": -0.0771592479862579, "compression_ratio": 1.5191256830601092, "no_speech_prob": 6.475933105321019e-07}, {"id": 755, "seek": 427124, "start": 4291.2, "end": 4293.24, "text": " it's a 7.", "tokens": [309, 311, 257, 1614, 13], "temperature": 0.0, "avg_logprob": -0.0771592479862579, "compression_ratio": 1.5191256830601092, "no_speech_prob": 6.475933105321019e-07}, {"id": 756, "seek": 427124, "start": 4293.24, "end": 4297.48, "text": " And the idea is that a computer should be able to as well.", "tokens": [400, 264, 1558, 307, 300, 257, 3820, 820, 312, 1075, 281, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.0771592479862579, "compression_ratio": 1.5191256830601092, "no_speech_prob": 6.475933105321019e-07}, {"id": 757, "seek": 429748, "start": 4297.48, "end": 4306.48, "text": " And if we randomly delete different bits of the activations each time, then the computer", "tokens": [400, 498, 321, 16979, 12097, 819, 9239, 295, 264, 2430, 763, 1184, 565, 11, 550, 264, 3820], "temperature": 0.0, "avg_logprob": -0.11268114462131407, "compression_ratio": 1.7142857142857142, "no_speech_prob": 4.5209338850327185e-07}, {"id": 758, "seek": 429748, "start": 4306.48, "end": 4314.44, "text": " is forced to learn the underlying real representation rather than overfitting.", "tokens": [307, 7579, 281, 1466, 264, 14217, 957, 10290, 2831, 813, 670, 69, 2414, 13], "temperature": 0.0, "avg_logprob": -0.11268114462131407, "compression_ratio": 1.7142857142857142, "no_speech_prob": 4.5209338850327185e-07}, {"id": 759, "seek": 429748, "start": 4314.44, "end": 4320.679999999999, "text": " You can think of this as data augmentation, but it's data augmentation not for the inputs,", "tokens": [509, 393, 519, 295, 341, 382, 1412, 14501, 19631, 11, 457, 309, 311, 1412, 14501, 19631, 406, 337, 264, 15743, 11], "temperature": 0.0, "avg_logprob": -0.11268114462131407, "compression_ratio": 1.7142857142857142, "no_speech_prob": 4.5209338850327185e-07}, {"id": 760, "seek": 429748, "start": 4320.679999999999, "end": 4323.36, "text": " but data augmentation for the activations.", "tokens": [457, 1412, 14501, 19631, 337, 264, 2430, 763, 13], "temperature": 0.0, "avg_logprob": -0.11268114462131407, "compression_ratio": 1.7142857142857142, "no_speech_prob": 4.5209338850327185e-07}, {"id": 761, "seek": 429748, "start": 4323.36, "end": 4327.08, "text": " So this is called a dropout layer.", "tokens": [407, 341, 307, 1219, 257, 3270, 346, 4583, 13], "temperature": 0.0, "avg_logprob": -0.11268114462131407, "compression_ratio": 1.7142857142857142, "no_speech_prob": 4.5209338850327185e-07}, {"id": 762, "seek": 432708, "start": 4327.08, "end": 4334.16, "text": " And so dropout layers are really helpful for avoiding overfitting.", "tokens": [400, 370, 3270, 346, 7914, 366, 534, 4961, 337, 20220, 670, 69, 2414, 13], "temperature": 0.0, "avg_logprob": -0.1111812418157404, "compression_ratio": 1.5253164556962024, "no_speech_prob": 4.356862518761773e-06}, {"id": 763, "seek": 432708, "start": 4334.16, "end": 4348.12, "text": " And you can decide how much you want to compromise between good generalization, so avoiding overfitting,", "tokens": [400, 291, 393, 4536, 577, 709, 291, 528, 281, 18577, 1296, 665, 2674, 2144, 11, 370, 20220, 670, 69, 2414, 11], "temperature": 0.0, "avg_logprob": -0.1111812418157404, "compression_ratio": 1.5253164556962024, "no_speech_prob": 4.356862518761773e-06}, {"id": 764, "seek": 432708, "start": 4348.12, "end": 4352.34, "text": " versus getting something that works really well on the training data.", "tokens": [5717, 1242, 746, 300, 1985, 534, 731, 322, 264, 3097, 1412, 13], "temperature": 0.0, "avg_logprob": -0.1111812418157404, "compression_ratio": 1.5253164556962024, "no_speech_prob": 4.356862518761773e-06}, {"id": 765, "seek": 435234, "start": 4352.34, "end": 4357.4800000000005, "text": " And so the more dropout you use, the less good it's going to be on the training data,", "tokens": [400, 370, 264, 544, 3270, 346, 291, 764, 11, 264, 1570, 665, 309, 311, 516, 281, 312, 322, 264, 3097, 1412, 11], "temperature": 0.0, "avg_logprob": -0.13190646852765764, "compression_ratio": 1.4142011834319526, "no_speech_prob": 1.3709549193663406e-06}, {"id": 766, "seek": 435234, "start": 4357.4800000000005, "end": 4363.52, "text": " but the better it ought to generalize.", "tokens": [457, 264, 1101, 309, 13416, 281, 2674, 1125, 13], "temperature": 0.0, "avg_logprob": -0.13190646852765764, "compression_ratio": 1.4142011834319526, "no_speech_prob": 1.3709549193663406e-06}, {"id": 767, "seek": 435234, "start": 4363.52, "end": 4372.360000000001, "text": " And so this comes from a paper by Jeffrey Hinton's group quite a few years ago now.", "tokens": [400, 370, 341, 1487, 490, 257, 3035, 538, 28721, 389, 12442, 311, 1594, 1596, 257, 1326, 924, 2057, 586, 13], "temperature": 0.0, "avg_logprob": -0.13190646852765764, "compression_ratio": 1.4142011834319526, "no_speech_prob": 1.3709549193663406e-06}, {"id": 768, "seek": 435234, "start": 4372.360000000001, "end": 4377.400000000001, "text": " Ruslan's now at Apple I think.", "tokens": [13155, 8658, 311, 586, 412, 6373, 286, 519, 13], "temperature": 0.0, "avg_logprob": -0.13190646852765764, "compression_ratio": 1.4142011834319526, "no_speech_prob": 1.3709549193663406e-06}, {"id": 769, "seek": 437740, "start": 4377.4, "end": 4383.799999999999, "text": " And then Kujewski and Hinton went on to found Google Brain.", "tokens": [400, 550, 591, 4579, 1023, 18020, 293, 389, 12442, 1437, 322, 281, 1352, 3329, 29783, 13], "temperature": 0.0, "avg_logprob": -0.17047001617123383, "compression_ratio": 1.5965665236051503, "no_speech_prob": 9.87460907708737e-07}, {"id": 770, "seek": 437740, "start": 4383.799999999999, "end": 4388.12, "text": " And so you can see here they've got this picture of a fully connected neural network, two layers", "tokens": [400, 370, 291, 393, 536, 510, 436, 600, 658, 341, 3036, 295, 257, 4498, 4582, 18161, 3209, 11, 732, 7914], "temperature": 0.0, "avg_logprob": -0.17047001617123383, "compression_ratio": 1.5965665236051503, "no_speech_prob": 9.87460907708737e-07}, {"id": 771, "seek": 437740, "start": 4388.12, "end": 4390.04, "text": " just like the one we built.", "tokens": [445, 411, 264, 472, 321, 3094, 13], "temperature": 0.0, "avg_logprob": -0.17047001617123383, "compression_ratio": 1.5965665236051503, "no_speech_prob": 9.87460907708737e-07}, {"id": 772, "seek": 437740, "start": 4390.04, "end": 4393.339999999999, "text": " And here, look, they're kind of randomly deleting some of the activations.", "tokens": [400, 510, 11, 574, 11, 436, 434, 733, 295, 16979, 48946, 512, 295, 264, 2430, 763, 13], "temperature": 0.0, "avg_logprob": -0.17047001617123383, "compression_ratio": 1.5965665236051503, "no_speech_prob": 9.87460907708737e-07}, {"id": 773, "seek": 437740, "start": 4393.339999999999, "end": 4396.12, "text": " And all that's left is these connections.", "tokens": [400, 439, 300, 311, 1411, 307, 613, 9271, 13], "temperature": 0.0, "avg_logprob": -0.17047001617123383, "compression_ratio": 1.5965665236051503, "no_speech_prob": 9.87460907708737e-07}, {"id": 774, "seek": 437740, "start": 4396.12, "end": 4403.839999999999, "text": " And so that's a different bunch that's going to be deleted each batch.", "tokens": [400, 370, 300, 311, 257, 819, 3840, 300, 311, 516, 281, 312, 22981, 1184, 15245, 13], "temperature": 0.0, "avg_logprob": -0.17047001617123383, "compression_ratio": 1.5965665236051503, "no_speech_prob": 9.87460907708737e-07}, {"id": 775, "seek": 440384, "start": 4403.84, "end": 4408.76, "text": " I thought this was an interesting point.", "tokens": [286, 1194, 341, 390, 364, 1880, 935, 13], "temperature": 0.0, "avg_logprob": -0.15586319217434177, "compression_ratio": 1.495049504950495, "no_speech_prob": 1.9947060536651406e-06}, {"id": 776, "seek": 440384, "start": 4408.76, "end": 4415.46, "text": " So dropout, which is super important, was actually developed in a master's thesis.", "tokens": [407, 3270, 346, 11, 597, 307, 1687, 1021, 11, 390, 767, 4743, 294, 257, 4505, 311, 22288, 13], "temperature": 0.0, "avg_logprob": -0.15586319217434177, "compression_ratio": 1.495049504950495, "no_speech_prob": 1.9947060536651406e-06}, {"id": 777, "seek": 440384, "start": 4415.46, "end": 4422.0, "text": " And it was rejected from the main neural networks conference, then called NIPS, now called NeurIPS.", "tokens": [400, 309, 390, 15749, 490, 264, 2135, 18161, 9590, 7586, 11, 550, 1219, 18482, 6273, 11, 586, 1219, 1734, 374, 40, 6273, 13], "temperature": 0.0, "avg_logprob": -0.15586319217434177, "compression_ratio": 1.495049504950495, "no_speech_prob": 1.9947060536651406e-06}, {"id": 778, "seek": 440384, "start": 4422.0, "end": 4431.72, "text": " So it ended up being disseminated through Archive, which is a preprint server.", "tokens": [407, 309, 4590, 493, 885, 34585, 770, 807, 10984, 488, 11, 597, 307, 257, 659, 14030, 7154, 13], "temperature": 0.0, "avg_logprob": -0.15586319217434177, "compression_ratio": 1.495049504950495, "no_speech_prob": 1.9947060536651406e-06}, {"id": 779, "seek": 443172, "start": 4431.72, "end": 4439.8, "text": " And as has just been pointed out on our chat, that Ilya was one of the founders of OpenAI.", "tokens": [400, 382, 575, 445, 668, 10932, 484, 322, 527, 5081, 11, 300, 286, 45106, 390, 472, 295, 264, 25608, 295, 7238, 48698, 13], "temperature": 0.0, "avg_logprob": -0.1634965624128069, "compression_ratio": 1.471111111111111, "no_speech_prob": 4.495063876674976e-06}, {"id": 780, "seek": 443172, "start": 4439.8, "end": 4440.96, "text": " I don't know what happened to Nitish.", "tokens": [286, 500, 380, 458, 437, 2011, 281, 37942, 742, 13], "temperature": 0.0, "avg_logprob": -0.1634965624128069, "compression_ratio": 1.471111111111111, "no_speech_prob": 4.495063876674976e-06}, {"id": 781, "seek": 443172, "start": 4440.96, "end": 4446.08, "text": " I think he went to Google Brain as well, maybe.", "tokens": [286, 519, 415, 1437, 281, 3329, 29783, 382, 731, 11, 1310, 13], "temperature": 0.0, "avg_logprob": -0.1634965624128069, "compression_ratio": 1.471111111111111, "no_speech_prob": 4.495063876674976e-06}, {"id": 782, "seek": 443172, "start": 4446.08, "end": 4455.6, "text": " So peer review is a very fallible thing in both directions.", "tokens": [407, 15108, 3131, 307, 257, 588, 2100, 964, 551, 294, 1293, 11095, 13], "temperature": 0.0, "avg_logprob": -0.1634965624128069, "compression_ratio": 1.471111111111111, "no_speech_prob": 4.495063876674976e-06}, {"id": 783, "seek": 443172, "start": 4455.6, "end": 4459.52, "text": " And it's great that we have preprint servers so we can read stuff like this, even if reviewers", "tokens": [400, 309, 311, 869, 300, 321, 362, 659, 14030, 15909, 370, 321, 393, 1401, 1507, 411, 341, 11, 754, 498, 45837], "temperature": 0.0, "avg_logprob": -0.1634965624128069, "compression_ratio": 1.471111111111111, "no_speech_prob": 4.495063876674976e-06}, {"id": 784, "seek": 445952, "start": 4459.52, "end": 4462.76, "text": " decide it's not worthy.", "tokens": [4536, 309, 311, 406, 14829, 13], "temperature": 0.0, "avg_logprob": -0.16725767111476464, "compression_ratio": 1.5164835164835164, "no_speech_prob": 5.255340511212125e-06}, {"id": 785, "seek": 445952, "start": 4462.76, "end": 4468.88, "text": " It's been one of the most important papers ever.", "tokens": [467, 311, 668, 472, 295, 264, 881, 1021, 10577, 1562, 13], "temperature": 0.0, "avg_logprob": -0.16725767111476464, "compression_ratio": 1.5164835164835164, "no_speech_prob": 5.255340511212125e-06}, {"id": 786, "seek": 445952, "start": 4468.88, "end": 4472.200000000001, "text": " OK.", "tokens": [2264, 13], "temperature": 0.0, "avg_logprob": -0.16725767111476464, "compression_ratio": 1.5164835164835164, "no_speech_prob": 5.255340511212125e-06}, {"id": 787, "seek": 445952, "start": 4472.200000000001, "end": 4477.040000000001, "text": " Now I think that's given us a good tour now.", "tokens": [823, 286, 519, 300, 311, 2212, 505, 257, 665, 3512, 586, 13], "temperature": 0.0, "avg_logprob": -0.16725767111476464, "compression_ratio": 1.5164835164835164, "no_speech_prob": 5.255340511212125e-06}, {"id": 788, "seek": 445952, "start": 4477.040000000001, "end": 4481.040000000001, "text": " We've really seen quite a few ways of dealing with input to a neural network, quite a few", "tokens": [492, 600, 534, 1612, 1596, 257, 1326, 2098, 295, 6260, 365, 4846, 281, 257, 18161, 3209, 11, 1596, 257, 1326], "temperature": 0.0, "avg_logprob": -0.16725767111476464, "compression_ratio": 1.5164835164835164, "no_speech_prob": 5.255340511212125e-06}, {"id": 789, "seek": 445952, "start": 4481.040000000001, "end": 4484.4400000000005, "text": " of the things that can happen in the middle of a neural network.", "tokens": [295, 264, 721, 300, 393, 1051, 294, 264, 2808, 295, 257, 18161, 3209, 13], "temperature": 0.0, "avg_logprob": -0.16725767111476464, "compression_ratio": 1.5164835164835164, "no_speech_prob": 5.255340511212125e-06}, {"id": 790, "seek": 448444, "start": 4484.44, "end": 4491.919999999999, "text": " We've only talked about rectified linear units, which is this one here.", "tokens": [492, 600, 787, 2825, 466, 11048, 2587, 8213, 6815, 11, 597, 307, 341, 472, 510, 13], "temperature": 0.0, "avg_logprob": -0.17277709386681997, "compression_ratio": 1.5272727272727273, "no_speech_prob": 4.029395768156974e-06}, {"id": 791, "seek": 448444, "start": 4491.919999999999, "end": 4496.12, "text": " 0 if x is less than 0 or x otherwise.", "tokens": [1958, 498, 2031, 307, 1570, 813, 1958, 420, 2031, 5911, 13], "temperature": 0.0, "avg_logprob": -0.17277709386681997, "compression_ratio": 1.5272727272727273, "no_speech_prob": 4.029395768156974e-06}, {"id": 792, "seek": 448444, "start": 4496.12, "end": 4500.599999999999, "text": " These are some of the other activations you can use.", "tokens": [1981, 366, 512, 295, 264, 661, 2430, 763, 291, 393, 764, 13], "temperature": 0.0, "avg_logprob": -0.17277709386681997, "compression_ratio": 1.5272727272727273, "no_speech_prob": 4.029395768156974e-06}, {"id": 793, "seek": 448444, "start": 4500.599999999999, "end": 4504.96, "text": " Don't use this one, of course, because you end up with a linear model.", "tokens": [1468, 380, 764, 341, 472, 11, 295, 1164, 11, 570, 291, 917, 493, 365, 257, 8213, 2316, 13], "temperature": 0.0, "avg_logprob": -0.17277709386681997, "compression_ratio": 1.5272727272727273, "no_speech_prob": 4.029395768156974e-06}, {"id": 794, "seek": 448444, "start": 4504.96, "end": 4507.799999999999, "text": " But they're all just different functions.", "tokens": [583, 436, 434, 439, 445, 819, 6828, 13], "temperature": 0.0, "avg_logprob": -0.17277709386681997, "compression_ratio": 1.5272727272727273, "no_speech_prob": 4.029395768156974e-06}, {"id": 795, "seek": 448444, "start": 4507.799999999999, "end": 4512.4, "text": " I should mention, it turns out these don't matter very much.", "tokens": [286, 820, 2152, 11, 309, 4523, 484, 613, 500, 380, 1871, 588, 709, 13], "temperature": 0.0, "avg_logprob": -0.17277709386681997, "compression_ratio": 1.5272727272727273, "no_speech_prob": 4.029395768156974e-06}, {"id": 796, "seek": 451240, "start": 4512.4, "end": 4519.16, "text": " Pretty much any non-linearity works fine.", "tokens": [10693, 709, 604, 2107, 12, 1889, 17409, 1985, 2489, 13], "temperature": 0.0, "avg_logprob": -0.1710160823350542, "compression_ratio": 1.5784753363228698, "no_speech_prob": 5.014654107071692e-06}, {"id": 797, "seek": 451240, "start": 4519.16, "end": 4523.04, "text": " So we don't spend much time talking about activation functions, even in part two of", "tokens": [407, 321, 500, 380, 3496, 709, 565, 1417, 466, 24433, 6828, 11, 754, 294, 644, 732, 295], "temperature": 0.0, "avg_logprob": -0.1710160823350542, "compression_ratio": 1.5784753363228698, "no_speech_prob": 5.014654107071692e-06}, {"id": 798, "seek": 451240, "start": 4523.04, "end": 4526.04, "text": " the course, just a little bit.", "tokens": [264, 1164, 11, 445, 257, 707, 857, 13], "temperature": 0.0, "avg_logprob": -0.1710160823350542, "compression_ratio": 1.5784753363228698, "no_speech_prob": 5.014654107071692e-06}, {"id": 799, "seek": 451240, "start": 4526.04, "end": 4529.599999999999, "text": " So yeah, so we understand there are inputs.", "tokens": [407, 1338, 11, 370, 321, 1223, 456, 366, 15743, 13], "temperature": 0.0, "avg_logprob": -0.1710160823350542, "compression_ratio": 1.5784753363228698, "no_speech_prob": 5.014654107071692e-06}, {"id": 800, "seek": 451240, "start": 4529.599999999999, "end": 4536.96, "text": " They can be one-hot encoded or embeddings, which is a computational shortcut.", "tokens": [814, 393, 312, 472, 12, 12194, 2058, 12340, 420, 12240, 29432, 11, 597, 307, 257, 28270, 24822, 13], "temperature": 0.0, "avg_logprob": -0.1710160823350542, "compression_ratio": 1.5784753363228698, "no_speech_prob": 5.014654107071692e-06}, {"id": 801, "seek": 451240, "start": 4536.96, "end": 4541.92, "text": " There are sandwich layers of matrix multipliers and activation functions.", "tokens": [821, 366, 11141, 7914, 295, 8141, 12788, 4890, 293, 24433, 6828, 13], "temperature": 0.0, "avg_logprob": -0.1710160823350542, "compression_ratio": 1.5784753363228698, "no_speech_prob": 5.014654107071692e-06}, {"id": 802, "seek": 454192, "start": 4541.92, "end": 4549.4800000000005, "text": " The matrix multipliers can sometimes be special cases, such as the convolutions or the embeddings.", "tokens": [440, 8141, 12788, 4890, 393, 2171, 312, 2121, 3331, 11, 1270, 382, 264, 3754, 15892, 420, 264, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.16666429815158038, "compression_ratio": 1.554945054945055, "no_speech_prob": 3.4465588214516174e-06}, {"id": 803, "seek": 454192, "start": 4549.4800000000005, "end": 4554.04, "text": " The output can go through some tweaking, such as the softmax.", "tokens": [440, 5598, 393, 352, 807, 512, 6986, 2456, 11, 1270, 382, 264, 2787, 41167, 13], "temperature": 0.0, "avg_logprob": -0.16666429815158038, "compression_ratio": 1.554945054945055, "no_speech_prob": 3.4465588214516174e-06}, {"id": 804, "seek": 454192, "start": 4554.04, "end": 4560.96, "text": " And then of course, you've got the loss function, such as cross entropy loss or mean squared", "tokens": [400, 550, 295, 1164, 11, 291, 600, 658, 264, 4470, 2445, 11, 1270, 382, 3278, 30867, 4470, 420, 914, 8889], "temperature": 0.0, "avg_logprob": -0.16666429815158038, "compression_ratio": 1.554945054945055, "no_speech_prob": 3.4465588214516174e-06}, {"id": 805, "seek": 454192, "start": 4560.96, "end": 4563.2, "text": " error or mean absolute error.", "tokens": [6713, 420, 914, 8236, 6713, 13], "temperature": 0.0, "avg_logprob": -0.16666429815158038, "compression_ratio": 1.554945054945055, "no_speech_prob": 3.4465588214516174e-06}, {"id": 806, "seek": 456320, "start": 4563.2, "end": 4573.84, "text": " But there's nothing too crazy going on in there.", "tokens": [583, 456, 311, 1825, 886, 3219, 516, 322, 294, 456, 13], "temperature": 0.0, "avg_logprob": -0.09955218960257138, "compression_ratio": 1.5379746835443038, "no_speech_prob": 1.9333481304784073e-06}, {"id": 807, "seek": 456320, "start": 4573.84, "end": 4579.36, "text": " So I feel like we've got a good sense now of what goes inside a wide range of neural", "tokens": [407, 286, 841, 411, 321, 600, 658, 257, 665, 2020, 586, 295, 437, 1709, 1854, 257, 4874, 3613, 295, 18161], "temperature": 0.0, "avg_logprob": -0.09955218960257138, "compression_ratio": 1.5379746835443038, "no_speech_prob": 1.9333481304784073e-06}, {"id": 808, "seek": 456320, "start": 4579.36, "end": 4580.36, "text": " nets.", "tokens": [36170, 13], "temperature": 0.0, "avg_logprob": -0.09955218960257138, "compression_ratio": 1.5379746835443038, "no_speech_prob": 1.9333481304784073e-06}, {"id": 809, "seek": 456320, "start": 4580.36, "end": 4583.5199999999995, "text": " You're not going to see anything too weird from here.", "tokens": [509, 434, 406, 516, 281, 536, 1340, 886, 3657, 490, 510, 13], "temperature": 0.0, "avg_logprob": -0.09955218960257138, "compression_ratio": 1.5379746835443038, "no_speech_prob": 1.9333481304784073e-06}, {"id": 810, "seek": 456320, "start": 4583.5199999999995, "end": 4587.5, "text": " And we've also seen a wide range of applications.", "tokens": [400, 321, 600, 611, 1612, 257, 4874, 3613, 295, 5821, 13], "temperature": 0.0, "avg_logprob": -0.09955218960257138, "compression_ratio": 1.5379746835443038, "no_speech_prob": 1.9333481304784073e-06}, {"id": 811, "seek": 458750, "start": 4587.5, "end": 4597.32, "text": " So before you come back to do part two, you know, what now?", "tokens": [407, 949, 291, 808, 646, 281, 360, 644, 732, 11, 291, 458, 11, 437, 586, 30], "temperature": 0.0, "avg_logprob": -0.1051344536898429, "compression_ratio": 1.3308823529411764, "no_speech_prob": 7.811463547113817e-07}, {"id": 812, "seek": 458750, "start": 4597.32, "end": 4601.28, "text": " And we're going to have a little AMA session here.", "tokens": [400, 321, 434, 516, 281, 362, 257, 707, 6475, 32, 5481, 510, 13], "temperature": 0.0, "avg_logprob": -0.1051344536898429, "compression_ratio": 1.3308823529411764, "no_speech_prob": 7.811463547113817e-07}, {"id": 813, "seek": 458750, "start": 4601.28, "end": 4604.88, "text": " And in fact, one of the questions was what now?", "tokens": [400, 294, 1186, 11, 472, 295, 264, 1651, 390, 437, 586, 30], "temperature": 0.0, "avg_logprob": -0.1051344536898429, "compression_ratio": 1.3308823529411764, "no_speech_prob": 7.811463547113817e-07}, {"id": 814, "seek": 458750, "start": 4604.88, "end": 4610.86, "text": " So this is quite good.", "tokens": [407, 341, 307, 1596, 665, 13], "temperature": 0.0, "avg_logprob": -0.1051344536898429, "compression_ratio": 1.3308823529411764, "no_speech_prob": 7.811463547113817e-07}, {"id": 815, "seek": 461086, "start": 4610.86, "end": 4620.4, "text": " One thing I strongly suggest is if you've got this far, it's probably worth you investing", "tokens": [1485, 551, 286, 10613, 3402, 307, 498, 291, 600, 658, 341, 1400, 11, 309, 311, 1391, 3163, 291, 10978], "temperature": 0.0, "avg_logprob": -0.0717801893911054, "compression_ratio": 1.4567901234567902, "no_speech_prob": 1.9033740272789146e-06}, {"id": 816, "seek": 461086, "start": 4620.4, "end": 4629.599999999999, "text": " your time in reading Radek's book, which is meta-learning.", "tokens": [428, 565, 294, 3760, 497, 762, 74, 311, 1446, 11, 597, 307, 19616, 12, 47204, 13], "temperature": 0.0, "avg_logprob": -0.0717801893911054, "compression_ratio": 1.4567901234567902, "no_speech_prob": 1.9033740272789146e-06}, {"id": 817, "seek": 461086, "start": 4629.599999999999, "end": 4635.96, "text": " And so meta-learning is very heavily based on the kind of teachings of fast AI over the", "tokens": [400, 370, 19616, 12, 47204, 307, 588, 10950, 2361, 322, 264, 733, 295, 21037, 295, 2370, 7318, 670, 264], "temperature": 0.0, "avg_logprob": -0.0717801893911054, "compression_ratio": 1.4567901234567902, "no_speech_prob": 1.9033740272789146e-06}, {"id": 818, "seek": 463596, "start": 4635.96, "end": 4644.68, "text": " last few years and is all about how to learn deep learning and learn pretty much anything.", "tokens": [1036, 1326, 924, 293, 307, 439, 466, 577, 281, 1466, 2452, 2539, 293, 1466, 1238, 709, 1340, 13], "temperature": 0.0, "avg_logprob": -0.1586416448865618, "compression_ratio": 1.5179856115107915, "no_speech_prob": 4.289210210117744e-06}, {"id": 819, "seek": 463596, "start": 4644.68, "end": 4650.4800000000005, "text": " Yeah, because you know, you've got to this point, you may as well know how to get to", "tokens": [865, 11, 570, 291, 458, 11, 291, 600, 658, 281, 341, 935, 11, 291, 815, 382, 731, 458, 577, 281, 483, 281], "temperature": 0.0, "avg_logprob": -0.1586416448865618, "compression_ratio": 1.5179856115107915, "no_speech_prob": 4.289210210117744e-06}, {"id": 820, "seek": 463596, "start": 4650.4800000000005, "end": 4660.8, "text": " the next point as well as possible.", "tokens": [264, 958, 935, 382, 731, 382, 1944, 13], "temperature": 0.0, "avg_logprob": -0.1586416448865618, "compression_ratio": 1.5179856115107915, "no_speech_prob": 4.289210210117744e-06}, {"id": 821, "seek": 466080, "start": 4660.8, "end": 4673.24, "text": " And the main thing you'll see that Radek talks about, or one of the main things, is practicing", "tokens": [400, 264, 2135, 551, 291, 603, 536, 300, 497, 762, 74, 6686, 466, 11, 420, 472, 295, 264, 2135, 721, 11, 307, 11350], "temperature": 0.0, "avg_logprob": -0.15456030919001654, "compression_ratio": 1.4709677419354839, "no_speech_prob": 1.1015772543032654e-06}, {"id": 822, "seek": 466080, "start": 4673.24, "end": 4675.72, "text": " and writing.", "tokens": [293, 3579, 13], "temperature": 0.0, "avg_logprob": -0.15456030919001654, "compression_ratio": 1.4709677419354839, "no_speech_prob": 1.1015772543032654e-06}, {"id": 823, "seek": 466080, "start": 4675.72, "end": 4685.4800000000005, "text": " So if you've kind of zipped through the videos on 2x and haven't done any exercises, go back", "tokens": [407, 498, 291, 600, 733, 295, 710, 5529, 807, 264, 2145, 322, 568, 87, 293, 2378, 380, 1096, 604, 11900, 11, 352, 646], "temperature": 0.0, "avg_logprob": -0.15456030919001654, "compression_ratio": 1.4709677419354839, "no_speech_prob": 1.1015772543032654e-06}, {"id": 824, "seek": 466080, "start": 4685.4800000000005, "end": 4686.4800000000005, "text": " and watch the videos again.", "tokens": [293, 1159, 264, 2145, 797, 13], "temperature": 0.0, "avg_logprob": -0.15456030919001654, "compression_ratio": 1.4709677419354839, "no_speech_prob": 1.1015772543032654e-06}, {"id": 825, "seek": 468648, "start": 4686.48, "end": 4691.08, "text": " You know, a lot of the best students end up watching them two or three times, probably", "tokens": [509, 458, 11, 257, 688, 295, 264, 1151, 1731, 917, 493, 1976, 552, 732, 420, 1045, 1413, 11, 1391], "temperature": 0.0, "avg_logprob": -0.14173912707670236, "compression_ratio": 1.5833333333333333, "no_speech_prob": 7.29595603843336e-06}, {"id": 826, "seek": 468648, "start": 4691.08, "end": 4694.12, "text": " more like three times.", "tokens": [544, 411, 1045, 1413, 13], "temperature": 0.0, "avg_logprob": -0.14173912707670236, "compression_ratio": 1.5833333333333333, "no_speech_prob": 7.29595603843336e-06}, {"id": 827, "seek": 468648, "start": 4694.12, "end": 4700.48, "text": " And actually go through and code as you watch and experiment.", "tokens": [400, 767, 352, 807, 293, 3089, 382, 291, 1159, 293, 5120, 13], "temperature": 0.0, "avg_logprob": -0.14173912707670236, "compression_ratio": 1.5833333333333333, "no_speech_prob": 7.29595603843336e-06}, {"id": 828, "seek": 468648, "start": 4700.48, "end": 4706.16, "text": " You know, write posts, blog posts about what you're doing.", "tokens": [509, 458, 11, 2464, 12300, 11, 6968, 12300, 466, 437, 291, 434, 884, 13], "temperature": 0.0, "avg_logprob": -0.14173912707670236, "compression_ratio": 1.5833333333333333, "no_speech_prob": 7.29595603843336e-06}, {"id": 829, "seek": 468648, "start": 4706.16, "end": 4713.5599999999995, "text": " Spend time on the forum, both helping others and seeing other people's answers to questions.", "tokens": [1738, 521, 565, 322, 264, 17542, 11, 1293, 4315, 2357, 293, 2577, 661, 561, 311, 6338, 281, 1651, 13], "temperature": 0.0, "avg_logprob": -0.14173912707670236, "compression_ratio": 1.5833333333333333, "no_speech_prob": 7.29595603843336e-06}, {"id": 830, "seek": 471356, "start": 4713.56, "end": 4718.400000000001, "text": " Read the success stories on the forum and of people's projects to get inspiration for", "tokens": [17604, 264, 2245, 3676, 322, 264, 17542, 293, 295, 561, 311, 4455, 281, 483, 10249, 337], "temperature": 0.0, "avg_logprob": -0.18310922191989037, "compression_ratio": 1.6181818181818182, "no_speech_prob": 6.438743184844498e-06}, {"id": 831, "seek": 471356, "start": 4718.400000000001, "end": 4721.56, "text": " things you could try.", "tokens": [721, 291, 727, 853, 13], "temperature": 0.0, "avg_logprob": -0.18310922191989037, "compression_ratio": 1.6181818181818182, "no_speech_prob": 6.438743184844498e-06}, {"id": 832, "seek": 471356, "start": 4721.56, "end": 4725.6, "text": " One of the most important things to do is to get together with other people.", "tokens": [1485, 295, 264, 881, 1021, 721, 281, 360, 307, 281, 483, 1214, 365, 661, 561, 13], "temperature": 0.0, "avg_logprob": -0.18310922191989037, "compression_ratio": 1.6181818181818182, "no_speech_prob": 6.438743184844498e-06}, {"id": 833, "seek": 471356, "start": 4725.6, "end": 4731.4800000000005, "text": " For example, you can do a Zoom study group, in fact on our Discord, which you can find", "tokens": [1171, 1365, 11, 291, 393, 360, 257, 13453, 2979, 1594, 11, 294, 1186, 322, 527, 32623, 11, 597, 291, 393, 915], "temperature": 0.0, "avg_logprob": -0.18310922191989037, "compression_ratio": 1.6181818181818182, "no_speech_prob": 6.438743184844498e-06}, {"id": 834, "seek": 471356, "start": 4731.4800000000005, "end": 4732.96, "text": " through our forum.", "tokens": [807, 527, 17542, 13], "temperature": 0.0, "avg_logprob": -0.18310922191989037, "compression_ratio": 1.6181818181818182, "no_speech_prob": 6.438743184844498e-06}, {"id": 835, "seek": 471356, "start": 4732.96, "end": 4735.620000000001, "text": " There's always study groups going on.", "tokens": [821, 311, 1009, 2979, 3935, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.18310922191989037, "compression_ratio": 1.6181818181818182, "no_speech_prob": 6.438743184844498e-06}, {"id": 836, "seek": 471356, "start": 4735.620000000001, "end": 4737.64, "text": " Or you can create your own.", "tokens": [1610, 291, 393, 1884, 428, 1065, 13], "temperature": 0.0, "avg_logprob": -0.18310922191989037, "compression_ratio": 1.6181818181818182, "no_speech_prob": 6.438743184844498e-06}, {"id": 837, "seek": 473764, "start": 4737.64, "end": 4744.04, "text": " You know, a study group to go through the book together.", "tokens": [509, 458, 11, 257, 2979, 1594, 281, 352, 807, 264, 1446, 1214, 13], "temperature": 0.0, "avg_logprob": -0.20493663625514252, "compression_ratio": 1.6150234741784038, "no_speech_prob": 9.368201972392853e-06}, {"id": 838, "seek": 473764, "start": 4744.04, "end": 4746.0, "text": " And of course, build stuff.", "tokens": [400, 295, 1164, 11, 1322, 1507, 13], "temperature": 0.0, "avg_logprob": -0.20493663625514252, "compression_ratio": 1.6150234741784038, "no_speech_prob": 9.368201972392853e-06}, {"id": 839, "seek": 473764, "start": 4746.0, "end": 4754.400000000001, "text": " And sometimes it's tricky to always be able to build stuff for work, because maybe you're", "tokens": [400, 2171, 309, 311, 12414, 281, 1009, 312, 1075, 281, 1322, 1507, 337, 589, 11, 570, 1310, 291, 434], "temperature": 0.0, "avg_logprob": -0.20493663625514252, "compression_ratio": 1.6150234741784038, "no_speech_prob": 9.368201972392853e-06}, {"id": 840, "seek": 473764, "start": 4754.400000000001, "end": 4758.12, "text": " not quite in the right area or they're not quite ready to try out deep learning yet.", "tokens": [406, 1596, 294, 264, 558, 1859, 420, 436, 434, 406, 1596, 1919, 281, 853, 484, 2452, 2539, 1939, 13], "temperature": 0.0, "avg_logprob": -0.20493663625514252, "compression_ratio": 1.6150234741784038, "no_speech_prob": 9.368201972392853e-06}, {"id": 841, "seek": 473764, "start": 4758.12, "end": 4761.56, "text": " But that's okay.", "tokens": [583, 300, 311, 1392, 13], "temperature": 0.0, "avg_logprob": -0.20493663625514252, "compression_ratio": 1.6150234741784038, "no_speech_prob": 9.368201972392853e-06}, {"id": 842, "seek": 473764, "start": 4761.56, "end": 4764.08, "text": " You know, build some hobby projects.", "tokens": [509, 458, 11, 1322, 512, 18240, 4455, 13], "temperature": 0.0, "avg_logprob": -0.20493663625514252, "compression_ratio": 1.6150234741784038, "no_speech_prob": 9.368201972392853e-06}, {"id": 843, "seek": 473764, "start": 4764.08, "end": 4765.08, "text": " Build some stuff just for fun.", "tokens": [11875, 512, 1507, 445, 337, 1019, 13], "temperature": 0.0, "avg_logprob": -0.20493663625514252, "compression_ratio": 1.6150234741784038, "no_speech_prob": 9.368201972392853e-06}, {"id": 844, "seek": 476508, "start": 4765.08, "end": 4768.0, "text": " Or build some stuff that you're passionate about.", "tokens": [1610, 1322, 512, 1507, 300, 291, 434, 11410, 466, 13], "temperature": 0.0, "avg_logprob": -0.15008281778406213, "compression_ratio": 1.4267515923566878, "no_speech_prob": 4.3567511056608055e-06}, {"id": 845, "seek": 476508, "start": 4768.0, "end": 4775.88, "text": " Yeah, so it's really important to not just put the videos away and go away and do something", "tokens": [865, 11, 370, 309, 311, 534, 1021, 281, 406, 445, 829, 264, 2145, 1314, 293, 352, 1314, 293, 360, 746], "temperature": 0.0, "avg_logprob": -0.15008281778406213, "compression_ratio": 1.4267515923566878, "no_speech_prob": 4.3567511056608055e-06}, {"id": 846, "seek": 476508, "start": 4775.88, "end": 4784.64, "text": " else because you'll forget everything you've learned and you won't have practiced.", "tokens": [1646, 570, 291, 603, 2870, 1203, 291, 600, 3264, 293, 291, 1582, 380, 362, 19268, 13], "temperature": 0.0, "avg_logprob": -0.15008281778406213, "compression_ratio": 1.4267515923566878, "no_speech_prob": 4.3567511056608055e-06}, {"id": 847, "seek": 478464, "start": 4784.64, "end": 4797.52, "text": " So one of our community members went on to create an activation function, for example,", "tokens": [407, 472, 295, 527, 1768, 2679, 1437, 322, 281, 1884, 364, 24433, 2445, 11, 337, 1365, 11], "temperature": 0.0, "avg_logprob": -0.1521011007593033, "compression_ratio": 1.3307692307692307, "no_speech_prob": 1.4508353160636034e-05}, {"id": 848, "seek": 478464, "start": 4797.52, "end": 4806.9400000000005, "text": " which is MISH, which is now, as Tanishka's just reminded me on our forums, is now used", "tokens": [597, 307, 376, 18842, 11, 597, 307, 586, 11, 382, 314, 7524, 2330, 311, 445, 15920, 385, 322, 527, 26998, 11, 307, 586, 1143], "temperature": 0.0, "avg_logprob": -0.1521011007593033, "compression_ratio": 1.3307692307692307, "no_speech_prob": 1.4508353160636034e-05}, {"id": 849, "seek": 480694, "start": 4806.94, "end": 4816.44, "text": " in many of the state-of-the-art networks around the world, which is pretty cool.", "tokens": [294, 867, 295, 264, 1785, 12, 2670, 12, 3322, 12, 446, 9590, 926, 264, 1002, 11, 597, 307, 1238, 1627, 13], "temperature": 0.0, "avg_logprob": -0.3137392693377556, "compression_ratio": 1.4278350515463918, "no_speech_prob": 6.143869086372433e-06}, {"id": 850, "seek": 480694, "start": 4816.44, "end": 4821.04, "text": " And he's now at Miele, I think, a research, one of the top research labs in the world.", "tokens": [400, 415, 311, 586, 412, 376, 15949, 11, 286, 519, 11, 257, 2132, 11, 472, 295, 264, 1192, 2132, 20339, 294, 264, 1002, 13], "temperature": 0.0, "avg_logprob": -0.3137392693377556, "compression_ratio": 1.4278350515463918, "no_speech_prob": 6.143869086372433e-06}, {"id": 851, "seek": 480694, "start": 4821.04, "end": 4823.839999999999, "text": " I wonder how that's doing.", "tokens": [286, 2441, 577, 300, 311, 884, 13], "temperature": 0.0, "avg_logprob": -0.3137392693377556, "compression_ratio": 1.4278350515463918, "no_speech_prob": 6.143869086372433e-06}, {"id": 852, "seek": 480694, "start": 4823.839999999999, "end": 4824.839999999999, "text": " Let's have a look.", "tokens": [961, 311, 362, 257, 574, 13], "temperature": 0.0, "avg_logprob": -0.3137392693377556, "compression_ratio": 1.4278350515463918, "no_speech_prob": 6.143869086372433e-06}, {"id": 853, "seek": 480694, "start": 4824.839999999999, "end": 4827.839999999999, "text": " Go to Google Scholar.", "tokens": [1037, 281, 3329, 2065, 15276, 13], "temperature": 0.0, "avg_logprob": -0.3137392693377556, "compression_ratio": 1.4278350515463918, "no_speech_prob": 6.143869086372433e-06}, {"id": 854, "seek": 480694, "start": 4827.839999999999, "end": 4829.839999999999, "text": " Nice.", "tokens": [5490, 13], "temperature": 0.0, "avg_logprob": -0.3137392693377556, "compression_ratio": 1.4278350515463918, "no_speech_prob": 6.143869086372433e-06}, {"id": 855, "seek": 480694, "start": 4829.839999999999, "end": 4832.839999999999, "text": " 486 citations.", "tokens": [11174, 21, 4814, 763, 13], "temperature": 0.0, "avg_logprob": -0.3137392693377556, "compression_ratio": 1.4278350515463918, "no_speech_prob": 6.143869086372433e-06}, {"id": 856, "seek": 480694, "start": 4832.839999999999, "end": 4836.839999999999, "text": " They're doing great.", "tokens": [814, 434, 884, 869, 13], "temperature": 0.0, "avg_logprob": -0.3137392693377556, "compression_ratio": 1.4278350515463918, "no_speech_prob": 6.143869086372433e-06}, {"id": 857, "seek": 483684, "start": 4836.84, "end": 4841.54, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.30597329751039165, "compression_ratio": 1.1458333333333333, "no_speech_prob": 5.954928383289371e-06}, {"id": 858, "seek": 483684, "start": 4841.54, "end": 4852.32, "text": " Let's have a look at how our AMA topic is going and pick out some of the highest ranked", "tokens": [961, 311, 362, 257, 574, 412, 577, 527, 6475, 32, 4829, 307, 516, 293, 1888, 484, 512, 295, 264, 6343, 20197], "temperature": 0.0, "avg_logprob": -0.30597329751039165, "compression_ratio": 1.1458333333333333, "no_speech_prob": 5.954928383289371e-06}, {"id": 859, "seek": 483684, "start": 4852.32, "end": 4855.32, "text": " AMAs.", "tokens": [6475, 10884, 13], "temperature": 0.0, "avg_logprob": -0.30597329751039165, "compression_ratio": 1.1458333333333333, "no_speech_prob": 5.954928383289371e-06}, {"id": 860, "seek": 483684, "start": 4855.32, "end": 4862.4400000000005, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.30597329751039165, "compression_ratio": 1.1458333333333333, "no_speech_prob": 5.954928383289371e-06}, {"id": 861, "seek": 486244, "start": 4862.44, "end": 4868.879999999999, "text": " So the first one is from Lucas, and actually maybe I should, actually let's switch our", "tokens": [407, 264, 700, 472, 307, 490, 19178, 11, 293, 767, 1310, 286, 820, 11, 767, 718, 311, 3679, 527], "temperature": 0.0, "avg_logprob": -0.14658010296705293, "compression_ratio": 1.5297029702970297, "no_speech_prob": 5.173805675440235e-06}, {"id": 862, "seek": 486244, "start": 4868.879999999999, "end": 4872.48, "text": " view here.", "tokens": [1910, 510, 13], "temperature": 0.0, "avg_logprob": -0.14658010296705293, "compression_ratio": 1.5297029702970297, "no_speech_prob": 5.173805675440235e-06}, {"id": 863, "seek": 486244, "start": 4872.48, "end": 4880.16, "text": " So our first AMA is from Lucas, and Lucas asks, how do you stay motivated?", "tokens": [407, 527, 700, 6475, 32, 307, 490, 19178, 11, 293, 19178, 8962, 11, 577, 360, 291, 1754, 14515, 30], "temperature": 0.0, "avg_logprob": -0.14658010296705293, "compression_ratio": 1.5297029702970297, "no_speech_prob": 5.173805675440235e-06}, {"id": 864, "seek": 486244, "start": 4880.16, "end": 4884.0, "text": " I often find myself overwhelmed in this field.", "tokens": [286, 2049, 915, 2059, 19042, 294, 341, 2519, 13], "temperature": 0.0, "avg_logprob": -0.14658010296705293, "compression_ratio": 1.5297029702970297, "no_speech_prob": 5.173805675440235e-06}, {"id": 865, "seek": 486244, "start": 4884.0, "end": 4889.5199999999995, "text": " There are so many new things coming up that I feel like I have to put so much energy just", "tokens": [821, 366, 370, 867, 777, 721, 1348, 493, 300, 286, 841, 411, 286, 362, 281, 829, 370, 709, 2281, 445], "temperature": 0.0, "avg_logprob": -0.14658010296705293, "compression_ratio": 1.5297029702970297, "no_speech_prob": 5.173805675440235e-06}, {"id": 866, "seek": 488952, "start": 4889.52, "end": 4895.4400000000005, "text": " to keep my head above the waterline.", "tokens": [281, 1066, 452, 1378, 3673, 264, 1281, 1889, 13], "temperature": 0.0, "avg_logprob": -0.17821669578552246, "compression_ratio": 1.4081632653061225, "no_speech_prob": 2.60156025433389e-06}, {"id": 867, "seek": 488952, "start": 4895.4400000000005, "end": 4899.240000000001, "text": " That's a very interesting question.", "tokens": [663, 311, 257, 588, 1880, 1168, 13], "temperature": 0.0, "avg_logprob": -0.17821669578552246, "compression_ratio": 1.4081632653061225, "no_speech_prob": 2.60156025433389e-06}, {"id": 868, "seek": 488952, "start": 4899.240000000001, "end": 4908.120000000001, "text": " I think Lucas, the important thing is to realize you don't have to know everything.", "tokens": [286, 519, 19178, 11, 264, 1021, 551, 307, 281, 4325, 291, 500, 380, 362, 281, 458, 1203, 13], "temperature": 0.0, "avg_logprob": -0.17821669578552246, "compression_ratio": 1.4081632653061225, "no_speech_prob": 2.60156025433389e-06}, {"id": 869, "seek": 488952, "start": 4908.120000000001, "end": 4912.900000000001, "text": " In fact, nobody knows everything, and that's okay.", "tokens": [682, 1186, 11, 5079, 3255, 1203, 11, 293, 300, 311, 1392, 13], "temperature": 0.0, "avg_logprob": -0.17821669578552246, "compression_ratio": 1.4081632653061225, "no_speech_prob": 2.60156025433389e-06}, {"id": 870, "seek": 491290, "start": 4912.9, "end": 4920.679999999999, "text": " What people do is they take an interest in some area, and they follow that, and they", "tokens": [708, 561, 360, 307, 436, 747, 364, 1179, 294, 512, 1859, 11, 293, 436, 1524, 300, 11, 293, 436], "temperature": 0.0, "avg_logprob": -0.12708729284780998, "compression_ratio": 1.7916666666666667, "no_speech_prob": 1.084510699911334e-06}, {"id": 871, "seek": 491290, "start": 4920.679999999999, "end": 4926.679999999999, "text": " try and do the best job they can of keeping up with some little sub area.", "tokens": [853, 293, 360, 264, 1151, 1691, 436, 393, 295, 5145, 493, 365, 512, 707, 1422, 1859, 13], "temperature": 0.0, "avg_logprob": -0.12708729284780998, "compression_ratio": 1.7916666666666667, "no_speech_prob": 1.084510699911334e-06}, {"id": 872, "seek": 491290, "start": 4926.679999999999, "end": 4931.839999999999, "text": " If your little sub area is too much to keep up on, pick a sub sub area.", "tokens": [759, 428, 707, 1422, 1859, 307, 886, 709, 281, 1066, 493, 322, 11, 1888, 257, 1422, 1422, 1859, 13], "temperature": 0.0, "avg_logprob": -0.12708729284780998, "compression_ratio": 1.7916666666666667, "no_speech_prob": 1.084510699911334e-06}, {"id": 873, "seek": 491290, "start": 4931.839999999999, "end": 4937.4, "text": " Yeah, there's no need for it to be demotivating that there's a lot of people doing a lot of", "tokens": [865, 11, 456, 311, 572, 643, 337, 309, 281, 312, 1371, 310, 592, 990, 300, 456, 311, 257, 688, 295, 561, 884, 257, 688, 295], "temperature": 0.0, "avg_logprob": -0.12708729284780998, "compression_ratio": 1.7916666666666667, "no_speech_prob": 1.084510699911334e-06}, {"id": 874, "seek": 491290, "start": 4937.4, "end": 4940.599999999999, "text": " interesting work and a lot of different sub fields.", "tokens": [1880, 589, 293, 257, 688, 295, 819, 1422, 7909, 13], "temperature": 0.0, "avg_logprob": -0.12708729284780998, "compression_ratio": 1.7916666666666667, "no_speech_prob": 1.084510699911334e-06}, {"id": 875, "seek": 491290, "start": 4940.599999999999, "end": 4941.599999999999, "text": " That's cool.", "tokens": [663, 311, 1627, 13], "temperature": 0.0, "avg_logprob": -0.12708729284780998, "compression_ratio": 1.7916666666666667, "no_speech_prob": 1.084510699911334e-06}, {"id": 876, "seek": 494160, "start": 4941.6, "end": 4946.4800000000005, "text": " It used to be kind of dull when there's only basically five labs in the world working on", "tokens": [467, 1143, 281, 312, 733, 295, 23471, 562, 456, 311, 787, 1936, 1732, 20339, 294, 264, 1002, 1364, 322], "temperature": 0.0, "avg_logprob": -0.16274724183259187, "compression_ratio": 1.632, "no_speech_prob": 4.092873950867215e-06}, {"id": 877, "seek": 494160, "start": 4946.4800000000005, "end": 4951.4800000000005, "text": " Neuronets.", "tokens": [1734, 374, 266, 1385, 13], "temperature": 0.0, "avg_logprob": -0.16274724183259187, "compression_ratio": 1.632, "no_speech_prob": 4.092873950867215e-06}, {"id": 878, "seek": 494160, "start": 4951.4800000000005, "end": 4955.92, "text": " And yeah, from time to time, take a dip into other areas that maybe you're not following", "tokens": [400, 1338, 11, 490, 565, 281, 565, 11, 747, 257, 10460, 666, 661, 3179, 300, 1310, 291, 434, 406, 3480], "temperature": 0.0, "avg_logprob": -0.16274724183259187, "compression_ratio": 1.632, "no_speech_prob": 4.092873950867215e-06}, {"id": 879, "seek": 494160, "start": 4955.92, "end": 4956.92, "text": " as closely.", "tokens": [382, 8185, 13], "temperature": 0.0, "avg_logprob": -0.16274724183259187, "compression_ratio": 1.632, "no_speech_prob": 4.092873950867215e-06}, {"id": 880, "seek": 494160, "start": 4956.92, "end": 4963.120000000001, "text": " But when you're just starting out, you'll find that things are not changing that fast", "tokens": [583, 562, 291, 434, 445, 2891, 484, 11, 291, 603, 915, 300, 721, 366, 406, 4473, 300, 2370], "temperature": 0.0, "avg_logprob": -0.16274724183259187, "compression_ratio": 1.632, "no_speech_prob": 4.092873950867215e-06}, {"id": 881, "seek": 494160, "start": 4963.120000000001, "end": 4964.8, "text": " at all, really.", "tokens": [412, 439, 11, 534, 13], "temperature": 0.0, "avg_logprob": -0.16274724183259187, "compression_ratio": 1.632, "no_speech_prob": 4.092873950867215e-06}, {"id": 882, "seek": 494160, "start": 4964.8, "end": 4968.04, "text": " It can kind of look that way because people are always putting out press releases about", "tokens": [467, 393, 733, 295, 574, 300, 636, 570, 561, 366, 1009, 3372, 484, 1886, 16952, 466], "temperature": 0.0, "avg_logprob": -0.16274724183259187, "compression_ratio": 1.632, "no_speech_prob": 4.092873950867215e-06}, {"id": 883, "seek": 494160, "start": 4968.04, "end": 4970.68, "text": " their new tweaks.", "tokens": [641, 777, 46664, 13], "temperature": 0.0, "avg_logprob": -0.16274724183259187, "compression_ratio": 1.632, "no_speech_prob": 4.092873950867215e-06}, {"id": 884, "seek": 497068, "start": 4970.68, "end": 4977.280000000001, "text": " But fundamentally, the stuff that is in the course now is not that different to what was", "tokens": [583, 17879, 11, 264, 1507, 300, 307, 294, 264, 1164, 586, 307, 406, 300, 819, 281, 437, 390], "temperature": 0.0, "avg_logprob": -0.15480719293866838, "compression_ratio": 1.5369458128078817, "no_speech_prob": 4.565932158584474e-06}, {"id": 885, "seek": 497068, "start": 4977.280000000001, "end": 4981.8, "text": " in the course five years ago.", "tokens": [294, 264, 1164, 1732, 924, 2057, 13], "temperature": 0.0, "avg_logprob": -0.15480719293866838, "compression_ratio": 1.5369458128078817, "no_speech_prob": 4.565932158584474e-06}, {"id": 886, "seek": 497068, "start": 4981.8, "end": 4983.6, "text": " The foundations haven't changed.", "tokens": [440, 22467, 2378, 380, 3105, 13], "temperature": 0.0, "avg_logprob": -0.15480719293866838, "compression_ratio": 1.5369458128078817, "no_speech_prob": 4.565932158584474e-06}, {"id": 887, "seek": 497068, "start": 4983.6, "end": 4987.4400000000005, "text": " And it's not that different, in fact, to the convolutional neural network that Jan LeCun", "tokens": [400, 309, 311, 406, 300, 819, 11, 294, 1186, 11, 281, 264, 45216, 304, 18161, 3209, 300, 4956, 1456, 34, 409], "temperature": 0.0, "avg_logprob": -0.15480719293866838, "compression_ratio": 1.5369458128078817, "no_speech_prob": 4.565932158584474e-06}, {"id": 888, "seek": 497068, "start": 4987.4400000000005, "end": 4993.08, "text": " used on MNIST back in 1996.", "tokens": [1143, 322, 376, 45, 19756, 646, 294, 22690, 13], "temperature": 0.0, "avg_logprob": -0.15480719293866838, "compression_ratio": 1.5369458128078817, "no_speech_prob": 4.565932158584474e-06}, {"id": 889, "seek": 497068, "start": 4993.08, "end": 4998.08, "text": " The basic ideas I've described are forever.", "tokens": [440, 3875, 3487, 286, 600, 7619, 366, 5680, 13], "temperature": 0.0, "avg_logprob": -0.15480719293866838, "compression_ratio": 1.5369458128078817, "no_speech_prob": 4.565932158584474e-06}, {"id": 890, "seek": 499808, "start": 4998.08, "end": 5002.44, "text": " The way the inputs work and the sandwiches of matrix multipliers and activation functions", "tokens": [440, 636, 264, 15743, 589, 293, 264, 29022, 295, 8141, 12788, 4890, 293, 24433, 6828], "temperature": 0.0, "avg_logprob": -0.10076854940046344, "compression_ratio": 1.717241379310345, "no_speech_prob": 2.60159617937461e-06}, {"id": 891, "seek": 499808, "start": 5002.44, "end": 5005.68, "text": " and the stuff you do to the final layer.", "tokens": [293, 264, 1507, 291, 360, 281, 264, 2572, 4583, 13], "temperature": 0.0, "avg_logprob": -0.10076854940046344, "compression_ratio": 1.717241379310345, "no_speech_prob": 2.60159617937461e-06}, {"id": 892, "seek": 499808, "start": 5005.68, "end": 5008.24, "text": " Everything else is tweaks.", "tokens": [5471, 1646, 307, 46664, 13], "temperature": 0.0, "avg_logprob": -0.10076854940046344, "compression_ratio": 1.717241379310345, "no_speech_prob": 2.60159617937461e-06}, {"id": 893, "seek": 499808, "start": 5008.24, "end": 5014.72, "text": " And the more you learn about those basic ideas, the more you'll recognize those tweaks as", "tokens": [400, 264, 544, 291, 1466, 466, 729, 3875, 3487, 11, 264, 544, 291, 603, 5521, 729, 46664, 382], "temperature": 0.0, "avg_logprob": -0.10076854940046344, "compression_ratio": 1.717241379310345, "no_speech_prob": 2.60159617937461e-06}, {"id": 894, "seek": 499808, "start": 5014.72, "end": 5018.96, "text": " simple little tweaks that you'll be able to quickly get your head around.", "tokens": [2199, 707, 46664, 300, 291, 603, 312, 1075, 281, 2661, 483, 428, 1378, 926, 13], "temperature": 0.0, "avg_logprob": -0.10076854940046344, "compression_ratio": 1.717241379310345, "no_speech_prob": 2.60159617937461e-06}, {"id": 895, "seek": 499808, "start": 5018.96, "end": 5022.9, "text": " So then Lucas goes on to ask or to comment, another thing that constantly bothers me is", "tokens": [407, 550, 19178, 1709, 322, 281, 1029, 420, 281, 2871, 11, 1071, 551, 300, 6460, 33980, 385, 307], "temperature": 0.0, "avg_logprob": -0.10076854940046344, "compression_ratio": 1.717241379310345, "no_speech_prob": 2.60159617937461e-06}, {"id": 896, "seek": 499808, "start": 5022.9, "end": 5027.2, "text": " I feel the field is getting more and more skewed towards bigger and more computationally", "tokens": [286, 841, 264, 2519, 307, 1242, 544, 293, 544, 8756, 26896, 3030, 3801, 293, 544, 24903, 379], "temperature": 0.0, "avg_logprob": -0.10076854940046344, "compression_ratio": 1.717241379310345, "no_speech_prob": 2.60159617937461e-06}, {"id": 897, "seek": 502720, "start": 5027.2, "end": 5030.4, "text": " expensive models and huge amounts of data.", "tokens": [5124, 5245, 293, 2603, 11663, 295, 1412, 13], "temperature": 0.0, "avg_logprob": -0.16396312486557735, "compression_ratio": 1.5109170305676856, "no_speech_prob": 1.7603306332603097e-06}, {"id": 898, "seek": 502720, "start": 5030.4, "end": 5035.08, "text": " I keep wondering if in some years from now I would still be able to train reasonable", "tokens": [286, 1066, 6359, 498, 294, 512, 924, 490, 586, 286, 576, 920, 312, 1075, 281, 3847, 10585], "temperature": 0.0, "avg_logprob": -0.16396312486557735, "compression_ratio": 1.5109170305676856, "no_speech_prob": 1.7603306332603097e-06}, {"id": 899, "seek": 502720, "start": 5035.08, "end": 5040.08, "text": " models with a single GPU or if everything is going to require a compute cluster.", "tokens": [5245, 365, 257, 2167, 18407, 420, 498, 1203, 307, 516, 281, 3651, 257, 14722, 13630, 13], "temperature": 0.0, "avg_logprob": -0.16396312486557735, "compression_ratio": 1.5109170305676856, "no_speech_prob": 1.7603306332603097e-06}, {"id": 900, "seek": 502720, "start": 5040.08, "end": 5043.12, "text": " Yeah, that's a great question.", "tokens": [865, 11, 300, 311, 257, 869, 1168, 13], "temperature": 0.0, "avg_logprob": -0.16396312486557735, "compression_ratio": 1.5109170305676856, "no_speech_prob": 1.7603306332603097e-06}, {"id": 901, "seek": 502720, "start": 5043.12, "end": 5045.679999999999, "text": " I get that a lot.", "tokens": [286, 483, 300, 257, 688, 13], "temperature": 0.0, "avg_logprob": -0.16396312486557735, "compression_ratio": 1.5109170305676856, "no_speech_prob": 1.7603306332603097e-06}, {"id": 902, "seek": 502720, "start": 5045.679999999999, "end": 5054.679999999999, "text": " But interestingly, I've been teaching people machine learning and data science stuff for", "tokens": [583, 25873, 11, 286, 600, 668, 4571, 561, 3479, 2539, 293, 1412, 3497, 1507, 337], "temperature": 0.0, "avg_logprob": -0.16396312486557735, "compression_ratio": 1.5109170305676856, "no_speech_prob": 1.7603306332603097e-06}, {"id": 903, "seek": 505468, "start": 5054.68, "end": 5061.96, "text": " nearly 30 years and I've had a variation of this question throughout.", "tokens": [6217, 2217, 924, 293, 286, 600, 632, 257, 12990, 295, 341, 1168, 3710, 13], "temperature": 0.0, "avg_logprob": -0.1006829545304582, "compression_ratio": 1.5215311004784688, "no_speech_prob": 2.769353613985004e-06}, {"id": 904, "seek": 505468, "start": 5061.96, "end": 5069.68, "text": " And the reason is that engineers always want to push the envelope on the biggest computers", "tokens": [400, 264, 1778, 307, 300, 11955, 1009, 528, 281, 2944, 264, 19989, 322, 264, 3880, 10807], "temperature": 0.0, "avg_logprob": -0.1006829545304582, "compression_ratio": 1.5215311004784688, "no_speech_prob": 2.769353613985004e-06}, {"id": 905, "seek": 505468, "start": 5069.68, "end": 5071.64, "text": " they can find.", "tokens": [436, 393, 915, 13], "temperature": 0.0, "avg_logprob": -0.1006829545304582, "compression_ratio": 1.5215311004784688, "no_speech_prob": 2.769353613985004e-06}, {"id": 906, "seek": 505468, "start": 5071.64, "end": 5075.12, "text": " That's just this fun thing engineers love to do.", "tokens": [663, 311, 445, 341, 1019, 551, 11955, 959, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.1006829545304582, "compression_ratio": 1.5215311004784688, "no_speech_prob": 2.769353613985004e-06}, {"id": 907, "seek": 505468, "start": 5075.12, "end": 5080.8, "text": " And by definition, they're going to get slightly better results than people doing exactly the", "tokens": [400, 538, 7123, 11, 436, 434, 516, 281, 483, 4748, 1101, 3542, 813, 561, 884, 2293, 264], "temperature": 0.0, "avg_logprob": -0.1006829545304582, "compression_ratio": 1.5215311004784688, "no_speech_prob": 2.769353613985004e-06}, {"id": 908, "seek": 508080, "start": 5080.8, "end": 5085.360000000001, "text": " same thing on smaller computers.", "tokens": [912, 551, 322, 4356, 10807, 13], "temperature": 0.0, "avg_logprob": -0.11689225832621257, "compression_ratio": 1.5555555555555556, "no_speech_prob": 2.3320601485465886e-06}, {"id": 909, "seek": 508080, "start": 5085.360000000001, "end": 5093.360000000001, "text": " So it always looks like, oh, you need big computers to be state of the art.", "tokens": [407, 309, 1009, 1542, 411, 11, 1954, 11, 291, 643, 955, 10807, 281, 312, 1785, 295, 264, 1523, 13], "temperature": 0.0, "avg_logprob": -0.11689225832621257, "compression_ratio": 1.5555555555555556, "no_speech_prob": 2.3320601485465886e-06}, {"id": 910, "seek": 508080, "start": 5093.360000000001, "end": 5095.92, "text": " But that's actually never true, right?", "tokens": [583, 300, 311, 767, 1128, 2074, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.11689225832621257, "compression_ratio": 1.5555555555555556, "no_speech_prob": 2.3320601485465886e-06}, {"id": 911, "seek": 508080, "start": 5095.92, "end": 5102.04, "text": " Because there's always smarter ways to do things, not just bigger ways to do things.", "tokens": [1436, 456, 311, 1009, 20294, 2098, 281, 360, 721, 11, 406, 445, 3801, 2098, 281, 360, 721, 13], "temperature": 0.0, "avg_logprob": -0.11689225832621257, "compression_ratio": 1.5555555555555556, "no_speech_prob": 2.3320601485465886e-06}, {"id": 912, "seek": 508080, "start": 5102.04, "end": 5110.04, "text": " And so when you look at fast AI's Dawnbench success, when we trained ImageNet faster than", "tokens": [400, 370, 562, 291, 574, 412, 2370, 7318, 311, 26001, 47244, 2245, 11, 562, 321, 8895, 29903, 31890, 4663, 813], "temperature": 0.0, "avg_logprob": -0.11689225832621257, "compression_ratio": 1.5555555555555556, "no_speech_prob": 2.3320601485465886e-06}, {"id": 913, "seek": 511004, "start": 5110.04, "end": 5117.44, "text": " anybody had trained it before on standard GPUs, me and a bunch of students, that was", "tokens": [4472, 632, 8895, 309, 949, 322, 3832, 18407, 82, 11, 385, 293, 257, 3840, 295, 1731, 11, 300, 390], "temperature": 0.0, "avg_logprob": -0.14304784628061148, "compression_ratio": 1.3876404494382022, "no_speech_prob": 5.507402420334984e-06}, {"id": 914, "seek": 511004, "start": 5117.44, "end": 5118.92, "text": " not meant to happen.", "tokens": [406, 4140, 281, 1051, 13], "temperature": 0.0, "avg_logprob": -0.14304784628061148, "compression_ratio": 1.3876404494382022, "no_speech_prob": 5.507402420334984e-06}, {"id": 915, "seek": 511004, "start": 5118.92, "end": 5123.16, "text": " Google was working very hard with their TPU introduction to try to show how good they", "tokens": [3329, 390, 1364, 588, 1152, 365, 641, 314, 8115, 9339, 281, 853, 281, 855, 577, 665, 436], "temperature": 0.0, "avg_logprob": -0.14304784628061148, "compression_ratio": 1.3876404494382022, "no_speech_prob": 5.507402420334984e-06}, {"id": 916, "seek": 511004, "start": 5123.16, "end": 5124.16, "text": " were.", "tokens": [645, 13], "temperature": 0.0, "avg_logprob": -0.14304784628061148, "compression_ratio": 1.3876404494382022, "no_speech_prob": 5.507402420334984e-06}, {"id": 917, "seek": 511004, "start": 5124.16, "end": 5133.56, "text": " Intel was using 256 PCs in parallel or something.", "tokens": [19762, 390, 1228, 38882, 46913, 294, 8952, 420, 746, 13], "temperature": 0.0, "avg_logprob": -0.14304784628061148, "compression_ratio": 1.3876404494382022, "no_speech_prob": 5.507402420334984e-06}, {"id": 918, "seek": 513356, "start": 5133.56, "end": 5140.56, "text": " But yeah, we used common sense and smarts and showed what can be done.", "tokens": [583, 1338, 11, 321, 1143, 2689, 2020, 293, 4069, 82, 293, 4712, 437, 393, 312, 1096, 13], "temperature": 0.0, "avg_logprob": -0.13756258894757525, "compression_ratio": 1.5813953488372092, "no_speech_prob": 3.844868388114264e-06}, {"id": 919, "seek": 513356, "start": 5140.56, "end": 5143.64, "text": " It's also a case of picking the problems you solve.", "tokens": [467, 311, 611, 257, 1389, 295, 8867, 264, 2740, 291, 5039, 13], "temperature": 0.0, "avg_logprob": -0.13756258894757525, "compression_ratio": 1.5813953488372092, "no_speech_prob": 3.844868388114264e-06}, {"id": 920, "seek": 513356, "start": 5143.64, "end": 5148.88, "text": " So I would not be probably doing like going head to head up against codecs and trying", "tokens": [407, 286, 576, 406, 312, 1391, 884, 411, 516, 1378, 281, 1378, 493, 1970, 3089, 14368, 293, 1382], "temperature": 0.0, "avg_logprob": -0.13756258894757525, "compression_ratio": 1.5813953488372092, "no_speech_prob": 3.844868388114264e-06}, {"id": 921, "seek": 513356, "start": 5148.88, "end": 5155.92, "text": " to create code from English descriptions.", "tokens": [281, 1884, 3089, 490, 3669, 24406, 13], "temperature": 0.0, "avg_logprob": -0.13756258894757525, "compression_ratio": 1.5813953488372092, "no_speech_prob": 3.844868388114264e-06}, {"id": 922, "seek": 513356, "start": 5155.92, "end": 5161.56, "text": " Because that's a problem that does probably require very large neural nets and very large", "tokens": [1436, 300, 311, 257, 1154, 300, 775, 1391, 3651, 588, 2416, 18161, 36170, 293, 588, 2416], "temperature": 0.0, "avg_logprob": -0.13756258894757525, "compression_ratio": 1.5813953488372092, "no_speech_prob": 3.844868388114264e-06}, {"id": 923, "seek": 516156, "start": 5161.56, "end": 5163.76, "text": " amounts of data.", "tokens": [11663, 295, 1412, 13], "temperature": 0.0, "avg_logprob": -0.1443120894893523, "compression_ratio": 1.4444444444444444, "no_speech_prob": 1.1365606269464479e-06}, {"id": 924, "seek": 516156, "start": 5163.76, "end": 5172.68, "text": " But if you pick areas in different domains, there's still huge areas where much smaller", "tokens": [583, 498, 291, 1888, 3179, 294, 819, 25514, 11, 456, 311, 920, 2603, 3179, 689, 709, 4356], "temperature": 0.0, "avg_logprob": -0.1443120894893523, "compression_ratio": 1.4444444444444444, "no_speech_prob": 1.1365606269464479e-06}, {"id": 925, "seek": 516156, "start": 5172.68, "end": 5177.96, "text": " models are still going to be state of the art.", "tokens": [5245, 366, 920, 516, 281, 312, 1785, 295, 264, 1523, 13], "temperature": 0.0, "avg_logprob": -0.1443120894893523, "compression_ratio": 1.4444444444444444, "no_speech_prob": 1.1365606269464479e-06}, {"id": 926, "seek": 516156, "start": 5177.96, "end": 5180.120000000001, "text": " So hopefully that helped answer your question.", "tokens": [407, 4696, 300, 4254, 1867, 428, 1168, 13], "temperature": 0.0, "avg_logprob": -0.1443120894893523, "compression_ratio": 1.4444444444444444, "no_speech_prob": 1.1365606269464479e-06}, {"id": 927, "seek": 516156, "start": 5180.120000000001, "end": 5187.4400000000005, "text": " Let's see what else we've got here.", "tokens": [961, 311, 536, 437, 1646, 321, 600, 658, 510, 13], "temperature": 0.0, "avg_logprob": -0.1443120894893523, "compression_ratio": 1.4444444444444444, "no_speech_prob": 1.1365606269464479e-06}, {"id": 928, "seek": 518744, "start": 5187.44, "end": 5193.24, "text": " So Daniel has obviously been following my journey with teaching my daughter math.", "tokens": [407, 8033, 575, 2745, 668, 3480, 452, 4671, 365, 4571, 452, 4653, 5221, 13], "temperature": 0.0, "avg_logprob": -0.18808402753856085, "compression_ratio": 1.5, "no_speech_prob": 6.962053248571465e-06}, {"id": 929, "seek": 518744, "start": 5193.24, "end": 5196.04, "text": " So I homeschool my daughter.", "tokens": [407, 286, 7388, 21856, 452, 4653, 13], "temperature": 0.0, "avg_logprob": -0.18808402753856085, "compression_ratio": 1.5, "no_speech_prob": 6.962053248571465e-06}, {"id": 930, "seek": 518744, "start": 5196.04, "end": 5204.48, "text": " And Daniel asks, how do you homeschool young children, science in general and math in particular?", "tokens": [400, 8033, 8962, 11, 577, 360, 291, 7388, 21856, 2037, 2227, 11, 3497, 294, 2674, 293, 5221, 294, 1729, 30], "temperature": 0.0, "avg_logprob": -0.18808402753856085, "compression_ratio": 1.5, "no_speech_prob": 6.962053248571465e-06}, {"id": 931, "seek": 518744, "start": 5204.48, "end": 5208.2, "text": " Would you share your experiences by blogging or in lectures someday?", "tokens": [6068, 291, 2073, 428, 5235, 538, 6968, 3249, 420, 294, 16564, 19412, 30], "temperature": 0.0, "avg_logprob": -0.18808402753856085, "compression_ratio": 1.5, "no_speech_prob": 6.962053248571465e-06}, {"id": 932, "seek": 518744, "start": 5208.2, "end": 5212.4, "text": " Yeah, I could do that.", "tokens": [865, 11, 286, 727, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.18808402753856085, "compression_ratio": 1.5, "no_speech_prob": 6.962053248571465e-06}, {"id": 933, "seek": 521240, "start": 5212.4, "end": 5219.96, "text": " So I actually spent quite a few months just reading research papers about education recently.", "tokens": [407, 286, 767, 4418, 1596, 257, 1326, 2493, 445, 3760, 2132, 10577, 466, 3309, 3938, 13], "temperature": 0.0, "avg_logprob": -0.13174890588831017, "compression_ratio": 1.579185520361991, "no_speech_prob": 5.093553681945195e-06}, {"id": 934, "seek": 521240, "start": 5219.96, "end": 5224.839999999999, "text": " So I do probably have a lot I probably need to talk about at some stage.", "tokens": [407, 286, 360, 1391, 362, 257, 688, 286, 1391, 643, 281, 751, 466, 412, 512, 3233, 13], "temperature": 0.0, "avg_logprob": -0.13174890588831017, "compression_ratio": 1.579185520361991, "no_speech_prob": 5.093553681945195e-06}, {"id": 935, "seek": 521240, "start": 5224.839999999999, "end": 5235.36, "text": " But yeah, broadly speaking, I lean into using computers and tablets a lot more than most", "tokens": [583, 1338, 11, 19511, 4124, 11, 286, 11659, 666, 1228, 10807, 293, 27622, 257, 688, 544, 813, 881], "temperature": 0.0, "avg_logprob": -0.13174890588831017, "compression_ratio": 1.579185520361991, "no_speech_prob": 5.093553681945195e-06}, {"id": 936, "seek": 521240, "start": 5235.36, "end": 5236.759999999999, "text": " people.", "tokens": [561, 13], "temperature": 0.0, "avg_logprob": -0.13174890588831017, "compression_ratio": 1.579185520361991, "no_speech_prob": 5.093553681945195e-06}, {"id": 937, "seek": 521240, "start": 5236.759999999999, "end": 5241.08, "text": " Because actually there's an awful lot of really great apps that are super compelling.", "tokens": [1436, 767, 456, 311, 364, 11232, 688, 295, 534, 869, 7733, 300, 366, 1687, 20050, 13], "temperature": 0.0, "avg_logprob": -0.13174890588831017, "compression_ratio": 1.579185520361991, "no_speech_prob": 5.093553681945195e-06}, {"id": 938, "seek": 524108, "start": 5241.08, "end": 5245.32, "text": " They're adaptive, so they go at the right speed for the student.", "tokens": [814, 434, 27912, 11, 370, 436, 352, 412, 264, 558, 3073, 337, 264, 3107, 13], "temperature": 0.0, "avg_logprob": -0.12710064589375197, "compression_ratio": 1.60352422907489, "no_speech_prob": 8.3975974121131e-06}, {"id": 939, "seek": 524108, "start": 5245.32, "end": 5247.92, "text": " And they're fun.", "tokens": [400, 436, 434, 1019, 13], "temperature": 0.0, "avg_logprob": -0.12710064589375197, "compression_ratio": 1.60352422907489, "no_speech_prob": 8.3975974121131e-06}, {"id": 940, "seek": 524108, "start": 5247.92, "end": 5251.36, "text": " And I really like my daughter to have fun.", "tokens": [400, 286, 534, 411, 452, 4653, 281, 362, 1019, 13], "temperature": 0.0, "avg_logprob": -0.12710064589375197, "compression_ratio": 1.60352422907489, "no_speech_prob": 8.3975974121131e-06}, {"id": 941, "seek": 524108, "start": 5251.36, "end": 5255.84, "text": " I really don't like to force her to do things.", "tokens": [286, 534, 500, 380, 411, 281, 3464, 720, 281, 360, 721, 13], "temperature": 0.0, "avg_logprob": -0.12710064589375197, "compression_ratio": 1.60352422907489, "no_speech_prob": 8.3975974121131e-06}, {"id": 942, "seek": 524108, "start": 5255.84, "end": 5262.6, "text": " And for example, there's a really cool app called Dragon Box Algebra 5 Plus, which teaches", "tokens": [400, 337, 1365, 11, 456, 311, 257, 534, 1627, 724, 1219, 11517, 15112, 967, 19983, 1025, 7721, 11, 597, 16876], "temperature": 0.0, "avg_logprob": -0.12710064589375197, "compression_ratio": 1.60352422907489, "no_speech_prob": 8.3975974121131e-06}, {"id": 943, "seek": 524108, "start": 5262.6, "end": 5267.5199999999995, "text": " algebra to five-year-olds by using a really fun computer game involving helping dragon", "tokens": [21989, 281, 1732, 12, 5294, 12, 31518, 538, 1228, 257, 534, 1019, 3820, 1216, 17030, 4315, 12165], "temperature": 0.0, "avg_logprob": -0.12710064589375197, "compression_ratio": 1.60352422907489, "no_speech_prob": 8.3975974121131e-06}, {"id": 944, "seek": 524108, "start": 5267.5199999999995, "end": 5269.68, "text": " eggs to hatch.", "tokens": [6466, 281, 17387, 13], "temperature": 0.0, "avg_logprob": -0.12710064589375197, "compression_ratio": 1.60352422907489, "no_speech_prob": 8.3975974121131e-06}, {"id": 945, "seek": 526968, "start": 5269.68, "end": 5276.16, "text": " And it turns out that yeah, algebra, the basic ideas of algebra are no more complex than", "tokens": [400, 309, 4523, 484, 300, 1338, 11, 21989, 11, 264, 3875, 3487, 295, 21989, 366, 572, 544, 3997, 813], "temperature": 0.0, "avg_logprob": -0.1897844950358073, "compression_ratio": 1.63013698630137, "no_speech_prob": 3.785269655054435e-06}, {"id": 946, "seek": 526968, "start": 5276.16, "end": 5280.64, "text": " the basic ideas that we do in other kindergarten math.", "tokens": [264, 3875, 3487, 300, 321, 360, 294, 661, 26671, 5221, 13], "temperature": 0.0, "avg_logprob": -0.1897844950358073, "compression_ratio": 1.63013698630137, "no_speech_prob": 3.785269655054435e-06}, {"id": 947, "seek": 526968, "start": 5280.64, "end": 5285.08, "text": " And all the parents I know of who have given their kids Dragon Box Algebra 5 Plus, their", "tokens": [400, 439, 264, 3152, 286, 458, 295, 567, 362, 2212, 641, 2301, 11517, 15112, 967, 19983, 1025, 7721, 11, 641], "temperature": 0.0, "avg_logprob": -0.1897844950358073, "compression_ratio": 1.63013698630137, "no_speech_prob": 3.785269655054435e-06}, {"id": 948, "seek": 526968, "start": 5285.08, "end": 5287.320000000001, "text": " kids have successfully learned algebra.", "tokens": [2301, 362, 10727, 3264, 21989, 13], "temperature": 0.0, "avg_logprob": -0.1897844950358073, "compression_ratio": 1.63013698630137, "no_speech_prob": 3.785269655054435e-06}, {"id": 949, "seek": 526968, "start": 5287.320000000001, "end": 5289.92, "text": " So that would be an example.", "tokens": [407, 300, 576, 312, 364, 1365, 13], "temperature": 0.0, "avg_logprob": -0.1897844950358073, "compression_ratio": 1.63013698630137, "no_speech_prob": 3.785269655054435e-06}, {"id": 950, "seek": 526968, "start": 5289.92, "end": 5297.08, "text": " But yeah, we should talk about this more at some point.", "tokens": [583, 1338, 11, 321, 820, 751, 466, 341, 544, 412, 512, 935, 13], "temperature": 0.0, "avg_logprob": -0.1897844950358073, "compression_ratio": 1.63013698630137, "no_speech_prob": 3.785269655054435e-06}, {"id": 951, "seek": 529708, "start": 5297.08, "end": 5301.76, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.11784636974334717, "compression_ratio": 1.5450643776824033, "no_speech_prob": 5.682328719558427e-06}, {"id": 952, "seek": 529708, "start": 5301.76, "end": 5307.76, "text": " Let's see what else we've got here.", "tokens": [961, 311, 536, 437, 1646, 321, 600, 658, 510, 13], "temperature": 0.0, "avg_logprob": -0.11784636974334717, "compression_ratio": 1.5450643776824033, "no_speech_prob": 5.682328719558427e-06}, {"id": 953, "seek": 529708, "start": 5307.76, "end": 5312.88, "text": " So Farah says, the walkthroughs have been a game changer for me.", "tokens": [407, 9067, 545, 1619, 11, 264, 1792, 11529, 82, 362, 668, 257, 1216, 22822, 337, 385, 13], "temperature": 0.0, "avg_logprob": -0.11784636974334717, "compression_ratio": 1.5450643776824033, "no_speech_prob": 5.682328719558427e-06}, {"id": 954, "seek": 529708, "start": 5312.88, "end": 5316.4, "text": " The knowledge and tips you shared in those sessions are skills required to become an", "tokens": [440, 3601, 293, 6082, 291, 5507, 294, 729, 11081, 366, 3942, 4739, 281, 1813, 364], "temperature": 0.0, "avg_logprob": -0.11784636974334717, "compression_ratio": 1.5450643776824033, "no_speech_prob": 5.682328719558427e-06}, {"id": 955, "seek": 529708, "start": 5316.4, "end": 5320.8, "text": " effective machine learning practitioner and utilize fast AI more effectively.", "tokens": [4942, 3479, 2539, 32125, 293, 16117, 2370, 7318, 544, 8659, 13], "temperature": 0.0, "avg_logprob": -0.11784636974334717, "compression_ratio": 1.5450643776824033, "no_speech_prob": 5.682328719558427e-06}, {"id": 956, "seek": 529708, "start": 5320.8, "end": 5324.12, "text": " Have you considered making the walkthroughs a more formal part of the course, doing a", "tokens": [3560, 291, 4888, 1455, 264, 1792, 11529, 82, 257, 544, 9860, 644, 295, 264, 1164, 11, 884, 257], "temperature": 0.0, "avg_logprob": -0.11784636974334717, "compression_ratio": 1.5450643776824033, "no_speech_prob": 5.682328719558427e-06}, {"id": 957, "seek": 532412, "start": 5324.12, "end": 5329.44, "text": " separate software engineering course or continuing live coding sessions between part one and", "tokens": [4994, 4722, 7043, 1164, 420, 9289, 1621, 17720, 11081, 1296, 644, 472, 293], "temperature": 0.0, "avg_logprob": -0.14388270213686186, "compression_ratio": 1.7740740740740741, "no_speech_prob": 2.7693754418578465e-06}, {"id": 958, "seek": 532412, "start": 5329.44, "end": 5330.44, "text": " two?", "tokens": [732, 30], "temperature": 0.0, "avg_logprob": -0.14388270213686186, "compression_ratio": 1.7740740740740741, "no_speech_prob": 2.7693754418578465e-06}, {"id": 959, "seek": 532412, "start": 5330.44, "end": 5333.36, "text": " So yes, I am going to keep doing live coding sessions.", "tokens": [407, 2086, 11, 286, 669, 516, 281, 1066, 884, 1621, 17720, 11081, 13], "temperature": 0.0, "avg_logprob": -0.14388270213686186, "compression_ratio": 1.7740740740740741, "no_speech_prob": 2.7693754418578465e-06}, {"id": 960, "seek": 532412, "start": 5333.36, "end": 5338.2, "text": " At the moment, we've switched to those specifically to focusing on APL.", "tokens": [1711, 264, 1623, 11, 321, 600, 16858, 281, 729, 4682, 281, 8416, 322, 5372, 43, 13], "temperature": 0.0, "avg_logprob": -0.14388270213686186, "compression_ratio": 1.7740740740740741, "no_speech_prob": 2.7693754418578465e-06}, {"id": 961, "seek": 532412, "start": 5338.2, "end": 5342.48, "text": " And then in a couple of weeks, they're going to be going to fast AI study groups.", "tokens": [400, 550, 294, 257, 1916, 295, 3259, 11, 436, 434, 516, 281, 312, 516, 281, 2370, 7318, 2979, 3935, 13], "temperature": 0.0, "avg_logprob": -0.14388270213686186, "compression_ratio": 1.7740740740740741, "no_speech_prob": 2.7693754418578465e-06}, {"id": 962, "seek": 532412, "start": 5342.48, "end": 5346.76, "text": " And then after that, they'll gradually turn back into more live coding sessions.", "tokens": [400, 550, 934, 300, 11, 436, 603, 13145, 1261, 646, 666, 544, 1621, 17720, 11081, 13], "temperature": 0.0, "avg_logprob": -0.14388270213686186, "compression_ratio": 1.7740740740740741, "no_speech_prob": 2.7693754418578465e-06}, {"id": 963, "seek": 532412, "start": 5346.76, "end": 5353.5599999999995, "text": " But yeah, the thing I try to do in my live coding or study groups or whatever is definitely", "tokens": [583, 1338, 11, 264, 551, 286, 853, 281, 360, 294, 452, 1621, 17720, 420, 2979, 3935, 420, 2035, 307, 2138], "temperature": 0.0, "avg_logprob": -0.14388270213686186, "compression_ratio": 1.7740740740740741, "no_speech_prob": 2.7693754418578465e-06}, {"id": 964, "seek": 535356, "start": 5353.56, "end": 5360.96, "text": " try to show the foundational techniques that just make life easier as a coder or a data", "tokens": [853, 281, 855, 264, 32195, 7512, 300, 445, 652, 993, 3571, 382, 257, 17656, 260, 420, 257, 1412], "temperature": 0.0, "avg_logprob": -0.14010301549383936, "compression_ratio": 1.6916299559471366, "no_speech_prob": 4.092755261808634e-06}, {"id": 965, "seek": 535356, "start": 5360.96, "end": 5361.96, "text": " scientist.", "tokens": [12662, 13], "temperature": 0.0, "avg_logprob": -0.14010301549383936, "compression_ratio": 1.6916299559471366, "no_speech_prob": 4.092755261808634e-06}, {"id": 966, "seek": 535356, "start": 5361.96, "end": 5366.92, "text": " When I say foundational, I mean, yeah, the stuff which you can reuse again and again", "tokens": [1133, 286, 584, 32195, 11, 286, 914, 11, 1338, 11, 264, 1507, 597, 291, 393, 26225, 797, 293, 797], "temperature": 0.0, "avg_logprob": -0.14010301549383936, "compression_ratio": 1.6916299559471366, "no_speech_prob": 4.092755261808634e-06}, {"id": 967, "seek": 535356, "start": 5366.92, "end": 5373.360000000001, "text": " and again, like learning regular expressions really well or knowing how to use a VM or", "tokens": [293, 797, 11, 411, 2539, 3890, 15277, 534, 731, 420, 5276, 577, 281, 764, 257, 18038, 420], "temperature": 0.0, "avg_logprob": -0.14010301549383936, "compression_ratio": 1.6916299559471366, "no_speech_prob": 4.092755261808634e-06}, {"id": 968, "seek": 535356, "start": 5373.360000000001, "end": 5380.6, "text": " understanding how to use the terminal and command line, you know, all that kind of stuff", "tokens": [3701, 577, 281, 764, 264, 14709, 293, 5622, 1622, 11, 291, 458, 11, 439, 300, 733, 295, 1507], "temperature": 0.0, "avg_logprob": -0.14010301549383936, "compression_ratio": 1.6916299559471366, "no_speech_prob": 4.092755261808634e-06}, {"id": 969, "seek": 535356, "start": 5380.6, "end": 5381.6, "text": " never goes out of style.", "tokens": [1128, 1709, 484, 295, 3758, 13], "temperature": 0.0, "avg_logprob": -0.14010301549383936, "compression_ratio": 1.6916299559471366, "no_speech_prob": 4.092755261808634e-06}, {"id": 970, "seek": 538160, "start": 5381.6, "end": 5383.8, "text": " It never gets old.", "tokens": [467, 1128, 2170, 1331, 13], "temperature": 0.0, "avg_logprob": -0.23161611885860048, "compression_ratio": 1.463768115942029, "no_speech_prob": 1.4823456240264932e-06}, {"id": 971, "seek": 538160, "start": 5383.8, "end": 5392.56, "text": " And yeah, I do plan to at some point, hopefully actually do a course really all about that", "tokens": [400, 1338, 11, 286, 360, 1393, 281, 412, 512, 935, 11, 4696, 767, 360, 257, 1164, 534, 439, 466, 300], "temperature": 0.0, "avg_logprob": -0.23161611885860048, "compression_ratio": 1.463768115942029, "no_speech_prob": 1.4823456240264932e-06}, {"id": 972, "seek": 538160, "start": 5392.56, "end": 5393.56, "text": " stuff specifically.", "tokens": [1507, 4682, 13], "temperature": 0.0, "avg_logprob": -0.23161611885860048, "compression_ratio": 1.463768115942029, "no_speech_prob": 1.4823456240264932e-06}, {"id": 973, "seek": 538160, "start": 5393.56, "end": 5399.200000000001, "text": " But yeah, for now, the for now, the best approach is follow along with the live coding and stuff.", "tokens": [583, 1338, 11, 337, 586, 11, 264, 337, 586, 11, 264, 1151, 3109, 307, 1524, 2051, 365, 264, 1621, 17720, 293, 1507, 13], "temperature": 0.0, "avg_logprob": -0.23161611885860048, "compression_ratio": 1.463768115942029, "no_speech_prob": 1.4823456240264932e-06}, {"id": 974, "seek": 538160, "start": 5399.200000000001, "end": 5406.4800000000005, "text": " Okay, WGPUBs, which is Wade, asks, how do you turn a model into a business?", "tokens": [1033, 11, 343, 38, 8115, 33, 82, 11, 597, 307, 28001, 11, 8962, 11, 577, 360, 291, 1261, 257, 2316, 666, 257, 1606, 30], "temperature": 0.0, "avg_logprob": -0.23161611885860048, "compression_ratio": 1.463768115942029, "no_speech_prob": 1.4823456240264932e-06}, {"id": 975, "seek": 540648, "start": 5406.48, "end": 5412.16, "text": " Specifically, how does a coder with little or no startup experience turn an ML-based", "tokens": [26058, 11, 577, 775, 257, 17656, 260, 365, 707, 420, 572, 18578, 1752, 1261, 364, 21601, 12, 6032], "temperature": 0.0, "avg_logprob": -0.13560226440429687, "compression_ratio": 1.4230769230769231, "no_speech_prob": 8.990916171569552e-07}, {"id": 976, "seek": 540648, "start": 5412.16, "end": 5415.759999999999, "text": " Gradio prototype into a legitimate business venture?", "tokens": [16710, 1004, 19475, 666, 257, 17956, 1606, 18474, 30], "temperature": 0.0, "avg_logprob": -0.13560226440429687, "compression_ratio": 1.4230769230769231, "no_speech_prob": 8.990916171569552e-07}, {"id": 977, "seek": 540648, "start": 5415.759999999999, "end": 5422.599999999999, "text": " Okay, I plan to do a course about this at some point as well.", "tokens": [1033, 11, 286, 1393, 281, 360, 257, 1164, 466, 341, 412, 512, 935, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.13560226440429687, "compression_ratio": 1.4230769230769231, "no_speech_prob": 8.990916171569552e-07}, {"id": 978, "seek": 540648, "start": 5422.599999999999, "end": 5431.24, "text": " So you know, obviously there isn't a two-minute version to this, but the key thing with creating", "tokens": [407, 291, 458, 11, 2745, 456, 1943, 380, 257, 732, 12, 18256, 3037, 281, 341, 11, 457, 264, 2141, 551, 365, 4084], "temperature": 0.0, "avg_logprob": -0.13560226440429687, "compression_ratio": 1.4230769230769231, "no_speech_prob": 8.990916171569552e-07}, {"id": 979, "seek": 543124, "start": 5431.24, "end": 5438.36, "text": " a legitimate business venture is to solve a legitimate problem, you know, a problem", "tokens": [257, 17956, 1606, 18474, 307, 281, 5039, 257, 17956, 1154, 11, 291, 458, 11, 257, 1154], "temperature": 0.0, "avg_logprob": -0.10025955879524963, "compression_ratio": 1.670391061452514, "no_speech_prob": 8.3152048091506e-07}, {"id": 980, "seek": 543124, "start": 5438.36, "end": 5445.4, "text": " that people need solving and which they will pay you to solve.", "tokens": [300, 561, 643, 12606, 293, 597, 436, 486, 1689, 291, 281, 5039, 13], "temperature": 0.0, "avg_logprob": -0.10025955879524963, "compression_ratio": 1.670391061452514, "no_speech_prob": 8.3152048091506e-07}, {"id": 981, "seek": 543124, "start": 5445.4, "end": 5451.8, "text": " And so it's important not to start with your fun Gradio prototype as a basis for your business,", "tokens": [400, 370, 309, 311, 1021, 406, 281, 722, 365, 428, 1019, 16710, 1004, 19475, 382, 257, 5143, 337, 428, 1606, 11], "temperature": 0.0, "avg_logprob": -0.10025955879524963, "compression_ratio": 1.670391061452514, "no_speech_prob": 8.3152048091506e-07}, {"id": 982, "seek": 543124, "start": 5451.8, "end": 5456.679999999999, "text": " but instead start with here's a problem I want to solve.", "tokens": [457, 2602, 722, 365, 510, 311, 257, 1154, 286, 528, 281, 5039, 13], "temperature": 0.0, "avg_logprob": -0.10025955879524963, "compression_ratio": 1.670391061452514, "no_speech_prob": 8.3152048091506e-07}, {"id": 983, "seek": 545668, "start": 5456.68, "end": 5463.4400000000005, "text": " And generally speaking, you should try to pick a problem that you understand better", "tokens": [400, 5101, 4124, 11, 291, 820, 853, 281, 1888, 257, 1154, 300, 291, 1223, 1101], "temperature": 0.0, "avg_logprob": -0.10876360718084842, "compression_ratio": 1.8099547511312217, "no_speech_prob": 1.7603331343707396e-06}, {"id": 984, "seek": 545668, "start": 5463.4400000000005, "end": 5465.08, "text": " than most people.", "tokens": [813, 881, 561, 13], "temperature": 0.0, "avg_logprob": -0.10876360718084842, "compression_ratio": 1.8099547511312217, "no_speech_prob": 1.7603331343707396e-06}, {"id": 985, "seek": 545668, "start": 5465.08, "end": 5470.240000000001, "text": " So it's either a problem that you face day to day in your work or in some hobby or passion", "tokens": [407, 309, 311, 2139, 257, 1154, 300, 291, 1851, 786, 281, 786, 294, 428, 589, 420, 294, 512, 18240, 420, 5418], "temperature": 0.0, "avg_logprob": -0.10876360718084842, "compression_ratio": 1.8099547511312217, "no_speech_prob": 1.7603331343707396e-06}, {"id": 986, "seek": 545668, "start": 5470.240000000001, "end": 5477.16, "text": " that you have or that, you know, your club has or your local school has or your spouse", "tokens": [300, 291, 362, 420, 300, 11, 291, 458, 11, 428, 6482, 575, 420, 428, 2654, 1395, 575, 420, 428, 23013], "temperature": 0.0, "avg_logprob": -0.10876360718084842, "compression_ratio": 1.8099547511312217, "no_speech_prob": 1.7603331343707396e-06}, {"id": 987, "seek": 545668, "start": 5477.16, "end": 5480.400000000001, "text": " deals with in their workplace.", "tokens": [11215, 365, 294, 641, 15328, 13], "temperature": 0.0, "avg_logprob": -0.10876360718084842, "compression_ratio": 1.8099547511312217, "no_speech_prob": 1.7603331343707396e-06}, {"id": 988, "seek": 545668, "start": 5480.400000000001, "end": 5486.400000000001, "text": " You know, it's something where you understand that there's something that doesn't work as", "tokens": [509, 458, 11, 309, 311, 746, 689, 291, 1223, 300, 456, 311, 746, 300, 1177, 380, 589, 382], "temperature": 0.0, "avg_logprob": -0.10876360718084842, "compression_ratio": 1.8099547511312217, "no_speech_prob": 1.7603331343707396e-06}, {"id": 989, "seek": 548640, "start": 5486.4, "end": 5491.96, "text": " well as it ought to, particularly something where you think to yourself, you know, if", "tokens": [731, 382, 309, 13416, 281, 11, 4098, 746, 689, 291, 519, 281, 1803, 11, 291, 458, 11, 498], "temperature": 0.0, "avg_logprob": -0.12872114060800285, "compression_ratio": 1.542857142857143, "no_speech_prob": 6.0488059716590215e-06}, {"id": 990, "seek": 548640, "start": 5491.96, "end": 5499.92, "text": " they just used deep learning here or some algorithm here or some better compute here,", "tokens": [436, 445, 1143, 2452, 2539, 510, 420, 512, 9284, 510, 420, 512, 1101, 14722, 510, 11], "temperature": 0.0, "avg_logprob": -0.12872114060800285, "compression_ratio": 1.542857142857143, "no_speech_prob": 6.0488059716590215e-06}, {"id": 991, "seek": 548640, "start": 5499.92, "end": 5502.96, "text": " that problem would go away.", "tokens": [300, 1154, 576, 352, 1314, 13], "temperature": 0.0, "avg_logprob": -0.12872114060800285, "compression_ratio": 1.542857142857143, "no_speech_prob": 6.0488059716590215e-06}, {"id": 992, "seek": 548640, "start": 5502.96, "end": 5506.74, "text": " And that's the start of a business.", "tokens": [400, 300, 311, 264, 722, 295, 257, 1606, 13], "temperature": 0.0, "avg_logprob": -0.12872114060800285, "compression_ratio": 1.542857142857143, "no_speech_prob": 6.0488059716590215e-06}, {"id": 993, "seek": 548640, "start": 5506.74, "end": 5514.2, "text": " And so then my friend Eric Reese wrote a book called The Lean Startup where he describes", "tokens": [400, 370, 550, 452, 1277, 9336, 49474, 4114, 257, 1446, 1219, 440, 49303, 6481, 1010, 689, 415, 15626], "temperature": 0.0, "avg_logprob": -0.12872114060800285, "compression_ratio": 1.542857142857143, "no_speech_prob": 6.0488059716590215e-06}, {"id": 994, "seek": 551420, "start": 5514.2, "end": 5517.72, "text": " what you do next, which is basically you fake it.", "tokens": [437, 291, 360, 958, 11, 597, 307, 1936, 291, 7592, 309, 13], "temperature": 0.0, "avg_logprob": -0.12252716168965379, "compression_ratio": 1.7932203389830508, "no_speech_prob": 7.88909073889954e-06}, {"id": 995, "seek": 551420, "start": 5517.72, "end": 5520.24, "text": " You create, so he calls it the minimum viable product.", "tokens": [509, 1884, 11, 370, 415, 5498, 309, 264, 7285, 22024, 1674, 13], "temperature": 0.0, "avg_logprob": -0.12252716168965379, "compression_ratio": 1.7932203389830508, "no_speech_prob": 7.88909073889954e-06}, {"id": 996, "seek": 551420, "start": 5520.24, "end": 5525.48, "text": " You create something that solves that problem that takes you as little time as possible", "tokens": [509, 1884, 746, 300, 39890, 300, 1154, 300, 2516, 291, 382, 707, 565, 382, 1944], "temperature": 0.0, "avg_logprob": -0.12252716168965379, "compression_ratio": 1.7932203389830508, "no_speech_prob": 7.88909073889954e-06}, {"id": 997, "seek": 551420, "start": 5525.48, "end": 5526.48, "text": " to create.", "tokens": [281, 1884, 13], "temperature": 0.0, "avg_logprob": -0.12252716168965379, "compression_ratio": 1.7932203389830508, "no_speech_prob": 7.88909073889954e-06}, {"id": 998, "seek": 551420, "start": 5526.48, "end": 5527.48, "text": " It could be very manual.", "tokens": [467, 727, 312, 588, 9688, 13], "temperature": 0.0, "avg_logprob": -0.12252716168965379, "compression_ratio": 1.7932203389830508, "no_speech_prob": 7.88909073889954e-06}, {"id": 999, "seek": 551420, "start": 5527.48, "end": 5528.78, "text": " It can be loss making.", "tokens": [467, 393, 312, 4470, 1455, 13], "temperature": 0.0, "avg_logprob": -0.12252716168965379, "compression_ratio": 1.7932203389830508, "no_speech_prob": 7.88909073889954e-06}, {"id": 1000, "seek": 551420, "start": 5528.78, "end": 5529.78, "text": " It's fine.", "tokens": [467, 311, 2489, 13], "temperature": 0.0, "avg_logprob": -0.12252716168965379, "compression_ratio": 1.7932203389830508, "no_speech_prob": 7.88909073889954e-06}, {"id": 1001, "seek": 551420, "start": 5529.78, "end": 5532.2, "text": " You know, even the bit in the middle where you're like, oh, there's going to be a neural", "tokens": [509, 458, 11, 754, 264, 857, 294, 264, 2808, 689, 291, 434, 411, 11, 1954, 11, 456, 311, 516, 281, 312, 257, 18161], "temperature": 0.0, "avg_logprob": -0.12252716168965379, "compression_ratio": 1.7932203389830508, "no_speech_prob": 7.88909073889954e-06}, {"id": 1002, "seek": 551420, "start": 5532.2, "end": 5533.48, "text": " net here.", "tokens": [2533, 510, 13], "temperature": 0.0, "avg_logprob": -0.12252716168965379, "compression_ratio": 1.7932203389830508, "no_speech_prob": 7.88909073889954e-06}, {"id": 1003, "seek": 551420, "start": 5533.48, "end": 5538.88, "text": " It's fine to like launch without the neural net and do everything by hand.", "tokens": [467, 311, 2489, 281, 411, 4025, 1553, 264, 18161, 2533, 293, 360, 1203, 538, 1011, 13], "temperature": 0.0, "avg_logprob": -0.12252716168965379, "compression_ratio": 1.7932203389830508, "no_speech_prob": 7.88909073889954e-06}, {"id": 1004, "seek": 551420, "start": 5538.88, "end": 5541.639999999999, "text": " You're just trying to find out are people going to pay for this and is this actually", "tokens": [509, 434, 445, 1382, 281, 915, 484, 366, 561, 516, 281, 1689, 337, 341, 293, 307, 341, 767], "temperature": 0.0, "avg_logprob": -0.12252716168965379, "compression_ratio": 1.7932203389830508, "no_speech_prob": 7.88909073889954e-06}, {"id": 1005, "seek": 551420, "start": 5541.639999999999, "end": 5543.26, "text": " useful?", "tokens": [4420, 30], "temperature": 0.0, "avg_logprob": -0.12252716168965379, "compression_ratio": 1.7932203389830508, "no_speech_prob": 7.88909073889954e-06}, {"id": 1006, "seek": 554326, "start": 5543.26, "end": 5548.4800000000005, "text": " And then once you have, you know, hopefully confirmed that the need is real and that people", "tokens": [400, 550, 1564, 291, 362, 11, 291, 458, 11, 4696, 11341, 300, 264, 643, 307, 957, 293, 300, 561], "temperature": 0.0, "avg_logprob": -0.16854371388753256, "compression_ratio": 1.725, "no_speech_prob": 1.0348468322263216e-06}, {"id": 1007, "seek": 554326, "start": 5548.4800000000005, "end": 5553.280000000001, "text": " will pay for it and you can solve the need, you can gradually make it less and less of", "tokens": [486, 1689, 337, 309, 293, 291, 393, 5039, 264, 643, 11, 291, 393, 13145, 652, 309, 1570, 293, 1570, 295], "temperature": 0.0, "avg_logprob": -0.16854371388753256, "compression_ratio": 1.725, "no_speech_prob": 1.0348468322263216e-06}, {"id": 1008, "seek": 554326, "start": 5553.280000000001, "end": 5559.88, "text": " a fake, you know, and do, you know, more and more getting the product to where you want", "tokens": [257, 7592, 11, 291, 458, 11, 293, 360, 11, 291, 458, 11, 544, 293, 544, 1242, 264, 1674, 281, 689, 291, 528], "temperature": 0.0, "avg_logprob": -0.16854371388753256, "compression_ratio": 1.725, "no_speech_prob": 1.0348468322263216e-06}, {"id": 1009, "seek": 554326, "start": 5559.88, "end": 5563.88, "text": " it to be.", "tokens": [309, 281, 312, 13], "temperature": 0.0, "avg_logprob": -0.16854371388753256, "compression_ratio": 1.725, "no_speech_prob": 1.0348468322263216e-06}, {"id": 1010, "seek": 556388, "start": 5563.88, "end": 5577.32, "text": " Okay, I don't know how to pronounce the name M-I-W-O-J-C. M-I-W-O-J-C says, Jeremy, can", "tokens": [1033, 11, 286, 500, 380, 458, 577, 281, 19567, 264, 1315, 376, 12, 40, 12, 54, 12, 46, 12, 41, 12, 34, 13, 376, 12, 40, 12, 54, 12, 46, 12, 41, 12, 34, 1619, 11, 17809, 11, 393], "temperature": 0.0, "avg_logprob": -0.16576623916625977, "compression_ratio": 1.3652694610778444, "no_speech_prob": 1.6797229136500391e-06}, {"id": 1011, "seek": 556388, "start": 5577.32, "end": 5582.0, "text": " you share some of your productivity hacks from the content you produce?", "tokens": [291, 2073, 512, 295, 428, 15604, 33617, 490, 264, 2701, 291, 5258, 30], "temperature": 0.0, "avg_logprob": -0.16576623916625977, "compression_ratio": 1.3652694610778444, "no_speech_prob": 1.6797229136500391e-06}, {"id": 1012, "seek": 556388, "start": 5582.0, "end": 5585.88, "text": " It may seem you work 24 hours a day.", "tokens": [467, 815, 1643, 291, 589, 4022, 2496, 257, 786, 13], "temperature": 0.0, "avg_logprob": -0.16576623916625977, "compression_ratio": 1.3652694610778444, "no_speech_prob": 1.6797229136500391e-06}, {"id": 1013, "seek": 556388, "start": 5585.88, "end": 5589.76, "text": " Okay I certainly don't do that.", "tokens": [1033, 286, 3297, 500, 380, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.16576623916625977, "compression_ratio": 1.3652694610778444, "no_speech_prob": 1.6797229136500391e-06}, {"id": 1014, "seek": 558976, "start": 5589.76, "end": 5595.16, "text": " I think one of my main productivity hacks actually is not to work too hard or at least", "tokens": [286, 519, 472, 295, 452, 2135, 15604, 33617, 767, 307, 406, 281, 589, 886, 1152, 420, 412, 1935], "temperature": 0.0, "avg_logprob": -0.12456038625616776, "compression_ratio": 1.6545454545454545, "no_speech_prob": 1.3496838846549508e-06}, {"id": 1015, "seek": 558976, "start": 5595.16, "end": 5599.04, "text": " no not to work too hard, not to work too much.", "tokens": [572, 406, 281, 589, 886, 1152, 11, 406, 281, 589, 886, 709, 13], "temperature": 0.0, "avg_logprob": -0.12456038625616776, "compression_ratio": 1.6545454545454545, "no_speech_prob": 1.3496838846549508e-06}, {"id": 1016, "seek": 558976, "start": 5599.04, "end": 5606.6, "text": " I spend probably less hours a day working than most people, I would guess.", "tokens": [286, 3496, 1391, 1570, 2496, 257, 786, 1364, 813, 881, 561, 11, 286, 576, 2041, 13], "temperature": 0.0, "avg_logprob": -0.12456038625616776, "compression_ratio": 1.6545454545454545, "no_speech_prob": 1.3496838846549508e-06}, {"id": 1017, "seek": 558976, "start": 5606.6, "end": 5610.6, "text": " But I think I do a couple of things differently when I'm working.", "tokens": [583, 286, 519, 286, 360, 257, 1916, 295, 721, 7614, 562, 286, 478, 1364, 13], "temperature": 0.0, "avg_logprob": -0.12456038625616776, "compression_ratio": 1.6545454545454545, "no_speech_prob": 1.3496838846549508e-06}, {"id": 1018, "seek": 558976, "start": 5610.6, "end": 5618.820000000001, "text": " One is I've spent half, at least half of every working day since I was about 18, learning", "tokens": [1485, 307, 286, 600, 4418, 1922, 11, 412, 1935, 1922, 295, 633, 1364, 786, 1670, 286, 390, 466, 2443, 11, 2539], "temperature": 0.0, "avg_logprob": -0.12456038625616776, "compression_ratio": 1.6545454545454545, "no_speech_prob": 1.3496838846549508e-06}, {"id": 1019, "seek": 561882, "start": 5618.82, "end": 5620.92, "text": " or practicing something new.", "tokens": [420, 11350, 746, 777, 13], "temperature": 0.0, "avg_logprob": -0.13758955831113068, "compression_ratio": 1.6561085972850678, "no_speech_prob": 4.495103439694503e-06}, {"id": 1020, "seek": 561882, "start": 5620.92, "end": 5628.639999999999, "text": " Could be a new language, could be a new algorithm, could be something I read about.", "tokens": [7497, 312, 257, 777, 2856, 11, 727, 312, 257, 777, 9284, 11, 727, 312, 746, 286, 1401, 466, 13], "temperature": 0.0, "avg_logprob": -0.13758955831113068, "compression_ratio": 1.6561085972850678, "no_speech_prob": 4.495103439694503e-06}, {"id": 1021, "seek": 561882, "start": 5628.639999999999, "end": 5636.599999999999, "text": " And nearly all of that time therefore I've been doing that thing more slowly than I would", "tokens": [400, 6217, 439, 295, 300, 565, 4412, 286, 600, 668, 884, 300, 551, 544, 5692, 813, 286, 576], "temperature": 0.0, "avg_logprob": -0.13758955831113068, "compression_ratio": 1.6561085972850678, "no_speech_prob": 4.495103439694503e-06}, {"id": 1022, "seek": 561882, "start": 5636.599999999999, "end": 5642.36, "text": " if I just used something I already knew.", "tokens": [498, 286, 445, 1143, 746, 286, 1217, 2586, 13], "temperature": 0.0, "avg_logprob": -0.13758955831113068, "compression_ratio": 1.6561085972850678, "no_speech_prob": 4.495103439694503e-06}, {"id": 1023, "seek": 561882, "start": 5642.36, "end": 5647.16, "text": " Which often drives my co-workers crazy because they're like, you know, why aren't you focusing", "tokens": [3013, 2049, 11754, 452, 598, 12, 37101, 3219, 570, 436, 434, 411, 11, 291, 458, 11, 983, 3212, 380, 291, 8416], "temperature": 0.0, "avg_logprob": -0.13758955831113068, "compression_ratio": 1.6561085972850678, "no_speech_prob": 4.495103439694503e-06}, {"id": 1024, "seek": 561882, "start": 5647.16, "end": 5648.759999999999, "text": " on getting that thing done?", "tokens": [322, 1242, 300, 551, 1096, 30], "temperature": 0.0, "avg_logprob": -0.13758955831113068, "compression_ratio": 1.6561085972850678, "no_speech_prob": 4.495103439694503e-06}, {"id": 1025, "seek": 564876, "start": 5648.76, "end": 5658.12, "text": " But in the other 50% of the time I'm constantly building up this kind of exponentially improving", "tokens": [583, 294, 264, 661, 2625, 4, 295, 264, 565, 286, 478, 6460, 2390, 493, 341, 733, 295, 37330, 11470], "temperature": 0.0, "avg_logprob": -0.12062504484846785, "compression_ratio": 1.5792079207920793, "no_speech_prob": 8.530033483111765e-06}, {"id": 1026, "seek": 564876, "start": 5658.12, "end": 5661.68, "text": " base of expertise in a wide range of areas.", "tokens": [3096, 295, 11769, 294, 257, 4874, 3613, 295, 3179, 13], "temperature": 0.0, "avg_logprob": -0.12062504484846785, "compression_ratio": 1.5792079207920793, "no_speech_prob": 8.530033483111765e-06}, {"id": 1027, "seek": 564876, "start": 5661.68, "end": 5671.0, "text": " And so now I do find I can do things, often orders of magnitude faster than people around", "tokens": [400, 370, 586, 286, 360, 915, 286, 393, 360, 721, 11, 2049, 9470, 295, 15668, 4663, 813, 561, 926], "temperature": 0.0, "avg_logprob": -0.12062504484846785, "compression_ratio": 1.5792079207920793, "no_speech_prob": 8.530033483111765e-06}, {"id": 1028, "seek": 564876, "start": 5671.0, "end": 5676.12, "text": " me or certainly many multiples faster than people around me because I know a whole bunch", "tokens": [385, 420, 3297, 867, 46099, 4663, 813, 561, 926, 385, 570, 286, 458, 257, 1379, 3840], "temperature": 0.0, "avg_logprob": -0.12062504484846785, "compression_ratio": 1.5792079207920793, "no_speech_prob": 8.530033483111765e-06}, {"id": 1029, "seek": 567612, "start": 5676.12, "end": 5683.44, "text": " of tools and skills and ideas which other people don't necessarily know.", "tokens": [295, 3873, 293, 3942, 293, 3487, 597, 661, 561, 500, 380, 4725, 458, 13], "temperature": 0.0, "avg_logprob": -0.18364407265023008, "compression_ratio": 1.5397727272727273, "no_speech_prob": 7.071632353472523e-06}, {"id": 1030, "seek": 567612, "start": 5683.44, "end": 5685.64, "text": " So I think that's one thing that's been helpful.", "tokens": [407, 286, 519, 300, 311, 472, 551, 300, 311, 668, 4961, 13], "temperature": 0.0, "avg_logprob": -0.18364407265023008, "compression_ratio": 1.5397727272727273, "no_speech_prob": 7.071632353472523e-06}, {"id": 1031, "seek": 567612, "start": 5685.64, "end": 5692.64, "text": " And then another is trying to really not overdo things, like get good sleep and eat well and", "tokens": [400, 550, 1071, 307, 1382, 281, 534, 406, 670, 2595, 721, 11, 411, 483, 665, 2817, 293, 1862, 731, 293], "temperature": 0.0, "avg_logprob": -0.18364407265023008, "compression_ratio": 1.5397727272727273, "no_speech_prob": 7.071632353472523e-06}, {"id": 1032, "seek": 567612, "start": 5692.64, "end": 5698.28, "text": " exercise well.", "tokens": [5380, 731, 13], "temperature": 0.0, "avg_logprob": -0.18364407265023008, "compression_ratio": 1.5397727272727273, "no_speech_prob": 7.071632353472523e-06}, {"id": 1033, "seek": 567612, "start": 5698.28, "end": 5702.8, "text": " And also I think it's a case of tenacity.", "tokens": [400, 611, 286, 519, 309, 311, 257, 1389, 295, 2064, 19008, 13], "temperature": 0.0, "avg_logprob": -0.18364407265023008, "compression_ratio": 1.5397727272727273, "no_speech_prob": 7.071632353472523e-06}, {"id": 1034, "seek": 570280, "start": 5702.8, "end": 5709.2, "text": " I've noticed a lot of people give up much earlier than I do.", "tokens": [286, 600, 5694, 257, 688, 295, 561, 976, 493, 709, 3071, 813, 286, 360, 13], "temperature": 0.0, "avg_logprob": -0.13600805827549525, "compression_ratio": 1.6502242152466369, "no_speech_prob": 2.9479833756340668e-06}, {"id": 1035, "seek": 570280, "start": 5709.2, "end": 5719.56, "text": " So yeah, if you just keep going until something's actually finished then that's going to put", "tokens": [407, 1338, 11, 498, 291, 445, 1066, 516, 1826, 746, 311, 767, 4335, 550, 300, 311, 516, 281, 829], "temperature": 0.0, "avg_logprob": -0.13600805827549525, "compression_ratio": 1.6502242152466369, "no_speech_prob": 2.9479833756340668e-06}, {"id": 1036, "seek": 570280, "start": 5719.56, "end": 5722.16, "text": " you in a small minority to be honest.", "tokens": [291, 294, 257, 1359, 16166, 281, 312, 3245, 13], "temperature": 0.0, "avg_logprob": -0.13600805827549525, "compression_ratio": 1.6502242152466369, "no_speech_prob": 2.9479833756340668e-06}, {"id": 1037, "seek": 570280, "start": 5722.16, "end": 5723.4800000000005, "text": " Most people don't do that.", "tokens": [4534, 561, 500, 380, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.13600805827549525, "compression_ratio": 1.6502242152466369, "no_speech_prob": 2.9479833756340668e-06}, {"id": 1038, "seek": 570280, "start": 5723.4800000000005, "end": 5727.8, "text": " When I say finished, like finish something really nicely.", "tokens": [1133, 286, 584, 4335, 11, 411, 2413, 746, 534, 9594, 13], "temperature": 0.0, "avg_logprob": -0.13600805827549525, "compression_ratio": 1.6502242152466369, "no_speech_prob": 2.9479833756340668e-06}, {"id": 1039, "seek": 570280, "start": 5727.8, "end": 5732.12, "text": " And I try to make it like, so I particularly like coding and so I try to do a lot of coding", "tokens": [400, 286, 853, 281, 652, 309, 411, 11, 370, 286, 4098, 411, 17720, 293, 370, 286, 853, 281, 360, 257, 688, 295, 17720], "temperature": 0.0, "avg_logprob": -0.13600805827549525, "compression_ratio": 1.6502242152466369, "no_speech_prob": 2.9479833756340668e-06}, {"id": 1040, "seek": 573212, "start": 5732.12, "end": 5733.4, "text": " related stuff.", "tokens": [4077, 1507, 13], "temperature": 0.0, "avg_logprob": -0.17696202885020862, "compression_ratio": 1.541871921182266, "no_speech_prob": 6.7478540586307645e-06}, {"id": 1041, "seek": 573212, "start": 5733.4, "end": 5739.32, "text": " So I create things like NBdev and NBdev makes it much much easier for me to finish something", "tokens": [407, 286, 1884, 721, 411, 426, 33, 40343, 293, 426, 33, 40343, 1669, 309, 709, 709, 3571, 337, 385, 281, 2413, 746], "temperature": 0.0, "avg_logprob": -0.17696202885020862, "compression_ratio": 1.541871921182266, "no_speech_prob": 6.7478540586307645e-06}, {"id": 1042, "seek": 573212, "start": 5739.32, "end": 5742.48, "text": " nicely.", "tokens": [9594, 13], "temperature": 0.0, "avg_logprob": -0.17696202885020862, "compression_ratio": 1.541871921182266, "no_speech_prob": 6.7478540586307645e-06}, {"id": 1043, "seek": 573212, "start": 5742.48, "end": 5747.12, "text": " So in my kind of chosen area I've spent quite a bit of time trying to make sure it's really", "tokens": [407, 294, 452, 733, 295, 8614, 1859, 286, 600, 4418, 1596, 257, 857, 295, 565, 1382, 281, 652, 988, 309, 311, 534], "temperature": 0.0, "avg_logprob": -0.17696202885020862, "compression_ratio": 1.541871921182266, "no_speech_prob": 6.7478540586307645e-06}, {"id": 1044, "seek": 573212, "start": 5747.12, "end": 5753.88, "text": " easy for me to like get out a blog post, get out a Python library, get out a notebook analysis,", "tokens": [1858, 337, 385, 281, 411, 483, 484, 257, 6968, 2183, 11, 483, 484, 257, 15329, 6405, 11, 483, 484, 257, 21060, 5215, 11], "temperature": 0.0, "avg_logprob": -0.17696202885020862, "compression_ratio": 1.541871921182266, "no_speech_prob": 6.7478540586307645e-06}, {"id": 1045, "seek": 573212, "start": 5753.88, "end": 5755.24, "text": " whatever.", "tokens": [2035, 13], "temperature": 0.0, "avg_logprob": -0.17696202885020862, "compression_ratio": 1.541871921182266, "no_speech_prob": 6.7478540586307645e-06}, {"id": 1046, "seek": 575524, "start": 5755.24, "end": 5765.04, "text": " So yeah, trying to make these things I want to do easier and so then I'll do them more.", "tokens": [407, 1338, 11, 1382, 281, 652, 613, 721, 286, 528, 281, 360, 3571, 293, 370, 550, 286, 603, 360, 552, 544, 13], "temperature": 0.0, "avg_logprob": -0.1221637842131824, "compression_ratio": 1.4950980392156863, "no_speech_prob": 1.5294058357540052e-06}, {"id": 1047, "seek": 575524, "start": 5765.04, "end": 5768.96, "text": " So well thank you everybody.", "tokens": [407, 731, 1309, 291, 2201, 13], "temperature": 0.0, "avg_logprob": -0.1221637842131824, "compression_ratio": 1.4950980392156863, "no_speech_prob": 1.5294058357540052e-06}, {"id": 1048, "seek": 575524, "start": 5768.96, "end": 5771.639999999999, "text": " That's been a lot of fun.", "tokens": [663, 311, 668, 257, 688, 295, 1019, 13], "temperature": 0.0, "avg_logprob": -0.1221637842131824, "compression_ratio": 1.4950980392156863, "no_speech_prob": 1.5294058357540052e-06}, {"id": 1049, "seek": 575524, "start": 5771.639999999999, "end": 5775.679999999999, "text": " Really appreciate you taking the time to go through this course with me.", "tokens": [4083, 4449, 291, 1940, 264, 565, 281, 352, 807, 341, 1164, 365, 385, 13], "temperature": 0.0, "avg_logprob": -0.1221637842131824, "compression_ratio": 1.4950980392156863, "no_speech_prob": 1.5294058357540052e-06}, {"id": 1050, "seek": 575524, "start": 5775.679999999999, "end": 5782.84, "text": " Yeah, if you enjoyed it, it would really help if you would give a like on YouTube because", "tokens": [865, 11, 498, 291, 4626, 309, 11, 309, 576, 534, 854, 498, 291, 576, 976, 257, 411, 322, 3088, 570], "temperature": 0.0, "avg_logprob": -0.1221637842131824, "compression_ratio": 1.4950980392156863, "no_speech_prob": 1.5294058357540052e-06}, {"id": 1051, "seek": 578284, "start": 5782.84, "end": 5787.92, "text": " it really helps other people find the course, goes into the YouTube recommendation system.", "tokens": [309, 534, 3665, 661, 561, 915, 264, 1164, 11, 1709, 666, 264, 3088, 11879, 1185, 13], "temperature": 0.0, "avg_logprob": -0.12284068868617819, "compression_ratio": 1.598326359832636, "no_speech_prob": 1.0782675417431165e-05}, {"id": 1052, "seek": 578284, "start": 5787.92, "end": 5793.68, "text": " And please do come and help other beginners on forums.fast.ai.", "tokens": [400, 1767, 360, 808, 293, 854, 661, 26992, 322, 26998, 13, 7011, 13, 1301, 13], "temperature": 0.0, "avg_logprob": -0.12284068868617819, "compression_ratio": 1.598326359832636, "no_speech_prob": 1.0782675417431165e-05}, {"id": 1053, "seek": 578284, "start": 5793.68, "end": 5798.08, "text": " It's a great way to learn yourself is to try to teach other people.", "tokens": [467, 311, 257, 869, 636, 281, 1466, 1803, 307, 281, 853, 281, 2924, 661, 561, 13], "temperature": 0.0, "avg_logprob": -0.12284068868617819, "compression_ratio": 1.598326359832636, "no_speech_prob": 1.0782675417431165e-05}, {"id": 1054, "seek": 578284, "start": 5798.08, "end": 5803.72, "text": " And yeah, I hope you'll join us in part two.", "tokens": [400, 1338, 11, 286, 1454, 291, 603, 3917, 505, 294, 644, 732, 13], "temperature": 0.0, "avg_logprob": -0.12284068868617819, "compression_ratio": 1.598326359832636, "no_speech_prob": 1.0782675417431165e-05}, {"id": 1055, "seek": 578284, "start": 5803.72, "end": 5805.84, "text": " Thanks everybody very much.", "tokens": [2561, 2201, 588, 709, 13], "temperature": 0.0, "avg_logprob": -0.12284068868617819, "compression_ratio": 1.598326359832636, "no_speech_prob": 1.0782675417431165e-05}, {"id": 1056, "seek": 578284, "start": 5805.84, "end": 5811.6, "text": " I've really enjoyed this process and I hope to get to meet more of you in person in the", "tokens": [286, 600, 534, 4626, 341, 1399, 293, 286, 1454, 281, 483, 281, 1677, 544, 295, 291, 294, 954, 294, 264], "temperature": 0.0, "avg_logprob": -0.12284068868617819, "compression_ratio": 1.598326359832636, "no_speech_prob": 1.0782675417431165e-05}, {"id": 1057, "seek": 581160, "start": 5811.6, "end": 5812.92, "text": " future.", "tokens": [2027, 13], "temperature": 0.0, "avg_logprob": -0.5033738878038194, "compression_ratio": 0.6, "no_speech_prob": 0.00019657955272123218}, {"id": 1058, "seek": 581292, "start": 5812.92, "end": 5841.32, "text": " Bye.", "tokens": [50364, 4621, 13, 51784], "temperature": 0.0, "avg_logprob": -0.6630663871765137, "compression_ratio": 0.3333333333333333, "no_speech_prob": 9.6967451099772e-05}], "language": "en"}