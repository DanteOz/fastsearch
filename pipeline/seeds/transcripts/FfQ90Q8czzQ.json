{"text": " How to learn deep learning when you're not a computer science PhD or even if you are. So I'm Rachel Thomas. I'm on Twitter at math underscore Rachel and I blog about data science at fast.ai. Briefly, my background is I studied math and computer science in undergrad and then did a PhD in pure math. I worked as a quant and energy trading for two years and then I was a data scientist and backend engineer at Uber. I taught full stack software development at Hackbrite. I really love teaching. I had taught calculus while I was in grad school and I think I'll probably always return to teaching. One year ago, I started fast.ai together with Jeremy Howard, who's the previous president of Kaggle and founder of Enlytic which was the first company to use deep learning for medicine. Our goal is to make deep learning more accessible and easier to use. So I'm going to describe the gap that we saw in deep learning and with technical teaching, how we addressed it with our course. So we have a course that we taught in person together with the University of San Francisco's Data Institute. It was open to the community and then we released it for free online, and over 50,000 people have started it. Sorry, and then I'll share some lessons that will hopefully be useful for you on your learning journey. I'm going to begin my story at the Bay Area Machine Learning Meetup back in 2013. I want to thank Dave that he's been organizing this for so long. It's a huge help to our community. I first got interested in deep learning in 2013, and so I was really excited to see that Ilya Suskovor was going to be speaking. At that point, there were not that many deep learning materials available online, and this was before any of the open source frameworks had been released. I was really looking for practical info on how I could implement a neural net at home, and Ilya's talk was mostly theoretical. I asked a question at the end about how he had initialized his weights, and he said, that's part of a dirty bag of tricks that nobody publishes. I was so disappointed, but that was the message I was getting a lot of places at that point, that everyone doing deep learning had gotten their PhD with the same four advisors, and they weren't publishing the practical info. The field has come a long way since then, but there's still this problem with exclusivity. This is a comment from Hacker News that was voted to the top of its thread a few months ago, and I completely disagree with it, but it's saying that to get into machine learning, you first need to spend years taking quote boring math courses, and then you need to learn CUDA and MPI, and then years later, you can start touching Theano and TensorFlow. I disagree with this advice, but I think this attitude keeps people out of the field and from trying it. There's an essay that I really love called A Mathematician's Lament by Paul Lockhart. Paul Lockhart was a math professor at Brown before he quit to go teach K-12, because he thought the state of math education in the US was such a disaster. He describes this nightmare world where children are not allowed to sing songs, or play instruments until graduate school, because they need to have over a decade of studying music notation and transcribing sheet music by hand before they can be trusted to sing. Hopefully, this sounds horrifying to you. He says, that's what we do with math. We force students to learn dry disconnected notation and save the really fun and creative parts till after most of them have dropped out of the field. Sadly, we're carrying this over into deep learning as well often. This is how the most popular textbook on deep learning encourages you to develop intuition for back propagation through time. So what do you think? Is this pretty intuitive? This is Ian Goodfellow's book, and I think it's a useful resource, but ultimately it's a theoretical math textbook. There's no code in the book. So to summarize, some of the problems that I see with technical teaching, it tends to be math-centric. I love math, however, if you're trying to build something, code is more useful. There's something that Harvard professor David Perkins calls elementitis, and that's feeling like you have to teach each individual element before years later, you can assemble those components to build something cool. He gives an analogy with baseball and says, we don't say that little kids have to memorize all the rules for baseball and understand all the technical details before they're allowed to play. They can play and get a general idea of the game, even if they don't know all the details. Then of the more practical or code-focused tutorials or blog post out there, many of them are on simple toy problems and settle for these good enough results that wouldn't be acceptable in the workplace. So fast.ai, we wanted to address this. Where our slogan is making neural nets uncool again. That's because being cool is about being exclusive, which is the opposite of what we want. We're particularly interested in people using smaller obscure data sets, and people that don't have many resources and can't afford much computation time or GPU power. So we taught a course in partnership with the University of San Francisco's Data Institute. This was open to the community and it was one evening a week for seven weeks, although many hours of homework outside of class to go along with that. We had this hypothesis, can we teach deep learning to coders with no explicit math prerequisites? We weren't even sure that it could be done, but it was a big success. We've now released the course online completely for free. All the videos are on YouTube, the code is on GitHub, and 50,000 people have started the course. Some of the success stories we've heard, one of our students who has two years of coding experience was just accepted to the Google Brain residency program, which is very prestigious. We have another student who he does not even call himself a data scientist, but he came up with a new fraud detection technique for his company. He said it's only like 10 lines of code, but it earned him a large bonus. We have several more students that have received job offers or internship offers, that have been able to switch teams at work. I know of one student that's contributed back to TensorFlow's code, and then four students that have won hackathons. I want to introduce you to a few of our students that really, I think, capture our mission. On the bottom right is Samar Haider. He's a natural language researcher in Pakistan. Pakistan has 70 spoken languages and none of them have that many resources, not even Urdu, which is the most popular language for translation to and from, for other text. As part of our course, Samar put together the largest corpus of Urdu that's been assembled, and he trained word embeddings on that and has found some really interesting connections in the language. For machine learning, he's largely self-taught using online courses. Above him are some pictures sent to me by Sahil Singla of Farm Guide in India. Thousands of farmers commit suicide every year in India, because they're taking these very predatory loans from loan sharks, who then threaten them with violence and harass their families. Part of the problem is they can't prove how much land they own, or what type of crops they're growing. Sahil is scraping data from Google Earth and then applying CNNs to identify their plots and use this so that they can qualify for better loans. Bottom left is Sarah Hooker and a team of data scientists from Delta Analytics. Delta Analytics partners non-profits with teams of data scientists who volunteer to help work with them. Rainforest Connection is a startup that is putting recycled cell phones in endangered rainforest, and then streaming audio to identify chainsaw noises, and send out an alarm when there's someone cutting down the forest. This is a team of data scientists, they were running some tests in Berkeley, but they're using deep learning to identify chainsaws. Then above that, top left is a picture of Tosin Maisha. She's a student in Bangladesh. She's done a project analyzing how the major Bangladeshi newspaper covers violence against women by doing a visualization of having scraped the text. She's blogged about the challenges of doing machine learning with intermittent electricity. She currently has some free AWS credits that she was gifted, but she doesn't know what she's going to do when they run out, because the expense is going to be a lot. These are the type of problems and the type of students that we are really interested in and thinking about people with few resources who have projects that they're really interested in. So most math classes are bottom up. You learn each building block you need, and then eventually you can put them together. It's hard for a lot of students to maintain motivation with that. It's also hard to know what the big picture is or where you're going. It's hard to know which pieces you'll actually need. So I sometimes talk to people that are studying theory that they think they're going to need for deep learning, but don't actually end up needing. So we tried to get students using a neural net right away and six lines of code. We spent a lot of time refactoring our code to help make it concise and modular. We give students an Amazon machine image that's set up with everything they need. Then over time, we do gradually peel back the layers and get to lower level stuff. It's always motivated though by trying to improve our performance or to tackle a harder problem. So eventually they are understanding the low level components. We've just gone in the reverse order. Then we are huge fans of transfer learning. Transfer learning is the technique where you take a pre-trained network that was trained on a different dataset and a different problem and apply it to your problem, and often you retrain the last layer or the last few layers. It's really powerful because it lets people with smaller datasets and limited computational power take advantage of these networks that have been trained elsewhere. So this is from lesson one. It's six lines of code and it gets you better than 97 percent accuracy on dogs versus cats, which has been a Kaggle competition twice now. Here we're using VGG, which was the ImageNet winner in 2014, and then just retraining the last layer. So what does all this mean for you? What lessons can you take away? So my very opinionated recommendation is to start with Keras. Keras is a Python library that sits on top of either Theano or TensorFlow, and the abstractions in Keras are just so well-mapped to the abstractions of neural networks that I think it makes it a lot easier to learn both. I wrote a blog post several months ago where I said, using TensorFlow makes me feel like I'm not smart enough to use TensorFlow, whereas using Keras makes me feel like neural networks are easier than I realized. This was a very popular post and a lot of people said they agreed. So we used Theano and Keras in part one of the course. We do use TensorFlow and PyTorch, which is also excellent in part two, but I still really recommend starting with Keras. I should say I have not tried MXNet yet, and so I'm looking forward to hearing the next talk about that. Don't read or watch tutorials without taking time to code. It's by coding that you learn. Then hitting Shift-Enter to go through somebody else's Jupyter Notebook does not count as coding. You really need to type it out yourself. You need to modify the inputs and see how that impacts the outputs. Then have a programming project that you're interested in and let that drive your learning. Don't feel like you need to spend several months learning before you can tackle the project you're interested in. Start right away and then that'll keep you always learning the most relevant things you need to tackle the problem you care about. Then finally, a lot of programmers are very snobby about Excel, but it's actually a very useful visual tool. Jeremy spent a lot of time thinking about how to implement Stochastic Gradient Descent, as well as other optimizers like RMSProp and AdaGrad in Excel. It provides a really visual picture of which cells depend on which cells and how the calculations are updating. I encourage you to check out those notebooks. Also, if you're ever stumped on a math concept, try implementing it in Excel and then once you have it down, put it in Python. There are two tests of whether you understand something. One is whether you can code it and build something with it. The other is can you explain it to somebody else? Ways to practice explaining what you're learning. One is to write a blog post. Here are two blog posts by our students that I thought they were really well written and they were also very accessible. Remember, you don't have to be an expert to start blogging. Your target audience is the person that's just one or two steps behind you and you're actually best positioned to help that person. The other place that you can practice explaining what you're learning is answering other people's questions. Two places you can do that are on our forums, which are forums.fast.ai. There are always interesting discussions happening related to deep learning. Here, people are talking about the Kaggle fisheries competition, about average pooling instead of dense layers, or how to use persistent AWS spot instances. Definitely, check them out. Another place that you can ask or answer questions is the learn machine learning subreddit, which is different than the machine learning subreddit. Here's a quote from one of our students after part one of the course. I personally fell into the habit of watching the lectures too much and Googling definitions too much without running the code. At first, I thought that I should read the code quickly and then spend time researching the theory behind it. In retrospect, I should have spent the majority of my time on the actual code, running it and seeing what goes into it and what comes out of it. This is excellent advice, learn from this person's mistake, stay focused on the code. In summary, some keys for learning are to have a practical coding project that you care about and let that lead the way for you. Get something working as fast as possible. It's okay if you don't understand all the components at first, and that's actually better. Then as time goes on, you will learn about the components as you improve your performance and dig down. Then help others answer questions and write about what you're learning. I value altruism, but that's not why I recommend it. I recommend it because it's a key part to cementing your own understanding is to explain it. Then I wanted to leave you with a few resources. There's our free course in our forums, the Learn Machine Learning subreddit. Some of my favorite deep learning blogs are Steven Marity, Chris Ola. He doesn't update his old blog anymore, but you should still read his old post. He now spearheads distilled.pub. Then Andre Carpathy, I think the Stanford CNN course, which is freely available is also excellent. I'll tweet a link to these slides probably tomorrow. There's a link to a blog post I wrote that links to like 15 blog posts I like in the area of deep learning. Yes, I can take some questions now. Again, I am at math underscore Rachel on Twitter and I blog at fast.ai. I also have an Ask Data Scientist advice column there. Thank you. That's a great question. The question was, this is very focused on deep learning. What do I think about other tools such as random forest? We actually in part two of our course, we use a neural net on a structured data set that you would much more traditionally see with a random forest or XGBoost. I definitely think random forests are still worth learning about. I think it's possible in the future though that deep learning is going to be used more and more on datasets that are still being solved with random forest today. I would recommend with not enough data, like I think a lot is being done with data augmentation to combine with neural networks to try to deal with that use case. I'm thinking of just like flipping and random crops and applying color tents, if you're using picture data but ways to. Yeah. Yeah. I think there are a lot of techniques for augmenting the size of your dataset. Then I think when you have imbalanced classes, I mean you can do sampling where you're taking more equal numbers from each class to try to counteract that. Yes. That's a good question. I think some of it maybe just like search for an interesting dataset that appeals to you. So there are a lot. I mean Kaggle is always a great place to find interesting datasets or other resources out there. Then there are these organizations like Delta Analytics where you can volunteer to partner with a non-profit. Then I think another source might be like seeing if there are any particular papers that appeal to you. Yes. I am. So Jeremy is actually the one who had the Excel idea, although I love it. Yeah. It's always good to use tools you're familiar with. I think though Excel is like the world's most popular functional programming language. There's a reason for that. I think Excel is not hard to learn and there's some things that are very visual with it. Although you're right, it is good to be familiar with your tools.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 6.9, "text": " How to learn deep learning when you're not a computer science PhD or even if you are.", "tokens": [1012, 281, 1466, 2452, 2539, 562, 291, 434, 406, 257, 3820, 3497, 14476, 420, 754, 498, 291, 366, 13], "temperature": 0.0, "avg_logprob": -0.2329778403879326, "compression_ratio": 1.6653225806451613, "no_speech_prob": 0.0027562410105019808}, {"id": 1, "seek": 0, "start": 6.9, "end": 9.74, "text": " So I'm Rachel Thomas.", "tokens": [407, 286, 478, 14246, 8500, 13], "temperature": 0.0, "avg_logprob": -0.2329778403879326, "compression_ratio": 1.6653225806451613, "no_speech_prob": 0.0027562410105019808}, {"id": 2, "seek": 0, "start": 9.74, "end": 14.58, "text": " I'm on Twitter at math underscore Rachel and I blog about data science at fast.ai.", "tokens": [286, 478, 322, 5794, 412, 5221, 37556, 14246, 293, 286, 6968, 466, 1412, 3497, 412, 2370, 13, 1301, 13], "temperature": 0.0, "avg_logprob": -0.2329778403879326, "compression_ratio": 1.6653225806451613, "no_speech_prob": 0.0027562410105019808}, {"id": 3, "seek": 0, "start": 14.58, "end": 17.900000000000002, "text": " Briefly, my background is I studied math and computer science in", "tokens": [39805, 356, 11, 452, 3678, 307, 286, 9454, 5221, 293, 3820, 3497, 294], "temperature": 0.0, "avg_logprob": -0.2329778403879326, "compression_ratio": 1.6653225806451613, "no_speech_prob": 0.0027562410105019808}, {"id": 4, "seek": 0, "start": 17.900000000000002, "end": 20.76, "text": " undergrad and then did a PhD in pure math.", "tokens": [14295, 293, 550, 630, 257, 14476, 294, 6075, 5221, 13], "temperature": 0.0, "avg_logprob": -0.2329778403879326, "compression_ratio": 1.6653225806451613, "no_speech_prob": 0.0027562410105019808}, {"id": 5, "seek": 0, "start": 20.76, "end": 24.1, "text": " I worked as a quant and energy trading for two years and then I", "tokens": [286, 2732, 382, 257, 4426, 293, 2281, 9529, 337, 732, 924, 293, 550, 286], "temperature": 0.0, "avg_logprob": -0.2329778403879326, "compression_ratio": 1.6653225806451613, "no_speech_prob": 0.0027562410105019808}, {"id": 6, "seek": 0, "start": 24.1, "end": 27.2, "text": " was a data scientist and backend engineer at Uber.", "tokens": [390, 257, 1412, 12662, 293, 38087, 11403, 412, 21839, 13], "temperature": 0.0, "avg_logprob": -0.2329778403879326, "compression_ratio": 1.6653225806451613, "no_speech_prob": 0.0027562410105019808}, {"id": 7, "seek": 2720, "start": 27.2, "end": 30.16, "text": " I taught full stack software development at Hackbrite.", "tokens": [286, 5928, 1577, 8630, 4722, 3250, 412, 35170, 1443, 642, 13], "temperature": 0.0, "avg_logprob": -0.17293154865229896, "compression_ratio": 1.5875912408759123, "no_speech_prob": 8.797734153631609e-06}, {"id": 8, "seek": 2720, "start": 30.16, "end": 31.24, "text": " I really love teaching.", "tokens": [286, 534, 959, 4571, 13], "temperature": 0.0, "avg_logprob": -0.17293154865229896, "compression_ratio": 1.5875912408759123, "no_speech_prob": 8.797734153631609e-06}, {"id": 9, "seek": 2720, "start": 31.24, "end": 33.32, "text": " I had taught calculus while I was in grad school and I", "tokens": [286, 632, 5928, 33400, 1339, 286, 390, 294, 2771, 1395, 293, 286], "temperature": 0.0, "avg_logprob": -0.17293154865229896, "compression_ratio": 1.5875912408759123, "no_speech_prob": 8.797734153631609e-06}, {"id": 10, "seek": 2720, "start": 33.32, "end": 35.92, "text": " think I'll probably always return to teaching.", "tokens": [519, 286, 603, 1391, 1009, 2736, 281, 4571, 13], "temperature": 0.0, "avg_logprob": -0.17293154865229896, "compression_ratio": 1.5875912408759123, "no_speech_prob": 8.797734153631609e-06}, {"id": 11, "seek": 2720, "start": 35.92, "end": 40.76, "text": " One year ago, I started fast.ai together with Jeremy Howard,", "tokens": [1485, 1064, 2057, 11, 286, 1409, 2370, 13, 1301, 1214, 365, 17809, 17626, 11], "temperature": 0.0, "avg_logprob": -0.17293154865229896, "compression_ratio": 1.5875912408759123, "no_speech_prob": 8.797734153631609e-06}, {"id": 12, "seek": 2720, "start": 40.76, "end": 44.120000000000005, "text": " who's the previous president of Kaggle and founder of", "tokens": [567, 311, 264, 3894, 3868, 295, 48751, 22631, 293, 14917, 295], "temperature": 0.0, "avg_logprob": -0.17293154865229896, "compression_ratio": 1.5875912408759123, "no_speech_prob": 8.797734153631609e-06}, {"id": 13, "seek": 2720, "start": 44.120000000000005, "end": 48.480000000000004, "text": " Enlytic which was the first company to use deep learning for medicine.", "tokens": [2193, 356, 40907, 597, 390, 264, 700, 2237, 281, 764, 2452, 2539, 337, 7195, 13], "temperature": 0.0, "avg_logprob": -0.17293154865229896, "compression_ratio": 1.5875912408759123, "no_speech_prob": 8.797734153631609e-06}, {"id": 14, "seek": 2720, "start": 48.480000000000004, "end": 53.32, "text": " Our goal is to make deep learning more accessible and easier to use.", "tokens": [2621, 3387, 307, 281, 652, 2452, 2539, 544, 9515, 293, 3571, 281, 764, 13], "temperature": 0.0, "avg_logprob": -0.17293154865229896, "compression_ratio": 1.5875912408759123, "no_speech_prob": 8.797734153631609e-06}, {"id": 15, "seek": 5332, "start": 53.32, "end": 59.88, "text": " So I'm going to describe the gap that we saw in deep learning and with technical teaching,", "tokens": [407, 286, 478, 516, 281, 6786, 264, 7417, 300, 321, 1866, 294, 2452, 2539, 293, 365, 6191, 4571, 11], "temperature": 0.0, "avg_logprob": -0.15852877071925572, "compression_ratio": 1.6544117647058822, "no_speech_prob": 2.812638285831781e-06}, {"id": 16, "seek": 5332, "start": 59.88, "end": 61.44, "text": " how we addressed it with our course.", "tokens": [577, 321, 13847, 309, 365, 527, 1164, 13], "temperature": 0.0, "avg_logprob": -0.15852877071925572, "compression_ratio": 1.6544117647058822, "no_speech_prob": 2.812638285831781e-06}, {"id": 17, "seek": 5332, "start": 61.44, "end": 63.68, "text": " So we have a course that we taught in person", "tokens": [407, 321, 362, 257, 1164, 300, 321, 5928, 294, 954], "temperature": 0.0, "avg_logprob": -0.15852877071925572, "compression_ratio": 1.6544117647058822, "no_speech_prob": 2.812638285831781e-06}, {"id": 18, "seek": 5332, "start": 63.68, "end": 66.32, "text": " together with the University of San Francisco's Data Institute.", "tokens": [1214, 365, 264, 3535, 295, 5271, 12279, 311, 11888, 9446, 13], "temperature": 0.0, "avg_logprob": -0.15852877071925572, "compression_ratio": 1.6544117647058822, "no_speech_prob": 2.812638285831781e-06}, {"id": 19, "seek": 5332, "start": 66.32, "end": 71.66, "text": " It was open to the community and then we released it for free online,", "tokens": [467, 390, 1269, 281, 264, 1768, 293, 550, 321, 4736, 309, 337, 1737, 2950, 11], "temperature": 0.0, "avg_logprob": -0.15852877071925572, "compression_ratio": 1.6544117647058822, "no_speech_prob": 2.812638285831781e-06}, {"id": 20, "seek": 5332, "start": 71.66, "end": 75.32, "text": " and over 50,000 people have started it.", "tokens": [293, 670, 2625, 11, 1360, 561, 362, 1409, 309, 13], "temperature": 0.0, "avg_logprob": -0.15852877071925572, "compression_ratio": 1.6544117647058822, "no_speech_prob": 2.812638285831781e-06}, {"id": 21, "seek": 5332, "start": 75.32, "end": 78.58, "text": " Sorry, and then I'll share some lessons that will hopefully", "tokens": [4919, 11, 293, 550, 286, 603, 2073, 512, 8820, 300, 486, 4696], "temperature": 0.0, "avg_logprob": -0.15852877071925572, "compression_ratio": 1.6544117647058822, "no_speech_prob": 2.812638285831781e-06}, {"id": 22, "seek": 5332, "start": 78.58, "end": 81.6, "text": " be useful for you on your learning journey.", "tokens": [312, 4420, 337, 291, 322, 428, 2539, 4671, 13], "temperature": 0.0, "avg_logprob": -0.15852877071925572, "compression_ratio": 1.6544117647058822, "no_speech_prob": 2.812638285831781e-06}, {"id": 23, "seek": 8160, "start": 81.6, "end": 88.64, "text": " I'm going to begin my story at the Bay Area Machine Learning Meetup back in 2013.", "tokens": [286, 478, 516, 281, 1841, 452, 1657, 412, 264, 7840, 19405, 22155, 15205, 22963, 1010, 646, 294, 9012, 13], "temperature": 0.0, "avg_logprob": -0.1391731243507535, "compression_ratio": 1.5742971887550201, "no_speech_prob": 6.238499281607801e-06}, {"id": 24, "seek": 8160, "start": 88.64, "end": 91.91999999999999, "text": " I want to thank Dave that he's been organizing this for so long.", "tokens": [286, 528, 281, 1309, 11017, 300, 415, 311, 668, 17608, 341, 337, 370, 938, 13], "temperature": 0.0, "avg_logprob": -0.1391731243507535, "compression_ratio": 1.5742971887550201, "no_speech_prob": 6.238499281607801e-06}, {"id": 25, "seek": 8160, "start": 91.91999999999999, "end": 96.08, "text": " It's a huge help to our community.", "tokens": [467, 311, 257, 2603, 854, 281, 527, 1768, 13], "temperature": 0.0, "avg_logprob": -0.1391731243507535, "compression_ratio": 1.5742971887550201, "no_speech_prob": 6.238499281607801e-06}, {"id": 26, "seek": 8160, "start": 96.08, "end": 99.19999999999999, "text": " I first got interested in deep learning in 2013,", "tokens": [286, 700, 658, 3102, 294, 2452, 2539, 294, 9012, 11], "temperature": 0.0, "avg_logprob": -0.1391731243507535, "compression_ratio": 1.5742971887550201, "no_speech_prob": 6.238499281607801e-06}, {"id": 27, "seek": 8160, "start": 99.19999999999999, "end": 103.32, "text": " and so I was really excited to see that Ilya Suskovor was going to be speaking.", "tokens": [293, 370, 286, 390, 534, 2919, 281, 536, 300, 286, 45106, 9545, 4093, 8453, 390, 516, 281, 312, 4124, 13], "temperature": 0.0, "avg_logprob": -0.1391731243507535, "compression_ratio": 1.5742971887550201, "no_speech_prob": 6.238499281607801e-06}, {"id": 28, "seek": 8160, "start": 103.32, "end": 107.91999999999999, "text": " At that point, there were not that many deep learning materials available online,", "tokens": [1711, 300, 935, 11, 456, 645, 406, 300, 867, 2452, 2539, 5319, 2435, 2950, 11], "temperature": 0.0, "avg_logprob": -0.1391731243507535, "compression_ratio": 1.5742971887550201, "no_speech_prob": 6.238499281607801e-06}, {"id": 29, "seek": 10792, "start": 107.92, "end": 112.08, "text": " and this was before any of the open source frameworks had been released.", "tokens": [293, 341, 390, 949, 604, 295, 264, 1269, 4009, 29834, 632, 668, 4736, 13], "temperature": 0.0, "avg_logprob": -0.11294648813646893, "compression_ratio": 1.669871794871795, "no_speech_prob": 6.852257229184033e-06}, {"id": 30, "seek": 10792, "start": 112.08, "end": 116.92, "text": " I was really looking for practical info on how I could implement a neural net at home,", "tokens": [286, 390, 534, 1237, 337, 8496, 13614, 322, 577, 286, 727, 4445, 257, 18161, 2533, 412, 1280, 11], "temperature": 0.0, "avg_logprob": -0.11294648813646893, "compression_ratio": 1.669871794871795, "no_speech_prob": 6.852257229184033e-06}, {"id": 31, "seek": 10792, "start": 116.92, "end": 119.76, "text": " and Ilya's talk was mostly theoretical.", "tokens": [293, 286, 45106, 311, 751, 390, 5240, 20864, 13], "temperature": 0.0, "avg_logprob": -0.11294648813646893, "compression_ratio": 1.669871794871795, "no_speech_prob": 6.852257229184033e-06}, {"id": 32, "seek": 10792, "start": 119.76, "end": 123.68, "text": " I asked a question at the end about how he had initialized his weights,", "tokens": [286, 2351, 257, 1168, 412, 264, 917, 466, 577, 415, 632, 5883, 1602, 702, 17443, 11], "temperature": 0.0, "avg_logprob": -0.11294648813646893, "compression_ratio": 1.669871794871795, "no_speech_prob": 6.852257229184033e-06}, {"id": 33, "seek": 10792, "start": 123.68, "end": 127.96000000000001, "text": " and he said, that's part of a dirty bag of tricks that nobody publishes.", "tokens": [293, 415, 848, 11, 300, 311, 644, 295, 257, 9360, 3411, 295, 11733, 300, 5079, 11374, 279, 13], "temperature": 0.0, "avg_logprob": -0.11294648813646893, "compression_ratio": 1.669871794871795, "no_speech_prob": 6.852257229184033e-06}, {"id": 34, "seek": 10792, "start": 127.96000000000001, "end": 129.56, "text": " I was so disappointed,", "tokens": [286, 390, 370, 13856, 11], "temperature": 0.0, "avg_logprob": -0.11294648813646893, "compression_ratio": 1.669871794871795, "no_speech_prob": 6.852257229184033e-06}, {"id": 35, "seek": 10792, "start": 129.56, "end": 132.4, "text": " but that was the message I was getting a lot of places at that point,", "tokens": [457, 300, 390, 264, 3636, 286, 390, 1242, 257, 688, 295, 3190, 412, 300, 935, 11], "temperature": 0.0, "avg_logprob": -0.11294648813646893, "compression_ratio": 1.669871794871795, "no_speech_prob": 6.852257229184033e-06}, {"id": 36, "seek": 10792, "start": 132.4, "end": 136.8, "text": " that everyone doing deep learning had gotten their PhD with the same four advisors,", "tokens": [300, 1518, 884, 2452, 2539, 632, 5768, 641, 14476, 365, 264, 912, 1451, 29136, 11], "temperature": 0.0, "avg_logprob": -0.11294648813646893, "compression_ratio": 1.669871794871795, "no_speech_prob": 6.852257229184033e-06}, {"id": 37, "seek": 13680, "start": 136.8, "end": 139.68, "text": " and they weren't publishing the practical info.", "tokens": [293, 436, 4999, 380, 17832, 264, 8496, 13614, 13], "temperature": 0.0, "avg_logprob": -0.11444381653793215, "compression_ratio": 1.6171617161716172, "no_speech_prob": 2.8402859243215062e-05}, {"id": 38, "seek": 13680, "start": 139.68, "end": 141.92000000000002, "text": " The field has come a long way since then,", "tokens": [440, 2519, 575, 808, 257, 938, 636, 1670, 550, 11], "temperature": 0.0, "avg_logprob": -0.11444381653793215, "compression_ratio": 1.6171617161716172, "no_speech_prob": 2.8402859243215062e-05}, {"id": 39, "seek": 13680, "start": 141.92000000000002, "end": 144.48000000000002, "text": " but there's still this problem with exclusivity.", "tokens": [457, 456, 311, 920, 341, 1154, 365, 15085, 4253, 13], "temperature": 0.0, "avg_logprob": -0.11444381653793215, "compression_ratio": 1.6171617161716172, "no_speech_prob": 2.8402859243215062e-05}, {"id": 40, "seek": 13680, "start": 144.48000000000002, "end": 150.16000000000003, "text": " This is a comment from Hacker News that was voted to the top of its thread a few months ago,", "tokens": [639, 307, 257, 2871, 490, 389, 23599, 7987, 300, 390, 13415, 281, 264, 1192, 295, 1080, 7207, 257, 1326, 2493, 2057, 11], "temperature": 0.0, "avg_logprob": -0.11444381653793215, "compression_ratio": 1.6171617161716172, "no_speech_prob": 2.8402859243215062e-05}, {"id": 41, "seek": 13680, "start": 150.16000000000003, "end": 152.4, "text": " and I completely disagree with it,", "tokens": [293, 286, 2584, 14091, 365, 309, 11], "temperature": 0.0, "avg_logprob": -0.11444381653793215, "compression_ratio": 1.6171617161716172, "no_speech_prob": 2.8402859243215062e-05}, {"id": 42, "seek": 13680, "start": 152.4, "end": 155.64000000000001, "text": " but it's saying that to get into machine learning,", "tokens": [457, 309, 311, 1566, 300, 281, 483, 666, 3479, 2539, 11], "temperature": 0.0, "avg_logprob": -0.11444381653793215, "compression_ratio": 1.6171617161716172, "no_speech_prob": 2.8402859243215062e-05}, {"id": 43, "seek": 13680, "start": 155.64000000000001, "end": 160.24, "text": " you first need to spend years taking quote boring math courses,", "tokens": [291, 700, 643, 281, 3496, 924, 1940, 6513, 9989, 5221, 7712, 11], "temperature": 0.0, "avg_logprob": -0.11444381653793215, "compression_ratio": 1.6171617161716172, "no_speech_prob": 2.8402859243215062e-05}, {"id": 44, "seek": 13680, "start": 160.24, "end": 162.84, "text": " and then you need to learn CUDA and MPI,", "tokens": [293, 550, 291, 643, 281, 1466, 29777, 7509, 293, 14146, 40, 11], "temperature": 0.0, "avg_logprob": -0.11444381653793215, "compression_ratio": 1.6171617161716172, "no_speech_prob": 2.8402859243215062e-05}, {"id": 45, "seek": 13680, "start": 162.84, "end": 166.64000000000001, "text": " and then years later, you can start touching Theano and TensorFlow.", "tokens": [293, 550, 924, 1780, 11, 291, 393, 722, 11175, 440, 3730, 293, 37624, 13], "temperature": 0.0, "avg_logprob": -0.11444381653793215, "compression_ratio": 1.6171617161716172, "no_speech_prob": 2.8402859243215062e-05}, {"id": 46, "seek": 16664, "start": 166.64, "end": 168.64, "text": " I disagree with this advice,", "tokens": [286, 14091, 365, 341, 5192, 11], "temperature": 0.0, "avg_logprob": -0.1252298355102539, "compression_ratio": 1.5342960288808665, "no_speech_prob": 1.2405133929860312e-05}, {"id": 47, "seek": 16664, "start": 168.64, "end": 172.88, "text": " but I think this attitude keeps people out of the field and from trying it.", "tokens": [457, 286, 519, 341, 10157, 5965, 561, 484, 295, 264, 2519, 293, 490, 1382, 309, 13], "temperature": 0.0, "avg_logprob": -0.1252298355102539, "compression_ratio": 1.5342960288808665, "no_speech_prob": 1.2405133929860312e-05}, {"id": 48, "seek": 16664, "start": 173.72, "end": 179.2, "text": " There's an essay that I really love called A Mathematician's Lament by Paul Lockhart.", "tokens": [821, 311, 364, 16238, 300, 286, 534, 959, 1219, 316, 15776, 14911, 952, 311, 441, 2466, 538, 4552, 16736, 42535, 13], "temperature": 0.0, "avg_logprob": -0.1252298355102539, "compression_ratio": 1.5342960288808665, "no_speech_prob": 1.2405133929860312e-05}, {"id": 49, "seek": 16664, "start": 179.2, "end": 184.2, "text": " Paul Lockhart was a math professor at Brown before he quit to go teach K-12,", "tokens": [4552, 16736, 42535, 390, 257, 5221, 8304, 412, 8030, 949, 415, 10366, 281, 352, 2924, 591, 12, 4762, 11], "temperature": 0.0, "avg_logprob": -0.1252298355102539, "compression_ratio": 1.5342960288808665, "no_speech_prob": 1.2405133929860312e-05}, {"id": 50, "seek": 16664, "start": 184.2, "end": 188.88, "text": " because he thought the state of math education in the US was such a disaster.", "tokens": [570, 415, 1194, 264, 1785, 295, 5221, 3309, 294, 264, 2546, 390, 1270, 257, 11293, 13], "temperature": 0.0, "avg_logprob": -0.1252298355102539, "compression_ratio": 1.5342960288808665, "no_speech_prob": 1.2405133929860312e-05}, {"id": 51, "seek": 16664, "start": 188.88, "end": 194.44, "text": " He describes this nightmare world where children are not allowed to sing songs,", "tokens": [634, 15626, 341, 18724, 1002, 689, 2227, 366, 406, 4350, 281, 1522, 5781, 11], "temperature": 0.0, "avg_logprob": -0.1252298355102539, "compression_ratio": 1.5342960288808665, "no_speech_prob": 1.2405133929860312e-05}, {"id": 52, "seek": 19444, "start": 194.44, "end": 197.12, "text": " or play instruments until graduate school,", "tokens": [420, 862, 12190, 1826, 8080, 1395, 11], "temperature": 0.0, "avg_logprob": -0.11501559536013983, "compression_ratio": 1.6925795053003534, "no_speech_prob": 1.7490960090071894e-05}, {"id": 53, "seek": 19444, "start": 197.12, "end": 200.96, "text": " because they need to have over a decade of studying music notation and", "tokens": [570, 436, 643, 281, 362, 670, 257, 10378, 295, 7601, 1318, 24657, 293], "temperature": 0.0, "avg_logprob": -0.11501559536013983, "compression_ratio": 1.6925795053003534, "no_speech_prob": 1.7490960090071894e-05}, {"id": 54, "seek": 19444, "start": 200.96, "end": 206.2, "text": " transcribing sheet music by hand before they can be trusted to sing.", "tokens": [1145, 39541, 8193, 1318, 538, 1011, 949, 436, 393, 312, 16034, 281, 1522, 13], "temperature": 0.0, "avg_logprob": -0.11501559536013983, "compression_ratio": 1.6925795053003534, "no_speech_prob": 1.7490960090071894e-05}, {"id": 55, "seek": 19444, "start": 206.2, "end": 208.8, "text": " Hopefully, this sounds horrifying to you.", "tokens": [10429, 11, 341, 3263, 40227, 281, 291, 13], "temperature": 0.0, "avg_logprob": -0.11501559536013983, "compression_ratio": 1.6925795053003534, "no_speech_prob": 1.7490960090071894e-05}, {"id": 56, "seek": 19444, "start": 208.8, "end": 210.56, "text": " He says, that's what we do with math.", "tokens": [634, 1619, 11, 300, 311, 437, 321, 360, 365, 5221, 13], "temperature": 0.0, "avg_logprob": -0.11501559536013983, "compression_ratio": 1.6925795053003534, "no_speech_prob": 1.7490960090071894e-05}, {"id": 57, "seek": 19444, "start": 210.56, "end": 215.16, "text": " We force students to learn dry disconnected notation and save", "tokens": [492, 3464, 1731, 281, 1466, 4016, 29426, 24657, 293, 3155], "temperature": 0.0, "avg_logprob": -0.11501559536013983, "compression_ratio": 1.6925795053003534, "no_speech_prob": 1.7490960090071894e-05}, {"id": 58, "seek": 19444, "start": 215.16, "end": 220.04, "text": " the really fun and creative parts till after most of them have dropped out of the field.", "tokens": [264, 534, 1019, 293, 5880, 3166, 4288, 934, 881, 295, 552, 362, 8119, 484, 295, 264, 2519, 13], "temperature": 0.0, "avg_logprob": -0.11501559536013983, "compression_ratio": 1.6925795053003534, "no_speech_prob": 1.7490960090071894e-05}, {"id": 59, "seek": 19444, "start": 220.04, "end": 224.4, "text": " Sadly, we're carrying this over into deep learning as well often.", "tokens": [29628, 11, 321, 434, 9792, 341, 670, 666, 2452, 2539, 382, 731, 2049, 13], "temperature": 0.0, "avg_logprob": -0.11501559536013983, "compression_ratio": 1.6925795053003534, "no_speech_prob": 1.7490960090071894e-05}, {"id": 60, "seek": 22440, "start": 224.4, "end": 229.0, "text": " This is how the most popular textbook on", "tokens": [639, 307, 577, 264, 881, 3743, 25591, 322], "temperature": 0.0, "avg_logprob": -0.21711748123168945, "compression_ratio": 1.5742971887550201, "no_speech_prob": 1.3843386113876477e-05}, {"id": 61, "seek": 22440, "start": 229.0, "end": 233.92000000000002, "text": " deep learning encourages you to develop intuition for back propagation through time.", "tokens": [2452, 2539, 28071, 291, 281, 1499, 24002, 337, 646, 38377, 807, 565, 13], "temperature": 0.0, "avg_logprob": -0.21711748123168945, "compression_ratio": 1.5742971887550201, "no_speech_prob": 1.3843386113876477e-05}, {"id": 62, "seek": 22440, "start": 233.92000000000002, "end": 237.92000000000002, "text": " So what do you think? Is this pretty intuitive?", "tokens": [407, 437, 360, 291, 519, 30, 1119, 341, 1238, 21769, 30], "temperature": 0.0, "avg_logprob": -0.21711748123168945, "compression_ratio": 1.5742971887550201, "no_speech_prob": 1.3843386113876477e-05}, {"id": 63, "seek": 22440, "start": 237.92000000000002, "end": 240.32, "text": " This is Ian Goodfellow's book,", "tokens": [639, 307, 19595, 2205, 69, 21348, 311, 1446, 11], "temperature": 0.0, "avg_logprob": -0.21711748123168945, "compression_ratio": 1.5742971887550201, "no_speech_prob": 1.3843386113876477e-05}, {"id": 64, "seek": 22440, "start": 240.32, "end": 241.92000000000002, "text": " and I think it's a useful resource,", "tokens": [293, 286, 519, 309, 311, 257, 4420, 7684, 11], "temperature": 0.0, "avg_logprob": -0.21711748123168945, "compression_ratio": 1.5742971887550201, "no_speech_prob": 1.3843386113876477e-05}, {"id": 65, "seek": 22440, "start": 241.92000000000002, "end": 244.96, "text": " but ultimately it's a theoretical math textbook.", "tokens": [457, 6284, 309, 311, 257, 20864, 5221, 25591, 13], "temperature": 0.0, "avg_logprob": -0.21711748123168945, "compression_ratio": 1.5742971887550201, "no_speech_prob": 1.3843386113876477e-05}, {"id": 66, "seek": 22440, "start": 244.96, "end": 247.48000000000002, "text": " There's no code in the book.", "tokens": [821, 311, 572, 3089, 294, 264, 1446, 13], "temperature": 0.0, "avg_logprob": -0.21711748123168945, "compression_ratio": 1.5742971887550201, "no_speech_prob": 1.3843386113876477e-05}, {"id": 67, "seek": 22440, "start": 247.68, "end": 252.44, "text": " So to summarize, some of the problems that I see with technical teaching,", "tokens": [407, 281, 20858, 11, 512, 295, 264, 2740, 300, 286, 536, 365, 6191, 4571, 11], "temperature": 0.0, "avg_logprob": -0.21711748123168945, "compression_ratio": 1.5742971887550201, "no_speech_prob": 1.3843386113876477e-05}, {"id": 68, "seek": 25244, "start": 252.44, "end": 254.68, "text": " it tends to be math-centric.", "tokens": [309, 12258, 281, 312, 5221, 12, 45300, 13], "temperature": 0.0, "avg_logprob": -0.13290227662532703, "compression_ratio": 1.620817843866171, "no_speech_prob": 1.695884930086322e-05}, {"id": 69, "seek": 25244, "start": 254.68, "end": 257.88, "text": " I love math, however, if you're trying to build something,", "tokens": [286, 959, 5221, 11, 4461, 11, 498, 291, 434, 1382, 281, 1322, 746, 11], "temperature": 0.0, "avg_logprob": -0.13290227662532703, "compression_ratio": 1.620817843866171, "no_speech_prob": 1.695884930086322e-05}, {"id": 70, "seek": 25244, "start": 257.88, "end": 259.92, "text": " code is more useful.", "tokens": [3089, 307, 544, 4420, 13], "temperature": 0.0, "avg_logprob": -0.13290227662532703, "compression_ratio": 1.620817843866171, "no_speech_prob": 1.695884930086322e-05}, {"id": 71, "seek": 25244, "start": 259.92, "end": 264.88, "text": " There's something that Harvard professor David Perkins calls elementitis,", "tokens": [821, 311, 746, 300, 13378, 8304, 4389, 3026, 10277, 5498, 4478, 16074, 11], "temperature": 0.0, "avg_logprob": -0.13290227662532703, "compression_ratio": 1.620817843866171, "no_speech_prob": 1.695884930086322e-05}, {"id": 72, "seek": 25244, "start": 264.88, "end": 266.68, "text": " and that's feeling like you have to teach", "tokens": [293, 300, 311, 2633, 411, 291, 362, 281, 2924], "temperature": 0.0, "avg_logprob": -0.13290227662532703, "compression_ratio": 1.620817843866171, "no_speech_prob": 1.695884930086322e-05}, {"id": 73, "seek": 25244, "start": 266.68, "end": 269.64, "text": " each individual element before years later,", "tokens": [1184, 2609, 4478, 949, 924, 1780, 11], "temperature": 0.0, "avg_logprob": -0.13290227662532703, "compression_ratio": 1.620817843866171, "no_speech_prob": 1.695884930086322e-05}, {"id": 74, "seek": 25244, "start": 269.64, "end": 272.68, "text": " you can assemble those components to build something cool.", "tokens": [291, 393, 22364, 729, 6677, 281, 1322, 746, 1627, 13], "temperature": 0.0, "avg_logprob": -0.13290227662532703, "compression_ratio": 1.620817843866171, "no_speech_prob": 1.695884930086322e-05}, {"id": 75, "seek": 25244, "start": 272.68, "end": 275.76, "text": " He gives an analogy with baseball and says,", "tokens": [634, 2709, 364, 21663, 365, 14323, 293, 1619, 11], "temperature": 0.0, "avg_logprob": -0.13290227662532703, "compression_ratio": 1.620817843866171, "no_speech_prob": 1.695884930086322e-05}, {"id": 76, "seek": 25244, "start": 275.76, "end": 279.0, "text": " we don't say that little kids have to memorize all the rules for", "tokens": [321, 500, 380, 584, 300, 707, 2301, 362, 281, 27478, 439, 264, 4474, 337], "temperature": 0.0, "avg_logprob": -0.13290227662532703, "compression_ratio": 1.620817843866171, "no_speech_prob": 1.695884930086322e-05}, {"id": 77, "seek": 27900, "start": 279.0, "end": 282.6, "text": " baseball and understand all the technical details before they're allowed to play.", "tokens": [14323, 293, 1223, 439, 264, 6191, 4365, 949, 436, 434, 4350, 281, 862, 13], "temperature": 0.0, "avg_logprob": -0.14806349300643773, "compression_ratio": 1.66, "no_speech_prob": 1.1124144293717109e-05}, {"id": 78, "seek": 27900, "start": 282.6, "end": 284.88, "text": " They can play and get a general idea of the game,", "tokens": [814, 393, 862, 293, 483, 257, 2674, 1558, 295, 264, 1216, 11], "temperature": 0.0, "avg_logprob": -0.14806349300643773, "compression_ratio": 1.66, "no_speech_prob": 1.1124144293717109e-05}, {"id": 79, "seek": 27900, "start": 284.88, "end": 287.96, "text": " even if they don't know all the details.", "tokens": [754, 498, 436, 500, 380, 458, 439, 264, 4365, 13], "temperature": 0.0, "avg_logprob": -0.14806349300643773, "compression_ratio": 1.66, "no_speech_prob": 1.1124144293717109e-05}, {"id": 80, "seek": 27900, "start": 287.96, "end": 293.76, "text": " Then of the more practical or code-focused tutorials or blog post out there,", "tokens": [1396, 295, 264, 544, 8496, 420, 3089, 12, 44062, 17616, 420, 6968, 2183, 484, 456, 11], "temperature": 0.0, "avg_logprob": -0.14806349300643773, "compression_ratio": 1.66, "no_speech_prob": 1.1124144293717109e-05}, {"id": 81, "seek": 27900, "start": 293.76, "end": 298.08, "text": " many of them are on simple toy problems and settle for", "tokens": [867, 295, 552, 366, 322, 2199, 12058, 2740, 293, 11852, 337], "temperature": 0.0, "avg_logprob": -0.14806349300643773, "compression_ratio": 1.66, "no_speech_prob": 1.1124144293717109e-05}, {"id": 82, "seek": 27900, "start": 298.08, "end": 302.64, "text": " these good enough results that wouldn't be acceptable in the workplace.", "tokens": [613, 665, 1547, 3542, 300, 2759, 380, 312, 15513, 294, 264, 15328, 13], "temperature": 0.0, "avg_logprob": -0.14806349300643773, "compression_ratio": 1.66, "no_speech_prob": 1.1124144293717109e-05}, {"id": 83, "seek": 27900, "start": 302.68, "end": 307.56, "text": " So fast.ai, we wanted to address this.", "tokens": [407, 2370, 13, 1301, 11, 321, 1415, 281, 2985, 341, 13], "temperature": 0.0, "avg_logprob": -0.14806349300643773, "compression_ratio": 1.66, "no_speech_prob": 1.1124144293717109e-05}, {"id": 84, "seek": 30756, "start": 307.56, "end": 311.48, "text": " Where our slogan is making neural nets uncool again.", "tokens": [2305, 527, 33052, 307, 1455, 18161, 36170, 6219, 1092, 797, 13], "temperature": 0.0, "avg_logprob": -0.18862411250238834, "compression_ratio": 1.5150375939849625, "no_speech_prob": 1.362801867799135e-05}, {"id": 85, "seek": 30756, "start": 311.48, "end": 314.0, "text": " That's because being cool is about being exclusive,", "tokens": [663, 311, 570, 885, 1627, 307, 466, 885, 13005, 11], "temperature": 0.0, "avg_logprob": -0.18862411250238834, "compression_ratio": 1.5150375939849625, "no_speech_prob": 1.362801867799135e-05}, {"id": 86, "seek": 30756, "start": 314.0, "end": 316.16, "text": " which is the opposite of what we want.", "tokens": [597, 307, 264, 6182, 295, 437, 321, 528, 13], "temperature": 0.0, "avg_logprob": -0.18862411250238834, "compression_ratio": 1.5150375939849625, "no_speech_prob": 1.362801867799135e-05}, {"id": 87, "seek": 30756, "start": 316.16, "end": 321.36, "text": " We're particularly interested in people using smaller obscure data sets,", "tokens": [492, 434, 4098, 3102, 294, 561, 1228, 4356, 34443, 1412, 6352, 11], "temperature": 0.0, "avg_logprob": -0.18862411250238834, "compression_ratio": 1.5150375939849625, "no_speech_prob": 1.362801867799135e-05}, {"id": 88, "seek": 30756, "start": 321.36, "end": 329.04, "text": " and people that don't have many resources and can't afford much computation time or GPU power.", "tokens": [293, 561, 300, 500, 380, 362, 867, 3593, 293, 393, 380, 6157, 709, 24903, 565, 420, 18407, 1347, 13], "temperature": 0.0, "avg_logprob": -0.18862411250238834, "compression_ratio": 1.5150375939849625, "no_speech_prob": 1.362801867799135e-05}, {"id": 89, "seek": 30756, "start": 329.04, "end": 334.4, "text": " So we taught a course in partnership with the University of San Francisco's Data Institute.", "tokens": [407, 321, 5928, 257, 1164, 294, 9982, 365, 264, 3535, 295, 5271, 12279, 311, 11888, 9446, 13], "temperature": 0.0, "avg_logprob": -0.18862411250238834, "compression_ratio": 1.5150375939849625, "no_speech_prob": 1.362801867799135e-05}, {"id": 90, "seek": 33440, "start": 334.4, "end": 338.52, "text": " This was open to the community and it was one evening a week for seven weeks,", "tokens": [639, 390, 1269, 281, 264, 1768, 293, 309, 390, 472, 5634, 257, 1243, 337, 3407, 3259, 11], "temperature": 0.0, "avg_logprob": -0.11283564946008107, "compression_ratio": 1.5945945945945945, "no_speech_prob": 1.8339113012189046e-05}, {"id": 91, "seek": 33440, "start": 338.52, "end": 343.59999999999997, "text": " although many hours of homework outside of class to go along with that.", "tokens": [4878, 867, 2496, 295, 14578, 2380, 295, 1508, 281, 352, 2051, 365, 300, 13], "temperature": 0.0, "avg_logprob": -0.11283564946008107, "compression_ratio": 1.5945945945945945, "no_speech_prob": 1.8339113012189046e-05}, {"id": 92, "seek": 33440, "start": 343.59999999999997, "end": 345.2, "text": " We had this hypothesis,", "tokens": [492, 632, 341, 17291, 11], "temperature": 0.0, "avg_logprob": -0.11283564946008107, "compression_ratio": 1.5945945945945945, "no_speech_prob": 1.8339113012189046e-05}, {"id": 93, "seek": 33440, "start": 345.2, "end": 350.44, "text": " can we teach deep learning to coders with no explicit math prerequisites?", "tokens": [393, 321, 2924, 2452, 2539, 281, 17656, 433, 365, 572, 13691, 5221, 38333, 15398, 3324, 30], "temperature": 0.0, "avg_logprob": -0.11283564946008107, "compression_ratio": 1.5945945945945945, "no_speech_prob": 1.8339113012189046e-05}, {"id": 94, "seek": 33440, "start": 350.44, "end": 352.35999999999996, "text": " We weren't even sure that it could be done,", "tokens": [492, 4999, 380, 754, 988, 300, 309, 727, 312, 1096, 11], "temperature": 0.0, "avg_logprob": -0.11283564946008107, "compression_ratio": 1.5945945945945945, "no_speech_prob": 1.8339113012189046e-05}, {"id": 95, "seek": 33440, "start": 352.35999999999996, "end": 354.2, "text": " but it was a big success.", "tokens": [457, 309, 390, 257, 955, 2245, 13], "temperature": 0.0, "avg_logprob": -0.11283564946008107, "compression_ratio": 1.5945945945945945, "no_speech_prob": 1.8339113012189046e-05}, {"id": 96, "seek": 33440, "start": 354.2, "end": 357.32, "text": " We've now released the course online completely for free.", "tokens": [492, 600, 586, 4736, 264, 1164, 2950, 2584, 337, 1737, 13], "temperature": 0.0, "avg_logprob": -0.11283564946008107, "compression_ratio": 1.5945945945945945, "no_speech_prob": 1.8339113012189046e-05}, {"id": 97, "seek": 33440, "start": 357.32, "end": 358.88, "text": " All the videos are on YouTube,", "tokens": [1057, 264, 2145, 366, 322, 3088, 11], "temperature": 0.0, "avg_logprob": -0.11283564946008107, "compression_ratio": 1.5945945945945945, "no_speech_prob": 1.8339113012189046e-05}, {"id": 98, "seek": 33440, "start": 358.88, "end": 360.79999999999995, "text": " the code is on GitHub,", "tokens": [264, 3089, 307, 322, 23331, 11], "temperature": 0.0, "avg_logprob": -0.11283564946008107, "compression_ratio": 1.5945945945945945, "no_speech_prob": 1.8339113012189046e-05}, {"id": 99, "seek": 33440, "start": 360.79999999999995, "end": 363.88, "text": " and 50,000 people have started the course.", "tokens": [293, 2625, 11, 1360, 561, 362, 1409, 264, 1164, 13], "temperature": 0.0, "avg_logprob": -0.11283564946008107, "compression_ratio": 1.5945945945945945, "no_speech_prob": 1.8339113012189046e-05}, {"id": 100, "seek": 36388, "start": 363.88, "end": 366.08, "text": " Some of the success stories we've heard,", "tokens": [2188, 295, 264, 2245, 3676, 321, 600, 2198, 11], "temperature": 0.0, "avg_logprob": -0.11553144862509182, "compression_ratio": 1.6241610738255035, "no_speech_prob": 1.643130053707864e-05}, {"id": 101, "seek": 36388, "start": 366.08, "end": 369.68, "text": " one of our students who has two years of coding experience was just", "tokens": [472, 295, 527, 1731, 567, 575, 732, 924, 295, 17720, 1752, 390, 445], "temperature": 0.0, "avg_logprob": -0.11553144862509182, "compression_ratio": 1.6241610738255035, "no_speech_prob": 1.643130053707864e-05}, {"id": 102, "seek": 36388, "start": 369.68, "end": 372.36, "text": " accepted to the Google Brain residency program,", "tokens": [9035, 281, 264, 3329, 29783, 34014, 1461, 11], "temperature": 0.0, "avg_logprob": -0.11553144862509182, "compression_ratio": 1.6241610738255035, "no_speech_prob": 1.643130053707864e-05}, {"id": 103, "seek": 36388, "start": 372.36, "end": 374.24, "text": " which is very prestigious.", "tokens": [597, 307, 588, 33510, 13], "temperature": 0.0, "avg_logprob": -0.11553144862509182, "compression_ratio": 1.6241610738255035, "no_speech_prob": 1.643130053707864e-05}, {"id": 104, "seek": 36388, "start": 374.24, "end": 378.2, "text": " We have another student who he does not even call himself a data scientist,", "tokens": [492, 362, 1071, 3107, 567, 415, 775, 406, 754, 818, 3647, 257, 1412, 12662, 11], "temperature": 0.0, "avg_logprob": -0.11553144862509182, "compression_ratio": 1.6241610738255035, "no_speech_prob": 1.643130053707864e-05}, {"id": 105, "seek": 36388, "start": 378.2, "end": 382.0, "text": " but he came up with a new fraud detection technique for his company.", "tokens": [457, 415, 1361, 493, 365, 257, 777, 14560, 17784, 6532, 337, 702, 2237, 13], "temperature": 0.0, "avg_logprob": -0.11553144862509182, "compression_ratio": 1.6241610738255035, "no_speech_prob": 1.643130053707864e-05}, {"id": 106, "seek": 36388, "start": 382.0, "end": 383.8, "text": " He said it's only like 10 lines of code,", "tokens": [634, 848, 309, 311, 787, 411, 1266, 3876, 295, 3089, 11], "temperature": 0.0, "avg_logprob": -0.11553144862509182, "compression_ratio": 1.6241610738255035, "no_speech_prob": 1.643130053707864e-05}, {"id": 107, "seek": 36388, "start": 383.8, "end": 385.8, "text": " but it earned him a large bonus.", "tokens": [457, 309, 12283, 796, 257, 2416, 10882, 13], "temperature": 0.0, "avg_logprob": -0.11553144862509182, "compression_ratio": 1.6241610738255035, "no_speech_prob": 1.643130053707864e-05}, {"id": 108, "seek": 36388, "start": 385.8, "end": 392.0, "text": " We have several more students that have received job offers or internship offers,", "tokens": [492, 362, 2940, 544, 1731, 300, 362, 4613, 1691, 7736, 420, 16861, 7736, 11], "temperature": 0.0, "avg_logprob": -0.11553144862509182, "compression_ratio": 1.6241610738255035, "no_speech_prob": 1.643130053707864e-05}, {"id": 109, "seek": 39200, "start": 392.0, "end": 394.12, "text": " that have been able to switch teams at work.", "tokens": [300, 362, 668, 1075, 281, 3679, 5491, 412, 589, 13], "temperature": 0.0, "avg_logprob": -0.14385730529499946, "compression_ratio": 1.6171875, "no_speech_prob": 3.4460888400644762e-06}, {"id": 110, "seek": 39200, "start": 394.12, "end": 398.2, "text": " I know of one student that's contributed back to TensorFlow's code,", "tokens": [286, 458, 295, 472, 3107, 300, 311, 18434, 646, 281, 37624, 311, 3089, 11], "temperature": 0.0, "avg_logprob": -0.14385730529499946, "compression_ratio": 1.6171875, "no_speech_prob": 3.4460888400644762e-06}, {"id": 111, "seek": 39200, "start": 398.2, "end": 401.76, "text": " and then four students that have won hackathons.", "tokens": [293, 550, 1451, 1731, 300, 362, 1582, 10339, 998, 892, 13], "temperature": 0.0, "avg_logprob": -0.14385730529499946, "compression_ratio": 1.6171875, "no_speech_prob": 3.4460888400644762e-06}, {"id": 112, "seek": 39200, "start": 401.88, "end": 405.12, "text": " I want to introduce you to a few of our students that really,", "tokens": [286, 528, 281, 5366, 291, 281, 257, 1326, 295, 527, 1731, 300, 534, 11], "temperature": 0.0, "avg_logprob": -0.14385730529499946, "compression_ratio": 1.6171875, "no_speech_prob": 3.4460888400644762e-06}, {"id": 113, "seek": 39200, "start": 405.12, "end": 407.52, "text": " I think, capture our mission.", "tokens": [286, 519, 11, 7983, 527, 4447, 13], "temperature": 0.0, "avg_logprob": -0.14385730529499946, "compression_ratio": 1.6171875, "no_speech_prob": 3.4460888400644762e-06}, {"id": 114, "seek": 39200, "start": 407.52, "end": 410.24, "text": " On the bottom right is Samar Haider.", "tokens": [1282, 264, 2767, 558, 307, 4832, 289, 4064, 1438, 13], "temperature": 0.0, "avg_logprob": -0.14385730529499946, "compression_ratio": 1.6171875, "no_speech_prob": 3.4460888400644762e-06}, {"id": 115, "seek": 39200, "start": 410.24, "end": 413.32, "text": " He's a natural language researcher in Pakistan.", "tokens": [634, 311, 257, 3303, 2856, 21751, 294, 15985, 13], "temperature": 0.0, "avg_logprob": -0.14385730529499946, "compression_ratio": 1.6171875, "no_speech_prob": 3.4460888400644762e-06}, {"id": 116, "seek": 39200, "start": 413.32, "end": 419.48, "text": " Pakistan has 70 spoken languages and none of them have that many resources,", "tokens": [15985, 575, 5285, 10759, 8650, 293, 6022, 295, 552, 362, 300, 867, 3593, 11], "temperature": 0.0, "avg_logprob": -0.14385730529499946, "compression_ratio": 1.6171875, "no_speech_prob": 3.4460888400644762e-06}, {"id": 117, "seek": 41948, "start": 419.48, "end": 423.88, "text": " not even Urdu, which is the most popular language for translation to and from,", "tokens": [406, 754, 9533, 769, 11, 597, 307, 264, 881, 3743, 2856, 337, 12853, 281, 293, 490, 11], "temperature": 0.0, "avg_logprob": -0.16909254642955043, "compression_ratio": 1.5942028985507246, "no_speech_prob": 6.63939545120229e-06}, {"id": 118, "seek": 41948, "start": 423.88, "end": 426.28000000000003, "text": " for other text.", "tokens": [337, 661, 2487, 13], "temperature": 0.0, "avg_logprob": -0.16909254642955043, "compression_ratio": 1.5942028985507246, "no_speech_prob": 6.63939545120229e-06}, {"id": 119, "seek": 41948, "start": 426.28000000000003, "end": 427.56, "text": " As part of our course,", "tokens": [1018, 644, 295, 527, 1164, 11], "temperature": 0.0, "avg_logprob": -0.16909254642955043, "compression_ratio": 1.5942028985507246, "no_speech_prob": 6.63939545120229e-06}, {"id": 120, "seek": 41948, "start": 427.56, "end": 430.8, "text": " Samar put together the largest corpus of Urdu that's been assembled,", "tokens": [4832, 289, 829, 1214, 264, 6443, 1181, 31624, 295, 9533, 769, 300, 311, 668, 24204, 11], "temperature": 0.0, "avg_logprob": -0.16909254642955043, "compression_ratio": 1.5942028985507246, "no_speech_prob": 6.63939545120229e-06}, {"id": 121, "seek": 41948, "start": 430.8, "end": 433.44, "text": " and he trained word embeddings on that and has found", "tokens": [293, 415, 8895, 1349, 12240, 29432, 322, 300, 293, 575, 1352], "temperature": 0.0, "avg_logprob": -0.16909254642955043, "compression_ratio": 1.5942028985507246, "no_speech_prob": 6.63939545120229e-06}, {"id": 122, "seek": 41948, "start": 433.44, "end": 436.52000000000004, "text": " some really interesting connections in the language.", "tokens": [512, 534, 1880, 9271, 294, 264, 2856, 13], "temperature": 0.0, "avg_logprob": -0.16909254642955043, "compression_ratio": 1.5942028985507246, "no_speech_prob": 6.63939545120229e-06}, {"id": 123, "seek": 41948, "start": 436.52000000000004, "end": 441.64000000000004, "text": " For machine learning, he's largely self-taught using online courses.", "tokens": [1171, 3479, 2539, 11, 415, 311, 11611, 2698, 12, 1328, 1599, 1228, 2950, 7712, 13], "temperature": 0.0, "avg_logprob": -0.16909254642955043, "compression_ratio": 1.5942028985507246, "no_speech_prob": 6.63939545120229e-06}, {"id": 124, "seek": 41948, "start": 441.64000000000004, "end": 447.56, "text": " Above him are some pictures sent to me by Sahil Singla of Farm Guide in India.", "tokens": [32691, 796, 366, 512, 5242, 2279, 281, 385, 538, 18280, 388, 7474, 875, 295, 19991, 18727, 294, 5282, 13], "temperature": 0.0, "avg_logprob": -0.16909254642955043, "compression_ratio": 1.5942028985507246, "no_speech_prob": 6.63939545120229e-06}, {"id": 125, "seek": 44756, "start": 447.56, "end": 451.64, "text": " Thousands of farmers commit suicide every year in India,", "tokens": [40535, 295, 11339, 5599, 12308, 633, 1064, 294, 5282, 11], "temperature": 0.0, "avg_logprob": -0.14055098376227815, "compression_ratio": 1.6111111111111112, "no_speech_prob": 5.053007407695986e-05}, {"id": 126, "seek": 44756, "start": 451.64, "end": 455.32, "text": " because they're taking these very predatory loans from loan sharks,", "tokens": [570, 436, 434, 1940, 613, 588, 3852, 4745, 15443, 490, 10529, 26312, 11], "temperature": 0.0, "avg_logprob": -0.14055098376227815, "compression_ratio": 1.6111111111111112, "no_speech_prob": 5.053007407695986e-05}, {"id": 127, "seek": 44756, "start": 455.32, "end": 458.76, "text": " who then threaten them with violence and harass their families.", "tokens": [567, 550, 29864, 552, 365, 6270, 293, 16910, 641, 4466, 13], "temperature": 0.0, "avg_logprob": -0.14055098376227815, "compression_ratio": 1.6111111111111112, "no_speech_prob": 5.053007407695986e-05}, {"id": 128, "seek": 44756, "start": 458.76, "end": 461.8, "text": " Part of the problem is they can't prove how much land they own,", "tokens": [4100, 295, 264, 1154, 307, 436, 393, 380, 7081, 577, 709, 2117, 436, 1065, 11], "temperature": 0.0, "avg_logprob": -0.14055098376227815, "compression_ratio": 1.6111111111111112, "no_speech_prob": 5.053007407695986e-05}, {"id": 129, "seek": 44756, "start": 461.8, "end": 463.72, "text": " or what type of crops they're growing.", "tokens": [420, 437, 2010, 295, 16829, 436, 434, 4194, 13], "temperature": 0.0, "avg_logprob": -0.14055098376227815, "compression_ratio": 1.6111111111111112, "no_speech_prob": 5.053007407695986e-05}, {"id": 130, "seek": 44756, "start": 463.72, "end": 469.2, "text": " Sahil is scraping data from Google Earth and then applying CNNs to", "tokens": [18280, 388, 307, 43738, 1412, 490, 3329, 4755, 293, 550, 9275, 24859, 82, 281], "temperature": 0.0, "avg_logprob": -0.14055098376227815, "compression_ratio": 1.6111111111111112, "no_speech_prob": 5.053007407695986e-05}, {"id": 131, "seek": 44756, "start": 469.2, "end": 473.8, "text": " identify their plots and use this so that they can qualify for better loans.", "tokens": [5876, 641, 28609, 293, 764, 341, 370, 300, 436, 393, 20276, 337, 1101, 15443, 13], "temperature": 0.0, "avg_logprob": -0.14055098376227815, "compression_ratio": 1.6111111111111112, "no_speech_prob": 5.053007407695986e-05}, {"id": 132, "seek": 47380, "start": 473.8, "end": 479.72, "text": " Bottom left is Sarah Hooker and a team of data scientists from Delta Analytics.", "tokens": [38289, 1411, 307, 9519, 33132, 260, 293, 257, 1469, 295, 1412, 7708, 490, 18183, 25944, 13], "temperature": 0.0, "avg_logprob": -0.15137681753739066, "compression_ratio": 1.6763485477178424, "no_speech_prob": 8.076741505647078e-05}, {"id": 133, "seek": 47380, "start": 479.72, "end": 483.40000000000003, "text": " Delta Analytics partners non-profits with teams of", "tokens": [18183, 25944, 4462, 2107, 12, 32480, 365, 5491, 295], "temperature": 0.0, "avg_logprob": -0.15137681753739066, "compression_ratio": 1.6763485477178424, "no_speech_prob": 8.076741505647078e-05}, {"id": 134, "seek": 47380, "start": 483.40000000000003, "end": 486.84000000000003, "text": " data scientists who volunteer to help work with them.", "tokens": [1412, 7708, 567, 13835, 281, 854, 589, 365, 552, 13], "temperature": 0.0, "avg_logprob": -0.15137681753739066, "compression_ratio": 1.6763485477178424, "no_speech_prob": 8.076741505647078e-05}, {"id": 135, "seek": 47380, "start": 486.84000000000003, "end": 490.48, "text": " Rainforest Connection is a startup that is putting", "tokens": [14487, 36629, 11653, 313, 307, 257, 18578, 300, 307, 3372], "temperature": 0.0, "avg_logprob": -0.15137681753739066, "compression_ratio": 1.6763485477178424, "no_speech_prob": 8.076741505647078e-05}, {"id": 136, "seek": 47380, "start": 490.48, "end": 493.8, "text": " recycled cell phones in endangered rainforest,", "tokens": [30674, 2815, 10216, 294, 37539, 48531, 11], "temperature": 0.0, "avg_logprob": -0.15137681753739066, "compression_ratio": 1.6763485477178424, "no_speech_prob": 8.076741505647078e-05}, {"id": 137, "seek": 47380, "start": 493.8, "end": 497.68, "text": " and then streaming audio to identify chainsaw noises,", "tokens": [293, 550, 11791, 6278, 281, 5876, 12626, 1607, 14620, 11], "temperature": 0.0, "avg_logprob": -0.15137681753739066, "compression_ratio": 1.6763485477178424, "no_speech_prob": 8.076741505647078e-05}, {"id": 138, "seek": 47380, "start": 497.68, "end": 501.68, "text": " and send out an alarm when there's someone cutting down the forest.", "tokens": [293, 2845, 484, 364, 14183, 562, 456, 311, 1580, 6492, 760, 264, 6719, 13], "temperature": 0.0, "avg_logprob": -0.15137681753739066, "compression_ratio": 1.6763485477178424, "no_speech_prob": 8.076741505647078e-05}, {"id": 139, "seek": 50168, "start": 501.68, "end": 504.24, "text": " This is a team of data scientists,", "tokens": [639, 307, 257, 1469, 295, 1412, 7708, 11], "temperature": 0.0, "avg_logprob": -0.14564246431403205, "compression_ratio": 1.6373626373626373, "no_speech_prob": 9.364467587147374e-06}, {"id": 140, "seek": 50168, "start": 504.24, "end": 506.14, "text": " they were running some tests in Berkeley,", "tokens": [436, 645, 2614, 512, 6921, 294, 23684, 11], "temperature": 0.0, "avg_logprob": -0.14564246431403205, "compression_ratio": 1.6373626373626373, "no_speech_prob": 9.364467587147374e-06}, {"id": 141, "seek": 50168, "start": 506.14, "end": 510.48, "text": " but they're using deep learning to identify chainsaws.", "tokens": [457, 436, 434, 1228, 2452, 2539, 281, 5876, 12626, 12282, 13], "temperature": 0.0, "avg_logprob": -0.14564246431403205, "compression_ratio": 1.6373626373626373, "no_speech_prob": 9.364467587147374e-06}, {"id": 142, "seek": 50168, "start": 510.48, "end": 514.6, "text": " Then above that, top left is a picture of Tosin Maisha.", "tokens": [1396, 3673, 300, 11, 1192, 1411, 307, 257, 3036, 295, 314, 329, 259, 4042, 16546, 13], "temperature": 0.0, "avg_logprob": -0.14564246431403205, "compression_ratio": 1.6373626373626373, "no_speech_prob": 9.364467587147374e-06}, {"id": 143, "seek": 50168, "start": 514.6, "end": 516.6800000000001, "text": " She's a student in Bangladesh.", "tokens": [1240, 311, 257, 3107, 294, 35260, 13], "temperature": 0.0, "avg_logprob": -0.14564246431403205, "compression_ratio": 1.6373626373626373, "no_speech_prob": 9.364467587147374e-06}, {"id": 144, "seek": 50168, "start": 516.6800000000001, "end": 521.5600000000001, "text": " She's done a project analyzing how the major Bangladeshi newspaper covers", "tokens": [1240, 311, 1096, 257, 1716, 23663, 577, 264, 2563, 32123, 2977, 4954, 13669, 10538], "temperature": 0.0, "avg_logprob": -0.14564246431403205, "compression_ratio": 1.6373626373626373, "no_speech_prob": 9.364467587147374e-06}, {"id": 145, "seek": 50168, "start": 521.5600000000001, "end": 526.72, "text": " violence against women by doing a visualization of having scraped the text.", "tokens": [6270, 1970, 2266, 538, 884, 257, 25801, 295, 1419, 13943, 3452, 264, 2487, 13], "temperature": 0.0, "avg_logprob": -0.14564246431403205, "compression_ratio": 1.6373626373626373, "no_speech_prob": 9.364467587147374e-06}, {"id": 146, "seek": 50168, "start": 526.72, "end": 530.72, "text": " She's blogged about the challenges of doing machine learning with intermittent", "tokens": [1240, 311, 6968, 3004, 466, 264, 4759, 295, 884, 3479, 2539, 365, 44084], "temperature": 0.0, "avg_logprob": -0.14564246431403205, "compression_ratio": 1.6373626373626373, "no_speech_prob": 9.364467587147374e-06}, {"id": 147, "seek": 53072, "start": 530.72, "end": 536.0600000000001, "text": " electricity. She currently has some free AWS credits that she was gifted,", "tokens": [10356, 13, 1240, 4362, 575, 512, 1737, 17650, 16816, 300, 750, 390, 27104, 11], "temperature": 0.0, "avg_logprob": -0.16254019536891906, "compression_ratio": 1.7366548042704626, "no_speech_prob": 1.1841530067613348e-05}, {"id": 148, "seek": 53072, "start": 536.0600000000001, "end": 537.9200000000001, "text": " but she doesn't know what she's going to do when they run out,", "tokens": [457, 750, 1177, 380, 458, 437, 750, 311, 516, 281, 360, 562, 436, 1190, 484, 11], "temperature": 0.0, "avg_logprob": -0.16254019536891906, "compression_ratio": 1.7366548042704626, "no_speech_prob": 1.1841530067613348e-05}, {"id": 149, "seek": 53072, "start": 537.9200000000001, "end": 540.0, "text": " because the expense is going to be a lot.", "tokens": [570, 264, 18406, 307, 516, 281, 312, 257, 688, 13], "temperature": 0.0, "avg_logprob": -0.16254019536891906, "compression_ratio": 1.7366548042704626, "no_speech_prob": 1.1841530067613348e-05}, {"id": 150, "seek": 53072, "start": 540.0, "end": 543.64, "text": " These are the type of problems and the type of students that we are really", "tokens": [1981, 366, 264, 2010, 295, 2740, 293, 264, 2010, 295, 1731, 300, 321, 366, 534], "temperature": 0.0, "avg_logprob": -0.16254019536891906, "compression_ratio": 1.7366548042704626, "no_speech_prob": 1.1841530067613348e-05}, {"id": 151, "seek": 53072, "start": 543.64, "end": 545.96, "text": " interested in and thinking about people with", "tokens": [3102, 294, 293, 1953, 466, 561, 365], "temperature": 0.0, "avg_logprob": -0.16254019536891906, "compression_ratio": 1.7366548042704626, "no_speech_prob": 1.1841530067613348e-05}, {"id": 152, "seek": 53072, "start": 545.96, "end": 551.32, "text": " few resources who have projects that they're really interested in.", "tokens": [1326, 3593, 567, 362, 4455, 300, 436, 434, 534, 3102, 294, 13], "temperature": 0.0, "avg_logprob": -0.16254019536891906, "compression_ratio": 1.7366548042704626, "no_speech_prob": 1.1841530067613348e-05}, {"id": 153, "seek": 53072, "start": 551.36, "end": 554.48, "text": " So most math classes are bottom up.", "tokens": [407, 881, 5221, 5359, 366, 2767, 493, 13], "temperature": 0.0, "avg_logprob": -0.16254019536891906, "compression_ratio": 1.7366548042704626, "no_speech_prob": 1.1841530067613348e-05}, {"id": 154, "seek": 53072, "start": 554.48, "end": 556.36, "text": " You learn each building block you need,", "tokens": [509, 1466, 1184, 2390, 3461, 291, 643, 11], "temperature": 0.0, "avg_logprob": -0.16254019536891906, "compression_ratio": 1.7366548042704626, "no_speech_prob": 1.1841530067613348e-05}, {"id": 155, "seek": 53072, "start": 556.36, "end": 558.6800000000001, "text": " and then eventually you can put them together.", "tokens": [293, 550, 4728, 291, 393, 829, 552, 1214, 13], "temperature": 0.0, "avg_logprob": -0.16254019536891906, "compression_ratio": 1.7366548042704626, "no_speech_prob": 1.1841530067613348e-05}, {"id": 156, "seek": 55868, "start": 558.68, "end": 562.68, "text": " It's hard for a lot of students to maintain motivation with that.", "tokens": [467, 311, 1152, 337, 257, 688, 295, 1731, 281, 6909, 12335, 365, 300, 13], "temperature": 0.0, "avg_logprob": -0.10690572112798691, "compression_ratio": 1.743859649122807, "no_speech_prob": 1.5778688293721643e-06}, {"id": 157, "seek": 55868, "start": 562.68, "end": 566.52, "text": " It's also hard to know what the big picture is or where you're going.", "tokens": [467, 311, 611, 1152, 281, 458, 437, 264, 955, 3036, 307, 420, 689, 291, 434, 516, 13], "temperature": 0.0, "avg_logprob": -0.10690572112798691, "compression_ratio": 1.743859649122807, "no_speech_prob": 1.5778688293721643e-06}, {"id": 158, "seek": 55868, "start": 566.52, "end": 568.7199999999999, "text": " It's hard to know which pieces you'll actually need.", "tokens": [467, 311, 1152, 281, 458, 597, 3755, 291, 603, 767, 643, 13], "temperature": 0.0, "avg_logprob": -0.10690572112798691, "compression_ratio": 1.743859649122807, "no_speech_prob": 1.5778688293721643e-06}, {"id": 159, "seek": 55868, "start": 568.7199999999999, "end": 571.9599999999999, "text": " So I sometimes talk to people that are studying theory that they think they're", "tokens": [407, 286, 2171, 751, 281, 561, 300, 366, 7601, 5261, 300, 436, 519, 436, 434], "temperature": 0.0, "avg_logprob": -0.10690572112798691, "compression_ratio": 1.743859649122807, "no_speech_prob": 1.5778688293721643e-06}, {"id": 160, "seek": 55868, "start": 571.9599999999999, "end": 573.62, "text": " going to need for deep learning,", "tokens": [516, 281, 643, 337, 2452, 2539, 11], "temperature": 0.0, "avg_logprob": -0.10690572112798691, "compression_ratio": 1.743859649122807, "no_speech_prob": 1.5778688293721643e-06}, {"id": 161, "seek": 55868, "start": 573.62, "end": 575.8, "text": " but don't actually end up needing.", "tokens": [457, 500, 380, 767, 917, 493, 18006, 13], "temperature": 0.0, "avg_logprob": -0.10690572112798691, "compression_ratio": 1.743859649122807, "no_speech_prob": 1.5778688293721643e-06}, {"id": 162, "seek": 55868, "start": 575.8, "end": 580.2399999999999, "text": " So we tried to get students using a neural net right away and six lines of code.", "tokens": [407, 321, 3031, 281, 483, 1731, 1228, 257, 18161, 2533, 558, 1314, 293, 2309, 3876, 295, 3089, 13], "temperature": 0.0, "avg_logprob": -0.10690572112798691, "compression_ratio": 1.743859649122807, "no_speech_prob": 1.5778688293721643e-06}, {"id": 163, "seek": 55868, "start": 580.2399999999999, "end": 585.4399999999999, "text": " We spent a lot of time refactoring our code to help make it concise and modular.", "tokens": [492, 4418, 257, 688, 295, 565, 1895, 578, 3662, 527, 3089, 281, 854, 652, 309, 44882, 293, 31111, 13], "temperature": 0.0, "avg_logprob": -0.10690572112798691, "compression_ratio": 1.743859649122807, "no_speech_prob": 1.5778688293721643e-06}, {"id": 164, "seek": 58544, "start": 585.44, "end": 589.96, "text": " We give students an Amazon machine image that's set up with everything they need.", "tokens": [492, 976, 1731, 364, 6795, 3479, 3256, 300, 311, 992, 493, 365, 1203, 436, 643, 13], "temperature": 0.0, "avg_logprob": -0.11810645060752754, "compression_ratio": 1.7280966767371602, "no_speech_prob": 5.337683433026541e-06}, {"id": 165, "seek": 58544, "start": 589.96, "end": 594.24, "text": " Then over time, we do gradually peel back the layers and get to lower level stuff.", "tokens": [1396, 670, 565, 11, 321, 360, 13145, 13889, 646, 264, 7914, 293, 483, 281, 3126, 1496, 1507, 13], "temperature": 0.0, "avg_logprob": -0.11810645060752754, "compression_ratio": 1.7280966767371602, "no_speech_prob": 5.337683433026541e-06}, {"id": 166, "seek": 58544, "start": 594.24, "end": 596.36, "text": " It's always motivated though by trying to improve", "tokens": [467, 311, 1009, 14515, 1673, 538, 1382, 281, 3470], "temperature": 0.0, "avg_logprob": -0.11810645060752754, "compression_ratio": 1.7280966767371602, "no_speech_prob": 5.337683433026541e-06}, {"id": 167, "seek": 58544, "start": 596.36, "end": 599.6400000000001, "text": " our performance or to tackle a harder problem.", "tokens": [527, 3389, 420, 281, 14896, 257, 6081, 1154, 13], "temperature": 0.0, "avg_logprob": -0.11810645060752754, "compression_ratio": 1.7280966767371602, "no_speech_prob": 5.337683433026541e-06}, {"id": 168, "seek": 58544, "start": 599.6400000000001, "end": 602.4000000000001, "text": " So eventually they are understanding the low level components.", "tokens": [407, 4728, 436, 366, 3701, 264, 2295, 1496, 6677, 13], "temperature": 0.0, "avg_logprob": -0.11810645060752754, "compression_ratio": 1.7280966767371602, "no_speech_prob": 5.337683433026541e-06}, {"id": 169, "seek": 58544, "start": 602.4000000000001, "end": 604.7600000000001, "text": " We've just gone in the reverse order.", "tokens": [492, 600, 445, 2780, 294, 264, 9943, 1668, 13], "temperature": 0.0, "avg_logprob": -0.11810645060752754, "compression_ratio": 1.7280966767371602, "no_speech_prob": 5.337683433026541e-06}, {"id": 170, "seek": 58544, "start": 604.7600000000001, "end": 607.8800000000001, "text": " Then we are huge fans of transfer learning.", "tokens": [1396, 321, 366, 2603, 4499, 295, 5003, 2539, 13], "temperature": 0.0, "avg_logprob": -0.11810645060752754, "compression_ratio": 1.7280966767371602, "no_speech_prob": 5.337683433026541e-06}, {"id": 171, "seek": 58544, "start": 607.8800000000001, "end": 609.96, "text": " Transfer learning is the technique where you take", "tokens": [35025, 2539, 307, 264, 6532, 689, 291, 747], "temperature": 0.0, "avg_logprob": -0.11810645060752754, "compression_ratio": 1.7280966767371602, "no_speech_prob": 5.337683433026541e-06}, {"id": 172, "seek": 58544, "start": 609.96, "end": 611.8000000000001, "text": " a pre-trained network that was trained on", "tokens": [257, 659, 12, 17227, 2001, 3209, 300, 390, 8895, 322], "temperature": 0.0, "avg_logprob": -0.11810645060752754, "compression_ratio": 1.7280966767371602, "no_speech_prob": 5.337683433026541e-06}, {"id": 173, "seek": 58544, "start": 611.8000000000001, "end": 615.0, "text": " a different dataset and a different problem and apply it to your problem,", "tokens": [257, 819, 28872, 293, 257, 819, 1154, 293, 3079, 309, 281, 428, 1154, 11], "temperature": 0.0, "avg_logprob": -0.11810645060752754, "compression_ratio": 1.7280966767371602, "no_speech_prob": 5.337683433026541e-06}, {"id": 174, "seek": 61500, "start": 615.0, "end": 618.68, "text": " and often you retrain the last layer or the last few layers.", "tokens": [293, 2049, 291, 1533, 7146, 264, 1036, 4583, 420, 264, 1036, 1326, 7914, 13], "temperature": 0.0, "avg_logprob": -0.16430637055793695, "compression_ratio": 1.5408163265306123, "no_speech_prob": 6.048303475836292e-06}, {"id": 175, "seek": 61500, "start": 618.68, "end": 621.6, "text": " It's really powerful because it lets people with smaller datasets and", "tokens": [467, 311, 534, 4005, 570, 309, 6653, 561, 365, 4356, 42856, 293], "temperature": 0.0, "avg_logprob": -0.16430637055793695, "compression_ratio": 1.5408163265306123, "no_speech_prob": 6.048303475836292e-06}, {"id": 176, "seek": 61500, "start": 621.6, "end": 624.08, "text": " limited computational power take advantage", "tokens": [5567, 28270, 1347, 747, 5002], "temperature": 0.0, "avg_logprob": -0.16430637055793695, "compression_ratio": 1.5408163265306123, "no_speech_prob": 6.048303475836292e-06}, {"id": 177, "seek": 61500, "start": 624.08, "end": 627.48, "text": " of these networks that have been trained elsewhere.", "tokens": [295, 613, 9590, 300, 362, 668, 8895, 14517, 13], "temperature": 0.0, "avg_logprob": -0.16430637055793695, "compression_ratio": 1.5408163265306123, "no_speech_prob": 6.048303475836292e-06}, {"id": 178, "seek": 61500, "start": 627.48, "end": 629.92, "text": " So this is from lesson one.", "tokens": [407, 341, 307, 490, 6898, 472, 13], "temperature": 0.0, "avg_logprob": -0.16430637055793695, "compression_ratio": 1.5408163265306123, "no_speech_prob": 6.048303475836292e-06}, {"id": 179, "seek": 61500, "start": 629.92, "end": 635.88, "text": " It's six lines of code and it gets you better than 97 percent accuracy on dogs versus cats,", "tokens": [467, 311, 2309, 3876, 295, 3089, 293, 309, 2170, 291, 1101, 813, 23399, 3043, 14170, 322, 7197, 5717, 11111, 11], "temperature": 0.0, "avg_logprob": -0.16430637055793695, "compression_ratio": 1.5408163265306123, "no_speech_prob": 6.048303475836292e-06}, {"id": 180, "seek": 61500, "start": 635.88, "end": 639.32, "text": " which has been a Kaggle competition twice now.", "tokens": [597, 575, 668, 257, 48751, 22631, 6211, 6091, 586, 13], "temperature": 0.0, "avg_logprob": -0.16430637055793695, "compression_ratio": 1.5408163265306123, "no_speech_prob": 6.048303475836292e-06}, {"id": 181, "seek": 61500, "start": 639.68, "end": 642.0, "text": " Here we're using VGG,", "tokens": [1692, 321, 434, 1228, 691, 27561, 11], "temperature": 0.0, "avg_logprob": -0.16430637055793695, "compression_ratio": 1.5408163265306123, "no_speech_prob": 6.048303475836292e-06}, {"id": 182, "seek": 61500, "start": 642.0, "end": 644.64, "text": " which was the ImageNet winner in 2014,", "tokens": [597, 390, 264, 29903, 31890, 8507, 294, 8227, 11], "temperature": 0.0, "avg_logprob": -0.16430637055793695, "compression_ratio": 1.5408163265306123, "no_speech_prob": 6.048303475836292e-06}, {"id": 183, "seek": 64464, "start": 644.64, "end": 647.4399999999999, "text": " and then just retraining the last layer.", "tokens": [293, 550, 445, 49356, 1760, 264, 1036, 4583, 13], "temperature": 0.0, "avg_logprob": -0.1275981721423921, "compression_ratio": 1.6090534979423867, "no_speech_prob": 3.784963837460964e-06}, {"id": 184, "seek": 64464, "start": 647.4399999999999, "end": 650.0, "text": " So what does all this mean for you?", "tokens": [407, 437, 775, 439, 341, 914, 337, 291, 30], "temperature": 0.0, "avg_logprob": -0.1275981721423921, "compression_ratio": 1.6090534979423867, "no_speech_prob": 3.784963837460964e-06}, {"id": 185, "seek": 64464, "start": 650.0, "end": 653.0, "text": " What lessons can you take away?", "tokens": [708, 8820, 393, 291, 747, 1314, 30], "temperature": 0.0, "avg_logprob": -0.1275981721423921, "compression_ratio": 1.6090534979423867, "no_speech_prob": 3.784963837460964e-06}, {"id": 186, "seek": 64464, "start": 653.0, "end": 659.12, "text": " So my very opinionated recommendation is to start with Keras.", "tokens": [407, 452, 588, 4800, 770, 11879, 307, 281, 722, 365, 591, 6985, 13], "temperature": 0.0, "avg_logprob": -0.1275981721423921, "compression_ratio": 1.6090534979423867, "no_speech_prob": 3.784963837460964e-06}, {"id": 187, "seek": 64464, "start": 659.12, "end": 664.16, "text": " Keras is a Python library that sits on top of either Theano or TensorFlow,", "tokens": [591, 6985, 307, 257, 15329, 6405, 300, 12696, 322, 1192, 295, 2139, 440, 3730, 420, 37624, 11], "temperature": 0.0, "avg_logprob": -0.1275981721423921, "compression_ratio": 1.6090534979423867, "no_speech_prob": 3.784963837460964e-06}, {"id": 188, "seek": 64464, "start": 664.16, "end": 667.48, "text": " and the abstractions in Keras are just so well-mapped to", "tokens": [293, 264, 12649, 626, 294, 591, 6985, 366, 445, 370, 731, 12, 1696, 3320, 281], "temperature": 0.0, "avg_logprob": -0.1275981721423921, "compression_ratio": 1.6090534979423867, "no_speech_prob": 3.784963837460964e-06}, {"id": 189, "seek": 64464, "start": 667.48, "end": 671.8, "text": " the abstractions of neural networks that I think it makes it a lot easier to learn both.", "tokens": [264, 12649, 626, 295, 18161, 9590, 300, 286, 519, 309, 1669, 309, 257, 688, 3571, 281, 1466, 1293, 13], "temperature": 0.0, "avg_logprob": -0.1275981721423921, "compression_ratio": 1.6090534979423867, "no_speech_prob": 3.784963837460964e-06}, {"id": 190, "seek": 67180, "start": 671.8, "end": 674.92, "text": " I wrote a blog post several months ago where I said,", "tokens": [286, 4114, 257, 6968, 2183, 2940, 2493, 2057, 689, 286, 848, 11], "temperature": 0.0, "avg_logprob": -0.10160276384064645, "compression_ratio": 1.6952054794520548, "no_speech_prob": 1.805465217330493e-05}, {"id": 191, "seek": 67180, "start": 674.92, "end": 679.0, "text": " using TensorFlow makes me feel like I'm not smart enough to use TensorFlow,", "tokens": [1228, 37624, 1669, 385, 841, 411, 286, 478, 406, 4069, 1547, 281, 764, 37624, 11], "temperature": 0.0, "avg_logprob": -0.10160276384064645, "compression_ratio": 1.6952054794520548, "no_speech_prob": 1.805465217330493e-05}, {"id": 192, "seek": 67180, "start": 679.0, "end": 683.12, "text": " whereas using Keras makes me feel like neural networks are easier than I realized.", "tokens": [9735, 1228, 591, 6985, 1669, 385, 841, 411, 18161, 9590, 366, 3571, 813, 286, 5334, 13], "temperature": 0.0, "avg_logprob": -0.10160276384064645, "compression_ratio": 1.6952054794520548, "no_speech_prob": 1.805465217330493e-05}, {"id": 193, "seek": 67180, "start": 683.12, "end": 686.4799999999999, "text": " This was a very popular post and a lot of people said they agreed.", "tokens": [639, 390, 257, 588, 3743, 2183, 293, 257, 688, 295, 561, 848, 436, 9166, 13], "temperature": 0.0, "avg_logprob": -0.10160276384064645, "compression_ratio": 1.6952054794520548, "no_speech_prob": 1.805465217330493e-05}, {"id": 194, "seek": 67180, "start": 686.4799999999999, "end": 690.24, "text": " So we used Theano and Keras in part one of the course.", "tokens": [407, 321, 1143, 440, 3730, 293, 591, 6985, 294, 644, 472, 295, 264, 1164, 13], "temperature": 0.0, "avg_logprob": -0.10160276384064645, "compression_ratio": 1.6952054794520548, "no_speech_prob": 1.805465217330493e-05}, {"id": 195, "seek": 67180, "start": 690.24, "end": 692.28, "text": " We do use TensorFlow and PyTorch,", "tokens": [492, 360, 764, 37624, 293, 9953, 51, 284, 339, 11], "temperature": 0.0, "avg_logprob": -0.10160276384064645, "compression_ratio": 1.6952054794520548, "no_speech_prob": 1.805465217330493e-05}, {"id": 196, "seek": 67180, "start": 692.28, "end": 694.04, "text": " which is also excellent in part two,", "tokens": [597, 307, 611, 7103, 294, 644, 732, 11], "temperature": 0.0, "avg_logprob": -0.10160276384064645, "compression_ratio": 1.6952054794520548, "no_speech_prob": 1.805465217330493e-05}, {"id": 197, "seek": 67180, "start": 694.04, "end": 697.12, "text": " but I still really recommend starting with Keras.", "tokens": [457, 286, 920, 534, 2748, 2891, 365, 591, 6985, 13], "temperature": 0.0, "avg_logprob": -0.10160276384064645, "compression_ratio": 1.6952054794520548, "no_speech_prob": 1.805465217330493e-05}, {"id": 198, "seek": 67180, "start": 697.12, "end": 699.4, "text": " I should say I have not tried MXNet yet,", "tokens": [286, 820, 584, 286, 362, 406, 3031, 47509, 31890, 1939, 11], "temperature": 0.0, "avg_logprob": -0.10160276384064645, "compression_ratio": 1.6952054794520548, "no_speech_prob": 1.805465217330493e-05}, {"id": 199, "seek": 69940, "start": 699.4, "end": 703.88, "text": " and so I'm looking forward to hearing the next talk about that.", "tokens": [293, 370, 286, 478, 1237, 2128, 281, 4763, 264, 958, 751, 466, 300, 13], "temperature": 0.0, "avg_logprob": -0.11475878690196349, "compression_ratio": 1.6556776556776556, "no_speech_prob": 1.3843709893990308e-05}, {"id": 200, "seek": 69940, "start": 703.88, "end": 708.28, "text": " Don't read or watch tutorials without taking time to code.", "tokens": [1468, 380, 1401, 420, 1159, 17616, 1553, 1940, 565, 281, 3089, 13], "temperature": 0.0, "avg_logprob": -0.11475878690196349, "compression_ratio": 1.6556776556776556, "no_speech_prob": 1.3843709893990308e-05}, {"id": 201, "seek": 69940, "start": 708.28, "end": 710.8, "text": " It's by coding that you learn.", "tokens": [467, 311, 538, 17720, 300, 291, 1466, 13], "temperature": 0.0, "avg_logprob": -0.11475878690196349, "compression_ratio": 1.6556776556776556, "no_speech_prob": 1.3843709893990308e-05}, {"id": 202, "seek": 69940, "start": 710.8, "end": 716.06, "text": " Then hitting Shift-Enter to go through somebody else's Jupyter Notebook does not count as coding.", "tokens": [1396, 8850, 28304, 12, 16257, 391, 281, 352, 807, 2618, 1646, 311, 22125, 88, 391, 11633, 2939, 775, 406, 1207, 382, 17720, 13], "temperature": 0.0, "avg_logprob": -0.11475878690196349, "compression_ratio": 1.6556776556776556, "no_speech_prob": 1.3843709893990308e-05}, {"id": 203, "seek": 69940, "start": 716.06, "end": 718.3199999999999, "text": " You really need to type it out yourself.", "tokens": [509, 534, 643, 281, 2010, 309, 484, 1803, 13], "temperature": 0.0, "avg_logprob": -0.11475878690196349, "compression_ratio": 1.6556776556776556, "no_speech_prob": 1.3843709893990308e-05}, {"id": 204, "seek": 69940, "start": 718.3199999999999, "end": 722.52, "text": " You need to modify the inputs and see how that impacts the outputs.", "tokens": [509, 643, 281, 16927, 264, 15743, 293, 536, 577, 300, 11606, 264, 23930, 13], "temperature": 0.0, "avg_logprob": -0.11475878690196349, "compression_ratio": 1.6556776556776556, "no_speech_prob": 1.3843709893990308e-05}, {"id": 205, "seek": 69940, "start": 722.52, "end": 727.28, "text": " Then have a programming project that you're interested in and let that drive your learning.", "tokens": [1396, 362, 257, 9410, 1716, 300, 291, 434, 3102, 294, 293, 718, 300, 3332, 428, 2539, 13], "temperature": 0.0, "avg_logprob": -0.11475878690196349, "compression_ratio": 1.6556776556776556, "no_speech_prob": 1.3843709893990308e-05}, {"id": 206, "seek": 72728, "start": 727.28, "end": 729.72, "text": " Don't feel like you need to spend several months learning", "tokens": [1468, 380, 841, 411, 291, 643, 281, 3496, 2940, 2493, 2539], "temperature": 0.0, "avg_logprob": -0.14529500045175628, "compression_ratio": 1.6633663366336633, "no_speech_prob": 1.1188941471118596e-06}, {"id": 207, "seek": 72728, "start": 729.72, "end": 732.24, "text": " before you can tackle the project you're interested in.", "tokens": [949, 291, 393, 14896, 264, 1716, 291, 434, 3102, 294, 13], "temperature": 0.0, "avg_logprob": -0.14529500045175628, "compression_ratio": 1.6633663366336633, "no_speech_prob": 1.1188941471118596e-06}, {"id": 208, "seek": 72728, "start": 732.24, "end": 735.4399999999999, "text": " Start right away and then that'll keep you always learning", "tokens": [6481, 558, 1314, 293, 550, 300, 603, 1066, 291, 1009, 2539], "temperature": 0.0, "avg_logprob": -0.14529500045175628, "compression_ratio": 1.6633663366336633, "no_speech_prob": 1.1188941471118596e-06}, {"id": 209, "seek": 72728, "start": 735.4399999999999, "end": 739.72, "text": " the most relevant things you need to tackle the problem you care about.", "tokens": [264, 881, 7340, 721, 291, 643, 281, 14896, 264, 1154, 291, 1127, 466, 13], "temperature": 0.0, "avg_logprob": -0.14529500045175628, "compression_ratio": 1.6633663366336633, "no_speech_prob": 1.1188941471118596e-06}, {"id": 210, "seek": 72728, "start": 739.72, "end": 744.72, "text": " Then finally, a lot of programmers are very snobby about Excel,", "tokens": [1396, 2721, 11, 257, 688, 295, 41504, 366, 588, 2406, 996, 2322, 466, 19060, 11], "temperature": 0.0, "avg_logprob": -0.14529500045175628, "compression_ratio": 1.6633663366336633, "no_speech_prob": 1.1188941471118596e-06}, {"id": 211, "seek": 72728, "start": 744.72, "end": 747.56, "text": " but it's actually a very useful visual tool.", "tokens": [457, 309, 311, 767, 257, 588, 4420, 5056, 2290, 13], "temperature": 0.0, "avg_logprob": -0.14529500045175628, "compression_ratio": 1.6633663366336633, "no_speech_prob": 1.1188941471118596e-06}, {"id": 212, "seek": 72728, "start": 747.56, "end": 751.88, "text": " Jeremy spent a lot of time thinking about how to implement Stochastic Gradient Descent,", "tokens": [17809, 4418, 257, 688, 295, 565, 1953, 466, 577, 281, 4445, 745, 8997, 2750, 16710, 1196, 3885, 2207, 11], "temperature": 0.0, "avg_logprob": -0.14529500045175628, "compression_ratio": 1.6633663366336633, "no_speech_prob": 1.1188941471118596e-06}, {"id": 213, "seek": 72728, "start": 751.88, "end": 755.92, "text": " as well as other optimizers like RMSProp and AdaGrad in Excel.", "tokens": [382, 731, 382, 661, 5028, 22525, 411, 497, 10288, 47, 1513, 293, 32276, 38, 6206, 294, 19060, 13], "temperature": 0.0, "avg_logprob": -0.14529500045175628, "compression_ratio": 1.6633663366336633, "no_speech_prob": 1.1188941471118596e-06}, {"id": 214, "seek": 75592, "start": 755.92, "end": 759.9599999999999, "text": " It provides a really visual picture of which cells depend on which cells", "tokens": [467, 6417, 257, 534, 5056, 3036, 295, 597, 5438, 5672, 322, 597, 5438], "temperature": 0.0, "avg_logprob": -0.20044831306703628, "compression_ratio": 1.7098976109215016, "no_speech_prob": 5.507104560820153e-06}, {"id": 215, "seek": 75592, "start": 759.9599999999999, "end": 762.36, "text": " and how the calculations are updating.", "tokens": [293, 577, 264, 20448, 366, 25113, 13], "temperature": 0.0, "avg_logprob": -0.20044831306703628, "compression_ratio": 1.7098976109215016, "no_speech_prob": 5.507104560820153e-06}, {"id": 216, "seek": 75592, "start": 762.36, "end": 764.5999999999999, "text": " I encourage you to check out those notebooks.", "tokens": [286, 5373, 291, 281, 1520, 484, 729, 43782, 13], "temperature": 0.0, "avg_logprob": -0.20044831306703628, "compression_ratio": 1.7098976109215016, "no_speech_prob": 5.507104560820153e-06}, {"id": 217, "seek": 75592, "start": 764.5999999999999, "end": 767.3199999999999, "text": " Also, if you're ever stumped on a math concept,", "tokens": [2743, 11, 498, 291, 434, 1562, 43164, 292, 322, 257, 5221, 3410, 11], "temperature": 0.0, "avg_logprob": -0.20044831306703628, "compression_ratio": 1.7098976109215016, "no_speech_prob": 5.507104560820153e-06}, {"id": 218, "seek": 75592, "start": 767.3199999999999, "end": 772.12, "text": " try implementing it in Excel and then once you have it down, put it in Python.", "tokens": [853, 18114, 309, 294, 19060, 293, 550, 1564, 291, 362, 309, 760, 11, 829, 309, 294, 15329, 13], "temperature": 0.0, "avg_logprob": -0.20044831306703628, "compression_ratio": 1.7098976109215016, "no_speech_prob": 5.507104560820153e-06}, {"id": 219, "seek": 75592, "start": 773.64, "end": 776.68, "text": " There are two tests of whether you understand something.", "tokens": [821, 366, 732, 6921, 295, 1968, 291, 1223, 746, 13], "temperature": 0.0, "avg_logprob": -0.20044831306703628, "compression_ratio": 1.7098976109215016, "no_speech_prob": 5.507104560820153e-06}, {"id": 220, "seek": 75592, "start": 776.68, "end": 779.88, "text": " One is whether you can code it and build something with it.", "tokens": [1485, 307, 1968, 291, 393, 3089, 309, 293, 1322, 746, 365, 309, 13], "temperature": 0.0, "avg_logprob": -0.20044831306703628, "compression_ratio": 1.7098976109215016, "no_speech_prob": 5.507104560820153e-06}, {"id": 221, "seek": 75592, "start": 779.88, "end": 782.5999999999999, "text": " The other is can you explain it to somebody else?", "tokens": [440, 661, 307, 393, 291, 2903, 309, 281, 2618, 1646, 30], "temperature": 0.0, "avg_logprob": -0.20044831306703628, "compression_ratio": 1.7098976109215016, "no_speech_prob": 5.507104560820153e-06}, {"id": 222, "seek": 75592, "start": 782.5999999999999, "end": 785.8, "text": " Ways to practice explaining what you're learning.", "tokens": [343, 3772, 281, 3124, 13468, 437, 291, 434, 2539, 13], "temperature": 0.0, "avg_logprob": -0.20044831306703628, "compression_ratio": 1.7098976109215016, "no_speech_prob": 5.507104560820153e-06}, {"id": 223, "seek": 78580, "start": 785.8, "end": 787.4399999999999, "text": " One is to write a blog post.", "tokens": [1485, 307, 281, 2464, 257, 6968, 2183, 13], "temperature": 0.0, "avg_logprob": -0.15466700562643348, "compression_ratio": 1.6728624535315986, "no_speech_prob": 8.26581526780501e-06}, {"id": 224, "seek": 78580, "start": 787.4399999999999, "end": 792.24, "text": " Here are two blog posts by our students that I thought they were really well written", "tokens": [1692, 366, 732, 6968, 12300, 538, 527, 1731, 300, 286, 1194, 436, 645, 534, 731, 3720], "temperature": 0.0, "avg_logprob": -0.15466700562643348, "compression_ratio": 1.6728624535315986, "no_speech_prob": 8.26581526780501e-06}, {"id": 225, "seek": 78580, "start": 792.24, "end": 794.56, "text": " and they were also very accessible.", "tokens": [293, 436, 645, 611, 588, 9515, 13], "temperature": 0.0, "avg_logprob": -0.15466700562643348, "compression_ratio": 1.6728624535315986, "no_speech_prob": 8.26581526780501e-06}, {"id": 226, "seek": 78580, "start": 794.56, "end": 798.04, "text": " Remember, you don't have to be an expert to start blogging.", "tokens": [5459, 11, 291, 500, 380, 362, 281, 312, 364, 5844, 281, 722, 6968, 3249, 13], "temperature": 0.0, "avg_logprob": -0.15466700562643348, "compression_ratio": 1.6728624535315986, "no_speech_prob": 8.26581526780501e-06}, {"id": 227, "seek": 78580, "start": 798.04, "end": 801.4799999999999, "text": " Your target audience is the person that's just one or two steps behind you", "tokens": [2260, 3779, 4034, 307, 264, 954, 300, 311, 445, 472, 420, 732, 4439, 2261, 291], "temperature": 0.0, "avg_logprob": -0.15466700562643348, "compression_ratio": 1.6728624535315986, "no_speech_prob": 8.26581526780501e-06}, {"id": 228, "seek": 78580, "start": 801.4799999999999, "end": 804.8, "text": " and you're actually best positioned to help that person.", "tokens": [293, 291, 434, 767, 1151, 24889, 281, 854, 300, 954, 13], "temperature": 0.0, "avg_logprob": -0.15466700562643348, "compression_ratio": 1.6728624535315986, "no_speech_prob": 8.26581526780501e-06}, {"id": 229, "seek": 78580, "start": 804.8, "end": 808.16, "text": " The other place that you can practice explaining what you're learning", "tokens": [440, 661, 1081, 300, 291, 393, 3124, 13468, 437, 291, 434, 2539], "temperature": 0.0, "avg_logprob": -0.15466700562643348, "compression_ratio": 1.6728624535315986, "no_speech_prob": 8.26581526780501e-06}, {"id": 230, "seek": 78580, "start": 808.16, "end": 810.56, "text": " is answering other people's questions.", "tokens": [307, 13430, 661, 561, 311, 1651, 13], "temperature": 0.0, "avg_logprob": -0.15466700562643348, "compression_ratio": 1.6728624535315986, "no_speech_prob": 8.26581526780501e-06}, {"id": 231, "seek": 81056, "start": 810.56, "end": 816.1999999999999, "text": " Two places you can do that are on our forums, which are forums.fast.ai.", "tokens": [4453, 3190, 291, 393, 360, 300, 366, 322, 527, 26998, 11, 597, 366, 26998, 13, 7011, 13, 1301, 13], "temperature": 0.0, "avg_logprob": -0.1915922924480607, "compression_ratio": 1.6996466431095407, "no_speech_prob": 2.7517107810126618e-05}, {"id": 232, "seek": 81056, "start": 816.1999999999999, "end": 820.7199999999999, "text": " There are always interesting discussions happening related to deep learning.", "tokens": [821, 366, 1009, 1880, 11088, 2737, 4077, 281, 2452, 2539, 13], "temperature": 0.0, "avg_logprob": -0.1915922924480607, "compression_ratio": 1.6996466431095407, "no_speech_prob": 2.7517107810126618e-05}, {"id": 233, "seek": 81056, "start": 820.7199999999999, "end": 824.8, "text": " Here, people are talking about the Kaggle fisheries competition,", "tokens": [1692, 11, 561, 366, 1417, 466, 264, 48751, 22631, 20698, 530, 6211, 11], "temperature": 0.0, "avg_logprob": -0.1915922924480607, "compression_ratio": 1.6996466431095407, "no_speech_prob": 2.7517107810126618e-05}, {"id": 234, "seek": 81056, "start": 824.8, "end": 827.28, "text": " about average pooling instead of dense layers,", "tokens": [466, 4274, 7005, 278, 2602, 295, 18011, 7914, 11], "temperature": 0.0, "avg_logprob": -0.1915922924480607, "compression_ratio": 1.6996466431095407, "no_speech_prob": 2.7517107810126618e-05}, {"id": 235, "seek": 81056, "start": 827.28, "end": 831.4, "text": " or how to use persistent AWS spot instances.", "tokens": [420, 577, 281, 764, 24315, 17650, 4008, 14519, 13], "temperature": 0.0, "avg_logprob": -0.1915922924480607, "compression_ratio": 1.6996466431095407, "no_speech_prob": 2.7517107810126618e-05}, {"id": 236, "seek": 81056, "start": 831.4, "end": 832.76, "text": " Definitely, check them out.", "tokens": [12151, 11, 1520, 552, 484, 13], "temperature": 0.0, "avg_logprob": -0.1915922924480607, "compression_ratio": 1.6996466431095407, "no_speech_prob": 2.7517107810126618e-05}, {"id": 237, "seek": 81056, "start": 832.76, "end": 837.0, "text": " Another place that you can ask or answer questions is the learn machine learning subreddit,", "tokens": [3996, 1081, 300, 291, 393, 1029, 420, 1867, 1651, 307, 264, 1466, 3479, 2539, 1422, 986, 17975, 11], "temperature": 0.0, "avg_logprob": -0.1915922924480607, "compression_ratio": 1.6996466431095407, "no_speech_prob": 2.7517107810126618e-05}, {"id": 238, "seek": 83700, "start": 837.0, "end": 841.16, "text": " which is different than the machine learning subreddit.", "tokens": [597, 307, 819, 813, 264, 3479, 2539, 1422, 986, 17975, 13], "temperature": 0.0, "avg_logprob": -0.07231903857872134, "compression_ratio": 1.7604166666666667, "no_speech_prob": 5.013176178181311e-06}, {"id": 239, "seek": 83700, "start": 841.16, "end": 845.32, "text": " Here's a quote from one of our students after part one of the course.", "tokens": [1692, 311, 257, 6513, 490, 472, 295, 527, 1731, 934, 644, 472, 295, 264, 1164, 13], "temperature": 0.0, "avg_logprob": -0.07231903857872134, "compression_ratio": 1.7604166666666667, "no_speech_prob": 5.013176178181311e-06}, {"id": 240, "seek": 83700, "start": 845.32, "end": 848.76, "text": " I personally fell into the habit of watching the lectures too much", "tokens": [286, 5665, 5696, 666, 264, 7164, 295, 1976, 264, 16564, 886, 709], "temperature": 0.0, "avg_logprob": -0.07231903857872134, "compression_ratio": 1.7604166666666667, "no_speech_prob": 5.013176178181311e-06}, {"id": 241, "seek": 83700, "start": 848.76, "end": 851.8, "text": " and Googling definitions too much without running the code.", "tokens": [293, 45005, 1688, 21988, 886, 709, 1553, 2614, 264, 3089, 13], "temperature": 0.0, "avg_logprob": -0.07231903857872134, "compression_ratio": 1.7604166666666667, "no_speech_prob": 5.013176178181311e-06}, {"id": 242, "seek": 83700, "start": 851.8, "end": 854.08, "text": " At first, I thought that I should read the code quickly", "tokens": [1711, 700, 11, 286, 1194, 300, 286, 820, 1401, 264, 3089, 2661], "temperature": 0.0, "avg_logprob": -0.07231903857872134, "compression_ratio": 1.7604166666666667, "no_speech_prob": 5.013176178181311e-06}, {"id": 243, "seek": 83700, "start": 854.08, "end": 857.0, "text": " and then spend time researching the theory behind it.", "tokens": [293, 550, 3496, 565, 24176, 264, 5261, 2261, 309, 13], "temperature": 0.0, "avg_logprob": -0.07231903857872134, "compression_ratio": 1.7604166666666667, "no_speech_prob": 5.013176178181311e-06}, {"id": 244, "seek": 83700, "start": 857.0, "end": 861.12, "text": " In retrospect, I should have spent the majority of my time on the actual code,", "tokens": [682, 34997, 11, 286, 820, 362, 4418, 264, 6286, 295, 452, 565, 322, 264, 3539, 3089, 11], "temperature": 0.0, "avg_logprob": -0.07231903857872134, "compression_ratio": 1.7604166666666667, "no_speech_prob": 5.013176178181311e-06}, {"id": 245, "seek": 83700, "start": 861.12, "end": 864.36, "text": " running it and seeing what goes into it and what comes out of it.", "tokens": [2614, 309, 293, 2577, 437, 1709, 666, 309, 293, 437, 1487, 484, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.07231903857872134, "compression_ratio": 1.7604166666666667, "no_speech_prob": 5.013176178181311e-06}, {"id": 246, "seek": 86436, "start": 864.36, "end": 868.08, "text": " This is excellent advice, learn from this person's mistake,", "tokens": [639, 307, 7103, 5192, 11, 1466, 490, 341, 954, 311, 6146, 11], "temperature": 0.0, "avg_logprob": -0.1765195779633104, "compression_ratio": 1.674074074074074, "no_speech_prob": 1.9823588445433415e-05}, {"id": 247, "seek": 86436, "start": 868.08, "end": 870.6, "text": " stay focused on the code.", "tokens": [1754, 5178, 322, 264, 3089, 13], "temperature": 0.0, "avg_logprob": -0.1765195779633104, "compression_ratio": 1.674074074074074, "no_speech_prob": 1.9823588445433415e-05}, {"id": 248, "seek": 86436, "start": 871.28, "end": 874.64, "text": " In summary, some keys for learning are to have", "tokens": [682, 12691, 11, 512, 9317, 337, 2539, 366, 281, 362], "temperature": 0.0, "avg_logprob": -0.1765195779633104, "compression_ratio": 1.674074074074074, "no_speech_prob": 1.9823588445433415e-05}, {"id": 249, "seek": 86436, "start": 874.64, "end": 879.52, "text": " a practical coding project that you care about and let that lead the way for you.", "tokens": [257, 8496, 17720, 1716, 300, 291, 1127, 466, 293, 718, 300, 1477, 264, 636, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.1765195779633104, "compression_ratio": 1.674074074074074, "no_speech_prob": 1.9823588445433415e-05}, {"id": 250, "seek": 86436, "start": 879.52, "end": 882.32, "text": " Get something working as fast as possible.", "tokens": [3240, 746, 1364, 382, 2370, 382, 1944, 13], "temperature": 0.0, "avg_logprob": -0.1765195779633104, "compression_ratio": 1.674074074074074, "no_speech_prob": 1.9823588445433415e-05}, {"id": 251, "seek": 86436, "start": 882.32, "end": 885.12, "text": " It's okay if you don't understand all the components at first,", "tokens": [467, 311, 1392, 498, 291, 500, 380, 1223, 439, 264, 6677, 412, 700, 11], "temperature": 0.0, "avg_logprob": -0.1765195779633104, "compression_ratio": 1.674074074074074, "no_speech_prob": 1.9823588445433415e-05}, {"id": 252, "seek": 86436, "start": 885.12, "end": 886.6, "text": " and that's actually better.", "tokens": [293, 300, 311, 767, 1101, 13], "temperature": 0.0, "avg_logprob": -0.1765195779633104, "compression_ratio": 1.674074074074074, "no_speech_prob": 1.9823588445433415e-05}, {"id": 253, "seek": 86436, "start": 886.6, "end": 888.24, "text": " Then as time goes on,", "tokens": [1396, 382, 565, 1709, 322, 11], "temperature": 0.0, "avg_logprob": -0.1765195779633104, "compression_ratio": 1.674074074074074, "no_speech_prob": 1.9823588445433415e-05}, {"id": 254, "seek": 86436, "start": 888.24, "end": 893.88, "text": " you will learn about the components as you improve your performance and dig down.", "tokens": [291, 486, 1466, 466, 264, 6677, 382, 291, 3470, 428, 3389, 293, 2528, 760, 13], "temperature": 0.0, "avg_logprob": -0.1765195779633104, "compression_ratio": 1.674074074074074, "no_speech_prob": 1.9823588445433415e-05}, {"id": 255, "seek": 89388, "start": 893.88, "end": 897.92, "text": " Then help others answer questions and write about what you're learning.", "tokens": [1396, 854, 2357, 1867, 1651, 293, 2464, 466, 437, 291, 434, 2539, 13], "temperature": 0.0, "avg_logprob": -0.14235242207845053, "compression_ratio": 1.6433333333333333, "no_speech_prob": 6.851057150925044e-06}, {"id": 256, "seek": 89388, "start": 897.92, "end": 900.56, "text": " I value altruism, but that's not why I recommend it.", "tokens": [286, 2158, 4955, 894, 1434, 11, 457, 300, 311, 406, 983, 286, 2748, 309, 13], "temperature": 0.0, "avg_logprob": -0.14235242207845053, "compression_ratio": 1.6433333333333333, "no_speech_prob": 6.851057150925044e-06}, {"id": 257, "seek": 89388, "start": 900.56, "end": 902.8, "text": " I recommend it because it's a key part to", "tokens": [286, 2748, 309, 570, 309, 311, 257, 2141, 644, 281], "temperature": 0.0, "avg_logprob": -0.14235242207845053, "compression_ratio": 1.6433333333333333, "no_speech_prob": 6.851057150925044e-06}, {"id": 258, "seek": 89388, "start": 902.8, "end": 906.08, "text": " cementing your own understanding is to explain it.", "tokens": [19729, 278, 428, 1065, 3701, 307, 281, 2903, 309, 13], "temperature": 0.0, "avg_logprob": -0.14235242207845053, "compression_ratio": 1.6433333333333333, "no_speech_prob": 6.851057150925044e-06}, {"id": 259, "seek": 89388, "start": 906.08, "end": 908.6, "text": " Then I wanted to leave you with a few resources.", "tokens": [1396, 286, 1415, 281, 1856, 291, 365, 257, 1326, 3593, 13], "temperature": 0.0, "avg_logprob": -0.14235242207845053, "compression_ratio": 1.6433333333333333, "no_speech_prob": 6.851057150925044e-06}, {"id": 260, "seek": 89388, "start": 908.6, "end": 911.12, "text": " There's our free course in our forums,", "tokens": [821, 311, 527, 1737, 1164, 294, 527, 26998, 11], "temperature": 0.0, "avg_logprob": -0.14235242207845053, "compression_ratio": 1.6433333333333333, "no_speech_prob": 6.851057150925044e-06}, {"id": 261, "seek": 89388, "start": 911.12, "end": 913.24, "text": " the Learn Machine Learning subreddit.", "tokens": [264, 17216, 22155, 15205, 1422, 986, 17975, 13], "temperature": 0.0, "avg_logprob": -0.14235242207845053, "compression_ratio": 1.6433333333333333, "no_speech_prob": 6.851057150925044e-06}, {"id": 262, "seek": 89388, "start": 913.24, "end": 918.0, "text": " Some of my favorite deep learning blogs are Steven Marity, Chris Ola.", "tokens": [2188, 295, 452, 2954, 2452, 2539, 31038, 366, 12754, 2039, 507, 11, 6688, 422, 875, 13], "temperature": 0.0, "avg_logprob": -0.14235242207845053, "compression_ratio": 1.6433333333333333, "no_speech_prob": 6.851057150925044e-06}, {"id": 263, "seek": 89388, "start": 918.0, "end": 919.8, "text": " He doesn't update his old blog anymore,", "tokens": [634, 1177, 380, 5623, 702, 1331, 6968, 3602, 11], "temperature": 0.0, "avg_logprob": -0.14235242207845053, "compression_ratio": 1.6433333333333333, "no_speech_prob": 6.851057150925044e-06}, {"id": 264, "seek": 89388, "start": 919.8, "end": 921.88, "text": " but you should still read his old post.", "tokens": [457, 291, 820, 920, 1401, 702, 1331, 2183, 13], "temperature": 0.0, "avg_logprob": -0.14235242207845053, "compression_ratio": 1.6433333333333333, "no_speech_prob": 6.851057150925044e-06}, {"id": 265, "seek": 92188, "start": 921.88, "end": 924.4, "text": " He now spearheads distilled.pub.", "tokens": [634, 586, 26993, 29481, 1483, 6261, 13, 79, 836, 13], "temperature": 0.0, "avg_logprob": -0.19086415788768668, "compression_ratio": 1.5151515151515151, "no_speech_prob": 2.947802840935765e-06}, {"id": 266, "seek": 92188, "start": 924.4, "end": 928.0, "text": " Then Andre Carpathy, I think the Stanford CNN course,", "tokens": [1396, 20667, 2741, 79, 9527, 11, 286, 519, 264, 20374, 24859, 1164, 11], "temperature": 0.0, "avg_logprob": -0.19086415788768668, "compression_ratio": 1.5151515151515151, "no_speech_prob": 2.947802840935765e-06}, {"id": 267, "seek": 92188, "start": 928.0, "end": 930.92, "text": " which is freely available is also excellent.", "tokens": [597, 307, 16433, 2435, 307, 611, 7103, 13], "temperature": 0.0, "avg_logprob": -0.19086415788768668, "compression_ratio": 1.5151515151515151, "no_speech_prob": 2.947802840935765e-06}, {"id": 268, "seek": 92188, "start": 930.92, "end": 935.16, "text": " I'll tweet a link to these slides probably tomorrow.", "tokens": [286, 603, 15258, 257, 2113, 281, 613, 9788, 1391, 4153, 13], "temperature": 0.0, "avg_logprob": -0.19086415788768668, "compression_ratio": 1.5151515151515151, "no_speech_prob": 2.947802840935765e-06}, {"id": 269, "seek": 92188, "start": 935.16, "end": 938.48, "text": " There's a link to a blog post I wrote that links to", "tokens": [821, 311, 257, 2113, 281, 257, 6968, 2183, 286, 4114, 300, 6123, 281], "temperature": 0.0, "avg_logprob": -0.19086415788768668, "compression_ratio": 1.5151515151515151, "no_speech_prob": 2.947802840935765e-06}, {"id": 270, "seek": 92188, "start": 938.48, "end": 942.88, "text": " like 15 blog posts I like in the area of deep learning.", "tokens": [411, 2119, 6968, 12300, 286, 411, 294, 264, 1859, 295, 2452, 2539, 13], "temperature": 0.0, "avg_logprob": -0.19086415788768668, "compression_ratio": 1.5151515151515151, "no_speech_prob": 2.947802840935765e-06}, {"id": 271, "seek": 92188, "start": 942.88, "end": 945.2, "text": " Yes, I can take some questions now.", "tokens": [1079, 11, 286, 393, 747, 512, 1651, 586, 13], "temperature": 0.0, "avg_logprob": -0.19086415788768668, "compression_ratio": 1.5151515151515151, "no_speech_prob": 2.947802840935765e-06}, {"id": 272, "seek": 92188, "start": 945.2, "end": 950.52, "text": " Again, I am at math underscore Rachel on Twitter and I blog at fast.ai.", "tokens": [3764, 11, 286, 669, 412, 5221, 37556, 14246, 322, 5794, 293, 286, 6968, 412, 2370, 13, 1301, 13], "temperature": 0.0, "avg_logprob": -0.19086415788768668, "compression_ratio": 1.5151515151515151, "no_speech_prob": 2.947802840935765e-06}, {"id": 273, "seek": 95052, "start": 950.52, "end": 955.64, "text": " I also have an Ask Data Scientist advice column there. Thank you.", "tokens": [286, 611, 362, 364, 12320, 11888, 18944, 468, 5192, 7738, 456, 13, 1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.42268271195261103, "compression_ratio": 0.9154929577464789, "no_speech_prob": 1.3618703633255791e-05}, {"id": 274, "seek": 95564, "start": 955.64, "end": 982.68, "text": " That's a great question.", "tokens": [663, 311, 257, 869, 1168, 13], "temperature": 0.0, "avg_logprob": -0.3201648235321045, "compression_ratio": 0.75, "no_speech_prob": 0.00012498382420744747}, {"id": 275, "seek": 98268, "start": 982.68, "end": 986.16, "text": " The question was, this is very focused on deep learning.", "tokens": [440, 1168, 390, 11, 341, 307, 588, 5178, 322, 2452, 2539, 13], "temperature": 0.0, "avg_logprob": -0.16868757970124773, "compression_ratio": 1.664, "no_speech_prob": 8.737144526094198e-05}, {"id": 276, "seek": 98268, "start": 986.16, "end": 989.2399999999999, "text": " What do I think about other tools such as random forest?", "tokens": [708, 360, 286, 519, 466, 661, 3873, 1270, 382, 4974, 6719, 30], "temperature": 0.0, "avg_logprob": -0.16868757970124773, "compression_ratio": 1.664, "no_speech_prob": 8.737144526094198e-05}, {"id": 277, "seek": 98268, "start": 989.2399999999999, "end": 991.4, "text": " We actually in part two of our course,", "tokens": [492, 767, 294, 644, 732, 295, 527, 1164, 11], "temperature": 0.0, "avg_logprob": -0.16868757970124773, "compression_ratio": 1.664, "no_speech_prob": 8.737144526094198e-05}, {"id": 278, "seek": 98268, "start": 991.4, "end": 995.88, "text": " we use a neural net on a structured data set that you would much more", "tokens": [321, 764, 257, 18161, 2533, 322, 257, 18519, 1412, 992, 300, 291, 576, 709, 544], "temperature": 0.0, "avg_logprob": -0.16868757970124773, "compression_ratio": 1.664, "no_speech_prob": 8.737144526094198e-05}, {"id": 279, "seek": 98268, "start": 995.88, "end": 1001.4799999999999, "text": " traditionally see with a random forest or XGBoost.", "tokens": [19067, 536, 365, 257, 4974, 6719, 420, 1783, 8769, 78, 555, 13], "temperature": 0.0, "avg_logprob": -0.16868757970124773, "compression_ratio": 1.664, "no_speech_prob": 8.737144526094198e-05}, {"id": 280, "seek": 98268, "start": 1002.12, "end": 1005.92, "text": " I definitely think random forests are still worth learning about.", "tokens": [286, 2138, 519, 4974, 21700, 366, 920, 3163, 2539, 466, 13], "temperature": 0.0, "avg_logprob": -0.16868757970124773, "compression_ratio": 1.664, "no_speech_prob": 8.737144526094198e-05}, {"id": 281, "seek": 98268, "start": 1005.92, "end": 1009.8, "text": " I think it's possible in the future though that deep learning is going to be", "tokens": [286, 519, 309, 311, 1944, 294, 264, 2027, 1673, 300, 2452, 2539, 307, 516, 281, 312], "temperature": 0.0, "avg_logprob": -0.16868757970124773, "compression_ratio": 1.664, "no_speech_prob": 8.737144526094198e-05}, {"id": 282, "seek": 100980, "start": 1009.8, "end": 1016.1999999999999, "text": " used more and more on datasets that are still being solved with random forest today.", "tokens": [50364, 1143, 544, 293, 544, 322, 42856, 300, 366, 920, 885, 13041, 365, 4974, 6719, 965, 13, 50684], "temperature": 0.0, "avg_logprob": -0.1726294567710475, "compression_ratio": 1.12, "no_speech_prob": 0.0002729420375544578}, {"id": 283, "seek": 103980, "start": 1039.8, "end": 1059.12, "text": " I would recommend with not enough data,", "tokens": [286, 576, 2748, 365, 406, 1547, 1412, 11], "temperature": 0.0, "avg_logprob": -0.30577945709228516, "compression_ratio": 1.3728813559322033, "no_speech_prob": 0.00394287658855319}, {"id": 284, "seek": 103980, "start": 1059.12, "end": 1061.96, "text": " like I think a lot is being done with data augmentation", "tokens": [411, 286, 519, 257, 688, 307, 885, 1096, 365, 1412, 14501, 19631], "temperature": 0.0, "avg_logprob": -0.30577945709228516, "compression_ratio": 1.3728813559322033, "no_speech_prob": 0.00394287658855319}, {"id": 285, "seek": 106196, "start": 1061.96, "end": 1070.04, "text": " to combine with neural networks to try to deal with that use case.", "tokens": [281, 10432, 365, 18161, 9590, 281, 853, 281, 2028, 365, 300, 764, 1389, 13], "temperature": 0.0, "avg_logprob": -0.4013402817097116, "compression_ratio": 1.3676470588235294, "no_speech_prob": 5.825997141073458e-05}, {"id": 286, "seek": 106196, "start": 1072.52, "end": 1079.08, "text": " I'm thinking of just like flipping and random crops and applying color tents,", "tokens": [286, 478, 1953, 295, 445, 411, 26886, 293, 4974, 16829, 293, 9275, 2017, 39283, 11], "temperature": 0.0, "avg_logprob": -0.4013402817097116, "compression_ratio": 1.3676470588235294, "no_speech_prob": 5.825997141073458e-05}, {"id": 287, "seek": 106196, "start": 1079.08, "end": 1082.3600000000001, "text": " if you're using picture data but ways to.", "tokens": [498, 291, 434, 1228, 3036, 1412, 457, 2098, 281, 13], "temperature": 0.0, "avg_logprob": -0.4013402817097116, "compression_ratio": 1.3676470588235294, "no_speech_prob": 5.825997141073458e-05}, {"id": 288, "seek": 108236, "start": 1082.36, "end": 1096.4399999999998, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.3725106165959285, "compression_ratio": 1.2454545454545454, "no_speech_prob": 1.2405485904309899e-05}, {"id": 289, "seek": 108236, "start": 1100.6799999999998, "end": 1106.6, "text": " Yeah. I think there are a lot of techniques for augmenting the size of your dataset.", "tokens": [865, 13, 286, 519, 456, 366, 257, 688, 295, 7512, 337, 29919, 278, 264, 2744, 295, 428, 28872, 13], "temperature": 0.0, "avg_logprob": -0.3725106165959285, "compression_ratio": 1.2454545454545454, "no_speech_prob": 1.2405485904309899e-05}, {"id": 290, "seek": 108236, "start": 1106.6, "end": 1110.0, "text": " Then I think when you have imbalanced classes,", "tokens": [1396, 286, 519, 562, 291, 362, 566, 40251, 5359, 11], "temperature": 0.0, "avg_logprob": -0.3725106165959285, "compression_ratio": 1.2454545454545454, "no_speech_prob": 1.2405485904309899e-05}, {"id": 291, "seek": 111000, "start": 1110.0, "end": 1115.64, "text": " I mean you can do sampling where you're taking", "tokens": [286, 914, 291, 393, 360, 21179, 689, 291, 434, 1940], "temperature": 0.0, "avg_logprob": -0.2920026310154649, "compression_ratio": 1.41875, "no_speech_prob": 2.260175051560509e-06}, {"id": 292, "seek": 111000, "start": 1116.0, "end": 1124.12, "text": " more equal numbers from each class to try to counteract that. Yes.", "tokens": [544, 2681, 3547, 490, 1184, 1508, 281, 853, 281, 5682, 578, 300, 13, 1079, 13], "temperature": 0.0, "avg_logprob": -0.2920026310154649, "compression_ratio": 1.41875, "no_speech_prob": 2.260175051560509e-06}, {"id": 293, "seek": 111000, "start": 1130.16, "end": 1132.96, "text": " That's a good question.", "tokens": [663, 311, 257, 665, 1168, 13], "temperature": 0.0, "avg_logprob": -0.2920026310154649, "compression_ratio": 1.41875, "no_speech_prob": 2.260175051560509e-06}, {"id": 294, "seek": 111000, "start": 1132.96, "end": 1135.36, "text": " I think some of it maybe just like search for", "tokens": [286, 519, 512, 295, 309, 1310, 445, 411, 3164, 337], "temperature": 0.0, "avg_logprob": -0.2920026310154649, "compression_ratio": 1.41875, "no_speech_prob": 2.260175051560509e-06}, {"id": 295, "seek": 111000, "start": 1135.36, "end": 1138.24, "text": " an interesting dataset that appeals to you.", "tokens": [364, 1880, 28872, 300, 32603, 281, 291, 13], "temperature": 0.0, "avg_logprob": -0.2920026310154649, "compression_ratio": 1.41875, "no_speech_prob": 2.260175051560509e-06}, {"id": 296, "seek": 113824, "start": 1138.24, "end": 1141.44, "text": " So there are a lot. I mean Kaggle is always a great place to find", "tokens": [407, 456, 366, 257, 688, 13, 286, 914, 48751, 22631, 307, 1009, 257, 869, 1081, 281, 915], "temperature": 0.0, "avg_logprob": -0.22496542930603028, "compression_ratio": 1.5727699530516432, "no_speech_prob": 1.221587990585249e-05}, {"id": 297, "seek": 113824, "start": 1141.44, "end": 1145.4, "text": " interesting datasets or other resources out there.", "tokens": [1880, 42856, 420, 661, 3593, 484, 456, 13], "temperature": 0.0, "avg_logprob": -0.22496542930603028, "compression_ratio": 1.5727699530516432, "no_speech_prob": 1.221587990585249e-05}, {"id": 298, "seek": 113824, "start": 1145.4, "end": 1148.32, "text": " Then there are these organizations like Delta Analytics where you", "tokens": [1396, 456, 366, 613, 6150, 411, 18183, 25944, 689, 291], "temperature": 0.0, "avg_logprob": -0.22496542930603028, "compression_ratio": 1.5727699530516432, "no_speech_prob": 1.221587990585249e-05}, {"id": 299, "seek": 113824, "start": 1148.32, "end": 1152.44, "text": " can volunteer to partner with a non-profit.", "tokens": [393, 13835, 281, 4975, 365, 257, 2107, 12, 14583, 13], "temperature": 0.0, "avg_logprob": -0.22496542930603028, "compression_ratio": 1.5727699530516432, "no_speech_prob": 1.221587990585249e-05}, {"id": 300, "seek": 113824, "start": 1153.04, "end": 1155.96, "text": " Then I think another source might be like seeing if there are", "tokens": [1396, 286, 519, 1071, 4009, 1062, 312, 411, 2577, 498, 456, 366], "temperature": 0.0, "avg_logprob": -0.22496542930603028, "compression_ratio": 1.5727699530516432, "no_speech_prob": 1.221587990585249e-05}, {"id": 301, "seek": 115596, "start": 1155.96, "end": 1179.68, "text": " any particular papers that appeal to you. Yes.", "tokens": [604, 1729, 10577, 300, 13668, 281, 291, 13, 1079, 13], "temperature": 0.0, "avg_logprob": -0.3370656044252457, "compression_ratio": 1.092783505154639, "no_speech_prob": 1.7060427808246459e-06}, {"id": 302, "seek": 115596, "start": 1179.68, "end": 1184.44, "text": " I am. So Jeremy is actually the one who had the Excel idea,", "tokens": [286, 669, 13, 407, 17809, 307, 767, 264, 472, 567, 632, 264, 19060, 1558, 11], "temperature": 0.0, "avg_logprob": -0.3370656044252457, "compression_ratio": 1.092783505154639, "no_speech_prob": 1.7060427808246459e-06}, {"id": 303, "seek": 118444, "start": 1184.44, "end": 1186.2, "text": " although I love it. Yeah.", "tokens": [4878, 286, 959, 309, 13, 865, 13], "temperature": 0.0, "avg_logprob": -0.18121660672701323, "compression_ratio": 1.624413145539906, "no_speech_prob": 1.6958541891654022e-05}, {"id": 304, "seek": 118444, "start": 1186.2, "end": 1188.6000000000001, "text": " It's always good to use tools you're familiar with.", "tokens": [467, 311, 1009, 665, 281, 764, 3873, 291, 434, 4963, 365, 13], "temperature": 0.0, "avg_logprob": -0.18121660672701323, "compression_ratio": 1.624413145539906, "no_speech_prob": 1.6958541891654022e-05}, {"id": 305, "seek": 118444, "start": 1188.6000000000001, "end": 1191.16, "text": " I think though Excel is like", "tokens": [286, 519, 1673, 19060, 307, 411], "temperature": 0.0, "avg_logprob": -0.18121660672701323, "compression_ratio": 1.624413145539906, "no_speech_prob": 1.6958541891654022e-05}, {"id": 306, "seek": 118444, "start": 1191.16, "end": 1194.3600000000001, "text": " the world's most popular functional programming language.", "tokens": [264, 1002, 311, 881, 3743, 11745, 9410, 2856, 13], "temperature": 0.0, "avg_logprob": -0.18121660672701323, "compression_ratio": 1.624413145539906, "no_speech_prob": 1.6958541891654022e-05}, {"id": 307, "seek": 118444, "start": 1194.3600000000001, "end": 1196.8400000000001, "text": " There's a reason for that.", "tokens": [821, 311, 257, 1778, 337, 300, 13], "temperature": 0.0, "avg_logprob": -0.18121660672701323, "compression_ratio": 1.624413145539906, "no_speech_prob": 1.6958541891654022e-05}, {"id": 308, "seek": 118444, "start": 1196.8400000000001, "end": 1201.72, "text": " I think Excel is not hard to learn and there's some things that are very visual with it.", "tokens": [286, 519, 19060, 307, 406, 1152, 281, 1466, 293, 456, 311, 512, 721, 300, 366, 588, 5056, 365, 309, 13], "temperature": 0.0, "avg_logprob": -0.18121660672701323, "compression_ratio": 1.624413145539906, "no_speech_prob": 1.6958541891654022e-05}, {"id": 309, "seek": 120172, "start": 1201.72, "end": 1215.24, "text": " Although you're right, it is good to be familiar with your tools.", "tokens": [50364, 5780, 291, 434, 558, 11, 309, 307, 665, 281, 312, 4963, 365, 428, 3873, 13, 51040], "temperature": 0.0, "avg_logprob": -0.27519522772894967, "compression_ratio": 0.9558823529411765, "no_speech_prob": 3.1140869396040216e-05}], "language": "en"}