{"text": " computer. There we go. Yeah. Question. Yeah. It's a show. The training sort of process in fast AI, like is there a concept or capability to do like early stopping or best kind of thing? Or if there isn't, do I, is there a reason why you chose not to do that? I never remember because I don't use it myself. So what I would check and just checking now is the callbacks, which is under trainings. Let's go to the docs training callbacks. And if anybody else knows, please shout out. Early stopping callback. Yeah, I found it. Okay, it's under tracking callbacks. So if you go to the docs training callbacks tracker, there's an early stopping callback. So perhaps the more interesting part then is like, why do I not use it? So I don't even know whether it exists. There's a few reasons. One is that it, it doesn't play nicely with, you know, one cycle training or fine tuning. If you stop early, then the learning rate hasn't got a chance to go down. So and for that reason, it's almost never the case that earlier APOCs have better accuracy, because it's at learning rate hasn't hasn't settled down yet. If, if I was doing one cycle training, and I saw that an earlier APOC had a much better accuracy, then I would know that I'm overfitting, in which case I would be adding more data augmentation, rather than doing early stopping, because it's it's good to train for the amount of time that you have. So yeah, I can't think offhand of a situation where I would, I mean, I haven't come across a situation where I've personally wanted to use early stopping. So like in some of the training examples, like, where you had the error rate, like some of the prior runs may have had a better lower error rate. Oh, I mean, in the ones I've shown, like, a tiny bit better. Yeah, but like, not enough to be like, meaningful, you know. And yeah, so that there's no reason to believe that those would those are actually better models. And there's plenty of a priori reasons to believe that they're actually not, which is that the learning rate still hasn't settled down at that point. So we haven't let it fine tune into the best spot yet. So yeah, if it's kind of going down, down and down, it's kind of bottoming out and just bumps a little bit at the bottom. That's not a reason to use early stopping. It's also, I think, important to realize that the validation set is relatively small as well. So it's only a representation of, you know, of the distribution that the data is coming from. So reading too much into those small fluctuations can be very counterproductive. I know that I've wasted a lot of time in the past, you know, doing that. But yeah, a lot of time. We're looking for changes that dramatically improve things, you know, like changing from ResNet 2016 to ConvNext, we improved by what, four or 500%? It's like, okay, that's, that's an improvement. Over the weekend, I went on my own server that I have here behind me, that I have is a 10 ATI. And I run or like 35 models with, for the patty thing. And I was just, I didn't do the example, but I was thinking about these that when when I was taking algebra back in high school or college, you have some of these expressions that you have the function of X is equal to X squared for the X greater than something, and the absolute value of X when you have X equal to something. So it just got me my idea that the idea is that I maybe, maybe I think that the idea is that maybe some of the data set is going to fail the target value for every single one of the models that we tried, but if we try different, different models is going to be successful. So can we do that? I mean, of course we can, but I mean, what would be the easiest approach to say for this validation when X is equal to this or greater than that, this is the model to use, but then if this is all the other model, this is what you have to use. Yeah, I mean, you could do that, right? And like a really simple way to do that, which I've, I've seen used to some success on Kaggle is to train lots of models. And then to trade a gradient boosting machine whose inputs are those model predictions that whose output is the targets. And so that'll do exactly what you just described. It's very easy to overfit when you do that. And you're only going to get it if you've trained them well, you're only going to get a tiny increase, right? Because, because the neural net is very flexible, it shouldn't have that situation where this, this part of the space, it has bad predictions and this part of the space, it has good predictions, like it's, that's not really how neural nets work. If you had a variety of types of like different, totally different types of model, like a random forest, energy, BM and a neural net, I could see that maybe. But most of the time, one of those will be dramatically better than the other ones. And so like, I don't that often find myself wanting to ensemble across totally different types of model. So, I'd say it's another one of these things like early stopping, which like a lot of people waste huge amounts of time on, you know, and it's not really where the big benefits are going to be seen. But yeah, if you're like, in gold medal zone on a Kaggle competition, and you need another point 002%, or something, then these are all things you can certainly try at that point. And I think you can also... It kind of reminded me of AutoML, like the regime of tools. I don't know how you feel about how you feel about those things. Yeah, we talked about that last night's lesson actually, so you'll have to catch up to see what I said, if you haven't seen the lesson yet. Yeah. I'll mention also, reading Kaggle winners descriptions of their approaches is great. But you've got to be very careful. Because remember, the Kaggle winners are the people who did get that last 0.002%, you know, because like everybody found all the low hanging fruit, and the people who won, grabbed the really high hanging fruit. And so every time you win a Kaggle winners description, they almost always have complex ensembling methods. And that's why, you know, in like something like a big image recognition competition, it's very hard to win or probably impossible to win with a single model, unless you invent some amazing new architecture or something. And so you're kind of, you might get the impression then that ensembling is the big thing that gets you all the low hanging fruit, but it's not. ensembling is the thing which, or particularly complex ensembling is the thing that gets you that last fraction of a fraction of a percent. One more question. Yeah, of course. TTL concept, right? So I'm trying to understand conceptually why TTL improves the score. Because technically, when you're training, it is using those augmented sort of pictures and providing them providing a percentage number. But when you're kind of, when you run that TTL function, why is it able to predict that? Sure. So like, you know, sometimes you're like, looking at some like, I don't know, a screw head or a socket or something that's really small, and you can't quite see like what, how many pins are in it, or what type is it or whatever. And you're kind of like, look at it from different angles, and you're kind of like, put it up to the light and you try to like, at some point, you're like, okay, I see it. Right. And there's like some angle and some lighting that you can see it. That's what you're doing for the computer, you're giving it different angles, and you're giving it different lighting in the hope that in one of those, it's going to be really clear. And for the ones where it's easy, it's not going to make any difference, right? But for the ones where it's like, oh, I don't know if it's this disease or that disease, but oh, you know, when it's a bit brighter, and you kind of zoom into that section, like, oh, now I can see. And so when you then average them out, you know, all the other ones are all like, oh, I don't know which kind of is it, which kinds of it's like point five point five point five, and then this one is like point six. And so that's the one that in the average, it's going to end up picking. That's basically what happens. It also has another benefit, which is when we train our models, I don't know if you've noticed, but our training loss generally gets much, much lower than our validation loss. And sometimes they validate sometimes our well, so basically, like, what's happening there is that on the training set, the model is getting very confident, right? So even though we're using data augmentation, it's seeing slightly different versions of the same image dozens of times. And it's like, oh, I know how to recognize these. And so what it does is that the probabilities that associates with them is like point nine point nine nine, you know, like saying, I'm very confident of these. And it actually gets overconfident, which actually doesn't necessarily impact our accuracy, you know, to be overconfident. But at some point, it can. And so we are systematically going to have like overconfident predictions of probability. When even when it doesn't really know, just because it's really seen that kind of image before. So then on the validation set, it's going to be, you know, over picking probabilities as well. And so one nice benefit is that when you average out a few augmented versions, you know, it's like our point nine point nine probability is this one. And then on the next one, it's like, next augmented version of the same image, like, oh, no point one probability is that one. And they'll kind of average out to much more reasonable probabilities, which can, you know, allow it sometimes to combine these ideas into an average that that makes more sense. And so that can improve accuracy, but in particular, it improves the actual probabilities to get rid of that overconfidence. Is it fair to say that when you train without when you train, it's not able to separate the replicated sort of images or the distorted slightly where the variant of the original image, but when you use the TTA, it is able to group all the four images and that's what TTA is. We present them all together and average out that group. Yes. But in training, it's not able to average out that group. Yes. But in training, we don't indicate in any way that they're the same image, or that they're the same underlying object. One other question, Jeremy, we, I'm glad we touched on stumbling and how to pick the best on stumbling. I was gonna ask about that. But another question is, we have a fairly unbalanced data set, I guess, with the normal versus the disease states. Yeah. When you're doing augmentation, is there any benefit to sort of over over representing the minority classes? So let's, let's pull away augmentation. So it's actually got nothing to do with augmentation. So more generally, when you're training, does it make sense to over represent the minority class? And the answer is, maybe. Yeah, it can. Right. And so, okay, so just for those who aren't following, the issue Matt's talking about is that there was, you know, a couple of diseases which appear lots and lots in the data, and a couple which hardly appear at all. And so, you know, do we want to try to balance this data? You know, do we want to try to balance this out more? And one thing that people often do to balance it out more is that they'll throw away some of the images in their more represent the highly represented classes. And I can certainly tell you straight away, you should never ever do that. You never want to throw away data. But Matt's question was, well, could we, you know, oversample the less common diseases? And the answer is, yeah, absolutely, you could. And in fast AI, if you go into the docs. Now, where is it? There is a weighted data loader somewhere. Weighted. Search for that. Here we go. Of course, it's a callback. So if you go to the callbacks data section, you'll find a weighted DL callback or a weighted data loader's method. Are you sharing your screen? I'm not. No, I'm just telling you where to look. Thanks for checking. So, yeah, I mean, let's look at that today, right? Because I kind of want to look at like, things we can do to improve things today. It doesn't necessarily help. Because it does mean, you know, given that you're, you know, let's say you do 10 epochs of 1000 images, it's going to get to look at 10,000 images, right? And if you oversample a class, then that also means that it's going to get it's going to see less of some images and going to get more repetition of other images, which could be a problem, you know, and really it just depends on a few things. If it's like really unbalanced, like 99% all of one type, then you're going to have a whole lot of batches that it never sees anything of the underrepresented class. And so basically, there's nothing for it to learn from. So at some point, you probably certainly need weighted sampling. It also depends on the evaluation, you know, if people like say in the evaluation, okay, we're going to kind of average out for each disease, how accurate you were. So every disease will then be like equally weighted, then you would definitely need to use weighted sampling. But in this case, you know, presuming, presuming that the test set has a similar distribution as the training set, weighted sampling might not help. Because they're going to care the most about how well we do on the highly represented diseases. I'll note my experience with like over sampling and things like that. I think one time I had done with I think diabetic retinopathy, there was a competition for that. And I had used weighted sampling or over sampling and it did seem to help. And then also a while back, I did an experiment where I think this was back with FASAI version one, where I took like the minced data set, and then I, I, like, artificially added some sort of imbalance. And then I trained with and without weighted sampling. And I saw like there was an improvement with the weighted sampling on accuracy on like, just a regular missed validation set. So I, so from that, from those couple experiments, I'd say like, I've at least seen some help and improvement with weighted sampling. Cool. And was that cases where that data set was like, highly unbalanced? Or was it more like the data set that we're looking at, at the moment? It wasn't highly unbalanced. It was maybe like, I don't know, like, maybe like, yeah, just 75% versus 25% or something like that. It's not like 99, 99 versus 1%, nothing like that. It was more, it wasn't that bad. Let's try it today. Yeah. I see we've got a new face today as well. Hello, Zach. Thanks for joining. Hey, hey, glad I could finally make these. Yeah. Are you joining from Florida? No, I'm in Maryland now. Maryland now. Okay. That's quite a change. Yes. Much more up North. Okay. Great. So let's, let's try something. Okay. So let's connect to my little computer upstairs. Is there a way to shrink my zoom out of the way? It takes up so much space. Hide floating meeting controls. I guess that's what I want. Control alt shift H. Wow. Press escape to show floating meeting controls. That doesn't work very well with VIM. Oh, well, control alt shift H. Okay. All right. We're not doing tabular today. So let's get rid of that. So I think what I might do is, you know, cause we're iterating. Well, I guess we could start with the multitask one. Cause this is our kind of like things to try to improve version. So close that. I'll leave that open just in case we want it. Okay. By the way, if you've got multiple GPUs, this is how you just use one of them. You can just set an environment variable. Okay. So this is where we did the multi target model. Okay. Just moved everything slightly. Comp. Comp. Not comp path. Right. Back to where we were. Okay. So now what? What's broken? Data block. Get image files. Well, this is working the other day. So I guess we better try to do some debugging. So the obvious thing to do would be to call this thing here, get image files on the thing that we passed in here, which is train path. Okay. So that's working. Then the other thing to do would be to check our data by doing show batch. Okay. That's working. Then I guess, all right, and it's showing our two different things. That's good. Oh, is it? Oh, right. We've got the two category blocks. So we can't use this one. We have to use this one. So fit one cycle. Yeah, okay. So to remind you, we have this is the one where we had two categories and one input. And to get the two categories, we use the parent label and this function, which looked up the variety from this dictionary. Okay. And then when we fine-tuned it, and let's just check, yes, C equals 42. So that's our standard set. We should be able to then compare that to small models trained for 12 epochs. And then that was this one. Part two. And let's see. They're not quite the same because this was 480 squish, or else this was rectangular pad. Let's do five epochs. Let's do it the same as this one. Yeah, let's do this one because we want to be able to do quick iterations. Let's see. Resize 192 squish. There we go. And then we trained it for 0.01 with FP16. And five epochs. All right. So this will be our base case. Well, you know, I mean, I guess this is our base case. 0.045. This will be our next case. Okay. So while that's running, the next thing I wanted to talk about is progressive resizing. So this is training at a size of 128, which is not very big, and we wouldn't expect it to do very well. So, but it's certainly better than nothing. And as you can see, it's not error, disease error, it's down to 7.5% error already, and it's not even done. So that's not bad. And, you know, in the past, what we've then done is we've said, okay, well, that's working pretty well. Let's throw that away and try bigger. But there's actually something more interesting we can do, which is we don't have to throw it away. What we could do is to continue training it on larger images. So we're basically saying, okay, this is a model which is fine tuned to recognize 128 by 128 pixel images of rice. That's fine tune it to recognize 192 by 192 pixel images of rice. And we could even like, and like, there's a few benefits to that. One is like, it's very fast, you know, to do the smaller images. And it can recognize the key features of it. So, you know, this lets us do a lot of epochs quickly. And then like the difference between small images of rice disease and large images of rice disease isn't very big difference. So you would expect it would probably fine tune to bigger images of rice disease quite easily. So we might get it most of the benefit of training on big on big images, but without most of the time. The second benefit is it's a kind of data augmentation, which is we're actually giving it different sized images. So that should that should help. So here's how we would do that. Let's grab this data block. Let's make it into a function. Get DL. Okay, and the key thing I guess we're going to do. Well, let's just do the item transforms and the batch transforms as usual. Whoops. So so the things we're going to change are the item transforms and the batch transforms. And then we're going to return the data loader for that, which is here. Okay. So let's try going up a bit. DLs equals get DL. I guess it should be get DLs really because it returns data loaders get DLs. Okay, so let's see what we did last time. Let's be scaled up a bit. This is going to be data augmentation as well. We're going to change how we scale. So we'll scale with zero padding. And let's go up to 160. Okay. Okay. So then we need to learn. So our where's our squish one here? Squish. So the squish here got 0.45. Our multitask got 0.48. So it's actually a little bit worse. This might not be a great test actually, because I feel like one of the reasons that doing a multitask model might be useful is it might be able to train for more epochs. Because we're kind of giving it more signal. So we should probably revisit this with like 20 epochs. Any questions or comments about progressive resizing while we wait for this to train? Sorry, I can't see how you progressively change the size. I actually didn't. I messed it up. Whoops. Thank you. I have to do that again. I actually did it. Oh, and we need to get out to Yale's back as well. Okay, let's start again. Okay. And let's in case I mess this up again, let's export this. We'll call this like stage one. Yeah, so the problem was we created a new learner. So what we should have done is gone. Learn dot the L's equals the L's. That's actually so that would actually change the data loaders inside the learner without recreating it. Was that where you were heading with your comment? There was an unfreeze method, right? Like, I wouldn't be using that. Sorry. There was an unfreeze method like in the same way that we mentioned using the unfreeze method. There is an unfreeze method. Yes. What were you saying about the unfreeze method? Is an unfreeze required for progressive resizing? No, because fine tune has already unfrozen. Although I actually want to fine tune again. So if anything, I kind of actually want to actually want to refreeze it. Because we've changed the resolution. I think fine tuning the head might be a good idea to do again. Which line of code is doing the progressive resizing part? Just to be clear. It's not our line of code. It's basically this. It's basically saying our current learner is getting new data loaders. And the new data loaders have a size of 160, whereas the old data loaders had a size of 128. And our old data loaders did a pre-sizing of 192 squish and our new data loaders are doing a pre-sizing of rectangular padding. Does that make sense? Why are you calling it progressive in this case? Just are you going to keep changing the size or something like that? Yeah, it's changing the size of the images without resetting the learner. Just looked it up because I was curious. Fine tune calls a freeze first. I had a feeling it did. Thanks for checking, Zach. Right. So this time, let's see. It'll be interesting to see how it does. So after the initial epoch, it's got 0.09. Whereas previously it had 0.27. So obviously it's better than last time, but it's actually worse than the final point. This time it got all the way to 0.418. Whereas this time it has got worse. So it's got some work to do to learn to recognize what 160 pixel images look like. Can I just clarify, Jeremy? You're doing one more step in the progressive resizing here. It's not kind of an automated resizing. Correct. There isn't anything in Fast.ai to do this for you. And in fact, this technique is something that we invented. So it doesn't exist in other libraries at all. So yeah, it's the name of a technique. It's not the name of like a method in Fast.ai. And yeah, the technique is basically to replace the data loaders with ones at a larger size. And we invented it as part of a competition called Dawnbench, which is where we work very well on a competition for ImageNet training. And Google then took the idea and studied it a lot further as part of a paper called EfficientNet V2 and found ways to make it work even better. Oh my gosh, look at this. So we've gone from 0.418 to 0.418. To 0.0336. Have we done training at 160 before? I don't think we have. Oh, I should be checking this one. 128, 128. 171 by 128. No, we haven't. This is 256 by 192. So eventually, I guess we're going to get to that point. So let's keep going. So OK, so we're down to 2.9% error. How did you come up with the idea for this? Is it something that you just wanted to try? Or did you stumble upon it while looking at something else? Oh, I mean, it just seemed very obviously to me, like something which obviously we should do. Because we were spending, OK, so on Dawnbench, we were training on ImageNet. It was taking 12 hours, I guess, to train a single model. And the vast majority of that time, it's just recognizing very, very basic things about images. You know, it's not learning the finer details of different cat breeds or whatever, but it's just trying to understand about the concepts of like fur or sky or metal. And I thought, well, there's no, there's absolutely no reason to need 224 by 224 pixel images to be able to do that. You know? Like, it just seemed obviously stupid that we would do it. And partly it was like also like I was just generally interested in changing things during training. So what if, you know, in particular learning rates, right? So the idea of changing learning rates during training goes back a lot longer than Dawnbench, that people have been generally training them by having a learning rate that kind of dropped by a little bit. Learning rate that kind of dropped by a lot and then stayed flat and dropped by a lot and stayed flat. And Leslie Smith in particular came up with this idea of kind of like gradually increasing it over a curve and then gradually decreasing it following another curve. And so I was definitely in the mindset of like, oh, there's kind of interesting things we can change during training. So I was looking at like, oh, what if we change data augmentation during training, for example? Like maybe towards the end of training, we should like turn off data augmentation so it could learn what unaugmented images look like because that's what we really care about, for example. So yeah, that was the kind of stuff that I was kind of interested in at the time. And so yeah, definitely this thing of like, you know, why are we looking over 224 by 224 pixel images the entire time? Like that just seemed obviously stupid. And so it wasn't something where I was like, wow, here's a crazy idea. I bet it won't work. As soon as I thought of it, I just thought, okay, this is definitely going to work. You know, that it did. Interesting. Thanks. One question I have for you, Jeremy. There was a paper that came out like in 2019 called fixing the test train resolution discrepancy. Yeah, were they like trained on 224 and then did inference finally on like 320 by 320? Yeah. Have you seen that still sort of work? Have you done that at all in your workflow? I mean, honestly, I don't remember. I need to revisit that paper because you're right. It's important tonight. You know, I would generally try to fine tune on the final size I was going to be predicting on anyway. So yeah, I guess we'll kind of see how we go with this, right? I mean, you can definitely take a model that was trained on 224 by 224 images and use it to predict 360 by 360 images. And it will generally go pretty well. But I think it'll go better if you first fine tune it on 360 by 360 images. Yeah, I don't think they tried pre-training and then also training on like 320 versus just 320 and the 224. Yeah. That would definitely be an interesting experiment. Yeah, it would be an interesting experiment. And it's definitely something that any of us here could do. You know, I think it'd be cool. Right. So let's try scaling this up. So we can change these two lines to one. And so this is one something I often do is I do things like, yep. I think we don't have your screen. OK. So as I was saying, previously, I had like two cells to do this. And so now I'm just going to combine it into one cell. This is what I tend to do as I fiddle around as I try to like gradually make things a little bit more concise, you know. OK. Does it make sense to go smaller than the size of the original pre-training like ConvNet? ConvNext? Yeah. I mean, you can fine tune to any size. But I think it's a little bit more complicated. So I think it's a little bit more complicated. But yeah, I mean, you can fine tune to any size you like. Absolutely. I'm just going to get rid of the zero padding because again, I want to like try to change it a little bit each time just to kind of, you know, it's a kind of augmentation, right? So OK. So it's got to 192. You know, one thing I find encouraging is that, you know, my training loss isn't getting way underneath the validation loss. It's not like we're... It feels like we could do this for ages before our error rates start going up. Interestingly, when I re-ran this, my error rate was much better, 0.418. You've got a good memory to remember these old papers, Zach. It's very helpful to be able to do that. Usually what I wind up doing is my dad and I will email back and forth papers to each other so I can just go through my sent look at archive. And usually if I don't remember the name of it, I remember the subject of it in some degree. Yeah, I can just go through it all. I mean, it's a very, very good idea to use a paper manager of some sort to save papers, you know, whether it be Mendeley or the Noto or Archive Sanity or whatever or Bookmarks or something. Yeah, because otherwise these things disappear. Personally, I just tend to like tweet or favorite tweets about papers I'm interested in. And then I've set up pinboard.in. I don't know if you guys have seen that, but it's a really nice little thing, which basically anytime you're on a website, you can click a button and the extension and it adds it to pinboard, but it also automatically adds all of your tweets and favorites. And it's got a full text search of the thing that the URLs link to, which is very helpful. See, you favorited something that just says, oh, shit. No, I actually wrote something that just said, oh, shit. That was me writing, oh, shit. It was this, I mean, totally off topic, but this absolutely disaster. I hope it's wrong, but this absolutely disastrous sounding paper that came out yesterday, that basically, where was this key thing? People who've had one COVID infection have a list of one sick rely of 8.4%, two infections 23%, three infections 36%. It's like my worst nightmare is like the more people get infected with COVID, the more likely it is that they'll get long-term symptoms, which is horrifying. That was my most shit moment. It was very horrifying. It's really awful. Okay. So it keeps going down, right? Which is cool. Let's keep bringing it along, I suppose. I guess, you know, what we could do is just grab this whole damn thing here. We kind of have a bit of a comparison. So we're basically going to run exactly the same thing we did earlier. About this time with some pre-sizing first. All right. So that'll be an interesting experiment. So while that's running, you know, this is where I hit the old duplicate button. And this is why it's nice if you can to have the second card, because while something's running, you can try something else. CUDA, visible devices. There we go. So we can keep working. Okay. So, um, way to data loader. So this is something I added to fast AI a while ago and haven't used much myself since. But if I just search for weighted, here it is. Here it is. So you can see in the docs, it shows you exactly how to use weighted data loaders. And so, um, we pass in a batch size. We pass in some weights as this is the weights. There's going to be one, two, three, four, five, six, seven, eight, or actually zero, one, two, three, four, five, six, seven. And then some item transforms. So like, these are kind of really, really interesting in the docs. In some ways, it's extremely advanced and it's other ways, it's extremely simple, which is to say, if you look at this example in the docs, everything is totally manual. Right. So our labels are some random integers, kind of them. And I've even added a comment here, right? Eight is going to be in the training set. Two are going to be in the validation set. So our data block is going to contain one category block because we've just got the one thing, right? And rather than doing get X and get Y, you can also just say getters because get X and get Y basically become getters, which is a list of transformations to do. And so this is going to be a single getter or a single get X, if you like, which is going to return the I label and a splitter, which is going to decide whether something's valid or not. And then the other category is going to be the label and a splitter, which is going to decide whether something's valid or not based on this function. So you can see this whole thing is like totally manual, you know, so we can create our data set by passing in a list of the numbers from not nine and a single item transform that's going to convert that to a tensor. And then our weights will be the numbers from not to seven. And so then we can take our data set or data sets and turn them into data loaders using those weights. So with a batch size of one, if we say show batch, we get back a single number, okay? And it's not doing random shuffling, so we get the number zero because that was the first thing in our data set. Let's see, what do we do next? Now we've got to do n equals 160. So now we've got all of the numbers from not to 159. Yes, for getters, yep. You mentioned, so this is for x or y? This is a list, it's whatever, right? So there is just one thing, I don't know if you call that x or you call it y, it's just one thing. So if you have a get x and a get y, that's the same as having a getters with a list of two things. Okay. So yeah, I think I could just write getter, this has been ages since I've done this, but I think I could just write get x here and put this not in a list, would probably be the same thing. Okay. Probably handle a little bit of mystery that might be happening as well. The data block has an input parameter, which is how it determines what of the getters is x versus y. Correct, which we actually looked at last time here when we created our multi-image block. But that was before you joined, Zach. But yes, useful reminder. Okay, so here we see in a histogram of how often, so our, we created like a little synthetic learner that doesn't really do anything, but we can pass callbacks to it. And there's a callback called collect data callback, which just collects the data that's part, that is called in the learner. And so this is how we can then find out what data was passed to the learner, get a histogram of it, and we can see that, yep, the number 160 was received a lot more often when we trained this learner, which is what you would expect. This is the source of the weighted data loader class here. And as you can see, other than the boilerplate, it's one, two, three, four, five lines of code. And then the weighted data loader's method is one, two lines of code. So there's actually a lot more lines of example than there is of actual code. So often it's easier just to read the source code because, you know, thanks to the very layered approach to Fast.ai, we can do so much stuff with so little code. And so in this case, if we look through the code, we're passing in some weights. And basically the key thing here is that we set, if you pass in no weights at all, then we're just going to set it equal to the number one repeated n times. So everything's going to get one, a weight of one. And then we divide the weights by the sum of the weights so that the sum of the weights ends up summing up to one, which is what we want. And then, if you're not shuffling, then there's no weighted anything to do. So we just pass back the indexes. And if we are shuffling, we will grab a random choice of based on the weights. So that's the way we do it. Based on the based on the weights. Cool. All right. So there's going to be one weight per row. All right. Let's come back to that because I want to see how our thing's gone. It looks like it's finished. Notice that the fav icon in Jupyter will change depending on whether something's running or not. So that's how you can quickly tell if something's finished. 0.216. 0.221. Okay. I mean, it's not a huge difference, but maybe it's a tiny bit better. I don't know. Like it's... Let's... To... The key thing though is this lets us use our resources better. Right? So we often will end up with a better answer, but you can train for a lot less time. In fact, you can see that the error was at 0.216 back here. So, you know, we could probably have trained for a lot less epochs. So that's progressive resizing. Is there a way to look at that and go, oh, actually I'd like to take the outputs from epoch nine because it had a better... That was the question we got earlier about that's called early stopping. And the answer is no, you probably wouldn't want to do early stopping. But you can't go back to a previous like epoch. There's no history. You can. You have to use the early stopping callback to do that. All right. Cool. Okay. Yeah. Okay. I'll look at that. Or there's other things you can use. As I say, I don't think you should, but you can. If I go training callbacks, tracker. Okay. So the other part of that is, yeah, is it counterproductive or... Yeah, it's kind of a cheat if it works, but not if it doesn't. It's probably not a good idea. Probably will make it worse. Yeah. Okay. Great. So the other thing you can do is to save model callback, which saves, which is kind of like early stopping, but it doesn't stop. It saves the parameters of the best model during training, which is probably what you want instead of early stopping. But I don't think you should do that either. For the same reason we discussed earlier. Why shouldn't you do this? It seems like you could just ignore it if you didn't want it or like it might not hurt you. Well, so this actually automatically loads the best set of parameters at the end. And you're just going to end up with this kind of like model that just so happened to look a tiny bit better on the validation set at an earlier epoch. But at that earlier epoch, the loading rate hadn't yet stabilized and it's very unlikely it really is better. So you've probably actually just picked something that's slightly worse and made your process slightly more complicated for no good reason. Being better on an epoch there doesn't necessarily say anything about the final hidden test set. Yeah, we have a strong prior belief that it will improve each epoch unless you're overfitting. And if you're overfitting then you shouldn't be doing early stopping. You should be doing more augmentation. It seems like a good opportunity for somebody to document the arguments because I'm like curious what add-in does. Yes, that would be a great opportunity for somebody to document the arguments. And if somebody is interested in doing that, we have a really cool thing called documents which I only invented after we created Fast AI. Oh, this is like, that's not, I should delete this because this is the old version, part of Fast Core. And documents, you document each parameter by putting a comment after it and you document the return by putting a comment after it. And Zach actually started a project to, after I created documents, to add documents comments to everything in Fast AI, which of course is not finished because Fast AI is pretty big. And so here's an example of something that doesn't yet have documents comments. So if somebody wants to go and add a comment to each of these things and put that into a PR, then that will end up in the documentation. I highly recommend that anyone who wants to do that to do it. I was just going to say Zach, something we should do is to add a comment and then we can add a comment. Something we should do Zach is to actually include an example in the documents documentation of what it ends up looking like in NB dev. Because I can see that's missing. That might be a good idea. I can see if I can get on that tomorrow. Yeah. Sorry, Hamel, what were you saying? No, I just wanted to encourage everybody that like writing the documentation and learning deeply how everything works. And what ends up happening is you write this documentation and somebody like Jeremy will review it carefully and let you know what you don't understand. And that's how I learned about some other Fast AI libraries. So I highly recommend it going and doing that. And here's what it ends up looking like. So this is the source of optimizer and you can see it's got a little table underneath. And if we look at the source of optimizer, you'll see that each parameter has a comment next to it. So those parameters are automatically turned into this table. All right. Documents are super cool. They are super cool. This sounds like a good place to wrap up. Any questions or comments or anything before we wrap up? I have a question regarding to progressive resizing. Yes. We didn't do actually LR find after each step. Don't you think it's something helpful? The LR find, did you say? Yeah. Yeah. Yeah, to be honest, I don't use LR find much anymore nowadays because at least for object recognition in computer vision, the optimal learning rate is pretty much always the same. It's always around 0.08, 0.01. Yeah, there's no reason to believe that we have any need to change it just because we changed the resolution. So yeah, I wouldn't bother. Just leave it where it was. Jeremy, if your training and validation loss is still decreasing after 12 people, can you pick up and train for a little longer without restarting? You can. The first thing I'll say is you shouldn't be looking at the validation loss to see if you're overfitting. You should be looking at the error rate. So the validation loss can get worse whilst the error rate gets better. And that doesn't count as overfitting because the thing you want is to improve is the error rate. That can happen if it gets overconfident, but it's still improving. Yeah, you can keep training for longer because we're using, if you're using fit1 cycle or fine tune and fine tune uses fit1 cycle behind the scenes. Continuing to train further, your learning rate is going to go up and then down and then up and down each time, which is not necessarily a bad thing. But if you basically want to keep training at that point, you would probably want to decrease the learning rate by maybe 4x or so. And in fact, I think after this, I'm going to rerun this whole notebook. But half the learning rate each time, because I think that would be potentially a good idea. I have a question. I don't know if it's too late, but I think it might be useful to discuss when you do the progressive resizing, what part of the model gets dropped? Is there some part of the model that needs to be reinitialized? No, nothing needs to be reinitialized. No. I found this on the web. Who found what on the web? I thought you were talking to me, but you're talking to Siri. I'm offended. Siri, teach me deep learning. Yeah, ConfNext is what we call a resolution-independent architecture, which means it works for any input resolution. And time permitting in the next lesson, we will see how convolutional neural networks actually work, but I guess a lot of you probably already know. So for those of you that do, if you think you can do it, it's basically going patch by patch and doing this kind of mini matrix multiplier for each patch. So if you change the input resolution, it just has more patches to cover, but it doesn't change the parameters at all. So there's nothing to reinitialize. Does that make sense, Hamill? Yeah, that makes sense. I was just going to say that, I think, is ResNet resolution-independent? Basically, everything we use is normally, but in the, like, have a look at that, like, best fine-tuning, and then we can do the same thing. So that's the way it works. So I think that's a good point. I think that's a good point. I think that's a good point. But in the, like, have a look at that, like, best fine-tuning models notebook, and you'll see that two of the best ones are called VIT and SWIN, and also SWINv2. None of those are resolution-independent. Although there is a trick you can use to kind of make them resolution-independent, which we should try out in a future walkthrough. Is that fiddling with the head? Or something? Oh, there's a Tim. There's a thing you can pass to Tim. I don't know if we can use it to support progressive resizing or not. It'll be interesting to experiment with. It's basically changing the positional encodings. I have a question. Interesting. Yeah. After you've done your experiment, progressive resizing and fine-tuning, how do you, in Fast.AI, train with the whole training set? I never got around to do that. Do you skate a dummy? I almost never do. Like, instead, I do what we saw in the last walkthrough, which is I just train on a few different randomly selected valid training sets. Because that way, you know, you get the benefit on sombering. You're going to end up seeing all the images, at least one anyway. And you can also kind of see if something's messed up because you've still got a validation set each time. So, yeah, I used to, like, do this thing where I would create a validation set with a single item in to, like, get that last bit of juice. But I don't, I don't know. I don't even do that anymore. Okay, thanks. No worries. All right, gang. Enjoy the rest of your day slash evening. Nice to see you all. Bye. Goodbye. Thanks. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 3.2800000000000002, "text": " computer. There we go. Yeah.", "tokens": [3820, 13, 821, 321, 352, 13, 865, 13], "temperature": 0.0, "avg_logprob": -0.29876887798309326, "compression_ratio": 1.5188679245283019, "no_speech_prob": 0.02593192830681801}, {"id": 1, "seek": 0, "start": 4.88, "end": 6.44, "text": " Question. Yeah.", "tokens": [14464, 13, 865, 13], "temperature": 0.0, "avg_logprob": -0.29876887798309326, "compression_ratio": 1.5188679245283019, "no_speech_prob": 0.02593192830681801}, {"id": 2, "seek": 0, "start": 6.76, "end": 12.120000000000001, "text": " It's a show. The training sort of process in fast AI, like is", "tokens": [467, 311, 257, 855, 13, 440, 3097, 1333, 295, 1399, 294, 2370, 7318, 11, 411, 307], "temperature": 0.0, "avg_logprob": -0.29876887798309326, "compression_ratio": 1.5188679245283019, "no_speech_prob": 0.02593192830681801}, {"id": 3, "seek": 0, "start": 12.120000000000001, "end": 17.52, "text": " there a concept or capability to do like early stopping or best", "tokens": [456, 257, 3410, 420, 13759, 281, 360, 411, 2440, 12767, 420, 1151], "temperature": 0.0, "avg_logprob": -0.29876887798309326, "compression_ratio": 1.5188679245283019, "no_speech_prob": 0.02593192830681801}, {"id": 4, "seek": 0, "start": 17.84, "end": 20.76, "text": " kind of thing? Or if there isn't, do I, is there a reason", "tokens": [733, 295, 551, 30, 1610, 498, 456, 1943, 380, 11, 360, 286, 11, 307, 456, 257, 1778], "temperature": 0.0, "avg_logprob": -0.29876887798309326, "compression_ratio": 1.5188679245283019, "no_speech_prob": 0.02593192830681801}, {"id": 5, "seek": 0, "start": 20.76, "end": 22.56, "text": " why you chose not to do that?", "tokens": [983, 291, 5111, 406, 281, 360, 300, 30], "temperature": 0.0, "avg_logprob": -0.29876887798309326, "compression_ratio": 1.5188679245283019, "no_speech_prob": 0.02593192830681801}, {"id": 6, "seek": 0, "start": 25.32, "end": 29.16, "text": " I never remember because I don't use it myself. So what I would", "tokens": [286, 1128, 1604, 570, 286, 500, 380, 764, 309, 2059, 13, 407, 437, 286, 576], "temperature": 0.0, "avg_logprob": -0.29876887798309326, "compression_ratio": 1.5188679245283019, "no_speech_prob": 0.02593192830681801}, {"id": 7, "seek": 2916, "start": 29.16, "end": 33.28, "text": " check and just checking now is the callbacks, which is under", "tokens": [1520, 293, 445, 8568, 586, 307, 264, 818, 17758, 11, 597, 307, 833], "temperature": 0.0, "avg_logprob": -0.19925753275553384, "compression_ratio": 1.5, "no_speech_prob": 6.708029832225293e-05}, {"id": 8, "seek": 2916, "start": 33.28, "end": 36.64, "text": " trainings. Let's go to the docs training callbacks.", "tokens": [33856, 13, 961, 311, 352, 281, 264, 45623, 3097, 818, 17758, 13], "temperature": 0.0, "avg_logprob": -0.19925753275553384, "compression_ratio": 1.5, "no_speech_prob": 6.708029832225293e-05}, {"id": 9, "seek": 2916, "start": 44.92, "end": 47.16, "text": " And if anybody else knows, please shout out.", "tokens": [400, 498, 4472, 1646, 3255, 11, 1767, 8043, 484, 13], "temperature": 0.0, "avg_logprob": -0.19925753275553384, "compression_ratio": 1.5, "no_speech_prob": 6.708029832225293e-05}, {"id": 10, "seek": 2916, "start": 52.04, "end": 53.28, "text": " Early stopping callback.", "tokens": [18344, 12767, 818, 3207, 13], "temperature": 0.0, "avg_logprob": -0.19925753275553384, "compression_ratio": 1.5, "no_speech_prob": 6.708029832225293e-05}, {"id": 11, "seek": 2916, "start": 53.32, "end": 56.28, "text": " Yeah, I found it. Okay, it's under tracking callbacks. So if", "tokens": [865, 11, 286, 1352, 309, 13, 1033, 11, 309, 311, 833, 11603, 818, 17758, 13, 407, 498], "temperature": 0.0, "avg_logprob": -0.19925753275553384, "compression_ratio": 1.5, "no_speech_prob": 6.708029832225293e-05}, {"id": 12, "seek": 5628, "start": 56.28, "end": 59.84, "text": " you go to the docs training callbacks tracker, there's an", "tokens": [291, 352, 281, 264, 45623, 3097, 818, 17758, 37516, 11, 456, 311, 364], "temperature": 0.0, "avg_logprob": -0.11210753312751429, "compression_ratio": 1.4567901234567902, "no_speech_prob": 2.2471545889857225e-05}, {"id": 13, "seek": 5628, "start": 59.84, "end": 64.84, "text": " early stopping callback. So perhaps the more interesting part", "tokens": [2440, 12767, 818, 3207, 13, 407, 4317, 264, 544, 1880, 644], "temperature": 0.0, "avg_logprob": -0.11210753312751429, "compression_ratio": 1.4567901234567902, "no_speech_prob": 2.2471545889857225e-05}, {"id": 14, "seek": 5628, "start": 64.84, "end": 68.68, "text": " then is like, why do I not use it? So I don't even know whether", "tokens": [550, 307, 411, 11, 983, 360, 286, 406, 764, 309, 30, 407, 286, 500, 380, 754, 458, 1968], "temperature": 0.0, "avg_logprob": -0.11210753312751429, "compression_ratio": 1.4567901234567902, "no_speech_prob": 2.2471545889857225e-05}, {"id": 15, "seek": 5628, "start": 68.68, "end": 78.8, "text": " it exists. There's a few reasons. One is that it, it", "tokens": [309, 8198, 13, 821, 311, 257, 1326, 4112, 13, 1485, 307, 300, 309, 11, 309], "temperature": 0.0, "avg_logprob": -0.11210753312751429, "compression_ratio": 1.4567901234567902, "no_speech_prob": 2.2471545889857225e-05}, {"id": 16, "seek": 7880, "start": 78.8, "end": 87.32, "text": " doesn't play nicely with, you know, one cycle training or", "tokens": [1177, 380, 862, 9594, 365, 11, 291, 458, 11, 472, 6586, 3097, 420], "temperature": 0.0, "avg_logprob": -0.10594624739426833, "compression_ratio": 1.4131736526946108, "no_speech_prob": 4.784698830917478e-06}, {"id": 17, "seek": 7880, "start": 87.32, "end": 92.12, "text": " fine tuning. If you stop early, then the learning rate hasn't", "tokens": [2489, 15164, 13, 759, 291, 1590, 2440, 11, 550, 264, 2539, 3314, 6132, 380], "temperature": 0.0, "avg_logprob": -0.10594624739426833, "compression_ratio": 1.4131736526946108, "no_speech_prob": 4.784698830917478e-06}, {"id": 18, "seek": 7880, "start": 92.12, "end": 97.75999999999999, "text": " got a chance to go down. So and for that reason, it's almost", "tokens": [658, 257, 2931, 281, 352, 760, 13, 407, 293, 337, 300, 1778, 11, 309, 311, 1920], "temperature": 0.0, "avg_logprob": -0.10594624739426833, "compression_ratio": 1.4131736526946108, "no_speech_prob": 4.784698830917478e-06}, {"id": 19, "seek": 7880, "start": 97.75999999999999, "end": 106.75999999999999, "text": " never the case that earlier APOCs have better accuracy,", "tokens": [1128, 264, 1389, 300, 3071, 5372, 30087, 82, 362, 1101, 14170, 11], "temperature": 0.0, "avg_logprob": -0.10594624739426833, "compression_ratio": 1.4131736526946108, "no_speech_prob": 4.784698830917478e-06}, {"id": 20, "seek": 10676, "start": 106.76, "end": 109.88000000000001, "text": " because it's at learning rate hasn't hasn't settled down yet.", "tokens": [570, 309, 311, 412, 2539, 3314, 6132, 380, 6132, 380, 14819, 760, 1939, 13], "temperature": 0.0, "avg_logprob": -0.23945947126908737, "compression_ratio": 1.5807860262008733, "no_speech_prob": 3.9049873521435075e-06}, {"id": 21, "seek": 10676, "start": 110.4, "end": 115.76, "text": " If, if I was doing one cycle training, and I saw that an", "tokens": [759, 11, 498, 286, 390, 884, 472, 6586, 3097, 11, 293, 286, 1866, 300, 364], "temperature": 0.0, "avg_logprob": -0.23945947126908737, "compression_ratio": 1.5807860262008733, "no_speech_prob": 3.9049873521435075e-06}, {"id": 22, "seek": 10676, "start": 115.76, "end": 120.68, "text": " earlier APOC had a much better accuracy, then I would know that", "tokens": [3071, 5372, 30087, 632, 257, 709, 1101, 14170, 11, 550, 286, 576, 458, 300], "temperature": 0.0, "avg_logprob": -0.23945947126908737, "compression_ratio": 1.5807860262008733, "no_speech_prob": 3.9049873521435075e-06}, {"id": 23, "seek": 10676, "start": 121.2, "end": 124.76, "text": " I'm overfitting, in which case I would be adding more data", "tokens": [286, 478, 670, 69, 2414, 11, 294, 597, 1389, 286, 576, 312, 5127, 544, 1412], "temperature": 0.0, "avg_logprob": -0.23945947126908737, "compression_ratio": 1.5807860262008733, "no_speech_prob": 3.9049873521435075e-06}, {"id": 24, "seek": 10676, "start": 124.76, "end": 129.12, "text": " augmentation, rather than doing early stopping, because it's", "tokens": [14501, 19631, 11, 2831, 813, 884, 2440, 12767, 11, 570, 309, 311], "temperature": 0.0, "avg_logprob": -0.23945947126908737, "compression_ratio": 1.5807860262008733, "no_speech_prob": 3.9049873521435075e-06}, {"id": 25, "seek": 10676, "start": 129.28, "end": 134.04000000000002, "text": " it's good to train for the amount of time that you have. So", "tokens": [309, 311, 665, 281, 3847, 337, 264, 2372, 295, 565, 300, 291, 362, 13, 407], "temperature": 0.0, "avg_logprob": -0.23945947126908737, "compression_ratio": 1.5807860262008733, "no_speech_prob": 3.9049873521435075e-06}, {"id": 26, "seek": 13404, "start": 134.04, "end": 138.0, "text": " yeah, I can't think offhand of a situation where I would, I", "tokens": [1338, 11, 286, 393, 380, 519, 766, 5543, 295, 257, 2590, 689, 286, 576, 11, 286], "temperature": 0.0, "avg_logprob": -0.28755147248795887, "compression_ratio": 1.7047619047619047, "no_speech_prob": 8.939067811297718e-06}, {"id": 27, "seek": 13404, "start": 138.0, "end": 140.4, "text": " mean, I haven't come across a situation where I've personally", "tokens": [914, 11, 286, 2378, 380, 808, 2108, 257, 2590, 689, 286, 600, 5665], "temperature": 0.0, "avg_logprob": -0.28755147248795887, "compression_ratio": 1.7047619047619047, "no_speech_prob": 8.939067811297718e-06}, {"id": 28, "seek": 13404, "start": 140.4, "end": 141.68, "text": " wanted to use early stopping.", "tokens": [1415, 281, 764, 2440, 12767, 13], "temperature": 0.0, "avg_logprob": -0.28755147248795887, "compression_ratio": 1.7047619047619047, "no_speech_prob": 8.939067811297718e-06}, {"id": 29, "seek": 13404, "start": 144.72, "end": 148.76, "text": " So like in some of the training examples, like, where you had", "tokens": [407, 411, 294, 512, 295, 264, 3097, 5110, 11, 411, 11, 689, 291, 632], "temperature": 0.0, "avg_logprob": -0.28755147248795887, "compression_ratio": 1.7047619047619047, "no_speech_prob": 8.939067811297718e-06}, {"id": 30, "seek": 13404, "start": 148.79999999999998, "end": 153.84, "text": " the error rate, like some of the prior runs may have had a better", "tokens": [264, 6713, 3314, 11, 411, 512, 295, 264, 4059, 6676, 815, 362, 632, 257, 1101], "temperature": 0.0, "avg_logprob": -0.28755147248795887, "compression_ratio": 1.7047619047619047, "no_speech_prob": 8.939067811297718e-06}, {"id": 31, "seek": 13404, "start": 155.04, "end": 156.2, "text": " lower error rate.", "tokens": [3126, 6713, 3314, 13], "temperature": 0.0, "avg_logprob": -0.28755147248795887, "compression_ratio": 1.7047619047619047, "no_speech_prob": 8.939067811297718e-06}, {"id": 32, "seek": 13404, "start": 156.23999999999998, "end": 159.48, "text": " Oh, I mean, in the ones I've shown, like, a tiny bit better.", "tokens": [876, 11, 286, 914, 11, 294, 264, 2306, 286, 600, 4898, 11, 411, 11, 257, 5870, 857, 1101, 13], "temperature": 0.0, "avg_logprob": -0.28755147248795887, "compression_ratio": 1.7047619047619047, "no_speech_prob": 8.939067811297718e-06}, {"id": 33, "seek": 15948, "start": 159.48, "end": 167.76, "text": " Yeah, but like, not enough to be like, meaningful, you know. And", "tokens": [865, 11, 457, 411, 11, 406, 1547, 281, 312, 411, 11, 10995, 11, 291, 458, 13, 400], "temperature": 0.0, "avg_logprob": -0.21942514242585173, "compression_ratio": 1.7023255813953488, "no_speech_prob": 9.367680831928737e-06}, {"id": 34, "seek": 15948, "start": 168.92, "end": 172.56, "text": " yeah, so that there's no reason to believe that those would", "tokens": [1338, 11, 370, 300, 456, 311, 572, 1778, 281, 1697, 300, 729, 576], "temperature": 0.0, "avg_logprob": -0.21942514242585173, "compression_ratio": 1.7023255813953488, "no_speech_prob": 9.367680831928737e-06}, {"id": 35, "seek": 15948, "start": 172.6, "end": 174.6, "text": " those are actually better models. And there's plenty of", "tokens": [729, 366, 767, 1101, 5245, 13, 400, 456, 311, 7140, 295], "temperature": 0.0, "avg_logprob": -0.21942514242585173, "compression_ratio": 1.7023255813953488, "no_speech_prob": 9.367680831928737e-06}, {"id": 36, "seek": 15948, "start": 174.6, "end": 177.35999999999999, "text": " a priori reasons to believe that they're actually not, which is", "tokens": [257, 4059, 72, 4112, 281, 1697, 300, 436, 434, 767, 406, 11, 597, 307], "temperature": 0.0, "avg_logprob": -0.21942514242585173, "compression_ratio": 1.7023255813953488, "no_speech_prob": 9.367680831928737e-06}, {"id": 37, "seek": 15948, "start": 177.35999999999999, "end": 179.28, "text": " that the learning rate still hasn't settled down at that", "tokens": [300, 264, 2539, 3314, 920, 6132, 380, 14819, 760, 412, 300], "temperature": 0.0, "avg_logprob": -0.21942514242585173, "compression_ratio": 1.7023255813953488, "no_speech_prob": 9.367680831928737e-06}, {"id": 38, "seek": 15948, "start": 179.28, "end": 185.79999999999998, "text": " point. So we haven't let it fine tune into the best spot yet. So", "tokens": [935, 13, 407, 321, 2378, 380, 718, 309, 2489, 10864, 666, 264, 1151, 4008, 1939, 13, 407], "temperature": 0.0, "avg_logprob": -0.21942514242585173, "compression_ratio": 1.7023255813953488, "no_speech_prob": 9.367680831928737e-06}, {"id": 39, "seek": 18580, "start": 185.8, "end": 189.64000000000001, "text": " yeah, if it's kind of going down, down and down, it's kind of", "tokens": [1338, 11, 498, 309, 311, 733, 295, 516, 760, 11, 760, 293, 760, 11, 309, 311, 733, 295], "temperature": 0.0, "avg_logprob": -0.24403467866563305, "compression_ratio": 1.644859813084112, "no_speech_prob": 0.00015348076703958213}, {"id": 40, "seek": 18580, "start": 189.64000000000001, "end": 192.60000000000002, "text": " bottoming out and just bumps a little bit at the bottom. That's", "tokens": [2767, 278, 484, 293, 445, 27719, 257, 707, 857, 412, 264, 2767, 13, 663, 311], "temperature": 0.0, "avg_logprob": -0.24403467866563305, "compression_ratio": 1.644859813084112, "no_speech_prob": 0.00015348076703958213}, {"id": 41, "seek": 18580, "start": 192.60000000000002, "end": 194.16000000000003, "text": " not a reason to use early stopping.", "tokens": [406, 257, 1778, 281, 764, 2440, 12767, 13], "temperature": 0.0, "avg_logprob": -0.24403467866563305, "compression_ratio": 1.644859813084112, "no_speech_prob": 0.00015348076703958213}, {"id": 42, "seek": 18580, "start": 197.96, "end": 203.08, "text": " It's also, I think, important to realize that the validation set", "tokens": [467, 311, 611, 11, 286, 519, 11, 1021, 281, 4325, 300, 264, 24071, 992], "temperature": 0.0, "avg_logprob": -0.24403467866563305, "compression_ratio": 1.644859813084112, "no_speech_prob": 0.00015348076703958213}, {"id": 43, "seek": 18580, "start": 203.08, "end": 207.12, "text": " is relatively small as well. So it's only a representation of,", "tokens": [307, 7226, 1359, 382, 731, 13, 407, 309, 311, 787, 257, 10290, 295, 11], "temperature": 0.0, "avg_logprob": -0.24403467866563305, "compression_ratio": 1.644859813084112, "no_speech_prob": 0.00015348076703958213}, {"id": 44, "seek": 18580, "start": 207.64000000000001, "end": 211.04000000000002, "text": " you know, of the distribution that the data is coming from. So", "tokens": [291, 458, 11, 295, 264, 7316, 300, 264, 1412, 307, 1348, 490, 13, 407], "temperature": 0.0, "avg_logprob": -0.24403467866563305, "compression_ratio": 1.644859813084112, "no_speech_prob": 0.00015348076703958213}, {"id": 45, "seek": 21104, "start": 211.04, "end": 215.88, "text": " reading too much into those small fluctuations can be very", "tokens": [3760, 886, 709, 666, 729, 1359, 45276, 393, 312, 588], "temperature": 0.0, "avg_logprob": -0.33753431068276457, "compression_ratio": 1.5435684647302905, "no_speech_prob": 0.00018210665439255536}, {"id": 46, "seek": 21104, "start": 215.88, "end": 219.68, "text": " counterproductive. I know that I've wasted a lot of time in the", "tokens": [5682, 14314, 20221, 13, 286, 458, 300, 286, 600, 19496, 257, 688, 295, 565, 294, 264], "temperature": 0.0, "avg_logprob": -0.33753431068276457, "compression_ratio": 1.5435684647302905, "no_speech_prob": 0.00018210665439255536}, {"id": 47, "seek": 21104, "start": 219.68, "end": 224.12, "text": " past, you know, doing that. But yeah, a lot of time.", "tokens": [1791, 11, 291, 458, 11, 884, 300, 13, 583, 1338, 11, 257, 688, 295, 565, 13], "temperature": 0.0, "avg_logprob": -0.33753431068276457, "compression_ratio": 1.5435684647302905, "no_speech_prob": 0.00018210665439255536}, {"id": 48, "seek": 21104, "start": 224.92, "end": 228.48, "text": " We're looking for changes that dramatically improve things, you", "tokens": [492, 434, 1237, 337, 2962, 300, 17548, 3470, 721, 11, 291], "temperature": 0.0, "avg_logprob": -0.33753431068276457, "compression_ratio": 1.5435684647302905, "no_speech_prob": 0.00018210665439255536}, {"id": 49, "seek": 21104, "start": 228.48, "end": 233.12, "text": " know, like changing from ResNet 2016 to ConvNext, we improved", "tokens": [458, 11, 411, 4473, 490, 5015, 31890, 6549, 281, 2656, 85, 31002, 11, 321, 9689], "temperature": 0.0, "avg_logprob": -0.33753431068276457, "compression_ratio": 1.5435684647302905, "no_speech_prob": 0.00018210665439255536}, {"id": 50, "seek": 21104, "start": 233.12, "end": 236.39999999999998, "text": " by what, four or 500%? It's like, okay, that's, that's an", "tokens": [538, 437, 11, 1451, 420, 5923, 4, 30, 467, 311, 411, 11, 1392, 11, 300, 311, 11, 300, 311, 364], "temperature": 0.0, "avg_logprob": -0.33753431068276457, "compression_ratio": 1.5435684647302905, "no_speech_prob": 0.00018210665439255536}, {"id": 51, "seek": 21104, "start": 236.39999999999998, "end": 236.92, "text": " improvement.", "tokens": [10444, 13], "temperature": 0.0, "avg_logprob": -0.33753431068276457, "compression_ratio": 1.5435684647302905, "no_speech_prob": 0.00018210665439255536}, {"id": 52, "seek": 23692, "start": 236.92, "end": 242.39999999999998, "text": " Over the weekend, I went on my own server that I have here", "tokens": [4886, 264, 6711, 11, 286, 1437, 322, 452, 1065, 7154, 300, 286, 362, 510], "temperature": 0.0, "avg_logprob": -0.3767897108910789, "compression_ratio": 1.44375, "no_speech_prob": 4.7564197302563116e-05}, {"id": 53, "seek": 23692, "start": 242.39999999999998, "end": 248.67999999999998, "text": " behind me, that I have is a 10 ATI. And I run or like 35", "tokens": [2261, 385, 11, 300, 286, 362, 307, 257, 1266, 316, 5422, 13, 400, 286, 1190, 420, 411, 6976], "temperature": 0.0, "avg_logprob": -0.3767897108910789, "compression_ratio": 1.44375, "no_speech_prob": 4.7564197302563116e-05}, {"id": 54, "seek": 23692, "start": 248.72, "end": 257.68, "text": " models with, for the patty thing. And I was just, I didn't", "tokens": [5245, 365, 11, 337, 264, 1947, 874, 551, 13, 400, 286, 390, 445, 11, 286, 994, 380], "temperature": 0.0, "avg_logprob": -0.3767897108910789, "compression_ratio": 1.44375, "no_speech_prob": 4.7564197302563116e-05}, {"id": 55, "seek": 23692, "start": 257.68, "end": 262.48, "text": " do the example, but I was thinking about these that when", "tokens": [360, 264, 1365, 11, 457, 286, 390, 1953, 466, 613, 300, 562], "temperature": 0.0, "avg_logprob": -0.3767897108910789, "compression_ratio": 1.44375, "no_speech_prob": 4.7564197302563116e-05}, {"id": 56, "seek": 26248, "start": 262.48, "end": 266.92, "text": " when I was taking algebra back in high school or college, you", "tokens": [562, 286, 390, 1940, 21989, 646, 294, 1090, 1395, 420, 3859, 11, 291], "temperature": 0.0, "avg_logprob": -0.32649233341217043, "compression_ratio": 1.6263157894736842, "no_speech_prob": 4.0042919863481075e-05}, {"id": 57, "seek": 26248, "start": 266.92, "end": 270.24, "text": " have some of these expressions that you have the function of X", "tokens": [362, 512, 295, 613, 15277, 300, 291, 362, 264, 2445, 295, 1783], "temperature": 0.0, "avg_logprob": -0.32649233341217043, "compression_ratio": 1.6263157894736842, "no_speech_prob": 4.0042919863481075e-05}, {"id": 58, "seek": 26248, "start": 270.24, "end": 276.72, "text": " is equal to X squared for the X greater than something, and the", "tokens": [307, 2681, 281, 1783, 8889, 337, 264, 1783, 5044, 813, 746, 11, 293, 264], "temperature": 0.0, "avg_logprob": -0.32649233341217043, "compression_ratio": 1.6263157894736842, "no_speech_prob": 4.0042919863481075e-05}, {"id": 59, "seek": 26248, "start": 277.0, "end": 282.84000000000003, "text": " absolute value of X when you have X equal to something. So it", "tokens": [8236, 2158, 295, 1783, 562, 291, 362, 1783, 2681, 281, 746, 13, 407, 309], "temperature": 0.0, "avg_logprob": -0.32649233341217043, "compression_ratio": 1.6263157894736842, "no_speech_prob": 4.0042919863481075e-05}, {"id": 60, "seek": 26248, "start": 282.84000000000003, "end": 288.88, "text": " just got me my idea that the idea is that I maybe, maybe I", "tokens": [445, 658, 385, 452, 1558, 300, 264, 1558, 307, 300, 286, 1310, 11, 1310, 286], "temperature": 0.0, "avg_logprob": -0.32649233341217043, "compression_ratio": 1.6263157894736842, "no_speech_prob": 4.0042919863481075e-05}, {"id": 61, "seek": 28888, "start": 288.88, "end": 295.2, "text": " think that the idea is that maybe some of the data set is", "tokens": [519, 300, 264, 1558, 307, 300, 1310, 512, 295, 264, 1412, 992, 307], "temperature": 0.0, "avg_logprob": -0.21950850235788447, "compression_ratio": 1.6869158878504673, "no_speech_prob": 7.596274372190237e-05}, {"id": 62, "seek": 28888, "start": 295.2, "end": 299.04, "text": " going to fail the target value for every single one of the", "tokens": [516, 281, 3061, 264, 3779, 2158, 337, 633, 2167, 472, 295, 264], "temperature": 0.0, "avg_logprob": -0.21950850235788447, "compression_ratio": 1.6869158878504673, "no_speech_prob": 7.596274372190237e-05}, {"id": 63, "seek": 28888, "start": 299.04, "end": 302.36, "text": " models that we tried, but if we try different, different models", "tokens": [5245, 300, 321, 3031, 11, 457, 498, 321, 853, 819, 11, 819, 5245], "temperature": 0.0, "avg_logprob": -0.21950850235788447, "compression_ratio": 1.6869158878504673, "no_speech_prob": 7.596274372190237e-05}, {"id": 64, "seek": 28888, "start": 302.71999999999997, "end": 308.44, "text": " is going to be successful. So can we do that? I mean, of", "tokens": [307, 516, 281, 312, 4406, 13, 407, 393, 321, 360, 300, 30, 286, 914, 11, 295], "temperature": 0.0, "avg_logprob": -0.21950850235788447, "compression_ratio": 1.6869158878504673, "no_speech_prob": 7.596274372190237e-05}, {"id": 65, "seek": 28888, "start": 308.44, "end": 312.36, "text": " course we can, but I mean, what would be the easiest approach", "tokens": [1164, 321, 393, 11, 457, 286, 914, 11, 437, 576, 312, 264, 12889, 3109], "temperature": 0.0, "avg_logprob": -0.21950850235788447, "compression_ratio": 1.6869158878504673, "no_speech_prob": 7.596274372190237e-05}, {"id": 66, "seek": 28888, "start": 312.36, "end": 317.04, "text": " to say for this validation when X is equal to this or greater", "tokens": [281, 584, 337, 341, 24071, 562, 1783, 307, 2681, 281, 341, 420, 5044], "temperature": 0.0, "avg_logprob": -0.21950850235788447, "compression_ratio": 1.6869158878504673, "no_speech_prob": 7.596274372190237e-05}, {"id": 67, "seek": 31704, "start": 317.04, "end": 321.72, "text": " than that, this is the model to use, but then if this is all the", "tokens": [813, 300, 11, 341, 307, 264, 2316, 281, 764, 11, 457, 550, 498, 341, 307, 439, 264], "temperature": 0.0, "avg_logprob": -0.13340580463409424, "compression_ratio": 1.5966850828729282, "no_speech_prob": 6.961883173062233e-06}, {"id": 68, "seek": 31704, "start": 321.72, "end": 323.8, "text": " other model, this is what you have to use.", "tokens": [661, 2316, 11, 341, 307, 437, 291, 362, 281, 764, 13], "temperature": 0.0, "avg_logprob": -0.13340580463409424, "compression_ratio": 1.5966850828729282, "no_speech_prob": 6.961883173062233e-06}, {"id": 69, "seek": 31704, "start": 324.36, "end": 331.32000000000005, "text": " Yeah, I mean, you could do that, right? And like a really simple", "tokens": [865, 11, 286, 914, 11, 291, 727, 360, 300, 11, 558, 30, 400, 411, 257, 534, 2199], "temperature": 0.0, "avg_logprob": -0.13340580463409424, "compression_ratio": 1.5966850828729282, "no_speech_prob": 6.961883173062233e-06}, {"id": 70, "seek": 31704, "start": 331.32000000000005, "end": 339.28000000000003, "text": " way to do that, which I've, I've seen used to some success on", "tokens": [636, 281, 360, 300, 11, 597, 286, 600, 11, 286, 600, 1612, 1143, 281, 512, 2245, 322], "temperature": 0.0, "avg_logprob": -0.13340580463409424, "compression_ratio": 1.5966850828729282, "no_speech_prob": 6.961883173062233e-06}, {"id": 71, "seek": 31704, "start": 339.28000000000003, "end": 344.88, "text": " Kaggle is to train lots of models. And then to trade a", "tokens": [48751, 22631, 307, 281, 3847, 3195, 295, 5245, 13, 400, 550, 281, 4923, 257], "temperature": 0.0, "avg_logprob": -0.13340580463409424, "compression_ratio": 1.5966850828729282, "no_speech_prob": 6.961883173062233e-06}, {"id": 72, "seek": 34488, "start": 344.88, "end": 349.08, "text": " gradient boosting machine whose inputs are those model", "tokens": [16235, 43117, 3479, 6104, 15743, 366, 729, 2316], "temperature": 0.0, "avg_logprob": -0.20659250962106804, "compression_ratio": 1.6284153005464481, "no_speech_prob": 3.3719279599608853e-05}, {"id": 73, "seek": 34488, "start": 349.2, "end": 353.68, "text": " predictions that whose output is the targets. And so that'll do", "tokens": [21264, 300, 6104, 5598, 307, 264, 12911, 13, 400, 370, 300, 603, 360], "temperature": 0.0, "avg_logprob": -0.20659250962106804, "compression_ratio": 1.6284153005464481, "no_speech_prob": 3.3719279599608853e-05}, {"id": 74, "seek": 34488, "start": 353.68, "end": 360.48, "text": " exactly what you just described. It's very easy to overfit when", "tokens": [2293, 437, 291, 445, 7619, 13, 467, 311, 588, 1858, 281, 670, 6845, 562], "temperature": 0.0, "avg_logprob": -0.20659250962106804, "compression_ratio": 1.6284153005464481, "no_speech_prob": 3.3719279599608853e-05}, {"id": 75, "seek": 34488, "start": 360.48, "end": 368.64, "text": " you do that. And you're only going to get it if you've", "tokens": [291, 360, 300, 13, 400, 291, 434, 787, 516, 281, 483, 309, 498, 291, 600], "temperature": 0.0, "avg_logprob": -0.20659250962106804, "compression_ratio": 1.6284153005464481, "no_speech_prob": 3.3719279599608853e-05}, {"id": 76, "seek": 34488, "start": 368.64, "end": 372.0, "text": " trained them well, you're only going to get a tiny increase,", "tokens": [8895, 552, 731, 11, 291, 434, 787, 516, 281, 483, 257, 5870, 3488, 11], "temperature": 0.0, "avg_logprob": -0.20659250962106804, "compression_ratio": 1.6284153005464481, "no_speech_prob": 3.3719279599608853e-05}, {"id": 77, "seek": 37200, "start": 372.0, "end": 377.04, "text": " right? Because, because the neural net is very flexible, it", "tokens": [558, 30, 1436, 11, 570, 264, 18161, 2533, 307, 588, 11358, 11, 309], "temperature": 0.0, "avg_logprob": -0.17793068678482718, "compression_ratio": 1.7586206896551724, "no_speech_prob": 6.747662609996041e-06}, {"id": 78, "seek": 37200, "start": 377.04, "end": 380.6, "text": " shouldn't have that situation where this, this part of the", "tokens": [4659, 380, 362, 300, 2590, 689, 341, 11, 341, 644, 295, 264], "temperature": 0.0, "avg_logprob": -0.17793068678482718, "compression_ratio": 1.7586206896551724, "no_speech_prob": 6.747662609996041e-06}, {"id": 79, "seek": 37200, "start": 380.6, "end": 383.4, "text": " space, it has bad predictions and this part of the space, it", "tokens": [1901, 11, 309, 575, 1578, 21264, 293, 341, 644, 295, 264, 1901, 11, 309], "temperature": 0.0, "avg_logprob": -0.17793068678482718, "compression_ratio": 1.7586206896551724, "no_speech_prob": 6.747662609996041e-06}, {"id": 80, "seek": 37200, "start": 383.4, "end": 386.64, "text": " has good predictions, like it's, that's not really how neural", "tokens": [575, 665, 21264, 11, 411, 309, 311, 11, 300, 311, 406, 534, 577, 18161], "temperature": 0.0, "avg_logprob": -0.17793068678482718, "compression_ratio": 1.7586206896551724, "no_speech_prob": 6.747662609996041e-06}, {"id": 81, "seek": 37200, "start": 386.64, "end": 393.36, "text": " nets work. If you had a variety of types of like different,", "tokens": [36170, 589, 13, 759, 291, 632, 257, 5673, 295, 3467, 295, 411, 819, 11], "temperature": 0.0, "avg_logprob": -0.17793068678482718, "compression_ratio": 1.7586206896551724, "no_speech_prob": 6.747662609996041e-06}, {"id": 82, "seek": 37200, "start": 393.36, "end": 396.0, "text": " totally different types of model, like a random forest,", "tokens": [3879, 819, 3467, 295, 2316, 11, 411, 257, 4974, 6719, 11], "temperature": 0.0, "avg_logprob": -0.17793068678482718, "compression_ratio": 1.7586206896551724, "no_speech_prob": 6.747662609996041e-06}, {"id": 83, "seek": 39600, "start": 396.0, "end": 402.92, "text": " energy, BM and a neural net, I could see that maybe. But most", "tokens": [2281, 11, 15901, 293, 257, 18161, 2533, 11, 286, 727, 536, 300, 1310, 13, 583, 881], "temperature": 0.0, "avg_logprob": -0.21217613613482603, "compression_ratio": 1.5702127659574467, "no_speech_prob": 9.972099178412464e-06}, {"id": 84, "seek": 39600, "start": 402.92, "end": 406.92, "text": " of the time, one of those will be dramatically better than the", "tokens": [295, 264, 565, 11, 472, 295, 729, 486, 312, 17548, 1101, 813, 264], "temperature": 0.0, "avg_logprob": -0.21217613613482603, "compression_ratio": 1.5702127659574467, "no_speech_prob": 9.972099178412464e-06}, {"id": 85, "seek": 39600, "start": 406.92, "end": 411.44, "text": " other ones. And so like, I don't that often find myself wanting", "tokens": [661, 2306, 13, 400, 370, 411, 11, 286, 500, 380, 300, 2049, 915, 2059, 7935], "temperature": 0.0, "avg_logprob": -0.21217613613482603, "compression_ratio": 1.5702127659574467, "no_speech_prob": 9.972099178412464e-06}, {"id": 86, "seek": 39600, "start": 411.44, "end": 415.72, "text": " to ensemble across totally different types of model. So,", "tokens": [281, 19492, 2108, 3879, 819, 3467, 295, 2316, 13, 407, 11], "temperature": 0.0, "avg_logprob": -0.21217613613482603, "compression_ratio": 1.5702127659574467, "no_speech_prob": 9.972099178412464e-06}, {"id": 87, "seek": 39600, "start": 415.96, "end": 418.56, "text": " I'd say it's another one of these things like early stopping,", "tokens": [286, 1116, 584, 309, 311, 1071, 472, 295, 613, 721, 411, 2440, 12767, 11], "temperature": 0.0, "avg_logprob": -0.21217613613482603, "compression_ratio": 1.5702127659574467, "no_speech_prob": 9.972099178412464e-06}, {"id": 88, "seek": 39600, "start": 418.6, "end": 425.16, "text": " which like a lot of people waste huge amounts of time on, you", "tokens": [597, 411, 257, 688, 295, 561, 5964, 2603, 11663, 295, 565, 322, 11, 291], "temperature": 0.0, "avg_logprob": -0.21217613613482603, "compression_ratio": 1.5702127659574467, "no_speech_prob": 9.972099178412464e-06}, {"id": 89, "seek": 42516, "start": 425.16, "end": 429.84000000000003, "text": " know, and it's not really where the big benefits are going to be", "tokens": [458, 11, 293, 309, 311, 406, 534, 689, 264, 955, 5311, 366, 516, 281, 312], "temperature": 0.0, "avg_logprob": -0.2930394609769185, "compression_ratio": 1.5315315315315314, "no_speech_prob": 4.068902853759937e-05}, {"id": 90, "seek": 42516, "start": 429.84000000000003, "end": 434.44, "text": " seen. But yeah, if you're like, in gold medal zone on a Kaggle", "tokens": [1612, 13, 583, 1338, 11, 498, 291, 434, 411, 11, 294, 3821, 21364, 6668, 322, 257, 48751, 22631], "temperature": 0.0, "avg_logprob": -0.2930394609769185, "compression_ratio": 1.5315315315315314, "no_speech_prob": 4.068902853759937e-05}, {"id": 91, "seek": 42516, "start": 434.44, "end": 437.92, "text": " competition, and you need another point 002%, or something,", "tokens": [6211, 11, 293, 291, 643, 1071, 935, 7143, 17, 8923, 420, 746, 11], "temperature": 0.0, "avg_logprob": -0.2930394609769185, "compression_ratio": 1.5315315315315314, "no_speech_prob": 4.068902853759937e-05}, {"id": 92, "seek": 42516, "start": 437.92, "end": 442.6, "text": " then these are all things you can certainly try at that point.", "tokens": [550, 613, 366, 439, 721, 291, 393, 3297, 853, 412, 300, 935, 13], "temperature": 0.0, "avg_logprob": -0.2930394609769185, "compression_ratio": 1.5315315315315314, "no_speech_prob": 4.068902853759937e-05}, {"id": 93, "seek": 42516, "start": 445.28000000000003, "end": 446.44000000000005, "text": " And I think you can also...", "tokens": [400, 286, 519, 291, 393, 611, 485], "temperature": 0.0, "avg_logprob": -0.2930394609769185, "compression_ratio": 1.5315315315315314, "no_speech_prob": 4.068902853759937e-05}, {"id": 94, "seek": 42516, "start": 446.44000000000005, "end": 454.32000000000005, "text": " It kind of reminded me of AutoML, like the regime of tools. I", "tokens": [467, 733, 295, 15920, 385, 295, 13738, 12683, 11, 411, 264, 13120, 295, 3873, 13, 286], "temperature": 0.0, "avg_logprob": -0.2930394609769185, "compression_ratio": 1.5315315315315314, "no_speech_prob": 4.068902853759937e-05}, {"id": 95, "seek": 45432, "start": 454.32, "end": 456.84, "text": " don't know how you feel about how you feel about those things.", "tokens": [500, 380, 458, 577, 291, 841, 466, 577, 291, 841, 466, 729, 721, 13], "temperature": 0.0, "avg_logprob": -0.19193454624451312, "compression_ratio": 1.64, "no_speech_prob": 8.47892661113292e-05}, {"id": 96, "seek": 45432, "start": 458.28, "end": 460.84, "text": " Yeah, we talked about that last night's lesson actually, so", "tokens": [865, 11, 321, 2825, 466, 300, 1036, 1818, 311, 6898, 767, 11, 370], "temperature": 0.0, "avg_logprob": -0.19193454624451312, "compression_ratio": 1.64, "no_speech_prob": 8.47892661113292e-05}, {"id": 97, "seek": 45432, "start": 460.84, "end": 463.84, "text": " you'll have to catch up to see what I said, if you haven't", "tokens": [291, 603, 362, 281, 3745, 493, 281, 536, 437, 286, 848, 11, 498, 291, 2378, 380], "temperature": 0.0, "avg_logprob": -0.19193454624451312, "compression_ratio": 1.64, "no_speech_prob": 8.47892661113292e-05}, {"id": 98, "seek": 45432, "start": 463.84, "end": 470.56, "text": " seen the lesson yet. Yeah. I'll mention also, reading Kaggle", "tokens": [1612, 264, 6898, 1939, 13, 865, 13, 286, 603, 2152, 611, 11, 3760, 48751, 22631], "temperature": 0.0, "avg_logprob": -0.19193454624451312, "compression_ratio": 1.64, "no_speech_prob": 8.47892661113292e-05}, {"id": 99, "seek": 45432, "start": 470.56, "end": 475.2, "text": " winners descriptions of their approaches is great. But you've", "tokens": [17193, 24406, 295, 641, 11587, 307, 869, 13, 583, 291, 600], "temperature": 0.0, "avg_logprob": -0.19193454624451312, "compression_ratio": 1.64, "no_speech_prob": 8.47892661113292e-05}, {"id": 100, "seek": 45432, "start": 475.2, "end": 479.92, "text": " got to be very careful. Because remember, the Kaggle winners are", "tokens": [658, 281, 312, 588, 5026, 13, 1436, 1604, 11, 264, 48751, 22631, 17193, 366], "temperature": 0.0, "avg_logprob": -0.19193454624451312, "compression_ratio": 1.64, "no_speech_prob": 8.47892661113292e-05}, {"id": 101, "seek": 47992, "start": 479.92, "end": 484.88, "text": " the people who did get that last 0.002%, you know, because like", "tokens": [264, 561, 567, 630, 483, 300, 1036, 1958, 13, 628, 17, 8923, 291, 458, 11, 570, 411], "temperature": 0.0, "avg_logprob": -0.09227194609465422, "compression_ratio": 1.6823529411764706, "no_speech_prob": 6.048113846190972e-06}, {"id": 102, "seek": 47992, "start": 484.88, "end": 488.32, "text": " everybody found all the low hanging fruit, and the people who", "tokens": [2201, 1352, 439, 264, 2295, 8345, 6773, 11, 293, 264, 561, 567], "temperature": 0.0, "avg_logprob": -0.09227194609465422, "compression_ratio": 1.6823529411764706, "no_speech_prob": 6.048113846190972e-06}, {"id": 103, "seek": 47992, "start": 488.32, "end": 492.24, "text": " won, grabbed the really high hanging fruit. And so every time", "tokens": [1582, 11, 18607, 264, 534, 1090, 8345, 6773, 13, 400, 370, 633, 565], "temperature": 0.0, "avg_logprob": -0.09227194609465422, "compression_ratio": 1.6823529411764706, "no_speech_prob": 6.048113846190972e-06}, {"id": 104, "seek": 47992, "start": 492.24, "end": 495.04, "text": " you win a Kaggle winners description, they almost always", "tokens": [291, 1942, 257, 48751, 22631, 17193, 3855, 11, 436, 1920, 1009], "temperature": 0.0, "avg_logprob": -0.09227194609465422, "compression_ratio": 1.6823529411764706, "no_speech_prob": 6.048113846190972e-06}, {"id": 105, "seek": 47992, "start": 495.04, "end": 500.88, "text": " have complex ensembling methods. And that's why, you know, in", "tokens": [362, 3997, 12567, 2504, 1688, 7150, 13, 400, 300, 311, 983, 11, 291, 458, 11, 294], "temperature": 0.0, "avg_logprob": -0.09227194609465422, "compression_ratio": 1.6823529411764706, "no_speech_prob": 6.048113846190972e-06}, {"id": 106, "seek": 47992, "start": 500.88, "end": 503.84000000000003, "text": " like something like a big image recognition competition, it's", "tokens": [411, 746, 411, 257, 955, 3256, 11150, 6211, 11, 309, 311], "temperature": 0.0, "avg_logprob": -0.09227194609465422, "compression_ratio": 1.6823529411764706, "no_speech_prob": 6.048113846190972e-06}, {"id": 107, "seek": 47992, "start": 503.84000000000003, "end": 507.68, "text": " very hard to win or probably impossible to win with a single", "tokens": [588, 1152, 281, 1942, 420, 1391, 6243, 281, 1942, 365, 257, 2167], "temperature": 0.0, "avg_logprob": -0.09227194609465422, "compression_ratio": 1.6823529411764706, "no_speech_prob": 6.048113846190972e-06}, {"id": 108, "seek": 50768, "start": 507.68, "end": 511.04, "text": " model, unless you invent some amazing new architecture or", "tokens": [2316, 11, 5969, 291, 7962, 512, 2243, 777, 9482, 420], "temperature": 0.0, "avg_logprob": -0.12058015970083383, "compression_ratio": 1.8195876288659794, "no_speech_prob": 6.704332190565765e-05}, {"id": 109, "seek": 50768, "start": 511.04, "end": 515.76, "text": " something. And so you're kind of, you might get the impression", "tokens": [746, 13, 400, 370, 291, 434, 733, 295, 11, 291, 1062, 483, 264, 9995], "temperature": 0.0, "avg_logprob": -0.12058015970083383, "compression_ratio": 1.8195876288659794, "no_speech_prob": 6.704332190565765e-05}, {"id": 110, "seek": 50768, "start": 515.76, "end": 519.92, "text": " then that ensembling is the big thing that gets you all the low", "tokens": [550, 300, 12567, 2504, 1688, 307, 264, 955, 551, 300, 2170, 291, 439, 264, 2295], "temperature": 0.0, "avg_logprob": -0.12058015970083383, "compression_ratio": 1.8195876288659794, "no_speech_prob": 6.704332190565765e-05}, {"id": 111, "seek": 50768, "start": 519.92, "end": 523.2, "text": " hanging fruit, but it's not. ensembling is the thing which,", "tokens": [8345, 6773, 11, 457, 309, 311, 406, 13, 12567, 2504, 1688, 307, 264, 551, 597, 11], "temperature": 0.0, "avg_logprob": -0.12058015970083383, "compression_ratio": 1.8195876288659794, "no_speech_prob": 6.704332190565765e-05}, {"id": 112, "seek": 50768, "start": 524.16, "end": 526.08, "text": " or particularly complex ensembling is the thing that", "tokens": [420, 4098, 3997, 12567, 2504, 1688, 307, 264, 551, 300], "temperature": 0.0, "avg_logprob": -0.12058015970083383, "compression_ratio": 1.8195876288659794, "no_speech_prob": 6.704332190565765e-05}, {"id": 113, "seek": 50768, "start": 526.08, "end": 529.36, "text": " gets you that last fraction of a fraction of a percent.", "tokens": [2170, 291, 300, 1036, 14135, 295, 257, 14135, 295, 257, 3043, 13], "temperature": 0.0, "avg_logprob": -0.12058015970083383, "compression_ratio": 1.8195876288659794, "no_speech_prob": 6.704332190565765e-05}, {"id": 114, "seek": 52936, "start": 529.36, "end": 537.28, "text": " One more question. Yeah, of course.", "tokens": [1485, 544, 1168, 13, 865, 11, 295, 1164, 13], "temperature": 0.0, "avg_logprob": -0.4762379010518392, "compression_ratio": 1.0526315789473684, "no_speech_prob": 6.597759784199297e-05}, {"id": 115, "seek": 52936, "start": 538.8000000000001, "end": 552.96, "text": " TTL concept, right? So I'm trying to understand conceptually why", "tokens": [32576, 43, 3410, 11, 558, 30, 407, 286, 478, 1382, 281, 1223, 3410, 671, 983], "temperature": 0.0, "avg_logprob": -0.4762379010518392, "compression_ratio": 1.0526315789473684, "no_speech_prob": 6.597759784199297e-05}, {"id": 116, "seek": 55296, "start": 552.96, "end": 559.84, "text": " TTL improves the score. Because technically, when you're training,", "tokens": [32576, 43, 24771, 264, 6175, 13, 1436, 12120, 11, 562, 291, 434, 3097, 11], "temperature": 0.0, "avg_logprob": -0.17274681329727173, "compression_ratio": 1.5606060606060606, "no_speech_prob": 6.048418526916066e-06}, {"id": 117, "seek": 55296, "start": 559.84, "end": 563.6800000000001, "text": " it is using those augmented sort of pictures and providing them", "tokens": [309, 307, 1228, 729, 36155, 1333, 295, 5242, 293, 6530, 552], "temperature": 0.0, "avg_logprob": -0.17274681329727173, "compression_ratio": 1.5606060606060606, "no_speech_prob": 6.048418526916066e-06}, {"id": 118, "seek": 55296, "start": 563.6800000000001, "end": 568.8000000000001, "text": " providing a percentage number. But when you're kind of, when", "tokens": [6530, 257, 9668, 1230, 13, 583, 562, 291, 434, 733, 295, 11, 562], "temperature": 0.0, "avg_logprob": -0.17274681329727173, "compression_ratio": 1.5606060606060606, "no_speech_prob": 6.048418526916066e-06}, {"id": 119, "seek": 55296, "start": 568.8000000000001, "end": 573.12, "text": " you run that TTL function, why is it able to predict that?", "tokens": [291, 1190, 300, 32576, 43, 2445, 11, 983, 307, 309, 1075, 281, 6069, 300, 30], "temperature": 0.0, "avg_logprob": -0.17274681329727173, "compression_ratio": 1.5606060606060606, "no_speech_prob": 6.048418526916066e-06}, {"id": 120, "seek": 55296, "start": 573.12, "end": 579.84, "text": " Sure. So like, you know, sometimes you're like, looking at", "tokens": [4894, 13, 407, 411, 11, 291, 458, 11, 2171, 291, 434, 411, 11, 1237, 412], "temperature": 0.0, "avg_logprob": -0.17274681329727173, "compression_ratio": 1.5606060606060606, "no_speech_prob": 6.048418526916066e-06}, {"id": 121, "seek": 57984, "start": 579.84, "end": 587.76, "text": " some like, I don't know, a screw head or a socket or something", "tokens": [512, 411, 11, 286, 500, 380, 458, 11, 257, 5630, 1378, 420, 257, 19741, 420, 746], "temperature": 0.0, "avg_logprob": -0.14783424377441406, "compression_ratio": 1.8146551724137931, "no_speech_prob": 5.223979314905591e-05}, {"id": 122, "seek": 57984, "start": 587.76, "end": 591.2800000000001, "text": " that's really small, and you can't quite see like what, how", "tokens": [300, 311, 534, 1359, 11, 293, 291, 393, 380, 1596, 536, 411, 437, 11, 577], "temperature": 0.0, "avg_logprob": -0.14783424377441406, "compression_ratio": 1.8146551724137931, "no_speech_prob": 5.223979314905591e-05}, {"id": 123, "seek": 57984, "start": 591.2800000000001, "end": 594.08, "text": " many pins are in it, or what type is it or whatever. And", "tokens": [867, 16392, 366, 294, 309, 11, 420, 437, 2010, 307, 309, 420, 2035, 13, 400], "temperature": 0.0, "avg_logprob": -0.14783424377441406, "compression_ratio": 1.8146551724137931, "no_speech_prob": 5.223979314905591e-05}, {"id": 124, "seek": 57984, "start": 594.08, "end": 596.08, "text": " you're kind of like, look at it from different angles, and", "tokens": [291, 434, 733, 295, 411, 11, 574, 412, 309, 490, 819, 14708, 11, 293], "temperature": 0.0, "avg_logprob": -0.14783424377441406, "compression_ratio": 1.8146551724137931, "no_speech_prob": 5.223979314905591e-05}, {"id": 125, "seek": 57984, "start": 596.08, "end": 598.5600000000001, "text": " you're kind of like, put it up to the light and you try to like,", "tokens": [291, 434, 733, 295, 411, 11, 829, 309, 493, 281, 264, 1442, 293, 291, 853, 281, 411, 11], "temperature": 0.0, "avg_logprob": -0.14783424377441406, "compression_ratio": 1.8146551724137931, "no_speech_prob": 5.223979314905591e-05}, {"id": 126, "seek": 57984, "start": 599.2800000000001, "end": 602.72, "text": " at some point, you're like, okay, I see it. Right. And", "tokens": [412, 512, 935, 11, 291, 434, 411, 11, 1392, 11, 286, 536, 309, 13, 1779, 13, 400], "temperature": 0.0, "avg_logprob": -0.14783424377441406, "compression_ratio": 1.8146551724137931, "no_speech_prob": 5.223979314905591e-05}, {"id": 127, "seek": 57984, "start": 602.72, "end": 608.64, "text": " there's like some angle and some lighting that you can see it.", "tokens": [456, 311, 411, 512, 5802, 293, 512, 9577, 300, 291, 393, 536, 309, 13], "temperature": 0.0, "avg_logprob": -0.14783424377441406, "compression_ratio": 1.8146551724137931, "no_speech_prob": 5.223979314905591e-05}, {"id": 128, "seek": 60864, "start": 608.64, "end": 612.0, "text": " That's what you're doing for the computer, you're giving it", "tokens": [663, 311, 437, 291, 434, 884, 337, 264, 3820, 11, 291, 434, 2902, 309], "temperature": 0.0, "avg_logprob": -0.08121166910443987, "compression_ratio": 1.9027237354085602, "no_speech_prob": 1.6699612388038076e-05}, {"id": 129, "seek": 60864, "start": 612.0, "end": 614.16, "text": " different angles, and you're giving it different lighting in", "tokens": [819, 14708, 11, 293, 291, 434, 2902, 309, 819, 9577, 294], "temperature": 0.0, "avg_logprob": -0.08121166910443987, "compression_ratio": 1.9027237354085602, "no_speech_prob": 1.6699612388038076e-05}, {"id": 130, "seek": 60864, "start": 614.16, "end": 617.76, "text": " the hope that in one of those, it's going to be really clear.", "tokens": [264, 1454, 300, 294, 472, 295, 729, 11, 309, 311, 516, 281, 312, 534, 1850, 13], "temperature": 0.0, "avg_logprob": -0.08121166910443987, "compression_ratio": 1.9027237354085602, "no_speech_prob": 1.6699612388038076e-05}, {"id": 131, "seek": 60864, "start": 619.68, "end": 622.72, "text": " And for the ones where it's easy, it's not going to make", "tokens": [400, 337, 264, 2306, 689, 309, 311, 1858, 11, 309, 311, 406, 516, 281, 652], "temperature": 0.0, "avg_logprob": -0.08121166910443987, "compression_ratio": 1.9027237354085602, "no_speech_prob": 1.6699612388038076e-05}, {"id": 132, "seek": 60864, "start": 622.72, "end": 625.04, "text": " any difference, right? But for the ones where it's like, oh, I", "tokens": [604, 2649, 11, 558, 30, 583, 337, 264, 2306, 689, 309, 311, 411, 11, 1954, 11, 286], "temperature": 0.0, "avg_logprob": -0.08121166910443987, "compression_ratio": 1.9027237354085602, "no_speech_prob": 1.6699612388038076e-05}, {"id": 133, "seek": 60864, "start": 625.04, "end": 629.1999999999999, "text": " don't know if it's this disease or that disease, but oh, you", "tokens": [500, 380, 458, 498, 309, 311, 341, 4752, 420, 300, 4752, 11, 457, 1954, 11, 291], "temperature": 0.0, "avg_logprob": -0.08121166910443987, "compression_ratio": 1.9027237354085602, "no_speech_prob": 1.6699612388038076e-05}, {"id": 134, "seek": 60864, "start": 629.1999999999999, "end": 631.68, "text": " know, when it's a bit brighter, and you kind of zoom into that", "tokens": [458, 11, 562, 309, 311, 257, 857, 19764, 11, 293, 291, 733, 295, 8863, 666, 300], "temperature": 0.0, "avg_logprob": -0.08121166910443987, "compression_ratio": 1.9027237354085602, "no_speech_prob": 1.6699612388038076e-05}, {"id": 135, "seek": 60864, "start": 631.68, "end": 636.16, "text": " section, like, oh, now I can see. And so when you then average", "tokens": [3541, 11, 411, 11, 1954, 11, 586, 286, 393, 536, 13, 400, 370, 562, 291, 550, 4274], "temperature": 0.0, "avg_logprob": -0.08121166910443987, "compression_ratio": 1.9027237354085602, "no_speech_prob": 1.6699612388038076e-05}, {"id": 136, "seek": 63616, "start": 636.16, "end": 639.28, "text": " them out, you know, all the other ones are all like, oh, I", "tokens": [552, 484, 11, 291, 458, 11, 439, 264, 661, 2306, 366, 439, 411, 11, 1954, 11, 286], "temperature": 0.0, "avg_logprob": -0.15411092684819147, "compression_ratio": 1.7156398104265402, "no_speech_prob": 1.8057000488624908e-05}, {"id": 137, "seek": 63616, "start": 639.28, "end": 641.36, "text": " don't know which kind of is it, which kinds of it's like point", "tokens": [500, 380, 458, 597, 733, 295, 307, 309, 11, 597, 3685, 295, 309, 311, 411, 935], "temperature": 0.0, "avg_logprob": -0.15411092684819147, "compression_ratio": 1.7156398104265402, "no_speech_prob": 1.8057000488624908e-05}, {"id": 138, "seek": 63616, "start": 641.36, "end": 643.4399999999999, "text": " five point five point five, and then this one is like point", "tokens": [1732, 935, 1732, 935, 1732, 11, 293, 550, 341, 472, 307, 411, 935], "temperature": 0.0, "avg_logprob": -0.15411092684819147, "compression_ratio": 1.7156398104265402, "no_speech_prob": 1.8057000488624908e-05}, {"id": 139, "seek": 63616, "start": 643.4399999999999, "end": 646.24, "text": " six. And so that's the one that in the average, it's going to", "tokens": [2309, 13, 400, 370, 300, 311, 264, 472, 300, 294, 264, 4274, 11, 309, 311, 516, 281], "temperature": 0.0, "avg_logprob": -0.15411092684819147, "compression_ratio": 1.7156398104265402, "no_speech_prob": 1.8057000488624908e-05}, {"id": 140, "seek": 63616, "start": 646.24, "end": 655.12, "text": " end up picking. That's basically what happens. It also has", "tokens": [917, 493, 8867, 13, 663, 311, 1936, 437, 2314, 13, 467, 611, 575], "temperature": 0.0, "avg_logprob": -0.15411092684819147, "compression_ratio": 1.7156398104265402, "no_speech_prob": 1.8057000488624908e-05}, {"id": 141, "seek": 63616, "start": 655.12, "end": 663.1999999999999, "text": " another benefit, which is when we train our models, I don't", "tokens": [1071, 5121, 11, 597, 307, 562, 321, 3847, 527, 5245, 11, 286, 500, 380], "temperature": 0.0, "avg_logprob": -0.15411092684819147, "compression_ratio": 1.7156398104265402, "no_speech_prob": 1.8057000488624908e-05}, {"id": 142, "seek": 66320, "start": 663.2, "end": 666.96, "text": " know if you've noticed, but our training loss generally gets", "tokens": [458, 498, 291, 600, 5694, 11, 457, 527, 3097, 4470, 5101, 2170], "temperature": 0.0, "avg_logprob": -0.1454705930735967, "compression_ratio": 1.549222797927461, "no_speech_prob": 1.0952071534120478e-05}, {"id": 143, "seek": 66320, "start": 666.96, "end": 673.44, "text": " much, much lower than our validation loss. And sometimes", "tokens": [709, 11, 709, 3126, 813, 527, 24071, 4470, 13, 400, 2171], "temperature": 0.0, "avg_logprob": -0.1454705930735967, "compression_ratio": 1.549222797927461, "no_speech_prob": 1.0952071534120478e-05}, {"id": 144, "seek": 66320, "start": 673.44, "end": 683.9200000000001, "text": " they validate sometimes our well, so basically, like, what's", "tokens": [436, 29562, 2171, 527, 731, 11, 370, 1936, 11, 411, 11, 437, 311], "temperature": 0.0, "avg_logprob": -0.1454705930735967, "compression_ratio": 1.549222797927461, "no_speech_prob": 1.0952071534120478e-05}, {"id": 145, "seek": 66320, "start": 683.9200000000001, "end": 687.84, "text": " happening there is that on the training set, the model is", "tokens": [2737, 456, 307, 300, 322, 264, 3097, 992, 11, 264, 2316, 307], "temperature": 0.0, "avg_logprob": -0.1454705930735967, "compression_ratio": 1.549222797927461, "no_speech_prob": 1.0952071534120478e-05}, {"id": 146, "seek": 66320, "start": 687.84, "end": 690.48, "text": " getting very confident, right? So even though we're using data", "tokens": [1242, 588, 6679, 11, 558, 30, 407, 754, 1673, 321, 434, 1228, 1412], "temperature": 0.0, "avg_logprob": -0.1454705930735967, "compression_ratio": 1.549222797927461, "no_speech_prob": 1.0952071534120478e-05}, {"id": 147, "seek": 69048, "start": 690.48, "end": 693.2, "text": " augmentation, it's seeing slightly different versions of", "tokens": [14501, 19631, 11, 309, 311, 2577, 4748, 819, 9606, 295], "temperature": 0.0, "avg_logprob": -0.09057392916836582, "compression_ratio": 1.6604651162790698, "no_speech_prob": 4.029140654893126e-06}, {"id": 148, "seek": 69048, "start": 693.2, "end": 696.88, "text": " the same image dozens of times. And it's like, oh, I know how to", "tokens": [264, 912, 3256, 18431, 295, 1413, 13, 400, 309, 311, 411, 11, 1954, 11, 286, 458, 577, 281], "temperature": 0.0, "avg_logprob": -0.09057392916836582, "compression_ratio": 1.6604651162790698, "no_speech_prob": 4.029140654893126e-06}, {"id": 149, "seek": 69048, "start": 696.88, "end": 703.04, "text": " recognize these. And so what it does is that the probabilities", "tokens": [5521, 613, 13, 400, 370, 437, 309, 775, 307, 300, 264, 33783], "temperature": 0.0, "avg_logprob": -0.09057392916836582, "compression_ratio": 1.6604651162790698, "no_speech_prob": 4.029140654893126e-06}, {"id": 150, "seek": 69048, "start": 703.04, "end": 706.88, "text": " that associates with them is like point nine point nine nine,", "tokens": [300, 36914, 365, 552, 307, 411, 935, 4949, 935, 4949, 4949, 11], "temperature": 0.0, "avg_logprob": -0.09057392916836582, "compression_ratio": 1.6604651162790698, "no_speech_prob": 4.029140654893126e-06}, {"id": 151, "seek": 69048, "start": 706.88, "end": 710.72, "text": " you know, like saying, I'm very confident of these. And it", "tokens": [291, 458, 11, 411, 1566, 11, 286, 478, 588, 6679, 295, 613, 13, 400, 309], "temperature": 0.0, "avg_logprob": -0.09057392916836582, "compression_ratio": 1.6604651162790698, "no_speech_prob": 4.029140654893126e-06}, {"id": 152, "seek": 69048, "start": 710.72, "end": 716.4, "text": " actually gets overconfident, which actually doesn't", "tokens": [767, 2170, 670, 24697, 1078, 11, 597, 767, 1177, 380], "temperature": 0.0, "avg_logprob": -0.09057392916836582, "compression_ratio": 1.6604651162790698, "no_speech_prob": 4.029140654893126e-06}, {"id": 153, "seek": 71640, "start": 716.4, "end": 721.6, "text": " necessarily impact our accuracy, you know, to be overconfident.", "tokens": [4725, 2712, 527, 14170, 11, 291, 458, 11, 281, 312, 670, 24697, 1078, 13], "temperature": 0.0, "avg_logprob": -0.11901538848876952, "compression_ratio": 1.5, "no_speech_prob": 8.80026891536545e-06}, {"id": 154, "seek": 71640, "start": 724.88, "end": 731.84, "text": " But at some point, it can. And so we are systematically going", "tokens": [583, 412, 512, 935, 11, 309, 393, 13, 400, 370, 321, 366, 39531, 516], "temperature": 0.0, "avg_logprob": -0.11901538848876952, "compression_ratio": 1.5, "no_speech_prob": 8.80026891536545e-06}, {"id": 155, "seek": 71640, "start": 731.84, "end": 734.9599999999999, "text": " to have like overconfident predictions of probability.", "tokens": [281, 362, 411, 670, 24697, 1078, 21264, 295, 8482, 13], "temperature": 0.0, "avg_logprob": -0.11901538848876952, "compression_ratio": 1.5, "no_speech_prob": 8.80026891536545e-06}, {"id": 156, "seek": 71640, "start": 736.72, "end": 740.24, "text": " When even when it doesn't really know, just because it's", "tokens": [1133, 754, 562, 309, 1177, 380, 534, 458, 11, 445, 570, 309, 311], "temperature": 0.0, "avg_logprob": -0.11901538848876952, "compression_ratio": 1.5, "no_speech_prob": 8.80026891536545e-06}, {"id": 157, "seek": 71640, "start": 740.24, "end": 742.56, "text": " really seen that kind of image before. So then on the", "tokens": [534, 1612, 300, 733, 295, 3256, 949, 13, 407, 550, 322, 264], "temperature": 0.0, "avg_logprob": -0.11901538848876952, "compression_ratio": 1.5, "no_speech_prob": 8.80026891536545e-06}, {"id": 158, "seek": 74256, "start": 742.56, "end": 748.64, "text": " validation set, it's going to be, you know, over picking", "tokens": [24071, 992, 11, 309, 311, 516, 281, 312, 11, 291, 458, 11, 670, 8867], "temperature": 0.0, "avg_logprob": -0.1325400405459934, "compression_ratio": 1.9074074074074074, "no_speech_prob": 1.3006404515181202e-05}, {"id": 159, "seek": 74256, "start": 748.64, "end": 751.92, "text": " probabilities as well. And so one nice benefit is that when", "tokens": [33783, 382, 731, 13, 400, 370, 472, 1481, 5121, 307, 300, 562], "temperature": 0.0, "avg_logprob": -0.1325400405459934, "compression_ratio": 1.9074074074074074, "no_speech_prob": 1.3006404515181202e-05}, {"id": 160, "seek": 74256, "start": 751.92, "end": 758.0, "text": " you average out a few augmented versions, you know, it's like", "tokens": [291, 4274, 484, 257, 1326, 36155, 9606, 11, 291, 458, 11, 309, 311, 411], "temperature": 0.0, "avg_logprob": -0.1325400405459934, "compression_ratio": 1.9074074074074074, "no_speech_prob": 1.3006404515181202e-05}, {"id": 161, "seek": 74256, "start": 758.0, "end": 760.4799999999999, "text": " our point nine point nine probability is this one. And", "tokens": [527, 935, 4949, 935, 4949, 8482, 307, 341, 472, 13, 400], "temperature": 0.0, "avg_logprob": -0.1325400405459934, "compression_ratio": 1.9074074074074074, "no_speech_prob": 1.3006404515181202e-05}, {"id": 162, "seek": 74256, "start": 760.4799999999999, "end": 762.8, "text": " then on the next one, it's like, next augmented version of the", "tokens": [550, 322, 264, 958, 472, 11, 309, 311, 411, 11, 958, 36155, 3037, 295, 264], "temperature": 0.0, "avg_logprob": -0.1325400405459934, "compression_ratio": 1.9074074074074074, "no_speech_prob": 1.3006404515181202e-05}, {"id": 163, "seek": 74256, "start": 762.8, "end": 765.52, "text": " same image, like, oh, no point one probability is that one. And", "tokens": [912, 3256, 11, 411, 11, 1954, 11, 572, 935, 472, 8482, 307, 300, 472, 13, 400], "temperature": 0.0, "avg_logprob": -0.1325400405459934, "compression_ratio": 1.9074074074074074, "no_speech_prob": 1.3006404515181202e-05}, {"id": 164, "seek": 74256, "start": 765.52, "end": 769.28, "text": " they'll kind of average out to much more reasonable", "tokens": [436, 603, 733, 295, 4274, 484, 281, 709, 544, 10585], "temperature": 0.0, "avg_logprob": -0.1325400405459934, "compression_ratio": 1.9074074074074074, "no_speech_prob": 1.3006404515181202e-05}, {"id": 165, "seek": 76928, "start": 769.28, "end": 775.68, "text": " probabilities, which can, you know, allow it sometimes to", "tokens": [33783, 11, 597, 393, 11, 291, 458, 11, 2089, 309, 2171, 281], "temperature": 0.0, "avg_logprob": -0.285249932607015, "compression_ratio": 1.50920245398773, "no_speech_prob": 4.682960934587754e-05}, {"id": 166, "seek": 76928, "start": 779.1999999999999, "end": 783.36, "text": " combine these ideas into an average that that makes more", "tokens": [10432, 613, 3487, 666, 364, 4274, 300, 300, 1669, 544], "temperature": 0.0, "avg_logprob": -0.285249932607015, "compression_ratio": 1.50920245398773, "no_speech_prob": 4.682960934587754e-05}, {"id": 167, "seek": 76928, "start": 783.36, "end": 787.92, "text": " sense. And so that can improve accuracy, but in particular, it", "tokens": [2020, 13, 400, 370, 300, 393, 3470, 14170, 11, 457, 294, 1729, 11, 309], "temperature": 0.0, "avg_logprob": -0.285249932607015, "compression_ratio": 1.50920245398773, "no_speech_prob": 4.682960934587754e-05}, {"id": 168, "seek": 76928, "start": 787.92, "end": 792.0, "text": " improves the actual probabilities to get rid of", "tokens": [24771, 264, 3539, 33783, 281, 483, 3973, 295], "temperature": 0.0, "avg_logprob": -0.285249932607015, "compression_ratio": 1.50920245398773, "no_speech_prob": 4.682960934587754e-05}, {"id": 169, "seek": 76928, "start": 792.0, "end": 792.9599999999999, "text": " that overconfidence.", "tokens": [300, 670, 47273, 13], "temperature": 0.0, "avg_logprob": -0.285249932607015, "compression_ratio": 1.50920245398773, "no_speech_prob": 4.682960934587754e-05}, {"id": 170, "seek": 79296, "start": 792.96, "end": 797.44, "text": " Is it fair to say that when you train without when you train,", "tokens": [1119, 309, 3143, 281, 584, 300, 562, 291, 3847, 1553, 562, 291, 3847, 11], "temperature": 0.0, "avg_logprob": -0.3346259689331055, "compression_ratio": 1.7535545023696681, "no_speech_prob": 1.983160836971365e-05}, {"id": 171, "seek": 79296, "start": 797.52, "end": 801.9200000000001, "text": " it's not able to separate the replicated sort of images or", "tokens": [309, 311, 406, 1075, 281, 4994, 264, 46365, 1333, 295, 5267, 420], "temperature": 0.0, "avg_logprob": -0.3346259689331055, "compression_ratio": 1.7535545023696681, "no_speech_prob": 1.983160836971365e-05}, {"id": 172, "seek": 79296, "start": 801.9200000000001, "end": 804.8000000000001, "text": " the distorted slightly where the variant of the original", "tokens": [264, 33431, 4748, 689, 264, 17501, 295, 264, 3380], "temperature": 0.0, "avg_logprob": -0.3346259689331055, "compression_ratio": 1.7535545023696681, "no_speech_prob": 1.983160836971365e-05}, {"id": 173, "seek": 79296, "start": 805.84, "end": 809.44, "text": " image, but when you use the TTA, it is able to group all the", "tokens": [3256, 11, 457, 562, 291, 764, 264, 314, 8241, 11, 309, 307, 1075, 281, 1594, 439, 264], "temperature": 0.0, "avg_logprob": -0.3346259689331055, "compression_ratio": 1.7535545023696681, "no_speech_prob": 1.983160836971365e-05}, {"id": 174, "seek": 79296, "start": 809.44, "end": 811.2, "text": " four images and", "tokens": [1451, 5267, 293], "temperature": 0.0, "avg_logprob": -0.3346259689331055, "compression_ratio": 1.7535545023696681, "no_speech_prob": 1.983160836971365e-05}, {"id": 175, "seek": 79296, "start": 811.76, "end": 817.12, "text": " that's what TTA is. We present them all together and average", "tokens": [300, 311, 437, 314, 8241, 307, 13, 492, 1974, 552, 439, 1214, 293, 4274], "temperature": 0.0, "avg_logprob": -0.3346259689331055, "compression_ratio": 1.7535545023696681, "no_speech_prob": 1.983160836971365e-05}, {"id": 176, "seek": 79296, "start": 817.12, "end": 820.48, "text": " out that group. Yes. But in training, it's not able to", "tokens": [484, 300, 1594, 13, 1079, 13, 583, 294, 3097, 11, 309, 311, 406, 1075, 281], "temperature": 0.0, "avg_logprob": -0.3346259689331055, "compression_ratio": 1.7535545023696681, "no_speech_prob": 1.983160836971365e-05}, {"id": 177, "seek": 82048, "start": 820.48, "end": 823.28, "text": " average out that group. Yes. But in training, we don't", "tokens": [4274, 484, 300, 1594, 13, 1079, 13, 583, 294, 3097, 11, 321, 500, 380], "temperature": 0.0, "avg_logprob": -0.21337304512659708, "compression_ratio": 1.6208530805687205, "no_speech_prob": 2.078252873616293e-05}, {"id": 178, "seek": 82048, "start": 824.72, "end": 827.28, "text": " indicate in any way that they're the same image, or that", "tokens": [13330, 294, 604, 636, 300, 436, 434, 264, 912, 3256, 11, 420, 300], "temperature": 0.0, "avg_logprob": -0.21337304512659708, "compression_ratio": 1.6208530805687205, "no_speech_prob": 2.078252873616293e-05}, {"id": 179, "seek": 82048, "start": 827.28, "end": 828.5600000000001, "text": " they're the same underlying object.", "tokens": [436, 434, 264, 912, 14217, 2657, 13], "temperature": 0.0, "avg_logprob": -0.21337304512659708, "compression_ratio": 1.6208530805687205, "no_speech_prob": 2.078252873616293e-05}, {"id": 180, "seek": 82048, "start": 834.32, "end": 838.8000000000001, "text": " One other question, Jeremy, we, I'm glad we touched on", "tokens": [1485, 661, 1168, 11, 17809, 11, 321, 11, 286, 478, 5404, 321, 9828, 322], "temperature": 0.0, "avg_logprob": -0.21337304512659708, "compression_ratio": 1.6208530805687205, "no_speech_prob": 2.078252873616293e-05}, {"id": 181, "seek": 82048, "start": 838.8000000000001, "end": 840.8000000000001, "text": " stumbling and how to pick the best", "tokens": [342, 14188, 293, 577, 281, 1888, 264, 1151], "temperature": 0.0, "avg_logprob": -0.21337304512659708, "compression_ratio": 1.6208530805687205, "no_speech_prob": 2.078252873616293e-05}, {"id": 182, "seek": 82048, "start": 842.0, "end": 845.84, "text": " on stumbling. I was gonna ask about that. But another", "tokens": [322, 342, 14188, 13, 286, 390, 799, 1029, 466, 300, 13, 583, 1071], "temperature": 0.0, "avg_logprob": -0.21337304512659708, "compression_ratio": 1.6208530805687205, "no_speech_prob": 2.078252873616293e-05}, {"id": 183, "seek": 84584, "start": 845.84, "end": 851.0400000000001, "text": " question is, we have a fairly unbalanced data set, I guess,", "tokens": [1168, 307, 11, 321, 362, 257, 6457, 517, 40251, 1412, 992, 11, 286, 2041, 11], "temperature": 0.0, "avg_logprob": -0.1491621912053201, "compression_ratio": 1.7398373983739837, "no_speech_prob": 2.3548420358565636e-05}, {"id": 184, "seek": 84584, "start": 851.0400000000001, "end": 854.72, "text": " with the normal versus the disease states. Yeah. When", "tokens": [365, 264, 2710, 5717, 264, 4752, 4368, 13, 865, 13, 1133], "temperature": 0.0, "avg_logprob": -0.1491621912053201, "compression_ratio": 1.7398373983739837, "no_speech_prob": 2.3548420358565636e-05}, {"id": 185, "seek": 84584, "start": 854.72, "end": 857.52, "text": " you're doing augmentation, is there any benefit to sort of", "tokens": [291, 434, 884, 14501, 19631, 11, 307, 456, 604, 5121, 281, 1333, 295], "temperature": 0.0, "avg_logprob": -0.1491621912053201, "compression_ratio": 1.7398373983739837, "no_speech_prob": 2.3548420358565636e-05}, {"id": 186, "seek": 84584, "start": 857.52, "end": 860.1600000000001, "text": " over over representing the minority classes?", "tokens": [670, 670, 13460, 264, 16166, 5359, 30], "temperature": 0.0, "avg_logprob": -0.1491621912053201, "compression_ratio": 1.7398373983739837, "no_speech_prob": 2.3548420358565636e-05}, {"id": 187, "seek": 84584, "start": 861.76, "end": 864.32, "text": " So let's, let's pull away augmentation. So it's actually", "tokens": [407, 718, 311, 11, 718, 311, 2235, 1314, 14501, 19631, 13, 407, 309, 311, 767], "temperature": 0.0, "avg_logprob": -0.1491621912053201, "compression_ratio": 1.7398373983739837, "no_speech_prob": 2.3548420358565636e-05}, {"id": 188, "seek": 84584, "start": 864.32, "end": 866.88, "text": " got nothing to do with augmentation. So more generally,", "tokens": [658, 1825, 281, 360, 365, 14501, 19631, 13, 407, 544, 5101, 11], "temperature": 0.0, "avg_logprob": -0.1491621912053201, "compression_ratio": 1.7398373983739837, "no_speech_prob": 2.3548420358565636e-05}, {"id": 189, "seek": 84584, "start": 867.44, "end": 869.84, "text": " when you're training, does it make sense to over represent", "tokens": [562, 291, 434, 3097, 11, 775, 309, 652, 2020, 281, 670, 2906], "temperature": 0.0, "avg_logprob": -0.1491621912053201, "compression_ratio": 1.7398373983739837, "no_speech_prob": 2.3548420358565636e-05}, {"id": 190, "seek": 84584, "start": 869.84, "end": 870.8000000000001, "text": " the minority class?", "tokens": [264, 16166, 1508, 30], "temperature": 0.0, "avg_logprob": -0.1491621912053201, "compression_ratio": 1.7398373983739837, "no_speech_prob": 2.3548420358565636e-05}, {"id": 191, "seek": 87080, "start": 870.8, "end": 879.8399999999999, "text": " And the answer is, maybe. Yeah, it can. Right. And so, okay,", "tokens": [400, 264, 1867, 307, 11, 1310, 13, 865, 11, 309, 393, 13, 1779, 13, 400, 370, 11, 1392, 11], "temperature": 0.0, "avg_logprob": -0.24562571911101647, "compression_ratio": 1.5609756097560976, "no_speech_prob": 1.6438345483038574e-05}, {"id": 192, "seek": 87080, "start": 879.8399999999999, "end": 883.04, "text": " so just for those who aren't following, the issue Matt's", "tokens": [370, 445, 337, 729, 567, 3212, 380, 3480, 11, 264, 2734, 7397, 311], "temperature": 0.0, "avg_logprob": -0.24562571911101647, "compression_ratio": 1.5609756097560976, "no_speech_prob": 1.6438345483038574e-05}, {"id": 193, "seek": 87080, "start": 883.04, "end": 886.24, "text": " talking about is that there was, you know, a couple of", "tokens": [1417, 466, 307, 300, 456, 390, 11, 291, 458, 11, 257, 1916, 295], "temperature": 0.0, "avg_logprob": -0.24562571911101647, "compression_ratio": 1.5609756097560976, "no_speech_prob": 1.6438345483038574e-05}, {"id": 194, "seek": 87080, "start": 886.24, "end": 889.76, "text": " diseases which appear lots and lots in the data, and a couple", "tokens": [11044, 597, 4204, 3195, 293, 3195, 294, 264, 1412, 11, 293, 257, 1916], "temperature": 0.0, "avg_logprob": -0.24562571911101647, "compression_ratio": 1.5609756097560976, "no_speech_prob": 1.6438345483038574e-05}, {"id": 195, "seek": 87080, "start": 889.76, "end": 890.88, "text": " which hardly appear at all.", "tokens": [597, 13572, 4204, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.24562571911101647, "compression_ratio": 1.5609756097560976, "no_speech_prob": 1.6438345483038574e-05}, {"id": 196, "seek": 87080, "start": 894.64, "end": 898.64, "text": " And so, you know, do we want to try to balance this data?", "tokens": [400, 370, 11, 291, 458, 11, 360, 321, 528, 281, 853, 281, 4772, 341, 1412, 30], "temperature": 0.0, "avg_logprob": -0.24562571911101647, "compression_ratio": 1.5609756097560976, "no_speech_prob": 1.6438345483038574e-05}, {"id": 197, "seek": 89864, "start": 898.64, "end": 902.8, "text": " You know, do we want to try to balance this out more? And one", "tokens": [509, 458, 11, 360, 321, 528, 281, 853, 281, 4772, 341, 484, 544, 30, 400, 472], "temperature": 0.0, "avg_logprob": -0.0975609370640346, "compression_ratio": 1.7063829787234042, "no_speech_prob": 8.138656085066032e-06}, {"id": 198, "seek": 89864, "start": 902.8, "end": 905.1999999999999, "text": " thing that people often do to balance it out more is that", "tokens": [551, 300, 561, 2049, 360, 281, 4772, 309, 484, 544, 307, 300], "temperature": 0.0, "avg_logprob": -0.0975609370640346, "compression_ratio": 1.7063829787234042, "no_speech_prob": 8.138656085066032e-06}, {"id": 199, "seek": 89864, "start": 905.1999999999999, "end": 909.84, "text": " they'll throw away some of the images in their more", "tokens": [436, 603, 3507, 1314, 512, 295, 264, 5267, 294, 641, 544], "temperature": 0.0, "avg_logprob": -0.0975609370640346, "compression_ratio": 1.7063829787234042, "no_speech_prob": 8.138656085066032e-06}, {"id": 200, "seek": 89864, "start": 909.84, "end": 913.36, "text": " represent the highly represented classes. And I can", "tokens": [2906, 264, 5405, 10379, 5359, 13, 400, 286, 393], "temperature": 0.0, "avg_logprob": -0.0975609370640346, "compression_ratio": 1.7063829787234042, "no_speech_prob": 8.138656085066032e-06}, {"id": 201, "seek": 89864, "start": 913.36, "end": 915.1999999999999, "text": " certainly tell you straight away, you should never ever do", "tokens": [3297, 980, 291, 2997, 1314, 11, 291, 820, 1128, 1562, 360], "temperature": 0.0, "avg_logprob": -0.0975609370640346, "compression_ratio": 1.7063829787234042, "no_speech_prob": 8.138656085066032e-06}, {"id": 202, "seek": 89864, "start": 915.1999999999999, "end": 921.28, "text": " that. You never want to throw away data. But Matt's question", "tokens": [300, 13, 509, 1128, 528, 281, 3507, 1314, 1412, 13, 583, 7397, 311, 1168], "temperature": 0.0, "avg_logprob": -0.0975609370640346, "compression_ratio": 1.7063829787234042, "no_speech_prob": 8.138656085066032e-06}, {"id": 203, "seek": 89864, "start": 921.28, "end": 927.76, "text": " was, well, could we, you know, oversample the less common", "tokens": [390, 11, 731, 11, 727, 321, 11, 291, 458, 11, 15488, 335, 781, 264, 1570, 2689], "temperature": 0.0, "avg_logprob": -0.0975609370640346, "compression_ratio": 1.7063829787234042, "no_speech_prob": 8.138656085066032e-06}, {"id": 204, "seek": 92776, "start": 927.76, "end": 931.92, "text": " diseases? And the answer is, yeah, absolutely, you could.", "tokens": [11044, 30, 400, 264, 1867, 307, 11, 1338, 11, 3122, 11, 291, 727, 13], "temperature": 0.0, "avg_logprob": -0.15029106821332658, "compression_ratio": 1.5168539325842696, "no_speech_prob": 4.1324503399664536e-05}, {"id": 205, "seek": 92776, "start": 932.64, "end": 936.3199999999999, "text": " And in fast AI, if you go into the docs.", "tokens": [400, 294, 2370, 7318, 11, 498, 291, 352, 666, 264, 45623, 13], "temperature": 0.0, "avg_logprob": -0.15029106821332658, "compression_ratio": 1.5168539325842696, "no_speech_prob": 4.1324503399664536e-05}, {"id": 206, "seek": 92776, "start": 942.08, "end": 947.2, "text": " Now, where is it? There is a weighted data loader somewhere.", "tokens": [823, 11, 689, 307, 309, 30, 821, 307, 257, 32807, 1412, 3677, 260, 4079, 13], "temperature": 0.0, "avg_logprob": -0.15029106821332658, "compression_ratio": 1.5168539325842696, "no_speech_prob": 4.1324503399664536e-05}, {"id": 207, "seek": 92776, "start": 948.8, "end": 954.08, "text": " Weighted. Search for that. Here we go. Of course, it's a", "tokens": [44464, 292, 13, 17180, 337, 300, 13, 1692, 321, 352, 13, 2720, 1164, 11, 309, 311, 257], "temperature": 0.0, "avg_logprob": -0.15029106821332658, "compression_ratio": 1.5168539325842696, "no_speech_prob": 4.1324503399664536e-05}, {"id": 208, "seek": 92776, "start": 954.08, "end": 957.52, "text": " callback. So if you go to the callbacks data section,", "tokens": [818, 3207, 13, 407, 498, 291, 352, 281, 264, 818, 17758, 1412, 3541, 11], "temperature": 0.0, "avg_logprob": -0.15029106821332658, "compression_ratio": 1.5168539325842696, "no_speech_prob": 4.1324503399664536e-05}, {"id": 209, "seek": 95752, "start": 957.52, "end": 961.4399999999999, "text": " you'll find a weighted DL callback or a weighted data", "tokens": [291, 603, 915, 257, 32807, 413, 43, 818, 3207, 420, 257, 32807, 1412], "temperature": 0.0, "avg_logprob": -0.11923896434695222, "compression_ratio": 1.435897435897436, "no_speech_prob": 1.061507919075666e-05}, {"id": 210, "seek": 95752, "start": 961.4399999999999, "end": 967.04, "text": " loader's method. Are you sharing your screen?", "tokens": [3677, 260, 311, 3170, 13, 2014, 291, 5414, 428, 2568, 30], "temperature": 0.0, "avg_logprob": -0.11923896434695222, "compression_ratio": 1.435897435897436, "no_speech_prob": 1.061507919075666e-05}, {"id": 211, "seek": 95752, "start": 967.04, "end": 970.3199999999999, "text": " I'm not. No, I'm just telling you where to look. Thanks for", "tokens": [286, 478, 406, 13, 883, 11, 286, 478, 445, 3585, 291, 689, 281, 574, 13, 2561, 337], "temperature": 0.0, "avg_logprob": -0.11923896434695222, "compression_ratio": 1.435897435897436, "no_speech_prob": 1.061507919075666e-05}, {"id": 212, "seek": 95752, "start": 970.3199999999999, "end": 981.4399999999999, "text": " checking. So, yeah, I mean, let's look at that today, right?", "tokens": [8568, 13, 407, 11, 1338, 11, 286, 914, 11, 718, 311, 574, 412, 300, 965, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.11923896434695222, "compression_ratio": 1.435897435897436, "no_speech_prob": 1.061507919075666e-05}, {"id": 213, "seek": 95752, "start": 981.4399999999999, "end": 984.3199999999999, "text": " Because I kind of want to look at like, things we can do to", "tokens": [1436, 286, 733, 295, 528, 281, 574, 412, 411, 11, 721, 321, 393, 360, 281], "temperature": 0.0, "avg_logprob": -0.11923896434695222, "compression_ratio": 1.435897435897436, "no_speech_prob": 1.061507919075666e-05}, {"id": 214, "seek": 98432, "start": 984.32, "end": 991.44, "text": " improve things today. It doesn't necessarily help. Because it", "tokens": [3470, 721, 965, 13, 467, 1177, 380, 4725, 854, 13, 1436, 309], "temperature": 0.0, "avg_logprob": -0.08473793511251801, "compression_ratio": 1.722488038277512, "no_speech_prob": 1.2804070138372481e-05}, {"id": 215, "seek": 98432, "start": 991.44, "end": 995.2800000000001, "text": " does mean, you know, given that you're, you know, let's say you", "tokens": [775, 914, 11, 291, 458, 11, 2212, 300, 291, 434, 11, 291, 458, 11, 718, 311, 584, 291], "temperature": 0.0, "avg_logprob": -0.08473793511251801, "compression_ratio": 1.722488038277512, "no_speech_prob": 1.2804070138372481e-05}, {"id": 216, "seek": 98432, "start": 995.2800000000001, "end": 999.5200000000001, "text": " do 10 epochs of 1000 images, it's going to get to look at", "tokens": [360, 1266, 30992, 28346, 295, 9714, 5267, 11, 309, 311, 516, 281, 483, 281, 574, 412], "temperature": 0.0, "avg_logprob": -0.08473793511251801, "compression_ratio": 1.722488038277512, "no_speech_prob": 1.2804070138372481e-05}, {"id": 217, "seek": 98432, "start": 999.5200000000001, "end": 1003.9200000000001, "text": " 10,000 images, right? And if you oversample a class, then that", "tokens": [1266, 11, 1360, 5267, 11, 558, 30, 400, 498, 291, 15488, 335, 781, 257, 1508, 11, 550, 300], "temperature": 0.0, "avg_logprob": -0.08473793511251801, "compression_ratio": 1.722488038277512, "no_speech_prob": 1.2804070138372481e-05}, {"id": 218, "seek": 98432, "start": 1003.9200000000001, "end": 1007.12, "text": " also means that it's going to get it's going to see less of", "tokens": [611, 1355, 300, 309, 311, 516, 281, 483, 309, 311, 516, 281, 536, 1570, 295], "temperature": 0.0, "avg_logprob": -0.08473793511251801, "compression_ratio": 1.722488038277512, "no_speech_prob": 1.2804070138372481e-05}, {"id": 219, "seek": 98432, "start": 1007.12, "end": 1010.32, "text": " some images and going to get more repetition of other", "tokens": [512, 5267, 293, 516, 281, 483, 544, 30432, 295, 661], "temperature": 0.0, "avg_logprob": -0.08473793511251801, "compression_ratio": 1.722488038277512, "no_speech_prob": 1.2804070138372481e-05}, {"id": 220, "seek": 101032, "start": 1010.32, "end": 1015.2, "text": " images, which could be a problem, you know, and really it", "tokens": [5267, 11, 597, 727, 312, 257, 1154, 11, 291, 458, 11, 293, 534, 309], "temperature": 0.0, "avg_logprob": -0.10360655533640008, "compression_ratio": 1.5217391304347827, "no_speech_prob": 4.565669314615661e-06}, {"id": 221, "seek": 101032, "start": 1015.2, "end": 1021.44, "text": " just depends on a few things. If it's like really unbalanced,", "tokens": [445, 5946, 322, 257, 1326, 721, 13, 759, 309, 311, 411, 534, 517, 40251, 11], "temperature": 0.0, "avg_logprob": -0.10360655533640008, "compression_ratio": 1.5217391304347827, "no_speech_prob": 4.565669314615661e-06}, {"id": 222, "seek": 101032, "start": 1021.44, "end": 1025.28, "text": " like 99% all of one type, then you're going to have a whole", "tokens": [411, 11803, 4, 439, 295, 472, 2010, 11, 550, 291, 434, 516, 281, 362, 257, 1379], "temperature": 0.0, "avg_logprob": -0.10360655533640008, "compression_ratio": 1.5217391304347827, "no_speech_prob": 4.565669314615661e-06}, {"id": 223, "seek": 101032, "start": 1025.28, "end": 1028.4, "text": " lot of batches that it never sees anything of the", "tokens": [688, 295, 15245, 279, 300, 309, 1128, 8194, 1340, 295, 264], "temperature": 0.0, "avg_logprob": -0.10360655533640008, "compression_ratio": 1.5217391304347827, "no_speech_prob": 4.565669314615661e-06}, {"id": 224, "seek": 101032, "start": 1028.4, "end": 1031.2, "text": " underrepresented class. And so basically, there's nothing for", "tokens": [833, 38293, 1508, 13, 400, 370, 1936, 11, 456, 311, 1825, 337], "temperature": 0.0, "avg_logprob": -0.10360655533640008, "compression_ratio": 1.5217391304347827, "no_speech_prob": 4.565669314615661e-06}, {"id": 225, "seek": 101032, "start": 1031.2, "end": 1033.8400000000001, "text": " it to learn from. So at some point, you probably certainly", "tokens": [309, 281, 1466, 490, 13, 407, 412, 512, 935, 11, 291, 1391, 3297], "temperature": 0.0, "avg_logprob": -0.10360655533640008, "compression_ratio": 1.5217391304347827, "no_speech_prob": 4.565669314615661e-06}, {"id": 226, "seek": 103384, "start": 1033.84, "end": 1040.32, "text": " need weighted sampling. It also depends on the evaluation, you", "tokens": [643, 32807, 21179, 13, 467, 611, 5946, 322, 264, 13344, 11, 291], "temperature": 0.0, "avg_logprob": -0.0766593751453218, "compression_ratio": 1.8033472803347281, "no_speech_prob": 1.983112633752171e-05}, {"id": 227, "seek": 103384, "start": 1040.32, "end": 1042.6399999999999, "text": " know, if people like say in the evaluation, okay, we're going", "tokens": [458, 11, 498, 561, 411, 584, 294, 264, 13344, 11, 1392, 11, 321, 434, 516], "temperature": 0.0, "avg_logprob": -0.0766593751453218, "compression_ratio": 1.8033472803347281, "no_speech_prob": 1.983112633752171e-05}, {"id": 228, "seek": 103384, "start": 1042.6399999999999, "end": 1047.4399999999998, "text": " to kind of average out for each disease, how accurate you were.", "tokens": [281, 733, 295, 4274, 484, 337, 1184, 4752, 11, 577, 8559, 291, 645, 13], "temperature": 0.0, "avg_logprob": -0.0766593751453218, "compression_ratio": 1.8033472803347281, "no_speech_prob": 1.983112633752171e-05}, {"id": 229, "seek": 103384, "start": 1047.4399999999998, "end": 1050.1599999999999, "text": " So every disease will then be like equally weighted, then you", "tokens": [407, 633, 4752, 486, 550, 312, 411, 12309, 32807, 11, 550, 291], "temperature": 0.0, "avg_logprob": -0.0766593751453218, "compression_ratio": 1.8033472803347281, "no_speech_prob": 1.983112633752171e-05}, {"id": 230, "seek": 103384, "start": 1050.1599999999999, "end": 1054.48, "text": " would definitely need to use weighted sampling. But in this", "tokens": [576, 2138, 643, 281, 764, 32807, 21179, 13, 583, 294, 341], "temperature": 0.0, "avg_logprob": -0.0766593751453218, "compression_ratio": 1.8033472803347281, "no_speech_prob": 1.983112633752171e-05}, {"id": 231, "seek": 103384, "start": 1054.48, "end": 1059.52, "text": " case, you know, presuming, presuming that the test set has", "tokens": [1389, 11, 291, 458, 11, 18028, 278, 11, 18028, 278, 300, 264, 1500, 992, 575], "temperature": 0.0, "avg_logprob": -0.0766593751453218, "compression_ratio": 1.8033472803347281, "no_speech_prob": 1.983112633752171e-05}, {"id": 232, "seek": 103384, "start": 1059.52, "end": 1063.36, "text": " a similar distribution as the training set, weighted sampling", "tokens": [257, 2531, 7316, 382, 264, 3097, 992, 11, 32807, 21179], "temperature": 0.0, "avg_logprob": -0.0766593751453218, "compression_ratio": 1.8033472803347281, "no_speech_prob": 1.983112633752171e-05}, {"id": 233, "seek": 106336, "start": 1063.36, "end": 1068.56, "text": " might not help. Because they're going to care the most about how", "tokens": [1062, 406, 854, 13, 1436, 436, 434, 516, 281, 1127, 264, 881, 466, 577], "temperature": 0.0, "avg_logprob": -0.12053526531566273, "compression_ratio": 1.6445497630331753, "no_speech_prob": 1.7501732145319693e-05}, {"id": 234, "seek": 106336, "start": 1068.56, "end": 1070.7199999999998, "text": " well we do on the highly represented diseases.", "tokens": [731, 321, 360, 322, 264, 5405, 10379, 11044, 13], "temperature": 0.0, "avg_logprob": -0.12053526531566273, "compression_ratio": 1.6445497630331753, "no_speech_prob": 1.7501732145319693e-05}, {"id": 235, "seek": 106336, "start": 1076.4799999999998, "end": 1079.76, "text": " I'll note my experience with like over sampling and things", "tokens": [286, 603, 3637, 452, 1752, 365, 411, 670, 21179, 293, 721], "temperature": 0.0, "avg_logprob": -0.12053526531566273, "compression_ratio": 1.6445497630331753, "no_speech_prob": 1.7501732145319693e-05}, {"id": 236, "seek": 106336, "start": 1079.76, "end": 1084.8799999999999, "text": " like that. I think one time I had done with I think diabetic", "tokens": [411, 300, 13, 286, 519, 472, 565, 286, 632, 1096, 365, 286, 519, 50238], "temperature": 0.0, "avg_logprob": -0.12053526531566273, "compression_ratio": 1.6445497630331753, "no_speech_prob": 1.7501732145319693e-05}, {"id": 237, "seek": 106336, "start": 1084.8799999999999, "end": 1089.36, "text": " retinopathy, there was a competition for that. And I had", "tokens": [1533, 259, 404, 9527, 11, 456, 390, 257, 6211, 337, 300, 13, 400, 286, 632], "temperature": 0.0, "avg_logprob": -0.12053526531566273, "compression_ratio": 1.6445497630331753, "no_speech_prob": 1.7501732145319693e-05}, {"id": 238, "seek": 106336, "start": 1089.36, "end": 1092.0, "text": " used weighted sampling or over sampling and it did seem to", "tokens": [1143, 32807, 21179, 420, 670, 21179, 293, 309, 630, 1643, 281], "temperature": 0.0, "avg_logprob": -0.12053526531566273, "compression_ratio": 1.6445497630331753, "no_speech_prob": 1.7501732145319693e-05}, {"id": 239, "seek": 109200, "start": 1092.0, "end": 1097.12, "text": " help. And then also a while back, I did an experiment where I", "tokens": [854, 13, 400, 550, 611, 257, 1339, 646, 11, 286, 630, 364, 5120, 689, 286], "temperature": 0.0, "avg_logprob": -0.15295098225275675, "compression_ratio": 1.6561085972850678, "no_speech_prob": 2.07784596568672e-05}, {"id": 240, "seek": 109200, "start": 1097.12, "end": 1100.56, "text": " think this was back with FASAI version one, where I took like", "tokens": [519, 341, 390, 646, 365, 479, 3160, 32, 40, 3037, 472, 11, 689, 286, 1890, 411], "temperature": 0.0, "avg_logprob": -0.15295098225275675, "compression_ratio": 1.6561085972850678, "no_speech_prob": 2.07784596568672e-05}, {"id": 241, "seek": 109200, "start": 1100.56, "end": 1106.8, "text": " the minced data set, and then I, I, like, artificially added", "tokens": [264, 36442, 1412, 992, 11, 293, 550, 286, 11, 286, 11, 411, 11, 39905, 2270, 3869], "temperature": 0.0, "avg_logprob": -0.15295098225275675, "compression_ratio": 1.6561085972850678, "no_speech_prob": 2.07784596568672e-05}, {"id": 242, "seek": 109200, "start": 1106.8, "end": 1112.24, "text": " some sort of imbalance. And then I trained with and without", "tokens": [512, 1333, 295, 43007, 13, 400, 550, 286, 8895, 365, 293, 1553], "temperature": 0.0, "avg_logprob": -0.15295098225275675, "compression_ratio": 1.6561085972850678, "no_speech_prob": 2.07784596568672e-05}, {"id": 243, "seek": 109200, "start": 1112.24, "end": 1115.44, "text": " weighted sampling. And I saw like there was an improvement", "tokens": [32807, 21179, 13, 400, 286, 1866, 411, 456, 390, 364, 10444], "temperature": 0.0, "avg_logprob": -0.15295098225275675, "compression_ratio": 1.6561085972850678, "no_speech_prob": 2.07784596568672e-05}, {"id": 244, "seek": 109200, "start": 1115.44, "end": 1119.6, "text": " with the weighted sampling on accuracy on like, just a regular", "tokens": [365, 264, 32807, 21179, 322, 14170, 322, 411, 11, 445, 257, 3890], "temperature": 0.0, "avg_logprob": -0.15295098225275675, "compression_ratio": 1.6561085972850678, "no_speech_prob": 2.07784596568672e-05}, {"id": 245, "seek": 111960, "start": 1119.6, "end": 1123.9199999999998, "text": " missed validation set. So I, so from that, from those couple", "tokens": [6721, 24071, 992, 13, 407, 286, 11, 370, 490, 300, 11, 490, 729, 1916], "temperature": 0.0, "avg_logprob": -0.11078131198883057, "compression_ratio": 1.6798418972332017, "no_speech_prob": 1.0782414392451756e-05}, {"id": 246, "seek": 111960, "start": 1123.9199999999998, "end": 1127.9199999999998, "text": " experiments, I'd say like, I've at least seen some help and", "tokens": [12050, 11, 286, 1116, 584, 411, 11, 286, 600, 412, 1935, 1612, 512, 854, 293], "temperature": 0.0, "avg_logprob": -0.11078131198883057, "compression_ratio": 1.6798418972332017, "no_speech_prob": 1.0782414392451756e-05}, {"id": 247, "seek": 111960, "start": 1127.9199999999998, "end": 1129.52, "text": " improvement with weighted sampling.", "tokens": [10444, 365, 32807, 21179, 13], "temperature": 0.0, "avg_logprob": -0.11078131198883057, "compression_ratio": 1.6798418972332017, "no_speech_prob": 1.0782414392451756e-05}, {"id": 248, "seek": 111960, "start": 1130.08, "end": 1133.6799999999998, "text": " Cool. And was that cases where that data set was like, highly", "tokens": [8561, 13, 400, 390, 300, 3331, 689, 300, 1412, 992, 390, 411, 11, 5405], "temperature": 0.0, "avg_logprob": -0.11078131198883057, "compression_ratio": 1.6798418972332017, "no_speech_prob": 1.0782414392451756e-05}, {"id": 249, "seek": 111960, "start": 1133.6799999999998, "end": 1136.48, "text": " unbalanced? Or was it more like the data set that we're", "tokens": [517, 40251, 30, 1610, 390, 309, 544, 411, 264, 1412, 992, 300, 321, 434], "temperature": 0.0, "avg_logprob": -0.11078131198883057, "compression_ratio": 1.6798418972332017, "no_speech_prob": 1.0782414392451756e-05}, {"id": 250, "seek": 111960, "start": 1136.48, "end": 1137.9199999999998, "text": " looking at, at the moment?", "tokens": [1237, 412, 11, 412, 264, 1623, 30], "temperature": 0.0, "avg_logprob": -0.11078131198883057, "compression_ratio": 1.6798418972332017, "no_speech_prob": 1.0782414392451756e-05}, {"id": 251, "seek": 111960, "start": 1139.4399999999998, "end": 1142.1599999999999, "text": " It wasn't highly unbalanced. It was maybe like, I don't know,", "tokens": [467, 2067, 380, 5405, 517, 40251, 13, 467, 390, 1310, 411, 11, 286, 500, 380, 458, 11], "temperature": 0.0, "avg_logprob": -0.11078131198883057, "compression_ratio": 1.6798418972332017, "no_speech_prob": 1.0782414392451756e-05}, {"id": 252, "seek": 111960, "start": 1142.1599999999999, "end": 1148.56, "text": " like, maybe like, yeah, just 75% versus 25% or something like", "tokens": [411, 11, 1310, 411, 11, 1338, 11, 445, 9562, 4, 5717, 3552, 4, 420, 746, 411], "temperature": 0.0, "avg_logprob": -0.11078131198883057, "compression_ratio": 1.6798418972332017, "no_speech_prob": 1.0782414392451756e-05}, {"id": 253, "seek": 114856, "start": 1148.56, "end": 1152.32, "text": " that. It's not like 99, 99 versus 1%, nothing like that. It", "tokens": [300, 13, 467, 311, 406, 411, 11803, 11, 11803, 5717, 502, 8923, 1825, 411, 300, 13, 467], "temperature": 0.0, "avg_logprob": -0.1734777709185067, "compression_ratio": 1.5210084033613445, "no_speech_prob": 3.535260111675598e-05}, {"id": 254, "seek": 114856, "start": 1152.32, "end": 1154.56, "text": " was more, it wasn't that bad.", "tokens": [390, 544, 11, 309, 2067, 380, 300, 1578, 13], "temperature": 0.0, "avg_logprob": -0.1734777709185067, "compression_ratio": 1.5210084033613445, "no_speech_prob": 3.535260111675598e-05}, {"id": 255, "seek": 114856, "start": 1154.56, "end": 1158.48, "text": " Let's try it today. Yeah. I see we've got a new face today as", "tokens": [961, 311, 853, 309, 965, 13, 865, 13, 286, 536, 321, 600, 658, 257, 777, 1851, 965, 382], "temperature": 0.0, "avg_logprob": -0.1734777709185067, "compression_ratio": 1.5210084033613445, "no_speech_prob": 3.535260111675598e-05}, {"id": 256, "seek": 114856, "start": 1158.48, "end": 1161.28, "text": " well. Hello, Zach. Thanks for joining.", "tokens": [731, 13, 2425, 11, 21028, 13, 2561, 337, 5549, 13], "temperature": 0.0, "avg_logprob": -0.1734777709185067, "compression_ratio": 1.5210084033613445, "no_speech_prob": 3.535260111675598e-05}, {"id": 257, "seek": 114856, "start": 1163.6799999999998, "end": 1165.52, "text": " Hey, hey, glad I could finally make these.", "tokens": [1911, 11, 4177, 11, 5404, 286, 727, 2721, 652, 613, 13], "temperature": 0.0, "avg_logprob": -0.1734777709185067, "compression_ratio": 1.5210084033613445, "no_speech_prob": 3.535260111675598e-05}, {"id": 258, "seek": 114856, "start": 1165.52, "end": 1168.3999999999999, "text": " Yeah. Are you joining from Florida?", "tokens": [865, 13, 2014, 291, 5549, 490, 9117, 30], "temperature": 0.0, "avg_logprob": -0.1734777709185067, "compression_ratio": 1.5210084033613445, "no_speech_prob": 3.535260111675598e-05}, {"id": 259, "seek": 114856, "start": 1169.44, "end": 1171.6799999999998, "text": " No, I'm in Maryland now.", "tokens": [883, 11, 286, 478, 294, 19939, 586, 13], "temperature": 0.0, "avg_logprob": -0.1734777709185067, "compression_ratio": 1.5210084033613445, "no_speech_prob": 3.535260111675598e-05}, {"id": 260, "seek": 114856, "start": 1171.6799999999998, "end": 1174.0, "text": " Maryland now. Okay. That's quite a change.", "tokens": [19939, 586, 13, 1033, 13, 663, 311, 1596, 257, 1319, 13], "temperature": 0.0, "avg_logprob": -0.1734777709185067, "compression_ratio": 1.5210084033613445, "no_speech_prob": 3.535260111675598e-05}, {"id": 261, "seek": 117400, "start": 1174.0, "end": 1177.84, "text": " Yes. Much more up North.", "tokens": [1079, 13, 12313, 544, 493, 4067, 13], "temperature": 0.0, "avg_logprob": -0.3112504061530618, "compression_ratio": 1.1219512195121952, "no_speech_prob": 1.5686677215853706e-05}, {"id": 262, "seek": 117400, "start": 1181.68, "end": 1186.8, "text": " Okay. Great. So let's, let's try something.", "tokens": [1033, 13, 3769, 13, 407, 718, 311, 11, 718, 311, 853, 746, 13], "temperature": 0.0, "avg_logprob": -0.3112504061530618, "compression_ratio": 1.1219512195121952, "no_speech_prob": 1.5686677215853706e-05}, {"id": 263, "seek": 118680, "start": 1186.8, "end": 1209.04, "text": " Okay. So let's connect to my little computer upstairs.", "tokens": [50364, 1033, 13, 407, 718, 311, 1745, 281, 452, 707, 3820, 16462, 13, 51476], "temperature": 0.0, "avg_logprob": -0.48783782323201497, "compression_ratio": 0.8852459016393442, "no_speech_prob": 1.8337917936150916e-05}, {"id": 264, "seek": 121680, "start": 1217.04, "end": 1223.2, "text": " Is there a way to shrink my zoom out of the way? It takes up so", "tokens": [1119, 456, 257, 636, 281, 23060, 452, 8863, 484, 295, 264, 636, 30, 467, 2516, 493, 370], "temperature": 0.0, "avg_logprob": -0.2360718424727277, "compression_ratio": 1.5483870967741935, "no_speech_prob": 0.0018671875586733222}, {"id": 265, "seek": 121680, "start": 1223.2, "end": 1227.6, "text": " much space. Hide floating meeting controls. I guess that's", "tokens": [709, 1901, 13, 35118, 12607, 3440, 9003, 13, 286, 2041, 300, 311], "temperature": 0.0, "avg_logprob": -0.2360718424727277, "compression_ratio": 1.5483870967741935, "no_speech_prob": 0.0018671875586733222}, {"id": 266, "seek": 121680, "start": 1227.6, "end": 1230.08, "text": " what I want. Control alt shift H.", "tokens": [437, 286, 528, 13, 12912, 4955, 5513, 389, 13], "temperature": 0.0, "avg_logprob": -0.2360718424727277, "compression_ratio": 1.5483870967741935, "no_speech_prob": 0.0018671875586733222}, {"id": 267, "seek": 121680, "start": 1231.44, "end": 1235.04, "text": " Wow. Press escape to show floating meeting controls.", "tokens": [3153, 13, 6776, 7615, 281, 855, 12607, 3440, 9003, 13], "temperature": 0.0, "avg_logprob": -0.2360718424727277, "compression_ratio": 1.5483870967741935, "no_speech_prob": 0.0018671875586733222}, {"id": 268, "seek": 121680, "start": 1235.84, "end": 1237.76, "text": " That doesn't work very well with VIM.", "tokens": [663, 1177, 380, 589, 588, 731, 365, 691, 6324, 13], "temperature": 0.0, "avg_logprob": -0.2360718424727277, "compression_ratio": 1.5483870967741935, "no_speech_prob": 0.0018671875586733222}, {"id": 269, "seek": 123776, "start": 1237.76, "end": 1248.4, "text": " Oh, well, control alt shift H. Okay. All right. We're not", "tokens": [876, 11, 731, 11, 1969, 4955, 5513, 389, 13, 1033, 13, 1057, 558, 13, 492, 434, 406], "temperature": 0.0, "avg_logprob": -0.19723787847554908, "compression_ratio": 1.2265625, "no_speech_prob": 2.521514716136153e-06}, {"id": 270, "seek": 123776, "start": 1248.4, "end": 1250.48, "text": " doing tabular today. So let's get rid of that.", "tokens": [884, 4421, 1040, 965, 13, 407, 718, 311, 483, 3973, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.19723787847554908, "compression_ratio": 1.2265625, "no_speech_prob": 2.521514716136153e-06}, {"id": 271, "seek": 123776, "start": 1258.48, "end": 1261.6, "text": " So I think what I might do is, you know, cause we're", "tokens": [407, 286, 519, 437, 286, 1062, 360, 307, 11, 291, 458, 11, 3082, 321, 434], "temperature": 0.0, "avg_logprob": -0.19723787847554908, "compression_ratio": 1.2265625, "no_speech_prob": 2.521514716136153e-06}, {"id": 272, "seek": 126160, "start": 1261.6, "end": 1270.8799999999999, "text": " iterating. Well, I guess we could start with the multitask", "tokens": [17138, 990, 13, 1042, 11, 286, 2041, 321, 727, 722, 365, 264, 42338, 3863], "temperature": 0.0, "avg_logprob": -0.22991940849705747, "compression_ratio": 1.1962616822429906, "no_speech_prob": 8.664134838909376e-06}, {"id": 273, "seek": 126160, "start": 1270.8799999999999, "end": 1277.1999999999998, "text": " one. Cause this is our kind of like things to try to improve", "tokens": [472, 13, 10865, 341, 307, 527, 733, 295, 411, 721, 281, 853, 281, 3470], "temperature": 0.0, "avg_logprob": -0.22991940849705747, "compression_ratio": 1.1962616822429906, "no_speech_prob": 8.664134838909376e-06}, {"id": 274, "seek": 126160, "start": 1279.76, "end": 1280.24, "text": " version.", "tokens": [3037, 13], "temperature": 0.0, "avg_logprob": -0.22991940849705747, "compression_ratio": 1.1962616822429906, "no_speech_prob": 8.664134838909376e-06}, {"id": 275, "seek": 128024, "start": 1280.24, "end": 1294.64, "text": " So close that. I'll leave that open just in case we want it.", "tokens": [407, 1998, 300, 13, 286, 603, 1856, 300, 1269, 445, 294, 1389, 321, 528, 309, 13], "temperature": 0.0, "avg_logprob": -0.15328435544614438, "compression_ratio": 1.3014705882352942, "no_speech_prob": 9.368007340526674e-06}, {"id": 276, "seek": 128024, "start": 1295.92, "end": 1302.4, "text": " Okay. By the way, if you've got multiple GPUs, this is how you", "tokens": [1033, 13, 3146, 264, 636, 11, 498, 291, 600, 658, 3866, 18407, 82, 11, 341, 307, 577, 291], "temperature": 0.0, "avg_logprob": -0.15328435544614438, "compression_ratio": 1.3014705882352942, "no_speech_prob": 9.368007340526674e-06}, {"id": 277, "seek": 128024, "start": 1302.4, "end": 1304.88, "text": " just use one of them. You can just set an environment", "tokens": [445, 764, 472, 295, 552, 13, 509, 393, 445, 992, 364, 2823], "temperature": 0.0, "avg_logprob": -0.15328435544614438, "compression_ratio": 1.3014705882352942, "no_speech_prob": 9.368007340526674e-06}, {"id": 278, "seek": 130488, "start": 1304.88, "end": 1329.1200000000001, "text": " variable. Okay. So this is where we did the multi target model.", "tokens": [50364, 7006, 13, 1033, 13, 407, 341, 307, 689, 321, 630, 264, 4825, 3779, 2316, 13, 51576], "temperature": 0.0, "avg_logprob": -0.19612043433719212, "compression_ratio": 0.9402985074626866, "no_speech_prob": 1.2606436030182522e-05}, {"id": 279, "seek": 133488, "start": 1335.5200000000002, "end": 1346.48, "text": " Okay. Just moved everything slightly.", "tokens": [1033, 13, 1449, 4259, 1203, 4748, 13], "temperature": 0.0, "avg_logprob": -0.6686495228817588, "compression_ratio": 0.98, "no_speech_prob": 0.002630889415740967}, {"id": 280, "seek": 133488, "start": 1352.0, "end": 1352.3200000000002, "text": " Comp.", "tokens": [6620, 13], "temperature": 0.0, "avg_logprob": -0.6686495228817588, "compression_ratio": 0.98, "no_speech_prob": 0.002630889415740967}, {"id": 281, "seek": 133488, "start": 1355.7600000000002, "end": 1356.0800000000002, "text": " Comp.", "tokens": [6620, 13], "temperature": 0.0, "avg_logprob": -0.6686495228817588, "compression_ratio": 0.98, "no_speech_prob": 0.002630889415740967}, {"id": 282, "seek": 135608, "start": 1356.08, "end": 1368.0, "text": " Not comp path. Right. Back to where we were. Okay.", "tokens": [1726, 715, 3100, 13, 1779, 13, 5833, 281, 689, 321, 645, 13, 1033, 13], "temperature": 0.0, "avg_logprob": -0.34363335576550713, "compression_ratio": 0.9629629629629629, "no_speech_prob": 2.0143283109064214e-05}, {"id": 283, "seek": 135608, "start": 1371.6, "end": 1378.24, "text": " So now what?", "tokens": [407, 586, 437, 30], "temperature": 0.0, "avg_logprob": -0.34363335576550713, "compression_ratio": 0.9629629629629629, "no_speech_prob": 2.0143283109064214e-05}, {"id": 284, "seek": 137824, "start": 1378.24, "end": 1391.84, "text": " What's broken? Data block. Get image files.", "tokens": [708, 311, 5463, 30, 11888, 3461, 13, 3240, 3256, 7098, 13], "temperature": 0.0, "avg_logprob": -0.4815558751424154, "compression_ratio": 0.8431372549019608, "no_speech_prob": 1.7777341781766154e-05}, {"id": 285, "seek": 139184, "start": 1391.84, "end": 1409.36, "text": " Well, this is working the other day. So I guess we better try to", "tokens": [1042, 11, 341, 307, 1364, 264, 661, 786, 13, 407, 286, 2041, 321, 1101, 853, 281], "temperature": 0.0, "avg_logprob": -0.13160170146397182, "compression_ratio": 1.2448979591836735, "no_speech_prob": 8.800283467280678e-06}, {"id": 286, "seek": 139184, "start": 1409.36, "end": 1417.52, "text": " do some debugging. So the obvious thing to do would be to", "tokens": [360, 512, 45592, 13, 407, 264, 6322, 551, 281, 360, 576, 312, 281], "temperature": 0.0, "avg_logprob": -0.13160170146397182, "compression_ratio": 1.2448979591836735, "no_speech_prob": 8.800283467280678e-06}, {"id": 287, "seek": 141752, "start": 1417.52, "end": 1421.84, "text": " call this thing here, get image files on the thing that we", "tokens": [818, 341, 551, 510, 11, 483, 3256, 7098, 322, 264, 551, 300, 321], "temperature": 0.0, "avg_logprob": -0.09595158023218955, "compression_ratio": 1.528169014084507, "no_speech_prob": 1.14782060336438e-05}, {"id": 288, "seek": 141752, "start": 1421.84, "end": 1427.84, "text": " passed in here, which is train path. Okay. So that's working.", "tokens": [4678, 294, 510, 11, 597, 307, 3847, 3100, 13, 1033, 13, 407, 300, 311, 1364, 13], "temperature": 0.0, "avg_logprob": -0.09595158023218955, "compression_ratio": 1.528169014084507, "no_speech_prob": 1.14782060336438e-05}, {"id": 289, "seek": 141752, "start": 1429.12, "end": 1433.36, "text": " Then the other thing to do would be to check our data by", "tokens": [1396, 264, 661, 551, 281, 360, 576, 312, 281, 1520, 527, 1412, 538], "temperature": 0.0, "avg_logprob": -0.09595158023218955, "compression_ratio": 1.528169014084507, "no_speech_prob": 1.14782060336438e-05}, {"id": 290, "seek": 141752, "start": 1433.36, "end": 1438.6399999999999, "text": " doing show batch. Okay. That's working.", "tokens": [884, 855, 15245, 13, 1033, 13, 663, 311, 1364, 13], "temperature": 0.0, "avg_logprob": -0.09595158023218955, "compression_ratio": 1.528169014084507, "no_speech_prob": 1.14782060336438e-05}, {"id": 291, "seek": 143864, "start": 1438.64, "end": 1447.0400000000002, "text": " Then I guess, all right, and it's showing our two different", "tokens": [1396, 286, 2041, 11, 439, 558, 11, 293, 309, 311, 4099, 527, 732, 819], "temperature": 0.0, "avg_logprob": -0.19070613861083985, "compression_ratio": 1.2439024390243902, "no_speech_prob": 6.7478740675142035e-06}, {"id": 292, "seek": 143864, "start": 1447.0400000000002, "end": 1453.8400000000001, "text": " things. That's good. Oh, is it?", "tokens": [721, 13, 663, 311, 665, 13, 876, 11, 307, 309, 30], "temperature": 0.0, "avg_logprob": -0.19070613861083985, "compression_ratio": 1.2439024390243902, "no_speech_prob": 6.7478740675142035e-06}, {"id": 293, "seek": 143864, "start": 1456.8000000000002, "end": 1462.3200000000002, "text": " Oh, right. We've got the two category blocks. So we can't use", "tokens": [876, 11, 558, 13, 492, 600, 658, 264, 732, 7719, 8474, 13, 407, 321, 393, 380, 764], "temperature": 0.0, "avg_logprob": -0.19070613861083985, "compression_ratio": 1.2439024390243902, "no_speech_prob": 6.7478740675142035e-06}, {"id": 294, "seek": 146232, "start": 1462.32, "end": 1474.6399999999999, "text": " this one. We have to use this one. So fit one cycle.", "tokens": [341, 472, 13, 492, 362, 281, 764, 341, 472, 13, 407, 3318, 472, 6586, 13], "temperature": 0.0, "avg_logprob": -0.1424370941362883, "compression_ratio": 1.2777777777777777, "no_speech_prob": 6.854103503428632e-06}, {"id": 295, "seek": 146232, "start": 1480.08, "end": 1486.24, "text": " Yeah, okay. So to remind you, we have this is the one where we", "tokens": [865, 11, 1392, 13, 407, 281, 4160, 291, 11, 321, 362, 341, 307, 264, 472, 689, 321], "temperature": 0.0, "avg_logprob": -0.1424370941362883, "compression_ratio": 1.2777777777777777, "no_speech_prob": 6.854103503428632e-06}, {"id": 296, "seek": 148624, "start": 1486.24, "end": 1496.56, "text": " had two categories and one input. And to get the two", "tokens": [632, 732, 10479, 293, 472, 4846, 13, 400, 281, 483, 264, 732], "temperature": 0.0, "avg_logprob": -0.10186673402786255, "compression_ratio": 1.4017857142857142, "no_speech_prob": 4.425418865139363e-06}, {"id": 297, "seek": 148624, "start": 1496.56, "end": 1501.28, "text": " categories, we use the parent label and this function, which", "tokens": [10479, 11, 321, 764, 264, 2596, 7645, 293, 341, 2445, 11, 597], "temperature": 0.0, "avg_logprob": -0.10186673402786255, "compression_ratio": 1.4017857142857142, "no_speech_prob": 4.425418865139363e-06}, {"id": 298, "seek": 148624, "start": 1501.28, "end": 1505.44, "text": " looked up the variety from this dictionary.", "tokens": [2956, 493, 264, 5673, 490, 341, 25890, 13], "temperature": 0.0, "avg_logprob": -0.10186673402786255, "compression_ratio": 1.4017857142857142, "no_speech_prob": 4.425418865139363e-06}, {"id": 299, "seek": 150544, "start": 1505.44, "end": 1513.04, "text": " Okay. And then when we fine-tuned it,", "tokens": [1033, 13, 400, 550, 562, 321, 2489, 12, 83, 43703, 309, 11], "temperature": 0.0, "avg_logprob": -0.1930325698852539, "compression_ratio": 1.2307692307692308, "no_speech_prob": 7.766454473312479e-06}, {"id": 300, "seek": 150544, "start": 1516.64, "end": 1520.16, "text": " and let's just check, yes, C equals 42. So that's our standard", "tokens": [293, 718, 311, 445, 1520, 11, 2086, 11, 383, 6915, 14034, 13, 407, 300, 311, 527, 3832], "temperature": 0.0, "avg_logprob": -0.1930325698852539, "compression_ratio": 1.2307692307692308, "no_speech_prob": 7.766454473312479e-06}, {"id": 301, "seek": 150544, "start": 1521.1200000000001, "end": 1524.88, "text": " set. We should be able to then compare that to small models", "tokens": [992, 13, 492, 820, 312, 1075, 281, 550, 6794, 300, 281, 1359, 5245], "temperature": 0.0, "avg_logprob": -0.1930325698852539, "compression_ratio": 1.2307692307692308, "no_speech_prob": 7.766454473312479e-06}, {"id": 302, "seek": 152488, "start": 1524.88, "end": 1537.44, "text": " trained for 12 epochs. And then that was this one.", "tokens": [8895, 337, 2272, 30992, 28346, 13, 400, 550, 300, 390, 341, 472, 13], "temperature": 0.0, "avg_logprob": -0.5347378594534737, "compression_ratio": 0.9230769230769231, "no_speech_prob": 1.2028395758534316e-05}, {"id": 303, "seek": 153744, "start": 1537.44, "end": 1547.6000000000001, "text": " Part two.", "tokens": [4100, 732, 13], "temperature": 0.0, "avg_logprob": -0.9010657582964215, "compression_ratio": 0.5294117647058824, "no_speech_prob": 4.83016119687818e-05}, {"id": 304, "seek": 154760, "start": 1547.6, "end": 1567.76, "text": " And let's see. They're not quite the same because", "tokens": [400, 718, 311, 536, 13, 814, 434, 406, 1596, 264, 912, 570], "temperature": 0.0, "avg_logprob": -0.2600971758365631, "compression_ratio": 0.8596491228070176, "no_speech_prob": 1.3628806300403085e-05}, {"id": 305, "seek": 156776, "start": 1567.76, "end": 1583.92, "text": " this was 480 squish, or else this was rectangular pad.", "tokens": [341, 390, 1017, 4702, 31379, 11, 420, 1646, 341, 390, 31167, 6887, 13], "temperature": 0.0, "avg_logprob": -0.3213034543124112, "compression_ratio": 1.1494252873563218, "no_speech_prob": 1.3210908036853652e-05}, {"id": 306, "seek": 156776, "start": 1589.28, "end": 1592.72, "text": " Let's do five epochs. Let's do it the same as", "tokens": [961, 311, 360, 1732, 30992, 28346, 13, 961, 311, 360, 309, 264, 912, 382], "temperature": 0.0, "avg_logprob": -0.3213034543124112, "compression_ratio": 1.1494252873563218, "no_speech_prob": 1.3210908036853652e-05}, {"id": 307, "seek": 159272, "start": 1592.72, "end": 1597.1200000000001, "text": " this one.", "tokens": [341, 472, 13], "temperature": 0.0, "avg_logprob": -0.23417491912841798, "compression_ratio": 1.14, "no_speech_prob": 2.0461433450691402e-05}, {"id": 308, "seek": 159272, "start": 1602.0, "end": 1604.48, "text": " Yeah, let's do this one because we want to be able to do quick", "tokens": [865, 11, 718, 311, 360, 341, 472, 570, 321, 528, 281, 312, 1075, 281, 360, 1702], "temperature": 0.0, "avg_logprob": -0.23417491912841798, "compression_ratio": 1.14, "no_speech_prob": 2.0461433450691402e-05}, {"id": 309, "seek": 159272, "start": 1604.48, "end": 1605.1200000000001, "text": " iterations.", "tokens": [36540, 13], "temperature": 0.0, "avg_logprob": -0.23417491912841798, "compression_ratio": 1.14, "no_speech_prob": 2.0461433450691402e-05}, {"id": 310, "seek": 160512, "start": 1605.12, "end": 1619.6799999999998, "text": " Let's see. Resize 192 squish.", "tokens": [961, 311, 536, 13, 5015, 1125, 1294, 17, 31379, 13], "temperature": 0.0, "avg_logprob": -0.5246259144374302, "compression_ratio": 0.7837837837837838, "no_speech_prob": 4.830626494367607e-05}, {"id": 311, "seek": 161968, "start": 1619.68, "end": 1629.3600000000001, "text": " There we go.", "tokens": [821, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.26702156933871185, "compression_ratio": 0.9166666666666666, "no_speech_prob": 8.011954378162045e-06}, {"id": 312, "seek": 162936, "start": 1629.36, "end": 1646.24, "text": " And then we trained it for 0.01 with FP16.", "tokens": [400, 550, 321, 8895, 309, 337, 1958, 13, 10607, 365, 36655, 6866, 13], "temperature": 0.0, "avg_logprob": -0.4177858408759622, "compression_ratio": 0.84, "no_speech_prob": 2.295827925991034e-06}, {"id": 313, "seek": 164624, "start": 1646.24, "end": 1654.8, "text": " And five epochs.", "tokens": [400, 1732, 30992, 28346, 13], "temperature": 0.0, "avg_logprob": -0.2177303275283502, "compression_ratio": 1.3047619047619048, "no_speech_prob": 4.7106532292673364e-06}, {"id": 314, "seek": 164624, "start": 1658.4, "end": 1664.8, "text": " All right. So this will be our base case. Well, you know, I", "tokens": [1057, 558, 13, 407, 341, 486, 312, 527, 3096, 1389, 13, 1042, 11, 291, 458, 11, 286], "temperature": 0.0, "avg_logprob": -0.2177303275283502, "compression_ratio": 1.3047619047619048, "no_speech_prob": 4.7106532292673364e-06}, {"id": 315, "seek": 164624, "start": 1664.8, "end": 1671.76, "text": " mean, I guess this is our base case. 0.045. This will be our", "tokens": [914, 11, 286, 2041, 341, 307, 527, 3096, 1389, 13, 1958, 13, 15, 8465, 13, 639, 486, 312, 527], "temperature": 0.0, "avg_logprob": -0.2177303275283502, "compression_ratio": 1.3047619047619048, "no_speech_prob": 4.7106532292673364e-06}, {"id": 316, "seek": 167176, "start": 1671.76, "end": 1681.04, "text": " next case. Okay. So while that's running, the next thing I", "tokens": [958, 1389, 13, 1033, 13, 407, 1339, 300, 311, 2614, 11, 264, 958, 551, 286], "temperature": 0.0, "avg_logprob": -0.14866509437561035, "compression_ratio": 1.1063829787234043, "no_speech_prob": 9.874473789750482e-07}, {"id": 317, "seek": 167176, "start": 1681.04, "end": 1686.96, "text": " wanted to talk about is progressive resizing.", "tokens": [1415, 281, 751, 466, 307, 16131, 725, 3319, 13], "temperature": 0.0, "avg_logprob": -0.14866509437561035, "compression_ratio": 1.1063829787234043, "no_speech_prob": 9.874473789750482e-07}, {"id": 318, "seek": 168696, "start": 1686.96, "end": 1713.8400000000001, "text": " So this is training at a size of 128, which is not very big,", "tokens": [407, 341, 307, 3097, 412, 257, 2744, 295, 29810, 11, 597, 307, 406, 588, 955, 11], "temperature": 0.0, "avg_logprob": -0.1979335993528366, "compression_ratio": 1.0842105263157895, "no_speech_prob": 2.1906737401877763e-06}, {"id": 319, "seek": 171384, "start": 1713.84, "end": 1719.52, "text": " and we wouldn't expect it to do very well. So, but it's", "tokens": [293, 321, 2759, 380, 2066, 309, 281, 360, 588, 731, 13, 407, 11, 457, 309, 311], "temperature": 0.0, "avg_logprob": -0.12588743368784586, "compression_ratio": 1.5913978494623655, "no_speech_prob": 4.1325642087031156e-05}, {"id": 320, "seek": 171384, "start": 1719.52, "end": 1723.52, "text": " certainly better than nothing. And as you can see, it's not", "tokens": [3297, 1101, 813, 1825, 13, 400, 382, 291, 393, 536, 11, 309, 311, 406], "temperature": 0.0, "avg_logprob": -0.12588743368784586, "compression_ratio": 1.5913978494623655, "no_speech_prob": 4.1325642087031156e-05}, {"id": 321, "seek": 171384, "start": 1723.52, "end": 1727.1999999999998, "text": " error, disease error, it's down to 7.5% error already, and it's", "tokens": [6713, 11, 4752, 6713, 11, 309, 311, 760, 281, 1614, 13, 20, 4, 6713, 1217, 11, 293, 309, 311], "temperature": 0.0, "avg_logprob": -0.12588743368784586, "compression_ratio": 1.5913978494623655, "no_speech_prob": 4.1325642087031156e-05}, {"id": 322, "seek": 171384, "start": 1727.1999999999998, "end": 1736.3999999999999, "text": " not even done. So that's not bad. And, you know, in the past,", "tokens": [406, 754, 1096, 13, 407, 300, 311, 406, 1578, 13, 400, 11, 291, 458, 11, 294, 264, 1791, 11], "temperature": 0.0, "avg_logprob": -0.12588743368784586, "compression_ratio": 1.5913978494623655, "no_speech_prob": 4.1325642087031156e-05}, {"id": 323, "seek": 171384, "start": 1736.3999999999999, "end": 1738.1599999999999, "text": " what we've then done is we've said, okay, well, that's", "tokens": [437, 321, 600, 550, 1096, 307, 321, 600, 848, 11, 1392, 11, 731, 11, 300, 311], "temperature": 0.0, "avg_logprob": -0.12588743368784586, "compression_ratio": 1.5913978494623655, "no_speech_prob": 4.1325642087031156e-05}, {"id": 324, "seek": 173816, "start": 1738.16, "end": 1747.92, "text": " working pretty well. Let's throw that away and try bigger.", "tokens": [1364, 1238, 731, 13, 961, 311, 3507, 300, 1314, 293, 853, 3801, 13], "temperature": 0.0, "avg_logprob": -0.13300789618978695, "compression_ratio": 1.3507462686567164, "no_speech_prob": 1.4508406820823438e-05}, {"id": 325, "seek": 173816, "start": 1748.96, "end": 1752.5600000000002, "text": " But there's actually something more interesting we can do,", "tokens": [583, 456, 311, 767, 746, 544, 1880, 321, 393, 360, 11], "temperature": 0.0, "avg_logprob": -0.13300789618978695, "compression_ratio": 1.3507462686567164, "no_speech_prob": 1.4508406820823438e-05}, {"id": 326, "seek": 173816, "start": 1754.64, "end": 1761.0400000000002, "text": " which is we don't have to throw it away. What we could do is to", "tokens": [597, 307, 321, 500, 380, 362, 281, 3507, 309, 1314, 13, 708, 321, 727, 360, 307, 281], "temperature": 0.0, "avg_logprob": -0.13300789618978695, "compression_ratio": 1.3507462686567164, "no_speech_prob": 1.4508406820823438e-05}, {"id": 327, "seek": 176104, "start": 1761.04, "end": 1767.92, "text": " continue training it on larger images. So we're basically", "tokens": [2354, 3097, 309, 322, 4833, 5267, 13, 407, 321, 434, 1936], "temperature": 0.0, "avg_logprob": -0.18909023381486723, "compression_ratio": 1.548913043478261, "no_speech_prob": 9.817872523854021e-06}, {"id": 328, "seek": 176104, "start": 1767.92, "end": 1769.92, "text": " saying, okay, this is a model which is fine tuned to", "tokens": [1566, 11, 1392, 11, 341, 307, 257, 2316, 597, 307, 2489, 10870, 281], "temperature": 0.0, "avg_logprob": -0.18909023381486723, "compression_ratio": 1.548913043478261, "no_speech_prob": 9.817872523854021e-06}, {"id": 329, "seek": 176104, "start": 1769.92, "end": 1780.72, "text": " recognize 128 by 128 pixel images of rice. That's fine", "tokens": [5521, 29810, 538, 29810, 19261, 5267, 295, 5090, 13, 663, 311, 2489], "temperature": 0.0, "avg_logprob": -0.18909023381486723, "compression_ratio": 1.548913043478261, "no_speech_prob": 9.817872523854021e-06}, {"id": 330, "seek": 176104, "start": 1780.72, "end": 1785.68, "text": " tune it to recognize 192 by 192 pixel images of rice. And we", "tokens": [10864, 309, 281, 5521, 1294, 17, 538, 1294, 17, 19261, 5267, 295, 5090, 13, 400, 321], "temperature": 0.0, "avg_logprob": -0.18909023381486723, "compression_ratio": 1.548913043478261, "no_speech_prob": 9.817872523854021e-06}, {"id": 331, "seek": 176104, "start": 1785.68, "end": 1789.2, "text": " could even like, and like, there's a few benefits to that.", "tokens": [727, 754, 411, 11, 293, 411, 11, 456, 311, 257, 1326, 5311, 281, 300, 13], "temperature": 0.0, "avg_logprob": -0.18909023381486723, "compression_ratio": 1.548913043478261, "no_speech_prob": 9.817872523854021e-06}, {"id": 332, "seek": 178920, "start": 1789.2, "end": 1793.3600000000001, "text": " One is like, it's very fast, you know, to do the smaller", "tokens": [1485, 307, 411, 11, 309, 311, 588, 2370, 11, 291, 458, 11, 281, 360, 264, 4356], "temperature": 0.0, "avg_logprob": -0.1479458988837476, "compression_ratio": 1.7641921397379912, "no_speech_prob": 2.4297320123878308e-05}, {"id": 333, "seek": 178920, "start": 1793.3600000000001, "end": 1801.3600000000001, "text": " images. And it can recognize the key features of it. So, you", "tokens": [5267, 13, 400, 309, 393, 5521, 264, 2141, 4122, 295, 309, 13, 407, 11, 291], "temperature": 0.0, "avg_logprob": -0.1479458988837476, "compression_ratio": 1.7641921397379912, "no_speech_prob": 2.4297320123878308e-05}, {"id": 334, "seek": 178920, "start": 1801.3600000000001, "end": 1806.72, "text": " know, this lets us do a lot of epochs quickly. And then like", "tokens": [458, 11, 341, 6653, 505, 360, 257, 688, 295, 30992, 28346, 2661, 13, 400, 550, 411], "temperature": 0.0, "avg_logprob": -0.1479458988837476, "compression_ratio": 1.7641921397379912, "no_speech_prob": 2.4297320123878308e-05}, {"id": 335, "seek": 178920, "start": 1806.72, "end": 1808.88, "text": " the difference between small images of rice disease and", "tokens": [264, 2649, 1296, 1359, 5267, 295, 5090, 4752, 293], "temperature": 0.0, "avg_logprob": -0.1479458988837476, "compression_ratio": 1.7641921397379912, "no_speech_prob": 2.4297320123878308e-05}, {"id": 336, "seek": 178920, "start": 1808.88, "end": 1812.88, "text": " large images of rice disease isn't very big difference. So", "tokens": [2416, 5267, 295, 5090, 4752, 1943, 380, 588, 955, 2649, 13, 407], "temperature": 0.0, "avg_logprob": -0.1479458988837476, "compression_ratio": 1.7641921397379912, "no_speech_prob": 2.4297320123878308e-05}, {"id": 337, "seek": 178920, "start": 1812.88, "end": 1815.04, "text": " you would expect it would probably fine tune to bigger", "tokens": [291, 576, 2066, 309, 576, 1391, 2489, 10864, 281, 3801], "temperature": 0.0, "avg_logprob": -0.1479458988837476, "compression_ratio": 1.7641921397379912, "no_speech_prob": 2.4297320123878308e-05}, {"id": 338, "seek": 178920, "start": 1815.04, "end": 1818.48, "text": " images of rice disease quite easily. So we might get it", "tokens": [5267, 295, 5090, 4752, 1596, 3612, 13, 407, 321, 1062, 483, 309], "temperature": 0.0, "avg_logprob": -0.1479458988837476, "compression_ratio": 1.7641921397379912, "no_speech_prob": 2.4297320123878308e-05}, {"id": 339, "seek": 181848, "start": 1818.48, "end": 1822.56, "text": " most of the benefit of training on big on big images, but", "tokens": [881, 295, 264, 5121, 295, 3097, 322, 955, 322, 955, 5267, 11, 457], "temperature": 0.0, "avg_logprob": -0.17165877268864557, "compression_ratio": 1.5934065934065933, "no_speech_prob": 4.49481467512669e-06}, {"id": 340, "seek": 181848, "start": 1822.56, "end": 1828.08, "text": " without most of the time. The second benefit is it's a kind of", "tokens": [1553, 881, 295, 264, 565, 13, 440, 1150, 5121, 307, 309, 311, 257, 733, 295], "temperature": 0.0, "avg_logprob": -0.17165877268864557, "compression_ratio": 1.5934065934065933, "no_speech_prob": 4.49481467512669e-06}, {"id": 341, "seek": 181848, "start": 1828.08, "end": 1830.8, "text": " data augmentation, which is we're actually giving it", "tokens": [1412, 14501, 19631, 11, 597, 307, 321, 434, 767, 2902, 309], "temperature": 0.0, "avg_logprob": -0.17165877268864557, "compression_ratio": 1.5934065934065933, "no_speech_prob": 4.49481467512669e-06}, {"id": 342, "seek": 181848, "start": 1831.6, "end": 1836.8, "text": " different sized images. So that should that should help. So", "tokens": [819, 20004, 5267, 13, 407, 300, 820, 300, 820, 854, 13, 407], "temperature": 0.0, "avg_logprob": -0.17165877268864557, "compression_ratio": 1.5934065934065933, "no_speech_prob": 4.49481467512669e-06}, {"id": 343, "seek": 181848, "start": 1836.8, "end": 1842.96, "text": " here's how we would do that. Let's grab this data block.", "tokens": [510, 311, 577, 321, 576, 360, 300, 13, 961, 311, 4444, 341, 1412, 3461, 13], "temperature": 0.0, "avg_logprob": -0.17165877268864557, "compression_ratio": 1.5934065934065933, "no_speech_prob": 4.49481467512669e-06}, {"id": 344, "seek": 184296, "start": 1842.96, "end": 1851.1200000000001, "text": " Let's make it into a function. Get DL. Okay, and the key thing", "tokens": [961, 311, 652, 309, 666, 257, 2445, 13, 3240, 413, 43, 13, 1033, 11, 293, 264, 2141, 551], "temperature": 0.0, "avg_logprob": -0.2153745480437777, "compression_ratio": 1.5733333333333333, "no_speech_prob": 1.64417961059371e-05}, {"id": 345, "seek": 184296, "start": 1851.1200000000001, "end": 1854.08, "text": " I guess we're going to do. Well, let's just do the item", "tokens": [286, 2041, 321, 434, 516, 281, 360, 13, 1042, 11, 718, 311, 445, 360, 264, 3174], "temperature": 0.0, "avg_logprob": -0.2153745480437777, "compression_ratio": 1.5733333333333333, "no_speech_prob": 1.64417961059371e-05}, {"id": 346, "seek": 184296, "start": 1854.08, "end": 1859.92, "text": " transforms and the batch transforms as usual. Whoops. So", "tokens": [35592, 293, 264, 15245, 35592, 382, 7713, 13, 45263, 13, 407], "temperature": 0.0, "avg_logprob": -0.2153745480437777, "compression_ratio": 1.5733333333333333, "no_speech_prob": 1.64417961059371e-05}, {"id": 347, "seek": 185992, "start": 1859.92, "end": 1869.92, "text": " so the things we're going to change are the item transforms", "tokens": [370, 264, 721, 321, 434, 516, 281, 1319, 366, 264, 3174, 35592], "temperature": 0.0, "avg_logprob": -0.23748985203829678, "compression_ratio": 1.4672897196261683, "no_speech_prob": 4.936758159601595e-06}, {"id": 348, "seek": 185992, "start": 1877.04, "end": 1878.24, "text": " and the batch transforms.", "tokens": [293, 264, 15245, 35592, 13], "temperature": 0.0, "avg_logprob": -0.23748985203829678, "compression_ratio": 1.4672897196261683, "no_speech_prob": 4.936758159601595e-06}, {"id": 349, "seek": 185992, "start": 1881.8400000000001, "end": 1883.92, "text": " And then we're going to return the data loader for that,", "tokens": [400, 550, 321, 434, 516, 281, 2736, 264, 1412, 3677, 260, 337, 300, 11], "temperature": 0.0, "avg_logprob": -0.23748985203829678, "compression_ratio": 1.4672897196261683, "no_speech_prob": 4.936758159601595e-06}, {"id": 350, "seek": 188392, "start": 1883.92, "end": 1887.44, "text": " which is here.", "tokens": [597, 307, 510, 13], "temperature": 0.0, "avg_logprob": -0.58930418226454, "compression_ratio": 0.8048780487804879, "no_speech_prob": 0.00012911127123516053}, {"id": 351, "seek": 188392, "start": 1895.3600000000001, "end": 1895.68, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.58930418226454, "compression_ratio": 0.8048780487804879, "no_speech_prob": 0.00012911127123516053}, {"id": 352, "seek": 188392, "start": 1901.8400000000001, "end": 1902.8000000000002, "text": " So let's try", "tokens": [407, 718, 311, 853], "temperature": 0.0, "avg_logprob": -0.58930418226454, "compression_ratio": 0.8048780487804879, "no_speech_prob": 0.00012911127123516053}, {"id": 353, "seek": 190280, "start": 1902.8, "end": 1913.2, "text": " going up a bit.", "tokens": [516, 493, 257, 857, 13], "temperature": 0.0, "avg_logprob": -0.25588835369456897, "compression_ratio": 1.2197802197802199, "no_speech_prob": 2.318360930075869e-05}, {"id": 354, "seek": 190280, "start": 1917.36, "end": 1918.72, "text": " DLs equals", "tokens": [413, 43, 82, 6915], "temperature": 0.0, "avg_logprob": -0.25588835369456897, "compression_ratio": 1.2197802197802199, "no_speech_prob": 2.318360930075869e-05}, {"id": 355, "seek": 190280, "start": 1922.48, "end": 1926.72, "text": " get DL. I guess it should be get DLs really because it returns", "tokens": [483, 413, 43, 13, 286, 2041, 309, 820, 312, 483, 413, 43, 82, 534, 570, 309, 11247], "temperature": 0.0, "avg_logprob": -0.25588835369456897, "compression_ratio": 1.2197802197802199, "no_speech_prob": 2.318360930075869e-05}, {"id": 356, "seek": 190280, "start": 1926.72, "end": 1928.3999999999999, "text": " data loaders get DLs.", "tokens": [1412, 3677, 433, 483, 413, 43, 82, 13], "temperature": 0.0, "avg_logprob": -0.25588835369456897, "compression_ratio": 1.2197802197802199, "no_speech_prob": 2.318360930075869e-05}, {"id": 357, "seek": 192840, "start": 1928.4, "end": 1934.72, "text": " Okay, so let's", "tokens": [1033, 11, 370, 718, 311], "temperature": 0.0, "avg_logprob": -0.2101306234087263, "compression_ratio": 1.1886792452830188, "no_speech_prob": 7.527220077463426e-06}, {"id": 358, "seek": 192840, "start": 1939.0400000000002, "end": 1943.1200000000001, "text": " see what we did last time. Let's be scaled up a bit.", "tokens": [536, 437, 321, 630, 1036, 565, 13, 961, 311, 312, 36039, 493, 257, 857, 13], "temperature": 0.0, "avg_logprob": -0.2101306234087263, "compression_ratio": 1.1886792452830188, "no_speech_prob": 7.527220077463426e-06}, {"id": 359, "seek": 192840, "start": 1950.88, "end": 1952.72, "text": " This is going to be data augmentation as well. We're going", "tokens": [639, 307, 516, 281, 312, 1412, 14501, 19631, 382, 731, 13, 492, 434, 516], "temperature": 0.0, "avg_logprob": -0.2101306234087263, "compression_ratio": 1.1886792452830188, "no_speech_prob": 7.527220077463426e-06}, {"id": 360, "seek": 195272, "start": 1952.72, "end": 1959.52, "text": " to change how we scale. So we'll scale with zero padding.", "tokens": [281, 1319, 577, 321, 4373, 13, 407, 321, 603, 4373, 365, 4018, 39562, 13], "temperature": 0.0, "avg_logprob": -0.22487464547157288, "compression_ratio": 1.0116279069767442, "no_speech_prob": 7.182224635471357e-06}, {"id": 361, "seek": 195272, "start": 1962.16, "end": 1964.8, "text": " And let's go up to 160.", "tokens": [400, 718, 311, 352, 493, 281, 21243, 13], "temperature": 0.0, "avg_logprob": -0.22487464547157288, "compression_ratio": 1.0116279069767442, "no_speech_prob": 7.182224635471357e-06}, {"id": 362, "seek": 195272, "start": 1967.6000000000001, "end": 1968.0, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.22487464547157288, "compression_ratio": 1.0116279069767442, "no_speech_prob": 7.182224635471357e-06}, {"id": 363, "seek": 196800, "start": 1968.0, "end": 1976.0, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.5943396274860089, "compression_ratio": 0.7894736842105263, "no_speech_prob": 8.597312989877537e-05}, {"id": 364, "seek": 197600, "start": 1976.0, "end": 1994.0, "text": " So then we need to learn.", "tokens": [50364, 407, 550, 321, 643, 281, 1466, 13, 51264], "temperature": 0.0, "avg_logprob": -0.6914515972137452, "compression_ratio": 0.7575757575757576, "no_speech_prob": 8.187533967429772e-05}, {"id": 365, "seek": 200600, "start": 2006.08, "end": 2012.48, "text": " So our", "tokens": [407, 527], "temperature": 0.0, "avg_logprob": -0.22874236875964749, "compression_ratio": 1.3846153846153846, "no_speech_prob": 0.0009694857290014625}, {"id": 366, "seek": 200600, "start": 2015.36, "end": 2021.84, "text": " where's our squish one here? Squish. So the squish here got", "tokens": [689, 311, 527, 31379, 472, 510, 30, 8683, 742, 13, 407, 264, 31379, 510, 658], "temperature": 0.0, "avg_logprob": -0.22874236875964749, "compression_ratio": 1.3846153846153846, "no_speech_prob": 0.0009694857290014625}, {"id": 367, "seek": 200600, "start": 2023.2, "end": 2029.04, "text": " 0.45. Our multitask got 0.48. So it's actually a little bit worse.", "tokens": [1958, 13, 8465, 13, 2621, 42338, 3863, 658, 1958, 13, 13318, 13, 407, 309, 311, 767, 257, 707, 857, 5324, 13], "temperature": 0.0, "avg_logprob": -0.22874236875964749, "compression_ratio": 1.3846153846153846, "no_speech_prob": 0.0009694857290014625}, {"id": 368, "seek": 200600, "start": 2032.96, "end": 2035.6, "text": " This might not be a great test actually, because I feel like one", "tokens": [639, 1062, 406, 312, 257, 869, 1500, 767, 11, 570, 286, 841, 411, 472], "temperature": 0.0, "avg_logprob": -0.22874236875964749, "compression_ratio": 1.3846153846153846, "no_speech_prob": 0.0009694857290014625}, {"id": 369, "seek": 203560, "start": 2035.6, "end": 2040.0, "text": " of the reasons that doing a multitask model might be useful", "tokens": [295, 264, 4112, 300, 884, 257, 42338, 3863, 2316, 1062, 312, 4420], "temperature": 0.0, "avg_logprob": -0.09708162357932643, "compression_ratio": 1.4974358974358974, "no_speech_prob": 5.771511496277526e-06}, {"id": 370, "seek": 203560, "start": 2040.0, "end": 2042.08, "text": " is it might be able to train for more epochs.", "tokens": [307, 309, 1062, 312, 1075, 281, 3847, 337, 544, 30992, 28346, 13], "temperature": 0.0, "avg_logprob": -0.09708162357932643, "compression_ratio": 1.4974358974358974, "no_speech_prob": 5.771511496277526e-06}, {"id": 371, "seek": 203560, "start": 2046.0, "end": 2049.2, "text": " Because we're kind of giving it more signal. So we should", "tokens": [1436, 321, 434, 733, 295, 2902, 309, 544, 6358, 13, 407, 321, 820], "temperature": 0.0, "avg_logprob": -0.09708162357932643, "compression_ratio": 1.4974358974358974, "no_speech_prob": 5.771511496277526e-06}, {"id": 372, "seek": 203560, "start": 2049.2, "end": 2052.7999999999997, "text": " probably revisit this with like 20 epochs.", "tokens": [1391, 32676, 341, 365, 411, 945, 30992, 28346, 13], "temperature": 0.0, "avg_logprob": -0.09708162357932643, "compression_ratio": 1.4974358974358974, "no_speech_prob": 5.771511496277526e-06}, {"id": 373, "seek": 203560, "start": 2058.56, "end": 2061.52, "text": " Any questions or comments about progressive resizing while we", "tokens": [2639, 1651, 420, 3053, 466, 16131, 725, 3319, 1339, 321], "temperature": 0.0, "avg_logprob": -0.09708162357932643, "compression_ratio": 1.4974358974358974, "no_speech_prob": 5.771511496277526e-06}, {"id": 374, "seek": 203560, "start": 2061.52, "end": 2063.2799999999997, "text": " wait for this to train?", "tokens": [1699, 337, 341, 281, 3847, 30], "temperature": 0.0, "avg_logprob": -0.09708162357932643, "compression_ratio": 1.4974358974358974, "no_speech_prob": 5.771511496277526e-06}, {"id": 375, "seek": 206328, "start": 2063.28, "end": 2070.88, "text": " Sorry, I can't see how you progressively change the size.", "tokens": [4919, 11, 286, 393, 380, 536, 577, 291, 46667, 1319, 264, 2744, 13], "temperature": 0.0, "avg_logprob": -0.426932786640368, "compression_ratio": 1.434782608695652, "no_speech_prob": 7.364841439994052e-05}, {"id": 376, "seek": 206328, "start": 2070.88, "end": 2073.1200000000003, "text": " I actually didn't. I messed it up.", "tokens": [286, 767, 994, 380, 13, 286, 16507, 309, 493, 13], "temperature": 0.0, "avg_logprob": -0.426932786640368, "compression_ratio": 1.434782608695652, "no_speech_prob": 7.364841439994052e-05}, {"id": 377, "seek": 206328, "start": 2074.32, "end": 2076.96, "text": " Whoops. Thank you.", "tokens": [45263, 13, 1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.426932786640368, "compression_ratio": 1.434782608695652, "no_speech_prob": 7.364841439994052e-05}, {"id": 378, "seek": 206328, "start": 2078.32, "end": 2079.6800000000003, "text": " I have to do that again.", "tokens": [286, 362, 281, 360, 300, 797, 13], "temperature": 0.0, "avg_logprob": -0.426932786640368, "compression_ratio": 1.434782608695652, "no_speech_prob": 7.364841439994052e-05}, {"id": 379, "seek": 206328, "start": 2082.2400000000002, "end": 2084.88, "text": " I actually did it. Oh, and we need to get out to Yale's back", "tokens": [286, 767, 630, 309, 13, 876, 11, 293, 321, 643, 281, 483, 484, 281, 26711, 311, 646], "temperature": 0.0, "avg_logprob": -0.426932786640368, "compression_ratio": 1.434782608695652, "no_speech_prob": 7.364841439994052e-05}, {"id": 380, "seek": 206328, "start": 2084.88, "end": 2086.7200000000003, "text": " as well. Okay, let's start again.", "tokens": [382, 731, 13, 1033, 11, 718, 311, 722, 797, 13], "temperature": 0.0, "avg_logprob": -0.426932786640368, "compression_ratio": 1.434782608695652, "no_speech_prob": 7.364841439994052e-05}, {"id": 381, "seek": 208672, "start": 2086.72, "end": 2091.4399999999996, "text": " Okay. And let's in case I mess this up again, let's export this.", "tokens": [1033, 13, 400, 718, 311, 294, 1389, 286, 2082, 341, 493, 797, 11, 718, 311, 10725, 341, 13], "temperature": 0.0, "avg_logprob": -0.5230758303687686, "compression_ratio": 1.3571428571428572, "no_speech_prob": 1.7776901586330496e-05}, {"id": 382, "seek": 208672, "start": 2092.8799999999997, "end": 2094.48, "text": " We'll call this like stage one.", "tokens": [492, 603, 818, 341, 411, 3233, 472, 13], "temperature": 0.0, "avg_logprob": -0.5230758303687686, "compression_ratio": 1.3571428571428572, "no_speech_prob": 1.7776901586330496e-05}, {"id": 383, "seek": 208672, "start": 2103.12, "end": 2103.52, "text": " Yeah, so the problem was we created a new learner. So what we", "tokens": [865, 11, 370, 264, 1154, 390, 321, 2942, 257, 777, 33347, 13, 407, 437, 321], "temperature": 0.0, "avg_logprob": -0.5230758303687686, "compression_ratio": 1.3571428571428572, "no_speech_prob": 1.7776901586330496e-05}, {"id": 384, "seek": 208672, "start": 2103.52, "end": 2111.4399999999996, "text": " should have done is gone. Learn dot the L's equals", "tokens": [820, 362, 1096, 307, 2780, 13, 17216, 5893, 264, 441, 311, 6915], "temperature": 0.0, "avg_logprob": -0.5230758303687686, "compression_ratio": 1.3571428571428572, "no_speech_prob": 1.7776901586330496e-05}, {"id": 385, "seek": 211144, "start": 2111.44, "end": 2121.52, "text": " the L's. That's actually so that would actually change the", "tokens": [264, 441, 311, 13, 663, 311, 767, 370, 300, 576, 767, 1319, 264], "temperature": 0.0, "avg_logprob": -0.565054146640272, "compression_ratio": 1.6284153005464481, "no_speech_prob": 1.4509642824123148e-05}, {"id": 386, "seek": 211144, "start": 2121.52, "end": 2124.48, "text": " data loaders inside the learner without recreating it.", "tokens": [1412, 3677, 433, 1854, 264, 33347, 1553, 850, 44613, 309, 13], "temperature": 0.0, "avg_logprob": -0.565054146640272, "compression_ratio": 1.6284153005464481, "no_speech_prob": 1.4509642824123148e-05}, {"id": 387, "seek": 211144, "start": 2124.48, "end": 2127.36, "text": " Was that where you were heading with your comment?", "tokens": [3027, 300, 689, 291, 645, 9864, 365, 428, 2871, 30], "temperature": 0.0, "avg_logprob": -0.565054146640272, "compression_ratio": 1.6284153005464481, "no_speech_prob": 1.4509642824123148e-05}, {"id": 388, "seek": 211144, "start": 2130.4, "end": 2133.2000000000003, "text": " There was an unfreeze method, right? Like, I wouldn't be using", "tokens": [821, 390, 364, 3971, 701, 1381, 3170, 11, 558, 30, 1743, 11, 286, 2759, 380, 312, 1228], "temperature": 0.0, "avg_logprob": -0.565054146640272, "compression_ratio": 1.6284153005464481, "no_speech_prob": 1.4509642824123148e-05}, {"id": 389, "seek": 211144, "start": 2133.2000000000003, "end": 2134.4, "text": " that. Sorry.", "tokens": [300, 13, 4919, 13], "temperature": 0.0, "avg_logprob": -0.565054146640272, "compression_ratio": 1.6284153005464481, "no_speech_prob": 1.4509642824123148e-05}, {"id": 390, "seek": 211144, "start": 2134.4, "end": 2137.2000000000003, "text": " There was an unfreeze method like in the same way that we", "tokens": [821, 390, 364, 3971, 701, 1381, 3170, 411, 294, 264, 912, 636, 300, 321], "temperature": 0.0, "avg_logprob": -0.565054146640272, "compression_ratio": 1.6284153005464481, "no_speech_prob": 1.4509642824123148e-05}, {"id": 391, "seek": 213720, "start": 2137.2, "end": 2143.2, "text": " mentioned using the unfreeze method.", "tokens": [2835, 1228, 264, 3971, 701, 1381, 3170, 13], "temperature": 0.0, "avg_logprob": -0.2846934344317462, "compression_ratio": 1.5766871165644172, "no_speech_prob": 3.480508166830987e-05}, {"id": 392, "seek": 213720, "start": 2143.2, "end": 2149.4399999999996, "text": " There is an unfreeze method. Yes. What were you saying about", "tokens": [821, 307, 364, 3971, 701, 1381, 3170, 13, 1079, 13, 708, 645, 291, 1566, 466], "temperature": 0.0, "avg_logprob": -0.2846934344317462, "compression_ratio": 1.5766871165644172, "no_speech_prob": 3.480508166830987e-05}, {"id": 393, "seek": 213720, "start": 2149.4399999999996, "end": 2150.3199999999997, "text": " the unfreeze method?", "tokens": [264, 3971, 701, 1381, 3170, 30], "temperature": 0.0, "avg_logprob": -0.2846934344317462, "compression_ratio": 1.5766871165644172, "no_speech_prob": 3.480508166830987e-05}, {"id": 394, "seek": 213720, "start": 2150.3199999999997, "end": 2154.3999999999996, "text": " Is an unfreeze required for progressive resizing?", "tokens": [1119, 364, 3971, 701, 1381, 4739, 337, 16131, 725, 3319, 30], "temperature": 0.0, "avg_logprob": -0.2846934344317462, "compression_ratio": 1.5766871165644172, "no_speech_prob": 3.480508166830987e-05}, {"id": 395, "seek": 213720, "start": 2154.3999999999996, "end": 2158.8799999999997, "text": " No, because fine tune has already unfrozen.", "tokens": [883, 11, 570, 2489, 10864, 575, 1217, 3971, 340, 2904, 13], "temperature": 0.0, "avg_logprob": -0.2846934344317462, "compression_ratio": 1.5766871165644172, "no_speech_prob": 3.480508166830987e-05}, {"id": 396, "seek": 213720, "start": 2159.4399999999996, "end": 2162.24, "text": " Although I actually want to fine tune again.", "tokens": [5780, 286, 767, 528, 281, 2489, 10864, 797, 13], "temperature": 0.0, "avg_logprob": -0.2846934344317462, "compression_ratio": 1.5766871165644172, "no_speech_prob": 3.480508166830987e-05}, {"id": 397, "seek": 216224, "start": 2162.24, "end": 2165.68, "text": " So if anything, I kind of actually want to actually want", "tokens": [407, 498, 1340, 11, 286, 733, 295, 767, 528, 281, 767, 528], "temperature": 0.0, "avg_logprob": -0.3811174580748652, "compression_ratio": 1.4880952380952381, "no_speech_prob": 3.647000266937539e-05}, {"id": 398, "seek": 216224, "start": 2165.68, "end": 2167.68, "text": " to refreeze it.", "tokens": [281, 1895, 701, 1381, 309, 13], "temperature": 0.0, "avg_logprob": -0.3811174580748652, "compression_ratio": 1.4880952380952381, "no_speech_prob": 3.647000266937539e-05}, {"id": 399, "seek": 216224, "start": 2170.08, "end": 2177.2799999999997, "text": " Because we've changed the resolution. I think fine tuning", "tokens": [1436, 321, 600, 3105, 264, 8669, 13, 286, 519, 2489, 15164], "temperature": 0.0, "avg_logprob": -0.3811174580748652, "compression_ratio": 1.4880952380952381, "no_speech_prob": 3.647000266937539e-05}, {"id": 400, "seek": 216224, "start": 2177.2799999999997, "end": 2179.7599999999998, "text": " the head might be a good idea to do again.", "tokens": [264, 1378, 1062, 312, 257, 665, 1558, 281, 360, 797, 13], "temperature": 0.0, "avg_logprob": -0.3811174580748652, "compression_ratio": 1.4880952380952381, "no_speech_prob": 3.647000266937539e-05}, {"id": 401, "seek": 216224, "start": 2181.7599999999998, "end": 2187.7599999999998, "text": " Which line of code is doing the progressive resizing part?", "tokens": [3013, 1622, 295, 3089, 307, 884, 264, 16131, 725, 3319, 644, 30], "temperature": 0.0, "avg_logprob": -0.3811174580748652, "compression_ratio": 1.4880952380952381, "no_speech_prob": 3.647000266937539e-05}, {"id": 402, "seek": 216224, "start": 2187.7599999999998, "end": 2188.9599999999996, "text": " Just to be clear.", "tokens": [1449, 281, 312, 1850, 13], "temperature": 0.0, "avg_logprob": -0.3811174580748652, "compression_ratio": 1.4880952380952381, "no_speech_prob": 3.647000266937539e-05}, {"id": 403, "seek": 218896, "start": 2188.96, "end": 2192.48, "text": " It's not our line of code. It's basically this.", "tokens": [467, 311, 406, 527, 1622, 295, 3089, 13, 467, 311, 1936, 341, 13], "temperature": 0.0, "avg_logprob": -0.2534015110560826, "compression_ratio": 1.8802083333333333, "no_speech_prob": 2.110961577272974e-05}, {"id": 404, "seek": 218896, "start": 2192.48, "end": 2195.12, "text": " It's basically saying our current learner is getting new", "tokens": [467, 311, 1936, 1566, 527, 2190, 33347, 307, 1242, 777], "temperature": 0.0, "avg_logprob": -0.2534015110560826, "compression_ratio": 1.8802083333333333, "no_speech_prob": 2.110961577272974e-05}, {"id": 405, "seek": 218896, "start": 2195.12, "end": 2201.12, "text": " data loaders. And the new data loaders have a size of 160,", "tokens": [1412, 3677, 433, 13, 400, 264, 777, 1412, 3677, 433, 362, 257, 2744, 295, 21243, 11], "temperature": 0.0, "avg_logprob": -0.2534015110560826, "compression_ratio": 1.8802083333333333, "no_speech_prob": 2.110961577272974e-05}, {"id": 406, "seek": 218896, "start": 2201.12, "end": 2205.12, "text": " whereas the old data loaders had a size of 128.", "tokens": [9735, 264, 1331, 1412, 3677, 433, 632, 257, 2744, 295, 29810, 13], "temperature": 0.0, "avg_logprob": -0.2534015110560826, "compression_ratio": 1.8802083333333333, "no_speech_prob": 2.110961577272974e-05}, {"id": 407, "seek": 218896, "start": 2205.12, "end": 2209.92, "text": " And our old data loaders did a pre-sizing of 192 squish and", "tokens": [400, 527, 1331, 1412, 3677, 433, 630, 257, 659, 12, 82, 3319, 295, 1294, 17, 31379, 293], "temperature": 0.0, "avg_logprob": -0.2534015110560826, "compression_ratio": 1.8802083333333333, "no_speech_prob": 2.110961577272974e-05}, {"id": 408, "seek": 218896, "start": 2209.92, "end": 2213.12, "text": " our new data loaders are doing a pre-sizing of rectangular", "tokens": [527, 777, 1412, 3677, 433, 366, 884, 257, 659, 12, 82, 3319, 295, 31167], "temperature": 0.0, "avg_logprob": -0.2534015110560826, "compression_ratio": 1.8802083333333333, "no_speech_prob": 2.110961577272974e-05}, {"id": 409, "seek": 218896, "start": 2213.12, "end": 2216.0, "text": " padding. Does that make sense?", "tokens": [39562, 13, 4402, 300, 652, 2020, 30], "temperature": 0.0, "avg_logprob": -0.2534015110560826, "compression_ratio": 1.8802083333333333, "no_speech_prob": 2.110961577272974e-05}, {"id": 410, "seek": 221600, "start": 2216.0, "end": 2219.36, "text": " Why are you calling it progressive in this case?", "tokens": [1545, 366, 291, 5141, 309, 16131, 294, 341, 1389, 30], "temperature": 0.0, "avg_logprob": -0.34195518493652344, "compression_ratio": 1.5333333333333334, "no_speech_prob": 8.800547220744193e-06}, {"id": 411, "seek": 221600, "start": 2219.36, "end": 2222.8, "text": " Just are you going to keep changing the size or something", "tokens": [1449, 366, 291, 516, 281, 1066, 4473, 264, 2744, 420, 746], "temperature": 0.0, "avg_logprob": -0.34195518493652344, "compression_ratio": 1.5333333333333334, "no_speech_prob": 8.800547220744193e-06}, {"id": 412, "seek": 221600, "start": 2222.8, "end": 2223.28, "text": " like that?", "tokens": [411, 300, 30], "temperature": 0.0, "avg_logprob": -0.34195518493652344, "compression_ratio": 1.5333333333333334, "no_speech_prob": 8.800547220744193e-06}, {"id": 413, "seek": 221600, "start": 2223.28, "end": 2227.92, "text": " Yeah, it's changing the size of the images without resetting", "tokens": [865, 11, 309, 311, 4473, 264, 2744, 295, 264, 5267, 1553, 14322, 783], "temperature": 0.0, "avg_logprob": -0.34195518493652344, "compression_ratio": 1.5333333333333334, "no_speech_prob": 8.800547220744193e-06}, {"id": 414, "seek": 221600, "start": 2227.92, "end": 2229.92, "text": " the learner.", "tokens": [264, 33347, 13], "temperature": 0.0, "avg_logprob": -0.34195518493652344, "compression_ratio": 1.5333333333333334, "no_speech_prob": 8.800547220744193e-06}, {"id": 415, "seek": 221600, "start": 2231.92, "end": 2234.24, "text": " Just looked it up because I was curious. Fine tune calls", "tokens": [1449, 2956, 309, 493, 570, 286, 390, 6369, 13, 12024, 10864, 5498], "temperature": 0.0, "avg_logprob": -0.34195518493652344, "compression_ratio": 1.5333333333333334, "no_speech_prob": 8.800547220744193e-06}, {"id": 416, "seek": 221600, "start": 2234.24, "end": 2236.24, "text": " a freeze first.", "tokens": [257, 15959, 700, 13], "temperature": 0.0, "avg_logprob": -0.34195518493652344, "compression_ratio": 1.5333333333333334, "no_speech_prob": 8.800547220744193e-06}, {"id": 417, "seek": 221600, "start": 2236.24, "end": 2238.24, "text": " I had a feeling it did.", "tokens": [286, 632, 257, 2633, 309, 630, 13], "temperature": 0.0, "avg_logprob": -0.34195518493652344, "compression_ratio": 1.5333333333333334, "no_speech_prob": 8.800547220744193e-06}, {"id": 418, "seek": 221600, "start": 2238.24, "end": 2240.24, "text": " Thanks for checking, Zach.", "tokens": [2561, 337, 8568, 11, 21028, 13], "temperature": 0.0, "avg_logprob": -0.34195518493652344, "compression_ratio": 1.5333333333333334, "no_speech_prob": 8.800547220744193e-06}, {"id": 419, "seek": 221600, "start": 2240.24, "end": 2242.24, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.34195518493652344, "compression_ratio": 1.5333333333333334, "no_speech_prob": 8.800547220744193e-06}, {"id": 420, "seek": 224224, "start": 2242.24, "end": 2248.24, "text": " So this time, let's see.", "tokens": [407, 341, 565, 11, 718, 311, 536, 13], "temperature": 0.0, "avg_logprob": -0.19142028263636998, "compression_ratio": 1.597883597883598, "no_speech_prob": 1.1125152923341375e-05}, {"id": 421, "seek": 224224, "start": 2248.24, "end": 2250.08, "text": " It'll be interesting to see how it does.", "tokens": [467, 603, 312, 1880, 281, 536, 577, 309, 775, 13], "temperature": 0.0, "avg_logprob": -0.19142028263636998, "compression_ratio": 1.597883597883598, "no_speech_prob": 1.1125152923341375e-05}, {"id": 422, "seek": 224224, "start": 2252.08, "end": 2256.3199999999997, "text": " So after the initial epoch, it's got 0.09.", "tokens": [407, 934, 264, 5883, 30992, 339, 11, 309, 311, 658, 1958, 13, 13811, 13], "temperature": 0.0, "avg_logprob": -0.19142028263636998, "compression_ratio": 1.597883597883598, "no_speech_prob": 1.1125152923341375e-05}, {"id": 423, "seek": 224224, "start": 2257.68, "end": 2259.6, "text": " Whereas previously it had 0.27.", "tokens": [13813, 8046, 309, 632, 1958, 13, 10076, 13], "temperature": 0.0, "avg_logprob": -0.19142028263636998, "compression_ratio": 1.597883597883598, "no_speech_prob": 1.1125152923341375e-05}, {"id": 424, "seek": 224224, "start": 2259.6, "end": 2262.4799999999996, "text": " So obviously it's better than last time, but it's actually", "tokens": [407, 2745, 309, 311, 1101, 813, 1036, 565, 11, 457, 309, 311, 767], "temperature": 0.0, "avg_logprob": -0.19142028263636998, "compression_ratio": 1.597883597883598, "no_speech_prob": 1.1125152923341375e-05}, {"id": 425, "seek": 224224, "start": 2262.4799999999996, "end": 2264.16, "text": " worse than the final point.", "tokens": [5324, 813, 264, 2572, 935, 13], "temperature": 0.0, "avg_logprob": -0.19142028263636998, "compression_ratio": 1.597883597883598, "no_speech_prob": 1.1125152923341375e-05}, {"id": 426, "seek": 224224, "start": 2264.9599999999996, "end": 2267.7599999999998, "text": " This time it got all the way to 0.418.", "tokens": [639, 565, 309, 658, 439, 264, 636, 281, 1958, 13, 19, 6494, 13], "temperature": 0.0, "avg_logprob": -0.19142028263636998, "compression_ratio": 1.597883597883598, "no_speech_prob": 1.1125152923341375e-05}, {"id": 427, "seek": 224224, "start": 2268.64, "end": 2271.2, "text": " Whereas this time it has got worse.", "tokens": [13813, 341, 565, 309, 575, 658, 5324, 13], "temperature": 0.0, "avg_logprob": -0.19142028263636998, "compression_ratio": 1.597883597883598, "no_speech_prob": 1.1125152923341375e-05}, {"id": 428, "seek": 227120, "start": 2271.2, "end": 2275.52, "text": " So it's got some work to do to learn to recognize what 160", "tokens": [407, 309, 311, 658, 512, 589, 281, 360, 281, 1466, 281, 5521, 437, 21243], "temperature": 0.0, "avg_logprob": -0.2045330235987534, "compression_ratio": 1.3877551020408163, "no_speech_prob": 1.952325510501396e-05}, {"id": 429, "seek": 227120, "start": 2275.52, "end": 2276.64, "text": " pixel images look like.", "tokens": [19261, 5267, 574, 411, 13], "temperature": 0.0, "avg_logprob": -0.2045330235987534, "compression_ratio": 1.3877551020408163, "no_speech_prob": 1.952325510501396e-05}, {"id": 430, "seek": 227120, "start": 2280.7999999999997, "end": 2282.64, "text": " Can I just clarify, Jeremy?", "tokens": [1664, 286, 445, 17594, 11, 17809, 30], "temperature": 0.0, "avg_logprob": -0.2045330235987534, "compression_ratio": 1.3877551020408163, "no_speech_prob": 1.952325510501396e-05}, {"id": 431, "seek": 227120, "start": 2283.9199999999996, "end": 2288.56, "text": " You're doing one more step in the progressive resizing here.", "tokens": [509, 434, 884, 472, 544, 1823, 294, 264, 16131, 725, 3319, 510, 13], "temperature": 0.0, "avg_logprob": -0.2045330235987534, "compression_ratio": 1.3877551020408163, "no_speech_prob": 1.952325510501396e-05}, {"id": 432, "seek": 227120, "start": 2288.56, "end": 2291.3599999999997, "text": " It's not kind of an automated resizing.", "tokens": [467, 311, 406, 733, 295, 364, 18473, 725, 3319, 13], "temperature": 0.0, "avg_logprob": -0.2045330235987534, "compression_ratio": 1.3877551020408163, "no_speech_prob": 1.952325510501396e-05}, {"id": 433, "seek": 227120, "start": 2291.3599999999997, "end": 2291.8599999999997, "text": " Correct.", "tokens": [12753, 13], "temperature": 0.0, "avg_logprob": -0.2045330235987534, "compression_ratio": 1.3877551020408163, "no_speech_prob": 1.952325510501396e-05}, {"id": 434, "seek": 227120, "start": 2294.16, "end": 2296.7999999999997, "text": " There isn't anything in Fast.ai to do this for you.", "tokens": [821, 1943, 380, 1340, 294, 15968, 13, 1301, 281, 360, 341, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.2045330235987534, "compression_ratio": 1.3877551020408163, "no_speech_prob": 1.952325510501396e-05}, {"id": 435, "seek": 229680, "start": 2296.8, "end": 2300.48, "text": " And in fact, this technique is something that we invented.", "tokens": [400, 294, 1186, 11, 341, 6532, 307, 746, 300, 321, 14479, 13], "temperature": 0.0, "avg_logprob": -0.2543231728150673, "compression_ratio": 1.6476190476190475, "no_speech_prob": 1.5932258975226432e-05}, {"id": 436, "seek": 229680, "start": 2300.48, "end": 2305.2000000000003, "text": " So it doesn't exist in other libraries at all.", "tokens": [407, 309, 1177, 380, 2514, 294, 661, 15148, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.2543231728150673, "compression_ratio": 1.6476190476190475, "no_speech_prob": 1.5932258975226432e-05}, {"id": 437, "seek": 229680, "start": 2305.84, "end": 2308.2400000000002, "text": " So yeah, it's the name of a technique.", "tokens": [407, 1338, 11, 309, 311, 264, 1315, 295, 257, 6532, 13], "temperature": 0.0, "avg_logprob": -0.2543231728150673, "compression_ratio": 1.6476190476190475, "no_speech_prob": 1.5932258975226432e-05}, {"id": 438, "seek": 229680, "start": 2308.2400000000002, "end": 2311.44, "text": " It's not the name of like a method in Fast.ai.", "tokens": [467, 311, 406, 264, 1315, 295, 411, 257, 3170, 294, 15968, 13, 1301, 13], "temperature": 0.0, "avg_logprob": -0.2543231728150673, "compression_ratio": 1.6476190476190475, "no_speech_prob": 1.5932258975226432e-05}, {"id": 439, "seek": 229680, "start": 2312.4, "end": 2315.04, "text": " And yeah, the technique is basically to replace the data", "tokens": [400, 1338, 11, 264, 6532, 307, 1936, 281, 7406, 264, 1412], "temperature": 0.0, "avg_logprob": -0.2543231728150673, "compression_ratio": 1.6476190476190475, "no_speech_prob": 1.5932258975226432e-05}, {"id": 440, "seek": 229680, "start": 2315.04, "end": 2318.2400000000002, "text": " loaders with ones at a larger size.", "tokens": [3677, 433, 365, 2306, 412, 257, 4833, 2744, 13], "temperature": 0.0, "avg_logprob": -0.2543231728150673, "compression_ratio": 1.6476190476190475, "no_speech_prob": 1.5932258975226432e-05}, {"id": 441, "seek": 229680, "start": 2320.2400000000002, "end": 2324.32, "text": " And we invented it as part of a competition called Dawnbench,", "tokens": [400, 321, 14479, 309, 382, 644, 295, 257, 6211, 1219, 26001, 47244, 11], "temperature": 0.0, "avg_logprob": -0.2543231728150673, "compression_ratio": 1.6476190476190475, "no_speech_prob": 1.5932258975226432e-05}, {"id": 442, "seek": 232432, "start": 2324.32, "end": 2330.0800000000004, "text": " which is where we work very well on a competition for ImageNet", "tokens": [597, 307, 689, 321, 589, 588, 731, 322, 257, 6211, 337, 29903, 31890], "temperature": 0.0, "avg_logprob": -0.3541037680088789, "compression_ratio": 1.3762376237623761, "no_speech_prob": 2.354896059841849e-05}, {"id": 443, "seek": 232432, "start": 2330.0800000000004, "end": 2330.5800000000004, "text": " training.", "tokens": [3097, 13], "temperature": 0.0, "avg_logprob": -0.3541037680088789, "compression_ratio": 1.3762376237623761, "no_speech_prob": 2.354896059841849e-05}, {"id": 444, "seek": 232432, "start": 2332.0800000000004, "end": 2338.56, "text": " And Google then took the idea and studied it a lot further", "tokens": [400, 3329, 550, 1890, 264, 1558, 293, 9454, 309, 257, 688, 3052], "temperature": 0.0, "avg_logprob": -0.3541037680088789, "compression_ratio": 1.3762376237623761, "no_speech_prob": 2.354896059841849e-05}, {"id": 445, "seek": 232432, "start": 2338.56, "end": 2344.7200000000003, "text": " as part of a paper called EfficientNet V2 and found ways", "tokens": [382, 644, 295, 257, 3035, 1219, 462, 7816, 31890, 691, 17, 293, 1352, 2098], "temperature": 0.0, "avg_logprob": -0.3541037680088789, "compression_ratio": 1.3762376237623761, "no_speech_prob": 2.354896059841849e-05}, {"id": 446, "seek": 232432, "start": 2344.7200000000003, "end": 2345.92, "text": " to make it work even better.", "tokens": [281, 652, 309, 589, 754, 1101, 13], "temperature": 0.0, "avg_logprob": -0.3541037680088789, "compression_ratio": 1.3762376237623761, "no_speech_prob": 2.354896059841849e-05}, {"id": 447, "seek": 232432, "start": 2347.2000000000003, "end": 2348.4, "text": " Oh my gosh, look at this.", "tokens": [876, 452, 6502, 11, 574, 412, 341, 13], "temperature": 0.0, "avg_logprob": -0.3541037680088789, "compression_ratio": 1.3762376237623761, "no_speech_prob": 2.354896059841849e-05}, {"id": 448, "seek": 232432, "start": 2348.4, "end": 2354.2400000000002, "text": " So we've gone from 0.418 to 0.418.", "tokens": [407, 321, 600, 2780, 490, 1958, 13, 19, 6494, 281, 1958, 13, 19, 6494, 13], "temperature": 0.0, "avg_logprob": -0.3541037680088789, "compression_ratio": 1.3762376237623761, "no_speech_prob": 2.354896059841849e-05}, {"id": 449, "seek": 235424, "start": 2354.24, "end": 2355.8399999999997, "text": " To 0.0336.", "tokens": [1407, 1958, 13, 15, 10191, 21, 13], "temperature": 0.0, "avg_logprob": -0.28975920407277234, "compression_ratio": 1.1140350877192982, "no_speech_prob": 5.142711233929731e-05}, {"id": 450, "seek": 235424, "start": 2358.3199999999997, "end": 2361.6, "text": " Have we done training at 160 before?", "tokens": [3560, 321, 1096, 3097, 412, 21243, 949, 30], "temperature": 0.0, "avg_logprob": -0.28975920407277234, "compression_ratio": 1.1140350877192982, "no_speech_prob": 5.142711233929731e-05}, {"id": 451, "seek": 235424, "start": 2362.72, "end": 2364.08, "text": " I don't think we have.", "tokens": [286, 500, 380, 519, 321, 362, 13], "temperature": 0.0, "avg_logprob": -0.28975920407277234, "compression_ratio": 1.1140350877192982, "no_speech_prob": 5.142711233929731e-05}, {"id": 452, "seek": 235424, "start": 2368.3199999999997, "end": 2369.52, "text": " Oh, I should be checking this one.", "tokens": [876, 11, 286, 820, 312, 8568, 341, 472, 13], "temperature": 0.0, "avg_logprob": -0.28975920407277234, "compression_ratio": 1.1140350877192982, "no_speech_prob": 5.142711233929731e-05}, {"id": 453, "seek": 235424, "start": 2370.9599999999996, "end": 2372.72, "text": " 128, 128.", "tokens": [29810, 11, 29810, 13], "temperature": 0.0, "avg_logprob": -0.28975920407277234, "compression_ratio": 1.1140350877192982, "no_speech_prob": 5.142711233929731e-05}, {"id": 454, "seek": 237272, "start": 2372.72, "end": 2379.7599999999998, "text": " 171 by 128.", "tokens": [3282, 16, 538, 29810, 13], "temperature": 0.0, "avg_logprob": -0.17998862633338342, "compression_ratio": 1.2196969696969697, "no_speech_prob": 1.8340984752285294e-05}, {"id": 455, "seek": 237272, "start": 2386.64, "end": 2388.0, "text": " No, we haven't.", "tokens": [883, 11, 321, 2378, 380, 13], "temperature": 0.0, "avg_logprob": -0.17998862633338342, "compression_ratio": 1.2196969696969697, "no_speech_prob": 1.8340984752285294e-05}, {"id": 456, "seek": 237272, "start": 2389.6, "end": 2392.24, "text": " This is 256 by 192.", "tokens": [639, 307, 38882, 538, 1294, 17, 13], "temperature": 0.0, "avg_logprob": -0.17998862633338342, "compression_ratio": 1.2196969696969697, "no_speech_prob": 1.8340984752285294e-05}, {"id": 457, "seek": 237272, "start": 2392.24, "end": 2394.08, "text": " So eventually, I guess we're going to get to that point.", "tokens": [407, 4728, 11, 286, 2041, 321, 434, 516, 281, 483, 281, 300, 935, 13], "temperature": 0.0, "avg_logprob": -0.17998862633338342, "compression_ratio": 1.2196969696969697, "no_speech_prob": 1.8340984752285294e-05}, {"id": 458, "seek": 237272, "start": 2395.8399999999997, "end": 2398.56, "text": " So let's keep going.", "tokens": [407, 718, 311, 1066, 516, 13], "temperature": 0.0, "avg_logprob": -0.17998862633338342, "compression_ratio": 1.2196969696969697, "no_speech_prob": 1.8340984752285294e-05}, {"id": 459, "seek": 239856, "start": 2398.56, "end": 2404.24, "text": " So OK, so we're down to 2.9% error.", "tokens": [407, 2264, 11, 370, 321, 434, 760, 281, 568, 13, 24, 4, 6713, 13], "temperature": 0.0, "avg_logprob": -0.13331469551461642, "compression_ratio": 1.5884615384615384, "no_speech_prob": 2.17814595089294e-05}, {"id": 460, "seek": 239856, "start": 2404.24, "end": 2405.84, "text": " How did you come up with the idea for this?", "tokens": [1012, 630, 291, 808, 493, 365, 264, 1558, 337, 341, 30], "temperature": 0.0, "avg_logprob": -0.13331469551461642, "compression_ratio": 1.5884615384615384, "no_speech_prob": 2.17814595089294e-05}, {"id": 461, "seek": 239856, "start": 2405.84, "end": 2407.52, "text": " Is it something that you just wanted to try?", "tokens": [1119, 309, 746, 300, 291, 445, 1415, 281, 853, 30], "temperature": 0.0, "avg_logprob": -0.13331469551461642, "compression_ratio": 1.5884615384615384, "no_speech_prob": 2.17814595089294e-05}, {"id": 462, "seek": 239856, "start": 2408.24, "end": 2412.32, "text": " Or did you stumble upon it while looking at something else?", "tokens": [1610, 630, 291, 41302, 3564, 309, 1339, 1237, 412, 746, 1646, 30], "temperature": 0.0, "avg_logprob": -0.13331469551461642, "compression_ratio": 1.5884615384615384, "no_speech_prob": 2.17814595089294e-05}, {"id": 463, "seek": 239856, "start": 2412.32, "end": 2415.6, "text": " Oh, I mean, it just seemed very obviously to me,", "tokens": [876, 11, 286, 914, 11, 309, 445, 6576, 588, 2745, 281, 385, 11], "temperature": 0.0, "avg_logprob": -0.13331469551461642, "compression_ratio": 1.5884615384615384, "no_speech_prob": 2.17814595089294e-05}, {"id": 464, "seek": 239856, "start": 2415.6, "end": 2417.52, "text": " like something which obviously we should do.", "tokens": [411, 746, 597, 2745, 321, 820, 360, 13], "temperature": 0.0, "avg_logprob": -0.13331469551461642, "compression_ratio": 1.5884615384615384, "no_speech_prob": 2.17814595089294e-05}, {"id": 465, "seek": 239856, "start": 2417.52, "end": 2420.64, "text": " Because we were spending, OK, so on Dawnbench,", "tokens": [1436, 321, 645, 6434, 11, 2264, 11, 370, 322, 26001, 47244, 11], "temperature": 0.0, "avg_logprob": -0.13331469551461642, "compression_ratio": 1.5884615384615384, "no_speech_prob": 2.17814595089294e-05}, {"id": 466, "seek": 239856, "start": 2420.64, "end": 2421.92, "text": " we were training on ImageNet.", "tokens": [321, 645, 3097, 322, 29903, 31890, 13], "temperature": 0.0, "avg_logprob": -0.13331469551461642, "compression_ratio": 1.5884615384615384, "no_speech_prob": 2.17814595089294e-05}, {"id": 467, "seek": 239856, "start": 2421.92, "end": 2426.48, "text": " It was taking 12 hours, I guess, to train a single model.", "tokens": [467, 390, 1940, 2272, 2496, 11, 286, 2041, 11, 281, 3847, 257, 2167, 2316, 13], "temperature": 0.0, "avg_logprob": -0.13331469551461642, "compression_ratio": 1.5884615384615384, "no_speech_prob": 2.17814595089294e-05}, {"id": 468, "seek": 242648, "start": 2426.48, "end": 2429.92, "text": " And the vast majority of that time,", "tokens": [400, 264, 8369, 6286, 295, 300, 565, 11], "temperature": 0.0, "avg_logprob": -0.2855108243609787, "compression_ratio": 1.6099585062240664, "no_speech_prob": 2.246854273835197e-05}, {"id": 469, "seek": 242648, "start": 2430.96, "end": 2434.72, "text": " it's just recognizing very, very basic things about images.", "tokens": [309, 311, 445, 18538, 588, 11, 588, 3875, 721, 466, 5267, 13], "temperature": 0.0, "avg_logprob": -0.2855108243609787, "compression_ratio": 1.6099585062240664, "no_speech_prob": 2.246854273835197e-05}, {"id": 470, "seek": 242648, "start": 2434.72, "end": 2438.0, "text": " You know, it's not learning the finer details", "tokens": [509, 458, 11, 309, 311, 406, 2539, 264, 39130, 4365], "temperature": 0.0, "avg_logprob": -0.2855108243609787, "compression_ratio": 1.6099585062240664, "no_speech_prob": 2.246854273835197e-05}, {"id": 471, "seek": 242648, "start": 2438.0, "end": 2439.36, "text": " of different cat breeds or whatever,", "tokens": [295, 819, 3857, 41609, 420, 2035, 11], "temperature": 0.0, "avg_logprob": -0.2855108243609787, "compression_ratio": 1.6099585062240664, "no_speech_prob": 2.246854273835197e-05}, {"id": 472, "seek": 242648, "start": 2439.36, "end": 2440.4, "text": " but it's just trying to understand", "tokens": [457, 309, 311, 445, 1382, 281, 1223], "temperature": 0.0, "avg_logprob": -0.2855108243609787, "compression_ratio": 1.6099585062240664, "no_speech_prob": 2.246854273835197e-05}, {"id": 473, "seek": 242648, "start": 2440.4, "end": 2443.2, "text": " about the concepts of like fur or sky or metal.", "tokens": [466, 264, 10392, 295, 411, 2687, 420, 5443, 420, 5760, 13], "temperature": 0.0, "avg_logprob": -0.2855108243609787, "compression_ratio": 1.6099585062240664, "no_speech_prob": 2.246854273835197e-05}, {"id": 474, "seek": 242648, "start": 2444.08, "end": 2446.48, "text": " And I thought, well, there's no, there's absolutely no reason", "tokens": [400, 286, 1194, 11, 731, 11, 456, 311, 572, 11, 456, 311, 3122, 572, 1778], "temperature": 0.0, "avg_logprob": -0.2855108243609787, "compression_ratio": 1.6099585062240664, "no_speech_prob": 2.246854273835197e-05}, {"id": 475, "seek": 242648, "start": 2446.48, "end": 2450.96, "text": " to need 224 by 224 pixel images to be able to do that.", "tokens": [281, 643, 5853, 19, 538, 5853, 19, 19261, 5267, 281, 312, 1075, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.2855108243609787, "compression_ratio": 1.6099585062240664, "no_speech_prob": 2.246854273835197e-05}, {"id": 476, "seek": 242648, "start": 2450.96, "end": 2452.48, "text": " You know?", "tokens": [509, 458, 30], "temperature": 0.0, "avg_logprob": -0.2855108243609787, "compression_ratio": 1.6099585062240664, "no_speech_prob": 2.246854273835197e-05}, {"id": 477, "seek": 245248, "start": 2452.48, "end": 2456.88, "text": " Like, it just seemed obviously stupid that we would do it.", "tokens": [1743, 11, 309, 445, 6576, 2745, 6631, 300, 321, 576, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.30438711093022275, "compression_ratio": 1.7295081967213115, "no_speech_prob": 6.240421043912647e-06}, {"id": 478, "seek": 245248, "start": 2457.76, "end": 2460.16, "text": " And partly it was like also like I was just generally", "tokens": [400, 17031, 309, 390, 411, 611, 411, 286, 390, 445, 5101], "temperature": 0.0, "avg_logprob": -0.30438711093022275, "compression_ratio": 1.7295081967213115, "no_speech_prob": 6.240421043912647e-06}, {"id": 479, "seek": 245248, "start": 2460.16, "end": 2464.4, "text": " interested in changing things during training.", "tokens": [3102, 294, 4473, 721, 1830, 3097, 13], "temperature": 0.0, "avg_logprob": -0.30438711093022275, "compression_ratio": 1.7295081967213115, "no_speech_prob": 6.240421043912647e-06}, {"id": 480, "seek": 245248, "start": 2465.36, "end": 2468.0, "text": " So what if, you know, in particular learning rates, right?", "tokens": [407, 437, 498, 11, 291, 458, 11, 294, 1729, 2539, 6846, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.30438711093022275, "compression_ratio": 1.7295081967213115, "no_speech_prob": 6.240421043912647e-06}, {"id": 481, "seek": 245248, "start": 2468.8, "end": 2471.84, "text": " So the idea of changing learning rates during training", "tokens": [407, 264, 1558, 295, 4473, 2539, 6846, 1830, 3097], "temperature": 0.0, "avg_logprob": -0.30438711093022275, "compression_ratio": 1.7295081967213115, "no_speech_prob": 6.240421043912647e-06}, {"id": 482, "seek": 245248, "start": 2471.84, "end": 2474.72, "text": " goes back a lot longer than Dawnbench,", "tokens": [1709, 646, 257, 688, 2854, 813, 26001, 47244, 11], "temperature": 0.0, "avg_logprob": -0.30438711093022275, "compression_ratio": 1.7295081967213115, "no_speech_prob": 6.240421043912647e-06}, {"id": 483, "seek": 245248, "start": 2474.72, "end": 2476.2400000000002, "text": " that people have been generally training them", "tokens": [300, 561, 362, 668, 5101, 3097, 552], "temperature": 0.0, "avg_logprob": -0.30438711093022275, "compression_ratio": 1.7295081967213115, "no_speech_prob": 6.240421043912647e-06}, {"id": 484, "seek": 245248, "start": 2476.2400000000002, "end": 2479.04, "text": " by having a learning rate that kind of dropped", "tokens": [538, 1419, 257, 2539, 3314, 300, 733, 295, 8119], "temperature": 0.0, "avg_logprob": -0.30438711093022275, "compression_ratio": 1.7295081967213115, "no_speech_prob": 6.240421043912647e-06}, {"id": 485, "seek": 245248, "start": 2479.04, "end": 2480.4, "text": " by a little bit.", "tokens": [538, 257, 707, 857, 13], "temperature": 0.0, "avg_logprob": -0.30438711093022275, "compression_ratio": 1.7295081967213115, "no_speech_prob": 6.240421043912647e-06}, {"id": 486, "seek": 248040, "start": 2480.4, "end": 2482.64, "text": " Learning rate that kind of dropped by a lot", "tokens": [15205, 3314, 300, 733, 295, 8119, 538, 257, 688], "temperature": 0.0, "avg_logprob": -0.07782555307660784, "compression_ratio": 1.9347079037800687, "no_speech_prob": 4.984692350262776e-05}, {"id": 487, "seek": 248040, "start": 2482.64, "end": 2484.7200000000003, "text": " and then stayed flat and dropped by a lot and stayed flat.", "tokens": [293, 550, 9181, 4962, 293, 8119, 538, 257, 688, 293, 9181, 4962, 13], "temperature": 0.0, "avg_logprob": -0.07782555307660784, "compression_ratio": 1.9347079037800687, "no_speech_prob": 4.984692350262776e-05}, {"id": 488, "seek": 248040, "start": 2485.44, "end": 2488.7200000000003, "text": " And Leslie Smith in particular came up with this idea", "tokens": [400, 28140, 8538, 294, 1729, 1361, 493, 365, 341, 1558], "temperature": 0.0, "avg_logprob": -0.07782555307660784, "compression_ratio": 1.9347079037800687, "no_speech_prob": 4.984692350262776e-05}, {"id": 489, "seek": 248040, "start": 2488.7200000000003, "end": 2491.52, "text": " of kind of like gradually increasing it over a curve", "tokens": [295, 733, 295, 411, 13145, 5662, 309, 670, 257, 7605], "temperature": 0.0, "avg_logprob": -0.07782555307660784, "compression_ratio": 1.9347079037800687, "no_speech_prob": 4.984692350262776e-05}, {"id": 490, "seek": 248040, "start": 2491.52, "end": 2494.0, "text": " and then gradually decreasing it following another curve.", "tokens": [293, 550, 13145, 23223, 309, 3480, 1071, 7605, 13], "temperature": 0.0, "avg_logprob": -0.07782555307660784, "compression_ratio": 1.9347079037800687, "no_speech_prob": 4.984692350262776e-05}, {"id": 491, "seek": 248040, "start": 2494.7200000000003, "end": 2496.56, "text": " And so I was definitely in the mindset of like,", "tokens": [400, 370, 286, 390, 2138, 294, 264, 12543, 295, 411, 11], "temperature": 0.0, "avg_logprob": -0.07782555307660784, "compression_ratio": 1.9347079037800687, "no_speech_prob": 4.984692350262776e-05}, {"id": 492, "seek": 248040, "start": 2496.56, "end": 2497.92, "text": " oh, there's kind of interesting things", "tokens": [1954, 11, 456, 311, 733, 295, 1880, 721], "temperature": 0.0, "avg_logprob": -0.07782555307660784, "compression_ratio": 1.9347079037800687, "no_speech_prob": 4.984692350262776e-05}, {"id": 493, "seek": 248040, "start": 2497.92, "end": 2499.28, "text": " we can change during training.", "tokens": [321, 393, 1319, 1830, 3097, 13], "temperature": 0.0, "avg_logprob": -0.07782555307660784, "compression_ratio": 1.9347079037800687, "no_speech_prob": 4.984692350262776e-05}, {"id": 494, "seek": 248040, "start": 2499.28, "end": 2502.56, "text": " So I was looking at like, oh, what if we change data", "tokens": [407, 286, 390, 1237, 412, 411, 11, 1954, 11, 437, 498, 321, 1319, 1412], "temperature": 0.0, "avg_logprob": -0.07782555307660784, "compression_ratio": 1.9347079037800687, "no_speech_prob": 4.984692350262776e-05}, {"id": 495, "seek": 248040, "start": 2502.56, "end": 2504.7200000000003, "text": " augmentation during training, for example?", "tokens": [14501, 19631, 1830, 3097, 11, 337, 1365, 30], "temperature": 0.0, "avg_logprob": -0.07782555307660784, "compression_ratio": 1.9347079037800687, "no_speech_prob": 4.984692350262776e-05}, {"id": 496, "seek": 248040, "start": 2504.7200000000003, "end": 2507.12, "text": " Like maybe towards the end of training,", "tokens": [1743, 1310, 3030, 264, 917, 295, 3097, 11], "temperature": 0.0, "avg_logprob": -0.07782555307660784, "compression_ratio": 1.9347079037800687, "no_speech_prob": 4.984692350262776e-05}, {"id": 497, "seek": 248040, "start": 2507.12, "end": 2509.28, "text": " we should like turn off data augmentation", "tokens": [321, 820, 411, 1261, 766, 1412, 14501, 19631], "temperature": 0.0, "avg_logprob": -0.07782555307660784, "compression_ratio": 1.9347079037800687, "no_speech_prob": 4.984692350262776e-05}, {"id": 498, "seek": 250928, "start": 2509.28, "end": 2511.92, "text": " so it could learn what unaugmented images look like", "tokens": [370, 309, 727, 1466, 437, 517, 20056, 14684, 5267, 574, 411], "temperature": 0.0, "avg_logprob": -0.07687631086869673, "compression_ratio": 1.6265560165975104, "no_speech_prob": 1.1477739462861791e-05}, {"id": 499, "seek": 250928, "start": 2511.92, "end": 2514.7200000000003, "text": " because that's what we really care about, for example.", "tokens": [570, 300, 311, 437, 321, 534, 1127, 466, 11, 337, 1365, 13], "temperature": 0.0, "avg_logprob": -0.07687631086869673, "compression_ratio": 1.6265560165975104, "no_speech_prob": 1.1477739462861791e-05}, {"id": 500, "seek": 250928, "start": 2518.2400000000002, "end": 2520.0, "text": " So yeah, that was the kind of stuff", "tokens": [407, 1338, 11, 300, 390, 264, 733, 295, 1507], "temperature": 0.0, "avg_logprob": -0.07687631086869673, "compression_ratio": 1.6265560165975104, "no_speech_prob": 1.1477739462861791e-05}, {"id": 501, "seek": 250928, "start": 2520.8, "end": 2523.52, "text": " that I was kind of interested in at the time.", "tokens": [300, 286, 390, 733, 295, 3102, 294, 412, 264, 565, 13], "temperature": 0.0, "avg_logprob": -0.07687631086869673, "compression_ratio": 1.6265560165975104, "no_speech_prob": 1.1477739462861791e-05}, {"id": 502, "seek": 250928, "start": 2523.52, "end": 2525.36, "text": " And so yeah, definitely this thing of like,", "tokens": [400, 370, 1338, 11, 2138, 341, 551, 295, 411, 11], "temperature": 0.0, "avg_logprob": -0.07687631086869673, "compression_ratio": 1.6265560165975104, "no_speech_prob": 1.1477739462861791e-05}, {"id": 503, "seek": 250928, "start": 2528.32, "end": 2534.6400000000003, "text": " you know, why are we looking over 224 by 224 pixel images", "tokens": [291, 458, 11, 983, 366, 321, 1237, 670, 5853, 19, 538, 5853, 19, 19261, 5267], "temperature": 0.0, "avg_logprob": -0.07687631086869673, "compression_ratio": 1.6265560165975104, "no_speech_prob": 1.1477739462861791e-05}, {"id": 504, "seek": 250928, "start": 2534.6400000000003, "end": 2535.6800000000003, "text": " the entire time?", "tokens": [264, 2302, 565, 30], "temperature": 0.0, "avg_logprob": -0.07687631086869673, "compression_ratio": 1.6265560165975104, "no_speech_prob": 1.1477739462861791e-05}, {"id": 505, "seek": 250928, "start": 2535.6800000000003, "end": 2537.6800000000003, "text": " Like that just seemed obviously stupid.", "tokens": [1743, 300, 445, 6576, 2745, 6631, 13], "temperature": 0.0, "avg_logprob": -0.07687631086869673, "compression_ratio": 1.6265560165975104, "no_speech_prob": 1.1477739462861791e-05}, {"id": 506, "seek": 250928, "start": 2537.6800000000003, "end": 2539.0400000000004, "text": " And so it wasn't something where I was like,", "tokens": [400, 370, 309, 2067, 380, 746, 689, 286, 390, 411, 11], "temperature": 0.0, "avg_logprob": -0.07687631086869673, "compression_ratio": 1.6265560165975104, "no_speech_prob": 1.1477739462861791e-05}, {"id": 507, "seek": 253904, "start": 2539.04, "end": 2540.08, "text": " wow, here's a crazy idea.", "tokens": [6076, 11, 510, 311, 257, 3219, 1558, 13], "temperature": 0.0, "avg_logprob": -0.16683055736400462, "compression_ratio": 1.4743589743589745, "no_speech_prob": 2.7533451429917477e-05}, {"id": 508, "seek": 253904, "start": 2540.08, "end": 2541.2, "text": " I bet it won't work.", "tokens": [286, 778, 309, 1582, 380, 589, 13], "temperature": 0.0, "avg_logprob": -0.16683055736400462, "compression_ratio": 1.4743589743589745, "no_speech_prob": 2.7533451429917477e-05}, {"id": 509, "seek": 253904, "start": 2541.2, "end": 2542.8, "text": " As soon as I thought of it, I just thought,", "tokens": [1018, 2321, 382, 286, 1194, 295, 309, 11, 286, 445, 1194, 11], "temperature": 0.0, "avg_logprob": -0.16683055736400462, "compression_ratio": 1.4743589743589745, "no_speech_prob": 2.7533451429917477e-05}, {"id": 510, "seek": 253904, "start": 2542.8, "end": 2544.56, "text": " okay, this is definitely going to work.", "tokens": [1392, 11, 341, 307, 2138, 516, 281, 589, 13], "temperature": 0.0, "avg_logprob": -0.16683055736400462, "compression_ratio": 1.4743589743589745, "no_speech_prob": 2.7533451429917477e-05}, {"id": 511, "seek": 253904, "start": 2544.56, "end": 2546.24, "text": " You know, that it did.", "tokens": [509, 458, 11, 300, 309, 630, 13], "temperature": 0.0, "avg_logprob": -0.16683055736400462, "compression_ratio": 1.4743589743589745, "no_speech_prob": 2.7533451429917477e-05}, {"id": 512, "seek": 253904, "start": 2548.08, "end": 2548.88, "text": " Interesting. Thanks.", "tokens": [14711, 13, 2561, 13], "temperature": 0.0, "avg_logprob": -0.16683055736400462, "compression_ratio": 1.4743589743589745, "no_speech_prob": 2.7533451429917477e-05}, {"id": 513, "seek": 253904, "start": 2552.16, "end": 2554.0, "text": " One question I have for you, Jeremy.", "tokens": [1485, 1168, 286, 362, 337, 291, 11, 17809, 13], "temperature": 0.0, "avg_logprob": -0.16683055736400462, "compression_ratio": 1.4743589743589745, "no_speech_prob": 2.7533451429917477e-05}, {"id": 514, "seek": 253904, "start": 2555.2799999999997, "end": 2558.24, "text": " There was a paper that came out like in 2019", "tokens": [821, 390, 257, 3035, 300, 1361, 484, 411, 294, 6071], "temperature": 0.0, "avg_logprob": -0.16683055736400462, "compression_ratio": 1.4743589743589745, "no_speech_prob": 2.7533451429917477e-05}, {"id": 515, "seek": 253904, "start": 2558.24, "end": 2560.96, "text": " called fixing the test train resolution discrepancy.", "tokens": [1219, 19442, 264, 1500, 3847, 8669, 2983, 265, 6040, 1344, 13], "temperature": 0.0, "avg_logprob": -0.16683055736400462, "compression_ratio": 1.4743589743589745, "no_speech_prob": 2.7533451429917477e-05}, {"id": 516, "seek": 253904, "start": 2563.68, "end": 2566.08, "text": " Yeah, were they like trained on 224", "tokens": [865, 11, 645, 436, 411, 8895, 322, 5853, 19], "temperature": 0.0, "avg_logprob": -0.16683055736400462, "compression_ratio": 1.4743589743589745, "no_speech_prob": 2.7533451429917477e-05}, {"id": 517, "seek": 256608, "start": 2566.08, "end": 2570.0, "text": " and then did inference finally on like 320 by 320?", "tokens": [293, 550, 630, 38253, 2721, 322, 411, 42429, 538, 42429, 30], "temperature": 0.0, "avg_logprob": -0.11254709820414699, "compression_ratio": 1.4676616915422886, "no_speech_prob": 1.0129199836228509e-05}, {"id": 518, "seek": 256608, "start": 2570.0, "end": 2570.5, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.11254709820414699, "compression_ratio": 1.4676616915422886, "no_speech_prob": 1.0129199836228509e-05}, {"id": 519, "seek": 256608, "start": 2571.7599999999998, "end": 2574.3199999999997, "text": " Have you seen that still sort of work?", "tokens": [3560, 291, 1612, 300, 920, 1333, 295, 589, 30], "temperature": 0.0, "avg_logprob": -0.11254709820414699, "compression_ratio": 1.4676616915422886, "no_speech_prob": 1.0129199836228509e-05}, {"id": 520, "seek": 256608, "start": 2574.3199999999997, "end": 2577.12, "text": " Have you done that at all in your workflow?", "tokens": [3560, 291, 1096, 300, 412, 439, 294, 428, 20993, 30], "temperature": 0.0, "avg_logprob": -0.11254709820414699, "compression_ratio": 1.4676616915422886, "no_speech_prob": 1.0129199836228509e-05}, {"id": 521, "seek": 256608, "start": 2577.12, "end": 2578.72, "text": " I mean, honestly, I don't remember.", "tokens": [286, 914, 11, 6095, 11, 286, 500, 380, 1604, 13], "temperature": 0.0, "avg_logprob": -0.11254709820414699, "compression_ratio": 1.4676616915422886, "no_speech_prob": 1.0129199836228509e-05}, {"id": 522, "seek": 256608, "start": 2578.72, "end": 2581.04, "text": " I need to revisit that paper because you're right.", "tokens": [286, 643, 281, 32676, 300, 3035, 570, 291, 434, 558, 13], "temperature": 0.0, "avg_logprob": -0.11254709820414699, "compression_ratio": 1.4676616915422886, "no_speech_prob": 1.0129199836228509e-05}, {"id": 523, "seek": 256608, "start": 2581.04, "end": 2582.08, "text": " It's important tonight.", "tokens": [467, 311, 1021, 4440, 13], "temperature": 0.0, "avg_logprob": -0.11254709820414699, "compression_ratio": 1.4676616915422886, "no_speech_prob": 1.0129199836228509e-05}, {"id": 524, "seek": 256608, "start": 2591.36, "end": 2594.24, "text": " You know, I would generally try to fine tune", "tokens": [509, 458, 11, 286, 576, 5101, 853, 281, 2489, 10864], "temperature": 0.0, "avg_logprob": -0.11254709820414699, "compression_ratio": 1.4676616915422886, "no_speech_prob": 1.0129199836228509e-05}, {"id": 525, "seek": 259424, "start": 2594.24, "end": 2598.56, "text": " on the final size I was going to be predicting on anyway.", "tokens": [322, 264, 2572, 2744, 286, 390, 516, 281, 312, 32884, 322, 4033, 13], "temperature": 0.0, "avg_logprob": -0.0951773416428339, "compression_ratio": 1.5550660792951543, "no_speech_prob": 2.42974183493061e-05}, {"id": 526, "seek": 259424, "start": 2601.3599999999997, "end": 2604.72, "text": " So yeah, I guess we'll kind of see how we go with this, right?", "tokens": [407, 1338, 11, 286, 2041, 321, 603, 733, 295, 536, 577, 321, 352, 365, 341, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.0951773416428339, "compression_ratio": 1.5550660792951543, "no_speech_prob": 2.42974183493061e-05}, {"id": 527, "seek": 259424, "start": 2604.72, "end": 2606.3199999999997, "text": " I mean, you can definitely take a model", "tokens": [286, 914, 11, 291, 393, 2138, 747, 257, 2316], "temperature": 0.0, "avg_logprob": -0.0951773416428339, "compression_ratio": 1.5550660792951543, "no_speech_prob": 2.42974183493061e-05}, {"id": 528, "seek": 259424, "start": 2606.3199999999997, "end": 2609.4399999999996, "text": " that was trained on 224 by 224 images", "tokens": [300, 390, 8895, 322, 5853, 19, 538, 5853, 19, 5267], "temperature": 0.0, "avg_logprob": -0.0951773416428339, "compression_ratio": 1.5550660792951543, "no_speech_prob": 2.42974183493061e-05}, {"id": 529, "seek": 259424, "start": 2610.8799999999997, "end": 2614.24, "text": " and use it to predict 360 by 360 images.", "tokens": [293, 764, 309, 281, 6069, 13898, 538, 13898, 5267, 13], "temperature": 0.0, "avg_logprob": -0.0951773416428339, "compression_ratio": 1.5550660792951543, "no_speech_prob": 2.42974183493061e-05}, {"id": 530, "seek": 259424, "start": 2615.04, "end": 2616.3999999999996, "text": " And it will generally go pretty well.", "tokens": [400, 309, 486, 5101, 352, 1238, 731, 13], "temperature": 0.0, "avg_logprob": -0.0951773416428339, "compression_ratio": 1.5550660792951543, "no_speech_prob": 2.42974183493061e-05}, {"id": 531, "seek": 259424, "start": 2617.52, "end": 2619.8399999999997, "text": " But I think it'll go better if you first fine tune it", "tokens": [583, 286, 519, 309, 603, 352, 1101, 498, 291, 700, 2489, 10864, 309], "temperature": 0.0, "avg_logprob": -0.0951773416428339, "compression_ratio": 1.5550660792951543, "no_speech_prob": 2.42974183493061e-05}, {"id": 532, "seek": 259424, "start": 2619.8399999999997, "end": 2621.4399999999996, "text": " on 360 by 360 images.", "tokens": [322, 13898, 538, 13898, 5267, 13], "temperature": 0.0, "avg_logprob": -0.0951773416428339, "compression_ratio": 1.5550660792951543, "no_speech_prob": 2.42974183493061e-05}, {"id": 533, "seek": 262144, "start": 2621.44, "end": 2624.7200000000003, "text": " Yeah, I don't think they tried pre-training", "tokens": [865, 11, 286, 500, 380, 519, 436, 3031, 659, 12, 17227, 1760], "temperature": 0.0, "avg_logprob": -0.3153500701441909, "compression_ratio": 1.735408560311284, "no_speech_prob": 4.784870725416113e-06}, {"id": 534, "seek": 262144, "start": 2624.7200000000003, "end": 2629.36, "text": " and then also training on like 320 versus just 320 and the 224.", "tokens": [293, 550, 611, 3097, 322, 411, 42429, 5717, 445, 42429, 293, 264, 5853, 19, 13], "temperature": 0.0, "avg_logprob": -0.3153500701441909, "compression_ratio": 1.735408560311284, "no_speech_prob": 4.784870725416113e-06}, {"id": 535, "seek": 262144, "start": 2629.36, "end": 2629.86, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.3153500701441909, "compression_ratio": 1.735408560311284, "no_speech_prob": 4.784870725416113e-06}, {"id": 536, "seek": 262144, "start": 2629.86, "end": 2631.76, "text": " That would definitely be an interesting experiment.", "tokens": [663, 576, 2138, 312, 364, 1880, 5120, 13], "temperature": 0.0, "avg_logprob": -0.3153500701441909, "compression_ratio": 1.735408560311284, "no_speech_prob": 4.784870725416113e-06}, {"id": 537, "seek": 262144, "start": 2631.76, "end": 2633.36, "text": " Yeah, it would be an interesting experiment.", "tokens": [865, 11, 309, 576, 312, 364, 1880, 5120, 13], "temperature": 0.0, "avg_logprob": -0.3153500701441909, "compression_ratio": 1.735408560311284, "no_speech_prob": 4.784870725416113e-06}, {"id": 538, "seek": 262144, "start": 2633.36, "end": 2636.4, "text": " And it's definitely something that any of us here could do.", "tokens": [400, 309, 311, 2138, 746, 300, 604, 295, 505, 510, 727, 360, 13], "temperature": 0.0, "avg_logprob": -0.3153500701441909, "compression_ratio": 1.735408560311284, "no_speech_prob": 4.784870725416113e-06}, {"id": 539, "seek": 262144, "start": 2636.4, "end": 2638.4, "text": " You know, I think it'd be cool.", "tokens": [509, 458, 11, 286, 519, 309, 1116, 312, 1627, 13], "temperature": 0.0, "avg_logprob": -0.3153500701441909, "compression_ratio": 1.735408560311284, "no_speech_prob": 4.784870725416113e-06}, {"id": 540, "seek": 262144, "start": 2639.28, "end": 2639.76, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.3153500701441909, "compression_ratio": 1.735408560311284, "no_speech_prob": 4.784870725416113e-06}, {"id": 541, "seek": 262144, "start": 2639.76, "end": 2641.52, "text": " So let's try scaling this up.", "tokens": [407, 718, 311, 853, 21589, 341, 493, 13], "temperature": 0.0, "avg_logprob": -0.3153500701441909, "compression_ratio": 1.735408560311284, "no_speech_prob": 4.784870725416113e-06}, {"id": 542, "seek": 262144, "start": 2641.52, "end": 2645.76, "text": " So we can change these two lines to one.", "tokens": [407, 321, 393, 1319, 613, 732, 3876, 281, 472, 13], "temperature": 0.0, "avg_logprob": -0.3153500701441909, "compression_ratio": 1.735408560311284, "no_speech_prob": 4.784870725416113e-06}, {"id": 543, "seek": 262144, "start": 2645.76, "end": 2648.48, "text": " And so this is one something I often do is I do things like,", "tokens": [400, 370, 341, 307, 472, 746, 286, 2049, 360, 307, 286, 360, 721, 411, 11], "temperature": 0.0, "avg_logprob": -0.3153500701441909, "compression_ratio": 1.735408560311284, "no_speech_prob": 4.784870725416113e-06}, {"id": 544, "seek": 262144, "start": 2648.48, "end": 2648.98, "text": " yep.", "tokens": [18633, 13], "temperature": 0.0, "avg_logprob": -0.3153500701441909, "compression_ratio": 1.735408560311284, "no_speech_prob": 4.784870725416113e-06}, {"id": 545, "seek": 264898, "start": 2648.98, "end": 2650.98, "text": " I think we don't have your screen.", "tokens": [286, 519, 321, 500, 380, 362, 428, 2568, 13], "temperature": 0.0, "avg_logprob": -0.3557355595731187, "compression_ratio": 1.4307692307692308, "no_speech_prob": 3.762783308047801e-05}, {"id": 546, "seek": 264898, "start": 2650.98, "end": 2651.48, "text": " OK.", "tokens": [2264, 13], "temperature": 0.0, "avg_logprob": -0.3557355595731187, "compression_ratio": 1.4307692307692308, "no_speech_prob": 3.762783308047801e-05}, {"id": 547, "seek": 264898, "start": 2652.18, "end": 2659.94, "text": " So as I was saying, previously, I had like two cells to do this.", "tokens": [407, 382, 286, 390, 1566, 11, 8046, 11, 286, 632, 411, 732, 5438, 281, 360, 341, 13], "temperature": 0.0, "avg_logprob": -0.3557355595731187, "compression_ratio": 1.4307692307692308, "no_speech_prob": 3.762783308047801e-05}, {"id": 548, "seek": 264898, "start": 2659.94, "end": 2662.1, "text": " And so now I'm just going to combine it into one cell.", "tokens": [400, 370, 586, 286, 478, 445, 516, 281, 10432, 309, 666, 472, 2815, 13], "temperature": 0.0, "avg_logprob": -0.3557355595731187, "compression_ratio": 1.4307692307692308, "no_speech_prob": 3.762783308047801e-05}, {"id": 549, "seek": 264898, "start": 2662.9, "end": 2664.66, "text": " This is what I tend to do as I fiddle around", "tokens": [639, 307, 437, 286, 3928, 281, 360, 382, 286, 24553, 2285, 926], "temperature": 0.0, "avg_logprob": -0.3557355595731187, "compression_ratio": 1.4307692307692308, "no_speech_prob": 3.762783308047801e-05}, {"id": 550, "seek": 264898, "start": 2664.66, "end": 2668.18, "text": " as I try to like gradually make things", "tokens": [382, 286, 853, 281, 411, 13145, 652, 721], "temperature": 0.0, "avg_logprob": -0.3557355595731187, "compression_ratio": 1.4307692307692308, "no_speech_prob": 3.762783308047801e-05}, {"id": 551, "seek": 264898, "start": 2669.06, "end": 2672.18, "text": " a little bit more concise, you know.", "tokens": [257, 707, 857, 544, 44882, 11, 291, 458, 13], "temperature": 0.0, "avg_logprob": -0.3557355595731187, "compression_ratio": 1.4307692307692308, "no_speech_prob": 3.762783308047801e-05}, {"id": 552, "seek": 267218, "start": 2672.18, "end": 2676.18, "text": " OK.", "tokens": [2264, 13], "temperature": 0.0, "avg_logprob": -0.832360538435571, "compression_ratio": 1.5477707006369428, "no_speech_prob": 5.560568388318643e-05}, {"id": 553, "seek": 267218, "start": 2678.18, "end": 2686.98, "text": " Does it make sense to go smaller than the size of the original pre-training", "tokens": [4402, 309, 652, 2020, 281, 352, 4356, 813, 264, 2744, 295, 264, 3380, 659, 12, 17227, 1760], "temperature": 0.0, "avg_logprob": -0.832360538435571, "compression_ratio": 1.5477707006369428, "no_speech_prob": 5.560568388318643e-05}, {"id": 554, "seek": 267218, "start": 2687.62, "end": 2688.4199999999996, "text": " like ConvNet?", "tokens": [411, 2656, 85, 31890, 30], "temperature": 0.0, "avg_logprob": -0.832360538435571, "compression_ratio": 1.5477707006369428, "no_speech_prob": 5.560568388318643e-05}, {"id": 555, "seek": 267218, "start": 2689.62, "end": 2691.14, "text": " ConvNext?", "tokens": [2656, 85, 31002, 30], "temperature": 0.0, "avg_logprob": -0.832360538435571, "compression_ratio": 1.5477707006369428, "no_speech_prob": 5.560568388318643e-05}, {"id": 556, "seek": 267218, "start": 2692.2599999999998, "end": 2692.7599999999998, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.832360538435571, "compression_ratio": 1.5477707006369428, "no_speech_prob": 5.560568388318643e-05}, {"id": 557, "seek": 267218, "start": 2692.7599999999998, "end": 2694.58, "text": " I mean, you can fine tune to any size.", "tokens": [286, 914, 11, 291, 393, 2489, 10864, 281, 604, 2744, 13], "temperature": 0.0, "avg_logprob": -0.832360538435571, "compression_ratio": 1.5477707006369428, "no_speech_prob": 5.560568388318643e-05}, {"id": 558, "seek": 267218, "start": 2694.58, "end": 2697.14, "text": " But I think it's a little bit more complicated.", "tokens": [583, 286, 519, 309, 311, 257, 707, 857, 544, 6179, 13], "temperature": 0.0, "avg_logprob": -0.832360538435571, "compression_ratio": 1.5477707006369428, "no_speech_prob": 5.560568388318643e-05}, {"id": 559, "seek": 267218, "start": 2697.14, "end": 2699.14, "text": " So I think it's a little bit more complicated.", "tokens": [407, 286, 519, 309, 311, 257, 707, 857, 544, 6179, 13], "temperature": 0.0, "avg_logprob": -0.832360538435571, "compression_ratio": 1.5477707006369428, "no_speech_prob": 5.560568388318643e-05}, {"id": 560, "seek": 269914, "start": 2699.14, "end": 2702.74, "text": " But yeah, I mean, you can fine tune to any size you like.", "tokens": [583, 1338, 11, 286, 914, 11, 291, 393, 2489, 10864, 281, 604, 2744, 291, 411, 13], "temperature": 0.0, "avg_logprob": -0.16355483796861436, "compression_ratio": 1.4473684210526316, "no_speech_prob": 5.7716820265341084e-06}, {"id": 561, "seek": 269914, "start": 2704.58, "end": 2705.22, "text": " Absolutely.", "tokens": [7021, 13], "temperature": 0.0, "avg_logprob": -0.16355483796861436, "compression_ratio": 1.4473684210526316, "no_speech_prob": 5.7716820265341084e-06}, {"id": 562, "seek": 269914, "start": 2706.18, "end": 2708.02, "text": " I'm just going to get rid of the zero padding", "tokens": [286, 478, 445, 516, 281, 483, 3973, 295, 264, 4018, 39562], "temperature": 0.0, "avg_logprob": -0.16355483796861436, "compression_ratio": 1.4473684210526316, "no_speech_prob": 5.7716820265341084e-06}, {"id": 563, "seek": 269914, "start": 2708.02, "end": 2710.9, "text": " because again, I want to like try to change it a little bit each time", "tokens": [570, 797, 11, 286, 528, 281, 411, 853, 281, 1319, 309, 257, 707, 857, 1184, 565], "temperature": 0.0, "avg_logprob": -0.16355483796861436, "compression_ratio": 1.4473684210526316, "no_speech_prob": 5.7716820265341084e-06}, {"id": 564, "seek": 269914, "start": 2710.9, "end": 2714.98, "text": " just to kind of, you know, it's a kind of augmentation, right?", "tokens": [445, 281, 733, 295, 11, 291, 458, 11, 309, 311, 257, 733, 295, 14501, 19631, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.16355483796861436, "compression_ratio": 1.4473684210526316, "no_speech_prob": 5.7716820265341084e-06}, {"id": 565, "seek": 269914, "start": 2718.1, "end": 2718.5, "text": " So OK.", "tokens": [407, 2264, 13], "temperature": 0.0, "avg_logprob": -0.16355483796861436, "compression_ratio": 1.4473684210526316, "no_speech_prob": 5.7716820265341084e-06}, {"id": 566, "seek": 269914, "start": 2718.5, "end": 2719.7, "text": " So it's got to 192.", "tokens": [407, 309, 311, 658, 281, 1294, 17, 13], "temperature": 0.0, "avg_logprob": -0.16355483796861436, "compression_ratio": 1.4473684210526316, "no_speech_prob": 5.7716820265341084e-06}, {"id": 567, "seek": 271970, "start": 2719.7, "end": 2732.98, "text": " You know, one thing I find encouraging is that, you know, my training loss isn't getting way", "tokens": [509, 458, 11, 472, 551, 286, 915, 14580, 307, 300, 11, 291, 458, 11, 452, 3097, 4470, 1943, 380, 1242, 636], "temperature": 0.0, "avg_logprob": -0.17984736760457357, "compression_ratio": 1.4675324675324675, "no_speech_prob": 5.09328401676612e-06}, {"id": 568, "seek": 271970, "start": 2732.98, "end": 2734.2599999999998, "text": " underneath the validation loss.", "tokens": [7223, 264, 24071, 4470, 13], "temperature": 0.0, "avg_logprob": -0.17984736760457357, "compression_ratio": 1.4675324675324675, "no_speech_prob": 5.09328401676612e-06}, {"id": 569, "seek": 271970, "start": 2734.2599999999998, "end": 2734.98, "text": " It's not like we're...", "tokens": [467, 311, 406, 411, 321, 434, 485], "temperature": 0.0, "avg_logprob": -0.17984736760457357, "compression_ratio": 1.4675324675324675, "no_speech_prob": 5.09328401676612e-06}, {"id": 570, "seek": 271970, "start": 2737.54, "end": 2743.8599999999997, "text": " It feels like we could do this for ages before our error rates start going up.", "tokens": [467, 3417, 411, 321, 727, 360, 341, 337, 12357, 949, 527, 6713, 6846, 722, 516, 493, 13], "temperature": 0.0, "avg_logprob": -0.17984736760457357, "compression_ratio": 1.4675324675324675, "no_speech_prob": 5.09328401676612e-06}, {"id": 571, "seek": 274386, "start": 2743.86, "end": 2753.1400000000003, "text": " Interestingly, when I re-ran this, my error rate was much better, 0.418.", "tokens": [30564, 11, 562, 286, 319, 12, 4257, 341, 11, 452, 6713, 3314, 390, 709, 1101, 11, 1958, 13, 19, 6494, 13], "temperature": 0.0, "avg_logprob": -0.2577869606018066, "compression_ratio": 0.9473684210526315, "no_speech_prob": 6.0486336224130355e-06}, {"id": 572, "seek": 275314, "start": 2753.14, "end": 2774.2599999999998, "text": " You've got a good memory to remember these old papers, Zach.", "tokens": [509, 600, 658, 257, 665, 4675, 281, 1604, 613, 1331, 10577, 11, 21028, 13], "temperature": 0.0, "avg_logprob": -0.14290137474353498, "compression_ratio": 1.355072463768116, "no_speech_prob": 1.4059350178285968e-05}, {"id": 573, "seek": 275314, "start": 2774.2599999999998, "end": 2776.8199999999997, "text": " It's very helpful to be able to do that.", "tokens": [467, 311, 588, 4961, 281, 312, 1075, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.14290137474353498, "compression_ratio": 1.355072463768116, "no_speech_prob": 1.4059350178285968e-05}, {"id": 574, "seek": 275314, "start": 2778.58, "end": 2782.98, "text": " Usually what I wind up doing is my dad and I will email back and forth papers to each", "tokens": [11419, 437, 286, 2468, 493, 884, 307, 452, 3546, 293, 286, 486, 3796, 646, 293, 5220, 10577, 281, 1184], "temperature": 0.0, "avg_logprob": -0.14290137474353498, "compression_ratio": 1.355072463768116, "no_speech_prob": 1.4059350178285968e-05}, {"id": 575, "seek": 278298, "start": 2782.98, "end": 2785.94, "text": " other so I can just go through my sent look at archive.", "tokens": [661, 370, 286, 393, 445, 352, 807, 452, 2279, 574, 412, 23507, 13], "temperature": 0.0, "avg_logprob": -0.1304981909065603, "compression_ratio": 1.619047619047619, "no_speech_prob": 2.078111174341757e-05}, {"id": 576, "seek": 278298, "start": 2785.94, "end": 2791.54, "text": " And usually if I don't remember the name of it, I remember the subject of it in some degree.", "tokens": [400, 2673, 498, 286, 500, 380, 1604, 264, 1315, 295, 309, 11, 286, 1604, 264, 3983, 295, 309, 294, 512, 4314, 13], "temperature": 0.0, "avg_logprob": -0.1304981909065603, "compression_ratio": 1.619047619047619, "no_speech_prob": 2.078111174341757e-05}, {"id": 577, "seek": 278298, "start": 2791.54, "end": 2793.14, "text": " Yeah, I can just go through it all.", "tokens": [865, 11, 286, 393, 445, 352, 807, 309, 439, 13], "temperature": 0.0, "avg_logprob": -0.1304981909065603, "compression_ratio": 1.619047619047619, "no_speech_prob": 2.078111174341757e-05}, {"id": 578, "seek": 278298, "start": 2793.14, "end": 2799.54, "text": " I mean, it's a very, very good idea to use a paper manager of some sort to save papers,", "tokens": [286, 914, 11, 309, 311, 257, 588, 11, 588, 665, 1558, 281, 764, 257, 3035, 6598, 295, 512, 1333, 281, 3155, 10577, 11], "temperature": 0.0, "avg_logprob": -0.1304981909065603, "compression_ratio": 1.619047619047619, "no_speech_prob": 2.078111174341757e-05}, {"id": 579, "seek": 278298, "start": 2799.54, "end": 2802.7400000000002, "text": " you know, whether it be Mendeley or the Noto or", "tokens": [291, 458, 11, 1968, 309, 312, 376, 5445, 3420, 420, 264, 1726, 78, 420], "temperature": 0.0, "avg_logprob": -0.1304981909065603, "compression_ratio": 1.619047619047619, "no_speech_prob": 2.078111174341757e-05}, {"id": 580, "seek": 280274, "start": 2802.74, "end": 2812.02, "text": " Archive Sanity or whatever or Bookmarks or something.", "tokens": [10984, 488, 5271, 507, 420, 2035, 420, 9476, 37307, 420, 746, 13], "temperature": 0.0, "avg_logprob": -0.25658577608774946, "compression_ratio": 1.4722222222222223, "no_speech_prob": 1.833884380175732e-05}, {"id": 581, "seek": 280274, "start": 2812.02, "end": 2814.02, "text": " Yeah, because otherwise these things disappear.", "tokens": [865, 11, 570, 5911, 613, 721, 11596, 13], "temperature": 0.0, "avg_logprob": -0.25658577608774946, "compression_ratio": 1.4722222222222223, "no_speech_prob": 1.833884380175732e-05}, {"id": 582, "seek": 280274, "start": 2814.02, "end": 2822.02, "text": " Personally, I just tend to like tweet or favorite tweets about papers I'm interested in.", "tokens": [21079, 11, 286, 445, 3928, 281, 411, 15258, 420, 2954, 25671, 466, 10577, 286, 478, 3102, 294, 13], "temperature": 0.0, "avg_logprob": -0.25658577608774946, "compression_ratio": 1.4722222222222223, "no_speech_prob": 1.833884380175732e-05}, {"id": 583, "seek": 280274, "start": 2822.02, "end": 2824.8999999999996, "text": " And then I've set up pinboard.in.", "tokens": [400, 550, 286, 600, 992, 493, 5447, 3787, 13, 259, 13], "temperature": 0.0, "avg_logprob": -0.25658577608774946, "compression_ratio": 1.4722222222222223, "no_speech_prob": 1.833884380175732e-05}, {"id": 584, "seek": 280274, "start": 2824.8999999999996, "end": 2830.5, "text": " I don't know if you guys have seen that, but it's a really nice little thing, which basically", "tokens": [286, 500, 380, 458, 498, 291, 1074, 362, 1612, 300, 11, 457, 309, 311, 257, 534, 1481, 707, 551, 11, 597, 1936], "temperature": 0.0, "avg_logprob": -0.25658577608774946, "compression_ratio": 1.4722222222222223, "no_speech_prob": 1.833884380175732e-05}, {"id": 585, "seek": 283050, "start": 2830.5, "end": 2838.1, "text": " anytime you're on a website, you can click a button and the extension and it adds it", "tokens": [13038, 291, 434, 322, 257, 3144, 11, 291, 393, 2052, 257, 2960, 293, 264, 10320, 293, 309, 10860, 309], "temperature": 0.0, "avg_logprob": -0.27177108685994883, "compression_ratio": 1.6441441441441442, "no_speech_prob": 1.4508268577628769e-05}, {"id": 586, "seek": 283050, "start": 2838.1, "end": 2844.5, "text": " to pinboard, but it also automatically adds all of your tweets and favorites.", "tokens": [281, 5447, 3787, 11, 457, 309, 611, 6772, 10860, 439, 295, 428, 25671, 293, 16907, 13], "temperature": 0.0, "avg_logprob": -0.27177108685994883, "compression_ratio": 1.6441441441441442, "no_speech_prob": 1.4508268577628769e-05}, {"id": 587, "seek": 283050, "start": 2844.5, "end": 2852.5, "text": " And it's got a full text search of the thing that the URLs link to, which is very helpful.", "tokens": [400, 309, 311, 658, 257, 1577, 2487, 3164, 295, 264, 551, 300, 264, 43267, 2113, 281, 11, 597, 307, 588, 4961, 13], "temperature": 0.0, "avg_logprob": -0.27177108685994883, "compression_ratio": 1.6441441441441442, "no_speech_prob": 1.4508268577628769e-05}, {"id": 588, "seek": 283050, "start": 2853.46, "end": 2855.46, "text": " See, you favorited something that just says, oh, shit.", "tokens": [3008, 11, 291, 2294, 1226, 746, 300, 445, 1619, 11, 1954, 11, 4611, 13], "temperature": 0.0, "avg_logprob": -0.27177108685994883, "compression_ratio": 1.6441441441441442, "no_speech_prob": 1.4508268577628769e-05}, {"id": 589, "seek": 283050, "start": 2856.1, "end": 2858.1, "text": " No, I actually wrote something that just said, oh, shit.", "tokens": [883, 11, 286, 767, 4114, 746, 300, 445, 848, 11, 1954, 11, 4611, 13], "temperature": 0.0, "avg_logprob": -0.27177108685994883, "compression_ratio": 1.6441441441441442, "no_speech_prob": 1.4508268577628769e-05}, {"id": 590, "seek": 285810, "start": 2858.1, "end": 2862.1, "text": " That was me writing, oh, shit.", "tokens": [663, 390, 385, 3579, 11, 1954, 11, 4611, 13], "temperature": 0.0, "avg_logprob": -0.24444763834883526, "compression_ratio": 1.5172413793103448, "no_speech_prob": 7.03019613865763e-05}, {"id": 591, "seek": 285810, "start": 2862.1, "end": 2868.42, "text": " It was this, I mean, totally off topic, but this absolutely disaster.", "tokens": [467, 390, 341, 11, 286, 914, 11, 3879, 766, 4829, 11, 457, 341, 3122, 11293, 13], "temperature": 0.0, "avg_logprob": -0.24444763834883526, "compression_ratio": 1.5172413793103448, "no_speech_prob": 7.03019613865763e-05}, {"id": 592, "seek": 285810, "start": 2868.42, "end": 2876.18, "text": " I hope it's wrong, but this absolutely disastrous sounding paper that came out yesterday,", "tokens": [286, 1454, 309, 311, 2085, 11, 457, 341, 3122, 44502, 24931, 3035, 300, 1361, 484, 5186, 11], "temperature": 0.0, "avg_logprob": -0.24444763834883526, "compression_ratio": 1.5172413793103448, "no_speech_prob": 7.03019613865763e-05}, {"id": 593, "seek": 285810, "start": 2876.8199999999997, "end": 2879.62, "text": " that basically, where was this key thing?", "tokens": [300, 1936, 11, 689, 390, 341, 2141, 551, 30], "temperature": 0.0, "avg_logprob": -0.24444763834883526, "compression_ratio": 1.5172413793103448, "no_speech_prob": 7.03019613865763e-05}, {"id": 594, "seek": 285810, "start": 2882.66, "end": 2887.06, "text": " People who've had one COVID infection have a list of one sick rely of 8.4%,", "tokens": [3432, 567, 600, 632, 472, 4566, 11764, 362, 257, 1329, 295, 472, 4998, 10687, 295, 1649, 13, 19, 8923], "temperature": 0.0, "avg_logprob": -0.24444763834883526, "compression_ratio": 1.5172413793103448, "no_speech_prob": 7.03019613865763e-05}, {"id": 595, "seek": 288706, "start": 2887.06, "end": 2890.02, "text": " two infections 23%, three infections 36%.", "tokens": [732, 19478, 6673, 8923, 1045, 19478, 8652, 6856], "temperature": 0.0, "avg_logprob": -0.1920015078324538, "compression_ratio": 1.5665236051502145, "no_speech_prob": 1.2218524716445245e-05}, {"id": 596, "seek": 288706, "start": 2890.74, "end": 2895.22, "text": " It's like my worst nightmare is like the more people get infected with COVID,", "tokens": [467, 311, 411, 452, 5855, 18724, 307, 411, 264, 544, 561, 483, 15414, 365, 4566, 11], "temperature": 0.0, "avg_logprob": -0.1920015078324538, "compression_ratio": 1.5665236051502145, "no_speech_prob": 1.2218524716445245e-05}, {"id": 597, "seek": 288706, "start": 2895.22, "end": 2901.22, "text": " the more likely it is that they'll get long-term symptoms, which is horrifying.", "tokens": [264, 544, 3700, 309, 307, 300, 436, 603, 483, 938, 12, 7039, 8332, 11, 597, 307, 40227, 13], "temperature": 0.0, "avg_logprob": -0.1920015078324538, "compression_ratio": 1.5665236051502145, "no_speech_prob": 1.2218524716445245e-05}, {"id": 598, "seek": 288706, "start": 2901.22, "end": 2902.82, "text": " That was my most shit moment.", "tokens": [663, 390, 452, 881, 4611, 1623, 13], "temperature": 0.0, "avg_logprob": -0.1920015078324538, "compression_ratio": 1.5665236051502145, "no_speech_prob": 1.2218524716445245e-05}, {"id": 599, "seek": 288706, "start": 2902.82, "end": 2904.18, "text": " It was very horrifying.", "tokens": [467, 390, 588, 40227, 13], "temperature": 0.0, "avg_logprob": -0.1920015078324538, "compression_ratio": 1.5665236051502145, "no_speech_prob": 1.2218524716445245e-05}, {"id": 600, "seek": 288706, "start": 2904.18, "end": 2905.38, "text": " It's really awful.", "tokens": [467, 311, 534, 11232, 13], "temperature": 0.0, "avg_logprob": -0.1920015078324538, "compression_ratio": 1.5665236051502145, "no_speech_prob": 1.2218524716445245e-05}, {"id": 601, "seek": 288706, "start": 2905.94, "end": 2906.44, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.1920015078324538, "compression_ratio": 1.5665236051502145, "no_speech_prob": 1.2218524716445245e-05}, {"id": 602, "seek": 288706, "start": 2906.82, "end": 2909.22, "text": " So it keeps going down, right?", "tokens": [407, 309, 5965, 516, 760, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1920015078324538, "compression_ratio": 1.5665236051502145, "no_speech_prob": 1.2218524716445245e-05}, {"id": 603, "seek": 288706, "start": 2909.22, "end": 2909.7799999999997, "text": " Which is cool.", "tokens": [3013, 307, 1627, 13], "temperature": 0.0, "avg_logprob": -0.1920015078324538, "compression_ratio": 1.5665236051502145, "no_speech_prob": 1.2218524716445245e-05}, {"id": 604, "seek": 290978, "start": 2909.78, "end": 2916.1800000000003, "text": " Let's keep bringing it along, I suppose.", "tokens": [961, 311, 1066, 5062, 309, 2051, 11, 286, 7297, 13], "temperature": 0.0, "avg_logprob": -0.2884777972572728, "compression_ratio": 1.4041450777202074, "no_speech_prob": 1.280489595956169e-05}, {"id": 605, "seek": 290978, "start": 2916.1800000000003, "end": 2920.1800000000003, "text": " I guess, you know, what we could do is just grab this whole damn thing here.", "tokens": [286, 2041, 11, 291, 458, 11, 437, 321, 727, 360, 307, 445, 4444, 341, 1379, 8151, 551, 510, 13], "temperature": 0.0, "avg_logprob": -0.2884777972572728, "compression_ratio": 1.4041450777202074, "no_speech_prob": 1.280489595956169e-05}, {"id": 606, "seek": 290978, "start": 2922.7400000000002, "end": 2925.2200000000003, "text": " We kind of have a bit of a comparison.", "tokens": [492, 733, 295, 362, 257, 857, 295, 257, 9660, 13], "temperature": 0.0, "avg_logprob": -0.2884777972572728, "compression_ratio": 1.4041450777202074, "no_speech_prob": 1.280489595956169e-05}, {"id": 607, "seek": 290978, "start": 2925.2200000000003, "end": 2931.2200000000003, "text": " So we're basically going to run exactly the same thing we did earlier.", "tokens": [407, 321, 434, 1936, 516, 281, 1190, 2293, 264, 912, 551, 321, 630, 3071, 13], "temperature": 0.0, "avg_logprob": -0.2884777972572728, "compression_ratio": 1.4041450777202074, "no_speech_prob": 1.280489595956169e-05}, {"id": 608, "seek": 293122, "start": 2931.22, "end": 2938.8999999999996, "text": " About this time with some pre-sizing first.", "tokens": [50364, 7769, 341, 565, 365, 512, 659, 12, 82, 3319, 700, 13, 50748], "temperature": 0.0, "avg_logprob": -0.4580944946834019, "compression_ratio": 0.8775510204081632, "no_speech_prob": 4.681636346504092e-05}, {"id": 609, "seek": 296122, "start": 2961.54, "end": 2963.14, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.18739734870800073, "compression_ratio": 1.5670731707317074, "no_speech_prob": 0.005728797521442175}, {"id": 610, "seek": 296122, "start": 2967.8599999999997, "end": 2969.4599999999996, "text": " So that'll be an interesting experiment.", "tokens": [407, 300, 603, 312, 364, 1880, 5120, 13], "temperature": 0.0, "avg_logprob": -0.18739734870800073, "compression_ratio": 1.5670731707317074, "no_speech_prob": 0.005728797521442175}, {"id": 611, "seek": 296122, "start": 2971.3799999999997, "end": 2976.66, "text": " So while that's running, you know, this is where I hit the old duplicate button.", "tokens": [407, 1339, 300, 311, 2614, 11, 291, 458, 11, 341, 307, 689, 286, 2045, 264, 1331, 23976, 2960, 13], "temperature": 0.0, "avg_logprob": -0.18739734870800073, "compression_ratio": 1.5670731707317074, "no_speech_prob": 0.005728797521442175}, {"id": 612, "seek": 296122, "start": 2982.1, "end": 2987.06, "text": " And this is why it's nice if you can to have the second card,", "tokens": [400, 341, 307, 983, 309, 311, 1481, 498, 291, 393, 281, 362, 264, 1150, 2920, 11], "temperature": 0.0, "avg_logprob": -0.18739734870800073, "compression_ratio": 1.5670731707317074, "no_speech_prob": 0.005728797521442175}, {"id": 613, "seek": 298706, "start": 2987.06, "end": 2991.46, "text": " because while something's running, you can try something else.", "tokens": [570, 1339, 746, 311, 2614, 11, 291, 393, 853, 746, 1646, 13], "temperature": 0.0, "avg_logprob": -0.4115731007344014, "compression_ratio": 1.161904761904762, "no_speech_prob": 4.067774352733977e-05}, {"id": 614, "seek": 298706, "start": 2994.5, "end": 2998.1, "text": " CUDA, visible devices.", "tokens": [29777, 7509, 11, 8974, 5759, 13], "temperature": 0.0, "avg_logprob": -0.4115731007344014, "compression_ratio": 1.161904761904762, "no_speech_prob": 4.067774352733977e-05}, {"id": 615, "seek": 298706, "start": 3002.66, "end": 3003.22, "text": " There we go.", "tokens": [821, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.4115731007344014, "compression_ratio": 1.161904761904762, "no_speech_prob": 4.067774352733977e-05}, {"id": 616, "seek": 300322, "start": 3003.22, "end": 3017.62, "text": " So we can keep working.", "tokens": [407, 321, 393, 1066, 1364, 13], "temperature": 0.0, "avg_logprob": -0.4390267225412222, "compression_ratio": 0.890625, "no_speech_prob": 1.384320785291493e-05}, {"id": 617, "seek": 300322, "start": 3026.18, "end": 3026.8199999999997, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.4390267225412222, "compression_ratio": 0.890625, "no_speech_prob": 1.384320785291493e-05}, {"id": 618, "seek": 300322, "start": 3026.8199999999997, "end": 3030.2599999999998, "text": " So, um, way to data loader.", "tokens": [407, 11, 1105, 11, 636, 281, 1412, 3677, 260, 13], "temperature": 0.0, "avg_logprob": -0.4390267225412222, "compression_ratio": 0.890625, "no_speech_prob": 1.384320785291493e-05}, {"id": 619, "seek": 303026, "start": 3030.26, "end": 3043.46, "text": " So this is something I added to fast AI a while ago and haven't used much myself since.", "tokens": [407, 341, 307, 746, 286, 3869, 281, 2370, 7318, 257, 1339, 2057, 293, 2378, 380, 1143, 709, 2059, 1670, 13], "temperature": 0.0, "avg_logprob": -0.1610694509564024, "compression_ratio": 1.43125, "no_speech_prob": 5.8625983001547866e-06}, {"id": 620, "seek": 303026, "start": 3046.5800000000004, "end": 3049.5400000000004, "text": " But if I just search for weighted, here it is.", "tokens": [583, 498, 286, 445, 3164, 337, 32807, 11, 510, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.1610694509564024, "compression_ratio": 1.43125, "no_speech_prob": 5.8625983001547866e-06}, {"id": 621, "seek": 303026, "start": 3052.0200000000004, "end": 3052.5, "text": " Here it is.", "tokens": [1692, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.1610694509564024, "compression_ratio": 1.43125, "no_speech_prob": 5.8625983001547866e-06}, {"id": 622, "seek": 303026, "start": 3054.6600000000003, "end": 3060.0200000000004, "text": " So you can see in the docs, it shows you exactly how to use weighted data loaders.", "tokens": [407, 291, 393, 536, 294, 264, 45623, 11, 309, 3110, 291, 2293, 577, 281, 764, 32807, 1412, 3677, 433, 13], "temperature": 0.0, "avg_logprob": -0.1610694509564024, "compression_ratio": 1.43125, "no_speech_prob": 5.8625983001547866e-06}, {"id": 623, "seek": 306002, "start": 3060.02, "end": 3066.74, "text": " And so, um, we pass in a batch size.", "tokens": [400, 370, 11, 1105, 11, 321, 1320, 294, 257, 15245, 2744, 13], "temperature": 0.0, "avg_logprob": -0.18021976341635493, "compression_ratio": 1.7692307692307692, "no_speech_prob": 9.66552397585474e-06}, {"id": 624, "seek": 306002, "start": 3066.74, "end": 3069.94, "text": " We pass in some weights as this is the weights.", "tokens": [492, 1320, 294, 512, 17443, 382, 341, 307, 264, 17443, 13], "temperature": 0.0, "avg_logprob": -0.18021976341635493, "compression_ratio": 1.7692307692307692, "no_speech_prob": 9.66552397585474e-06}, {"id": 625, "seek": 306002, "start": 3069.94, "end": 3071.78, "text": " There's going to be one, two, three, four, five, six, seven, eight,", "tokens": [821, 311, 516, 281, 312, 472, 11, 732, 11, 1045, 11, 1451, 11, 1732, 11, 2309, 11, 3407, 11, 3180, 11], "temperature": 0.0, "avg_logprob": -0.18021976341635493, "compression_ratio": 1.7692307692307692, "no_speech_prob": 9.66552397585474e-06}, {"id": 626, "seek": 306002, "start": 3071.78, "end": 3073.54, "text": " or actually zero, one, two, three, four, five, six, seven.", "tokens": [420, 767, 4018, 11, 472, 11, 732, 11, 1045, 11, 1451, 11, 1732, 11, 2309, 11, 3407, 13], "temperature": 0.0, "avg_logprob": -0.18021976341635493, "compression_ratio": 1.7692307692307692, "no_speech_prob": 9.66552397585474e-06}, {"id": 627, "seek": 306002, "start": 3075.54, "end": 3077.54, "text": " And then some item transforms.", "tokens": [400, 550, 512, 3174, 35592, 13], "temperature": 0.0, "avg_logprob": -0.18021976341635493, "compression_ratio": 1.7692307692307692, "no_speech_prob": 9.66552397585474e-06}, {"id": 628, "seek": 306002, "start": 3079.7, "end": 3082.1, "text": " So like, these are kind of really, really interesting in the docs.", "tokens": [407, 411, 11, 613, 366, 733, 295, 534, 11, 534, 1880, 294, 264, 45623, 13], "temperature": 0.0, "avg_logprob": -0.18021976341635493, "compression_ratio": 1.7692307692307692, "no_speech_prob": 9.66552397585474e-06}, {"id": 629, "seek": 306002, "start": 3083.38, "end": 3088.18, "text": " In some ways, it's extremely advanced and it's other ways, it's extremely simple,", "tokens": [682, 512, 2098, 11, 309, 311, 4664, 7339, 293, 309, 311, 661, 2098, 11, 309, 311, 4664, 2199, 11], "temperature": 0.0, "avg_logprob": -0.18021976341635493, "compression_ratio": 1.7692307692307692, "no_speech_prob": 9.66552397585474e-06}, {"id": 630, "seek": 308818, "start": 3088.18, "end": 3093.2999999999997, "text": " which is to say, if you look at this example in the docs, everything is totally manual.", "tokens": [597, 307, 281, 584, 11, 498, 291, 574, 412, 341, 1365, 294, 264, 45623, 11, 1203, 307, 3879, 9688, 13], "temperature": 0.0, "avg_logprob": -0.15047851852748706, "compression_ratio": 1.5933014354066986, "no_speech_prob": 8.93944343260955e-06}, {"id": 631, "seek": 308818, "start": 3093.2999999999997, "end": 3094.02, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.15047851852748706, "compression_ratio": 1.5933014354066986, "no_speech_prob": 8.93944343260955e-06}, {"id": 632, "seek": 308818, "start": 3094.02, "end": 3098.3399999999997, "text": " So our labels are some random integers, kind of them.", "tokens": [407, 527, 16949, 366, 512, 4974, 41674, 11, 733, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.15047851852748706, "compression_ratio": 1.5933014354066986, "no_speech_prob": 8.93944343260955e-06}, {"id": 633, "seek": 308818, "start": 3101.8599999999997, "end": 3103.62, "text": " And I've even added a comment here, right?", "tokens": [400, 286, 600, 754, 3869, 257, 2871, 510, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.15047851852748706, "compression_ratio": 1.5933014354066986, "no_speech_prob": 8.93944343260955e-06}, {"id": 634, "seek": 308818, "start": 3103.62, "end": 3104.98, "text": " Eight is going to be in the training set.", "tokens": [17708, 307, 516, 281, 312, 294, 264, 3097, 992, 13], "temperature": 0.0, "avg_logprob": -0.15047851852748706, "compression_ratio": 1.5933014354066986, "no_speech_prob": 8.93944343260955e-06}, {"id": 635, "seek": 308818, "start": 3104.98, "end": 3106.5, "text": " Two are going to be in the validation set.", "tokens": [4453, 366, 516, 281, 312, 294, 264, 24071, 992, 13], "temperature": 0.0, "avg_logprob": -0.15047851852748706, "compression_ratio": 1.5933014354066986, "no_speech_prob": 8.93944343260955e-06}, {"id": 636, "seek": 308818, "start": 3111.2999999999997, "end": 3115.22, "text": " So our data block is going to contain one category block", "tokens": [407, 527, 1412, 3461, 307, 516, 281, 5304, 472, 7719, 3461], "temperature": 0.0, "avg_logprob": -0.15047851852748706, "compression_ratio": 1.5933014354066986, "no_speech_prob": 8.93944343260955e-06}, {"id": 637, "seek": 311522, "start": 3115.22, "end": 3117.14, "text": " because we've just got the one thing, right?", "tokens": [570, 321, 600, 445, 658, 264, 472, 551, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.4479406992594401, "compression_ratio": 1.8601694915254237, "no_speech_prob": 2.212457729910966e-05}, {"id": 638, "seek": 311522, "start": 3117.14, "end": 3124.02, "text": " And rather than doing get X and get Y, you can also just say getters", "tokens": [400, 2831, 813, 884, 483, 1783, 293, 483, 398, 11, 291, 393, 611, 445, 584, 483, 1559], "temperature": 0.0, "avg_logprob": -0.4479406992594401, "compression_ratio": 1.8601694915254237, "no_speech_prob": 2.212457729910966e-05}, {"id": 639, "seek": 311522, "start": 3124.02, "end": 3127.54, "text": " because get X and get Y basically become getters, which is a list of transformations to do.", "tokens": [570, 483, 1783, 293, 483, 398, 1936, 1813, 483, 1559, 11, 597, 307, 257, 1329, 295, 34852, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.4479406992594401, "compression_ratio": 1.8601694915254237, "no_speech_prob": 2.212457729910966e-05}, {"id": 640, "seek": 311522, "start": 3128.3399999999997, "end": 3132.5, "text": " And so this is going to be a single getter or a single get X, if you like,", "tokens": [400, 370, 341, 307, 516, 281, 312, 257, 2167, 483, 391, 420, 257, 2167, 483, 1783, 11, 498, 291, 411, 11], "temperature": 0.0, "avg_logprob": -0.4479406992594401, "compression_ratio": 1.8601694915254237, "no_speech_prob": 2.212457729910966e-05}, {"id": 641, "seek": 311522, "start": 3133.2999999999997, "end": 3137.14, "text": " which is going to return the I label and a splitter,", "tokens": [597, 307, 516, 281, 2736, 264, 286, 7645, 293, 257, 4732, 3904, 11], "temperature": 0.0, "avg_logprob": -0.4479406992594401, "compression_ratio": 1.8601694915254237, "no_speech_prob": 2.212457729910966e-05}, {"id": 642, "seek": 311522, "start": 3137.14, "end": 3139.06, "text": " which is going to decide whether something's valid or not.", "tokens": [597, 307, 516, 281, 4536, 1968, 746, 311, 7363, 420, 406, 13], "temperature": 0.0, "avg_logprob": -0.4479406992594401, "compression_ratio": 1.8601694915254237, "no_speech_prob": 2.212457729910966e-05}, {"id": 643, "seek": 311522, "start": 3139.06, "end": 3140.5, "text": " And then the other category is going to be the", "tokens": [400, 550, 264, 661, 7719, 307, 516, 281, 312, 264], "temperature": 0.0, "avg_logprob": -0.4479406992594401, "compression_ratio": 1.8601694915254237, "no_speech_prob": 2.212457729910966e-05}, {"id": 644, "seek": 314050, "start": 3140.5, "end": 3145.86, "text": " label and a splitter, which is going to decide whether something's valid or not based on this", "tokens": [7645, 293, 257, 4732, 3904, 11, 597, 307, 516, 281, 4536, 1968, 746, 311, 7363, 420, 406, 2361, 322, 341], "temperature": 0.0, "avg_logprob": -0.12126811345418294, "compression_ratio": 1.794979079497908, "no_speech_prob": 1.9832485122606158e-05}, {"id": 645, "seek": 314050, "start": 3145.86, "end": 3150.74, "text": " function. So you can see this whole thing is like totally manual, you know,", "tokens": [2445, 13, 407, 291, 393, 536, 341, 1379, 551, 307, 411, 3879, 9688, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.12126811345418294, "compression_ratio": 1.794979079497908, "no_speech_prob": 1.9832485122606158e-05}, {"id": 646, "seek": 314050, "start": 3151.54, "end": 3155.62, "text": " so we can create our data set by passing in a list of the numbers from not nine", "tokens": [370, 321, 393, 1884, 527, 1412, 992, 538, 8437, 294, 257, 1329, 295, 264, 3547, 490, 406, 4949], "temperature": 0.0, "avg_logprob": -0.12126811345418294, "compression_ratio": 1.794979079497908, "no_speech_prob": 1.9832485122606158e-05}, {"id": 647, "seek": 314050, "start": 3157.46, "end": 3160.58, "text": " and a single item transform that's going to convert that to a tensor.", "tokens": [293, 257, 2167, 3174, 4088, 300, 311, 516, 281, 7620, 300, 281, 257, 40863, 13], "temperature": 0.0, "avg_logprob": -0.12126811345418294, "compression_ratio": 1.794979079497908, "no_speech_prob": 1.9832485122606158e-05}, {"id": 648, "seek": 314050, "start": 3162.18, "end": 3164.5, "text": " And then our weights will be the numbers from not to seven.", "tokens": [400, 550, 527, 17443, 486, 312, 264, 3547, 490, 406, 281, 3407, 13], "temperature": 0.0, "avg_logprob": -0.12126811345418294, "compression_ratio": 1.794979079497908, "no_speech_prob": 1.9832485122606158e-05}, {"id": 649, "seek": 316450, "start": 3164.5, "end": 3173.86, "text": " And so then we can take our data set or data sets and turn them into data loaders using those weights.", "tokens": [400, 370, 550, 321, 393, 747, 527, 1412, 992, 420, 1412, 6352, 293, 1261, 552, 666, 1412, 3677, 433, 1228, 729, 17443, 13], "temperature": 0.0, "avg_logprob": -0.14227649642200005, "compression_ratio": 1.5677083333333333, "no_speech_prob": 6.240697075554635e-06}, {"id": 650, "seek": 316450, "start": 3177.06, "end": 3183.54, "text": " So with a batch size of one, if we say show batch, we get back a single number, okay?", "tokens": [407, 365, 257, 15245, 2744, 295, 472, 11, 498, 321, 584, 855, 15245, 11, 321, 483, 646, 257, 2167, 1230, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.14227649642200005, "compression_ratio": 1.5677083333333333, "no_speech_prob": 6.240697075554635e-06}, {"id": 651, "seek": 316450, "start": 3184.82, "end": 3187.22, "text": " And it's not doing random shuffling, so we get the number zero", "tokens": [400, 309, 311, 406, 884, 4974, 402, 1245, 1688, 11, 370, 321, 483, 264, 1230, 4018], "temperature": 0.0, "avg_logprob": -0.14227649642200005, "compression_ratio": 1.5677083333333333, "no_speech_prob": 6.240697075554635e-06}, {"id": 652, "seek": 318722, "start": 3187.22, "end": 3198.1, "text": " because that was the first thing in our data set. Let's see, what do we do next?", "tokens": [570, 300, 390, 264, 700, 551, 294, 527, 1412, 992, 13, 961, 311, 536, 11, 437, 360, 321, 360, 958, 30], "temperature": 0.0, "avg_logprob": -0.13925866157777847, "compression_ratio": 1.3194444444444444, "no_speech_prob": 4.495025223150151e-06}, {"id": 653, "seek": 318722, "start": 3198.66, "end": 3205.4599999999996, "text": " Now we've got to do n equals 160. So now we've got all of the numbers from not to 159.", "tokens": [823, 321, 600, 658, 281, 360, 297, 6915, 21243, 13, 407, 586, 321, 600, 658, 439, 295, 264, 3547, 490, 406, 281, 2119, 24, 13], "temperature": 0.0, "avg_logprob": -0.13925866157777847, "compression_ratio": 1.3194444444444444, "no_speech_prob": 4.495025223150151e-06}, {"id": 654, "seek": 318722, "start": 3211.4599999999996, "end": 3212.66, "text": " Yes, for getters, yep.", "tokens": [1079, 11, 337, 483, 1559, 11, 18633, 13], "temperature": 0.0, "avg_logprob": -0.13925866157777847, "compression_ratio": 1.3194444444444444, "no_speech_prob": 4.495025223150151e-06}, {"id": 655, "seek": 321266, "start": 3212.66, "end": 3218.18, "text": " You mentioned, so this is for x or y?", "tokens": [509, 2835, 11, 370, 341, 307, 337, 2031, 420, 288, 30], "temperature": 0.0, "avg_logprob": -0.16515179338126346, "compression_ratio": 1.6851851851851851, "no_speech_prob": 2.7534362743608654e-05}, {"id": 656, "seek": 321266, "start": 3218.98, "end": 3223.2999999999997, "text": " This is a list, it's whatever, right? So there is just one thing,", "tokens": [639, 307, 257, 1329, 11, 309, 311, 2035, 11, 558, 30, 407, 456, 307, 445, 472, 551, 11], "temperature": 0.0, "avg_logprob": -0.16515179338126346, "compression_ratio": 1.6851851851851851, "no_speech_prob": 2.7534362743608654e-05}, {"id": 657, "seek": 321266, "start": 3223.2999999999997, "end": 3225.94, "text": " I don't know if you call that x or you call it y, it's just one thing.", "tokens": [286, 500, 380, 458, 498, 291, 818, 300, 2031, 420, 291, 818, 309, 288, 11, 309, 311, 445, 472, 551, 13], "temperature": 0.0, "avg_logprob": -0.16515179338126346, "compression_ratio": 1.6851851851851851, "no_speech_prob": 2.7534362743608654e-05}, {"id": 658, "seek": 321266, "start": 3226.8199999999997, "end": 3231.62, "text": " So if you have a get x and a get y, that's the same as having a getters with a list of two things.", "tokens": [407, 498, 291, 362, 257, 483, 2031, 293, 257, 483, 288, 11, 300, 311, 264, 912, 382, 1419, 257, 483, 1559, 365, 257, 1329, 295, 732, 721, 13], "temperature": 0.0, "avg_logprob": -0.16515179338126346, "compression_ratio": 1.6851851851851851, "no_speech_prob": 2.7534362743608654e-05}, {"id": 659, "seek": 321266, "start": 3233.3799999999997, "end": 3233.7, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.16515179338126346, "compression_ratio": 1.6851851851851851, "no_speech_prob": 2.7534362743608654e-05}, {"id": 660, "seek": 321266, "start": 3235.8599999999997, "end": 3240.1, "text": " So yeah, I think I could just write getter, this has been ages since I've done this,", "tokens": [407, 1338, 11, 286, 519, 286, 727, 445, 2464, 483, 391, 11, 341, 575, 668, 12357, 1670, 286, 600, 1096, 341, 11], "temperature": 0.0, "avg_logprob": -0.16515179338126346, "compression_ratio": 1.6851851851851851, "no_speech_prob": 2.7534362743608654e-05}, {"id": 661, "seek": 324010, "start": 3240.1, "end": 3242.74, "text": " but I think I could just write get x here and put this not in a list,", "tokens": [457, 286, 519, 286, 727, 445, 2464, 483, 2031, 510, 293, 829, 341, 406, 294, 257, 1329, 11], "temperature": 0.0, "avg_logprob": -0.14180361364305633, "compression_ratio": 1.5714285714285714, "no_speech_prob": 3.1380886866827495e-06}, {"id": 662, "seek": 324010, "start": 3243.2999999999997, "end": 3244.5, "text": " would probably be the same thing.", "tokens": [576, 1391, 312, 264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.14180361364305633, "compression_ratio": 1.5714285714285714, "no_speech_prob": 3.1380886866827495e-06}, {"id": 663, "seek": 324010, "start": 3245.7, "end": 3246.2, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.14180361364305633, "compression_ratio": 1.5714285714285714, "no_speech_prob": 3.1380886866827495e-06}, {"id": 664, "seek": 324010, "start": 3247.86, "end": 3250.5, "text": " Probably handle a little bit of mystery that might be happening as well.", "tokens": [9210, 4813, 257, 707, 857, 295, 11422, 300, 1062, 312, 2737, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.14180361364305633, "compression_ratio": 1.5714285714285714, "no_speech_prob": 3.1380886866827495e-06}, {"id": 665, "seek": 324010, "start": 3251.62, "end": 3259.54, "text": " The data block has an input parameter, which is how it determines what of the getters is x versus y.", "tokens": [440, 1412, 3461, 575, 364, 4846, 13075, 11, 597, 307, 577, 309, 24799, 437, 295, 264, 483, 1559, 307, 2031, 5717, 288, 13], "temperature": 0.0, "avg_logprob": -0.14180361364305633, "compression_ratio": 1.5714285714285714, "no_speech_prob": 3.1380886866827495e-06}, {"id": 666, "seek": 324010, "start": 3259.54, "end": 3267.2999999999997, "text": " Correct, which we actually looked at last time here when we created our multi-image block.", "tokens": [12753, 11, 597, 321, 767, 2956, 412, 1036, 565, 510, 562, 321, 2942, 527, 4825, 12, 26624, 3461, 13], "temperature": 0.0, "avg_logprob": -0.14180361364305633, "compression_ratio": 1.5714285714285714, "no_speech_prob": 3.1380886866827495e-06}, {"id": 667, "seek": 326730, "start": 3267.3, "end": 3269.3, "text": " But that was before you joined, Zach.", "tokens": [583, 300, 390, 949, 291, 6869, 11, 21028, 13], "temperature": 0.0, "avg_logprob": -0.36088836193084717, "compression_ratio": 1.377245508982036, "no_speech_prob": 8.800681825960055e-06}, {"id": 668, "seek": 326730, "start": 3271.78, "end": 3273.2200000000003, "text": " But yes, useful reminder.", "tokens": [583, 2086, 11, 4420, 13548, 13], "temperature": 0.0, "avg_logprob": -0.36088836193084717, "compression_ratio": 1.377245508982036, "no_speech_prob": 8.800681825960055e-06}, {"id": 669, "seek": 326730, "start": 3273.2200000000003, "end": 3290.02, "text": " Okay, so here we see in a histogram of how often, so our, we created like a little synthetic learner", "tokens": [1033, 11, 370, 510, 321, 536, 294, 257, 49816, 295, 577, 2049, 11, 370, 527, 11, 321, 2942, 411, 257, 707, 23420, 33347], "temperature": 0.0, "avg_logprob": -0.36088836193084717, "compression_ratio": 1.377245508982036, "no_speech_prob": 8.800681825960055e-06}, {"id": 670, "seek": 326730, "start": 3290.7400000000002, "end": 3293.7000000000003, "text": " that doesn't really do anything, but we can pass callbacks to it.", "tokens": [300, 1177, 380, 534, 360, 1340, 11, 457, 321, 393, 1320, 818, 17758, 281, 309, 13], "temperature": 0.0, "avg_logprob": -0.36088836193084717, "compression_ratio": 1.377245508982036, "no_speech_prob": 8.800681825960055e-06}, {"id": 671, "seek": 329370, "start": 3293.7, "end": 3299.7, "text": " And there's a callback called collect data callback, which just collects the data that's part,", "tokens": [400, 456, 311, 257, 818, 3207, 1219, 2500, 1412, 818, 3207, 11, 597, 445, 39897, 264, 1412, 300, 311, 644, 11], "temperature": 0.0, "avg_logprob": -0.12545257933596346, "compression_ratio": 1.6966824644549763, "no_speech_prob": 1.2804681318812072e-05}, {"id": 672, "seek": 329370, "start": 3299.7, "end": 3302.58, "text": " that is called in the learner.", "tokens": [300, 307, 1219, 294, 264, 33347, 13], "temperature": 0.0, "avg_logprob": -0.12545257933596346, "compression_ratio": 1.6966824644549763, "no_speech_prob": 1.2804681318812072e-05}, {"id": 673, "seek": 329370, "start": 3302.58, "end": 3307.7799999999997, "text": " And so this is how we can then find out what data was passed to the learner,", "tokens": [400, 370, 341, 307, 577, 321, 393, 550, 915, 484, 437, 1412, 390, 4678, 281, 264, 33347, 11], "temperature": 0.0, "avg_logprob": -0.12545257933596346, "compression_ratio": 1.6966824644549763, "no_speech_prob": 1.2804681318812072e-05}, {"id": 674, "seek": 329370, "start": 3307.7799999999997, "end": 3314.8199999999997, "text": " get a histogram of it, and we can see that, yep, the number 160 was received a lot more often", "tokens": [483, 257, 49816, 295, 309, 11, 293, 321, 393, 536, 300, 11, 18633, 11, 264, 1230, 21243, 390, 4613, 257, 688, 544, 2049], "temperature": 0.0, "avg_logprob": -0.12545257933596346, "compression_ratio": 1.6966824644549763, "no_speech_prob": 1.2804681318812072e-05}, {"id": 675, "seek": 329370, "start": 3315.54, "end": 3318.98, "text": " when we trained this learner, which is what you would expect.", "tokens": [562, 321, 8895, 341, 33347, 11, 597, 307, 437, 291, 576, 2066, 13], "temperature": 0.0, "avg_logprob": -0.12545257933596346, "compression_ratio": 1.6966824644549763, "no_speech_prob": 1.2804681318812072e-05}, {"id": 676, "seek": 331898, "start": 3318.98, "end": 3325.54, "text": " This is the source of the weighted data loader class here.", "tokens": [639, 307, 264, 4009, 295, 264, 32807, 1412, 3677, 260, 1508, 510, 13], "temperature": 0.0, "avg_logprob": -0.23644149871099562, "compression_ratio": 1.755813953488372, "no_speech_prob": 1.4144508213576046e-06}, {"id": 677, "seek": 331898, "start": 3326.58, "end": 3333.62, "text": " And as you can see, other than the boilerplate, it's one, two, three, four, five lines of code.", "tokens": [400, 382, 291, 393, 536, 11, 661, 813, 264, 39228, 37008, 11, 309, 311, 472, 11, 732, 11, 1045, 11, 1451, 11, 1732, 3876, 295, 3089, 13], "temperature": 0.0, "avg_logprob": -0.23644149871099562, "compression_ratio": 1.755813953488372, "no_speech_prob": 1.4144508213576046e-06}, {"id": 678, "seek": 331898, "start": 3334.58, "end": 3338.42, "text": " And then the weighted data loader's method is one, two lines of code.", "tokens": [400, 550, 264, 32807, 1412, 3677, 260, 311, 3170, 307, 472, 11, 732, 3876, 295, 3089, 13], "temperature": 0.0, "avg_logprob": -0.23644149871099562, "compression_ratio": 1.755813953488372, "no_speech_prob": 1.4144508213576046e-06}, {"id": 679, "seek": 331898, "start": 3339.46, "end": 3343.62, "text": " So there's actually a lot more lines of example than there is of actual code.", "tokens": [407, 456, 311, 767, 257, 688, 544, 3876, 295, 1365, 813, 456, 307, 295, 3539, 3089, 13], "temperature": 0.0, "avg_logprob": -0.23644149871099562, "compression_ratio": 1.755813953488372, "no_speech_prob": 1.4144508213576046e-06}, {"id": 680, "seek": 334362, "start": 3343.62, "end": 3350.74, "text": " So often it's easier just to read the source code because, you know, thanks to the very layered", "tokens": [407, 2049, 309, 311, 3571, 445, 281, 1401, 264, 4009, 3089, 570, 11, 291, 458, 11, 3231, 281, 264, 588, 34666], "temperature": 0.0, "avg_logprob": -0.24659520929509943, "compression_ratio": 1.570048309178744, "no_speech_prob": 7.410967555188108e-06}, {"id": 681, "seek": 334362, "start": 3350.74, "end": 3356.18, "text": " approach to Fast.ai, we can do so much stuff with so little code.", "tokens": [3109, 281, 15968, 13, 1301, 11, 321, 393, 360, 370, 709, 1507, 365, 370, 707, 3089, 13], "temperature": 0.0, "avg_logprob": -0.24659520929509943, "compression_ratio": 1.570048309178744, "no_speech_prob": 7.410967555188108e-06}, {"id": 682, "seek": 334362, "start": 3356.98, "end": 3360.74, "text": " And so in this case, if we look through the code, we're passing in some weights.", "tokens": [400, 370, 294, 341, 1389, 11, 498, 321, 574, 807, 264, 3089, 11, 321, 434, 8437, 294, 512, 17443, 13], "temperature": 0.0, "avg_logprob": -0.24659520929509943, "compression_ratio": 1.570048309178744, "no_speech_prob": 7.410967555188108e-06}, {"id": 683, "seek": 334362, "start": 3362.2599999999998, "end": 3368.42, "text": " And basically the key thing here is that we set, if you pass in no weights at all,", "tokens": [400, 1936, 264, 2141, 551, 510, 307, 300, 321, 992, 11, 498, 291, 1320, 294, 572, 17443, 412, 439, 11], "temperature": 0.0, "avg_logprob": -0.24659520929509943, "compression_ratio": 1.570048309178744, "no_speech_prob": 7.410967555188108e-06}, {"id": 684, "seek": 336842, "start": 3368.42, "end": 3374.02, "text": " then we're just going to set it equal to the number one repeated n times.", "tokens": [550, 321, 434, 445, 516, 281, 992, 309, 2681, 281, 264, 1230, 472, 10477, 297, 1413, 13], "temperature": 0.0, "avg_logprob": -0.3156311328594501, "compression_ratio": 1.7, "no_speech_prob": 3.3210173569386825e-05}, {"id": 685, "seek": 336842, "start": 3374.02, "end": 3375.94, "text": " So everything's going to get one, a weight of one.", "tokens": [407, 1203, 311, 516, 281, 483, 472, 11, 257, 3364, 295, 472, 13], "temperature": 0.0, "avg_logprob": -0.3156311328594501, "compression_ratio": 1.7, "no_speech_prob": 3.3210173569386825e-05}, {"id": 686, "seek": 336842, "start": 3376.9, "end": 3381.7000000000003, "text": " And then we divide the weights by the sum of the weights so that the sum of the weights", "tokens": [400, 550, 321, 9845, 264, 17443, 538, 264, 2408, 295, 264, 17443, 370, 300, 264, 2408, 295, 264, 17443], "temperature": 0.0, "avg_logprob": -0.3156311328594501, "compression_ratio": 1.7, "no_speech_prob": 3.3210173569386825e-05}, {"id": 687, "seek": 336842, "start": 3381.7000000000003, "end": 3384.1, "text": " ends up summing up to one, which is what we want.", "tokens": [5314, 493, 2408, 2810, 493, 281, 472, 11, 597, 307, 437, 321, 528, 13], "temperature": 0.0, "avg_logprob": -0.3156311328594501, "compression_ratio": 1.7, "no_speech_prob": 3.3210173569386825e-05}, {"id": 688, "seek": 336842, "start": 3388.7400000000002, "end": 3389.78, "text": " And then,", "tokens": [400, 550, 11], "temperature": 0.0, "avg_logprob": -0.3156311328594501, "compression_ratio": 1.7, "no_speech_prob": 3.3210173569386825e-05}, {"id": 689, "seek": 338978, "start": 3389.78, "end": 3393.46, "text": " if you're not shuffling, then there's no weighted anything to do.", "tokens": [498, 291, 434, 406, 402, 1245, 1688, 11, 550, 456, 311, 572, 32807, 1340, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.5066767765925481, "compression_ratio": 1.4206896551724137, "no_speech_prob": 1.1658684343274217e-05}, {"id": 690, "seek": 338978, "start": 3393.46, "end": 3395.86, "text": " So we just pass back the indexes.", "tokens": [407, 321, 445, 1320, 646, 264, 8186, 279, 13], "temperature": 0.0, "avg_logprob": -0.5066767765925481, "compression_ratio": 1.4206896551724137, "no_speech_prob": 1.1658684343274217e-05}, {"id": 691, "seek": 338978, "start": 3395.86, "end": 3404.34, "text": " And if we are shuffling, we will grab a random choice of based on the weights.", "tokens": [400, 498, 321, 366, 402, 1245, 1688, 11, 321, 486, 4444, 257, 4974, 3922, 295, 2361, 322, 264, 17443, 13], "temperature": 0.0, "avg_logprob": -0.5066767765925481, "compression_ratio": 1.4206896551724137, "no_speech_prob": 1.1658684343274217e-05}, {"id": 692, "seek": 338978, "start": 3409.86, "end": 3411.86, "text": " So that's the way we do it.", "tokens": [407, 300, 311, 264, 636, 321, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.5066767765925481, "compression_ratio": 1.4206896551724137, "no_speech_prob": 1.1658684343274217e-05}, {"id": 693, "seek": 341186, "start": 3411.86, "end": 3421.6200000000003, "text": " Based on the based on the weights.", "tokens": [18785, 322, 264, 2361, 322, 264, 17443, 13], "temperature": 0.0, "avg_logprob": -0.19017770415858218, "compression_ratio": 1.5060240963855422, "no_speech_prob": 5.594220965576824e-06}, {"id": 694, "seek": 341186, "start": 3421.6200000000003, "end": 3422.1200000000003, "text": " Cool.", "tokens": [8561, 13], "temperature": 0.0, "avg_logprob": -0.19017770415858218, "compression_ratio": 1.5060240963855422, "no_speech_prob": 5.594220965576824e-06}, {"id": 695, "seek": 341186, "start": 3427.54, "end": 3430.6600000000003, "text": " All right. So there's going to be one weight per row.", "tokens": [1057, 558, 13, 407, 456, 311, 516, 281, 312, 472, 3364, 680, 5386, 13], "temperature": 0.0, "avg_logprob": -0.19017770415858218, "compression_ratio": 1.5060240963855422, "no_speech_prob": 5.594220965576824e-06}, {"id": 696, "seek": 341186, "start": 3432.58, "end": 3435.2200000000003, "text": " All right. Let's come back to that because I want to see how our thing's gone.", "tokens": [1057, 558, 13, 961, 311, 808, 646, 281, 300, 570, 286, 528, 281, 536, 577, 527, 551, 311, 2780, 13], "temperature": 0.0, "avg_logprob": -0.19017770415858218, "compression_ratio": 1.5060240963855422, "no_speech_prob": 5.594220965576824e-06}, {"id": 697, "seek": 341186, "start": 3435.2200000000003, "end": 3436.1800000000003, "text": " It looks like it's finished.", "tokens": [467, 1542, 411, 309, 311, 4335, 13], "temperature": 0.0, "avg_logprob": -0.19017770415858218, "compression_ratio": 1.5060240963855422, "no_speech_prob": 5.594220965576824e-06}, {"id": 698, "seek": 341186, "start": 3436.98, "end": 3440.42, "text": " Notice that the fav icon in Jupyter will change", "tokens": [13428, 300, 264, 33801, 6528, 294, 22125, 88, 391, 486, 1319], "temperature": 0.0, "avg_logprob": -0.19017770415858218, "compression_ratio": 1.5060240963855422, "no_speech_prob": 5.594220965576824e-06}, {"id": 699, "seek": 344042, "start": 3440.42, "end": 3441.78, "text": " depending on whether something's running or not.", "tokens": [5413, 322, 1968, 746, 311, 2614, 420, 406, 13], "temperature": 0.0, "avg_logprob": -0.2521570645845853, "compression_ratio": 1.3803680981595092, "no_speech_prob": 2.21250593313016e-05}, {"id": 700, "seek": 344042, "start": 3441.78, "end": 3443.94, "text": " So that's how you can quickly tell if something's finished.", "tokens": [407, 300, 311, 577, 291, 393, 2661, 980, 498, 746, 311, 4335, 13], "temperature": 0.0, "avg_logprob": -0.2521570645845853, "compression_ratio": 1.3803680981595092, "no_speech_prob": 2.21250593313016e-05}, {"id": 701, "seek": 344042, "start": 3447.62, "end": 3449.3, "text": " 0.216.", "tokens": [1958, 13, 4436, 21, 13], "temperature": 0.0, "avg_logprob": -0.2521570645845853, "compression_ratio": 1.3803680981595092, "no_speech_prob": 2.21250593313016e-05}, {"id": 702, "seek": 344042, "start": 3452.98, "end": 3454.82, "text": " 0.221.", "tokens": [1958, 13, 17, 4436, 13], "temperature": 0.0, "avg_logprob": -0.2521570645845853, "compression_ratio": 1.3803680981595092, "no_speech_prob": 2.21250593313016e-05}, {"id": 703, "seek": 344042, "start": 3454.82, "end": 3457.86, "text": " Okay. I mean, it's not a huge difference, but maybe it's a tiny bit better.", "tokens": [1033, 13, 286, 914, 11, 309, 311, 406, 257, 2603, 2649, 11, 457, 1310, 309, 311, 257, 5870, 857, 1101, 13], "temperature": 0.0, "avg_logprob": -0.2521570645845853, "compression_ratio": 1.3803680981595092, "no_speech_prob": 2.21250593313016e-05}, {"id": 704, "seek": 344042, "start": 3457.86, "end": 3458.58, "text": " I don't know.", "tokens": [286, 500, 380, 458, 13], "temperature": 0.0, "avg_logprob": -0.2521570645845853, "compression_ratio": 1.3803680981595092, "no_speech_prob": 2.21250593313016e-05}, {"id": 705, "seek": 344042, "start": 3458.58, "end": 3458.98, "text": " Like it's...", "tokens": [1743, 309, 311, 485], "temperature": 0.0, "avg_logprob": -0.2521570645845853, "compression_ratio": 1.3803680981595092, "no_speech_prob": 2.21250593313016e-05}, {"id": 706, "seek": 345898, "start": 3458.98, "end": 3466.26, "text": " Let's...", "tokens": [961, 311, 485], "temperature": 0.0, "avg_logprob": -0.24911177648256902, "compression_ratio": 1.396341463414634, "no_speech_prob": 6.6429865910322405e-06}, {"id": 707, "seek": 345898, "start": 3466.26, "end": 3466.76, "text": " To...", "tokens": [1407, 485], "temperature": 0.0, "avg_logprob": -0.24911177648256902, "compression_ratio": 1.396341463414634, "no_speech_prob": 6.6429865910322405e-06}, {"id": 708, "seek": 345898, "start": 3472.18, "end": 3476.26, "text": " The key thing though is this lets us use our resources better.", "tokens": [440, 2141, 551, 1673, 307, 341, 6653, 505, 764, 527, 3593, 1101, 13], "temperature": 0.0, "avg_logprob": -0.24911177648256902, "compression_ratio": 1.396341463414634, "no_speech_prob": 6.6429865910322405e-06}, {"id": 709, "seek": 345898, "start": 3476.26, "end": 3478.26, "text": " Right? So we often will end up with a better answer,", "tokens": [1779, 30, 407, 321, 2049, 486, 917, 493, 365, 257, 1101, 1867, 11], "temperature": 0.0, "avg_logprob": -0.24911177648256902, "compression_ratio": 1.396341463414634, "no_speech_prob": 6.6429865910322405e-06}, {"id": 710, "seek": 345898, "start": 3479.7, "end": 3481.86, "text": " but you can train for a lot less time.", "tokens": [457, 291, 393, 3847, 337, 257, 688, 1570, 565, 13], "temperature": 0.0, "avg_logprob": -0.24911177648256902, "compression_ratio": 1.396341463414634, "no_speech_prob": 6.6429865910322405e-06}, {"id": 711, "seek": 345898, "start": 3481.86, "end": 3485.78, "text": " In fact, you can see that the error was at 0.216 back here.", "tokens": [682, 1186, 11, 291, 393, 536, 300, 264, 6713, 390, 412, 1958, 13, 4436, 21, 646, 510, 13], "temperature": 0.0, "avg_logprob": -0.24911177648256902, "compression_ratio": 1.396341463414634, "no_speech_prob": 6.6429865910322405e-06}, {"id": 712, "seek": 348578, "start": 3485.78, "end": 3489.3, "text": " So, you know, we could probably have trained for a lot less epochs.", "tokens": [407, 11, 291, 458, 11, 321, 727, 1391, 362, 8895, 337, 257, 688, 1570, 30992, 28346, 13], "temperature": 0.0, "avg_logprob": -0.21785335776246625, "compression_ratio": 1.4874371859296482, "no_speech_prob": 2.392101305304095e-05}, {"id": 713, "seek": 348578, "start": 3490.98, "end": 3492.42, "text": " So that's progressive resizing.", "tokens": [407, 300, 311, 16131, 725, 3319, 13], "temperature": 0.0, "avg_logprob": -0.21785335776246625, "compression_ratio": 1.4874371859296482, "no_speech_prob": 2.392101305304095e-05}, {"id": 714, "seek": 348578, "start": 3498.1000000000004, "end": 3501.2200000000003, "text": " Is there a way to look at that and go,", "tokens": [1119, 456, 257, 636, 281, 574, 412, 300, 293, 352, 11], "temperature": 0.0, "avg_logprob": -0.21785335776246625, "compression_ratio": 1.4874371859296482, "no_speech_prob": 2.392101305304095e-05}, {"id": 715, "seek": 348578, "start": 3502.5, "end": 3510.98, "text": " oh, actually I'd like to take the outputs from epoch nine because it had a better...", "tokens": [1954, 11, 767, 286, 1116, 411, 281, 747, 264, 23930, 490, 30992, 339, 4949, 570, 309, 632, 257, 1101, 485], "temperature": 0.0, "avg_logprob": -0.21785335776246625, "compression_ratio": 1.4874371859296482, "no_speech_prob": 2.392101305304095e-05}, {"id": 716, "seek": 348578, "start": 3512.1800000000003, "end": 3514.9, "text": " That was the question we got earlier about that's called early stopping.", "tokens": [663, 390, 264, 1168, 321, 658, 3071, 466, 300, 311, 1219, 2440, 12767, 13], "temperature": 0.0, "avg_logprob": -0.21785335776246625, "compression_ratio": 1.4874371859296482, "no_speech_prob": 2.392101305304095e-05}, {"id": 717, "seek": 351490, "start": 3514.9, "end": 3517.62, "text": " And the answer is no, you probably wouldn't want to do early stopping.", "tokens": [400, 264, 1867, 307, 572, 11, 291, 1391, 2759, 380, 528, 281, 360, 2440, 12767, 13], "temperature": 0.0, "avg_logprob": -0.28570370557831554, "compression_ratio": 1.6888888888888889, "no_speech_prob": 3.169087358401157e-05}, {"id": 718, "seek": 351490, "start": 3519.62, "end": 3524.42, "text": " But you can't go back to a previous like epoch.", "tokens": [583, 291, 393, 380, 352, 646, 281, 257, 3894, 411, 30992, 339, 13], "temperature": 0.0, "avg_logprob": -0.28570370557831554, "compression_ratio": 1.6888888888888889, "no_speech_prob": 3.169087358401157e-05}, {"id": 719, "seek": 351490, "start": 3524.42, "end": 3525.78, "text": " There's no history.", "tokens": [821, 311, 572, 2503, 13], "temperature": 0.0, "avg_logprob": -0.28570370557831554, "compression_ratio": 1.6888888888888889, "no_speech_prob": 3.169087358401157e-05}, {"id": 720, "seek": 351490, "start": 3525.78, "end": 3526.7400000000002, "text": " You can.", "tokens": [509, 393, 13], "temperature": 0.0, "avg_logprob": -0.28570370557831554, "compression_ratio": 1.6888888888888889, "no_speech_prob": 3.169087358401157e-05}, {"id": 721, "seek": 351490, "start": 3526.7400000000002, "end": 3528.98, "text": " You have to use the early stopping callback to do that.", "tokens": [509, 362, 281, 764, 264, 2440, 12767, 818, 3207, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.28570370557831554, "compression_ratio": 1.6888888888888889, "no_speech_prob": 3.169087358401157e-05}, {"id": 722, "seek": 351490, "start": 3528.98, "end": 3530.1, "text": " All right. Cool.", "tokens": [1057, 558, 13, 8561, 13], "temperature": 0.0, "avg_logprob": -0.28570370557831554, "compression_ratio": 1.6888888888888889, "no_speech_prob": 3.169087358401157e-05}, {"id": 723, "seek": 351490, "start": 3530.1, "end": 3530.98, "text": " Okay. Yeah. Okay.", "tokens": [1033, 13, 865, 13, 1033, 13], "temperature": 0.0, "avg_logprob": -0.28570370557831554, "compression_ratio": 1.6888888888888889, "no_speech_prob": 3.169087358401157e-05}, {"id": 724, "seek": 351490, "start": 3530.98, "end": 3531.86, "text": " I'll look at that.", "tokens": [286, 603, 574, 412, 300, 13], "temperature": 0.0, "avg_logprob": -0.28570370557831554, "compression_ratio": 1.6888888888888889, "no_speech_prob": 3.169087358401157e-05}, {"id": 725, "seek": 351490, "start": 3532.42, "end": 3535.54, "text": " Or there's other things you can use.", "tokens": [1610, 456, 311, 661, 721, 291, 393, 764, 13], "temperature": 0.0, "avg_logprob": -0.28570370557831554, "compression_ratio": 1.6888888888888889, "no_speech_prob": 3.169087358401157e-05}, {"id": 726, "seek": 351490, "start": 3536.1, "end": 3539.06, "text": " As I say, I don't think you should, but you can.", "tokens": [1018, 286, 584, 11, 286, 500, 380, 519, 291, 820, 11, 457, 291, 393, 13], "temperature": 0.0, "avg_logprob": -0.28570370557831554, "compression_ratio": 1.6888888888888889, "no_speech_prob": 3.169087358401157e-05}, {"id": 727, "seek": 353906, "start": 3539.06, "end": 3544.74, "text": " If I go training callbacks, tracker.", "tokens": [759, 286, 352, 3097, 818, 17758, 11, 37516, 13], "temperature": 0.0, "avg_logprob": -0.2075659653236126, "compression_ratio": 1.6995515695067265, "no_speech_prob": 3.13808368446189e-06}, {"id": 728, "seek": 353906, "start": 3545.7799999999997, "end": 3548.98, "text": " Okay. So the other part of that is, yeah, is it counterproductive or...", "tokens": [1033, 13, 407, 264, 661, 644, 295, 300, 307, 11, 1338, 11, 307, 309, 5682, 14314, 20221, 420, 485], "temperature": 0.0, "avg_logprob": -0.2075659653236126, "compression_ratio": 1.6995515695067265, "no_speech_prob": 3.13808368446189e-06}, {"id": 729, "seek": 353906, "start": 3548.98, "end": 3552.02, "text": " Yeah, it's kind of a cheat if it works, but not if it doesn't.", "tokens": [865, 11, 309, 311, 733, 295, 257, 17470, 498, 309, 1985, 11, 457, 406, 498, 309, 1177, 380, 13], "temperature": 0.0, "avg_logprob": -0.2075659653236126, "compression_ratio": 1.6995515695067265, "no_speech_prob": 3.13808368446189e-06}, {"id": 730, "seek": 353906, "start": 3553.62, "end": 3554.82, "text": " It's probably not a good idea.", "tokens": [467, 311, 1391, 406, 257, 665, 1558, 13], "temperature": 0.0, "avg_logprob": -0.2075659653236126, "compression_ratio": 1.6995515695067265, "no_speech_prob": 3.13808368446189e-06}, {"id": 731, "seek": 353906, "start": 3554.82, "end": 3555.86, "text": " Probably will make it worse.", "tokens": [9210, 486, 652, 309, 5324, 13], "temperature": 0.0, "avg_logprob": -0.2075659653236126, "compression_ratio": 1.6995515695067265, "no_speech_prob": 3.13808368446189e-06}, {"id": 732, "seek": 353906, "start": 3555.86, "end": 3556.98, "text": " Yeah. Okay. Great.", "tokens": [865, 13, 1033, 13, 3769, 13], "temperature": 0.0, "avg_logprob": -0.2075659653236126, "compression_ratio": 1.6995515695067265, "no_speech_prob": 3.13808368446189e-06}, {"id": 733, "seek": 353906, "start": 3558.02, "end": 3560.1, "text": " So the other thing you can do is to save model callback,", "tokens": [407, 264, 661, 551, 291, 393, 360, 307, 281, 3155, 2316, 818, 3207, 11], "temperature": 0.0, "avg_logprob": -0.2075659653236126, "compression_ratio": 1.6995515695067265, "no_speech_prob": 3.13808368446189e-06}, {"id": 734, "seek": 353906, "start": 3560.82, "end": 3564.18, "text": " which saves, which is kind of like early stopping, but it doesn't stop.", "tokens": [597, 19155, 11, 597, 307, 733, 295, 411, 2440, 12767, 11, 457, 309, 1177, 380, 1590, 13], "temperature": 0.0, "avg_logprob": -0.2075659653236126, "compression_ratio": 1.6995515695067265, "no_speech_prob": 3.13808368446189e-06}, {"id": 735, "seek": 356418, "start": 3564.18, "end": 3570.5, "text": " It saves the parameters of the best model during training,", "tokens": [467, 19155, 264, 9834, 295, 264, 1151, 2316, 1830, 3097, 11], "temperature": 0.0, "avg_logprob": -0.14517284574962797, "compression_ratio": 1.5528846153846154, "no_speech_prob": 4.7849398470134474e-06}, {"id": 736, "seek": 356418, "start": 3570.5, "end": 3574.1, "text": " which is probably what you want instead of early stopping.", "tokens": [597, 307, 1391, 437, 291, 528, 2602, 295, 2440, 12767, 13], "temperature": 0.0, "avg_logprob": -0.14517284574962797, "compression_ratio": 1.5528846153846154, "no_speech_prob": 4.7849398470134474e-06}, {"id": 737, "seek": 356418, "start": 3574.1, "end": 3575.7, "text": " But I don't think you should do that either.", "tokens": [583, 286, 500, 380, 519, 291, 820, 360, 300, 2139, 13], "temperature": 0.0, "avg_logprob": -0.14517284574962797, "compression_ratio": 1.5528846153846154, "no_speech_prob": 4.7849398470134474e-06}, {"id": 738, "seek": 356418, "start": 3575.7, "end": 3577.3799999999997, "text": " For the same reason we discussed earlier.", "tokens": [1171, 264, 912, 1778, 321, 7152, 3071, 13], "temperature": 0.0, "avg_logprob": -0.14517284574962797, "compression_ratio": 1.5528846153846154, "no_speech_prob": 4.7849398470134474e-06}, {"id": 739, "seek": 356418, "start": 3580.1, "end": 3581.06, "text": " Why shouldn't you do this?", "tokens": [1545, 4659, 380, 291, 360, 341, 30], "temperature": 0.0, "avg_logprob": -0.14517284574962797, "compression_ratio": 1.5528846153846154, "no_speech_prob": 4.7849398470134474e-06}, {"id": 740, "seek": 356418, "start": 3581.06, "end": 3587.06, "text": " It seems like you could just ignore it if you didn't want it or like it might not hurt you.", "tokens": [467, 2544, 411, 291, 727, 445, 11200, 309, 498, 291, 994, 380, 528, 309, 420, 411, 309, 1062, 406, 4607, 291, 13], "temperature": 0.0, "avg_logprob": -0.14517284574962797, "compression_ratio": 1.5528846153846154, "no_speech_prob": 4.7849398470134474e-06}, {"id": 741, "seek": 358706, "start": 3587.06, "end": 3597.7799999999997, "text": " Well, so this actually automatically loads the best set of parameters at the end.", "tokens": [1042, 11, 370, 341, 767, 6772, 12668, 264, 1151, 992, 295, 9834, 412, 264, 917, 13], "temperature": 0.0, "avg_logprob": -0.1499121435757341, "compression_ratio": 1.484076433121019, "no_speech_prob": 9.222194421454333e-06}, {"id": 742, "seek": 358706, "start": 3599.86, "end": 3606.5, "text": " And you're just going to end up with this kind of like", "tokens": [400, 291, 434, 445, 516, 281, 917, 493, 365, 341, 733, 295, 411], "temperature": 0.0, "avg_logprob": -0.1499121435757341, "compression_ratio": 1.484076433121019, "no_speech_prob": 9.222194421454333e-06}, {"id": 743, "seek": 358706, "start": 3607.06, "end": 3613.38, "text": " model that just so happened to look a tiny bit better on the validation set at an earlier epoch.", "tokens": [2316, 300, 445, 370, 2011, 281, 574, 257, 5870, 857, 1101, 322, 264, 24071, 992, 412, 364, 3071, 30992, 339, 13], "temperature": 0.0, "avg_logprob": -0.1499121435757341, "compression_ratio": 1.484076433121019, "no_speech_prob": 9.222194421454333e-06}, {"id": 744, "seek": 361338, "start": 3613.38, "end": 3617.46, "text": " But at that earlier epoch, the loading rate hadn't yet stabilized", "tokens": [583, 412, 300, 3071, 30992, 339, 11, 264, 15114, 3314, 8782, 380, 1939, 48384], "temperature": 0.0, "avg_logprob": -0.27382898934279815, "compression_ratio": 1.5585585585585586, "no_speech_prob": 3.168767216266133e-05}, {"id": 745, "seek": 361338, "start": 3617.46, "end": 3619.1400000000003, "text": " and it's very unlikely it really is better.", "tokens": [293, 309, 311, 588, 17518, 309, 534, 307, 1101, 13], "temperature": 0.0, "avg_logprob": -0.27382898934279815, "compression_ratio": 1.5585585585585586, "no_speech_prob": 3.168767216266133e-05}, {"id": 746, "seek": 361338, "start": 3620.5, "end": 3623.3, "text": " So you've probably actually just picked something that's slightly worse", "tokens": [407, 291, 600, 1391, 767, 445, 6183, 746, 300, 311, 4748, 5324], "temperature": 0.0, "avg_logprob": -0.27382898934279815, "compression_ratio": 1.5585585585585586, "no_speech_prob": 3.168767216266133e-05}, {"id": 747, "seek": 361338, "start": 3624.26, "end": 3629.2200000000003, "text": " and made your process slightly more complicated for no good reason.", "tokens": [293, 1027, 428, 1399, 4748, 544, 6179, 337, 572, 665, 1778, 13], "temperature": 0.0, "avg_logprob": -0.27382898934279815, "compression_ratio": 1.5585585585585586, "no_speech_prob": 3.168767216266133e-05}, {"id": 748, "seek": 361338, "start": 3629.7000000000003, "end": 3638.34, "text": " Being better on an epoch there doesn't necessarily say anything about the final hidden test set.", "tokens": [8891, 1101, 322, 364, 30992, 339, 456, 1177, 380, 4725, 584, 1340, 466, 264, 2572, 7633, 1500, 992, 13], "temperature": 0.0, "avg_logprob": -0.27382898934279815, "compression_ratio": 1.5585585585585586, "no_speech_prob": 3.168767216266133e-05}, {"id": 749, "seek": 363834, "start": 3638.34, "end": 3645.38, "text": " Yeah, we have a strong prior belief that it will improve each epoch", "tokens": [865, 11, 321, 362, 257, 2068, 4059, 7107, 300, 309, 486, 3470, 1184, 30992, 339], "temperature": 0.0, "avg_logprob": -0.11922034298080995, "compression_ratio": 1.551219512195122, "no_speech_prob": 7.182676654338138e-06}, {"id": 750, "seek": 363834, "start": 3649.2200000000003, "end": 3650.58, "text": " unless you're overfitting.", "tokens": [5969, 291, 434, 670, 69, 2414, 13], "temperature": 0.0, "avg_logprob": -0.11922034298080995, "compression_ratio": 1.551219512195122, "no_speech_prob": 7.182676654338138e-06}, {"id": 751, "seek": 363834, "start": 3650.58, "end": 3653.7000000000003, "text": " And if you're overfitting then you shouldn't be doing early stopping.", "tokens": [400, 498, 291, 434, 670, 69, 2414, 550, 291, 4659, 380, 312, 884, 2440, 12767, 13], "temperature": 0.0, "avg_logprob": -0.11922034298080995, "compression_ratio": 1.551219512195122, "no_speech_prob": 7.182676654338138e-06}, {"id": 752, "seek": 363834, "start": 3653.7000000000003, "end": 3655.3, "text": " You should be doing more augmentation.", "tokens": [509, 820, 312, 884, 544, 14501, 19631, 13], "temperature": 0.0, "avg_logprob": -0.11922034298080995, "compression_ratio": 1.551219512195122, "no_speech_prob": 7.182676654338138e-06}, {"id": 753, "seek": 363834, "start": 3657.38, "end": 3661.1400000000003, "text": " It seems like a good opportunity for somebody to document the arguments", "tokens": [467, 2544, 411, 257, 665, 2650, 337, 2618, 281, 4166, 264, 12869], "temperature": 0.0, "avg_logprob": -0.11922034298080995, "compression_ratio": 1.551219512195122, "no_speech_prob": 7.182676654338138e-06}, {"id": 754, "seek": 363834, "start": 3661.86, "end": 3663.86, "text": " because I'm like curious what add-in does.", "tokens": [570, 286, 478, 411, 6369, 437, 909, 12, 259, 775, 13], "temperature": 0.0, "avg_logprob": -0.11922034298080995, "compression_ratio": 1.551219512195122, "no_speech_prob": 7.182676654338138e-06}, {"id": 755, "seek": 366386, "start": 3663.86, "end": 3666.9, "text": " Yes, that would be a great opportunity for somebody to document the arguments.", "tokens": [1079, 11, 300, 576, 312, 257, 869, 2650, 337, 2618, 281, 4166, 264, 12869, 13], "temperature": 0.0, "avg_logprob": -0.3309650421142578, "compression_ratio": 1.5513513513513513, "no_speech_prob": 1.4734315300302114e-05}, {"id": 756, "seek": 366386, "start": 3666.9, "end": 3676.02, "text": " And if somebody is interested in doing that, we have a really cool thing called documents", "tokens": [400, 498, 2618, 307, 3102, 294, 884, 300, 11, 321, 362, 257, 534, 1627, 551, 1219, 8512], "temperature": 0.0, "avg_logprob": -0.3309650421142578, "compression_ratio": 1.5513513513513513, "no_speech_prob": 1.4734315300302114e-05}, {"id": 757, "seek": 366386, "start": 3676.98, "end": 3682.5, "text": " which I only invented after we created Fast AI.", "tokens": [597, 286, 787, 14479, 934, 321, 2942, 15968, 7318, 13], "temperature": 0.0, "avg_logprob": -0.3309650421142578, "compression_ratio": 1.5513513513513513, "no_speech_prob": 1.4734315300302114e-05}, {"id": 758, "seek": 366386, "start": 3687.7000000000003, "end": 3690.9, "text": " Oh, this is like, that's not, I should delete this because this is the", "tokens": [876, 11, 341, 307, 411, 11, 300, 311, 406, 11, 286, 820, 12097, 341, 570, 341, 307, 264], "temperature": 0.0, "avg_logprob": -0.3309650421142578, "compression_ratio": 1.5513513513513513, "no_speech_prob": 1.4734315300302114e-05}, {"id": 759, "seek": 369090, "start": 3690.9, "end": 3694.1, "text": " old version, part of Fast Core.", "tokens": [1331, 3037, 11, 644, 295, 15968, 14798, 13], "temperature": 0.0, "avg_logprob": -0.32419806790639116, "compression_ratio": 1.760204081632653, "no_speech_prob": 1.3844978639099281e-05}, {"id": 760, "seek": 369090, "start": 3695.46, "end": 3701.14, "text": " And documents, you document each parameter by putting a comment after it", "tokens": [400, 8512, 11, 291, 4166, 1184, 13075, 538, 3372, 257, 2871, 934, 309], "temperature": 0.0, "avg_logprob": -0.32419806790639116, "compression_ratio": 1.760204081632653, "no_speech_prob": 1.3844978639099281e-05}, {"id": 761, "seek": 369090, "start": 3701.94, "end": 3704.5, "text": " and you document the return by putting a comment after it.", "tokens": [293, 291, 4166, 264, 2736, 538, 3372, 257, 2871, 934, 309, 13], "temperature": 0.0, "avg_logprob": -0.32419806790639116, "compression_ratio": 1.760204081632653, "no_speech_prob": 1.3844978639099281e-05}, {"id": 762, "seek": 369090, "start": 3704.5, "end": 3710.7400000000002, "text": " And Zach actually started a project to, after I created documents,", "tokens": [400, 21028, 767, 1409, 257, 1716, 281, 11, 934, 286, 2942, 8512, 11], "temperature": 0.0, "avg_logprob": -0.32419806790639116, "compression_ratio": 1.760204081632653, "no_speech_prob": 1.3844978639099281e-05}, {"id": 763, "seek": 369090, "start": 3710.7400000000002, "end": 3713.94, "text": " to add documents comments to everything in Fast AI,", "tokens": [281, 909, 8512, 3053, 281, 1203, 294, 15968, 7318, 11], "temperature": 0.0, "avg_logprob": -0.32419806790639116, "compression_ratio": 1.760204081632653, "no_speech_prob": 1.3844978639099281e-05}, {"id": 764, "seek": 369090, "start": 3714.7400000000002, "end": 3717.78, "text": " which of course is not finished because Fast AI is pretty big.", "tokens": [597, 295, 1164, 307, 406, 4335, 570, 15968, 7318, 307, 1238, 955, 13], "temperature": 0.0, "avg_logprob": -0.32419806790639116, "compression_ratio": 1.760204081632653, "no_speech_prob": 1.3844978639099281e-05}, {"id": 765, "seek": 371778, "start": 3717.78, "end": 3721.46, "text": " And so here's an example of something that doesn't yet have documents comments.", "tokens": [400, 370, 510, 311, 364, 1365, 295, 746, 300, 1177, 380, 1939, 362, 8512, 3053, 13], "temperature": 0.0, "avg_logprob": -0.3352932188245985, "compression_ratio": 1.7205882352941178, "no_speech_prob": 9.972327461582609e-06}, {"id": 766, "seek": 371778, "start": 3721.46, "end": 3726.42, "text": " So if somebody wants to go and add a comment to each of these things", "tokens": [407, 498, 2618, 2738, 281, 352, 293, 909, 257, 2871, 281, 1184, 295, 613, 721], "temperature": 0.0, "avg_logprob": -0.3352932188245985, "compression_ratio": 1.7205882352941178, "no_speech_prob": 9.972327461582609e-06}, {"id": 767, "seek": 371778, "start": 3727.1400000000003, "end": 3734.82, "text": " and put that into a PR, then that will end up in the documentation.", "tokens": [293, 829, 300, 666, 257, 11568, 11, 550, 300, 486, 917, 493, 294, 264, 14333, 13], "temperature": 0.0, "avg_logprob": -0.3352932188245985, "compression_ratio": 1.7205882352941178, "no_speech_prob": 9.972327461582609e-06}, {"id": 768, "seek": 371778, "start": 3740.26, "end": 3742.9, "text": " I highly recommend that anyone who wants to do that to do it.", "tokens": [286, 5405, 2748, 300, 2878, 567, 2738, 281, 360, 300, 281, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.3352932188245985, "compression_ratio": 1.7205882352941178, "no_speech_prob": 9.972327461582609e-06}, {"id": 769, "seek": 371778, "start": 3742.9, "end": 3746.5800000000004, "text": " I was just going to say Zach, something we should do is to add a comment", "tokens": [286, 390, 445, 516, 281, 584, 21028, 11, 746, 321, 820, 360, 307, 281, 909, 257, 2871], "temperature": 0.0, "avg_logprob": -0.3352932188245985, "compression_ratio": 1.7205882352941178, "no_speech_prob": 9.972327461582609e-06}, {"id": 770, "seek": 374658, "start": 3746.58, "end": 3748.34, "text": " and then we can add a comment.", "tokens": [293, 550, 321, 393, 909, 257, 2871, 13], "temperature": 0.0, "avg_logprob": -0.46039570294893706, "compression_ratio": 1.5731707317073171, "no_speech_prob": 2.7966325433226302e-05}, {"id": 771, "seek": 374658, "start": 3748.34, "end": 3753.7799999999997, "text": " Something we should do Zach is to actually include an example in the documents", "tokens": [6595, 321, 820, 360, 21028, 307, 281, 767, 4090, 364, 1365, 294, 264, 8512], "temperature": 0.0, "avg_logprob": -0.46039570294893706, "compression_ratio": 1.5731707317073171, "no_speech_prob": 2.7966325433226302e-05}, {"id": 772, "seek": 374658, "start": 3753.7799999999997, "end": 3756.98, "text": " documentation of what it ends up looking like in NB dev.", "tokens": [14333, 295, 437, 309, 5314, 493, 1237, 411, 294, 426, 33, 1905, 13], "temperature": 0.0, "avg_logprob": -0.46039570294893706, "compression_ratio": 1.5731707317073171, "no_speech_prob": 2.7966325433226302e-05}, {"id": 773, "seek": 374658, "start": 3757.62, "end": 3758.9, "text": " Because I can see that's missing.", "tokens": [1436, 286, 393, 536, 300, 311, 5361, 13], "temperature": 0.0, "avg_logprob": -0.46039570294893706, "compression_ratio": 1.5731707317073171, "no_speech_prob": 2.7966325433226302e-05}, {"id": 774, "seek": 374658, "start": 3759.7, "end": 3760.9, "text": " That might be a good idea.", "tokens": [663, 1062, 312, 257, 665, 1558, 13], "temperature": 0.0, "avg_logprob": -0.46039570294893706, "compression_ratio": 1.5731707317073171, "no_speech_prob": 2.7966325433226302e-05}, {"id": 775, "seek": 374658, "start": 3760.9, "end": 3763.38, "text": " I can see if I can get on that tomorrow.", "tokens": [286, 393, 536, 498, 286, 393, 483, 322, 300, 4153, 13], "temperature": 0.0, "avg_logprob": -0.46039570294893706, "compression_ratio": 1.5731707317073171, "no_speech_prob": 2.7966325433226302e-05}, {"id": 776, "seek": 374658, "start": 3763.38, "end": 3764.9, "text": " Yeah. Sorry, Hamel, what were you saying?", "tokens": [865, 13, 4919, 11, 8234, 338, 11, 437, 645, 291, 1566, 30], "temperature": 0.0, "avg_logprob": -0.46039570294893706, "compression_ratio": 1.5731707317073171, "no_speech_prob": 2.7966325433226302e-05}, {"id": 777, "seek": 374658, "start": 3766.1, "end": 3771.7799999999997, "text": " No, I just wanted to encourage everybody that like writing the documentation", "tokens": [883, 11, 286, 445, 1415, 281, 5373, 2201, 300, 411, 3579, 264, 14333], "temperature": 0.0, "avg_logprob": -0.46039570294893706, "compression_ratio": 1.5731707317073171, "no_speech_prob": 2.7966325433226302e-05}, {"id": 778, "seek": 377178, "start": 3771.78, "end": 3776.82, "text": " and learning deeply how everything works.", "tokens": [293, 2539, 8760, 577, 1203, 1985, 13], "temperature": 0.0, "avg_logprob": -0.32530089787074495, "compression_ratio": 1.5806451612903225, "no_speech_prob": 1.696301478659734e-05}, {"id": 779, "seek": 377178, "start": 3776.82, "end": 3781.38, "text": " And what ends up happening is you write this documentation", "tokens": [400, 437, 5314, 493, 2737, 307, 291, 2464, 341, 14333], "temperature": 0.0, "avg_logprob": -0.32530089787074495, "compression_ratio": 1.5806451612903225, "no_speech_prob": 1.696301478659734e-05}, {"id": 780, "seek": 377178, "start": 3781.38, "end": 3784.42, "text": " and somebody like Jeremy will review it carefully", "tokens": [293, 2618, 411, 17809, 486, 3131, 309, 7500], "temperature": 0.0, "avg_logprob": -0.32530089787074495, "compression_ratio": 1.5806451612903225, "no_speech_prob": 1.696301478659734e-05}, {"id": 781, "seek": 377178, "start": 3784.42, "end": 3786.42, "text": " and let you know what you don't understand.", "tokens": [293, 718, 291, 458, 437, 291, 500, 380, 1223, 13], "temperature": 0.0, "avg_logprob": -0.32530089787074495, "compression_ratio": 1.5806451612903225, "no_speech_prob": 1.696301478659734e-05}, {"id": 782, "seek": 377178, "start": 3787.38, "end": 3792.98, "text": " And that's how I learned about some other Fast AI libraries.", "tokens": [400, 300, 311, 577, 286, 3264, 466, 512, 661, 15968, 7318, 15148, 13], "temperature": 0.0, "avg_logprob": -0.32530089787074495, "compression_ratio": 1.5806451612903225, "no_speech_prob": 1.696301478659734e-05}, {"id": 783, "seek": 377178, "start": 3792.98, "end": 3796.5800000000004, "text": " So I highly recommend it going and doing that.", "tokens": [407, 286, 5405, 2748, 309, 516, 293, 884, 300, 13], "temperature": 0.0, "avg_logprob": -0.32530089787074495, "compression_ratio": 1.5806451612903225, "no_speech_prob": 1.696301478659734e-05}, {"id": 784, "seek": 377178, "start": 3797.2200000000003, "end": 3798.9, "text": " And here's what it ends up looking like.", "tokens": [400, 510, 311, 437, 309, 5314, 493, 1237, 411, 13], "temperature": 0.0, "avg_logprob": -0.32530089787074495, "compression_ratio": 1.5806451612903225, "no_speech_prob": 1.696301478659734e-05}, {"id": 785, "seek": 379890, "start": 3798.9, "end": 3802.42, "text": " So this is the source of optimizer and you can see it's got a little table underneath.", "tokens": [407, 341, 307, 264, 4009, 295, 5028, 6545, 293, 291, 393, 536, 309, 311, 658, 257, 707, 3199, 7223, 13], "temperature": 0.2, "avg_logprob": -0.34000523885091144, "compression_ratio": 1.6415094339622642, "no_speech_prob": 2.8407015634002164e-05}, {"id": 786, "seek": 379890, "start": 3803.06, "end": 3807.38, "text": " And if we look at the source of optimizer, you'll see that", "tokens": [400, 498, 321, 574, 412, 264, 4009, 295, 5028, 6545, 11, 291, 603, 536, 300], "temperature": 0.2, "avg_logprob": -0.34000523885091144, "compression_ratio": 1.6415094339622642, "no_speech_prob": 2.8407015634002164e-05}, {"id": 787, "seek": 379890, "start": 3809.3, "end": 3811.2200000000003, "text": " each parameter has a comment next to it.", "tokens": [1184, 13075, 575, 257, 2871, 958, 281, 309, 13], "temperature": 0.2, "avg_logprob": -0.34000523885091144, "compression_ratio": 1.6415094339622642, "no_speech_prob": 2.8407015634002164e-05}, {"id": 788, "seek": 379890, "start": 3811.2200000000003, "end": 3818.26, "text": " So those parameters are automatically turned into this table.", "tokens": [407, 729, 9834, 366, 6772, 3574, 666, 341, 3199, 13], "temperature": 0.2, "avg_logprob": -0.34000523885091144, "compression_ratio": 1.6415094339622642, "no_speech_prob": 2.8407015634002164e-05}, {"id": 789, "seek": 379890, "start": 3819.94, "end": 3820.7400000000002, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.2, "avg_logprob": -0.34000523885091144, "compression_ratio": 1.6415094339622642, "no_speech_prob": 2.8407015634002164e-05}, {"id": 790, "seek": 379890, "start": 3820.7400000000002, "end": 3822.1, "text": " Documents are super cool.", "tokens": [16024, 4697, 366, 1687, 1627, 13], "temperature": 0.2, "avg_logprob": -0.34000523885091144, "compression_ratio": 1.6415094339622642, "no_speech_prob": 2.8407015634002164e-05}, {"id": 791, "seek": 379890, "start": 3822.1, "end": 3822.98, "text": " They are super cool.", "tokens": [814, 366, 1687, 1627, 13], "temperature": 0.2, "avg_logprob": -0.34000523885091144, "compression_ratio": 1.6415094339622642, "no_speech_prob": 2.8407015634002164e-05}, {"id": 792, "seek": 379890, "start": 3822.98, "end": 3824.34, "text": " This sounds like a good place to wrap up.", "tokens": [639, 3263, 411, 257, 665, 1081, 281, 7019, 493, 13], "temperature": 0.2, "avg_logprob": -0.34000523885091144, "compression_ratio": 1.6415094339622642, "no_speech_prob": 2.8407015634002164e-05}, {"id": 793, "seek": 382434, "start": 3824.34, "end": 3830.7400000000002, "text": " Any questions or comments or anything before we wrap up?", "tokens": [2639, 1651, 420, 3053, 420, 1340, 949, 321, 7019, 493, 30], "temperature": 0.0, "avg_logprob": -0.23667650858561198, "compression_ratio": 1.3735632183908046, "no_speech_prob": 0.00016114763275254518}, {"id": 794, "seek": 382434, "start": 3830.7400000000002, "end": 3836.7400000000002, "text": " I have a question regarding to progressive resizing.", "tokens": [286, 362, 257, 1168, 8595, 281, 16131, 725, 3319, 13], "temperature": 0.0, "avg_logprob": -0.23667650858561198, "compression_ratio": 1.3735632183908046, "no_speech_prob": 0.00016114763275254518}, {"id": 795, "seek": 382434, "start": 3836.7400000000002, "end": 3837.2400000000002, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.23667650858561198, "compression_ratio": 1.3735632183908046, "no_speech_prob": 0.00016114763275254518}, {"id": 796, "seek": 382434, "start": 3838.9, "end": 3844.26, "text": " We didn't do actually LR find after each step.", "tokens": [492, 994, 380, 360, 767, 441, 49, 915, 934, 1184, 1823, 13], "temperature": 0.0, "avg_logprob": -0.23667650858561198, "compression_ratio": 1.3735632183908046, "no_speech_prob": 0.00016114763275254518}, {"id": 797, "seek": 382434, "start": 3844.82, "end": 3847.86, "text": " Don't you think it's something helpful?", "tokens": [1468, 380, 291, 519, 309, 311, 746, 4961, 30], "temperature": 0.0, "avg_logprob": -0.23667650858561198, "compression_ratio": 1.3735632183908046, "no_speech_prob": 0.00016114763275254518}, {"id": 798, "seek": 382434, "start": 3849.2200000000003, "end": 3850.34, "text": " The LR find, did you say?", "tokens": [440, 441, 49, 915, 11, 630, 291, 584, 30], "temperature": 0.0, "avg_logprob": -0.23667650858561198, "compression_ratio": 1.3735632183908046, "no_speech_prob": 0.00016114763275254518}, {"id": 799, "seek": 382434, "start": 3851.2200000000003, "end": 3851.86, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.23667650858561198, "compression_ratio": 1.3735632183908046, "no_speech_prob": 0.00016114763275254518}, {"id": 800, "seek": 382434, "start": 3851.86, "end": 3852.58, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.23667650858561198, "compression_ratio": 1.3735632183908046, "no_speech_prob": 0.00016114763275254518}, {"id": 801, "seek": 385258, "start": 3852.58, "end": 3859.46, "text": " Yeah, to be honest, I don't use LR find much anymore nowadays because", "tokens": [865, 11, 281, 312, 3245, 11, 286, 500, 380, 764, 441, 49, 915, 709, 3602, 13434, 570], "temperature": 0.0, "avg_logprob": -0.11889905368580538, "compression_ratio": 1.5023474178403755, "no_speech_prob": 5.421894002211047e-06}, {"id": 802, "seek": 385258, "start": 3863.54, "end": 3867.14, "text": " at least for object recognition in computer vision,", "tokens": [412, 1935, 337, 2657, 11150, 294, 3820, 5201, 11], "temperature": 0.0, "avg_logprob": -0.11889905368580538, "compression_ratio": 1.5023474178403755, "no_speech_prob": 5.421894002211047e-06}, {"id": 803, "seek": 385258, "start": 3867.14, "end": 3869.2999999999997, "text": " the optimal learning rate is pretty much always the same.", "tokens": [264, 16252, 2539, 3314, 307, 1238, 709, 1009, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.11889905368580538, "compression_ratio": 1.5023474178403755, "no_speech_prob": 5.421894002211047e-06}, {"id": 804, "seek": 385258, "start": 3870.1, "end": 3872.66, "text": " It's always around 0.08, 0.01.", "tokens": [467, 311, 1009, 926, 1958, 13, 16133, 11, 1958, 13, 10607, 13], "temperature": 0.0, "avg_logprob": -0.11889905368580538, "compression_ratio": 1.5023474178403755, "no_speech_prob": 5.421894002211047e-06}, {"id": 805, "seek": 385258, "start": 3875.86, "end": 3878.8199999999997, "text": " Yeah, there's no reason to believe that we have any need to change it", "tokens": [865, 11, 456, 311, 572, 1778, 281, 1697, 300, 321, 362, 604, 643, 281, 1319, 309], "temperature": 0.0, "avg_logprob": -0.11889905368580538, "compression_ratio": 1.5023474178403755, "no_speech_prob": 5.421894002211047e-06}, {"id": 806, "seek": 387882, "start": 3878.82, "end": 3883.46, "text": " just because we changed the resolution.", "tokens": [445, 570, 321, 3105, 264, 8669, 13], "temperature": 0.0, "avg_logprob": -0.1341224453313564, "compression_ratio": 1.7840909090909092, "no_speech_prob": 3.2188476325245574e-05}, {"id": 807, "seek": 387882, "start": 3883.46, "end": 3884.98, "text": " So yeah, I wouldn't bother.", "tokens": [407, 1338, 11, 286, 2759, 380, 8677, 13], "temperature": 0.0, "avg_logprob": -0.1341224453313564, "compression_ratio": 1.7840909090909092, "no_speech_prob": 3.2188476325245574e-05}, {"id": 808, "seek": 387882, "start": 3885.94, "end": 3886.82, "text": " Just leave it where it was.", "tokens": [1449, 1856, 309, 689, 309, 390, 13], "temperature": 0.0, "avg_logprob": -0.1341224453313564, "compression_ratio": 1.7840909090909092, "no_speech_prob": 3.2188476325245574e-05}, {"id": 809, "seek": 387882, "start": 3890.6600000000003, "end": 3894.98, "text": " Jeremy, if your training and validation loss is still decreasing after 12 people,", "tokens": [17809, 11, 498, 428, 3097, 293, 24071, 4470, 307, 920, 23223, 934, 2272, 561, 11], "temperature": 0.0, "avg_logprob": -0.1341224453313564, "compression_ratio": 1.7840909090909092, "no_speech_prob": 3.2188476325245574e-05}, {"id": 810, "seek": 387882, "start": 3894.98, "end": 3898.6600000000003, "text": " can you pick up and train for a little longer without restarting?", "tokens": [393, 291, 1888, 493, 293, 3847, 337, 257, 707, 2854, 1553, 21022, 278, 30], "temperature": 0.0, "avg_logprob": -0.1341224453313564, "compression_ratio": 1.7840909090909092, "no_speech_prob": 3.2188476325245574e-05}, {"id": 811, "seek": 387882, "start": 3899.54, "end": 3899.94, "text": " You can.", "tokens": [509, 393, 13], "temperature": 0.0, "avg_logprob": -0.1341224453313564, "compression_ratio": 1.7840909090909092, "no_speech_prob": 3.2188476325245574e-05}, {"id": 812, "seek": 387882, "start": 3899.94, "end": 3902.7400000000002, "text": " The first thing I'll say is you shouldn't be looking at the validation loss to see if", "tokens": [440, 700, 551, 286, 603, 584, 307, 291, 4659, 380, 312, 1237, 412, 264, 24071, 4470, 281, 536, 498], "temperature": 0.0, "avg_logprob": -0.1341224453313564, "compression_ratio": 1.7840909090909092, "no_speech_prob": 3.2188476325245574e-05}, {"id": 813, "seek": 387882, "start": 3902.7400000000002, "end": 3903.3, "text": " you're overfitting.", "tokens": [291, 434, 670, 69, 2414, 13], "temperature": 0.0, "avg_logprob": -0.1341224453313564, "compression_ratio": 1.7840909090909092, "no_speech_prob": 3.2188476325245574e-05}, {"id": 814, "seek": 387882, "start": 3903.3, "end": 3905.06, "text": " You should be looking at the error rate.", "tokens": [509, 820, 312, 1237, 412, 264, 6713, 3314, 13], "temperature": 0.0, "avg_logprob": -0.1341224453313564, "compression_ratio": 1.7840909090909092, "no_speech_prob": 3.2188476325245574e-05}, {"id": 815, "seek": 390506, "start": 3905.06, "end": 3908.9, "text": " So the validation loss can get worse whilst the error rate gets better.", "tokens": [407, 264, 24071, 4470, 393, 483, 5324, 18534, 264, 6713, 3314, 2170, 1101, 13], "temperature": 0.0, "avg_logprob": -0.15822974495265796, "compression_ratio": 1.7186311787072244, "no_speech_prob": 1.6280029058179935e-06}, {"id": 816, "seek": 390506, "start": 3908.9, "end": 3911.7799999999997, "text": " And that doesn't count as overfitting because the thing you want is to improve", "tokens": [400, 300, 1177, 380, 1207, 382, 670, 69, 2414, 570, 264, 551, 291, 528, 307, 281, 3470], "temperature": 0.0, "avg_logprob": -0.15822974495265796, "compression_ratio": 1.7186311787072244, "no_speech_prob": 1.6280029058179935e-06}, {"id": 817, "seek": 390506, "start": 3911.7799999999997, "end": 3912.5, "text": " is the error rate.", "tokens": [307, 264, 6713, 3314, 13], "temperature": 0.0, "avg_logprob": -0.15822974495265796, "compression_ratio": 1.7186311787072244, "no_speech_prob": 1.6280029058179935e-06}, {"id": 818, "seek": 390506, "start": 3913.2999999999997, "end": 3916.42, "text": " That can happen if it gets overconfident, but it's still improving.", "tokens": [663, 393, 1051, 498, 309, 2170, 670, 24697, 1078, 11, 457, 309, 311, 920, 11470, 13], "temperature": 0.0, "avg_logprob": -0.15822974495265796, "compression_ratio": 1.7186311787072244, "no_speech_prob": 1.6280029058179935e-06}, {"id": 819, "seek": 390506, "start": 3917.7799999999997, "end": 3924.2599999999998, "text": " Yeah, you can keep training for longer because we're using,", "tokens": [865, 11, 291, 393, 1066, 3097, 337, 2854, 570, 321, 434, 1228, 11], "temperature": 0.0, "avg_logprob": -0.15822974495265796, "compression_ratio": 1.7186311787072244, "no_speech_prob": 1.6280029058179935e-06}, {"id": 820, "seek": 390506, "start": 3924.2599999999998, "end": 3929.14, "text": " if you're using fit1 cycle or fine tune and fine tune uses fit1 cycle behind the scenes.", "tokens": [498, 291, 434, 1228, 3318, 16, 6586, 420, 2489, 10864, 293, 2489, 10864, 4960, 3318, 16, 6586, 2261, 264, 8026, 13], "temperature": 0.0, "avg_logprob": -0.15822974495265796, "compression_ratio": 1.7186311787072244, "no_speech_prob": 1.6280029058179935e-06}, {"id": 821, "seek": 390506, "start": 3931.7799999999997, "end": 3934.66, "text": " Continuing to train further, your learning rate is going to go up", "tokens": [47585, 281, 3847, 3052, 11, 428, 2539, 3314, 307, 516, 281, 352, 493], "temperature": 0.0, "avg_logprob": -0.15822974495265796, "compression_ratio": 1.7186311787072244, "no_speech_prob": 1.6280029058179935e-06}, {"id": 822, "seek": 393466, "start": 3934.66, "end": 3938.66, "text": " and then down and then up and down each time, which is not necessarily a bad thing.", "tokens": [293, 550, 760, 293, 550, 493, 293, 760, 1184, 565, 11, 597, 307, 406, 4725, 257, 1578, 551, 13], "temperature": 0.0, "avg_logprob": -0.07848822443108809, "compression_ratio": 1.5265957446808511, "no_speech_prob": 9.515430974715855e-06}, {"id": 823, "seek": 393466, "start": 3940.18, "end": 3951.7, "text": " But if you basically want to keep training at that point,", "tokens": [583, 498, 291, 1936, 528, 281, 1066, 3097, 412, 300, 935, 11], "temperature": 0.0, "avg_logprob": -0.07848822443108809, "compression_ratio": 1.5265957446808511, "no_speech_prob": 9.515430974715855e-06}, {"id": 824, "seek": 393466, "start": 3951.7, "end": 3956.2599999999998, "text": " you would probably want to decrease the learning rate by maybe 4x or so.", "tokens": [291, 576, 1391, 528, 281, 11514, 264, 2539, 3314, 538, 1310, 1017, 87, 420, 370, 13], "temperature": 0.0, "avg_logprob": -0.07848822443108809, "compression_ratio": 1.5265957446808511, "no_speech_prob": 9.515430974715855e-06}, {"id": 825, "seek": 393466, "start": 3956.8199999999997, "end": 3961.54, "text": " And in fact, I think after this, I'm going to rerun this whole notebook.", "tokens": [400, 294, 1186, 11, 286, 519, 934, 341, 11, 286, 478, 516, 281, 43819, 409, 341, 1379, 21060, 13], "temperature": 0.0, "avg_logprob": -0.07848822443108809, "compression_ratio": 1.5265957446808511, "no_speech_prob": 9.515430974715855e-06}, {"id": 826, "seek": 396154, "start": 3961.54, "end": 3970.82, "text": " But half the learning rate each time, because I think that would be potentially a good idea.", "tokens": [583, 1922, 264, 2539, 3314, 1184, 565, 11, 570, 286, 519, 300, 576, 312, 7263, 257, 665, 1558, 13], "temperature": 0.0, "avg_logprob": -0.25031880364901776, "compression_ratio": 1.4230769230769231, "no_speech_prob": 8.011640602489933e-06}, {"id": 827, "seek": 396154, "start": 3974.66, "end": 3975.7799999999997, "text": " I have a question.", "tokens": [286, 362, 257, 1168, 13], "temperature": 0.0, "avg_logprob": -0.25031880364901776, "compression_ratio": 1.4230769230769231, "no_speech_prob": 8.011640602489933e-06}, {"id": 828, "seek": 396154, "start": 3975.7799999999997, "end": 3980.82, "text": " I don't know if it's too late, but I think it might be useful to discuss", "tokens": [286, 500, 380, 458, 498, 309, 311, 886, 3469, 11, 457, 286, 519, 309, 1062, 312, 4420, 281, 2248], "temperature": 0.0, "avg_logprob": -0.25031880364901776, "compression_ratio": 1.4230769230769231, "no_speech_prob": 8.011640602489933e-06}, {"id": 829, "seek": 396154, "start": 3980.82, "end": 3988.58, "text": " when you do the progressive resizing, what part of the model gets dropped?", "tokens": [562, 291, 360, 264, 16131, 725, 3319, 11, 437, 644, 295, 264, 2316, 2170, 8119, 30], "temperature": 0.0, "avg_logprob": -0.25031880364901776, "compression_ratio": 1.4230769230769231, "no_speech_prob": 8.011640602489933e-06}, {"id": 830, "seek": 398858, "start": 3988.58, "end": 3993.86, "text": " Is there some part of the model that needs to be reinitialized?", "tokens": [1119, 456, 512, 644, 295, 264, 2316, 300, 2203, 281, 312, 6561, 270, 831, 1602, 30], "temperature": 0.0, "avg_logprob": -0.37865816313644934, "compression_ratio": 1.6, "no_speech_prob": 5.737713581766002e-05}, {"id": 831, "seek": 398858, "start": 3993.86, "end": 3996.1, "text": " No, nothing needs to be reinitialized.", "tokens": [883, 11, 1825, 2203, 281, 312, 6561, 270, 831, 1602, 13], "temperature": 0.0, "avg_logprob": -0.37865816313644934, "compression_ratio": 1.6, "no_speech_prob": 5.737713581766002e-05}, {"id": 832, "seek": 398858, "start": 3996.1, "end": 3996.66, "text": " No.", "tokens": [883, 13], "temperature": 0.0, "avg_logprob": -0.37865816313644934, "compression_ratio": 1.6, "no_speech_prob": 5.737713581766002e-05}, {"id": 833, "seek": 398858, "start": 3996.66, "end": 3997.86, "text": " I found this on the web.", "tokens": [286, 1352, 341, 322, 264, 3670, 13], "temperature": 0.0, "avg_logprob": -0.37865816313644934, "compression_ratio": 1.6, "no_speech_prob": 5.737713581766002e-05}, {"id": 834, "seek": 398858, "start": 3997.86, "end": 3999.86, "text": " Who found what on the web?", "tokens": [2102, 1352, 437, 322, 264, 3670, 30], "temperature": 0.0, "avg_logprob": -0.37865816313644934, "compression_ratio": 1.6, "no_speech_prob": 5.737713581766002e-05}, {"id": 835, "seek": 398858, "start": 4001.86, "end": 4004.18, "text": " I thought you were talking to me, but you're talking to Siri.", "tokens": [286, 1194, 291, 645, 1417, 281, 385, 11, 457, 291, 434, 1417, 281, 33682, 13], "temperature": 0.0, "avg_logprob": -0.37865816313644934, "compression_ratio": 1.6, "no_speech_prob": 5.737713581766002e-05}, {"id": 836, "seek": 398858, "start": 4004.18, "end": 4004.98, "text": " I'm offended.", "tokens": [286, 478, 26776, 13], "temperature": 0.0, "avg_logprob": -0.37865816313644934, "compression_ratio": 1.6, "no_speech_prob": 5.737713581766002e-05}, {"id": 837, "seek": 398858, "start": 4007.94, "end": 4009.86, "text": " Siri, teach me deep learning.", "tokens": [33682, 11, 2924, 385, 2452, 2539, 13], "temperature": 0.0, "avg_logprob": -0.37865816313644934, "compression_ratio": 1.6, "no_speech_prob": 5.737713581766002e-05}, {"id": 838, "seek": 400986, "start": 4009.86, "end": 4014.6600000000003, "text": " Yeah, ConfNext is what we call a resolution-independent architecture,", "tokens": [865, 11, 11701, 31002, 307, 437, 321, 818, 257, 8669, 12, 471, 4217, 317, 9482, 11], "temperature": 0.0, "avg_logprob": -0.3689621743701753, "compression_ratio": 1.5188679245283019, "no_speech_prob": 9.078210496227257e-06}, {"id": 839, "seek": 400986, "start": 4014.6600000000003, "end": 4018.1, "text": " which means it works for any input resolution.", "tokens": [597, 1355, 309, 1985, 337, 604, 4846, 8669, 13], "temperature": 0.0, "avg_logprob": -0.3689621743701753, "compression_ratio": 1.5188679245283019, "no_speech_prob": 9.078210496227257e-06}, {"id": 840, "seek": 400986, "start": 4018.98, "end": 4027.78, "text": " And time permitting in the next lesson, we will see how convolutional neural networks", "tokens": [400, 565, 4784, 2414, 294, 264, 958, 6898, 11, 321, 486, 536, 577, 45216, 304, 18161, 9590], "temperature": 0.0, "avg_logprob": -0.3689621743701753, "compression_ratio": 1.5188679245283019, "no_speech_prob": 9.078210496227257e-06}, {"id": 841, "seek": 400986, "start": 4027.78, "end": 4029.86, "text": " actually work, but I guess a lot of you probably already know.", "tokens": [767, 589, 11, 457, 286, 2041, 257, 688, 295, 291, 1391, 1217, 458, 13], "temperature": 0.0, "avg_logprob": -0.3689621743701753, "compression_ratio": 1.5188679245283019, "no_speech_prob": 9.078210496227257e-06}, {"id": 842, "seek": 400986, "start": 4029.86, "end": 4034.6600000000003, "text": " So for those of you that do, if you think you can do it,", "tokens": [407, 337, 729, 295, 291, 300, 360, 11, 498, 291, 519, 291, 393, 360, 309, 11], "temperature": 0.0, "avg_logprob": -0.3689621743701753, "compression_ratio": 1.5188679245283019, "no_speech_prob": 9.078210496227257e-06}, {"id": 843, "seek": 403466, "start": 4034.66, "end": 4040.42, "text": " it's basically going patch by patch and doing this kind of mini matrix", "tokens": [309, 311, 1936, 516, 9972, 538, 9972, 293, 884, 341, 733, 295, 8382, 8141], "temperature": 0.0, "avg_logprob": -0.40402486372967156, "compression_ratio": 1.5681818181818181, "no_speech_prob": 1.7501221009297296e-05}, {"id": 844, "seek": 403466, "start": 4040.42, "end": 4041.94, "text": " multiplier for each patch.", "tokens": [44106, 337, 1184, 9972, 13], "temperature": 0.0, "avg_logprob": -0.40402486372967156, "compression_ratio": 1.5681818181818181, "no_speech_prob": 1.7501221009297296e-05}, {"id": 845, "seek": 403466, "start": 4044.2599999999998, "end": 4048.18, "text": " So if you change the input resolution, it just has more patches to cover,", "tokens": [407, 498, 291, 1319, 264, 4846, 8669, 11, 309, 445, 575, 544, 26531, 281, 2060, 11], "temperature": 0.0, "avg_logprob": -0.40402486372967156, "compression_ratio": 1.5681818181818181, "no_speech_prob": 1.7501221009297296e-05}, {"id": 846, "seek": 403466, "start": 4048.18, "end": 4050.18, "text": " but it doesn't change the parameters at all.", "tokens": [457, 309, 1177, 380, 1319, 264, 9834, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.40402486372967156, "compression_ratio": 1.5681818181818181, "no_speech_prob": 1.7501221009297296e-05}, {"id": 847, "seek": 403466, "start": 4050.18, "end": 4053.54, "text": " So there's nothing to reinitialize.", "tokens": [407, 456, 311, 1825, 281, 6561, 270, 831, 1125, 13], "temperature": 0.0, "avg_logprob": -0.40402486372967156, "compression_ratio": 1.5681818181818181, "no_speech_prob": 1.7501221009297296e-05}, {"id": 848, "seek": 403466, "start": 4054.58, "end": 4056.1, "text": " Does that make sense, Hamill?", "tokens": [4402, 300, 652, 2020, 11, 8234, 373, 30], "temperature": 0.0, "avg_logprob": -0.40402486372967156, "compression_ratio": 1.5681818181818181, "no_speech_prob": 1.7501221009297296e-05}, {"id": 849, "seek": 403466, "start": 4056.8199999999997, "end": 4058.02, "text": " Yeah, that makes sense.", "tokens": [865, 11, 300, 1669, 2020, 13], "temperature": 0.0, "avg_logprob": -0.40402486372967156, "compression_ratio": 1.5681818181818181, "no_speech_prob": 1.7501221009297296e-05}, {"id": 850, "seek": 403466, "start": 4058.02, "end": 4059.7799999999997, "text": " I was just going to say that, I think,", "tokens": [286, 390, 445, 516, 281, 584, 300, 11, 286, 519, 11], "temperature": 0.0, "avg_logprob": -0.40402486372967156, "compression_ratio": 1.5681818181818181, "no_speech_prob": 1.7501221009297296e-05}, {"id": 851, "seek": 405978, "start": 4059.78, "end": 4064.98, "text": " is ResNet resolution-independent?", "tokens": [307, 5015, 31890, 8669, 12, 471, 4217, 317, 30], "temperature": 0.0, "avg_logprob": -0.9587661900471166, "compression_ratio": 1.7852760736196318, "no_speech_prob": 6.501442840090021e-05}, {"id": 852, "seek": 405978, "start": 4064.98, "end": 4069.0600000000004, "text": " Basically, everything we use is normally, but in the,", "tokens": [8537, 11, 1203, 321, 764, 307, 5646, 11, 457, 294, 264, 11], "temperature": 0.0, "avg_logprob": -0.9587661900471166, "compression_ratio": 1.7852760736196318, "no_speech_prob": 6.501442840090021e-05}, {"id": 853, "seek": 405978, "start": 4069.0600000000004, "end": 4074.02, "text": " like, have a look at that, like, best fine-tuning,", "tokens": [411, 11, 362, 257, 574, 412, 300, 11, 411, 11, 1151, 2489, 12, 83, 37726, 11], "temperature": 0.0, "avg_logprob": -0.9587661900471166, "compression_ratio": 1.7852760736196318, "no_speech_prob": 6.501442840090021e-05}, {"id": 854, "seek": 405978, "start": 4074.02, "end": 4076.02, "text": " and then we can do the same thing.", "tokens": [293, 550, 321, 393, 360, 264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.9587661900471166, "compression_ratio": 1.7852760736196318, "no_speech_prob": 6.501442840090021e-05}, {"id": 855, "seek": 405978, "start": 4076.02, "end": 4078.02, "text": " So that's the way it works.", "tokens": [407, 300, 311, 264, 636, 309, 1985, 13], "temperature": 0.0, "avg_logprob": -0.9587661900471166, "compression_ratio": 1.7852760736196318, "no_speech_prob": 6.501442840090021e-05}, {"id": 856, "seek": 405978, "start": 4078.02, "end": 4080.02, "text": " So I think that's a good point.", "tokens": [407, 286, 519, 300, 311, 257, 665, 935, 13], "temperature": 0.0, "avg_logprob": -0.9587661900471166, "compression_ratio": 1.7852760736196318, "no_speech_prob": 6.501442840090021e-05}, {"id": 857, "seek": 405978, "start": 4080.02, "end": 4082.02, "text": " I think that's a good point.", "tokens": [286, 519, 300, 311, 257, 665, 935, 13], "temperature": 0.0, "avg_logprob": -0.9587661900471166, "compression_ratio": 1.7852760736196318, "no_speech_prob": 6.501442840090021e-05}, {"id": 858, "seek": 405978, "start": 4082.02, "end": 4084.02, "text": " I think that's a good point.", "tokens": [286, 519, 300, 311, 257, 665, 935, 13], "temperature": 0.0, "avg_logprob": -0.9587661900471166, "compression_ratio": 1.7852760736196318, "no_speech_prob": 6.501442840090021e-05}, {"id": 859, "seek": 408402, "start": 4084.02, "end": 4090.02, "text": " But in the, like, have a look at that, like, best fine-tuning", "tokens": [583, 294, 264, 11, 411, 11, 362, 257, 574, 412, 300, 11, 411, 11, 1151, 2489, 12, 83, 37726], "temperature": 0.0, "avg_logprob": -0.11769225809833792, "compression_ratio": 1.580188679245283, "no_speech_prob": 1.165822959592333e-05}, {"id": 860, "seek": 408402, "start": 4091.7, "end": 4094.66, "text": " models notebook, and you'll see that two of the best ones are called", "tokens": [5245, 21060, 11, 293, 291, 603, 536, 300, 732, 295, 264, 1151, 2306, 366, 1219], "temperature": 0.0, "avg_logprob": -0.11769225809833792, "compression_ratio": 1.580188679245283, "no_speech_prob": 1.165822959592333e-05}, {"id": 861, "seek": 408402, "start": 4095.3, "end": 4098.9, "text": " VIT and SWIN, and also SWINv2.", "tokens": [691, 3927, 293, 20346, 1464, 11, 293, 611, 20346, 1464, 85, 17, 13], "temperature": 0.0, "avg_logprob": -0.11769225809833792, "compression_ratio": 1.580188679245283, "no_speech_prob": 1.165822959592333e-05}, {"id": 862, "seek": 408402, "start": 4098.9, "end": 4100.66, "text": " None of those are resolution-independent.", "tokens": [14492, 295, 729, 366, 8669, 12, 471, 4217, 317, 13], "temperature": 0.0, "avg_logprob": -0.11769225809833792, "compression_ratio": 1.580188679245283, "no_speech_prob": 1.165822959592333e-05}, {"id": 863, "seek": 408402, "start": 4102.98, "end": 4107.86, "text": " Although there is a trick you can use to kind of make them resolution-independent,", "tokens": [5780, 456, 307, 257, 4282, 291, 393, 764, 281, 733, 295, 652, 552, 8669, 12, 471, 4217, 317, 11], "temperature": 0.0, "avg_logprob": -0.11769225809833792, "compression_ratio": 1.580188679245283, "no_speech_prob": 1.165822959592333e-05}, {"id": 864, "seek": 408402, "start": 4107.86, "end": 4110.82, "text": " which we should try out in a future walkthrough.", "tokens": [597, 321, 820, 853, 484, 294, 257, 2027, 1792, 11529, 13], "temperature": 0.0, "avg_logprob": -0.11769225809833792, "compression_ratio": 1.580188679245283, "no_speech_prob": 1.165822959592333e-05}, {"id": 865, "seek": 411082, "start": 4110.82, "end": 4112.0199999999995, "text": " Is that fiddling with the head?", "tokens": [1119, 300, 283, 14273, 1688, 365, 264, 1378, 30], "temperature": 0.0, "avg_logprob": -0.3799920581635975, "compression_ratio": 1.558139534883721, "no_speech_prob": 1.8056696717394516e-05}, {"id": 866, "seek": 411082, "start": 4113.219999999999, "end": 4113.78, "text": " Or something?", "tokens": [1610, 746, 30], "temperature": 0.0, "avg_logprob": -0.3799920581635975, "compression_ratio": 1.558139534883721, "no_speech_prob": 1.8056696717394516e-05}, {"id": 867, "seek": 411082, "start": 4113.78, "end": 4114.74, "text": " Oh, there's a Tim.", "tokens": [876, 11, 456, 311, 257, 7172, 13], "temperature": 0.0, "avg_logprob": -0.3799920581635975, "compression_ratio": 1.558139534883721, "no_speech_prob": 1.8056696717394516e-05}, {"id": 868, "seek": 411082, "start": 4114.74, "end": 4116.259999999999, "text": " There's a thing you can pass to Tim.", "tokens": [821, 311, 257, 551, 291, 393, 1320, 281, 7172, 13], "temperature": 0.0, "avg_logprob": -0.3799920581635975, "compression_ratio": 1.558139534883721, "no_speech_prob": 1.8056696717394516e-05}, {"id": 869, "seek": 411082, "start": 4119.0599999999995, "end": 4122.259999999999, "text": " I don't know if we can use it to support progressive resizing or not.", "tokens": [286, 500, 380, 458, 498, 321, 393, 764, 309, 281, 1406, 16131, 725, 3319, 420, 406, 13], "temperature": 0.0, "avg_logprob": -0.3799920581635975, "compression_ratio": 1.558139534883721, "no_speech_prob": 1.8056696717394516e-05}, {"id": 870, "seek": 411082, "start": 4122.259999999999, "end": 4124.74, "text": " It'll be interesting to experiment with.", "tokens": [467, 603, 312, 1880, 281, 5120, 365, 13], "temperature": 0.0, "avg_logprob": -0.3799920581635975, "compression_ratio": 1.558139534883721, "no_speech_prob": 1.8056696717394516e-05}, {"id": 871, "seek": 411082, "start": 4125.7, "end": 4128.34, "text": " It's basically changing the positional encodings.", "tokens": [467, 311, 1936, 4473, 264, 2535, 304, 2058, 378, 1109, 13], "temperature": 0.0, "avg_logprob": -0.3799920581635975, "compression_ratio": 1.558139534883721, "no_speech_prob": 1.8056696717394516e-05}, {"id": 872, "seek": 411082, "start": 4131.86, "end": 4132.74, "text": " I have a question.", "tokens": [286, 362, 257, 1168, 13], "temperature": 0.0, "avg_logprob": -0.3799920581635975, "compression_ratio": 1.558139534883721, "no_speech_prob": 1.8056696717394516e-05}, {"id": 873, "seek": 411082, "start": 4132.74, "end": 4133.299999999999, "text": " Interesting.", "tokens": [14711, 13], "temperature": 0.0, "avg_logprob": -0.3799920581635975, "compression_ratio": 1.558139534883721, "no_speech_prob": 1.8056696717394516e-05}, {"id": 874, "seek": 411082, "start": 4133.299999999999, "end": 4133.799999999999, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.3799920581635975, "compression_ratio": 1.558139534883721, "no_speech_prob": 1.8056696717394516e-05}, {"id": 875, "seek": 411082, "start": 4135.219999999999, "end": 4138.34, "text": " After you've done your experiment,", "tokens": [2381, 291, 600, 1096, 428, 5120, 11], "temperature": 0.0, "avg_logprob": -0.3799920581635975, "compression_ratio": 1.558139534883721, "no_speech_prob": 1.8056696717394516e-05}, {"id": 876, "seek": 413834, "start": 4138.34, "end": 4146.34, "text": " progressive resizing and fine-tuning, how do you, in Fast.AI, train with the whole training set?", "tokens": [16131, 725, 3319, 293, 2489, 12, 83, 37726, 11, 577, 360, 291, 11, 294, 15968, 13, 48698, 11, 3847, 365, 264, 1379, 3097, 992, 30], "temperature": 0.0, "avg_logprob": -0.4151315255598588, "compression_ratio": 1.4878048780487805, "no_speech_prob": 1.1300169717287645e-05}, {"id": 877, "seek": 413834, "start": 4147.14, "end": 4148.74, "text": " I never got around to do that.", "tokens": [286, 1128, 658, 926, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.4151315255598588, "compression_ratio": 1.4878048780487805, "no_speech_prob": 1.1300169717287645e-05}, {"id": 878, "seek": 413834, "start": 4149.62, "end": 4150.9800000000005, "text": " Do you skate a dummy?", "tokens": [1144, 291, 18237, 257, 35064, 30], "temperature": 0.0, "avg_logprob": -0.4151315255598588, "compression_ratio": 1.4878048780487805, "no_speech_prob": 1.1300169717287645e-05}, {"id": 879, "seek": 413834, "start": 4150.9800000000005, "end": 4152.9800000000005, "text": " I almost never do.", "tokens": [286, 1920, 1128, 360, 13], "temperature": 0.0, "avg_logprob": -0.4151315255598588, "compression_ratio": 1.4878048780487805, "no_speech_prob": 1.1300169717287645e-05}, {"id": 880, "seek": 413834, "start": 4152.9800000000005, "end": 4159.9400000000005, "text": " Like, instead, I do what we saw in the last walkthrough, which is I just train", "tokens": [1743, 11, 2602, 11, 286, 360, 437, 321, 1866, 294, 264, 1036, 1792, 11529, 11, 597, 307, 286, 445, 3847], "temperature": 0.0, "avg_logprob": -0.4151315255598588, "compression_ratio": 1.4878048780487805, "no_speech_prob": 1.1300169717287645e-05}, {"id": 881, "seek": 413834, "start": 4160.66, "end": 4165.46, "text": " on a few different randomly selected valid training sets.", "tokens": [322, 257, 1326, 819, 16979, 8209, 7363, 3097, 6352, 13], "temperature": 0.0, "avg_logprob": -0.4151315255598588, "compression_ratio": 1.4878048780487805, "no_speech_prob": 1.1300169717287645e-05}, {"id": 882, "seek": 416546, "start": 4165.46, "end": 4172.66, "text": " Because that way, you know, you get the benefit on sombering.", "tokens": [1436, 300, 636, 11, 291, 458, 11, 291, 483, 264, 5121, 322, 3307, 607, 278, 13], "temperature": 0.0, "avg_logprob": -0.3549258688221807, "compression_ratio": 1.6570247933884297, "no_speech_prob": 7.645819096069317e-06}, {"id": 883, "seek": 416546, "start": 4172.66, "end": 4175.06, "text": " You're going to end up seeing all the images, at least one anyway.", "tokens": [509, 434, 516, 281, 917, 493, 2577, 439, 264, 5267, 11, 412, 1935, 472, 4033, 13], "temperature": 0.0, "avg_logprob": -0.3549258688221807, "compression_ratio": 1.6570247933884297, "no_speech_prob": 7.645819096069317e-06}, {"id": 884, "seek": 416546, "start": 4176.02, "end": 4180.66, "text": " And you can also kind of see if something's messed up because you've still got a validation set each time.", "tokens": [400, 291, 393, 611, 733, 295, 536, 498, 746, 311, 16507, 493, 570, 291, 600, 920, 658, 257, 24071, 992, 1184, 565, 13], "temperature": 0.0, "avg_logprob": -0.3549258688221807, "compression_ratio": 1.6570247933884297, "no_speech_prob": 7.645819096069317e-06}, {"id": 885, "seek": 416546, "start": 4181.38, "end": 4187.06, "text": " So, yeah, I used to, like, do this thing where I would create a validation set with a single item in", "tokens": [407, 11, 1338, 11, 286, 1143, 281, 11, 411, 11, 360, 341, 551, 689, 286, 576, 1884, 257, 24071, 992, 365, 257, 2167, 3174, 294], "temperature": 0.0, "avg_logprob": -0.3549258688221807, "compression_ratio": 1.6570247933884297, "no_speech_prob": 7.645819096069317e-06}, {"id": 886, "seek": 416546, "start": 4187.62, "end": 4189.06, "text": " to, like, get that last bit of juice.", "tokens": [281, 11, 411, 11, 483, 300, 1036, 857, 295, 8544, 13], "temperature": 0.0, "avg_logprob": -0.3549258688221807, "compression_ratio": 1.6570247933884297, "no_speech_prob": 7.645819096069317e-06}, {"id": 887, "seek": 416546, "start": 4189.06, "end": 4191.38, "text": " But I don't, I don't know.", "tokens": [583, 286, 500, 380, 11, 286, 500, 380, 458, 13], "temperature": 0.0, "avg_logprob": -0.3549258688221807, "compression_ratio": 1.6570247933884297, "no_speech_prob": 7.645819096069317e-06}, {"id": 888, "seek": 419138, "start": 4191.38, "end": 4196.5, "text": " I don't even do that anymore.", "tokens": [286, 500, 380, 754, 360, 300, 3602, 13], "temperature": 0.0, "avg_logprob": -0.24799960209773136, "compression_ratio": 1.282442748091603, "no_speech_prob": 0.0004229841579217464}, {"id": 889, "seek": 419138, "start": 4199.86, "end": 4200.900000000001, "text": " Okay, thanks.", "tokens": [1033, 11, 3231, 13], "temperature": 0.0, "avg_logprob": -0.24799960209773136, "compression_ratio": 1.282442748091603, "no_speech_prob": 0.0004229841579217464}, {"id": 890, "seek": 419138, "start": 4200.900000000001, "end": 4201.400000000001, "text": " No worries.", "tokens": [883, 16340, 13], "temperature": 0.0, "avg_logprob": -0.24799960209773136, "compression_ratio": 1.282442748091603, "no_speech_prob": 0.0004229841579217464}, {"id": 891, "seek": 419138, "start": 4203.7, "end": 4204.74, "text": " All right, gang.", "tokens": [1057, 558, 11, 10145, 13], "temperature": 0.0, "avg_logprob": -0.24799960209773136, "compression_ratio": 1.282442748091603, "no_speech_prob": 0.0004229841579217464}, {"id": 892, "seek": 419138, "start": 4204.74, "end": 4206.900000000001, "text": " Enjoy the rest of your day slash evening.", "tokens": [15411, 264, 1472, 295, 428, 786, 17330, 5634, 13], "temperature": 0.0, "avg_logprob": -0.24799960209773136, "compression_ratio": 1.282442748091603, "no_speech_prob": 0.0004229841579217464}, {"id": 893, "seek": 419138, "start": 4206.900000000001, "end": 4207.7, "text": " Nice to see you all.", "tokens": [5490, 281, 536, 291, 439, 13], "temperature": 0.0, "avg_logprob": -0.24799960209773136, "compression_ratio": 1.282442748091603, "no_speech_prob": 0.0004229841579217464}, {"id": 894, "seek": 419138, "start": 4208.900000000001, "end": 4209.400000000001, "text": " Bye.", "tokens": [4621, 13], "temperature": 0.0, "avg_logprob": -0.24799960209773136, "compression_ratio": 1.282442748091603, "no_speech_prob": 0.0004229841579217464}, {"id": 895, "seek": 419138, "start": 4210.02, "end": 4210.5, "text": " Goodbye.", "tokens": [15528, 13], "temperature": 0.0, "avg_logprob": -0.24799960209773136, "compression_ratio": 1.282442748091603, "no_speech_prob": 0.0004229841579217464}, {"id": 896, "seek": 419138, "start": 4210.5, "end": 4211.0, "text": " Thanks.", "tokens": [2561, 13], "temperature": 0.0, "avg_logprob": -0.24799960209773136, "compression_ratio": 1.282442748091603, "no_speech_prob": 0.0004229841579217464}, {"id": 897, "seek": 421100, "start": 4211.0, "end": 4221.0, "text": " Thank you.", "tokens": [50364, 1044, 291, 13, 50864], "temperature": 0.0, "avg_logprob": -0.5525321960449219, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.00025561568327248096}], "language": "en"}