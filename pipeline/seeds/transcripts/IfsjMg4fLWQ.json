{"text": " Alright, so next we're going to be getting into translation as our next task. And so this chart will kind of show the spoiler alert of where we're going, but we're going to approach translation through kind of a variety of ways. We're going to start with a more basic sequence to sequence model. And over here this column is showing the accuracy we'll get with that. We'll improve it by adding teacher forcing to it, and we'll improve it even more by adding attention. And then in the next lesson we will cover transformers. So we probably won't get to this until later next week, and that'll give us even much higher accuracy. But it's important to kind of understand some of these components. Teacher forcing is a very, very useful technique. Attention is what's used in transformers, so understanding that will be helpful. It will kind of be building up better and better accuracy by adding more to our seek to seek model and then switching to a transformer model. And so this work I should say is based off of notebooks created by Sylvain Gugger, our kind of researcher, one of our researchers at Fast AI. We'll be looking at the task of translating from French to English, and we're trying to keep this as a kind of manageable size task, so we're going to limit ourselves to translating questions. There's a great data set that was put together by Chris Callison Birch. And this, I should say this question of even creating data sets is a really important one of just kind of how do you create the data sets you need. And he used that in Canada, websites have to have a French version and an English version since they speak both languages, and so that gives a nice kind of nice set of parallel text. So he kind of crawled millions of web pages to create this. And I should note that seek to seek can be more challenging than classification since the input or the output is a variable length, right? The translation of a sentence is not necessarily going to be the same length as your input sentence, whereas when you're doing something like classification, you know that your output is one or language modeling even kind of you're always just predicting the next word, whereas here you're kind of having this output of variable length. So these are the steps for downloading the data set. We have this available on Fast AI. And then I've already run through these, so the lines that are commented out you only have to do once. So basically we're kind of downloading all these parallel texts of English and French. And then we're going to use our favorite regular expressions to just pick out the questions. And so you can see here, carrot is saying starts with, so basically we want stuff that starts with a wh, and then has a question mark afterwards, and we're picking out kind of from the wh to the question mark, and it can have kind of any sort of characters in between. And that's what we'll be using. And we're doing this just to try to keep our task manageable since translation is a pretty tricky task. We kind of process these, and again you'll need to uncomment and run all this code once, but then because it's kind of slow, definitely save your processed and formatted code so that you can just, or data that you can use it again in the future. Here we're saving it as a CSV, questions easy dot CSV. So I'm going to go ahead to, it's already been processed, and we can take a look at what some of these questions are. What is light? Who are we? Where did we come from? Does anyone in this class speak French? Okay, nobody, me neither. So we'll have to kind of trust some of these kind of make sense that Lumiere would be light. And you can see here how you get the output can be longer than the input, or it can be shorter, that there's a lot of variation. So what we need to do is to collate our inputs and targets into a batch. Because they all have different lengths, we're going to add padding so that we can have this kind of in batches of a fixed length. And so that's just adding zeros to fill out the length. Actually I should have, I guess I did this above. Okay, I'm sorry. Later on we're actually going to throw away sentences that are too long, again, to make this simpler and have kind of an easy batch that we can put on the GPO. So here's our collating method that kind of is just adding the padding and getting these together. Then we're going to create a data bunch. And so these are concepts from the Fast AI library. For methods in the Fast AI library, you can use doc to see the docs. So here a data set class is an abstract class representing a data set. Have you guys talked about abstract classes in the program? No, okay. So this is kind of a Python concept and you're kind of defining kind of these things that we want a data set to have. And in this case it's, we're saying any specific type of data set you create is going to need to have a length as well as a get item, which is basically a way to index into it. So this is what gives you the ability to index into something and we'll subclass it, which means we're kind of making a class that inherits from it. So we'll see this in a moment, but you can have kind of specific types of data sets. Jeremy? Can I? Oh. Oh, sure. Yeah. So are you familiar with DunderLen and DunderGetItem? Okay. I see lots of nodding. And that's good. And yeah, for anyone who needs to review these, these are kind of important Python concepts and that's helpful. You can kind of look over in a lot of Python tutorials. So kind of the basic thing, so a data set is something that's got a length and there's a way to index into it. And we'll use that as part of a data loader. And a data loader is going to give us a way to iterate over a data set. So we want something that we can kind of call and it will return a batch of the data. So this has a batch size, the option of whether you want to shuffle your batches, which just means changing the order. And this is useful kind of with neural networks since we're only looking at a batch of data at a time, you need a way to kind of get out a batch of data at a time. So data loader kind of builds on that. And then data bunch builds on data loader. And this gives you three or two or three data loaders. You want to have a training data loader, a validation one, and optionally a test one. So this is kind of a way to store your training set and your validation set and your test set if you have one all together in one place. And you'll be able to return batches of any of those from this data bunch. So this is a really useful object for kind of keeping these pieces that you need together. And so then seek to seek data bunch will kind of create a type of text data bunch that's suitable for training an RNN. And we do that by adding a class method that is just how you would create your seek to seek data bunch. This uses a sortish sampler. This is from the Fast AI library. Actually, let me get the docs for this is probably better. So in a Jupyter notebook, question mark, question mark will work on any method. Doc is specific to the Fast AI library. This goes through text data by order of length, but with a little bit of randomness. Okay, this is handy. This will take you to the docs on the website. Is it what up a bit? Right here. Is there more you wanted to say about this Jeremy? Okay, yeah. So here then you get the links of, so this inherits from sampler, which is a PyTorch class. Every sampler class or subclass has to provide an iter method. So it's a way to iterate over indices of data set elements and a length. And this is useful to kind of be able to navigate the docs, both for Fast AI as well as for PyTorch or kind of whatever library you're using. Going back here. We're not going to get into seek to seek text lists so much. So yeah, don't get too distracted. I know this is a lot of kind of hierarchy of classes that we're using. For now, be more focused on kind of what we're doing with these. But kind of wanted to show you how you can get more information on them if you're so interested. But for now, it's a kind of focus on what we're going to do with these classes. And yeah, for the microphone, Jeremy just said this is covered a lot in the Fast AI deep learning class if you did want to hear more. So here what I mentioned before, so before we kind of processed our French and English questions, put those into a data frame. Now we'll create a seek to seek text list from that. And the... I haven't been running these, so let me see if this works, if I have stuff loaded in. Yeah, SRC, seek to seek text list has a lot of nice stuff in it, including our training set and our test set. This is a good way to access things. And so then here we just kind of wanted to look at the 90th percentile. So we're using a NumPy method. Basically we wanted to see kind of how long are these items. And we get that 28 is the 90th percentile for X, 23 is the 90th percentile for Y. And so we'll remove items longer than 30 tokens long. Just since not many items fall in that, it's less than 10%. And this will kind of make it simpler for what we're working with. And we're not losing that, we're not throwing away that much data by doing this. Kind of interesting to note that the English sentences, I guess, seem to be longer than the French ones on the whole, since here we got 23 for our Ys, which are the French sentences, the Xs are the English sentences. So we'll go through, filter those out where either X or Y is greater than 30. And then this is kind of our final data bunch that we're creating, our data, and this is what we'll be using. So I wanted to kind of go through how we construct this data set. But again, don't worry too much about the different class types. You can look at our data, it's a seek to seek data bunch, has training. Oh, sorry, I had it backwards. French is the X, English is the Ys. So the French are the longer ones, English is the shorter ones. Here we can see some examples of what is light, who are we, where did we come from. What is the major aboriginal group on Vancouver Island? So different questions that have been answered. What would be the resulting effects on the pre-accession instrument and humanitarian aid that are not co-decided? So there's still like a good variety of types of questions in here. This is, this will be key. So you kind of only need to run this code once and then in the future you can load it from your path where you've saved it. So future we don't have to go through these steps. All right. So now we're ready, we've kind of gotten our data ready, we're ready to create our model. We're going to be using pre-trained word embeddings and I believe you've covered word embeddings with Yanet in the, I guess, machine learning part two course. We're going to use ones from Fast Text, which I believe was from Facebook. And so we'll just download these. This is neat, they have pre-trained word vectors for 157 languages, which can be a useful tool and then here are some links to kind of different tutorials and a workshop if you wanted to review more about word embeddings. We'll need to install Fast Text. Yeah, as you can see here it's from Facebook research. We'll install it. The following lines only have to be run once, which is downloading the English vector word embeddings and the French word embeddings. So we load those in and kind of process them to get our embeddings in the format that we want. We can look at the, and so we'll have an encoder and a decoder. Have you talked about encoders and decoders in any class previously? Okay. I see some maybes or a little bit, so we'll talk about that some more. So an encoder is... Okay, yeah, so I will tell you more about the... Let me see if I have the picture. Okay, yeah, here's the picture. And this is from a great blog post that Stephen Marity wrote and he wrote specifically about Google's neural machine translation. But the idea is kind of... So here he's going from English to German. We'll be going from French to English. That you'll take your embeddings, kind of put them through this encoder and get out a hidden state. And note here kind of all these... This is an RNN and so all the words are going in. He loved to eat, are all going through the encoder and at the end you're kind of getting out this one hidden state that then you'll put through the decoder to get back into... Or not back, but get into German embeddings. Actually I'll come back up. But the encoder is an RNN that we feed our input to. For now we're not worrying about the output. You can imagine outputs coming out of here as well. In addition, kind of getting an output from each state. Here we're just kind of keeping the state at the end. This is a hidden state. This is just a set of activations. Again, thinking of a neural network is just parameters and activations. That hidden state is given to the decoder, which is a second RNN. So we have two RNNs going on here. And that uses it to get the translation. And we keep going until the decoder produces either a padding token or at 30 iterations will stop. Since we've limited our size to 30, we want to avoid an infinite loop or an infinitely long sentence here. So that's what we're creating here is kind of our... Oh, I should say these are our embeddings that will feed into the encoder and these are the embeddings that will be fed into the decoder. In this example, we'll use GRU for our encoder and a second GRU for our decoder. And as I said, we'll come back to GRUs. Although if you're interested, there are some links here. Yeah, so here is an explanation of LSTMs and GRUs if you did want to read up. All right, so this... All right, so here for our seek to seek RNN, I'll come back to... So we're kind of initializing a number of things. I want to talk about what the forward step does first though, since I think that'll be the clearest. The forward step is going to put the batch size and the input into an encoder and get out a hidden state. Our decoder input, we kind of need to initialize to be the right size and to start with beginning of string. And then we'll go through a for loop where in each step we're putting our decoder input and our hidden state, which we got from the encoder just right here. We're putting those into the decoder and we're getting out the hidden state and an output. We'll take the max of that to get the decoder input. We want to hold on to kind of what our output was from the decoder. Then if we have a padding token, we'll break out of this and at the end we'll kind of return our results. So the kind of high level what's going on is you're putting your input into an encoder and then in a for loop you are getting something out of the decoder each time. Then kind of digging into what are the encoder and the decoder doing, the encoder is taking the embedding that we had prepared for the encoder. So when we put the input into the embedding that's basically taking the French words and then picking out what were the vectors, word embeddings for those is what you get here. And then we're applying dropout to that. Since we talked about dropout, it's important for regularizing. Then we put that into our first GRU since this uses two different GRUs. This is the one we're learning for the encoder. We'll want the hidden state of what came before as well as the embedding. And then out encoder, so you can look up here, that's just a linear layer. So we'll put that through a linear layer and then that's kind of returning our final hidden state. Any questions about what the encoder does? I think this can feel tricky, like I know this is a really long definition for our seek to seek class. It's helpful to keep in mind that it's kind of mostly composed of things that you know other than GRU, which we'll cover next time, but it's looking things up in the set of word embeddings that we've got and then it's got dropout, it's linear layers, some nonlinearities. Even though it's got many pieces, you know what most of these individual pieces are. And then for the decoder, we get the decoder input. Does anyone know what unsqueeze is? Do you want to say it louder? Increase the dimension? Yes, I believe that adds a dimension, where a squeeze kind of reduces a dimension. And then we put our embedding and the hidden state through our GRU decoder. This is our second GRU. Apply some dropout. Then apply self.out, which is another linear layer and get our output from there. So we'll be returning the hidden state and the output and we're going to kind of keep track of the outputs and then the hidden state will be used again the next time we go through our for loop since we're holding on to our history here. Here you can kind of look at our RNN. So we've created the seek to seek RNN. We have to give it the embeddings that we're using for our encoder. That's just our French embeddings. Give it our English embeddings at 256 is the number of hidden layers and 30 is the length, sequence length. And we can see, because this is another way to visualize what the RNN looks like. It's an embedding dropout, the first GRU linear layer, another embedding GRU dropout, another linear layer. That are kind of the pieces of it. And it's helpful. It can be helpful to kind of look at the sizes of things. Here the hidden state size is 2 by 64 by 300. So the two corresponds to, we have two hidden layers inside the GRU. You can see that kind of up here. Two hidden layers. 64 is our batch size and 300 is the dimension of the embeddings. All right. And then this is, let me check. Yeah, it's 1205. So let's stop for a break. We'll meet back at 1210 and keep going with this. All right, let's start back up. So just during the break, Jeremy suggested I show you a new GitHub feature that's just been recently released. And so first we can look at the docs for data bunch. And so that'll pull up a link if we want to go to the source code. Actually, let me open that in a separate window. So now we're on GitHub. Actually, let me make that clear. You can see this is GitHub on the source code. And now if you run your arrow over things, it'll highlight some of the methods that are defined elsewhere. So you can click on this and it shows us, oh, this is where listify was defined. We're not going to look at the old version. Let's look at the current version. You can click on it and it takes us to the source code. And this only works within a repository. It's not going to link you to other repositories, but it makes it easier to kind of navigate if you want to see, hey, what's the definition of such and such? So that's neat to know about and not specific to fast AI, but just a nice feature on GitHub. So returning to our task. So we'll need to pad our loss so that it's the right shape. And now we're ready to train our model. And so here loss is defined as... Well, sorry. We're using a cross entropy loss. We're padding the lengths. So we can run these, get a plot of the learning rate finder. Let's try 1e-2. Again, we're kind of choosing a point where the loss is relatively low, but still decreasing. And so this will show us the training loss and validation loss. And this is not very meaningful to me. It's nice to see that they're both decreasing, but in terms of human interpretability, we don't really know how good is our model based on these losses. We don't have a meaning for that. Oh, well, also we can delete our French vectors and English vectors to free up some memory. So in addition to looking at the loss, we can also look at the accuracy. And so this is what percentage of the time are we getting the right word out? However, measuring the... So accuracy is this technical definition, but measuring how good a translation is, is a little bit trickier. So something might not have the exact same words, but could still be a reasonable translation. And so one of the metrics that's been designed to deal with this is blue. And so I'm going to link now to Rachel Tatman, who has a PhD in computational linguistics and works for Kaggle, has a really fantastic post on evaluating text output in NLP. And so I want to go through a little bit of this. So here she highlights some general sequence to sequence tasks in NLP. So that also includes text summarization, text simplification, and actually let me go back up to the top. Just so you see this, this is evaluating text output in NLP blue at your own risk, because there are upsides and downsides to using blue, and it's sometimes used inappropriately. And there is some... It's definitely an area of active research. And Rachel says later in the post that she thinks the question of even evaluating how good your seek to seek NLP model is, is one of the very hard open problems in NLP. And she'll give some examples that I want to show. So here are some other seek to seek tasks. And she highlights there's no simple answer about what metrics you should use. And blue has some major drawbacks. But first let's talk about what blue does. So she gives the example, actually she's also going from French to English, of this sentence and suggests, I have eaten three hazelnuts or I ate three filberts. So filberts is a synonym used, I think, in the UK for hazelnut. So you can already see, like here are two different ways of translating the same sentence, both we've got this difference between hazelnuts and filberts. Also I have eaten and I ate, you're not always going to get a clear mapping of, you know, there are multiple ways to use a verb tense in the past, present or future. And so she gives the example of, okay, I ate three hazelnuts, how good a translation is this? And so this does not match either of these exactly, but it's actually, we know it's a very good translation given this, because it actually combines the I ate three and then uses hazelnuts for filberts. And so she brings up kind of this core problem is, how can I assign a single numerical score to this translation that says how good it is? And notice also we already have this issue of length too. You know, here we've got two correct answers. One is five words long and one is four words long. And so I'm going to kind of skip ahead to blue. First she notes that you don't just want to look at the presence of words, like the order matters as well, because eight hazelnuts, I three, that's not correct English at all. And so that's not a very good translation, even though it's got all the right words in it, but they're jumbled up. And so the solution that blue uses is to look at n grams. And so it'll look at one grams, which are just capturing, do you have the right words in there? But also it will look at two grams, three grams and four grams. It's important to note, because there are examples where, let me think of one, like I ate a sandwich since I'm hungry. You could also say, since I'm hungry, I ate a sandwich. And there I've changed the order, but that's still correct grammar. Those two sentences are saying the same thing. So n grams don't perfectly capture that, but they do. In that example, the phrases, I ate a sandwich and since I'm hungry, those n grams will be correct, even though you can switch the order. So blue typically uses the unigram, bigram, trigram and four grams. And so they'll kind of say, how many correct unigrams do you have out of the total number? How many correct bigrams out of the total number? And so on. And then a limitation of this, so we're kind of building up to blue. It has these components. You need to keep in mind that, so I ate all those, I mean, those unigrams are correct. There's only a single bigram here, but it's correct. You could say, oh, this has got, you know, all the bigrams in here are in the original or in the correct translation. But this is not a great translation because we left out three hazelnuts, which was a key part of the sentence. And so blue assigns what's called a brevity penalty, and basically if your translation is shorter than the correct translation, it says that's wrong because you've probably left out some information that you needed, even if all the stuff you had was correct. Going back to my, I ate a sandwich, that sentence is grammatically correct. It was in, is in the full one, but I've left out the since I'm hungry. So that's the brevity penalty. So now I'm going to switch over. So this, I really like this post and I recommend reading the whole thing. I made a few slides about it. Oops. Sorry. So later in the post, Rachel highlights the strengths of blue are it's fast and easy to calculate, particularly compared to having human translators. And so that was kind of historically has been a technique and is an ongoing technique is you can always get people and pay them to label your sentences of, you know, find bilingual language speakers who can say like, this is a good translation or it's not. And blue is also very commonly used. So it makes it easy to compare to benchmarks. So those are, those are two, two important strengths. However, there are some weaknesses. And so one is that it doesn't consider meaning. Rachel and her post gives the example. Suppose you have a sentence. I think she has the French sentence that translates to I ate the apple. Here's some wrong answers that all would have the same blue score. I ate an apple. And so, I mean, this is a pretty good translation, but it's got an instead of the, I consumed the apple, which is also pretty good. It's just consumed is another word for, for eight. I ate the potato. This is not a good translation. The meaning of this is different, right? Like the first two are, you can even say great translations. The third one, it's not yet. These all get the same blue score, right? And you can also look at them. They're all basically off by one word from the correct translation, but it's actually significant which word they're off by and whether the, the thing that's given means the same thing. And so that's, that's one, one limitation of blue. Now there is, it doesn't, it doesn't directly consider sentence structure. So you can get stuff that's, that's ungrammatical. Doesn't handle morphologically rich languages well. Does anyone remember what morphologically rich languages or what an example of one is? Turkish. Yeah, that's right. So we saw Turkish last time and that basically means morphemes are kind of these little pieces of words, Turkish, you have a lot of morphemes that can be put together in words and they have meaning and blue doesn't capture that, right? We don't have a way of looking at kind of those pieces and assigning, assigning them scores. It doesn't necessarily correspond with human judgments either. So these are, these are some limitations of it. I like, so Rachel's kind of recommendation in the end as someone who has studied this a lot is if you're putting your system into production, she thinks you should do at least one round of evaluation with human experts, which I think makes a lot of sense in terms of there's not, right now there's not a perfect metric that captures all the things that we as humans think about with, with language and sentence. But you are, you're still going to want an automatic metric to use kind of as your training and before you get to that point because human evaluation is more expensive. And so she says to use blue if and only if you're doing machine translation. I think the issue is people sometimes apply it to other seek to seek tasks, which is not what it was developed for. You're evaluating across an entire corpus and you're familiar with the limitations. You know, this is not a perfect metric. And then one other thing I wanted to highlight from her blog posts, and this is more just if you're really interested in this, is she, let me find it. She covered, and so she goes into a lot, a lot more detail than I've covered. So she covers kind of what the literature says about both the limitations of, of blue. So papers on kind of times when you, you don't want to use blue. A lot of resources on that. What are alternatives? So there are people kind of working on this question of what are other options to blue and including so there are measures from other areas of NLP like perplexity or F score or word error rate that you could use. And then here, this is, this is an area of active research of people are developing other metrics to use as alternatives to blue. And so this is more just if you're interested in the topic, I thought it was an interesting topic. And I'll say this question of how, how to evaluate the goodness of your model, like whether that's an NLP or in another area is important to think about because metrics, I would say metrics are always just a proxy for what you truly care about. And it can be dangerous to think that the metric is capturing everything. So returning to this, this is kind of how we're implementing blue. And let me find the key line. I think here is basically looking at multiplying together four perc... or taking the average of these four precisions and these correspond to the unigrams, bigrams, trigrams and four grams that we've captured averaging those and then multiplying by the brevity penalty. And just to review, can anyone remind me why we need a brevity penalty? Yeah, so the answer was in case it's too short. So we saw the example with I8-3 hazelnuts, I8, all the unigrams in there match, all the bigrams in there match, but it's short, it's left out information and we want to penalize that. So that's how, that's how blue deals with that. Any other questions about blue? All right, I want to highlight, there's a... So this is probably going to be in a weird place. There's another notebook that I've added to the repo, Blue Metrics, that Sylvain created. And this is also a nice notebook. He walks through an example of the cat is walking in the garden, how good is the cat is running in the fields as a translation. I'm sorry, this is the correct one and this is what your model gives, evaluating that. And he kind of walks through, okay, we have three correct bigrams, the cat is, and in the looking at these shows how to calculate the blue length penalty. And so that's helpful too. And so the reason we've implemented our own is that NLTK uses list of tokenized text, so it's much slower. So we wanted something quicker since we have already numericalized the text and have our numericalized answers. So check out this notebook for more detail about how this works. All right. So this will be nice. So we're going to add to our learner for metrics, both the accuracy and the blue score to get more information. So kind of jumping back. The problem that had motivated us is here we're training, we can see that the loss is decreasing on the training and validation sets, but this wasn't very human interpretable. And so we talked about kind of two ways we're going to make this more human interpretable is adding the accuracy and then adding the blue score. So going back down, we can run this again. And here these are entered as a list of metrics into our learner. We can run this again and our losses should be similar, but now we can see the accuracy by word as well as the blue score. And then we can also test this out and see what are the predictions on particular pieces of data, which is always useful to do. Hold on a moment. And so here we'll append onto our inputs, targets, and outputs to get the predictions. We can see this is not a very good one. The actual answer is what are the short and long term expected outcomes of the alley and to what extent have they been achieved and we get what were the results, the comma, comma, comma, and, and, and, which is not a great answer. It started out okay with what were the results, but a lot is missing. Another one we can look at which of the following additional information is necessary to estimate the company's actual profit for the coming year. What is the V22VV parentheses, parentheses? Another, sorry, I like this one too. What experience and specific capacities do the implementing organizations bring to the project? What are the key and, and, and, and, of, of, of? So this is, our answers are beginning pretty well, but then they're really falling apart as we go on and typically ending up with kind of these repeated words. And before I talk about how we address that, I'm going to ask if there are questions on this part. So what we've done so far is kind of just construct this. I say basic, I mean, seek to seek is a harder task, but for a seek to seek model, it's on the basic side in that it's, it's just using two, two GRUs, which are a type of RNN. It has, actually I can put the structure here again. Did we call it? Yeah. So here's our seek to seek RNN. So we can see, you know, it's embedding, dropout, GRU, linear layer, embedding, GRU, dropout, linear layer is what we're doing. And so we will, we will improve upon this. But questions about what we've done so far? Okay. So, to kind of address this issue of starting well, but then ending up with repeated words that aren't very good, part of the issue is once the, kind of once the RNN goes astray, it doesn't really have a chance to get back on track because the input is the previous words. And so if we give it, you know, what are the key and, and, and, and it's, you know, it's looking at those previous words, like, how is it going to get back on track? Right? It's got its own, its own incorrect output becomes input for the next state, which is the problem. And so the technique to deal with this is called teacher forcing. And so this is, we're going to help out the decoder and instead of giving it its own predictions, which may not be very good, particularly when you first start training the model, instead we'll give it the correct answers. And so if it makes a wrong prediction, we'll still give it the correct answer for its next prediction because we basically don't want, you know, a few wrong predictions then to just like totally screw it up. We'll do this all the time at the beginning. However, as we go on, we want to do it less and less. The one thing about teacher forcing is it, it slightly incentivizes the model to take more risk because if it gets something wrong, you know, it'll still be fine the next time since you're going to show it the answer before its next prediction. So basically we'll gradually reduce the amount of teacher forcing, but at the beginning we want to do a lot because the model, it's not going to be very good. Like we're just starting to train it. Of course it's going to get stuff wrong and we don't want that to just screw it up so that it can't learn. And so what this does here is we'll have a variable called the probability of forcing and that's going to start high, it'll start at one and then gradually decrease until kind of by the end of the epoch, we want the probability of forcing to be zero since, you know, in the real world we're not going to have the answer every time. This is just as we're training it, a way to help it, but we gradually reduce the help as it trains. So we're going to add this code to our forward method and basically we're going to draw a random number between zero and one. If that's less than the probability of forcing, then we'll set, we also need to make sure we're not done, like if we're at the end of our word we don't need to worry about this. We'll set the decoder input to be the target word. So our input for the next time, we've set it to be the correct answer. And so this initially, if PR force is one, it's kind of always going to enter this loop. As it decreases though and by the end PR force will be zero and it'll never go into this loop. And so this is the only change we've made from before. I call this class seek to seek RNN underscore TF for teacher forcing just to let you know that's the one change we've made. And so encoder is the same as before, decoder is the same as before, but you see down here we've added this if statement. Then we can run this. And now our accuracy is getting up to 40%. And just to compare, let me go back up and see what it was before. So up here we were getting 37%. Blue score of 0.28. So here accuracy 40% and blue score of 0.31. So it's improved some. We can also look at the results, look at a few examples and see who has the authority to change the electricity and gas inspection regulations and the weights and measures regulations. So this is still doing some of that behavior from before. We see less repeated words happening. For instance, this one, where will national marine conservation areas of Canada be located? Where are the Canadian regulations located in the Canadian? It's not a great translation, but it is an improvement over and, and, and, comma, comma, comma. Any other questions about teacher forcing? All right. So this is, we're going to continue to, to build on this model and continue improving it. So we've seen kind of starting with our basic seek to seek. We added teacher forcing and then I broke a, broke the next example into a second notebook. And this was mostly, mostly just to make it clearer which lines you need to rerun if you're starting from the beginning. So in this notebook, we're not going to go through all the pre-processing, but are just going to load, load everything in. So this, this whole section, the code to rerun from start, that's all duplicate from the previous notebook, but it loads everything in that you've saved. So run notebook seven before, before you do seven B. So to improve seek to seek translation, we're going to add attention and intention. Attention is a really important concept. It's also kind of a key part of transformers. So that'll be very relevant even as we go on to, to transformers next week. And basically, so we have two things we're going to do. Before we were kind of only keeping the final hidden state from the encoder. Now we want to keep the output at every state of the encoder. And we also want to know which to focus on. Jay Alomar has written a really nice blog post on this. And this is on, on transformer, but he covers, covers attention in it. Actually I should look at this post on attention. Okay. I'll, I'll come back to this. But so this, this picture is from, from his blog post and he's using a tensor to tensor notebook, which you can check out on Google CoLab. And here in this example, they're looking at a self encode, an auto encoder. And so it's encoding kind of this sentence. And if you have the animal didn't cross the street because it was too tired. When you're working with it, it's really important that you know it is related to the animal, right? Kind of reading. We wouldn't want to think it is the street, right? And this is something that, you know, we as humans recognize the animal didn't cross the street because it was too tired. We know what it's referring to. And so the idea is kind of in your output, you want to know what to focus on from the input. And so we're going to use weights or rather we're going to learn weights to kind of learn that information of what we should be focusing on. And so basically we're going to end up taking a weighted average of the output from the encoder at each step to kind of know what were the key steps of the encoder for the particular word I'm on in the output. You know, and this will be different for each word of the output, what you want to focus on from the input. So that's the key change we're making here. Kind of a second change we'll make to improve our model is to make it bidirectional. And basically this just doubles the number of inputs to the output layer of the encoder. Basically we're going, kind of doing one version that's going forwards, one version that's going backwards to get kind of double this information. And so that'll show up here. So what this looks like in practice is, let me just get my drink. Both our encoder and our decoder are going to have to change some. So encoder starts off kind of the way it was before. Here the only difference is we have two times the number of hidden states since we've added this bidirectional component. Here we're just, this is just changing the order of it. To put, I always get these backwards. I think we want the number of layers, the batch size, and then two, and then the number of hidden layers, or number of hidden states. So here we're still calling the self.out encode. So basically these two lines with the hid.view, that's basically just kind of formatting how we're dealing with the fact that we now have this extra dimension since we've doubled it for the bidirectional aspect. So that's not specific to attention. This is relating to the fact that we have switched it to be bidirectional. The decoder is where we're going to deal with attention. And so we need, so we've already put kind of the encoder out and hid through linear layers. Here what we want to do is combine kind of the attention on the encoder and the attention on the hidden layers into you. And that's the information that we have about kind of each time step is the encoder for that time step and the, sorry, the result of the encoder for that time step and the hidden state for that time step. And then we're going to learn attention weights. And so we're multiplying by V, this gives us the actual weights, V is what we're learning and this is just initialized to be a matrix. And then CTX stands for context. And so we're multiplying the attention weights by the output from the encoder to get the context, but that's just a weighted average. So we have the output from the encoder at each step and we're taking a weighted average of that to know what are the important steps. So going back to this, the animal crossed the street because it was tired. We want to know that it is referring to the animal. We would put more, or hopefully the network would learn to put more weight on those steps. So this is kind of where the piece with attention is happening. And then down here into our GRU, we're going to feed both the embedding and the context. And again, the context is just the weighted average of these hidden layers having learned kind of what's important. So you can see this in the forward, sorry, yeah, in forward. It's kind of very similar to before. We do have to kind of initialize this encoder attention, which is just a linear layer up here. Hidden attention is also a linear layer. So we called a linear layer inside our for loop. We're still calling decoder the same way we did before. We're keeping track of all our outputs. We're still going to use teacher forcing. So this is kind of a, today we've been incrementally improving our model kind of by adding more to it. It's helpful to look at what the size of all the pieces are. So here hidden is two by 64 by 300. You'll remember 64 is the batch size. The two is since it's bidirectional and the 300 is the embedding size that we were using. For attention on the encoder, 64 for the batch size. 30 is the length of our sequence. And that's kind of a key dimension of wanting to know which of the 30 is most important as you go through it. And then 300 is the size, again, of the embeddings. Accuracy in weights, again, this is 64 by 30 since we're taking the weights at the different of the 30 time steps. Yeah. And so if we call all this and then train it, the accuracy is now 49%, almost 50%. And the blue score is up to 0.41. Let me just review, go back to what it was previously. So last time the accuracy was just 41%. We've gone up to 50%. That's a nice improvement. And the blue score has gone from 0.31 to, what did I say, 0.4? And higher blue scores are better, 0.41. So it's improved. All right, so we're at time since I need to head to the airport. I'll go over this again next time. So next time we will talk about attention in more detail. We'll also kind of depend it on time, either next time or the time after that, get into kind of the details of what GRUs are doing. But for now I wanted you to see kind of these incremental improvements on how we can make our seek to seek model better. Cool. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.7, "text": " Alright, so next we're going to be getting into translation as our next task.", "tokens": [2798, 11, 370, 958, 321, 434, 516, 281, 312, 1242, 666, 12853, 382, 527, 958, 5633, 13], "temperature": 0.0, "avg_logprob": -0.18892152574327256, "compression_ratio": 1.7511961722488039, "no_speech_prob": 0.03960170969367027}, {"id": 1, "seek": 0, "start": 7.7, "end": 14.040000000000001, "text": " And so this chart will kind of show the spoiler alert of where we're going, but we're going", "tokens": [400, 370, 341, 6927, 486, 733, 295, 855, 264, 26927, 9615, 295, 689, 321, 434, 516, 11, 457, 321, 434, 516], "temperature": 0.0, "avg_logprob": -0.18892152574327256, "compression_ratio": 1.7511961722488039, "no_speech_prob": 0.03960170969367027}, {"id": 2, "seek": 0, "start": 14.040000000000001, "end": 18.72, "text": " to approach translation through kind of a variety of ways.", "tokens": [281, 3109, 12853, 807, 733, 295, 257, 5673, 295, 2098, 13], "temperature": 0.0, "avg_logprob": -0.18892152574327256, "compression_ratio": 1.7511961722488039, "no_speech_prob": 0.03960170969367027}, {"id": 3, "seek": 0, "start": 18.72, "end": 24.54, "text": " We're going to start with a more basic sequence to sequence model.", "tokens": [492, 434, 516, 281, 722, 365, 257, 544, 3875, 8310, 281, 8310, 2316, 13], "temperature": 0.0, "avg_logprob": -0.18892152574327256, "compression_ratio": 1.7511961722488039, "no_speech_prob": 0.03960170969367027}, {"id": 4, "seek": 0, "start": 24.54, "end": 28.16, "text": " And over here this column is showing the accuracy we'll get with that.", "tokens": [400, 670, 510, 341, 7738, 307, 4099, 264, 14170, 321, 603, 483, 365, 300, 13], "temperature": 0.0, "avg_logprob": -0.18892152574327256, "compression_ratio": 1.7511961722488039, "no_speech_prob": 0.03960170969367027}, {"id": 5, "seek": 2816, "start": 28.16, "end": 33.28, "text": " We'll improve it by adding teacher forcing to it, and we'll improve it even more by adding", "tokens": [492, 603, 3470, 309, 538, 5127, 5027, 19030, 281, 309, 11, 293, 321, 603, 3470, 309, 754, 544, 538, 5127], "temperature": 0.0, "avg_logprob": -0.1802502013089364, "compression_ratio": 1.7348484848484849, "no_speech_prob": 0.0001313375250902027}, {"id": 6, "seek": 2816, "start": 33.28, "end": 35.08, "text": " attention.", "tokens": [3202, 13], "temperature": 0.0, "avg_logprob": -0.1802502013089364, "compression_ratio": 1.7348484848484849, "no_speech_prob": 0.0001313375250902027}, {"id": 7, "seek": 2816, "start": 35.08, "end": 39.88, "text": " And then in the next lesson we will cover transformers.", "tokens": [400, 550, 294, 264, 958, 6898, 321, 486, 2060, 4088, 433, 13], "temperature": 0.0, "avg_logprob": -0.1802502013089364, "compression_ratio": 1.7348484848484849, "no_speech_prob": 0.0001313375250902027}, {"id": 8, "seek": 2816, "start": 39.88, "end": 44.6, "text": " So we probably won't get to this until later next week, and that'll give us even much higher", "tokens": [407, 321, 1391, 1582, 380, 483, 281, 341, 1826, 1780, 958, 1243, 11, 293, 300, 603, 976, 505, 754, 709, 2946], "temperature": 0.0, "avg_logprob": -0.1802502013089364, "compression_ratio": 1.7348484848484849, "no_speech_prob": 0.0001313375250902027}, {"id": 9, "seek": 2816, "start": 44.6, "end": 45.6, "text": " accuracy.", "tokens": [14170, 13], "temperature": 0.0, "avg_logprob": -0.1802502013089364, "compression_ratio": 1.7348484848484849, "no_speech_prob": 0.0001313375250902027}, {"id": 10, "seek": 2816, "start": 45.6, "end": 49.28, "text": " But it's important to kind of understand some of these components.", "tokens": [583, 309, 311, 1021, 281, 733, 295, 1223, 512, 295, 613, 6677, 13], "temperature": 0.0, "avg_logprob": -0.1802502013089364, "compression_ratio": 1.7348484848484849, "no_speech_prob": 0.0001313375250902027}, {"id": 11, "seek": 2816, "start": 49.28, "end": 52.72, "text": " Teacher forcing is a very, very useful technique.", "tokens": [19745, 19030, 307, 257, 588, 11, 588, 4420, 6532, 13], "temperature": 0.0, "avg_logprob": -0.1802502013089364, "compression_ratio": 1.7348484848484849, "no_speech_prob": 0.0001313375250902027}, {"id": 12, "seek": 2816, "start": 52.72, "end": 56.96, "text": " Attention is what's used in transformers, so understanding that will be helpful.", "tokens": [31858, 307, 437, 311, 1143, 294, 4088, 433, 11, 370, 3701, 300, 486, 312, 4961, 13], "temperature": 0.0, "avg_logprob": -0.1802502013089364, "compression_ratio": 1.7348484848484849, "no_speech_prob": 0.0001313375250902027}, {"id": 13, "seek": 5696, "start": 56.96, "end": 61.96, "text": " It will kind of be building up better and better accuracy by adding more to our seek", "tokens": [467, 486, 733, 295, 312, 2390, 493, 1101, 293, 1101, 14170, 538, 5127, 544, 281, 527, 8075], "temperature": 0.0, "avg_logprob": -0.17930508685368363, "compression_ratio": 1.5991379310344827, "no_speech_prob": 2.9309734600246884e-05}, {"id": 14, "seek": 5696, "start": 61.96, "end": 67.44, "text": " to seek model and then switching to a transformer model.", "tokens": [281, 8075, 2316, 293, 550, 16493, 281, 257, 31782, 2316, 13], "temperature": 0.0, "avg_logprob": -0.17930508685368363, "compression_ratio": 1.5991379310344827, "no_speech_prob": 2.9309734600246884e-05}, {"id": 15, "seek": 5696, "start": 67.44, "end": 73.76, "text": " And so this work I should say is based off of notebooks created by Sylvain Gugger, our", "tokens": [400, 370, 341, 589, 286, 820, 584, 307, 2361, 766, 295, 43782, 2942, 538, 3902, 14574, 491, 460, 697, 1321, 11, 527], "temperature": 0.0, "avg_logprob": -0.17930508685368363, "compression_ratio": 1.5991379310344827, "no_speech_prob": 2.9309734600246884e-05}, {"id": 16, "seek": 5696, "start": 73.76, "end": 78.08, "text": " kind of researcher, one of our researchers at Fast AI.", "tokens": [733, 295, 21751, 11, 472, 295, 527, 10309, 412, 15968, 7318, 13], "temperature": 0.0, "avg_logprob": -0.17930508685368363, "compression_ratio": 1.5991379310344827, "no_speech_prob": 2.9309734600246884e-05}, {"id": 17, "seek": 5696, "start": 78.08, "end": 82.92, "text": " We'll be looking at the task of translating from French to English, and we're trying to", "tokens": [492, 603, 312, 1237, 412, 264, 5633, 295, 35030, 490, 5522, 281, 3669, 11, 293, 321, 434, 1382, 281], "temperature": 0.0, "avg_logprob": -0.17930508685368363, "compression_ratio": 1.5991379310344827, "no_speech_prob": 2.9309734600246884e-05}, {"id": 18, "seek": 8292, "start": 82.92, "end": 89.0, "text": " keep this as a kind of manageable size task, so we're going to limit ourselves to translating", "tokens": [1066, 341, 382, 257, 733, 295, 38798, 2744, 5633, 11, 370, 321, 434, 516, 281, 4948, 4175, 281, 35030], "temperature": 0.0, "avg_logprob": -0.1419491206898409, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.0001659872505115345}, {"id": 19, "seek": 8292, "start": 89.0, "end": 90.0, "text": " questions.", "tokens": [1651, 13], "temperature": 0.0, "avg_logprob": -0.1419491206898409, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.0001659872505115345}, {"id": 20, "seek": 8292, "start": 90.0, "end": 94.88, "text": " There's a great data set that was put together by Chris Callison Birch.", "tokens": [821, 311, 257, 869, 1412, 992, 300, 390, 829, 1214, 538, 6688, 7807, 2770, 7145, 339, 13], "temperature": 0.0, "avg_logprob": -0.1419491206898409, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.0001659872505115345}, {"id": 21, "seek": 8292, "start": 94.88, "end": 99.12, "text": " And this, I should say this question of even creating data sets is a really important one", "tokens": [400, 341, 11, 286, 820, 584, 341, 1168, 295, 754, 4084, 1412, 6352, 307, 257, 534, 1021, 472], "temperature": 0.0, "avg_logprob": -0.1419491206898409, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.0001659872505115345}, {"id": 22, "seek": 8292, "start": 99.12, "end": 102.4, "text": " of just kind of how do you create the data sets you need.", "tokens": [295, 445, 733, 295, 577, 360, 291, 1884, 264, 1412, 6352, 291, 643, 13], "temperature": 0.0, "avg_logprob": -0.1419491206898409, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.0001659872505115345}, {"id": 23, "seek": 8292, "start": 102.4, "end": 108.64, "text": " And he used that in Canada, websites have to have a French version and an English version", "tokens": [400, 415, 1143, 300, 294, 6309, 11, 12891, 362, 281, 362, 257, 5522, 3037, 293, 364, 3669, 3037], "temperature": 0.0, "avg_logprob": -0.1419491206898409, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.0001659872505115345}, {"id": 24, "seek": 10864, "start": 108.64, "end": 113.44, "text": " since they speak both languages, and so that gives a nice kind of nice set of parallel", "tokens": [1670, 436, 1710, 1293, 8650, 11, 293, 370, 300, 2709, 257, 1481, 733, 295, 1481, 992, 295, 8952], "temperature": 0.0, "avg_logprob": -0.15403674996417502, "compression_ratio": 1.646551724137931, "no_speech_prob": 2.9765584258711897e-05}, {"id": 25, "seek": 10864, "start": 113.44, "end": 116.24, "text": " text.", "tokens": [2487, 13], "temperature": 0.0, "avg_logprob": -0.15403674996417502, "compression_ratio": 1.646551724137931, "no_speech_prob": 2.9765584258711897e-05}, {"id": 26, "seek": 10864, "start": 116.24, "end": 121.8, "text": " So he kind of crawled millions of web pages to create this.", "tokens": [407, 415, 733, 295, 13999, 1493, 6803, 295, 3670, 7183, 281, 1884, 341, 13], "temperature": 0.0, "avg_logprob": -0.15403674996417502, "compression_ratio": 1.646551724137931, "no_speech_prob": 2.9765584258711897e-05}, {"id": 27, "seek": 10864, "start": 121.8, "end": 127.44, "text": " And I should note that seek to seek can be more challenging than classification since", "tokens": [400, 286, 820, 3637, 300, 8075, 281, 8075, 393, 312, 544, 7595, 813, 21538, 1670], "temperature": 0.0, "avg_logprob": -0.15403674996417502, "compression_ratio": 1.646551724137931, "no_speech_prob": 2.9765584258711897e-05}, {"id": 28, "seek": 10864, "start": 127.44, "end": 131.04, "text": " the input or the output is a variable length, right?", "tokens": [264, 4846, 420, 264, 5598, 307, 257, 7006, 4641, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.15403674996417502, "compression_ratio": 1.646551724137931, "no_speech_prob": 2.9765584258711897e-05}, {"id": 29, "seek": 10864, "start": 131.04, "end": 134.76, "text": " The translation of a sentence is not necessarily going to be the same length as your input", "tokens": [440, 12853, 295, 257, 8174, 307, 406, 4725, 516, 281, 312, 264, 912, 4641, 382, 428, 4846], "temperature": 0.0, "avg_logprob": -0.15403674996417502, "compression_ratio": 1.646551724137931, "no_speech_prob": 2.9765584258711897e-05}, {"id": 30, "seek": 13476, "start": 134.76, "end": 141.51999999999998, "text": " sentence, whereas when you're doing something like classification, you know that your output", "tokens": [8174, 11, 9735, 562, 291, 434, 884, 746, 411, 21538, 11, 291, 458, 300, 428, 5598], "temperature": 0.0, "avg_logprob": -0.13311570650571353, "compression_ratio": 1.608695652173913, "no_speech_prob": 5.473877536132932e-05}, {"id": 31, "seek": 13476, "start": 141.51999999999998, "end": 146.39999999999998, "text": " is one or language modeling even kind of you're always just predicting the next word, whereas", "tokens": [307, 472, 420, 2856, 15983, 754, 733, 295, 291, 434, 1009, 445, 32884, 264, 958, 1349, 11, 9735], "temperature": 0.0, "avg_logprob": -0.13311570650571353, "compression_ratio": 1.608695652173913, "no_speech_prob": 5.473877536132932e-05}, {"id": 32, "seek": 13476, "start": 146.39999999999998, "end": 152.56, "text": " here you're kind of having this output of variable length.", "tokens": [510, 291, 434, 733, 295, 1419, 341, 5598, 295, 7006, 4641, 13], "temperature": 0.0, "avg_logprob": -0.13311570650571353, "compression_ratio": 1.608695652173913, "no_speech_prob": 5.473877536132932e-05}, {"id": 33, "seek": 13476, "start": 152.56, "end": 156.84, "text": " So these are the steps for downloading the data set.", "tokens": [407, 613, 366, 264, 4439, 337, 32529, 264, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.13311570650571353, "compression_ratio": 1.608695652173913, "no_speech_prob": 5.473877536132932e-05}, {"id": 34, "seek": 13476, "start": 156.84, "end": 162.12, "text": " We have this available on Fast AI.", "tokens": [492, 362, 341, 2435, 322, 15968, 7318, 13], "temperature": 0.0, "avg_logprob": -0.13311570650571353, "compression_ratio": 1.608695652173913, "no_speech_prob": 5.473877536132932e-05}, {"id": 35, "seek": 16212, "start": 162.12, "end": 167.08, "text": " And then I've already run through these, so the lines that are commented out you only", "tokens": [400, 550, 286, 600, 1217, 1190, 807, 613, 11, 370, 264, 3876, 300, 366, 26940, 484, 291, 787], "temperature": 0.0, "avg_logprob": -0.16091485456986862, "compression_ratio": 1.5769230769230769, "no_speech_prob": 2.7966158086201176e-05}, {"id": 36, "seek": 16212, "start": 167.08, "end": 169.0, "text": " have to do once.", "tokens": [362, 281, 360, 1564, 13], "temperature": 0.0, "avg_logprob": -0.16091485456986862, "compression_ratio": 1.5769230769230769, "no_speech_prob": 2.7966158086201176e-05}, {"id": 37, "seek": 16212, "start": 169.0, "end": 174.68, "text": " So basically we're kind of downloading all these parallel texts of English and French.", "tokens": [407, 1936, 321, 434, 733, 295, 32529, 439, 613, 8952, 15765, 295, 3669, 293, 5522, 13], "temperature": 0.0, "avg_logprob": -0.16091485456986862, "compression_ratio": 1.5769230769230769, "no_speech_prob": 2.7966158086201176e-05}, {"id": 38, "seek": 16212, "start": 174.68, "end": 180.72, "text": " And then we're going to use our favorite regular expressions to just pick out the questions.", "tokens": [400, 550, 321, 434, 516, 281, 764, 527, 2954, 3890, 15277, 281, 445, 1888, 484, 264, 1651, 13], "temperature": 0.0, "avg_logprob": -0.16091485456986862, "compression_ratio": 1.5769230769230769, "no_speech_prob": 2.7966158086201176e-05}, {"id": 39, "seek": 16212, "start": 180.72, "end": 185.9, "text": " And so you can see here, carrot is saying starts with, so basically we want stuff that", "tokens": [400, 370, 291, 393, 536, 510, 11, 22767, 307, 1566, 3719, 365, 11, 370, 1936, 321, 528, 1507, 300], "temperature": 0.0, "avg_logprob": -0.16091485456986862, "compression_ratio": 1.5769230769230769, "no_speech_prob": 2.7966158086201176e-05}, {"id": 40, "seek": 18590, "start": 185.9, "end": 195.48000000000002, "text": " starts with a wh, and then has a question mark afterwards, and we're picking out kind", "tokens": [3719, 365, 257, 315, 11, 293, 550, 575, 257, 1168, 1491, 10543, 11, 293, 321, 434, 8867, 484, 733], "temperature": 0.0, "avg_logprob": -0.17264140633975758, "compression_ratio": 1.634020618556701, "no_speech_prob": 1.8057689885608852e-05}, {"id": 41, "seek": 18590, "start": 195.48000000000002, "end": 200.04000000000002, "text": " of from the wh to the question mark, and it can have kind of any sort of characters in", "tokens": [295, 490, 264, 315, 281, 264, 1168, 1491, 11, 293, 309, 393, 362, 733, 295, 604, 1333, 295, 4342, 294], "temperature": 0.0, "avg_logprob": -0.17264140633975758, "compression_ratio": 1.634020618556701, "no_speech_prob": 1.8057689885608852e-05}, {"id": 42, "seek": 18590, "start": 200.04000000000002, "end": 202.96, "text": " between.", "tokens": [1296, 13], "temperature": 0.0, "avg_logprob": -0.17264140633975758, "compression_ratio": 1.634020618556701, "no_speech_prob": 1.8057689885608852e-05}, {"id": 43, "seek": 18590, "start": 202.96, "end": 204.84, "text": " And that's what we'll be using.", "tokens": [400, 300, 311, 437, 321, 603, 312, 1228, 13], "temperature": 0.0, "avg_logprob": -0.17264140633975758, "compression_ratio": 1.634020618556701, "no_speech_prob": 1.8057689885608852e-05}, {"id": 44, "seek": 18590, "start": 204.84, "end": 209.20000000000002, "text": " And we're doing this just to try to keep our task manageable since translation is a pretty", "tokens": [400, 321, 434, 884, 341, 445, 281, 853, 281, 1066, 527, 5633, 38798, 1670, 12853, 307, 257, 1238], "temperature": 0.0, "avg_logprob": -0.17264140633975758, "compression_ratio": 1.634020618556701, "no_speech_prob": 1.8057689885608852e-05}, {"id": 45, "seek": 18590, "start": 209.20000000000002, "end": 214.04000000000002, "text": " tricky task.", "tokens": [12414, 5633, 13], "temperature": 0.0, "avg_logprob": -0.17264140633975758, "compression_ratio": 1.634020618556701, "no_speech_prob": 1.8057689885608852e-05}, {"id": 46, "seek": 21404, "start": 214.04, "end": 220.95999999999998, "text": " We kind of process these, and again you'll need to uncomment and run all this code once,", "tokens": [492, 733, 295, 1399, 613, 11, 293, 797, 291, 603, 643, 281, 8585, 518, 293, 1190, 439, 341, 3089, 1564, 11], "temperature": 0.0, "avg_logprob": -0.17049181777819067, "compression_ratio": 1.6923076923076923, "no_speech_prob": 6.813911750214174e-05}, {"id": 47, "seek": 21404, "start": 220.95999999999998, "end": 226.39999999999998, "text": " but then because it's kind of slow, definitely save your processed and formatted code so", "tokens": [457, 550, 570, 309, 311, 733, 295, 2964, 11, 2138, 3155, 428, 18846, 293, 1254, 32509, 3089, 370], "temperature": 0.0, "avg_logprob": -0.17049181777819067, "compression_ratio": 1.6923076923076923, "no_speech_prob": 6.813911750214174e-05}, {"id": 48, "seek": 21404, "start": 226.39999999999998, "end": 230.35999999999999, "text": " that you can just, or data that you can use it again in the future.", "tokens": [300, 291, 393, 445, 11, 420, 1412, 300, 291, 393, 764, 309, 797, 294, 264, 2027, 13], "temperature": 0.0, "avg_logprob": -0.17049181777819067, "compression_ratio": 1.6923076923076923, "no_speech_prob": 6.813911750214174e-05}, {"id": 49, "seek": 21404, "start": 230.35999999999999, "end": 235.35999999999999, "text": " Here we're saving it as a CSV, questions easy dot CSV.", "tokens": [1692, 321, 434, 6816, 309, 382, 257, 48814, 11, 1651, 1858, 5893, 48814, 13], "temperature": 0.0, "avg_logprob": -0.17049181777819067, "compression_ratio": 1.6923076923076923, "no_speech_prob": 6.813911750214174e-05}, {"id": 50, "seek": 21404, "start": 235.35999999999999, "end": 241.56, "text": " So I'm going to go ahead to, it's already been processed, and we can take a look at", "tokens": [407, 286, 478, 516, 281, 352, 2286, 281, 11, 309, 311, 1217, 668, 18846, 11, 293, 321, 393, 747, 257, 574, 412], "temperature": 0.0, "avg_logprob": -0.17049181777819067, "compression_ratio": 1.6923076923076923, "no_speech_prob": 6.813911750214174e-05}, {"id": 51, "seek": 21404, "start": 241.56, "end": 243.68, "text": " what some of these questions are.", "tokens": [437, 512, 295, 613, 1651, 366, 13], "temperature": 0.0, "avg_logprob": -0.17049181777819067, "compression_ratio": 1.6923076923076923, "no_speech_prob": 6.813911750214174e-05}, {"id": 52, "seek": 24368, "start": 243.68, "end": 244.68, "text": " What is light?", "tokens": [708, 307, 1442, 30], "temperature": 0.0, "avg_logprob": -0.1929658508300781, "compression_ratio": 1.5363636363636364, "no_speech_prob": 7.071684649417875e-06}, {"id": 53, "seek": 24368, "start": 244.68, "end": 245.76000000000002, "text": " Who are we?", "tokens": [2102, 366, 321, 30], "temperature": 0.0, "avg_logprob": -0.1929658508300781, "compression_ratio": 1.5363636363636364, "no_speech_prob": 7.071684649417875e-06}, {"id": 54, "seek": 24368, "start": 245.76000000000002, "end": 246.76000000000002, "text": " Where did we come from?", "tokens": [2305, 630, 321, 808, 490, 30], "temperature": 0.0, "avg_logprob": -0.1929658508300781, "compression_ratio": 1.5363636363636364, "no_speech_prob": 7.071684649417875e-06}, {"id": 55, "seek": 24368, "start": 246.76000000000002, "end": 249.76000000000002, "text": " Does anyone in this class speak French?", "tokens": [4402, 2878, 294, 341, 1508, 1710, 5522, 30], "temperature": 0.0, "avg_logprob": -0.1929658508300781, "compression_ratio": 1.5363636363636364, "no_speech_prob": 7.071684649417875e-06}, {"id": 56, "seek": 24368, "start": 249.76000000000002, "end": 253.32, "text": " Okay, nobody, me neither.", "tokens": [1033, 11, 5079, 11, 385, 9662, 13], "temperature": 0.0, "avg_logprob": -0.1929658508300781, "compression_ratio": 1.5363636363636364, "no_speech_prob": 7.071684649417875e-06}, {"id": 57, "seek": 24368, "start": 253.32, "end": 261.28000000000003, "text": " So we'll have to kind of trust some of these kind of make sense that Lumiere would be light.", "tokens": [407, 321, 603, 362, 281, 733, 295, 3361, 512, 295, 613, 733, 295, 652, 2020, 300, 35978, 14412, 576, 312, 1442, 13], "temperature": 0.0, "avg_logprob": -0.1929658508300781, "compression_ratio": 1.5363636363636364, "no_speech_prob": 7.071684649417875e-06}, {"id": 58, "seek": 24368, "start": 261.28000000000003, "end": 266.2, "text": " And you can see here how you get the output can be longer than the input, or it can be", "tokens": [400, 291, 393, 536, 510, 577, 291, 483, 264, 5598, 393, 312, 2854, 813, 264, 4846, 11, 420, 309, 393, 312], "temperature": 0.0, "avg_logprob": -0.1929658508300781, "compression_ratio": 1.5363636363636364, "no_speech_prob": 7.071684649417875e-06}, {"id": 59, "seek": 24368, "start": 266.2, "end": 271.32, "text": " shorter, that there's a lot of variation.", "tokens": [11639, 11, 300, 456, 311, 257, 688, 295, 12990, 13], "temperature": 0.0, "avg_logprob": -0.1929658508300781, "compression_ratio": 1.5363636363636364, "no_speech_prob": 7.071684649417875e-06}, {"id": 60, "seek": 27132, "start": 271.32, "end": 277.88, "text": " So what we need to do is to collate our inputs and targets into a batch.", "tokens": [407, 437, 321, 643, 281, 360, 307, 281, 1263, 473, 527, 15743, 293, 12911, 666, 257, 15245, 13], "temperature": 0.0, "avg_logprob": -0.20407846995762416, "compression_ratio": 1.5685279187817258, "no_speech_prob": 1.4969869880587794e-05}, {"id": 61, "seek": 27132, "start": 277.88, "end": 282.08, "text": " Because they all have different lengths, we're going to add padding so that we can have this", "tokens": [1436, 436, 439, 362, 819, 26329, 11, 321, 434, 516, 281, 909, 39562, 370, 300, 321, 393, 362, 341], "temperature": 0.0, "avg_logprob": -0.20407846995762416, "compression_ratio": 1.5685279187817258, "no_speech_prob": 1.4969869880587794e-05}, {"id": 62, "seek": 27132, "start": 282.08, "end": 285.9, "text": " kind of in batches of a fixed length.", "tokens": [733, 295, 294, 15245, 279, 295, 257, 6806, 4641, 13], "temperature": 0.0, "avg_logprob": -0.20407846995762416, "compression_ratio": 1.5685279187817258, "no_speech_prob": 1.4969869880587794e-05}, {"id": 63, "seek": 27132, "start": 285.9, "end": 289.36, "text": " And so that's just adding zeros to fill out the length.", "tokens": [400, 370, 300, 311, 445, 5127, 35193, 281, 2836, 484, 264, 4641, 13], "temperature": 0.0, "avg_logprob": -0.20407846995762416, "compression_ratio": 1.5685279187817258, "no_speech_prob": 1.4969869880587794e-05}, {"id": 64, "seek": 27132, "start": 289.36, "end": 296.24, "text": " Actually I should have, I guess I did this above.", "tokens": [5135, 286, 820, 362, 11, 286, 2041, 286, 630, 341, 3673, 13], "temperature": 0.0, "avg_logprob": -0.20407846995762416, "compression_ratio": 1.5685279187817258, "no_speech_prob": 1.4969869880587794e-05}, {"id": 65, "seek": 29624, "start": 296.24, "end": 302.6, "text": " Okay, I'm sorry.", "tokens": [1033, 11, 286, 478, 2597, 13], "temperature": 0.0, "avg_logprob": -0.1685202477992266, "compression_ratio": 1.5265700483091786, "no_speech_prob": 2.1443574951263145e-05}, {"id": 66, "seek": 29624, "start": 302.6, "end": 307.16, "text": " Later on we're actually going to throw away sentences that are too long, again, to make", "tokens": [11965, 322, 321, 434, 767, 516, 281, 3507, 1314, 16579, 300, 366, 886, 938, 11, 797, 11, 281, 652], "temperature": 0.0, "avg_logprob": -0.1685202477992266, "compression_ratio": 1.5265700483091786, "no_speech_prob": 2.1443574951263145e-05}, {"id": 67, "seek": 29624, "start": 307.16, "end": 313.0, "text": " this simpler and have kind of an easy batch that we can put on the GPO.", "tokens": [341, 18587, 293, 362, 733, 295, 364, 1858, 15245, 300, 321, 393, 829, 322, 264, 26039, 46, 13], "temperature": 0.0, "avg_logprob": -0.1685202477992266, "compression_ratio": 1.5265700483091786, "no_speech_prob": 2.1443574951263145e-05}, {"id": 68, "seek": 29624, "start": 313.0, "end": 321.68, "text": " So here's our collating method that kind of is just adding the padding and getting these", "tokens": [407, 510, 311, 527, 1263, 990, 3170, 300, 733, 295, 307, 445, 5127, 264, 39562, 293, 1242, 613], "temperature": 0.0, "avg_logprob": -0.1685202477992266, "compression_ratio": 1.5265700483091786, "no_speech_prob": 2.1443574951263145e-05}, {"id": 69, "seek": 29624, "start": 321.68, "end": 322.68, "text": " together.", "tokens": [1214, 13], "temperature": 0.0, "avg_logprob": -0.1685202477992266, "compression_ratio": 1.5265700483091786, "no_speech_prob": 2.1443574951263145e-05}, {"id": 70, "seek": 29624, "start": 322.68, "end": 325.32, "text": " Then we're going to create a data bunch.", "tokens": [1396, 321, 434, 516, 281, 1884, 257, 1412, 3840, 13], "temperature": 0.0, "avg_logprob": -0.1685202477992266, "compression_ratio": 1.5265700483091786, "no_speech_prob": 2.1443574951263145e-05}, {"id": 71, "seek": 32532, "start": 325.32, "end": 328.59999999999997, "text": " And so these are concepts from the Fast AI library.", "tokens": [400, 370, 613, 366, 10392, 490, 264, 15968, 7318, 6405, 13], "temperature": 0.0, "avg_logprob": -0.24485255650111606, "compression_ratio": 1.5914634146341464, "no_speech_prob": 1.1659031770250294e-05}, {"id": 72, "seek": 32532, "start": 328.59999999999997, "end": 335.4, "text": " For methods in the Fast AI library, you can use doc to see the docs.", "tokens": [1171, 7150, 294, 264, 15968, 7318, 6405, 11, 291, 393, 764, 3211, 281, 536, 264, 45623, 13], "temperature": 0.0, "avg_logprob": -0.24485255650111606, "compression_ratio": 1.5914634146341464, "no_speech_prob": 1.1659031770250294e-05}, {"id": 73, "seek": 32532, "start": 335.4, "end": 340.76, "text": " So here a data set class is an abstract class representing a data set.", "tokens": [407, 510, 257, 1412, 992, 1508, 307, 364, 12649, 1508, 13460, 257, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.24485255650111606, "compression_ratio": 1.5914634146341464, "no_speech_prob": 1.1659031770250294e-05}, {"id": 74, "seek": 32532, "start": 340.76, "end": 345.96, "text": " Have you guys talked about abstract classes in the program?", "tokens": [3560, 291, 1074, 2825, 466, 12649, 5359, 294, 264, 1461, 30], "temperature": 0.0, "avg_logprob": -0.24485255650111606, "compression_ratio": 1.5914634146341464, "no_speech_prob": 1.1659031770250294e-05}, {"id": 75, "seek": 32532, "start": 345.96, "end": 348.2, "text": " No, okay.", "tokens": [883, 11, 1392, 13], "temperature": 0.0, "avg_logprob": -0.24485255650111606, "compression_ratio": 1.5914634146341464, "no_speech_prob": 1.1659031770250294e-05}, {"id": 76, "seek": 34820, "start": 348.2, "end": 355.32, "text": " So this is kind of a Python concept and you're kind of defining kind of these things that", "tokens": [407, 341, 307, 733, 295, 257, 15329, 3410, 293, 291, 434, 733, 295, 17827, 733, 295, 613, 721, 300], "temperature": 0.0, "avg_logprob": -0.14301047890873278, "compression_ratio": 1.7791164658634537, "no_speech_prob": 1.3630760804517195e-05}, {"id": 77, "seek": 34820, "start": 355.32, "end": 356.84, "text": " we want a data set to have.", "tokens": [321, 528, 257, 1412, 992, 281, 362, 13], "temperature": 0.0, "avg_logprob": -0.14301047890873278, "compression_ratio": 1.7791164658634537, "no_speech_prob": 1.3630760804517195e-05}, {"id": 78, "seek": 34820, "start": 356.84, "end": 363.12, "text": " And in this case it's, we're saying any specific type of data set you create is going to need", "tokens": [400, 294, 341, 1389, 309, 311, 11, 321, 434, 1566, 604, 2685, 2010, 295, 1412, 992, 291, 1884, 307, 516, 281, 643], "temperature": 0.0, "avg_logprob": -0.14301047890873278, "compression_ratio": 1.7791164658634537, "no_speech_prob": 1.3630760804517195e-05}, {"id": 79, "seek": 34820, "start": 363.12, "end": 368.88, "text": " to have a length as well as a get item, which is basically a way to index into it.", "tokens": [281, 362, 257, 4641, 382, 731, 382, 257, 483, 3174, 11, 597, 307, 1936, 257, 636, 281, 8186, 666, 309, 13], "temperature": 0.0, "avg_logprob": -0.14301047890873278, "compression_ratio": 1.7791164658634537, "no_speech_prob": 1.3630760804517195e-05}, {"id": 80, "seek": 34820, "start": 368.88, "end": 374.4, "text": " So this is what gives you the ability to index into something and we'll subclass it, which", "tokens": [407, 341, 307, 437, 2709, 291, 264, 3485, 281, 8186, 666, 746, 293, 321, 603, 1422, 11665, 309, 11, 597], "temperature": 0.0, "avg_logprob": -0.14301047890873278, "compression_ratio": 1.7791164658634537, "no_speech_prob": 1.3630760804517195e-05}, {"id": 81, "seek": 34820, "start": 374.4, "end": 377.56, "text": " means we're kind of making a class that inherits from it.", "tokens": [1355, 321, 434, 733, 295, 1455, 257, 1508, 300, 9484, 1208, 490, 309, 13], "temperature": 0.0, "avg_logprob": -0.14301047890873278, "compression_ratio": 1.7791164658634537, "no_speech_prob": 1.3630760804517195e-05}, {"id": 82, "seek": 37756, "start": 377.56, "end": 382.32, "text": " So we'll see this in a moment, but you can have kind of specific types of data sets.", "tokens": [407, 321, 603, 536, 341, 294, 257, 1623, 11, 457, 291, 393, 362, 733, 295, 2685, 3467, 295, 1412, 6352, 13], "temperature": 0.0, "avg_logprob": -0.19860834779992567, "compression_ratio": 1.4646017699115044, "no_speech_prob": 1.6441619663964957e-05}, {"id": 83, "seek": 37756, "start": 382.32, "end": 383.32, "text": " Jeremy?", "tokens": [17809, 30], "temperature": 0.0, "avg_logprob": -0.19860834779992567, "compression_ratio": 1.4646017699115044, "no_speech_prob": 1.6441619663964957e-05}, {"id": 84, "seek": 37756, "start": 383.32, "end": 384.32, "text": " Can I?", "tokens": [1664, 286, 30], "temperature": 0.0, "avg_logprob": -0.19860834779992567, "compression_ratio": 1.4646017699115044, "no_speech_prob": 1.6441619663964957e-05}, {"id": 85, "seek": 37756, "start": 384.32, "end": 385.32, "text": " Oh.", "tokens": [876, 13], "temperature": 0.0, "avg_logprob": -0.19860834779992567, "compression_ratio": 1.4646017699115044, "no_speech_prob": 1.6441619663964957e-05}, {"id": 86, "seek": 37756, "start": 385.32, "end": 386.32, "text": " Oh, sure.", "tokens": [876, 11, 988, 13], "temperature": 0.0, "avg_logprob": -0.19860834779992567, "compression_ratio": 1.4646017699115044, "no_speech_prob": 1.6441619663964957e-05}, {"id": 87, "seek": 37756, "start": 386.32, "end": 387.32, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.19860834779992567, "compression_ratio": 1.4646017699115044, "no_speech_prob": 1.6441619663964957e-05}, {"id": 88, "seek": 37756, "start": 387.32, "end": 393.32, "text": " So are you familiar with DunderLen and DunderGetItem?", "tokens": [407, 366, 291, 4963, 365, 413, 6617, 43, 268, 293, 413, 6617, 18133, 3522, 443, 30], "temperature": 0.0, "avg_logprob": -0.19860834779992567, "compression_ratio": 1.4646017699115044, "no_speech_prob": 1.6441619663964957e-05}, {"id": 89, "seek": 37756, "start": 393.32, "end": 394.32, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.19860834779992567, "compression_ratio": 1.4646017699115044, "no_speech_prob": 1.6441619663964957e-05}, {"id": 90, "seek": 37756, "start": 394.32, "end": 395.32, "text": " I see lots of nodding.", "tokens": [286, 536, 3195, 295, 15224, 3584, 13], "temperature": 0.0, "avg_logprob": -0.19860834779992567, "compression_ratio": 1.4646017699115044, "no_speech_prob": 1.6441619663964957e-05}, {"id": 91, "seek": 37756, "start": 395.32, "end": 396.32, "text": " And that's good.", "tokens": [400, 300, 311, 665, 13], "temperature": 0.0, "avg_logprob": -0.19860834779992567, "compression_ratio": 1.4646017699115044, "no_speech_prob": 1.6441619663964957e-05}, {"id": 92, "seek": 37756, "start": 396.32, "end": 403.04, "text": " And yeah, for anyone who needs to review these, these are kind of important Python concepts", "tokens": [400, 1338, 11, 337, 2878, 567, 2203, 281, 3131, 613, 11, 613, 366, 733, 295, 1021, 15329, 10392], "temperature": 0.0, "avg_logprob": -0.19860834779992567, "compression_ratio": 1.4646017699115044, "no_speech_prob": 1.6441619663964957e-05}, {"id": 93, "seek": 37756, "start": 403.04, "end": 404.04, "text": " and that's helpful.", "tokens": [293, 300, 311, 4961, 13], "temperature": 0.0, "avg_logprob": -0.19860834779992567, "compression_ratio": 1.4646017699115044, "no_speech_prob": 1.6441619663964957e-05}, {"id": 94, "seek": 40404, "start": 404.04, "end": 410.08000000000004, "text": " You can kind of look over in a lot of Python tutorials.", "tokens": [509, 393, 733, 295, 574, 670, 294, 257, 688, 295, 15329, 17616, 13], "temperature": 0.0, "avg_logprob": -0.13532906700583064, "compression_ratio": 1.6171428571428572, "no_speech_prob": 8.530106242687907e-06}, {"id": 95, "seek": 40404, "start": 410.08000000000004, "end": 414.36, "text": " So kind of the basic thing, so a data set is something that's got a length and there's", "tokens": [407, 733, 295, 264, 3875, 551, 11, 370, 257, 1412, 992, 307, 746, 300, 311, 658, 257, 4641, 293, 456, 311], "temperature": 0.0, "avg_logprob": -0.13532906700583064, "compression_ratio": 1.6171428571428572, "no_speech_prob": 8.530106242687907e-06}, {"id": 96, "seek": 40404, "start": 414.36, "end": 416.44, "text": " a way to index into it.", "tokens": [257, 636, 281, 8186, 666, 309, 13], "temperature": 0.0, "avg_logprob": -0.13532906700583064, "compression_ratio": 1.6171428571428572, "no_speech_prob": 8.530106242687907e-06}, {"id": 97, "seek": 40404, "start": 416.44, "end": 422.08000000000004, "text": " And we'll use that as part of a data loader.", "tokens": [400, 321, 603, 764, 300, 382, 644, 295, 257, 1412, 3677, 260, 13], "temperature": 0.0, "avg_logprob": -0.13532906700583064, "compression_ratio": 1.6171428571428572, "no_speech_prob": 8.530106242687907e-06}, {"id": 98, "seek": 40404, "start": 422.08000000000004, "end": 429.48, "text": " And a data loader is going to give us a way to iterate over a data set.", "tokens": [400, 257, 1412, 3677, 260, 307, 516, 281, 976, 505, 257, 636, 281, 44497, 670, 257, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.13532906700583064, "compression_ratio": 1.6171428571428572, "no_speech_prob": 8.530106242687907e-06}, {"id": 99, "seek": 42948, "start": 429.48, "end": 435.92, "text": " So we want something that we can kind of call and it will return a batch of the data.", "tokens": [407, 321, 528, 746, 300, 321, 393, 733, 295, 818, 293, 309, 486, 2736, 257, 15245, 295, 264, 1412, 13], "temperature": 0.0, "avg_logprob": -0.08876041412353515, "compression_ratio": 1.574468085106383, "no_speech_prob": 5.422088179329876e-06}, {"id": 100, "seek": 42948, "start": 435.92, "end": 443.24, "text": " So this has a batch size, the option of whether you want to shuffle your batches, which just", "tokens": [407, 341, 575, 257, 15245, 2744, 11, 264, 3614, 295, 1968, 291, 528, 281, 39426, 428, 15245, 279, 11, 597, 445], "temperature": 0.0, "avg_logprob": -0.08876041412353515, "compression_ratio": 1.574468085106383, "no_speech_prob": 5.422088179329876e-06}, {"id": 101, "seek": 42948, "start": 443.24, "end": 450.76, "text": " means changing the order.", "tokens": [1355, 4473, 264, 1668, 13], "temperature": 0.0, "avg_logprob": -0.08876041412353515, "compression_ratio": 1.574468085106383, "no_speech_prob": 5.422088179329876e-06}, {"id": 102, "seek": 42948, "start": 450.76, "end": 455.28000000000003, "text": " And this is useful kind of with neural networks since we're only looking at a batch of data", "tokens": [400, 341, 307, 4420, 733, 295, 365, 18161, 9590, 1670, 321, 434, 787, 1237, 412, 257, 15245, 295, 1412], "temperature": 0.0, "avg_logprob": -0.08876041412353515, "compression_ratio": 1.574468085106383, "no_speech_prob": 5.422088179329876e-06}, {"id": 103, "seek": 45528, "start": 455.28, "end": 461.88, "text": " at a time, you need a way to kind of get out a batch of data at a time.", "tokens": [412, 257, 565, 11, 291, 643, 257, 636, 281, 733, 295, 483, 484, 257, 15245, 295, 1412, 412, 257, 565, 13], "temperature": 0.0, "avg_logprob": -0.11274445193937455, "compression_ratio": 1.7396449704142012, "no_speech_prob": 1.2218766642035916e-05}, {"id": 104, "seek": 45528, "start": 461.88, "end": 465.71999999999997, "text": " So data loader kind of builds on that.", "tokens": [407, 1412, 3677, 260, 733, 295, 15182, 322, 300, 13], "temperature": 0.0, "avg_logprob": -0.11274445193937455, "compression_ratio": 1.7396449704142012, "no_speech_prob": 1.2218766642035916e-05}, {"id": 105, "seek": 45528, "start": 465.71999999999997, "end": 471.35999999999996, "text": " And then data bunch builds on data loader.", "tokens": [400, 550, 1412, 3840, 15182, 322, 1412, 3677, 260, 13], "temperature": 0.0, "avg_logprob": -0.11274445193937455, "compression_ratio": 1.7396449704142012, "no_speech_prob": 1.2218766642035916e-05}, {"id": 106, "seek": 45528, "start": 471.35999999999996, "end": 476.96, "text": " And this gives you three or two or three data loaders.", "tokens": [400, 341, 2709, 291, 1045, 420, 732, 420, 1045, 1412, 3677, 433, 13], "temperature": 0.0, "avg_logprob": -0.11274445193937455, "compression_ratio": 1.7396449704142012, "no_speech_prob": 1.2218766642035916e-05}, {"id": 107, "seek": 45528, "start": 476.96, "end": 483.32, "text": " You want to have a training data loader, a validation one, and optionally a test one.", "tokens": [509, 528, 281, 362, 257, 3097, 1412, 3677, 260, 11, 257, 24071, 472, 11, 293, 3614, 379, 257, 1500, 472, 13], "temperature": 0.0, "avg_logprob": -0.11274445193937455, "compression_ratio": 1.7396449704142012, "no_speech_prob": 1.2218766642035916e-05}, {"id": 108, "seek": 48332, "start": 483.32, "end": 487.96, "text": " So this is kind of a way to store your training set and your validation set and your test", "tokens": [407, 341, 307, 733, 295, 257, 636, 281, 3531, 428, 3097, 992, 293, 428, 24071, 992, 293, 428, 1500], "temperature": 0.0, "avg_logprob": -0.11448291728371068, "compression_ratio": 1.6236559139784945, "no_speech_prob": 4.425385213835398e-06}, {"id": 109, "seek": 48332, "start": 487.96, "end": 491.71999999999997, "text": " set if you have one all together in one place.", "tokens": [992, 498, 291, 362, 472, 439, 1214, 294, 472, 1081, 13], "temperature": 0.0, "avg_logprob": -0.11448291728371068, "compression_ratio": 1.6236559139784945, "no_speech_prob": 4.425385213835398e-06}, {"id": 110, "seek": 48332, "start": 491.71999999999997, "end": 495.48, "text": " And you'll be able to return batches of any of those from this data bunch.", "tokens": [400, 291, 603, 312, 1075, 281, 2736, 15245, 279, 295, 604, 295, 729, 490, 341, 1412, 3840, 13], "temperature": 0.0, "avg_logprob": -0.11448291728371068, "compression_ratio": 1.6236559139784945, "no_speech_prob": 4.425385213835398e-06}, {"id": 111, "seek": 48332, "start": 495.48, "end": 503.44, "text": " So this is a really useful object for kind of keeping these pieces that you need together.", "tokens": [407, 341, 307, 257, 534, 4420, 2657, 337, 733, 295, 5145, 613, 3755, 300, 291, 643, 1214, 13], "temperature": 0.0, "avg_logprob": -0.11448291728371068, "compression_ratio": 1.6236559139784945, "no_speech_prob": 4.425385213835398e-06}, {"id": 112, "seek": 50344, "start": 503.44, "end": 516.12, "text": " And so then seek to seek data bunch will kind of create a type of text data bunch that's", "tokens": [400, 370, 550, 8075, 281, 8075, 1412, 3840, 486, 733, 295, 1884, 257, 2010, 295, 2487, 1412, 3840, 300, 311], "temperature": 0.0, "avg_logprob": -0.1369428907121931, "compression_ratio": 1.5590062111801242, "no_speech_prob": 2.123359081451781e-06}, {"id": 113, "seek": 50344, "start": 516.12, "end": 518.8, "text": " suitable for training an RNN.", "tokens": [12873, 337, 3097, 364, 45702, 45, 13], "temperature": 0.0, "avg_logprob": -0.1369428907121931, "compression_ratio": 1.5590062111801242, "no_speech_prob": 2.123359081451781e-06}, {"id": 114, "seek": 50344, "start": 518.8, "end": 525.16, "text": " And we do that by adding a class method that is just how you would create your seek to", "tokens": [400, 321, 360, 300, 538, 5127, 257, 1508, 3170, 300, 307, 445, 577, 291, 576, 1884, 428, 8075, 281], "temperature": 0.0, "avg_logprob": -0.1369428907121931, "compression_ratio": 1.5590062111801242, "no_speech_prob": 2.123359081451781e-06}, {"id": 115, "seek": 50344, "start": 525.16, "end": 529.2, "text": " seek data bunch.", "tokens": [8075, 1412, 3840, 13], "temperature": 0.0, "avg_logprob": -0.1369428907121931, "compression_ratio": 1.5590062111801242, "no_speech_prob": 2.123359081451781e-06}, {"id": 116, "seek": 50344, "start": 529.2, "end": 531.52, "text": " This uses a sortish sampler.", "tokens": [639, 4960, 257, 1333, 742, 3247, 22732, 13], "temperature": 0.0, "avg_logprob": -0.1369428907121931, "compression_ratio": 1.5590062111801242, "no_speech_prob": 2.123359081451781e-06}, {"id": 117, "seek": 53152, "start": 531.52, "end": 533.52, "text": " This is from the Fast AI library.", "tokens": [639, 307, 490, 264, 15968, 7318, 6405, 13], "temperature": 0.0, "avg_logprob": -0.2180086851119995, "compression_ratio": 1.5309278350515463, "no_speech_prob": 2.7967415007879026e-05}, {"id": 118, "seek": 53152, "start": 533.52, "end": 537.28, "text": " Actually, let me get the docs for this is probably better.", "tokens": [5135, 11, 718, 385, 483, 264, 45623, 337, 341, 307, 1391, 1101, 13], "temperature": 0.0, "avg_logprob": -0.2180086851119995, "compression_ratio": 1.5309278350515463, "no_speech_prob": 2.7967415007879026e-05}, {"id": 119, "seek": 53152, "start": 537.28, "end": 542.76, "text": " So in a Jupyter notebook, question mark, question mark will work on any method.", "tokens": [407, 294, 257, 22125, 88, 391, 21060, 11, 1168, 1491, 11, 1168, 1491, 486, 589, 322, 604, 3170, 13], "temperature": 0.0, "avg_logprob": -0.2180086851119995, "compression_ratio": 1.5309278350515463, "no_speech_prob": 2.7967415007879026e-05}, {"id": 120, "seek": 53152, "start": 542.76, "end": 546.16, "text": " Doc is specific to the Fast AI library.", "tokens": [16024, 307, 2685, 281, 264, 15968, 7318, 6405, 13], "temperature": 0.0, "avg_logprob": -0.2180086851119995, "compression_ratio": 1.5309278350515463, "no_speech_prob": 2.7967415007879026e-05}, {"id": 121, "seek": 53152, "start": 546.16, "end": 551.92, "text": " This goes through text data by order of length, but with a little bit of randomness.", "tokens": [639, 1709, 807, 2487, 1412, 538, 1668, 295, 4641, 11, 457, 365, 257, 707, 857, 295, 4974, 1287, 13], "temperature": 0.0, "avg_logprob": -0.2180086851119995, "compression_ratio": 1.5309278350515463, "no_speech_prob": 2.7967415007879026e-05}, {"id": 122, "seek": 55192, "start": 551.92, "end": 563.3199999999999, "text": " Okay, this is handy.", "tokens": [1033, 11, 341, 307, 13239, 13], "temperature": 0.0, "avg_logprob": -0.3075383676064981, "compression_ratio": 1.075268817204301, "no_speech_prob": 1.4284803910413757e-05}, {"id": 123, "seek": 55192, "start": 563.3199999999999, "end": 572.9599999999999, "text": " This will take you to the docs on the website.", "tokens": [639, 486, 747, 291, 281, 264, 45623, 322, 264, 3144, 13], "temperature": 0.0, "avg_logprob": -0.3075383676064981, "compression_ratio": 1.075268817204301, "no_speech_prob": 1.4284803910413757e-05}, {"id": 124, "seek": 55192, "start": 572.9599999999999, "end": 579.28, "text": " Is it what up a bit?", "tokens": [1119, 309, 437, 493, 257, 857, 30], "temperature": 0.0, "avg_logprob": -0.3075383676064981, "compression_ratio": 1.075268817204301, "no_speech_prob": 1.4284803910413757e-05}, {"id": 125, "seek": 55192, "start": 579.28, "end": 580.28, "text": " Right here.", "tokens": [1779, 510, 13], "temperature": 0.0, "avg_logprob": -0.3075383676064981, "compression_ratio": 1.075268817204301, "no_speech_prob": 1.4284803910413757e-05}, {"id": 126, "seek": 58028, "start": 580.28, "end": 583.92, "text": " Is there more you wanted to say about this Jeremy?", "tokens": [1119, 456, 544, 291, 1415, 281, 584, 466, 341, 17809, 30], "temperature": 0.0, "avg_logprob": -0.33674510557260084, "compression_ratio": 1.391025641025641, "no_speech_prob": 2.282675814058166e-05}, {"id": 127, "seek": 58028, "start": 583.92, "end": 587.28, "text": " Okay, yeah.", "tokens": [1033, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.33674510557260084, "compression_ratio": 1.391025641025641, "no_speech_prob": 2.282675814058166e-05}, {"id": 128, "seek": 58028, "start": 587.28, "end": 595.36, "text": " So here then you get the links of, so this inherits from sampler, which is a PyTorch", "tokens": [407, 510, 550, 291, 483, 264, 6123, 295, 11, 370, 341, 9484, 1208, 490, 3247, 22732, 11, 597, 307, 257, 9953, 51, 284, 339], "temperature": 0.0, "avg_logprob": -0.33674510557260084, "compression_ratio": 1.391025641025641, "no_speech_prob": 2.282675814058166e-05}, {"id": 129, "seek": 58028, "start": 595.36, "end": 598.92, "text": " class.", "tokens": [1508, 13], "temperature": 0.0, "avg_logprob": -0.33674510557260084, "compression_ratio": 1.391025641025641, "no_speech_prob": 2.282675814058166e-05}, {"id": 130, "seek": 58028, "start": 598.92, "end": 603.1999999999999, "text": " Every sampler class or subclass has to provide an iter method.", "tokens": [2048, 3247, 22732, 1508, 420, 1422, 11665, 575, 281, 2893, 364, 17138, 3170, 13], "temperature": 0.0, "avg_logprob": -0.33674510557260084, "compression_ratio": 1.391025641025641, "no_speech_prob": 2.282675814058166e-05}, {"id": 131, "seek": 60320, "start": 603.2, "end": 610.88, "text": " So it's a way to iterate over indices of data set elements and a length.", "tokens": [407, 309, 311, 257, 636, 281, 44497, 670, 43840, 295, 1412, 992, 4959, 293, 257, 4641, 13], "temperature": 0.0, "avg_logprob": -0.18965656199353806, "compression_ratio": 1.5, "no_speech_prob": 1.3211572877480648e-05}, {"id": 132, "seek": 60320, "start": 610.88, "end": 616.2800000000001, "text": " And this is useful to kind of be able to navigate the docs, both for Fast AI as well as for", "tokens": [400, 341, 307, 4420, 281, 733, 295, 312, 1075, 281, 12350, 264, 45623, 11, 1293, 337, 15968, 7318, 382, 731, 382, 337], "temperature": 0.0, "avg_logprob": -0.18965656199353806, "compression_ratio": 1.5, "no_speech_prob": 1.3211572877480648e-05}, {"id": 133, "seek": 60320, "start": 616.2800000000001, "end": 621.32, "text": " PyTorch or kind of whatever library you're using.", "tokens": [9953, 51, 284, 339, 420, 733, 295, 2035, 6405, 291, 434, 1228, 13], "temperature": 0.0, "avg_logprob": -0.18965656199353806, "compression_ratio": 1.5, "no_speech_prob": 1.3211572877480648e-05}, {"id": 134, "seek": 60320, "start": 621.32, "end": 622.32, "text": " Going back here.", "tokens": [10963, 646, 510, 13], "temperature": 0.0, "avg_logprob": -0.18965656199353806, "compression_ratio": 1.5, "no_speech_prob": 1.3211572877480648e-05}, {"id": 135, "seek": 60320, "start": 622.32, "end": 628.6, "text": " We're not going to get into seek to seek text lists so much.", "tokens": [492, 434, 406, 516, 281, 483, 666, 8075, 281, 8075, 2487, 14511, 370, 709, 13], "temperature": 0.0, "avg_logprob": -0.18965656199353806, "compression_ratio": 1.5, "no_speech_prob": 1.3211572877480648e-05}, {"id": 136, "seek": 60320, "start": 628.6, "end": 631.36, "text": " So yeah, don't get too distracted.", "tokens": [407, 1338, 11, 500, 380, 483, 886, 21658, 13], "temperature": 0.0, "avg_logprob": -0.18965656199353806, "compression_ratio": 1.5, "no_speech_prob": 1.3211572877480648e-05}, {"id": 137, "seek": 63136, "start": 631.36, "end": 636.12, "text": " I know this is a lot of kind of hierarchy of classes that we're using.", "tokens": [286, 458, 341, 307, 257, 688, 295, 733, 295, 22333, 295, 5359, 300, 321, 434, 1228, 13], "temperature": 0.0, "avg_logprob": -0.18070091688928525, "compression_ratio": 1.7911646586345382, "no_speech_prob": 3.119848042842932e-05}, {"id": 138, "seek": 63136, "start": 636.12, "end": 638.88, "text": " For now, be more focused on kind of what we're doing with these.", "tokens": [1171, 586, 11, 312, 544, 5178, 322, 733, 295, 437, 321, 434, 884, 365, 613, 13], "temperature": 0.0, "avg_logprob": -0.18070091688928525, "compression_ratio": 1.7911646586345382, "no_speech_prob": 3.119848042842932e-05}, {"id": 139, "seek": 63136, "start": 638.88, "end": 642.76, "text": " But kind of wanted to show you how you can get more information on them if you're so", "tokens": [583, 733, 295, 1415, 281, 855, 291, 577, 291, 393, 483, 544, 1589, 322, 552, 498, 291, 434, 370], "temperature": 0.0, "avg_logprob": -0.18070091688928525, "compression_ratio": 1.7911646586345382, "no_speech_prob": 3.119848042842932e-05}, {"id": 140, "seek": 63136, "start": 642.76, "end": 643.76, "text": " interested.", "tokens": [3102, 13], "temperature": 0.0, "avg_logprob": -0.18070091688928525, "compression_ratio": 1.7911646586345382, "no_speech_prob": 3.119848042842932e-05}, {"id": 141, "seek": 63136, "start": 643.76, "end": 651.92, "text": " But for now, it's a kind of focus on what we're going to do with these classes.", "tokens": [583, 337, 586, 11, 309, 311, 257, 733, 295, 1879, 322, 437, 321, 434, 516, 281, 360, 365, 613, 5359, 13], "temperature": 0.0, "avg_logprob": -0.18070091688928525, "compression_ratio": 1.7911646586345382, "no_speech_prob": 3.119848042842932e-05}, {"id": 142, "seek": 63136, "start": 651.92, "end": 656.2, "text": " And yeah, for the microphone, Jeremy just said this is covered a lot in the Fast AI", "tokens": [400, 1338, 11, 337, 264, 10952, 11, 17809, 445, 848, 341, 307, 5343, 257, 688, 294, 264, 15968, 7318], "temperature": 0.0, "avg_logprob": -0.18070091688928525, "compression_ratio": 1.7911646586345382, "no_speech_prob": 3.119848042842932e-05}, {"id": 143, "seek": 63136, "start": 656.2, "end": 660.16, "text": " deep learning class if you did want to hear more.", "tokens": [2452, 2539, 1508, 498, 291, 630, 528, 281, 1568, 544, 13], "temperature": 0.0, "avg_logprob": -0.18070091688928525, "compression_ratio": 1.7911646586345382, "no_speech_prob": 3.119848042842932e-05}, {"id": 144, "seek": 66016, "start": 660.16, "end": 666.8399999999999, "text": " So here what I mentioned before, so before we kind of processed our French and English", "tokens": [407, 510, 437, 286, 2835, 949, 11, 370, 949, 321, 733, 295, 18846, 527, 5522, 293, 3669], "temperature": 0.0, "avg_logprob": -0.24565237446835167, "compression_ratio": 1.4526315789473685, "no_speech_prob": 2.7964464607066475e-05}, {"id": 145, "seek": 66016, "start": 666.8399999999999, "end": 669.64, "text": " questions, put those into a data frame.", "tokens": [1651, 11, 829, 729, 666, 257, 1412, 3920, 13], "temperature": 0.0, "avg_logprob": -0.24565237446835167, "compression_ratio": 1.4526315789473685, "no_speech_prob": 2.7964464607066475e-05}, {"id": 146, "seek": 66016, "start": 669.64, "end": 674.7199999999999, "text": " Now we'll create a seek to seek text list from that.", "tokens": [823, 321, 603, 1884, 257, 8075, 281, 8075, 2487, 1329, 490, 300, 13], "temperature": 0.0, "avg_logprob": -0.24565237446835167, "compression_ratio": 1.4526315789473685, "no_speech_prob": 2.7964464607066475e-05}, {"id": 147, "seek": 66016, "start": 674.7199999999999, "end": 675.7199999999999, "text": " And the...", "tokens": [400, 264, 485], "temperature": 0.0, "avg_logprob": -0.24565237446835167, "compression_ratio": 1.4526315789473685, "no_speech_prob": 2.7964464607066475e-05}, {"id": 148, "seek": 66016, "start": 675.7199999999999, "end": 681.36, "text": " I haven't been running these, so let me see if this works, if I have stuff loaded in.", "tokens": [286, 2378, 380, 668, 2614, 613, 11, 370, 718, 385, 536, 498, 341, 1985, 11, 498, 286, 362, 1507, 13210, 294, 13], "temperature": 0.0, "avg_logprob": -0.24565237446835167, "compression_ratio": 1.4526315789473685, "no_speech_prob": 2.7964464607066475e-05}, {"id": 149, "seek": 68136, "start": 681.36, "end": 692.0, "text": " Yeah, SRC, seek to seek text list has a lot of nice stuff in it, including our training", "tokens": [865, 11, 20840, 34, 11, 8075, 281, 8075, 2487, 1329, 575, 257, 688, 295, 1481, 1507, 294, 309, 11, 3009, 527, 3097], "temperature": 0.0, "avg_logprob": -0.19995788905931555, "compression_ratio": 1.5121951219512195, "no_speech_prob": 6.240666607482126e-06}, {"id": 150, "seek": 68136, "start": 692.0, "end": 694.12, "text": " set and our test set.", "tokens": [992, 293, 527, 1500, 992, 13], "temperature": 0.0, "avg_logprob": -0.19995788905931555, "compression_ratio": 1.5121951219512195, "no_speech_prob": 6.240666607482126e-06}, {"id": 151, "seek": 68136, "start": 694.12, "end": 696.6800000000001, "text": " This is a good way to access things.", "tokens": [639, 307, 257, 665, 636, 281, 2105, 721, 13], "temperature": 0.0, "avg_logprob": -0.19995788905931555, "compression_ratio": 1.5121951219512195, "no_speech_prob": 6.240666607482126e-06}, {"id": 152, "seek": 68136, "start": 696.6800000000001, "end": 703.24, "text": " And so then here we just kind of wanted to look at the 90th percentile.", "tokens": [400, 370, 550, 510, 321, 445, 733, 295, 1415, 281, 574, 412, 264, 4289, 392, 3043, 794, 13], "temperature": 0.0, "avg_logprob": -0.19995788905931555, "compression_ratio": 1.5121951219512195, "no_speech_prob": 6.240666607482126e-06}, {"id": 153, "seek": 68136, "start": 703.24, "end": 706.36, "text": " So we're using a NumPy method.", "tokens": [407, 321, 434, 1228, 257, 22592, 47, 88, 3170, 13], "temperature": 0.0, "avg_logprob": -0.19995788905931555, "compression_ratio": 1.5121951219512195, "no_speech_prob": 6.240666607482126e-06}, {"id": 154, "seek": 68136, "start": 706.36, "end": 708.9200000000001, "text": " Basically we wanted to see kind of how long are these items.", "tokens": [8537, 321, 1415, 281, 536, 733, 295, 577, 938, 366, 613, 4754, 13], "temperature": 0.0, "avg_logprob": -0.19995788905931555, "compression_ratio": 1.5121951219512195, "no_speech_prob": 6.240666607482126e-06}, {"id": 155, "seek": 70892, "start": 708.92, "end": 717.0, "text": " And we get that 28 is the 90th percentile for X, 23 is the 90th percentile for Y.", "tokens": [400, 321, 483, 300, 7562, 307, 264, 4289, 392, 3043, 794, 337, 1783, 11, 6673, 307, 264, 4289, 392, 3043, 794, 337, 398, 13], "temperature": 0.0, "avg_logprob": -0.11695230109059912, "compression_ratio": 1.7171314741035857, "no_speech_prob": 1.1658407856884878e-05}, {"id": 156, "seek": 70892, "start": 717.0, "end": 720.5999999999999, "text": " And so we'll remove items longer than 30 tokens long.", "tokens": [400, 370, 321, 603, 4159, 4754, 2854, 813, 2217, 22667, 938, 13], "temperature": 0.0, "avg_logprob": -0.11695230109059912, "compression_ratio": 1.7171314741035857, "no_speech_prob": 1.1658407856884878e-05}, {"id": 157, "seek": 70892, "start": 720.5999999999999, "end": 726.16, "text": " Just since not many items fall in that, it's less than 10%.", "tokens": [1449, 1670, 406, 867, 4754, 2100, 294, 300, 11, 309, 311, 1570, 813, 1266, 6856], "temperature": 0.0, "avg_logprob": -0.11695230109059912, "compression_ratio": 1.7171314741035857, "no_speech_prob": 1.1658407856884878e-05}, {"id": 158, "seek": 70892, "start": 726.16, "end": 729.5999999999999, "text": " And this will kind of make it simpler for what we're working with.", "tokens": [400, 341, 486, 733, 295, 652, 309, 18587, 337, 437, 321, 434, 1364, 365, 13], "temperature": 0.0, "avg_logprob": -0.11695230109059912, "compression_ratio": 1.7171314741035857, "no_speech_prob": 1.1658407856884878e-05}, {"id": 159, "seek": 70892, "start": 729.5999999999999, "end": 733.04, "text": " And we're not losing that, we're not throwing away that much data by doing this.", "tokens": [400, 321, 434, 406, 7027, 300, 11, 321, 434, 406, 10238, 1314, 300, 709, 1412, 538, 884, 341, 13], "temperature": 0.0, "avg_logprob": -0.11695230109059912, "compression_ratio": 1.7171314741035857, "no_speech_prob": 1.1658407856884878e-05}, {"id": 160, "seek": 70892, "start": 733.04, "end": 737.4, "text": " Kind of interesting to note that the English sentences, I guess, seem to be longer than", "tokens": [9242, 295, 1880, 281, 3637, 300, 264, 3669, 16579, 11, 286, 2041, 11, 1643, 281, 312, 2854, 813], "temperature": 0.0, "avg_logprob": -0.11695230109059912, "compression_ratio": 1.7171314741035857, "no_speech_prob": 1.1658407856884878e-05}, {"id": 161, "seek": 73740, "start": 737.4, "end": 745.16, "text": " the French ones on the whole, since here we got 23 for our Ys, which are the French sentences,", "tokens": [264, 5522, 2306, 322, 264, 1379, 11, 1670, 510, 321, 658, 6673, 337, 527, 398, 82, 11, 597, 366, 264, 5522, 16579, 11], "temperature": 0.0, "avg_logprob": -0.14877089014593162, "compression_ratio": 1.6754385964912282, "no_speech_prob": 8.664284905535169e-06}, {"id": 162, "seek": 73740, "start": 745.16, "end": 747.16, "text": " the Xs are the English sentences.", "tokens": [264, 1783, 82, 366, 264, 3669, 16579, 13], "temperature": 0.0, "avg_logprob": -0.14877089014593162, "compression_ratio": 1.6754385964912282, "no_speech_prob": 8.664284905535169e-06}, {"id": 163, "seek": 73740, "start": 747.16, "end": 755.9599999999999, "text": " So we'll go through, filter those out where either X or Y is greater than 30.", "tokens": [407, 321, 603, 352, 807, 11, 6608, 729, 484, 689, 2139, 1783, 420, 398, 307, 5044, 813, 2217, 13], "temperature": 0.0, "avg_logprob": -0.14877089014593162, "compression_ratio": 1.6754385964912282, "no_speech_prob": 8.664284905535169e-06}, {"id": 164, "seek": 73740, "start": 755.9599999999999, "end": 760.76, "text": " And then this is kind of our final data bunch that we're creating, our data, and this is", "tokens": [400, 550, 341, 307, 733, 295, 527, 2572, 1412, 3840, 300, 321, 434, 4084, 11, 527, 1412, 11, 293, 341, 307], "temperature": 0.0, "avg_logprob": -0.14877089014593162, "compression_ratio": 1.6754385964912282, "no_speech_prob": 8.664284905535169e-06}, {"id": 165, "seek": 73740, "start": 760.76, "end": 761.76, "text": " what we'll be using.", "tokens": [437, 321, 603, 312, 1228, 13], "temperature": 0.0, "avg_logprob": -0.14877089014593162, "compression_ratio": 1.6754385964912282, "no_speech_prob": 8.664284905535169e-06}, {"id": 166, "seek": 73740, "start": 761.76, "end": 764.96, "text": " So I wanted to kind of go through how we construct this data set.", "tokens": [407, 286, 1415, 281, 733, 295, 352, 807, 577, 321, 7690, 341, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.14877089014593162, "compression_ratio": 1.6754385964912282, "no_speech_prob": 8.664284905535169e-06}, {"id": 167, "seek": 76496, "start": 764.96, "end": 770.76, "text": " But again, don't worry too much about the different class types.", "tokens": [583, 797, 11, 500, 380, 3292, 886, 709, 466, 264, 819, 1508, 3467, 13], "temperature": 0.0, "avg_logprob": -0.18478509491565182, "compression_ratio": 1.5739910313901346, "no_speech_prob": 2.6685436750994995e-05}, {"id": 168, "seek": 76496, "start": 770.76, "end": 775.96, "text": " You can look at our data, it's a seek to seek data bunch, has training.", "tokens": [509, 393, 574, 412, 527, 1412, 11, 309, 311, 257, 8075, 281, 8075, 1412, 3840, 11, 575, 3097, 13], "temperature": 0.0, "avg_logprob": -0.18478509491565182, "compression_ratio": 1.5739910313901346, "no_speech_prob": 2.6685436750994995e-05}, {"id": 169, "seek": 76496, "start": 775.96, "end": 780.84, "text": " Oh, sorry, I had it backwards.", "tokens": [876, 11, 2597, 11, 286, 632, 309, 12204, 13], "temperature": 0.0, "avg_logprob": -0.18478509491565182, "compression_ratio": 1.5739910313901346, "no_speech_prob": 2.6685436750994995e-05}, {"id": 170, "seek": 76496, "start": 780.84, "end": 783.6800000000001, "text": " French is the X, English is the Ys.", "tokens": [5522, 307, 264, 1783, 11, 3669, 307, 264, 398, 82, 13], "temperature": 0.0, "avg_logprob": -0.18478509491565182, "compression_ratio": 1.5739910313901346, "no_speech_prob": 2.6685436750994995e-05}, {"id": 171, "seek": 76496, "start": 783.6800000000001, "end": 788.1600000000001, "text": " So the French are the longer ones, English is the shorter ones.", "tokens": [407, 264, 5522, 366, 264, 2854, 2306, 11, 3669, 307, 264, 11639, 2306, 13], "temperature": 0.0, "avg_logprob": -0.18478509491565182, "compression_ratio": 1.5739910313901346, "no_speech_prob": 2.6685436750994995e-05}, {"id": 172, "seek": 76496, "start": 788.1600000000001, "end": 794.88, "text": " Here we can see some examples of what is light, who are we, where did we come from.", "tokens": [1692, 321, 393, 536, 512, 5110, 295, 437, 307, 1442, 11, 567, 366, 321, 11, 689, 630, 321, 808, 490, 13], "temperature": 0.0, "avg_logprob": -0.18478509491565182, "compression_ratio": 1.5739910313901346, "no_speech_prob": 2.6685436750994995e-05}, {"id": 173, "seek": 79488, "start": 794.88, "end": 798.4399999999999, "text": " What is the major aboriginal group on Vancouver Island?", "tokens": [708, 307, 264, 2563, 410, 29042, 1594, 322, 26563, 7637, 30], "temperature": 0.0, "avg_logprob": -0.18036291599273682, "compression_ratio": 1.4857142857142858, "no_speech_prob": 3.372952050995082e-05}, {"id": 174, "seek": 79488, "start": 798.4399999999999, "end": 803.72, "text": " So different questions that have been answered.", "tokens": [407, 819, 1651, 300, 362, 668, 10103, 13], "temperature": 0.0, "avg_logprob": -0.18036291599273682, "compression_ratio": 1.4857142857142858, "no_speech_prob": 3.372952050995082e-05}, {"id": 175, "seek": 79488, "start": 803.72, "end": 807.32, "text": " What would be the resulting effects on the pre-accession instrument and humanitarian", "tokens": [708, 576, 312, 264, 16505, 5065, 322, 264, 659, 12, 326, 29881, 7198, 293, 25096], "temperature": 0.0, "avg_logprob": -0.18036291599273682, "compression_ratio": 1.4857142857142858, "no_speech_prob": 3.372952050995082e-05}, {"id": 176, "seek": 79488, "start": 807.32, "end": 809.8, "text": " aid that are not co-decided?", "tokens": [9418, 300, 366, 406, 598, 12, 42821, 2112, 30], "temperature": 0.0, "avg_logprob": -0.18036291599273682, "compression_ratio": 1.4857142857142858, "no_speech_prob": 3.372952050995082e-05}, {"id": 177, "seek": 79488, "start": 809.8, "end": 818.24, "text": " So there's still like a good variety of types of questions in here.", "tokens": [407, 456, 311, 920, 411, 257, 665, 5673, 295, 3467, 295, 1651, 294, 510, 13], "temperature": 0.0, "avg_logprob": -0.18036291599273682, "compression_ratio": 1.4857142857142858, "no_speech_prob": 3.372952050995082e-05}, {"id": 178, "seek": 79488, "start": 818.24, "end": 820.24, "text": " This is, this will be key.", "tokens": [639, 307, 11, 341, 486, 312, 2141, 13], "temperature": 0.0, "avg_logprob": -0.18036291599273682, "compression_ratio": 1.4857142857142858, "no_speech_prob": 3.372952050995082e-05}, {"id": 179, "seek": 82024, "start": 820.24, "end": 826.4, "text": " So you kind of only need to run this code once and then in the future you can load it", "tokens": [407, 291, 733, 295, 787, 643, 281, 1190, 341, 3089, 1564, 293, 550, 294, 264, 2027, 291, 393, 3677, 309], "temperature": 0.0, "avg_logprob": -0.20704182492026799, "compression_ratio": 1.5895953757225434, "no_speech_prob": 1.8921493392554112e-05}, {"id": 180, "seek": 82024, "start": 826.4, "end": 829.52, "text": " from your path where you've saved it.", "tokens": [490, 428, 3100, 689, 291, 600, 6624, 309, 13], "temperature": 0.0, "avg_logprob": -0.20704182492026799, "compression_ratio": 1.5895953757225434, "no_speech_prob": 1.8921493392554112e-05}, {"id": 181, "seek": 82024, "start": 829.52, "end": 834.8, "text": " So future we don't have to go through these steps.", "tokens": [407, 2027, 321, 500, 380, 362, 281, 352, 807, 613, 4439, 13], "temperature": 0.0, "avg_logprob": -0.20704182492026799, "compression_ratio": 1.5895953757225434, "no_speech_prob": 1.8921493392554112e-05}, {"id": 182, "seek": 82024, "start": 834.8, "end": 841.72, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.20704182492026799, "compression_ratio": 1.5895953757225434, "no_speech_prob": 1.8921493392554112e-05}, {"id": 183, "seek": 82024, "start": 841.72, "end": 846.32, "text": " So now we're ready, we've kind of gotten our data ready, we're ready to create our model.", "tokens": [407, 586, 321, 434, 1919, 11, 321, 600, 733, 295, 5768, 527, 1412, 1919, 11, 321, 434, 1919, 281, 1884, 527, 2316, 13], "temperature": 0.0, "avg_logprob": -0.20704182492026799, "compression_ratio": 1.5895953757225434, "no_speech_prob": 1.8921493392554112e-05}, {"id": 184, "seek": 84632, "start": 846.32, "end": 851.2, "text": " We're going to be using pre-trained word embeddings and I believe you've covered word embeddings", "tokens": [492, 434, 516, 281, 312, 1228, 659, 12, 17227, 2001, 1349, 12240, 29432, 293, 286, 1697, 291, 600, 5343, 1349, 12240, 29432], "temperature": 0.0, "avg_logprob": -0.15536699589994765, "compression_ratio": 1.6088888888888888, "no_speech_prob": 2.545991083025001e-05}, {"id": 185, "seek": 84632, "start": 851.2, "end": 857.84, "text": " with Yanet in the, I guess, machine learning part two course.", "tokens": [365, 13633, 302, 294, 264, 11, 286, 2041, 11, 3479, 2539, 644, 732, 1164, 13], "temperature": 0.0, "avg_logprob": -0.15536699589994765, "compression_ratio": 1.6088888888888888, "no_speech_prob": 2.545991083025001e-05}, {"id": 186, "seek": 84632, "start": 857.84, "end": 866.84, "text": " We're going to use ones from Fast Text, which I believe was from Facebook.", "tokens": [492, 434, 516, 281, 764, 2306, 490, 15968, 18643, 11, 597, 286, 1697, 390, 490, 4384, 13], "temperature": 0.0, "avg_logprob": -0.15536699589994765, "compression_ratio": 1.6088888888888888, "no_speech_prob": 2.545991083025001e-05}, {"id": 187, "seek": 84632, "start": 866.84, "end": 869.2, "text": " And so we'll just download these.", "tokens": [400, 370, 321, 603, 445, 5484, 613, 13], "temperature": 0.0, "avg_logprob": -0.15536699589994765, "compression_ratio": 1.6088888888888888, "no_speech_prob": 2.545991083025001e-05}, {"id": 188, "seek": 84632, "start": 869.2, "end": 876.2800000000001, "text": " This is neat, they have pre-trained word vectors for 157 languages, which can be a useful tool", "tokens": [639, 307, 10654, 11, 436, 362, 659, 12, 17227, 2001, 1349, 18875, 337, 2119, 22, 8650, 11, 597, 393, 312, 257, 4420, 2290], "temperature": 0.0, "avg_logprob": -0.15536699589994765, "compression_ratio": 1.6088888888888888, "no_speech_prob": 2.545991083025001e-05}, {"id": 189, "seek": 87628, "start": 876.28, "end": 888.48, "text": " and then here are some links to kind of different tutorials and a workshop if you wanted to", "tokens": [293, 550, 510, 366, 512, 6123, 281, 733, 295, 819, 17616, 293, 257, 13541, 498, 291, 1415, 281], "temperature": 0.0, "avg_logprob": -0.1951467002310404, "compression_ratio": 1.5093457943925233, "no_speech_prob": 3.169002229697071e-05}, {"id": 190, "seek": 87628, "start": 888.48, "end": 892.12, "text": " review more about word embeddings.", "tokens": [3131, 544, 466, 1349, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.1951467002310404, "compression_ratio": 1.5093457943925233, "no_speech_prob": 3.169002229697071e-05}, {"id": 191, "seek": 87628, "start": 892.12, "end": 894.12, "text": " We'll need to install Fast Text.", "tokens": [492, 603, 643, 281, 3625, 15968, 18643, 13], "temperature": 0.0, "avg_logprob": -0.1951467002310404, "compression_ratio": 1.5093457943925233, "no_speech_prob": 3.169002229697071e-05}, {"id": 192, "seek": 87628, "start": 894.12, "end": 896.9599999999999, "text": " Yeah, as you can see here it's from Facebook research.", "tokens": [865, 11, 382, 291, 393, 536, 510, 309, 311, 490, 4384, 2132, 13], "temperature": 0.0, "avg_logprob": -0.1951467002310404, "compression_ratio": 1.5093457943925233, "no_speech_prob": 3.169002229697071e-05}, {"id": 193, "seek": 87628, "start": 896.9599999999999, "end": 898.56, "text": " We'll install it.", "tokens": [492, 603, 3625, 309, 13], "temperature": 0.0, "avg_logprob": -0.1951467002310404, "compression_ratio": 1.5093457943925233, "no_speech_prob": 3.169002229697071e-05}, {"id": 194, "seek": 87628, "start": 898.56, "end": 903.28, "text": " The following lines only have to be run once, which is downloading the English vector word", "tokens": [440, 3480, 3876, 787, 362, 281, 312, 1190, 1564, 11, 597, 307, 32529, 264, 3669, 8062, 1349], "temperature": 0.0, "avg_logprob": -0.1951467002310404, "compression_ratio": 1.5093457943925233, "no_speech_prob": 3.169002229697071e-05}, {"id": 195, "seek": 90328, "start": 903.28, "end": 910.24, "text": " embeddings and the French word embeddings.", "tokens": [12240, 29432, 293, 264, 5522, 1349, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.16223871417161895, "compression_ratio": 1.550561797752809, "no_speech_prob": 5.682215942215407e-06}, {"id": 196, "seek": 90328, "start": 910.24, "end": 917.4, "text": " So we load those in and kind of process them to get our embeddings in the format that we", "tokens": [407, 321, 3677, 729, 294, 293, 733, 295, 1399, 552, 281, 483, 527, 12240, 29432, 294, 264, 7877, 300, 321], "temperature": 0.0, "avg_logprob": -0.16223871417161895, "compression_ratio": 1.550561797752809, "no_speech_prob": 5.682215942215407e-06}, {"id": 197, "seek": 90328, "start": 917.4, "end": 918.4, "text": " want.", "tokens": [528, 13], "temperature": 0.0, "avg_logprob": -0.16223871417161895, "compression_ratio": 1.550561797752809, "no_speech_prob": 5.682215942215407e-06}, {"id": 198, "seek": 90328, "start": 918.4, "end": 924.0799999999999, "text": " We can look at the, and so we'll have an encoder and a decoder.", "tokens": [492, 393, 574, 412, 264, 11, 293, 370, 321, 603, 362, 364, 2058, 19866, 293, 257, 979, 19866, 13], "temperature": 0.0, "avg_logprob": -0.16223871417161895, "compression_ratio": 1.550561797752809, "no_speech_prob": 5.682215942215407e-06}, {"id": 199, "seek": 90328, "start": 924.0799999999999, "end": 928.16, "text": " Have you talked about encoders and decoders in any class previously?", "tokens": [3560, 291, 2825, 466, 2058, 378, 433, 293, 979, 378, 433, 294, 604, 1508, 8046, 30], "temperature": 0.0, "avg_logprob": -0.16223871417161895, "compression_ratio": 1.550561797752809, "no_speech_prob": 5.682215942215407e-06}, {"id": 200, "seek": 90328, "start": 928.16, "end": 929.52, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.16223871417161895, "compression_ratio": 1.550561797752809, "no_speech_prob": 5.682215942215407e-06}, {"id": 201, "seek": 92952, "start": 929.52, "end": 935.92, "text": " I see some maybes or a little bit, so we'll talk about that some more.", "tokens": [286, 536, 512, 815, 6446, 420, 257, 707, 857, 11, 370, 321, 603, 751, 466, 300, 512, 544, 13], "temperature": 0.0, "avg_logprob": -0.30080801248550415, "compression_ratio": 1.5654205607476634, "no_speech_prob": 2.7103815227746964e-05}, {"id": 202, "seek": 92952, "start": 935.92, "end": 937.48, "text": " So an encoder is...", "tokens": [407, 364, 2058, 19866, 307, 485], "temperature": 0.0, "avg_logprob": -0.30080801248550415, "compression_ratio": 1.5654205607476634, "no_speech_prob": 2.7103815227746964e-05}, {"id": 203, "seek": 92952, "start": 937.48, "end": 946.3199999999999, "text": " Okay, yeah, so I will tell you more about the...", "tokens": [1033, 11, 1338, 11, 370, 286, 486, 980, 291, 544, 466, 264, 485], "temperature": 0.0, "avg_logprob": -0.30080801248550415, "compression_ratio": 1.5654205607476634, "no_speech_prob": 2.7103815227746964e-05}, {"id": 204, "seek": 92952, "start": 946.3199999999999, "end": 947.96, "text": " Let me see if I have the picture.", "tokens": [961, 385, 536, 498, 286, 362, 264, 3036, 13], "temperature": 0.0, "avg_logprob": -0.30080801248550415, "compression_ratio": 1.5654205607476634, "no_speech_prob": 2.7103815227746964e-05}, {"id": 205, "seek": 92952, "start": 947.96, "end": 950.76, "text": " Okay, yeah, here's the picture.", "tokens": [1033, 11, 1338, 11, 510, 311, 264, 3036, 13], "temperature": 0.0, "avg_logprob": -0.30080801248550415, "compression_ratio": 1.5654205607476634, "no_speech_prob": 2.7103815227746964e-05}, {"id": 206, "seek": 92952, "start": 950.76, "end": 956.56, "text": " And this is from a great blog post that Stephen Marity wrote and he wrote specifically about", "tokens": [400, 341, 307, 490, 257, 869, 6968, 2183, 300, 13391, 2039, 507, 4114, 293, 415, 4114, 4682, 466], "temperature": 0.0, "avg_logprob": -0.30080801248550415, "compression_ratio": 1.5654205607476634, "no_speech_prob": 2.7103815227746964e-05}, {"id": 207, "seek": 92952, "start": 956.56, "end": 958.4399999999999, "text": " Google's neural machine translation.", "tokens": [3329, 311, 18161, 3479, 12853, 13], "temperature": 0.0, "avg_logprob": -0.30080801248550415, "compression_ratio": 1.5654205607476634, "no_speech_prob": 2.7103815227746964e-05}, {"id": 208, "seek": 95844, "start": 958.44, "end": 961.0, "text": " But the idea is kind of...", "tokens": [583, 264, 1558, 307, 733, 295, 485], "temperature": 0.0, "avg_logprob": -0.1734324195168235, "compression_ratio": 1.7285067873303168, "no_speech_prob": 1.0129358997801319e-05}, {"id": 209, "seek": 95844, "start": 961.0, "end": 963.0, "text": " So here he's going from English to German.", "tokens": [407, 510, 415, 311, 516, 490, 3669, 281, 6521, 13], "temperature": 0.0, "avg_logprob": -0.1734324195168235, "compression_ratio": 1.7285067873303168, "no_speech_prob": 1.0129358997801319e-05}, {"id": 210, "seek": 95844, "start": 963.0, "end": 965.44, "text": " We'll be going from French to English.", "tokens": [492, 603, 312, 516, 490, 5522, 281, 3669, 13], "temperature": 0.0, "avg_logprob": -0.1734324195168235, "compression_ratio": 1.7285067873303168, "no_speech_prob": 1.0129358997801319e-05}, {"id": 211, "seek": 95844, "start": 965.44, "end": 972.5200000000001, "text": " That you'll take your embeddings, kind of put them through this encoder and get out", "tokens": [663, 291, 603, 747, 428, 12240, 29432, 11, 733, 295, 829, 552, 807, 341, 2058, 19866, 293, 483, 484], "temperature": 0.0, "avg_logprob": -0.1734324195168235, "compression_ratio": 1.7285067873303168, "no_speech_prob": 1.0129358997801319e-05}, {"id": 212, "seek": 95844, "start": 972.5200000000001, "end": 973.5200000000001, "text": " a hidden state.", "tokens": [257, 7633, 1785, 13], "temperature": 0.0, "avg_logprob": -0.1734324195168235, "compression_ratio": 1.7285067873303168, "no_speech_prob": 1.0129358997801319e-05}, {"id": 213, "seek": 95844, "start": 973.5200000000001, "end": 979.72, "text": " And note here kind of all these...", "tokens": [400, 3637, 510, 733, 295, 439, 613, 485], "temperature": 0.0, "avg_logprob": -0.1734324195168235, "compression_ratio": 1.7285067873303168, "no_speech_prob": 1.0129358997801319e-05}, {"id": 214, "seek": 95844, "start": 979.72, "end": 982.96, "text": " This is an RNN and so all the words are going in.", "tokens": [639, 307, 364, 45702, 45, 293, 370, 439, 264, 2283, 366, 516, 294, 13], "temperature": 0.0, "avg_logprob": -0.1734324195168235, "compression_ratio": 1.7285067873303168, "no_speech_prob": 1.0129358997801319e-05}, {"id": 215, "seek": 95844, "start": 982.96, "end": 987.1600000000001, "text": " He loved to eat, are all going through the encoder and at the end you're kind of getting", "tokens": [634, 4333, 281, 1862, 11, 366, 439, 516, 807, 264, 2058, 19866, 293, 412, 264, 917, 291, 434, 733, 295, 1242], "temperature": 0.0, "avg_logprob": -0.1734324195168235, "compression_ratio": 1.7285067873303168, "no_speech_prob": 1.0129358997801319e-05}, {"id": 216, "seek": 98716, "start": 987.16, "end": 993.12, "text": " out this one hidden state that then you'll put through the decoder to get back into...", "tokens": [484, 341, 472, 7633, 1785, 300, 550, 291, 603, 829, 807, 264, 979, 19866, 281, 483, 646, 666, 485], "temperature": 0.0, "avg_logprob": -0.21330769227282836, "compression_ratio": 1.6222222222222222, "no_speech_prob": 8.013050319277681e-06}, {"id": 217, "seek": 98716, "start": 993.12, "end": 999.16, "text": " Or not back, but get into German embeddings.", "tokens": [1610, 406, 646, 11, 457, 483, 666, 6521, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.21330769227282836, "compression_ratio": 1.6222222222222222, "no_speech_prob": 8.013050319277681e-06}, {"id": 218, "seek": 98716, "start": 999.16, "end": 1003.76, "text": " Actually I'll come back up.", "tokens": [5135, 286, 603, 808, 646, 493, 13], "temperature": 0.0, "avg_logprob": -0.21330769227282836, "compression_ratio": 1.6222222222222222, "no_speech_prob": 8.013050319277681e-06}, {"id": 219, "seek": 98716, "start": 1003.76, "end": 1008.28, "text": " But the encoder is an RNN that we feed our input to.", "tokens": [583, 264, 2058, 19866, 307, 364, 45702, 45, 300, 321, 3154, 527, 4846, 281, 13], "temperature": 0.0, "avg_logprob": -0.21330769227282836, "compression_ratio": 1.6222222222222222, "no_speech_prob": 8.013050319277681e-06}, {"id": 220, "seek": 98716, "start": 1008.28, "end": 1010.16, "text": " For now we're not worrying about the output.", "tokens": [1171, 586, 321, 434, 406, 18788, 466, 264, 5598, 13], "temperature": 0.0, "avg_logprob": -0.21330769227282836, "compression_ratio": 1.6222222222222222, "no_speech_prob": 8.013050319277681e-06}, {"id": 221, "seek": 98716, "start": 1010.16, "end": 1012.72, "text": " You can imagine outputs coming out of here as well.", "tokens": [509, 393, 3811, 23930, 1348, 484, 295, 510, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.21330769227282836, "compression_ratio": 1.6222222222222222, "no_speech_prob": 8.013050319277681e-06}, {"id": 222, "seek": 98716, "start": 1012.72, "end": 1016.04, "text": " In addition, kind of getting an output from each state.", "tokens": [682, 4500, 11, 733, 295, 1242, 364, 5598, 490, 1184, 1785, 13], "temperature": 0.0, "avg_logprob": -0.21330769227282836, "compression_ratio": 1.6222222222222222, "no_speech_prob": 8.013050319277681e-06}, {"id": 223, "seek": 101604, "start": 1016.04, "end": 1019.5999999999999, "text": " Here we're just kind of keeping the state at the end.", "tokens": [1692, 321, 434, 445, 733, 295, 5145, 264, 1785, 412, 264, 917, 13], "temperature": 0.0, "avg_logprob": -0.12843998356869346, "compression_ratio": 1.6218905472636815, "no_speech_prob": 4.832144986721687e-05}, {"id": 224, "seek": 101604, "start": 1019.5999999999999, "end": 1021.64, "text": " This is a hidden state.", "tokens": [639, 307, 257, 7633, 1785, 13], "temperature": 0.0, "avg_logprob": -0.12843998356869346, "compression_ratio": 1.6218905472636815, "no_speech_prob": 4.832144986721687e-05}, {"id": 225, "seek": 101604, "start": 1021.64, "end": 1023.0799999999999, "text": " This is just a set of activations.", "tokens": [639, 307, 445, 257, 992, 295, 2430, 763, 13], "temperature": 0.0, "avg_logprob": -0.12843998356869346, "compression_ratio": 1.6218905472636815, "no_speech_prob": 4.832144986721687e-05}, {"id": 226, "seek": 101604, "start": 1023.0799999999999, "end": 1028.52, "text": " Again, thinking of a neural network is just parameters and activations.", "tokens": [3764, 11, 1953, 295, 257, 18161, 3209, 307, 445, 9834, 293, 2430, 763, 13], "temperature": 0.0, "avg_logprob": -0.12843998356869346, "compression_ratio": 1.6218905472636815, "no_speech_prob": 4.832144986721687e-05}, {"id": 227, "seek": 101604, "start": 1028.52, "end": 1032.08, "text": " That hidden state is given to the decoder, which is a second RNN.", "tokens": [663, 7633, 1785, 307, 2212, 281, 264, 979, 19866, 11, 597, 307, 257, 1150, 45702, 45, 13], "temperature": 0.0, "avg_logprob": -0.12843998356869346, "compression_ratio": 1.6218905472636815, "no_speech_prob": 4.832144986721687e-05}, {"id": 228, "seek": 101604, "start": 1032.08, "end": 1035.24, "text": " So we have two RNNs going on here.", "tokens": [407, 321, 362, 732, 45702, 45, 82, 516, 322, 510, 13], "temperature": 0.0, "avg_logprob": -0.12843998356869346, "compression_ratio": 1.6218905472636815, "no_speech_prob": 4.832144986721687e-05}, {"id": 229, "seek": 101604, "start": 1035.24, "end": 1041.36, "text": " And that uses it to get the translation.", "tokens": [400, 300, 4960, 309, 281, 483, 264, 12853, 13], "temperature": 0.0, "avg_logprob": -0.12843998356869346, "compression_ratio": 1.6218905472636815, "no_speech_prob": 4.832144986721687e-05}, {"id": 230, "seek": 104136, "start": 1041.36, "end": 1047.9599999999998, "text": " And we keep going until the decoder produces either a padding token or at 30 iterations", "tokens": [400, 321, 1066, 516, 1826, 264, 979, 19866, 14725, 2139, 257, 39562, 14862, 420, 412, 2217, 36540], "temperature": 0.0, "avg_logprob": -0.14994481976112622, "compression_ratio": 1.5405405405405406, "no_speech_prob": 4.611008625943214e-05}, {"id": 231, "seek": 104136, "start": 1047.9599999999998, "end": 1048.9599999999998, "text": " will stop.", "tokens": [486, 1590, 13], "temperature": 0.0, "avg_logprob": -0.14994481976112622, "compression_ratio": 1.5405405405405406, "no_speech_prob": 4.611008625943214e-05}, {"id": 232, "seek": 104136, "start": 1048.9599999999998, "end": 1053.4799999999998, "text": " Since we've limited our size to 30, we want to avoid an infinite loop or an infinitely", "tokens": [4162, 321, 600, 5567, 527, 2744, 281, 2217, 11, 321, 528, 281, 5042, 364, 13785, 6367, 420, 364, 36227], "temperature": 0.0, "avg_logprob": -0.14994481976112622, "compression_ratio": 1.5405405405405406, "no_speech_prob": 4.611008625943214e-05}, {"id": 233, "seek": 104136, "start": 1053.4799999999998, "end": 1059.08, "text": " long sentence here.", "tokens": [938, 8174, 510, 13], "temperature": 0.0, "avg_logprob": -0.14994481976112622, "compression_ratio": 1.5405405405405406, "no_speech_prob": 4.611008625943214e-05}, {"id": 234, "seek": 104136, "start": 1059.08, "end": 1061.32, "text": " So that's what we're creating here is kind of our...", "tokens": [407, 300, 311, 437, 321, 434, 4084, 510, 307, 733, 295, 527, 485], "temperature": 0.0, "avg_logprob": -0.14994481976112622, "compression_ratio": 1.5405405405405406, "no_speech_prob": 4.611008625943214e-05}, {"id": 235, "seek": 104136, "start": 1061.32, "end": 1065.56, "text": " Oh, I should say these are our embeddings that will feed into the encoder and these", "tokens": [876, 11, 286, 820, 584, 613, 366, 527, 12240, 29432, 300, 486, 3154, 666, 264, 2058, 19866, 293, 613], "temperature": 0.0, "avg_logprob": -0.14994481976112622, "compression_ratio": 1.5405405405405406, "no_speech_prob": 4.611008625943214e-05}, {"id": 236, "seek": 106556, "start": 1065.56, "end": 1075.1599999999999, "text": " are the embeddings that will be fed into the decoder.", "tokens": [366, 264, 12240, 29432, 300, 486, 312, 4636, 666, 264, 979, 19866, 13], "temperature": 0.0, "avg_logprob": -0.19906753217670278, "compression_ratio": 1.4591194968553458, "no_speech_prob": 1.384569623041898e-05}, {"id": 237, "seek": 106556, "start": 1075.1599999999999, "end": 1082.1599999999999, "text": " In this example, we'll use GRU for our encoder and a second GRU for our decoder.", "tokens": [682, 341, 1365, 11, 321, 603, 764, 10903, 52, 337, 527, 2058, 19866, 293, 257, 1150, 10903, 52, 337, 527, 979, 19866, 13], "temperature": 0.0, "avg_logprob": -0.19906753217670278, "compression_ratio": 1.4591194968553458, "no_speech_prob": 1.384569623041898e-05}, {"id": 238, "seek": 106556, "start": 1082.1599999999999, "end": 1086.8, "text": " And as I said, we'll come back to GRUs.", "tokens": [400, 382, 286, 848, 11, 321, 603, 808, 646, 281, 10903, 29211, 13], "temperature": 0.0, "avg_logprob": -0.19906753217670278, "compression_ratio": 1.4591194968553458, "no_speech_prob": 1.384569623041898e-05}, {"id": 239, "seek": 106556, "start": 1086.8, "end": 1091.44, "text": " Although if you're interested, there are some links here.", "tokens": [5780, 498, 291, 434, 3102, 11, 456, 366, 512, 6123, 510, 13], "temperature": 0.0, "avg_logprob": -0.19906753217670278, "compression_ratio": 1.4591194968553458, "no_speech_prob": 1.384569623041898e-05}, {"id": 240, "seek": 109144, "start": 1091.44, "end": 1101.3600000000001, "text": " Yeah, so here is an explanation of LSTMs and GRUs if you did want to read up.", "tokens": [865, 11, 370, 510, 307, 364, 10835, 295, 441, 6840, 26386, 293, 10903, 29211, 498, 291, 630, 528, 281, 1401, 493, 13], "temperature": 0.0, "avg_logprob": -0.2479226485542629, "compression_ratio": 1.3522012578616351, "no_speech_prob": 3.822644066531211e-05}, {"id": 241, "seek": 109144, "start": 1101.3600000000001, "end": 1108.4, "text": " All right, so this...", "tokens": [1057, 558, 11, 370, 341, 485], "temperature": 0.0, "avg_logprob": -0.2479226485542629, "compression_ratio": 1.3522012578616351, "no_speech_prob": 3.822644066531211e-05}, {"id": 242, "seek": 109144, "start": 1108.4, "end": 1117.4, "text": " All right, so here for our seek to seek RNN, I'll come back to...", "tokens": [1057, 558, 11, 370, 510, 337, 527, 8075, 281, 8075, 45702, 45, 11, 286, 603, 808, 646, 281, 485], "temperature": 0.0, "avg_logprob": -0.2479226485542629, "compression_ratio": 1.3522012578616351, "no_speech_prob": 3.822644066531211e-05}, {"id": 243, "seek": 109144, "start": 1117.4, "end": 1120.4, "text": " So we're kind of initializing a number of things.", "tokens": [407, 321, 434, 733, 295, 5883, 3319, 257, 1230, 295, 721, 13], "temperature": 0.0, "avg_logprob": -0.2479226485542629, "compression_ratio": 1.3522012578616351, "no_speech_prob": 3.822644066531211e-05}, {"id": 244, "seek": 112040, "start": 1120.4, "end": 1125.3600000000001, "text": " I want to talk about what the forward step does first though, since I think that'll be", "tokens": [286, 528, 281, 751, 466, 437, 264, 2128, 1823, 775, 700, 1673, 11, 1670, 286, 519, 300, 603, 312], "temperature": 0.0, "avg_logprob": -0.13111479337825332, "compression_ratio": 1.6321243523316062, "no_speech_prob": 4.19849093304947e-05}, {"id": 245, "seek": 112040, "start": 1125.3600000000001, "end": 1128.0400000000002, "text": " the clearest.", "tokens": [264, 1233, 17363, 13], "temperature": 0.0, "avg_logprob": -0.13111479337825332, "compression_ratio": 1.6321243523316062, "no_speech_prob": 4.19849093304947e-05}, {"id": 246, "seek": 112040, "start": 1128.0400000000002, "end": 1132.8400000000001, "text": " The forward step is going to put the batch size and the input into an encoder and get", "tokens": [440, 2128, 1823, 307, 516, 281, 829, 264, 15245, 2744, 293, 264, 4846, 666, 364, 2058, 19866, 293, 483], "temperature": 0.0, "avg_logprob": -0.13111479337825332, "compression_ratio": 1.6321243523316062, "no_speech_prob": 4.19849093304947e-05}, {"id": 247, "seek": 112040, "start": 1132.8400000000001, "end": 1136.4, "text": " out a hidden state.", "tokens": [484, 257, 7633, 1785, 13], "temperature": 0.0, "avg_logprob": -0.13111479337825332, "compression_ratio": 1.6321243523316062, "no_speech_prob": 4.19849093304947e-05}, {"id": 248, "seek": 112040, "start": 1136.4, "end": 1142.96, "text": " Our decoder input, we kind of need to initialize to be the right size and to start with beginning", "tokens": [2621, 979, 19866, 4846, 11, 321, 733, 295, 643, 281, 5883, 1125, 281, 312, 264, 558, 2744, 293, 281, 722, 365, 2863], "temperature": 0.0, "avg_logprob": -0.13111479337825332, "compression_ratio": 1.6321243523316062, "no_speech_prob": 4.19849093304947e-05}, {"id": 249, "seek": 112040, "start": 1142.96, "end": 1145.1200000000001, "text": " of string.", "tokens": [295, 6798, 13], "temperature": 0.0, "avg_logprob": -0.13111479337825332, "compression_ratio": 1.6321243523316062, "no_speech_prob": 4.19849093304947e-05}, {"id": 250, "seek": 114512, "start": 1145.12, "end": 1151.9199999999998, "text": " And then we'll go through a for loop where in each step we're putting our decoder input", "tokens": [400, 550, 321, 603, 352, 807, 257, 337, 6367, 689, 294, 1184, 1823, 321, 434, 3372, 527, 979, 19866, 4846], "temperature": 0.0, "avg_logprob": -0.08423933982849122, "compression_ratio": 1.744186046511628, "no_speech_prob": 1.0783043762785383e-05}, {"id": 251, "seek": 114512, "start": 1151.9199999999998, "end": 1157.8799999999999, "text": " and our hidden state, which we got from the encoder just right here.", "tokens": [293, 527, 7633, 1785, 11, 597, 321, 658, 490, 264, 2058, 19866, 445, 558, 510, 13], "temperature": 0.0, "avg_logprob": -0.08423933982849122, "compression_ratio": 1.744186046511628, "no_speech_prob": 1.0783043762785383e-05}, {"id": 252, "seek": 114512, "start": 1157.8799999999999, "end": 1165.3999999999999, "text": " We're putting those into the decoder and we're getting out the hidden state and an output.", "tokens": [492, 434, 3372, 729, 666, 264, 979, 19866, 293, 321, 434, 1242, 484, 264, 7633, 1785, 293, 364, 5598, 13], "temperature": 0.0, "avg_logprob": -0.08423933982849122, "compression_ratio": 1.744186046511628, "no_speech_prob": 1.0783043762785383e-05}, {"id": 253, "seek": 114512, "start": 1165.3999999999999, "end": 1171.9199999999998, "text": " We'll take the max of that to get the decoder input.", "tokens": [492, 603, 747, 264, 11469, 295, 300, 281, 483, 264, 979, 19866, 4846, 13], "temperature": 0.0, "avg_logprob": -0.08423933982849122, "compression_ratio": 1.744186046511628, "no_speech_prob": 1.0783043762785383e-05}, {"id": 254, "seek": 117192, "start": 1171.92, "end": 1180.48, "text": " We want to hold on to kind of what our output was from the decoder.", "tokens": [492, 528, 281, 1797, 322, 281, 733, 295, 437, 527, 5598, 390, 490, 264, 979, 19866, 13], "temperature": 0.0, "avg_logprob": -0.1158194798295216, "compression_ratio": 1.645631067961165, "no_speech_prob": 1.3211739315011073e-05}, {"id": 255, "seek": 117192, "start": 1180.48, "end": 1187.6000000000001, "text": " Then if we have a padding token, we'll break out of this and at the end we'll kind of return", "tokens": [1396, 498, 321, 362, 257, 39562, 14862, 11, 321, 603, 1821, 484, 295, 341, 293, 412, 264, 917, 321, 603, 733, 295, 2736], "temperature": 0.0, "avg_logprob": -0.1158194798295216, "compression_ratio": 1.645631067961165, "no_speech_prob": 1.3211739315011073e-05}, {"id": 256, "seek": 117192, "start": 1187.6000000000001, "end": 1189.28, "text": " our results.", "tokens": [527, 3542, 13], "temperature": 0.0, "avg_logprob": -0.1158194798295216, "compression_ratio": 1.645631067961165, "no_speech_prob": 1.3211739315011073e-05}, {"id": 257, "seek": 117192, "start": 1189.28, "end": 1195.3600000000001, "text": " So the kind of high level what's going on is you're putting your input into an encoder", "tokens": [407, 264, 733, 295, 1090, 1496, 437, 311, 516, 322, 307, 291, 434, 3372, 428, 4846, 666, 364, 2058, 19866], "temperature": 0.0, "avg_logprob": -0.1158194798295216, "compression_ratio": 1.645631067961165, "no_speech_prob": 1.3211739315011073e-05}, {"id": 258, "seek": 117192, "start": 1195.3600000000001, "end": 1200.48, "text": " and then in a for loop you are getting something out of the decoder each time.", "tokens": [293, 550, 294, 257, 337, 6367, 291, 366, 1242, 746, 484, 295, 264, 979, 19866, 1184, 565, 13], "temperature": 0.0, "avg_logprob": -0.1158194798295216, "compression_ratio": 1.645631067961165, "no_speech_prob": 1.3211739315011073e-05}, {"id": 259, "seek": 120048, "start": 1200.48, "end": 1209.76, "text": " Then kind of digging into what are the encoder and the decoder doing, the encoder is taking", "tokens": [1396, 733, 295, 17343, 666, 437, 366, 264, 2058, 19866, 293, 264, 979, 19866, 884, 11, 264, 2058, 19866, 307, 1940], "temperature": 0.0, "avg_logprob": -0.17551165041716202, "compression_ratio": 1.83248730964467, "no_speech_prob": 1.5689343854319304e-05}, {"id": 260, "seek": 120048, "start": 1209.76, "end": 1214.28, "text": " the embedding that we had prepared for the encoder.", "tokens": [264, 12240, 3584, 300, 321, 632, 4927, 337, 264, 2058, 19866, 13], "temperature": 0.0, "avg_logprob": -0.17551165041716202, "compression_ratio": 1.83248730964467, "no_speech_prob": 1.5689343854319304e-05}, {"id": 261, "seek": 120048, "start": 1214.28, "end": 1219.24, "text": " So when we put the input into the embedding that's basically taking the French words and", "tokens": [407, 562, 321, 829, 264, 4846, 666, 264, 12240, 3584, 300, 311, 1936, 1940, 264, 5522, 2283, 293], "temperature": 0.0, "avg_logprob": -0.17551165041716202, "compression_ratio": 1.83248730964467, "no_speech_prob": 1.5689343854319304e-05}, {"id": 262, "seek": 120048, "start": 1219.24, "end": 1226.68, "text": " then picking out what were the vectors, word embeddings for those is what you get here.", "tokens": [550, 8867, 484, 437, 645, 264, 18875, 11, 1349, 12240, 29432, 337, 729, 307, 437, 291, 483, 510, 13], "temperature": 0.0, "avg_logprob": -0.17551165041716202, "compression_ratio": 1.83248730964467, "no_speech_prob": 1.5689343854319304e-05}, {"id": 263, "seek": 120048, "start": 1226.68, "end": 1229.2, "text": " And then we're applying dropout to that.", "tokens": [400, 550, 321, 434, 9275, 3270, 346, 281, 300, 13], "temperature": 0.0, "avg_logprob": -0.17551165041716202, "compression_ratio": 1.83248730964467, "no_speech_prob": 1.5689343854319304e-05}, {"id": 264, "seek": 122920, "start": 1229.2, "end": 1234.16, "text": " Since we talked about dropout, it's important for regularizing.", "tokens": [4162, 321, 2825, 466, 3270, 346, 11, 309, 311, 1021, 337, 3890, 3319, 13], "temperature": 0.0, "avg_logprob": -0.12772158954454504, "compression_ratio": 1.509090909090909, "no_speech_prob": 1.92226634680992e-05}, {"id": 265, "seek": 122920, "start": 1234.16, "end": 1240.0800000000002, "text": " Then we put that into our first GRU since this uses two different GRUs.", "tokens": [1396, 321, 829, 300, 666, 527, 700, 10903, 52, 1670, 341, 4960, 732, 819, 10903, 29211, 13], "temperature": 0.0, "avg_logprob": -0.12772158954454504, "compression_ratio": 1.509090909090909, "no_speech_prob": 1.92226634680992e-05}, {"id": 266, "seek": 122920, "start": 1240.0800000000002, "end": 1243.04, "text": " This is the one we're learning for the encoder.", "tokens": [639, 307, 264, 472, 321, 434, 2539, 337, 264, 2058, 19866, 13], "temperature": 0.0, "avg_logprob": -0.12772158954454504, "compression_ratio": 1.509090909090909, "no_speech_prob": 1.92226634680992e-05}, {"id": 267, "seek": 122920, "start": 1243.04, "end": 1251.4, "text": " We'll want the hidden state of what came before as well as the embedding.", "tokens": [492, 603, 528, 264, 7633, 1785, 295, 437, 1361, 949, 382, 731, 382, 264, 12240, 3584, 13], "temperature": 0.0, "avg_logprob": -0.12772158954454504, "compression_ratio": 1.509090909090909, "no_speech_prob": 1.92226634680992e-05}, {"id": 268, "seek": 122920, "start": 1251.4, "end": 1254.88, "text": " And then out encoder, so you can look up here, that's just a linear layer.", "tokens": [400, 550, 484, 2058, 19866, 11, 370, 291, 393, 574, 493, 510, 11, 300, 311, 445, 257, 8213, 4583, 13], "temperature": 0.0, "avg_logprob": -0.12772158954454504, "compression_ratio": 1.509090909090909, "no_speech_prob": 1.92226634680992e-05}, {"id": 269, "seek": 125488, "start": 1254.88, "end": 1259.3600000000001, "text": " So we'll put that through a linear layer and then that's kind of returning our final hidden", "tokens": [407, 321, 603, 829, 300, 807, 257, 8213, 4583, 293, 550, 300, 311, 733, 295, 12678, 527, 2572, 7633], "temperature": 0.0, "avg_logprob": -0.17207049214562706, "compression_ratio": 1.5687203791469195, "no_speech_prob": 1.2218581105116755e-05}, {"id": 270, "seek": 125488, "start": 1259.3600000000001, "end": 1262.2, "text": " state.", "tokens": [1785, 13], "temperature": 0.0, "avg_logprob": -0.17207049214562706, "compression_ratio": 1.5687203791469195, "no_speech_prob": 1.2218581105116755e-05}, {"id": 271, "seek": 125488, "start": 1262.2, "end": 1265.6000000000001, "text": " Any questions about what the encoder does?", "tokens": [2639, 1651, 466, 437, 264, 2058, 19866, 775, 30], "temperature": 0.0, "avg_logprob": -0.17207049214562706, "compression_ratio": 1.5687203791469195, "no_speech_prob": 1.2218581105116755e-05}, {"id": 272, "seek": 125488, "start": 1265.6000000000001, "end": 1273.8400000000001, "text": " I think this can feel tricky, like I know this is a really long definition for our seek", "tokens": [286, 519, 341, 393, 841, 12414, 11, 411, 286, 458, 341, 307, 257, 534, 938, 7123, 337, 527, 8075], "temperature": 0.0, "avg_logprob": -0.17207049214562706, "compression_ratio": 1.5687203791469195, "no_speech_prob": 1.2218581105116755e-05}, {"id": 273, "seek": 125488, "start": 1273.8400000000001, "end": 1275.8000000000002, "text": " to seek class.", "tokens": [281, 8075, 1508, 13], "temperature": 0.0, "avg_logprob": -0.17207049214562706, "compression_ratio": 1.5687203791469195, "no_speech_prob": 1.2218581105116755e-05}, {"id": 274, "seek": 125488, "start": 1275.8000000000002, "end": 1279.8400000000001, "text": " It's helpful to keep in mind that it's kind of mostly composed of things that you know", "tokens": [467, 311, 4961, 281, 1066, 294, 1575, 300, 309, 311, 733, 295, 5240, 18204, 295, 721, 300, 291, 458], "temperature": 0.0, "avg_logprob": -0.17207049214562706, "compression_ratio": 1.5687203791469195, "no_speech_prob": 1.2218581105116755e-05}, {"id": 275, "seek": 127984, "start": 1279.84, "end": 1289.1599999999999, "text": " other than GRU, which we'll cover next time, but it's looking things up in the set of word", "tokens": [661, 813, 10903, 52, 11, 597, 321, 603, 2060, 958, 565, 11, 457, 309, 311, 1237, 721, 493, 294, 264, 992, 295, 1349], "temperature": 0.0, "avg_logprob": -0.1508594669707834, "compression_ratio": 1.4861878453038675, "no_speech_prob": 7.484132947865874e-05}, {"id": 276, "seek": 127984, "start": 1289.1599999999999, "end": 1301.6, "text": " embeddings that we've got and then it's got dropout, it's linear layers, some nonlinearities.", "tokens": [12240, 29432, 300, 321, 600, 658, 293, 550, 309, 311, 658, 3270, 346, 11, 309, 311, 8213, 7914, 11, 512, 2107, 28263, 1088, 13], "temperature": 0.0, "avg_logprob": -0.1508594669707834, "compression_ratio": 1.4861878453038675, "no_speech_prob": 7.484132947865874e-05}, {"id": 277, "seek": 127984, "start": 1301.6, "end": 1307.52, "text": " Even though it's got many pieces, you know what most of these individual pieces are.", "tokens": [2754, 1673, 309, 311, 658, 867, 3755, 11, 291, 458, 437, 881, 295, 613, 2609, 3755, 366, 13], "temperature": 0.0, "avg_logprob": -0.1508594669707834, "compression_ratio": 1.4861878453038675, "no_speech_prob": 7.484132947865874e-05}, {"id": 278, "seek": 130752, "start": 1307.52, "end": 1315.12, "text": " And then for the decoder, we get the decoder input.", "tokens": [400, 550, 337, 264, 979, 19866, 11, 321, 483, 264, 979, 19866, 4846, 13], "temperature": 0.0, "avg_logprob": -0.3815549503673207, "compression_ratio": 1.4834437086092715, "no_speech_prob": 1.723060995573178e-05}, {"id": 279, "seek": 130752, "start": 1315.12, "end": 1320.68, "text": " Does anyone know what unsqueeze is?", "tokens": [4402, 2878, 458, 437, 2693, 1077, 10670, 307, 30], "temperature": 0.0, "avg_logprob": -0.3815549503673207, "compression_ratio": 1.4834437086092715, "no_speech_prob": 1.723060995573178e-05}, {"id": 280, "seek": 130752, "start": 1320.68, "end": 1325.24, "text": " Do you want to say it louder?", "tokens": [1144, 291, 528, 281, 584, 309, 22717, 30], "temperature": 0.0, "avg_logprob": -0.3815549503673207, "compression_ratio": 1.4834437086092715, "no_speech_prob": 1.723060995573178e-05}, {"id": 281, "seek": 130752, "start": 1325.24, "end": 1328.16, "text": " Increase the dimension?", "tokens": [30367, 651, 264, 10139, 30], "temperature": 0.0, "avg_logprob": -0.3815549503673207, "compression_ratio": 1.4834437086092715, "no_speech_prob": 1.723060995573178e-05}, {"id": 282, "seek": 130752, "start": 1328.16, "end": 1337.4, "text": " Yes, I believe that adds a dimension, where a squeeze kind of reduces a dimension.", "tokens": [1079, 11, 286, 1697, 300, 10860, 257, 10139, 11, 689, 257, 13578, 733, 295, 18081, 257, 10139, 13], "temperature": 0.0, "avg_logprob": -0.3815549503673207, "compression_ratio": 1.4834437086092715, "no_speech_prob": 1.723060995573178e-05}, {"id": 283, "seek": 133740, "start": 1337.4, "end": 1346.68, "text": " And then we put our embedding and the hidden state through our GRU decoder.", "tokens": [400, 550, 321, 829, 527, 12240, 3584, 293, 264, 7633, 1785, 807, 527, 10903, 52, 979, 19866, 13], "temperature": 0.0, "avg_logprob": -0.18859318841861772, "compression_ratio": 1.5856353591160222, "no_speech_prob": 1.1125224773422815e-05}, {"id": 284, "seek": 133740, "start": 1346.68, "end": 1349.8400000000001, "text": " This is our second GRU.", "tokens": [639, 307, 527, 1150, 10903, 52, 13], "temperature": 0.0, "avg_logprob": -0.18859318841861772, "compression_ratio": 1.5856353591160222, "no_speech_prob": 1.1125224773422815e-05}, {"id": 285, "seek": 133740, "start": 1349.8400000000001, "end": 1351.76, "text": " Apply some dropout.", "tokens": [25264, 512, 3270, 346, 13], "temperature": 0.0, "avg_logprob": -0.18859318841861772, "compression_ratio": 1.5856353591160222, "no_speech_prob": 1.1125224773422815e-05}, {"id": 286, "seek": 133740, "start": 1351.76, "end": 1360.68, "text": " Then apply self.out, which is another linear layer and get our output from there.", "tokens": [1396, 3079, 2698, 13, 346, 11, 597, 307, 1071, 8213, 4583, 293, 483, 527, 5598, 490, 456, 13], "temperature": 0.0, "avg_logprob": -0.18859318841861772, "compression_ratio": 1.5856353591160222, "no_speech_prob": 1.1125224773422815e-05}, {"id": 287, "seek": 133740, "start": 1360.68, "end": 1365.72, "text": " So we'll be returning the hidden state and the output and we're going to kind of keep", "tokens": [407, 321, 603, 312, 12678, 264, 7633, 1785, 293, 264, 5598, 293, 321, 434, 516, 281, 733, 295, 1066], "temperature": 0.0, "avg_logprob": -0.18859318841861772, "compression_ratio": 1.5856353591160222, "no_speech_prob": 1.1125224773422815e-05}, {"id": 288, "seek": 136572, "start": 1365.72, "end": 1371.04, "text": " track of the outputs and then the hidden state will be used again the next time we go through", "tokens": [2837, 295, 264, 23930, 293, 550, 264, 7633, 1785, 486, 312, 1143, 797, 264, 958, 565, 321, 352, 807], "temperature": 0.0, "avg_logprob": -0.10086345672607422, "compression_ratio": 1.5942028985507246, "no_speech_prob": 1.3630529792862944e-05}, {"id": 289, "seek": 136572, "start": 1371.04, "end": 1379.84, "text": " our for loop since we're holding on to our history here.", "tokens": [527, 337, 6367, 1670, 321, 434, 5061, 322, 281, 527, 2503, 510, 13], "temperature": 0.0, "avg_logprob": -0.10086345672607422, "compression_ratio": 1.5942028985507246, "no_speech_prob": 1.3630529792862944e-05}, {"id": 290, "seek": 136572, "start": 1379.84, "end": 1382.44, "text": " Here you can kind of look at our RNN.", "tokens": [1692, 291, 393, 733, 295, 574, 412, 527, 45702, 45, 13], "temperature": 0.0, "avg_logprob": -0.10086345672607422, "compression_ratio": 1.5942028985507246, "no_speech_prob": 1.3630529792862944e-05}, {"id": 291, "seek": 136572, "start": 1382.44, "end": 1385.48, "text": " So we've created the seek to seek RNN.", "tokens": [407, 321, 600, 2942, 264, 8075, 281, 8075, 45702, 45, 13], "temperature": 0.0, "avg_logprob": -0.10086345672607422, "compression_ratio": 1.5942028985507246, "no_speech_prob": 1.3630529792862944e-05}, {"id": 292, "seek": 136572, "start": 1385.48, "end": 1389.0, "text": " We have to give it the embeddings that we're using for our encoder.", "tokens": [492, 362, 281, 976, 309, 264, 12240, 29432, 300, 321, 434, 1228, 337, 527, 2058, 19866, 13], "temperature": 0.0, "avg_logprob": -0.10086345672607422, "compression_ratio": 1.5942028985507246, "no_speech_prob": 1.3630529792862944e-05}, {"id": 293, "seek": 136572, "start": 1389.0, "end": 1390.72, "text": " That's just our French embeddings.", "tokens": [663, 311, 445, 527, 5522, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.10086345672607422, "compression_ratio": 1.5942028985507246, "no_speech_prob": 1.3630529792862944e-05}, {"id": 294, "seek": 139072, "start": 1390.72, "end": 1412.32, "text": " Give it our English embeddings at 256 is the number of hidden layers and 30 is the length,", "tokens": [5303, 309, 527, 3669, 12240, 29432, 412, 38882, 307, 264, 1230, 295, 7633, 7914, 293, 2217, 307, 264, 4641, 11], "temperature": 0.0, "avg_logprob": -0.1510947322845459, "compression_ratio": 1.303448275862069, "no_speech_prob": 1.0288985322404187e-05}, {"id": 295, "seek": 139072, "start": 1412.32, "end": 1413.32, "text": " sequence length.", "tokens": [8310, 4641, 13], "temperature": 0.0, "avg_logprob": -0.1510947322845459, "compression_ratio": 1.303448275862069, "no_speech_prob": 1.0288985322404187e-05}, {"id": 296, "seek": 139072, "start": 1413.32, "end": 1419.84, "text": " And we can see, because this is another way to visualize what the RNN looks like.", "tokens": [400, 321, 393, 536, 11, 570, 341, 307, 1071, 636, 281, 23273, 437, 264, 45702, 45, 1542, 411, 13], "temperature": 0.0, "avg_logprob": -0.1510947322845459, "compression_ratio": 1.303448275862069, "no_speech_prob": 1.0288985322404187e-05}, {"id": 297, "seek": 141984, "start": 1419.84, "end": 1431.9199999999998, "text": " It's an embedding dropout, the first GRU linear layer, another embedding GRU dropout, another", "tokens": [467, 311, 364, 12240, 3584, 3270, 346, 11, 264, 700, 10903, 52, 8213, 4583, 11, 1071, 12240, 3584, 10903, 52, 3270, 346, 11, 1071], "temperature": 0.0, "avg_logprob": -0.24788896537121433, "compression_ratio": 1.5529411764705883, "no_speech_prob": 2.1443789592012763e-05}, {"id": 298, "seek": 141984, "start": 1431.9199999999998, "end": 1432.9199999999998, "text": " linear layer.", "tokens": [8213, 4583, 13], "temperature": 0.0, "avg_logprob": -0.24788896537121433, "compression_ratio": 1.5529411764705883, "no_speech_prob": 2.1443789592012763e-05}, {"id": 299, "seek": 141984, "start": 1432.9199999999998, "end": 1435.1999999999998, "text": " That are kind of the pieces of it.", "tokens": [663, 366, 733, 295, 264, 3755, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.24788896537121433, "compression_ratio": 1.5529411764705883, "no_speech_prob": 2.1443789592012763e-05}, {"id": 300, "seek": 141984, "start": 1435.1999999999998, "end": 1437.3999999999999, "text": " And it's helpful.", "tokens": [400, 309, 311, 4961, 13], "temperature": 0.0, "avg_logprob": -0.24788896537121433, "compression_ratio": 1.5529411764705883, "no_speech_prob": 2.1443789592012763e-05}, {"id": 301, "seek": 141984, "start": 1437.3999999999999, "end": 1442.6, "text": " It can be helpful to kind of look at the sizes of things.", "tokens": [467, 393, 312, 4961, 281, 733, 295, 574, 412, 264, 11602, 295, 721, 13], "temperature": 0.0, "avg_logprob": -0.24788896537121433, "compression_ratio": 1.5529411764705883, "no_speech_prob": 2.1443789592012763e-05}, {"id": 302, "seek": 141984, "start": 1442.6, "end": 1448.1599999999999, "text": " Here the hidden state size is 2 by 64 by 300.", "tokens": [1692, 264, 7633, 1785, 2744, 307, 568, 538, 12145, 538, 6641, 13], "temperature": 0.0, "avg_logprob": -0.24788896537121433, "compression_ratio": 1.5529411764705883, "no_speech_prob": 2.1443789592012763e-05}, {"id": 303, "seek": 144816, "start": 1448.16, "end": 1453.48, "text": " So the two corresponds to, we have two hidden layers inside the GRU.", "tokens": [407, 264, 732, 23249, 281, 11, 321, 362, 732, 7633, 7914, 1854, 264, 10903, 52, 13], "temperature": 0.0, "avg_logprob": -0.27918652387765736, "compression_ratio": 1.454954954954955, "no_speech_prob": 1.3630528883368243e-05}, {"id": 304, "seek": 144816, "start": 1453.48, "end": 1457.28, "text": " You can see that kind of up here.", "tokens": [509, 393, 536, 300, 733, 295, 493, 510, 13], "temperature": 0.0, "avg_logprob": -0.27918652387765736, "compression_ratio": 1.454954954954955, "no_speech_prob": 1.3630528883368243e-05}, {"id": 305, "seek": 144816, "start": 1457.28, "end": 1458.52, "text": " Two hidden layers.", "tokens": [4453, 7633, 7914, 13], "temperature": 0.0, "avg_logprob": -0.27918652387765736, "compression_ratio": 1.454954954954955, "no_speech_prob": 1.3630528883368243e-05}, {"id": 306, "seek": 144816, "start": 1458.52, "end": 1464.96, "text": " 64 is our batch size and 300 is the dimension of the embeddings.", "tokens": [12145, 307, 527, 15245, 2744, 293, 6641, 307, 264, 10139, 295, 264, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.27918652387765736, "compression_ratio": 1.454954954954955, "no_speech_prob": 1.3630528883368243e-05}, {"id": 307, "seek": 144816, "start": 1464.96, "end": 1468.1200000000001, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.27918652387765736, "compression_ratio": 1.454954954954955, "no_speech_prob": 1.3630528883368243e-05}, {"id": 308, "seek": 144816, "start": 1468.1200000000001, "end": 1470.0800000000002, "text": " And then this is, let me check.", "tokens": [400, 550, 341, 307, 11, 718, 385, 1520, 13], "temperature": 0.0, "avg_logprob": -0.27918652387765736, "compression_ratio": 1.454954954954955, "no_speech_prob": 1.3630528883368243e-05}, {"id": 309, "seek": 144816, "start": 1470.0800000000002, "end": 1471.28, "text": " Yeah, it's 1205.", "tokens": [865, 11, 309, 311, 10411, 20, 13], "temperature": 0.0, "avg_logprob": -0.27918652387765736, "compression_ratio": 1.454954954954955, "no_speech_prob": 1.3630528883368243e-05}, {"id": 310, "seek": 144816, "start": 1471.28, "end": 1472.92, "text": " So let's stop for a break.", "tokens": [407, 718, 311, 1590, 337, 257, 1821, 13], "temperature": 0.0, "avg_logprob": -0.27918652387765736, "compression_ratio": 1.454954954954955, "no_speech_prob": 1.3630528883368243e-05}, {"id": 311, "seek": 144816, "start": 1472.92, "end": 1475.96, "text": " We'll meet back at 1210 and keep going with this.", "tokens": [492, 603, 1677, 646, 412, 2272, 3279, 293, 1066, 516, 365, 341, 13], "temperature": 0.0, "avg_logprob": -0.27918652387765736, "compression_ratio": 1.454954954954955, "no_speech_prob": 1.3630528883368243e-05}, {"id": 312, "seek": 147596, "start": 1475.96, "end": 1480.8, "text": " All right, let's start back up.", "tokens": [1057, 558, 11, 718, 311, 722, 646, 493, 13], "temperature": 0.0, "avg_logprob": -0.13628827018299322, "compression_ratio": 1.4739336492890995, "no_speech_prob": 5.224935739533976e-05}, {"id": 313, "seek": 147596, "start": 1480.8, "end": 1486.72, "text": " So just during the break, Jeremy suggested I show you a new GitHub feature that's just", "tokens": [407, 445, 1830, 264, 1821, 11, 17809, 10945, 286, 855, 291, 257, 777, 23331, 4111, 300, 311, 445], "temperature": 0.0, "avg_logprob": -0.13628827018299322, "compression_ratio": 1.4739336492890995, "no_speech_prob": 5.224935739533976e-05}, {"id": 314, "seek": 147596, "start": 1486.72, "end": 1489.04, "text": " been recently released.", "tokens": [668, 3938, 4736, 13], "temperature": 0.0, "avg_logprob": -0.13628827018299322, "compression_ratio": 1.4739336492890995, "no_speech_prob": 5.224935739533976e-05}, {"id": 315, "seek": 147596, "start": 1489.04, "end": 1494.6200000000001, "text": " And so first we can look at the docs for data bunch.", "tokens": [400, 370, 700, 321, 393, 574, 412, 264, 45623, 337, 1412, 3840, 13], "temperature": 0.0, "avg_logprob": -0.13628827018299322, "compression_ratio": 1.4739336492890995, "no_speech_prob": 5.224935739533976e-05}, {"id": 316, "seek": 147596, "start": 1494.6200000000001, "end": 1497.68, "text": " And so that'll pull up a link if we want to go to the source code.", "tokens": [400, 370, 300, 603, 2235, 493, 257, 2113, 498, 321, 528, 281, 352, 281, 264, 4009, 3089, 13], "temperature": 0.0, "avg_logprob": -0.13628827018299322, "compression_ratio": 1.4739336492890995, "no_speech_prob": 5.224935739533976e-05}, {"id": 317, "seek": 147596, "start": 1497.68, "end": 1505.8400000000001, "text": " Actually, let me open that in a separate window.", "tokens": [5135, 11, 718, 385, 1269, 300, 294, 257, 4994, 4910, 13], "temperature": 0.0, "avg_logprob": -0.13628827018299322, "compression_ratio": 1.4739336492890995, "no_speech_prob": 5.224935739533976e-05}, {"id": 318, "seek": 150584, "start": 1505.84, "end": 1506.84, "text": " So now we're on GitHub.", "tokens": [407, 586, 321, 434, 322, 23331, 13], "temperature": 0.0, "avg_logprob": -0.19361597592713403, "compression_ratio": 1.728744939271255, "no_speech_prob": 1.2218588381074369e-05}, {"id": 319, "seek": 150584, "start": 1506.84, "end": 1509.1999999999998, "text": " Actually, let me make that clear.", "tokens": [5135, 11, 718, 385, 652, 300, 1850, 13], "temperature": 0.0, "avg_logprob": -0.19361597592713403, "compression_ratio": 1.728744939271255, "no_speech_prob": 1.2218588381074369e-05}, {"id": 320, "seek": 150584, "start": 1509.1999999999998, "end": 1513.32, "text": " You can see this is GitHub on the source code.", "tokens": [509, 393, 536, 341, 307, 23331, 322, 264, 4009, 3089, 13], "temperature": 0.0, "avg_logprob": -0.19361597592713403, "compression_ratio": 1.728744939271255, "no_speech_prob": 1.2218588381074369e-05}, {"id": 321, "seek": 150584, "start": 1513.32, "end": 1522.6, "text": " And now if you run your arrow over things, it'll highlight some of the methods that are", "tokens": [400, 586, 498, 291, 1190, 428, 11610, 670, 721, 11, 309, 603, 5078, 512, 295, 264, 7150, 300, 366], "temperature": 0.0, "avg_logprob": -0.19361597592713403, "compression_ratio": 1.728744939271255, "no_speech_prob": 1.2218588381074369e-05}, {"id": 322, "seek": 150584, "start": 1522.6, "end": 1523.6, "text": " defined elsewhere.", "tokens": [7642, 14517, 13], "temperature": 0.0, "avg_logprob": -0.19361597592713403, "compression_ratio": 1.728744939271255, "no_speech_prob": 1.2218588381074369e-05}, {"id": 323, "seek": 150584, "start": 1523.6, "end": 1527.3999999999999, "text": " So you can click on this and it shows us, oh, this is where listify was defined.", "tokens": [407, 291, 393, 2052, 322, 341, 293, 309, 3110, 505, 11, 1954, 11, 341, 307, 689, 1329, 2505, 390, 7642, 13], "temperature": 0.0, "avg_logprob": -0.19361597592713403, "compression_ratio": 1.728744939271255, "no_speech_prob": 1.2218588381074369e-05}, {"id": 324, "seek": 150584, "start": 1527.3999999999999, "end": 1529.56, "text": " We're not going to look at the old version.", "tokens": [492, 434, 406, 516, 281, 574, 412, 264, 1331, 3037, 13], "temperature": 0.0, "avg_logprob": -0.19361597592713403, "compression_ratio": 1.728744939271255, "no_speech_prob": 1.2218588381074369e-05}, {"id": 325, "seek": 150584, "start": 1529.56, "end": 1530.9599999999998, "text": " Let's look at the current version.", "tokens": [961, 311, 574, 412, 264, 2190, 3037, 13], "temperature": 0.0, "avg_logprob": -0.19361597592713403, "compression_ratio": 1.728744939271255, "no_speech_prob": 1.2218588381074369e-05}, {"id": 326, "seek": 150584, "start": 1530.9599999999998, "end": 1534.84, "text": " You can click on it and it takes us to the source code.", "tokens": [509, 393, 2052, 322, 309, 293, 309, 2516, 505, 281, 264, 4009, 3089, 13], "temperature": 0.0, "avg_logprob": -0.19361597592713403, "compression_ratio": 1.728744939271255, "no_speech_prob": 1.2218588381074369e-05}, {"id": 327, "seek": 153484, "start": 1534.84, "end": 1537.8, "text": " And this only works within a repository.", "tokens": [400, 341, 787, 1985, 1951, 257, 25841, 13], "temperature": 0.0, "avg_logprob": -0.16039762741480118, "compression_ratio": 1.4923076923076923, "no_speech_prob": 4.092786184628494e-06}, {"id": 328, "seek": 153484, "start": 1537.8, "end": 1542.0, "text": " It's not going to link you to other repositories, but it makes it easier to kind of navigate", "tokens": [467, 311, 406, 516, 281, 2113, 291, 281, 661, 22283, 2083, 11, 457, 309, 1669, 309, 3571, 281, 733, 295, 12350], "temperature": 0.0, "avg_logprob": -0.16039762741480118, "compression_ratio": 1.4923076923076923, "no_speech_prob": 4.092786184628494e-06}, {"id": 329, "seek": 153484, "start": 1542.0, "end": 1548.56, "text": " if you want to see, hey, what's the definition of such and such?", "tokens": [498, 291, 528, 281, 536, 11, 4177, 11, 437, 311, 264, 7123, 295, 1270, 293, 1270, 30], "temperature": 0.0, "avg_logprob": -0.16039762741480118, "compression_ratio": 1.4923076923076923, "no_speech_prob": 4.092786184628494e-06}, {"id": 330, "seek": 153484, "start": 1548.56, "end": 1557.8, "text": " So that's neat to know about and not specific to fast AI, but just a nice feature on GitHub.", "tokens": [407, 300, 311, 10654, 281, 458, 466, 293, 406, 2685, 281, 2370, 7318, 11, 457, 445, 257, 1481, 4111, 322, 23331, 13], "temperature": 0.0, "avg_logprob": -0.16039762741480118, "compression_ratio": 1.4923076923076923, "no_speech_prob": 4.092786184628494e-06}, {"id": 331, "seek": 155780, "start": 1557.8, "end": 1566.04, "text": " So returning to our task.", "tokens": [407, 12678, 281, 527, 5633, 13], "temperature": 0.0, "avg_logprob": -0.24762812682560512, "compression_ratio": 1.34375, "no_speech_prob": 6.339102583297063e-06}, {"id": 332, "seek": 155780, "start": 1566.04, "end": 1575.12, "text": " So we'll need to pad our loss so that it's the right shape.", "tokens": [407, 321, 603, 643, 281, 6887, 527, 4470, 370, 300, 309, 311, 264, 558, 3909, 13], "temperature": 0.0, "avg_logprob": -0.24762812682560512, "compression_ratio": 1.34375, "no_speech_prob": 6.339102583297063e-06}, {"id": 333, "seek": 155780, "start": 1575.12, "end": 1577.6, "text": " And now we're ready to train our model.", "tokens": [400, 586, 321, 434, 1919, 281, 3847, 527, 2316, 13], "temperature": 0.0, "avg_logprob": -0.24762812682560512, "compression_ratio": 1.34375, "no_speech_prob": 6.339102583297063e-06}, {"id": 334, "seek": 155780, "start": 1577.6, "end": 1581.44, "text": " And so here loss is defined as...", "tokens": [400, 370, 510, 4470, 307, 7642, 382, 485], "temperature": 0.0, "avg_logprob": -0.24762812682560512, "compression_ratio": 1.34375, "no_speech_prob": 6.339102583297063e-06}, {"id": 335, "seek": 155780, "start": 1581.44, "end": 1586.84, "text": " Well, sorry.", "tokens": [1042, 11, 2597, 13], "temperature": 0.0, "avg_logprob": -0.24762812682560512, "compression_ratio": 1.34375, "no_speech_prob": 6.339102583297063e-06}, {"id": 336, "seek": 158684, "start": 1586.84, "end": 1590.48, "text": " We're using a cross entropy loss.", "tokens": [492, 434, 1228, 257, 3278, 30867, 4470, 13], "temperature": 0.0, "avg_logprob": -0.14573657512664795, "compression_ratio": 1.560747663551402, "no_speech_prob": 8.93933065526653e-06}, {"id": 337, "seek": 158684, "start": 1590.48, "end": 1594.32, "text": " We're padding the lengths.", "tokens": [492, 434, 39562, 264, 26329, 13], "temperature": 0.0, "avg_logprob": -0.14573657512664795, "compression_ratio": 1.560747663551402, "no_speech_prob": 8.93933065526653e-06}, {"id": 338, "seek": 158684, "start": 1594.32, "end": 1599.6399999999999, "text": " So we can run these, get a plot of the learning rate finder.", "tokens": [407, 321, 393, 1190, 613, 11, 483, 257, 7542, 295, 264, 2539, 3314, 915, 260, 13], "temperature": 0.0, "avg_logprob": -0.14573657512664795, "compression_ratio": 1.560747663551402, "no_speech_prob": 8.93933065526653e-06}, {"id": 339, "seek": 158684, "start": 1599.6399999999999, "end": 1600.6399999999999, "text": " Let's try 1e-2.", "tokens": [961, 311, 853, 502, 68, 12, 17, 13], "temperature": 0.0, "avg_logprob": -0.14573657512664795, "compression_ratio": 1.560747663551402, "no_speech_prob": 8.93933065526653e-06}, {"id": 340, "seek": 158684, "start": 1600.6399999999999, "end": 1608.8, "text": " Again, we're kind of choosing a point where the loss is relatively low, but still decreasing.", "tokens": [3764, 11, 321, 434, 733, 295, 10875, 257, 935, 689, 264, 4470, 307, 7226, 2295, 11, 457, 920, 23223, 13], "temperature": 0.0, "avg_logprob": -0.14573657512664795, "compression_ratio": 1.560747663551402, "no_speech_prob": 8.93933065526653e-06}, {"id": 341, "seek": 158684, "start": 1608.8, "end": 1612.6799999999998, "text": " And so this will show us the training loss and validation loss.", "tokens": [400, 370, 341, 486, 855, 505, 264, 3097, 4470, 293, 24071, 4470, 13], "temperature": 0.0, "avg_logprob": -0.14573657512664795, "compression_ratio": 1.560747663551402, "no_speech_prob": 8.93933065526653e-06}, {"id": 342, "seek": 158684, "start": 1612.6799999999998, "end": 1615.0, "text": " And this is not very meaningful to me.", "tokens": [400, 341, 307, 406, 588, 10995, 281, 385, 13], "temperature": 0.0, "avg_logprob": -0.14573657512664795, "compression_ratio": 1.560747663551402, "no_speech_prob": 8.93933065526653e-06}, {"id": 343, "seek": 161500, "start": 1615.0, "end": 1621.36, "text": " It's nice to see that they're both decreasing, but in terms of human interpretability, we", "tokens": [467, 311, 1481, 281, 536, 300, 436, 434, 1293, 23223, 11, 457, 294, 2115, 295, 1952, 7302, 2310, 11, 321], "temperature": 0.0, "avg_logprob": -0.11936891597250233, "compression_ratio": 1.5530973451327434, "no_speech_prob": 1.3419377864920534e-05}, {"id": 344, "seek": 161500, "start": 1621.36, "end": 1624.92, "text": " don't really know how good is our model based on these losses.", "tokens": [500, 380, 534, 458, 577, 665, 307, 527, 2316, 2361, 322, 613, 15352, 13], "temperature": 0.0, "avg_logprob": -0.11936891597250233, "compression_ratio": 1.5530973451327434, "no_speech_prob": 1.3419377864920534e-05}, {"id": 345, "seek": 161500, "start": 1624.92, "end": 1627.24, "text": " We don't have a meaning for that.", "tokens": [492, 500, 380, 362, 257, 3620, 337, 300, 13], "temperature": 0.0, "avg_logprob": -0.11936891597250233, "compression_ratio": 1.5530973451327434, "no_speech_prob": 1.3419377864920534e-05}, {"id": 346, "seek": 161500, "start": 1627.24, "end": 1635.2, "text": " Oh, well, also we can delete our French vectors and English vectors to free up some memory.", "tokens": [876, 11, 731, 11, 611, 321, 393, 12097, 527, 5522, 18875, 293, 3669, 18875, 281, 1737, 493, 512, 4675, 13], "temperature": 0.0, "avg_logprob": -0.11936891597250233, "compression_ratio": 1.5530973451327434, "no_speech_prob": 1.3419377864920534e-05}, {"id": 347, "seek": 161500, "start": 1635.2, "end": 1642.8, "text": " So in addition to looking at the loss, we can also look at the accuracy.", "tokens": [407, 294, 4500, 281, 1237, 412, 264, 4470, 11, 321, 393, 611, 574, 412, 264, 14170, 13], "temperature": 0.0, "avg_logprob": -0.11936891597250233, "compression_ratio": 1.5530973451327434, "no_speech_prob": 1.3419377864920534e-05}, {"id": 348, "seek": 164280, "start": 1642.8, "end": 1648.28, "text": " And so this is what percentage of the time are we getting the right word out?", "tokens": [400, 370, 341, 307, 437, 9668, 295, 264, 565, 366, 321, 1242, 264, 558, 1349, 484, 30], "temperature": 0.0, "avg_logprob": -0.18332786817808408, "compression_ratio": 1.5505050505050506, "no_speech_prob": 1.7501475667813793e-05}, {"id": 349, "seek": 164280, "start": 1648.28, "end": 1651.36, "text": " However, measuring the...", "tokens": [2908, 11, 13389, 264, 485], "temperature": 0.0, "avg_logprob": -0.18332786817808408, "compression_ratio": 1.5505050505050506, "no_speech_prob": 1.7501475667813793e-05}, {"id": 350, "seek": 164280, "start": 1651.36, "end": 1658.84, "text": " So accuracy is this technical definition, but measuring how good a translation is, is", "tokens": [407, 14170, 307, 341, 6191, 7123, 11, 457, 13389, 577, 665, 257, 12853, 307, 11, 307], "temperature": 0.0, "avg_logprob": -0.18332786817808408, "compression_ratio": 1.5505050505050506, "no_speech_prob": 1.7501475667813793e-05}, {"id": 351, "seek": 164280, "start": 1658.84, "end": 1660.08, "text": " a little bit trickier.", "tokens": [257, 707, 857, 4282, 811, 13], "temperature": 0.0, "avg_logprob": -0.18332786817808408, "compression_ratio": 1.5505050505050506, "no_speech_prob": 1.7501475667813793e-05}, {"id": 352, "seek": 164280, "start": 1660.08, "end": 1668.04, "text": " So something might not have the exact same words, but could still be a reasonable translation.", "tokens": [407, 746, 1062, 406, 362, 264, 1900, 912, 2283, 11, 457, 727, 920, 312, 257, 10585, 12853, 13], "temperature": 0.0, "avg_logprob": -0.18332786817808408, "compression_ratio": 1.5505050505050506, "no_speech_prob": 1.7501475667813793e-05}, {"id": 353, "seek": 166804, "start": 1668.04, "end": 1675.1599999999999, "text": " And so one of the metrics that's been designed to deal with this is blue.", "tokens": [400, 370, 472, 295, 264, 16367, 300, 311, 668, 4761, 281, 2028, 365, 341, 307, 3344, 13], "temperature": 0.0, "avg_logprob": -0.05165238976478577, "compression_ratio": 1.441747572815534, "no_speech_prob": 2.1439680494950153e-05}, {"id": 354, "seek": 166804, "start": 1675.1599999999999, "end": 1681.12, "text": " And so I'm going to link now to Rachel Tatman, who has a PhD in computational linguistics", "tokens": [400, 370, 286, 478, 516, 281, 2113, 586, 281, 14246, 19645, 1601, 11, 567, 575, 257, 14476, 294, 28270, 21766, 6006], "temperature": 0.0, "avg_logprob": -0.05165238976478577, "compression_ratio": 1.441747572815534, "no_speech_prob": 2.1439680494950153e-05}, {"id": 355, "seek": 166804, "start": 1681.12, "end": 1688.48, "text": " and works for Kaggle, has a really fantastic post on evaluating text output in NLP.", "tokens": [293, 1985, 337, 48751, 22631, 11, 575, 257, 534, 5456, 2183, 322, 27479, 2487, 5598, 294, 426, 45196, 13], "temperature": 0.0, "avg_logprob": -0.05165238976478577, "compression_ratio": 1.441747572815534, "no_speech_prob": 2.1439680494950153e-05}, {"id": 356, "seek": 166804, "start": 1688.48, "end": 1693.6399999999999, "text": " And so I want to go through a little bit of this.", "tokens": [400, 370, 286, 528, 281, 352, 807, 257, 707, 857, 295, 341, 13], "temperature": 0.0, "avg_logprob": -0.05165238976478577, "compression_ratio": 1.441747572815534, "no_speech_prob": 2.1439680494950153e-05}, {"id": 357, "seek": 169364, "start": 1693.64, "end": 1699.0400000000002, "text": " So here she highlights some general sequence to sequence tasks in NLP.", "tokens": [407, 510, 750, 14254, 512, 2674, 8310, 281, 8310, 9608, 294, 426, 45196, 13], "temperature": 0.0, "avg_logprob": -0.168301038652937, "compression_ratio": 1.627906976744186, "no_speech_prob": 6.107483932282776e-05}, {"id": 358, "seek": 169364, "start": 1699.0400000000002, "end": 1703.76, "text": " So that also includes text summarization, text simplification, and actually let me go", "tokens": [407, 300, 611, 5974, 2487, 14611, 2144, 11, 2487, 6883, 3774, 11, 293, 767, 718, 385, 352], "temperature": 0.0, "avg_logprob": -0.168301038652937, "compression_ratio": 1.627906976744186, "no_speech_prob": 6.107483932282776e-05}, {"id": 359, "seek": 169364, "start": 1703.76, "end": 1704.76, "text": " back up to the top.", "tokens": [646, 493, 281, 264, 1192, 13], "temperature": 0.0, "avg_logprob": -0.168301038652937, "compression_ratio": 1.627906976744186, "no_speech_prob": 6.107483932282776e-05}, {"id": 360, "seek": 169364, "start": 1704.76, "end": 1710.88, "text": " Just so you see this, this is evaluating text output in NLP blue at your own risk, because", "tokens": [1449, 370, 291, 536, 341, 11, 341, 307, 27479, 2487, 5598, 294, 426, 45196, 3344, 412, 428, 1065, 3148, 11, 570], "temperature": 0.0, "avg_logprob": -0.168301038652937, "compression_ratio": 1.627906976744186, "no_speech_prob": 6.107483932282776e-05}, {"id": 361, "seek": 169364, "start": 1710.88, "end": 1717.0400000000002, "text": " there are upsides and downsides to using blue, and it's sometimes used inappropriately.", "tokens": [456, 366, 15497, 1875, 293, 21554, 1875, 281, 1228, 3344, 11, 293, 309, 311, 2171, 1143, 24728, 1592, 13], "temperature": 0.0, "avg_logprob": -0.168301038652937, "compression_ratio": 1.627906976744186, "no_speech_prob": 6.107483932282776e-05}, {"id": 362, "seek": 169364, "start": 1717.0400000000002, "end": 1718.0400000000002, "text": " And there is some...", "tokens": [400, 456, 307, 512, 485], "temperature": 0.0, "avg_logprob": -0.168301038652937, "compression_ratio": 1.627906976744186, "no_speech_prob": 6.107483932282776e-05}, {"id": 363, "seek": 169364, "start": 1718.0400000000002, "end": 1720.24, "text": " It's definitely an area of active research.", "tokens": [467, 311, 2138, 364, 1859, 295, 4967, 2132, 13], "temperature": 0.0, "avg_logprob": -0.168301038652937, "compression_ratio": 1.627906976744186, "no_speech_prob": 6.107483932282776e-05}, {"id": 364, "seek": 172024, "start": 1720.24, "end": 1724.56, "text": " And Rachel says later in the post that she thinks the question of even evaluating how", "tokens": [400, 14246, 1619, 1780, 294, 264, 2183, 300, 750, 7309, 264, 1168, 295, 754, 27479, 577], "temperature": 0.0, "avg_logprob": -0.15230708761313527, "compression_ratio": 1.5864978902953586, "no_speech_prob": 2.2124226234154776e-05}, {"id": 365, "seek": 172024, "start": 1724.56, "end": 1730.88, "text": " good your seek to seek NLP model is, is one of the very hard open problems in NLP.", "tokens": [665, 428, 8075, 281, 8075, 426, 45196, 2316, 307, 11, 307, 472, 295, 264, 588, 1152, 1269, 2740, 294, 426, 45196, 13], "temperature": 0.0, "avg_logprob": -0.15230708761313527, "compression_ratio": 1.5864978902953586, "no_speech_prob": 2.2124226234154776e-05}, {"id": 366, "seek": 172024, "start": 1730.88, "end": 1734.56, "text": " And she'll give some examples that I want to show.", "tokens": [400, 750, 603, 976, 512, 5110, 300, 286, 528, 281, 855, 13], "temperature": 0.0, "avg_logprob": -0.15230708761313527, "compression_ratio": 1.5864978902953586, "no_speech_prob": 2.2124226234154776e-05}, {"id": 367, "seek": 172024, "start": 1734.56, "end": 1739.0, "text": " So here are some other seek to seek tasks.", "tokens": [407, 510, 366, 512, 661, 8075, 281, 8075, 9608, 13], "temperature": 0.0, "avg_logprob": -0.15230708761313527, "compression_ratio": 1.5864978902953586, "no_speech_prob": 2.2124226234154776e-05}, {"id": 368, "seek": 172024, "start": 1739.0, "end": 1743.48, "text": " And she highlights there's no simple answer about what metrics you should use.", "tokens": [400, 750, 14254, 456, 311, 572, 2199, 1867, 466, 437, 16367, 291, 820, 764, 13], "temperature": 0.0, "avg_logprob": -0.15230708761313527, "compression_ratio": 1.5864978902953586, "no_speech_prob": 2.2124226234154776e-05}, {"id": 369, "seek": 172024, "start": 1743.48, "end": 1746.44, "text": " And blue has some major drawbacks.", "tokens": [400, 3344, 575, 512, 2563, 2642, 17758, 13], "temperature": 0.0, "avg_logprob": -0.15230708761313527, "compression_ratio": 1.5864978902953586, "no_speech_prob": 2.2124226234154776e-05}, {"id": 370, "seek": 174644, "start": 1746.44, "end": 1750.96, "text": " But first let's talk about what blue does.", "tokens": [583, 700, 718, 311, 751, 466, 437, 3344, 775, 13], "temperature": 0.0, "avg_logprob": -0.20681692629444356, "compression_ratio": 1.543103448275862, "no_speech_prob": 1.618705027794931e-05}, {"id": 371, "seek": 174644, "start": 1750.96, "end": 1757.2, "text": " So she gives the example, actually she's also going from French to English, of this sentence", "tokens": [407, 750, 2709, 264, 1365, 11, 767, 750, 311, 611, 516, 490, 5522, 281, 3669, 11, 295, 341, 8174], "temperature": 0.0, "avg_logprob": -0.20681692629444356, "compression_ratio": 1.543103448275862, "no_speech_prob": 1.618705027794931e-05}, {"id": 372, "seek": 174644, "start": 1757.2, "end": 1763.3200000000002, "text": " and suggests, I have eaten three hazelnuts or I ate three filberts.", "tokens": [293, 13409, 11, 286, 362, 12158, 1045, 11008, 9878, 3648, 420, 286, 8468, 1045, 1387, 607, 1373, 13], "temperature": 0.0, "avg_logprob": -0.20681692629444356, "compression_ratio": 1.543103448275862, "no_speech_prob": 1.618705027794931e-05}, {"id": 373, "seek": 174644, "start": 1763.3200000000002, "end": 1768.64, "text": " So filberts is a synonym used, I think, in the UK for hazelnut.", "tokens": [407, 1387, 607, 1373, 307, 257, 5451, 12732, 1143, 11, 286, 519, 11, 294, 264, 7051, 337, 11008, 9878, 325, 13], "temperature": 0.0, "avg_logprob": -0.20681692629444356, "compression_ratio": 1.543103448275862, "no_speech_prob": 1.618705027794931e-05}, {"id": 374, "seek": 174644, "start": 1768.64, "end": 1773.6000000000001, "text": " So you can already see, like here are two different ways of translating the same sentence,", "tokens": [407, 291, 393, 1217, 536, 11, 411, 510, 366, 732, 819, 2098, 295, 35030, 264, 912, 8174, 11], "temperature": 0.0, "avg_logprob": -0.20681692629444356, "compression_ratio": 1.543103448275862, "no_speech_prob": 1.618705027794931e-05}, {"id": 375, "seek": 177360, "start": 1773.6, "end": 1777.08, "text": " both we've got this difference between hazelnuts and filberts.", "tokens": [1293, 321, 600, 658, 341, 2649, 1296, 11008, 9878, 3648, 293, 1387, 607, 1373, 13], "temperature": 0.0, "avg_logprob": -0.1502536137898763, "compression_ratio": 1.619047619047619, "no_speech_prob": 3.3210297260666266e-05}, {"id": 376, "seek": 177360, "start": 1777.08, "end": 1781.76, "text": " Also I have eaten and I ate, you're not always going to get a clear mapping of, you know,", "tokens": [2743, 286, 362, 12158, 293, 286, 8468, 11, 291, 434, 406, 1009, 516, 281, 483, 257, 1850, 18350, 295, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.1502536137898763, "compression_ratio": 1.619047619047619, "no_speech_prob": 3.3210297260666266e-05}, {"id": 377, "seek": 177360, "start": 1781.76, "end": 1788.6, "text": " there are multiple ways to use a verb tense in the past, present or future.", "tokens": [456, 366, 3866, 2098, 281, 764, 257, 9595, 18760, 294, 264, 1791, 11, 1974, 420, 2027, 13], "temperature": 0.0, "avg_logprob": -0.1502536137898763, "compression_ratio": 1.619047619047619, "no_speech_prob": 3.3210297260666266e-05}, {"id": 378, "seek": 177360, "start": 1788.6, "end": 1792.36, "text": " And so she gives the example of, okay, I ate three hazelnuts, how good a translation is", "tokens": [400, 370, 750, 2709, 264, 1365, 295, 11, 1392, 11, 286, 8468, 1045, 11008, 9878, 3648, 11, 577, 665, 257, 12853, 307], "temperature": 0.0, "avg_logprob": -0.1502536137898763, "compression_ratio": 1.619047619047619, "no_speech_prob": 3.3210297260666266e-05}, {"id": 379, "seek": 177360, "start": 1792.36, "end": 1793.36, "text": " this?", "tokens": [341, 30], "temperature": 0.0, "avg_logprob": -0.1502536137898763, "compression_ratio": 1.619047619047619, "no_speech_prob": 3.3210297260666266e-05}, {"id": 380, "seek": 177360, "start": 1793.36, "end": 1798.8, "text": " And so this does not match either of these exactly, but it's actually, we know it's a", "tokens": [400, 370, 341, 775, 406, 2995, 2139, 295, 613, 2293, 11, 457, 309, 311, 767, 11, 321, 458, 309, 311, 257], "temperature": 0.0, "avg_logprob": -0.1502536137898763, "compression_ratio": 1.619047619047619, "no_speech_prob": 3.3210297260666266e-05}, {"id": 381, "seek": 179880, "start": 1798.8, "end": 1804.2, "text": " very good translation given this, because it actually combines the I ate three and then", "tokens": [588, 665, 12853, 2212, 341, 11, 570, 309, 767, 29520, 264, 286, 8468, 1045, 293, 550], "temperature": 0.0, "avg_logprob": -0.1356537860372792, "compression_ratio": 1.5608695652173914, "no_speech_prob": 2.355039214307908e-05}, {"id": 382, "seek": 179880, "start": 1804.2, "end": 1807.24, "text": " uses hazelnuts for filberts.", "tokens": [4960, 11008, 9878, 3648, 337, 1387, 607, 1373, 13], "temperature": 0.0, "avg_logprob": -0.1356537860372792, "compression_ratio": 1.5608695652173914, "no_speech_prob": 2.355039214307908e-05}, {"id": 383, "seek": 179880, "start": 1807.24, "end": 1813.0, "text": " And so she brings up kind of this core problem is, how can I assign a single numerical score", "tokens": [400, 370, 750, 5607, 493, 733, 295, 341, 4965, 1154, 307, 11, 577, 393, 286, 6269, 257, 2167, 29054, 6175], "temperature": 0.0, "avg_logprob": -0.1356537860372792, "compression_ratio": 1.5608695652173914, "no_speech_prob": 2.355039214307908e-05}, {"id": 384, "seek": 179880, "start": 1813.0, "end": 1816.52, "text": " to this translation that says how good it is?", "tokens": [281, 341, 12853, 300, 1619, 577, 665, 309, 307, 30], "temperature": 0.0, "avg_logprob": -0.1356537860372792, "compression_ratio": 1.5608695652173914, "no_speech_prob": 2.355039214307908e-05}, {"id": 385, "seek": 179880, "start": 1816.52, "end": 1820.56, "text": " And notice also we already have this issue of length too.", "tokens": [400, 3449, 611, 321, 1217, 362, 341, 2734, 295, 4641, 886, 13], "temperature": 0.0, "avg_logprob": -0.1356537860372792, "compression_ratio": 1.5608695652173914, "no_speech_prob": 2.355039214307908e-05}, {"id": 386, "seek": 179880, "start": 1820.56, "end": 1823.32, "text": " You know, here we've got two correct answers.", "tokens": [509, 458, 11, 510, 321, 600, 658, 732, 3006, 6338, 13], "temperature": 0.0, "avg_logprob": -0.1356537860372792, "compression_ratio": 1.5608695652173914, "no_speech_prob": 2.355039214307908e-05}, {"id": 387, "seek": 182332, "start": 1823.32, "end": 1829.6399999999999, "text": " One is five words long and one is four words long.", "tokens": [1485, 307, 1732, 2283, 938, 293, 472, 307, 1451, 2283, 938, 13], "temperature": 0.0, "avg_logprob": -0.12131235550860969, "compression_ratio": 1.6044444444444443, "no_speech_prob": 2.9310249374248087e-05}, {"id": 388, "seek": 182332, "start": 1829.6399999999999, "end": 1836.08, "text": " And so I'm going to kind of skip ahead to blue.", "tokens": [400, 370, 286, 478, 516, 281, 733, 295, 10023, 2286, 281, 3344, 13], "temperature": 0.0, "avg_logprob": -0.12131235550860969, "compression_ratio": 1.6044444444444443, "no_speech_prob": 2.9310249374248087e-05}, {"id": 389, "seek": 182332, "start": 1836.08, "end": 1840.6799999999998, "text": " First she notes that you don't just want to look at the presence of words, like the order", "tokens": [2386, 750, 5570, 300, 291, 500, 380, 445, 528, 281, 574, 412, 264, 6814, 295, 2283, 11, 411, 264, 1668], "temperature": 0.0, "avg_logprob": -0.12131235550860969, "compression_ratio": 1.6044444444444443, "no_speech_prob": 2.9310249374248087e-05}, {"id": 390, "seek": 182332, "start": 1840.6799999999998, "end": 1847.6799999999998, "text": " matters as well, because eight hazelnuts, I three, that's not correct English at all.", "tokens": [7001, 382, 731, 11, 570, 3180, 11008, 9878, 3648, 11, 286, 1045, 11, 300, 311, 406, 3006, 3669, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.12131235550860969, "compression_ratio": 1.6044444444444443, "no_speech_prob": 2.9310249374248087e-05}, {"id": 391, "seek": 182332, "start": 1847.6799999999998, "end": 1851.96, "text": " And so that's not a very good translation, even though it's got all the right words in", "tokens": [400, 370, 300, 311, 406, 257, 588, 665, 12853, 11, 754, 1673, 309, 311, 658, 439, 264, 558, 2283, 294], "temperature": 0.0, "avg_logprob": -0.12131235550860969, "compression_ratio": 1.6044444444444443, "no_speech_prob": 2.9310249374248087e-05}, {"id": 392, "seek": 185196, "start": 1851.96, "end": 1854.3600000000001, "text": " it, but they're jumbled up.", "tokens": [309, 11, 457, 436, 434, 361, 19928, 493, 13], "temperature": 0.0, "avg_logprob": -0.13505021049862817, "compression_ratio": 1.6322869955156951, "no_speech_prob": 1.2029418030579109e-05}, {"id": 393, "seek": 185196, "start": 1854.3600000000001, "end": 1859.4, "text": " And so the solution that blue uses is to look at n grams.", "tokens": [400, 370, 264, 3827, 300, 3344, 4960, 307, 281, 574, 412, 297, 11899, 13], "temperature": 0.0, "avg_logprob": -0.13505021049862817, "compression_ratio": 1.6322869955156951, "no_speech_prob": 1.2029418030579109e-05}, {"id": 394, "seek": 185196, "start": 1859.4, "end": 1863.54, "text": " And so it'll look at one grams, which are just capturing, do you have the right words", "tokens": [400, 370, 309, 603, 574, 412, 472, 11899, 11, 597, 366, 445, 23384, 11, 360, 291, 362, 264, 558, 2283], "temperature": 0.0, "avg_logprob": -0.13505021049862817, "compression_ratio": 1.6322869955156951, "no_speech_prob": 1.2029418030579109e-05}, {"id": 395, "seek": 185196, "start": 1863.54, "end": 1864.54, "text": " in there?", "tokens": [294, 456, 30], "temperature": 0.0, "avg_logprob": -0.13505021049862817, "compression_ratio": 1.6322869955156951, "no_speech_prob": 1.2029418030579109e-05}, {"id": 396, "seek": 185196, "start": 1864.54, "end": 1870.28, "text": " But also it will look at two grams, three grams and four grams.", "tokens": [583, 611, 309, 486, 574, 412, 732, 11899, 11, 1045, 11899, 293, 1451, 11899, 13], "temperature": 0.0, "avg_logprob": -0.13505021049862817, "compression_ratio": 1.6322869955156951, "no_speech_prob": 1.2029418030579109e-05}, {"id": 397, "seek": 185196, "start": 1870.28, "end": 1879.2, "text": " It's important to note, because there are examples where, let me think of one, like", "tokens": [467, 311, 1021, 281, 3637, 11, 570, 456, 366, 5110, 689, 11, 718, 385, 519, 295, 472, 11, 411], "temperature": 0.0, "avg_logprob": -0.13505021049862817, "compression_ratio": 1.6322869955156951, "no_speech_prob": 1.2029418030579109e-05}, {"id": 398, "seek": 185196, "start": 1879.2, "end": 1881.52, "text": " I ate a sandwich since I'm hungry.", "tokens": [286, 8468, 257, 11141, 1670, 286, 478, 8067, 13], "temperature": 0.0, "avg_logprob": -0.13505021049862817, "compression_ratio": 1.6322869955156951, "no_speech_prob": 1.2029418030579109e-05}, {"id": 399, "seek": 188152, "start": 1881.52, "end": 1884.4, "text": " You could also say, since I'm hungry, I ate a sandwich.", "tokens": [509, 727, 611, 584, 11, 1670, 286, 478, 8067, 11, 286, 8468, 257, 11141, 13], "temperature": 0.0, "avg_logprob": -0.14822103539291692, "compression_ratio": 1.6837209302325582, "no_speech_prob": 7.030597043922171e-05}, {"id": 400, "seek": 188152, "start": 1884.4, "end": 1888.18, "text": " And there I've changed the order, but that's still correct grammar.", "tokens": [400, 456, 286, 600, 3105, 264, 1668, 11, 457, 300, 311, 920, 3006, 22317, 13], "temperature": 0.0, "avg_logprob": -0.14822103539291692, "compression_ratio": 1.6837209302325582, "no_speech_prob": 7.030597043922171e-05}, {"id": 401, "seek": 188152, "start": 1888.18, "end": 1891.0, "text": " Those two sentences are saying the same thing.", "tokens": [3950, 732, 16579, 366, 1566, 264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.14822103539291692, "compression_ratio": 1.6837209302325582, "no_speech_prob": 7.030597043922171e-05}, {"id": 402, "seek": 188152, "start": 1891.0, "end": 1894.4, "text": " So n grams don't perfectly capture that, but they do.", "tokens": [407, 297, 11899, 500, 380, 6239, 7983, 300, 11, 457, 436, 360, 13], "temperature": 0.0, "avg_logprob": -0.14822103539291692, "compression_ratio": 1.6837209302325582, "no_speech_prob": 7.030597043922171e-05}, {"id": 403, "seek": 188152, "start": 1894.4, "end": 1900.6, "text": " In that example, the phrases, I ate a sandwich and since I'm hungry, those n grams will be", "tokens": [682, 300, 1365, 11, 264, 20312, 11, 286, 8468, 257, 11141, 293, 1670, 286, 478, 8067, 11, 729, 297, 11899, 486, 312], "temperature": 0.0, "avg_logprob": -0.14822103539291692, "compression_ratio": 1.6837209302325582, "no_speech_prob": 7.030597043922171e-05}, {"id": 404, "seek": 188152, "start": 1900.6, "end": 1905.68, "text": " correct, even though you can switch the order.", "tokens": [3006, 11, 754, 1673, 291, 393, 3679, 264, 1668, 13], "temperature": 0.0, "avg_logprob": -0.14822103539291692, "compression_ratio": 1.6837209302325582, "no_speech_prob": 7.030597043922171e-05}, {"id": 405, "seek": 190568, "start": 1905.68, "end": 1911.7, "text": " So blue typically uses the unigram, bigram, trigram and four grams.", "tokens": [407, 3344, 5850, 4960, 264, 517, 33737, 11, 955, 2356, 11, 504, 33737, 293, 1451, 11899, 13], "temperature": 0.0, "avg_logprob": -0.13183121400720932, "compression_ratio": 1.6941176470588235, "no_speech_prob": 2.6687022909754887e-05}, {"id": 406, "seek": 190568, "start": 1911.7, "end": 1919.64, "text": " And so they'll kind of say, how many correct unigrams do you have out of the total number?", "tokens": [400, 370, 436, 603, 733, 295, 584, 11, 577, 867, 3006, 517, 33737, 82, 360, 291, 362, 484, 295, 264, 3217, 1230, 30], "temperature": 0.0, "avg_logprob": -0.13183121400720932, "compression_ratio": 1.6941176470588235, "no_speech_prob": 2.6687022909754887e-05}, {"id": 407, "seek": 190568, "start": 1919.64, "end": 1923.16, "text": " How many correct bigrams out of the total number?", "tokens": [1012, 867, 3006, 955, 2356, 82, 484, 295, 264, 3217, 1230, 30], "temperature": 0.0, "avg_logprob": -0.13183121400720932, "compression_ratio": 1.6941176470588235, "no_speech_prob": 2.6687022909754887e-05}, {"id": 408, "seek": 190568, "start": 1923.16, "end": 1925.48, "text": " And so on.", "tokens": [400, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.13183121400720932, "compression_ratio": 1.6941176470588235, "no_speech_prob": 2.6687022909754887e-05}, {"id": 409, "seek": 190568, "start": 1925.48, "end": 1931.42, "text": " And then a limitation of this, so we're kind of building up to blue.", "tokens": [400, 550, 257, 27432, 295, 341, 11, 370, 321, 434, 733, 295, 2390, 493, 281, 3344, 13], "temperature": 0.0, "avg_logprob": -0.13183121400720932, "compression_ratio": 1.6941176470588235, "no_speech_prob": 2.6687022909754887e-05}, {"id": 410, "seek": 193142, "start": 1931.42, "end": 1936.2, "text": " It has these components.", "tokens": [467, 575, 613, 6677, 13], "temperature": 0.0, "avg_logprob": -0.09920119402701394, "compression_ratio": 1.6582278481012658, "no_speech_prob": 2.4681532522663474e-05}, {"id": 411, "seek": 193142, "start": 1936.2, "end": 1943.0, "text": " You need to keep in mind that, so I ate all those, I mean, those unigrams are correct.", "tokens": [509, 643, 281, 1066, 294, 1575, 300, 11, 370, 286, 8468, 439, 729, 11, 286, 914, 11, 729, 517, 33737, 82, 366, 3006, 13], "temperature": 0.0, "avg_logprob": -0.09920119402701394, "compression_ratio": 1.6582278481012658, "no_speech_prob": 2.4681532522663474e-05}, {"id": 412, "seek": 193142, "start": 1943.0, "end": 1945.1200000000001, "text": " There's only a single bigram here, but it's correct.", "tokens": [821, 311, 787, 257, 2167, 955, 2356, 510, 11, 457, 309, 311, 3006, 13], "temperature": 0.0, "avg_logprob": -0.09920119402701394, "compression_ratio": 1.6582278481012658, "no_speech_prob": 2.4681532522663474e-05}, {"id": 413, "seek": 193142, "start": 1945.1200000000001, "end": 1948.92, "text": " You could say, oh, this has got, you know, all the bigrams in here are in the original", "tokens": [509, 727, 584, 11, 1954, 11, 341, 575, 658, 11, 291, 458, 11, 439, 264, 955, 2356, 82, 294, 510, 366, 294, 264, 3380], "temperature": 0.0, "avg_logprob": -0.09920119402701394, "compression_ratio": 1.6582278481012658, "no_speech_prob": 2.4681532522663474e-05}, {"id": 414, "seek": 193142, "start": 1948.92, "end": 1951.0, "text": " or in the correct translation.", "tokens": [420, 294, 264, 3006, 12853, 13], "temperature": 0.0, "avg_logprob": -0.09920119402701394, "compression_ratio": 1.6582278481012658, "no_speech_prob": 2.4681532522663474e-05}, {"id": 415, "seek": 193142, "start": 1951.0, "end": 1955.5600000000002, "text": " But this is not a great translation because we left out three hazelnuts, which was a key", "tokens": [583, 341, 307, 406, 257, 869, 12853, 570, 321, 1411, 484, 1045, 11008, 9878, 3648, 11, 597, 390, 257, 2141], "temperature": 0.0, "avg_logprob": -0.09920119402701394, "compression_ratio": 1.6582278481012658, "no_speech_prob": 2.4681532522663474e-05}, {"id": 416, "seek": 193142, "start": 1955.5600000000002, "end": 1957.3200000000002, "text": " part of the sentence.", "tokens": [644, 295, 264, 8174, 13], "temperature": 0.0, "avg_logprob": -0.09920119402701394, "compression_ratio": 1.6582278481012658, "no_speech_prob": 2.4681532522663474e-05}, {"id": 417, "seek": 195732, "start": 1957.32, "end": 1962.9199999999998, "text": " And so blue assigns what's called a brevity penalty, and basically if your translation", "tokens": [400, 370, 3344, 6269, 82, 437, 311, 1219, 257, 1403, 23110, 16263, 11, 293, 1936, 498, 428, 12853], "temperature": 0.0, "avg_logprob": -0.15569398461318598, "compression_ratio": 1.724264705882353, "no_speech_prob": 9.516016689303797e-06}, {"id": 418, "seek": 195732, "start": 1962.9199999999998, "end": 1967.08, "text": " is shorter than the correct translation, it says that's wrong because you've probably", "tokens": [307, 11639, 813, 264, 3006, 12853, 11, 309, 1619, 300, 311, 2085, 570, 291, 600, 1391], "temperature": 0.0, "avg_logprob": -0.15569398461318598, "compression_ratio": 1.724264705882353, "no_speech_prob": 9.516016689303797e-06}, {"id": 419, "seek": 195732, "start": 1967.08, "end": 1972.32, "text": " left out some information that you needed, even if all the stuff you had was correct.", "tokens": [1411, 484, 512, 1589, 300, 291, 2978, 11, 754, 498, 439, 264, 1507, 291, 632, 390, 3006, 13], "temperature": 0.0, "avg_logprob": -0.15569398461318598, "compression_ratio": 1.724264705882353, "no_speech_prob": 9.516016689303797e-06}, {"id": 420, "seek": 195732, "start": 1972.32, "end": 1976.56, "text": " Going back to my, I ate a sandwich, that sentence is grammatically correct.", "tokens": [10963, 646, 281, 452, 11, 286, 8468, 257, 11141, 11, 300, 8174, 307, 17570, 5030, 3006, 13], "temperature": 0.0, "avg_logprob": -0.15569398461318598, "compression_ratio": 1.724264705882353, "no_speech_prob": 9.516016689303797e-06}, {"id": 421, "seek": 195732, "start": 1976.56, "end": 1982.32, "text": " It was in, is in the full one, but I've left out the since I'm hungry.", "tokens": [467, 390, 294, 11, 307, 294, 264, 1577, 472, 11, 457, 286, 600, 1411, 484, 264, 1670, 286, 478, 8067, 13], "temperature": 0.0, "avg_logprob": -0.15569398461318598, "compression_ratio": 1.724264705882353, "no_speech_prob": 9.516016689303797e-06}, {"id": 422, "seek": 195732, "start": 1982.32, "end": 1983.32, "text": " So that's the brevity penalty.", "tokens": [407, 300, 311, 264, 1403, 23110, 16263, 13], "temperature": 0.0, "avg_logprob": -0.15569398461318598, "compression_ratio": 1.724264705882353, "no_speech_prob": 9.516016689303797e-06}, {"id": 423, "seek": 195732, "start": 1983.32, "end": 1986.08, "text": " So now I'm going to switch over.", "tokens": [407, 586, 286, 478, 516, 281, 3679, 670, 13], "temperature": 0.0, "avg_logprob": -0.15569398461318598, "compression_ratio": 1.724264705882353, "no_speech_prob": 9.516016689303797e-06}, {"id": 424, "seek": 198608, "start": 1986.08, "end": 1989.8, "text": " So this, I really like this post and I recommend reading the whole thing.", "tokens": [407, 341, 11, 286, 534, 411, 341, 2183, 293, 286, 2748, 3760, 264, 1379, 551, 13], "temperature": 0.0, "avg_logprob": -0.24390265146891277, "compression_ratio": 1.5570175438596492, "no_speech_prob": 2.3185557438409887e-05}, {"id": 425, "seek": 198608, "start": 1989.8, "end": 1992.8799999999999, "text": " I made a few slides about it.", "tokens": [286, 1027, 257, 1326, 9788, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.24390265146891277, "compression_ratio": 1.5570175438596492, "no_speech_prob": 2.3185557438409887e-05}, {"id": 426, "seek": 198608, "start": 1992.8799999999999, "end": 1994.36, "text": " Oops.", "tokens": [21726, 13], "temperature": 0.0, "avg_logprob": -0.24390265146891277, "compression_ratio": 1.5570175438596492, "no_speech_prob": 2.3185557438409887e-05}, {"id": 427, "seek": 198608, "start": 1994.36, "end": 1996.12, "text": " Sorry.", "tokens": [4919, 13], "temperature": 0.0, "avg_logprob": -0.24390265146891277, "compression_ratio": 1.5570175438596492, "no_speech_prob": 2.3185557438409887e-05}, {"id": 428, "seek": 198608, "start": 1996.12, "end": 2004.8, "text": " So later in the post, Rachel highlights the strengths of blue are it's fast and easy to", "tokens": [407, 1780, 294, 264, 2183, 11, 14246, 14254, 264, 16986, 295, 3344, 366, 309, 311, 2370, 293, 1858, 281], "temperature": 0.0, "avg_logprob": -0.24390265146891277, "compression_ratio": 1.5570175438596492, "no_speech_prob": 2.3185557438409887e-05}, {"id": 429, "seek": 198608, "start": 2004.8, "end": 2008.84, "text": " calculate, particularly compared to having human translators.", "tokens": [8873, 11, 4098, 5347, 281, 1419, 1952, 5105, 3391, 13], "temperature": 0.0, "avg_logprob": -0.24390265146891277, "compression_ratio": 1.5570175438596492, "no_speech_prob": 2.3185557438409887e-05}, {"id": 430, "seek": 198608, "start": 2008.84, "end": 2012.9199999999998, "text": " And so that was kind of historically has been a technique and is an ongoing technique is", "tokens": [400, 370, 300, 390, 733, 295, 16180, 575, 668, 257, 6532, 293, 307, 364, 10452, 6532, 307], "temperature": 0.0, "avg_logprob": -0.24390265146891277, "compression_ratio": 1.5570175438596492, "no_speech_prob": 2.3185557438409887e-05}, {"id": 431, "seek": 201292, "start": 2012.92, "end": 2019.76, "text": " you can always get people and pay them to label your sentences of, you know, find bilingual", "tokens": [291, 393, 1009, 483, 561, 293, 1689, 552, 281, 7645, 428, 16579, 295, 11, 291, 458, 11, 915, 48757], "temperature": 0.0, "avg_logprob": -0.17808961868286133, "compression_ratio": 1.5901639344262295, "no_speech_prob": 5.682133632944897e-06}, {"id": 432, "seek": 201292, "start": 2019.76, "end": 2024.92, "text": " language speakers who can say like, this is a good translation or it's not.", "tokens": [2856, 9518, 567, 393, 584, 411, 11, 341, 307, 257, 665, 12853, 420, 309, 311, 406, 13], "temperature": 0.0, "avg_logprob": -0.17808961868286133, "compression_ratio": 1.5901639344262295, "no_speech_prob": 5.682133632944897e-06}, {"id": 433, "seek": 201292, "start": 2024.92, "end": 2028.8000000000002, "text": " And blue is also very commonly used.", "tokens": [400, 3344, 307, 611, 588, 12719, 1143, 13], "temperature": 0.0, "avg_logprob": -0.17808961868286133, "compression_ratio": 1.5901639344262295, "no_speech_prob": 5.682133632944897e-06}, {"id": 434, "seek": 201292, "start": 2028.8000000000002, "end": 2032.72, "text": " So it makes it easy to compare to benchmarks.", "tokens": [407, 309, 1669, 309, 1858, 281, 6794, 281, 43751, 13], "temperature": 0.0, "avg_logprob": -0.17808961868286133, "compression_ratio": 1.5901639344262295, "no_speech_prob": 5.682133632944897e-06}, {"id": 435, "seek": 201292, "start": 2032.72, "end": 2035.3200000000002, "text": " So those are, those are two, two important strengths.", "tokens": [407, 729, 366, 11, 729, 366, 732, 11, 732, 1021, 16986, 13], "temperature": 0.0, "avg_logprob": -0.17808961868286133, "compression_ratio": 1.5901639344262295, "no_speech_prob": 5.682133632944897e-06}, {"id": 436, "seek": 201292, "start": 2035.3200000000002, "end": 2037.96, "text": " However, there are some weaknesses.", "tokens": [2908, 11, 456, 366, 512, 24381, 13], "temperature": 0.0, "avg_logprob": -0.17808961868286133, "compression_ratio": 1.5901639344262295, "no_speech_prob": 5.682133632944897e-06}, {"id": 437, "seek": 201292, "start": 2037.96, "end": 2040.88, "text": " And so one is that it doesn't consider meaning.", "tokens": [400, 370, 472, 307, 300, 309, 1177, 380, 1949, 3620, 13], "temperature": 0.0, "avg_logprob": -0.17808961868286133, "compression_ratio": 1.5901639344262295, "no_speech_prob": 5.682133632944897e-06}, {"id": 438, "seek": 204088, "start": 2040.88, "end": 2044.3200000000002, "text": " Rachel and her post gives the example.", "tokens": [14246, 293, 720, 2183, 2709, 264, 1365, 13], "temperature": 0.0, "avg_logprob": -0.1591709755562447, "compression_ratio": 1.6804979253112033, "no_speech_prob": 7.41094800105202e-06}, {"id": 439, "seek": 204088, "start": 2044.3200000000002, "end": 2045.3200000000002, "text": " Suppose you have a sentence.", "tokens": [21360, 291, 362, 257, 8174, 13], "temperature": 0.0, "avg_logprob": -0.1591709755562447, "compression_ratio": 1.6804979253112033, "no_speech_prob": 7.41094800105202e-06}, {"id": 440, "seek": 204088, "start": 2045.3200000000002, "end": 2051.12, "text": " I think she has the French sentence that translates to I ate the apple.", "tokens": [286, 519, 750, 575, 264, 5522, 8174, 300, 28468, 281, 286, 8468, 264, 10606, 13], "temperature": 0.0, "avg_logprob": -0.1591709755562447, "compression_ratio": 1.6804979253112033, "no_speech_prob": 7.41094800105202e-06}, {"id": 441, "seek": 204088, "start": 2051.12, "end": 2055.08, "text": " Here's some wrong answers that all would have the same blue score.", "tokens": [1692, 311, 512, 2085, 6338, 300, 439, 576, 362, 264, 912, 3344, 6175, 13], "temperature": 0.0, "avg_logprob": -0.1591709755562447, "compression_ratio": 1.6804979253112033, "no_speech_prob": 7.41094800105202e-06}, {"id": 442, "seek": 204088, "start": 2055.08, "end": 2057.2000000000003, "text": " I ate an apple.", "tokens": [286, 8468, 364, 10606, 13], "temperature": 0.0, "avg_logprob": -0.1591709755562447, "compression_ratio": 1.6804979253112033, "no_speech_prob": 7.41094800105202e-06}, {"id": 443, "seek": 204088, "start": 2057.2000000000003, "end": 2063.04, "text": " And so, I mean, this is a pretty good translation, but it's got an instead of the, I consumed", "tokens": [400, 370, 11, 286, 914, 11, 341, 307, 257, 1238, 665, 12853, 11, 457, 309, 311, 658, 364, 2602, 295, 264, 11, 286, 21226], "temperature": 0.0, "avg_logprob": -0.1591709755562447, "compression_ratio": 1.6804979253112033, "no_speech_prob": 7.41094800105202e-06}, {"id": 444, "seek": 204088, "start": 2063.04, "end": 2065.1600000000003, "text": " the apple, which is also pretty good.", "tokens": [264, 10606, 11, 597, 307, 611, 1238, 665, 13], "temperature": 0.0, "avg_logprob": -0.1591709755562447, "compression_ratio": 1.6804979253112033, "no_speech_prob": 7.41094800105202e-06}, {"id": 445, "seek": 204088, "start": 2065.1600000000003, "end": 2069.12, "text": " It's just consumed is another word for, for eight.", "tokens": [467, 311, 445, 21226, 307, 1071, 1349, 337, 11, 337, 3180, 13], "temperature": 0.0, "avg_logprob": -0.1591709755562447, "compression_ratio": 1.6804979253112033, "no_speech_prob": 7.41094800105202e-06}, {"id": 446, "seek": 206912, "start": 2069.12, "end": 2071.2599999999998, "text": " I ate the potato.", "tokens": [286, 8468, 264, 7445, 13], "temperature": 0.0, "avg_logprob": -0.15970956786604953, "compression_ratio": 1.756, "no_speech_prob": 1.1658898984023836e-05}, {"id": 447, "seek": 206912, "start": 2071.2599999999998, "end": 2073.48, "text": " This is not a good translation.", "tokens": [639, 307, 406, 257, 665, 12853, 13], "temperature": 0.0, "avg_logprob": -0.15970956786604953, "compression_ratio": 1.756, "no_speech_prob": 1.1658898984023836e-05}, {"id": 448, "seek": 206912, "start": 2073.48, "end": 2075.8399999999997, "text": " The meaning of this is different, right?", "tokens": [440, 3620, 295, 341, 307, 819, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.15970956786604953, "compression_ratio": 1.756, "no_speech_prob": 1.1658898984023836e-05}, {"id": 449, "seek": 206912, "start": 2075.8399999999997, "end": 2079.64, "text": " Like the first two are, you can even say great translations.", "tokens": [1743, 264, 700, 732, 366, 11, 291, 393, 754, 584, 869, 37578, 13], "temperature": 0.0, "avg_logprob": -0.15970956786604953, "compression_ratio": 1.756, "no_speech_prob": 1.1658898984023836e-05}, {"id": 450, "seek": 206912, "start": 2079.64, "end": 2081.8399999999997, "text": " The third one, it's not yet.", "tokens": [440, 2636, 472, 11, 309, 311, 406, 1939, 13], "temperature": 0.0, "avg_logprob": -0.15970956786604953, "compression_ratio": 1.756, "no_speech_prob": 1.1658898984023836e-05}, {"id": 451, "seek": 206912, "start": 2081.8399999999997, "end": 2083.88, "text": " These all get the same blue score, right?", "tokens": [1981, 439, 483, 264, 912, 3344, 6175, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.15970956786604953, "compression_ratio": 1.756, "no_speech_prob": 1.1658898984023836e-05}, {"id": 452, "seek": 206912, "start": 2083.88, "end": 2084.98, "text": " And you can also look at them.", "tokens": [400, 291, 393, 611, 574, 412, 552, 13], "temperature": 0.0, "avg_logprob": -0.15970956786604953, "compression_ratio": 1.756, "no_speech_prob": 1.1658898984023836e-05}, {"id": 453, "seek": 206912, "start": 2084.98, "end": 2089.8399999999997, "text": " They're all basically off by one word from the correct translation, but it's actually", "tokens": [814, 434, 439, 1936, 766, 538, 472, 1349, 490, 264, 3006, 12853, 11, 457, 309, 311, 767], "temperature": 0.0, "avg_logprob": -0.15970956786604953, "compression_ratio": 1.756, "no_speech_prob": 1.1658898984023836e-05}, {"id": 454, "seek": 206912, "start": 2089.8399999999997, "end": 2095.6, "text": " significant which word they're off by and whether the, the thing that's given means", "tokens": [4776, 597, 1349, 436, 434, 766, 538, 293, 1968, 264, 11, 264, 551, 300, 311, 2212, 1355], "temperature": 0.0, "avg_logprob": -0.15970956786604953, "compression_ratio": 1.756, "no_speech_prob": 1.1658898984023836e-05}, {"id": 455, "seek": 206912, "start": 2095.6, "end": 2096.6, "text": " the same thing.", "tokens": [264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.15970956786604953, "compression_ratio": 1.756, "no_speech_prob": 1.1658898984023836e-05}, {"id": 456, "seek": 209660, "start": 2096.6, "end": 2100.7599999999998, "text": " And so that's, that's one, one limitation of blue.", "tokens": [400, 370, 300, 311, 11, 300, 311, 472, 11, 472, 27432, 295, 3344, 13], "temperature": 0.0, "avg_logprob": -0.19785401717476223, "compression_ratio": 1.742063492063492, "no_speech_prob": 9.972272891900502e-06}, {"id": 457, "seek": 209660, "start": 2100.7599999999998, "end": 2105.16, "text": " Now there is, it doesn't, it doesn't directly consider sentence structure.", "tokens": [823, 456, 307, 11, 309, 1177, 380, 11, 309, 1177, 380, 3838, 1949, 8174, 3877, 13], "temperature": 0.0, "avg_logprob": -0.19785401717476223, "compression_ratio": 1.742063492063492, "no_speech_prob": 9.972272891900502e-06}, {"id": 458, "seek": 209660, "start": 2105.16, "end": 2109.68, "text": " So you can get stuff that's, that's ungrammatical.", "tokens": [407, 291, 393, 483, 1507, 300, 311, 11, 300, 311, 517, 1342, 15677, 804, 13], "temperature": 0.0, "avg_logprob": -0.19785401717476223, "compression_ratio": 1.742063492063492, "no_speech_prob": 9.972272891900502e-06}, {"id": 459, "seek": 209660, "start": 2109.68, "end": 2112.52, "text": " Doesn't handle morphologically rich languages well.", "tokens": [12955, 380, 4813, 25778, 17157, 4593, 8650, 731, 13], "temperature": 0.0, "avg_logprob": -0.19785401717476223, "compression_ratio": 1.742063492063492, "no_speech_prob": 9.972272891900502e-06}, {"id": 460, "seek": 209660, "start": 2112.52, "end": 2117.64, "text": " Does anyone remember what morphologically rich languages or what an example of one is?", "tokens": [4402, 2878, 1604, 437, 25778, 17157, 4593, 8650, 420, 437, 364, 1365, 295, 472, 307, 30], "temperature": 0.0, "avg_logprob": -0.19785401717476223, "compression_ratio": 1.742063492063492, "no_speech_prob": 9.972272891900502e-06}, {"id": 461, "seek": 209660, "start": 2117.64, "end": 2118.64, "text": " Turkish.", "tokens": [18565, 13], "temperature": 0.0, "avg_logprob": -0.19785401717476223, "compression_ratio": 1.742063492063492, "no_speech_prob": 9.972272891900502e-06}, {"id": 462, "seek": 209660, "start": 2118.64, "end": 2119.64, "text": " Yeah, that's right.", "tokens": [865, 11, 300, 311, 558, 13], "temperature": 0.0, "avg_logprob": -0.19785401717476223, "compression_ratio": 1.742063492063492, "no_speech_prob": 9.972272891900502e-06}, {"id": 463, "seek": 209660, "start": 2119.64, "end": 2124.4, "text": " So we saw Turkish last time and that basically means morphemes are kind of these little pieces", "tokens": [407, 321, 1866, 18565, 1036, 565, 293, 300, 1936, 1355, 25778, 443, 279, 366, 733, 295, 613, 707, 3755], "temperature": 0.0, "avg_logprob": -0.19785401717476223, "compression_ratio": 1.742063492063492, "no_speech_prob": 9.972272891900502e-06}, {"id": 464, "seek": 212440, "start": 2124.4, "end": 2130.6600000000003, "text": " of words, Turkish, you have a lot of morphemes that can be put together in words and they", "tokens": [295, 2283, 11, 18565, 11, 291, 362, 257, 688, 295, 25778, 443, 279, 300, 393, 312, 829, 1214, 294, 2283, 293, 436], "temperature": 0.0, "avg_logprob": -0.1777816577391191, "compression_ratio": 1.6150234741784038, "no_speech_prob": 6.240529728529509e-06}, {"id": 465, "seek": 212440, "start": 2130.6600000000003, "end": 2134.2000000000003, "text": " have meaning and blue doesn't capture that, right?", "tokens": [362, 3620, 293, 3344, 1177, 380, 7983, 300, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1777816577391191, "compression_ratio": 1.6150234741784038, "no_speech_prob": 6.240529728529509e-06}, {"id": 466, "seek": 212440, "start": 2134.2000000000003, "end": 2141.64, "text": " We don't have a way of looking at kind of those pieces and assigning, assigning them", "tokens": [492, 500, 380, 362, 257, 636, 295, 1237, 412, 733, 295, 729, 3755, 293, 49602, 11, 49602, 552], "temperature": 0.0, "avg_logprob": -0.1777816577391191, "compression_ratio": 1.6150234741784038, "no_speech_prob": 6.240529728529509e-06}, {"id": 467, "seek": 212440, "start": 2141.64, "end": 2142.64, "text": " scores.", "tokens": [13444, 13], "temperature": 0.0, "avg_logprob": -0.1777816577391191, "compression_ratio": 1.6150234741784038, "no_speech_prob": 6.240529728529509e-06}, {"id": 468, "seek": 212440, "start": 2142.64, "end": 2148.04, "text": " It doesn't necessarily correspond with human judgments either.", "tokens": [467, 1177, 380, 4725, 6805, 365, 1952, 40337, 2139, 13], "temperature": 0.0, "avg_logprob": -0.1777816577391191, "compression_ratio": 1.6150234741784038, "no_speech_prob": 6.240529728529509e-06}, {"id": 469, "seek": 212440, "start": 2148.04, "end": 2152.0, "text": " So these are, these are some limitations of it.", "tokens": [407, 613, 366, 11, 613, 366, 512, 15705, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.1777816577391191, "compression_ratio": 1.6150234741784038, "no_speech_prob": 6.240529728529509e-06}, {"id": 470, "seek": 215200, "start": 2152.0, "end": 2156.72, "text": " I like, so Rachel's kind of recommendation in the end as someone who has studied this", "tokens": [286, 411, 11, 370, 14246, 311, 733, 295, 11879, 294, 264, 917, 382, 1580, 567, 575, 9454, 341], "temperature": 0.0, "avg_logprob": -0.09924437075245138, "compression_ratio": 1.6639676113360324, "no_speech_prob": 2.3921127649373375e-05}, {"id": 471, "seek": 215200, "start": 2156.72, "end": 2163.04, "text": " a lot is if you're putting your system into production, she thinks you should do at least", "tokens": [257, 688, 307, 498, 291, 434, 3372, 428, 1185, 666, 4265, 11, 750, 7309, 291, 820, 360, 412, 1935], "temperature": 0.0, "avg_logprob": -0.09924437075245138, "compression_ratio": 1.6639676113360324, "no_speech_prob": 2.3921127649373375e-05}, {"id": 472, "seek": 215200, "start": 2163.04, "end": 2169.04, "text": " one round of evaluation with human experts, which I think makes a lot of sense in terms", "tokens": [472, 3098, 295, 13344, 365, 1952, 8572, 11, 597, 286, 519, 1669, 257, 688, 295, 2020, 294, 2115], "temperature": 0.0, "avg_logprob": -0.09924437075245138, "compression_ratio": 1.6639676113360324, "no_speech_prob": 2.3921127649373375e-05}, {"id": 473, "seek": 215200, "start": 2169.04, "end": 2174.36, "text": " of there's not, right now there's not a perfect metric that captures all the things that we", "tokens": [295, 456, 311, 406, 11, 558, 586, 456, 311, 406, 257, 2176, 20678, 300, 27986, 439, 264, 721, 300, 321], "temperature": 0.0, "avg_logprob": -0.09924437075245138, "compression_ratio": 1.6639676113360324, "no_speech_prob": 2.3921127649373375e-05}, {"id": 474, "seek": 215200, "start": 2174.36, "end": 2177.36, "text": " as humans think about with, with language and sentence.", "tokens": [382, 6255, 519, 466, 365, 11, 365, 2856, 293, 8174, 13], "temperature": 0.0, "avg_logprob": -0.09924437075245138, "compression_ratio": 1.6639676113360324, "no_speech_prob": 2.3921127649373375e-05}, {"id": 475, "seek": 217736, "start": 2177.36, "end": 2182.88, "text": " But you are, you're still going to want an automatic metric to use kind of as your training", "tokens": [583, 291, 366, 11, 291, 434, 920, 516, 281, 528, 364, 12509, 20678, 281, 764, 733, 295, 382, 428, 3097], "temperature": 0.0, "avg_logprob": -0.15819272455179467, "compression_ratio": 1.6729323308270676, "no_speech_prob": 4.092620656592771e-06}, {"id": 476, "seek": 217736, "start": 2182.88, "end": 2187.96, "text": " and before you get to that point because human evaluation is more expensive.", "tokens": [293, 949, 291, 483, 281, 300, 935, 570, 1952, 13344, 307, 544, 5124, 13], "temperature": 0.0, "avg_logprob": -0.15819272455179467, "compression_ratio": 1.6729323308270676, "no_speech_prob": 4.092620656592771e-06}, {"id": 477, "seek": 217736, "start": 2187.96, "end": 2191.32, "text": " And so she says to use blue if and only if you're doing machine translation.", "tokens": [400, 370, 750, 1619, 281, 764, 3344, 498, 293, 787, 498, 291, 434, 884, 3479, 12853, 13], "temperature": 0.0, "avg_logprob": -0.15819272455179467, "compression_ratio": 1.6729323308270676, "no_speech_prob": 4.092620656592771e-06}, {"id": 478, "seek": 217736, "start": 2191.32, "end": 2195.56, "text": " I think the issue is people sometimes apply it to other seek to seek tasks, which is not", "tokens": [286, 519, 264, 2734, 307, 561, 2171, 3079, 309, 281, 661, 8075, 281, 8075, 9608, 11, 597, 307, 406], "temperature": 0.0, "avg_logprob": -0.15819272455179467, "compression_ratio": 1.6729323308270676, "no_speech_prob": 4.092620656592771e-06}, {"id": 479, "seek": 217736, "start": 2195.56, "end": 2198.2000000000003, "text": " what it was developed for.", "tokens": [437, 309, 390, 4743, 337, 13], "temperature": 0.0, "avg_logprob": -0.15819272455179467, "compression_ratio": 1.6729323308270676, "no_speech_prob": 4.092620656592771e-06}, {"id": 480, "seek": 217736, "start": 2198.2000000000003, "end": 2203.84, "text": " You're evaluating across an entire corpus and you're familiar with the limitations.", "tokens": [509, 434, 27479, 2108, 364, 2302, 1181, 31624, 293, 291, 434, 4963, 365, 264, 15705, 13], "temperature": 0.0, "avg_logprob": -0.15819272455179467, "compression_ratio": 1.6729323308270676, "no_speech_prob": 4.092620656592771e-06}, {"id": 481, "seek": 220384, "start": 2203.84, "end": 2207.6000000000004, "text": " You know, this is not a perfect metric.", "tokens": [509, 458, 11, 341, 307, 406, 257, 2176, 20678, 13], "temperature": 0.0, "avg_logprob": -0.16633964337800677, "compression_ratio": 1.5990990990990992, "no_speech_prob": 4.812420684174867e-07}, {"id": 482, "seek": 220384, "start": 2207.6000000000004, "end": 2211.8, "text": " And then one other thing I wanted to highlight from her blog posts, and this is more just", "tokens": [400, 550, 472, 661, 551, 286, 1415, 281, 5078, 490, 720, 6968, 12300, 11, 293, 341, 307, 544, 445], "temperature": 0.0, "avg_logprob": -0.16633964337800677, "compression_ratio": 1.5990990990990992, "no_speech_prob": 4.812420684174867e-07}, {"id": 483, "seek": 220384, "start": 2211.8, "end": 2220.92, "text": " if you're really interested in this, is she, let me find it.", "tokens": [498, 291, 434, 534, 3102, 294, 341, 11, 307, 750, 11, 718, 385, 915, 309, 13], "temperature": 0.0, "avg_logprob": -0.16633964337800677, "compression_ratio": 1.5990990990990992, "no_speech_prob": 4.812420684174867e-07}, {"id": 484, "seek": 220384, "start": 2220.92, "end": 2224.48, "text": " She covered, and so she goes into a lot, a lot more detail than I've covered.", "tokens": [1240, 5343, 11, 293, 370, 750, 1709, 666, 257, 688, 11, 257, 688, 544, 2607, 813, 286, 600, 5343, 13], "temperature": 0.0, "avg_logprob": -0.16633964337800677, "compression_ratio": 1.5990990990990992, "no_speech_prob": 4.812420684174867e-07}, {"id": 485, "seek": 220384, "start": 2224.48, "end": 2230.6400000000003, "text": " So she covers kind of what the literature says about both the limitations of, of blue.", "tokens": [407, 750, 10538, 733, 295, 437, 264, 10394, 1619, 466, 1293, 264, 15705, 295, 11, 295, 3344, 13], "temperature": 0.0, "avg_logprob": -0.16633964337800677, "compression_ratio": 1.5990990990990992, "no_speech_prob": 4.812420684174867e-07}, {"id": 486, "seek": 223064, "start": 2230.64, "end": 2235.2, "text": " So papers on kind of times when you, you don't want to use blue.", "tokens": [407, 10577, 322, 733, 295, 1413, 562, 291, 11, 291, 500, 380, 528, 281, 764, 3344, 13], "temperature": 0.0, "avg_logprob": -0.17828541240472903, "compression_ratio": 1.5769230769230769, "no_speech_prob": 1.644081567064859e-05}, {"id": 487, "seek": 223064, "start": 2235.2, "end": 2238.24, "text": " A lot of resources on that.", "tokens": [316, 688, 295, 3593, 322, 300, 13], "temperature": 0.0, "avg_logprob": -0.17828541240472903, "compression_ratio": 1.5769230769230769, "no_speech_prob": 1.644081567064859e-05}, {"id": 488, "seek": 223064, "start": 2238.24, "end": 2239.24, "text": " What are alternatives?", "tokens": [708, 366, 20478, 30], "temperature": 0.0, "avg_logprob": -0.17828541240472903, "compression_ratio": 1.5769230769230769, "no_speech_prob": 1.644081567064859e-05}, {"id": 489, "seek": 223064, "start": 2239.24, "end": 2247.24, "text": " So there are people kind of working on this question of what are other options to blue", "tokens": [407, 456, 366, 561, 733, 295, 1364, 322, 341, 1168, 295, 437, 366, 661, 3956, 281, 3344], "temperature": 0.0, "avg_logprob": -0.17828541240472903, "compression_ratio": 1.5769230769230769, "no_speech_prob": 1.644081567064859e-05}, {"id": 490, "seek": 223064, "start": 2247.24, "end": 2253.64, "text": " and including so there are measures from other areas of NLP like perplexity or F score or", "tokens": [293, 3009, 370, 456, 366, 8000, 490, 661, 3179, 295, 426, 45196, 411, 680, 18945, 507, 420, 479, 6175, 420], "temperature": 0.0, "avg_logprob": -0.17828541240472903, "compression_ratio": 1.5769230769230769, "no_speech_prob": 1.644081567064859e-05}, {"id": 491, "seek": 223064, "start": 2253.64, "end": 2258.3599999999997, "text": " word error rate that you could use.", "tokens": [1349, 6713, 3314, 300, 291, 727, 764, 13], "temperature": 0.0, "avg_logprob": -0.17828541240472903, "compression_ratio": 1.5769230769230769, "no_speech_prob": 1.644081567064859e-05}, {"id": 492, "seek": 225836, "start": 2258.36, "end": 2262.6200000000003, "text": " And then here, this is, this is an area of active research of people are developing other", "tokens": [400, 550, 510, 11, 341, 307, 11, 341, 307, 364, 1859, 295, 4967, 2132, 295, 561, 366, 6416, 661], "temperature": 0.0, "avg_logprob": -0.15699050386073227, "compression_ratio": 1.713768115942029, "no_speech_prob": 3.704697155626491e-05}, {"id": 493, "seek": 225836, "start": 2262.6200000000003, "end": 2265.36, "text": " metrics to use as alternatives to blue.", "tokens": [16367, 281, 764, 382, 20478, 281, 3344, 13], "temperature": 0.0, "avg_logprob": -0.15699050386073227, "compression_ratio": 1.713768115942029, "no_speech_prob": 3.704697155626491e-05}, {"id": 494, "seek": 225836, "start": 2265.36, "end": 2268.6800000000003, "text": " And so this is more just if you're interested in the topic, I thought it was an interesting", "tokens": [400, 370, 341, 307, 544, 445, 498, 291, 434, 3102, 294, 264, 4829, 11, 286, 1194, 309, 390, 364, 1880], "temperature": 0.0, "avg_logprob": -0.15699050386073227, "compression_ratio": 1.713768115942029, "no_speech_prob": 3.704697155626491e-05}, {"id": 495, "seek": 225836, "start": 2268.6800000000003, "end": 2270.1600000000003, "text": " topic.", "tokens": [4829, 13], "temperature": 0.0, "avg_logprob": -0.15699050386073227, "compression_ratio": 1.713768115942029, "no_speech_prob": 3.704697155626491e-05}, {"id": 496, "seek": 225836, "start": 2270.1600000000003, "end": 2277.76, "text": " And I'll say this question of how, how to evaluate the goodness of your model, like", "tokens": [400, 286, 603, 584, 341, 1168, 295, 577, 11, 577, 281, 13059, 264, 8387, 295, 428, 2316, 11, 411], "temperature": 0.0, "avg_logprob": -0.15699050386073227, "compression_ratio": 1.713768115942029, "no_speech_prob": 3.704697155626491e-05}, {"id": 497, "seek": 225836, "start": 2277.76, "end": 2282.8, "text": " whether that's an NLP or in another area is important to think about because metrics,", "tokens": [1968, 300, 311, 364, 426, 45196, 420, 294, 1071, 1859, 307, 1021, 281, 519, 466, 570, 16367, 11], "temperature": 0.0, "avg_logprob": -0.15699050386073227, "compression_ratio": 1.713768115942029, "no_speech_prob": 3.704697155626491e-05}, {"id": 498, "seek": 225836, "start": 2282.8, "end": 2287.58, "text": " I would say metrics are always just a proxy for what you truly care about.", "tokens": [286, 576, 584, 16367, 366, 1009, 445, 257, 29690, 337, 437, 291, 4908, 1127, 466, 13], "temperature": 0.0, "avg_logprob": -0.15699050386073227, "compression_ratio": 1.713768115942029, "no_speech_prob": 3.704697155626491e-05}, {"id": 499, "seek": 228758, "start": 2287.58, "end": 2295.86, "text": " And it can be dangerous to think that the metric is capturing everything.", "tokens": [400, 309, 393, 312, 5795, 281, 519, 300, 264, 20678, 307, 23384, 1203, 13], "temperature": 0.0, "avg_logprob": -0.20878998935222626, "compression_ratio": 1.4943181818181819, "no_speech_prob": 9.665655852586497e-06}, {"id": 500, "seek": 228758, "start": 2295.86, "end": 2303.24, "text": " So returning to this, this is kind of how we're implementing blue.", "tokens": [407, 12678, 281, 341, 11, 341, 307, 733, 295, 577, 321, 434, 18114, 3344, 13], "temperature": 0.0, "avg_logprob": -0.20878998935222626, "compression_ratio": 1.4943181818181819, "no_speech_prob": 9.665655852586497e-06}, {"id": 501, "seek": 228758, "start": 2303.24, "end": 2306.16, "text": " And let me find the key line.", "tokens": [400, 718, 385, 915, 264, 2141, 1622, 13], "temperature": 0.0, "avg_logprob": -0.20878998935222626, "compression_ratio": 1.4943181818181819, "no_speech_prob": 9.665655852586497e-06}, {"id": 502, "seek": 228758, "start": 2306.16, "end": 2315.18, "text": " I think here is basically looking at multiplying together four perc... or taking the average", "tokens": [286, 519, 510, 307, 1936, 1237, 412, 30955, 1214, 1451, 680, 66, 485, 420, 1940, 264, 4274], "temperature": 0.0, "avg_logprob": -0.20878998935222626, "compression_ratio": 1.4943181818181819, "no_speech_prob": 9.665655852586497e-06}, {"id": 503, "seek": 231518, "start": 2315.18, "end": 2323.8399999999997, "text": " of these four precisions and these correspond to the unigrams, bigrams, trigrams and four", "tokens": [295, 613, 1451, 4346, 4252, 293, 613, 6805, 281, 264, 517, 33737, 82, 11, 955, 2356, 82, 11, 504, 33737, 82, 293, 1451], "temperature": 0.0, "avg_logprob": -0.21588018536567688, "compression_ratio": 1.5897435897435896, "no_speech_prob": 5.682070423063124e-06}, {"id": 504, "seek": 231518, "start": 2323.8399999999997, "end": 2332.16, "text": " grams that we've captured averaging those and then multiplying by the brevity penalty.", "tokens": [11899, 300, 321, 600, 11828, 47308, 729, 293, 550, 30955, 538, 264, 1403, 23110, 16263, 13], "temperature": 0.0, "avg_logprob": -0.21588018536567688, "compression_ratio": 1.5897435897435896, "no_speech_prob": 5.682070423063124e-06}, {"id": 505, "seek": 231518, "start": 2332.16, "end": 2337.44, "text": " And just to review, can anyone remind me why we need a brevity penalty?", "tokens": [400, 445, 281, 3131, 11, 393, 2878, 4160, 385, 983, 321, 643, 257, 1403, 23110, 16263, 30], "temperature": 0.0, "avg_logprob": -0.21588018536567688, "compression_ratio": 1.5897435897435896, "no_speech_prob": 5.682070423063124e-06}, {"id": 506, "seek": 233744, "start": 2337.44, "end": 2347.68, "text": " Yeah, so the answer was in case it's too short.", "tokens": [865, 11, 370, 264, 1867, 390, 294, 1389, 309, 311, 886, 2099, 13], "temperature": 0.0, "avg_logprob": -0.17188699952848666, "compression_ratio": 1.6411764705882352, "no_speech_prob": 3.905366156686796e-06}, {"id": 507, "seek": 233744, "start": 2347.68, "end": 2354.08, "text": " So we saw the example with I8-3 hazelnuts, I8, all the unigrams in there match, all the", "tokens": [407, 321, 1866, 264, 1365, 365, 286, 23, 12, 18, 11008, 9878, 3648, 11, 286, 23, 11, 439, 264, 517, 33737, 82, 294, 456, 2995, 11, 439, 264], "temperature": 0.0, "avg_logprob": -0.17188699952848666, "compression_ratio": 1.6411764705882352, "no_speech_prob": 3.905366156686796e-06}, {"id": 508, "seek": 233744, "start": 2354.08, "end": 2359.32, "text": " bigrams in there match, but it's short, it's left out information and we want to penalize", "tokens": [955, 2356, 82, 294, 456, 2995, 11, 457, 309, 311, 2099, 11, 309, 311, 1411, 484, 1589, 293, 321, 528, 281, 13661, 1125], "temperature": 0.0, "avg_logprob": -0.17188699952848666, "compression_ratio": 1.6411764705882352, "no_speech_prob": 3.905366156686796e-06}, {"id": 509, "seek": 233744, "start": 2359.32, "end": 2360.32, "text": " that.", "tokens": [300, 13], "temperature": 0.0, "avg_logprob": -0.17188699952848666, "compression_ratio": 1.6411764705882352, "no_speech_prob": 3.905366156686796e-06}, {"id": 510, "seek": 233744, "start": 2360.32, "end": 2363.12, "text": " So that's how, that's how blue deals with that.", "tokens": [407, 300, 311, 577, 11, 300, 311, 577, 3344, 11215, 365, 300, 13], "temperature": 0.0, "avg_logprob": -0.17188699952848666, "compression_ratio": 1.6411764705882352, "no_speech_prob": 3.905366156686796e-06}, {"id": 511, "seek": 236312, "start": 2363.12, "end": 2367.68, "text": " Any other questions about blue?", "tokens": [2639, 661, 1651, 466, 3344, 30], "temperature": 0.0, "avg_logprob": -0.23561358451843262, "compression_ratio": 1.4534883720930232, "no_speech_prob": 6.438724540203111e-06}, {"id": 512, "seek": 236312, "start": 2367.68, "end": 2374.4, "text": " All right, I want to highlight, there's a...", "tokens": [1057, 558, 11, 286, 528, 281, 5078, 11, 456, 311, 257, 485], "temperature": 0.0, "avg_logprob": -0.23561358451843262, "compression_ratio": 1.4534883720930232, "no_speech_prob": 6.438724540203111e-06}, {"id": 513, "seek": 236312, "start": 2374.4, "end": 2377.12, "text": " So this is probably going to be in a weird place.", "tokens": [407, 341, 307, 1391, 516, 281, 312, 294, 257, 3657, 1081, 13], "temperature": 0.0, "avg_logprob": -0.23561358451843262, "compression_ratio": 1.4534883720930232, "no_speech_prob": 6.438724540203111e-06}, {"id": 514, "seek": 236312, "start": 2377.12, "end": 2383.68, "text": " There's another notebook that I've added to the repo, Blue Metrics, that Sylvain created.", "tokens": [821, 311, 1071, 21060, 300, 286, 600, 3869, 281, 264, 49040, 11, 8510, 6377, 10716, 11, 300, 3902, 14574, 491, 2942, 13], "temperature": 0.0, "avg_logprob": -0.23561358451843262, "compression_ratio": 1.4534883720930232, "no_speech_prob": 6.438724540203111e-06}, {"id": 515, "seek": 236312, "start": 2383.68, "end": 2387.16, "text": " And this is also a nice notebook.", "tokens": [400, 341, 307, 611, 257, 1481, 21060, 13], "temperature": 0.0, "avg_logprob": -0.23561358451843262, "compression_ratio": 1.4534883720930232, "no_speech_prob": 6.438724540203111e-06}, {"id": 516, "seek": 238716, "start": 2387.16, "end": 2393.8799999999997, "text": " He walks through an example of the cat is walking in the garden, how good is the cat", "tokens": [634, 12896, 807, 364, 1365, 295, 264, 3857, 307, 4494, 294, 264, 7431, 11, 577, 665, 307, 264, 3857], "temperature": 0.0, "avg_logprob": -0.15527446071306863, "compression_ratio": 1.716279069767442, "no_speech_prob": 9.972638508770615e-06}, {"id": 517, "seek": 238716, "start": 2393.8799999999997, "end": 2396.68, "text": " is running in the fields as a translation.", "tokens": [307, 2614, 294, 264, 7909, 382, 257, 12853, 13], "temperature": 0.0, "avg_logprob": -0.15527446071306863, "compression_ratio": 1.716279069767442, "no_speech_prob": 9.972638508770615e-06}, {"id": 518, "seek": 238716, "start": 2396.68, "end": 2404.0, "text": " I'm sorry, this is the correct one and this is what your model gives, evaluating that.", "tokens": [286, 478, 2597, 11, 341, 307, 264, 3006, 472, 293, 341, 307, 437, 428, 2316, 2709, 11, 27479, 300, 13], "temperature": 0.0, "avg_logprob": -0.15527446071306863, "compression_ratio": 1.716279069767442, "no_speech_prob": 9.972638508770615e-06}, {"id": 519, "seek": 238716, "start": 2404.0, "end": 2409.8399999999997, "text": " And he kind of walks through, okay, we have three correct bigrams, the cat is, and in", "tokens": [400, 415, 733, 295, 12896, 807, 11, 1392, 11, 321, 362, 1045, 3006, 955, 2356, 82, 11, 264, 3857, 307, 11, 293, 294], "temperature": 0.0, "avg_logprob": -0.15527446071306863, "compression_ratio": 1.716279069767442, "no_speech_prob": 9.972638508770615e-06}, {"id": 520, "seek": 238716, "start": 2409.8399999999997, "end": 2416.64, "text": " the looking at these shows how to calculate the blue length penalty.", "tokens": [264, 1237, 412, 613, 3110, 577, 281, 8873, 264, 3344, 4641, 16263, 13], "temperature": 0.0, "avg_logprob": -0.15527446071306863, "compression_ratio": 1.716279069767442, "no_speech_prob": 9.972638508770615e-06}, {"id": 521, "seek": 241664, "start": 2416.64, "end": 2420.48, "text": " And so that's helpful too.", "tokens": [400, 370, 300, 311, 4961, 886, 13], "temperature": 0.0, "avg_logprob": -0.17273223933888904, "compression_ratio": 1.467065868263473, "no_speech_prob": 1.450866420782404e-05}, {"id": 522, "seek": 241664, "start": 2420.48, "end": 2427.68, "text": " And so the reason we've implemented our own is that NLTK uses list of tokenized text,", "tokens": [400, 370, 264, 1778, 321, 600, 12270, 527, 1065, 307, 300, 426, 43, 51, 42, 4960, 1329, 295, 14862, 1602, 2487, 11], "temperature": 0.0, "avg_logprob": -0.17273223933888904, "compression_ratio": 1.467065868263473, "no_speech_prob": 1.450866420782404e-05}, {"id": 523, "seek": 241664, "start": 2427.68, "end": 2429.16, "text": " so it's much slower.", "tokens": [370, 309, 311, 709, 14009, 13], "temperature": 0.0, "avg_logprob": -0.17273223933888904, "compression_ratio": 1.467065868263473, "no_speech_prob": 1.450866420782404e-05}, {"id": 524, "seek": 241664, "start": 2429.16, "end": 2433.8399999999997, "text": " So we wanted something quicker since we have already numericalized the text and have our", "tokens": [407, 321, 1415, 746, 16255, 1670, 321, 362, 1217, 29054, 1602, 264, 2487, 293, 362, 527], "temperature": 0.0, "avg_logprob": -0.17273223933888904, "compression_ratio": 1.467065868263473, "no_speech_prob": 1.450866420782404e-05}, {"id": 525, "seek": 241664, "start": 2433.8399999999997, "end": 2436.08, "text": " numericalized answers.", "tokens": [29054, 1602, 6338, 13], "temperature": 0.0, "avg_logprob": -0.17273223933888904, "compression_ratio": 1.467065868263473, "no_speech_prob": 1.450866420782404e-05}, {"id": 526, "seek": 243608, "start": 2436.08, "end": 2451.84, "text": " So check out this notebook for more detail about how this works.", "tokens": [407, 1520, 484, 341, 21060, 337, 544, 2607, 466, 577, 341, 1985, 13], "temperature": 0.0, "avg_logprob": -0.17495085158438053, "compression_ratio": 1.3941605839416058, "no_speech_prob": 2.4824369120324263e-06}, {"id": 527, "seek": 243608, "start": 2451.84, "end": 2457.3199999999997, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.17495085158438053, "compression_ratio": 1.3941605839416058, "no_speech_prob": 2.4824369120324263e-06}, {"id": 528, "seek": 243608, "start": 2457.3199999999997, "end": 2458.44, "text": " So this will be nice.", "tokens": [407, 341, 486, 312, 1481, 13], "temperature": 0.0, "avg_logprob": -0.17495085158438053, "compression_ratio": 1.3941605839416058, "no_speech_prob": 2.4824369120324263e-06}, {"id": 529, "seek": 243608, "start": 2458.44, "end": 2465.08, "text": " So we're going to add to our learner for metrics, both the accuracy and the blue score to get", "tokens": [407, 321, 434, 516, 281, 909, 281, 527, 33347, 337, 16367, 11, 1293, 264, 14170, 293, 264, 3344, 6175, 281, 483], "temperature": 0.0, "avg_logprob": -0.17495085158438053, "compression_ratio": 1.3941605839416058, "no_speech_prob": 2.4824369120324263e-06}, {"id": 530, "seek": 246508, "start": 2465.08, "end": 2466.64, "text": " more information.", "tokens": [544, 1589, 13], "temperature": 0.0, "avg_logprob": -0.09699787182754345, "compression_ratio": 1.6575342465753424, "no_speech_prob": 1.983249603654258e-05}, {"id": 531, "seek": 246508, "start": 2466.64, "end": 2474.64, "text": " So kind of jumping back.", "tokens": [407, 733, 295, 11233, 646, 13], "temperature": 0.0, "avg_logprob": -0.09699787182754345, "compression_ratio": 1.6575342465753424, "no_speech_prob": 1.983249603654258e-05}, {"id": 532, "seek": 246508, "start": 2474.64, "end": 2478.96, "text": " The problem that had motivated us is here we're training, we can see that the loss is", "tokens": [440, 1154, 300, 632, 14515, 505, 307, 510, 321, 434, 3097, 11, 321, 393, 536, 300, 264, 4470, 307], "temperature": 0.0, "avg_logprob": -0.09699787182754345, "compression_ratio": 1.6575342465753424, "no_speech_prob": 1.983249603654258e-05}, {"id": 533, "seek": 246508, "start": 2478.96, "end": 2484.04, "text": " decreasing on the training and validation sets, but this wasn't very human interpretable.", "tokens": [23223, 322, 264, 3097, 293, 24071, 6352, 11, 457, 341, 2067, 380, 588, 1952, 7302, 712, 13], "temperature": 0.0, "avg_logprob": -0.09699787182754345, "compression_ratio": 1.6575342465753424, "no_speech_prob": 1.983249603654258e-05}, {"id": 534, "seek": 246508, "start": 2484.04, "end": 2488.3199999999997, "text": " And so we talked about kind of two ways we're going to make this more human interpretable", "tokens": [400, 370, 321, 2825, 466, 733, 295, 732, 2098, 321, 434, 516, 281, 652, 341, 544, 1952, 7302, 712], "temperature": 0.0, "avg_logprob": -0.09699787182754345, "compression_ratio": 1.6575342465753424, "no_speech_prob": 1.983249603654258e-05}, {"id": 535, "seek": 246508, "start": 2488.3199999999997, "end": 2494.22, "text": " is adding the accuracy and then adding the blue score.", "tokens": [307, 5127, 264, 14170, 293, 550, 5127, 264, 3344, 6175, 13], "temperature": 0.0, "avg_logprob": -0.09699787182754345, "compression_ratio": 1.6575342465753424, "no_speech_prob": 1.983249603654258e-05}, {"id": 536, "seek": 249422, "start": 2494.22, "end": 2498.16, "text": " So going back down, we can run this again.", "tokens": [407, 516, 646, 760, 11, 321, 393, 1190, 341, 797, 13], "temperature": 0.0, "avg_logprob": -0.11157382480681889, "compression_ratio": 1.5098039215686274, "no_speech_prob": 2.9943785193609074e-06}, {"id": 537, "seek": 249422, "start": 2498.16, "end": 2506.3999999999996, "text": " And here these are entered as a list of metrics into our learner.", "tokens": [400, 510, 613, 366, 9065, 382, 257, 1329, 295, 16367, 666, 527, 33347, 13], "temperature": 0.0, "avg_logprob": -0.11157382480681889, "compression_ratio": 1.5098039215686274, "no_speech_prob": 2.9943785193609074e-06}, {"id": 538, "seek": 249422, "start": 2506.3999999999996, "end": 2513.8399999999997, "text": " We can run this again and our losses should be similar, but now we can see the accuracy", "tokens": [492, 393, 1190, 341, 797, 293, 527, 15352, 820, 312, 2531, 11, 457, 586, 321, 393, 536, 264, 14170], "temperature": 0.0, "avg_logprob": -0.11157382480681889, "compression_ratio": 1.5098039215686274, "no_speech_prob": 2.9943785193609074e-06}, {"id": 539, "seek": 249422, "start": 2513.8399999999997, "end": 2519.9199999999996, "text": " by word as well as the blue score.", "tokens": [538, 1349, 382, 731, 382, 264, 3344, 6175, 13], "temperature": 0.0, "avg_logprob": -0.11157382480681889, "compression_ratio": 1.5098039215686274, "no_speech_prob": 2.9943785193609074e-06}, {"id": 540, "seek": 251992, "start": 2519.92, "end": 2527.08, "text": " And then we can also test this out and see what are the predictions on particular pieces", "tokens": [400, 550, 321, 393, 611, 1500, 341, 484, 293, 536, 437, 366, 264, 21264, 322, 1729, 3755], "temperature": 0.0, "avg_logprob": -0.1402381420135498, "compression_ratio": 1.2946428571428572, "no_speech_prob": 8.267689736385364e-06}, {"id": 541, "seek": 251992, "start": 2527.08, "end": 2529.88, "text": " of data, which is always useful to do.", "tokens": [295, 1412, 11, 597, 307, 1009, 4420, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.1402381420135498, "compression_ratio": 1.2946428571428572, "no_speech_prob": 8.267689736385364e-06}, {"id": 542, "seek": 251992, "start": 2529.88, "end": 2544.12, "text": " Hold on a moment.", "tokens": [6962, 322, 257, 1623, 13], "temperature": 0.0, "avg_logprob": -0.1402381420135498, "compression_ratio": 1.2946428571428572, "no_speech_prob": 8.267689736385364e-06}, {"id": 543, "seek": 254412, "start": 2544.12, "end": 2552.3199999999997, "text": " And so here we'll append onto our inputs, targets, and outputs to get the predictions.", "tokens": [400, 370, 510, 321, 603, 34116, 3911, 527, 15743, 11, 12911, 11, 293, 23930, 281, 483, 264, 21264, 13], "temperature": 0.0, "avg_logprob": -0.16732886497010577, "compression_ratio": 1.6919431279620853, "no_speech_prob": 7.295993327716133e-06}, {"id": 544, "seek": 254412, "start": 2552.3199999999997, "end": 2556.88, "text": " We can see this is not a very good one.", "tokens": [492, 393, 536, 341, 307, 406, 257, 588, 665, 472, 13], "temperature": 0.0, "avg_logprob": -0.16732886497010577, "compression_ratio": 1.6919431279620853, "no_speech_prob": 7.295993327716133e-06}, {"id": 545, "seek": 254412, "start": 2556.88, "end": 2562.44, "text": " The actual answer is what are the short and long term expected outcomes of the alley and", "tokens": [440, 3539, 1867, 307, 437, 366, 264, 2099, 293, 938, 1433, 5176, 10070, 295, 264, 26660, 293], "temperature": 0.0, "avg_logprob": -0.16732886497010577, "compression_ratio": 1.6919431279620853, "no_speech_prob": 7.295993327716133e-06}, {"id": 546, "seek": 254412, "start": 2562.44, "end": 2568.04, "text": " to what extent have they been achieved and we get what were the results, the comma, comma,", "tokens": [281, 437, 8396, 362, 436, 668, 11042, 293, 321, 483, 437, 645, 264, 3542, 11, 264, 22117, 11, 22117, 11], "temperature": 0.0, "avg_logprob": -0.16732886497010577, "compression_ratio": 1.6919431279620853, "no_speech_prob": 7.295993327716133e-06}, {"id": 547, "seek": 254412, "start": 2568.04, "end": 2572.08, "text": " comma, and, and, and, which is not a great answer.", "tokens": [22117, 11, 293, 11, 293, 11, 293, 11, 597, 307, 406, 257, 869, 1867, 13], "temperature": 0.0, "avg_logprob": -0.16732886497010577, "compression_ratio": 1.6919431279620853, "no_speech_prob": 7.295993327716133e-06}, {"id": 548, "seek": 257208, "start": 2572.08, "end": 2578.2799999999997, "text": " It started out okay with what were the results, but a lot is missing.", "tokens": [467, 1409, 484, 1392, 365, 437, 645, 264, 3542, 11, 457, 257, 688, 307, 5361, 13], "temperature": 0.0, "avg_logprob": -0.2290094354179468, "compression_ratio": 1.5933609958506223, "no_speech_prob": 2.026122274401132e-06}, {"id": 549, "seek": 257208, "start": 2578.2799999999997, "end": 2583.44, "text": " Another one we can look at which of the following additional information is necessary to estimate", "tokens": [3996, 472, 321, 393, 574, 412, 597, 295, 264, 3480, 4497, 1589, 307, 4818, 281, 12539], "temperature": 0.0, "avg_logprob": -0.2290094354179468, "compression_ratio": 1.5933609958506223, "no_speech_prob": 2.026122274401132e-06}, {"id": 550, "seek": 257208, "start": 2583.44, "end": 2587.04, "text": " the company's actual profit for the coming year.", "tokens": [264, 2237, 311, 3539, 7475, 337, 264, 1348, 1064, 13], "temperature": 0.0, "avg_logprob": -0.2290094354179468, "compression_ratio": 1.5933609958506223, "no_speech_prob": 2.026122274401132e-06}, {"id": 551, "seek": 257208, "start": 2587.04, "end": 2592.0, "text": " What is the V22VV parentheses, parentheses?", "tokens": [708, 307, 264, 691, 7490, 53, 53, 34153, 11, 34153, 30], "temperature": 0.0, "avg_logprob": -0.2290094354179468, "compression_ratio": 1.5933609958506223, "no_speech_prob": 2.026122274401132e-06}, {"id": 552, "seek": 257208, "start": 2592.0, "end": 2597.12, "text": " Another, sorry, I like this one too.", "tokens": [3996, 11, 2597, 11, 286, 411, 341, 472, 886, 13], "temperature": 0.0, "avg_logprob": -0.2290094354179468, "compression_ratio": 1.5933609958506223, "no_speech_prob": 2.026122274401132e-06}, {"id": 553, "seek": 257208, "start": 2597.12, "end": 2601.4, "text": " What experience and specific capacities do the implementing organizations bring to the", "tokens": [708, 1752, 293, 2685, 39396, 360, 264, 18114, 6150, 1565, 281, 264], "temperature": 0.0, "avg_logprob": -0.2290094354179468, "compression_ratio": 1.5933609958506223, "no_speech_prob": 2.026122274401132e-06}, {"id": 554, "seek": 260140, "start": 2601.4, "end": 2602.4, "text": " project?", "tokens": [1716, 30], "temperature": 0.0, "avg_logprob": -0.171632892680618, "compression_ratio": 1.5982905982905984, "no_speech_prob": 3.647659104899503e-05}, {"id": 555, "seek": 260140, "start": 2602.4, "end": 2606.96, "text": " What are the key and, and, and, and, of, of, of?", "tokens": [708, 366, 264, 2141, 293, 11, 293, 11, 293, 11, 293, 11, 295, 11, 295, 11, 295, 30], "temperature": 0.0, "avg_logprob": -0.171632892680618, "compression_ratio": 1.5982905982905984, "no_speech_prob": 3.647659104899503e-05}, {"id": 556, "seek": 260140, "start": 2606.96, "end": 2612.56, "text": " So this is, our answers are beginning pretty well, but then they're really falling apart", "tokens": [407, 341, 307, 11, 527, 6338, 366, 2863, 1238, 731, 11, 457, 550, 436, 434, 534, 7440, 4936], "temperature": 0.0, "avg_logprob": -0.171632892680618, "compression_ratio": 1.5982905982905984, "no_speech_prob": 3.647659104899503e-05}, {"id": 557, "seek": 260140, "start": 2612.56, "end": 2617.96, "text": " as we go on and typically ending up with kind of these repeated words.", "tokens": [382, 321, 352, 322, 293, 5850, 8121, 493, 365, 733, 295, 613, 10477, 2283, 13], "temperature": 0.0, "avg_logprob": -0.171632892680618, "compression_ratio": 1.5982905982905984, "no_speech_prob": 3.647659104899503e-05}, {"id": 558, "seek": 260140, "start": 2617.96, "end": 2621.36, "text": " And before I talk about how we address that, I'm going to ask if there are questions on", "tokens": [400, 949, 286, 751, 466, 577, 321, 2985, 300, 11, 286, 478, 516, 281, 1029, 498, 456, 366, 1651, 322], "temperature": 0.0, "avg_logprob": -0.171632892680618, "compression_ratio": 1.5982905982905984, "no_speech_prob": 3.647659104899503e-05}, {"id": 559, "seek": 260140, "start": 2621.36, "end": 2624.44, "text": " this part.", "tokens": [341, 644, 13], "temperature": 0.0, "avg_logprob": -0.171632892680618, "compression_ratio": 1.5982905982905984, "no_speech_prob": 3.647659104899503e-05}, {"id": 560, "seek": 260140, "start": 2624.44, "end": 2627.48, "text": " So what we've done so far is kind of just construct this.", "tokens": [407, 437, 321, 600, 1096, 370, 1400, 307, 733, 295, 445, 7690, 341, 13], "temperature": 0.0, "avg_logprob": -0.171632892680618, "compression_ratio": 1.5982905982905984, "no_speech_prob": 3.647659104899503e-05}, {"id": 561, "seek": 262748, "start": 2627.48, "end": 2633.44, "text": " I say basic, I mean, seek to seek is a harder task, but for a seek to seek model, it's on", "tokens": [286, 584, 3875, 11, 286, 914, 11, 8075, 281, 8075, 307, 257, 6081, 5633, 11, 457, 337, 257, 8075, 281, 8075, 2316, 11, 309, 311, 322], "temperature": 0.0, "avg_logprob": -0.25654664290578744, "compression_ratio": 1.4946808510638299, "no_speech_prob": 1.922239243867807e-05}, {"id": 562, "seek": 262748, "start": 2633.44, "end": 2641.04, "text": " the basic side in that it's, it's just using two, two GRUs, which are a type of RNN.", "tokens": [264, 3875, 1252, 294, 300, 309, 311, 11, 309, 311, 445, 1228, 732, 11, 732, 10903, 29211, 11, 597, 366, 257, 2010, 295, 45702, 45, 13], "temperature": 0.0, "avg_logprob": -0.25654664290578744, "compression_ratio": 1.4946808510638299, "no_speech_prob": 1.922239243867807e-05}, {"id": 563, "seek": 262748, "start": 2641.04, "end": 2646.44, "text": " It has, actually I can put the structure here again.", "tokens": [467, 575, 11, 767, 286, 393, 829, 264, 3877, 510, 797, 13], "temperature": 0.0, "avg_logprob": -0.25654664290578744, "compression_ratio": 1.4946808510638299, "no_speech_prob": 1.922239243867807e-05}, {"id": 564, "seek": 262748, "start": 2646.44, "end": 2650.2, "text": " Did we call it?", "tokens": [2589, 321, 818, 309, 30], "temperature": 0.0, "avg_logprob": -0.25654664290578744, "compression_ratio": 1.4946808510638299, "no_speech_prob": 1.922239243867807e-05}, {"id": 565, "seek": 262748, "start": 2650.2, "end": 2651.2, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.25654664290578744, "compression_ratio": 1.4946808510638299, "no_speech_prob": 1.922239243867807e-05}, {"id": 566, "seek": 262748, "start": 2651.2, "end": 2653.4, "text": " So here's our seek to seek RNN.", "tokens": [407, 510, 311, 527, 8075, 281, 8075, 45702, 45, 13], "temperature": 0.0, "avg_logprob": -0.25654664290578744, "compression_ratio": 1.4946808510638299, "no_speech_prob": 1.922239243867807e-05}, {"id": 567, "seek": 265340, "start": 2653.4, "end": 2659.44, "text": " So we can see, you know, it's embedding, dropout, GRU, linear layer, embedding, GRU, dropout,", "tokens": [407, 321, 393, 536, 11, 291, 458, 11, 309, 311, 12240, 3584, 11, 3270, 346, 11, 10903, 52, 11, 8213, 4583, 11, 12240, 3584, 11, 10903, 52, 11, 3270, 346, 11], "temperature": 0.0, "avg_logprob": -0.2045819734272204, "compression_ratio": 1.5472636815920398, "no_speech_prob": 1.38455525302561e-05}, {"id": 568, "seek": 265340, "start": 2659.44, "end": 2661.6, "text": " linear layer is what we're doing.", "tokens": [8213, 4583, 307, 437, 321, 434, 884, 13], "temperature": 0.0, "avg_logprob": -0.2045819734272204, "compression_ratio": 1.5472636815920398, "no_speech_prob": 1.38455525302561e-05}, {"id": 569, "seek": 265340, "start": 2661.6, "end": 2664.1600000000003, "text": " And so we will, we will improve upon this.", "tokens": [400, 370, 321, 486, 11, 321, 486, 3470, 3564, 341, 13], "temperature": 0.0, "avg_logprob": -0.2045819734272204, "compression_ratio": 1.5472636815920398, "no_speech_prob": 1.38455525302561e-05}, {"id": 570, "seek": 265340, "start": 2664.1600000000003, "end": 2667.52, "text": " But questions about what we've done so far?", "tokens": [583, 1651, 466, 437, 321, 600, 1096, 370, 1400, 30], "temperature": 0.0, "avg_logprob": -0.2045819734272204, "compression_ratio": 1.5472636815920398, "no_speech_prob": 1.38455525302561e-05}, {"id": 571, "seek": 265340, "start": 2667.52, "end": 2669.1600000000003, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.2045819734272204, "compression_ratio": 1.5472636815920398, "no_speech_prob": 1.38455525302561e-05}, {"id": 572, "seek": 265340, "start": 2669.1600000000003, "end": 2678.52, "text": " So, to kind of address this issue of starting well, but then ending up with repeated words", "tokens": [407, 11, 281, 733, 295, 2985, 341, 2734, 295, 2891, 731, 11, 457, 550, 8121, 493, 365, 10477, 2283], "temperature": 0.0, "avg_logprob": -0.2045819734272204, "compression_ratio": 1.5472636815920398, "no_speech_prob": 1.38455525302561e-05}, {"id": 573, "seek": 267852, "start": 2678.52, "end": 2684.6, "text": " that aren't very good, part of the issue is once the, kind of once the RNN goes astray,", "tokens": [300, 3212, 380, 588, 665, 11, 644, 295, 264, 2734, 307, 1564, 264, 11, 733, 295, 1564, 264, 45702, 45, 1709, 5357, 3458, 11], "temperature": 0.0, "avg_logprob": -0.12563965015841605, "compression_ratio": 1.7626459143968871, "no_speech_prob": 1.0129500878974795e-05}, {"id": 574, "seek": 267852, "start": 2684.6, "end": 2690.04, "text": " it doesn't really have a chance to get back on track because the input is the previous", "tokens": [309, 1177, 380, 534, 362, 257, 2931, 281, 483, 646, 322, 2837, 570, 264, 4846, 307, 264, 3894], "temperature": 0.0, "avg_logprob": -0.12563965015841605, "compression_ratio": 1.7626459143968871, "no_speech_prob": 1.0129500878974795e-05}, {"id": 575, "seek": 267852, "start": 2690.04, "end": 2691.04, "text": " words.", "tokens": [2283, 13], "temperature": 0.0, "avg_logprob": -0.12563965015841605, "compression_ratio": 1.7626459143968871, "no_speech_prob": 1.0129500878974795e-05}, {"id": 576, "seek": 267852, "start": 2691.04, "end": 2695.64, "text": " And so if we give it, you know, what are the key and, and, and, and it's, you know, it's", "tokens": [400, 370, 498, 321, 976, 309, 11, 291, 458, 11, 437, 366, 264, 2141, 293, 11, 293, 11, 293, 11, 293, 309, 311, 11, 291, 458, 11, 309, 311], "temperature": 0.0, "avg_logprob": -0.12563965015841605, "compression_ratio": 1.7626459143968871, "no_speech_prob": 1.0129500878974795e-05}, {"id": 577, "seek": 267852, "start": 2695.64, "end": 2699.08, "text": " looking at those previous words, like, how is it going to get back on track?", "tokens": [1237, 412, 729, 3894, 2283, 11, 411, 11, 577, 307, 309, 516, 281, 483, 646, 322, 2837, 30], "temperature": 0.0, "avg_logprob": -0.12563965015841605, "compression_ratio": 1.7626459143968871, "no_speech_prob": 1.0129500878974795e-05}, {"id": 578, "seek": 267852, "start": 2699.08, "end": 2700.08, "text": " Right?", "tokens": [1779, 30], "temperature": 0.0, "avg_logprob": -0.12563965015841605, "compression_ratio": 1.7626459143968871, "no_speech_prob": 1.0129500878974795e-05}, {"id": 579, "seek": 267852, "start": 2700.08, "end": 2706.12, "text": " It's got its own, its own incorrect output becomes input for the next state, which is", "tokens": [467, 311, 658, 1080, 1065, 11, 1080, 1065, 18424, 5598, 3643, 4846, 337, 264, 958, 1785, 11, 597, 307], "temperature": 0.0, "avg_logprob": -0.12563965015841605, "compression_ratio": 1.7626459143968871, "no_speech_prob": 1.0129500878974795e-05}, {"id": 580, "seek": 267852, "start": 2706.12, "end": 2707.84, "text": " the problem.", "tokens": [264, 1154, 13], "temperature": 0.0, "avg_logprob": -0.12563965015841605, "compression_ratio": 1.7626459143968871, "no_speech_prob": 1.0129500878974795e-05}, {"id": 581, "seek": 270784, "start": 2707.84, "end": 2712.7200000000003, "text": " And so the technique to deal with this is called teacher forcing.", "tokens": [400, 370, 264, 6532, 281, 2028, 365, 341, 307, 1219, 5027, 19030, 13], "temperature": 0.0, "avg_logprob": -0.09268886702401298, "compression_ratio": 1.8093385214007782, "no_speech_prob": 1.6796936961327447e-06}, {"id": 582, "seek": 270784, "start": 2712.7200000000003, "end": 2718.76, "text": " And so this is, we're going to help out the decoder and instead of giving it its own predictions,", "tokens": [400, 370, 341, 307, 11, 321, 434, 516, 281, 854, 484, 264, 979, 19866, 293, 2602, 295, 2902, 309, 1080, 1065, 21264, 11], "temperature": 0.0, "avg_logprob": -0.09268886702401298, "compression_ratio": 1.8093385214007782, "no_speech_prob": 1.6796936961327447e-06}, {"id": 583, "seek": 270784, "start": 2718.76, "end": 2723.44, "text": " which may not be very good, particularly when you first start training the model, instead", "tokens": [597, 815, 406, 312, 588, 665, 11, 4098, 562, 291, 700, 722, 3097, 264, 2316, 11, 2602], "temperature": 0.0, "avg_logprob": -0.09268886702401298, "compression_ratio": 1.8093385214007782, "no_speech_prob": 1.6796936961327447e-06}, {"id": 584, "seek": 270784, "start": 2723.44, "end": 2726.56, "text": " we'll give it the correct answers.", "tokens": [321, 603, 976, 309, 264, 3006, 6338, 13], "temperature": 0.0, "avg_logprob": -0.09268886702401298, "compression_ratio": 1.8093385214007782, "no_speech_prob": 1.6796936961327447e-06}, {"id": 585, "seek": 270784, "start": 2726.56, "end": 2731.44, "text": " And so if it makes a wrong prediction, we'll still give it the correct answer for its next", "tokens": [400, 370, 498, 309, 1669, 257, 2085, 17630, 11, 321, 603, 920, 976, 309, 264, 3006, 1867, 337, 1080, 958], "temperature": 0.0, "avg_logprob": -0.09268886702401298, "compression_ratio": 1.8093385214007782, "no_speech_prob": 1.6796936961327447e-06}, {"id": 586, "seek": 270784, "start": 2731.44, "end": 2736.36, "text": " prediction because we basically don't want, you know, a few wrong predictions then to", "tokens": [17630, 570, 321, 1936, 500, 380, 528, 11, 291, 458, 11, 257, 1326, 2085, 21264, 550, 281], "temperature": 0.0, "avg_logprob": -0.09268886702401298, "compression_ratio": 1.8093385214007782, "no_speech_prob": 1.6796936961327447e-06}, {"id": 587, "seek": 273636, "start": 2736.36, "end": 2739.6, "text": " just like totally screw it up.", "tokens": [445, 411, 3879, 5630, 309, 493, 13], "temperature": 0.0, "avg_logprob": -0.11462879871976549, "compression_ratio": 1.7946127946127945, "no_speech_prob": 1.6963735106401145e-05}, {"id": 588, "seek": 273636, "start": 2739.6, "end": 2741.48, "text": " We'll do this all the time at the beginning.", "tokens": [492, 603, 360, 341, 439, 264, 565, 412, 264, 2863, 13], "temperature": 0.0, "avg_logprob": -0.11462879871976549, "compression_ratio": 1.7946127946127945, "no_speech_prob": 1.6963735106401145e-05}, {"id": 589, "seek": 273636, "start": 2741.48, "end": 2744.56, "text": " However, as we go on, we want to do it less and less.", "tokens": [2908, 11, 382, 321, 352, 322, 11, 321, 528, 281, 360, 309, 1570, 293, 1570, 13], "temperature": 0.0, "avg_logprob": -0.11462879871976549, "compression_ratio": 1.7946127946127945, "no_speech_prob": 1.6963735106401145e-05}, {"id": 590, "seek": 273636, "start": 2744.56, "end": 2749.96, "text": " The one thing about teacher forcing is it, it slightly incentivizes the model to take", "tokens": [440, 472, 551, 466, 5027, 19030, 307, 309, 11, 309, 4748, 35328, 5660, 264, 2316, 281, 747], "temperature": 0.0, "avg_logprob": -0.11462879871976549, "compression_ratio": 1.7946127946127945, "no_speech_prob": 1.6963735106401145e-05}, {"id": 591, "seek": 273636, "start": 2749.96, "end": 2754.32, "text": " more risk because if it gets something wrong, you know, it'll still be fine the next time", "tokens": [544, 3148, 570, 498, 309, 2170, 746, 2085, 11, 291, 458, 11, 309, 603, 920, 312, 2489, 264, 958, 565], "temperature": 0.0, "avg_logprob": -0.11462879871976549, "compression_ratio": 1.7946127946127945, "no_speech_prob": 1.6963735106401145e-05}, {"id": 592, "seek": 273636, "start": 2754.32, "end": 2757.88, "text": " since you're going to show it the answer before its next prediction.", "tokens": [1670, 291, 434, 516, 281, 855, 309, 264, 1867, 949, 1080, 958, 17630, 13], "temperature": 0.0, "avg_logprob": -0.11462879871976549, "compression_ratio": 1.7946127946127945, "no_speech_prob": 1.6963735106401145e-05}, {"id": 593, "seek": 273636, "start": 2757.88, "end": 2762.04, "text": " So basically we'll gradually reduce the amount of teacher forcing, but at the beginning we", "tokens": [407, 1936, 321, 603, 13145, 5407, 264, 2372, 295, 5027, 19030, 11, 457, 412, 264, 2863, 321], "temperature": 0.0, "avg_logprob": -0.11462879871976549, "compression_ratio": 1.7946127946127945, "no_speech_prob": 1.6963735106401145e-05}, {"id": 594, "seek": 273636, "start": 2762.04, "end": 2765.04, "text": " want to do a lot because the model, it's not going to be very good.", "tokens": [528, 281, 360, 257, 688, 570, 264, 2316, 11, 309, 311, 406, 516, 281, 312, 588, 665, 13], "temperature": 0.0, "avg_logprob": -0.11462879871976549, "compression_ratio": 1.7946127946127945, "no_speech_prob": 1.6963735106401145e-05}, {"id": 595, "seek": 276504, "start": 2765.04, "end": 2767.72, "text": " Like we're just starting to train it.", "tokens": [1743, 321, 434, 445, 2891, 281, 3847, 309, 13], "temperature": 0.0, "avg_logprob": -0.11478874033147639, "compression_ratio": 1.725, "no_speech_prob": 3.024035322596319e-05}, {"id": 596, "seek": 276504, "start": 2767.72, "end": 2771.64, "text": " Of course it's going to get stuff wrong and we don't want that to just screw it up so", "tokens": [2720, 1164, 309, 311, 516, 281, 483, 1507, 2085, 293, 321, 500, 380, 528, 300, 281, 445, 5630, 309, 493, 370], "temperature": 0.0, "avg_logprob": -0.11478874033147639, "compression_ratio": 1.725, "no_speech_prob": 3.024035322596319e-05}, {"id": 597, "seek": 276504, "start": 2771.64, "end": 2773.7599999999998, "text": " that it can't learn.", "tokens": [300, 309, 393, 380, 1466, 13], "temperature": 0.0, "avg_logprob": -0.11478874033147639, "compression_ratio": 1.725, "no_speech_prob": 3.024035322596319e-05}, {"id": 598, "seek": 276504, "start": 2773.7599999999998, "end": 2781.16, "text": " And so what this does here is we'll have a variable called the probability of forcing", "tokens": [400, 370, 437, 341, 775, 510, 307, 321, 603, 362, 257, 7006, 1219, 264, 8482, 295, 19030], "temperature": 0.0, "avg_logprob": -0.11478874033147639, "compression_ratio": 1.725, "no_speech_prob": 3.024035322596319e-05}, {"id": 599, "seek": 276504, "start": 2781.16, "end": 2786.7599999999998, "text": " and that's going to start high, it'll start at one and then gradually decrease until kind", "tokens": [293, 300, 311, 516, 281, 722, 1090, 11, 309, 603, 722, 412, 472, 293, 550, 13145, 11514, 1826, 733], "temperature": 0.0, "avg_logprob": -0.11478874033147639, "compression_ratio": 1.725, "no_speech_prob": 3.024035322596319e-05}, {"id": 600, "seek": 276504, "start": 2786.7599999999998, "end": 2792.7599999999998, "text": " of by the end of the epoch, we want the probability of forcing to be zero since, you know, in", "tokens": [295, 538, 264, 917, 295, 264, 30992, 339, 11, 321, 528, 264, 8482, 295, 19030, 281, 312, 4018, 1670, 11, 291, 458, 11, 294], "temperature": 0.0, "avg_logprob": -0.11478874033147639, "compression_ratio": 1.725, "no_speech_prob": 3.024035322596319e-05}, {"id": 601, "seek": 279276, "start": 2792.76, "end": 2795.32, "text": " the real world we're not going to have the answer every time.", "tokens": [264, 957, 1002, 321, 434, 406, 516, 281, 362, 264, 1867, 633, 565, 13], "temperature": 0.0, "avg_logprob": -0.0965805997942934, "compression_ratio": 1.632034632034632, "no_speech_prob": 1.0783046491269488e-05}, {"id": 602, "seek": 279276, "start": 2795.32, "end": 2799.5200000000004, "text": " This is just as we're training it, a way to help it, but we gradually reduce the help", "tokens": [639, 307, 445, 382, 321, 434, 3097, 309, 11, 257, 636, 281, 854, 309, 11, 457, 321, 13145, 5407, 264, 854], "temperature": 0.0, "avg_logprob": -0.0965805997942934, "compression_ratio": 1.632034632034632, "no_speech_prob": 1.0783046491269488e-05}, {"id": 603, "seek": 279276, "start": 2799.5200000000004, "end": 2804.42, "text": " as it trains.", "tokens": [382, 309, 16329, 13], "temperature": 0.0, "avg_logprob": -0.0965805997942934, "compression_ratio": 1.632034632034632, "no_speech_prob": 1.0783046491269488e-05}, {"id": 604, "seek": 279276, "start": 2804.42, "end": 2810.88, "text": " So we're going to add this code to our forward method and basically we're going to draw a", "tokens": [407, 321, 434, 516, 281, 909, 341, 3089, 281, 527, 2128, 3170, 293, 1936, 321, 434, 516, 281, 2642, 257], "temperature": 0.0, "avg_logprob": -0.0965805997942934, "compression_ratio": 1.632034632034632, "no_speech_prob": 1.0783046491269488e-05}, {"id": 605, "seek": 279276, "start": 2810.88, "end": 2813.84, "text": " random number between zero and one.", "tokens": [4974, 1230, 1296, 4018, 293, 472, 13], "temperature": 0.0, "avg_logprob": -0.0965805997942934, "compression_ratio": 1.632034632034632, "no_speech_prob": 1.0783046491269488e-05}, {"id": 606, "seek": 279276, "start": 2813.84, "end": 2821.1200000000003, "text": " If that's less than the probability of forcing, then we'll set, we also need to make sure", "tokens": [759, 300, 311, 1570, 813, 264, 8482, 295, 19030, 11, 550, 321, 603, 992, 11, 321, 611, 643, 281, 652, 988], "temperature": 0.0, "avg_logprob": -0.0965805997942934, "compression_ratio": 1.632034632034632, "no_speech_prob": 1.0783046491269488e-05}, {"id": 607, "seek": 282112, "start": 2821.12, "end": 2826.56, "text": " we're not done, like if we're at the end of our word we don't need to worry about this.", "tokens": [321, 434, 406, 1096, 11, 411, 498, 321, 434, 412, 264, 917, 295, 527, 1349, 321, 500, 380, 643, 281, 3292, 466, 341, 13], "temperature": 0.0, "avg_logprob": -0.1050652782473944, "compression_ratio": 1.6939655172413792, "no_speech_prob": 2.668767592695076e-05}, {"id": 608, "seek": 282112, "start": 2826.56, "end": 2829.88, "text": " We'll set the decoder input to be the target word.", "tokens": [492, 603, 992, 264, 979, 19866, 4846, 281, 312, 264, 3779, 1349, 13], "temperature": 0.0, "avg_logprob": -0.1050652782473944, "compression_ratio": 1.6939655172413792, "no_speech_prob": 2.668767592695076e-05}, {"id": 609, "seek": 282112, "start": 2829.88, "end": 2835.0, "text": " So our input for the next time, we've set it to be the correct answer.", "tokens": [407, 527, 4846, 337, 264, 958, 565, 11, 321, 600, 992, 309, 281, 312, 264, 3006, 1867, 13], "temperature": 0.0, "avg_logprob": -0.1050652782473944, "compression_ratio": 1.6939655172413792, "no_speech_prob": 2.668767592695076e-05}, {"id": 610, "seek": 282112, "start": 2835.0, "end": 2841.3199999999997, "text": " And so this initially, if PR force is one, it's kind of always going to enter this loop.", "tokens": [400, 370, 341, 9105, 11, 498, 11568, 3464, 307, 472, 11, 309, 311, 733, 295, 1009, 516, 281, 3242, 341, 6367, 13], "temperature": 0.0, "avg_logprob": -0.1050652782473944, "compression_ratio": 1.6939655172413792, "no_speech_prob": 2.668767592695076e-05}, {"id": 611, "seek": 282112, "start": 2841.3199999999997, "end": 2845.6, "text": " As it decreases though and by the end PR force will be zero and it'll never go into this", "tokens": [1018, 309, 24108, 1673, 293, 538, 264, 917, 11568, 3464, 486, 312, 4018, 293, 309, 603, 1128, 352, 666, 341], "temperature": 0.0, "avg_logprob": -0.1050652782473944, "compression_ratio": 1.6939655172413792, "no_speech_prob": 2.668767592695076e-05}, {"id": 612, "seek": 282112, "start": 2845.6, "end": 2846.6, "text": " loop.", "tokens": [6367, 13], "temperature": 0.0, "avg_logprob": -0.1050652782473944, "compression_ratio": 1.6939655172413792, "no_speech_prob": 2.668767592695076e-05}, {"id": 613, "seek": 284660, "start": 2846.6, "end": 2851.72, "text": " And so this is the only change we've made from before.", "tokens": [400, 370, 341, 307, 264, 787, 1319, 321, 600, 1027, 490, 949, 13], "temperature": 0.0, "avg_logprob": -0.18236047675810665, "compression_ratio": 1.7142857142857142, "no_speech_prob": 2.4299313736264594e-05}, {"id": 614, "seek": 284660, "start": 2851.72, "end": 2857.7999999999997, "text": " I call this class seek to seek RNN underscore TF for teacher forcing just to let you know", "tokens": [286, 818, 341, 1508, 8075, 281, 8075, 45702, 45, 37556, 40964, 337, 5027, 19030, 445, 281, 718, 291, 458], "temperature": 0.0, "avg_logprob": -0.18236047675810665, "compression_ratio": 1.7142857142857142, "no_speech_prob": 2.4299313736264594e-05}, {"id": 615, "seek": 284660, "start": 2857.7999999999997, "end": 2861.3199999999997, "text": " that's the one change we've made.", "tokens": [300, 311, 264, 472, 1319, 321, 600, 1027, 13], "temperature": 0.0, "avg_logprob": -0.18236047675810665, "compression_ratio": 1.7142857142857142, "no_speech_prob": 2.4299313736264594e-05}, {"id": 616, "seek": 284660, "start": 2861.3199999999997, "end": 2867.72, "text": " And so encoder is the same as before, decoder is the same as before, but you see down here", "tokens": [400, 370, 2058, 19866, 307, 264, 912, 382, 949, 11, 979, 19866, 307, 264, 912, 382, 949, 11, 457, 291, 536, 760, 510], "temperature": 0.0, "avg_logprob": -0.18236047675810665, "compression_ratio": 1.7142857142857142, "no_speech_prob": 2.4299313736264594e-05}, {"id": 617, "seek": 284660, "start": 2867.72, "end": 2874.72, "text": " we've added this if statement.", "tokens": [321, 600, 3869, 341, 498, 5629, 13], "temperature": 0.0, "avg_logprob": -0.18236047675810665, "compression_ratio": 1.7142857142857142, "no_speech_prob": 2.4299313736264594e-05}, {"id": 618, "seek": 287472, "start": 2874.72, "end": 2880.04, "text": " Then we can run this.", "tokens": [1396, 321, 393, 1190, 341, 13], "temperature": 0.0, "avg_logprob": -0.221130277596268, "compression_ratio": 1.2734375, "no_speech_prob": 9.223322194884531e-06}, {"id": 619, "seek": 287472, "start": 2880.04, "end": 2883.2, "text": " And now our accuracy is getting up to 40%.", "tokens": [400, 586, 527, 14170, 307, 1242, 493, 281, 3356, 6856], "temperature": 0.0, "avg_logprob": -0.221130277596268, "compression_ratio": 1.2734375, "no_speech_prob": 9.223322194884531e-06}, {"id": 620, "seek": 287472, "start": 2883.2, "end": 2893.8399999999997, "text": " And just to compare, let me go back up and see what it was before.", "tokens": [400, 445, 281, 6794, 11, 718, 385, 352, 646, 493, 293, 536, 437, 309, 390, 949, 13], "temperature": 0.0, "avg_logprob": -0.221130277596268, "compression_ratio": 1.2734375, "no_speech_prob": 9.223322194884531e-06}, {"id": 621, "seek": 287472, "start": 2893.8399999999997, "end": 2898.2799999999997, "text": " So up here we were getting 37%.", "tokens": [407, 493, 510, 321, 645, 1242, 13435, 6856], "temperature": 0.0, "avg_logprob": -0.221130277596268, "compression_ratio": 1.2734375, "no_speech_prob": 9.223322194884531e-06}, {"id": 622, "seek": 289828, "start": 2898.28, "end": 2906.2400000000002, "text": " Blue score of 0.28.", "tokens": [8510, 6175, 295, 1958, 13, 11205, 13], "temperature": 0.0, "avg_logprob": -0.15034363004896376, "compression_ratio": 1.5027624309392265, "no_speech_prob": 6.438922810048098e-06}, {"id": 623, "seek": 289828, "start": 2906.2400000000002, "end": 2909.6400000000003, "text": " So here accuracy 40% and blue score of 0.31.", "tokens": [407, 510, 14170, 3356, 4, 293, 3344, 6175, 295, 1958, 13, 12967, 13], "temperature": 0.0, "avg_logprob": -0.15034363004896376, "compression_ratio": 1.5027624309392265, "no_speech_prob": 6.438922810048098e-06}, {"id": 624, "seek": 289828, "start": 2909.6400000000003, "end": 2911.92, "text": " So it's improved some.", "tokens": [407, 309, 311, 9689, 512, 13], "temperature": 0.0, "avg_logprob": -0.15034363004896376, "compression_ratio": 1.5027624309392265, "no_speech_prob": 6.438922810048098e-06}, {"id": 625, "seek": 289828, "start": 2911.92, "end": 2918.0, "text": " We can also look at the results, look at a few examples and see who has the authority", "tokens": [492, 393, 611, 574, 412, 264, 3542, 11, 574, 412, 257, 1326, 5110, 293, 536, 567, 575, 264, 8281], "temperature": 0.0, "avg_logprob": -0.15034363004896376, "compression_ratio": 1.5027624309392265, "no_speech_prob": 6.438922810048098e-06}, {"id": 626, "seek": 289828, "start": 2918.0, "end": 2925.5600000000004, "text": " to change the electricity and gas inspection regulations and the weights and measures regulations.", "tokens": [281, 1319, 264, 10356, 293, 4211, 22085, 12563, 293, 264, 17443, 293, 8000, 12563, 13], "temperature": 0.0, "avg_logprob": -0.15034363004896376, "compression_ratio": 1.5027624309392265, "no_speech_prob": 6.438922810048098e-06}, {"id": 627, "seek": 292556, "start": 2925.56, "end": 2931.44, "text": " So this is still doing some of that behavior from before.", "tokens": [407, 341, 307, 920, 884, 512, 295, 300, 5223, 490, 949, 13], "temperature": 0.0, "avg_logprob": -0.21882620418772977, "compression_ratio": 1.6411483253588517, "no_speech_prob": 1.496998083894141e-05}, {"id": 628, "seek": 292556, "start": 2931.44, "end": 2935.2, "text": " We see less repeated words happening.", "tokens": [492, 536, 1570, 10477, 2283, 2737, 13], "temperature": 0.0, "avg_logprob": -0.21882620418772977, "compression_ratio": 1.6411483253588517, "no_speech_prob": 1.496998083894141e-05}, {"id": 629, "seek": 292556, "start": 2935.2, "end": 2940.84, "text": " For instance, this one, where will national marine conservation areas of Canada be located?", "tokens": [1171, 5197, 11, 341, 472, 11, 689, 486, 4048, 20246, 16185, 3179, 295, 6309, 312, 6870, 30], "temperature": 0.0, "avg_logprob": -0.21882620418772977, "compression_ratio": 1.6411483253588517, "no_speech_prob": 1.496998083894141e-05}, {"id": 630, "seek": 292556, "start": 2940.84, "end": 2943.7999999999997, "text": " Where are the Canadian regulations located in the Canadian?", "tokens": [2305, 366, 264, 12641, 12563, 6870, 294, 264, 12641, 30], "temperature": 0.0, "avg_logprob": -0.21882620418772977, "compression_ratio": 1.6411483253588517, "no_speech_prob": 1.496998083894141e-05}, {"id": 631, "seek": 292556, "start": 2943.7999999999997, "end": 2950.12, "text": " It's not a great translation, but it is an improvement over and, and, and, comma, comma,", "tokens": [467, 311, 406, 257, 869, 12853, 11, 457, 309, 307, 364, 10444, 670, 293, 11, 293, 11, 293, 11, 22117, 11, 22117, 11], "temperature": 0.0, "avg_logprob": -0.21882620418772977, "compression_ratio": 1.6411483253588517, "no_speech_prob": 1.496998083894141e-05}, {"id": 632, "seek": 292556, "start": 2950.12, "end": 2951.12, "text": " comma.", "tokens": [22117, 13], "temperature": 0.0, "avg_logprob": -0.21882620418772977, "compression_ratio": 1.6411483253588517, "no_speech_prob": 1.496998083894141e-05}, {"id": 633, "seek": 295112, "start": 2951.12, "end": 2955.7999999999997, "text": " Any other questions about teacher forcing?", "tokens": [2639, 661, 1651, 466, 5027, 19030, 30], "temperature": 0.0, "avg_logprob": -0.22877103854448366, "compression_ratio": 1.528497409326425, "no_speech_prob": 7.030332926660776e-05}, {"id": 634, "seek": 295112, "start": 2955.7999999999997, "end": 2961.68, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.22877103854448366, "compression_ratio": 1.528497409326425, "no_speech_prob": 7.030332926660776e-05}, {"id": 635, "seek": 295112, "start": 2961.68, "end": 2967.6, "text": " So this is, we're going to continue to, to build on this model and continue improving", "tokens": [407, 341, 307, 11, 321, 434, 516, 281, 2354, 281, 11, 281, 1322, 322, 341, 2316, 293, 2354, 11470], "temperature": 0.0, "avg_logprob": -0.22877103854448366, "compression_ratio": 1.528497409326425, "no_speech_prob": 7.030332926660776e-05}, {"id": 636, "seek": 295112, "start": 2967.6, "end": 2968.6, "text": " it.", "tokens": [309, 13], "temperature": 0.0, "avg_logprob": -0.22877103854448366, "compression_ratio": 1.528497409326425, "no_speech_prob": 7.030332926660776e-05}, {"id": 637, "seek": 295112, "start": 2968.6, "end": 2971.04, "text": " So we've seen kind of starting with our basic seek to seek.", "tokens": [407, 321, 600, 1612, 733, 295, 2891, 365, 527, 3875, 8075, 281, 8075, 13], "temperature": 0.0, "avg_logprob": -0.22877103854448366, "compression_ratio": 1.528497409326425, "no_speech_prob": 7.030332926660776e-05}, {"id": 638, "seek": 295112, "start": 2971.04, "end": 2978.96, "text": " We added teacher forcing and then I broke a, broke the next example into a second notebook.", "tokens": [492, 3869, 5027, 19030, 293, 550, 286, 6902, 257, 11, 6902, 264, 958, 1365, 666, 257, 1150, 21060, 13], "temperature": 0.0, "avg_logprob": -0.22877103854448366, "compression_ratio": 1.528497409326425, "no_speech_prob": 7.030332926660776e-05}, {"id": 639, "seek": 297896, "start": 2978.96, "end": 2985.28, "text": " And this was mostly, mostly just to make it clearer which lines you need to rerun if you're", "tokens": [400, 341, 390, 5240, 11, 5240, 445, 281, 652, 309, 26131, 597, 3876, 291, 643, 281, 43819, 409, 498, 291, 434], "temperature": 0.0, "avg_logprob": -0.1131818936421321, "compression_ratio": 1.7336244541484715, "no_speech_prob": 2.6685109332902357e-05}, {"id": 640, "seek": 297896, "start": 2985.28, "end": 2986.6, "text": " starting from the beginning.", "tokens": [2891, 490, 264, 2863, 13], "temperature": 0.0, "avg_logprob": -0.1131818936421321, "compression_ratio": 1.7336244541484715, "no_speech_prob": 2.6685109332902357e-05}, {"id": 641, "seek": 297896, "start": 2986.6, "end": 2991.16, "text": " So in this notebook, we're not going to go through all the pre-processing, but are just", "tokens": [407, 294, 341, 21060, 11, 321, 434, 406, 516, 281, 352, 807, 439, 264, 659, 12, 41075, 278, 11, 457, 366, 445], "temperature": 0.0, "avg_logprob": -0.1131818936421321, "compression_ratio": 1.7336244541484715, "no_speech_prob": 2.6685109332902357e-05}, {"id": 642, "seek": 297896, "start": 2991.16, "end": 2993.44, "text": " going to load, load everything in.", "tokens": [516, 281, 3677, 11, 3677, 1203, 294, 13], "temperature": 0.0, "avg_logprob": -0.1131818936421321, "compression_ratio": 1.7336244541484715, "no_speech_prob": 2.6685109332902357e-05}, {"id": 643, "seek": 297896, "start": 2993.44, "end": 2998.16, "text": " So this, this whole section, the code to rerun from start, that's all duplicate from the", "tokens": [407, 341, 11, 341, 1379, 3541, 11, 264, 3089, 281, 43819, 409, 490, 722, 11, 300, 311, 439, 23976, 490, 264], "temperature": 0.0, "avg_logprob": -0.1131818936421321, "compression_ratio": 1.7336244541484715, "no_speech_prob": 2.6685109332902357e-05}, {"id": 644, "seek": 297896, "start": 2998.16, "end": 3002.48, "text": " previous notebook, but it loads everything in that you've saved.", "tokens": [3894, 21060, 11, 457, 309, 12668, 1203, 294, 300, 291, 600, 6624, 13], "temperature": 0.0, "avg_logprob": -0.1131818936421321, "compression_ratio": 1.7336244541484715, "no_speech_prob": 2.6685109332902357e-05}, {"id": 645, "seek": 300248, "start": 3002.48, "end": 3013.04, "text": " So run notebook seven before, before you do seven B. So to improve seek to seek translation,", "tokens": [407, 1190, 21060, 3407, 949, 11, 949, 291, 360, 3407, 363, 13, 407, 281, 3470, 8075, 281, 8075, 12853, 11], "temperature": 0.0, "avg_logprob": -0.15669542626489566, "compression_ratio": 1.5989304812834224, "no_speech_prob": 3.2694453693693504e-05}, {"id": 646, "seek": 300248, "start": 3013.04, "end": 3016.12, "text": " we're going to add attention and intention.", "tokens": [321, 434, 516, 281, 909, 3202, 293, 7789, 13], "temperature": 0.0, "avg_logprob": -0.15669542626489566, "compression_ratio": 1.5989304812834224, "no_speech_prob": 3.2694453693693504e-05}, {"id": 647, "seek": 300248, "start": 3016.12, "end": 3017.48, "text": " Attention is a really important concept.", "tokens": [31858, 307, 257, 534, 1021, 3410, 13], "temperature": 0.0, "avg_logprob": -0.15669542626489566, "compression_ratio": 1.5989304812834224, "no_speech_prob": 3.2694453693693504e-05}, {"id": 648, "seek": 300248, "start": 3017.48, "end": 3021.04, "text": " It's also kind of a key part of transformers.", "tokens": [467, 311, 611, 733, 295, 257, 2141, 644, 295, 4088, 433, 13], "temperature": 0.0, "avg_logprob": -0.15669542626489566, "compression_ratio": 1.5989304812834224, "no_speech_prob": 3.2694453693693504e-05}, {"id": 649, "seek": 300248, "start": 3021.04, "end": 3028.08, "text": " So that'll be very relevant even as we go on to, to transformers next week.", "tokens": [407, 300, 603, 312, 588, 7340, 754, 382, 321, 352, 322, 281, 11, 281, 4088, 433, 958, 1243, 13], "temperature": 0.0, "avg_logprob": -0.15669542626489566, "compression_ratio": 1.5989304812834224, "no_speech_prob": 3.2694453693693504e-05}, {"id": 650, "seek": 302808, "start": 3028.08, "end": 3033.48, "text": " And basically, so we have two things we're going to do.", "tokens": [400, 1936, 11, 370, 321, 362, 732, 721, 321, 434, 516, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.14098203472974824, "compression_ratio": 1.5392670157068062, "no_speech_prob": 1.5205880117719062e-05}, {"id": 651, "seek": 302808, "start": 3033.48, "end": 3038.08, "text": " Before we were kind of only keeping the final hidden state from the encoder.", "tokens": [4546, 321, 645, 733, 295, 787, 5145, 264, 2572, 7633, 1785, 490, 264, 2058, 19866, 13], "temperature": 0.0, "avg_logprob": -0.14098203472974824, "compression_ratio": 1.5392670157068062, "no_speech_prob": 1.5205880117719062e-05}, {"id": 652, "seek": 302808, "start": 3038.08, "end": 3043.44, "text": " Now we want to keep the output at every state of the encoder.", "tokens": [823, 321, 528, 281, 1066, 264, 5598, 412, 633, 1785, 295, 264, 2058, 19866, 13], "temperature": 0.0, "avg_logprob": -0.14098203472974824, "compression_ratio": 1.5392670157068062, "no_speech_prob": 1.5205880117719062e-05}, {"id": 653, "seek": 302808, "start": 3043.44, "end": 3047.56, "text": " And we also want to know which to focus on.", "tokens": [400, 321, 611, 528, 281, 458, 597, 281, 1879, 322, 13], "temperature": 0.0, "avg_logprob": -0.14098203472974824, "compression_ratio": 1.5392670157068062, "no_speech_prob": 1.5205880117719062e-05}, {"id": 654, "seek": 302808, "start": 3047.56, "end": 3052.52, "text": " Jay Alomar has written a really nice blog post on this.", "tokens": [11146, 967, 298, 289, 575, 3720, 257, 534, 1481, 6968, 2183, 322, 341, 13], "temperature": 0.0, "avg_logprob": -0.14098203472974824, "compression_ratio": 1.5392670157068062, "no_speech_prob": 1.5205880117719062e-05}, {"id": 655, "seek": 305252, "start": 3052.52, "end": 3058.12, "text": " And this is on, on transformer, but he covers, covers attention in it.", "tokens": [400, 341, 307, 322, 11, 322, 31782, 11, 457, 415, 10538, 11, 10538, 3202, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.21138152369746455, "compression_ratio": 1.6103896103896105, "no_speech_prob": 1.4063532944419421e-05}, {"id": 656, "seek": 305252, "start": 3058.12, "end": 3061.12, "text": " Actually I should look at this post on attention.", "tokens": [5135, 286, 820, 574, 412, 341, 2183, 322, 3202, 13], "temperature": 0.0, "avg_logprob": -0.21138152369746455, "compression_ratio": 1.6103896103896105, "no_speech_prob": 1.4063532944419421e-05}, {"id": 657, "seek": 305252, "start": 3061.12, "end": 3062.64, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.21138152369746455, "compression_ratio": 1.6103896103896105, "no_speech_prob": 1.4063532944419421e-05}, {"id": 658, "seek": 305252, "start": 3062.64, "end": 3065.28, "text": " I'll, I'll come back to this.", "tokens": [286, 603, 11, 286, 603, 808, 646, 281, 341, 13], "temperature": 0.0, "avg_logprob": -0.21138152369746455, "compression_ratio": 1.6103896103896105, "no_speech_prob": 1.4063532944419421e-05}, {"id": 659, "seek": 305252, "start": 3065.28, "end": 3070.64, "text": " But so this, this picture is from, from his blog post and he's using a tensor to tensor", "tokens": [583, 370, 341, 11, 341, 3036, 307, 490, 11, 490, 702, 6968, 2183, 293, 415, 311, 1228, 257, 40863, 281, 40863], "temperature": 0.0, "avg_logprob": -0.21138152369746455, "compression_ratio": 1.6103896103896105, "no_speech_prob": 1.4063532944419421e-05}, {"id": 660, "seek": 305252, "start": 3070.64, "end": 3074.72, "text": " notebook, which you can check out on Google CoLab.", "tokens": [21060, 11, 597, 291, 393, 1520, 484, 322, 3329, 3066, 37880, 13], "temperature": 0.0, "avg_logprob": -0.21138152369746455, "compression_ratio": 1.6103896103896105, "no_speech_prob": 1.4063532944419421e-05}, {"id": 661, "seek": 305252, "start": 3074.72, "end": 3081.08, "text": " And here in this example, they're looking at a self encode, an auto encoder.", "tokens": [400, 510, 294, 341, 1365, 11, 436, 434, 1237, 412, 257, 2698, 2058, 1429, 11, 364, 8399, 2058, 19866, 13], "temperature": 0.0, "avg_logprob": -0.21138152369746455, "compression_ratio": 1.6103896103896105, "no_speech_prob": 1.4063532944419421e-05}, {"id": 662, "seek": 308108, "start": 3081.08, "end": 3083.72, "text": " And so it's encoding kind of this sentence.", "tokens": [400, 370, 309, 311, 43430, 733, 295, 341, 8174, 13], "temperature": 0.0, "avg_logprob": -0.15421359795184175, "compression_ratio": 1.9515418502202644, "no_speech_prob": 9.515737474430352e-06}, {"id": 663, "seek": 308108, "start": 3083.72, "end": 3088.44, "text": " And if you have the animal didn't cross the street because it was too tired.", "tokens": [400, 498, 291, 362, 264, 5496, 994, 380, 3278, 264, 4838, 570, 309, 390, 886, 5868, 13], "temperature": 0.0, "avg_logprob": -0.15421359795184175, "compression_ratio": 1.9515418502202644, "no_speech_prob": 9.515737474430352e-06}, {"id": 664, "seek": 308108, "start": 3088.44, "end": 3094.84, "text": " When you're working with it, it's really important that you know it is related to the animal,", "tokens": [1133, 291, 434, 1364, 365, 309, 11, 309, 311, 534, 1021, 300, 291, 458, 309, 307, 4077, 281, 264, 5496, 11], "temperature": 0.0, "avg_logprob": -0.15421359795184175, "compression_ratio": 1.9515418502202644, "no_speech_prob": 9.515737474430352e-06}, {"id": 665, "seek": 308108, "start": 3094.84, "end": 3095.84, "text": " right?", "tokens": [558, 30], "temperature": 0.0, "avg_logprob": -0.15421359795184175, "compression_ratio": 1.9515418502202644, "no_speech_prob": 9.515737474430352e-06}, {"id": 666, "seek": 308108, "start": 3095.84, "end": 3098.04, "text": " Kind of reading.", "tokens": [9242, 295, 3760, 13], "temperature": 0.0, "avg_logprob": -0.15421359795184175, "compression_ratio": 1.9515418502202644, "no_speech_prob": 9.515737474430352e-06}, {"id": 667, "seek": 308108, "start": 3098.04, "end": 3100.92, "text": " We wouldn't want to think it is the street, right?", "tokens": [492, 2759, 380, 528, 281, 519, 309, 307, 264, 4838, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.15421359795184175, "compression_ratio": 1.9515418502202644, "no_speech_prob": 9.515737474430352e-06}, {"id": 668, "seek": 308108, "start": 3100.92, "end": 3106.0, "text": " And this is something that, you know, we as humans recognize the animal didn't cross the", "tokens": [400, 341, 307, 746, 300, 11, 291, 458, 11, 321, 382, 6255, 5521, 264, 5496, 994, 380, 3278, 264], "temperature": 0.0, "avg_logprob": -0.15421359795184175, "compression_ratio": 1.9515418502202644, "no_speech_prob": 9.515737474430352e-06}, {"id": 669, "seek": 308108, "start": 3106.0, "end": 3108.3199999999997, "text": " street because it was too tired.", "tokens": [4838, 570, 309, 390, 886, 5868, 13], "temperature": 0.0, "avg_logprob": -0.15421359795184175, "compression_ratio": 1.9515418502202644, "no_speech_prob": 9.515737474430352e-06}, {"id": 670, "seek": 308108, "start": 3108.3199999999997, "end": 3110.7999999999997, "text": " We know what it's referring to.", "tokens": [492, 458, 437, 309, 311, 13761, 281, 13], "temperature": 0.0, "avg_logprob": -0.15421359795184175, "compression_ratio": 1.9515418502202644, "no_speech_prob": 9.515737474430352e-06}, {"id": 671, "seek": 311080, "start": 3110.8, "end": 3118.1200000000003, "text": " And so the idea is kind of in your output, you want to know what to focus on from the", "tokens": [400, 370, 264, 1558, 307, 733, 295, 294, 428, 5598, 11, 291, 528, 281, 458, 437, 281, 1879, 322, 490, 264], "temperature": 0.0, "avg_logprob": -0.11164437719138272, "compression_ratio": 1.8135593220338984, "no_speech_prob": 4.860324679611949e-06}, {"id": 672, "seek": 311080, "start": 3118.1200000000003, "end": 3119.5600000000004, "text": " input.", "tokens": [4846, 13], "temperature": 0.0, "avg_logprob": -0.11164437719138272, "compression_ratio": 1.8135593220338984, "no_speech_prob": 4.860324679611949e-06}, {"id": 673, "seek": 311080, "start": 3119.5600000000004, "end": 3125.2400000000002, "text": " And so we're going to use weights or rather we're going to learn weights to kind of learn", "tokens": [400, 370, 321, 434, 516, 281, 764, 17443, 420, 2831, 321, 434, 516, 281, 1466, 17443, 281, 733, 295, 1466], "temperature": 0.0, "avg_logprob": -0.11164437719138272, "compression_ratio": 1.8135593220338984, "no_speech_prob": 4.860324679611949e-06}, {"id": 674, "seek": 311080, "start": 3125.2400000000002, "end": 3131.2000000000003, "text": " that information of what we should be focusing on.", "tokens": [300, 1589, 295, 437, 321, 820, 312, 8416, 322, 13], "temperature": 0.0, "avg_logprob": -0.11164437719138272, "compression_ratio": 1.8135593220338984, "no_speech_prob": 4.860324679611949e-06}, {"id": 675, "seek": 311080, "start": 3131.2000000000003, "end": 3135.84, "text": " And so basically we're going to end up taking a weighted average of the output from the", "tokens": [400, 370, 1936, 321, 434, 516, 281, 917, 493, 1940, 257, 32807, 4274, 295, 264, 5598, 490, 264], "temperature": 0.0, "avg_logprob": -0.11164437719138272, "compression_ratio": 1.8135593220338984, "no_speech_prob": 4.860324679611949e-06}, {"id": 676, "seek": 313584, "start": 3135.84, "end": 3141.8, "text": " encoder at each step to kind of know what were the key steps of the encoder for the", "tokens": [2058, 19866, 412, 1184, 1823, 281, 733, 295, 458, 437, 645, 264, 2141, 4439, 295, 264, 2058, 19866, 337, 264], "temperature": 0.0, "avg_logprob": -0.16071502883712968, "compression_ratio": 1.5780346820809248, "no_speech_prob": 9.66578591032885e-06}, {"id": 677, "seek": 313584, "start": 3141.8, "end": 3144.2400000000002, "text": " particular word I'm on in the output.", "tokens": [1729, 1349, 286, 478, 322, 294, 264, 5598, 13], "temperature": 0.0, "avg_logprob": -0.16071502883712968, "compression_ratio": 1.5780346820809248, "no_speech_prob": 9.66578591032885e-06}, {"id": 678, "seek": 313584, "start": 3144.2400000000002, "end": 3147.76, "text": " You know, and this will be different for each word of the output, what you want to focus", "tokens": [509, 458, 11, 293, 341, 486, 312, 819, 337, 1184, 1349, 295, 264, 5598, 11, 437, 291, 528, 281, 1879], "temperature": 0.0, "avg_logprob": -0.16071502883712968, "compression_ratio": 1.5780346820809248, "no_speech_prob": 9.66578591032885e-06}, {"id": 679, "seek": 313584, "start": 3147.76, "end": 3155.48, "text": " on from the input.", "tokens": [322, 490, 264, 4846, 13], "temperature": 0.0, "avg_logprob": -0.16071502883712968, "compression_ratio": 1.5780346820809248, "no_speech_prob": 9.66578591032885e-06}, {"id": 680, "seek": 313584, "start": 3155.48, "end": 3158.44, "text": " So that's the key change we're making here.", "tokens": [407, 300, 311, 264, 2141, 1319, 321, 434, 1455, 510, 13], "temperature": 0.0, "avg_logprob": -0.16071502883712968, "compression_ratio": 1.5780346820809248, "no_speech_prob": 9.66578591032885e-06}, {"id": 681, "seek": 315844, "start": 3158.44, "end": 3167.6, "text": " Kind of a second change we'll make to improve our model is to make it bidirectional.", "tokens": [9242, 295, 257, 1150, 1319, 321, 603, 652, 281, 3470, 527, 2316, 307, 281, 652, 309, 12957, 621, 41048, 13], "temperature": 0.0, "avg_logprob": -0.12830694619711344, "compression_ratio": 1.675392670157068, "no_speech_prob": 5.1735914894379675e-06}, {"id": 682, "seek": 315844, "start": 3167.6, "end": 3174.2000000000003, "text": " And basically this just doubles the number of inputs to the output layer of the encoder.", "tokens": [400, 1936, 341, 445, 31634, 264, 1230, 295, 15743, 281, 264, 5598, 4583, 295, 264, 2058, 19866, 13], "temperature": 0.0, "avg_logprob": -0.12830694619711344, "compression_ratio": 1.675392670157068, "no_speech_prob": 5.1735914894379675e-06}, {"id": 683, "seek": 315844, "start": 3174.2000000000003, "end": 3178.6, "text": " Basically we're going, kind of doing one version that's going forwards, one version that's", "tokens": [8537, 321, 434, 516, 11, 733, 295, 884, 472, 3037, 300, 311, 516, 30126, 11, 472, 3037, 300, 311], "temperature": 0.0, "avg_logprob": -0.12830694619711344, "compression_ratio": 1.675392670157068, "no_speech_prob": 5.1735914894379675e-06}, {"id": 684, "seek": 315844, "start": 3178.6, "end": 3182.88, "text": " going backwards to get kind of double this information.", "tokens": [516, 12204, 281, 483, 733, 295, 3834, 341, 1589, 13], "temperature": 0.0, "avg_logprob": -0.12830694619711344, "compression_ratio": 1.675392670157068, "no_speech_prob": 5.1735914894379675e-06}, {"id": 685, "seek": 318288, "start": 3182.88, "end": 3189.7400000000002, "text": " And so that'll show up here.", "tokens": [400, 370, 300, 603, 855, 493, 510, 13], "temperature": 0.0, "avg_logprob": -0.15152126948038738, "compression_ratio": 1.0804597701149425, "no_speech_prob": 2.156787559215445e-06}, {"id": 686, "seek": 318288, "start": 3189.7400000000002, "end": 3208.1600000000003, "text": " So what this looks like in practice is, let me just get my drink.", "tokens": [407, 437, 341, 1542, 411, 294, 3124, 307, 11, 718, 385, 445, 483, 452, 2822, 13], "temperature": 0.0, "avg_logprob": -0.15152126948038738, "compression_ratio": 1.0804597701149425, "no_speech_prob": 2.156787559215445e-06}, {"id": 687, "seek": 320816, "start": 3208.16, "end": 3213.56, "text": " Both our encoder and our decoder are going to have to change some.", "tokens": [6767, 527, 2058, 19866, 293, 527, 979, 19866, 366, 516, 281, 362, 281, 1319, 512, 13], "temperature": 0.0, "avg_logprob": -0.11453983451746687, "compression_ratio": 1.596774193548387, "no_speech_prob": 4.565880317386473e-06}, {"id": 688, "seek": 320816, "start": 3213.56, "end": 3219.3199999999997, "text": " So encoder starts off kind of the way it was before.", "tokens": [407, 2058, 19866, 3719, 766, 733, 295, 264, 636, 309, 390, 949, 13], "temperature": 0.0, "avg_logprob": -0.11453983451746687, "compression_ratio": 1.596774193548387, "no_speech_prob": 4.565880317386473e-06}, {"id": 689, "seek": 320816, "start": 3219.3199999999997, "end": 3223.2, "text": " Here the only difference is we have two times the number of hidden states since we've added", "tokens": [1692, 264, 787, 2649, 307, 321, 362, 732, 1413, 264, 1230, 295, 7633, 4368, 1670, 321, 600, 3869], "temperature": 0.0, "avg_logprob": -0.11453983451746687, "compression_ratio": 1.596774193548387, "no_speech_prob": 4.565880317386473e-06}, {"id": 690, "seek": 320816, "start": 3223.2, "end": 3229.6, "text": " this bidirectional component.", "tokens": [341, 12957, 621, 41048, 6542, 13], "temperature": 0.0, "avg_logprob": -0.11453983451746687, "compression_ratio": 1.596774193548387, "no_speech_prob": 4.565880317386473e-06}, {"id": 691, "seek": 320816, "start": 3229.6, "end": 3234.04, "text": " Here we're just, this is just changing the order of it.", "tokens": [1692, 321, 434, 445, 11, 341, 307, 445, 4473, 264, 1668, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.11453983451746687, "compression_ratio": 1.596774193548387, "no_speech_prob": 4.565880317386473e-06}, {"id": 692, "seek": 323404, "start": 3234.04, "end": 3238.64, "text": " To put, I always get these backwards.", "tokens": [1407, 829, 11, 286, 1009, 483, 613, 12204, 13], "temperature": 0.0, "avg_logprob": -0.25144217411677044, "compression_ratio": 1.5132743362831858, "no_speech_prob": 1.6701136701158248e-05}, {"id": 693, "seek": 323404, "start": 3238.64, "end": 3246.24, "text": " I think we want the number of layers, the batch size, and then two, and then the number", "tokens": [286, 519, 321, 528, 264, 1230, 295, 7914, 11, 264, 15245, 2744, 11, 293, 550, 732, 11, 293, 550, 264, 1230], "temperature": 0.0, "avg_logprob": -0.25144217411677044, "compression_ratio": 1.5132743362831858, "no_speech_prob": 1.6701136701158248e-05}, {"id": 694, "seek": 323404, "start": 3246.24, "end": 3260.44, "text": " of hidden layers, or number of hidden states.", "tokens": [295, 7633, 7914, 11, 420, 1230, 295, 7633, 4368, 13], "temperature": 0.0, "avg_logprob": -0.25144217411677044, "compression_ratio": 1.5132743362831858, "no_speech_prob": 1.6701136701158248e-05}, {"id": 695, "seek": 326044, "start": 3260.44, "end": 3269.6, "text": " So here we're still calling the self.out encode.", "tokens": [407, 510, 321, 434, 920, 5141, 264, 2698, 13, 346, 2058, 1429, 13], "temperature": 0.0, "avg_logprob": -0.12864652856603845, "compression_ratio": 1.596774193548387, "no_speech_prob": 7.071666914271191e-06}, {"id": 696, "seek": 326044, "start": 3269.6, "end": 3275.28, "text": " So basically these two lines with the hid.view, that's basically just kind of formatting how", "tokens": [407, 1936, 613, 732, 3876, 365, 264, 16253, 13, 1759, 11, 300, 311, 1936, 445, 733, 295, 39366, 577], "temperature": 0.0, "avg_logprob": -0.12864652856603845, "compression_ratio": 1.596774193548387, "no_speech_prob": 7.071666914271191e-06}, {"id": 697, "seek": 326044, "start": 3275.28, "end": 3280.92, "text": " we're dealing with the fact that we now have this extra dimension since we've doubled it", "tokens": [321, 434, 6260, 365, 264, 1186, 300, 321, 586, 362, 341, 2857, 10139, 1670, 321, 600, 24405, 309], "temperature": 0.0, "avg_logprob": -0.12864652856603845, "compression_ratio": 1.596774193548387, "no_speech_prob": 7.071666914271191e-06}, {"id": 698, "seek": 326044, "start": 3280.92, "end": 3284.08, "text": " for the bidirectional aspect.", "tokens": [337, 264, 12957, 621, 41048, 4171, 13], "temperature": 0.0, "avg_logprob": -0.12864652856603845, "compression_ratio": 1.596774193548387, "no_speech_prob": 7.071666914271191e-06}, {"id": 699, "seek": 326044, "start": 3284.08, "end": 3287.58, "text": " So that's not specific to attention.", "tokens": [407, 300, 311, 406, 2685, 281, 3202, 13], "temperature": 0.0, "avg_logprob": -0.12864652856603845, "compression_ratio": 1.596774193548387, "no_speech_prob": 7.071666914271191e-06}, {"id": 700, "seek": 328758, "start": 3287.58, "end": 3293.84, "text": " This is relating to the fact that we have switched it to be bidirectional.", "tokens": [639, 307, 23968, 281, 264, 1186, 300, 321, 362, 16858, 309, 281, 312, 12957, 621, 41048, 13], "temperature": 0.0, "avg_logprob": -0.12995907568162487, "compression_ratio": 1.457516339869281, "no_speech_prob": 5.093640538689215e-06}, {"id": 701, "seek": 328758, "start": 3293.84, "end": 3297.92, "text": " The decoder is where we're going to deal with attention.", "tokens": [440, 979, 19866, 307, 689, 321, 434, 516, 281, 2028, 365, 3202, 13], "temperature": 0.0, "avg_logprob": -0.12995907568162487, "compression_ratio": 1.457516339869281, "no_speech_prob": 5.093640538689215e-06}, {"id": 702, "seek": 328758, "start": 3297.92, "end": 3307.2, "text": " And so we need, so we've already put kind of the encoder out and hid through linear", "tokens": [400, 370, 321, 643, 11, 370, 321, 600, 1217, 829, 733, 295, 264, 2058, 19866, 484, 293, 16253, 807, 8213], "temperature": 0.0, "avg_logprob": -0.12995907568162487, "compression_ratio": 1.457516339869281, "no_speech_prob": 5.093640538689215e-06}, {"id": 703, "seek": 328758, "start": 3307.2, "end": 3309.92, "text": " layers.", "tokens": [7914, 13], "temperature": 0.0, "avg_logprob": -0.12995907568162487, "compression_ratio": 1.457516339869281, "no_speech_prob": 5.093640538689215e-06}, {"id": 704, "seek": 330992, "start": 3309.92, "end": 3317.88, "text": " Here what we want to do is combine kind of the attention on the encoder and the attention", "tokens": [1692, 437, 321, 528, 281, 360, 307, 10432, 733, 295, 264, 3202, 322, 264, 2058, 19866, 293, 264, 3202], "temperature": 0.0, "avg_logprob": -0.14001587459019252, "compression_ratio": 2.006172839506173, "no_speech_prob": 2.29589250011486e-06}, {"id": 705, "seek": 330992, "start": 3317.88, "end": 3322.2400000000002, "text": " on the hidden layers into you.", "tokens": [322, 264, 7633, 7914, 666, 291, 13], "temperature": 0.0, "avg_logprob": -0.14001587459019252, "compression_ratio": 2.006172839506173, "no_speech_prob": 2.29589250011486e-06}, {"id": 706, "seek": 330992, "start": 3322.2400000000002, "end": 3326.84, "text": " And that's the information that we have about kind of each time step is the encoder for", "tokens": [400, 300, 311, 264, 1589, 300, 321, 362, 466, 733, 295, 1184, 565, 1823, 307, 264, 2058, 19866, 337], "temperature": 0.0, "avg_logprob": -0.14001587459019252, "compression_ratio": 2.006172839506173, "no_speech_prob": 2.29589250011486e-06}, {"id": 707, "seek": 330992, "start": 3326.84, "end": 3332.7200000000003, "text": " that time step and the, sorry, the result of the encoder for that time step and the", "tokens": [300, 565, 1823, 293, 264, 11, 2597, 11, 264, 1874, 295, 264, 2058, 19866, 337, 300, 565, 1823, 293, 264], "temperature": 0.0, "avg_logprob": -0.14001587459019252, "compression_ratio": 2.006172839506173, "no_speech_prob": 2.29589250011486e-06}, {"id": 708, "seek": 330992, "start": 3332.7200000000003, "end": 3336.2400000000002, "text": " hidden state for that time step.", "tokens": [7633, 1785, 337, 300, 565, 1823, 13], "temperature": 0.0, "avg_logprob": -0.14001587459019252, "compression_ratio": 2.006172839506173, "no_speech_prob": 2.29589250011486e-06}, {"id": 709, "seek": 333624, "start": 3336.24, "end": 3340.9599999999996, "text": " And then we're going to learn attention weights.", "tokens": [400, 550, 321, 434, 516, 281, 1466, 3202, 17443, 13], "temperature": 0.0, "avg_logprob": -0.10520798227061397, "compression_ratio": 1.7412935323383085, "no_speech_prob": 2.0904180928482674e-06}, {"id": 710, "seek": 333624, "start": 3340.9599999999996, "end": 3347.2, "text": " And so we're multiplying by V, this gives us the actual weights, V is what we're learning", "tokens": [400, 370, 321, 434, 30955, 538, 691, 11, 341, 2709, 505, 264, 3539, 17443, 11, 691, 307, 437, 321, 434, 2539], "temperature": 0.0, "avg_logprob": -0.10520798227061397, "compression_ratio": 1.7412935323383085, "no_speech_prob": 2.0904180928482674e-06}, {"id": 711, "seek": 333624, "start": 3347.2, "end": 3353.4399999999996, "text": " and this is just initialized to be a matrix.", "tokens": [293, 341, 307, 445, 5883, 1602, 281, 312, 257, 8141, 13], "temperature": 0.0, "avg_logprob": -0.10520798227061397, "compression_ratio": 1.7412935323383085, "no_speech_prob": 2.0904180928482674e-06}, {"id": 712, "seek": 333624, "start": 3353.4399999999996, "end": 3356.0, "text": " And then CTX stands for context.", "tokens": [400, 550, 19529, 55, 7382, 337, 4319, 13], "temperature": 0.0, "avg_logprob": -0.10520798227061397, "compression_ratio": 1.7412935323383085, "no_speech_prob": 2.0904180928482674e-06}, {"id": 713, "seek": 333624, "start": 3356.0, "end": 3361.7599999999998, "text": " And so we're multiplying the attention weights by the output from the encoder to get the", "tokens": [400, 370, 321, 434, 30955, 264, 3202, 17443, 538, 264, 5598, 490, 264, 2058, 19866, 281, 483, 264], "temperature": 0.0, "avg_logprob": -0.10520798227061397, "compression_ratio": 1.7412935323383085, "no_speech_prob": 2.0904180928482674e-06}, {"id": 714, "seek": 333624, "start": 3361.7599999999998, "end": 3364.2799999999997, "text": " context, but that's just a weighted average.", "tokens": [4319, 11, 457, 300, 311, 445, 257, 32807, 4274, 13], "temperature": 0.0, "avg_logprob": -0.10520798227061397, "compression_ratio": 1.7412935323383085, "no_speech_prob": 2.0904180928482674e-06}, {"id": 715, "seek": 336428, "start": 3364.28, "end": 3368.2200000000003, "text": " So we have the output from the encoder at each step and we're taking a weighted average", "tokens": [407, 321, 362, 264, 5598, 490, 264, 2058, 19866, 412, 1184, 1823, 293, 321, 434, 1940, 257, 32807, 4274], "temperature": 0.0, "avg_logprob": -0.08984355556154713, "compression_ratio": 1.7291666666666667, "no_speech_prob": 4.425432052812539e-06}, {"id": 716, "seek": 336428, "start": 3368.2200000000003, "end": 3371.0800000000004, "text": " of that to know what are the important steps.", "tokens": [295, 300, 281, 458, 437, 366, 264, 1021, 4439, 13], "temperature": 0.0, "avg_logprob": -0.08984355556154713, "compression_ratio": 1.7291666666666667, "no_speech_prob": 4.425432052812539e-06}, {"id": 717, "seek": 336428, "start": 3371.0800000000004, "end": 3376.6400000000003, "text": " So going back to this, the animal crossed the street because it was tired.", "tokens": [407, 516, 646, 281, 341, 11, 264, 5496, 14622, 264, 4838, 570, 309, 390, 5868, 13], "temperature": 0.0, "avg_logprob": -0.08984355556154713, "compression_ratio": 1.7291666666666667, "no_speech_prob": 4.425432052812539e-06}, {"id": 718, "seek": 336428, "start": 3376.6400000000003, "end": 3379.0400000000004, "text": " We want to know that it is referring to the animal.", "tokens": [492, 528, 281, 458, 300, 309, 307, 13761, 281, 264, 5496, 13], "temperature": 0.0, "avg_logprob": -0.08984355556154713, "compression_ratio": 1.7291666666666667, "no_speech_prob": 4.425432052812539e-06}, {"id": 719, "seek": 336428, "start": 3379.0400000000004, "end": 3386.84, "text": " We would put more, or hopefully the network would learn to put more weight on those steps.", "tokens": [492, 576, 829, 544, 11, 420, 4696, 264, 3209, 576, 1466, 281, 829, 544, 3364, 322, 729, 4439, 13], "temperature": 0.0, "avg_logprob": -0.08984355556154713, "compression_ratio": 1.7291666666666667, "no_speech_prob": 4.425432052812539e-06}, {"id": 720, "seek": 336428, "start": 3386.84, "end": 3392.1800000000003, "text": " So this is kind of where the piece with attention is happening.", "tokens": [407, 341, 307, 733, 295, 689, 264, 2522, 365, 3202, 307, 2737, 13], "temperature": 0.0, "avg_logprob": -0.08984355556154713, "compression_ratio": 1.7291666666666667, "no_speech_prob": 4.425432052812539e-06}, {"id": 721, "seek": 339218, "start": 3392.18, "end": 3397.24, "text": " And then down here into our GRU, we're going to feed both the embedding and the context.", "tokens": [400, 550, 760, 510, 666, 527, 10903, 52, 11, 321, 434, 516, 281, 3154, 1293, 264, 12240, 3584, 293, 264, 4319, 13], "temperature": 0.0, "avg_logprob": -0.12681746777193045, "compression_ratio": 1.5175879396984924, "no_speech_prob": 1.0952931916108355e-05}, {"id": 722, "seek": 339218, "start": 3397.24, "end": 3404.44, "text": " And again, the context is just the weighted average of these hidden layers having learned", "tokens": [400, 797, 11, 264, 4319, 307, 445, 264, 32807, 4274, 295, 613, 7633, 7914, 1419, 3264], "temperature": 0.0, "avg_logprob": -0.12681746777193045, "compression_ratio": 1.5175879396984924, "no_speech_prob": 1.0952931916108355e-05}, {"id": 723, "seek": 339218, "start": 3404.44, "end": 3408.2799999999997, "text": " kind of what's important.", "tokens": [733, 295, 437, 311, 1021, 13], "temperature": 0.0, "avg_logprob": -0.12681746777193045, "compression_ratio": 1.5175879396984924, "no_speech_prob": 1.0952931916108355e-05}, {"id": 724, "seek": 339218, "start": 3408.2799999999997, "end": 3414.08, "text": " So you can see this in the forward, sorry, yeah, in forward.", "tokens": [407, 291, 393, 536, 341, 294, 264, 2128, 11, 2597, 11, 1338, 11, 294, 2128, 13], "temperature": 0.0, "avg_logprob": -0.12681746777193045, "compression_ratio": 1.5175879396984924, "no_speech_prob": 1.0952931916108355e-05}, {"id": 725, "seek": 339218, "start": 3414.08, "end": 3416.24, "text": " It's kind of very similar to before.", "tokens": [467, 311, 733, 295, 588, 2531, 281, 949, 13], "temperature": 0.0, "avg_logprob": -0.12681746777193045, "compression_ratio": 1.5175879396984924, "no_speech_prob": 1.0952931916108355e-05}, {"id": 726, "seek": 341624, "start": 3416.24, "end": 3425.4399999999996, "text": " We do have to kind of initialize this encoder attention, which is just a linear layer up", "tokens": [492, 360, 362, 281, 733, 295, 5883, 1125, 341, 2058, 19866, 3202, 11, 597, 307, 445, 257, 8213, 4583, 493], "temperature": 0.0, "avg_logprob": -0.12567062811418014, "compression_ratio": 1.6770833333333333, "no_speech_prob": 4.860326953348704e-06}, {"id": 727, "seek": 341624, "start": 3425.4399999999996, "end": 3426.4399999999996, "text": " here.", "tokens": [510, 13], "temperature": 0.0, "avg_logprob": -0.12567062811418014, "compression_ratio": 1.6770833333333333, "no_speech_prob": 4.860326953348704e-06}, {"id": 728, "seek": 341624, "start": 3426.4399999999996, "end": 3430.2799999999997, "text": " Hidden attention is also a linear layer.", "tokens": [41156, 3202, 307, 611, 257, 8213, 4583, 13], "temperature": 0.0, "avg_logprob": -0.12567062811418014, "compression_ratio": 1.6770833333333333, "no_speech_prob": 4.860326953348704e-06}, {"id": 729, "seek": 341624, "start": 3430.2799999999997, "end": 3433.04, "text": " So we called a linear layer inside our for loop.", "tokens": [407, 321, 1219, 257, 8213, 4583, 1854, 527, 337, 6367, 13], "temperature": 0.0, "avg_logprob": -0.12567062811418014, "compression_ratio": 1.6770833333333333, "no_speech_prob": 4.860326953348704e-06}, {"id": 730, "seek": 341624, "start": 3433.04, "end": 3436.3599999999997, "text": " We're still calling decoder the same way we did before.", "tokens": [492, 434, 920, 5141, 979, 19866, 264, 912, 636, 321, 630, 949, 13], "temperature": 0.0, "avg_logprob": -0.12567062811418014, "compression_ratio": 1.6770833333333333, "no_speech_prob": 4.860326953348704e-06}, {"id": 731, "seek": 341624, "start": 3436.3599999999997, "end": 3439.8799999999997, "text": " We're keeping track of all our outputs.", "tokens": [492, 434, 5145, 2837, 295, 439, 527, 23930, 13], "temperature": 0.0, "avg_logprob": -0.12567062811418014, "compression_ratio": 1.6770833333333333, "no_speech_prob": 4.860326953348704e-06}, {"id": 732, "seek": 341624, "start": 3439.8799999999997, "end": 3441.58, "text": " We're still going to use teacher forcing.", "tokens": [492, 434, 920, 516, 281, 764, 5027, 19030, 13], "temperature": 0.0, "avg_logprob": -0.12567062811418014, "compression_ratio": 1.6770833333333333, "no_speech_prob": 4.860326953348704e-06}, {"id": 733, "seek": 344158, "start": 3441.58, "end": 3448.6, "text": " So this is kind of a, today we've been incrementally improving our model kind of by adding more", "tokens": [407, 341, 307, 733, 295, 257, 11, 965, 321, 600, 668, 26200, 379, 11470, 527, 2316, 733, 295, 538, 5127, 544], "temperature": 0.0, "avg_logprob": -0.17066273790724734, "compression_ratio": 1.5209302325581395, "no_speech_prob": 1.1842704225273337e-05}, {"id": 734, "seek": 344158, "start": 3448.6, "end": 3450.84, "text": " to it.", "tokens": [281, 309, 13], "temperature": 0.0, "avg_logprob": -0.17066273790724734, "compression_ratio": 1.5209302325581395, "no_speech_prob": 1.1842704225273337e-05}, {"id": 735, "seek": 344158, "start": 3450.84, "end": 3455.7599999999998, "text": " It's helpful to look at what the size of all the pieces are.", "tokens": [467, 311, 4961, 281, 574, 412, 437, 264, 2744, 295, 439, 264, 3755, 366, 13], "temperature": 0.0, "avg_logprob": -0.17066273790724734, "compression_ratio": 1.5209302325581395, "no_speech_prob": 1.1842704225273337e-05}, {"id": 736, "seek": 344158, "start": 3455.7599999999998, "end": 3462.04, "text": " So here hidden is two by 64 by 300.", "tokens": [407, 510, 7633, 307, 732, 538, 12145, 538, 6641, 13], "temperature": 0.0, "avg_logprob": -0.17066273790724734, "compression_ratio": 1.5209302325581395, "no_speech_prob": 1.1842704225273337e-05}, {"id": 737, "seek": 344158, "start": 3462.04, "end": 3464.72, "text": " You'll remember 64 is the batch size.", "tokens": [509, 603, 1604, 12145, 307, 264, 15245, 2744, 13], "temperature": 0.0, "avg_logprob": -0.17066273790724734, "compression_ratio": 1.5209302325581395, "no_speech_prob": 1.1842704225273337e-05}, {"id": 738, "seek": 344158, "start": 3464.72, "end": 3471.56, "text": " The two is since it's bidirectional and the 300 is the embedding size that we were using.", "tokens": [440, 732, 307, 1670, 309, 311, 12957, 621, 41048, 293, 264, 6641, 307, 264, 12240, 3584, 2744, 300, 321, 645, 1228, 13], "temperature": 0.0, "avg_logprob": -0.17066273790724734, "compression_ratio": 1.5209302325581395, "no_speech_prob": 1.1842704225273337e-05}, {"id": 739, "seek": 347156, "start": 3471.56, "end": 3481.0, "text": " For attention on the encoder, 64 for the batch size.", "tokens": [1171, 3202, 322, 264, 2058, 19866, 11, 12145, 337, 264, 15245, 2744, 13], "temperature": 0.0, "avg_logprob": -0.17730473147498238, "compression_ratio": 1.4761904761904763, "no_speech_prob": 1.95252760022413e-05}, {"id": 740, "seek": 347156, "start": 3481.0, "end": 3483.12, "text": " 30 is the length of our sequence.", "tokens": [2217, 307, 264, 4641, 295, 527, 8310, 13], "temperature": 0.0, "avg_logprob": -0.17730473147498238, "compression_ratio": 1.4761904761904763, "no_speech_prob": 1.95252760022413e-05}, {"id": 741, "seek": 347156, "start": 3483.12, "end": 3489.4, "text": " And that's kind of a key dimension of wanting to know which of the 30 is most important", "tokens": [400, 300, 311, 733, 295, 257, 2141, 10139, 295, 7935, 281, 458, 597, 295, 264, 2217, 307, 881, 1021], "temperature": 0.0, "avg_logprob": -0.17730473147498238, "compression_ratio": 1.4761904761904763, "no_speech_prob": 1.95252760022413e-05}, {"id": 742, "seek": 347156, "start": 3489.4, "end": 3490.4, "text": " as you go through it.", "tokens": [382, 291, 352, 807, 309, 13], "temperature": 0.0, "avg_logprob": -0.17730473147498238, "compression_ratio": 1.4761904761904763, "no_speech_prob": 1.95252760022413e-05}, {"id": 743, "seek": 347156, "start": 3490.4, "end": 3501.16, "text": " And then 300 is the size, again, of the embeddings.", "tokens": [400, 550, 6641, 307, 264, 2744, 11, 797, 11, 295, 264, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.17730473147498238, "compression_ratio": 1.4761904761904763, "no_speech_prob": 1.95252760022413e-05}, {"id": 744, "seek": 350116, "start": 3501.16, "end": 3505.16, "text": " Accuracy in weights, again, this is 64 by 30 since we're taking the weights at the different", "tokens": [5725, 374, 2551, 294, 17443, 11, 797, 11, 341, 307, 12145, 538, 2217, 1670, 321, 434, 1940, 264, 17443, 412, 264, 819], "temperature": 0.0, "avg_logprob": -0.2697604104374232, "compression_ratio": 1.438423645320197, "no_speech_prob": 8.139581950672437e-06}, {"id": 745, "seek": 350116, "start": 3505.16, "end": 3508.68, "text": " of the 30 time steps.", "tokens": [295, 264, 2217, 565, 4439, 13], "temperature": 0.0, "avg_logprob": -0.2697604104374232, "compression_ratio": 1.438423645320197, "no_speech_prob": 8.139581950672437e-06}, {"id": 746, "seek": 350116, "start": 3508.68, "end": 3510.68, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.2697604104374232, "compression_ratio": 1.438423645320197, "no_speech_prob": 8.139581950672437e-06}, {"id": 747, "seek": 350116, "start": 3510.68, "end": 3520.3999999999996, "text": " And so if we call all this and then train it, the accuracy is now 49%, almost 50%.", "tokens": [400, 370, 498, 321, 818, 439, 341, 293, 550, 3847, 309, 11, 264, 14170, 307, 586, 16513, 8923, 1920, 2625, 6856], "temperature": 0.0, "avg_logprob": -0.2697604104374232, "compression_ratio": 1.438423645320197, "no_speech_prob": 8.139581950672437e-06}, {"id": 748, "seek": 350116, "start": 3520.3999999999996, "end": 3522.8799999999997, "text": " And the blue score is up to 0.41.", "tokens": [400, 264, 3344, 6175, 307, 493, 281, 1958, 13, 17344, 13], "temperature": 0.0, "avg_logprob": -0.2697604104374232, "compression_ratio": 1.438423645320197, "no_speech_prob": 8.139581950672437e-06}, {"id": 749, "seek": 350116, "start": 3522.8799999999997, "end": 3528.3599999999997, "text": " Let me just review, go back to what it was previously.", "tokens": [961, 385, 445, 3131, 11, 352, 646, 281, 437, 309, 390, 8046, 13], "temperature": 0.0, "avg_logprob": -0.2697604104374232, "compression_ratio": 1.438423645320197, "no_speech_prob": 8.139581950672437e-06}, {"id": 750, "seek": 352836, "start": 3528.36, "end": 3538.4, "text": " So last time the accuracy was just 41%.", "tokens": [407, 1036, 565, 264, 14170, 390, 445, 18173, 6856], "temperature": 0.0, "avg_logprob": -0.1973425460188356, "compression_ratio": 1.3375796178343948, "no_speech_prob": 8.397778401558753e-06}, {"id": 751, "seek": 352836, "start": 3538.4, "end": 3539.6800000000003, "text": " We've gone up to 50%.", "tokens": [492, 600, 2780, 493, 281, 2625, 6856], "temperature": 0.0, "avg_logprob": -0.1973425460188356, "compression_ratio": 1.3375796178343948, "no_speech_prob": 8.397778401558753e-06}, {"id": 752, "seek": 352836, "start": 3539.6800000000003, "end": 3541.38, "text": " That's a nice improvement.", "tokens": [663, 311, 257, 1481, 10444, 13], "temperature": 0.0, "avg_logprob": -0.1973425460188356, "compression_ratio": 1.3375796178343948, "no_speech_prob": 8.397778401558753e-06}, {"id": 753, "seek": 352836, "start": 3541.38, "end": 3548.6, "text": " And the blue score has gone from 0.31 to, what did I say, 0.4?", "tokens": [400, 264, 3344, 6175, 575, 2780, 490, 1958, 13, 12967, 281, 11, 437, 630, 286, 584, 11, 1958, 13, 19, 30], "temperature": 0.0, "avg_logprob": -0.1973425460188356, "compression_ratio": 1.3375796178343948, "no_speech_prob": 8.397778401558753e-06}, {"id": 754, "seek": 352836, "start": 3548.6, "end": 3552.0, "text": " And higher blue scores are better, 0.41.", "tokens": [400, 2946, 3344, 13444, 366, 1101, 11, 1958, 13, 17344, 13], "temperature": 0.0, "avg_logprob": -0.1973425460188356, "compression_ratio": 1.3375796178343948, "no_speech_prob": 8.397778401558753e-06}, {"id": 755, "seek": 352836, "start": 3552.0, "end": 3553.0, "text": " So it's improved.", "tokens": [407, 309, 311, 9689, 13], "temperature": 0.0, "avg_logprob": -0.1973425460188356, "compression_ratio": 1.3375796178343948, "no_speech_prob": 8.397778401558753e-06}, {"id": 756, "seek": 355300, "start": 3553.0, "end": 3559.32, "text": " All right, so we're at time since I need to head to the airport.", "tokens": [1057, 558, 11, 370, 321, 434, 412, 565, 1670, 286, 643, 281, 1378, 281, 264, 10155, 13], "temperature": 0.0, "avg_logprob": -0.19217866208372997, "compression_ratio": 1.6550387596899225, "no_speech_prob": 4.6107576054055244e-05}, {"id": 757, "seek": 355300, "start": 3559.32, "end": 3561.36, "text": " I'll go over this again next time.", "tokens": [286, 603, 352, 670, 341, 797, 958, 565, 13], "temperature": 0.0, "avg_logprob": -0.19217866208372997, "compression_ratio": 1.6550387596899225, "no_speech_prob": 4.6107576054055244e-05}, {"id": 758, "seek": 355300, "start": 3561.36, "end": 3565.76, "text": " So next time we will talk about attention in more detail.", "tokens": [407, 958, 565, 321, 486, 751, 466, 3202, 294, 544, 2607, 13], "temperature": 0.0, "avg_logprob": -0.19217866208372997, "compression_ratio": 1.6550387596899225, "no_speech_prob": 4.6107576054055244e-05}, {"id": 759, "seek": 355300, "start": 3565.76, "end": 3569.64, "text": " We'll also kind of depend it on time, either next time or the time after that, get into", "tokens": [492, 603, 611, 733, 295, 5672, 309, 322, 565, 11, 2139, 958, 565, 420, 264, 565, 934, 300, 11, 483, 666], "temperature": 0.0, "avg_logprob": -0.19217866208372997, "compression_ratio": 1.6550387596899225, "no_speech_prob": 4.6107576054055244e-05}, {"id": 760, "seek": 355300, "start": 3569.64, "end": 3571.88, "text": " kind of the details of what GRUs are doing.", "tokens": [733, 295, 264, 4365, 295, 437, 10903, 29211, 366, 884, 13], "temperature": 0.0, "avg_logprob": -0.19217866208372997, "compression_ratio": 1.6550387596899225, "no_speech_prob": 4.6107576054055244e-05}, {"id": 761, "seek": 355300, "start": 3571.88, "end": 3576.56, "text": " But for now I wanted you to see kind of these incremental improvements on how we can make", "tokens": [583, 337, 586, 286, 1415, 291, 281, 536, 733, 295, 613, 35759, 13797, 322, 577, 321, 393, 652], "temperature": 0.0, "avg_logprob": -0.19217866208372997, "compression_ratio": 1.6550387596899225, "no_speech_prob": 4.6107576054055244e-05}, {"id": 762, "seek": 355300, "start": 3576.56, "end": 3578.28, "text": " our seek to seek model better.", "tokens": [527, 8075, 281, 8075, 2316, 1101, 13], "temperature": 0.0, "avg_logprob": -0.19217866208372997, "compression_ratio": 1.6550387596899225, "no_speech_prob": 4.6107576054055244e-05}, {"id": 763, "seek": 355300, "start": 3578.28, "end": 3579.28, "text": " Cool.", "tokens": [8561, 13], "temperature": 0.0, "avg_logprob": -0.19217866208372997, "compression_ratio": 1.6550387596899225, "no_speech_prob": 4.6107576054055244e-05}, {"id": 764, "seek": 357928, "start": 3579.28, "end": 3583.2000000000003, "text": " Thank you.", "tokens": [50364, 1044, 291, 13, 50560], "temperature": 0.0, "avg_logprob": -0.7106300989786783, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.00018169979739468545}], "language": "en"}